
Eric Lehman
Google Inc.
F Thomson Leighton
Department of Mathematics
and the Computer Science and AI Laboratory,
Massachussetts Institute of Technology;
Akamai Technologies
Albert R Meyer
Department of Electrical Engineering and Computer Science
and the Computer Science and AI Laboratory,
Massachussetts Institute of Technology
Mathematics for Computer Science
revised Monday 4th June, 2012, 15:49
Copyright ¬© 
, Eric Lehman, F Tom Leighton, Albert R Meyer . All rights reserved.
2012


Contents
I
Proofs
1
What is a Proof?
5
1.1
Propositions
5
1.2
Predicates
8
1.3
The Axiomatic Method
8
1.4
Our Axioms
9
1.5
Proving an Implication
11
1.6
Proving an ‚ÄúIf and Only If‚Äù
13
1.7
Proof by Cases
15
1.8
Proof by Contradiction
16
1.9
Good Proofs in Practice
17
2
The Well Ordering Principle
25
2.1
Well Ordering Proofs
25
2.2
Template for Well Ordering Proofs
26
2.3
Factoring into Primes
28
2.4
Well Ordered Sets
29
3
Logical Formulas
37
3.1
Propositions from Propositions
38
3.2
Propositional Logic in Computer Programs
41
3.3
Equivalence and Validity
44
3.4
The Algebra of Propositions
46
3.5
The SAT Problem
51
3.6
Predicate Formulas
52
4
Mathematical Data Types
73
4.1
Sets
73
4.2
Sequences
77
4.3
Functions
77
4.4
Binary Relations
80
4.5
Finite Cardinality
84
5
Induction
99
5.1
Ordinary Induction
99
5.2
Strong Induction
108
5.3
Strong Induction vs. Induction vs. Well Ordering
113

Contents
iv
5.4
State Machines
114
6
Recursive Data Types
151
6.1
Recursive DeÔ¨Ånitions and Structural Induction
151
6.2
Strings of Matched Brackets
155
6.3
Recursive Functions on Nonnegative Integers
158
6.4
Arithmetic Expressions
161
6.5
Induction in Computer Science
166
7
InÔ¨Ånite Sets
179
7.1
InÔ¨Ånite Cardinality
180
7.2
The Halting Problem
184
7.3
The Logic of Sets
188
7.4
Does All This Really Work?
191
8
Number Theory
203
8.1
Divisibility
203
8.2
The Greatest Common Divisor
208
8.3
Prime Mysteries
214
8.4
The Fundamental Theorem of Arithmetic
217
8.5
Alan Turing
219
8.6
Modular Arithmetic
223
8.7
Remainder Arithmetic
225
8.8
Turing‚Äôs Code (Version 2.0)
228
8.9
Multiplicative Inverses and Cancelling
230
8.10 Euler‚Äôs Theorem
234
8.11 RSA Public Key Encryption
241
8.12 What has SAT got to do with it?
244
II Structures
9
Directed graphs & Partial Orders
273
9.1
Digraphs & Vertex Degrees
275
9.2
Adjacency Matrices
279
9.3
Walk Relations
282
9.4
Directed Acyclic Graphs & Partial Orders
283
9.5
Weak Partial Orders
286
9.6
Representing Partial Orders by Set Containment
288
9.7
Path-Total Orders
289
9.8
Product Orders
290

Contents
v
9.9
Scheduling
291
9.10 Equivalence Relations
297
9.11 Summary of Relational Properties
299
10
Communication Networks
325
10.1 Complete Binary Tree
325
10.2 Routing Problems
325
10.3 Network Diameter
326
10.4 Switch Count
327
10.5 Network Latency
328
10.6 Congestion
328
10.7 2-D Array
329
10.8 ButterÔ¨Çy
331
10.9 BeneÀás Network
333
11
Simple Graphs
345
11.1 Vertex Adjacency and Degrees
345
11.2 Sexual Demographics in America
347
11.3 Some Common Graphs
349
11.4 Isomorphism
351
11.5 Bipartite Graphs & Matchings
353
11.6 The Stable Marriage Problem
358
11.7 Coloring
365
11.8 Getting from u to v in a Graph
370
11.9 Connectivity
371
11.10 Odd Cycles and 2-Colorability
375
11.11 Forests & Trees
376
12
Planar Graphs
413
12.1 Drawing Graphs in the Plane
413
12.2 DeÔ¨Ånitions of Planar Graphs
413
12.3 Euler‚Äôs Formula
424
12.4 Bounding the Number of Edges in a Planar Graph
425
12.5 Returning to K5 and K3;3
426
12.6 Coloring Planar Graphs
427
12.7 Classifying Polyhedra
429
12.8 Another Characterization for Planar Graphs
432

Contents
vi
III Counting
13
Sums and Asymptotics
443
13.1 The Value of an Annuity
444
13.2 Sums of Powers
450
13.3 Approximating Sums
452
13.4 Hanging Out Over the Edge
456
13.5 Products
463
13.6 Double Trouble
465
13.7 Asymptotic Notation
468
14
Cardinality Rules
487
14.1 Counting One Thing by Counting Another
487
14.2 Counting Sequences
488
14.3 The Generalized Product Rule
491
14.4 The Division Rule
495
14.5 Counting Subsets
498
14.6 Sequences with Repetitions
500
14.7 Counting Practice: Poker Hands
503
14.8 The Pigeonhole Principle
508
14.9 Inclusion-Exclusion
518
14.10 Combinatorial Proofs
524
15
Generating Functions
559
15.1 InÔ¨Ånite Series
559
15.2 Counting with Generating Functions
560
15.3 Partial Fractions
567
15.4 Solving Linear Recurrences
569
15.5 Formal Power Series
575
IV Probability
16
Events and Probability Spaces
591
16.1 Let‚Äôs Make a Deal
591
16.2 The Four Step Method
592
16.3 Strange Dice
601
16.4 Set Theory and Probability
608
16.5 Conditional Probability
614
16.6 Independence
626

Contents
vii
17
Random Variables
659
17.1 Random Variable Examples
659
17.2 Independence
661
17.3 Distribution Functions
662
17.4 Great Expectations
670
17.5 Linearity of Expectation
682
18
Deviation from the Mean
707
18.1 Why the Mean?
707
18.2 Markov‚Äôs Theorem
708
18.3 Chebyshev‚Äôs Theorem
710
18.4 Properties of Variance
714
18.5 Estimation by Random Sampling
719
18.6 ConÔ¨Ådence versus Probability
724
18.7 Sums of Random Variables
725
18.8 Really Great Expectations
735
19
Random Processes
755
19.1 Gamblers‚Äô Ruin
755
19.2 Random Walks on Graphs
764
V
Recurrences
20
Recurrences
783
20.1 The Towers of Hanoi
783
20.2 Merge Sort
786
20.3 Linear Recurrences
790
20.4 Divide-and-Conquer Recurrences
797
20.5 A Feel for Recurrences
804
Bibliography
806
Index
808


I
Proofs


Introduction
This text explains how to use mathematical models and methods to analyze prob-
lems that arise in computer science. Proofs play a central role in this work because
the authors share a belief with most mathematicians that proofs are essential for
genuine understanding. Proofs also play a growing role in computer science; they
are used to certify that software and hardware will always behave correctly, some-
thing that no amount of testing can do.
Simply put, a proof is a method of establishing truth. Like beauty, ‚Äútruth‚Äù some-
times depends on the eye of the beholder, and it should not be surprising that what
constitutes a proof differs among Ô¨Åelds. For example, in the judicial system, legal
truth is decided by a jury based on the allowable evidence presented at trial. In the
business world, authoritative truth is speciÔ¨Åed by a trusted person or organization,
or maybe just your boss. In Ô¨Åelds such as physics or biology, scientiÔ¨Åc truth1 is
conÔ¨Årmed by experiment. In statistics, probable truth is established by statistical
analysis of sample data.
Philosophical proof involves careful exposition and persuasion typically based
on a series of small, plausible arguments. The best example begins with ‚ÄúCogito
ergo sum,‚Äù a Latin sentence that translates as ‚ÄúI think, therefore I am.‚Äù This phrase
comes from the beginning of a 17th century essay by the mathematician/philosopher,
Ren¬¥e Descartes, and it is one of the most famous quotes in the world: do a web
search for it, and you will be Ô¨Çooded with hits.
1Actually, only scientiÔ¨Åc falsehood can be demonstrated by an experiment‚Äîwhen the experiment
fails to behave as predicted. But no amount of experiment can conÔ¨Årm that the next experiment won‚Äôt
fail. For this reason, scientists rarely speak of truth, but rather of theories that accurately predict past,
and anticipated future, experiments.

Part I
Proofs
4
Deducing your existence from the fact that you‚Äôre thinking about your existence
is a pretty cool and persuasive-sounding idea. However, with just a few more lines
of argument in this vein, Descartes goes on to conclude that there is an inÔ¨Ånitely
beneÔ¨Åcent God. Whether or not you believe in an inÔ¨Ånitely beneÔ¨Åcent God, you‚Äôll
probably agree that any very short ‚Äúproof‚Äù of God‚Äôs inÔ¨Ånite beneÔ¨Åcence is bound
to be far-fetched. So even in masterful hands, this approach is not reliable.
Mathematics has its own speciÔ¨Åc notion of ‚Äúproof.‚Äù
DeÔ¨Ånition. A mathematical proof of a proposition is a chain of logical deductions
leading to the proposition from a base set of axioms.
The three key ideas in this deÔ¨Ånition are highlighted: proposition, logical deduc-
tion, and axiom. Chapter 1 examines these three ideas along with some basic ways
of organizing proofs. Chapter 2 introduces the Well Ordering Principle, a basic
method of proof; later, Chapter 5 introduces the closely related proof method of
Induction.
If you‚Äôre going to prove a proposition, you‚Äôd better have a precise understand-
ing of what the proposition means. To avoid ambiguity and uncertain deÔ¨Ånitions
in ordinary language, mathematicians use language very precisely, and they often
express propositions using logical formulas; these are the subject of Chapter 3.
The Ô¨Årst three Chapters assume the reader is familiar with a few mathematical
concepts like sets and functions. Chapters 4 and 7 offer a more careful look at
such mathematical data types, examining in particular properties and methods for
proving things about inÔ¨Ånite sets. Chapter 6 goes on to examine recursively deÔ¨Åned
data types.
Number theory is the study of properties of the integers. This part of the text
ends with Chapter 8 on Number theory because there are lots of easy-to-state and
interesting-to-prove properties of numbers. This subject was once thought to have
few, if any, practical applications, but it has turned out to have multiple applications
in Computer Science. For example, most modern data encryption methods are
based on Number theory.

1
What is a Proof?
1.1
Propositions
DeÔ¨Ånition. A proposition is a statement that is either true or false.
For example, both of the following statements are propositions. The Ô¨Årst is true,
and the second is false.
Proposition 1.1.1. 2 + 3 = 5.
Proposition 1.1.2. 1 + 1 = 3.
Being true or false doesn‚Äôt sound like much of a limitation, but it does exclude
statements such as, ‚ÄúWherefore art thou Romeo?‚Äù and ‚ÄúGive me an A!‚Äù It also ex-
cludes statements whose truth varies with circumstance such as, ‚ÄúIt‚Äôs Ô¨Åve o‚Äôclock,‚Äù
or ‚Äúthe stock market will rise tomorrow.‚Äù
Unfortunately it is not always easy to decide if a proposition is true or false:
Proposition 1.1.3. For every nonnegative integer, n, the value of n2 C n C 41 is
prime.
(A prime is an integer greater than 1 that is not divisible by any other integer
greater than 1. For example, 2, 3, 5, 7, 11, are the Ô¨Årst Ô¨Åve primes.) Let‚Äôs try some
numerical experimentation to check this proposition. Let 1
p.n/ WWD n2 C n C 41:
(1.1)
We begin with p.0/ D 41 which is prime; then
p.1/ D 43; p.2/ D 47; p.3/ D 53; : : : ; p.20/ D 461
are each prime. Hmmm, starts to look like a plausible claim. In fact we can keep
checking through n D 39 and conÔ¨Årm that p.39/ D 1601 is prime.
But p.40/ D 402 C 40 C 41 D 41  41, which is not prime. So it‚Äôs not true that
the expression is prime for all nonnegative integers. In fact, it‚Äôs not hard to show
that no polynomial with integer coefÔ¨Åcients can map all nonnegative numbers into
prime numbers, unless it‚Äôs a constant (see Problem 1.6). The point is that in general
1The symbol WWD means ‚Äúequal by deÔ¨Ånition.‚Äù It‚Äôs always ok simply to write ‚Äú=‚Äù instead of WWD,
but reminding the reader that an equality holds by deÔ¨Ånition can be helpful.

Chapter 1
What is a Proof?
6
you can‚Äôt check a claim about an inÔ¨Ånite set by checking a Ô¨Ånite set of its elements,
no matter how large the Ô¨Ånite set.
By the way, propositions like this about all numbers or all items of some kind
are so common that there is a special notation for them. With this notation, Propo-
sition 1.1.3 would be
8n 2 N: p.n/ is prime:
(1.2)
Here the symbol 8 is read ‚Äúfor all.‚Äù The symbol N stands for the set of nonnegative
integers, namely, 0, 1, 2, 3, ...(ask your instructor for the complete list). The
symbol ‚Äú2‚Äù is read as ‚Äúis a member of,‚Äù or ‚Äúbelongs to,‚Äù or simply as ‚Äúis in.‚Äù The
period after the N is just a separator between phrases.
Here are two even more extreme examples:
Proposition 1.1.4. [Euler‚Äôs Conjecture] The equation
a4 C b4 C c4 D d 4
has no solution when a; b; c; d are positive integers.
Euler (pronounced ‚Äúoiler‚Äù) conjectured this in 1769. But the proposition was
proved false 218 years later by Noam Elkies at a liberal arts school up Mass Ave.
The solution he found was a D 95800; b D 217519; c D 414560; d D 422481.
In logical notation, Euler‚Äôs Conjecture could be written,
8a 2 ZC 8b 2 ZC 8c 2 ZC 8d 2 ZC: a4 C b4 C c4 ¬§ d 4:
Here, ZC is a symbol for the positive integers. Strings of 8‚Äôs like this are usually
abbreviated for easier reading:
8a; b; c; d 2 ZC: a4 C b4 C c4 ¬§ d 4:
Proposition 1.1.5. 313.x3 C y3/ D z3 has no solution when x; y; z 2 ZC.
This proposition is also false, but the smallest counterexample has more than
1000 digits!
It‚Äôs worth mentioning a couple of further famous propositions whose proofs were
sought for centuries before Ô¨Ånally being discovered:
Proposition 1.1.6 (Four Color Theorem). Every map can be colored with 4 colors
so that adjacent2 regions have different colors.
2Two regions are adjacent only when they share a boundary segment of positive length. They are
not considered to be adjacent if their boundaries meet only at a few points.

1.1. Propositions
7
Several incorrect proofs of this theorem have been published, including one that
stood for 10 years in the late 19th century before its mistake was found. A laborious
proof was Ô¨Ånally found in 1976 by mathematicians Appel and Haken, who used a
complex computer program to categorize the four-colorable maps; the program left
a few thousand maps uncategorized, and these were checked by hand by Haken
and his assistants ‚Äîincluding his 15-year-old daughter. There was reason to doubt
whether this was a legitimate proof: the proof was too big to be checked without a
computer, and no one could guarantee that the computer calculated correctly, nor
was anyone enthusiastic about exerting the effort to recheck the four-colorings of
thousands of maps that were done by hand. Two decades later a mostly intelligible
proof of the Four Color Theorem was found, though a computer is still needed to
check four-colorability of several hundred special maps.3
Proposition 1.1.7 (Fermat‚Äôs Last Theorem). There are no positive integers x, y,
and z such that
xn C yn D zn
for some integer n > 2.
In a book he was reading around 1630, Fermat claimed to have a proof but not
enough space in the margin to write it down. Over the years it was proved to hold
for all n up to 4,000,000, but we‚Äôve seen that this shouldn‚Äôt necessarily inspire
conÔ¨Ådence that it holds for all n; there is, after all, a clear resemblance between
Fermat‚Äôs Last Theorem and Euler‚Äôs false Conjecture. Finally, in 1994, Andrew
Wiles gave a proof, after seven years of working in secrecy and isolation in his
attic. His proof did not Ô¨Åt in any margin.4
Finally, let‚Äôs mention another simply stated proposition whose truth remains un-
known.
Proposition 1.1.8 (Goldbach‚Äôs Conjecture). Every even integer greater than 2 is
the sum of two primes.
Goldbach‚Äôs Conjecture dates back to 1742. It is known to hold for all numbers
up to 1016, but to this day, no one knows whether it‚Äôs true or false.
For a computer scientist, some of the most important things to prove are the
correctness of programs and systems ‚Äîwhether a program or system does what
3The story of the proof of the Four Color Theorem is told in a well-reviewed popular (non-
technical) book: ‚ÄúFour Colors SufÔ¨Åce. How the Map Problem was Solved.‚Äù Robin Wilson. Princeton
Univ. Press, 2003, 276pp. ISBN 0-691-11533-8.
4In fact, Wiles‚Äô original proof was wrong, but he and several collaborators used his ideas to arrive
at a correct proof a year later. This story is the subject of the popular book, Fermat‚Äôs Enigma by
Simon Singh, Walker & Company, November, 1997.

Chapter 1
What is a Proof?
8
it‚Äôs supposed to. Programs are notoriously buggy, and there‚Äôs a growing community
of researchers and practitioners trying to Ô¨Ånd ways to prove program correctness.
These efforts have been successful enough in the case of CPU chips that they are
now routinely used by leading chip manufacturers to prove chip correctness and
avoid mistakes like the notorious Intel division bug in the 1990‚Äôs.
Developing mathematical methods to verify programs and systems remains an
active research area. We‚Äôll illustrate some of these methods in Chapter 5.
1.2
Predicates
A predicate is a proposition whose truth depends on the value of one or more vari-
ables.
Most of the propositions above were deÔ¨Åned in terms of predicates. For example,
‚Äún is a perfect square‚Äù
is a predicate whose truth depends on the value of n. The predicate is true for n D 4
since four is a perfect square, but false for n D 5 since Ô¨Åve is not a perfect square.
Like other propositions, predicates are often named with a letter. Furthermore, a
function-like notation is used to denote a predicate supplied with speciÔ¨Åc variable
values. For example, we might name our earlier predicate P :
P.n/ WWD ‚Äún is a perfect square‚Äù:
So P.4/ is true, and P.5/ is false.
This notation for predicates is confusingly similar to ordinary function notation.
If P is a predicate, then P.n/ is either true or false, depending on the value of n.
On the other hand, if p is an ordinary function, like n2C1, then p.n/ is a numerical
quantity. Don‚Äôt confuse these two!
1.3
The Axiomatic Method
The standard procedure for establishing truth in mathematics was invented by Eu-
clid, a mathematician working in Alexandria, Egypt around 300 BC. His idea was
to begin with Ô¨Åve assumptions about geometry, which seemed undeniable based on
direct experience. (For example, ‚ÄúThere is a straight line segment between every
pair of points.) Propositions like these that are simply accepted as true are called
axioms.

1.4. Our Axioms
9
Starting from these axioms, Euclid established the truth of many additional propo-
sitions by providing ‚Äúproofs.‚Äù A proof is a sequence of logical deductions from
axioms and previously-proved statements that concludes with the proposition in
question. You probably wrote many proofs in high school geometry class, and
you‚Äôll see a lot more in this text.
There are several common terms for a proposition that has been proved. The
different terms hint at the role of the proposition within a larger body of work.
 Important true propositions are called theorems.
 A lemma is a preliminary proposition useful for proving later propositions.
 A corollary is a proposition that follows in just a few logical steps from a
theorem.
These deÔ¨Ånitions are not precise. In fact, sometimes a good lemma turns out to be
far more important than the theorem it was originally used to prove.
Euclid‚Äôs axiom-and-proof approach, now called the axiomatic method, remains
the foundation for mathematics today. In fact, just a handful of axioms, called the
axioms Zermelo-Frankel with Choice (ZFC), together with a few logical deduction
rules, appear to be sufÔ¨Åcient to derive essentially all of mathematics. We‚Äôll examine
these in Chapter 7.
1.4
Our Axioms
The ZFC axioms are important in studying and justifying the foundations of math-
ematics, but for practical purposes, they are much too primitive. Proving theorems
in ZFC is a little like writing programs in byte code instead of a full-Ô¨Çedged pro-
gramming language‚Äîby one reckoning, a formal proof in ZFC that 2 C 2 D 4
requires more than 20,000 steps! So instead of starting with ZFC, we‚Äôre going to
take a huge set of axioms as our foundation: we‚Äôll accept all familiar facts from
high school math.
This will give us a quick launch, but you may Ô¨Ånd this imprecise speciÔ¨Åcation
of the axioms troubling at times. For example, in the midst of a proof, you may
start to wonder, ‚ÄúMust I prove this little fact or can I take it as an axiom?‚Äù There
really is no absolute answer, since what‚Äôs reasonable to assume and what requires
proof depends on the circumstances and the audience. A good general guideline is
simply to be up front about what you‚Äôre assuming.

Chapter 1
What is a Proof?
10
1.4.1
Logical Deductions
Logical deductions, or inference rules, are used to prove new propositions using
previously proved ones.
A fundamental inference rule is modus ponens. This rule says that a proof of P
together with a proof that P IMPLIES Q is a proof of Q.
Inference rules are sometimes written in a funny notation. For example, modus
ponens is written:
Rule.
P;
P IMPLIES Q
Q
When the statements above the line, called the antecedents, are proved, then we
can consider the statement below the line, called the conclusion or consequent, to
also be proved.
A key requirement of an inference rule is that it must be sound: an assignment
of truth values to the letters, P , Q, ..., that makes all the antecedents true must
also make the consequent true. So if we start off with true axioms and apply sound
inference rules, everything we prove will also be true.
There are many other natural, sound inference rules, for example:
Rule.
P IMPLIES Q;
Q IMPLIES R
P IMPLIES R
Rule.
NOT.P / IMPLIES NOT.Q/
Q IMPLIES P
On the other hand,
Non-Rule.
NOT.P / IMPLIES NOT.Q/
P IMPLIES Q
is not sound: if P is assigned T and Q is assigned F, then the antecedent is true
and the consequent is not.
Note that a propositional inference rule is sound precisely when the conjunction
(AND) of all its antecedents implies its consequent.
As with axioms, we will not be too formal about the set of legal inference rules.
Each step in a proof should be clear and ‚Äúlogical‚Äù; in particular, you should state
what previously proved facts are used to derive each new conclusion.

1.5. Proving an Implication
11
1.4.2
Patterns of Proof
In principle, a proof can be any sequence of logical deductions from axioms and
previously proved statements that concludes with the proposition in question. This
freedom in constructing a proof can seem overwhelming at Ô¨Årst. How do you even
start a proof?
Here‚Äôs the good news: many proofs follow one of a handful of standard tem-
plates. Each proof has it own details, of course, but these templates at least provide
you with an outline to Ô¨Åll in. We‚Äôll go through several of these standard patterns,
pointing out the basic idea and common pitfalls and giving some examples. Many
of these templates Ô¨Åt together; one may give you a top-level outline while others
help you at the next level of detail. And we‚Äôll show you other, more sophisticated
proof techniques later on.
The recipes below are very speciÔ¨Åc at times, telling you exactly which words to
write down on your piece of paper. You‚Äôre certainly free to say things your own
way instead; we‚Äôre just giving you something you could say so that you‚Äôre never at
a complete loss.
1.5
Proving an Implication
Propositions of the form ‚ÄúIf P , then Q‚Äù are called implications. This implication
is often rephrased as ‚ÄúP IMPLIES Q.‚Äù
Here are some examples:
 (Quadratic Formula) If ax2 C bx C c D 0 and a ¬§ 0, then
x D

 b Àô
p
b2   4ac

=2a:
 (Goldbach‚Äôs Conjecture 1.1.8 rephrased) If n is an even integer greater than
2, then n is a sum of two primes.
 If 0  x  2, then  x3 C 4x C 1 > 0.
There are a couple of standard methods for proving an implication.
1.5.1
Method #1
In order to prove that P IMPLIES Q:
1. Write, ‚ÄúAssume P .‚Äù
2. Show that Q logically follows.

Chapter 1
What is a Proof?
12
Example
Theorem 1.5.1. If 0  x  2, then  x3 C 4x C 1 > 0.
Before we write a proof of this theorem, we have to do some scratchwork to
Ô¨Ågure out why it is true.
The inequality certainly holds for x D 0; then the left side is equal to 1 and
1 > 0. As x grows, the 4x term (which is positive) initially seems to have greater
magnitude than  x3 (which is negative). For example, when x D 1, we have
4x D 4, but  x3 D  1 only. In fact, it looks like  x3 doesn‚Äôt begin to dominate
until x > 2. So it seems the  x3 C4x part should be nonnegative for all x between
0 and 2, which would imply that  x3 C 4x C 1 is positive.
So far, so good. But we still have to replace all those ‚Äúseems like‚Äù phrases with
solid, logical arguments. We can get a better handle on the critical  x3 C 4x part
by factoring it, which is not too hard:
 x3 C 4x D x.2   x/.2 C x/
Aha! For x between 0 and 2, all of the terms on the right side are nonnegative. And
a product of nonnegative terms is also nonnegative. Let‚Äôs organize this blizzard of
observations into a clean proof.
Proof. Assume 0  x  2. Then x, 2 x, and 2Cx are all nonnegative. Therefore,
the product of these terms is also nonnegative. Adding 1 to this product gives a
positive number, so:
x.2   x/.2 C x/ C 1 > 0
Multiplying out on the left side proves that
 x3 C 4x C 1 > 0
as claimed.

There are a couple points here that apply to all proofs:
 You‚Äôll often need to do some scratchwork while you‚Äôre trying to Ô¨Ågure out
the logical steps of a proof. Your scratchwork can be as disorganized as you
like‚Äîfull of dead-ends, strange diagrams, obscene words, whatever. But
keep your scratchwork separate from your Ô¨Ånal proof, which should be clear
and concise.
 Proofs typically begin with the word ‚ÄúProof‚Äù and end with some sort of de-
limiter like  or ‚ÄúQED.‚Äù The only purpose for these conventions is to clarify
where proofs begin and end.

1.6. Proving an ‚ÄúIf and Only If‚Äù
13
1.5.2
Method #2 - Prove the Contrapositive
An implication (‚ÄúP IMPLIES Q‚Äù) is logically equivalent to its contrapositive
NOT.Q/ IMPLIES NOT.P / :
Proving one is as good as proving the other, and proving the contrapositive is some-
times easier than proving the original statement. If so, then you can proceed as
follows:
1. Write, ‚ÄúWe prove the contrapositive:‚Äù and then state the contrapositive.
2. Proceed as in Method #1.
Example
Theorem 1.5.2. If r is irrational, then pr is also irrational.
A number is rational when it equals a quotient of integers ‚Äîthat is, if it equals
m=n for some integers m and n. If it‚Äôs not rational, then it‚Äôs called irrational. So
we must show that if r is not a ratio of integers, then pr is also not a ratio of
integers. That‚Äôs pretty convoluted! We can eliminate both not‚Äôs and make the proof
straightforward by using the contrapositive instead.
Proof. We prove the contrapositive: if pr is rational, then r is rational.
Assume that pr is rational. Then there exist integers m and n such that:
pr D m
n
Squaring both sides gives:
r D m2
n2
Since m2 and n2 are integers, r is also rational.

1.6
Proving an ‚ÄúIf and Only If‚Äù
Many mathematical theorems assert that two statements are logically equivalent;
that is, one holds if and only if the other does. Here is an example that has been
known for several thousand years:
Two triangles have the same side lengths if and only if two side lengths
and the angle between those sides are the same.
The phrase ‚Äúif and only if‚Äù comes up so often that it is often abbreviated ‚Äúiff.‚Äù

Chapter 1
What is a Proof?
14
1.6.1
Method #1: Prove Each Statement Implies the Other
The statement ‚ÄúP IFF Q‚Äù is equivalent to the two statements ‚ÄúP IMPLIES Q‚Äù and
‚ÄúQ IMPLIES P .‚Äù So you can prove an ‚Äúiff‚Äù by proving two implications:
1. Write, ‚ÄúWe prove P implies Q and vice-versa.‚Äù
2. Write, ‚ÄúFirst, we show P implies Q.‚Äù Do this by one of the methods in
Section 1.5.
3. Write, ‚ÄúNow, we show Q implies P .‚Äù Again, do this by one of the methods
in Section 1.5.
1.6.2
Method #2: Construct a Chain of Iffs
In order to prove that P is true iff Q is true:
1. Write, ‚ÄúWe construct a chain of if-and-only-if implications.‚Äù
2. Prove P is equivalent to a second statement which is equivalent to a third
statement and so forth until you reach Q.
This method sometimes requires more ingenuity than the Ô¨Årst, but the result can be
a short, elegant proof.
Example
The standard deviation of a sequence of values x1; x2; : : : ; xn is deÔ¨Åned to be:
s
.x1   /2 C .x2   /2 C    C .xn   /2
n
(1.3)
where  is the mean of the values:
 WWD x1 C x2 C    C xn
n
Theorem 1.6.1. The standard deviation of a sequence of values x1; : : : ; xn is zero
iff all the values are equal to the mean.
For example, the standard deviation of test scores is zero if and only if everyone
scored exactly the class average.
Proof. We construct a chain of ‚Äúiff‚Äù implications, starting with the statement that
the standard deviation (1.3) is zero:
s
.x1   /2 C .x2   /2 C    C .xn   /2
n
D 0:
(1.4)

1.7. Proof by Cases
15
Now since zero is the only number whose square root is zero, equation (1.4) holds
iff
.x1   /2 C .x2   /2 C    C .xn   /2 D 0:
(1.5)
Now squares of real numbers are always nonnegative, so every term on the left
hand side of equation (1.5) is nonnegative. This means that (1.5) holds iff
Every term on the left hand side of (1.5) is zero.
(1.6)
But a term .xi   /2 is zero iff xi D , so (1.6) is true iff
Every xi equals the mean.

1.7
Proof by Cases
Breaking a complicated proof into cases and proving each case separately is a com-
mon, useful proof strategy. Here‚Äôs an amusing example.
Let‚Äôs agree that given any two people, either they have met or not. If every pair
of people in a group has met, we‚Äôll call the group a club. If every pair of people in
a group has not met, we‚Äôll call it a group of strangers.
Theorem. Every collection of 6 people includes a club of 3 people or a group of 3
strangers.
Proof. The proof is by case analysis5. Let x denote one of the six people. There
are two cases:
1. Among 5 other people besides x, at least 3 have met x.
2. Among the 5 other people, at least 3 have not met x.
Now we have to be sure that at least one of these two cases must hold,6 but that‚Äôs
easy: we‚Äôve split the 5 people into two groups, those who have shaken hands with
x and those who have not, so one of the groups must have at least half the people.
Case 1: Suppose that at least 3 people did meet x.
This case splits into two subcases:
5Describing your approach at the outset helps orient the reader.
6Part of a case analysis argument is showing that you‚Äôve covered all the cases. Often this is
obvious, because the two cases are of the form ‚ÄúP ‚Äù and ‚Äúnot P .‚Äù However, the situation above is not
stated quite so simply.

Chapter 1
What is a Proof?
16
Case 1.1: No pair among those people met each other. Then these
people are a group of at least 3 strangers. So the Theorem holds in this
subcase.
Case 1.2: Some pair among those people have met each other. Then
that pair, together with x, form a club of 3 people. So the Theorem
holds in this subcase.
This implies that the Theorem holds in Case 1.
Case 2: Suppose that at least 3 people did not meet x.
This case also splits into two subcases:
Case 2.1: Every pair among those people met each other. Then these
people are a club of at least 3 people. So the Theorem holds in this
subcase.
Case 2.2: Some pair among those people have not met each other.
Then that pair, together with x, form a group of at least 3 strangers. So
the Theorem holds in this subcase.
This implies that the Theorem also holds in Case 2, and therefore holds in all cases.

1.8
Proof by Contradiction
In a proof by contradiction or indirect proof, you show that if a proposition were
false, then some false fact would be true. Since by deÔ¨Ånition, a false fact can‚Äôt be
true, the proposition must be true.
Proof by contradiction is always a viable approach. However, as the name sug-
gests, indirect proofs can be a little convoluted, so direct proofs are generally prefer-
able when they are available.
Method: In order to prove a proposition P by contradiction:
1. Write, ‚ÄúWe use proof by contradiction.‚Äù
2. Write, ‚ÄúSuppose P is false.‚Äù
3. Deduce something known to be false (a logical contradiction).
4. Write, ‚ÄúThis is a contradiction. Therefore, P must be true.‚Äù

1.9. Good Proofs in Practice
17
Example
Remember that a number is rational if it is equal to a ratio of integers. For example,
3:5 D 7=2 and 0:1111    D 1=9 are rational numbers. On the other hand, we‚Äôll
prove by contradiction that
p
2 is irrational.
Theorem 1.8.1.
p
2 is irrational.
Proof. We use proof by contradiction. Suppose the claim is false; that is,
p
2 is
rational. Then we can write
p
2 as a fraction n=d in lowest terms.
Squaring both sides gives 2 D n2=d 2 and so 2d 2 D n2. This implies that n is a
multiple of 2. Therefore n2 must be a multiple of 4. But since 2d 2 D n2, we know
2d 2 is a multiple of 4 and so d 2 is a multiple of 2. This implies that d is a multiple
of 2.
So the numerator and denominator have 2 as a common factor, which contradicts
the fact that n=d is in lowest terms. So
p
2 must be irrational.

1.9
Good Proofs in Practice
One purpose of a proof is to establish the truth of an assertion with absolute cer-
tainty. Mechanically checkable proofs of enormous length or complexity can ac-
complish this. But humanly intelligible proofs are the only ones that help someone
understand the subject. Mathematicians generally agree that important mathemati-
cal results can‚Äôt be fully understood until their proofs are understood. That is why
proofs are an important part of the curriculum.
To be understandable and helpful, more is required of a proof than just logical
correctness: a good proof must also be clear. Correctness and clarity usually go
together; a well-written proof is more likely to be a correct proof, since mistakes
are harder to hide.
In practice, the notion of proof is a moving target. Proofs in a professional
research journal are generally unintelligible to all but a few experts who know all
the terminology and prior results used in the proof. Conversely, proofs in the Ô¨Årst
weeks of a beginning course like 6.042 would be regarded as tediously long-winded
by a professional mathematician. In fact, what we accept as a good proof later in
the term will be different from what we consider good proofs in the Ô¨Årst couple
of weeks of 6.042. But even so, we can offer some general tips on writing good
proofs:
State your game plan. A good proof begins by explaining the general line of rea-
soning, for example, ‚ÄúWe use case analysis‚Äù or ‚ÄúWe argue by contradiction.‚Äù

Chapter 1
What is a Proof?
18
Keep a linear Ô¨Çow. Sometimes proofs are written like mathematical mosaics, with
juicy tidbits of independent reasoning sprinkled throughout. This is not good.
The steps of an argument should follow one another in an intelligible order.
A proof is an essay, not a calculation. Many students initially write proofs the way
they compute integrals. The result is a long sequence of expressions without
explanation, making it very hard to follow. This is bad. A good proof usually
looks like an essay with some equations thrown in. Use complete sentences.
Avoid excessive symbolism. Your reader is probably good at understanding words,
but much less skilled at reading arcane mathematical symbols. So use words
where you reasonably can.
Revise and simplify. Your readers will be grateful.
Introduce notation thoughtfully. Sometimes an argument can be greatly simpli-
Ô¨Åed by introducing a variable, devising a special notation, or deÔ¨Åning a new
term. But do this sparingly since you‚Äôre requiring the reader to remember
all that new stuff. And remember to actually deÔ¨Åne the meanings of new
variables, terms, or notations; don‚Äôt just start using them!
Structure long proofs. Long programs are usually broken into a hierarchy of smaller
procedures. Long proofs are much the same. When your proof needed facts
that are easily stated, but not readily proved, those fact are best pulled out
as preliminary lemmas. Also, if you are repeating essentially the same argu-
ment over and over, try to capture that argument in a general lemma, which
you can cite repeatedly instead.
Be wary of the ‚Äúobvious.‚Äù When familiar or truly obvious facts are needed in a
proof, it‚Äôs OK to label them as such and to not prove them. But remember
that what‚Äôs obvious to you, may not be‚Äîand typically is not‚Äîobvious to
your reader.
Most especially, don‚Äôt use phrases like ‚Äúclearly‚Äù or ‚Äúobviously‚Äù in an attempt
to bully the reader into accepting something you‚Äôre having trouble proving.
Also, go on the alert whenever you see one of these phrases in someone else‚Äôs
proof.
Finish. At some point in a proof, you‚Äôll have established all the essential facts
you need. Resist the temptation to quit and leave the reader to draw the
‚Äúobvious‚Äù conclusion. Instead, tie everything together yourself and explain
why the original claim follows.

1.9. Good Proofs in Practice
19
Creating a good proof is a lot like creating a beautiful work of art. In fact,
mathematicians often refer to really good proofs as being ‚Äúelegant‚Äù or ‚Äúbeautiful.‚Äù
It takes a practice and experience to write proofs that merit such praises, but to
get you started in the right direction, we will provide templates for the most useful
proof techniques.
Throughout the text there are also examples of bogus proofs ‚Äîarguments that
look like proofs but aren‚Äôt. Sometimes a bogus proof can reach false conclusions
because of missteps or mistaken assumptions. More subtle bogus proofs reach
correct conclusions, but do so in improper ways, for example by circular reasoning,
by leaping to unjustiÔ¨Åed conclusions, or by saying that the hard part of ‚Äúthe proof
is left to the reader.‚Äù Learning to spot the Ô¨Çaws in improper proofs will hone your
skills at seeing how each proof step follows logically from prior steps. It will also
enable you to spot Ô¨Çaws in your own proofs.
The analogy between good proofs and good programs extends beyond structure.
The same rigorous thinking needed for proofs is essential in the design of criti-
cal computer systems. When algorithms and protocols only ‚Äúmostly work‚Äù due
to reliance on hand-waving arguments, the results can range from problematic to
catastrophic. An early example was the Therac 25, a machine that provided radia-
tion therapy to cancer victims, but occasionally killed them with massive overdoses
due to a software race condition. A more recent (August 2004) example involved a
single faulty command to a computer system used by United and American Airlines
that grounded the entire Ô¨Çeet of both companies‚Äîand all their passengers!
It is a certainty that we‚Äôll all one day be at the mercy of critical computer systems
designed by you and your classmates. So we really hope that you‚Äôll develop the
ability to formulate rock-solid logical arguments that a system actually does what
you think it does!
Problems for Section 1.1
Class Problems
Problem 1.1.
Identify exactly where the bugs are in each of the following bogus proofs.7
(a) Bogus Claim: 1=8 > 1=4:
7From Stueben, Michael and Diane Sandford. Twenty Years Before the Blackboard, Mathematical
Association of America, ¬©1998.

Chapter 1
What is a Proof?
20
Bogus proof.
3 > 2
3 log10.1=2/ > 2 log10.1=2/
log10.1=2/3 > log10.1=2/2
.1=2/3 > .1=2/2;
and the claim now follows by the rules for multiplying fractions.

(b) Bogus proof: 1¬¢ D $0:01 D .$0:1/2 D .10¬¢/2 D 100¬¢ D $1:

(c) Bogus Claim: If a and b are two equal real numbers, then a D 0.
Bogus proof.
a D b
a2 D ab
a2   b2 D ab   b2
.a   b/.a C b/ D .a   b/b
a C b D b
a D 0:

Problem 1.2.
It‚Äôs a fact that the Arithmetic Mean is at least as large the Geometric Mean, namely,
a C b
2

p
ab
for all nonnegative real numbers a and b. But there‚Äôs something objectionable
about the following proof of this fact. What‚Äôs the objection, and how would you Ô¨Åx
it?

1.9. Good Proofs in Practice
21
Bogus proof.
a C b
2
‚Äπ
p
ab;
so
a C b
‚Äπ 2
p
ab;
so
a2 C 2ab C b2 ‚Äπ 4ab;
so
a2   2ab C b2 ‚Äπ 0;
so
.a   b/2  0
which we know is true.
The last statement is true because a   b is a real number, and the square of a real
number is never negative. This proves the claim.

Problem 1.3.
Albert announces to his class that he plans to surprise them with a quiz sometime
next week.
His students Ô¨Årst wonder if the quiz could be on Friday of next week. They
reason that it can‚Äôt: if Albert didn‚Äôt give the quiz before Friday, then by midnight
Thursday, they would know the quiz had to be on Friday, and so the quiz wouldn‚Äôt
be a surprise any more.
Next the students wonder whether Albert could give the surprise quiz Thursday.
They observe that if the quiz wasn‚Äôt given before Thursday, it would have to be
given on the Thursday, since they already know it can‚Äôt be given on Friday. But
having Ô¨Ågured that out, it wouldn‚Äôt be a surprise if the quiz was on Thursday either.
Similarly, the students reason that the quiz can‚Äôt be on Wednesday, Tuesday, or
Monday. Namely, it‚Äôs impossible for Albert to give a surprise quiz next week. All
the students now relax, having concluded that Albert must have been blufÔ¨Ång.
And since no one expects the quiz, that‚Äôs why, when Albert gives it on Tuesday
next week, it really is a surprise!
What do you think is wrong with the students‚Äô reasoning?
Problems for Section 1.5
Homework Problems
Problem 1.4.
Show that log7 n is either an integer or irrational, where n is a positive integer. Use
whatever familiar facts about integers and primes you need, but explicitly state such
facts.

Chapter 1
What is a Proof?
22
Problems for Section 1.7
Class Problems
Problem 1.5.
If we raise an irrational number to an irrational power, can the result be rational?
Show that it can by considering
p
2
p
2 and arguing by cases.
Homework Problems
Problem 1.6.
For n D 40, the value of polynomial p.n/ WWD n2 C n C 41 is not prime, as noted
in Section 1.1. But we could have predicted based on general principles that no
nonconstant polynomial can generate only prime numbers.
In particular, let q.n/ be a polynomial with integer coefÔ¨Åcients, and let c WWDq.0/
be the constant term of q.
(a) Verify that q.cm/ is a multiple of c for all m 2 Z.
(b) Show that if q is nonconstant and c > 1, then as n ranges over the nonnegative
integers, N, there are inÔ¨Ånitely many q.n/ 2 Z that are not primes.
Hint: You may assume the familiar fact that the magnitude of any nonconstant
polynomial, q.n/, grows unboundedly as n grows.
(c) Conclude immediately that for every nonconstant polynomial, q, there must
be an n 2 N such that q.n/ is not prime.
Problems for Section 1.8
Class Problems
Problem 1.7.
Prove that if ab D n, then a or b must be  pn, where a; b, and n are nonnegative
integers. Hint: by contradiction, Section 1.8.
Problem 1.8.
Generalize the proof of Theorem 1.8.1 that
p
2 is irrational. For example, how
about
3p
2?
Problem 1.9.
Prove that log4 6 is irrational.

1.9. Good Proofs in Practice
23
Problem 1.10.
Here is a different proof that
p
2 is irrational, taken from the American Mathemat-
ical Monthly, v.116, #1, Jan. 2009, p.69:
Proof. Suppose for the sake of contradiction that
p
2 is rational, and choose the
least integer, q > 0, such that
p
2   1

q is a nonnegative integer. Let q0 WWD
p
2   1

q. Clearly 0 < q0 < q. But an easy computation shows that
p
2   1

q0
is a nonnegative integer, contradicting the minimality of q.

(a) This proof was written for an audience of college teachers, and at this point it
is a little more concise than desirable. Write out a more complete version which
includes an explanation of each step.
(b) Now that you have justiÔ¨Åed the steps in this proof, do you have a preference
for one of these proofs over the other? Why? Discuss these questions with your
teammates for a few minutes and summarize your team‚Äôs answers on your white-
board.
Problem 1.11.
Here is a generalization of Problem 1.8 that you may not have thought of:
Lemma. Let the coefÔ¨Åcients of the polynomial
a0 C a1x C a2x2 C    C am 1xm 1 C xm
be integers. Then any real root of the polynomial is either integral or irrational.
(a) Explain why the Lemma immediately implies that
mp
k is irrational whenever
k is not an mth power of some integer.
(b) Carefully prove the Lemma.
You may Ô¨Ånd it helpful to appeal to:
Fact. If a prime, p, is a factor of some power of an integer, then it is a factor of
that integer.
You may assume this Fact without writing down its proof, but see if you can explain
why it is true.

Chapter 1
What is a Proof?
24
Homework Problems
Problem 1.12.
The fact that that there are irrational numbers a; b such that ab is rational was
proved in Problem 1.5. Unfortunately, that proof was nonconstructive: it didn‚Äôt
reveal a speciÔ¨Åc pair, a; b, with this property. But in fact, it‚Äôs easy to do this: let
a WWD
p
2 and b WWD 2 log2 3.
We know
p
2 is irrational, and obviously ab D 3. Finish the proof that this a; b
pair works, by showing that 2 log2 3 is irrational.
Exam Problems
Problem 1.13.
Prove that log9 12 is irrational.

2
The Well Ordering Principle
Every nonempty set of nonnegative integers has a smallest element.
This statement is known as The Well Ordering Principle. Do you believe it?
Seems sort of obvious, right? But notice how tight it is: it requires a nonempty
set ‚Äîit‚Äôs false for the empty set which has no smallest element because it has no
elements at all! And it requires a set of nonnegative integers ‚Äîit‚Äôs false for the
set of negative integers and also false for some sets of nonnegative rationals ‚Äîfor
example, the set of positive rationals. So, the Well Ordering Principle captures
something special about the nonnegative integers.
2.1
Well Ordering Proofs
While the Well Ordering Principle may seem obvious, it‚Äôs hard to see offhand why
it is useful. But in fact, it provides one of the most important proof rules in discrete
mathematics.
In fact, looking back, we took the Well Ordering Principle for granted in proving
that
p
2 is irrational. That proof assumed that for any positive integers m and n,
the fraction m=n can be written in lowest terms, that is, in the form m0=n0 where
m0 and n0 are positive integers with no common prime factors. How do we know
this is always possible?
Suppose to the contrary that there are positive integers m and n such that the
fraction m=n cannot be written in lowest terms. Now let C be the set of positive
integers that are numerators of such fractions. Then m 2 C, so C is nonempty.
Therefore, by Well Ordering, there must be a smallest integer, m0 2 C. So by
deÔ¨Ånition of C, there is an integer n0 > 0 such that
the fraction m0
n0
cannot be written in lowest terms.
This means that m0 and n0 must have a common prime factor, p > 1. But
m0=p
n0=p D m0
n0
;
so any way of expressing the left hand fraction in lowest terms would also work for

Chapter 2
The Well Ordering Principle
26
m0=n0, which implies
the fraction m0=p
n0=p cannot be in written in lowest terms either.
So by deÔ¨Ånition of C, the numerator, m0=p, is in C. But m0=p < m0, which
contradicts the fact that m0 is the smallest element of C.
Since the assumption that C is nonempty leads to a contradiction, it follows that
C must be empty. That is, that there are no numerators of fractions that can‚Äôt be
written in lowest terms, and hence there are no such fractions at all.
We‚Äôve been using the Well Ordering Principle on the sly from early on!
2.2
Template for Well Ordering Proofs
More generally, there is a standard way to use Well Ordering to prove that some
property, P.n/ holds for every nonnegative integer, n. Here is a standard way to
organize such a well ordering proof:
To prove that ‚ÄúP.n/ is true for all n 2 N‚Äù using the Well Ordering Principle:
 DeÔ¨Åne the set, C, of counterexamples to P being true. Namely, deÔ¨Åne
C WWD fn 2 N j P.n/ is falseg:
(The notation fn j P.n/g means ‚Äúthe set of all elements n, for which P.n/
is true,‚Äù see Section 4.1.5.)
 Assume for proof by contradiction that C is nonempty.
 By the Well Ordering Principle, there will be a smallest element, n, in C.
 Reach a contradiction (somehow) ‚Äîoften by showing how to use n to Ô¨Ånd
another member of C that is smaller than n. (This is the open-ended part
of the proof task.)
 Conclude that C must be empty, that is, no counterexamples exist.

2.2.1
Summing the Integers
Let‚Äôs use this template to prove

2.2. Template for Well Ordering Proofs
27
Theorem 2.2.1.
1 C 2 C 3 C    C n D n.n C 1/=2
(2.1)
for all nonnegative integers, n.
First, we‚Äôd better address a couple of ambiguous special cases before they trip us
up:
 If n D 1, then there is only one term in the summation, and so 1 C 2 C 3 C
   C n is just the term 1. Don‚Äôt be misled by the appearance of 2 and 3 and
the suggestion that 1 and n are distinct terms!
 If n  0, then there are no terms at all in the summation. By convention, the
sum in this case is 0.
So, while the three dots notation, which is called an ellipsis, is convenient, you
have to watch out for these special cases where the notation is misleading! In
fact, whenever you see an ellipsis, you should be on the lookout to be sure you
understand the pattern, watching out for the beginning and the end.
We could have eliminated the need for guessing by rewriting the left side of (2.1)
with summation notation:
n
X
iD1
i
or
X
1in
i:
Both of these expressions denote the sum of all values taken by the expression to
the right of the sigma as the variable, i, ranges from 1 to n. Both expressions make
it clear what (2.1) means when n D 1. The second expression makes it clear that
when n D 0, there are no terms in the sum, though you still have to know the
convention that a sum of no numbers equals 0 (the product of no numbers is 1, by
the way).
OK, back to the proof:
Proof. By contradiction. Assume that Theorem 2.2.1 is false. Then, some nonneg-
ative integers serve as counterexamples to it. Let‚Äôs collect them in a set:
C WWD fn 2 N j 1 C 2 C 3 C    C n ¬§ n.n C 1/
2
g:
Assuming there are counterexamples, C is a nonempty set of nonnegative integers.
So, by the Well Ordering Principle, C has a minimum element, call it c. That is,
among the nonnegative integers, c is the smallest counterexample to equation (2.1).
Since c is the smallest counterexample, we know that (2.1) is false for n D c but
true for all nonnegative integers n < c. But (2.1) is true for n D 0, so c > 0. This

Chapter 2
The Well Ordering Principle
28
means c   1 is a nonnegative integer, and since it is less than c, equation (2.1) is
true for c   1. That is,
1 C 2 C 3 C    C .c   1/ D .c   1/c
2
:
But then, adding c to both sides we get
1 C 2 C 3 C    C .c   1/ C c D .c   1/c
2
C c D c2   c C 2c
2
D c.c C 1/
2
;
which means that (2.1) does hold for c, after all! This is a contradiction, and we
are done.

2.3
Factoring into Primes
We‚Äôve previously taken for granted the Prime Factorization Theorem that every
integer greater than one has a unique1 expression as a product of prime numbers.
This is another of those familiar mathematical facts which are not really obvious.
We‚Äôll prove the uniqueness of prime factorization in a later chapter, but well order-
ing gives an easy proof that every integer greater than one can be expressed as some
product of primes.
Theorem 2.3.1. Every positive integer greater than one can be factored as a prod-
uct of primes.
Proof. The proof is by Well Ordering.
Let C be the set of all integers greater than one that cannot be factored as a
product of primes. We assume C is not empty and derive a contradiction.
If C is not empty, there is a least element, n 2 C, by Well Ordering. The n can‚Äôt
be prime, because a prime by itself is considered a (length one) product of primes
and no such products are in C.
So n must be a product of two integers a and b where 1 < a; b < n. Since a
and b are smaller than the smallest element in C, we know that a; b ‚Ä¶ C. In other
words, a can be written as a product of primes p1p2    pk and b as a product of
primes q1    ql. Therefore, n D p1    pkq1    ql can be written as a product of
primes, contradicting the claim that n 2 C. Our assumption that C is not empty
must therefore be false.

1. ..unique up to the order in which the prime factors appear

2.4. Well Ordered Sets
29
2.4
Well Ordered Sets
A set of numbers is well ordered when each of its nonempty subsets has a minimum
element. The Well Ordering principle says, of course, that the set of nonnegative
integers is well ordered, but so are lots of other sets, for example, the set rN of
numbers of the form rn, where r is a positive real number and n 2 N.
Well ordering commonly comes up in Computer Science as a method for proving
that computations won‚Äôt run forever. The idea is to assign a value to the successive
steps of a computation so that the values get smaller at every step. If the values are
all from a well ordered set, then the computation can‚Äôt run forever, because if it did,
the values assigned to its successive steps would deÔ¨Åne a subset with no minimum
element. You‚Äôll see several examples of this technique applied in Section 5.4 to
prove that various state machines will eventually terminate.
Notice that a set may have a minimum element but not be well ordered. The set
of nonnegative rational numbers is an example: it has a minimum element, namely
zero, but it also has nonempty subsets that don‚Äôt have minimum elements ‚Äîthe
positive rationals, for example.
The following theorem is a tiny generalization of the Well Ordering Principle.
Theorem 2.4.1. For any nonnegative integer, n, the set of integers greater than or
equal to  n is well ordered.
This theorem is just as obvious as the Well Ordering Principle, and it would
be harmless to accept it as another axiom. But repeatedly introducing axioms gets
worrisome after a while, and it‚Äôs worth noticing when a potential axiom can actually
be proved. We can easily prove Theorem 2.4.1 using the Well Ordering Principle:
Proof. Let S be any nonempty set of integers   n. Now add n to each of the
elements in S; let‚Äôs call this new set S C n. Now S C n is a nonempty set of
nonnegative integers, and so by the Well Ordering Principle, it has a minimum
element, m. But then it‚Äôs easy to see that m   n is the minimum element of S.

The deÔ¨Ånition of well ordering implies that every subset of a well ordered set
is well ordered, and this yields two convenient, immediate corollaries of Theo-
rem 2.4.1:
DeÔ¨Ånition 2.4.2. A lower bound (respectively, upper bound) for a set, S, of real
numbers is a number, b, such that b  s (respectively, b  s) for every s 2 S.
Note that a lower or upper bound of set S is not required to be in the set.

Chapter 2
The Well Ordering Principle
30
Corollary 2.4.3. Any set of integers with a lower bound is well ordered.
Proof. A set of integers with a lower bound b 2 R will also have the integer n D
bbc as a lower bound, where bbc, called the Ô¨Çoor of b, is gotten by rounding down
b to the nearest integer. So Theorem 2.4.1 implies the set is well ordered.

Corollary 2.4.4. Any nonempty set of integers with an upper bound has a maximum
element.
Proof. Suppose a set, S, of integers has an upper bound b 2 R. Now multiply each
element of S by -1; let‚Äôs call this new set of elements  S. Now, of course,  b is a
lower bound of  S. So  S has a minimum element  m by Corollary 2.4.3. But
then it‚Äôs easy to see that m is the maximum element of S.

2.4.1
A Different Well Ordered Set
[Optional] Another example of a well ordered set of numbers is the set To1 of fractions increasing to
the limit 1:
0
1; 1
2; 2
3; 3
4; : : : ;
n
n C 1; : : : :
The minimum element of any nonempty subset of To1 is simply the one with the minimum numerator
when expressed in the form n=.n C 1/.
Now we can deÔ¨Åne a very different well ordered set by adding nonnegative integers to numbers in
To1. That is, we take all the numbers of the form n C f where n is a nonnegative integer and f is a
fraction in To1. Let‚Äôs call this set of numbers ‚Äîyou guessed it ‚ÄîN C To1. There is a simple recipe
for Ô¨Ånding the minimum number in any nonempty subset of N C To1, which explains why this set is
well ordered:
Lemma 2.4.5. N C To1 is well ordered.
Proof. Given any nonempty subset, S, of N C To1, look at all the nonnegative integers, n, such that
n C f is in S for some f 2 To1. This is a nonempty set nonnegative integers, so by the WOP, there
is a minimum one; call it ns.
By deÔ¨Ånition of ns, there is some f 2 To1 such that nS Cf is in the set S. So the set all fractions
f such that nS C f 2 S is a nonempty subset To1, and since To1 is well ordered, this nonempty set
contains a minimum element; call it fS. Now, using the fact that every fraction f 2 twdone is a
nonnegative number less than 1, it easy to verify that nS C fS is the minimum element of S.

The set N C To1 is different from the earlier examples. In all the earlier examples, each element
was greater than only a Ô¨Ånite number of other elements. In N C To1, every element greater than
or equal to 1 can be the Ô¨Årst element in strictly decreasing sequences of elements of arbitrary Ô¨Ånite
length. For example, the following decreasing sequences of elements in N C To1 all start with 1:
1; 0:
1; 1
2; 0:
1; 2
3; 1
2; 0:
1; 3
4; 2
3; 1
2; 0:
:::
Nevertheless, since N C To1 is well ordered, it is impossible to Ô¨Ånd an inÔ¨Ånite decreasing sequence
of elements in N C To1, because the set of elements in such a sequence would have no minimum.

2.4. Well Ordered Sets
31
Problems for Section 2.2
Practice Problems
Problem 2.1.
For practice using the Well Ordering Principle, Ô¨Åll in the template of an easy to
prove fact: every amount of postage that can be assembled using only 10 cent and
15 cent stamps is divisible by 5.
In particular, let the notation ‚Äúj j k‚Äù indicate that integer j is a divisor of integer
k, and let S.n/ mean that exactly n cents postage can be assembled using only 10
and 15 cent stamps. Then the proof shows that
S.n/ IMPLIES 5 j n;
for all nonnegative integers n:
(2.2)
Fill in the missing portions (indicated by ‚Äú...‚Äù) of the following proof of (2.2).
Let C be the set of counterexamples to (2.2), namely
C WWD fn j : : :g
Assume for the purpose of obtaining a contradiction that C is nonempty.
Then by the WOP, there is a smallest number, m 2 C. This m must be
positive because ....
But if S.m/ holds and m is positive, then S.m   10/ or S.m   15/
must hold, because ....
So suppose S.m   10/ holds. Then 5 j .m   10/, because...
But if 5 j .m   10/, then obviously 5 j m, contradicting the fact that m
is a counterexample.
Next, if S.m 15/ holds, we arrive at a contradiction in the same way.
Since we get a contradiction in both cases, we conclude that...
which proves that (2.2) holds.
Problem 2.2.
The Fibonacci numbers F.0/; F.1/; F.2/; : : : are deÔ¨Åned as follows:
F.0/ WWD 0;
F.1/ WWD 1;
F.n/ WWD F.n   1/ C F.n   2/
for n  2:
(2.3)
Exactly which sentence(s) in the following bogus proof contain logical errors?
Explain.

Chapter 2
The Well Ordering Principle
32
False Claim. Every Fibonacci number is even.
Bogus proof. Let all the variables n; m; k mentioned below be nonnegative integer
valued.
1. The proof is by the WOP.
2. Let Even.n/ mean that F.n/ is even.
3. Let C be the set of counterexamples to the assertion that Even.n/ holds for
all n 2 N, namely,
C WWD fn 2 N j NOT.Even.n//g:
4. We prove by contradiction that C is empty. So assume that C is not empty.
5. By WOP, there is a least nonnegative integer, m 2 C,
6. Then m > 0, since F.0/ D 0 is an even number.
7. Since m is the minimum counterexample, F.k/ is even for all k < m.
8. In particular, F.m   1/ and F.m   2/ are both even.
9. But by the deÔ¨Åning equation (2.3), F.m/ equals the sum F.m 1/CF.m 2/
of two even numbers, and so it is also even.
10. That is, Even.m/ is true.
11. This contradicts the condition in the deÔ¨Ånition of m that NOT.Even.m//
holds.
12. This contradition implies that C must be empty. Hence, F.n/ is even for all
n 2 N.

Problem 2.3.
In Chapter 2, the Well Ordering Principle was used to show that all positive rational
numbers can be written in ‚Äúlowest terms,‚Äù that is, as a ratio of positive integers with
no common factor prime factor. Below is a different proof which also arrives at this
correct conclusion, but this proof is bogus. Identify every step at which the proof
makes an unjustiÔ¨Åed inference.

2.4. Well Ordered Sets
33
Bogus proof. Suppose to the contrary that there was positive rational, q, such that
q cannot be written in lowest terms. Now let C be the set of such rational numbers
that cannot be written in lowest terms. Then q 2 C, so C is nonempty. So there
must be a smallest rational, q0 2 C. So since q0=2 < q0, it must be possible to
express q0=2 in lowest terms, namely,
q0
2 D m
n
(2.4)
for positive integers m; n with no common prime factor. Now we consider two
cases:
Case 1: [n is odd]. Then 2m and n also have no common prime factor, and
therefore
q0 D 2 
m
n

D 2m
n
expresses q0 in lowest terms, a contradiction.
Case 2: [n is even]. Any common prime factor of m and n=2 would also be a
common prime factor of m and n. Therefore m and n=2 have no common prime
factor, and so
q0 D m
n=2
expresses q0 in lowest terms, a contradiction.
Since the assumption that C is nonempty leads to a contradiction, it follows that
C is empty‚Äîthat is, there are no counterexamples.

Class Problems
Problem 2.4.
Use the Well Ordering Principle to prove that
n
X
kD0
k2 D n.n C 1/.2n C 1/
6
:
(2.5)
for all nonnegative integers, n.
Problem 2.5.
Use the Well Ordering Principle to prove that there is no solution over the positive
integers to the equation:
4a3 C 2b3 D c3:

Chapter 2
The Well Ordering Principle
34
Homework Problems
Problem 2.6.
Use the Well Ordering Principle to prove that any integer greater than or equal to 8
can be represented as the sum of integer multiples of 3 and 5.
Problem 2.7.
Euler‚Äôs Conjecture in 1769 was that there are no positive integer solutions to the
equation
a4 C b4 C c4 D d 4:
Integer values for a; b; c; d that do satisfy this equation were Ô¨Årst discovered in
1986. So Euler guessed wrong, but it took more two hundred years to prove it.
Now let‚Äôs consider Lehman‚Äôs equation, similar to Euler‚Äôs but with some coefÔ¨Å-
cients:
8a4 C 4b4 C 2c4 D d 4
(2.6)
Prove that Lehman‚Äôs equation (2.6) really does not have any positive integer
solutions.
Hint: Consider the minimum value of a among all possible solutions to (2.6).
Exam Problems
Problem 2.8.
Except for an easily repaired omission, the following proof using the Well Ordering
Principle shows that every amount of postage that can be paid exactly using only
10 cent and 15 cent stamps, is divisible by 5.
Namely, let the notation ‚Äúj j k‚Äù indicate that integer j is a divisor of integer k,
and let S.n/ mean that exactly n cents postage can be assembled using only 10 and
15 cent stamps. Then the proof shows that
S.n/ IMPLIES 5 j n;
for all nonnegative integers n:
(2.7)
Fill in the missing portions (indicated by ‚Äú...‚Äù) of the following proof of (2.7), and
at the end, identify the minor mistake in the proof and how to Ô¨Åx it.
Let C be the set of counterexamples to (2.7), namely
C WWD fn j S.n/ and NOT.5 j n/g
Assume for the purpose of obtaining a contradiction that C is nonempty.
Then by the WOP, there is a smallest number, m 2 C. Then S.m 10/

2.4. Well Ordered Sets
35
or S.m   15/ must hold, because the m cents postage is made from 10
and 15 cent stamps, so we remove one.
So suppose S.m   10/ holds. Then 5 j .m   10/, because...
But if 5 j .m   10/, then 5 j m, because...
contradicting the fact that m is a counterexample.
Next suppose S.m   15/ holds. Then the proof for m   10 carries
over directly for m   15 to yield a contradiction in this case as well.
Since we get a contradiction in both cases, we conclude that C must
be empty. That is, there are no counterexamples to (2.7), which proves
that (2.7) holds.
The proof makes an implicit assumption about the value of m. State the assump-
tion and justify it in one sentence.
Problem 2.9.
We‚Äôll prove that for every positive integer, n, the sum of the Ô¨Årst n odd numbers is
n2, that is,
n
X
iD1
.2.i   1/ C 1/ D n2;
(2.8)
for all n 2 N.
Assume to the contrary that equation (2.8) failed for some positive integer, n.
Let m be the least such number.
(a) Why must there be such an m?
(b) Explain why m  2.
(c) Explain why part (b) implies that
m 1
X
iD1
.2.i   1/ C 1/ D .m   1/2:
(2.9)
(d) What term should be added to the left hand side of (2.9) so the result equals
m
X
iD1
.2.i   1/ C 1/‚Äπ
(e) Conclude that equation (2.8) holds for all positive integers, n.

Chapter 2
The Well Ordering Principle
36
Problems for Section 2.4
Practice Problems
Problem 2.10.
Indicate which of the following sets of numbers have a minimum element and
which are well ordered. For those that are not well ordered, give an example of
a subset with no minimum element.
(a) The integers   p
2.
(b) The rational numbers 
p
2.
(c) The set of rationals of the form 1=n where n is a positive integer.
(d) The set G of rationals of the form m=n where m; n > 0 and n  g where g is
a google, namely, 10100.
(e) The set To1 of fractions increasing to the limit 1:
0
1; 1
2; 2
3; 3
4; : : : ;
n
n C 1; : : : :
(f) The set W consisting of the nonnegative integers along with all the fractions
in To1. Do you notice anything different about W compared to the earlier well
ordered examples?
Problem 2.11.
Use the Well Ordering Principle to prove that every Ô¨Ånite, nonempty set of real
numbers has a minimum element.
Class Problems
Problem 2.12.
Prove that a set, R, of real numbers is well ordered iff there is no inÔ¨Ånite sequence
r0 > r1 > r2 > : : :
(2.10)
of elements ri 2 R.

3
Logical Formulas
It is amazing that people manage to cope with all the ambiguities in the English
language. Here are some sentences that illustrate the issue:
 ‚ÄúYou may have cake, or you may have ice cream.‚Äù
 ‚ÄúIf pigs can Ô¨Çy, then you can understand the Chebyshev bound.‚Äù
 ‚ÄúIf you can solve any problem we come up with, then you get an A for the
course.‚Äù
 ‚ÄúEvery American has a dream.‚Äù
What precisely do these sentences mean? Can you have both cake and ice cream or
must you choose just one dessert? Pigs can‚Äôt Ô¨Çy, so does the second sentence say
anything about your understanding the Chebyshev bound? If you can solve some
problems we come up with, can you get an A for the course? And if you can‚Äôt
solve a single one of the problems, does it mean you can‚Äôt get an A? Finally, does
the last sentence imply that all Americans have the same dream ‚Äîsay of owning a
house ‚Äîor might different Americans have different dreams ‚Äîsay, Eric dreams of
designing a killer software application, Tom of being a tennis champion, Albert of
being able to sing?
Some uncertainty is tolerable in normal conversation. But when we need to
formulate ideas precisely‚Äîas in mathematics and programming‚Äîthe ambiguities
inherent in everyday language can be a real problem. We can‚Äôt hope to make an
exact argument if we‚Äôre not sure exactly what the statements mean. So before we
start into mathematics, we need to investigate the problem of how to talk about
mathematics.
To get around the ambiguity of English, mathematicians have devised a spe-
cial language for talking about logical relationships. This language mostly uses
ordinary English words and phrases such as ‚Äúor,‚Äù ‚Äúimplies,‚Äù and ‚Äúfor all.‚Äù But
mathematicians give these words precise and unambiguous deÔ¨Ånitions.
Surprisingly, in the midst of learning the language of logic, we‚Äôll come across
the most important open problem in computer science‚Äîa problem whose solution
could change the world.

Chapter 3
Logical Formulas
38
3.1
Propositions from Propositions
In English, we can modify, combine, and relate propositions with words such as
‚Äúnot,‚Äù ‚Äúand,‚Äù ‚Äúor,‚Äù ‚Äúimplies,‚Äù and ‚Äúif-then.‚Äù For example, we can combine three
propositions into one like this:
If all humans are mortal and all Greeks are human, then all Greeks are mortal.
For the next while, we won‚Äôt be much concerned with the internals of propositions‚Äî
whether they involve mathematics or Greek mortality‚Äîbut rather with how propo-
sitions are combined and related. So we‚Äôll frequently use variables such as P and
Q in place of speciÔ¨Åc propositions such as ‚ÄúAll humans are mortal‚Äù and ‚Äú2 C 3 D
5.‚Äù The understanding is that these propositional variables, like propositions, can
take on only the values T (true) and F (false). Propositional variables are also
called Boolean variables after their inventor, the nineteenth century mathematician
George‚Äîyou guessed it‚ÄîBoole.
3.1.1
NOT, AND, and OR
Mathematicians use the words NOT, AND, and OR for operations that change or
combine propositions. The precise mathematical meaning of these special words
can be speciÔ¨Åed by truth tables. For example, if P is a proposition, then so is
‚ÄúNOT.P /,‚Äù and the truth value of the proposition ‚ÄúNOT.P /‚Äù is determined by the
truth value of P according to the following truth table:
P
NOT.P /
T
F
F
T
The Ô¨Årst row of the table indicates that when proposition P is true, the proposition
‚ÄúNOT.P /‚Äù is false. The second line indicates that when P is false, ‚ÄúNOT.P /‚Äù is
true. This is probably what you would expect.
In general, a truth table indicates the true/false value of a proposition for each
possible set of truth values for the variables. For example, the truth table for the
proposition ‚ÄúP AND Q‚Äù has four lines, since there are four settings of truth values
for the two variables:
P
Q
P AND Q
T
T
T
T
F
F
F
T
F
F
F
F

3.1. Propositions from Propositions
39
According to this table, the proposition ‚ÄúP AND Q‚Äù is true only when P and Q are
both true. This is probably the way you ordinarily think about the word ‚Äúand.‚Äù
There is a subtlety in the truth table for ‚ÄúP OR Q‚Äù:
P
Q
P OR Q
T
T
T
T
F
T
F
T
T
F
F
F
The Ô¨Årst row of this table says that ‚ÄúP OR Q‚Äù is true even if both P and Q are true.
This isn‚Äôt always the intended meaning of ‚Äúor‚Äù in everyday speech, but this is the
standard deÔ¨Ånition in mathematical writing. So if a mathematician says, ‚ÄúYou may
have cake, or you may have ice cream,‚Äù he means that you could have both.
If you want to exclude the possibility of both having and eating, you should
combine them with the exclusive-or operation, XOR:
P
Q
P XOR Q
T
T
F
T
F
T
F
T
T
F
F
F
3.1.2
IMPLIES
The combining operation with the least intuitive technical meaning is ‚Äúimplies.‚Äù
Here is its truth table, with the lines labeled so we can refer to them later.
P
Q
P IMPLIES Q
T
T
T
(tt)
T
F
F
(tf)
F
T
T
(ft)
F
F
T
(ff)
The truth table for implications can be summarized in words as follows:
An implication is true exactly when the if-part is false or the then-part is true.
This sentence is worth remembering; a large fraction of all mathematical statements
are of the if-then form!
Let‚Äôs experiment with this deÔ¨Ånition. For example, is the following proposition
true or false?

Chapter 3
Logical Formulas
40
‚ÄúIf Goldbach‚Äôs Conjecture is true, then x2  0 for every real number x.‚Äù
Now, we already mentioned that no one knows whether Goldbach‚Äôs Conjecture,
Proposition 1.1.8, is true or false. But that doesn‚Äôt prevent you from answering the
question! This proposition has the form P IMPLIES Q where the hypothesis, P ,
is ‚ÄúGoldbach‚Äôs Conjecture is true‚Äù and the conclusion, Q, is ‚Äúx2  0 for every
real number x.‚Äù Since the conclusion is deÔ¨Ånitely true, we‚Äôre on either line (tt) or
line (ft) of the truth table. Either way, the proposition as a whole is true!
One of our original examples demonstrates an even stranger side of implications.
‚ÄúIf pigs Ô¨Çy, then you can understand the Chebyshev bound.‚Äù
Don‚Äôt take this as an insult; we just need to Ô¨Ågure out whether this proposition is
true or false. Curiously, the answer has nothing to do with whether or not you can
understand the Chebyshev bound. Pigs do not Ô¨Çy, so we‚Äôre on either line (ft) or line
(ff) of the truth table. In both cases, the proposition is true!
In contrast, here‚Äôs an example of a false implication:
‚ÄúIf the moon shines white, then the moon is made of white cheddar.‚Äù
Yes, the moon shines white. But, no, the moon is not made of white cheddar cheese.
So we‚Äôre on line (tf) of the truth table, and the proposition is false.
False Hypotheses
It often bothers people when they Ô¨Årst learn that implications which have false
hypotheses are considered to be true. But implications with false hypotheses hardly
ever come up in ordinary settings, so there‚Äôs not much reason to be bothered by
whatever truth assignment logicians and mathematicians choose to give them.
There are, of course, good reasons for the mathematical convention that implica-
tions are true when their hypotheses are false. An illustrative example is a system
speciÔ¨Åcation (see Problem 3.10) which consisted of a series of, say, a dozen rules,
if Ci: the system sensors are in condition i, then Ai: the system takes
action i,
or more concisely,
Ci IMPLIES Ai
for 1  i  12. Then the fact that the system obeys the speciÔ¨Åcation would be
expressed by saying that the AND
≈íC1 IMPLIES A1¬ç AND ≈íC2 IMPLIES A2¬ç AND    AND ≈íC12 IMPLIES A12¬ç (3.1)
of these rules was always true.

3.2. Propositional Logic in Computer Programs
41
For example, suppose only conditions C2 and C5 are true, and the system indeed
takes the speciÔ¨Åed actions A2 and A5. This means that in this case the system is
behaving according to speciÔ¨Åcation, and accordingly we want the formula (3.1) to
come out true. Now the implications C2 IMPLIES A2 and C5 IMPLIES A5 are
both true because both their hypotheses and their conclusions are true. But in order
for (3.1) to be true, we need all the other implications with the false hypotheses Ci
for i ¬§ 2; 5 to be true. This is exactly what the rule for implications with false
hypotheses accomplishes.
3.1.3
If and Only If
Mathematicians commonly join propositions in one additional way that doesn‚Äôt
arise in ordinary speech. The proposition ‚ÄúP if and only if Q‚Äù asserts that P and
Q have the same truth value, that is, either both are true or both are false.
P
Q
P IFF Q
T
T
T
T
F
F
F
T
F
F
F
T
For example, the following if-and-only-if statement is true for every real number
x:
x2   4  0 IFF jxj  2:
For some values of x, both inequalities are true. For other values of x, neither
inequality is true. In every case, however, the IFF proposition as a whole is true.
3.2
Propositional Logic in Computer Programs
Propositions and logical connectives arise all the time in computer programs. For
example, consider the following snippet, which could be either C, C++, or Java:
if ( x > 0 || (x <= 0 && y > 100) )
:::
(further instructions)
Java uses the symbol || for ‚ÄúOR,‚Äù and the symbol && for ‚ÄúAND.‚Äù The further
instructions are carried out only if the proposition following the word if is true.
On closer inspection, this big expression is built from two simpler propositions.

Chapter 3
Logical Formulas
42
Let A be the proposition that x > 0, and let B be the proposition that y > 100.
Then we can rewrite the condition as
A OR .NOT.A/ AND B/:
(3.2)
3.2.1
Truth Table Calculation
A truth table calculation reveals that the more complicated expression 3.2 always
has the same truth value as
A OR B:
(3.3)
Namely, we begin with a table with just the truth values of A and B:
A
B
A
OR
.NOT.A/
AND
B/
A OR B
T
T
T
F
F
T
F
F
These values are enough to Ô¨Åll in two more columns:
A
B
A
OR
.NOT.A/
AND
B/
A OR B
T
T
F
T
T
F
F
T
F
T
T
T
F
F
T
F
Now we have the values needed to Ô¨Åll in the AND column:
A
B
A
OR
.NOT.A/
AND
B/
A OR B
T
T
F
F
T
T
F
F
F
T
F
T
T
T
T
F
F
T
F
F
and this provides the values needed to Ô¨Åll in the remaining column for the Ô¨Årst OR:
A
B
A
OR
.NOT.A/
AND
B/
A OR B
T
T
T
F
F
T
T
F
T
F
F
T
F
T
T
T
T
T
F
F
F
T
F
F
Expressions whose truth values always match are called equivalent. Since the two
emphasized columns of truth values of the two expressions are the same, they are

3.2. Propositional Logic in Computer Programs
43
equivalent. So we can simplify the code snippet without changing the program‚Äôs
behavior by replacing the complicated expression with an equivalent simpler one:
if ( x > 0 || y > 100 )
:::
(further instructions)
The equivalence of (3.2) and (3.3) can also be conÔ¨Årmed reasoning by cases:
A is T. An expression of the form .T OR anything/ is equivalent to T. Since A is T
both (3.2) and (3.3) in this case are of this form, so they have the same truth
value, namely, T.
A is F. An expression of the form .F OR anything/ will have same truth value as
anything. Since A is F, (3.3) has the same truth value as B.
An expression of the form .T AND anything/ is equivalent to anything, as is
any expression of the form F ORanything. So in this case A OR.NOT.A/ AND
B/ is equivalent to .NOT.A/ AND B/, which in turn is equivalent to B.
Therefore both (3.2) and (3.3) will have the same truth value in this case,
namely, the value of B.
Simplifying logical expressions has real practical importance in computer sci-
ence. Expression simpliÔ¨Åcation in programs like the one above can make a program
easier to read and understand, and can also make it faster since fewer operations
are needed. In hardware, simplifying expressions can decrease the number of logic
gates on a chip. That‚Äôs because digital circuits can be described by logical formu-
las (see Problems 3.5 and 3.6), and minimizing the logical formulas corresponds
to reducing the number of gates in the circuit. The payoff of gate minimization is
potentially enormous: a chip with fewer gates is smaller, consumes less power, has
a lower defect rate, and is cheaper to manufacture.
3.2.2
Cryptic Notation
Java uses symbols like ‚Äú&&‚Äù and ‚Äújj‚Äù in place of AND and OR. Circuit designers
use ‚Äú‚Äù and ‚ÄúC,‚Äù and actually refer to AND as a product and OR as a sum. Mathe-
maticians use still other symbols given in the table below.

Chapter 3
Logical Formulas
44
English
Symbolic Notation
NOT.P /
:P
(alternatively, P )
P AND Q
P ^ Q
P OR Q
P _ Q
P IMPLIES Q
P  ! Q
if P then Q
P  ! Q
P IFF Q
P  ! Q
P XOR Q
P Àö Q
For example, using this notation, ‚ÄúIf P AND NOT.Q/, then R‚Äù would be written:
.P ^ Q/  ! R:
The mathematical notation is concise but cryptic. Words such as ‚ÄúAND‚Äù and
‚ÄúOR‚Äù are easier to remember and won‚Äôt get confused with operations on numbers.
We will often use P as an abbreviation for NOT.P /, but aside from that, we mostly
stick to the words ‚Äîexcept when formulas would otherwise run off the page.
3.3
Equivalence and Validity
3.3.1
Implications and Contrapositives
Do these two sentences say the same thing?
If I am hungry, then I am grumpy.
If I am not grumpy, then I am not hungry.
We can settle the issue by recasting both sentences in terms of propositional logic.
Let P be the proposition ‚ÄúI am hungry‚Äù and Q be ‚ÄúI am grumpy.‚Äù The Ô¨Årst sentence
says ‚ÄúP
IMPLIES Q‚Äù and the second says ‚ÄúNOT.Q/ IMPLIES NOT.P /.‚Äù Once
more, we can compare these two statements in a truth table:
P
Q
.P IMPLIES Q/
.NOT.Q/
IMPLIES
NOT.P //
T
T
T
F
T
F
T
F
F
T
F
F
F
T
T
F
T
T
F
F
T
T
T
T
Sure enough, the highlighted columns showing the truth values of these two state-
ments are the same. A statement of the form ‚Äú(NOT Q/ IMPLIES .NOT P /‚Äù is called

3.3. Equivalence and Validity
45
the contrapositive of the implication ‚ÄúP IMPLIES Q.‚Äù The truth table shows that
an implication and its contrapositive are equivalent ‚Äîthey are just different ways
of saying the same thing.
In contrast, the converse of ‚ÄúP IMPLIES Q‚Äù is the statement ‚ÄúQ IMPLIES P .‚Äù
In terms of our example, the converse is:
If I am grumpy, then I am hungry.
This sounds like a rather different contention, and a truth table conÔ¨Årms this suspi-
cion:
P
Q
P IMPLIES Q
Q IMPLIES P
T
T
T
T
T
F
F
T
F
T
T
F
F
F
T
T
Now the highlighted columns differ in the second and third row, conÔ¨Årming that an
implication is generally not equivalent to its converse.
One Ô¨Ånal relationship: an implication and its converse together are equivalent to
an iff statement, speciÔ¨Åcally, to these two statements together. For example,
If I am grumpy then I am hungry, and if I am hungry then I am grumpy.
are equivalent to the single statement:
I am grumpy iff I am hungry.
Once again, we can verify this with a truth table.
P
Q
.P IMPLIES Q/
AND
.Q IMPLIES P /
P IFF Q
T
T
T
T
T
T
T
F
F
F
T
F
F
T
T
F
F
F
F
F
T
T
T
T
The fourth column giving the truth values of
.P IMPLIES Q/ AND .Q IMPLIES P /
is the same as the sixth column giving the truth values of P IFF Q, which conÔ¨Årms
that the AND of the implications is equivalent to the IFF statement.

Chapter 3
Logical Formulas
46
3.3.2
Validity and SatisÔ¨Åability
A valid formula is one which is always true, no matter what truth values its vari-
ables may have. The simplest example is
P OR NOT.P /:
You can think about valid formulas as capturing fundamental logical truths. For
example, a property of implication that we take for granted is that if one statement
implies a second one, and the second one implies a third, then the Ô¨Årst implies the
third. The following valid formula conÔ¨Årms the truth of this property of implication.
≈í.P IMPLIES Q/ AND .Q IMPLIES R/¬ç IMPLIES .P IMPLIES R/:
Equivalence of formulas is really a special case of validity. Namely, statements
F and G are equivalent precisely when the statement .F IFF G/ is valid. For
example, the equivalence of the expressions (3.3) and (3.2) means that
.A OR B/ IFF .A OR .NOT.A/ AND B//
is valid. Of course, validity can also be viewed as an aspect of equivalence. Namely,
a formula is valid iff it is equivalent to T.
A satisÔ¨Åable formula is one which can sometimes be true. That is, there is some
assignment of truth values to its variables that makes it true. One way satisÔ¨Åabil-
ity comes up is when there are a collection of system speciÔ¨Åcations. The job of
the system designer is to come up with a system that follows all the specs. This
means that the AND of all the specs had better be satisÔ¨Åable or the system will be
impossible (see Problem 3.10).
There is also a close relationship between validity and satisÔ¨Åability, namely, a
statement P is satisÔ¨Åable iff its negation NOT.P / is not valid.
3.4
The Algebra of Propositions
3.4.1
Propositions in Normal Form
Every propositional formula is equivalent to a ‚Äúsum-of-products‚Äù or disjunctive
form. More precisely, a disjunctive form is simply an OR of AND-terms, where
each AND-term is an AND of variables or negations of variables, for example,
.A AND B/ OR .A AND C/:
(3.4)

3.4. The Algebra of Propositions
47
You can read a disjunctive form for any propositional formula directly from its
truth table. For example, the formula
A AND .B OR C/
(3.5)
has truth table:
A
B
C
A
AND
.B OR C/
T
T
T
T
T
T
F
T
T
F
T
T
T
F
F
F
F
T
T
F
F
T
F
F
F
F
T
F
F
F
F
F
The formula (3.5) is true in the Ô¨Årst row when A, B, and C are all true, that is, where
A AND B AND C is true. It is also true in the second row where A AND B AND C
is true, and in the third row when A AND B AND C is true, and that‚Äôs all. So (3.5)
is true exactly when
.A AND B AND C/ OR .A AND B AND C/ OR .A AND B AND C/
(3.6)
is true. So (3.5) and (3.6) are equivalent.
The expression (3.6) is a disjunctive form where each AND-term is an AND of
every one of the variables or their negations in turn. An expression of this form is
called a disjunctive normal form (DNF). A DNF formula can often be simpliÔ¨Åed
into a smaller disjuctive form. For example, the DNF (3.6) further simpliÔ¨Åes to the
equivalent disjunctive form (3.4) above.
Incidentally, this equivalence of A AND .B OR C/ and .A AND B/ OR .A AND C/
is called the distributive law of AND over OR because of its obvious resemblance to
the distributivity of multiplication over addition for numbers.
Applying the same reasoning to the F entries of a truth table yields a conjunctive
form for any formula, namely an AND of OR-terms, where the OR-terms are OR‚Äôs
only of variables or their negations. For example, formula (3.5) is false in the fourth
row of its truth table (3.4.1) where A is T, B is F and C is F. But this is exactly
the one row where .A OR B OR C/ is F! Likewise, the (3.5) is false in the Ô¨Åfth
row which is exactly where .A OR B OR C/ is F. This means that (3.5) will be F
whenever the AND of these two OR-terms is false. Continuing in this way with the
OR-terms corresponding to the remaining three rows where (3.5) is false, we get a
conjunctive normal form (CNF) that is equivalent to (3.5), namely,
.A OR B OR C/ AND .A OR B OR C/ AND .A OR B OR C/AND
.A OR B OR C/ AND .A OR B OR C/

Chapter 3
Logical Formulas
48
The methods above can obviously be applied to any truth table, which implies
Theorem 3.4.1. Every propositional formula is equivalent to both a disjunctive
normal form and a conjunctive normal form.
3.4.2
Proving Equivalences
A check of equivalence or validity by truth table runs out of steam pretty quickly:
a proposition with n variables has a truth table with 2n lines, so the effort required
to check a proposition grows exponentially with the number of variables. For a
proposition with just 30 variables, that‚Äôs already over a billion lines to check!
An alternative approach that sometimes helps is to use algebra to prove equiv-
alence. A lot of different operators may appear in a propositional formula, so a
useful Ô¨Årst step is to get rid of all but three: AND, OR, and NOT. This is easy
because each of the operators is equivalent to a simple formula using only these
three. For example, A IMPLIES B is equivalent to NOT.A/ OR B. Formulas using
onlyAND, OR, and NOT for the remaining operators are left to Problem 3.11.
We list below a bunch of equivalence axioms with the symbol ‚Äú  ! ‚Äù between
equivalent formulas. These axioms are important because they are all that‚Äôs needed
to prove every possible equivalence. We‚Äôll start with some equivalences for AND‚Äôs
that look like the familiar ones for multiplication of numbers:
A AND B  ! B AND A
(commutativity of AND)
(3.7)
.A AND B/ AND C  ! A AND .B AND C/
(associativity of AND)
(3.8)
T AND A  ! A
(identity for AND)
F AND A  ! F
(zero for AND)
Three axioms that don‚Äôt directly correspond to number properties are
A AND A  ! A
(idempotence for AND)
A AND A  ! F
(contradiction for AND)
(3.9)
NOT.A/  ! A
(double negation)
It is associativity (3.8) that justiÔ¨Åes writing A AND B AND C without specifying
whether it is parenthesized as A AND .B AND C/ or .A AND B/ AND C. That‚Äôs
because both ways of inserting parentheses yield equivalent formulas.
There are a corresponding set of equivalences for OR which we won‚Äôt bother to
list, except for the OR rule corresponding to contradiction for AND (3.9):
A OR A  ! T
(validity for OR)

3.4. The Algebra of Propositions
49
There is also a familiar rule connecting AND and OR:
A AND .B OR C/
 ! .A AND B/ OR .A AND C/
(distributivity of AND over OR) (3.10)
Finally, there are DeMorgan‚Äôs Laws which explain how to distribute NOT‚Äôs over
AND‚Äôs and OR‚Äôs:
NOT.A AND B/  ! A OR B
(DeMorgan for AND)
(3.11)
NOT.A OR B/  ! A AND B
(DeMorgan for OR)
(3.12)
All these axioms can be veriÔ¨Åed easily with truth tables.
These axioms are all that‚Äôs needed to convert any formula to a disjunctive normal
form. We can illustrate how they work by applying them to turn the negation of
formula (3.5), namely,
NOT..A AND B/ OR .A AND C//:
(3.13)
into disjunctive normal form.
We start by applying DeMorgan‚Äôs Law for OR (3.12) to (3.13) in order to move
the NOT deeper into the formula. This gives
NOT.A AND B/ AND NOT.A AND C/:
Now applying Demorgan‚Äôs Law for AND (3.11) to the two innermost AND-terms,
gives
.A OR B/ AND .A OR C/:
(3.14)
At this point NOT only applies to variables, and we won‚Äôt need Demorgan‚Äôs Laws
any further.
Now we will repeatedly apply the distributivity of AND over OR (3.10) to turn (3.14)
into a disjunctive form. To start, we‚Äôll distribute .A OR B/ over AND to get
..A OR B/ AND A/ OR ..A OR B/ AND C/:
Using distributivity over both AND‚Äôs we get
..A AND A/ OR .B AND A// OR ..A AND C/ OR .B AND C//:
By the way, we‚Äôve implicitly used commutativity (3.7) here to justify distributing
over an AND from the right. Now applying idempotence to remove the duplicate
occurrence of A we get
.A OR .B AND A// OR ..A AND C/ OR .B AND C//:

Chapter 3
Logical Formulas
50
Associativity now allows dropping the parentheses around the terms being OR‚Äôd to
yield the following disjunctive form for (3.13):
A OR .B AND A/ OR .A AND C/ OR .B AND C/:
(3.15)
The last step is to turn each of these AND-terms into a disjunctive normal form
with all three variables A, B, and C. We‚Äôll illustrate how to do this for the second
AND-term .B AND A/. This term needs to mention C to be in normal form. To
introduce C, we use validity for OR and identity for AND to conclude that
.B AND A/  ! .B AND A/ AND .C OR C/:
Now distributing .B AND A/ over the OR yields the disjunctive normal form
.B AND A AND C/ OR .B AND A AND C/:
Doing the same thing to the other AND-terms in (3.15) Ô¨Ånally gives a disjunctive
normal form for (3.5):
.A AND B AND C/ OR .A AND B AND C/ OR
.A AND B AND C/ OR .A AND B AND C/ OR
.B AND A AND C/ OR .B AND A AND C/ OR
.A AND C AND B/ OR .A AND C AND B/ OR
.B AND C AND A/ OR .B AND C AND A/:
Using commutativity to sort the term and OR-idempotence to remove duplicates,
Ô¨Ånally yields a unique sorted DNF:
.A AND B AND C/ OR
.A AND B AND C/ OR
.A AND B AND C/ OR
.A AND B AND C/ OR
.A AND B AND C/:
This example illustrates a strategy for applying these equivalences to convert any
formula into disjunctive normal form, and conversion to conjunctive normal form
works similarly, which explains:
Theorem 3.4.2. Any propositional formula can be transformed into disjunctive
normal form or a conjunctive normal form using the equivalences listed above.
What has this got to do with equivalence? That‚Äôs easy: to prove that two for-
mulas are equivalent, convert them both to disjunctive normal form over the set of

3.5. The SAT Problem
51
variables that appear in the terms. Then use commutativity to sort the variables and
AND-terms so they all appear in some standard order. We claim the formulas are
equivalent iff they have the same sorted disjunctive normal form. This is obvious
if they do have the same disjunctive normal form. But conversely, the way we read
off a disjunctive normal form from a truth table shows that two different sorted
DNF‚Äôs over the same set of variables correspond to different truth tables and hence
to inequivalent formulas. This proves
Theorem 3.4.3 (Completeness of the propositional equivalence axioms). Two propo-
sitional formula are equivalent iff they can be proved equivalent using the equiva-
lence axioms listed above.
The beneÔ¨Åt of the axioms is that they leave room for ingeniously applying them
to prove equivalences with less effort than the truth table method. Theorem 3.4.3
then adds the reassurance that the axioms are guaranteed to prove every equiva-
lence, which is a great punchline for this section. But we don‚Äôt want to mislead
you: it‚Äôs important to realize that using the strategy we gave for applying the ax-
ioms involves essentially the same effort it would take to construct truth tables, and
there is no guarantee that applying the axioms will generally be any easier than
using truth tables.
3.5
The SAT Problem
Determining whether or not a more complicated proposition is satisÔ¨Åable is not so
easy. How about this one?
.P OR Q OR R/ AND .P OR Q/ AND .P OR R/ AND .R OR Q/
The general problem of deciding whether a proposition is satisÔ¨Åable is called
SAT. One approach to SAT is to construct a truth table and check whether or not a
T ever appears, but as for validity, this approach quickly bogs down for formulas
with many variables because truth tables grow exponentially with the number of
variables.
Is there a more efÔ¨Åcient solution to SAT? In particular, is there some brilliant
procedure that determines in a number of steps that grows polynomially ‚Äîlike
n2 or n14, instead of exponentially ‚Äîwhether any given proposition of size n is
satisÔ¨Åable or not? No one knows. And an awful lot hangs on the answer.
The general deÔ¨Ånition of an ‚ÄúefÔ¨Åcient‚Äù procedure is one that runs in polynomial
time, that is, that runs in a number of basic steps bounded by a polynomial in s,

Chapter 3
Logical Formulas
52
where s is the size of an input. It turns out that an efÔ¨Åcient solution to SAT would
immediately imply efÔ¨Åcient solutions to many, many other important problems in-
volving packing, scheduling, routing, and circuit veriÔ¨Åcation, among other things.
This would be wonderful, but there would also be worldwide chaos. Decrypting
coded messages would also become an easy task, so online Ô¨Ånancial transactions
would be insecure and secret communications could be read by everyone. Why this
would happen is explained in Section 8.12.
Of course, the situation is the same for validity checking, since you can check for
validity by checking for satisÔ¨Åability of a negated formula. This also explains why
the simpliÔ¨Åcation of formulas mentioned in Section 3.2 would be hard ‚Äîvalidity
testing is a special case of determining if a formula simpliÔ¨Åes to T.
Recently there has been exciting progress on SAT-solvers for practical applica-
tions like digital circuit veriÔ¨Åcation. These programs Ô¨Ånd satisfying assignments
with amazing efÔ¨Åciency even for formulas with millions of variables. Unfortu-
nately, it‚Äôs hard to predict which kind of formulas are amenable to SAT-solver meth-
ods, and for formulas that are unsatisÔ¨Åable, SAT-solvers generally get nowhere.
So no one has a good idea how to solve SAT in polynomial time, or how to
prove that it can‚Äôt be done ‚Äîresearchers are completely stuck. The problem of
determining whether or not SAT has a polynomial time solution is known as the
‚ÄúP vs. NP‚Äù problem.1 It is the outstanding unanswered question in theoretical
computer science. It is also one of the seven Millenium Problems: the Clay Institute
will award you $1,000,000 if you solve the P vs. NP problem.
3.6
Predicate Formulas
3.6.1
QuantiÔ¨Åers
The ‚Äúfor all‚Äù notation, 8, has already made an early appearance in Section 1.1. For
example, the predicate
‚Äúx2  0‚Äù
is always true when x is a real number. That is,
8x 2 R: x2  0
is a true statement. On the other hand, the predicate
‚Äú5x2   7 D 0‚Äù
1P stands for problems whose instances can be solved in time that grows polynomially with the
size of the instance. NP stands for nondeterministtic polynomial time, but we‚Äôll leave an explanation
of what that is to texts on the theory of computational complexity.

3.6. Predicate Formulas
53
is only sometimes true; speciÔ¨Åcally, when x D Àô
p
7=5. There is a ‚Äúthere exists‚Äù
notation, 9, to indicate that a predicate is true for at least one, but not necessarily
all objects. So
9x 2 R: 5x2   7 D 0
is true, while
8x 2 R: 5x2   7 D 0
is not true.
There are several ways to express the notions of ‚Äúalways true‚Äù and ‚Äúsometimes
true‚Äù in English. The table below gives some general formats on the left and speciÔ¨Åc
examples using those formats on the right. You can expect to see such phrases
hundreds of times in mathematical writing!
Always True
For all x 2 D, P.x/ is true.
For all x 2 R, x2  0.
P.x/ is true for every x in the set, D.
x2  0 for every x 2 R.
Sometimes True
There is an x 2 D such that P.x/ is true.
There is an x 2 R such that 5x2   7 D 0.
P.x/ is true for some x in the set, D.
5x2   7 D 0 for some x 2 R.
P.x/ is true for at least one x 2 D.
5x2   7 D 0 for at least one x 2 R.
All these sentences ‚Äúquantify‚Äù how often the predicate is true. SpeciÔ¨Åcally, an
assertion that a predicate is always true is called a universal quantiÔ¨Åcation, and an
assertion that a predicate is sometimes true is an existential quantiÔ¨Åcation. Some-
times the English sentences are unclear with respect to quantiÔ¨Åcation:
If you can solve any problem we come up with,
then you get an A for the course.
(3.16)
The phrase ‚Äúyou can solve any problem we can come up with‚Äù could reasonably be
interpreted as either a universal or existential quantiÔ¨Åcation:
you can solve every problem we come up with,
(3.17)
or maybe
you can solve at least one problem we come up with.
(3.18)
To be precise, let Probs be the set of problems we come up with, Solves.x/ be
the predicate ‚ÄúYou can solve problem x,‚Äù and G be the proposition, ‚ÄúYou get an A

Chapter 3
Logical Formulas
54
for the course.‚Äù Then the two different interpretations of (3.16) can be written as
follows:
.8x 2 Probs: Solves.x// IMPLIES G;
for (3.17);
.9x 2 Probs: Solves.x// IMPLIES G:
for (3.18):
3.6.2
Mixing QuantiÔ¨Åers
Many mathematical statements involve several quantiÔ¨Åers. For example, we al-
ready described
Goldbach‚Äôs Conjecture 1.1.8: Every even integer greater than 2 is the
sum of two primes.
Let‚Äôs write this out in more detail to be precise about the quantiÔ¨Åcation:
For every even integer n greater than 2, there exist primes p and q such
that n D p C q.
Let Evens be the set of even integers greater than 2, and let Primes be the set of
primes. Then we can write Goldbach‚Äôs Conjecture in logic notation as follows:
8n 2 Evens
‚Äû
∆í‚Äö
‚Ä¶
for every even
integer n > 2
9p 2 Primes 9q 2 Primes:
‚Äû
∆í‚Äö
‚Ä¶
there exist primes
p and q such that
n D p C q:
3.6.3
Order of QuantiÔ¨Åers
Swapping the order of different kinds of quantiÔ¨Åers (existential or universal) usually
changes the meaning of a proposition. For example, let‚Äôs return to one of our initial,
confusing statements:
‚ÄúEvery American has a dream.‚Äù
This sentence is ambiguous because the order of quantiÔ¨Åers is unclear. Let A be
the set of Americans, let D be the set of dreams, and deÔ¨Åne the predicate H.a; d/
to be ‚ÄúAmerican a has dream d.‚Äù Now the sentence could mean there is a single
dream that every American shares‚Äîsuch as the dream of owning their own home:
9 d 2 D 8a 2 A: H.a; d/
Or it could mean that every American has a personal dream:
8a 2 A 9 d 2 D: H.a; d/

3.6. Predicate Formulas
55
For example, some Americans may dream of a peaceful retirement, while others
dream of continuing practicing their profession as long as they live, and still others
may dream of being so rich they needn‚Äôt think about work at all.
Swapping quantiÔ¨Åers in Goldbach‚Äôs Conjecture creates a patently false statement
that every even number  2 is the sum of the same two primes:
9 p 2 Primes 9 q 2 Primes:
‚Äû
∆í‚Äö
‚Ä¶
there exist primes
p and q such that
8n 2 Evens
‚Äû
∆í‚Äö
‚Ä¶
for every even
integer n > 2
n D p C q:
3.6.4
Variables Over One Domain
When all the variables in a formula are understood to take values from the same
nonempty set, D, it‚Äôs conventional to omit mention of D. For example, instead of
8x 2 D 9y 2 D: Q.x; y/ we‚Äôd write 8x9y: Q.x; y/. The unnamed nonempty set
that x and y range over is called the domain of discourse, or just plain domain, of
the formula.
It‚Äôs easy to arrange for all the variables to range over one domain. For exam-
ple, Goldbach‚Äôs Conjecture could be expressed with all variables ranging over the
domain N as
8n: n 2 Evens IMPLIES .9 p 9 q: p 2 Primes AND q 2 Primes AND n D p C q/:
3.6.5
Negating QuantiÔ¨Åers
There is a simple relationship between the two kinds of quantiÔ¨Åers. The following
two sentences mean the same thing:
Not everyone likes ice cream.
There is someone who does not like ice cream.
The equivalence of these sentences is a instance of a general equivalence that holds
between predicate formulas:
NOT.8x: P.x//
is equivalent to
9x: NOT.P.x//:
(3.19)
Similarly, these sentences mean the same thing:
There is no one who likes being mocked.
Everyone dislikes being mocked.
The corresponding predicate formula equivalence is
NOT.9x: P.x//
is equivalent to
8x: NOT.P.x//:
(3.20)
The general principle is that moving a NOT across a quantiÔ¨Åer changes the kind of
quantiÔ¨Åer. Note that (3.20) follows from negating both sides of (3.19).

Chapter 3
Logical Formulas
56
3.6.6
Validity for Predicate Formulas
The idea of validity extends to predicate formulas, but to be valid, a formula now
must evaluate to true no matter what values its variables may take over any possi-
ble domain, no matter what interpretation a predicate variable may be given. For
example, we already observed that the rule for negating a quantiÔ¨Åer is captured by
the valid assertion (3.20).
Another useful example of a valid assertion is
9x8y: P.x; y/ IMPLIES 8y9x: P.x; y/:
(3.21)
Here‚Äôs an explanation why this is valid:
Let D be the domain for the variables and P0 be some binary predi-
cate2 on D. We need to show that if
9x 2 D: 8y 2 D: P0.x; y/
(3.22)
holds under this interpretation, then so does
8y 2 D 9x 2 D: P0.x; y/:
(3.23)
So suppose (3.22) is true. Then by deÔ¨Ånition of 9, this means that some
element d0 2 D has the property that
8y 2 D: P0.d0; y/:
By deÔ¨Ånition of 8, this means that
P0.d0; d/
is true for all d 2 D. So given any d 2 D, there is an element in D,
namely, d0, such that P0.d0; d/ is true. But that‚Äôs exactly what (3.23)
means, so we‚Äôve proved that (3.23) holds under this interpretation, as
required.
We hope this is helpful as an explanation, but we don‚Äôt really want to call it a
‚Äúproof.‚Äù The problem is that with something as basic as (3.21), it‚Äôs hard to see
what more elementary axioms are ok to use in proving it. What the explanation
above did was translate the logical formula (3.21) into English and then appeal to
the meaning, in English, of ‚Äúfor all‚Äù and ‚Äúthere exists‚Äù as justiÔ¨Åcation.
2That is, a predicate that depends on two variables.

3.6. Predicate Formulas
57
In contrast to (3.21), the formula
8y9x: P.x; y/ IMPLIES 9x8y: P.x; y/:
(3.24)
is not valid. We can prove this just by describing an interpretation where the hy-
pothesis, 8y9x: P.x; y/, is true but the conclusion, 9x8y: P.x; y/, is not true. For
example, let the domain be the integers and P.x; y/ mean x > y. Then the hy-
pothesis would be true because, given a value, n, for y we could choose the value
of x to be n C 1, for example. But under this interpretation the conclusion asserts
that there is an integer that is bigger than all integers, which is certainly false. An
interpretation like this that falsiÔ¨Åes an assertion is called a counter model to that
assertion.
Problems for Section 3.1
Practice Problems
Problem 3.1.
Some people are uncomfortable with the idea that from a false hypothesis you can
prove everything, and instead of having P IMPLIES Q be true when P is false,
they want P IMPLIES Q to be false when P is false. This would lead to IMPLIES
having the same truth table as what propositional connective?
Problem 3.2.
Suppose you are taking a class, and that class has a textbook and a Ô¨Ånal exam. Let
the propositional variables P , Q, and R have the following meanings:
P D You get an A on the Ô¨Ånal exam.
Q D You do every exercise in the book.
R D You get an A in the class.
Write the following propositions using P , Q, and R and logical connectives.
(a) You get an A in the class, but you do not do every exercise in the book.
(b) You get an A on the Ô¨Ånal, you do every exercise in the book, and you get an A
in the class.
(c) To get an A in the class, it is necessary for you to get an A on the Ô¨Ånal.
(d) You get an A on the Ô¨Ånal, but you don‚Äôt do every exercise in this book; never-
theless, you get an A in this class.

Chapter 3
Logical Formulas
58
Class Problems
Problem 3.3.
When the mathematician says to his student, ‚ÄúIf a function is not continuous, then it
is not differentiable,‚Äù then letting D stand for ‚Äúdifferentiable‚Äù and C for continuous,
the only proper translation of the mathematician‚Äôs statement would be
NOT.C/ IMPLIES NOT.D/;
or equivalently,
D IMPLIES C:
But when a mother says to her son, ‚ÄúIf you don‚Äôt do your homework, then you
can‚Äôt watch TV,‚Äù then letting T stand for ‚Äúcan watch TV‚Äù and H for ‚Äúdo your
homework,‚Äù a reasonable translation of the mother‚Äôs statement would be
NOT.H/ IFF NOT.T /;
or equivalently,
H IFF T:
Explain why it is reasonable to translate these two IF-THEN statements in dif-
ferent ways into propositional formulas.
Homework Problems
Problem 3.4.
Describe a simple recursive procedure which, given a positive integer argument,
n, produces a truth table whose rows are all the assignments of truth values to n
propositional variables. For example, for n D 2, the table might look like:
T
T
T
F
F
T
F
F
Your description can be in English, or a simple program in some familiar lan-
guage such as Scheme or Java. If you do write a program, be sure to include some
sample output.
Problems for Section 3.2
Class Problems
Problem 3.5.
Propositional logic comes up in digital circuit design using the convention that T

3.6. Predicate Formulas
59
corresponds to 1 and F to 0. A simple example is a 2-bit half-adder circuit. This
circuit has 3 binary inputs, a1; a0 and b, and 3 binary outputs, c; s1; s0. The 2-bit
word a1a0 gives the binary representation of an integer, k, between 0 and 3. The
3-bit word cs1s0 gives the binary representation of k C b. The third output bit, c,
is called the Ô¨Ånal carry bit.
So if k and b were both 1, then the value of a1a0 would be 01 and the value of
the output cs1s0 would 010, namely, the 3-bit binary representation of 1 C 1.
In fact, the Ô¨Ånal carry bit equals 1 only when all three binary inputs are 1, that is,
when k D 3 and b D 1. In that case, the value of cs1s0 is 100, namely, the binary
representation of 3 C 1.
This 2-bit half-adder could be described by the following formulas:
c0 D b
s0 D a0 XOR c0
c1 D a0 AND c0
the carry into column 1
s1 D a1 XOR c1
c2 D a1 AND c1
the carry into column 2
c D c2:
(a) Generalize the above construction of a 2-bit half-adder to an n C 1 bit half-
adder with inputs an; : : : ; a1; a0 and b for arbitrary n  0. That is, give simple
formulas for si and ci for 0  i  n C 1, where ci is the carry into column i C 1,
and c D cnC1.
(b) Write similar deÔ¨Ånitions for the digits and carries in the sum of two n C 1-bit
binary numbers an : : : a1a0 and bn : : : b1b0.
Visualized as digital circuits, the above adders consist of a sequence of single-
digit half-adders or adders strung together in series. These circuits mimic ordinary
pencil-and-paper addition, where a carry into a column is calculated directly from
the carry into the previous column, and the carries have to ripple across all the
columns before the carry into the Ô¨Ånal column is determined. Circuits with this
design are called ripple-carry adders. Ripple-carry adders are easy to understand
and remember and require a nearly minimal number of operations. But the higher-
order output bits and the Ô¨Ånal carry take time proportional to n to reach their Ô¨Ånal
values.
(c) How many of each of the propositional operations does your adder from part (b)
use to calculate the sum?

Chapter 3
Logical Formulas
60
Homework Problems
Problem 3.6.
There are adder circuits that are much faster than the ripple-carry circuits of Prob-
lem 3.5. They work by computing the values in later columns for both a carry of 0
and a carry of 1, in parallel. Then, when the carry from the earlier columns Ô¨Ånally
arrives, the pre-computed answer can be quickly selected. We‚Äôll illustrate this idea
by working out the equations for an .n C 1/-bit parallel half-adder.
Parallel half-adders are built out of parallel add1 modules. An .n C 1/-bit add1
module takes as input the .n C 1/-bit binary representation, an : : : a1a0, of an inte-
ger, s, and produces as output the binary representation, c pn : : : p1 p0, of s C 1.
(a) A 1-bit add1 module just has input a0. Write propositional formulas for its
outputs c and p0.
(b) Explain how to build an .nC1/-bit parallel half-adder from an .nC1/-bit add1
module by writing a propositional formula for the half-adder output, oi, using only
the variables ai, pi, and b.
We can build a double-size add1 module with 2.n C 1/ inputs using two single-
size add1 modules with nC1 inputs. Suppose the inputs of the double-size module
are a2nC1; : : : ; a1; a0 and the outputs are c; p2nC1; : : : ; p1; p0. The setup is illus-
trated in Figure 3.1.
Namely, the Ô¨Årst single size add1 module handles the Ô¨Årst n C 1 inputs. The in-
puts to this module are the low-order nC1 input bits an; : : : ; a1; a0, and its outputs
will serve as the Ô¨Årst n C 1 outputs pn; : : : ; p1; p0 of the double-size module. Let
c.1/ be the remaining carry output from this module.
The inputs to the second single-size module are the higher-order n C 1 input bits
a2nC1; : : : ; anC2; anC1. Call its Ô¨Årst n C 1 outputs rn; : : : ; r1; r0 and let c.2/ be its
carry.
(c) Write a formula for the carry, c, in terms of c.1/ and c.2/.
(d) Complete the speciÔ¨Åcation of the double-size module by writing propositional
formulas for the remaining outputs, pi, for n C 1  i  2n C 1. The formula for
pi should only involve the variables ai, ri .nC1/, and c.1/.
(e) Parallel half-adders are exponentially faster than ripple-carry half-adders. Con-
Ô¨Årm this by determining the largest number of propositional operations required to
compute any one output bit of an n-bit add module. (You may assume n is a power
of 2.)

3.6. Predicate Formulas
61
.nC1/-bit add1
2.nC2/-bit add1 module
.nC1/-bit add1
a2nC1
anC2 anC1
rn
r1
r0
an
a1
a0
pn
p1
p0
p2nC1
p2nC2 pnC1
c
c.1/
c.2/
Figure 3.1
Structure of a Double-size add1 Module.

Chapter 3
Logical Formulas
62
Exam Problems
Problem 3.7.
Show that there are exactly two truth assignments for the variables P,Q,R,S that
satisfy the following formula:
.P OR Q/ AND .Q OR R/ AND .R OR S/ AND .S OR P /
Hint: A truth table will do the job, but it will have a bunch of rows. A proof by
cases can be quicker; if you do use cases, be sure each one is clearly speciÔ¨Åed.
Problems for Section 3.3
Practice Problems
Problem 3.8.
Indicate whether each of the following propositional formulas is valid (V), satis-
Ô¨Åable but not valid (S), or not satisÔ¨Åable (N). For the satisÔ¨Åable ones, indicate a
satisfying truth assignment.
M IMPLIES Q
M IMPLIES .P OR Q/
M IMPLIES ≈íM AND .P IMPLIES M/¬ç
.P OR Q/ IMPLIES Q
.P OR Q/ IMPLIES .P AND Q/
.P OR Q/ IMPLIES ≈íM AND .P IMPLIES M/¬ç
.P XOR Q/ IMPLIES Q
.P XOR Q/ IMPLIES .P OR Q/
.P XOR Q/ IMPLIES ≈íM AND .P IMPLIES M/¬ç
Class Problems
Problem 3.9. (a) Verify by truth table that
.P
IMPLIES Q/ OR .Q IMPLIES P /
is valid.
(b) Let P and Q be propositional formulas. Describe a single formula, R, using
AND‚Äôs, OR‚Äôs, and NOT‚Äôs such that R is valid iff P and Q are equivalent.

3.6. Predicate Formulas
63
(c) A propositional formula is satisÔ¨Åable iff there is an assignment of truth values
to its variables ‚Äîan environment ‚Äîwhich makes it true. Explain why
P is valid
iff
NOT.P / is not satisÔ¨Åable.
(d) A set of propositional formulas P1; : : : ; Pk is consistent iff there is an envi-
ronment in which they are all true. Write a formula, S, so that the set P1; : : : ; Pk
is not consistent iff S is valid.
Problem 3.10.
This problem3 examines whether the following speciÔ¨Åcations are satisÔ¨Åable:
1. If the Ô¨Åle system is not locked, then
(a) new messages will be queued.
(b) new messages will be sent to the messages buffer.
(c) the system is functioning normally, and conversely, if the system is
functioning normally, then the Ô¨Åle system is not locked.
2. If new messages are not queued, then they will be sent to the messages buffer.
3. New messages will not be sent to the message buffer.
(a) Begin by translating the Ô¨Åve speciÔ¨Åcations into propositional formulas using
four propositional variables:
L WWD Ô¨Åle system locked;
Q WWD new messages are queued;
B WWD new messages are sent to the message buffer;
N WWD system functioning normally:
(b) Demonstrate that this set of speciÔ¨Åcations is satisÔ¨Åable by describing a single
truth assignment for the variables L; Q; B; N and verifying that under this assign-
ment, all the speciÔ¨Åcations are true.
(c) Argue that the assignment determined in part (b) is the only one that does the
job.
3From Rosen, 5th edition, Exercise 1.1.36

Chapter 3
Logical Formulas
64
Problems for Section 3.4
Practice Problems
Problem 3.11.
A half dozen different operators may appear in propositional formulas, but just
AND, OR, and NOT are enough to do the job. That is because each of the operators
is equivalent to a simple formula using only these three operators. For example,
A IMPLIES B is equivalent to NOT.A/ OR B. So all occurences of IMPLIES in a
formula can be replaced using just NOT and OR.
(a) Write formulas using only AND, OR, NOT that are equivalent to each of AIFFB
and A XOR B. Conclude that every propositional formula is equivalent to an AND-
OR-NOT formula.
(b) Explain why you don‚Äôt even need AND.
(c) Explain how to get by with the single operator NAND where A NAND B is
equivalent by deÔ¨Ånition to NOT.A AND B/.
Class Problems
Problem 3.12.
Explain how to Ô¨Ånd a conjunctive form for a propositional formula directly from a
disjunctive form for its complement.
Homework Problems
Problem 3.13.
Use the equivalence axioms of Section 3.4.2 to convert the following formula to
disjunctive form:
A XOR B XOR C:
Problems for Section 3.5
Homework Problems
Problem 3.14.
A 3-conjunctive form (3CF) formula is a conjunctive form formula in which each
OR-term is an OR of at most 3 variables or negations of variables. Although it
may be hard to tell if a propositional formula, F , is satisÔ¨Åable, it is always easy to
construct a formula, C.F /, that is
 in 3-conjunctive form,

3.6. Predicate Formulas
65
 has at most 24 times as many occurrences of variables as F , and
 is satisÔ¨Åable iff F is satisÔ¨Åable.
To construct C.F /, introduce a different new variables, one for each operator that
occurs in F . For example, if F was
..P XOR Q/ XOR R/ OR .P AND S/
(3.25)
we might use new variables X1, X2, O, and A corresponding to the the operator
occurrences as follows:
..P
XOR
‚Äû∆í‚Äö‚Ä¶
X1
Q/ XOR
‚Äû∆í‚Äö‚Ä¶
X2
R/
OR
‚Äû∆í‚Äö‚Ä¶
O
.P
AND
‚Äû∆í‚Äö‚Ä¶
A
S/:
Next we write a formula that constrains each new variable to have the same truth
value as the subformula determined by its corresponding operator. For the example
above, these constraining formulas would be
X1 IFF .P XOR Q/;
X2 IFF .X1 XOR R/;
A IFF .P AND S/;
O IFF .X2 XOR A/
(a) Explain why the AND of the four constraining formulas above along with a
Ô¨Åfth formula consisting of just the variable O will be satisÔ¨Åable iff (3.25) is satisÔ¨Å-
able.
(b) Explain why each constraining formula will be equivalent to a 3CF formula
with at most 24 occurrences of variables.
(c) Using the ideas illustrated in the previous parts, explain how to construct C.F /
for an arbitrary propositional formula, F .
Problems for Section 3.6
Practice Problems
Problem 3.15.
For each of the following propositions:
1. 8x 9y: 2x   y D 0
2. 8x 9y: x   2y D 0

Chapter 3
Logical Formulas
66
3. 8x: x < 10 IMPLIES .8y: y < x IMPLIES y < 9/
4. 8x 9y: ≈íy > x ^ 9z: y C z D 100¬ç
determine which propositions are true when the variables range over:
(a) the nonnegative integers.
(b) the integers.
(c) the real numbers.
Problem 3.16.
Let Q.x; y/ be the statement
‚Äúx has been a contestant on television show y.‚Äù
The universe of discourse for x is the set of all students at your school and for y is
the set of all quiz shows that have ever been on television.
Determine whether or not each of the following expressions is logically equiva-
lent to the sentence:
‚ÄúNo student at your school has ever been a contestant on a television quiz show.‚Äù
(a) 8x 8y: NOT.Q.x; y//
(b) 9x 9y: NOT.Q.x; y//
(c) NOT.8x 8y: Q.x; y//
(d) NOT.9x 9y: Q.x; y//
Problem 3.17.
Find a counter model showing the following is not valid.
9x:P.x/ IMPLIES 8x:P.x/
(Just deÔ¨Åne your counter model. You do not need to verify that it is correct.)

3.6. Predicate Formulas
67
Problem 3.18.
Find a counter model showing the following is not valid.
≈í9x: P.x/ AND 9x:Q.x/¬ç IMPLIES 9x:≈íP.x/ AND Q.x/¬ç
(Just deÔ¨Åne your counter model. You do not need to verify that it is correct.)
Problem 3.19.
Which of the following are valid?
(a) 9x9y: P.x; y/ IMPLIES 9y9x: P.x; y/
(b) 8x9y: Q.x; y/ IMPLIES 9y8x: Q.x; y/
(c) 9x8y: R.x; y/ IMPLIES 8y9x: R.x; y/
(d) NOT.9x S.x// IFF 8x NOT.S.x//
Class Problems
Problem 3.20.
A media tycoon has an idea for an all-news television network called LNN: The
Logic News Network. Each segment will begin with a deÔ¨Ånition of the domain of
discourse and a few predicates. The day‚Äôs happenings can then be communicated
concisely in logic notation. For example, a broadcast might begin as follows:
THIS IS LNN. The domain of discourse is
fAlbert; Ben; Claire; David; Emilyg:
Let D.x/ be a predicate that is true if x is deceitful. Let L.x; y/
be a predicate that is true if x likes y. Let G.x; y/ be a predicate that
is true if x gave gifts to y.
Translate the following broadcasts in logic notation into (English) statements.
(a)
NOT.D.Ben/ OR D.David// IMPLIES .L.Albert; Ben/ AND L.Ben; Albert//
(b)
8x ..x D Claire AND NOT.L.x; Emily/// OR .x ¬§ Claire AND L.x; Emily/// AND
8x ..x D David AND L.x; Claire// OR .x ¬§ David AND NOT.L.x; Claire////

Chapter 3
Logical Formulas
68
(c)
NOT.D.Claire// IMPLIES .G.Albert; Ben/ AND 9x: G.Ben; x//
(d)
8x9y9z .y ¬§ z/ AND L.x; y/ AND NOT.L.x; z//
(e) How could you express ‚ÄúEveryone except for Claire likes Emily‚Äù using just
propositional connectives without using any quantiÔ¨Åers (8; 9)? Can you generalize
to explain how any logical formula over this domain of discourse can be expressed
without quantiÔ¨Åers? How big would the formula in the previous part be if it was
expressed this way?
Problem 3.21.
The goal of this problem is to translate some assertions about binary strings into
logic notation. The domain of discourse is the set of all Ô¨Ånite-length binary strings:
, 0, 1, 00, 01, 10, 11, 000, 001, .... (Here  denotes the empty string.) In your
translations, you may use all the ordinary logic symbols (including =), variables,
and the binary symbols 0, 1 denoting 0, 1.
A string like 01x0y of binary symbols and variables denotes the concatenation
of the symbols and the binary strings represented by the variables. For example, if
the value of x is 011 and the value of y is 1111, then the value of 01x0y is the
binary string 0101101111.
Here are some examples of formulas and their English translations. Names for
these predicates are listed in the third column so that you can reuse them in your
solutions (as we do in the deÔ¨Ånition of the predicate NO-1S below).
Meaning
Formula
Name
x is a preÔ¨Åx of y
9z .xz D y/
PREFIX(x; y)
x is a substring of y
9u9v .uxv D y/
SUBSTRING(x; y)
x is empty or a string of 0‚Äôs
NOT.SUBSTRING.1; x//
NO-1S(x)
(a) x consists of three copies of some string.
(b) x is an even-length string of 0‚Äôs.
(c) x does not contain both a 0 and a 1.
(d) x is the binary representation of 2k C 1 for some integer k  0.

3.6. Predicate Formulas
69
(e) An elegant, slightly trickier way to deÔ¨Åne NO-1S.x/ is:
PREFIX.x; 0x/:
(*)
Explain why (*) is true only when x is a string of 0‚Äôs.
Problem 3.22.
For each of the logical formulas, indicate whether or not it is true when the do-
main of discourse is N, (the nonnegative integers 0, 1, 2, ...), Z (the integers), Q
(the rationals), R (the real numbers), and C (the complex numbers). Add a brief
explanation to the few cases that merit one.
9x: x2 D 2
8x:9y: x2 D y
8y:9x: x2 D y
8x ¬§ 0:9y: xy D 1
9x:9y: x C 2y D 2 AND 2x C 4y D 5
Problem 3.23.
Show that
.8x9y: P.x; y//  ! 8z: P.z; z/
is not valid by describing a counter-model.
Homework Problems
Problem 3.24.
Express each of the following predicates and propositions in formal logic notation.
The domain of discourse is the nonnegative integers, N. Moreover, in addition to
the propositional operators, variables and quantiÔ¨Åers, you may deÔ¨Åne predicates
using addition, multiplication, and equality symbols, and nonnegative integer con-
stants (0, 1,. . . ), but no exponentiation (like xy). For example, the predicate ‚Äún is
an even number‚Äù could be deÔ¨Åned by either of the following formulas:
9m: .2m D n/;
9m: .m C m D n/:
(a) m is a divisor of n.

Chapter 3
Logical Formulas
70
(b) n is a prime number.
(c) n is a power of a prime.
Problem 3.25.
Translate the following sentence into a predicate formula:
There is a student who has emailed exactly two other people in the
class, besides possibly herself.
The domain of discourse should be the set of students in the class; in addition,
the only predicates that you may use are
 equality, and
 E.x; y/, meaning that ‚Äúx has sent e-mail to y.‚Äù
Exam Problems
Problem 3.26.
The following predicate logic formula is invalid:
8x; 9y:P.x; y/  ! 9y; 8x:P.x; y/
Which of the following are counter models for it?
1. The predicate P.x; y/ D ‚Äòy  x D 1‚Äô where the domain of discourse is Q.
2. The predicate P.x; y/ D ‚Äòy < x‚Äô where the domain of discourse is R.
3. The predicate P.x; y/ D ‚Äòy  x D 2‚Äô where the domain of discourse is R
without 0.
4. The predicate P.x; y/ D ‚Äòyxy D x‚Äô where the domain of discourse is the
set of all binary strings, including the empty string.
Problem 3.27.
Some students from a large class will be lined up left to right. There will be at least
two stduents in the line. Translate each of the following assertions into predicate
formulas with the set of students in the class as the domain of discourse. The only
predicates you may use are

3.6. Predicate Formulas
71
 equality and,
 F.x; y/, meaning that ‚Äúx is somewhere to the left of y in the line.‚Äù For
example, in the line ‚ÄúCDA‚Äù, both F.C; A/ and F.C; D/ are true.
Once you have deÔ¨Åned a formula for a predicate P you may use the abbreviation
‚ÄúP ‚Äù in further formulas.
(a) Student x is in the line.
(b) Student x is Ô¨Årst in line.
(c) Student x is immediately to the right of student y.
(d) Student x is second.
Problem 3.28.
We want to Ô¨Ånd predicate formulas about the nonnegative integers, N, in which 
is the only predicate that appears, and no constants appear.
For example, there is such a formula deÔ¨Åning the equality predicate:
≈íx D y¬ç WWD ≈íx  y AND y  x¬ç:
Once predicate is shown to be expressible solely in terms of , it may then be used
in subsequent translations. For example,
≈íx > 0¬ç WWD 9y: NOT.x D y/ AND y  x:
(a) ≈íx D 0¬ç.
(b) ≈íx D y C 1¬ç
(c) x D 3


4
Mathematical Data Types
We‚Äôve mentioned the sets, sequences, and functions repeatedly, assuming these
concepts are familiar. We‚Äôll now take a more careful look at these mathematical
data types. We‚Äôll quickly review the basic deÔ¨Ånitions, add a few such as ‚Äúimages‚Äù
and ‚Äúinverse images‚Äù that may not be familiar, and end the chapter with some meth-
ods for comparing the sizes of sets.
4.1
Sets
Informally, a set is a bunch of objects, which are called the elements of the set.
The elements of a set can be just about anything: numbers, points in space, or even
other sets. The conventional way to write down a set is to list the elements inside
curly-braces. For example, here are some sets:
A
D
fAlex; Tippy; Shells; Shadowg
dead pets
B
D
fred; blue; yellowg
primary colors
C
D
ffa; bg; fa; cg; fb; cgg
a set of sets
This works Ô¨Åne for small Ô¨Ånite sets. Other sets might be deÔ¨Åned by indicating how
to generate a list of them:
D D f1; 2; 4; 8; 16; : : :g
the powers of 2
The order of elements is not signiÔ¨Åcant, so fx; yg and fy; xg are the same set
written two different ways. Also, any object is, or is not, an element of a given set
‚Äîthere is no notion of an element appearing more than once in a set.1 So writing
fx; xg is just indicating the same thing twice, speciÔ¨Åcally, that x is in the set. In
particular, fx; xg D fxg.
The expression e 2 S asserts that e is an element of set S. For example, 32 2 D
and blue 2 B, but Tailspin 62 A ‚Äîyet.
Sets are simple, Ô¨Çexible, and everywhere. You‚Äôll Ô¨Ånd some set mentioned in
nearly every section of this text.
1It‚Äôs not hard to develop a notion of multisets in which elements can occur more than once, but
multisets are not ordinary sets.

Chapter 4
Mathematical Data Types
74
4.1.1
Some Popular Sets
Mathematicians have devised special symbols to represent some common sets.
symbol
set
elements
;
the empty set
none
N
nonnegative integers
f0; 1; 2; 3; : : :g
Z
integers
f: : : ;  3;  2;  1; 0; 1; 2; 3; : : :g
Q
rational numbers
1
2;  5
3; 16; etc.
R
real numbers
; e;  9;
p
2; etc.
C
complex numbers
i; 19
2 ;
p
2   2i; etc.
A superscript ‚ÄúC‚Äù restricts a set to its positive elements; for example, RC denotes
the set of positive real numbers. Similarly, Z  denotes the set of negative integers.
4.1.2
Comparing and Combining Sets
The expression S  T indicates that set S is a subset of set T , which means that
every element of S is also an element of T (it could be that S D T ). For example,
N  Z (every nonnegative integer is a integer), Q  R (every rational number is a
real number), but C 6 R (not every complex number is a real number).
As a memory trick, notice that the  points to the smaller set, just like a  sign
points to the smaller number. Actually, this connection goes a little further: there
is a symbol  analogous to the ‚Äúless than‚Äù symbol <. Thus, S  T means that S
is a subset of T , but the two are not equal. So A  A, but A 6 A, for every set A.
There are several ways to combine sets. Let‚Äôs deÔ¨Åne a couple of sets for use in
examples:
X WWD f1; 2; 3g
Y WWD f2; 3; 4g
 The union of sets X and Y (denoted X [ Y ) contains all elements appearing
in X or Y or both. So, X [ Y D f1; 2; 3; 4g.
 The intersection of X and Y (denoted X \ Y ) consists of all elements that
appear in both X and Y . So, X \ Y D f2; 3g.
 The set difference of X and Y (denoted X   Y ) consists of all elements that
are in X, but not in Y . So, X   Y D f1g and Y   X D f4g.

4.1. Sets
75
4.1.3
Complement of a Set
Sometimes we are focused on a particular domain, D. Then for any subset, A, of
D, we deÔ¨Åne A to be the set of all elements of D not in A. That is, A WWD D   A.
The set A is called the complement of A.
For example, when the domain we‚Äôre working with is the real numbers, the com-
plement of the positive real numbers is the set of negative real numbers together
with zero. That is,
RC D R  [ f0g:
It can be helpful to rephrase properties of sets using complements. For example,
two sets, A and B, are said to be disjoint iff they have no elements in common, that
is, A \ B D ;. This is the same as saying that A is a subset of the complement of
B, that is, A  B.
4.1.4
Power Set
The set of all the subsets of a set, A, is called the power set, pow.A/, of A. So
B 2 pow.A/ iff B  A. For example, the elements of pow.f1; 2g/ are ;; f1g; f2g
and f1; 2g.
More generally, if A has n elements, then there are 2n sets in pow.A/. For this
reason, some authors use the notation 2A instead of pow.A/.
4.1.5
Set Builder Notation
An important use of predicates is in set builder notation. We‚Äôll often want to talk
about sets that cannot be described very well by listing the elements explicitly or
by taking unions, intersections, etc., of easily described sets. Set builder notation
often comes to the rescue. The idea is to deÔ¨Åne a set using a predicate; in particular,
the set consists of all values that make the predicate true. Here are some examples
of set builder notation:
A WWD fn 2 N j n is a prime and n D 4k C 1 for some integer kg
B WWD fx 2 R j x3   3x C 1 > 0g
C WWD fa C bi 2 C j a2 C 2b2  1g
The set A consists of all nonnegative integers n for which the predicate
‚Äún is a prime and n D 4k C 1 for some integer k‚Äù
is true. Thus, the smallest elements of A are:
5; 13; 17; 29; 37; 41; 53; 57; 61; 73; : : : :

Chapter 4
Mathematical Data Types
76
Trying to indicate the set A by listing these Ô¨Årst few elements wouldn‚Äôt work very
well; even after ten terms, the pattern is not obvious! Similarly, the set B consists
of all real numbers x for which the predicate
x3   3x C 1 > 0
is true. In this case, an explicit description of the set B in terms of intervals would
require solving a cubic equation. Finally, set C consists of all complex numbers
a C bi such that:
a2 C 2b2  1
This is an oval-shaped region around the origin in the complex plane.
4.1.6
Proving Set Equalities
Two sets are deÔ¨Åned to be equal if they contain exactly the same elements. That
is, X D Y means that z 2 X if and only if z 2 Y , for all elements, z.2 So set
equalities can be formulated and proved as ‚Äúiff‚Äù theorems. For example:
Theorem 4.1.1 (Distributive Law for Sets). Let A, B, and C be sets. Then:
A \ .B [ C/ D .A \ B/ [ .A \ C/
(4.1)
Proof. The equality (4.1) is equivalent to the assertion that
z 2 A \ .B [ C/
iff
z 2 .A \ B/ [ .A \ C/
(4.2)
for all z. Now we‚Äôll prove (4.2) by a chain of iff‚Äôs.
Now we have
z 2 A \ .B [ C/
iff
.z 2 A/ AND .z 2 B [ C/
(def of \)
iff
.z 2 A/ AND .z 2 B OR z 2 C/
(def of [)
iff
.z 2 A AND z 2 B/ OR .z 2 A AND z 2 C/
(AND distributivity (3.10))
iff
.z 2 A \ B/ OR .z 2 A \ C/
(def of \)
iff
z 2 .A \ B/ [ .A \ C/
(def of [)

2This is actually the Ô¨Årst of the ZFC axioms axioms for set theory mentioned at the end of Sec-
tion 1.3 and discussed further in Section 7.3.2.

4.2. Sequences
77
4.2
Sequences
Sets provide one way to group a collection of objects. Another way is in a se-
quence, which is a list of objects called terms or components. Short sequences
are commonly described by listing the elements between parentheses; for example,
.a; b; c/ is a sequence with three terms.
While both sets and sequences perform a gathering role, there are several differ-
ences.
 The elements of a set are required to be distinct, but terms in a sequence can
be the same. Thus, .a; b; a/ is a valid sequence of length three, but fa; b; ag
is a set with two elements ‚Äînot three.
 The terms in a sequence have a speciÔ¨Åed order, but the elements of a set do
not. For example, .a; b; c/ and .a; c; b/ are different sequences, but fa; b; cg
and fa; c; bg are the same set.
 Texts differ on notation for the empty sequence; we use  for the empty
sequence.
The product operation is one link between sets and sequences. A product of sets,
S1S2  Sn, is a new set consisting of all sequences where the Ô¨Årst component
is drawn from S1, the second from S2, and so forth. For example, N  fa; bg is
the set of all pairs whose Ô¨Årst element is a nonnegative integer and whose second
element is an a or a b:
N  fa; bg D f.0; a/; .0; b/; .1; a/; .1; b/; .2; a/; .2; b/; : : :g
A product of n copies of a set S is denoted Sn. For example, f0; 1g3 is the set of
all 3-bit sequences:
f0; 1g3 D f.0; 0; 0/; .0; 0; 1/; .0; 1; 0/; .0; 1; 1/; .1; 0; 0/; .1; 0; 1/; .1; 1; 0/; .1; 1; 1/g
4.3
Functions
A function assigns an element of one set, called the domain, to an element of an-
other set, called the codomain. The notation
f W A ! B

Chapter 4
Mathematical Data Types
78
indicates that f is a function with domain, A, and codomain, B. The familiar
notation ‚Äúf .a/ D b‚Äù indicates that f assigns the element b 2 B to a. Here b
would be called the value of f at argument a.
Functions are often deÔ¨Åned by formulas as in:
f1.x/ WWD 1
x2
where x is a real-valued variable, or
f2.y; z/ WWD y10yz
where y and z range over binary strings, or
f3.x; n/ WWD the pair .n; x/
where n ranges over the nonnegative integers.
A function with a Ô¨Ånite domain could be speciÔ¨Åed by a table that shows the value
of the function at each element of the domain. For example, a function f4.P; Q/
where P and Q are propositional variables is speciÔ¨Åed by:
P
Q
f4.P; Q/
T
T
T
T
F
F
F
T
T
F
F
T
Notice that f4 could also have been described by a formula:
f4.P; Q/ WWD ≈íP IMPLIES Q¬ç:
A function might also be deÔ¨Åned by a procedure for computing its value at any
element of its domain, or by some other kind of speciÔ¨Åcation. For example, deÔ¨Åne
f5.y/ to be the length of a left to right search of the bits in the binary string y until
a 1 appears, so
f5.0010/
D
3;
f5.100/
D
1;
f5.0000/
is
undeÔ¨Åned:
Notice that f5 does not assign a value to any string of just 0‚Äôs. This illustrates an
important fact about functions: they need not assign a value to every element in the
domain. In fact this came up in our Ô¨Årst example f1.x/ D 1=x2, which does not

4.3. Functions
79
assign a value to 0. So in general, functions may be partial functions, meaning that
there may be domain elements for which the function is not deÔ¨Åned. If a function
is deÔ¨Åned on every element of its domain, it is called a total function.
It‚Äôs often useful to Ô¨Ånd the set of values a function takes when applied to the
elements in a set of arguments. So if f W A ! B, and S is a subset of A, we deÔ¨Åne
f .S/ to be the set of all the values that f takes when it is applied to elements of S.
That is,
f .S/ WWD fb 2 B j f .s/ D b for some s 2 Sg:
For example, if we let ≈ír; s¬ç denote set of numbers in the interval from r to s on the
real line, then f1.≈í1; 2¬ç/ D ≈í1=4; 1¬ç.
For another example, let‚Äôs take the ‚Äúsearch for a 1‚Äù function, f5. If we let X be
the set of binary words which start with an even number of 0‚Äôs followed by a 1,
then f5.X/ would be the odd nonnegative integers.
Applying f to a set, S, of arguments is referred to as ‚Äúapplying f pointwise to
S‚Äù, and the set f .S/ is referred to as the image of S under f .3 The set of values
that arise from applying f to all possible arguments is called the range of f . That
is,
range.f / WWD f .domain.f //:
Some authors refer to the codomain as the range of a function, but they shouldn‚Äôt.
The distinction between the range and codomain will be important later in Sec-
tions 4.5 when we relate sizes of sets to properties of functions between them.
4.3.1
Function Composition
Doing things step by step is a universal idea. Taking a walk is a literal example, but
so is cooking from a recipe, executing a computer program, evaluating a formula,
and recovering from substance abuse.
Abstractly, taking a step amounts to applying a function, and going step by step
corresponds to applying functions one after the other. This is captured by the op-
eration of composing functions. Composing the functions f and g means that Ô¨Årst
f is applied to some argument, x, to produce f .x/, and then g is applied to that
result to produce g.f .x//.
DeÔ¨Ånition 4.3.1. For functions f W A ! B and g W B ! C, the composition,
g ƒ± f , of g with f is deÔ¨Åned to be the function from A to C deÔ¨Åned by the rule:
.g ƒ± f /.x/ WWD g.f .x//;
3There is a picky distinction between the function f which applies to elements of A and the
function which applies f pointwise to subsets of A, because the domain of f is A, while the domain
of pointwise-f is pow.A/. It is usually clear from context whether f or pointwise-f is meant, so
there is no harm in overloading the symbol f in this way.

Chapter 4
Mathematical Data Types
80
for all x 2 A.
Function composition is familiar as a basic concept from elementary calculus,
and it plays an equally basic role in discrete mathematics.
4.4
Binary Relations
Binary relations deÔ¨Åne relations between two objects. For example, ‚Äúless-than‚Äù on
the real numbers relates every real number, a, to a real number, b, precisely when
a < b. Similarly, the subset relation relates a set, A, to another set, B, precisely
when A  B. A function f W A ! B is a special case of binary relation in which
an element a 2 A is related to an element b 2 B precisely when b D f .a/.
In this section we‚Äôll deÔ¨Åne some basic vocabulary and properties of binary rela-
tions.
DeÔ¨Ånition 4.4.1. A binary relation, R, consists of a set, A, called the domain of
R, a set, B, called the codomain of R, and a subset of A  B called the graph of R.
A relation whose domain is A and codomain is B is said to be ‚Äúbetween A and
B‚Äù, or ‚Äúfrom A to B.‚Äù As with functions, we write R W A ! B to indicate that
R is a relation from A to B. When the domain and codomain are the same set, A,
we simply say the relation is ‚Äúon A.‚Äù It‚Äôs common to use inÔ¨Åx notation ‚Äúa R b‚Äù to
mean that the pair .a; b/ is in the graph of R.
Notice that DeÔ¨Ånition 4.4.1 is exactly the same as the deÔ¨Ånition in Section 4.3
of a function, except that it doesn‚Äôt require the functional condition that, for each
domain element, a, there is at most one pair in the graph whose Ô¨Årst coordinate is
a. As we said, a function is a special case of a binary relation.
The ‚Äúin-charge of‚Äù relation, chrg, for MIT in Spring ‚Äô10 subjects and instructors
is a handy example of a binary relation. Its domain, Fac, is the names of all the MIT
faculty and instructional staff, and its codomain is the set, SubNums, of subject
numbers in the Fall ‚Äô09‚ÄìSpring ‚Äô10 MIT subject listing. The graph of chrg contains
precisely the pairs of the form
.hinstructor-namei ; hsubject-numi/
such that the faculty member named hinstructor-namei is in charge of the subject
with number hsubject-numi that was offered in Spring ‚Äô10. So graph.chrg/ contains

4.4. Binary Relations
81
pairs like
.A. R. Meyer;
6.042/;
.A. R. Meyer;
18.062/;
.A. R. Meyer;
6.844/;
.T. Leighton;
6.042/;
.T. Leighton;
18.062/;
.G. Freeman;
6.011/;
.G. Freeman;
6.UAT/;
.G. Freeman;
6.881/
.G. Freeman;
6.882/
.T. Eng;
6.UAT/
.J. Guttag;
6.00/
:::
Some subjects in the codomain, SubNums, do not appear among this list of pairs
‚Äîthat is, they are not in range.chrg/. These are the Fall term-only subjects. Simi-
larly, there are instructors in the domain, Fac, who do not appear in the list because
all their in-charge subjects are Fall term-only.
4.4.1
Relation Diagrams
Some standard properties of a relation can be visualized in terms of a diagram. The
diagram for a binary relation, R, has points corresponding to the elements of the
domain appearing in one column (a very long column if domain.R/ is inÔ¨Ånite). All
the elements of the codomain appear in another column which we‚Äôll usually picture
as being to the right of the domain column. There is an arrow going from a point,
a, in the lefthand, domain column to a point, b, in the righthand, codomain column,
precisely when the corresponding elements are related by R. For example, here are
diagrams for two functions:
A
B
a
-
1
b PPPPPP
q
2
c PPPPPP
q
3
d 
3
4
e 
3
A
B
a
-
1
b PPPPPP
q
2
c QQQQQQ
s
3
d 
3
4
5
Being a function is certainly an important property of a binary relation. What it
means is that every point in the domain column has at most one arrow coming out

Chapter 4
Mathematical Data Types
82
of it. So we can describe being a function as the ‚Äú 1 arrow out‚Äù property. There
are four more standard properties of relations that come up all the time. Here are
all Ô¨Åve properties deÔ¨Åned in terms of arrows:
DeÔ¨Ånition 4.4.2. A binary relation, R is
 is a function when it has the ≈í 1 arrow out¬ç property.
 is surjective when it has the ≈í 1 arrows in¬ç property. That is, every point in
the righthand, codomain column has at least one arrow pointing to it.
 is total when it has the ≈í 1 arrows out¬ç property.
 is injective when it has the ≈í 1 arrow in¬ç property.
 is bijective when it has both the ≈íD 1 arrow out¬ç and the ≈íD 1 arrow in¬ç
property.
From here on, we‚Äôll stop mentioning the arrows in these properties and for ex-
ample, just write ≈í 1 in¬ç instead of ≈í 1 arrows in¬ç.
So in the diagrams above, the relation on the left has the ≈íD 1 out¬ç and ≈í 1 in¬ç
properties, which means it is a total, surjective, function. But it does not have the
≈í 1 in¬ç property because element 3 has two arrows going into it; in other words, it
is not injective.
The relation on the right has the ≈íD 1 out¬ç and ≈í 1 in¬ç properties, which means
it is a total, injective function. But it does not have the ≈í 1 in¬ç property because
element 4 has no arrow going into it; in other words, it is not surjective.
Of course the arrows in a diagram for R correspond precisely to the pairs in
the graph of R. Notice that knowing just where the arrows are is not enough to
determine, for example, if R has the ≈í 1 out¬ç, total, property. If all we know is
the arrows, we wouldn‚Äôt know about any points in the domain column that had no
arrows out. In other words, graph.R/ alone does not determine whether R is total:
we also need to know what domain.R/ is.
Example 4.4.3. The function deÔ¨Åned by the formula 1=x2 has the ≈í 1 out¬ç prop-
erty if its domain is RC, but not if its domain is some set of real numbers including
0. It has the ≈íD 1 in¬ç and ≈íD 1 out¬ç property if its domain and codomain are both
RC, but it has neither the ≈í 1 in¬ç nor the ≈í 1 out¬ç property if its domain and
codomain are both R.
4.4.2
Relational Images
The idea of the image of a set under a function extends directly to relations.

4.4. Binary Relations
83
DeÔ¨Ånition 4.4.4. The image of a set, Y , under a relation, R, written R.Y /, is the
set of elements of the codomain, B, of R that are related to some element in Y . In
terms of the relation diagram, R.Y / is the set of points with an arrow coming in
that starts from some point in Y .
For example, the set of subject numbers that Meyer is in charge of in Spring ‚Äô10
is exactly chrg.A. Meyer/. To Ô¨Ågure out what this is, we look for all the arrows
in the chrg diagram that start at ‚ÄúA. Meyer,‚Äù and see which subject-numbers are at
the other end of these arrows. The set of these subject-numbers happened to be
f6.042, 18.062, 6.844g. Similarly, to Ô¨Ånd the subject numbers that either Freeman
or Eng are in charge of, we can collect all the arrows that start at either ‚ÄúG. Free-
man,‚Äù or ‚ÄúT. Eng‚Äù and, again, see which subject-numbers are at the other end of
these arrows. This, by deÔ¨Ånition, is chrg.fG. Freeman; T. Engg/. The partial list of
pairs in graph.chrg/ given above implies that
f6.011, 6.881, 6.882, 6.UATg  chrg.fG. Freeman; T. Engg/:
Finally, Fac is the set of all in-charge instructors, so chrg.Fac/ is the set of all the
subjects listed for Spring ‚Äô10.
Inverse Relations and Images
DeÔ¨Ånition 4.4.5. The inverse, R 1 of a relation R W A ! B is the relation from B
to A deÔ¨Åned by the rule
b R 1 a
IFF a R b:
In other words, R 1 is the relation you get by reversing the direction of the
arrows in the diagram for R.
DeÔ¨Ånition 4.4.6. The image of a set under the relation, R 1, is called the inverse
image of the set. That is, the inverse image of a set, X, under the relation, R, is
deÔ¨Åned to be R 1.X/.
Continuing with the in-charge example above, the set of instructors in charge
of 6.UAT in Spring ‚Äô10 is exactly the inverse image of f6.UATg under the chrg
relation. They turn out to be Eng and Freeman. That is,
chrg 1.f6.UATg/ D fT. Eng; D. Freemang:
Now let Intro be the set of introductory course 6 subject numbers. These are the
subject numbers that start with ‚Äú6.0.‚Äù So the set of names of the instructors who
were in-charge of introductory course 6 subjects in Spring ‚Äô10, is chrg 1.Intro/.
From the part of the graph of chrg shown above, we can see that Meyer, Leighton,

Chapter 4
Mathematical Data Types
84
Freeman, and Guttag were among the instructors in charge of introductory subjects
in Spring ‚Äô10. That is,
fMeyer, Leighton, Freeman, Guttagg  chrg 1.Intro/:
Finally, chrg 1.SubNums/, is the set of all instructors who were in charge of a
subject listed for Spring ‚Äô10.
4.5
Finite Cardinality
A Ô¨Ånite set is one that has only a Ô¨Ånite number of elements. This number of ele-
ments is the ‚Äúsize‚Äù or cardinality of the set:
DeÔ¨Ånition 4.5.1. If A is a Ô¨Ånite set, the cardinality of A, written jAj, is the number
of elements in A.
A Ô¨Ånite set may have no elements (the empty set), or one element, or two ele-
ments,. . . , so the cardinality of Ô¨Ånite sets is always a nonnegative integer.
Now suppose R W A ! B is a function. This means that every element of A
contributes at most one arrow to the diagram for R, so the number of arrows is at
most the number of elements in A. That is, if R is a function, then
jAj  #arrows:
If R is also surjective, then every element of B has an arrow into it, so there must
be at least as many arrows in the diagram as the size of B. That is,
#arrows  jBj:
Combining these inequalities implies that if R is a surjective function, then jAj 
jBj.
In short, if we write A surj B to mean that there is a surjective function from A
to B, then we‚Äôve just proved a lemma: if A surj B, then jAj  jBj. The following
deÔ¨Ånition and lemma lists this statement and three similar rules relating domain
and codomain size to relational properties.
DeÔ¨Ånition 4.5.2. Let A; B be (not necessarily Ô¨Ånite) sets. Then
1. A surj B iff there is a surjective function from A to B.
2. A inj B iff there is a total, injective relation from A to B.

4.5. Finite Cardinality
85
3. A bij B iff there is a bijection from A to B.
4. A strict B iff B surj A, but not A surj B.
Lemma 4.5.3.
1. If A surj B, then jAj  jBj.
2. If A inj B, then jAj  jBj.
3. If A bij B, then jAj D jBj.
Proof. We‚Äôve already given an ‚Äúarrow‚Äù proof of implication 1. Implication 2. fol-
lows immediately from the fact that if R has the ≈í 1 out¬ç, function property, and
the ≈í 1 in¬ç, surjective property, then R 1 is total and injective, so A surj B iff
B inj A. Finally, since a bijection is both a surjective function and a total injective
relation, implication 3. is an immediate consequence of the Ô¨Årst two.

Lemma 4.5.3.1. has a converse: if the size of a Ô¨Ånite set, A, is greater than
or equal to the size of another Ô¨Ånite set, B, then it‚Äôs always possible to deÔ¨Åne a
surjective function from A to B. In fact, the surjection can be a total function. To
see how this works, suppose for example that
A D fa0; a1; a2; a3; a4; a5g
B D fb0; b1; b2; b3g:
Then deÔ¨Åne a total function f W A ! B by the rules
f .a0/ WWD b0; f .a1/ WWD b1; f .a2/ WWD b2; f .a3/ D f .a4/ D f .a5/ WWD b3:
More concisely,
f .ai/ WWD bmin.i;3/;
for 0  i  5. Since 5  3, this f is a surjection. So we have Ô¨Ågured out that if
A and B are Ô¨Ånite sets, then jAj  jBj if and only if A surj B. So it follows that
A strict B iff jAj < jBj. All told, this argument wraps up the proof of the Theorem
that summarizes the whole Ô¨Ånite cardinality story:
Theorem 4.5.4. [Mapping Rules] For Ô¨Ånite sets, A; B,
jAj  jBj
iff
A surj B;
(4.3)
jAj  jBj
iff
A inj B;
(4.4)
jAj D jBj
iff
A bij B;
(4.5)
jAj < jBj
iff
A strict B:
(4.6)

Chapter 4
Mathematical Data Types
86
4.5.1
How Many Subsets of a Finite Set?
As an application of the bijection mapping rule (4.5), we can give an easy proof of:
Theorem 4.5.5. There are 2n subsets of an n-element set. That is,
jAj D n
implies
j pow.A/j D 2n:
For example, the three-element set fa1; a2; a3g has eight different subsets:
;
fa1g
fa2g
fa1; a2g
fa3g
fa1; a3g
fa2; a3g
fa1; a2; a3g
Theorem 4.5.5 follows from the fact that there is a simple bijection from subsets
of A to f0; 1gn, the n-bit sequences. Namely, let a1; a2; : : : ; an be the elements
of A. The bijection maps each subset of S  A to the bit sequence .b1; : : : ; bn/
deÔ¨Åned by the rule that
bi D 1
iff
ai 2 S:
For example, if n D 10, then the subset fa2; a3; a5; a7; a10g maps to a 10-bit
sequence as follows:
subset:
f
a2;
a3;
a5;
a7;
a10
g
sequence:
.
0;
1;
1;
0;
1;
0;
1;
0;
0;
1
/
Now by bijection case of the Mapping Rules 4.5.4.(4.5),
j pow.A/j D jf0; 1gnj:
But every computer scientist knows4 that there are 2n n-bit sequences! So we‚Äôve
proved Theorem 4.5.5!
Problems for Section 4.1
Homework Problems
Problem 4.1.
Let A, B, and C be sets. Prove that:
A [ B [ C D .A   B/ [ .B   C/ [ .C   A/ [ .A \ B \ C/:
(4.7)
Hint: P OR Q OR R is equivalent to
.P AND Q/ OR .Q AND R/ OR .R AND P / OR .P AND Q AND R/:
4In case you‚Äôre someone who doesn‚Äôt know how many n-bit sequences there are, you‚Äôll Ô¨Ånd the
2n explained in Section 14.2.2.

4.5. Finite Cardinality
87
Class Problems
Problem 4.2.
Set Formulas and Propositional Formulas.
(a) Verify that the propositional formula .P AND Q/ OR .P AND Q/ is equivalent
to P .
(b) Prove that5
A D .A   B/ [ .A \ B/
for all sets, A; B, by using a chain of iff‚Äôs to show that
x 2 A IFF x 2 .A   B/ [ .A \ B/
for all elements, x.
Problem 4.3.
Subset take-away6 is a two player game involving a Ô¨Åxed Ô¨Ånite set, A. Players
alternately choose nonempty subsets of A with the conditions that a player may not
choose
 the whole set A, or
 any set containing a set that was named earlier.
The Ô¨Årst player who is unable to move loses the game.
For example, if A is f1g, then there are no legal moves and the second player
wins. If A is f1; 2g, then the only legal moves are f1g and f2g. Each is a good reply
to the other, and so once again the second player wins.
The Ô¨Årst interesting case is when A has three elements. This time, if the Ô¨Årst
player picks a subset with one element, the second player picks the subset with
the other two elements. If the Ô¨Årst player picks a subset with two elements, the
second player picks the subset whose sole member is the third element. Both cases
produce positions equivalent to the starting position when A has two elements, and
thus leads to a win for the second player.
5The set difference, A   B, of sets A and B is
A   B WWD fa 2 A j a ‚Ä¶ Bg:
6From Christenson & Tilford, David Gale‚Äôs Subset Takeaway Game, American Mathematical
Monthly, Oct. 1997

Chapter 4
Mathematical Data Types
88
Verify that when A has four elements, the second player still has a winning strat-
egy.7
Practice Problems
Problem 4.4.
For any set A, let pow.A/ be its power set, the set of all its subsets; note that A is
itself a member of pow.A/. Let ; denote the empty set.
(a) The elements of pow.f1; 2g/ are:
(b) The elements of pow.f;; f;gg/ are:
(c) How many elements are there in pow.f1; 2; : : : ; 8g/?
Problem 4.5.
How many relations are there on a set of size n when:
(a) n D 1?
(b) n D 2?
(c) n D 3?
Exam Problems
Problem 4.6.
Below is a familiar ‚Äúchain of IFF‚Äôs‚Äù proof of the set equality
A [ .B \ A/ D A:
(4.8)
Proof.
x 2 A [ .B \ A/ IFF x 2 A OR x 2 .B \ A/
(def of [)
IFF x 2 A OR .x 2 B AND x 2 A/
(def of \)
IFF x 2 A;
where the last IFF follows from the fact that
the propositional formulas P OR .Q AND P / and P are equivalent.

7David Gale worked out some of the properties of this game and conjectured that the second
player wins the game for any set A. This remains an open problem.

4.5. Finite Cardinality
89
State a similar propositional equivalence that would justify the key step in a chain
of IFF‚Äôs proof for the following set equality.
A   B D
 A   C

[ .B \ C/ [
  A [ B

\ C

(4.9)
(You are not being asked to write out a IFF proof of the equality or a proof of the
propositional equivalence. Just state the equivalence.)
Problems for Section 4.2
Homework Problems
Problem 4.7.
Prove that for any sets A, B, C, and D, if AB and C D are disjoint, then either
A and C are disjoint or B and D are disjoint.
Problem 4.8. (a) Give an example where the following result fails:
False Theorem. For sets A, B, C, and D, let
L WWD .A [ B/  .C [ D/;
R WWD .A  C/ [ .B  D/:
Then L D R.
(b) Identify the mistake in the following proof of the False Theorem.
Bogus proof. Since L and R are both sets of pairs, it‚Äôs sufÔ¨Åcient to prove that
.x; y/ 2 L  ! .x; y/ 2 R for all x; y.
The proof will be a chain of iff implications:
.x; y/ 2 R
iff
.x; y/ 2 .A  C/ [ .B  D/
iff
.x; y/ 2 A  C, or .x; y/ 2 B  D
iff
(x 2 A and y 2 C) or else (x 2 B and y 2 D)
iff
either x 2 A or x 2 B, and either y 2 C or y 2 D
iff
x 2 A [ B and y 2 C [ D
iff
.x; y/ 2 L.

(c) Fix the proof to show that R  L.

Chapter 4
Mathematical Data Types
90
Problems for Section 4.4
Practice Problems
Problem 4.9.
For a binary relation, R W A ! B, some properties of R can be determined from
just the arrows of R, that is, from graph.R/, and others require knowing if there are
elements in the domain, A, or the codomain, B, that don‚Äôt show up in graph.R/.
For each of the following possible properties of R, indicate whether it is always
determined by
1. graph.R/ alone,
2. graph.R/ and A alone,
3. graph.R/ and B alone,
4. all three parts of R.
Properties:
(a) surjective
(b) injective
(c) total
(d) function
(e) bijection
Problem 4.10.
The inverse, R 1, of a binary relation, R, from A to B, is the relation from B to A
deÔ¨Åned by:
b R 1 a
iff
a R b:
In other words, you get the diagram for R 1 from R by ‚Äúreversing the arrows‚Äù in
the diagram describing R. Now many of the relational properties of R correspond
to different properties of R 1. For example, R is total iff R 1 is a surjection.
Fill in the remaining entries is this table:

4.5. Finite Cardinality
91
R is
iff
R 1 is
total
a surjection
a function
a surjection
an injection
a bijection
Hint: Explain what‚Äôs going on in terms of ‚Äúarrows‚Äù from A to B in the diagram
for R.
Problem 4.11.
For each of the following real-valued functions on the real numbers, indicate whether
it is a bijection, a surjection but not a bijection, an injection but not a bijection, or
neither an injection nor a surjection.
(a) x ! x C 2
(b) x ! 2x
(c) x ! x2
(d) x ! x3
(e) x ! sin x
(f) x ! x sin x
(g) x ! ex
Problem 4.12.
Let f W A ! B and g W B ! C be functions and h W A ! C be their composition,
namely, h.a/ WWD g.f .a// for all a 2 A.
(a) Prove that if f and g are surjections, then so is h.
(b) Prove that if f and g are bijections, then so is h.
(c) If f is a bijection, then so is f  1.
Class Problems
Problem 4.13. (a) Prove that if A surj B and B surj C, then A surj C.

Chapter 4
Mathematical Data Types
92
(b) Explain why A surj B iff B inj A.
(c) Conclude from (a) and (b) that if A inj B and B inj C, then A inj C.
(d) Explain why A inj B iff there is a total injective function (≈íD 1 out;  1 in¬ç)
from A to B. 8
Problem 4.14.
Let A be the following set of Ô¨Åve propositional formulas shown below on the left,
and let C be the set of three propositional formulas on the right. The ‚Äúimplies‚Äù
binary relation, I, from A to C is deÔ¨Åned by the rule
F I G
iff
≈íthe formula .F IMPLIES G/ is valid¬ç:
For example, .P AND Q/ I P , because the formula .P AND Q/ does imply P .
Also, it is not true that .P OR Q/ I P since .P OR Q/ IMPLIES P is not valid.
(a) Fill in the arrows so the following Ô¨Ågure describes the graph of the relation, I:
A
arrows
C
M
M AND .P IMPLIES M/
P AND Q
Q
P OR Q
P OR Q
NOT.P AND Q/
P XOR Q
8The ofÔ¨Åcial deÔ¨Ånition of inj is with a total injective relation (≈í 1 out;  1 in¬ç)

4.5. Finite Cardinality
93
(b) Circle the properties below possessed by the relation I:
FUNCTION
TOTAL
INJECTIVE
SURJECTIVE
BIJECTIVE
(c) Circle the properties below possessed by the relation I  1:
FUNCTION
TOTAL
INJECTIVE
SURJECTIVE
BIJECTIVE
Homework Problems
Problem 4.15.
Let f W A ! B and g W B ! C be functions.
(a) Prove that if the composition g ƒ± f is a bijection, then f is a total injection
and g is a surjection.
(b) Show there is a total injection, f , and a bijection, g, such that g ƒ± f is not a
bijection?
Problem 4.16.
Let A, B, and C be nonempty sets, and let f W B ! C and g W A ! B be
functions. Let h WWD f ƒ± g be the composition function of f and g, namely, the
function with domain A and range C such that h.x/ D f .g.x//.
(a) Prove that if h is surjective and f is total and injective, then g must be surjec-
tive.
Hint: contradiction.
(b) Suppose that h is injective and f is total. Prove that g must be injective and
provide a counterexample showing how this claim could fail if f was not total.
Problem 4.17.
Let A, B, and C be sets, and let f W B ! C and g W A ! B be functions. Let
h W A ! C be the composition, f ƒ± g, that is, h.x/ WWD f .g.x// for x 2 A. Prove
or disprove the following claims:
(a) If h is surjective, then f must be surjective.
(b) If h is surjective, then g must be surjective.
(c) If h is injective, then f must be injective.

Chapter 4
Mathematical Data Types
94
(d) If h is injective and f is total, then g must be injective.
Problem 4.18.
The language of sets and relations may seem remote from the practical world of
programming, but in fact there is a close connection to relational databases, a
very popular software application building block implemented by such software
packages as MySQL. This problem explores the connection by considering how to
manipulate and analyze a large data set using operators over sets and relations. Sys-
tems like MySQL are able to execute very similar high-level instructions efÔ¨Åciently
on standard computer hardware, which helps programmers focus on high-level de-
sign.
Consider a basic Web search engine, which stores information on Web pages and
processes queries to Ô¨Ånd pages satisfying conditions provided by users. At a high
level, we can formalize the key information as:
 A set P of pages that the search engine knows about
 A binary relation L (for link) over pages, deÔ¨Åned such that p1Lp2 iff page
p1 links to p2
 A set E of endorsers, people who have recorded their opinions about which
pages are high-quality
 A binary relation R (for recommends) between endorsers and pages, such
that eRp iff person e has recommended page p
 A set W of words that may appear on pages
 A binary relation M (for mentions) between pages and words, where pMw
iff word w appears on page p
Each part of this problem describes an intuitive, informal query over the data,
and your job is to produce a single expression using the standard set and relation
operators, such that the expression can be interpreted as answering the query cor-
rectly, for any data set. Your answers should use only the set and relation symbols
given above, in addition to terms standing for constant elements of E or W , plus
the following operators introduced in the text:
 set union, [.
 set intersection, \.

4.5. Finite Cardinality
95
 set difference,  .
 relational image ‚Äîfor example, R.A/ for some set A, or R.a/ for some
speciÔ¨Åc element a.
 relational inverse  1.
 . . . and one extra: relational composition which generalizes composition of
functions
a .R ƒ± S/ c WWD 9b 2 B: .a S b/ AND .b R c/:
In other words, a is related to c in R ƒ± S if starting at a you can follow an S
arrow to the start of an R arrow and then follow the R arrow to get to c.9
Here is one worked example to get you started:
 Search description: The set of pages containing the word ‚Äúlogic‚Äù
 Solution expression: M  1.‚Äúlogic‚Äù/
Find similar solutions for each of the following searches:
(a) The set of pages containing the word ‚Äúlogic‚Äù but not the word ‚Äúpredicate‚Äù
(b) The set of pages containing the word ‚Äúset‚Äù that have been recommended by
‚ÄúMeyer‚Äù
(c) The set of endorsers who have recommended pages containing the word ‚Äúal-
gebra‚Äù
(d) The relation that relates endorser e and word w iff e has recommended a page
containing w
(e) The set of pages that have at least one incoming or outgoing link
(f) The relation that relates word w and page p iff w appears on a page that links
to p
(g) The relation that relates word w and endorser e iff w appears on a page that
links to a page that e recommends
(h) The relation that relates pages p1 and p2 iff p2 can be reached from p1 by
following a sequence of exactly 3 links
9Note the reversal of R and S in the deÔ¨Ånition; this is to make relational composition work like
function composition. For functions, f ƒ± g means you apply g Ô¨Årst. That is, if we let h be f ƒ± g,
then h.x/ D f .g.x//.

Chapter 4
Mathematical Data Types
96
Exam Problems
Problem 4.19.
Let A be the set containing the Ô¨Åve sets: fag; fb; cg; fb; dg; fa; eg; fe; f g, and let
B be the set containing the three sets: fa; bg; fb; c; dg; fe; f g. Let R be the ‚Äúis
subset of‚Äù binary relation from A to B deÔ¨Åned by the rule:
X R Y
IFF
X  Y:
(a) Fill in the arrows so the following Ô¨Ågure describes the graph of the relation,
R:
A
arrows
B
fag
fa; bg
fb; cg
fb; c; dg
fb; dg
fe; f g
fa; eg
fe; f g
(b) Circle the properties below possessed by the relation R:
FUNCTION
TOTAL
INJECTIVE
SURJECTIVE
BIJECTIVE
(c) Circle the properties below possessed by the relation R 1:
FUNCTION
TOTAL
INJECTIVE
SURJECTIVE
BIJECTIVE

4.5. Finite Cardinality
97
Problems for Section 4.5
Practice Problems
Problem 4.20.
For any function f W A ! B and subset, A0  A, we deÔ¨Åne
f .A0/ WWD ff .a/ j a 2 A0g
For example, if f .x/ is the doubling function, 2x, with domain and codomain equal
to the real numbers, then f .Z/ deÔ¨Ånes the set of even integers.
Now assume f is total and A is Ô¨Ånite, and replace the ? with one of ; D;  to
produce the strongest correct version of the following statements:
(a) jf .A/j ? jBj.
(b) If f is a surjection, then jAj ? jBj.
(c) If f is a surjection, then jf .A/j ? jBj.
(d) If f is an injection, then jf .A/j ? jAj.
(e) If f is a bijection, then jAj ? jBj.
Class Problems
Problem 4.21.
Let A D fa0; a1; : : : ; an 1g be a set of size n, and B D fb0; b1; : : : ; bm 1g a set
of size m. Prove that jA  Bj D mn by deÔ¨Åning a simple bijection from A  B to
the nonnegative integers from 0 to mn   1.
Problem 4.22.
Let R W A ! B be a binary relation. Use an arrow counting argument to prove the
following generalization of the Mapping Rule 1.
Lemma. If R is a function, and X  A, then
jXj  jR.X/j:


5
Induction
Induction is a powerful method for showing a property is true for all nonnegative in-
tegers. Induction plays a central role in discrete mathematics and computer science.
In fact, its use is a deÔ¨Åning characteristic of discrete ‚Äîas opposed to continuous
‚Äîmathematics. This chapter introduces two versions of induction, Ordinary and
Strong, and explains why they work and how to use them in proofs. It also intro-
duces the Invariant Principle, which is a version of induction specially adapted for
reasoning about step-by-step processes.
5.1
Ordinary Induction
To understand how induction works, suppose there is a professor who brings a
bottomless bag of assorted miniature candy bars to her large class. She offers to
share the candy in the following way. First, she lines the students up in order. Next
she states two rules:
1. The student at the beginning of the line gets a candy bar.
2. If a student gets a candy bar, then the following student in line also gets a
candy bar.
Let‚Äôs number the students by their order in line, starting the count with 0, as usual
in computer science. Now we can understand the second rule as a short description
of a whole sequence of statements:
 If student 0 gets a candy bar, then student 1 also gets one.
 If student 1 gets a candy bar, then student 2 also gets one.
 If student 2 gets a candy bar, then student 3 also gets one.
:::
Of course this sequence has a more concise mathematical description:
If student n gets a candy bar, then student n C 1 gets a candy bar, for
all nonnegative integers n.

Chapter 5
Induction
100
So suppose you are student 17. By these rules, are you entitled to a miniature candy
bar? Well, student 0 gets a candy bar by the Ô¨Årst rule. Therefore, by the second
rule, student 1 also gets one, which means student 2 gets one, which means student
3 gets one as well, and so on. By 17 applications of the professor‚Äôs second rule,
you get your candy bar! Of course the rules really guarantee a candy bar to every
student, no matter how far back in line they may be.
5.1.1
A Rule for Ordinary Induction
The reasoning that led us to conclude that every student gets a candy bar is essen-
tially all there is to induction.
The Induction Principle.
Let P be a predicate on nonnegative integers. If
 P.0/ is true, and
 P.n/ IMPLIES P.n C 1/ for all nonnegative integers, n,
then
 P.m/ is true for all nonnegative integers, m.
Since we‚Äôre going to consider several useful variants of induction in later sec-
tions, we‚Äôll refer to the induction method described above as ordinary induction
when we need to distinguish it. Formulated as a proof rule as in Section 1.4.1, this
would be
Rule. Induction Rule
P.0/;
8n 2 N: P.n/ IMPLIES P.n C 1/
8m 2 N: P.m/
This Induction Rule works for the same intuitive reason that all the students get
candy bars, and we hope the explanation using candy bars makes it clear why the
soundness of ordinary induction can be taken for granted. In fact, the rule is so
obvious that it‚Äôs hard to see what more basic principle could be used to justify it.1
What‚Äôs not so obvious is how much mileage we get by using it.
1But see Section 5.3.

5.1. Ordinary Induction
101
5.1.2
A Familiar Example
Below is the formula (5.1) for the sum of the nonnegative integers up to n. The
formula holds for all nonnegative integers, so it is the kind of statement to which
induction applies directly. We‚Äôve already proved this formula using the Well Or-
dering Principle (Theorem 2.2.1), but now we‚Äôll prove it by induction, that is, using
the Induction Principle.
Theorem 5.1.1. For all n 2 N,
1 C 2 C 3 C    C n D n.n C 1/
2
(5.1)
To prove the theorem by induction, deÔ¨Åne predicate P.n/ to be the equation (5.1).
Now the theorem can be restated as the claim that P.n/ is true for all n 2 N. This
is great, because the Induction Principle lets us reach precisely that conclusion,
provided we establish two simpler facts:
 P.0/ is true.
 For all n 2 N, P.n/ IMPLIES P.n C 1/.
So now our job is reduced to proving these two statements.
The Ô¨Årst statement follows because of the convention that a sum of zero terms
is equal to 0. So P.0/ is the true assertion that a sum of zero terms is equal to
0.0 C 1/=2 D 0.
The second statement is more complicated. But remember the basic plan from
Section 1.5 for proving the validity of any implication: assume the statement on
the left and then prove the statement on the right. In this case, we assume P.n/
‚Äînamely, equation (5.1) ‚Äîin order to prove P.n C 1/, which is the equation
1 C 2 C 3 C    C n C .n C 1/ D .n C 1/.n C 2/
2
:
(5.2)
These two equations are quite similar; in fact, adding .n C 1/ to both sides of
equation (5.1) and simplifying the right side gives the equation (5.2):
1 C 2 C 3 C    C n C .n C 1/ D n.n C 1/
2
C .n C 1/
D .n C 2/.n C 1/
2
Thus, if P.n/ is true, then so is P.n C 1/. This argument is valid for every non-
negative integer n, so this establishes the second fact required by the induction
proof. Therefore, the Induction Principle says that the predicate P.m/ is true for
all nonnegative integers, m, so the theorem is proved.

Chapter 5
Induction
102
5.1.3
A Template for Induction Proofs
The proof of equation (5.1) was relatively simple, but even the most complicated
induction proof follows exactly the same template. There are Ô¨Åve components:
1. State that the proof uses induction. This immediately conveys the overall
structure of the proof, which helps your reader follow your argument.
2. DeÔ¨Åne an appropriate predicate P.n/. The predicate P.n/ is called the
induction hypothesis. The eventual conclusion of the induction argument
will be that P.n/ is true for all nonnegative n. A clearly stated induction
hypothesis is often the most important part of an induction proof, and its
omission is the largest source of confused proofs by students.
In the simplest cases, the induction hypothesis can be lifted straight from the
proposition you are trying to prove, as we did with equation (5.1). Sometimes
the induction hypothesis will involve several variables, in which case you
should indicate which variable serves as n.
3. Prove that P.0/ is true. This is usually easy, as in the example above. This
part of the proof is called the base case or basis step.
4. Prove that P.n/ implies P.n C 1/ for every nonnegative integer n. This
is called the inductive step. The basic plan is always the same: assume that
P.n/ is true and then use this assumption to prove that P.n C 1/ is true.
These two statements should be fairly similar, but bridging the gap may re-
quire some ingenuity. Whatever argument you give must be valid for every
nonnegative integer n, since the goal is to prove that all the following impli-
cations are true:
P.0/ ! P.1/; P.1/ ! P.2/; P.2/ ! P.3/; : : : :
5. Invoke induction. Given these facts, the induction principle allows you to
conclude that P.n/ is true for all nonnegative n. This is the logical capstone
to the whole argument, but it is so standard that it‚Äôs usual not to mention it
explicitly.
Always be sure to explicitly label the base case and the inductive step. Doing
so will make your proofs clearer and will decrease the chance that you forget a key
step ‚Äîlike checking the base case.

5.1. Ordinary Induction
103
5.1.4
A Clean Writeup
The proof of Theorem 5.1.1 given above is perfectly valid; however, it contains a
lot of extraneous explanation that you won‚Äôt usually see in induction proofs. The
writeup below is closer to what you might see in print and should be prepared to
produce yourself.
Revised proof of Theorem 5.1.1. We use induction. The induction hypothesis, P.n/,
will be equation (5.1).
Base case: P.0/ is true, because both sides of equation (5.1) equal zero when
n D 0.
Inductive step: Assume that P.n/ is true, where n is any nonnegative integer.
Then
1 C 2 C 3 C    C n C .n C 1/ D n.n C 1/
2
C .n C 1/
(by induction hypothesis)
D .n C 1/.n C 2/
2
(by simple algebra)
which proves P.n C 1/.
So it follows by induction that P.n/ is true for all nonnegative n.

It probably bothers you that induction led to a proof of this summation formula
but didn‚Äôt explain where the formula came from in the Ô¨Årst place. Nor does the
induction proof offer an intuitive way to understand the formula. This is both a
weakness and a strength. It is a weakness when a proof does not provide insight.
But is a strength that a proof can provide a reader with a reliable guarantee of
correctness without requiring insight.2
5.1.5
A More Challenging Example
During the development of MIT‚Äôs famous Stata Center, as costs rose further and
further beyond budget, some radical fundraising ideas were proposed. One rumored
plan was to install a big square courtyard divided into unit squares. The big square
would be 2n units on a side for some undetermined nonnegative integer n, and one
of the unit squares in the center3 occupied by a statue of a wealthy potential donor
‚Äîwhom the fund raisers privately referred to as ‚ÄúBill.‚Äù The n D 3 case is shown
in Figure 5.1.
2Methods for Ô¨Ånding such formulas are covered in Part III of the text.
3In the special case n D 0, the whole courtyard consists of a single central square; otherwise,
there are four central squares.

Chapter 5
Induction
104
2n
2n
Figure 5.1
A 2n  2n courtyard for n D 3.
Figure 5.2
The special L-shaped tile.
A complication was that the building‚Äôs unconventional architect, Frank Gehry,
was alleged to require that only special L-shaped tiles (shown in Figure 5.2) be
used for the courtyard. For n D 2, a courtyard meeting these constraints is shown
in Figure 5.3. But what about for larger values of n? Is there a way to tile a 2n  2n
courtyard with L-shaped tiles around a statue in the center? Let‚Äôs try to prove that
this is so.
Theorem 5.1.2. For all n  0 there exists a tiling of a 2n  2n courtyard with Bill
in a central square.
Proof. (doomed attempt) The proof is by induction. Let P.n/ be the proposition
that there exists a tiling of a 2n  2n courtyard with Bill in the center.
Base case: P.0/ is true because Bill Ô¨Ålls the whole courtyard.
Inductive step: Assume that there is a tiling of a 2n  2n courtyard with Bill in the
center for some n  0. We must prove that there is a way to tile a 2nC1  2nC1
courtyard with Bill in the center ....


5.1. Ordinary Induction
105
B
Figure 5.3
A tiling using L-shaped tiles for n D 2 with Bill in a center square.
Now we‚Äôre in trouble! The ability to tile a smaller courtyard with Bill in the
center isn‚Äôt much help in tiling a larger courtyard with Bill in the center. We haven‚Äôt
Ô¨Ågured out how to bridge the gap between P.n/ and P.n C 1/.
So if we‚Äôre going to prove Theorem 5.1.2 by induction, we‚Äôre going to need some
other induction hypothesis than simply the statement about n that we‚Äôre trying to
prove.
When this happens, your Ô¨Årst fallback should be to look for a stronger induction
hypothesis; that is, one which implies your previous hypothesis. For example,
we could make P.n/ the proposition that for every location of Bill in a 2n  2n
courtyard, there exists a tiling of the remainder.
This advice may sound bizarre: ‚ÄúIf you can‚Äôt prove something, try to prove some-
thing grander!‚Äù But for induction arguments, this makes sense. In the inductive
step, where you have to prove P.n/ IMPLIES P.n C 1/, you‚Äôre in better shape
because you can assume P.n/, which is now a more powerful statement. Let‚Äôs see
how this plays out in the case of courtyard tiling.
Proof (successful attempt). The proof is by induction. Let P.n/ be the proposition
that for every location of Bill in a 2n  2n courtyard, there exists a tiling of the
remainder.
Base case: P.0/ is true because Bill Ô¨Ålls the whole courtyard.
Inductive step: Assume that P.n/ is true for some n  0; that is, for every location
of Bill in a 2n  2n courtyard, there exists a tiling of the remainder. Divide the
2nC1  2nC1 courtyard into four quadrants, each 2n  2n. One quadrant contains
Bill (B in the diagram below). Place a temporary Bill (X in the diagram) in each of
the three central squares lying outside this quadrant as shown in Figure 5.4.

Chapter 5
Induction
106
X
X
B
X
2n
2n
2n
2n
Figure 5.4
Using a stronger inductive hypothesis to prove Theorem 5.1.2.
Now we can tile each of the four quadrants by the induction assumption. Replac-
ing the three temporary Bills with a single L-shaped tile completes the job. This
proves that P.n/ implies P.n C 1/ for all n  0. Thus P.m/ is true for all m 2 N,
and the theorem follows as a special case where we put Bill in a central square.

This proof has two nice properties. First, not only does the argument guarantee
that a tiling exists, but also it gives an algorithm for Ô¨Ånding such a tiling. Second,
we have a stronger result: if Bill wanted a statue on the edge of the courtyard, away
from the pigeons, we could accommodate him!
Strengthening the induction hypothesis is often a good move when an induction
proof won‚Äôt go through. But keep in mind that the stronger assertion must actually
be true; otherwise, there isn‚Äôt much hope of constructing a valid proof! Sometimes
Ô¨Ånding just the right induction hypothesis requires trial, error, and insight. For
example, mathematicians spent almost twenty years trying to prove or disprove
the conjecture that every planar graph is 5-choosable.4 Then, in 1994, Carsten
Thomassen gave an induction proof simple enough to explain on a napkin. The
key turned out to be Ô¨Ånding an extremely clever induction hypothesis; with that in
hand, completing the argument was easy!
45-choosability is a slight generalization of 5-colorability. Although every planar graph is 4-
colorable and therefore 5-colorable, not every planar graph is 4-choosable. If this all sounds like
nonsense, don‚Äôt panic. We‚Äôll discuss graphs, planarity, and coloring in Part II of the text.

5.1. Ordinary Induction
107
5.1.6
A Faulty Induction Proof
If we have done a good job in writing this text, right about now you should be
thinking, ‚ÄúHey, this induction stuff isn‚Äôt so hard after all ‚Äîjust show P.0/ is true
and that P.n/ implies P.n C 1/ for any number n.‚Äù And, you would be right,
although sometimes when you start doing induction proofs on your own, you can
run into trouble. For example, we will now use induction to ‚Äúprove‚Äù that all horses
are the same color. . .just when you thought it was safe to skip class and work on
your robot program instead. Sorry!
False Theorem. All horses are the same color.
Notice that no n is mentioned in this assertion, so we‚Äôre going to have to re-
formulate it in a way that makes an n explicit. In particular, we‚Äôll (falsely) prove
that
False Theorem 5.1.3. In every set of n  1 horses, all the horses are the same
color.
This is a statement about all integers n  1 rather  0, so it‚Äôs natural to use a
slight variation on induction: prove P.1/ in the base case and then prove that P.n/
implies P.nC1/ for all n  1 in the inductive step. This is a perfectly valid variant
of induction and is not the problem with the proof below.
Bogus proof. The proof is by induction on n. The induction hypothesis, P.n/, will
be
In every set of n horses, all are the same color.
(5.3)
Base case: (n D 1). P.1/ is true, because in a set of horses of size 1, there‚Äôs only
one horse, and this horse is deÔ¨Ånitely the same color as itself.
Inductive step: Assume that P.n/ is true for some n  1. That is, assume that in
every set of n horses, all are the same color. Now suppose we have a set of n C 1
horses:
h1; h2; : : : ; hn; hnC1:
We need to prove these n C 1 horses are all the same color.
By our assumption, the Ô¨Årst n horses are the same color:
h1; h2; : : : ; hn
‚Äû
∆í‚Äö
‚Ä¶
same color
; hnC1
Also by our assumption, the last n horses are the same color:
h1; h2; : : : ; hn; hnC1
‚Äû
∆í‚Äö
‚Ä¶
same color

Chapter 5
Induction
108
So h1 is the same color as the remaining horses besides hnC1 ‚Äîthat is, h2; : : : ; hn.
Likewise, hnC1 is the same color as the remaining horses besides h1 ‚Äîthat is,
h2; : : : ; hn, again. Since h1 and hnC1 are the same color as h2; : : : ; hn, all n C 1
horses must be the same color, and so P.n C 1/ is true. Thus, P.n/ implies P.n C
1/.
By the principle of induction, P.n/ is true for all n  1.

We‚Äôve proved something false! Does this mean that math broken and we should
all take up poetry instead? Of course not! It just means that this proof has a mistake.
The mistake in this argument is in the sentence that begins ‚ÄúSo h1 is the same
color as the remaining horses besides hnC1 ‚Äîthat is h2; : : : ; hn; : : : .‚Äù The ellipis
notation (‚Äú: : : ‚Äù) in the expression ‚Äúh1; h2; : : : ; hn; hnC1‚Äù creates the impression
that there are some remaining horses ‚Äînamely h2; : : : ; hn ‚Äîbesides h1 and hnC1.
However, this is not true when n D 1. In that case, h1; h2; : : : ; hn; hnC1 is just
h1; h2 and there are no ‚Äúremaining‚Äù horses for h1 to share a color with. And of
course in this case h1 and h2 really don‚Äôt need to be the same color.
This mistake knocks a critical link out of our induction argument. We proved
P.1/ and we correctly proved P.2/  ! P.3/, P.3/  ! P.4/, etc. But we failed
to prove P.1/  ! P.2/, and so everything falls apart: we cannot conclude that
P.2/, P.3/, etc., are true. And, naturally these propositions are all false; there are
sets of n horses of different colors for all n  2.
Students sometimes explain that the mistake in the proof is because P.n/ is
false for n  2, and the proof assumes something false, namely, P.n/, in order to
prove P.n C 1/. You should think about how to explain to such a student why this
explanation would get no credit on a Math for Computer Science exam.
5.2
Strong Induction
A useful variant of induction is called Strong Induction. Strong induction and ordi-
nary induction are used for exactly the same thing: proving that a predicate is true
for all nonnegative integers. Strong induction is useful when a simple proof that
the predicate holds for n C 1 does not follow just from the fact that it holds at n,
but from the fact that it holds for other values  n.

5.2. Strong Induction
109
5.2.1
A Rule for Strong Induction
Principle of Strong Induction.
Let P be a predicate on nonnegative integers. If
 P.0/ is true, and
 for all n 2 N, P.0/, P.1/, ..., P.n/ together imply P.n C 1/,
then P.m/ is true for all m 2 N.
The only change from the ordinary induction principle is that strong induction
allows you make more assumptions in the inductive step of your proof! In an
ordinary induction argument, you assume that P.n/ is true and try to prove that
P.n C 1/ is also true. In a strong induction argument, you may assume that P.0/,
P.1/, . . . , and P.n/ are all true when you go to prove P.nC1/. So you can assume
a stronger set of hypotheses which can make your job easier.
Formulated as a proof rule, strong induction is
Rule. Strong Induction Rule
P.0/;
8n 2 N:
 P.0/ AND P.1/ AND : : : AND P.n/

IMPLIES P.n C 1/
8m 2 N: P.m/
Stated more succintly, the rule is
Rule.
P.0/;
≈í8k  n 2 N: P.k/¬ç IMPLIES P.n C 1/
8m 2 N: P.m/
The template for strong induction proofs is identical to the template given in
Section 5.1.3 for ordinary induction except for two things:
 you should state that your proof is by strong induction, and
 you can assume that P.0/, P.1/, ..., P.n/ are all true instead of only P.n/
during the inductive step.
5.2.2
Products of Primes
As a Ô¨Årst example, we‚Äôll use strong induction to re-prove Theorem 2.3.1 which we
previously proved using Well Ordering.
Theorem. Every integer greater than 1 is a product of primes.

Chapter 5
Induction
110
Proof. We will prove the Theorem by strong induction, letting the induction hy-
pothesis, P.n/, be
n is a product of primes:
So the Theorem will follow if we prove that P.n/ holds for all n  2.
Base Case: (n D 2): P.2/ is true because 2 is prime, so it is a length one product
of primes by convention.
Inductive step: Suppose that n  2 and that every number from 2 to n is a product
of primes. We must show that P.n C 1/ holds, namely, that n C 1 is also a product
of primes. We argue by cases:
If n C 1 is itself prime, then it is a length one product of primes by convention,
and so P.n C 1/ holds in this case.
Otherwise, nC1 is not prime, which by deÔ¨Ånition means nC1 D k m for some
integers k; m between 2 and n. Now by the strong induction hypothesis, we know
that both k and m are products of primes. By multiplying these products, it follows
immediately that k  m D n C 1 is also a product of primes. Therefore, P.n C 1/
holds in this case as well.
So P.n C 1/ holds in any case, which completes the proof by strong induction
that P.n/ holds for all n  2.

5.2.3
Making Change
The country Inductia, whose unit of currency is the Strong, has coins worth 3Sg
(3 Strongs) and 5Sg. Although the Inductians have some trouble making small
change like 4Sg or 7Sg, it turns out that they can collect coins to make change for
any number that is at least 8 Strongs.
Strong induction makes this easy to prove for n C 1  11, because then .n C
1/   3  8, so by strong induction the Inductians can make change for exactly
.nC1/ 3 Strongs, and then they can add a 3Sg coin to get .nC1/Sg. So the only
thing to do is check that they can make change for all the amounts from 8 to 10Sg,
which is not too hard to do.
Here‚Äôs a detailed writeup using the ofÔ¨Åcial format:
Proof. We prove by strong induction that the Inductians can make change for any
amount of at least 8Sg. The induction hypothesis, P.n/ will be:
There is a collection of coins whose value is n C 8 Strongs.
We now proceed with the induction proof:
Base case: P.0/ is true because a 3Sg coin together with a 5Sg coin makes 8Sg.

5.2. Strong Induction
111
Inductive step: We assume P.k/ holds for all k  n, and prove that P.n C 1/
holds. We argue by cases:
Case (n C 1 = 1): We have to make .n C 1/ C 8 D 9Sg. We can do this using
three 3Sg coins.
Case (n C 1 = 2): We have to make .n C 1/ C 8 D 10Sg. Use two 5Sg coins.
Case (n C 1  3): Then 0  n   2  n, so by the strong induction hypothesis,
the Inductians can make change for n   2 Strong. Now by adding a 3Sg coin, they
can make change for .n C 1/Sg.
Since n  0, we know that n C 1  1 and thus that the three cases cover
every possibility. Since P.n C 1/ is true in every case, we can conclude by strong
induction that for all n  0, the Inductians can make change for nC8 Strong. That
is, they can make change for any number of eight or more Strong.

5.2.4
The Stacking Game
Here is another exciting game that‚Äôs surely about to sweep the nation!
You begin with a stack of n boxes. Then you make a sequence of moves. In each
move, you divide one stack of boxes into two nonempty stacks. The game ends
when you have n stacks, each containing a single box. You earn points for each
move; in particular, if you divide one stack of height a C b into two stacks with
heights a and b, then you score ab points for that move. Your overall score is the
sum of the points that you earn for each move. What strategy should you use to
maximize your total score?
As an example, suppose that we begin with a stack of n D 10 boxes. Then the
game might proceed as shown in Figure 5.5. Can you Ô¨Ånd a better strategy?
Analyzing the Game
Let‚Äôs use strong induction to analyze the unstacking game. We‚Äôll prove that your
score is determined entirely by the number of boxes ‚Äîyour strategy is irrelevant!
Theorem 5.2.1. Every way of unstacking n blocks gives a score of n.n   1/=2
points.
There are a couple technical points to notice in the proof:
 The template for a strong induction proof mirrors the one for ordinary induc-
tion.
 As with ordinary induction, we have some freedom to adjust indices. In this
case, we prove P.1/ in the base case and prove that P.1/; : : : ; P.n/ imply
P.n C 1/ for all n  1 in the inductive step.

Chapter 5
Induction
112
Stack Heights
Score
10
5
5
25 points
5
3
2
6
4
3
2
1
4
2
3
2
1
2
4
2
2
2
1
2
1
2
1
2
2
1
2
1
1
1
1
1
2
1
2
1
1
1
1
1
1
1
1
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
Total Score
D
45 points
Figure 5.5
An example of the stacking game with n D 10 boxes. On each line,
the underlined stack is divided in the next step.
Proof. The proof is by strong induction. Let P.n/ be the proposition that every
way of unstacking n blocks gives a score of n.n   1/=2.
Base case: If n D 1, then there is only one block. No moves are possible, and so
the total score for the game is 1.1   1/=2 D 0. Therefore, P.1/ is true.
Inductive step: Now we must show that P.1/, ..., P.n/ imply P.n C 1/ for all
n  1. So assume that P.1/, ..., P.n/ are all true and that we have a stack of
n C 1 blocks. The Ô¨Årst move must split this stack into substacks with positive sizes
a and b where a C b D n C 1 and 0 < a; b  n. Now the total score for the game
is the sum of points for this Ô¨Årst move plus points obtained by unstacking the two
resulting substacks:
total score D (score for 1st move)
C (score for unstacking a blocks)
C (score for unstacking b blocks)
D ab C a.a   1/
2
C b.b   1/
2
by P.a/ and P.b/
D .a C b/2   .a C b/
2
D .a C b/..a C b/   1/
2
D .n C 1/n
2
This shows that P.1/, P.2/, ..., P.n/ imply P.n C 1/.

5.3. Strong Induction vs. Induction vs. Well Ordering
113
Therefore, the claim is true by strong induction.

5.3
Strong Induction vs. Induction vs. Well Ordering
Strong induction looks genuinely ‚Äústronger‚Äù than ordinary induction ‚Äîafter all,
you can assume a lot more when proving the induction step. Since ordinary in-
duction is a special case of strong induction, you might wonder why anyone would
bother with the ordinary induction.
But strong induction really isn‚Äôt any stronger, because a simple text manipula-
tion program can automatically reformat any proof using strong induction into a
proof using ordinary induction ‚Äîjust by decorating the induction hypothesis with
a universal quantiÔ¨Åer in a standard way. Still, it‚Äôs worth distinguishing these two
kinds of induction, since which you use will signal whether the inductive step for
n C 1 follows directly from the case for n or requires cases smaller than n, and that
is generally good for your reader to know.
The template for the two kinds of induction rules looks nothing like the one for
the Well Ordering Principle, but this chapter included a couple of examples where
induction was used to prove something already proved using Well Ordering. In
fact, this can always be done. As the examples may suggest, any Well Ordering
proof can automatically be reformatted into an Induction proof. So theoretically,
no one need bother with the Well Ordering Principle either.
But wait a minute ‚Äîit‚Äôs equally easy to go the other way, and automatically
reformat any Strong Induction proof into a Well Ordering proof. The three proof
methods ‚ÄìWell Ordering, Induction, and Strong Induction, are simply different for-
mats for presenting the same mathematical reasoning!
So why three methods? Well, sometimes induction proofs are clearer because
they don‚Äôt require proof by contradiction. Also, induction proofs often provide
recursive procedures that reduce large inputs to smaller ones. On the other hand,
Well Ordering can come out slightly shorter and sometimes seem more natural ‚Äî
and less worrisome to beginners.
So which method should you use? There is no simple recipe. Sometimes the
only way to decide is to write up a proof using more than one method and compare
how they come out. But whichever method you choose, be sure to state the method
up front to help a reader follow your proof.

Chapter 5
Induction
114
0
1
2
99
overflow
start
state
Figure 5.6
State transitions for the 99-bounded counter.
5.4
State Machines
State machines are a simple abstract model of step-by-step processes. Since com-
puter programs can be understood as deÔ¨Åning step-by-step computational processes,
it‚Äôs not surprising that state machines come up regularly in computer science. They
also come up in many other settings such as designing digital circuits and mod-
eling probabilistic processes. This section introduces Floyd‚Äôs Invariant Principle
which is a version of induction tailored speciÔ¨Åcally for proving properties of state
machines.
One of the most important uses of induction in computer science involves prov-
ing one or more desirable properties continues to hold at every step in a process.
A property that is preserved through a series of operations or steps is known as an
invariant. Examples of desirable invariants include properties such as a variable
never exceeding a certain value, the altitude of a plane never dropping below 1,000
feet without the wingÔ¨Çaps being deployed, and the temperature of a nuclear reactor
never exceeding the threshold for a meltdown.
5.4.1
States and Transitions
Formally, a state machine is nothing more than a binary relation on a set, except
that the elements of the set are called ‚Äústates,‚Äù the relation is called the transition
relation, and an arrow in the graph of the transition relation is called a transition.
A transition from state q to state r will be written q  ! r. The transition relation
is also called the state graph of the machine. A state machine also comes equipped
with a designated start state.
A simple example is a bounded counter, which counts from 0 to 99 and overÔ¨Çows
at 100. This state machine is pictured in Figure 5.6, with states pictured as circles,
transitions by arrows, and with start state 0 indicated by the double circle. To be

5.4. State Machines
115
precise, what the picture tells us is that this bounded counter machine has
states WWD f0; 1; : : : ; 99; overÔ¨Çowg;
start state WWD 0;
transitions WWD fn  ! n C 1 j 0  n < 99g
[ f99  ! overÔ¨Çow; overÔ¨Çow  ! overÔ¨Çowg:
This machine isn‚Äôt much use once it overÔ¨Çows, since it has no way to get out of its
overÔ¨Çow state.
State machines for digital circuits and string pattern matching algorithms, for ex-
ample, usually have only a Ô¨Ånite number of states. Machines that model continuing
computations typically have an inÔ¨Ånite number of states. For example, instead of
the 99-bounded counter, we could easily deÔ¨Åne an ‚Äúunbounded‚Äù counter that just
keeps counting up without overÔ¨Çowing. The unbounded counter has an inÔ¨Ånite
state set, namely, the nonnegative integers, which makes its state diagram harder to
draw.
State machines are often deÔ¨Åned with labels on states and/or transitions to indi-
cate such things as input or output values, costs, capacities, or probabilities. Our
state machines don‚Äôt include any such labels because they aren‚Äôt needed for our
purposes. We do name states, as in Figure 5.6, so we can talk about them, but the
names aren‚Äôt part of the state machine.
5.4.2
Invariant for a Diagonally-Moving Robot
Suppose we have a robot that starts at the origin and moves on an inÔ¨Ånite 2-
dimensional integer grid. The state of the robot at any time can be speciÔ¨Åed by
the integer coordinates .x; y/ of the robot‚Äôs current position. So the start state
is .0; 0/. At each step, the robot may move to a diagonally adjacent grid point, as
illustrated in Figure 5.7.
To be precise, the robot‚Äôs transitions are:
f.m; n/  ! .m Àô 1; n Àô 1/ j m; n 2 Zg:
For example, after the Ô¨Årst step, the robot could be in states .1; 1/, .1;  1/, . 1; 1/,
or . 1;  1/. After two steps, there are 9 possible states for the robot, includ-
ing .0; 0/. The question is, can the robot ever reach position .1; 0/?
If you play around with the robot a bit, you‚Äôll probably notice that the robot can
only reach positions .m; n/ for which m C n is even, which means, of course, that
it can‚Äôt reach .1; 0/. This all follows because evenness of the sum of coordinates is
preserved by transitions.

Chapter 5
Induction
116
0
1
2
3
0
1
2
x
y
Figure 5.7
The Diagonally Moving Robot.
This once, let‚Äôs go through this preserved-property argument again carefully
highlighting where induction comes in. Namely, deÔ¨Åne the even-sum property of
states to be:
Even-sum..m; n// WWD ≈ím C n is even¬ç:
Lemma 5.4.1. For any transition, q  ! r, of the diagonally-moving robot, if
Even-sum(q), then Even-sum(r).
This lemma follows immediately from the deÔ¨Ånition of the robot‚Äôs transitions:
.m; n/  ! .m Àô 1; n Àô 1/. After a transition, the sum of coordinates changes by
.Àô1/ C .Àô1/, that is, by 0, 2, or -2. Of course, adding 0, 2 or -2 to an even number
gives an even number. So by a trivial induction on the number of transitions, we
can prove:
Theorem 5.4.2. The sum of the coordinates of any state reachable by the diagonally-
moving robot is even.
Proof. The proof is induction on the number of transitions the robot has made. The
induction hypothesis is
P.n/ WWD if q is a state reachable in n transitions, then Even-sum(q):

5.4. State Machines
117
0
1
2
3
0
1
2
x
y
goal
‚Äπ‚Äπ
Figure 5.8
Can the Robot get to .1; 0/?

Chapter 5
Induction
118
base case: P.0/ is true since the only state reachable in 0 transitions is the start
state .0; 0/, and 0 C 0 is even.
inductive step: Assume that P.n/ is true, and let r be any state reachable in n C 1
transitions. We need to prove that Even-sum(r) holds.
Since r is reachable in n C 1 transitions, there must be a state, q, reachable in n
transitions such that q  ! r. Since P.n/ is assumed to be true, Even-sum(q) holds,
and so by Lemma 5.4.1, Even-sum(r) also holds. This proves that P.n/ IMPLIES
P.n C 1/ as required, completing the proof of the inductive step.
We conclude by induction that for all n  0, if q is reachable in n transitions, then
Even-sum(q). This implies that every reachable state has the Even-sum property.

Corollary 5.4.3. The robot can never reach position .1; 0/.
Proof. By Theorem 5.4.2, we know the robot can only reach positions with coor-
dinates that sum to an even number, and thus it cannot reach position .1; 0/.

5.4.3
The Invariant Principle
Using the Even-sum invariant to understand the diagonally-moving robot is a sim-
ple example of a basic proof method called The Invariant Principle. The Principle
summarizes how induction on the number of steps to reach a state applies to invari-
ants.
A state machine execution describes a possible sequence of steps a machine
might take.
DeÔ¨Ånition 5.4.4. An execution of the state machine is a (possibly inÔ¨Ånite) sequence
of states with the property that
 it begins with the start state, and
 if q and r are consecutive states in the sequence, the q  ! r.
A state is called reachable if it appears in some execution.
DeÔ¨Ånition 5.4.5. A preserved invariant of a state machine is a predicate, P , on
states, such that whenever P.q/ is true of a state, q, and q  ! r for some state, r,
then P.r/ holds.
The Invariant Principle
If a preserved invariant of a state machine is true for the start state,
then it is true for all reachable states.

5.4. State Machines
119
The Invariant Principle is nothing more than the Induction Principle reformulated
in a convenient form for state machines. Showing that a predicate is true in the start
state is the base case of the induction, and showing that a predicate is a preserved
invariant corresponds to the inductive step.5
5Preserved invariants are commonly just called ‚Äúinvariants‚Äù in the literature on program correct-
ness, but we decided to throw in the extra adjective to avoid confusion with other deÔ¨Ånitions. For
example, other texts (as well as another subject at MIT) use ‚Äúinvariant‚Äù to mean ‚Äúpredicate true of
all reachable states.‚Äù Let‚Äôs call this deÔ¨Ånition ‚Äúinvariant-2.‚Äù Now invariant-2 seems like a reason-
able deÔ¨Ånition, since unreachable states by deÔ¨Ånition don‚Äôt matter, and all we want to show is that
a desired property is invariant-2. But this confuses the objective of demonstrating that a property is
invariant-2 with the method of Ô¨Ånding a preserved invariant to show that it is invariant-2.

Chapter 5
Induction
120
Robert W Floyd
The Invariant Principle was formulated by Robert W Floyd at Carnegie Tech in
1967. (The following year, Carnegie Tech was renamed Carnegie-Mellon Univ.)
Floyd was already famous for work on formal grammars that transformed the Ô¨Åeld
of programming language parsing; that was how he got to be a professor even
though he never got a Ph.D. (He was admitted to a PhD program as a teenage
prodigy, but Ô¨Çunked out and never went back.)
In that same year, Albert R Meyer was appointed Assistant Professor in the
Carnegie Tech Computer Science Department where he Ô¨Årst met Floyd. Floyd and
Meyer were the only theoreticians in the department, and they were both delighted
to talk about their shared interests. After just a few conversations, Floyd‚Äôs new
junior colleague decided that Floyd was the smartest person he had ever met.
Naturally, one of the Ô¨Årst things Floyd wanted to tell Meyer about was his new,
as yet unpublished, Invariant Principle. Floyd explained the result to Meyer, and
Meyer wondered (privately) how someone as brilliant as Floyd could be excited
by such a trivial observation. Floyd had to show Meyer a bunch of examples be-
fore Meyer understood Floyd‚Äôs excitement ‚Äînot at the truth of the utterly obvious
Invariant Principle, but rather at the insight that such a simple method could be so
widely and easily applied in verifying programs.
Floyd left for Stanford the following year. He won the Turing award ‚Äîthe
‚ÄúNobel prize‚Äù of computer science ‚Äîin the late 1970‚Äôs, in recognition both of his
work on grammars and on the foundations of program veriÔ¨Åcation. He remained
at Stanford from 1968 until his death in September, 2001. You can learn more
about Floyd‚Äôs life and work by reading the eulogy at
http://oldwww.acm.org/pubs/membernet/stories/Ô¨Çoyd.pdf
written by his closest colleague, Don Knuth.

5.4. State Machines
121
5.4.4
The Die Hard Example
The movie Die Hard 3: With a Vengeance includes an amusing example of a state
machine. The lead characters played by Samuel L. Jackson and Bruce Willis have
to disarm a bomb planted by the diabolical Simon Gruber:
Simon: On the fountain, there should be 2 jugs, do you see them? A 5-
gallon and a 3-gallon. Fill one of the jugs with exactly 4 gallons of water
and place it on the scale and the timer will stop. You must be precise;
one ounce more or less will result in detonation. If you‚Äôre still alive in 5
minutes, we‚Äôll speak.
Bruce: Wait, wait a second. I don‚Äôt get it. Do you get it?
Samuel: No.
Bruce: Get the jugs. Obviously, we can‚Äôt Ô¨Åll the 3-gallon jug with 4 gal-
lons of water.
Samuel: Obviously.
Bruce: All right. I know, here we go. We Ô¨Åll the 3-gallon jug exactly to
the top, right?
Samuel: Uh-huh.
Bruce: Okay, now we pour this 3 gallons into the 5-gallon jug, giving us
exactly 3 gallons in the 5-gallon jug, right?
Samuel: Right, then what?
Bruce: All right. We take the 3-gallon jug and Ô¨Åll it a third of the way...
Samuel: No! He said, ‚ÄúBe precise.‚Äù Exactly 4 gallons.
Bruce: Sh - -. Every cop within 50 miles is running his a - - off and I‚Äôm
out here playing kids games in the park.
Samuel: Hey, you want to focus on the problem at hand?
Fortunately, they Ô¨Ånd a solution in the nick of time. You can work out how.
The Die Hard 3 State Machine
The jug-Ô¨Ålling scenario can be modeled with a state machine that keeps track of
the amount, b, of water in the big jug, and the amount, l, in the little jug. With the
3 and 5 gallon water jugs, the states formally will be pairs, .b; l/ of real numbers

Chapter 5
Induction
122
such that 0  b  5; 0  l  3. (We can prove that the reachable values of b and
l will be nonnegative integers, but we won‚Äôt assume this.) The start state is .0; 0/,
since both jugs start empty.
Since the amount of water in the jug must be known exactly, we will only con-
sider moves in which a jug gets completely Ô¨Ålled or completely emptied. There are
several kinds of transitions:
1. Fill the little jug: .b; l/  ! .b; 3/ for l < 3.
2. Fill the big jug: .b; l/  ! .5; l/ for b < 5.
3. Empty the little jug: .b; l/  ! .b; 0/ for l > 0.
4. Empty the big jug: .b; l/  ! .0; l/ for b > 0.
5. Pour from the little jug into the big jug: for l > 0,
.b; l/  !
(
.b C l; 0/
if b C l  5,
.5; l   .5   b//
otherwise.
6. Pour from big jug into little jug: for b > 0,
.b; l/  !
(
.0; b C l/
if b C l  3,
.b   .3   l/; 3/
otherwise.
Note that in contrast to the 99-counter state machine, there is more than one pos-
sible transition out of states in the Die Hard machine. Machines like the 99-counter
with at most one transition out of each state are called deterministic. The Die Hard
machine is nondeterministic because some states have transitions to several differ-
ent states.
The Die Hard 3 bomb gets disarmed successfully because the state (4,3) is reach-
able.
Die Hard Once and For All
The Die Hard series is getting tired, so we propose a Ô¨Ånal Die Hard Once and For
All. Here Simon‚Äôs brother returns to avenge him, and he poses the same challenge,
but with the 5 gallon jug replaced by a 9 gallon one. The state machine has the
same speciÔ¨Åcation as in Die Hard 3, with all occurrences of ‚Äú5‚Äù replaced by ‚Äú9.‚Äù
Now reaching any state of the form .4; l/ is impossible. We prove this using the
Invariant Principle. Namely, we deÔ¨Åne the preserved invariant predicate, P..b; l//,
to be that b and l are nonnegative integer multiples of 3.

5.4. State Machines
123
To prove that P is a preserved invariant of Die-Hard-Once-and-For-All machine,
we assume P.q/ holds for some state q WWD .b; l/ and that q  ! r. We have to
show that P.r/ holds. The proof divides into cases, according to which transition
rule is used.
One case is a ‚ÄúÔ¨Åll the little jug‚Äù transition. This means r D .b; 3/. But P.q/
implies that b is an integer multiple of 3, and of course 3 is an integer multiple of
3, so P.r/ still holds.
Another case is a ‚Äúpour from big jug into little jug‚Äù transition. For the subcase
when there isn‚Äôt enough room in the little jug to hold all the water, namely, when
b C l > 3, we have r D .b   .3   l/; 3/. But P.q/ implies that b and l are integer
multiples of 3, which means b   .3   l/ is too, so in this case too, P.r/ holds.
We won‚Äôt bother to crank out the remaining cases, which can all be checked
just as easily. Now by the Invariant Principle, we conclude that every reachable
state satisiÔ¨Åes P . But since no state of the form .4; l/ satisiÔ¨Åes P , we have proved
rigorously that Bruce dies once and for all!
By the way, notice that the state (1,0), which satisÔ¨Åes NOT.P /, has a transition
to (0,0), which satisÔ¨Åes P . So the negation of a preserved invariant may not be a
preserved invariant.
5.4.5
Fast Exponentiation
Partial Correctness & Termination
Floyd distinguished two required properties to verify a program. The Ô¨Årst property
is called partial correctness; this is the property that the Ô¨Ånal results, if any, of the
process must satisfy system requirements.
You might suppose that if a result was only partially correct, then it might also
be partially incorrect, but that‚Äôs not what Floyd meant. The word ‚Äúpartial‚Äù comes
from viewing a process that might not terminate as computing a partial relation.
Partial correctness means that when there is a result, it is correct, but the process
might not always produce a result, perhaps because it gets stuck in a loop.
The second correctness property called termination is that the process does al-
ways produce some Ô¨Ånal value.
Partial correctness can commonly be proved using the Invariant Principle. Termi-
nation can commonly be proved using the Well Ordering Principle. We‚Äôll illustrate
this by verifying a Fast Exponentiation procedure.
Exponentiating
The most straightforward way to compute the bth power of a number, a, is to mul-
tiply a by itself b   1 times. There is another way to do it using considerably fewer

Chapter 5
Induction
124
multiplications called Fast Exponentiation. The register machine program below
deÔ¨Ånes the fast exponentiation algorithm. The letters x; y; z; r denote registers that
hold numbers. An assignment statement has the form ‚Äúz WD a‚Äù and has the effect
of setting the number in register z to be the number a.
A Fast Exponentiation Program
Given inputs a 2 R; b 2 N, initialize registers x; y; z to a; 1; b respectively, and
repeat the following sequence of steps until termination:
 if z D 0 return y and terminate
 r WD remainder.z; 2/
 z WD quotient.z; 2/
 if r D 1, then y WD xy
 x WD x2
We claim this program always terminates and leaves y D ab.
To begin, we‚Äôll model the behavior of the program with a state machine:
1. states WWD R  R  N,
2. start state WWD .a; 1; b/,
3. transitions are deÔ¨Åned by the rule
.x; y; z/  !
(
.x2; y; quotient.z; 2//
if z is nonzero and even;
.x2; xy; quotient.z; 2//
if z is nonzero and odd:
The preserved invariant, P..x; y; z//, will be
z 2 N AND yxz D ab:
(5.4)
To prove that P is preserved, assume P..x; y; z// holds and that .x; y; z/  !
.xt; yt; zt/. We must prove that P..xt; yt; zt// holds, that is,
zt 2 N AND ytxzt
t
D ab:
(5.5)
Since there is a transition from .x; y; z/, we have z ¬§ 0, and since z 2 N
by (5.4), we can consider just two cases:

5.4. State Machines
125
If z is even, then we have that xt D x2; yt D y; zt D z=2. Therefore, zt 2 N
and
ytxzt
t
D y.x2/z=2
D yx2z=2
D yxz
D ab
(by (5.4))
If z is odd, then we have that xt D x2; yt D xy; zt D .z   1/=2. Therefore,
zt 2 N and
ytxzt
t
D xy.x2/.z 1/=2
D yx1C2.z 1/=2
D yx1C.z 1/
D yxz
D ab
(by (5.4))
So in both cases, (5.5) holds, proving that P is a preserved invariant.
Now it‚Äôs easy to prove partial correctness, namely, if the Fast Exponentiation
program terminates, it does so with ab in register y. This works because obviously
1ab D ab, which means that the start state, .a; 1; b/, satisiÔ¨Åes P . By the Invariant
Principle, P holds for all reachable states. But the program only stops when z D 0,
so if a terminated state, .x; y; 0/ is reachable, then y D yx0 D ab as required.
Ok, it‚Äôs partially correct, but what‚Äôs fast about it? The answer is that the number
of multiplications it performs to compute ab is roughly the length of the binary
representation of b. That is, the Fast Exponentiation program uses roughly log2 b
multiplications compared to the naive approach of multiplying by a a total of b   1
times.
More precisely, it requires at most 2.dlog2 be C 1/ multiplications for the Fast
Exponentiation algorithm to compute ab for b > 1. The reason is that the number
in register z is initially b, and gets at least halved with each transition. So it can‚Äôt
be halved more than dlog2 beC1 times before hitting zero and causing the program
to terminate. Since each of the transitions involves at most two multiplications, the
total number of multiplications until z D 0 is at most 2.dlog2 be C 1/ for b > 0
(see Problem 5.32).
5.4.6
Derived Variables
The preceding termination proofs involved Ô¨Ånding a nonnegative integer-valued
measure to assign to states. We might call this measure the ‚Äúsize‚Äù of the state.

Chapter 5
Induction
126
We then showed that the size of a state decreased with every state transition. By
the Well Ordering Principle, the size can‚Äôt decrease indeÔ¨Ånitely, so when a mini-
mum size state is reached, there can‚Äôt be any transitions possible: the process has
terminated.
More generally, the technique of assigning values to states ‚Äînot necessarily non-
negative integers and not necessarily decreasing under transitions ‚Äîis often useful
in the analysis of algorithms. Potential functions play a similar role in physics. In
the context of computational processes, such value assignments for states are called
derived variables.
For example, for the Die Hard machines we could have introduced a derived
variable, f W states ! R, for the amount of water in both buckets, by setting
f ..a; b// WWD a C b. Similarly, in the robot problem, the position of the robot along
the x-axis would be given by the derived variable x-coord, where x-coord..i; j//WWD i.
There are a few standard properties of derived variables that are handy in ana-
lyzing state machines.
DeÔ¨Ånition 5.4.6. A derived variable f W states ! R is strictly decreasing iff
q  ! q0 IMPLIES f .q0/ < f .q/:
It is weakly decreasing iff
q  ! q0 IMPLIES f .q0/  f .q/:
Strictly increasing and weakly increasing derived variables are deÔ¨Åned simi-
larly.6
We conÔ¨Årmed termination of the Fast Exponentiation procedure by noticing that
the derived variable y was nonnegative integer-valued and strictly decreasing. We
can summarize this approach to proving termination as follows:
Theorem 5.4.7. If f is a strictly decreasing N-valued derived variable of a state
machine, then the length of any execution starting at state q is at most f .q/.
Of course we could prove Theorem 5.4.7 by induction on the value of f .q/, but
think about what it says: ‚ÄúIf you start counting down at some nonnegative integer
f .q/, then you can‚Äôt count down more than f .q/ times.‚Äù Put this way, it‚Äôs obvious.
Theorem 5.4.7 generalizes straightforwardly to derived variables taking values
in a well ordered set.
6Weakly increasing variables are often also called nondecreasing. We will avoid this terminology
to prevent confusion between nondecreasing variables and variables with the much weaker property
of not being a decreasing variable.

5.4. State Machines
127
Theorem 5.4.8. If there exists a strictly decreasing derived variable whose range
is a well ordered set, then every execution terminates.
Theorem 5.4.8 follows immediately from the observation that a set of numbers
is well ordered iff it has no inÔ¨Ånite decreasing sequences (Problem 2.12).
Note that the existence of a weakly decreasing derived variable does not guar-
antee that every execution terminates. That‚Äôs because an inÔ¨Ånite execution could
proceed through states in which a weakly decreasing variable remained constant.
A Southeast Jumping Robot
[Optional]
Here‚Äôs a contrived, simple example of proving termination based on a variable that is strictly
decreasing over a well ordered set. Let‚Äôs think about a robot positioned at an integer lattice-point in
the Northeast quadrant of the plane, that is, at .x; y/ 2 N2.
At every second when it is away from the origin, .0; 0/, the robot must make a move, which may
be
 a unit distance West when it is not at the boundary of the Northeast quadrant (that is, .x; y/  !
.x   1; y/ for x > 0), or
 a unit distance South combined with an arbitrary jump East (that is, .x; y/  ! .z; y   1/ for
z  x).
Claim 5.4.9. The robot will always get stuck at the origin.
If we think of the robot as a nondeterministic state machine, then Claim 5.4.9 is a termination
assertion. The Claim may seem obvious, but it really has a different character than termination based
on nonnegative integer-valued variables. That‚Äôs because, even knowing that the robot is at position
.0; 1/, for example, there is no way to bound the time it takes for the robot to get stuck. It can delay
getting stuck for as many seconds as it wants by making its next move to a distant point in the Far
East. This rules out proving termination using Theorem 5.4.7.
So does Claim 5.4.9 still seem obvious?
Well it is if you see the trick. DeÔ¨Åne a derived variable, v, mapping robot states to the numbers in
the well ordered set N C To1 of Lemma 2.4.5. In particular, deÔ¨Åne v W N2 ! N C To1 as follows
v.x; y/ WWD y C
x
x C 1:
Now it‚Äôs easy to check that if .x; y/  ! .x0; y0/ is a legitimate robot move, then v..x0; y0// <
v..x; y//. In particular, v is a strictly decreasing derived variable, so Theorem 5.4.8 implies that the
robot always get stuck ‚Äîeven though we can‚Äôt say how many moves it will take until it does.
Problems for Section 5.1
Practice Problems
Problem 5.1.
Prove by induction that every nonempty Ô¨Ånite set of real numbers has a minimum
element.

Chapter 5
Induction
128
Class Problems
Problem 5.2.
Use induction to prove that
13 C 23 C    C n3 D
n.n C 1/
2
2
:
(5.6)
for all n  1.
Remember to formally
1. Declare proof by induction.
2. Identify the induction hypothesis P.n/.
3. Establish the base case.
4. Prove that P.n/ ) P.n C 1/.
5. Conclude that P.n/ holds for all n  1.
as in the Ô¨Åve part template.
Problem 5.3.
Prove by induction on n that
1 C r C r2 C    C rn D rnC1   1
r   1
for all n 2 N and numbers r ¬§ 1.
Problem 5.4.
Prove by induction:
1 C 1
4 C 1
9 C    C 1
n2 < 2   1
n;
(5.7)
for all n > 1.
Problem 5.5. (a) Prove by induction that a 2n  2n courtyard with a 1  1 statue
of Bill in a corner can be covered with L-shaped tiles. (Do not assume or reprove
the (stronger) result of Theorem 5.1.2 that Bill can be placed anywhere. The point
of this problem is to show a different induction hypothesis that works.)

5.4. State Machines
129
(b) Use the result of part (a) to prove the original claim that there is a tiling with
Bill in the middle.
Problem 5.6.
We‚Äôve proved in two different ways that
1 C 2 C 3 C    C n D n.n C 1/
2
But now we‚Äôre going to prove a contradictory theorem!
False Theorem. For all n  0,
2 C 3 C 4 C    C n D n.n C 1/
2
Proof. We use induction. Let P.n/ be the proposition that 2 C 3 C 4 C    C n D
n.n C 1/=2.
Base case: P.0/ is true, since both sides of the equation are equal to zero. (Recall
that a sum with no terms is zero.)
Inductive step: Now we must show that P.n/ implies P.n C 1/ for all n  0. So
suppose that P.n/ is true; that is, 2 C 3 C 4 C    C n D n.n C 1/=2. Then we can
reason as follows:
2 C 3 C 4 C    C n C .n C 1/ D ≈í2 C 3 C 4 C    C n¬ç C .n C 1/
D n.n C 1/
2
C .n C 1/
D .n C 1/.n C 2/
2
Above, we group some terms, use the assumption P.n/, and then simplify. This
shows that P.n/ implies P.n C 1/. By the principle of induction, P.n/ is true for
all n 2 N.

Where exactly is the error in this proof?
Homework Problems
Problem 5.7.
The Fibonacci numbers F.0/; F.1/; F.2/; : : : are deÔ¨Åned as follows:
F.0/ WWD 0;
F.1/ WWD 1;
F.n/ WWD F.n   1/ C F.n   2/
for n  2:

Chapter 5
Induction
130
Thus, the Ô¨Årst few Fibonacci numbers are 0, 1, 1, 2, 3, 5, 8, 13, and 21. Prove by
induction that for all n  1,
F.n   1/  F.n C 1/   F.n/2 D . 1/n:
(5.8)
Problem 5.8.
For any binary string, Àõ, let num .Àõ/ be the nonnegative integer it represents in
binary notation. For example, num .10/ D 2, and num .0101/ D 5.
An n C 1-bit adder adds two n C 1-bit binary numbers. More precisely, an
n C 1-bit adder takes two length n C 1 binary strings
Àõn WWD an : : : a1a0;
Àán WWD bn : : : b1b0;
and a binary digit, c0, as inputs, and produces a length n C 1 binary string
n WWD sn : : : s1s0;
and a binary digit, cnC1, as outputs, and satisÔ¨Åes the speciÔ¨Åcation:
num .Àõn/ C num .Àán/ C c0 D 2nC1cnC1 C num .n/ :
(5.9)
There is a straighforward way to implement an nC1-bit adder as a digital circuit:
an n C 1-bit ripple-carry circuit has 1 C 2.n C 1/ binary inputs
an; : : : ; a1; a0; bn; : : : ; b1; b0; c0;
and n C 2 binary outputs,
cnC1; sn; : : : ; s1; s0:
As in Problem 3.5, the ripple-carry circuit is speciÔ¨Åed by the following formulas:
si WWD ai XOR bi XOR ci
(5.10)
ciC1 WWD .ai AND bi/ OR .ai AND ci/ OR .bi AND ci/; :
(5.11)
for 0  i  n.
(a) Verify that deÔ¨Ånitions (5.10) and (5.11) imply that
an C bn C cn D 2cnC1 C sn:
(5.12)
for all n 2 N.

5.4. State Machines
131
(b) Prove by induction on n that an nC1-bit ripple-carry circuit really is an nC1-
bit adder, that is, its outputs satisfy (5.9).
Hint: You may assume that, by deÔ¨Ånition of binary representation of integers,
num .ÀõnC1/ D anC12nC1 C num .Àõn/ :
(5.13)
Problem 5.9.
The Math for Computer Science mascot, Theory Hippotamus, made a startling
discovery while playing with his prized collection of unit squares over the weekend.
Here is what happened.
First, Theory Hippotamus put his favorite unit square down on the Ô¨Çoor as in
Figure 5.9 (a). He noted that the length of the periphery of the resulting shape was
4, an even number. Next, he put a second unit square down next to the Ô¨Årst so that
the two squares shared an edge as in Figure 5.9 (b). He noticed that the length
of the periphery of the resulting shape was now 6, which is also an even number.
(The periphery of each shape in the Ô¨Ågure is indicated by a thicker line.) Theory
Hippotamus continued to place squares so that each new square shared an edge
with at least one previously-placed square and no squares overlapped. Eventually,
he arrived at the shape in Figure 5.9 (c). He realized that the length of the periphery
of this shape was 36, which is again an even number.
Our plucky porcine pal is perplexed by this peculiar pattern. Use induction on
the number of squares to prove that the length of the periphery is always even, no
matter how many squares Theory Hippotamus places or how he arranges them.
Exam Problems
Problem 5.10.
Suppose P.n/ is a predicate on natural numbers and suppose
8k: P.k/ IMPLIES P.k C 2/:
(5.14)
For P ‚Äôs that satisfy (5.14), some of the assertions below Can hold for some,
but not all, such P , other assertions Always hold no matter what the P may be,
and some Never hold for any such P . Indicate which case applies for each of the
assertions and brieÔ¨Çy explain why.
(a) 8n  0: P.n/
(b) NOT.P.0// AND 8n  1: P.n/
(c) 8n  0: NOT.P.n//

Chapter 5
Induction
132
(a)
(b)
(c)
Figure 5.9
Some shapes that Theory Hippotamus created.
(d) .8n  100: P.n// AND .8n > 100: NOT.P.n///
(e) .8n  100: NOT.P.n/// AND .8n > 100: P.n//
(f) P.0/ IMPLIES 8n: P.n C 2/
(g) ≈í9n: P.2n/¬ç IMPLIES 8n: P.2n C 2/
(h) P.1/ IMPLIES 8n: P.2n C 1/
(i) ≈í9n: P.2n/¬ç IMPLIES 8n: P.2n C 2/
(j) 9n: 9m > n: ≈íP.2n/ AND NOT.P.2m//¬ç
(k) ≈í9n: P.n/¬ç IMPLIES 8n: 9m > n: P.m/
(l) NOT.P.0// IMPLIES 8n: NOT.P.2n//
Problem 5.11.

5.4. State Machines
133
Consider the following sequence of predicates:
Q1.x1/
WWD x1
Q2.x1; x2/
WWD x1 IMPLIES x2
Q3.x1; x2; x3/
WWD .x1 IMPLIES x2/ IMPLIES x3
Q4.x1; x2; x3; x4/
WWD ..x1 IMPLIES x2/ IMPLIES x3/ IMPLIES x4
Q5.x1; x2; x3; x4; x5/
WWD ...x1 IMPLIES x2/ IMPLIES x3/ IMPLIES x4/ IMPLIES x5
: : :
: : :
Let Tn be the number of different true/false settings of the variables x1; x2; : : : ; xn
for which Qn.x1; x2; : : : ; xn/ is true. For example, T2 D 3 since Q2.x1; x2/ is
true for 3 different settings of the variables x1 and x2:
x1
x2
Q2.x1; x2/
T
T
T
T
F
F
F
T
T
F
F
T
(a) Express TnC1 in terms of Tn, assuming n  1.
(b) Use induction to prove that Tn D 1
3.2nC1 C . 1/n/ for n  1. You may
assume your answer to the previous part without proof.
Problems for Section 5.2
Practice Problems
Problem 5.12.
Some fundamental principles for reasoning about nonnegative integers are:
1. The Induction Principle,
2. The Strong Induction Principle,
3. The Well-ordering Principle.
Identify which, if any, of the above principles is captured by each of the following
inference rules.
(a)
P.0/; 8m: .8k  m: P.k// IMPLIES P.m C 1/
8n: P.n/

Chapter 5
Induction
134
(b)
P.b/; 8k  b: P.k/ IMPLIES P.k C 1/
8k  b: P.k/
(c)
9n: P.n/
9m: ≈íP.m/ AND .8k: P.k/ IMPLIES k  m/¬ç
(d)
P.0/; 8k > 0: P.k/ IMPLIES P.k C 1/
8n: P.n/
(e)
8m: .8k < m: P.k// IMPLIES P.m/
8n: P.n/
Problem 5.13.
The nth Fibonacci number, F.n/, is deÔ¨Åned as follows
F.0/ WWD 0;
F.1/ WWD 1;
F.n/ WWD F.n   1/ C F.n   2/
for n  2:
Which sentences in the proof below contain logical errors?
False Claim. Every Fibonacci number is even.
False proof.
1. We use strong induction.
2. The induction hypothesis is that F.n/ is even.
3. We will Ô¨Årst show that this hypothesis holds for n D 0.
4. This is true, since F.0/ D 0, which is an even number.
5. Now, suppose n  2. We will show that F.n/ is even, assuming that F.k/ is
even for all k < n.
6. By assumption, both F.n   1/ and F.n   2/ are even.
7. Therefore, F.n/ is even, since F.n/ D F.n   1/ C F.n   2/ and the sum of
two even numbers is even.

5.4. State Machines
135
8. Thus, the strong induction principle implies that F.n/ is even for all n > 0.

Problem 5.14.
The nth Fibonacci number, F.n/, is deÔ¨Åned as follows
F.0/ WWD 0;
(5.15)
F.1/ WWD 1;
(5.16)
F.n/ WWD F.n   1/ C F.n   2/
for n > 1:
(5.17)
Indicate exactly which sentence(s) in the following bogus proof contain logical
errors? Explain.
False Claim. Every Fibonacci number is even.
Bogus proof. Let all the variables n; m; k mentioned below be nonnegative integer
valued. Let Even.n/ mean that F.n/ is even. The proof is by strong induction with
induction hypothesis Even.n/.
base case: F.0/ D 0 is an even number, so Even.0/ is true.
inductive step: We assume may assume the strong induction hypothesis
Even.k/ for 0  k  n;
and we must prove Even.n C 1/.
Then by strong induction hypothesis, Even.n/ and Even.n   1/ are true, that is,
F.n/ and F.n   1/ are both even. But by the deÔ¨Åning equation (5.17), F.n C 1/
equals the sum, F.n/CF.n 1/, of two even numbers, and so it is also even. This
proves Even.n C 1/ as required.
Hence, F.m/ is even for all m 2 N by the Strong Induction Principle.

Problem 5.15.
Alice wants to prove by induction that a predicate, P , holds for certain nonnegative
integers. She has proven that for all nonnegative integers n D 0; 1; : : :
P.n/ IMPLIES P.n C 3/:
(a) Suppose Alice also proves that P.5/ holds. Which of the following proposi-
tions can she infer?

Chapter 5
Induction
136
1. P.n/ holds for all n  5
2. P.3n/ holds for all n  5
3. P.n/ holds for n D 8; 11; 14; : : :
4. P.n/ does not hold for n < 5
5. 8n: P.3n C 5/
6. 8n > 2: P.3n   1/
7. P.0/ IMPLIES 8n: P.3n C 2/
8. P.0/ IMPLIES 8n: P.3n/
(b) Which of the following could Alice prove in order to conclude that P.n/ holds
for all n  5?
1. P.0/
2. P.5/
3. P.5/ and P.6/
4. P.0/, P.1/, and P.2/
5. P.5/, P.6/, and P.7/
6. P.2/, P.4/, and P.5/
7. P.2/, P.4/, and P.6/
8. P.3/, P.5/, and P.7/
Class Problems
Problem 5.16.
The Fibonacci numbers F0; F1; F2; : : : are deÔ¨Åned as follows:
Fn WWD
8
ÀÜ<
ÀÜ:
0
if n D 0;
1
if n D 1;
Fn 1 C Fn 2
if n > 1:
Prove, using strong induction, the following closed-form formula for Fn.7
Fn D pn   qn
p
5
7This mind-boggling formula is known as Binet‚Äôs formula. We‚Äôll explain in Chapter 15 and again
in Chapter 20 where it comes from in the Ô¨Årst place.

5.4. State Machines
137
where p D 1C
p
5
2
and q D 1 p
5
2
.
Hint: Note that p and q are the roots of x2   x   1 D 0, and so p2 D p C 1 and
q2 D q C 1.
Problem 5.17.
A sequence of numbers is weakly decreasing when each number in the sequence is
 the numbers after it. (This implies that a sequence of just one number is weakly
decreasing.)
Here‚Äôs a bogus proof of a very important true fact, every integer greater than 1 is
a product of a unique weakly decreasing sequence of primes ‚Äîa pusp, for short.
Explain what‚Äôs bogus about the proof.
Lemma. Every integer greater than 1 is a pusp.
For example, 252 D 7  3  3  2  2, and no other weakly decreasing sequence of
primes will have a product equal to 252.
Bogus proof. We will prove the lemma by strong induction, letting the induction
hypothesis, P.n/, be
n is a pusp:
So the lemma will follow if we prove that P.n/ holds for all n  2.
Base Case (n D 2): P.2/ is true because 2 is prime, and so it is a length one
product of primes, and this is obviously the only sequence of primes whose product
can equal 2.
Inductive step: Suppose that n  2 and that i is a pusp for every integer i where
2  i < n C 1. We must show that P.n C 1/ holds, namely, that n C 1 is also a
pusp. We argue by cases:
If n C 1 is itself prime, then it is the product of a length one sequence consisting
of itself. This sequence is unique, since by deÔ¨Ånition of prime, n C 1 has no other
prime factors. So n C 1 is a pusp, that is P.n C 1/ holds in this case.
Otherwise, n C 1 is not prime, which by deÔ¨Ånition means n C 1 D km for some
integers k; m such that 2  k; m < n C 1. Now by the strong induction hypothesis,
we know that k and m are pusps. It follows immediately that by merging the unique
prime sequences for k and m, in sorted order, we get a unique weakly decreasing
sequence of primes whose product equals n C 1. So n C 1 is a pusp, in this case as
well.
So P.n C 1/ holds in any case, which completes the proof by strong induction
that P.n/ holds for all n  2.


Chapter 5
Induction
138
Problem 5.18.
DeÔ¨Åne the potential, p.S/, of a stack of blocks, S, to be k.k   1/=2 where k is the
number of blocks in S. DeÔ¨Åne the potential, p.A/, of a set of stacks, A, to be the
sum of the potentials of the stacks in A.
Generalize Theorem 5.2.1 about scores in the stacking game to show that for any
set of stacks, A, if a sequence of moves starting with A leads to another set of stacks,
B, then p.A/  p.B/, and the score for this sequence of moves is p.A/   p.B/.
Hint: Try induction on the number of moves to get from A to B.
Homework Problems
Problem 5.19.
A group of n  1 people can be divided into teams, each containing either 4 or
7 people. What are all the possible values of n? Use induction to prove that your
answer is correct.
Problem 5.20.
The following Lemma is true, but the proof given for it below is defective. Pin-
point exactly where the proof Ô¨Årst makes an unjustiÔ¨Åed step and explain why it is
unjustiÔ¨Åed.
Lemma. For any prime p and positive integers n; x1; x2; : : : ; xn, if p j x1x2 : : : xn,
then p j xi for some 1  i  n.
Bogus proof. Proof by strong induction on n. The induction hypothesis, P.n/, is
that Lemma holds for n.
Base case n D 1: When n D 1, we have p j x1, therefore we can let i D 1 and
conclude p j xi.
Induction step: Now assuming the claim holds for all k  n, we must prove it
for n C 1.
So suppose p j x1x2    xnC1. Let yn D xnxnC1, so x1x2    xnC1 D x1x2    xn 1yn.
Since the righthand side of this equality is a product of n terms, we have by induc-
tion that p divides one of them. If p j xi for some i < n, then we have the desired
i. Otherwise p j yn. But since yn is a product of the two terms xn; xnC1, we have
by strong induction that p divides one of them. So in this case p j xi for i D n or
i D n C 1.

Exam Problems
Problem 5.21.
Use strong induction to prove that n  3n=3 for every integer n  0.

5.4. State Machines
139
Problem 5.22.
The Fibonacci numbers F0; F1; F2; : : : are deÔ¨Åned as follows:
Fn WWD
8
ÀÜ<
ÀÜ:
0
if n D 0;
1
if n D 1;
Fn 1 C Fn 2
if n > 1:
These numbers satisfy many unexpected identities, such as
F 2
0 C F 2
1 C    C F 2
n D FnFnC1
(5.18)
Equation (5.18) can be proved to hold for all n 2 N by induction, using the equation
itself as the induction hypothesis, P.n/.
(a) Prove the
base case .n D 0/.
(b) Now prove the
inductive step.
Problem 5.23.
Let S.n/ mean that exactly n cents of postage can be paid using only 4 and 7 cent
stamps. USe strong induction to prove that
8n: n  18 IMPLIES S.n/:
Problem 5.24.
Any amount of ten or more cents postage that is a multiple of Ô¨Åve can be made
using only 10¬¢ and 15¬¢ stamps. Prove this by induction (ordinary or strong, but say
which) using the induction hypothesis
S.n/ WWD .5n C 10/¬¢ postage can be made using only 10¬¢ and 15¬¢ stamps:
Problems for Section 5.4
Practice Problems
Problem 5.25.
Which states of the Die Hard 3 machine below have transitions to exactly two
states?

Chapter 5
Induction
140
Die Hard Transitions
1. Fill the little jug: .b; l/  ! .b; 3/ for l < 3.
2. Fill the big jug: .b; l/  ! .5; l/ for b < 5.
3. Empty the little jug: .b; l/  ! .b; 0/ for l > 0.
4. Empty the big jug: .b; l/  ! .0; l/ for b > 0.
5. Pour from the little jug into the big jug: for l > 0,
.b; l/  !
(
.b C l; 0/
if b C l  5,
.5; l   .5   b//
otherwise.
6. Pour from big jug into little jug: for b > 0,
.b; l/  !
(
.0; b C l/
if b C l  3,
.b   .3   l/; 3/
otherwise.
Problem 5.26.
Prove that every amount of postage of 12 cents or more can be formed using just
4-cent and 5-cent stamps.
Homework Problems
Problem 5.27.
Here is a game you can analyze with number theory and always beat me. We start
with two distinct, positive integers written on a blackboard. Call them a and b.
Now we take turns. (I‚Äôll let you decide who goes Ô¨Årst.) On each turn, the player
must write a new positive integer on the board that is the difference of two numbers
that are already there. If a player cannot play, then they lose.
For example, suppose that 12 and 15 are on the board initially. Your Ô¨Årst play
must be 3, which is 15   12. Then I might play 9, which is 12   3. Then you might
play 6, which is 15   9. Then I can‚Äôt play, so I lose.
(a) Show that every number on the board at the end of the game is a multiple of
gcd.a; b/.
(b) Show that every positive multiple of gcd.a; b/ up to max.a; b/ is on the board
at the end of the game.

5.4. State Machines
141
(c) Describe a strategy that lets you win this game every time.
Problem 5.28.
In the late 1960s, the military junta that ousted the government of the small re-
public of Nerdia completely outlawed built-in multiplication operations, and also
forbade division by any number other than 3. Fortunately, a young dissident found
a way to help the population multiply any two nonnegative integers without risking
persecution by the junta. The procedure he taught people is:
procedure multiply.x; y: nonnegative integers/
r WD x;
s WD y;
a WD 0;
while s ¬§ 0 do
if 3 j s then
r WD r C r C r;
s WD s=3;
else if 3 j .s   1/ then
a WD a C r;
r WD r C r C r;
s WD .s   1/=3;
else
a WD a C r C r;
r WD r C r C r;
s WD .s   2/=3;
return a;
We can model the algorithm as a state machine whose states are triples of non-
negative integers .r; s; a/. The initial state is .x; y; 0/. The transitions are given by
the rule that for s > 0:
.r; s; a/ !
8
ÀÜ<
ÀÜ:
.3r; s=3; a/
if 3 j s
.3r; .s   1/=3; a C r/
if 3 j .s   1/
.3r; .s   2/=3; a C 2r/
otherwise:
(a) List the sequence of steps that appears in the execution of the algorithm for
inputs x D 5 and y D 10.
(b) Use the Invariant Method to prove that the algorithm is partially correct‚Äîthat
is, if s D 0, then a D xy.

Chapter 5
Induction
142
(c) Prove that the algorithm terminates after at most 1 C log3 y executions of the
body of the do statement.
Problem 5.29.
A robot named Wall-E wanders around a two-dimensional grid. He starts out at
.0; 0/ and is allowed to take four different types of step:
1. .C2;  1/
2. .C1;  2/
3. .C1; C1/
4. . 3; 0/
Thus, for example, Wall-E might walk as follows. The types of his steps are
listed above the arrows.
.0; 0/
1! .2;  1/
3! .3; 0/
2! .4;  2/
4! .1;  2/ ! : : :
Wall-E‚Äôs true love, the fashionable and high-powered robot, Eve, awaits at .0; 2/.
(a) Describe a state machine model of this problem.
(b) Will Wall-E ever Ô¨Ånd his true love? Either Ô¨Ånd a path from Wall-E to Eve or
use the Invariant Principle to prove that no such path exists.
Problem 5.30.
A hungry ant is placed on an unbounded grid. Each square of the grid either con-
tains a crumb or is empty. The squares containing crumbs form a path in which,
except at the ends, every crumb is adjacent to exactly two other crumbs. The ant is
placed at one end of the path and on a square containing a crumb. For example, the
Ô¨Ågure below shows a situation in which the ant faces North, and there is a trail of
food leading approximately Southeast. The ant has already eaten the crumb upon
which it was initially placed.
The ant can only smell food directly in front of it. The ant can only remember
a small number of things, and what it remembers after any move only depends on
what it remembered and smelled immediately before the move. Based on smell and
memory, the ant may choose to move forward one square, or it may turn right or
left. It eats a crumb when it lands on it.

5.4. State Machines
143
The above scenario can be nicely modelled as a state machine in which each state
is a pair consisting of the ‚Äúant‚Äôs memory‚Äù and ‚Äúeverything else‚Äù‚Äîfor example,
information about where things are on the grid. Work out the details of such a
model state machine; design the ant-memory part of the state machine so the ant
will eat all the crumbs on any Ô¨Ånite path at which it starts and then signal when it
is done. Be sure to clearly describe the possible states, transitions, and inputs and
outputs (if any) in your model. BrieÔ¨Çy explain why your ant will eat all the crumbs.
Note that the last transition is a self-loop; the ant signals done for eternity. One
could also add another end state so that the ant signals done only once.
Problem 5.31.
Suppose that you have a regular deck of cards arranged as follows, from top to
bottom:
A~ 2~ : : : K~ A 2 : : : K A| 2| : : : K| A} 2} : : : K}
Only two operations on the deck are allowed: inshufÔ¨Çing and outshufÔ¨Çing. In
both, you begin by cutting the deck exactly in half, taking the top half into your
right hand and the bottom into your left. Then you shufÔ¨Çe the two halves together
so that the cards are perfectly interlaced; that is, the shufÔ¨Çed deck consists of one
card from the left, one from the right, one from the left, one from the right, etc. The
top card in the shufÔ¨Çed deck comes from the right hand in an outshufÔ¨Çe and from
the left hand in an inshufÔ¨Çe.
(a) Model this problem as a state machine.
(b) Use the Invariant Principle to prove that you cannot make the entire Ô¨Årst half
of the deck black through a sequence of inshufÔ¨Çes and outshufÔ¨Çes.

Chapter 5
Induction
144
Note: Discovering a suitable invariant can be difÔ¨Åcult! The standard approach is
to identify a bunch of reachable states and then look for a pattern, some feature that
they all share.
Problem 5.32.
Prove that the fast exponentiation state machine of Section 5.4.5 will halt after
dlog2 ne C 1
(5.19)
transitions starting from any state where the value of z is n 2 ZC.
Hint: Strong induction.
Class Problems
Problem 5.33.
In this problem you will establish a basic property of a puzzle toy called the Fifteen
Puzzle using the method of invariants. The Fifteen Puzzle consists of sliding square
tiles numbered 1; : : : ; 15 held in a 4  4 frame with one empty square. Any tile
adjacent to the empty square can slide into it.
The standard initial position is
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
We would like to reach the target position (known in the oldest author‚Äôs youth as
‚Äúthe impossible‚Äù):
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
A state machine model of the puzzle has states consisting of a 4  4 matrix with
16 entries consisting of the integers 1; : : : ; 15 as well as one ‚Äúempty‚Äù entry‚Äîlike
each of the two arrays above.
The state transitions correspond to exchanging the empty square and an adjacent
numbered tile. For example, an empty at position .2; 2/ can exchange position with

5.4. State Machines
145
tile above it, namely, at position .1; 2/:
n1
n2
n3
n4
n5
n6
n7
n8
n9
n10
n11
n12
n13
n14
n15
 !
n1
n3
n4
n5
n2
n6
n7
n8
n9
n10
n11
n12
n13
n14
n15
We will use the invariant method to prove that there is no way to reach the target
state starting from the initial state.
We begin by noting that a state can also be represented as a pair consisting of
two things:
1. a list of the numbers 1; : : : ; 15 in the order in which they appear‚Äîreading
rows left-to-right from the top row down, ignoring the empty square, and
2. the coordinates of the empty square‚Äîwhere the upper left square has coor-
dinates .1; 1/, the lower right .4; 4/.
(a) Write out the ‚Äúlist‚Äù representation of the start state and the ‚Äúimpossible‚Äù state.
Let L be a list of the numbers 1; : : : ; 15 in some order. A pair of integers is
an out-of-order pair in L when the Ô¨Årst element of the pair both comes earlier in
the list and is larger, than the second element of the pair. For example, the list
1; 2; 4; 5; 3 has two out-of-order pairs: (4,3) and (5,3). The increasing list 1; 2 : : : n
has no out-of-order pairs.
Let a state, S, be a pair .L; .i; j// described above. We deÔ¨Åne the parity of S to
be 0 or 1 depending on whether the sum of the number of out-of-order pairs in L
and the row-number of the empty square is even or odd. that is
parity.S/ WWD
(
0
.if p.L/ C i is even;
1
otherwise:
(b) Verify that the parity of the start state and the target state are different.
(c) Show that the parity of a state is preserved under transitions. Conclude that
‚Äúthe impossible‚Äù is impossible to reach.
By the way, if two states have the same parity, then in fact there is a way to get
from one to the other. If you like puzzles, you‚Äôll enjoy working this out on your
own.
Problem 5.34.
The Massachusetts Turnpike Authority is concerned about the integrity of the new

Chapter 5
Induction
146
Zakim bridge. Their consulting architect has warned that the bridge may collapse
if more than 1000 cars are on it at the same time. The Authority has also been
warned by their trafÔ¨Åc consultants that the rate of accidents from cars speeding
across bridges has been increasing.
Both to lighten trafÔ¨Åc and to discourage speeding, the Authority has decided to
make the bridge one-way and to put tolls at both ends of the bridge (don‚Äôt laugh, this
is Massachusetts). So cars will pay tolls both on entering and exiting the bridge, but
the tolls will be different. In particular, a car will pay $3 to enter onto the bridge and
will pay $2 to exit. To be sure that there are never too many cars on the bridge, the
Authority will let a car onto the bridge only if the difference between the amount
of money currently at the entry toll booth and the amount at the exit toll booth is
strictly less than a certain threshold amount of $T0.
The consultants have decided to model this scenario with a state machine whose
states are triples of nonnegative integers, .A; B; C/, where
 A is an amount of money at the entry booth,
 B is an amount of money at the exit booth, and
 C is a number of cars on the bridge.
Any state with C > 1000 is called a collapsed state, which the Authority dearly
hopes to avoid. There will be no transition out of a collapsed state.
Since the toll booth collectors may need to start off with some amount of money
in order to make change, and there may also be some number of ‚ÄúofÔ¨Åcial‚Äù cars
already on the bridge when it is opened to the public, the consultants must be ready
to analyze the system started at any uncollapsed state. So let A0 be the initial
number of dollars at the entrance toll booth, B0 the initial number of dollars at the
exit toll booth, and C0  1000 the number of ofÔ¨Åcial cars on the bridge when it is
opened. You should assume that even ofÔ¨Åcial cars pay tolls on exiting or entering
the bridge after the bridge is opened.
(a) Give a mathematical model of the Authority‚Äôs system for letting cars on and off
the bridge by specifying a transition relation between states of the form .A; B; C/
above.
(b) Characterize each of the following derived variables
A; B; A C B; A   B; 3C   A; 2A   3B; B C 3C; 2A   3B   6C; 2A   2B   3C
as one of the following

5.4. State Machines
147
constant
C
strictly increasing
SI
strictly decreasing
SD
weakly increasing but not constant
WI
weakly decreasing but not constant
WD
none of the above
N
and brieÔ¨Çy explain your reasoning.
The Authority has asked their engineering consultants to determine T and to
verify that this policy will keep the number of cars from exceeding 1000.
The consultants reason that if C0 is the number of ofÔ¨Åcial cars on the bridge
when it is opened, then an additional 1000   C0 cars can be allowed on the bridge.
So as long as A   B has not increased by 3.1000   C0/, there shouldn‚Äôt more than
1000 cars on the bridge. So they recommend deÔ¨Åning
T0 WWD 3.1000   C0/ C .A0   B0/;
(5.20)
where A0 is the initial number of dollars at the entrance toll booth, B0 is the initial
number of dollars at the exit toll booth.
(c) Use the results of part (b) to deÔ¨Åne a simple predicate, P , on states of the
transition system which is satisÔ¨Åed by the start state ‚Äîthat is P.A0; B0; C0/ holds
‚Äîis not satisÔ¨Åed by any collapsed state, and is a preserved invariant of the system.
Explain why your P has these properties. Conclude that the trafÔ¨Åc won‚Äôt cause the
bridge to collapse.
(d) A clever MIT intern working for the Turnpike Authority agrees that the Turn-
pike‚Äôs bridge management policy will be safe: the bridge will not collapse. But she
warns her boss that the policy will lead to deadlock‚Äîa situation where trafÔ¨Åc can‚Äôt
move on the bridge even though the bridge has not collapsed.
Explain more precisely in terms of system transitions what the intern means, and
brieÔ¨Çy, but clearly, justify her claim.
Problem 5.35.
Start with 102 coins on a table, 98 showing heads and 4 showing tails. There are
two ways to change the coins:
(i) Ô¨Çip over any ten coins, or
(ii) let n be the number of heads showing. Place n C 1 additional coins, all
showing tails, on the table.

Chapter 5
Induction
148
For example, you might begin by Ô¨Çipping nine heads and one tail, yielding 90
heads and 12 tails, then add 91 tails, yielding 90 heads and 103 tails.
(a) Model this situation as a state machine, carefully deÔ¨Åning the set of states, the
start state, and the possible state transitions.
(b) Explain how to reach a state with exactly one tail showing.
(c) DeÔ¨Åne the following derived variables:
C
WWD
the number of coins on the table;
H
WWD
the number of heads;
T
WWD
the number of tails;
C2
WWD
remainder.C=2/;
H2
WWD
remainder.H=2/;
T2
WWD
remainder.T=2/:
Which of these variables is
1. strictly increasing
2. weakly increasing
3. strictly decreasing
4. weakly decreasing
5. constant
(d) Prove that it is not possible to reach a state in which there is exactly one head
showing.
Problem 5.36.
A classroom is designed so students sit in a square arrangement. An outbreak of
beaver Ô¨Çu sometimes infects students in the class; beaver Ô¨Çu is a rare variant of bird
Ô¨Çu that lasts forever, with symptoms including a yearning for more quizzes and the
thrill of late night problem set sessions.
Here is an illustration of a 66-seat classroom with seats represented by squares.
The locations of infected students are marked with an asterisk.








Outbreaks of infection spread rapidly step by step. A student is infected after a
step if either

5.4. State Machines
149
 the student was infected at the previous step (since beaver Ô¨Çu lasts forever),
or
 the student was adjacent to at least two already-infected students at the pre-
vious step.
Here adjacent means the students‚Äô individual squares share an edge (front, back,
left or right); they are not adjacent if they only share a corner point. So each student
is adjacent to 2, 3 or 4 others.
In the example, the infection spreads as shown below.








)
















)






















In this example, over the next few time-steps, all the students in class become
infected.
Theorem. If fewer than n students among those in an nn arrangment are initially
infected in a Ô¨Çu outbreak, then there will be at least one student who never gets
infected in this outbreak, even if students attend all the lectures.
Prove this theorem.
Hint: Think of the state of an outbreak as an n  n square above, with asterisks
indicating infection. The rules for the spread of infection then deÔ¨Åne the transitions
of a state machine. Find a weakly decreasing derived variable that leads to a proof
of this theorem.


6
Recursive Data Types
Recursive data types play a central role in programming, and induction is really all
about them.
Recursive data types are speciÔ¨Åed by recursive deÔ¨Ånitions that say how to con-
struct new data elements from previous ones. Along with each recursive data type
there are recursive deÔ¨Ånitions of properties or functions on the data type. Most
importantly, based on a recursive deÔ¨Ånition, there is a structural induction method
for proving that all data of the given type have some property.
This chapter examines a few examples of recursive data types and recursively
deÔ¨Åned functions on them:
 strings of characters,
 the ‚Äúbalanced‚Äù strings of brackets,
 the nonnegative integers, and
 arithmetic expressions.
6.1
Recursive DeÔ¨Ånitions and Structural Induction
We‚Äôll start off illustrating recursive deÔ¨Ånitions and proofs using the example of
character strings. Normally we‚Äôd take strings of characters for granted, but it‚Äôs
informative to treat them as a recursive data type. In particular, strings are a nice
Ô¨Årst example because you will see recursive deÔ¨Ånitions of things that are easy to
understand or you already know, so you can focus on how the deÔ¨Ånitions work
without having to Ô¨Ågure out what they are for.
DeÔ¨Ånitions of recursive data types have two parts:
 Base case(s) specifying that some known mathematical elements are in the
data type, and
 Constructor case(s) that specify how to construct new data elements from
previously constructed elements or from base elements.
The deÔ¨Ånition of strings over a given character set, A, follows this pattern:

Chapter 6
Recursive Data Types
152
DeÔ¨Ånition 6.1.1. Let A be a nonempty set called an alphabet, whose elements are
referred to as characters, letters, or symbols. The recursive data type, A, of strings
over alphabet, A, are deÔ¨Åned as follows:
 Base case: the empty string, , is in A.
 Constructor case: If a 2 A and s 2 A, then the pair ha; si 2 A.
So f0; 1g are supposed to be the binary strings.
The usual way to treat binary strings is as sequences of 0‚Äôs and 1‚Äôs. For example,
we have identiÔ¨Åed the length-4 binary string 1011 as a sequence of bits, of a 4-
tuple, namely, .1; 0; 1; 1/. But according to the recursive DeÔ¨Ånition 6.1.1, this string
would be represented by nested pairs, namely
h1; h0; h1; h1; iiii :
These nested pairs are deÔ¨Ånitely cumbersome, and may also seem bizarre, but they
actually reÔ¨Çect the way lists of characters would be represented in programming
languages like Scheme or Python, where ha; si would correspond to cons.a; s/.
Notice that we haven‚Äôt said exactly how the empty string is represented. It really
doesn‚Äôt matter as long as we can recognize the empty string and not confuse it with
any nonempty string.
Continuing the recursive approach, let‚Äôs deÔ¨Åne the length of a string.
DeÔ¨Ånition 6.1.2. The length, jsj, of a string, s, is deÔ¨Åned recursively based on the
deÔ¨Ånition of s 2 A:
Base case: jj WWD 0.
Constructor case: j ha; si j WWD 1 C jsj.
This deÔ¨Ånition of length follows a standard pattern: functions on recursive data
types can be deÔ¨Åned recursively using the same cases as the data type deÔ¨Ånition.
Namely, to deÔ¨Åne a function, f , on a recursive data type, deÔ¨Åne the value of f for
the base cases of the data type deÔ¨Ånition, and then deÔ¨Åne the value of f in each
constructor case in terms of the values of f on the component data items.
Let‚Äôs do another example: the concatenation s  t of the strings s and t is the
string consisting of the letters of s followed by the letters of t. This is a per-
fectly clear mathematical deÔ¨Ånition of concatenation (except maybe for what to do
with the empty string), and in terms of Scheme/Python lists, s  t would be the list
append.s; t/. Here‚Äôs a recursive deÔ¨Ånition of concatenation.

6.1. Recursive DeÔ¨Ånitions and Structural Induction
153
DeÔ¨Ånition 6.1.3. The concatenation s  t of the strings s; t 2 A is deÔ¨Åned recur-
sively based on the deÔ¨Ånition of s 2 A:
Base case:
  t WWD t:
Constructor case:
ha; si  t WWD ha; s  ti :
Structural induction is a method for proving that all the elements of a recursively
deÔ¨Åned data type have some property. A structural induction proof has two parts
corresponding to the recursive deÔ¨Ånition:
 Prove that each base case element has the property.
 Prove that each constructor case element has the property, when the construc-
tor is applied to elements that have the property.
For example, we can verify the familiar fact that the length of the concatenation
of two strings is the sum of their lengths using structural induction:
Theorem 6.1.4. For all s; t 2 A,
js  tj D jsj C jtj:
Proof. By structural induction on the deÔ¨Ånition of s 2 A. The induction hypoth-
esis is
P.s/ WWD 8t 2 A: js  tj D jsj C jtj:
Base case (s D ):
js  tj D j  tj
D jtj
(def , base case)
D 0 C jtj
D jsj C jtj
(def length, base case)
Constructor case: Suppose sWWDha; ri and assume the induction hypothesis, P.r/.

Chapter 6
Recursive Data Types
154
We must show that P.s/ holds:
js  tj D j ha; ri  tj
D j ha; r  ti j
(concat def, constructor case)
D 1 C jr  tj
(length def, constructor case)
D 1 C .jrj C jtj/
since P.r/ holds
D .1 C jrj/ C jtj
D j ha; ri j C jtj
(length def, constructor case)
D jsj C jtj:
This proves that P.s/ holds as required, completing the constructor case. By struc-
tural induction we conclude that P.s/ holds for all strings s 2 A.

This proof illustrates the general principle:
The Principle of Structural Induction.
Let P be a predicate on a recursively deÔ¨Åned data type R. If
 P.b/ is true for each base case element, b 2 R, and
 for all two argument constructors, c,
≈íP.r/ AND P.s/¬ç IMPLIES P.c.r; s//
for all r; s 2 R,
and likewise for all constructors taking other numbers of arguments,
then
P.r/ is true for all r 2 R:
The number, #c.s/, of occurrences of the character c 2 A in the string s has a
simple recursive deÔ¨Ånition based on the deÔ¨Ånition of s 2 A:
DeÔ¨Ånition 6.1.5.
Base case: #c./ WWD 0.
Constructor case:
#c.ha; si/ WWD
(
#c.s/
if a ¬§ c;
1 C #c.s/
if a D c:

6.2. Strings of Matched Brackets
155
We‚Äôll need the following lemma in the next section:
Lemma 6.1.6.
#c.s  t/ D #c.s/ C #c.t/:
The easy proof by structural induction is an exercise (Problem 6.7).
6.2
Strings of Matched Brackets
Let f] ; [ g be the set of all strings of square brackets. For example, the following
two strings are in f] ; [ g:
[ ] ] [ [ [ [ [ ] ]
and
[ [ [ ] ] [ ] ] [ ]
(6.1)
A string, s 2 f] ; [ g, is called a matched string if its brackets ‚Äúmatch up‚Äù in
the usual way. For example, the left hand string above is not matched because its
second right bracket does not have a matching left bracket. The string on the right
is matched.
We‚Äôre going to examine several different ways to deÔ¨Åne and prove properties
of matched strings using recursively deÔ¨Åned sets and functions. These properties
are pretty straightforward, and you might wonder whether they have any particular
relevance in computer science. The honest answer is ‚Äúnot much relevance, any
more.‚Äù The reason for this is one of the great successes of computer science as
explained in the text box below.

Chapter 6
Recursive Data Types
156
Expression Parsing
During the early development of computer science in the 1950‚Äôs and 60‚Äôs, creation
of effective programming language compilers was a central concern. A key aspect
in processing a program for compilation was expression parsing. One signiÔ¨Åcant
problem was to take an expression like
x C y  z2  y C 7
and put in the brackets that determined how it should be evaluated ‚Äîshould it be
≈í≈íx C y¬ç  z2  y¬ç C 7; or;
x C ≈íy  z2  ≈íy C 7¬ç¬ç; or;
≈íx C ≈íy  z2¬ç¬ç  ≈íy C 7¬ç; or : : :‚Äπ
The Turing award (the ‚ÄúNobel Prize‚Äù of computer science) was ultimately be-
stowed on Robert W Floyd, for, among other things, discovering simple proce-
dures that would insert the brackets properly.
In the 70‚Äôs and 80‚Äôs, this parsing technology was packaged into high-level
compiler-compilers that automatically generated parsers from expression gram-
mars. This automation of parsing was so effective that the subject no longer
demanded attention. It largely disappeared from the computer science curriculum
by the 1990‚Äôs.
The matched strings can be nicely characterized as a recursive data type:
DeÔ¨Ånition 6.2.1. Recursively deÔ¨Åne the set, RecMatch, of strings as follows:
 Base case:  2 RecMatch.
 Constructor case: If s; t 2 RecMatch, then
[ s ] t 2 RecMatch:
Here [ s ] t refers to the concatenation of strings which would be written in full
as
[  .s  .]  t//:
From now on, we‚Äôll usually omit the ‚Äú‚Äôs.‚Äù
Using this deÔ¨Ånition,  2 RecMatch by the Base case, so letting s D t D  in
the constructor case implies
[ ]  D [ ] 2 RecMatch:

6.2. Strings of Matched Brackets
157
Now,
[ ] [ ] D [ ] [ ] 2 RecMatch
(letting s D ; t D [ ] )
[ [ ] ]  D [ [ ] ] 2 RecMatch
(letting s D [ ] ; t D )
[ [ ] ] [ ] 2 RecMatch
(letting s D [ ] ; t D [ ] )
are also strings in RecMatch by repeated applications of the Constructor case; and
so on.
It‚Äôs pretty obvious that in order for brackets to match, there better be an equal
number of left and right ones. For further practice, let‚Äôs carefully prove this from
the recursive deÔ¨Ånitions.
Lemma. Every string in RecMatch has an equal number of left and right brackets.
Proof. The proof is by structural induction with induction hypothesis
P.s/ WWD #[ .s/ D #] .s/:
Base case: P./ holds because
#[ ./ D 0 D #] ./
by the base case of DeÔ¨Ånition 6.1.5 of #c./.
Constructor case: By structural induction hypothesis, we assume P.s/ and P.t/
and must show P.[ s ] t/:
#[ .[ s ] t/ D #[ .[ / C #[ .s/ C #[ .] / C #[ .t/
(Lemma 6.1.6)
D 1 C #[ .s/ C 0 C #[ .t/
(def #[ ./)
D 1 C #] .s/ C 0 C #] .t/
(by P.s/ and P.t/)
D 0 C #] .s/ C 1 C #] .t/
D #] .[ / C #] .s/ C #] .] / C #] .t/
(def #] ./)
D #] .[ s ] t/
(Lemma 6.1.6)
This completes the proof of the constructor case. We conclude by structural induc-
tion that P.s/ holds for all s 2 RecMatch.

Warning: When a recursive deÔ¨Ånition of a data type allows the same element
to be constructed in more than one way, the deÔ¨Ånition is said to be ambiguous.
We were careful to choose an unambiguous deÔ¨Ånition of RecMatch to ensure that
functions deÔ¨Åned recursively on its deÔ¨Ånition would always be well-deÔ¨Åned. Re-
cursively deÔ¨Åning a function on an ambiguous data type deÔ¨Ånition usually will not
work. To illustrate the problem, here‚Äôs another deÔ¨Ånition of the matched strings.

Chapter 6
Recursive Data Types
158
DeÔ¨Ånition 6.2.2. DeÔ¨Åne the set, AmbRecMatch  f] ; [ g recursively as follows:
 Base case:  2 AmbRecMatch,
 Constructor cases: if s; t 2 AmbRecMatch, then the strings [ s ] and st are
also in AmbRecMatch.
It‚Äôs pretty easy to see that the deÔ¨Ånition of AmbRecMatch is just another way
to deÔ¨Åne RecMatch, that is AmbRecMatch D RecMatch (see Problem 6.15). The
deÔ¨Ånition of AmbRecMatch is arguably easier to understand, but we didn‚Äôt use it
because it‚Äôs ambiguous, while the trickier deÔ¨Ånition of RecMatch is unambiguous.
Here‚Äôs why this matters. Let‚Äôs deÔ¨Åne the number of operations, f .s/, to construct
a matched string s recursively on the deÔ¨Ånition of s 2 AmbRecMatch:
f ./ WWD 0;
(f base case)
f .[ s ] / WWD 1 C f .s/;
f .st/ WWD 1 C f .s/ C f .t/:
(f concat case)
This deÔ¨Ånition may seem ok, but it isn‚Äôt: f ./ winds up with two values, and
consequently:
0 D f ./
(f base case))
D f .  /
(concat def, base case)
D 1 C f ./ C f ./
(f concat case);
D 1 C 0 C 0 D 1
(f base case):
This is deÔ¨Ånitely not a situation we want to be in!
6.3
Recursive Functions on Nonnegative Integers
The nonnegative integers can be understood as a recursive data type.
DeÔ¨Ånition 6.3.1. The set, N, is a data type deÔ¨Åned recursively as:
 0 2 N.
 If n 2 N, then the successor, n C 1, of n is in N.
The point here is to make it clear that ordinary induction is simply the special
case of structural induction on the recursive DeÔ¨Ånition 6.3.1. This also justiÔ¨Åes the
familiar recursive deÔ¨Ånitions of functions on the nonnegative integers.

6.3. Recursive Functions on Nonnegative Integers
159
6.3.1
Some Standard Recursive Functions on N
Example 6.3.2. The Factorial function. This function is often written ‚Äún≈†.‚Äù You
will see a lot of it in later chapters. Here we‚Äôll use the notation fac.n/:
 fac.0/ WWD 1.
 fac.n C 1/ WWD .n C 1/  fac.n/ for n  0.
Example 6.3.3. The Fibonacci numbers. Fibonacci numbers arose out of an effort
800 years ago to model population growth. They have a continuing fan club of
people captivated by their extraordinary properties (see Problems 5.7, 5.16, 5.22).
The nth Fibonacci number, Ô¨Åb, can be deÔ¨Åned recursively by:
F.0/ WWD 0;
F.1/ WWD 1;
F.n/ WWD F.n   1/ C F.n   2/
for n  2.
Here the recursive step starts at n D 2 with base cases for 0 and 1. This is needed
since the recursion relies on two previous values.
What is F.4/? Well, F.2/ D F.1/ C F.0/ D 1, F.3/ D F.2/ C F.1/ D 2, so
F.4/ D 3. The sequence starts out 0; 1; 1; 2; 3; 5; 8; 13; 21; : : : .
Example 6.3.4. Sum-notation. Let ‚ÄúS.n/‚Äù abbreviate the expression ‚ÄúPn
iD1 f .i/.‚Äù
We can recursively deÔ¨Åne S.n/ with the rules
 S.0/ WWD 0.
 S.n C 1/ WWD f .n C 1/ C S.n/ for n  0.
6.3.2
Ill-formed Function DeÔ¨Ånitions
There are some other blunders to watch out for when deÔ¨Åning functions recursively.
The main problems come when recursive deÔ¨Ånitions don‚Äôt follow the recursive def-
inition of the underlying data type. Below are some function speciÔ¨Åcations that
resemble good deÔ¨Ånitions of functions on the nonnegative integers, but they aren‚Äôt.
f1.n/ WWD 2 C f1.n   1/:
(6.2)
This ‚ÄúdeÔ¨Ånition‚Äù has no base case. If some function, f1, satisÔ¨Åed (6.2), so would a
function obtained by adding a constant to the value of f1. So equation (6.2) does
not uniquely deÔ¨Åne an f1.

Chapter 6
Recursive Data Types
160
f2.n/ WWD
(
0;
if n D 0;
f2.n C 1/
otherwise:
(6.3)
This ‚ÄúdeÔ¨Ånition‚Äù has a base case, but still doesn‚Äôt uniquely determine f2. Any
function that is 0 at 0 and constant everywhere else would satisfy the speciÔ¨Åcation,
so (6.3) also does not uniquely deÔ¨Åne anything.
In a typical programming language, evaluation of f2.1/ would begin with a re-
cursive call of f2.2/, which would lead to a recursive call of f2.3/, ...with recur-
sive calls continuing without end. This ‚Äúoperational‚Äù approach interprets (6.3) as
deÔ¨Åning a partial function, f2, that is undeÔ¨Åned everywhere but 0.
f3.n/ WWD
8
ÀÜ<
ÀÜ:
0;
if n is divisible by 2,
1;
if n is divisible by 3,
2;
otherwise.
(6.4)
This ‚ÄúdeÔ¨Ånition‚Äù is inconsistent: it requires f3.6/ D 0 and f3.6/ D 1, so (6.4)
doesn‚Äôt deÔ¨Åne anything.
Mathematicians have been wondering about this function speciÔ¨Åcation for a
while:
f4.n/ WWD
8
ÀÜ<
ÀÜ:
1;
if n  1;
f4.n=2/
if n > 1 is even;
f4.3n C 1/
if n > 1 is odd:
(6.5)
For example, f4.3/ D 1 because
f4.3/WWDf4.10/WWDf4.5/WWDf4.16/WWDf4.8/WWDf4.4/WWDf4.2/WWDf4.1/WWD1:
The constant function equal to 1 will satisfy (6.5) (why?), but it‚Äôs not known if
another function does too. The problem is that the third case speciÔ¨Åes f4.n/ in
terms of f4 at arguments larger than n, and so cannot be justiÔ¨Åed by induction on
N. It‚Äôs known that any f4 satisfying (6.5) equals 1 for all n up to over a billion.
A Ô¨Ånal example is Ackermann‚Äôs function, which is an extremely fast-growing
function of two nonnegative arguments. Its inverse is correspondingly slow-growing
‚Äîit grows slower than log n, log log n, log log log n, ..., but it does grow un-
boundly. This inverse actually comes up analyzing a useful, highly efÔ¨Åcient proce-
dure known as the Union-Find algorithm. This algorithm was conjectured to run
in a number of steps that grew linearly in the size of its input, but turned out to be

6.4. Arithmetic Expressions
161
‚Äúlinear‚Äù but with a slow growing coefÔ¨Åcient nearly equal to the inverse Ackermann
function. This means that pragmatically Union-Find is linear since the theoretically
growing coefÔ¨Åcient is less than 5 for any input that could conceivably come up.
Ackermann‚Äôs function can be deÔ¨Åned recursively as the function, A, given by the
following rules:
A.m; n/ D 2n;
if m D 0 or n  1;
(6.6)
A.m; n/ D A.m   1; A.m; n   1//;
otherwise:
(6.7)
Now these rules are unusual because the deÔ¨Ånition of A.m; n/ involves an eval-
uation of A at arguments that may be a lot bigger than m and n. The deÔ¨Ånitions
of f2 above showed how deÔ¨Ånitions of function values at small argument values in
terms of larger one can easily lead to nonterminating evaluations. The deÔ¨Ånition
of Ackermann‚Äôs function is actually ok, but proving this takes some ingenuity (see
Problem 6.17).
6.4
Arithmetic Expressions
Expression evaluation is a key feature of programming languages, and recognition
of expressions as a recursive data type is a key to understanding how they can be
processed.
To illustrate this approach we‚Äôll work with a toy example: arithmetic expressions
like 3x2 C 2x C 1 involving only one variable, ‚Äúx.‚Äù We‚Äôll refer to the data type of
such expressions as Aexp. Here is its deÔ¨Ånition:
DeÔ¨Ånition 6.4.1.
 Base cases:
‚Äì The variable, x, is in Aexp.
‚Äì The arabic numeral, k, for any nonnegative integer, k, is in Aexp.
 Constructor cases: If e; f 2 Aexp, then
‚Äì [ e + f ] 2 Aexp. The expression [ e + f ] is called a sum. The Aexp‚Äôs
e and f are called the components of the sum; they‚Äôre also called the
summands.

Chapter 6
Recursive Data Types
162
‚Äì [ e  f ] 2 Aexp. The expression [ e  f ] is called a product. The
Aexp‚Äôs e and f are called the components of the product; they‚Äôre also
called the multiplier and multiplicand.
‚Äì - [ e] 2 Aexp. The expression - [ e] is called a negative.
Notice that Aexp‚Äôs are fully bracketed, and exponents aren‚Äôt allowed. So the
Aexp version of the polynomial expression 3x2C2xC1 would ofÔ¨Åcially be written
as
[ [ 3  [ x  x] ] + [ [ 2  x] + 1] ] :
(6.8)
These brackets and ‚Äôs clutter up examples, so we‚Äôll often use simpler expressions
like ‚Äú3x2C2xC1‚Äù instead of (6.8). But it‚Äôs important to recognize that 3x2C2xC1
is not an Aexp; it‚Äôs an abbreviation for an Aexp.
6.4.1
Evaluation and Substitution with Aexp‚Äôs
Evaluating Aexp‚Äôs
Since the only variable in an Aexp is x, the value of an Aexp is determined by the
value of x. For example, if the value of x is 3, then the value of 3x2 C 2x C 1
is obviously 34. In general, given any Aexp, e, and an integer value, n, for the
variable, x, we can evaluate e to Ô¨Ånds its value, eval.e; n/. It‚Äôs easy, and useful, to
specify this evaluation process with a recursive deÔ¨Ånition.
DeÔ¨Ånition 6.4.2. The evaluation function, eval W Aexp  Z ! Z, is deÔ¨Åned recur-
sively on expressions, e 2 Aexp, as follows. Let n be any integer.
 Base cases:
eval.x; n/ WWD n;
(value of variable x is n.)
(6.9)
eval.k; n/ WWD k;
(value of numeral k is k, regardless of x.)
(6.10)
 Constructor cases:
eval.[ e1 + e2] ; n/ WWD eval.e1; n/ C eval.e2; n/;
(6.11)
eval.[ e1  e2] ; n/ WWD eval.e1; n/  eval.e2; n/;
(6.12)
eval.- [ e1] ; n/ WWD   eval.e1; n/:
(6.13)

6.4. Arithmetic Expressions
163
For example, here‚Äôs how the recursive deÔ¨Ånition of eval would arrive at the value
of 3 C x2 when x is 2:
eval.[ 3 + [ x  x] ] ; 2/ D eval.3; 2/ C eval.[ x  x] ; 2/
(by Def 6.4.2.6.11)
D 3 C eval.[ x  x] ; 2/
(by Def 6.4.2.6.10)
D 3 C .eval.x; 2/  eval.x; 2//
(by Def 6.4.2.6.12)
D 3 C .2  2/
(by Def 6.4.2.6.9)
D 3 C 4 D 7:
Substituting into Aexp‚Äôs
Substituting expressions for variables is a standard operation used by compilers
and algebra systems. For example, the result of substituting the expression 3x for
x in the expression x.x   1/ would be 3x.3x   1/. We‚Äôll use the general notation
subst.f; e/ for the result of substituting an Aexp, f , for each of the x‚Äôs in an Aexp,
e. So as we just explained,
subst.3x; x.x   1// D 3x.3x   1/:
This substitution function has a simple recursive deÔ¨Ånition:
DeÔ¨Ånition 6.4.3. The substitution function from Aexp  Aexp to Aexp is deÔ¨Åned
recursively on expressions, e 2 Aexp, as follows. Let f be any Aexp.
 Base cases:
subst.f; x/ WWD f;
(subbing f for variable, x, just gives f )
(6.14)
subst.f; k/ WWD k
(subbing into a numeral does nothing.)
(6.15)
 Constructor cases:
subst.f; [ e1 + e2] / WWD [ subst.f; e1/ + subst.f; e2/]
(6.16)
subst.f; [ e1  e2] / WWD [ subst.f; e1/  subst.f; e2/]
(6.17)
subst.f; - [ e1] / WWD - [ subst.f; e1/] :
(6.18)

Chapter 6
Recursive Data Types
164
Here‚Äôs how the recursive deÔ¨Ånition of the substitution function would Ô¨Ånd the
result of substituting 3x for x in the x.x   1/:
subst.3x; x.x   1//
D subst.[ 3  x] ; [ x  [ x + - [ 1] ] ] /
(unabbreviating)
D [ subst.[ 3  x] ; x/ 
subst.[ 3  x] ; [ x + - [ 1] ] /]
(by Def 6.4.3 6.17)
D [ [ 3  x]  subst.[ 3  x] ; [ x + - [ 1] ] /]
(by Def 6.4.3 6.14)
D [ [ 3  x]  [ subst.[ 3  x] ; x/
+ subst.[ 3  x] ; - [ 1] /] ]
(by Def 6.4.3 6.16)
D [ [ 3  x]  [ [ 3  x] + - [ subst.[ 3  x] ; 1/] ] ]
(by Def 6.4.3 6.14 & 6.18)
D [ [ 3  x]  [ [ 3  x] + - [ 1] ] ]
(by Def 6.4.3 6.15)
D 3x.3x   1/
(abbreviation)
Now suppose we have to Ô¨Ånd the value of subst.3x; x.x   1// when x D 2.
There are two approaches.
First, we could actually do the substitution above to get 3x.3x   1/, and then
we could evaluate 3x.3x   1/ when x D 2, that is, we could recursively calculate
eval.3x.3x   1/; 2/ to get the Ô¨Ånal value 30. This approach is described by the
expression
eval.subst.3x; x.x   1//; 2/
(6.19)
In programming jargon, this would be called evaluation using the Substitution
Model. With this approach, the formula 3x appears twice after substitution, so
the multiplication 3  2 that computes its value gets performed twice.
The other approach is called evaluation using the Environment Model. Namely,
to compute the value of (6.19), we evaluate 3x when x D 2 using just 1 multiplica-
tion to get the value 6. Then we evaluate x.x   1/ when x has this value 6 to arrive
at the value 6  5 D 30. This approach is described by the expression
eval.x.x   1/; eval.3x; 2//:
(6.20)
So the Environment Model only computes the value of 3x once, and so it requires
one fewer multiplication than the Substitution model to compute (6.20).
But how do we know that these Ô¨Ånal values reached by these two approaches,
namely, the Ô¨Ånal integer values of (6.19) and (6.20), agree? In fact we can prove
pretty easily that these two approaches always agree by structural induction on the
deÔ¨Ånitions of the two approaches. More precisely, what we want to prove is

6.4. Arithmetic Expressions
165
Theorem 6.4.4. For all expressions e; f 2 Aexp and n 2 Z,
eval.subst.f; e/; n/ D eval.e; eval.f; n//:
(6.21)
Proof. The proof is by structural induction on e.1
Base cases:
 Case[x]
The left hand side of equation (6.21) equals eval.f; n/ by this base case in
DeÔ¨Ånition 6.4.3 of the substitution function, and the right hand side also
equals eval.f; n/ by this base case in DeÔ¨Ånition 6.4.2 of eval.
 Case[k].
The left hand side of equation (6.21) equals k by this base case in DeÔ¨Åni-
tions 6.4.3 and 6.4.2 of the substitution and evaluation functions. Likewise,
the right hand side equals k by two applications of this base case in the Def-
inition 6.4.2 of eval.
Constructor cases:
 Case[[ e1 + e2] ]
By the structural induction hypothesis (6.21), we may assume that for all
f 2 Aexp and n 2 Z,
eval.subst.f; ei/; n/ D eval.ei; eval.f; n//
(6.22)
for i D 1; 2. We wish to prove that
eval.subst.f; [ e1 + e2] /; n/ D eval.[ e1 + e2] ; eval.f; n//
(6.23)
But the left hand side of (6.23) equals
eval.[ subst.f; e1/ + subst.f; e2/] ; n/
by DeÔ¨Ånition 6.4.3.6.16 of substitution into a sum expression. But this equals
eval.subst.f; e1/; n/ C eval.subst.f; e2/; n/
1This is an example of why it‚Äôs useful to notify the reader what the induction variable is ‚Äîin this
case it isn‚Äôt n.

Chapter 6
Recursive Data Types
166
by DeÔ¨Ånition 6.4.2.(6.11) of eval for a sum expression. By induction hypoth-
esis (6.22), this in turn equals
eval.e1; eval.f; n// C eval.e2; eval.f; n//:
Finally, this last expression equals the right hand side of (6.23) by DeÔ¨Åni-
tion 6.4.2.(6.11) of eval for a sum expression. This proves (6.23) in this case.
 Case[[ e1  e2] ] Similar.
 Case[ [ e1] ] Even easier.
This covers all the constructor cases, and so completes the proof by structural
induction.

6.5
Induction in Computer Science
Induction is a powerful and widely applicable proof technique, which is why we‚Äôve
devoted two entire chapters to it. Strong induction and its special case of ordinary
induction are applicable to any kind of thing with nonnegative integer sizes ‚Äî
which is an awful lot of things, including all step-by-step computational processes.
Structural induction then goes beyond number counting, and offers a simple,
natural approach to proving things about recursive data types and recursive compu-
tation.
In many cases a nonnegative integer size can be deÔ¨Åned for a recursively deÔ¨Åned
datum, such as the length of a string, or the number of operations in an Aexp. It is
then possible to prove properties of data by ordinary induction on their size. But
this approach often produces more cumbersome proofs than structural induction.
In fact, structural induction is theoretically more powerful than ordinary induc-
tion. However, it‚Äôs only more powerful when it comes to reasoning about inÔ¨Ånite
data types ‚Äîlike inÔ¨Ånite trees, for example ‚Äîso this greater power doesn‚Äôt matter
in practice. What does matter is that for recursively deÔ¨Åned data types, structural
induction is a simple and natural approach. This makes it a technique every com-
puter scientist should embrace.

6.5. Induction in Computer Science
167
Problems for Section 6.1
Class Problems
Problem 6.1.
Prove that for all strings r; s; t 2 A
.r  s/  t D r  .s  t/:
Problem 6.2.
The reversal of a string is the string written backwards, for example, rev.abcde/ D
edcba.
(a) Give a simple recursive deÔ¨Ånition of rev.s/ based on the recursive deÔ¨Åni-
tion 6.1.1 of s 2 A and using the concatenation operation 6.1.3.
(b) Prove that
rev.s  t/ D rev.t/  rev.s/;
for all strings s; t 2 A.
Problem 6.3.
The Elementary 18.01 Functions (F18‚Äôs) are the set of functions of one real variable
deÔ¨Åned recursively as follows:
Base cases:
 The identity function, id.x/ WWD x is an F18,
 any constant function is an F18,
 the sine function is an F18,
Constructor cases:
If f; g are F18‚Äôs, then so are
1. f C g, fg, 2g,
2. the inverse function f  1,
3. the composition f ƒ± g.
(a) Prove that the function 1=x is an F18.
Warning: Don‚Äôt confuse 1=x D x 1 with the inverse id 1 of the identity function
id.x/. The inverse id 1 is equal to id.

Chapter 6
Recursive Data Types
168
(b) Prove by Structural Induction on this deÔ¨Ånition that the Elementary 18.01
Functions are closed under taking derivatives. That is, show that if f .x/ is an F18,
then so is f 0 WWD df=dx. (Just work out 2 or 3 of the most interesting constructor
cases; you may skip the less interesting ones.)
Problem 6.4.
Here is a simple recursive deÔ¨Ånition of the set, E, of even integers:
DeÔ¨Ånition. Base case: 0 2 E.
Constructor cases: If n 2 E, then so are n C 2 and  n.
Provide similar simple recursive deÔ¨Ånitions of the following sets:
(a) The set S WWD f2k3m5n 2 N j k; m; n 2 Ng.
(b) The set T WWD f2k32kCm5mCn 2 N j k; m; n 2 Ng.
(c) The set L WWD f.a; b/ 2 Z2 j .a   b/ is a multiple of 3g.
Let L0 be the set deÔ¨Åned by the recursive deÔ¨Ånition you gave for L in the previous
part. Now if you did it right, then L0 D L, but maybe you made a mistake. So let‚Äôs
check that you got the deÔ¨Ånition right.
(d) Prove by structural induction on your deÔ¨Ånition of L0 that
L0  L:
(e) ConÔ¨Årm that you got the deÔ¨Ånition right by proving that
L  L0:
(f) See if you can give an unambiguous recursive deÔ¨Ånition of L.
Problem 6.5.
DeÔ¨Ånition. The recursive data type, binary-2PTG, of binary trees with leaf labels,
L, is deÔ¨Åned recursively as follows:
 Base case: hleaf; li 2 binary-2PTG, for all labels l 2 L.
 Constructor case: If G1; G2 2 binary-2PTG, then
hbintree; G1; G2i 2 binary-2PTG:

6.5. Induction in Computer Science
169
The size, jGj, of G 2 binary-2PTG is deÔ¨Åned recursively on this deÔ¨Ånition by:
 Base case:
j hleaf; li j WWD 1;
for all l 2 L:
 Constructor case:
j hbintree; G1; G2i j WWD jG1j C jG2j C 1:
For example, the size of the binary-2PTG, G, pictured in Figure 6.1, is 7.
G
G1
win
G1,2
win
lose
win
Figure 6.1
A picture of a binary tree G.
(a) Write out (using angle brackets and labels bintree, leaf, etc.) the binary-2PTG,
G, pictured in Figure 6.1.
The value of Ô¨Çatten.G/ for G 2 binary-2PTG is the sequence of labels in L of
the leaves of G. For example, for the binary-2PTG, G, pictured in Figure 6.1,
Ô¨Çatten.G/ D .win; lose; win; win/:
(b) Give a recursive deÔ¨Ånition of Ô¨Çatten. (You may use the operation of concate-
nation (append) of two sequences.)
(c) Prove by structural induction on the deÔ¨Ånitions of Ô¨Çatten and size that
2  length.Ô¨Çatten.G// D jGj C 1:
(6.24)

Chapter 6
Recursive Data Types
170
Homework Problems
Problem 6.6.
Let m; n be integers, not both zero. DeÔ¨Åne a set of integers, Lm;n, recursively as
follows:
 Base cases: m; n 2 Lm;n.
 Constructor cases: If j; k 2 Lm;n, then
1.  j 2 Lm;n,
2. j C k 2 Lm;n.
Let L be an abbreviation for Lm;n in the rest of this problem.
(a) Prove by structural induction that every common divisor of m and n also di-
vides every member of L.
(b) Prove that any integer multiple of an element of L is also in L.
(c) Show that if j; k 2 L and k ¬§ 0, then rem .j; k/ 2 L.
(d) Show that there is a positive integer g 2 L which divides every member of L.
Hint: The least positive integer in L.
(e) Conclude that g D GCD.m; n/ for g from part (d).
Problem 6.7.
DeÔ¨Ånition. DeÔ¨Åne the number, #c.s/, of occurrences of the character c 2 A in the
string s recursively on the deÔ¨Ånition of s 2 A:
base case: #c./ WWD 0.
constructor case:
#c.ha; si/ WWD
(
#c.s/
if a ¬§ c;
1 C #c.s/
if a D c:
Prove by structural induction that for all s; t 2 A and c 2 A
#c.scdott/ D #c.s/ C #c.t/:

6.5. Induction in Computer Science
171
Figure 6.2
Constructing the Koch SnowÔ¨Çake.
Problem 6.8.
Fractals are an example of mathematical objects that can be deÔ¨Åned recursively.
In this problem, we consider the Koch snowÔ¨Çake. Any Koch snowÔ¨Çake can be
constructed by the following recursive deÔ¨Ånition.
 base case: An equilateral triangle with a positive integer side length is a
Koch snowÔ¨Çake.
 constructor case: Let K be a Koch snowÔ¨Çake, and let l be a line segment
on the snowÔ¨Çake. Remove the middle third of l, and replace it with two line
segments of the same length as is done in Figure 6.2
The resulting Ô¨Ågure is also a Koch snowÔ¨Çake.
Prove by structural induction that the area inside any Koch snowÔ¨Çake is of the
form q
p
3, where q is a rational number.
Problem 6.9.
Let L be some convenient set whose elements will be called labels. The labeled
binary trees, LBT‚Äôs, are deÔ¨Åned recursively as follows:
DeÔ¨Ånition. If l is a label,
Base case: hl; leafi is an LBT, and
Constructor case: if B and C are LBT‚Äôs, then hl; B; Ci is an LBT.
The leaf-labels and internal-labels of an LBT, are deÔ¨Åned recursively in the
obvious way:
DeÔ¨Ånition. Base case: The set of leaf-labels of the LBT hl; leafi is flg and its
set of internal-labels is the empty set.
Constructor case: The set of leaf labels of the LBT hl; B; Ci is the union of the
leaf-labels of B and of C; the set of internal-labels is the union of flg and the sets
of internal-labels of B and of C.
The set of labels of an LBT is the union of its leaf- and internal-labels.
The LBT‚Äôs with unique labels are also deÔ¨Åned recursively:

Chapter 6
Recursive Data Types
172
DeÔ¨Ånition. Base case: The LBT hl; leafi has unique labels.
Constructor case: The LBT hl; B; Ci has unique labels iff l is not a label of B or
C, and no label is a label of both B and C.
If B is an LBT, let nB be the number of internal-labels appearing in B and fB
be the number of leaf labels of B.
Prove by structural induction that
fB D nB C 1
(6.25)
for all LBT‚Äôs with unique labels. This equation can obviously fail if labels are not
unique, so your proof had better use uniqueness of labels at some point; be sure to
indicate where.
Exam Problems
Problem 6.10.
The Arithmetic Trig Functions (Atrig‚Äôs) are the set of functions of one real variable
deÔ¨Åned recursively as follows:
Base cases:
 The identity function, id.x/ WWD x is an Atrig,
 any constant function is an Atrig,
 the sine function is an Atrig,
Constructor cases:
If f; g are Atrig‚Äôs, then so are
1. f C g
2. f  g
3. the composition f ƒ± g.
Prove by Structural Induction on this deÔ¨Ånition that if f .x/ is an Atrig, then so
is f 0 WWD df=dx.

6.5. Induction in Computer Science
173
Problem 6.11.
The Limited 18.01 Functions (LF18‚Äôs) are deÔ¨Åned similarly to the F18 functions
from class problem 6.3, but they don‚Äôt have function composition or inverse as a
constructor. Namely,
DeÔ¨Ånition. LF18 is the set of functions of one complex variable deÔ¨Åned recursively
as follows:
Base cases:
 The identity function, id.z/ WWD z for z 2 C, is an LF18,
 any constant function is an LF18.
Constructor cases: If f; g are LF18‚Äôs, then so are
1. f C g, fg, and 2f .
Prove by structural induction that LF18 is closed under composition. That is,
using the induction hypothesis,
P.f / WWD 8g 2 LF18 : f ƒ± g 2 LF18;
prove that P.f / holds for all f 2 LF18. Make sure to indicate explicitly
 each of the base cases, and
 each of the constructor cases.
Problem 6.12.
DeÔ¨Ånition. The set RAF of rational functions of one real variable is the set of
functions deÔ¨Åned recursively as follows:
Base cases:
 The identity function, id.r/ WWD r for r 2 R (the real numbers), is an RAF,
 any constant function on R is an RAF.
Constructor cases: If f; g are RAF‚Äôs, then so are
1. f C g, fg, and f=g.

Chapter 6
Recursive Data Types
174
(a) Prove by structural induction that RAF is closed under composition. That is,
using the induction hypothesis,
P.h/ WWD 8g 2 RAF : h ƒ± g 2 RAF;
prove that P.h/ holds for all h 2 RAF. Make sure to indicate explicitly
 each of the base cases, and
 each of the constructor cases.
(b) BrieÔ¨Çy explain why a similar proof using the induction hypothesis
Q.g/ WWD 8h 2 RAF : h ƒ± g 2 RAF;
would break down.
Problems for Section 6.2
Practice Problems
Problem 6.13. (a) To prove that the set RecMatch, of matched strings of DeÔ¨Åni-
tion 6.2.1 equals the set AmbRecMatch of ambiguous matched strings of DeÔ¨Åni-
tion 6.2.2, you could Ô¨Årst prove that
8r 2 RecMatch: r 2 AmbRecMatch;
and then prove that
8u 2 AmbRecMatch: u 2 RecMatch:
Of these two statements, circle the one that would be simpler to prove by structural
induction directly from the deÔ¨Ånitions.
(b) Suppose structural induction was being used to prove that AmbRecMatch 
RecMatch. Circle the one predicate below that would Ô¨Åt the format for a structural
induction hypothesis in such a proof.
 P0.n/ WWD jsj  n IMPLIES s 2 RecMatch.
 P1.n/ WWD jsj  n IMPLIES s 2 AmbRecMatch.
 P2.s/ WWD s 2 RecMatch.
 P3.s/ WWD s 2 AmbRecMatch.
 P4.s/ WWD .s 2 RecMatch IMPLIES s 2 AmbRecMatch/.

6.5. Induction in Computer Science
175
(c) The recursive deÔ¨Ånition AmbRecMatch is ambiguous because it allows the
s  t constructor to apply when s or t is the empty string. But even Ô¨Åxing that,
ambiguity remains. Demonstrate this by giving two different derivations for the
string ‚Äù[ ] [ ] [ ] according to AmbRecMatch but only using the s  t constructor
when s ¬§  and t ¬§ .
Class Problems
Problem 6.14.
Let p be the string [ ] . A string of brackets is said to be erasable iff it can be
reduced to the empty string by repeatedly erasing occurrences of p. For example,
here‚Äôs how to erase the string [ [ [ ] ] [ ] ] [ ] :
[ [ [ ] ] [ ] ] [ ] ! [ [ ] ] ! [ ] ! :
On the other hand the string [ ] ] [ [ [ [ [ ] ] is not erasable because when we try to
erase, we get stuck:
[ ] ] [ [ [ [ [ ] ] ! ] [ [ [ [ ] ! ] [ [ [ 6!
Let Erasable be the set of erasable strings of brackets. Let RecMatch be the
recursive data type of strings of matched brackets given in DeÔ¨Ånition 6.2.1.
(a) Use structural induction to prove that
RecMatch  Erasable:
(b) Supply the missing parts (labeled by ‚Äú(*)‚Äù) of the following proof that
Erasable  RecMatch:
Proof. We prove by strong induction that every length-n string in Erasable is also
in RecMatch. The induction hypothesis is
P.n/ WWD 8x 2 Erasable: jxj D n IMPLIES x 2 RecMatch:
Base case:
(*) What is the base case? Prove that P is true in this case.
Inductive step: To prove P.n C 1/, suppose jxj D n C 1 and x 2 Erasable. We
need to show that x 2 RecMatch.
Let‚Äôs say that a string y is an erase of a string z iff y is the result of erasing a single
occurrence of p in z.

Chapter 6
Recursive Data Types
176
Since x 2 Erasable and has positive length, there must be an erase, y 2 Erasable,
of x. So jyj D n   1  0, and since y 2 Erasable, we may assume by induction
hypothesis that y 2 RecMatch.
Now we argue by cases:
Case (y is the empty string):
(*) Prove that x 2 RecMatch in this case.
Case (y D [ s ] t for some strings s; t 2 RecMatch): Now we argue by subcases.
 Subcase(x D py):
(*) Prove that x 2 RecMatch in this subcase.
 Subcase (x is of the form [ s0 ] t where s is an erase of s0):
Since s 2 RecMatch, it is erasable by part (b), which implies that s0 2
Erasable. But js0j < jxj, so by induction hypothesis, we may assume that
s0 2 RecMatch. This shows that x is the result of the constructor step of
RecMatch, and therefore x 2 RecMatch.
 Subcase (x is of the form [ s ] t0 where t is an erase of t0):
(*) Prove that x 2 RecMatch in this subcase.
(*) Explain why the above cases are sufÔ¨Åcient.
This completes the proof by strong induction on n, so we conclude that P.n/ holds
for all n 2 N. Therefore x 2 RecMatch for every string x 2 Erasable. That is,
Erasable  RecMatch. Combined with part (a), we conclude that
Erasable D RecMatch:

Problem 6.15. (a) Prove that the set RecMatch, of matched strings of DeÔ¨Ånition 6.2.1
is closed under string concatenation. Namely, if s; t 2 RecMatch, then s  t 2
RecMatch.
(b) Prove AmbRecMatch  RecMatch, where AmbRecMatch is the set of am-
biguous matched strings of DeÔ¨Ånition 6.2.2.
(c) Prove that RecMatch D AmbRecMatch.

6.5. Induction in Computer Science
177
Problem 6.16.
One way to determine if a string has matching brackets, that is, if it is in the set,
RecMatch, of DeÔ¨Ånition 6.2.1 is to start with 0 and read the string from left to right,
adding 1 to the count for each left bracket and subtracting 1 from the count for each
right bracket. For example, here are the counts for two sample strings:
[
]
]
[
[
[
[
[
]
]
]
]
0
1
0
 1
0
1
2
3
4
3
2
1
0
[
[
[
]
]
[
]
]
[
]
0
1
2
3
2
1
2
1
0
1
0
A string has a good count if its running count never goes negative and ends with 0.
So the second string above has a good count, but the Ô¨Årst one does not because its
count went negative at the third step. Let
GoodCount WWD fs 2 f] ; [ g j s has a good countg:
The empty string has a length 0 running count we‚Äôll take as a good count by
convention, that is,  2 GoodCount. The matched strings can now be characterized
precisely as this set of strings with good counts.
(a) Prove that GoodCount contains RecMatch by structural induction on the deÔ¨Å-
nition of RecMatch.
(b) Conversely, prove that RecMatch contains GoodCount.
Hint: By induction on the length of strings in GoodCount. Consider when the
running count equals 0 for the second time.
Problems for Section 6.3
Homework Problems
Problem 6.17.
Ackermann‚Äôs function, A W N2 ! N, is deÔ¨Åned recursively by the following rules:
A.m; n/ WWD 2n;
if m D 0 or n  1
(A-base)
A.m; n/ WWD A.m   1; A.m; n   1//;
otherwise:
(AA)
Prove that if B W N2 ! N is a partial function that satisÔ¨Åes this same deÔ¨Ånition,
then B is total and B D A.

Chapter 6
Recursive Data Types
178
Problems for Section 6.4
Practice Problems
Problem 6.18. (a) Write out the evaluation of
eval.subst.3x; x.x   1//; 2/
according to the Environment Model and the Substitution Model, indicating where
the rule for each case of the recursive deÔ¨Ånitions of eval.; / and ≈íWD] or substitution
is Ô¨Årst used. Compare the number of arithmetic operations and variable lookups.
(b) Describe an example along the lines of part (a) where the Environment Model
would perform 6 fewer multiplications than the Substitution model. You need not
carry out the evaluations.
(c) Describe an example along the lines of part (a) where the Substitution Model
would perform 6 fewer multiplications than the Environment model. You need not
carry out the evaluations.
Homework Problems
Problem 6.19. (a) Give a recursive deÔ¨Ånition of a function erase.e/ that erases all
the symbols in e 2 Aexp but the brackets. For example
erase.[ [ 3  [ x  x] ] + [ [ 2  x] + 1] ] / D [ [ [ ] ] [ [ 2  x] + 1] ] :
(b) Prove that erase.e/ 2 RecMatch for all e 2 Aexp.
(c) Give an example of a small string s 2 RecMatch such that [ s] ¬§ erase.e/ for
any e 2 Aexp.

7
InÔ¨Ånite Sets
This chapter is about inÔ¨Ånite sets and some challenges in proving things about
them.
Wait a minute! Why bring up inÔ¨Ånity in a Mathematics for Computer Science
text? After all, any data set in a computer memory is limited by the size of memory,
and there is a bound on the possible size of computer memory, for the simple reason
that the universe is (or at least appears to be) bounded. So why not stick with Ô¨Ånite
sets of some (maybe pretty big) bounded size? This is a good question, but let‚Äôs see
if we can persuade you that dealing with inÔ¨Ånite sets is inevitable.
You may not have noticed, but up to now you‚Äôve already accepted the routine
use of the integers, the rationals and irrationals, and sequences of these ‚ÄîinÔ¨Ånite
sets all. Further, do you really want Physics or the other sciences to give up the real
numbers on the grounds that only a bounded number of bounded measurements can
be made in a bounded universe? It‚Äôs pretty convincing and a lot simpler to ignore
such big and uncertain bounds (the universe seems to be getting bigger all the time)
and accept theories using real numbers.
Likewise in computer science, it simply isn‚Äôt plausible that writing a program
to add nonnegative integers with up to as many digits as, say, the stars in the sky
(billions of galaxies each with billions of stars), would be any different than writing
a program that would add any two integers no matter how many digits they had. The
same is true in designing a compiler: it‚Äôs neither useful nor sensible to make use of
the fact that in a bounded universe, only a bounded number of programs will ever
be compiled.
InÔ¨Ånite sets also provide a nice setting to practice proof methods, because it‚Äôs
harder to sneak in unjustiÔ¨Åed steps under the guise of intuition. And there has
been a truly astonishing outcome of studying inÔ¨Ånite sets. It led to the discovery of
widespread logical limits on what computers can possibly do. For example, in sec-
tion 7.2, we‚Äôll use reasoning developed for inÔ¨Ånite sets to prove that it‚Äôs impossible
to have a perfect type-checker for a programming language.
So in this chapter we ask you to bite the bullet and start learning to cope with
inÔ¨Ånity.

Chapter 7
InÔ¨Ånite Sets
180
7.1
InÔ¨Ånite Cardinality
In the late nineteenth century, the mathematician Georg Cantor was studying the
convergence of Fourier series and found some series that he wanted to say con-
verged ‚Äúmost of the time,‚Äù even though there were an inÔ¨Ånite number of points
where they didn‚Äôt converge. So Cantor needed a way to compare the size of in-
Ô¨Ånite sets. To get a grip on this, he got the idea of extending Theorem 4.5.4 to
inÔ¨Ånite sets, by regarding two inÔ¨Ånite sets as having the ‚Äúsame size‚Äù when there
was a bijection between them. Likewise, an inÔ¨Ånite set A is considered ‚Äúas big as‚Äù
a set B when A surj B, and ‚Äústrictly smaller‚Äù than B when A strict B. Cantor got
diverted from his study of Fourier series by his effort to develop a theory of inÔ¨Ånite
sizes based on these ideas. His theory ultimately had profound consequences for
the foundations of mathematics and computer science. But Cantor made a lot of
enemies in his own time because of his work: the general mathematical commu-
nity doubted the relevance of what they called ‚ÄúCantor‚Äôs paradise‚Äù of unheard-of
inÔ¨Ånite sizes.
A nice technical feature of Cantor‚Äôs idea is that it avoids the need for a deÔ¨Ånition
of what the ‚Äúsize‚Äù of an inÔ¨Ånite set might be ‚Äîall it does is compare ‚Äúsizes.‚Äù
Warning: We haven‚Äôt, and won‚Äôt, deÔ¨Åne what the ‚Äúsize‚Äù of an inÔ¨Ånite set is. The
deÔ¨Ånition of inÔ¨Ånite ‚Äúsizes‚Äù is cumbersome and technical, and we can get by just
Ô¨Åne without it. All we need are the ‚Äúas big as‚Äù and ‚Äúsame size‚Äù relations, surj and
bij, between sets.
But there‚Äôs something else to watch out for: we‚Äôve referred to surj as an ‚Äúas big
as‚Äù relation and bij as a ‚Äúsame size‚Äù relation on sets. Of course most of the ‚Äúas big
as‚Äù and ‚Äúsame size‚Äù properties of surj and bij on Ô¨Ånite sets do carry over to inÔ¨Ånite
sets, but some important ones don‚Äôt ‚Äîas we‚Äôre about to show. So you have to be
careful: don‚Äôt assume that surj has any particular ‚Äúas big as‚Äù property on inÔ¨Ånite
sets until it‚Äôs been proved.
Let‚Äôs begin with some familiar properties of the ‚Äúas big as‚Äù and ‚Äúsame size‚Äù
relations on Ô¨Ånite sets that do carry over exactly to inÔ¨Ånite sets:
Lemma 7.1.1. For any sets, A; B; C,
1. A surj B iff B inj A.
2. If A surj B and B surj C, then A surj C.
3. If A bij B and B bij C, then A bij C.
4. A bij B iff B bij A.

7.1. InÔ¨Ånite Cardinality
181
Part 1. follows from the fact that R has the ≈í 1 out;  1 in¬ç surjective function
property iff R 1 has the ≈í 1 out;  1 in¬ç total, injective property. Part 2. follows
from the fact that compositions of surjections are surjections. Parts 3. and 4. fol-
low from the Ô¨Årst two parts because R is a bijection iff R and R 1 are surjective
functions. We‚Äôll leave veriÔ¨Åcation of these facts to Problem 4.13.
Another familiar property of Ô¨Ånite sets carries over to inÔ¨Ånite sets, but this time
it‚Äôs not so obvious:
Theorem 7.1.2. [Schr¬®oder-Bernstein] For any sets A; B, if A surj B and B surj A,
then A bij B.
That is, the Schr¬®oder-Bernstein Theorem says that if A is at least as big as B
and conversely, B is at least as big as A, then A is the same size as B. Phrased
this way, you might be tempted to take this theorem for granted, but that would be
a mistake. For inÔ¨Ånite sets A and B, the Schr¬®oder-Bernstein Theorem is actually
pretty technical. Just because there is a surjective function f W A ! B ‚Äîwhich
need not be a bijection ‚Äîand a surjective function g W B ! A ‚Äîwhich also need
not be a bijection ‚Äîit‚Äôs not at all clear that there must be a bijection e W A ! B.
The idea is to construct e from parts of both f and g. We‚Äôll leave the actual
construction to Problem 7.6.
Another familiar property similar to the one resolved by the Schr¬®oder-Bernstein
Theorem is that if a set is not as big as another, then it must be strictly smaller, that
is,
NOT.A surj B/ IMPLIES A strict B:
This property of Ô¨Ånite sets indeed also holds for inÔ¨Ånite sets, but proving it requires
methods that go well beyond the scope of this text.
7.1.1
InÔ¨Ånity is different
A basic property of Ô¨Ånite sets that does not carry over to inÔ¨Ånite sets is that adding
something new makes a set bigger. That is, if A is a Ô¨Ånite set and b ‚Ä¶ A, then
jA [ fbgj D jAj C 1, and so A and A [ fbg are not the same size. But if A is
inÔ¨Ånite, then these two sets are the same size!
Lemma 7.1.3. Let A be a set and b ‚Ä¶ A. Then A is inÔ¨Ånite iff A bij A [ fbg.
Proof. Since A is not the same size as A [ fbg when A is Ô¨Ånite, we only have to
show that A [ fbg is the same size as A when A is inÔ¨Ånite.
That is, we have to Ô¨Ånd a bijection between A [ fbg and A when A is inÔ¨Ånite.
Here‚Äôs how: since A is inÔ¨Ånite, it certainly has at least one element; call it a0. But
since A is inÔ¨Ånite, it has at least two elements, and one of them must not be equal to

Chapter 7
InÔ¨Ånite Sets
182
a0; call this new element a1. But since A is inÔ¨Ånite, it has at least three elements,
one of which must not equal a0 or a1; call this new element a2. Continuing in
this way, we conclude that there is an inÔ¨Ånite sequence a0; a1; a2; : : : ; an; : : : of
different elements of A. Now it‚Äôs easy to deÔ¨Åne a bijection e W A [ fbg ! A:
e.b/ WWD a0;
e.an/ WWD anC1
for n 2 N;
e.a/ WWD a
for a 2 A   fb; a0; a1; : : :g:

7.1.2
Countable Sets
A set, C, is countable iff its elements can be listed in order, that is, the distinct
elements in C are precisely
c0; c1; : : : ; cn; : : : :
This means that if we deÔ¨Åned a function, f , on the nonnegative integers by the rule
that f .i/ WWD ci, then f would be a bijection from N to C. More formally,
DeÔ¨Ånition 7.1.4. A set, C, is countably inÔ¨Ånite iff N bij C. A set is countable iff
it is Ô¨Ånite or countably inÔ¨Ånite.
For example, the most basic countably inÔ¨Ånite set is the set, N, itself. But the
set, Z, of all integers is also countably inÔ¨Ånite, because the integers can be listed in
the order,
0;  1; 1;  2; 2;  3; 3; : : : :
(7.1)
In this case, there is a simple formula for the nth element of the list (7.1). That is,
the bijection f W N ! Z such that f .n/ is the nth element of the list can be deÔ¨Åned
as:
f .n/ WWD
(
n=2
if n is even;
 .n C 1/=2
if n is odd:
There is also a simple way to list all pairs of nonnegative integers, which shows that
.NN/ is also countably inÔ¨Ånite. From that it‚Äôs a small step to reach the conclusion
that the set, Q0, of nonnegative rational numbers is countable. This may be a
surprise ‚Äîafter all, the rationals densely Ô¨Åll up the space between integers, and for
any two, there‚Äôs another in between, so it might seem as though you couldn‚Äôt write
them all out in a list, but Problem 7.5 illustrates how to do it. More generally, it is
easy to show that countable sets are closed under unions and products (Problems 7.1
and 7.11) which implies the countability of a bunch of familiar sets:

7.1. InÔ¨Ånite Cardinality
183
Corollary 7.1.5. The following sets are countably inÔ¨Ånite:
ZC; Z; N  N; QC; Z  Z; Q:
A small modiÔ¨Åcation of the proof of Lemma 7.1.3 shows that countably inÔ¨Å-
nite sets are the ‚Äúsmallest‚Äù inÔ¨Ånite sets, namely, if A is an inÔ¨Ånite set, and B is
countable, then A surj B (see Problem 7.4).
Since adding one new element to an inÔ¨Ånite set doesn‚Äôt change its size, it‚Äôs obvi-
ous that neither will adding any Ô¨Ånite number of elements. It‚Äôs a common mistake
to think that this proves that you can throw in inÔ¨Ånitely many new elements. But
just because it‚Äôs ok to do something any Ô¨Ånite number of times doesn‚Äôt make it OK
to do an inÔ¨Ånite number of times. For example, starting from 3, you can increment
by 1 any Ô¨Ånite number of times and the result will be some integer greater than
or equal to 3. But if you increment an inÔ¨Ånite number of times, you don‚Äôt get an
integer at all.
The good news is that you really can add a countably inÔ¨Ånite number of new
elements to an inÔ¨Ånite set and still wind up with just a set of the same size; see
Problem 7.8.
7.1.3
Power sets are strictly bigger
Cantor‚Äôs astonishing discovery was that not all inÔ¨Ånite sets are the same size. In
particular, he proved that for any set, A, the power set, pow.A/, is ‚Äústrictly bigger‚Äù
than A. That is,
Theorem 7.1.6. [Cantor] For any set, A,
A strict pow.A/:
Proof. First of all, pow.A/ is as big as A: for example, the partial function f W
pow.A/ ! A, where f .fag/ WWD a for a 2 A and f is only deÔ¨Åned on one-element
sets, is a surjection.
To show that pow.A/ is strictly bigger than A, we have to show that if g is a
function from A to pow.A/, then g is not a surjection. To do this, we‚Äôll simply
Ô¨Ånd a subset, Ag  A that is not in the range of g. The idea is, for any element
a 2 A, to look at the set g.a/  A and ask whether or not a happens to be in g.a/.
Namely deÔ¨Åne
Ag WWD fa 2 A j a ‚Ä¶ g.a/g:
Now Ag is a well-deÔ¨Åned subset of A, which means it is a member of pow.A/. But
Ag can‚Äôt be in the range of g, because if it were, we would have
Ag D g.a0/

Chapter 7
InÔ¨Ånite Sets
184
for some a0 2 A, so by deÔ¨Ånition of Ag,
a 2 g.a0/
iff
a 2 Ag
iff
a ‚Ä¶ g.a/
for all a 2 A. Now letting a D a0 yields the contradiction
a0 2 g.a0/
iff
a0 ‚Ä¶ g.a0/:
So g is not a surjection, because there is an element in the power set of A, namely
the set Ag, that is not in the range of g.

Cantor‚Äôs Theorem immediately implies:
Corollary 7.1.7. pow.N/ is uncountable.
The bijection between subsets of an n-element set and the length n bit-strings,
f0; 1gn, used to prove Theorem 4.5.5, carries over to a bijection between subsets of
a countably inÔ¨Ånite set and the inÔ¨Ånite bit-strings, f0; 1g!. That is,
pow.N/ bij f0; 1g!:
This immediately implies
Corollary 7.1.8. f0; 1g! is uncountable.
Larger InÔ¨Ånities
There are lots of different sizes of inÔ¨Ånite sets. For example, starting with the
inÔ¨Ånite set, N, of nonnegative integers, we can build the inÔ¨Ånite sequence of sets
N strict pow.N/ strict pow.pow.N// strict pow.pow.pow.N/// strict : : : :
By Theorem 7.1.6, each of these sets is strictly bigger than all the preceding ones.
But that‚Äôs not all: the union of all the sets in the sequence is strictly bigger than each
set in the sequence (see Problem 7.16). In this way you can keep going indeÔ¨Ånitely,
building ‚Äúbigger‚Äù inÔ¨Ånities all the way.
7.2
The Halting Problem
Granted that towers of larger and larger inÔ¨Ånite sets are at best just a romantic
concern for a computer scientist, the reasoning that leads to these conclusions plays
a critical role in the theory of computation. Cantor‚Äôs proof embodies the simplest

7.2. The Halting Problem
185
form of what is known as a ‚Äúdiagonal argument.‚Äù Diagonal arguments are used to
show that lots of problems logically just can‚Äôt be solved by computation, and there
is no getting around it.
This story begins with a reminder that having procedures operate on programs
is a basic part of computer science technology. For example, compilation refers to
taking any given program text written in some ‚Äúhigh level‚Äù programming language
like Java, C++, Python, ..., and then generating a program of low-level instruc-
tions that does the same thing but is targeted to run well on available hardware.
Similarly, interpreters or virtual machines are procedures that take a program text
designed to be run on one kind of computer and simulate it on another kind of com-
puter. Routine features of compilers involve ‚Äútype-checking‚Äù programs to ensure
that certain kinds of run-time errors won‚Äôt happen, and ‚Äúoptimizing‚Äù the generated
programs so they run faster or use less memory.
Now the fundamental thing that logically just can‚Äôt be done by computation is
a perfect job of type-checking, optimizing, or any kind of analysis of the overall
run time behavior of programs. In this section we‚Äôll illustrate this with a basic
example known as the Halting Problem. The general Halting Problem for some
programming language is, given an arbitrary program, recognize when running the
program will not Ô¨Ånish successfully ‚Äîhalt ‚Äîbecause it aborts with some kind of
error, or because it simply never stops. Of course it‚Äôs easy to detect when any
given program will halt: just run it on a virtual machine and wait. The problem is
what if the given program does not halt ‚Äîhow do you recognize that? We will use
a diagonal argument to prove that if an analysis program tries to recognize non-
halting programs, it is bound to give wrong answers, or no answers, for an inÔ¨Ånite
number of programs it might have to analyze!
To be precise about this, let‚Äôs call a programming procedure ‚Äîwritten in your
favorite programming language such as C++, or Java, or Python ‚Äîa string proce-
dure when it is applicable to strings over a standard alphabet ‚Äîsay the 256 char-
acter ASCII alphabet ASCII. When a string procedure applied to an ASCII string
returns the boolean value True, we‚Äôll say the procedure recognizes the string. If
the procedure does anything else ‚Äîreturns a value other than True, aborts with an
error, runs forever,. . .‚Äîthen it doesn‚Äôt recognize the string.
As a simple example, you might think about how to write a string procedure that
recognizes precisely those double letter ASCII strings in which every character
occurs twice in a row. For example, aaCC33, and zz++ccBB are double letter
ASCII strings, but aa;bb, b33, and AAAAA are not. Even better, how about
actually writing a recognizer for the double letter ASCII strings in your favorite
programming language?
We‚Äôll call a set of strings recognizable if there is a procedure that recognizes

Chapter 7
InÔ¨Ånite Sets
186
precisely that set of strings. So the set of double letter strings is recognizable.
Let ASCII be the set of (Ô¨Ånite) strings of ASCII characters. There is no harm in
assuming that every program can be written using only the ASCII characters; they
usually are anyway. When a string s 2 ASCII is actually the ASCII description of
some string procedure, we‚Äôll refer to that string procedure as Ps. You can think of
Ps as the result of compiling s.1 It‚Äôs technically helpful to treat every ASCII string
as a program for a string procedure. So when a string s 2 ASCII doesn‚Äôt parse
as a proper string procedure, we‚Äôll deÔ¨Åne Ps to be some default string procedure
‚Äîsay one that always returns False.
Now we can deÔ¨Åne the precise set of strings that describe non-halting programs:
DeÔ¨Ånition 7.2.1.
No-halt WWD fs 2 ASCII j Ps does not recognize sg:
(7.2)
Recognizing the strings in No-halt is a special case of the Halting Problem. We‚Äôll
blow away any chance of having a program solve the general problem by showing
that no program can solve this special case. In particular, we‚Äôre going to prove
Theorem 7.2.2. No-halt is not recognizable.
We‚Äôll use an argument just like Cantor‚Äôs in the proof of Theorem 7.1.6.
Proof. Namely for any string s 2 ASCII, let f .s/ be the set of strings recognized
by Ps:
f .s/ WWD ft 2 ASCII j Ps recognizes tg:
By convention, we associated a string procedure, Ps, with every string, s 2 ASCII,
which makes f a total function, and by deÔ¨Ånition,
s 2 No-halt IFF s ‚Ä¶ f .s/;
(7.3)
for all strings, s 2 ASCII.
Now suppose to the contrary that No-halt was recognizable. This means there is
some procedure Ps0 that recognizes No-halt, which is the same as saying that
No-halt D f .s0/:
1The string, s 2 ASCII, and the procedure, Ps, have to be distinguished to avoid a type error:
you can‚Äôt apply a string to string. For example, let s be the string that you wrote as your program
to recognize the double letter strings. Applying s to a string argument, say aabbccdd, should
throw a type exception; what you need to do is compile s to the procedure Ps and then apply Ps to
aabbccdd.

7.2. The Halting Problem
187
Combined with (7.3), we get
s 2 f .s0/
iff
s ‚Ä¶ f .s/
(7.4)
for all s 2 ASCII. Now letting s D s0 in (7.4) yields the immediate contradiction
s0 2 f .s0/
iff
s0 ‚Ä¶ f .s0/:
This contradiction implies that No-halt cannot be recognized by any string pro-
cedure.

So that does it: it‚Äôs logically impossible for programs in any particular language
to solve just this special case of the general Halting Problem for programs in that
language. And having proved that it‚Äôs impossible to have a procedure that Ô¨Ågures
out whether an arbitrary program returns True, it‚Äôs easy to show that it‚Äôs impossible
to have a procedure that is a perfect recognizer for any overall run time property.2
For example, most compilers do ‚Äústatic‚Äù type-checking at compile time to ensure
that programs won‚Äôt make run-time type errors. A program that type-checks is
guaranteed not to cause a run-time type-error. But since it‚Äôs impossible to recognize
perfectly when programs won‚Äôt cause type-errors, it follows that the type-checker
must be rejecting programs that really wouldn‚Äôt cause a type-error. The conclusion
is that no type-checker is perfect ‚Äîyou can always do better!
It‚Äôs a different story if we think about the practical possibility of writing pro-
gramming analyzers. The fact that it‚Äôs logically impossible to analyze perfectly
arbitrary programs does not mean that you can‚Äôt do a very good job analyzing in-
teresting programs that come up in practice. In fact these ‚Äúinteresting‚Äù programs are
commonly intended to be analyzable in order to conÔ¨Årm that they do what they‚Äôre
supposed to do.
So it‚Äôs not clear how much of a hurdle this theoretical limitation implies in prac-
tice. What the theory does provide is some perspective on claims about general
analysis methods for programs. The theory tells us that people who make such
claims either
 are exaggerating the power (if any) of their methods ‚Äîsay to make a sale or
get a grant, or
 are trying to keep things simple by not going into technical limitations they‚Äôre
aware of, or
2The weasel word ‚Äúoverall‚Äù creeps in here to rule out some run time properties that are easy
to recognize because they depend only on part of the run time behavior. For example, the set of
programs that halt after executing at most 100 instructions is recognizable.

Chapter 7
InÔ¨Ånite Sets
188
 perhaps most commonly, are so excited about some useful practical successes
of their methods that they haven‚Äôt bothered to think about the limitations
which you know must be there.
So from now on, if you hear people making claims about having general program
analysis/veriÔ¨Åcation/optimization methods, you‚Äôll know they can‚Äôt be telling the
whole story.
One more important point: there‚Äôs no hope of getting around this by switching
programming languages. Our proof covered programs written in some given pro-
gramming language like Java, for example, and concluded that no Java program can
perfectly analyze all Java programs. Could there be a C++ analysis procedure that
successfully takes on all Java programs? After all, C++ does allow more intimate
manipulation of computer memory than Java does. But there is no loophole here:
it‚Äôs possible to write a virtual machine for C++ in Java, so if there were a C++ pro-
cedure that analyzed Java programs, the Java virtual machine would be able to do
it too, and that‚Äôs impossible. These logical limitations on the power of computation
apply no matter what kinds of programs or computers you use.
7.3
The Logic of Sets
7.3.1
Russell‚Äôs Paradox
Reasoning naively about sets turns out to be risky. In fact, one of the earliest at-
tempts to come up with precise axioms for sets in the late nineteenth century by
the logician Gotlob Frege, was shot down by a three line argument known as Rus-
sell‚Äôs Paradox3 which reasons in nearly the same way as the proof of Cantor‚Äôs
Theorem 7.1.6. This was an astonishing blow to efforts to provide an axiomatic
foundation for mathematics:
3Bertrand Russell was a mathematician/logician at Cambridge University at the turn of the Twen-
tieth Century. He reported that when he felt too old to do mathematics, he began to study and write
about philosophy, and when he was no longer smart enough to do philosophy, he began writing about
politics. He was jailed as a conscientious objector during World War I. For his extensive philosophical
and political writing, he won a Nobel Prize for Literature.

7.3. The Logic of Sets
189
Russell‚Äôs Paradox
Let S be a variable ranging over all sets, and deÔ¨Åne
W WWD fS j S 62 Sg:
So by deÔ¨Ånition,
S 2 W iff S 62 S;
for every set S. In particular, we can let S be W , and obtain the
contradictory result that
W 2 W iff W 62 W:
So the simplest reasoning about sets crashes mathematics! Russell and his col-
league Whitehead spent years trying to develop a set theory that was not contra-
dictory, but would still do the job of serving as a solid logical foundation for all of
mathematics.
Actually, a way out of the paradox was clear to Russell and others at the time:
it‚Äôs unjustiÔ¨Åed to assume that W is a set. So the step in the proof where we let S
be W has no justiÔ¨Åcation, because S ranges over sets, and W may not be a set. In
fact, the paradox implies that W had better not be a set!
But denying that W is a set means we must reject the very natural axiom that
every mathematically well-deÔ¨Åned collection of sets is actually a set. The prob-
lem faced by Frege, Russell and their fellow logicians was how to specify which
well-deÔ¨Åned collections are sets. Russell and his Cambridge University colleague
Whitehead immediately went to work on this problem. They spent a dozen years
developing a huge new axiom system in an even huger monograph called Principia
Mathematica, but basically their approach failed. It was so cumbersome no one
ever used it, and it was subsumed by a much simpler, and now widely accepted,
axiomatization of set theory due to the logicians Zermelo and Frankel.
7.3.2
The ZFC Axioms for Sets
It‚Äôs generally agreed that, using some simple logical deduction rules, essentially all
of mathematics can be derived from some axioms about sets called the Axioms of
Zermelo-Frankel Set Theory with Choice (ZFC).
We‚Äôre not going to be studying these axioms in this text, but we thought you
might like to see them ‚Äìand while you‚Äôre at it, get some practice reading quantiÔ¨Åed

Chapter 7
InÔ¨Ånite Sets
190
formulas:
Extensionality. Two sets are equal if they have the same members. In a logic
formula of set theory, this would be stated as:
.8z: z 2 x IFF z 2 y/ IMPLIES x D y:
Pairing. For any two sets x and y, there is a set, fx; yg, with x and y as its only
elements:
8x; y: 9u: 8z: ≈íz 2 u IFF .z D x OR z D y/¬ç
Union. The union, u, of a collection, z, of sets is also a set:
8z: 9u: 8x: .9y: x 2 y AND y 2 z/ IFF x 2 u:
InÔ¨Ånity. There is an inÔ¨Ånite set. SpeciÔ¨Åcally, there is a nonempty set, x, such that
for any set y 2 x, the set fyg is also a member of x.
Subset. Given any set, x, and any deÔ¨Ånable property of sets, there is a set contain-
ing precisely those elements y 2 x that have the property.
8x: 9z: 8y: y 2 z IFF ≈íy 2 x AND .y/¬ç
where .y/ is any assertion about y deÔ¨Ånable in the notation of set theory.
Power Set. All the subsets of a set form another set:
8x: 9p: 8u: u  x IFF u 2 p:
Replacement. Suppose a formula, , of set theory deÔ¨Ånes the graph of a function,
that is,
8x; y; z: ≈í.x; y/ AND .x; z/¬ç IMPLIES y D z:
Then the image of any set, s, under that function is also a set, t. Namely,
8s 9t 8y: ≈í9x: .x; y/ IFF y 2 t¬ç:
Foundation. There cannot be an inÔ¨Ånite sequence
   2 xn 2    2 x1 2 x0

7.4. Does All This Really Work?
191
of sets each of which is a member of the previous one. This is equivalent
to saying every nonempty set has a ‚Äúmember-minimal‚Äù element. Namely,
deÔ¨Åne
member-minimal.m; x/ WWD ≈ím 2 x AND 8y 2 x: y ‚Ä¶ m¬ç:
Then the Foundation axiom is
8x: x ¬§ ; IMPLIES 9m: member-minimal.m; x/:
Choice. Given a set, s, whose members are nonempty sets no two of which have
any element in common, then there is a set, c, consisting of exactly one
element from each set in s. The formula is given in Problem 7.20.
7.3.3
Avoiding Russell‚Äôs Paradox
These modern ZFC axioms for set theory are much simpler than the system Russell
and Whitehead Ô¨Årst came up with to avoid paradox. In fact, the ZFC axioms are
as simple and intuitive as Frege‚Äôs original axioms, with one technical addition: the
Foundation axiom. Foundation captures the intuitive idea that sets must be built
up from ‚Äúsimpler‚Äù sets in certain standard ways. And in particular, Foundation
implies that no set is ever a member of itself. So the modern resolution of Russell‚Äôs
paradox goes as follows: since S 62 S for all sets S, it follows that W , deÔ¨Åned
above, contains every set. This means W can‚Äôt be a set ‚Äîor it would be a member
of itself.
7.4
Does All This Really Work?
So this is where mainstream mathematics stands today: there is a handful of ZFC
axioms from which virtually everything else in mathematics can be logically de-
rived. This sounds like a rosy situation, but there are several dark clouds, suggest-
ing that the essence of truth in mathematics is not completely resolved.
 The ZFC axioms weren‚Äôt etched in stone by God. Instead, they were mostly
made up by Zermelo, who may have been a brilliant logician, but was also
a fallible human being ‚Äîprobably some days he forgot his house keys. So
maybe Zermelo, just like Frege, didn‚Äôt get his axioms right and will be shot
down by some successor to Russell who will use his axioms to prove a propo-
sition P and its negation P . Then math would be broken. This sounds crazy,
but after all, it has happened before.

Chapter 7
InÔ¨Ånite Sets
192
In fact, while there is broad agreement that the ZFC axioms are capable of
proving all of standard mathematics, the axioms have some further conse-
quences that sound paradoxical. For example, the Banach-Tarski Theorem
says that, as a consequence of the Axiom of Choice, a solid ball can be di-
vided into six pieces and then the pieces can be rigidly rearranged to give two
solid balls of the same size as the original!
 Some basic questions about the nature of sets remain unresolved. For exam-
ple, Cantor raised the question whether there is a set whose size is strictly
between the smallest inÔ¨Ånite set, N (see Problem 7.4), and the strictly larger
set, pow.N/? Cantor guessed not:
Cantor‚Äôs Continuum Hypothesis: There is no set, A, such that
N strict A strict pow.N/:
The Continuum Hypothesis remains an open problem a century later. Its
difÔ¨Åculty arises from one of the deepest results in modern Set Theory ‚Äî
discovered in part by G¬®odel in the 1930‚Äôs and Paul Cohen in the 1960‚Äôs
‚Äînamely, the ZFC axioms are not sufÔ¨Åcient to settle the Continuum Hy-
pothesis: there are two collections of sets, each obeying the laws of ZFC,
and in one collection the Continuum Hypothesis is true, and in the other it is
false. So settling the Continuum Hypothesis requires a new understanding of
what Sets should be to arrive at persuasive new axioms that extend ZFC and
are strong enough to determine the truth of the Continuum Hypothesis one
way or the other.
 But even if we use more or different axioms about sets, there are some un-
avoidable problems. In the 1930‚Äôs, G¬®odel proved that, assuming that an ax-
iom system like ZFC is consistent ‚Äîmeaning you can‚Äôt prove both P and P
for any proposition, P ‚Äîthen the very proposition that the system is consis-
tent (which is not too hard to express as a logical formula) cannot be proved
in the system. In other words, no consistent system is strong enough to verify
itself.
7.4.1
Large InÔ¨Ånities in Computer Science
If the romance of different size inÔ¨Ånities and continuum hypotheses doesn‚Äôt appeal
to you, not knowing about them is not going to limit you as a computer scientist.
These abstract issues about inÔ¨Ånite sets rarely come up in mainstream mathematics,
and they don‚Äôt come up at all in computer science, where the focus is generally on
‚Äúcountable,‚Äù and often just Ô¨Ånite, sets. In practice, only logicians and set theorists

7.4. Does All This Really Work?
193
have to worry about collections that are ‚Äútoo big‚Äù to be sets. That‚Äôs part of the
reason that the 19th century mathematical community made jokes about ‚ÄúCantor‚Äôs
paradise‚Äù of obscure inÔ¨Ånite sets. But the challenge of reasoning correctly about
this far out stuff led directly to the profound discoveries about the logical limits of
computation described in Section 7.2, and that really is something every computer
scientist should understand.
Problems for Section 7.1
Practice Problems
Problem 7.1.
Prove that if A and B are countable sets, then so is A [ B.
Problem 7.2.
Show that the set f0; 1g of Ô¨Ånite binary strings is countable.
Class Problems
Problem 7.3.
Show that the set N of Ô¨Ånite sequences of nonnegative integers is countable.
Problem 7.4. (a) Several students felt the proof of Lemma 7.1.3 was worrisome,
if not circular. What do you think?
(b) Use the proof of Lemma 7.1.3 to show that if A is an inÔ¨Ånite set, then A surj N,
that is, every inÔ¨Ånite set is ‚Äúas big as‚Äù the set of nonnegative integers.
Problem 7.5.
The rational numbers Ô¨Åll the space between integers, so a Ô¨Årst thought is that there
must be more of them than the integers, but it‚Äôs not true. In this problem you‚Äôll
show that there are the same number of positive rationals as positive integers. That
is, the positive rationals are countable.
(a) DeÔ¨Åne a bijection between the set, ZC, of positive integers, and the set, .ZC 

Chapter 7
InÔ¨Ånite Sets
194
ZC/, of all pairs of positive integers:
.1; 1/; .1; 2/; .1; 3/; .1; 4/; .1; 5/; : : :
.2; 1/; .2; 2/; .2; 3/; .2; 4/; .2; 5/; : : :
.3; 1/; .3; 2/; .3; 3/; .3; 4/; .3; 5/; : : :
.4; 1/; .4; 2/; .4; 3/; .4; 4/; .4; 5/; : : :
.5; 1/; .5; 2/; .5; 3/; .5; 4/; .5; 5/; : : :
:::
(b) Conclude that the set, QC, of all positive rational numbers is countable.
Problem 7.6.
This problem provides a proof of the [Schr¬®oder-Bernstein] Theorem:
If A surj B and B surj A, then A bij B.
(7.5)
(a) It is OK to assume that A and B are disjoint. Why?
(b) Explain why there are total injective functions f W A ! B, and g W B ! A.
Picturing the diagrams for f and g, there is exactly one arrow out of each ele-
ment ‚Äîa left-to-right f -arrow if the element is in A and a right-to-left g-arrow if
the element is in B. This is because f and g are total functions. Also, there is at
most one arrow into any element, because f and g are injections.
So starting at any element, there is a unique and unending path of arrows going
forwards. There is also a unique path of arrows going backwards, which might be
unending, or might end at an element that has no arrow into it. These paths are
completely separate: if two ran into each other, there would be two arrows into the
element where they ran together.
This divides all the elements into separate paths of four kinds:
i. paths that are inÔ¨Ånite in both directions,
ii. paths that are inÔ¨Ånite going forwards starting from some element of A.
iii. paths that are inÔ¨Ånite going forwards starting from some element of B.
iv. paths that are unending but Ô¨Ånite.
(c) What do the paths of the last type (iv) look like?
(d) Show that for each type of path, either

7.4. Does All This Really Work?
195
 the f -arrows deÔ¨Åne a bijection between the A and B elements on the path, or
 the g-arrows deÔ¨Åne a bijection between B and A elements on the path, or
 both sets of arrows deÔ¨Åne bijections.
For which kinds of paths do both sets of arrows deÔ¨Åne bijections?
(e) Explain how to piece these bijections together to prove that A and B are the
same size.
Problem 7.7.
Prove that if there is a surjective function (≈í 1 out;  1 in¬ç mapping) f W N ! S,
then S is countable.
Hint: A Computer Science proof involves Ô¨Åltering for duplicates.
Homework Problems
Problem 7.8.
Prove that if A is an inÔ¨Ånite set and C is a countable set, then
A bij A [ C:
Hint: See Problem 7.4.
Problem 7.9.
In this problem you will prove a fact that may surprise you ‚Äîor make you even
more convinced that set theory is nonsense: the half-open unit interval is actually
the same size as the nonnegative quadrant of the real plane!4 Namely, there is a
bijection from .0; 1¬ç to ≈í0; 1/2.
(a) Describe a bijection from .0; 1¬ç to ≈í0; 1/.
Hint: 1=x almost works.
(b) An inÔ¨Ånite sequence of the decimal digits f0; 1; : : : ; 9g will be called long if
it has inÔ¨Ånitely many occurrences of some digit other than 0. Let L be the set of
all such long sequences. Describe a bijection from L to the half-open real interval
.0; 1¬ç.
Hint: Put a decimal point at the beginning of the sequence.
4The half open unit interval, .0; 1¬ç, is fr 2 R j 0 < r  1g. Similarly, ≈í0; 1/ WWD fr 2 R j r  0g.

Chapter 7
InÔ¨Ånite Sets
196
(c) Describe a surjective function from L to L2 that involves alternating digits
from two long sequences. Hint: The surjection need not be total.
(d) Prove the following lemma and use it to conclude that there is a bijection from
L2 to .0; 1¬ç2.
Lemma 7.4.1. Let A and B be nonempty sets. If there is a bijection from A to B,
then there is also a bijection from A  A to B  B.
(e) Conclude from the previous parts that there is a surjection from .0; 1¬ç and
.0; 1¬ç2. Then appeal to the Schr¬®oder-Bernstein Theorem to show that there is actu-
ally a bijection from .0; 1¬ç and .0; 1¬ç2.
(f) Complete the proof that there is a bijection from .0; 1¬ç to ≈í0; 1/2.
Exam Problems
Problem 7.10.
Prove that if A0; A1; : : : ; An; : : : is an inÔ¨Ånite sequence of countable sets, then so
is
1
[
nD0
An
Problem 7.11.
Let A and B denote two countably inÔ¨Ånite sets:
A D fa0; a1; a2; a3; : : :g
B D fb0; b1; b2; b3; : : :g
Show that their product, A  B, is also a countable set by showing how to list
the elements of A  B. You need only show enough of the initial terms in your
sequence to make the pattern clear ‚Äîa half dozen or so terms usually sufÔ¨Åce.
Problem 7.12. (a) Prove that if A and B are countable sets, then so is A [ B.
(b) Prove that if C is a countable set and D is inÔ¨Ånite, then there is a bijection
between D and C [ D.
Problem 7.13.
Let f0; 1g! be the uncountable set of inÔ¨Ånite binary sequences, and let Fn 

7.4. Does All This Really Work?
197
f0; 1g! be the set of inÔ¨Ånite binary sequences whose bits are all 0 after the nth
bit. That is, if s WWD .s0; s1; s2; : : : / 2 f0; 1g!, then
s 2 Fn IFF 8i > n: si D 0:
For example, the sequence t that starts 001101 with 0‚Äôs after that is in F5, since by
deÔ¨Ånition ti D 0 for all i > 5. In fact, t is by deÔ¨Ånition also in F6; F7; : : : .
(a) What is the size, jFnj, of Fn?
(b) Explain why the set F  f0; 1g! of sequences with only Ô¨Ånitely many 1‚Äôs, is
a countable set.
(c) Prove that the set of inÔ¨Ånite binary sequences with inÔ¨Ånitely many 1‚Äôs is un-
countable. Hint: Use parts (a) and (b); a direct proof by diagonalization is tricky.
Problem 7.14.
A real number is called quadratic when it is a root of a degree two polynomial with
integer coefÔ¨Åcients. Explain why there are only countably many quadratic reals.
Problems for Section 7.2
Class Problems
Problem 7.15.
Let N! be the set of inÔ¨Ånite sequences of nonnegative integers. For example, some
sequences of this kind are:
.0; 1; 2; 3; 4; : : : /;
.2; 3; 5; 7; 11; : : : /;
.3; 1; 4; 5; 9; : : : /:
Prove that this set of sequences is uncountable.
Problem 7.16.
There are lots of different sizes of inÔ¨Ånite sets. For example, starting with the
inÔ¨Ånite set, N, of nonnegative integers, we can build the inÔ¨Ånite sequence of sets
N strict pow.N/ strict pow.pow.N// strict pow.pow.pow.N/// strict : : : :

Chapter 7
InÔ¨Ånite Sets
198
where each set is ‚Äústrictly smaller‚Äù than the next one by Theorem 7.1.6.
Let
pown.N/ be the nth set in the sequence, and
U WWD
1
[
nD0
n
pow.N/:
Prove that
n
pow.N/ strict U
for all n 2 N.
Now of course, we could take U; pow.U /; pow.pow.U //; : : : and keep on in this
way building still bigger inÔ¨Ånities indeÔ¨Ånitely.
Problem 7.17.
The method used to prove Cantor‚Äôs Theorem that the power set is ‚Äúbigger‚Äù than the
set, leads to many important results in logic and computer science. In this problem
we‚Äôll apply that idea to describe a set of binary strings that can‚Äôt be described by
ordinary logical formulas. To be provocative, we could say that we will describe
an undescribable set of strings!
The following logical formula illustrates how a formula can describe a set of
strings. The formula
NOT≈í9y: 9z: s D y1z¬ç;
(no-1s.s/)
where the variables range over the set, f0; 1g, of Ô¨Ånite binary strings, says that the
binary string, s, does not contain a 1.
We‚Äôll call such a predicate formula, G.s/, about strings a string formula, and
we‚Äôll use the notation strings.G/ for the set of binary strings with the property
described by G. That is,
strings.G/ WWD fs 2 f0; 1g j G.s/g:
A set of binary strings is describable if it equals strings.G/ for some string for-
mula, G. So the set, 0, of Ô¨Ånite strings of 0‚Äôs is describable because it equals
strings.no-1s/.5
The idea of representing data in binary is a no-brainer for a computer scientist, so
it won‚Äôt be a stretch to agree that any string formula can be represented by a binary
string. We‚Äôll use the notation Gx for the string formula with binary representation
5no-1s and similar formulas were examined in Problem 3.21, but it is not necessary to have done
that problem to do this one.

7.4. Does All This Really Work?
199
x 2 f0; 1g. The details of the representation don‚Äôt matter, except that there ought
to be a display procedure that can actually display Gx given x.
Standard binary representations of formulas are often based on character-by-
character translation into binary, which means that only a sparse set of binary
strings actually represent string formulas. It will be technically convenient to have
every binary string represent some string formula. This is easy to do: tweak the
display procedure so it displays some default formula, say no-1s, when it gets a bi-
nary string that isn‚Äôt a standard representation of a string formula. With this tweak,
every binary string, x, will now represent a string formula, Gx.
Now we have just the kind of situation where a Cantor-style diagonal argu-
ment can be applied, namely, we‚Äôll ask whether a string describes a property of
itself! That may sound like a mind-bender, but all we‚Äôre asking is whether x 2
strings.Gx/.
For example, using character-by-character translations of formulas into binary,
neither the string 0000 nor the string 10 would be the binary representation of a
formula, so the display procedure applied to either of them would display no-1s.
That is, G0000 D G10 D no-1s and so strings.G0000/ D strings.G10/ D 0. This
means that
0000 2 strings.G0000/
and
10 ‚Ä¶ strings.G10/:
Now we are in a position to give a precise mathematical description of an ‚Äúun-
describable‚Äù set of binary strings, namely, let
Theorem. DeÔ¨Åne
U WWD fx 2 f0; 1g j x ‚Ä¶ strings.Gx/g:
(7.6)
The set U is not describable.
Use reasoning similar to Cantor‚Äôs Theorem 7.1.6 to prove this Theorem.
Homework Problems
Problem 7.18.
For any sets, A, and B, let ≈íA ! B¬ç be the set of total functions from A to B.
Prove that if A is not empty and B has more than one element, then NOT.A surj
≈íA ! B¬ç/.
Hint: Suppose that  is a function from A to ≈íA ! B¬ç mapping each element
a 2 A to a function a W A ! B. Pick any two elements of B; call them 0 and 1.
Then deÔ¨Åne
diag.a/ WWD
(
0 if a.a/ D 1;
1 otherwise:

Chapter 7
InÔ¨Ånite Sets
200
Exam Problems
Problem 7.19.
Let f1; 2; 3g! be the set of inÔ¨Ånite sequences containing only the numbers 1, 2, and
3. For example, some sequences of this kind are:
.1; 1; 1; 1:::/;
.2; 2; 2; 2:::/;
.3; 2; 1; 3:::/:
Prove that f1; 2; 3g! is uncountable.
Hint: One approach is to deÔ¨Åne a surjective function from f1; 2; 3g! to the power
set pow.N/.
Problems for Section 7.3
Class Problems
Problem 7.20.
The Axiom of Choice says that if s is a set whose members are nonempty sets that
are pairwise disjoint ‚Äîthat is no two sets in s have an element in common ‚Äîthen
there is a set, c, consisting of exactly one element from each set in s.
In formal logic, we could describe s with the formula,
pairwise-disjoint.s/ WWD
8x 2 s: x ¬§ ;AND8x; y 2 s: x ¬§ y IMPLIES x\y D ;:
Similarly we could describe c with the formula
choice-set.c; s/ WWD
8x 2 s: 9≈†z: z 2 c \ x:
Here ‚Äú9≈†z:‚Äù is fairly standard notation for ‚Äúthere exists a unique z.‚Äù
Now we can give the formal deÔ¨Ånition:
DeÔ¨Ånition (Axiom of Choice).
8s: pairwise-disjoint.s/ IMPLIES 9c: choice-set.c; s/:
The only issue here is that Set Theory is technically supposed to be expressed
in terms of pure formulas in the language of sets, which means formula that uses
only the membership relation, 2, propositional connectives, the two quantiÔ¨Åes 8
and 9, and variables ranging over all sets. Verify that the Axiom of Choice can be
expressed as a pure formula, by explaining how to replace all impure subformulas
above with equivalent pure formulas.
For example, the formula x D y could be replaced with the pure formula 8z: z 2
x IFF z 2 y.

7.4. Does All This Really Work?
201
Problem 7.21.
Let R W A ! A be a binary relation on a set, A. If a1 R a0, we‚Äôll say that a1 is ‚ÄúR-
smaller‚Äù than a0. R is called well founded when there is no inÔ¨Ånite ‚ÄúR-decreasing‚Äù
sequence:
   R an R    R a1 R a0;
(7.7)
of elements ai 2 A.
For example, if A D N and R is the <-relation, then R is well founded because
if you keep counting down with nonnegative integers, you eventually get stuck at
zero:
0 <    < n   1 < n:
But you can keep counting up forever, so the >-relation is not well founded:
   > n >    > 1 > 0:
Also, the -relation on N is not well founded because a constant sequence of, say,
2‚Äôs, gets -smaller forever:
    2      2  2:
(a) If B is a subset of A, an element b 2 B is deÔ¨Åned to be R-minimal in B iff
there is no R-smaller element in B. Prove that R W A ! A is well founded iff every
nonempty subset of A has an R-minimal element.
A logic formula of set theory has only predicates of the form ‚Äúx 2 y‚Äù for vari-
ables x; y ranging over sets, along with quantiÔ¨Åers and propositional operations.
For example,
isempty.x/ WWD 8w: NOT.w 2 x/
is a formula of set theory that means that ‚Äúx is empty.‚Äù
(b) Write a formula, member-minimal.u; v/, of set theory that means that u is
2-minimal in v.
(c) The Foundation axiom of set theory says that 2 is a well founded relation
on sets. Express the Foundation axiom as a formula of set theory. You may use
‚Äúmember-minimal‚Äù and ‚Äúisempty‚Äù in your formula as abbreviations for the formu-
las deÔ¨Åned above.
(d) Explain why the Foundation axiom implies that no set is a member of itself.


8
Number Theory
Number theory is the study of the integers. Why anyone would want to study the
integers is not immediately obvious. First of all, what‚Äôs to know? There‚Äôs 0, there‚Äôs
1, 2, 3, and so on, and, oh yeah, -1, -2, .... Which one don‚Äôt you understand?
Second, what practical value is there in it?
The mathematician G. H. Hardy delighted at its impracticality; he wrote:
[Number theorists] may be justiÔ¨Åed in rejoicing that there is one sci-
ence, at any rate, and that their own, whose very remoteness from or-
dinary human activities should keep it gentle and clean.
Hardy was specially concerned that number theory not be used in warfare; he
was a paciÔ¨Åst. You may applaud his sentiments, but he got it wrong: number theory
underlies modern cryptography, which is what makes secure online communication
possible. Secure communication is of course crucial in war ‚Äîwhich may leave
poor Hardy spinning in his grave. It‚Äôs also central to online commerce. Every time
you buy a book from Amazon, use a certiÔ¨Åcate to access a web page, or use a
PayPal account, you are relying on number theoretic algorithms.
Number theory also provides an excellent environment for us to practice and
apply the proof techniques that we developed in previous chapters. We‚Äôll work out
properties of greatest common divisors (gcd‚Äôs) and use them to prove that integers
factor uniquely into primes. Then we‚Äôll introduce modular arithmetic and work out
enough of its properties to explain the RSA public key crypto-system.
Since we‚Äôll be focusing on properties of the integers, we‚Äôll adopt the default
convention in this chapter that variables range over the set, Z, of integers.
8.1
Divisibility
The nature of number theory emerges as soon as we consider the divides relation.
DeÔ¨Ånition 8.1.1. a divides b (notation a j b) iff there is an integer k such that
ak D b:
The divides relation comes up so frequently that multiple synonyms for it are
used all the time. The following phrases all say the same thing:

Chapter 8
Number Theory
204
 a j b,
 a divides b,
 a is a divisor of b,
 a is a factor of b,
 b is divisible by a,
 b is a multiple of a.
Some immediate consequences of DeÔ¨Ånition 8.1.1 are that for all n
n j 0;
n j n; and
Àô 1 j n:
Also,
0 j n IMPLIES n D 0:
Dividing seems simple enough, but let‚Äôs play with this deÔ¨Ånition. The Pythagore-
ans, an ancient sect of mathematical mystics, said that a number is perfect if it
equals the sum of its positive integral divisors, excluding itself.
For example,
6 D 1 C 2 C 3 and 28 D 1 C 2 C 4 C 7 C 14 are perfect numbers. On the
other hand, 10 is not perfect because 1 C 2 C 5 D 8, and 12 is not perfect because
1 C 2 C 3 C 4 C 6 D 16. Euclid characterized all the even perfect numbers around
300 BC (see Problem 8.3). But is there an odd perfect number? More than two
thousand years later, we still don‚Äôt know! All numbers up to about 10300 have been
ruled out, but no one has proved that there isn‚Äôt an odd perfect number waiting just
over the horizon.
So a half-page into number theory, we‚Äôve strayed past the outer limits of human
knowledge. This is pretty typical; number theory is full of questions that are easy to
pose, but incredibly difÔ¨Åcult to answer. We‚Äôll mention a few more such questions
in later sections.1
8.1.1
Facts about Divisibility
The following lemma collects some basic facts about divisibility.
Lemma 8.1.2.
1. If a j b and b j c, then a j c.
1Don‚Äôt Panic ‚Äîwe‚Äôre going to stick to some relatively benign parts of number theory. These
super-hard unsolved problems rarely get put on problem sets.

8.1. Divisibility
205
2. If a j b and a j c, then a j sb C tc for all s and t.
3. For all c ¬§ 0, a j b if and only if ca j cb.
Proof. These facts all follow directly from DeÔ¨Ånition 8.1.1. To illustrate this, we‚Äôll
prove just part 2:
Given that a j b, there is some k1 2 Z such that ak1 D b. Likewise, ak2 D c,
so
sb C tc D s.k1a/ C t.k2a/ D .sk1 C tk2/a:
Therefore sb C tc D k3a where k3 WWD .sk1 C tk2/, which means that
a j sb C tc:

A number of the form sb Ctc is called an integer linear combination of b and c,
or, since in this chapter we‚Äôre only talking about integers, just a linear combination.
So Lemma 8.1.2.2 can be rephrased as
If a divides b and c, then a divides every linear combination of b and c.
We‚Äôll be making good use of linear combinations, so let‚Äôs get the general deÔ¨Ånition
on record:
DeÔ¨Ånition 8.1.3. An integer n is a linear combination of numbers b0; : : : ; bk iff
n D s0b0 C s1b1 C    C skbk
for some integers s0; : : : ; sk.
8.1.2
When Divisibility Goes Bad
As you learned in elementary school, if one number does not evenly divide another,
you get a ‚Äúquotient‚Äù and a ‚Äúremainder‚Äù left over. More precisely:
Theorem 8.1.4. [Division Theorem]2 Let n and d be integers such that d > 0.
Then there exists a unique pair of integers q and r, such that
n D q  d C r AND 0  r < d:
(8.1)
2This theorem is often called the ‚ÄúDivision Algorithm,‚Äù but we prefer to call it a theorem since it
does not actually describe a division procedure for computing the quotient and remainder.

Chapter 8
Number Theory
206
The number q is called the quotient and the number r is called the remainder of
n divided by d. We use the notation qcnt.n; d/ for the quotient and rem .n; d/ for
the remainder. For example, qcnt.2716; 10/ D 271 and rem .2716; 10/ D 6, since
2716 D 271  10 C 6. Similarly, rem . 11; 7/ D 3, since  11 D . 2/  7 C 3.
There is a remainder operator built into many programming languages. For ex-
ample, ‚Äú32 % 5‚Äù will be familiar as remainder notation to programmers in Java, C,
and C++; it evaluates to rem .32; 5/ D 2 in all three languages. On the other hand,
these languages treat remainders involving negative numbers idiosyncratically, so if
you program in one those languages, remember to stick to the deÔ¨Ånition according
to the Division Theorem 8.1.4.
The remainder on division by n is a number in the (integer) interval from 0 to
n   1. Such intervals come up so often that it is useful to have a simple notation for
them.
.k; n/ WWD
fi j k < i < ng;
.k; n¬ç WWD
.k; n/ [ fng;
≈ík; n/ WWD
fkg [ .k; n/;
≈ík; n¬ç WWD
fkg [ .k; n/ [ fng D fi j k  i  ng:
8.1.3
Die Hard
Die Hard 3 is just a B-grade action movie, but we think it has an inner message:
everyone should learn at least a little number theory. In Section 5.4.4, we formal-
ized a state machine for the Die Hard jug-Ô¨Ålling problem using 3 and 5 gallon jugs,
and also with 3 and 9 gallon jugs, and came to different conclusions about bomb
explosions. What‚Äôs going on in general? For example, how about getting 4 gallons
from 12- and 18-gallon jugs, getting 32 gallons with 899- and 1147-gallon jugs, or
getting 3 gallons into a jug using just 21- and 26-gallon jugs?
It would be nice if we could solve all these silly water jug questions at once. This
is where number theory comes in handy.
A Water Jug Invariant
Suppose that we have water jugs with capacities a and b with b  a. Let‚Äôs carry
out some sample operations of the state machine and see what happens, assuming

8.1. Divisibility
207
the b-jug is big enough:
.0; 0/ ! .a; 0/
Ô¨Åll Ô¨Årst jug
! .0; a/
pour Ô¨Årst into second
! .a; a/
Ô¨Åll Ô¨Årst jug
! .2a   b; b/
pour Ô¨Årst into second (assuming 2a  b)
! .2a   b; 0/
empty second jug
! .0; 2a   b/
pour Ô¨Årst into second
! .a; 2a   b/
Ô¨Åll Ô¨Årst
! .3a   2b; b/
pour Ô¨Årst into second (assuming 3a  2b)
What leaps out is that at every step, the amount of water in each jug is a linear
combination of a and b. This is easy to prove by induction on the number of
transitions:
Lemma 8.1.5 (Water Jugs). In the Die Hard state machine of Section 5.4.4 with
jugs of sizes a and b, the amount of water in each jug is always a linear combination
of a and b.
Proof. The induction hypothesis, P.n/, is the proposition that after n transitions,
the amount of water in each jug is a linear combination of a and b.
Base case (n D 0): P.0/ is true, because both jugs are initially empty, and 0  a C
0  b D 0.
Inductive step: Suppose the machine is in state .x; y/ after n steps, that is, the little
jug contains x gallons and the big one contains y gallons. There are two cases:
 If we Ô¨Åll a jug from the fountain or empty a jug into the fountain, then that jug
is empty or full. The amount in the other jug remains a linear combination
of a and b. So P.n C 1/ holds.
 Otherwise, we pour water from one jug to another until one is empty or the
other is full. By our assumption, the amount x and y in each jug is a linear
combination of a and b before we begin pouring. After pouring, one jug is
either empty (contains 0 gallons) or full (contains a or b gallons). Thus, the
other jug contains either x C y gallons, x C y   a, or x C y   b gallons, all
of which are linear combinations of a and b since x and y are. So P.n C 1/
holds in this case as well.
Since P.n C 1/ holds in any case, this proves the inductive step, completing the
proof by induction.


Chapter 8
Number Theory
208
So we have established that the jug problem has a preserved invariant, namely,
the amount of water in every jug is a linear combination of the capacities of the
jugs. Lemma 8.1.5 has an important corollary:
Corollary. Getting 4 gallons from 12- and 18-gallon jugs, and likewise getting 32
gallons from 899- and 1147-gallon jugs,
Bruce dies!
Proof. By the Water Jugs Lemma 8.1.5, with 12- and 18-gallon jugs, the amount
in any jug is a linear combination of 12 and 18. This is always a multiple of 6 by
Lemma 8.1.2.2, so Bruce can‚Äôt get 4 gallons. Likewise, the amount in any jug using
899- and 1147-gallon jugs is a multiple of 31, so he can‚Äôt get 32 either.

But the Water Jugs Lemma doesn‚Äôt tell the complete story. For example, it leaves
open the question of getting 3 gallons into a jug using just 21- and 26-gallon jugs:
the only positive factor of both 21 and 26 is 1, and of course 1 divides 3, so the
Lemma neither rules out nor conÔ¨Årms the possibility of getting 3 gallons.
A bigger issue is that we‚Äôve just managed to recast a pretty understandable ques-
tion about water jugs into a technical question about linear combinations. This
might not seem like a lot of progress. Fortunately, linear combinations are closely
related to something more familiar, namely greatest common divisors, and these
will help us solve the general water jug problem.
8.2
The Greatest Common Divisor
A common divisor of a and b is a number that divides them both. The greatest
common divisor of a and b is written gcd.a; b/. For example, gcd.18; 24/ D 6.
As long as a and b are not both 0, they will have a gcd. The gcd turns out to be a
very valuable piece of information about the relationship between a and b and for
reasoning about integers in general. We‚Äôll be making lots of use of gcd‚Äôs in what
follows.
Some immediate consequences of the deÔ¨Ånition of gcd are that for n > 0,
gcd.n; n/ D n;
gcd.n; 1/ D 1; and
gcd.n; 0/ D n;
where the last equality follows from the fact that everything is a divisor of 0.

8.2. The Greatest Common Divisor
209
8.2.1
Euclid‚Äôs Algorithm
The Ô¨Årst thing to Ô¨Ågure out is how to Ô¨Ånd gcd‚Äôs. A good way called Euclid‚Äôs
Algorithm has been known for several thousand years. It is based on the following
elementary observation.
Lemma 8.2.1. For b ¬§ 0,
gcd.a; b/ D gcd.b; rem .a; b//:
Proof. By the Division Theorem 8.1.4,
a D qb C r
(8.2)
where r D rem .a; b/. So a is a linear combination of b and r, which implies that
any divisor of b and r is a divisor of a by Lemma 8.1.2.2. Likewise, r is a linear
combination, a   qb, of a and b, so any divisor of a and b is a divisor of r. This
means that a and b have the same common divisors as b and r, and so they have
the same greatest common divisor.

Lemma 8.2.1 is useful for quickly computing the greatest common divisor of
two numbers. For example, we could compute the greatest common divisor of
1147 and 899 by repeatedly applying it:
gcd.1147; 899/ D gcd
0
@899; rem .1147; 899/
‚Äû
∆í‚Äö
‚Ä¶
D248
1
A
D gcd .248; rem .899; 248/ D 155/
D gcd .155; rem .248; 155/ D 93/
D gcd .93; rem .155; 93/ D 62/
D gcd .62; rem .93; 62/ D 31/
D gcd .31; rem .62; 31/ D 0/
D 31
This calculation that gcd.1147; 899/ D 31 was how we Ô¨Ågured out that with water
jugs of sizes 1147 and 899, Bruce dies trying to get 32 gallons.
On the other hand, applying Euclid‚Äôs algorithm to 26 and 21 gives
gcd.26; 21/ D gcd.21; 5/ D gcd.5; 1/ D 1;
so we can‚Äôt use the reasoning above to rule out Bruce getting 3 gallons into the big
jug. As a matter of fact, because the gcd here is 1, Bruce will be able to get any
number of gallons into the big jug up to its capacity. To explain this, we will need
a little more number theory.

Chapter 8
Number Theory
210
Euclid‚Äôs Algorithm as a State Machine
By the way, Euclid‚Äôs algorithm can easily be formalized as a state machine. The
set of states is N2 and there is one transition rule:
.x; y/  ! .y; rem .x; y//;
(8.3)
for y > 0. By Lemma 8.2.1, the gcd stays the same from one state to the next. That
means the predicate
gcd.x; y/ D gcd.a; b/
is a preserved invariant on the states .x; y/. This preserved invariant is, of course,
true in the start state .a; b/. So by the Invariant Principle, if y ever becomes 0, the
invariant will be true and so
x D gcd.x; 0/ D gcd.a; b/:
Namely, the value of x will be the desired gcd.
What‚Äôs more, x, and therefore also y, gets to be 0 pretty fast. To see why, note
that starting from .x; y/, two transitions leads to a state whose the Ô¨Årst coordinate
is rem .x; y/, which is at most half the size of x.3 Since x starts off equal to a and
gets halved or smaller every two steps, it will reach its minimum value ‚Äîwhich is
gcd.a; b/ ‚Äîafter at most 2 log a transitions. After that, the algorithm takes at most
one more transition to terminate. In other words, Euclid‚Äôs algorithm terminates
after at most 1 C 2 log a transitions.4
8.2.2
The Pulverizer
We will get a lot of mileage out of the following key fact:
Theorem 8.2.2. The greatest common divisor of a and b is a linear combination
of a and b. That is,
gcd.a; b/ D sa C tb;
for some integers s and t.
We already know from Lemma 8.1.2.2 that every linear combination of a and b is
divisible by any common factor of a and b, so it is certainly divisible by the greatest
3In other words,
rem .x; y/  x=2
for 0 < y  x:
(8.4)
This is immediate if y  x=2, since the remainder of x divided by y is less than y by deÔ¨Ånition. On
the other hand, if y > x=2, then rem .x; y/ D x   y < x=2.
4A tighter analysis shows that at most log'.a/ transitions are possible where ' is the golden ratio
.1 C
p
5/=2, see Problem 8.10.

8.2. The Greatest Common Divisor
211
of these common divisors. Since any constant multiple of a linear combination is
also a linear combination, Theorem 8.2.2 implies that any multiple of the gcd is a
linear combination, giving:
Corollary 8.2.3. An integer is a linear combination of a and b iff it is a multiple of
gcd.a; b/.
We‚Äôll prove Theorem 8.2.2 directly by explaining how to Ô¨Ånd s and t. This job
is tackled by a mathematical tool that dates back to sixth-century India, where it
was called kuttak, which means ‚ÄúThe Pulverizer.‚Äù Today, the Pulverizer is more
commonly known as ‚Äúthe extended Euclidean GCD algorithm,‚Äù because it is so
close to Euclid‚Äôs Algorithm.
For example, following Euclid‚Äôs Algorithm, we can compute the GCD of 259
and 70 as follows:
gcd.259; 70/ D gcd.70; 49/
since rem .259; 70/ D 49
D gcd.49; 21/
since rem .70; 49/ D 21
D gcd.21; 7/
since rem .49; 21/ D 7
D gcd.7; 0/
since rem .21; 7/ D 0
D 7:
The Pulverizer goes through the same steps, but requires some extra bookkeeping
along the way: as we compute gcd.a; b/, we keep track of how to write each of
the remainders (49, 21, and 7, in the example) as a linear combination of a and b.
This is worthwhile, because our objective is to write the last nonzero remainder,
which is the GCD, as such a linear combination. For our example, here is this extra
bookkeeping:
x
y
.rem .x; y//
D
x   q  y
259
70
49
D
259   3  70
70
49
21
D
70   1  49
D
70   1  .259   3  70/
D
 1  259 C 4  70
49
21
7
D
49   2  21
D
.259   3  70/   2  . 1  259 C 4  70/
D
3  259   11  70
21
7
0
We began by initializing two variables, x D a and y D b. In the Ô¨Årst two columns
above, we carried out Euclid‚Äôs algorithm. At each step, we computed rem .x; y/
which equals x   qcnt.x; y/  y. Then, in this linear combination of x and y, we

Chapter 8
Number Theory
212
replaced x and y by equivalent linear combinations of a and b, which we already
had computed. After simplifying, we were left with a linear combination of a and
b equal to rem .x; y/, as desired. The Ô¨Ånal solution is boxed.
This should make it pretty clear how and why the Pulverizer works. If you have
doubts, it may help to work through Problem 8.9, where the Pulverizer is formalized
as a state machine and then veriÔ¨Åed using an invariant that is an extension of the
one used for Euclid‚Äôs algorithm.
Since the Pulverizer requires only a little more computation than Euclid‚Äôs algo-
rithm, you can ‚Äúpulverize‚Äù very large numbers very quickly by using this algorithm.
As we will soon see, its speed makes the Pulverizer a very useful tool in the Ô¨Åeld
of cryptography.
Now we can restate the Water Jugs Lemma 8.1.5 in terms of the greatest common
divisor:
Corollary 8.2.4. Suppose that we have water jugs with capacities a and b. Then
the amount of water in each jug is always a multiple of gcd.a; b/.
For example, there is no way to form 4 gallons using 3- and 6-gallon jugs, be-
cause 4 is not a multiple of gcd.3; 6/ D 3.
8.2.3
One Solution for All Water Jug Problems
Corollary 8.2.3 says that 3 can be written as a linear combination of 21 and 26,
since 3 is a multiple of gcd.21; 26/ D 1. So the Pulverizer will give us integers s
and t such that
3 D s  21 C t  26
(8.5)
Now the coefÔ¨Åcient s could be either positive or negative. However, we can
readily transform this linear combination into an equivalent linear combination
3 D s0  21 C t0  26
(8.6)
where the coefÔ¨Åcient s0 is positive. The trick is to notice that if in equation (8.5) we
increase s by 26 and decrease t by 21, then the value of the expression s 21Ct 26
is unchanged overall. Thus, by repeatedly increasing the value of s (by 26 at a
time) and decreasing the value of t (by 21 at a time), we get a linear combination
s0  21 C t0  26 D 3 where the coefÔ¨Åcient s0 is positive. (Of course t0 must then be
negative; otherwise, this expression would be much greater than 3.)
Now we can form 3 gallons using jugs with capacities 21 and 26: We simply
repeat the following steps s0 times:
1. Fill the 21-gallon jug.

8.2. The Greatest Common Divisor
213
2. Pour all the water in the 21-gallon jug into the 26-gallon jug. If at any time
the 26-gallon jug becomes full, empty it out, and continue pouring the 21-
gallon jug into the 26-gallon jug.
At the end of this process, we must have emptied the 26-gallon jug exactly  t0
times. Here‚Äôs why: we‚Äôve taken s0  21 gallons of water from the fountain, and
we‚Äôve poured out some multiple of 26 gallons. If we emptied fewer than  t0 times,
then by (8.6), the big jug would be left with at least 3 C 26 gallons, which is more
than it can hold; if we emptied it more times, the big jug would be left containing
at most 3 26 gallons, which is nonsense. But once we have emptied the 26-gallon
jug exactly  t0 times, equation (8.6) implies that there are exactly 3 gallons left.
Remarkably, we don‚Äôt even need to know the coefÔ¨Åcients s0 and t0 in order to
use this strategy! Instead of repeating the outer loop s0 times, we could just repeat
until we obtain 3 gallons, since that must happen eventually. Of course, we have to
keep track of the amounts in the two jugs so we know when we‚Äôre done. Here‚Äôs the
solution using this approach starting with empty jugs, that is, at .0; 0/:
Ô¨Åll 21
   !
.21; 0/
pour 21 into 26
         !
.0; 21/
Ô¨Åll 21
   !
.21; 21/
pour 21 to 26
       !
.16; 26/
empty 26
     !
.16; 0/
pour 21 to 26
       !
.0; 16/
Ô¨Åll 21
   !
.21; 16/
pour 21 to 26
       !
.11; 26/
empty 26
     !
.11; 0/
pour 21 to 26
       !
.0; 11/
Ô¨Åll 21
   !
.21; 11/
pour 21 to 26
       !
.6; 26/
empty 26
     !
.6; 0/
pour 21 to 26
       !
.0; 6/
Ô¨Åll 21
   !
.21; 6/
pour 21 to 26
       !
.1; 26/
empty 26
     !
.1; 0/
pour 21 to 26
       !
.0; 1/
Ô¨Åll 21
   !
.21; 1/
pour 21 to 26
       !
.0; 22/
Ô¨Åll 21
   !
.21; 22/
pour 21 to 26
       !
.17; 26/
empty 26
     !
.17; 0/
pour 21 to 26
       !
.0; 17/
Ô¨Åll 21
   !
.21; 17/
pour 21 to 26
       !
.12; 26/
empty 26
     !
.12; 0/
pour 21 to 26
       !
.0; 12/
Ô¨Åll 21
   !
.21; 12/
pour 21 to 26
       !
.7; 26/
empty 26
     !
.7; 0/
pour 21 to 26
       !
.0; 7/
Ô¨Åll 21
   !
.21; 7/
pour 21 to 26
       !
.2; 26/
empty 26
     !
.2; 0/
pour 21 to 26
       !
.0; 2/
Ô¨Åll 21
   !
.21; 2/
pour 21 to 26
       !
.0; 23/
Ô¨Åll 21
   !
.21; 23/
pour 21 to 26
       !
.18; 26/
empty 26
     !
.18; 0/
pour 21 to 26
       !
.0; 18/
Ô¨Åll 21
   !
.21; 18/
pour 21 to 26
       !
.13; 26/
empty 26
     !
.13; 0/
pour 21 to 26
       !
.0; 13/
Ô¨Åll 21
   !
.21; 13/
pour 21 to 26
       !
.8; 26/
empty 26
     !
.8; 0/
pour 21 to 26
       !
.0; 8/
Ô¨Åll 21
   !
.21; 8/
pour 21 to 26
       !
.3; 26/
empty 26
     !
.3; 0/
pour 21 to 26
       !
.0; 3/
The same approach works regardless of the jug capacities and even regardless of
the amount we‚Äôre trying to produce! Simply repeat these two steps until the desired
amount of water is obtained:

Chapter 8
Number Theory
214
1. Fill the smaller jug.
2. Pour all the water in the smaller jug into the larger jug. If at any time the
larger jug becomes full, empty it out, and continue pouring the smaller jug
into the larger jug.
By the same reasoning as before, this method eventually generates every multiple
‚Äîup to the size of the larger jug ‚Äîof the greatest common divisor of the jug capac-
ities, namely, all the quantities we can possibly produce. No ingenuity is needed at
all!
So now we have the complete water jug story:
Theorem 8.2.5. Suppose that we have water jugs with capacities a and b. For
any c 2 ≈í0; a¬ç, it is possible to get c gallons in the size a jug iff c is a multiple of
gcd.a; b/.
8.3
Prime Mysteries
Some of the greatest mysteries and insights in number theory concern properties of
prime numbers:
DeÔ¨Ånition 8.3.1. A prime is a number greater than 1 that is divisible only by itself
and 1. A number other than 0, 1, and  1 that is not a prime is called composite.5
Here are three famous mysteries:
Twin Prime Conjecture There are inÔ¨Ånitely many primes p such that p C2 is also
a prime.
In 1966 Chen showed that there are inÔ¨Ånitely many primes p such that p C2
is the product of at most two primes. So the conjecture is known to be almost
true!
Conjectured InefÔ¨Åciency of Factoring Given the product of two large primes n D
pq, there is no efÔ¨Åcient procedure to recover the primes p and q. That is, no
polynomial time procedure (see Section 3.5) guaranteed to Ô¨Ånd p and q in a
number of steps bounded by a polynomial log n, which is the number of bits
in the binary representation of n.
5So 0, 1, and  1 are the only integers that are neither prime nor composite.

8.3. Prime Mysteries
215
The best algorithm known is the ‚Äúnumber Ô¨Åeld sieve,‚Äù which runs in time
proportional to:
e1:9.ln n/1=3.ln ln n/2=3:
This number grows more rapidly than any polynomial in log n and is infea-
sible when n has 300 digits or more.
EfÔ¨Åcient factoring is a mystery of particular importance in computer science,
as we‚Äôll explain later in this chapter.
Goldbach Conjecture We‚Äôve already mentioned Goldbach‚Äôs Conjecture 1.1.8 sev-
eral times: every even integer greater than two is equal to the sum of two
primes. For example, 4 D 2 C 2, 6 D 3 C 3, 8 D 3 C 5, etc.
In 1939 Schnirelman proved that every even number can be written as the
sum of not more than 300,000 primes, which was a start. Today, we know
that every even number is the sum of at most 6 primes.
Primes show up erratically in the sequence of integers. In fact, their distribution
seems almost random:
2; 3; 5; 7; 11; 13; 17; 19; 23; 29; 31; 37; 41; 43; : : : :
One of the great insights about primes is that their density among the integers has
a precise limit. Namely, let .n/ denote the number of primes up to n:
DeÔ¨Ånition 8.3.2.
.n/ WWD jfp 2 ≈í2; n¬ç j p is primegj:
For example, .1/ D 0; .2/ D 1, and .10/ D 4 because 2, 3, 5, and 7 are the
primes less than or equal to 10. Step by step,  grows erratically according to the
erratic spacing between successive primes, but its overall growth rate is known to
smooth out to be the same as the growth of the function n= ln n:
Theorem 8.3.3 (Prime Number Theorem).
lim
n!1
.n/
n= ln n D 1:
Thus, primes gradually taper off. As a rule of thumb, about 1 integer out of every
ln n in the vicinity of n is a prime.
The Prime Number Theorem was conjectured by Legendre in 1798 and proved
a century later by de la Vallee Poussin and Hadamard in 1896. However, after
his death, a notebook of Gauss was found to contain the same conjecture, which

Chapter 8
Number Theory
216
he apparently made in 1791 at age 15. (You sort of have to feel sorry for all the
otherwise ‚Äúgreat‚Äù mathematicians who had the misfortune of being contemporaries
of Gauss.)
A proof of the Prime Number Theorem is beyond our scope, but there is a man-
ageable proof (see Problem 8.14) of a related result that is sufÔ¨Åcient for our appli-
cations:
Theorem 8.3.4 (Chebyshev‚Äôs Theorem on Prime Density). For n > 1,
.n/ >
n
3 ln n:
A Prime for Google
In late 2004 a billboard appeared in various locations around the country:
 Ô¨Årst 10-digit prime found
in consecutive digits of e

. com
Substituting the correct number for the expression in curly-braces produced the
URL for a Google employment page. The idea was that Google was interested in
hiring the sort of people that could and would solve such a problem.
How hard is this problem? Would you have to look through thousands or millions
or billions of digits of e to Ô¨Ånd a 10-digit prime? The rule of thumb derived from
the Prime Number Theorem says that among 10-digit numbers, about 1 in
ln 1010  23
is prime. This suggests that the problem isn‚Äôt really so hard! Sure enough, the
Ô¨Årst 10-digit prime in consecutive digits of e appears quite early:
e D2:718281828459045235360287471352662497757247093699959574966
9676277240766303535475945713821785251664274274663919320030
599218174135966290435729003342952605956307381323286279434 : : :

8.4. The Fundamental Theorem of Arithmetic
217
8.4
The Fundamental Theorem of Arithmetic
There is an important fact about primes that you probably already know: every
positive integer number has a unique prime factorization. So every positive integer
can be built up from primes in exactly one way. These quirky prime numbers are
the building blocks for the integers.
Since the value of a product of numbers is the same if the numbers appear in a
different order, there usually isn‚Äôt a unique way to express a number as a product
of primes. For example, there are three ways to write 12 as a product of primes:
12 D 2  2  3 D 2  3  2 D 3  2  2:
What‚Äôs unique about the prime factorization of 12 is that any product of primes
equal to 12 will have exactly one 3 and two 2‚Äôs. This means that if we sort the
primes by size, then the product really will be unique.
Let‚Äôs state this more carefully. A sequence of numbers is weakly decreasing
when each number in the sequence is at least as big as the numbers after it. Note
that a sequence of just one number as well as a sequence of no numbers ‚Äîthe
empty sequence ‚Äîis weakly decreasing by this deÔ¨Ånition.
Theorem 8.4.1. [Fundamental Theorem of Arithmetic] Every positive integer is a
product of a unique weakly decreasing sequence of primes.
For example, 75237393 is the product of the weakly decreasing sequence of
primes
23; 17; 17; 11; 7; 7; 7; 3;
and no other weakly decreasing sequence of primes will give 75237393.6
Notice that the theorem would be false if 1 were considered a prime; for example,
15 could be written as 5  3, or 5  3  1, or 5  3  1  1, ....
There is a certain wonder in unique factorization, especially in view of the prime
number mysteries we‚Äôve already mentioned. It‚Äôs a mistake to take it for granted,
even if you‚Äôve known it since you were in a crib. In fact, unique factorization actu-
ally fails for many integer-like sets of numbers, for example, the complex numbers
of the form n C m
p
 5 for m; n 2 Z (see Problem 8.16).
The Fundamental Theorem is also called the Unique Factorization Theorem,
which is a more descriptive, less pretentious, name ‚Äîbut hey, we really want to
get your attention to the importance and non-obviousness of unique factorization.
6The ‚Äúproduct‚Äù of just one number is deÔ¨Åned to be that number, and the product of no numbers is
by convention deÔ¨Åned to be 1. So each prime, p, is uniquely the product of the primes in the length-
one sequence consisting solely of p, and 1, which remember is not a prime, is even so uniquely the
product of the empty sequence.

Chapter 8
Number Theory
218
8.4.1
Proving Unique Factorization
The Fundamental Theorem is not hard to prove, but we‚Äôll need a couple of prelim-
inary facts.
Lemma 8.4.2. If p is a prime and p j ab, then p j a or p j b.
Now Lemma 8.4.2 follows immediately from Unique Factorization: the primes
in the product ab are exactly the primes from a and from b. But proving the lemma
this way would be cheating: we‚Äôre going to need this lemma to prove Unique Fac-
torization, and it would be circular to assume it. Instead, we‚Äôll use the proper-
ties of gcd‚Äôs and linear combinations to give an easy, noncircular way to prove
Lemma 8.4.2.
Proof. One case is if gcd.a; p/ D p. Then the claim holds, because a is a multiple
of p.
Otherwise, gcd.a; p/ ¬§ p. In this case gcd.a; p/ must be 1, since 1 and p are
the only positive divisors of p. Now gcd.a; p/ is a linear combination of a and p,
so we have 1 D sa C tp for some s; t. Then b D s.ab/ C .tb/p, that is, b is a
linear combination of ab and p. Since p divides both ab and p, it also divides their
linear combination b.

A routine induction argument extends this statement to:
Lemma 8.4.3. Let p be a prime. If p j a1a2    an, then p divides some ai.
Now we‚Äôre ready to prove the Fundamental Theorem of Arithmetic.
Proof. Theorem 2.3.1 showed, using the Well Ordering Principle, that every posi-
tive integer can be expressed as a product of primes. So we just have to prove this
expression is unique. We will use Well Ordering to prove this too.
The proof is by contradiction: assume, contrary to the claim, that there exist
positive integers that can be written as products of primes in more than one way.
By the Well Ordering Principle, there is a smallest integer with this property. Call
this integer n, and let
n D p1  p2    pj ;
D q1  q2    qk;
where both products are in weakly decreasing order and p1  q1.
If q1 D p1, then n=q1 would also be the product of different weakly decreasing
sequences of primes, namely,
p2    pj ;
q2    qk:

8.5. Alan Turing
219
Figure 8.1
Alan Turing
Since n=q1 < n, this can‚Äôt be true, so we conclude that p1 < q1.
Since the pi‚Äôs are weakly decreasing, all the pi‚Äôs are less than q1. But
q1 j n D p1  p2    pj ;
so Lemma 8.4.3 implies that q1 divides one of the pi‚Äôs, which contradicts the fact
that q1 is bigger than all them.

8.5
Alan Turing
The man pictured in Figure 8.1 is Alan Turing, the most important Ô¨Ågure in the
history of computer science. For decades, his fascinating life story was shrouded
by government secrecy, societal taboo, and even his own deceptions.
At age 24, Turing wrote a paper entitled On Computable Numbers, with an Ap-
plication to the Entscheidungsproblem. The crux of the paper was an elegant way
to model a computer in mathematical terms. This was a breakthrough, because it
allowed the tools of mathematics to be brought to bear on questions of computation.
For example, with his model in hand, Turing immediately proved that there exist
problems that no computer can solve ‚Äîno matter how ingenious the programmer.
Turing‚Äôs paper is all the more remarkable because he wrote it in 1936, a full decade

Chapter 8
Number Theory
220
before any electronic computer actually existed.
The word ‚ÄúEntscheidungsproblem‚Äù in the title refers to one of the 28 mathemat-
ical problems posed by David Hilbert in 1900 as challenges to mathematicians of
the 20th century. Turing knocked that one off in the same paper. And perhaps
you‚Äôve heard of the ‚ÄúChurch-Turing thesis‚Äù? Same paper. So Turing was obviously
a brilliant guy who generated lots of amazing ideas. But this lecture is about one of
Turing‚Äôs less-amazing ideas. It involved codes. It involved number theory. And it
was sort of stupid.
Let‚Äôs look back to the fall of 1937. Nazi Germany was rearming under Adolf
Hitler, world-shattering war looked imminent, and ‚Äîlike us ‚ÄîAlan Turing was
pondering the usefulness of number theory. He foresaw that preserving military
secrets would be vital in the coming conÔ¨Çict and proposed a way to encrypt com-
munications using number theory. This is an idea that has ricocheted up to our own
time. Today, number theory is the basis for numerous public-key cryptosystems,
digital signature schemes, cryptographic hash functions, and electronic payment
systems. Furthermore, military funding agencies are among the biggest investors
in cryptographic research. Sorry Hardy!
Soon after devising his code, Turing disappeared from public view, and half a
century would pass before the world learned the full story of where he‚Äôd gone and
what he did there. We‚Äôll come back to Turing‚Äôs life in a little while; for now, let‚Äôs
investigate the code Turing left behind. The details are uncertain, since he never
formally published the idea, so we‚Äôll consider a couple of possibilities.
8.5.1
Turing‚Äôs Code (Version 1.0)
The Ô¨Årst challenge is to translate a text message into an integer so we can perform
mathematical operations on it. This step is not intended to make a message harder
to read, so the details are not too important. Here is one approach: replace each
letter of the message with two digits (A D 01, B D 02, C D 03, etc.) and string all
the digits together to form one huge number. For example, the message ‚Äúvictory‚Äù
could be translated this way:
v
i
c
t
o
r
y
!
22
09
03
20
15
18
25
Turing‚Äôs code requires the message to be a prime number, so we may need to pad
the result with some more digits to make a prime. The Prime Number Theorem
indicates that padding with relatively few digits will work. In this case, appending
the digits 13 gives the number 2209032015182513, which is prime.
Here is how the encryption process works. In the description below, m is the
unencoded message (which we want to keep secret), m is the encrypted message
(which the Nazis may intercept), and k is the key.

8.5. Alan Turing
221
Beforehand The sender and receiver agree on a secret key, which is a large prime k.
Encryption The sender encrypts the message m by computing:
m D m  k
Decryption The receiver decrypts m by computing:
m
k
D m:
For example, suppose that the secret key is the prime number k D 22801763489
and the message m is ‚Äúvictory.‚Äù Then the encrypted message is:
m D m  k
D 2209032015182513  22801763489
D 50369825549820718594667857
There are a couple of basic questions to ask about Turing‚Äôs code.
1. How can the sender and receiver ensure that m and k are prime numbers, as
required?
The general problem of determining whether a large number is prime or com-
posite has been studied for centuries, and tests for primes that worked well
in practice were known even in Turing‚Äôs time. In the past few decades, fast,
guaranteed primality tests have been found as described in the text box below.
2. Is Turing‚Äôs code secure?
The Nazis see only the encrypted message m D m  k, so recovering the
original message m requires factoring m. Despite immense efforts, no re-
ally efÔ¨Åcient factoring algorithm has ever been found. It appears to be a
fundamentally difÔ¨Åcult problem. So, although a breakthrough someday can‚Äôt
be ruled out, the conjecture that there is no efÔ¨Åcient way to factor is widely
accepted. In effect, Turing‚Äôs code puts to practical use his discovery that
there are limits to the power of computation. Thus, provided m and k are
sufÔ¨Åciently large, the Nazis seem to be out of luck!
This all sounds promising, but there is a major Ô¨Çaw in Turing‚Äôs code.

Chapter 8
Number Theory
222
Primality Testing
It‚Äôs easy to see that an integer n is prime iff it is not divisible by any number from
2 to
pn
Àò
(see Problem 1.7). Of course this naive way to test if n is prime takes
more than pn steps, which is exponential in the size of n measured by the number
of digits in the decimal or binary representation of n. Through the early 1970‚Äôs,
no prime testing procedure was known that would never blow up like this.
In 1974, Volker Strassen invented a simple, fast probabilistic primality test.
Strassens‚Äôs test gives the right answer when applied to any prime number, but
has some probability of giving a wrong answer on a nonprime number. However,
the probability of a wrong answer on any given number is so tiny that relying on
the answer is the best bet you‚Äôll ever make.
Still, the theoretical possibility of a wrong answer was intellectually bothersome
‚Äîeven if the probability of being wrong was a lot less than the probability of an
undetectable computer hardware error leading to a wrong answer. Finally in 2002,
in an amazing, breakthrough paper beginning with a quote from Gauss emphasiz-
ing the importance and antiquity of primality testing, Manindra Agrawal, Neeraj
Kayal, and Nitin Saxena presented a thirteen line description of a polynomial time
primality test.
In particular, the Agrawal et al. test is guaranteed to give the correct answer about
primality of any number n in about .log n/12 steps, that is, a number of steps
bounded by a twelfth degree polynomial in the length (in bits) of the input, n.
This deÔ¨Ånitively places primality testing way below the problems of exponential
difÔ¨Åculty.
Unfortunately, a running time that grows like a 12th degree polynomial is much
too slow for practical purposes, and probabilistic primality tests remain the
method used in practice today. It‚Äôs reasonable to expect that improved nonproba-
bilistic tests will be discovered, but matching the speed of the known probabilistic
tests remains a daunting challenge.

8.6. Modular Arithmetic
223
8.5.2
Breaking Turing‚Äôs Code (Version 1.0)
Let‚Äôs consider what happens when the sender transmits a second message using
Turing‚Äôs code and the same key. This gives the Nazis two encrypted messages to
look at:
m
1 D m1  k
and
m
2 D m2  k
The greatest common divisor of the two encrypted messages, m
1 and m
2, is the
secret key k. And, as we‚Äôve seen, the GCD of two numbers can be computed very
efÔ¨Åciently. So after the second message is sent, the Nazis can recover the secret key
and read every message!
A mathematician as brilliant as Turing is not likely to have overlooked such a
glaring problem, and we can guess that he had a slightly different system in mind,
one based on modular arithmetic.
8.6
Modular Arithmetic
On the Ô¨Årst page of his masterpiece on number theory, Disquisitiones Arithmeticae,
Gauss introduced the notion of ‚Äúcongruence.‚Äù Now, Gauss is another guy who
managed to cough up a half-decent idea every now and then, so let‚Äôs take a look
at this one. Gauss said that a is congruent to b modulo n iff n j .a   b/. This is
written
a  b
.mod n/:
For example:
29  15
.mod 7/
because 7 j .29   15/:
It‚Äôs not useful to allow a modulus n  1, and so we will assume from now on
that moduli are greater than 1.
There is a close connection between congruences and remainders:
Lemma 8.6.1 (Remainder).
a  b
.mod n/
iff
rem .a; n/ D rem .b; n/ :
Proof. By the Division Theorem 8.1.4, there exist unique pairs of integers q1; r1
and q2; r2 such that:
a D q1n C r1
b D q2n C r2;

Chapter 8
Number Theory
224
where r1; r2 2 ≈í0; n/. Subtracting the second equation from the Ô¨Årst gives:
a   b D .q1   q2/n C .r1   r2/;
where r1 r2 is in the interval . n; n/. Now a  b .mod n/ if and only if n divides
the left side of this equation. This is true if and only if n divides the right side, which
holds if and only if r1   r2 is a multiple of n. Given the bounds on r1   r2, this
happens precisely when r1 D r2, that is, when rem .a; n/ D rem .b; n/.

So we can also see that
29  15
.mod 7/
because rem .29; 7/ D 1 D rem .15; 7/ :
Notice that even though ‚Äú(mod 7)‚Äù appears on the end, the  symbol isn‚Äôt any more
strongly associated with the 15 than with the 29. It would really be clearer to write
29 mod 7 15 for example, but the notation with the modulus at the end is Ô¨Årmly
entrenched, and we‚Äôll just live with it.
The Remainder Lemma 8.6.1 explains why the congruence relation has proper-
ties like an equality relation. In particular, the following properties7 follow imme-
diately:
Lemma 8.6.2.
a  a
.mod n/
(reÔ¨Çexivity)
a  b
iff
b  a
.mod n/
(symmetry)
.a  b and b  c/
implies
a  c
.mod n/
(transitivity)
We‚Äôll make frequent use of another immediate corollary of the Remainder Lemma 8.6.1:
Corollary 8.6.3.
a  rem .a; n/
.mod n/
Still another way to think about congruence modulo n is that it deÔ¨Ånes a partition
of the integers into n sets so that congruent numbers are all in the same set. For
example, suppose that we‚Äôre working modulo 3. Then we can partition the integers
into 3 sets as follows:
f
: : : ;
 6;
 3;
0;
3;
6;
9;
: : :
g
f
: : : ;
 5;
 2;
1;
4;
7;
10;
: : :
g
f
: : : ;
 4;
 1;
2;
5;
8;
11;
: : :
g
7Binary relations with these properties are called equivalence relations, see Section 9.10.

8.7. Remainder Arithmetic
225
according to whether their remainders on division by 3 are 0, 1, or 2. The upshot
is that when arithmetic is done modulo n there are really only n different kinds
of numbers to worry about, because there are only n possible remainders. In this
sense, modular arithmetic is a simpliÔ¨Åcation of ordinary arithmetic.
The next most useful fact about congruences is that they are preserved by addi-
tion and multiplication:
Lemma 8.6.4 (Congruence). If a  b .mod n/ and c  d .mod n/, then
1. a C c  b C d .mod n/,
2. ac  bd .mod n/.
Proof. We have that n divides .b   a/ which is equal to .b C c/   .a C c/, so
a C c  b C c
.mod n/:
Also, n divides .d   c/, so by the same reasoning
b C c  b C d
.mod n/:
Combining these according to Lemma 8.6.2, we get
a C c  b C d
.mod n/:
The proof for multiplication is virtually identical, using the fact that if n divides
.b   a/, then it obviously divides .bc   ac/ as well.

8.7
Remainder Arithmetic
The Congruence Lemma 8.6.1 says that two numbers are congruent iff their remain-
ders are equal, so we can understand congruences by working out arithmetic with
remainders. And if all we want is the remainder modulo n of a series of additions,
multiplications, subtractions applied to some numbers, we can take remainders at
every step so that the entire computation only involves number in the range ≈í0; n/.

Chapter 8
Number Theory
226
General Principle of Remainder Arithmetic
To Ô¨Ånd the remainder modulo n of the result of a series of additions and multipli-
cations, applied to some integers
 replace each integer by its remainder modulo n,
 keep each result of an addition or multiplication in the range ≈í0; n/ by im-
mediately replacing any result outside that range by its remainder on divi-
sion by n.
For example, suppose we want to Ô¨Ånd
rem
 .444273456789 C 155558585555/4036666666; 36

:
(8.7)
This looks really daunting if you think about computing these large powers and
then taking remainders. For example, the decimal representation of 444273456789
has about 20 million digits, so we certainly don‚Äôt want to go that route. But re-
membering that integer exponents specify a series of multiplications, we follow the
General Principle and replace the numbers being multiplied by their remainders.
Since rem .44427; 36/ D 3; rem .15555858; 36/ D 6, and rem .403; 36/ D 7, we
Ô¨Ånd that (8.7) equals the remainder on division by 36 of
.33456789 C 65555/76666666:
(8.8)
That‚Äôs a little better, but 33456789 has about a million digits in its decimal represen-
tation, so we still don‚Äôt want to compute that. But let‚Äôs look at the remainders of
the Ô¨Årst few powers of 3:
rem .3; 36/ D 3
rem
 32; 36

D 9
rem
 33; 36

D 27
rem
 34; 36

D 9:
We got a repeat of the second step, rem
 32; 36

after just two more steps. This
means means that starting at 32, the sequence of remainders of successive powers
of 3 will keep repeating every 2 steps. So a product of an odd number of three of
more 3‚Äôs will have the same remainder modulo 36 as a product of just three 3‚Äôs.
Therefore,
rem
 33456789; 36

D rem
 33; 36

D 27:

8.7. Remainder Arithmetic
227
What a win!
Powers of 6 are even easier because rem
 62; 36

D 0, so 0‚Äôs keep repeating
after the second step. Powers of 7 repeat after six steps, but on the Ô¨Åfth step you get
a 1, so (8.8) successively simpliÔ¨Åes to be the remainders of the following terms:
.33456789 C 65555/76666666
.33 C 62  65553/.76/1111111
.33 C 0  65553/11111111
D 27:
Notice that it would be a disastrous blunder to replace an exponent by its
remainder. The General Principle applies to numbers that are operands of plus and
times, whereas the exponent is a number that controls how many multiplications to
perform. Watch out for this blunder.
8.7.1
The ring Zn
It‚Äôs time to be more precise about the General Principle and why it works. To begin,
let‚Äôs introduce the notation Cn for doing an addition and then immediately taking
a remainder on division by n, as speciÔ¨Åed by the General Principle; likewise for
multiplying:
i Cn j WWD rem .i C j; n/ ;
i n j WWD rem .ij; n/ :
The General Principle is simply the repeated application of the following lemma
which provides the formal justiÔ¨Åcation for remainder arithmetic:
Lemma 8.7.1.
rem .i C j; n/ D rem .i; n/ Cn rem .j; n/ ;
(8.9)
rem .ij; n/ D rem .i; n/ n rem .j; n/ :
(8.10)
Proof. By Corollary 8.6.3, i  rem .i; n/ and j  rem .j; n/, so by the Congru-
ence Lemma 8.6.4
i C j  rem .i; n/ C rem .j; n/
.mod n/:
By Corollary 8.6.3 again, the remainders on each side of this congruence are equal,
which immediately gives (8.9). An identical proof applies to (8.10).


Chapter 8
Number Theory
228
The set of integers in the range ≈í0; n/ together with the operations Cn and n is
referred to as Zn, the ring of integers modulo n. As a consequence of Lemma 8.7.1,
the familiar rules of arithmetic hold in Zn, for example:8
.i n j / n k D i n .j n k/
(associativity of n);
.i Cn j / Cn k D i Cn .j Cn k/
(associativity of Cn);
1 n k D k
(identity for n);
0 Cn k D k
(identity for Cn);
k Cn . k/ D 0
(inverse for Cn);
i Cn j D j Cn i
(commutativity of Cn)
i n .j Cn k/ D .i n j/ Cn .i n k/
(distributivity);
i n j D j n i
(commutativity of n)
Associativity implies the familiar fact that it‚Äôs safe to omit the parentheses in
products:
k1 n k2 n    n km
comes out the same no matter how it is parenthesized.
The overall theme is that remainder arithmetic is a lot like ordinary arithmetic.
But there are a couple of exceptions we‚Äôre about to examine.
8.8
Turing‚Äôs Code (Version 2.0)
In 1940, France had fallen before Hitler‚Äôs army, and Britain stood alone against
the Nazis in western Europe. British resistance depended on a steady Ô¨Çow of sup-
plies brought across the north Atlantic from the United States by convoys of ships.
These convoys were engaged in a cat-and-mouse game with German ‚ÄúU-boats‚Äù ‚Äî
submarines ‚Äîwhich prowled the Atlantic, trying to sink supply ships and starve
Britain into submission. The outcome of this struggle pivoted on a balance of in-
formation: could the Germans locate convoys better than the Allies could locate
U-boats or vice versa?
Germany lost.
8A set with addition and multiplication operations that satisy these equalities is known as a com-
mutative ring. In addition to Zn, the integers, reals, and polynomials with integer coefÔ¨Åcients, are all
examples of commutative rings. On the other hand, the set fT; Fg of truth values with OR for addition
and AND for multiplication is not a ring; it satisÔ¨Åes most, but not all, of these equalities.

8.8. Turing‚Äôs Code (Version 2.0)
229
But a critical reason behind Germany‚Äôs loss was made public only in 1974: Ger-
many‚Äôs naval code, Enigma, had been broken by the Polish Cipher Bureau9 and the
secret had been turned over to the British a few weeks before the Nazi invasion of
Poland in 1939. Throughout much of the war, the Allies were able to route con-
voys around German submarines by listening in to German communications. The
British government didn‚Äôt explain how Enigma was broken until 1996. When it
was Ô¨Ånally released (by the US), the story revealed that Alan Turing had joined the
secret British codebreaking effort at Bletchley Park in 1939, where he became the
lead developer of methods for rapid, bulk decryption of German Enigma messages.
Turing‚Äôs Enigma deciphering was an invaluable contribution to the Allied victory
over Hitler.
Governments are always tight-lipped about cryptography, but the half-century of
ofÔ¨Åcial silence about Turing‚Äôs role in breaking Enigma and saving Britain may be
related to some disturbing events after the war. More on that later. Let‚Äôs get back to
number theory and consider an alternative interpretation of Turing‚Äôs code. Perhaps
we had the basic idea right (multiply the message by the key), but erred in using
conventional arithmetic instead of modular arithmetic. Maybe this is what Turing
meant:
Beforehand The sender and receiver agree on a large number n, which may be
made public. (This will be the modulus for all our arithmetic.) As in Version
1.0, they also agree that some prime number k < n will be the secret key.
Encryption As in Version 1.0, the message m should be another prime in ≈í0; n/.
The sender encrypts the message m to produce m by computing mk, but
this time in Zn:
m WWD m n k
(8.11)
Decryption (Uh-oh.)
The decryption step is a problem. We might hope to decrypt in the same way as
before by dividing the encrypted message m by the key k. The difÔ¨Åculty is that
m is the remainder when mk is divided by n. So dividing m by k might not even
give us an integer!
This decoding difÔ¨Åculty can be overcome with a better understanding of when it
is ok to divide by k in modular arithmetic.
9See http://en.wikipedia.org/wiki/Polish Cipher Bureau.

Chapter 8
Number Theory
230
8.9
Multiplicative Inverses and Cancelling
The multiplicative inverse of a number x is another number x 1 such that
x 1  x D 1:
From now on, when we say ‚Äúinverse,‚Äù we mean multiplicative inverse.
For example, over the rational numbers, 1=3 is, of course, an inverse of 3, since,
1
3  3 D 1:
In fact, with the sole exception of 0, every rational number n=m has an inverse,
namely, m=n. On the other hand, over the integers, only 1 and -1 have inverses.
Over the ring Zn, things get a little more complicated. For example, in Z15, 2 is a
multiplicative inverse of 8, since
2 15 8 D 1:
On the other hand, 3 does not have a multiplicative inverse in Z15. We can prove
this by contradiction: suppose there was an inverse j for 3, that is
1 D 3 15 j
Then multiplying both sides of this equality by 5 ‚Äîin the ring Z15 ‚Äîleads directly
to the contradiction 5 D 0:
5 D 5 15 .3 15 j/
D .5 15 3/ 15 j
D 0 15 j D 0;
So there can‚Äôt be any such inverse j.
So some numbers have inverses modulo 15 and others don‚Äôt. This may seem a
little unsettling at Ô¨Årst, but there‚Äôs a simple explanation of what‚Äôs going on.
8.9.1
Relative Primality
Integers that have no prime factor in common are called relatively prime.10 This
is the same as having no common divisor (prime or not) greater than 1. It is also
equivalent to saying gcd.a; b/ D 1.
For example, 8 and 15 are relatively prime, since gcd.8; 15/ D 1. On the other
hand, 3 and 15 are not relatively prime, since gcd.3; 15/ D 3 ¬§ 1. This turns out
to explain why 8 has an inverse over Z15 and 3 does not.
10Other texts call them coprime.

8.9. Multiplicative Inverses and Cancelling
231
Lemma 8.9.1. If k is relatively prime to n, then k has an inverse in Zn.
Proof. If k is relatively prime to n, then gcd.n; k/ D 1 by deÔ¨Ånition of gcd. So we
can use the Pulverizer from section 8.2.2 to Ô¨Ånd a linear combination of n and k
equal to 1:
sn C tk D 1:
So taking remainders of division by n of both sides of this equality, and then apply-
ing Lemma 8.7.1, we get
.rem .s; n/ n rem .n; n// Cn .rem .t; n/ n k/ D 1:
But rem .n; n/ D 0, so
rem .t; n/ n k D 1
Thus, rem .t; n/ is a multiplicative inverse of k.

By the way, it‚Äôs nice to know that when they exist, inverses are unique. That is,
Lemma 8.9.2. If i and j are both inverses of k in Zn, then i D j .
Proof.
i D i n 1 D i n .k n j/ D .i n k/ n j D 1 n j D j:

So the proof of Lemma 8.9.1 shows that the unique inverse in Zn for any k
relatively prime to n can be found simply by taking the remainder of the coefÔ¨Åcient
of k in a linear combination of k and n that equals 1.
Notice that working with a prime modulus, p, is attractive because, like the ratio-
nal and real numbers, in Zp every nonzero number has an inverse. But arithmetic
modulo a composite is really only a little more painful than working modulo a
prime ‚Äîthough you may think this is like the doctor saying, ‚ÄúThis is only going to
hurt a little,‚Äù before he jams a big needle in your arm.
8.9.2
Cancellation
Another sense in which real numbers are nice is that it‚Äôs ok to cancel common
factors. In other words, if we know that rt D st for real numbers r; s; t, then
as long as t ¬§ 0, we can cancel the t‚Äôs and conclude that r D s. In general,
cancellation is not valid in Zn. For example,
4 15 10 D 1 15 10
.mod 15/;
but cancelling the 10‚Äôs leads to the absurd conclusion that 4 equals 1.
The fact that multiplicative terms cannot be canceled is the most signiÔ¨Åcant way
in which congruences differ from ordinary integer equations.

Chapter 8
Number Theory
232
DeÔ¨Ånition 8.9.3. A number k is cancellable modulo in Zn iff
a n k D b n k
implies
a D b
for all a; b 2 ≈í0; n/.
If a number is relatively prime to 15, it can be cancelled by multiplying by its
inverse. So cancelling obviously works for numbers that have inverses:
Lemma 8.9.4. If k has an inverse modulo n, then k is cancellable modulo n.
But 10 is not relatively prime to 15, and that‚Äôs why it is not cancellable. More
generally, if k is not relatively prime to n, then it‚Äôs easy to see that it isn‚Äôt can-
cellable in Zn. Namely, suppose gcd.k; n/ D m > 1. So k=m and n=m are
positive integers, and we have
.n=m/  k D n  .k=m/;
rem ..n=m/  k; n/ D rem .n  .k=m/; n/ ;
.n=m/ n k D 0 D 0 n k:
Now k can‚Äôt be cancelled or we would reach the false conclusion that n=m D 0.
To summarize, we have
Theorem 8.9.5. The following are equivalent for k 2 ≈í0; n/:
gcd.k; n/ D 1;
k has an inverse in Zn;
k is cancellable in Zn:
8.9.3
Decrypting (Version 2.0)
Multiplicative inverses are the key to decryption in Turing‚Äôs code. SpeciÔ¨Åcally,
we can recover the original message by multiplying the encoded message by the
Zn-inverse, j , of the key:
m n j D .m n k/ n j D m n .k n j/ D m n 1 D m:
So all we need to decrypt the message is to Ô¨Ånd an inverse of the secret key k, which
will be easy using the Pulverizer ‚Äîproviding k has an inverse. But k is positive
and less than the modulus n, so one simple way to ensure that k is relatively prime
to the modulus is to have n be a prime number.

8.9. Multiplicative Inverses and Cancelling
233
8.9.4
Breaking Turing‚Äôs Code (Version 2.0)
The Germans didn‚Äôt bother to encrypt their weather reports with the highly-secure
Enigma system. After all, so what if the Allies learned that there was rain off the
south coast of Iceland? But, amazingly, this practice provided the British with a
critical edge in the Atlantic naval battle during 1941.
The problem was that some of those weather reports had originally been trans-
mitted using Enigma from U-boats out in the Atlantic. Thus, the British obtained
both unencrypted reports and the same reports encrypted with Enigma. By com-
paring the two, the British were able to determine which key the Germans were
using that day and could read all other Enigma-encoded trafÔ¨Åc. Today, this would
be called a known-plaintext attack.
Let‚Äôs see how a known-plaintext attack would work against Turing‚Äôs code. Sup-
pose that the Nazis know both the plain text, m, and its encrypted form, m. Now
in Version 2.0,
m D m n k
and since m is positive and less than the prime n, the Nazis can use the Pulverizer
to Ô¨Ånd the Zn-inverse, j , of m. Now
j n m D j n .m n k/ D .j n m/ n k D 1 n k D k:
So by computing j n m D k, the Nazis get the secret key and can then decrypt
any message!
This is a huge vulnerability, so Turing‚Äôs hypothetical Version 2.0 code has no
practical value. Fortunately, Turing got better at cryptography after devising this
code; his subsequent deciphering of Enigma messages surely saved thousands of
lives, if not the whole of Britain.
8.9.5
Turing Postscript
A few years after the war, Turing‚Äôs home was robbed. Detectives soon determined
that a former homosexual lover of Turing‚Äôs had conspired in the robbery. So they
arrested him ‚Äîthat is, they arrested Alan Turing ‚Äîbecause homosexuality was a
British crime punishable by up to two years in prison at that time. Turing was
sentenced to a hormonal ‚Äútreatment‚Äù for his homosexuality: he was given estrogen
injections. He began to develop breasts.
Three years later, Alan Turing, the founder of computer science, was dead. His
mother explained what happened in a biography of her own son. Despite her re-
peated warnings, Turing carried out chemistry experiments in his own home. Ap-
parently, her worst fear was realized: by working with potassium cyanide while
eating an apple, he poisoned himself.

Chapter 8
Number Theory
234
However, Turing remained a puzzle to the very end. His mother was a devout
woman who considered suicide a sin. And, other biographers have pointed out,
Turing had previously discussed committing suicide by eating a poisoned apple.
Evidently, Alan Turing, who founded computer science and saved his country, took
his own life in the end, and in just such a way that his mother could believe it was
an accident.
Turing‚Äôs last project before he disappeared from public view in 1939 involved the
construction of an elaborate mechanical device to test a mathematical conjecture
called the Riemann Hypothesis. This conjecture Ô¨Årst appeared in a sketchy paper by
Bernhard Riemann in 1859 and is now one of the most famous unsolved problems
in mathematics.
8.10
Euler‚Äôs Theorem
The RSA cryptosystem examined in the next section, and other current schemes
for encoding secret messages, involve computing remainders of numbers raised to
large powers. A basic fact about remainders of powers follows from a theorem due
to Euler about congruences.
DeÔ¨Ånition 8.10.1. For n > 0, deÔ¨Åne11
.n/ WWD the number of integers in ≈í0; n/, that are relatively prime to n.
This function  is known as Euler‚Äôs  function.12
For example, .7/ D 6 because all 6 positive numbers in ≈í0; 7/ are relatively
prime to the prime number 7. Only 0 is not relatively prime to 7. Also, .12/ D 4
since 1, 5, 7, and 11 are the only numbers in ≈í0; 12/ that are relatively prime to 12.
More generally, if p is prime, then .p/ D p   1 since every positive number in
≈í0; p/ is relatively prime to p. When n is composite, however, the  function gets
a little complicated. We‚Äôll get back to it in the next section.
Theorem 8.10.2 (Euler‚Äôs Theorem). If n and k are relatively prime, then
k.n/  1
.mod n/:
(8.12)
11Since 0 is not relatively prime to anything, .n/ could equivalently be deÔ¨Åned using the interval
≈í1; n/ instead of ≈í0; n/.
12Some texts call it Euler‚Äôs totient function.

8.10. Euler‚Äôs Theorem
235
The Riemann Hypothesis
The formula for the sum of an inÔ¨Ånite geometric series says:
1 C x C x2 C x3 C    D
1
1   x
Substituting x D
1
2s , x D
1
3s , x D
1
5s , and so on for each prime number gives a
sequence of equations:
1 C 1
2s C 1
22s C 1
23s C    D
1
1   1=2s
1 C 1
3s C 1
32s C 1
33s C    D
1
1   1=3s
1 C 1
5s C 1
52s C 1
53s C    D
1
1   1=5s
etc.
Multiplying together all the left sides and all the right sides gives:
1
X
nD1
1
ns D
Y
p2primes

1
1   1=ps

The sum on the left is obtained by multiplying out all the inÔ¨Ånite series and ap-
plying the Fundamental Theorem of Arithmetic. For example, the term 1=300s
in the sum is obtained by multiplying 1=22s from the Ô¨Årst equation by 1=3s in
the second and 1=52s in the third. Riemann noted that every prime appears in the
expression on the right. So he proposed to learn about the primes by studying
the equivalent, but simpler expression on the left. In particular, he regarded s as
a complex number and the left side as a function, .s/. Riemann found that the
distribution of primes is related to values of s for which .s/ D 0, which led to
his famous conjecture:
DeÔ¨Ånition 8.9.6. The Riemann Hypothesis: Every nontrivial zero of the zeta
function .s/ lies on the line s D 1=2 C ci in the complex plane.
A proof would immediately imply, among other things, a strong form of the Prime
Number Theorem.
Researchers continue to work intensely to settle this conjecture, as they have for
over a century. It is another of the Millennium Problems whose solver will earn
$1,000,000 from the Clay Institute.

Chapter 8
Number Theory
236
Rephrased in terms of the ring Zn, (8.12) is equivalent to
k.n/ D 1
.Zn/
(8.13)
Here, and in the rest of this section, the arithmetic is done in Zn. In particular,
k.n/ is the n-product of k with itself .n/ times.
Equation (8.13) will follow from a series of easy lemmas.
DeÔ¨Ånition 8.10.3. Let gcd1fng be the integers in ≈í0; n/, that are relatively prime
to n:13
gcd1fng WWD fk 2 ≈í0; n/ j gcd.k; n/ D 1g:
(8.14)
Consequently,
.n/ D j gcd1fngj:
We know every element in gcd1fng has a Zn-inverse (Theorem 8.9.5) and there-
fore is cancellable. Also gcd1fng is closed under multiplication in Zn:
Lemma 8.10.4. If j; k 2 gcd1fng, then j  k 2 gcd1fng.
There are lots of easy ways to prove this (see Problem 8.45).
DeÔ¨Ånition 8.10.5. DeÔ¨Åne the order of k 2 ≈í0; n/ over Zn to be
ord.k; n/ WWD minfm  0 j km D 1g:
If no power of k equals 1 in Zn, then ord.k; n/ WWD 1.
Lemma 8.10.6. Every element of gcd1fng has Ô¨Ånite order.
Proof. Suppose k 2 gcd1fng. We need to show is that some power of k over Zn
equals 1.
But since gcd1fng has fewer than n elements, some number must occur twice in
the list
k1; k2; : : : ; kn:
That is,
kiCm D ki
(8.15)
for some m > 0 and i 2 ≈í0; n/. But k is cancellable over Zn, so we can cancel the
Ô¨Årst i of the k‚Äôs on both sides of (8.15) to get
km D 1:

13Other texts use the notation n for gcd1fng.

8.10. Euler‚Äôs Theorem
237
Now let‚Äôs work out an example that illustrates the remaining ideas needed to
prove Euler‚Äôs Theorem. Suppose n D 28, so
gcd1f28g D f1; 3; 5; 9; 11; 13; 15; 17; 19; 23; 25; 27g; and
(8.16)
.28/ D j gcd1f28gj D 12:
We pick any element of gcd1f28g, for example, 9. Let P9 be all the positive
powers of 9 in Z28, so
P9 WWD f9; 92; : : : ; 9k; : : :g:
The order of 9 in Z28 turns out to be 3, since 92 D 25 and 93 D 1. So P9 really
has just these 3 elements:
P9 D f9; 25; 1g:
DeÔ¨Ånition 8.10.7. For any m 2 ≈í0; n/ and subset P  ≈í0; n/, deÔ¨Åne
mP WWD fm  p j p 2 P g:
Let‚Äôs look at 3P9. Multiplying each of the elements in P9 by 3 gives
3P9 D f27; 19; 3g:
The Ô¨Årst thing to notice is that 3P9 also has 3 elements. We could have predicted
this: different elements of P9 must map to different elements of 3P9 since 3 2
gcd1f28g is cancellable.
Lemma 8.10.8. For any set, P  ≈í0; n/, if k 2 gcd1fng, then
jP j D jkP j:
(8.17)
Proof. DeÔ¨Åne a function fk W P ! kP by the rule
fk.p/ WWD k  p:
The function fk is total and surjective by deÔ¨Ånition. It is also an injection because
fk.p1/ D fk.p2/
means
k  p1 D k  p2;
which implies that p1 D p2 since k 2 gcd1fng is cancellable. This shows that fk
is a bijection, and (8.17) follows by the Mapping Rule 4.5.


Chapter 8
Number Theory
238
Continuing with the example, the next number in the list (8.16) of elements of
gcd1f28g is 5, so let‚Äôs look at
5P9 D f17; 13; 5g:
Again 5P9 has 3 elements since 5 2 gcd1f28g, but now notice something else: 5P9
has no elements in common with 3P9, and neither 3P9 nor 5P9 have any elements
in common with P9. The following lemma explains this.
Lemma 8.10.9. Let Pk WWDfk; k2; : : : ; ki; : : :g be the set of powers of some element
k 2 gcd1fng, and suppose a; b 2 ≈í0; n/. If the sets aPk and bPk have an element
in common, then aPk D bPk.
Proof. So suppose aPk and bPk have an element in common. That is,
aki D bkj
for some i; j  0. Then multiplying both sides of this equality by an arbitrary
power of k, we conclude that a times any large power of k equals b times another
large power of k, and conversely, b times any large power of k equals a times a
large power of k. But since k 2 gcd1fng has Ô¨Ånite order, every element in Pk can
be expressed as a large power of k, and we conclude that aPk D bPk.

Notice that since P9 D 1P9, Lemma 8.10.9 explains not only why 3P9 and 5P9
don‚Äôt overlap, but also why neither of them overlaps with P9.
The next number in the list of elements of gcd1f28g is 9, which brings us to
9P9 D f25; 1; 9g D P:
Of course we could have predicted that 9P D P without actually multiplying
each element of P9 by 9; since 1 2 P9, we know that 9 D 9  1 2 9P9, so 9P9
and 1P9 have the element 9 in common, and therefore must be equal according to
Lemma 8.10.9.
Next, we come to
11P9 D f15; 23; 11g:
Now we‚Äôre done, because we have 4 different size 3 subsets of gcd1f28g, and since
gcd1f28g has 12 elements, we must have them all. That is,
gcd1f28g D 1P9 [ 3P9 [ 5P9 [ 11P9:
This means there‚Äôs no need to examine mP9 for any of the remaining numbers
m 2 gcd1f28g since they are bound to overlap with, and therefore be equal to,

8.10. Euler‚Äôs Theorem
239
one of the four sets 1P9; 3P9; 5P9, and 11P9, that we already have. For example,
we could conclude without further calculation that the next set, 13P9, must be the
same as 5P9, since both include the number 13.
We can also see why the size of P9 had to divide .28/ ‚Äîbecause gcd1f28g is a
union of non-overlapping sets of the same size as P9.
Lemma 8.10.10. If k 2 gcd1fng, then
ord.k; n/ j .n/:
Proof. Let Pk be the powers of k, so Pk has ord.k; n/ elements, namely,
Pk D fk; k2; : : : ; kord.k;n/g
By Lemma 8.10.4, both Pk and mPk are subsets of gcd1fng for m 2 gcd1fng.
Since 1 2 Pk, we have m 2 mPk for all m 2 ≈í0; n/. Therefore,
gcd1fng D
[
m2gcd1fng
mPk:
By Lemma 8.10.8, jmPkj D ord.k; n/, and by Lemma 8.10.9, distinct mPk‚Äôs don‚Äôt
overlap, it follows that
j gcd1fngj D ord.k; n/  jfmPk j m 2 gcd1fnggj :
So ord.k; n/ divides j gcd1fngj D .n/.

In particular, Lemma 8.10.10 implies that .n/ D ord.k; n/  c for some number
c, and so
k.n/ D kord.k;n/c D

kord.k;n/c
D 1c D 1:
(8.18)
Euler‚Äôs theorem now follows immediately, since it is simply the restatement of the
Zn equation (8.18) in terms of congruence mod n.
Euler‚Äôs theorem offers another way to Ô¨Ånd inverses modulo n: if k is relatively
prime to n, then k.n/ 1 is a Zn-inverse of k, and we can compute this power of
k efÔ¨Åciently using fast exponentiation. However, this approach requires computing
.n/. In the next section, we‚Äôll show that computing .n/ is easy if we know the
prime factorization of n. But we know that Ô¨Ånding the factors of n is generally hard
to do when n is large, and so the Pulverizer remains the best approach to computing
inverses modulo n.

Chapter 8
Number Theory
240
Fermat‚Äôs Little Theorem
For the record, we mention a famous special case of Euler‚Äôs Theorem that was
known to Fermat a century earlier.
Corollary 8.10.11 (Fermat‚Äôs Little Theorem). Suppose p is a prime and k is not a
multiple of p. Then:
kp 1  1
.mod p/
8.10.1
Computing Euler‚Äôs  Function
RSA works using arithmetic modulo the product of two large primes, so we begin
with an elementary explanation of how to compute .pq/ for primes p and q:
Lemma 8.10.12.
.pq/ D .p   1/.q   1/
for primes p ¬§ q.
Proof. Since p and q are prime, any number that is not relatively prime to pq must
be a multiple of p or a multiple of q. Among the pq numbers in ≈í0; pq/, there are
precisely q multiples of p and p multiples of q. Since p and q are relatively prime,
the only number in ≈í0; pq/ that is a multiple of both p and q is 0. Hence, there are
p C q   1 numbers in ≈í0; pq/ that are not relatively prime to n. This means that
.pq/ D pq   .p C q   1/
D .p   1/.q   1/;
as claimed.14

The following theorem provides a way to calculate .n/ for arbitrary n.
Theorem 8.10.13.
(a) If p is a prime, then .pk/ D pk   pk 1 for k  1.
(b) If a and b are relatively prime, then .ab/ D .a/.b/.
Here‚Äôs an example of using Theorem 8.10.13 to compute .300/:
.300/ D .22  3  52/
D .22/  .3/  .52/
(by Theorem 8.10.13.(b))
D .22   21/.31   30/.52   51/
(by Theorem 8.10.13.(a))
D 80:
14This proof previews a kind of counting argument that we will explore more fully in Part III.

8.11. RSA Public Key Encryption
241
To prove Theorem 8.10.13.(a), notice that every pth number among the pk num-
bers in ≈í0; pk/ is divisible by p, and only these are divisible by p. So 1=p of these
numbers are divisible by p and the remaining ones are not. That is,
.pk/ D pk   .1=p/pk D pk   pk 1:
We‚Äôll leave a proof of Theorem 8.10.13.(b) to Problem 8.43.
As a consequence of Theorem 8.10.13, we have
Corollary 8.10.14. For any number n, if p1, p2, ..., pj are the (distinct) prime
factors of n, then
.n/ D n

1   1
p1
 
1   1
p2

  

1   1
pj

:
We‚Äôll give another proof of Corollary 8.10.14 in a few weeks based on rules for
counting.
8.11
RSA Public Key Encryption
Turing‚Äôs code did not work as he hoped. However, his essential idea ‚Äîusing num-
ber theory as the basis for cryptography ‚Äîsucceeded spectacularly in the decades
after his death.
In 1977, Ronald Rivest, Adi Shamir, and Leonard Adleman at MIT proposed a
highly secure cryptosystem (called RSA) based on number theory. The purpose of
the RSA scheme is to transmit secret messages over public communication chan-
nels. As with Turing‚Äôs codes, the messages transmitted will actually be nonnegative
integers of some Ô¨Åxed size.
Moreover, RSA has a major advantage over traditional codes: the sender and
receiver of an encrypted message need not meet beforehand to agree on a secret key.
Rather, the receiver has both a private key, which they guard closely, and a public
key, which they distribute as widely as possible. A sender wishing to transmit a
secret message to the receiver encrypts their message using the receiver‚Äôs widely-
distributed public key. The receiver can then decrypt the received message using
their closely-held private key. The use of such a public key cryptography system
allows you and Amazon, for example, to engage in a secure transaction without
meeting up beforehand in a dark alley to exchange a key.
Interestingly, RSA does not operate modulo a prime, as Turing‚Äôs hypotheti-
cal Version 2.0 may have, but rather modulo the product of two large primes ‚Äî
typically primes that are hundreds of digits long. Also, instead of encrypting by

Chapter 8
Number Theory
242
multiplication with a secret key, RSA exponentiates to a secret power ‚Äîwhich is
why Euler‚Äôs Theorem is central to understanding RSA.
The scheme for RSA public key encryption appears in the box.
If the message m is relatively prime to n, Euler‚Äôs Theorem immediately implies
that this way of decoding the encrypted message indeed reproduces the original
unencrypted message. In fact, the decoding always works ‚Äîeven in (the highly
unlikely) case that m is not relatively prime to n. The details are worked out in
Problem 8.57.
Why is RSA thought to be secure? It would be easy to Ô¨Ågure out the private key d
if you knew p and q ‚Äîyou could do it the same way the Receiver does using the
Pulverizer. But assuming the conjecture that it is hopelessly hard to factor a number
that is the product of two primes with hundreds of digits, an effort to factor n is not
going to break RSA.
Could there be another approach to reverse engineer the private key d from the
public key that did not involve factoring n? Not really. It turns out that given just
the private and the public keys, it is easy to factor n (a proof of this is sketched in
Problem 8.59). So if we are conÔ¨Ådent that factoring is hopelessly hard, then we
can be equally conÔ¨Ådent that Ô¨Ånding the private key just from the public key will
be hopeless.
But even if we are conÔ¨Ådent that an RSA private key won‚Äôt be found, this doesn‚Äôt
rule out the possibility of decoding RSA messages in a way that sidesteps the pri-
vate key. It is an important unproven conjecture in cryptography that any way of
cracking RSA ‚Äînot just by Ô¨Ånding the secret key ‚Äîwould imply the ability to
factor. This would be a much stronger theoretical assurance of RSA security than
is presently known.
But the real reason for conÔ¨Ådence is that RSA has withstood all attacks by the
world‚Äôs most sophisticated cryptographers for over 30 years. Despite decades of
these attacks, no signiÔ¨Åcant weakness has been found. That‚Äôs why the mathemat-
ical, Ô¨Ånancial, and intelligence communities are betting the family jewels on the
security of RSA encryption.
You can hope that with more studying of number theory, you will be the Ô¨Årst to
Ô¨Ågure out how to do factoring quickly and, among other things, break RSA. But
be further warned that even Gauss worked on factoring for years without a lot to
show for his efforts ‚Äîand if you do Ô¨Ågure it out, you might wind up meeting some
humorless fellows working for a Federal agency....

8.11. RSA Public Key Encryption
243
The RSA Cryptosystem
A Receiver who wants to be able to receive secret numerical messages creates a
private key, which they keep secret, and a public key which they make publicly
available. Anyone with the public key can then be a Sender who can publicly
send secret messages to the Receiver ‚Äîeven if they have never communicated or
shared any information besides the public key.
Here is how they do it:
Beforehand The Receiver creates a public key and a private key as follows.
1. Generate two distinct primes, p and q. These are used to generate the
private key, and they must be kept hidden. (In current practice, p and
q are chosen to be hundreds of digits long.)
2. Let n WWD pq.
3. Select an integer e 2 ≈í0; n/ such that gcd.e; .p   1/.q   1// D 1.
The public key is the pair .e; n/. This should be distributed widely.
4. Let the private key d
2 ≈í0; n/ be the inverse of e in the ring
Z.p 1/.q 1/. This private key can be found using the Pulverizer. The
private key d should be kept hidden!
Encoding To transmit a message m 2 ≈í0; n/ to Receiver, a Sender uses the
public key to encrypt m into a numerical message
m WWD me .Zn/:
The Sender can then publicly transmit m to the Receiver.
Decoding The Receiver decrypts message m back to message m using the pri-
vate key:
m D .m/d .Zn/:

Chapter 8
Number Theory
244
8.12
What has SAT got to do with it?
So why does the world, or at least the world‚Äôs secret codes, fall apart if there is an
efÔ¨Åcient test for satisÔ¨Åability (SAT) as we claimed in Section 3.5? To explain this,
remember that RSA can be managed computationally because multiplication of two
primes is fast, but factoring a product of two primes seems to be overwhelmingly
demanding.
Now, designing digital multiplication circuits is completely routine. This means
we can easily build a digital circuit out of AND, OR, and NOT gates that can take
two input strings u; v of length n, and a third input string, z, of length 2n, and
‚Äúchecks‚Äù if the numbers represented by u and v are both greater than 1 and that z
represents their product. The circuit gives output 1 if z represents such a product
and gives output 0 otherwise.
Now here‚Äôs how to factor any number with a length 2n representation using a
SAT solver. Fix the z input to be the representation of the number to be factored.
Set the Ô¨Årst digit of the u input to 1, and do a SAT test to see if there is a satisfying
assignment of values for the remaining bits of u and v. That is, see if the remaining
bits of u and v can be Ô¨Ålled in to cause the circuit to give output 1. If there is such
an assignment, Ô¨Åx the Ô¨Årst bit of u to 1, otherwise Ô¨Åx the Ô¨Årst bit of u to be 0. Now
do the same thing to Ô¨Åx the second bit of u and then third, proceeding in this way
through all the bits of u and then of v. The result is that after 2n SAT tests, we
have found an assignment of values for u and v that makes the circuit give output
1. So u and v represent factors of the number represented by z. This means that if
SAT could be done in time bounded by a degree d polynomial in n, then 2n digit
numbers can be factored in time bounded by a polynomial in n of degree d C 1. In
sum, if SAT was easy, then so is factoring, and so RSA would be easy to break.
Problems for Section 8.1
Practice Problems
Problem 8.1.
Prove that a linear combination of linear combinations of integers a0; : : : ; an is a
linear combination of a0; : : : ; an.
Problem 8.2. (a) Find integer coefÔ¨Åcients, x, y, such that 25xC32y D GCD.25; 32/.
(b) What is the inverse (mod 25) of 32?

8.12. What has SAT got to do with it?
245
Class Problems
Problem 8.3.
A number is perfect if it is equal to the sum of its positive divisors, other than itself.
For example, 6 is perfect, because 6 D 1 C 2 C 3. Similarly, 28 is perfect, because
28 D 1 C 2 C 4 C 7 C 14. Explain why 2k 1.2k   1/ is perfect when 2k   1 is
prime.15
Problems for Section 8.2
Practice Problems
Problem 8.4.
Let
x WWD 21212121;
y WWD 12121212:
Use the Euclidean algorithm to Ô¨Ånd the GCD of x and y. Hint: Looks scary, but
it‚Äôs not.
Problem 8.5.
Let
x WWD 1788  315  372  591000
y WWD 19.922/  3712  533678  5929:
(a) What is gcd.x; y/?
(b) What is lcm.x; y/?
(lcm is least common multiple.)
Class Problems
Problem 8.6.
Use the Euclidean Algorithm to prove that
gcd.13a C 8b; 5a C 3b/ D gcd.a; b/:
15Euclid
proved
this
2300
years
ago.
About
250
years
ago,
Euler
proved
the
converse:
every
even
perfect
number
is
of
this
form
(for
a
simple
proof
see
http://primes.utm.edu/notes/proofs/EvenPerfect.html).
As is typical in
number theory, apparently simple results lie at the brink of the unknown. For example, it is not
known if there are an inÔ¨Ånite number of even perfect numbers or any odd perfect numbers at all.

Chapter 8
Number Theory
246
Problem 8.7.
(a) Use the Pulverizer to Ô¨Ånd integers x; y such that
x30 C y22 D gcd.30; 22/:
(b) Now Ô¨Ånd integers x0; y0 with 0  y0 < 30 such that
x030 C y022 D gcd.30; 22/
Problem 8.8.
For nonzero integers, a, b, prove the following properties of divisibility and GCD‚ÄôS.
(You may use the fact that gcd.a; b/ is an integer linear combination of a and b.
You may not appeal to uniqueness of prime factorization because the properties
below are needed to prove unique factorization.)
(a) Every common divisor of a and b divides gcd.a; b/.
(b) If a j bc and gcd.a; b/ D 1, then a j c.
(c) If p j bc for some prime, p, then p j b or p j c.
(d) Let m be the smallest integer linear combination of a and b that is positive.
Show that m D gcd.a; b/.
Homework Problems
Problem 8.9.
DeÔ¨Åne the Pulverizer State machine to have:
states WWD N6
start state WWD .a; b; 0; 1; 1; 0/
(where a  b > 0)
transitions WWD .x; y; s; t; u; v/  !
.y; rem .x; y/ ; u   sq; v   tq; s; t/
(for q D qcnt.x; y/; y > 0):
(a) Show that the following properties are preserved invariants of the Pulverizer
machine:
gcd.x; y/ D gcd.a; b/;
(8.19)
sa C tb D y; and
(8.20)
ua C vb D x:
(8.21)

8.12. What has SAT got to do with it?
247
(b) Conclude that the Pulverizer machine is partially correct.
(c) Explain why the machine terminates after at most the same number of transi-
tions as the Euclidean algorithm.
Problem 8.10.
Prove that the smallest positive integers a  b for which, starting in state .a; b/,
the Euclidean state machine will make n transitions are F.n C 1/ and F.n/, where
F.n/ is the nth Fibonacci number.
Hint: Induction.
In a later chapter, we‚Äôll show that F.n/  'n where ' is the golden ratio
.1 C
p
5/=2. This implies that the Euclidean algorithm halts after at most log'.a/
transitions. This is a somewhat smaller than the 2 log2 a bound derived from equa-
tion (8.4).
Problem 8.11.
Let‚Äôs extend the jug Ô¨Ålling scenario of Section 8.1.3 to three jugs and a receptacle.
Suppose the jugs can hold a, b, and c gallons of water, respectively.
The receptacle can be used to store an unlimited amount of water, but has no
measurement markings. Excess water can be dumped into the drain. Among the
possible moves are:
1. Ô¨Åll a bucket from the hose,
2. pour from the receptacle to a bucket until the bucket is full or the receptacle
is empty, whichever happens Ô¨Årst,
3. empty a bucket to the drain,
4. empty a bucket to the receptacle,
5. pour from one bucket to another until either the Ô¨Årst is empty or the second
is full,
(a) Model this scenario with a state machine. (What are the states? How does a
state change in response to a move?)
(b) Prove that Bruce can get k 2 N gallons of water into the receptacle using the
above operations only if gcd.a; b; c/ j k.

Chapter 8
Number Theory
248
(c) Prove conversely, that if gcd.a; b; c/ j k, then Bruce can get actually get k
gallons of water into the receptacle.
Problem 8.12.
The binary-GCD state machine computes the GCD of a and b using only division
by 2 and subtraction, which makes it run very efÔ¨Åciently on hardware that uses bi-
nary representation of numbers. In practice, it runs more quickly than the Euclidean
algorithm state machine (8.3).
states WWD N3
start state WWD .a; b; 1/
(where a > b > 0)
transitions WWD if min.x; y/ > 0; then .x; y; e/  !
the Ô¨Årst possible state according to the rules:
8
ÀÜÀÜÀÜÀÜÀÜÀÜÀÜÀÜÀÜÀÜÀÜ<
ÀÜÀÜÀÜÀÜÀÜÀÜÀÜÀÜÀÜÀÜÀÜ:
.1; 0; ex/
(if x D y)
.1; 0; e/
(if y D 1);
.x=2; y=2; 2e/
(if 2 j x and 2 j y);
.y; x; e/
(if y > x)
.x; y=2; e/
(if 2 j y)
.x=2; y; e/
(if 2 j x)
.x   y; y; e/
(otherwise):
(a) Prove that if this machine reaches a ‚ÄúÔ¨Ånal‚Äù state .x; y; e/ in which no transition
is possible, then e D gcd.a; b/.
(b) Prove that the machine reaches a Ô¨Ånal state in at most 3 C 2 log max.a; b/
transitions.
Hint: Strong induction on max.a; b/.
Exam Problems
Problem 8.13.
Prove that gcd.mb C r; b/ D gcd.b; r/ for all integers m; b; r.
Hint: We proved a similar result in class when r was a remainder in ≈í0; b/.

8.12. What has SAT got to do with it?
249
Problems for Section 8.3
Homework Problems
Problem 8.14.
TBA - Chebyshvev lower bound in prime density, based on Shoup pp.75‚Äì76
Problems for Section 8.4
Class Problems
Problem 8.15. (a) Let m D 295241171712 and n D 2372211211131179192. What
is the gcd.m; n/? What is the least common multiple, lcm.m; n/, of m and n? Verify
that
gcd.m; n/  lcm.m; n/ D mn:
(8.22)
(b) Describe in general how to Ô¨Ånd the gcd.m; n/ and lcm.m; n/ from the prime
factorizations of m and n. Conclude that equation (8.22) holds for all positive
integers m; n.
Homework Problems
Problem 8.16.
The set of complex numbers that are equal to m C n
p
 5 for some integers m; n
is called Z≈í
p
 5¬ç. It will turn out that in Z≈í
p
 5¬ç, not all numbers have unique
factorizations.
A sum or product of numbers in Z≈í
p
 5¬ç is in Z≈í
p
 5¬ç, and since Z≈í
p
 5¬ç is a
subset of the complex numbers, all the usual rules for addition and multiplication
are true for it. But some weird things do happen. For example, the prime 29 has
factors:
(a) Find x; y 2 Z≈í
p
 5¬ç such that xy D 29 and x ¬§ Àô1 ¬§ y.
On the other hand, the number 3 is still a ‚Äúprime‚Äù even in Z≈í
p
 5¬ç. More pre-
cisely, a number p 2 Z≈í
p
 5¬ç is called irreducible over Z≈í
p
 5¬ç iff when xy D p
for some x; y 2 Z≈í
p
 5¬ç, either x D Àô1 or y D Àô1.
Claim. The numbers 3; 2 C
p
 5, and 2  p
 5 are irreducible over Z≈í
p
 5¬ç.
In particular, this Claim implies that the number 9 factors into irreducibles over
Z≈í
p
 5¬ç in two different ways:
3  3 D 9 D .2 C
p
 5/.2  p
 5/:
(8.23)
So Z≈í
p
 5¬ç is an example of what is called a non-unique factorization domain.
To verify the Claim, we‚Äôll appeal (without proof) to a familiar technical property
of complex numbers given in the following Lemma.

Chapter 8
Number Theory
250
DeÔ¨Ånition. For a complex number c D r C si where r; s 2 R and i is
p
 1, the
norm, jcj, of c is
p
r2 C s2.
Lemma. For c; d 2 C,
jcdj D jcj jdj :
(b) Prove that jxj2 ¬§ 3 for all x 2 Z≈í
p
 5¬ç.
(c) Prove that if x 2 Z≈í
p
 5¬ç and jxj D 1, then x D Àô1.
(d) Prove that if jxyj D 3 for some x; y 2 Z≈í
p
 5¬ç, then x D Àô1 or y D Àô1.
Hint: jzj2 2 N for z 2 Z≈í
p
 5¬ç.
(e) Complete the proof of the Claim.
Problems for Section 8.6
Class Problems
Problem 8.17. (a) Prove if n is not divisible by 3, then n2  1 .mod 3/.
(b) Show that if n is odd, then n2  1 .mod 8/.
(c) Conclude that if p is a prime greater than 3, then p2   1 is divisible by 24.
Problem 8.18.
The values of polynomial p.n/ WWD n2 C n C 41 are prime for all the integers from
0 to 39 (see Section 1.1). Well, p didn‚Äôt work, but are there any other polynomials
whose values are always prime? No way! In fact, we‚Äôll prove a much stronger
claim.
Suppose q is a polynomial with integer coefÔ¨Åcients whose domain is restricted
to be the nonnegative integers. We‚Äôll say that q produces multiples if, for every
nonzero value in the range of q, there are inÔ¨Ånitely many multiples of that value
also in the range.
For example, if q produces multiples and q.4/ D 7, then there are inÔ¨Ånitely
many different multiples of 7 in the range of q, and of course, except for 7 itself,
none of these multiples is prime.
Claim. If q is not a constant function, then q produces multiples.
(a) Prove that if j  k .mod n/, then q.j/  q.k/ .mod n/.
Hint: The set, A, of polynomial functions with integer coefÔ¨Åcients can be deÔ¨Åned
recursively:

8.12. What has SAT got to do with it?
251
 Base cases:
‚Äì the identity function, i.x/ WWD x is in A.
‚Äì for any integer, k, the constant function, c.x/ WWD k is in A.
 Constructor cases. If r; s 2 A, then r C s and r  s 2 A.
(b) Prove the Claim 8.18.
Claim 8.18 implies that if an integer polynomial is not constant then its range
includes inÔ¨Ånitely many nonprimes. This fact no longer holds true for multivari-
ate polynomials. An amazing consequence of Matijesevich‚Äôs solution to Hilbert‚Äôs
Tenth Problem, TBA - reference , is that multivariate polynomials can be under-
stood as general purpose programs for generating sets of integers. If a set of non-
negative integers can be generated by any program, then it equals the set of nonneg-
ative integers in the range of a multivariate integer polynomial! In particular, there
is an integer polynomial p.x1; : : : ; x7/ whose nonnegative values as x1; : : : ; x7
range over N are precisely the set of all prime numbers!
Problems for Section 8.7
Practice Problems
Problem 8.19.
A majority of the following statements are equivalent to each other. List all state-
ments in this majority. Assume that n > 0 and a and b are integers. BrieÔ¨Çy explain
your reasoning.
1. a  b .mod n/
2. a D b
3. rem .a; n/ D rem .b; n/
4. n j .a   b/
5. 9k 2 Z: a D b C nk
6. .a   b/ is a multiple of n
7. n j a OR n j b

Chapter 8
Number Theory
252
Homework Problems
Problem 8.20.
Prove that congruence is preserved by arithmetic expressions. Namely, prove that
a  b
.mod n/;
(8.24)
then
eval.e; a/  eval.e; b/
.mod n/;
(8.25)
for all e 2 Aexp (see Section 6.4).
Problem 8.21.
The sum of the digits of the base 10 representation of an integer is congruent mod-
ulo 9 to that integer. For example
763  7 C 6 C 3
.mod 9/:
This is not always true for the hexadecimal (base 16) representation, however. For
example,
.763/16 D 7  162 C 6  16 C 3  1 6 7  7 C 6 C 3
.mod 9/:
(a) For exactly what integers k > 1 is it true that the sum of the digits of the base
16 representation of an integer is congruent modulo k to that integer? Justify your
answer.
(b) Give a rule that generalizes this sum-of-digits rule from base b D 16 to an
arbitrary number base b > 1, and explain why your rule is correct.
Class Problems
Problem 8.22.
Find
remainder

98763456789  9995555   67893414259; 14

:
(8.26)
Problem 8.23.
The following properties of equivalence mod n follow directly from its deÔ¨Ånition
and simple properties of divisibility. See if you can prove them without looking up
the proofs in the text.

8.12. What has SAT got to do with it?
253
(a) If a  b .mod n/, then ac  bc .mod n/.
(b) If a  b .mod n/ and b  c .mod n/, then a  c .mod n/.
(c) If a  b .mod n/ and c  d .mod n/, then ac  bd .mod n/.
(d) rem .a; n/  a .mod n/.
Problem 8.24. (a) Why is a number written in decimal evenly divisible by 9 if and
only if the sum of its digits is a multiple of 9? Hint: 10  1 .mod 9/.
(b) Take a big number, such as 37273761261. Sum the digits, where every other
one is negated:
3 C . 7/ C 2 C . 7/ C 3 C . 7/ C 6 C . 1/ C 2 C . 6/ C 1 D  11
Explain why the original number is a multiple of 11 if and only if this sum is a
multiple of 11.
Problem 8.25.
At one time, the Guinness Book of World Records reported that the ‚Äúgreatest human
calculator‚Äù was a guy who could compute 13th roots of 100-digit numbers that were
powers of 13. What a curious choice of tasks....
In this problem, we prove
n13  n
.mod 10/
(8.27)
for all n.
(a) Explain why (8.27) does not follow immediatetly from Euler‚Äôs Theorem.
(b) Prove that
d 13  d
.mod 10/
(8.28)
for 0  d < 10.
(c) Now prove the congruence (8.27).
Problem 8.26. (a) Ten pirates Ô¨Ånd a chest Ô¨Ålled with gold and silver coins. There
are twice as many silver coins in the chest as there are gold. They divide the gold
coins in such a way that the difference in the number of coins given to any two

Chapter 8
Number Theory
254
pirates is not divisible by 10. They will only take the silver coins if it is possible
to divide them the same way. Is this possible, or will they have to leave the silver
behind? Prove your answer.
(b) There are also 3 sacks in the chest, containing 5, 49, and 51 rubies respec-
tively. The treasurer of the pirate ship is bored and decides to play a game with the
following rules:
 He can merge any two piles together into one pile, and
 he can divide a pile with an even number of rubies into two piles of equal size.
He makes one move every day, and he will Ô¨Ånish the game when he has divided the
rubies into 105 piles of one. Is it possible for him to Ô¨Ånish the game?
Exam Problems
Problem 8.27.
We deÔ¨Åne the sequence of numbers
an D
(
an 1 C an 2 C an 3 C an 4
if n  4,
1
if 0  n  3.
Prove that an  1 .mod 3/ for all n  0.
Problems for Section 8.8
Exam Problems
Problem 8.28.
The set Aexp of Arithmetic Expressions in the variable x was deÔ¨Åned recursively:
expressions consisting solely of the variable x or an arabic numeral, k, were the
base cases, and the contructors were forming the sum, [ e1+e2] , product, [ e1e2] ,
or minus - [ e1] of Aexp‚Äôs e1; e2. Then the value eval.e; n/ of an Aexp e when the
variable x is equal to the integer n has an immediate recursive deÔ¨Ånition based on
the deÔ¨Ånition of Aexp‚Äôs.
Prove by structural induction that for all Aexp‚Äôs e,
8m; n; d 2 Z; d > 1: ≈ím  n
.mod d/¬ç
IMPLIES
≈íeval.e; m/  eval.e; n/
.mod d/¬ç:
(8.29)
Hint: Be sure to consider both base cases. The proofs for the three constructors
are very similar, so just write out the case for the sum constructor.

8.12. What has SAT got to do with it?
255
Problems for Section 8.9
Practice Problems
Problem 8.29.
What is the multiplicative inverse (mod 7) of 2? Reminder: by deÔ¨Ånition, your
answer must be an integer between 0 and 6.
Problem 8.30. (a) Use the Pulverizer to Ô¨Ånd integers s; t such that
40s C 7t D gcd.40; 7/:
(b) Adjust your answer to part (a) to Ô¨Ånd an inverse modulo 40 of 7 in ≈í1; 40/.
Class Problems
Problem 8.31.
Two nonparallel lines in the real plane intersect at a point. Algebraically, this means
that the equations
y D m1x C b1
y D m2x C b2
have a unique solution .x; y/, provided m1 ¬§ m2. This statement would be false if
we restricted x and y to the integers, since the two lines could cross at a noninteger
point:
However, an analogous statement holds if we work over the integers modulo a

Chapter 8
Number Theory
256
prime, p. Find a solution to the congruences
y  m1x C b1
.mod p/
y  m2x C b2
.mod p/
when m1 6 m2 .mod p/. Express your solution in the form x ‚Äπ .mod p/ and
y ‚Äπ .mod p/ where the ?‚Äôs denote expressions involving m1, m2, b1, and b2.
You may Ô¨Ånd it helpful to solve the original equations over the reals Ô¨Årst.
Problems for Section 8.10
Practice Problems
Problem 8.32.
Prove that k 2 ≈í0; n/ has an inverse modulo n iff it has an inverse in Zn.
Problem 8.33.
What is rem
 2478; 79

? Hint: 79 is prime. You should not need to do any calcu-
lation!
Problem 8.34. (a) Prove that 2212001 has a multiplicative inverse modulo 175.
(b) What is the value of .175/, where  is Euler‚Äôs function?
(c) What is the remainder of 2212001 divided by 175?
Problem 8.35.
How many numbers between 1 and 6042 (inclusive) are relatively prime to 3780?
Hint: 53 is a factor.
Problem 8.36.
How many numbers between 1 and 3780 (inclusive) are relatively prime to 3780?
Problem 8.37.
(a) What is the probability that an integer from 1 to 360 selected with uniform
probability is relatively prime to 360?

8.12. What has SAT got to do with it?
257
(b) What is the value of rem
 798; 360

?
Class Problems
Problem 8.38.
Find the last digit of 7777
.
Problem 8.39.
Use Fermat‚Äôs theorem to Ô¨Ånd the inverse, i, of 13 modulo 23 with 1  i < 23.
Problem 8.40.
Let Sk D 1k C 2k C : : : C .p   1/k, where p is an odd prime and k is a positive
multiple of p   1. Use Fermat‚Äôs theorem to prove that Sk   1 .mod p/.
Problem 8.41.
Let a and b be relatively prime positive integers.
(a) How many integers in the interval ≈í0; ab/ are divisible by a?
(b) How many integers in the interval ≈í0; ab/ are divisible by both a and b?
(c) How many integers in the interval ≈í0; ab/ are divisible by either a or b?
(d) Now suppose p ¬§ q are both primes. How many integers in the interval
≈í0; pq/ are not relatively prime to pq? Observe that a different answer is required
if p and q were merely relatively prime numbers a and b as in part (c).
(e) Conclude that
.pq/ D .p   1/.q   1/:
Problem 8.42.
Suppose a; b are relatively prime and greater than 1. In this problem you will prove
the Chinese Remainder Theorem, which says that for all m; n, there is an x such

Chapter 8
Number Theory
258
that
x  m mod a;
(8.30)
x  n mod b:
(8.31)
Moreover, x is unique up to congruence modulo ab, namely, if x0 also satis-
Ô¨Åes (8.30) and (8.31), then
x0  x mod ab:
(a) Prove that for any m; n, there is some x satisfying (8.30) and (8.31).
Hint: Let b 1 be an inverse of b modulo a and deÔ¨Åne ea WWD b 1b. DeÔ¨Åne eb
similarly. Let x D mea C neb.
(b) Prove that
≈íx  0 mod a AND x  0 mod b¬ç
implies
x  0 mod ab:
(c) Conclude that

x  x0 mod a AND x  x0 mod b

implies
x  x0 mod ab:
(d) Conclude that the Chinese Remainder Theorem is true.
(e) What about the converse of the implication in part (c)?
Homework Problems
Problem 8.43.
Suppose a; b are relatively prime integers greater than 1. In this problem you will
prove that Euler‚Äôs function is multiplicative, namely, that
.ab/ D .a/.b/:
The proof is an easy consequence of the Chinese Remainder Theorem (Problem 8.42).
(a) Conclude from the Chinese Remainder Theorem that the function f W ≈í0; ab/ !
≈í0; a/  ≈í0; b/ deÔ¨Åned by
f .x/ WWD .rem .x; a/ ; rem .x; b//
is a bijection.
(b) For any positive integer, k, let gcd1fkg be the integers in ≈í0; k/ that are rela-
tively prime to k. Prove that the function f from part (a) also deÔ¨Ånes a bijection
from gcd1fabg to gcd1fag  gcd1fbg.

8.12. What has SAT got to do with it?
259
(c) Conclude from the preceding parts of this problem that
.ab/ D .a/.b/:
(8.32)
(d) Prove Corollary 8.10.14: for any number n > 1, if p1, p2, ..., pj are the
(distinct) prime factors of n, then
.n/ D n

1   1
p1
 
1   1
p2

  

1   1
pj

:
Problem 8.44.
The general version of the Chinese Remainder theorem (Problem 8.42) extends to
more than two relatively prime moduli. Namely,
Theorem (General Chinese Remainder). Suppose a1; : : : ; ak are integers greater
than 1 and each is relatively prime to the others. Let n WWD a1  a2    ak. Then for
any integers m1; m2; : : : ; mk, there is a unique x 2 ≈í0; n/ such that
x  mi
.mod ai/;
for 1  i  k.
The proof is a routine induction on k using a fact that follows immediately from
unique factorization: if a number is relatively prime to some other numbers, then it
is relatively prime to their product.
Now suppose an n-bit number, N , was a product of relatively prime k-bit num-
bers, where n was big, but k was small enough to be handled by cheap and available
arithmetic hardware units. Suppose a calculation requiring a large number of addi-
tions and multiplications modulo N had to be performed starting with some small
set of n-bit numbers. For example, suppose we wanted to compute
rem
 .x   3/110033..y C 7/27123   z4328/ ; N

which would require several dozen n-bit operations starting from the three numbers
x; y; z.
Doing a multiplication or addition modulo N directly requires breaking up the
n-bit numbers x; y; z and all the intermediate results of the mod N calculation into
k-bit pieces, using the hardware to perform the additions and multiplications on
the pieces, and then reassembling the k-bit results into an n-bit answer after each
operation. Suppose N was a product of m relatively prime k-bit numbers.
Explain how the General Chinese Remainder Theorem offers a far more efÔ¨Åcient
approach to performing the required operations.

Chapter 8
Number Theory
260
Exam Problems
Problem 8.45.
Prove that if k1 and k2 are relatively prime to n, then so is k1 n k2,
(a) . . . using the fact that k is relatively prime to n iff k has an inverse modulo n
Hint: Recall that k1k2  k1 n k2 .mod n/.
(b) . . . using the fact that k is relatively prime to n iff k is cancellable modulo n.
(c) . . . using the Unique Factorization Theorem and the basic GCD properties such
as Lemma 8.2.1.
Problem 8.46.
Circle true or false for the statements below, and provide counterexamples for
those that are false. Variables, a; b; c; m; n range over the integers and m; n > 1.
(a) gcd.1 C a; 1 C b/ D 1 C gcd.a; b/.
true
false
(b) If a  b .mod n/, then p.a/  p.b/ .mod n/
for any polynomial p.x/ with integer coefÔ¨Åcients.
true
false
(c) If a j bc and gcd.a; b/ D 1, then a j c.
true
false
(d) gcd.an; bn/ D .gcd.a; b//n
true
false
(e) If gcd.a; b/ ¬§ 1 and gcd.b; c/ ¬§ 1, then gcd.a; c/ ¬§ 1.
true
false
(f) If an integer linear combination of a and b equals 1,
then so does some integer linear combination of a2 and b2.
true
false
(g) If no integer linear combination of a and b equals 2,
then neither does any integer linear combination of a2 and b2.
true
false
(h) If ac  bc .mod n/ and n does not divide c,
then a  b .mod n/.
true
false
(i) Assuming a; b have inverses modulo n,
if a 1  b 1 .mod n/, then a  b .mod n/.
true
false
(j) If ac  bc .mod n/ and n does not divide c,
then a  b .mod n/.
true
false

8.12. What has SAT got to do with it?
261
(k) If a  b .mod .n// for a; b > 0, then ca  cb .mod n/.
true
false
(l) If a  b .mod nm/, then a  b .mod n/.
true
false
(m) If gcd.m; n/ D 1, then
≈ía  b .mod m/ AND a  b .mod n/¬ç iff ≈ía  b .mod mn/¬ç
true
false
(n) If gcd.a; n/ D 1, then an 1  1 .mod n/
true
false
(o) If a; b > 1, then
[a has a inverse mod b iff b has an inverse mod a].
true
false
Problem 8.47.
Find the remainder of 261818181 divided by 297.
Hint: 1818181 D .180  10101/ C 1; use Euler‚Äôs theorem.
Problem 8.48.
Find an integer k > 1 such that n and nk agree in their last three digits whenever n
is divisible by neither 2 nor 5. Hint: Euler‚Äôs theorem.
Problem 8.49.
What is the remainder of 639601 divided by 220?
Problem 8.50.
(a) Explain why . 12/482 has a multiplicative inverse modulo 175.
(b) What is the value of .175/, where  is Euler‚Äôs function?
(c) Call a number from 0 to 174 powerful iff some positive power of the number
is congruent to 1 modulo 175. What is the probability that a random number from
0 to 174 is powerful?

Chapter 8
Number Theory
262
(d) What is the remainder of . 12/482 divided by 175?
Problem 8.51. (a) Calculate the remainder of 3586 divided by 29.
(b) Part (a) implies that the remainder of 3586 divided by 29 is not equal to 1. So
there there must be a mistake in the following proof, where all the congruences are
taken with modulus 29:
1 6 3586
(by part (a))
(8.33)
 686
(since 35  6
.mod 29/)
(8.34)
 628
(since 86  28
.mod 29/)
(8.35)
 1
(by Fermat‚Äôs Little Theorem)
(8.36)
Identify the exact line containing the mistake and explain the logical error.
Problem 8.52.
Give counterexamples for each of the statements below that are false.
(a) For integers a and b there are integers x and y such that: ax C by D 1
(b) gcd.mb C r; b/ D gcd.r; b/ for all integers m; r and b.
(c) For every prime p and every integer k, kp 1  1 .mod p/.
(d) For primes p ¬§ q, .pq/ D .p 1/.q 1/, where  is Euler‚Äôs totient fucntion.
(e) Suppose a; b; c; d 2 N and a and b are relatively prime to d. Then
≈íac  bc mod d¬ç
IMPLIES
≈ía  b mod d¬ç:
Problems for Section 8.11
Practice Problems
Problem 8.53.
Suppose a cracker knew how to factor the RSA modulus n into the product of
distinct primes p and q. Explain how the cracker could use the public key-pair
.e; n/ to Ô¨Ånd a private key-pair .d; n/ that would allow him to read any message
encrypted with the public key.

8.12. What has SAT got to do with it?
263
Problem 8.54.
Suppose the RSA modulus n D pq is the product of distinct 200 digit primes p and
q. A message m 2 ≈í0; n/ is called dangerous if gcd.m; n/ D p, because such an m
can be used to factor n and so crack RSA. Circle the best estimate of the fraction
of messages in ≈í0; n/ that are dangerous.
1
200
1
400
1
20010
1
10200
1
40010
1
10400
Class Problems
Problem 8.55.
Let‚Äôs try out RSA!
(a) Go through the beforehand steps.
 Choose primes p and q to be relatively small, say in the range 10-40. In
practice, p and q might contain hundreds of digits, but small numbers are
easier to handle with pencil and paper.
 Try e D 3; 5; 7; : : : until you Ô¨Ånd something that works. Use Euclid‚Äôs algo-
rithm to compute the gcd.
 Find d (using the Pulverizer or Euler‚Äôs Theorem).
When you‚Äôre done, put your public key on the board. This lets another team send
you a message.
(b) Now send an encrypted message to another team using their public key. Select
your message m from the codebook below:
 2 = Greetings and salutations!
 3 = Yo, wassup?
 4 = You guys are slow!
 5 = All your base are belong to us.
 6 = Someone on our team thinks someone on your team is kinda cute.
 7 = You are the weakest link. Goodbye.
(c) Decrypt the message sent to you and verify that you received what the other
team sent!

Chapter 8
Number Theory
264
Problem 8.56. (a) Just as RSA would be trivial to crack knowing the factorization
into two primes of n in the public key, explain why RSA would also be trivial to
crack knowing .n/.
(b) Show that if you knew n, .n/, and that n was the product of two primes, then
you could easily factor n.
Hint: Suppose n D pq, replace q by n=p in the expression for .n/, and solve for
p.
Problem 8.57.
A critical fact about RSA is, of course, that decrypting an encrypted message, m,
always gives back the original message, m. Namely, if n D pq where p and q are
distinct primes, m 2 ≈í0; pq/, and
d  e  1
.mod .p   1/.q   1//;
then
rem

rem

md; n
e
; n

D m:
(8.37)
We‚Äôll now prove this.
(a) Verify that if
.md/e  m
.mod n/;
(8.38)
then (8.37) is true.
(b) Prove that if p is prime, then ma  m .mod p/ for all a 2 N congruent to 1
mod p   1.
(c) Prove that if a  b .mod pi/ for distinct primes p1; p2; : : : ; pn, then a  b
.mod p1p1    pn/.
(d) Prove
Lemma. If n is a product of distinct primes and a 2 N is  1 .mod .n//, then
ma  m .mod n/.
(e) Combine the previous parts to complete the proof of (8.37).
Homework Problems
Problem 8.58.
Although RSA has successfully withstood cryptographic attacks for a more than a

8.12. What has SAT got to do with it?
265
quarter century, it is not known that breaking RSA would imply that factoring is
easy.
In this problem we will examine the Rabin cryptosystem that does have such
a security certiÔ¨Åcation. Namely, if someone has the ability to break the Rabin
cryptosystem efÔ¨Åciently, then they also have the ability to factor numbers that are
products of two primes.
Why should that convince us that it is hard to break the cryptosystem efÔ¨Åciently?
Well, mathematicians have been trying to factor efÔ¨Åciently for centuries, and they
still haven‚Äôt Ô¨Ågured out how to do it.
What is the Rabin cryptosystem? The public key will be a number N that is a
product of two very large primes p; q such that p  q  3 .mod 4/. To send the
message x, send rem
 x2; N

.16
The private key is the factorization of N , namely, the primes p; q. We need to
show that if the person being sent the message knows p; q, then they can decode
the message. On the other hand, if an eavesdropper who doesn‚Äôt know p; q listens
in, then we must show that they are very unlikely to Ô¨Ågure out this message.
First some deÔ¨Ånitions. We know what it means for a number to be a square over
the integers, that is s is a square if there is another integer x such that s D x2. Over
the numbers mod N , we say that s is a square modulo N if there is an x such that
s  x2 .mod N /. If x is such that 0  x < N and s  x2 .mod N/, then x is
the square root of s.
(a) What are the squares modulo 5? For each nonzero square in the interval ≈í0; 5/,
how many square roots does it have?
(b) For each integer in ≈í1; 15/ that is relatively prime to 15, how many square roots
(modulo 15) does it have? Note that all the square roots are also relatively prime to
15. We won‚Äôt go through why this is so here, but keep in mind that this is a general
phenomenon!
(c) Suppose that p is a prime such that p  3 .mod 4/. It turns out that squares
modulo p have exactly 2 square roots. First show that .p C 1/=4 is an integer.
Next Ô¨Ågure out the two square roots of 1 modulo p. Then show that you can Ô¨Ånd a
‚Äúsquare root mod a prime p‚Äù of a number by raising the number to the .p C 1/=4th
power. That is, given s, to Ô¨Ånd x such that s  x2 .mod p/, you can compute
rem

s.pC1/=4; p

.
(d) The Chinese Remainder Theorem (Problem 8.42) implies that if p; q are dis-
16We will see soon, that there are other numbers that would be encrypted by rem
 x2; N

, so we‚Äôll
have to disallow those other numbers as possible messages in order to make it possible to decode this
cryptosystem, but let‚Äôs ignore that for now.

Chapter 8
Number Theory
266
tinct primes, then s is a square modulo pq if and only if s is a square modulo p and
s is a square modulo q. In particular, if s  x2 .mod p/  .x0/2 .mod p/ and
s  y2 .mod p/  .y0/2 .mod p/ then s has exactly four square roots, namely,
s  .xy/2  .x0y/2  .xy0/2  .x0y0/2
.mod pq/:
So, if you know p; q, then using the solution to part (c), you can efÔ¨Åciently Ô¨Ånd the
square roots of s! Thus, given the private key, decoding is easy.
But what if you don‚Äôt know p; q? Suppose N WWD pq, where p; q are two primes
equivalent to 3 .mod 4/. Let‚Äôs assume that the evil message interceptor claims
to have a program that can Ô¨Ånd all four square roots of any number modulo N .
Show that he can actually use this program to efÔ¨Åciently Ô¨Ånd the factorization of
N . Thus, unless this evil message interceptor is extremely smart and has Ô¨Ågured
out something that the rest of the scientiÔ¨Åc community has been working on for
years, it is very unlikely that this efÔ¨Åcient square root program exists!
Hint: Pick r arbitrarily from ≈í1; N /. If gcd.N; r/ > 1, then you are done (why?)
so you can halt. Otherwise, use the program to Ô¨Ånd all four square roots of r, call
them r;  r; r0;  r0. Note that r2  r02 .mod N/. How can you use these roots to
factor N ?
(e) If the evil message interceptor knows that the message is the encoding one of
two possible candidate messages (that is, either ‚Äúmeet at dome at dusk‚Äù or ‚Äúmeet at
dome at dawn‚Äù) and is just trying to Ô¨Ågure out which of the two, then can he break
this cryptosystem?
Problem 8.59.
You‚Äôve seen how the RSA encryption scheme works, but why is it hard to break?
In this problem, you will see that Ô¨Ånding private keys is as hard as Ô¨Ånding the
prime factorizations of integers. Since there is a general consensus in the crypto
community (enough to persuade many large Ô¨Ånancial institutions, for example)
that factoring numbers with a few hundred digits requires astronomical computing
resources, we can therefore be sure it will take the same kind of overwhelming
effort to Ô¨Ånd RSA private keys of a few hundred digits. This means we can be
conÔ¨Ådent the private RSA keys are not somehow revealed by the public keys 17
For this problem, assume that n D p  q where p; q are both odd primes and that
e is the public key and d the private key of the RSA protocol.. Let x WWD e  d   1.
17This is a very weak kind of ‚Äúsecurity‚Äù property, because it doesn‚Äôt even rule out the possibility
of deciphering RSA encoded messages by some method that did not require knowing the private key.
Nevertheless, over twenty years experience supports the security of RSA in practice.

8.12. What has SAT got to do with it?
267
(a) Show that .n/ divides x.
(b) Conclude that 4 divides x.
(c) Show that if gcd.r; n/ D 1, then rx  1 .mod n/:
A square root of m modulo n is a nonnegative integer s < n such that s2  m
.mod n/. Here is a nice fact to know: when n is a product of two odd primes, then
every number m such that gcd.m; n/ D 1 has 4 square roots modulo n.
In particular, the number 1 has four square roots modulo n. The two trivial ones
are 1 and n   1 (which is   1 .mod n/). The other two are called the nontrivial
square roots of 1.
(d) Since you know x, then for any integer, r, you can also compute the remainder,
y, of rx=2 divided by n. So y2  rx .mod n/. Now if r is relatively prime to n,
then y will be a square root of 1 modulo n by part (c).
Show that if y turns out to be a nontrivial root of 1 modulo n, then you can factor
n. Hint: From the fact that y2   1 D .y C 1/.y   1/, show that y C 1 must be
divisible by exactly one of q and p.
(e) It turns out that at least half the positive integers r < n that are relatively
prime to n will yield y‚Äôs in part (d) that are nontrivial roots of 1. Conclude that if,
in addition to n and the public key, e, you also knew the private key d, then you
can be sure of being able to factor n.


II
Structures


Introduction
Structure is fundamental in computer science. Whether you are writing code, solv-
ing an optimization problem, or designing a network, you will be dealing with
structure. The better you can understand the structure, the better your results will
be. And if you can reason about structure, then you will be in a good position to
convince others (and yourself) that your results are worthy.
The most important structure in computer science is a graph, also known as a
network). Graphs provide an excellent mechanism for modeling associations be-
tween pairs of objects; for example, two exams that cannot be given at the same
time, two people that like each other, or two subroutines that can be run indepen-
dently. In Chapter 9, we study directed graphs which model one-way relationships
such as being bigger than, loving (sadly, it‚Äôs often not mutual), being a prerequisite
for. A highlight is the special case of acyclic digraphs (DAGs) that correspond to a
class of relations called partial orders. Partial orders arise frequently in the study
of scheduling and concurrency. Digraphs as models for data communication and
routing problems are the topic of Chapter 10.
In Chapter 11 we focus on simple graphs that represent mutual or symmetric
relationships, such as being congruent modulo 17, being in conÔ¨Çict, being compat-
ible, being independent, being capable of running in parallel. Simple graphs that
can be drawn in the plane are examined in Chapter 12. The impossibility of placing
50 geocentric satellites in orbit so that they uniformly blanket the globe will be one
of the conclusions reached in this chapter.
This part of the text concludes with Chapter 13 which elaborates the use of the
state machines in program veriÔ¨Åcation and modeling concurrent computation.


9
Directed graphs & Partial Orders
Directed graphs, called digraphs for short, provide a handy way to represent how
things are connected together and how to get from one thing to another by following
the connections. They are usually pictured as a bunch of dots or circles with arrows
between some of the dots as in Figure 9.1. The dots are called nodes (or vertices)
and the lines are called directed edges or arrows, so the digraph in Figure 9.1 has 4
nodes and 6 directed edges.
Digraphs appear everywhere in computer science. In Chapter 10, we‚Äôll use di-
graphs to describe communication nets for routing data packets. The digraph in
Figure 9.2 has three ‚Äúin‚Äù nodes (pictured as little squares) representing locations
where packets may arrive at the net, the three ‚Äúout‚Äù nodes representing destina-
tion locations for packets, and the remaining six nodes (pictured with little circles)
represent switches. The 16 edges indicate paths that packets can take through the
router.
Another digraph example is the hyperlink structure of the World Wide Web. Let-
ting the vertices x1; : : : ; xn correspond to web pages, and using arrows to indicate
when one page has a hyperlink to another, yields a digraph like the one in Fig-
ure 9.3. In the graph of the real World Wide Web, n would be a number in the
billions and probably even the trillions. At Ô¨Årst glance, this graph wouldn‚Äôt seem to
be very interesting. But in 1995, two students at Stanford, Larry Page and Sergey
Brin, ultimately became multibillionaires from the realization of how useful the
structure of this graph could be in building a search engine. So pay attention to
graph theory, and who knows what might happen!
a
c
b
d
Figure 9.1
A 4-node directed graph with 6 edges.

Chapter 9
Directed graphs & Partial Orders
274
in2
in1
in0
out0
out1
out2
Figure 9.2
A 6-switch packet routing digraph.
x1
x3
x4
x7
x6
x2
x5
Figure 9.3
Links among Web Pages.

9.1. Digraphs & Vertex Degrees
275
u
v
e
head
tail
Figure 9.4
A directed edge e D hu!vi. The edge e starts at the tail vertex, u,
and ends at the head vertex, v.
9.1
Digraphs & Vertex Degrees
DeÔ¨Ånition 9.1.1. A directed graph, G, consists of a nonempty set, V.G/, called
the vertices of G, and a set, E.G/, called the edges of G. An element of V.G/ is
called a vertex. A vertex is also called a node; the words ‚Äúvertex‚Äù and ‚Äúnode‚Äù are
used interchangeably. An element of E.G/ is called a directed edge. A directed
edge is also called an ‚Äúarrow‚Äù or simply an ‚Äúedge.‚Äù A directed edge starts at some
vertex, u, called the tail of the edge, and ends at some vertex, v, called the head
of the edge, as in Figure 9.4. Such an edge can be represented by the ordered pair
.u; v/. The notation hu!vi denotes this edge.
There is nothing new in DeÔ¨Ånition 9.1.1 except for a lot of vocabulary. Formally,
a digraph G is the same as a binary relation on the set, V D V.G/ ‚Äîthat is, a
digraph is just a binary relation whose domain and codomain are the same set, V .
In fact we‚Äôve already referred to the arrows in a relation G as the ‚Äúgraph‚Äù of G.
For example, the divisibility relation on the integers in the interval ≈í1; 12¬ç could be
pictured by the digraph in Figure 9.5.
The in-degree of a vertex in a digraph is the number of arrows coming into it and
similarly its out-degree is the number of arrows out of it. More precisely,
DeÔ¨Ånition 9.1.2. If G is a digraph and v 2 V.G/, then
indeg.v/ WWD jfe 2 E.G/ j head.e/ D vgj
outdeg.v/ WWD jfe 2 E.G/ j tail.e/ D vgj
An immediate consequence of this deÔ¨Ånition is
Lemma 9.1.3.
X
v2V.G/
indeg.v/ D
X
v2V.G/
outdeg.v/:
Proof. Both sums are obviously equal to jE.G/j.


Chapter 9
Directed graphs & Partial Orders
276
12
6
1
8
2
4
10
5
7
11
9
3
Figure 9.5
The Digraph for Divisibility on f1; 2; : : : ; 12g.
Picturing digraphs with points and arrows makes it natural to talk about following
successive edges through the graph. For example, in the digraph of Figure 9.5, you
might start at vertex 1, successively follow the edges from vertex 1 to vertex 2, from
2 to 4, from 4 to 12, and then from 12 to 12 twice (or as many times as you like).
The sequence of edges followed in this way is called a walk through the graph.
The obvious way to represent a walk is with the sequence of sucessive vertices it
went through, in this case:
1 2 4 12 12 12:
However, it is conventional to represent a walk by an alternating sequence of suc-
cessive vertices and edges, so this walk would formally be
1 h1!2i 2 h2!4i 4 h4!12i 12 h12!12i 12 h12!12i 12:
(9.1)
The redundancy of this deÔ¨Ånition is enough to make any computer scientist cringe,
but it does make it easy to talk about how many times vertices and edges occur on
the walk. Here is a formal deÔ¨Ånition:
DeÔ¨Ånition 9.1.4. A walk in a digraph, G, is an alternating sequence of vertices and
edges that begins with a vertex, ends with a vertex, and such that for every edge
hu!vi in the walk, vertex u is the element just before the edge, and vertex v is the
next element after the edge.
So a walk, v, is a sequence of the form
v WWD v0 hv0 !v1i v1 hv1 !v2i v2 : : : hvk 1 !vki vk
where hvi !viC1i 2 E.G/ for i 2 ≈í0; k/. The walk is said to start at v0, to end at
vk, and the length, jvj, of the walk is deÔ¨Åned to be k. The walk is a path iff all the
vi‚Äôs are different, that is, if i ¬§ j, then vi ¬§ vj .

9.1. Digraphs & Vertex Degrees
277
A closed walk is a walk that begins and ends at the same vertex. A cycle is a
closed walk whose vertices are distinct except for the beginning and end vertices.
Note that a single vertex counts as a length zero path, and also a length zero
cycle, that begins and ends at itself.
Although a walk is ofÔ¨Åcially an alternating sequence of vertices and edges, it
is completely determined just by the sequence of successive vertices on it, or by
the sequence of edges on it, and we will describe walks that way whenever it‚Äôs
convenient. For example, for the graph in Figure 9.1,
 .a; b; d/, or simply abd, is (a vertex-sequence description of) a length-2
path,
 .ha!bi ; hb !di/, or simply ha!bi hb !di, is (an edge-sequence de-
scription of) the same length-2 path,
 abcbd is a length-4 walk,
 dcbcbd is a length-5 closed walk,
 bdcb is a length-3 cycle,
 hb !ci hc !bi is a length-2 cycle, and
 hc !bi hb  ai ha!di is not a walk. A walk is not allowed to follow edges
in the wrong direction.
Length-1 cycles are also possible. The graph in Figure 9.1 has none, but ev-
ery vertex in the divisibility relation digraph of Figure 9.5 is in a length-1 cycle.
Length-1 cycles are sometimes called self-loops.
If you walk for a while, stop for a rest at some vertex, and then continue walking,
you have broken a walk into two parts. For example, stopping to rest after following
two edges in the walk (9.1) through the divisibility graph breaks the walk into the
Ô¨Årst part of the walk
1 h1!2i 2 h2!4i 4
(9.2)
from 1 to 4, and the rest of the walk
4 h4!12i 12 h12!12i 12 h12!12i 12:
(9.3)
from 4 to 12, and we‚Äôll say the whole walk (9.1) is the merge of the walks (9.2)
and (9.3). In general, if a walk f ends with a vertex, v, and a walk r starts with the
same vertex, v, we‚Äôll say that their merge, fbr, is the walk that starts with f and

Chapter 9
Directed graphs & Partial Orders
278
continues with r.1 Two walks can only be merged if the Ô¨Årst ends with the same
vertex, v, that the second one starts with. Sometimes it‚Äôs useful to name the node v
where the walks merge; we‚Äôll use the notation fbv r to describe the merge of a walk
f that ends at v with a walk r that begins at v.
A consequence of this deÔ¨Ånition is that
Lemma 9.1.5.
jfbrj D jfj C jrj:
In the next section we‚Äôll get mileage out of walking this way.
9.1.1
Finding a Path
If you were trying to walk somewhere quickly, you‚Äôd know you were in trouble if
you came to the same place twice. This is actually a basic theorem of graph theory.
Theorem 9.1.6. The shortest walk from one vertex to another is a path.
Proof. If there is a walk from vertex u to v, there must, by the Well Ordering
Principle, be a minimum length walk w from u to v. We claim w is a path.
To prove the claim, suppose to the contrary that w is not a path, namely, some
vertex x occurs twice on this walk. That is,
w D ebx fbx g
for some walks e; f; g where the length of f is positive. But then ‚Äúdeleting‚Äù f yields
a strictly shorter walk
ebx g
from u to v, contradicting the minimality of w.

DeÔ¨Ånition 9.1.7. The distance dist .u; v/, in a graph from vertex u to vertex v is
the length of a shortest path from u to v.
As would be expected, this deÔ¨Ånition of distance satisÔ¨Åes:
Lemma 9.1.8. [The Triangle Inequality]
dist .u; v/  dist .u; x/ C dist .x; v/
for all vertices u; v; x with equality holding iff x is on a shortest path from u to v.
1It‚Äôs tempting to say the merge is the concatenation of the two walks, but that wouldn‚Äôt quite be
right because if the walks were concatenated, the vertex v would appear twice in a row where the
walks meet.

9.2. Adjacency Matrices
279
Of course you may expect this property to be true, but distance has a technical
deÔ¨Ånition and its properties can‚Äôt be taken for granted. For example, unlike ordinary
distance in space, the distance from u to v is typically different from the distance
from v to u. So let‚Äôs prove the Triangle Inequality:
Proof. To prove the inequality, suppose f is a shortest path from u to x and r
is a shortest path from x to v. Then by Lemma 9.1.5, f bx r is a walk of length
dist .u; x/ C dist .x; v/ from u to v, so this sum is an upper bound on the length of
the shortest path from u to v by Theorem 9.1.6.
To prove the ‚Äúiff‚Äù from left to right, suppose dist .u; v/ D dist .u; x/Cdist .x; v/.
Then merging a shortest path from u to x with shortest path from x to v yields a
walk whose length is dist .u; x/Cdist .x; v/ which by assumption equals dist .u; v/.
This walk must be a path or it could be shortened, giving a smaller distance from u
to v. So this is a shortest path containing x.
To prove the ‚Äúiff‚Äù from right to left, suppose vertex x is on a shortest path w
from u to v, namely, w is a shortest path of the form f bx r. The path f must be a
shortest path from u to x; otherwise replacing f by a shorter path from u to x would
yield a shorter path from u to v than w. Likewise r must be a shortest path from x
to v. So dist .u; v/ D jwj D jfj C jrj D dist .u; x/ C dist .x; v/.

9.2
Adjacency Matrices
If a graph, G, has n vertices, v0; v1; : : : ; vn 1, a useful way to represent it is with
an n  n matrix of zeroes and ones called its adjacency matrix, AG. The ij th entry,
.AG/ij , of the adjacency matrix is 1 if there is an edge from vertex vi to vertex vj ,
and 0 otherwise. That is,
.AG/ij WWD
(
1
if
Àù
vi !vj
Àõ
2 E.G/;
0
otherwise:
For example, let H be the 4-node graph shown in Figure 9.1. Then its adjacency
matrix AH is the 4  4 matrix:
AH D
a
b
c
d
a
0
1
0
1
b
0
0
1
1
c
0
1
0
0
d
0
0
1
0

Chapter 9
Directed graphs & Partial Orders
280
A payoff of this representation is that we can use matrix powers to count numbers
of walks between vertices. For example, there are two length-2 walks between
vertices a and c in the graph H, namely
a ha!bi b hb !ci c
a ha!di d hd !ci c
and these are the only length-2 walks from a to c. Also, there is exactly one length-
2 walk from b to c and exactly one length-2 walk from c to c and from d to b, and
these are the only length-2 walks in H. It turns out we could have read these counts
from the entries in the matrix .AH/2:
.AH/2 D
a
b
c
d
a
0
0
2
1
b
0
1
1
0
c
0
0
1
1
d
0
1
0
0
More generally, the matrix .AG/k provides a count of the number of length k
walks between vertices in any digraph, G, as we‚Äôll now explain.
DeÔ¨Ånition 9.2.1. The length-k walk counting matrix for an n-vertex graph G is the
n  n matrix C such that
Cuv WWD the number of length-k walks from u to v:
(9.4)
Notice that the adjacency matrix AG is the length-1 walk counting matrix for
G, and that.AG/0, which by convention is the identity matrix, is the length-0 walk
counting matrix.
Theorem 9.2.2. If C is the length-k walk counting matrix for a graph G, and D
is the length-m walk counting matrix, then CD is the length k C m walk counting
matrix for G.
According to this theorem, the square .AG/2 of the adjacency matrix is the
length-2 walk counting matrix for G. Applying the theorem again to .AG/2AG,
shows that the length-3 walk counting matrix is .AG/3. More generally, it follows
by induction that
Corollary 9.2.3. The length-k counting matrix of a digraph, G, is .AG/k, for all
k 2 N.

9.2. Adjacency Matrices
281
In other words, you can determine the number of length k walks between any
pair of vertices simply by computing the kth power of the adjacency matrix!
That may seem amazing, but the proof uncovers this simple relationship between
matrix multiplication and numbers of walks.
Proof of Theorem 9.2.2. Any length-.kCm/ walk between vertices u and v begins
with a length-k walk starting at u and ending at some vertex, w, followed by a
length-m walk starting at w and ending at v. So the number of length-.k C m/
walks from u to v that go through w at the kth step equals the number Cuw of
length-k walks from u to w, times the number Dwv of length-m walks from w to
v. We can get the total number of length-.k C m/ walks from u to v by summing,
over all possible vertices w, the number of such walks that go through w at the kth
step. In other words,
#length-.k C m/ walks from u to v D
X
w2V.G/
Cuw  Dwv
(9.5)
But the right hand side of (9.5) is precisely the deÔ¨Ånition of .CD/uv. Thus, CD is
indeed the length-.k C m/ walk counting matrix.

9.2.1
Shortest Paths
The relation between powers of the adjacency matrix and numbers of walks is cool
(to us math nerds at least), but a much more important problem is Ô¨Ånding shortest
paths between pairs of nodes. For example, when you drive home for vacation, you
generally want to take the shortest-time route.
One simple way to Ô¨Ånd the lengths of all the shortest paths in an n-vertex graph,
G, is to compute the successive powers of AG one by one up to the n   1st, watch-
ing for the Ô¨Årst power at which each entry becomes positive. That‚Äôs because The-
orem 9.2.2 implies that the length of the shortest path, if any, between u and v,
that is, the distance from u to v, will be the smallest value k for which .AG/k
uv is
nonzero, and if there is a shortest path, its length will be  n   1. ReÔ¨Ånements of
this idea lead to methods that Ô¨Ånd shortest paths in reasonably efÔ¨Åcient ways. The
methods apply as well to weighted graphs, where edges are labelled with weights
or costs and the objective is to Ô¨Ånd least weight, cheapest paths. These reÔ¨Ånements
are typically covered in introductory algorithm courses, and we won‚Äôt go into them
here any further.

Chapter 9
Directed graphs & Partial Orders
282
9.3
Walk Relations
A basic question about a digraph is whether there is a path from one particular
vertex to another. So for any digraph, G, we are interested in a binary relation, G,
called the walk relation on V.G/ where
u G v WWD there is a walk in G from u to v:
(9.6)
Similarly, there is a positive walk relation
u GC v WWD there is a positive length walk in G from u to v:
(9.7)
Since merging a walk from u to v with a walk from v to w gives a walk from u
to w, both walk relations have a relational property called transitivity:
DeÔ¨Ånition 9.3.1. A binary relation, R, on a set, A, is transitive iff
.a R b AND b R c/ IMPLIES a R c
for every a; b; c 2 A.
Since there is a length-0 walk from any vertex to itself, the walk relation has
another relational property called reÔ¨Çexivity:
DeÔ¨Ånition 9.3.2. A binary relation, R, on a set, A, is reÔ¨Çexive iff a R a for all
a 2 A.
9.3.1
Composition of Relations
There is a simple way to extend composition of functions to composition of rela-
tions, and this gives another way to talk about walks and paths in digraphs.
DeÔ¨Ånition 9.3.3. Let R W B ! C and S W A ! B be binary relations. Then the
composition of R with S is the binary relation .R ƒ± S/ W A ! C deÔ¨Åned by the
rule
a .R ƒ± S/ c WWD 9b 2 B: .a S b/ AND .b R c/:
(9.8)
This agrees with the DeÔ¨Ånition 4.3.1 of composition in the special case when R
and S are functions.2
2The reversal of the order of R and S in (9.8) is not a typo. This is so that relational composition
generalizes function composition. The value of function f composed with function g at an argument,
x, is f .g.x//. So in the composition, f ƒ± g, the function g is applied Ô¨Årst.

9.4. Directed Acyclic Graphs & Partial Orders
283
Remembering that a digraph is a binary relation on its vertices, it makes sense
to compose a digraph G with itself. Then if we let Gn denote the composition of
G with itself n times, it‚Äôs easy to check (see Problem 9.11) that Gn is the length-n
walk relation:
a Gn b
iff
there is a length-n walk in G from a to b:
This even works for n D 0, with the usual convention that G0 is the identity relation
IdV.G/ on the set of vertices.3 Since there is a walk iff there is a path, and every
path is of length at most jV.G/j   1, we now have4
G D G0 [ G1 [ G2 [ : : : [ GjV.G/j 1 D .G [ G0/jV.G/j 1:
(9.9)
The Ô¨Ånal equality points to the use of repeated squaring as a way to compute G
with log n rather than n   1 compositions of relations.
9.4
Directed Acyclic Graphs & Partial Orders
Some of the prerequisites of MIT computer science subjects are shown in Fig-
ure 9.6. An edge going from subject s to subject t indicates that s is listed in the
catalogue as a direct prerequisite of t. Of course, in order to take subject t, you
not only have to take subject s Ô¨Årst, but you also have to take all the prerequisites
of s, as well as any prerequisites of these prerequisites, and so on. We can state
this precisely in terms of the positive walk relation: if D is the direct prerequisite
relation on subjects, then subject u has to be completed before taking subject v iff
u DC v.
It would clearly have a dire effect on the time it takes to graduate if this direct
prerequisite graph had a positive length cycle :-) So the direct prerequisite graph
among subjects had better be acyclic:
DeÔ¨Ånition 9.4.1. A directed acyclic graph (DAG) is a directed graph with no posi-
tive length cycles.
3The identity relation, IdA, on a set, A, is the equality relation:
a IdA b
iff
a D b;
for a; b 2 A.
4Equation (9.9) involves a harmless abuse of notation: we should have written
graph.G/ D graph.G0/ [ graph.G1/ : : : :

Chapter 9
Directed graphs & Partial Orders
284
New 6-3: SB in Computer Science and Engineering
All subjects are 12 units
6.UAT 
6 units 
6.UAT
6.UAT 
6 units
6 units
6.UAP 
6 units 
6.UAP
6.UAP 
6 units
6 units
Subjects
Advanced Undergraduate Subjects
AUS http://www.eecs.mit.edu/ug/newcurriculum/aus.html
Advanced Undergraduate Subjects
Advanced Undergraduate Subjects
AUS 
AUS http://
http://www.eecs.mit.edu/ug/newcurriculum/aus.html
www.eecs.mit.edu/ug/newcurriculum/aus.html
2
1
3 
Header
6.033 
comp sys 
6.033
6.033 
comp sys
comp sys
6.034 
AI 
6.034
6.034 
AI
AI
6.046 
adv algorithms 
6.046
6.046 
adv algorithms
adv algorithms
6.006* 
algorithms 
6.006*
6.006* 
algorithms
algorithms
6.01* 
intro EECS I 
6.01*
6.01* 
intro EECS I
intro EECS I
6.02* 
intro EECS II 
6.02*
6.02* 
intro EECS II
intro EECS II
Software Lab
(http://www.eecs.mit.edu/ug/newcurriculum/verghese_6.005.html)
Software Lab
Software Lab
((http://www.eecs.mit.edu/ug/newcurriculum/verghese_6.005.html)
http://www.eecs.mit.edu/ug/newcurriculum/verghese_6.005.html)
8.02
8.02
8.02
coreq
6.004 
comp architecture 
6.004
6.004 
comp architecture
comp architecture
coreq
3 
Foundation
¬Ω + ¬Ω
2 
Introductory
(= 1 Institute Lab)
2 
Math 
(= 2 REST)
Elementary 
exposure to programming 
(high school, IAP, or 6.00) 
Elementary 
Elementary 
exposure to programming
exposure to programming 
(high school, IAP, or 6.00)
(high school, IAP, or 6.00)
*new subject
June 2009
18.06  or 18.03
18.06 
linear algebra 
18.06
18.06 
linear algebra
linear algebra
18.03 
diff eqs 
18.03
18.03 
diff 
diff eqs
eqs
6.042 
discrete math 
6.042
6.042 
discrete math
discrete math
6.005* 
software 
6.005*
6.005* 
software
software
Figure 9.6
Subject prerequisites for MIT Computer Science (6-3) Majors.

9.4. Directed Acyclic Graphs & Partial Orders
285
DAG‚Äôs come up constantly because, among other things, they model task schedul-
ing problems, where nodes represent tasks to be completed and arrows indicate
which tasks must be completed before others can begin. They have particular im-
portance in computer science because, besides modeling task scheduling problems,
they capture key concepts used, for example, in analyzing concurrency control;
we‚Äôll expand on this in Section 9.9.
The relationship between walks and paths extends to closed walks and cycles.
Lemma 9.4.2. The shortest positive length closed walk through a vertex is a posi-
tive length cycle through that vertex.
The proof is essentially the same as for Theorem 9.1.6 (see Problem 9.9). This
implies that a graph D is a DAG iff it has no positive length walk from any vertex
to itself. This relational property of DC is called irreÔ¨Çexivity.
DeÔ¨Ånition 9.4.3. A binary relation, R, on a set, A, is irreÔ¨Çexive iff
NOT.a R a/
for all a 2 A.
So we have
Lemma 9.4.4. R is a DAG iff RC is irreÔ¨Çexive.
DeÔ¨Ånition 9.4.5. A relation that is transitive and irreÔ¨Çexive is called a strict partial
order.
Since we know that the positive walk relation is transitive, we have
Lemma 9.4.6. If D is a DAG, then DC is a strict partial order.
The transitivity property of a relation says that where there‚Äôs a length two walk,
there is an edge. This implies by induction that where there is a walk of any positive
length, there is an edge (see Problem 9.10), namely:
Lemma 9.4.7. If a binary relation R is transitive, then RC D R.
Corollary 9.4.8. If R is a strict partial order, then R is a DAG.
Proof. If vertex a is on a positive length cycle in the graph of R, then a RC a holds
by deÔ¨Ånition, which in particular implies that RC is not irreÔ¨Çexive. This means that
if RC is irreÔ¨Çexive, then R must be a DAG.
But if R is a strict partial order, then by deÔ¨Ånition it is irreÔ¨Çexive and by Lemma 9.4.7
RC D R, so RC is indeed irreÔ¨Çexive.


Chapter 9
Directed graphs & Partial Orders
286
To summarize, we have
Theorem 9.4.9. A relation is a strict partial order iff it is the positive walk relation
of a DAG.
Another consequence of Lemma 9.4.2 is that if a graph is a DAG, it cannot
have two vertices with positive length walks in both directions between them. This
relational property of a positive walk relation is called asymmetry.
DeÔ¨Ånition 9.4.10. A binary relation, R, on a set, A, is asymmetric iff
a R b IMPLIES NOT.b R a/
for all a; b 2 A.
That is, Lemma 9.4.2 implies
Corollary 9.4.11. R is a DAG iff RC is asymmetric.
And immediately from Corollary 9.4.11 and Theorem 9.4.9 we get
Corollary 9.4.12. R is a strict partial order iff it is transitive and asymmetric.5
A strict partial order may be the positive walk relation of different DAG‚Äôs. This
raises the question of Ô¨Ånding a DAG with the smallest number of edges that deter-
mines a given strict partial order. For Ô¨Ånite strict partial orders, the smallest such
DAG turns out to be unique and easy to Ô¨Ånd (see Problem 9.5).
9.5
Weak Partial Orders
Partial orders come up in many situations which on the face of it have nothing to do
with digraphs. For example, the less-than order, <, on numbers is a partial order:
 if x < y and y < z then x < z, so less-than is transitive, and
 if x < y then y 6< x, so less-than is asymmetric.
The proper containment relation  is also a partial order:
 if A  B and B  C then A  C, so containment is transitive, and
 A 6 A, so proper containment is irreÔ¨Çexive.
5Some texts use this Corollary to deÔ¨Åne strict partial orders.

9.5. Weak Partial Orders
287
The less-than-or-equal relation, , is at least as familiar as the less-than strict
partial order, and the ordinary containment relation, , is even more common than
the proper containment relation. These are examples of weak partial orders, which
are just strict partial orders with the additional condition that every element is re-
lated to itself. To state this precisely, we have to relax the asymmetry property so it
does not apply when a vertex is compared to itself; this relaxed property is called
antisymmetry:
DeÔ¨Ånition 9.5.1. A binary relation, R, on a set A, is antisymmetric iff
a R b IMPLIES NOT.b R a/
for all a ¬§ b 2 A.
Now we can give an axiomatic deÔ¨Ånition of weak partial orders that parallels the
deÔ¨Ånition of strict partial orders.6
DeÔ¨Ånition 9.5.2. A binary relation on a set is a weak partial order iff it is transitive,
reÔ¨Çexive, and antisymmetric.
The following lemma gives another characterization of weak partial orders that
follows directly from this deÔ¨Ånition.
Lemma 9.5.3. A relation R on a set, A, is a weak partial order iff there is a strict
partial order, S, on A such that
a R b
iff
.a S b OR a D b/;
for all a; b 2 A.
Since a length zero walk goes from a vertex to itself, this lemma combined with
Theorem 9.4.9 yields:
Corollary 9.5.4. A relation is a weak partial order iff it is the walk relation of a
DAG.
For weak partial orders in general, we often write an ordering-style symbol like
 or v instead of a letter symbol like R.7 Likewise, we generally use  or @ to
indicate a strict partial order.
Two more examples of partial orders are worth mentioning:
6Some authors deÔ¨Åne partial orders to be what we call weak partial orders, but we‚Äôll use the phrase
‚Äúpartial order‚Äù to mean either a weak or strict one.
7General relations are usually denoted by a letter like R instead of a cryptic squiggly symbol, so
 is kind of like the musical performer/composer Prince, who redeÔ¨Åned the spelling of his name to
be his own squiggly symbol. A few years ago he gave up and went back to the spelling ‚ÄúPrince.‚Äù

Chapter 9
Directed graphs & Partial Orders
288
Example 9.5.5. Let A be some family of sets and deÔ¨Åne a R b iff a  b. Then R
is a strict partial order.
For integers, m; n we write m j n to mean that m divides n, namely, there is an
integer, k, such that n D km.
Example 9.5.6. The divides relation is a weak partial order on the nonnegative
integers.
9.6
Representing Partial Orders by Set Containment
Axioms can be a great way to abstract and reason about important properties of
objects, but it helps to have a clear picture of the things that satisfy the axioms.
DAG‚Äôs provide one way to picture partial orders, but it also can help to picture
them in terms of other familiar mathematical objects. In this section we‚Äôll show that
every partial order can be pictured as a collection of sets related by containment.
That is, every partial order has the ‚Äúsame shape‚Äù as such a collection. The technical
word for ‚Äúsame shape‚Äù is ‚Äúisomorphic.‚Äù
DeÔ¨Ånition 9.6.1. A binary relation, R, on a set, A, is isomorphic to a relation, S,
on a set B iff there is a relation-preserving bijection from A to B. That is, there is
a bijection f W A ! B, such that for all a; a0 2 A,
a R a0
iff
f .a/ S f .a0/:
To picture a partial order, , on a set, A, as a collection of sets, we simply
represent each element A by the set of elements that are  to that element, that is,
a  ! fb 2 A j b  ag:
For example, if  is the divisibility relation on the set of integers, f1; 3; 4; 6; 8; 12g,
then we represent each of these integers by the set of integers in A that divides it.
So
1  ! f1g
3  ! f1; 3g
4  ! f1; 4g
6  ! f1; 3; 6g
8  ! f1; 4; 8g
12  ! f1; 3; 4; 6; 12g

9.7. Path-Total Orders
289
So, the fact that 3 j 12 corresponds to the fact that f1; 3g  f1; 3; 4; 6; 12g.
In this way we have completely captured the weak partial order  by the subset
relation on the corresponding sets. Formally, we have
Lemma 9.6.2. Let  be a weak partial order on a set, A. Then  is isomorphic to
the subset relation, , on the collection of inverse images under the  relation of
elements a 2 A.
We leave the proof to Problem 9.19. Essentially the same construction shows
that strict partial orders can be represented by sets under the proper subset relation,
 (Problem 9.20). To summarize:
Theorem 9.6.3. Every weak partial order, , is isomorphic to the subset relation,
, on a collection of sets.
Every strict partial order, , is isomorphic to the proper subset relation, , on a
collection of sets.
9.7
Path-Total Orders
The familiar order relations on numbers have an important additional property:
given two different numbers, one will be bigger than the other. Partial orders with
this property are said to be path-total orders.8
DeÔ¨Ånition 9.7.1. Let R be a binary relation on a set, A, and let a; b be elements of
A. Then a and b are comparable with respect to R iff ≈ía R b OR b R a¬ç. A partial
order for which every two different elements are comparable is called a path-total
order.
So < and  are path-total orders on R. On the other hand, the subset relation is
not path-total, since, for example, any two different Ô¨Ånite sets of the same size will
be incomparable under . The prerequisite relation on Course 6 required subjects
is also not path-total because, for example, neither 8.01 nor 6.042 is a prerequisite
of the other.
The name path-total is based on the following
8Path-total partial orders are conventionally just called ‚Äútotal.‚Äù But this terminology conÔ¨Çicts with
the deÔ¨Ånition of ‚Äútotal relation,‚Äù and it regularly confuses students. So we chose the terminology
‚Äúpath-total‚Äù to avoid the confusion. Some texts use linear orders as the name for path-total orders.
Being a path-total partial order is a much stronger condition than being a partial order that is a
total relation. For example, any weak partial order such as  is a total relation but generally won‚Äôt be
path-total.

Chapter 9
Directed graphs & Partial Orders
290
Lemma 9.7.2. For any Ô¨Ånite, nonempty set of vertices from a path-total digraph,
there is a directed path going through exactly these vertices. In fact, if the digraph
is a DAG, the directed path is unique.
Lemma 9.7.2 is easy to prove by induction on the size of the set of vertices. The
proof is given in Problem 9.6.
9.8
Product Orders
Taking the product of two relations is a useful way to construct new relations from
old ones.
DeÔ¨Ånition 9.8.1. The product, R1  R2, of relations R1 and R2 is deÔ¨Åned to be
the relation with
domain.R1  R2/
WWD
domain.R1/  domain.R2/;
codomain.R1  R2/
WWD
codomain.R1/  codomain.R2/;
.a1; a2/ .R1  R2/ .b1; b2/
iff
≈ía1 R1 b1 and a2 R2 b2¬ç:
Example 9.8.2. DeÔ¨Åne a relation, Y , on age-height pairs of being younger and
shorter. This is the relation on the set of pairs .y; h/ where y is a nonnegative
integer  2400 which we interpret as an age in months, and h is a nonnegative
integer  120 describing height in inches. We deÔ¨Åne Y by the rule
.y1; h1/ Y .y2; h2/
iff
y1  y2 AND h1  h2:
That is, Y is the product of the -relation on ages and the -relation on heights.
It follows directly from the deÔ¨Ånitions that products preserve the properties of
transitivity, reÔ¨Çexivity, irreÔ¨Çexivity, and antisymmetry, as shown in Problem 9.29.
That is, if R1 and R2 both have one of these properties, then so does R1 R2. This
implies that if R1 and R2 are both partial orders, then so is R1  R2.
On the other hand, the property of being a path-total order is not preserved. For
example, the age-height relation Y is the product of two path-total orders, but it
is not path-total: the age 240 months, height 68 inches pair, (240,68), and the pair
(228,72) are incomparable under Y .

9.9. Scheduling
291
underwear
shirt
jacket
belt
right shoe
left shoe
right sock
left sock
tie
pants
Figure 9.7
DAG describing which clothing items have to be put on before others.
9.9
Scheduling
Scheduling problems are a common source of partial orders: there is a set, A, of
tasks and a set of constraints specifying that starting a certain task depends on
other tasks being completed beforehand. We can picture the constraints by drawing
labelled boxes corresponding to different tasks, with an arrow from one box to
another if the Ô¨Årst box corresponds to a task that must be completed before starting
the second one.
For example, the DAG for in Figure 9.7 describes how a guy might get dressed
for a formal occasion. The vertices correspond to garments and the edges specify
which garments have to be put on before others are.
When we have a partial order like this on the order in which tasks can be per-
formed, it can be useful to have an order in which to perform all the tasks, one at a
time, while respecting the dependency constraints. This amounts to Ô¨Ånding a path-
total order that is consistent with the partial order. This task of Ô¨Ånding a path-total
ordering that is consistent with a partial order is known as topological sorting.

Chapter 9
Directed graphs & Partial Orders
292
underwear
left sock
shirt
shirt
pants
tie
belt
underwear
tie
right sock
jacket
pants
left sock
right shoe
right sock
belt
left shoe
jacket
right shoe
left shoe
(a)
(b)
Figure 9.8
Two possible topological sorts of the partial order described in Fig-
ure 9.7. In each case, the elements are listed so that x  y iff x is above y in the
list.
DeÔ¨Ånition 9.9.1. A topological sort of a partial order, , on a set, A, is a path-total
ordering, @, on A such that
a  b IMPLIES a @ b:
There are several path-total orders that are consistent with the partial order shown
in Figure 9.7. We have shown two of them in list form in Figure 9.8. Each such
list is a topological sort for the partial order in Figure 9.7. In what follows, we will
prove that every Ô¨Ånite partial order has a topological sort. You can think of this as a
mathematical proof that you can get dressed in the morning (and then show up for
math lecture).
Topological sorts for partial orders on Ô¨Ånite sets are easy to construct by starting
from minimal elements:
DeÔ¨Ånition 9.9.2. Let  be a partial order on a set, A. An element a0 2 A is
minimum iff it is  every other element of A, that is, a0  b for all b ¬§ a0.
The element a0 is minimal iff no other element is  a0, that is, NOT.b  a0/ for
all b ¬§ a0.
There are corresponding deÔ¨Ånitions for maximum and maximal. Alternatively, a
maximum(al) element for a relation, R, could be deÔ¨Åned as a minimum(al) element
for R 1.
In a path-total order, minimum and minimal elements are the same thing. But a
partial order may have no minimum element but lots of minimal elements. There

9.9. Scheduling
293
are four minimal elements in the clothes example: leftsock, rightsock, underwear,
and shirt.
To construct a path-total ordering for getting dressed, we pick one of these min-
imal elements, say shirt. Next we pick a minimal element among the remaining
ones. For example, once we have removed shirt, tie becomes minimal. We con-
tinue in this way removing successive minimal elements until all elements have
been picked. The sequence of elements in the order they were picked will be a
topological sort. This is how the topological sort above for getting dressed was
constructed.
So our construction shows:
Theorem 9.9.3. Every partial order on a Ô¨Ånite set has a topological sort.
There are many other ways of constructing topological sorts. For example, in-
stead of starting ‚Äúfrom the bottom‚Äù with minimal elements, we could build a path-
total ordering starting anywhere and simply keep putting additional elements into
the path-total order wherever they will Ô¨Åt. In fact, the domain of the partial order
need not even be Ô¨Ånite: we won‚Äôt prove it, but all partial orders, even inÔ¨Ånite ones,
have topological sorts.
9.9.1
Parallel Task Scheduling
For a partial order of task dependencies, topological sorting provides a way to ex-
ecute tasks one after another while respecting the dependencies. But what if we
have the ability to execute more than one task at the same time? For example, say
tasks are programs, the partial order indicates data dependence, and we have a par-
allel machine with lots of processors instead of a sequential machine with only one.
How should we schedule the tasks? Our goal should be to minimize the total time
to complete all the tasks. For simplicity, let‚Äôs say all the tasks take the same amount
of time and all the processors are identical.
So, given a Ô¨Ånite partially ordered set of tasks, how long does it take to do them
all, in an optimal parallel schedule? We can also use partial order concepts to
analyze this problem.
In the Ô¨Årst unit of time, we should do all minimal items, so we would put on our
left sock, our right sock, our underwear, and our shirt.9 In the second unit of time,
we should put on our pants and our tie. Note that we cannot put on our left or right
shoe yet, since we have not yet put on our pants. In the third unit of time, we should
9Yes, we know that you can‚Äôt actually put on both socks at once, but imagine you are being dressed
by a bunch of robot processors and you are in a big hurry. Still not working for you? Ok, forget about
the clothes and imagine they are programs with the precedence constraints shown in Figure 9.7.

Chapter 9
Directed graphs & Partial Orders
294
underwear
shirt
jacket
belt
right shoe
left shoe
right sock
left sock
tie
pants
A1
A2
A3
A4
Figure 9.9
A parallel schedule for the tasks-in-getting-dressed partial order in
Figure 9.7. The tasks in Ai can be performed in step i for 1  i  4. A chain of
length 4 (the critical path in this example) is shown with bold edges.
put on our left shoe, our right shoe, and our belt. Finally, in the last unit of time,
we can put on our jacket. This schedule is illustrated in Figure 9.9.
The total time to do these tasks is 4 units. We cannot do better than 4 units of
time because there is a sequence of 4 tasks, each needing to be done before the
next, of length 4. For example, we must put on our shirt before our pants, our pants
before our belt, and our belt before our jacket. Such a sequence of items is known
as a chain.
DeÔ¨Ånition 9.9.4. A chain in a partial order is a set of elements such that any two
different elements in the set are comparable. A chain is said to end at its maximum
element.
Thus, the time it takes to schedule tasks, even with an unlimited number of pro-
cessors, is at least the length of the longest chain. Indeed, if we used less time, then
two items from a longest chain would have to be done at the same time, which con-
tradicts the precedence constraints. For this reason, a longest chain is also known
as a critical path. For example, Figure 9.9 shows the critical path for the getting-

9.9. Scheduling
295
dressed partial order.
In this example, we were in fact able to schedule all the tasks in t steps, where t
is the length of the longest chain. The really nice thing about partial orders is that
this is always possible! In other words, for any partial order, there is a legal parallel
schedule that runs in t steps, where t is the length of the longest chain.
In general, a schedule for performing tasks speciÔ¨Åes which tasks to do at succes-
sive steps. Every task, a, has to be scheduled at some step, and all the tasks that
have to be completed before task a must be scheduled for an earlier step.
DeÔ¨Ånition 9.9.5. A partition of a set A is a set of nonempty subsets of A called the
blocks10 of the partition, such that
 every element of A is in some block, and
 if B and B0 are different blocks, then B \ B0 D ;.
For example, one possible partition of the set fa; b; c; d; eg into three blocks is
fa; cg
fb; eg
fdg:
DeÔ¨Ånition 9.9.6. A parallel schedule for a strict partial order, , on a set, A, is a
partition of A into blocks A0; A1; : : : ; such that for all a; b 2 A, k 2 N,
≈ía 2 Ak AND b  a¬ç
IMPLIES
b 2 Aj for some j < k:
The block Ak is called the set of elements scheduled at step k, and the length of
the schedule is the number of blocks in the partition. The maximum number of
elements scheduled at any step is called the number of processors required by the
schedule.
In general, the earliest step at which an element a can ever be scheduled must be
at least as large as any chain that ends at a. A largest chain ending at a is called a
critical path to a, and the size of the critical path is called the depth of a. So in any
possible parallel schedule, it takes at least depth .a/ steps to complete task a.
There is a very simple schedule that completes every task in this minimum num-
ber of steps. Just use a ‚Äúgreedy‚Äù strategy of performing tasks as soon as possible.
Namely, schedule all the elements of depth k at step k. That‚Äôs how we found the
schedule for getting dressed given above.
Theorem 9.9.7. Let  be a strict partial order on a set, A. A minimum length
schedule for  consists of the sets A0; A1; : : : ; where
Ak WWD fa j depth .a/ D kg:
10We think it would be nicer to call them the parts of the partition, but ‚Äúblocks‚Äù is the standard
terminology.

Chapter 9
Directed graphs & Partial Orders
296
We‚Äôll leave to Problem 9.37 the proof that the sets Ak are a parallel schedule
according to DeÔ¨Ånition 9.9.6.
The minimum number of steps needed to schedule a partial order, , is called
the parallel time required by , and a largest possible chain in  is called a critical
path for . So we can summarize the story above in this way: with an unlimited
number of processors, the parallel time to complete all tasks is simply the size of a
critical path:
Corollary 9.9.8. Parallel time = length of critical path.
Things get a little more interesting when the number of processors is bounded
(see Problem 9.39).
9.9.2
Dilworth‚Äôs Lemma
DeÔ¨Ånition 9.9.9. An antichain in a partial order is a set of elements such that any
two elements in the set are incomparable.
Our conclusions about scheduling also tell us something about antichains.
Corollary 9.9.10. If the largest chain in a partial order on a set, A, is of size t,
then A can be partitioned into t antichains.
Proof. Let the antichains be the sets Ak WWD fa j depth .a/ D kg. It is an easy
exercise to verify that each Ak is an antichain (Problem 9.37).

Corollary 9.9.10 implies a famous result11 about partially ordered sets:
Lemma 9.9.11 (Dilworth). For all t > 0, every partially ordered set with n ele-
ments must have either a chain of size greater than t or an antichain of size at least
n=t.
Proof. Assume there is no chain of size greater than t, that is, the largest chain is
of size  t. Then by Corollary 9.9.10, the n elements can be partitioned into t or
fewer antichains. Let ` be the size of the largest antichain. Since every element
belongs to exactly one antichain, and there are at most t antichains, there can‚Äôt
be more than `t elements, namely, `t  n. So there is an antichain with at least
`  n=t elements.

Corollary 9.9.12. Every partially ordered set with n elements has a chain of size
greater than pn or an antichain of size at least pn.
11Lemma 9.9.11 also follows from a more general result known as Dilworth‚Äôs Theorem which we
will not discuss.

9.10. Equivalence Relations
297
Proof. Set t D pn in Lemma 9.9.11.

Example 9.9.13. In the dressing partially ordered set, n D 10.
Try t D 3. There is a chain of size 4.
Try t D 4. There is no chain of size 5, but there is an antichain of size 4  10=4.
Example 9.9.14. Suppose we have a class of 101 students. Then using the product
partial order, Y , from Example 9.8.2, we can apply Dilworth‚Äôs Lemma to conclude
that there is a chain of 11 students who get taller as they get older, or an antichain of
11 students who get taller as they get younger, which makes for an amusing in-class
demo.
9.10
Equivalence Relations
DeÔ¨Ånition 9.10.1. A relation is an equivalence relation if it is reÔ¨Çexive, symmetric,
and transitive.
Congruence modulo n is an excellent example of an equivalence relation:
 It is reÔ¨Çexive because x  x .mod n/.
 It is symmetric because x  y .mod n/ implies y  x .mod n/.
 It is transitive because x  y .mod n/ and y  z .mod n/ imply that x  z
.mod n/.
There is an even more well-known example of an equivalence relation: equality
itself.
Any total function deÔ¨Ånes an equivalence relation on its domain:
DeÔ¨Ånition 9.10.2. If f W A ! B is a total function, deÔ¨Åne a relation f by the
rule:
a f a0 IFF f .a/ D f .a0/:
From its deÔ¨Ånition, f is reÔ¨Çexive, symmetric and transitive because these are
properties of equality. That is, f is an equivalence relation. This observation
gives another way to see that congruence modulo n is an equivalence relation:
the Remainder Lemma 8.6.1 implies that congruence modulo n is the same as r
where r.a/ is the remainder of a divided by n.
In fact, a relation is an equivalence relation iff it equal f for some total func-
tion f (see Problem 9.43). So equivalence relations could be been deÔ¨Åned using
DeÔ¨Ånition 9.10.2.

Chapter 9
Directed graphs & Partial Orders
298
9.10.1
Equivalence Classes
Equivalence relations are closely related to partitions because the images of ele-
ments under an equivalence relation form the blocks of a partition.
DeÔ¨Ånition 9.10.3. Given an equivalence relation R W A ! A, the equivalence
class, ≈ía¬çR, of an element a 2 A is the set of all elements of A related to a by R.
Namely,
≈ía¬çR WWD fx 2 A j a R xg:
In other words, ≈ía¬çR is the image R.a/.
For example, suppose that A D Z and a R b means that a  b .mod 5/. Then
≈í7¬çR D f: : : ;  3; 2; 7; 12; 22; : : :g:
Notice that 7, 12, 17, etc., all have the same equivalence class; that is, ≈í7¬çR D
≈í12¬çR D ≈í17¬çR D    .
There is an exact correspondence between equivalence relations on A and parti-
tions of A. Namely, given on one hand any partition of a set, then being in the same
block is obviously an equivalence relation. On the other hand we have:
Theorem 9.10.4. The equivalence classes of an equivalence relation on a set A
form a partition of A.
We‚Äôll leave the proof of Theorem 9.10.4 as an easy exercise in axiomatic reason-
ing (see Problem 9.42), but let‚Äôs look at an example. The congruent-mod-5 relation
partitions the integers into Ô¨Åve equivalence classes:
f: : : ;  5; 0; 5; 10; 15; 20; : : :g
f: : : ;  4; 1; 6; 11; 16; 21; : : :g
f: : : ;  3; 2; 7; 12; 17; 22; : : :g
f: : : ;  2; 3; 8; 13; 18; 23; : : :g
f: : : ;  1; 4; 9; 14; 19; 24; : : :g
In these terms, x  y .mod 5/ is equivalent to the assertion that x and y are both
in the same block of this partition. For example, 6  16 .mod 5/, because they‚Äôre
both in the second block, but 2 6 9 .mod 5/ because 2 is in the third block while
9 is in the last block.
In social terms, if ‚Äúlikes‚Äù were an equivalence relation, then everyone would be
partitioned into cliques of friends who all like each other and no one else.

9.11. Summary of Relational Properties
299
9.11
Summary of Relational Properties
A relation R W A ! A is the same as a digraph with vertices A.
ReÔ¨Çexivity R is reÔ¨Çexive when
8x 2 A: x R x:
Every vertex in R has a self-loop.
IrreÔ¨Çexivity R is irreÔ¨Çexive when
NOT≈í9x 2 A: x R x¬ç:
There are no self-loops in R.
Symmetry R is symmetric when
8x; y 2 A: x R y IMPLIES y R x:
If there is an edge from x to y in R, then there is an edge back from y to x
as well.
Asymmetry R is asymmetric when
8x; y 2 A: x R y IMPLIES NOT.y R x/:
There is at most one directed edge between any two vertices in R, and there
are no self-loops.
Antisymmetry R is antisymmetric when
8x ¬§ y 2 A: x R y IMPLIES NOT.y R x/:
Equivalently,
8x; y 2 A: .x R y AND y R x/ IMPLIES x D y:
There is at most one directed edge between any two distinct vertices, but
there may be self-loops.

Chapter 9
Directed graphs & Partial Orders
300
Transitivity R is transitive when
8x; y; z 2 A: .x R y AND y R z/ IMPLIES x R z:
If there is a positive length path from u to v, then there is an edge from u
to v.
Path-Total R is path-total when
8x ¬§ y 2 A: .x R y OR y R x/
Given any two vertices in R, there is an edge in one direction or the other
between them.
For any Ô¨Ånite, nonempty set of vertices of R, there is a directed path going
through exactly these vertices.
Strict Partial Order R is a strict partial order iff R is transitive and irreÔ¨Çexive iff
R is transitive and asymmetric iff it is the positive length walk relation of a
DAG.
Weak Partial Order R is a weak partial order iff R is transitive and anti-symmetric
and reÔ¨Çexive iff R is the walk relation of a DAG.
Equivalence Relation R is an equivalence relation iff R is reÔ¨Çexive, symmetric
and transitive iff R equals the in-the-same-block-relation for some partition
of domain.R/.
Problems for Section 9.3
Practice Problems
Problem 9.1.
Let
A WWD f1; 2; 3g
B WWD f4; 5; 6g
R WWD f.1; 4/; .1; 5/; .2; 5/; .3; 6/g
S WWD f.4; 5/; .4; 6/; .5; 4/g:
Note that R is a relation from A to B and S is a relation from B to B.
List the pairs in each of the relations below.

9.11. Summary of Relational Properties
301
(a) S ƒ± R.
(b) S ƒ± S.
(c) S 1 ƒ± R.
Homework Problems
Problem 9.2.
There is a simple and useful way to extend composition of functions to composition
of relations. Namely, let R W B ! C and S W A ! B be relations. Then the
composition of R with S is the binary relation .R ƒ± S/ W A ! C deÔ¨Åned by the
rule
a .R ƒ± S/ c WWD 9b 2 B: .b R c/ AND .a S b/:
This agrees with the DeÔ¨Ånition 4.3.1 of composition in the special case when R
and S are functions.
We can represent a relation, S, between two sets A D fa1; : : : ; ang and B D
fb1; : : : ; bmg as an nm matrix, MS, of zeroes and ones, with the elements of MS
deÔ¨Åned by the rule
MS.i; j/ D 1
IFF
ai S bj :
If we represent relations as matrices this way, then we can compute the compo-
sition of two relations R and S by a ‚Äúboolean‚Äù matrix multiplication, Àù, of their
matrices. Boolean matrix multiplication is the same as matrix multiplication except
that addition is replaced by OR, multiplication is replaced by AND, and 0 and 1 are
used as the Boolean values False and True. Namely, suppose R W B ! C is a bi-
nary relation with C D fc1; : : : ; cpg. So MR is an m  p matrix. Then MS Àù MR
is an n  p matrix deÔ¨Åned by the rule:
≈íMS Àù MR¬ç.i; j/ WWD ORm
kD1≈íMS.i; k/ AND MR.k; j/¬ç:
(9.10)
Prove that the matrix representation, MRƒ±S, of R ƒ± S equals MS Àù MR (note
the reversal of R and S).
Problems for Section 9.4
Practice Problems
Problem 9.3.
In this DAG (Figure 9.10) for the divisibility relation on f1; : : : ; 12g, there is an
upward path from a to b iff ajb. If 24 was added as a vertex, what is the mini-
mum number of edges that must be added to the DAG to represent divisibility on
f1; : : : ; 12; 24g? What are those edges?

Chapter 9
Directed graphs & Partial Orders
302
1
4
8
5
10
6
3
12
7
11
9
2
Figure 9.10
Problem 9.4. (a) Why is every strict partial order a DAG?
(b) Give an example of a DAG that is not a strict partial order.
(c) Why is the positive walk relation of a DAG a strict partial order?
Class Problems
Problem 9.5.
If a and b are distinct nodes of a digraph, then a is said to cover b if there is an
edge from a to b and every path from a to b includes this edge. If a covers b, the
edge from a to b is called a covering edge.
(a) What are the covering edges in the DAG in Figure 9.11?
(b) Let covering .D/ be the subgraph of D consisting of only the covering edges.
Suppose D is a Ô¨Ånite DAG. Explain why covering .D/ has the same positive walk
relation as D.
Hint: Consider longest paths between a pair of vertices.
(c) Show that if two DAG‚Äôs have the same positive walk relation, then they have
the same set of covering edges.
(d) Conclude that covering .D/ is the unique DAG with the smallest number of
edges among all digraphs with the same positive walk relation as D.
The following examples show that the above results don‚Äôt work in general for
digraphs with cycles.

9.11. Summary of Relational Properties
303
1
2
4
3
5
6
Figure 9.11
DAG with edges not needed in paths
(e) Describe two graphs with vertices f1; 2g which have the same set of covering
edges, but not the same positive walk relation (Hint: Self-loops.)
(f)
(i) The complete digraph without self-loops on vertices 1; 2; 3 has edges
between every two distinct vertices. What are its covering edges?
(ii) What are the covering edges of the graph with vertices 1; 2; 3 and edges
h1!2i ; h2!3i ; h3!1i?
(iii) What about their positive walk relations?
Problem 9.6.
In a round-robin tournament, every two distinct players play against each other
just once. For a round-robin tournament with no tied games, a record of who beat
whom can be described with a tournament digraph, where the vertices correspond
to players and there is an edge hx !yi iff x beat y in their game.
A ranking is a path that includes all the players. So in a ranking, each player won
the game against the next lowest ranked player, but may very well have lost their
games against much lower ranked players ‚Äîwhoever does the ranking may have a
lot of room to play favorites.
(a) Give an example of a tournament digraph with more than one ranking.
(b) Prove that if a tournament digraph is a DAG, then it has at most one ranking.
(c) Prove that every Ô¨Ånite tournament digraph has a ranking.

Chapter 9
Directed graphs & Partial Orders
304
(d) Prove that the greater-than relation, >, on the rational numbers, Q, is a DAG
and a tournament graph that has no ranking.
Problem 9.7.
In an n-player round-robin tournament, every pair of distinct players compete in a
single game. Assume that every game has a winner ‚Äîthere are no ties. The results
of such a tournament can then be represented with a tournament digraph where the
vertices correspond to players and there is an edge hx !yi iff x beat y in their
game.
(a) Explain why a tournament digraph cannot have cycles of length 1 or 2.
(b) Is the ‚Äúbeats‚Äù relation for a tournament graph always/sometimes/never:
 asymmetric?
 reÔ¨Çexive?
 irreÔ¨Çexive?
 transitive?
Explain.
(c) Show that a tournament graph represents a path-total order iff there are no
cycles of length 3.
Problem 9.8.
Suppose that there are n chickens in a farmyard. Chickens are rather aggressive
birds that tend to establish dominance in relationships by pecking. (Hence the term
‚Äúpecking order.‚Äù) In particular, for each pair of distinct chickens, either the Ô¨Årst
pecks the second or the second pecks the Ô¨Årst, but not both. We say that chicken u
virtually pecks chicken v if either:
 Chicken u directly pecks chicken v, or
 Chicken u pecks some other chicken w who in turn pecks chicken v.
A chicken that virtually pecks every other chicken is called a king chicken.
We can model this situation with a chicken digraph whose vertices are chickens
with an edge from chicken u to chicken v precisely when u pecks v. In the graph
in Figure 9.12, three of the four chickens are kings. Chicken c is not a king in
this example since it does not peck chicken b and it does not peck any chicken that

9.11. Summary of Relational Properties
305
a
b
c
d
king
king
king
not a king
Figure 9.12
A 4-chicken tournament in which chickens a, b, and d are kings.
.
pecks chicken b. Chicken a is a king since it pecks chicken d, who in turn pecks
chickens b and c.
(a) DeÔ¨Åne a 10-chicken graph with a king chicken that has degree 1.
(b) Describe a 5-chicken graph in which every player is a king.
(c) Prove
Theorem (King Chicken Theorem). The chicken with the largest outdegree in an
n-chicken tournament is a king.
The King Chicken Theorem means that if the player with the most victories is
defeated by another player x, then at least he/she defeats some third player that
defeats x. In this sense, the player with the most victories has some sort of bragging
rights over every other player. Unfortunately, as Figure 9.12 illustrates, there can
be many other players with such bragging rights, even some with fewer victories.
Homework Problems
Problem 9.9. (a) Give an example of a digraph that has a closed walk including
two vertices but has no cycle including those vertices.
(b) Prove Lemma 9.4.2:
Lemma. The shortest positive length closed walk through a vertex is a cycle.
Problem 9.10.
Prove that if R is a transitive binary relation on a set, A, then R D RC.
Problem 9.11.

Chapter 9
Directed graphs & Partial Orders
306
Let R be a binary relation on a set A and C n be the composition of R with itself n
times for n  0. So C 0 WWD IdA, and C nC1 WWD R ƒ± C n. Regarding R as a digraph,
let Rn denote the length-n walk relation in the digraph R, that is,
a Rn b WWD there is a length-n walk from a to b in R:
Prove that
Rn D C n
(9.11)
for all n 2 N.
Problem 9.12.
If R is a binary relation on a set, A, then Rk denotes the relational composition of
R with itself k times.
(a) Prove that if R is a relation on a Ô¨Ånite set, A, then
a .R [ IA/n b
iff
there is a path in R of length length  n from a to b:
(b) Conclude that if A is a Ô¨Ånite set, then
R D .R [ IA/jAj 1:
(9.12)
Problem 9.13.
Prove that the shortest odd-length closed walk through a vertex is an odd-length
cycle.
Problem 9.14.
An Euler tour12 of a graph is a closed walk that includes every edge exactly once.
Such walks are named after the famous 17th century mathematician Leonhard Eu-
ler. (Same Euler as for the constant e  2:718 and the totient function  ‚Äîhe did
a lot of stuff.)
So how do you tell in general whether a graph has an Euler tour? At Ô¨Årst glance
this may seem like a daunting problem (the similar sounding problem of Ô¨Ånding
a cycle that touches every vertex exactly once is one of those million dollar NP-
complete problems known as the Traveling Salesman Problem) ‚Äîbut it turns out
to be easy.
12In some other texts, this is called an Euler circuit.

9.11. Summary of Relational Properties
307
(a) Show that if a graph has an Euler tour, then the in-degree of each vertex equals
its out-degree.
A digraph is weakly connected if there is a ‚Äúpath‚Äù between any two vertices that
may follow edges backwards or forwards.13 In the remaining parts, we‚Äôll work out
the converse: if a graph is weakly connected, and if the in-degree of every vertex
equals its out-degree, then the graph has an Euler tour.
A trail is a walk in which each edge occurs at most once.
(b) Suppose that a trail in a weakly connected graph does not include every edge.
Explain why there must be an edge not on the trail that starts or ends at a vertex on
the trail.
In the remaining parts, let w be the longest trail in the graph.
(c) Show that if w is closed, then it must be an Euler tour.
Hint: part (b)
(d) Explain why all the edges starting at the end of w must be on w.
(e) Show that if w was not closed, then the in-degree of the end would be bigger
than its out-degree.
Hint: part (d)
(f) Conclude that if in a Ô¨Ånite, weakly connected digraph, the in-degree of every
vertex equals its out-degree, then the digraph has an Euler tour.
Problem 9.15.
A 3-bit string is a string made up of 3 characters, each a 0 or a 1. Suppose you‚Äôd
like to write out, in one string, all eight of the 3-bit strings in any convenient order.
For example, if you wrote out the 3-bit strings in the usual order starting with 000
001 010. . . , you could concatenate them together to get a length 3  8 D 24 string
that started 000001010....
But you can get a shorter string containing all eight 3-bit strings by starting with
00010. . . . Now 000 is present as bits 1 through 3, and 001 is present as bits 2
through 4, and 010 is present as bits 3 through 5, ....
13More precisely, a graph G is weakly connected iff there is a path from any vertex to any other
vertex in the graph H with
V.H/ D V.G/; and
E.H/ D E.G/ [ fhv !ui j hu!vi 2 E.G/g:
In other words H D G [ G 1.

Chapter 9
Directed graphs & Partial Orders
308
(a) Say a string is 3-good if it contains every 3-bit string as 3 consecutive bits
somewhere in it. Find a 3-good string of length 10, and explain why this is the
minimum length for any string that is 3-good.
(b) Explain how any walk that includes every edge in the graph shown in Fig-
ure 9.13 determines a string that is 3-good. Find the walk in this graph that deter-
mines your 3-good string from part (a).
(c) Explain why a walk in the graph of Figure 9.13 that includes every every edge
exactly once provides a minimum length 3-good string.
(d) The situation above generalizes to k  2. Namely, there is a digraph, Bk, such
that V.Bk/ WWD f0; 1gk, and any walk through Bk that contains every edge exactly
once determines a minimum length .k C 1/-good bit-string. What is this minimum
length?
DeÔ¨Åne the transitions of Bk. Verify that the in-degree and out-degree of every
vertex is even, and that there is a positive path from any vertex to any other vertex
(including itself) of length at most k.14
Exam Problems
Problem 9.16.
Indicate which of the following relations below are equivalence relations, (E), strict
partial orders (S), weak partial orders (W). For the partial orders, also indicate
whether it is path-total (T).
If a relation is none of the above, indicate whether it is transitive (Tr),
sym-
metric (Sym),
asymmetric (Asym).
(a) The relation a D b C 1 between integers, a, b,
(b) The superset relation,  on the power set of the integers.
(c) The empty relation on the set of rationals.
(d) The divides relation on the nonegatitve integers.
(e) The divides relation on the integers.
14Problem 9.14 shows that if the in-degree of every vertex of a digraph is equal to its out-degree,
and there are paths between any two vertices, then there is a closed walk that includes every edge
exactly once. So the graph Bk implies that there always is a length-2kC1 C k bit-string in which
every length-.k C1/ bit-string appears as a substring. Such strings are known as de Bruijn sequences
having been studied by the great Dutch mathematician/logician Nicolaas de Bruijn, who died in
February, 2012 at the age of 94.

9.11. Summary of Relational Properties
309
+0
+1
+0
+0
+1
+1
+1
00
11
10
01
+0
Figure 9.13
The 2-bit graph.
(f) The divides relation on the positive powers of 4.
(g) The relatively prime relation on the nonnegative integers.
The less-than, <, relation on real-valued functions, f .x/, of the form f .x/ D
ax C b for constants a; b 2 reals.
The relation ‚Äúhas the same prime factors‚Äù on the integers.
Problems for Section 9.6
Class Problems
Problem 9.17.

Chapter 9
Directed graphs & Partial Orders
310
Direct Prerequisites
Subject
18.01
6.042
18.01
18.02
18.01
18.03
8.01
8.02
8.01
6.01
6.042
6.046
18.02, 18.03, 8.02, 6.01
6.02
6.01, 6.042
6.006
6.01
6.034
6.02
6.004
(a) For the above table of MIT subject prerequisites, draw a diagram showing the
subject numbers with a line going down to every subject from each of its (direct)
prerequisites.
(b) Give an example of a collection of sets partially ordered by the proper subset
relation, , that is isomorphic to (‚Äúsame shape as‚Äù) the prerequisite relation among
MIT subjects from part (a).
(c) Explain why the empty relation is a strict partial order and describe a collection
of sets partially ordered by the proper subset relation that is isomorphic to the empty
relation on Ô¨Åve elements‚Äîthat is, the relation under which none of the Ô¨Åve elements
is related to anything.
(d) Describe a simple collection of sets partially ordered by the proper subset re-
lation that is isomorphic to the ‚Äùproperly contains‚Äù relation, , on pow f1; 2; 3; 4g.
Problem 9.18.
The proper subset relation, , deÔ¨Ånes a strict partial order on the subsets of ≈í1; 6¬ç,
that is pow ≈í1; 6¬ç.
(a) What is the size of a maximal chain in this partial order? Describe one.
(b) Describe the largest antichain you can Ô¨Ånd in this partial order.
(c) What are the maximal and minimal elements? Are they maximum and mini-
mum?
(d) Answer the previous part for the  partial order on the set pow f1; 2; : : : ; 6g  ;.

9.11. Summary of Relational Properties
311
Problem 9.19.
This problem asks for a proof of Lemma 9.6.2 showing that every weak partial
order can be represented by (is isomorphic to) a collection of sets partially ordered
under set inclusion (). Namely,
Lemma. Let  be a weak partial order on a set, A. For any element a 2 A, let
L.a/ WWD fb 2 A j b  ag;
L WWD fL.a/ j a 2 Ag:
Then the function L./ W A ! L is an isomorphism from the  relation on A, to the
subset relation on L.
(a) Prove that the function L./ W A ! L is a bijection.
(b) Complete the proof by showing that
a  b
iff
L.a/  L.b/
(9.13)
for all a; b 2 A.
Homework Problems
Problem 9.20.
Every partial order is isomorphic to a collection of sets under the subset relation
(see Section 9.6). In particular, if R is a strict partial order on a set, A, and a 2 A,
deÔ¨Åne
L.a/ WWD fag [ fx 2 A j x R ag:
(9.14)
Then
a R b
iff
L.a/  L.b/
(9.15)
holds for all a; b 2 A.
(a) Carefully prove statement (9.15), starting from the deÔ¨Ånitions of strict partial
order and the strict subset relation, .
(b) Prove that if L.a/ D L.b/ then a D b.
(c) Give an example showing that the conclusion of part (b) would not hold if the
deÔ¨Ånition of L.a/ in equation (9.14) had omitted the expression ‚Äúfag[.‚Äù

Chapter 9
Directed graphs & Partial Orders
312
Problems for Section 9.7
Practice Problems
Problem 9.21.
For each of the binary relations below, state whether it is a strict partial order, a
weak partial order, or neither. If it is not a partial order, indicate which of the
axioms for partial order it violates.
(a) The superset relation,  on the power set pow f1; 2; 3; 4; 5g.
(b) The relation between any two nonnegative integers, a, b that a  b .mod 8/.
(c) The relation between propositional formulas, G, H, that G IMPLIES H is
valid.
(d) The relation ‚Äôbeats‚Äô on Rock, Paper and Scissor (for those who don‚Äôt know the
game Rock, Paper, Scissors, Rock beats Scissors, Scissors beats Paper and Paper
beats Rock).
(e) The empty relation on the set of real numbers.
(f) The identity relation on the set of integers.
Problem 9.22. (a) Verify that the divisibility relation on the set of nonnegative
integers is a weak partial order.
(b) What about the divisibility relation on the set of integers?
Problem 9.23.
Prove directly from the deÔ¨Ånitions (without appealing to DAG properties) that if a
binary relation R on a set A is transitive and irreÔ¨Çexive, then it is asymmetric.
Class Problems
Problem 9.24.
Show that the set of nonnegative integers partially ordered under the divides rela-
tion. . .
(a) . . . has a minimum element.
(b) . . . has a maximum element.

9.11. Summary of Relational Properties
313
(c) . . . has an inÔ¨Ånite chain.
(d) . . . has an inÔ¨Ånite antichain.
(e) What are the minimal elements of divisibility on the integers greater than 1?
What are the maximal elements?
Problem 9.25.
How many binary relations are there on the set f0; 1g?
How many are there that are transitive?, ...asymmetric?, ...reÔ¨Çexive?, ...irreÔ¨Çexive?,
. . . strict partial orders?, ...weak partial orders?
Hint: There are easier ways to Ô¨Ånd these numbers than listing all the relations
and checking which properties each one has.
Problem 9.26.
Prove that if R is a partial order, then so is R 1
Homework Problems
Problem 9.27.
Let R and S be transitive binary relations on the same set, A. Which of the follow-
ing new relations must also be transitive? For each part, justify your answer with a
brief argument if the new relation is transitive and a counterexample if it is not.
(a) R 1
(b) R \ S
(c) R ƒ± R
(d) R ƒ± S
Exam Problems
Problem 9.28.
(a) For each row in the following table, indicate whether the binary relation, R, on
the set, A, is a weak partial order or a path-total order by Ô¨Ålling in the appropriate
entries with either Y = YES or N = NO. In addition, list the minimal and maximal
elements for each relation.

Chapter 9
Directed graphs & Partial Orders
314
A
a R b
weak p. o.
path-total order
minimal(s)
maximal(s)
R   RC
a j b
pow.f1; 2; 3g/
a  b
N [ fig
a > b
(b) What is the longest chain on the subset relation, , on pow.f1; 2; 3g/? (If
there is more than one, provide one of them.)
(c) What is the longest antichain on the subset relation, , on pow.f1; 2; 3g/? (If
there is more than one, provide one of them.)
Problems for Section 9.8
Class Problems
Problem 9.29.
Let R1, R2 be binary relations on the same set, A. A relational property is preserved
under product, if R1  R2 has the property whenever both R1 and R2 have the
property.
(a) Verify that each of the following properties are preserved under product.
1. reÔ¨Çexivity,
2. antisymmetry,
3. transitivity.
(b) Verify that if either of R1 or R2 is irreÔ¨Çexive, then so is R1  R2.
Note that it now follows immediately that if if R1 and R2 are partial orders and
at least one of them is strict, then R1  R2 is a strict partial order.
Problems for Section 9.9
Practice Problems
Problem 9.30.
What is the size of the longest chain that is guaranteed to exist in any partially
ordered set of n elements? What about the largest antichain?

9.11. Summary of Relational Properties
315
Problem 9.31.
Describe a sequence consisting of the integers from 1 to 10,000 in some order so
that there is no increasing or decreasing subsequence of size 101.
Problem 9.32.
What is the smallest number of partially ordered tasks for which there can be more
than one minimum time schedule, if there are unlimited number of processors?
Explain your answer.
Class Problems
Problem 9.33.
The table below lists some prerequisite information for some subjects in the MIT
Computer Science program (in 2006). This deÔ¨Ånes an indirect prerequisite relation
that is a DAG with these subjects as vertices.
18:01 ! 6:042
18:01 ! 18:02
18:01 ! 18:03
6:046 ! 6:840
8:01 ! 8:02
6:001 ! 6:034
6:042 ! 6:046
18:03; 8:02 ! 6:002
6:001; 6:002 ! 6:003
6:001; 6:002 ! 6:004
6:004 ! 6:033
6:033 ! 6:857
(a) Explain why exactly six terms are required to Ô¨Ånish all these subjects, if you
can take as many subjects as you want per term. Using a greedy subject selection
strategy, you should take as many subjects as possible each term. Exhibit your
complete class schedule each term using a greedy strategy.
(b) In the second term of the greedy schedule, you took Ô¨Åve subjects including
18.03. Identify a set of Ô¨Åve subjects not including 18.03 such that it would be
possible to take them in any one term (using some nongreedy schedule). Can you
Ô¨Ågure out how many such sets there are?
(c) Exhibit a schedule for taking all the courses‚Äîbut only one per term.
(d) Suppose that you want to take all of the subjects, but can handle only two per
term. Exactly how many terms are required to graduate? Explain why.
(e) What if you could take three subjects per term?

Chapter 9
Directed graphs & Partial Orders
316
Problem 9.34.
A pair of Math for Computer Science Teaching Assistants, Oshani and Oscar, have
decided to devote some of their spare time this term to establishing dominion over
the entire galaxy. Recognizing this as an ambitious project, they worked out the
following table of tasks on the back of Oscar‚Äôs copy of the lecture notes.
1. Devise a logo and cool imperial theme music - 8 days.
2. Build a Ô¨Çeet of Hyperwarp Stardestroyers out of eating paraphernalia swiped
from Lobdell - 18 days.
3. Seize control of the United Nations - 9 days, after task #1.
4. Get shots for Oshani‚Äôs cat, Tailspin - 11 days, after task #1.
5. Open a Starbucks chain for the army to get their caffeine - 10 days, after
task #3.
6. Train an army of elite interstellar warriors by dragging people to see The
Phantom Menace dozens of times - 4 days, after tasks #3, #4, and #5.
7. Launch the Ô¨Çeet of Stardestroyers, crush all sentient alien species, and es-
tablish a Galactic Empire - 6 days, after tasks #2 and #6.
8. Defeat Microsoft - 8 days, after tasks #2 and #6.
We picture this information in Figure 9.14 below by drawing a point for each
task, and labelling it with the name and weight of the task. An edge between
two points indicates that the task for the higher point must be completed before
beginning the task for the lower one.
(a) Give some valid order in which the tasks might be completed.
Oshani and Oscar want to complete all these tasks in the shortest possible time.
However, they have agreed on some constraining work rules.
 Only one person can be assigned to a particular task; they cannot work to-
gether on a single task.
 Once a person is assigned to a task, that person must work exclusively on the
assignment until it is completed. So, for example, Oshani cannot work on
building a Ô¨Çeet for a few days, run to get shots for Tailspin, and then return
to building the Ô¨Çeet.

9.11. Summary of Relational Properties
317
v
v
v
v
v
v
v
v






















QQQQQQQQ
A
A
A
A
A
AA
PPPPPPPPPPPPPPPP
QQQQQQQQQ























E
E
E
E
E
E
E
E
E
E
E
E
E
E
E
E
E
E
E
E
E
E
E
B
B
B
B
B
B
B
B
B
B
B
6
seize control
open chain
10
train army
devise logo
build Ô¨Çeet
launch Ô¨Çeet
8
11
4
get shots
8
18
defeat Microsoft
9
Figure 9.14
Graph representing the task precedence constraints.

Chapter 9
Directed graphs & Partial Orders
318
(b) Oshani and Oscar want to know how long conquering the galaxy will take.
Oscar suggests dividing the total number of days of work by the number of workers,
which is two. What lower bound on the time to conquer the galaxy does this give,
and why might the actual time required be greater?
(c) Oshani proposes a different method for determining the duration of their project.
She suggests looking at the duration of the ‚Äúcritical path‚Äù, the most time-consuming
sequence of tasks such that each depends on the one before. What lower bound does
this give, and why might it also be too low?
(d) What is the minimum number of days that Oshani and Oscar need to conquer
the galaxy? No proof is required.
Problem 9.35. (a) What are the maximal and minimal elements, if any, of the
power set pow.f1; : : : ; ng/, where n is a positive integer, under the empty relation?
(b) What are the maximal and minimal elements, if any, of the set, N, of all non-
negative integers under divisibility? Is there a minimum or maximum element?
(c) What are the minimal and maximal elements, if any, of the set of integers
greater than 1 under divisibility?
(d) Describe a partially ordered set that has no minimal or maximal elements.
(e) Describe a partially ordered set that has a unique minimal element, but no
minimum element. Hint: It will have to be inÔ¨Ånite.
Homework Problems
Problem 9.36.
The following procedure can be applied to any digraph, G:
1. Delete an edge that is in a cycle.
2. Delete edge hu!vi if there is a path from vertex u to vertex v that does not
include hu!vi.
3. Add edge hu!vi if there is no path in either direction between vertex u and
vertex v.
Repeat these operations until none of them are applicable.
This procedure can be modeled as a state machine. The start state is G, and the
states are all possible digraphs with the same vertices as G.

9.11. Summary of Relational Properties
319
(a) Let G be the graph with vertices f1; 2; 3; 4g and edges
fh1!2i ; h2!3i ; h3!4i ; h3!2i ; h1!4ig
What are the possible Ô¨Ånal states reachable from G?
A line graph is a graph whose edges are all on one path. All the Ô¨Ånal graphs in
part (a) are line graphs.
(b) Prove that if the procedure terminates with a digraph, H, then H is a line
graph with the same vertices as G.
Hint: Show that if H is not a line graph, then some operation must be applicable.
(c) Prove that being a DAG is a preserved invariant of the procedure.
(d) Prove that if G is a DAG and the procedure terminates, then the walk relation
of the Ô¨Ånal line graph is a topological sort of G.
Hint: Verify that the predicate
P.u; v/ WWD there is a directed path from u to v
is a preserved invariant of the procedure, for any two vertices u; v of a DAG.
(e) Prove that if G is Ô¨Ånite, then the procedure terminates.
Hint: Let s be the number of cycles, e be the number of edges, and p be the number
of pairs of vertices with a directed path (in either direction) between them. Note
that p  n2 where n is the number of vertices of G. Find coefÔ¨Åcients a; b; c such
that as Cbp Ce Cc is nonnegative integer valued and decreases at each transition.
Problem 9.37.
Let  be a partial order on a set, A, and let
Ak WWD fa j depth .a/ D kg
where k 2 N.
(a) Prove that A0; A1; : : : is a parallel schedule for  according to DeÔ¨Ånition 9.9.6.
(b) Prove that Ak is an antichain.

Chapter 9
Directed graphs & Partial Orders
320
Problem 9.38.
Let S be a sequence of n different numbers. A subsequence of S is a sequence that
can be obtained by deleting elements of S.
For example, if
S D .6; 4; 7; 9; 1; 2; 5; 3; 8/
Then 647 and 7253 are both subsequences of S (for readability, we have dropped
the parentheses and commas in sequences, so 647 abbreviates .6; 4; 7/, for exam-
ple).
An increasing subsequence of S is a subsequence of whose successive elements
get larger. For example, 1238 is an increasing subsequence of S. Decreasing sub-
sequences are deÔ¨Åned similarly; 641 is a decreasing subsequence of S.
(a) List all the maximum length increasing subsequences of S, and all the maxi-
mum length decreasing subsequences.
Now let A be the set of numbers in S. (So A D f1; 2; 3; : : : ; 9g for the example
above.) There are two straightforward ways to path-total order A. The Ô¨Årst is to
order its elements numerically, that is, to order A with the < relation. The second
is to order the elements by which comes Ô¨Årst in S; call this order <S. So for the
example above, we would have
6 <S 4 <S 7 <S 9 <S 1 <S 2 <S 5 <S 3 <S 8
Next, deÔ¨Åne the partial order  on A deÔ¨Åned by the rule
a  a0
WWD
a < a0 and a <S a0:
(It‚Äôs not hard to prove that  is strict partial order, but you may assume it.)
(b) Draw a diagram of the partial order, , on A. What are the maximal ele-
ments,. . . the minimal elements?
(c) Explain the connection between increasing and decreasing subsequences of S,
and chains and anti-chains under .
(d) Prove that every sequence, S, of length n has an increasing subsequence of
length greater than pn or a decreasing subsequence of length at least pn.
(e) (Optional, tricky) Devise an efÔ¨Åcient procedure for Ô¨Ånding the longest in-
creasing and the longest decreasing subsequence in any given sequence of integers.
(There is a nice one.)

9.11. Summary of Relational Properties
321
Problem 9.39.
We want to schedule n tasks with prerequisite constraints among the tasks deÔ¨Åned
by a DAG.
(a) Explain why any schedule that requires only p processors must take time at
least dn=pe.
(b) Let Dn;t be the DAG with n elements that consists of a chain of t  1 elements,
with the bottom element in the chain being a prerequisite of all the remaining ele-
ments as in the following Ô¨Ågure:
. . .
. . .
t - 1
n - (t - 1)
What is the minimum time schedule for Dn;t? Explain why it is unique. How many
processors does it require?
(c) Write a simple formula, M.n; t; p/, for the minimum time of a p-processor
schedule to complete Dn;t.
(d) Show that every partial order with n vertices and maximum chain size, t, has
a p-processor schedule that runs in time M.n; t; p/.
Hint: Induction on t.
Problems for Section 9.10
Practice Problems
Problem 9.40.
For each of the following relations, decide whether it is reÔ¨Çexive, whether it is
symmetric, whether it is transitive, and whether it is an equivalence relation.

Chapter 9
Directed graphs & Partial Orders
322
(a) f.a; b/ j a and b are the same ageg
(b) f.a; b/ j a and b have the same parentsg
(c) f.a; b/ j a and b speak a common languageg
Problem 9.41.
For each of the binary relations below, state whether it is a strict partial order, a
weak partial order, an equivalence relation or none of these. If it is a partial order,
state whether it is a path-total order. If it is none, indicate which of the axioms for
partial order and equivalence relations it violates.
(a) The superset relation,  on the power set pow f1; 2; 3; 4; 5g.
(b) The relation between any two nonnegative integers, a, b that a  b .mod 8/.
(c) The relation between propositional formulas, G, H, that ≈íG IMPLIES H¬ç is
valid.
(d) The relation between propositional formulas, G, H, that ≈íG IFF H¬ç is valid.
(e) The relation ‚Äôbeats‚Äô on Rock, Paper and Scissor (for those who don‚Äôt know the
game Rock, Paper, Scissors, Rock beats Scissors, Scissors beats Paper and Paper
beats Rock).
(f) The empty relation on the set of real numbers.
(g) The identity relation on the set of integers.
(h) The divisibility relation on the integers, Z.
Class Problems
Problem 9.42.
Prove Theorem 9.10.4: The equivalence classes of an equivalence relation form a
partition of the domain.
Namely, let R be an equivalence relation on a set, A, and deÔ¨Åne the equivalence
class of an element a 2 A to be
≈ía¬çR WWD fb 2 A j a R bg:
That is, ≈ía¬çR D R.a/.
(a) Prove that every block is nonempty and every element of A is in some block.

9.11. Summary of Relational Properties
323
(b) Prove that if ≈ía¬çR \ ≈íb¬çR ¬§ ;, then a R b. Conclude that the sets ≈ía¬çR for
a 2 A are a partition of A.
(c) Prove that a R b iff ≈ía¬çR D ≈íb¬çR.
Problem 9.43.
For any total function f W A ! B deÔ¨Åne a relation f by the rule:
a f a0
iff
f .a/ D f .a0/:
(9.16)
(a) Observe that f is an equivalence relation on A.
(b) Prove that every equivalence relation, R, on a set, A, is equal to f for the
function f W A ! pow.A/ deÔ¨Åned as
f .a/ WWD fa0 2 A j a R a0g:
That is, f .a/ D R.a/.
Homework Problems
Problem 9.44.
Let R1 and R2 be two equivalence relations on a set, A. Which of the following
relations must also be equivalence relations? Prove it.
(a) R1 \ R2.
(b) R1 [ R2 .


10
Communication Networks
Modeling communication networks is an important application of digraphs in com-
puter science. In this such models, vertices represent computers, processors, and
switches; edges will represent wires, Ô¨Åber, or other transmission lines through
which data Ô¨Çows. For some communication networks, like the internet, the cor-
responding graph is enormous and largely chaotic. Highly structured networks, by
contrast, Ô¨Ånd application in telephone switching systems and the communication
hardware inside parallel computers. In this chapter, we‚Äôll look at some of the nicest
and most commonly used structured networks.
10.1
Complete Binary Tree
Let‚Äôs start with a complete binary tree. Here is an example with 4 inputs and 4
outputs. The kinds of communication networks we consider aim to transmit packets
of data between computers, processors, telephones, or other devices. The term
packet refers to some roughly Ô¨Åxed-size quantity of data‚Äî 256 bytes or 4096 bytes
or whatever. In this diagram and many that follow, the squares represent terminals,
sources and destinations for packets of data. The circles represent switches, which
direct packets through the network. A switch receives packets on incoming edges
and relays them forward along the outgoing edges. Thus, you can imagine a data
packet hopping through the network from an input terminal, through a sequence of
switches joined by directed edges, to an output terminal.
Recall that there is a unique path between every pair of vertices in a tree. So
the natural way to route a packet of data from an input terminal to an output in the
complete binary tree is along the corresponding directed path. For example, the
route of a packet traveling from input 1 to output 3 is shown in bold.
10.2
Routing Problems
Communication networks are supposed to get packets from inputs to outputs, with
each packet entering the network at its own input switch and arriving at its own
output switch. We‚Äôre going to consider several different communication network
designs, where each network has N inputs and N outputs; for convenience, we‚Äôll

Chapter 10
Communication Networks
326
IN
OUT
IN
IN
IN
OUT
OUT
OUT
0
0
1
1
2
2
3
3
assume N is a power of two.
Which input is supposed to go where is speciÔ¨Åed by a permutation of f0; 1; : : : ; N 1g. So a permutation, , deÔ¨Ånes a routing problem: get a packet that starts at in-
put i to output .i/. A routing, P , that solves a routing problem, , is a set of
paths from each input to its speciÔ¨Åed output. That is, P is a set of n paths, Pi, for
i D 0 : : : ; N   1, where Pi goes from input i to output .i/.
10.3
Network Diameter
The delay between the time that a packets arrives at an input and arrives at its
designated output is a critical issue in communication networks. Generally this
delay is proportional to the length of the path a packet follows. Assuming it takes
one time unit to travel across a wire, the delay of a packet will be the number of
wires it crosses going from input to output.
Generally packets are routed to go from input to output by the shortest path pos-
sible. With a shortest path routing, the worst case delay is the distance between the
input and output that are farthest apart. This is called the diameter of the network.
In other words, the diameter of a network1 is the maximum length of any shortest
1The usual deÔ¨Ånition of diameter for a general graph (simple or directed) is the largest distance
between any two vertices, but in the context of a communication network we‚Äôre only interested in the
distance between inputs and outputs, not between arbitrary pairs of vertices.

10.4. Switch Count
327
path between an input and an output. For example, in the complete binary tree
above, the distance from input 1 to output 3 is six. No input and output are farther
apart than this, so the diameter of this tree is also six.
More generally, the diameter of a complete binary tree with N inputs and outputs
is 2 log N C2. (All logarithms in this lecture‚Äî and in most of computer science ‚Äî
are base 2.) This is quite good, because the logarithm function grows very slowly.
We could connect up 210 D 1024 inputs and outputs using a complete binary tree
and the worst input-output delay for any packet would be this diameter, namely,
2 log.210/ C 2 D 22.
10.3.1
Switch Size
One way to reduce the diameter of a network is to use larger switches. For example,
in the complete binary tree, most of the switches have three incoming edges and
three outgoing edges, which makes them 3  3 switches. If we had 4  4 switches,
then we could construct a complete ternary tree with an even smaller diameter. In
principle, we could even connect up all the inputs and outputs via a single monster
N  N switch.
This isn‚Äôt very productive, however, since we‚Äôve just concealed the original net-
work design problem inside this abstract switch. Eventually, we‚Äôll have to design
the internals of the monster switch using simpler components, and then we‚Äôre right
back where we started. So the challenge in designing a communication network
is Ô¨Åguring out how to get the functionality of an N  N switch using Ô¨Åxed size,
elementary devices, like 3  3 switches.
10.4
Switch Count
Another goal in designing a communication network is to use as few switches as
possible. The number of switches in a complete binary tree is 1C2C4C8C  CN ,
since there is 1 switch at the top (the ‚Äúroot switch‚Äù), 2 below it, 4 below those, and
so forth. By the formula for geometric sums from Problem 5.3,
n
X
iD0
ri D rnC1   1
r   1
;
the total number of switches is 2N   1, which is nearly the best possible with 3  3
switches.

Chapter 10
Communication Networks
328
10.5
Network Latency
We‚Äôll sometimes be choosing routings through a network that optimize some quan-
tity besides delay. For example, in the next section we‚Äôll be trying to minimize
packet congestion. When we‚Äôre not minimizing delay, shortest routings are not al-
ways the best, and in general, the delay of a packet will depend on how it is routed.
For any routing, the most delayed packet will be the one that follows the longest
path in the routing. The length of the longest path in a routing is called its latency.
The latency of a network depends on what‚Äôs being optimized. It is measured by
assuming that optimal routings are always chosen in getting inputs to their speciÔ¨Åed
outputs. That is, for each routing problem, , we choose an optimal routing that
solves . Then network latency is deÔ¨Åned to be the largest routing latency among
these optimal routings. Network latency will equal network diameter if routings
are always chosen to optimize delay, but it may be signiÔ¨Åcantly larger if routings
are chosen to optimize something else.
For the networks we consider below, paths from input to output are uniquely
determined (in the case of the tree) or all paths are the same length, so network
latency will always equal network diameter.
10.6
Congestion
The complete binary tree has a fatal drawback: the root switch is a bottleneck. At
best, this switch must handle an enormous amount of trafÔ¨Åc: every packet traveling
from the left side of the network to the right or vice-versa. Passing all these packets
through a single switch could take a long time. At worst, if this switch fails, the
network is broken into two equal-sized pieces.
For example, if the routing problem is given by the identity permutation, Id.i/WWD
i, then there is an easy routing, P , that solves the problem: let Pi be the path from
input i up through one switch and back down to output i. On the other hand, if the
problem was given by .i/ WWD .N   1/   i, then in any solution, Q, for , each
path Qi beginning at input i must eventually loop all the way up through the root
switch and then travel back down to output .N   1/   i. These two situations are
illustrated below. We can distinguish between a ‚Äúgood‚Äù set of paths and a ‚Äúbad‚Äù set
based on congestion. The congestion of a routing, P , is equal to the largest number
of paths in P that pass through a single switch. For example, the congestion of the
routing on the left is 1, since at most 1 path passes through each switch. However,

10.7. 2-D Array
329
IN
OUT
IN
IN
IN
OUT
OUT
OUT
0
0
1
1
2
2
3
3
IN
OUT
IN
IN
IN
OUT
OUT
OUT
0
0
1
1
2
2
3
3
the congestion of the routing on the right is 4, since 4 paths pass through the root
switch (and the two switches directly below the root). Generally, lower congestion
is better since packets can be delayed at an overloaded switch.
By extending the notion of congestion to networks, we can also distinguish be-
tween ‚Äúgood‚Äù and ‚Äúbad‚Äù networks with respect to bottleneck problems. For each
routing problem, , for the network, we assume a routing is chosen that optimizes
congestion, that is, that has the minimum congestion among all routings that solve
. Then the largest congestion that will ever be suffered by a switch will be the
maximum congestion among these optimal routings. This ‚Äúmaximin‚Äù congestion
is called the congestion of the network.
So for the complete binary tree, the worst permutation would be .i/ WWD .N  1/   i. Then in every possible solution for , every packet, would have to follow
a path passing through the root switch. Thus, the max congestion of the complete
binary tree is N ‚Äîwhich is horrible!
Let‚Äôs tally the results of our analysis so far:
network
diameter
switch size
# switches
congestion
complete binary tree
2 log N C 2
3  3
2N   1
N
10.7
2-D Array
Let‚Äôs look at an another communication network. This one is called a 2-dimensional
array or grid.
Here there are four inputs and four outputs, so N D 4.
The diameter in this example is 8, which is the number of edges between input 0
and output 3. More generally, the diameter of an array with N inputs and outputs is
2N, which is much worse than the diameter of 2 log N C 2 in the complete binary
tree. On the other hand, replacing a complete binary tree with an array almost
eliminates congestion.
Theorem 10.7.1. The congestion of an N-input array is 2.

Chapter 10
Communication Networks
330
in0
in1
in2
in3
out3
out2
out1
out0

10.8. ButterÔ¨Çy
331
Proof. First, we show that the congestion is at most 2. Let  be any permutation.
DeÔ¨Åne a solution, P , for  to be the set of paths, Pi, where Pi goes to the right
from input i to column .i/ and then goes down to output .i/. Thus, the switch in
row i and column j transmits at most two packets: the packet originating at input
i and the packet destined for output j.
Next, we show that the congestion is at least 2. This follows because in any
routing problem, , where .0/ D 0 and .N   1/ D N   1, two packets must
pass through the lower left switch.

As with the tree, the network latency when minimizing congestion is the same
as the diameter. That‚Äôs because all the paths between a given input and output are
the same length.
Now we can record the characteristics of the 2-D array.
network
diameter
switch size
# switches
congestion
complete binary tree
2 log N C 2
3  3
2N   1
N
2-D array
2N
2  2
N 2
2
The crucial entry here is the number of switches, which is N 2. This is a major
defect of the 2-D array; a network of size N D 1000 would require a million
2  2 switches! Still, for applications where N is small, the simplicity and low
congestion of the array make it an attractive choice.
10.8
ButterÔ¨Çy
The Holy Grail of switching networks would combine the best properties of the
complete binary tree (low diameter, few switches) and of the array (low conges-
tion). The butterÔ¨Çy is a widely-used compromise between the two.
A good way to understand butterÔ¨Çy networks is as a recursive data type. The
recursive deÔ¨Ånition works better if we deÔ¨Åne just the switches and their connec-
tions, omitting the terminals. So we recursively deÔ¨Åne Fn to be the switches and
connections of the butterÔ¨Çy net with N WWD 2n input and output switches.
The base case is F1 with 2 input switches and 2 output switches connected as in
Figure 10.1.
In the constructor step, we construct FnC1 with 2nC1 inputs and outputs out
of two Fn nets connected to a new set of 2nC1 input switches, as shown in as in
Figure 10.2. That is, the ith and 2n C ith new input switches are each connected
to the same two switches, namely, to the ith input switches of each of two Fn

Chapter 10
Communication Networks
332
2 outputs
2 inputs
√Ä
√Ä
ND21
Figure 10.1
F1, the ButterÔ¨Çy Net switches with N D 21.
components for i D 1; : : : ; 2n. The output switches of FnC1 are simply the output
switches of each of the Fn copies.
So FnC1 is laid out in columns of height 2nC1 by adding one more column of
switches to the columns in Fn. Since the construction starts with two columns
when n D 1, the FnC1 switches are arrayed in n C 1 columns. The total number
of switches is the height of the columns times the number of columns, namely,
2nC1.n C 1/. Remembering that n D log N , we conclude that the ButterÔ¨Çy Net
with N inputs has N.log N C 1/ switches.
Since every path in FnC1 from an input switch to an output is the same length,
namely, n C 1, the diameter of the ButterÔ¨Çy net with 2nC1 inputs is this length
plus two because of the two edges connecting to the terminals (square boxes) ‚Äî
one edge from input terminal to input switch (circle) and one from output switch to
output terminal.
There is an easy recursive procedure to route a packet through the ButterÔ¨Çy Net.
In the base case, there is obviously only one way to route a packet from one of the
two inputs to one of the two outputs. Now suppose we want to route a packet from
an input switch to an output switch in FnC1. If the output switch is in the ‚Äútop‚Äù
copy of Fn, then the Ô¨Årst step in the route must be from the input switch to the
unique switch it is connected to in the top copy; the rest of the route is determined
by recursively routing the rest of the way in the top copy of Fn. Likewise, if the
output switch is in the ‚Äúbottom‚Äù copy of Fn, then the Ô¨Årst step in the route must
be to the switch in the bottom copy, and the rest of the route is determined by
recursively routing in the bottom copy of Fn. In fact, this argument shows that the
routing is unique: there is exactly one path in the ButterÔ¨Çy Net from each input to
each output, which implies that the network latency when minimizing congestion
is the same as the diameter.
The congestion of the butterÔ¨Çy network is about
p
N , more precisely, the con-

10.9. BeneÀás Network
333
‚éß
‚é®
F
‚é®
‚é©
2n
Fn
2n 1
t
t
‚é©
‚éß
‚é®
F
2n+1  outputs
‚é®
‚é©
2n
Fn
F
new inputs
‚é©
Fn+1
Figure 10.2
FnC1, the ButterÔ¨Çy Net switches with 2nC1 inputs and outputs.
gestion is
p
N if N is an even power of 2 and
p
N=2 if N is an odd power of 2. A
simple proof of this appears in Problem10.8.
Let‚Äôs add the butterÔ¨Çy data to our comparison table:
network
diameter
switch size
# switches
congestion
complete binary tree
2 log N C 2
3  3
2N   1
N
2-D array
2N
2  2
N 2
2
butterÔ¨Çy
log N C 2
2  2
N.log.N/ C 1/
p
N or
p
N=2
The butterÔ¨Çy has lower congestion than the complete binary tree. And it uses fewer
switches and has lower diameter than the array. However, the butterÔ¨Çy does not
capture the best qualities of each network, but rather is a compromise somewhere
between the two. So our quest for the Holy Grail of routing networks goes on.
10.9
BeneÀás Network
In the 1960‚Äôs, a researcher at Bell Labs named BeneÀás had a remarkable idea. He
obtained a marvelous communication network with congestion 1 by placing two
butterÔ¨Çies back-to-back. This amounts to recursively growing BeneÀás nets by adding

Chapter 10
Communication Networks
334
Bn
Bn
BnC1
2n
2n
2nC1
√Ä
√Ä
new inputs
new outputs
Figure 10.3
BnC1, the BeneÀás Net switches with 2nC1 inputs and outputs.
both inputs and outputs at each stage. Now we recursively deÔ¨Åne Bn to be the
switches and connections (without the terminals) of the BeneÀás net with N WWD 2n
input and output switches.
The base case, B1, with 2 input switches and 2 output switches is exactly the
same as F1 in Figure 10.1.
In the constructor step, we construct BnC1 out of two Bn nets connected to a
new set of 2nC1 input switches and also a new set of 2nC1 output switches. This is
illustrated in Figure 10.3.
Namely, the ith and 2n C ith new input switches are each connected to the same
two switches, namely, to the ith input switches of each of two Bn components for
i D 1; : : : ; 2n, exactly as in the ButterÔ¨Çy net. In addition, the ith and 2n C ith new
output switches are connected to the same two switches, namely, to the ith output
switches of each of two Bn components.
Now BnC1 is laid out in columns of height 2nC1 by adding two more columns
of switches to the columns in Bn. So the BnC1 switches are arrayed in 2.n C 1/
columns. The total number of switches is the number of columns times the height
of the columns, namely, 2.n C 1/2nC1.
All paths in BnC1 from an input switch to an output are the same length, namely,
2.n C 1/   1, and the diameter of the BeneÀás net with 2nC1 inputs is this length plus
two because of the two edges connecting to the terminals.
So BeneÀás has doubled the number of switches and the diameter, of course, but

10.9. BeneÀás Network
335
completely eliminates congestion problems! The proof of this fact relies on a clever
induction argument that we‚Äôll come to in a moment. Let‚Äôs Ô¨Årst see how the BeneÀás
network stacks up:
network
diameter
switch size
# switches
congestion
complete binary tree
2 log N C 2
3  3
2N   1
N
2-D array
2N
2  2
N 2
2
butterÔ¨Çy
log N C 2
2  2
N.log.N/ C 1/
p
N or
p
N=2
BeneÀás
2 log N C 1
2  2
2N log N
1
The BeneÀás network has small size and diameter, and completely eliminates conges-
tion. The Holy Grail of routing networks is in hand!
Theorem 10.9.1. The congestion of the N -input BeneÀás network is 1.
Proof. By induction on n where N D 2n. So the induction hypothesis is
P.n/ WWD the congestion of Bn is 1:
Base case (n D 1): B1 D F1 is shown in Figure 10.1. The unique routings in F1
have congestion 1.
Inductive step: We assume that the congestion of an N D 2n-input BeneÀás network
is 1 and prove that the congestion of a 2N-input BeneÀás network is also 1.
Digression. Time out! Let‚Äôs work through an example, develop some intuition,
and then complete the proof. In the BeneÀás network shown in Figure 10.4 with
N D 8 inputs and outputs, the two 4-input/output subnetworks are in dashed boxes.
By the inductive assumption, the subnetworks can each route an arbitrary per-
mutation with congestion 1. So if we can guide packets safely through just the Ô¨Årst
and last levels, then we can rely on induction for the rest! Let‚Äôs see how this works
in an example. Consider the following permutation routing problem:
.0/ D 1
.4/ D 3
.1/ D 5
.5/ D 6
.2/ D 4
.6/ D 0
.3/ D 7
.7/ D 2
We can route each packet to its destination through either the upper subnetwork
or the lower subnetwork. However, the choice for one packet may constrain the
choice for another. For example, we cannot route both packet 0 and packet 4
through the same network since that would cause two packets to collide at a single

Chapter 10
Communication Networks
336
in1
out1
in0
out0
in2
out2
in3
out3
in4
out4
in5
out5
in6
out6
in7
out7
Figure 10.4
BeneÀás net B3.
switch, resulting in congestion. So one packet must go through the upper network
and the other through the lower network. Similarly, packets 1 and 5, 2 and 6, and 3
and 7 must be routed through different networks. Let‚Äôs record these constraints in
a graph. The vertices are the 8 packets. If two packets must pass through different
networks, then there is an edge between them. Thus, our constraint graph looks
like this:
1
5
0
4
2
6
7
3
Notice that at most one edge is incident to each vertex.
The output side of the network imposes some further constraints. For example,
the packet destined for output 0 (which is packet 6) and the packet destined for
output 4 (which is packet 2) cannot both pass through the same network; that would
require both packets to arrive from the same switch. Similarly, the packets destined
for outputs 1 and 5, 2 and 6, and 3 and 7 must also pass through different switches.

10.9. BeneÀás Network
337
We can record these additional constraints in our graph with gray edges:
1
5
0
4
2
6
7
3
Notice that at most one new edge is incident to each vertex. The two lines drawn
between vertices 2 and 6 reÔ¨Çect the two different reasons why these packets must
be routed through different networks. However, we intend this to be a simple graph;
the two lines still signify a single edge.
Now here‚Äôs the key insight: suppose that we could color each vertex either red
or blue so that adjacent vertices are colored differently. Then all constraints are
satisÔ¨Åed if we send the red packets through the upper network and the blue packets
through the lower network. Such a 2-coloring of the graph corresponds to a solu-
tion to the routing problem. The only remaining question is whether the constraint
graph is 2-colorable, which is easy to verify:
Lemma 10.9.2. Prove that if the edges of a graph can be grouped into two sets such
that every vertex has at most 1 edge from each set incident to it, then the graph is
2-colorable.
Proof. It is not hard to show that a graph is 2-colorable iff every cycle in it has even
length (see Theorem 11.10.1). We‚Äôll take this for granted here.
So all we have to do is show that every cycle has even length. Since the two sets
of edges may overlap, let‚Äôs call an edge that is in both sets a doubled edge.
There are two cases:
Case 1: [The cycle contains a doubled edge.] No other edge can be incident
to either of the endpoints of a doubled edge, since that endpoint would then be
incident to two edges from the same set. So a cycle traversing a doubled edge has
nowhere to go but back and forth along the edge an even number of times.
Case 2: [No edge on the cycle is doubled.] Since each vertex is incident to
at most one edge from each set, any path with no doubled edges must traverse
successive edges that alternate from one set to the other. In particular, a cycle must
traverse a path of alternating edges that begins and ends with edges from different
sets. This means the cycle has to be of even length.


Chapter 10
Communication Networks
338
For example, here is a 2-coloring of the constraint graph:
blue
blue
blue
blue
red
red
red
red
1
5
0
4
2
6
7
3
The solution to this graph-coloring problem provides a start on the packet routing
problem:
We can complete the routing in the two smaller BeneÀás networks by induction!
Back to the proof. End of Digression.
Let  be an arbitrary permutation of f0; 1; : : : ; N  1g. Let G be the graph whose
vertices are packet numbers 0; 1; : : : ; N   1 and whose edges come from the union
of these two sets:
E1WWDfhu‚Äîvi j ju   vj D N=2g; and
E2WWDfhu‚Äîwi j j.u/   .w/j D N=2g:
Now any vertex, u, is incident to at most two edges: a unique edge hu‚Äîvi 2 E1
and a unique edge hu‚Äîwi 2 E2. So according to Lemma 10.9.2, there is a 2-
coloring for the vertices of G. Now route packets of one color through the upper
subnetwork and packets of the other color through the lower subnetwork. Since
for each edge in E1, one vertex goes to the upper subnetwork and the other to the
lower subnetwork, there will not be any conÔ¨Çicts in the Ô¨Årst level. Since for each
edge in E2, one vertex comes from the upper subnetwork and the other from the
lower subnetwork, there will not be any conÔ¨Çicts in the last level. We can complete
the routing within each subnetwork by the induction hypothesis P.n/.

Problems for Section 10.9
Exam Problems
Problem 10.1.
Consider the following communication network:
(a) What is the max congestion?
0.5in

10.9. BeneÀás Network
339
in2
in1
in0
out0
out1
out2
(b) Give an input/output permutation, 0, that forces maximum congestion:
0.0/ D
0.1/ D
0.2/ D
(c) Give an input/output permutation, 1, that allows minimum congestion:
1.0/ D
1.1/ D
1.2/ D
(d) What is the latency for the permutation 1? (If you could not Ô¨Ånd 1, just
choose a permutation and Ô¨Ånd its latency.)
0.5in
Class Problems
Problem 10.2.
The BeneÀás network has a max congestion of 1; that is, every permutation can be
routed in such a way that a single packet passes through each switch. Let‚Äôs work
through an example. Within the BeneÀás network of size N D 8 shown in Fig-
ure 10.4, the two subnetworks of size N D 4 are marked. We‚Äôll refer to these as
the upper and lower subnetworks.
(a) Now consider the following permutation routing problem:
.0/ D 3
.4/ D 2
.1/ D 1
.5/ D 0
.2/ D 6
.6/ D 7
.3/ D 5
.7/ D 4
Each packet must be routed through either the upper subnetwork or the lower sub-
network. Construct a graph with vertices 0, 1, .. ., 7 and draw a dashed edge
between each pair of packets that can not go through the same subnetwork because
a collision would occur in the second column of switches.
(b) Add a solid edge in your graph between each pair of packets that can not go

Chapter 10
Communication Networks
340
through the same subnetwork because a collision would occur in the next-to-last
column of switches.
(c) Color the vertices of your graph red and blue so that adjacent vertices get
different colors. Why must this be possible, regardless of the permutation ?
(d) Suppose that red vertices correspond to packets routed through the upper sub-
network and blue vertices correspond to packets routed through the lower subnet-
work. On the attached copy of the BeneÀás network, highlight the Ô¨Årst and last edge
traversed by each packet.
(e) All that remains is to route packets through the upper and lower subnetworks.
One way to do this is by applying the procedure described above recursively on
each subnetwork. However, since the remaining problems are small, see if you can
complete all the paths on your own.
Problem 10.3.
A multiple binary-tree network has n inputs and n outputs, where n is a power of 2.
Each input is connected to the root of a binary tree with n=2 leaves and with edges
pointing away from the root. Likewise, each output is connected to the root of a
binary tree with n=2 leaves and with edges pointing toward the root.
Two edges point from each leaf of an input tree, and each of these edges points
to a leaf of an output tree. The matching of leaf edges is arranged so that for every
input and output tree, there is an edge from a leaf of the input tree to a leaf of the
output tree, and every output tree leaf has exactly two edges pointing to it.
(a) Draw such a multiple binary-tree net for n D 4.
(b) Fill in the table, and explain your entries.
# switches
switch size
diameter
max congestion
Problem 10.4.
The n-input 2-D Array network was shown to have congestion 2. An n-input 2-
Layer Array consisting of two n-input 2-D Arrays connected as pictured below for
n D 4.
In general, an n-input 2-Layer Array has two layers of switches, with each layer
connected like an n-input 2-D Array. There is also an edge from each switch in

10.9. BeneÀás Network
341
in0
in1
in2
in3
out0
out1
out2
out3
the Ô¨Årst layer to the corresponding switch in the second layer. The inputs of the
2-Layer Array enter the left side of the Ô¨Årst layer, and the n outputs leave from the
bottom row of either layer.
(a) For any given input-output permutation, there is a way to route packets that
achieves congestion 1. Describe how to route the packets in this way.
(b) What is the latency of a routing designed to minimize latency?
(c) Explain why the congestion of any minimum latency (CML) routing of packets
through this network is greater than the network‚Äôs congestion.
Problem 10.5.
A 5-path communication network is shown below. From this, it‚Äôs easy to see what
an n-path network would be. Fill in the table of properties below, and be prepared
to justify your answers.
network
# switches
switch size
diameter
max congestion
5-path
n-path
Problem 10.6.
Tired of being a TA, Megumi has decided to become famous by coming up with a

Chapter 10
Communication Networks
342
in4
in2
in0
out1
out3
out4
out2
out0
in1
in3
Figure 10.5
5-Path
new, better communication network design. Her network has the following speciÔ¨Å-
cations: every input node will be sent to a butterÔ¨Çy network, a BeneÀás network and
a 2-d array network. At the end, the outputs of all three networks will converge on
the new output.
In the Megumi-net a minimum latency routing does not have minimum conges-
tion. The latency for min-congestion (LMC) of a net is the best bound on latency
achievable using routings that minimize congestion. Likewise, the congestion for
min-latency (CML) is the best bound on congestion achievable using routings that
minimize latency.
2-d Array
Butterfly
Bene≈°
.
.
.
.
.
.
out1
out3
out2
outN
in1
in3
in2
inN
Fill in the following chart for Megumi‚Äôs new net and explain your answers.
network
diameter
# switches
congestion
LMC
CML
Megumi‚Äôs net

10.9. BeneÀás Network
343
Homework Problems
Problem 10.7.
Louis Reasoner Ô¨Ågures that, wonderful as the BeneÀás network may be, the butterÔ¨Çy
network has a few advantages, namely: fewer switches, smaller diameter, and an
easy way to route packets through it. So Louis designs an N -input/output network
he modestly calls a Reasoner-net with the aim of combining the best features of
both the butterÔ¨Çy and BeneÀás nets:
The ith input switch in a Reasoner-net connects to two switches, ai and
bi, and likewise, the j th output switch has two switches, yj and zj ,
connected to it. Then the Reasoner-net has an N -input BeneÀás network
connected using the ai switches as input switches and the yj switches
as its output switches. The Reasoner-net also has an N-input butterÔ¨Çy
net connected using the bi switches as inputs and¬° the zj switches as
outputs.
In the Reasoner-net a minimum latency routing does not have minimum conges-
tion. The latency for min-congestion (LMC) of a net is the best bound on latency
achievable using routings that minimize congestion. Likewise, the congestion for
min-latency (CML) is the best bound on congestion achievable using routings that
minimize latency.
Fill in the following chart for the Reasoner-net and brieÔ¨Çy explain your answers.
diameter
switch size(s)
# switches
congestion
LMC
CML
Problem 10.8.
Show that the congestion of the butterÔ¨Çy net, Fn, is exactly
p
N when n is even.
Hint:
 There is a unique path from each input to each output, so the congestion is
the maximum number of messages passing through a vertex for any routing
problem.
 If v is a vertex in column i of the butterÔ¨Çy network, there is a path from ex-
actly 2i input vertices to v and a path from v to exactly 2n i output vertices.
 At which column of the butterÔ¨Çy network must the congestion be worst?
What is the congestion of the topmost switch in that column of the network?


11
Simple Graphs
Simple graphs model relationships that are symmetric, meaning that the relationship
is mutual. Examples of such mutual relationships are being married, speaking the
same language, not speaking the same language, occurring during overlapping time
intervals, or being connected by a conducting wire. They come up in all sorts of
applications, including scheduling, constraint satisfaction, computer graphics, and
communications, but we‚Äôll start with an application designed to get your attention:
we are going to make a professional inquiry into sexual behavior. Namely, we‚Äôll
look at some data about who, on average, has more opposite-gender partners, men
or women.
Sexual demographics have been the subject of many studies. In one of the largest
studies, researchers from the University of Chicago interviewed a random sample
of 2500 people over several years to try to get an answer to this question. Their
study, published in 1994, and entitled The Social Organization of Sexuality found
that on average men have 74% more opposite-gender partners than women.
Other studies have found that the disparity is even larger. In particular, ABC
News claimed that the average man has 20 partners over his lifetime, and the aver-
age woman has 6, for a percentage disparity of 233%. The ABC News study, aired
on Primetime Live in 2004, purported to be one of the most scientiÔ¨Åc ever done,
with only a 2.5% margin of error. It was called ‚ÄúAmerican Sex Survey: A peek
between the sheets,‚Äù ‚Äîwhich raises some questions about the seriousness of their
reporting.
Yet again, in August, 2007, the N.Y. Times reported on a study by the National
Center for Health Statistics of the U.S. government showing that men had seven
partners while women had four. Anyway, whose numbers do you think are more
accurate, the University of Chicago, ABC News, or the National Center? ‚Äîdon‚Äôt
answer; this is a setup question like ‚ÄúWhen did you stop beating your wife?‚Äù Using
a little graph theory, we‚Äôll explain why none of these Ô¨Åndings can be anywhere near
the truth.
11.1
Vertex Adjacency and Degrees
Simple graphs are deÔ¨Åned as digraphs in which edges are undirected ‚Äîthey just
connect two vertices without pointing in either direction between the vertices. So
instead of a directed edge hv !wi which starts at vertex v and ends at vertex w, a

Chapter 11
Simple Graphs
346
simple graph only has an undirected edge, hv‚Äîwi, that connects v and w.
DeÔ¨Ånition 11.1.1. A simple graph, G, consists of a nonempty set, V.G/, called the
vertices of G, and a set E.G/ called the edges of G. An element of V.G/ is called
a vertex. A vertex is also called a node; the words ‚Äúvertex‚Äù and ‚Äúnode‚Äù are used
interchangeably. An element of E.G/ is an undirected edge or simply an ‚Äúedge.‚Äù
An undirected edge has two vertices u ¬§ v called its endpoints. Such an edge
can be represented by the two element set fu; vg. The notation hu‚Äîvi denotes this
edge.
Both hu‚Äîvi and hv‚Äîui deÔ¨Åne the same undirected edge, namely the one whose
endpoints are u and v.
b
g
h
i
d
f
c
e
a
Figure 11.1
An example of a graph with 9 nodes and 8 edges.
For example, let H be the graph pictured in Figure 11.1. The vertices of H
correspond to the nine dots in Figure 11.1, that is,
V.H/ D fa; b; c; d; e; f; g; h; ig :
The edges correspond to the eight lines, that is,
E.H/ D f ha‚Äîbi ; ha‚Äîci ; hb‚Äîdi ; hc‚Äîdi ; hc‚Äîei ; he‚Äîf i ; he‚Äîgi ; hh‚Äîii g:
Mathematically, that‚Äôs all there is to the graph H.
DeÔ¨Ånition 11.1.2. Two vertices in a simple graph are said to be adjacent iff they
are the endpoints of the same edge, and an edge is said to be incident to each of its
endpoints. The number of edges incident to a vertex v is called the degree of the
vertex and is denoted by deg.v/. Equivalently, the degree of a vertex is the number
of vertices adjacent to it.
For example, for the graph H of Figure 11.1, vertex a is adjacent to vertex b, and
b is adjacent to d. The edge ha‚Äîci is incident to its endpoints a and c. Vertex h
has degree 1, d has degree 2, and deg.e/ D 3. It is possible for a vertex to have
degree 0, in which case it is not adjacent to any other vertices. A simple graph, G,

11.2. Sexual Demographics in America
347
does not need to have any edges at all, namely jE.G/j could be zero, which implies
that the degree of every vertex is also zero. But a simple graph must have at least
one vertex, that is, jV.G/j is required to be at least one.
An edge whose endpoints are the same is called a self-loop. Self-loops aren‚Äôt al-
lowed in simple graphs.1 In a more general class of graphs called multigraphs there
can be more than one edge with the same two endpoints, but this doesn‚Äôt happen in
simple graphs since every edge is uniquely determined by its two endpoints.
Sometimes graphs with no vertices, with self-loops, or with more than one edge
between the same two vertices are convenient to have, but we don‚Äôt need them, and
sticking with simple graphs is simpler. :-)
For the rest of this chapter we‚Äôll use ‚Äúgraphs‚Äù as an abbreviation for ‚Äúsimple
graphs.‚Äù
A synonym for ‚Äúvertices‚Äù is ‚Äúnodes,‚Äù and we‚Äôll use these words interchangeably.
Simple graphs are sometimes called networks, edges are sometimes called arcs.
We mention this as a ‚Äúheads up‚Äù in case you look at other graph theory literature;
we won‚Äôt use these words.
11.2
Sexual Demographics in America
Let‚Äôs model the question of heterosexual partners in graph theoretic terms. To do
this, we‚Äôll let G be the graph whose vertices, V , are all the people in America.
Then we split V into two separate subsets: M, which contains all the males, and
F , which contains all the females.2 We‚Äôll put an edge between a male and a female
iff they have been sexual partners. This graph is pictured in Figure 11.2 with males
on the left and females on the right.
Actually, this is a pretty hard graph to Ô¨Ågure out, let alone draw. The graph is
enormous: the US population is about 300 million, so jV j  300M. Of these,
approximately 50.8% are female and 49.2% are male, so jMj  147:6M, and
jF j  152:4M. And we don‚Äôt even have trustworthy estimates of how many
edges there are, let alone exactly which couples are adjacent. But it turns out that
we don‚Äôt need to know any of this ‚Äîwe just need to Ô¨Ågure out the relationship
between the average number of partners per male and partners per female. To do
this, we note that every edge has exactly one endpoint at an M vertex (remember,
we‚Äôre only considering male-female relationships); so the sum of the degrees of
the M vertices equals the number of edges. For the same reason, the sum of the
1You might try to represent a self-loop going between a vertex v and itself as fv; vg, but this
equals fvg, and it wouldn‚Äôt be an edge which is deÔ¨Åned to be a set of two vertices.
2For simplicity, we‚Äôll ignore the possibility of someone being both a man and a woman, or neither.

Chapter 11
Simple Graphs
348
F
M
Figure 11.2
The sex partners graph.
degrees of the F vertices equals the number of edges. So these sums are equal:
X
x2M
deg.x/ D
X
y2F
deg.y/:
Now suppose we divide both sides of this equation by the product of the sizes of
the two sets, jMj  jF j:
P
x2M deg.x/
jMj

 1
jF j D
 P
y2F deg.y/
jF j
!

1
jMj
The terms above in parentheses are the average degree of an M vertex and the
average degree of a F vertex. So we know:
Avg. deg in M D jF j
jMj  Avg. deg in F
(11.1)
In other words, we‚Äôve proved that the average number of female partners of
males in the population compared to the average number of males per female is
determined solely by the relative number of males and females in the population.
Now the Census Bureau reports that there are slightly more females than males in
America; in particular jF j=jMj is about 1.035. So we know that on average, males
have 3.5% more opposite-gender partners than females, and this tells us nothing
about any sex‚Äôs promiscuity or selectivity. Rather, it just has to do with the relative
number of males and females. Collectively, males and females have the same num-
ber of opposite gender partners, since it takes one of each set for every partnership,

11.3. Some Common Graphs
349
but there are fewer males, so they have a higher ratio. This means that the Uni-
versity of Chicago, ABC, and the Federal government studies are way off. After a
huge effort, they gave a totally wrong answer.
There‚Äôs no deÔ¨Ånite explanation for why such surveys are consistently wrong.
One hypothesis is that males exaggerate their number of partners ‚Äîor maybe fe-
males downplay theirs ‚Äîbut these explanations are speculative. Interestingly, the
principal author of the National Center for Health Statistics study reported that she
knew the results had to be wrong, but that was the data collected, and her job was
to report it.
The same underlying issue has led to serious misinterpretations of other survey
data. For example, a couple of years ago, the Boston Globe ran a story on a survey
of the study habits of students on Boston area campuses. Their survey showed that
on average, minority students tended to study with non-minority students more than
the other way around. They went on at great length to explain why this ‚Äúremarkable
phenomenon‚Äù might be true. But it‚Äôs not remarkable at all ‚Äîusing our graph theory
formulation, we can see that all it says is that there are fewer minority students than
non-minority students, which is, of course, what ‚Äúminority‚Äù means.
11.2.1
Handshaking Lemma
The previous argument hinged on the connection between a sum of degrees and the
number of edges. There is a simple connection between these in any graph:
Lemma 11.2.1. The sum of the degrees of the vertices in a graph equals twice the
number of edges.
Proof. Every edge contributes two to the sum of the degrees, one for each of its
endpoints.

Lemma 11.2.1 is sometimes called the Handshake Lemma: if we total up the
number of people each person at a party shakes hands with, the total will be twice
the number of handshakes that occurred.
11.3
Some Common Graphs
Some graphs come up so frequently that they have names. A complete graph Kn
has n vertices and an edge between every two vertices, for a total of n.n   1/=2
edges. For example, K5 is shown in Figure 11.3.
The empty graph has no edges at all. For example, the empty graph with 5 nodes
is shown in Figure 11.4.

Chapter 11
Simple Graphs
350
Figure 11.3
K5: the complete graph on 5 nodes.
Figure 11.4
An empty graph with 5 nodes.
An n-node graph containing n 1 edges in sequence is known as a line graph Ln.
More formally, Ln has
V.Ln/ D fv1; v2; : : : ; vng
and
E.Ln/ D f hv1‚Äîv2i ; hv2‚Äîv3i ; : : : ; hvn 1‚Äîvni g
For example, L5 is pictured in Figure 11.5.
There is also a one-way inÔ¨Ånite line graph L1 which can be deÔ¨Åned by letting
the nonnegative integers N be the vertices with edges hk‚Äî.k C 1/i for all k 2 N.
If we add the edge hvn‚Äîv1i to the line graph Ln, we get a graph called a length-
n cycle Cn. Figure 11.6 shows a picture of length-5 cycle.
Figure 11.5
L5: a 5-node line graph.

11.4. Isomorphism
351
Figure 11.6
C5: a 5-node cycle graph.
b
c
a
d
(a)
2
3
1
4
(b)
Figure 11.7
Two Isomorphic graphs.
11.4
Isomorphism
Two graphs that look the same might actually be different in a formal sense. For
example, the two graphs in Figure 11.7 are both 4-vertex, 5-edge graphs and you
get graph (b) by a 90o clockwise rotation of graph (a).
Strictly speaking, these graphs are different mathematical objects, but this dif-
ference doesn‚Äôt reÔ¨Çect the fact that the two graphs can be described by the same
picture ‚Äîexcept for the labels on the vertices. This idea of having the same picture
‚Äúup to relabeling‚Äù can be captured neatly by adapting DeÔ¨Ånition 9.6.1 of isomor-
phism of digraphs to handle simple graphs. An isomorphism between two graphs
is an edge-preserving bijection between their sets of vertices:
DeÔ¨Ånition 11.4.1. An isomorphism between graphs G and H is a bijection f W
V.G/ ! V.H/ such that
hu‚Äîvi 2 E.G/
iff
hf .u/‚Äîf .v/i 2 E.H/
for all u; v 2 V.G/. Two graphs are isomorphic when there is an isomorphism
between them.

Chapter 11
Simple Graphs
352
Figure 11.8
Isomorphic C5 graphs.
Here is an isomorphism, f , between the two graphs in Figure 11.7:
f .a/ WWD 2
f .b/ WWD 3
f .c/ WWD 4
f .d/ WWD 1:
You can check that there is an edge between two vertices in the graph on the left if
and only if there is an edge between the two corresponding vertices in the graph on
the right.
Two isomorphic graphs may be drawn very differently. For example, Figure 11.8
shows two different ways of drawing C5.
Notice that if f is an isomorphism between G and H, then f  1 is an isomor-
phism between H and G. Isomorphism is also transitive because the composition
of isomorphisms is an isomorphism. So isomorphism is in fact an equivalence
relation.
Isomorphism preserves the connection properties of a graph, abstracting out what
the vertices are called, what they are made out of, or where they appear in a drawing
of the graph. More precisely, a property of a graph is said to be preserved under
isomorphism if whenever G has that property, every graph isomorphic to G also
has that property. For example, since an isomorphism is a bijection between sets of
vertices, isomorphic graphs must have the same number of vertices. What‚Äôs more,
if f is a graph isomorphism that maps a vertex, v, of one graph to the vertex, f .v/,
of an isomorphic graph, then by deÔ¨Ånition of isomorphism, every vertex adjacent
to v in the Ô¨Årst graph will be mapped by f to a vertex adjacent to f .v/ in the
isomorphic graph. That is, v and f .v/ will have the same degree. So if one graph
has a vertex of degree 4 and another does not, then they can‚Äôt be isomorphic. In
fact, they can‚Äôt be isomorphic if the number of degree 4 vertices in each of the
graphs is not the same.
Looking for preserved properties can make it easy to determine that two graphs
are not isomorphic, or to guide the search for an isomorphism when there is one.
It‚Äôs generally easy in practice to decide whether two graphs are isomorphic. How-
ever, no one has yet found a procedure for determining whether two graphs are

11.5. Bipartite Graphs & Matchings
353
isomorphic that is guaranteed to run in polynomial time on all pairs of graphs.3
Having such a procedure would be useful. For example, it would make it easy
to search for a particular molecule in a database given the molecular bonds. On
the other hand, knowing there is no such efÔ¨Åcient procedure would also be valu-
able: secure protocols for encryption and remote authentication can be built on the
hypothesis that graph isomorphism is computationally exhausting.
The deÔ¨Ånitions of bijection and isomorphism apply to inÔ¨Ånite graphs as well as
Ô¨Ånite graphs, as do most of the results in the rest of this chapter. But graph theory
focuses mostly on Ô¨Ånite graphs, and we will too. So in the rest of this chapter we‚Äôll
assume graphs are Ô¨Ånite.
We‚Äôve actually been taking isomorphism for granted ever since we wrote ‚ÄúKn
has n vertices. . . ‚Äù at the beginning of section 11.3.
Graph theory is all about properties preserved by isomorphism.
11.5
Bipartite Graphs & Matchings
There were two kinds of vertices in the ‚ÄúSex in America‚Äù graph ‚Äîmales and fe-
males, and edges only went between the two kinds. Graphs like this come up so
frequently that they have earned a special name ‚Äîthey are called bipartite graphs.
DeÔ¨Ånition 11.5.1. A bipartite graph is a graph whose vertices can be partitioned4
into two sets, L.G/ and R.G/, such that every edge has one endpoint in L.G/ and
the other endpoint in R.G/.
So every bipartite graph looks something like the graph in Figure 11.2.
11.5.1
The Bipartite Matching Problem
The bipartite matching problem is related to the sex-in-America problem that we
just studied; only now the goal is to get everyone happily married. As you might
imagine, this is not possible for a variety of reasons, not the least of which is the
fact that there are more women in America than men. So, it is simply not possible
to marry every woman to a man so that every man is married at most once.
But what about getting a mate for every man so that every woman is married at
most once? Is it possible to do this so that each man is paired with a woman that
3A procedure runs in polynomial time when it needs an amount of time of at most p.n/, where n
is the total number of vertices and p./ is a Ô¨Åxed polynomial.
4Partitioning a set means cutting it up into nonempty pieces. In this case, it means that L.G/ and
R.G/ are nonempty, L.G/ [ R.G/ D V.G/, and L.G/ \ R.G/ D ;.

Chapter 11
Simple Graphs
354
Chuck
Tom
Michael
John
Alice
Martha
Sara
Jane
Mergatroid
Figure 11.9
A graph where an edge between a man and woman denotes that the
man likes the woman.
he likes? The answer, of course, depends on the bipartite graph that represents who
likes who, but the good news is that it is possible to Ô¨Ånd natural properties of the
who-likes-who graph that completely determine the answer to this question.
In general, suppose that we have a set of men and an equal-sized or larger set of
women, and there is a graph with an edge between a man and a woman if the man
likes the woman. In this scenario, the ‚Äúlikes‚Äù relationship need not be symmetric,
since for the time being, we will only worry about Ô¨Ånding a mate for each man
that he likes.5 (Later, we will consider the ‚Äúlikes‚Äù relationship from the female
perspective as well.) For example, we might obtain the graph in Figure 11.9.
A matching is deÔ¨Åned to be an assignment of a woman to each man so that
different men are assigned to different women, and a man is always assigned a
woman that he likes. For example, one possible matching for the men is shown in
Figure 11.10.
The Matching Condition
A famous result known as Hall‚Äôs Matching Theorem gives necessary and sufÔ¨Åcient
conditions for the existence of a matching in a bipartite graph. It turns out to be a
remarkably useful mathematical tool.
We‚Äôll state and prove Hall‚Äôs Theorem using man-likes-woman terminology. De-
Ô¨Åne the set of women liked by a given set of men to consist of all women liked by
5By the way, we do not mean to imply that marriage should or should not be of a heterosexual
nature. Nor do we mean to imply that men should get their choice instead of women. It‚Äôs just that
with bipartite graphs, the edges only connected male nodes to female nodes and there are fewer men
in America. So please don‚Äôt take offense.

11.5. Bipartite Graphs & Matchings
355
Chuck
Tom
Michael
John
Alice
Martha
Sara
Jane
Mergatroid
Figure 11.10
One possible matching for the men is shown with bold edges. For
example, John is matched with Mergatroid.
at least one of those men. For example, the set of women liked by Tom and John in
Figure 11.9 consists of Martha, Sara, and Mergatroid. For us to have any chance at
all of matching up the men, the following matching condition must hold:
The Matching Condition: every subset of men likes at least as large a set of women.
For example, we cannot Ô¨Ånd a matching if some set of 4 men like only 3 women.
Hall‚Äôs Theorem says that this necessary condition is actually sufÔ¨Åcient; if the match-
ing condition holds, then a matching exists.
Theorem 11.5.2. A matching for a set M of men with a set W of women can be
found if and only if the matching condition holds.
Proof. First, let‚Äôs suppose that a matching exists and show that the matching condi-
tion holds. For any subset of men, each man likes at least the woman he is matched
with and a woman is matched with at most one man. Therefore, every subset of
men likes at least as large a set of women. Thus, the matching condition holds.
Next, let‚Äôs suppose that the matching condition holds and show that a matching
exists. We use strong induction on jMj, the number of men, on the predicate:
P.m/ WWD if the matching condition holds for a set, M,
of m men, then there is a matching for M.
Base case (jMj D 1): If jMj D 1, then the matching condition implies that the
lone man likes at least one woman, and so a matching exists.

Chapter 11
Simple Graphs
356
Inductive Step: Suppose that jMj D m C 1  2. To Ô¨Ånd a matching for M, there
are two cases.
Case 1: Every nonempty subset of at most m men likes a strictly larger set of
women. In this case, we have some latitude: we pair an arbitrary man with
a woman he likes and send them both away. This leaves m men and one
fewer women, and the matching condition will still hold. So the induction
hypothesis P.m/ implies we can match the remaining m men.
Case 2: Some nonempty subset, X, of at most m men likes an equal-size set, Y , of
women. The matching condition must hold within X, so the strong induction
hypothesis implies we can match the men in X with the women in Y . This
leaves the problem of matching the set M   X of men to the set W   Y of
women.
But the problem of matching M  X against W  Y also satisÔ¨Åes the Match-
ing condition, because any subset of men in M   X who liked fewer women
in W   Y would imply there was a set of men who liked fewer women in the
whole set W . Namely, if a subset M0  M   X liked only a strictly smaller
subset of women W0  W   Y , then the set M0 [ X of men would like only
women in the strictly smaller set W0 [ Y . So again the strong induction hy-
pothesis implies we can match the men in M  X with the women in W  Y ,
which completes a matching for M.
So in both cases, there is a matching for the men, which completes the proof of
the Inductive step. The theorem follows by induction.

The proof of Theorem 11.5.2 gives an algorithm for Ô¨Ånding a matching in a
bipartite graph, albeit not a very efÔ¨Åcient one. However, efÔ¨Åcient algorithms for
Ô¨Ånding a matching in a bipartite graph do exist. Thus, if a problem can be reduced
to Ô¨Ånding a matching, the problem is essentially solved from a computational per-
spective.
A Formal Statement
Let‚Äôs restate Theorem 11.5.2 in abstract terms so that you‚Äôll not always be con-
demned to saying, ‚ÄúNow this group of men likes at least as many women...‚Äù
DeÔ¨Ånition 11.5.3. A matching in a graph G is a set M of edges of G such that no
vertex is an endpoint of more than one edge in M. A matching is said to cover a
set, S, of vertices iff each vertex in S is an endpoint of an edge of the matching. A
matching is said to be perfect if it covers V.G/. In any graph, G, the set E.S/ of

11.5. Bipartite Graphs & Matchings
357
neighbors of some set S of vertices is the image of S under the edge-relation, that
is,
E.S/ WWD f r j hs‚Äîri 2 E.G/ for some s 2 S g:
S is called a bottleneck if
jSj > jE.S/j:
Theorem 11.5.4 (Hall‚Äôs Theorem). Let G be a bipartite graph. There is a matching
in G that covers L.G/ iff no subset of L.G/ is a bottleneck.
An Easy Matching Condition
The bipartite matching condition requires that every subset of men has a certain
property. In general, verifying that every subset has some property, even if it‚Äôs easy
to check any particular subset for the property, quickly becomes overwhelming
because the number of subsets of even relatively small sets is enormous ‚Äîover a
billion subsets for a set of size 30. However, there is a simple property of vertex
degrees in a bipartite graph that guarantees the existence of a matching. Namely,
call a bipartite graph degree-constrained if vertex degrees on the left are at least as
large as those on the right. More precisely,
DeÔ¨Ånition 11.5.5. A bipartite graph G is degree-constrained when deg.l/  deg.r/
for every l 2 L.G/ and r 2 R.G/.
For example, the graph in Figure 11.9 is degree-constrained since every node on
the left is adjacent to at least two nodes on the right while every node on the right
is adjacent to at most two nodes on the left.
Theorem 11.5.6. If G is a degree-constrained bipartite graph, then there is a
matching that covers L.G/.
Proof. We will show that G satisÔ¨Åes Hall‚Äôs condition, namely, if S is an arbitrary
subset of L.G/, then
jE.S/j  jSj:
(11.2)
Since G is degree-constrained, there is a d > 0 such that deg.l/  d  deg.r/
for every l 2 L and r 2 R. Since every edge with an endpoint in S has its other
endpoint in E.S/ by deÔ¨Ånition, and every node in E.S/ is incident to at most d
edges, we know that
djE.S/j  #edges with an endpoint in S:
Also, since every node in S is the endpoint of at least d edges,
#edges incident to a vertex in S  djSj:

Chapter 11
Simple Graphs
358
It follows that djE.S/j  djSj. Cancelling d completes the derivation of equa-
tion (11.2).

Regular graphs are a large class of degree-constrained graphs that often arise in
practice. Hence, we can use Theorem 11.5.6 to prove that every regular bipartite
graph has a perfect matching. This turns out to be a surprisingly useful result in
computer science.
DeÔ¨Ånition 11.5.7. A graph is said to be regular if every node has the same degree.
Theorem 11.5.8. Every regular bipartite graph has a perfect matching.
Proof. Let G be a regular bipartite graph. Since regular graphs are degree-constrained,
we know by Theorem 11.5.6 that there must be a matching in G that covers L.G/.
Such a matching is only possible when jL.G/j  jR.G/j. But G is also degree-
constrained if the roles of L.G/ and R.G/ are switched, which implies that jR.G/j 
jL.G/j also. That is, L.G/ and R.G/ are the same size, and any matching covering
L.G/ will also cover R.G/. So every node in G is an endpoint of an edge in the
matching, and thus G has a perfect matching.

11.6
The Stable Marriage Problem
We next consider a version of the bipartite matching problem where there are an
equal number of men and women, and where each person has preferences about
who they would like to marry. In fact, we assume that each man has a complete list
of all the women ranked according to his preferences, with no ties. Likewise, each
woman has a ranked list of all of the men.
The preferences don‚Äôt have to be symmetric. That is, Jennifer might like Brad
best, but Brad doesn‚Äôt necessarily like Jennifer best. The goal is to marry everyone:
every man must marry exactly one woman and vice-versa ‚Äîno polygamy. More-
over, we would like to Ô¨Ånd a matching between men and women that is stable in
the sense that there is no pair of people that prefer each other to their spouses.
For example, suppose every man likes Angelina best, and every woman likes
Brad best, but Brad and Angelina are married to other people, say Jennifer and Billy
Bob. Now Brad and Angelina prefer each other to their spouses, which puts their
marriages at risk: pretty soon, they‚Äôre likely to start spending late nights together
working on problem sets!
This unfortunate situation is illustrated in Figure 11.11, where the digits ‚Äú1‚Äù
and ‚Äú2‚Äù near a man shows which of the two women he ranks Ô¨Årst and second,
respectively, and similarly for the women.

11.6. The Stable Marriage Problem
359
Brad
Billy Bob
Jennifer
Angelina
1
2
2
1
1
2
2
1
Figure 11.11
Preferences for four people. Both men like Angelina best and both
women like Brad best.
More generally, in any matching, a man and woman who are not married to each
other and who like each other better than their spouses, is called a rogue couple. In
the situation shown in Figure 11.11, Brad and Angelina would be a rogue couple.
Having a rogue couple is not a good thing, since it threatens the stability of the
marriages. On the other hand, if there are no rogue couples, then for any man and
woman who are not married to each other, at least one likes their spouse better than
the other, and so they won‚Äôt be tempted to start an affair.
DeÔ¨Ånition 11.6.1. A stable matching is a matching with no rogue couples.
The question is, given everybody‚Äôs preferences, how do you Ô¨Ånd a stable set of
marriages? In the example consisting solely of the four people in Figure 11.11, we
could let Brad and Angelina both have their Ô¨Årst choices by marrying each other.
Now neither Brad nor Angelina prefers anybody else to their spouse, so neither
will be in a rogue couple. This leaves Jen not-so-happily married to Billy Bob, but
neither Jen nor Billy Bob can entice somebody else to marry them, and so there is
a stable matching.
Surprisingly, there always is a stable matching among a group of men and women.
The surprise springs in part from considering the apparently similar ‚Äúbuddy‚Äù match-
ing problem. That is, if people can be paired off as buddies, regardless of gender,
then a stable matching may not be possible. For example, Figure 11.12 shows a
situation with a love triangle and a fourth person who is everyone‚Äôs last choice. In
this Ô¨Ågure Mergatroid‚Äôs preferences aren‚Äôt shown because they don‚Äôt even matter.
Let‚Äôs see why there is no stable matching.
Lemma 11.6.2. There is no stable buddy matching among the four people in Fig-
ure 11.12.
Proof. We‚Äôll prove this by contradiction.
Assume, for the purposes of contradiction, that there is a stable matching. Then
there are two members of the love triangle that are matched. Since preferences in

Chapter 11
Simple Graphs
360
Robin
Bobby Joe
Alex
Mergatroid
3
3
2
1
1
2
3
1
2
Figure 11.12
Some preferences with no stable buddy matching.
the triangle are symmetric, we may assume in particular, that Robin and Alex are
matched. Then the other pair must be Bobby-Joe matched with Mergatroid.
But then there is a rogue couple: Alex likes Bobby-Joe best, and Bobby-Joe
prefers Alex to his buddy Mergatroid. That is, Alex and Bobby-Joe are a rogue
couple, contradicting the assumed stability of the matching.

So getting a stable buddy matching may not only be hard, it may be impossible.
But when men are only allowed to marry women, and vice versa, then it turns out
that a stable matching can always be found.6
11.6.1
The Mating Ritual
The procedure for Ô¨Ånding a stable matching involves a Mating Ritual that takes
place over several days. The following events happen each day:
Morning: Each woman stands on her balcony. Each man stands under the bal-
cony of his favorite among the women on his list, and he serenades her. If a man
has no women left on his list, he stays home and does his math homework.
Afternoon: Each woman who has one or more suitors serenading her, says to
her favorite among them, ‚ÄúWe might get engaged. Come back tomorrow.‚Äù To the
other suitors, she says, ‚ÄúNo. I will never marry you! Take a hike!‚Äù
Evening: Any man who is told by a woman to take a hike, crosses that woman
off his list.
Termination condition: When a day arrives in which every woman has at most
one suitor, the ritual ends with each woman marrying her suitor, if she has one.
There are a number of facts about this Mating Ritual that we would like to prove:
6Once again, we disclaim any political statement here ‚Äîit‚Äôs just the way that the math works out.

11.6. The Stable Marriage Problem
361
 The Ritual eventually reaches the termination condition.
 Everybody ends up married.
 The resulting marriages are stable.
11.6.2
There is a Marriage Day
It‚Äôs easy to see why the Mating Ritual has a terminal day when people Ô¨Ånally get
married. Every day on which the ritual hasn‚Äôt terminated, at least one man crosses
a woman off his list. (If the ritual hasn‚Äôt terminated, there must be some woman
serenaded by at least two men, and at least one of them will have to cross her off his
list). If we start with n men and n women, then each of the n men‚Äôs lists initially
has n women on it, for a total of n2 list entries. Since no women ever gets added
to a list, the total number of entries on the lists decreases every day that the Ritual
continues, and so the Ritual can continue for at most n2 days.
11.6.3
They All Live Happily Ever After...
We still have to prove that the Mating Ritual leaves everyone in a stable marriage.
To do this, we note one very useful fact about the Ritual: if a woman has a favorite
suitor on some morning of the Ritual, then that favorite suitor will still be serenad-
ing her the next morning ‚Äîbecause his list won‚Äôt have changed. So she is sure to
have today‚Äôs favorite man among her suitors tomorrow. That means she will be able
to choose a favorite suitor tomorrow who is at least as desirable to her as today‚Äôs
favorite. So day by day, her favorite suitor can stay the same or get better, never
worse. This sounds like an invariant, and it is.
DeÔ¨Ånition 11.6.3. Let P be the predicate: For every woman, w, and every man,
m, if w is crossed off m‚Äôs list, then w has a suitor whom she prefers over m.
Lemma 11.6.4. P is an invariant for The Mating Ritual.
Proof. By induction on the number of days.
Base case: In the beginning ‚Äîthat is, at the end of day 0‚Äîevery woman is on
every list. So no one has been crossed off, and P is vacuously true.
Inductive Step: Assume P is true at the end of day d and let w be a woman that
has been crossed off a man m‚Äôs list by the end of day d C 1.
Case 1: w was crossed off m‚Äôs list on day d C 1. Then, w must have a suitor she
prefers on day d C 1.

Chapter 11
Simple Graphs
362
Case 2: w was crossed off m‚Äôs list prior to day d C1. Since P is true at the end of
day d, this means that w has a suitor she prefers to m on day d. She therefore
has the same suitor or someone she prefers better at the end of day d C 1.
In both cases, P is true at the end of day d C 1 and so P must be an invariant.

With Lemma 11.6.4 in hand, we can now prove:
Theorem 11.6.5. Everyone is married by the Mating Ritual.
Proof. By contradiction. Assume that it is the last day of the Mating Ritual and
someone does not get married. Since there are an equal number of men and women,
and since bigamy is not allowed, this means that at least one man (call him Bob)
and at least one woman do not get married.
Since Bob is not married, he can‚Äôt be serenading anybody and so his list must
be empty. This means that Bob has crossed every woman off his list and so, by
invariant P , every woman has a suitor whom she prefers to Bob. Since it is the last
day and every woman still has a suitor, this means that every woman gets married.
This is a contradiction since we already argued that at least one woman is not
married. Hence our assumption must be false and so everyone must be married.

Theorem 11.6.6. The Mating Ritual produces a stable matching.
Proof. Let Brad and Jen be any man and woman, respectively, that are not married
to each other on the last day of the Mating Ritual. We will prove that Brad and Jen
are not a rogue couple, and thus that all marriages on the last day are stable. There
are two cases to consider.
Case 1: Jen is not on Brad‚Äôs list by the end. Then by invariant P , we know that
Jen has a suitor (and hence a husband) that she prefers to Brad. So she‚Äôs not
going to run off with Brad ‚ÄîBrad and Jen cannot be a rogue couple.
Case 2: Jen is on Brad‚Äôs list. But since Brad is not married to Jen, he must be
choosing to serenade his wife instead of Jen, so he must prefer his wife. So
he‚Äôs not going to run off with Jen ‚Äîonce again, Brad and Jen are not a rogue
couple.

11.6.4
...Especially the Men
Who is favored by the Mating Ritual, the men or the women? The women seem
to have all the power: they stand on their balconies choosing the Ô¨Ånest among
their suitors and spurning the rest. What‚Äôs more, we know their suitors can only
change for the better as the Ritual progresses. Similarly, a man keeps serenading

11.6. The Stable Marriage Problem
363
the woman he most prefers among those on his list until he must cross her off,
at which point he serenades the next most preferred woman on his list. So from
the man‚Äôs perspective, the woman he is serenading can only change for the worse.
Sounds like a good deal for the women.
But it‚Äôs not! The fact is that from the beginning, the men are serenading their
Ô¨Årst choice woman, and the desirability of the woman being serenaded decreases
only enough to ensure overall stability. The Mating Ritual actually does as well as
possible for all the men and does the worst possible job for the women.
To explain all this we need some deÔ¨Ånitions. Let‚Äôs begin by observing that while
The Mating Ritual produces one stable matching, there may be other stable match-
ings among the same set of men and women. For example, reversing the roles of
men and women will often yield a different stable matching among them.
But some spouses might be out of the question in all possible stable matchings.
For example, given the preferences shown in Figure 11.11, Brad is just not in the
realm of possibility for Jennifer, since if you ever pair them, Brad and Angelina
will form a rogue couple.
DeÔ¨Ånition 11.6.7. Given a set of preference lists for all men and women, one per-
son is in another person‚Äôs realm of possible spouses if there is a stable matching
in which the two people are married. A person‚Äôs optimal spouse is their most pre-
ferred person within their realm of possibility. A person‚Äôs pessimal spouse is their
least preferred person in their realm of possibility.
Everybody has an optimal and a pessimal spouse, since we know there is at least
one stable matching, namely, the one produced by the Mating Ritual. Now here is
the shocking truth about the Mating Ritual:
Theorem 11.6.8. The Mating Ritual marries every man to his optimal spouse.
Proof. By contradiction. Assume for the purpose of contradiction that some man
does not get his optimal spouse. Then there must have been a day when he crossed
off his optimal spouse ‚Äîotherwise he would still be serenading (and would ulti-
mately marry) her or some even more desirable woman.
By the Well Ordering Principle, there must be a Ô¨Årst day when a man (call him
Keith) crosses off his optimal spouse (call her Nicole). According to the rules of
the Ritual, Keith crosses off Nicole because Nicole has a preferred suitor (call him
Tom), so
Nicole prefers Tom to Keith.
()
Since this is the Ô¨Årst day an optimal woman gets crossed off, we know that Tom
had not previously crossed off his optimal spouse, and so
Tom ranks Nicole at least as high as his optimal spouse.
()

Chapter 11
Simple Graphs
364
By the deÔ¨Ånition of an optimal spouse, there must be some stable set of marriages in
which Keith gets his optimal spouse, Nicole. But then the preferences given in ()
and () imply that Nicole and Tom are a rogue couple within this supposedly
stable set of marriages (think about it). This is a contradiction.

Theorem 11.6.9. The Mating Ritual marries every woman to her pessimal spouse.
Proof. Assume for the sake of contradiction that the theorem is not true. Hence
there must be a stable set of marriages M where some woman (call her Nicole) is
married to a man (call him Tom) that she likes less than her spouse in The Mating
Ritual (call him Keith). This means that
Nicole prefers Keith to Tom.
(+)
By Theorem 11.6.8 and the fact that Nicole and Keith are married in the Mating
Ritual, we know that
Keith prefers Nicole to his spouse in M.
(++)
This means that Keith and Nicole form a rogue couple in M, which contradicts the
stability of M.

11.6.5
Applications
The Mating Ritual was Ô¨Årst announced in a paper by D. Gale and L.S. Shapley in
1962, but ten years before the Gale-Shapley paper was published, and unknown
by them, a similar algorithm was being used to assign residents to hospitals by
the National Resident Matching Program (NRMP)7. The NRMP has, since the turn
of the twentieth century, assigned each year‚Äôs pool of medical school graduates to
hospital residencies (formerly called ‚Äúinternships‚Äù) with hospitals and graduates
playing the roles of men and women. (In this case, there may be multiple women
married to one man, a scenario we consider in the problem section at the end of the
chapter.). Before the Ritual-like algorithm was adopted, there were chronic disrup-
tions and awkward countermeasures taken to preserve assignments of graduates to
residencies. The Ritual resolved these problems so successfully, that it was used
essentially without change at least through 1989.8
7Of course, there is no serenading going on in the hospitals ‚Äîthe preferences are submitted to a
program and the whole process is carried out by a computer.
8Much more about the Stable Marriage Problem can be found in the very readable mathematical
monograph by Dan GusÔ¨Åeld and Robert W. Irving, The Stable Marriage Problem: Structure and
Algorithms, MIT Press, Cambridge, Massachusetts, 1989, 240 pp.

11.7. Coloring
365
The Internet infrastructure company, Akamai, also uses a variation of the Mating
Ritual to assign web trafÔ¨Åc to its servers. In the early days, Akamai used other com-
binatorial optimization algorithms that got to be too slow as the number of servers
(over 65,000 in 2010) and requests (over 800 billion per day) increased. Akamai
switched to a Ritual-like approach since it is fast and can be run in a distributed
manner. In this case, web requests correspond to women and web servers corre-
spond to men. The web requests have preferences based on latency and packet loss,
and the web servers have preferences based on cost of bandwidth and colocation.
Not surprisingly, the Mating Ritual is also used by at least one large online dating
agency. Even here, there is no serenading going on ‚Äîeverything is handled by
computer.
11.7
Coloring
In Section 11.2, we used edges to indicate an afÔ¨Ånity between a pair of nodes. But
there are lots of situations where edges will correspond to conÔ¨Çicts between nodes.
Exam scheduling is a typical example.
11.7.1
An Exam Scheduling Problem
Each term, the MIT Schedules OfÔ¨Åce must assign a time slot for each Ô¨Ånal exam.
This is not easy, because some students are taking several classes with Ô¨Ånals, and
(even at MIT) a student can take only one test during a particular time slot. The
Schedules OfÔ¨Åce wants to avoid all conÔ¨Çicts. Of course, you can make such a
schedule by having every exam in a different slot, but then you would need hun-
dreds of slots for the hundreds of courses, and the exam period would run all year!
So, the Schedules OfÔ¨Åce would also like to keep exam period short.
The Schedules OfÔ¨Åce‚Äôs problem is easy to describe as a graph. There will be a
vertex for each course with a Ô¨Ånal exam, and two vertices will be adjacent exactly
when some student is taking both courses. For example, suppose we need to sched-
ule exams for 6.041, 6.042, 6.002, 6.003 and 6.170. The scheduling graph might
appear as in Figure 11.13.
6.002 and 6.042 cannot have an exam at the same time since there are students in
both courses, so there is an edge between their nodes. On the other hand, 6.042 and
6.170 can have an exam at the same time if they‚Äôre taught at the same time (which
they sometimes are), since no student can be enrolled in both (that is, no student
should be enrolled in both when they have a timing conÔ¨Çict).
We next identify each time slot with a color. For example, Monday morning

Chapter 11
Simple Graphs
366
6:002
6:170
6:003
6:042
6:041
Figure 11.13
A scheduling graph for Ô¨Åve exams. Exams connected by an edge
cannot be given at the same time.
red
blue
green
blue
green
Figure 11.14
A 3-coloring of the exam graph from Figure 11.13.
is red, Monday afternoon is blue, Tuesday morning is green, etc. Assigning an
exam to a time slot is then equivalent to coloring the corresponding vertex. The
main constraint is that adjacent vertices must get different colors ‚Äîotherwise, some
student has two exams at the same time. Furthermore, in order to keep the exam
period short, we should try to color all the vertices using as few different colors as
possible. As shown in Figure 11.14, three colors sufÔ¨Åce for our example.
The coloring in Figure 11.14 corresponds to giving one Ô¨Ånal on Monday morning
(red), two Monday afternoon (blue), and two Tuesday morning (green). Can we use
fewer than three colors? No! We can‚Äôt use only two colors since there is a triangle
in the graph, and three vertices in a triangle must all have different colors.
This is an example of a graph coloring problem: given a graph G, assign colors
to each node such that adjacent nodes have different colors. A color assignment
with this property is called a valid coloring of the graph ‚Äîa ‚Äúcoloring,‚Äù for short.
A graph G is k-colorable if it has a coloring that uses at most k colors.
DeÔ¨Ånition 11.7.1. The minimum value of k for which a graph, G, has a valid

11.7. Coloring
367
coloring is called its chromatic number, .G/.
So G is k-colorable iff .G/  k.
In general, trying to Ô¨Ågure out if you can color a graph with a Ô¨Åxed number of
colors can take a long time. It‚Äôs a classic example of a problem for which no fast
algorithms are known. In fact, it is easy to check if a coloring works, but it seems
really hard to Ô¨Ånd it. (If you Ô¨Ågure out how, then you can get a $1 million Clay
prize.)
11.7.2
Some Coloring Bounds
There are some simple properties of graphs that give useful bounds on colorability.
The simplest property is being a cycle: an even-length closed cycle is 2-colorable,
and since by deÔ¨Ånition it must have some edges, it is not 1-colorable. So
.Ceven/ D 2:
On the other hand, an odd-length cycle requires 3 colors, that is,
.Codd/ D 3:
(11.3)
You should take a moment to think about why this equality holds. Another simple
example is a complete graph Kn:
.Kn/ D n
since no two vertices can have the same color.
Being bipartite is another property closely related to colorability. If a graph is
bipartite, then you can color it with 2 colors using one color for the nodes on the
‚Äúleft‚Äù and a second color for the nodes on the ‚Äúright.‚Äù Conversely, graphs with
chromatic number 2 are all bipartite with all the vertices of one color on the ‚Äúleft‚Äù
and those with the other color on the right. Since only graphs with no edges ‚Äîthe
empty graphs ‚Äîhave chromatic number 1, we have:
Lemma 11.7.2. A graph, G, with at least one edge is bipartite iff .G/ D 2.
The chromatic number of a graph can also be shown to be small if the vertex
degrees of the graph are small. In particular, if we have an upper bound on the
degrees of all the vertices in a graph, then we can easily Ô¨Ånd a coloring with only
one more color than the degree bound.
Theorem 11.7.3. A graph with maximum degree at most k is .k C 1/-colorable.

Chapter 11
Simple Graphs
368
Since k is the only nonnegative integer valued variable mentioned in the the-
orem, you might be tempted to try to prove this theorem using induction on k.
Unfortunately, this approach leads to disaster ‚Äîwe don‚Äôt know of any reasonable
way to do this and expect it would ruin your week if you tried it on a problem set.
When you encounter such a disaster using induction on graphs, it is usually best to
change what you are inducting on. In graphs, typical good choices for the induction
parameter are n, the number of nodes, or e, the number of edges.
Proof of Theorem 11.7.3. We use induction on the number of vertices in the graph,
which we denote by n. Let P.n/ be the proposition that an n-vertex graph with
maximum degree at most k is .k C 1/-colorable.
Base case (n D 1): A 1-vertex graph has maximum degree 0 and is 1-colorable, so
P.1/ is true.
Inductive step: Now assume that P.n/ is true, and let G be an .nC1/-vertex graph
with maximum degree at most k. Remove a vertex v (and all edges incident to it),
leaving an n-vertex subgraph, H. The maximum degree of H is at most k, and so
H is .k C 1/-colorable by our assumption P.n/. Now add back vertex v. We can
assign v a color (from the set of k C 1 colors) that is different from all its adjacent
vertices, since there are at most k vertices adjacent to v and so at least one of the
k C 1 colors is still available. Therefore, G is .k C 1/-colorable. This completes
the inductive step, and the theorem follows by induction.

Sometimes k C 1 colors is the best you can do. For example, .Kn/ D n
and every node in Kn has degree k D n   1 and so this is an example where
Theorem 11.7.3 gives the best possible bound. By a similar argument, we can
show that Theorem 11.7.3 gives the best possible bound for any graph with degree
bounded by k that has KkC1 as a subgraph.
But sometimes k C 1 colors is far from the best that you can do. For example,
the n-node star graph shown in Figure 11.15 has maximum degree n   1 but can
be colored using just 2 colors.
11.7.3
Why coloring?
One reason coloring problems frequently arise in practice is because scheduling
conÔ¨Çicts are so common. For example, at Akamai, a new version of software is
deployed over each of 65,000 servers every few days. The updates cannot be done
at the same time since the servers need to be taken down in order to deploy the
software. Also, the servers cannot be handled one at a time, since it would take
forever to update them all (each one takes about an hour). Moreover, certain pairs
of servers cannot be taken down at the same time since they have common critical

11.7. Coloring
369
Figure 11.15
A 7-node star graph.
functions. This problem was eventually solved by making a 65,000-node conÔ¨Çict
graph and coloring it with 8 colors ‚Äîso only 8 waves of install are needed!
Another example comes from the need to assign frequencies to radio stations. If
two stations have an overlap in their broadcast area, they can‚Äôt be given the same
frequency. Frequencies are precious and expensive, so you want to minimize the
number handed out. This amounts to Ô¨Ånding the minimum coloring for a graph
whose vertices are the stations and whose edges connect stations with overlapping
areas.
Coloring also comes up in allocating registers for program variables. While a
variable is in use, its value needs to be saved in a register. Registers can be reused
for different variables but two variables need different registers if they are refer-
enced during overlapping intervals of program execution. So register allocation is
the coloring problem for a graph whose vertices are the variables: vertices are ad-
jacent if their intervals overlap, and the colors are registers. Once again, the goal is
to minimize the number of colors needed to color the graph.
Finally, there‚Äôs the famous map coloring problem stated in Proposition 1.1.6. The
question is how many colors are needed to color a map so that adjacent territories
get different colors? This is the same as the number of colors needed to color a
graph that can be drawn in the plane without edges crossing. A proof that four
colors are enough for planar graphs was acclaimed when it was discovered about
thirty years ago. Implicit in that proof was a 4-coloring procedure that takes time
proportional to the number of vertices in the graph (countries in the map).
Surprisingly, it‚Äôs another of those million dollar prize questions to Ô¨Ånd an efÔ¨Å-
cient procedure to tell if a planar graph really needs four colors, or if three will
actually do the job. A proof that testing 3-colorability of graphs is as hard as the
million dollar SAT problem is given in Problem 11.25; this turns out to be true
even for planar graphs. (It is easy to tell if a graph is 2-colorable, as explained in
Section 11.10.) In Chapter 12, we‚Äôll develop enough planar graph theory to present
an easy proof that all planar graphs are 5-colorable.

Chapter 11
Simple Graphs
370
11.8
Getting from u to v in a Graph
Walks and paths in simple graphs are esentially the same as in digraphs. We just
modify the digraph deÔ¨Ånitions using undirected edges instead of directed ones. For
example, the formal deÔ¨Ånition of a walk in a simple graph is a virtually the same
as the DeÔ¨Ånition 9.1.4 of a walk in a digraph:
DeÔ¨Ånition 11.8.1. A walk in a simple graph, G, is an alternating sequence of ver-
tices and edges that begins with a vertex, ends with a vertex, and such that for every
edge hu‚Äîvi in the walk, one of the endpoints u, v is the element just before the
edge, and the other endpoint is the next element after the edge. The length of a
walk is the total number of occurrences of edges in it.
So a walk, v, is a sequence of the form
v WWD v0 hv0‚Äîv1i v1 hv1‚Äîv2i v2 : : : hvk 1‚Äîvki vk
where hvi‚ÄîviC1i 2 E.G/ for i 2 ≈í0; k/. The walk is said to start at v0, to end
at vk, and the length, jvj, of the walk is k. The walk is a path iff all the vi‚Äôs are
different, that is, if i ¬§ j , then vi ¬§ vj .
A closed walk is a walk that begins and ends at the same vertex. A cycle is
a closed walk of length three or more whose vertices are distinct except for the
beginning and end vertices.
Note that a single vertex counts as a length zero path and closed walk. But in
contrast to digraphs, a single vertex is not considered to be a cycle.
As in digraphs, the length of a walk is one less than the number of occurrences of
vertices in it. For example, the graph in Figure 11.16 has a length 6 path through the
seven successive vertices abcdefg. This is the longest path in the graph. The graph
in Figure 11.16 also has three cycles through successive vertices bhecb, cdec, and
bcdehb.
11.8.1
Cycles as Subgraphs
A cycle does not really have a beginning or an end, and so can be described by any
of the paths that go around it. For example, in the graph in Figure 11.16, the cycle
starting at b and going through vertices bcdehb can also be described as starting
at d and going through decbcd. Furthermore, cycles in simple graphs don‚Äôt have
a direction: dcbced describes the same cycle as though it started and ended at d
but went in the opposite direction.
A precise way to explain which closed walks describe the same cycle is to deÔ¨Åne
cycle as a subgraph instead of as a closed walk. Namely, we could deÔ¨Åne a cycle
in G to be a subgraph of G that looks like a length-n cycle for n  3.

11.9. Connectivity
371
a
b
c
d
e
f
g
h
Figure 11.16
A graph with 3 cycles: bhecb, cdec, bcdehb.
DeÔ¨Ånition 11.8.2. A graph G is said to be a subgraph of a graph H if V.G/ 
V.H/ and E.G/  E.H/.
For example, the one-edge graph G where
V.G/ D fg; h; ig
and
E.G/ D f hh‚Äîii g
is a subgraph of the graph H in Figure 11.1. On the other hand, any graph con-
taining an edge hg‚Äîhi will not be a subgraph of H because this edge is not in
E.H/. Another example is an empty graph on n nodes, which will be a subgraph
of an Ln with the same set of nodes; similarly, Ln is a subgraph of Cn, and Cn is
a subgraph of Kn.
DeÔ¨Ånition 11.8.3. For n  3, let Cn be the graph with vertices 1; : : : ; n and edges
h1‚Äî2i ; h2‚Äî3i ; : : : ; h.n   1/‚Äîni ; hn‚Äî1i :
A cycle of a graph, G, is a subgraph of G that is isomorphic to Cn for some
n  3.
This deÔ¨Ånition formally captures the idea that cycles don‚Äôt have direction or be-
ginnings or ends.
11.9
Connectivity
DeÔ¨Ånition 11.9.1. Two vertices are connected in a graph when there is a path that
begins at one and ends at the other. By convention, every vertex is connected to
itself by a path of length zero. A graph is connected when every pair of vertices
are connected.

Chapter 11
Simple Graphs
372
11.9.1
Connected Components
Being connected is usually a good property for a graph to have. For example, it
could mean that it is possible to get from any node to any other node, or that it is
possible to communicate between any pair of nodes, depending on the application.
But not all graphs are connected. For example, the graph where nodes represent
cities and edges represent highways might be connected for North American cities,
but would surely not be connected if you also included cities in Australia. The
same is true for communication networks like the Internet ‚Äîin order to be protected
from viruses that spread on the Internet, some government networks are completely
isolated from the Internet.
Figure 11.17
One graph with 3 connected components.
Another example, is shown in Figure 11.17, which looks like a picture of three
graphs, but is intended to be a picture of one graph. This graph consists of three
pieces (subgraphs). Each piece by itself is connected, but there are no paths be-
tween vertices in different pieces. These connected pieces of a graph are called its
connected components.
DeÔ¨Ånition 11.9.2. A connected component of a graph is a subgraph consisting of
some vertex and every node and edge that is connected to that vertex.
So a graph is connected iff it has exactly one connected component. At the other
extreme, the empty graph on n vertices has n connected components.
11.9.2
k-Connected Graphs
If we think of a graph as modeling cables in a telephone network, or oil pipelines,
or electrical power lines, then we not only want connectivity, but we want connec-
tivity that survives component failure. So more generally we want to deÔ¨Åne how
strongly two vertices are connected. One measure of connection strength is how
many links must fail before connectedness fails. In particular, two vertices are k-
edge connected when it takes at least k ‚Äúedge-failures‚Äù to disconnect them. More
precisely:

11.9. Connectivity
373
DeÔ¨Ånition 11.9.3. Two vertices in a graph are k-edge connected when they remain
connected in every subgraph obtained by deleting up to k   1 edges. A graph is
k-edge connected when it has more than one vertex, and every subgraph obtained
by deleting at most k   1 edges is connected.
So two vertices are connected according to DeÔ¨Ånition 11.9.1 iff they are 1-edge
connected according to DeÔ¨Ånition 11.9.3; likewise for any graph with more than
one vertex.
There are other kinds of connectedness but edge-connectedness will be enough
for us, so from now on we‚Äôll drop the ‚Äúedge‚Äù modiÔ¨Åer and just say ‚Äúconnected.‚Äù9
For example, in the graph in Figure 11.16, vertices c and e are 3 connected, b
and e are 2 connected, g and e are 1 connected, and no vertices are 4 connected.
The graph as a whole is only 1 connected. A complete graph, Kn, is .n   1/
connected. Every cycle is 2-connected.
The idea of a cut edge is a useful way to explain 2-connectivity.
DeÔ¨Ånition 11.9.4. If two vertices are connected in a graph G, but not connected
when an edge e is removed, then e is called a cut edge of G.
So a graph with more than one vertex is 2-connected iff it is connected, and
has no cut edges. The following Lemma is another immediate consequence of the
deÔ¨Ånition:
Lemma 11.9.5. An edge is a cut edge iff it is not on a cycle.
More generally, if two vertices are connected by k edge-disjoint paths ‚Äîthat is,
no edge occurs in two paths ‚Äîthen they must be k connected, since at least one
edge will have to be removed from each of the paths before they could disconnect.
A fundamental fact, whose ingenious proof we omit, is Menger‚Äôs theorem which
conÔ¨Årms that the converse is also true: if two vertices are k-connected, then there
are k edge-disjoint paths connecting them. It takes some ingenuity to prove this
just for the case k D 2.
11.9.3
The Minimum Number of Edges in a Connected Graph
The following theorem says that a graph with few edges must have many connected
components.
Theorem 11.9.6. Every graph, G, has at least jV.G/j   jE.G/j connected com-
ponents.
9There is an obvious deÔ¨Ånition of k-vertex connectedness based on deleting vertices rather than
edges. Graph theory texts usually use ‚Äúk-connected‚Äù as shorthand for ‚Äúk-vertex connected.‚Äù

Chapter 11
Simple Graphs
374
Of course for Theorem 11.9.6 to be of any use, there must be fewer edges than
vertices.
Proof. We use induction on the number, k, of edges. Let P.k/ be the proposition
that
every graph, G, with k edges has at least jV.G/j   k connected com-
ponents.
Base case (k D 0): In a graph with 0 edges, each vertex is itself a connected
component, and so there are exactly jV.G/j D jV.G/j   0 connected components.
So P.0/ holds.
Inductive step:
Let Ge be the graph that results from removing an edge, e 2 E.G/. So Ge
has k edges, and by the induction hypothesis P.k/, we may assume that Ge has
at least jV.G/j   k connected components. Now add back the edge e to obtain
the original graph G. If the endpoints of e were in the same connected component
of Ge, then G has the same sets of connected vertices as Ge, so G has at least
jV.G/j   k > jV.G/j   .k C 1/ components. Alternatively, if the endpoints of
e were in different connected components of Ge, then these two components are
merged into one component in G, while all other components remain unchanged,
so that G has one fewer connected component than Ge. That is, G has at least
.jV.G/j   k/   1 D jV.G/j   .k C 1/ connected components. So in either case, G
has at least jV.G/j   .k C 1/ components, as claimed.
This completes the inductive step and hence the entire proof by induction.

Corollary 11.9.7. Every connected graph with n vertices has at least n   1 edges.
A couple of points about the proof of Theorem 11.9.6 are worth noticing. First,
we used induction on the number of edges in the graph. This is very common in
proofs involving graphs, as is induction on the number of vertices. When you‚Äôre
presented with a graph problem, these two approaches should be among the Ô¨Årst
you consider.
The second point is more subtle. Notice that in the inductive step, we took an
arbitrary .kC1/-edge graph, threw out an edge so that we could apply the induction
assumption, and then put the edge back. You‚Äôll see this shrink-down, grow-back
process very often in the inductive steps of proofs related to graphs. This might
seem like needless effort: why not start with an k-edge graph and add one more to
get an .k C 1/-edge graph? That would work Ô¨Åne in this case, but opens the door
to a nasty logical error called buildup error illustrated in Problem 11.30.

11.10. Odd Cycles and 2-Colorability
375
11.10
Odd Cycles and 2-Colorability
We have already seen that determining the chromatic number of a graph is a chal-
lenging problem. There is one special case where this problem is very easy, namely,
when the graph is 2-colorable.
Theorem 11.10.1. The following graph properties are equivalent:
1. The graph contains an odd length cycle.
2. The graph is not 2-colorable.
3. The graph contains an odd length closed walk.
In other words, if a graph has any one of the three properties above, then it has
all of the properties.
We will show the following implications among these properties:
1. IMPLIES 2. IMPLIES 3. IMPLIES 1:
So each of these properties implies the other two, which means they all are equiva-
lent.
1 IMPLIES 2 Proof. This follows from equation 11.3.

2 IMPLIES 3 If we prove this implication for connected graphs, then it will hold
for an arbitrary graph because it will hold for each connected component. So
we can assume that G is connected.
Proof. Pick an arbitrary vertex r of G. Since G is connected, for every node
u 2 V.G/, there will be a walk wu starting at u and ending at r. Assign
colors to vertices of G as follows:
color.u/ D
(
black;
if jwuj is even;
white;
otherwise:
Now since G is not colorable, this can‚Äôt be a valid coloring. So there must
be an edge between two nodes u and v with the same color. But in that case
wubreverse.wv/bhv‚Äîui

Chapter 11
Simple Graphs
376
is a closed walk starting and ending at u, and its length is
jwuj C jwvj C 1:
This length is odd, since wu and wv are both even length or are both odd
length.

3 IMPLIES 1 Proof. Since there is an odd length closed walk, the WOP implies
there is an odd length closed walk w of minimum length. We claim w must
be a cycle. To show this, assume to the contrary that there is vertex x that ap-
pears twice on the walk, so w consists of a closed walk from x to x followed
by another such walk. That is,
w D fbx r
for some positive length walks f and r that begin and end at x. Since
jwj D jfj C jrj
is odd, exactly one of f and g must have odd length, and that one will be an
odd length closed walk shorter than w, a contradiction.

This completes the proof of Theorem 11.10.1.
Theorem 11.10.1 turns out to be useful since bipartite graphs come up fairly often
in practice. We‚Äôll see examples when we talk about planar graphs in Chapter 12.
11.11
Forests & Trees
We‚Äôve already made good use of digraphs without cycles, but simple graphs without
cycles are arguably the most important graphs of all in computer science.
11.11.1
Leaves, Parents & Children
DeÔ¨Ånition 11.11.1. An acyclic graph is called a forest. A connected acyclic graph
is called a tree.
The graph shown in Figure 11.18 is a forest. Each of its connected components
is by deÔ¨Ånition a tree.
One of the Ô¨Årst things you will notice about trees is that they tend to have a lot
of nodes with degree one. Such nodes are called leaves.

11.11. Forests & Trees
377
Figure 11.18
A 6-node forest consisting of 2 component trees.
a
b
d
c
e
h
i
f
g
Figure 11.19
A 9-node tree with 5 leaves.
DeÔ¨Ånition 11.11.2. A degree 1 node in a forest is called a leaf.
The forest in Figure 11.18 has 4 leaves. The tree in Figure 11.19 has 5 leaves.
Trees are a fundamental data structure in computer science. For example, in-
formation is often stored in tree-like data structures and the execution of many
recursive programs can be modeled as the traversal of a tree. In such cases, it is
often useful to arrange the nodes in levels, where the node at the top level is iden-
tiÔ¨Åed as the root and where every edge joins a parent to a child one level below.
Figure 11.20 shows the tree of Figure 11.19 redrawn in this way. Node d is a child
of node e and the parent of nodes b and c.
11.11.2
Properties
Trees have many unique properties. We have listed some of them in the following
theorem.
Theorem 11.11.3. Every tree has the following properties:
1. Every connected subgraph is a tree.
2. There is a unique path between every pair of vertices.
3. Adding an edge between nonadjacent nodes in a tree creates a graph with a
cycle.
4. Removing any edge disconnects the graph. That is, every edge is a cut edge.
5. If the tree has at least two vertices, then it has at least two leaves.

Chapter 11
Simple Graphs
378
a
b
c
f
h
i
e
d
g
Figure 11.20
The tree from Figure 11.19 redrawn with node e as the root and the
other nodes arranged in levels.
6. The number of vertices in a tree is one larger than the number of edges.
Proof.
1. A cycle in a subgraph is also a cycle in the whole graph, so any sub-
graph of an acyclic graph must also be acyclic. If the subgraph is also con-
nected, then by deÔ¨Ånition, it is a tree.
2. Since a tree is connected, there is at least one path between every pair of ver-
tices. Suppose for the purposes of contradiction, that there are two different
paths between some pair of vertices. Then there are two distinct paths p ¬§ q
between the same two vertices with minimum total length jpj C jqj. If these
paths shared a vertex, w, other than at the start and end of the paths, then
the parts of p and q from start to w, or the parts of p and q from w to the
end, must be distinct paths between the same vertices with total length less
than jpj C jqj, contradicting the minimality of this sum. Therefore, p and q
have no vertices in common besides their endpoints, and so pbreverse.q/ is
a cycle.
3. An additional edge hu‚Äîvi together with the unique path between u and v
forms a cycle.
4. Suppose that we remove edge hu‚Äîvi. Since the tree contained a unique path
between u and v, that path must have been hu‚Äîvi. Therefore, when that
edge is removed, no path remains, and so the graph is not connected.
5. Since the tree has at least two vertices, the longest path in the tree will have
different endpoints u and v. We claim u is a leaf. This follows because,
since by deÔ¨Ånition of endpoint, u is incident to at most one edge on the path.

11.11. Forests & Trees
379
Figure 11.21
A graph where the edges of a spanning tree have been thickened.
Also, if u was incident to an edge not on the path, then the path could be
lengthened by adding that edge, contradicting the fact that the path was as
long as possible. It follows that u is incident only to a single edge, that is u
is a leaf. The same hold for v.
6. We use induction on the proposition
P.n/ WWD there are n   1 edges in any n-vertex tree:
Base case (n D 1): P.1/ is true since a tree with 1 node has 0 edges and
1   1 D 0.
Inductive step: Now suppose that P.n/ is true and consider an .nC1/-vertex
tree, T . Let v be a leaf of the tree. You can verify that deleting a vertex of
degree 1 (and its incident edge) from any connected graph leaves a connected
subgraph. So by Theorem 11.11.3.1, deleting v and its incident edge gives
a smaller tree, and this smaller tree has n   1 edges by induction. If we re-
attach the vertex, v, and its incident edge, we Ô¨Ånd that T has n D .nC1/ 1
edges. Hence, P.n C 1/ is true, and the induction proof is complete.

Various subsets of properties in Theorem 11.11.3 provide alternative characteri-
zations of trees. For example,
Lemma 11.11.4. A graph G is a tree iff G is a forest and jV.G/j D jE.G/j C 1.
The proof is an easy consequence of Theorem 11.9.6.6.
11.11.3
Spanning Trees
Trees are everywhere. In fact, every connected graph contains a subgraph that is a
tree with the same vertices as the graph. This is called a spanning tree for the graph.
For example, Figure 11.21 is a connected graph with a spanning tree highlighted.

Chapter 11
Simple Graphs
380
DeÔ¨Ånition 11.11.5. DeÔ¨Åne a spanning subgraph of a graph, G, to be a subgraph
containing all the vertices of G.
Theorem 11.11.6. Every connected graph contains a spanning tree.
Proof. Suppose G is a connected graph, so the graph G itself is a connected, span-
ning subgraph. So by WOP, G must have a minimum-edge connected, spanning
subgraph, T . We claim T is a spanning tree. Since T is a connected, spanning
subgraph by deÔ¨Ånition, all we have to show is that T is acyclic.
But suppose to the contrary that T contained a cycle C. By Lemma 11.9.5,
an edge e of C will not be a cut edge, so removing it would leave a connected,
spanning subgraph that was smaller than T , contradicting the minimality to T .

11.11.4
Minimum Weight Spanning Trees
Spanning trees are interesting because they connect all the nodes of a graph using
the smallest possible number of edges. For example the spanning tree for the 6-
node graph shown in Figure 11.21 has 5 edges.
Spanning trees are very useful in practice, but in the real world, not all span-
ning trees are equally desirable. That‚Äôs because, in practice, there are often costs
associated with the edges of the graph.
For example, suppose the nodes of a graph represent buildings or towns and
edges represent connections between buildings or towns. The cost to actually make
a connection may vary a lot from one pair of buildings or towns to another. The
cost might depend on distance or topography. For example, the cost to connect LA
to NY might be much higher than that to connect NY to Boston. Or the cost of a
pipe through Manhattan might be more than the cost of a pipe through a cornÔ¨Åeld.
In any case, we typically represent the cost to connect pairs of nodes with a
weighted edge, where the weight of the edge is its cost. The weight of a spanning
tree is then just the sum of the weights of the edges in the tree. For example, the
weight of the spanning tree shown in Figure 11.22 is 19.
The goal, of course, is to Ô¨Ånd the spanning tree with minimum weight, called the
minimum weight spanning tree (MST for short).
DeÔ¨Ånition 11.11.7. A minimum weight spanning tree (MST) of an edge-weighted
graph G is a spanning tree of G with the smallest possible sum of edge weights.
Is the spanning tree shown in Figure 11.22(a) an MST of the weighted graph
shown in Figure 11.22(b)? Actually, it is not, since the tree shown in Figure 11.23
is also a spanning tree of the graph shown in Figure 11.22(b), and this spanning
tree has weight 17.

11.11. Forests & Trees
381
2
3
3
7
1
2
1
(a)
2
3
3
7
1
2
1
1
4
1
3
(b)
Figure 11.22
A spanning tree (a) with weight 19 for a graph (b).
2
3
7
1
2
1
1
Figure 11.23
An MST with weight 17 for the graph in Figure 11.22(b).

Chapter 11
Simple Graphs
382
What about the tree shown in Figure 11.23? Is it an MST? It seems to be, but
how do we prove it? In general, how do we Ô¨Ånd an MST for a connected graph G?
We could try enumerating all subtrees of G, but that approach would be hopeless
for large graphs.
There actually are many good ways to Ô¨Ånd MST‚Äôs based on an invariance prop-
erty of some subgraphs of G called pre-MST‚Äôs.
DeÔ¨Ånition 11.11.8. A pre-MST for a graph G is a spanning subgraph of G that is
also a subgraph of some MST of G.
So a pre-MST will necessarily be a forest.
For example, the empty graph with the same vertices as G is guaranteed to be a
pre-MST of G, and so is any actual MST of G.
If e is an edge of G and S is a spanning subgraph, we‚Äôll write S C e for the
spanning subgraph with edges E.S/ [ feg.
DeÔ¨Ånition 11.11.9. If F is a pre-MST and e is a new edge, that is e 2 E.G/  E.F /, then e extends F when F C e is also a pre-MST.
So being a pre-MST is by deÔ¨Ånition an invariant under addition of extending
edges.
The standard methods for Ô¨Ånding MST‚Äôs all start with the empty spanning for-
est and build up to an MST by adding one extending edge after another. Since
the empty spanning forest is a pre-MST, and being a pre-MST is invariant under
extensions, every forest built in this way will be a pre-MST. But no spanning tree
can be a subgraph of a different spanning tree. So when the pre-MST Ô¨Ånally grows
enough to become a tree, it will be an MST. By Lemma 11.11.4, this happens after
exactly jV.G/j   1 edge extensions.
So the problem of Ô¨Ånding MST‚Äôs reduces to the question of how to tell if an edge
is an extending edge. Here‚Äôs how:
DeÔ¨Ånition 11.11.10. Let F be a pre-MST, and color the vertices in each connected
component of F either all black or all white. At least one component of each color
is required. Call this a solid coloring of F . A gray edge of a solid coloring is an
edge of G with different colored endpoints.
Any path in G from a white vertex to a black vertex obviously must include a
gray edge, so for any solid coloring, there is guaranteed to be at least one gray edge.
In fact, there will have to be at least as many gray edges as there are components
with the same color. Here‚Äôs the punchline:
Lemma 11.11.11. An edge extends a pre-MST F if it is a minimum weight gray
edge in some solid coloring of F .

11.11. Forests & Trees
383
2
7
1
2
1
1
3
Figure 11.24
A spanning tree found by Algorithm 1.
So to extend a pre-MST, choose any solid coloring, Ô¨Ånd the gray edges, and
among them choose one with minimum weight. Each of these steps is easy to do,
so it is easy to keep extending and arrive at an MST. For example, here are three
known algorithms that are explained by Lemma 11.11.11:
Algorithm 1. [Prim] Grow a tree one edge at a time by adding a minimum weight
edge among the edges that have exactly one endpoint in the tree.
This is the algorithm that comes from coloring the growing tree white and all the
vertices not in the tree black. Then the gray edges are the ones with exactly one
endpoint in the tree.
Algorithm 2. [Kruskal] Grow a forest one edge at a time by adding a minimum
weight edge among the edges with endpoints in different connected components.
An edge does not create a cycle iff it connects different components. The edge
chosen by Kruskal‚Äôs algorithm will be the minimum weight gray edge when the
components it connects are assigned different colors.
For example, in the weighted graph we have been considering, we might run
Algorithm 1 as follows. We would start by choosing one of the weight 1 edges,
since this is the smallest weight in the graph. Suppose we chose the weight 1 edge
on the bottom of the triangle of weight 1 edges in our graph. This edge is incident
to the same vertex as two weight 1 edges, a weight 4 edge, a weight 7 edge, and
a weight 3 edge. We would then choose the incident edge of minimum weight. In
this case, one of the two weight 1 edges. At this point, we cannot choose the third
weight 1 edge: it won‚Äôt be gray because its endpoints are both in the tree, and so
are both colored white. But we can continue by choosing a weight 2 edge. We
might end up with the spanning tree shown in Figure 11.24, which has weight 17,
the smallest we‚Äôve seen so far.

Chapter 11
Simple Graphs
384
Now suppose we instead ran Algorithm 2 on our graph. We might again choose
the weight 1 edge on the bottom of the triangle of weight 1 edges in our graph.
Now, instead of choosing one of the weight 1 edges it touches, we might choose
the weight 1 edge on the top of the graph. This edge still has minimum weight, and
will be gray if we simply color its endpoints differently, so Algorithm 2 can choose
it. We would then choose one of the remaining weight 1 edges. Note that neither
causes us to form a cycle. Continuing the algorithm, we could end up with the same
spanning tree in Figure 11.24, though this will depend on how the tie breaking rules
used to choose among gray edges with the same minimum weight. For example, if
the weight of every edge in G is one, then all spanning trees are MST‚Äôs with weight
jV.G/j   1, and both of these algorithms can arrive at each of these spanning trees
by suitable tie-breaking.
The coloring that explains Algorithm 1 also justiÔ¨Åes a more Ô¨Çexible algorithm
which has Algorithm 1 as a special case:
Algorithm 3. Grow a forest one edge at a time by picking any component and
adding a minimum weight edge among the edges leaving that component.
This algorithm allows components that are not too close to grow in parallel and
independently, which is great for ‚Äúdistributed‚Äù computation where separate proces-
sors share the work with limited communication between processors.
These are examples of greedy approaches to optimization. Sometimes greediness
works and sometimes it doesn‚Äôt. The good news is that it does work to Ô¨Ånd the
MST. So we can be sure that the MST for our example graph has weight 17 since it
was produced by Algorithm 2. And we have a fast algorithm for Ô¨Ånding a minimum
weight spanning tree for any graph.
Ok, to wrap up this story, all that‚Äôs left is the proof that minimal gray edges are
extending edges. This might sound like a chore, but it just uses the same reasoning
we used to be sure there would be a gray edge when you need it.
Proof. (of Lemma 11.11.11)
Let F be a pre-MST that is a subgraph of some MST M of G, and suppose e is a
minimum weight gray edge under some solid coloring of F . We want to show that
F C e is also a pre-MST.
If e happens to be an edge of M, then F C e remains a subgraph of M, and so
is a pre-MST.
The other case is when e is not an edge of M. In that case, M C e will be a
connected, spanning subgraph. Also M has a path p between the different colored
endpoints of e, so M C e has a cycle consisting of e together with p. Now p has
both a black endpoint and a white one, so it must contain some gray edge g ¬§ e.
The trick is to remove g from M C e to obtain a subgraph M C e   g. Since gray

11.11. Forests & Trees
385
edges by deÔ¨Ånition are not edges of F , the graph M C e   g contains F C e. We
claim that M C e   g is an MST, which proves the claim that e extends F .
To prove this claim, note that M Ce is a connected, spanning subgraph, and g is
on a cycle of M C e, so by Lemma 11.9.5, removing g won‚Äôt disconnect anything.
Therefore, M Ce g is still a connected, spanning subgraph. Moreover, M Ce g
has the same number of edges as M, so Lemma 11.11.4 implies that it must be a
spanning tree. Finally, since e is minimum weight among gray edges,
w.M C e   g/ D w.M/ C w.e/   w.g/  w.M/:
This means that M C e   g is a spanning tree whose weight is at most that of an
MST, which implies that M C e   g is also an MST.

Another interesting fact falls out of the proof of Lemma 11.11.11:
Corollary 11.11.12. If all edges in a weighted graph have distinct weights, then
the graph has a unique MST.
The proof of Corollary 11.11.12 is left to Problem 11.45.
Problems for Section 11.2
Class Problems
Problem 11.1. (a) Prove that in every simple graph, there are an even number of
vertices of odd degree.
Hint: The Handshaking Lemma 11.2.1.
(b) Conclude that at a party where some people shake hands, the number of people
who shake hands an odd number of times is an even number.
(c) Call a sequence of two or more different people at the party a handshake se-
quence if each person in the sequence has shaken hands with the next person, if
any, in the sequence.
Suppose George was at the party and has shaken hands with an odd number of
people. Explain why, starting with George, there must be a handshake sequence
ending with a different person who has shaken an odd number of hands.
Hint: Just look at all the people who appear in handshake sequences that start with
George.

Chapter 11
Simple Graphs
386
Exam Problems
Problem 11.2.
A researcher analyzing data on heterosexual sexual behavior in a group of m males
and f females found that within the group, the male average number of female
partners was 10% larger that the female average number of male partners.
(a) Comment on the following claim. ‚ÄúSince we‚Äôre assuming that each encounter
involves one man and one woman, the average numbers should be the same, so the
males must be exaggerating.‚Äù
(b) For what constant c is m D c  f ?
(c) The data shows that approximately 20% of the females were virgins, while
only 5% of the males were. The researcher wonders how excluding virgins from
the population would change the averages. If he knew graph theory, the researcher
would realize that the nonvirgin male average number of partners will be x.f=m/
times the nonvirgin female average number of partners. What is x?
(d) For purposes of further research, it would be helpful to pair each female in the
group with a unique male in the group. Explain why this is not possible.
Problems for Section 11.4
Class Problems
Problem 11.3.
For each of the following pairs of graphs, either deÔ¨Åne an isomorphism between
them, or prove that there is none. (We write ab as shorthand for ha‚Äîbi.)
(a)
G1 with V1 D f1; 2; 3; 4; 5; 6g; E1 D f12; 23; 34; 14; 15; 35; 45g
G2 with V2 D f1; 2; 3; 4; 5; 6g; E2 D f12; 23; 34; 45; 51; 24; 25g
(b)
G3 with V3 D f1; 2; 3; 4; 5; 6g; E3 D f12; 23; 34; 14; 45; 56; 26g
G4 with V4 D fa; b; c; d; e; f g; E4 D fab; bc; cd; de; ae; ef; cf g
Homework Problems
Problem 11.4.
Determine which among the four graphs pictured in the Figure 11.25 are isomor-
phic. If two of these graphs are isomorphic, describe an isomorphism between

11.11. Forests & Trees
387
1
2
3
4
5
6
7
8
9
10
(a) G1
1
2
3
4
5
6
8
9
7
10
(b) G2
1
2
3
4
5
6
8
9
7
10
(c) G3
1
2
3
4
5
6
8
9
7
10
(d) G4
Figure 11.25
Which graphs are isomorphic?
them. If they are not, give a property that is preserved under isomorphism such that
one graph has the property, but the other does not. For at least one of the properties
you choose, prove that it is indeed preserved under isomorphism (you only need
prove one of them).
Problem 11.5. (a) For any vertex, v, in a graph, let E.v/ be the set of neighbors
of v, namely, the vertices adjacent to v:
E.v/ WWD fu j hu‚Äîvi is an edge of the graphg:
Suppose f is an isomorphism from graph G to graph H. Prove that f .E.v// D
E.f .v//.

Chapter 11
Simple Graphs
388
Your proof should follow by simple reasoning using the deÔ¨Ånitions of isomorphism
and neighbors ‚Äîno pictures or handwaving.
Hint: Prove by a chain of iff‚Äôs that
h 2 E.f .v//
iff
h 2 f .E.v//
for every h 2 VH. Use the fact that h D f .u/ for some u 2 VG.
(b) Conclude that if G and H are isomorphic graphs, then for each k 2 N, they
have the same number of degree k vertices.
Problem 11.6.
Let‚Äôs say that a graph has ‚Äútwo ends‚Äù if it has exactly two vertices of degree 1 and
all its other vertices have degree 2. For example, here is one such graph:
(a) A line graph is a graph whose vertices can be listed in a sequence with edges
between consecutive vertices only. So the two-ended graph above is also a line
graph of length 4.
Prove that the following theorem is false by drawing a counterexample.
False Theorem. Every two-ended graph is a line graph.
(b) Point out the Ô¨Årst erroneous statement in the following bogus proof of the false
theorem and describe the error.
Bogus proof. We use induction. The induction hypothesis is that every two-ended
graph with n edges is a path.
Base case (n D 1): The only two-ended graph with a single edge consists of two
vertices joined by an edge:
Sure enough, this is a line graph.
Inductive case: We assume that the induction hypothesis holds for some n  1
and prove that it holds for n C 1. Let Gn be any two-ended graph with n edges.
By the induction assumption, Gn is a line graph. Now suppose that we create a

11.11. Forests & Trees
389
two-ended graph GnC1 by adding one more edge to Gn. This can be done in only
one way: the new edge must join an endpoint of Gn to a new vertex; otherwise,
GnC1 would not be two-ended.
gn
new edge
‚Üë
Clearly, GnC1 is also a line graph. Therefore, the induction hypothesis holds for
all graphs with n C 1 edges, which completes the proof by induction.

Exam Problems
Problem 11.7.
There are four isomorphisms between the two graphs give in Figure 11.26. List
them.
1
2
6
3
4
5
a
b
f
c
d
e
Figure 11.26
Graphs with several isomorphisms
Problems for Section 11.5
Class Problems
Problem 11.8.
A certain Institute of Technology has a lot of student clubs; these are loosely over-
seen by the Student Association. Each eligible club would like to delegate one of its
members to appeal to the Dean for funding, but the Dean will not allow a student to

Chapter 11
Simple Graphs
390
be the delegate of more than one club. Fortunately, the Association VP took Math
for Computer Science and recognizes a matching problem when she sees one.
(a) Explain how to model the delegate selection problem as a bipartite matching
problem.
(b) The VP‚Äôs records show that no student is a member of more than 9 clubs. The
VP also knows that to be eligible for support from the Dean‚Äôs ofÔ¨Åce, a club must
have at least 13 members. That‚Äôs enough for her to guarantee there is a proper
delegate selection. Explain. (If only the VP had taken an Algorithms, she could
even have found a delegate selection without much effort.)
Problem 11.9.
A Latin square is n  n array whose entries are the number 1; : : : ; n. These en-
tries satisfy two constraints: every row contains all n integers in some order, and
also every column contains all n integers in some order. Latin squares come up
frequently in the design of scientiÔ¨Åc experiments for reasons illustrated by a little
story in a footnote10
10At Guinness brewery in the eary 1900‚Äôs, W. S. Gosset (a chemist) and E. S. Beavan (a ‚Äúmaltster‚Äù)
were trying to improve the barley used to make the brew. The brewery used different varieties of
barley according to price and availability, and their agricultural consultants suggested a different
fertilizer mix and best planting month for each variety.
Somewhat sceptical about paying high prices for customized fertilizer, Gosset and Beavan planned
a season long test of the inÔ¨Çuence of fertilizer and planting month on barley yields. For as many
months as there were varieties of barley, they would plant one sample of each variety using a different
one of the fertilizers. So every month, they would have all the barley varieties planted and all the
fertilizers used, which would give them a way to judge the overall quality of that planting month.
But they also wanted to judge the fertilizers, so they wanted each fertilizer to be used on each variety
during the course of the season. Now they had a little mathematical problem, which we can abstract
as follows.
Suppose there are n barley varieties and an equal number of recommended fertilizers. Form an
n  n array with a column for each fertilizer and a row for each planting month. We want to Ô¨Åll in
the entries of this array with the integers 1,. . . ,n numbering the barley varieties, so that every row
contains all n integers in some order (so every month each variety is planted and each fertilizer is
used), and also every column contains all n integers (so each fertilizer is used on all the varieties over
the course of the growing season).

11.11. Forests & Trees
391
For example, here is a 4  4 Latin square:
1 2 3 4
3 4 2 1
2 1 4 3
4 3 1 2
(a) Here are three rows of what could be part of a 5  5 Latin square:
2 4 5 3 1
4 1 3 2 5
3 2 1 5 4
Fill in the last two rows to extend this ‚ÄúLatin rectangle‚Äù to a complete Latin square.
(b) Show that Ô¨Ålling in the next row of an n  n Latin rectangle is equivalent to
Ô¨Ånding a matching in some 2n-vertex bipartite graph.
(c) Prove that a matching must exist in this bipartite graph and, consequently, a
Latin rectangle can always be extended to a Latin square.
Exam Problems
Problem 11.10.
Overworked and over-caffeinated, the Teaching Assistant‚Äôs (TA‚Äôs) decide to oust
the lecturer and teach their own recitations. They will run a recitation session at 4
different times in the same room. There are exactly 20 chairs to which a student
can be assigned in each recitation. Each student has provided the TA‚Äôs with a list of
the recitation sessions her schedule allows and no student‚Äôs schedule conÔ¨Çicts with
all 4 sessions. The TA‚Äôs must assign each student to a chair during recitation at a
time she can attend, if such an assignment is possible.
Describe how to model this situation as a matching problem. Be sure to spec-
ify what the vertices/edges should be and brieÔ¨Çy describe how a matching would
determine seat assignments for each student in a recitation that does not conÔ¨Çict
with his schedule. This is a modeling problem ‚Äîyou need not determine whether
a match is always possible.

Chapter 11
Simple Graphs
392
Problem 11.11.
Because of the incredible popularity of Math for Computer Science, Rajeev decides
to give up on regular ofÔ¨Åce hours. Instead, each student can join some study groups.
Each group must choose a representative to talk to the staff, but there is a staff rule
that a student can only represent one group. The problem is to Ô¨Ånd a representative
from each group while obeying the staff rule.
(a) Explain how to model the delegate selection problem as a bipartite matching
problem.
(b) The staff‚Äôs records show that no student is a member of more than 4 groups,
and all the groups must have at least 4 members. That‚Äôs enough to guarantee there
is a proper delegate selection. Explain.
Homework Problems
Problem 11.12.
Take a regular deck of 52 cards. Each card has a suit and a value. The suit is one of
four possibilities: heart, diamond, club, spade. The value is one of 13 possibilities,
A; 2; 3; : : : ; 10; J; Q; K. There is exactly one card for each of the 4  13 possible
combinations of suit and value.
Ask your friend to lay the cards out into a grid with 4 rows and 13 columns.
They can Ô¨Åll the cards in any way they‚Äôd like. In this problem you will show that
you can always pick out 13 cards, one from each column of the grid, so that you
wind up with cards of all 13 possible values.
(a) Explain how to model this trick as a bipartite matching problem between the
13 column vertices and the 13 value vertices. Is the graph necessarily degree-
constrained?
(b) Show that any n columns must contain at least n different values and prove
that a matching must exist.
Problem 11.13.
Scholars through the ages have identiÔ¨Åed twenty fundamental human virtues: hon-
esty, generosity, loyalty, prudence, completing the weekly course reading-response,
etc. At the beginning of the term, every student in Math for Computer Science pos-
sessed exactly eight of these virtues. Furthermore, every student was unique; that
is, no two students possessed exactly the same set of virtues. The Math for Com-
puter Science course staff must select one additional virtue to impart to each student
by the end of the term. Prove that there is a way to select an additional virtue for

11.11. Forests & Trees
393
each student so that every student is unique at the end of the term as well.
Suggestion: Use Hall‚Äôs theorem. Try various interpretations for the vertices on
the left and right sides of your bipartite graph.
Problems for Section 11.6
Practice Problems
Problem 11.14.
Four Students want separate assignments to four VI-A Companies. Here are their
preference rankings:
Student
Companies
Albert:
HP, Bellcore, AT&T, Draper
Nick:
AT&T, Bellcore, Draper, HP
Oshani:
HP, Draper, AT&T, Bellcore
Ali:
Draper, AT&T, Bellcore, HP
Company
Students
AT&T:
Ali, Albert, Oshani, Nick
Bellcore:
Oshani, Nick, Albert, Ali
HP:
Ali, Oshani, Albert, Nick
Draper:
Nick, Ali, Oshani, Albert
(a) Use the Mating Ritual to Ô¨Ånd two stable assignments of Students to Compa-
nies.
(b) Describe a simple procedure to determine whether any given stable marriage
problem has a unique solution, that is, only one possible stable matching.
Problem 11.15.
We are interested in invariants of the Mating Ritual (Section 11.6) for Ô¨Ånding stable
marriages. Let Angelina and Jen be two of the girls, and Keith and Tom be two of
the boys.
Which of the following predicates are invariants of the Mating Ritual no matter
what the preferences are among the boys and girls? (Remember that a predicate
that is always false is an invariant‚Äîcheck the deÔ¨Ånition of invariant to see why.)
(a) Angelina is crossed off Tom‚Äôs list and she has a suitor that she prefers to Tom.
(b) Tom is serenading Jen.
(c) Tom is not serenading Jen.

Chapter 11
Simple Graphs
394
(d) Tom‚Äôs list of girls to serenade is empty.
(e) All the boys have the same number of girls left uncrossed in their lists.
(f) Jen is crossed off Keith‚Äôs list.
(g) Jen is crossed off Keith‚Äôs list and Keith prefers Jen to anyone he is serenading.
(h) Jen is the only girl on Keith‚Äôs list.
Class Problems
Problem 11.16.
Consider a stable marriage problem with 4 boys and 4 girls and the following partial
information about their preferences:
B1:
G1
G2
‚Äì
‚Äì
B2:
G2
G1
‚Äì
‚Äì
B3:
‚Äì
‚Äì
G4
G3
B4:
‚Äì
‚Äì
G3
G4
G1:
B2
B1
‚Äì
‚Äì
G2:
B1
B2
‚Äì
‚Äì
G3:
‚Äì
‚Äì
B3
B4
G4:
‚Äì
‚Äì
B4
B3
(a) Verify that
.B1; G1/; .B2; G2/; .B3; G3/; .B4; G4/
will be a stable matching whatever the unspeciÔ¨Åed preferences may be.
(b) Explain why the stable matching above is neither boy-optimal nor boy-pessimal
and so will not be an outcome of the Mating Ritual.
(c) Describe how to deÔ¨Åne a set of marriage preferences among n boys and n girls
which have at least 2n=2 stable assignments.
Hint: Arrange the boys into a list of n=2 pairs, and likewise arrange the girls into
a list of n=2 pairs of girls. Choose preferences so that the kth pair of boys ranks
the kth pair of girls just below the previous pairs of girls, and likewise for the kth
pair of girls. Within the kth pairs, make sure each boy‚Äôs Ô¨Årst choice girl in the pair
prefers the other boy in the pair.

11.11. Forests & Trees
395
Problem 11.17.
Suppose there are more boys than girls.
(a) DeÔ¨Åne what a stable matching should mean in this case.
(b) Explain why applying the Mating Ritual in this case will yield a stable match-
ing in which every girl is married.
Homework Problems
Problem 11.18.
The most famous application of stable matching was in assigning graduating med-
ical students to hospital residencies. Each hospital has a preference ranking of
students and each student has a preference order of hospitals, but unlike the setup
in the notes where there are an equal number of boys and girls and monogamous
marriages, hospitals generally have differing numbers of available residencies, and
the total number of residencies may not equal the number of graduating students.
Modify the deÔ¨Ånition of stable matching so it applies in this situation, and explain
how to modify the Mating Ritual so it yields stable assignments of students to resi-
dencies.
BrieÔ¨Çy indicate what, if any, modiÔ¨Åcations of the preserved invariant used to
verify the original Mating are needed to verify this one for hospitals and students.
Problem 11.19.
Give an example of a stable matching between 3 boys and 3 girls where no person
gets their Ô¨Årst choice. BrieÔ¨Çy explain why your matching is stable. Can your
matching be obtained from the Mating Ritual or the Ritual with boys and girls
reversed.?
Problem 11.20.
In a stable matching between n boys and girls produced by the Mating Ritual, call
a person lucky if they are matched up with one of their dn=2e top choices. We will
prove:
Theorem. There must be at least one lucky person.
To prove this, deÔ¨Åne the following derived variables for the Mating Ritual:
q.B/ D j, where j is the rank of the girl that boy B is courting. That is to say,
boy B is always courting the jth girl on his list.

Chapter 11
Simple Graphs
396
r.G/ is the number of boys that girl G has rejected.
(a) Let
S WWD
X
B2Boys
q.B/  X
G2Girls
r.G/:
(11.4)
Show that S remains the same from one day to the next in the Mating Ritual.
(b) Prove the Theorem above. (You may assume for simplicity that n is even.)
Hint: A girl is sure to be lucky if she has rejected half the boys.
Exam Problems
Problem 11.21.
Four unfortunate children want to be adopted by four foster families of ill repute.
A child can only be adopted by one family, and a family can only adopt one child.
Here are their preference rankings (most-favored to least-favored):
Child
Families
Bottlecap:
HatÔ¨Åelds, McCoys, Grinches, Scrooges
Lucy:
Grinches, Scrooges, McCoys, HatÔ¨Åelds
Dingdong:
HatÔ¨Åelds, Scrooges, Grinches, McCoys
Zippy:
McCoys, Grinches, Scrooges, HatÔ¨Åelds
Family
Children
Grinches:
Zippy, Dingdong, Bottlecap, Lucy
HatÔ¨Åelds:
Zippy, Bottlecap, Dingdong, Lucy
Scrooges:
Bottlecap, Lucy, Dingdong, Zippy
McCoys:
Lucy, Zippy, Bottlecap, Dingdong
(a) Exhibit two different stable matching of Children and Families.
Family
Child in 1st match
Child in 2nd match
Grinches:
HatÔ¨Åelds:
Scrooges:
McCoys:
(b) Explain why the matchings of part (a) are the only two possible stable match-
ings between Children and Families.

11.11. Forests & Trees
397
Problems for Section 11.7
Class Problems
Problem 11.22.
Let G be the graph below11. Carefully explain why .G/ D 4.
Homework Problems
Problem 11.23.
6.042 is often taught using recitations. Suppose it happened that 8 recitations were
needed, with two or three staff members running each recitation. The assignment
of staff to recitation sections, using their secret codenames, is as follows:
 R1: Maverick, Goose, Iceman
 R2: Maverick, Stinger, Viper
 R3: Goose, Merlin
 R4: Slider, Stinger, Cougar
 R5: Slider, Jester, Viper
 R6: Jester, Merlin
 R7: Jester, Stinger
 R8: Goose, Merlin, Viper
Two recitations can not be held in the same 90-minute time slot if some staff
member is assigned to both recitations. The problem is to determine the minimum
number of time slots required to complete all the recitations.
11From Discrete Mathematics, Lov¬¥asz, Pelikan, and Vesztergombi. Springer, 2003. Exercise
13.3.1

Chapter 11
Simple Graphs
398
(a) Recast this problem as a question about coloring the vertices of a particular
graph. Draw the graph and explain what the vertices, edges, and colors represent.
(b) Show a coloring of this graph using the fewest possible colors. What schedule
of recitations does this imply?
Problem 11.24.
This problem generalizes the result proved Theorem 11.7.3 that any graph with
maximum degree at most w is .w C 1/-colorable.
A simple graph, G, is said to have width, w, iff its vertices can be arranged in a
sequence such that each vertex is adjacent to at most w vertices that precede it in
the sequence. If the degree of every vertex is at most w, then the graph obviously
has width at most w ‚Äîjust list the vertices in any order.
(a) Describe an example of a graph with 100 vertices, width 3, but average degree
more than 5. Hint: Don‚Äôt get stuck on this; if you don‚Äôt see it after Ô¨Åve minutes,
ask for a hint.
(b) Prove that every graph with width at most w is .w C 1/-colorable.
(c) Prove that the average degree of a graph of width w is at most 2w.
Problem 11.25.
This problem will show that 3-coloring a graph is just as difÔ¨Åcult as Ô¨Ånding a sat-
isfying truth assignment for a propositional formula. The graphs considered will
all be taken to have three designated color-vertices connected in a triangle to force
them to have different colors in any coloring of the graph. The colors assigned to
the color-vertices will be called T; F and N .
Suppose f is an n-argument truth function. That is,
f W fT; F gn ! fT; F g:
A graph G is called a 3-color-f-gate iff G has n designated input vertices and a
designated output vertex, such that
 G can be 3-colored only if its input vertices are colored with T ‚Äôs and F ‚Äôs.
 For every sequence b1; b2; : : : ; bn 2 fT; F g, there is a 3-coloring of G in
which the input vertices v1; v2; : : : ; vn 2 V.G/ have the colors b1; b2; : : : ; bn 2
fT; F g.

11.11. Forests & Trees
399
[h]
T	 ¬†
F	 ¬†
N	 ¬†
P 
NOT(P) 
Figure 11.27
A 3-color NOT-gate
 In any 3-coloring of G where the input vertices v1; v2; : : : ; vn 2 V.G/ have
colors b1; b2; : : : ; bn 2 fT; F g, the output vertex has color f .b1; b2; : : : ; bn/.
For example, a 3-color-NOT-gate consists simply of two adjacent vertices. One
vertex is designated to be the input vertex, P , and the other is designated to be
the output vertex. Both vertices have to be constrained so they can only be colored
with T ‚Äôs or F ‚Äôs in any proper 3-coloring. This constraint can be imposed by making
them adjacent to the color-vertex N, as shown in Figure 11.27.
(a) Verify that the graph in Figure 11.28 is a 3-color-OR-gate. (The dotted lines
indicate edges to color-vertex N; these edges constrain the P , Q and P OR Q
vertices to be colored T or F in any proper 3-coloring.)
(b) Let E be an n-variable propositional formula, and suppose E deÔ¨Ånes a truth
function f W fT; F gn ! fT; F g. Explain a simple way to construct a graph that is
a 3-color-f-gate.
(c) Explain why an efÔ¨Åcient procedure for determining if a graph was 3-colorable
would lead to an efÔ¨Åcient procedure to solve the satisÔ¨Åability problem, SAT.

Chapter 11
Simple Graphs
400
[h]
P OR Q 
P 
Q 
N 
F 
T 
Figure 11.28
A 3-color OR-gate
Exam Problems
Problem 11.26.
False Claim. Let G be a graph whose vertex degrees are all  k. If G has a vertex
of degree strictly less than k, then G is k-colorable.
(a) Give a counterexample to the False Claim when k D 2.
(b) Underline the exact sentence or part of a sentence that is the Ô¨Årst unjustiÔ¨Åed
step in the following bogus proof of the False Claim.
Bogus proof. Proof by induction on the number n of vertices:
The induction hypothesis, P.n/ is:
Let G be an n-vertex graph whose vertex degrees are all  k. If G
also has a vertex of degree strictly less than k, then G is k-colorable.
Base case: (n D 1) G has one vertex, the degree of which is 0. Since G is
1-colorable, P.1/ holds.
Inductive step: We may assume P.n/. To prove P.n C 1/, let GnC1 be
a graph with n C 1 vertices whose vertex degrees are all k or less. Also,

11.11. Forests & Trees
401
suppose GnC1 has a vertex, v, of degree strictly less than k. Now we only
need to prove that GnC1 is k-colorable.
To do this, Ô¨Årst remove the vertex v to produce a graph, Gn, with n vertices.
Let u be a vertex that is adjacent to v in GnC1. Removing v reduces the
degree of u by 1. So in Gn, vertex u has degree strictly less than k. Since no
edges were added, the vertex degrees of Gn remain  k. So Gn satisÔ¨Åes the
conditions of the induction hypothesis, P.n/, and so we conclude that Gn is
k-colorable.
Now a k-coloring of Gn gives a coloring of all the vertices of GnC1, except for
v. Since v has degree less than k, there will be fewer than k colors assigned
to the nodes adjacent to v. So among the k possible colors, there will be a
color not used to color these adjacent nodes, and this color can be assigned to
v to form a k-coloring of GnC1.

(c) With a slightly strengthened condition, the preceding proof of the False Claim
could be revised into a sound proof of the following Claim:
Claim. Let G be a graph whose vertex degrees are all  k. If hstatement inserted from belowi
has a vertex of degree strictly less than k, then G is k-colorable.
Circle each of the statements below that could be inserted to make the proof correct.
 G is connected and
 G has no vertex of degree zero and
 G does not contain a complete graph on k vertices and
 every connected component of G
 some connected component of G
Problems for Section 11.9
Class Problems
Problem 11.27.
The n-dimensional hypercube, Hn, is a graph whose vertices are the binary strings
of length n. Two vertices are adjacent if and only if they differ in exactly 1 bit. For
example, in H3, vertices 111 and 011 are adjacent because they differ only in the
Ô¨Årst bit, while vertices 101 and 011 are not adjacent because they differ at both
the Ô¨Årst and second bits.
(a) Prove that it is impossible to Ô¨Ånd two spanning trees of H3 that do not share
some edge.

Chapter 11
Simple Graphs
402
(b) Verify that for any two vertices x ¬§ y of H3, there are 3 paths from x to y in
H3, such that, besides x and y, no two of those paths have a vertex in common.
(c) Conclude that the connectivity of H3 is 3.
(d) Try extending your reasoning to H4. (In fact, the connectivity of Hn is n for
all n  1. A proof appears in the problem solution.)
Problem 11.28.
A set, M, of vertices of a graph is a maximal connected set if every pair of vertices
in the set are connected, and any set of vertices properly containing M will contain
two vertices that are not connected.
(a) What are the maximal connected subsets of the following (unconnected) graph?
(b) Explain the connection between maximal connected sets and connected com-
ponents. Prove it.
Problem 11.29. (a) Prove that Kn is .n   1/-edge connected for n > 1.
Let Mn be a graph deÔ¨Åned as follows: begin by taking n graphs with non-
overlapping sets of vertices, where each of the n graphs is .n   1/-edge connected
(they could be disjoint copies of Kn, for example). These will be subgraphs of Mn.
Then pick n vertices, one from each subgraph, and add enough edges between pairs
of picked vertices that the subgraph of the n picked vertices is also .n   1/-edge
connected.
(b) Draw a picture of M4.

11.11. Forests & Trees
403
(c) Explain why Mn is .n   1/-edge connected.
Problem 11.30.
False Claim. If every vertex in a graph has positive degree, then the graph is
connected.
(a) Prove that this Claim is indeed false by providing a counterexample.
(b) Since the Claim is false, there must be an logical mistake in the following
bogus proof. Pinpoint the Ô¨Årst logical mistake (unjustiÔ¨Åed step) in the proof.
Bogus proof. We prove the Claim above by induction. Let P.n/ be the proposition
that if every vertex in an n-vertex graph has positive degree, then the graph is
connected.
Base cases: (n  2). In a graph with 1 vertex, that vertex cannot have positive
degree, so P.1/ holds vacuously.
P.2/ holds because there is only one graph with two vertices of positive degree,
namely, the graph with an edge between the vertices, and this graph is connected.
Inductive step: We must show that P.n/ implies P.n C 1/ for all n  2. Consider
an n-vertex graph in which every vertex has positive degree. By the assumption
P.n/, this graph is connected; that is, there is a path between every pair of vertices.
Now we add one more vertex x to obtain an .n C 1/-vertex graph:
x
y
z
n-node 
connected 
graph
All that remains is to check that there is a path from x to every other vertex z. Since
x has positive degree, there is an edge from x to some other vertex, y. Thus, we
can obtain a path from x to z by going from x to y and then following the path
from y to z. This proves P.n C 1/.

Chapter 11
Simple Graphs
404
By the principle of induction, P.n/ is true for all n  0, which proves the Claim.

Homework Problems
Problem 11.31. (a) Give an example of a simple graph that has two vertices u ¬§ v
and two distinct paths between u and v, but no cycle including either u or v.
(b) Prove that if there are different paths between two vertices in a simple graph,
then the graph has a cycle.
Problem 11.32.
The entire Ô¨Åeld of graph theory began when Euler asked whether the seven bridges
of K¬®onigsberg could all be crossed exactly once. Abstractly, we can represent the
parts of the city separated by rivers as vertices and the bridges as edges between
the vertices. Then Euler‚Äôs question asks whether there is a closed walk through the
graph that includes every edge in a graph exactly once. In his honor, such a walk is
called an Euler tour.
So how do you tell in general whether a graph has an Euler tour? At Ô¨Årst glance
this may seem like a daunting problem. The similar sounding problem of Ô¨Ånding
a cycle that touches every vertex exactly once is one of those Millenium Prize NP-
complete problems known as the Traveling Salesman Problem). But it turns out to
be easy to characterize which graphs have Euler tours.
Theorem. A connected graph has an Euler tour if and only if every vertex has even
degree.
(a) Show that if a graph has an Euler tour, then the degree of each of its vertices
is even.
In the remaining parts, we‚Äôll work out the converse: if the degree of every vertex
of a connected Ô¨Ånite graph is even, then it has an Euler tour. To do this, let‚Äôs deÔ¨Åne
an Euler walk to be a walk that includes each edge at most once.
(b) Suppose that an Euler walk in a connected graph does not include every edge.
Explain why there must be an unincluded edge that is incident to a vertex on the
walk.
In the remaining parts, let w be the longest Euler walk in some Ô¨Ånite, connected
graph.
(c) Show that if w is a closed walk, then it must be an Euler tour.
Hint: part (b)

11.11. Forests & Trees
405
(d) Explain why all the edges incident to the end of w must already be in w.
(e) Show that if the end of w was not equal to the start of w, then the degree of
the end would be odd.
Hint: part (d)
(f) Conclude that if every vertex of a Ô¨Ånite, connected graph has even degree, then
it has an Euler tour.
Homework Problems
Problem 11.33.
An edge is said to leave a set of vertices if one end of the edge is in the set and the
other end is not.
(a) An n-node graph is said to be mangled if there is an edge leaving every set of
bn=2c or fewer vertices. Prove the following:
Claim. Every mangled graph is connected.
An n-node graph is said to be tangled if there is an edge leaving every set of
dn=3e or fewer vertices.
(b) Draw a tangled graph that is not connected.
(c) Find the error in the bogus proof of the following
False Claim. Every tangled graph is connected.
Bogus proof. The proof is by strong induction on the number of vertices in the
graph. Let P.n/ be the proposition that if an n-node graph is tangled, then it is
connected. In the base case, P.1/ is true because the graph consisting of a single
node is trivially connected.
For the inductive case, assume n  1 and P.1/; : : : ; P.n/ hold. We must prove
P.n C 1/, namely, that if an .n C 1/-node graph is tangled, then it is connected.
So let G be a tangled, .nC1/-node graph. Choose dn=3e of the vertices and let G1
be the tangled subgraph of G with these vertices and G2 be the tangled subgraph
with the rest of the vertices. Note that since n  1, the graph G has a least two
vertices, and so both G1 and G2 contain at least one vertex. Since G1 and G2 are
tangled, we may assume by strong induction that both are connected. Also, since
G is tangled, there is an edge leaving the vertices of G1 which necessarily connects
to a vertex of G2. This means there is a path between any two vertices of G: a path
within one subgraph if both vertices are in the same subgraph, and a path traversing
the connecting edge if the vertices are in separate subgraphs. Therefore, the entire

Chapter 11
Simple Graphs
406
graph, G, is connected. This completes the proof of the inductive case, and the
Claim follows by strong induction.

Problem 11.34.
Let G be the graph formed from C2n, the cycle of length 2n, by connecting every
pair of vertices at maximum distance from each other in C2n by an edge in G.
(a) Given two vertices of G Ô¨Ånd their distance in G.
(b) What is the diameter of G, that is, the largest distance between two vertices?
(c) Prove that the graph is not 4-connected.
(d) Prove that the graph is 3-connected.
Exam Problems
Problem 11.35.
We apply the following operation to a simple graph G: pick two vertices u ¬§ v
such that either
1. there is an edge of G between u and v, and there is also a path from u to v
which does not include this edge; in this case, delete the edge fu; vg.
2. there is no path from u to v; in this case, add the edge fu; vg.
Keep repeating these operations until it is no longer possible to Ô¨Ånd two vertices
u ¬§ v to which an operation applies.
Assume the vertices of G are the integers 1; 2; : : : ; n for some n  2. This
procedure can be modelled as a state machine whose states are all possible simple
graphs with vertices 1; 2; : : : ; n. G is the start state, and the Ô¨Ånal states are the
graphs on which no operation is possible.
(a) Let G be the graph with vertices f1; 2; 3; 4g and edges
ff1; 2g; f3; 4gg
How many possible Ô¨Ånal states are reachable from start state G?
1in
(b) On the line next to each of the derived state variables below, indicate the
strongest property from the list below that the variable is guaranteed to satisfy,
no matter what the starting graph G is. The properties are:

11.11. Forests & Trees
407
constant
increasing
decreasing
nonincreasing
nondecreasing
none of these
For any state, let e be the number of edges in it, and let c be the number of con-
nected components it has. Since e may increase or decrease in a transition, it does
not have any of the Ô¨Årst four properties. The derived variables are:
0) e
none of these
i) c
1.0in
ii) c C e
1.0in
iii) 2c C e
1.0in
iv) c C
e
eC1
1.0in
(c) Explain why, starting from any state, G, the procedure terminates. If your ex-
planation depends on answers you gave to part (b), you must justify those answers.
(d) Prove that any Ô¨Ånal state must be an unordered tree on the set of vertices, that
is, a spanning tree.
Problems for Section 11.11
Practice Problems
Problem 11.36. (a) Prove that the average degree of a tree is less than 2.
(b) Suppose every vertex in a graph has degree at least k. Explain why the graph
has a path of length k.
Hint: Consider a longest path.
Exam Problems
Problem 11.37.
The n-dimensional hypercube, Hn, is a simple graph whose vertices are the binary
strings of length n. Two vertices are adjacent if and only if they differ in exactly
one bit. Consider for example H3, shown in Figure 11.29. (Here, vertices 111 and
011 are adjacent because they differ only in the Ô¨Årst bit, while vertices 101 and
011 are not adjacent because they differ in both the Ô¨Årst and second bits.)
Explain why it is impossible to Ô¨Ånd two spanning trees of H3 that have no edges
in common.

Chapter 11
Simple Graphs
408
000
001
010
011
100
101
110
111
Figure 11.29
H3 .
Problem 11.38.
(a) Circle all the properties below that are preserved under graph isomorphism.
 There is a cycle that includes all the vertices.
 Two edges are of equal length.
 The graph remains connected if any two edges are removed.
 There exists an edge that is an edge of every spanning tree.
 The negation of a property that is preserved under isomorphism.
(b) For the following statements about Ô¨Ånite trees, circle true or false, and pro-
vide counterexamples for those that are false.
 Any connected subgraph is a tree.
true
false
 Adding an edge between two nonadjacent vertices creates a cycle.
true
false
 The number of vertices is one less than twice the number of leaves.
true
false

11.11. Forests & Trees
409
 The number of vertices is one less than the number of edges. true
false
 For every Ô¨Ånite graph (not necessarily a tree), there is one (a Ô¨Ånite tree) that
spans it.
true
false
Class Problems
Problem 11.39.
Procedure Mark starts with a connected, simple graph with all edges unmarked and
then marks some edges. At any point in the procedure a path that includes only
marked edges is called a fully marked path, and an edge that has no fully marked
path between its endpoints is called eligible.
Procedure Mark simply keeps marking eligible edges, and terminates when there
are none.
Prove that Mark terminates, and that when it does, the set of marked edges forms
a spanning tree of the original graph.
Problem 11.40.
A procedure for connecting up a (possibly disconnected) simple graph and creating
a spanning tree can be modelled as a state machine whose states are Ô¨Ånite simple
graphs. A state is Ô¨Ånal when no further transitions are possible. The transitions are
determined by the following rules:
Procedure create-spanning-tree
1. If there is an edge hu‚Äîvi on a cycle, then delete hu‚Äîvi.
2. If vertices u and v are not connected, then add the edge hu‚Äîvi.
(a) Draw all the possible Ô¨Ånal states reachable starting with the graph with vertices
f1; 2; 3; 4g and edges
fh1‚Äî2i ; h3‚Äî4ig:
(b) Prove that if the machine reaches a Ô¨Ånal state, then the Ô¨Ånal state will be a tree
on the vertices graph on which it started.
(c) For any graph, G0, let e be the number of edges in G0, c be the number of
connected components it has, and s be the number of cycles. For each of the quan-
tities below, indicate the strongest of the properties that it is guaranteed to satisfy,
no matter what the starting graph is.

Chapter 11
Simple Graphs
410
The choices for properties are: constant, strictly increasing, strictly decreasing,
weakly increasing, weakly decreasing, none of these.
(i) e
(ii) c
(iii) s
(iv) e   s
(v) c C e
(vi) 3c C 2e
(vii) c C s
(d) Prove that one of the quantities from part (c) strictly decreases at each transi-
tion. Conclude that for every starting state, the machine will reach a Ô¨Ånal state.
Problem 11.41.
Prove that a graph is a tree iff it has a unique path between every two vertices.
Problem 11.42.
Let G be a weighted graph and suppose there is a unique edge e 2 E.G/ with
smallest weight, that is, w.e/ < w.f / for all edges f 2 E.G/   feg. Prove that
any minimum weight spanning tree (MST) of G must include e.
Problem 11.43.
Let G be a 4  4 grid with vertical and horizontal edges between neighboring
vertices. Formally,
V.G/ D ≈í0; 3¬ç2 WWD f.k; j/ j 0  k; j  3g:
Letting hi;j be the horizontal edge h.i; j/‚Äî.i C 1; j/i and vj;i be the vertical edge
h.j; i/‚Äî.j; i C 1/i for i 2 ≈í0; 2¬ç; j 2 ≈í0; 3¬ç. The weights of these edges are
w.hi;j / WWD 4i C j
100 ;
w.vj;i/ WWD 1 C i C 4j
100 :
(A picture of G would help; you might like to draw one.)

11.11. Forests & Trees
411
(a) Construct a minimum weight spanning tree (MST) for G by initially selecting
the minimum weight edge, and then successively selecting the minimum weight
edge that does not create a cycle with the previously selected edges. Stop when the
selected edges form a spanning tree of G. (This is Kruskal‚Äôs MST algorithm.)
(b) Grow an MST for G starting with the tree consisting of the single vertex .1; 2/
and successively adding the minimum weight edge with exactly one endpoint in the
tree. Stop when the tree spans G. (This is Prim‚Äôs MST algorithm.)
(c) Grow an MST for G by treating the vertices .0; 0/; .0; 3/; .2; 3/ as 1-vertex
trees and then successively adding, for each tree in parallel, the minimum weight
edge among the edges with one endpoint in the tree. Continue as long as there is
no edge between two trees, then go back to applying the general gray edge method
until the parallel trees merge to form a spanning tree of G. (This is 6.042‚Äôs parallel
MST algorithm.)
(d) Verify that you got the same MST each time.
(e) Look up the proof of the ‚Äúgray edge‚Äù Lemma 11.11.11, and spend up to 15
minutes drawing one or two Ô¨Ågures that could be added to the text to help make the
proof clearer.
Problem 11.44.
In this problem you will prove:
Theorem. A graph G is 2-colorable iff it contains no odd length closed walk.
As usual with ‚Äúiff‚Äù assertions, the proof splits into two proofs: part (a) asks you
to prove that the left side of the ‚Äúiff‚Äù implies the right side. The other problem parts
prove that the right side implies the left.
(a) Assume the left side and prove the right side. Three to Ô¨Åve sentences should
sufÔ¨Åce.
(b) Now assume the right side. As a Ô¨Årst step toward proving the left side, explain
why we can focus on a single connected component H within G.
(c) As a second step, explain how to 2-color any tree.
(d) Choose any 2-coloring of a spanning tree, T , of H.
Prove that H is 2-
colorable by showing that any edge not in T must also connect different-colored
vertices.

Chapter 11
Simple Graphs
412
Homework Problems
Problem 11.45.
Prove Corollary 11.11.12: If all edges in a Ô¨Ånite weighted graph have distinct
weights, then the graph has a unique MST.
Hint: Suppose M and N were different MST‚Äôs of the same graph. Let e be the
smallest edge in one and not the other, say e 2 M   N, and observe that N C e
must have a cycle.

12
Planar Graphs
12.1
Drawing Graphs in the Plane
Suppose there are three dog houses and three human houses, as shown in Fig-
ure 12.1. Can you Ô¨Ånd a route from each dog house to each human house such that
no route crosses any other route?
A similar question comes up about a little-known animal called a quadrapus that
looks like an octopus with four stretchy arms instead of eight. If Ô¨Åve quadrapi are
resting on the sea Ô¨Çoor, as shown in Figure 12.2, can each quadrapus simultane-
ously shake hands with every other in such a way that no arms cross?
Both these puzzles can be understood as asking about drawing graphs in the
plane. Replacing dogs and houses by nodes, the dog house puzzle can be rephrased
as asking whether there is a planar drawing of the graph with six nodes and edges
between each of the Ô¨Årst three nodes and each of the second three nodes. This
graph is called the complete bipartite graph K3;3 and is shown in Figure 12.3.(a).
The quadrapi puzzle asks whether there is a planar drawing of the complete graph
K5 shown in Figure 12.3.(b).
In each case, the answer is, ‚ÄúNo ‚Äîbut almost!‚Äù In fact, if you remove an edge
from either of these graphs, then the resulting graph can be redrawn in the plane so
that no edges cross, as shown in Figure 12.4.
Planar drawings have applications in circuit layout and are helpful in displaying
graphical data such as program Ô¨Çow charts, organizational charts, and scheduling
conÔ¨Çicts. For these applications, the goal is to draw the graph in the plane with as
few edge crossings as possible. (See the box on the following page for one such
example.)
12.2
DeÔ¨Ånitions of Planar Graphs
We took the idea of a planar drawing for granted in the previous section, but if
we‚Äôre going to prove things about planar graphs, we better have precise deÔ¨Ånitions.
DeÔ¨Ånition 12.2.1. A drawing of a graph assigns to each node a distinct point in
the plane and assigns to each edge a smooth curve in the plane whose endpoints
correspond to the nodes incident to the edge. The drawing is planar if none of the

Chapter 12
Planar Graphs
414
Figure 12.1
Three dog houses and and three human houses. Is there a route from
each dog house to each human house so that no pair of routes cross each other?

12.2. DeÔ¨Ånitions of Planar Graphs
415
Figure 12.2
Five quadrapi (4-armed creatures).
(a)
(b)
Figure 12.3
K3;3 (a) and K5 (b). Can you redraw these graphs so that no pairs
of edges cross?

Chapter 12
Planar Graphs
416
v
u
(a)
u
v
(b)
Figure 12.4
Planar drawings of (a) K3;3 without hu‚Äîvi, and (b) K5 without
hu‚Äîvi.
Steve Wozniak and a Planar Circuit Design
When wires are arranged on a surface, like a circuit board or microchip, cross-
ings require troublesome three-dimensional structures.
When Steve Wozniak
designed the disk drive for the early Apple II computer, he struggled might-
ily to achieve a nearly planar design according to the following excerpt from
apple2history.org which in turn quotes Fire in the Valley by Freiberger
and Swaine:
For two weeks, he worked late each night to make a satisfactory de-
sign. When he was Ô¨Ånished, he found that if he moved a connector
he could cut down on feedthroughs, making the board more reliable.
To make that move, however, he had to start over in his design. This
time it only took twenty hours. He then saw another feedthrough
that could be eliminated, and again started over on his design. ‚ÄúThe
Ô¨Ånal design was generally recognized by computer engineers as bril-
liant and was by engineering aesthetics beautiful. Woz later said, ‚ÄôIt‚Äôs
something you can only do if you‚Äôre the engineer and the PC board
layout person yourself. That was an artistic layout. The board has
virtually no feedthroughs.‚Äô

12.2. DeÔ¨Ånitions of Planar Graphs
417
curves cross themselves or other curves, namely, the only points that appear more
than once on any of the curves are the node points. A graph is planar when it has a
planar drawing.
DeÔ¨Ånition 12.2.1 is precise but depends on further concepts: ‚Äúsmooth planar
curves‚Äù and ‚Äúpoints appearing more than once‚Äù on them. We haven‚Äôt deÔ¨Åned these
concepts ‚Äîwe just showed the simple picture in Figure 12.4 and hoped you would
get the idea.
Pictures can be a great way to get a new idea across, but it is generally not a good
idea to use a picture to replace precise mathematics. Relying solely on pictures can
sometimes lead to disaster ‚Äîor to bogus proofs, anyway. There is a long history of
bogus proofs about planar graphs based on misleading pictures.
The bad news is that to prove things about planar graphs using the planar draw-
ings of DeÔ¨Ånition 12.2.1, we‚Äôd have to take a chapter-long excursion into contin-
uous mathematics just to develop the needed concepts from plane geometry and
point-set topology. The good news is that there is another way to deÔ¨Åne planar
graphs that uses only discrete mathematics. In particular, we can deÔ¨Åne planar
graphs as a recursive data type. In order to understand how it works, we Ô¨Årst need
to understand the concept of a face in a planar drawing.
12.2.1
Faces
The curves in a planar drawing divide up the plane into connected regions called
the continuous faces1 of the drawing. For example, the drawing in Figure 12.5 has
four continuous faces. Face IV, which extends off to inÔ¨Ånity in all directions, is
called the outside face.
The vertices along the boundary of each continuous face in Figure 12.5 form a
cycle. For example, labeling the vertices as in Figure 12.6, the cycles for each of
the face boundaries can be described by the vertex sequences
abca
abda
bcdb
acda:
(12.1)
These four cycles correspond nicely to the four continuous faces in Figure 12.6 ‚Äî
so nicely, in fact, that we can identify each of the faces in Figure 12.6 by its cycle.
For example, the cycle abca identiÔ¨Åes face III. The cycles in list 12.1 are called the
discrete faces of the graph in Figure 12.6. We use the term ‚Äúdiscrete‚Äù since cycles
in a graph are a discrete data type ‚Äîas opposed to a region in the plane, which is a
continuous data type.
1Most texts drop the adjective continuous from the deÔ¨Ånition of a face as a connected region. We
need the adjective to distinguish continuous faces from the discrete faces we‚Äôre about to deÔ¨Åne.

Chapter 12
Planar Graphs
418
II
I
III
IV
Figure 12.5
A planar drawing with four continuous faces.
d
c
b
a
II
I
III
IV
Figure 12.6
The drawing with labeled vertices.

12.2. DeÔ¨Ånitions of Planar Graphs
419
b
e
f
g
c
d
a
Figure 12.7
A planar drawing with a bridge.
Unfortunately, continuous faces in planar drawings are not always bounded by
cycles in the graph ‚Äîthings can get a little more complicated. For example, the
planar drawing in Figure 12.7 has what we will call a bridge, namely, a cut edge
hc‚Äîei. The sequence of vertices along the boundary of the outer region of the
drawing is
abcefgecda:
This sequence deÔ¨Ånes a closed walk, but does not deÔ¨Åne a cycle since the walk has
two occurrences of the bridge hc‚Äîei and each of its endpoints.
The planar drawing in Figure 12.8 illustrates another complication. This drawing
has what we will call a dongle, namely, the nodes v, x, y, and w, and the edges
incident to them. The sequence of vertices along the boundary of the inner region
is
rstvxyxvwvtur:
This sequence deÔ¨Ånes a closed walk, but once again does not deÔ¨Åne a cycle because
it has two occurrences of every edge of the dongle ‚Äîonce ‚Äúcoming‚Äù and once
‚Äúgoing.‚Äù
It turns out that bridges and dongles are the only complications, at least for con-
nected graphs. In particular, every continuous face in a planar drawing corresponds
to a closed walk in the graph. These closed walks will be called the discrete faces
of the drawing, and we‚Äôll deÔ¨Åne them next.
12.2.2
A Recursive DeÔ¨Ånition for Planar Embeddings
The association between the continuous faces of a planar drawing and closed walks
provides the discrete data type we can use instead of continuous drawings. We‚Äôll
deÔ¨Åne a planar embedding of connected graph to be the set of closed walks that are
its face boundaries. Since all we care about in a graph are the connections between

Chapter 12
Planar Graphs
420
s
t
u
r
v
x
y
w
Figure 12.8
A planar drawing with a dongle.
vertices ‚Äînot what a drawing of the graph actually looks like ‚Äîplanar embeddings
are exactly what we need.
The question is how to deÔ¨Åne planar embeddings without appealing to continu-
ous drawings. There is a simple way to do this based on the idea that any continuous
drawing can drawn step by step:
 either draw a new point somewhere in the plane to represent a vertex,
 or draw a curve between two vertex points that have already been laid down,
making sure the new curve doesn‚Äôt cross any of the previously drawn curves.
A new curve won‚Äôt cross any other curves precisely when it stays within one
of the continuous faces. Alternatively, a new curve won‚Äôt have to cross any other
curves if it can go between the outer faces of two different drawings. So to be sure
it‚Äôs ok to draw a new curve, we just need to check that its endpoints are on the
boundary of the same face, or that its endpoints are on the outer faces of different
drawings. Of course drawing the new curve changes the faces slightly, so the face
boundaries will have to be updated once the new curve is drawn. This is the idea
behind the following recursive deÔ¨Ånition.
DeÔ¨Ånition 12.2.2. A planar embedding of a connected graph consists of a nonempty
set of closed walks of the graph called the discrete faces of the embedding. Planar
embeddings are deÔ¨Åned recursively as follows:
Base case: If G is a graph consisting of a single vertex, v, then a planar embedding
of G has one discrete face, namely, the length zero closed walk, v.

12.2. DeÔ¨Ånitions of Planar Graphs
421
y
w
a
z
b
x
Figure 12.9
The ‚Äúsplit a face‚Äù case: awxbyza splits into awxba and abyza.
Constructor case (split a face): Suppose G is a connected graph with a planar
embedding, and suppose a and b are distinct, nonadjacent vertices of G that occur
in some discrete face, , of the planar embedding. That is,  is a closed walk of the
form
 D ÀõbÀá
where Àõ is a walk from a to b and Àá is a walk from b to a. Then the graph obtained
by adding the edge ha‚Äîbi to the edges of G has a planar embedding with the same
discrete faces as G, except that face  is replaced by the two discrete faces2
Àõbhb‚Äîai
and
ha‚Äîbi bÀá
(12.2)
as illustrated in Figure 12.9.3
Constructor case (add a bridge): Suppose G and H are connected graphs with
planar embeddings and disjoint sets of vertices. Let  be a discrete face of the
embedding of G and suppose that  begins and ends at vertex a.
Similarly, let ƒ± be a discrete face of the embedding of H that begins and ends at
vertex b.
2 There is a minor exception to this deÔ¨Ånition of embedding in the special case when G is a line
graph beginning with a and ending with b. In this case the cycles into which  splits are actually
the same. That‚Äôs because adding edge ha‚Äîbi creates a cycle that divides the plane into ‚Äúinner‚Äù and
‚Äúouter‚Äù continuous faces that are both bordered by this cycle. In order to maintain the correspondence
between continuous faces and discrete faces in this case, we deÔ¨Åne the two discrete faces of the
embedding to be two ‚Äúcopies‚Äù of this same cycle.
3Formally, merge is an operation on walks, not a walk and an edge, so in (12.2), we should have
used a walk .a ha‚Äîbi b/ instead of an edge ha‚Äîbi and written
Àõb.b hb‚Äîai a/
and
.a ha‚Äîbi b/bÀá

Chapter 12
Planar Graphs
422
z
b
t
v
a
x
y
u
w
Figure 12.10
The ‚Äúadd a bridge‚Äù case.
Then the graph obtained by connecting G and H with a new edge, ha‚Äîbi, has a
planar embedding whose discrete faces are the union of the discrete faces of G and
H, except that faces  and ƒ± are replaced by one new face
bha‚Äîbi bƒ±bhb‚Äîai :
This is illustrated in Figure 12.10, where the vertex sequences of the faces of G
and H are:
G W faxyza; axya; ayzag
H W fbtuvwb; btvwb; tuvtg;
and after adding the bridge ha‚Äîbi, there is a single connected graph whose faces
have the vertex sequences
faxyzabtuvwba; axya; ayza; btvwb; tuvtg:
A bridge is simply a cut edge, but in the context of planar embeddings, the
bridges are precisely the edges that occur twice on the same discrete face ‚Äîas
opposed to once on each of two faces. Dongles are trees made of bridges; we only
use dongles in illustrations, so there‚Äôs no need to deÔ¨Åne them more precisely.
12.2.3
Does It Work?
Yes! In general, a graph is planar because it has a planar drawing according to
DeÔ¨Ånition 12.2.1 if and only if each of its connected components has a planar em-
bedding as speciÔ¨Åed in DeÔ¨Ånition 12.2.2. Of course we can‚Äôt prove this without an
excursion into exactly the kind of continuous math that we‚Äôre trying to avoid. But
now that the recursive deÔ¨Ånition of planar graphs is in place, we won‚Äôt ever need to
fall back on the continuous stuff. That‚Äôs the good news.
The bad news is that DeÔ¨Ånition 12.2.2 is a lot more technical than the intuitively
simple notion of a drawing whose edges don‚Äôt cross. In many cases it‚Äôs easier to

12.2. DeÔ¨Ånitions of Planar Graphs
423
s
t
r
u
u
t
r
s
Figure 12.11
Two illustrations of the same embedding.
stick to the idea of planar drawings and give proofs in those terms. For example, it‚Äôs
obvious that erasing edges from a planar drawing leaves a planar drawing. On the
other hand, it‚Äôs not at all obvious, though of course it is true, that you can delete an
edge from a planar embedding and still get a planar embedding (see Problem 12.9).
In the hands of experts, and perhaps in your hands too with a little more expe-
rience, proofs about planar graphs by appeal to drawings can be convincing and
reliable. But given the long history of mistakes in such proofs, it‚Äôs safer to work
from the precise deÔ¨Ånition of planar embedding. More generally, it‚Äôs also important
to see how the abstract properties of curved drawings in the plane can be modelled
successfully using a discrete data type.
12.2.4
Where Did the Outer Face Go?
Every planar drawing has an immediately-recognizable outer face ‚Äîit‚Äôs the one
that goes to inÔ¨Ånity in all directions. But where is the outer face in a planar embed-
ding?
There isn‚Äôt one! That‚Äôs because there really isn‚Äôt any need to distinguish one face
from another. In fact, a planar embedding could be drawn with any given face on
the outside. An intuitive explanation of this is to think of drawing the embedding
on a sphere instead of the plane. Then any face can be made the outside face by
‚Äúpuncturing‚Äù that face of the sphere, stretching the puncture hole to a circle around
the rest of the faces, and Ô¨Çattening the circular drawing onto the plane.
So pictures that show different ‚Äúoutside‚Äù boundaries may actually be illustra-
tions of the same planar embedding. For example, the two embeddings shown in
Figure 12.11 are really the same ‚Äîcheck it: they have the same boundary cycles.
This is what justiÔ¨Åes the ‚Äúadd bridge‚Äù case in DeÔ¨Ånition 12.2.2: whatever face
is chosen in the embeddings of each of the disjoint planar graphs, we can draw
a bridge between them without needing to cross any other edges in the drawing,
because we can assume the bridge connects two ‚Äúouter‚Äù faces.

Chapter 12
Planar Graphs
424
12.3
Euler‚Äôs Formula
The value of the recursive deÔ¨Ånition is that it provides a powerful technique for
proving properties of planar graphs, namely, structural induction. For example,
we will now use DeÔ¨Ånition 12.2.2 and structural induction to establish one of the
most basic properties of a connected planar graph, namely, that the number of ver-
tices and edges completely determines the number of faces in every possible planar
embedding of the graph.
Theorem 12.3.1 (Euler‚Äôs Formula). If a connected graph has a planar embedding,
then
v   e C f D 2
where v is the number of vertices, e is the number of edges, and f is the number of
faces.
For example, in Figure 12.5, v D 4, e D 6, and f D 4. Sure enough, 4 6C4 D
2, as Euler‚Äôs Formula claims.
Proof. The proof is by structural induction on the deÔ¨Ånition of planar embeddings.
Let P.E/ be the proposition that v   e C f D 2 for an embedding, E.
Base case (E is the one-vertex planar embedding): By deÔ¨Ånition, v D 1, e D 0,
and f D 1, and 1   0 C 1 D 2, so P.E/ indeed holds.
Constructor case (split a face): Suppose G is a connected graph with a planar
embedding, and suppose a and b are distinct, nonadjacent vertices of G that appear
on some discrete face,  D a : : : b    a, of the planar embedding.
Then the graph obtained by adding the edge ha‚Äîbi to the edges of G has a
planar embedding with one more face and one more edge than G. So the quantity
v   e C f will remain the same for both graphs, and since by structural induction
this quantity is 2 for G‚Äôs embedding, it‚Äôs also 2 for the embedding of G with the
added edge. So P holds for the constructed embedding.
Constructor case (add bridge): Suppose G and H are connected graphs with pla-
nar embeddings and disjoint sets of vertices. Then connecting these two graphs
with a bridge merges the two bridged faces into a single face, and leaves all other
faces unchanged. So the bridge operation yields a planar embedding of a connected

12.4. Bounding the Number of Edges in a Planar Graph
425
graph with vG C vH vertices, eG C eH C 1 edges, and fG C fH   1 faces. Since
.vG C vH/   .eG C eH C 1/ C .fG C fH   1/
D .vG   eG C fG/ C .vH   eH C fH/   2
D .2/ C .2/   2
(by structural induction hypothesis)
D 2;
v   e C f remains equal to 2 for the constructed embedding. That is, P.E/ also
holds in this case.
This completes the proof of the constructor cases, and the theorem follows by
structural induction.

12.4
Bounding the Number of Edges in a Planar Graph
Like Euler‚Äôs formula, the following lemmas follow by structural induction directly
from DeÔ¨Ånition 12.2.2.
Lemma 12.4.1. In a planar embedding of a connected graph, each edge occurs
once in each of two different faces, or occurs exactly twice in one face.
Lemma 12.4.2. In a planar embedding of a connected graph with at least three
vertices, each face is of length at least three.
Combining Lemmas 12.4.1 and 12.4.2 with Euler‚Äôs Formula, we can now prove
that planar graphs have a limited number of edges:
Theorem 12.4.3. Suppose a connected planar graph has v  3 vertices and e
edges. Then
e  3v   6:
(12.3)
Proof. By deÔ¨Ånition, a connected graph is planar iff it has a planar embedding. So
suppose a connected graph with v vertices and e edges has a planar embedding
with f faces. By Lemma 12.4.1, every edge has exactly two occurrences in the
face boundaries. So the sum of the lengths of the face boundaries is exactly 2e.
Also by Lemma 12.4.2, when v  3, each face boundary is of length at least three,
so this sum is at least 3f . This implies that
3f  2e:
(12.4)

Chapter 12
Planar Graphs
426
But f D e   v C 2 by Euler‚Äôs formula, and substituting into (12.4) gives
3.e   v C 2/  2e
e   3v C 6  0
e  3v   6

12.5
Returning to K5 and K3;3
Finally we have a simple way to answer the quadrapi question at the beginning of
this chapter: the Ô¨Åve quadrapi can‚Äôt all shake hands without crossing. The reason
is that we know the quadrupi question is the same as asking whether a complete
graph K5 is planar, and Theorem 12.4.3 has the immediate:
Corollary 12.5.1. K5 is not planar.
Proof. K5 is connected and has 5 vertices and 10 edges. But since 10 > 3  5   6,
K5 does not satisfy the inequality (12.3) that holds in all planar graphs.

We can also use Euler‚Äôs Formula to show that K3;3 is not planar. The proof is
similar to that of Theorem 12.3 except that we use the additional fact that K3;3 is a
bipartite graph.
Lemma 12.5.2. In a planar embedding of a connected bipartite graph with at least
3 vertices, each face has length at least 4.
Proof. By Lemma 12.4.2, every face of a planar embedding of the graph has length
at least 3. But by Lemma 11.7.2 and Theorem 11.10.1.3, a bipartite graph can‚Äôt
have odd length closed walks. Since the faces of a planar embedding are closed
walks, there can‚Äôt be any faces of length 3 in a bipartite embedding. So every face
must have length at least 4.

Theorem 12.5.3. Suppose a connected bipartite graph with v  3 vertices and e
edges is planar. Then
e  2v   4:
(12.5)
Proof. Lemma 12.5.2 implies that all the faces of an embedding of the graph have
length at least 4. Now arguing as in the proof of Theorem 12.4.3, we Ô¨Ånd that the
sum of the lengths of the face boundaries is exactly 2e and at least 4f . Hence,
4f  2e
(12.6)

12.6. Coloring Planar Graphs
427
for any embedding of a planar bipartite graph. By Euler‚Äôs theorem, f D 2   v C e.
Substituting 2   v C e for f in (12.6), we have
4.2   v C e/  2e;
which simplies to (12.5).

Corollary 12.5.4. K3;3 is not planar.
Proof. K3;3 is connected, bipartite and has 6 vertices and 9 edges. But since 9 >
2  6   4, K3;3 does not satisfy the inequality (12.3) that holds in all bipartite planar
graphs.

12.6
Coloring Planar Graphs
We‚Äôve covered a lot of ground with planar graphs, but not nearly enough to prove
the famous 4-color theorem. But we can get awfully close. Indeed, we have done
almost enough work to prove that every planar graph can be colored using only 5
colors.
There are two familiar facts about planarity that we will need.
Lemma 12.6.1. Any subgraph of a planar graph is planar.
Lemma 12.6.2. Merging two adjacent vertices of a planar graph leaves another
planar graph.
Merging two adjacent vertices, n1 and n2 of a graph means deleting the two
vertices and then replacing them by a new ‚Äúmerged‚Äù vertex, m, adjacent to all the
vertices that were adjacent to either of n1 or n2, as illustrated in Figure 12.12.
Many authors take Lemmas 12.6.1 and 12.6.2 for granted for continuous draw-
ings of planar graphs described by DeÔ¨Ånition 12.2.1. With the recursive DeÔ¨Åni-
tion 12.2.2 both Lemmas can actually be proved using structural induction (see
Problem 12.9).
We need only one more lemma:
Lemma 12.6.3. Every planar graph has a vertex of degree at most Ô¨Åve.
Proof. Assuming to the contrary that every vertex of some planar graph had degree
at least 6, then the sum of the vertex degrees is at least 6v. But the sum of the
vertex degrees equals 2e by the Handshake Lemma 11.2.1, so we have e  3v
contradicting the fact that e  3v   6 < 3v by Theorem 12.4.3.


Chapter 12
Planar Graphs
428
n2
n1
n2
n1
m
!
!
Figure 12.12
Merging adjacent vertices n1 and n2 into new vertex, m.
Theorem 12.6.4. Every planar graph is Ô¨Åve-colorable.
Proof. The proof will be by strong induction on the number, v, of vertices, with
induction hypothesis:
Every planar graph with v vertices is Ô¨Åve-colorable.
Base cases (v  5): immediate.
Inductive case: Suppose G is a planar graph with v C 1 vertices. We will describe
a Ô¨Åve-coloring of G.
First, choose a vertex, g, of G with degree at most 5; Lemma 12.6.3 guarantees
there will be such a vertex.
Case 1: (deg.g/ < 5): Deleting g from G leaves a graph, H, that is planar by
Lemma 12.6.1, and, since H has v vertices, it is Ô¨Åve-colorable by induction
hypothesis. Now deÔ¨Åne a Ô¨Åve coloring of G as follows: use the Ô¨Åve-coloring
of H for all the vertices besides g, and assign one of the Ô¨Åve colors to g that
is not the same as the color assigned to any of its neighbors. Since there are
fewer than 5 neighbors, there will always be such a color available for g.
Case 2: (deg.g/ D 5): If the Ô¨Åve neighbors of g in G were all adjacent to each
other, then these Ô¨Åve vertices would form a nonplanar subgraph isomorphic
to K5, contradicting Lemma 12.6.1 (since K5 is not planar). So there must

12.7. Classifying Polyhedra
429
be two neighbors, n1 and n2, of g that are not adjacent. Now merge n1 and
g into a new vertex, m. In this new graph, n2 is adjacent to m, and the graph
is planar by Lemma 12.6.2. So we can then merge m and n2 into a another
new vertex, m0, resulting in a new graph, G0, which by Lemma 12.6.2 is
also planar. Since G0 has v   1 vertices, it is Ô¨Åve-colorable by the induction
hypothesis.
Now deÔ¨Åne a Ô¨Åve coloring of G as follows: use the Ô¨Åve-coloring of G0 for
all the vertices besides g, n1 and n2. Next assign the color of m0 in G0 to
be the color of the neighbors n1 and n2. Since n1 and n2 are not adjacent
in G, this deÔ¨Ånes a proper Ô¨Åve-coloring of G except for vertex g. But since
these two neighbors of g have the same color, the neighbors of g have been
colored using fewer than Ô¨Åve colors altogether. So complete the Ô¨Åve-coloring
of G by assigning one of the Ô¨Åve colors to g that is not the same as any of
the colors assigned to its neighbors.

12.7
Classifying Polyhedra
The Pythagoreans had two great mathematical secrets, the irrationality of
p
2 and
a geometric construct that we‚Äôre about to rediscover!
A polyhedron is a convex, three-dimensional region bounded by a Ô¨Ånite number
of polygonal faces. If the faces are identical regular polygons and an equal number
of polygons meet at each corner, then the polyhedron is regular. Three examples
of regular polyhedra are shown in Figure 12.13: the tetrahedron, the cube, and the
octahedron.
We can determine how many more regular polyhedra there are by thinking about
planarity. Suppose we took any polyhedron and placed a sphere inside it. Then we
could project the polyhedron face boundaries onto the sphere, which would give
an image that was a planar graph embedded on the sphere, with the images of the
corners of the polyhedron corresponding to vertices of the graph. We‚Äôve already
observed that embeddings on a sphere are the same as embeddings on the plane, so
Euler‚Äôs formula for planar graphs can help guide our search for regular polyhedra.
For example, planar embeddings of the three polyhedra in Figure 12.1 are shown
in Figure 12.14.
Let m be the number of faces that meet at each corner of a polyhedron, and let n
be the number of edges on each face. In the corresponding planar graph, there are
m edges incident to each of the v vertices. By the Handshake Lemma 11.2.1, we

Chapter 12
Planar Graphs
430
(a)
(b)
(c)
Figure 12.13
The tetrahedron (a), cube (b), and octahedron (c).
v
(a)
(b)
(c)
Figure 12.14
Planar embeddings of the tetrahedron (a), cube (b), and octahe-
dron (c).

12.7. Classifying Polyhedra
431
n
m
v
e
f
polyhedron
3
3
4
6
4
tetrahedron
4
3
8
12
6
cube
3
4
6
12
8
octahedron
3
5
12
30
20
icosahedron
5
3
20
30
12
dodecahedron
Figure 12.15
The only possible regular polyhedra.
know:
mv D 2e:
Also, each face is bounded by n edges. Since each edge is on the boundary of two
faces, we have:
nf D 2e
Solving for v and f in these equations and then substituting into Euler‚Äôs formula
gives:
2e
m   e C 2e
n D 2
which simpliÔ¨Åes to
1
m C 1
n D 1
e C 1
2
(12.7)
Equation 12.7 places strong restrictions on the structure of a polyhedron. Every
nondegenerate polygon has at least 3 sides, so n  3. And at least 3 polygons
must meet to form a corner, so m  3. On the other hand, if either n or m were
6 or more, then the left side of the equation could be at most 1=3 C 1=6 D 1=2,
which is less than the right side. Checking the Ô¨Ånitely-many cases that remain turns
up only Ô¨Åve solutions, as shown in Figure 12.15. For each valid combination of n
and m, we can compute the associated number of vertices v, edges e, and faces f .
And polyhedra with these properties do actually exist. The largest polyhedron, the
dodecahedron, was the other great mathematical secret of the Pythagorean sect.
The 5 polyhedra in Figure 12.15 are the only possible regular polyhedra. So if
you want to put more than 20 geocentric satellites in orbit so that they uniformly
blanket the globe‚Äîtough luck!

Chapter 12
Planar Graphs
432
12.8
Another Characterization for Planar Graphs
We did not pick K5 and K3;3 as examples because of their application to dog
houses or quadrapi shaking hands. We really picked them because they provide
another, famous, discrete characterizarion of planar graphs:
Theorem 12.8.1 (Kuratowski). A graph is not planar if and only if it contains K5
or K3;3 as a minor.
DeÔ¨Ånition 12.8.2. A minor of a graph G is a graph that can be obtained by repeat-
edly4 deleting vertices, deleting edges, and merging adjacent vertices of G.
For example, Figure 12.16 illustrates why C3 is a minor of the graph in Fig-
ure 12.16(a). In fact C3 is a minor of a connected graph G if and only if G is not a
tree.
The known proofs of Kuratowski‚Äôs Theorem 12.8.1 are a little too long to include
in an introductory text, so we won‚Äôt give one.
Problems for Section 12.2
Practice Problems
Problem 12.1.
What are the discrete faces of the following two graphs?
Write each cycle as a sequence of letters without spaces, starting with the alpha-
betically earliest letter in the clockwise direction, for example ‚Äúadbfa.‚Äù Separate
the sequences with spaces.
(a)
b
e
f
g
c
d
a
(b)
4The three operations can each be performed any number of times in any order.

12.8. Another Characterization for Planar Graphs
433
(a)
(b)
(c)
(d)
(e)
(f)
e1
v1
v2
e2
v3
Figure 12.16
One method by which the graph in (a) can be reduced to C3 (f),
thereby showing that C3 is a minor of the graph. The steps are: merging the nodes
incident to e1 (b), deleting v1 and all edges incident to it (c), deleting v2 (d), delet-
ing e2, and deleting v3 (f).

Chapter 12
Planar Graphs
434
s
t
u
r
v
x
y
w
Problems for Section 12.8
Exam Problems
Problem 12.2.
b
c
d
e
a
g
h
i
f
j
k
l
m
n o
g1
g2
g3
(a) Describe an isomorphism between graphs G1 and G2, and another isomor-
phism between G2 and G3.
(b) Why does part .a/ imply that there is an isomorphism between graphs G1 and
G3?
Let G and H be planar graphs. An embedding EG of G is isomorphic to an em-
bedding EH of H iff there is an isomorphism from G to H that also maps each
face of EG to a face of EH.
(c) One of the embeddings pictured above is not isomorphic to either of the others.
Which one? BrieÔ¨Çy explain why.
(d) Explain why all embeddings of two isomorphic planar graphs must have the
same number of faces.

12.8. Another Characterization for Planar Graphs
435
Problem 12.3. (a) Give an example of a planar graph with two planar embeddings,
where the Ô¨Årst embedding has a face whose length is not equal to the length of any
face in the secoind embedding. Draw the two embeddings to demonstrate this.
(b) DeÔ¨Åne the length of a planar embedding, E, to be the sum of the lengths of
the faces of E. Prove that all embeddings of the same planar graph have the same
length.
Problem 12.4.
DeÔ¨Ånition 12.2.2 of planar graph embeddings applied only to connected planar
graphs. The deÔ¨Ånition can be extended to planar graphs that are not necessarily
connected by adding the following additional constructor case to the deÔ¨Ånition:
 Constructor Case: (collect disjoint graphs) Suppose E1 and E2 are planar
embeddings with no vertices in common. Then E1 [ E2 is a planar embed-
ding.
Euler‚Äôs Planar Graph Theorem now generalizes to unconnected graphs as fol-
lows: if a planar embedding, E, has v vertices, e edges, f faces, and c connected
components, then
v   e C f   2c D 0:
(12.8)
This can be proved by structural induction on the deÔ¨Ånition of planar embedding.
(a) State and prove the base case of the structural induction.
(b) Let vi; ei; fi; and ci be the number of vertices, edges, faces, and connected
components in embedding Ei and let v; e; f; c be the numbers for the embedding
from the (collect disjoint graphs) constructor case. Express v; e; f; c in terms of
vi; ei; fi; ci.
(c) Prove the (collect disjoint graphs) case of the structural induction.
Problem 12.5. (a) A simple graph has 8 vertices and 24 edges. What is the average
degree per vertex?
(b) A connected planar simple graph has 5 more edges than it has vertices. How
many faces does it have?
(c) A connected simple graph has one more vertex than it has edges. Explain why
it is a planar graph.

Chapter 12
Planar Graphs
436
(d) How many faces does a planar graph from part c have?
(e) How many distinct isomorphisms are there between the graph given in Fig-
ure 12.17 and itself? (Include the identity isomorphism.)
a
b
c
d
e
f
Figure 12.17
Class Problems
Problem 12.6.
Figure 12.18 shows four different pictures of planar graphs.
(a) For each picture, describe its discrete faces (closed walks that deÔ¨Åne the region
borders).
(b) Which of the pictured graphs are isomorphic? Which pictures represent the
same planar embedding? ‚Äîthat is, they have the same discrete faces.
(c) Describe a way to construct the embedding in Figure 4 according to the recur-
sive DeÔ¨Ånition 12.2.2 of planar embedding. For each application of a constructor
rule, be sure to indicate the faces (cycles) to which the rule was applied and the
cycles which result from the application.
Problem 12.7.
Prove the following assertions by structural induction on the deÔ¨Ånition of planar
embedding.
(a) In a planar embedding of a graph, each edge occurs exactly twice in the faces
of the embedding.
(b) In a planar embedding of a connected graph with at least three vertices, each
face is of length at least three.

12.8. Another Characterization for Planar Graphs
437
a
b
d
c
e
e
figure 1
figure 2
figure 3
figure 4
a
b
d
c
a
b
d
c
a
b
d
c
Figure 12.18
Homework Problems
Problem 12.8.
A simple graph is triangle-free when it has no cycle of length three.
(a) Prove for any connected triangle-free planar graph with v > 2 vertices and e
edges,
e  2v   4:
(12.9)
(b) Show that any connected triangle-free planar graph has at least one vertex of
degree three or less.
(c) Prove that any connected triangle-free planar graph is 4-colorable.

Chapter 12
Planar Graphs
438
Problem 12.9. (a) Prove
Lemma (Switch Edges). Suppose that, starting from some embeddings of planar
graphs with disjoint sets of vertices, it is possible by two successive applications of
constructor operations to add edges e and then f to obtain a planar embedding, F.
Then starting from the same embeddings, it is also possible to obtain F by adding
f and then e with two successive applications of constructor operations.
Hint: There are four cases to analyze, depending on which two constructor opera-
tions are applied to add e and then f . Structural induction is not needed.
(b) Prove
Corollary (Permute Edges). Suppose that, starting from some embeddings of pla-
nar graphs with disjoint sets of vertices, it is possible to add a sequence of edges
e0; e1; : : : ; en by successive applications of constructor operations to obtain a pla-
nar embedding, F. Then starting from the same embeddings, it is also possible
to obtain F by applications of constructor operations that successively add any
permutation5 of the edges e0; e1; : : : ; en.
Hint: By induction on the number of switches of adjacent elements needed to con-
vert the sequence 0,1,...,n into a permutation .0/; .1/; : : : ; .n/.
(c) Prove
Corollary (Delete Edge). Deleting an edge from a planar graph leaves a planar
graph.
(d) Conclude that any subgraph of a planar graph is planar.
5If  W f0; 1; : : : ; ng ! f0; 1; : : : ; ng is a bijection, then the sequence e.0/; e.1/; : : : ; e.n/ is
called a permutation of the sequence e0; e1; : : : ; en.

III
Counting


Introduction
Counting is useful in computer science for several reasons:
 Determining the time and storage required to solve a computational problem
‚Äîa central objective in computer science ‚Äîoften comes down to solving a
counting problem.
 Counting is the basis of probability theory, which plays a central role in all
sciences, including computer science.
 Two remarkable proof techniques, the ‚Äúpigeonhole principle‚Äù and ‚Äúcombina-
torial proof,‚Äù rely on counting.
Counting seems easy enough: 1, 2, 3, 4, etc. This direct approach works well for
counting simple things ‚Äîlike your toes ‚Äîand may be the only approach for ex-
tremely complicated things with no identiÔ¨Åable structure. However, subtler meth-
ods can help you count many things in the vast middle ground, such as:
 The number of different ways to select a dozen doughnuts when there are
Ô¨Åve varieties available.
 The number of 16-bit numbers with exactly 4 ones.
Perhaps surprisingly, but certainly not coincidentally, these two numbersa are the
same: 1820.
We begin our study of counting in Chapter 13 with a collection of rules and
methods for Ô¨Ånding closed-form expressions for commonly-occurring sums and
products such as Pn
iD1 xi and n≈† D Qn
iD1 i. We also introduce asymptotic nota-
tions such as , O, and ‚Äö that are commonly used in computer science to express

Part III
Counting
442
the how a quantity such as the running time of a program grows with the size of the
input.
Chapter 14 describes the most basic rules for determining the cardinality of a
set. These rules are actually theorems, but our focus won‚Äôt be on their proofs per se
‚Äîour objective is to teach you simple counting as a practical skill, like integration.
But counting can be tricky, and people make counting mistakes all the time,
so a crucial part of counting skill is being able to verify a counting argument.
Sometimes this can be done simply by Ô¨Ånding an alternative way to count and
then comparing answers ‚Äîthey better agree. But most elementary counting argu-
ments reduce to Ô¨Ånding a bijection between objects to be counted and easy-to-count
sequences. The chapter shows how explicitly deÔ¨Åning these bijections ‚Äîand veri-
fying that they are bijections ‚Äîis another useful way to verify counting arguments.
The material in Chapter 14 is simple yet powerful, and it provides a great tool set
for use in your future career.

13
Sums and Asymptotics
Sums and products arise regularly in the analysis of algorithms, Ô¨Ånancial appli-
cations, physical problems, and probabilistic systems. For example, according to
Theorem 2.2.1,
1 C 2 C 3 C    C n D n.n C 1/
2
:
(13.1)
Of course the lefthand sum could be expressed concisely as a subscripted summa-
tion
n
X
iD1
i;
but the right hand expression n.n C 1/=2 is not only concise, it is also easier to
evaluate, and it more clearly reveals properties such as the growth rate of the sum.
Expressions like n.n C 1/=2 that do not make use of subscripted summations or
products ‚Äîor those handy but sometimes troublesome dots ‚Äîare called closed
forms.
Another example is the closed form for a geometric sum
1 C x C x2 C x3 C    C xn D 1   xnC1
1   x
(13.2)
given in Problem 5.3. The sum as described on the left hand side of (13.2) involves
n additions and 1 C 2 C    C .n   1/ D .n   1/n=2 multiplications, but its closed
form on the right hand side can be evaluated using fast exponentiation with at most
2 log n multiplications, a division, and a couple of subtractions. Also, the closed
form makes the growth and limiting behavior of the sum much more apparent.
Equations (13.1) and (13.2) were easy to verify by induction, but, as is often the
case, the proofs by induction gave no hint about how these formulas were found in
the Ô¨Årst place. Finding them is part math and part art, which we‚Äôll start examining
in this chapter.
A Ô¨Årst motivating example will be Ô¨Åguring out the value of a Ô¨Ånancial instrument
known as an annuity. The value will be a large and nasty-looking sum. We will
then describe several methods for Ô¨Ånding closed forms for several sorts of sums,
including those for annuities. In some cases, a closed form for a sum may not exist,
and so we will provide a general method for Ô¨Ånding closed forms for good upper
and lower bounds on the sum.
The methods we develop for sums will also work for products since any product
can be converted into a sum by taking a logarithm of the product. As an example,

Chapter 13
Sums and Asymptotics
444
we will use this approach to Ô¨Ånd a good closed-form approximation to the factorial
function
n≈† WWD 1  2  3    n:
We conclude the chapter with a discussion of asymptotic notation. Asymptotic
notation is often used to bound the error terms when there is no exact closed form
expression for a sum or product. It also provides a convenient way to express the
growth rate or order of magnitude of a sum or product.
13.1
The Value of an Annuity
Would you prefer a million dollars today or $50,000 a year for the rest of your life?
On the one hand, instant gratiÔ¨Åcation is nice. On the other hand, the total dollars
received at $50K per year is much larger if you live long enough.
Formally, this is a question about the value of an annuity. An annuity is a Ô¨Ånan-
cial instrument that pays out a Ô¨Åxed amount of money at the beginning of every year
for some speciÔ¨Åed number of years. In particular, an n-year, m-payment annuity
pays m dollars at the start of each year for n years. In some cases, n is Ô¨Ånite, but
not always. Examples include lottery payouts, student loans, and home mortgages.
There are even Wall Street people who specialize in trading annuities.1
A key question is, ‚ÄúWhat is an annuity worth?‚Äù For example, lotteries often pay
out jackpots over many years. Intuitively, $50,000 a year for 20 years ought to be
worth less than a million dollars right now. If you had all the cash right away, you
could invest it and begin collecting interest. But what if the choice were between
$50,000 a year for 20 years and a half million dollars today? Now it is not clear
which option is better.
13.1.1
The Future Value of Money
In order to answer such questions, we need to know what a dollar paid out in the
future is worth today. To model this, let‚Äôs assume that money can be invested at a
Ô¨Åxed annual interest rate p. We‚Äôll assume an 8% rate2 for the rest of the discussion,
so p D 0:08.
Here is why the interest rate p matters. Ten dollars invested today at interest rate
1Such trading ultimately led to the subprime mortgage disaster in 2008‚Äì2009. We‚Äôll talk more
about that in a later chapter.
2U.S. interest rates have dropped steadily for several years, and ordinary bank deposits now earn
around 1.0%. But just a few years ago the rate was 8%; this rate makes some of our examples a little
more dramatic. The rate has been as high as 17% in the past thirty years.

13.1. The Value of an Annuity
445
p will become .1 C p/  10 D 10:80 dollars in a year, .1 C p/2  10  11:66 dollars
in two years, and so forth. Looked at another way, ten dollars paid out a year from
now is only really worth 1=.1 C p/  10  9:26 dollars today. The reason is that if
we had the $9.26 today, we could invest it and would have $10.00 in a year anyway.
Therefore, p determines the value of money paid out in the future.
So for an n-year, m-payment annuity, the Ô¨Årst payment of m dollars is truly worth
m dollars. But the second payment a year later is worth only m=.1 C p/ dollars.
Similarly, the third payment is worth m=.1 C p/2, and the n-th payment is worth
only m=.1 C p/n 1. The total value, V , of the annuity is equal to the sum of the
payment values. This gives:
V D
n
X
iD1
m
.1 C p/i 1
D m 
n 1
X
jD0

1
1 C p
j
(substitute j D i   1)
D m 
n 1
X
jD0
xj
(substitute x D 1=.1 C p/):
(13.3)
The goal of the preceding substitutions was to get the summation into the form
of a simple geometric sum. This leads us to an explanation of a way you could have
discovered the closed form (13.2) in the Ô¨Årst place using the Perturbation Method.
13.1.2
The Perturbation Method
Given a sum that has a nice structure, it is often useful to ‚Äúperturb‚Äù the sum so that
we can somehow combine the sum with the perturbation to get something much
simpler. For example, suppose
S D 1 C x C x2 C    C xn:
An example of a perturbation would be
xS D x C x2 C    C xnC1:
The difference between S and xS is not so great, and so if we were to subtract xS
from S, there would be massive cancellation:
S D 1 C x C x2 C x3 C    C xn
 xS D
  x   x2   x3        xn   xnC1:

Chapter 13
Sums and Asymptotics
446
The result of the subtraction is
S   xS D 1   xnC1:
Solving for S gives the desired closed-form expression in equation 13.2, namely,
S D 1   xnC1
1   x
:
We‚Äôll see more examples of this method when we introduce generating functions
in Chapter 15.
13.1.3
A Closed Form for the Annuity Value
Using equation 13.2, we can derive a simple formula for V , the value of an annuity
that pays m dollars at the start of each year for n years.
V D m
1   xn
1   x

(by equations 13.3 and 13.2)
(13.4)
D m
 
1 C p   .1=.1 C p//n 1
p
!
(substituting x D 1=.1 C p/):
(13.5)
Equation 13.5 is much easier to use than a summation with dozens of terms. For
example, what is the real value of a winning lottery ticket that pays $50,000 per
year for 20 years? Plugging in m D $50,000, n D 20, and p D 0:08 gives
V  $530,180. So because payments are deferred, the million dollar lottery is
really only worth about a half million dollars! This is a good trick for the lottery
advertisers.
13.1.4
InÔ¨Ånite Geometric Series
The question we began with was whether you would prefer a million dollars today
or $50,000 a year for the rest of your life. Of course, this depends on how long
you live, so optimistically assume that the second option is to receive $50,000 a
year forever. This sounds like inÔ¨Ånite money! But we can compute the value of an
annuity with an inÔ¨Ånite number of payments by taking the limit of our geometric
sum in equation 13.2 as n tends to inÔ¨Ånity.
Theorem 13.1.1. If jxj < 1, then
1
X
iD0
xi D
1
1   x :

13.1. The Value of an Annuity
447
Proof.
1
X
iD0
xi WWD lim
n!1
n
X
iD0
xi
D lim
n!1
1   xnC1
1   x
(by equation 13.2)
D
1
1   x :
The Ô¨Ånal line follows from the fact that limn!1 xnC1 D 0 when jxj < 1.

In our annuity problem, x D 1=.1 C p/ < 1, so Theorem 13.1.1 applies, and we
get
V D m 
1
X
jD0
xj
(by equation 13.3)
D m 
1
1   x
(by Theorem 13.1.1)
D m  1 C p
p
.x D 1=.1 C p//:
Plugging in m D $50,000 and p D 0:08, we see that the value V is only $675,000.
Amazingly, a million dollars today is worth much more than $50,000 paid every
year forever! Then again, if we had a million dollars today in the bank earning 8%
interest, we could take out and spend $80,000 a year forever. So on second thought,
this answer really isn‚Äôt so amazing.
13.1.5
Examples
Equation 13.2 and Theorem 13.1.1 are incredibly useful in computer science.
Here are some other common sums that can be put into closed form using equa-

Chapter 13
Sums and Asymptotics
448
tion 13.2 and Theorem 13.1.1:
1 C 1=2 C 1=4 C    D
1
X
iD0
1
2
i
D
1
1   .1=2/ D 2
(13.6)
0:99999    D 0:9
1
X
iD0
 1
10
i
D 0:9
 
1
1   1=10
!
D 0:9
 
10
9
!
D 1
(13.7)
1   1=2 C 1=4      D
1
X
iD0
 1
2
i
D
1
1   . 1=2/ D 2
3
(13.8)
1 C 2 C 4 C    C 2n 1 D
n 1
X
iD0
2i D 1   2n
1   2 D 2n   1
(13.9)
1 C 3 C 9 C    C 3n 1 D
n 1
X
iD0
3i D 1   3n
1   3 D 3n   1
2
(13.10)
If the terms in a geometric sum grow smaller, as in equation 13.6, then the sum is
said to be geometrically decreasing. If the terms in a geometric sum grow progres-
sively larger, as in equations 13.9 and 13.10, then the sum is said to be geometrically
increasing. In either case, the sum is usually approximately equal to the term in the
sum with the greatest absolute value. For example, in equations 13.6 and 13.8, the
largest term is equal to 1 and the sums are 2 and 2/3, both relatively close to 1. In
equation 13.9, the sum is about twice the largest term. In equation 13.10, the largest
term is 3n 1 and the sum is .3n   1/=2, which is only about a factor of 1:5 greater.
You can see why this rule of thumb works by looking carefully at equation 13.2
and Theorem 13.1.1.
13.1.6
Variations of Geometric Sums
We now know all about geometric sums ‚Äîif you have one, life is easy. But in
practice one often encounters sums that cannot be transformed by simple variable
substitutions to the form P xi.
A non-obvious, but useful way to obtain new summation formulas from old ones
is by differentiating or integrating with respect to x. As an example, consider the
following sum:
n 1
X
iD1
ixi D x C 2x2 C 3x3 C    C .n   1/xn 1
This is not a geometric sum, since the ratio between successive terms is not Ô¨Åxed,
and so our formula for the sum of a geometric sum cannot be directly applied. But

13.1. The Value of an Annuity
449
differentiating equation 13.2 leads to:
d
dx
 n 1
X
iD0
xi
!
D d
dx
1   xn
1   x

:
(13.11)
The left-hand side of equation 13.11 is simply
n 1
X
iD0
d
dx .xi/ D
n 1
X
iD0
ixi 1:
The right-hand side of equation 13.11 is
 nxn 1.1   x/   . 1/.1   xn/
.1   x/2
D  nxn 1 C nxn C 1   xn
.1   x/2
D 1   nxn 1 C .n   1/xn
.1   x/2
:
Hence, equation 13.11 means that
n 1
X
iD0
ixi 1 D 1   nxn 1 C .n   1/xn
.1   x/2
:
Incidentally, Problem 13.2 shows how the perturbation method could also be ap-
plied to derive this formula.
Often, differentiating or integrating messes up the exponent of x in every term.
In this case, we now have a formula for a sum of the form P ixi 1, but we want a
formula for the series P ixi. The solution is simple: multiply by x. This gives:
n 1
X
iD1
ixi D x   nxn C .n   1/xnC1
.1   x/2
(13.12)
and we have the desired closed-form expression for our sum3. It‚Äôs a little compli-
cated looking, but it‚Äôs easier to work with than the sum.
Notice that if jxj < 1, then this series converges to a Ô¨Ånite value even if there
are inÔ¨Ånitely many terms. Taking the limit of equation 13.12 as n tends to inÔ¨Ånity
gives the following theorem:
3Since we could easily have made a mistake in the calculation, it is always a good idea to go back
and validate a formula obtained this way with a proof by induction.

Chapter 13
Sums and Asymptotics
450
Theorem 13.1.2. If jxj < 1, then
1
X
iD1
ixi D
x
.1   x/2 :
(13.13)
As a consequence, suppose that there is an annuity that pays im dollars at the
end of each year i forever. For example, if m D $50,000, then the payouts are
$50,000 and then $100,000 and then $150,000 and so on. It is hard to believe that
the value of this annuity is Ô¨Ånite! But we can use Theorem 13.1.2 to compute the
value:
V D
1
X
iD1
im
.1 C p/i
D m  1=.1 C p/
.1  1
1Cp/2
D m  1 C p
p2
:
The second line follows by an application of Theorem 13.1.2. The third line is
obtained by multiplying the numerator and denominator by .1 C p/2.
For example, if m D $50,000, and p D 0:08 as usual, then the value of the
annuity is V D $8,437,500. Even though the payments increase every year, the in-
crease is only additive with time; by contrast, dollars paid out in the future decrease
in value exponentially with time. The geometric decrease swamps out the additive
increase. Payments in the distant future are almost worthless, so the value of the
annuity is Ô¨Ånite.
The important thing to remember is the trick of taking the derivative (or integral)
of a summation formula. Of course, this technique requires one to compute nasty
derivatives correctly, but this is at least theoretically possible!
13.2
Sums of Powers
In Chapter 5, we veriÔ¨Åed the formula (13.1), but the source of this formula is still a
mystery. Sure, we can prove it is true using well ordering or induction, but where
did the expression on the right come from in the Ô¨Årst place? Even more inexplicable
is the closed form expression for the sum of consecutive squares:
n
X
iD1
i2 D .2n C 1/.n C 1/n
6
:
(13.14)

13.2. Sums of Powers
451
It turns out that there is a way to derive these expressions, but before we explain
it, we thought it would be fun4 to show you how Gauss is supposed to have proved
equation 13.1 when he was a young boy.
Gauss‚Äôs idea is related to the perturbation method we used in Section 13.1.2. Let
S D
n
X
iD1
i:
Then we can write the sum in two orders:
S D 1 C
2
C : : : C .n   1/ C n;
S D n C .n   1/ C : : : C
2
C 1:
Adding these two equations gives
2S D .n C 1/ C .n C 1/ C    C .n C 1/ C .n C 1/
D n.n C 1/:
Hence,
S D n.n C 1/
2
:
Not bad for a young child ‚ÄîGauss showed some potential....
Unfortunately, the same trick does not work for summing consecutive squares.
However, we can observe that the result might be a third-degree polynomial in n,
since the sum contains n terms that average out to a value that grows quadratically
in n. So we might guess that
n
X
iD1
i2 D an3 C bn2 C cn C d:
If the guess is correct, then we can determine the parameters a, b, c, and d by
plugging in a few values for n. Each such value gives a linear equation in a, b,
c, and d. If we plug in enough values, we may get a linear system with a unique
solution. Applying this method to our example gives:
n D 0
implies
0 D d
n D 1
implies
1 D a C b C c C d
n D 2
implies
5 D 8a C 4b C 2c C d
n D 3
implies
14 D 27a C 9b C 3c C d:
4OK, our deÔ¨Ånition of ‚Äúfun‚Äù may be different than yours.

Chapter 13
Sums and Asymptotics
452
Solving this system gives the solution a D 1=3, b D 1=2, c D 1=6, d D 0.
Therefore, if our initial guess at the form of the solution was correct, then the
summation is equal to n3=3 C n2=2 C n=6, which matches equation 13.14.
The point is that if the desired formula turns out to be a polynomial, then once
you get an estimate of the degree of the polynomial, all the coefÔ¨Åcients of the
polynomial can be found automatically.
Be careful! This method lets you discover formulas, but it doesn‚Äôt guarantee
they are right! After obtaining a formula by this method, it‚Äôs important to go back
and prove it using induction or some other method, because if the initial guess at
the solution was not of the right form, then the resulting formula will be completely
wrong! A later chapter will describe a method based on generating functions that
does not require any guessing at all.
13.3
Approximating Sums
Unfortunately, it is not always possible to Ô¨Ånd a closed-form expression for a sum.
For example, consider the sum
S D
n
X
iD1
p
i:
No closed form expression is known for S.
In such cases, we need to resort to approximations for S if we want to have a
closed form. The good news is that there is a general method to Ô¨Ånd closed-form
upper and lower bounds that works well for many sums. Even better, the method
is simple and easy to remember. It works by replacing the sum by an integral and
then adding either the Ô¨Årst or last term in the sum.
DeÔ¨Ånition 13.3.1. A function f W RC ! RC is strictly increasing when
x < y IMPLIES f .x/ < f .y/;
and it is weakly increasing5 when
x < y IMPLIES f .x/  f .y/:
5Weakly increasing functions are usually called nondecreasing functions. We will avoid this
terminology to prevent confusion between being a nondecreasing function and the much weaker
property of not being a decreasing function.

13.3. Approximating Sums
453
Similarly, f is strictly decreasing when
x < y IMPLIES f .x/ > f .y/;
and it is weakly decreasing6 when
x < y IMPLIES f .x/  f .y/:
For example, 2x and px are strictly increasing functions, while maxfx; 2g and
dxe are weakly increasing functions. The functions 1=x and 2 x are strictly de-
creasing, while minf1=x; 1=2g and b1=xc are weakly decreasing.
Theorem 13.3.2. Let f W RC ! RC be a weakly increasing function. DeÔ¨Åne
S WWD
n
X
iD1
f .i/
(13.15)
and
I WWD
Z n
1
f .x/ dx:
Then
I C f .1/  S  I C f .n/:
(13.16)
Similarly, if f is weakly decreasing, then
I C f .n/  S  I C f .1/:
Proof. Suppose f W RC ! RC is weakly increasing. The value of the sum S
in (13.15) is the sum of the areas of n unit-width rectangles of heights f .1/; f .2/; : : : ; f .n/.
This area of these rectangles is shown shaded in Figure 13.1.
The value of
I D
Z n
1
f .x/ dx
is the shaded area under the curve of f .x/ from 1 to n shown in Figure 13.2.
Comparing the shaded regions in Figures 13.1 and 13.2 shows that S is at least
I plus the area of the leftmost rectangle. Hence,
S  I C f .1/
(13.17)
This is the lower bound for S given in (13.16).
To derive the upper bound for S given in (13.16), we shift the curve of f .x/
from 1 to n one unit to the left as shown in Figure 13.3.
6Weakly decreasing functions are usually called nonincreasing.

Chapter 13
Sums and Asymptotics
454
0
1
2
3
ÓÄÅ ÓÄÅ ÓÄÅ
ÓÄÅ
 
ÓÄÅ
 
ÓÄÅ
n2
n1
n
f.n/
f.n1/
f.3/
f.2/
f.1/
Figure 13.1
The area of the ith rectangle is f .i/. The shaded region has area
Pn
iD1 f .i/.
0
1
2
3
ÓÄÅ ÓÄÅ ÓÄÅ
n2
n1
n
ÓÄÅ
 
ÓÄÅ
 
ÓÄÅ
f.n/
f.x/
f.n1/
f.3/
f.2/
f.1/
Figure 13.2
The shaded area under the curve of f .x/ from 1 to n (shown in bold)
is I D
R n
1 f .x/ dx.

13.3. Approximating Sums
455
0
1
2
3
ÓÄÅ ÓÄÅ ÓÄÅ
n2
n1
n
ÓÄÅ
 
ÓÄÅ
 
ÓÄÅ
f.n/
f.n1/
f.xC1/
f.3/
f.2/
f.1/
Figure 13.3
This curve is the same as the curve in Figure 13.2 shifted left by 1.
Comparing the shaded regions in Figures 13.1 and 13.3 shows that S is at most
I plus the area of the rightmost rectangle. That is,
S  I C f .n/;
which is the upper bound for S given in (13.16).
The very similar argument for the weakly decreasing case is left to Problem 13.7.

Theorem 13.3.2 provides good bounds for most sums. At worst, the bounds will
be off by the largest term in the sum. For example, we can use Theorem 13.3.2 to
bound the sum
S D
n
X
iD1
p
i
as follows.
We begin by computing
I D
Z n
1
px dx
D x3=2
3=2
ÀáÀáÀáÀáÀá
n
1
D 2
3.n3=2   1/:

Chapter 13
Sums and Asymptotics
456
We then apply Theorem 13.3.2 to conclude that
2
3.n3=2   1/ C 1  S  2
3.n3=2   1/ C pn
and thus that
2
3n3=2 C 1
3  S  2
3n3=2 C pn   2
3:
In other words, the sum is very close to 2
3n3=2.
We‚Äôll be using Theorem 13.3.2 extensively going forward. At the end of this
chapter, we will also introduce some notation that expresses phrases like ‚Äúthe sum
is very close to‚Äù in a more precise mathematical manner. But Ô¨Årst, we‚Äôll see how
Theorem 13.3.2 can be used to resolve a classic paradox in structural engineering.
13.4
Hanging Out Over the Edge
Suppose you have a bunch of books and you want to stack them up, one on top
of another in some off-center way, so the top book sticks out past books below it
without falling over. If you moved the stack to the edge of a table, how far past
the edge of the table do you think you could get the top book to go? Could the top
book stick out completely beyond the edge of table? You‚Äôre not supposed to use
glue or any other support to hold the stack in place.
Most people‚Äôs Ô¨Årst response to this question ‚Äîsometimes also their second and
third responses ‚Äîis ‚ÄúNo, the top book will never get completely past the edge of
the table.‚Äù But in fact, you can get the top book to stick out as far as you want: one
booklength, two booklengths, any number of booklengths!
13.4.1
Formalizing the Problem
We‚Äôll approach this problem recursively. How far past the end of the table can we
get one book to stick out? It won‚Äôt tip as long as its center of mass is over the table,
so we can get it to stick out half its length, as shown in Figure 13.4.
Now suppose we have a stack of books that will not tip over if the bottom book
rests on the table ‚Äîcall that a stable stack. Let‚Äôs deÔ¨Åne the overhang of a stable
stack to be the horizontal distance from the center of mass of the stack to the furthest
edge of the top book. So the overhang is purely a property of the stack, regardless
of its placement on the table. If we place the center of mass of the stable stack at
the edge of the table as in Figure 13.5, the overhang is how far we can get the top
book in the stack to stick out past the edge.

13.4. Hanging Out Over the Edge
457
table
1
2
center of mass
of book
Figure 13.4
One book can overhang half a book length.
table
center of mass
of the whole stack
overhang
Figure 13.5
Overhanging the edge of the table.

Chapter 13
Sums and Asymptotics
458
In general, a stack of n books will be stable if and only if the center of mass of
the top i books sits over the .i C 1/st book for i D 1, 2, ..., n   1.
So we want a formula for the maximum possible overhang, Bn, achievable with
a stable stack of n books.
We‚Äôve already observed that the overhang of one book is 1/2 a book length. That
is,
B1 D 1
2:
Now suppose we have a stable stack of n C 1 books with maximum overhang.
If the overhang of the n books on top of the bottom book was not maximum, we
could get a book to stick out further by replacing the top stack with a stack of n
books with larger overhang. So the maximum overhang, BnC1, of a stack of n C 1
books is obtained by placing a maximum overhang stable stack of n books on top
of the bottom book. And we get the biggest overhang for the stack of n C 1 books
by placing the center of mass of the n books right over the edge of the bottom book
as in Figure 13.6.
So we know where to place the n C 1st book to get maximum overhang. In fact,
the reasoning above actually shows that this way of stacking n C 1-books is the
unique way to build a stable stack where the top book extends as far as possible.
So all we have to do is calculate what this extension is. The simplest way to do
that is to let the center of mass of the top n books be the origin. That way the
horizontal coordinate of the center of mass of the whole stack of n C 1 books will
equal the increase in the overhang. But now the center of mass of the bottom book
has horizontal coordinate 1=2, so the horizontal coordinate of center of mass of the
whole stack of n C 1 books is
0  n C .1=2/  1
n C 1
D
1
2.n C 1/:
In other words,
BnC1 D Bn C
1
2.n C 1/;
(13.18)
as shown in Figure 13.6.
Expanding equation (13.18), we have
BnC1 D Bn 1 C 1
2n C
1
2.n C 1/
D B1 C
1
2  2 C    C 1
2n C
1
2.n C 1/
D 1
2
nC1
X
iD1
1
i :
(13.19)

13.4. Hanging Out Over the Edge
459
table
}
2( n+1)
1
n
top
books
}
center of mass
of top books
n
center of mass
of all +1 books
n
Figure 13.6
Additional overhang with n C 1 books.
So our next task is to examine the behavior of Bn as n grows.
13.4.2
Harmonic Numbers
DeÔ¨Ånition 13.4.1. The nth Harmonic number, Hn, is
Hn WWD
n
X
iD1
1
i :
So (13.19) means that
Bn D Hn
2 :
The Ô¨Årst few Harmonic numbers are easy to compute. For example, H4 D
1C 1
2 C 1
3 C 1
4 D 25
12 > 2. The fact that H4 is greater than 2 has special signiÔ¨Åcance:
it implies that the total extension of a 4-book stack is greater than one full book!
This is the situation shown in Figure 13.7.
There is good news and bad news about Harmonic numbers. The bad news is
that there is no closed-form expression known for the Harmonic numbers. The
good news is that we can use Theorem 13.3.2 to get close upper and lower bounds
on Hn. In particular, since
Z n
1
1
x dx D ln.x/
ÀáÀáÀá
n
1 D ln.n/;
Theorem 13.3.2 means that
ln.n/ C 1
n  Hn  ln.n/ C 1:
(13.20)

Chapter 13
Sums and Asymptotics
460
Table
1/2
1/4
1/6
1/8
Figure 13.7
Stack of four books with maximum overhang.
In other words, the nth Harmonic number is very close to ln.n/.
Because the Harmonic numbers frequently arise in practice, mathematicians
have worked hard to get even better approximations for them. In fact, it is now
known that
Hn D ln.n/ C  C 1
2n C
1
12n2 C .n/
120n4
(13.21)
Here  is a value 0:577215664 : : : called Euler‚Äôs constant, and .n/ is between 0
and 1 for all n. We will not prove this formula.
We are now Ô¨Ånally done with our analysis of the book stacking problem. Plug-
ging the value of Hn into (13.19), we Ô¨Ånd that the maximum overhang for n books
is very close to 1=2 ln.n/. Since ln.n/ grows to inÔ¨Ånity as n increases, this means
that if we are given enough books (in theory anyway), we can get a book to hang
out arbitrarily far over the edge of the table. Of course, the number of books we
need will grow as an exponential function of the overhang; it will take 227 books
just to achieve an overhang of 3, never mind an overhang of 100.
Extending Further Past the End of the Table
The overhang we analyzed above was the furthest out the top could book could
extend past the table. This leaves open the question of there is some better way
to build a stable stack where some book other than the top stuck out furthest. For
example, Figure 13.8 shows a stable stack of two books where the bottom book
extends further out than the top book. Moreover, the bottom book extends 3/4 of a
book length past the end of the table, which is the same as the maximum overhang
for the top book in a two book stack.
Since the two book arrangement in Figure 13.8(a) ties the maximum overhang
stack in Figure 13.8(b), we could take the unique stable stack of n books where the
top book extends furthest, and switch the top two books to look like Figure 13.8(a).
This would give a stable stack of n books where the second from the top book ex-

13.4. Hanging Out Over the Edge
461
1=2
3=4
table
(a)
1=2
1=4
table
(b)
Figure 13.8
Figure (a) shows a stable stack of two books where the bottom book
extends the same amount past the end of the table as the maximum overhang two-
book stack shown in Figure (b).

Chapter 13
Sums and Asymptotics
462
tends the same maximum overhang distance. So for n > 1, there are at least two
ways of building a stable stack of n books which both extend the maximum over-
hang distance ‚Äîone way where top book is furthest out, and another way where
the second from the top book is furthest out.
It turns out that there is no way to beat these two ways of making stable stacks.
In fact, these are the only two ways to get a stable stack of books that achieves
maximum overhang (see Problem 13.12).
But there is more to the story. All our reasoning above was about stacks in which
one book rests on another. It turns out that by building structures in which more
than one book rests on top of another book ‚Äîthink of an inverted pyramid ‚Äîit is
possible to get a stack of n books to extend proportional to
3pn ‚Äîmuch more than
ln n ‚Äîbook lengths without falling over.7
13.4.3
Asymptotic Equality
For cases like equation 13.21 where we understand the growth of a function like Hn
up to some (unimportant) error terms, we use a special notation, , to denote the
leading term of the function. For example, we say that Hn  ln.n/ to indicate that
the leading term of Hn is ln.n/. More precisely:
DeÔ¨Ånition 13.4.2. For functions f; g W R ! R, we say f is asymptotically equal
to g, in symbols,
f .x/  g.x/
iff
lim
x!1 f .x/=g.x/ D 1:
Although it is tempting to write Hn  ln.n/ C  to indicate the two leading
terms, this is not really right. According to DeÔ¨Ånition 13.4.2, Hn  ln.n/ C c
where c is any constant. The correct way to indicate that  is the second-largest
term is Hn   ln.n/  .
The reason that the  notation is useful is that often we do not care about lower
order terms. For example, if n D 100, then we can compute H.n/ to great precision
using only the two leading terms:
jHn   ln.n/   j 
ÀáÀáÀáÀá
1
200  1
120000 C
1
120  1004
ÀáÀáÀáÀá <
1
200:
We will spend a lot more time talking about asymptotic notation at the end of the
chapter. But for now, let‚Äôs get back to using sums.
7See Paterson, M, et al., Maximum Overhang, MAA Monthly, v.116, Nov. 2009, pp.763‚Äì787.

13.5. Products
463
13.5
Products
We‚Äôve covered several techniques for Ô¨Ånding closed forms for sums but no methods
for dealing with products. Fortunately, we do not need to develop an entirely new
set of tools when we encounter a product such as
n≈† WWD
n
Y
iD1
i:
(13.22)
That‚Äôs because we can convert any product into a sum by taking a logarithm. For
example, if
P D
n
Y
iD1
f .i/;
then
ln.P / D
n
X
iD1
ln.f .i//:
We can then apply our summing tools to Ô¨Ånd a closed form (or approximate closed
form) for ln.P / and then exponentiate at the end to undo the logarithm.
For example, let‚Äôs see how this works for the factorial function n≈† We start by
taking the logarithm:
ln.n≈†/ D ln.1  2  3    .n   1/  n/
D ln.1/ C ln.2/ C ln.3/ C    C ln.n   1/ C ln.n/
D
n
X
iD1
ln.i/:
Unfortunately, no closed form for this sum is known. However, we can apply
Theorem 13.3.2 to Ô¨Ånd good closed-form bounds on the sum. To do this, we Ô¨Årst
compute
Z n
1
ln.x/ dx D x ln.x/   x
ÀáÀáÀá
n
1
D n ln.n/   n C 1:
Plugging into Theorem 13.3.2, this means that
n ln.n/   n C 1 
n
X
iD1
ln.i/  n ln.n/   n C 1 C ln.n/:

Chapter 13
Sums and Asymptotics
464
Exponentiating then gives
nn
en 1  n≈†  nnC1
en 1 :
(13.23)
This means that n≈† is within a factor of n of nn=en 1.
13.5.1
Stirling‚Äôs Formula
n≈† is probably the most commonly used product in discrete mathematics, and so
mathematicians have put in the effort to Ô¨Ånd much better closed-form bounds on its
value. The most useful bounds are given in Theorem 13.5.1.
Theorem 13.5.1 (Stirling‚Äôs Formula). For all n  1,
n≈† D
p
2n
n
e
n
e.n/
where
1
12n C 1  .n/ 
1
12n:
Theorem 13.5.1 can be proved by induction on n, but the details are a bit painful
(even for us) and so we will not go through them here.
There are several important things to notice about Stirling‚Äôs Formula. First, .n/
is always positive. This means that
n≈† >
p
2n
n
e
n
(13.24)
for all n 2 NC.
Second, .n/ tends to zero as n gets large. This means that
n≈† 
p
2n
n
e
n
(13.25)
which is rather surprising. After all, who would expect both  and e to show up in
a closed-form expression that is asymptotically equal to n≈†?
Third, .n/ is small even for small values of n. This means that Stirling‚Äôs For-
mula provides good approximations for n≈† for most all values of n. For example, if
we use
p
2n
n
e
n
as the approximation for n≈†, as many people do, we are guaranteed to be within a
factor of
e.n/  e
1
12n

13.6. Double Trouble
465
Approximation
n  1
n  10
n  100
n  1000
p
2n
  n
e
n
< 10%
< 1%
< 0.1%
< 0.01%
p
2n
  n
e
n e1=12n
< 1%
< 0.01%
< 0.0001%
< 0.000001%
Table 13.1
Error bounds on common approximations for n≈† from Theo-
rem 13.5.1.
For example, if n  100, then
p
2n
  n
e
n approximates n≈† to
within 0.1%.
of the correct value. For n  10, this means we will be within 1% of the correct
value. For n  100, the error will be less than 0.1%.
If we need an even closer approximation for n≈†, then we could use either
p
2n
n
e
n
e1=12n
or
p
2n
n
e
n
e1=.12nC1/
depending on whether we want an upper bound or a lower bound, respectively. By
Theorem 13.5.1, we know that both bounds will be within a factor of
e
1
12n  1
12nC1 D e
1
144n2C12n
of the correct value. For n  10, this means that either bound will be within 0.01%
of the correct value. For n  100, the error will be less than 0.0001%.
For quick future reference, these facts are summarized in Corollary 13.5.2 and
Table 13.1.
Corollary 13.5.2.
n≈† <
p
2n
n
e
n

8
ÀÜ<
ÀÜ:
1:09
for n  1;
1:009
for n  10;
1:0009
for n  100:
13.6
Double Trouble
Sometimes we have to evaluate sums of sums, otherwise known as double summa-
tions. This sounds hairy, and sometimes it is. But usually, it is straightforward‚Äî
you just evaluate the inner sum, replace it with a closed form, and then evaluate the

Chapter 13
Sums and Asymptotics
466
outer sum (which no longer has a summation inside it). For example,8
1
X
nD0
 
yn
n
X
iD0
xi
!
D
1
X
nD0

yn 1   xnC1
1   x

equation 13.2
D

1
1   x
 1
X
nD0
yn  
1
1   x
 1
X
nD0
ynxnC1
D
1
.1   x/.1   y/  
x
1   x
 1
X
nD0
.xy/n
Theorem 13.1.1
D
1
.1   x/.1   y/  x
.1   x/.1   xy/
Theorem 13.1.1
D
.1   xy/   x.1   y/
.1   x/.1   y/.1   xy/
D
1   x
.1   x/.1   y/.1   xy/
D
1
.1   y/.1   xy/:
When there‚Äôs no obvious closed form for the inner sum, a special trick that is
often useful is to try exchanging the order of summation. For example, suppose we
want to compute the sum of the Ô¨Årst n Harmonic numbers
n
X
kD1
Hk D
n
X
kD1
k
X
jD1
1
j
(13.26)
For intuition about this sum, we can apply Theorem 13.3.2 to equation 13.20 to
conclude that the sum is close to
Z n
1
ln.x/ dx D x ln.x/   x
ÀáÀáÀá
n
1 D n ln.n/   n C 1:
Now let‚Äôs look for an exact answer. If we think about the pairs .k; j/ over which
8Ok, so maybe this one is a little hairy, but it is also fairly straightforward. Wait till you see the
next one!

13.6. Double Trouble
467
we are summing, they form a triangle:
j
1
2
3
4
5
: : :
n
k
1
1
2
1
1=2
3
1
1=2
1=3
4
1
1=2
1=3
1=4
: : :
n
1
1=2
: : :
1=n
The summation in equation 13.26 is summing each row and then adding the row
sums. Instead, we can sum the columns and then add the column sums. Inspecting
the table we see that this double sum can be written as
n
X
kD1
Hk D
n
X
kD1
k
X
jD1
1
j
D
n
X
j D1
n
X
kDj
1
j
D
n
X
j D1
1
j
n
X
kDj
1
D
n
X
j D1
1
j .n   j C 1/
D
n
X
j D1
n C 1
j
 n
X
jD1
j
j
D .n C 1/
n
X
j D1
1
j  n
X
j D1
1
D .n C 1/Hn   n:
(13.27)

Chapter 13
Sums and Asymptotics
468
13.7
Asymptotic Notation
Asymptotic notation is a shorthand used to give a quick measure of the behavior of
a function f .n/ as n grows large. For example, the asymptotic notation  of DeÔ¨Å-
nition 13.4.2 is a binary relation indicating that two functions grow at the same rate.
There is also a binary relation indicating that one function grows at a signiÔ¨Åcantly
slower rate than another.
13.7.1
Little Oh
DeÔ¨Ånition 13.7.1. For functions f; g W R ! R, with g nonnegative, we say f is
asymptotically smaller than g, in symbols,
f .x/ D o.g.x//;
iff
lim
x!1 f .x/=g.x/ D 0:
For example, 1000x1:9 D o.x2/, because 1000x1:9=x2 D 1000=x0:1 and since
x0:1 goes to inÔ¨Ånity with x and 1000 is constant, we have limx!1 1000x1:9=x2 D
0. This argument generalizes directly to yield
Lemma 13.7.2. xa D o.xb/ for all nonnegative constants a < b.
Using the familiar fact that log x < x for all x > 1, we can prove
Lemma 13.7.3. log x D o.x/ for all  > 0.
Proof. Choose  > ƒ± > 0 and let x D zƒ± in the inequality log x < x. This implies
log z < zƒ±=ƒ± D o.z/
by Lemma 13.7.2:
(13.28)

Corollary 13.7.4. xb D o.ax/ for any a; b 2 R with a > 1.
Lemma 13.7.3 and Corollary 13.7.4 can also be proved using l‚ÄôHÀÜopital‚Äôs Rule or
the McLaurin Series for log x and ex. Proofs can be found in most calculus texts.

13.7. Asymptotic Notation
469
13.7.2
Big Oh
Big Oh is the most frequently used asymptotic notation. It is used to give an upper
bound on the growth of a function, such as the running time of an algorithm.
DeÔ¨Ånition 13.7.5. Given nonnegative functions f; g W R ! R, we say that
f D O.g/
iff
lim sup
x!1
f .x/=g.x/ < 1:
This deÔ¨Ånition9 makes it clear that
Lemma 13.7.6. If f D o.g/ or f  g, then f D O.g/.
Proof. lim f=g D 0 or lim f=g D 1 implies lim f=g < 1.

It is easy to see that the converse of Lemma 13.7.6 is not true. For example,
2x D O.x/, but 2x 6 x and 2x ¬§ o.x/.
The usual formulation of Big Oh spells out the deÔ¨Ånition of lim sup without
mentioning it. Namely, here is an equivalent deÔ¨Ånition:
DeÔ¨Ånition 13.7.7. Given functions f; g W R ! R, we say that
f D O.g/
iff there exists a constant c  0 and an x0 such that for all x  x0, jf .x/j  cg.x/.
This deÔ¨Ånition is rather complicated, but the idea is simple: f .x/ D O.g.x//
means f .x/ is less than or equal to g.x/, except that we‚Äôre willing to ignore a
constant factor, namely, c, and to allow exceptions for small x, namely, x < x0.
We observe,
Lemma 13.7.8. If f D o.g/, then it is not true that g D O.f /.
9We can‚Äôt simply use the limit as x ! 1 in the deÔ¨Ånition of O./, because if f .x/=g.x/ oscillates
between, say, 3 and 5 as x grows, then f D O.g/ because f  5g, but limx!1 f .x/=g.x/
does not exist. So instead of limit, we use the technical notion of lim sup. In this oscillating case,
lim supx!1 f .x/=g.x/ D 5.
The precise deÔ¨Ånition of lim sup is
lim sup
x!1
h.x/ WWD lim
x!1 lubyxh.y/;
where ‚Äúlub‚Äù abbreviates ‚Äúleast upper bound.‚Äù

Chapter 13
Sums and Asymptotics
470
Proof.
lim
x!1
g.x/
f .x/ D
1
limx!1 f .x/=g.x/ D 1
0 D 1;
so g ¬§ O.f /.

Proposition 13.7.9. 100x2 D O.x2/.
Proof. Choose c D 100 and x0 D 1. Then the proposition holds, since for all
x  1,
ÀáÀá100x2ÀáÀá  100x2.

Proposition 13.7.10. x2 C 100x C 10 D O.x2/.
Proof. .x2C100xC10/=x2 D 1C100=xC10=x2 and so its limit as x approaches
inÔ¨Ånity is 1C0C0 D 1. So in fact, x2C100xC10  x2, and therefore x2C100xC
10 D O.x2/. Indeed, it‚Äôs conversely true that x2 D O.x2 C 100x C 10/.

Proposition 13.7.10 generalizes to an arbitrary polynomial:
Proposition 13.7.11. akxk C ak 1xk 1 C    C a1x C a0 D O.xk/.
We‚Äôll omit the routine proof.
Big Oh notation is especially useful when describing the running time of an
algorithm. For example, the usual algorithm for multiplying n  n matrices uses
a number of operations proportional to n3 in the worst case. This fact can be
expressed concisely by saying that the running time is O.n3/. So this asymptotic
notation allows the speed of the algorithm to be discussed without reference to
constant factors or lower-order terms that might be machine speciÔ¨Åc. It turns out
that there is another, ingenious matrix multiplication procedure that uses O.n2:55/
operations. This procedure will therefore be much more efÔ¨Åcient on large enough
matrices. Unfortunately, the O.n2:55/-operation multiplication procedure is almost
never used in practice because it happens to be less efÔ¨Åcient than the usual O.n3/
procedure on matrices of practical size.10
13.7.3
Theta
Sometimes we want to specify that a running time T .n/ is precisely quadratic up to
constant factors (both upper bound and lower bound). We could do this by saying
that T .n/ D O.n2/ and n2 D O.T .n//, but rather than say both, mathematicians
have devised yet another symbol, ‚Äö, to do the job.
10It is even conceivable that there is an O.n2/ matrix multiplication procedure, but none is known.

13.7. Asymptotic Notation
471
DeÔ¨Ånition 13.7.12.
f D ‚Äö.g/
iff
f D O.g/ and g D O.f /:
The statement f D ‚Äö.g/ can be paraphrased intuitively as ‚Äúf and g are equal
to within a constant factor.‚Äù
The Theta notation allows us to highlight growth rates and allow suppression
of distracting factors and low-order terms. For example, if the running time of an
algorithm is
T .n/ D 10n3   20n2 C 1;
then we can more simply write
T .n/ D ‚Äö.n3/:
In this case, we would say that T is of order n3 or that T .n/ grows cubically, which
is probably what we really want to know. Another such example is
23x 7 C .2:7x113 C x9   86/4
px
  1:083x D ‚Äö.3x/:
Just knowing that the running time of an algorithm is ‚Äö.n3/, for example, is use-
ful, because if n doubles we can predict that the running time will by and large11
increase by a factor of at most 8 for large n. In this way, Theta notation preserves in-
formation about the scalability of an algorithm or system. Scalability is, of course,
a big issue in the design of algorithms and systems.
13.7.4
Pitfalls with Asymptotic Notation
There is a long list of ways to make mistakes with asymptotic notation. This section
presents some of the ways that Big Oh notation can lead to ruin and despair. With
minimal effort, you can cause just as much chaos with the other symbols.
The Exponential Fiasco
Sometimes relationships involving Big Oh are not so obvious. For example, one
might guess that 4x D O.2x/ since 4 is only a constant factor larger than 2. This
reasoning is incorrect, however; 4x actually grows as the square of 2x.
11Since ‚Äö.n3/ only implies that the running time, T .n/, is between cn3 and dn3 for constants
0 < c < d, the time T .2n/ could regularly exceed T .n/ by a factor as large as 8d=c. The factor is
sure to be close to 8 for all large n only if T .n/  n3.

Chapter 13
Sums and Asymptotics
472
Constant Confusion
Every constant is O.1/. For example, 17 D O.1/. This is true because if we let
f .x/ D 17 and g.x/ D 1, then there exists a c > 0 and an x0 such that jf .x/j 
cg.x/. In particular, we could choose c = 17 and x0 D 1, since j17j  17  1 for all
x  1. We can construct a false theorem that exploits this fact.
False Theorem 13.7.13.
n
X
iD1
i D O.n/
Bogus proof. DeÔ¨Åne f .n/ D Pn
iD1 i D 1C2C3C  Cn. Since we have shown
that every constant i is O.1/, f .n/ D O.1/ C O.1/ C    C O.1/ D O.n/.

Of course in reality Pn
iD1 i D n.n C 1/=2 ¬§ O.n/.
The error stems from confusion over what is meant in the statement i D O.1/.
For any constant i 2 N it is true that i D O.1/. More precisely, if f is any constant
function, then f D O.1/. But in this False Theorem, i is not constant ‚Äîit ranges
over a set of values 0; 1; : : : ; n that depends on n.
And anyway, we should not be adding O.1/‚Äôs as though they were numbers. We
never even deÔ¨Åned what O.g/ means by itself; it should only be used in the context
‚Äúf D O.g/‚Äù to describe a relation between functions f and g.
Lower Bound Blunder
Sometimes people incorrectly use Big Oh in the context of a lower bound. For
example, they might say, ‚ÄúThe running time, T .n/, is at least O.n2/,‚Äù when they
probably mean ‚Äún2 D O.T .n//.‚Äù 12
Equality Blunder
The notation f D O.g/ is too Ô¨Årmly entrenched to avoid, but the use of ‚Äú=‚Äù
is really regrettable. For example, if f D O.g/, it seems quite reasonable to
write O.g/ D f . But doing so might tempt us to the following blunder: because
2n D O.n/, we can say O.n/ D 2n. But n D O.n/, so we conclude that n D
O.n/ D 2n, and therefore n D 2n. To avoid such nonsense, we will never write
‚ÄúO.f / D g.‚Äù
Similarly, you will often see statements like
Hn D ln.n/ C  C O
1
n

12This would more usually be expressed as ‚ÄúT .n/ D .n2/.‚Äù

13.7. Asymptotic Notation
473
or
n≈† D .1 C o.1//
p
2n
n
e
n
:
In such cases, the true meaning is
Hn D ln.n/ C  C f .n/
for some f .n/ where f .n/ D O.1=n/, and
n≈† D .1 C g.n//
p
2n
n
e
n
where g.n/ D o.1/. These last transgressions are OK as long as you (and your
reader) know what you mean.
13.7.5
Omega
Suppose you want to make a statement of the form ‚Äúthe running time of the algo-
rithm is at least. . . .‚Äù Can you say it is ‚Äúat least O.n2/‚Äù? No! This statement is
meaningless since big-oh can only be used for upper bounds. For lower bounds,
we use a different symbol, called ‚Äúbig-Omega.‚Äù
DeÔ¨Ånition 13.7.14. Given functions f; g W R ! R, deÔ¨Åne
f D .g/
to mean
g D O.f /:
For example, x2 D .x/, 2x D .x2/, and x=100 D .100x C px/.
So if the running time of your algorithm on inputs of size n is T .n/, and you
want to say it is at least quadratic, say
T .n/ D .n2/:
Likewise, there is also a symbol called little-omega, analogous to little-oh, to
denote that one function grows strictly faster than another function.
DeÔ¨Ånition 13.7.15. For functions f; g W R ! R with f nonnegative, deÔ¨Åne
f D !.g/
to mean
g D o.f /:
For example, x1:5 D !.x/ and px D !.ln2.x//.
The little-omega symbol is not as widely used as the other asymptotic symbols
we deÔ¨Åned.

Chapter 13
Sums and Asymptotics
474
Problems for Section 13.1
Class Problems
Problem 13.1.
We begin with two large glasses. The Ô¨Årst glass contains a pint of water, and the
second contains a pint of wine. We pour 1/3 of a pint from the Ô¨Årst glass into the
second, stir up the wine/water mixture in the second glass, and then pour 1/3 of
a pint of the mix back into the Ô¨Årst glass and repeat this pouring back-and-forth
process a total of n times.
(a) Describe a closed form formula for the amount of wine in the Ô¨Årst glass after
n back-and-forth pourings.
(b) What is the limit of the amount of wine in each glass as n approaches inÔ¨Ånity?
Problem 13.2.
You‚Äôve seen this neat trick for evaluating a geometric sum:
S D 1 C z C z2 C : : : C zn
zS D z C z2 C : : : C zn C znC1
S   zS D 1   znC1
S D 1   znC1
1   z
(where z ¬§ 1/
Use the same approach to Ô¨Ånd a closed-form expression for this sum:
T D 1z C 2z2 C 3z3 C : : : C nzn
Homework Problems
Problem 13.3.
Is a Harvard degree really worth more than an MIT degree?! Let us say that a
person with a Harvard degree starts with $40,000 and gets a $20,000 raise every
year after graduation, whereas a person with an MIT degree starts with $30,000,
but gets a 20% raise every year. Assume inÔ¨Çation is a Ô¨Åxed 8% every year. That is,
$1.08 a year from now is worth $1.00 today.
(a) How much is a Harvard degree worth today if the holder will work for n years
following graduation?
(b) How much is an MIT degree worth in this case?

13.7. Asymptotic Notation
475
(c) If you plan to retire after twenty years, which degree would be worth more?
Problem 13.4.
Suppose you deposit $100 into your MIT Credit Union account today, $99 in one
month from now, $98 in two months from now, and so on. Given that the interest
rate is constantly 0.3% per month, how long will it take to save $5,000?
Problems for Section 13.3
Practice Problems
Problem 13.5.
Let
S WWD
5
X
nD1
n
1
3
Using the Integral Method, we can Ô¨Ånd integers, a, b, c, d, and a real number, e,
such that
Z b
a
xe dx  S 
Z d
c
xe dx
What are appropriate values for a‚Äìe ?
Exam Problems
Problem 13.6.
Assume n is an integer larger than 1. Circle all the correct inequalities below.
Explanations are not required, but partial credit for wrong answers will not be
given without them. Hint: You may Ô¨Ånd the graphs in Figure 13.9 helpful.

n
X
iD1
ln.i C 1/  ln 2 C
Z n
1
ln.x C 1/dx

n
X
iD1
ln.i C 1/ 
Z n
0
ln.x C 2/dx

n
X
iD1
1
i 
Z n
0
1
x C 1dx

Chapter 13
Sums and Asymptotics
476
0
1
2
3
4
5
6
7
8
0
0.5
1
1.5
2
2.5
y = ln(x+2)
y = ln(x+1)
0
1
2
3
4
5
6
7
8
0
0.2
0.4
0.6
0.8
1
y = 1/(x+1)
y = 1/x
Figure 13.9
Integral bounds for two sums

13.7. Asymptotic Notation
477
Homework Problems
Problem 13.7.
Let f W RC ! RC be a weakly decreasing function. DeÔ¨Åne
S WWD
n
X
iD1
f .i/
and
I WWD
Z n
1
f .x/ dx:
Prove that
I C f .n/  S  I C f .1/:
Problem 13.8.
Use integration to Ô¨Ånd upper and lower bounds that differ by at most 0.1 for the
following sum. (You may need to add the Ô¨Årst few terms explicitly and then use
integrals to bound the sum of the remaining terms.)
1
X
iD1
1
.2i C 1/2
Problems for Section 13.4
Class Problems
Problem 13.9.
An explorer is trying to reach the Holy Grail, which she believes is located in a
desert shrine d days walk from the nearest oasis. In the desert heat, the explorer
must drink continuously. She can carry at most 1 gallon of water, which is enough
for 1 day. However, she is free to make multiple trips carrying up to a gallon each
time to create water caches out in the desert.
For example, if the shrine were 2=3 of a day‚Äôs walk into the desert, then she could
recover the Holy Grail after two days using the following strategy. She leaves the
oasis with 1 gallon of water, travels 1=3 day into the desert, caches 1=3 gallon, and
then walks back to the oasis‚Äîarriving just as her water supply runs out. Then she
picks up another gallon of water at the oasis, walks 1=3 day into the desert, tops off
her water supply by taking the 1=3 gallon in her cache, walks the remaining 1=3
day to the shrine, grabs the Holy Grail, and then walks for 2=3 of a day back to the
oasis‚Äîagain arriving with no water to spare.
But what if the shrine were located farther away?

Chapter 13
Sums and Asymptotics
478
(a) What is the most distant point that the explorer can reach and then return to
the oasis, with no water precached in the desert, if she takes a total of only 1 gallon
from the oasis?
(b) What is the most distant point the explorer can reach and still return to the
oasis if she takes a total of only 2 gallons from the oasis? No proof is required; just
do the best you can.
(c) The explorer will travel using a recursive strategy to go far into the desert and
back drawing a total of n gallons of water from the oasis. Her strategy is to build
up a cache of n   1 gallons, plus enough to get home, a certain fraction of a day‚Äôs
distance into the desert. On the last delivery to the cache, instead of returning home,
she proceeds recursively with her n   1 gallon strategy to go farther into the desert
and return to the cache. At this point, the cache has just enough water left to get
her home.
Prove that with n gallons of water, this strategy will get her Hn=2 days into the
desert and back, where Hn is the nth Harmonic number:
Hn WWD 1
1 C 1
2 C 1
3 C    C 1
n:
Conclude that she can reach the shrine, however far it is from the oasis.
(d) Suppose that the shrine is d D 10 days walk into the desert. Use the asymp-
totic approximation Hn  ln n to show that it will take more than a million years
for the explorer to recover the Holy Grail.
(e) This is an open-ended question. Unlike with the book-stacking problem in
the text, where we prove by construction the optimality of the number of books
used to get some distance over the edge (for books stacked one by one; we can do
better with, say, inverted pyramids), we don‚Äôt have a proof for the optimality of the
explorer‚Äôs strategy, and the staff is open to suggestions. Can you come up with a
proof or disproof that the explorer‚Äôs strategy is optimal? Groups that come up with
an answer will be awarded accordingly!
Problem 13.10.
There is a number a such that P1
iD1 ip converges iff p < a. What is the value of
a?
Hint: Find a value for a you think that works, then apply the integral bound.

13.7. Asymptotic Notation
479
Homework Problems
Problem 13.11.
There is a bug on the edge of a 1-meter rug. The bug wants to cross to the other
side of the rug. It crawls at 1 cm per second. However, at the end of each second,
a malicious Ô¨Årst-grader named Mildred Anderson stretches the rug by 1 meter. As-
sume that her action is instantaneous and the rug stretches uniformly. Thus, here‚Äôs
what happens in the Ô¨Årst few seconds:
 The bug walks 1 cm in the Ô¨Årst second, so 99 cm remain ahead.
 Mildred stretches the rug by 1 meter, which doubles its length. So now there
are 2 cm behind the bug and 198 cm ahead.
 The bug walks another 1 cm in the next second, leaving 3 cm behind and 197
cm ahead.
 Then Mildred strikes, stretching the rug from 2 meters to 3 meters. So there
are now 3  .3=2/ D 4:5 cm behind the bug and 197  .3=2/ D 295:5 cm
ahead.
 The bug walks another 1 cm in the third second, and so on.
Your job is to determine this poor bug‚Äôs fate.
(a) During second i, what fraction of the rug does the bug cross?
(b) Over the Ô¨Årst n seconds, what fraction of the rug does the bug cross altogether?
Express your answer in terms of the Harmonic number Hn.
(c) The known universe is thought to be about 3  1010 light years in diameter.
How many universe diameters must the bug travel to get to the end of the rug?
(This distance is NOT the inÔ¨Çated distance caused by the stretching but only the
actual walking done by the bug).
Problem 13.12.
TBA - pending
Problems for Section 13.7
Practice Problems
Problem 13.13.
Find the least nonnegative integer, n, such that f .x/ is O.xn/ when f is deÔ¨Åned
by each of the expressions below.

Chapter 13
Sums and Asymptotics
480
(a) 2x3 C .log x/x2
(b) 2x2 + .log x/x3
(c) .1:1/x
(d) .0:1/x
(e) .x4 C x2 C 1/=.x3 C 1/
(f) .x4 C 5 log x/=.x4 C 1/
(g) 2.3 log2 x2/
Problem 13.14.
Let f .n/ D n3. For each function g.n/ in the table below, indicate which of the
indicated asymptotic relations hold.
g.n/
f D O.g/
f D o.g/
g D O.f /
g D o.f /
6   5n   4n2 C 3n3
n3 log n
.sin .n=2/ C 2/ n3
nsin.n=2/C2
log n≈†
e0:2n   100n3
Problem 13.15.
Circle each of the true statements below.
Explanations are not required, but partial credit for wrong answers will not be
given without them.
 n2  n2 C n
 3n D O
 2n
 nsin.n=2/C1 D o
 n2
 n D ‚Äö

3n3
.n C 1/.n   1/


13.7. Asymptotic Notation
481
Problem 13.16.
Show that
ln.n2≈†/ D ‚Äö.n2 ln n/
Problem 13.17.
The quantity
.2n/≈†
22n.n≈†/2
(13.29)
will come up later in the course (it is the probability that in 22n Ô¨Çips of a fair coin,
exactly n will be Heads). Show that it is asymptotically equal to
1
pn.
Homework Problems
Problem 13.18. (a) Prove that log x < x for all x > 1 (requires elementary calcu-
lus).
(b) Prove that the relation, R, on functions such that f R g iff f D o.g/ is a
strict partial order.
(c) Prove that f  g iff f D g C h for some function h D o.g/.
Problem 13.19.
Indicate which of the following holds for each pair of functions .f .n/; g.n// in
the table below. Assume k  1,  > 0, and c > 1 are constants. Pick the four
table entries you consider to be the most challenging or interesting and justify your
answers to these.
f .n/
g.n/
f D O.g/
f D o.g/
g D O.f /
g D o.f /
f D ‚Äö.g/
f  g
2n
2n=2
pn
nsin.n=2/
log.n≈†/
log.nn/
nk
cn
logk n
n

Chapter 13
Sums and Asymptotics
482
Problem 13.20.
Let f , g be nonnegative real-valued functions such that limx!1 f .x/ D 1 and
f  g.
(a) Give an example of f; g such that NOT.2f  2g/.
(b) Prove that log f  log g.
(c) Use Stirling‚Äôs formula to prove that in fact
log.n≈†/  n log n
Problem 13.21.
Determine which of these choices
‚Äö.n/;
‚Äö.n2 log n/;
‚Äö.n2/;
‚Äö.1/;
‚Äö.2n/;
‚Äö.2n ln n/;
none of these
describes each function‚Äôs asymptotic behavior. Full proofs are not required, but
brieÔ¨Çy explain your answers.
(a)
n C ln n C .ln n/2
(b)
n2 C 2n   3
n2   7
(c)
n
X
iD0
22iC1
(d)
ln.n2≈†/
(e)
n
X
kD1
k

1   1
2k

Problem 13.22. (a) Either prove or disprove each of the following statements.
 n≈† D O..n C 1/≈†/

13.7. Asymptotic Notation
483
 .n C 1/≈† D O.n≈†/
 n≈† D ‚Äö..n C 1/≈†/
 n≈† D o..n C 1/≈†/
 .n C 1/≈† D o.n≈†/
(b) Show that
 n
3
nCe D o.n≈†/.
Problem 13.23.
Prove that Pn
kD1 k6 D ‚Äö.n7/.
Class Problems
Problem 13.24.
Give an elementary proof (without appealing to Stirling‚Äôs formula) that log.n≈†/ D
‚Äö.n log n/.
Problem 13.25.
Suppose f; g W NC ! NC and f  g.
(a) Prove that 2f  2g.
(b) Prove that f 2  g2.
(c) Give examples of f and g such that 2f 6 2g.
Problem 13.26.
Recall that for functions f; g on N, f D O.g/ iff
9c 2 N 9n0 2 N 8n  n0
c  g.n/  jf .n/j :
(13.30)
For each pair of functions below, determine whether f D O.g/ and whether
g D O.f /. In cases where one function is O() of the other, indicate the smallest
nonnegative integer, c, and for that smallest c, the smallest corresponding nonneg-
ative integer n0 ensuring that condition (13.30) applies.
(a) f .n/ D n2; g.n/ D 3n.
f D O.g/
YES
NO
If YES, c D
, n0 =
g D O.f /
YES
NO
If YES, c D
, n0 =

Chapter 13
Sums and Asymptotics
484
(b) f .n/ D .3n   7/=.n C 4/; g.n/ D 4
f D O.g/
YES
NO
If YES, c D
, n0 =
g D O.f /
YES
NO
If YES, c D
, n0 =
(c) f .n/ D 1 C .n sin.n=2//2; g.n/ D 3n
f D O.g/
YES
NO
If yes, c D
n0 =
g D O.f /
YES
NO
If yes, c D
n0 =
Problem 13.27.
False Claim.
2n D O.1/:
(13.31)
Explain why the claim is false. Then identify and explain the mistake in the
following bogus proof.
Bogus proof. The proof is by induction on n where the induction hypothesis, P.n/,
is the assertion (13.31).
base case: P.0/ holds trivially.
inductive step: We may assume P.n/, so there is a constant c > 0 such that
2n  c  1. Therefore,
2nC1 D 2  2n  .2c/  1;
which implies that 2nC1 D O.1/. That is, P.n C 1/ holds, which completes the
proof of the inductive step.
We conclude by induction that 2n D O.1/ for all n. That is, the exponential
function is bounded by a constant.

Problem 13.28. (a) Prove that the relation, R, on functions such that f R g iff
f D o.g/ is a strict partial order.
(b) Describe two functions f; g that are incomparable under big Oh:
f ¬§ O.g/ AND g ¬§ O.f /:
Conclude that R is not a path-total order.

13.7. Asymptotic Notation
485
Exam Problems
Problem 13.29. (a) Show that
.an/b=n  1:
where a; b are positive constants and  denotes asymptotic equality. Hint: an D
a2log2 n.
(b) You may assume that if f .n/  1 and g.n/  1 for all n, then f  g  !
f
1
n  g
1
n . Show that
np
n≈† D ‚Äö.n/:
Problem 13.30.
(a) DeÔ¨Åne a function f .n/ such that f D ‚Äö.n2/ and NOT.f  n2/.
(b) DeÔ¨Åne a function g.n/ such that g D O.n2/, g ¬§ ‚Äö.n2/, g ¬§ o.n2/, and
n D O.g/.
Problem 13.31. (a) Show that
.an/b=n  1:
where a; b are positive constants and  denotes asymptotic equality. Hint: an D
a2log2 n.
(b) Show that
np
n≈† D ‚Äö.n/:
Problem 13.32.
(a) Indicate which of the following asymptotic relations below on the set of non-
negative real-valued functions are equivalence relations, (E), strict partial orders
(S), weak partial orders (W), or none of the above (N).
 f  g, the ‚Äúasymptotically Equal‚Äù relation.
 f D o.g/, the ‚Äúlittle Oh‚Äù relation.

Chapter 13
Sums and Asymptotics
486
 f D O.g/, the ‚Äúbig Oh‚Äù relation.
 f D ‚Äö.g/, the ‚ÄúTheta‚Äù relation.
 f D O.g/ AND NOT.g D O.f //.
(b) DeÔ¨Åne two functions f; g that are incomparable under big Oh:
f ¬§ O.g/ AND g ¬§ O.f /:
Problem 13.33.
Recall that if f and g are nonnegative real-valued functions on ZC, then f D O.g/
iff there exist c; n0 2 ZC such that
8n  n0: f .n/  cg.n/:
For each pair of functions f and g below, indicate the smallest c 2 ZC, and
for that smallest c, the smallest corresponding n0 2 ZC, that would establish
f D O.g/ by the deÔ¨Ånition given above. If there is no such c, write 1.
(a) f .n/ D 1
2 ln n2; g.n/ D n.
c D
, n0 =
(b) f .n/ D n; g.n/ D n ln n.
c D
, n0 =
(c) f .n/ D 2n; g.n/ D n4 ln n
c D
, n0 =
(d) f .n/ D 3 sin
.n   1/
100

C 2; g.n/ D 0:2.
c D
, n0 =

14
Cardinality Rules
14.1
Counting One Thing by Counting Another
How do you count the number of people in a crowded room? You could count
heads, since for each person there is exactly one head. Alternatively, you could
count ears and divide by two. Of course, you might have to adjust the calculation
if someone lost an ear in a pirate raid or someone was born with three ears. The
point here is that you can often count one thing by counting another, though some
fudge factors may be required. This is a central theme of counting, from the easiest
problems to the hardest. In fact, we‚Äôve already seen this technique used in Theo-
rem 4.5.5 where the number of subsets of an n-element set was proved to be the
same as the number of length-n bit-strings by describing a bijection between the
subsets and the bit-strings.
The most direct way to count one thing by counting another is to Ô¨Ånd a bijection
between them, since if there is a bijection between two sets, then the sets have the
same size. This important fact is commonly known as the Bijection Rule. We‚Äôve
already seen it as the Mapping Rules bijective case (4.5).
14.1.1
The Bijection Rule
The Bijection Rule acts as a magniÔ¨Åer of counting ability; if you Ô¨Ågure out the size
of one set, then you can immediately determine the sizes of many other sets via
bijections. For example, let‚Äôs look at the two sets mentioned at the beginning of
Part III:
A D all ways to select a dozen donuts when Ô¨Åve varieties are available
B D all 16-bit sequences with exactly 4 ones
An example of an element of set A is:
0 0
‚Äû∆í‚Äö‚Ä¶
chocolate
‚Äû∆í‚Äö‚Ä¶
lemon-Ô¨Ålled
0 0 0 0 0 0
‚Äû
∆í‚Äö
‚Ä¶
sugar
0 0
‚Äû∆í‚Äö‚Ä¶
glazed
0 0
‚Äû∆í‚Äö‚Ä¶
plain
Here, we‚Äôve depicted each donut with a 0 and left a gap between the different
varieties. Thus, the selection above contains two chocolate donuts, no lemon-Ô¨Ålled,
six sugar, two glazed, and two plain. Now let‚Äôs put a 1 into each of the four gaps:
0 0
‚Äû∆í‚Äö‚Ä¶
chocolate
1
‚Äû∆í‚Äö‚Ä¶
lemon-Ô¨Ålled
1
0 0 0 0 0 0
‚Äû
∆í‚Äö
‚Ä¶
sugar
1
0 0
‚Äû∆í‚Äö‚Ä¶
glazed
1
0 0
‚Äû∆í‚Äö‚Ä¶
plain

Chapter 14
Cardinality Rules
488
and close up the gaps:
0011000000100100 :
We‚Äôve just formed a 16-bit number with exactly 4 ones ‚Äîan element of B!
This example suggests a bijection from set A to set B: map a dozen donuts
consisting of:
c chocolate, l lemon-Ô¨Ålled, s sugar, g glazed, and p plain
to the sequence:
0 : : : 0
‚Äû ∆í‚Äö ‚Ä¶
c
1
0 : : : 0
‚Äû ∆í‚Äö ‚Ä¶
l
1
0 : : : 0
‚Äû ∆í‚Äö ‚Ä¶
s
1
0 : : : 0
‚Äû ∆í‚Äö ‚Ä¶
g
1
0 : : : 0
‚Äû ∆í‚Äö ‚Ä¶
p
The resulting sequence always has 16 bits and exactly 4 ones, and thus is an
element of B. Moreover, the mapping is a bijection; every such bit sequence comes
from exactly one order of a dozen donuts. Therefore, jAj D jBj by the Bijection
Rule! More generally,
Lemma 14.1.1. The number of ways to select n donuts when k Ô¨Çavors are available
is the same as the number of length-n binary sequences with k   1 ones.
This example demonstrates the magnifying power of the bijection rule. We man-
aged to prove that two very different sets are actually the same size ‚Äîeven though
we don‚Äôt know exactly how big either one is. But as soon as we Ô¨Ågure out the size
of one set, we‚Äôll immediately know the size of the other.
This particular bijection might seem frighteningly ingenious if you‚Äôve not seen
it before. But you‚Äôll use essentially this same argument over and over, and soon
you‚Äôll consider it routine.
14.2
Counting Sequences
The Bijection Rule lets us count one thing by counting another. This suggests a
general strategy: get really good at counting just a few things and then use bijections
to count everything else. This is the strategy we‚Äôll follow. In particular, we‚Äôll get
really good at counting sequences. When we want to determine the size of some
other set T , we‚Äôll Ô¨Ånd a bijection from T to a set of sequences S. Then we‚Äôll
use our super-ninja sequence-counting skills to determine jSj, which immediately
gives us jT j. We‚Äôll need to hone this idea somewhat as we go along, but that‚Äôs
pretty much the plan!

14.2. Counting Sequences
489
14.2.1
The Product Rule
The Product Rule gives the size of a product of sets. Recall that if P1; P2; : : : ; Pn
are sets, then
P1  P2  : : :  Pn
is the set of all sequences whose Ô¨Årst term is drawn from P1, second term is drawn
from P2 and so forth.
Rule 14.2.1 (Product Rule). If P1; P2; : : : Pn are Ô¨Ånite sets, then:
jP1  P2  : : :  Pnj D jP1j  jP2j    jPnj
For example, suppose a daily diet consists of a breakfast selected from set B, a
lunch from set L, and a dinner from set D where:
B D fpancakes; bacon and eggs; bagel; Doritosg
L D fburger and fries; garden salad; Doritosg
D D fmacaroni; pizza; frozen burrito; pasta; Doritosg
Then BLD is the set of all possible daily diets. Here are some sample elements:
.pancakes; burger and fries; pizza/
.bacon and eggs; garden salad; pasta/
.Doritos; Doritos; frozen burrito/
The Product Rule tells us how many different daily diets are possible:
jB  L  Dj D jBj  jLj  jDj
D 4  3  5
D 60:
14.2.2
Subsets of an n-element Set
The fact that there are 2n subsets of an n-element set was proved in Theorem 4.5.5
by setting up a bijection between the subsets and the length-n bit-strings. So the
original problem about subsets was tranformed into a question about sequences ‚Äî
exactly according to plan! Now we can Ô¨Åll in the missing explanation of why there
are 2n length-n bit-strings: we can write the set of all n-bit sequences as a product
of sets:
f0; 1gn WWD f0; 1g  f0; 1g      f0; 1g
‚Äû
∆í‚Äö
‚Ä¶
n terms
:
Then Product Rule gives the answer:
jf0; 1gnj D jf0; 1gjn D 2n:

Chapter 14
Cardinality Rules
490
14.2.3
The Sum Rule
Bart allocates his little sister Lisa a quota of 20 crabby days, 40 irritable days,
and 60 generally surly days. On how many days can Lisa be out-of-sorts one way
or another? Let set C be her crabby days, I be her irritable days, and S be the
generally surly. In these terms, the answer to the question is jC [ I [ Sj. Now
assuming that she is permitted at most one bad quality each day, the size of this
union of sets is given by the Sum Rule:
Rule 14.2.2 (Sum Rule). If A1; A2; : : : ; An are disjoint sets, then:
jA1 [ A2 [ : : : [ Anj D jA1j C jA2j C : : : C jAnj
Thus, according to Bart‚Äôs budget, Lisa can be out-of-sorts for:
jC [ I [ Sj D jCj C jIj C jSj
D 20 C 40 C 60
D 120 days
Notice that the Sum Rule holds only for a union of disjoint sets. Finding the size
of a union of overlapping sets is a more complicated problem that we‚Äôll take up in
Section 14.9.
14.2.4
Counting Passwords
Few counting problems can be solved with a single rule. More often, a solution is
a Ô¨Çurry of sums, products, bijections, and other methods.
For solving problems involving passwords, telephone numbers, and license plates,
the sum and product rules are useful together. For example, on a certain computer
system, a valid password is a sequence of between six and eight symbols. The Ô¨Årst
symbol must be a letter (which can be lowercase or uppercase), and the remain-
ing symbols must be either letters or digits. How many different passwords are
possible?
Let‚Äôs deÔ¨Åne two sets, corresponding to valid symbols in the Ô¨Årst and subsequent
positions in the password.
F D fa; b; : : : ; z; A; B; : : : ; Zg
S D fa; b; : : : ; z; A; B; : : : ; Z; 0; 1; : : : ; 9g
In these terms, the set of all possible passwords is:1
.F  S5/ [ .F  S6/ [ .F  S7/
1The notation S5 means S  S  S  S  S.

14.3. The Generalized Product Rule
491
Thus, the length-six passwords are in the set F  S5, the length-seven passwords
are in F  S6, and the length-eight passwords are in F  S7. Since these sets
are disjoint, we can apply the Sum Rule and count the total number of possible
passwords as follows:
j.F  S5/ [ .F  S6/ [ .F  S7/j
D jF  S5j C jF  S6j C jF  S7j
Sum Rule
D jF j  jSj5 C jF j  jSj6 C jF j  jSj7
Product Rule
D 52  625 C 52  626 C 52  627
 1:8  1014 different passwords:
14.3
The Generalized Product Rule
In how many ways can, say, a Nobel prize, a Japan prize, and a Pulitzer prize be
awarded to n people? This is easy to answer using our strategy of translating the
problem about awards into a problem about sequences. Let P be the set of n people
taking the course. Then there is a bijection from ways of awarding the three prizes
to the set P 3 WWD P  P  P . In particular, the assignment:
‚ÄúBarak wins a Nobel, George wins a Japan, and Bill wins a Pulitzer prize‚Äù
maps to the sequence .Barak; George; Bill/. By the Product Rule, we have jP 3j D
jP j3 D n3, so there are n3 ways to award the prizes to a class of n people. Notice
that P 3 includes triples like .Barak; Bill; Barak/ where one person wins more than
one prize.
But what if the three prizes must be awarded to different students? As before,
we could map the assignment to the triple .Bill; George; Barak/ 2 P 3. But this
function is no longer a bijection. For example, no valid assignment maps to the
triple .Barak; Bill; Barak/ because now we‚Äôre not allowing Barak to receive two
prizes. However, there is a bijection from prize assignments to the set:
S D f.x; y; z/ 2 P 3 j x, y, and z are different peopleg
This reduces the original problem to a problem of counting sequences. Unfortu-
nately, the Product Rule does not apply directly to counting sequences of this type
because the entries depend on one another; in particular, they must all be different.
However, a slightly sharper tool does the trick.

Chapter 14
Cardinality Rules
492
Prizes for truly exceptional Coursework
Given everyone‚Äôs hard work on this material, the instructors considered award-
ing some prizes for truly exceptional coursework. Here are three possible prize
categories:
Best Administrative Critique We asserted that the quiz was closed-book. On
the cover page, one strong candidate for this award wrote, ‚ÄúThere is no
book.‚Äù
Awkward Question Award ‚ÄúOkay, the left sock, right sock, and pants are in
an antichain, but how ‚Äîeven with assistance ‚Äîcould I put on all three at
once?‚Äù
Best Collaboration Statement Inspired by a student who wrote ‚ÄúI worked alone‚Äù
on Quiz 1.
Rule 14.3.1 (Generalized Product Rule). Let S be a set of length-k sequences. If
there are:
 n1 possible Ô¨Årst entries,
 n2 possible second entries for each Ô¨Årst entry,
:::
 nk possible kth entries for each sequence of Ô¨Årst k   1 entries,
then:
jSj D n1  n2  n3    nk
In the awards example, S consists of sequences .x; y; z/. There are n ways to
choose x, the recipient of prize #1. For each of these, there are n 1 ways to choose
y, the recipient of prize #2, since everyone except for person x is eligible. For each
combination of x and y, there are n   2 ways to choose z, the recipient of prize #3,
because everyone except for x and y is eligible. Thus, according to the Generalized
Product Rule, there are
jSj D n  .n   1/  .n   2/
ways to award the 3 prizes to different people.

14.3. The Generalized Product Rule
493
14.3.1
Defective Dollar Bills
A dollar bill is defective if some digit appears more than once in the 8-digit serial
number. If you check your wallet, you‚Äôll be sad to discover that defective bills
are all-too-common. In fact, how common are nondefective bills? Assuming that
the digit portions of serial numbers all occur equally often, we could answer this
question by computing
fraction of nondefective bills D jfserial #‚Äôs with all digits differentgj
jfserial numbersgj
:
(14.1)
Let‚Äôs Ô¨Årst consider the denominator. Here there are no restrictions; there are 10
possible Ô¨Årst digits, 10 possible second digits, 10 third digits, and so on. Thus, the
total number of 8-digit serial numbers is 108 by the Product Rule.
Next, let‚Äôs turn to the numerator. Now we‚Äôre not permitted to use any digit twice.
So there are still 10 possible Ô¨Årst digits, but only 9 possible second digits, 8 possible
third digits, and so forth. Thus, by the Generalized Product Rule, there are
10  9  8  7  6  5  4  3 D 10≈†
2 D 1;814;400
serial numbers with all digits different. Plugging these results into Equation 14.1,
we Ô¨Ånd:
fraction of nondefective bills D
1;814;400
100;000;000 D 1:8144%
14.3.2
A Chess Problem
In how many different ways can we place a pawn (P ), a knight (N ), and a bishop
(B) on a chessboard so that no two pieces share a row or a column? A valid con-
Ô¨Åguration is shown in Figure 14.1(a), and an invalid conÔ¨Åguration is shown in Fig-
ure 14.1(b).
First, we map this problem about chess pieces to a question about sequences.
There is a bijection from conÔ¨Ågurations to sequences
.rP ; cP ; rN ; cN ; rB; cB/
where rP , rN , and rB are distinct rows and cP , cN , and cB are distinct columns.
In particular, rP is the pawn‚Äôs row, cP is the pawn‚Äôs column, rN is the knight‚Äôs
row, etc. Now we can count the number of such sequences using the Generalized
Product Rule:
 rP is one of 8 rows

Chapter 14
Cardinality Rules
494
8 0Z0Z0Z0Z
7 Z0Z0m0Z0
6 0Z0Z0Z0Z
5 Z0Z0Z0Z0
4 0a0Z0Z0Z
3 Z0Z0Z0Z0
2 0Z0Z0o0Z
1 Z0Z0Z0Z0
a
b
c
d
e
f
g
h
(a) valid
8 0Z0Z0Z0Z
7 Z0Z0Z0Z0
6 0Z0ZpZ0Z
5 Z0Z0Z0Z0
4 0Z0Z0Z0Z
3 Z0a0ZnZ0
2 0Z0Z0Z0Z
1 Z0Z0Z0Z0
a
b
c
d
e
f
g
h
(b) invalid
Figure 14.1
Two ways of placing a pawn (p), a knight (N), and a bishop (B) on
a chessboard. The conÔ¨Åguration shown in (b) is invalid because the bishop and the
knight are in the same row.
 cP is one of 8 columns
 rN is one of 7 rows (any one but rP )
 cN is one of 7 columns (any one but cP )
 rB is one of 6 rows (any one but rP or rN )
 cB is one of 6 columns (any one but cP or cN )
Thus, the total number of conÔ¨Ågurations is .8  7  6/2.
14.3.3
Permutations
A permutation of a set S is a sequence that contains every element of S exactly
once. For example, here are all the permutations of the set fa; b; cg:
.a; b; c/
.a; c; b/
.b; a; c/
.b; c; a/
.c; a; b/
.c; b; a/
How many permutations of an n-element set are there? Well, there are n choices
for the Ô¨Årst element. For each of these, there are n   1 remaining choices for the
second element. For every combination of the Ô¨Årst two elements, there are n   2
ways to choose the third element, and so forth. Thus, there are a total of
n  .n   1/  .n   2/    3  2  1 D n≈†
permutations of an n-element set. In particular, this formula says that there are

14.4. The Division Rule
495
3≈† D 6 permutations of the 3-element set fa; b; cg, which is the number we found
above.
Permutations will come up again in this course approximately 1.6 bazillion times.
In fact, permutations are the reason why factorial comes up so often and why we
taught you Stirling‚Äôs approximation:
n≈† 
p
2n
n
e
n
:
14.4
The Division Rule
Counting ears and dividing by two is a silly way to count the number of people in
a room, but this approach is representative of a powerful counting principle.
A k-to-1 function maps exactly k elements of the domain to every element of
the codomain. For example, the function mapping each ear to its owner is 2-to-1.
Similarly, the function mapping each Ô¨Ånger to its owner is 10-to-1, and the function
mapping each Ô¨Ånger and toe to its owner is 20-to-1. The general rule is:
Rule 14.4.1 (Division Rule). If f W A ! B is k-to-1, then jAj D k  jBj.
For example, suppose A is the set of ears in the room and B is the set of people.
There is a 2-to-1 mapping from ears to people, so by the Division Rule, jAj D
2  jBj. Equivalently, jBj D jAj=2, expressing what we knew all along: the number
of people is half the number of ears. Unlikely as it may seem, many counting
problems are made much easier by initially counting every item multiple times and
then correcting the answer using the Division Rule. Let‚Äôs look at some examples.
14.4.1
Another Chess Problem
In how many different ways can you place two identical rooks on a chessboard
so that they do not share a row or column? A valid conÔ¨Åguration is shown in
Figure 14.2(a), and an invalid conÔ¨Åguration is shown in Figure 14.2(b).
Let A be the set of all sequences
.r1; c1; r2; c2/
where r1 and r2 are distinct rows and c1 and c2 are distinct columns. Let B be the
set of all valid rook conÔ¨Ågurations. There is a natural function f from set A to set
B; in particular, f maps the sequence .r1; c1; r2; c2/ to a conÔ¨Åguration with one
rook in row r1, column c1 and the other rook in row r2, column c2.

Chapter 14
Cardinality Rules
496
8 0Z0Z0Z0s
7 Z0Z0Z0Z0
6 0Z0Z0Z0Z
5 Z0Z0Z0Z0
4 0Z0Z0Z0Z
3 Z0Z0Z0Z0
2 0Z0Z0Z0Z
1 s0Z0Z0Z0
a
b
c
d
e
f
g
h
(a) valid
8 0Z0Z0Z0Z
7 Z0Z0Z0Z0
6 0Z0s0Z0Z
5 Z0Z0Z0Z0
4 0Z0Z0Z0Z
3 Z0Z0Z0Z0
2 0Z0Z0Z0Z
1 Z0ZrZ0Z0
a
b
c
d
e
f
g
h
(b) invalid
Figure 14.2
Two ways to place 2 rooks (R) on a chessboard. The conÔ¨Åguration
in (b) is invalid because the rooks are in the same column.
But now there‚Äôs a snag. Consider the sequences:
.1; 1; 8; 8/
and
.8; 8; 1; 1/
The Ô¨Årst sequence maps to a conÔ¨Åguration with a rook in the lower-left corner and
a rook in the upper-right corner. The second sequence maps to a conÔ¨Åguration with
a rook in the upper-right corner and a rook in the lower-left corner. The problem is
that those are two different ways of describing the same conÔ¨Åguration! In fact, this
arrangement is shown in Figure 14.2(a).
More generally, the function f maps exactly two sequences to every board con-
Ô¨Åguration; that is f is a 2-to-1 function. Thus, by the quotient rule, jAj D 2  jBj.
Rearranging terms gives:
jBj D jAj
2 D .8  7/2
2
:
On the second line, we‚Äôve computed the size of A using the General Product Rule
just as in the earlier chess problem.
14.4.2
Knights of the Round Table
In how many ways can King Arthur arrange to seat his n different knights at his
round table? Two seatings are considered to be the same arrangement if they yield
the same sequence of knights starting at knight number 1 and going clockwise
around the table. For example, the following two seatings determine the same
arrangement:

14.4. The Division Rule
497
"!
# 
k1
k2
k3
k4
"!
# 
k3
k4
k1
k2
So a seating is determined by the sequence of knights going clockwise around
the table starting at the top seat. This means seatings are formally the same as the
set, A, of all permutations of the knights. An arrangement is determined by the
sequence of knights going clockwise around the table starting after knight number
1, so it is formally the same as the set, B, of all permutations of knights 2 through
n. We can map each permutation in A to an arrangement in set B by seating the
Ô¨Årst knight in the permutation at the top of the table, putting the second knight to
his left, the third knight to the left of the second, and so forth all the way around
the table. For example:
.k2; k4; k1; k3/
 !
"!
# 
k1
k3
k2
k4
This mapping is actually an n-to-1 function from A to B, since all n cyclic shifts of
the original sequence map to the same seating arrangement. In the example, n D 4
different sequences map to the same seating arrangement:
.k2; k4; k1; k3/
.k4; k1; k3; k2/
.k1; k3; k2; k4/
.k3; k2; k4; k1/
 !
"!
# 
k1
k3
k2
k4
Therefore, by the division rule, the number of circular seating arrangements is:
jBj D jAj
n D n≈†
n D .n   1/≈†
Note that jAj D n≈† since there are n≈† permutations of n knights.

Chapter 14
Cardinality Rules
498
14.5
Counting Subsets
How many k-element subsets of an n-element set are there? This question arises
all the time in various guises:
 In how many ways can I select 5 books from my collection of 100 to bring
on vacation?
 How many different 13-card Bridge hands can be dealt from a 52-card deck?
 In how many ways can I select 5 toppings for my pizza if there are 14 avail-
able toppings?
This number comes up so often that there is a special notation for it:
 
n
k
!
WWD the number of k-element subsets of an n-element set.
The expression
 n
k

is read ‚Äún choose k.‚Äù Now we can immediately express the
answers to all three questions above:
 I can select 5 books from 100 in
 100
5

ways.
 There are
 52
13

different Bridge hands.
 There are
 14
5

different 5-topping pizzas, if 14 toppings are available.
14.5.1
The Subset Rule
We can derive a simple formula for the n choose k number using the Division Rule.
We do this by mapping any permutation of an n-element set fa1; : : : ; ang into a k-
element subset simply by taking the Ô¨Årst k elements of the permutation. That is,
the permutation a1a2 : : : an will map to the set fa1; a2; : : : ; akg.
Notice that any other permutation with the same Ô¨Årst k elements a1; : : : ; ak in
any order and the same remaining elements n   k elements in any order will also
map to this set. What‚Äôs more, a permutation can only map to fa1; a2; : : : ; akg
if its Ô¨Årst k elements are the elements a1; : : : ; ak in some order. Since there are
k≈† possible permutations of the Ô¨Årst k elements and .n   k/≈† permutations of the
remaining elements, we conclude from the Product Rule that exactly k≈†.n   k/≈†
permutations of the n-element set map to the particular subset, S. In other words,
the mapping from permutations to k-element subsets is k≈†.n   k/≈†-to-1.

14.5. Counting Subsets
499
But we know there are n≈† permutations of an n-element set, so by the Division
Rule, we conclude that
n≈† D k≈†.n   k/≈†
 
n
k
!
which proves:
Rule 14.5.1 (Subset Rule). The number of k-element subsets of an n-element set is
 
n
k
!
D
n≈†
k≈† .n   k/≈†:
Notice that this works even for 0-element subsets: n≈†=0≈†n≈† D 1. Here we use the
fact that 0≈† is a product of 0 terms, which by convention2 equals 1.
14.5.2
Bit Sequences
How many n-bit sequences contain exactly k ones? We‚Äôve already seen the straight-
forward bijection between subsets of an n-element set and n-bit sequences. For
example, here is a 3-element subset of fx1; x2; : : : ; x8g and the associated 8-bit
sequence:
f
x1;
x4;
x5
g
.
1;
0;
0;
1;
1;
0;
0;
0
/
Notice that this sequence has exactly 3 ones, each corresponding to an element
of the 3-element subset. More generally, the n-bit sequences corresponding to a
k-element subset will have exactly k ones. So by the Bijection Rule,
Corollary 14.5.2. The number of n-bit sequences with exactly k ones is
 
n
k
!
.
Also, the bijection between selections of Ô¨Çavored donuts and bit sequences of
Lemma 14.1.1 now implies,
Corollary 14.5.3. The number of ways to select n donuts when k Ô¨Çavors are avail-
able is
 
n C .k   1/
n
!
:
2We don‚Äôt use it here, but a sum of zero terms equals 0.

Chapter 14
Cardinality Rules
500
14.6
Sequences with Repetitions
14.6.1
Sequences of Subsets
Choosing a k-element subset of an n-element set is the same as splitting the set
into a pair of subsets: the Ô¨Årst subset of size k and the second subset consisting of
the remaining n   k elements. So the Subset Rule can be understood as a rule for
counting the number of such splits into pairs of subsets.
We can generalize this to splits into more than two subsets. Namely, let A be
an n-element set and k1; k2; : : : ; km be nonnegative integers whose sum is n. A
.k1; k2; : : : ; km/-split of A is a sequence
.A1; A2; : : : ; Am/
where the Ai are disjoint subsets of A and jAij D ki for i D 1; : : : ; m.
To count the number of splits we take the same approach as for the Subset
Rule. Namely, we map any permutation a1a2 : : : an of an n-element set A into
a .k1; k2; : : : ; km/-split by letting the 1st subset in the split be the Ô¨Årst k1 elements
of the permutation, the 2nd subset of the split be the next k2 elements, ..., and the
mth subset of the split be the Ô¨Ånal km elements of the permutation. This map is
a k1≈† k2≈†    km≈†-to-1 function from the n≈† permutations to the .k1; k2; : : : ; km/-
splits of A, so from the Division Rule we conclude the Subset Split Rule:
DeÔ¨Ånition 14.6.1. For n; k1; : : : ; km 2 N, such that k1Ck2C  Ckm D n, deÔ¨Åne
the multinomial coefÔ¨Åcient
 
n
k1; k2; : : : ; km
!
WWD
n≈†
k1≈† k2≈† : : : km≈†:
Rule 14.6.2 (Subset Split Rule). The number of .k1; k2; : : : ; km/-splits of an n-
element set is
 
n
k1; : : : ; km
!
:
14.6.2
The Bookkeeper Rule
We can also generalize our count of n-bit sequences with k ones to counting se-
quences of n letters over an alphabet with more than two letters. For example,
how many sequences can be formed by permuting the letters in the 10-letter word
BOOKKEEPER?

14.6. Sequences with Repetitions
501
Notice that there are 1 B, 2 O‚Äôs, 2 K‚Äôs, 3 E‚Äôs, 1 P, and 1 R in BOOKKEEPER. This
leads to a straightforward bijection between permutations of BOOKKEEPER and
(1,2,2,3,1,1)-splits of f1; 2; : : : ; 10g. Namely, map a permutation to the sequence
of sets of positions where each of the different letters occur.
For example, in the permutation BOOKKEEPER itself, the B is in the 1st posi-
tion, the O‚Äôs occur in the 2nd and 3rd positions, K‚Äôs in 4th and 5th, the E‚Äôs in the
6th, 7th and 9th, P in the 8th, and R is in the 10th position. So BOOKKEEPER
maps to
.f1g; f2; 3g; f4; 5g; f6; 7; 9g; f8g; f10g/:
From this bijection and the Subset Split Rule, we conclude that the number of ways
to rearrange the letters in the word BOOKKEEPER is:
total letters
‚Äö‚Ä¶‚Äû∆í
10≈†
1≈†
‚Äû∆í‚Äö‚Ä¶
B‚Äôs
2≈†
‚Äû∆í‚Äö‚Ä¶
O‚Äôs
2≈†
‚Äû∆í‚Äö‚Ä¶
K‚Äôs
3≈†
‚Äû∆í‚Äö‚Ä¶
E‚Äôs
1≈†
‚Äû∆í‚Äö‚Ä¶
P‚Äôs
1≈†
‚Äû∆í‚Äö‚Ä¶
R‚Äôs
This example generalizes directly to an exceptionally useful counting principle
which we will call the
Rule 14.6.3 (Bookkeeper Rule). Let l1; : : : ; lm be distinct elements. The number
of sequences with k1 occurrences of l1, and k2 occurrences of l2, ..., and km
occurrences of lm is
 
k1 C k2 C    C km
k1; : : : ; km
!
:
For example, suppose you are planning a 20-mile walk, which should include 5
northward miles, 5 eastward miles, 5 southward miles, and 5 westward miles. How
many different walks are possible?
There is a bijection between such walks and sequences with 5 N‚Äôs, 5 E‚Äôs, 5 S‚Äôs,
and 5 W‚Äôs. By the Bookkeeper Rule, the number of such sequences is:
20≈†
.5≈†/4 :
A Word about Words
Someday you might refer to the Subset Split Rule or the Bookkeeper Rule in front
of a roomful of colleagues and discover that they‚Äôre all staring back at you blankly.
This is not because they‚Äôre dumb, but rather because we made up the name ‚ÄúBook-
keeper Rule.‚Äù However, the rule is excellent and the name is apt, so we suggest

Chapter 14
Cardinality Rules
502
that you play through: ‚ÄúYou know? The Bookkeeper Rule? Don‚Äôt you guys know
anything???‚Äù
The Bookkeeper Rule is sometimes called the ‚Äúformula for permutations with
indistinguishable objects.‚Äù The size k subsets of an n-element set are sometimes
called k-combinations. Other similar-sounding descriptions are ‚Äúcombinations with
repetition, permutations with repetition, r-permutations, permutations with indis-
tinguishable objects,‚Äù and so on. However, the counting rules we‚Äôve taught you are
sufÔ¨Åcient to solve all these sorts of problems without knowing this jargon, so we
won‚Äôt burden you with it.
14.6.3
The Binomial Theorem
Counting gives insight into one of the basic theorems of algebra. A binomial is a
sum of two terms, such as a C b. Now consider its 4th power, .a C b/4.
By repeatedly using distributivity of products over sums to multiply out this 4th
power expression completely, we get
.a C b/4
D
aaaa
C
aaab
C
aaba
C
aabb
C
abaa
C
abab
C
abba
C
abbb
C
baaa
C
baab
C
baba
C
babb
C
bbaa
C
bbab
C
bbba
C
bbbb
Notice that there is one term for every sequence of a‚Äôs and b‚Äôs. So there are 24
terms, and the number of terms with k copies of b and n   k copies of a is:
n≈†
k≈† .n   k/≈† D
 
n
k
!
by the Bookkeeper Rule. Hence, the coefÔ¨Åcient of an kbk is
 n
k

. So for n D 4,
this means:
.a C b/4 D
 
4
0
!
 a4b0 C
 
4
1
!
 a3b1 C
 
4
2
!
 a2b2 C
 
4
3
!
 a1b3 C
 
4
4
!
 a0b4
In general, this reasoning gives the Binomial Theorem:
Theorem 14.6.4 (Binomial Theorem). For all n 2 N and a; b 2 R:
.a C b/n D
n
X
kD0
 
n
k
!
an kbk

14.7. Counting Practice: Poker Hands
503
The Binomial Theorem explains why the n choose k number is called a binomial
coefÔ¨Åcient.
This reasoning about binomials extends nicely to multinomials, which are sums
of two or more terms. For example, suppose we wanted the coefÔ¨Åcient of
bo2k2e3pr
in the expansion of .b C o C k C e C p C r/10. Each term in this expansion is a
product of 10 variables where each variable is one of b, o, k, e, p, or r. Now, the
coefÔ¨Åcient of bo2k2e3pr is the number of those terms with exactly 1 b, 2 o‚Äôs, 2
k‚Äôs, 3 e‚Äôs, 1 p, and 1 r. And the number of such terms is precisely the number of
rearrangements of the word BOOKKEEPER:
 
10
1; 2; 2; 3; 1; 1
!
D
10≈†
1≈† 2≈† 2≈† 3≈† 1≈† 1≈†:
This reasoning extends to a general theorem.
Theorem 14.6.5 (Multinomial Theorem). For all n 2 N,
.z1 C z2 C    C zm/n D
X
k1;:::;km2N
k1CCkmDn
 
n
k1; k2; : : : ; km
!
zk1
1 zk2
2    zkm
m :
You‚Äôll be better off remembering the reasoning behind the Multinomial Theorem
rather than this cumbersome formal statement.
14.7
Counting Practice: Poker Hands
Five-Card Draw is a card game in which each player is initially dealt a hand con-
sisting of 5 cards from a deck of 52 cards.3 (Then the game gets complicated, but
3There are 52 cards in a standard deck. Each card has a suit and a rank. There are four suits:
 (spades)
~ (hearts)
| (clubs)
} (diamonds)
And there are 13 ranks, listed here from lowest to highest:
Ace
A ; 2 ; 3 ; 4 ; 5 ; 6 ; 7 ; 8 ; 9 ;
Jack
J ;
Queen
Q
;
King
K :
Thus, for example, 8~ is the 8 of hearts and A is the ace of spades.

Chapter 14
Cardinality Rules
504
let‚Äôs not worry about that.) The number of different hands in Five-Card Draw is the
number of 5-element subsets of a 52-element set, which is
 
52
5
!
D 2; 598; 960:
Let‚Äôs get some counting practice by working out the number of hands with various
special properties.
14.7.1
Hands with a Four-of-a-Kind
A Four-of-a-Kind is a set of four cards with the same rank. How many different
hands contain a Four-of-a-Kind? Here are a couple examples:
f8; 8}; Q~; 8~; 8|g
fA|; 2|; 2~; 2}; 2g
As usual, the Ô¨Årst step is to map this question to a sequence-counting problem. A
hand with a Four-of-a-Kind is completely described by a sequence specifying:
1. The rank of the four cards.
2. The rank of the extra card.
3. The suit of the extra card.
Thus, there is a bijection between hands with a Four-of-a-Kind and sequences con-
sisting of two distinct ranks followed by a suit. For example, the three hands above
are associated with the following sequences:
.8; Q; ~/ $ f 8; 8}; 8~; 8|; Q~g
.2; A; |/ $ f2|; 2~; 2}; 2; A|g
Now we need only count the sequences. There are 13 ways to choose the Ô¨Årst rank,
12 ways to choose the second rank, and 4 ways to choose the suit. Thus, by the
Generalized Product Rule, there are 13  12  4 D 624 hands with a Four-of-a-Kind.
This means that only 1 hand in about 4165 has a Four-of-a-Kind. Not surprisingly,
Four-of-a-Kind is considered to be a very good poker hand!

14.7. Counting Practice: Poker Hands
505
14.7.2
Hands with a Full House
A Full House is a hand with three cards of one rank and two cards of another rank.
Here are some examples:
f2; 2|; 2}; J |; J }g
f5}; 5|; 5~; 7~; 7|g
Again, we shift to a problem about sequences. There is a bijection between Full
Houses and sequences specifying:
1. The rank of the triple, which can be chosen in 13 ways.
2. The suits of the triple, which can be selected in
 4
3

ways.
3. The rank of the pair, which can be chosen in 12 ways.
4. The suits of the pair, which can be selected in
 4
2

ways.
The example hands correspond to sequences as shown below:
.2; f; |; }g; J; f|; }g/ $ f2; 2|; 2}; J |; J }g
.5; f}; |; ~g; 7; f~; |g/ $ f5}; 5|; 5~; 7~; 7|g
By the Generalized Product Rule, the number of Full Houses is:
13 
 
4
3
!
 12 
 
4
2
!
:
We‚Äôre on a roll ‚Äîbut we‚Äôre about to hit a speed bump.
14.7.3
Hands with Two Pairs
How many hands have Two Pairs; that is, two cards of one rank, two cards of
another rank, and one card of a third rank? Here are examples:
f3}; 3; Q}; Q~; A|g
f9~; 9}; 5~; 5|; Kg
Each hand with Two Pairs is described by a sequence consisting of:
1. The rank of the Ô¨Årst pair, which can be chosen in 13 ways.
2. The suits of the Ô¨Årst pair, which can be selected
 4
2

ways.

Chapter 14
Cardinality Rules
506
3. The rank of the second pair, which can be chosen in 12 ways.
4. The suits of the second pair, which can be selected in
 4
2

ways.
5. The rank of the extra card, which can be chosen in 11 ways.
6. The suit of the extra card, which can be selected in
 4
1

D 4 ways.
Thus, it might appear that the number of hands with Two Pairs is:
13 
 
4
2
!
 12 
 
4
2
!
 11  4:
Wrong answer! The problem is that there is not a bijection from such sequences to
hands with Two Pairs. This is actually a 2-to-1 mapping. For example, here are the
pairs of sequences that map to the hands given above:
.3; f}; g; Q; f}; ~g; A; |/
&
f3}; 3; Q}; Q~; A|g
.Q; f}; ~g; 3; f}; g; A; |/
%
.9; f~; }g; 5; f~; |g; K; /
&
f9~; 9}; 5~; 5|; Kg
.5; f~; |g; 9; f~; }g; K; /
%
The problem is that nothing distinguishes the Ô¨Årst pair from the second. A pair of
5‚Äôs and a pair of 9‚Äôs is the same as a pair of 9‚Äôs and a pair of 5‚Äôs. We avoided this
difÔ¨Åculty in counting Full Houses because, for example, a pair of 6‚Äôs and a triple of
kings is different from a pair of kings and a triple of 6‚Äôs.
We ran into precisely this difÔ¨Åculty last time, when we went from counting ar-
rangements of different pieces on a chessboard to counting arrangements of two
identical rooks. The solution then was to apply the Division Rule, and we can do
the same here. In this case, the Division rule says there are twice as many sequences
as hands, so the number of hands with Two Pairs is actually:
13 
 4
2

 12 
 4
2

 11  4
2
:
Another Approach
The preceding example was disturbing! One could easily overlook the fact that the
mapping was 2-to-1 on an exam, fail the course, and turn to a life of crime. You
can make the world a safer place in two ways:

14.7. Counting Practice: Poker Hands
507
1. Whenever you use a mapping f W A ! B to translate one counting problem
to another, check that the same number elements in A are mapped to each
element in B. If k elements of A map to each of element of B, then apply
the Division Rule using the constant k.
2. As an extra check, try solving the same problem in a different way. Multiple
approaches are often available ‚Äîand all had better give the same answer!
(Sometimes different approaches give answers that look different, but turn
out to be the same after some algebra.)
We already used the Ô¨Årst method; let‚Äôs try the second. There is a bijection be-
tween hands with two pairs and sequences that specify:
1. The ranks of the two pairs, which can be chosen in
 13
2

ways.
2. The suits of the lower-rank pair, which can be selected in
 4
2

ways.
3. The suits of the higher-rank pair, which can be selected in
 4
2

ways.
4. The rank of the extra card, which can be chosen in 11 ways.
5. The suit of the extra card, which can be selected in
 4
1

D 4 ways.
For example, the following sequences and hands correspond:
.f3; Qg; f}; g; f}; ~g; A; |/ $ f3}; 3; Q}; Q~; A|g
.f9; 5g; f~; |g; f~; }g; K; / $ f9~; 9}; 5~; 5|; Kg
Thus, the number of hands with two pairs is:
 
13
2
!

 
4
2
!

 
4
2
!
 11  4:
This is the same answer we got before, though in a slightly different form.
14.7.4
Hands with Every Suit
How many hands contain at least one card from every suit? Here is an example of
such a hand:
f7}; K|; 3}; A~; 2g
Each such hand is described by a sequence that speciÔ¨Åes:
1. The ranks of the diamond, the club, the heart, and the spade, which can be
selected in 13  13  13  13 D 134 ways.

Chapter 14
Cardinality Rules
508
2. The suit of the extra card, which can be selected in 4 ways.
3. The rank of the extra card, which can be selected in 12 ways.
For example, the hand above is described by the sequence:
.7; K; A; 2; }; 3/ $ f7}; K|; A~; 2; 3}g:
Are there other sequences that correspond to the same hand? There is one more!
We could equally well regard either the 3} or the 7} as the extra card, so this
is actually a 2-to-1 mapping. Here are the two sequences corresponding to the
example hand:
.7; K; A; 2; }; 3/
&
f7}; K|; A~; 2; 3}g
.3; K; A; 2; }; 7/
%
Therefore, the number of hands with every suit is:
134  4  12
2
:
14.8
The Pigeonhole Principle
Here is an old puzzle:
A drawer in a dark room contains red socks, green socks, and blue
socks. How many socks must you withdraw to be sure that you have a
matching pair?
For example, picking out three socks is not enough; you might end up with one
red, one green, and one blue. The solution relies on the
Pigeonhole Principle
If there are more pigeons than holes they occupy, then at least two
pigeons must be in the same hole.

14.8. The Pigeonhole Principle
509
1st sock
A
f
2nd sock
3rd sock
4th sock
red
B
green
blue
Figure 14.3
One possible mapping of four socks to three colors.
What pigeons have to do with selecting footwear under poor lighting conditions
may not be immediately obvious, but if we let socks be pigeons and the colors be
three pigeonholes, then as soon as you pick four socks, there are bound to be two
in the same hole, that is, with the same color. So four socks are enough to ensure
a matched pair. For example, one possible mapping of four socks to three colors is
shown in Figure 14.3.
A rigorous statement of the Principle goes this way:
Rule 14.8.1 (Pigeonhole Principle). If jAj > jBj, then for every total function
f W A ! B, there exist two different elements of A that are mapped by f to the
same element of B.
Stating the Principle this way may be less intuitive, but it should now sound
familiar: it is simply the contrapositive of the Mapping Rules injective case (4.4).
Here, the pigeons form set A, the pigeonholes are the set B, and f describes which
hole each pigeon occupies.
Mathematicians have come up with many ingenious applications for the pigeon-
hole principle. If there were a cookbook procedure for generating such arguments,
we‚Äôd give it to you. Unfortunately, there isn‚Äôt one. One helpful tip, though: when
you try to solve a problem with the pigeonhole principle, the key is to clearly iden-
tify three things:
1. The set A (the pigeons).
2. The set B (the pigeonholes).
3. The function f (the rule for assigning pigeons to pigeonholes).

Chapter 14
Cardinality Rules
510
14.8.1
Hairs on Heads
There are a number of generalizations of the pigeonhole principle. For example:
Rule 14.8.2 (Generalized Pigeonhole Principle). If jAj > k  jBj, then every total
function f W A ! B maps at least kC1 different elements of A to the same element
of B.
For example, if you pick two people at random, surely they are extremely un-
likely to have exactly the same number of hairs on their heads. However, in the
remarkable city of Boston, Massachusetts there are actually three people who have
exactly the same number of hairs! Of course, there are many bald people in Boston,
and they all have zero hairs. But we‚Äôre talking about non-bald people; say a person
is non-bald if they have at least ten thousand hairs on their head.
Boston has about 500,000 non-bald people, and the number of hairs on a person‚Äôs
head is at most 200,000. Let A be the set of non-bald people in Boston, let B D
f10; 000; 10; 001; : : : ; 200; 000g, and let f map a person to the number of hairs on
his or her head. Since jAj > 2jBj, the Generalized Pigeonhole Principle implies
that at least three people have exactly the same number of hairs. We don‚Äôt know
who they are, but we know they exist!
14.8.2
Subsets with the Same Sum
For your reading pleasure, we have displayed ninety 25-digit numbers in Fig-
ure 14.4. Are there two different subsets of these 25-digit numbers that have the
same sum? For example, maybe the sum of the last ten numbers in the Ô¨Årst column
is equal to the sum of the Ô¨Årst eleven numbers in the second column?
Finding two subsets with the same sum may seem like a silly puzzle, but solving
these sorts of problems turns out to be useful in diverse applications such as Ô¨Ånding
good ways to Ô¨Åt packages into shipping containers and decoding secret messages.
It turns out that it is hard to Ô¨Ånd different subsets with the same sum, which
is why this problem arises in cryptography. But it is easy to prove that two such
subsets exist. That‚Äôs where the Pigeonhole Principle comes in.
Let A be the collection of all subsets of the 90 numbers in the list. Now the sum
of any subset of numbers is at most 90  1025, since there are only 90 numbers and
every 25-digit number is less than 1025. So let B be the set of integers f0; 1; : : : ; 90
1025g, and let f map each subset of numbers (in A) to its sum (in B).
We proved that an n-element set has 2n different subsets in Section 14.2. There-
fore:
jAj D 290  1:237  1027

14.8. The Pigeonhole Principle
511
0020480135385502964448038
3171004832173501394113017
5763257331083479647409398
8247331000042995311646021
0489445991866915676240992
3208234421597368647019265
5800949123548989122628663
8496243997123475922766310
1082662032430379651370981
3437254656355157864869113
6042900801199280218026001
8518399140676002660747477
1178480894769706178994993
3574883393058653923711365
6116171789137737896701405
8543691283470191452333763
1253127351683239693851327
3644909946040480189969149
6144868973001582369723512
8675309258374137092461352
1301505129234077811069011
3790044132737084094417246
6247314593851169234746152
8694321112363996867296665
1311567111143866433882194
3870332127437971355322815
6814428944266874963488274
8772321203608477245851154
1470029452721203587686214
4080505804577801451363100
6870852945543886849147881
8791422161722582546341091
1578271047286257499433886
4167283461025702348124920
6914955508120950093732397
9062628024592126283973285
1638243921852176243192354
4235996831123777788211249
6949632451365987152423541
9137845566925526349897794
1763580219131985963102365
4670939445749439042111220
7128211143613619828415650
9153762966803189291934419
1826227795601842231029694
4815379351865384279613427
7173920083651862307925394
9270880194077636406984249
1843971862675102037201420
4837052948212922604442190
7215654874211755676220587
9324301480722103490379204
2396951193722134526177237
5106389423855018550671530
7256932847164391040233050
9436090832146695147140581
2781394568268599801096354
5142368192004769218069910
7332822657075235431620317
9475308159734538249013238
2796605196713610405408019
5181234096130144084041856
7426441829541573444964139
9492376623917486974923202
2931016394761975263190347
5198267398125617994391348
7632198126531809327186321
9511972558779880288252979
2933458058294405155197296
5317592940316231219758372
7712154432211912882310511
9602413424619187112552264
3075514410490975920315348
5384358126771794128356947
7858918664240262356610010
9631217114906129219461111
8149436716871371161932035
3157693105325111284321993
3111474985252793452860017
5439211712248901995423441
7898156786763212963178679
9908189853102753335981319
3145621587936120118438701
5610379826092838192760458
8147591017037573337848616
9913237476341764299813987
3148901255628881103198549
5632317555465228677676044
5692168374637019617423712
8176063831682536571306791
Figure 14.4
Ninety 25-digit numbers. Can you Ô¨Ånd two different subsets of these
numbers that have the same sum?

Chapter 14
Cardinality Rules
512
On the other hand:
jBj D 90  1025 C 1  0:901  1027:
Both quantities are enormous, but jAj is a bit greater than jBj. This means that f
maps at least two elements of A to the same element of B. In other words, by the
Pigeonhole Principle, two different subsets must have the same sum!
Notice that this proof gives no indication which two sets of numbers have the
same sum. This frustrating variety of argument is called a nonconstructive proof.
The $100 prize for two same-sum subsets
To see if was possible to actually Ô¨Ånd two different subsets of the ninety 25-digit
numbers with the same sum, we offered a $100 prize to the Ô¨Årst student who did it.
We didn‚Äôt expect to have to pay off this bet, but we underestimated the ingenuity
and initiative of the students. One computer science major wrote a program that
cleverly searched only among a reasonably small set of ‚Äúplausible‚Äù sets, sorted
them by their sums, and actually found a couple with the same sum. He won the
prize. A few days later, a math major Ô¨Ågured out how to reformulate the sum
problem as a ‚Äúlattice basis reduction‚Äù problem; then he found a software package
implementing an efÔ¨Åcient basis reduction procedure, and using it, he very quickly
found lots of pairs of subsets with the same sum. He didn‚Äôt win the prize, but he
got a standing ovation from the class ‚Äîstaff included.

14.8. The Pigeonhole Principle
513
The $500 Prize for Sets with Distinct Subset Sums
How can we construct a set of n positive integers such that all its subsets have
distinct sums? One way is to use powers of two:
f1; 2; 4; 8; 16g
This approach is so natural that one suspects all other such sets must involve
larger numbers. (For example, we could safely replace 16 by 17, but not by 15.)
Remarkably, there are examples involving smaller numbers. Here is one:
f6; 9; 11; 12; 13g
One of the top mathematicians of the Twentieth Century, Paul ErdÀùos, conjectured
in 1931 that there are no such sets involving signiÔ¨Åcantly smaller numbers. More
precisely, he conjectured that the largest number in such a set must be greater
than c2n for some constant c > 0. He offered $500 to anyone who could prove
or disprove his conjecture, but the problem remains unsolved.
14.8.3
A Magic Trick
A Magician sends an Assistant into the audience with a deck of 52 cards while the
Magician looks away.
Five audience members each select one card from the deck. The Assistant then
gathers up the Ô¨Åve cards and holds up four of them so the Magician can see them.
The Magician concentrates for a short time and then correctly names the secret,
Ô¨Åfth card!
Since we don‚Äôt really believe the Magician can read minds, we know the Assis-
tant has somehow communicated the secret card to the Magician. Real Magicians
and Assistants are not to be trusted, so we expect that the Assistant would secretly
signal the Magician with coded phrases or body language, but for this trick they
don‚Äôt have to cheat. In fact, the Magician and Assistant could be kept out of sight
of each other while some audience member holds up the 4 cards designated by the
Assistant for the Magician to see.
Of course, without cheating, there is still an obvious way the Assistant can com-
municate to the Magician: he can choose any of the 4≈† D 24 permutations of the
4 cards as the order in which to hold up the cards. However, this alone won‚Äôt
quite work: there are 48 cards remaining in the deck, so the Assistant doesn‚Äôt have
enough choices of orders to indicate exactly what the secret card is (though he
could narrow it down to two cards).

Chapter 14
Cardinality Rules
514
f8~;KÓÅø;QÓÅø;2};6}g
f8~;KÓÅø;QÓÅø;9|;6}g
fKÓÅø;8~;6};QÓÅøg
fKÓÅø;8~;QÓÅø;2}g
f8~;KÓÅø;QÓÅø;2}g
ÓÄÅ
ÓÄÅ
ÓÄÅ
ÓÄÅ
ÓÄÅ
ÓÄÅ
ÓÄÅ
ÓÄÅ
ÓÄÅ
ÓÄÅ
ÓÄÅ
ÓÄÅ
ÓÄÅ
ÓÄÅ
ÓÄÅ
xDall
sets of 
5 cards
yDall
sequences of 4
distinct cards
Figure 14.5
The bipartite graph where the nodes on the left correspond to sets
of 5 cards and the nodes on the right correspond to sequences of 4 cards. There is
an edge between a set and a sequence whenever all the cards in the sequence are
contained in the set.
14.8.4
The Secret
The method the Assistant can use to communicate the Ô¨Åfth card exactly is a nice
application of what we know about counting and matching.
The Assistant has a second legitimate way to communicate: he can choose which
of the Ô¨Åve cards to keep hidden. Of course, it‚Äôs not clear how the Magician could
determine which of these Ô¨Åve possibilities the Assistant selected by looking at the
four visible cards, but there is a way, as we‚Äôll now explain.
The problem facing the Magician and Assistant is actually a bipartite matching
problem. Each vertex on left will correspond to the information available to the
Assistant, namely, a set of 5 cards. So the set X of left hand vertices will have
 52
5

elements.
Each vertex on right will correspond to the information available to the Magician,
namely, a sequence of 4 distinct cards. So the set Y of right hand vertices will have
52515049 elements. When the audience selects a set of 5 cards, then the Assistant
must reveal a sequence of 4 cards from that hand. This constraint is represented by
having an edge between a set of 5 cards on the left and a sequence of 4 cards on the
right precisely when every card in the sequence is also in the set. This speciÔ¨Åes the
bipartite graph. Some edges are shown in the diagram in Figure 14.5.

14.8. The Pigeonhole Principle
515
For example,
f8~; K; Q; 2}; 6}g
(14.2)
is an element of X on the left. If the audience selects this set of 5 cards, then
there are many different 4-card sequences on the right in set Y that the Assis-
tant could choose to reveal, including .8~; K; Q; 2}/, .K; 8~; Q; 2}/, and
.K; 8~; 6}; Q/.
What the Magician and his Assistant need to perform the trick is a matching for
the X vertices. If they agree in advance on some matching, then when the audience
selects a set of 5 cards, the Assistant reveals the matching sequence of 4 cards. The
Magician uses the matching to Ô¨Ånd the audience‚Äôs chosen set of 5 cards, and so he
can name the one not already revealed.
For example, suppose the Assistant and Magician agree on a matching containing
the two bold edges in Figure 14.5. If the audience selects the set
f8~; K; Q; 9|; 6}g;
(14.3)
then the Assistant reveals the corresponding sequence
.K; 8~; 6}; Q/:
(14.4)
Using the matching, the Magician sees that the hand (14.3) is matched to the se-
quence (14.4), so he can name the one card in the corresponding set not already
revealed, namely, the 9|. Notice that the fact that the sets are matched, that is,
that different sets are paired with distinct sequences, is essential. For example, if
the audience picked the previous hand (14.2), it would be possible for the Assistant
to reveal the same sequence (14.4), but he better not do that; if he did, then the
Magician would have no way to tell if the remaining card was the 9| or the 2}.
So how can we be sure the needed matching can be found? The answer is that
each vertex on the left has degree 54≈† D 120, since there are Ô¨Åve ways to select the
card kept secret and there are 4≈† permutations of the remaining 4 cards. In addition,
each vertex on the right has degree 48, since there are 48 possibilities for the Ô¨Åfth
card. So this graph is degree-constrained according to DeÔ¨Ånition 11.5.5, and so has
a matching by Theorem 11.5.6.
In fact, this reasoning shows that the Magician could still pull off the trick if 120
cards were left instead of 48, that is, the trick would work with a deck as large as
124 different cards ‚Äîwithout any magic!
14.8.5
The Real Secret
But wait a minute! It‚Äôs all very well in principle to have the Magician and his
Assistant agree on a matching, but how are they supposed to remember a matching

Chapter 14
Cardinality Rules
516
A
2
3
4
5
6
7
8
9
10
J
Q
K
Figure 14.6
The 13 card ranks arranged in cyclic order.
with
 52
5

D 2; 598; 960 edges? For the trick to work in practice, there has to be a
way to match hands and card sequences mentally and on the Ô¨Çy.
We‚Äôll describe one approach. As a running example, suppose that the audience
selects:
10~
9}
3~
Q
J }:
 The Assistant picks out two cards of the same suit. In the example, the
assistant might choose the 3~ and 10~. This is always possible because of
the Pigeonhole Principle ‚Äîthere are Ô¨Åve cards and 4 suits so two cards must
be in the same suit.
 The Assistant locates the ranks of these two cards on the cycle shown in Fig-
ure 14.6. For any two distinct ranks on this cycle, one is always between 1
and 6 hops clockwise from the other. For example, the 3~ is 6 hops clock-
wise from the 10~.
 The more counterclockwise of these two cards is revealed Ô¨Årst, and the other
becomes the secret card. Thus, in our example, the 10~ would be revealed,
and the 3~ would be the secret card. Therefore:
‚Äì The suit of the secret card is the same as the suit of the Ô¨Årst card re-
vealed.
‚Äì The rank of the secret card is between 1 and 6 hops clockwise from the
rank of the Ô¨Årst card revealed.

14.8. The Pigeonhole Principle
517
 All that remains is to communicate a number between 1 and 6. The Magician
and Assistant agree beforehand on an ordering of all the cards in the deck
from smallest to largest such as:
A| A} A~ A 2| 2} 2~ 2 : : : K~ K
The order in which the last three cards are revealed communicates the num-
ber according to the following scheme:
. small;
medium;
large
/
= 1
. small;
large;
medium /
= 2
. medium;
small;
large
/
= 3
. medium;
large;
small /
= 4
.
large;
small;
medium /
= 5
.
large;
medium;
small /
= 6
In the example, the Assistant wants to send 6 and so reveals the remaining
three cards in large, medium, small order. Here is the complete sequence that
the Magician sees:
10~
Q
J }
9}
 The Magician starts with the Ô¨Årst card, 10~, and hops 6 ranks clockwise to
reach 3~, which is the secret card!
So that‚Äôs how the trick can work with a standard deck of 52 cards. On the other
hand, Hall‚Äôs Theorem implies that the Magician and Assistant can in principle per-
form the trick with a deck of up to 124 cards. It turns out that there is a method
which they could actually learn to use with a reasonable amount of practice for a
124-card deck, but we won‚Äôt explain it here.4
14.8.6
The Same Trick with Four Cards?
Suppose that the audience selects only four cards and the Assistant reveals a se-
quence of three to the Magician. Can the Magician determine the fourth card?
Let X be all the sets of four cards that the audience might select, and let Y be all
the sequences of three cards that the Assistant might reveal. Now, on one hand, we
have
jXj D
 
52
4
!
D 270; 725
4See The Best Card Trick by Michael Kleber for more information.

Chapter 14
Cardinality Rules
518
by the Subset Rule. On the other hand, we have
jY j D 52  51  50 D 132; 600
by the Generalized Product Rule. Thus, by the Pigeonhole Principle, the Assistant
must reveal the same sequence of three cards for at least
270; 725
132; 600

D 3
different four-card hands. This is bad news for the Magician: if he sees that se-
quence of three, then there are at least three possibilities for the fourth card which
he cannot distinguish. So there is no legitimate way for the Assistant to communi-
cate exactly what the fourth card is!
14.9
Inclusion-Exclusion
How big is a union of sets? For example, suppose there are 60 math majors, 200
EECS majors, and 40 physics majors. How many students are there in these three
departments? Let M be the set of math majors, E be the set of EECS majors, and
P be the set of physics majors. In these terms, we‚Äôre asking for jM [ E [ P j.
The Sum Rule says that if M, E, and P are disjoint, then the sum of their sizes
is
jM [ E [ P j D jMj C jEj C jP j:
However, the sets M, E, and P might not be disjoint. For example, there might
be a student majoring in both math and physics. Such a student would be counted
twice on the right side of this equation, once as an element of M and once as an
element of P . Worse, there might be a triple-major5 counted three times on the
right side!
Our most-complicated counting rule determines the size of a union of sets that
are not necessarily disjoint. Before we state the rule, let‚Äôs build some intuition by
considering some easier special cases: unions of just two or three sets.
14.9.1
Union of Two Sets
For two sets, S1 and S2, the Inclusion-Exclusion Rule is that the size of their union
is:
jS1 [ S2j D jS1j C jS2j   jS1 \ S2j
(14.5)
5. ..though not at MIT anymore.

14.9. Inclusion-Exclusion
519
Intuitively, each element of S1 is accounted for in the Ô¨Årst term, and each element
of S2 is accounted for in the second term. Elements in both S1 and S2 are counted
twice ‚Äîonce in the Ô¨Årst term and once in the second. This double-counting is
corrected by the Ô¨Ånal term.
14.9.2
Union of Three Sets
So how many students are there in the math, EECS, and physics departments? In
other words, what is jM [ E [ P j if:
jMj D 60
jEj D 200
jP j D 40:
The size of a union of three sets is given by a more complicated Inclusion-Exclusion
formula:
jS1 [ S2 [ S3j D jS1j C jS2j C jS3j
  jS1 \ S2j   jS1 \ S3j   jS2 \ S3j
C jS1 \ S2 \ S3j:
Remarkably, the expression on the right accounts for each element in the union of
S1, S2, and S3 exactly once. For example, suppose that x is an element of all three
sets. Then x is counted three times (by the jS1j, jS2j, and jS3j terms), subtracted
off three times (by the jS1 \ S2j, jS1 \ S3j, and jS2 \ S3j terms), and then counted
once more (by the jS1 \ S2 \ S3j term). The net effect is that x is counted just
once.
If x is in two sets (say, S1 and S2), then x is counted twice (by the jS1j and
jS2j terms) and subtracted once (by the jS1 \ S2j term). In this case, x does not
contribute to any of the other terms, since x ‚Ä¶ S3.
So we can‚Äôt answer the original question without knowing the sizes of the various
intersections. Let‚Äôs suppose that there are:
4
math - EECS double majors
3
math - physics double majors
11
EECS - physics double majors
2
triple majors
Then jM \Ej D 4C2, jM \P j D 3C2, jE\P j D 11C2, and jM \E\P j D 2.

Chapter 14
Cardinality Rules
520
Plugging all this into the formula gives:
jM [ E [ P j D jMj C jEj C jP j   jM \ Ej   jM \ P j   jE \ P j C jM \ E \ P j
D 60 C 200 C 40   6   5   13 C 2
D 278
14.9.3
Sequences with 42, 04, or 60
In how many permutations of the set f0; 1; 2; : : : ; 9g do either 4 and 2, 0 and 4, or
6 and 0 appear consecutively? For example, none of these pairs appears in:
.7; 2; 9; 5; 4; 1; 3; 8; 0; 6/:
The 06 at the end doesn‚Äôt count; we need 60. On the other hand, both 04 and 60
appear consecutively in this permutation:
.7; 2; 5; 6; 0; 4; 3; 8; 1; 9/:
Let P42 be the set of all permutations in which 42 appears. DeÔ¨Åne P60 and P04
similarly. Thus, for example, the permutation above is contained in both P60 and
P04, but not P42. In these terms, we‚Äôre looking for the size of the set P42 [ P04 [
P60.
First, we must determine the sizes of the individual sets, such as P60. We can use
a trick: group the 6 and 0 together as a single symbol. Then there is an immediate
bijection between permutations of f0; 1; 2; : : : 9g containing 6 and 0 consecutively
and permutations of:
f60; 1; 2; 3; 4; 5; 7; 8; 9g:
For example, the following two sequences correspond:
.7; 2; 5; 6; 0; 4; 3; 8; 1; 9/  ! .7; 2; 5; 60; 4; 3; 8; 1; 9/:
There are 9≈† permutations of the set containing 60, so jP60j D 9≈† by the Bijection
Rule. Similarly, jP04j D jP42j D 9≈† as well.
Next, we must determine the sizes of the two-way intersections, such as P42 \
P60. Using the grouping trick again, there is a bijection with permutations of the
set:
f42; 60; 1; 3; 5; 7; 8; 9g:
Thus, jP42 \ P60j D 8≈†. Similarly, jP60 \ P04j D 8≈† by a bijection with the set:
f604; 1; 2; 3; 5; 7; 8; 9g:

14.9. Inclusion-Exclusion
521
And jP42 \ P04j D 8≈† as well by a similar argument. Finally, note that jP60 \
P04 \ P42j D 7≈† by a bijection with the set:
f6042; 1; 3; 5; 7; 8; 9g:
Plugging all this into the formula gives:
jP42 [ P04 [ P60j D 9≈† C 9≈† C 9≈†   8≈†   8≈†   8≈† C 7≈†:
14.9.4
Union of n Sets
The size of a union of n sets is given by the following rule.
Rule 14.9.1 (Inclusion-Exclusion).
jS1 [ S2 [    [ Snj D
the sum of the sizes of the individual sets
minus
the sizes of all two-way intersections
plus
the sizes of all three-way intersections
minus
the sizes of all four-way intersections
plus
the sizes of all Ô¨Åve-way intersections, etc.
The formulas for unions of two and three sets are special cases of this general
rule.
This way of expressing Inclusion-Exclusion is easy to understand and nearly
as precise as expressing it in mathematical symbols, but we‚Äôll need the symbolic
version below, so let‚Äôs work on deciphering it now.
We already have a concise notation for the sum of sizes of the individual sets,
namely,
n
X
iD1
jSij:
A ‚Äútwo-way intersection‚Äù is a set of the form Si \Sj for i ¬§ j. We regard Sj \Si
as the same two-way intersection as Si \ Sj , so we can assume that i < j. Now
we can express the sum of the sizes of the two-way intersections as
X
1i<j n
jSi \ Sj j:
Similarly, the sum of the sizes of the three-way intersections is
X
1i<j <kn
jSi \ Sj \ Skj:

Chapter 14
Cardinality Rules
522
These sums have alternating signs in the Inclusion-Exclusion formula, with the
sum of the k-way intersections getting the sign . 1/k 1. This Ô¨Ånally leads to a
symbolic version of the rule:
Rule (Inclusion-Exclusion).
ÀáÀáÀáÀáÀá
n
[
iD1
Si
ÀáÀáÀáÀáÀá D
n
X
iD1
jSij
 X
1i<j n
jSi \ Sj j
C
X
1i<j <kn
jSi \ Sj \ Skj C   
C . 1/n 1
ÀáÀáÀáÀáÀá
n
\
iD1
Si
ÀáÀáÀáÀáÀá :
While it‚Äôs often handy express the rule in this way as a sum of sums, it is not
necessary to group the terms by how many sets are in the intersections. So another
way to state the rule is:
Rule (Inclusion-Exclusion-II).
ÀáÀáÀáÀáÀá
n
[
iD1
Si
ÀáÀáÀáÀáÀá D
X
;¬§If1;:::;ng
. 1/jIjC1
ÀáÀáÀáÀáÀá
\
i2I
Si
ÀáÀáÀáÀáÀá
A proof of these rules using just highschool algebra is given in Problem 14.48.
14.9.5
Computing Euler‚Äôs Function
As an example, let‚Äôs use Inclusion-Exclusion to derive an explicit formula (14.6)
for Euler‚Äôs function, .n/. By deÔ¨Ånition, .n/ is the number of nonnegative inte-
gers less than a positive integer n that are relatively prime to n. But the set S of
nonnegative integers less than n that are not relatively prime to n will be easier to
count.
Suppose the prime factorization of n is pe1
1    pem
m for distinct primes pi. This
means that the integers in S are precisely the nonnegative integers less than n that
are divisible by at least one of the pi‚Äôs. Letting Ca be the set of nonnegative integers
less than n that are divisible by a, we have
S D
m
[
iD1
Cpi:

14.9. Inclusion-Exclusion
523
We‚Äôll be able to Ô¨Ånd the size of this union using Inclusion-Exclusion because the
intersections of the Cp‚Äôs are easy to count. For example, Cp \ Cq \ Cr is the set
of nonnegative integers less than n that are divisible by each of p, q and r. But
since the p; q; r are distinct primes, being divisible by each of them is the same
as being divisible by their product. Now observe that if k is a positive divisor of
n, then exactly n=k nonnegative integers less than n are divisible by k, namely,
0; k; 2k; : : : ; ..n=k/   1/k. So exactly n=pqr nonnegative integers less than n are
divisible by all three primes p, q, r. In other words,
jCp \ Cq \ Crj D
n
pqr :
Reasoning this way about all the intersections among the Cp‚Äôs and applying
Inclusion-Exclusion, we get
jSj D
ÀáÀáÀáÀáÀá
m
[
iD1
Cpi
ÀáÀáÀáÀáÀá
D
m
X
iD1
ÀáÀáCpi
ÀáÀá  X
1i<jm
ÀáÀáCpi \ Cpj
ÀáÀá
C
X
1i<j<km
ÀáÀáCpi \ Cpj \ Cpk
ÀáÀá      C . 1/m 1
ÀáÀáÀáÀáÀá
m
\
iD1
Cpi
ÀáÀáÀáÀáÀá
D
m
X
iD1
n
pi
 X
1i<j m
n
pipj
C
X
1i<j<km
n
pipj pk
     C . 1/m 1
n
p1p2    pn
D n
0
@
m
X
iD1
1
pi
 X
1i<j m
1
pipj
C
X
1i<j <km
1
pipj pk
     C . 1/m 1
1
p1p2    pn
1
A
But .n/ D n   jSj by deÔ¨Ånition, so
.n/ D n
0
@1  m
X
iD1
1
pi
C
X
1i<jm
1
pipj
 X
1i<j <km
1
pipj pk
C    C . 1/m
1
p1p2    pn
1
A
D n
m
Y
iD1

1   1
pi

:
(14.6)

Chapter 14
Cardinality Rules
524
Yikes! That was pretty hairy. Are you getting tired of all that nasty algebra? If
so, then good news is on the way. In the next section, we will show you how to
prove some heavy-duty formulas without using any algebra at all. Just a few words
and you are done. No kidding.
14.10
Combinatorial Proofs
Suppose you have n different T-shirts, but only want to keep k. You could equally
well select the k shirts you want to keep or select the complementary set of n   k
shirts you want to throw out. Thus, the number of ways to select k shirts from
among n must be equal to the number of ways to select n   k shirts from among n.
Therefore:
 
n
k
!
D
 
n
n   k
!
:
This is easy to prove algebraically, since both sides are equal to:
n≈†
k≈† .n   k/≈†:
But we didn‚Äôt really have to resort to algebra; we just used counting principles.
Hmmm.. . .
14.10.1
Pascal‚Äôs Identity
Bob, famed Math for Computer Science Teaching Assistant, has decided to try out
for the US Olympic boxing team. After all, he‚Äôs watched all of the Rocky movies
and spent hours in front of a mirror sneering, ‚ÄúYo, you wanna piece a‚Äô me?!‚Äù Bob
Ô¨Ågures that n people (including himself) are competing for spots on the team and
only k will be selected. As part of maneuvering for a spot on the team, he needs to
work out how many different teams are possible. There are two cases to consider:
 Bob is selected for the team, and his k   1 teammates are selected from
among the other n   1 competitors. The number of different teams that can
be formed in this way is:
 
n   1
k   1
!
:
 Bob is not selected for the team, and all k team members are selected from
among the other n   1 competitors. The number of teams that can be formed

14.10. Combinatorial Proofs
525
this way is:
 
n   1
k
!
:
All teams of the Ô¨Årst type contain Bob, and no team of the second type does;
therefore, the two sets of teams are disjoint. Thus, by the Sum Rule, the total
number of possible Olympic boxing teams is:
 
n   1
k   1
!
C
 
n   1
k
!
:
Ted, equally-famed Teaching Assistant, thinks Bob isn‚Äôt so tough and so he
might as well also try out. He reasons that n people (including himself) are try-
ing out for k spots. Thus, the number of ways to select the team is simply:
 
n
k
!
:
Ted and Bob each correctly counted the number of possible boxing teams. Thus,
their answers must be equal. So we know:
Lemma 14.10.1 (Pascal‚Äôs Identity).
 
n
k
!
D
 
n   1
k   1
!
C
 
n   1
k
!
:
(14.7)
This is called Pascal‚Äôs Identity. And we proved it without any algebra! Instead,
we relied purely on counting techniques.
14.10.2
Giving a Combinatorial Proof
A combinatorial proof is an argument that establishes an algebraic fact by relying
on counting principles. Many such proofs follow the same basic outline:
1. DeÔ¨Åne a set S.
2. Show that jSj D n by counting one way.
3. Show that jSj D m by counting another way.
4. Conclude that n D m.

Chapter 14
Cardinality Rules
526
In the preceding example, S was the set of all possible Olympic boxing teams. Bob
computed
jSj D
 
n   1
k   1
!
C
 
n   1
k
!
by counting one way, and Ted computed
jSj D
 
n
k
!
by counting another way. Equating these two expressions gave Pascal‚Äôs Identity.
Checking a Combinatorial Proof
Combinatorial proofs are based on counting the same thing in different ways. This
is Ô¨Åne when you‚Äôve become practiced at different counting methods, but when in
doubt, you can fall back on bijections and sequence counting to check such proofs.
For example, let‚Äôs take a closer look at the combinatorial proof of Pascal‚Äôs Iden-
tity (14.7). In this case, the set S of things to be counted is the collection of all
size-k subsets of integers in the interval ≈í1; n¬ç.
Now we‚Äôve already counted S one way, via the Bookkeeper Rule, and found
jSj D
 n
k

. The other ‚Äúway‚Äù corresponds to deÔ¨Åning a bijection between S and the
disjoint union of two sets A and B where,
A WWD f.1; X/ j X  ≈í2; n¬ç AND jXj D k   1g
B WWD f.0; Y / j Y  ≈í2; n¬ç AND jY j D kg:
Clearly A and B are disjoint since the pairs in the two sets have different Ô¨Årst
coordinates, so jA [ Bj D jAj C jBj. Also,
jAj D # speciÔ¨Åed sets X D
 
n   1
k   1
!
;
jBj D # speciÔ¨Åed sets Y D
 
n   1
k
!
:
Now Ô¨Ånding a bijection f W .A [ B/ ! S will prove the identity (14.7). In
particular, we can deÔ¨Åne
f .c/ WWD
(
X [ f1g
if c D .1; X/;
Y
if c D .0; Y /:
It should be obvious that f is a bijection.

14.10. Combinatorial Proofs
527
14.10.3
A Colorful Combinatorial Proof
The set that gets counted in a combinatorial proof in different ways is usually de-
Ô¨Åned in terms of simple sequences or sets rather than an elaborate story about
Teaching Assistants. Here is another colorful example of a combinatorial argu-
ment.
Theorem 14.10.2.
n
X
rD0
 
n
r
! 
2n
n   r
!
D
 
3n
n
!
Proof. We give a combinatorial proof. Let S be all n-card hands that can be dealt
from a deck containing n different red cards and 2n different black cards. First,
note that every 3n-element set has
jSj D
 
3n
n
!
n-element subsets.
From another perspective, the number of hands with exactly r red cards is
 
n
r
! 
2n
n   r
!
since there are
 n
r

ways to choose the r red cards and
  2n
n r

ways to choose the
n   r black cards. Since the number of red cards can be anywhere from 0 to n, the
total number of n-card hands is:
jSj D
n
X
rD0
 
n
r
! 
2n
n   r
!
:
Equating these two expressions for jSj proves the theorem.

Finding a Combinatorial Proof
Combinatorial proofs are almost magical. Theorem 14.10.2 looks pretty scary, but
we proved it without any algebraic manipulations at all. The key to constructing a
combinatorial proof is choosing the set S properly, which can be tricky. Generally,
the simpler side of the equation should provide some guidance. For example, the
right side of Theorem 14.10.2 is
 3n
n

, which suggests that it will be helpful to
choose S to be all n-element subsets of some 3n-element set.

Chapter 14
Cardinality Rules
528
Problems for Section 14.2
Practice Problems
Problem 14.1.
Alice is thinking of a number between 1 and 1000.
What is the least number of yes/no questions you could ask her and be guaranteed
to discover what it is? (Alice always answers truthfully.)
(a)
Problem 14.2.
In how many different ways is it possible to answer the next chapter‚Äôs practice
problems if:
 the Ô¨Årst problem has four true/false questions,
 the second problem requires choosing one of four alternatives, and
 the answer to the third problem is an integer  15 and  20?
Problem 14.3.
How many total functions are there from set A to set B if jAj D 3 and jBj D 7?
Problem 14.4.
Consider a 6 element set X with elements fx1; x2; x3; x4; x5; x6g.
(a) How many subsets of X contain x1?
(b) How many subsets of X contain x2 and x3 but do not contain x6?
Class Problems
Problem 14.5.
A license plate consists of either:
 3 letters followed by 3 digits (standard plate)
 5 letters (vanity plate)
 2 characters‚Äîletters or numbers (big shot plate)

14.10. Combinatorial Proofs
529
Let L be the set of all possible license plates.
(a) Express L in terms of
A D fA; B; C; : : : ; Zg
D D f0; 1; 2; : : : ; 9g
using unions ([) and set products ().
(b) Compute jLj, the number of different license plates, using the sum and product
rules.
Problem 14.6. (a) How many of the billion numbers in the range from 1 to 109
contain the digit 1? (Hint: How many don‚Äôt?)
(b) There are 20 books arranged in a row on a shelf. Describe a bijection between
ways of choosing 6 of these books so that no two adjacent books are selected and
15-bit strings with exactly 6 ones.
Problem 14.7.
(a) Let Sn;k be the possible nonnegative integer solutions to the inequality
x1 C x2 C    C xk  n:
(14.8)
That is
Sn;k WWD f.x1; x2; : : : ; xk/ 2 Nk j (14.8) is trueg:
Describe a bijection between Sn;k and the set of binary strings with n zeroes and k
ones.
(b) Let Ln;k be the length k weakly increasing sequences of nonnegative integers
 n. That is
Ln;k WWD f.y1; y2; : : : ; yk/ 2 Nk j y1  y2      yk  ng:
Describe a bijection between Ln;k and Sn;k.

Chapter 14
Cardinality Rules
530
1
3
7
5
4
2
6
65622
code
tree
1
2
3
4
5
432
Figure 14.7
Problem 14.8.
An n-vertex numbered tree is a tree whose vertex set is f1; 2; : : : ; ng for some
n > 2. We deÔ¨Åne the code of the numbered tree to be a sequence of n   2 integers
from 1 to n obtained by the following recursive process:6
If there are more than two vertices left, write down the father of the largest leaf,
delete this leaf, and continue this process on the resulting smaller tree. If there
are only two vertices left, then stop ‚Äîthe code is complete.
For example, the codes of a couple of numbered trees are shown in the Fig-
ure 14.7.
(a) Describe a procedure for reconstructing a numbered tree from its code.
(b) Conclude there is a bijection between the n-vertex numbered trees and f1; : : : ; ngn 2,
and state how many n-vertex numbered trees there are.
Problem 14.9.
Let X and Y be Ô¨Ånite sets.
(a) How many binary relations from X to Y are there?
6The necessarily unique node adjacent to a leaf is called its father.

14.10. Combinatorial Proofs
531
(b) DeÔ¨Åne a bijection between the set ≈íX ! Y ¬ç of all total functions from X to
Y and the set Y jXj. (Recall Y n is the cartesian product of Y with itself n times.)
Based on that, what is j ≈íX ! Y ¬ç j?
(c) Using the previous part how many functions, not necessarily total, are there
from X to Y ? How does the fraction of functions vs. total functions grow as the
size of X grows? Is it O.1/, O.jXj/, O.2jXj/,...?
(d) Show a bijection between the powerset, pow.X/, and the set ≈íX ! f0; 1g¬ç of
0-1-valued total functions on X.
(e) Let X WWD f1; 2; : : : ; ng. In this problem we count how many bijections there
are from X to itself. Consider the set BX;X of all bijections from set X to set X.
Show a bijection from BX;X to the set of all permuations of X (as deÔ¨Åned in the
notes). Using that, count BX;X.
Problems for Section 14.4
Homework Problems
Problem 14.10.
Here is a purely combinatorial proof of Fermat‚Äôs Little Theorem 8.10.11.
(a) Suppose there are beads available in a different colors for some integer a > 1,
and let p be a prime number. How many different colored length p sequences of
beads can be strung together? How many of them contain beads of at least two
different colors?
(b) Make each string of p beads with at least two colors into a bracelet by tying
the two ends of the string together. Two bracelets are the same if one can be rotated
to yield the other. (Note, however, that you cannot ‚ÄùÔ¨Çip‚Äù a bracelet over or reÔ¨Çect
it.) Show that for every bracelet, there are exactly p strings of beads that yield it.
Hint: Both the fact that p is prime and that the bracelet consists of at least two
colors are needed for this to be true.
(c) Conclude that p j .ap   a/ and from this conclude Fermat‚Äôs Little Theorem.
Problems for Section 14.5
Practice Problems
Problem 14.11.
8 students‚ÄîAnna, Brian, Caine,...‚Äìare to be seated around a circular table in a
circular room. Two seatings are regarded as deÔ¨Åning the same arrangement if each

Chapter 14
Cardinality Rules
532
student has the same student on his or her right in both seatings: it does not matter
which way they face. We‚Äôll be interested in counting how many arrangements there
are of these 8 students, given some restrictions.
(a) As a start, how many different arrangements of these 8 students around the
table are there without any restrictions?
(b) How many arrangements of these 8 students are there with Anna sitting next
to Brian?
(c) How many arrangements are there with if Brian sitting next to both Anna AND
Caine?
(d) How many arrangements are there with Brian sitting next to Anna OR Caine?
Problem 14.12.
How many different ways are there to select three dozen colored roses if red, yellow,
pink, white, purple and orange roses are available?
Problem 14.13.
Suppose you want to select k out of n books on a shelf so that there are always
at least 3 unselected books between selected books. Describe a bijection between
book selection and bit-strings of length L containing exactly M 1‚Äôs, so that count-
ing the number of all such bit-strings gives us the number of book selections. Find
L and M and brieÔ¨Çy explain why it works.
(Assume n is large enough for this to be possible.)
Class Problems
Problem 14.14.
Your class tutorial has 12 students, who are supposed to break up into 4 groups of
3 students each. Your Teaching Assistant (TA) has observed that the students waste
too much time trying to form balanced groups, so he decided to pre-assign students
to groups and email the group assignments to his students.
(a) Your TA has a list of the 12 students in front of him, so he divides the list into
consecutive groups of 3. For example, if the list is ABCDEFGHIJKL, the TA would
deÔ¨Åne a sequence of four groups to be .fA; B; Cg; fD; E; F g; fG; H; Ig; fJ; K; Lg/.
This way of forming groups deÔ¨Ånes a mapping from a list of twelve students to a
sequence of four groups. This is a k-to-1 mapping for what k?

14.10. Combinatorial Proofs
533
(b) A group assignment speciÔ¨Åes which students are in the same group, but not
any order in which the groups should be listed. If we map a sequence of 4 groups,
.fA; B; Cg; fD; E; F g; fG; H; Ig; fJ; K; Lg/;
into a group assignment
ffA; B; Cg; fD; E; F g; fG; H; Ig; fJ; K; Lgg;
this mapping is j-to-1 for what j ?
(c) How many group assignments are possible?
(d) In how many ways can 3n students be broken up into n groups of 3?
Problem 14.15.
A pizza house is having a promotional sale. Their commercial reads:
We offer 9 different toppings for your pizza! Buy 3 large pizzas at
the regular price, and you can get each one with as many different
toppings as you wish, absolutely free. That‚Äôs 22; 369; 621 different
ways to choose your pizzas!
The ad writer was a former Harvard student who had evaluated the formula .29/3=3≈†
on his calculator and gotten close to 22; 369; 621. Unfortunately, .29/3=3≈† is ob-
viously not an integer, so clearly something is wrong. What mistaken reasoning
might have led the ad writer to this formula? Explain how to Ô¨Åx the mistake and
get a correct formula.
Problem 14.16.
Answer the following quesions using the Generalized Product Rule.
(a) Next week, I‚Äôm going to get really Ô¨Åt! On day 1, I‚Äôll exercise for 5 minutes.
On each subsequent day, I‚Äôll exercise 0, 1, 2, or 3 minutes more than the previous
day. For example, the number of minutes that I exercise on the seven days of next
week might be 5, 6, 9, 9, 9, 11, 12. How many such sequences are possible?
(b) An r-permutation of a set is a sequence of r distinct elements of that set. For
example, here are all the 2-permutations of fa; b; c; dg:
.a; b/
.a; c/
.a; d/
.b; a/
.b; c/
.b; d/
.c; a/
.c; b/
.c; d/
.d; a/
.d; b/
.d; c/

Chapter 14
Cardinality Rules
534
How many r-permutations of an n-element set are there? Express your answer
using factorial notation.
(c) How many nn matrices are there with distinct entries drawn from f1; : : : ; pg,
where p  n2?
Problem 14.17. (a) There are 30 books arranged in a row on a shelf. In how many
ways can eight of these books be selected so that there are at least two unselected
books between any two selected books?
(b) How many nonnegative integer solutions are there for the following equality?
x1 C x2 C    C xm D k:
(14.9)
(c) How many nonnegative integer solutions are there for the following inequal-
ity?
x1 C x2 C    C xm  k:
(14.10)
(d) How many length-m weakly increasing sequences of nonnegative integers  k
are there?
Homework Problems
Problem 14.18.
This problem is about binary relations on the set of integers in the interval ≈í1; n¬ç,
and digraphs and simple graphs whose vertex set is ≈í1; n¬ç.
(a) How many digraphs are there?
(b) How many simple graphs are there?
(c) How many asymmetric binary relations are there?
(d) How many path-total strict partial orders are there?
Problem 14.19.
Answer the following questions with a number or a simple formula involving fac-
torials and binomial coefÔ¨Åcients. BrieÔ¨Çy explain your answers.
(a) How many ways are there to order the 26 letters of the alphabet so that no two
of the vowels a, e, i, o, u appear consecutively and the last letter in the ordering
is not a vowel?

14.10. Combinatorial Proofs
535
Hint: Every vowel appears to the left of a consonant.
(b) How many ways are there to order the 26 letters of the alphabet so that there
are at least two consonants immediately following each vowel?
(c) In how many different ways can 2n students be paired up?
(d) Two n-digit sequences of digits 0,1,...,9 are said to be of the same type if the
digits of one are a permutation of the digits of the other. For n D 8, for example,
the sequences 03088929 and 00238899 are the same type. How many types of
n-digit integers are there?
Problem 14.20.
In a standard 52-card deck, each card has one of thirteen ranks in the set, R, and
one of four suits in the set, S, where
R WWD fA; 2; : : : ; 10; J; Q; Kg;
S WWD f|; }; ~; g:
A 5-card hand is a set of Ô¨Åve distinct cards from the deck.
For each part describe a bijection between a set that can easily be counted using
the Product and Sum Rules of Ch. 14.1, and the set of hands matching the speciÔ¨Å-
cation. Give bijections, not numerical answers.
For instance, consider the set of 5-card hands containing all 4 suits. Each such
hand must have 2 cards of one suit. We can describe a bijection between such hands
and the set S  R2  R3 where R2 is the set of two-element subsets of R. Namely,
an element
.s; fr1; r2g; .r3; r4; r5// 2 S  R2  R3
indicates
1. the repeated suit, s 2 S,
2. the set, fr1; r2g 2 R2, of ranks of the cards of suit, s, and
3. the ranks .r3; r4; r5/ of the remaining three cards, listed in increasing suit
order where |  }  ~  .
For example,
.|; f10; Ag; .J; J; 2//  ! fA|; 10|; J }; J ~; 2g:
(a) A single pair of the same rank (no 3-of-a-kind, 4-of-a-kind, or second pair).

Chapter 14
Cardinality Rules
536
(b) Three or more aces.
Problem 14.21.
Suppose you have seven dice ‚Äîeach a different color of the rainbow; otherwise
the dice are standard, with faces numbered 1 to 6. A roll is a sequence specify-
ing a value for each die in rainbow (ROYGBIV) order. For example, one roll is
.3; 1; 6; 1; 4; 5; 2/ indicating that the red die showed a 3, the orange die showed 1,
the yellow 6,. . . .
For the problems below, describe a bijection between the speciÔ¨Åed set of rolls
and another set that is easily counted using the Product, Generalized Product, and
similar rules. Then write a simple arithmetic formula, possibly involving factorials
and binomial coefÔ¨Åcients, for the size of the set of rolls. You do not need to prove
that the correspondence between sets you describe is a bijection, and you do not
need to simplify the expression you come up with.
For example, let A be the set of rolls where 4 dice come up showing the same
number, and the other 3 dice also come up the same, but with a different number.
Let R be the set of seven rainbow colors and S WWD ≈í1; 6¬ç be the set of dice values.
DeÔ¨Åne B WWD PS;2  R3, where PS;2 is the set of 2-permutations of S and R3
is the set of size-3 subsets of R. Then deÔ¨Åne a bijection from A to B by mapping
a roll in A to the sequence in B whose Ô¨Årst element is an ordered pair consisting
of the number that came up three times followed by the number that came up four
times, and whose second element is the set of colors of the three matching dice.
For example, the roll
.4; 4; 2; 2; 4; 2; 4/ 2 A
maps to
..2; 4/; fyellow,green,indigog/ 2 B:
Now by the Bijection rule jAj D jBj, and by the Generalized Product and Subset
rules,
jBj D 6  5 
 
7
3
!
:
(a) For how many rolls do exactly two dice have the value 6 and the remaining
Ô¨Åve dice all have different values?
Example: .6; 2; 6; 1; 3; 4; 5/ is a roll of this type, but .1; 1; 2; 6; 3; 4; 5/ and .6; 6; 1; 2; 4; 3; 4/
are not.
(b) For how many rolls do two dice have the same value and the remaining Ô¨Åve
dice all have different values?

14.10. Combinatorial Proofs
537
Example: .4; 2; 4; 1; 3; 6; 5/ is a roll of this type, but .1; 1; 2; 6; 1; 4; 5/ and .6; 6; 1; 2; 4; 3; 4/
are not.
(c) For how many rolls do two dice have one value, two different dice have a
second value, and the remaining three dice a third value?
Example: .6; 1; 2; 1; 2; 6; 6/ is a roll of this type, but .4; 4; 4; 4; 1; 3; 5/ and .5; 5; 5; 6; 6; 1; 2/
are not.
Exam Problems
Problem 14.22.
Suppose that two identical 52-card decks are mixed together. Write a simple for-
mula for the number of distinct permutations of the 104 cards.
Problems for Section 14.6
Practice Problems
Problem 14.23.
How many different permutations are there of the sequence of letters in ‚ÄúMISSIS-
SIPPI‚Äù?
Exam Problems
Problem 14.24.
There is a robot that steps between integer positions in 3-dimensional space. Each
step of the robot increments one coordinate and leaves the other two unchanged.
(a) How many paths can the robot follow going from the origin .0; 0; 0/ to .3; 4; 5/?
(b) How many paths can the robot follow going from the origin .i; j; k/ to .m; n; p/?
Problems for Section 14.6
Class Problems
Problem 14.25.
The Tao of BOOKKEEPER: we seek enlightenment through contemplation of the
word BOOKKEEPER.
(a) In how many ways can you arrange the letters in the word POKE?
(b) In how many ways can you arrange the letters in the word BO1O2K? Observe
that we have subscripted the O‚Äôs to make them distinct symbols.

Chapter 14
Cardinality Rules
538
(c) Suppose we map arrangements of the letters in BO1O2K to arrangements
of the letters in BOOK by erasing the subscripts. Indicate with arrows how the
arrangements on the left are mapped to the arrangements on the right.
O2BO1K
KO2BO1
O1BO2K
KO1BO2
BO1O2K
BO2O1K
: : :
BOOK
OBOK
KOBO
: : :
(d) What kind of mapping is this, young grasshopper?
(e) In light of the Division Rule, how many arrangements are there of BOOK?
(f) Very good, young master! How many arrangements are there of the letters in
KE1E2PE3R?
(g) Suppose we map each arrangement of KE1E2PE3R to an arrangement of
KEEPER by erasing subscripts. List all the different arrangements of KE1E2PE3R
that are mapped to REPEEK in this way.
(h) What kind of mapping is this?
(i) So how many arrangements are there of the letters in KEEPER?
Now you are ready to face the BOOKKEEPER!
(j) How many arrangements of BO1O2K1K2E1E2PE3R are there?
(k) How many arrangements of BOOK1K2E1E2PE3R are there?
(l) How many arrangements of BOOKKE1E2PE3R are there?
(m) How many arrangements of BOOKKEEPER are there?
Remember well what you have learned: subscripts on, subscripts off.
This is the Tao of Bookkeeper.
(n) How many arrangements of VOODOODOLL are there?
(o) How many length 52 sequences of digits contain exactly 17 two‚Äôs, 23 Ô¨Åves,
and 12 nines?

14.10. Combinatorial Proofs
539
Problems for Section 14.6
Class Problems
Problem 14.26.
Find the coefÔ¨Åcients of
(a) x5 in .1 C x/11
(b) x8y9 in .3x C 2y/17
(c) a6b6 in .a2 C b3/5
Problem 14.27. (a) Use the Multinomial Theorem 14.6.5 to prove that
.x1 C x2 C    C xn/p  xp
1 C xp
2 C    C xp
n
.mod p/
(14.11)
for all primes p. (Do not prove it using Fermat‚Äôs ‚Äúlittle‚Äù Theorem. The point of
this problem is to offer an independent proof of Fermat‚Äôs theorem.)
Hint: Explain why
 p
k1;k2;:::;kn

is divisible by p if all the ki‚Äôs are positive integers
less than p.
(b) Explain how (14.11) immediately proves Fermat‚Äôs Little Theorem 8.10.11:
np 1  1 .mod p/ when n is not a multiple of p.
Homework Problems
Problem 14.28.
The degree sequence of a simple graph is the weakly decreasing sequence of de-
grees of its vertices. For example, the degree sequence for the 5-vertex numbered
tree pictured in the Figure 14.7 in Problem 14.8 is .2; 2; 2; 1; 1/ and for the 7-vertex
tree it is .3; 3; 2; 1; 1; 1; 1/.
We‚Äôre interested in counting how many numbered trees there are with a given
degree sequence. We‚Äôll do this using the bijection deÔ¨Åned in Problem 14.8 between
n-vertex numbered trees and length n 2 code words whose characters are integers
between 1 and n.
The occurrence number for a character in a word is the number of times that
the character occurs in the word. For example, in the word 65622, the occurrence
number for 6 is two, and the occurrence number for 5 is one. The occurrence
sequence of a word is the weakly decreasing sequence of occurrence numbers of
characters in the word. The occurrence sequence for this word is .2; 2; 1/ because
it has two occurrences of each of the characters 6 and 2, and one occurrence of 5.

Chapter 14
Cardinality Rules
540
(a) There is a simple relationship between the degree sequence of an n-vertex
numbered tree and the occurrence sequence of its code. Describe this relationship
and explain why it holds. Conclude that counting n-vertex numbered trees with a
given degree sequence is the same as counting the number of length n   2 code
words with a given occurrence sequence.
Hint: How many times does a vertex of degree, d, occur in the code?
For simplicity, let‚Äôs focus on counting 9-vertex numbered trees with a given de-
gree sequence. By part (a), this is the same as counting the number of length 7 code
words with a given occurrence sequence.
Any length 7 code word has a pattern, which is another length 7 word over the
alphabet a,b,c,d,e,f,g that has the same occurrence sequence.
(b) How many length 7 patterns are there with three occurrences of a, two occur-
rences of b, and one occurrence of c and d?
(c) How many ways are there to assign occurrence numbers to integers 1; 2; : : : ; 9
so that a code word with those occurrence numbers would have the occurrence
sequence 3; 2; 1; 1; 0; 0; 0; 0; 0?
In general, to Ô¨Ånd the pattern of a code word, list its characters in decreasing order
by number of occurrences, and list characters with the same number of occurrences
in decreasing order. Then replace successive characters in the list by successive
letters a,b,c,d,e,f,g. The code word 2468751, for example, has the pattern
fecabdg, which is obtained by replacing its characters 8,7,6,5,4,2,1 by
a,b,c,d,e,f,g, respectively. The code word 2449249 has pattern caabcab,
which is obtained by replacing its characters 4,9,2 by a,b,c, respectively.
(d) What length 7 code word has three occurrences of 7, two occurrences of 8,
one occurrence each of 2 and 9, and pattern abacbad?
(e) Explain why the number of 9-vertex numbered trees with degree sequence
.4; 3; 2; 2; 1; 1; 1; 1; 1/ is the product of the answers to parts (b) and (c).
Problems for Section 14.7
Practice Problems
Problem 14.29.
Indicate how many 5-card hands there are of each of the following kinds.
(a) A Sequence is a hand consisting of Ô¨Åve consecutive cards of any suit, such as
5~   6~   7   8   9|:
Note that an Ace may either be high (as in 10-J-Q-K-A), or low (as in A-2-3-4-5),
but can‚Äôt go ‚Äúaround the corner‚Äù (that is, Q-K-A-2-3 is not a sequence).

14.10. Combinatorial Proofs
541
How many different Sequence hands are possible?
(b) A Matching Suit is a hand consisting of cards that are all of the same suit in
any order.
How many different Matching Suit hands are possible?
(c) A Straight Flush is a hand that is both a Sequence and a Matching Suit.
How many different Straight Flush hands are possible?
(d) A Straight is a hand that is a Sequence but not a Matching Suit.
How many possible Straights are there?
(e) A Flush is a hand that is a Matching Suit but not a Sequence.
How many possible Flushes are there?
Class Problems
Problem 14.30.
Solve the following counting problems. DeÔ¨Åne an appropriate mapping (bijective
or k-to-1) between a set whose size you know and the set in question.
(a) An independent living group is hosting nine new candidates for membership.
Each candidate must be assigned a task: 1 must wash pots, 2 must clean the kitchen,
3 must clean the bathrooms, 1 must clean the common area, and 2 must serve
dinner. Write a multinomial coefÔ¨Åcient for the number of ways this can be done.
(b) How many nonnegative integers less than 1,000,000 have exactly one digit
equal to 9 and have a sum of digits equal to 17?
Problem 14.31.
Here are the solutions to the next 7 short answer questions, in no particular order.
Indicate the solutions for the questions and brieÔ¨Çy explain your answers.
1:
n≈†
.n   m/≈†
2:
 
n C m
m
!
3:
.n   m/≈†
4:
mn
5:
 
n   1 C m
m
!
6:
 
n   1 C m
n
!
7:
2mn
8:
nm
(a) How many length m words can be formed from an n-letter alphabet, if no letter
is used more than once?

Chapter 14
Cardinality Rules
542
(b) How many length m words can be formed from an n-letter alphabet, if letters
can be reused?
(c) How many binary relations are there from set A to set B when jAj D m and
jBj D n?
(d) How many total injective functions are there from set A to set B, where jAj D
m and jBj D n  m?
(e) How many ways are there to place a total of m distinguishable balls into n
distinguishable urns, with some urns possibly empty or with several balls?
(f) How many ways are there to place a total of m indistinguishable balls into n
distinguishable urns, with some urns possibly empty or with several balls?
(g) How many ways are there to put a total of m distinguishable balls into n dis-
tinguishable urns with at most one ball in each urn?
Exam Problems
Problem 14.32. (a) How many solutions over the positive integers are there to the
inequality:
x1 C x2 C : : : C x10  100
(b) In how many ways can Mr. and Mrs. Grumperson distribute 13 identical
pieces of coal to their three children for Christmas so that each child gets at least
one piece?
Problems for Section 14.8
Practice Problems
Problem 14.33.
Below is a list of properties that a group of people might possess.
For each property, either give the minimum number of people that must be in a
group to ensure that the property holds, or else indicate that the property need not
hold even for arbitrarily large groups of people.
(Assume that every year has exactly 365 days; ignore leap years.)
(a) At least 2 people were born on the same day of the year (ignore year of birth).
(b) At least 2 people were born on January 1.

14.10. Combinatorial Proofs
543
(c) At least 3 people were born on the same day of the week.
(d) At least 4 people were born in the same month.
(e) At least 2 people were born exactly one week apart.
Class Problems
Problem 14.34.
Solve the following problems using the pigeonhole principle. For each problem,
try to identify the pigeons, the pigeonholes, and a rule assigning each pigeon to a
pigeonhole.
(a) In a certain Institute of Technology, every ID number starts with a 9. Suppose
that each of the 75 students in a class sums the nine digits of their ID number.
Explain why two people must arrive at the same sum.
(b) In every set of 100 integers, there exist two whose difference is a multiple of
37.
(c) For any Ô¨Åve points inside a unit square (not on the boundary), there are two
points at distance less than 1=
p
2.
(d) Show that if n C 1 numbers are selected from f1; 2; 3; : : : ; 2ng, two must be
consecutive, that is, equal to k and k C 1 for some k.
Problem 14.35. (a) Prove that every positive integer divides a number such as 70,
700, 7770, 77000, whose decimal representation consists of one or more 7‚Äôs fol-
lowed by one or more 0‚Äôs.
Hint: 7; 77; 777; 7777; : : :
(b) Conclude that if a positive number is not divisible by 2 or 5, then it divides a
number whose decimal representation is all 7‚Äôs.
Problem 14.36. (a) Show that the Magician could not pull off the trick with a deck
larger than 124 cards.
Hint: Compare the number of 5-card hands in an n-card deck with the number of
4-card sequences.
(b) Show that, in principle, the Magician could pull off the Card Trick with a deck
of 124 cards.

Chapter 14
Cardinality Rules
544
Hint: Hall‚Äôs Theorem and degree-constrained (11.5.5) graphs.
Problem 14.37.
The Magician can determine the 5th card in a poker hand when his Assisant reveals
the other 4 cards. Describe a similar method for determining 2 hidden cards in a
hand of 9 cards when your Assisant reveals the other 7 cards.
Homework Problems
Problem 14.38. (a) Show that any odd integer x in the range 109 < x < 2  109
containing all ten digits 0; 1; : : : ; 9 must have consecutive even digits. Hint: What
can you conclude about the parities of the Ô¨Årst and last digit?
(b) Show that there are 2 vertices of equal degree in any Ô¨Ånite undirected graph
with n  2 vertices. Hint: Cases conditioned upon the existence of a degree zero
vertex.
Problem 14.39.
Show that for any set of 201 positive integers less than 300, there must be two
whose quotient is a power of three (with no remainder).
Problem 14.40. (a) Color each point in the plane with integer coordinates either
red, white or blue. Let R be a 4  82 rectangular grid of these points. Explain why
at least two of the 82 rows in R must have the same sequence colors.
(b) Conclude that R contains four points with the same color that form the corners
of a rectangle.
(c) Generalize the above argument to a coloring using the rainbow colors Red,
Orange, Yellow, Green, Blue, Indigo, Violet as well as White and Black.
Problem 14.41.
Section 14.8.6 explained why it is not possible to perform a four-card variant of the
hidden-card magic trick with one card hidden. But the Magician and her Assistant
are determined to Ô¨Ånd a way to make a trick like this work. They decide to change
the rules slightly: instead of the Assistant lining up the three unhidden cards for

14.10. Combinatorial Proofs
545
the Magician to see, he will line up all four cards with one card face down and the
other three visible. We‚Äôll call this the face-down four-card trick.
For example, suppose the audience members had selected the cards 9~, 10},
A|, 5|. Then the Assistant could choose to arrange the 4 cards in any order so
long as one is face down and the others are visible. Two possibilities are:
A|
?
10}
5|
?
5|
9~
10}
(a) Explain how to model this face-down four-card trick as a matching problem,
and show that there must be a bipartite matching which theoretically will allow the
Magician and Assistant to perform the trick.
(b) There is actually a simple way to perform the face-down four-card trick.7
Case 1. there are two cards with the same suit: Say there are two  cards. The
Assistant proceeds as in the original card trick: he puts one of the  cards face
up as the Ô¨Årst card. He will place the second  card face down. He then uses a
permutation of the face down card and the remaining two face up cards to code
the offset of the face down card from the Ô¨Årst card.
Case 2. all four cards have different suits: Assign numbers 0; 1; 2; 3 to the four
suits in some agreed upon way. The Assistant computes, s, the sum modulo 4
of the ranks of the four cards, and chooses the card with suit s to be placed face
down as the Ô¨Årst card. He then uses a permutation of the remaining three face-up
cards to code the rank of the face down card.
Explain how in Case 2. the Magician can determine the face down card from the
cards the Assistant shows her.
(c) Explain how any method for performing the face-down four-card trick can be
adapted to perform the regular (5-card hand, show 4 cards) with a 52-card deck
consisting of the usual 52 cards along with a 53rd card called the joker.
7This elegant method was devised in Fall ‚Äô09 by student Katie E Everett.

Chapter 14
Cardinality Rules
546
Problem 14.42.
This problem will use the Pigeonhole Principle and elementary properties of con-
gruences to prove that every positive integer divides inÔ¨Ånitely many Fibonacci num-
bers.
A function f W N ! N that satisiÔ¨Åes
f .n/ D c1f .n   1/ C c2f .n   2/ C    C cdf .n   d/
(14.12)
for some ci 2 N and all n  d is called degree d linear-recursive.
A function f W N ! N has a degree d repeat modulo m at n and k when it
satisÔ¨Åes the following repeat congruences:
f .n/

f .k/
.mod m/;
f .n   1/

f .k   1/
.mod m/;
:::
f .n   .d   1//

f .k   .d   1//
.mod m/:
for k > n  d   1.
For the rest of this problem, assume linear-recursive functions and repeats are
degree d > 0.
(a) Prove that if a linear-recursive function has a repeat modulo m at n and k, then
it has one at n C 1 and k C 1.
(b) Prove that for all m > 1, every linear-recursive function repeats modulo m at
n and k for some n; k 2 ≈íd   1; d C md/.
(c) A linear-recursive function is reverse-linear if its dth coefÔ¨Åcient cd D Àô1.
Prove that if a reverse-linear function repeats modulo m at n and k for some n  d,
then it repeats modulo m at n   1 and k   1.
(d) Conclude that every reverse-linear function must repeat modulo m at d   1
and .d   1/ C j for some j > 0.
(e) Conclude that if f is an reverse-linear function and f .k/ D 0 for some k 2
≈í0; d/, then every positive integer is a divisor of f .n/ for inÔ¨Ånitely many n.
(f) Conclude that every positive integer is a divisor of inÔ¨Ånitely many Fibonacci
numbers.
Hint: Start the Fibonacci sequence with the values 0,1 instead of 1, 1.

14.10. Combinatorial Proofs
547
Exam Problems
Problem 14.43.
A standard 52 card deck has 13 cards of each suit. Use the Pigeonhole Principle to
determine the smallest k such that every set of k cards from the deck contains Ô¨Åve
cards of the same suit (called a Ô¨Çush). Clearly indicate what are the pigeons, holes,
and rules for assigning a pigeon to a hole.
Problems for Section 14.9
Practice Problems
Problem 14.44.
Let A1, A2, A3 be sets with jA1j D 100, jA2j D 1; 000, and jA3j D 10; 000.
Determine jA1 [ A2 [ A3j in each of the following cases:
(a) A1  A2  A3.
(b) The sets are pairwise disjoint.
(c) For any two of the sets, there is exactly one element in both.
(d) There are two elements common to each pair of sets and one element in all
three sets.
Problem 14.45.
The working days in the next year can be numbered 1, 2, 3, ..., 300. I‚Äôd like to
avoid as many as possible.
 On even-numbered days, I‚Äôll say I‚Äôm sick.
 On days that are a multiple of 3, I‚Äôll say I was stuck in trafÔ¨Åc.
 On days that are a multiple of 5, I‚Äôll refuse to come out from under the
blankets.
In total, how many work days will I avoid in the coming year?
Class Problems
Problem 14.46.
A certain company wants to have security for their computer systems. So they have
given everyone a password. A length 10 word containing each of the characters:
a, d, e, f, i, l, o, p, r, s,

Chapter 14
Cardinality Rules
548
is called a cword. A password will be a cword which does not contain any of the
subwords ‚Äúfails‚Äù, ‚Äúfailed‚Äù, or ‚Äúdrop.‚Äù
For example, the following two words are passwords:
adeÔ¨Åloprs, srpolifeda,
but the following three cwords are not:
adropeÔ¨Çis, failedrops, dropefails.
(a) How many cwords contain the subword ‚Äúdrop‚Äù?
(b) How many cwords contain both ‚Äúdrop‚Äù and ‚Äúfails‚Äù?
(c) Use the Inclusion-Exclusion Principle to Ô¨Ånd a simple arithmetic formula in-
volving factorials for the number of passwords.
Problem 14.47.
We want to count step-by-step paths between points in the plane with integer coor-
dinates. Only two kinds of step are allowed: a right-step which increments the x
coordinate, and an up-step which increments the y coordinate.
(a) How many paths are there from .0; 0/ to .20; 30/?
(b) How many paths are there from .0; 0/ to .20; 30/ that go through the point
.10; 10/?
(c) How many paths are there from .0; 0/ to .20; 30/ that do not go through either
of the points .10; 10/ and .15; 20/?
Hint: Let P be the set of paths from .0; 0/ to .20; 30/, N1 be the paths in P that go
through .10; 10/ and N2 be the paths in P that go through .15; 20/.
Problem 14.48.
Let‚Äôs develop a proof of the Inclusion-Exclusion formula using high school algebra.
(a) Most high school students will get freaked by the following formula, even
though they actually know the rule it expresses. How would you explain it to them?
n
Y
iD1
.1   xi/ D
X
If1;:::;ng
. 1/jIj Y
j 2I
xj :
(14.13)
Hint: Show them an example.
For any set, S, let MS be the membership function of S:
MS.x/ D
(
1
if x 2 S;
0
if x ‚Ä¶ S:

14.10. Combinatorial Proofs
549
Let S1; : : : ; Sn be a sequence of Ô¨Ånite sets, and abbreviate MSi as Mi. Let the
domain of discourse, D, be the union of the Si‚Äôs. That is, we let
D WWD
n
[
iD1
Si;
and take complements with respect to D, that is,
T WWD D   T;
for T  D.
(b) Verify that for T  D and I  f1; : : : ng,
MT D 1   MT ;
(14.14)
M.
T
i2I Si/ D
Y
i2I
MSi;
(14.15)
M.
S
i2I Si/ D 1  Y
i2I
.1   Mi/:
(14.16)
(Note that (14.15) holds when I is empty because, by convention, an empty product
equals 1, and an empty intersection equals the domain of discourse, D.)
(c) Use (14.13) and (14.16) to prove
MD D
X
;¬§If1;:::;ng
. 1/jIjC1 Y
j2I
Mj :
(14.17)
(d) Prove that
jT j D
X
u2D
MT .u/:
(14.18)
(e) Now use the previous parts to prove
jDj D
X
;¬§If1;:::;ng
. 1/jIjC1
ÀáÀáÀáÀáÀá
\
i2I
Si
ÀáÀáÀáÀáÀá
(14.19)
(f) Finally, explain why (14.19) immediately implies the usual form of the Inclusion-
Exclusion Principle:
jDj D
n
X
iD1
. 1/iC1
X
If1;:::;ng
jIjDi
ÀáÀáÀáÀáÀáÀá
\
j2I
Sj
ÀáÀáÀáÀáÀáÀá
:
(14.20)

Chapter 14
Cardinality Rules
550
Homework Problems
Problem 14.49.
How many paths are there from point .0; 0/ to .50; 50/ if each step along a path
increments one coordinate and leaves the other unchanged? How many are there
when there are impassable boulders sitting at points .10; 11/ and .21; 20/? (You
do not have to calculate the number explicitly; your answer may be an expression
involving binomial coefÔ¨Åcients.)
Hint: Inclusion-Exclusion.
Problem 14.50.
A derangement is a permutation .x1; x2; : : : ; xn/ of the set f1; 2; : : : ; ng such that
xi ¬§ i for all i. For example, .2; 3; 4; 5; 1/ is a derangement, but .2; 1; 3; 5; 4/
is not because 3 appears in the third position. The objective of this problem is to
count derangements.
It turns out to be easier to start by counting the permutations that are not de-
rangements. Let Si be the set of all permutations .x1; x2; : : : ; xn/ that are not
derangements because xi D i. So the set of non-derangements is
n
[
iD1
Si:
(a) What is jSij?
(b) What is
ÀáÀáSi \ Sj
ÀáÀá where i ¬§ j?
(c) What is
ÀáÀáSi1 \ Si2 \    \ Sik
ÀáÀá where i1; i2; : : : ; ik are all distinct?
(d) Use the inclusion-exclusion formula to express the number of non-derangements
in terms of sizes of possible intersections of the sets S1; : : : ; Sn.
(e) How many terms in the expression in part (d) have the form
ÀáÀáSi1 \ Si2 \    \ Sik
ÀáÀá?
(f) Combine your answers to the preceding parts to prove the number of non-
derangements is:
n≈†
 1
1≈†   1
2≈† C 1
3≈†      Àô 1
n≈†

:
Conclude that the number of derangements is
n≈†

1   1
1≈† C 1
2≈†   1
3≈† C    Àô 1
n≈†

:

14.10. Combinatorial Proofs
551
(g) As n goes to inÔ¨Ånity, the number of derangements approaches a constant frac-
tion of all permutations. What is that constant? Hint:
ex D 1 C x C x2
2≈† C x3
3≈† C   
Problem 14.51.
How many of the numbers 2; : : : ; n are prime? The Inclusion-Exclusion Principle
offers a useful way to calculate the answer when n is large. Actually, we will use
Inclusion-Exclusion to count the number of composite (nonprime) integers from 2
to n. Subtracting this from n   1 gives the number of primes.
Let Cn be the set of composites from 2 to n, and let Am be the set of numbers in
the range m C 1; : : : ; n that are divisible by m. Notice that by deÔ¨Ånition, Am D ;
for m  n. So
Cn D
n 1
[
iD2
Ai:
(14.21)
(a) Verify that if m j k, then Am  Ak.
(b) Explain why the right hand side of (14.21) equals
[
primes ppn
Ap:
(14.22)
(c) Explain why jAmj D bn=mc   1 for m  2.
(d) Consider any two relatively prime numbers p; q  n. What is the one number
in .Ap \ Aq/   Apq?
(e) Let P be a Ô¨Ånite set of at least two primes. Give a simple formula for
j
\
p2P
Apj:
(f) Use the Inclusion-Exclusion principle to obtain a formula for jC150j in terms
the sizes of intersections among the sets A2; A3; A5; A7; A11. (Omit the intersec-
tions that are empty; for example, any intersection of more than three of these sets
must be empty.)
(g) Use this formula to Ô¨Ånd the number of primes up to 150.

Chapter 14
Cardinality Rules
552
Exam Problems
Problem 14.52. (a) How many length n binary strings are there in which 011
occurs starting at the 4th position?
(b) Let Ai be the set of length n binary strings in which 011 occurs starting at the
ith position. (So Ai is empty for i > n   2.) For i < j , the intersections Ai \ Aj
that are nonempty are all the same size. What is jAi \ Aj j in this case?
(c) Let t be the number of intersections Ai \ Aj that are nonempty, where i < j.
Express t as a binomial coefÔ¨Åcient.
(d) How many length 9 binary strings are there that contain the substring 011?
You should express your answer as an integer or as a simple expression which may
include the constant, t, of part (c).
Hint: Inclusion-exclusion for
ÀáÀáÀáS7
1 Ai
ÀáÀáÀá.
Problem 14.53.
There are 10 students A; B; : : : ; J who will be lined up left to right according to
the some rules below.
Rule I: Student A must not be rightmost.
Rule II: Student B must be adjacent to C (directly to the left or right of C).
Rule III: Student D is always second.
You may answer the following questions with a numerical formula that may
involve factorials.
(a) How many possible lineups are there that satisfy all three of these rules?
(b) How many possible lineups are there that satisfy at least one of these rules?
Problem 14.54.
A robot on a point in the 3-D integer lattice can move a unit distance in one direction

14.10. Combinatorial Proofs
553
at a time. That is, from position .x; y; z/, it can move to either .x C 1; y; z/,
.x; y C 1; z/, or .x; y; z C 1/. For any two points, P and Q, in space, let n.P; Q/
denote the number of distinct paths the spacecraft can follow to go from P to Q.
Let
A D .0; 10; 20/; B D .30; 50; 70/; C D .80; 90; 100/; D D .200; 300; 400/:
(a) Express n.A; B/ as a single multinomial coefÔ¨Åcient.
Answer the following questions with arithmetic expressions involving terms n.P; Q/
for P; Q 2 fA; B; C; Dg. Do not use numbers.
(b) How many paths from A to C go through B?
(c) How many paths from B to D do not go through C?
(d) How many paths from A to D go through neither B nor C?
Problem 14.55.
In a standard 52-card deck (13 ranks and 4 suits), a hand is a 5-card subset of the set
of 52 cards. Express the answer to each part as a formula using factorial, binomial,
or multinomial notation.
(a) Let H be the set of all hands.
What is jHj?
(b) Let HNP be the set of all hands that does not include a pair, that is, no two
card in the hand have the same rank.
What is jHNP j?
(c) Let HS be the set of all hands that is a straight, i.e. the rank of the Ô¨Åve cards
are consecutive. The order of the ranks is .A; 2; 3; 4; 5; 6; 7; 8; 9; 10; J; Q; k; A/,
note that A is appears twice.
What is jHSj?
(d) Let HF be the set of all hands that is a Ô¨Çush, that is, the suit of the Ô¨Åve cards
are identical.
What is jHF j?
(e) Let HSF be the set of all straight Ô¨Çush hands that is both a straight and a Ô¨Çush.
What is jHSF j?

Chapter 14
Cardinality Rules
554
(f) Let HHC be the set of all high card hands that is hands that do not include a
pair, are not straights, and are not Ô¨Çushs.
What is jHHC j?
Problems for Section 14.10
Class Problems
Problem 14.56.
According to the Multinomial theorem, .w C x C y C z/n can be expressed as a
sum of terms of the form
 
n
r1; r2; r3; r4
!
wr1xr2yr3zr4:
(a) How many terms are there in the sum?
(b) The sum of these multinomial coefÔ¨Åcients has an easily expressed value. What
is it?
X
r1Cr2Cr3Cr4Dn;
ri2N
 
n
r1; r2; r3; r4
!
D‚Äπ
(14.23)
Hint: How many terms are there when .w C x C y C z/n is expressed as a sum
of monomials in w; x; y; z before terms with like powers of these variables are
collected together under a single coefÔ¨Åcient?
Problem 14.57.
(a) Give a combinatorial proof of the following identity by letting S be the set of
all length-n sequences of letters a, b and a single c and counting jSj is two different
ways.
n2n 1 D
n
X
kD1
k
 
n
k
!
(14.24)
(b) Now prove (14.24) algebraically by applying the Binomial Theorem to .1 C
x/n and taking derivatives.

14.10. Combinatorial Proofs
555
Problem 14.58.
What do the following expressions equal? Give both algebraic and combinatorial
proofs for your answers.
(a)
n
X
iD0
 
n
i
!
(b)
n
X
iD0
 
n
i
!
. 1/i
Hint: Consider the bit strings with an even number of ones and an odd number of
ones.
Homework Problems
Problem 14.59.
Prove the following identity by algebraic manipulation and by giving a combinato-
rial argument:
 
n
r
! 
r
k
!
D
 
n
k
! 
n   k
r   k
!
Problem 14.60. (a) Find a combinatorial (not algebraic) proof that
n
X
iD0
 
n
i
!
D 2n:
(b) Below is a combinatorial proof of an equation. What is the equation?
Proof. Stinky Peterson owns n newts, t toads, and s slugs. Conveniently, he lives
in a dorm with n C t C s other students. (The students are distinguishable, but
creatures of the same variety are not distinguishable.) Stinky wants to put one
creature in each neighbor‚Äôs bed. Let W be the set of all ways in which this can be
done.
On one hand, he could Ô¨Årst determine who gets the slugs. Then, he could decide
who among his remaining neighbors has earned a toad. Therefore, jW j is equal to
the expression on the left.

Chapter 14
Cardinality Rules
556
On the other hand, Stinky could Ô¨Årst decide which people deserve newts and slugs
and then, from among those, determine who truly merits a newt. This shows that
jW j is equal to the expression on the right.
Since both expressions are equal to jW j, they must be equal to each other.

(Combinatorial proofs are real proofs. They are not only rigorous, but also con-
vey an intuitive understanding that a purely algebraic argument might not reveal.
However, combinatorial proofs are usually less colorful than this one.)
Problem 14.61.
According to the Multinomial Theorem 14.6.5, .x1 C x2 C    C xk/n can be
expressed as a sum of terms of the form
 
n
r1; r2; : : : ; rk
!
xr1
1 xr2
2 : : : xrk
k :
(a) How many terms are there in the sum?
(b) The sum of these multinomial coefÔ¨Åcients has an easily expressed value:
X
r1Cr2CCrkDn;
ri2N
 
n
r1; r2; : : : ; rk
!
D kn
(14.25)
Give a combinatorial proof of this identity.
Hint: How many terms are there when .x1 Cx2 C  Cxk/n is expressed as a sum
of monomials in xi before terms with like powers of these variables are collected
together under a single coefÔ¨Åcient?
Problem 14.62.
You want to choose a team of m people for your startup company from a pool of n
applicants, and from these m people you want to choose k to be the team managers.
You took a Math for Computer Science subject, so you know you can do this in
 
n
m
! 
m
k
!

14.10. Combinatorial Proofs
557
ways. But your CFO, who went to Harvard Business School, comes up with the
formula
 
n
k
! 
n   k
m   k
!
:
Before doing the reasonable thing‚Äîdump on your CFO or Harvard Business School‚Äî
you decide to check his answer against yours.
(a) Give a combinatorial proof that your CFO‚Äôs formula agrees with yours.
(b) Verify this combinatorial proof by giving an algebraic proof of this same fact.


15
Generating Functions
Generating Functions are one of the most surprising and useful inventions in Dis-
crete Mathematics. Roughly speaking, generating functions transform problems
about sequences into problems about functions. This is great because we‚Äôve got
piles of mathematical machinery for manipulating functions. Thanks to generating
functions, we can apply all that machinery to problems about sequences. In this
way, we can use generating functions to solve all sorts of counting problems.
Several Ô¨Çavors of generating functions such as ordinary, exponential, and Dirich-
let come up regularly in combinatorial mathematics. In addition, Z-transforms,
which are closely related to ordinary generating functions, are important in control
theory and signal processing. But ordinary generating functions are enough to il-
lustrate the power of the idea, so we‚Äôll stick to them. So from now on generating
function will mean the ordinary kind, and we will offer a taste of this large subject
by showing how generating functions can be used to solve certain kinds of count-
ing problems and how they can be used to Ô¨Ånd simple formulas for linear-recursive
functions.
15.1
InÔ¨Ånite Series
Informally, a generating function, F.x/, is an inÔ¨Ånite series
F.x/ D f0 C f1x C f2x2 C f3x3 C    :
(15.1)
For example, the inÔ¨Ånite geometric series
G.x/ WWD 1 C x C x2 C    C xn C    :
(15.2)
is a familiar generating function, and we can illustrate typical reasoning about gen-
erating functions by deriving a simple formula for G.x/. The approach is actually
a simpler version of the perturbation method of Section 13.1.2. Namely,
G.x/ D 1 C x C x2 C x3 C    C xn C   
 xG.x/ D
  x   x2   x3        xn     
G.x/   xG.x/ D 1:
Solving for G.x/ gives
1
X
nD0
xn D G.x/ D
1
1   x :
(15.3)

Chapter 15
Generating Functions
560
Continuing with this approach yields a nice formula for
N.x/ WWD 1 C 2x C 3x2 C    C .n C 1/xn C    :
(15.4)
Namely
N.x/ D
1
C 2x C 3x2 C 4x3 C    C .n C 1/xn C   
 xN.x/ D
  x   2x2   3x3       nxn
    
N.x/   xN.x/ D
1
C x C x2 C x3 C    C
xn
C   
D G.x/:
Solving for N.x/ gives
1
X
nD0
.n C 1/xn D N.x/ D G.x/
1   x D
1
.1   x/2 :
(15.5)
We use the notation ≈íxn¬çF.x/ for the coefÔ¨Åcient of xn in the generating function
F.x/. That is, ≈íxn¬çF.x/ WWD fn for F.x/ given by equation (15.1). For example,
we now have
≈íxn¬ç

1
1   x

D 1
≈íxn¬ç

1
.1   x/2

D n C 1:
15.1.1
Never Mind Convergence
The numerical values of G.x/ are undeÔ¨Åned when jxj  1 because the geometric
series diverges. So equation (15.3) holds numerically only when jxj < 1; likewise
for equation (15.5). But in the context of generating functions, we regard inÔ¨Ånite
series as formal algebraic objects and equations such as (15.3) and (15.5) as sym-
bolic identities that hold for purely algebraic reasons. In fact, good use can be made
of generating functions determined by inÔ¨Ånite series that don‚Äôt converge anywhere.
We‚Äôll explain this further at the end of the chapter, but for now it‚Äôs enough to know
that we needn‚Äôt worry about convergence.
15.2
Counting with Generating Functions
Generating functions are particularly useful for representing and counting the num-
ber of ways to select n things. For example, if there are two Ô¨Çavors of donuts

15.2. Counting with Generating Functions
561
‚Äîchocolate and vanilla ‚Äîlet dn be the number of ways to select n chocolate or
vanilla Ô¨Çavored donuts. So dn D n C 1 because there are n C 1 such donut se-
lections, namely, all chocolate, 1 vanilla and n   1 chocolate, 2 vanilla and n   2
chocolate,. . . , all vanilla. We deÔ¨Åne a generating function, D.x/, for counting these
donut selections by letting the coefÔ¨Åcient of xn be dn. So by equation (15.5)
D.x/ D
1
.1   x/2 :
(15.6)
More generally, suppose we have two kinds of things ‚Äîsay apples and bananas
‚Äîand some constraints on how many of each may be selected. Say there are an
ways to select n apples and bn ways to select n bananas. So the generating function
for counting apples would be
A.x/ WWD
1
X
nD0
anxn;
and for bananas would be
B.x/ WWD
1
X
nD0
bnxn:
Now suppose apples come in baskets of 6, so there is no way to select 1 to 5
apples, one way to select 6 apples, no way to select 7, etc. In other words,
an D
(
1
if n is a multiple of 6;
0
otherwise:
In this case we would have
A.x/ D 1 C x6 C x12 C    C x6n C   
D 1 C x6 C
 x62 C    C
 x6n C   
D
1
1   x6 :
Let‚Äôs also suppose there are two kinds of bananas ‚Äîred and yellow. Now bn D
n C 1 by the same reasoning used to count selections of n chocolate and vanilla
donuts, so we would have
B.x/ D
1
.1   x/2 :
So how many ways are there to select a mix of n apples and bananas? We could
select one apple in a1 ways and then n   1 bananas in bn 1 ways, for a total of

Chapter 15
Generating Functions
562
a1bn 1 ways to select n apples and bananas using only one apple. More generally,
we could select k apples in ak ways and then n   k bananas in bn k ways, for
a total of akbn k ways to select select n apples and bananas including exactly k
apples. So the total number of ways to select a mix of n apples and bananas is
a0bn C a1bn 1 C a2bn 1 C    C anb0:
(15.7)
Now here‚Äôs the cool connection between counting and generating functions: ex-
pression (15.7) is equal to the coefÔ¨Åcient of xn in the product A.x/B.x/.
15.2.1
Products of Generating Functions
In other words, we‚Äôre claiming that
Rule (Product).
≈íxn¬ç.A.x/  B.x// D a0bn C a1bn 1 C a2bn 1 C    C anb0:
(15.8)
To explain the generating function Product Rule, we can think about evaluating
the product A.x/  B.x/ by using a table to identify all the cross-terms from the
product of the sums:
b0x0
b1x1
b2x2
b3x3
: : :
a0x0
a0b0x0
a0b1x1
a0b2x2
a0b3x3
: : :
a1x1
a1b0x1
a1b1x2
a1b2x3
: : :
a2x2
a2b0x2
a2b1x3
: : :
a3x3
a3b0x3
: : :
:::
: : :
In this layout, all the terms involving the same power of x lie on a 45-degree sloped
diagonal. So the index-n diagonal contains all the xn-terms, and the coefÔ¨Åcient of
xn in the product A.x/  B.x/ is the sum of all the coefÔ¨Åcients of the terms on this
diagonal, namely, (15.7). The sequence of coefÔ¨Åcients of the product A.x/  B.x//
is called the convolution of the sequences .a0; a1; a2; : : : / and .b0; b1; b2; : : : /. In
addition to their algebraic role, convolutions of sequences play a prominent role in
signal processing and control theory.

15.2. Counting with Generating Functions
563
This Product Rule provides the algebraic justiÔ¨Åcation for the fact that a geometric
series equals 1=.1 x/ regardless of convergence. Namely, according to the Product
Rule, the product of the geometric series and the series
1 C . 1/x C 0x2 C    C 0xn C   
for 1   x is the series
1 C 0x C 0x2 C    C 0xn C   
for the constant 1. So with multiplication deÔ¨Åned by the Product Rule, the geomet-
ric series is the multiplicative inverse, 1=.1   x/, of 1   x.
Similar reasoning justiÔ¨Åes multiplying a generating function by a constant term
by term. That is, a special case of the Product Rule is the
Rule (Constant Factor). For any constant, c, and generating function, F.x/,
≈íxn¬ç.c  F.x// D c  ≈íxn¬çF.x/:
(15.9)
15.2.2
The Convolution Rule
We can summarize the discussion above with the
Rule (Convolution). Let A.x/ be the generating function for selecting items from
a set A, and let B.x/ be the generating function for selecting items from a set B
disjoint from A. The generating function for selecting items from the union A [ B
is the product A.x/  B.x/.
The Rule depends on a precise deÔ¨Ånition of what ‚Äúselecting items from the union
A [ B‚Äù means. Informally, the idea is that the restrictions on the selection of
items from sets A and B carry over to selecting items from A [ B. Formally, the
Convolution Rule applies when there is a bijection between n-element selections
from A[B and ordered pairs of selections from the sets A and B containing a total
of n elements. We think the informal statement is clear enough.
15.2.3
Counting Donuts with the Convolution Rule
We can use the Convolution Rule to derive in another way the generating function
D.x/ for the number of ways to select chocolate and vanilla donuts given in (15.6).
Namely, there is only one way to select exactly n chocolate donuts. That means
every coefÔ¨Åcient of the generating function for selecting n chocolate donuts equals
one. So the generating function for chocolate donut selections is 1=.1 x/; likewise

Chapter 15
Generating Functions
564
for the generating function for selecting only vanilla donuts. Now by the Convolu-
tion Rule, the generating function for the number of ways to select n donuts when
both chocolate and vanilla Ô¨Çavors are available is
D.x/ D
1
1   x 
1
1   x D
1
.1   x/2 :
So we have derived (15.6) without appeal to (15.5).
The Ô¨Årst general counting problem we considered was the number of ways to
select a n doughnuts when k Ô¨Çavors were available. Our application of the Convo-
lution Rule for two Ô¨Çavors carries right over to this general case, and we conclude
that the generating function for selections of donuts when k Ô¨Çavors are available is
1=.1   x/k. So we have
≈íxn¬ç

1
.1   x/k

D
 
n C .k   1/
n
!
(15.10)
by Corollary 14.5.3.
Extracting CoefÔ¨Åcients from Maclauren‚Äôs Theorem
We‚Äôve used a donut-counting argument to derive the coefÔ¨Åcients of 1=.1   x/k,
but it‚Äôs instructive to derive this coefÔ¨Åcient algebraically, which we can do using
Maclauren‚Äôs Theorem:
Theorem 15.2.1 (Maclauren‚Äôs Theorem).
f .x/ D f .0/ C f 0.0/x C f 00.0/
2≈†
x2 C f 000.0/
3≈†
x3 C    C f .n/.0/
n≈†
xn C    :
This theorem says that the nth coefÔ¨Åcient of 1=.1 x/k is equal to its nth deriva-
tive evaluated at 0 and divided by n≈†. Computing the nth derivative turns out not to
be very difÔ¨Åcult
d n
d nx
1
.1   x/k D k.k C 1/    .k C n   1/.1   x/ .kCn/
(see Problem 15.3), so
≈íxn¬ç

1
.1   x/k

D
 d n
d nx
1
.1   x/k

.0/ 1
n≈†
D k.k C 1/    .k C n   1/.1   0/ .kCn/
n≈†
D
 
n C .k   1/
n
!
:

15.2. Counting with Generating Functions
565
So instead of using the donut-counting formula (15.10) to Ô¨Ånd the coefÔ¨Åcients of
xn, we could have used this algebraic argument and the Convolution Rule to derive
the donut-counting formula.
15.2.4
The Binomial Theorem from the Convolution Rule
The Convolution Rule also provides a new perspective on the Binomial Theo-
rem 14.6.4. Here is how. First, consider a single-element set fa1g. The generating
function for the number of ways to select n elements from this set is simply 1 C x:
we have 1 way to select zero elements, 1 way to select the one element, and 0 ways
to select more than one element. Similarly, the number of ways to select n elements
from any single-element set faig has the same generating function 1 C x. Now by
the Convolution Rule, the generating function for choosing a subset of n elements
from the set fa1; a2; : : : ; amg is the product .1Cx/m of the generating function for
selecting from each of the m one-element sets. Since we know that the number of
ways to select n elements from a set of size m is
 m
n

, we conclude that that
≈íxn¬ç.1 C x/m D
 
m
n
!
;
which is a restatement of the Binomial Theorem 14.6.4.
So we have proved the Binomial Theorem without having to analyze the expan-
sion of the expression .1 C x/m into a sum of products.
15.2.5
An ‚ÄúImpossible‚Äù Counting Problem
So far everything we‚Äôve done with generating functions we could have done another
way. But here is an absurd counting problem ‚Äîreally over the top! In how many
ways can we Ô¨Åll a bag with n fruits subject to the following constraints?
 The number of apples must be even.
 The number of bananas must be a multiple of 5.
 There can be at most four oranges.
 There can be at most one pear.
For example, there are 7 ways to form a bag with 6 fruits:
Apples
6
4
4
2
2
0
0
Bananas
0
0
0
0
0
5
5
Oranges
0
2
1
4
3
1
0
Pears
0
0
1
0
1
0
1

Chapter 15
Generating Functions
566
These constraints are so complicated that getting a nice answer may seem impossi-
ble. But let‚Äôs see what generating functions reveal.
Let‚Äôs Ô¨Årst construct a generating function for choosing apples. We can choose a
set of 0 apples in one way, a set of 1 apple in zero ways (since the number of apples
must be even), a set of 2 apples in one way, a set of 3 apples in zero ways, and so
forth. So we have:
A.x/ D 1 C x2 C x4 C x6 C    D
1
1   x2
Similarly, the generating function for choosing bananas is:
B.x/ D 1 C x5 C x10 C x15 C    D
1
1   x5
Now, we can choose a set of 0 oranges in one way, a set of 1 orange in one way,
and so on. However, we cannot choose more than four oranges, so we have the
generating function:
O.x/ D 1 C x C x2 C x3 C x4 D 1   x5
1   x
Here we‚Äôre using the formula (13.2) for a Ô¨Ånite geometric sum. Finally, we can
choose only zero or one pear, so we have:
P.x/ D 1 C x
The Convolution Rule says that the generating function for choosing from among
all four kinds of fruit is:
A.x/B.x/O.x/P.x/ D
1
1   x2
1
1   x5
1   x5
1   x .1 C x/
D
1
.1   x/2
D 1 C 2x C 3x2 C 4x3 C   
Almost everything cancels! We‚Äôre left with 1=.1   x/2, which we found a power
series for earlier: the coefÔ¨Åcient of xn is simply nC1. Thus, the number of ways to
form a bag of n fruits is just n C 1. This is consistent with the example we worked
out, since there were 7 different fruit bags containing 6 fruits. Amazing!

15.3. Partial Fractions
567
15.3
Partial Fractions
We got a simple solution to the ‚Äúimpossible‚Äù counting problem of Section 15.2.5
because its generating function simpliÔ¨Åed to the expression 1=.1 x/2 whose power
series coefÔ¨Åcients we already knew. Of course the problem was contrived so this
would work out. To solve more general problems using generating functions, we
need ways to Ô¨Ånd power series coefÔ¨Åcients for generating functions given as formu-
las. Maclauren‚Äôs Theorem 15.2.1 is a very general method for Ô¨Ånding coefÔ¨Åcients,
but it only applies when formulas for repeated derivatives can be found, which isn‚Äôt
often. However, there is an automatic way to Ô¨Ånd the power series coefÔ¨Åcients
for any formula that is a quotient of polynomials, namely, by using the method of
partial fractions from elementary calculus.
The partial fraction method is based on the fact that quotients of polynomials
can be expressed as sums of terms whose power series coefÔ¨Åcients have nice for-
mulas. For example when the denominator polynomial has distint nonzero roots,
the method rests on
Lemma 15.3.1. Let p.x/ be a polynomial of degree less than n and let Àõ1; : : : ; Àõn
be distinct, nonzero numbers. Then there are constants c1; : : : ; cn such that
p.x/
.1   Àõ1x/.1   Àõ2x/    .1   Àõnx/ D
c1
1   Àõ1x C
c2
1   Àõ2x C    C
cn
1   Àõnx :
Let‚Äôs illustrate the use of Lemma 15.3.1 by Ô¨Ånding the power series coefÔ¨Åcients
for the function
R.x/ WWD
x
1   x   x2 :
We can use the quadratic formula to Ô¨Ånd the roots r1; r2 of the denominator, 1  x   x2, namely
r1 D  1  p
5
2
; r2 D  1 C
p
5
2
:
So
1   x   x2 D .x   r1/.x   r2/ D r1r2.1   x=r1/.1   x=r2/:
With a little algebra, we Ô¨Ånd that
R.x/ D
x
.1   Àõ1x/.1   Àõ2x/

Chapter 15
Generating Functions
568
where
Àõ1 D 1 C
p
5
2
Àõ2 D 1  p
5
2
:
Next we Ô¨Ånd c1 and c2 which satisfy:
x
.1   Àõ1x/.1   Àõ2x/ D
c1
1   Àõ1x C
c2
1   Àõ2x
(15.11)
In general, we can do this by plugging in a couple of values for x to generate two
linear equations in c1 and c2 and then solve the equations for c1 and c2. A simpler
approach in this case comes from multiplying both sides of (15.11) by the left hand
denominator to get
x D c1.1   Àõ2x/ C c2.1   Àõ1x/:
Now letting x D 1=Àõ2 we obtain
c2 D
1=Àõ2
1   Àõ1=Àõ2
D
1
Àõ2   Àõ1
D   1
p
5
;
and similarly, letting x D 1=Àõ1 we obtain
c1 D
1
p
5
:
Plugging these values for c1; c2 into equation (15.11) Ô¨Ånally gives the partial frac-
tion expansion
R.x/ D
x
1   x   x2 D
1
p
5

1
1   Àõ1x  1
1   Àõ2x

Each term in the partial fractions expansion has a simple power series given by the
geometric sum formula:
1
1   Àõ1x D 1 C Àõ1x C Àõ2
1x2 C   
1
1   Àõ2x D 1 C Àõ2x C Àõ2
2x2 C   
Substituting in these series gives a power series for the generating function:
R.x/ D
1
p
5
 .1 C Àõ1x C Àõ2
1x2 C    /   .1 C Àõ2x C Àõ2
2x2 C    /

;

15.4. Solving Linear Recurrences
569
so
≈íxn¬çR.x/ D Àõn
1   Àõn
2
p
5
D
1
p
5
  
1 C
p
5
2
!n
  
1  p
5
2
!n!
(15.12)
15.3.1
Partial Fractions with Repeated Roots
Lemma 15.3.1 generalizes to the case when the denominator polynomial has a re-
peated nonzero root with multiplicity m by expanding the quotient into a sum a
terms of the form
c
.1   Àõx/k
where Àõ is the reciprocal of the root and k  m. A formula for the coefÔ¨Åcients of
such a term follows from the donut formula (15.10). Namely,
≈íxn¬ç

c
.1   Àõx/k

D cÀõn
 
n   .k   1/
n
!
:
(15.13)
When Àõ D 1, this follows from the donut formula (15.10) and termwise multipli-
cation by the constant c. The case for arbitrary Àõ follows by substituting Àõx for x
in the power series; this changes xn into .Àõx/n and so has the effect of multiplying
the coefÔ¨Åcient of xn by Àõn.1
15.4
Solving Linear Recurrences
15.4.1
A Generating Function for the Fibonacci Numbers
The Fibonacci numbers f0; f1; : : : ; fn; : : : are deÔ¨Åned recursively as follows:
f0 WWD 0
f1 WWD 1
fn D WWDfn 1 C fn 2
(for n  2):
Generating functions will now allow us to derive an astonishing closed formula for
fn.
1In other words,
≈íxn¬çF.Àõx/ D Àõn  ≈íxn¬çF.x/:

Chapter 15
Generating Functions
570
Namely, let F.x/ be the generating function for the sequence of Fibonacci num-
bers, that is,
F.x/ WWD f0 C f1x C f2x2 C    fnxn C    :
Reasoning as we did at the start of this chapter to derive the formula for a geometric
series, we have
F.x/
D
f0
C
f1x
C
f2x2
C
  
C
fnxn C    :
 xF.x/
D
 f0x
 f1x2
   
 fn 1xn C    :
 x2F.x/
D
 f0x2
   
 fn 2xn C    :
F.x/.1   x   x2/
D
f0
C
.f1   f0/x
C
0x2
C
  
C
0xn C    :
D
0
C
1x
C
0x2
D
x;
so
F.x/ D
x
1   x   x2 :
But wait, F.x/ is the same as the function we used to illustrate the partial fraction
method for Ô¨Ånding coefÔ¨Åcients in Section 15.3. So by equation (15.12), we Ô¨Ånd
that
fn D
1
p
5
  
1 C
p
5
2
!n
  
1  p
5
2
!n!
As a formula for Fibonacci numbers, this is astonishing and maybe scary. From the
formula, it‚Äôs not even obvious that its value is an integer. But the formula is very
useful. For example, it provides (via the repeated squaring method) a much more
efÔ¨Åcient way to compute Fibonacci numbers than crunching through the recurrence.
It also clearly reveals the exponential growth of these numbers.
15.4.2
The Towers of Hanoi
According to legend, there is a temple in Hanoi with three posts and 64 gold disks
of different sizes. Each disk has a hole through the center so that it Ô¨Åts on a post.
In the misty past, all the disks were on the Ô¨Årst post, with the largest on the bottom
and the smallest on top, as shown in Figure 15.1.
Monks in the temple have labored through the years since to move all the disks
to one of the other two posts according to the following rules:
 The only permitted action is removing the top disk from one post and drop-
ping it onto another post.
 A larger disk can never lie above a smaller disk on any post.

15.4. Solving Linear Recurrences
571
Figure 15.1
The initial conÔ¨Åguration of the disks in the Towers of Hanoi problem.
So, for example, picking up the whole stack of disks at once and dropping them on
another post is illegal. That‚Äôs good, because the legend says that when the monks
complete the puzzle, the world will end!
To clarify the problem, suppose there were only 3 gold disks instead of 64. Then
the puzzle could be solved in 7 steps as shown in Figure 15.2.
The questions we must answer are, ‚ÄúGiven sufÔ¨Åcient time, can the monks suc-
ceed?‚Äù If so, ‚ÄúHow long until the world ends?‚Äù And, most importantly, ‚ÄúWill this
happen before the Ô¨Ånal exam?‚Äù
A Recursive Solution
The Towers of Hanoi problem can be solved recursively. As we describe the pro-
cedure, we‚Äôll also analyze the minimum number, tn, of steps required to solve the
n-disk problem. For example, some experimentation shows that t1 D 1 and t2 D 3.
The procedure illustrated above shows that t3 is at most 7, though there might be a
solution with fewer steps.
The recursive solution has three stages, which are described below and illustrated
in Figure 15.3. For clarity, the largest disk is shaded in the Ô¨Ågures.
Stage 1. Move the top n 1 disks from the Ô¨Årst post to the second using the solution
for n   1 disks. This can be done in tn 1 steps.
Stage 2. Move the largest disk from the Ô¨Årst post to the third post. This takes just
1 step.
Stage 3. Move the n   1 disks from the second post to the third post, again using
the solution for n   1 disks. This can also be done in tn 1 steps.
This algorithm shows that tn, the minimum number of steps required to move n
disks to a different post, is at most tn 1 C 1 C tn 1 D 2tn 1 C 1. We can use this
fact to upper bound the number of operations required to move towers of various

Chapter 15
Generating Functions
572
1
2
3
4
5
6
7
Figure 15.2
The 7-step solution to the Towers of Hanoi problem when there are
n D 3 disks.
1
2
3
Figure 15.3
A recursive solution to the Towers of Hanoi problem.

15.4. Solving Linear Recurrences
573
heights:
t3  2  t2 C 1 D 7
t4  2  t3 C 1  15
Continuing in this way, we could eventually compute an upper bound on t64, the
number of steps required to move 64 disks. So this algorithm answers our Ô¨Årst
question: given sufÔ¨Åcient time, the monks can Ô¨Ånish their task and end the world.
This is a shame. After all that effort, they‚Äôd probably want to smack a few high-Ô¨Åves
and go out for burgers and ice cream, but nope ‚Äîworld‚Äôs over.
Finding a Recurrence
We cannot yet compute the exact number of steps that the monks need to move the
64 disks, only an upper bound. Perhaps, having pondered the problem since the
beginning of time, the monks have devised a better algorithm.
In fact, there is no better algorithm, and here is why. At some step, the monks
must move the largest disk from the Ô¨Årst post to a different post. For this to happen,
the n   1 smaller disks must all be stacked out of the way on the only remaining
post. Arranging the n 1 smaller disks this way requires at least tn 1 moves. After
the largest disk is moved, at least another tn 1 moves are required to pile the n   1
smaller disks on top.
This argument shows that the number of steps required is at least 2tn 1 C 1.
Since we gave an algorithm using exactly that number of steps, we can now write
an expression for tn, the number of moves required to complete the Towers of Hanoi
problem with n disks:
t0 D 0
tn D 2tn 1 C 1
(for n  1):
Solving the Recurrence
We can now Ô¨Ånd a formula for tn using generating functions. Namely, let T .x/ be
the generating function for the tn‚Äôs, that is,
T .x/ WWD t0 C t1x C t2x2 C    tnxn C    :
Reasoning as we did for the Fibonacci recurrence, we have
T .x/
D
t0
C
t1x
C
  
C
tnxn C   
 2xT .x/
D
 2t0x
   
 2tn 1xn C   
 1=.1   x/
D
 1
 1x
   
 1xn C   
T .x/.1   2x/   1=.1   x/
D
t0   1
C
0x
C
  
C
0xn C   
D
 1;

Chapter 15
Generating Functions
574
so
T .x/.1   2x/ D
1
1   x   1 D
x
1   x ;
and
T .x/ D
x
.1   2x/.1   x/:
Using partial fractions,
x
.1   2x/.1   x/ D
c1
1   2x C
c2
1   x
for some constants c1; c2. Now multiplying both sides by the left hand denominator
gives
x D c1.1   x/ C c2.1   2x/:
Substituting 1=2 for x yields c1 D 1 and substituting 1 for x yields c2 D  1,
which gives
T .x/ D
1
1   2x  1
1   x :
Finally we can read off the simple formula for the numbers of steps needed to move
a stack of n disks:
tn D ≈íxn¬çT .x/ D ≈íxn¬ç

1
1   2x

  ≈íxn¬ç

1
1   x

D 2n   1:
15.4.3
Solving General Linear Recurrences
An equation of the form
f .n/ D c1f .n   1/ C c2f .n   2/ C    C cdf .n   d/ C h.n/
(15.14)
for constants ci 2 C is called a degree d linear recurrence with inhomogeneous
term h.n/.
The methods above extend straightforwardly to solving linear recurrences with a
large class of inhomogeneous terms. In particular, when the inhomogeneous term
itself has a generating function that can be expressed as a quotient of polynomials,
the approach used above to derive generating functions for the Fibonacci and Tower
of Hanoi examples carries over to yield a quotient of polynomials that deÔ¨Ånes the
generating function f .0/ C f .1/x C f .2/x2 C    . Then partial fractions can be
used to Ô¨Ånd a formula for f .n/ that is a linear combination of terms of the form
nkÀõn where k is a nonnegative integer  d and Àõ is the reciprocal of a root of
the denominator polynomial. For example, see Problems 15.11, 15.15, 15.14 and
15.12.

15.5. Formal Power Series
575
15.5
Formal Power Series
TBA - to appear
Problems for Section 15.3
Practice Problems
Problem 15.1.
You would like to buy a bouquet of Ô¨Çowers. You Ô¨Ånd an online service that will
make bouquets of lilies, roses and tulips, subject to the following constraints:
 there must be at most 1 lily,
 there must be an odd number of tulips,
 there must be at least two roses.
Example: A bouquet of no lilies, 3 tulips, and 5 roses satisÔ¨Åes the constraints.
Express B.x/, the generating function for the number of ways to select a bouquet
of n Ô¨Çowers, as a quotient of polynomials (or products of polynomials). You do not
need to simplify this expression.
Problem 15.2.
Write a formula for the generating function whose successive coefÔ¨Åcients are given
by the sequence:
(a) 0, 0, 1, 1, 1,. . .
(b) 1, 1, 0, 0, 0,. . .
(c) 1, 0, 1, 0, 1, 0, 1,...
(d) 1, 4, 6, 4, 1, 0, 0, 0,...
(e) 1, 1, 1/2, 1/6, 1/24, 1/120,...
(f) 1, 2, 3, 4, 5,. . .
(g) 1, 4, 9, 16, 25,. . .

Chapter 15
Generating Functions
576
Class Problems
Problem 15.3.
Let A.x/ D P1
nD0 anxn. Then it‚Äôs easy to check that
an D A.n/.0/
n≈†
;
where A.n/ is the nth derivative of A. Use this fact (which you may assume) instead
of the Convolution Counting Principle, to prove that
1
.1   x/k D
1
X
nD0
 
n C k   1
k   1
!
xn:
So if we didn‚Äôt already know the Bookkeeper Rule, we could have proved it from
this calculation and the Convolution Rule for generating functions.
Problem 15.4.
We are interested in generating functions for the number of different ways to com-
pose a bag of n donuts subject to various restrictions. For each of the restrictions
in (a)-(e) below, Ô¨Ånd a closed form for the corresponding generating function.
(a) All the donuts are chocolate and there are at least 3.
(b) All the donuts are glazed and there are at most 2.
(c) All the donuts are coconut and there are exactly 2 or there are none.
(d) All the donuts are plain and their number is a multiple of 4.
(e) The donuts must be chocolate, glazed, coconut, or plain with the numbers of
each Ô¨Çavor subject to the constraints above.
(f) Find a closed form for the number of ways to select n donuts subject to the
constraints of the previous part.
Problem 15.5. (a) Let
S.x/ WWD x2 C x
.1   x/3 :
What is the coefÔ¨Åcient of xn in the generating function series for S.x/?

15.5. Formal Power Series
577
(b) Explain why S.x/=.1   x/ is the generating function for the sums of squares.
That is, the coefÔ¨Åcient of xn in the series for S.x/=.1   x/ is Pn
kD1 k2.
(c) Use the previous parts to prove that
n
X
kD1
k2 D n.n C 1/.2n C 1/
6
:
Homework Problems
Problem 15.6.
We will use generating functions to determine how many ways there are to use
pennies, nickels, dimes, quarters, and half-dollars to give n cents change.
(a) Write the sequence Pn for the number of ways to use only pennies to change
n cents. Write the generating function for that sequence.
(b) Write the sequence Nn for the number of ways to use only nickels to change
n cents. Write the generating function for that sequence.
(c) Write the generating function for the number of ways to use only nickels and
pennies to change n cents.
(d) Write the generating function for the number of ways to use pennies, nickels,
dimes, quarters, and half-dollars to give n cents change.
(e) Explain how to use this function to Ô¨Ånd out how many ways are there to change
50 cents; you do not have to provide the answer or actually carry out the process.
Problem 15.7.
Taking derivatives of generating functions is another useful operation. This is done
termwise, that is, if
F.x/ D f0 C f1x C f2x2 C f3x3 C    ;
then
F 0.x/ WWD f1 C 2f2x C 3f3x2 C    :
For example,
1
.1   x/2 D

1
.1   x/
0
D 1 C 2x C 3x2 C   

Chapter 15
Generating Functions
578
so
H.x/ WWD
x
.1   x/2 D 0 C 1x C 2x2 C 3x3 C   
is the generating function for the sequence of nonnegative integers. Therefore
1 C x
.1   x/3 D H 0.x/ D 1 C 22x C 32x2 C 42x3 C    ;
so
x2 C x
.1   x/3 D xH 0.x/ D 0 C 1x C 22x2 C 32x3 C    C n2xn C   
is the generating function for the nonnegative integer squares.
(a) Prove that for all k 2 N, the generating function for the nonnegative integer
kth powers is a quotient of polynomials in x. That is, for all k 2 N there are
polynomials Rk.x/ and Sk.x/ such that
≈íxn¬ç
Rk.x/
Sk.x/

D nk:
(15.15)
Hint: Observe that the derivative of a quotient of polynomials is also a quotient of
polynomials. It is not necessary work out explicit formulas for Rk and Sk to prove
this part.
(b) Conclude that if f .n/ is a function on the nonnegative integers deÔ¨Åned recur-
sively in the form
f .n/ D af .n   1/ C bf .n   2/ C cf .n   3/ C p.n/Àõn
where the a; b; c; Àõ 2 C and p is a polynomial with complex coefÔ¨Åcients, then
the generating function for the sequence f .0/; f .1/; f .2/; : : : will be a quotient of
polynomials in x, and hence there is a closed form expression for f .n/.
Hint: Consider
Rk.Àõx/
Sk.Àõx/
Problem 15.8.
Miss McGillicuddy never goes outside without a collection of pets. In particular:
 She brings a positive number of songbirds, which always come in pairs.
 She may or may not bring her alligator, Freddy.

15.5. Formal Power Series
579
 She brings at least 2 cats.
 She brings two or more chihuahuas and labradors leashed together in a line.
Let Pn denote the number of different collections of n pets that can accompany
her, where we regard chihuahuas and labradors leashed up in different orders as
different collections, even if there are the same number chihuahuas and labradors
leashed in the line.
For example, P6 D 4 since there are 4 possible collections of 6 pets:
 2 songbirds, 2 cats, 2 chihuahuas leashed in line
 2 songbirds, 2 cats, 2 labradors leashed in line
 2 songbirds, 2 cats, a labrador leashed behind a chihuahua
 2 songbirds, 2 cats, a chihuahua leashed behind a labrador
And P7 D 16 since there are 16 possible collections of 7 pets:
 2 songbirds, 3 cats, 2 chihuahuas leashed in line
 2 songbirds, 3 cats, 2 labradors leashed in line
 2 songbirds, 3 cats, a labrador leashed behind a chihuahua
 2 songbirds, 3 cats, a chihuahua leashed behind a labrador
 4 collections consisting of 2 songbirds, 2 cats, 1 alligator, and a line of 2 dogs
 8 collections consisting of 2 songbirds, 2 cats, and a line of 3 dogs.
(a) Let
P.x/ WWD P0 C P1x C P2x2 C P3x3 C   
be the generating function for the number of Miss McGillicuddy‚Äôs pet collections.
Verify that
P.x/ D
4x6
.1   x/2.1   2x/:
(b) Find a simple formula for Pn.

Chapter 15
Generating Functions
580
Exam Problems
Problem 15.9.
T-Pain is planning an epic boat trip and he needs to decide what to bring with him.
 He must bring some burgers, but they only come in packs of 6.
 He and his two friends can‚Äôt decide whether they want to dress formally or
casually. He‚Äôll either bring 0 pairs of Ô¨Çip Ô¨Çops or 3 pairs.
 He doesn‚Äôt have very much room in his suitcase for towels, so he can bring
at most 2.
 In order for the boat trip to be truly epic, he has to bring at least 1 nautical-
themed pashmina afghan.
(a) Let B.x/ be the generating function for the number of ways to bring n burgers,
F.x/ for the number of ways to bring n pairs of Ô¨Çip Ô¨Çops, T .x/ for towels, and
A.x/ for Afghans. Write simple formulas for each of these.
B.x/ D
F.x/ D
T .x/ D
A.x/ D
(b) Let gn be the the number of different ways for T-Pain to bring n items (burg-
ers, pairs of Ô¨Çip Ô¨Çops, towels, and/or afghans) on his boat trip. Let G.x/ be the
generating function P1
nD0 gnxn. Verify that
G.x/ D
x7
.1   x/2 :
(c) Find a simple formula for gn.
Problems for Section 15.4
Practice Problems
Problem 15.10.
Let b, c, a0, a1, a2,. . .be real numbers such that
an D b.an 1/ C c
for n  1.
Let G.x/ be the generating function for this sequence.

15.5. Formal Power Series
581
(a) Express the coefÔ¨Åcient of xn for n  1 in the series expansion of bxG.x/ in
terms of b and ai for suitable i.
(b) The coefÔ¨Åcient of xn for n  1 in the series expansion of cx=.1   x/ is
(c) Therefore, G.x/   bxG.x/   cx=.1   x/ D
(d) Using the method of partial fractions, we can Ô¨Ånd real numbers d and e such
that
G.x/ D d=L.x/ C e=M.x/:
What are L.x/ and M.x/?
Class Problems
Problem 15.11.
The famous mathematician, Fibonacci, has decided to start a rabbit farm to Ô¨Åll up
his time while he‚Äôs not making new sequences to torment future college students.
Fibonacci starts his farm on month zero (being a mathematician), and at the start of
month one he receives his Ô¨Årst pair of rabbits. Each pair of rabbits takes a month
to mature, and after that breeds to produce one new pair of rabbits each month.
Fibonacci decides that in order never to run out of rabbits or money, every time a
batch of new rabbits is born, he‚Äôll sell a number of newborn pairs equal to the total
number of pairs he had three months earlier. Fibonacci is convinced that this way
he‚Äôll never run out of stock.
(a) DeÔ¨Åne the number, rn, of pairs of rabbits Fibonacci has in month n, using a
recurrence relation. That is, deÔ¨Åne rn in terms of various ri where i < n.
(b) Let R.x/ be the generating function for rabbit pairs,
R.x/ WWD r0 C r1x C r2x2 C   
Express R.x/ as a quotient of polynomials.
(c) Find a partial fraction decomposition of the generating function R.x/.
(d) Finally, use the partial fraction decomposition to come up with a closed form
expression for the number of pairs of rabbits Fibonacci has on his farm on month
n.
Problem 15.12.
Less well-known than the Towers of Hanoi ‚Äîbut no less fascinating ‚Äîare the

Chapter 15
Generating Functions
582
Towers of Sheboygan. As in Hanoi, the puzzle in Sheboygan involves 3 posts and
n rings of different sizes. The rings are placed on post #1 in order of size with the
smallest ring on top and largest on bottom.
The objective is to transfer all n rings to post #2 via a sequence of moves. As
in the Hanoi version, a move consists of removing the top ring from one post and
dropping it onto another post with the restriction that a larger ring can never lie
above a smaller ring. But unlike Hanoi, a local ordinance requires that a ring can
only be moved from post #1 to post #2, from post #2 to post #3, or from post
#3 to post #1. Thus, for example, moving a ring directly from post #1 to post #3 is
not permitted.
(a) One procedure that solves the Sheboygan puzzle is deÔ¨Åned recursively: to
move an initial stack of n rings to the next post, move the top stack of n   1 rings
to the furthest post by moving it to the next post two times, then move the big, nth
ring to the next post, and Ô¨Ånally move the top stack another two times to land on
top of the big ring. Let sn be the number of moves that this procedure uses. Write
a simple linear recurrence for sn.
(b) Let S.x/ be the generating function for the sequence hs0; s1; s2; : : : i. Care-
fully show that
S.x/ D
x
.1   x/.1   4x/:
(c) Give a simple formula for sn.
(d) A better (indeed optimal, but we won‚Äôt prove this) procedure to solve the Tow-
ers of Sheboygan puzzle can be deÔ¨Åned in terms of two mutually recursive proce-
dures, procedure P1.n/ for moving a stack of n rings 1 pole forward, and P2.n/
for moving a stack of n rings 2 poles forward. This is trivial for n D 0. For n > 0,
deÔ¨Åne:
P1.n/: Apply P2.n   1/ to move the top n   1 rings two poles forward to the third
pole. Then move the remaining big ring once to land on the second pole. Then
apply P2.n   1/ again to move the stack of n   1 rings two poles forward from the
third pole to land on top of the big ring.
P2.n/: Apply P2.n   1/ to move the top n   1 rings two poles forward to land on
the third pole. Then move the remaining big ring to the second pole. Then apply
P1.n   1/ to move the stack of n   1 rings one pole forward to land on the Ô¨Årst
pole. Now move the big ring 1 pole forward again to land on the third pole. Finally,
apply P2.n   1/ again to move the stack of n   1 rings two poles forward to land
on the big ring.

15.5. Formal Power Series
583
Let tn be the number of moves needed to solve the Sheboygan puzzle using proce-
dure P1.n/. Show that
tn D 2tn 1 C 2tn 2 C 3;
(15.16)
for n > 1.
Hint: Let un be the number of moves used by procedure P2.n/. Express each of tn
and un as linear combinations of tn 1 and un 1 and solve for tn.
(e) Derive values a; b; c; Àõ; Àá such that
tn D aÀõn C bÀán C c:
Conclude that tn D o.sn/.
Homework Problems
Problem 15.13.
Generating functions provide an interesting way to count the number of strings of
matched brackets. To do this, we‚Äôll use a description of these strings as the set,
GoodCount, of strings of brackets with a good count.
Namely, one precise way to determine if a string is matched is to start with 0
and read the string from left to right, adding 1 to the count for each left bracket
and subtracting 1 from the count for each right bracket. For example, here are the
counts for the two strings above
[
]
]
[
[
[
[
[
]
]
]
]
0
1
0
 1
0
1
2
3
4
3
2
1
0
[
[
[
]
]
[
]
]
[
]
0
1
2
3
2
1
2
1
0
1
0
A string has a good count if its running count never goes negative and ends with 0.
So the second string above has a good count, but the Ô¨Årst one does not because its
count went negative at the third step.
DeÔ¨Ånition. Let
GoodCount WWD fs 2 f] ; [ g j s has a good countg:
The matched strings can now be characterized precisely as this set of strings with
good counts.

Chapter 15
Generating Functions
584
Let cn be the number of strings in GoodCount with exactly n left brackets, and
let C.x/ be the generating function for these numbers:
C.x/ WWD c0 C c1x C c2x2 C    :
(a) The wrap of a string, s, is the string, [ s] , that starts with a left bracket fol-
lowed by the characters of s, and then ends with a right bracket. Explain why the
generating function for the wraps of strings with a good count is xC.x/.
Hint: The wrap of a string with good count also has a good count that starts and
ends with 0 and remains positive everywhere else.
(b) Explain why, for every string, s, with a good count, there is a unique sequence
of strings s1; : : : ; sk that are wraps of strings with good counts and s D s1    sk.
For example, the string r WWD [ [ ] ] [ ] [ [ ] [ ] ] 2 GoodCount equals s1s2s3 where
s1 WWD [ [ ] ] ; s2 WWD [ ] ; s3 WWD [ [ ] [ ] ] , and this is the only way to express r as a
sequence of wraps of strings with good counts.
(c) Conclude that
C D 1 C xC C .xC/2 C    C .xC/n C    ;
(15.17)
so
C D
1
1   xC ;
(15.18)
and hence
C D 1 Àô
p
1   4x
2x
:
(15.19)
Let D.x/ WWD 2xC.x/. Expressing D as a power series
D.x/ D d0 C d1x C d2x2 C    ;
we have
cn D dnC1
2
:
(15.20)
(d) Use (15.19), (15.20), and the value of c0 to conclude that
D.x/ D 1  p
1   4x:
(e) Prove that
dn D .2n   3/  .2n   5/    5  3  1  2n
n≈†
:
Hint: dn D D.n/.0/=n≈†

15.5. Formal Power Series
585
(f) Conclude that
cn D
1
n C 1
 
2n
n
!
:
Exam Problems
Problem 15.14.
DeÔ¨Åne the sequence r0; r1; r2; : : : recursively by the rule that r0 WWD 1 and
rn WWD 7rn 1 C .n C 1/
for n > 0:
Let R.x/ WWD P1
0 rnxn be the generating function of this sequence. Express R.x/
as a quotient of polynomials or products of polynomials. You do not have to Ô¨Ånd a
closed form for rn.
Problem 15.15.
Alyssa Hacker sends out a video that spreads like wildÔ¨Åre over the UToob network.
On the day of the release ‚Äîcall it day zero ‚Äîand the day following ‚Äîcall it day one
‚Äîthe video doesn‚Äôt receive any hits. However, starting with day two, the number
of hits, rn, can be expressed as seven times the number of hits on the previous day,
four times the number of hits the day before that, and the number of days that has
passed since the release of the video plus one. So, for example on day 2, there will
be 7  0 C 4  0 C 3 D 3 hits.
(a) Give a linear a recurrence for rn.
(b) Express the generating function R.x/ WWD P1
0 rnxn as a quotient of polyno-
mials or products of polynomials. You do not have to Ô¨Ånd a closed form for rn.


IV
Probability


Introduction
Probability is one of the most important disciplines in all of the sciences. It is also
one of the least well understood.
Probability is especially important in computer science‚Äîit arises in virtually
every branch of the Ô¨Åeld. In algorithm design and game theory, for example, ran-
domized algorithms and strategies (those that use a random number generator as a
key input for decision making) frequently outperform deterministic algorithms and
strategies. In information theory and signal processing, an understanding of ran-
domness is critical for Ô¨Åltering out noise and compressing data. In cryptography
and digital rights management, probability is crucial for achieving security. The
list of examples is long.
Given the impact that probability has on computer science, it seems strange that
probability should be so misunderstood by so many. Perhaps the trouble is that
basic human intuition is wrong as often as it is right when it comes to problems
involving random events. As a consequence, many students develop a fear of prob-
ability. Indeed, we have witnessed many graduate oral exams where a student will
solve the most horrendous calculation, only to then be tripped up by the simplest
probability question. Indeed, even some faculty will start squirming if you ask them
a question that starts ‚ÄúWhat is the probability that...?‚Äù
Our goal in the remaining chapters is to equip you with the tools that will enable
you to solve basic problems involving probability easily and conÔ¨Ådently.
Chapter 16 introduces the basic deÔ¨Ånitions and an elementary 4-step process
that can be used to determine the probability that a speciÔ¨Åed event occurs. We il-
lustrate the method on two famous problems where your intuition will probably fail
you. The key concepts of Conditional probability and independence are introduced,
along with examples of their use, and regrettable misuse, in practice: the probabil-
ity you have a disease given that a diagnostic test says you do, and the probability

Part IV
Probability
590
that a suspect is guilty given that his blood type matches the blood found at the
scene of the crime.
Random variables provide a more quantitative way to measure random events
and We study them in Chapter 17. For example, instead of determining the proba-
bility that it will rain, we may want to determine how much or how long it is likely
to rain. The fundamental concept of the expected value of a random variable is
introduced and some of its key properties are developed.
Chapter 18 examines the probability that a random variable deviates signiÔ¨Åcantly
from its expected value. Probability of deviation provides the theoretical basis for
estimation by sampling which is fundamental in science, engineering, and human
affairs. It is also especially important in engineering practice, where things are
generally Ô¨Åne if they are going as expected, and you would like to be assured that
the probability of an unexpected event is very low.
A Ô¨Ånal chapter applies the previously probabilitic tools to solve problems involv-
ing more complex random processes. You will see why you will probably never get
very far ahead at the casino and how two Stanford graduate students became bil-
lionaires by combining graph theory and probability theory to design a better search
engine for the web.

16
Events and Probability Spaces
16.1
Let‚Äôs Make a Deal
In the September 9, 1990 issue of Parade magazine, columnist Marilyn vos Savant
responded to this letter:
Suppose you‚Äôre on a game show, and you‚Äôre given the choice of three
doors. Behind one door is a car, behind the others, goats. You pick a
door, say number 1, and the host, who knows what‚Äôs behind the doors,
opens another door, say number 3, which has a goat. He says to you,
‚ÄùDo you want to pick door number 2?‚Äù Is it to your advantage to
switch your choice of doors?
Craig. F. Whitaker
Columbia, MD
The letter describes a situation like one faced by contestants in the 1970‚Äôs game
show Let‚Äôs Make a Deal, hosted by Monty Hall and Carol Merrill. Marilyn replied
that the contestant should indeed switch. She explained that if the car was behind
either of the two unpicked doors‚Äîwhich is twice as likely as the the car being
behind the picked door‚Äîthe contestant wins by switching. But she soon received
a torrent of letters, many from mathematicians, telling her that she was wrong. The
problem became known as the Monty Hall Problem and it generated thousands of
hours of heated debate.
This incident highlights a fact about probability: the subject uncovers lots of
examples where ordinary intuition leads to completely wrong conclusions. So until
you‚Äôve studied probabilities enough to have reÔ¨Åned your intuition, a way to avoid
errors is to fall back on a rigorous, systematic approach such as the Four Step
Method that we will describe shortly. First, let‚Äôs make sure we really understand
the setup for this problem. This is always a good thing to do when you are dealing
with probability.
16.1.1
Clarifying the Problem
Craig‚Äôs original letter to Marilyn vos Savant is a bit vague, so we must make some
assumptions in order to have any hope of modeling the game formally. For exam-
ple, we will assume that:

Chapter 16
Events and Probability Spaces
592
1. The car is equally likely to be hidden behind each of the three doors.
2. The player is equally likely to pick each of the three doors, regardless of the
car‚Äôs location.
3. After the player picks a door, the host must open a different door with a goat
behind it and offer the player the choice of staying with the original door or
switching.
4. If the host has a choice of which door to open, then he is equally likely to
select each of them.
In making these assumptions, we‚Äôre reading a lot into Craig Whitaker‚Äôs letter. There
are other plausible interpretations that lead to different answers. But let‚Äôs accept
these assumptions for now and address the question, ‚ÄúWhat is the probability that
a player who switches wins the car?‚Äù
16.2
The Four Step Method
Every probability problem involves some sort of randomized experiment, process,
or game. And each such problem involves two distinct challenges:
1. How do we model the situation mathematically?
2. How do we solve the resulting mathematical problem?
In this section, we introduce a four step approach to questions of the form, ‚ÄúWhat
is the probability that...?‚Äù In this approach, we build a probabilistic model step-
by-step, formalizing the original question in terms of that model. Remarkably, the
structured thinking that this approach imposes provides simple solutions to many
famously-confusing problems. For example, as you‚Äôll see, the four step method
cuts through the confusion surrounding the Monty Hall problem like a Ginsu knife.
16.2.1
Step 1: Find the Sample Space
Our Ô¨Årst objective is to identify all the possible outcomes of the experiment. A
typical experiment involves several randomly-determined quantities. For example,
the Monty Hall game involves three such quantities:
1. The door concealing the car.
2. The door initially chosen by the player.

16.2. The Four Step Method
593
car location
A
B
C
Figure 16.1
The Ô¨Årst level in a tree diagram for the Monty Hall Problem. The
branches correspond to the door behind which the car is located.
3. The door that the host opens to reveal a goat.
Every possible combination of these randomly-determined quantities is called an
outcome. The set of all possible outcomes is called the sample space for the exper-
iment.
A tree diagram is a graphical tool that can help us work through the four step
approach when the number of outcomes is not too large or the problem is nicely
structured. In particular, we can use a tree diagram to help understand the sample
space of an experiment. The Ô¨Årst randomly-determined quantity in our experiment
is the door concealing the prize. We represent this as a tree with three branches, as
shown in Figure 16.1. In this diagram, the doors are called A, B, and C instead of
1, 2, and 3, because we‚Äôll be adding a lot of other numbers to the picture later.
For each possible location of the prize, the player could initially choose any of
the three doors. We represent this in a second layer added to the tree. Then a third
layer represents the possibilities of the Ô¨Ånal step when the host opens a door to
reveal a goat, as shown in Figure 16.2.
Notice that the third layer reÔ¨Çects the fact that the host has either one choice
or two, depending on the position of the car and the door initially selected by the
player. For example, if the prize is behind door A and the player picks door B, then

Chapter 16
Events and Probability Spaces
594
car location
A
B
C
A
B
C
A
B
C
A
B
C
player‚Äôs
intial
guess
B
A
A
B
A
C
A
C
B
C
C
B
door
revealed
Figure 16.2
The full tree diagram for the Monty Hall Problem. The second level
indicates the door initially chosen by the player. The third level indicates the door
revealed by Monty Hall.

16.2. The Four Step Method
595
the host must open door C. However, if the prize is behind door A and the player
picks door A, then the host could open either door B or door C.
Now let‚Äôs relate this picture to the terms we introduced earlier: the leaves of the
tree represent outcomes of the experiment, and the set of all leaves represents the
sample space. Thus, for this experiment, the sample space consists of 12 outcomes.
For reference, we‚Äôve labeled each outcome in Figure 16.3 with a triple of doors
indicating:
.door concealing prize; door initially chosen; door opened to reveal a goat/:
In these terms, the sample space is the set
S D
 .A; A; B/; .A; A; C/; .A; B; C/; .A; C; B/; .B; A; C/; .B; B; A/;
.B; B; C/; .B; C; A/; .C; A; B/; .C; B; A/; .C; C; A/; .C; C; B/

The tree diagram has a broader interpretation as well: we can regard the whole
experiment as following a path from the root to a leaf, where the branch taken at
each stage is ‚Äúrandomly‚Äù determined. Keep this interpretation in mind; we‚Äôll use it
again later.
16.2.2
Step 2: DeÔ¨Åne Events of Interest
Our objective is to answer questions of the form ‚ÄúWhat is the probability that ...?‚Äù,
where, for example, the missing phrase might be ‚Äúthe player wins by switching‚Äù,
‚Äúthe player initially picked the door concealing the prize‚Äù, or ‚Äúthe prize is behind
door C.‚Äù Each of these phrases characterizes a set of outcomes. For example, the
outcomes speciÔ¨Åed by ‚Äúthe prize is behind door C‚Äù is:
f.C; A; B/; .C; B; A/; .C; C; A/; .C; C; B/g:
A set of outcomes is called an event and it is a subset of the sample space. So the
event that the player initially picked the door concealing the prize is the set:
f.A; A; B/; .A; A; C/; .B; B; A/; .B; B; C/; .C; C; A/; .C; C; B/g:
And what we‚Äôre really after, the event that the player wins by switching, is the set
of outcomes:
≈íswitching-wins¬ç
WWD f.A; B; C/; .A; C; B/; .B; A; C/; .B; C; A/; .C; A; B/; .C; B; A/g:
(16.1)
These outcomes have check marks in Figure 16.4.
Notice that exactly half of the outcomes are checked, meaning that the player
wins by switching in half of all outcomes. You might be tempted to conclude that
a player who switches wins with probability 1=2. This is wrong. The reason is that
these outcomes are not all equally likely, as we‚Äôll see shortly.

Chapter 16
Events and Probability Spaces
596
car location
A
B
C
A
B
C
A
B
C
A
B
C
player‚Äôs
intial
guess
B
A
A
B
A
C
A
C
B
C
C
B
door
revealed
outcome
.A;A;B/
.A;A;C/
.A;B;C/
.A;C;B/
.B;A;C/
.B;B;A/
.B;B;C/
.B;C;A/
.C;A;B/
.C;B;A/
.C;C;A/
.C;C;B/
Figure 16.3
The tree diagram for the Monty Hal Problem with the outcomes la-
beled for each path from root to leaf. For example, outcome .A; A; B/ corresponds
to the car being behind door A, the player initially choosing door A, and Monty
Hall revealing the goat behind door B.

16.2. The Four Step Method
597
car location
A
B
C
A
B
C
A
B
C
A
B
C
player‚Äôs
intial
guess
B
A
A
B
A
C
A
C
B
C
C
B
door
revealed
outcome
.A;A;B/
.A;A;C/
.A;B;C/
.A;C;B/
.B;A;C/
.B;B;A/
.B;B;C/
.B;C;A/
.C;A;B/
.C;B;A/
.C;C;A/
.C;C;B/
switch
wins
ÔÅî
ÔÅî
ÔÅî
ÔÅî
ÔÅî
ÔÅî
Figure 16.4
The tree diagram for the Monty Hall Problem where the outcomes
in the event where the player wins by switching are denoted with a check mark.

Chapter 16
Events and Probability Spaces
598
16.2.3
Step 3: Determine Outcome Probabilities
So far we‚Äôve enumerated all the possible outcomes of the experiment. Now we
must start assessing the likelihood of those outcomes. In particular, the goal of this
step is to assign each outcome a probability, indicating the fraction of the time this
outcome is expected to occur. The sum of all outcome probabilities must be one,
reÔ¨Çecting the fact that there always is an outcome.
Ultimately, outcome probabilities are determined by the phenomenon we‚Äôre mod-
eling and thus are not quantities that we can derive mathematically. However, math-
ematics can help us compute the probability of every outcome based on fewer and
more elementary modeling decisions. In particular, we‚Äôll break the task of deter-
mining outcome probabilities into two stages.
Step 3a: Assign Edge Probabilities
First, we record a probability on each edge of the tree diagram.
These edge-
probabilities are determined by the assumptions we made at the outset: that the
prize is equally likely to be behind each door, that the player is equally likely to
pick each door, and that the host is equally likely to reveal each goat, if he has a
choice. Notice that when the host has no choice regarding which door to open, the
single branch is assigned probability 1. For example, see Figure 16.5.
Step 3b: Compute Outcome Probabilities
Our next job is to convert edge probabilities into outcome probabilities. This is a
purely mechanical process:
the probability of an outcome is equal to the product of the edge-
probabilities on the path from the root to that outcome.
For example, the probability of the topmost outcome in Figure 16.5, .A; A; B/, is
1
3  1
3  1
2 D 1
18:
There‚Äôs an easy, intuitive justiÔ¨Åcation for this rule. As the steps in an experiment
progress randomly along a path from the root of the tree to a leaf, the probabilities
on the edges indicate how likely the path is to proceed along each branch. For
example, a path starting at the root in our example is equally likely to go down
each of the three top-level branches.
How likely is such a path to arrive at the topmost outcome, .A; A; B/? Well,
there is a 1-in-3 chance that a path would follow the A-branch at the top level,
a 1-in-3 chance it would continue along the A-branch at the second level, and 1-
in-2 chance it would follow the B-branch at the third level. Thus, it seems that

16.2. The Four Step Method
599
1 path in 18 should arrive at the .A; A; B/ leaf, which is precisely the probability
we assign it.
We have illustrated all of the outcome probabilities in Figure 16.5.
Specifying the probability of each outcome amounts to deÔ¨Åning a function that
maps each outcome to a probability. This function is usually called Pr≈í¬ç. In these
terms, we‚Äôve just determined that:
Pr≈í.A; A; B/¬ç D 1
18;
Pr≈í.A; A; C/¬ç D 1
18;
Pr≈í.A; B; C/¬ç D 1
9;
etc.
16.2.4
Step 4: Compute Event Probabilities
We now have a probability for each outcome, but we want to determine the proba-
bility of an event. The probability of an event E is denoted by Pr≈íE¬ç and it is the
sum of the probabilities of the outcomes in E. For example, the probability of the
[switching wins] event (16.1) is
Pr≈íswitching wins¬ç
D Pr≈í.A; B; C/¬ç C Pr≈í.A; C; B/¬ç C Pr≈í.B; A; C/¬çC
Pr≈í.B; C; A/¬ç C Pr≈í.C; A; B/¬ç C Pr≈í.C; B; A/¬ç
D 1
9 C 1
9 C 1
9 C 1
9 C 1
9 C 1
9
D 2
3:
It seems Marilyn‚Äôs answer is correct! A player who switches doors wins the car
with probability 2=3. In contrast, a player who stays with his or her original door
wins with probability 1=3, since staying wins if and only if switching loses.
We‚Äôre done with the problem! We didn‚Äôt need any appeals to intuition or inge-
nious analogies. In fact, no mathematics more difÔ¨Åcult than adding and multiplying
fractions was required. The only hard part was resisting the temptation to leap to
an ‚Äúintuitively obvious‚Äù answer.
16.2.5
An Alternative Interpretation of the Monty Hall Problem
Was Marilyn really right? Our analysis indicates that she was. But a more accurate
conclusion is that her answer is correct provided we accept her interpretation of the

Chapter 16
Events and Probability Spaces
600
car location
A
B
C
1=3
1=3
1=3
A
B
C
A
B
C
A
B
C
1=3
1=3
1=3
1=3
1=3
1=3
1=3
1=3
1=3
player‚Äôs
intial
guess
B
A
A
B
A
C
A
C
B
C
C
B
1=2
1=2
1
1
1
1=2
1=2
1
1
1
1=2
1=2
door
revealed
outcome
.A;A;B/
.A;A;C/
.A;B;C/
.A;C;B/
.B;A;C/
.B;B;A/
.B;B;C/
.B;C;A/
.C;A;B/
.C;B;A/
.C;C;A/
.C;C;B/
switch
wins
ÔÅî
ÔÅî
ÔÅî
ÔÅî
ÔÅî
ÔÅî
probability
1=18
1=18
1=9
1=9
1=9
1=18
1=18
1=9
1=9
1=9
1=18
1=18
Figure 16.5
The tree diagram for the Monty Hall Problem where edge weights
denote the probability of that branch being taken given that we are at the parent of
that branch. For example, if the car is behind door A, then there is a 1/3 chance that
the player‚Äôs initial selection is door B. The rightmost column shows the outcome
probabilities for the Monty Hall Problem. Each outcome probability is simply the
product of the probabilities on the path from the root to the outcome leaf.

16.3. Strange Dice
601
A
B
C
Figure 16.6
The strange dice. The number of pips on each concealed face is the
same as the number on the opposite face. For example, when you roll die A, the
probabilities of getting a 2, 6, or 7 are each 1=3.
question. There is an equally plausible interpretation in which Marilyn‚Äôs answer
is wrong. Notice that Craig Whitaker‚Äôs original letter does not say that the host is
required to reveal a goat and offer the player the option to switch, merely that he
did these things. In fact, on the Let‚Äôs Make a Deal show, Monty Hall sometimes
simply opened the door that the contestant picked initially. Therefore, if he wanted
to, Monty could give the option of switching only to contestants who picked the
correct door initially. In this case, switching never works!
16.3
Strange Dice
The four-step method is surprisingly powerful. Let‚Äôs get some more practice with
it. Imagine, if you will, the following scenario.
It‚Äôs a typical Saturday night. You‚Äôre at your favorite pub, contemplating the true
meaning of inÔ¨Ånite cardinalities, when a burly-looking biker plops down on the
stool next to you. Just as you are about to get your mind around pow.pow.R//,
biker dude slaps three strange-looking dice on the bar and challenges you to a $100
wager. His rules are simple. Each player selects one die and rolls it once. The
player with the lower value pays the other player $100.
Naturally, you are skeptical, especially after you see that these are not ordinary
dice. Each die has the usual six sides, but opposite sides have the same number on
them, and the numbers on the dice are different, as shown in Figure 16.6.
Biker dude notices your hesitation, so he sweetens his offer: he will pay you
$105 if you roll the higher number, but you only need pay him $100 if he rolls

Chapter 16
Events and Probability Spaces
602
higher, and he will let you pick a die Ô¨Årst, after which he will pick one of the other
two. The sweetened deal sounds persuasive since it gives you a chance to pick what
you think is the best die, so you decide you will play. But which of the dice should
you choose? Die B is appealing because it has a 9, which is a sure winner if it
comes up. Then again, die A has two fairly large numbers and die C has an 8 and
no really small values.
In the end, you choose die B because it has a 9, and then biker dude selects
die A. Let‚Äôs see what the probability is that you will win. (Of course, you probably
should have done this before picking die B in the Ô¨Årst place.) Not surprisingly, we
will use the four-step method to compute this probability.
16.3.1
Die A versus Die B
Step 1: Find the sample space.
The tree diagram for this scenario is shown in Figure 16.7. In particular, the sample
space for this experiment are the nine pairs of values that might be rolled with Die A
and Die B:
For this experiment, the sample space is a set of nine outcomes:
S D f .2; 1/; .2; 5/; .2; 9/; .6; 1/; .6; 5/; .6; 9/; .7; 1/; .7; 5/; .7; 9/ g:
Step 2: DeÔ¨Åne events of interest.
We are interested in the event that the number on die A is greater than the number
on die B. This event is a set of Ô¨Åve outcomes:
f .2; 1/; .6; 1/; .6; 5/; .7; 1/; .7; 5/ g:
These outcomes are marked A in the tree diagram in Figure 16.7.
Step 3: Determine outcome probabilities.
To Ô¨Ånd outcome probabilities, we Ô¨Årst assign probabilities to edges in the tree di-
agram. Each number on each die comes up with probability 1=3, regardless of
the value of the other die. Therefore, we assign all edges probability 1=3. The
probability of an outcome is the product of the probabilities on the correspond-
ing root-to-leaf path, which means that every outcome has probability 1=9. These
probabilities are recorded on the right side of the tree diagram in Figure 16.7.
Step 4: Compute event probabilities.
The probability of an event is the sum of the probabilities of the outcomes in that
event. In this case, all the outcome probabilities are the same, so we say that the
sample space is uniform. Computing event probabilities for uniform sample spaces

16.3. Strange Dice
603
2
6
7
1=3
1=3
1=3
die A
1=3
1=3
1=3
9
1
5
1=3
1=3
1=3
9
1
5
1=3
1=3
1=3
9
1
5
die B
winner
A
B
B
A
A
B
A
A
B
probability 
of outcome
1=9
1=9
1=9
1=9
1=9
1=9
1=9
1=9
1=9
Figure 16.7
The tree diagram for one roll of die A versus die B. Die A wins with
probability 5=9.

Chapter 16
Events and Probability Spaces
604
is particularly easy since you just have to compute the number of outcomes in the
event. In particular, for any event E in a uniform sample space S,
Pr≈íE¬ç D jEj
jSj :
(16.2)
In this case, E is the event that die A beats die B, so jEj D 5, jSj D 9, and
Pr≈íE¬ç D 5=9:
This is bad news for you. Die A beats die B more than half the time and, not
surprisingly, you just lost $100.
Biker dude consoles you on your ‚Äúbad luck‚Äù and, given that he‚Äôs a sensitive guy
beneath all that leather, he offers to go double or nothing.1 Given that your wallet
only has $25 in it, this sounds like a good plan. Plus, you Ô¨Ågure that choosing die A
will give you the advantage.
So you choose A, and then biker dude chooses C. Can you guess who is more
likely to win? (Hint: it is generally not a good idea to gamble with someone you
don‚Äôt know in a bar, especially when you are gambling with strange dice.)
16.3.2
Die A versus Die C
We can construct the three diagram and outcome probabilities as before. The result
is shown in Figure 16.8 and there is bad news again. Die C will beat die A with
probability 5=9, and you lose once again.
You now owe the biker dude $200 and he asks for his money. You reply that you
need to go to the bathroom.
16.3.3
Die B versus Die C
Being a sensitive guy, biker dude nods understandingly and offers yet another wa-
ger. This time, he‚Äôll let you have die C. He‚Äôll even let you raise the wager to $200
so you can win your money back.
This is too good a deal to pass up. You know that die C is likely to beat die A
and that die A is likely to beat die B, and so die C is surely the best. Whether biker
dude picks A or B, the odds would be in your favor this time. Biker dude must
really be a nice guy.
So you pick C, and then biker dude picks B. Wait, how come you haven‚Äôt
caught on yet and worked out the tree diagram before you took this bet :-) ? If
1Double or nothing is slang for doing another wager after you have lost the Ô¨Årst. If you lose again,
you will owe biker dude double what you owed him before. If you win, you will owe him nothing;
in fact, since he should pay you $210 if he loses, you would come out $10 ahead.

16.3. Strange Dice
605
3
4
8
1=3
1=3
1=3
die C
1=3
1=3
1=3
7
2
6
1=3
1=3
1=3
7
2
6
1=3
1=3
1=3
7
2
6
die A
winner
C
A
A
C
A
A
C
C
C
probability 
of outcome
1=9
1=9
1=9
1=9
1=9
1=9
1=9
1=9
1=9
Figure 16.8
The tree diagram for one roll of die C versus die A. Die C wins with
probability 5=9.

Chapter 16
Events and Probability Spaces
606
you do it now, you‚Äôll see by the same reasoning as before that B beats C with
probability 5=9. But surely there is a mistake! How is it possible that
C beats A with probability 5=9,
A beats B with probability 5=9,
B beats C with probability 5=9?
The problem is not with the math, but with your intuition. Since A will beat B
more often than not, and B will beat C more often than not, it seems like A ought
to beat C more often than not, that is, the ‚Äúbeats more often‚Äù relation ought to be
transitive. But this intuitive idea is simply false: whatever die you pick, biker dude
can pick one of the others and be likely to win. So picking Ô¨Årst is actually a big
disadvantage, and as a result, you now owe biker dude $400.
Just when you think matters can‚Äôt get worse, biker dude offers you one Ô¨Ånal
wager for $1,000. This time, instead of rolling each die once, you will each roll
your die twice, and your score is the sum of your rolls, and he will even let you
pick your die second, that is, after he picks his. Biker dude chooses die B. Now
you know that die A will beat die B with probability 5=9 on one roll, so, jumping
at this chance to get ahead, you agree to play, and you pick die A. After all, you
Ô¨Ågure that since a roll of die A beats a roll of die B more often that not, two rolls
of die A are even more likely to beat two rolls of die B, right?
Wrong! (Did we mention that playing strange gambling games with strangers in
a bar is a bad idea?)
16.3.4
Rolling Twice
If each player rolls twice, the tree diagram will have four levels and 34 D 81
outcomes. This means that it will take a while to write down the entire tree dia-
gram. But it‚Äôs easy to write down the Ô¨Årst two levels as in Figure 16.9(a) and then
notice that the remaining two levels consist of nine identical copies of the tree in
Figure 16.9(b).
The probability of each outcome is .1=3/4 D 1=81 and so, once again, we have a
uniform probability space. By equation (16.2), this means that the probability that
A wins is the number of outcomes where A beats B divided by 81.
To compute the number of outcomes where A beats B, we observe that the two
rolls of die A result in nine equally likely outcomes in a sample space SA in which
the two-roll sums take the values
.4; 8; 8; 9; 9; 12; 13; 13; 14/:

16.3. Strange Dice
607
1st A
roll
2nd A
roll
sum of
A rolls
2
2
7
6
7
7
6
2
2
6
6
7
4
8
9
8
12
13
9
13
14
1st B
roll
2nd B
roll
sum of
B rolls
1
1
9
5
9
9
5
1
1
5
5
9
2
6
10
6
10
14
10
14
18
‚Äπ
Figure 16.9
Parts of the tree diagram for die B versus die A where each die is
rolled twice. The Ô¨Årst two levels are shown in (a). The last two levels consist of
nine copies of the tree in (b).
Likewise, two rolls of die B result in nine equally likely outcomes in a sample
space SB in which the two-roll sums take the values
.2; 6; 6; 10; 10; 10; 14; 14; 18/:
We can treat the outcome of rolling both dice twice as a pair .x; y/ 2 SA  SB,
where A wins iff the sum of the two A-rolls of outcome x is larger the sum of the
two B-rolls of outcome y. If the A-sum is 4, there is only one y with a smaller
B-sum, namely, when the B-sum is 2. If the A-sum is 8, there are three y‚Äôs with
a smaller B-sum, namely, when the B-sum is 2 or 6. Continuing the count in this
way, the number of pairs .x; y/ for which the A-sum is larger than the B-sum is
1 C 3 C 3 C 3 C 3 C 6 C 6 C 6 C 6 D 37:
A similar count shows that there are 42 pairs for which B-sum is larger than the
A-sum, and there are two pairs where the sums are equal, namely, when they both
equal 14. This means that A loses to B with probability 42=81 > 1=2 and ties with
probability 2=81. Die A wins with probability only 37=81.
How can it be that A is more likely than B to win with one roll, but B is more
likely to win with two rolls? Well, why not? The only reason we‚Äôd think otherwise
is our unreliable, untrained intuition. (Even the authors were surprised when they

Chapter 16
Events and Probability Spaces
608
Ô¨Årst learned about this, but at least we didn‚Äôt lose $1400 to biker dude. :-) ) In fact,
the die strength reverses no matter which two die we picked. So for one roll,
A  B  C  A;
but for two rolls,
A  B  C  A;
where we have used the symbols  and  to denote which die is more likely to
result in the larger value.
Even Stranger Dice
The weird behavior of the three strange dice above generalizes in a remarkable
way.2 The idea is that you can Ô¨Ånd arbitrarily large sets of dice which will beat
each other in any desired pattern according to how many times the dice are rolled.
The precise statement of this result involves several alternations of universal and
existential quantiÔ¨Åers, so it may take a few readings to understand what it is saying:
Theorem 16.3.1. For any n  2, there is a set of n dice with the following property:
for any n-node digraph with exactly one directed edge between every two distinct
nodes,3 there is a number of rolls k such that the sum of k rolls of the ith die is
bigger than the sum for the jth die with probability greater than 1=2 iff there is an
edge from the ith to the jth node in the graph.
For example, the eight possible relative strengths for n D 3 dice are shown in
Figure 16.10.
Our analysis for the dice in Figure 16.6 showed that for one roll, we have the
relative strengths shown in Figure 16.10(a), and for two rolls, we have the (reverse)
relative strengths shown in Figure 16.10(b). If you are prone to gambling with
strangers in bars, it would be a good idea to try Ô¨Åguring out what other relative
strengths are possible for the dice in Figure 16.6 when using more rolls.
16.4
Set Theory and Probability
Let‚Äôs abstract what we‚Äôve just done with the Monty Hall and strange dice examples
into a general mathematical deÔ¨Ånition of sample spaces and probability.
2 TBA - Reference Ron Graham paper.
3In other words, for every pair of nodes u ¬§ v, either hu!vi or hv !ui, but not both, are edges
of the graph. Such graphs are called tournament graphs, see Problem 9.7.

16.4. Set Theory and Probability
609
D1
D3
D2
D1
D3
D2
D1
D3
D2
D1
D3
D2
D1
D3
D2
D1
D3
D2
D1
D3
D2
D1
D3
D2
.a/
.b/
.c/
.d/
.e/
.f /
.g/
.h/
Figure 16.10
All possible relative strengths for three dice D1, D2, and D3. The
edge
Àù
Di !Dj
Àõ
denotes that the sum of rolls for Di is likely to be greater than the
sum of rolls for Dj .
16.4.1
Probability Spaces
DeÔ¨Ånition 16.4.1. A countable sample space S is a nonempty countable set.4 An
element ! 2 S is called an outcome. A subset of S is called an event.
DeÔ¨Ånition 16.4.2. A probability function on a sample space S is a total function
Pr W S ! R such that
 Pr≈í!¬ç  0 for all ! 2 S, and
 P
!2S Pr≈í!¬ç D 1.
A sample space together with a probability function is called a probability space.
For any event E  S, the probability of E is deÔ¨Åned to be the sum of the probabil-
ities of the outcomes in E:
Pr≈íE¬ç WWD
X
!2E
Pr≈í!¬ç:
In the previous examples there were only Ô¨Ånitely many possible outcomes, but
we‚Äôll quickly come to examples that have a countably inÔ¨Ånite number of outcomes.
4Yes, sample spaces can be inÔ¨Ånite. If you did not read Chapter 7, don‚Äôt worry ‚Äîcountable just
means that you can list the elements of the sample space as !0, !1, !2, .. ..

Chapter 16
Events and Probability Spaces
610
The study of probability is closely tied to set theory because any set can be a
sample space and any subset can be an event. General probability theory deals
with uncountable sets like the set of real numbers, but we won‚Äôt need these, and
sticking to countable sets lets us deÔ¨Åne the probability of events using sums instead
of integrals. It also lets us avoid some distracting technical problems in set theory
like the Banach-Tarski ‚Äúparadox‚Äù mentioned in Chapter 7.
16.4.2
Probability Rules from Set Theory
Most of the rules and identities that we have developed for Ô¨Ånite sets extend very
naturally to probability.
An immediate consequence of the deÔ¨Ånition of event probability is that for dis-
joint events E and F ,
Pr≈íE [ F ¬ç D Pr≈íE¬ç C Pr≈íF ¬ç:
This generalizes to a countable number of events, as follows.
Rule 16.4.3 (Sum Rule). If fE0; E1; : : :g is collection of disjoint events, then
Pr
" [
n2N
En
#
D
X
n2N
Pr≈íEn¬ç:
The Sum Rule lets us analyze a complicated event by breaking it down into
simpler cases. For example, if the probability that a randomly chosen MIT student
is native to the United States is 60%, to Canada is 5%, and to Mexico is 5%, then
the probability that a random MIT student is native to North America is 70%.
Another consequence of the Sum Rule is that Pr≈íA¬ç C Pr≈íA¬ç D 1, which follows
because Pr≈íS¬ç D 1 and S is the union of the disjoint sets A and A. This equation
often comes up in the form:
Pr≈íA¬ç D 1   Pr≈íA¬ç:
(Complement Rule)
Sometimes the easiest way to compute the probability of an event is to compute the
probability of its complement and then apply this formula.
Some further basic facts about probability parallel facts about cardinalities of
Ô¨Ånite sets. In particular:
Pr≈íB   A¬ç D Pr≈íB¬ç   Pr≈íA \ B¬ç,
(Difference Rule)
Pr≈íA [ B¬ç D Pr≈íA¬ç C Pr≈íB¬ç   Pr≈íA \ B¬ç,
(Inclusion-Exclusion)
Pr≈íA [ B¬ç  Pr≈íA¬ç C Pr≈íB¬ç,
(Boole‚Äôs Inequality)
If A  B, then Pr≈íA¬ç  Pr≈íB¬ç.
(Monotonicity Rule)

16.4. Set Theory and Probability
611
The Difference Rule follows from the Sum Rule because B is the union of the
disjoint sets B   A and A \ B. Inclusion-Exclusion then follows from the Sum
and Difference Rules, because A [ B is the union of the disjoint sets A and B  A. Boole‚Äôs inequality is an immediate consequence of Inclusion-Exclusion since
probabilities are nonnegative. Monotonicity follows from the deÔ¨Ånition of event
probability and the fact that outcome probabilities are nonnegative.
The two-event Inclusion-Exclusion equation above generalizes to n events in
the same way as the corresponding Inclusion-Exclusion rule for n sets. Boole‚Äôs
inequality also generalizes to
Rule 16.4.4 (Union Bound).
Pr≈íE1 [    [ En¬ç  Pr≈íE1¬ç C    C Pr≈íEn¬ç:
(16.3)
This simple Union Bound is useful in many calculations. For example, suppose
that Ei is the event that the i-th critical component in a spacecraft fails. Then
E1 [    [ En is the event that some critical component fails. If Pn
iD1 Pr≈íEi¬ç
is small, then the Union Bound can give an adequate upper bound on this vital
probability.
16.4.3
Uniform Probability Spaces
DeÔ¨Ånition 16.4.5. A Ô¨Ånite probability space, S, is said to be uniform if Pr≈í!¬ç is the
same for every outcome ! 2 S.
As we saw in the strange dice problem, uniform sample spaces are particularly
easy to work with. That‚Äôs because for any event E  S,
Pr≈íE¬ç D jEj
jSj :
(16.4)
This means that once we know the cardinality of E and S, we can immediately
obtain Pr≈íE¬ç. That‚Äôs great news because we developed lots of tools for computing
the cardinality of a set in Part III.
For example, suppose that you select Ô¨Åve cards at random from a standard deck
of 52 cards. What is the probability of having a full house? Normally, this question
would take some effort to answer. But from the analysis in Section 14.7.2, we know
that
jSj D
 
52
5
!
and
jEj D 13 
 
4
3
!
 12 
 
4
2
!

Chapter 16
Events and Probability Spaces
612
1=2
1=2
1=2
1=2
H
H
H
H
T
T
T
T
1=2
1=2
1=2
1=2
1=2
1=4
1=8
1=16
1st 
player
1st 
player
2nd 
player
2nd 
player
Figure 16.11
The tree diagram for the game where players take turns Ô¨Çipping a
fair coin. The Ô¨Årst player to Ô¨Çip heads wins.
where E is the event that we have a full house. Since every Ô¨Åve-card hand is equally
likely, we can apply equation (16.4) to Ô¨Ånd that
Pr≈íE¬ç D 13  12 
 4
3


 4
2

 52
5

D 13  12  4  6  5  4  3  2
52  51  50  49  48
D
18
12495

1
694:
16.4.4
InÔ¨Ånite Probability Spaces
InÔ¨Ånite probability spaces are fairly common. For example, two players take turns
Ô¨Çipping a fair coin. Whoever Ô¨Çips heads Ô¨Årst is declared the winner. What is the
probability that the Ô¨Årst player wins? A tree diagram for this problem is shown in
Figure 16.11.
The event that the Ô¨Årst player wins contains an inÔ¨Ånite number of outcomes, but

16.4. Set Theory and Probability
613
we can still sum their probabilities:
Pr≈íÔ¨Årst player wins¬ç D 1
2 C 1
8 C 1
32 C
1
128 C   
D 1
2
1
X
nD0
1
4
n
D 1
2

1
1   1=4

D 2
3:
Similarly, we can compute the probability that the second player wins:
Pr≈ísecond player wins¬ç D 1
4 C 1
16 C 1
64 C
1
256 C    D 1
3:
In this case, the sample space is the inÔ¨Ånite set
S WWD f TnH j n 2 N g;
where Tn stands for a length n string of T‚Äôs. The probability function is
Pr≈íTnH¬ç WWD
1
2nC1 :
To verify that this is a probability space, we just have to check that all the probabili-
ties are nonnegative and that they sum to 1. Nonnegativity is obvious, and applying
the formula for the sum of a geometric series, we Ô¨Ånd that
X
n2N
Pr≈íTnH¬ç D
X
n2N
1
2nC1 D 1:
Notice that this model does not have an outcome corresponding to the possi-
bility that both players keep Ô¨Çipping tails forever ‚Äîin the diagram, Ô¨Çipping for-
ever corresponds to following the inÔ¨Ånite path in the tree without ever reaching
a leaf/outcome. If leaving this possibility out of the model bothers you, you‚Äôre
welcome to Ô¨Åx it by adding another outcome, !forever, to indicate that that‚Äôs what
happened. Of course since the probabililities of the other outcomes already sum to
1, you have to deÔ¨Åne the probability of !forever to be 0. Now outcomes with prob-
ability zero will have no impact on our calculations, so there‚Äôs no harm in adding
it in if it makes you happier. On the other hand, in countable probability spaces
it isn‚Äôt necessary to have outcomes with probability zero, and we will generally
ignore them.

Chapter 16
Events and Probability Spaces
614
16.5
Conditional Probability
Suppose that we pick a random person in the world. Everyone has an equal chance
of being selected. Let A be the event that the person is an MIT student, and let
B be the event that the person lives in Cambridge. What are the probabilities of
these events? Intuitively, we‚Äôre picking a random point in the big ellipse shown in
Figure 16.12 and asking how likely that point is to fall into region A or B.
set of all people
in the world
set of people 
who live in 
Cambridge
set of MIT
students
B
A
Figure 16.12
Selecting a random person. A is the event that the person is an MIT
student. B is the event that the person lives in Cambridge.
The vast majority of people in the world neither live in Cambridge nor are MIT
students, so events A and B both have low probability. But what about the prob-
ability that a person is an MIT student, given that the person lives in Cambridge?
This should be much greater ‚Äîbut what is it exactly?
What we‚Äôre asking for is called a conditional probability; that is, the probability
that one event happens, given that some other event deÔ¨Ånitely happens. Questions
about conditional probabilities come up all the time:
 What is the probability that it will rain this afternoon, given that it is cloudy
this morning?
 What is the probability that two rolled dice sum to 10, given that both are
odd?

16.5. Conditional Probability
615
 What is the probability that I‚Äôll get four-of-a-kind in Texas No Limit Hold
‚ÄôEm Poker, given that I‚Äôm initially dealt two queens?
There is a special notation for conditional probabilities. In general, Pr

A j B

denotes the probability of event A, given that event B happens. So, in our example,
Pr

A j B

is the probability that a random person is an MIT student, given that he
or she is a Cambridge resident.
How do we compute Pr

A j B

? Since we are given that the person lives in
Cambridge, we can forget about everyone in the world who does not. Thus, all
outcomes outside event B are irrelevant. So, intuitively, Pr

A j B

should be the
fraction of Cambridge residents that are also MIT students; that is, the answer
should be the probability that the person is in set A \ B (the darkly shaded region
in Figure 16.12) divided by the probability that the person is in set B (the lightly
shaded region). This motivates the deÔ¨Ånition of conditional probability:
DeÔ¨Ånition 16.5.1.
Pr

A j B

WWD Pr≈íA \ B¬ç
Pr≈íB¬ç
If Pr≈íB¬ç D 0, then the conditional probability Pr

A j B

is undeÔ¨Åned.
Pure probability is often counterintuitive, but conditional probability is even
worse! Conditioning can subtly alter probabilities and produce unexpected results
in randomized algorithms and computer systems as well as in betting games. Yet,
the mathematical deÔ¨Ånition of conditional probability given above is very simple
and should give you no trouble ‚Äîprovided that you rely on mathematical reasoning
and not intuition. The four-step method will also be very helpful as we will see in
the next examples.
16.5.1
The Four-Step Method for Conditional Probability: The
‚ÄúHalting Problem‚Äù
The Halting Problem was the Ô¨Årst example of a property that could not be tested
by any program. It was introduced by Alan Turing in his seminal 1936 paper. The
problem is to determine whether a Turing machine halts on a given ...yadda yadda
yadda . . . more importantly, it was the name of the MIT EECS department‚Äôs famed
C-league hockey team.
In a best-of-three tournament, the Halting Problem wins the Ô¨Årst game with prob-
ability 1=2. In subsequent games, their probability of winning is determined by the
outcome of the previous game. If the Halting Problem won the previous game,
then they are invigorated by victory and win the current game with probability 2=3.
If they lost the previous game, then they are demoralized by defeat and win the

Chapter 16
Events and Probability Spaces
616
current game with probability only 1=3. What is the probability that the Halting
Problem wins the tournament, given that they win the Ô¨Årst game?
This is a question about a conditional probability. Let A be the event that the
Halting Problem wins the tournament, and let B be the event that they win the Ô¨Årst
game. Our goal is then to determine the conditional probability Pr

A j B

.
We can tackle conditional probability questions just like ordinary probability
problems: using a tree diagram and the four step method. A complete tree diagram
is shown in Figure 16.13.
W
W
W
L
L
L
W
L
W
L
1=2
1=2
2=3
1=3
2=3
1=3
1=3
2=3
1=3
2=3
WW
WLW
WLL
LWW
LWL
LL
ÔÅî
ÔÅî
ÔÅî
ÔÅî
ÔÅî
ÔÅî
1=3
1=18
1=9
1=9
1=18
1=3
game 1
game 2
game 3
outcome
event A:
win the
series
event B:
win
game 1
outcome
probability
Figure 16.13
The tree diagram for computing the probability that the ‚ÄúHalting
Problem‚Äù wins two out of three games given that they won the Ô¨Årst game.
Step 1: Find the Sample Space
Each internal vertex in the tree diagram has two children, one corresponding to
a win for the Halting Problem (labeled W ) and one corresponding to a loss (la-
beled L). The complete sample space is:
S D fW W; WLW; WLL; LW W; LWL; LLg:
Step 2: DeÔ¨Åne Events of Interest
The event that the Halting Problem wins the whole tournament is:
T D fW W; WLW; LW W g:
And the event that the Halting Problem wins the Ô¨Årst game is:
F D fW W; WLW; WLLg:

16.5. Conditional Probability
617
The outcomes in these events are indicated with check marks in the tree diagram in
Figure 16.13.
Step 3: Determine Outcome Probabilities
Next, we must assign a probability to each outcome. We begin by labeling edges
as speciÔ¨Åed in the problem statement. SpeciÔ¨Åcally, The Halting Problem has a 1=2
chance of winning the Ô¨Årst game, so the two edges leaving the root are each as-
signed probability 1=2. Other edges are labeled 1=3 or 2=3 based on the outcome
of the preceding game. We then Ô¨Ånd the probability of each outcome by multi-
plying all probabilities along the corresponding root-to-leaf path. For example, the
probability of outcome WLL is:
1
2  1
3  2
3 D 1
9:
Step 4: Compute Event Probabilities
We can now compute the probability that The Halting Problem wins the tourna-
ment, given that they win the Ô¨Årst game:
Pr

A j B

D Pr≈íA \ B¬ç
Pr≈íB¬ç
D
Pr≈ífW W; WLW g¬ç
Pr≈ífW W; WLW; WLLg¬ç
D
1=3 C 1=18
1=3 C 1=18 C 1=9
D 7
9:
We‚Äôre done! If the Halting Problem wins the Ô¨Årst game, then they win the whole
tournament with probability 7=9.
16.5.2
Why Tree Diagrams Work
We‚Äôve now settled into a routine of solving probability problems using tree dia-
grams. But we‚Äôve left a big question unaddressed: what is the mathematical justiÔ¨Å-
cation behind those funny little pictures? Why do they work?
The answer involves conditional probabilities.
In fact, the probabilities that
we‚Äôve been recording on the edges of tree diagrams are conditional probabilities.
For example, consider the uppermost path in the tree diagram for the Halting Prob-
lem, which corresponds to the outcome W W . The Ô¨Årst edge is labeled 1=2, which
is the probability that the Halting Problem wins the Ô¨Årst game. The second edge

Chapter 16
Events and Probability Spaces
618
is labeled 2=3, which is the probability that the Halting Problem wins the second
game, given that they won the Ô¨Årst ‚Äîthat‚Äôs a conditional probability! More gener-
ally, on each edge of a tree diagram, we record the probability that the experiment
proceeds along that path, given that it reaches the parent vertex.
So we‚Äôve been using conditional probabilities all along. But why can we multiply
edge probabilities to get outcome probabilities? For example, we concluded that:
Pr≈íW W ¬ç D 1
2  2
3 D 1
3:
Why is this correct?
The answer goes back to DeÔ¨Ånition 16.5.1 of conditional probability which could
be written in a form called the Product Rule for probabilities:
Rule (Product Rule: 2 Events). If Pr≈íE1¬ç ¬§ 0, then:
Pr≈íE1 \ E2¬ç D Pr≈íE1¬ç  Pr

E2 j E1

:
Multiplying edge probabilities in a tree diagram amounts to evaluating the right
side of this equation. For example:
Pr≈íwin Ô¨Årst game \ win second game¬ç
D Pr≈íwin Ô¨Årst game¬ç  Pr

win second game j win Ô¨Årst game

D 1
2  2
3:
So the Product Rule is the formal justiÔ¨Åcation for multiplying edge probabilities to
get outcome probabilities! Of course to justify multiplying edge probabilities along
longer paths, we need a Product Rule for n events.
Rule (Product Rule: n Events).
Pr≈íE1 \ E2 \ : : : \ En¬ç D Pr≈íE1¬ç  Pr

E2 j E1

 Pr

E3 j E1 \ E2

  
 Pr

En j E1 \ E2 \ : : : \ En 1

provided that
Pr≈íE1 \ E2 \    \ En 1¬ç ¬§ 0:
This rule follows by routine induction from the deÔ¨Ånition of conditional proba-
bility.

16.5. Conditional Probability
619
16.5.3
Medical Testing
There is an unpleasant condition called BO suffered by 10% of the population.
There are no prior symptoms; victims just suddenly start to stink. Fortunately,
there is a test for latent BO before things start to smell. The test is not perfect,
however:
 If you have the condition, there is a 10% chance that the test will say you do
not have it. These are called ‚Äúfalse negatives.‚Äù
 If you do not have the condition, there is a 30% chance that the test will say
you do. These are ‚Äúfalse positives.‚Äù
Suppose a random person is tested for latent BO. If the test is positive, then what
is the probability that the person has the condition?
Step 1: Find the Sample Space
The sample space is found with the tree diagram in Figure 16.14.
yes
pos
no
neg
pos
neg
0:9
0:1
0:9
0:1
0:3
0:7
ÔÅî
ÔÅî
ÔÅî
ÔÅî
0:09
0:01
0:27
0:63
person
has BO
test result
event A:
has BO
event B:
tests 
positive
outcome
probability
ÔÅî
event 
A\B
Figure 16.14
The tree diagram for the BO problem.
Step 2: DeÔ¨Åne Events of Interest
Let A be the event that the person has BO. Let B be the event that the test was
positive. The outcomes in each event are marked in the tree diagram. We want

Chapter 16
Events and Probability Spaces
620
to Ô¨Ånd Pr

A j B

, the probability that a person has BO, given that the test was
positive.
Step 3: Find Outcome Probabilities
First, we assign probabilities to edges. These probabilities are drawn directly from
the problem statement. By the Product Rule, the probability of an outcome is the
product of the probabilities on the corresponding root-to-leaf path. All probabilities
are shown in Figure 16.14.
Step 4: Compute Event Probabilities
From DeÔ¨Ånition 16.5.1, we have
Pr

A j B

D Pr≈íA \ B¬ç
Pr≈íB¬ç
D
0:09
0:09 C 0:27 D 1
4:
So, if you test positive, then there is only a 25% chance that you have the condition!
This answer is initially surprising, but makes sense on reÔ¨Çection. There are two
ways you could test positive. First, it could be that you have the condition and the
test is correct. Second, it could be that you are healthy and the test is incorrect. The
problem is that almost everyone is healthy; therefore, most of the positive results
arise from incorrect tests of healthy people!
We can also compute the probability that the test is correct for a random person.
This event consists of two outcomes. The person could have the condition and
test positive (probability 0:09), or the person could be healthy and test negative
(probability 0:63). Therefore, the test is correct with probability 0:09 C 0:63 D
0:72. This is a relief; the test is correct almost three-quarters of the time.
But wait! There is a simple way to make the test correct 90% of the time: always
return a negative result! This ‚Äútest‚Äù gives the right answer for all healthy people
and the wrong answer only for the 10% that actually have the condition. So a better
strategy by this measure is to completely ignore the test result!
There is a similar paradox in weather forecasting. During winter, almost all days
in Boston are wet and overcast. Predicting miserable weather every day may be
more accurate than really trying to get it right!
16.5.4
A Posteriori Probabilities
If you think about it too much, the medical testing problem we just considered
could start to trouble you. The concern would be that by the time you take the test,
you either have the BO condition or you don‚Äôt ‚Äîyou just don‚Äôt know which it is.
So you may wonder if a statement like ‚ÄúIf you tested positive, then you have the
condition with probability 25%‚Äù makes sense.

16.5. Conditional Probability
621
In fact, such a statement does make sense. It means that 25% of the people who
test positive actually have the condition. It is true that any particular person has it
or they don‚Äôt, but a randomly selected person among those who test positive will
have the condition with probability 25%.
Anyway, if the medical testing example bothers you, you will deÔ¨Ånitely be wor-
ried by the following examples, which go even further down this path.
16.5.5
The ‚ÄúHalting Problem,‚Äù in Reverse
Suppose that we turn the hockey question around: what is the probability that the
Halting Problem won their Ô¨Årst game, given that they won the series?
This seems like an absurd question! After all, if the Halting Problem won the
series, then the winner of the Ô¨Årst game has already been determined. Therefore,
who won the Ô¨Årst game is a question of fact, not a question of probability. However,
our mathematical theory of probability contains no notion of one event preceding
another‚Äîthere is no notion of time at all. Therefore, from a mathematical perspec-
tive, this is a perfectly valid question. And this is also a meaningful question from
a practical perspective. Suppose that you‚Äôre told that the Halting Problem won the
series, but not told the results of individual games. Then, from your perspective, it
makes perfect sense to wonder how likely it is that The Halting Problem won the
Ô¨Årst game.
A conditional probability Pr

B j A

is called a posteriori if event B precedes
event A in time. Here are some other examples of a posteriori probabilities:
 The probability it was cloudy this morning, given that it rained in the after-
noon.
 The probability that I was initially dealt two queens in Texas No Limit Hold
‚ÄôEm poker, given that I eventually got four-of-a-kind.
Mathematically, a posteriori probabilities are no different from ordinary probabil-
ities; the distinction is only at a higher, philosophical level. Our only reason for
drawing attention to them is to say, ‚ÄúDon‚Äôt let them rattle you.‚Äù
Let‚Äôs return to the original problem. The probability that the Halting Problem
won their Ô¨Årst game, given that they won the series is Pr

B j A

. We can com-
pute this using the deÔ¨Ånition of conditional probability and the tree diagram in
Figure 16.13:
Pr

B j A

D Pr≈íB \ A¬ç
Pr≈íA¬ç
D
1=3 C 1=18
1=3 C 1=18 C 1=9 D 7
9:
This answer is suspicious! In the preceding section, we showed that Pr

A j B

was also 7=9. Could it be true that Pr

A j B

D Pr

B j A

in general? Some

Chapter 16
Events and Probability Spaces
622
reÔ¨Çection suggests this is unlikely. For example, the probability that I feel uneasy,
given that I was abducted by aliens, is pretty large. But the probability that I was
abducted by aliens, given that I feel uneasy, is rather small.
Let‚Äôs work out the general conditions under which Pr

A j B

D Pr

B j A

.
By the deÔ¨Ånition of conditional probability, this equation holds if an only if:
Pr≈íA \ B¬ç
Pr≈íB¬ç
D Pr≈íA \ B¬ç
Pr≈íA¬ç
This equation, in turn, holds only if the denominators are equal or the numerator
is 0; namely if
Pr≈íB¬ç D Pr≈íA¬ç
or
Pr≈íA \ B¬ç D 0:
The former condition holds in the hockey example; the probability that the Halting
Problem wins the series (event A) is equal to the probability that it wins the Ô¨Årst
game (event B) since both probabilities are 1=2.
In general, such pairs of probabilities are related by Bayes‚Äô Rule:
Theorem 16.5.2 (Bayes‚Äô Rule). If Pr≈íA¬ç and Pr≈íB¬ç are nonzero, then:
Pr

B j A

D Pr

A j B

 Pr≈íB¬ç
Pr≈íA¬ç
(16.5)
Proof. When Pr≈íA¬ç and Pr≈íB¬ç are nonzero, we have
Pr

A j B

 Pr≈íB¬ç D Pr≈íA \ B¬ç D Pr

B j A

 Pr≈íA¬ç
by deÔ¨Ånition of conditional probability. Dividing by Pr≈íA¬ç gives (16.5).

16.5.6
The Law of Total Probability
Breaking a probability calculation into cases simpliÔ¨Åes many problems. The idea
is to calculate the probability of an event A by splitting into two cases based on
whether or not another event E occurs. That is, calculate the probability of A \ E
and A\E. By the Sum Rule, the sum of these probabilities equals Pr≈íA¬ç. Express-
ing the intersection probabilities as conditional probabilities yields:
Rule 16.5.3 (Law of Total Probability, single event). If Pr≈íE¬ç and Pr≈íE¬ç are nonzero,
then
Pr≈íA¬ç D Pr

A j E

 Pr≈íE¬ç C Pr

A
ÀáÀá E

 Pr≈íE¬ç:
For example, suppose we conduct the following experiment. First, we Ô¨Çip a fair
coin. If heads comes up, then we roll one die and take the result. If tails comes up,
then we roll two dice and take the sum of the two results. What is the probability

16.5. Conditional Probability
623
that this process yields a 2? Let E be the event that the coin comes up heads,
and let A be the event that we get a 2 overall. Assuming that the coin is fair,
Pr≈íE¬ç D Pr≈íE¬ç D 1=2. There are now two cases. If we Ô¨Çip heads, then we roll
a 2 on a single die with probability Pr

A j E

D 1=6. On the other hand, if we
Ô¨Çip tails, then we get a sum of 2 on two dice with probability Pr

A
ÀáÀá E

D 1=36.
Therefore, the probability that the whole process yields a 2 is
Pr≈íA¬ç D 1
2  1
6 C 1
2  1
36 D 7
72:
There is also a form of the rule to handle more than two cases.
Rule 16.5.4 (Law of Total Probability). If E1; : : : ; En are disjoint events whose
union is the whole sample space, then:
Pr≈íA¬ç D
n
X
iD1
Pr

A j Ei

 Pr≈íEi¬ç:
16.5.7
Conditioning on a Single Event
The probability rules that we derived in Section 16.4.2 extend to probabilities con-
ditioned on the same event. For example, the Inclusion-Exclusion formula for two
sets holds when all probabilities are conditioned on an event C:
Pr

A [ B j C

D Pr

A j C

C Pr

B j C

  Pr

A \ B j C

:
This is easy to verify by plugging in the DeÔ¨Ånition 16.5.1 of conditional probabil-
ity.5
It is important not to mix up events before and after the conditioning bar. For
example, the following is not a valid identity:
False Claim.
Pr

A j B [ C

D Pr

A j B

C Pr

A j C

  Pr

A j B \ C

:
(16.6)
A simple counter-example is to let B and C be events over a uniform space with
most of their outcomes in A, but not overlapping. This ensures that Pr

A j B

and
Pr

A j C

are both close to 1. For example,
B WWD ≈í0; 9¬ç;
C WWD ≈í10; 18¬ç [ f0g;
A WWD ≈í1; 18¬ç;
5Problem 16.23 explains why this and similar conditional identities follow on general principles
from the corresponding unconditional identities.

Chapter 16
Events and Probability Spaces
624
so
Pr

A j B

D 9
10 D Pr

A j C

:
Also, since 0 is the only outcome in B \ C and 0 ‚Ä¶ A, we have
Pr

A j B \ C

D 0
So the right hand side of (16.6) is 1.8, while the left hand side is a probability which
can be at most 1 ‚Äîactually, it is 18/19.
16.5.8
Discrimination Lawsuit
Several years ago there was a sex discrimination lawsuit against a famous univer-
sity. A woman math professor was denied tenure, allegedly because she was a
woman. She argued that in every one of the university‚Äôs 22 departments, the per-
centage of men candidates granted tenure was greater than the percentage of women
candidates granted tenure. This sounds very suspicious!
However, the university‚Äôs lawyers argued that across the university as a whole,
the percentage of male candidates granted tenure was actually lower than the per-
centage for women candidates. This suggests that if there was any sex discrimi-
nation, then it was against men! Surely, at least one party in the dispute must be
lying.
Let‚Äôs clarify the problem by expressing both arguments in terms of conditional
probabilities. To simplify matters, suppose that there are only two departments, EE
and CS, and consider the experiment where we pick a random candidate. DeÔ¨Åne
the following events:
 AWWD the candidate is granted tenure,
 FEEWWD the candidate is a woman in the EE department,
 FCSWWD the candidate is a woman in the CS department,
 MEEWWD the candidate is a man in the EE department,
 MCSWWD the candidate is a man in the CS department.
Assume that all candidates are either men or women, and that no candidate be-
longs to both departments. That is, the events FEE, FCS, MEE, and MCS are all
disjoint.
In these terms, the plaintiff is making the following argument:
Pr

A j FEE

< Pr

A j MEE

and
Pr

A j FCS

< Pr

A j MCS

:

16.5. Conditional Probability
625
CS
0 women granted tenure, 1 candidates
0%
50 men granted tenure, 100 candidates
50%
EE
70 women granted tenure, 100 candidates
70%
1 man granted tenure, 1 candidates
100%
Overall
70 women granted tenure, 101 candidates
 70%
51 men granted tenure, 101 candidates
 51%
Table 16.1
A scenario where women are less likely to be granted tenure than men
in each department, but more likely to be granted tenure overall.
That is, in both departments, the probability that a woman candidate is granted
tenure is less than the probability for a man.
The university retorts that overall, a woman candidate is more likely to be granted
tenure than a man; namely that
Pr

A j FEE [ FCS

> Pr

A j MEE [ MCS

:
It is easy to believe that these two positions are contradictory, and the phe-
nomenon illustrated here is widely referred to as ‚ÄúSimpson‚Äôs Paradox.‚Äù But there is
no contradiction or paradox, and in fact, Table 16.1 shows a set of candidate statis-
tics for which the assertions of both the plaintiff and the university hold. In this
case, a higher percentage of men candidates were granted tenure in each depart-
ment, but overall a higher percentage of women candidates were granted tenure!
How do we make sense of this?
With data like this showing that at the department level, women candidates were
less likely to be granted tenure than men, university administrators would likely
see an indication of bias against women, and the departments would be directed to
reexamine their tenure procedures.
But suppose we replaced ‚Äúthe candidate is a man/woman in the EE department,‚Äù
by ‚Äúthe candidate is a man/woman for whom a tenure decision was made during an
odd-numbered day of the month,‚Äù and likewise with CS and an even-numbered day
of the month. Since we don‚Äôt think the parity of a date is a cause for the outcome
of a tenure decision, we would ignore the ‚Äúcoincidence‚Äù that on both odd and even
dates, men are more frequently granted tenure. Instead, we would judge, based on
the overall data showing women more likely to be granted tenure, that gender bias
against women was not an issue in the university.
The point is that it‚Äôs the same data that we interpret differently based on our
implicit causal beliefs. It would be circular to claim that the gender correlation
observed in the data corroborates our belief that there is discrimination, since our
interpretation of the data correlation depends on our beliefs about the causes of

Chapter 16
Events and Probability Spaces
626
tenure decisions.6 This illustrates a basic principle in statistics which people con-
stantly ignore: never assume that correlation implies causation.
16.6
Independence
Suppose that we Ô¨Çip two fair coins simultaneously on opposite sides of a room.
Intuitively, the way one coin lands does not affect the way the other coin lands.
The mathematical concept that captures this intuition is called independence.
DeÔ¨Ånition 16.6.1. An event with probability 0 is deÔ¨Åned to be independent of every
event (including itself). If Pr≈íB¬ç ¬§ 0, then event A is independent of event B iff
Pr

A j B

D Pr≈íA¬ç:
(16.7)
In other words, A and B are independent if knowing that B happens does not al-
ter the probability that A happens, as is the case with Ô¨Çipping two coins on opposite
sides of a room.
Potential Pitfall
Students sometimes get the idea that disjoint events are independent. The opposite
is true: if A \ B D ;, then knowing that A happens means you know that B
does not happen. So disjoint events are never independent‚Äîunless one of them has
probability zero.
16.6.1
Alternative Formulation
Sometimes it is useful to express independence in an alternate form which follows
immediately from DeÔ¨Ånition 16.6.1:
Theorem 16.6.2. A is independent of B if and only if
Pr≈íA \ B¬ç D Pr≈íA¬ç  Pr≈íB¬ç:
(16.8)
Notice that Theorem 16.6.2 makes apparent the symmetry between A being in-
dependent of B and B being independent of A:
Corollary 16.6.3. A is independent of B iff B is independent of A.
6These issues are thoughtfully examined in Causality: Models, Reasoning and Inference, Judea
Pearl, Cambridge U. Press, 2001.

16.6. Independence
627
16.6.2
Independence Is an Assumption
Generally, independence is something that you assume in modeling a phenomenon.
For example, consider the experiment of Ô¨Çipping two fair coins. Let A be the event
that the Ô¨Årst coin comes up heads, and let B be the event that the second coin is
heads. If we assume that A and B are independent, then the probability that both
coins come up heads is:
Pr≈íA \ B¬ç D Pr≈íA¬ç  Pr≈íB¬ç D 1
2  1
2 D 1
4:
In this example, the assumption of independence is reasonable. The result of one
coin toss should have negligible impact on the outcome of the other coin toss. And
if we were to repeat the experiment many times, we would be likely to have A \ B
about 1/4 of the time.
There are, of course, many examples of events where assuming independence is
not justiÔ¨Åed. For example, let C be the event that tomorrow is cloudy and R be the
event that tomorrow is rainy. Perhaps Pr≈íC¬ç D 1=5 and Pr≈íR¬ç D 1=10 in Boston.
If these events were independent, then we could conclude that the probability of a
rainy, cloudy day was quite small:
Pr≈íR \ C¬ç D Pr≈íR¬ç  Pr≈íC¬ç D 1
5  1
10 D 1
50:
Unfortunately, these events are deÔ¨Ånitely not independent; in particular, every rainy
day is cloudy. Thus, the probability of a rainy, cloudy day is actually 1=10.
Deciding when to assume that events are independent is a tricky business. In
practice, there are strong motivations to assume independence since many useful
formulas (such as equation (16.8)) only hold if the events are independent. But you
need to be careful: we‚Äôll describe several famous examples where (false) assump-
tions of independence led to trouble. This problem gets even trickier when there
are more than two events in play.
16.6.3
Mutual Independence
We have deÔ¨Åned what it means for two events to be independent. What if there are
more than two events? For example, how can we say that the Ô¨Çips of n coins are
all independent of one another? A set of events is said to be mutually independent
if the probability of each event in the set is the same no matter which of the other
events has occurred. We could formalize this with conditional probabilities as in
DeÔ¨Ånition 16.6.1, but we‚Äôll jump directly to the cleaner deÔ¨Ånition based on products
of probabilities as in Theorem 16.6.2:

Chapter 16
Events and Probability Spaces
628
DeÔ¨Ånition 16.6.4. A set of events E1; E2; : : : ; En is mutually independent iff for
all subsets S  ≈í1; n¬ç,
Pr
2
4 \
j2S
Ej
3
5 D
Y
j2S
Pr≈íEj ¬ç:
DeÔ¨Ånition 16.6.4 says that E1; E2; : : : ; En are mutually independent if and only
if all of the following equations hold for all distinct i, j, k, and l:
Pr≈íEi \ Ej ¬ç D Pr≈íEi¬ç  Pr≈íEj ¬ç
Pr≈íEi \ Ej \ Ek¬ç D Pr≈íEi¬ç  Pr≈íEj ¬ç  Pr≈íEk¬ç
Pr≈íEi \ Ej \ Ek \ El¬ç D Pr≈íEi¬ç  Pr≈íEj ¬ç  Pr≈íEk¬ç  Pr≈íEl¬ç
:::
Pr≈íE1 \    \ En¬ç D Pr≈íE1¬ç    Pr≈íEn¬ç:
For example, if we toss n fair coins, the tosses are mutually independent iff for
every subset of m coins, the probability that every coin in the subset comes up
heads is 2 m.
16.6.4
DNA Testing
Assumptions about independence are routinely made in practice. Frequently, such
assumptions are quite reasonable. Sometimes, however, the reasonableness of an
independence assumption is not so clear, and the consequences of a faulty assump-
tion can be severe.
For example, consider the following testimony from the O. J. Simpson murder
trial on May 15, 1995:
Mr. Clarke: When you make these estimations of frequency‚Äîand I believe you
touched a little bit on a concept called independence?
Dr. Cotton: Yes, I did.
Mr. Clarke: And what is that again?
Dr. Cotton: It means whether or not you inherit one allele that you have is not‚Äî
does not affect the second allele that you might get. That is, if you inherit
a band at 5,000 base pairs, that doesn‚Äôt mean you‚Äôll automatically or with
some probability inherit one at 6,000. What you inherit from one parent is
what you inherit from the other.

16.6. Independence
629
Mr. Clarke: Why is that important?
Dr. Cotton: Mathematically that‚Äôs important because if that were not the case, it
would be improper to multiply the frequencies between the different genetic
locations.
Mr. Clarke: How do you‚Äîwell, Ô¨Årst of all, are these markers independent that
you‚Äôve described in your testing in this case?
Presumably, this dialogue was as confusing to you as it was for the jury. Es-
sentially, the jury was told that genetic markers in blood found at the crime scene
matched Simpson‚Äôs. Furthermore, they were told that the probability that the mark-
ers would be found in a randomly-selected person was at most 1 in 170 million.
This astronomical Ô¨Ågure was derived from statistics such as:
 1 person in 100 has marker A.
 1 person in 50 marker B.
 1 person in 40 has marker C.
 1 person in 5 has marker D.
 1 person in 170 has marker E.
Then these numbers were multiplied to give the probability that a randomly-selected
person would have all Ô¨Åve markers:
Pr≈íA \ B \ C \ D \ E¬ç D Pr≈íA¬ç  Pr≈íB¬ç  Pr≈íC¬ç  Pr≈íD¬ç  Pr≈íE¬ç
D
1
100  1
50  1
40  1
5 
1
170 D
1
170;000;000:
The defense pointed out that this assumes that the markers appear mutually in-
dependently. Furthermore, all the statistics were based on just a few hundred blood
samples.
After the trial, the jury was widely mocked for failing to ‚Äúunderstand‚Äù the DNA
evidence. If you were a juror, would you accept the 1 in 170 million calculation?
16.6.5
Pairwise Independence
The deÔ¨Ånition of mutual independence seems awfully complicated ‚Äîthere are so
many subsets of events to consider! Here‚Äôs an example that illustrates the subtlety
of independence when more than two events are involved. Suppose that we Ô¨Çip
three fair, mutually-independent coins. DeÔ¨Åne the following events:

Chapter 16
Events and Probability Spaces
630
 A1 is the event that coin 1 matches coin 2.
 A2 is the event that coin 2 matches coin 3.
 A3 is the event that coin 3 matches coin 1.
Are A1, A2, A3 mutually independent?
The sample space for this experiment is:
fHHH; HHT; HTH; HT T; THH; THT; T TH; T T T g:
Every outcome has probability .1=2/3 D 1=8 by our assumption that the coins are
mutually independent.
To see if events A1, A2, and A3 are mutually independent, we must check a
sequence of equalities. It will be helpful Ô¨Årst to compute the probability of each
event Ai:
Pr≈íA1¬ç D Pr≈íHHH¬ç C Pr≈íHHT ¬ç C Pr≈íT TH¬ç C Pr≈íT T T ¬ç
D 1
8 C 1
8 C 1
8 C 1
8 D 1
2:
By symmetry, Pr≈íA2¬ç D Pr≈íA3¬ç D 1=2 as well. Now we can begin checking all the
equalities required for mutual independence in DeÔ¨Ånition 16.6.4:
Pr≈íA1 \ A2¬ç D Pr≈íHHH¬ç C Pr≈íT T T ¬ç D 1
8 C 1
8 D 1
4 D 1
2  1
2
D Pr≈íA1¬ç Pr≈íA2¬ç:
By symmetry, Pr≈íA1 \ A3¬ç D Pr≈íA1¬ç  Pr≈íA3¬ç and Pr≈íA2 \ A3¬ç D Pr≈íA2¬ç  Pr≈íA3¬ç
must hold also. Finally, we must check one last condition:
Pr≈íA1 \ A2 \ A3¬ç D Pr≈íHHH¬ç C Pr≈íT T T ¬ç D 1
8 C 1
8 D 1
4
¬§1
8 D Pr≈íA1¬ç Pr≈íA2¬ç Pr≈íA3¬ç:
The three events A1, A2, and A3 are not mutually independent even though any
two of them are independent! This not-quite mutual independence seems weird at
Ô¨Årst, but it happens. It even generalizes:
DeÔ¨Ånition 16.6.5. A set A1, A2, ..., of events is k-way independent iff every set
of k of these events is mutually independent. The set is pairwise independent iff it
is 2-way independent.

16.6. Independence
631
So the sets A1, A2, A3 above are pairwise independent, but not mutually inde-
pendent. Pairwise independence is a much weaker property than mutual indepen-
dence.
For example, suppose that the prosecutors in the O. J. Simpson trial were wrong
and markers A, B, C, D, and E appear only pairwise independently. Then the
probability that a randomly-selected person has all Ô¨Åve markers is no more than:
Pr≈íA \ B \ C \ D \ E¬ç  Pr≈íA \ E¬ç D Pr≈íA¬ç  Pr≈íE¬ç
D
1
100 
1
170 D
1
17;000:
The Ô¨Årst line uses the fact that A\B \C \D \E is a subset of A\E. (We picked
out the A and E markers because they‚Äôre the rarest.) We use pairwise independence
on the second line. Now the probability of a random match is 1 in 17,000 ‚Äîa far cry
from 1 in 170 million! And this is the strongest conclusion we can reach assuming
only pairwise independence.
On the other hand, the 1 in 17,000 bound that we get by assuming pairwise
independence is a lot better than the bound that we would have if there were no
independence at all. For example, if the markers are dependent, then it is possible
that
everyone with marker E has marker A,
everyone with marker A has marker B,
everyone with marker B has marker C, and
everyone with marker C has marker D.
In such a scenario, the probability of a match is
Pr≈íE¬ç D
1
170:
So a stronger independence assumption leads to a smaller bound on the prob-
ability of a match. The trick is to Ô¨Ågure out what independence assumption is
reasonable. Assuming that the markers are mutually independent may well not be
reasonable unless you have examined hundreds of millions of blood samples. Oth-
erwise, how would you know that marker D does not show up more frequently
whenever the other four markers are simultaneously present?
We will conclude our discussion of independence with a useful, and somewhat
famous, example known as the Birthday Principle.

Chapter 16
Events and Probability Spaces
632
16.6.6
The Birthday Principle
There are 95 students in a class. What is the probability that some birthday is
shared by two people? Comparing 95 students to the 365 possible birthdays, you
might guess the probability lies somewhere around 1=4 ‚Äîbut you‚Äôd be wrong: the
probability that there will be two people in the class with matching birthdays is
actually more than 0:9999.
To work this out, we‚Äôll assume that the probability that a randomly chosen stu-
dent has a given birthday is 1=d, where d D 365 in this case. We‚Äôll also assume
that a class is composed of n randomly and independently selected students, with
n D 95 in this case. These randomness assumptions are not really true, since
more babies are born at certain times of year, and students‚Äô class selections are
typically not independent of each other, but simplifying in this way gives us a start
on analyzing the problem. More importantly, these assumptions are justiÔ¨Åable in
important computer science applications of birthday matching. For example, the
birthday matching is a good model for collisions between items randomly inserted
into a hash table. So we won‚Äôt worry about things like Spring procreation prefer-
ences that make January birthdays more common, or about twins‚Äô preferences to
take classes together (or not).
Selecting a sequence of n students for a class yields a sequence of n birthdays.
Under the assumptions above, the d n possible birthday sequences are equally likely
outcomes. Let‚Äôs examine the consequences of this probability model by focussing
on the ith and j th elements in a birthday sequence, where 1  i ¬§ j  n. It
makes for a better story if we refer to the ith birthday as ‚ÄúAlice‚Äôs‚Äù and the jth as
‚ÄúBob‚Äôs.‚Äù
Now if Alice, Bob, Carol, and Don are four different people, then whether Alice
and Bob have matching birthdays is independent of whether Carol and Don do.
What‚Äôs more interesting is that whether Alice and Carol have the same birthday is
independent of whether Alice and Bob do. This follows because Carol is as likely
to have the same birthday as Alice, independently of whatever birthdays Alice and
Bob happen to have; a formal proof of this claim appears in Problem 17.2. In short,
the set of all events that a couple has matching birthdays is pairwise independent,
even for overlapping couples. This will be important in Chapter 18 because pair-
wise independence will be enough to justify some conclusions about the expected
number of matches. However, these matching birthday events are obviously not
even 3-way independent: if Alice and Bob match, and also Alice and Carol match,
then Bob and Carol will match.
It turns out that as long as the number of students is noticeably smaller than the
number of possible birthdays, we can get a pretty good estimate of the birthday
matching probabilities by pretending that the matching events are mutually inde-

16.6. Independence
633
pendent. (An intuitive justiÔ¨Åcation for this is that with only a small number of
matching pairs, it‚Äôs likely that none of the pairs overlap.) Then the probability of
no matching birthdays would be the same as the rth power of the probability that a
couple does not have matching birthdays, where r WWD
 n
2

is the number of couples.
That is, the probability of no matching birthdays would be
.1   1=d/.n
2/:
(16.9)
Using the fact that 1 C x < ex for all x,7 we would conclude that the probability
of no matching birthdays is at most
e .n
2/=d:
(16.10)
The matching birthday problem Ô¨Åts in here so far as a nice example illustrat-
ing pairwise and mutual independence, but it‚Äôs actually not hard to justify the
bound (16.10) without any pretence of independence. Namely, there are d.d  1/.d   2/    .d   .n   1// length n sequences of distinct birthdays. So the proba-
bility that everyone has a different birthday is:
d.d   1/.d   2/    .d   .n   1//
d n
D d
d  d   1
d
 d   2
d
   d   .n   1/
d
D

1   0
d
 
1   1
d
 
1   2
d

  

1   n   1
d

< e0  e 1=d  e 2=d    e .n 1/=d
(since 1 C x < ex)
D e Pn 1
iD1 i=d

D e .n.n 1/=2d/
D the bound (16.10):
For n D 85 and d D 365, the value of (16.10) is less than 1=17; 000, which
means the probability of having some pair of matching birthdays actually is more
than 1   1=17; 000 > 0:9999. So it would be pretty astonishing if there were no
pair of students in the class with matching birthdays.
For d  n2=2, the probability of no match turns out to be asymptotically equal
to the upper bound (16.10). For d D n2=2 in particular, the probability of no
match is asymptotically equal to 1=e. This leads to a rule of thumb which is useful
in many contexts in computer science:
7This approximation is obtained by truncating the Taylor series e x D 1 xCx2=2≈† x3=3≈†C   .
The approximation e x  1   x is pretty accurate when x is small.

Chapter 16
Events and Probability Spaces
634
The Birthday Principle
If there are d days in a year and
p
2d people in a room, then the probability
that two share a birthday is about 1   1=e  0:632.
For example, the Birthday Principle says that if you have
p
2  365  27 people
in a room, then the probability that two share a birthday is about 0:632. The actual
probability is about 0:626, so the approximation is quite good.
Among other applications, it implies that to use a hash function that maps n
items into a hash table of size d, you can expect many collisions unless n2 is a
small fraction of d. The Birthday Principle also famously comes into play as the
basis of ‚Äúbirthday attacks‚Äù that crack certain cryptographic systems.
Problems for Section 16.2
Practice Problems
Problem 16.1.
Let B be the number of heads that come up on 2n independent tosses of a fair coin.
(a) Pr≈íB D n¬ç is asymptotically equal to one of the expressions given below.
Explain which one.
1.
1
p
2n
2.
2
pn
3.
1
pn
4.
q
2
n
Problem 16.2.
Suppose you Ô¨Çip a fair coin 100 times. The coin Ô¨Çips are all mutually independent.
(a) What is the expected number of heads?
(b) What upper bound on the probability that the number of heads is at least 70
can we derive using Markov‚Äôs Theorem?
(c) What is the variance of the number of heads?
(d) What upper bound does Chebyshev‚Äôs Theorem give us on the probability that
the number of heads is either less than 30 or greater than 70?

16.6. Independence
635
Exam Problems
Problem 16.3. (a) What‚Äôs the probability that 0 doesn‚Äôt appear among k digits
chosen independently and uniformly at random?
(b) A box contains 90 good and 10 defective screws. What‚Äôs the probability that
if we pick 10 screws from the box, none will be defective?
(c) First one digit is chosen uniformly at random from f1; 2; 3; 4; 5g and is re-
moved from the set; then a second digit is chosen uniformly at random from the
remaining digits. What is the probability that an odd digit is picked the second
time?
(d) Suppose that you randomly permute the digits 1; 2;    ; n, that is, you select
a permutation uniformly at random. What is the probability the digit k ends up in
the ith position after the permutation?
(e) A fair coin is Ô¨Çipped n times. What‚Äôs the probability that all the heads occur
at the end of the sequence? (If no heads occur, then ‚Äúall the heads are at the end of
the sequence‚Äù is vacuously true.)
Class Problems
Problem 16.4.
In the alternate universe where the Red Sox don‚Äôt regularly collapse at the end of
their season, the New York Yankees and the Boston Red Sox are playing a two-out-
of-three series. (In other words, they play until one team has won two games. Then
that team is declared the overall winner and the series ends. Again, a fantasy.)
Assume that the Red Sox win each game with probability 3=5, regardless of the
outcomes of previous games.
Answer the questions below using the four step method. You can use the same
tree diagram for all three problems.
(a) What is the probability that a total of 3 games are played?
(b) What is the probability that the winner of the series loses the Ô¨Årst game?
(c) What is the probability that the correct team wins the series?
Problem 16.5.
To determine which of two people gets a prize, a coin is Ô¨Çipped twice. If the Ô¨Çips
are a Head and then a Tail, the Ô¨Årst player wins. If the Ô¨Çips are a Tail and then a

Chapter 16
Events and Probability Spaces
636
Head, the second player wins. However, if both coins land the same way, the Ô¨Çips
don‚Äôt count and whole the process starts over.
Assume that on each Ô¨Çip, a Head comes up with probability p, regardless of
what happened on other Ô¨Çips. Use the four step method to Ô¨Ånd a simple formula
for the probability that the Ô¨Årst player wins. What is the probability that neither
player wins?
Suggestions: The tree diagram and sample space are inÔ¨Ånite, so you‚Äôre not going
to Ô¨Ånish drawing the tree. Try drawing only enough to see a pattern. Summing
all the winning outcome probabilities directly is difÔ¨Åcult. However, a neat trick
solves this problem and many others. Let s be the sum of all winning outcome
probabilities in the whole tree. Notice that you can write the sum of all the winning
probabilities in certain subtrees as a function of s. Use this observation to write an
equation in s and then solve.
Problem 16.6.
Suppose you need a fair coin to decide which door to choose in the 6.042 Monty
Hall game. After making everyone in your group empty their pockets, all you
managed to turn up are some old collaboration statements, a few used tissues, and
one penny. However, the penny was from Prof. Meyer‚Äôs pocket, so it is not safe to
assume that it is a fair coin.
How can we use a coin of unknown bias to get the same effect as a fair coin of
bias 1=2? Draw the tree diagram for your solution, but since it is inÔ¨Ånite, draw only
enough to see a pattern.
Suggestion: A neat trick allows you to sum all the outcome probabilities that
cause you to say ‚ÄúHeads‚Äù: Let s be the sum of all ‚ÄúHeads‚Äù outcome probabilities
in the whole tree. Notice that you can write the sum of all the ‚ÄúHeads‚Äù outcome
probabilities in certain subtrees as a function of s. Use this observation to write an
equation in s and then solve.
Homework Problems
Problem 16.7.
Let‚Äôs see what happens when Let‚Äôs Make a Deal is played with four doors. A prize
is hidden behind one of the four doors. Then the contestant picks a door. Next, the
host opens an unpicked door that has no prize behind it. The contestant is allowed
to stick with their original door or to switch to one of the two unopened, unpicked
doors. The contestant wins if their Ô¨Ånal choice is the door hiding the prize.
Let‚Äôs make the same assumptions as in the original problem:
1. The prize is equally likely to be behind each door.

16.6. Independence
637
2. The contestant is equally likely to pick each door initially, regardless of the
prize‚Äôs location.
3. The host is equally likely to reveal each door that does not conceal the prize
and was not selected by the player.
Use The Four Step Method to Ô¨Ånd the following probabilities. The tree diagram
may become awkwardly large, in which case just draw enough of it to make its
structure clear.
(a) Contestant Stu, a sanitation engineer from Trenton, New Jersey, stays with his
original door. What is the probability that Stu wins the prize?
(b) Contestant Zelda, an alien abduction researcher from Helena, Montana, switches
to one of the remaining two doors with equal probability. What is the probability
that Zelda wins the prize?
Now let‚Äôs revise our assumptions about how contestants choose doors. Say the
doors are labeled A, B, C, and D. Suppose that Carol always opens the earliest door
possible (the door whose label is earliest in the alphabet) with the restriction that
she can neither reveal the prize nor open the door that the player picked.
This gives contestant Mergatroid ‚Äîan engineering student from Cambridge, MA
‚Äîjust a little more information about the location of the prize. Suppose that Mer-
gatroid always switches to the earliest door, excluding his initial pick and the one
Carol opened.
(c) What is the probability that Mergatroid wins the prize?
Problem 16.8.
We play a game with a deck of 52 regular playing cards, of which 26 are red and
26 are black. I randomly shufÔ¨Çe the cards and place the deck face down on a table.
You have the option of ‚Äútaking‚Äù or ‚Äúskipping‚Äù the top card. If you skip the top card,
then that card is revealed and we continue playing with the remaining deck. If you
take the top card, then the game ends; you win if the card you took was revealed
to be black, and you lose if it was red. If we get to a point where there is only one
card left in the deck, you must take it. Prove that you have no better strategy than
to take the top card ‚Äîwhich means your probability of winning is 1/2.
Hint: Prove by induction the more general claim that for a randomly shufÔ¨Çed
deck of n cards that are red or black ‚Äînot necessarily with the same number of red
cards and black cards ‚Äîthere is no better strategy than taking the top card.

Chapter 16
Events and Probability Spaces
638
Problems for Section 16.4
Class Problems
Problem 16.9.
Suppose there is a system, built by Caltech graduates, with n components. We
know from past experience that any particular component will fail in a given year
with probability p. That is, letting Fi be the event that the ith component fails
within one year, we have
Pr≈íFi¬ç D p
for 1  i  n. The system will fail if any one of its components fails. What can we
say about the probability that the system will fail within one year?
Let F be the event that the system fails within one year. Without any additional
assumptions, we can‚Äôt get an exact answer for Pr≈íF ¬ç. However, we can give useful
upper and lower bounds, namely,
p  Pr≈íF ¬ç  np:
(16.11)
We may as well assume p < 1=n, since the upper bound is trivial otherwise. For
example, if n D 100 and p D 10 5, we conclude that there is at most one chance
in 1000 of system failure within a year and at least one chance in 100,000.
Let‚Äôs model this situation with the sample space S WWD pow.≈í1; n¬ç/ whose out-
comes are subsets of positive integers  n, where s 2 S corresponds to the indices
of exactly those components that fail within one year. For example, f2; 5g is the
outcome that the second and Ô¨Åfth components failed within a year and none of the
other components failed. So the outcome that the system did not fail corresponds
to the empty set, ;.
(a) Show that the probability that the system fails could be as small as p by de-
scribing appropriate probabilities for the outcomes. Make sure to verify that the
sum of your outcome probabilities is 1.
(b) Show that the probability that the system fails could actually be as large as np
by describing appropriate probabilities for the outcomes. Make sure to verify that
the sum of your outcome probabilities is 1.
(c) Prove inequality (16.11).
Problem 16.10.
Here are some handy rules for reasoning about probabilities that all follow directly
from the Disjoint Sum Rule. Prove them.

16.6. Independence
639
Pr≈íA   B¬ç D Pr≈íA¬ç   Pr≈íA \ B¬ç
(Difference Rule)
Pr≈íA¬ç D 1   Pr≈íA¬ç
(Complement Rule)
Pr≈íA [ B¬ç D Pr≈íA¬ç C Pr≈íB¬ç   Pr≈íA \ B¬ç
(Inclusion-Exclusion)
Pr≈íA [ B¬ç  Pr≈íA¬ç C Pr≈íB¬ç
(2-event Union Bound)
If A  B; then Pr≈íA¬ç  Pr≈íB¬ç
(Monotonicity)
Homework Problems
Problem 16.11.
Prove the following probabilistic identity, referred to as the Union Bound. You
may assume the theorem that the probability of a union of disjoint sets is the sum
of their probabilities.
Let A1; : : : ; An be a collection of events. Then
Pr≈íA1 [ A2 [    [ An¬ç 
n
X
iD1
Pr≈íAi¬ç:
Hint: Induction.
Problem 16.12.
A round robin tournament of n contestants is one in which every two contestants
play each other exactly once and one of them wins. For a Ô¨Åxed integer k < n,
a question of interest is whether there is tournament for every k players, there is
another player who beats them all. This problem shows that if
 
n
k
! "
1  1
2
k#n k
< 1;
then such an outcome is possible.
(a) Start by numbering the sets of k contestants. How many such sets are there?
(b) Let Bi be the event that no contestant beat all the k contestants in set i. Com-
pute Pr≈íBi¬ç. (Note that you must choose probabilities for each match in order to
compute this).

Chapter 16
Events and Probability Spaces
640
(c) Give an upper bound on Pr≈íS Bi¬ç.
(d) Explain why this result can be used to prove the existence of the desired tour-
nament outcome.
Problems for Section 16.5
Practice Problems
Problem 16.13.
Dirty Harry places two bullets in the six-shell cylinder of his revolver. He gives the
cylinder a random spin and says ‚ÄúFeeling lucky?‚Äù as he holds the gun against your
heart.
(a) What is the probability that you will get shot if he pulls the trigger?
(b) Suppose he pulls the trigger and you don‚Äôt get shot. What is the probability
that you will get shot if he pulls the trigger a second time?
(c) Suppose you noticed that he placed the two shells next to each other in the
cylinder. How does this change the answers to the previous two questions?
Class Problems
Problem 16.14.
There are two decks of cards. One is complete, but the other is missing the Ace
of spades. Suppose you pick one of the two decks with equal probability and then
select a card from that deck uniformly at random. What is the probability that you
picked the complete deck, given that you selected the eight of hearts? Use the
four-step method and a tree diagram.
Problem 16.15.
Suppose you have three cards: A~, A, and a Jack. From these, you choose a
random hand (that is, each card is equally likely to be chosen) of two cards, and let
K be the number of Aces in your hand. You then randomly pick one of the cards
in the hand and reveal it.
(a) Describe a simple probability space (that is, outcomes and their probabilities)
for this scenario, and list the outcomes in each of the following events:
1. ≈íK  1¬ç, (that is, your hand has an Ace in it),
2. A~ is in your hand,
3. the revealed card is an A~,

16.6. Independence
641
4. the revealed card is an Ace.
(b) Then calculate Pr

K D 2 j E

for E equal to each of the four events in
part (a). Notice that most, but not all, of these probabilities are equal.
Now suppose you have a deck with d distinct cards, a different kinds of Aces
(including an A~), you draw a random hand with h cards, and then reveal a random
card from your hand.
(c) Prove that Pr≈íA~ is in your hand¬ç D h=d.
(d) Prove that
Pr

K D 2 j A~ is in your hand

D Pr≈íK D 2¬ç  2d
ah :
(16.12)
(e) Conclude that
Pr

K D 2 j the revealed card is an Ace

D Pr

K D 2 j A~ is in your hand

:
Problem 16.16.
There are three prisoners in a maximum-security prison for Ô¨Åctional villains: the
Evil Wizard Voldemort, the Dark Lord Sauron, and Little Bunny Foo-Foo. The
parole board has declared that it will release two of the three, chosen uniformly at
random, but has not yet released their names. Naturally, Sauron Ô¨Ågures that he will
be released to his home in Mordor, where the shadows lie, with probability 2=3.
A guard offers to tell Sauron the name of one of the other prisoners who will be
released (either Voldemort or Foo-Foo). If the guard has a choice of naming either
Voldemort or Foo-Foo (because both are to be released), he names one of the two
with equal probability.
Sauron knows the guard to be a truthful fellow. However, Sauron declines this
offer. He reasons that if the guard says, for example, ‚ÄúLittle Bunny Foo-Foo will
be released‚Äù, then his own probability of release will drop to 1=2. This is because
he will then know that either he or Voldemort will also be released, and these two
events are equally likely.
Dark Lord Sauron has made a typical mistake when reasoning about conditional
probability. Using a tree diagram and the four-step method, explain his mistake.
What is the probability that Sauron is released given that the guard says Foo-Foo is
released?

Chapter 16
Events and Probability Spaces
642
Hint: DeÔ¨Åne the events S, F , and ‚ÄúF ‚Äù as follows:
‚ÄúF ‚Äù D Guard says Foo-Foo is released
F D Foo-Foo is released
S D Sauron is released
Problem 16.17.
Every Skywalker serves either the light side or the dark side.
 The Ô¨Årst Skywalker serves the dark side.
 For n  2, the n-th Skywalker serves the same side as the .n   1/-st Sky-
walker with probability 1=4, and the opposite side with probability 3=4.
Let dn be the probability that the n-th Skywalker serves the dark side.
(a) Express dn with a recurrence equation and sufÔ¨Åcient base cases.
(b) Derive a simple expression for the generating function D.x/ WWD P1
1 dnxn.
(c) Give a simple closed formula for dn.
Problem 16.18. (a) For the directed acyclic graph (DAG) G0 in Figure 16.15, a
minimum-edge DAG with the same walk relation can be obtained by removing
some edges. List these edges (use notation hu!vi for an edge from u to v):
(b) List the vertices in a maximal chain in G0.
Let G be the simple graph shown in Figure 16.16.
A directed graph  !
G can be randomly constructed from G by assigning a direction
to each edge independently with equal likelihood.

16.6. Independence
643
Figure 16.15
The DAG G0
(c) What is the probability that  !
G D G0?
DeÔ¨Åne the following events with respect to the random graph  !
G:
T1 WWD vertices 2; 3; 4 are on a length-3 directed cycle;
T2 WWD vertices 1; 3; 4 are on a length-3 directed cycle;
T3 WWD vertices 1; 2; 4 are on a length-3 directed cycle;
T4 WWD vertices 1; 2; 3 are on a length-3 directed cycle:
(d) What is
Pr≈íT1¬ç‚Äπ
Pr≈íT1 \ T2¬ç‚Äπ
Pr≈íT1 \ T2 \ T3¬ç‚Äπ

Chapter 16
Events and Probability Spaces
644
Figure 16.16
Simple graph G
(e)  !
G has the property that if it has a directed cycle, then it has a length-3 directed
cycle. Use this fact to Ô¨Ånd the probability that  !
G is a DAG.
Homework Problems
Problem 16.19.
Outside of their hum-drum duties as Math for Computer Science Teaching Assis-
tants, Oscar is trying to learn to levitate using only intense concentration and Liz is
trying to become the world champion Ô¨Çaming torch juggler. Suppose that Oscar‚Äôs
probability of success is 1=6, Liz‚Äôs chance of success is 1=4, and these two events
are independent.
(a) If at least one of them succeeds, what is the probability that Oscar learns to
levitate?

16.6. Independence
645
(b) If at most one of them succeeds, what is the probability that Liz becomes the
world Ô¨Çaming torch juggler champion?
(c) If exactly one of them succeeds, what is the probability that it is Oscar?
Problem 16.20.
There is a course‚Äînot 6.042, naturally‚Äîin which 10% of the assigned problems
contain errors. If you ask a Teaching Assistant (TA) whether a problem has an
error, then they will answer correctly 80% of the time. This 80% accuracy holds
regardless of whether or not a problem has an error. Likewise when you ask a
lecturer, but with only 75% accuracy.
We formulate this as an experiment of choosing one problem randomly and ask-
ing a particular TA and Lecturer about it. DeÔ¨Åne the following events:
E WWD ‚Äúthe problem has an error,‚Äù
T WWD ‚Äúthe TA says the problem has an error,‚Äù
L WWD ‚Äúthe lecturer says the problem has an error.‚Äù
(a) Translate the description above into a precise set of equations involving con-
ditional probabilities among the events E, T , and L.
(b) Suppose you have doubts about a problem and ask a TA about it, and they tell
you that the problem is correct. To double-check, you ask a lecturer, who says that
the problem has an error. Assuming that the correctness of the lecturers‚Äô answer
and the TA‚Äôs answer are independent of each other, regardless of whether there is
an error8, what is the probability that there is an error in the problem?
(c) Is the event that ‚Äúthe TA says that there is an error‚Äù, independent of the event
that ‚Äúthe lecturer says that there is an error‚Äù?
Problem 16.21. (a) Suppose you repeatedly Ô¨Çip a fair coin until you see the se-
quence HHT or the sequence TTH. What is the probability you will see HHT Ô¨Årst?
Hint: Symmetry between Heads and Tails.
(b) What is the probability you see the sequence HTT before you see the sequence
HHT? Hint: Try to Ô¨Ånd the probability that HHT comes before HTT conditioning on
whether you Ô¨Årst toss an H or a T. The answer is not 1=2.
8This assumption is questionable: by and large, we would expect the lecturer and the TA‚Äôs to spot
the same glaring errors and to be fooled by the same subtle ones.

Chapter 16
Events and Probability Spaces
646
Problem 16.22.
A 52-card deck is thoroughly shufÔ¨Çed and you are dealt a hand of 13 cards.
(a) If you have one ace, what is the probability that you have a second ace?
(b) If you have the ace of spades, what is the probability that you have a second
ace? Remarkably, the answer is different from part (a).
Problem 16.23.
Suppose Pr≈í¬ç W S ! ≈í0; 1¬ç is a probability function on a sample space, S, and let B
be an event such that Pr≈íB¬ç > 0. DeÔ¨Åne a function PrB≈í¬ç on outcomes ! 2 S by
the rule:
PrB≈í!¬ç WWD
(
Pr≈í!¬ç= Pr≈íB¬ç
if ! 2 B;
0
if ! ‚Ä¶ B:
(16.13)
(a) Prove that PrB≈í¬ç is also a probability function on S according to DeÔ¨Åni-
tion 16.4.2.
(b) Prove that
PrB≈íA¬ç D Pr≈íA \ B¬ç
Pr≈íB¬ç
for all A  S.
(c) Explain why the Disjoint Sum Rule carries over for conditional probabilities,
namely,
Pr

C [ D j B

D Pr

C j B

C Pr

D j B

.C; D disjoint/:
Give examples of several further such rules.
Exam Problems
Problem 16.24.
Here‚Äôs a variation of Monty Hall‚Äôs game: the contestant still picks one of three
doors, with a prize randomly placed behind one door and goats behind the other
two. But now, instead of always opening a door to reveal a goat, Monty instructs
Carol to randomly open one of the two doors that the contestant hasn‚Äôt picked. This
means she may reveal a goat, or she may reveal the prize. If she reveals the prize,
then the entire game is restarted, that is, the prize is again randomly placed behind
some door, the contestant again picks a door, and so on until Carol Ô¨Ånally picks a
door with a goat behind it. Then the contestant can choose to stick with his original

16.6. Independence
647
choice of door or switch to the other unopened door. He wins if the prize is behind
the door he Ô¨Ånally chooses.
To analyze this setup, we deÔ¨Åne two events:
GP : The event that the contestant guesses the door with the prize behind it on his
Ô¨Årst guess.
OP : The event that the game is restarted at least once. Another way to describe
this is as the event that the door Carol Ô¨Årst opens has a prize behind it.
(a) What is Pr≈íGP ¬ç? ...Pr

OP
ÀáÀá GP

?
(b) What is Pr≈íOP ¬ç?
(c) Let R be the number of times the game is restarted before Carol picks a goat.
What is Ex≈íR¬ç? You may express the answer as a simple closed form in terms of
p WWD Pr≈íOP ¬ç.
(d) What is the probability the game will continue forever?
(e) When Carol Ô¨Ånally picks the goat, the contestant has the choice of sticking or
switching. Let‚Äôs say that the contestant adopts the strategy of sticking. Let W be
the event that the contestant wins with this strategy, and let w WWD Pr≈íW ¬ç. Express
the following conditional probabilities as simple closed forms in terms of w.
i) Pr

W j GP

D
ii) Pr

W
ÀáÀá GP \ OP

D
iii) Pr

W
ÀáÀá GP \ OP

D
(f) What is Pr≈íW ¬ç?
(g) For any Ô¨Ånal outcome where the contestant wins with a ‚Äústick‚Äù strategy, he
would lose if he had used a ‚Äúswitch‚Äù strategy, and vice versa. In the original Monty
Hall game, we concluded immediately that the probability that he would win with
a ‚Äúswitch‚Äù strategy was 1   Pr≈íW ¬ç. Why isn‚Äôt this conclusion quite as obvious for
this new, restartable game? Is this conclusion still sound? BrieÔ¨Çy explain.
Problem 16.25.
There are two decks of cards, the red deck and the blue deck. They differ slightly
in a way that makes drawing the eight of hearts slightly more likely from the red
deck than from the blue deck.

Chapter 16
Events and Probability Spaces
648
One of the decks is randomly chosen and hidden in a box. You reach in the
box and randomly pick a card that turns out to be the eight of hearts. You believe
intuitively that this makes the red deck more likely to be in the box than the blue
deck.
Your intuitive judgment about the red deck can be formalized and veriÔ¨Åed using
some inequalities between probabilities and conditional probabilities involving the
events
R WWD Red deck is in the box;
B WWD Blue deck is in the box;
E WWD Eight of hearts is picked from the deck in the box:
(a) State an inequality between probabilities and/or conditional probabilities that
formalizes the assertion, ‚Äúpicking the eight of hearts from the red deck is more
likely than from the blue deck.‚Äù
(b) State a similar inequality that formalizes the assertion ‚Äúpicking the eight of
hearts from the deck in the box makes the red deck more likely to be in the box
than the blue deck.‚Äù
(c) Assuming the each deck is equally likely to be the one in the box, prove that
the inequality of part (a) implies the inequality of part (b).
(d) Suppose you couldn‚Äôt be sure that the red deck and blue deck were equally
likely to be in the box. Could you still conclude that picking the eight of hearts
from the deck in the box makes the red deck more likely to be in the box than the
blue deck? BrieÔ¨Çy explain.
Problem 16.26.
A Ô¨Çip of Coin 1 is x times as likely to come up Heads as a Ô¨Çip of Coin 2. A biased
random choice of one of these coins made, where the probability of choosing Coin
1 is w times that of Coin 2. The chosen coin is Ô¨Çipped and comes up Heads.
(a) Restate the information above using probabilities and conditional probabilities

16.6. Independence
649
involving the events
C1 WWD Coin 1 was chosen;
C2 WWD Coin 2 was chosen;
H WWD the chosen coin came up Heads:
(b) State an inequality involving conditional probabilities of the above events that
formalizes the assertion ‚ÄúGiven that the chosen coin came up Heads, the chosen
coin is more likely to have been Coin 1 than Coin 2.‚Äù
(c) Prove that, given that the chosen coin came up Heads, the chosen coin is more
likely to have been Coin 1 than Coin 2 iff
wx > 1:
Problem 16.27.
There is a rare and serious disease called Beaver Fever which afÔ¨Çicts about 1 person
in 1000. Victims of this disease start telling math jokes in social settings, believing
other people will think they‚Äôre funny.
Doctor Meyer has some fairly reliable tests for this disease. In particular:
 If a person has Beaver Fever, the probability that Meyer diagnoses the person
as having the disease is 0.99.
 If a person doesn‚Äôt have it, the probability that Meyer diagnoses that person
as not having Beaver Fever is 0.97.
Let B be the event that a randomly chosen person has Beaver Fever, and Y be
the event that Meyer‚Äôs diagnosis is ‚ÄúYes, that person has Beaver Fever,‚Äù with B and
Y the complements of these events.
(a) The description above explicitly gives the values of the following quantities.
What are their values?
Pr≈íB¬ç
Pr

Y j B

Pr

Y
ÀáÀá B

(b) Write formulas for Pr≈íB¬ç and Pr

Y
ÀáÀá B

solely in terms of the explicitly given
expressions. Literally use the expressions, not their numeric values.

Chapter 16
Events and Probability Spaces
650
(c) Write a formula for the probability that Doctor Meyer says a person has the
disease solely in terms of Pr≈íB¬ç, Pr≈íB¬ç, Pr

Y j B

and Pr

Y
ÀáÀá B

.
(d) Write a formula solely in terms of the expressions given in part (a) for the
probability that a person has Beaver Fever given that Doctor Meyer says the person
has it.
Problem 16.28.
Suppose that Let‚Äôs Make a Deal is played according to slightly different rules and
with a red goat and a blue goat. There are three doors, with a prize hidden behind
one of them and the goats behind the others. No doors are opened until the con-
testant makes a Ô¨Ånal choice to stick or switch. The contestant is allowed to pick a
door and ask a certain question that the host then answers honestly. The contestant
may then stick with their chosen door, or switch to either of the other doors.
(a) If the contestant asks ‚Äúis there is a goat behind one of the unchosen doors?‚Äù
and the host answers ‚Äúyes,‚Äù is the contestant more likely to win the prize if they
stick, switch, or does it not matter? Clearly identify the probability space of out-
comes and their probabilities you use to model this situation. What is the contes-
tant‚Äôs probability of winning if he uses the best strategy?
(b) If the contestant asks ‚Äúis the red goat behind one of the unchosen doors?‚Äù and
the host answers ‚Äúyes,‚Äù is the contestant more likely to win the prize if they stick,
switch, or does it not matter? Clearly identify the probability space of outcomes
and their probabilities you use to model this situation. What is the contestant‚Äôs
probability of winning if he uses the best strategy?
Problem 16.29.
You are organizing a neighborhood census and instruct your census takers to knock
on doors and note the sex of any child that answers the knock. Assume that there
are two children in every household and that girls and boys are equally likely to be
children and equally likely to open the door.
A sample space for this experiment has outcomes that are triples whose Ô¨Årst
element is either B or G for the sex of the elder child, likewise for the second
element and the sex of the younger child, and whose third coordinate is E or Y
indicating whether the elder child or younger child opened the door. For example,

16.6. Independence
651
.B; G; Y/ is the outcome that the elder child is a boy, the younger child is a girl, and
the girl opened the door.
(a) Let T be the event that the household has two girls, and O be the event that a
girl opened the door. List the outcomes in T and O.
(b) What is the probability Pr

T j O

, that both children are girls, given that a
girl opened the door?
(c) What mistake is made in the following argument? (Note: merely stating the
correct probability is not an explanation of the mistake.)
If a girl opens the door, then we know that there is at least one girl in the
household. The probability that there is at least one girl is
1   Pr≈íboth children are boys¬ç D 1   .1=2  1=2/ D 3=4:
(16.14)
So,
Pr

T j there is at least one girl in the household

(16.15)
D Pr≈íT \ there is at least one girl in the household¬ç
Pr≈íthere is at least one girl in the household¬ç
(16.16)
D
Pr≈íT ¬ç
Pr≈íthere is at least one girl in the household¬ç
(16.17)
D .1=4/=.3=4/ D 1=3:
(16.18)
Therefore, given that a girl opened the door, the probability that there
are two girls in the household is 1/3.
Problem 16.30.
A guard is going to release exactly two of the three prisoners, Sauron, Voldemort,
and Bunny Foo Foo, and he‚Äôs equally likely to release any set of two prisoners.
(a) What is the probability that Voldemort will be released?
The guard will truthfully tell Voldemort the name of one of the prisoners to be
released. We‚Äôre interested in the following events:
V: Voldemort is released.
‚ÄúF‚Äù: The guard tells Voldemort that Foo Foo will be released.
‚ÄúS‚Äù: The guard tells Voldemort that Sauron will be released.

Chapter 16
Events and Probability Spaces
652
The guard has two rules for choosing whom he names:
 never say that Voldemort will be released,
 if both Foo Foo and Sauron are getting released, say ‚ÄúFoo Foo.‚Äù
(b) What is Pr

V j ‚ÄúF‚Äù

?
(c) What is Pr

V j ‚ÄúS‚Äù

?
(d) Show how to use the Law of Total Probability to combine your answers to
parts (b) and (c) to verify that the result matches the answer to part (a).
Problems for Section 16.6
Practice Problems
Problem 16.31.
Bruce Lee, on a movie that didn‚Äôt go public, is practicing by breaking 5 boards with
his Ô¨Åsts. He is able to break a board with probability 0.8 ‚Äîhe is practicing with his
left Ô¨Åst, that‚Äôs why it‚Äôs not 1 ‚Äîand he breaks each board independently.
(a) What is the probability that Bruce breaks exactly 2 out of the 5 boards that are
placed before him?
(b) What is the probability that Bruce breaks at most 3 out of the 5 boards that are
placed before him?
(c) What is the expected number of boards Bruce will break?
Problem 16.32.
Suppose 120 students take a Ô¨Ånal exam and the mean of their scores is 90. You have
no other information about the students and the exam, e.g. you should not assume
that the highest possible score is 100. You may, however, assume that exam scores
are nonnegative.
(a) State the best possible upper bound on the number of students who scored at
least 180.
(b) Now suppose somebody tells you that the lowest score on the exam is 30.
Compute the new best possible upper bound on the number of students who scored
at least 180.

16.6. Independence
653
Exam Problems
Problem 16.33.
Sally Smart just graduated from high school. She was accepted to three top col-
leges.
 With probability 4=12, she attends Yale.
 With probability 5=12, she attends MIT.
 With probability 3=12, she attends Little Hoop Community College.
Sally will either be happy or unhappy in college.
 If she attends Yale, she is happy with probability 4=12.
 If she attends MIT, she is happy with probability 7=12.
 If she attends Little Hoop, she is happy with probability 11=12.
(a) A tree diagram for Sally‚Äôs situation is shown below. On the diagram, Ô¨Åll in the
edge probabilities and at each leaf write the probabilty of that outcome.
Yale
MIT
Little Hoop
happy
unhappy
happy
happy
unhappy
unhappy
(b) What is the probability that Sally is happy in college?
(c) What is the probability that Sally Smart attends Yale, given that she is happy
in college?
(d) Show that the event that Sally attends Yale is not independent of the event that
she is happy.

Chapter 16
Events and Probability Spaces
654
(e) Show that the event that Sally Smart attends MIT is independent of the event
that she is happy.
Problem 16.34.
Construct a probability space S such that S contains three events A, B, and C with
the following properties:
 The three events satisfy the ‚Äúproduct rule.‚Äù That is,
Pr≈íA \ B \ C¬ç D Pr≈íA¬ç  Pr≈íB¬ç  Pr≈íC¬ç:
 The events are not mutually independent.
Hint: It may be helpful to draw a Venn diagram for S containing the three events,
and then incrementally Ô¨Åll in the probabilities of the disjoint regions.
Class Problems
Problem 16.35.
Let A; B; C be events. For each of the following statements, prove it or give a
counterexample.
(a) If A is independent of B, and A is independent of C, then A is independent of
B \ C.
(b) If A is independent of B, and A is independent of C, then A is independent of
B [ C.
(c) If A is independent of B, and A is independent of C, and A is independent of
B \ C, then A is independent of B [ C.
Problem 16.36.
Suppose that you Ô¨Çip three fair, mutually independent coins. DeÔ¨Åne the following
events:
 Let A be the event that the Ô¨Årst coin is heads.
 Let B be the event that the second coin is heads.
 Let C be the event that the third coin is heads.
 Let D be the event that an even number of coins are heads.

16.6. Independence
655
(a) Use the four step method to determine the probability space for this experiment
and the probability of each of A; B; C; D.
(b) Show that these events are not mutually independent.
(c) Show that they are 3-way independent.
Homework Problems
Problem 16.37.
DeÔ¨Åne the events A; FEE; FCS; MEE, and MCS as in Section 16.5.8.
In these terms, the plaintiff in a discrimination suit against a university makes the
argument that in both departments, the probability that a woman is granted tenure
is less than the probability for a man. That is,
Pr

A j FEE

< Pr

A j MEE

and
(16.19)
Pr

A j FCS

< Pr

A j MCS

:
(16.20)
The university‚Äôs defence attorneys retort that overall, a woman applicant is more
likely to be granted tenure than a man, namely, that
Pr

A j FEE [ FCS

> Pr

A j MEE [ MCS

:
(16.21)
The judge then interrupts the trial and calls the plaintiff and defence attorneys to
a conference in his ofÔ¨Åce to resolve what he thinks are contradictory statements of
facts about the tenure data. The judge points out that:
Pr

A j FEE [ FCS

D Pr

A j FEE

C Pr

A j FCS

(because FEE and FCS are disjoint)
< Pr

A j MEE

C Pr

A j MCS

(by (16.19) and (16.20))
D Pr

A j MEE [ MCS

(because FEE and FCS are disjoint)
so
Pr

A j FEE [ FCS

< Pr

A j MEE [ MCS

;
which directly contradicts the university‚Äôs position (16.21)!
But the judge is mistaken; an example where the plaintiff and defence assertions
are all true appears in Section 16.5.8. What is the mistake in the judge‚Äôs proof?
Problem 16.38.
Graphs, Logic & Probability

Chapter 16
Events and Probability Spaces
656
Let G be an undirected simple graph with n > 3 vertices. Let E.x; y/ mean that
G has an edge between vertices x and y, and let P.x; y/ mean that there is a length
2 path in G between x and y.
(a) Explain why E.x; y/ implies P.x; x/.
(b) Circle the mathematical formula that best expresses the deÔ¨Ånition of P.x; y/.
 P.x; y/ WWD 9z: E.x; z/ AND E.y; z/
 P.x; y/ WWD x ¬§ y AND 9z: E.x; z/ AND E.y; z/
 P.x; y/ WWD 8z: E.x; z/ OR E.y; z/
 P.x; y/ WWD 8z: x ¬§ y IMPLIES ≈íE.x; z/ OR E.y; z/¬ç
For the following parts (c)‚Äì(e), let V be a Ô¨Åxed set of n > 3 vertices, and let G be a
graph with these vertices constructed randomly as follows: for all distinct vertices
x; y 2 V , independently include edge hx‚Äîyi as an edge of G with probability p.
In particular, Pr≈íE.x; y/¬ç D p for all x ¬§ y.
(c) For distinct vertices w, x, y and z in V , circle the event pairs that are indepen-
dent.
1. E.w; x/ versus E.x; y/
2. ≈íE.w; x/ AND E.w; y/¬ç versus ≈íE.z; x/ AND E.z; y/¬ç
3. E.x; y/ versus P.x; y/
4. P.w; x/ versus P.x; y/
5. P.w; x/ versus P.y; z/
(d) Write a simple formula in terms of n and p for Pr≈íNOT P.x; y/¬ç, for distinct
vertices x and y in V .
Hint: Use part (c), item 2.
(e) What is the probability that two distinct vertices x and y lie on a three-
cycle in G? Answer with a simple expression in terms of p and r, where r WWD
Pr≈íNOT P.x; y/¬ç is the correct answer to part (d).

16.6. Independence
657
Hint: Express x and y being on a three-cycle as a simple formula involving E.x; y/
and P.x; y/.


17
Random Variables
Thus far, we have focused on probabilities of events. For example, we computed
the probability that you win the Monty Hall game or that you have a rare medical
condition given that you tested positive. But, in many cases we would like to know
more. For example, how many contestants must play the Monty Hall game until
one of them Ô¨Ånally wins? How long will this condition last? How much will I lose
gambling with strange dice all night? To answer such questions, we need to work
with random variables.
17.1
Random Variable Examples
DeÔ¨Ånition 17.1.1. A random variable R on a probability space is a total function
whose domain is the sample space.
The codomain of R can be anything, but will usually be a subset of the real
numbers. Notice that the name ‚Äúrandom variable‚Äù is a misnomer; random variables
are actually functions.
For example, suppose we toss three independent, unbiased coins. Let C be the
number of heads that appear. Let M D 1 if the three coins come up all heads or all
tails, and let M D 0 otherwise. Now every outcome of the three coin Ô¨Çips uniquely
determines the values of C and M. For example, if we Ô¨Çip heads, tails, heads, then
C D 2 and M D 0. If we Ô¨Çip tails, tails, tails, then C D 0 and M D 1. In effect,
C counts the number of heads, and M indicates whether all the coins match.
Since each outcome uniquely determines C and M, we can regard them as func-
tions mapping outcomes to numbers. For this experiment, the sample space is:
S D fHHH; HHT; HTH; HT T; THH; THT; T TH; T T T g:
Now C is a function that maps each outcome in the sample space to a number as
follows:
C.HHH/
D
3
C.THH/
D
2
C.HHT /
D
2
C.THT /
D
1
C.HTH/
D
2
C.T TH/
D
1
C.HT T /
D
1
C.T T T /
D
0:

Chapter 17
Random Variables
660
Similarly, M is a function mapping each outcome another way:
M.HHH/
D
1
M.THH/
D
0
M.HHT /
D
0
M.THT /
D
0
M.HTH/
D
0
M.T TH/
D
0
M.HT T /
D
0
M.T T T /
D
1:
So C and M are random variables.
17.1.1
Indicator Random Variables
An indicator random variable is a random variable that maps every outcome to
either 0 or 1. Indicator random variables are also called Bernoulli variables. The
random variable M is an example. If all three coins match, then M D 1; otherwise,
M D 0.
Indicator random variables are closely related to events. In particular, an in-
dicator random variable partitions the sample space into those outcomes mapped
to 1 and those outcomes mapped to 0. For example, the indicator M partitions the
sample space into two blocks as follows:
HHH
T T T
‚Äû
∆í‚Äö
‚Ä¶
M D 1
HHT
HTH
HT T
THH
THT
T TH
‚Äû
∆í‚Äö
‚Ä¶
M D 0
:
In the same way, an event E partitions the sample space into those outcomes
in E and those not in E. So E is naturally associated with an indicator random
variable, IE, where IE.!/ D 1 for outcomes ! 2 E and IE.!/ D 0 for outcomes
! ‚Ä¶ E. Thus, M D IE where E is the event that all three coins match.
17.1.2
Random Variables and Events
There is a strong relationship between events and more general random variables
as well. A random variable that takes on several values partitions the sample space
into several blocks. For example, C partitions the sample space as follows:
T T T
‚Äû∆í‚Äö‚Ä¶
C D 0
T TH
THT
HT T
‚Äû
∆í‚Äö
‚Ä¶
C D 1
THH
HTH
HHT
‚Äû
∆í‚Äö
‚Ä¶
C D 2
HHH
‚Äû∆í‚Äö‚Ä¶
C D 3
:
Each block is a subset of the sample space and is therefore an event. So the assertion
that C D 2 deÔ¨Ånes the event
≈íC D 2¬ç D fTHH; HTH; HHT g;
and this event has probability
Pr≈íC D 2¬ç D Pr≈íTHH¬ç C Pr≈íHTH¬ç C Pr≈íHHT ¬ç D 1
8 C 1
8 C 1
8 D 3=8:

17.2. Independence
661
Likewise ≈íM D 1¬ç is the event fT T T; HHHg and has probability 1=4.
More generally, any assertion about the values of random variables deÔ¨Ånes an
event. For example, the assertion that C  1 deÔ¨Ånes
≈íC  1¬ç D fT T T; T TH; THT; HT T g;
and so Pr≈íC  1¬ç D 1=2.
Another example is the assertion that C M is an odd number. If you think about
it for a minute, you‚Äôll realize that this is an obscure way of saying that all three
coins came up heads, namely,
≈íC  M is odd¬ç D fHHHg:
17.2
Independence
The notion of independence carries over from events to random variables as well.
Random variables R1 and R2 are independent iff for all x1; x2, the two events
≈íR1 D x1¬ç
and
≈íR2 D x2¬ç
are independent.
For example, are C and M independent? Intuitively, the answer should be ‚Äúno.‚Äù
The number of heads, C, completely determines whether all three coins match; that
is, whether M D 1. But, to verify this intuition, we must Ô¨Ånd some x1; x2 2 R
such that:
Pr≈íC D x1 AND M D x2¬ç ¬§ Pr≈íC D x1¬ç  Pr≈íM D x2¬ç:
One appropriate choice of values is x1 D 2 and x2 D 1. In this case, we have:
Pr≈íC D 2 AND M D 1¬ç D 0 ¬§ 1
4  3
8 D Pr≈íM D 1¬ç  Pr≈íC D 2¬ç:
The Ô¨Årst probability is zero because we never have exactly two heads (C D 2)
when all three coins match (M D 1). The other two probabilities were computed
earlier.
On the other hand, let H1 be the indicator variable for the event that the Ô¨Årst Ô¨Çip
is a Head, so
≈íH1 D 1¬ç D fHHH; HTH; HHT; HT T g:

Chapter 17
Random Variables
662
Then H1 is independent of M, since
Pr≈íM D 1¬ç D 1=4 D Pr

M D 1 j H1 D 1

D Pr

M D 1 j H1 D 0

Pr≈íM D 0¬ç D 3=4 D Pr

M D 0 j H1 D 1

D Pr

M D 0 j H1 D 0

This example is an instance of:
Lemma 17.2.1. Two events are independent iff their indicator variables are inde-
pendent.
The simple proof is left to Problem 17.1.
Intuitively, the independence of two random variables means that knowing some
information about one variable doesn‚Äôt provide any information about the other
one. We can formalize what ‚Äúsome information‚Äù about a variable R is by deÔ¨Åning
it to be the value of some quantity that depends on R. This intuitive property of
independence then simply means that functions of independent variables are also
independent:
Lemma 17.2.2. Let R and S be independent random variables, and f and g be
functions such that domain.f / D codomain.R/ and domain.g/ D codomain.S/.
Then f .R/ and g.S/ are independent random variables.
The proof is another simple exercise left to Problem 17.26.
As with events, the notion of independence generalizes to more than two random
variables.
DeÔ¨Ånition 17.2.3. Random variables R1; R2; : : : ; Rn are mutually independent iff
for all x1; x2; : : : ; xn, the n events
≈íR1 D x1¬ç; ≈íR2 D x2¬ç; : : : ; ≈íRn D xn¬ç
are mutually independent. They are k-way independent iff every subset of k of
them are mutually independent.
Lemmas 17.2.1 and 17.2.2 both extend straightforwardly to k-way independent
variables.
17.3
Distribution Functions
A random variable maps outcomes to values. The probability density function,
PDFR.x/, of a random variable, R, measures the probability that R takes the value

17.3. Distribution Functions
663
x, and the closely related cumulative distribution function, CDFR.x/, measures
the probability that R  x. Random variables that show up for different spaces
of outcomes often wind up behaving in much the same way because they have the
same probability of taking different values, that is, because they have same pdf/cdf.
DeÔ¨Ånition 17.3.1. Let R be a random variable with codomain V . The probability
density function of R is a function PDFR W V ! ≈í0; 1¬ç deÔ¨Åned by:
PDFR.x/ WWD
(
Pr≈íR D x¬ç
if x 2 range.R/;
0
if x ‚Ä¶ range.R/:
If the codomain is a subset of the real numbers, then the cumulative distribution
function is the function CDFR W R ! ≈í0; 1¬ç deÔ¨Åned by:
CDFR.x/ WWD Pr≈íR  x¬ç:
A consequence of this deÔ¨Ånition is that
X
x2range.R/
PDFR.x/ D 1:
This is because R has a value for each outcome, so summing the probabilities over
all outcomes is the same as summing over the probabilities of each value in the
range of R.
As an example, suppose that you roll two unbiased, independent, 6-sided dice.
Let T be the random variable that equals the sum of the two rolls. This random
variable takes on values in the set V D f2; 3; : : : ; 12g. A plot of the probability
density function for T is shown in Figure 17.1. The lump in the middle indicates
that sums close to 7 are the most likely. The total area of all the rectangles is 1
since the dice must take on exactly one of the sums in V D f2; 3; : : : ; 12g.
The cumulative distribution function for T is shown in Figure 17.2: The height
of the ith bar in the cumulative distribution function is equal to the sum of the
heights of the leftmost i bars in the probability density function. This follows from
the deÔ¨Ånitions of pdf and cdf:
CDFR.x/ D Pr≈íR  x¬ç D
X
yx
Pr≈íR D y¬ç D
X
yx
PDFR.y/:
It also follows from the deÔ¨Ånition that
lim
x!1 CDFR.x/ D 1 and
lim
x! 1 CDFR.x/ D 0:

Chapter 17
Random Variables
664
3=36
6=36
x 2 V
2
3
4
5
6
7
8
9
10 11 12
PDFT.x/
Figure 17.1
The probability density function for the sum of two 6-sided dice.
0
1=2
1
x 2 V
0
1
2
3
4
5
6
7
8
9
10 11 12
: : :
CDFT.x/
Figure 17.2
The cumulative distribution function for the sum of two 6-sided dice.

17.3. Distribution Functions
665
Both PDFR and CDFR capture the same information about R, so take your choice.
The key point here is that neither the probability density function nor the cumulative
distribution function involves the sample space of an experiment.
One of the really interesting things about density functions and distribution func-
tions is that many random variables turn out to have the same pdf and cdf. In other
words, even though R and S are different random variables on different probability
spaces, it is often the case that
PDFR D PDFS:
In fact, some pdf‚Äôs are so common that they are given special names. For exam-
ple, the three most important distributions in computer science are the Bernoulli
distribution, the uniform distribution, and the binomial distribution. We look more
closely at these common distributions in the next several sections.
17.3.1
Bernoulli Distributions
The Bernoulli distribution is the simplest and most common distribution func-
tion. That‚Äôs because it is the distribution function for an indicator random vari-
able. SpeciÔ¨Åcally, the Bernoulli distribution has a probability density function of
the form fp W f0; 1g ! ≈í0; 1¬ç where
fp.0/ D p;
and
fp.1/ D 1   p;
for some p 2 ≈í0; 1¬ç. The corresponding cumulative distribution function is Fp W
R ! ≈í0; 1¬ç where
Fp.x/ WWD
8
ÀÜ<
ÀÜ:
0
if x < 0
p
if 0  x < 1
1
if 1  x:
17.3.2
Uniform Distributions
A random variable that takes on each possible value in its codomain with the same
probability is said to be uniform. If the codomain V has n elements, then the
uniform distribution has a pdf of the form
f W V ! ≈í0; 1¬ç
where
f .v/ D 1
n

Chapter 17
Random Variables
666
for all v 2 V .
Uniform distributions come up all the time. For example, the number rolled on
a fair die is uniform on the set f1; 2; : : : ; 6g. An indicator variable is uniform when
its pdf is f1=2.
17.3.3
The Numbers Game
Enough deÔ¨Ånitions ‚Äîlet‚Äôs play a game! We have two envelopes. Each contains
an integer in the range 0; 1; : : : ; 100, and the numbers are distinct. To win the
game, you must determine which envelope contains the larger number. To give
you a Ô¨Åghting chance, we‚Äôll let you peek at the number in one envelope selected
at random. Can you devise a strategy that gives you a better than 50% chance of
winning?
For example, you could just pick an envelope at random and guess that it contains
the larger number. But this strategy wins only 50% of the time. Your challenge is
to do better.
So you might try to be more clever. Suppose you peek in one envelope and see
the number 12. Since 12 is a small number, you might guess that the number in the
other envelope is larger. But perhaps we‚Äôve been tricky and put small numbers in
both envelopes. Then your guess might not be so good!
An important point here is that the numbers in the envelopes may not be random.
We‚Äôre picking the numbers and we‚Äôre choosing them in a way that we think will
defeat your guessing strategy. We‚Äôll only use randomization to choose the numbers
if that serves our purpose: making you lose!
Intuition Behind the Winning Strategy
People are surprised when they Ô¨Årst learn that there is a strategy that wins more
than 50% of the time, regardless of what numbers we put in the envelopes.
Suppose that you somehow knew a number x that was in between the numbers
in the envelopes. Now you peek in one envelope and see a number. If it is bigger
than x, then you know you‚Äôre peeking at the higher number. If it is smaller than x,
then you‚Äôre peeking at the lower number. In other words, if you know a number x
between the numbers in the envelopes, then you are certain to win the game.
The only Ô¨Çaw with this brilliant strategy is that you do not know such an x. This
sounds like a dead end, but there‚Äôs a cool way to salvage things: try to guess x!
There is some probability that you guess correctly. In this case, you win 100%
of the time. On the other hand, if you guess incorrectly, then you‚Äôre no worse off
than before; your chance of winning is still 50%. Combining these two cases, your
overall chance of winning is better than 50%.
Many intuitive arguments about probability are wrong despite sounding persua-

17.3. Distribution Functions
667
sive. But this one goes the other way: it may not convince you, but it‚Äôs actually
correct. To justify this, we‚Äôll go over the argument in a more rigorous way ‚Äîand
while we‚Äôre at it, work out the optimal way to play.
Analysis of the Winning Strategy
For generality, suppose that we can choose numbers from the set f0; 1; : : : ; ng. Call
the lower number L and the higher number H.
Your goal is to guess a number x between L and H. To avoid confusing equality
cases, you select x at random from among the half-integers:
1
2; 11
2; 21
2; : : : ; n   1
2

But what probability distribution should you use?
The uniform distribution ‚Äîselecting each of these half-integers with equal prob-
ability ‚Äîturns out to be your best bet. An informal justiÔ¨Åcation is that if we Ô¨Ågured
out that you were unlikely to pick some number ‚Äîsay 501
2 ‚Äîthen we‚Äôd always put
50 and 51 in the envelopes. Then you‚Äôd be unlikely to pick an x between L and H
and would have less chance of winning.
After you‚Äôve selected the number x, you peek into an envelope and see some
number T . If T > x, then you guess that you‚Äôre looking at the larger number.
If T < x, then you guess that the other number is larger.
All that remains is to determine the probability that this strategy succeeds. We
can do this with the usual four step method and a tree diagram.
Step 1: Find the sample space.
You either choose x too low (< L), too high (> H), or just right (L < x < H).
Then you either peek at the lower number (T D L) or the higher number (T D H).
This gives a total of six possible outcomes, as show in Figure 17.3.
Step 2: DeÔ¨Åne events of interest.
The four outcomes in the event that you win are marked in the tree diagram.
Step 3: Assign outcome probabilities.
First, we assign edge probabilities. Your guess x is too low with probability L=n,
too high with probability .n   H/=n, and just right with probability .H   L/=n.
Next, you peek at either the lower or higher number with equal probability. Multi-
plying along root-to-leaf paths gives the outcome probabilities.

Chapter 17
Random Variables
668
choices 
of x
number
peeked at
TDH
TDL
TDH
TDL
TDH
TDL
1=2
1=2
1=2
1=2
1=2
1=2
L=n
.HL/=n
.nH/=n
result
lose
win
win
win
win
lose
probability
L=2n
L=2n
.HL/=2n
.HL/=2n
.nH/=2n
.nH/=2n
x too low
x too high
x just right
Figure 17.3
The tree diagram for the numbers game.
Step 4: Compute event probabilities.
The probability of the event that you win is the sum of the probabilities of the four
outcomes in that event:
Pr≈íwin¬ç D L
2n C H   L
2n
C H   L
2n
C n   H
2n
D 1
2 C H   L
2n
 1
2 C 1
2n
The Ô¨Ånal inequality relies on the fact that the higher number H is at least 1 greater
than the lower number L since they are required to be distinct.
Sure enough, you win with this strategy more than half the time, regardless of the
numbers in the envelopes! So with numbers chosen from the range 0; 1; : : : ; 100,
you win with probability at least 1=2 C 1=200 D 50:5%. If instead we agree to
stick to numbers 0; : : : ; 10, then your probability of winning rises to 55%. By Las
Vegas standards, those are great odds.
The best strategy to win the numbers game is an example of a randomized algo-
rithm ‚Äîit uses random numbers to inÔ¨Çuence decisions. Protocols and algorithms

17.3. Distribution Functions
669
f20.k/
0:18
0:16
0:14
0:12
0:10
0:08
0:06
0:04
0:02
0
k
10
15
20
5
0
Figure 17.4
The pdf for the unbiased binomial distribution for n D 20, f20.k/.
that make use of random numbers are very important in computer science. We‚Äôll
see a further example in section 18.7.5.
17.3.4
Binomial Distributions
The third commonly-used distribution in computer science is the binomial distri-
bution. The standard example of a random variable with a binomial distribution is
the number of heads that come up in n independent Ô¨Çips of a coin. If the coin is
fair, then the number of heads has an unbiased binomial distribution, speciÔ¨Åed by
the pdf fn W f0; 1; : : : ; ng ! ≈í0; 1¬ç:
fn.k/ WWD
 
n
k
!
2 n:
This is because there are
 n
k

sequences of n coin tosses with exactly k heads, and
each such sequence has probability 2 n.
A plot of f20.k/ is shown in Figure 17.4. The most likely outcome is k D 10
heads, and the probability falls off rapidly for larger and smaller values of k. The
falloff regions to the left and right of the main hump are called the tails of the
distribution.
In many Ô¨Åelds, including Computer Science, probability analyses come down to
getting small bounds on the tails of the binomial distribution. In the context of a

Chapter 17
Random Variables
670
problem, this typically means that there is very small probability that something
bad happens, which could be a server or communication link overloading or a ran-
domized algorithm running for an exceptionally long time or producing the wrong
result.
The tails do get small very fast. For example, the probability of Ô¨Çipping at most
25 heads in 100 tosses is less than 1 in 3,000,000. In fact, the tail of the distribution
falls off so rapidly that the probability of Ô¨Çipping exactly 25 heads is nearly twice
the probability of Ô¨Çipping exactly 24 heads plus the probability of Ô¨Çipping exactly
23 heads plus . . . the probability of Ô¨Çipping no heads.
The General Binomial Distribution
If the coins are biased so that each coin is heads with probability p, then the
number of heads has a general binomial density function speciÔ¨Åed by the pdf
fn;p W f0; 1; : : : ; ng ! ≈í0; 1¬ç where
fn;p.k/ D
 
n
k
!
pk.1   p/n k:
(17.1)
for some n 2 NC and p 2 ≈í0; 1¬ç. This is because there are
 n
k

sequences with
k heads and n   k tails, but now the probability of each such sequence is pk.1  p/n k.
For example, the plot in Figure 17.5 shows the probability density function
fn;p.k/ corresponding to Ô¨Çipping n D 20 independent coins that are heads with
probability p D 0:75. The graph shows that we are most likely to get k D 15
heads, as you might expect. Once again, the probability falls off quickly for larger
and smaller values of k.
17.4
Great Expectations
The expectation or expected value of a random variable is a single number that re-
veals a lot about the behavior of the variable. The expectation of a random variable
is also known as its mean or average. For example, the Ô¨Årst thing you typically
want to know when you see your grade on an exam is the average score of the
class. This average score turns out to be precisely the expectation of the random
variable equal to the score of a random student.
More precisely, the expectation of a random variable its ‚Äúaverage‚Äù value when
each value is weighted according to its probability. Formally, the expected value of
a random variable is deÔ¨Åned as follows:

17.4. Great Expectations
671
f20;:75.k/
0:25
0:2
0:15
0:1
0:05
0
k
10
15
20
5
0
Figure 17.5
The pdf for the general binomial distribution fn;p.k/ for n D 20
and p D :75.
DeÔ¨Ånition 17.4.1. If R is a random variable deÔ¨Åned on a sample space S, then the
expectation of R is
Ex≈íR¬ç WWD
X
!2S
R.!/ Pr≈í!¬ç:
(17.2)
Let‚Äôs work through some examples.
17.4.1
The Expected Value of a Uniform Random Variable
Rolling a 6-sided die provides an example of a uniform random variable. Let R be
the value that comes up when you roll a fair 6-sided die. Then by (17.2), the
expected value of R is
Ex≈íR¬ç D 1  1
6 C 2  1
6 C 3  1
6 C 4  1
6 C 5  1
6 C 6  1
6 D 7
2:
This calculation shows that the name ‚Äúexpected‚Äù value is a little misleading; the
random variable might never actually take on that value. No one expects to roll a
31
2 on an ordinary die!
In general, if Rn is a random variable with a uniform distribution on fa1; a2; : : : ; ang,

Chapter 17
Random Variables
672
then the expectation of Rn is simply the average of the ai‚Äôs:
Ex≈íRn¬ç D a1 C a2 C    C an
n
:
17.4.2
The Expected Value of a Reciprocal Random Variable
DeÔ¨Åne a random variable S to be the reciprocal of the value that comes up when
you roll a fair 6-sided die. That is, S D 1=R where R is the value that you roll.
Now,
Ex≈íS¬ç D Ex
 1
R

D 1
1  1
6 C 1
2  1
6 C 1
3  1
6 C 1
4  1
6 C 1
5  1
6 C 1
6  1
6 D 49
120:
Notice that
Ex

1=R

¬§ 1= Ex≈íR¬ç:
Assuming that these two quantities are equal is a common mistake.
17.4.3
The Expected Value of an Indicator Random Variable
The expected value of an indicator random variable for an event is just the proba-
bility of that event.
Lemma 17.4.2. If IA is the indicator random variable for event A, then
Ex≈íIA¬ç D Pr≈íA¬ç:
Proof.
Ex≈íIA¬ç D 1  Pr≈íIA D 1¬ç C 0  Pr≈íIA D 0¬ç D Pr≈íIA D 1¬ç
D Pr≈íA¬ç:
(def of IA)
For example, if A is the event that a coin with bias p comes up heads, then
Ex≈íIA¬ç D Pr≈íIA D 1¬ç D p.
17.4.4
Alternate DeÔ¨Ånition of Expectation
There is another standard way to deÔ¨Åne expectation.
Theorem 17.4.3. For any random variable R,
Ex≈íR¬ç D
X
x2range.R/
x  Pr≈íR D x¬ç:
(17.3)

17.4. Great Expectations
673
The proof of Theorem 17.4.3, like many of the elementary proofs about expec-
tation in this chapter, follows by judicious regrouping of terms in equation (17.2):
Proof. Suppose R is deÔ¨Åned on a sample space S. Then,
Ex≈íR¬ç WWD
X
!2S
R.!/ Pr≈í!¬ç
D
X
x2range.R/
X
!2≈íRDx¬ç
R.!/ Pr≈í!¬ç
D
X
x2range.R/
X
!2≈íRDx¬ç
x Pr≈í!¬ç
(def of the event ≈íR D x¬ç)
D
X
x2range.R/
x
0
@
X
!2≈íRDx¬ç
Pr≈í!¬ç
1
A
(factoring x from the inner sum)
D
X
x2range.R/
x  Pr≈íR D x¬ç:
(def of Pr≈íR D x¬ç)
The Ô¨Årst equality follows because the events ≈íR D x¬ç for x 2 range.R/ partition
the sample space S, so summing over the outcomes in ≈íR D x¬ç for x 2 range.R/
is the same as summing over S.

In general, equation (17.3) is more useful than the deÔ¨Åning equation (17.2) for
calculating expected values. It also has the advantage that it does not depend on
the sample space, but only on the density function of the random variable. On
the other hand, summing over all outcomes as in equation (17.2) sometimes yields
easier proofs about general properties of expectation.
17.4.5
Conditional Expectation
Just like event probabilities, expectations can be conditioned on some event. Given
a random variable R, the expected value of R conditioned on an event A is the
probability-weighted average value of R over outcomes in A. More formally:
DeÔ¨Ånition 17.4.4. The conditional expectation Ex≈íR j A¬ç of a random variable R
given event A is:
Ex≈íR j A¬ç WWD
X
r2range.R/
r  Pr

R D r j A

:
(17.4)
For example, we can compute the expected value of a roll of a fair die, given that
the number rolled is at least 4. We do this by letting R be the outcome of a roll of

Chapter 17
Random Variables
674
the die. Then by equation (17.4),
Ex≈íR j R  4¬ç D
6
X
iD1
iPr

R D i j R  4

D 10C20C30C41
3C51
3C61
3 D 5:
Conditional expectation is useful in dividing complicated expectation calcula-
tions into simpler cases. We can Ô¨Ånd a desired expectation by calculating the con-
ditional expectation in each simple case and averaging them, weighing each case
by its probability.
For example, suppose that 49.8% of the people in the world are male and the
rest female ‚Äîwhich is more or less true. Also suppose the expected height of a
randomly chosen male is 50 1100, while the expected height of a randomly chosen
female is 50 5:00 What is the expected height of a randomly chosen person? We can
calculate this by averaging the heights of men and women. Namely, let H be the
height (in feet) of a randomly chosen person, and let M be the event that the person
is male and F the event that the person is female. Then
Ex≈íH¬ç D Ex≈íH j M¬ç Pr≈íM¬ç C Ex≈íH j F ¬ç Pr≈íF ¬ç
D .5 C 11=12/  0:498 C .5 C 5=12/  0:502
D 5:665
which is a little less than 5‚Äô 8.‚Äù
This method is justiÔ¨Åed by:
Theorem 17.4.5 (Law of Total Expectation). Let R be a random variable on a
sample space S, and suppose that A1, A2, ..., is a partition of S. Then
Ex≈íR¬ç D
X
i
Ex≈íR j Ai¬ç Pr≈íAi¬ç:

17.4. Great Expectations
675
Proof.
Ex≈íR¬ç D
X
r2range.R/
r  Pr≈íR D r¬ç
(by 17.3)
D
X
r
r 
X
i
Pr

R D r j Ai

Pr≈íAi¬ç
(Law of Total Probability)
D
X
r
X
i
r  Pr

R D r j Ai

Pr≈íAi¬ç
(distribute constant r)
D
X
i
X
r
r  Pr

R D r j Ai

Pr≈íAi¬ç
(exchange order of summation)
D
X
i
Pr≈íAi¬ç
X
r
r  Pr

R D r j Ai

(factor constant Pr≈íAi¬ç)
D
X
i
Pr≈íAi¬ç Ex≈íR j Ai¬ç:
(Def 17.4.4 of cond. expectation)

17.4.6
Mean Time to Failure
A computer program crashes at the end of each hour of use with probability p, if
it has not crashed already. What is the expected time until the program crashes?
This will be easy to Ô¨Ågure out using the Law of Total Expectation, Theorem 17.4.5.
SpeciÔ¨Åcally, we want to Ô¨Ånd Ex≈íC¬ç where C is the number of hours until the Ô¨Årst
crash. We‚Äôll do this by conditioning on whether or not the crash occurs in the Ô¨Årst
hour.
So let A to be the event that the system fails on the Ô¨Årst step and A to be the
complementary event that the system does not fail on the Ô¨Årst step. Then the mean
time to failure Ex≈íC¬ç is
Ex≈íC¬ç D Ex≈íC j A¬ç Pr≈íA¬ç C Ex≈íC j A¬ç Pr≈íA¬ç:
(17.5)
Since A is the condition that the system crashes on the Ô¨Årst step, we know that
Ex≈íC j A¬ç D 1:
(17.6)
Since A is the condition that the system does not crash on the Ô¨Årst step, conditioning
on A is equivalent to taking a Ô¨Årst step without failure and then starting over without
conditioning. Hence,
Ex≈íC j A¬ç D 1 C Ex≈íC¬ç:
(17.7)

Chapter 17
Random Variables
676
Plugging (17.6) and (17.7) into (17.5):
Ex≈íC¬ç D 1  p C .1 C Ex≈íC¬ç/.1   p/
D p C 1   p C .1   p/ Ex≈íC¬ç
D 1 C .1   p/ Ex≈íC¬ç:
Then, rearranging terms gives
1 D Ex≈íC¬ç   .1   p/ Ex≈íC¬ç D p Ex≈íC¬ç;
and thus
Ex≈íC¬ç D 1=p:
The general principle here is well-worth remembering.
Mean Time to Failure
If a system independently fails at each time step with probability p, then the
expected number of steps up to the Ô¨Årst failure is 1=p.
So, for example, if there is a 1% chance that the program crashes at the end of
each hour, then the expected time until the program crashes is 1=0:01 D 100 hours.
As a further example, suppose a couple wants to have a baby girl. For simplicity
assume there is a 50% chance that each child they have is a girl, and the genders
of their children are mutually independent. If the couple insists on having children
until they get a girl, then how many baby boys should they expect Ô¨Årst?
This is really a variant of the previous problem. The question, ‚ÄúHow many hours
until the program crashes?‚Äù is mathematically the same as the question, ‚ÄúHow
many children must the couple have until they get a girl?‚Äù In this case, a crash
corresponds to having a girl, so we should set p D 1=2. By the preceding analysis,
the couple should expect a baby girl after having 1=p D 2 children. Since the last
of these will be the girl, they should expect just one boy.
Something to think about: If every couple follows the strategy of having children
until they get a girl, what will eventually happen to the fraction of girls born in this
world?
For the record, we‚Äôll state a formal version of this result. A random variable
like C that counts steps to Ô¨Årst failure is said to have a geometric distribution with
parameter p.

17.4. Great Expectations
677
DeÔ¨Ånition 17.4.6. A random variable, C, has a geometric distribution with param-
eter p iff codomain.C/ D ZC and
Pr≈íC D i¬ç D .1   p/i 1p:
Lemma 17.4.7. If a random variable C has a geometric distribution with param-
eter p, then
Ex≈íC¬ç D 1
p:
(17.8)
17.4.7
Expected Returns in Gambling Games
Some of the most interesting examples of expectation can be explained in terms of
gambling games. For straightforward games where you win w dollars with proba-
bility p and you lose x dollars with probability 1   p, it is easy to compute your
expected return or winnings. It is simply
pw   .1   p/x dollars:
For example, if you are Ô¨Çipping a fair coin and you win $1 for heads and you lose $1
for tails, then your expected winnings are
1
2  1  
1   1
2

 1 D 0:
In such cases, the game is said to be fair since your expected return is zero.
Now let‚Äôs look at another apparently fair game that turns out not to be so fair.
Splitting the Pot
After your last encounter with biker dude, one thing led to another and you have
dropped out of school and become a Hell‚Äôs Angel. It‚Äôs late on a Friday night and,
feeling nostalgic for the old days, you drop by your old hangout, where you en-
counter two of your former TAs, Eric and Nick. Eric and Nick propose that you
join them in a simple wager. Each player will put $2 on the bar and secretly write
‚Äúheads‚Äù or ‚Äútails‚Äù on their napkin. Then one player will Ô¨Çip a fair coin. The $6 on
the bar will then be ‚Äúsplit‚Äù ‚Äîthat is, be divided equally ‚Äîamong the players who
correctly predicted the outcome of the coin toss. Pot splitting like this is a familiar
feature in poker games, betting pools, and lotteries.
After your life-altering encounter with strange dice, you are more than a little
skeptical. So Eric and Nick agree to let you be the one to Ô¨Çip the coin. This
certainly seems fair. How can you lose?

Chapter 17
Random Variables
678
you guess
right?
Eric guesses
right?
no
yes
no
yes
1=2
1=2
1=2
1=2
1=2
1=2
yes
1=2
no
1=2
yes
1=2
no
1=2
yes
1=2
no
1=2
yes
1=2
no
1=2
yes
no
your
payoff
$0
$1
$1
$4
$2
$2
$2
$0
probability
1=8
1=8
1=8
1=8
1=8
1=8
1=8
1=8
Nick guesses
right?
Figure 17.6
The tree diagram for the game where three players each wager $2
and then guess the outcome of a fair coin toss. The winners split the pot.

17.4. Great Expectations
679
But you have learned your lesson and so before agreeing, you go through the
four-step method and write out the tree diagram to compute your expected return.
The tree diagram is shown in Figure 17.6.
The ‚Äúpayoff‚Äù values in Figure 17.6 are computed by dividing the $6 pot1 among
those players who guessed correctly and then subtracting the $2 that you put into
the pot at the beginning. For example, if all three players guessed correctly, then
your payoff is $0, since you just get back your $2 wager. If you and Nick guess
correctly and Eric guessed wrong, then your payoff is
6
2   2 D 1:
In the case that everyone is wrong, you all agree to split the pot and so, again, your
payoff is zero.
To compute your expected return, you use equation (17.3):
Ex≈ípayoff¬ç D 0  1
8 C 1  1
8 C 1  1
8 C 4  1
8
C . 2/  1
8 C . 2/  1
8 C . 2/  1
8 C 0  1
8
D 0:
This conÔ¨Årms that the game is fair. So, for old time‚Äôs sake, you break your solemn
vow to never ever engage in strange gambling games.
The Impact of Collusion
Needless to say, things are not turning out well for you. The more times you play
the game, the more money you seem to be losing. After 1000 wagers, you have
lost over $500. As Nick and Eric are consoling you on your ‚Äúbad luck,‚Äù you do a
back-of-the-envelope calculation and decide that the probability of losing $500 in
1000 fair $2 wagers is very very small.
Now it is possible of course that you are very very unlucky. But it is more likely
that something Ô¨Åshy is going on, and the tree diagram in Figure 17.6 is not a good
model of the game.
The ‚Äúsomething‚Äù that‚Äôs Ô¨Åshy turns out to be the possibility for Nick and Eric to
collude against you. To be sure, Nick and Eric can only guess the outcome of the
coin toss with probability 1=2, but what if Nick and Eric always guess differently?
In other words, what if Nick always guesses ‚Äútails‚Äù when Eric guesses ‚Äúheads,‚Äù
and vice-versa? This would result in a slightly different tree diagram, as shown in
Figure 17.7.
1The money invested in a wager is commonly referred to as the pot.

Chapter 17
Random Variables
680
you guess
right?
Eric guesses
right?
no
yes
no
yes
1=2
1=2
1=2
1=2
1=2
1=2
yes
0
no
1
yes
1
no
0
yes
0
no
1
yes
1
no
0
yes
no
your
payoff
$0
$1
$1
$4
$2
$2
$2
$0
probability
0
1=4
1=4
0
0
1=4
1=4
0
Nick guesses
right?
Figure 17.7
The revised tree diagram reÔ¨Çecting the scenario where Nick always
guesses the opposite of Eric.

17.4. Great Expectations
681
The payoffs for each outcome are the same in Figures 17.6 and 17.7, but the
probabilities of the outcomes are different. For example, it is no longer possible
for all three players to guess correctly, since Nick and Eric are always guessing
differently. More importantly, the outcome where your payoff is $4 is also no
longer possible. Since Nick and Eric are always guessing differently, one of them
will always get a share of the pot. As you might imagine, this is not good for you!
When we use equation (17.3) to compute your expected return in the collusion
scenario, we Ô¨Ånd that
Ex≈ípayoff¬ç D 0  0 C 1  1
4 C 1  1
4 C 4  0
C . 2/  0 C . 2/  1
4 C . 2/  1
4 C 0  0
D  1
2:
This is very bad indeed. By colluding, Nick and Eric have made it so that you
expect to lose $.50 every time you play. No wonder you lost $500 over the course
of 1000 wagers.
Maybe it would be a good idea to go back to school ‚Äîyour Hell‚Äôs Angels buds
may not be too happy that you just lost their $500.
How to Win the Lottery
Similar opportunities to ‚Äúcollude‚Äù arise in many betting games. For example, con-
sider the typical weekly football betting pool, where each participant wagers $10
and the participants that pick the most games correctly split a large pot. The pool
seems fair if you think of it as in Figure 17.6. But, in fact, if two or more players
collude by guessing differently, they can get an ‚Äúunfair‚Äù advantage at your expense!
In some cases, the collusion is inadvertent and you can proÔ¨Åt from it. For ex-
ample, many years ago, a former MIT Professor of Mathematics named Herman
Chernoff Ô¨Ågured out a way to make money by playing the state lottery. This was
surprising since state lotteries typically have very poor expected returns. That‚Äôs be-
cause the state usually takes a large share of the wagers before distributing the rest
of the pot among the winners. Hence, anyone who buys a lottery ticket is expected
to lose money. So how did Chernoff Ô¨Ånd a way to make money? It turned out to be
easy!
In a typical state lottery,
 all players pay $1 to play and select 4 numbers from 1 to 36,
 the state draws 4 numbers from 1 to 36 uniformly at random,

Chapter 17
Random Variables
682
 the states divides 1/2 of the money collected among the people who guessed
correctly and spends the other half redecorating the governor‚Äôs residence.
This is a lot like the game you played with Nick and Eric, except that there are
more players and more choices. Chernoff discovered that a small set of numbers
was selected by a large fraction of the population. Apparently many people think
the same way; they pick the same numbers not on purpose as in the previous game
with Nick and Eric, but based on Manny‚Äôs batting average or today‚Äôs date.
It was as if the players were colluding to lose! If any one of them guessed
correctly, then they‚Äôd have to split the pot with many other players. By selecting
numbers uniformly at random, Chernoff was unlikely to get one of these favored
sequences. So if he won, he‚Äôd likely get the whole pot! By analyzing actual state
lottery data, he determined that he could win an average of 7 cents on the dollar. In
other words, his expected return was not  $:50 as you might think, but C$:07.2
Inadvertent collusion often arises in betting pools and is a phenomenon that you
can take advantage of. For example, suppose you enter a Super Bowl betting pool
where the goal is to get closest to the total number of points scored in the game.
Also suppose that the average Super Bowl has a total of 30 point scored and that
everyone knows this. Then most people will guess around 30 points. Where should
you guess? Well, you should guess just outside of this range because you get to
cover a lot more ground and you don‚Äôt share the pot if you win. Of course, if you
are in a pool with math students and they all know this strategy, then maybe you
should guess 30 points after all.
17.5
Linearity of Expectation
Expected values obey a simple, very helpful rule called Linearity of Expectation.
Its simplest form says that the expected value of a sum of random variables is the
sum of the expected values of the variables.
Theorem 17.5.1. For any random variables R1 and R2,
Ex≈íR1 C R2¬ç D Ex≈íR1¬ç C Ex≈íR2¬ç:
Proof. Let T WWD R1 C R2. The proof follows straightforwardly by rearranging
2Most lotteries now offer randomized tickets to help smooth out the distribution of selected se-
quences.

17.5. Linearity of Expectation
683
terms in equation (17.2) in the deÔ¨Ånition of expectation:
Ex≈íT ¬ç WWD
X
!2S
T .!/  Pr≈í!¬ç
D
X
!2S
.R1.!/ C R2.!//  Pr≈í!¬ç
(def of T )
D
X
!2S
R1.!/ Pr≈í!¬ç C
X
!2S
R2.!/ Pr≈í!¬ç
(rearranging terms)
D Ex≈íR1¬ç C Ex≈íR2¬ç:
(by (17.2))

A small extension of this proof, which we leave to the reader, implies
Theorem 17.5.2. For random variables R1, R2 and constants a1; a2 2 R,
Ex≈ía1R1 C a2R2¬ç D a1 Ex≈íR1¬ç C a2 Ex≈íR2¬ç:
In other words, expectation is a linear function. A routine induction extends the
result to more than two variables:
Corollary 17.5.3 (Linearity of Expectation). For any random variables R1; : : : ; Rk
and constants a1; : : : ; ak 2 R,
Ex
2
4
k
X
iD1
aiRi
3
5 D
k
X
iD1
ai Ex≈íRi¬ç:
The great thing about linearity of expectation is that no independence is required.
This is really useful, because dealing with independence is a pain, and we often
need to work with random variables that are not known to be independent.
As an example, let‚Äôs compute the expected value of the sum of two fair dice.
17.5.1
Expected Value of Two Dice
What is the expected value of the sum of two fair dice?
Let the random variable R1 be the number on the Ô¨Årst die, and let R2 be the
number on the second die. We observed earlier that the expected value of one die
is 3.5. We can Ô¨Ånd the expected value of the sum using linearity of expectation:
Ex≈íR1 C R2¬ç D Ex≈íR1¬ç C Ex≈íR2¬ç D 3:5 C 3:5 D 7:
Notice that we did not have to assume that the two dice were independent. The
expected sum of two dice is 7, even if they are glued together (provided each indi-
vidual die remains fair after the gluing). Proving that this expected sum is 7 with a
tree diagram would be a bother: there are 36 cases. And if we did not assume that
the dice were independent, the job would be really tough!

Chapter 17
Random Variables
684
17.5.2
Sums of Indicator Random Variables
Linearity of expectation is especially useful when you have a sum of indicator ran-
dom variables. As an example, suppose there is a dinner party where n men check
their hats. The hats are mixed up during dinner, so that afterward each man receives
a random hat. In particular, each man gets his own hat with probability 1=n. What
is the expected number of men who get their own hat?
Letting G be the number of men that get their own hat, we want to Ô¨Ånd the
expectation of G. But all we know about G is that the probability that a man gets
his own hat back is 1=n. There are many different probability distributions of hat
permutations with this property, so we don‚Äôt know enough about the distribution
of G to calculate its expectation directly. But linearity of expectation makes the
problem really easy.
The trick3 is to express G as a sum of indicator variables. In particular, let Gi be
an indicator for the event that the ith man gets his own hat. That is, Gi D 1 if the
ith man gets his own hat, and Gi D 0 otherwise. The number of men that get their
own hat is then the sum of these indicator random variables:
G D G1 C G2 C    C Gn:
(17.9)
These indicator variables are not mutually independent. For example, if n   1 men
all get their own hats, then the last man is certain to receive his own hat. But, since
we plan to use linearity of expectation, we don‚Äôt have worry about independence!
Since Gi is an indicator random variable, we know from Lemma 17.4.2 that
Ex≈íGi¬ç D Pr≈íGi D 1¬ç D 1=n:
(17.10)
By Linearity of Expectation and equation (17.9), this means that
Ex≈íG¬ç D Ex≈íG1 C G2 C    C Gn¬ç
D Ex≈íG1¬ç C Ex≈íG2¬ç C    C Ex≈íGn¬ç
D
n
‚Äö
‚Ä¶‚Äû
∆í
1
n C 1
n C    C 1
n
D 1:
So even though we don‚Äôt know much about how hats are scrambled, we‚Äôve Ô¨Ågured
out that on average, just one man gets his own hat back!
More generally, Linearity of Expectation provides a very good method for com-
puting the expected number of events that will happen.
3We are going to use this trick a lot so it is important to understand it.

17.5. Linearity of Expectation
685
Theorem 17.5.4. Given any collection of events A1; A2; : : : ; An, the expected
number of events that will occur is
n
X
iD1
Pr≈íAi¬ç:
For example, Ai could be the event that the ith man gets the right hat back. But
in general, it could be any subset of the sample space, and we are asking for the
expected number of events that will contain a random sample point.
Proof. DeÔ¨Åne Ri to be the indicator random variable for Ai, where Ri.!/ D 1 if
w 2 Ai and Ri.!/ D 0 if w ‚Ä¶ Ai. Let R D R1 C R2 C    C Rn. Then
Ex≈íR¬ç D
n
X
iD1
Ex≈íRi¬ç
(by Linearity of Expectation)
D
n
X
iD1
Pr≈íRi D 1¬ç
(by Lemma 17.4.2)
D
n
X
iD1
Pr≈íAi¬ç:
(def of indicator variable)
So whenever you are asked for the expected number of events that occur, all you
have to do is sum the probabilities that each event occurs. Independence is not
needed.
17.5.3
Expectation of a Binomial Distribution
Suppose that we independently Ô¨Çip n biased coins, each with probability p of com-
ing up heads. What is the expected number of heads?
Let J be the random variable denoting the number of heads. Then J has a
binomial distribution with parameters n, p, and
Pr≈íJ D k¬ç D
 
n
k
!
pk.1   p/n k:
Applying equation (17.3), this means that
Ex≈íJ ¬ç D
n
X
kD0
k Pr≈íJ D k¬ç D
n
X
kD0
k
 
n
k
!
pk.1   p/n k:
(17.11)

Chapter 17
Random Variables
686
This sum looks a tad nasty, but linearity of expectation leads to an easy derivation
of a simple closed form. We just express J as a sum of indicator random variables,
which is easy. Namely, let Ji be the indicator random variable for the ith coin
coming up heads, that is,
Ji WWD
(
1
if the ith coin is heads
0
if the ith coin is tails:
Then the number of heads is simply
J D J1 C J2 C    C Jn:
By Theorem 17.5.4,
Ex≈íJ ¬ç D
n
X
iD1
Pr≈íJi¬ç D pn:
(17.12)
That really was easy. If we Ô¨Çip n mutually independent coins, we expect to get
pn heads. Hence the expected value of a binomial distribution with parameters n
and p is simply pn.
But what if the coins are not mutually independent? It doesn‚Äôt matter ‚Äîthe
answer is still pn because Linearity of Expectation and Theorem 17.5.4 do not
assume any independence.
If you are not yet convinced that Linearity of Expectation and Theorem 17.5.4
are powerful tools, consider this: without even trying, we have used them to prove
a complicated looking identity, namely,
n
X
kD0
k
 
n
k
!
pk.1   p/n k D pn;
(17.13)
which follows by combining equations (17.11) and (17.12).4
The next section has an even more convincing illustration of the power of linear-
ity to solve a challenging problem.
4Equation (17.13) may look daunting initially, but it is, after all, pretty similar to the binomial
identity, and that connection leads to a simple derivation by algebra. Namely, starting with the bino-
mial identity
.x C y/n D
n
X
kD0
 
n
k
!
xkyn k:
we can differentiate with respect to x (as in Section 13.1.6) to get
n.x C y/n 1 D
n
X
kD0
k
 
n
k
!
xk 1yn k:

17.5. Linearity of Expectation
687
17.5.4
The Coupon Collector Problem
Every time we purchase a kid‚Äôs meal at Taco Bell, we are graciously presented with
a miniature ‚ÄúRacin‚Äô Rocket‚Äù car together with a launching device which enables us
to project our new vehicle across any tabletop or smooth Ô¨Çoor at high velocity.
Truly, our delight knows no bounds.
There are n different types of Racin‚Äô Rocket cars (blue, green, red, gray, etc.).
The type of car awarded to us each day by the kind woman at the Taco Bell reg-
ister appears to be selected uniformly and independently at random. What is the
expected number of kid‚Äôs meals that we must purchase in order to acquire at least
one of each type of Racin‚Äô Rocket car?
The same mathematical question shows up in many guises: for example, what
is the expected number of people you must poll in order to Ô¨Ånd at least one person
with each possible birthday? Here, instead of collecting Racin‚Äô Rocket cars, you‚Äôre
collecting birthdays. The general question is commonly called the coupon collector
problem after yet another interpretation.
A clever application of linearity of expectation leads to a simple solution to the
coupon collector problem. Suppose there are Ô¨Åve different types of Racin‚Äô Rocket
cars, and we receive this sequence:
blue
green
green
red
blue
orange
blue
orange
gray.
Let‚Äôs partition the sequence into 5 segments:
blue
‚Äû∆í‚Äö‚Ä¶
X0
green
‚Äû∆í‚Äö‚Ä¶
X1
green
red
‚Äû
∆í‚Äö
‚Ä¶
X2
blue
orange
‚Äû
∆í‚Äö
‚Ä¶
X3
blue
orange
gray
‚Äû
∆í‚Äö
‚Ä¶
X4
:
The rule is that a segment ends whenever we get a new kind of car. For example, the
middle segment ends when we get a red car for the Ô¨Årst time. In this way, we can
break the problem of collecting every type of car into stages. Then we can analyze
each stage individually and assemble the results using linearity of expectation.
Let‚Äôs return to the general case where we‚Äôre collecting n Racin‚Äô Rockets. Let
Xk be the length of the kth segment. The total number of kid‚Äôs meals we must
purchase to get all n Racin‚Äô Rockets is the sum of the lengths of all these segments:
T D X0 C X1 C    C Xn 1
Multiplying both sides by x gives
xn.x C y/n 1 D
n
X
kD0
k
 
n
k
!
xkyn k
(17.14)
Plugging p for x and 1   p for y in (17.14) then yields (17.13).

Chapter 17
Random Variables
688
Now let‚Äôs focus our attention on Xk, the length of the kth segment. At the
beginning of segment k, we have k different types of car, and the segment ends
when we acquire a new type. When we own k types, each kid‚Äôs meal contains a
type that we already have with probability k=n. Therefore, each meal contains a
new type of car with probability 1   k=n D .n   k/=n. Thus, the expected number
of meals until we get a new kind of car is n=.n   k/ by the Mean Time to Failure
rule. This means that
Ex≈íXk¬ç D
n
n   k :
Linearity of expectation, together with this observation, solves the coupon col-
lector problem:
Ex≈íT ¬ç D Ex≈íX0 C X1 C    C Xn 1¬ç
D Ex≈íX0¬ç C Ex≈íX1¬ç C    C Ex≈íXn 1¬ç
D
n
n   0 C
n
n   1 C    C n
3 C n
2 C n
1
D n
1
n C
1
n   1 C    C 1
3 C 1
2 C 1
1

D n
1
1 C 1
2 C 1
3 C    C
1
n   1 C 1
n

D nHn
(17.15)
 n ln n:
Wow! It‚Äôs those Harmonic Numbers again!
We can use equation (17.15) to answer some concrete questions. For example,
the expected number of die rolls required to see every number from 1 to 6 is:
6H6 D 14:7 : : : :
And the expected number of people you must poll to Ô¨Ånd at least one person with
each possible birthday is:
365H365 D 2364:6 : : : :
17.5.5
InÔ¨Ånite Sums
Linearity of expectation also works for an inÔ¨Ånite number of random variables
provided that the variables satisfy some stringent absolute convergence criteria.

17.5. Linearity of Expectation
689
Theorem 17.5.5 (Linearity of Expectation). Let R0, R1, ..., be random variables
such that
1
X
iD0
Ex≈í jRij ¬ç
converges. Then
Ex
" 1
X
iD0
Ri
#
D
1
X
iD0
Ex≈íRi¬ç:
Proof. Let T WWD P1
iD0 Ri.
We leave it to the reader to verify that, under the given convergence hypothesis,
all the sums in the following derivation are absolutely convergent, which justiÔ¨Åes
rearranging them as follows:
1
X
iD0
Ex≈íRi¬ç D
1
X
iD0
X
s2S
Ri.s/  Pr≈ís¬ç
(Def. 17.4.1)
D
X
s2S
1
X
iD0
Ri.s/  Pr≈ís¬ç
(exchanging order of summation)
D
X
s2S
" 1
X
iD0
Ri.s/
#
 Pr≈ís¬ç
(factoring out Pr≈ís¬ç)
D
X
s2S
T .s/  Pr≈ís¬ç
(Def. of T )
D Ex≈íT ¬ç
(Def. 17.4.1)
D Ex
" 1
X
iD0
Ri
#
:
(Def. of T ): 
17.5.6
Expectations of Products
While the expectation of a sum is the sum of the expectations, the same is usually
not true for products. For example, suppose that we roll a fair 6-sided die and
denote the outcome with the random variable R. Does Ex≈íR  R¬ç D Ex≈íR¬ç  Ex≈íR¬ç?
We know that Ex≈íR¬ç D 31
2 and thus Ex≈íR¬ç2 D 12 1
4. Let‚Äôs compute Ex≈íR2¬ç to
see if we get the same result.
Ex

R2
D
X
!2S
R2.!/ Pr≈íw¬ç D
6
X
iD1
i2  Pr≈íRi D i¬ç
D 12
6 C 22
6 C 32
6 C 42
6 C 52
6 C 62
6 D 15 1=6 ¬§ 12 1=4:

Chapter 17
Random Variables
690
That is,
Ex≈íR  R¬ç ¬§ Ex≈íR¬ç  Ex≈íR¬ç:
So the expectation of a product is not always equal to the product of the expecta-
tions.
There is a special case when such a relationship does hold however; namely,
when the random variables in the product are independent.
Theorem 17.5.6. For any two independent random variables R1, R2,
Ex≈íR1  R2¬ç D Ex≈íR1¬ç  Ex≈íR2¬ç:
The proof follows by judicious rearrangement of terms in the sum that deÔ¨Ånes
Ex≈íR1  R2¬ç. Details appear in Problem 17.22.
Theorem 17.5.6 extends routinely to a collection of mutually independent vari-
ables.
Corollary 17.5.7. [Expectation of Independent Product]
If random variables R1; R2; : : : ; Rk are mutually independent, then
Ex
2
4
k
Y
iD1
Ri
3
5 D
k
Y
iD1
Ex≈íRi¬ç:
Problems for Section 17.2
Practice Problems
Problem 17.1. (a) Prove that if A and B are independent events, then so are A and
B.
(b) Let IA and IB be the indicator variables for events A and B. Prove that IA
and IB are independent iff A and B are independent.
Hint: For any event, E, let E1 WWD E and E0 WWD E. So the event ≈íIE D a¬ç is the
same as Ea.
Homework Problems
Problem 17.2.
Let R, S, and T be random variables with the same codomain, V .
(a) Suppose R is uniform ‚Äîthat is,
Pr≈íR D b¬ç D
1
jV j;

17.5. Linearity of Expectation
691
for all b 2 V ‚Äîand R is independent of S. Originally this text had the following
argument:
The probability that R D S is the same as the probability that R takes
whatever value S happens to have, therefore
Pr≈íR D S¬ç D
1
jV j :
(17.16)
Are you convinced by this argument? Write out a careful proof of (17.16).
Hint: The event ≈íR D S¬ç is a disjoint union of events
≈íR D S¬ç D
[
b2V
≈íR D b AND S D b¬ç:
(b) Let S T be the random variable giving the values of S and T .5 Now suppose
R has a uniform distribution, and R is independent of S  T . How about this
argument?
The probability that R D S is the same as the probability that R equals
the Ô¨Årst coordinate of whatever value S  T happens to have, and this
probability remains equal to 1=jV j by independence.
Therefore the
event ≈íR D S¬ç is independent of ≈íS D T ¬ç.
Write out a careful proof that ≈íR D S¬ç is independent of ≈íS D T ¬ç.
(c) Let V D f1; 2; 3g and .R; S; T / takes the following triples of values with
equal probability,
.1; 1; 1/; .2; 1; 1/; .1; 2; 3/; .2; 2; 3/; .1; 3; 2/; .2; 3; 2/:
Verify that
1. R is independent of S  T ,
2. The event ≈íR D S¬ç is not independent of ≈íS D T ¬ç.
3. S and T have a uniform distribution,
5That is, S  T W S ! V  V where
.S  T /.!/ WWD .S.!/; T .!//
for every outcome ! 2 S.

Chapter 17
Random Variables
692
Problem 17.3.
Let R, S, and T be mutually independent random variables with the same codomain,
V . Problem 17.2 showed that if R is uniform ‚Äîthat is,
Pr≈íR D b¬ç D
1
jV j;
for all b 2 V , then
the events ≈íR D S¬ç and ≈íS D T ¬ç are independent.
This implies that these events are also independent if T is uniform, since R and
T are symmetric in this assertion. Prove converssely that if neither R nor T is
uniform, then these events are not independent.
Problems for Section 17.3
Practice Problems
Problem 17.4.
Suppose R, S, and T be mutually independent random variables on the same prob-
ability space with uniform distribution on the range ≈í1; 3¬ç.
Let M D maxfR; S; T g. Compute the values of the probability density function,
PDFM, of M.
Class Problems
Guess the Bigger Number Game
Team 1:
 Write different integers between 0 and 7 on two pieces of paper.
 Put the papers face down on a table.
Team 2:
 Turn over one paper and look at the number on it.
 Either stick with this number or switch to the unseen other number.
Team 2 wins if it chooses the larger number; else, Team 1 wins.

17.5. Linearity of Expectation
693
Problem 17.5.
The analysis in section 17.3.3 implies that Team 2 has a strategy that wins 4/7 of
the time no matter how Team 1 plays. Can Team 2 do better? The answer is ‚Äúno,‚Äù
because Team 1 has a strategy that guarantees that it wins at least 3/7 of the time,
no matter how Team 2 plays. Describe such a strategy for Team 1 and explain why
it works.
Problem 17.6.
Suppose you have a biased coin that has probability p of Ô¨Çipping heads. Let J be
the number of heads in n independent coin Ô¨Çips. So J has the general binomial
distribution:
PDFJ .k/ D
 
n
k
!
pkqn k
where q WWD 1   p.
(a) Show that
PDFJ .k   1/ < PDFJ .k/
for k < np C p;
PDFJ .k   1/ > PDFJ .k/
for k > np C p:
(b) Conclude that the maximum value of PDFJ is asymptotically equal to
1
p2npq :
Hint: For the asymptotic estimate, it‚Äôs ok to assume that np is an integer, so by
part (a), the maximum value is PDFJ .np/. Use Stirling‚Äôs formula (13.25).
Problem 17.7.
Let k be in the integer interval ≈í1; n¬ç, and let R1; R2; : : : ; Rm, be mutually inde-
pendent random variables with uniform distribution on ≈í1; n¬ç. Let M WWD maxfRi j
i 2 ≈í1; m¬ç g.
(a) Write a formula for PDFM.1/.
(b) Write a formula for PDFM.k/ in terms of Pr≈íM  j¬ç for suitable j ‚Äôs.

Chapter 17
Random Variables
694
(c) Write a formula for Pr≈íM  k¬ç.
Problem 17.8.
Let R and S be independent random variables on the same probability space with
the same Ô¨Ånite range, V . Suppose R is uniform ‚Äîthat is,
Pr≈íR D v¬ç D
1
jV j;
for all v 2 V . Then
the probability that R D S is the same as the probability that R takes
whatever value S happens to have and therefore
Pr≈íR D S¬ç D
1
jV j :
(17.17)
The argument above is actually OK, but may seem too informal to be completely
convincing. Give a careful proof of this claim.
Hint: Use Total Probability on Pr

R D S j S D v

.
Homework Problems
Problem 17.9.
A drunken sailor wanders along main street, which conveniently consists of the
points along the x axis with integral coordinates. In each step, the sailor moves
one unit left or right along the x axis. A particular path taken by the sailor can be
described by a sequence of ‚Äúleft‚Äù and ‚Äúright‚Äù steps. For example, hleft,left,righti
describes the walk that goes left twice then goes right.
We model this scenario with a random walk graph whose vertices are the integers
and with edges going in each direction between consecutive integers. All edges are
labelled 1=2.
The sailor begins his random walk at the origin. This is described by an initial
distribution which labels the origin with probability 1 and all other vertices with
probability 0. After one step, the sailor is equally likely to be at location 1 or  1,
so the distribution after one step gives label 1/2 to the vertices 1 and  1 and labels
all other vertices with probability 0.
(a) Give the distributions after the 2nd, 3rd, and 4th step by Ô¨Ålling in the table of
probabilities below, where omitted entries are 0. For each row, write all the nonzero
entries so they have the same denominator.

17.5. Linearity of Expectation
695
location
-4
-3
-2
-1
0
1
2
3
4
initially
1
after 1 step
1=2
0
1=2
after 2 steps
?
?
?
?
?
after 3 steps
?
?
?
?
?
?
?
after 4 steps
?
?
?
?
?
?
?
?
?
(b)
1. What is the Ô¨Ånal location of a t-step path that moves right exactly i times?
2. How many different paths are there that end at that location?
3. What is the probability that the sailor ends at this location?
(c) Let L be the random variable giving the sailor‚Äôs location after t steps, and let
BWWD.LCt/=2. Use the answer to part (b) to show that B has an unbiased binomial
density function.
(d) Again let L be the random variable giving the sailor‚Äôs location after t steps,
where t is even. Show that
Pr≈íjLj <
pt
2 ¬ç < 1
2 :
So there is a better than even chance that the sailor ends up at least pt=2 steps from
where he started.
Hint: Work in terms of B. Then you can use an estimate that bounds the binomial
distribution. Alternatively, observe that the origin is the most likely Ô¨Ånal location
and then use the asymptotic estimate
Pr≈íL D 0¬ç D Pr≈íB D t=2¬ç 
r
2
t :
Problems for Section 17.4
Practice Problems
Problem 17.10.
A news article reporting on the departure of a school ofÔ¨Åcial from California to
Alabama dryly commented that this move would raise the average IQ in both states.
Explain.

Chapter 17
Random Variables
696
H
H
T
T
D
D
D
Figure 17.8
Sample space tree for coin toss until two consective heads.
Class Problems
Problem 17.11.
Let‚Äôs see what it takes to make Carnival Dice fair. Here‚Äôs the game with payoff
parameter k: make three independent rolls of a fair die. If you roll a six
 no times, then you lose 1 dollar.
 exactly once, then you win 1 dollar.
 exactly twice, then you win two dollars.
 all three times, then you win k dollars.
For what value of k is this game fair?
Problem 17.12. (a) Suppose we Ô¨Çip a fair coin and let NTT be the number of Ô¨Çips
until the Ô¨Årst time two Tails in a row appear. What is Ex≈íNTT¬ç?
Hint: Let D be the tree diagram for this process. Explain why D can be described
by the tree in Figure 17.8
Use the Law of Total Expectation 17.4.5.
(b) Suppose we Ô¨Çip a fair coin until a Tail immediately followed by a Head comes
up. What is the expectation of the number NTH of Ô¨Çips we perform?
(c) Suppose we now play a game: Ô¨Çip a fair coin until either TT or TH Ô¨Årst occurs.
You win if TT comes up Ô¨Årst, lose if TH comes up Ô¨Årst. Since TT takes 50% longer

17.5. Linearity of Expectation
697
on average to turn up, your opponent agrees that he has the advantage. So you tell
him you‚Äôre willing to play if you pay him $5 when he wins, but he merely pays you
a 20% premium, that is, $6, when you win.
If you do this, you‚Äôre sneakily taking advantage of your opponent‚Äôs untrained intu-
ition, since you‚Äôve gotten him to agree to unfair odds. What is your expected proÔ¨Åt
per game?
Problem 17.13.
A record of who beat whom in a round-robin tournament can be described with a
tournament digraph, where the vertices correspond to players and there is an edge
hx !yi iff x beat y in their game. A ranking of the players is a path that includes
all the players. A tournament digraph may in general have one or more rankings.6
Suppose we contruct a random tournament digraph by letting each of the players
in a match be equally likely to win and having results of all the matches be mutually
independent. Find a formula for the expected number of rankings in a random 10-
player tournament. Conclude that there is a 10-vertex tournament digraph with
more than 7000 rankings.
This problem is an instance of the probabilistic method. It uses probability to
prove the existence of an object without constructing it.
Exam Problems
Problem 17.14.
A coin with probability p of Ô¨Çipping Heads and probability q WWD 1   p of Ô¨Çipping
tails is repeatedly Ô¨Çipped until three consecutive Heads occur. The outcome tree,
D, for this setup is illustrated in Figure 17.9.
Let e.T / be the expected number of Ô¨Çips starting at the root of subtree T of D.
So we‚Äôre interested in Ô¨Ånding e.D/.
Write a small system of equations involving e.D/; e.B/, and e.C/ that could be
solved to Ô¨Ånd e.D/. You do not need to solve the equations.
Problem 17.15.
A coin with probability p of Ô¨Çipping Heads and probability q WWD 1   p of Ô¨Çipping
tails is repeatedly Ô¨Çipped until two consecutive Ô¨Çips match ‚Äîthat is, until HH or
TT occurs. The outcome tree, A, for this setup is illustrated in Figure 17.10.
6It has a unique ranking iff it is a DAG, see Problem 9.6.

Chapter 17
Random Variables
698
H 
T 
H 
T 
H 
T 
D 
D 
D 
D 
C 
B 
Figure 17.9
Outcome Tree for Flipping Until HHH
H 
T 
A 
C 
B 
C 
H 
T 
B 
T 
H 
Figure 17.10
Outcome Tree for Flipping Until HH or TT

17.5. Linearity of Expectation
699
Let e.T / be the expected number of Ô¨Çips starting at the root of subtree T of A.
So we‚Äôre interested in Ô¨Ånding e.A/.
Write a small system of equations involving e.A/; e.B/, and e.C/ that could be
solved to Ô¨Ånd e.A/. You do not need to solve the equations.
Homework Problems
Problem 17.16 (Deviations from the mean).
Let B be a random variable with unbiased binomial distribution, nemely,
Pr≈íB D k¬ç D
 
n
k
!
2 n :
Assume n is even. Prove the following formula for the expected absolute deviation
of B from its mean:
Ex≈íjB   Ex≈íB¬çj¬ç D
 
n
n
2
!
n
2nC1 :
Problems for Section 17.5
Practice Problems
Problem 17.17.
MIT students sometimes delay laundry for a few days. Assume all random values
described below are mutually independent.
(a) A busy student must complete 3 problem sets before doing laundry. Each
problem set requires 1 day with probability 2=3 and 2 days with probability 1=3.
Let B be the number of days a busy student delays laundry. What is Ex≈íB¬ç?
Example: If the Ô¨Årst problem set requires 1 day and the second and third problem
sets each require 2 days, then the student delays for B D 5 days.
(b) A relaxed student rolls a fair, 6-sided die in the morning. If he rolls a 1, then he
does his laundry immediately (with zero days of delay). Otherwise, he delays for
one day and repeats the experiment the following morning. Let R be the number
of days a relaxed student delays laundry. What is Ex≈íR¬ç?
Example: If the student rolls a 2 the Ô¨Årst morning, a 5 the second morning, and a 1
the third morning, then he delays for R D 2 days.
(c) Before doing laundry, an unlucky student must recover from illness for a num-
ber of days equal to the product of the numbers rolled on two fair, 6-sided dice.

Chapter 17
Random Variables
700
Let U be the expected number of days an unlucky student delays laundry. What is
Ex≈íU ¬ç?
Example: If the rolls are 5 and 3, then the student delays for U D 15 days.
(d) A student is busy with probability 1=2, relaxed with probability 1=3, and un-
lucky with probability 1=6. Let D be the number of days the student delays laundry.
What is Ex≈íD¬ç?
Problem 17.18.
Each Math for Computer Science Ô¨Ånal exam will be graded according to a rigorous
procedure:
 With probability 4
7 the exam is graded by a TA,with probability 2
7 it is graded
by a lecturer, and with probability 1
7, it is accidentally dropped behind the
radiator and arbitrarily given a score of 84.
 TAs score an exam by scoring each problem individually and then taking the
sum.
‚Äì There are ten true/false questions worth 2 points each. For each, full
credit is given with probability 3
4, and no credit is given with probability
1
4.
‚Äì There are four questions worth 15 points each. For each, the score is
determined by rolling two fair dice, summing the results, and adding 3.
‚Äì The single 20 point question is awarded either 12 or 18 points with
equal probability.
 Lecturers score an exam by rolling a fair die twice, multiplying the results,
and then adding a ‚Äúgeneral impression‚Äùscore.
‚Äì With probability 4
10, the general impression score is 40.
‚Äì With probability 3
10, the general impression score is 50.
‚Äì With probability 3
10, the general impression score is 60.
Assume all random choices during the grading process are independent.
(a) What is the expected score on an exam graded by a TA?
(b) What is the expected score on an exam graded by a lecturer?
(c) What is the expected score on a Math for Computer Science Ô¨Ånal exam?

17.5. Linearity of Expectation
701
Class Problems
Problem 17.19.
A classroom has sixteen desks in a 4  4 arrangement as shown below.
If there is a girl in front, behind, to the left, or to the right of a boy, then the two of
them Ô¨Çirt. One student may be in multiple Ô¨Çirting couples; for example, a student
in a corner of the classroom can Ô¨Çirt with up to two others, while a student in
the center can Ô¨Çirt with as many as four others. Suppose that desks are occupied
by boys and girls with equal probability and mutually independently. What is the
expected number of Ô¨Çirting couples? Hint: Linearity.
Problem 17.20.
Here are seven propositions:
x1
OR
x3
OR
x7
x5
OR
x6
OR
x7
x2
OR
x4
OR
x6
x4
OR
x5
OR
x7
x3
OR
x5
OR
x8
x9
OR
x8
OR
x2
x3
OR
x9
OR
x4
Note that:
1. Each proposition is the disjunction (OR) of three terms of the form xi or the
form xi.

Chapter 17
Random Variables
702
2. The variables in the three terms in each proposition are all different.
Suppose that we assign true/false values to the variables x1; : : : ; x9 indepen-
dently and with equal probability.
(a) What is the expected number of true propositions?
Hint: Let Ti be an indicator for the event that the i-th proposition is true.
(b) Use your answer to prove that for any set of 7 propositions satisfying the
conditions 1. and 2., there is an assignment to the variables that makes all 7 of the
propositions true.
Problem 17.21.
A literal is a propositional variable or its negation. A k-clause is an OR of k literals,
with no variable occurring more than once in the clause. For example,
P OR Q OR R OR V;
is a 4-clause, but
V OR Q OR X OR V;
is not, since V appears twice.
Let S be a set of n distinct k-clauses involving v variables. The variables in
different k-clauses may overlap or be completely different, so k  v  nk.
A random assignment of true/false values will be made independently to each of
the v variables, with true and false assignments equally likely. Write formulas in n,
k, and v in answer to the Ô¨Årst two parts below.
(a) What is the probability that the last k-clause in S is true under the random
assignment?
(b) What is the expected number of true k-clauses in S?
(c) A set of propositions is satisÔ¨Åable iff there is an assignment to the variables
that makes all of the propositions true. Use your answer to part (b) to prove that if
n < 2k, then S is satisÔ¨Åable.

17.5. Linearity of Expectation
703
Problem 17.22.
Justify each line of the following proof that if R1 and R2 are independent, then
Ex≈íR1  R2¬ç D Ex≈íR1¬ç  Ex≈íR2¬ç:
Proof.
Ex≈íR1  R2¬ç
D
X
r2range.R1R2/
r  Pr≈íR1  R2 D r¬ç
D
X
ri2range.Ri/
r1r2  Pr≈íR1 D r1 and R2 D r2¬ç
D
X
r12range.R1/
X
r22range.R2/
r1r2  Pr≈íR1 D r1 and R2 D r2¬ç
D
X
r12range.R1/
X
r22range.R2/
r1r2  Pr≈íR1 D r1¬ç  Pr≈íR2 D r2¬ç
D
X
r12range.R1/
0
@r1 Pr≈íR1 D r1¬ç 
X
r22range.R2/
r2 Pr≈íR2 D r2¬ç
1
A
D
X
r12range.R1/
r1 Pr≈íR1 D r1¬ç  Ex≈íR2¬ç
D Ex≈íR2¬ç 
X
r12range.R1/
r1 Pr≈íR1 D r1¬ç
D Ex≈íR2¬ç  Ex≈íR1¬ç:

Homework Problems
Problem 17.23.
A coin will be Ô¨Çipped repeatedly until the sequence tail/tail/head (TTH) comes
up. Successive Ô¨Çips are independent, and the coin has probability p of coming up
heads. Let NTTH be the number of coin tosses until TTH Ô¨Årst appears. What value
of p minimizes Ex≈íNTTH¬ç?
Problem 17.24.
(A true story from world war two).

Chapter 17
Random Variables
704
The army needs to test each of its soldiers for a disease. There is a blood test that
accurately determines when a blood sample contains blood from a diseased soldier.
Assume that p is the fraction of diseased soldiers and that there are n soldiers.
Approach 1. is to test blood from each soldier individually; this requires n tests.
Approach 2./ is to randomly group the soldiers into g groups of k soldiers, where
n D gk. Then blend the blood samples of each group, and apply the test once to
each of the g blended samples. If the group-blend is free of the disease, we are
done with that group after one test. If the group-blend fails the test, then someone
in the group has the disease, and we then test all k people for a total of k C 1 tests
on that group.
(a) What is the expected number of tests in Approach 2. as a function of the num-
ber of soldiers n, the disease fraction p, and the group size k? (Assume that the
probability that a soldier who is chosen to be in a group is diseased remains equal
to p, independently of which other soldiers are chosen to be in the group. This
approximation is justiÔ¨Åed if k is small relative to pn.)
(b) Assuming p is reasonably small, show how to choose k so that the expected
number of tests using Approach 2. is approximately npp.
(c) What fraction of the work does Approach 2. expect to save over Approach 1.
in a million-strong army with disease incidence 1%?
(d) Can you come up with a better scheme by using multiple levels of grouping,
that is, groups of groups?
Problem 17.25.
A wheel-of-fortune has the numbers from 1 to 2n arranged in a circle. The wheel
has a spinner, and a spin randomly determines the two numbers at the opposite ends
of the spinner. How would you arrange the numbers on the wheel to maximize the
expected value of:
(a) the sum of the numbers chosen?
(b) the product of the numbers chosen?
Problem 17.26.
Let R and S be independent random variables, and f and g be any functions such
that domain.f / D codomain.R/ and domain.g/ D codomain.S/. Prove that f .R/
and g.S/ are independent random variables. Hint: The event ≈íf .R/ D a¬ç is the

17.5. Linearity of Expectation
705
disjoint union of all the events ≈íR D r¬ç for r such that f .r/ D a.
Problem 17.27.
Peeta bakes between 1 and 2n loaves of bread to sell every day. Each day he rolls
a fair, n-sided die to get a number from 1 to n, then Ô¨Çips a fair coin. If the coin is
heads, he bakes a number of loaves of bread equal to the value on the die, and if the
coin is tails, he bakes twice that many loaves.
(a) For any positive integer k  2n, what is the probability that Peeta will make
k loaves of bread on any given day? (You can express your solution by cases.)
(b) What is the expected number of loaves Peeta will bake on any given day?
(c) Continuing this process, Peeta bakes bread every day for 30 days. What is the
expected total number of loaves Peeta will have baked?
Exam Problems
Problem 17.28.
A box initially contains n balls, all colored black. A ball is drawn from the box at
random.
 If the drawn ball is black, then a biased coin with probability, p > 0, of
coming up heads is Ô¨Çipped. If the coin comes up heads, a white ball is put
into the box; otherwise the black ball is returned to the box.
 If the drawn ball is white, then it is returned to the box.
This process is repeated until the box contains n white balls.
Let D be the number of balls drawn until the process ends with the box full of
white balls. Prove that Ex≈íD¬ç D nHn=p, where Hn is the nth Harmonic number.
Hint: Let Di be the number of draws after the ith white ball until the draw when
the .i C 1/st white ball is put into the box.


18
Deviation from the Mean
18.1
Why the Mean?
In the previous chapter we took it for granted that expectation is important, and we
developed a bunch of techniques for calculating expected values. But why should
we care about this value? After all, a random variable may never take a value
anywhere near its expected value.
The most important reason to care about the mean value comes from its con-
nection to estimation by sampling. For example, suppose we want to estimate the
average age, income, family size, or other measure of a population. To do this,
we determine a random process for selecting people ‚Äîsay throwing darts at census
lists. This process makes the selected person‚Äôs age, income, and so on into a random
variable whose mean equals the actual average age or income of the population. So
we can select a random sample of people and calculate the average of people in the
sample to estimate the true average in the whole population. But when we make an
estimate by repeated sampling, we need to know how much conÔ¨Ådence we should
have that our estimate is OK or how large a sample is needed to reach a given con-
Ô¨Ådence level. The issue is also fundamental in all experimental science. Because of
random errors ‚Äînoise ‚Äîrepeated measurements of the same quantity rarely come
out exactly the same. Determining how much conÔ¨Ådence to put in experimental
measurements is a fundamental and universal scientiÔ¨Åc issue. Technically, judg-
ing sampling or measurement accuracy reduces to Ô¨Ånding the probability that an
estimate deviates by a given amount from its expected value.
Another aspect of this issue comes up in engineering. When designing a sea
wall, you need to know how strong to make it to withstand tsunamis for, say, at
least a century. If you‚Äôre assembling a computer network, you need to know how
many component failures it should tolerate to likely operate without maintenance
for, say, at least a month. If your business is insurance, you need to know how
large a Ô¨Ånancial reserve to maintain to be nearly certain of paying beneÔ¨Åts for,
say, the next three decades. Technically, such questions come down to Ô¨Ånding the
probability of extreme deviations from the mean.
This issue of deviation from the mean is the focus of this chapter.

Chapter 18
Deviation from the Mean
708
18.2
Markov‚Äôs Theorem
Markov‚Äôs theorem gives a generally coarse estimate of the probability that a random
variable takes a value much larger than its mean. It is an almost trivial result by
itself, but it actually leads fairly directly to much stronger results.
The idea behind Markov‚Äôs Theorem can be explained with a simple example of
intelligence quotient, IQ. This quantity was devised so that the average IQ mea-
surement would be 100. Now from this fact alone we can conclude that at most
1/3 of the population can have an IQ of 300 or more, because if more than a third
had an IQ of 300, then the average would have to be more than .1=3/  300 D 100,
contradicting the fact that the average is 100. So the probability that a randomly
chosen person has an IQ of 300 or more is at most 1/3. Of course this is not a very
strong conclusion; in fact no IQ of over 300 has ever been recorded. But by the
same logic, we can also conclude that at most 2/3 of the population can have an
IQ of 150 or more. IQ‚Äôs of over 150 have certainly been recorded, though again, a
much smaller fraction than 2/3 of the population actually has an IQ that high.
Although these conclusions about IQ are weak, they are actually the strongest
general conclusions that can be reached about a random variable using only the fact
that it is nonnegative and its mean is 100. For example, if we choose a random
variable equal to 300 with probability 1/3, and 0 with probability 2/3, then its mean
is 100, and the probability of a value of 300 or more really is 1/3. So we can‚Äôt hope
to get a better upper bound based solely on this limited amount of information.
Theorem 18.2.1 (Markov‚Äôs Theorem). If R is a nonnegative random variable, then
for all x > 0
Pr≈íR  x¬ç  Ex≈íR¬ç
x
:
(18.1)
Proof. Let y vary over the range of R. Then for any x > 0
Ex≈íR¬ç WWD
X
y
y Pr≈íR D y¬ç

X
yx
y Pr≈íR D y¬ç 
X
yx
x Pr≈íR D y¬ç D x
X
yx
Pr≈íR D y¬ç
D x Pr≈íR  x¬ç;
(18.2)
where the Ô¨Årst inequality follows from the fact that R  0.
Dividing the Ô¨Årst and last expressions in (18.2) by x gives the desired result.


18.2. Markov‚Äôs Theorem
709
Our focus is deviation from the mean, so it‚Äôs useful to rephrase Markov‚Äôs Theo-
rem this way:
Corollary 18.2.2. If R is a nonnegative random variable, then for all c  1
Pr≈íR  c  Ex≈íR¬ç ¬ç  1
c :
(18.3)
This Corollary follows immediately from Markov‚Äôs Theorem(18.2.1) by letting
x be c  Ex≈íR¬ç.
18.2.1
Applying Markov‚Äôs Theorem
Let‚Äôs go back to the Hat-Check problem of Section 17.5.2. Now we ask what
the probability is that x or more men get the right hat, this is, what the value of
Pr≈íG  x¬ç is.
We can compute an upper bound with Markov‚Äôs Theorem.
Since we know
Ex≈íG¬ç D 1, Markov‚Äôs Theorem implies
Pr≈íG  x¬ç  Ex≈íG¬ç
x
D 1
x :
For example, there is no better than a 20% chance that 5 men get the right hat,
regardless of the number of people at the dinner party.
The Chinese Appetizer problem is similar to the Hat-Check problem. In this
case, n people are eating appetizers arranged on a circular, rotating Chinese banquet
tray. Someone then spins the tray so that each person receives a random appetizer.
What is the probability that everyone gets the same appetizer as before?
There are n equally likely orientations for the tray after it stops spinning. Ev-
eryone gets the right appetizer in just one of these n orientations. Therefore, the
correct answer is 1=n.
But what probability do we get from Markov‚Äôs Theorem? Let the random vari-
able, R, be the number of people that get the right appetizer. Then of course
Ex≈íR¬ç D 1 (right?), so applying Markov‚Äôs Theorem, we Ô¨Ånd:
Pr≈íR  n¬ç  Ex≈íR¬ç
n
D 1
n :
So for the Chinese appetizer problem, Markov‚Äôs Theorem is tight!
On the other hand, Markov‚Äôs Theorem gives the same 1=n bound in the Hat-
Check problem where the probability that everyone gets their hat is 1=.n≈†/. So for
this case, Markov‚Äôs Theorem gives a probability bound that is way too large.

Chapter 18
Deviation from the Mean
710
18.2.2
Markov‚Äôs Theorem for Bounded Variables
Suppose we learn that the average IQ among MIT students is 150 (which is not
true, by the way). What can we say about the probability that an MIT student has
an IQ of more than 200? Markov‚Äôs theorem immediately tells us that no more than
150=200 or 3=4 of the students can have such a high IQ. Here we simply applied
Markov‚Äôs Theorem to the random variable, R, equal to the IQ of a random MIT
student to conclude:
Pr≈íR > 200¬ç  Ex≈íR¬ç
200
D 150
200 D 3
4:
But let‚Äôs observe an additional fact (which may be true): no MIT student has an
IQ less than 100. This means that if we let T WWD R   100, then T is nonnegative
and Ex≈íT ¬ç D 50, so we can apply Markov‚Äôs Theorem to T and conclude:
Pr≈íR > 200¬ç D Pr≈íT > 100¬ç  Ex≈íT ¬ç
100
D 50
100 D 1
2:
So only half, not 3/4, of the students can be as amazing as they think they are. A
bit of a relief!
In fact, we can get better bounds applying Markov‚Äôs Theorem to R   b instead
of R for any lower bound b > 0 on R (see Problem 18.3). Similarly, if we have
any upper bound, u, on a random variable, S, then u   S will be a nonnegative
random variable, and applying Markov‚Äôs Theorem to u   S will allow us to bound
the probability that S is much less than its expectation.
18.3
Chebyshev‚Äôs Theorem
We‚Äôve seen that Markov‚Äôs Theorem can give a better bound when applied to R   b
rather than R. More generally, a good trick for getting stronger bounds on a ran-
dom variable R out of Markov‚Äôs Theorem is to apply the theorem to some cleverly
chosen function of R. Choosing functions that are powers of jRj turns out to be
specially useful. In particular, since jRjÀõ is nonnegative, Markov‚Äôs inequality also
applies to the event ≈í jRjÀõ  xÀõ¬ç. But this event is equivalent to the event ≈í jRj  x¬ç,
so we have:
Lemma 18.3.1. For any random variable R and positive real numbers Àõ and x,
Pr≈íjRj  x¬ç  Ex≈í jRjÀõ¬ç
xÀõ
:

18.3. Chebyshev‚Äôs Theorem
711
Rephrasing (18.3.1) in terms of the random variable, jR   Ex≈íR¬ç j, that measures
R‚Äôs deviation from its mean, we get
Pr≈í jR   Ex≈íR¬ç j  x¬ç  Ex≈í.R   Ex≈íR¬ç/Àõ¬ç
xÀõ
:
(18.4)
The case when Àõ D 2 turns out to be so important that the numerator of the right
hand side of (18.4) has been given a name:
DeÔ¨Ånition 18.3.2. The variance, Var≈íR¬ç, of a random variable, R, is:
Var≈íR¬ç WWD Ex

.R   Ex≈íR¬ç/2
:
Variance is also known as mean square deviation.
The restatement of (18.4) for Àõ D 2 is known as Chebyshev‚Äôs Theorem.
Theorem 18.3.3 (Chebyshev). Let R be a random variable and x 2 RC. Then
Pr≈íjR   Ex≈íR¬ç j  x¬ç  Var≈íR¬ç
x2
:
The expression Ex≈í.R   Ex≈íR¬ç/2¬ç for variance is a bit cryptic; the best approach
is to work through it from the inside out. The innermost expression, R   Ex≈íR¬ç, is
precisely the deviation of R above its mean. Squaring this, we obtain, .R Ex≈íR¬ç/2.
This is a random variable that is near 0 when R is close to the mean and is a large
positive number when R deviates far above or below the mean. So if R is always
close to the mean, then the variance will be small. If R is often far from the mean,
then the variance will be large.
18.3.1
Variance in Two Gambling Games
The relevance of variance is apparent when we compare the following two gam-
bling games.
Game A: We win $2 with probability 2=3 and lose $1 with probability 1=3.
Game B: We win $1002 with probability 2=3 and lose $2001 with probability
1=3.
Which game is better Ô¨Ånancially? We have the same probability, 2/3, of winning
each game, but that does not tell the whole story. What about the expected return for
each game? Let random variables A and B be the payoffs for the two games. For
example, A is 2 with probability 2/3 and -1 with probability 1/3. We can compute
the expected payoff for each game as follows:
Ex≈íA¬ç D 2  2
3 C . 1/  1
3 D 1;
Ex≈íB¬ç D 1002  2
3 C . 2001/  1
3 D 1:

Chapter 18
Deviation from the Mean
712
The expected payoff is the same for both games, but they are obviously very
different! This difference is not apparent in their expected value, but is captured by
variance. We can compute the Var≈íA¬ç by working ‚Äúfrom the inside out‚Äù as follows:
A   Ex≈íA¬ç
D

1
with probability 2
3
 2
with probability 1
3
.A   Ex≈íA¬ç/2
D
 1
with probability 2
3
4
with probability 1
3
Ex≈í.A   Ex≈íA¬ç/2¬ç
D
1  2
3 C 4  1
3
Var≈íA¬ç
D
2:
Similarly, we have for Var≈íB¬ç:
B   Ex≈íB¬ç
D

1001
with probability 2
3
 2002
with probability 1
3
.B   Ex≈íB¬ç/2
D
 1; 002; 001
with probability 2
3
4; 008; 004
with probability 1
3
Ex≈í.B   Ex≈íB¬ç/2¬ç
D
1; 002; 001  2
3 C 4; 008; 004  1
3
Var≈íB¬ç
D
2; 004; 002:
The variance of Game A is 2 and the variance of Game B is more than two
million! Intuitively, this means that the payoff in Game A is usually close to the
expected value of $1, but the payoff in Game B can deviate very far from this
expected value.
High variance is often associated with high risk. For example, in ten rounds of
Game A, we expect to make $10, but could conceivably lose $10 instead. On the
other hand, in ten rounds of game B, we also expect to make $10, but could actually
lose more than $20,000!
18.3.2
Standard Deviation
Because of its deÔ¨Ånition in terms of the square of a random variable, the variance
of a random variable may be very far from a typical deviation from the mean. For
example, in Game B above, the deviation from the mean is 1001 in one outcome and
-2002 in the other. But the variance is a whopping 2,004,002. From a dimensional
analysis viewpoint, the ‚Äúunits‚Äù of variance are wrong: if the random variable is in
dollars, then the expectation is also in dollars, but the variance is in square dollars.
For this reason, people often describe random variables using standard deviation
instead of variance.

18.3. Chebyshev‚Äôs Theorem
713
mean
O.¬¢/
Figure 18.1
The standard deviation of a distribution indicates how wide the
‚Äúmain part‚Äù of it is.
DeÔ¨Ånition 18.3.4. The standard deviation, R, of a random variable, R, is the
square root of the variance:
R WWD
p
Var≈íR¬ç D
q
Ex≈í.R   Ex≈íR¬ç/2¬ç:
So the standard deviation is the square root of the mean square deviation, or the
root mean square for short. It has the same units ‚Äîdollars in our example ‚Äîas
the original random variable and as the mean. Intuitively, it measures the average
deviation from the mean, since we can think of the square root on the outside as
canceling the square on the inside.
Example 18.3.5. The standard deviation of the payoff in Game B is:
B D
p
Var≈íB¬ç D
p
2; 004; 002  1416:
The random variable B actually deviates from the mean by either positive 1001
or negative 2002; therefore, the standard deviation of 1416 describes this situation
reasonably well.
Informally, the standard deviation measures the ‚Äúwidth‚Äù of the ‚Äúmain part‚Äù of the
distribution graph, as illustrated in Figure 18.1.
It‚Äôs useful to rephrase Chebyshev‚Äôs Theorem in terms of standard deviation which
we can do by substituting x D cR in (18.1):
Corollary 18.3.6. Let R be a random variable, and let c be a positive real number.
Pr≈íjR   Ex≈íR¬çj  cR¬ç  1
c2 :
(18.5)

Chapter 18
Deviation from the Mean
714
Here we see explicitly how the ‚Äúlikely‚Äù values of R are clustered in an O.R/-
sized region around Ex≈íR¬ç, conÔ¨Årming that the standard deviation measures how
spread out the distribution of R is around its mean.
The IQ Example
Suppose that, in addition to the national average IQ being 100, we also know the
standard deviation of IQ‚Äôs is 10. How rare is an IQ of 300 or more?
Let the random variable, R, be the IQ of a random person. So we are supposing
that Ex≈íR¬ç D 100, R D 10, and R is nonnegative. We want to compute Pr≈íR 
300¬ç.
We have already seen that Markov‚Äôs Theorem 18.2.1 gives a coarse bound, namely,
Pr≈íR  300¬ç  1
3:
Now we apply Chebyshev‚Äôs Theorem to the same problem:
Pr≈íR  300¬ç D Pr≈íjR   100j  200¬ç  Var≈íR¬ç
2002
D 102
2002 D
1
400:
So Chebyshev‚Äôs Theorem implies that at most one person in four hundred has an
IQ of 300 or more. We have gotten a much tighter bound using the additional infor-
mation, namely the variance of R, than we could get knowing only the expectation.
18.4
Properties of Variance
Focus on the variance and standard deviation of R may seem a little unexpected.
After all, these deÔ¨Ånitions arose from asking about the probability that the abso-
lute deviation, jR   Ex≈íR¬çj, was large. To get a better grip on the probability of
deviation, we squared it to get the Chebyshev Bound, this led us to the convoluted
concept of root mean square deviation.
It might seem more straighforward to measure the actual average deviation di-
rectly:
DeÔ¨Ånition 18.4.1. The expected absolute deviation of a real-valued random vari-
able, R, is deÔ¨Åned to be
Ex≈í jR   Ex≈íR¬çj ¬ç:
In contrast to this direct measure, standard deviation gives more weight to val-
ues that lie farther from the expected value. For this reason, standard deviation is

18.4. Properties of Variance
715
always at least as large as expected absolute deviation (see Problem 18.10). In this
section we‚Äôll describe a number of useful properties of variance and standard de-
viation that lead to their being more important concepts in probability theory than
the direct measure of expected absolute deviation.
18.4.1
A Formula for Variance
Applying linearity of expectation to the formula for variance yields a convenient
alternative formula.
Lemma 18.4.2.
Var≈íR¬ç D Ex≈íR2¬ç   Ex2≈íR¬ç;
for any random variable, R.
Here we use the notation Ex2≈íR¬ç as shorthand for .Ex≈íR¬ç/2.
Proof. Let  D Ex≈íR¬ç. Then
Var≈íR¬ç D Ex≈í.R   Ex≈íR¬ç/2¬ç
(Def 18.3.2 of variance)
D Ex≈í.R   /2¬ç
(def of )
D Ex≈íR2   2R C 2¬ç
D Ex≈íR2¬ç   2 Ex≈íR¬ç C 2
(linearity of expectation)
D Ex≈íR2¬ç   22 C 2
(def of )
D Ex≈íR2¬ç   2
D Ex≈íR2¬ç   Ex2≈íR¬ç:
(def of )

A simple and very useful formula for the variance of an indicator variable is an
immediate consequence.
Corollary 18.4.3. If B is a Bernoulli variable where p WWD Pr≈íB D 1¬ç, then
Var≈íB¬ç D p   p2 D p.1   p/:
(18.6)
Proof. By Lemma 17.4.2, Ex≈íB¬ç D p. But B only takes values 0 and 1, so B2 D B
and equation (18.6) follows immediately from Lemma 18.4.2.


Chapter 18
Deviation from the Mean
716
18.4.2
Variance of Time to Failure
According to section 17.4.6, the mean time to failure is 1=p for a process that fails
during any given hour with probability p. What about the variance?
By Lemma 18.4.2,
Var≈íC¬ç D Ex≈íC 2¬ç   .1=p/2
(18.7)
so all we need is a formula for Ex≈íC 2¬ç.
Reasoning about C using conditional expectation worked nicely in section 17.4.6
to Ô¨Ånd mean time to failure, and a similar approach works for C 2. Namely, the
expected value of C 2 is the probability, p, of failure in the Ô¨Årst hour times 12, plus
the probability, .1   p/, of non-failure in the Ô¨Årst hour times the expected value of
.C C 1/2. So
Ex≈íC 2¬ç D p  12 C .1   p/ Ex≈í.C C 1/2¬ç
D p C .1   p/

Ex≈íC 2¬ç C 2
p C 1

D p C .1   p/ Ex≈íC 2¬ç C .1   p/
 2
p C 1

;
so
p Ex≈íC 2¬ç D p C .1   p/
 2
p C 1

D p2 C .1   p/.2 C p/
p
and
Ex≈íC 2¬ç D 2   p
p2
Combining this with (18.7) proves
Lemma 18.4.4. If failures occur with probability p independently at each step, and
C is the number of steps until the Ô¨Årst failure1, then
Var≈íC¬ç D 1   p
p2
:
(18.8)
18.4.3
Dealing with Constants
It helps to know how to calculate the variance of aR C b:
Theorem 18.4.5. [Square Multiple Rule for Variance] Let R be a random variable
and a a constant. Then
Var≈íaR¬ç D a2 Var≈íR¬ç:
(18.9)
1That is, C has the geometric distribution with parameter p according to DeÔ¨Ånition 17.4.6.

18.4. Properties of Variance
717
Proof. Beginning with the deÔ¨Ånition of variance and repeatedly applying linearity
of expectation, we have:
Var≈íaR¬ç WWD Ex≈í.aR   Ex≈íaR¬ç/2¬ç
D Ex≈í.aR/2   2aR Ex≈íaR¬ç C Ex2≈íaR¬ç¬ç
D Ex≈í.aR/2¬ç   Ex≈í2aR Ex≈íaR¬ç¬ç C Ex2≈íaR¬ç
D a2 Ex≈íR2¬ç   2 Ex≈íaR¬ç Ex≈íaR¬ç C Ex2≈íaR¬ç
D a2 Ex≈íR2¬ç   a2 Ex2≈íR¬ç
D a2  Ex≈íR2¬ç   Ex2≈íR¬ç

D a2 Var≈íR¬ç
(Lemma 18.4.2)

It‚Äôs even simpler to prove that adding a constant does not change the variance, as
the reader can verify:
Theorem 18.4.6. Let R be a random variable, and b a constant. Then
Var≈íR C b¬ç D Var≈íR¬ç:
(18.10)
Recalling that the standard deviation is the square root of variance, this implies
that the standard deviation of aR C b is simply jaj times the standard deviation of
R:
Corollary 18.4.7.
.aRCb/ D jaj R:
18.4.4
Variance of a Sum
In general, the variance of a sum is not equal to the sum of the variances, but
variances do add for independent variables. In fact, mutual independence is not
necessary: pairwise independence will do. This is useful to know because there are
some important situations involving variables that are pairwise independent but not
mutually independent.
Theorem 18.4.8. If R1 and R2 are independent random variables, then
Var≈íR1 C R2¬ç D Var≈íR1¬ç C Var≈íR2¬ç:
(18.11)
Proof. We may assume that Ex≈íRi¬ç D 0 for i D 1; 2, since we could always replace
Ri by Ri   Ex≈íRi¬ç in equation (18.11). This substitution preserves the indepen-
dence of the variables, and by Theorem 18.4.6, does not change the variances.

Chapter 18
Deviation from the Mean
718
Now by Lemma 18.4.2, Var≈íRi¬ç D Ex≈íR2
i ¬ç and Var≈íR1CR2¬ç D Ex≈í.R1CR2/2¬ç,
so we need only prove
Ex≈í.R1 C R2/2¬ç D Ex≈íR2
1¬ç C Ex≈íR2
2¬ç:
(18.12)
But (18.12) follows from linearity of expectation and the fact that
Ex≈íR1R2¬ç D Ex≈íR1¬ç Ex≈íR2¬ç
(18.13)
since R1 and R2 are independent:
Ex≈í.R1 C R2/2¬ç D Ex≈íR2
1 C 2R1R2 C R2
2¬ç
D Ex≈íR2
1¬ç C 2 Ex≈íR1R2¬ç C Ex≈íR2
2¬ç
D Ex≈íR2
1¬ç C 2 Ex≈íR1¬ç Ex≈íR2¬ç C Ex≈íR2
2¬ç
(by (18.13))
D Ex≈íR2
1¬ç C 2  0  0 C Ex≈íR2
2¬ç
D Ex≈íR2
1¬ç C Ex≈íR2
2¬ç

It‚Äôs easy to see that additivity of variance does not generally hold for variables
that are not independent. For example, if R1 D R2, then equation (18.11) becomes
Var≈íR1CR1¬ç D Var≈íR1¬çCVar≈íR1¬ç. By the Square Multiple Rule, Theorem 18.4.5,
this holds iff 4 Var≈íR1¬ç D 2 Var≈íR1¬ç, which implies that Var≈íR1¬ç D 0. So equa-
tion (18.11) fails when R1 D R2 and R1 has nonzero variance.
The proof of Theorem 18.4.8 carries over straightforwardly to the sum of any
Ô¨Ånite number of variables. So we have:
Theorem 18.4.9. [Pairwise Independent Additivity of Variance] If R1; R2; : : : ; Rn
are pairwise independent random variables, then
Var≈íR1 C R2 C    C Rn¬ç D Var≈íR1¬ç C Var≈íR2¬ç C    C Var≈íRn¬ç:
(18.14)
Now we have a simple way of computing the variance of a variable, J , that has
an .n; p/-binomial distribution. We know that J D Pn
kD1 Ik where the Ik are
mutually independent indicator variables with Pr≈íIk D 1¬ç D p. The variance of
each Ik is p.1   p/ by Corollary 18.4.3, so by linearity of variance, we have
Lemma (Variance of the Binomial Distribution). If J has the .n; p/-binomial dis-
tribution, then
Var≈íJ ¬ç D n Var≈íIk¬ç D np.1   p/:
(18.15)

18.5. Estimation by Random Sampling
719
18.5
Estimation by Random Sampling
Democratic politicians were astonished in 2010 when their early polls of sample
voters showed Republican Scott Brown was favored by a majority of voters and so
would win the special election to Ô¨Åll the Senate seat Democrat Teddy Kennedy had
occupied for over 40 years. Based on their poll results, they mounted an intense,
but ultimately unsuccessful, effort to save the seat for their party.
18.5.1
A Voter Poll
How did polling give an advance estimate of the fraction of the Massachusetts
voters who favored Scott Brown over his Democratic opponent?
Suppose at some time before the election that p was the fraction of voters favor-
ing Scott Brown. We want to estimate this unknown fraction p. Suppose we have
some random process ‚Äîsay throwing darts at voter registration lists ‚Äîwhich will
select each voter with equal probability. We can deÔ¨Åne a Bernoulli variable, K, by
the rule that K D 1 if the random voter most prefers Brown, and K D 0 otherwise.
Now to estimate p, we take a large number, n, of random choices of voters2
and count the fraction who favor Brown. That is, we deÔ¨Åne variables K1; K2; : : : ,
where Ki is interpreted to be the indicator variable for the event that the ith cho-
sen voter prefers Brown. Since our choices are made independently, the Ki‚Äôs are
independent. So formally, we model our estimation process by simply assuming
we have mutually independent Bernoulli variables K1; K2; : : : ; each with the same
probability, p, of being equal to 1. Now let Sn be their sum, that is,
Sn WWD
n
X
iD1
Ki:
(18.16)
The variable Sn=n describes the fraction of sampled voters who favor Scott Brown.
Most people intuitively expect this sample fraction to give a useful approximation
to the unknown fraction, p ‚Äîand they would be right. So we will use the sample
value, Sn=n, as our statistical estimate of p. We know that Sn has the binomial
distribution with parameters n and p, where we can choose n, but p is unknown.
2We‚Äôre choosing a random voter n times with replacement. That is, we don‚Äôt remove a chosen
voter from the set of voters eligible to be chosen later; so we might choose the same voter more than
once in n tries! We would get a slightly better estimate if we required n different people to be chosen,
but doing so complicates both the selection process and its analysis, with little gain in accuracy.

Chapter 18
Deviation from the Mean
720
How Large a Sample?
Suppose we want our estimate to be within 0:04 of the fraction, p, at least 95% of
the time. This means we want
Pr
 ÀáÀáÀáÀá
Sn
n   p
ÀáÀáÀáÀá  0:04

 0:95 :
(18.17)
So we better determine the number, n, of times we must poll voters so that inequal-
ity (18.17) will hold. Chebyshev‚Äôs Theorem offers a simple way to determine such
a n.
Since Sn is binomially distributed, equation (18.15) gives
Var≈íSn¬ç D n.p.1   p//  n  1
4 D n
4:
The bound of 1/4 follows from the fact that p.1 p/ is maximized when p D 1 p,
that is, when p D 1=2 (check this yourself!).
Next, we bound the variance of Sn=n:
Var
Sn
n

D
1
n
2
Var≈íSn¬ç
(Square Multiple Rule for Variance (18.9))

1
n
2 n
4
(by (18.5.1))
D 1
4n
(18.18)
Using Chebyshev‚Äôs bound and (18.18) we have:
Pr
 ÀáÀáÀáÀá
Sn
n   p
ÀáÀáÀáÀá  0:04

 Var≈íSn=n¬ç
.0:04/2

1
4n.0:04/2 D 156:25
n
(18.19)
To make our our estimate with 95% conÔ¨Ådence, we want the righthand side
of (18.19) to be at most 1/20. So we choose n so that
156:25
n
 1
20;
that is,
n  3; 125:
Section 18.7.2 describes how to get tighter estimates of the tails of binomial dis-
tributions that lead to a bound on n that is about four times smaller than the one
above. But working through this example using only the variance has the virtue of
illustrating an approach to estimation that is applicable to arbitrary random vari-
ables, not just binomial variables.

18.5. Estimation by Random Sampling
721
18.5.2
Matching Birthdays
There are important cases where the relevant distributions are not binomial because
the mutual independence properties of the voter preference example do not hold.
In these cases, estimation methods based on the Chebyshev bound may be the best
approach. Birthday Matching is an example. We already saw in Section 16.6.6 that
in a class of 85 students it is virtually certain that two or more students will have
the same birthday. This suggests that quite a few pairs of students are likely to have
the same birthday. How many?
So as before, suppose there are n students and d days in the year, and let D be the
number of pairs of students with the same birthday. Now it will be easy to calculate
the expected number of pairs of students with matching birthdays. Then we can
take the same approach as we did in estimating voter preferences to get an estimate
of the probability of getting a number of pairs close to the expected number.
Unlike the situation with voter preferences, having matching birthdays for dif-
ferent pairs of students are not mutually independent events, but the matchings are
pairwise independent ‚Äîas explained in Section 16.6.6 and proved in Problem 17.2.
This will allow us to apply the same reasoning to Birthday Matching as we did for
voter preference. Namely, let B1; B2; : : : ; Bn be the birthdays of n independently
chosen people, and let Ei;j be the indicator variable for the event that the ith and
jth people chosen have the same birthdays, that is, the event ≈íBi D Bj ¬ç. So in
our probability model, the Bi‚Äôs are mutually independent variables, and the Ei;j ‚Äôs
are pairwise independent. Also, the expectations of Ei;j for i ¬§ j equals the
probability that Bi D Bj , namely, 1=d.
Now, D, the number of matching pairs of birthdays among the n choices, is
simply the sum of the Ei;j ‚Äôs:
D WWD
X
1i<jn
Ei;j :
(18.20)
So by linearity of expectation
Ex≈íD¬ç D Ex
2
4
X
1i<j n
Ei;j
3
5 D
X
1i<j n
Ex≈íEi;j ¬ç D
 
n
2
!
 1
d :

Chapter 18
Deviation from the Mean
722
Similarly,
Var≈íD¬ç D Var
2
4
X
1i<j n
Ei;j
3
5
D
X
1i<j n
Var≈íEi;j ¬ç
(Theorem 18.4.9)
D
 
n
2
!
 1
d

1   1
d

:
(Corollary 18.4.3)
In particular, for a class of n D 95 students with d D 365 possible birthdays, we
have Ex≈íD¬ç  12:23 and Var≈íD¬ç  12:23.1   1=365/ < 12:2. So by Chebyshev‚Äôs
Theorem
Pr≈íjD   Ex≈íD¬çj  x¬ç < 12:2
x2 :
Letting x D 7, we conclude that there is a better than 75% chance that in a class of
95 students, the number of pairs of students with the same birthday will be within
7 of 12.23, namely will be between 6 and 20.
18.5.3
Pairwise Independent Sampling
The reasoning we used above to analyze voter polling and matching birthdays is
very similar. We summarize it in slightly more general form with a basic result we
call the Pairwise Independent Sampling Theorem. In particular, we do not need
to restrict ourselves to sums of zero-one valued variables, or to variables with the
same distribution. For simplicity, we state the Theorem for pairwise independent
variables with possibly different distributions but with the same mean and variance.
Theorem 18.5.1 (Pairwise Independent Sampling). Let G1; : : : ; Gn be pairwise
independent variables with the same mean, , and deviation, . DeÔ¨Åne
Sn WWD
n
X
iD1
Gi:
(18.21)
Then
Pr
 ÀáÀáÀáÀá
Sn
n   
ÀáÀáÀáÀá  x

 1
n

x
2
:

18.5. Estimation by Random Sampling
723
Proof. We observe Ô¨Årst that the expectation of Sn=n is :
Ex
Sn
n

D Ex
Pn
iD1 Gi
n

(def of Sn)
D
Pn
iD1 Ex≈íGi¬ç
n
(linearity of expectation)
D
Pn
iD1 
n
D n
n D :
The second important property of Sn=n is that its variance is the variance of Gi
divided by n:
Var
Sn
n

D
1
n
2
Var≈íSn¬ç
(Square Multiple Rule for Variance (18.9))
D 1
n2 Var
" n
X
iD1
Gi
#
(def of Sn)
D 1
n2
n
X
iD1
Var≈íGi¬ç
(pairwise independent additivity)
D 1
n2  n2 D 2
n :
(18.22)
This is enough to apply Chebyshev‚Äôs Theorem and conclude:
Pr
 ÀáÀáÀáÀá
Sn
n   
ÀáÀáÀáÀá  x

 Var ≈íSn=n¬ç
x2
:
(Chebyshev‚Äôs bound)
D 2=n
x2
(by (18.22))
D 1
n

x
2
:

The Pairwise Independent Sampling Theorem provides a precise general state-
ment about how the average of independent samples of a random variable ap-
proaches the mean. In particular, it proves what is known as the Law of Large
Numbers3: by choosing a large enough sample size, we can get arbitrarily accurate
estimates of the mean with conÔ¨Ådence arbitrarily close to 100%.
3This is the Weak Law of Large Numbers. As you might suppose, there is also a Strong Law, but
it‚Äôs outside the scope of 6.042.

Chapter 18
Deviation from the Mean
724
Corollary 18.5.2. [Weak Law of Large Numbers] Let G1; : : : ; Gn be pairwise in-
dependent variables with the same mean, , and the same Ô¨Ånite deviation, and
let
Sn WWD
Pn
iD1 Gi
n
:
Then for every  > 0,
lim
n!1 Pr≈íjSn   j  ¬ç D 1:
18.6
ConÔ¨Ådence versus Probability
So Chebyshev‚Äôs Bound implies that sampling 3,125 voters will yield a fraction that,
95% of the time, is within 0.04 of the actual fraction of the voting population who
prefer Brown.
Notice that the actual size of the voting population was never considered because
it did not matter. People who have not studied probability theory often insist that
the population size should matter. But our analysis shows that polling a little over
3000 people people is always sufÔ¨Åcient, whether there are ten thousand, or a mil-
lion, or a billion . . . voters. You should think about an intuitive explanation that
might persuade someone who thinks population size matters.
Now suppose a pollster actually takes a sample of 3,125 random voters to esti-
mate the fraction of voters who prefer Brown, and the pollster Ô¨Ånds that 1250 of
them prefer Brown. It‚Äôs tempting, but sloppy, to say that this means:
False Claim. With probability 0.95, the fraction, p, of voters who prefer Brown is
1250=3125 Àô 0:04. Since 1250=3125   0:04 > 1=3, there is a 95% chance that
more than a third of the voters prefer Brown to all other candidates.
What‚Äôs objectionable about this statement is that it talks about the probability or
‚Äúchance‚Äù that a real world fact is true, namely that the actual fraction, p, of voters
favoring Brown is more than 1/3. But p is what it is, and it simply makes no sense
to talk about the probability that it is something else. For example, suppose p is
actually 0.3; then it‚Äôs nonsense to ask about the probability that it is within 0.04 of
1250/3125 ‚Äîit simply isn‚Äôt.
This example of voter preference is typical: we want to estimate a Ô¨Åxed, un-
known real-world quantity. But being unknown does not make this quantity a ran-
dom variable, so it makes no sense to talk about the probability that it has some
property.
A more careful summary of what we have accomplished goes this way:

18.7. Sums of Random Variables
725
We have described a probabilistic procedure for estimating the value
of the actual fraction, p. The probability that our estimation procedure
will yield a value within 0.04 of p is 0.95.
This is a bit of a mouthful, so special phrasing closer to the sloppy language is
commonly used. The pollster would describe his conclusion by saying that
At the 95% conÔ¨Ådence level, the fraction of voters who prefer Brown
is 1250=3125 Àô 0:04.
So conÔ¨Ådence levels refer to the results of estimation procedures for real-world
quantities. The phrase ‚ÄúconÔ¨Ådence level‚Äù should be heard as a reminder that some
statistical procedure was used to obtain an estimate, and in judging the credibility
of the estimate, it may be important to learn just what this procedure was.
18.7
Sums of Random Variables
If all you know about a random variable is its mean and variance, then Chebyshev‚Äôs
Theorem is the best you can do when it comes to bounding the probability that
the random variable deviates from its mean. In some cases, however, we know
more ‚Äîfor example, that the random variable has a binomial distribution ‚Äîand
then it is possible to prove much stronger bounds. Instead of polynomially small
bounds such as 1=c2, we can sometimes even obtain exponentially small bounds
such as 1=ec. As we will soon discover, this is the case whenever the random
variable T is the sum of n mutually independent random variables T1, T2, ..., Tn
where 0  Ti  1. A random variable with a binomial distribution is just one of
many examples of such a T . Here is another.
18.7.1
A Motivating Example
Fussbook is a new social networking site oriented toward unpleasant people.
Like all major web services, Fussbook has a load balancing problem. Specif-
ically, Fussbook receives 24,000 forum posts every 10 minutes. Each post is as-
signed to one of m computers for processing, and each computer works sequen-
tially through its assigned tasks. Processing an average post takes a computer 1=4
second. Some posts, such as pointless grammar critiques and snide witticisms, are
easier. But the most protracted harangues require 1 full second.
Balancing the work load across the m computers is vital; if any computer is as-
signed more than 10 minutes of work in a 10-minute interval, then that computer is

Chapter 18
Deviation from the Mean
726
overloaded and system performance suffers. That would be bad, because Fussbook
users are not a tolerant bunch.
An early idea was to assign each computer an alphabetic range of forum topics.
(‚ÄúThat oughta work!‚Äù, one programmer said.) But after the computer handling the
‚Äúprivacy‚Äù and ‚Äúpreferred text editor‚Äù threads melted, the drawback of an ad hoc
approach was clear: there are no guarantees.
If the length of every task were known in advance, then Ô¨Ånding a balanced dis-
tribution would be a kind of ‚Äúbin packing‚Äù problem. Such problems are hard to
solve exactly, though approximation algorithms can come close. But in this case,
task lengths are not known in advance, which is typical for workload problems in
the real world.
So the load balancing problem seems sort of hopeless, because there is no data
available to guide decisions. Heck, we might as well assign tasks to computers at
random!
As it turns out, random assignment not only balances load reasonably well, but
also permits provable performance guarantees in place of ‚ÄúThat oughta work!‚Äù as-
sertions. In general, a randomized approach to a problem is worth considering when
a deterministic solution is hard to compute or requires unavailable information.
Some arithmetic shows that Fussbook‚Äôs trafÔ¨Åc is sufÔ¨Åcient to keep m D 10 com-
puters running at 100% capacity with perfect load balancing. Surely, more than 10
servers are needed to cope with random Ô¨Çuctuations in task length and imperfect
load balance. But how many is enough? 11? 15? 20? 100? We‚Äôll answer that
question with a new mathematical tool.
18.7.2
The Chernoff Bound
The Chernoff4 bound is a hammer that you can use to nail a great many problems.
Roughly, the Chernoff bound says that certain random variables are very unlikely
to signiÔ¨Åcantly exceed their expectation. For example, if the expected load on
a computer is just a bit below its capacity, then that computer is unlikely to be
overloaded, provided the conditions of the Chernoff bound are satisÔ¨Åed.
More precisely, the Chernoff Bound says that the sum of lots of little, indepen-
dent random variables is unlikely to signiÔ¨Åcantly exceed the mean of the sum. The
Markov and Chebyshev bounds lead to the same kind of conclusion but typically
provide much weaker bounds. In particular, the Markov and Chebyshev bounds are
polynomial, while the Chernoff bound is exponential.
Here is the theorem. The proof will come later in Section 18.7.6.
4Yes, this is the same Chernoff who Ô¨Ågured out how to beat the state lottery ‚Äîthis guy knows a
thing or two.

18.7. Sums of Random Variables
727
Theorem 18.7.1 (Chernoff Bound). Let T1; : : : Tn be mutually independent ran-
dom variables such that 0  Ti  1 for all i. Let T D T1 C    C Tn. Then for all
c  1,
Pr≈íT  c Ex≈íT ¬ç¬ç  e Àá.c/ Ex≈íT ¬ç
(18.23)
where Àá.c/ WWD c ln c   c C 1.
The Chernoff bound applies only to distributions of sums of independent random
variables that take on values in the interval ≈í0; 1¬ç. The binomial distribution is
of course such a distribution, but there are lots of other distributions because the
Chernoff bound allows the variables in the sum to have differing, arbitrary, and
even unknown distributions over the range ≈í0; 1¬ç. Furthermore, there is no direct
dependence on the number of random variables in the sum or their expectations. In
short, the Chernoff bound gives strong results for lots of problems based on little
information ‚Äîno wonder it is widely used!
18.7.3
Chernoff Bound for Binomial Tails
The Chernoff bound is pretty easy to apply, though the details can be daunting at
Ô¨Årst. Let‚Äôs walk through a simple example to get the hang of it: getting bounds on
the tail of a binomial distribution, for example, bounding the probability that the
number of heads that come up in 1000 independent tosses of a coin exceeds the
expectation by 20% or more? Let Ti be an indicator variable for the event that the
ith coin is heads. Then the total number of heads is
T D T1 C    C T1000:
The Chernoff bound requires that the random variables Ti be mutually independent
and take on values in the range ≈í0; 1¬ç. Both conditions hold here. In this example
the Ti‚Äôs only take the two values 0 and 1, since they‚Äôre indicators.
The goal is to bound the probability that the number of heads exceeds its expec-
tation by 20% or more; that is, to bound Pr≈íT  c Ex≈íT ¬ç¬ç where c = 1:2. To that
end, we compute Àá.c/ as deÔ¨Åned in the theorem:
Àá.c/ D c ln.c/   c C 1 D 0:0187 : : : :
If we assume the coin is fair, then Ex≈íT ¬ç D 500. Plugging these values into the
Chernoff bound gives:
Pr

T  1:2 Ex≈íT ¬ç

 e Àá.c/: Ex≈íT ¬ç
D e .0:0187::: /500 < 0:0000834:

Chapter 18
Deviation from the Mean
728
So the probability of getting 20% or more extra heads on 1000 coins is less than 1
in 10,000.
The bound becomes much stronger as the number of coins increases, because
the expected number of heads appears in the exponent of the upper bound. For
example, the probability of getting at least 20% extra heads on a million coins is at
most
e .0:0187::: /500000 < e 9392;
which is an inconceivably small number.
Alternatively, the bound also becomes stronger for larger deviations. For exam-
ple, suppose we‚Äôre interested in the odds of getting 30% or more extra heads in
1000 tosses, rather than 20%. In that case, c D 1:3 instead of 1:2. Consequently,
the parameter Àá.c/ rises from 0:0187 to about 0:0410, which may not seem sig-
niÔ¨Åcant, but because Àá.c/ appears in the exponent of the upper bound, the Ô¨Ånal
probability decreases from around 1 in 10,000 to about 1 in a billion!
18.7.4
Chernoff Bound for a Lottery Game
Pick-4 is a lottery game where you pay $1 to pick a 4-digit number between 0000
and 9999. If your number comes up in a random drawing, then you win $5,000.
Your chance of winning is 1 in 10,000. If 10 million people play, then the expected
number of winners is 1000. When there are exactly 1000 winners, the lottery keeps
$5 million of the $10 million paid for tickets. The lottery operator‚Äôs nightmare is
that the number of winners is much greater ‚Äîsay at the 2000 or greater point where
the lottery has to pay out more than it received. What is the probability that will
happen?
Let Ti be an indicator for the event that the ith player wins. Then T D T1C  C
Tn is the total number of winners. If we assume5 that the players‚Äô picks and the
winning number are random, independent and uniform, then the indicators Ti are
independent, as required by the Chernoff bound.
Since 2000 winners would be twice the expected number, we choose c D 2,
compute Àá.c/ D 0:386 : : : , and plug these values into the Chernoff bound:
Pr≈íT  2000¬ç D Pr

T  2 Ex≈íT ¬ç

 e k Ex≈íT ¬ç D e .0:386::: /1000
< e 386:
5As we noted in Chapter 17, human choices are often not uniform and they can be highly depen-
dent. For example, lots of people will pick an important date. So the lottery folks should not get
too much comfort from the analysis that follows, unless they assign random 4-digit numbers to each
player.

18.7. Sums of Random Variables
729
So there is almost no chance that the lottery operator pays out double. In fact, the
number of winners won‚Äôt even be 10% higher than expected very often. To prove
that, let c D 1:1, compute Àá.c/ D 0:00484 : : : , and plug in again:
Pr

T  1:1 Ex≈íT ¬ç

 e k Ex≈íT ¬ç
D e .0:00484/1000 < 0:01:
So the Pick-4 lottery may be exciting for the players, but the lottery operator has
little doubt about the outcome!
18.7.5
Randomized Load Balancing
Now let‚Äôs return to Fussbook and its load balancing problem. SpeciÔ¨Åcally, we need
to determine how many machines sufÔ¨Åce to ensure that no server is overloaded;
that is, assigned to do more than 10 minutes of work in a 10-minute interval. So a
server is overloaded if it gets assigned more than 600 seconds of work.
To begin, let‚Äôs Ô¨Ånd the probability that the Ô¨Årst server is overloaded. Letting T be
the number of seconds of work assigned to the Ô¨Årst server, this means we want an
upper bound on Pr≈íT  600¬ç. Let Ti be the number of seconds that the Ô¨Årst server
spends on the ith task: then Ti is zero if the task is assigned to another machine,
and otherwise Ti is the length of the task. So T D Pn
iD1 Ti is the total length of
tasks assigned to the Ô¨Årst server, where n D 24;000.
The Chernoff bound is applicable only if the Ti are mutually independent and
take on values in the range ≈í0; 1¬ç. The Ô¨Årst condition is satisÔ¨Åed if we assume that
task lengths and assignments are independent. And the second condition is satisÔ¨Åed
because processing even the most interminable harangue takes at most 1 second.
In all, there are 24,000 tasks, each with an expected length of 1/4 second. Since
tasks are assigned to computers at random, the expected load on the Ô¨Årst server is:
Ex≈íT ¬ç D 24;000 tasks  1=4 second per task
m machines
D 6000=m seconds:
(18.24)
For example, if there are fewer than 10 machines, then the expected load on the
Ô¨Årst server is greater than its capacity, and we can expect it to be overloded. If there
are exactly 10 machines, then the server is expected to run for 6000=10 D 600
seconds, which is 100% of its capacity.
Now we can use the Chernoff bound to upper bound the probability that the Ô¨Årst
server is overloaded. We have from (18.24)
600 D c Ex≈íT ¬ç
where c WWD m=10;

Chapter 18
Deviation from the Mean
730
so by the Chernoff bound
Pr≈íT  600¬ç D Pr≈íT  c Ex≈íT ¬ç¬ç  e .c ln.c/ cC1/6000=m;
The probability that some server is overloaded is at most m times the probability
that the Ô¨Årst server is overloaded, by the Union Bound in Section 16.4.2. So
Pr≈ísome server is overloaded¬ç 
m
X
iD1
Pr≈íserver i is overloaded¬ç
D m Pr≈íthe Ô¨Årst server is overloaded¬ç
 me .c ln.c/ cC1/6000=m;
where c D m=10. Some values of this upper bound are tabulated below:
m
D
11 W
0:784 : : :
m
D
12 W
0:000999 : : :
m
D
13 W
0:0000000760 : : :
These values suggest that a system with m D 11 machines might suffer immediate
overload, m D 12 machines could fail in a few days, but m D 13 should be Ô¨Åne for
a century or two!
18.7.6
Proof of the Chernoff Bound
The proof of the Chernoff bound is somewhat involved. Heck, even Chernoff didn‚Äôt
come up with it! His friend, Herman Rubin, showed him the argument. Thinking
the bound not very signiÔ¨Åcant, Chernoff did not credit Rubin in print. He felt pretty
bad when it became famous!6
Proof. (of Theorem 18.7.1)
For clarity, we‚Äôll go through the proof ‚Äútop down.‚Äù That is, we‚Äôll use facts that
are proved immediately afterward.
The key step is to exponentiate both sides of the inequality T  c Ex≈íT ¬ç and
6See ‚ÄúA Conversation with Herman Chernoff,‚Äù Statistical Science 1996, Vol. 11, No. 4, pp 335‚Äì
350.

18.7. Sums of Random Variables
731
then apply the Markov bound:
Pr≈íT  c Ex≈íT ¬ç¬ç D Pr≈ícT  cc Ex≈íT ¬ç¬ç
 Ex≈ícT ¬ç
cc Ex≈íT ¬ç
(Markov Bound)
 e.c 1/ Ex≈íT ¬ç
cc Ex≈íT ¬ç
(Lemma 18.7.2 below)
D e.c 1/ Ex≈íT ¬ç
ec ln.c/ Ex≈íT ¬ç D e .c ln.c/ cC1/ Ex≈íT ¬ç:

Algebra aside, there is a brilliant idea in this proof: in this context, exponenti-
ating somehow supercharges the Markov bound. This is not true in general! One
unfortunate side-effect is that we have to bound some nasty expectations involving
exponentials in order to complete the proof. This is done in the two lemmas below,
where variables take on values as in Theorem 18.7.1.
Lemma 18.7.2.
Ex
h
cT i
 e.c 1/ Ex≈íT ¬ç:
Proof.
Ex
h
cT i
D Ex
h
cT1CCTn
i
(def of T )
D Ex
h
cT1    cTn
i
D Ex
h
cT1
i
   Ex≈ícTn¬ç
(independent product Cor 17.5.7)
 e.c 1/ Ex≈íT1¬ç    e.c 1/ Ex≈íTn¬ç
(Lemma 18.7.3 below)
D e.c 1/.Ex≈íT1¬çCCEx≈íTn¬ç/
D e.c 1/ Ex≈íT1CCTn¬ç
(linearity of Ex≈í¬ç)
D e.c 1/ Ex≈íT ¬ç:
The third equality depends on the fact that functions of independent variables are
also independent (see Lemma 17.2.2).

Lemma 18.7.3.
Ex≈ícTi¬ç  e.c 1/ Ex≈íTi¬ç

Chapter 18
Deviation from the Mean
732
Proof. All summations below range over values v taken by the random variable Ti,
which are all required to be in the interval ≈í0; 1¬ç.
Ex≈ícTi¬ç D
X
cv Pr≈íTi D v¬ç
(def of Ex≈í¬ç)

X
.1 C .c   1/v/ Pr≈íTi D v¬ç
(convexity ‚Äîsee below)
D
X
Pr≈íTi D v¬ç C .c   1/v Pr≈íTi D v¬ç
D
X
Pr≈íTi D v¬ç C .c   1/
X
v Pr≈íTi D v¬ç
D 1 C .c   1/ Ex≈íTi¬ç
 e.c 1/ Ex≈íTi¬ç
(since 1 C z  ez):
The second step relies on the inequality
cv  1 C .c   1/v;
which holds for all v in ≈í0; 1¬ç and c  1. This follows from the general principle
that a convex function, namely cv, is less than the linear function, 1 C .c   1/v,
between their points of intersection, namely v D 0 and 1. This inequality is why
the variables Ti are restricted to the interval ≈í0; 1¬ç.

18.7.7
Comparing the Bounds
Suppose that we have a collection of mutually independent events A1, A2, ..., An,
and we want to know how many of the events are likely to occur.
Let Ti be the indicator random variable for Ai and deÔ¨Åne
pi D Pr≈íTi D 1¬ç D Pr

Ai

for 1  i  n. DeÔ¨Åne
T D T1 C T2 C    C Tn
to be the number of events that occur.
We know from Linearity of Expectation that
Ex≈íT ¬ç D Ex≈íT1¬ç C Ex≈íT2¬ç C    C Ex≈íTn¬ç
D
n
X
iD1
pi:
This is true even if the events are not independent.

18.7. Sums of Random Variables
733
By Theorem 18.4.9, we also know that
Var≈íT ¬ç D Var≈íT1¬ç C Var≈íT2¬ç C    C Var≈íTn¬ç
D
n
X
iD1
pi.1   pi/;
and thus that
T D
v
u
u
t
n
X
iD1
pi.1   pi/:
This is true even if the events are only pairwise independent.
Markov‚Äôs Theorem tells us that for any c > 1,
Pr≈íT  c Ex≈íT ¬ç¬ç  1
c :
Chebyshev‚Äôs Theorem gives us the stronger result that
Pr≈íjT   Ex≈íT ¬çj  cT ¬ç  1
c2 :
The Chernoff Bound gives us an even stronger result, namely, that for any c > 0,
Pr≈íT   Ex≈íT ¬ç  c Ex≈íT ¬ç¬ç  e .c ln.c/ cC1/ Ex≈íT ¬ç:
In this case, the probability of exceeding the mean by c Ex≈íT ¬ç decreases as an
exponentially small function of the deviation.
By considering the random variable n   T , we can also use the Chernoff Bound
to prove that the probability that T is much lower than Ex≈íT ¬ç is also exponentially
small.
18.7.8
Murphy‚Äôs Law
If the expectation of a random variable is much less than 1, then Markov‚Äôs Theorem
implies that there is only a small probability that the variable has a value of 1 or
more. On the other hand, a result that we call Murphy‚Äôs Law7 says that if a random
variable is an independent sum of 0-1-valued variables and has a large expectation,
then there is a huge probability of getting a value of at least 1.
7This is in reference and deference to the famous saying that ‚ÄúIf something can go wrong, it
probably will.‚Äù

Chapter 18
Deviation from the Mean
734
Theorem 18.7.4 (Murphy‚Äôs Law). Let A1, A2, ..., An be mutually independent
events. Let Ti be the indicator random variable for Ai and deÔ¨Åne
T WWD T1 C T2 C    C Tn
to be the number of events that occur. Then
Pr≈íT D 0¬ç  e  Ex≈íT ¬ç:
Proof.
Pr≈íT D 0¬ç D Pr≈íA1 \ A2 \ : : : \ An¬ç
(T D 0 iff no Ai occurs)
D
n
Y
iD1
Pr≈íAi¬ç
(independence of Ai)
D
n
Y
iD1
.1   Pr≈íAi¬ç/

n
Y
iD1
e  Pr≈íAi¬ç
(since 1   x  e x)
D e  Pn
iD1 Pr≈íAi¬ç
D e  Pn
iD1 Ex≈íTi¬ç
(since Ti is an indicator for Ai)
D e  Ex≈íT ¬ç
(linearity of expectation)

For example, given any set of mutually independent events, if you expect 10 of
them to happen, then at least one of them will happen with probability at least 1  e 10. The probability that none of them happen is at most e 10 < 1=22000.
So if there are a lot of independent things that can go wrong and their probabil-
ities sum to a number much greater than 1, then Theorem 18.7.4 proves that some
of them surely will go wrong.
This result can help to explain ‚Äúcoincidences,‚Äù ‚Äúmiracles,‚Äù and crazy events that
seem to have been very unlikely to happen. Such events do happen, in part, because
there are so many possible unlikely events that the sum of their probabilities is
greater than one. For example, someone does win the lottery.
In fact, if there are 100,000 random tickets in Pick-4, Theorem 18.7.4 says that
the probability that there is no winner is less than e 10 < 1=22000. More generally,
there are literally millions of one-in-a-million possible events and so some of them
will surely occur.

18.8. Really Great Expectations
735
18.8
Really Great Expectations
Making independent tosses of a fair coin until some desired pattern comes up is a
simple process you should feel solidly in command of by now, right? So how about
a bet about the simplest such process ‚Äîtossing until a head comes up? Ok, you‚Äôre
wary of betting with us, but how about this: we‚Äôll let you set the odds.
18.8.1
Repeating Yourself
Here‚Äôs the bet: you make independent tosses of a fair coin until a head comes up.
Then you will repeat the process. If a second head comes up in the same or fewer
tosses than the Ô¨Årst, you have to start over yet again. You keep starting over until
you Ô¨Ånally toss a run of tails longer than your Ô¨Årst one. The payment rules are that
you will pay me 1 cent each time you start over. When you win by Ô¨Ånally getting a
run of tails longer than your Ô¨Årst one, I will pay you some generous amount. And
by the way, you‚Äôre certain to win ‚Äîwhatever your initial run of tails happened to
be, a longer run will occur again with probability 1!
For example, if your Ô¨Årst tosses are TTTH, then you will keep tossing until you
get a run of 4 tails. So your winning Ô¨Çips might be
TTTHTHTTHHTTHTHTTTHTHHHTTTT:
In this run there are 10 heads, which means you had to start over 9 times. So you
would have paid me 9 cents by the time you Ô¨Ånally won by tossing 4 tails. Now
you‚Äôve won, and I‚Äôll pay you generously ‚Äîhow does 25 cents sound? Maybe you‚Äôd
rather have $1? How about $10?
Of course there‚Äôs a trap here. Let‚Äôs calculate your expected winnings.
Suppose your initial run of tails had length k. After that, each time a head comes
up, you have to start over and try to get kC1 tails in a row. If we regard your getting
k C 1 tails in a row as a ‚Äúfailed‚Äù try, and regard your having to start over because a
head came up too soon as a ‚Äúsuccessful‚Äù try, then the number of times you have to
start over is the number of tries till the Ô¨Årst failure. So the expected number of tries
will be the mean time to failure, which is 2kC1, because the probability of tossing
k C 1 tails in a row is 2 .kC1/.
Let T be the length of your initial run of tails. So T D k means that your initial
tosses were TkH. Let R be the number of times you repeat trying to beat your
original run of tails. The number of cents you expect to Ô¨Ånish with is the number
of cents in my generous payment minus Ex≈íR¬ç. It‚Äôs now easy to calculate Ex≈íR¬ç by

Chapter 18
Deviation from the Mean
736
conditioning on the value of T :
Ex≈íR¬ç D
X
k2N
Ex≈íR j T D k¬ç  Pr≈íT D k¬ç D
X
k2N
2kC1  2 .kC1/ D
X
k2N
1 D 1:
So you can expect to pay me an inÔ¨Ånite number of cents before winning my
‚Äúgenerous‚Äù payment. No amount of generosity can make this bet fair!
We haven‚Äôt faced inÔ¨Ånite expectations until now, but they just popped up in a
very simple way. In fact this particular example is a special case of an astonish-
ingly general one worked out in Problem 18.29: the expected waiting time for any
random variable to achieve a larger value is inÔ¨Ånite.
18.8.2
The St. Petersburg Paradox
One of the simplest casino bets is on ‚Äúred‚Äù or ‚Äúblack‚Äù at the roulette table. In each
play at roulette, a small ball is set spinning around a roulette wheel until it lands in
a red, black, or green colored slot. The payoff for a bet on red or black matches the
bet; for example, if you bet $10 on red and the ball lands in a red slot, you get back
your original $10 bet plus another matching $10.
In the US, a roulette wheel has two green slots among 18 black and 18 red slots,
so the probability of red is 18=38  0:473. In Europe, where roulette wheels have
only one green slot, the odds for red are a little better ‚Äîthat is, 18=37  0:486
‚Äîbut still less than even.
There is a notorious gambling strategy allegedly used against the casino in St.
Peterburg back in czarist days: bet $10 on red, and keep doubling the bet until a red
comes up. This strategy implies that a player will leave the game as a net winner
of $10 as soon as the red Ô¨Årst appears.
But wait a minute. As long as there is a Ô¨Åxed, positive probability of red ap-
pearing on each spin of the wheel, it‚Äôs certain that red will eventually come up, so
you can be certain of leaving the casino having won $10. Probability theory really
implies that even with the odds heavily against you, you‚Äôre certain to win! This
crazy conclusion is known as the St. Petersburg Paradox.
It‚Äôs tempting to reject any theory that leads to such an absurd conclusion, but
we shouldn‚Äôt fault the theory for reaching an absurd conclusion from an absurd
assumption. We‚Äôve implicitly assumed that it‚Äôs possible to keep doubling your bets.
The problem is that to follow this strategy, you need to have an inÔ¨Ånite bankroll.
To be precise, let L be the number of dollars you need to have in order to keep
betting until the wheel Ô¨Ånally spins red. If red Ô¨Årst comes up on the ith spin, then
L would equal
10.1 C 2 C 4 C    C 2i/ D 10.2iC1   1/

18.8. Really Great Expectations
737
By Total Expectation,
Ex≈íL¬ç D
X
i2ZC
Ex≈íL j 1st red in ith spin¬ç  Pr≈í1st red in ith spin¬ç
D
X
i2ZC
.10  .2iC1   1//  2 i 
X
i2ZC
10 D 1:
That is, you can expect to lose an inÔ¨Ånite amount of money before Ô¨Ånally winning
$10.
On the other hand, it‚Äôs a routine exercise to verify that, if you have only a Ô¨Ånite
amount of money when you start following the bet doubling strategy, then your
expected win comes out sensibly: it will be zero against a fair wheel and be negative
when the wheel is biased against you.
Problems for Section 18.2
Practice Problems
Problem 18.1.
The vast majority of people have an above average number of Ô¨Ångers. Which of
the following statements accounts for this phenomenon? Explain your reasoning.
1. Most people have a super secret extra bonus Ô¨Ånger of which they are un-
aware.
2. A pedantic minority don‚Äôt count their thumbs as Ô¨Ångers, while the majority
of people do.
3. Polydactyly is rarer than amputation.
4. When you add up the total number of Ô¨Ångers among the world‚Äôs population
and then divide by the size of the population, you get a number less than ten.
5. This follows from Markov‚Äôs Theorem, since no one has a negative number
of Ô¨Ångers.
6. Missing Ô¨Ångers are much more common than extra ones.
7. Missing Ô¨Ångers are at least slightly more common than extra ones.
Class Problems
Problem 18.2.
A herd of cows is stricken by an outbreak of cold cow disease. The disease lowers

Chapter 18
Deviation from the Mean
738
the normal body temperature of a cow, and a cow will die if its temperature goes
below 90 degrees F. The disease epidemic is so intense that it lowered the average
temperature of the herd to 85 degrees. Body temperatures as low as 70 degrees, but
no lower, were actually found in the herd.
(a) Prove that at most 3/4 of the cows could have survived.
Hint: Let T be the temperature of a random cow. Make use of Markov‚Äôs bound.
(b) Suppose there are 400 cows in the herd. Show that the bound of part (a) is
the best possible by giving an example set of temperatures for the cows so that the
average herd temperature is 85, and with probability 3/4, a randomly chosen cow
will have a high enough temperature to survive.
Homework Problems
Problem 18.3.
If R is a nonnegative random variable, then Markov‚Äôs Theorem gives an upper
bound on Pr≈íR  x¬ç for any real number x > Ex≈íR¬ç. If b is a lower bound on R,
then Markov‚Äôs Theorem can also be applied to R   b to obtain a possibly different
bound on Pr≈íR  x¬ç.
(a) Show that if b > 0, applying Markov‚Äôs Theorem to R   b gives a smaller
upper bound on Pr≈íR  x¬ç than simply applying Markov‚Äôs Theorem directly to R.
(b) What value of b  0 in part (a) gives the best bound?
Problems for Section 18.4
Practice Problems
Problem 18.4.
Tom has a gambling problem. He plays 240 hands of draw poker, 120 hands of
black jack, and 40 hands of stud poker per day. He wins a hand of draw poker with
probability 1/6, a hand of black jack with probability 1/2, and a hand of stud poker
with probability 1/5.
(a) What is the expected number of hands that Tom wins in a day?
(b) What would the Markov bound be on the probability that Tom will win at least
216 hands on a given day?

18.8. Really Great Expectations
739
(c) Assume the outcomes of the card games are pairwise independent. What is the
variance in the number of hands won per day? You may answer with a numerical
expression that is not completely evaluated.
(d) What would the Chebyshev bound be on the probability that Tom will win at
least 216 hands on a given day? You may answer with a numerical expression that
is not completely evaluated.
Class Problems
Problem 18.5.
The hat-check staff has had a long day serving at a party, and at the end of the party
they simply return the n checked hats in a random way such that the probability
that any particular person gets their own hat back is 1=n.
Let Xi be the indicator variable for the ith person getting their own hat back. Let
Sn be the total number of people who get their own hat back.
(a) What is the expected number of people who get their own hat back?
(b) Write a simple formula for Ex≈íXiXj ¬ç for i ¬§ j.
Hint: What is Pr

Xj D 1 j Xi D 1

?
(c) Explain why you cannot use the variance of sums formula to calculate Var≈íSn¬ç.
(d) Show that Ex≈íS2
n¬ç D 2. Hint: X2
i D Xi.
(e) What is the variance of Sn?
(f) Show that there is at most a 1% chance that more than 10 people get their own
hat back. Try to give an intuitive explanation of why the chance remains this small
regardless of n.

Chapter 18
Deviation from the Mean
740
Problem 18.6.
For any random variable, R, with mean, , and standard deviation, , the Cheby-
shev Bound says that for any real number x > 0,
Pr≈íjR   j  x¬ç 

x
2
:
Show that for any real number, , and real numbers x   > 0, there is an R for
which the Chebyshev Bound is tight, that is,
Pr≈íjRj  x¬ç D

x
2
:
(18.25)
Hint: First assume  D 0 and let R only take values 0;  x; and x.
Problem 18.7. (a) A computer program crashes at the end of each hour of use with
probability 1=p, if it has not crashed already. If H is the number of hours until the
Ô¨Årst crash, we know
Ex≈íH¬ç D 1
p;
Var≈íH¬ç D q
p2 ;
where q WWD 1   p.
(b) What is the Chebyshev bound on
Pr≈íjH   .1=p/j > x=p¬ç
where x > 0?
(c) Conclude from part (b) that for a  2,
Pr≈íH > a=p¬ç 
1   p
.a   1/2
Hint: Check that jH   .1=p/j > .a   1/=p iff H > a=p.
(d) What actually is
Pr≈íH > a=p¬ç‚Äπ
Conclude that for any Ô¨Åxed p > 0, the probability that H > a=p is an asymptoti-
cally smaller function of a than the Chebyshev bound of part (c).

18.8. Really Great Expectations
741
Problem 18.8.
Let R be a nonnegative integer valued random variable.
(a) If Ex≈íR¬ç D 1, how large can Var≈íR¬ç be?
(b) If R is always positive (nonzero), how large can Ex≈í1=R¬ç be?
Homework Problems
Problem 18.9.
A man has a set of n keys, one of which Ô¨Åts the door to his apartment. He tries
the keys until he Ô¨Ånds the correct one. Give the expectation and variance for the
number of trials until success when:
(a) he tries the keys at random (possibly repeating a key tried earlier).
(b) he chooses keys randomly from among those he has not yet tried.
Problem 18.10.
Prove that the expected absolute deviation given in DeÔ¨Ånition 18.4.1 is always less
than or equal to the standard deviation, . (For simplicity, you may assume that R
is deÔ¨Åned on a Ô¨Ånite sample space.)
Hint: Suppose the sample space outcomes are !1; !2; : : : ; !n, and let
p WWD .p1; p2; : : : ; pn/
where pi D
p
Pr≈í!i¬ç;
r WWD .r1; r2; : : : ; rn/
where ri D jR.!i/   j
p
Pr≈í!i¬ç:
As usual, let v  w WWD Pn
iD1 viui denote the dot product of n-vectors v; w, and let
jvj be the norm of v, namely, pv  v.
Then verify that
jpj D 1;
jrj D ;
and
Ex≈í jR   j ¬ç D r  p:
Problem 18.11.
There is a ‚Äúone-sided‚Äù version of Chebyshev‚Äôs bound for deviation above the mean:
Lemma (One-sided Chebyshev bound).
Pr≈íR   Ex≈íR¬ç  x¬ç 
Var≈íR¬ç
x2 C Var≈íR¬ç:
Hint: Let Sa WWD .R   Ex≈íR¬ç C a/2, for 0  a 2 R. So R   Ex≈íR¬ç  x
implies Sa  .x C a/2. Apply Markov‚Äôs bound to Pr≈íSa  .x C a/2¬ç. Choose a to
minimize this last bound.

Chapter 18
Deviation from the Mean
742
Exam Problems
Problem 18.12.
Let Kn be the complete graph with n vertices. Each of the edges of the graph
will be randomly assigned one of the colors red, green, or blue. The assignments
of colors to edges are mutually independent, and the probabilty of an edge being
assigned red is r, blue is b, and green is g (so r C b C g D 1).
A set of three vertices in the graph is called a triangle. A triangle is monochro-
matic if the three edges connecting the vertices are all the same color.
(a) Let m be the probability that any given triangle, T , is monochromatic. Write
a simple formula for m in terms of r; b; and g.
(b) Let IT be the indicator variable for whether T is monochromatic. Write simple
formulas in terms of m; r; b; and g for Ex≈íIT ¬ç and Var≈íIT ¬ç.
Ex≈íIT ¬ç D
Var≈íIT ¬ç D
Now assume r D b D g D 1
3.
Let T and U be distinct triangles.
(c) What is the probability that T and U are both monochromatic?
(d) Show that IT and IU are independent random variables.
(e) Let M be the number of monochromatic triangles. Write simple formulas in
terms of n; m; r; b; and g for Ex≈íM¬ç and Var≈íM¬ç.
Ex≈íM¬ç D
Var≈íM¬ç D
(f) Let  WWD Ex≈íM¬ç. Prove that
Pr
h
jM   j >
p
 log 
i
D O

1
log n


18.8. Really Great Expectations
743
Problems for Section 18.6
Class Problems
Problem 18.13.
A recent Gallup poll found that 35% of the adult population of the United States
believes that the theory of evolution is ‚Äúwell-supported by the evidence.‚Äù Gallup
polled 1928 Americans selected uniformly and independently at random. Of these,
675 asserted belief in evolution, leading to Gallup‚Äôs estimate that the fraction of
Americans who believe in evolution is 675=1928  0:350. Gallup claims a margin
of error of 3 percentage points, that is, he claims to be conÔ¨Ådent that his estimate is
within 0.03 of the actual percentage.
(a) What is the largest variance an indicator variable can have?
(b) Use the Pairwise Independent Sampling Theorem to determine a conÔ¨Ådence
level with which Gallup can make his claim.
(c) Gallup actually claims greater than 99% conÔ¨Ådence in his estimate. How
might he have arrived at this conclusion? (Just explain what quantity he could
calculate; you do not need to carry out a calculation.)
(d) Accepting the accuracy of all of Gallup‚Äôs polling data and calculations, can
you conclude that there is a high probability that the number of adult Americans
who believe in evolution is 35 Àô 3 percent?
Problem 18.14.
Let B1; B2; : : : ; Bn be mutually independent random variables with a uniform
distribution on the integer interval ≈í1; d¬ç. Let D equal to the number of events
≈íBi D Bj ¬ç that happen where i ¬§ j. It was observed in Section 16.6.6 (and
proved in Problem 17.2) that Pr≈íBi D Bj ¬ç D 1=d for i ¬§ j and that the events
≈íBi D Bj ¬ç are pairwise independent.
Let Ei;j be the indicator variable for the event ≈íBi D Bj ¬ç.
(a) What are Ex≈íEi;j ¬ç and Var≈íEi;j ¬ç for i ¬§ j?
(b) What are Ex≈íD¬ç and Var≈íD¬ç?
(c) In a 6.01 class of 500 students, the youngest student was born 15 years ago
and the oldest 35 years ago. Let D be the number of students in the class who were
born on exactly the same date. What is the probability that 4  D  32? (For
simplicity, assume that the distribution of birthdays is uniform over the 7305 days
in the two decade interval from 35 years ago to 15 years ago.)

Chapter 18
Deviation from the Mean
744
Problem 18.15.
A defendent in trafÔ¨Åc court is trying to beat a speeding ticket on the grounds that‚Äî
since virtually everybody speeds on the turnpike‚Äîthe police have unconstitutional
discretion in giving tickets to anyone they choose. (By the way, we don‚Äôt recom-
mend this defense :-).)
To support his argument, the defendent arranged to get a random sample of trips
by 3,125 cars on the turnpike and found that 94% of them broke the speed limit
at some point during their trip. He says that as a consequence of sampling theory
(in particular, the Pairwise Independent Sampling Theorem), the court can be 95%
conÔ¨Ådent that the actual percentage of all cars that were speeding is 94 Àô 4%.
The judge observes that the actual number of car trips on the turnpike was never
considered in making this estimate. He is skeptical that, whether there were a
thousand, a million, or 100,000,000 car trips on the turnpike, sampling only 3,125
is sufÔ¨Åcient to be so conÔ¨Ådent.
Suppose you were were the defendent. How would you explain to the judge
why the number of randomly selected cars that have to be checked for speeding
does not depend on the number of recorded trips? Remember that judges are not
trained to understand formulas, so you have to provide an intuitive, nonquantitative
explanation.
Problem 18.16.
The proof of the Pairwise Independent Sampling Theorem 18.5.1 was given for
a sequence R1; R2; : : : of pairwise independent random variables with the same
mean and variance.
The theorem generalizes straighforwardly to sequences of pairwise independent
random variables, possibly with different distributions, as long as all their variances
are bounded by some constant.
Theorem (Generalized Pairwise Independent Sampling). Let X1; X2; : : : be a se-
quence of pairwise independent random variables such that Var≈íXi¬ç  b for some
b  0 and all i  1. Let
An WWD X1 C X2 C    C Xn
n
;
n WWD Ex≈íAn¬ç:
Then for every  > 0,
Pr≈íjAn   nj > ¬ç  b
2  1
n:
(18.26)
(a) Prove the Generalized Pairwise Independent Sampling Theorem.

18.8. Really Great Expectations
745
(b) Conclude that the following holds:
Corollary (Generalized Weak Law of Large Numbers). For every  > 0,
lim
n!1 Pr≈íjAn   nj  ¬ç D 1:
Problem 18.17.
An International Journal of Epidemiology has a policy of publishing papers about
drug trial results only if the conclusion about the drug‚Äôs effectiveness (or lack
thereof) holds at the 95% conÔ¨Ådence level. The editors and reviewers carefully
check that any trial whose results they publish was properly performed and accu-
rately reported. They are also careful to check that trials whose results they publish
have been conducted independently of each other.
The editors of the Journal reason that under this policy, their readership can be
conÔ¨Ådent that at most 5% of the published studies will be mistaken. Later, the
editors are embarrassed ‚Äîand astonished ‚Äîto learn that every one of the 20 drug
trial results they published during the year was wrong. The editors thought that
because the trials were conducted independently, the probability of publishing 20
wrong results was negligible, namely, .1=20/20 < 10 25.
Write a brief explanation to these befuddled editors explaining what‚Äôs wrong
with their reasoning and how it could be that all 20 published studies were wrong.
Hint: xkcd comic: ‚ÄúsigniÔ¨Åcant‚Äù
Exam Problems
Problem 18.18.
You work for the president and you want to estimate the fraction p of voters in the
entire nation that will prefer him in the upcoming elections. You do this by random
sampling. SpeciÔ¨Åcally, you select a random voter and ask them who they are going
to vote for. You do this n times, with each voter selected with uniform probability
and independently of other selections. Finally, you use the fraction P of voters
who said they will vote for the President as an estimate for p.
(a) Our theorems about sampling and distributions allow us to calculate how con-
Ô¨Ådent we can be that the random variable, P , takes a value near the constant, p.
This calculation uses some facts about voters and the way they are chosen. Circle
the true facts among the following:
1. Given a particular voter, the probability of that voter preferring the President
is p.

Chapter 18
Deviation from the Mean
746
2. The probability that some voter is chosen more than once in the random sam-
ple goes to one as n increases.
3. The probability that some voter is chosen more than once in the random sam-
ple goes to zero as the population of voters grows.
4. All voters are equally likely to be selected as the third in the random sample
of n voters (assuming n  3).
5. The probability that the second voter in the random sample will favor the
President, given that the Ô¨Årst voter prefers the President, is greater than p.
6. The probability that the second voter in the random sample will favor the
President, given that the second voter is from the same state as the Ô¨Årst, may
not equal p.
(b) Suppose that according to your calculations, the following is true about your
polling:
Pr≈íjP   pj  0:04¬ç  0:95:
You do the asking, you count how many said they will vote for the President, you
divide by n, and Ô¨Ånd the fraction is 0.53. Among the following, circle the legitimate
things you might say in a call to the President:
1. Mr. President, p D 0:53!
2. Mr. President, with probability at least 95 percent, p is within 0.04 of 0.53.
3. Mr. President, either p is within 0.04 of 0.53 or something very strange (5-
in-100) has happened.
4. Mr. President, we can be 95% conÔ¨Ådent that p is within 0.04 of 0.53.
Problem 18.19.
Yesterday, the programmers at a local company wrote a large program. To estimate
the fraction, b, of lines of code in this program that are buggy, the QA team will
take a small sample of lines chosen randomly and independently (so it is possible,
though unlikely, that the same line of code might be chosen more than once). For
each line chosen, they can run tests that determine whether that line of code is
buggy, after which they will use the fraction of buggy lines in their sample as their
estimate of the fraction b.
The company statistician can use estimates of a binomial distribution to calculate
a value, s, for a number of lines of code to sample which ensures that with 97%
conÔ¨Ådence, the fraction of buggy lines in the sample will be within 0.006 of the
actual fraction, b, of buggy lines in the program.

18.8. Really Great Expectations
747
Mathematically, the program is an actual outcome that already happened. The
random sample is a random variable deÔ¨Åned by the process for randomly choosing
s lines from the program. The justiÔ¨Åcation for the statistician‚Äôs conÔ¨Ådence depends
on some properties of the program and how the random sample of s lines of code
from the program are chosen. These properties are described in some of the state-
ments below. Indicate which of these statements are true, and explain your answers.
1. The probability that the ninth line of code in the program is buggy is b.
2. The probability that the ninth line of code chosen for the random sample is
defective, is b.
3. All lines of code in the program are equally likely to be the third line chosen
in the random sample.
4. Given that the Ô¨Årst line chosen for the random sample is buggy, the probabil-
ity that the second line chosen will also be buggy is greater than b.
5. Given that the last line in the program is buggy, the probability that the next-
to-last line in the program will also be buggy is greater than b.
6. The expectation of the indicator variable for the last line in the random sam-
ple being buggy is b.
7. Given that the Ô¨Årst two lines of code selected in the random sample are the
same kind of statement ‚Äîthey might both be assignment statements, or both
be conditional statements, or both loop statements,...‚Äîthe probability that
the Ô¨Årst line is buggy may be greater than b.
8. There is zero probability that all the lines in the random sample will be dif-
ferent.
Problem 18.20.
Let G1; G2; G3; : : : ; be an inÔ¨Ånite sequence of pairwise independent random vari-
ables with the same expectation, , and the same Ô¨Ånite variance. Let
f .n; / WWD Pr
 ÀáÀáÀáÀá
Pn
iD1 Gi
n
  
ÀáÀáÀáÀá  

:
The Weak Law of Large Numbers can be expressed as a logical formula of the
form:
8 > 0 Q1 Q2 : : : ≈íf .n; /  1   ƒ±¬ç

Chapter 18
Deviation from the Mean
748
where Q1Q2 : : : is a sequence of quantiÔ¨Åers from among:
8n
9n
8n0
9n0
8n  n0
9n  n0
8ƒ± > 0
9ƒ± > 0
8ƒ±  0
9ƒ±  0
Here the n and n0 range over nonnegative integers, and ƒ± and  range over real
numbers.
Write out the proper sequence Q1Q2 : : :
Problems for Section 18.7
Practice Problems
Problem 18.21.
A gambler plays 120 hands of draw poker, 60 hands of black jack, and 20 hands of
stud poker per day. He wins a hand of draw poker with probability 1/6, a hand of
black jack with probability 1/2, and a hand of stud poker with probability 1/5.
(a) What is the expected number of hands the gambler wins in a day?
(b) What would the Markov bound be on the probability that the gambler will win
at least 108 hands on a given day?
(c) Assume the outcomes of the card games are pairwise, but possibly not mutu-
ally, independent. What is the variance in the number of hands won per day? You
may answer with a numerical expression that is not completely evaluated.
(d) What would the Chebyshev bound be on the probability that the gambler will
win at least 108 hands on a given day? You may answer with a numerical expres-
sion that is not completely evaluated.
(e) Assuming outcomes of the card games are mutually independent, show that
the probability that the gambler will win at least 108 hands on a given day is much
smaller than the bound in part (d). Hint: e1 2 ln 2  0:7

18.8. Really Great Expectations
749
Class Problems
Problem 18.22.
We want to store 2 billion records into a hash table that has 1 billion slots. Assum-
ing the records are randomly and independently chosen with uniform probability
of being assigned to each slot, two records are expected to be stored in each slot.
Of course under a random assignment, some slots may be assigned more than two
records.
(a) Show that the probability that a given slot gets assigned more than 23 records
is less than e 36.
Hint: Use Chernoff‚Äôs Bound, Theorem 18.7.1,. Note that Àá.12/ > 18, where
Àá.c/ WWD c ln c   c C 1.
(b) Show that the probability that there is a slot that gets assigned more than 23
records is less than e 15, which is less than 1=3; 000; 000. Hint: 109 < e21; use
part (a).
Problem 18.23.
Sometimes I forget a few items when I leave the house in the morning. For example,
here are probabilities that I forget various pieces of footwear:
left sock
0:2
right sock
0:1
left shoe
0:1
right shoe
0:3
(a) Let X be the number of these that I forget. What is Ex≈íX¬ç?
(b) Give a tight upper bound on the probability that I forget one or more items
when no independence assumption is made about forgetting different items.
(c) Use the Markov Bound to derive an upper bound on the probability that I
forget 3 or more items.
(d) Now suppose that I forget each item of footwear independently.
Use the
Chebyshev Bound to derive an upper bound on the probability that I forget two
or more items.
(e) Use Murphy‚Äôs Law, Theorem 18.7.4, to derive a lower bound on the probabil-
ity that I forget one or more items.

Chapter 18
Deviation from the Mean
750
(f) I‚Äôm supposed to remember many other items, of course: clothing, watch, back-
pack, notebook, pencil, kleenex, ID, keys, etc. Let X be the total number of items
I remember. Suppose I remember items mutually independently and Ex≈íX¬ç D 36.
Use Chernoff‚Äôs Bound to give an upper bound on the probability that I remember
48 or more items.
(g) Give an upper bound on the probability that I remember 108 or more items.
Problem 18.24.
Reasoning based on the Chernoff bound goes a long way in explaining the recent
subprime mortgage collapse. A bit of standard vocabulary about the mortgage
market is needed:
 A loan is money lent to a borrower. If the borrower does not pay on the
loan, the loan is said to be in default, and collateral is seized. In the case of
mortgage loans, the borrower‚Äôs home is used as collateral.
 A bond is a collection of loans, packaged into one entity. A bond can be
divided into tranches, in some ordering, which tell us how to assign losses
from defaults. Suppose a bond contains 1000 loans, and is divided into 10
tranches of 100 bonds each. Then, all the defaults must Ô¨Åll up the lowest
tranche before the affect others. For example, suppose 150 defaults hap-
pened. Then, the Ô¨Årst 100 defaults would occur in tranche 1, and the next 50
defaults would happen in tranche 2.
 The lowest tranche of a bond is called the mezzanine tranche.
 We can make a ‚Äúsuper bond‚Äù of tranches called a collateralized debt obli-
gation (CDO) by collecting mezzanine tranches from different bonds. This
super bond can then be itself separated into tranches, which are again ordered
to indicate how to assign losses.
(a) Suppose that 1000 loans make up a bond, and the fail rate is 5% in a year.
Assuming mutual independence, give an upper bound for the probability that there
are one or more failures in the second-worst tranche. What is the probability that
there are failures in the best Tranche?
(b) Now, do not assume that the loans are independent. Give an upper bound for
the probability that there are one or more failures in the second tranche. What is an
upper bound for the probability that the entire bond defaults? Show that it is a tight
bound. Hint: Use Markov‚Äôs theorem.

18.8. Really Great Expectations
751
(c) Given this setup (and assuming mutual independence between the loans), what
is the expected failure rate in the mezzanine tranche?
(d) We take the mezzanine tranches from 100 bonds and create a CDO. What is
the expected number of underlying failures to hit the CDO?
(e) We divide this CDO into 10 tranches of 1000 bonds each. Assuming mutual
independence, give an upper bound on the probability of one or more failures in the
best tranche. The third tranche?
(f) Repeat the previous question without the assumption of mutual independence.
Homework Problems
Problem 18.25.
An inÔ¨Ånite version of Murphy‚Äôs Law is that if an inÔ¨Ånite number of mutually inde-
pendent events are expected to happen, then the probability that only Ô¨Ånitely many
happen is 0. This is known as the Ô¨Årst Borel-Cantelli Lemma.
(a) Let A0; A1; : : : be any inÔ¨Ånite sequence of mutually independent events such
that
X
n2N
Pr≈íAn¬ç D 1:
(18.27)
Prove that Pr≈íno An occurs¬ç D 0.
Hint: Bk the event that no An with n  k occurs. So the event that no An occurs is
B WWD
\
k2N
Bk:
Apply Murphy‚Äôs Law, Theorem 18.7.4, to Bk.
(b) Conclude that Pr≈íonly Ô¨Ånitely many An‚Äôs occur¬ç D 0.
Hint: Let Ck be the event that no An with n  k occurs. So the event that only
Ô¨Ånitely many An‚Äôs occur is
C WWD
[
k2N
Ck:
Apply part (a) to Ck.

Chapter 18
Deviation from the Mean
752
Problems for Section 18.8
Practice Problems
Problem 18.26.
Let R be a positive integer valued random variable such that
PDFR.n/ D
1
cn3 ;
where
c WWD
1
X
nD1
1
n3 :
(a) Prove that Ex≈íR¬ç is Ô¨Ånite.
(b) Prove that Var≈íR¬ç is inÔ¨Ånite.
A joking way to phrase the point of this example is ‚ÄúThe square root of inÔ¨Ånity
may be Ô¨Ånite.‚Äù Namely, let T WWD R2. Then part (b) implies that Ex≈íT ¬ç D 1 while
Ex≈í
p
T ¬ç < 1 by (a).
Problem 18.27.
Let T be a positive integer valued random variable such that
PDFT .n/ D
1
an2 ;
where
a WWD
X
n2ZC
1
n2 :
(a) Prove that Ex≈íT ¬ç is inÔ¨Ånite.
(b) Prove that Ex≈í
p
T ¬ç is Ô¨Ånite.
Note that if we deÔ¨Åne R D
p
T , then R has Ô¨Ånite expectation, but the variance of
R is inÔ¨Ånite.
Class Problems
Problem 18.28.
You have a biased coin with nonzero probability p < 1 of tossing a Head. You toss
until a Head comes up and record the number, k, of Tails that preceded this Ô¨Årst
Head. Then, similar the example in Section 18.8, you keep tossing until you get

18.8. Really Great Expectations
753
another run of tails of nearly the same length, namely, of length minfk   10; 0g.
Prove that the expected number of Heads you toss is inÔ¨Ånite.
Problem 18.29.
Let T0; T1; : : : be a sequence of mutually independent random variables with the
same distribution. Let
R WWD minfk > 0 j Tk > T0g:
(a) Suppose the range of the T0 is the set ft0 < t1 < t2 <   g. Explain why the
following Theorem implies that Ex≈íR¬ç D 1.
Theorem. If p0 C p1 C p2 C    D 1 and all pi  0, then the sum
 WWD
X
k2N
pk
pkC1 C pkC2 C   :
diverges.
(b) Let
Sk WWD pk C pkC1 C : : : ;
and
ak WWD
Sk
SkC1
  1:
Prove that
 D
X
k2N
ak:
(18.28)
(c) Prove that
Y
kn
.ak C 1/ D
1
SnC1
:
(d) Conclude from part (c) that
Y
k2N
.ak C 1/ D 1:
(18.29)
(e) Conclude that e D 1 and hence  D 1.

Chapter 18
Deviation from the Mean
754
Exam Problems
Problem 18.30.
You have a process for generating a positive integer, K. The behavior of your
process each time you use it is (mutually) independent of all its other uses. You use
your process to generate a random integer, and then use your procedure repeatedly
until you generate an integer as big as your Ô¨Årst one. Let R be the number of
additional integers you have to generate.
(a) State and brieÔ¨Çy explain a simple closed formula for Ex≈íR j K D k¬ç in terms
of Pr≈íK  k¬ç.
Suppose Pr≈íK D k¬ç D ‚Äö.k 4/.
(b) Show that Pr≈íK  k¬ç D ‚Äö.k 3/.
(c) Show that Ex≈íR¬ç is inÔ¨Ånite.
Problem 18.31.
A gambler bets $10 on ‚Äúred‚Äù at a roulette table (the odds of red are 18/38, slightly
less than even) to win $10. If he wins, he gets back twice the amount of his bet,
and he quits. Otherwise, he doubles his previous bet and continues.
For example, if he loses his Ô¨Årst two bets but wins his third bet, the total spent
on his three bets is 10 C 20 C 40 dollars, but he gets back 2  40 dollars after his
win on the third bet, for a net proÔ¨Åt of$10.
(a) What is the expected number of bets the gambler makes before he wins?
(b) What is his probability of winning?
(c) What is his expected Ô¨Ånal proÔ¨Åt (amount won minus amount lost)?
(d) The fact that the gambler‚Äôs expected proÔ¨Åt is positive, despite the fact that the
game is biased against him, is known as the St. Petersburg paradox. The paradox
is explained by the fact that bet doubling is not a feasible strategy: prove that the
expected size of the gambler‚Äôs last bet is inÔ¨Ånite.

19
Random Processes
Random Walks are used to model situations in which an object moves in a sequence
of steps in randomly chosen directions. For example in Physics, three-dimensional
random walks are used to model Brownian motion and gas diffusion. In this chapter
we‚Äôll examine two examples of random walks. First, we‚Äôll model gambling as
a simple 1-dimensional random walk ‚Äîa walk along a straight line. Then we‚Äôll
explain how the Google search engine used random walks through the graph of
world-wide web links to determine the relative importance of websites.
19.1
Gamblers‚Äô Ruin
Suppose a gambler starts with an initial stake of n dollars and makes a sequence of
$1 bets. If he wins an individual bet, he gets his money back plus another $1. If he
loses the bet, he loses the $1.
We can model this scenario as a random walk between integer points on the real
line. The position on the line at any time corresponds to the gambler‚Äôs cash-on-
hand or capital. Walking one step to the right corresponds to winning a $1 bet
and thereby increasing his capital by $1. Similarly, walking one step to the left
corresponds to losing a $1 bet.
The gambler plays until either he runs out of money or increases his capital to a
target amount of T dollars. The amount T   n is deÔ¨Åned to be his intended proÔ¨Åt.
If he reaches his target, then he is called an overall winner, and he will have won
his intended proÔ¨Åt. If his capital reaches zero dollars before reaching his target,
then we say that he is ‚Äúruined‚Äù or goes broke, and he will have lost n dollars. We‚Äôll
assume that the gambler has the same probability, p, of winning each individual
$1 bet and that the bets are mutually independent. We‚Äôd like to Ô¨Ånd the probability
that the gambler wins.
The gambler‚Äôs situation as he proceeds with his $1 bets is illustrated in Fig-
ure 19.1. The random walk has boundaries at 0 and T . If the random walk ever
reaches either of these boundary values, then it terminates.
In a fair game, the gambler is equally likely to win or lose each bet, that is
p D 1=2. The corresponding random walk is called unbiased. The gambler is more
likely to win if p > 1=2 and less likely to win if p < 1=2; these random walks
are called biased. We want to determine the probability that the walk terminates
at boundary T , namely, the probability that the gambler wins. We‚Äôll do this in

Chapter 19
Random Processes
756
capital
gambler‚Äôs
n
T = n + m
time
bet outcomes:
WLLWLWWLLL
Figure 19.1
A graph of the gambler‚Äôs capital versus time for one possible se-
quence of bet outcomes. At each time step, the graph goes up with probabil-
ity p and down with probability 1   p. The gambler continues betting until the
graph reaches either 0 or T . If he starts with $n, his intended proÔ¨Åt is $m where
T D n C m.
Section 19.1.1, but before we derive the probability, let‚Äôs just look at what it turns
out to be.
Let‚Äôs begin by supposing the coin is fair, the gambler starts with 100 dollars, and
he wants to double his money. That is, he plays until he goes broke or reaches a
target of 200 dollars. Since he starts equidistant from his target and bankruptcy, it‚Äôs
clear by symmetry that his probability of winning in this case is 1/2.
We‚Äôll show below that starting with n dollars and aiming for a target of T  n
dollars, the probability the gambler reaches his target before going broke is n=T .
For example, suppose he wants to win the same $100, but instead starts out with
$500. Now his chances are pretty good: the probability of his making the 100
dollars is 5=6. And if he started with one million dollars still aiming to win $100
dollars he almost certain to win: the probability is 1M=.1M C 100/ > :9999.
So in the fair game, the larger the initial stake relative to the target, the higher
the probability the gambler will win, which makes some intuitive sense. But note
that although the gambler now wins nearly all the time, the game is still fair. When
he wins, he only wins $100; when he loses, he loses big: $1M. So the gambler‚Äôs
average win is actually zero dollars.
Another way to describe this scenario is as a game between two players. Say
Albert starts with $500, and Eric starts with $100. They Ô¨Çip a fair coin, and every

19.1. Gamblers‚Äô Ruin
757
time a Head appears, Albert wins $1 from Eric, and vice versa for Tails. They
play this game until one person goes bankrupt. This problem is identical to the
Gambler‚Äôs Ruin problem with n D 500 and T D 100 C 500 D 600. So the
probability of Albert winning is 500=600 D 5=6.
Now suppose instead that the gambler chooses to play roulette in an American
casino, always betting $1 on red. This game is slightly biased against the gambler:
the probability of winning a single bet is p D 18=38  0:47. (It‚Äôs the two green
numbers that slightly bias the bets and give the casino an edge.) Still, the bets
are almost fair, and you might expect that starting with $500, the gambler has a
reasonable chance of winning $100 ‚Äîthe 5/6 probability of winning in the unbiased
game surely gets reduced, but perhaps not too drastically.
Not so! The gambler‚Äôs odds of winning $100 making one dollar bets against the
‚Äúslightly‚Äù unfair roulette wheel are less than 1 in 37,000. If that seems surpris-
ing, listen to this: no matter how much money the gambler has to start ‚Äî$5000,
$50,000, $5  1012 ‚Äîhis odds are still less than 1 in 37,000 of winning a mere 100
dollars!
Moral: Don‚Äôt play!
The theory of random walks is Ô¨Ålled with such fascinating and counter-intuitive
conclusions.
19.1.1
The Probability of Avoiding Ruin
We will determine the probability that the gambler wins using an idea of Pascal‚Äôs
dating back to the beginnings of the subject of probability.
Pascal viewed the walk as a two-player game between Albert and Eric as de-
scribed above. Albert starts with a stack of n chips and Eric starts with a stack of
m D T   n chips. At each bet, Albert wins Eric‚Äôs top chip with probabillity p and
loses his top chip to Eric with probabillity q WWD 1   p. They play this game until
one person goes bankrupt.
Pascal‚Äôs ingenious idea was to alter the value of the chips to make the game fair.
Namely, Albert‚Äôs bottom chip will be given payoff value r where r WWD q=p, and
the successive chips up his stack will be worth r2; r3; : : : up to his top chip with
payoff value rn. Eric‚Äôs top chip will be worth rnC1 and the successive chips down
his stack will be worth rnC2; rnC3; : : : down to his bottom chip worth rnCm.
Now the expected change in Albert‚Äôs chip values on the Ô¨Årst bet is
rnC1  p   rn  q D

rn  q
p

 p   rn  q D 0;
so this payoff makes the bet fair. Moreover, whether Albert wins or loses the bet,
the successive chip values counting up Albert‚Äôs stack and then down Eric‚Äôs remain

Chapter 19
Random Processes
758
r; r2; : : : ; rn; : : : ; rnCm, ensuring by the same reasoning that every bet payoff re-
mains fair. So Albert‚Äôs expected payoff at the end of the game is the sum of the
expectations of his payoffs of each bet, namely 0. Here we‚Äôre legitimately appeal-
ing to inÔ¨Ånite linearity, since the payoff amounts remain bounded independent of
the number of bets.
When Albert wins all of Eric‚Äôs chips his total payoff gain is PnCm
iDnC1 ri, and
when he loses all his chips to Eric, his total payoff loss is Pn
iD1 ri. Letting wn be
Albert‚Äôs probability of winning, we now have
0 D Ex≈íAlbert‚Äôs payoff¬ç D
 nCm
X
iDnC1
ri
!
 wn   n
X
iD1
ri
!
 .1   wn/:
In the truly fair game when r D 1, we have 0 D mwn   n.1   wn/, so wn D
n=.n C m/, as claimed above.
In the biased game with r ¬§ 1, we have
0 D r  rnCm   rn
r   1
 wn   r  rn   1
r   1  .1   wn/:
Solving for wn gives
wn D
rn   1
rnCm   1 D rn   1
rT   1
(19.1)
We have now proved
Theorem 19.1.1. In the Gambler‚Äôs Ruin game with initial capital, n, target, T , and
probability p of winning each individual bet,
Pr≈íthe gambler wins¬ç D
8
ÀÜÀÜÀÜ<
ÀÜÀÜÀÜ:
n
T
for p D 1
2;
rn   1
rT   1
for p ¬§ 1
2;
(19.2)
where r WWD q=p.
19.1.2
A Recurrence for the Probability of Winning
Pascal was obviously a clever fellow, but fortunately for the rest of us less ingenious
folks, linear recurrences offer a methodical, if less inspiring, approach to Gambler‚Äôs
Ruin.
The probability that the gambler wins is a function of his initial capital, n, his
target, T  n, and the probability, p, that he wins an individual one dollar bet.

19.1. Gamblers‚Äô Ruin
759
For Ô¨Åxed p and T , let wn be the gambler‚Äôs probability of winning when his initial
capital is n dollars. For example, w0 is the probability that the gambler will win
given that he starts off broke and wT is the probability he will win if he starts off
with his target amount, so clearly
w0 D 0;
(19.3)
wT D 1:
(19.4)
Otherwise, the gambler starts with n dollars, where 0 < n < T . Now suppose
the gambler wins his Ô¨Årst bet. In this case, he is left with nC1 dollars and becomes
a winner with probability wnC1. On the other hand, if he loses the Ô¨Årst bet, he is
left with n   1 dollars and becomes a winner with probability wn 1. By the Total
Probability Rule, he wins with probability wn D pwnC1 C qwn 1. Solving for
wnC1 we have
wnC1 D wn
p   rwn 1
(19.5)
where r is q=p as in section 19.1.1.
This recurrence holds only for n C 1  T , but there‚Äôs no harm in using (19.5) to
deÔ¨Åne wnC1 for all n C 1 > 1. Now, letting
W.x/ WWD w0 C w1x C w2x2 C   
be the generating function for the wn, we derive from (19.5) and (19.3) using our
generating function methods that
W.x/ D
w1x
rx2   x=p C 1:
(19.6)
But it‚Äôs easy to check that the denominator factors:
rx2   x
p C 1 D .1   x/.1   rx/:
Now if p ¬§ q, then using partial fractions we conclude that
W.x/ D
A
1   x C
B
1   rx ;
(19.7)
for some constants A; B. To solve for A; B, note that by (19.6) and (19.7),
w1x D A.1   rx/ C B.1   x/;
so letting x D 1, we get A D w1=.1   r/, and letting x D 1=r, we get B D
w1=.r   1/. Therefore,
W.x/ D
w1
r   1

1
1   rx  1
1   x

;

Chapter 19
Random Processes
760
which implies
wn D w1
rn   1
r   1 :
(19.8)
Finally, we can use (19.8) to solve for w1 by letting n D T to get
w1 D r   1
rT   1:
Plugging this value of w1 into (19.8), we arrive at the solution:
wn D rn   1
rT   1;
matching Pascal‚Äôs result (19.1).
In the unbiased case where p D q, we get from (19.6) that
W.x/ D
w1x
.1   x/2 ;
and again can use partial fractions to match Pascal‚Äôs result (19.2).
A simpler expression for the biased case
The expression (19.1) for the probability that the Gambler wins in the biased game
is a little hard to interpret. There is a simpler upper bound which is nearly tight
when the gambler‚Äôs starting capital is large and the game is biased against the
gambler. Then r > 1, both the numerator and denominator in (19.1) are positive,
and the numerator is smaller. This implies that
wn < rn
rT D
1
r
T  n
and gives:
Corollary 19.1.2. In the Gambler‚Äôs Ruin game with initial capital, n, target, T ,
and probability p < 1=2 of winning each individual bet,
Pr≈íthe gambler wins¬ç <
1
r
T  n
(19.9)
where r WWD q=p > 1.
So the gambler gains his intended proÔ¨Åt before going broke with probability at
most 1=r raised to the intended proÔ¨Åt power. Notice that this upper bound does
not depend on the gambler‚Äôs starting capital, but only on his intended proÔ¨Åt. This

19.1. Gamblers‚Äô Ruin
761
has the amazing consequence we announced above: no matter how much money he
starts with, if he makes $1 bets on red in roulette aiming to win $100, the probability
that he wins is less than
18=38
20=38
100
D
 9
10
100
<
1
37; 648:
The bound (19.9) decreases exponentially with the intended proÔ¨Åt. So, for ex-
ample, doubling his intended proÔ¨Åt will square his probability of winning. In par-
ticular, the probability that the gambler‚Äôs stake goes up 200 dollars before he goes
broke playing roulette is at most
.9=10/200 D ..9=10/100/2 <

1
37; 648
2
;
which is about 1 in 1.4 billion.
19.1.3
Intuition
Why is the gambler so unlikely to make money when the game is slightly biased
against him? Intuitively, there are two forces at work. First, the gambler‚Äôs capital
has random upward and downward swings due to runs of good and bad luck. Sec-
ond, the gambler‚Äôs capital will have a steady, downward drift, because the negative
bias means an average loss of a few cents on each $1 bet. The situation is shown in
Figure 19.2.
Our intuition is that if the gambler starts with, say, a billion dollars, then he is
sure to play for a very long time, so at some point there should be a lucky, upward
swing that puts him $100 ahead. The problem is that his capital is steadily drifting
downward. If the gambler does not have a lucky, upward swing early on, then he is
doomed. After his capital drifts downward a few hundred dollars, he needs a huge
upward swing to save himself. And such a huge swing is extremely improbable.
As a rule of thumb, drift dominates swings in the long term.
We can quantify these drifts and swings. After k rounds for k  min.m; n/, the
number of wins by our player has a binomial distribution with parameters p < 1=2
and k. His expected win on any single bet is p   q D 2p   1 dollars, so his
expected capital is n   k.1   2p/. Now to be a winner, his actual number of wins
must exceed the expected number by m C k.1   2p/. But we saw before that
the binomial distribution has a standard deviation of only
p
kp.1   p/. So for the
gambler to win, he needs his number of wins to deviate by
m C k.1   2p/
p
kp.1   2p/
D ‚Äö.
p
k/

Chapter 19
Random Processes
762
w
n
0
downward
drift
gambler‚Äôs
wealth
time
upward
swing
(too late)
Figure 19.2
In a biased random walk, the downward drift usually dominates
swings of good luck.
times its standard deviation. In our study of binomial tails, we saw that this was
extremely unlikely.
In a fair game, there is no drift; swings are the only effect. In the absence of
downward drift, our earlier intuition is correct. If the gambler starts with a trillion
dollars then almost certainly there will eventually be a lucky swing that puts him
$100 ahead.
19.1.4
How Long a Walk?
Now that we know the probability, wn, that the gambler is a winner in both fair and
unfair games, we consider how many bets he needs on average to either win or go
broke. A linear recurrence approach works here as well.
For Ô¨Åxed p and T , let en be the expected number of bets until the game ends
when the gambler‚Äôs initial capital is n dollars. Since the game is over in zero steps
if n D 0 or T , the boundary conditions this time are e0 D eT D 0.
Otherwise, the gambler starts with n dollars, where 0 < n < T . Now by the
conditional expectation rule, the expected number of steps can be broken down
into the expected number of steps given the outcome of the Ô¨Årst bet weighted by
the probability of that outcome. But after the gambler wins the Ô¨Årst bet, his capital
is n C 1, so he can expect to make another enC1 bets. That is,
Ex≈íen j gambler wins Ô¨Årst bet¬ç D 1 C enC1:
Similarly, after the gambler loses his Ô¨Årst bet, he can expect to make another en 1

19.1. Gamblers‚Äô Ruin
763
bets:
Ex≈íen j gambler loses Ô¨Årst bet¬ç D 1 C en 1:
So we have
en D p Ex≈íen j gambler wins Ô¨Årst bet¬ç C q Ex≈íen j gambler loses Ô¨Årst bet¬ç
D p.1 C enC1/ C q.1 C en 1/ D penC1 C qen 1 C 1:
This yields the linear recurrence
enC1 D 1
pen   q
pen 1   1
p:
(19.10)
The routine solution of this linear recurrence yields:
Theorem 19.1.3. In the Gambler‚Äôs Ruin game with initial capital n, target T , and
probability p of winning each bet,
Ex≈ínumber of bets¬ç D
8
ÀÜÀÜÀÜÀÜ<
ÀÜÀÜÀÜÀÜ:
n.T   n/
for p D 1
2;
rn 1
rT  1  T   n
p   q
for p ¬§ 1
2:
(19.11)
In the unbiased case, (19.11) can be rephrased simply as
Ex≈ínumber of fair bets¬ç D initial capital  intended proÔ¨Åt:
(19.12)
For example, if the gambler starts with $10 dollars and plays until he is broke or
ahead $10, then 10  10 D 100 bets are required on average. If he starts with $500
and plays until he is broke or ahead $100, then the expected number of bets until
the game is over is 500  100 D 50; 000. This simple formula (19.12) cries out for
an intuitive proof, but we have not found one (where are you, Pascal?).
19.1.5
Quit While You Are Ahead
Suppose that the gambler never quits while he is ahead. That is, he starts with
n > 0 dollars, ignores any target T , but plays until he is Ô¨Çat broke. Call this the
unbounded Gambler‚Äôs ruin game. It turns out that if the game is not favorable, that
is, p  1=2, the gambler is sure to go broke. In particular, even in a ‚Äúfair‚Äù game
with p D 1=2, he is sure to go broke.
Lemma 19.1.4. If the gambler starts with one or more dollars and plays a fair
unbounded game, then he will go broke with probability 1.

Chapter 19
Random Processes
764
Proof. If the gambler has initial capital n and goes broke in a game without reach-
ing a target T , then he would also go broke if he were playing and ignored the
target. So the probability that he will lose if he keeps playing without stopping at
any target T must be at least as large as the probability that he loses when he has a
target T > n.
But we know that in a fair game, the probability that he loses is 1   n=T . This
number can be made arbitrarily close to 1 by choosing a sufÔ¨Åciently large value of
T . Hence, the probability of his losing while playing without any target has a lower
bound arbitrarily close to 1, which means it must in fact be 1.

So even if the gambler starts with a million dollars and plays a perfectly fair
game, he will eventually lose it all with probability 1. But there is good news: if
the game is fair, he can ‚Äúexpect‚Äù to play forever:
Lemma 19.1.5. If the gambler starts with one or more dollars and plays a fair
unbounded game, then his expected number of plays is inÔ¨Ånite.
A proof appears in Problem 19.2.
So even starting with just one dollar, the expected number of plays before going
broke is inÔ¨Ånite! Of course, this does not mean that the gambler is likely to play for
long ‚Äîthere is even a 50% chance he will lose the very Ô¨Årst bet and go broke right
away.
Lemma 19.1.5 says that the gambler can ‚Äúexpect‚Äù to play forever, while Lemma 19.1.4
says that he is certain to go broke. These facts sound contradictory, but they are
sound consequences of the technical mathematical deÔ¨Ånition of expectation. The
moral here, as in section 18.8, is that naive intuition is unreliable when it comes to
inÔ¨Ånite expectation.
19.2
Random Walks on Graphs
The hyperlink structure of the World Wide Web can be described as a digraph. The
vertices are the web pages with a directed edge from vertex x to vertex y if x has
a link to y. For example, in the following graph the vertices x1; : : : ; xn correspond
to web pages and
Àù
xi !xj
Àõ
is a directed edge when page xi contains a hyperlink to
page xj .

19.2. Random Walks on Graphs
765
x1
x3
x4
x7
x6
x2
x5
The web graph is an enormous graph with many billions and probably even tril-
lions of vertices. At Ô¨Årst glance, this graph wouldn‚Äôt seem to be very interesting.
But in 1995, two students at Stanford, Larry Page and Sergey Brin realized that the
structure of this graph could be very useful in building a search engine. Traditional
document searching programs had been around for a long time and they worked in
a fairly straightforward way. Basically, you would enter some search terms and the
searching program would return all documents containing those terms. A relevance
score might also be returned for each document based on the frequency or position
that the search terms appeared in the document. For example, if the search term
appeared in the title or appeared 100 times in a document, that document would
get a higher score. So if an author wanted a document to get a higher score for
certain keywords, he would put the keywords in the title and make it appear in lots
of places. You can even see this today with some bogus web sites.
This approach works Ô¨Åne if you only have a few documents that match a search
term. But on the web, there are billions of documents and millions of matches to a
typical search.
For example, on May 2, 2012, a search on Google for ‚Äú ‚ÄòMathematics for Com-
puter Science‚Äô text‚Äù gave 482,000 hits! How does Google decide which 10 or 20
to show Ô¨Årst? It wouldn‚Äôt be smart to pick a page that gets a high keyword score
because it has ‚ÄúMathematics Mathematics : : : Mathematics‚Äù across the front of the
document.
One way to get placed high on the list is to pay Google an advertising fee ‚Äî
and Google gets an enormous revenue stream from these fees. Of course an early
listing is worth a fee only if an advertiser‚Äôs target audience is attracted to the listing.
But an audience does get attracted to Google listings because its ranking method
is really good at determining the most relevant web pages. For example, Google
demonstrated its accuracy in our case by giving Ô¨Årst rank to our 6.042 text1 :-) .
So how did Google know to pick 6.042 to be Ô¨Årst out of 482,000?
Well back in 1995, Larry and Sergey got the idea to allow the digraph structure
of the web to determine which pages are likely to be the most important.
1First rank for some reason was an early version archived at Princeton; the Spring 2010 version
on the MIT Open Courseware site ranked 4th and 5th.

Chapter 19
Random Processes
766
19.2.1
A First Crack at Page Rank
Looking at the web graph, any idea which vertex/page might be the best to rank
1st? Assume that all the pages match the search terms for now. Well, intuitively,
we should choose x2, since lots of other pages point to it. This leads us to their Ô¨Årst
idea: try deÔ¨Åning the page rank of x to be the number of links pointing to x, that
is, indegree.x/. The idea is to think of web pages as voting for the most important
page ‚Äîthe more votes, the better rank.
Of course, there are some problems with this idea. Suppose you wanted to have
your page get a high ranking. One thing you could do is to create lots of dummy
pages with links to your page.
+n
There is another problem ‚Äîa page could become unfairly inÔ¨Çuential by having
lots of links to other pages it wanted to hype.
+1
+1
+1
+1
+1
So this strategy for high ranking would amount to, ‚Äúvote early, vote often,‚Äù which
is no good if you want to build a search engine that‚Äôs worth paying fees for. So,
admittedly, their original idea was not so great. It was better than nothing, but
certainly not worth billions of dollars.

19.2. Random Walks on Graphs
767
19.2.2
Random Walk on the Web Graph
But then Sergey and Larry thought some more and came up with a couple of im-
provements. Instead of just counting the indegree of a vertex, they considered the
probability of being at each page after a long random walk on the web graph. In
particular, they decided to model a user‚Äôs web experience as following each link on
a page with uniform probability. That is, they assigned each edge x ! y of the
web graph with a probability conditioned on being on page x:
Pr

follow link hx !yi j at page x

WWD
1
outdegree.x/:
The user experience is then just a random walk on the web graph.
For example, if the user is at page x, and there are three links from page x, then
each link is followed with probability 1=3.
We can also compute the probability of arriving at a particular page, y, by sum-
ming over all edges pointing to y. We thus have
Pr≈ígo to y¬ç
D
X
edges hx!yi
Pr

follow link hx !yi j at page x

 Pr≈íat page x¬ç
D
X
edges hx!yi
Pr≈íat x¬ç
outdegree.x/
(19.13)
For example, in our web graph, we have
Pr≈ígo to x4¬ç D Pr≈íat x7¬ç
2
C Pr≈íat x2¬ç
1
:
One can think of this equation as x7 sending half its probability to x2 and the other
half to x4. The page x2 sends all of its probability to x4.
There‚Äôs one aspect of the web graph described thus far that doesn‚Äôt mesh with the
user experience ‚Äîsome pages have no hyperlinks out. Under the current model,
the user cannot escape these pages. In reality, however, the user doesn‚Äôt fall off the
end of the web into a void of nothingness. Instead, he restarts his web journey.
To model this aspect of the web, Sergey and Larry added a supervertex to the web
graph and had every page with no hyperlinks point to it. Moreover, the supervertex
points to every other vertex in the graph, allowing you to restart the walk from a
random place. For example, below left is a graph and below right is the same graph
after adding the supervertex xNC1.

Chapter 19
Random Processes
768
x1
x2
x3
1
1/2
1/2
xN+1
x1
x2
x3
The addition of the supervertex also removes the possibility that the value 1=outdegree.x/
might involve a division by zero.
19.2.3
Stationary Distribution & Page Rank
The basic idea of page rank is just a stationary distribution over the web graph, so
let‚Äôs deÔ¨Åne a stationary distribution.
Suppose each vertex is assigned a probability that corresponds, intuitively, to the
likelihood that a random walker is at that vertex at a randomly chosen time. We
assume that the walk never leaves the vertices in the graph, so we require that
X
vertices x
Pr≈íat x¬ç D 1:
(19.14)
DeÔ¨Ånition 19.2.1. An assignment of probabilities to vertices in a digraph is a sta-
tionary distribution if for all vertices x
Pr≈íat x¬ç D Pr≈ígo to x at next step¬ç
Sergey and Larry deÔ¨Åned their page ranks to be a stationary distribution. They
did this by solving the following system of linear equations: Ô¨Ånd a nonnegative
number, PR.x/, for each vertex, x, such that
PR.x/ D
X
edges hy!xi
PR.y/
outdegree.y/;
(19.15)
corresponding to the intuitive equations given in (19.13). These numbers must also
satisfy the additional constraint corresponding to (19.14):
X
vertices x
PR.x/ D 1:
(19.16)
So if there are n vertices, then equations (19.15) and (19.16) provide a system
of n C 1 linear equations in the n variables, PR.x/. Note that constraint (19.16)

19.2. Random Walks on Graphs
769
is needed because the remaining constraints (19.15) could be satisÔ¨Åed by letting
PR.x/ WWD 0 for all x, which is useless.
Sergey and Larry were smart fellows, and they set up their page rank algorithm
so it would always have a meaningful solution. Their addition of a supervertex
ensures there is always a unique stationary distribution. Moreover, starting from
any vertex and taking a sufÔ¨Åciently long random walk on the graph, the probability
of being at each page will get closer and closer to the stationary distribution. Note
that general digraphs without supervertices may have neither of these properties:
there may not be a unique stationary distribution, and even when there is, there
may be starting points from which the probabilities of positions during a random
walk do not converge to the stationary distribution. Examples of this appear in
some of the problems below.
Now just keeping track of the digraph whose vertices are billions of web pages
is a daunting task. That‚Äôs why Google is building power plants. Indeed, Larry
and Sergey named their system Google after the number 10100 ‚Äîwhich is called a
‚Äúgoogol‚Äù ‚Äîto reÔ¨Çect the fact that the web graph is so enormous.
Anyway, now you can see how 6.042 ranked Ô¨Årst out of 378,000 matches. Lots
of other universities used our notes and presumably have links to the 6.042 open
courseware site, and the university sites themselves are legitimate, which ultimately
leads to 6.042 getting a high page rank in the web graph.
Problems for Section 19.1
Practice Problems
Problem 19.1.
Suppose that a gambler is playing a game in which he makes a series of $1 bets.
He wins each one with probability 0.49, and he keeps betting until he either runs
out of money or reaches some Ô¨Åxed goal of T dollars.
Let t.n/ be the expected number of bets the gambler makes until the game ends,
where n is the number of dollars the gambler has when he starts betting. Then the
function t satisÔ¨Åes a linear recurrence of the form
t.n/ D a  t.n C 1/ C b  t.n   1/ C c
for real constants a, b, c and 0 < n < T .
(a) What are the values of a, b and c?
(b) What is t.0/?
(c) What is t.T /?

Chapter 19
Random Processes
770
Class Problems
Problem 19.2.
In a gambler‚Äôs ruin scenario, the gambler makes independent $1 bets, where the
probability of winning a bet is p and of losing is q WWD 1   p. The gambler keeps
betting until he goes broke or reaches a target of T dollars.
Suppose T D 1, that is, the gambler keeps playing until he goes broke. Let
r be the probability that starting with n > 0 dollars, the gambler‚Äôs stake ever gets
reduced to n   1 dollars.
(a) Explain why
r D q C pr2:
(b) Conclude that if p  1=2, then r D 1.
(c) Prove that even in a fair game, the gambler is sure to get ruined no matter how
much money he starts with!
(d) Let t be the expected time for the gambler‚Äôs stake to go down by 1 dollar.
Verify that
t D q C p.1 C 2t/:
Conclude that starting with a 1 dollar stake in a fair game, the gambler can expect
to play forever!
Problem 19.3.
A gambler is placing $1 bets on the ‚Äú1st dozen‚Äù in roulette. This bet wins when a
number from one to twelve comes in, and then the gambler gets his $1 back plus
$2 more. Recall that there are 38 numbers on the roulette wheel.
The gambler‚Äôs initial stake in $n and his target is $T . He will keep betting until
he runs out of money (‚Äúgoes broke‚Äù) or reaches his target. Let wn be the probability
of the gambler winning, that is, reaching target $T before going broke.
(a) Write a linear recurrence for wn; you need not solve the recurrence.
(b) Let en be the expected number of bets until the game ends. Write a linear
recurrence for en; you need not solve the recurrence.
Problem 19.4.
In the fair Gambler‚Äôs Ruin game with initial stake of n dollars and target of T
dollars, let en be the number of $1 bets the gambler makes until the game ends
(because he reaches his target or goes broke).

19.2. Random Walks on Graphs
771
(a) Describe constants a; b; c such that
en D aen 1 C ben 2 C c:
(19.17)
for 1 < n < T .
(b) Let en be deÔ¨Åned by (19.17) for all n > 1, where e0 D 0 and e1 D d for
some constant d. Derive a closed form (involving d) for the generating function
E.x/ WWD P1
0 enxn.
(c) Find a closed form (involving d) for en.
(d) Use part (c) to solve for d.
(e) Prove that en D n.T   n/.
a
Problems for Section 19.2
Practice Problems
Problem 19.5.
Consider the following random-walk graphs:
x
y
1
1
Figure 19.3
w
z
1
0.9
0.1
Figure 19.4
(a) Find d.x/ for a stationary distribution for graph 19.3.

Chapter 19
Random Processes
772
a
b
1
c
1/2
1/2
1/2
1
d
1/2
Figure 19.5
(b) Find d.y/ for a stationary distribution for graph 19.3.
(c) If you start at node x in graph 19.3 and take a (long) random walk, does the
distribution over nodes ever get close to the stationary distribution?
(d) Find d.w/ for a stationary distribution for graph 19.4.
(e) Find d.z/ for a stationary distribution for graph 19.4.
(f) If you start at node w in graph 19.4 and take a (long) random walk, does the
distribution over nodes ever get close to the stationary distribution? (Hint: try a
few steps and watch what is happening.)
(g) How many stationary distributions are there for graph 19.5?
(h) If you start at node b in graph 19.5 and take a (long) random walk, what will
be the approximate probability that you are at node d?
Problem 19.6.
A sink in a digraph is a vertex with no edges leaving it. Circle whichever of the
following assertions are true of stable distributions on Ô¨Ånite digraphs with exactly
two sinks:
 there may not be any
 there may be a unique one
 there are exactly two
 there may be a countably inÔ¨Ånite number
 there may be a uncountable number
 there always is an uncountable number

19.2. Random Walks on Graphs
773
Problem 19.7.
Explain why there are an uncountable number of stationary distributions for the
following random walk graph.
a
b
1
c
1/2
1/2
1/2
1
d
1/2
Class Problems
Problem 19.8. (a) Find a stationary distribution for the random walk graph in Fig-
ure 19.6.
x
y
1
1
Figure 19.6
(b) If you start at node x in Figure 19.6 and take a (long) random walk, does the
distribution over nodes ever get close to the stationary distribution? Explain.
(c) Find a stationary distribution for the random walk graph in Figure 19.7.
w
z
1
0.9
0.1
Figure 19.7
(d) If you start at node w Figure 19.7 and take a (long) random walk, does the
distribution over nodes ever get close to the stationary distribution? You needn‚Äôt
prove anything here, just write out a few steps and see what‚Äôs happening.

Chapter 19
Random Processes
774
a
b
1
c
1/2
1/2
1/2
1
d
1/2
Figure 19.8
(e) Find a stationary distribution for the random walk graph in Figure 19.8.
(f) If you start at node b in Figure 19.8 and take a long random walk, the proba-
bility you are at node d will be close to what fraction? Explain.
Problem 19.9.
We use random walks on a digraph, G, to model the typical movement pattern of a
Math for CS student right after the Ô¨Ånal exam.
The student comes out of the Ô¨Ånal exam located on a particular node of the
graph, corresponding to the exam room. What happens next is unpredictable, as
the student is in a total haze. At each step of the walk, if the student is at node
u at the end of the previous step, they pick one of the edges hu!vi uniformly at
random from the set of all edges directed out of u, and then walk to the node v.
Let n WWD jV.G/j and deÔ¨Åne the vector P .j / to be
P .j / WWD .p.j /
1 ; : : : ; p.j /
n /
where p.j/
i
is the probability of being at node i after j steps.
(a) We will start by looking at a simple graph. If the student starts at node 1 (the
top node) in the following graph, what is P .0/; P .1/; P .2/? Give a nice expression
for P .n/.
1/2
1/2
1

19.2. Random Walks on Graphs
775
(b) Given an arbitrary graph, show how to write an expression for p.j /
i
in terms
of the p.j 1/
k
‚Äôs.
(c) Does your answer to the last part look like any other system of equations
you‚Äôve seen in this course?
(d) Let the limiting distribution vector, , be
lim
k!1
Pk
iD1 P .i/
k
:
What is the limiting distribution of the graph from part a? Would it change if the
start distribution were P .0/ D .1=2; 1=2/ or P .0/ D .1=3; 2=3/?
(e) Let‚Äôs consider another directed graph. If the student starts at node 1 with
probability 1/2 and node 2 with probability 1/2, what is P .0/; P .1/; P .2/ in the
following graph? What is the limiting distribution?
1/3
1/3
1/3
1/3
1/3 1/3
1/3
1/3
1/3
1
2
3
(f) Now we are ready for the real problem. In order to make it home, the poor
Math for student is faced with n doors along a long hall way. Unbeknownst to him,
the door that goes outside to paradise (that is, freedom from the class and more
importantly, vacation!) is at the very end. At each step along the way, he passes
by a door which he opens up and goes through with probability 1/2. Every time he
does this, he gets teleported back to the exam room. Let‚Äôs Ô¨Ågure out how long it
will take the poor guy to escape from the class. What is P .0/; P .1/; P .2/? What is
the limiting distribution?
...
1
2
3
n
0
1/2
1/2
1/2
1/2
1/2
1/2
1/2
1
1

Chapter 19
Random Processes
776
(g) Show that the expected number, T .n/, of teleportations you make back to the
exam room before you escape to the outside world is 2n 1   1.
Problem 19.10.
Prove that for Ô¨Ånite random walk graphs, the uniform distribution is stationary if
and only the probabilities of the edges coming into each vertex always sum to 1,
namely
X
u2into.v/
p.u; v/ D 1;
(19.18)
where into.w/ WWD fv j hv !wi is an edgeg.
Problem 19.11.
A Google-graph is a random-walk graph such that every edge leaving any given
vertex has the same probability. That is, the probability of each edge hv !wi is
1= outdeg.v/.
A digraph is symmetric if, whenever hv !wi is an edge, so is hw !vi. Given
any Ô¨Ånite, symmetric Google-graph, let
d.v/ WWD outdeg.v/
e
;
where e is the total number of edges in the graph.
(a) If d was used for webpage ranking, how could you hack this to give your page
a high rank? ...and explain informally why this wouldn‚Äôt work for ‚Äúreal‚Äù page rank
using digraphs?
(b) Show that d is a stationary distribution.
Homework Problems
Problem 19.12.
A digraph is strongly connected iff there is a directed path between every pair of
distinct vertices. In this problem we consider a Ô¨Ånite random walk graph that is
strongly connected.
(a) Let d1 and d2 be distinct distributions for the graph, and deÔ¨Åne the maximum
dilation, , of d1 over d2 to be
 WWD max
x2V
d1.x/
d2.x/ :

19.2. Random Walks on Graphs
777
0.5
0.5
0.5
0.5
0.5
0.5
0.5
0.5
0.5
0.5
1
1
1
0.5
0.5
1
Figure 19.9
Which ones have uniform stationary distribution?
Call a vertex, x, dilated if d1.x/=d2.x/ D . Show that there is an edge, hy !zi,
from an undilated vertex y to a dilated vertex, z. Hint: Choose any dilated vertex,
x, and consider the set, D, of dilated vertices connected to x by a directed path
(going to x) that only uses dilated vertices. Explain why D ¬§ V , and then use the
fact that the graph is strongly connected.
(b) Prove that the graph has at most one stationary distribution. (There always is
a stationary distribution, but we‚Äôre not asking you prove this.) Hint: Let d1 be a
stationary distribution and d2 be a different distribution. Let z be the vertex from
part (a). Show that starting from d2, the probability of z changes at the next step.
That is, b
d2.z/ ¬§ d2.z/.
Exam Problems
Problem 19.13.
For which of the graphs in Figure 19.9 is the uniform distribution over nodes a
stationary distribution? The edges are labeled with transition probabilities. Explain
your reasoning.


V
Recurrences


Introduction
A recurrence describes a sequence of numbers. Early terms are speciÔ¨Åed explic-
itly, and later terms are expressed as a function of their predecessors. As a trivial
example, here is a recurrence describing the sequence 1; 2; 3; : : : :
T1 D 1
Tn D Tn 1 C 1
(for n  2):
Here, the Ô¨Årst term is deÔ¨Åned to be 1 and each subsequent term is one more than its
predecessor.
Recurrences turn out to be a powerful tool. In this chapter, we‚Äôll emphasize using
recurrences to analyze the performance of recursive algorithms. However, recur-
rences have other applications in computer science as well, such as enumeration of
structures and analysis of random processes. And, as we saw in Section 13.4, they
also arise in the analysis of problems in the physical sciences.
A recurrence in isolation is not a very useful description of a sequence. Sim-
ple questions such as, ‚ÄúWhat is the hundredth term?‚Äù or ‚ÄúWhat is the asymptotic
growth rate?‚Äù are not in general easy to answer by inspection of the recurrence. So
a typical goal is to solve a recurrence ‚Äîthat is, to Ô¨Ånd a closed-form expression for
the nth term.
We‚Äôll Ô¨Årst introduce two general solving techniques: guess-and-verify and plug-
and-chug. These methods are applicable to every recurrence, but their success re-
quires a Ô¨Çash of insight ‚Äîsometimes an unrealistically brilliant Ô¨Çash. So we‚Äôll also
introduce two big classes of recurrences, linear and divide-and-conquer, that often
come up in computer science. Essentially all recurrences in these two classes are
solvable using cookbook techniques; you follow the recipe and get the answer. A
drawback is that calculation replaces insight. The ‚ÄúAha!‚Äù moment that is essential

Part V
Recurrences
782
in the guess-and-verify and plug-and-chug methods is replaced by a ‚ÄúHuh‚Äù at the
end of a cookbook procedure.
At the end of the chapter, we‚Äôll develop rules of thumb to help you assess many
recurrences without any calculation. These rules can help you distinguish promis-
ing approaches from bad ideas early in the process of designing an algorithm.
Recurrences are one aspect of a broad theme in computer science: reducing a big
problem to progressively smaller problems until easy base cases are reached. This
same idea underlies both induction proofs and recursive algorithms. As we‚Äôll see,
all three ideas snap together nicely. For example, the running time of a recursive
algorithm could be described with a recurrence with induction used to verify the
solution.

20
Recurrences
20.1
The Towers of Hanoi
There are several methods for solving recurrence equations. The simplest is to
guess the solution and then verify that the guess is correct with an induction proof.
For example, as a alternative to the generating function derivation in Section 15.4.2
of the value of the number, Tn, of moves in the Tower of Hanoi problem with n
disks, we could have tried guessing. As a basis for a good guess, let‚Äôs look for a
pattern in the values of Tn computed above: 1, 3, 7, 15, 31, 63. A natural guess
is Tn D 2n   1. But whenever you guess a solution to a recurrence, you should
always verify it with a proof, typically by induction. After all, your guess might be
wrong. (But why bother to verify in this case? After all, if we‚Äôre wrong, its not the
end of the. . . no, let‚Äôs check.)
Claim 20.1.1. Tn D 2n   1 satisÔ¨Åes the recurrence:
T1 D 1
Tn D 2Tn 1 C 1
(for n  2):
Proof. The proof is by induction on n. The induction hypothesis is that Tn D
2n   1. This is true for n D 1 because T1 D 1 D 21   1. Now assume that
Tn 1 D 2n 1   1 in order to prove that Tn D 2n   1, where n  2:
Tn D 2Tn 1 C 1
D 2.2n 1   1/ C 1
D 2n   1:
The Ô¨Årst equality is the recurrence equation, the second follows from the induction
assumption, and the last step is simpliÔ¨Åcation.

Such veriÔ¨Åcation proofs are especially tidy because recurrence equations and
induction proofs have analogous structures. In particular, the base case relies on
the Ô¨Årst line of the recurrence, which deÔ¨Ånes T1. And the inductive step uses the
second line of the recurrence, which deÔ¨Ånes Tn as a function of preceding terms.
Our guess is veriÔ¨Åed. So we can now resolve our remaining questions about the
64-disk puzzle. Since T64 D 264   1, the monks must complete more than 18
billion billion steps before the world ends. Better study for the Ô¨Ånal.

Chapter 20
Recurrences
784
20.1.1
The Upper Bound Trap
When the solution to a recurrence is complicated, one might try to prove that some
simpler expression is an upper bound on the solution. For example, the exact so-
lution to the Towers of Hanoi recurrence is Tn D 2n   1. Let‚Äôs try to prove the
‚Äúnicer‚Äù upper bound Tn  2n, proceeding exactly as before.
Proof. (Failed attempt.) The proof is by induction on n. The induction hypothesis
is that Tn  2n. This is true for n D 1 because T1 D 1  21. Now assume that
Tn 1  2n 1 in order to prove that Tn  2n, where n  2:
Tn D 2Tn 1 C 1
 2.2n 1/ C 1
6 2n
IMPLIES Uh-oh!
The Ô¨Årst equality is the recurrence relation, the second follows from the induction
hypothesis, and the third step is a Ô¨Çaming train wreck.

The proof doesn‚Äôt work! As is so often the case with induction proofs, the ar-
gument only goes through with a stronger hypothesis. This isn‚Äôt to say that upper
bounding the solution to a recurrence is hopeless, but this is a situation where in-
duction and recurrences do not mix well.
20.1.2
Plug and Chug
Guess-and-verify is a simple and general way to solve recurrence equations. But
there is one big drawback: you have to guess right. That was not hard for the
Towers of Hanoi example. But sometimes the solution to a recurrence has a strange
form that is quite difÔ¨Åcult to guess. Practice helps, of course, but so can some other
methods.
Plug-and-chug is another way to solve recurrences. This is also sometimes called
‚Äúexpansion‚Äù or ‚Äúiteration.‚Äù As in guess-and-verify, the key step is identifying a
pattern. But instead of looking at a sequence of numbers, you have to spot a pattern
in a sequence of expressions, which is sometimes easier. The method consists of
three steps, which are described below and illustrated with the Towers of Hanoi
example.
Step 1: Plug and Chug Until a Pattern Appears
The Ô¨Årst step is to expand the recurrence equation by alternately ‚Äúplugging‚Äù (apply-
ing the recurrence) and ‚Äúchugging‚Äù (simplifying the result) until a pattern appears.
Be careful: too much simpliÔ¨Åcation can make a pattern harder to spot. The rule

20.1. The Towers of Hanoi
785
to remember‚Äîindeed, a rule applicable to the whole of college life‚Äîis chug in
moderation.
Tn D 2Tn 1 C 1
D 2.2Tn 2 C 1/ C 1
plug
D 4Tn 2 C 2 C 1
chug
D 4.2Tn 3 C 1/ C 2 C 1
plug
D 8Tn 3 C 4 C 2 C 1
chug
D 8.2Tn 4 C 1/ C 4 C 2 C 1
plug
D 16Tn 4 C 8 C 4 C 2 C 1
chug
Above, we started with the recurrence equation. Then we replaced Tn 1 with
2Tn 2 C 1, since the recurrence says the two are equivalent. In the third step,
we simpliÔ¨Åed a little‚Äîbut not too much! After several similar rounds of plugging
and chugging, a pattern is apparent. The following formula seems to hold:
Tn D 2kTn k C 2k 1 C 2k 2 C    C 22 C 21 C 20
D 2kTn k C 2k   1
Once the pattern is clear, simplifying is safe and convenient. In particular, we‚Äôve
collapsed the geometric sum to a closed form on the second line.
Step 2: Verify the Pattern
The next step is to verify the general formula with one more round of plug-and-
chug.
Tn D 2kTn k C 2k   1
D 2k.2Tn .kC1/ C 1/ C 2k   1
plug
D 2kC1Tn .kC1/ C 2kC1   1
chug
The Ô¨Ånal expression on the right is the same as the expression on the Ô¨Årst line,
except that k is replaced by k C 1. Surprisingly, this effectively proves that the
formula is correct for all k. Here is why: we know the formula holds for k D 1,
because that‚Äôs the original recurrence equation. And we‚Äôve just shown that if the
formula holds for some k  1, then it also holds for k C 1. So the formula holds
for all k  1 by induction.

Chapter 20
Recurrences
786
Step 3: Write Tn Using Early Terms with Known Values
The last step is to express Tn as a function of early terms whose values are known.
Here, choosing k D n   1 expresses Tn in terms of T1, which is equal to 1. Sim-
plifying gives a closed-form expression for Tn:
Tn D 2n 1T1 C 2n 1   1
D 2n 1  1 C 2n 1   1
D 2n   1:
We‚Äôre done! This is the same answer we got from guess-and-verify.
Let‚Äôs compare guess-and-verify with plug-and-chug. In the guess-and-verify
method, we computed several terms at the beginning of the sequence, T1, T2, T3,
etc., until a pattern appeared. We generalized to a formula for the nth term, Tn. In
contrast, plug-and-chug works backward from the nth term. SpeciÔ¨Åcally, we started
with an expression for Tn involving the preceding term, Tn 1, and rewrote this us-
ing progressively earlier terms, Tn 2, Tn 3, etc. Eventually, we noticed a pattern,
which allowed us to express Tn using the very Ô¨Årst term, T1, whose value we knew.
Substituting this value gave a closed-form expression for Tn. So guess-and-verify
and plug-and-chug tackle the problem from opposite directions.
20.2
Merge Sort
Algorithms textbooks traditionally claim that sorting is an important, fundamental
problem in computer science. Then they smack you with sorting algorithms until
life as a disk-stacking monk in Hanoi sounds delightful. Here, we‚Äôll cover just one
well-known sorting algorithm, Merge Sort. The analysis introduces another kind of
recurrence.
Here is how Merge Sort works. The input is a list of n numbers, and the output
is those same numbers in nondecreasing order. There are two cases:
 If the input is a single number, then the algorithm does nothing, because the
list is already sorted.
 Otherwise, the list contains two or more numbers. The Ô¨Årst half and the
second half of the list are each sorted recursively. Then the two halves are
merged to form a sorted list with all n numbers.
Let‚Äôs work through an example. Suppose we want to sort this list:

20.2. Merge Sort
787
10, 7, 23, 5, 2, 8, 6, 9.
Since there is more than one number, the Ô¨Årst half (10, 7, 23, 5) and the second half
(2, 8, 6, 9) are sorted recursively. The results are 5, 7, 10, 23 and 2, 6, 8, 9. All that
remains is to merge these two lists. This is done by repeatedly emitting the smaller
of the two leading terms. When one list is empty, the whole other list is emitted.
The example is worked out below. In this table, underlined numbers are about to
be emitted.
First Half
Second Half
Output
5, 7, 10, 23
2, 6, 8, 9
5, 7, 10, 23
6, 8, 9
2
7, 10, 23
6, 8, 9
2, 5
7, 10, 23
8, 9
2, 5, 6
10, 23
8, 9
2, 5, 6, 7
10, 23
9
2, 5, 6, 7, 8
10, 23
2, 5, 6, 7, 8, 9
2, 5, 6, 7, 8, 9, 10, 23
The leading terms are initially 5 and 2. So we output 2. Then the leading terms are
5 and 6, so we output 5. Eventually, the second list becomes empty. At that point,
we output the whole Ô¨Årst list, which consists of 10 and 23. The complete output
consists of all the numbers in sorted order.
20.2.1
Finding a Recurrence
A traditional question about sorting algorithms is, ‚ÄúWhat is the maximum number
of comparisons used in sorting n items?‚Äù This is taken as an estimate of the running
time. In the case of Merge Sort, we can express this quantity with a recurrence. Let
Tn be the maximum number of comparisons used while Merge Sorting a list of n
numbers. For now, assume that n is a power of 2. This ensures that the input can
be divided in half at every stage of the recursion.
 If there is only one number in the list, then no comparisons are required, so
T1 D 0.
 Otherwise, Tn includes comparisons used in sorting the Ô¨Årst half (at most
Tn=2), in sorting the second half (also at most Tn=2), and in merging the two
halves. The number of comparisons in the merging step is at most n   1.
This is because at least one number is emitted after each comparison and one
more number is emitted at the end when one list becomes empty. Since n
items are emitted in all, there can be at most n   1 comparisons.

Chapter 20
Recurrences
788
Therefore, the maximum number of comparisons needed to Merge Sort n items is
given by this recurrence:
T1 D 0
Tn D 2Tn=2 C n   1
(for n  2 and a power of 2):
This fully describes the number of comparisons, but not in a very useful way; a
closed-form expression would be much more helpful. To get that, we have to solve
the recurrence.
20.2.2
Solving the Recurrence
Let‚Äôs Ô¨Årst try to solve the Merge Sort recurrence with the guess-and-verify tech-
nique. Here are the Ô¨Årst few values:
T1 D 0
T2 D 2T1 C 2   1 D 1
T4 D 2T2 C 4   1 D 5
T8 D 2T4 C 8   1 D 17
T16 D 2T8 C 16   1 D 49:
We‚Äôre in trouble! Guessing the solution to this recurrence is hard because there is
no obvious pattern. So let‚Äôs try the plug-and-chug method instead.
Step 1: Plug and Chug Until a Pattern Appears
First, we expand the recurrence equation by alternately plugging and chugging until
a pattern appears.
Tn D 2Tn=2 C n   1
D 2.2Tn=4 C n=2   1/ C .n   1/
plug
D 4Tn=4 C .n   2/ C .n   1/
chug
D 4.2Tn=8 C n=4   1/ C .n   2/ C .n   1/
plug
D 8Tn=8 C .n   4/ C .n   2/ C .n   1/
chug
D 8.2Tn=16 C n=8   1/ C .n   4/ C .n   2/ C .n   1/
plug
D 16Tn=16 C .n   8/ C .n   4/ C .n   2/ C .n   1/
chug
A pattern is emerging. In particular, this formula seems holds:
Tn D 2kTn=2k C .n   2k 1/ C .n   2k 2/ C    C .n   20/
D 2kTn=2k C kn   2k 1   2k 2      20
D 2kTn=2k C kn   2k C 1:

20.2. Merge Sort
789
On the second line, we grouped the n terms and powers of 2. On the third, we
collapsed the geometric sum.
Step 2: Verify the Pattern
Next, we verify the pattern with one additional round of plug-and-chug. If we
guessed the wrong pattern, then this is where we‚Äôll discover the mistake.
Tn D 2kTn=2k C kn   2k C 1
D 2k.2Tn=2kC1 C n=2k   1/ C kn   2k C 1
plug
D 2kC1Tn=2kC1 C .k C 1/n   2kC1 C 1
chug
The formula is unchanged except that k is replaced by k C 1. This amounts to the
induction step in a proof that the formula holds for all k  1.
Step 3: Write Tn Using Early Terms with Known Values
Finally, we express Tn using early terms whose values are known. SpeciÔ¨Åcally, if
we let k D log n, then Tn=2k D T1, which we know is 0:
Tn D 2kTn=2k C kn   2k C 1
D 2log nTn=2log n C n log n   2log n C 1
D nT1 C n log n   n C 1
D n log n   n C 1:
We‚Äôre done! We have a closed-form expression for the maximum number of com-
parisons used in Merge Sorting a list of n numbers. In retrospect, it is easy to see
why guess-and-verify failed: this formula is fairly complicated.
As a check, we can conÔ¨Årm that this formula gives the same values that we
computed earlier:
n
Tn
n log n   n C 1
1
0
1 log 1   1 C 1 D 0
2
1
2 log 2   2 C 1 D 1
4
5
4 log 4   4 C 1 D 5
8
17
8 log 8   8 C 1 D 17
16
49
16 log 16   16 C 1 D 49
As a double-check, we could write out an explicit induction proof. This would be
straightforward, because we already worked out the guts of the proof in step 2 of
the plug-and-chug procedure.

Chapter 20
Recurrences
790
20.3
Linear Recurrences
So far we‚Äôve solved recurrences with two techniques: guess-and-verify and plug-
and-chug. These methods require spotting a pattern in a sequence of numbers or
expressions. In this section and the next, we‚Äôll give cookbook solutions for two
large classes of recurrences. These methods require no Ô¨Çash of insight; you just
follow the recipe and get the answer.
20.3.1
Climbing Stairs
How many different ways are there to climb n stairs, if you can either step up one
stair or hop up two? For example, there are Ô¨Åve different ways to climb four stairs:
1. step, step, step, step
2. hop, hop
3. hop, step, step
4. step, hop step
5. step, step, hop
Working through this problem will demonstrate the major features of our Ô¨Årst cook-
book method for solving recurrences. We‚Äôll Ô¨Åll in the details of the general solution
afterward.
Finding a Recurrence
As special cases, there is 1 way to climb 0 stairs (do nothing) and 1 way to climb
1 stair (step up). In general, an ascent of n stairs consists of either a step followed
by an ascent of the remaining n   1 stairs or a hop followed by an ascent of n   2
stairs. So the total number of ways to climb n stairs is equal to the number of ways
to climb n   1 plus the number of ways to climb n   2. These observations deÔ¨Åne
a recurrence:
f .0/ D 1
f .1/ D 1
f .n/ D f .n   1/ C f .n   2/
for n  2:
Here, f .n/ denotes the number of ways to climb n stairs. Also, we‚Äôve switched
from subscript notation to functional notation, from Tn to fn. Here the change is
cosmetic, but the expressiveness of functions will be useful later.

20.3. Linear Recurrences
791
This is the Fibonacci recurrence, the most famous of all recurrence equations.
Fibonacci numbers arise in all sorts of applications and in nature. Fibonacci intro-
duced the numbers in 1202 to study rabbit reproduction. Fibonacci numbers also
appear, oddly enough, in the spiral patterns on the faces of sunÔ¨Çowers. And the
input numbers that make Euclid‚Äôs GCD algorithm require the greatest number of
steps are consecutive Fibonacci numbers.
Solving the Recurrence
The Fibonacci recurrence belongs to the class of linear recurrences, which are es-
sentially all solvable with a technique that you can learn in an hour. This is some-
what amazing, since the Fibonacci recurrence remained unsolved for almost six
centuries!
In general, a homogeneous linear recurrence has the form
f .n/ D a1f .n   1/ C a2f .n   2/ C    C adf .n   d/
where a1; a2; : : : ; ad and d are constants. The order of the recurrence is d. Com-
monly, the value of the function f is also speciÔ¨Åed at a few points; these are called
boundary conditions. For example, the Fibonacci recurrence has order d D 2 with
coefÔ¨Åcients a1 D a2 D 1 and g.n/ D 0. The boundary conditions are f .0/ D 1
and f .1/ D 1. The word ‚Äúhomogeneous‚Äù sounds scary, but effectively means ‚Äúthe
simpler kind.‚Äù We‚Äôll consider linear recurrences with a more complicated form
later.
Let‚Äôs try to solve the Fibonacci recurrence with the beneÔ¨Åt centuries of hindsight.
In general, linear recurrences tend to have exponential solutions. So let‚Äôs guess that
f .n/ D xn
where x is a parameter introduced to improve our odds of making a correct guess.
We‚Äôll Ô¨Ågure out the best value for x later. To further improve our odds, let‚Äôs neglect
the boundary conditions, f .0/ D 0 and f .1/ D 1, for now. Plugging this guess
into the recurrence f .n/ D f .n   1/ C f .n   2/ gives
xn D xn 1 C xn 2:
Dividing both sides by xn 2 leaves a quadratic equation:
x2 D x C 1:
Solving this equation gives two plausible values for the parameter x:
x D 1 Àô
p
5
2
:

Chapter 20
Recurrences
792
This suggests that there are at least two different solutions to the recurrence, ne-
glecting the boundary conditions.
f .n/ D
 
1 C
p
5
2
!n
or
f .n/ D
 
1  p
5
2
!n
A charming features of homogeneous linear recurrences is that any linear com-
bination of solutions is another solution.
Theorem 20.3.1. If f .n/ and g.n/ are both solutions to a homogeneous linear
recurrence, then h.n/ D sf .n/ C tg.n/ is also a solution for all s; t 2 R.
Proof.
h.n/ D sf .n/ C tg.n/
D s .a1f .n   1/ C    C adf .n   d// C t .a1g.n   1/ C    C adg.n   d//
D a1.sf .n   1/ C tg.n   1// C    C ad.sf .n   d/ C tg.n   d//
D a1h.n   1/ C    C adh.n   d/
The Ô¨Årst step uses the deÔ¨Ånition of the function h, and the second uses the fact that
f and g are solutions to the recurrence. In the last two steps, we rearrange terms
and use the deÔ¨Ånition of h again. Since the Ô¨Årst expression is equal to the last, h is
also a solution to the recurrence.

The phenomenon described in this theorem ‚Äîa linear combination of solutions is
another solution ‚Äîalso holds for many differential equations and physical systems.
In fact, linear recurrences are so similar to linear differential equations that you can
safely snooze through that topic in some future math class.
Returning to the Fibonacci recurrence, this theorem implies that
f .n/ D s
 
1 C
p
5
2
!n
C t
 
1  p
5
2
!n
is a solution for all real numbers s and t. The theorem expanded two solutions to
a whole spectrum of possibilities! Now, given all these options to choose from,
we can Ô¨Ånd one solution that satisÔ¨Åes the boundary conditions, f .0/ D 1 and
f .1/ D 1. Each boundary condition puts some constraints on the parameters s and
t. In particular, the Ô¨Årst boundary condition implies that
f .0/ D s
 
1 C
p
5
2
!0
C t
 
1  p
5
2
!0
D s C t D 1:

20.3. Linear Recurrences
793
Similarly, the second boundary condition implies that
f .1/ D s
 
1 C
p
5
2
!1
C t
 
1  p
5
2
!1
D 1:
Now we have two linear equations in two unknowns. The system is not degenerate,
so there is a unique solution:
s D
1
p
5
 1 C
p
5
2
t D   1
p
5
 1  p
5
2
:
These values of s and t identify a solution to the Fibonacci recurrence that also
satisÔ¨Åes the boundary conditions:
f .n/ D
1
p
5
 1 C
p
5
2
 
1 C
p
5
2
!n
  1
p
5
 1  p
5
2
 
1  p
5
2
!n
D
1
p
5
 
1 C
p
5
2
!nC1
  1
p
5
 
1  p
5
2
!nC1
:
It is easy to see why no one stumbled across this solution for almost six centuries.
All Fibonacci numbers are integers, but this expression is full of square roots of
Ô¨Åve! Amazingly, the square roots always cancel out. This expression really does
give the Fibonacci numbers if we plug in n D 0; 1; 2, etc.
This closed-form for Fibonacci numbers has some interesting corollaries. The
Ô¨Årst term tends to inÔ¨Ånity because the base of the exponential, .1 C
p
5/=2 D
1:618 : : : is greater than one. This value is often denoted  and called the ‚Äúgolden
ratio.‚Äù The second term tends to zero, because .1  p
5/=2 D  0:618033988 : : :
has absolute value less than 1. This implies that the nth Fibonacci number is:
f .n/ D nC1
p
5
C o.1/:
Remarkably, this expression involving irrational numbers is actually very close to
an integer for all large n ‚Äînamely, a Fibonacci number! For example:
20
p
5
D 6765:000029     f .19/:
This also implies that the ratio of consecutive Fibonacci numbers rapidly approaches
the golden ratio. For example:
f .20/
f .19/ D 10946
6765 D 1:618033998 : : : :

Chapter 20
Recurrences
794
20.3.2
Solving Homogeneous Linear Recurrences
The method we used to solve the Fibonacci recurrence can be extended to solve
any homogeneous linear recurrence; that is, a recurrence of the form
f .n/ D a1f .n   1/ C a2f .n   2/ C    C adf .n   d/
where a1; a2; : : : ; ad and d are constants. Substituting the guess f .n/ D xn, as
with the Fibonacci recurrence, gives
xn D a1xn 1 C a2xn 2 C    C adxn d:
Dividing by xn d gives
xd D a1xd 1 C a2xd 2 C    C ad 1x C ad:
This is called the characteristic equation of the recurrence. The characteristic equa-
tion can be read off quickly since the coefÔ¨Åcients of the equation are the same as
the coefÔ¨Åcients of the recurrence.
The solutions to a linear recurrence are deÔ¨Åned by the roots of the characteristic
equation. Neglecting boundary conditions for the moment:
 If r is a nonrepeated root of the characteristic equation, then rn is a solution
to the recurrence.
 If r is a repeated root with multiplicity k then rn, nrn, n2rn, ..., nk 1rn
are all solutions to the recurrence.
Theorem 20.3.1 implies that every linear combination of these solutions is also a
solution.
For example, suppose that the characteristic equation of a recurrence has roots s,
t, and u twice. These four roots imply four distinct solutions:
f .n/ D sn
f .n/ D tn
f .n/ D un
f .n/ D nun:
Furthermore, every linear combination
f .n/ D a  sn C b  tn C c  un C d  nun
(20.1)
is also a solution.
All that remains is to select a solution consistent with the boundary conditions
by choosing the constants appropriately. Each boundary condition implies a linear
equation involving these constants. So we can determine the constants by solving
a system of linear equations. For example, suppose our boundary conditions were

20.3. Linear Recurrences
795
f .0/ D 0, f .1/ D 1, f .2/ D 4, and f .3/ D 9. Then we would obtain four
equations in four unknowns:
f .0/ D 0
implies
a  s0 C b  t0 C c  u0 C d  0u0 D 0
f .1/ D 1
implies
a  s1 C b  t1 C c  u1 C d  1u1 D 1
f .2/ D 4
implies
a  s2 C b  t2 C c  u2 C d  2u2 D 4
f .3/ D 9
implies
a  s3 C b  t3 C c  u3 C d  3u3 D 9
This looks nasty, but remember that s, t, and u are just constants. Solving this sys-
tem gives values for a, b, c, and d that deÔ¨Åne a solution to the recurrence consistent
with the boundary conditions.
20.3.3
Solving General Linear Recurrences
We can now solve all linear homogeneous recurrences, which have the form
f .n/ D a1f .n   1/ C a2f .n   2/ C    C adf .n   d/:
Many recurrences that arise in practice do not quite Ô¨Åt this mold. For example, the
Towers of Hanoi problem led to this recurrence:
f .1/ D 1
f .n/ D 2f .n   1/ C 1
(for n  2):
The problem is the extra C1; that is not allowed in a homogeneous linear recur-
rence. In general, adding an extra function g.n/ to the right side of a linear recur-
rence gives an inhomogeneous linear recurrence:
f .n/ D a1f .n   1/ C a2f .n   2/ C    C adf .n   d/ C g.n/:
Solving inhomogeneous linear recurrences is neither very different nor very dif-
Ô¨Åcult. We can divide the whole job into Ô¨Åve steps:
1. Replace g.n/ by 0, leaving a homogeneous recurrence. As before, Ô¨Ånd roots
of the characteristic equation.
2. Write down the solution to the homogeneous recurrence, but do not yet use
the boundary conditions to determine coefÔ¨Åcients. This is called the homo-
geneous solution.
3. Now restore g.n/ and Ô¨Ånd a single solution to the recurrence, ignoring bound-
ary conditions. This is called a particular solution. We‚Äôll explain how to Ô¨Ånd
a particular solution shortly.

Chapter 20
Recurrences
796
4. Add the homogeneous and particular solutions together to obtain the general
solution.
5. Now use the boundary conditions to determine constants by the usual method
of generating and solving a system of linear equations.
As an example, let‚Äôs consider a variation of the Towers of Hanoi problem. Sup-
pose that moving a disk takes time proportional to its size. SpeciÔ¨Åcally, moving the
smallest disk takes 1 second, the next-smallest takes 2 seconds, and moving the nth
disk then requires n seconds instead of 1. So, in this variation, the time to complete
the job is given by a recurrence with a Cn term instead of a C1:
f .1/ D 1
f .n/ D 2f .n   1/ C n
for n  2:
Clearly, this will take longer, but how much longer? Let‚Äôs solve the recurrence with
the method described above.
In Steps 1 and 2, dropping the Cn leaves the homogeneous recurrence f .n/ D
2f .n   1/. The characteristic equation is x D 2. So the homogeneous solution is
f .n/ D c2n.
In Step 3, we must Ô¨Ånd a solution to the full recurrence f .n/ D 2f .n   1/ C n,
without regard to the boundary condition. Let‚Äôs guess that there is a solution of the
form f .n/ D an C b for some constants a and b. Substituting this guess into the
recurrence gives
an C b D 2.a.n   1/ C b/ C n
0 D .a C 1/n C .b   2a/:
The second equation is a simpliÔ¨Åcation of the Ô¨Årst. The second equation holds for
all n if both a C 1 D 0 (which implies a D  1) and b   2a D 0 (which implies
that b D  2). So f .n/ D an C b D  n   2 is a particular solution.
In the Step 4, we add the homogeneous and particular solutions to obtain the
general solution
f .n/ D c2n   n   2:
Finally, in step 5, we use the boundary condition, f .1/ D 1, determine the value
of the constant c:
f .1/ D 1
IMPLIES
c21   1   2 D 1
IMPLIES
c D 2:

20.4. Divide-and-Conquer Recurrences
797
Therefore, the function f .n/ D 2  2n   n   2 solves this variant of the Towers
of Hanoi recurrence. For comparison, the solution to the original Towers of Hanoi
problem was 2n   1. So if moving disks takes time proportional to their size, then
the monks will need about twice as much time to solve the whole puzzle.
20.3.4
How to Guess a Particular Solution
Finding a particular solution can be the hardest part of solving inhomogeneous
recurrences. This involves guessing, and you might guess wrong.1 However, some
rules of thumb make this job fairly easy most of the time.
 Generally, look for a particular solution with the same form as the inhomo-
geneous term g.n/.
 If g.n/ is a constant, then guess a particular solution f .n/ D c. If this doesn‚Äôt
work, try polynomials of progressively higher degree: f .n/ D bn C c, then
f .n/ D an2 C bn C c, etc.
 More generally, if g.n/ is a polynomial, try a polynomial of the same degree,
then a polynomial of degree one higher, then two higher, etc. For example,
if g.n/ D 6n C 5, then try f .n/ D bn C c and then f .n/ D an2 C bn C c.
 If g.n/ is an exponential, such as 3n, then Ô¨Årst guess that f .n/ D c3n.
Failing that, try f .n/ D bn3n C c3n and then an23n C bn3n C c3n, etc.
The entire process is summarized on the following page.
20.4
Divide-and-Conquer Recurrences
We now have a recipe for solving general linear recurrences. But the Merge Sort
recurrence, which we encountered earlier, is not linear:
T .1/ D 0
T .n/ D 2T .n=2/ C n   1
(for n  2):
In particular, T .n/ is not a linear combination of a Ô¨Åxed number of immediately
preceding terms; rather, T .n/ is a function of T .n=2/, a term halfway back in the
sequence.
1Chapter 15 explains how to solve linear recurrences with generating functions ‚Äîit‚Äôs a little more
complicated, but it does not require guessing.

Chapter 20
Recurrences
798
Short Guide to Solving Linear Recurrences
A linear recurrence is an equation
f .n/ D a1f .n   1/ C a2f .n   2/ C    C adf .n   d/
‚Äû
∆í‚Äö
‚Ä¶
homogeneous part
C g.n/
‚Äû ∆í‚Äö ‚Ä¶
inhomogeneous part
together with boundary conditions such as f .0/ D b0, f .1/ D b1, etc. Linear
recurrences are solved as follows:
1. Find the roots of the characteristic equation
xn D a1xn 1 C a2xn 2 C    C ak 1x C ak:
2. Write down the homogeneous solution. Each root generates one term and
the homogeneous solution is their sum. A nonrepeated root r generates the
term crn, where c is a constant to be determined later. A root r with multi-
plicity k generates the terms
d1rn
d2nrn
d3n2rn
: : :
dknk 1rn
where d1; : : : dk are constants to be determined later.
3. Find a particular solution. This is a solution to the full recurrence that need
not be consistent with the boundary conditions. Use guess-and-verify. If
g.n/ is a constant or a polynomial, try a polynomial of the same degree, then
of one higher degree, then two higher. For example, if g.n/ D n, then try
f .n/ D bnCc and then an2 CbnCc. If g.n/ is an exponential, such as 3n,
then Ô¨Årst guess f .n/ D c3n. Failing that, try f .n/ D .bn C c/3n and then
.an2 C bn C c/3n, etc.
4. Form the general solution, which is the sum of the homogeneous solution
and the particular solution. Here is a typical general solution:
f .n/ D c2n C d. 1/n
‚Äû
∆í‚Äö
‚Ä¶
homogeneous solution
C
3n C 1.
‚Äû ∆í‚Äö ‚Ä¶
inhomogeneous solution
5. Substitute the boundary conditions into the general solution. Each boundary
condition gives a linear equation in the unknown constants. For example,
substituting f .1/ D 2 into the general solution above gives
2 D c  21 C d  . 1/1 C 3  1 C 1
IMPLIES
 2 D 2c   d:
Determine the values of these constants by solving the resulting system of
linear equations.

20.4. Divide-and-Conquer Recurrences
799
Merge Sort is an example of a divide-and-conquer algorithm: it divides the in-
put, ‚Äúconquers‚Äù the pieces, and combines the results. Analysis of such algorithms
commonly leads to divide-and-conquer recurrences, which have this form:
T .n/ D
k
X
iD1
aiT .bin/ C g.n/
Here a1; : : : ak are positive constants, b1; : : : ; bk are constants between 0 and 1,
and g.n/ is a nonnegative function. For example, setting a1 D 2, b1 D 1=2, and
g.n/ D n   1 gives the Merge Sort recurrence.
20.4.1
The Akra-Bazzi Formula
The solution to virtually all divide and conquer solutions is given by the amazing
Akra-Bazzi formula. Quite simply, the asymptotic solution to the general divide-
and-conquer recurrence
T .n/ D
k
X
iD1
aiT .bin/ C g.n/
is
T .n/ D ‚Äö

np

1 C
Z n
1
g.u/
upC1 du

(20.2)
where p satisÔ¨Åes
k
X
iD1
aibp
i D 1:
(20.3)
A rarely-troublesome requirement is that the function g.n/ must not grow or
oscillate too quickly. SpeciÔ¨Åcally, jg0.n/j must be bounded by some polynomial.
So, for example, the Akra-Bazzi formula is valid when g.n/ D x2 log n, but not
when g.n/ D 2n.
Let‚Äôs solve the Merge Sort recurrence again, using the Akra-Bazzi formula in-
stead of plug-and-chug. First, we Ô¨Ånd the value p that satisÔ¨Åes
2  .1=2/p D 1:

Chapter 20
Recurrences
800
Looks like p D 1 does the job. Then we compute the integral:
T .n/ D ‚Äö

n

1 C
Z n
1
u   1
u2
du

D ‚Äö

n

1 C

log u C 1
u
n
1

D ‚Äö

n

log n C 1
n

D ‚Äö.n log n/:
The Ô¨Årst step is integration and the second is simpliÔ¨Åcation. We can drop the 1=n
term in the last step, because the log n term dominates. We‚Äôre done!
Let‚Äôs try a scary-looking recurrence:
T .n/ D 2T .n=2/ C .8=9/T .3n=4/ C n2:
Here, a1 D 2, b1 D 1=2, a2 D 8=9, and b2 D 3=4. So we Ô¨Ånd the value p that
satisÔ¨Åes
2  .1=2/p C .8=9/.3=4/p D 1:
Equations of this form don‚Äôt always have closed-form solutions, so you may need
to approximate p numerically sometimes. But in this case the solution is simple:
p D 2. Then we integrate:
T .n/ D ‚Äö

n2

1 C
Z n
1
u2
u3 du

D ‚Äö
 n2.1 C log n/

D ‚Äö
 n2 log n

:
That was easy!
20.4.2
Two Technical Issues
Until now, we‚Äôve swept a couple issues related to divide-and-conquer recurrences
under the rug. Let‚Äôs address those issues now.
First, the Akra-Bazzi formula makes no use of boundary conditions. To see why,
let‚Äôs go back to Merge Sort. During the plug-and-chug analysis, we found that
Tn D nT1 C n log n   n C 1:
This expresses the nth term as a function of the Ô¨Årst term, whose value is speciÔ¨Åed
in a boundary condition. But notice that Tn D ‚Äö.n log n/ for every value of T1.
The boundary condition doesn‚Äôt matter!

20.4. Divide-and-Conquer Recurrences
801
This is the typical situation: the asymptotic solution to a divide-and-conquer
recurrence is independent of the boundary conditions. Intuitively, if the bottom-
level operation in a recursive algorithm takes, say, twice as long, then the overall
running time will at most double. This matters in practice, but the factor of 2 is
concealed by asymptotic notation. There are corner-case exceptions. For example,
the solution to T .n/ D 2T .n=2/ is either ‚Äö.n/ or zero, depending on whether
T .1/ is zero. These cases are of little practical interest, so we won‚Äôt consider them
further.
There is a second nagging issue with divide-and-conquer recurrences that does
not arise with linear recurrences. SpeciÔ¨Åcally, dividing a problem of size n may
create subproblems of non-integer size. For example, the Merge Sort recurrence
contains the term T .n=2/. So what if n is 15? How long does it take to sort seven-
and-a-half items? Previously, we dodged this issue by analyzing Merge Sort only
when the size of the input was a power of 2. But then we don‚Äôt know what happens
for an input of size, say, 100.
Of course, a practical implementation of Merge Sort would split the input ap-
proximately in half, sort the halves recursively, and merge the results. For example,
a list of 15 numbers would be split into lists of 7 and 8. More generally, a list of n
numbers would be split into approximate halves of size dn=2e and bn=2c. So the
maximum number of comparisons is actually given by this recurrence:
T .1/ D 0
T .n/ D T .dn=2e/ C T .bn=2c/ C n   1
(for n  2):
This may be rigorously correct, but the ceiling and Ô¨Çoor operations make the recur-
rence hard to solve exactly.
Fortunately, the asymptotic solution to a divide and conquer recurrence is un-
affected by Ô¨Çoors and ceilings. More precisely, the solution is not changed by
replacing a term T .bin/ with either T .ceilbin/ or T .bbinc/. So leaving Ô¨Çoors
and ceilings out of divide-and-conquer recurrences makes sense in many contexts;
those are complications that make no difference.
20.4.3
The Akra-Bazzi Theorem
The Akra-Bazzi formula together with our assertions about boundary conditions
and integrality all follow from the Akra-Bazzi Theorem, which is stated below.
Theorem 20.4.1 (Akra-Bazzi). Suppose that the function T W R ! R satisÔ¨Åes the

Chapter 20
Recurrences
802
recurrence
T .x/
8
ÀÜ<
ÀÜ:
is nonnegative and bounded
for 0  x  x0;
D
kP
iD1
aiT .bix C hi.x// C g.x/
for x > x0:
where:
1. a1; : : : ; ak are positive constants.
2. b1; : : : ; bk are constants between 0 and 1.
3. x0 is large enough so that T is well-deÔ¨Åned.
4. g.x/ is a nonnegative function such that jg0.x/j is bounded by a polynomial.
5. jhi.x/j D O.x= log2 x/.
Then
T .x/ D ‚Äö

xp

1 C
Z x
1
g.u/
upC1 du

where p satisÔ¨Åes
k
X
iD1
aibp
i D 1:
The Akra-Bazzi theorem can be proved using a complicated induction argument,
though we won‚Äôt do that here. But let‚Äôs at least go over the statement of the theorem.
All the recurrences we‚Äôve considered were deÔ¨Åned over the integers, and that is
the common case. But the Akra-Bazzi theorem applies more generally to functions
deÔ¨Åned over the real numbers.
The Akra-Bazzi formula is lifted directed from the theorem statement, except
that the recurrence in the theorem includes extra functions, hi. These functions
extend the theorem to address Ô¨Çoors, ceilings, and other small adjustments to the
sizes of subproblems. The trick is illustrated by this combination of parameters
a1 D 1
b1 D 1=2
h1.x/ D
lx
2
m
  x
2
a2 D 1
b2 D 1=2
h2.x/ D
jx
2
k
  x
2
g.x/ D x   1

20.4. Divide-and-Conquer Recurrences
803
which corresponds the recurrence
T .x/ D 1  T
x
2 C
lx
2
m
  x
2

C T
x
2 C
jx
2
k
  x
2

C x   1
D T
lx
2
m
C T
jx
2
k
C x   1:
This is the rigorously correct Merge Sort recurrence valid for all input sizes,
complete with Ô¨Çoor and ceiling operators. In this case, the functions h1.x/ and
h2.x/ are both at most 1, which is easily O.x= log2 x/ as required by the theorem
statement. These functions hi do not affect ‚Äîor even appear in ‚Äîthe asymptotic
solution to the recurrence. This justiÔ¨Åes our earlier claim that applying Ô¨Çoor and
ceiling operators to the size of a subproblem does not alter the asymptotic solution
to a divide-and-conquer recurrence.
20.4.4
The Master Theorem
There is a special case of the Akra-Bazzi formula known as the Master Theorem
that handles some of the recurrences that commonly arise in computer science. It
is called the Master Theorem because it was proved long before Akra and Bazzi
arrived on the scene and, for many years, it was the Ô¨Ånal word on solving divide-
and-conquer recurrences. We include the Master Theorem here because it is still
widely referenced in algorithms courses and you can use it without having to know
anything about integration.
Theorem 20.4.2 (Master Theorem). Let T be a recurrence of the form
T .n/ D aT
n
b

C g.n/:
Case 1: If g.n/ D O

nlogb.a/ 
for some constant  > 0, then
T .n/ D ‚Äö

nlogb.a/
:
Case 2: If g.n/ D ‚Äö

nlogb.a/ logk.n/

for some constant k  0, then
T .n/ D ‚Äö

nlogb.a/ logkC1.n/

:
Case 3: If g.n/ D 

nlogb.a/C
for some constant  > 0 and ag.n=b/ < cg.n/
for some constant c < 1 and sufÔ¨Åciently large n, then
T .n/ D ‚Äö.g.n//:
The Master Theorem can be proved by induction on n or, more easily, as a corol-
lary of Theorem 20.4.1. We will not include the details here.

Chapter 20
Recurrences
804
20.5
A Feel for Recurrences
We‚Äôve guessed and veriÔ¨Åed, plugged and chugged, found roots, computed integrals,
and solved linear systems and exponential equations. Now let‚Äôs step back and look
for some rules of thumb. What kinds of recurrences have what sorts of solutions?
Here are some recurrences we solved earlier:
Recurrence
Solution
Towers of Hanoi
Tn D 2Tn 1 C 1
Tn  2n
Merge Sort
Tn D 2Tn=2 C n   1
Tn  n log n
Hanoi variation
Tn D 2Tn 1 C n
Tn  2  2n
Fibonacci
Tn D Tn 1 C Tn 2
Tn  .1:618 : : : /nC1=
p
5
Notice that the recurrence equations for Towers of Hanoi and Merge Sort are some-
what similar, but the solutions are radically different. Merge Sorting n D 64 items
takes a few hundred comparisons, while moving n D 64 disks takes more than
1019 steps!
Each recurrence has one strength and one weakness. In the Towers of Hanoi,
we broke a problem of size n into two subproblem of size n   1 (which is large),
but needed only 1 additional step (which is small). In Merge Sort, we divided the
problem of size n into two subproblems of size n=2 (which is small), but needed
.n   1/ additional steps (which is large). Yet, Merge Sort is faster by a mile!
This suggests that generating smaller subproblems is far more important to al-
gorithmic speed than reducing the additional steps per recursive call. For example,
shifting to the variation of Towers of Hanoi increased the last term from C1 to Cn,
but the solution only doubled. And one of the two subproblems in the Fibonacci
recurrence is just slightly smaller than in Towers of Hanoi (size n   2 instead of
n 1). Yet the solution is exponentially smaller! More generally, linear recurrences
(which have big subproblems) typically have exponential solutions, while divide-
and-conquer recurrences (which have small subproblems) usually have solutions
bounded above by a polynomial.
All the examples listed above break a problem of size n into two smaller prob-
lems. How does the number of subproblems affect the solution? For example,
suppose we increased the number of subproblems in Towers of Hanoi from 2 to 3,
giving this recurrence:
Tn D 3Tn 1 C 1
This increases the root of the characteristic equation from 2 to 3, which raises the
solution exponentially, from ‚Äö.2n/ to ‚Äö.3n/.

20.5. A Feel for Recurrences
805
Divide-and-conquer recurrences are also sensitive to the number of subproblems.
For example, for this generalization of the Merge Sort recurrence:
T1 D 0
Tn D aTn=2 C n   1:
the Akra-Bazzi formula gives:
Tn D
8
ÀÜ<
ÀÜ:
‚Äö.n/
for a < 2
‚Äö.n log n/
for a D 2
‚Äö.nlog a/
for a > 2:
So the solution takes on three completely different forms as a goes from 1.99
to 2.01!
How do boundary conditions affect the solution to a recurrence? We‚Äôve seen
that they are almost irrelevant for divide-and-conquer recurrences. For linear re-
currences, the solution is usually dominated by an exponential whose base is de-
termined by the number and size of subproblems. Boundary conditions matter
greatly only when they give the dominant term a zero coefÔ¨Åcient, which changes
the asymptotic solution.
So now we have a rule of thumb! The performance of a recursive procedure is
usually dictated by the size and number of subproblems, rather than the amount
of work per recursive call or time spent at the base of the recursion. In particular,
if subproblems are smaller than the original by an additive factor, the solution is
most often exponential. But if the subproblems are only a fraction the size of the
original, then the solution is typically bounded by a polynomial.

806
Bibliography
[1] Martin Aigner and G¬®unter M. Ziegler. Proofs from The Book. Springer-Verlag,
1999.
[2] Eric Bach and Jeffrey Shallit. EfÔ¨Åcient Algorithms, volume 1 of Algorithmic
Number Theory. The MIT Press, 1996.
[3] B¬¥ela Bollob¬¥as.
Modern Graph Theory, volume 184 of Graduate Texts in
Mathematics. Springer-Verlag, 1998.
[4] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford
Stein. Introduction to Algorithms. The MIT Press, third edition, 2009.
[5] Antonella Cupillari. The Nuts and Bolts of Proofs. Academic Press, fourth
edition, 2012.
[6] William Feller. An Introduction to Probability Theory and Its Applications.
Vol. I. Third edition. John Wiley & Sons Inc., New York, 1968.
[7] Edgar G. Goodaire and Michael M. Parmenter. Discrete Mathematics with
Graph Theory. Prentice Hall, second edition, 2001.
[8] Ronald L. Graham, Donald E. Knuth, and Oren Patashnik. Concrete Math-
ematics: A Foundation for Computer Science. Addison-Wesley, second edi-
tion, 1994.
[9] Charles M. Grinstead and J. Laurie Snell. Introduction to Probability. Amer-
ican Mathematical Society, second revised edition, 1997.
[10] Gary Haggard, John Schlipf, and Sue Whitesides. Discrete Mathematics for
Computer Science. Brooks Cole, 2005.
[11] John G. Michaels and Kenneth H. Rosen. Applications of Discrete Mathe-
matics. McGraw-Hill, 1991.
[12] G. Polya. How to Solve It: A New Aspect of Mathematical Method. Princeton
University Press, second edition, 1971.
[13] Sheldon M. Ross. Probability Models for Computer Science. Academic Press,
2001.
[14] Edward A. Scheinerman.
Mathematics: A Discrete Introduction.
Brooks
Cole, third edition, 2012.

BIBLIOGRAPHY
807
[15] Victor Shoup. A Computational Introduction to Number Theory and Algebra.
Cambridge University Press, 2005.
[16] Gilbert Strang. Introduction to Applied Mathematics. Wellesley-Cambridge
Press, Wellesley, Massachusetts, 1986.
[17] Herbert S. Wilf. generatingfunctionology. Academic Press, 1990.

Index
 , set difference, 74
.k1; k2; : : : ; km/-split of A, 500
Cn, 350, 371
IE, indicator for event E, 660
K3;3, 413
K5, 413
big omega, 473
‚Äö./, 470
Zn, 228
bij, 85
C, 74
;, 74
WWD, 5
 .mod n/, 223
Ex2≈íR¬ç, 715
8, 6
2, 6
inj, 84
Z, 74
Z , 74
\, 74
, 77
N, 6, 74
A, 75
ZC, 6
pow.A/, 75
Q, 74
R, 74
RC, 74
, 468
 (asymptotic equality), 462
strict, 85
, 74
, 74
surj, 84
[, 74
k-combinations, 502
k-edge connected, 372
k-to-1 function, 495
k-way independent, 630, 662
n C 1-bit adder, 130
r-permutation, 533
IQ, 708, 714
icr , 380
2-D Array, 340
2-Layer Array, 340
2-dimensional array, 329
5-choosable, 106
acyclic, 283
adjacency matrix, 279
adjacent, 346
Adleman, 241
Akra-Bazzi formula, 799
Akra-Bazzi Theorem, 801
alphabet, 152
annuity, 443, 444
antecedents, 10
antichain, 296, 314
antisymmetric, 287, 299
antisymmetry, 287
a posteriori, 621
arrows, 273
assignment statement, 124
asymmetric, 286, 312
asymmetry, 286
asymptotically equal, 462
asymptotically smaller, 468
asymptotic relations, 480
average, 670, 707
average degree, 348, 407
axiomatic method, 9
Axiom of Choice, 192
axioms, 4, 8

INDEX
809
Banach-Tarski, 192
base case, 102
basis step, 102
Bayes‚Äô Rule, 622
BeneÀás nets, 333
Bernoulli distribution, 665
Bernoulli variable, 715
Bernoulli variables, 660
biased, 755
bijection, 539
Bijection Rule, 487
bijective, 82
binary predicate, 56
binary relation, 80
Binary relations, 80
binary trees, 168
Binet‚Äôs formula, 136
binomial, 502
binomial coefÔ¨Åcient, 503
binomial coefÔ¨Åcients, 534, 536
binomial distribution, 665, 669, 718
Binomial Theorem, 502
bin packing, 726
bipartite graph, 353, 357, 393, 426
degree-constrained, 357
birthday principle, 634
blocks, 295
bogus proofs, 19
Bookkeeper Rule, 576
Boole‚Äôs inequality, 611
Boolean variables, 38
Borel-Cantelli Lemma, 751
bottleneck, 357
boundary conditions, 791
bridge, 422
Brin, Sergey, 273, 765
buildup error, 374
busy, 699, 700
butterÔ¨Çy, 331
butterÔ¨Çy net, 343
cancellable, 232
Cancellation, 231
Cantor‚Äôs paradise, 180, 193
cardinality, 84
carry bit, 59
CDO, 750
ceiling, 818
chain, 294, 314
chain of ‚Äúiff‚Äù, 14
characteristic equation, 794
characters, 152
Chebyshev‚Äôs bound, 741
Chebyshev‚Äôs Theorem, 711, 723
Chebyshev Bound, 749
Chebyshev bound, 740
Chernoff Bound, 726, 727
Chinese Appetizer problem, 709
Chinese Remainder Theorem, 257
Choice axiom, 191
chromatic number, 367
Church-Turing thesis, 220
closed forms, 443
closed walk, 277, 370
CML, 342, 343
CNF, 47
codomain, 77, 80
Cohen, 192
collateralized debt obligation, 750
colorable, 366
coloring, 366
solid, 382
combinatorial proof, 441, 525, 556,
557
common divisor, 208
communication nets, 273
commutative ring, 228
compilation, 185
complement, 75

INDEX
810
Complement Rule, 610
complete binary tree, 325
complete bipartite graph, 413
complete digraph, 303
complete graph, 349, 413
components, 77
composing, 79
composite, 214
composition, 79, 282, 301
concatenation, 152, 153, 278
conclusion, 10, 40
conditional expectation, 673
conditional probability, 614
conÔ¨Ådence, 746
conÔ¨Ådence level, 725, 745
congestion, 328, 343
congestion for min-latency, 342, 343
congestion of the network, 329
congruence, 223
congruent, 223
congruent modulo n, 223
Conjectured InefÔ¨Åciency of Factoring,
214
conjunctive form, 47
conjunctive normal form, 47, 50
connected, 371, 373
k-edge, 373
edge, 373
connected components, 372
connects, 346
consequent, 10
consistent, 192
continuous faces, 417
Continuum Hypothesis, 192
contrapositive, 13, 45
converse, 45
convex function, 732
Convolution, 563
convolution, 562
Convolution Counting Principle, 576
coprime, 230
corollary, 9
countable, 182, 192, 194
countably inÔ¨Ånite, 182
counter model, 57
coupon collector problem, 687
cover, 302, 356
covering edge, 302
critical path, 294, 295, 296
cumulative distribution function, 663
cut edge, 373
cycle, 277, 367, 370
of length n, 350
cycle of a graph, 371
DAG, 271, 302
de Bruijn sequences, 308
degree, 346
degree d linear recurrence, 574
degree d linear-recursive, 546
degree-constrained, 357, 515, 544
degree sequence, 539
DeMorgan‚Äôs Laws, 49
depth, 295
derived variables, 126
describable, 198
deviation from the mean, 707
diagonal argument, 185
diameter, 326
Die Hard, 206, 207
Difference Rule, 611
digraphs, 273
directed acyclic graph (DAG), 283
directed edge, 275
directed graph, 275
Directed graphs, 273
directed graphs, 271
discrete faces, 420
disjoint, 75

INDEX
811
disjunctive form, 46
disjunctive normal form, 47, 50
distance
between vertices, 278
Distributive Law, 76
distributive law, 47
divide-and-conquer, 799
divides, 203
divisibility relation, 275
divisible, 204
Division Rule, 495
Division Theorem, 205
divisor, 204
DNF, 47
domain, 55, 77, 80
domain of discourse, 55, 549
Dongles, 422
dot product, 741
double letter, 185
Double or nothing, 604
double summations, 465
drawing, 413
edge connected, 373
edge cover, 356
edges, 275, 346
efÔ¨Åcient solution, 51
elements, 73
Elkies, 6
ellipsis, 27
empty graph, 349, 367
empty relation, 308, 310, 312, 318,
322
empty sequence, 77
empty string, 68
end of chain, 294
endpoints, 346
end vertex, 275
Enigma, 229
equivalence class, 298, 322
equivalence classes, 322
equivalence relation, 297, 300, 322,
323
equivalence relations, 224
equivalent, 42
erasable, 175
Euclid, 8, 204, 245
Euclid‚Äôs Algorithm, 209
Euler, 6, 245
formula, 424
Euler‚Äôs  function, 234
Euler‚Äôs constant, 460
Euler‚Äôs formula, 431
Euler‚Äôs Theorem, 234
Euler‚Äôs theorem, 261
Euler circuit, 306
Euler tour, 306
evaluation function, 162
event, 595, 609
events, 659
exclusive-or, 39
execution, 118
existential, 53
expectation, 670
expected absolute deviation, 699, 714,
741
expected return, 677
expected value, 590, 670, 672, 707
exponential growth, 51
exponentially, 48, 51
extends F , 382
Extensionality, 190
face-down four-card trick, 545
factor, 204
factorial function, 444
factorials, 534, 536
fair, 677
fair game, 755
Fast Exponentiation, 124

INDEX
812
father, 530
Fermat‚Äôs Last Theorem, 7
Fermat‚Äôs Little Theorem, 240
Fermat‚Äôs theorem, 257
Fibonacci number, 134, 135
Fibonacci numbers, 31
Fibonacci recurrence, 791
Fifteen Puzzle, 144
Ô¨Çoor, 818
Floyd‚Äôs Invariant Principle, 114
Ô¨Çush, 547
Foundation, 190
Four Step Method, 637
four-step method, 640
Frege, 191
Frege, Gotlob, 188
function, 77, 82
Fundamental Theorem of Arithmetic,
217
G¬®odel, 192
Gale, 364
Gauss, 222, 223
gcd, 208
general binomial density function, 670
Generalized Pigeonhole Principle, 510
Generalized Product Rule, 492
generating function, 575, 582
Generating Functions, 559
geometric distribution, 677, 677
geometric series, 559
geometric sum, 443
Goldbach‚Äôs Conjecture, 54, 55
Goldbach Conjecture, 215
golden ratio, 210, 247
good count, 177, 583, 583
Google, 755
google, 36
graph
bipartite, 353
coloring problem, 366
matching, 356
perfect, 356
shortest path, 281
valid coloring, 366
graph coloring, 366
graph of R, 80
gray edge, 382
greatest common divisor, 208
greatest common divisors, 203
grid, 329
grows unboundedly, 22
guess-and-verify, 781
half-adder, 59
Hall‚Äôs Matching Theorem, 354
Hall‚Äôs Theorem, 357, 544
Hall‚Äôs theorem, 393
Halting Problem, 185
Handshake Lemma, 349
Hardy, 203, 220
Harmonic number, 459
Hat-Check problem, 709
head, 275
Herman Rubin, 730
homogeneous linear recurrence, 791
homogeneous solution, 795
hypercube, 401
hypothesis, 40
identity relation, 312, 322
image, 79, 83
implications, 11
incident, 346
Inclusion-Exclusion, 519, 521
inclusion-exclusion for probabilities,
611
Inclusion-Exclusion Rule, 518
increasing subsequence, 320
in-degree, 275

INDEX
813
independence, 626
independent, 717
independent random variables, 661
indicator random variable, 660
indicator variable, 672, 715, 739
indicator variables, 662
indirect proof, 16
Induction, 99
induction hypothesis, 102
inductive step, 102
inference rules, 10
inÔ¨Ånite, 179
InÔ¨Ånity axiom, 190
inÔ¨Åx notation, 80
inhomogeneous linear recurrence, 795
injective, 82
integer linear combination, 205
intended proÔ¨Åt, 755
interest rate, 475
interpreters, 185
intersection, 74
interval, 206
Invariant, 206
invariant, 114
inverse, 83, 90
inverse image, 83
irrational, 13
irreducible, 249
irreÔ¨Çexive, 285, 299, 312
irreÔ¨Çexivity, 285
isomorphic, 288, 311, 436
King Chicken Theorem, 305
known-plaintext attack, 233
latency, 328
latency for min-congestion, 342, 343
Latin square, 390
lattice basis reduction, 512
Law of Large Numbers, 723
leaf, 377
least common multiple, 245
lemma, 9
length-n cycle, 350
length-n walk relation, 283
length of a walk, 370
Let‚Äôs Make a Deal, 636
letters, 152
linear combination, 205
Linearity of Expectation, 682, 683
linear orders, 289
literal, 702
LMC, 342, 343
load balancing, 725, 729
logical deductions, 4
logical formulas, 4
lower bound, 29
lowest terms, 25
Mapping Rules, 487, 509
Markov‚Äôs bound, 741
Markov‚Äôs Theorem, 708, 737
Markov Bound, 749
Markov bound, 731
matched string, 155
matching, 354, 356
matching birthdays, 721
matching condition, 355
mathematical proof, 4
matrix multiplication, 470
maximal, 292
maximum, 292
maximum dilation, 776
mean, 14, 670
mean square deviation, 711
Menger, 373
merge, 277
Merge Sort, 786
merging vertices, 432
minimal, 201, 292, 292

INDEX
814
minimum, 292
minimum weight spanning tree, 380
minor, 432
modus ponens, 10
Monty Hall Problem, 591
multigraphs, 347
multinomial coefÔ¨Åcient, 500
multinomials, 503
Multinomial Theorem, 556
multiple, 204
multiplicative, 258
multiplicative inverse, 230
Multiplicative Inverses and Cancelling,
230
multisets, 73
Murphy‚Äôs Law, 733, 749
mutual independence, 717
mutually independent, 627, 654, 662,
721, 727
mutually recursive, 582
MySQL, 94
neighbors, 357, 387
network latency, 328
node, 275, 346
nodes, 347
nonconstant polynomial, 22
nonconstructive proof, 512
nondecreasing, 452
nondeterministic polynomial time, 52
nonincreasing, 453
non-unique factorization, 249
norm, 250, 741
not primes, 22
numbered tree, 530
numbered trees, 539
number of processors, 295
Number theory, 203
o(), asymptotically smaller, 468
O(), big oh, 469
o(), little oh, 468
one-sided Chebyshev bound, 741
optimal spouse, 363
order, 236, 791
order over Zn, 236
ordinary induction, 100
outcome, 593, 609
out-degree, 275
outside face, 417
overhang, 456
packet, 325
Page, Larry, 273, 765
page rank, 766, 768
Pairing, 190
pairwise disjoint, 200
pairwise independence, 717
pairwise independent, 630, 632, 718,
721
Pairwise Independent Additivity, 718
Pairwise Independent Sampling, 722,
744
parallel schedule, 295
parallel time, 296
parity, 145
partial correctness, 123
partial fractions, 567
partial functions, 79
partial order, 311
particular solution, 795
partition, 295, 322, 353
partitions, 298
Pascal‚Äôs Identity, 525
path, 694
path-total, 300
perfect graph, 356
perfect number, 204, 245
permutation, 438, 494, 535
Perturbation Method, 445

INDEX
815
perturbation method, 559
pessimal spouse, 363
Pick-4, 728
pigeonhole principle, 441
planar drawing, 413
planar embedding, 419, 420, 436
planar graph, 417
planar graphs, 369
planar subgraph, 427
plug-and-chug, 781
pointwise, 79
Polyhedra, 429
polyhedron, 429
polynomial growth, 51
polynomial time, 51, 214, 353
population size, 724
positive walk relation, 282
potential, 138
power set, 75, 88, 183
Power Set axiom, 190
Power sets, 183
predicate, 8
pre-MST, 382
preserved, 225
preserved invariant, 118
preserved under isomorphism, 352
prime, 5, 214
prime factorization, 246
Prime Factorization Theorem, 28
Prime Number Theorem, 215, 235
private key, 241
probability density function, 663
probability density function,, 662
probability function, 609, 646
probability of an event, 609
probability space, 609
product of sets, 77
Product Rule, 489, 618
product rule, 654
Product Rule for generating functions,
562
proof, 9
proof by contradiction, 16
proposition, 4, 5
propositional variables, 38
public key, 241
public key cryptography, 241
Pulverizer, 246
Pythagoreans, 429
quotient, 206
Rabin cryptosystem, 265
randomized, 589
randomized algorithm, 668
random sample, 747
random sampling, 745
random variable, 659
random variables, 660
random walk, 694, 767
Random Walks, 755
range, 79
rank, 535
rational, 13, 17
rational functions, 173
reachable, 118
recognizable, 185
recognizes, 185
recurrence, 781
Recursive data types, 151
recursive deÔ¨Ånitions, 151
reÔ¨Çexive, 282, 299
regular polyhedron, 429
relational databases, 94
relation on a set, 80
relatively prime, 230
relaxed, 699, 700
remainder, 206
Replacement axiom, 190

INDEX
816
reversal, 167
reverse-linear, 546
Riemann Hypothesis, 235, 235
ring of integers modulo n, 228
ripple-carry, 59
ripple-carry circuit, 130
Rivest, 241
root mean square, 713
round-robin tournament, 304
routing, 326
routing problem, 326
RSA, 241, 264
RSA public key crypto-system, 203
Russell, 188, 191
Russell‚Äôs Paradox, 188, 191
sample space, 593, 609
sampling, 745
SAT, 51
satisÔ¨Åable, 46, 51, 63, 702
SAT-solvers, 52
scheduled at step k, 295
Schr¬®oder-Bernstein, 181, 194
secret key, 221
self-loop, 347
self-loops, 277
sequence, 77
set, 73
covering, 356
set difference, 74, 87
Shamir, 241
Shapley, 364
simple graph, 346
Simple graphs, 345
simple graphs, 271
sink, 772
smallest counterexample, 27
solid coloring, 382
solves, 326
sound, 10
spanning subgraph, 380
spanning tree, 379
Square Multiple Rule, 716
St. Petersburg Paradox, 736
St. Petersburg paradox, 754
stable distributions, 772
stable matching, 359
stable stack, 456
standard deviation, 713, 714, 717
start vertex, 275
state graph, 114
state machines, 29, 271
stationary distribution, 768
Stirling‚Äôs formula, 693
strictly bigger, 183
strictly decreasing, 126, 453
Strictly increasing, 126
strictly increasing, 452
strict partial order, 285, 300
string procedure, 185
Strong Induction, 108
strongly connected, 776
Structural induction, 153
structural induction, 151, 172
subsequence, 320
subset, 74
subset relation, 311
substitution function, 163
suit, 535
summation notation, 27
Sum Rule, 490, 610
surjective, 82
switches, 325
symbols, 152
symmetric, 271, 299, 345, 776
tail, 275
tails, 669

INDEX
817
tails of the distribution, 669
terminals, 325
terms, 77
theorems, 9
topological sort, 292
total, 82
total expectation, 675
total function, 79
totient function, 234
tournament digraph, 303, 304, 697
Towers of Hanoi, 581, 783
trail, 307
transition, 114
transition relation, 114
transitive, 282, 300, 312, 606
Traveling Salesman Problem, 306, 404
tree diagram, 593, 640
truth tables, 38
Turing, 219, 220, 233
Turing‚Äôs code, 220, 228, 233
Twin Prime Conjecture, 214
type-checking, 185, 187
unbiased, 755
unbiased binomial distribution, 669,
699
unbounded Gambler‚Äôs ruin, 763
uncountable, 197, 200
undirected, 345
undirected edge, 346
uniform, 602, 611, 665
uniform distribution, 665
union, 74
Union axiom, 190
Union Bound, 611
unique factorization, 246
unique factorizations, 249
Unique Factorization Theorem, 217
universal, 53
unlucky, 699, 700
upper bound, 29
valid, 46
valid coloring, 366
value of an annuity, 446
variance, 711, 720, 739
Venn diagram, 654
vertex, 275, 346
vertex connected, 373
vertices, 275, 346
virtual machines, 185
walk, 404
walk counting matrix, 280
walk in a digraph, 276
walk in a simple graph, 370
walk relation, 282
Weak Law of Large Numbers, 723,
745
weakly connected, 307
weakly decreasing, 126, 137, 217, 453
weakly increasing, 126, 452
weak partial order, 300
well founded, 201
well ordered, 29
Well Ordering, 109
Well Ordering Principle, 25, 101, 113
width, 398
winnings, 677
wrap, 584
Zermelo, 191
Zermelo-Frankel, 9
Zermelo-Frankel Set Theory, 189
ZFC, 9, 189, 192
ZFC axioms, 191

INDEX
818
Glossary of Symbols
symbol
meaning
WWD
is deÔ¨Åned to be
¬§
not equal
^
and, AND
_
or, OR
 !
implies, if ..., then    , IMPLIES
 !
state transition
:P; P
not P , NOT.p/
 !
iff, equivalent, IFF
Àö
xor, exclusive-or, XOR
9
exists
8
for all
2
is a member of, is in

is a (possibly =) subset of
6
is not a (possibly =) subset of

is a proper (not =) subset of
6
is not a proper (not =) subset of
[
set union
\
set intersection
A
complement of set A
 set difference
pow.A/
powerset of set, A
;
the empty set, f g
Z
integers
N; Z0
nonnegative integers
ZC; NC
positive integers
Z negative integers
Q
rational numbers
R
real numbers
C
complex numbers
brc
the Ô¨Çoor of r: the greatest integer  r,
dre
the ceiling of r: the least integer  r,
R.X/
image of set X under binary relation R
R 1
inverse of binary relation R
R 1.X/
inverse image of set X under relation R

INDEX
819
symbol
meaning
surj
A surj B iff 9f W A ! B: f is a surjective function
inj
A inj B iff 9R W A ! B: R is an injective relation
bij
A bij B iff 9f W A ! B: f is a bijection
≈í 1 in¬ç
injective property of a relation
≈í 1 in¬ç
surjective property of a relation
≈í 1 out¬ç
function property of a relation
≈í 1 out¬ç
total property of a relation
≈íD 1 out; D 1 in¬ç
bijection relation

the empty string/list
A
the Ô¨Ånite strings over alphabet A
rev.s/
the reversal of string s
s  t
concatenation of strings s; t; append.s; t/
#c.s/
number of occurrences of character c in string s
m j n
integer m divides integer n; m is a factor of n
gcd
greatest common divisor
lcm
least common multiple
.k; n/
fi j k < i < ng
≈ík; n/
fi j k  i < ng
.k; n¬ç
fi j k < i  ng
≈ík; n¬ç
fi j k  i  ng
 .mod n/
congruence modulo n
6
not congruent
Zn
the ring of integers modulo n
Cn; n
addition and multiplication operations in Zn
km .Zn/
mth power of k in Zn
gcd1fng
the set of numbers in ≈í0; n/ relatively prime to n
.n/
Euler‚Äôs totient function D j gcd1fngj
ord.k; n/
the order of k in Zn
hu!vi
directed edge from vertex u to vertex v
IdA
identity relation on set A: aIdAa0 iff a D a0
R
path relation of relation R; reÔ¨Çexive transitive closure of R
RC
positive path relation of R; transitive closure of R
hu‚Äîvi
undirected edge connecting vertices u ¬§ v
E.G/
the edges of graph G
V.G/
the vertices of graph G

INDEX
820
symbol
meaning
Cn
the length-n undirected cycle
Ln
the length-n line graph
Kn
the n-vertex complete graph
Hn
the n-dimensional hypercube
L.G/
the ‚Äúleft‚Äù vertices of bipartite graph G
R.G/
the ‚Äúright‚Äù vertices of bipartite graph G
Kn;m
the complete bipartite graph with n left and m right vertices
Hn
the nth Harmonic number Pn
iD1 1=i

asymptotic equality
n≈†
n factorial WWDn  .n   1/    2  1
o./
asymptotic notation ‚Äúlittle oh‚Äù
O./
asymptotic notation ‚Äúbig oh‚Äù
‚Äö./
asymptotic notation ‚ÄúTheta‚Äù
./
asymptotic notation ‚Äúbig Omega‚Äù
!./
asymptotic notation ‚Äúlittle omega‚Äù
Pr≈íA¬ç
probability of event A
Pr

A j B

conditional probability of A given B
IA
indicator variable for event A
Ex≈íR¬ç
expectation of random variable R
Ex≈íR j A¬ç
conditional expectation of R given event A
Ex2≈íR¬ç
abbreviation for .Ex≈íR¬ç/2
Var≈íR¬ç
variance of R
R
standard deviation of R

