
Topics in
Quaternion Linear Algebra

Princeton Series in Applied Mathematics
Editors
Ingrid Daubechies (Princeton University); Weinan E (Princeton University); Jan
Karel Lenstra (Eindhoven University); Endre S¨uli (University of Oxford)
The Princeton Series in Applied Mathematics publishes high quality advanced texts
and monographs in all areas of applied mathematics.
Books include those of a
theoretical and general nature as well as those dealing with the mathematics of
speciﬁc applications areas and real-world situations.
A list of titles in this series appears at the back of the book.

Topics in
Quaternion Linear Algebra
Leiba Rodman
Princeton University Press
Princeton and Oxford

Copyright
c⃝2014 by Princeton University Press
Published by Princeton University Press, 41 William Street, Princeton, New Jersey
08540
In the United Kingdom: Princeton University Press, 6 Oxford Street, Woodstock,
Oxfordshire OX20 1TW
press.princeton.edu
All rights reserved
Library of Congress Cataloging-in-Publication Data
Rodman, L.
Topics in quaternion linear algebra / Leiba Rodman.
pages cm. – (Princeton series in applied mathematics)
Includes bibliographical references and index.
ISBN 978-0-691-16185-3 (hardcover)
1. Algebras, Linear–Textbooks. 2. Quaternions–Textbooks. I. Title.
QA196.R63 2014
512′.5–dc23
2013050581
British Library Cataloging-in-Publication Data is available
The publisher would like to acknowledge the author of this volume for providing
the camera-ready copy from which this book was printed.
This book has been composed in LATEX
Printed on acid-free paper ∞
Printed in the United States of America
10 9 8 7 6 5 4 3 2 1

To Ella

This page intentionally left blank 

Contents
Preface
xi
1
Introduction
1
1.1
Notation and conventions
5
1.2
Standard matrices
7
2
The algebra of quaternions
9
2.1
Basic deﬁnitions and properties
9
2.2
Real linear transformations and equations
11
2.3
The Sylvester equation
14
2.4
Automorphisms and involutions
17
2.5
Quadratic maps
21
2.6
Real and complex matrix representations
23
2.7
Exercises
24
2.8
Notes
26
3
Vector spaces and matrices: Basic theory
28
3.1
Finite dimensional quaternion vector spaces
28
3.2
Matrix algebra
30
3.3
Real matrix representation of quaternions
33
3.4
Complex matrix representation of quaternions
36
3.5
Numerical ranges with respect to conjugation
38
3.6
Matrix decompositions: nonstandard involutions
44
3.7
Numerical ranges with respect to nonstandard involutions
47
3.8
Proof of Theorem 3.7.5
52
3.9
The metric space of subspaces
56
3.10 Appendix: Multivariable real analysis
59
3.11 Exercises
61
3.12 Notes
63
4
Symmetric matrices and congruence
64
4.1
Canonical forms under congruence
64
4.2
Neutral and semideﬁnite subspaces
69
4.3
Proof of Theorem 4.2.6
72
4.4
Proof of Theorem 4.2.7
75
4.5
Representation of semideﬁnite subspaces
78
4.6
Exercises
80
4.7
Notes
82

viii
CONTENTS
5
Invariant subspaces and Jordan form
83
5.1
Root subspaces
83
5.2
Root subspaces and matrix representations
85
5.3
Eigenvalues and eigenvectors
90
5.4
Some properties of Jordan blocks
94
5.5
Jordan form
97
5.6
Proof of Theorem 5.5.3
102
5.7
Jordan forms of matrix representations
109
5.8
Comparison with real and complex similarity
111
5.9
Determinants
113
5.10 Determinants based on real matrix representations
115
5.11 Linear matrix equations
116
5.12 Companion matrices and polynomial equations
119
5.13 Eigenvalues of hermitian matrices
123
5.14 Diﬀerential and diﬀerence equations
123
5.15 Appendix: Continuous roots of polynomials
126
5.16 Exercises
127
5.17 Notes
130
6
Invariant neutral and semideﬁnite subspaces
131
6.1
Structured matrices and invariant neutral subspaces
132
6.2
Invariant semideﬁnite subspaces respecting conjugation
136
6.3
Proof of Theorem 6.2.6
139
6.4
Unitary, dissipative, and expansive matrices
143
6.5
Invariant semideﬁnite subspaces: Nonstandard involution
146
6.6
Appendix: Convex sets
148
6.7
Exercises
149
6.8
Notes
151
7
Smith form and Kronecker canonical form
153
7.1
Matrix polynomials with quaternion coeﬃcients
153
7.2
Nonuniqueness of the Smith form
158
7.3
Statement of the Kronecker form
161
7.4
Proof of Theorem 7.3.2: Existence
163
7.5
Proof of Theorem 7.3.2: Uniqueness
167
7.6
Comparison with real and complex strict equivalence
169
7.7
Exercises
170
7.8
Notes
171
8
Pencils of hermitian matrices
172
8.1
Canonical forms
172
8.2
Proof of Theorem 8.1.2
177
8.3
Positive semideﬁnite linear combinations
181
8.4
Proof of Theorem 8.3.3
183
8.5
Comparison with real and complex congruence
187
8.6
Expansive and plus-matrices: Singular H
188
8.7
Exercises
191
8.8
Notes
192

CONTENTS
ix
9
Skewhermitian and mixed pencils
194
9.1
Canonical forms for skewhermitian matrix pencils
194
9.2
Comparison with real and complex skewhermitian pencils
197
9.3
Canonical forms for mixed pencils: Strict equivalence
199
9.4
Canonical forms for mixed pencils: Congruence
202
9.5
Proof of Theorem 9.4.1: Existence
205
9.6
Proof of Theorem 9.4.1: Uniqueness
210
9.7
Comparison with real and complex pencils: Strict equivalence
215
9.8
Comparison with complex pencils: Congruence
219
9.9
Proofs of Theorems 9.7.2 and 9.8.1
221
9.10 Canonical forms for matrices under congruence
224
9.11 Exercises
226
9.12 Notes
227
10 Indeﬁnite inner products: Conjugation
228
10.1 H-hermitian and H-skewhermitian matrices
229
10.2 Invariant semideﬁnite subspaces
232
10.3 Invariant Lagrangian subspaces I
235
10.4 Diﬀerential equations I
238
10.5 Hamiltonian, skew-Hamiltonian matrices: Canonical forms
242
10.6 Invariant Lagrangian subspaces II
246
10.7 Extension of subspaces
248
10.8 Proofs of Theorems 10.7.2 and 10.7.5
250
10.9 Diﬀerential equations II
255
10.10 Exercises
257
10.11 Notes
259
11 Matrix pencils with symmetries: Nonstandard involution
261
11.1 Canonical forms for φ-hermitian pencils
261
11.2 Canonical forms for φ-skewhermitian pencils
263
11.3 Proof of Theorem 11.2.2
266
11.4 Numerical ranges and cones
274
11.5 Exercises
277
11.6 Notes
278
12 Mixed matrix pencils: Nonstandard involutions
279
12.1 Canonical forms for φ-mixed pencils: Strict equivalence
279
12.2 Proof of Theorem 12.1.2
281
12.3 Canonical forms of φ-mixed pencils: Congruence
284
12.4 Proof of Theorem 12.3.1
287
12.5 Strict equivalence versus φ-congruence
290
12.6 Canonical forms of matrices under φ-congruence
291
12.7 Comparison with real and complex matrices
292
12.8 Proof of Theorem 12.7.4
294
12.9 Exercises
298
12.10 Notes
299

x
CONTENTS
13 Indeﬁnite inner products: Nonstandard involution
300
13.1 Canonical forms: Symmetric inner products
301
13.2 Canonical forms: Skewsymmetric inner products
306
13.3 Extension of invariant semideﬁnite subspaces
309
13.4 Proofs of Theorems 13.3.3 and 13.3.4
313
13.5 Invariant Lagrangian subspaces
316
13.6 Boundedness of solutions of diﬀerential equations
321
13.7 Exercises
325
13.8 Notes
327
14 Matrix equations
328
14.1 Polynomial equations
328
14.2 Bilateral quadratic equations
331
14.3 Algebraic Riccati equations
332
14.4 Exercises
337
14.5 Notes
338
15 Appendix: Real and complex canonical forms
339
15.1 Jordan and Kronecker canonical forms
339
15.2 Real matrix pencils with symmetries
341
15.3 Complex matrix pencils with symmetries
348
Bibliography
353
Index
361

Preface
This is probably the ﬁrst book devoted entirely to linear algebra and matrix analysis
over the skew ﬁeld of real quaternions.
The book is intended for the primary audience of mathematicians working in the
area of linear algebra and matrix analysis, instructors and students of these subjects,
mathematicians working in related areas such as operator theory and diﬀerential
equations, researchers who work in other areas and for whom the book is intended as
a reference, and scientists (primarily physicists, chemists, and computer scientists)
and engineers who may use the book as a reference as well.
The exposition is accessible to upper undergraduate and graduate students in
mathematics, science, and engineering. A background in college linear algebra and
a modicum of complex analysis and multivariable calculus will suﬃce.
I intend to keep up with the use of the book.
So, I have a request of the
readers: please send remarks, corrections, criticism, etc., concerning the book to
me at lxrodm@gmail.com or lxrodm@math.wm.edu.
I thank J. Baez for consultation concerning automorphisms of the division alge-
bra of real quaternions, R. M. Guralnick and R. Pereira for consultations concerning
invariants for equivalence of matrices over noncommutative principal ideal domains
(in particular polynomials with quaternion coeﬃcients), C.-K. Li and Y.-T. Poon
for consultations concerning numerical ranges, F. Zhang for helping out with deter-
minants, and V. Bolotnikov. In the ﬁnal stages of preparation of the manuscript I
took advice from several people whose input is greatly appreciated: P. Lancaster,
N. J. Higham, H. Schneider, R. Brualdi, and H. J. Woerdeman. I also thank M.
Karow and two anonymous reviewers for careful reading of the manuscript and
many helpful suggestions.
Leiba Rodman
Williamsburg, Virginia, September 2013

xii
PREFACE
2000 Mathematics Subject Classiﬁcation: 15-01, 15-02, 15A21, 15A63, 15A22,
15A60, 15B57, 15A66, 15A99.
Key words: quaternion, quaternion matrix, canonical form, numerical range,
similarity, congruence, Smith form, Jordan form, Kronecker form, invariant sub-
spaces, semideﬁnite subspaces, matrix pencils, hermitian matrix, Hamiltonian ma-
trix, skewhermitian matrix, skew-Hamiltonian matrix, symmetric matrix.

Topics in
Quaternion Linear Algebra

This page intentionally left blank 

Chapter One
Introduction
Besides the introduction, front matter, back matter, and Appendix (Chapter 15),
the book consists of two parts. The ﬁrst part comprises Chapters 2–7. Here, fun-
damental properties and constructions of linear algebra are explored in the context
of quaternions, such as matrix decompositions, numerical ranges, Jordan and Kro-
necker canonical forms, canonical forms under congruence, determinants, invariant
subspaces, etc. The exposition in the ﬁrst part is on the level of an upper under-
graduate or graduate textbook. The second part comprises Chapters 8–14. Here,
the emphasis is on canonical forms of quaternion matrix pencils with symmetries
or, what is the same, pairs of matrices with symmetries, and the exposition ap-
proaches that of a research monograph. Applications are given to systems of linear
diﬀerential equations with symmetries, and matrix equations.
The mathematical tools used in the book are easily accessible to undergraduates
with a background in linear algebra and rudiments of complex analysis and, on
occasion, multivariable calculus. The exposition is largely based on tools of matrix
analysis.
The author strived to make the book self-contained and inclusive of
complete proofs as much as possible, at the same time keeping the size of the book
within reasonable limits. However, some compromises were inevitable here. Thus,
proofs are often omitted for many linear algebra results that are standard for real
and complex matrices, are often presented in textbooks, and are valid for quaternion
matrices as well with essentially the same proofs.
The book can be used in a variety of ways. More than 200 exercises are pro-
vided, on various levels of diﬃculty, ranging from routine veriﬁcation of facts and
numerical examples designed to illustrate the results to open-ended questions. The
exercises and detailed exposition make the book suitable in teaching as supplemen-
tary material for undergraduate courses in linear algebra, as well as for students’
independent study or reading courses. For students’ beneﬁt, several appendices
are included that contain background material used in the main text. The book
can serve as a basis for a graduate course named advanced linear algebra, topics in
linear algebra, or (for those who want to keep the narrower focus) quaternion linear
algebra. For example, one can build a graduate course based on Chapters 2–8 and
selections from later chapters.
Open problems presented in the book provide an opportunity to do original
research.
The open problems are on various levels: open-ended problems that
may serve as subject for research by mathematicians and concrete, more-speciﬁc
problems that are perhaps more suited for undergraduate research work under
faculty supervision, honors theses, and the like.
For working mathematicians in both theoretical and applied areas, the book
may serve as a reference source. Such areas include, in particular, vector calcu-
lus, ordinary and partial diﬀerential equations, and boundary value problems (see,
e.g., G¨urlebeck and Spr¨ossig [60]), and numerical analysis (Bunse-Gerstner et al.
[22]). The accessibility and importance of the mathematics should make this book

2
CHAPTER 1
a widely useful work not only for mathematicians, but also for scientists and engi-
neers.
Quaternions have become increasingly useful for practitioners in research, both
in theory and applications.
For example, a signiﬁcant number of research pa-
pers on quaternions, perhaps even most of them, appear regularly in mathematical
physics journals, and quantum mechanics based on quaternion analysis is main-
stream physics. In engineering, quaternions are often used in control systems, and
in computer science they play a role in computer graphics. Quaternion formalism
is also used in studies of molecular symmetry. For practitioners in these areas, the
book can serve as a valuable reference tool.
New, previously unpublished results presented in the book with complete proofs
will hopefully be useful for experts in linear algebra and matrix analysis. Much of
the material appears in a book for the ﬁrst time; this is true for Chapters 5–14,
most of Chapter 4, and a substantial part of Chapter 3.
As far as the author is aware, this is the ﬁrst book dedicated to systematic
exposition of quaternion linear algebra. So far, there are only a few expository
papers and chapters in books on the subject (for example, Chapter 1 in G¨urlebeck
and Spr¨ossig [60], Brieskorn [20], Zhang [164], or Farenick and Pidkowich [38]) as
well as algebraic treatises on skew ﬁelds (e.g., Cohn [29] or Wan [156]).
It is inevitable that many parts of quaternion linear algebra are not reﬂected
in the book, most notably those parts pertaining to numerical analysis (Bunse-
Gerstner et al. [22] and Faßbender et al. [40]). Also, the important classes of
orthogonal, unitary, and symplectic quaternion matrices are given only brief expo-
sure.
We now describe brieﬂy the contents of the book chapter by chapter.
Chapter 2 concerns (scalar) quaternions and the basic properties of quaternion
algebra, with emphasis on solution of equations such as axb = c and ax −xb = c.
Description of all automorphisms and antiautomoprhisms of quaternions is given,
and representations of quaternions in terms of 2 × 2 complex matrices and 4 × 4
real matrices are introduced. These representations will play an important role
throughout the book.
Chapter 3 covers basics on the vector space of columns with quaternion com-
ponents, matrix algebra, and various matrix decomposition. The real and complex
representations of quaternions are extended to vectors and matrices. Various ma-
trix decompositions are studied; in particular, Cholesky factorization is proved for
matrices that are hermitian with respect to involutions other than the conjugation.
A large part of this chapter is devoted to numerical ranges of quaternion matrices
with respect to conjugation as well as with respect to other involutions. Finally, a
brief exposition is given for the set of quaternion subspaces, understood as a metric
space with respect to the metric induced by the gap function.
In a short Chapter 4 we develop diagonal canonical forms and prove inertia
theorems for hermitian and skewhermitian matrices with respect to involutions
(including the conjugation).
We also identify dimensions of subspaces that are
neutral or semideﬁnite relative to a given hermitian matrix and are maximal with
respect to this property. The material in Chapters 3 and 4 does not depend on the
more involved constructions such as the Jordan form and its proof.
Chapter 5 is a key chapter in the book.
Root subspaces of quaternion ma-
trices are introduced and studied.
The Jordan form of a quaternion matrix is
presented in full detail, including a complete proof. The complex matrix represen-
tation plays a crucial role here. Although the standard deﬁnition of a determinant

INTRODUCTION
3
is not very useful when applied to quaternion matrices, nevertheless several notions
of determinant-like functions for matrices over quaternions have been deﬁned and
used in the literature; a few of these are explored in this chapter as well. Several
applications of the Jordan form are treated. These include matrix equations of the
form AX −XB = C, functions of matrices, and stability of systems of diﬀerential
equations of the form
Aℓx(ℓ)(t) + Aℓ−1x(ℓ−1)(t) + · · · + A1x′(t) + A0x(t) = 0,
t ∈R,
with constant quaternion matrix coeﬃcients Aℓ, . . . , A0. Stability of an analogous
system of diﬀerence equations is studied as well.
The main theme of Chapter 6 concerns subspaces that are simultaneously in-
variant for one matrix and semideﬁnite (or neutral) with respect to another. Such
subspaces show up in many applications, some of them presented later in Chapter
13. For a given invertible plus-matrix A, the main result here asserts that any
subspace which is A-invariant and at the same time nonnegative with respect to
the underlying indeﬁnite inner product can be extended to an A-invariant subspace
which is maximal nonnegative. Analogous results are proved for related classes of
matrices, such as unitary and dissipative, as well as in the context of indeﬁnite inn
er products induced by involutions other than the conjugation.
Chapter 7 treats matrix polynomials with quaternion coeﬃcients. A diagonal
form (known as the Smith form) is proved for such polynomials. In contrast to
matrix polynomials with real or complex coeﬃcients, a Smith form is generally not
unique. For matrix polynomials of ﬁrst degree, a Kronecker form—the canonical
form under strict equivalence—is available, which is presented with a complete
proof.
Furthermore, a comparison is given for the Kronecker forms of complex
or real matrix polynomials with the Kronecker forms of such matrix polynomials
under strict equivalence using quaternion matrices.
In Chapters 8, 9, and 10 we develop canonical forms of quaternion matrix pencils
A + tB in which the matrices A and B are either hermitian or skewhermitian and
their applications. Chapter 8 is concerned with the case when both matrices A and
B are hermitian. Full and detailed proofs of the canonical forms under strict equiv-
alence and simultaneous congruence are provided, based on the Kronecker form of
the pencil A + tB. Several variations of the canonical forms are included as well.
Among applications here: criteria for existence of a nontrivial positive semideﬁnite
real linear combination and suﬃcient conditions for simultaneous diagonalizability
of two hermitian matrices under simultaneous congruence. A comparison is made
with pencils of real symmetric or complex hermitian matrices. It turns out that two
pencils of real symmetric matrices are simultaneously congruent over the reals if
and only if they are simultaneously congruent over the quaternions. An analogous
statement holds true for two pencils of complex hermitian matrices.
The subject matter of Chapter 9 is concerned mainly with matrix pencils of
the form A + tB, where one of the matrices A or B is skewhermitian and the
other may be hermitian or skewhermitian. Canonical forms of such matrix pencils
are given under strict equivalence and under simultaneous congruence, with full
detailed proofs, again based on the Kronecker forms. Comparisons with real and
complex matrix pencils are presented. In contrast to hermitian matrix pencils, two
complex skewhermitian matrix pencils that are simultaneously congruent under
quaternions need not be simultaneously congruent under the complex ﬁeld, although
an analogous property is valid for pencils of real skewsymmetric matrices. Similar

4
CHAPTER 1
results hold for real or complex matrix pencils A+tB, where A is real symmetric or
complex hermitian and B is real skewsymmetric or complex skewhermitian. In each
case, we sort out the relationships of simultaneous congruence over the complex
ﬁeld of complex matrix pencils where one matrix is hermitian and the other is
skewhermitian versus simultaneous congruence over the skew ﬁeld of quaternions
for such pencils.
As an applications we obtain a canonical form for quaternion
matrices under (quaternion) congruence.
In Chapter 10 we study matrices (or linear transformations) that are selfadjoint
or skewadjoint with respect to a nondegenerate hermitian or skewhermitian inner
product. As an application of the canonical forms obtained in Chapters 8 and 9,
canonical forms for such matrices are derived. Matrices that are skewadjoint with
respect to skewhermitian inner products are known as Hamiltonian matrices; they
play a key role in many applications such as linear control systems (see Chapter 14).
The canonical forms allow us to study invariant Lagrangian subspaces; in particular,
they give criteria for existence of such subspaces.
Another application involves
boundedness and stable boundedness of linear systems of diﬀerential equations
with constant coeﬃcients under suitable symmetry requirements.
The development of material in Chapters 11, 12, and 13 is largely parallel to
that in Chapters 8, 9, and 10, but with respect to an involution of the quaternions
other than the conjugation and with respect to indeﬁnite inner products induced
by matrices that are hermitian or skewhermitian with respect to such involutions.
Thus, letting φ be a ﬁxed involution of the quaternions which is diﬀerent from the
conjugation, the canonical forms (under both strict equivalence and simultaneous
φ-congruence) of quaternion matrix pencils A+tB, where each of A and B is either
φ-hermitian or φ-skewhermitian, are given in Chapters 11 and 12. As before, full
and detailed proofs are supplied.
Applications are made in Chapter 14 to various types of matrix equations over
quaternions, such as
Zm +
m−1
X
j=0
AjZj = 0,
where A0, . . . , Am−1 are given n × n quaternion matrices,
ZBZ + ZA −DZ −C = 0,
where A, B, C, and D are given quaternion matrices of suitable sizes, and the
symmetric version of the latter equation,
ZDZ + ZA + A∗Z −C = 0,
(1.0.1)
where D and C are assumed to be hermitian. The theory of invariant subspaces
of quaternion matrices—and for equation (1.0.1) also of subspaces that are simul-
taneously invariant and semideﬁnite—plays a crucial role in study of these matrix
equations. Equation (1.0.1) and its solutions, especially hermitian solutions, are
important in linear control systems. A brief description of such systems and their
relation to equations of the type of (1.0.1) is also provided.
For the readers’ beneﬁt, in Chapter 15 we bring several well-known canonical
forms for real and complex matrices that are used extensively in the text.
No
proofs are given; instead we supply references that contain full proofs and further
bibliographical information.

INTRODUCTION
5
1.1
NOTATION AND CONVENTIONS
Numbers, sets, spaces
A := B—the expression or item A is deﬁned by the expression or item B
R—the real ﬁeld
⌊x⌋—the greatest integer not exceeding x ∈R
C—the complex ﬁeld
I(z) = (z −z)/(2i) ∈R—the imaginary part of a complex number z
C+—closed upper complex half-plane
Dϵ(λ) := {z ∈C+ : |z −λ| < ϵ}—part of the open circular disk centered at λ
with radius ϵ that lies in C+
C+,0—open upper complex half-plane
H—the skew ﬁeld of the quaternions
i, j, k—the standard quaternion imaginary units
R(x) = x0 and V(x) = x1i + x2j + x3k—the real and the vector part of x,
respectively, for x = x0 + x1i + x2j + x3k ∈H, where x0, x1, x2, x3 ∈R
|x| =
p
x2
0 + x2
1 + x2
2 + x2
3—the length of x ∈H
Inv (φ) := {x ∈H : φ(x) = x}—the set (real vector space) of quaternions
invariant under an involution φ of H
β(φ) ∈H—quaternion with the properties that φ(β(φ)) = −β(φ) and |β(φ)| = 1,
where φ is an involution of H that is diﬀerent from the quaternion
conjugation; for a given φ, the quaternion β(φ) ∈H is unique up to negation
Con (α) = {y∗αy : y ∈H \ {0}}—the congruence orbit of α ∈H
Sim (α) = {y−1αy : y ∈H \ {0}}—the similarity orbit of α ∈H
Fn×1—the vector space of n-components columns with components in F,
where F = R, F = C, or F = H; Hn×1 is understood as a right quaternion
vector space
ej ∈Hn×1—the vector with 1 in the jth component and zero elsewhere; n is
understood from context
⟨x, y⟩:= y∗x, x, y ∈Hn×1—the standard inner product deﬁned on Hn×1
∥x∥H =
p
⟨x, x⟩or ∥x∥—the norm of x ∈Hn×1
Fm×n—the vector space of m×n matrices with entries in F, where F = R, F = C,
or F = H, and Hm×n is understood as a left quaternion vector space;
if m = n, then Cn×n is a complex algebra, whereas Rn×n and Hn×n are
real algebras
Subspaces
PM ∈Fn×n—the orthogonal projection onto the subspace M ⊆Fn×1; here
F = R, F = C, or F = H
SM := {x ∈M ; ∥x∥= 1}—the unit sphere of a nonzero subspace M
A|R—restriction of a square-size matrix A to its invariant subspace R (we rep-
resent A|R as a matrix with respect to some basis in R)
M ˙+ N—direct sum of subspaces M and N
Span {x1, . . . , xp} or SpanH {x1, . . . , xp}—the quaternion subspace spanned by
vectors x1, . . . , xp ∈Hn×1
SpanR {x1, . . . , xp}—the real subspace spanned by vectors x1, . . . , xp ∈Hn×1
dimH M or dim M—the (quaternion) dimension of a quaternion vector
space M

6
CHAPTER 1
Grassn—the set of all (quaternion) subspaces in Hn×1
θ(M, N) = ∥PM −PN ∥—the gap between subspaces M and N
Matrix-related notation
In or I (with n understood from context)—n × n identity matrix
0u×v, abbreviated to 0u, if u = v—the u × v zero matrix, also 0 (with u and v
understood from context)
C-eigenvalues of A—for a square-size complex matrix A, deﬁned as the (com-
plex) roots of the characteristic polynomial of A, and σC(A) is the set of
all C-eigenvalues; e.g., for A =

i
1
0
2i

we have
σC(A) = {i, 2i},
σ(A) = {ai + bj + ck : a, b, c ∈R,
a2 + b2 + c2 = 1 or a2 + b2 + c2 = 4}
AT —transposed matrix
A∗—conjugate transposed matrix
A—the matrix obtained from A ∈Cm×n or A ∈Hm×n by replacing each entry
with its complex or quaternion conjugate
Aφ ∈Hn×m, A = [ai,j]m,n
i,j=1 ∈Hm×n—stands for the matrix [φ(aj,i)]n,m
j,i=1, where
φ is an involution of H
Ran A = {Ax : x ∈Fn×1} ⊆Fm×1—the image or range of A ∈Fm×n; here
F ∈{R, C, H} (understood from context)
Ker A = {x ∈Fn×1 : Ax = 0}—the kernel of A ∈Fm×n; here F ∈{R, C, H}
(understood from context)
∥A∥H or ∥A∥—the norm of A ∈Hm×n; it is taken to be the largest singular
value of A
rank A—the (quaternion) rank of a matrix A ∈Hm×n; if A is real or complex,
then rank A coincides with the rank of A as a real or complex matrix
B ≥C or C ≤B—for hermitian matrices B, C ∈Hn×n, indicates that the
diﬀerence B −C is positive semideﬁnite
In+(A), In−(A), In0(A)—the number of positive, negative, or zero eigenvalues
of a quaternion hermitian matrix A, respectively, counted with
algebraic multiplicities
(In+ (H), In−(H), In0 (H))—the β(φ)-inertia, or the β(φ)-signature, of a φ-
skewhermitian matrix H ∈Hn×n; here φ is an involution of H diﬀerent
from the quaternion conjugation
diag (X1, . . . , Xk) = diag (Xj)k
j=1 = X1 ⊕X2 ⊕· · ·⊕Xk—block diagonal matrix
with the diagonal blocks X1, X2, . . . , Xk (in that order)
rowj=1,2,...,p Xj = row (Xj)p
j=1 = [X1 X2 · · · Xp]—block row matrix
colj=1,2,...,p Xj = col (Xj)p
j=1 =


X1
X2
...
Xp

—block column matrix
X⊕m—X ∈Hδ1×δ2, the mδ1 × mδ2 matrix X ⊕· · · ⊕X, where X is repeated
m times

INTRODUCTION
7
1.2
STANDARD MATRICES
In this section we collect matrices in standard forms and ﬁxed notation that will be
used throughout the book, sometimes without reference. The subscript in notation
for a square-size matrix will always denote the size of the matrix.
Ir or I (with r understood from context)—the r × r identity matrix
0u×v—often abbreviated to 0u; if u = v or 0 (with u and v understood from
context), the u × v zero matrix
Jordan blocks:
Jm(λ) =


λ
1
0
· · ·
0
0
λ
1
· · ·
0
...
...
...
...
0
...
...
λ
1
0
0
· · ·
0
λ


∈Hm×m,
λ ∈H
(1.2.1)
real Jordan blocks:
J2m(a ± ib) =


a
b
1
0
· · ·
0
0
−b
a
0
1
· · ·
0
0
0
0
a
b
· · ·
0
0
0
0
−b
a
· · ·
...
...
...
...
...
...
1
0
...
...
...
...
0
1
0
0
0
0
· · ·
a
b
0
0
0
0
· · ·
−b
a


∈R2m×2m,
(1.2.2)
a ∈R,
b ∈R \ {0}
symmetric matrices:
Fm :=


0
· · ·
· · ·
0
1
...
1
0
...
...
...
0
1
...
1
0
· · ·
· · ·
0


= F −1
m ,
(1.2.3)
Gm :=


0
· · ·
· · ·
1
0
...
0
0
...
...
1
0
...
0
0
· · ·
· · ·
0


=
 Fm−1
0
0
0

,
(1.2.4)
eGm := FmGmFm
=

0
0
0
Fm−1

.
(1.2.5)

8
CHAPTER 1
We also deﬁne
Ξm(α) :=


0
0
· · ·
0
α
0
0
· · ·
−α
0
...
...
...
...
...
0
(−1)m−2α
· · ·
0
0
(−1)m−1α
0
· · ·
0
0


∈Hm×m,
(1.2.6)
where α ∈H, and
Φm(β)
:=


0
0
· · ·
0
0
β
0
0
· · ·
0
−β
−1
0
0
· · ·
β
−1
0
...
...
...
...
...
...
(−1)m−1β
−1
0
· · ·
0
0


(1.2.7)
=
Ξm(β) −eGm ∈Hm×m,
where β ∈H. Note that
Ξm(α) = (−1)m−1(Ξm(α))T ,
α ∈H;
in particular Ξm(α) = (−1)m(Ξm(α))∗if and only if the real part of α is zero.
Y2m =


0
1
0
0
−1
1
0
0
−1
...
1
0
0
−1
0


.
(1.2.8)
Note that Y2m is real symmetric.
Real matrix pencils—
Z2m(t, µ, ν) := (t + µ)F2m + νY2m +

F2m−2
0
0
02

,
(1.2.9)
where µ ∈R, ν ∈R \ {0}.
Singular matrix pencils—
Lε×(ε+1)(t) = [0ε×1 Iε] + t[Iε 0ε×1] ∈Hε×(ε+1).
(1.2.10)
Here ε is a positive integer; Lε×(ε+1)(t) is of size ε × (ε + 1).

Chapter Two
The algebra of quaternions
In this chapter we introduce the quaternions and their algebra: multiplication,
norm, automorphisms and antiautomorphisms, etc. We give matrix representations
of various real linear maps associated with quaternion algebra. We also introduce
representations of quaternions as real 4 × 4 matrices and as complex 2 × 2 matrices.
2.1
BASIC DEFINITIONS AND PROPERTIES
Fix an ordered basis {e, i, j, k} in a 4-dimensional real vector space H (we may
take H = R4, the vector space of columns consisting of four real components), and
introduce multiplication in H by the formulas
ei = ie = i,
ej = je = j,
ek = ke = k,
i2 = j2 = k2 = −e,
e2 = e,
ij = −ji = k,
jk = −kj = i,
ki = −ik = j,
and by the requirement that the multiplication of elements of H is distributive with
respect to addition and commutes with scalar multiplication:
x(y + z) = xy + xz,
(y + z)x = yx + yz,
x(λy) = (λx)y = λ(xy)
for all x, y, z ∈H and all λ ∈R.
Deﬁnition 2.1.1. The elements of H with the algebraic operations of H as a
real vector space and with the multiplication introduced as above are called the
(real) quaternions.
The letter H stands for William Rowan Hamilton (1805–1865), inventor of
quaternions. Clearly, the multiplication in the algebra H is noncommutative.
Proposition 2.1.2. H is a unital associative algebra with the unity e:
x(yz) = (xy)z,
ex = xe = x
for all x, y, z ∈H.
In the sequel we identify the real number λ with the quaternion λe; in particular,
1 stands for 1e. Also, it is easy to see that the real span of 1 and i is isomorphic (as
a subalgebra of H) to C; thus, we identify, when convenient, C with the subalgebra
of H spanned (as a real vector space) by 1 and i.
Deﬁnition 2.1.3. For a quaternion x = x0 + x1i + x2j + x3k, where x0, x1, x2, x3
∈R, we deﬁne R(x) = x0, the real part of x, and V(x) = x1i + x2j + x3k, the
vector part (or imaginary part) of x.
The conjugate quaternion of x is deﬁned
by x0 −x1i −x2j −x3k = R(x) −V(x) and denoted x or x∗. The norm of x is
|x| =
√
x∗x =
p
x2
0 + x2
1 + x2
2 + x2
3 ∈R. We say that x ∈H is a unit quaternion if
|x| = 1.

10
CHAPTER 2
Some elementary properties of the algebra of quaternions are listed below.
Proposition 2.1.4. Let x, y ∈H. Then:
1. x∗x = xx∗;
2. |x| = |x∗|;
3. | · | is indeed a norm on H; in more detail, for all x, y ∈H we have:
|x| ≥0 with equality if and only if x = 0;
|x + y| ≤|x| + |y|;
|xy| = |yx| = |x| · |y|;
4. jcj∗= kck∗= c for every c ∈C;
5. (xy)∗= y∗x∗;
6. x = x∗if and only if x ∈R;
7. if a ∈H, then ax = xa for every x ∈H if and only if a ∈R;
8. every x ∈H \ {0} has an inverse x−1 = x∗/|x|2 ∈H; in more detail,
x · (x∗/|x|2) = (x∗/|x|2) · x = 1;
9. |x−1| = |x|−1 for every x ∈H \ {0};
10. x ∈H and x∗are solutions of the following quadratic equation with real coef-
ﬁcients: t2 −2R(x)t + |x|2 = 0;
11. Cauchy-Schwarz-type inequality is max{|R(xy)| , |V(xy)|} ≤|x| · |y|;
12. R(xy) = R(yx) for all x, y ∈H;
13. if R(x) = 0, then x2 = −|x|2.
We indicate a proof of |xy| = |x| · |y|:
|xy|2 = xy(xy)∗= xyy∗x∗= yy∗xx∗= |y|2 |x|2,
for all x, y ∈H.
Thus, H is a division ring, i.e., a unital ring in which every nonzero element has a
multiplicative inverse, and also a 4-dimensional algebra over the ﬁeld of real numbers
R.
Note that the multiplication of quaternions with zero real parts can be expressed
in terms of the usual inner product and cross product of vectors in R3, namely, if
x = x1i + x2j + x3k, y = y1i + y2j + y3k ∈H, where xℓ, yℓ∈R, then
xy = −pT
x py + [i j k](px × py),
(2.1.1)
where
px = [x1 x2 x3]T ,
py = [y1 y2 y3]T ∈R3×1,
(2.1.2)
and where in the right-hand side of (2.1.1) × denotes the cross product (also known
as vector product) of vectors in R3×1:
[x1 x2 x3]T × [y1 y2 y3]T = (x2y3 −x3y2, −(x1y3 −x3y1), x1y2 −x2y1)T .
The veriﬁcation of (2.1.1) is straightforward. More generally, let
x = x0 + x1i + x2j + x3k,
y = y0 + y1i + y2j + y3k ∈H,
xℓ, yℓ∈R,
and deﬁne px, py by (2.1.2). Then
R(xy) = x0y0 −pT
x py,
V(xy) = x0V(y) + y0V(x) + [i j k](px × py).

THE ALGEBRA OF QUATERNIONS
11
2.2
REAL LINEAR TRANSFORMATIONS AND EQUATIONS
For ﬁxed a, b ∈H, the map x 7→axb is obviously a real linear transformation on
H. We give the matrix form of this transformation with respect to the (ordered)
basis 1, i, j, k of H as a real vector space.
Theorem 2.2.1. Let
a = a0 + a1i + a2j + a3k,
b = b0 + b1i + b2j + b3k ∈H,
where aj, bj ∈R for j = 0, 1, 2, 3. Let
Ta,bx = axb,
x ∈H,
(2.2.1)
be a real linear transformation. Then Ta,b is given by the following matrix with
respect to the ordered real basis {1, i, j, k} in H:


a0b0 −a1b1 −a2b2 −a3b3
−a0b1 −a1b0 + a2b3 −a3b2
a0b1 + a1b0 + a2b3 −a3b2
a0b0 −a1b1 + a2b2 + a3b3
a0b2 −a1b3 + a2b0 + a3b1
−a0b3 −a1b2 −a2b1 + a3b0
a0b3 + a1b2 −a2b1 + a3b0
a0b2 −a1b3 −a2b0 −a3b1
−a0b2 −a1b3 −a2b0 + a3b1
−a0b3 + a1b2 −a2b1 −a3b0
a0b3 −a1b2 −a2b1 −a3b0
−a0b2 −a1b3 + a2b0 −a3b1
a0b0 + a1b1 −a2b2 + a3b3
a0b1 −a1b0 −a2b3 −a3b2
−a0b1 + a1b0 −a2b3 −a3b2
a0b0 + a1b1 + a2b2 −a3b3

.
The proof is obtained by tedious but straightforward computation.
The following particular cases are of interest.
Corollary 2.2.2. The real linear transformations T1,b, Ta,1, Ta,a∗, and Ta,a−1
(in the latter case it is assumed a ̸= 0) are given by the following matrices, respec-
tively, with respect to the ordered real basis {1, i, j, k} of H and using the notation
of Theorem 2.2.1:


b0
−b1
−b2
−b3
b1
b0
b3
−b2
b2
−b3
b0
b1
b3
b2
−b1
b0

,


a0
−a1
−a2
−a3
a1
a0
−a3
a2
a2
a3
a0
−a1
a3
−a2
a1
a0

,


a2
0 + a2
1 + a2
2 + a2
3
0
0
a2
0 + a2
1 −a2
2 −a2
3
0
2a0a3 + 2a1a2
0
−2a0a2 + 2a1a3
0
0
−2a0a3 + 2a1a2
2a0a2 + 2a1a3
a2
0 −a2
1 + a2
2 −a2
3
−2a0a1 + 2a2a3
2a0a1 + 2a2a3
a2
0 −a2
1 −a2
2 + a2
3

,
(2.2.2)
and (a2
0 + a2
1 + a2
2 + a2
3)−1X, where X is the matrix (2.2.2).
The statement of Corollary 2.2.2 concerning Ta,a−1 follows from the observation
that Ta,a−1 = |a|−2Ta,a∗. Note that the matrices corresponding to Tb,1, resp. to
T1,a, are skewsymmetric if and only if R(b) = 0, resp. R(a) = 0, whereas the
matrices corresponding to Ta,a∗and Ta,a−1 are symmetric if and only if R(a) = 0
or V(a) = 0.

12
CHAPTER 2
Corollary 2.2.3. Let a = a0 + a1i + a2j + a3k ∈H, a0, a1, a2, a3 ∈R. Then the
real linear transformation T1,a −Ta,1 that maps x ∈H to xa −ax is given by the
skewsymmetric matrix


0
0
0
0
0
0
2a3
−2a2
0
−2a3
0
2a1
0
2a2
−2a1
0


with respect to the ordered real basis {1, i, j, k}.
Observe that for a = a0 + a1i + a2j + a3k ∈H \ {0}, where a0, a1, a2, a3 are real,
the matrix
U :=
1
|a|2


a2
0 + a2
1 −a2
2 −a2
3
−2a0a3 + 2a1a2
2a0a2 + 2a1a3
2a0a3 + 2a1a2
a2
0 −a2
1 + a2
2 −a2
3
−2a0a1 + 2a2a3
−2a0a2 + 2a1a3
2a0a1 + 2a2a3
a2
0 −a2
1 −a2
2 + a2
3


is orthogonal, i.e., U T U = I. A straightforward computation will verify this asser-
tion. Moreover, det U = 1. Indeed, the set of all nonzero quaternions is connected,
and det U is a continuous function of the components of a. Therefore, the values
of det U also form a connected set (Theorem 3.10.7).
But determinants of real
orthogonal matrices can be only 1 or −1. It follows that either det U = 1 for all
U, or det U = −1 for all U. Since for a = 1 we have U = I, the second possibility
cannot happen.
We obtain that 1 is an eigenvalue of U, and the corresponding eigenvector is
unique up to scaling (apart from the case U = I). So, in a suitable orthonormal
basis in R3, the matrix U has the form
U =


cos µ
−sin µ
0
sin µ
cos µ
0
0
0
1

,
0 ≤µ < 2π.
Comparing with Corollary 2.2.2, the following geometric description of the trans-
formation Ta,a−1 is obtained. In this description, H0 stands for the real vector space
of all quaternions with zero real parts, and orthogonality in H0 is understood in the
sense of the real-valued inner product that has i, j, k as an orthonormal basis.
Corollary 2.2.4. Let a ∈H \ {0}, and assume Ta,a−1 ̸= I. Then Ta,a−1 maps
H0 onto itself. Moreover, there is a unique (up to scaling) nonzero x0 ∈H0 such
that Ta,a−1x0 = x0, and denoting by H0 ⊖SpanR{x0} the 2-dimensional plane in
H0 orthogonal to x0, we have that Ta,a−1 acts as a rotation through a ﬁxed angle µ,
0 < µ < 2π, in H0 ⊖SpanR{x0}.
Deﬁnition 2.2.5. We say that two quaternions x, y are similar if axa−1 = y
for some a ∈H \ {0} and congruent if axa∗= y for some a ∈H \ {0}.
Clearly, both similarity and congruence are equivalence relations. Denote by
Sim (x) = {y ∈H : y similar to x}
and
Con (x) = {y ∈H : y congruent to x}
the similarity orbit and the congruence orbit of x ∈H, respectively.

THE ALGEBRA OF QUATERNIONS
13
We have
Con (x) =
[
λ>0
{λSim (x)}.
Indeed, this follows from the formula a∗= |a|2a−1, a ∈H \ {0}, with λ = |a|2.
Theorem 2.2.6. Fix x = x0 +x1i+x2j+x3k ∈H, where xj ∈R. The following
statements are equivalent for y = y0 + y1i + y2j + y3k ∈H, yj ∈R:
(1) y ∈Sim (x);
(2) y = axa∗for some unit quaternion a;
(3) [y0 y1 y2 y3]T =
 1
0
0
Q

[x0 x1 x2 x3]T for some 3 × 3 real orthogonal
matrix Q;
(4) [y0 y1 y2 y3]T =
 1
0
0
Q′

[x0 x1 x2 x3]T for some 3×3 real orthogonal
matrix Q′ having determinant 1;
(5) R(y) = R(x) and |V(y)| = |V(x)|.
Proof. (1) =⇒(2): If y = bxb−1, b ∈H \ {0}, then (2) holds with a = b/|b|.
(2) =⇒(4): Follows from Corollary 2.2.2 and Ex. 2.7.7.
(4) =⇒(3): Obvious.
(3) =⇒(5): Follows from the isometric property of real orthogonal matrices,
i.e., ∥Qu∥= ∥u∥for all real orthogonal Q ∈Rn×n and all u ∈Rn×1.
(5) =⇒(1): By hypothesis, y0 = x0 and y2
1 + y+
2 y2
3 = x2
1 + x2
2 + x2
3. Consider
the equation
(z0 + z1i + z2j + z3k)y = x(z0 + z1i + z2j + z3k)
(2.2.3)
with real unknowns z0, z1, z2, z3. Equating the coeﬃcients of each of 1, i, j, k in the
left and the right sides of (2.2.3), we see that (2.2.3), after some simple algebra,
boils down to the system of equations


0
x1 −y1
x2 −y2
x3 −y3
−x1 + y1
0
x3 + y3
−x2 −y2
−x2 + y2
−x3 −y3
0
x1 + y1
−x3 + y3
x2 + y2
−x1 −y1
0




z0
z1
z2
z3

= 0.
(2.2.4)
We claim that the matrix in the left-hand side of (2.2.4), call it X, is singular.
Indeed, X


0
x1 + y1
x2 + y2
x3 + y3

= 0, so, unless
x1 + y1 = x2 + y2 = x3 + y3 = 0,
(2.2.5)
the matrix X is singular. But if (2.2.5) holds, X is easily seen to be singular as
well. Thus, (2.2.3) has a nontrivial solution, and (1) follows.
□
Theorem 2.2.6 allows us to express the similarity orbits of quaternions in a more
detailed way:
Sim (x) = R(x) + |V(x)|S,

14
CHAPTER 2
where
S := {q ∈H : R(q) = 0,
|q| = 1} = {q ∈H : q2 = −1}.
(2.2.6)
Geometrically, S is the unit sphere in R3×1.
2.3
THE SYLVESTER EQUATION
Let a, b ∈H. In this section we study the Sylvester equation
ax −xb = y,
x, y ∈H,
and the corresponding real linear transformation
Sa,b(x) = ax −xb,
x ∈H.
(2.3.1)
In what follows, we make use of the inner product.
Deﬁnition 2.3.1. The real-valued inner product of two quaternions is deﬁned
by
\x0 +x1i+x2j+x3k, y0 +y1i+y2j+y3k/ := x0y0 +x1y1 +x2y2 +x3y3,
xℓ, yℓ∈R.
Note that \x, x/ = |x|2 for every x ∈H. Also, for x, y ∈H with zero real parts,
we have \x, xy/ = \y, xy/ = 0, as can be easily veriﬁed using formula (2.1.1).
For given a, b ∈H, deﬁne the quaternions {x+, y+, x−, y−} as follows:
(i) If V(a) and V(b) are linearly independent over R, we set
x±
=
(±|V(a)||V(b)| −(V(a))(V(b)))/n±,
y±
=
(|V(a)|V(b) ± |V(b)|V(a))/n±,
(2.3.2)
where
n± =
p
2|V(a)| |V(b)| (|V(a)||V(b)| ± \V(a), V(b)/).
Note that in view of the Cauchy-Schwarz inequality applied to V(a) and V(b)
(interpreted as vectors in R3) and the linear independence of V(a) and V(b),
we have |V(a)||V(b)| ± \V(a), V(b)/ > 0.
(ii) Suppose V(a) and V(b) are linearly dependent over R.
Then there exists
q ∈H with R(q) = 0, |q| = 1, V(a) = |V(a)|q, and V(b) = |V(b)|q or
V(b) = −|V(b)|q. Let bq ∈H be such that R(bq) = 0, |bq| = 1 and ⟨q, bq⟩= 0. If
V(b) = |V(b)|q, we deﬁne
x+ = 1,
y+ = q,
x−= bq,
y−= qbq.
If V(b) = −|V(b)|q, we deﬁne
x+ = bq,
y+ = qbq,
x−= 1,
y−= q.
Note that q and bq are not unique (for given a and b); more precisely, q is unique
if V(a) ̸= 0 and is unique up to negation if V(b) ̸= V(a) = 0.
Furthermore, we deﬁne the subspaces
V+
a,b = SpanR {x+, y+},
V−
a,b = SpanR {x−, y−}.

THE ALGEBRA OF QUATERNIONS
15
The subspaces V±
a,b are uniquely determined by a and b (unless V(a) = V(b) = 0),
see Ex. 2.7.25. Moreover, we have
V+
a,b = V−
a,b∗,
V−
a,b = V +
a,b∗
if at least one of a and b is nonreal.
Finally, we set
Q(α, β) :=
 α
−β
β
α

,
α, β ∈R.
Note the easily observed equality ∥Q(α, β)ξ∥=
p
α2 + β2 ∥ξ∥for all ξ ∈R2×1,
where we have used the euclidean norm in R2×1.
The main result of this section gives explicitly the orthonormal basis that re-
duces all three linear transformations Ta,1, T1,b (deﬁned in (2.2.1)) and Sa,b to a
real Jordan form.
Theorem 2.3.2. (a) The vectors x+, y+, x−, y−form an orthonormal basis (with
respect to ⟨·, ·⟩) of H.
(b) The equalities
 Ta,1(x+)
Ta,1(y+)
Ta,1(x−)
Ta,1(y−) 
=
 x+
y+
x−
y−

· (Q(R(a), |V(a)|) ⊕Q(R(a), |V(a)|))
and
 T1,b(x+)
T1,b(y+)
T1,b(x−)
T1,b(y−) 
=
 x+
y+
x−
y−

· (Q(R(b), |V(b)|) ⊕Q(R(b), −|V(b)|))
hold true.
(c) The subspaces V+
a,b and V−
a,b are both Ta,1- and T1,b-invariant.
(d) The equality
 Sa,b(x+)
Sa,b(y+)
Sa,b(x−)
Sa,b(y−) 
=
 x+
y+
x−
y−

· (Q(R(a) −R(b), |V(a)| −|V(b)|) ⊕Q(R(a) −R(b), |V(a)| + |V(b)|))
holds true.
Proof. Part (a) is established by a straightforward but tedious computation.
Parts (c) and (d) are immediate from (b) as Sa,b = Ta,1 −T1,b. The veriﬁcation of
(b) is straightforward (Proposition 2.1.4(13) is used repeatedly).
□
We can read oﬀmany important properties of Sa,b from Theorem 2.3.2, such as
the following.
Theorem 2.3.3. Let a, b ∈H, and Sa,b deﬁned by (2.3.1). Then:
(1) the four singular values of Sa,b are
σ1 = σ2 =
p
(R(a) −R(b))2 + (|V(a)| + |V(b)|)2,
σ3 = σ4 =
p
(R(a) −R(b))2 + (|V(a)| −|V(b)|)2;
moreover, |Sa,b(x)| = σ4|x| for x ∈V+
a,b, and |Sa,b(x)| = σ1|x| for x ∈V−
a,b;

16
CHAPTER 2
(2) Sa,b is singular if and only if R(a) = R(b) and |V(a)| = |V(b)|. If these
conditions hold and a, b ̸∈R, then
Ker Sa,b = V+
a,b = V−
a,b∗= Ran Sa,b∗,
Ran Sa,b = V−
a,b = V+
a,b∗= Ker Sa,b∗;
(3) Sa,b has a real eigenvalue (which then is R(a) −R(b)) if and only if |V(a)| =
|V(b)| and the associated eigenspace is V+
a,b;
(4) the centralizer of a ∈H is
Cen(a) := {x ∈H : ax = xa} = Ker Sa,a;
we have
Cen(a) =
(
H
if a ∈R,
V+
a,a = SpanR {1, a}
otherwise.
In the case a and b are similar (by Theorem 2.2.6 this happens if and only if
R(a) = R(b) and |V(a)| = |V(b)|), the kernel and image of Sa,b have alternative
descriptions.
Theorem 2.3.4. Assume a, b ∈H\R are similar, so that b = z−1az, z ∈H\{0}.
Then:
(a) Ran Sa,b = Ker Sa,b∗. In other words, the equation ax −xb = y has a solution
x if and only if ay = yb∗;
(b) Ker Sa,b = Cen (a) z = SpanR {z, az}.
Proof. Part (a) follows from Theorem 2.3.3(2). Part (b) is a consequence of
the identity
ax −x(z−1az) = (a(xz−1) −(xz−1)a)z.
□
We conclude this section with formulas for the unique solution of the Sylvester
equation, provided Sa,b is invertible. For a, b ∈H, deﬁne
f1(a, b) = b2 −2R(a)b + |a|2,
f2(a, b) = a2 −2R(b)a + |b|2.
Proposition 2.3.5. The following statements are equivalent:
(1) f1(a, b) = 0;
(2) f2(a, b) = 0;
(3) a and b are similar;
(4) R(a) = R(b) and |a| = |b|.
Proof. Equivalence of (3) and (4) follows from equivalence of (1) and (5) in The-
orem 2.2.6. The implications (3) ⇒(1) and (3) ⇒(2) follow easily from Proposition
2.1.4(10). Suppose (1) holds true. Subtracting the equality b2 −2R(b)b + |b|2 = 0
from f1(a, b) = 0, we obtain 2(R(a) −R(b))b = |a|2 −|b|2. If R(a) ̸= R(b), then b
must be real. Subtracting a2 −2R(a)a + |a|2 = 0 from f1(a, b) = 0 yields a = b.
If R(a) = R(b), then |a|2 −|b|2 = 0, and (4) follows. Analogously, one proves that
(2) implies (4).
□

THE ALGEBRA OF QUATERNIONS
17
Theorem 2.3.6. If Sa,b is nonsingular, then the unique solution to the equation
Sa,b(x) = y satisﬁes
x = a∗y(f1(a, b))−1 −y(f1(a, b))−1b = a(f2(a, b))−1y −(f2(a, b))−1yb∗.
The proof follows from the equalities
Sa,b(a∗z −zb) = zf1(a, b),
Sa,b(az −zb∗) = f2(a, b)z,
for all z ∈H, which can be veriﬁed without diﬃculty.
2.4
AUTOMORPHISMS AND INVOLUTIONS
Deﬁnition 2.4.1. An ordered triple of quaternions (q1, q2, q3) is said to be a
units triple if
q2
1 = q2
2 = q2
3 = −1,
q1q2 = −q2q1 = q3,
q2q3 = −q3q2 = q1,
q3q1 = −q1q3 = q2.
(2.4.1)
For example, {i, j, k} is a units triple.
Proposition 2.4.2. An ordered triple (q1, q2, q3), qj ∈H, is a units triple if and
only if there exists a 3 × 3 real orthogonal matrix P = [pα,β]3
α,β=1 with determinant
1 such that
qα = p1,αi + p2,αj + p3,αk,
α = 1, 2, 3.
(2.4.2)
Proof. A straightforward computation veriﬁes that x ∈H satisﬁes x2 = −1 if
and only if
x = a1i + a2j + a3k,
where a1, a2, a3 ∈R and a2
1 + a2
2 + a2
3 = 1. Thus, we may assume that qα are given
by (2.4.2) with the vectors pα := (p1,α, p2,α, p3,α)T ∈R3×1 having euclidean norm
1, for α = 1, 2, 3. Next, in view of (2.1.1) we have
quqv = −pT
u pv +
 i
j
k 
(pu× pv),
u, v ∈{1, 2, 3}.
(2.4.3)
The result of Proposition 2.4.2 now follows easily.
□
In particular, for every units triple (q1, q2, q3) the quaternions 1, q1, q2, q3 form
a basis of the real vector space H.
Next, we consider endomorphisms and antiendomorphisms of quaternions.
Deﬁnition 2.4.3. A map φ : H −→H is called an endomorphism, resp. an
antiendomorphism, if φ(xy) = φ(x)φ(y), resp., φ(xy) = φ(y)φ(x) for all x, y ∈H,
and φ(x + y) = φ(x) + φ(y) for all x, y ∈H. An antiendomorphism φ is called an
involution if φ(φ(x)) = x for every x ∈H.
An involution is necessarily one-to-one and onto.
Theorem 2.4.4. Let φ be an endomorphism or an antiendomorphism of H.
Assume that φ does not map H into zero. Then φ is one-to-one and onto H; thus,
φ is in fact an automorphism or an antiautomorphism. Moreover, φ is real linear,
and representing φ as a 4 × 4 real matrix with respect to the basis {1, i, j, k}, we
have:

18
CHAPTER 2
(a) φ is an automorphism if and only if
φ =

1
0
0
T

,
(2.4.4)
where T is a 3 × 3 real orthogonal matrix of determinant 1;
(b) φ is an antiautomorphism if and only if φ has the form (2.4.4), where T is a
3 × 3 real orthogonal matrix of determinant −1;
(c) φ is an involution if and only if
φ =
 1
0
0
T

,
where either T = −I3 or T is a 3 × 3 real orthogonal symmetric matrix with
eigenvalues 1, 1, −1.
Proof. Clearly, φ is one-to-one (indeed, if φ(x) = 0 for some nonzero x, then
for all y ∈H we have φ(y) = φ(yx−1)φ(x) = 0, a contradiction to the hypotheses
of Theorem 2.4.4). Also, φ(x) = x for every real rational x.
We will use an observation which can be easily checked by straightforward alge-
bra: if x ∈H is nonreal, then the commutant of x, namely, the set of all y ∈H such
that xy = yx coincides with the set of all quaternions of the form a + bx, where
a, b ∈R.
Next, we prove that φ maps reals into reals. Arguing by contradiction, assume
that φ(x) is nonreal for some real x. Since xy = yx for every y ∈H, we have that
φ(H) is contained in the commutant of φ(x), i.e., by the above observation,
φ(H) ⊆R + Rφ(x).
However, the set R + Rφ(x) contains only two square roots of −1, namely,
± V(φ(x))
|V(φ(x))|.
On the other hand, H contains a continuum of square roots of −1. Since φ maps
square roots of −1 onto square roots of −1, φ cannot be one-to-one, a contradiction.
Thus, φ maps reals into reals, and the restriction of φ to R is a nonzero endomor-
phism of the ﬁeld of real numbers. Now R has no nontrivial endomorphisms (indeed,
any nonzero endomorphism of R ﬁxes every rational number, and since only non-
negative real numbers have real square roots, any such endomorphism is also order
preserving, and these properties easily imply that any such endomorphism must be
the identity). Therefore, we must have φ(x) = x for all x ∈R. Now, clearly, φ is a
real linear map. Representing φ as a 4×4 matrix with respect to the basis {1, i, j, k},
we obtain the result of Part (a) from Proposition 2.4.2. For Part (b), note that if φ
is an antiautomorphism, then a composition of φ with any ﬁxed antiautomorphism
is an automorphism.
Taking a composition of φ with the antiautomorphism of
standard conjugation i −→−i, j −→−j, k −→−k, we see by Part (a) that the
composition has the form (2.4.4) with T a real orthogonal matrix of determinant
1. Since the standard conjugation has the form (2.4.4) with T = −I, we obtain
the result of Part (b). Finally, clearly φ is involutory if and only if the matrix T of
(2.4.4) has eigenvalues ±1, and (c) follows at once from (a) and (b).
□

THE ALGEBRA OF QUATERNIONS
19
Deﬁnition 2.4.5. If the former case of (c) holds true, then φ is the standard
conjugation, and we say that φ is standard. If the latter case of (c) holds true, we
say that these involutions are nonstandard.
Thus, the nonstandard involutions are parameterized by 1-dimensional real sub-
spaces (representing eigenvectors of T corresponding to the eigenvalue −1) in R3. In
other words, the set of nonstandard involutions can be identiﬁed (as a topological
space) with the 2-dimensional real projective space.
Here is another useful property of nonstandard involutions.
Lemma 2.4.6. Let φ1 and φ2 be two distinct nonstandard involutions. Then
for any α ∈H, the equality φ1(α) = φ2(α) holds if and only if φ1(α) = φ2(α) = α.
Proof. The “if” part is obvious. To prove the “only if” part, let T1 and T2 be
the 3 × 3 real orthogonal matrices with eigenvalues 1, 1, −1 such that
φj =
 1
0
0
Tj

,
j = 1, 2,
as in Theorem 2.4.4(c).
Then the “only if” part amounts to the following: if
T1 ̸= T2 and T1x = T2x for some x ∈R3×1, then T1x = T2x = x. Considering
the orthogonal complement of a common eigenvector of T1 and T2 corresponding
to the eigenvalue 1, the proof reduces to the statement that if bT1 ̸= bT2 are 2 × 2
real orthogonal symmetric matrices with determinants −1, then det ( bT1 −bT2) ̸= 0.
This can be veriﬁed by elementary matrix manipulations, taking bTj in the form

cos τj
sin τj
−sin τj
−cos τj

, where 0 ≤τj < 2π, for j = 1, 2.
□
Theorem 2.4.4 allows us to prove easily, using elementary linear algebra, the
following well-known fact.
Proposition 2.4.7. Every automorphism of H is inner—i.e., if φ : H →H is
an automorphism, then there exists α ∈H \ {0} such that
φ(x) = α−1xα,
for all x ∈H.
(2.4.5)
Proof. An elementary (but tedious) calculation shows that for
α = a + bi + cj + dk ∈H \ {0},
the 4 × 4 matrix representing the R-linear transformation x
7→
α−1xα in the
standard basis {1, i, j, k} is equal to
 1
0
0
S

,
where
S :=
1
|α|2





b
c
d

 b
c
d 
+


a
d
−c
−d
a
b
c
−b
a


2


(2.4.6)
(cf. Corollary 2.2.2). In view of Theorem 2.4.4, the matrix S is orthogonal with
determinant 1, because the transformation x
7→
α−1xα is an automorphism.
Conversely, every 3 × 3 real orthogonal matrix T with determinant 1 has the form

20
CHAPTER 2
of the right-hand side of (2.4.6): Indeed, if T = I, choose a ̸= 0 and b = c = d = 0.
If T ̸= I, choose (b, c, d)T as a unit length eigenvector of T corresponding to the
eigenvalue 1. Observe that (b, c, d)T is also an eigenvector of S corresponding to
the eigenvalue 1. Since the trace of S is equal to
1 +
2
a2 + b2 + c2 + d2 (a2 −b2 −c2 −d2),
it remains to choose a so that
a2 −b2 −c2 −d2
a2 + b2 + c2 + d2
coincides with the real part of those eigenvalues of T which are diﬀerent from 1.
The choice of a ∈R is always possible because the real part of the eigenvalues of T
other than 1 is between −1 and 1 and is not equal to 1.
□
One can write down concrete formulas for the automorphism φ given by (2.4.5).
Namely, without loss of generality, we can assume that α ∈H \ {0} has the form
α = cos
θ
2

−sin
θ
2

q3,
for some θ,
0 ≤θ < 2π,
where (q1, q2, q3) is a suitable units triple. Indeed, if α is real, we take θ = 0, and
if α ̸∈R, we take q3 = V(α)/|V(α)| and, in both cases, divide α by |α|. Then
φ(1)
=
1,
φ(q1) = cos(θ)q1 + sin(θ)q2,
φ(q2)
=
−sin(θ)q1 + cos(θ)q2,
φ(q3) = q3.
These formulas can be veriﬁed by direct computation. One also veriﬁes that in
terms of formula (2.4.4), the automorphism φ is given with
T = P ·


cos(θ)
−sin(θ)
0
sin(θ)
cos(θ)
0
0
0
1

· P T ,
(2.4.7)
where the real orthogonal matrix P ∈R3×3 is deﬁned by
[q1 q2 q3] = [i j k]P.
Since every antiautomorphism of H is a composition of a ﬁxed antiautomorphism
(such as the conjugation) and a suitable automorphism, Proposition 2.4.7 that
every antiautomorphism φ of H has the form φ(x) = β−1x∗β for some β ∈H with
|β| = 1. One easily veriﬁes that φ is an involution if and only if β2 is real. Also, φ is
nonstandard if and only if β2 = −1, and then φ(β) = −β. It follows from Theorem
2.4.4(c) that for every nonstandard involution φ, there is a unique (up to negation)
β ∈H such that φ(β) = −β and β2 = −1. We select one of the two quaternions
β with these properties and denote β(φ) = β. Letting (q1, q2, q3) be a units triple
with q1 = β(φ), we see that for every x0, x1, x2, x3 ∈R,
φ(x0 + x1q1 + x2q2 + x3q3) = x0 −x1q1 + x2q2 + x3q3.
(2.4.8)
In particular, φ(x) = −x, x ∈H, if and only if x ∈Rβ. We denote by Inv (φ) the
set of quaternions left invariant by a nonstandard involution φ:
Inv (φ) := {x ∈H : φ(x) = x} = SpanR {1, q2, q3}.

THE ALGEBRA OF QUATERNIONS
21
2.5
QUADRATIC MAPS
For a ﬁxed involution φ consider quadratic maps of the form x 7→φ(x)αx, where
α ∈H\{0} is such that either φ(α) = α or φ(α) = −α. (We exclude the trivial case
α = 0.) It is useful to ﬁnd information about ranges of these maps. For example,
if φ(α) = α, is it true that the range of the quadratic map coincides with the set of
quaternions that are ﬁxed by φ?
We use the notation Quadφ(α) for the map x 7→φ(x)αx, x ∈H.
If φ is a nonstandard involution, then there exists a unique (up to negation)
β ∈H such that φ(β) = −β and |β| = 1; note that β has zero real part. When
working with nonstandard involutions φ, we often ﬁx one such β and write β(φ) for
β.
Theorem 2.5.1.
(a) If φ is a nonstandard involution, then
φ(x)β(φ)x = β(φ)|x|2,
for every x ∈H.
(2.5.1)
In particular, the range of Quadφ(α) is the half-line
{aα : a ∈R, a ≥0}
(2.5.2)
for every α ̸= 0 such that φ(α) = −α.
(b) If φ is a nonstandard involution, then for every α ̸= 0 such that φ(α) = α,
the range of Quadφ(α) coincides with the set (actually, a real subspace of H)
Inv (φ) := {x ∈H : φ(x) = x}; moreover, for every λ ∈Inv (φ) there exists
x ∈Inv (φ) such that φ(x)αx = λ.
(c) If φ is the conjugation, then for every α ̸= 0 such that φ(α) = −α (in other
words, R(α) = 0, V(α) ̸= 0), the range of Quadφ(α) coincides with the set
(real subspace of H) of quaternions with zero real parts.
(d) If φ is the conjugation, then for every α ̸= 0 such that φ(α) = α (in other
words, R(α) ̸= 0, V(α) = 0), we have φ(x) = α|x|2 for all x ∈H.
In
particular, the range of Quadφ(α) is the half-line {x ∈R : x ≥0} if α > 0
and the half-line {x ∈R : x ≤0} if α < 0.
Proof. The veriﬁcation of (2.5.1) is straightforward, and Part (d) follows im-
mediately from the basic properties of Proposition 2.1.4. Part (c) follows from the
description of the conjugate orbit of α given in Ex. 2.7.24; indeed,
Quadφ(α) = Con (α) ∪{0}
if φ is the conjugation. Part (b) will be proved later.
□
For a proof of Theorem 2.5.1(b), we investigate the square root function on
quaternions, which is of independent interest. In the next theorem, we use the
notation
R−:= {x ∈H : R(x) ≤0, V(x) = 0},
and √· stands for the nonnegative square root of a nonnegative number.
Theorem 2.5.2.
(a) Let λ ∈R−. Then x2 = λ if and only if x =
p
|λ|q for
some q ∈H with R(q) = 0, |q| = 1.

22
CHAPTER 2
(b) Let λ ∈H \ R−. Then x2 = λ if and only if x = xλ or x = −xλ, where
xλ :=
|λ| + λ
p
2(|λ| + R(λ))
.
If λ ̸∈R then xλ can be written in the form
xλ =
r
|λ| + R(λ)
2
+
r
|λ| −R(λ)
2
V(λ)
|V(λ)|.
Since xλ in Part (b) is the unique square root of λ with positive real part, we
introduce the notation
√
λ := xλ,
for all λ ∈H \ R−.
(2.5.3)
Proof. We leave aside the trivial case λ = 0. Part (a) follows from the fact
(easily proved by straightforward veriﬁcation using the representation x = x0 +
x1i + x2j + x3k ∈H, where x0, x1, x2, x3 ∈R) that x2 = −1 if and only if R(x) = 0
and |x| = 1.
Part (b). The equation x2 = λ implies
λ = x2 = (2R(x) −x∗)x = 2R(x)x −|x|2 = 2R(x)x −|λ|.
Hence, |λ| + λ = 2R(x)x, and (taking the real part in this equality) |λ| + R(λ) =
2(R(x))2. If λ ̸∈R−then R(λ) + |λ| > 0, and it follows that
x = |λ| + λ
2R(x) =
|λ| + λ
2
p
(|λ| + R(λ))/2
=
|λ| + λ
p
2(|λ| + R(λ))
.
Thus,
R(x) =
|λ| + R(λ)
p
2(R(λ) + |λ|)
and, if V(λ) ̸= 0, then
V(x) =
V(λ)
p
2(|λ| + R(λ))
= R(λ)
p
|λ| −R(λ)
p
2(|λ|2 −R(λ)2)
=
p
(|λ| −R(λ))/2 · V(λ)
|V(λ)|,
and the theorem is proved.
□
Following from Theorem 2.5.2, the function λ 7→
√
λ is continuous (even real
analytic) on H \ {0}. However, there is no continuous square root function on H as
the following corollary shows.
Corollary 2.5.3. Let S be a nonempty and connected subset of H \ R−, and let
u : S →H be a function such that u(λ)2 = λ for all λ ∈S.
(a) The function u(·) is continuous if and only if either u(λ) =
√
λ for all λ ∈S
or u(λ) = −
√
λ for all λ ∈S.
(b) Suppose that the function u(·) is continuous.
Let µ ∈R−., µ ̸= 0.
Sup-
pose further that there is a sequence λk ∈S \ R such that limk→∞λk = µ
and limk→∞V(λk)/|V(λk)| does not exist. Then the function u(·) admits no
continuous extension to S ∪{µ}.

THE ALGEBRA OF QUATERNIONS
23
Proof. Part (a): For λ ̸∈R−the equation u(λ)2 = λ implies u(λ) = s(λ)
√
λ,
where s(λ) ∈{−1, 1}. Continuity of u and connectedness of S yield that s(λ) is
independent of λ.
Part (b): Without loss of generality, we may suppose u(λ) =
√
λ. Thus, for a
sequence λk as in (b), we have
u(λk) =
r
|λk| + R(λk)
2
|
{z
}
=ak
+
r
|λk| −R(λk)
2
|
{z
}
=bk
V(λk)
|V(λk)|.
As k tends to inﬁnity, ak tends to 0 and bk tends to
p
|µ|, while the sequence
(Vλk)/|V(λk)| does not converge. Thus each extension of u(·) to S ∪{µ} is discon-
tinuous at µ.
□
Proof of Theorem 2.5.1(b). Clearly, the range of Quadφ (α) is contained in
Inv (φ). We show that for all λ ∈Inv (φ), the equation xφαx = λ has a solution
x ∈Inv (φ). Suppose ﬁrst that αλ ̸∈R−. Then
x = α−1√
αλ = α−1
|αλ| + αλ
p
2(|αλ| + R(αλ))
=
α∗|λ|
|α| + λ
p
2(|αλ| + R(αλ))
is a solution. To see this, observe that
x ∈SpanR {α∗, λ} ⊆Inv (φ).
Thus,
xφαx = xαx = α−1(αx)2 = λ.
Now suppose that αλ ∈R−. Then for any q ∈H with R(q) = 0 and |q| = 1 the
quaternion x = α−1p
|αλ| q satisﬁes (αx)2 = αλ. Hence, xαx = λ. If α ̸∈R,
choose q = V(α)/|V(α)|. Then x ∈Inv (φ). If α ∈R, then x ∈Inv (φ) for any
q ∈Inv (φ).
□
2.6
REAL AND COMPLEX MATRIX REPRESENTATIONS
In the sequel it will be often useful to represent quaternions as 4 × 4 real matrices
or 2 × 2 complex matrices. These representations are described as follows.
Deﬁne the map
χ : H →R4×4,
χ(a0 + a1i + a2j + a3k) =


a0
−a1
a3
−a2
a1
a0
−a2
−a3
−a3
a2
a0
−a1
a2
a3
a1
a0

,
where a0, a1, a2, a3 ∈R.
Proposition 2.6.1. The map χ is a unital (i.e., χ(I) = I) isomorphism of H
onto the algebra of all 4 × 4 real matrices of the form λI + S, where λ ∈R and
S ∈R4×4 is a skew symmetric matrix of the form
S =









0
−a1
a3
−a2
a1
0
−a2
−a3
−a3
a2
0
−a1
a2
a3
a1
0

: a1, a2, a3 ∈R







.
(2.6.1)

24
CHAPTER 2
Proposition 2.6.1 can be proved by straightforward veriﬁcation that χ(x + y) =
χ(x) + χ(y), χ(xy) = χ(x)χ(y) for all x, y ∈H, χ(1) = I, and χ is one-to-one and
onto map on the indicated algebra of 4 × 4 real matrices (Ex. 2.7.10).
For α = a0 + ia1 + ja2 + ka3 ∈H, a0, a1, a2, a3 ∈R, deﬁne
ω(α) =

a0 + ia1
a2 + ia3
−a2 + ia3
a0 −ia1

∈C2×2.
Proposition 2.6.2. The map ω is a unital isomorphism of H onto the algebra
of all 2 × 2 complex matrices of the form

z
u
−u
z

, where z, u ∈C are arbitrary.
The proof is again by straightforward veriﬁcation of the required properties (Ex.
2.7.11).
The ordered triple of matrices
−iω(k) =
 0
1
1
0

,
−iω(j) =
 0
−i
i
0

,
−iω(i) =
 1
0
0
−1

is known as Pauli spin matrices, of importance of studies of spin in quantum me-
chanics. Note that the Pauli spin matrices are hermitian and unitary. Together
with I2, the Pauli spin matrices form a basis in the real vector space of 2 × 2
hermitian matrices.
2.7
EXERCISES
Ex. 2.7.1. Verify Proposition 2.1.2.
Ex. 2.7.2. Verify Proposition 2.1.4.
Ex. 2.7.3. Prove that |x + y| = |x| + |y|, x, y ∈H, holds if and only if either at
least one of x, y is zero or x ̸= 0 and y ̸= 0 are positive real multiples of each other.
Ex. 2.7.4. Solve the following equations:
(1) x4 + 1 = 0, x ∈H;
(2) xm + 1 = 0, x ∈H, where m is a ﬁxed even positive integer.
Hint: For a ﬁxed q ∈H, with R(q) = 0 and |q| = 1, consider solutions in SpanR {1, q}
which is isomorphic to C.
Ex. 2.7.5. Verify the following equality for x ∈H:
x2 = |R(x)|2 −|V(x)|2 + 2R(x)V(x).
Ex. 2.7.6. Let a ∈H be such that ax = xa for every x ∈H0, where H0 is a
real 3-dimensional subspace of H. Show that a ∈R. Show by example that this
statement is generally not true if H0 is taken to be a real 2-dimensional subspace
of H.
Ex. 2.7.7. Verify that the matrix Y := (a2
0 + a2
1 + a2
2 + a2
3)−1X of Corollary
2.2.2 is orthogonal and has determinant 1. Hint: The determinant of a real orthog-
onal matrix can be only ±1, and the case that the determinant is equal to −1 is
impossible here.

THE ALGEBRA OF QUATERNIONS
25
Ex. 2.7.8. Prove that H has no divisors of zero: If x1, . . . , xk ∈H are such that
x1 · · · xk = 0, then at least one of the xj’s is equal to zero.
Ex.
2.7.9. Find the kernel and the range of the real linear transformation
x 7→xa −ax, x ∈H, where a ∈H \ {0} is ﬁxed.
Ex. 2.7.10. Prove Proposition 2.6.1.
Ex. 2.7.11. Prove Proposition 2.6.2.
Ex. 2.7.12. Show that any two involutions φ1 and φ2 of H diﬀerent from the
conjugation are similar: There exists α ∈H \ {0} (which depends on φ1 and φ2)
such that φ2(x) = α−1φ1(x)α, x ∈H.
Ex. 2.7.13. Let (q1, q2, q3) be a units triple. Show that xq1 = −q1x, x ∈H if
and only if x is a real linear combination of q2 and q3.
Ex. 2.7.14. Let α and β be two quaternions with zero real parts. Prove that
αβ = β∗α if and only if α and β are orthogonal, i.e., writing
α = iα1 + jα2 + kα3,
β = iβ1 + jβ2 + kβ3,
αℓ, βℓ∈R for ℓ= 1, 2, 3,
we have
α1β1 + α2β2 + α3β3 = 0.
(2.7.1)
Ex. 2.7.15. Let u, v and u′, v′ be two pairs of unit length quaternions with zero
real parts. Prove that there exists an automorphism φ of H such that φ(u) = u′,
φ(v) = v′ if and only if the angle θ (0 ≤θ ≤π) between u and v is equal to that
between u′ and v′. Is φ unique?
Ex. 2.7.16. Let u, v and u′, v′ be as in Ex. 2.7.15. Prove that there exists an
antiautomorphism ψ of H such that ψ(u) = u′, ψ(v) = v′ if and only if the angle
between u and v is equal to that between u′ and v′. Is ψ unique?
Ex. 2.7.17. Identify all pairs (x, y) of quaternions in the set {1 + i, 1 + 2j, 2 +
2j, 1 + 3k} that are similar, resp. congruent, to each other. Find a ∈H such that
axa−1 = y in case x and y are similar or axa∗= y in case x and y are congruent.
Ex. 2.7.18. Find:
(a) all x ∈H such that ix = xj;
(b) all y ∈H such that 2iy = y(j + k);
(c) all z ∈H such that 3iz =
√
3z(i + j + k).
Ex. 2.7.19. Show that all solutions z of the equation z2 = p+iq, where p, q ∈R
and q ̸= 0 and z ∈H is to be found, belong to SpanR {1, i}.
Ex. 2.7.20. Prove Bohr’s inequality:
|z + w|2 ≤p|z|2 + q|w|2,
z, w ∈H,
(2.7.2)
where p, q are positive real numbers such that (1/p) + (1/q) = 1, with the equality
holding in (2.7.2) if and only if w = (p −1)z.
Ex.
2.7.21. Show that every quaternion can be written in inﬁnitely many
ways as product of two quaternions with zero real parts. Hint: Show that for all
x ∈SpanR {1, i}, x1 ∈SpanR {j, k} with x1 ̸= 0, there exists a y ∈SpanR {j, k} such
that x = x1y.

26
CHAPTER 2
Ex. 2.7.22. Let S := {x ∈H : |x| = 1} be the set of quaternions of norm one.
(1) Show that S is a (multiplicative) subgroup of H \ {0}, i.e., x, y ∈S implies
xy ∈S and x−1 ∈S.
(2) Prove that the function f : S × S →S deﬁned by f(x, y) = y−1x−1yx
maps onto S. Hint: Use the result of Ex. 2.7.21.
Ex. 2.7.23. Deﬁne S as in Ex. 2.7.22.
(a) Show that every automorphism or antiautomorphism of H maps S onto itself.
(b) Show that every automorphism φ of H has at least four ﬁxed points x in
S, i.e., x ∈S such that φ(x) = x. Hint: Every 3 × 3 real orthogonal matrix with
determinant one has eigenvalue 1.
Ex. 2.7.24. Verify the following formula for the congruence orbit of x ∈H:
Con (x) = {(R(x))r + S|Vx|r : r > 0},
where S is deﬁned in (2.2.6).
Ex. 2.7.25. Verify that if at least one of a, b ∈H is nonreal, then the subspaces
V±
a,b (deﬁned in Section 2.3) are uniquely determined by a and b.
Ex. 2.7.26. Verify formula (2.4.7).
2.8
NOTES
Most of the material in this chapter is standard (except Section 2.3), although some
of it, e.g., Theorem 2.2.1, is diﬃcult to locate in the literature. The representation of
quaternion multiplication as rotations in 3-dimensional real vector space, as stated
in Corollary 2.2.4, as well as connection with cross products (2.1.1), is a key to
many applications in geometry, mechanics, and engineering, which is written up in
many books; see, e.g., Ward [157]. For applications in feedback regulator problems
and stability analysis, see, e.g., Wie et al. [159].
Proposition 2.1.4 contains basic elementary properties of H; a large set of such
properties is found in Zhang [164]. The result of Theorem 2.3.3(2), as well as that
of Theorem 2.3.4(a), is found in Johnson [74]. Ex. 2.7.21 is taken from Koecher
and Remmert [81].
The material of Section 2.3 is taken from Karow [79]. Theorem 2.5.2 and Corol-
lary 2.5.3, as well as the proof of Theorem 2.5.1(b), were suggested by Karow [80].
For some material on antiautomorphisms of quaternions, see von Randow [127].
We mention the following “fundamental theorem of algebra” concerning poly-
nomial equations in quaternions:
Theorem 2.8.1. Let f(x) be a function of the form
f(x) = a0xa1xa2 · · · an−1xan + g(x),
x ∈H,
where a0, a1, . . . an ∈H \ {0}, and g(x) is a ﬁnite sum of monomials of the form
b0xb1x · · · bk−1xbk with k < n. Then there exists x0 ∈H such that f(x0) = 0.

THE ALGEBRA OF QUATERNIONS
27
Theorem 2.8.1 was proved by Eilenberg and Niven [36]. The complete proof is
based on topological methods and is beyond the scope of this book.
Quaternions were a starting point in many important developments in modern
algebra: octonions, division algebras, and Cliﬀord algebras, to name a few. While
we cannot here go in depth into any of these developments, some references for
further reading are provided: Conway and Smith [30], Lounesto [103], and Cohn
[29]. In particular, note that the division ring of quaternions is a special case of a
ﬁnite dimensional central simple algebra over a ﬁeld. Many well-known results for
such algebras apply to H, for example, the Skolem-Noether theorem that asserts
(in a basic formulation) that every automorphism of such algebras is inner (cf.
Proposition 2.4.7); see, e.g., Berhuy and Oggier [14] or Farb and Dennis [37] for
more details.
Rotations transforms in 3- and 4-dimensional real vector space via quaternions
(cf. Corollary 2.2.4) and related topics are studied in depth in many books, e.g.,
Ward [157], Vince [153], and Kuipers [84].
For remarks and further references concerning historical developments of quater-
nions, consult Koecher and Remmert [81] or Vince [153].

Chapter Three
Vector spaces and matrices: Basic theory
We introduce the basic structures in the quaternion vector space Hn×1 and in
quaternion matrix algebras, including various types of matrix decompositions and
factorizations. In particular, representations of quaternion matrix algebras in terms
of real and complex matrix algebra are developed. Numerical ranges, joint numer-
ical ranges, and their convexity properties are emphasized. We also provide an
Appendix containing a few basic facts on analysis of sets and continuous multivari-
able functions that are used in the book.
3.1
FINITE DIMENSIONAL QUATERNION VECTOR SPACES
Consider Hn×1, the set of all n-component columns with quaternion components,
as a right quaternion vector space; in other words, besides the standard addition,
we consider multiplication on the right by quaternions: For v =


v1
...
vn

∈Hn×1,
vj ∈H, and α ∈H, we let
vα =


v1α
...
vnα

∈Hn×1.
We will use standard linear algebra concepts and properties of vectors and subspaces
of Hn×1, such as linear independence, spanning set (of a subspace), basis, dimension,
direct sum, etc.; they work in exactly the same way as for ﬁnite dimensional vector
spaces over (commutative) ﬁelds. See, e.g., Hungerford [69] or Wan [156].
We denote by dim (M), or dimH (M) if the quaternion nature of M is to be
emphasized, the dimension (understood in the sense of right quaternion vector
spaces) of a subspace M of Hn×1. The subspace spanned by v1, . . . , vp ∈Hn×1 is
denoted
SpanH {v1, . . . , vp} := {v1α1 + · · · + vpαp : α1, . . . , αp ∈H}.
As an example, we prove the following replacement theorem:
Theorem 3.1.1. Let u1, . . . , us be a linearly independent subset of
SpanH {v1, . . . , vp},
where v1, . . . , vp ∈Hn×1.
Then there exist s elements vi1, . . . , vis, 1 ≤i1 < i2 < · · · < is ≤r, such that upon
replacing vi1, . . . , vis with u1, . . . , us, respectively, in v1, . . . , vp, a spanning set for
SpanH {v1, . . . , vp} is obtained.

VECTOR SPACES AND MATRICES: BASIC THEORY
29
Proof.
We use induction on s.
Suppose s = 1.
Then u1 ̸= 0 and u1 =
v1α1 + v2α2 + · · · + vpαp for some α1, . . . , αp ∈H. Not all the αj’s are zeros; if
αi1 ̸= 0, then it is easy to see that replacement of vi1 by u1 results in a spanning
set for SpanH {v1, . . . , vp}.
Assume now that the theorem holds for s −1. By this assumption, there exist
vi1, . . . , vis−1 such that
u1, . . . , us−1, vis, . . . , vir
is a spanning set for SpanH {v1, . . . , vp}; here {i1, . . . , is, . . . , ir} is a permutation of
{1, 2, . . . , r}. Since
us ∈SpanH {v1, . . . , vp} = SpanH {u1, . . . , us−1, vis, . . . , vir},
we have
us = u1α1 + · · · + us−1αs−1 + visαs + · · · + virαr,
where α1, . . . , αr ∈H. (3.1.1)
Since u1, . . . , us are linearly independent, there is a nonzero quaternion among
αs, . . . , αr.
We can adjust the permutation {i1, . . . , is, . . . , ir} so that αs ̸= 0.
Then, by replacing vis with us, we get a set such that
SpanH {u1, . . . , us−1, us, vis+1, . . . , vir}
= SpanH {u1, . . . , us−1, vis, vis+1, . . . , vir}.
(3.1.2)
Indeed, inclusion ⊆in (3.1.2) holds in view of (3.1.1), and since (3.1.1) can be
solved for vis:
vis = −u1α1α−1
s
−· · · −us−1αs−1α−1
s
+ usα−1
s
−vis+1αis+1α−1
s
−· · · −virαrα−1
s ;
the converse inclusion holds as well.
□
Deﬁnition 3.1.2. For v =


v1
...
vn

∈Hn×1, deﬁne the adjoint as the n-
component row v∗= [v∗
1 v∗
2
. . . v∗
n].
The vector space Hn×1 is endowed with the quaternion-valued inner product
⟨u, v⟩= v∗u, u, v ∈Hn×1. Observe the following properties of ⟨·, ·⟩:
⟨u1α1 + u2α2, v⟩
=
⟨u1, v⟩α1 + ⟨u2, v⟩α2,
u1, u2, v ∈Hn×1,
α1, α2 ∈H;
⟨u, v1α1 + v2α2⟩
=
α∗
1⟨u, v1⟩+ α∗
2⟨u, v2⟩,
u, v1, v2 ∈Hn×1,
α1, α2 ∈H;
⟨u, v⟩
=
⟨v, u⟩∗,
u, v ∈Hn×1;
⟨u, u⟩≥0 for all u ∈Hn×1, with equality only if u = 0.
Deﬁnition 3.1.3. We say that u, v ∈Hn×1 are orthogonal if ⟨u, v⟩= 0.
A p-tuple {v1, . . . , vp}, where v1, . . . , vp ∈Hn×1, is said to be orthogonal if
⟨vi, vj⟩= 0 for i ̸= j, and orthonormal if it is orthogonal and ⟨vi, vi⟩= 1 for
i = 1, 2, . . . , p.

30
CHAPTER 3
Standard concepts and results related to orthogonality, in particular, the Gram-
Schmidt algorithm, apply in this context, with essentially the same proofs (see,
e.g., Farenick and Pidkowich [38]; note that the deﬁnition of the inner product in
Farenick and Pidkowich is slightly diﬀerent). Thus, every nonzero subspace of Hn×1
has an orthonormal basis.
We use ∥u∥H :=
p
⟨u, u⟩, also denoted ∥u∥, as the norm on Hn×1. Analogous
norms on real and complex vector spaces are used: ∥u∥F =
√
u∗u for u ∈Fn×1,
where F = R or F = C.
Fact 3.1.4. Let F = R or F = C. It is easy to see that vectors v1, . . . , vp ∈
Fn×1 are linearly independent over F if and only if they are linearly indepen-
dent over H.
Thus, dimF Span {v1, . . . , vp} (as a subspace of Fn×1) is equal to
dimH Span {v1, . . . , vp} (as a subspace of Hn×1).
3.2
MATRIX ALGEBRA
Denote by Hm×n the set of all m × n matrices with entries in H, considered as a
left quaternion vector space, with the standard matrix addition and multiplication:
AB ∈Hp×m, where A ∈Hp×n, B ∈Hn×m.
The multiplication of quaternion
matrices, when deﬁned, is associative. The setting of the left quaternion vector
space allows us to interpret A ∈Hm×n as a linear transformation Hm×1 →Hn×1,
which acts by the standard matrix-vector multiplication. (There is an ambiguity
of notation here: Hn×1 is a right vector space, but Hn×1 is a left vector space when
considered as the set of linear transformations H →Hn×1. It will be clear from
context which interpretation applies in every particular instance.)
Deﬁnition 3.2.1. Deﬁne the adjoint matrix : A∗= [a∗
j,i]n,m
i=1,j=1 ∈Hm×n for
A = [ai,j]n,m
i=1,j=1 ∈Hn×m, where ai,j ∈H.
For example,
 1
i
j
k
i + j
1 + k
∗
=


1
−k
−i
−i −j
−j
1 −k

.
Note the following algebraic properties:
(a) (αA + βB)∗= A∗α∗+ B∗β∗,
for all α, β ∈H,
A, B ∈Hm×n.
(b) (Aα + Bβ)∗= α∗A∗+ β∗B∗,
for all α, β ∈H,
A, B ∈Hm×n.
(c) (AB)∗= B∗A∗,
for all A ∈Hm×n,
B ∈Hn×p.
(d) (A∗)∗= A,
for all A ∈Hm×n.
(e) If A ∈Hn×n is invertible, then (A∗)−1 = (A−1)∗.
In particular, the adjoint operation is real linear. The classes of matrices familiar
in the matrix analysis of real and complex matrices are introduced in the same way
for quaternion matrices:
Deﬁnition 3.2.2. A matrix A ∈Hn×n is called hermitian, positive deﬁnite,
positive semideﬁnite, skewhermitian, invertible (nonsingular), unitary, or normal
if A = A∗, x∗Ax is real and positive for all x ∈Hn×1 \ {0}, x∗Ax is real and
nonnegative for all x ∈Hn×1 and A = −A∗, there exists A−1 ∈Hn×n such that
A−1A = AA−1 = I, A is invertible and A−1 = A∗, or AA∗= A∗A, respectively.

VECTOR SPACES AND MATRICES: BASIC THEORY
31
We shall see later—Proposition 3.5.1—that x∗Ax is real for all x ∈Hn×1 if and
only if A is hermitian.
Deﬁnition 3.2.3. The rank of A ∈Hm×n is deﬁned as the (quaternion) dimen-
sion of
Ran (A) := {Ax : x ∈Hm×1},
the range of A; by convention, rank (0) = 0.
Several elementary properties of the rank function that will be useful in the
sequel are listed in the next proposition. For A ∈Hm×n, we denote by Aφ ∈Hn×m
the matrix obtained from the transposed matrix AT by applying an involution φ
entrywise.
Proposition 3.2.4. Let A ∈Hm×n. Then:
(1) if S ∈Hm×n, T ∈Hm×n are invertible, then rank (SAT) = rank A;
(2) rank A∗= rank A;
(3) if φ is a nonstandard involution, then rank Aφ = rank A.
See Section 3.6 for the algebraic properties of the map A 7→Aφ.
Many matrix decompositions that are standard in real and complex matrix
analysis remain valid for quaternion matrices, with essentially the same proofs. In
particular, the following holds.
Proposition 3.2.5. Let A ∈Hm×n. Then:
(a) A = XA0, where X ∈Hm×m is invertible and A0 ∈Hm×n is a row reduced
echelon form; moreover, A0 is unique, i.e., uniquely determined by A;
(b) A = A′
0Y , where Y ∈Hn×n is invertible and A′
0 ∈Hm×n is a column reduced
echelon form; moreover, A′
0 is unique;
(c) (QR factorization) if m = n, then A = QR, where Q is unitary and R is upper
triangular with nonnegative diagonal elements; moreover, if A is invertible,
then Q and R are unique;
(d) (polar decomposition) if m = n, then A = RU, where R is positive semideﬁ-
nite and U is unitary; moreover, if A is invertible, then R and U are unique;
(e) (rank decompositions) if rank (A) = k ̸= 0, then A = BC, where B ∈Hm×k,
C ∈Hk×n; also,
A = eB
 Ik
0
0
0(m−k)×(n−k)

eC,
where eB ∈Hm×m, eC ∈Hn×n are invertible;
(f) (singular value decomposition) if A ̸= 0, then there exist unitary U ∈Hm×m,
V ∈Hn×n, and real positive numbers a1 ≥a2 ≥· · · ≥ak, where k = rank (A),
such that
A = U
 diag (a1, . . . , ak)
0
0
0(m−k)×(n−k)

V ;
moreover, the aj’s are unique.

32
CHAPTER 3
For example, rank decompositions can be easily deduced from (a) and (b).
Deﬁnition 3.2.6. The aj’s of Proposition 3.2.5(f) are called the singular values
of A; by convention, the singular values of the zero matrix are zeros.
We also note the Cholesky decomposition (Proposition 3.2.7(b) and (c) below),
again proved analogously to the familiar cases of real and complex matrices:
Proposition 3.2.7. The following statements are equivalent for A ∈Hn×n,
A ̸= 0:
(a) A is positive semideﬁnite;
(b) A = L∗L for some lower triangular L ∈Hn×n with nonnegative elements on
the diagonal; moreover, L is unique;
(c) A = U ∗U for some upper triangular U ∈Hn×n with nonnegative elements on
the diagonal; moreover, U is unique;
(d) A = B∗B for some B ∈Hk×n, where k = rank (A).
For matrices, we use the operator norm throughout:
∥A∥H = max{∥Au∥H : u ∈Hn×1,
∥u∥H = 1},
where A ∈Hm×n. It is immediate to see that ∥· ∥H is indeed a multiplicative norm
on matrices:
∥A + B∥H ≤∥A∥H + ∥B∥H,
∥αA∥H = ∥A∥H |α|,
∥AC∥H ≤∥A∥H · ∥C∥H,
and ∥A∥H ≥0 with ∥A∥H = 0 only if A = 0; for all A, B ∈Hm×n, all C ∈Hn×k,
and all α ∈H.
Note that ∥A∥H coincides with the largest singular value of A. Analogous norms
on real and complex matrices are used:
∥A∥F = max{∥Au∥F : u ∈Fn×1,
∥u∥F = 1}
for A ∈Fm×n, where F = R or F = C. Since for A ∈Fm×n we have ∥A∥F = ∥A∥H
(indeed, both ∥A∥F and ∥A∥H are equal to the largest singular value of A), the ∥· ∥
notation for vector and matrix norms will be used unambiguously.
For future reference, we record the following characterization of linear indepen-
dence.
Proposition 3.2.8. Let A ∈Hm×n. Then the columns of A are linearly indepen-
dent (as elements of a right quaternion vector space) if and only A is left-invertible,
i.e., BA = I for some B ∈Hn×m.
Analogously, the rows of A are linearly independent (as elements of a left quater-
nion vector space) if and only if A is right-invertible.
A proof follows from a rank decomposition for A (see also Ex. 3.11.4).

VECTOR SPACES AND MATRICES: BASIC THEORY
33
3.3
REAL MATRIX REPRESENTATION OF QUATERNIONS
We extend to matrices the standard representation of quaternions as 4 × 4 real
matrices of Proposition 2.6.1. Deﬁne the map
χ : H →R4×4,
χ(a0 + a1i + a2j + a3k) =


a0
−a1
a3
−a2
a1
a0
−a2
−a3
−a3
a2
a0
−a1
a2
a3
a1
a0

,
where a0, a1, a2, a3 ∈R, and its matrix extension
χm,n : Hm×n →R4m×4n,
χm,n
 [xi,j]m,n
i,j=1

= [χ(xi,j)]m,n
i,j=1,
where xi,j ∈H. The following properties are routinely veriﬁed:
(i) χn,n is an isomorphism of the real algebra Hn×n onto the real subalgebra

[zi,j]n
i,j=1 : zi,j ∈{λI4 + S : λ ∈R,
S has the form (2.6.1)}
	
of R4n×4n, and χn,n(I) = I.
(ii) If X ∈Hm×n, Y ∈Hn×p, then χm,p(XY ) = χm,n(X)χn,p(Y ).
(iii) If X, Y ∈Hm×n and s, t ∈R, then
χm,n(sX + tY ) = sχm,n(X) + tχm,n(Y ).
(iv) χn,m(X∗) = (χm,n(X))T ,
for all X ∈Hm×n.
(v) There exist positive constants cm,n, Cm,n such that
cm,n∥χm,n(X)∥R ≤∥X∥H ≤Cm,n∥χm,n(X)∥R
for every X ∈Hm×n.
In fact, cm,n can be taken to be the minimal value of the nonzero continuous
function ∥X∥H on the compact set
{X ∈Hm×n : ∥χm.n(X)∥R = 1}
(cf. Theorem 3.10.5); an analogous statement is valid for 1/Cm,n.
Often, we will abbreviate χm,n to χ (with m, n understood from context).
The following properties of the map χ are useful.
Proposition 3.3.1. A matrix A ∈Hn×n is hermitian, positive deﬁnite, positive
semideﬁnite, skewhermitian, unitary, or normal if and only if χ(A) is symmet-
ric, positive deﬁnite, positive semideﬁnite, skewsymmetric, orthogonal, or normal,
respectively.
The proof is left as Ex. 3.11.5. To prove that if A is positive semideﬁnite, then
so is χ(A), use the Cholesky decomposition for A and property (ii) of χ.
Note that automorphisms, resp., antiautomorphisms, of H translate into unitary
similarity, resp., unitary similarity followed by transposition, in terms of the map
χ, a fact that will be useful in the sequel.

34
CHAPTER 3
Proposition 3.3.2. Let φ be an automorphism, resp. antiautomorphism, of H,
and let φ(A) be the matrix obtained from A ∈Hm×n, resp. AT ∈Hn×m, by applying
φ entrywise. Then there exist real orthogonal matrices Uφ,n ∈R4n×4n (independent
of A) such that
χ(φ(A)) = U T
φ,m(χ(A))Uφ,n
∀A ∈Hm×n,
resp.
χ(φ(A)) = U T
φ,n(χ(A))T Uφ,m
∀A ∈Hm×n.
Proof. We prove only the part for automorphisms. Use the fact that every
automorphism of H is inner (Proposition 2.4.7): there exists α0 ∈H with |α0| = 1
such that φ(α) = α−1
0 αα0 for every α ∈H. Then let
Uφ,n = χ(α0) ⊕χ(α0) ⊕· · · ⊕χ(α0)
|
{z
}
n
times
,
Uφ,m = χ(α0) ⊕χ(α0) ⊕· · · ⊕χ(α0)
|
{z
}
m
times
.
□
Proposition 3.3.3. Let u1, . . . , up ∈Hn×1. Then u1, . . . , up are linearly in-
dependent (over H) if and only if the columns of χn,p ([u1 . . . up]) are linearly
independent (over R).
Moreover, u1, . . . , up is an orthonormal, resp. orthogonal, set (over H) if and
only if the columns of χn,p ([u1 · · · up]) form an orthonormal, resp. orthogonal,
set (over R).
Proof. Assume χn,p ([u1 · · · up]) are linearly independent. Let α1, . . . , αp ∈H
be such that
[u1 · · · up]


α1
...
αp

= 0.
Applying χ to this equality and using the linear independence of χn,p[u1 . . . up],
we obtain that every column of χp,1





α1
...
αp




is zero. The deﬁnition of χ now
implies that αj = 0, j = 1, 2, . . . , p.
Assume u1, . . . , up are linearly independent. Let
χn,p ([u1 · · · up])


a1
...
ap

= 0
(3.3.1)
for some a1, . . . , ap ∈R4×1, and, in turn,
aj =


aj0
aj1
−aj3
aj2

,
ajℓ∈R,
ℓ= 0, 1, 2, 3.

VECTOR SPACES AND MATRICES: BASIC THEORY
35
Write
uj =


uj1
...
ujn

,
ujk ∈H,
(3.3.2)
and, in turn,
ujk = u(0)
jk + u(1)
jk i + u(2)
jk j + u(3)
jk k,
u(ℓ)
jk ∈R,
ℓ= 0, 1, 2, 3.
(3.3.3)
It will be convenient to use the following notation for block row matrices:
rowj=1,2,...,pXj = [X1 X2 . . . Xp].
(3.3.4)
Equation (3.3.1) now can be rewritten in the form
rowj=1,2,...,p


u(0)
jk
−u(1)
jk
u(3)
jk
−u(2)
jk
u(1)
jk
u(0)
jk
−u(2)
jk
−u(3)
jk
−u(3)
jk
u(2)
jk
u(0)
jk
−u(1)
jk
u(2)
jk
u(3)
jk
u(1)
jk
u(0)
jk


·


a10
a11
−a13
a12
a20
a21
−a23
a22
...
ap0
ap1
−ap3
ap2


= 0,
(3.3.5)
for k = 1, 2, . . . , n. Using (3.3.5), a straightforward computation veriﬁes that the
second, third, and forth columns in the matrix
rowj=1,2,...,p


u(0)
jk
−u(1)
jk
u(3)
jk
−u(2)
jk
u(1)
jk
u(0)
jk
−u(2)
jk
−u(3)
jk
−u(3)
jk
u(2)
jk
u(0)
jk
−u(1)
jk
u(2)
jk
u(3)
jk
u(1)
jk
u(0)
jk


·


χ(a10 + a11i + a12j + a13k)
...
χ(ap0 + ap1i + ap2j + ap3k)


are zero. Together with (3.3.5) this implies
p
X
j=1
uj(aj0 + aj1i + aj2j + aj3k) = 0,
and the linear independence of u1, . . . , up yields ajℓ= 0 for all j and ℓ, as required.
The statement of Proposition (3.3.3) concerning orthonormal and orthogonal
sets follows easily from properties (ii) and (iv) of χ, taking advantage of the or-
thogonality of the columns (and rows) of χ(x), for any x ∈H.
□
If u1, . . . , up of Proposition 3.3.3 form a basis of a subspace M ⊆Hn×1, then
the columns of χ([u1 . . . up]) form a basis of a subspace, denoted χ(M), of R4n×1.

36
CHAPTER 3
Note that χ(M) is independent of the choice of the basis u1, . . . , up. Indeed, if
u1, . . . , up and u′
1, . . . , u′
p are two bases for M, then
[u′
1 . . . u′
p] = [u1 . . . up] · S
(3.3.6)
for some invertible matrix S ∈Hp×p. Applying the map χ to (3.3.6), we see the
columns of χ([u1 . . . up]) and those of χ([u′
1 . . . u′
p]) span the same subspace in
R4n×1.
3.4
COMPLEX MATRIX REPRESENTATION OF QUATERNIONS
For α = a0 + ia1 + ja2 + ka3 ∈H, a0, a1, a2, a3 ∈R, we deﬁne the map ω as in
Proposition 2.6.2,
ω(α) =

a0 + ia1
a2 + ia3
−a2 + ia3
a0 −ia1

∈C2×2,
and extend ω entrywise to a map
ωm,n : Hm×n →C2m×2n,
ωm,n
 [xi,j]m,n
i,j=1

= [ω(xi,j)]m,n
i,j=1,
(3.4.1)
where xi,j ∈H. Analogously to the map χ, we have:
(i) ωn,n is a unital isomorphism of the real algebra Hn×n onto the real subalgebra

[zi,j]n
i,j=1 : zi,j ∈{

u
v
−v
u

, u, v ∈C}

of C2n×2n;
(ii) if X ∈Hm×n, Y ∈Hn×p, then ωm,p(XY ) = ωm,n(X)ωn,p(Y );
(iii) if X, Y ∈Hm×n and s, t ∈R, then
ωm,n(sX + tY ) = sωm,n(X) + tωm,n(Y );
(iv) ωn,m(X∗) = (ωm,n(X))∗,
for all X ∈Hm×n;
(v) there exist positive constants km,n, Km,n such that
km,n∥ωm,n(X)∥C ≤∥X∥H ≤Km,n∥ωm,n(X)∥C
for every X ∈Hm×n.
Often, we will abbreviate ωm,n to ω (with m, n understood from the context).
Proposition 3.4.1. A matrix A ∈Hn×n is hermitian, positive deﬁnite, positive
semideﬁnite, skewhermitian, unitary, or normal if and only if ω(A) is such.
See Ex. 3.11.6.
Proposition 3.4.2. Let u1, . . . , up ∈Hn×1. Then u1, . . . , up are linearly in-
dependent (over H) if and only if the columns of ωn,p ([u1 . . . up]) are linearly
independent (over C).
Moreover, u1, . . . , up is an orthonormal, resp. orthogonal, set (over H) if and
only if the columns of ωn,p ([u1 · · · up]) form an orthonormal, resp. orthogonal,
set (over C).

VECTOR SPACES AND MATRICES: BASIC THEORY
37
Proof. The “if” part is proved as in the proof of Proposition 3.3.3.
Assume u1, . . . , up are linearly independent. Let
ωn,p ([u1 . . . up])


a1
...
ap

= 0
(3.4.2)
for some a1, . . . , ap ∈C2×1, and, in turn,
aj =
 aj0
aj1

,
aj0, aj1 ∈C.
Partition uj as in (3.3.2), (3.3.3). Then (using the notation (3.3.4)) we have
rowj=1,2,...,p
"
u(0)
jk + iu(1)
jk
u(2)
jk + iu(3)
jk
−u(2)
jk + iu(3)
jk
u(0)
jk −iu(1)
jk
#
·


a10
a11
a20
a21
...
ap0
ap1


= 0,
(3.4.3)
for k = 1, 2, . . . , n. A straightforward computation shows that (3.4.3) also yields
rowj=1,2,...,p
"
u(0)
jk + iu(1)
jk
u(2)
jk + iu(3)
jk
−u(2)
jk + iu(3)
jk
u(0)
jk −iu(1)
jk
#
·


−a11
a10
...
−ap1
ap0


= 0,
for k = 1, 2, . . . , n, and
rowj=1,2,...,p
"
u(0)
jk + iu(1)
jk
u(2)
jk + iu(3)
jk
−u(2)
jk + iu(3)
jk
u(0)
jk −iu(1)
jk
#
·


ω(a10 −ja11)
...
ω(ap0 −jap1)

= 0,
which, in turn, gives
p
X
j=1
uj(aj0 −jaj1) = 0.
By the linear independence of u1, . . . , up we obtain aj = 0, j = 1, . . . , p, as required.
The statement of Proposition (3.4.2) concerning orthonormal and orthogonal
sets follows easily from properties (ii) and (iv) of ω, taking advantage of the or-
thogonality of the columns (and rows) of ω(x), for any x ∈H.
□
If M is a subspace of Hn×1, we let ω(M) be the subspace of C2n×1 spanned by
the columns of ω([u1 . . . up]), where u1, . . . , up is a basis for M. Analogously to
χ(M), one veriﬁes that ω(M) is independent of the choice of a basis for M.

38
CHAPTER 3
3.5
NUMERICAL RANGES WITH RESPECT TO CONJUGATION
In this section we will work with the standard involution, i.e., the conjugation. Let
A ∈Hn×n. Consider the quadratic form
⟨Ax, x⟩:= x∗Ax,
x ∈Hn×1.
We have the polarization identity (3.5.1), which can be veriﬁed by a straightforward
computation, as follows. Let (q1, q2, q3) be a units triple of quaternions. Using
Proposition 2.4.2, for A ∈Hn×n we then have
3
X
j=1
 3
X
k=0
qk
j (x + yqk
j )∗A(x + yqk
j )
!
=
12 y∗Ax + 8 V (x∗Ay)
for all
x, y ∈Hn×1.
(3.5.1)
Proposition 3.5.1. Let A ∈Hn×n. Then:
(1) x∗Ax = 0 for all x ∈Hn×1 if and only if A = 0;
(2) x∗Ax ∈R for all x ∈Hn×1 if and only if A = A∗;
(3) R (x∗Ax) = 0 for all x ∈Hn×1 if and only if A = −A∗.
Proof. The “if” parts are obvious. For the “only if” part of (1), assuming
x∗Ax = 0 for all x ∈Hn×1, (3.5.1) gives
R(y∗Ax) = 0
and
V (x∗Ay) = −3
2V (y∗Ax)
for all x, y ∈Hn×1.
Iterating the second equality we obtain
V (x∗Ay) = −3
2V (y∗Ax) = 9
4V (x∗Ay) ,
for all x, y ∈Hn×1;
hence, V (x∗Ay) = 0 for all x, y ∈Hn×1, and A = 0 follows.
The “only if” part of (2), resp. (3), follows by applying (1) to the matrix A−A∗,
resp. A + A∗.
□
Deﬁnition 3.5.2. The set
W H
∗(A) :=

x∗Ax : x∗x = 1,
x ∈Hn×1	
⊂H
is known as the (quaternion) numerical range of A ∈Hn×n with respect to the
conjugation.
Clearly, W H
∗(A) is compact and connected (Theorem 3.10.7).
Also, if y1 ∈
W H
∗(A) and y2 ∈H is such that Ry2 = Ry1 and |Vy2| = |Vy1|, then also y2 ∈
W H
∗(A). Indeed, by Theorem 2.2.6 we have y2 = a∗y1a for some a ∈H, |a| = 1, so
if y1 = x∗Ax for some x ∈Hn×1, x∗x = 1, then y2 = (xa)∗A(xa). Proposition 3.5.1
yields the following.
Corollary 3.5.3. For A ∈Hn×n, we have W H
∗(A) = {0}, resp. W H
∗(A) ⊂R or
R(W H
∗(A)) = {0}, if and only if A = 0, resp. A = A∗or A = −A∗.

VECTOR SPACES AND MATRICES: BASIC THEORY
39
Indeed, the “if” part is obvious.
Assume W H
∗(A) = {0}.
Then, by scaling
x ∈Hn×1, i.e., multiplying x on the right by a nonzero quaternion, we obtain that
x∗Ax = 0 for every nonzero x ∈H. Thus, x∗Ax = 0 for all x ∈Hn×1, and it remains
to apply Proposition 3.5.3(1) to complete the proof of the ﬁrst part of Corollary
3.5.3. The proof of other parts of the corollary follows the same pattern.
Some algebraic properties of the numerical ranges are worth noticing.
Proposition 3.5.4. For A ∈Hn×n, unitary U ∈Hn×n, and real a, we have
W H
∗(U ∗AU) = W H
∗(A),
W H
∗(A + aI) = a + W H
∗(A),
W H
∗(aA) = aW H
∗(A).
In contrast to the classical convexity property (the Toeplitz-Hausdorﬀtheorem)
of numerical ranges of complex matrices
W C
∗(B) :=

x∗Bx : x∗x = 1,
x ∈Cn×1	
⊂C,
where B ∈Cn×n (Theorem 3.5.7 below), the quaternion numerical ranges are gen-
erally nonconvex:
Example 3.5.5. Let
A =
 λ
0
0
A0

,
n ≥2,
where λ ∈H\{0} has zero real part, and A0 ∈H(n−1)×(n−1) is hermitian and either
positive or negative deﬁnite. Clearly, λ ∈W H
∗(A), and, therefore, also −λ ∈W H
∗(A)
(because −λ is congruent to λ). But one easily checks that 0 = 1
2λ + 1
2(−λ) does
not belong to W H
∗(A). Indeed, if we had
x∗λx + y∗A0y = 0,
for some x ∈H,
y ∈H(n−1)×1,
then, since R (x∗λx) = 0 and V (y∗A0y) = 0, we must have
x∗λx = y∗A0y = 0,
which yields x = 0 and y = 0.
□
Nevertheless, some convexity-like properties of quaternion numerical ranges turn
out to be valid. For example, it is proved by So and Thompson [148] that the
intersection of W H
∗(A) with the closed upper half-plane of the complex plane (or,
for that matter, with the closed upper half-plane of any 2-dimensional plane in
R4 = SpanR {1, i, j, k} that contains the reals) is convex, for any A ∈Hn×n.
We now introduce joint numerical ranges.
Deﬁnition 3.5.6. For F ∈{R, C, H} and for a p-tuple of hermitian (real sym-
metric in the case F = R) matrices A1, . . . , Ap ∈Fn×n, the F-joint numerical range
is deﬁned by
WJF
∗(A1, . . . , Ap) :=

(x∗A1x, . . . , x∗Apx) ∈Rp : x∗x = 1,
x ∈Fn×1	
⊂Rp.
Here x∗= xT if F = R.
Since WJF
∗(A1, . . . , Ap), F ∈{R, C, H}, are ranges of continuous functions de-
ﬁned on the compact sets {x ∈Fn×1 : x∗x = 1}, it follows that WJF
∗(A1, . . . , Ap)
themselves are compact.
The following is a basic convexity result for numerical ranges.

40
CHAPTER 3
Theorem 3.5.7. Let F be one of R, C, or H, and assume that n ̸= 2 in the case
F = R. Then the set WJF
∗(A, B) is convex for every pair of hermitian (symmetric
in the case F = R) matrices A, B ∈Fn×n.
The case F = C is just a reformulation of the Toeplitz-Hausdorﬀtheorem.
Proof. The case n = 1 is trivial, so assume n ≥2 (and n ≥3 in the real case).
We prove ﬁrst the following statement: if (0, 0) ̸∈WJF
∗(A, B), then for any
(a, b) ∈R2, (a, b) and (−a, −b) cannot both belong to WJF
∗(A, B).
Arguing by contradiction, suppose
(a, b) = (u∗
1Au1, u∗
1Bu1),
(−a, −b) = (u∗
2Au2, u∗
2Bu2)
for some u1, u2 ∈Fn×1, ∥u1∥= ∥u2∥= 1. Obviously, u1 and u2 are linearly
independent over F. Because of our assumptions on n, there exists u3 ∈Fn×1 such
that u1, u2, u3 are linearly independent over R. Now, for any (x, y, z) ∈R3 we have
(u1x + u2y + u3z)∗A(u1x + u2y + u3z)
= a(x2 −y2) + a1xy + a2xz + a3yz + a4z2
for some real numbers a1, a2, a3, a4 and
(u1x + u2y + u3z)∗B(u1x + u2y + u3z)
= b(x2 −y2) + b1xy + b2xz + b3yz + b4z2
for some real numbers b1, b2, b3, b4. For example, b1 = u∗
2Bu1 + u∗
1Bu2. It turns
out that the system
a(x2 −y2) + a1xy + a2xz + a3yz + a4z2
=
0,
b(x2 −y2) + b1xy + b2xz + b3yz + b4z2
=
0
(3.5.2)
has a nontrivial solution (x, y, z) ∈R3. Let us verify this claim. If ab1 −ba1 = 0,
set z = 0, and the resulting homogeneous linear system in the variables x2 −y2
and xy has a nontrivial solution, leading to a nontrivial solution (x, y, 0) of (3.5.2).
If ab1 −ba1 ̸= 0, then set z = 1. Since ab1 −ba1 ̸= 0, the system resulting from
(3.5.2) is equivalent to
x2 −y2 + a′
2x + a′
3y + a′
4 = 0,
xy + b′
2x + b′
3y + b′
4 = 0,
(3.5.3)
for some real numbers a′
2, a′
3, a′
4, b′
2, b′
3, b′
4. Making the change of variables x
7→
x + a′
2/2, y 7→y −a′
3/2, we may (and do) assume without loss of generality, that
a′
2 = a′
3 = 0. Solve the second equation in (3.5.3) for y (assuming x ̸= −b′
3), to
obtain y = (−b′
4 −b′
2x)(x+b′
3)−1, and substitute in the ﬁrst equation, which results
in
(x + b′
3)2x2 −(b′
4 + b′
2x)2 + a′
4(x + b′
3)2 = 0.
(3.5.4)
Unless b′
4 −b′
2b′
3 = 0, the value of the quartic polynomial in the left-hand side of
(3.5.4) at x = −b′
3 is negative; hence, the polynomial has a real root diﬀerent from
−b′
3, leading to a nontrivial solution (x, y, z) of (3.5.2). In the remaining case, when
b′
4 −b′
2b′
3 = 0, the second equation in (3.5.3) takes the form (x + b′
3)(y + b′
2) = 0;
thus, x = −b′
3 or y = −b′
2. If x = −b′
3, then the ﬁrst equation in (3.5.3) boils down
to
y2 = (b′
3)2 + a′
4,
(3.5.5)

VECTOR SPACES AND MATRICES: BASIC THEORY
41
and if y = −b′
2, then the ﬁrst equation in (3.5.3) is
x2 = (b′
2)2 −a′
4.
(3.5.6)
Clearly, at least one of (3.5.5) and (3.5.6) has a real solution. We have shown that
the system (3.5.2) has a nontrivial solution (x0, y0, z0) ∈R3. But now
(u1x0 + u2y0 + u3z0)∗A(u1x0 + u2y0 + u3z0)
= (u1x0 + u2y0 + u3z0)∗B(u1x0 + u2y0 + u3z0) = 0;
thus, (0, 0) ∈WJF
∗(A, B), a contradiction to our hypothesis.
This proves the
statement.
Now the proof of convexity of WJF
∗(A, B) can be easily completed. Assuming
the contrary, suppose (a, b), (a′, b′) ∈WJF
∗(A, B) but
(α0a + (1 −α0)a′, α0b + (1 −α0)b′) ̸∈WJF
∗(A, B)
for some real α0, 0 < α0 < 1. Let (α′, α′′) ⊆(0, 1) be the maximal open interval
that contains α0 and is such that
(αa + (1 −α)a′, αb + (1 −α)b′) ̸∈WJF
∗(A, B)
for all α ∈(α′, α′′).
(The existence of such an interval (α′, α′′) is guaranteed in view of the openness of
the complement of the numerical range WJF
∗(A, B).) Note that
(α′a + (1 −α′)a′, α′b + (1 −α′)b′),
(α′′a + (1 −α′′)a′, α′′b + (1 −α′′)b′) ∈WJF
∗(A, B)
(3.5.7)
because of compactness of WJF
∗(A, B). Set
eA
=
A −
1
2(α′ + α′′)a + (1 −1
2(α′ + α′′))a′

I,
eB
=
B −
1
2(α′ + α′′)b + (1 −1
2(α′ + α′′))b′

I.
Then (0, 0) ̸∈WJF
∗( eA, eB) but, setting z := 1
2α′ −1
2α′′, we have as a consequence of
(3.5.7),
±(za −za′, zb −zb′) ∈WJF
∗( eA, eB),
a contradiction to the statement proved above.
□
The result of Theorem 3.5.7 fails for F = R and n = 2 (Ex. 3.11.14).
This theorem has been extended and generalized in many ways, and the litera-
ture on this subject is voluminous. We indicate here only a few facts.
Theorem 3.5.8.
(1) If n ̸= 2 and B1, B2, B3 ∈Cn×n are hermitian, then the
joint numerical range WJC
∗(B1, B2, B3) is convex.
(2) If n ̸= 2 and A1, . . . , A5 ∈Hn×n are hermitian, then WJH
∗(A1, . . . , A5) is
convex.
(3) If A1, . . . , A4 ∈Hn×n are hermitian, then WJH
∗(A1, . . . , A4) is convex.

42
CHAPTER 3
Part (1) is proved by Au-Yeung and Tsing [12, 11], parts (2) and (3) are proved
by Au-Yeung and Poon [10], and a far-reaching, more-general result is proved by
Poon [120].
Using Theorem 3.5.8, we give a criterion for convexity of joint numerical ranges
of ﬁve 2 × 2 hermitian quaternion matrices.
Corollary 3.5.9. Let A1, . . . , A5 ∈H2×2 be hermitian matrices. Then the joint
numerical range WJH
∗(A1, . . . , A5) is convex if and only if the 6-tuple of matrices
{A1, . . . , A5, I2} is linearly dependent over the reals.
Proof. First consider a particular situation. Let
B1 =
 1
0
0
−1

,
B2 =
 0
1
1
0

,
B3 =
 0
i
−i
0

,
B4 =
 0
j
−j
0

,
B5 =

0
k
−k
0

.
We claim that WJH
∗(B1, . . . , B5, I2) ⊂R6 is not convex. Since
WJH
∗(B1, . . . , B5, I2) = (WJH
∗(B1, . . . , B5), 1),
it suﬃces to show that WJH
∗(B1, . . . , B5) is not convex. Indeed,
e∗
jB1ej = ±1,
e∗
jB2ej = · · · = e∗
jB5ej = 0,
j = 1, 2,
which shows that
(1, 0, 0, 0, 0), (−1, 0, 0, 0, 0) ∈WJH
∗(B1, . . . , B5).
On the other hand, 0 ̸∈WJH
∗(B1, . . . , B5). To establish that, we will prove that
the system of equations
x∗Bkx = 0
for k = 1, 2, 3, 4, 5,
x ∈H2×1,
(3.5.8)
has only the trivial solution x = 0. Write x =
 y
z

, where y, z ∈H. Since the
assumption that y = 0 yields only the trivial solution of (3.5.8), we may assume
y ̸= 0, and then by replacing x with xy−1, we may further assume that y = 1. But
then (3.5.8) gives
z∗i = iz,
z∗j = jz,
z∗k = kz.
(3.5.9)
It is easy to see from (3.5.9) that z must be real. Now
x∗B1x = 1 −z2 = 0,
x∗B2x = 2z = 0
leads to a contradiction.
If the 6-tuple {A1, . . . , A5, I2} is linearly independent over the reals, then (be-
cause the real vector space of 2×2 quaternion hermitian matrices is 6-dimensional)
we have


A1
...
A5
I2

= C


B1
...
B5
I2



VECTOR SPACES AND MATRICES: BASIC THEORY
43
for some real invertible 6 × 6 matrix C. Thus,
WJH
∗(A1, . . . , A5, I2) = C WJH
∗(B1, . . . , B5, I2),
and, since WJH
∗(B1, . . . , B5, I2) is not convex, WJH
∗(A1, . . . , A5, I2) is not convex
either, and the nonconvexity of WJH
∗(A1, . . . , A5) follows.
Conversely, assume that A1, . . . , A5, I2 are linearly dependent over the reals. If
A1, . . . , A5 are linearly dependent, say
A5 = a1A1 + · · · + a4A4,
where a1, a2, a3, a4 ∈R;
then
WJH
∗(A1, . . . , A5) =

I4
 a1
a2
a3
a4


WJH
∗(A1, A2, A3, A4),
and so the convexity of WJH
∗(A1, . . . , A5) follows from that of WJH
∗(A1, A2, A3, A4),
by Theorem 3.5.8. If A1, . . . , A5 are linearly independent, then, for some index
j ∈{1, 2, 3, 4, 5}, the matrix Aj is a real linear combination of
A1, . . . , Aj−1, Aj+1, . . . , A5, I2,
and as before, the convexity of WJH
∗(A1, . . . , A5) follows from that of the joint
numerical range WJH
∗(A1, . . . , Aj−1, Aj+1, . . . , A5, I2).
□
In a diﬀerent direction, we have another convexity property.
Theorem 3.5.10. Let S be any 2-dimensional subspace in R4 = SpanR {1, i, j, k}
that contains the reals, and let PS be the orthogonal projection on S. Then, for any
A ∈Hn×n, the set PS
 W H
∗(A)

is convex.
Proof. Let (q1, q2, q3) be a units triple and such that 1, q1 form an orthonormal
basis in S and q2, q3 orthogonal to S (one can take for q1 any of the two unit
quaternions in S with zero real part). Write x ∈Hn×1 in the form
x = y + q2z,
y, z ∈Rn×1 + q1Rn×1
and A ∈Hn×n in the form
A = B + q2C,
B, C ∈Rn×n + q1Rn×n.
Then for x ∈Hn×1, such that x∗x = 1, we have |y|2 + |z|2 = 1 and write
x∗Ax = (y + q2z)∗(B + q2C)(y + q2z) = Q1 + Q2,
where
Q1 ∈SpanR {1, q1},
Q2 ∈SpanR {q2, q3}.
A calculation shows that
Q1 =
 y∗
z∗ 
B
−C
C
B∗
  y
z

,
where C is obtained from C by replacing each entry with its conjugate.
Thus,
PS(W∗(A)) coincides with the complex numerical range of the complex matrix

B
−C
C
B∗

(if we identify a + q1b, a, b ∈R, with the compex number a + ib) and,
therefore, is convex by Theorem 3.5.7.
□

44
CHAPTER 3
3.6
MATRIX DECOMPOSITIONS: NONSTANDARD
INVOLUTIONS
In this section, we ﬁx a nonstandard involution φ. By analogy with conjugation,
for A ∈Hm×n, we denote by Aφ the n×m matrix obtained by applying φ entrywise
to the transposed matrix AT . For example, if φ is such that φ(i) = −i, φ(j) = j,
φ(k) = k, then


1
2 + i
3 + j
4 + k
i + j
i + k


φ
=

1
3 + j
−i + j
2 −i
4 + k
−i + k

.
Note the following algebraic properties:
(a) (αA + βB)φ = Aφφ(α) + Bφφ(β),
α, β ∈H,
A, B ∈Hm×n.
(b) (Aα + Bβ)φ = φ(α)Aφ + φ(β)Bφ,
α, β ∈H,
A, B ∈Hm×n.
(c) (AB)φ = BφAφ,
A ∈Hm×n,
B ∈Hn×p.
(d) (Aφ)φ = A,
A ∈Hm×n.
(e) If A ∈Hn×n is invertible, then (Aφ)−1 = (A−1)φ.
The standard classes of matrices with respect to φ are introduced in a familiar way.
Deﬁnition 3.6.1. A ∈Hn×n is said to be φ-hermitian, φ-skewhermitian, φ-
unitary, or φ-normal
if A = Aφ, A = −Aφ, A is invertible and A−1 = Aφ, or
AAφ = AφA, respectively.
Note that the sets of φ-hermitian and φ-skewhermitian matrices form real vector
subspaces of Hn×n (for a ﬁxed φ), whereas the set of φ-unitaries is a multiplicative
group: if U, V are φ-unitaries, then so are UV and U −1.
Deﬁnition 3.6.2. By analogy with the standard inner product ⟨·, ·⟩in Hn×1, we
introduce the quaternion-valued φ-inner product ⟨u, v⟩φ := vφu, for u, v ∈Hn×1.
The φ-inner product obeys linearity and symmetry properties,
⟨u1α1 + u2α2, v⟩φ
=
⟨u1, v⟩φ α1 + ⟨u2, v⟩φ α2,
u1, u2, v ∈Hn×1,
α1, α2 ∈H,
⟨u, v1α1 + v2α2⟩φ
=
(α1)φ⟨u, v1⟩φ + (α2)φ⟨u, v2⟩φ,
u, v1, v2 ∈Hn×1,
α1, α2 ∈H,
⟨u, v⟩φ
=
(⟨v, u⟩φ)φ,
u, v ∈Hn×1,
but not the positive deﬁniteness property: ⟨u, u⟩φ need not be nonnegative, or even
real. Note that
⟨u, Av⟩φ = ⟨Aφu, v⟩φ
for all u ∈Hn×1, v ∈Hm×1, A ∈Hm×n.
We say that u, v ∈Hn×1 are φ-orthogonal if ⟨u, v⟩φ = 0.
Deﬁnition 3.6.3. For a set Z ⊆Hn×1 deﬁne the φ-orthogonal companion
Z⊥φ := {x ∈Hn×1 : ⟨x, u⟩φ = 0 for all u ∈Z}.

VECTOR SPACES AND MATRICES: BASIC THEORY
45
We list some basic properties of φ-orthogonal companions.
Proposition 3.6.4. Let Z ⊆Hn×1. Then:
(a) Z⊥φ is a (quaternion) subspace in Hn×1;
(b) if Z is a subspace in Hn×1, then dim Z + dim Z⊥φ = n;
(c) ((Z)⊥φ)⊥φ ⊇SpanH {Z}, and if Z is a subspace, then ((Z)⊥φ)⊥φ = Z;
(d) if Z is a subspace, then Z⊥φ is a direct complement of Z in Hn×1 if and only
if Z does not contain a nonzero vector which is φ-orthogonal to Z.
Proof. We prove in detail only part (b). Let {f1, . . . , fd} be a basis for Z
(we leave aside the trivial case when Z = {0}), and choose fd+1, . . . , fn so that
{f1, f2, . . . , fn} is a basis for Hn×1. Then the matrix X := [f1 f2 . . . fn] is invert-
ible. Partition the inverse X−1 =

Y1
Y2

, where Y1 ∈Hd×n and Y2 ∈H(n−d)×n.
The equality X−1X = I yields Y2 · [f1 . . . fd] = 0. So the (n −d)-dimensional
subspace spanned by the linearly independent columns of (Y2)φ, call this subspace
N, is contained in Z⊥φ. If the subspace Z⊥φ were strictly larger than N, then we
would ﬁnd a nonzero vector in Z⊥φ which is a linear combination of the columns
of (Y1)φ. Thus, there would exist α1, . . . , αd ∈H, not all zeros, such that
[α1 α2 . . . αd] Y1 u = 0
for all u ∈Z,
a contradiction with the equality Y1 · [f1 f2 . . . fd] = I.
□
The analogues of QR factorization, polar decomposition, and singular value
decomposition (using φ-unitaries instead of unitaries) in the presence of a nonstan-
dard involution generally fail. Indeed, it may happen that xφx = 0 for a nonzero
x ∈Hn×1; hence, the Gram-Schmidt orthogonalization procedure is not always
available. However, a version of Cholesky factorization remains valid.
Theorem 3.6.5. (a) A nonzero matrix A = [ai,j]n
i,j=1 ∈Hn×n, ai,j ∈H, is
φ-hermitian if and only if it admits a factorization A = BφB for some B ∈Hk×n.
Here k is the rank of A.
(b) If the principal submatrices [ai,j]s
i,j=1, s = 1, 2, . . . , rank (A), are invertible,
then B can be taken upper triangular in A = BφB, and if the principal submatrices
[ai,j]n
i,j=n−s, s = 0, 1, . . . , rank (A) −1, are invertible, then B can be taken lower
triangular.
Proof. Part (a). We need to prove the nontrivial part “only if.” Use induction
on n. The case n = 1 is taken care of by Theorem 2.5.1(b).
Consider now the general case. We claim, replacing A if necessary by CφAC for
a suitable invertible matrix C, that it can be assumed a1,1 ̸= 0. Indeed, if aj,j ̸= 0
for some index j, just simultaneously interchange rows 1 and j and columns 1 and
j. And if all diagonal entries of A are zeros, but ai,j ̸= 0 for some indices i ̸= j,
then the following transformation in rows and columns i and j yields a φ-hermitian
matrix with a nonzero diagonal:

0
ai,j
(ai,j)φ
0

7→
 1
(ai,j)−1
φ
0
1
 
0
ai,j
(ai,j)φ
0
 
1
0
a−1
i,j
1

=

2
ai,j
(ai,j)φ
0

.

46
CHAPTER 3
Next, using Theorem 2.5.1(b), we can assume that actually a1,1 = 1. Now it is easy
to see that simultaneous row and column replacement operations yield a φ-hermitian
matrix of the form

1
01×n−1
0n−1×1
A′

, where A′ = A′
φ ∈H(n−1)×(n−1), and an
application of the induction hypothesis completes the proof of (a).
For Part (b), we use again induction on n. Say the matrices [ai,j]s
i,j=1, s =
1, 2, . . . , rank (A), are invertible. We distinguish two cases: (1) rank (A) = n, i.e.,
A is invertible (cf.
Ex.
3.11.7); (2) rank (A) < n.
Consider case (1).
By the
induction hypothesis, we have
[ai,j]n−1
i,j=1 = (B1)φB1
for some upper triangular, necessarily invertible (cf.
Ex.
3.11.8) matrix B1 ∈
H(n−1)×(n−1). We seek B in the form
B =
 B1
B2
0
B3

,
B2 ∈H(n−1)×1,
B3 ∈H;
then the equality A = BφB amounts to the following:


a1,n
a2,n
...
an−1,n

= (B1)φ B2,
(an,n)φ = (B2)φ B2 + (B3)φ B3.
(3.6.1)
The ﬁrst equation in (3.6.1) can be solved for B2 in view of the invertibility of
B1; then the second equation in (3.6.1) can be solved for B3 in view of Theorem
2.5.1(b).
Now let k := rank (A) < n.
Partition: A =

A1
A2
(A2)φ
A3

, where A1 =
(A1)φ ∈Hk×k, A3 = (A3)φ ∈H(n−k)×(n−k). Since rank (A) = k and the principal
submatrix [ai,j]k
i,j=1 is invertible, the last n−k columns of A are linearly dependent
on its ﬁrst k columns, in other words,we have
A

X
In−k

= 0
(3.6.2)
for some X ∈Hk×(n−k). Note that (by (3.6.2))
eA :=
 Ik
0
Xφ
In−k

A
 Ik
X
0
In−k

=
 A1
0
0
0

,
and apply the induction hypothesis to eA.
The case when the submatrices [ai,j]n
i,j=n−s, s = 0, 1, . . . , rank (A) −1 are in-
vertible, is treated analogously.
□
Denote by τφ : Hn×1 →H the map τφ(x) = xφx, x ∈Hn×1 (we suppress in
the notation the dependence of τφ on n). The map τφ has a local right inverse at
nonzero vectors.

VECTOR SPACES AND MATRICES: BASIC THEORY
47
Proposition 3.6.6. For every x ∈Hn×1 \{0}, there exist an open neighborhood
Ux of x in Hn×1, an open neighborhood Vx of τφ(x) in Inv (φ) such that τφ maps
Ux onto Vx, and a continuously diﬀerentiable map
ux : τφ(Ux) = Vx →Ux
such that λ = (ux(λ))φ ux(λ) for every λ ∈Vx, and ux(τφ(x)) = x.
Proof. Consider τφ as a map onto Inv (φ) (cf. Theorem 2.5.1(b)). Identify
Hn×1 = (SpanR {1, q1, q2, q3})n
and Inv (φ) = SpanR {1, q2, q3} with R4n and R3, respectively, where (q1, q2, q3) is a
suitable units triple. Then τφ becomes a map (for which we use the same notation)
τφ : R4n×1 →R3×1 given by
τφ



colj=1,2,...,n


aj
bj
cj
dj





=
n
X
j=1


a2
j + b2
j −c2
j −d2
j
2ajcj + 2bjdj
2ajdj −2bjcj

,
where aj, bj, cj, dj ∈R for j = 1, 2, . . . , n.
The Jacobian matrix of τφ is
Jac = 2 rowj=1,2,...,n




aj
bj
−cj
−dj
cj
dj
aj
bj
dj
−cj
−bj
aj



,
which has full rank (unless aj = bj = cj = dj = 0 for all j); indeed, if aj ̸= 0 for
some j, then the 3 × 3 submatrix formed by the ﬁrst, third, and fourth columns
of


aj
bj
−cj
−dj
cj
dj
aj
bj
dj
−cj
−bj
aj

is invertible, and if aj = 0 then the determinants of
the three other 3 × 3 submatrices of


aj
bj
−cj
−dj
cj
dj
aj
bj
dj
−cj
−bj
aj

are bj(b2
j + c2
j + d2
j),
cj(b2
j + c2
j + d2
j), and dj(b2
j + c2
j + d2
j). Now we use the implicit function theorem
of multivariable calculus (which is presented in many textbooks; see, for instance,
Kaplan [77], Edwards [35], or Fleming [44]) to complete the proof.
□
3.7
NUMERICAL RANGES WITH RESPECT TO
NONSTANDARD INVOLUTIONS
In this section we ﬁx a nonstandard involution φ. Let (q1, q2, q3) be a units triple
such that φ(q1) = −q1, φ(q2) = q2, φ(q3) = q3. A straightforward but tedious
calculation shows that for any A ∈Hn×n, we have the polarization identity
3
X
j=1
 3
X
k=0
qk
j (x + yqk
j )φA(x + yqk
j )
!
=
4 yφAx + 8 V (xφAy)
for all
x, y ∈Hn×1.
(3.7.1)
Thus, using (3.7.1), analogous to Proposition 3.5.1, the following result is obtained.

48
CHAPTER 3
Proposition 3.7.1. Let A ∈Hn×n. Then:
(1) xφAx = 0 for all x ∈Hn×1 if and only if A = 0;
(2) xφAx ∈SpanR {1, q2, q3} for all x ∈Hn×1 if and only if A = Aφ;
(3) xφAx ∈SpanR {q1} for all x ∈Hn×1 if and only if A = −Aφ.
Since xφx can take on any value in Inv (φ), it makes sense to introduce numerical
ranges of a matrix A ∈Hn×n with respect to φ in the following way, for a ﬁxed
α ∈Inv (φ):
W (α)
φ
(A) := {xφAx : xφx = α,
x ∈Hn×1}.
Writing α = γφγ for some γ ∈H (Theorem 2.5.1(b)) we see that
W (α)
φ
(A) = γφW (1)
φ (A)γ,
(3.7.2)
assuming α ̸= 0. Thus, we can focus on two particular numerical ranges for a given
A ∈Hn×n: W (1)
φ (A) and W (0)
φ (A). To avoid trivialities, in the latter case n ≥2
will be assumed. Note some elementary algebraic properties.
Proposition 3.7.2. (a) If A, U ∈Hn×n, where U is φ-unitary, and if a ∈R,
then
W (α)
φ
(UφAU) = W (α)
φ
(A),
W (α)
φ
(A + aI) = W (α)
φ
(A) + aα,
W (α)
φ
(aA) = a W (α)
φ
(A)
for every α ∈Inv (φ).
(b) If, for a ﬁxed α ∈Inv (φ), the numerical ranges of matrices A1, . . . , Ap ∈
Hn×n with respect to φ are contained in the same real subspace V ⊆H, then the
numerical ranges of all real linear combinations of A1, . . . , Ap with respect to φ are
also contained in V.
The proof is left to the reader as Ex. 3.11.10.
Proposition 3.7.1 yields the following information about the numerical ranges.
Proposition 3.7.3. Let A ∈Hn×n. Then:
(1) W (α)
φ
(A) = {0} for some (equivalently, for all) α ∈Inv (φ) \ {0} if and only
if A = 0;
(2) W (α)
φ
(A) ⊆Inv (φ) for some (equivalently, for all) α ∈Inv (φ) \ {0} if and
only if A = Aφ;
(3) W (α)
φ
(A) ⊆SpanR{q1} for some (equivalently, for all) α ∈Inv (φ)\{0} if and
only if A = −Aφ.
Proof. Note that (1) follows from (2) and (3).
We prove Part (2). The “if” statement is trivial. Conversely, assume W (α)
φ
(A) ⊆
Inv (φ) for some α ∈Inv (φ), α ̸= 0. Then (cf. (3.7.2)) W (α)
φ
(A) ⊆Inv (φ) for every
nonzero α ∈Inv (φ). In other words, xφAx ∈Inv (φ) for all x ∈Hn×1 such that
xφx ̸= 0. Since the set of all x ∈Hn×1 such that xφx ̸= 0 is dense in Hn×1 (indeed,
its complement
{x = col (xj)n
j=1 ∈Hn×1 : x1, . . . , xn ∈H and xφx = 0}

VECTOR SPACES AND MATRICES: BASIC THEORY
49
can be represented as common zeros of a set of polynomial equations in 4n real
variables, the coeﬃcients of the xj’s as real linear combinations of 1, q1, q2, q3), we
obtain xφAx ∈Inv (φ) for all x ∈Hn×1. Now use Proposition 3.7.1.
Part (3) is proved analogously.
□
The case α = 0 is excluded in Proposition 3.7.3. In fact, the result is generally
not valid for W (0)
φ (A), as the following example shows.
Example 3.7.4. Let A =
 q1
0
0
−q1

. We claim that W (0)
φ (A) = {0}. Indeed,
for x =
 b
c

∈H2×1, where b, c ∈H, the condition xφx = 0 amounts to bφb+cφc =
0, which, in turn, implies |b| = |c| (because φ is an isometry on H). On the other
hand, by Theorem 2.5.1(a) (with β(φ) = q1) we have bφq1b = q1|b|2; hence,
xφAx = bφq1b −cφq1c = q1(|b|2 −|c|2),
which is equal to zero as long as |b| = |c|.
□
A suitably revised analogue of Proposition 3.7.3, however, is valid for α = 0;
recall that φ(q1) = −q1, q2
1 = −1.
Theorem 3.7.5. Let A ∈Hn×n, n ≥2.
(1) W (0)
φ (A) = {0} if and only if either n = 2 and A has the form
A =

a0 + a1q1
a2 + a3q1
−a2 + a3q1
a0 −a1q1

(3.7.3)
for some a0, a1, a2, a3 ∈R, or n ≥3 and A = aI for some real a.
(2) W (0)
φ (A) ⊆Inv (φ) if and only if n ≥3 and A is φ-hermitian or n = 2 and A
has the form
A =

a1q1
a2 + a3q1
−a2 + a3q1
−a1q1

+ B,
(3.7.4)
for some a1, a2, a3 ∈R and some φ-hermitian B.
(3) W (0)
φ (A) ⊆SpanR{q1} if and only if A has the form A = aI +B, where a ∈R
and B is φ-skewhermitian.
The proof of Theorem 3.7.5 is rather long and is, therefore, relegated to the
next section.
Note that the numerical ranges and joint numerical ranges with respect to con-
jugation are connected (Ex. 3.11.13); everywhere in the book connectivity is un-
derstood in the sense of pathwise connectivity. It turns out that connectivity holds
also for numerical ranges with respect to a nonstandard involution.
Theorem 3.7.6. For all nonstandard involutions φ and all α ∈Inv (φ), the
numerical range W (α)
φ
(A) is connected for every A ∈Hn×n.
We need a lemma for the proof of Theorem 3.7.6.
Lemma 3.7.7. The set S(α)
n
is connected for every α ∈Inv (φ).

50
CHAPTER 3
Proof. The connectivity of S(0)
n
is obvious: the line segment {tx : 0 ≤t ≤1}
connects any x ∈S(0)
n
to zero within S(0)
n . The case α ̸= 0 is reduced to α = 1 by
virtue of (3.7.2). So, it suﬃces to prove that S(1)
n
is connected.
First, we verify that S(1)
1
is connected. Note that α = a+q1b+q2c+q3d, where
a, b, c, d ∈R, satisﬁes αφα = 1 if and only if
a2 + b2 −c2 −d2 = 1,
ac + bd = 0,
ad −bc = 0,
which boils down to
c = d = 0,
a2 + b2 = 1,
and this is a connected set.
More generally, we have: for every α ∈Inv (φ), the set
S(α) := {γ ∈H : γφαγ = α}
(3.7.5)
is connected. Indeed, for α = 0 this is trivial, and for α ̸= 0 write α = λφλ by
Theorem 2.5.1(b) for some λ ∈H \ {0} and note that γ ∈S(α) if and only if
λγλ−1 ∈S(1)
1 .
Consider now the case when n ≥2. First note that for every x ∈S(1)
n , the set
xH ∩S(1)
n
= {xα : α ∈S(1)
1 }
is connected. Suppose now that x, y ∈S(1)
n
are linearly independent. We construct
a continuous curve γ in S(1)
n
that connects x and y. Let the curves
γ0 : [0, 1] →Hn×1
and
λ : [0, 1] →H
be deﬁned by
γ0(t)
=
(1 −t)x + ty,
λ(t)
=
γ0(t)φγ0(t) = (1 −t)2 + t2 + t(1 −t)((xφy) + (xφy)φ) ∈Inv (φ).
Suppose R(xφy) ≥0.
Then R(λ(t)) > 0 for all t ∈[0, 1].
Thus,
p
λ(t) (see
(2.5.3) for the deﬁnition) is well deﬁned, is nonzero, and depends continuously on
t (Corollary 2.5.3). We have
(
p
λ(t))−1 ∈SpanR {1, λ(t)∗} ⊆Inv (φ).
The curve t
7→
γ(t) := γ0(t)(
p
λ(t))−1 is continuous and satisﬁes γ(0) = x,
γ(1) = y, γ(t)φγ(t) = 1 for all t ∈[0, 1]. Suppose R(xφy) < 0. Then choose a
continuous curve in xH ∩S(1)
n
that connects x with −x and apply the construction
of γ for −x and y instead of x and y.
Finally, if x, y ∈S(1)
n
are linearly dependent, then x, y ∈xH ∩S(1)
n , and, there-
fore, x and y are connected within xH ∩S(1)
n .
□
Since xφAx (for a ﬁxed A ∈Hn×n) is a continuous function of x ∈Hn×1, the
connectivity of W (α)
φ
(A) follows immediately from Lemma 3.7.7.
Since the sets S(α)
n , α = 0 or α = 1, are unbounded (if n > 1) we cannot expect
that the numerical ranges W (α)
φ
(A) be bounded, for a general A ∈Hn×n.

VECTOR SPACES AND MATRICES: BASIC THEORY
51
Open Problem 3.7.8. Characterize those matrices A ∈Hn×n for which the
numerical range W (α)
φ
(A) is bounded.
We have the answer for the case α = 0.
Theorem 3.7.9. The numerical range W (0)
φ (A), where A ∈Hn×n, n ≥2, is
bounded if and only if n ≥3 and A = aI for some real a, or n = 2 and A has the
form (3.7.3).
Proof. Since the vectors x ∈Hn×1 satisfying xφx = 0 can be arbitrarily scaled
without losing this property, it follows that W (0)
φ (A) is bounded if and only if
W (0)
φ (A) = {0}. In view of Theorem 3.7.5 we are done.
□
By analogy with the joint numerical ranges with respect to the conjugation, we
deﬁne the joint numerical ranges with respect to a nonstandard involution φ.
Deﬁnition 3.7.10. Fix α ∈Inv (φ), and for a p-tuple of φ-hermitian matrices
A1, . . . , Ap ∈Hn×n, let
WJ(α)
φ
(A1, . . . , Ap) := {(xφA1x, . . . , xφApx) ∈(Inv (φ))p : xφx = α,
x ∈Hn×1},
be the joint φ-numerical range of A1, . . . , Ap.
Open Problem 3.7.11. Study geometric properties of joint φ-numerical ranges
versus algebraic properties of the constituent matrices.
For φ-skewhermitian matrices we consider still another version of joint numerical
ranges and a result on their convexity.
We start with a result on convexity of numerical ranges.
Deﬁnition 3.7.12. Let there be given a p-tuple of φ-skewhermitian n×n quater-
nion matrices (A1, . . . , Ap); thus, Aj = −(Aj)φ, j = 1, 2, . . . , p. Deﬁne the joint
φ-numerical range
WJφ(A1, . . . , Ap)
:=
{(xφA1x, xφA2x, . . . , xφApx) : x ∈Hn×1,
∥x∥= 1} ⊆Hp.
Since φ(xφAx) = −xφAx, we clearly have that
WJφ(A1, . . . , Ap) ⊆{(y1β(φ), . . . , ypβ(φ)) : y1, y2, . . . , yp ∈R}.
(3.7.6)
Theorem 3.7.13. (1) If n ̸= 2, then the joint φ-numerical range
WJφ(A1, A2, A3, A4, A5)
is convex for every 5-tuple of φ-skewhermitian matrices A1, . . . , A5.
(2) If n = 2, then the joint φ-numerical range WJφ(A1, A2, A3, A4) is convex
for every 4-tuple of φ-skewhermitian matrices A1, . . . , A4.
(3) If n = 2, then the joint φ-numerical range WJφ(A1, A2, A3, A4, A5) is convex
for a 5-tuple of φ-skewhermitian matrices A1, . . . , A5 if and only if the 6-tuple of
φ-skewhermitian matrices (A1, . . . , A5, β(φ)I) is linearly dependent over the reals.

52
CHAPTER 3
Proof. It follows from (2.4.8) that
xφAx = β(φ)−1x∗(βφA)x
for all x ∈Hn×1,
A ∈Hn×n.
Therefore, A is φ-skewhermitian if and only if β(φ)A is hermitian (see Proposition
3.5.1(2)). Now the parts (1) and (3) follow by applying Theorem 3.5.7 and Corollary
3.5.8 to the matrices β(φ)Aj (cf. formula (3.7.6)). Part (2) follows from part (3)
by taking A5 = 0.
□
Another proof of Theorem 3.7.13 (in the case p = 2) can be obtained by reduc-
tion to the real joint numerical range, as follows. Write
x
=
x1 + x2i + x3j + x4k ∈Hn×1,
x1, x2, x3, x4 ∈Rn×1,
A
=
A1 + A2i + A3j + A4k ∈Hn×n,
A1, A2, A3, A4 ∈Rn×n.
Then we have xφAx = aβ(φ), where the real number a is represented as a bilinear
form of
ex := [x1
x2
x3
x4]T ∈R4n×1.
In other words, there exists a real symmetric 4n × 4n matrix eA such that
a = exT eAex,
ex ∈R4n×1.
Note also that ∥x∥= ∥ex∥. Now, clearly,
WJφ(A, B) = WJR
∗( eA, eB)β(φ),
where
WJR
∗(X, Y ) := {(xT Xx, xT Y x) : x ∈R4n×1,
∥x∥= 1}
is the real joint numerical range of the pair (X, Y ) of two real symmetric 4n × 4n
matrices X and Y . By Theorem 3.5.7 the result follows.
□
3.8
PROOF OF THEOREM 3.7.5
We start with preliminary results. As in the preceding section, φ is a nonstandard
involution, and (q1, q2, q3) is a units triple such that φ(q1) = −q1, φ(q2) = q2,
φ(q3) = q3.
Lemma 3.8.1. Let A = −Aφ ∈H2×2 be such that
W (0)
φ (A) = {0}.
(3.8.1)
Then A has the form
A =

a1q1
a2 + a3q1
−a2 + a3q1
−a1q1

(3.8.2)
for some a1, a2, a3 ∈R.
Conversely, if A has the form (3.8.2), then (3.8.1) holds.

VECTOR SPACES AND MATRICES: BASIC THEORY
53
Proof. Write
A =

a1q1
a2 + a3q1 + a4q2 + a5q3
−a2 + a3q1 −a4q2 −a5q3
a6q1

where a1, a2, a3, a4, a5, a6 ∈R. A computation shows that for b = b1 +b2q1 +b3q2 +
b4q3 ∈H, where b1, b2, b3, b4 ∈R, we have
[1 bφ]A
 1
b

= q1(a1 + a6(b2
1 + b2
2 + b2
3 + b2
4) + 2b1a3 + 2b2a2 −2b3a5 + 2b4a4).
(3.8.3)
On the other hand, the condition [1 bφ] ·
 1
b

= 0 boils down to the system of
equalities
b2
1 + b2
2 −b2
3 −b2
4 = −1,
b1b3 + b2b4 = 0,
b1b4 −b2b3 = 0.
(3.8.4)
It is easy to see (3.8.4) is equivalent to the system
b1 = b2 = 0,
b2
3 + b2
4 = 1.
(3.8.5)
Indeed, if at least one of b1 and b2 is nonzero, then the second and the third
equations in (3.8.4) yield only the trivial solution b3 = b4 = 0, which gives a
contradiction when substituted in the ﬁrst equation in (3.8.4). Now the hypothesis
(3.8.1) implies
b2
3 + b2
4 = 1
=⇒
a1 + a6 −2b3a5 + 2b4a4 = 0.
(3.8.6)
It is easy to see that (3.8.6) is possible only if a4 = a5 = 0 and a1 + a6 = 0.
For the converse statement, we have to prove, in view of Proposition 3.7.2(b),
only that
W (0)
φ (A1) = W (0)
φ (A2) = W (0)
φ (A3) = {0}
(3.8.7)
for
A1 =
 q1
0
0
−q1

,
A2 =

0
1
−1
0

,
A3 =
 0
q1
q1
0

.
For A1 see Example 3.7.4. For A2 note that for x :=
 c
b

∈H2×1 such that
xφx = 0 and c ̸= 0, the equalities (3.8.5) are valid, where b1, b2, b3, b4 are the real
coeﬃcients of the expansion of bc−1 as a linear combination of 1, q1, q2, q3, whereas
[cφ bφ]A

c
b

= cφ[1 (cφ)−1bφ]A

1
bc−1

c = cφ(2q1b2)c = 0,
as b2 = 0. If xφx = 0 and c = 0, then x = 0 and xφAx = 0 trivially holds. This
proves (3.8.7) for A2. An analogous proof works for A3 (cf. Ex. 3.11.16).
□
Lemma 3.8.2. Let A = −Aφ ∈Hn×n, n ≥3, be such that (3.8.1) holds true.
Then
A ∈Rn×n + q1Rn×n
(3.8.8)
and A has zero diagonal.

54
CHAPTER 3
Proof. We apply Lemma 3.8.1 to the 2 × 2 principal submatrices of A. Then
(3.8.8) follows from (3.8.2). Also, if ai,i and aj,j are the ith and the jth (i ̸= j)
diagonal entries of A, then ai,i + aj,j = 0. Since n ≥3, this is possible only if all
diagonal entries of A are zeros.
□
Lemma 3.8.3. If A = −Aφ ∈Hn×n, where n ≥3, and if (3.8.1) holds true,
then A = 0.
Proof. We identify R + q1R with the ﬁeld of complex numbers, with q1 play-
ing the role of the imaginary unit and φ acting as complex conjugation.
By
Lemma 3.8.2, A ∈Cn×n is skewhermitian under this identiﬁcation.
It is well
known that (complex) hermitian matrices—and, hence, also (complex) skewhermi-
tian matrices—can be diagonalized by (complex) unitary similarity; in other words,
there exists a φ-unitary U ∈R + q1R such that B := UφAU is diagonal. Obviously,
B satisﬁes the hypotheses of Lemma 3.8.2 (cf. Proposition 3.7.2), so B must have
zero diagonal; hence, B = 0 and A = 0.
□
Lemma 3.8.4. If A = Aφ ∈H2×2 is such that W (0)
φ (A) = {0}, then A = aI for
some real a.
Proof. Write
A =

a1 + a2q2 + a3q3
a4 + a5q1 + a6q2 + a7q3
a4 −a5q1 + a6q2 + a7q3
a8 + a9q2 + a10q3

,
a1, a2, . . . , a10 ∈R.
We compute xφAx, where x =

1
b3q2 + b4q3

, and b3, b4 are real numbers such
that b2
3 + b2
4 = 1 but otherwise arbitrary. By (3.8.5), we have xφx = 0. Now
xφAx
=
[1 b3q2 + b4q3]
·

a1 + a2q2 + a3q3
a4 + a5q1 + a6q2 + a7q3
a4 −a5q1 + a6q2 + a7q3
a8 + a9q2 + a10q3

·

1
b3q2 + b4q3

,
which, after simple straightforward algebra, turns out to be equal to
a1 −2a6b3 −2a7b4 −a8
+
q2(a2 + 2a4b3 −2a5b4 + a9(b2
4 −b2
3) −2a10b3b4)
+
q3(a3 −2a4b4 + 2a5b3 + a10(b2
3 −b2
4) −2a9b3b4).
Thus, the hypothesis that W (0)
φ (A) = {0} amounts to equalities
a1 −2a6b3 −2a7b4 −a8
=
0;
(3.8.9)
a2 + 2a4b3 −2a5b4 + a9(b2
4 −b2
3) −2a10b3b4
=
0;
(3.8.10)
a3 −2a4b4 + 2a5b3 + a10(b2
3 −b2
4) −2a9b3b4
=
0
(3.8.11)
for every pair b3, b4 ∈R such that b2
3 + b2
4 = 1. It is easy to see that (3.8.9) holds
for every such b3, b4 if and only if a6 = a7 = 0, a1 = a8. It will be convenient to

VECTOR SPACES AND MATRICES: BASIC THEORY
55
represent b3 = sin ψ, b4 = cos ψ, where ψ ∈R. Then (3.8.10) and (3.8.11) take the
form
a2 + 2a4 sin ψ −2a5 cos ψ + a9 cos(2ψ) −a10 sin(2ψ) = 0
(3.8.12)
and
a3 −2a4 cos ψ + 2a5 sin ψ −a10 cos(2ψ) −a9 sin(2ψ) = 0,
(3.8.13)
respectively. Denoting the left-hand side of (3.8.12) by f(ψ), a function of ψ, we
see that
lim sup
k →∞
|f (k)(0)| = ∞
(unless both a9 and a10 are equal to zero); on the other hand, f(ψ) is identically
zero by virtue of (3.8.12); therefore, so are all the derivatives of f(ψ). So, we must
have a9 = a10 = 0. Now we obtain, from (3.8.12) and (3.8.13), that a2, a3, a4, a5
are also all zeros.
□
Lemma 3.8.5. If A = Aφ ∈Hn×n, n ≥2, is such that W (0)
φ (A) = {0}, then
A = aI for some real a.
Proof. Apply Lemma 3.8.4 to 2 × 2 principal submatrices of A.
□
Proof of Theorem 3.7.5. As in Proposition 3.7.3, (1) is a consequence of (2)
and (3).
Proof of Part (2), the “if” part: The case n ≥3 is obvious, and the case n = 2
follows from Lemma 3.8.1 (cf. Proposition 3.7.2(b)).
Proof of Part (2), the “only if” part. Assume
W (0)
φ (A) ⊆Inv (φ)
(3.8.14)
Write
A = 1
2(A + Aφ) + 1
2(A −Aφ),
(3.8.15)
and note that
W (0)
φ
1
2(A −Aφ)

⊆W (0)
φ (A) −W (0)
φ
1
2(A + Aφ)

.
(3.8.16)
Since 1
2(A+Aφ) is φ-hermitian, the numerical range W (0)
φ ( 1
2(A+Aφ)) is contained in
Inv (φ). In view of (3.8.14), the right-hand side of (3.8.16) is contained in Inv (φ).
But
1
2(A −Aφ) is φ-skewhermitian; therefore, W (0)
φ ( 1
2(A −Aφ)) is contained in
SpanR {q1}. As
Inv (φ) ∩SpanR {q1} = {0},
we obtain that W (0)
φ ( 1
2(A −Aφ)) = {0}. If n = 2, we are done by Lemma 3.8.1
(with 1
2(A−Aφ) playing the role of A), and if n ≥3, then 1
2(A−Aφ) = 0 by Lemma
3.8.3, and we are done again.
Proof of Part (3). The “if” part being obvious (cf. Proposition 3.7.2(b)), we
focus on the “only if” part. Assume W (0)
φ (A) ⊆SpanR{q1}. Write A in the form
(3.8.15); then
W (0)
φ
1
2(A + Aφ)

⊆W (0)
φ (A) −W (0)
φ
1
2(A −Aφ)

.
(3.8.17)

56
CHAPTER 3
It follows that
W (0)
φ
1
2(A + Aφ)

⊆SpanR{q1}.
Since 1
2(A + Aφ) is skewhermitian, we obtain W (0)
φ ( 1
2(A + Aφ)) = {0}. Now 1
2(A +
Aφ) = aI for some real a by Lemma 3.8.5, and so A has the form as claimed in
(3).
□
3.9
THE METRIC SPACE OF SUBSPACES
Let M, N be (quaternion) subspaces in Hn×1. A standard measure of the closeness
of these two subspaces is the gap.
Deﬁnition 3.9.1. The gap between M and N is deﬁned by
θ (M, N) := ∥PM −PN ∥,
where we denote by PX ∈Hn×n the orthogonal projection on the subspace X ⊆
Hn×1.
Note that PX is characterized by the properties that Ran PX = X, PX x = x
for every x ∈X, and Ker PX = X ⊥, the orthogonal complement to X.
Also,
P 2
X = PX = P ∗
X and ∥PX ∥= 1 (unless X = {0}, in which case PX = 0).
If
{v1, . . . , vp} is an orthonormal basis for X, then
PX = [u1 u2 . . . up] [u1 u2 . . . up]∗.
(3.9.1)
Denote by Grassn the set of all (quaternion) subspaces in Hn×1 (the Grass-
mannian). The gap function is easily seen to be a metric on Grassn; i.e., for all
M, N, Q ∈Grassn we have:
(1) θ (M, N) = θ (N, M);
(2) θ (M, N) ≥0, and θ (M, N) = 0 if and only if M = N;
(3) triangle inequality: θ (M, N) ≤θ (M, Q) + θ (Q, N).
The convergence in Grassn is naturally deﬁned in terms of the gap: a sequence
{Xm}∞
m=1, Xm ∈Grassn, is said to converge to Y ∈Grassn if
lim
m →∞θ (Xm, Y) = 0.
This notion of convergence can be expressed in terms of vectors that belong to
the subspaces in question (Theorem 3.9.4 below) and turns Grassn into a compact
metric space; compactness is understood in the sense that every sequence contains
a subsequence that converges to an element of Grassn (cf. Theorem 3.10.2 and
Remark 3.10.3). A basic theory of metric spaces is found in many textbooks on
real analysis; see, e.g., Wade [154] or Bruckner et al. [21].
Theorem 3.9.2. Every sequence {Xm}∞
m=1, where Xm ∈Grassn, contains a
subsequence that converges to an element of Grassn.

VECTOR SPACES AND MATRICES: BASIC THEORY
57
Proof. It will suﬃce to prove that, for every ﬁxed integer k, 1 ≤k ≤n −1, the
set Grassn,k of all k-dimensional subspaces of Hn×1 is compact, with respect to the
gap metric.
To this end consider the set On,k of all (ordered) orthonormal k-tuples in Hn×1:
On,k
:=
{(u1, . . . , uk) : ⟨uj, ui⟩= 1 if i = j;
⟨uj, ui⟩= 0 if i ̸= j;
u1, . . . , uk ∈Hn×1}.
Let
Sn,k := {(x1, . . . , xk) : ∥x1∥2
H + · · · + ∥xk∥2
H = 1,
x1, . . . , xk ∈Hn×1}.
Then Sn,k can be identiﬁed (as a normed space) with the unit sphere in R4nk×1;
therefore, Sn,k is compact. It is easy to see that On,k is a closed subset of Sn,k,
and consequently On,k is compact as well (Theorem 3.10.2).
Deﬁne a map Ωn,k : On,k →Grassn,k by
Ωn,k((u1, . . . , uk)) = SpanH {u1, . . . , uk},
(u1, . . . , uk) ∈On,k.
It turns out that Ωn,k is continuous; moreover, it has the Lipschitz property (see
(3.9.6) below) with respect to the gap metric in Grassn,k and with respect to the
standard metric ℧induced by the norm in R4nk×1:
℧((u1, . . . , uk), (v1, . . . , vk))
:=
v
u
u
t
k
X
j=1
∥uj −vj∥2,
(u1, . . . , uk), (v1, . . . , vk) ∈On,k.
For the reader’s convenience, we present a proof of this fact (adapted from Gohberg
et al. [54, Section 13.4, 55, Section S.4]).
We need some preparation for the proof.
Let L ∈Grassn,k, and let v :=
(v1, . . . , vk) be an orthonormal basis for L. Pick some u = (u1, . . . , uk) ∈On,k,
and let M := Ωn,k(u). Then
∥(PM −PL)vi∥
=
∥PMvi −vi∥≤∥PM(vi −ui)∥+ ∥ui −vi∥
≤
∥PM∥∥vi −ui∥+ ∥ui −vi∥≤2℧(u, v)
for i = 1, 2, . . . , k. Thus, for x = Pk
i=1 viαi ∈L, αi ∈H, we have
∥(PM −PL)x∥≤2
k
X
j=1
|αj|℧(u, v).
(3.9.2)
Assuming ∥x∥= 1, it follows that Pk
j=1 |αj|2 = ∥x∥2 = 1; so, |αj| ≤1 for j =
1, 2, . . . , k, and (3.9.2) gives
∥(PM −PL)|L∥≤2k℧(u, v).
(3.9.3)
Now ﬁx some y ∈L⊥, ∥y∥= 1. We wish to evaluate PMy. For every x ∈L,
write
⟨x, PMy⟩= ⟨(PM −PL)x, y⟩+ ⟨x, y⟩= ⟨(PM −PL)x, y⟩;

58
CHAPTER 3
hence,
|⟨x, PMy⟩| ≤2k∥x∥℧(u, v)
(3.9.4)
by (3.9.3). On the other hand, if we write
PMy = u1α1 + · · · + ukαk,
α1, . . . , αk ∈H,
then for every z ∈L⊥:
⟨z, PMy⟩= ⟨z,
k
X
i=1
(ui −vi)αi⟩+ ⟨z,
k
X
i=1
viαi⟩= ⟨z,
k
X
i=1
(ui −vi)αi⟩
and
|⟨z, PMy⟩| ≤∥z∥∥
k
X
i=1
(ui −vi)αi∥≤∥z∥k

max
1≤i≤k (|αi| ∥ui −vi∥)

.
But ∥y∥= 1 implies Pk
i=1 |αi|2 ≤1, so max1≤i≤k{|α1|, . . . , |αk|} ≤1. Hence,
|⟨z, PMy⟩| ≤∥z∥k ℧(u, v),
and combining this inequality with (3.9.4), it is found that |⟨t, PMy⟩| ≤3 k ℧(u, v)
for every t ∈Hn×1 with ∥t∥= 1. Thus,
∥PMy∥≤3 k ℧(u, v).
(3.9.5)
Now continuity of the map Ωn,k can be easily proven. Indeed, pick an x ∈Hn×1
with ∥x∥= 1. Then, using (3.9.3) and (3.9.5), we have
∥(PM −PL)x∥≤∥(PM −PL)PLx∥+ ∥PM(x −PLx)∥≤5k℧(u, v),
so
θ(M, L) = ∥PM −PL∥≤5k℧(u, v),
(3.9.6)
and the continuity of Ωn,k follows.
Since the continuous map Ωn,k is onto and On,k is compact, we obtain that
Grassn,k is compact as well (Theorem 3.10.5).
□
It follows from Theorem 3.9.2 that Grassn, as well as each set Grassn,k, for
k = 1, 2, . . . , n −1, are complete, i.e., every Cauchy sequence of elements of the set
converges within the set.
The following property connecting the gap between subspaces and their dimen-
sions is very useful.
Theorem 3.9.3. If X, Y ∈Grassn and θ (X, Y) < 1, then dim X = dim Y.
Proof. We have
(I −(PY −PX ))PY = PX (I −(PX −PY)).
(3.9.7)
Since ∥PY −PX ∥< 1, the matrices (I −(PY −PX )) and (I −(PX −PY)) are
invertible, with the inverses given by the converging series P∞
j=0(PY −PX )j and
P∞
j=0(PX −PY)j, respectively, and hence, (3.9.7) gives rank PY = rank PX .
It
remains to observe that rank PY = dim Y, and similarly for X.
□

VECTOR SPACES AND MATRICES: BASIC THEORY
59
Theorem 3.9.4. Assume limm →∞Xm = Y, where Xm, Y ∈Grassn. Then Y
consists of exactly those vectors y ∈Hn×1 for which there exists a sequence {xm}∞
m=1
such that xm ∈Xm for m = 1, 2, . . . and limm →∞xm = y.
Proof. We ignore the trivial case Y = {0}.
Suppose y ∈Y. Let xm := PXm y. Then, clearly, xm ∈Xm, and
∥xm −y∥= ∥PXm y −PY y∥≤∥PXm −PY∥· ∥y∥→0
as m →∞.
Conversely, suppose y ∈Hn×1 is such that xm ∈Xm for m = 1, 2, . . . and
limm →∞xm = y for some sequence {xm}∞
m=1. Then
∥PYy −y∥
≤
∥PYy −PXmy∥+ ∥PXmy −PXmxm∥+ ∥xm −y∥
≤
θ (Y, Xm) ∥y∥+ ∥PXm∥∥y −xm∥+ ∥xm −y∥
≤
θ (Y, Xm) ∥y∥+ 2∥xm −y∥,
(3.9.8)
where the equality ∥PXm∥= 1 was used. The right-hand side of (3.9.8) tends to
zero as m →∞. Thus, PYy = y, and y ∈Y, as required.
□
3.10
APPENDIX: MULTIVARIABLE REAL ANALYSIS
For the reader’s convenience we collect here a few basic facts on analysis of sets
and continuous multivariable functions that are used in the main text. All this
material, and much more, can be found in many textbooks on real analysis; see,
e.g., Wade [154].
Deﬁnition 3.10.1. A set E ⊆Rn is called open if for every x ∈E there exists
δ > 0 such that the ball {y ∈Rn : ∥y −x∥R < δ} is contained in E.
The
complements (in Rm) of open sets are closed sets. An open cover of a set E ⊆Rn
is, by deﬁnition, a collection {Vi}i∈K of open sets Vi ⊆Rn indexed by the index
set K such that E ⊆∪i∈KVi. A set E ⊆Rn is said to be compact if every open
cover of E contains a ﬁnite subcover: if E ⊆∪i∈KVi, Vi are open, then there exists
a ﬁnite subset K0 ⊆K such that E ⊆∪i∈K0Vi.
Theorem 3.10.2. The following statements are equivalent for a set E ⊆Rn:
(a) E is compact;
(b) E is closed and bounded;
(c) E is sequentially compact, i.e., every sequence {xm}∞
m=1, where xm ∈E for
m = 1, 2, . . ., contains a subsequence that converges to an element of E.
This result is known as the Heine-Borel theorem.
Remark 3.10.3. Although the Heine-Borel theorem generally fails for subsets
of metric spaces (in general, only the implications (a) ⇒(c) ⇒(b) are valid), it
does hold true for subsets of the metric space Grassn with the gap metric. Indeed,
Grassn can be identiﬁed with the closed set of all orthogonal projections in Hn×n,
with the norm ∥· ∥. Furthermore, Hn×n with the norm ∥· ∥can be identiﬁed (by
considering the four real components of every entry in an n × n quaternion matrix)
with R4n2×1 but with a norm ∥· ∥d which is diﬀerent from ∥· ∥R. However, any two

60
CHAPTER 3
norms in R4n2×1 are equivalent, a well-known fact proved in many linear algebra
textbooks (see, e.g., Lancaster and Tismenetsky [94, Section 10.1] or Horn and
Johnson [62, Section 5.4.4]); thus, there exist positive constants m and M such
that
m∥x∥d ≤∥x∥R ≤M∥x∥d,
for all x ∈R4n2×1.
It is now not diﬃcult to see that Theorem 3.10.2 holds for R4n2×1 with the norm
∥· ∥d and, hence, also for Grassn.
Deﬁnition 3.10.4. A function f : E →Rm, where E is a subset of Rn is said
to be continuous if for every given x ∈E and ε > 0, there exists δ > 0 such that
∥f(y) −f(x)∥R < ε as soon as y ∈E and ∥y −x∥R < δ.
Equivalently, a function f, : E →Rm is continuous if for every open set V ⊆Rm,
the preimage f −1(V ) := {x ∈E : f(x) ∈V } is relatively open in E; in other words,
{x ∈E : f(x) ∈V } is the intersection of some open set in Rn with E. Also, f
is continuous if and only if the preimage f −1(U) of every closed set U ⊆Rm is
relatively closed in E.
Theorem 3.10.5. Let E ⊆Rn be a compact set and f : E →Rm a continuous
function. Then:
(1) the range of f,
Ran (f) := {f(x) : x ∈E},
is a compact set of Rm;
(2) the inﬁmum and supremum of the norm of f are attained; in other words,
there exist x0, y0 ∈E such that
∥f(x0)∥R = inf
x∈E ∥f(x)∥R,
∥f(y0)∥R = sup
x∈E
∥f(x)∥R.
We note in passing that in general the range of a continuous function deﬁned
on an open (resp. closed) set need not be open (resp. closed).
Finally. we consider connected sets.
Deﬁnition 3.10.6. A set E ⊆Rn is said to be (pathwise) connected if for any
two points x, y ∈E, there is a continuous function f : [0, 1]
→
E such that
f(0) = x and f(1) = y.
(This deﬁnition is slightly diﬀerent from the deﬁnition of connectedness given
in Wade [154].)
Theorem 3.10.7. If E is connected and f : E →Rm is a continuous function,
then the range of f is connected as well.
Proof. Let a = f(x), b = f(y), where x, y ∈E, be two arbitrary elements of
the range of f. Since E is connected, there is a continuous function g : [0, 1] →E
such that g(0) = x, g(1) = y. Then the composition f ◦g is a continuous function
from [0, 1] into the range of f such that (f ◦g)(0) = a, (f ◦g)(1) = b.
□

VECTOR SPACES AND MATRICES: BASIC THEORY
61
3.11
EXERCISES
Ex. 3.11.1. Verify that ∥· ∥H is indeed a norm on Hn×1:
∥u + v∥H ≤∥u∥H + ∥v∥H;
∥uα∥H = ∥u∥H |α|;
∥u∥H ≥0
for all u, v ∈Hn×1 and all α ∈H, and the equality ∥u∥H = 0 holds only for u = 0.
Ex.
3.11.2. Let A ∈Hn×n. Show that if there exists B ∈Hn×n such that
BA = I, then A is invertible and B = A−1.
Ex. 3.11.3. Repeat Ex. 3.11.2, with BA = I replaced by AB = I.
Ex. 3.11.4. The row rank of a matrix A ∈Hm×n is deﬁned as the dimension of
the (left quaternion) subspace spanned by the rows of A. Show that the row rank
of A coincides with rank (A). Hint: Use the rank decomposition for A.
Ex. 3.11.5. Prove Proposition 3.3.1.
Ex. 3.11.6. Prove Proposition 3.4.1.
Ex. 3.11.7. Show that the following three statements are equivalent for A ∈
Hn×n:
(1) A is invertible;
(2) Ker (A) = {0};
(3) Ran (A) = Hn×1.
Ex. 3.11.8. Prove that if A1, A2 ∈Hn×n and the product A1A2 is invertible,
then A1 and A2 are separately invertible.
Ex. 3.11.9. Let A =
 0
1
1
0

, and let φ be a nonstandard involution.
(a) Prove that A does not admit a decomposition A = BφB with either upper
triangular or lower triangular B ∈H2×2.
(b) Find B ∈H2×2 such that A = BφB.
Ex. 3.11.10. Prove Proposition 3.7.2.
Ex. 3.11.11. Prove parts (2) and (3) of Proposition 3.2.4.
Ex. 3.11.12. Verify formula (3.9.1).
Ex.
3.11.13. Show that the numerical ranges W H
∗(A), A ∈Hn×n and joint
numerical ranges WJH
∗(A1, . . . , Ap), where A1, . . . , Ap ∈Hn×n are hermitian, are
connected sets. Hint: These are ranges of the connected set (unit sphere) {x ∈
Hn×1 : x∗x = 1} under continuous functions.
Ex. 3.11.14. Find WJR
∗(A, B) for
A =
 1
0
0
−1

,
B =
 0
1
1
0

,
and verify that WJR
∗(A, B) is not convex.

62
CHAPTER 3
Ex. 3.11.15. Provide details for the proof of Proposition 3.3.2 in case φ is an
antiautomorphism.
Ex. 3.11.16. Prove that if φ is a nonstandard involution, and q1 ∈H is such
that φ(q1) = −q1, then
W (0)
φ
 0
q1
q1
0

= {0}.
Ex. 3.11.17. Show that a block upper triangular matrix B = [Bi,j]k
i,j=1, where
Bi,j ∈Hni×nj for i, j = 1, 2, . . . , k and Bi,j = 0 if i > j, is invertible if and only
if the diagonal blocks B1,1, . . . , Bk,k are invertible. Prove the analogous result for
block lower triangular matrices.
Ex. 3.11.18. Let A ∈Hn×n be a matrix which is block triangular with respect
to the “other” diagonal, and the “other” diagonal blocks are of square sizes
A = [Ai,j]k
i,j=1,
where Ai,j ∈Hnk+1−i×nj for i, j = 1, 2, . . . , k, and Ai,j = 0 if i + j < k + 1. Show
that A is invertible if and only if all “other” diagonal blocks Ak,1, Ak−1,2, . . . , A1,k
are invertible.
Ex. 3.11.19. Let F ∈{R, C, H} and let (A1, . . . , Ap) be a p-tuple of hermitian
matrices in Fn×n. Deﬁne the joint numerical cone of (A1, . . . , Ap) by
CJF
∗(A1, . . . ., Ap) := {(x∗A1x, . . . , x∗Apx) : x ∈Fn×1 ⊆Rp.
Show that if WJF
∗(A1, . . . , Ap) is convex, then so is CJF
∗(A1, . . . ., Ap).
Ex. 3.11.20. Give example of a situation when CJF
∗(A1, . . . ., Ap) is convex but
WJF
∗(A1, . . . ., Ap) is not.
Ex. 3.11.21. Given a real, resp. complex, skewsymmetric matrix X, show that
BT XB = diag

0
1
−1
0

, . . . ,

0
1
−1
0

, 0, . . . , 0

for some invertible real, resp. complex, matrix B.
Hint: Use induction on n. If the ﬁrst row of X has a nonzero entry, by simulta-
neous column and row interchange and column and row scaling, bring the matrix
X to the form where the top left 2 × 2 corner in the matrix is

0
1
−1
0

. Then,
by simultaneous row and column replacements, reduce the proof to the case when
X =

0
1
−1
0

⊕X0,
where X0 is again skewsymmetric. Now use the induction.
Conclude that a real or complex skewsymmetric matrix has even rank.
Ex. 3.11.22. Show that the skewsymmetric hermitian quaternion matrix A =


0
i
j
−i
0
k
−j
−k
0

has rank 3—in other words, is invertible. Find A−1. Hint: A2 =
2I −A.

VECTOR SPACES AND MATRICES: BASIC THEORY
63
Ex. 3.11.23. Show that if A ∈R2×2, then W H
∗(A) is convex.
Hint: We may assume A ̸= 0. By using transformations of types
A 7→U T AU,
A 7→αA,
A 7→A + λI,
where A is a 2 × 2 real matrix, U is a 2 × 2 real orthogonal matrix, and λ, α are
nonzero real numbers, we reduce the problem to one of the following two situations:
(1) A =

0
1
−1
0

; (2) A =

1
b
−b
0

, where b is real. In case (1) holds true,
show that
W H
∗(A) = {z ∈H : R(z) = 0,
|V(z)| ≤1},
which is convex. In case (2) holds true, show that
W H
∗(A) = {1 −α + bz : 0 ≤α ≤1,
R(z) = 0,
|V(z)| ≤
p
α −α2},
which is convex as well (to verify convexity use the fact that the function
√
α −α2
is convex on the interval α ∈[0, 1]).
3.12
NOTES
Real and complex matrix representations of quaternions are well known and found
in many sources. Propositions 3.3.3 and 3.4.2 are taken from Rodman [137]. The-
orem 3.1.1 and its proof are from Wan [156]. Theorem 3.6.5(b) is in the spirit of
a well-known result on LU decompositions of real and complex matrices; see, e.g.,
Golub and Van Loan [56]. Details on many matrix decompositions of Proposition
3.2.5 are found in Farenick and Pidkowich [38]; see also Loring [102]. Applications
to signal processing are found in Le Bihan and Mars [97].
Example 3.5.5 (with A0 = I) is due to Au-Yeung [9].
The case F = R of Theorem 3.5.7 was proved in Brickman [19]. The proof of
Theorem 3.5.7 is adapted from the papers by Au-Yeung [7, 8]. More information
about convexity of quaternion numerical ranges (in certain situations) is found in
Au-Yeung [9].
The results of Sections 3.7 and 3.8 seem to be new.
Many properties of the gap function that are well known in the context of real
and complex matrices and vector spaces also remain valid for quaternion matrices
and vector spaces, with essentially the same proofs; see, e.g., Gohberg et al. [54,
Chapter 13] and Rodman [137] for more details. The proof of Theorem 3.9.4 follows
the exposition in Section 13.4 of Gohberg et al. [54].

Chapter Four
Symmetric matrices and congruence
In this chapter we develop canonical forms of hermitian and skewhermitian matrices
under congruence, in the contexts of the conjugation and of nonstandard involu-
tions. These canonical forms allow us to identify maximal neutral and maximal
semideﬁnite subspaces, with respect to a given hermitian or skewhermitian matrix,
in terms of their dimensions.
4.1
CANONICAL FORMS UNDER CONGRUENCE
Deﬁnition 4.1.1. Two matrices A, B ∈Hn×n are said to be congruent if A =
S∗BS for some invertible S ∈Hn×n. If φ is a nonstandard involution, then we say
that A, B ∈Hn×n are φ-congruent if A = SφBS for some invertible S.
Clearly, congruence and φ-congruence are equivalence relations. Also, if A and
B are congruent and A is hermitian or skewhermitian, then so is B. Analogously,
if A and B are φ-congruent and A is φ-hermitian or φ-skewhermitian, then so is B.
To formulate the result on canonical forms of φ-skewhermitian matrices under
φ-congruence, we make use of a unit quaternion β such that βφ = −β.
Note
that the quaternion β with these properties is unique up to negation for a given
nonstandard involution φ. In what follows, we ﬁx one such β and denote it β(φ).
Note that β−1 = −β.
Theorem 4.1.2. Let φ be a nonstandard involution.
(a) For every φ-hermitian H ∈Hn×n, there exists an invertible S such that
SφHS =

Ir
0
0
0

(4.1.1)
for a nonnegative integer r ≤n. Moreover, r is uniquely determined by H.
(b) For every φ-skewhermitian matrix H ∈Hn×n, there exists an invertible
S ∈Hn×n such that
SφHS =


βIp
0
0
0
−βIq
0
0
0
0n−p−q

,
β = β(φ).
(4.1.2)
Moreover, the integers p and q are uniquely determined by H (for a ﬁxed β(φ)).
Proof. Part (a). To prove existence of S, we use induction on n and argue
similarly to the proof of Theorem 3.6.5, part (a). The uniqueness of r is evident;
in fact, r = rank H (cf. Proposition 3.2.4).
Part (b). We show the existence of S ﬁrst. Again, we use induction on n. The
case n = 1 is trivial. Now consider the general case. We may assume H ̸= 0. If
H = [hi,j]n
i,j=1 has a nonzero diagonal entry, say h1,1 ̸= 0, then by scaling we may

SYMMETRIC MATRICES AND CONGRUENCE
65
assume h1,1 = ±β; then by simultaneous row and column replacement operations,
we reduce A to a block diagonal form:


1
0
. . .
0
(h1,2)φ(∓β)
1
. . .
0
...
...
...
...
(h1,n)φ(∓β)
0
. . .
1




±β
h1,2
h1,3
. . .
h1,n
−(h1,2)φ
h2,2
h2,3
. . .
h2,n
...
...
...
. . .
...
−(h1,n)φ
hn,2
hn,3
. . .
hn,n


·


1
±βh1,2
±βh1,3
. . .
±βh1,n
0
1
0
. . .
0
...
...
...
. . .
...
0
0
0
. . .
1

=

±β
01×(n−1)
0(n−1)×1
∗

.
(∗stands for entries of no immediate interest.) It remains to use the induction hy-
pothesis. If H has zero diagonal, then a simultaneous row and column replacement
yields a φ-skewhermitian matrix with nonzero diagonal, thereby reducing the proof
to the case already considered. Say h1,2 ̸= 0; then, with b = β((h1,2)φ)−1, we have


1
b
0
0
1
0
0
0
In−2

H


1
0
0
bφ
1
0
0
0
In−2

=
 −2β
∗
∗
∗

.
It remains to prove the uniqueness of p and q. First note that p + q = rank H
is uniquely determined by H. Assume that (4.1.2) holds, and let
V
:=
SpanH {Se1, . . . , Sep, Sep+q+1, . . . , Sen},
W
:=
SpanH {Sep+1, . . . , Sep+q},
where e1, . . . , en are the standard unit coordinate vectors in Hn×1. From (2.5.1) it
follows that vφHvβ−1 ≥0 for all v ∈V and wφHwβ−1 < 0 for all nonzero vectors
w ∈W. It is now clear that q is the maximal dimension of a subspace M ⊆Hn×1
with the property that xφHxβ−1 < 0 for all nonzero x ∈M is equal to q. Indeed,
if there were such a subspace M of dimension larger than q, then it would have
a nonzero intersection with V, leading to a contradiction. Thus, q and, therefore,
also p, are uniquely determined by H.
□
Deﬁnition 4.1.3. We say that the ordered triple of nonnegative integers
(In+ (H), In−(H), In0 (H)) := (p, q, n −p −q)
is the β(φ)-inertia, or the β(φ)-signature, of a φ-skewhermitian matrix H ∈Hn×n,
as in Theorem 4.1.2(b). The matrix H is said to be β(φ)-positive deﬁnite, resp.
β(φ)-positive semideﬁnite,
if In+ (H) = n, resp. In+ (H) + In0 (H) = n. Analo-
gously, β(φ)-negative deﬁnite and β(φ)-negative semideﬁnite φ-skewhermitian ma-
trices are deﬁned.
We record the following corollary of Theorem 4.1.2 to be used later.
Corollary 4.1.4. Let φ be a nonstandard involution. Fix β(φ) ∈H such that
φ(β(φ)) = −β(φ), |β(φ)| = 1. Then for every nonzero α0 ∈Inv (φ), there exists
ω ∈H, |ω| = 1, with the properties that φ(ω)α0ω is real positive, and, hence,
φ(ω)α0ω = |α0|
and
φ(ω)β(φ)ω = β(φ).

66
CHAPTER 4
Proof. By Theorem 4.1.2(a) there exists ω ∈H, |ω| = 1, such that φ(ω)α0ω
is real positive.
The quaternion φ(ω)β(φ)ω is φ-skewhermitian.
The condition
|ω| = 1 implies that φ(ω)β(φ)ω = ±β(φ). In fact, we must have sign +. Indeed,
the continuous map F deﬁned by
ω ∈H \ {0}
7→
β(φ)−1φ(ω)β(φ)ω ∈R \ {0}
is deﬁned on a connected set H \ {0}; therefore, the range (or image) of F must
be also connected, and so Ran F ⊆(0, ∞) or Ran F ⊆(−∞, 0). But the latter
possibility is excluded because F(1) = 1.
□
Deﬁnition 4.1.5. For a nonstandard involution φ, a φ-skewhermitian H ∈Hn×n
is said to have balanced inertia if In+ (H) = In−(H).
Note that the property of having balanced inertia is independent of the choice
of β(φ), as In± (H) with respect to −β(φ) coincides with In∓(H) with respect to
β(φ).
For the (quaternion) conjugation, a theorem similar to Theorem 4.1.2 holds,
but with the inertia present in the case of hermitian matrices, in complete analogy
with the standard results of complex linear algebra.
Theorem 4.1.6. (a) For every hermitian H ∈Hn×n, there exists an invertible
S such that
S∗HS =


Ip
0
0
0
−Iq
0
0
0
0n−p−q

.
(4.1.3)
Moreover, the integers p and q are uniquely determined by H.
(b) For every skewhermitian matrix H ∈Hn×n, there exists an invertible S ∈
Hn×n such that
S∗HS =
 iIr
0
0
0

.
Moreover, the integer r is uniquely determined by H.
The quaternion i in Theorem 4.1.6 can be replaced by any nonzero quaternion
with zero real part; indeed, this can be achieved by diagonal scaling
S∗HS 7→diag (a∗
j)n
j=1S∗HSdiag (aj)n
j=1,
a1, . . . , an ∈H \ {0};
cf. Theorem 2.2.6(2)⇔(5).
The proof of Theorem 4.1.6(a) follows the same pattern as that of Theorem
4.1.2(b) and is, therefore, omitted. For the proof of part (b), argue as in the proof of
Theorem 3.6.5(a), using the statement in part (c) of Theorem 2.5.1. Alternatively,
Theorem 4.1.6 may be obtained from Theorem 4.1.2 using the next proposition
(Ex. 4.6.18). We state the proposition in the form suitable for later references.
Proposition 4.1.7. Let φ be a nonstandard involution.
(a) A matrix H ∈Hn×n is hermitian, resp. skewhermitian, if and only if β(φ)H
is φ-skewhermitian, resp.
φ-hermitian; equivalently Hβ(φ) is φ-skewhermitian,
resp. φ-hermitian.
(b) If X ∈Hm×n, Y ∈Hp×r, S ∈Hm×p, T ∈Hn×r, then the equality Y =
S∗XT holds true if and only if the equality
β(φ)Y = Sφ(β(φ)X)T
(4.1.4)
does.

SYMMETRIC MATRICES AND CONGRUENCE
67
Proof. Part (a): Assume H = [hi,j]n
i,j=1 = ±H∗, where hi,j ∈H. Using the
easily veriﬁable equalities
hi,j(−β(φ)) = −β(φ)h∗
i,j,
i, j = 1, 2, . . . , n,
we obtain
(β(φ)hi,j)φ = −β(φ)h∗
i,j = ∓β(φ)hj,i,
i, j = 1, 2, . . . , n;
therefore, (β(φ)H)φ = ∓β(φ)H. Reversing the argument, the “if” direction of part
(a) follows. Finally, a straightforward algebra shows that β(φ)H is φ-skewhermitian
or φ-hermitian if and only if Hβ(φ) is such.
Part (b): Letting the lowercase letter (with indices) denote the entries of a
matrix given by the corresponding capital letter, equation Y = S∗XT amounts to
yi,j =
X
k,ℓ
s∗
k,ixk,ℓtℓ,j,
i = 1, 2, . . . , p,
j = 1, 2, . . . , r,
(4.1.5)
whereas (4.1.4) amounts to
β(φ)yi,j =
X
k,ℓ
(sk,i)φβ(φ)xk,ℓtℓ,j,
i = 1, 2, . . . , p,
j = 1, 2, . . . , r.
(4.1.6)
Comparing (4.1.5) and (4.1.6), we see that statement (b) boils down to the equality
sφβ(φ) = β(φ)s∗, s ∈H, which can be easily veriﬁed (cf. (2.4.8)).
□
Deﬁnition 4.1.8. We say that the ordered triple of nonnegative integers
(In+ (H), In−(H), In0 (H)) := (p, q, n −p −q),
as in Theorem 4.1.6, is the inertia of a hermitian matrix H ∈Hn×n. The signature
of a hermitian matrix H ∈Hn×n is deﬁned as the diﬀerence between the number
of positive eigenvalues, counted with multiplicities, and the number of negative
eigenvalues, counted with multiplicities.
In terms of the inertia, the signature of H is equal to In+ (H) −In−(H).
Deﬁnition 4.1.9. The matrix H is said to be positive deﬁnite, resp.
posi-
tive semideﬁnite, negative deﬁnite, negative semideﬁnite, if In+ (H) = n, resp.
In+ (H) + In0 (H) = n, In−(H) = n, In−(H) + In0 (H) = n, in complete analogy
with the corresponding classes of real symmetric and complex hermitian matrices.
Note that this deﬁnition of positive (semi)deﬁniteness is equivalent to the deﬁ-
nition of positive (semi)deﬁniteness given in Section 3.2.
Theorems 4.1.2 and 4.1.6 lead to the inertia theorem.
Theorem 4.1.10. Two (quaternion) hermitian matrices are congruent if and
only if they have the same inertia.
For a given nonstandard involution φ, two
φ-skewhermitian matrices are φ-congruent if and only if they have the same β(φ)-
inertia.
For φ-skewhermitian matrices with balanced inertia and having even rank, an
alternative canonical form is available.
Theorem 4.1.11. Let φ be an involution of H. Assume either (a) or (b) holds:

68
CHAPTER 4
(a) φ is nonstandard, and H ∈Hn×n is a φ-skewhermitian matrix with balanced
inertia;
(b) φ is the conjugation, and H ∈Hn×n is a skewhermitian matrix of even rank.
Then there exists an invertible S such that
SφHS =


0
Ik/2
0
−Ik/2
0
0
0
0
0

,
(4.1.7)
where k is the rank of H (note that the rank of H is even in the case (a) as well).
Conversely, if equality (4.1.7) holds for some invertible S, then either (a) or (b)
is valid.
Proof. If (a) holds, then the result follows from the canonical form of Theorem
4.1.2(b) and the equality

1
−1
1
2β(φ)
1
2β(φ)
  β(φ)
0
0
−β(φ)
 
1
−1
2β(φ)
−1
−1
2β(φ)

=

0
1
−1
0

.
(4.1.8)
If (b) holds, then the result follows from the canonical form of Theorem 4.1.6(b)
and the equality
S∗
 i
0
0
i

S =
 0
−1
1
0

,
(4.1.9)
where
S =
1
√
2
 −j
−k
1
i

.
Note that S−1 = S∗.
The converse statement follows from the equalities (4.1.8) and (4.1.9) and the
uniqueness of the canonical form of A, as given in Theorems 4.1.2 and 4.1.6.
□
It is easy to see that the matrix in the right-hand side of (4.1.7) can be trans-
formed by congruence (or φ-congruence) to the form
diag






0
1
−1
0

, . . . ,

0
1
−1
0

|
{z
}
k/2 times
, 0, . . . , 0




.
Sometimes we are interested in diagonalization under congruence of matrices
that are φ-hermitian or φ-skewhermitian, not necessarily in the canonical form
under congruence. It turns out that this can be achieved using matrices S from a
restricted set. We denote by Qn(k, j) the matrix obtained from In by interchanging
the kth and jth rows (or columns) and by Qn(k, jλ) the matrix obtained from In by
adding λ times the jth column to the kth column. Here j ̸= k, j, k = 1, 2, . . . , n, and
λ ∈H. Further, denote by Gn the multiplicative group generated by all matrices
Qn(k, j) and Qn(k, jλ) (for a ﬁxed n); in other words, Gn consists of all products
of matrices of the form Qn(k, j), Qn(k, jλ). Note that Qn(k, j) is its own inverse
and Qn(k, jλ)−1 = Qn(k, j−λ).
Theorem 4.1.12. Let φ be the conjugation or a nonstandard involution, and let
H = ±Hφ ∈Hn×n. Then there exists S ∈Gn such that SφHS is diagonal.
The proof follows the pattern of the proof of Theorem 4.1.2, Part (b). We omit
details.

SYMMETRIC MATRICES AND CONGRUENCE
69
4.2
NEUTRAL AND SEMIDEFINITE SUBSPACES
Let φ be an involution; thus, either φ is nonstandard or φ is the conjugation. To
keep a uniform notation, let us agree that Xφ, X ∈Hm×n, where φ being the
conjugation simply means X∗.
Deﬁnition 4.2.1. Given a φ-hermitian or φ-skewhermitian matrix H ∈Hn×n,
in other words, Hφ = ±H, a subspace M ⊆Hn×1 is said to be (H, φ)-neutral, or
(H, φ)-isotropic if yφHx = 0 for all x, y ∈M or, equivalently, if xφHx = 0 for all
x ∈M.
The equivalence of these two deﬁnitions follows from the polarization identities
(3.5.1) and (3.7.1); cf. the proof of Proposition 3.5.1.
In the cases when φ is nonstandard and H ∈Hn×n is φ-skewhermitian or φ
is the conjugation and H is hermitian, we introduce also semideﬁnite and deﬁnite
subspaces.
Deﬁnition 4.2.2. Let H be hermitian and φ be the conjugation.
A sub-
space M ⊆Hn×n is said to be (H,∗)-nonnegative, resp. (H,∗)-positive, (H,∗)-
nonpositive, (H,∗)-negative,
if x∗Hx ≥0 holds for all x ∈M, resp. x∗Hx > 0
holds for all nonzero x ∈M, x∗Hx ≤0 holds for all x ∈M, x∗Hx < 0 holds for all
nonzero x ∈M. If φ is a nonstandard involution and H ∈Hn×n is φ-skewhermitian,
then we say that a subspace M ⊆Hn×n is (H, φ)-nonnegative, resp. (H, φ)-positive,
(H, φ)-nonpositive, (H, φ)-negative,
if (β(φ))−1xφHx ≥0 holds for all x ∈M,
resp. (β(φ))−1xφHx > 0 holds for all nonzero x ∈M, (β(φ))−1xφHx ≤0 holds
for all x ∈M, (β(φ))−1xφHx < 0 holds for all nonzero x ∈M.
Note that xφHx in the above deﬁnition is a real multiple of β(φ), so
(β(φ))−1xφHx = xφHx(β(φ))−1
is a real number.
We use the notation S0(H), S≥0(H), S>0(H), S≤0(H), and S<0(H) to desig-
nate the classes of all (H, φ)-neutral, all (H, φ)-nonnegative, all (H, φ)-positive, all
(H, φ)-nonpositive, and all (H, φ)-negative subspaces, respectively. In this nota-
tion, we suppress the dependence of Sµ(H), µ ∈{0, ≥0, > 0, ≤0, < 0}, on φ; it
will be clear from context whether φ is the conjugation or a nonstandard involution
and whether H is φ-hermitian or φ-skewhermitian.
Observe that by default (nonexistence of nonzero vectors) the zero subspace is
simultaneously (H, φ)-positive and (H, φ)-negative (here H is hermitian and φ is
the conjugation, or φ is a nonstandard involution and H is φ-skewhermitian).
Clearly, for a ﬁxed φ and H, if a subspace V ⊆Hn×1 belongs to one of the
classes S≥0(H), S>0(H), S≤0(H), S<0(H), S0(H), then so does every subspace of
V. On the other hand, the sum of two subspaces in S≥0(H) does not necessarily
belong to S≥0(H); this observation applies also to the other four classes S>0(H),
S≤0(H), S<0(H), S0(H).
The following example illustrates this phenomenon.
Example 4.2.3. (a) Let
H = H∗=
 1
a
a
1

∈H2×2,

70
CHAPTER 4
where a is real and |a| > 1. Then SpanH {e1}, SpanH {e2} ∈S>0(H), but
SpanH{e1} + SpanH{e2} = H2×1 /∈S>0(H),
because H is not positive deﬁnite.
(b) Let
H =
 0
1
1
0

∈H2×2.
Then SpanH {e1}, SpanH {e2} ∈S0(H), but
SpanH {e1} + SpanH {e2} = H2×1,
which is not in S0(H).
□
Let φ be the conjugation and H = H∗, or let φ be a nonstandard involution
and H = −Hφ.
Deﬁnition 4.2.4. A subspace M ∈S>0(H) is said to be maximal (H, φ)-
nonnegative if there is no larger subspace in the set S>0(H), in other words, if
N ∈S>0(H) and N ⊇M implies N = M.
Analogously, maximal (H, φ)-
nonpositive, maximal (H, φ)-neutral, maximal (H, φ)-positive, and maximal (H, φ)-
negative subspaces are deﬁned.
In Example 4.2.3(a), the subspaces SpanH {e1} and SpanH {e2} are maximal
(H, φ)-nonnegative, φ being the conjugation. In Example 4.2.3(b), SpanH {e1} and
SpanH {e2} are maximal (H, φ)-neutral.
Also, for φ the conjugation and H = −H∗or φ a nonstandard involution and
H = Hφ, we deﬁne maximal (H, φ)-neutral subspaces.
In this section we will be concerned with characterization of maximal (by inclu-
sion) neutral and semideﬁnite subspaces in terms of dimensions and inertia (when
inertia is deﬁned). First, a simple observation showing that all the classes of sub-
spaces introduced above in this section respect transformations of congruence.
Proposition 4.2.5. Let φ be an involution (conjugation or nonstandard).
(a) Let H = ±Hφ ∈Hn×n. Then for every invertible T ∈Hn×n, we have
S0(TφHT) = T −1 (S0(H)) .
(b) Let H ∈Hn×n be hermitian if φ is the conjugation or φ-skewhermitian if φ
is a nonstandard involution. Then for every invertible T ∈Hn×n, we have
Sµ(TφHT) = T −1 (Sµ(H)) ,
(4.2.1)
where µ is one of the symbols {0, ≥0, > 0, ≤0, < 0}.
The proof is immediate. Consider, for instance, the case µ = 0. If a subspace
M is such that xφHx = 0 for all x ∈M, then for any invertible T ∈Hn×n we have
(T −1x)φTφHT · (T −1x) = 0, so the set of vectors of the form T −1x, x ∈M, forms
a (TφHT, φ)-neutral subspace. This shows the ⊇inclusion in (4.2.1). The converse
inclusion follows by reversing the roles of H and TφHT.
It turns out that the maximal elements in each of the sets S≥0(H), S>0(H),
S0(H), S≤0(H), S<0(H) (when deﬁned) have the same dimension. In the next
theorem we state this property formally and identify the dimensions of the maximal
elements in terms of inertia.

SYMMETRIC MATRICES AND CONGRUENCE
71
Theorem 4.2.6. Assume that either φ is the conjugation and H ∈Hn×n is
hermitian or φ is a nonstandard involution and H ∈Hn×n is φ-skewhermitian. Let
(In+ (H), In−(H), In0 (H)) be the inertia of H in the former case and the β(φ)-
inertia of H in the latter case. Then:
(a) M ∈S≥0(H) is maximal (H, φ)-nonnegative if and only if dim M = In+(H)+
In0(H);
(b) M ∈S>0(H) is maximal (H, φ)-positive if and only if dim M = In+(H);
(c) M ∈S≤0(H) is maximal (H, φ)-nonpositive if and only if dim M = In−(H)+
In0(H);
(d) M ∈S<0(H) is maximal (H, φ)-negative if and only if dim M = In−(H);
(e) M ∈S0(H) is maximal (H, φ)-neutral if and only if
dim M = min(In+(H) + In0(H), In−(H) + In0(H)).
(4.2.2)
It follows from Theorem 4.2.6 that in each of the cases (a)–(e), there exists
a subspace in the indicated class Sµ(H), µ ∈{≥0, > 0, ≤0, < 0, 0}, having the
dimension equal to In+(H) + In0(H), In+(H), In−(H) + In0(H), In−(H), or the
right-hand side of (4.2.2), respectively.
The lengthy proof of Theorem 4.2.6 is given in Section 4.3.
If inertia is absent, maximal neutral subspaces can be characterized by dimen-
sion expressed in terms of the rank.
Theorem 4.2.7. Let H = −Hφ and φ be the conjugation or H = Hφ and φ be a
nonstandard involution. Let M be an (H, φ)-neutral subspace. Then M is maximal
(by inclusion) (H, φ)-neutral subspace if and only if
dim M = n −⌈rank H⌉
2
,
where ⌈x⌉stands for the least even integer greater than or equal to x.
In particular, there exists an (H, φ)-neutral subspace of dimension n−⌈rank H⌉/2.
The proof of Theorem 4.2.7 will be given in Section 4.4.
We indicate a simple but very useful corollary of Theorem 4.2.6.
Corollary 4.2.8. Under the notation and hypotheses of Theorem 4.2.6, suppose
that H ∈Hn×n has the form
H = H1 ⊕H2 ⊕· · · ⊕Hs,
where Hj ∈Hnj×nj,
for j = 1, 2, . . . , s.
(4.2.3)
Let there be given a subspace M of Hn×1 of the form
M =


M1
M2
...
Ms

:=











x1
x2
...
xs

: x1 ∈M1, . . . , xs ∈Ms









,
(4.2.4)
where Mj is a ﬁxed subspace of Hnj×1. Then:

72
CHAPTER 4
(1) M is (H, φ)-nonnegative if and only if each Mj is (Hj, φ)-nonnegative;
(2) M is maximal (H, φ)-nonnegative if and only if each Mj is maximal (Hj, φ)-
nonnegative;
(3) M is (H, φ)-negative if and only if each Mj is (Hj, φ)-negative;
(4) M is maximal (H, φ)-negative if and only if each Mj is maximal (Hj, φ)-
negative;
(5) M is (H, φ)-neutral if and only if each Mj is (Hj, φ)-neutral.
Remark 4.2.9. The statement obtained from Corollary 4.2.8 by replacing neg-
ative with positive everywhere in (1)–(4) is valid as well and can be veriﬁed by
applying Corollary 4.2.8 to −H.
Remark 4.2.10. Part (5) holds true also for H and M given by (4.2.3) and
(4.2.4), respectively, where H = −Hφ and φ is the conjugation or H = Hφ and φ
is a nonstandard involution.
Remark 4.2.11. If each Mj is maximal (Hj, φ)-neutral, then generally it does
not follow that M is maximal (H, φ)-neutral. For example, if H = 1⊕(−1) ∈H2×2,
then M1 = M2 = {0} is maximal 1-neutral as well as maximal −1-neutral, but
M =
 M1
M2

= {0} is not maximal H-neutral.
Proof. Parts (1), (3), and (5) follow without diﬃculty from the deﬁnitions of
(H, φ)-nonnegative and (H, φ)-negative subspaces. Parts (2) and (4) follow from
Theorem 4.2.6, the easily veriﬁed property that the inertia is additive with respect
to direct sum of matrices:
Inµ(H) = Inµ(H1) + · · · + Inµ(Hs),
(4.2.5)
where µ ∈{0, +, −}, and the additive property of dimensions:
dimH M =
s
X
j=1
dimH Mj.
□
4.3
PROOF OF THEOREM 4.2.6
First, observe that it suﬃces to prove statements (a), (b), and (e); statements (c)
and (d) follow by applying (a) and (b) to −H and using
S≥0(−H) = S≤0(H),
S>0(−H) = S<0(H).
Step 1.
We exhibit (H, φ)-nonnegative, (H, φ)-positive, and (H, φ)-neutral
subspaces of dimensions In+(H) + In0(H), In+(H), and
min(In+(H) + In0(H), In−(H) + In0(H)),
respectively. To this end, in view of Proposition 4.2.5 and Theorems 4.1.2 and 4.1.6,
we can assume that
H =


δIp
0
0
0
−δIq
0
0
0
0n−p−q

,
(4.3.1)

SYMMETRIC MATRICES AND CONGRUENCE
73
where δ = β(φ) if φ is a nonstandard involution, and δ = 1 if φ is the conjugation.
Now, clearly,
SpanH{e1, . . . , ep, ep+q+1, . . . , en},
SpanH{e1, . . . , ep},
and
SpanH{e1 + ep+1, e2 + ep+2, . . . , emin{p,q} + ep+min{p,q}, ep+q+1, . . . , en}
are (H, φ)-nonnegative, (H, φ)-positive, and (H, φ)-neutral subspaces, respectively,
of the requisite dimensions.
Step 2. Proof of the “if” parts.
Arguing by contradiction, suppose an (H, φ)-nonnegative subspace M has di-
mension In+(H) + In0(H), but it is not maximal (H, φ)-nonnegative. Let M′ be
an (H, φ)-nonnegative subspace strictly larger than M. By Step 1 (applied to −H
rather than H), there exists an (H, φ)-negative subspace N of dimension In (H).
Since dim M′ + dim N > n, there is a nonzero x ∈M′ ∩N. Then δ−1xφHx ≥0
as x ∈M′, and δ−1xφHx < 0 as x ∈N, a contradiction.
The “if” part of
(b) is proved analogously. For the “if” part of (e), assume without loss of gen-
erality, that In+(H) ≤In−(H) (otherwise consider −H in place of H; note that
S0(−H) = S0(H)). Now argue by contradiction as above.
In Steps 3, 4, and 5 below we suppose that φ is the conjugation and H ∈Hn×n
is hermitian.
Step 3. Proof of the “only if” part of (b).
Let M0 ∈S>0(H), and assume that dim M0 < In+(H). We have to prove
that there exists M ∈S>0(H) such that M0 is properly contained in M. Letting
d = dim M0, let f1, . . . , fn ∈Hn×1 be an orthonormal basis of Hn×1 in which the
ﬁrst d vectors f1, . . . , fd form an orthonormal basis of M0. (Such f1, . . . , fn exist
because the Gram-Schmidt orthonormalization process is available in Hn×1 with
respect to the conjugation.) Write H with respect to the basis {f1, . . . , fn}; in
other words, replace H with U ∗HU and M with U ∗M, where U = [f1f2 . . . fn]
is the unitary matrix formed by f1, . . . , fn. In view of Proposition 4.2.5, we can
assume without loss of generality, that M0 = SpanH {e1, . . . , ed}. Partition
H =
"
H11
H12
H∗
12
H22
#
,
where H11 is d × d and H22 is (n −d) × (n −d). Because of our assumption that
M0 ∈S>0(H), we have that H11 is positive deﬁnite (see Ex. 4.6.5). In particular,
H11 is invertible.
The equality
"
I
0
−H∗
12H−1
11
I
# "
H11
H12
H∗
12
H22
# "
I
−H−1
11 H12
0
I
#
=
" H11
0
0
H22 −H∗
12H−1
11 H12
#
and Proposition 4.2.5 show that we can (and do) further assume without loss of
generality, that H12 = 0. Since d < In+(H) and In+(H) = d + In+(H22) (Ex.

74
CHAPTER 4
4.6.6), we have In+ (H22) > 0, and so there is a nonzero v ∈H(n−d)×1 such that
v∗H22v > 0. Then the subspace M0 + Span

0d
v

is strictly larger than M0 and
is easily seen to be (H, φ)-positive. This completes the proof of Step 3.
Step 4. Proof of the “only if” part of (a).
Let M ∈S≥0(H), and assume that dim M < In+ (H)+In0 (H). Then for every
positive integer m, we have M ∈S>0 (H + (1/m)I). As in Step 1, without loss of
generality, it will be assumed that H has the form (4.3.1) with δ = 1. Evidently,
In+

H + 1
mI

= In+ (H) + In0 (H)
(m > 1).
By Step 3, there exists a subspace Mm ∈S>0 (H + (1/m)I) such that Mm ⊇M
and dim Mm = dim M + 1. By Theorem 3.9.2, the sequence {Mm}∞
m=1 has a
convergent subsequence {Mmk}∞
k=1. Denote by M′ the limit of Mmk (as k →∞).
Then dim M′ = dim M + 1 (Theorem 3.9.3). Also, M′ ∈S≥0(H). Indeed, by
Theorem 3.9.4 every vector x ∈M′ is the limit of some sequence {xmk}∞
k=1, where
xmk ∈Mmk (k = 1, 2, . . .). Now
⟨Hx, x⟩= lim
k→∞

H + 1
mI

xmk, xmk

≥0,
because

 H + 1
mI

xmk, xmk

≥0 for every k. Finally, M′ ⊇M because Mmk ⊇
M for k = 1, 2, . . .. This completes the proof of Step 4.
Step 5. Proof of the “only if” part of (e).
Let be given M0 ∈S0(H) having dimension
d < min(In+ (H) + In0 (H), In−(H) + In0 (H)).
We will prove that there exists M ∈S0(H) such that M ⊇M0 and dim M =
d + 1. Since S0(H) ⊆S≥0(H), by part (a) there exists M+ ⊇M0 such that dim
M+ = d + 1 and M+ ∈S≥0(H). Analogously, there exists M−∈S≤0(H) such
that M−⊇M0 and dim M−= d + 1. Select f+ ∈M+ \ M0 and f−∈M−\ M0.
We claim that ⟨Hf+, g⟩= 0 for every g ∈M0. Indeed, for every real λ and g ∈M0
we have
0 ≤⟨H(λf+ + g), λf+ + g⟩= λ2⟨Hf+, f+⟩+ λ · 2R(⟨Hf+, g⟩).
Since this inequality holds for all real λ and ⟨Hf+, f+⟩≥0, we must have the equal-
ity R(⟨Hf+, g⟩) = 0. Replacing here g by gα, α ∈H, it follows that R(α∗⟨Hf+, g⟩)
= 0 for all α ∈H, which is possible only if ⟨Hf+, g⟩= 0. Analogously, ⟨Hf−, g⟩= 0
for every g ∈M0.
We can (and do) assume that M−̸= M+ (otherwise, M−= M+ is H-neutral
of dimension d + 1, and we are done). For 0 ≤α ≤1, consider the subspace
Mα := SpanH {αf+ + (1 −αf−} + M0.
We have f+ /∈M0, f−/∈M0 and αf+ + (1 −α)f−/∈M0 for 0 < α < 1 (otherwise,
we would have M−= M+). Thus, dim Mα = d + 1 for every α ∈[0, 1]. Since

SYMMETRIC MATRICES AND CONGRUENCE
75
⟨Hf−, g⟩= ⟨Hf+, g⟩= 0 for every g ∈M0, we also have ⟨H(αf+ +(1−α)f−), g⟩=
0 for every g ∈M0. Since ⟨Hf+, f+⟩≥0 ≥⟨Hf−, f−⟩and the function
z(α) := ⟨H(αf+ + (1 −α)f−), αf+ + (1 −α)f−⟩
is a continuous real-valued function of α ∈[0, 1], there is α0 ∈[0, 1] such that
z(α0) = 0. Then the subspace Mα0 is H-neutral, has dimension d+1, and contains
M0.
Step 6. Proof of Theorem 4.2.6 for a nonstandard φ.
Using Proposition 4.2.5 and Theorem 4.1.2, we may assume that
H = diag (β(φ)Ip, −β(φ)Iq, 0n−q−p).
Let
bH := diag (Ip, −Iq, 0n−q−p) = (β(φ))−1H.
A straightforward computation using a units triple (q1, q2, q3) with q1 = β(φ) (so
that (q2)φ = q2, (q3)φ = q3) shows that
β(φ)xφ = x∗β(φ),
for all x ∈Hn×1.
Thus
β(φ)−1xφHx = x∗bHx,
for all x ∈Hn×1.
It follows that a subspace M ⊆Hn×1 is (H, φ)-nonnegative or (H, φ)-positive,
etc., if and only if M is ( bH, φ′)-nonnegative or ( bH, φ′)-positive, etc.; here φ′ is the
conjugation. We have reduced the proof to the case when the involution is the
conjugation and H is a hermitian matrix.
This concludes the proof of Theorem 4.2.6.
□
4.4
PROOF OF THEOREM 4.2.7
This section is devoted to the proof of Theorem 4.2.7.
It will be convenient to have a simple lemma ﬁrst.
Lemma 4.4.1. If φ is a nonstandard involution, then every subspace of Hn×1 of
dimension at least two contains a nonzero φ-selforthogonal vector x, i.e., xφx = 0.
Proof.
It suﬃces to consider the two-dimensional subspace M spanned by
vectors y1, y2 ∈Hn×1. If (y1)φy1 = 0 or (y2)φy2 = 0, we are done: take x = y1
or x = y2, as the case may be. Otherwise, consider the φ-orthogonal companion
(SpanH {y1})⊥φ of SpanH {y1}. By Proposition 3.6.4(b),
M ∩(SpanH {y1})⊥φ = SpanH {z}
for some nonzero z ∈M. If zφz = 0, we are done again. Otherwise, we seek x in
the form x = y1 + zα for a suitable α ∈H. The condition xφx = 0 can be rewritten
as
(y1)φy1 + αφ(zφz)α = 0.
(4.4.1)
Recall that (y1)φy1 ̸= 0, zφz ̸= 0 by our assumptions. By Theorem 2.5.1(b), we ﬁnd
γ1, γ2 ∈H\{0} such that (γ1)φγ1 = −(y1)φy1, (γ2)φγ2 = zφz. Now take α = γ−1
2 γ1
to satisfy (4.4.1).
□

76
CHAPTER 4
Proof of Theorem 4.2.7. We divide the proof into steps.
Step 1. Proof of the “if” part.
Let M be (H, φ)-neutral of dimension k. Letting P ∈Hn×n be an invertible
matrix whose ﬁrst k columns form a basis for M, we obtain
PφHP =

0k×k
H1
±(H1)φ
H2

(4.4.2)
for some H2 ∈H(n−k)×(n−k), H1 ∈Hk×(n−k). From (4.4.2), we have
rank H = rank (PφHP) ≤rank (H1)φ + rank

H1
H2

≤2(n −k),
or
k ≤n −rank H
2
,
which implies (since k is an integer) k ≤n −⌈rank H⌉/2. This shows that every
(H, φ)-neutral subspace of dimension n −⌈rank H⌉/2 is maximal, which proves the
“if” part of the theorem.
Such subspaces exist. Indeed, to construct an (H, φ)-neutral subspace of dimen-
sion n−⌈rank H⌉/2, in the case φ is nonstandard, without loss of generality, we may
assume in view of Theorem 4.1.2 that H = Ip ⊕0n−p, p = rank H. It is easy to see
that the construction boils down to constructing a one-dimensional (I2, φ)-neutral
subspace. One such subspace is SpanH

1
β(φ)

. If φ is the conjugation, then in
view of Theorem 4.1.6 we may assume H = iIr ⊕0n−r, r = rank H. In this case,
SpanH

1
j

is a one-dimensional (iI2, φ)-neutral subspace.
Step 2.
Proof of the “only if” part in the case H = I, φ a nonstandard
involution.
Let M be an (I, φ)-neutral subspace of dimension d ≤n −1 −⌈n⌉/2. We need
to show that there exists an (I, φ)-neutral subspace that is strictly larger than M.
By Proposition 3.6.4, the φ-orthogonal companion M⊥φ has dimension at least
1 + ⌈n⌉/2. Since
1 + ⌈n⌉
2
≥

n −1 −⌈n⌉
2

+ 2,
there exists a two-dimensional subspace in M⊥φ, call it N, such that N ∩M = {0}.
By Lemma 4.4.1, we ﬁnd a nonzero φ-selforthogonal vector x ∈N. Then it is easy
to see that the subspace spanned by M and x is strictly larger than M and is
(I, φ)-neutral.
Step 3. Proof of the “only if” part in the case H = Hφ, φ a nonstandard
involution.
As in Step 2, given an (H, φ)-neutral subspace M of dimension d with
d ≤n −1 −⌈rank H⌉
2
,
(4.4.3)
we need to show that there exists an (H, φ)-neutral subspace which is strictly larger
than M. By Proposition 4.2.5 and Theorem 4.1.2, we may (and do) assume that

SYMMETRIC MATRICES AND CONGRUENCE
77
H =
 Iℓ
0
0
0

, where ℓ= rank H. In view of the already proved part of Theorem
4.2.7 in Step 1, we suppose ℓ< n.
If M does not contain SpanH {eℓ+1, . . . , en}, then the (H, φ)-neutral subspace
M + SpanH {eℓ+1, . . . , en}
is strictly larger than M. We suppose, therefore, that
M ⊇SpanH {eℓ+1, . . . , en}.
(4.4.4)
In particular, (4.4.4) entails the assumption that d ≥n −ℓ. Choose a basis for M
of the form
{f1, . . . , fd−n+ℓ, eℓ+1, . . . , en},
where
f1 =

f ′
1
0

, f2 =

f ′
2
0

, . . . , fd−n+ℓ=
 f ′
d−n+ℓ
0

∈SpanH {e1, . . . , eℓ},
with f ′
1, f ′
2, . . . , f ′
d−n+ℓ∈Hℓ×1. Deﬁne the subspace
M′ = SpanH {f ′
1, f ′
2, . . . , f ′
d−n+ℓ} ⊆Hℓ×1.
Clearly M′ is (Iℓ, φ)-neutral and has dimension d−n+ℓ. Condition (4.4.3) implies
d −n + ℓ≤ℓ−⌈ℓ⌉
2 −1;
thus, by the result of Step 2, M′ is not a maximal (Iℓ, φ)-neutral subspace in Hℓ×1.
Now, if M′′ is an (Iℓ, φ)-neutral subspace strictly larger than M′ and if g1, . . . , gs
is a basis for M′′, then
SpanH
 g1
0

,
 g2
0

, . . . ,
 gs
0

+ SpanH {e1, . . . , eℓ}
is an (H, φ)-neutral subspace strictly larger than M.
It remains to prove the “only if” part in the case H = −Hφ, φ the conjugation.
This follows from the already proven part of Theorem 4.2.7 by using Proposition
4.1.7. Indeed, letting φ be any nonstandard involution, we see that the matrix
β(φ)H is φ-hermitian. Moreover, Proposition 4.1.7(b) shows, by taking X = H
and S and T vectors in a subspace M, that M is (H,∗)-neutral if and only if it is
(β(φ)H, φ)-neutral.
Alternatively, the “only if” part in the case H = −H∗can be proved analogously
to Steps 2 and 3, using the following lemma in place of Lemma 4.4.1.
Lemma 4.4.2. Let G ∈Hn×n be skewhermitian. Then every subspace M of
Hn×1 of dimension at least two contains a nonzero vector x which is selforthogonal
in the sense of G, i.e., x∗Gx = 0.
Proof. We may assume that the dimension of M is two. Let y1, . . . , yn be an
orthonormal basis in Hn×1 such that y1, y2 is a basis for M. Replacing M with

78
CHAPTER 4
S(M), where S =


y∗
1
y∗
2
...
y∗
n

, and replacing H with (S∗)−1HS−1, we can (and do)
assume that M = SpanH {e1, e2}. Using Theorem 4.1.6 we may further assume
that the 2 × 2 upper-left corner of G has one of the three forms (1)

i
0
0
i

; (2)
 i
0
0
0

; (3) 02. If (2) or (3) occurs, we take x = e2. If (1) occurs, we take
x = e1 + je2. In all cases, the equality x∗Gx = 0 is easily veriﬁed.
□
This completes the proof of Theorem 4.2.7.
4.5
REPRESENTATION OF SEMIDEFINITE SUBSPACES
In this section, either φ is the conjugation and H = H∗∈Hn×n or φ is a nonstan-
dard involution and H = −Hφ. We present here a representation of semideﬁnite
subspaces with respect to φ which will be useful later on.
Lemma 4.5.1. Let M ⊆Hn×1 be an (H, φ)-nonnegative subspace of dimension
d. Then there is a nonsingular P ∈Hn×n such that P −1M = SpanH {e1, . . . , ed}
and PφHP has the form
PφHP =


0d′′
0
0
0
0
0
0
0
Id′
0
0
0
♠Id2
0
0
0
δId′
0
0
0
0
0
0
0
H0


,
(4.5.1)
where d2 + d′ + d′′ = d and where ♠= 1, δ = 1 if φ = ∗and ♠= β(φ), δ = −1 if
φ is nonstandard.
Note that H0 in (4.5.1) is necessarily hermitian (if δ = 1) or φ-skewhermitian
(if δ = −1). The cases when some of d2, d′, d′′ are zeros are not excluded.
Proof. The result follows from properties analogous to those of skewly linked
subspaces in the context of complex hermitian matrices; see, e.g., Mehl et al. [110],
Iohvidov et al. [70], Bolshakov et al. [16]. For the case when H is hermitian and
invertible, Lemma 4.5.1 was proved in Alpay et al. [4]. We follow the approach of
Alpay et al.
Applying a transformation H 7→SφHS for some invertible S, we may assume
that M = SpanH {e1, . . . , ed}. Partition
H =

H1
H2
δ(H2)φ
H3

,
where H1 ∈Hd×d. Using one of the canonical forms (4.1.2) or (4.1.3), we may
partition further
H =


H11
H12
H13
H14
(H12)φ
H22
H23
H24
δ(H13)φ
δ(H23)φ
H33
H34
δ(H14)φ
δ(H24)φ
δ(H34)φ
H44

,

SYMMETRIC MATRICES AND CONGRUENCE
79
where

H11
H12
δ(H12)φ
H22

∈Hd×d
(4.5.2)
and where it is assumed H11 = 0, H12 = 0, and H22 = ♠Id2. Using the rank
decomposition for quaternion matrices (Proposition 3.2.5), we have
Q1

H13
H14

Q2 =

0
0
Id1
0

for some invertible (quaternion) matrices Q1 and Q2. Setting P1 = (Q1)φ⊕Id2⊕Q2,
we obtain that (P1)φHP1 has the form
(P1)φHP1 =


0
0
0
0
0
0
0
0
Id1
0
0
0
♠Id2
bH23
bH24
0
δId1
δ( bH23)φ
bH33
bH34
0
0
δ( bH24)φ
δ( bH34)φ
bH44


,
P −1
1
M = M.
Furthermore, setting
P2 = P1


I
0
0
0
0
0
Id1
0
1
2( bH∗
23 bH23 −bH23)
0
0
0
Id2
−bH23
−bH24
0
0
0
Id1
0
0
0
0
0
I


,
we obtain that P −1
2
M = M and
(P2)φHP2 =


0
0
0
0
0
0
0
0
Id1
0
0
0
♠Id2
0
0
0
δId1
0
0
eH34
0
0
0
δ( eH34)φ
eH44


,
and one more obvious transformation (P2)φHP2 7→(P3)φ(P2)φHP2P3 for a suit-
able invertible P3 completes the proof.
□
Analogous result holds for (H, φ)-nonpositive matrices. The only change to be
made is replacement of ♠Id2 in (4.5.1) with −♠Id2.
Corollary 4.5.2. Assume the hypotheses and notation of Lemma 4.5.1. Then:
(1) the subspace M is maximal (H, φ)-nonnegative if and only if H0 is negative
deﬁnite (when φ is the conjugation) or β(φ)H0 is negative deﬁnite (when φ is
nonstandard);
(2) M is (H, φ)-neutral if and only if d2 = 0;
(3) suppose φ is the conjugation; then M is maximal (H, φ)-neutral if and only
if d2 = 0 and H0 is deﬁnite (positive or negative);
(4) suppose φ is the nonstandard; then M is maximal (H, φ)-neutral if and only
if d2 = 0 and β(φ)H0 is deﬁnite (positive or negative).
We leave the proof as an exercise (Ex. 4.6.19).

80
CHAPTER 4
4.6
EXERCISES
Ex. 4.6.1. Provide details for the proof of Theorem 4.1.6.
Ex. 4.6.2. Provide details for the proof of Theorem 4.1.12.
Ex. 4.6.3. Show that the φ-skewhermitian matrix

0
z
−zφ
0

has balanced
inertia for every z ∈H.
Ex. 4.6.4. For a nonstandard involution φ and a φ-skewhermitian H ∈Hn×n,
show that H is β(φ)-positive semideﬁnite, β(φ)-positive deﬁnite, β(φ)-negative
semideﬁnite, or β(φ)-negative deﬁnite if and only if (β(φ))−1xφHx ≥0 for all
x ∈Hn×1, (β(φ))−1xφHx > 0 for all x ∈Hn×1 \ {0}, (β(φ))−1xφHx ≤0 for all
x ∈Hn×1, or (β(φ))−1xφHx < 0 for all x ∈Hn×1 \ {0}, respectively.
Ex. 4.6.5. State and prove results analogous to Ex. 4.6.4 for classes of matrices
with deﬁniteness properties with respect to the conjugation.
Ex.
4.6.6. Prove that inertia and β(φ)-inertia are additive with respect to
block diagonal decompositions: If H = diag (H1, . . . , Hk) ∈Hn×n is hermitian (if
φ is the conjugation) or φ-skewhermitian (if φ is a nonstandard involution), then
H1, . . . , Hk are hermitian or φ-skewhermitian as the case may be, and
Inµ (H) =
k
X
j=1
Inµ (Hj),
for µ = +, −, 0.
Ex. 4.6.7. For a ﬁxed H ∈Hn×n and φ, where either H is hermitian and φ
the conjugation, or H is φ-skewhermitian and φ a nonstandard involution, prove
that the sets S0(H), S≥0(H), and S≤0(H) are closed in the gap metric topology of
Grassn.
Ex. 4.6.8. For H and φ as in Ex. 4.6.7, prove that the sets S>0(H) and S<0(H)
are open in the gap metric topology of Grassn. Hint: Show that the complements
of the sets in question are closed in Grassn.
Ex. 4.6.9. Repeat Ex. 4.6.7 and Ex. 4.6.8, but use the sets of maximal (by
inclusion) elements in each of S0(H), S≥0(H), S≤0(H), S>0(H) and S<0(H) (rather
than using Sµ(H), µ ∈{0, ≥0, ≤0, > 0, < 0}, as in Ex. 4.6.7 and Ex. 4.6.8).
Ex. 4.6.10. Prove that if A ∈Hn×n is positive semideﬁnite, then for x ∈Hn×1
the equality x∗Ax = 0 is equivalent to Ax = 0. Hint: Use Theorem 4.1.6.
Ex. 4.6.11. Find all (Hj, φ)-neutral subspaces, j = 1, 2, for the following two
matrices:
H1 =

0
β(φ)
β(φ)
0

,
H2 =


0
0
β(φ)
0
β(φ)
0
β(φ)
0
0

.
Hint: For H2, consider SpanH


1
x
y

, x, y ∈H.
Ex. 4.6.12. Find the canonical form of Theorem 4.1.6 for the following hermi-
tian matrices in Hn×n:

SYMMETRIC MATRICES AND CONGRUENCE
81
(a)


1
α1
α2
. . .
αn−1
α∗
1
1
0
. . .
0
α∗
2
0
1
. . .
0
...
...
...
...
...
α∗
n−1
0
0
. . .
1


,
α1, . . . , αn−1 ∈H,
(the canonical form depends on α1, . . . , αn−1);
(b)


0
0
. . .
0
α1
0
0
. . .
α2
0
...
...
...
...
...
αn
0
. . .
0
0

,
α1, . . . , αn ∈H,
where αj = α∗
n+1−j for j = 1, 2, . . . , n;
(c)

0
α1
α∗
1
0

⊕· · · ⊕

0
αk
α∗
k
0

,
α1, . . . , αk ∈H,
where n is even and k = n/2.
Ex.
4.6.13. Find the canonical form of Theorem 4.1.2 for the following φ-
skewhermitian matrices in Hn×n:
(a)


β(φ)
α1
α2
. . .
αn−1
−φ(α1)
β(φ)
0
. . .
0
−φ(α2)
0
β(φ)
. . .
0
...
...
...
...
...
−φ(αn−1)
0
0
. . .
β(φ)


,
α1, . . . , αn−1 ∈H;
(b)


0
0
. . .
0
α1
0
0
. . .
α2
0
...
...
...
...
...
αn
0
. . .
0
0

,
α1, . . . , αn ∈H,
where αj = −φ(αn+1−j) for j = 1, 2, . . . , n;
(c)

0
α1
−φ(α1)
0

⊕· · · ⊕

0
αk
−φ(αk)
0

,
α1, . . . , αk ∈H,
where n is even and k = n/2.
Ex. 4.6.14. Let φ be a nonstandard involution. Find all subspaces in the fol-
lowing classes: maximal (H, φ)-nonnegative, maximal (H, φ)-nonpositive, maximal
(H, φ)-neutral, for each of the following φ-skewhermitian matrices H ∈H3×3:
(a)
H =


β(φ)
0
0
0
β(φ)
0
0
0
β(φ)

;
(b)
H =


β(φ)
0
α
0
−β(φ)
0
−φ(α)
0
β(φ)

;
(c)
H =


0
0
β(φ)
0
β(φ)
0
β(φ)
0
0

.

82
CHAPTER 4
Ex. 4.6.15. For each of the following φ-hermitian matrices H ∈H3×3, ﬁnd all
(H, φ)-neutral subspaces:
(a)
H =


1
α1
α2
φ(α1)
1
0
φ(α2)
0
1

,
α1, α2 ∈H;
(b)
H =


0
0
α1
0
α2
0
φ(α1)
0
1

,
α1, α2 ∈H,
φ(α2) = α2.
Ex. 4.6.16. Show that a hermitian matrix
 a
b
b∗
c

∈H2×2 is positive deﬁnite
if and only if a > 0, c > 0, and ac > |b|2.
Ex. 4.6.17. Show that a hermitian matrix
 a
b
b∗
c

∈H2×2 is positive semidef-
inite if and only if a ≥0, c ≥0, and ac ≥|b|2.
Ex. 4.6.18. Deduce Theorem 4.1.6 from Theorem 4.1.2 using Proposition 4.1.7.
Hint: Take T = S in Proposition 4.1.7.
Ex.
4.6.19. Prove Lemma 4.5.2.
Hint: Use Theorem 4.2.6 to identify the
dimensions of maximal (H, φ)-nonnegative and maximal (H, φ)-neutral subspaces.
4.7
NOTES
Results of Theorems 4.1.6(a) and Theorem 4.1.11, as well as the inertia theorem, are
standard for real and complex matrices. For quaternion matrices, Theorems 4.1.2,
4.1.6, and 4.1.11 with complete proofs are found in Brieskorn [20], for example.
Theorem 4.2.6 is known in the real and complex cases; see, e.g., Alpay et al. [4]
(the case of invertible H) and Mehl et al. [110]. The proof of Theorem 4.2.6 given
in Section 4.2 follows the same approach as in Alpay et al.
Theorem 4.2.7 holds in the context of real and complex matrices, with essentially
the same proof.

Chapter Five
Invariant subspaces and Jordan form
We start the chapter by introducing the notion of root subspaces for quaternion
matrices. These are basic invariant subspaces, and we prove in particular that they
enjoy the Lipschitz property with respect to perturbations of the matrix. Another
important class of invariant subspaces are the 1-dimensional ones, i.e., generated
by eigenvectors. Existence of quaternion eigenvalues and eigenvectors is proved,
which leads to the Schur triangularization theorem (in the context of quaternion
matrices) and its many consequences familiar for real and complex matrices. Jor-
dan canonical form for quaternion matrices is stated and proved (both the existence
and uniqueness parts) in full detail. We also discuss in this chapter various con-
cepts of determinants for square-size quaternion matrices. Several applications of
the Jordan form are given, including functions of matrices and boundedness prop-
erties of systems of diﬀerential and diﬀerence equations with constant quaternion
coeﬃcients.
5.1
ROOT SUBSPACES
Deﬁnition 5.1.1. A (quaternion) subspace M ⊆Hn×1 is said to be invariant
for a matrix A ∈Hn×n if AM ⊆M.
Clearly, {0} and Hn×1 are A-invariant for any A ∈Hn×n; these are the trivial
invariant subspaces. In this section we consider a class of A-invariant subspaces
called root subspaces.
If p(t) = Pm
j=0 pjtj is a polynomial of the independent variable t with real
coeﬃcients p0, . . . , pm and A ∈Hn×n, then p(A) is naturally deﬁned as p(A) =
Pm
j=0 pjAj ∈Hn×n. For a ﬁxed A, the map p(t)
7→
p(A) is a unital algebra
homomorphism:
(p + q)(A) = p(A) + q(A);
(pq)(A) = p(A)q(A);
(αp)(A) = αp(A);
1(A) = In
(5.1.1)
for all polynomials p, q with real coeﬃcients and all α ∈R. Also,
p(S−1AS) = S−1p(A)S,
for all invertible S ∈Hn×n.
Fix A ∈Hn×n. Since Hn×n is ﬁnite dimensional (as a real vector space), the
powers of A, {Aj}∞
j=0, cannot be linearly independent, so, p(A) = 0 for some
nonconstant polynomial p(t) with real coeﬃcients. Denote by p(A)(t) the monic
(i.e., with leading coeﬃcient 1) real polynomial of minimal degree such that p(A) =
0. The division algorithm for polynomials, together with the algebraic properties
(5.1.1), shows that p(A)(t) is unique and that p(A)(t) divides any polynomial q(t)
with real coeﬃcients such that q(A) = 0.

84
CHAPTER 5
Deﬁnition 5.1.2. The polynomial p(A)(t) is called the minimal polynomial of
A.
Write
p(A)(t) = p1(t)m1 · · · pk(t)mk,
(5.1.2)
where the pj(t)’s are distinct monic irreducible real polynomials (i.e., of the form
t −a, a real, or of the form t2 + pt + q, where p, q ∈R, with no real roots), and the
mj’s are positive integers.
Deﬁnition 5.1.3. The subspaces
Mj := {u : u ∈Hn×1,
pj(A)mju = 0},
j = 1, 2, . . . , k,
are called the root subspaces of A.
Since Apj(A)mj = pj(A)mjA, the root subspaces of A are A-invariant.
Root subspaces form “building blocks” for invariant subspaces.
Proposition 5.1.4. Let A ∈Hn×n. Then:
(a) the root subspaces decompose Hn×1 into a direct sum:
Hn×1 = M1 ˙+ M2 ˙+ · · · ˙+ Mk;
(5.1.3)
(b) for every A-invariant subspace M,
M = (M ∩M1) ˙+ · · · ˙+ (M ∩Mk).
(5.1.4)
Proof. (a) follows from (b) (take M = Hn×1 in (b)).
To prove (b), we use (5.1.2) and observe that the polynomials
qℓ(t) :=
Y
{j : j̸=ℓ}
pj(t)mj,
ℓ= 1, 2, . . . , k,
have no common real or complex roots.
We now use a well-known property of
polynomial algebra, which can be proved by repeated applications of division with
remainder and is exposed in many texts; see, e.g., Herstein [61] or Artin [6]. Namely,
for any ﬁnite collection of polynomials y1(t), . . . , yk(t) with coeﬃcients in a ﬁeld,
there exist polynomials z1(t), . . . , zk(t) with coeﬃcients in the same ﬁeld such that
y1(t)z1(t) + · · · + yk(t)zk(t) = greatest common divisor of {y1(t), . . . , yk(t)}.
Applying this fact to q1(t), . . . , qk(t), we see that there exist polynomials with real
coeﬃcients r1(t), . . . , rk(t) such that
q1(t)r1(t) + · · · + qk(t)rk(t) ≡1.
Thus, for every x ∈M we have
x = q1(A)r1(A)x + · · · + qk(A)rk(A)x.
Clearly, qj(A)rj(A)x ∈M ∩Mj for j = 1, 2, . . . , k, and the inclusion ⊆in (5.1.4)
follows. The converse inclusion is obvious.

INVARIANT SUBSPACES AND JORDAN FORM
85
It remains to prove that the Mj’s form a direct sum. Assume that x1+· · ·+xk =
0 for some x1 ∈M1, . . . , xk ∈Mk. We have:
p1(A)m1x1 = 0,
q1(A)(x2 + · · · + xk) = 0.
(5.1.5)
Since the polynomials p1(t)m1 and q1(t) are relatively prime, there exist polynomials
with real coeﬃcients s1(t) and s2(t) such that
s1(t)p1(t)m1 + s2(t)q1(t) ≡1.
Now
x1
=
(s1(A)p1(A)m1 + s2(A)q1(A))x1
=
s1(A)p1(A)m1x1 −s2(A)q1(A)(x2 + · · · + xk) = 0
in view of (5.1.5). Analogously, xj = 0 for j = 2, 3, . . . , k.
□
We note that the root subspaces are also given by
Mℓ= Ran


Y
{j : j̸=ℓ}
pj(A)mj

,
ℓ= 1, 2, . . . , k,
(5.1.6)
where the minimal polynomial of A is (5.1.2). To verify this, note that the right-
hand side of (5.1.6) is an A-invariant subspace, and apply Proposition 5.1.6(b) to
it.
For a given A ∈Hn×n, we will identify a sum M of root subspaces of A by the
set of (complex and real) roots of the minimal real polynomial of the restriction
A|M of A to M (the restriction A|M is understood as the matrix of the H-linear
transformation AM : M
→
M with respect to some basis in M). Note that
this set is necessarily closed under complex conjugation. Thus, for example, the
sum M of root subspaces of A corresponding to the set {1, i, −i} is identiﬁed by the
property that the minimal real polynomial of AM is of the form (x−1)m1(x2+1)m2
for some integers m1, m2.
Open Problem 5.1.5. Describe the set of A-invariant subspaces in terms of
analytic manifolds.
For complex invariant subspaces of complex matrices, such a description was
obtained by Shayman [144].
In view of Proposition 5.1.4, for the open problem 5.1.5 it suﬃces to consider
the case when A has only one root subspace.
5.2
ROOT SUBSPACES AND MATRIX REPRESENTATIONS
In this section, we study the transformation of sums of root subspaces of quaternion
matrices under real and complex representations developed in Sections 3.3 and 3.4.
At this point, we need to recall the concepts of root subspaces for real and
complex matrices which are familiar in real and complex linear algebra. The con-
struction of root subspaces for a matrix A ∈Rn×n is completely analogous to the
construction for quaternion matrices given in Section 5.1, but using Rn×1 rather

86
CHAPTER 5
than Hn×1; thus the root subspaces of A are (real) subspaces of Rn×1. For a com-
plex matrix A ∈Cn×n the minimal polynomial p(A)(t) is a polynomial with complex
coeﬃcients, and we use the factorization
p(A)(t) = (t −a1)m1 (t −a2)m2 · · · (t −ak)mk,
where a1, . . . , ak are all the distinct complex roots of p(A)(t). The root subspaces
of A are deﬁned as
{u : u ∈Cn×1,
(A −aj)mju = 0},
for j = 1, 2, . . . , k.
They are (complex) subspaces of Cn×1.
Analogously to the quaternion matrices in Section 5.1, a sum of root subspaces
M of a real, resp. complex, matrix will be identiﬁed by the roots of the real, resp.
complex, minimal polynomial of the restriction of the matrix to M.
Deﬁnition 5.2.1. For a real or complex matrix A ∈Fn×n, F = C or F = R,
the (real and complex) roots of the minimal polynomial of A will be called the
C-eigenvalues of A.
They are precisely the real and complex eigenvalues of A, to be distinguished
from quaternion eigenvalues of quaternion matrices that will be introduced later on
in this chapter.
Proposition 5.2.2. Let A ∈Hn×n, and let v1, . . . , vr be a basis for the sum
of root subspaces of A corresponding to a set T ⊆C. Then the columns of the
matrix [χ(v1) χ(v2) . . . χ(vr)] form a basis of the sum of root subspaces of χ(A)
corresponding to the same set T.
Proof. We have
A [v1 v2 . . . vr] = [v1 v2 . . . vr] X,
for some X ∈Hn×n with minimal real polynomial p(X)(x) having all its roots in T.
Applying χ and using the real algebras homomorphism property of χ, we have
χ(A) [χ(v1) χ(v2) . . . χ(vr)] = [χ(v1) χ(v2) . . . χ(vr)] χ(X)
and p(χ(X)) = 0. By Proposition 3.3.3 we have
dimR (column space of [χ(v1) χ(v2) . . . χ(vr)]) = 4r.
(5.2.1)
Write
p(A)(t) = p(t)q(t),
where p(t) and q(t) are relatively prime monic real polynomials. Analogously to
the above, we obtain the following properties:
(a)
dimR (column space of [χ(u1) χ(u2) . . . χ(us)]) = 4s,
(5.2.2)
where u1, . . . , us is a basis for the sum of root subspaces of A corresponding
to the roots of q(t);
(b) the column space of [χ(u1) χ(u2) . . . χ(us)] is χ(A)-invariant;

INVARIANT SUBSPACES AND JORDAN FORM
87
(c) the restriction of χ(A) to this column space is annihilated by q(t).
However, by (5.1.3) we have r + s = n, so 4r + 4s = 4n, the size of χ(A). It now
follows from (5.2.1) and (5.2.2) that the columns of [χ(v1) χ(v2) . . . χ(vr)], resp.
of [χ(u1) χ(u2) . . . χ(us)], actually form a basis in the sum of root subspaces of
χ(A) corresponding to the roots of p(t), resp. of q(t).
□
Corollary 5.2.3. For A ∈Hn×n, the set of the roots of p(A)(t) coincides with
the set of C-eigenvalues of χ(A).
Proof. Proposition 5.2.2 shows that the roots of p(A)(t) are C-eigenvalues of
χ(A). Conversely, by the homomorphism property of χ, we have p(A)(χ(A)) = 0.
Hence, the minimal polynomial of χ(A) divides p(A)(t). Since the C-eigenvalues of
χ(A) are precisely the roots of the minimal polynomial of χ(A), it follows that the
C-eigenvalues of χ(A) are roots of p(A)(t).
□
Proposition 5.2.4. Let A ∈Hn×n, and let v1, . . . , vr be a basis for the sum
of root subspaces of A corresponding to a set T ⊆C. Then the columns of the
matrix [ω(v1) ω(v2) . . . ω(vr)] form a basis of the sum of root subspaces of ω(A)
corresponding to the same set T.
Proof. Note that the set T is necessarily closed under complex conjugation.
The proof is analogous to that of Proposition 5.2.2, using the algebraic properties
of the map ω and Proposition 3.4.2.
□
As an application of the results of this section, we show that sums of root
subspaces behave like Lipschitz functions under changes of the underlying matrix
A. This statement is part of the next theorem. The notation X stands for the
complex conjugate of a set X ⊆C.
Theorem 5.2.5. (a) The roots of the minimal polynomial of a matrix depend
continuously on the matrix:
Fix A ∈Hn×n, and let λ1, . . . , λs be all the distinct roots of p(A)(t) in the closed
upper complex half-plane C+. Then for every ϵ > 0, there exists δ > 0 such that if
B ∈Hn×n satisﬁes ∥B −A∥< δ, then the roots of p(B)(t) in C+ are contained in
the union
∪s
j=1{z ∈C+ : |z −λj| < ϵ}.
(b) Sums of root subspaces are Lipschitz functions of the underlying matrix, in
the following sense.
Given A ∈Hn×n and λ1, . . . , λs as in part (a), there exist δ0, K0 > 0 such that
for every B ∈Hn×n satisfying ∥B −A∥< δ0, it holds that if T is any nonempty
subset of λ1, . . . , λs and T ′ is the set of all roots of p(B)(t) contained in
U := ∪j∈T {z ∈C+ : |z −λj| < δ0},
then the sum of root subspaces M′ of B corresponding to T ′ ∪T ′ and the sum of
root subspaces M of A corresponding to T ∪T satisfy the inequality
θ(M, M′) ≤K0 ∥B −A∥.

88
CHAPTER 5
Here θ(·, ·) is the gap function.
Proof. Part (a). The corresponding property for real or complex matrices is well
known and follows from the continuous dependence of the roots of the polynomial
equation det (tI −Y ) = 0 (i.e., the eigenvalues of Y ) on its coeﬃcients; here Y ∈
Rn×n or Y ∈Cn×n; see Theorem 5.15.1 in the Appendix. Then (a) follows by using
the map χ to reduce the proof to the real matrices and by taking advantage of
Corollary 5.2.3 and of property (v) of χ.
Part (b). Let v1, . . . , vr be an orthonormal basis for M. By Proposition 5.2.2,
the columns of [χ(v1, ) χ(v2) . . . χ(vr)] form a basis for a sum of root subspaces of
χ(A) corresponding to the eigenvalues in T ∪T. By Proposition 3.3.3, the columns
of [χ(v1) . . . χ(vr)] form an orthonormal basis in
f
M := column space of [χ(v1) χ(v2) . . . χ(vr)] ∈R4n×1.
We now use the corresponding Lipschitz property result for sums of root subspaces
for real matrices (see, e.g., Theorem 15.9.7 in Gohberg et al. [54]). Thus, there
exists δ′
0 > 0 such that every B′ ∈R4n×4n with ∥B′ −χ(A)∥≤δ′
0 has an invariant
subspace f
M′ ⊆R4n×1 such that
∥P f
M −P f
M′∥≤K′
0∥B′ −χ(A)∥,
(5.2.3)
where the positive constant K′
0 is independent of B′.
At this point we need a closer examination of the subspace f
M′. It turns out
that f
M′ is necessarily the sum N ′
U∪U of root subspaces of B′ corresponding to the
eigenvalues contained in U ∪U, at least for δ′
0 suﬃciently small. Let us provide a
proof of this statement. Denoting by N ′ ⊆R4n×1 the sum of root subspaces of B′
other than those in U ∪U and arguing by contradiction, suppose that f
M′∩N ′ ̸= {0}
for some B′ arbitrarily close to χ(A). Select a unit length eigenvector w′ ∈f
M′ ∩N ′
of B′ corresponding to a real eigenvalue µ′, or a pair of vectors u′, v′ ∈f
M′ ∩N ′
such that ∥u′∥2 + ∥v′∥2 = 1 and
B′
 u′
v′

=

µ′
ν′
−ν′
µ′
  u′
v′

,
where µ′, ν′ ∈R,
ν′ ̸= 0.
(The pair (u′, v′) correspond to nonreal eigenvalues µ′ ± iν′.) Passing to a limit
when B′
−→χ(A), in view of compactness of the unit sphere in R4n×1, we see
that partial limits of the eigenvector w′, or of the pairs of vectors (u′, v′) as the case
may be, must exist. Using these partial limits it follows by Theorem 3.9.4 that f
M
contains eigenvectors of A corresponding to eigenvalues not in T ∪T, or f
M contains
pairs of vectors (u, v), u, v ∈R4n×1 such that ∥u∥2 + ∥v∥2 = 1 and
χ(A)

u
v

=

µ
ν
−ν
µ
 
u
v

,
where µ, ν ∈R, ν ̸= 0, and µ±iν ̸∈T ∪T. In both cases we obtain a contradiction to
the statement (veriﬁed above) that f
M is the root subspace of χ(A) corresponding
to the eigenvalues in T ∪T. Thus, we have proved that
f
M′ ⊆N ′
U∪U.
(5.2.4)

INVARIANT SUBSPACES AND JORDAN FORM
89
The converse inclusion follows from the equality of dimensions
dimR f
M′ = dimR N ′
U∪U.
(5.2.5)
Indeed, by Theorem 5.15.1 (applied to the characteristic polynomial of χ(A)), the
sum of algebraic multiplicities of eigenvalues of χ(A) in T ∪T coincides with the
sum of those of B′ contained in T ′ ∪T ′ (if δ′
0 is suﬃciently small). Thus,
dimR f
M = dimR f
M′ ≤dimR N ′
U∪U = dimR f
M,
where the ﬁrst equality follows by Theorem 3.9.3, the inequality follows by (5.2.4),
and the latter equality follows from the remark immediately above. Thus, (5.2.5)
holds true, and then equality in (5.2.4) holds true as well.
Let δ0 = cn,nδ′
0, and let B ∈Hn×n be such that ∥B −A∥≤δ0. Then ∥χ(B) −
χ(A)∥≤δ′
0, and applying (5.2.3) with B′ = χ(B), we have
∥P f
M −P f
M′∥≤K′
0(cn,n)−1∥B −A∥.
(5.2.6)
Since f
M′ is a sum of root subspaces of χ(B), by choosing an orthonormal basis
u1, . . . , ur in the sum M′ of the root subspaces of B corresponding to the eigenvalues
in U ∪U, we have that the columns of [χ(u1) . . . χ(ur)] form an orthonormal basis
in f
M′. Thus, using formula (3.9.1), we obtain:
P f
M −P f
M′
=
[χ(v1) χ(v2) . . . χ(vr)][χ(v1) χ(v2) . . . χ(vr)]T
−[χ(u1) χ(u2) . . . χ(ur)][χ(u1) χ(u2) . . . χ(ur)]T
=
χ ([v1 v2 . . . vr][v1 v2 . . . vr]∗
−[u1 u2 . . . ur][u1 u2 . . . ur]∗)
=
χ(PM −PM′).
Therefore, ∥PM −PM′∥≤Cn,n∥P f
M −P f
M′∥, and, combining this inequality with
(5.2.6), the result follows.
□
Note that the degree of p(A)(t) need not be constant in a neighborhood of a given
A ∈Hn×n: if a ∈R, then p(a)(t) = t −a, whereas p(a+ib)(t) = t2 −2at + a2 + b2 for
b ∈R \ {0}.
We mention that sums of root subspaces of a given matrix A ∈Hn×n are isolated
(in the sense of the gap metric) in the set of all invariant subspaces of A. Indeed,
suppose not. Then there is M, the sum of root subspaces of A corresponding to
a set T ∪T of roots of p(A)(t), such that some sequence {Nm}∞
m=1 of distinct A-
invariant subspaces converges to M. Passing to a subsequence if necessary, we may
assume that none of the Nm’s is a sum of root subspaces for A and that in the
decompositions (5.1.4) we have
Nm = (Nm ∩M1) ˙+ · · · ˙+ (Nm ∩Mk),
m = 1, 2, . . . ,
with
{0} ̸= Nm ∩Mj0 ̸= Mj0,
for m = 1, 2, . . . ,
(5.2.7)
where the index j0 is independent of m. Passing again to a subsequence of {Nm}∞
m=1
if necessary, we may (and do) further assume that for a given index j ∈{1, 2, . . . , k},
either Nm ∩Mj = {0} for all m or Nm ∩Mj ̸= {0} for all m. If j is such that
Nm ∩Mj ̸= {0}, select xm ∈Nm ∩Mj, ∥xm∥= 1, and assume (without loss of

90
CHAPTER 5
generality) that the sequence {xm}∞
m=1 converges to some x ∈Hn×1. By Theorem
3.9.4, x ∈M∩Mj, and since M is a sum of root subspaces, we must have Mj ⊆M.
Setting K to be the set of all indices j such that Nm ∩Mj ̸= {0}, we now have
dim Nm
=
X
j∈K
dim (Nm ∩Mj) ≤
X
j∈K
dim Mj
=
dim

X
j∈K
Mj

≤dim M.
(5.2.8)
But by Theorem 3.9.3, dim Nm = dim M, at least for m large enough.
Thus,
equality prevails in (5.2.8), a contradiction with (5.2.7).
5.3
EIGENVALUES AND EIGENVECTORS
Deﬁnition 5.3.1. Let A ∈Hn×n. A vector v ∈Hn×1 \ {0} is said to be a right
eigenvector of A corresponding to the right eigenvalue α ∈H if the equality
Av = vα
(5.3.1)
holds.
The set of all right eigenvalues of A is denoted σ(A), the spectrum of A. Note
that σ(A) is closed under similarity of quaternions: if (5.3.1) holds, then
A(vλ) = (vλ)(λ−1αλ),
for all λ ∈H \ {0},
(5.3.2)
so vλ is a right eigenvector of A corresponding to the right eigenvalue λ−1αλ.
We introduce also the notion of left eigenvectors/eigenvalues.
Deﬁnition 5.3.2. A vector u ∈Hn×1 \ {0} is said to be a left eigenvector of A
corresponding to the left eigenvalue α ∈H if
Au = αu.
(5.3.3)
There are no obvious general connections between left and right eigenvalues or
left and right eigenvectors.
Example 5.3.3. Let
A =
 0
i
j
0

.
The equation for left eigenvalues λ of A boils down to the quadratic equation i =
−λjλ. A calculation shows that this equation has exactly two solutions (±1/
√
2) (i+
j). On the other hand, A4 + I = 0; therefore, the right eigenvalues µ of A must
satisfy
µ4 + 1 = 0.
(5.3.4)
In fact,
σ(A) = {µ ∈H : µ4 + 1 = 0}.
Letting
µ = µ0 + µ1i + µ2j + µ3k,
with µ0, µ1, µ2, µ3 ∈R,
(5.3.5)

INVARIANT SUBSPACES AND JORDAN FORM
91
equation (5.3.4) is satisﬁed if and only if ν2 + 1 = 0, where ν = µ2, i.e.,
R(µ2) = 0,
|V(µ2)| = 1.
(5.3.6)
Now (5.3.6) holds if and only if
µ2
0 = µ2
1 + µ2
2 + µ2
3 = 1
2.
(5.3.7)
Thus the right eigenvalues of A are given by (5.3.5) and (5.3.7). In particular, σ(A)
does not contain any left eigenvalues of A.
□
However, real right eigenvalues are also left eigenvalues (with left and right
eigenvectors being the same), and vice versa.
Proposition 5.3.4. For A ∈Hn×n the following statements are equivalent:
(1) A is not invertible;
(2) zero is a left eigenvalue of A;
(3) zero is a right eigenvalue of A.
Proof. Each of (2) and (3) amounts to the statement that Av = 0 for some
v ∈Hn×1 \ {0}. This is equivalent to noninvertibility of A, by Ex. 3.11.7.
□
In the sequel we will be interested in right eigenvalues and right eigenvectors,
so we will use the terminology eigenvalues and eigenvectors having in mind right
eigenvalues and right eigenvectors, respectively.
First, we establish the existence theorem.
Theorem 5.3.5. For every A ∈Hn×n, there exists an eigenvalue which is a
complex number with nonnegative imaginary part.
Proof. Write A ∈Hn×n and u ∈Hn×1 in the form
A = A1 + A2j,
u = u1 + u2j,
where A1, A2 ∈Cn×n,
u1, u2 ∈Cn×1.
A straightforward calculation shows that the following three equations are equiva-
lent, where λ is a complex number:
Au
=
uλ;
(5.3.8)

A1
A2
−A2
A1
 
u1
−u2

=
λ

u1
−u2

;
(5.3.9)

A1
A2
−A2
A1
 
u2
u1

=
λ

u2
u1

.
(5.3.10)
But (5.3.9) has a solution for some λ with

u1
−u2

̸= 0 because every complex
matrix has a complex eigenvalue (as follows from the Fundamental Theorem of
Algebra); hence, (5.3.10) and (5.3.8) hold as well for these values of λ, u1, u2. But
if λ ∈C is an eigenvalue of A, then so is λ. Thus, λ and λ are eigenvalues of A for
some λ ∈C.
□

92
CHAPTER 5
We mention in passing that every A ∈Hn×n has a left eigenvalue. The proof
of this fact is based on topological considerations and is beyond the scope of this
book; see Wood [163] and Zhang [164] for more details.
For a matrix A = [aij]n
i,j=1 ∈Hn×n, denote the diagonal of A by
D(A) = {λ ∈H : λ = aii for some i,
i = 1, 2, . . . , n}.
Note that in the case A is triangular, D(A) does not generally coincide with σ(A).
For example, if A = i, then D(A) = {i}, but
σ(A) = {bi + cj + dk : b, c, d ∈R and b2 + c2 + d2 = 1}.
Theorem 5.3.5 yields, in the usual manner (well known in complex linear al-
gebra), results on triangulation and diagonalization by unitary similarity, as the
next theorem shows. Observe that if A ∈Hn×n is hermitian, resp. skewhermitian,
unitary, or normal, then for any unitary U ∈Hn×n, the matrix U ∗AU is hermitian,
resp. skewhermitian, unitary, or normal, as the case may be.
Theorem 5.3.6. Let A ∈Hn×n. Then:
(a) (Schur’s triangularization theorem) there exists a unitary U ∈Hn×n such that
U ∗AU is upper triangular and the diagonal D(A) is complex;
(b) there exists a unitary U ∈Hn×n such that U ∗AU is lower triangular and the
diagonal D(A) is complex;
(c) if A is hermitian, then there exists a unitary U ∈Hn×n such that U ∗AU is
diagonal and real;
(d) if A is skewhermitian, then there exists a unitary U ∈Hn×n such that U ∗AU
is diagonal and D(A) ⊂iR;
(e) if A is unitary, then there exists a unitary U ∈Hn×n such that U ∗AU is
diagonal and consists of unit complex numbers;
(f) if A is normal, then there exists a unitary U ∈Hn×n such that U ∗AU is
diagonal and complex.
Proof. Part (a). Use induction on n, the case n = 1 being trivial. By Theorem
5.3.5, we ﬁnd u ∈Hn×1, ∥u∥= 1, such that Au = uλ for some complex λ. Let
U ∈Hn×n be a unitary matrix whose ﬁrst column is u. (To construct such U, let
{u1, . . . , un} be a basis for Hn×1 in which u1 = u, and apply the Gram-Schmidt
procedure to the basis {u1, . . . , un}.) We then have
AU = U

λ
A12
0
A22

,
(5.3.11)
for some A1,2 ∈H1×(n−1), A22 ∈H(n−1)×(n−1).
Rewrite (5.3.11) in the form
U ∗AU =

λ
A12
0
A22

, and use the induction hypothesis to complete the proof
of Part (a).
For Part (b), apply the result of Part (a) to the adjoint matrix A∗, and take
adjoints in the resulting equality U ∗A∗U = T, where T is upper triangular.

INVARIANT SUBSPACES AND JORDAN FORM
93
For Parts (c), (d), (e), and (f), note that an upper (or lower) triangular matrix
is normal only if it is diagonal. Indeed, suppose A = [ai,j]n
i,j=1 ∈Hn×n, where
ai,j ∈H with ai,j = 0 for i > j. Equating the (i, i) entry in A∗A with the (i, i)
entry in AA∗, for i = 1, 2, . . . , n, leads to the equalities (in writing down these
equalities we took advantage of the property that x∗x = xx∗for all x ∈H):
a∗
1,1a1,1
=
a∗
1,1a1,1 + a∗
1,2a1,2 + · · · + a∗
1,na1,n;
(5.3.12)
a∗
1,2a1,2 + a∗
2,2a2,2
=
a∗
2,2a2,2 + a∗
2,3a2,3 + · · · + a∗
2,na2,n;
(5.3.13)
etc.;
a∗
1,na1,n + a∗
2,na2,n + · · · + a∗
n,nan,n = a∗
n,nan,n.
(5.3.14)
It is easy to see that we must have ai,j = 0 for all i < j. Since hermitian, ske-
whermitian, and unitary matrices are, in particular, normal, the diagonal forms in
(c), (d), (e), and (f) follow from the triangular form of (a) (or of (b)). Finally,
observe that in the hermitian case the diagonal entries must be real, as dictated
by the hermitian property of the matrix; similarly, the nature of diagonal entries is
established in (c) and (d).
□
We have already seen above that, for triangular matrices, the spectrum of the
matrix does not necessarily coincide with the diagonal.
The exact relationship
between the spectrum and the diagonal is described next.
Proposition 5.3.7. If A ∈Hn×n is (upper or lower) triangular, then
σ(A) = {α−1βα : α ∈H \ {0}, β ∈D(A)}.
(5.3.15)
Proof. First, we prove the ⊇part of (5.3.15). We use induction on n, the case
n = 1 being trivial. Suppose A = [aij]n
i,j=1 is upper triangular, so aij = 0 if i > j.
By the induction hypotheses (applied to the (n−1)×(n−1) matrix A′ := [aij]n−1
i,j=1,
noting that the eigenvectors of A′ can be trivially extended to eigenvectors of A,
with the same eigenvalues), we have
σ(A) ⊇{α−1ajjα : α ∈H \ {0},
j = 1, 2, . . . , n −1}.
If R(ann) = R(aj,j) and |V(ann)| = |V(aj,j)| for some j = 1, 2, . . . , n −1, then also
ann ∈σ(A), and we are done. Otherwise, the equation
an−1,n−1x1 + an−1,n = x1ann
has a (unique) solution x1 ∈H, by Theorem 2.3.3(2). Again by Theorem 2.3.3(2),
let x2 be a (unique) solution of
an−2,n−2x2 + an−2,n−1x1 + an−2,n = x2ann,
x2 ∈H.
Continuing this way, we obtain x1, . . . xn−1 ∈H. From the construction it follows
that


xn−1
...
x1
1

is an eigenvector of A corresponding to the eigenvalue ann. This
proves the ⊇part of (5.3.15).

94
CHAPTER 5
Conversely, let v be an eigenvector of A corresponding to the eigenvalue γ. If
v = [vj]n
j=1 and j0 is the largest index such that vj0 ̸= 0, then aj0,j0vj0 = vj0γ, and
γ = v−1
j0 aj0,j0vj0 belongs to the right-hand side of (5.3.15).
□
It follows that for a triangular matrix A ∈Hn×n, the following three statements
are equivalent: (1) σ(A) is real; (2) D(A) is real; (3) σ(A) = D(A).
In particular (Theorem 5.3.6(c)), for every hermitian matrix H there is a unitary
U such that U ∗HU is diagonal with the eigenvalues (perhaps repeated) of H on the
main diagonal. Comparing with the canonical form of Theorem 4.1.6, the following
characterization of classes of hermitian matrices in terms of eigenvalues is easily
obtained.
Proposition 5.3.8. A hermitian matrix H is positive deﬁnite, resp. positive
semideﬁnite, negative deﬁnite, or negative semideﬁnite, if and only if all its eigen-
values are positive, resp. nonnegative, negative, or nonpositive.
We conclude this section with a statement on linear independence of eigenvectors
corresponding to nonsimilar eigenvalues.
Proposition 5.3.9. Let v1, . . . , vp be eigenvectors of a matrix A ∈Hn×n that
correspond to eigenvalues α1, . . . , , αp, respectively, and assume that α1, . . . , αp are
pairwise nonsimilar (over H). Then v1, . . . , vp are linearly independent.
Proof. Without loss of generality, we may assume that α1, . . . , αp are com-
plex numbers with nonnegative imaginary parts (see (5.3.2)).
In this case, the
condition of pairwise nonsimilarity simply means that α1, . . . , αp are all distinct.
Suppose Pp
j=1 vjλj = 0 for some λ1, . . . , λp ∈H. Let q(t) be a polynomial with
real coeﬃcients such that q(α1) ̸= 0 but q(αj) = 0 for j = 2, 3, . . . , p. Then
0 = q(A)
p
X
j=1
vjλj =
p
X
j=1
vjq(αj)λj = v1q(α1)λ1,
and in view of v1 ̸= 0, q(α1) ̸= 0, we must have λ1 = 0. Analogously, the equalities
λ2 = · · · = λp = 0 are proved.
□
5.4
SOME PROPERTIES OF JORDAN BLOCKS
Jordan blocks are matrices of type
Jm(λ) =


λ
1
0
· · ·
0
0
λ
1
· · ·
0
...
...
...
...
0
...
...
λ
1
0
0
· · ·
0
λ


∈Hm×m,
λ ∈H.
Later in the text we will frequently work with Jordan blocks and matrices that are
derived from Jordan blocks. The next two propositions are very useful tools in this
work.

INVARIANT SUBSPACES AND JORDAN FORM
95
Proposition 5.4.1. If α ∈H, β ∈H are not similar, then the equation
Jm(α)Y = Y Jp(β),
Y ∈Hm×p,
(5.4.1)
has only the trivial solution Y = 0.
Proof. Write
Y =


y1,1
y1,2
. . .
y1,p
y2,1
y2,2
. . .
y2,p
...
...
. . .
...
ym,1
ym,2
. . .
ym,p

,
yi,j ∈H.
Equating the elements in the bottom row of the left-hand side and the right-hand
side of equation (5.4.1), we obtain
αym,1 = ym,1β,
αym,2 = ym,1 + ym,2β,
. . . ,
αym,p = ym,p−1 + ym,pβ.
Repeatedly using the fact (which follows from nonsimilarity of α and β) that αx =
xβ, x ∈H, is possible only if x = 0, we see that ym,1 = ym,2 = · · · = ym,p = 0. Now
equate the elements in the (m −1)th row of the left-hand side and the right-hand
side of (5.4.1), which results in ym−1,1 = ym−1,2 = · · · = ym−1,p = 0. Continuing
in this fashion, we eventually obtain Y = 0.
□
To formulate the second proposition, we introduce notation for upper triangular
Toeplitz matrices: for α1, . . . , αq ∈H, we let
Toepq (α1, . . . , αq) =


α1
α2
α3
. . .
αq
0
α1
α2
. . .
αq−1
0
0
α1
. . .
αq−2
...
...
...
...
...
0
0
0
. . .
α1


be the upper triangular Toeplitz matrix with α1, . . . , αq on the ﬁrst, second, etc.,
superdiagonal, respectively.
Proposition 5.4.2.
(a) If λ ∈R, then the general solution of the homoge-
neous matrix equation
Jm(λ)X = XJn(λ)
(5.4.2)
with unknown X ∈Hm×n is given by
X =


























0m×(n−m)
Toepm (α1, . . . , αm)

,
α1, . . . αm ∈H
arbitrary
if m ≤n;
 Toepn (α1, . . . , αn)
0n×(m−n)

,
α1, . . . , αn ∈H
arbitrary
if m ≥n.
(5.4.3)

96
CHAPTER 5
(b) If λ ∈H \ R, then the general solution of (5.4.2) is given by
X =


























0m×(n−m)
Toepm (α1, . . . , αm)

,
α1, . . . αm
∈SpanR {1, λ}
arbitrary if m ≤n;
 Toepn (α1, . . . , αn)
0n×(m−n)

,
α1, . . . , αn
∈SpanR {1, λ}
arbitrary if m ≥n.
(5.4.4)
Proof. First consider the case λ ∈R. Then (5.4.2) is equivalent to
Jm(0)X = XJn(0).
(5.4.5)
Write X = [xi,j]m,n
i=1,j=1. Equating the entries of the ﬁrst row and the ﬁrst column
in the left-hand side of (5.4.5) to the corresponding entries in the right-hand side
yields the equalities
x2,1 = x3,1 = · · · = xm,1 = 0
and
xm,1 = xm,2 = · · · = xm,n−1 = 0.
(5.4.6)
Equating other entries on both sides of (5.4.5) also produces
xi+1,j = xi,j+1,
for i = 1, 2, . . . m
and j = 1, 2, . . . , n.
(5.4.7)
Equalities (5.4.6) and (5.4.7) now yield the desired form, (5.4.3).
Now assume λ ∈H\R. Subtracting from λ its real part, we can (and do) assume
without loss of generality, that R(λ) = 0. Let (q1, q2, q3) be a units triple such that
q1 is a scalar multiple of λ: λ = aq1, where a ∈R\{0}. Write X = X′ +X′′, where
X′ ∈SpanR {1, q1} and X′′ ∈SpanR {q2, q3}. Then the multiplication rules within
the units triple force equation (5.4.2) to decompose into two equations:
Jm(λ)X′ = X′Jn(λ),
Jm(λ)X′′ = X′′Jn(λ).
(5.4.8)
Since SpanR {1, q1} is isomorphic to the ﬁeld of complex numbers, the ﬁrst equation
in (5.4.8) is amenable to the standard analysis, as in the proof of Part (a), leading
to the general form of solutions as in (5.4.4).
We will show that the second equation in (5.4.8) has only the trivial solution
X′′ = 0, thereby proving Proposition 5.4.2. Letting X′′ = X2q2 + X3q3, where
X2, X3 are real matrices, we rewrite the second equation in (5.4.8) in the following
form:
Jm(aq1) (X2q2 + X3q3) = (X2q2 + X3q3) Jn(aq1).
(5.4.9)
It will be convenient to denote the matrix Jp(0) by Kp, so Jm(aq1) = aq1Im + Km.
We now compute the left-hand side of (5.4.9):
Jm(aq1) (X2q2 + X3q3)
=
(aq1Im + Km) (X2q2 + X3q3)
=
aq1X2q2 + aq1X3q3 + KmX2q2 + KmX3q3
=
aX2q3 −aX3q2 + KmX2q2 + KmX3q3. (5.4.10)
Analogously,
(X2q2 + X3q3) Jn(aq1) = −aX2q3 + aX3q2 + X2q2Kn + X3q3Kn.
(5.4.11)

INVARIANT SUBSPACES AND JORDAN FORM
97
Equating the right-hand sides of (5.4.10) and (5.4.11), we get
2aX2q3 −2aX3q2 = −KmX2q2 + X2q2Kn −KmX3q3 + X3q3Kn,
or
−2aX3 = −KmX2 + X2Kn,
2aX2 = −KmX3 + X3Kn.
(5.4.12)
Substitute the value of X3 from the ﬁrst equation in (5.4.12) into the second:
X2 =
1
(2a)2 (−K2
mX2 + 2KmX2Kn −X2K2
n).
(5.4.13)
We now repeatedly iterate equality (5.4.13), i.e., substitute for X2 in the right-hand
side of (5.4.13) its value given by (5.4.13). The result is equalities of the form
X2 =
2p
X
j=0
aj,2pKj
mX2K2p−j
n
,
p = 1, 2, . . . ,
for some real numbers aj,2p. Taking p so large that 2p ≥m + n, we see that for
every j = 0, . . . , 2p, at least one of the equalities j ≥m or 2p −j ≥n holds, so
Kj
m = 0 or K2p−j
n
= 0; hence, X2 = 0. But then also X3 = 0, and we are done.
□
We conclude this section with an easy observation concerning the minimal poly-
nomials of Jordan blocks.
Proposition 5.4.3. The minimal polynomial of Jm(λ) is (t −λ)m if λ is real
and (t2 −2R(λ)t + |λ|2)m if λ is nonreal.
Proof. In the case λ is real, we have
(Jm(λ)) −λI)k = Toepm (0, 0, . . . , 0
|
{z
}
k
zeros
, 1, 0, . . . , 0),
k = 1, 2, . . . , m,
(5.4.14)
and the result follows. In the nonreal case, writing λ = R(λ)+qI(λ), where q2 = −1
and I(λ) ∈R \ {0}, a computation shows that
(Jm(λ))2 −2R(λ)Jm(λ) + |λ|2I = 2qI(λ)Jm(0) + (Jm(0))2,
and so
((Jm(λ))2 −2R(λ)Jm(λ) + |λ|2I)k
= (Jm(0))k(2qI(λ)Im + Jm(0))k,
k = 1, 2, . . . , m.
Since the matrix 2qI(λ)Im+Jm(0) is invertible, the result now follows from (5.4.14)
(where we set λ = 0).
□
5.5
JORDAN FORM
In this section we state a key result of quaternion linear algebra—the Jordan canon-
ical form, Theorem 5.5.3 below.
We start with some preliminaries.
Deﬁnition 5.5.1. A matrix A ∈Hn×n is said to be similar to a matrix B ∈
Hn×n if B = S−1AS for some invertible matrix S ∈Hn×n.

98
CHAPTER 5
We check easily that similarity is an equivalence relation, so we can use unam-
biguously the language of similar matrices.
Proposition 5.5.2. If A, B ∈Hn×n are similar, i.e., B = S−1AS for some
invertible S, then:
(1) A and B have the same minimal polynomial;
(2) A and B have the same eigenvalues;
(3) x ∈Hn×1 is an eigenvector of A corresponding to the eigenvalue λ if and only
if S−1x is an eigenvector of B corresponding to the same eigenvalue λ;
(4) M is the root subspace of A corresponding to the eigenvalue λ if and only
if S−1(M) is the root subspace of B corresponding to the same eigenvalue
λ; in particular, the root subspaces of A and B corresponding to the same
eigenvalue, have equal dimensions.
Proof. (1) and (4) follow from the observation that p(B) = S−1p(A)S for every
polynomial p(t) with real coeﬃcients. (2) and (3) follow from the observation that
the eigenvector-eigenvalue equation Ax = xλ for A can be rewritten in the form
(SBS−1)x = xλ, or BS−1x = S−1xλ.
□
Note that the converse statements to Proposition 5.5.2 generally fail; cf. Ex.
5.16.15 and 5.16.16.
We now state our main result in this section.
Theorem 5.5.3.
(a) Let A ∈Hn×n. Then there exists an invertible S ∈Hn×n
such that S−1AS has the form
S−1AS = Jm1(λ1) ⊕· · · ⊕Jmp(λp),
λ1, . . . , λp ∈H,
(5.5.1)
where Jm(λ) is the m × m Jordan block having eigenvalue λ.
The form
(5.5.1) is uniquely determined by A up to an arbitrary permutation of blocks
{Jmj(λj)}p
j=1 and up to a replacement of λ1, . . . , λp with
α−1
1 λ1α1, . . . , α−1
p λpαp
within the blocks Jm1(λ1), . . . , Jmp(λp), respectively, where αj ∈H \ {0}, j =
1, 2, . . . , p, are arbitrary.
(b) For every A ∈Hn×n there is an invertible S ∈Hn×n such that S−1AS has the
form (5.5.1) with λ1, . . . , λp ∈C having nonnegative imaginary parts, and in
such case the form (5.5.1) is unique up to an arbitrary permutation of blocks.
Deﬁnition 5.5.4. The right-hand side of (5.5.1) is called the Jordan form of A.
Thus, the Jordan form represents a canonical form of a quaternion matrix under
similarity.
Part (b) of Theorem 5.5.3 follows immediately from Part (a) upon the obser-
vation that every similarity class {α−1λα : α ∈H \ {0}}, λ ∈H, contains unique
complex number with nonnegative imaginary part (see Theorem 2.2.6).
We postpone the proof of Part (a) of Theorem 5.5.3 to the next section.
Next, we characterize various elements of the Jordan form of a matrix in terms
of the matrix itself.

INVARIANT SUBSPACES AND JORDAN FORM
99
Theorem 5.5.5. Let A ∈Hn×n, and let λ ∈σ(A). Then:
(a) the number of Jordan blocks in the Jordan form of A corresponding to the
eigenvalues similar to λ is equal to the maximal number of H-linearly inde-
pendent eigenvectors corresponding to the eigenvalues similar to λ;
(b) the sum of the sizes of Jordan blocks in the Jordan form of A corresponding
to the eigenvalues similar to λ is equal to the dimension of the root subspace
of A corresponding to λ.
Proof. It suﬃces to prove the theorem for A in the Jordan form (cf. Proposition
5.5.2). Thus, we set
A = J = Jm1(λ1) ⊕· · · ⊕Jmp(λp) ⊕eJ,
(5.5.2)
where λ1, · · · , λp are similar to λ and eJ is the part of the Jordan form of A that
contains Jordan blocks with eigenvalues not similar to λ. Clearly the p eigenvectors
e1, em1+1, em1+m2+1, . . . , em1+m2+···+mp−1+1
(5.5.3)
are H-linearly independent. Now let x be an eigenvector of J corresponding to
an eigenvalue µ which is similar to λ, so µ = α−1
q λqαq, q = 1, 2, . . . , p for some
α1, . . . , αp ∈H \ {0}. We then have
Jmq(λq)xqα−1
q
= xqα−1
q λq,
q = 1, 2, . . . , p,
eJex = exµ,
where x =


x1
x2
...
xp
ex


is the partition of x consistent with (5.5.2). By Propositions
5.4.1 and 5.4.2, we have that ex = 0 and only the ﬁrst component in each xq may
be nonzero. It follows that x is a linear combination of the vectors (5.5.3). Thus,
there cannot be more than p linearly independent eigenvectors corresponding to
eigenvalues similar to λ.
For Part (b) just observe that by Proposition 5.4.3, the Jordan blocks whose
minimal polynomials are powers of t−λ (if λ is real) or powers of t2 −2R(λ)t+|λ|2
(if λ is nonreal) are exactly those that correspond to the eigenvalues similar to
λ.
□
Deﬁnition 5.5.6. The geometric multiplicity of the root subspace M of A ∈
Hn×n corresponding to a polynomial p(t)m, where p(t) is real irreducible, is deﬁned
as the maximal number of H-linearly independent eigenvectors of A in M or, equiv-
alently, by Theorem 5.5.5, the number of Jordan blocks J in the Jordan form of A
having the property that p(D(J)) = 0. We also say that the geometric multiplicity
of an eigenvalue λ ∈σ(A) is, by deﬁnition, the geometric multiplicity of the root
subspace corresponding to the polynomial (a factor of the minimal polynomial of
A) of which λ is a root. The sizes of Jordan blocks that have eigenvalues similar to
λ, each size repeated as many times as there are Jordan blocks of that size, in the
Jordan form of A, are the partial multiplicities corresponding to λ. The algebraic
multiplicity of a root subspace R of A and of any eigenvalue λ corresponding to R
is deﬁned as the (quaternion) dimension of R.

100
CHAPTER 5
By Theorem 5.5.5, the sum of the partial multiplicities corresponding to λ co-
incides with the (quaternion) dimension of the root subspace corresponding to λ.
Clearly, for any given eigenvalue of A, the geometric multiplicity cannot exceed
the algebraic multiplicity. It is easy to see that similar eigenvalues have the same
geometric multiplicity and the same algebraic multiplicity.
Note that the set of eigenvectors of A in a root subspace corresponding to
nonreal eigenvalues, together with the zero vector, is not necessarily a subspace of
Hn×1. For example, let A =
 i
0
0
−i

. Clearly, H2×1 is the (sole) root subspace of
A, and
 1
0

and
 0
1

are eigenvectors. If the set of eigenvectors of A, together
with zero, were a subspace of H2×1, then every nonzero vector in H2×1 would be
an eigenvector. However,
 a
b

, a ∈H \ {0}, b ∈H, is an eigenvector of A if and
only −i(ba−1) = (ba−1)i. We have the following general statement.
Theorem 5.5.7. Let R be a root subspace for A ∈Hn×n corresponding to a
power of a real irreducible polynomial p(x). Then the set of eigenvectors in R,
together with the zero vector, form a subspace of Hn×n if and only if p(x) is linear:
p(x) = x −a for some a ∈R, or p(x) is quadratic and A has only one eigenvector
(up to scaling) in R.
Proof. The “if” part is clear. For the “only if” part, let v1 and v2 be two
linearly independent eigenvectors of A corresponding to eigenvalue λ, where λ is
nonreal. Then v1 + v2α, α ∈H, is an eigenvector of A if and only if λα = αλ, and
since not all α ∈H satisfy this condition, the set of eigenvectors together with zero
is not a subspace of Hn×1.
□
As an easy consequence of Theorem 5.5.3, we identify the minimal polynomial
of a quaternion matrix.
Theorem 5.5.8. Let A ∈Hn×n. Assume that β1, . . . , βu are the distinct real
eigenvalues of A (if any) and βu+1, . . . , βu+v ∈H is a maximal set of nonreal
pairwise nonsimilar eigenvalues of A (if any). Furthermore, in a Jordan form of
A, let mj be the largest size of Jordan blocks corresponding to the eigenvalue βj,
j = 1, 2, . . . , u + v. Then the minimal polynomial of A is given by
(t −β1)m1 · · · (t −βu)mu
v
Y
j=1
(t2 −2R(βu+j)t + |βu+j|2)mu+j.
Indeed, A and its Jordan form have the same minimal polynomial. For a proof
of Theorem 5.5.8 for the Jordan form, use Proposition 5.4.3.
Combining Theorem 5.5.8 with the result on continuous dependence of the roots
of minimal polynomial (Theorem 5.2.5), it follows that the eigenvalues are also
continuous functions of the matrix.
Theorem 5.5.9. Let A ∈Hn×n, and let λ1, . . . , λs be all the distinct eigenvalues
of A in the closed upper complex half-plane C+. Then for every ϵ > 0, there exists
δ > 0 such that if B ∈Hn×n satisﬁes ∥B −A∥< δ; then the eigenvalues of B are
contained in the union
∪s
j=1{z ∈C+ : |z −λj| < ϵ}.

INVARIANT SUBSPACES AND JORDAN FORM
101
Moreover, if ϵ is suﬃciently small, then the sum of the algebraic multiplicities of B
at eigenvalues in {z ∈C+ : |z −λj| < ϵ} is equal to the algebraic multiplicity of A
at λj, for j = 1, 2, . . . , s.
Proof. To obtain the statement concerning algebraic multiplicities, we apply
the map ω to the equality A = S−1KS, where S is invertible and K is a Jordan form
with complex numbers with nonnegative imaginary parts on the diagonal. Since
ω is continuous, matrices that are close to A are transformed by ω to matrices as
close to (ω(S))−1ω(K)ω(S) as we wish. Now use Theorem 5.15.1 in the Appendix
applied to p(z) = det (zI −ω(K)) (the characteristic polynomial of ω(K)).
□
The Jordan form allows one to extend the deﬁnition of functions of quaternion
matrices beyond polynomials with real coeﬃcients by analogy with the standard
concept of functions of complex matrices (see, e.g., Gantmacher [50], Lancaster and
Tismenetsky [94], or Horn and Johnson [63]).
Theorem 5.5.10. (a) Let
f(t) =
∞
X
j=0
fj(t −t0)j
be a power series with real coeﬃcients fj centered at t0 ∈R and having nonzero
radius of convergence d. Then the series
∞
X
j=0
fj(A −t0I)j
(5.5.4)
converges for any matrix A ∈Hn×n such that
(R(λ) −t0)2 + |V(λ)|2 < d2
for all eigenvalues λ of A.
(b) Denoting the sum in (5.5.4) by f(A) and letting (5.5.1) be the Jordan form
of A, with λ1, . . . , λp ∈C having nonnegative imaginary parts, we have
f(A) = S
 f(Jm1(λ1)) ⊕· · · ⊕f(Jmp(λp))

S−1,
where each f(Jmj(λj)) is found by the formula
f(Jm(λ)) =


f(λ)
f ′(λ)
f ′′(λ)
2!
. . .
f (m−1)(λ)
(m −1)!
0
f(λ)
f ′(λ)
. . .
f (m−2)(λ)
(m −2)!
...
...
...
...
...
0
0
0
. . .
f(λ)


.
(5.5.5)
Note that the algebraic properties expressed in (5.1.1) extend to nonpolynomial
functions p and q given by power series, as in the right-hand side of (5.5.4), provided
the spectrum of A is contained in the intersection of the disks of convergence of
p and q. This can be easily veriﬁed using reduction to the Jordan form of A by
similarity.
For example, the function log(I + A) is well deﬁned as above for all A ∈Hn×n
satisfying |λ| < 1 for every λ ∈σ(A).

102
CHAPTER 5
Note that algebraic identities with real coeﬃcients involving real power series
carry over to the matrix case; this fact can be veriﬁed again using Theorem 5.5.3
and (5.1.1). So we have (sin A)2 + (cos A)2 = I for all A ∈Hn×n. In contrast, eiA
is generally not equal to cos A + i sin A. For example,
cos 1 + k sin 1 = ek = eij ̸= cos j + i sin j = 1
2(e + e−1) + ij
1
2(e −e−1)

.
We conclude this section with a convenient characterization of uniqueness of
invariant subspaces of ﬁxed dimension.
Theorem 5.5.11. For a ﬁxed k, 1 ≤k ≤n −1, and a ﬁxed matrix A ∈Hn×n,
the following statements are equivalent:
(a) A has a unique k-dimensional invariant subspace;
(b) A has only one eigenvector (up to scaling);
(c) the Jordan form of A has one Jordan block only.
Proof.
(b)
=⇒
(c) is obvious.
If (c) holds, then setting up the eigen-
value/eigenvector equation Ax = xλ, x ̸= 0, λ ∈H, and (without loss of generality)
letting x be of the form x = col (xi−1, x1, . . . , 1, 0, . . . , 0), with 1 on the ith position,
we obtain that D(A) = {λ} on the main diagonal and that λx1 + 1 = x1λ. The
latter equation is contradictory (Corollary 2.2.3) unless i = 1, i.e., x = e1, and (b)
follows.
Suppose (c) does not hold. We may (and do) assume that A is in the Jordan
form and D(A) ⊂C. At this point we use known results on the structure of complex
invariant subspaces of complex matrices; see, e.g., Gohberg et al. [54, Chapter 14]
or Shayman [144] for more detailed analytic structure. It follows that A has either a
continuum of complex k-dimensional invariant subspaces (in the case the geometric
multiplicity of some eigenvalue of A is larger than one) or a ﬁnite number, but at
least two, complex k-dimensional invariant subspaces (in the case the geometric
multiplicity of every eigenvalue is equal to one). So (a) does not hold.
Finally, if (c) holds and if M is a k-dimensional A-invariant subspace, then
(assuming A = Jn(λ) for some λ ∈H) M cannot contain any vector x = [xi]n
i=1 ∈
Hn×1 having at least one nonzero component xi with i > k; indeed, if M were to
contain such a vector x = [xi]n
i=1 ∈Hn×1, with xi0 ̸= 0, xi = 0 for i = i0 + 1, . . . , n,
and i0 > k, then by scaling x and applying A repeatedly, one easily sees that M
contains a linearly independent set of i0 vectors of the form
col (∗, ∗, . . . , 1, 0, . . . , 0),
col (∗, . . . , 1, 0, . . . , 0), . . . , col (1, 0, . . . , 0),
with the 1 appearing in the i0th, (i0 −1)th, etc., 1st position, respectively, a
contradiction to M having dimension k. It follows that M = SpanH {e1, . . . , ek},
hence, unique.
□
5.6
PROOF OF THEOREM 5.5.3
For the proof we need a slightly modiﬁed complex matrix representation of quater-
nions. Write a matrix A ∈Hn×n in the form
A = A1 + jA2,
A1, A2 ∈Cn×n,

INVARIANT SUBSPACES AND JORDAN FORM
103
and deﬁne the map eωn : Hn×n →C2n×2n by
eωn(A) =

A1
A2
−A2
A1

.
(5.6.1)
It is easy to see that the map eωn diﬀers from the map ωn,n introduced in Section
3.4 only by a simultaneous permutation of rows and columns:
eωn(A) = P −1
n (ωn,n(A))Pn,
(5.6.2)
where Pn is ﬁxed, i.e., independent of A ∈Hn×n but dependent on n, permutation
matrix. For example, if
A =
 a0 + ia1 + ja2 + ka3
b0 + ib1 + jb2 + kb3
c0 + ic1 + jc2 + kc3
d0 + id1 + jd2 + kd3

∈H2×2,
where aj, bj, cj, dj ∈R, then
ω2,2(A) =


a0 + ia1
a2 + ia3
b0 + ib1
b2 + ib3
−a2 + ia3
a0 −ia1
−b2 + ib3
b0 −ib1
c0 + ic1
c2 + ic3
d0 + id1
d2 + id3
−c2 + ic3
c0 −ic1
−d2 + id3
d0 −id1


and
eω2(A) =


a0 + ia1
b0 + ib1
a2 + ia3
b2 + ib3
c0 + ic1
d0 + id1
c2 + ic3
d2 + id3
−a2 + ia3
−b2 + ib3
a0 −ia1
b0 −ib1
−c2 + ic3
−d2 + id3
c0 −ic1
d0 −id1

,
so
P2 =


1
0
0
0
0
0
1
0
0
1
0
0
0
0
0
1

.
In general, Pn is the 2n × 2n matrix having 1s in the positions
(1, 1), (2, n + 1), (3, 2), (4, n + 2), . . . , (2n −1, n), (2n, 2n),
and zeros elsewhere.
Because of (5.6.2), the algebraic properties of ωn,n listed in Section 3.4 remain
valid for eωn; property (i) takes the following form.
(i′) eωn is an isomorphism of the real algebra Hn×n onto the real unital subalgebra
eΩ2n :=

A1
A2
−A2
A1

: A1, A2 ∈Cn×n

(5.6.3)
of C2n×2n, and eωn(I) = I.
We can also check directly that eΩ2n is a real unital subalgebra of C2n×2n (Ex.
5.16.13).
We will show that for every matrix B in eΩ2n, there exists an invertible matrix
T and a matrix in a (complex) Jordan form J in the same subalgebra such that
J = T −1BT; in other words, we will prove the following claim.

104
CHAPTER 5
Claim 5.6.1. For every B =

B1
B2
−B2
B1

, where B1, B2 ∈Cn×n, there exist
an invertible matrix T of the form T =

T1
T2
−T2
T1

, where T1, T2 ∈Cn×n, and a
Jordan form J1 ∈Cn×n such that the equality
 J1
0
0
J1

= T −1BT
(5.6.4)
holds.
The Jordan form J1 can be chosen to have all eigenvalues with nonnegative
imaginary parts.
Once Claim 5.6.1 is proved, the existence part of Theorem 5.5.3 follows easily.
Indeed, observe ﬁrst that if T ∈eΩ2n is invertible, then T −1 also belongs to eΩ2n.
That is because f(T) = 0, where f(t) is the minimal polynomial for T, and, there-
fore, T −1 can be expressed as a polynomial of T. Now, for a given A ∈Hn×n, we
let B = eωn(A) and apply the inverse map (eωn)−1 to equality (5.6.4). The following
equation results:
J1 = (eωn)−1(T −1)A(T1 + jT2) = (T1 + jT2)−1A(T1 + jT2),
and the existence of a Jordan form of any square-size quaternion matrix follows.
Proof of Claim 5.6.1. Denote
A :=

A1
A2
−A2
A1

,
A1, A2 ∈Cn×n.
(5.6.5)
Since A is a complex matrix, there exists a nonsingular matrix P such that P −1AP
= J is a Jordan form of A.
Step 1. Let α1, . . . , αm ∈C be the distinct eigenvalues of A. Then each column
of P is a column vector v of 2n components satisfying one and only one of the
following two relations:
(i) Av = vαj,
(ii) Av = w + vαj,
where w is the column of P adjacent to v on the left. All 2n columns of P are
linearly independent, and for each αj there is at least one column of type (i).
Step 2. Deﬁne the column vector v⋆relative to v ∈C2n×1 as follows: if
vT = [v1,1 v2,1
. . .
vn,1 w1,1 w2,1
. . .
wn,1],
where vk,1, wk,1 ∈C for k = 1, 2, . . . , n, then
v⋆= [−w1,1
−w2,1
. . .
−wn,1 v1,1 v2,1
. . .
vn,1]T .
If v is not the zero vector, then v and v⋆are linearly independent, because if
c1v + c2v⋆= 0, where c1, c2 ∈C \ {0}, then it follows that
(c1c1 + c2c2)wk,1 = 0,
(c1c1 + c2c2)vk,1 = 0,
for k = 1, 2, . . . , n,

INVARIANT SUBSPACES AND JORDAN FORM
105
so c1 = c2 = 0, a contradiction. Notice the following properties of the ⋆operation:
Av⋆= (Av)⋆;
if Av = vα,
then Av⋆= v⋆α.
(5.6.6)
Note also that (v⋆)⋆= −v and
(c1w1 + c2w2 + · · · + ckwk)⋆= c1w⋆
1 + c2w⋆
2 + · · · + ckw⋆
k,
where w1, . . . , wk ∈C2n×1, c1, . . . , ck ∈C.
Now let α be an eigenvalue of A, and suppose ﬁrst that α is real.
If v1 is
an eigenvector of A corresponding to α, then v1 and v⋆
1 are linearly independent
vectors of type (i) for α and either exhaust the number of such linearly independent
column vectors or else there exists another, say, v2, which is linearly independent
of v1 and v⋆
1 and satisﬁes Av2 = v2α. Then v1, v⋆
1, v2, v⋆
2 are linearly independent.
Indeed, suppose
c1v1 + c2v⋆
1 + c3v2 + c4v⋆
2 = 0,
c1, c2, c3, c4 ∈C.
(5.6.7)
We may assume c3, c4 are both nonzero (if, say, c3 = 0, then (5.6.7) yields
c1v⋆
1 −c2v1 −c4v2 = 0,
and we must have c1 = c2 = c4 = 0). Together with (5.6.7) we also have
c1v⋆
1 −c2v1 + c3v⋆
2 −c4v2 = 0.
Solving this equation for v⋆
2 and substituting in (5.6.7) gives
(c31 + c4c2)v1 + (c3c2 −c4c1)v⋆
1 + (c3c3 + c4c4)v2 = 0,
so c3 = c4 = 0, a contradiction. Either v1, v⋆
1, v2, v⋆
2 exhaust the number of linearly
independent vectors of type (i) for α, or they do not. In the latter case, we continue
this process and eventually obtain a set of linearly independent vectors of the form
v1, v⋆
1, . . . , vk, v⋆
k, which provide a basis for the vectors of type (i) corresponding to
α. Note that 2k is also the number of columns in P that are eigenvectors of A
corresponding to α; in particular, the number of such columns is even.
Suppose now α is complex nonreal. Then, if the matrix P contains a set of
column vectors v1, v2, . . . , vt such that Avj = vjα (j = 1, 2, . . . , t), then Av∗
j = v∗
j α
(j = 1, 2, . . . , t). So α is also an eigenvalue of A, and a basis of the set of vectors of
type (i) for α can be taken in the form v⋆
1, . . . , v⋆
t , where v1, . . . , vt is a basis of the
set of vectors of type (i) for α.
Step 3. Consider vectors v of type (ii): (A −αjI)v = w, which implies (A −
αjI)v⋆= w⋆.
Writing α for αj for brevity, assume ﬁrst that α is real. By Step 2, there is a
basis B := {v1, v2, . . . , v2k} for
KerC (A −αI) := {x ∈C2n×1 : (A −αI)x = 0}
such that v ∈B ⇒v⋆∈B, and for every pair of vectors v, v⋆in B at least one of
them is a column of P. Let v(1)
1 , . . . , v(1)
p
be the columns of P such that
(A −αI)v(1)
j
̸= 0,
(A −αI)2v(1)
j
= 0,
for j = 1, 2, . . . , p.
(5.6.8)

106
CHAPTER 5
If (5.6.8) holds, then we say that the v′
j’s are the (ﬁrst) generalized eigenvectors
associated with the eigenvectors (A −αI)v(1)
j . Clearly the vectors
v1, . . . , v2k, v(1)
1 , . . . , v(1)
p
form a basis for KerC (A −αI)2.
Consider v(1)
1 . Since v1, . . . , v2k, v(1)
1
are linearly independent, so are
v1, . . . , v2k, v(1)
1 , (v(1)
1 )⋆
(5.6.9)
(see the argument in Step 2, the case of real α).
But (v(1)
1 )⋆is a generalized
eigenvector associated with the eigenvector (A −αI) (v(1)
1 )⋆.
We now replace a
suitable pair of vectors vj, v⋆
j in B with
(A −αI) v(1)
1 , (A −αI) (v(1)
1 )⋆= ((A −αI) v(1)
1 )⋆,
so that the new set of vectors is again a basis for KerC (A −αI). For simplicity of
notation, we again denote the new set of vectors by v1, . . . , v2k.
If (5.6.9) (with the new set v1, . . . , v2k) is a basis for KerC (A −αI)2, we stop.
Otherwise, there is v(1)
j , which is linearly independent of (5.6.9), and we repeat the
procedure in the preceding paragraph. Continuing this way we eventually obtain a
basis in KerC (A −αI)2 of the following form (rearranging the vectors v1, . . . , v2k if
necessary):
B1
:=
{v(1)
1 , (v(1)
1 )⋆, . . . , v(q)
1 , (v(q)
1 )⋆,
v1, v2 = v⋆
1, v3, v4 = v⋆
3, . . . , v2q−1, v2q = v⋆
2q−1, . . . , v2k},
for some integer q ≤k, where
(A −αI)v(j)
1
= v2j−1,
for j = 1, 2, . . . , q.
In particular, the dimension of KerC (A −αI)2 is even.
If
KerC (A −αI)2 ̸= KerC (A −αI)3,
then we continue this process, working with v(2)
1 , . . . , v(2)
r , the columns of P such
that
(A −αI)2v(2)
j
̸= 0,
(A −αI)3v(2)
j
= 0,
for j = 1, 2, . . . , r.
(5.6.10)
The vectors v(2)
j
satisfying (5.6.10) are called the (second) generalized eigenvectors
associated with the eigenvector (A −αI)2v(2)
j . Thus, the set B1 ∪{v(2)
1 , (v(2)
1 )⋆} is
linearly independent. Consider the four vectors
(A −αI)v(2)
1 ,
(A −αI)(v(2)
1 )⋆
=
((A −αI)v(2)
1 )⋆,
(5.6.11)
(A −αI)2v(2)
1 ,
(A −αI)2(v(2)
1 )⋆
=
((A −αI)2v(2)
1 )⋆.
(5.6.12)
The equalities in (5.6.11) and (5.6.12) follow from (5.6.6) applied to A −αI and
(A −αI)2 in place of A, bearing in mind that A −αI and (A −αI)2 also have

INVARIANT SUBSPACES AND JORDAN FORM
107
the form as in (5.6.5). The vectors (5.6.11) and (5.6.12) are linearly independent.
Indeed, if
c1(A −αI)v(2)
1
+ c2(A −αI)(v(2)
1 )⋆+ c3(A −αI)2v(2)
1
+c4(A −αI)2(v(2)
1 )⋆= 0,
(5.6.13)
where c1, c2, c3, c4 ∈C, then applying A −αI to this equality, we obtain
c1(A −αI)2v(2)
1
+ c2(A −αI)2(v(2)
1 )⋆= 0.
Hence, c1 = c2 = 0, but then (5.6.13) gives c3 = c4 = 0 as well. We now replace
four suitable vectors in B1 with the vectors (5.6.11) and (5.6.12), so that the new
set is again a basis for KerC (A −αI)2. We continue this process until we obtain
a suitable basis in the root subspace KerC (A −αI)2n. Note that all subspaces
KerC (A −αI)i, i = 1, 2, . . . , 2n, are even dimensional.
Now let α be complex nonreal. If u1, . . . , us are the columns of P that represent
eigenvectors and associated generalized eigenvectors of A corresponding to α, then
s is the (complex) dimension of the root subspace R(α) := KerC (A −αI)2n of A
corresponding to α. Also, the columns
u⋆
1, . . . , u⋆
s
(5.6.14)
represent eigenvectors and associated generalized eigenvectors of A corresponding
to α ̸= α. Clearly, the set (5.6.14) is linearly independent. We claim that (5.6.14)
is a basis for the root subspace R(α). Indeed, suppose not. Then the dimension
of R(α) exceeds s, and by taking a basis for R(α) that consists of eigenvectors
and associated generalized eigenvectors and by applying the map ⋆to this basis,
we obtain a linearly independent set in R(α) of more than s vectors, which is
impossible. Thus (5.6.14) is a basis for R(α).
Step 4. Applying Steps 2 and 3 to all real eigenvalues and to all nonreal eigen-
values with positive imaginary parts of A, we eventually obtain a set of 2n linearly
independent vectors arranged in pairs w1, w∗
1, . . . , wn, w∗
n ∈C2n×1, i.e., a basis for
C2n×1, such that the set {w1, . . . , wn} consists of eigenvectors and associated gen-
eralized eigenvectors of A; then, necessarily, {w1, . . . , wn} consists of eigenvectors
and associated generalized eigenvectors as well; if wi1, . . . , wik is a chain consisting
of an eigenvector wi1 corresponding to an eigenvalue λ and associated generalized
eigenvectors wi2, . . . , wik, then w⋆
i1 is an eigenvector corresponding to the eigenvalue
α and w⋆
i2, . . . , w⋆
ik are associated generalized eigenvectors. Set
eP = [w1 w2
. . .
wn w∗
1 w∗
2
. . .
w∗
n] ∈Cn×n,
where the vectors w1, . . . , wn are arranged so that the generalized eigenvectors are
immediately to the right of the eigenvector they are associated with. Then clearly
eP is given by eP =
 B
−C
C
B

for some B, C ∈Cn×n, and the equality A eP = eP eJ
holds with a matrix eJ in the Jordan form such that eJ =
 J0
0
0
J0

for some
Jordan form matrix J0 ∈Cn×n.
Claim 5.6.1 is proved.
□
Proof of uniqueness of a Jordan form. Uniqueness of a Jordan form for
quaternion matrices can be proved by applying the map ω (or eω), thereby reducing

108
CHAPTER 5
the proof to the case of complex matrices. We will oﬀer an independent proof, which
will allow us to demonstrate techniques that may be useful on other occasions as
well.
Let J and eJ be two similar n × n matrices in the Jordan form. We may assume
without loss of generality, that the diagonals D(J) and D( eJ) consist of complex
numbers with nonnegative imaginary parts. We have to prove that J and eJ are
obtained from each other by a permutation of Jordan blocks. Observe that similar
matrices have same eigenvalues, and for each eigenvalue λ0 they have the same
geometric multiplicities (maximal number of linearly independent eigenvectors, or
what is the same, the number of Jordan blocks corresponding to eigenvalues similar
to λ0). Similar matrices also have the same algebraic multiplicities (dimension of
the root subspace corresponding to λ0 or, what is the same, the totality of the sizes
of Jordan blocks that have eigenvalues similar to λ0). In view of these observations,
we may (and do) further assume that
J = J1 ⊕· · · ⊕Jk,
eJ = eJ1 ⊕· · · ⊕eJk,
(5.6.15)
where for each index ℓ, Jℓand eJℓare matrices in the Jordan form of size nℓ×nℓwith
λℓon the diagonal; here λ1, . . . , λk are distinct complex numbers with nonnegative
imaginary parts. Moreover,
Jℓ= Jmℓ,1(λℓ) ⊕Jmℓ,2(λℓ) ⊕· · · ⊕Jmℓ,pℓ(λℓ),
for ℓ= 1, 2, . . . , k,
(5.6.16)
eJℓ= J e
mℓ,1(λℓ) ⊕J e
mℓ,2(λℓ) ⊕· · · ⊕J e
mℓ,pℓ(λℓ),
for ℓ= 1, 2, . . . , k,
(5.6.17)
where
mℓ,1 ≥mℓ,2 ≥· · · ≥mℓ,pℓ,
for ℓ= 1, 2, . . . , k,
emℓ,1 ≥emℓ,2 ≥· · · ≥emℓ,pℓ,
for ℓ= 1, 2, . . . , k,
and the following equalities hold:
pℓ
X
j=1
mℓ,j =
pℓ
X
j=1
emℓ,j,
for ℓ= 1, 2, . . . , k.
Thus, the number of Jordan blocks corresponding to each eigenvalue λℓis the same
in J and eJ. Write down the similarity between J and eJ:
JQ = Q eJ,
Q ∈Hn×n is invertible.
(5.6.18)
Partition Q conformally with the partitions (5.6.15), (5.6.16), and (5.6.17) of J and
eJ:
Q = [Qi,j]k
i,j=1,
Qℓ,ℓ= [Q(ℓ)
j,q]pℓ
j,q=1,
for ℓ= 1, 2, . . . , k,
where Q(ℓ)
j,q is of size mℓ,j × emℓ,q. We have JiQi,ℓ= Qi,ℓeJℓ, so by Proposition 5.4.1
Qi,ℓ= 0 if i ̸= ℓ. Thus, in fact, Q is block diagonal, and (5.6.18) boils down to
JℓQℓ,ℓ= Qℓ,ℓeJℓ,
ℓ= 1, 2, . . . , k,
(5.6.19)
where Qℓ,ℓare invertible.
The uniqueness of Jordan form will be proved if we show that
mℓ,j = emℓ,j,
for j = 1, 2, . . . , pℓand ℓ= 1, 2, . . . , k.

INVARIANT SUBSPACES AND JORDAN FORM
109
Suppose not, and for some ℓ0 we have
mℓ0,1 = emℓ0,1, mℓ0, = emℓ0,2, . . . , mℓ0,u−1 = emℓ0,u−1,
but mℓ0,u ̸= emℓ0,u. Say mℓ0,u > emℓ0,u. (If the opposite inequality holds, then
rewrite (5.6.19) in the form
eJℓQ−1
ℓ,ℓ= Q−1
ℓ,ℓJℓ,
ℓ= 1, 2, . . . , k,
and argue analogously with the roles of eJℓand Jℓinterchanged.) Then it follows
from Proposition 5.4.2 and from the equation
Jℓ0Qℓ0,ℓ0 = Qℓ0,ℓ0 eJℓ0,
or
Jmℓ0,j(λℓ0)Q(ℓ0)
j,q = Q(ℓ0)
j,q Jmℓ0,q(λℓ0),
j, q = 1, 2, . . . , pℓ0,
that the matrices Q(ℓ0)
j,q
have the form of (5.4.3) or (5.4.4). We obtain, therefore,
that each of the u rows
mℓ0,1th, (mℓ0,1 + mℓ0,2)th, . . . , (mℓ0,1 + mℓ0,2 + · · · + mℓ0,u)th
(5.6.20)
of Qℓ0,ℓ0 contain at most u −1 nonzero elements, and those nonzero elements must
be in the positions
mℓ0,1th, (mℓ0,1 + mℓ0,2)th, . . . , (mℓ0,1 + mℓ0,2 + · · · + mℓ0,u−1)th.
(If it happens that u = 1, then the mℓ0,1th row of Qℓ0,ℓ0 is zero.)
Thus, the
rows (5.6.20) (understood as left quaternion vectors) are linearly dependent, which
contradicts invertibility of Qℓ0,ℓ0.
This completes the proof of Theorem 5.5.3.
□
5.7
JORDAN FORMS OF MATRIX REPRESENTATIONS
By the complex Jordan form, resp. real Jordan form, of a complex, resp. real,
matrix X, we mean the familiar Jordan form under similarity X 7→S−1XS, where
S is an invertible complex, resp. real, matrix. Thus, for a complex matrix the
complex Jordan form is the direct sum of blocks of type Jm(λ), where λ ∈C, and
for a real matrix, the real Jordan form is the direct sum of blocks of type Jm(λ),
where λ ∈R, and of type J2m(a ± ib), where a, b ∈R and b ̸= 0 (Theorem 15.1.1).
Theorem 5.7.1. (a) If
Jm1(α1) ⊕· · · ⊕Jmr(αr),
α1 = a1 + ib1, . . . , αr = ar + ibr ∈C
(5.7.1)
is a Jordan form of A ∈Hn×n, then
 Jm1(α1)
0
0
Jm1(α1)

⊕· · · ⊕
 Jmr(αr)
0
0
Jmr(αr)

is the complex Jordan form of ωn(A).
(b) Let (5.7.1) be a Jordan form of A ∈Hn×n, where α1, . . . , αs are real and
αs+1, . . . , αr are nonreal. Then the real Jordan form of χn,n(A) is
(Jm1(α1) ⊕Jm1(α1) ⊕Jm1(α1) ⊕Jm1(α1))

110
CHAPTER 5
⊕
· · · ⊕(Jms(αs) ⊕Jms(αs) ⊕Jms(αs) ⊕Jms(αs))
⊕
 J2ms+1(as+1 ± ibs+1) ⊕J2ms+1(as+1 ± ibs+1)

⊕
· · · ⊕(J2mr(ar ± ibr) ⊕J2mr(ar ± ibr)) .
(5.7.2)
Proof. Part (a) is a by-product of the proof of Theorem 5.5.3.
For part (b), observe that a permutation similarity in the matrix χ (Jmj(αj))
(αj ∈R) yields
Jmj(αj) ⊕Jmj(αj) ⊕Jmj(αj) ⊕Jmj(αj).
Indeed, we have the equality
χ (Jmj(αj)) [e1 e5 . . . e4mj−3 e2 e6 . . . e4mj−2 . . . e4 e8 . . . e4mj]
= [e1 e5 . . . e4mj−3 e2 e6 . . . e4mj−2 . . . e4 e8 . . . e4mj]
·
 Jmj(αj) ⊕Jmj(αj) ⊕Jmj(αj) ⊕Jmj(αj)

.
(5.7.3)
Also, for j = s + 1, . . . , r we have
χ (Jmj(αj))
=


 aj
−bj
bj
aj

02
I2
02
. . .
02
02

aj
−bj
bj
aj

02
I2
. . .
02
...
...
...
...
...
...
02
02
02
02
. . .
 aj
−bj
bj
aj



.
(5.7.4)
A permutation similarity of the matrix in the right-hand side of (5.7.4) yields
J2mj(aj ± ibj) ⊕J2mj(aj ± ibj), and, thus, (Ex. 5.16.11):
χ (Jmj(αj)) = eP −1
4mj
 J2mj(aj ± ibj) ⊕J2mj(aj ± ibj)
 eP4mj,
(5.7.5)
where eP4mj is a certain permutation matrix.
Part (b) is now obvious from (5.7.3) and (5.7.5).
□
Corollary 5.7.2. The matrices A ∈Hn×n, A∗, Aφ for every nonstandard invo-
lution φ, are all similar (over the quaternions).
Proof. Let A = SJS−1, where J is given by (5.7.1) with α1, . . . , αs real and
αs+1, . . . , αr nonreal. Since χn,n(A∗) = (χn,n(A))T , we see by Theorem 5.7.1 that
the real Jordan form of χn,n(A∗) coincides with the real Jordan form of a matrix
that is transpose to (5.7.2). But every real Jordan block is similar to its transpose:
Jm(α)T = F −1
m Jm(α)Fm,
J2m(α ± iβ)T = F −1
2mJ2m(α ± iβ)F2m,
where α, β ∈R, β ̸= 0. (More generally, every real or complex matrix is similar to
its transpose, a well-known fact that can be found, e.g., in Horn and Johnson [62].)
Thus, the real Jordan form of χn,n(A∗) coincides with (5.7.2). By Theorem 5.7.1,
A and A∗have the same Jordan form.

INVARIANT SUBSPACES AND JORDAN FORM
111
For a nonstandard involution φ, ﬁrst observe that any Jordan matrix J (i.e., a
direct sum of Jordan blocks with generally quaternion diagonal) is similar to Jφ.
Indeed, if
J = Jp1(γ1) ⊕· · · ⊕Jps(γs),
γ1, . . . , γs ∈H,
then each γj is similar to (γj)φ: (γj)φ = λ−1
j γjλj for some λj ∈H \ {0}, and we
have
Jpj(γj)φ = (λjFpj)−1 Jp1(γ1) (λjFpj),
j = 1, 2, . . . , s.
Now, if A = SJS−1 for some Jordan matrix J and invertible matrix S, then
Aφ = S−1
φ JφSφ = S−1
φ TJT −1Sφ = S−1
φ TS−1AST −1Sφ,
where the invertible matrix T ∈Hn×n is a similarity matrix between Jφ and J.
□
5.8
COMPARISON WITH REAL AND COMPLEX SIMILARITY
It is instructive to compare Theorem 5.5.3 with results on similarity of real matrices
(over the reals) and complex matrices (over the complexes).
Theorem 5.8.1.
(a) Every square-size quaternion matrix is similar to a com-
plex matrix.
(b) A square-size quaternion matrix A is similar to a real matrix if and only if
for every m and every nonreal eigenvalue λ ∈H, the number of Jordan blocks
Jm(α) with α ∈H similar to λ in the Jordan form of A is even.
Proof. Part (a) is obvious from Theorem 5.5.3(b): the Jordan form there is
complex. For the proof of the Part (b) “if,” we may assume that the Jordan form
of A is given by the right-hand side of (5.5.1), where λ1, . . . , λp ∈C, with the
imaginary parts of the λj’s nonnegative. Using similarity (over quaternions), we
replace one of each pair of identical Jordan blocks Jm(λ), Jm(λ) with nonreal λ
by Jm(λ). Then, observe that the Jordan block with a real eigenvalue is real, and
Jm(λ) ⊕Jm(λ) is similar to a real Jordan block:
[e1 + i e2
e3 + i e4
. . .
e2m−1 + i e2m
e1 −i e2
e3 −i e4
. . .
e2m−1 −i e2m]
·
 Jm(λ) ⊕Jm(λ)

=
J2m(R(λ) ± i I(λ))
· [e1 + i e2
e3 + i e4
. . .
e2m−1 + i e2m
e1 −i e2
e3 −i e4
. . .
e2m−1 −i e2m].
(5.8.1)
Now assume A is similar to a real matrix. Then A is similar to a real Jordan
form. Formula (5.8.1) now shows that the Jordan blocks corresponding to nonreal
eigenvalues come in pairs Jm(λ), Jm(λ), where λ ∈C has positive imaginary part.
Since λ is similar to λ (over H), the condition in (b) follows.
□
Deﬁnition 5.8.2. For a matrix A ∈Hn×n, its quaternion similarity class con-
taining A consists of all matrices B ∈Hn×n for which B = S−1AS for some
invertible S ∈Hn×n. Analogously, for a real, resp. complex, square-size matrix A,
the real, resp. complex, similarity class containing A consists of those real, resp.
complex, matrices B for which B = S−1AS for some invertible real, resp. complex,
matrix S.

112
CHAPTER 5
Clearly, the real, resp. complex, similarity class containing a given real, resp.
complex, matrix A is no larger than the set of real, resp. complex, matrices in the
quaternion similarity class containing the same matrix A.
Theorem 5.8.3. (a) For all real matrices A ∈Rn×n, the real similarity class
containing A coincides with the set
QR (A) := {B ∈Rn×n : B = S−1AS for some S ∈Hn×n}.
(5.8.2)
(b) For all complex matrices A ∈Cn×n, the set
QC (A) := {B ∈Cn×n : B = S−1AS for some S ∈Hn×n}
(5.8.3)
comprises a ﬁnite number p of complex similarity classes. The number p is found
by the formula
p =
s
Y
j=1
 rj
Y
k=1
(pλj,k + 1)
!
,
(5.8.4)
where {λ1, . . . , λs} is a set of distinct nonreal complex eigenvalues of A which is
maximal (by inclusion) with respect to the property that it does not contain any
pair of complex conjugate numbers, and pλj,k is the number of Jordan blocks in the
complex Jordan form of A of size k with eigenvalue λj or λj.
Thus, two real matrices which are similar over H are actually similar over R.
However, two complex matrices which are similar over H are generally not similar
over C; this happens if and only if all eigenvalues of the matrices are real.
For example, if
A = J2(i) ⊕J2(i) ⊕J3(i) ⊕J4(−i) ⊕J1(2i) ⊕J3(−2i),
then one can take {λ1, λ2} = {i, −2i}, and pλ1,k is equal to 2 if k = 2, to 1 if k = 3
or k = 4, and to 0 for all other values of k, whereas pλ2,k is equal to 1 if k = 1 or
k = 3 and to 0 for all other values of k. The number p in this example is 48.
In Theorem 5.8.3(b), a typical complex similarity class contained in the set
QC (A) is constructed as follows. For every pair of values (j, k) as in (5.8.4) such
that pλj,k ̸= 0, select an integer ℓλj,k such that 0 ≤ℓλj,k ≤pλj,k.
Then the
complex similarity class corresponding to this selection of the ℓλj,k’s consists of
those complex matrices X whose complex Jordan form is
J0 ⊕⊕s
j=1 ⊕{k : pλj ,k̸=0}



Jk(λj) ⊕· · · ⊕Jk(λj)
|
{z
}
ℓλj ,k times
⊕Jk(λj) ⊕· · · ⊕Jk(λj)
|
{z
}
pλj ,k−ℓλj ,k times



, (5.8.5)
where J0 is the part of the complex Jordan form of A that consists of Jordan blocks
with real eigenvalues (if any).
Proof. Part (a). Clearly, the real similarity class containing A is contained
in QR (A).
Conversely, let B ∈QR (A), so B = S−1AS, S ∈Hn×n invertible.
Applying the real matrix representation map χn,n, we obtain
χn,n (B) = (χn,n (S))−1 χn,n (A) χn,n (S).
(5.8.6)

INVARIANT SUBSPACES AND JORDAN FORM
113
It is easy to see that χ(X) is similar (over the reals) to X⊕4 for any real square-
size matrix X; in fact, the similarity is provided by a suitable permutation matrix.
Thus A⊕4 is similar to B⊕4 in view of (5.8.6). By Corollary 15.1.4 A and B are
similar (over the reals).
Part (b).
Denote by QC({ℓλj,k}) the set of all complex matrices X having
the complex Jordan form (5.8.5) for a ﬁxed selection {ℓλj,k}, as described above.
Clearly, each QC({ℓλj,k}) is a complex similarity class, and one of those complex
similarity classes contains A. On the other hand, by Theorem 5.5.3, all matrices
X ∈Cn×n that belong to the union ∪QC({ℓλj,k}) (taken over all possible selections
{ℓλj,k}) are similar (over the quaternions). This shows the inclusion
∪QC({ℓλj,k}) ⊆QC (A).
The same Theorem 5.5.3 shows that the opposite inclusion holds as well.
This
proves Part (b).
□
5.9
DETERMINANTS
The notion of determinant is familiar for matrices over a (commutative) ﬁeld F:
det(A) =
X
τ

(−1)sign (τ) a1,τ(1)a2,τ(2) · · · an,τ(n)

,
(5.9.1)
where A = [ai,j]n
i,j=1 is a square-size matrix with entries in F, and the sum in
(5.9.1) is taken over all n! permutations τ of {1, 2, . . . , n}. The determinant has
many well-known remarkable properties, for example, multiplicativity: det (AB) =
det(A) det(B) for all A, B ∈Fn×n. However, the above deﬁnition of determinant
is not useful in the context of matrices over noncommutative division rings, such
as H. In this section we discuss a notion of determinants of square-size quaternion
matrices.
Recall the complex matrix representations
ωn,n : Hn×n →C2n×2n,
eωn : Hn×n →C2n×2n,
deﬁned in (3.4.1) and (5.6.1).
Deﬁnition 5.9.1. We deﬁne the determinant of A ∈Hn×n based on complex
matrix representations as follows:
detC (A) := det(ωn,n (A)) = det(eωn (A)),
where in the right-hand side we use the standard determinant function (5.9.1) for
the determinant of 2n × 2n complex matrices, and the equality det(ωn,n (A)) =
det(eωn (A)) follows from (5.6.2).
For example, detC (α) = |α|2 for all α ∈H.
This determinant shares many properties with the standard determinant (5.9.1)
for matrices over a ﬁeld. Some of these properties are listed below.
Theorem 5.9.2. For all A, B ∈Hn×n we have:
(1) detC (A) is a homogeneous polynomial of degree 2n with real coeﬃcients of
the 4n2 real variables a(k)
i,j , k = 0, 1, 2, 3; i, j = 1, 2, . . . , n, where
ai,j = a(0)
i,j + a(1)
i,j i + a(2)
i,j j + a(3)
i,j k
is the (i, j)th entry of A;

114
CHAPTER 5
(2) detC (A) is real and nonnegative, and detC (A) = 0 if and only if A is not
invertible;
(3) detC (AB) = detC (A) · detC (B);
(4) if A−1 exists, then detC (A−1) = (detC (A))−1;
(5) if A and B are similar, then detC (A) = detC (B);
(6) if λ1, . . . , λn are the eigenvalues of A which are complex numbers with non-
negative imaginary parts, and which are counted according to their algebraic
multiplicity, then
detC (A) = |λ1|2 |λ2|2 · · · |λn|2;
(5.9.2)
(7) if A is upper triangular or lower triangular with λ1, . . . , λn (λj ∈H for j =
1, 2, . . . , n) on the main diagonal, then (5.9.2) holds.
Proof. (3) follows from the deﬁnition of detC and the multiplicative property
of the standard determinant. (2) follows from Claim 5.6.1: in the notation of this
claim, we have
detC (B1 + jB2) = det(eωn(B1 + jB2)) = det(B) = det (J1J1) = | det (J1)|2.
Also, det (J1) = 0 if and only if det(B) = 0, which is equivalent to noninvertibility
of B or that of B1 + jB2. Now (2) follows from the deﬁnition of detC and the
inequality detC (A) ≥0 for all A ∈Hn×n.
Parts (4), (5), and (7) are an easy
consequence of the deﬁnition of detC and well-known properties of the standard
determinant. Finally, for (6) write A = T −1JT for some invertible matrix T, where
J is the Jordan form of A with λ1, . . . , λn on the diagonal, perhaps in a diﬀerent
order. Then
detC (A) = detC (J) = | det (J)|2 = |λ1|2 |λ2|2 · · · |λn|2,
as required.
□
With respect to elementary row and column operations, the quaternion deter-
minant behaves slightly diﬀerently from the standard determinant (5.9.1).
Theorem 5.9.3. Let A ∈Hn×n. Then:
(a) if B is obtained from A by a permutation of its rows or columns, then
detC (B) = detC (A);
(5.9.3)
(b) if B is obtained from A by multiplying one of its rows by λ ∈H on the left or
by multiplying one of its columns by λ ∈H on the right, then
detC (B) = |λ|2 detC (A);
(c) if B is obtained from A by adding one of its rows, multiplied by λ ∈H on the
left, to another row, then (5.9.3) holds;
(d) if B is obtained from A by adding one of its columns multiplied by λ ∈H on
the right to another column, then (5.9.3) holds.

INVARIANT SUBSPACES AND JORDAN FORM
115
Proof. For (a), because every permutation is a product of transpositions, it
suﬃces to consider the case when the permutation is actually a transposition. If
B is obtained from A by interchange of its ith and jth rows (columns), then eω (B)
is obtained from eω (A) by interchange of its ith and jth rows and interchange of
its (n + i)th and (n + j)th rows (columns). Thus, det (eω (B)) = det eω (A), and (a)
follows.
Part (b). We have B = QA or B = AQ, where
Q = diag (1, 1, . . . , λ, 1, . . . , 1) ∈Hn×n,
with λ on the ith place (if it is the ith row or column that gets multiplied by λ).
Now by Theorem 5.9.2(3), (7):
detC (B) = detC (Q) detC (A) = |λ|2 detC (A).
For Part (c) note that B = QA, where now Q has 1s on the diagonal and λ in
exactly one oﬀ-diagonal position. By Theorem 5.9.2(7) detC (Q) = 1, and (5.9.3)
follows. Part (d) is proved analogously.
□
5.10
DETERMINANTS BASED ON REAL MATRIX
REPRESENTATIONS
By analogy with the determinant detC (·) of the preceding section, we introduce
in this section the notion of determinant based on the real matrix representations.
Namely, for A ∈Hn×n we deﬁne
detR (A) := det(χn,n(A)),
where det(χn,n(A)) is the standard determinant of the 4n×4n real matrix χn,n(A).
For example, a straightforward calculation shows that detR (α) = |α|4 for every
α ∈H.
Before we proceed with the study of detR (·), we point out some properties of
invertible quaternion matrices, which are of interest in their own right.
Proposition 5.10.1. (a) A matrix A ∈Hn×n is invertible if and only if it is an
exponential, i.e., A = eB for some B ∈Hn×n.
(b) The set of invertible matrices of size n × n is (pathwise) connected, open,
and dense in Hn×n.
Proof.
If A = eB, then A is invertible with the inverse e−B.
Conversely,
assume A is invertible, and let J be its Jordan form. Since eS−1XS = S−1eXS
for any X ∈Hn×n and any invertible S ∈Hn×n, it suﬃces to prove that J is an
exponential, which, in turn, reduces to the proof that every invertible Jordan block
Jk(α) with α ∈C is an exponential. This is easily veriﬁed using formula (5.5.5):
Jk(α) = eB, where B is the upper triangular Toeplitz k × k matrix having the ﬁrst
row

log α,
1
α,
−1
2α2 ,
. . . ,
(−1)k
(k −1)αk−1

.
For Part (b), to prove that invertible matrices are dense, in view of the Jordan
form (5.5.1), it is enough to show that any Jordan block can be approximated (in
the norm ∥· ∥) as close as we wish by invertible matrices; indeed, Jk(0) can be

116
CHAPTER 5
approximated by Jk(ε), where ε is close to zero. Connectivity follows from Part
(a): if A = eB, then A is connected to I within the set of invertible matrices by the
path e(1−t)B, 0 ≤t ≤1. Finally, if A ∈Hn×n is invertible and C ∈Hn×n is such
that ∥C −A∥< ∥A−1∥−1, then C is invertible as well:
C−1
=
[(C −A) + A]−1 = A−1[A−1(C −A) + I]−1
=
[I −(A−1(C −A)) + (A−1(C −A))2 −. . .]A−1,
where the series converges because ∥A−1(C −A)∥≤∥A−1∥∥C −A∥< 1.
□
We now return to detR (·). It turns out that detR (·) takes only nonnegative
values; this and other properties of the determinant are summarized in the next
proposition.
Proposition 5.10.2. (1) detR (·) is multiplicative:
detR (AB) = detR (A) detR (B),
for all A, B ∈Hn×n;
(2) detR (A) ̸= 0 if and only if A is invertible;
(3) for every A ∈Hn×n, we have detR (A) ≥0.
Proof. (1) follows from the multiplicativity of the map χn,n and the multi-
plicative property of the determinant of χ(·).
If detR (A) ̸= 0, then χn,n(A) is
invertible in R4n×4n, and it is easy to see that its inverse also belongs to the subal-
gebra {χn,n(X) : X ∈Hn×n}. Thus, A is invertible in Hn×n (use the multiplicative
property of χn,n(·)). If detR (A) = 0, then χn,n(A) is not invertible, and so A is
not invertible as well. For the proof of (3), note that the set
{detR (A) : A ∈Hn×n
is invertible}
(5.10.1)
is contained in R \ {0}, and, being connected (by Proposition 5.10.1(b) and conti-
nuity of the determinant function), we must have that the set (5.10.1) is contained
in the set of positive real numbers. Now the denseness property of Proposition
5.10.1(b) implies that the range of detR (·) is contained in the nonnegative real
numbers.
□
At this point we make use of the following result concerning the axiomatic
description of quaternion determinants (proved by Cohen and De Leo [28] based on
results of Dieudonn´e [32]):
Theorem 5.10.3. There is a unique functional F that maps Hn×n into non-
negative real numbers, which is multiplicative and satisﬁes the scaling condition
F(qIn) = |q|n for every q ∈H.
Combining Theorem 5.10.3 with Theorem 5.9.2 and Proposition 5.10.2, we ob-
tain that
p
detC (A) =
4p
detR (A) for every A ∈Hn×n; in other words, (detC (·))2 =
detR (·).
5.11
LINEAR MATRIX EQUATIONS
As an application of the Jordan form, in this section we study certain linear matrix
equations with matrix quaternion coeﬃcients and unknowns.

INVARIANT SUBSPACES AND JORDAN FORM
117
Theorem 5.11.1. Let A ∈Hm×m, B ∈Hn×n, C ∈Hm×n. Then the equation
AX −XB = C,
X ∈Hm×n
(5.11.1)
has unique solution if and only if
σ(A) ∩σ(B) = ∅.
(5.11.2)
Otherwise, (5.11.1) has either inﬁnitely many solutions or no solutions.
Proof. Equation (5.11.1) is a system of real linear equations in 4mn real un-
knowns, the real constituents of the entries of X. Thus, by the standard theory
of systems of linear equations over a ﬁeld, we need to show that the homogeneous
equation
AX −XB = 0
(5.11.3)
has only the trivial solution X = 0 if and only if (5.11.2) holds.
Suppose (5.11.2) holds. We prove that (5.11.3) has only the trivial solution.
Applying a transformation
A →T −1AT,
B →S−1BS,
X →T −1XS,
(5.11.4)
we can assume that both A and B are in the Jordan forms:
A = ⊕k
j=1Jmj(αj);
B = ⊕ℓ
j=1Jpj(βj),
(5.11.5)
where α1, . . . αk, β1, . . . , βℓ∈H.
Partition X conformally with (5.11.5): X =
[Xi,j]k,ℓ
i,j=1, where Xi,j ∈Hmi×pj. Then equation (5.11.3) boils down to the sys-
tem
Jmi(αi)Xi,j −Xi,jJpj(βj) = 0,
i = 1, 2, . . . , k;
j = 1, 2, . . . , ℓ.
Thus the proof that (5.11.3) has only the trivial solution reduces to Proposition
5.4.1.
Conversely, if α ∈σ (A) ∩σ (B), then by Corollary 5.7.2 there exist nonzero
x, y ∈Hn×1 such that
Ax = xα,
B∗y = yα∗.
Then y∗B = αy∗, and X := xy∗̸= 0 satisﬁes the equation (5.11.3).
□
Equation (5.11.1) is, generally speaking, not linear in the sense of quaternion
vector spaces, because the left-hand side is not quaternion linear as a function of X,
regardless of whether Hm×n is given the structure of left or right quaternion vector
space. It is, however, linear over R, when Hm×n is given the standard structure of
4mn-dimensional real vector space, in terms of the four real components of each
entry in an m × n quaternion matrix.
Theorem 5.11.2. Let A ∈Hm×m, B ∈Hn×n, and assume that λ1, . . . , λk are
all distinct common eigenvalues of A and B, where the λj’s are taken to be complex
numbers with nonnegative imaginary parts, with λ1, . . . , λℓreal and λℓ+1, . . . , λk
nonreal (the cases when ℓ= 0 or ℓ= k are not excluded). Let
m1,j, m2,j, . . . , mpj,j
and
n1,j, n2,j, . . . , nqj,j

118
CHAPTER 5
be the partial multiplicities of λj as an eigenvalue of A and B, respectively. Then
the real dimension of the solution set of AX −XB = 0 as a real vector space is
equal to
4
ℓ
X
j=1
pj
X
i=1
qj
X
r=1
min{mi,j, nr,j} + 2
k
X
j=ℓ+1
pj
X
i=1
qj
X
r=1
min{mi,j, nr,j}.
Proof. Applying a transformation of the type (5.11.4), we may suppose that
A and B are in their Jordan forms:
A = Jm1(λ1) ⊕· · · ⊕Jmr(λr),
B = Jn1(µ1) ⊕· · · ⊕Jns(µs).
Partition X conformally: X = [Xi,j]r,s
i=1,j=1, where Xi,j ∈Hmi×nj.
Then the
equation AX = XB decomposes into rs independent equations
Jmi(λi)Xi,j = Xi,jJnj(µj),
for i = 1, 2, . . . , r,
and j = 1, 2, . . . .
(5.11.6)
Now use Propositions 5.4.1 and 5.4.2 for each equation, as appropriate, in (5.11.6).
□
The equation AXB −X = C can be analyzed along similar lines.
Theorem 5.11.3. Let A ∈Hm×m, B ∈Hn×n, C ∈Hm×n. Then the equation
AXB −X = C,
X ∈Hm×n
(5.11.7)
has unique solution if and only if
λµ ̸= 1 for all λ ∈σ (A), µ ∈σ (B).
(5.11.8)
Otherwise, (5.11.7) has either inﬁnitely many solutions or no solutions.
Proof. Analogous to the proof of Theorem 5.11.1, we need to verify the follow-
ing statement: The homogeneous equation
AXB = X,
X ∈Hm×n
(5.11.9)
has only the trivial solution X = 0 if and only if (5.11.8) holds.
Suppose (5.11.8) holds. We will show that (5.11.9) has only the trivial solution.
As in the proof of Theorem 5.11.1, the proof reduces to the case when A = Jm(α),
B = Jp(β):
Jm(α) X Jp(β) = X,
X ∈Hm×p,
(5.11.10)
with α, β ∈H. If β ̸= 0, then (5.11.10) is equivalent to Jm(α) X = X (Jp(β))−1,
and we are done by Theorem 5.11.1; note that the eigenvalues of (Jp(β))−1 are
precisely the inverses of the eigenvalues of Jp(β); cf. Ex. 5.16.8. Analogously, we
are done if α ̸= 0. In the remaining case α = β = 0, iterate equation (5.11.10):
X
=
Jm(0) X Jp(0) = (Jm(0))2 X (Jp(0))2
=
· · · = (Jm(0))q X (Jp(0))q,
(5.11.11)
which is equal to zero for q ≥min{m, p}.
Conversely, suppose λ0 µ0 = 1 for some eigenvalues λ0 of A and µ0 of B. We
have to show that (5.11.9) has nontrivial solutions.
Applying a transformation

INVARIANT SUBSPACES AND JORDAN FORM
119
(5.11.4), we may assume that both A and B are in the Jordan forms (5.11.5), where
without loss of generality, we further assume that α1 = λ0, β1 = µ0. Partitioning
X = [Xi,j]k,ℓ
i,j=1 conformally with (5.11.5), we see that (5.11.9) amounts to a system
of homogeneous equations
Jmi(αi) Xi,j Jpj(βj) = Xi,j,
i = 1, 2, . . . , k;
j = 1, 2, . . . , ℓ.
It remains to observe that the equation
Jm1(α1) X1,1 Jp1(β1) = X1,1,
or, equivalently,
Jm1(α1) X1,1 = X1,1 (Jp1(β1))−1,
has nontrivial solutions X1,1 by Theorem 5.11.1.
□
The equation AXB −X = C is also linear in the sense of vector spaces over R,
but not linear in the sense of quaternions. The analogue of Theorem 5.11.2 for the
equation AXB = X is as follows.
Theorem 5.11.4. Let A ∈Hm×m, B ∈Hn×n, and assume that λ1, . . . , λk
are all distinct nonzero eigenvalues of A, where the λj’s are taken to be complex
numbers with nonnegative imaginary parts, such that λ−1
1 , . . . , λ−1
k
are eigenvalues
of B. Assume furthermore that λ1, . . . , λℓare real and λℓ+1, . . . , λk are nonreal (the
cases when ℓ= 0 or ℓ= k are not excluded). Let
m1,j, m2,j, . . . , mpj,j
and
n1,j, n2,j, . . . , nqj,j
be the partial multiplicities of λj and λ−1
j
as eigenvalues of A and B, respectively,
for j = 1, 2, . . . , k. Then the real dimension of the solution set of AX −XB = 0 as
a real vector space is equal to
4
ℓ
X
j=1
pj
X
i=1
qj
X
r=1
min{mi,j, nr,j} + 2
k
X
j=ℓ+1
pj
X
i=1
qj
X
r=1
min{mi,j, nr,j}.
The proof of Theorem 5.11.4 is similar to that of Theorem 5.11.2 and is omitted;
see Ex. 5.16.14.
5.12
COMPANION MATRICES AND POLYNOMIAL EQUATIONS
Deﬁnition 5.12.1. An n × n matrix of the form
C =


0
1
0
0
. . .
0
0
0
1
0
. . .
0
...
...
...
...
. . .
...
0
0
0
0
. . .
1
−α0
−α1
−α2
−α3
. . .
−αn−1


,
α0, . . . , αn−1 ∈H,
is called a companion matrix.
Its eigenvectors have a speciﬁc form, as follows.

120
CHAPTER 5
Proposition 5.12.2. If x =


x1
x2
...
xn

∈Hn×1 is an eigenvector of C, then
x1 ̸= 0, and there exists z0 ∈H such that
xj = x1zj−1
0
for j = 2, 3, . . . , n.
(5.12.1)
In fact, z0 is the corresponding eigenvalue.
Proof. The eigenvalue-eigenvector equation reads


0
1
0
0
. . .
0
0
0
1
0
. . .
0
...
...
...
...
. . .
...
0
0
0
0
. . .
1
−α0
−α1
−α2
−α3
. . .
−αn−1




x1
x2
...
xn

=


x1
x2
...
xn

z0,
(5.12.2)
and we have xj = xj−1z0, for j = 2, 3, . . . , n. Thus, equalities (5.12.1) follow. If x1
were zero, then (5.12.1) would imply x = 0, an impossibility.
□
Equating the bottom components in the left- and right-hand sides of (5.12.2)
gives
−α0x1 −α1x1z0 −· · · −αn−1x1zn−1
0
= x1zn
0 ;
hence, scaling x so that x1 = 1, we obtain that z0 is a solution of the polynomial
equation
zn +
n−1
X
j=0
αjzj.
(5.12.3)
Theorem 5.12.3. If z0 ∈H is a solution of equation (5.12.3), then z0 ∈σ(C).
Conversely, if z0 ∈σ(C), then there exists α ∈H \ {0} such that α−1z0α is a
solution of (5.12.3).
Proof. The converse part we have seen above (α appears because of the need
to scale x which results in replacing the eigenvalue by a similar one). The direct
part is proved by reversing the argument.
□
Using Theorem 5.12.3 and the Jordan form of C, the following information
about solutions of polynomial equations is obtained.
Theorem 5.12.4. Let α0, . . . , αn ∈H with αn ̸= 0. Then the equation
αnzn + αn−1zn−1 + · · · + α1z + α0 = 0,
z ∈H,
(5.12.4)
has at least one solution z, and the number of quaternion similarity equivalence
classes that contain a solution of (5.12.4) does not exceed n.
Recall that similarity equivalence classes are sets of quaternions of the form
{β−1λβ : β ∈H \ {0}}, λ ∈H.
Proof. The proof quickly reduces to the case αn = 1 by replacing z with α−1zα.
The statement concerning existence of solution follows from existence of eigenvalues

INVARIANT SUBSPACES AND JORDAN FORM
121
of the companion matrix. The statement concerning similarity equivalence classes
containing solutions follows from the fact that the companion matrix cannot have
eigenvalues in more than n similarity equivalence classes, as is evident from the
uniqueness of its Jordan form.
□
As it turns out, for any given similarity equivalence class, one of the three
possibilities occurs:
(a) the entire class consists of zeros (perhaps not all zeros) of (5.12.4);
(b) the class contains exactly one zero of (5.12.4);
(c) the class does not contain any zeros of (5.12.4).
We will not prove this statement here and refer the reader to the original paper by
Janovska and Opfer [72]. By Theorem 5.12.4, the total number of classes for which
(a) or (b) holds cannot exceed n. A more reﬁned result is proved by Pogorui and
Shapiro [119]: the total quantity of the classes of type (b) and of the double number
of the classes of type (a) does not exceed n, the degree of the polynomial.
The following example is given in Janovska and Opfer [72].
Example 5.12.5. Let
p(z) := z6 + jz5 + iz4 −z2 −jz −i.
Then the polynomial p(z) has the zeros
1, −1, 1
2(1 −i −j −k), 1
2(−1 + i −j −k),
{q ∈H : R(q) = 0,
|V(q)| = 1}.
Thus, (a) holds true for the similarity equivalence classes {1}, {−1}, {q ∈H :
R(q) = 0,
|V(q)| = 1}, (b) holds true for the two similarity equivalence classes
(
q ∈H : R(q) = 1
2,
|V(q)| =
√
3
2
)
and
(
q ∈H : R(q) = −1
2,
|V(q)| =
√
3
2
)
,
and (c) holds true for all other similarity equivalence classes.
□
The next proposition is instructive.
Proposition 5.12.6. Let a, b ∈H be nonzero with zero real parts. Then the
equation
z2 −(a + b)z + ab = 0,
z ∈H,
(5.12.5)
has precisely two distinct solutions if |a| ̸= |b|, has the unique solution z′ = b if
|a| = |b| and a + b ̸= 0, and has the solutions {z′ ∈H : R(z′) = 0, |z′| = |b|} if
a + b = 0.

122
CHAPTER 5
Proof. The case when a+b = 0 is clear: the equation (5.12.5) reads z2+|b|2 = 0.
Assume now a+b ̸= 0. First note that b is indeed a solution of (5.12.5). By scaling
a and b, i.e., replacing a 7→ra, b 7→rb, where r > 0, we may assume |b| = 1.
By simultaneously applying an automorphism of H to a and b (Theorem 2.4.4), we
may further assume that b = i, a = a1i+a2j, where a1, a2 are real and either a2 ̸= 0
or a2 = 0, a1 ̸= −1. Let z′ = z0 + z1i + z2j + z3k, z0, z1, z2, z3 ∈R, be a solution of
(5.12.5). Then we have the equality
(z′)2 = ((a1 + 1)i + a2j)z′ + (a1 + a2k).
(5.12.6)
Writing out the real coeﬃcients of 1, i, j, k in both sides of (5.12.6), we see that
(5.12.6) boils down to the following four equations:
z2
0 −z2
1 −z2
2 −z2
3
=
−(a1 + 1)z1 −a2z2 + a1,
(5.12.7)
2z0z1
=
(a1 + 1)z0 + a2z3,
(5.12.8)
2z0z2
=
a2z0 −(a1 + 1)z3,
(5.12.9)
2z0z3
=
−a2z1 + (a1 + 1)z2 + a2.
(5.12.10)
Suppose ﬁrst z0 ̸= 0. Then (5.12.8) and (5.12.9) give
z2 = 1
2

a2 −(a1 + 1)z3
z0

,
z1 = 1
2

(a1 + 1) + a2z3
z0

,
(5.12.11)
and substituting in (5.12.10) yields, after some simple algebra, the equality
z3 =
a2z0
2z2
0 + 1
2a2
2 + 1
2(a1 + 1)2 .
(5.12.12)
Substituting (5.12.11) and (5.12.12) in (5.12.7), we obtain, after clearing denomi-
nators, a cubic equation in the variable y := z2
0 of the form
4y3 + c2y2 + c1y + c0 = 0,
c0, c1, c2 ∈R.
This equation cannot have positive solutions y0: if it did, then we would obtain at
least three pairwise nonsimilar solutions of (5.12.5), namely, b and the two solutions
obtained by setting z0 = ±√y0 in (5.12.11) and (5.12.12). However, this contradicts
Theorem 5.12.4. Thus, the assumption z0 ̸= 0 leads to a contradiction.
Now suppose z0 = 0.
Then we have, from (5.12.7)–(5.12.10): z3 = 0 and,
consequently,
−z2
1 −z2
2 = −(a1 + 1)z1 −a2z2 + a1,
0 = −a2z1 + (a1 + 1)z2 + a2. (5.12.13)
If a2 = 0, then we conclude that z2 = 0, and the ﬁrst equation in (5.12.13) gives
two values for z1: z′
1 = 1 (thus, z′ = b) and z′
1 = a1. So we obtain exactly two
zeros of (5.12.5) if |a| ̸= |b| and exactly one zero of (5.12.5) if |a| = |b|. Consider
now the case when a2 ̸= 0. Then we solve the second equation in (5.12.13) for z1:
z1 = (a1 + 1)z2 + a2
a2
.
Substituting in the ﬁrst equation in (5.12.13) gives rise to a quadratic equation for
z2 whose solutions are z′
2 = 0 and
z′′
2 = a2(−1 + a2
1 + a2
2)
(a1 + 1)2 + a2
2
.

INVARIANT SUBSPACES AND JORDAN FORM
123
Thus, if |a| = 1, then also z′′
2 = 0, and it is easy to see that we obtain z′ = b, the
unique solution of (5.12.5). If |a| ̸= 1, then we have two distinct values for z′
2 and
z′′
2 and, consequently, exactly two solutions of (5.12.5).
□
Let K(H) be the set of all nonempty compact subsets of H, considered as a metric
space with the Hausdorﬀdistance as a metric. The Hausdorﬀdistance between two
compact subsets L1, L2 ⊂H is deﬁned by
d(L1, L2) = max

max
x∈L1

min
y∈L2 |x −y|

,
max
x∈L2

min
y∈L1 |y −x|

.
See Ex. 5.16.29. Deﬁne the map Z : Hn →K(H) by
Z(α0, · · · , αn−1) = {the zero set of zn + αn−1zn−1 + · · · + α0}.
Proposition 5.12.6 shows that the map Z is generally discontinuous, in contrast
to the continuity of the complex roots of real or complex polynomials (Theorem
5.15.1).
Open Problem 5.12.7. Study the (dis)continuity properties of the map Z. For
example: What are the points of continuity of Z? Cf. Theorem 5.5.9.
5.13
EIGENVALUES OF HERMITIAN MATRICES
We record here some properties of eigenvalues of hermitian quaternion matrices
that will be used later in the text. First note that the eigenvalues of hermitian
matrices are real (Theorem 5.3.6(c)). Indeed, if A = A∗∈Hn×n and Ax = xλ for
some nonzero x ∈Hn×1 and λ ∈H, then x∗Ax = x∗xλ. But x∗Ax is real and x∗x
is positive, so λ must be real as well.
Theorem 5.13.1. Let A, B ∈Hn×n be hermitian matrices such that A −B is
positive semideﬁnite. If
λ1 ≥λ2 ≥· · · ≥λn,
µ1 ≥µ2 ≥· · · ≥µn
are the eigenvalues of A and B, respectively, arranged in nonincreasing order, then
λj ≥µj,
for j = 1, 2, . . . , n.
Theorem 5.13.2. If A ∈Hn× is hermitian and S ∈Hn×n, then the number of
positive, resp. negative, eigenvalues of SAS∗, counted with algebraic multiplicities,
does not exceed the number of positive, resp. negative, eigenvalues of A, also counted
with algebraic multiplicities.
The proofs of Theorems 5.13.1 and 5.13.2 are essentially the same as for the
complex hermitian matrices (see Sections 4.3, 7.7, and 4.5 in Horn and Johnson
[62], for example), and will not be repeated here.
5.14
DIFFERENTIAL AND DIFFERENCE EQUATIONS
Consider a system of linear diﬀerential equations with constant coeﬃcients
Aℓx(ℓ)(t) + Aℓ−1x(ℓ−1)(t) + · · · + A1x′(t) + A0x(t) = 0,
t ∈R,
(5.14.1)
where Aℓ, . . . , A1, A0 ∈Hn×n, and x(t) is an unknown ℓtimes continuously diﬀer-
entiable Hn×1-valued function of the real independent variable t.

124
CHAPTER 5
Deﬁnition 5.14.1. We say that the system (5.14.1) is forward stable if all so-
lutions x(t) tend to zero as t →+∞and forward bounded if every solution is a
bounded function as t →+∞.
Criteria will be given for stability and for boundedness in terms of location of
the spectrum of the companion matrix, in compete analogy with the well-known
case of equations (5.14.1) with complex coeﬃcients.
Assuming Aℓis invertible, let
C =


0
In
0
. . .
0
0
0
In
. . .
0
...
...
...
...
...
0
0
0
. . .
In
−A−1
ℓA0
−A−1
ℓA1
−A−1
ℓA2
. . .
−A−1
ℓAℓ−1


∈Hℓn×ℓn
be the companion matrix of system (5.14.1).
Theorem 5.14.2.
(a) The system (5.14.1) is forward stable if and only if all
eigenvalues of C have negative real parts.
(b) The system (5.14.1) is forward bounded if and only if all eigenvalues of C
have nonpositive real parts, and for those eigenvalues with zero real parts, the
geometric multiplicity coincides with the algebraic multiplicity.
The proof is based on the following lemma.
Lemma 5.14.3. Let X ∈Hm×m. Then:
(a) limt →+∞etX = 0 if and only if all eigenvalues of X have negative real parts;
(b) ∥etX∥is bounded as t →+∞if and only if all eigenvalues of X have nonpos-
itive real parts, and for those eigenvalues with zero real parts the geometric
multiplicity coincides with the algebraic multiplicity.
Proof. Part (a). If all eigenvalues of X have negative real parts, then
lim
t →∞∥etX∥= 0
(as one can readily see by reducing X to its Jordan form), and forward stability
of (5.14.1) follows.
Conversely, let λ0 be an eigenvalue of X with nonnegative
real part and corresponding eigenvector v0. We may (and do) choose λ0 to be a
complex number with nonnegative imaginary part. If S−1XS = J is a Jordan form
with complex eigenvalues having nonnegative imaginary parts, then S−1v0 is an
eigenvector of J corresponding to the eigenvalue λ0. Now
etXv0 = S(etJ(S−1v0)) = S(S−1v0etλ0) = v0etλ0 ̸→∞,
(5.14.2)
as t →∞.
Part (b). The “if” statement follows from formula (5.5.5), as the condition stated
in (b) means that the Jordan blocks in the Jordan form of X that correspond to
eigenvalues λ with zero real parts (if any) are in fact 1 × 1, and so, on these Jordan
blocks the function etX is simply etλ, which is bounded as t →∞.

INVARIANT SUBSPACES AND JORDAN FORM
125
Part (b). The “only if” statement: Formula (5.5.5) (applied to f(s) = ets, where
for a moment t is considered ﬁxed, and s ∈R is the independent variable) shows
that
et(Jm(λ)) =


etλ
tetλ
t2etλ
2!
. . .
tm−1etλ
(m −1)!
0
etλ
tetλ
. . .
tm−2etλ
(m −2)!
...
...
...
...
...
0
0
0
. . .
etλ


.
So, if the real part of λ is positive, or if m ≥2, we have that et(Jm(λ)) is unbounded
as t →∞.
□
Proof of Theorem 5.14.2. Let
y(t) =


x(t)
x′(t)
...
x(ℓ−1(t)

∈Hℓn×1.
It is easily veriﬁed that x(t) is a solution of (5.14.1) if and only if y(t) is a solution
of
y′(t) = Cy(t).
(5.14.3)
The general solution of (5.14.3) is
y(t) = etCy(0).
(5.14.4)
If C has an eigenvalue λ0 with nonnegative real part and corresponding eigenvector
v0, then a solution y(t) = etCv0 does not tend to zero as t →∞(cf. (5.14.2)).
Notice that because of the structure of C, the vector v0 necessarily has the form
v0 =


v1
v1λ0
...
v1λℓ−1
0

, where v1 is the ﬁrst component of v0. Therefore, it follows that
the ﬁrst component of y(t), i.e., x(t), does not tend to zero as t
→
∞. Thus,
(5.14.1) is not forward stable. This proves (a).
The proof of (b) is analogous to that of part (a), using Lemma 5.14.3(b).
□
A parallel development can be given to systems of diﬀerence equations. Consider
a system of linear diﬀerence equations with constant coeﬃcients
Aℓxj + Aℓ−1xj−1 + · · · + A0xj−ℓ= 0,
j = ℓ, ℓ+ 1, . . . .
(5.14.5)
Here A0, . . . , Aℓ∈Hn×n are given matrices, and {xj}∞
j=0 is a sequence of vectors
in Hn×1 to be found. Assuming Aℓis invertible, we deﬁne the companion matrix
C ∈Hℓn×ℓn as before. Letting
zj =


xj
xj−1
...
xj−ℓ

∈Hℓn×1,
j = ℓ, ℓ+ 1, . . . ,

126
CHAPTER 5
we see that (5.14.5) is equivalent to the system
zj+1 = Czj,
j = ℓ, ℓ+ 1, . . . .
(5.14.6)
The solutions of (5.14.6) are
zj = Cj−ℓzℓ,
j = ℓ+ 1, ℓ+ 2, . . . .
Theorem 5.14.4.
(a) All solutions of system (5.14.5) tend to zero as j →∞
if and only if all eigenvalues of C have norm less than one.
(b) All solutions of system (5.14.5) are bounded if and only if all eigenvalues of
C have norm less than or equal to one, and for those eigenvalues with norm
one, the geometric multiplicity coincides with the algebraic multiplicity.
The following lemma is used in the proof.
Lemma 5.14.5. Let X ∈Hm×m. Then:
(a) limp →∞Xp = 0 if and only if all eigenvalues of X have norm less than one;
(b) the sequence ∥Xp∥, p = 1, 2, . . ., is bounded if and only if all eigenvalues of
X have norm less than or equal to one, and for those eigenvalues with norm
one, the geometric multiplicity coincides with the algebraic multiplicity.
The proof of Lemma 5.14.5 is analogous to that of Lemma 5.14.3, using the
following formula for powers of Jordan blocks (a particular case of (5.5.5):
Jm(λ)p =


λp
pλp−1
p
2

λp−2
. . .

p
m −1

λp−m+1
0
λp
pλp−1
. . .

p
m −2

λp−m+2
...
...
...
...
...
0
0
0
. . .
λp


.
5.15
APPENDIX: CONTINUOUS ROOTS OF POLYNOMIALS
Continuity of complex roots of polynomial equations with real or complex coeﬃ-
cients is expressed in the following well-known result.
Theorem 5.15.1. Let p(z) := Pn
j=0 ajzj be a polynomial, where a0, . . . , an ∈F,
F = R or F = C and an ̸= 0. Let λ1, . . . , λk be the distinct roots in the complex
plane of the polynomial equation
p(z) = 0.
(5.15.1)
Then for every suﬃciently small ϵ > 0, there is δ > 0 such that if |a′
j −aj| < δ,
aj ∈F, for j = 0, 1, . . . , n, then all the roots of the polynomial equation
n
X
j=0
a′
jzj = 0
(5.15.2)
are contained in the union of open disks
∪k
j=1{λ ∈C : |λ −λj| < ϵ}.
Moreover, the number of roots of (5.15.2) in the disk {λ ∈C : |λ−λi| < ϵ} counted
with their multiplicities is equal to the multipicity of λi as a root of (5.15.1).

INVARIANT SUBSPACES AND JORDAN FORM
127
Proof. Clearly the real case F = R is just a particular situation of the more
general complex case, so we prove Theorem 5.15.1 for F = C only.
Let ϵ > 0 be so small that the disks
Eϵ/2(λ1) :=
n
λ ∈C : |λ −λ1| ≤ϵ
2
o
, . . . , Eϵ/2(λk) :=
n
λ ∈C : |λ −λk| ≤ϵ
2
o
do not pairwise intersect. Then, in particular, the polynomial p(z) := Pn
j=0 ajzj
has no roots on the circles
Eϵ/2 := ∪k
i=1
n
λ ∈C : |λ −λ1| = ϵ
2
o
.
Using continuity of p(z) (as a function of z and of the coeﬃcients aj) and com-
pactness of Eϵ/2, we see that there is δ (which depends on p(z) and ϵ) such that for
every z0 ∈Eϵ/2 the following implication holds:
|a′
j −aj| < δ
=⇒
|
n
X
j=0
(a′
j −aj)zj
0| < min{|p(z)| : z ∈Eϵ/2}.
We now use Rouch´e’s theorem, which is given in most textbooks on complex analysis
(see, e.g., Churchill and Brown [27]), to obtain that the number of roots, counted
with multiplicities, of p(z) and of Pn
j=0 a′
jzj in Eϵ/2(λi) is the same for every ﬁxed
i, i = 1, 2, . . . , k. This proves the theorem.
□
5.16
EXERCISES
Ex. 5.16.1. Verify formula (5.1.6).
Ex. 5.16.2. Verify that σ(A) = {µ ∈H : m4 = −1} for A =
 0
i
j
0

.
Ex. 5.16.3. For a ﬁxed A ∈Hn×n, show that the set of A- invariant subspaces
forms a lattice: If subspaces M1, M2 ∈Hn×1 are A-invariant, then so are M1+M2
and M1 ∩M2.
Ex. 5.16.4. Prove that the following statements are equivalent for A ∈Hn×n
and a subspace M ∈Hn×1:
1. M is A-invariant;
2. M⊥is A∗-invariant;
3. for any nonstandard involution φ the φ-orthogonal companion M⊥φ is Aφ-
invariant.
Ex. 5.16.5. Characterize all matrices A ∈Hn×n for which eiA = cos A + i sin A.
Hint: Complex matrices have this property.
Ex. 5.16.6. Show that if λ ∈H is an eigenvalue or a left eigenvalue of A ∈Hn×n,
then |λ| ≤∥A∥.
Ex. 5.16.7. Provide details for the proofs of Theorem 5.14.4 and Lemma 5.14.5.
Ex. 5.16.8. Prove that if A ∈Hn×n is invertible, then
σ (A−1) = {λ−1 : λ ∈σ (A)}.

128
CHAPTER 5
Ex.
5.16.9. Show explicitly that the Jordan blocks Jm(λ) and Jm(λ∗) are
similar (over the quaternions); here λ ∈H.
Ex.
5.16.10. Show that the matrix

1
i
j
k

is not similar to its transpose
 1
j
i
k

. Hint: One of these two matrices is invertible, the other is not.
Ex. 5.16.11. Find explicitly the permutation matrix eP4mj of (5.7.5).
Ex. 5.16.12. Prove that if A ∈Hn×n is invertible, then the inverse A−1 is given
by A−1 = p(A), where p(t) is a certain polynomial with real coeﬃcients. Hint:
Consider the minimal polynomial of A.
Ex. 5.16.13. Verify directly that eΩ2n is a real unital subalgebra of C2n×2n.
Ex. 5.16.14. Provide details for the proof of Theorem 5.11.4. Hint: Use the
fact that (Jm(λ))−1, λ ∈H \ {0}, is similar to Jm(λ−1).
Ex. 5.16.15. (a) Show that if two matrices A, B ∈Hn×n, n ≤3, have the same
minimal polynomial, then they are similar.
(b) Show by example that the statement in (a) fails if n ≥4.
Ex. 5.16.16. (a) Show that if two matrices A, B ∈Hn×n, n ≤6, have the same
minimal polynomial and the same geometric multiplicity for every eigenvalue, then
they are similar.
(b) Show by example that the statement in (a) fails if n ≥7.
Ex. 5.16.17. The system (5.14.1) is said to be backward bounded if all solutions
are bounded as t −→−∞. Prove the following analogue of Theorem 5.14.2(b):
the system (5.14.1) is backward bounded if and only if all eigenvalues of C have
nonnegative real parts, and for those eigenvalues with zero real parts, the geometric
multiplicity coincides with the algebraic multiplicity.
Ex. 5.16.18. Consider polynomial equation with real coeﬃcients
zn +
n−1
X
j=0
ajzj = 0,
z ∈H.
(5.16.1)
Let a1, . . . , ak be all the distinct solutions of (5.16.1) in the closed upper complex
half-plane, with a1, . . . ap real and ap+1, . . . , ak complex nonreal.
Show that all
solutions of (5.16.1) are given by a1, . . . , ap, αp+1, . . . , αk, where R(αj) = R(aj),
|V(αj)| = I(aj), for j = p + 1, p + 2, . . . , k.
Ex. 5.16.19. Find the Jordan forms of the following two matrices:
A1 =
 i + j
1
0
k + 1

;
A2 =


i
1
0
0
j
1
0
0
k

.
Ex. 5.16.20. Find the minimal polynomials of the matrices in Ex. 5.16.19.
Ex. 5.16.21. Find all invariant subspaces of the matrices
(a)
A =


i
1
0
0
i
0
0
0
j

;
(b)
B =


i
1
0
0
i
0
0
1
j

.

INVARIANT SUBSPACES AND JORDAN FORM
129
Ex. 5.16.22. Find all eigenvectors for each of the following matrices:
(a)
 j
0
0
j

;
(b)


j
0
0
0
j
0
0
0
j

;
(c)


i
0
0
0
j
0
0
0
k

.
Ex. 5.16.23. (a) Show that the degree of a minimal polynomial of an n × n
quaternion matrix does no exceed 2n. Hint: Use the Jordan form and Proposition
5.4.3.
(b) For every positive integer n, give an example of an n × n matrix whose
minimal polynomial has degree 2n.
Ex. 5.16.24. Find all solutions of the following matrix equations:
(a)
 i
1
0
i

X −X
 j
0
1
j

= 0;
(b)
 i
1
0
i

X −X
 1
0
1
1

=
 j
0
0
j

;
(c)
 i
1
0
i

X
 k
0
1
k

−X = 0;
(d)
 i
1
0
i

X
 2
0
1
2

−X =
 k
0
0
k

.
Ex. 5.16.25. Show that, for a given n, the number of complex similarity classes
in a ﬁxed quaternion similarity class of n × n complex matrices does not exceed
2n.
Hint: The expression (5.8.4) achieves maximum when s = n, rj = 1 for
j = 1, 2, . . . , n, and pλj,k = 1.
Ex. 5.16.26. Prove that if system (5.14.1) is forward stable, then every nearby
system is also forward stable, in other words, there exists ε > 0 such that every
system
A′
ℓx(ℓ)(t) + A′
ℓ−1x(ℓ−1)(t) + · · · + A′
1x′(t) + A′
0x(t) = 0,
A′
ℓ, . . . , A′
1, A′
0 ∈Hn×n
is forward stable, provided
∥A′
j −Aj∥< ε,
for j = 0, 1, . . . , ℓ.
Ex. 5.16.27. Show by example that a property analogous to Ex. 5.16.26 gen-
erally fails for forward bounded systems (5.14.1).
Ex. 5.16.28. State and prove analogues of Ex. 5.16.26 and 5.16.27 for systems
(5.14.5).
Ex. 5.16.29. Show that the quaternion solution set of every equation of the
form (5.12.3) is compact.
Ex. 5.16.30. Let A =
 0
x
y
0

, x, y ∈H. Show that the eigenvalues of A are
as follows:
σ(A) =















{0}
if xy = 0;
{±√xy}
if xy is real and positive;
√−xy · u
if xy < 0;
u−1(±√xy)u
if xy ̸∈R,

130
CHAPTER 5
where u ∈H with R(u) = 0 and |u| = 1, but otherwise arbitrary. Hint: Use the
complex matrix representation of A.
5.17
NOTES
The proof of Proposition 5.1.4 is standard for complex matrices.
Propositions 5.2.2 and 5.2.4 are taken from Rodman [137].
It turns out that sums of root subspaces (as well as the zero subspace) are the
only invariant subspaces of matrices that enjoy the Lipschitz property as described
in Theorem 5.2.5. This result was proved in Kaashoek et al. [75] for complex matri-
ces and in Rodman [137] for quaternion matrices. In contrast, generally speaking,
there may exist isolated invariant subspaces of a quaternion matrix other than sums
of root subspaces or zero. Invariant subspaces of quaternion matrices that are well
behaved (in various senses) under small perturbations of the matrices have been
characterized in Rodman [137, 138].
The proof of Theorem 5.3.8 is taken from Zhang [164]. The result on existence
of complex eigenvalues with nonnegative imaginary parts was proved in Brenner
[18] and Lee [98].
Part of Proposition 5.3.7 (namely, that D(A) ⊆σ(A)) is proved in Zhang [164].
The Jordan form for quaternion matrices is well known and can be found in
many sources; see, e.g., Jacobson [71], Zhang [164], Farenick and Pidkowich [38],
or Karow [78].
The proof in Section 5.6 is adapted from Wiegmann [160].
Theorems 5.5.8 and 5.7.1 and Theorem 5.11.1 and its proof are taken from
Rodman [132].
The proof of Theorem 5.15.1 is standard in the literature; see, e.g., Theorem
IV.1.1 in Stewart and Sun [149].
The part of Theorem 5.12.4 about the number of quaternion similarity equiva-
lence classes that contain a solution of (5.12.4) is proved in Gordon and Motzkin
[57].
Example 5.3.3 is taken from Zhang [164].
The analysis of Proposition 5.4.2, Part (a), is standard for complex matrices
or, more generally, matrices over a ﬁeld; see, e.g., Lancaster and Tismenetsky [94,
Section 12.4] or Gantmacher [50, Chapter VIII].
Various notions of determinants for quaternion matrices have been studied in the
literature, besides the determinant introduced in Section 5.9: Study determinant
[150], Dieudonn´e determinant [32], also [5], double determinant (see Chen [24, 25]
and Zhang [164]).
See also Cohen and De Leo [28] and references there for an
axiomatic approach to quaternion determinants.
Exposition in Section 5.14 follows a standard approach to stability and bound-
edness of linear diﬀerential and diﬀerence systems of equations with constant coef-
ﬁcients.
Exercise 5.16.30 is taken from [146].
Proposition 5.12.6 was suggested by Bolotnikov [15].

Chapter Six
Invariant neutral and semideﬁnite subspaces
In this chapter we study subspaces that are simultaneously neutral or semideﬁnite
for one matrix and invariant for another.
The presentation here does not use
canonical forms for pairs of matrices, which is developed in later chapters.
In this and subsequent chapters we will often work with matrices that enjoy
certain symmetry properties. For the reader’s convenience, we collect the nomen-
clature associated with classes of such matrices A ∈Hn×n, where F = R, F = C, or
F = H, in three tables.
The notation B ≥C or C ≤B, where B, C ∈Fn×n are hermitian matrices,
indicates that the diﬀerence B −C is positive semideﬁnite. For φ-skewhermitian
matrices D ∈Hn×n, where φ is a nonstandard involution, we use the notation
β(φ)−1D ≥0 to indicate that β(φ)−1xφDx ≥0 for all x ∈Hn×1.
Hφ = H φ-hermitian
Hφ = −H φ-skewhermitian
F = H
F = H
AφH = HA
(H, φ)-symmetric
(H, φ)-skew-Hamiltonian
AφH = −HA
(H, φ)-skewsymmetric
(H, φ)-Hamiltonian
AφHA = H
(H, φ)-orthogonal
(H, φ)-symplectic
β−1AφHA ≥β−1H
(H, φ)-expansive
β−1xφAφHAx ≥0
if β−1xφHx ≥0
(H, φ)-plus-matrix
H∗= H hermitian
H∗= −H skewhermitian
F ∈{R, C, H}
F = H
A∗H = HA
(H,∗)-hermitian
(H,∗)-skew-Hamiltonian
A∗H = −HA
(H,∗)-skewhermitian
(H,∗)-Hamiltonian
A∗HA = H
(H,∗)-unitary
(H,∗)-symplectic
A∗HA ≥H
(H,∗)-expansive
x∗A∗HAx ≥0 if x∗Hx ≥0
(H,∗)-plus-matrix
HT = H symmetric
HT = −H skewsymmetric
F ∈{C, R}
F ∈{C, R}
AT H = HA
(H,T )-symmetric
(H,T )-skew-Hamiltonian
AT H = −HA
(H,T )-skewsymmetric
(H,T )-Hamiltonian
AT HA = H
(H,T )-orthogonal
(H,T )-symplectic
Several remarks are in order here:

132
CHAPTER 6
1. In the above tables, φ stands for a nonstandard involution, and β = β(φ).
2. In the real case T is the same as ∗.
3. The matrix H is generally not assumed to be invertible.
4. In the sequel, the ∗and T will be sometimes omitted in the above notation
if it is clear from the context; e.g., H-Hamiltonian will be used in place of
(H,∗)-Hamiltonian.
6.1
STRUCTURED MATRICES AND INVARIANT NEUTRAL
SUBSPACES
Let φ be an involution; thus, either φ is the conjugation or φ is a nonstandard
involution.
Deﬁnition 6.1.1. Given a φ-skewhermitian H ∈Hn×n, a matrix A ∈Hn×n is
said to be (H, φ)-Hamiltonian if (HA)φ = HA, i.e., HA is φ-hermitian. A matrix
W ∈H2n×2n is said to be (H, φ)-skew-Hamiltonian if the equality (HW)φ = −HW
holds. For a φ-hermitian H, a matrix A ∈Hn×n is said to be (H, φ)-symmetric
(for φ nonstandard) or (H,∗)-hermitian (for φ the conjugation) if (HA)φ = HA;
the matrix A is said to be (H, φ)-skewsymmetric (for φ nonstandard) or (H,∗)-
skewhermitian (for φ the conjugation) if (HA)φ = −HA.
We do not generally assume in this chapter that H is invertible; such assumption
will be made as necessary for certain particular results.
Deﬁnition 6.1.2. If H is φ-skewhermitian, then a matrix U ∈Hn×n is said to
be (H, φ)-symplectic if
UφHU = H.
(6.1.1)
Analogously, if H is φ-hermitian, then a matrix U ∈Hn×n is said to be (H,∗)-
unitary (for φ the conjugation) or (H, φ)-orthogonal (for φ nonstandard) if the
equality
UφHU = H
(6.1.2)
holds true.
It is easy to verify the following proposition.
Proposition 6.1.3. If U is (H, φ)-symplectic, resp. (H,∗)-unitary or (H, φ)-
orthogonal, and invertible, then so is U −1; also, if U, V are (H, φ)-symplectic, resp.
(H,∗)-unitary or (H, φ)-orthogonal, then so is UV .
Assuming in addition that H is invertible and H−1 = ±H, then if U is (H, φ)-
symplectic, resp. (H,∗)-unitary or (H, φ)-orthogonal, then so is Uφ.
Proof. We provide details only for the second part. Indeed, taking inverses in
the equality UφHU = H, we get
±H = H−1 = U −1H−1(Uφ)−1 = ±U −1H(Uφ)−1;
hence, UHUφ = H, which proves that Uφ is (H, φ)-symplectic.
□
Note that if H is φ-skewhermitian, if A is (H, φ)-Hamiltonian, resp. (H, φ)-
skew-Hamiltonian, and U is (H, φ)-symplectic and invertible, then U −1AU is also

INVARIANT NEUTRAL AND SEMIDEFINITE SUBSPACES
133
(H, φ)-Hamiltonian or (H, φ)-skew-Hamiltonian, as the case may be. An analogous
statement holds for (H, φ)-symmetric and (H, φ)-skewsymmetric matrices, where
H is φ-hermitian, with respect to (H, φ)-unitary matrices.
Fix a φ-hermitian or φ-skewhermitian matrix H. Recall that a subspace M ⊆
Fn×1 is called (H, φ)-neutral if yφHx = 0 for all x, y ∈M or, equivalently, if
xφHx = 0 for all x ∈M. The equivalence of these two deﬁnitions follows from the
polarization identities (3.5.1) and (3.7.1).
The classes of matrices identiﬁed in the above tables are appropriately trans-
formed under simultaneous conjugation of H and similarity of A.
Proposition 6.1.4. Let H ∈Fn×n be as in the above tables, and assume that
A belongs to one of the classes identiﬁed in the tables, e.g., A is (H, φ)-orthogonal.
Then for every invertible S ∈Fn×n, the matrix S−1AS belongs to the same class
associated with SφHS or S∗HS, as the case may be.
The proof is straightforward and is left to the reader.
Deﬁnition 6.1.5. Given an (H, φ)-Hamiltonian or (H, φ)-skew-Hamiltonian
matrix A ∈Hn×n, a subspace M ∈Hn×1 is called maximal A-invariant (H, φ)-
neutral if M is A-invariant and (H, φ)-neutral but no subspace strictly larger than
M is A-invariant and H-neutral.
Theorem 6.1.6. Let H = δHφ ∈Hn×n, where δ = ±1, and let φ be an involu-
tion. Let A ∈Hn×n be (H, φ)-Hamiltonian or (H, φ)-skew-Hamiltonian (if δ = −1),
or (H, φ)-symmetric or (H, φ)-skewsymmetric (if φ is nonstandard and δ = 1), or
(H,∗)-hermitian or (H,∗)-skewhermitian (if φ =∗is the conjugation and δ = 1).
Then all maximal A-invariant (H, φ)-neutral subspaces have the same dimension.
Proof. In the proof, we write H-neutral, short for (H, φ)-neutral.
Denote by [·, ·] the H-inner product with respect to φ:
[x, y] := yφHx = δφ([y, x]),
x, y ∈Hn×1.
Deﬁnition 6.1.7. If L is a subset of Hn×1, we deﬁne
L[⊥] := {x ∈Hn×1 : [x, y] = 0
for all y ∈L},
the H-orthogonal companion of L.
Clearly, L[⊥] is a subspace of Hn×1. We have
[Ax, y] = τδ[x, Ay],
for all x, y ∈Hn×1,
(6.1.3)
where τ = −1 if A is (H, φ)-Hamiltonian and τ = 1 if A is (H, φ)-skew-Hamiltonian.
Note also that if L ⊆Hn×1 is an A-invariant subspace, then L[⊥] is A-invariant as
well.
Let L and M be maximal A-invariant H-neutral subspaces of Hn×1. We will
prove they have the same dimension.
Step 1. Assume L ∩M = {0}.
Note that
dim (L ∩M[⊥]) ≥dim L −dim M.
(6.1.4)

134
CHAPTER 6
Indeed, let f1, . . . , fℓand g1, . . . , gm be bases for L and M, respectively. Then
L ∩M[⊥] consists of all vectors Pℓ
j=1 fjαj, where α1, . . . , αℓ∈H, such that
ℓ
X
j=1
[fj, gk]αj = 0,
k = 1, 2, . . . , m.
The solution space of this system clearly has dimension at least ℓ−m. Therefore,
if
L ∩M[⊥] = L[⊥] ∩M = {0},
(6.1.5)
then by (6.1.4) dim L = dim M, and we are done. Thus, assume (6.1.5) does not
hold, say, D := L ∩M[⊥] ̸= {0}. Clearly, D is A-invariant and H-neutral. Since
L ∩M = {0}, the subspace D ˙+ M is A-invariant and H-neutral and contains M
properly, a contradiction with maximality of M.
Step 2. Now assume R := L ∩M ̸= {0}.
Let C := L + M, an A-invariant subspace, and let V be a direct complement
of R in C: C = R ˙+ V.
Deﬁne the projection P of C onto V along R, and let
A0 : V →V be an H-linear transformation on V deﬁned by A0x = PAx, x ∈V. We
claim that
[A0x, y] = τδ[x, A0y],
for all x, y ∈V.
(6.1.6)
To see this, observe ﬁrst that
[Px, y] = [x −(I −P)x, y] = [x, y] = [x, Py + (I −P)y] = [x, Py],
(6.1.7)
for all x, y ∈C, in view of the H-neutrality of L and M. Now (6.1.6) follows easily
from (6.1.3), (6.1.7), and the deﬁnition of A0. Indeed, for any x, y ∈V:
[A0x, y]
=
[PAx, y] = [Ax, Py] = [Ax, y]
=
τδ[x, Ay] = τδ[Px, Ay] = τδ[x, PAy] = τδ[x, A0y].
Deﬁne L0 := L ∩V, M0 := M ∩V. Then L0 ∩M0 = {0}. Clearly, L0 and M0
are H-neutral. These subspaces are also A0-invariant. Indeed, let v ∈L0. Then
Av ∈C; hence, Av = bv + r, r ∈R, bv ∈V. Furthermore, A0v = Av −r, and as
R ⊆L, A0v ∈L. Obviously, Av = PAv ∈V, so Av0 ∈L ∩V as claimed. A similar
argument proves that M0 is A-invariant.
Fix a basis B := {b1, . . . , bk} for V, and represent A0 as a k × k matrix with
respect to this basis. Then A0 is H0-Hamiltonian (or H0-skew-Hamiltonian, as the
case may be), where the k × k matrix H0 = [(H0)i,j]k
i,j=1 is deﬁned by (H0)i,j =
[bi, bj]. Note that (H0)φ = δH0. Using the coordinate mapping with respect to
the basis B—which is an isomorphism—we transform the subspaces L0, M0 into
subspaces eL0, f
M0 ⊆Hk×1, respectively.
Clearly, eL0, f
M0 are A0-invariant H0-
neutral, and
dim eL0 = dim L0,
dim f
M0 = dim M0,
eL0 ∩f
M0 = {0}.
(6.1.8)
(The latter equality follows from L0 ∩M0 = {0}.) Also, since R ⊆L, we have
L = (L ∩R) ˙+ (L ∩V) = R ˙+ L0;

INVARIANT NEUTRAL AND SEMIDEFINITE SUBSPACES
135
hence, dim L0 = dim L −dim R and, similarly, dim M0 = dim M −dim R. So,
it remains to show that L0, M0 are maximal A0-invariant H-neutral subspaces
in V. Indeed, then we will have that eL0, f
M0 are maximal A0-invariant H0-neutral
subspaces in Hk×1, and the equality dim L = dim M will follow from Step 1 applied
to the subspaces eL0 and f
M0, in view of (6.1.8).
To prove this maximality property for L0 (analogously, it can be proved for
M0), let Q0 be any A0-invariant H-neutral subspace such that L0 ⊆Q0 ⊆V, and
deﬁne Q := R ˙+ Q0. Then Q ⊇R ˙+ L0 = L and Q is H-neutral (this is because
R = L ∩M, Q0 ⊆C = L + M, Q0 is H-neutral, and both M, L are H-neutral).
We claim that Q is also A-invariant. To conﬁrm this, let q ∈Q and write q = q0+r,
q0 ∈Q0, r ∈R. Then
Aq = Aq0 + Ar = A0q0 + (I −P)Aq0 + Ar.
The last two terms are in R and the ﬁrst term is in Q0. By the maximality property
of L we must have Q = L; hence, Q0 = L0, and we are done.
□
Deﬁnition 6.1.8. The dimension identiﬁed in Theorem 6.1.6 is called the order
of neutrality of the pair (H, A) and denoted γ(H, A).
(This terminology was introduced in Lancaster et al. [85] for complex matrices
that are selfadjoint with respect to an indeﬁnite inner product.)
Open Problem 6.1.9. (a) Express the order of neutrality in terms of parame-
ters of the canonical forms for the pair (H, A), which are given in Chapters 10 and
13 (in the case of invertible H).
(b) For a given pair (H, A) of matrices as in Theorem 6.1.6, describe the set of
all maximal A-invariant (H, φ)-neutral subspaces in terms of analytic manifolds.
For Hamiltonian matrices, the order of neutrality has been identiﬁed in Rodman
[140]. For Part (b), Open Problem 5.1.5 may be relevant.
We remark that a result analogous to Theorem 6.1.6 but with respect to a pair
of φ-hermitian matrices (A, B) (where we take φ to be the conjugation) generally
does not hold, in other words, the subspaces that are simultaneously A-neutral and
B-neutral and are maximal with respect to this property need not have the same
dimension. The following example illustrates that.
Example 6.1.10. Let
A = A∗=


0
0
1
−1
0
1
0
0
1
0
0
0
−1
0
0
0

,
B = B∗=


0
0
0
1
0
0
1
0
0
±1
0
0
±1
0
0
0

∈H4×4.
Obviously, there exists a two-dimensional (A, B)- neutral subspace: SpanH {e3, e4}.
The subspace SpanH {e1} is also (A, B)-neutral. But it is not contained in any
two-dimensional (A, B)-neutral subspace. Indeed, if
M := SpanH {e1, e2x2 + e3x3 + e4x4} ,
x1, x2, x3 ∈H,

136
CHAPTER 6
were such a subspace, then we would have
0
=
 1
0
0
0 
A


0
x1
x2
x3

= x2 −x3,
0
=
 1
0
0
0 
B


0
x1
x2
x3

= x3,
0
=
 0
x1
x2
x3
∗A


0
x1
x2
x3

= |x1|2;
thus, x1 = x2 = x3 = 0, a contradiction with M being two-dimensional. So, M is
a maximal (A, B)-neutral subspace.
Note that in this example B−1A is (B,∗)-hermitian. However, the subspace
SpanH {e1} is not B−1A-invariant, whereas SpanH {e3, e4} is, as conﬁrmed by The-
orem 6.1.6.
□
Open Problem 6.1.11. Identify those pairs of hermitian matrices for which
the maximal neutral subspaces have the same dimension. An analogous question
occurs for pair of matrices that are φ-hermitian or φ-skewhermitian with respect to
a nonstandard involution φ.
6.2
INVARIANT SEMIDEFINITE SUBSPACES RESPECTING
CONJUGATION
In this and the next three sections we study subspaces of Hn×1 which are invariant
for a matrix in certain class and simultaneously have deﬁniteness properties with
respect to the indeﬁnite inner product induced by a hermitian matrix.
Let H = H∗∈Hn×n. The matrix H will be ﬁxed throughout this and the next
three sections. We denote by [ · , · ] the indeﬁnite scalar product in Hn×1 induced
by H:
[x, y] := ⟨x, Hy⟩= ⟨Hx, y⟩= y∗Hx,
x, y ∈Hn×1.
Several classes of subspaces of Hn×1 will be of main interest in this and the
next sections. These were introduced in Section 4.2; we recall the deﬁnitions. The
classes of nonnegative, nonpositive, and neutral subspaces deﬁned by
Sν(H) = {V : V is a subspace of Hn×1 such that [x, x]ν0 for every x ∈V},
where the symbol ν is ≥, ≤, =, respectively, and the classes of positive and negative
subspaces are deﬁned by
Sν0(H) = {V : V is a subspace of Hn×1 such that [x, x]ν0 for every x ∈V \ {0}},
where ν = > and ν = <, respectively.

INVARIANT NEUTRAL AND SEMIDEFINITE SUBSPACES
137
Deﬁnition 6.2.1. A matrix A ∈Hn×n is called an H-plus-matrix if [Ax, Ax] ≥0
for every x ∈Hn×1 such that [x, x] ≥0.
A useful characterization of H-plus-matrices is given in the next proposition.
Proposition 6.2.2. A ∈Hn×n is an H-plus-matrix if and only if there exists a
nonnegative µ such that
[Ax, Ax] ≥µ[x, x]
for all x ∈Hn×1.
(6.2.1)
Proof. The ‘if” part is clear from the deﬁnition of H-plus-matrices. Suppose A
is an H-plus-matrix. Then the joint numerical range WJH
∗(A∗HA, H) ⊆R2 does
not intersect the set
{(a, b) ∈R2 : b ≥0 and a < 0}.
(6.2.2)
Since the set WJH
∗(A∗HA, H) is compact and convex (Theorem 3.5.7), there is
a line that separates WJH
∗(A∗HA, H) from the convex set (6.2.2); see Theorem
6.6.2 in the Appendix. It is easy to see that the line can be taken to pass through
the origin and not to be horizontal. Then the inverse of the slope of this line is a
suitable value of µ.
□
Note that µ need not be unique (for a given A).
Example 6.2.3. Let
H =
 1
0
0
−1

,
A =
 1
0
0
0

.
Then for x =
 x1
x2

, where x1, x2 ∈H, we have [x, x] = |x1|2 −|x2|2, [Ax, Ax] =
|x1|2, so (6.2.1) holds true for any µ ∈[0, 1].
□
Deﬁnition 6.2.4. An H-plus-matrix A is said to be strict H-plus-matrix if there
exists µ > 0 satisfying Proposition 6.2.2.
We will write µ(A) in place of µ if the dependence on A is to be emphasized.
Observe that these classes of matrices and their invariant subspaces transform
with respect to simultaneous congruence of H and similarity of A.
Proposition 6.2.5. Let H = H∗∈Hn×n. Then:
(1) if A is H-plus, resp. strict H-plus, and S ∈Hn×n is invertible, then S−1AS
is S∗HS-plus, resp. strict S∗HS-plus;
(2) a subspace M ⊆Hn×1 belongs to the class Sν(H) if and only if S−1(M)
belongs to Sµ(S∗HS), where ν is one of the symbols {≥0, > 0, ≤0, < 0, 0};
(3) a subspace M is A-invariant if and only if S−1(M) is S−1AS-invariant.
The proof is by straightforward veriﬁcation.
The key result in this section is the following theorem on extension of H-
nonnegative subspaces that are invariant for H-plus-matrices.
Theorem 6.2.6. Assume H is invertible, and let A be an invertible H-plus-
matrix. Then:

138
CHAPTER 6
(a) for every A-invariant H-nonnegative subspace M0 ⊆Hn×1 there exists an
H-nonnegative A-invariant subspace f
M ⊇M0 such that
dim f
M = In+ (H);
(6.2.3)
(b) for every A-invariant H-nonpositive subspace M′
0 ⊆Hn×1 there exists an
H-nonpositive A-invariant subspace f
M′ ⊇M′
0 such that
dim f
M′ = In−(H).
(6.2.4)
Note that by Theorem 4.2.6(a) the right-hand side of (6.2.3), resp.
(6.2.4),
represents the maximal dimension of an H-nonnegative, resp. H-nonpositive, sub-
space.
The hypothesis that A is invertible is essential in Theorem 6.2.6. The following
example illustrates this.
Example 6.2.7. Let
A =


0
1
0
p
−1
0
1
q
0
0
0
0
0
0
0
0


,
H =


0
0
1
0
0
0
0
1
1
0
0
0
0
1
0
0


,
where p and q are real numbers such that 4p + q2 > 0. Clearly, A is an H-plus-
matrix; in fact, [Ax, Ax] = 0 for every x ∈H4×1. Let
M0 = SpanH [1 0 1 0]T .
Then AM0 = {0}, in particular, M0 is A-invariant. Since [1 0 1 0] H(e1+e3) = 2,
the subspace M0 is H-positive. On the other hand,
Ker A = SpanH









1
0
1
0

,


0
−p
−q
1









is not H-nonnegative (this is where the condition 4p + q2 > 0 is needed). Besides
Ker A, there is a continuum of 2-dimensional A-invariant subspaces that contain
M0, namely,
SpanH









1
0
1
0

,


1
α
0
0









,
α ∈H,
α2 = −1.
But
 1
α∗
0
0
1
0
1
0

H


1
1
α
0
0
1
0
0

=
 0
1
1
2

,
so none of these subspaces is H-nonnegative.
□

INVARIANT NEUTRAL AND SEMIDEFINITE SUBSPACES
139
We shall see later that the condition of invertibility of A can be relaxed (for Part
(a) of Theorem 6.2.6). Namely, the hypothesis that Ran A is not H-nonnegative,
or that A is a strict H-plus-matrix, will suﬃce (Corollary 6.4.3 below).
The proof of Theorem 6.2.6 is rather involved. It will be given in a separate
section.
Open Problem 6.2.8. Extend if possible the result of Theorem 6.2.6 to cases
when H and/or A are not necessarily invertible.
Some results in this direction will be given later on in Section 8.6.
6.3
PROOF OF THEOREM 6.2.6
First observe that it will suﬃce to prove Part (a). Indeed, we note that the inverse
matrix A−1 is (−H)-plus-matrix, as easily follows by post- and premultiplying the
positive semideﬁnite matrix A∗HA −H by (A∗)−1 and A−1, respectively, and ob-
serving that multiplying by −1 turns a positive semideﬁnite matrix into a negative
semideﬁnite one. Then applying part (a) to the (−H)-plus-matrix A−1 yields part
(b) for the H-plus-matrix A (and note that if A is invertible, then A and A−1 have
exactly the same set of invariant subspaces).
Therefore, we will prove part (a) only. We need some preliminaries for the proof.
First of all, we can assume that H has the form
H =
"
Ip
0
0
−Iq
#
.
(6.3.1)
Indeed, by applying a congruence H →S∗HS for a suitable invertible matrix S,
and simultaneously transforming A →S−1AS and M0 →S−1M0, the form (6.3.1)
can always be achieved, by Theorem 4.1.6.
With respect to the form (6.3.1) the H-nonnegative subspaces can be conve-
niently described, as in the next lemma.
Lemma 6.3.1. Let H be given by (6.3.1).
Then a subspace M ⊆Hn×1 of
dimension d is H-nonnegative if and only if M has the form
M = Ran

P
K

,
(6.3.2)
where P ∗P = I and K ∈Hq×d satisﬁes ∥K∥≤1.
The case when d = 0 is not excluded; in this case M is interpreted as the zero
subspace.
Proof. It is easy to see that every subspace of the form (6.3.2) is H-nonnegative.
Indeed, let
"
x1
x2
#
=
"
P
K
#
y

140
CHAPTER 6
for some y ∈Hd×1. Then
*
H
"
x1
x2
#
,
"
x1
x2
#+
=
*"
Ip
0
0
−Iq
# "
P
K
#
y,
"
P
K
#
y
+
=
⟨Py, Py⟩−⟨Ky, Ky⟩
=
⟨P ∗Py, y⟩−⟨Ky, Ky⟩
=
∥y∥2 −∥Ky∥2 ≥0,
(6.3.3)
where the last inequality follows in view of ∥K∥≤1.
Conversely, let M ∈S≥0(H), dim M = d. Let f1, . . . , fd be a basis of M (we
ignore the trivial case M = {0}) and partition:
fj =
"
f1j
f2j
#
,
f1j ∈Hp×1, f2j ∈Hq×1.
The vectors f11, . . . , f1d are linearly independent; otherwise, M would contain a
nonzero vector of the form
"
0p
f
#
, f ∈Hq×1 \ {0}, a contradiction with M being
H-nonnegative. Now let
P = [f11 f12 · · · f1d] T ∈Hp×d,
where the invertible matrix T is chosen so that P ∗P = Id. This choice is always
possible, because in view of the linear independence of f11, . . . , f1d, the matrix
[f11 f12 · · · f1d]∗[f11 f12 · · · f1d]
is invertible and, hence, positive deﬁnite. Finally, let
K = [f21 f22 · · · f2d]T ∈Hq×d.
With these deﬁnitions of P and K, the veriﬁcation of the formula (6.3.2) is imme-
diate, because
M
=
Ran[f1 · · · fd] = Ran
"
f11 · · · f1d
f21 · · · f2d
#
=
Ran
"
f11 · · · f1d
f21 · · · f2d
#
T = Ran
"
P
K
#
.
Using the equality P ∗P = I, one veriﬁes as in (6.3.3) that
*
H
"
P
K
#
y,
"
P
K
#
y
+
= ∥y∥2 −∥Ky∥2 ≥0,
for every y ∈Hd×1, and hence, ∥K∥≤1.
□

INVARIANT NEUTRAL AND SEMIDEFINITE SUBSPACES
141
Note that the representation (6.3.2) for a given M is not unique. To study the
problem of describing this nonuniqueness, ﬁrst of all observe that if
Ran
"
P
K1
#
= Ran
"
P
K2
#
,
(6.3.4)
where P ∗P = Id, then K1 = K2; in other words, the matrix K in (6.3.2) is uniquely
determined by M and P. The veriﬁcation is easy: if (6.3.4) holds and P ∗P = Id,
then, in particular, the columns of
"
P
K1
#
and of
"
P
K2
#
are linearly independent,
and we have
"
P
K1
#
=
"
P
K2
#
W
for some invertible matrix W. Consequently, P = PW. Multiplying this equality
on the left by P ∗, it follows that W = I. But then, also, K1 = K2. As for the
nonuniqueness of P in (6.3.2), assume that
Ran
"
P1
K1
#
= Ran
"
P2
K2
#
,
where P ∗
1 P1 = P ∗
2 P2 = Id. Then
"
P1
K1
#
= Ran
"
P2
K2
#
W
(6.3.5)
for a unique invertible matrix W.
In fact, W = P ∗
2 P1, as can be veriﬁed by
multiplying the equality P1 = P2W on the left by P ∗
2 . Moreover, (6.3.5) implies
P1W −1 = P2, and, therefore, W −1 = P ∗
1 P2. We see that W −1 = W ∗; in other
words, W is unitary. The following result is obtained.
Lemma 6.3.2. If
Ran
"
P1
K1
#
= Ran
"
P2
K2
#
,
(6.3.6)
where P1 and P2 are p × d matrices such that P ∗
1 P1 = P ∗
2 P2 = I, then P1 = P2W,
K1 = K2W for some unitary d × d matrix W. Such unitary matrix W is unique.
In particular, Lemma 6.3.2 applies to an H-nonnegative subspace of the form
(6.3.6). However, for the validity of the lemma, it is not necessary that ∥K1∥≤1,
∥K2∥≤1.
Corollary 6.3.3. Every maximal H-nonnegative subspace M can be written
uniquely in the form
M = Ran
"
Ip
K
#
,
(6.3.7)
where K ∈Hq×p satisﬁes ∥K∥≤1. Conversely, every subspace of the form (6.3.7)
is maximal H-nonnegative.

142
CHAPTER 6
Indeed, the uniqueness of (6.3.7) follows from Lemma 6.3.2. Existence of (6.3.7)
follows from Lemma 6.3.1 in which P is unitary, because in view of Theorem 4.2.6(a)
the dimension of M is equal to p.
Next, we express containment of an H-nonnegative subspace in a maximal such
subspace, in terms of the representation (6.3.2).
Lemma 6.3.4. Let
M0 = Ran
"
P0
K0
#
∈S≥0(H),
M = Ran
"
Ip
K
#
∈S≥0(H),
where P0 ∈Hp×d, K0 ∈Hq×d, K ∈Hq×p are such that P ∗
0 P0 = I and ∥K0∥≤1,
∥K∥≤1. (In particular, M is maximal H-nonnegative.) Then M0 ⊆M if and
only if K0 = KP0.
Proof. If K0 = KP0, then obviously
"
P0
K0
#
=
"
I
K
#
P0,
and, therefore, M0 ⊆M. Conversely, if M0 ⊆M, then there exists a matrix B
such that
"
P0
K0
#
=
"
I
K
#
B.
It is immediate from this equality that in fact B = P0.
□
Proof of Theorem 6.2.6. Let M0 be as in Theorem 6.2.6. Represent M0
according to Lemma 6.3.1:
M0 = Ran
"
P0
K0
#
,
where P ∗
0 P0 = I, ∥K0∥≤1. On the other hand, consider the set S of all maximal
H-nonnegative subspace M that contain M0. Writing
M = Ran
"
I
K
#
∈S,
in view of Lemma 6.3.4 we can identify S with the set of all q × p matrices K such
that ∥K∥≤1 and K0 = KP0. Note that S is closed and bounded convex set of
Hpq (if we identify the vector space of q × p quaternion matrices with Hpq). Next,
observe that
A(M) := {Ax : x ∈M} ∈S
for all M ∈S.
Indeed, the hypothesis that A is an H-plus-matrix implies that A(M) ∈S≥0(H)
for every M ∈S≥0(H). Since A is invertible, dim A(M) = dim M = In+ (H)
for every maximal H-nonnegative subspace M, which implies by Theorem 4.2.6
that A(M) is maximal H-nonnegative as well. Finally, since A(M0) = M0, we
obviously have A(M) ⊇M0 for every subspace M ⊇M0.
In other words, A maps S into itself. Identifying S with the set
K := {K ∈Hq×p| ∥K∥≤1 and K0 = KP0},

INVARIANT NEUTRAL AND SEMIDEFINITE SUBSPACES
143
the map eA induced by A is a continuous function of K ∈K. To verify this, write
A =
"
A11
A12
A21
A22
#
,
where A11 is p × p and A22 is q × q, and note that the equality
A
 
Ran
"
I
K
#!
= Ran
"
I
eA(K)
#
,
where ∥K∥≤1, K0 = KP0, can be rewritten in the form
"
A11
A12
A21
A22
# "
I
K
#
X =
"
I
eA(K)
#
,
for some invertible matrix X. It follows that X = (A11 + A12K)−1 and
eA(K) = (A21 + A22K)(A11 + A12K)−1.
(Note that the invertibility of A11 + A12K is guaranteed for every K ∈K.) This is
obviously a continuous function of K in K. Now the ﬁxed point theorem (Theorem
6.6.3 in the Appendix) guarantees existence of K′ ∈Hq×p such that ∥K′∥≤1,
K0 = K′P0 and eA(K′) = K′. Then
f
M = Ran
"
I
K′
#
satisﬁes all the requirements of Theorem 6.2.6.
□
6.4
UNITARY, DISSIPATIVE, AND EXPANSIVE MATRICES
Theorem 6.2.6 and its proof have many important corollaries. As in the preceding
two sections, we ﬁx in this section an invertible hermitian matrix H ∈Hn×n, and
let [x, y] = y∗Hx, where x, y ∈Hn×1.
To start with, every H-unitary matrix is obviously invertible and an H-plus-
matrix.
Theorem 6.4.1. Assume H = H∗∈Hn×n is invertible. Let A be H-unitary,
and let M0 ⊆Hn×1 be an A-invariant H-nonnegative (resp. H-nonpositive) sub-
space. Then there exists an A-invariant H-nonnegative (resp. H-nonpositive) sub-
space M such that M ⊇M0 and dim M = In+ (H) (resp. dim M = In−(H)).
The part of Theorem 6.4.1 concerning H-nonpositive subspaces follows by ap-
plying Theorem 6.2.6, Part (b).
We remark next that the proof of Theorem 6.2.6 shows the following result to
be true (relaxation of the invertibility condition in Theorem 6.2.6, Part (a)).
Theorem 6.4.2. Assume H is invertible. Let A be an H-plus-matrix having the
property that Ax ̸= 0 for all x ∈Hn×1 \{0} such that [x, x] ≥0. Let M0 ⊆Hn×1 be
an A-invariant subspace that is H-nonnegative. Then there exists H-nonnegative
A-invariant subspace f
M ⊇M0 such that dim f
M = In+ (H).

144
CHAPTER 6
Corollary 6.4.3. Assume H is invertible.
Let A be a strict H-plus-matrix.
Then for every A-invariant H-nonnegative subspace M0 ⊆Hn×1, there exists an
H-nonnegative A-invariant subspace f
M ⊇M0 such that dim f
M = In+ (H).
Proof. We verify that the conditions of Theorem 6.4.2 are satisﬁed. Assume
Ax = 0 for some x ∈Hn×1 \ {0} such that [x, x] ≥0. Since A is strict H-plus,
the matrix A∗HA −µH is positive semideﬁnite for some µ > 0. It follows that
−µ[x, x] ≥0, and so [x, x] = 0. Now
x∗(A∗HA −µH)x = 0,
and the positive semideﬁniteness of A∗HA −µH implies (A∗HA −µH)x = 0 (cf.
Ex. 4.6.10), which, in turn, yields Hx = 0. Thus, x = 0 in view of invertibility of
H, a contradiction.
□
Next, we use linear fractional transformations.
Deﬁnition 6.4.4. A matrix A ∈Hn×n is said to be H-expansive if [Ax, Ax] ≥
[x, x] for every x ∈Hn×1 or, equivalently, if A∗HA −H is positive semideﬁnite. A
matrix B ∈Hn×n is called H-dissipative if R([Bx, x]) ≤0 for every x ∈Hn×1.
The dissipativity condition can be easily interpreted in terms of negative semidef-
initeness: A matrix B is H-dissipative if and only if
HB + B∗H ≤0
(negative semideﬁnite).
(6.4.1)
Note that A is H-expansive, resp. H-dissipative, if and only if S−1AS is S∗HS-
expansive, resp. S∗HS-dissipative, for any invertible matrix S ∈Hn×n.
Lemma 6.4.5.
(a) Let A ∈Hn×n be H-expansive, and assume that η ̸∈σ (A),
where η = ±1. Then
B := −(A + ηI)(A −ηI)−1
(6.4.2)
is H-dissipative and −1 ̸∈σ (B).
(b) Let B ∈Hn×n be H-dissipative, and let ρ be a negative number not an eigen-
value of B. Then
A := (B −ρI)−1(ρI + B)η,
η = ±1,
(6.4.3)
is H-expansive and η ̸∈σ (A).
Proof. The proof of (a) is by straightforward veriﬁcation: if A is H-expansive
with ±1 = η ̸∈σ (A) and (6.4.2) holds, then
HB + B∗H
=
−H(A + ηI)(A −ηI)−1 −(A∗−ηI)−1(A∗+ ηI)H
=
−(A∗−ηI)−1 ((A∗−ηI)H(A + ηI)
+ (A∗+ ηI)H(A −ηI)) (A −ηI)−1
=
(A∗−ηI)−1(−2A∗HA + 2H)(A −ηI)−1,
and since A∗HA −H is positive semideﬁnite, the matrix
(A∗−ηI)−1(−2A∗HA + 2H)(A −ηI)−1

INVARIANT NEUTRAL AND SEMIDEFINITE SUBSPACES
145
is negative semideﬁnite. If we had Bx = −x for some x ∈Hn×1, then
(A + ηI)x = (A −ηI)x,
which is possible only if x = 0. So −1 ̸∈σ (B).
For Part (b), if B is H-dissipative with ρ ̸∈σ (B) and A is given by (6.4.3),
then
A∗HA −H
=
(ρI + B∗)(B∗−ρI)−1H(B −ρI)−1(ρI + B) −H
=
(B∗−ρI)−1 ((ρI + B∗)H(ρI + B)
−(B∗−ρI)H(B −ρI)) (B −ρI)−1
=
(B∗−ρI)−1(2ρB∗H + 2ρHB)(B −ρI)−1,
and the positive semideﬁniteness of A∗HA −H follows. One checks easily that η
cannot be an eigenvalue of A.
□
The transformations in (6.4.2) and (6.4.3) are, in fact, the inverses of each other
if ρ = −1. Thus, denote
g(t) = −(t + η)(t −η)−1,
t ∈R,
and
h(t) = η(t −ρ)−1(t + ρ),
t ∈R,
where η = ±1. Then for every matrix A ∈Hn×n not having eigenvalue η, the
number −1 is not an eigenvalue of the matrix B = g(A), and
A = h(B),
for ρ = −1.
(6.4.4)
Here the matrices g(A) and h(B) are understood in the sense of functions of matri-
ces; they are also given by formulas (6.4.2) and (6.4.3), respectively. The veriﬁcation
of (6.4.4) is easy:
h(g(t)) =

−t + η
t −η −ρ
−1 
−t + η
t −η + ρ

η,
(6.4.5)
which, for ρ = −1, after some simple algebra is seen to be equal to t. Therefore,
by the properties of functions of matrices, h(g(A)) = A.
Theorem 6.4.6. Assume H = H∗∈Hn×n is invertible. Let B ∈Hn×n be
H-dissipative.
(a) Let M0 ⊆Hn×1 be a B-invariant H-nonnegative subspace. Then there exists
a B-invariant maximal H-nonnegative subspace M such that M ⊇M0.
(b) Let M′
0 ⊆Hn×1 be a B-invariant H-nonpositive subspace. Then there exists
a B-invariant maximal H-nonpositive subspace M′ such that M′ ⊇M′
0.
Proof. Assume M0 is H-nonnegative. Let A be given by (6.4.3). Then A is
H-expansive and, in particular, A is a strict H-plus-matrix. Note also that M0
is A-invariant, because A is a function of B. By Corollary 6.4.3 there exists an
A-invariant subspace M which is maximal H-nonnegative and contains M0. Since
B is a function of A (given by formula (6.4.2)), M is also B-invariant. This proves

146
CHAPTER 6
part (a) of Theorem 6.4.6. If M′
0 is H-nonpositive, apply the already proved part
(a) of Theorem 6.4.6 to the (−H)-dissipative matrix −B.
□
We conclude this section with an open problem concerning spectral properties of
restrictions of H-expansive matrices to invariant maximal semideﬁnite subspaces.
The following result is known for complex matrices (proved in Iohvidov et al.
[70] in the context of linear operators on inﬁnite dimensional spaces).
Theorem 6.4.7. Assume H = H∗∈Cn×n is invertible, and let A ∈Cn×n be
H-expansive. Then:
(1) there exist A-invariant H-nonnegative subspaces M ⊆Cn×1 of dimension
In+ (H) and such that |λ| ≥1 for every eigenvalue λ of A|M;
(2) there exist A-invariant H-nonpositive subspaces M ⊆Cn×1 of dimension
In−(H) and such that |λ| ≤1 for every eigenvalue λ of A|M;
(3) every subspace M with the properties as in (1), resp. (2), contains all root
subspaces of A corresponding to its eigenvalues λ with |λ| > 1, resp. |λ| < 1.
The hypothesis on invertibility of H is essential in Theorem 6.4.7 (take H = 0
to see that).
Open Problem 6.4.8. Extend if possible the result of Theorem 6.4.7 to quater-
nion matrices.
6.5
INVARIANT SEMIDEFINITE SUBSPACES: NONSTANDARD
INVOLUTION
In this section we ﬁx a nonstandard involution φ. We study invariant semideﬁnite
subspaces with respect to φ.
Let H ∈Hn×n is a φ-skewhermitian matrix, i.e., Hφ = −H.
Recall the
deﬁnitions of (H, φ)-nonnegative, (H, φ)-nonpositive, (H, φ)-positive, and (H, φ)-
negativematrices given in Deﬁnition 4.2.2.
A subspace M ⊆Hn× is said to be
(H, φ)-neutral if β(φ)−1yφHx = 0 for all x, y ∈M.
The next observation follows from Proposition 4.1.7.
Proposition 6.5.1. Let H0 = H∗
0 ∈Hn×n.
Then a subspace M ⊆Hn×1
is (β(φ)H0, φ)-nonnegative, resp.
(β(φ)H0, φ)-nonpositive, (β(φ)H0, φ)-positive,
(β(φ)H0, φ)-negative, or (β(φ)H0, φ)-neutral, if and only if M is (H0,∗)-nonnega-
tive, resp. (H0,∗)-nonpositive, (H0,∗)-positive, (H0,∗)-negative, or (H0,∗)-neutral.
Proposition 6.5.1 allows us to obtain results concerning semideﬁnite or neutral
subspaces with respect to a nonstandard involution from the corresponding results
with respect to the conjugation. As an example of using this approach, see Theorem
6.5.5 below.
Let H ∈Hn×n be φ-skewhermitian.
It will be convenient to introduce the
(H, φ)-inner product by the formula
[x, y]H,φ = β(φ)−1yφHx,
for all x, y ∈Hn×1.
Note that [x, x]H,φ is real-valued.

INVARIANT NEUTRAL AND SEMIDEFINITE SUBSPACES
147
Deﬁnition 6.5.2. A matrix A ∈Hn×n is said to be (H, φ)-plus-matrix if the
inequality [Ax, Ax]H,φ ≥0 holds true for every x ∈Hn×1 such that [x, x]H,φ ≥0.
By analogy with Proposition 6.2.2, we obtain that A is an (H, φ)-plus-matrix if
and only if there exists a nonnegative µ such that
[Ax, Ax]H,φ ≥µ[x, x]H,φ,
for all x ∈Hn×1.
The proof of this statement can be obtained analogously to that of Proposition
6.2.2 (using Theorem 3.7.13 instead of Theorem 3.5.7).
Deﬁnition 6.5.3. A matrix A ∈Hn×n is called a strict (H, φ)-plus-matrix if µ
can be chosen to be positive (note that in general µ is not unique).
We write µ = µ(A) if we wish to emphasize the particular (H, φ)-plus-matrix A
to which the number µ refers.
Proposition 6.5.4. Let H = −Hφ ∈Hn×n. Then:
(1) if A is (H, φ)-plus, resp. strict (H, φ)-plus, and S ∈Hn×n is invertible, then
S−1AS is (SφHS, φ)-plus, resp.
strict (SφHS, φ)-plus; moreover, µ(A) =
µ(S−1AS);
(2) a subspace M ⊆Hn×1 is (H, φ)-nonnegative, resp. (H, φ)-negative, (H, φ)-
nonpositive, (H, φ)-positive, or (H, φ)-neutral, if and only if the subspace
S−1(M) is (SφHS, φ)-nonnegative, resp.
(SφHS, φ)-negative, (SφHS, φ)-
nonpositive, (SφHS, φ)-positive, or (SφHS, φ)-neutral;
(3) a subspace M is A-invariant if and only if S−1(M) is S−1AS-invariant.
Proof. We prove only (1), leaving proof of the rest for the reader.
Assume A is (strictly) (H, φ)-plus. Set B = S−1AS. Then the following string
of equalities and an inequality proves (1):
[Bx, Bx]SφHS,φ
=
β(φ)−1xφBφ(SφHS)Bx
=
β(φ)−1xφ(SφAφS−1
φ (SφHS)S−1ASx
=
β(φ)−1(Sx)φAφHA(Sx)
≥
µ(A) β(φ)−1(Sx)φHSx = µ(A) [x, x]SφHS,φ,
for all ∈Hn×1.
□
An analogue of Theorem 6.2.6 on extension of invariant semideﬁnite subspaces
holds for (H, φ)-plus-matrices.
Theorem 6.5.5. Assume H is invertible, and let A be an invertible (H, φ)-plus-
matrix. Denote by (In+ (H), In−(H), 0) the β(φ)-signature of the φ-skewhermitian
matrix H. Then:
(a) for every A-invariant (H, φ)-nonnegative subspace M0 ⊆Hn×1 there exists
an (H, φ)-nonnegative A-invariant subspace f
M ⊇M0 such that
dim f
M = In+ (H);
(6.5.1)

148
CHAPTER 6
(b) for every A-invariant (H, φ)-nonpositive subspace M′
0 ⊆Hn×1, there exists
an (H, φ)-nonpositive A-invariant subspace f
M′ ⊇M′
0 such that
dim f
M′ = In−(H).
(6.5.2)
Proof. In view of Proposition 6.5.4 and Theorem 4.1.2(b,) we may (and do)
assume that H = diag (β(φ)Ip, β(φ)(−Iq), 0n−p−q), for some integers p, q, 0 ≤p, q ≤
p + q ≤n. It remains to apply Proposition 6.5.1 with H0 = diag (Ip, −Iq, 0n−p−q)
and refer to Theorem 6.2.6.
□
6.6
APPENDIX: CONVEX SETS
Deﬁnition 6.6.1. A set X ⊆Rd is said to be convex it contains the line segment
between any two points in the set: x, y ∈X implies tx + (1 −t)y ∈X for every
t ∈[0, 1].
We present here two standard and important results concerning convex sets.
The ﬁrst one has to do with separation between convex sets. We formulate the
result in the form that is used in the text, without attempting to put it in a more
general formulation.
Theorem 6.6.2. Let X ⊆R2 be a convex compact set. Then:
(a) if Y ⊆R2 is a convex set, not necessarily compact, such that Y ∩X = ∅, then
there exists a line separating X and Y : there exist real numbers a, b, c with
a, b not both zero such that
X
⊆
{(x1, x2) ∈R2 : ax1 + bx2 ≥c},
Y
⊆
{(x1, x2) ∈R2 : ax1 + bx2 ≤c};
(b) if, in addition to the hypotheses in part (a), Y is compact, then there exists a
line strictly separating X and Y : there exist real numbers a, b, c1, c2 with a, b
not both zero and c1 < c2 such that
X
⊆
{(x1, x2) ∈R2 : ax1 + bx2 ≥c2},
Y
⊆
{(x1, x2) ∈R2 : ax1 + bx2 ≤c1}.
A proof can be found in many books on convex analysis; see, e.g., Rockafellar
[128, Theorems 11.3, 11.4] and Lay [96, Section 4].
The second result is the Brouwer ﬁxed point theorem. Again, it is presented in
the form to be used in the book (not in full generality).
Theorem 6.6.3. Let X ⊆Rd be a nonempty compact convex set, and let F :
X −→X be a continuous map. Then F has a ﬁxed point, i.e., there exists x ∈X
such that F(x) = x.
See, e.g., Border [17, Corollary 6.6] or Webster [158, Section 7.5], for a proof.

INVARIANT NEUTRAL AND SEMIDEFINITE SUBSPACES
149
6.7
EXERCISES
Ex. 6.7.1. If A and B are two commuting (H, φ)-Hamiltonian matrices, then
AB is (H, φ)-skew-Hamiltonian.
Ex. 6.7.2. Let φ be the conjugation or a nonstandard involution, and let H ∈
Hn×n be φ-skewhermitian. Show that if f(t) is a polynomial of t ∈R with real
coeﬃcients such that f(−t) = −f(t) and if A is (H, φ)-Hamiltonian, then so is
f(A).
Ex. 6.7.3. With φ and H as in Ex 6.7.2, show that if f(t) is a polynomial of
t ∈R with real coeﬃcients such that f(−t) = f(t), and if A is (H, φ)-Hamiltonian,
then f(A) is (H, φ)-skew-Hamiltonian.
Ex. 6.7.4. Let A ∈Hn×n be an H-plus-matrix, where H = H∗is not negative
deﬁnite. Show that the set of all nonnegative µ such that the equality [Ax, Ax] ≥
[x, x] holds for all x ∈Hn×1 forms a closed bounded interval in the half-line [0, ∞).
Hint: µ belongs to the set precisely when the hermitian matrix A∗HA −µH is
positive semideﬁnite. Now consider µ as a parameter and use continuity of the
smallest eigenvalue of A∗HA −µH as a function of µ.
Ex. 6.7.5. Show that the classes of (quaternion) H-plus-matrices, strictly H-
plus-matrices, and H-expansive matrices are closed under multiplication (for a ﬁxed
hermitian H ∈Hn×n). In other words, if A, B ∈Hn×n belong to one of those classes,
so does AB.
Ex. 6.7.6. Show that if X is (H, φ)-skewsymmetric, (H, φ)-Hamiltonian, (H,∗)-
skewhermitian, (H,∗)-Hamiltonian, (H,T )-skewsymmetric, or (H,T )-Hamiltonian,
then eX is (H, φ)-orthogonal, (H, φ)-symplectic, (H,∗)-unitary, (H,∗)-symplectic,
(H,T )-orthogonal, or (H,T )-symplectic, respectively.
Ex. 6.7.7. Assume n ≥2, and let U be one of the following four sets of matrices,
for a ﬁxed H ∈Hn×n:
(a) the (H,∗)-unitary matrices, where H = H∗;
(b) the (H,∗)-symplectic matrices, where H = −H∗;
(c) the (H, φ)-orthogonal matrices, where φ is a nonstandard involution and H =
Hφ;
(d) the (H, φ)-symplectic matrices, where φ is a nonstandard involution and H =
−Hφ.
Prove that U is unbounded, except in cases when H = H∗is (positive or negative)
deﬁnite or H = −Hφ is β(φ)-(positive or negative) deﬁnite.
Show that in the
exceptional cases the set U is bounded.
Ex. 6.7.8. Provide details for the proof of the ﬁrst part of Proposition 6.1.3.
Ex. 6.7.9. Find all 2 × 2 quaternion H-plus matrices for H =
 0
1
1
0

.

150
CHAPTER 6
Ex.
6.7.10. Find all Aj-invariant (Hj,∗)-neutral subspaces for each of the
following pairs of matrices (Aj, Hj):
(1)
H1 =


0
0
1
0
1
0
1
0
0

,
A1 =


0
0
0
0
0
0
1
0
0

,
(2)
H2 = A2 =


0
0
1
0
1
0
1
0
0

,
(3)
H3 =


0
0
−i
0
1
0
i
0
0

,
A3 =


0
0
j
0
0
0
0
0
0

,
(4)
H4 = H3,
A4 =


0
0
j
0
j
0
j
0
0

,
(5)
H5 =


0
0
i
0
i
0
i
0
0

,
A5 =


0
0
0
0
j
0
0
0
0

,
(6)
H6 = H5,
A6 =


i
0
0
0
i
0
0
0
i

.
Ex.
6.7.11. Find all Aj-invariant (Hj, φ)-neutral subspaces for each of the
following pairs of matrices (Aj, Hj), where φ is a nonstandard involution, β = β(φ),
and the nonzero quaternion x is such that βx = −xβ:
(1)
H1 =


0
0
β
0
β
0
β
0
0

,
A1 =


0
0
0
0
0
0
x
0
0

,
(2)
H2 = H1,
A2 =


0
0
x
0
x
0
x
0
0

;
(3)
H3 =


0
1
0
−1
0
0
0
0
β

,
A3 =


0
0
0
β
0
0
0
0
0

,
(4)
H4 = H3,
A4 =


0
0
−β
β
0
0
0
1
0

;
(5)
H5 =


0
0
x
0
x
0
x
0
0

,
A5 =


0
0
1
0
1
0
1
0
0

,
(6)
H6 = H5,
A6 =


β
x−1β
0
0
0
x−1β
0
0
−β

.
Ex. 6.7.12. Let H ∈Hn×n be hermitian. A matrix A ∈Hn×n is said to be
H-contractive, resp. H-strictly contractive, if [Ax, Ax] ≤[x, x] for all x ∈Hn×n,

INVARIANT NEUTRAL AND SEMIDEFINITE SUBSPACES
151
resp. [Ax, Ax] < [x, x] for all x ∈Hn×n \ {0}; here [x, y] = y∗Hx, x, y ∈Hn×n is
the indeﬁnite inner product induced by H. Matrices that are H-strictly expansive
are deﬁned similarly, replacing < with >.
Show that if A is H-expansive, resp. H-strictly expansive, and invertible, then
A−1 is H-contractive, resp. H-strictly contractive; conversely, if A is H-contractive,
resp. H-strictly contractive, and invertible, then A−1 is H-expansive, resp. H-
strictly expansive.
Ex. 6.7.13. Consider the following four sets of pairs of matrices:
υn(δ, ξ) := {(H, A) ∈Hn×n : H = δH∗, HA = ξA∗H},
where δ = ±1, ξ = ±1.
(a) Show that the sets υn(δ, ξ) are closed.
(b) Prove that the order of neutrality γ(H, A) is upper semicontinuous on each
of the four sets υn(δ, ξ): if {(Hm, Am)}∞
m=1 is a sequence of pair of matrices such
that (Hm, Am) ∈υn(δ, ξ) and γ(Hm, Am) ≥k for m = 1, 2, . . . and if there exist
the limits
H0 = lim
m→∞Hm,
A0 = lim
m→∞Hm,
then also γ(H, A) ≥k.
Ex. 6.7.14. Repeat Ex. 6.7.13 for the four sets
{(H, A) ∈Hn×n : H = δHφ, HA = ξAφH},
where δ = ±1, ξ = ±1, and φ is a nonstandard involution.
Ex. 6.7.15. (a) Let A and H be as in one of the tables at the beginning of
this chapter (except expansive and plus-matrix). Prove that Ker H is A-invariant.
Hint: Use the reduction H 7→SφHS (or H 7→SφHS), A 7→S−1AS to put H
in one of the canonical forms of Theorems 4.1.6 and 4.1.2.
(b) Deduce that if A and H are as in Theorem 6.1.6, then the order of neutrality
of (H, A) is equal to the dimension of Ker H plus the order of neutrality of (H1, A1),
where H1 is the invertible part of H, and A1 is the corresponding part of A.
6.8
NOTES
Real (H,T )-Hamiltonian matrices and complex (H,∗)-skewhermitian and (H,T )-
skew-Hamiltonian matrices have been extensively studied in the literature (e.g.,
Faßbender et al. [41], Faßbender and Ikramov [39], Mehl et al. [108], and Rodman
[131]). In particular, it was proved in Faßbender et al. [41] that every real (H,T )-
skew-Hamiltonian matrix has a real (H,T )-Hamiltonian square root. For quaternion
matrices, the problem of existence of such square roots in various classes was treated
in Rodman [139]. In contrast with the real case, Hamiltonian square roots of skew-
Hamiltonian quaternion matrices do not always exist.
Quadratic forms on ﬁnite dimensional quaternion vector spaces have been stud-
ied in depth in Brieskorn [20].
The proof of Theorem 6.1.6 follows closely the arguments given in Lancaster et
al. [87] in the context of complex matrices.

152
CHAPTER 6
For complex matrices with φ the conjugation, the notion of order of neutral-
ity was studied in Lancaster and Rodman [89] and (suitably modiﬁed for inﬁnite
dimensional spaces) in Lancaster et al. [85] and Zizler [165] in connection with
deﬁnitizable operators on Krein spaces.
Theorem 6.2.6 and its proof is adapted from Gohberg et al. [53].
The classes of plus-matrices and plus-operators (the analogue of plus-matrices
acting in inﬁnite dimensional Krein spaces) and expansive and dissipative operators
have been studied in depth in the context of indeﬁnite inner products in complex
vector spaces; see Iohvidov et al. [70] and Azizov and Iohvidov [13]. Note that in
the literature the generally adopted deﬁnition of dissipative operators is diﬀerent
from the one given in this chapter; namely, an operator is said to be dissipative if
its imaginary part is positive semideﬁnite. In the context of operators on complex
vector spaces, the two deﬁnitions are essentially equivalent, but in the context of
quaternion vector spaces we have to adopt the deﬁnition given in the text.
In the complex case, Theorem 6.4.7 is proved in Iohvidov et al. [70] in the more
general setting of operators on inﬁnite dimensional spaces; see Theorem 11.2 there.
Invariant nonnegative (nonpositive, neutral) subspaces play an important role
in applications, and for real and complex matrices have been extensively studied
in the literature; see, e.g., Gohberg et al. [52], [53], Lancaster and Rodman [89],
Rodman [130], Mehl et al. [110], Freiling et al. [46], and Ran and Rodman [123].
In particular, in the context of real and complex matrices, they appear in linear
control systems (see, e.g., Lancaster and Rodman [91], and in studies of symmetric
factorizations of matrix valued functions with symmetries: Gohberg et al. [55], Ran
and Rodman [124], [126], and Ran [122].
Ex. 6.7.9 for complex matrices is given in Gohberg et al. [53, Example 10.2.3].

Chapter Seven
Smith form and Kronecker canonical form
In this chapter we study polynomials with quaternion matrix coeﬃcients. The ex-
position is focused on two major results. One is the Smith form, which asserts
that every quaternion matrix polynomial can be brought to a diagonal form under
pre- and postmultiplication by unimodular matrix polynomials, with the appropri-
ate divisibility relations among the diagonal entries. The other is the Kronecker
canonical form for quaternion matrix polynomials of ﬁrst degree under pre- and
postmultiplication by invertible constant matrices. The Kronecker form generalizes
the Jordan canonical for matrices. Complete and detailed proofs are given for both
the Smith form and the Kronecker form.
7.1
MATRIX POLYNOMIALS WITH QUATERNION
COEFFICIENTS
Let H(t) be the noncommutative ring of polynomials with quaternion coeﬃcients,
with the real independent variable t. Note that t commutes with the quaternions.
Therefore, for every ﬁxed t0 ∈R, the evaluation map
f 7→f(t0),
f(t) ∈H(t)
is well deﬁned as a unital homomorphism of real algebras H(t) −→H.
Let p(t), q(t) ∈H(t).
Deﬁnition 7.1.1. A polynomial q(t) is called a divisor of p(t) if p(t) = q(t)s(t)
and p(t) = r(t)q(t) for some s(t), r(t) ∈H(t). A polynomial q is said to be a total
divisor of p if αq(t)α−1 is a divisor of p(t) for every α ∈H \ {0} or, equivalently, if
q(t) is a divisor of βp(t)β−1 for all β ∈H \ {0}.
This deﬁnition is equivalent to the following: q(t) is a total divisor of p(t) if and
only if the containment
q(t)H(t) ∩H(t)q(t) ⊇H(t)p(t)H(t)
(7.1.1)
holds true (Ex. 7.7.2).
Let H(t)m×n be the set of all m×n matrices with entries in H(t)m×n, which will
be called matrix polynomials with the standard operations of addition, right and
left multiplication by quaternions, and matrix multiplication: If A(t) ∈H(t)m×n
and B(t) ∈H(t)n×p, then A(t)B(t) ∈H(t)m×p.
In this section we derive the Smith form (Theorem 7.1.4 below) for matrix
polynomials with quaternion entries. Our exposition follows the general outline for
the standard proof of the Smith form for matrix polynomials over ﬁelds.
Deﬁnition 7.1.2. A matrix polynomial A(t) ∈H(t)n×n is said to be elementary
if it can be represented as a product (in any order) of diagonal n × n polynomials

154
CHAPTER 7
with constant nonzero quaternions on the diagonal and of n × n polynomials with
1s on the diagonal and a sole nonzero oﬀdiagonal entry.
Constant permutation matrices are elementary, as follows from the equality
(given in Jacobson [71])

0
1
1
0

=

1
1
0
1
 
1
0
−1
1
 
1
1
0
1
 
−1
0
0
1

.
Deﬁnition 7.1.3. A matrix polynomial A(t) ∈H(t)m×n is said to be unimodular
if
A(t)B(t) = B(t)A(t) ≡I
(7.1.2)
for some matrix polynomial B(t) ∈H(t)n×m. (In this case, clearly m = n.)
Obviously, every elementary matrix polynomial is unimodular. We shall see
later that the converse also holds.
Theorem 7.1.4. Let A(t) ∈H(t)m×n. Then there exist elementary matrix poly-
nomials D(t) ∈H(t)m×m, E(t) ∈H(t)n×n, and monic (i.e., with leading coeﬃcient
equal to 1) scalar polynomials a1(t), a2(t), . . . , ar(t) ∈H(t), 0 ≤r ≤min{m, n},
such that
D(t)A(t)E(t) = diag (a1(t), a2(t), . . . , ar(t), 0, . . . , 0),
(7.1.3)
where aj(t) is a total divisor of aj+1(t), for j = 1, 2, . . . , r −1.
Deﬁnition 7.1.5. The right-hand side of (7.1.3) will be called the Smith form
of A(t).
Proof.
Step 1.
We prove the existence of the diagonal form (7.1.3) with
monic polynomials a1(t), . . . ar(t), but not necessarily with the property that aj(t)
is a total divisor of aj+1(t), for j = 1, 2, . . . , r −1.
If A(t) ≡0 we are done. Otherwise, let α be the minimal degree of nonzero
entries of all matrix polynomials of the form D1(t)A(t)E1(t), where D1(t) and E1(t)
are elementary matrix polynomials. Thus,
D2(t)A(t)E2(t) = B(t)
for some elementary matrix polynomials D2(t) and E2(t), where
B(t) = [bp,q(t)]m,n
p=1,q=1
is such that the degree of b1,1(t) is equal to α. Consider b1,q(t) with q = 2, . . . , n.
Division with remainder
b1,q(t) = b1,1(t)c1,q(t) + d1,q(t),
c1,q(t), d1,q(t) ∈H(t),
where the degree of d1,q(t) is smaller than α, shows that we must have d1,q(t) ≡0,
for otherwise the product B(t)F(t), where the elementary matrix polynomial F(t)
has 1 on the main diagonal, −c1,q(t) in the (1, q)th position, and zero everywhere
else, will have a nonzero entry of degree less than α, a contradiction with the
deﬁnition of α. Thus,
b1,q(t) = b1,1(t)c1,q(t),
q = 2, 3, . . . , n,

SMITH FORM AND KRONECKER CANONICAL FORM
155
for some c1,q(t) ∈H(t). Now it is easy to see that there exists an elementary matrix
polynomial E3(t) of the form E3(t) = I + bE(t), where bE(t) may have nonzero
entries only in the positions (1, 2), . . . , (1, n), such that B(t)E3(t) has zero entries
in the positions (1, 2), . . . , (1, n). Similarly, we prove that there exists an elementary
matrix polynomial D3(t) such that D3(t)B(t)E3(t) has zero entries in all positions
(1, q), q ̸= 1 and (p, 1), p ̸= 1. Now induction on m + n completes the proof of
existence of (7.1.3). The base of induction, namely, the cases when m = 1 or n = 1,
can be dealt with analogously, and the case m = n = 1 is trivial.
Step 2. Now, clearly, r is an invariant of the diagonal form (7.1.3) with monic
polynomials aj(t). Indeed, r coincides with the maximal rank of the matrix A(t),
over all real t. In verifying this fact, use the property that each aj(t), being a
monic polynomial of real variable with quaternion coeﬃcients, may have only a
ﬁnite number of real roots (Theorem 5.12.4).
Step 3. Consider a diagonal form (7.1.3) with monic a1(t), . . . , ar(t) that have
the following additional property: The degrees δ1, . . . , δr of a1(t), . . . , ar(t), respec-
tively, are such that for any other diagonal form
eD(t)A(t) eE(t) = diag (b1(t), b2(t), . . . , br(t), 0, . . . , 0),
where eD(t) and eE(t) are elementary matrix polynomials, and b1(t), b2(t), . . . , br(t)
are monic scalar polynomials, the degrees δ′
1, . . . , δ′
r of b1(t), . . . , br(t), respectively,
satisfy the inequalities:
δ1 ≤δ′
1;
if δ1 = δ′
1, then δ2 ≤δ′
2;
if δj = δ′
j for j = 1, 2, then δ3 ≤δ′
3;
. . .
;
if δj = δ′
j for j = 1, 2, . . . , r −1, then δr ≤δ′
r.
Obviously, a diagonal form diag (a1(t), a2(t), . . . , ar(t), 0, . . . , 0) of A(t) with these
properties does exist. We claim that then aj(t) is a total divisor of aj+1(t), for
j = 1, 2, . . . , r −1. Suppose not, and assume that aj(t) is not a total divisor of
aj+1(t), for some j. Then there exists α ∈H \ {0} such that
αaj+1(t)α−1 = d(t)aj(t) + s(t),
where d(t), s(t) ∈H(t), s(t) ̸= 0, is such that the degree of s is smaller than δj, the
degree of aj. (If it happens that
αaj+1(t)α−1 = aj(t)d(t) + s(t),
the subsequent argument is completely analogous.) We have
 1
0
d
α
  aj
0
0
aj+1
 
1
0
−α−1
α−1

=
 aj
0
−s
αaj+1α−1

.
Since the degree of s is smaller than δj, Step 1 of the proof shows that for some
elementary 2 × 2 matrix polynomials D′(t) and E′(t), we have
D′(t)
 aj(t)
0
0
aj+1(t)

E′(t) =
 a′
j(t)
0
0
a′
j+1(t)

,

156
CHAPTER 7
where a′
j(t) and a′
j+1(t) are monic polynomials and the degree of a′
j(t) is smaller
than δj. Using
 a′
j(t)
0
0
a′
j+1(t)

in place of
 aj(t)
0
0
aj+1(t)

in
diag (a1(t), a2(t), . . . , ar(t), 0, . . . , 0),
we obtain a contradiction with the choice of a1(t), . . . , ar(t).
□
The well-known Smith form for matrix polynomials with real or complex co-
eﬃcients (or, more generally, with coeﬃcients in a ﬁeld) is deﬁned analogously to
(7.1.3); see, e.g., Gantmacher [50] or Gohberg et al. [55, Chapter S1]. Thus, for
F = R or F = C, consider A(t) ∈Fm×n, a matrix with entries in F(t), the commuta-
tive ring of polynomials with coeﬃcients in F. Then there exist elementary (or, what
is the same, unimodular) matrix polynomials D(t) ∈F(t)m×m, E(t) ∈H(t)n×n,
and monic scalar polynomials a1(t), a2(t), . . . , ar(t) ∈F(t), 0 ≤r ≤min{m, n},
such that
D(t)A(t)E(t) = diag (a1(t), a2(t), . . . , ar(t), 0, . . . , 0),
(7.1.4)
where aj(t) is a divisor of aj+1(t), for j = 1, 2, . . . , r −1. (In the context of real and
complex ﬁelds, the notion of total divisibility coincides with the standard notion of
divisibility of polynomials.)
Deﬁnition 7.1.6. The right-hand side of (7.1.4) will be called the F-Smith form
of A(t).
The following example (taken from Pereira [116, Example 3.4.4]; see also Pereira
et al. [117]) is also instructive.
Example 7.1.7. Consider
A(t) =
 t + i
0
0
t + i

.
(7.1.5)
This is not a Smith form, because t + i is not a total divisor of t + i. On the other
hand, calculations show that


k
2
i
2
tj + k
t −i

· A(t) ·


−j
−k
2(t + i)
−1
i
2(t −i)

=
 1
0
0
t2 + 1

,
(7.1.6)
and


k
2
i
2
tj + k
t −i

·


−j
−k
2(t + i)
−1
i
2(t −i)


=


0
1
2i
2i
t


=


t
−1
2i
−2i
0


−1
.
(7.1.7)

SMITH FORM AND KRONECKER CANONICAL FORM
157
It follows from (7.1.7) and Ex. 7.7.7 that the matrix polynomials


k
2
i
2
tj + k
t −i


and


−j
−k
2(t + i)
−1
−i
2(t −i)


are unimodular and that diag (1, t2 + 1) is the Smith form for A(t).
Note that
(7.1.5) is the C-Smith form of A(t), but diag (1, t2 + 1) is not.
□
In contrast, the R-Smith form of a real matrix polynomial is automatically its
(quaternion) Smith form.
We tackle the problem of uniqueness (or, more precisely, nonuniqueness) of the
Smith form in the next section. Here, we continue with corollaries from the Smith
form.
Corollary 7.1.8. A matrix polynomial A(t) is elementary if and only if A(t) is
unimodular.
Proof. The “only if” part was already observed. Assume that A(t) is such
that (7.1.2) holds for some matrix polynomial B(t). Clearly, A(t) ∈H(t)n×n for
some n. Without loss of generality, we may assume that A(t) is in the form (7.1.3).
Now equation (7.1.2) implies that r = n, and for every j = 1, 2, . . . , n we have
aj(t)bj(t) ≡1 for some bj(t) ∈H(t). Hence, aj(t) is a nonzero constant and we are
done.
□
Deﬁnition 7.1.9. For F ∈{R, C, H}, we say that matrix polynomials A(t), B(t)
∈F(t)m×n are F-equivalent or simply equivalent if F = H, if A(t) = D(t)B(t)E(t) for
some elementary (or unimodular) matrix polynomials D(t) ∈Fm×m, E(t) ∈Fn×n.
Clearly, this is an equivalence relation. The Smith form is a simple form for a
matrix polynomial under F-equivalence.
For matrix polynomials of degree one, the notion of equivalence simpliﬁes to
strict equivalence (to be studied in Section 7.3).
Theorem 7.1.10. Let A1t + A0, B1t + B0 ∈H(t)n×n and assume that A1 and
B1 are invertible. Then A1t + A0 and B1t + B0 are equivalent if and only if
P(A1t + A0)Q = B1t + B0
for some constant invertible matrices P, Q ∈Hn×n.
The proof is almost verbatim the same as for matrix polynomials with coeﬃ-
cients in a ﬁeld (a well-known result—see, for instance, Gantmacher [50, Chapter
VI] or Gohberg et al. [54, Appendix]) and will not be reproduced here.
Deﬁnition 7.1.11. The rank r(A(t)) of a matrix polynomial A(t) ∈H(t)m×n is
deﬁned as the maximal rank of quaternion matrices A(t0), over all t0 ∈R.
It is easy to see that the rank of A(t) coincides with integer r of Theorem 7.1.4.
Also, if the rank of A(t0) is smaller than a certain integer q for inﬁnitely many
real values of t0, then r(A(t)) < q. This follows from the fact that a nonidentically
zero quaternion polynomial s(t) ∈H(t) can have only ﬁnitely many real zeros, by
Theorem 5.12.4.

158
CHAPTER 7
Corollary 7.1.12. Let A(t) ∈H(t)m×n.
(a) Assume that r(A(t)) < n. (This condition is automatically satisﬁed if m < n.)
Then there exists Y (t) ∈H(t)n×(n−r(A(t))) such that
A(t)Y (t) ≡0
(7.1.8)
and Y (t) is left-invertible: eY (t)Y (t) ≡I for some eY (t) ∈H(t)(n−r(A(t)))×n;
in particular, r(Y (t)) = n −r(A(t)).
(b) Assume that r(A(t)) < m. (This condition is automatically satisﬁed if m >
n.) Then there exists Y (t) ∈H(t)(m−r(A(t)))×m such that
Y (t)A(t) ≡0
and Y (t) is right-invertible; in particular, r(Y (t)) = m −r(A(t)).
Proof. Part (a). Without loss of generality, we may assume that A(t) is in
the form of the right-hand side of (7.1.3).
(Indeed, if D(t)A(t)E(t) = B(t) for
some elementary matrix polynomials D(t) and E(t), and if (7.1.8) holds true, then
B(t) · (E(t)−1Y (t)) ≡0.) Now the hypothesis of the corollary implies that the
n −r(A(t)) right-most columns of A(t) are zero, and we may take Y (t) =

0
I

.
The proof of (b) is analogous.
□
7.2
NONUNIQUENESS OF THE SMITH FORM
Deﬁnition 7.2.1. We say that two scalar polynomials a(t), b(t) ∈H(t)1×1 are
H-similar if there exists α ∈H \ {0} such that α−1a(t)α = b(t) for all real t.
Clearly, H-similarity is an equivalence relation, and H-similar polynomials must
have the same degree.
Also, if
diag (a1(t), a2(t), . . . , ar(t), 0, . . . , 0)
(7.2.1)
is a Smith form of A(t) ∈Hm×n, then so is
diag (b1(t), b2(t), . . . , br(t), 0, . . . , 0),
(7.2.2)
for every choice of scalar polynomials b1(t), . . . , br(t) such that bj(t) is H-similar
to aj(t), for j = 1, 2, . . . , r. As it turns out, the converse is generally not true; in
other words, all Smith forms of a given matrix polynomial need not be related by
H-similarity, as (7.2.1) and (7.2.2) are. The following example (taken from Pereira
[116]) illustrates this phenomenon.
In contrast, it is well known that the Smith forms of matrix polynomials over a
ﬁeld are unique; see, e.g., Gantmacher [50].
Example 7.2.2. Let
A(t) =
 t −i
0
0
t −2j

∈H(t)2×2.
Then
Γ(t) :=


1
0
0

t + 3i + 4j
5

(t −2j)



SMITH FORM AND KRONECKER CANONICAL FORM
159
is a Smith form for A(t). Indeed, consider
U(t)
:=
1
5

i −2j
2j −i
−5t −8i + 6j
5t + 3i + 4j

∈H(t)2×2,
V (t)
:=
1
5
 5
(i −2j)t −4 −2k
5
(i −2j)t + 1 −2k

∈H(t)2×2,
and
W(t) :=
 (i −2j)t + 1 −2k
(−i + 2j)t + 4 −2k
−5
5

∈H(t)2×2.
Then
U(t)−1 =


t −i
1
5i −2
5j
t −2j
1
5i −2
5j


and
V (t)W(t) =

5
0
0
5 −4k

,
so U(t) and V (t) are unimodular (Ex. 7.7.7). Also, the equality Γ(t) = U(t)A(t)V (t)
is veriﬁed; thus, Γ(t) is indeed a Smith form for A(t) (Corollary 7.1.8).
On the other hand,
bΓ(t) :=

1
0
0
(t −i)(t −2j)

is also a Smith form for A(t). To verify this, let
bU(t) :=


5
9
1
3(i + 2j)
−1
3((i + 2j)t + 4 −2k)
−i + t


and
bV (t) :=


−1
5(3i + 6j)
−t + 2j
1
1
3((−i −2j)t −1 + 2k)

.
Then
bU(t)−1
=


1
5(−(3i + 6j)t −3 + 6k)
−1
t −2j
−1
3(i + 2j)

,
bV (t)−1
=


5
9(t −i)
1
3((i + 2j)t + 4 −2k)
−1
3(i + 2j)
1

,
so, bU(t) and bV (t) are unimodular. The equality bΓ(t) = bU(t)A(t)bV (t) shows that
bΓ(t) is indeed a Smith form for A(t).
However,

t + 3i + 4j
5

(t −2j)
and
(t −i)(t −2j)

160
CHAPTER 7
are not H-similar, because the quaternions
1
5(3i + 4j)(−2j)
and
(−i)(−2j)
have diﬀerent real parts and, therefore, cannot be similar.
□
We point out a particular situation when uniqueness of the Smith form is guar-
anteed.
Theorem 7.2.3. If A(t) ∈Hm×n has Smith forms
diag (a1(t), a2(t), . . . , ar(t), 0, . . . , 0)
(7.2.3)
and
diag (b1(t), b2(t), . . . , br(t), 0, . . . , 0),
(7.2.4)
and the aj(t)’s and bj(t)’s are polynomials with real coeﬃcients, then aj(t) = bj(t)
for j = 1, 2, . . . r.
Proof. Apply the map χ to the equality
diag (a1(t), a2(t), . . . , ar(t), 0, . . . , 0)
= U(t) diag (b1(t), b2(t), . . . , br(t), 0, . . . , 0) V (t),
where U(t) ∈H(t)m×m and V (t) ∈Hn×n are unimodular. We obtain that the
matrix polynomials
χ(diag (a1(t), a2(t), . . . , ar(t), 0, . . . , 0))
and
χ(diag (b1(t), b2(t), . . . , br(t), 0, . . . , 0))
are R-equivalent. But their R-Smith forms are easily seen to be
diag (a1(t)I4, . . . , ar(t)I4, 0, . . . , 0)),
resp. diag (b1(t)I4, . . . , br(t)I4, 0, . . . , 0)).
The result now follows from the uniqueness of the R-Smith form.
□
We conclude this section with an open problem.
Open Problem 7.2.4. Study connections between the C-Smith form of a matrix
polynomial A(t) ∈C(t)m×n and its Smith forms (over the quaternions).
For example, given a complex m × n matrix polynomial A(t), determine (in
terms of the C-Smith form of A(t)) the number of C-equivalence classes in the set
{B(t) ∈Cm×n : B(t) is H−equivalent to A(t)}.
The results of Pereira et al. [117] may be relevant here.
The corresponding problem for real matrix polynomials admits a simple solu-
tion.
Theorem 7.2.5. Two real matrix polynomials are R-equivalent if and only if
they are H-equivalent.
Proof. We need only to show that if two Smith forms (7.2.3) and (7.2.4) with
real polynomials a1(t), . . . , ar(t), b1(t), . . . , br(t) are H-equivalent, then they are
R-equivalent. But this is immediate from Theorem 7.2.3.
□

SMITH FORM AND KRONECKER CANONICAL FORM
161
7.3
STATEMENT OF THE KRONECKER FORM
In this section we present the Kronecker form of a pair of quaternion matrices, with
a complete proof (to follow) modeled after the standard proof for complex (or real)
matrices; see, e.g., Gantmacher [50, 49], Thompson [151], and also Gohberg et al.
[54]. Although an alternative proof can be given using the map ω (see the proof of
the Jordan form in Section 5.6), we prefer a direct proof, as it can be applicable to
matrices over more general division rings.
Consider two matrix polynomials of degree at most one, often called matrix
pencils: A1 + tB1 and A2 + tB2, where A1, B1, A2, B2 ∈Hm×n.
Deﬁnition 7.3.1. The matrix pencils Aj + tBj, j = 1, 2, are called strictly
equivalent if
A1 = PA2Q,
B1 = PB2Q
for some invertible matrices P ∈Hm×m and Q ∈Hn×n.
We develop here canonical form of matrix pencils under strict equivalence.
The Kronecker form of matrix polynomials A + tB is formulated in Theorem
7.3.2 below.
Introduce special matrix polynomials:
Lε×(ε+1)(t) = [0ε×1 Iε] + t[Iε 0ε×1] ∈Hε×(ε+1).
Here ε is a positive integer. Also, standard real symmetric matrices Fm, Gm, and
eGm deﬁned by (1.2.3), (1.2.4), and (1.2.5), respectively, will be used.
Theorem 7.3.2. Every pencil A + tB ∈H(t)m×n is strictly equivalent to a
matrix pencil with the block diagonal form:
0u×v
⊕
Lε1×(ε1+1) ⊕· · · ⊕Lεp×(εp+1) ⊕LT
η1×(η1+1) ⊕LT
ηq×(ηq+1)
⊕
(Ik1 + tJk1(0)) ⊕· · · ⊕(Ikr + tJkr(0))
⊕
(tIℓ1 + Jℓ1(α1)) ⊕· · · ⊕(tIℓs + Jℓs(αs)),
(7.3.1)
where ε1 ≤· · · ≤εp; η1 ≤· · · ≤ηq; k1 ≤· · · ≤kr, are positive integers, and
α1, . . . , αs ∈H.
Moreover, the integers u, v, and εi, ηj, kw are uniquely determined by the pair
A, B, and the part
(tIℓ1 + Jℓ1(α1)) ⊕· · · ⊕(tIℓs + Jℓs(αs))
is uniquely determined by A and B up to an arbitrary permutation of the diagonal
blocks and up to replacing αj with any quaternion similar to αj in each Jℓj(αj).
In particular, the αj’s can be chosen to be complex numbers with nonnegative
imaginary parts.
The following terminology is used in connection with the Kronecker form (7.3.1)
of the matrix pencil A + tB.
Deﬁnition 7.3.3. The integers ε1 ≤· · · ≤εp and η1 ≤· · · ≤ηq are called the
left indices and the right indices , respectively, of A+tB. The integers k1 ≤· · · ≤kr
are called the indices, or partial multiplicities, at inﬁnity of A+tB. The quaternions
−α1, . . . , −αs are called the eigenvalues of A + tB.

162
CHAPTER 7
The eigenvalues of A + tB are uniquely determined up to permutation and
similarity. Thus, the set of eigenvalues of tI + Jm(α) consists of all quaternions
similar to −α.
Deﬁnition 7.3.4. The part
0u×v ⊕Lε1×(ε1+1) ⊕· · · ⊕Lεp×(εp+1) ⊕LT
η1×(η1+1) ⊕LT
ηq×(ηq+1)
is termed the singular part of the form (7.3.1), and
(Ik1 + tJk1(0)) ⊕· · · ⊕(Ikr + tJkr(0)) ⊕(tIℓ1 + Jℓ1(α1)) ⊕· · · ⊕(tIℓs + Jℓs(αs))
is the regular part.
If we deﬁne the left kernel of a matrix Z ∈Hm×n by
Kel Z = {x ∈H1×m : xZ = 0},
then the parameters u and v in (7.3.1) can be identiﬁed as:
u = dimH
\
t∈R
Kel (A + tB),
(7.3.2)
v = dimH
\
t∈R
Ker (A + tB).
(7.3.3)
Note that if B is invertible, then the parts 0u×v, Lϵj×(ϵj+1), LT
ηj×(ηj+1), and I +
tJℓj(0) are absent in the Kronecker form; moreover, the eigenvalues of A + tB are
exactly the negatives of the eigenvalues of B−1A (or the eigenvalues of AB−1).
Indeed, we have
P(A + tB)Q = tI + J,
J := Jℓ1(α1)) ⊕· · · ⊕Jℓs(αs)
for some invertible matrices P and Q, so
PBQ = I,
PAQ = J,
and
P(AB−1)P −1 = PA · QP · P −1 = PAQ = J.
Thus, J is the Jordan form of AB−1. Analogously, one shows that J is the Jordan
form of B−1A.
Deﬁnition 7.3.5. For a ﬁxed eigenvalue α of A + tB, let i1 < · · · < iw be all
the subscripts in (7.3.1) such that αi1, · · · , αiw are similar to −α; then the integers
ℓi1, . . . , ℓiw are called the indices, or partial multiplicities, of the eigenvalue α of
A + tB. Note that there may be several indices at inﬁnity that are equal to a ﬁxed
positive integer; the same remark applies to the indices of a ﬁxed eigenvalue of
A + tB, to the left indices of A + tB, and to the right indices of A + tB.
Looking ahead, we indicate the following corollary, to be used in the sequel.
Corollary 7.3.6. Let φ be an involution, either the conjugation or a nonstan-
dard one. Then a matrix pencil A + tB is strictly equivalent to Aφ + tBφ if and
only if
dimH
\
t∈R
Kel (A + tB) = dimH
\
t∈R
Ker (A + tB),

SMITH FORM AND KRONECKER CANONICAL FORM
163
and the right indices of A + tB, arranged in the nondecreasing order, coincide with
its left indices, also arranged in the nondecreasing order. In particular, A + tB is
strictly equivalent to Aφ + tBφ if the singular part in the Kronecker form of A + tB
is absent.
Proof. Let A0 + tB0 be the Kronecker form of A + tB given by (7.3.1). Then
(A0)φ+t(B0)φ is strictly equivalent to Aφ+tBφ. Since by Corollary 5.7.2 (Jm(α))φ
is similar to Jm(α), we obtain that the Kronecker form of (A0)φ + t(B0)φ is
0v×u
⊕
LT
ε1×(ε1+1) ⊕· · · ⊕LT
εp×(εp+1) ⊕Lη1×(η1+1) ⊕Lηq×(ηq+1)
⊕
(Ik1 + tJk1(0)) ⊕· · · ⊕(Ikr + tJkr(0))
⊕
(tIℓ1 + Jℓ1(α1)) ⊕· · · ⊕(tIℓs + Jℓs(αs)).
(7.3.4)
In view of the uniqueness of the Kronecker form, it follows that the Kronecker form
of (A0)φ + t(B0)φ coincides with that of A + tB if and only if u = v, p = q, and
εj = ηj for j = 1, 2, . . . , p.
□
7.4
PROOF OF THEOREM 7.3.2: EXISTENCE
We start with a reduction theorem, which is the key to the proof of existence in
Theorem 7.3.2.
Theorem 7.4.1. Let A + tB ∈H(t)m×n be a matrix pencil such that the rank
of A + tB is smaller than n. Assume that
Ax = Bx = 0,
x ∈Hn×1
=⇒
x = 0.
(7.4.1)
Then A + tB is strictly equivalent to a direct sum
Lε×(ε+1)(t) ⊕(A′ + tB′)
(7.4.2)
for some integer ε > 0.
Proof. With some changes, the proof follows the proof of the reduction theorem
in Gantmacher [49, Chapter XII] and [48, Chapter 2]; see also Gohberg et al. [54,
Appendix] or Rodman [132].
By Corollary 7.1.12, there exists a nonzero polynomial y(t) ∈H(t)n×1 such that
(A + tB)y(t) ≡0.
(7.4.3)
Let ε −1 ≥1 be the smallest degree of a nonzero y(t) for which (7.4.3) holds. (We
cannot have ε −1 = 0 in view of hypothesis (7.4.1)). Write
y(t) =
ε−1
X
j=0
tj(−1)jyj,
y0, . . . , yε−1 ∈Hn×1,
yε−1 ̸= 0.
Then equation (7.4.3) reads


A
0m×n
· · ·
0m×n
B
A
· · ·
0m×n
0m×n
B
· · ·
...
...
...
...
A
0m×n
0m×n
· · ·
B




y0
−y1
...
(−1)ε−1yε−1

= 0.
(7.4.4)

164
CHAPTER 7
Denote by Z(ε+1)m×εn(A + tB) the (ε + 1)m × εn matrix in the left-hand side of
(7.4.4). In view of (7.4.4) we have
rank Z(ε+1)m×εn(A + tB) < εn,
(7.4.5)
and since ε −1 is the smallest possible degree of a nonzero vector y(x) for which
(7.4.3) holds, we have
rank Z(q+1)m×qn(A + tB) = qn,
q = 1, 2, . . . , ε −1.
(7.4.6)
Next, we prove that the vectors
Ay1, . . . , Ayε−1 ∈Hm×1
(7.4.7)
are linearly independent. Assume the contrary, and let
Ayh =
h−1
X
j=1
Ayjαh−j,
α1, . . . , αh−1 ∈H,
for some h, 2 ≤h ≤ε −1. (If Ay1 were equal to 0, then in view of (7.4.4) we would
have Ay0 = By0 = 0; hence, y0 = 0 by (7.4.1), and so y(t)/t would be a polynomial
of degree smaller than ε −1 still satisfying (7.4.3), a contradiction with the choice
of ε −1.) Equation (7.4.4) gives
Byh−1 =
h−1
X
j=1
Byj−1αh−j,
in other words,
Beyh−1 = 0,
eyh−1 := yh−1 −
h−2
X
j=0
yjαh−1−j.
Introducing the vectors
eyh−2 := yh−2 −
h−3
X
j=0
yjαh−2−j,
eyh−3 := yh−3 −
h−4
X
j=0
yjαh−3−j,
. . . ,
ey1 := y1 −y0α1,
ey0 := y0,
we obtain, using (7.4.4), the equalities
Aeyh−1 = Beyh−2,
. . . ,
Aey1 = Bey0,
Aey0 = 0.
Thus,
ey(t) =
h−1
X
j=0
tj(−1)jeyj
is a nonzero (because ey0 ̸= 0) polynomial of degree smaller than h ≤ε−1 satisfying
(7.4.3), a contradiction with the choice of ε −1.
Now it is easy to see that the vectors y0, . . . , yε−1 are linearly independent.
Indeed, if
y0α0 + · · · + yε−1αε−1 = 0,
αj ∈H,

SMITH FORM AND KRONECKER CANONICAL FORM
165
then applying A and using the linear independence of (7.4.7), we obtain α1 = · · · =
αε−1 = 0, and since y0 ̸= 0, the equality α0 = 0 follows as well.
Now let Q ∈Hn×n be an invertible matrix whose ﬁrst ε columns are y0, . . . , yε−1
(in that order), and let P ∈Hm×m be an invertible matrix whose ﬁrst ε−1 columns
are Ay1, . . . , Ayε−1. The existence of such P and Q follows from the replacement
Theorem 3.1.1, where we take v1 = e1, . . . , vn = en (for P), and v1 = e1, . . . , vn =
em (for Q), the standard bases in Hn×1 and in Hm×1, respectively.
Deﬁne the matrices A′ and B′ by the equality
(A + tB)Q = P(A′ + tB′).
Using (7.4.4), it is easily seen that
A′ + tB′ =

L(ε−1)×ε(t)
D + tF
0(m−(ε−1))×ε
A′′ + tB′′

,
(7.4.8)
for some matrices
A′′, B′′ ∈H(m−(ε−1))×(n−ε)
and
D, F ∈H(ε−1)×(n−ε).
We obviously have
Zεm×(ε−1)n(A + tB)
= P ′ ·
" Zε(ε−1)×(ε−1)ε(L(ε−1)×ε(t))
0
∗
Zε(m−(ε−1))×(ε−1)(n−ε)(A′′ + tB′′)

· Q′
(7.4.9)
for some invertible matrices P ′ and Q′. One easily checks that the matrix
Zε(ε−1)×(ε−1)ε(L(ε−1)×ε(t))
is invertible. Since by (7.4.6)
rank Zεm×(ε−1)n(A + tB) = (ε −1)n,
in view of (7.4.9) we have
rank Zε(m−(ε−1))×(ε−1)(n−ε)(A′′ + tB′′) = (ε −1)(n −ε).
(7.4.10)
Finally, we shall prove that by applying a suitable strict equivalence, one can
reduce the matrix polynomial in the right-hand side of (7.4.8) to the form (7.4.2).
More precisely, we shall show that for some quaternion matrices X and Y of suitable
sizes, the equality
L(ε−1)×ε(t) ⊕(A′′ + tB′′)
=
 Iε−1
Y
0
Im−(ε−1)
 
L(ε−1)×ε(t)
D + tF
0(m−(ε−1))×ε
A′′ + tB′′

·
 Iε
−X
0
In−ε

(7.4.11)
holds. Equality (7.4.11) can be rewritten in the form
L(ε−1)×ε(t)X = D + tF + Y (A′′ + tB′′).
(7.4.12)

166
CHAPTER 7
Introduce notation for the entries of D, F, X, for the rows of Y , and for the columns
of A′′ and B′′:
D = [di,k],
F = [fi,k],
X = [xj,k],
where
i = 1, 2, . . . , ε −1;
k = 1, 2, . . . , n −ε;
j = 1, 2, . . . , ε;
and
Y =


y1
y2
...
yε−1

,
A′′ =
 a1
a2
· · ·
an−ε

,
B′′ =
 b1
b2
· · ·
bn−ε

.
Then, equating the entries in the kth column of both sides of (7.4.12), we see that
(7.4.12) is equivalent to the following system of scalar equations, with unknowns
xj,k and yj:
x2,k + tx1,k
=
d1,k + tf1,k + y1ak + ty1bk,
(7.4.13)
k = 1, 2, . . . , n −ε,
x3,k + tx2,k
=
d2,k + tf2,k + y2ak + ty2bk,
k = 1, 2, . . . , n −ε,
...
xε,k + txε−1,k
=
dε−1,k + tfε−1,k + yε−1ak + tyε−1bk,
(7.4.14)
k = 1, 2, . . . , n −ε.
We ﬁrst solve the system of equations
y1ak −y2bk
=
f2,k −d1,k,
k = 1, 2, . . . , n −ε,
y2ak −y3bk
=
f3,k −d2,k,
k = 1, 2, . . . , n −ε,
...
yε−2ak −yε−1bk
=
fε−1,k −dε−2,k,
k = 1, 2, . . . , n −ε.
(7.4.15)
Indeed, (7.4.15) can be rewritten in the form
 y1
−y2
. . .
(−1)ε−1yε−2
(−1)εyε−1

· Z(ε−1)(m−ε+1)×(ε−2)(n−ε)(A′′ + tB′′)
=

[f2,k −d1,k]n−ε
k=1 [f3,k −d2,k]n−ε
k=1
. . . [fε−2,k −dε−3,k]n−ε
k=1 [fε−1,k −dε−2,k]n−ε
k=1

.
This equation can be always solved for y1, . . . , yε−1, because by (7.4.10) and Propo-
sition 3.2.8 the matrix
Z(ε−1)(m−ε+1)×(ε−2)(n−ε)(A′′ + tB′′)
is left-invertible. Once (7.4.15) is satisﬁed, a solution of (7.4.14) is easily obtained
by setting
x1,k = f1,k + y1bk,
x2,k = f2,k + y2bk, . . . , xε−1,k = fε−1,k + yε−1bk.

SMITH FORM AND KRONECKER CANONICAL FORM
167
(If ε = 2, then the system (7.4.14) has only equations (7.4.13), which can be easily
solved for x1,k, x2,k.) This concludes the proof of Theorem 7.4.1.
□
The dual statement of Theorem 7.4.1 reads as follows.
Theorem 7.4.2. Let A + tB ∈H(x)m×n be a matrix pencil such that the rank
of A + tB is smaller than m. Assume that
yA = yB = 0,
y ∈H1×m
=⇒
y = 0.
(7.4.16)
Then A + tB is strictly equivalent to a direct sum
(Lε×(ε+1)(t))T ⊕(A′ + tB′)
(7.4.17)
for some integer ε > 0.
The proof is reduced to Theorem 7.4.1 upon considering the matrix pencil A∗+
tB∗.
We are now ready to prove Theorem 7.3.2. First, we prove the existence of the
form (7.3.1). The proof proceeds by induction on the sizes m and n. If rank (A +
tB) < n and (7.4.1) holds true, or if rank (A+tB) < m and (7.4.16) holds true, then
we can use Theorem 7.4.1 or 7.4.2, as appropriate, to reduce the existence proof
to matrices of smaller sizes, and we are done by induction. If neither of (7.4.1)
and (7.4.16) holds true, then by selecting a basis {x1, . . . , xv} for the solution set
of Ax = Bx = 0 and a basis {y1, . . . , yu} for the solution set of yA = yB = 0, and
appropriately changing bases in Hn×1 and in H1×m, we see that A + tB has the
form
A + tB =
 0u,v
0
0
A′ + tB′

,
and we are done, again by the induction hypothesis. It remains to consider the case
when m = n and rank (A + t0B) = n for some real t0. In other words, the matrix
A + t0B is invertible. Now, an application of the Jordan form of (A + t0B)−1B
(Theorem 5.5.3) easily completes the proof of existence of the form (7.3.1). Indeed,
with A0 := A + t0B and t′ = t −t0 we have
A + tB
=
A0 + t′B = A0(I + t′A−1
0 B)
=
A0S−1(I + t′J)S = A0S−1(I −t0J + tJ)S
for some invertible S ∈Hn×n and some matrix J in the Jordan form. Using the
property that (Jk(α)−1, α ∈H \ {0}, is similar to Jk(α−1), one easily transforms
I−t0J +tJ to the form as in (7.3.1), with the parts 0u×v, Lϵj×(ϵj+1), and LT
ηj×(ηj+1)
absent.
7.5
PROOF OF THEOREM 7.3.2: UNIQUENESS
An independent proof of uniqueness can be given along the lines as in Gantmacher
[50, 49] or Gohberg et al. [54, Appendix]. We give a proof based on reduction to
the case of complex (or real) matrices.
We start with the Kronecker forms of matrix representations. This information
is of independent interest. It will be convenient to use the following notation: if
X ∈Hδ1×δ2, then X⊕m stands for the mδ1 × mδ2 matrix X ⊕· · · ⊕X, where X is
repeated m times.

168
CHAPTER 7
Theorem 7.5.1. Let
0u×v
⊕
Lε1×(ε1+1) ⊕· · · ⊕Lεp×(εp+1) ⊕LT
η1×(η1+1) ⊕LT
ηq×(ηq+1)
⊕
(Ik1 + tJk1(0)) ⊕· · · ⊕(Ikr + tJkr(0))
⊕
(tIℓ1 + Jℓ1(α1)) ⊕· · · ⊕(tIℓs + Jℓs(αs)),
(7.5.1)
be the Kronecker form, as in Theorem 7.3.2, of a matrix pencil A + tB, A, B ∈
Hm×n, where α1, . . . , αs are taken to be complex numbers. Then:
(1) the pencil of real matrices χm,n A + tχm,n (B) has the real Kronecker form
04u×4v
⊕
L⊕4
ε1×(ε1+1) ⊕· · · ⊕L⊕4
εp×(εp+1)
⊕
(LT
η1×(η1+1))⊕4 ⊕· · · ⊕(LT
ηq×(ηq+1))⊕4
⊕
(Ik1 + tJk1(0))⊕4 ⊕· · · ⊕(Ikr + tJkr(0))⊕4
⊕
(tIℓ1 + Jℓ1(α1))⊕4 ⊕· · · ⊕(tIℓw + Jℓw(αw))⊕4
⊕
 tI2ℓw+1 + J2ℓw+1(R(αw+1) ± iI(αw+1))

⊕
 tI2ℓw+1 + J2ℓw+1(R(αw+1) ± iI(αw+1))

⊕
· · · ⊕(tI2ℓs + J2ℓs(R(αs) ± iI(αs)))
⊕
(tI2ℓs + J2ℓs(R(αs) ± iI(αs))) ,
where α1, . . . , αw are real and αw+1, . . . , αs are nonreal;
(2) the pencil of complex matrices ωm,n (A)+tωm,n (B) has the complex Kronecker
form
02u×2v
⊕
L⊕2
ε1×(ε1+1) ⊕· · · ⊕L⊕2
εp×(εp+1)
⊕
(LT
η1×(η1+1))⊕2 ⊕· · · ⊕(LT
ηq×(ηq+1))⊕2
⊕
(Ik1 + tJk1(0))⊕2 ⊕· · · ⊕(Ikr + tJkr(0))⊕2
⊕
(tIℓ1 + Jℓ1(α1))⊕2 ⊕· · · ⊕(tIℓw + Jℓw(αw))⊕2
⊕
 tIℓw+1 + Jℓw+1(αw+1)

⊕
 tIℓw+1 + Jℓw+1(αw+1)

⊕
· · · ⊕(tIℓs + Jℓs(αs)) ⊕(tIℓs + Jℓs(αs)) .
(7.5.2)
Proof. The blocks tIℓu +Jℓu(αu) are taken care of in Theorem 5.7.1. Note that
the other blocks in 7.5.1 are real. Now it is easy to see from the deﬁnitions of the
maps χ and ω that the pencils χm,n(A) + tχ(B) and ωm,n(A) + tωm,n(B) have the
Kronecker form as claimed (a permutation of columns (resp. rows) is required to
put Lεv×(εv+1) (resp. LT
η×η+1)) in a canonical form).
□
Now the proof of uniqueness in Theorem 7.5.1 is not diﬃcult. Suppose that a
matrix pencil A+tB, where A, B ∈Hm×n is strictly equivalent to (7.3.1) (with the
αj’s taken to be complex numbers) as well as strictly equivalent to
0u′×v′
⊕
Lε′
1×(ε′
1+1) ⊕· · · ⊕Lε′
p′×(ε′
p′+1) ⊕LT
η′
1×(η′
1+1) ⊕LT
η′
q′×(η′
q′+1)
⊕
(Ik′
1 + tJk′
1(0)) ⊕· · · ⊕(Ik′
r′ + tJk′
r′(0))
⊕
(tIℓ′
1 + Jℓ′
1(α′
1)) ⊕· · · ⊕(tIℓ′
s′ + Jℓ′
s′(α′
s′)),
(7.5.3)

SMITH FORM AND KRONECKER CANONICAL FORM
169
where
ε′
1 ≤· · · ≤ε′
p′;
η′
1 ≤· · · ≤η′
q′;
k′
1 ≤· · · ≤k′
r′;
ℓ′
1 ≤· · · ≤ℓ′
s′
are positive integers and α′
1, . . . , α′
s′ ∈C.
Then by (7.3.2) and (7.3.3) we have
u′ = u and v′ = v. Moreover, denoting the pencils (7.3.1) and (7.5.3) by E + tG
and E′ + tG′, respectively, we have
P(E + tG)Q = E′ + tG′
for some invertible matrices P and Q. Applying the map ωm,n to this equality,
we see that the complex matrix pencils ωm,nE + tωm,nG and ωm,nE′ + tωm,nG′
are strictly equivalent. Now formula (7.5.2) (applied to ωm,nE + tωm,nG and to
ωm,nE′ + tωm,nG′) together with the uniqueness statement for Kronecker form of
complex matrix pencils (Theorem 15.1.3) imply the desired uniqueness in Theorem
7.3.2.
□
7.6
COMPARISON WITH REAL AND COMPLEX STRICT
EQUIVALENCE
Theorems 5.8.1 and 5.8.3 have straightforward analogues for matrix pencils and
strict equivalence.
Theorem 7.6.1. A quaternion matrix pencil A + tB is strictly equivalent to
a real matrix pencil if and only if for every positive integer m and every nonreal
λ ∈H, the number of blocks tIm +Jm(α) with α similar to λ in the Kronecker form
of A + tB is even.
The proof (which is omitted) is similar to that of Theorem 5.8.1, using the
Kronecker form of the pencil and equality (5.8.1).
Deﬁnition 7.6.2. We say that two real matrix pencils A+tB, A′ +tB′ ∈Rm×n
are strictly equivalent over the reals, or R-strictly equivalent, if P(A + tB)Q =
A′ + tB′ for some real invertible matrices P and Q. Analogously, the notion of
C-strictly equivalent complex matrix pencils is deﬁned.
Finally, two quaternion
matrix pencils A + tB, A′ + tB′ ∈Hm×n are said to be strictly equivalent over
quaternions, or H-strictly equivalent, if P(A + tB)Q = A′ + tB′ for some invertible
quaternion matrices P and Q.
In particular, we will apply the notion of H-strict equivalence to real and complex
matrix pencils.
Theorem 7.6.3.
(a) Two real matrix pencils are R-strictly equivalent if and
only if they are H-strictly equivalent.
(b) Let A + tB be a complex matrix pencil, and let
0u×v
⊕
Lε1×(ε1+1) ⊕· · · ⊕Lεp×(εp+1) ⊕LT
η1×(η1+1) ⊕LT
ηq×(ηq+1)
⊕
(Ik1 + tJk1(0)) ⊕· · · ⊕(Ikr + tJkr(0))
⊕
(tIℓ1 + Jℓ1(α1)) ⊕· · · ⊕(tIℓs + Jℓs(αs))
(7.6.1)
be the complex Kronecker form of A + tB. Then the set of all complex matrix
pencils that are H-strictly equivalent to A + tB consists of exactly p classes

170
CHAPTER 7
E1, . . . , Ep of mutually C-strictly equivalent complex matrix pencils so that
complex matrix pencils from diﬀerent Ej’s are not C-strictly equivalent. The
integer p is given by
p =
u
Y
j=1
 rj
Y
k=1
(pλj,k + 1)
!
,
(7.6.2)
where {λ1, . . . , λu} is a set of distinct nonreal complex numbers among {α1,
. . . , αs} that is maximal (by inclusion) with respect to the property that it
does not contain any pair of complex conjugate numbers, and pλj,k is the
total number of Jordan blocks in (7.6.1) of size k with eigenvalue λj or λj; rj
being the largest size of a Jordan block with eigenvalue λj or λj.
In Theorem 7.6.3(b), the classes E1, . . . , Ep are constructed as in Theorem 5.8.3
(b).
Namely, for every pair of values (j, k) as in (7.6.2) such that pλj,k ̸= 0,
select an integer ℓλj,k satisfying the inequalities 0 ≤ℓλj,k ≤pλj,k. Then the class
Ei corresponding to this selection of the ℓλj,k’s consists of those complex matrix
pencils whose complex Kronecker form is
K0
⊕
⊕u
j=1 ⊕{k : pλj ,k̸=0}



(tI + Jk(λj)) ⊕· · · ⊕(tI + Jk(λj))
|
{z
}
ℓλj ,k times
⊕(tI + Jk(λj)) ⊕· · · ⊕(tI + Jk(λj))
|
{z
}
pλj ,k−ℓλj ,k times



,
(7.6.3)
where K0 is the part of the complex Kronecker form of A + tB that consists of
blocks other than tIℓj + Jℓj(αj) with nonreal αj.
The proof is analogous to that of Theorem 5.8.3, using Theorem 7.5.1. We omit
the proof.
7.7
EXERCISES
Ex. 7.7.1. Provide details for the proof of Theorem 7.6.1.
Ex. 7.7.2. Show that q(t) ∈H(t) is a total divisor of p(t)H(t) if and only if
(7.1.1) holds.
Ex. 7.7.3. Prove equalities (7.3.2) and (7.3.3).
Ex. 7.7.4. Find the Smith form of the following upper triangular n × n matrix
polynomial:


p1(t)
p2(t)
0
. . .
0
0
α1
p3(t)
. . .
0
...
...
...
...
...
0
0
. . .
αn−2
pn(t)
0
0
. . .
0
αn−1


,
where α1, . . . , αn−1 ∈H and p1(t), . . . , pn(t) are scalar polynomials with quaternion
coeﬃcients.

SMITH FORM AND KRONECKER CANONICAL FORM
171
Ex. 7.7.5. Show that a complex matrix pencil A + tB has the property that
a complex matrix pencil is H-strictly equivalent to A + tB precisely when it is
C-strictly equivalent to A + tB, if and only if A + tB has no nonreal eigenvalues.
Ex. 7.7.6. If diag (a1(t), . . . , ar(t), 0, . . . , 0) is the Smith form of A(t) ∈Hn×n.
Find the Smith form of the following quaternion matrix polynomials:
(1) A(p(t)), where p(t) is a scalar polynomial with real coeﬃcients;
(2) A(t) ⊕· · · ⊕A(t)
|
{z
}
p
times
.
Ex. 7.7.7. (a) Show that if A(t) ∈H(t)n×n is a square-size matrix polynomial
such that A(t)B(t) ≡I for some B(t) ∈Hn×n, then A(t) is unimodular and,
necessarily, A(t)−1 = B(t).
(b) Repeat Part (a) but replace A(t)B(t) ≡I with B(t)A(t) ≡I. Hint: Use the
Smith form for B(t).
Ex. 7.7.8. Find all Smith forms of the diagonal matrix polynomial diag (t −
i, t −i, . . . , t −i) ∈Hn×n.
7.8
NOTES
The Smith form (Theorem 7.1.4) in the more general context of matrix polynomials
with coeﬃcients in a division ring was proved in Nakayama [115]; see also Jacobson
[71]. For matrix polynomials over a (commutative) ﬁeld, and more generally over
commutative principal ideal domains, the Smith form is widely known; see, e.g.,
Gantmacher [49], Gohberg et al. [54, 55], and Lancaster and Tismenetsky [94].
The Smith form is treated in Jacobson [71, Chapter 3]; see also Guralnick et
al. [59], Guralnick and Levy [58], Levy and Robson [100] in a more general context
of noncommutative principal ideal domains, and Pereira et al. [117] (in a diﬀerent
setup of Laurent polynomials).
A complete set of invariants for equivalence (in the more general context of
matrices over noncommutative principal ideal domains Λ) does not seem to be
known; see Guralnick et al. [59], where it is proved that if the rank of A ∈Λm×n
is at least two, then the isomorphism class of the left module Λn/(ΛmA) forms a
complete set of invariants for the equivalence relation. Examples given in Guralnick
and Levy [58] and Levy and Robson [100] show that the result fails for matrices of
rank one.
The Kronecker canonical form is well known for real and complex matrices and,
more generally, for matrices over ﬁelds (if the ﬁeld is not algebraically closed, a
rational normal from may be used instead of the Jordan form) and can be found in
many sources in literature. For quaternion matrices, the Kronecker canonical form
is also known; see, e.g., Sergeichuk [141] and Djokovi´c [33].
Ex. 7.7.6 (1) is found in Gohberg et al. [53], in the context of matrix polynomials
with complex coeﬃcients.
The exposition in this chapter follows Rodman [132].

Chapter Eight
Pencils of hermitian matrices
The main theme in this and the next chapter is the canonical forms of pairs of
quaternion matrices, or matrix pencils, under simultaneous congruence, where the
matrices are either hermitian or skewhermitian. For convenience of exposition, the
material is divided between two chapters: In the current chapter the case of two
hermitian matrices is studied, whereas the next chapter is devoted to pairs of ske-
whermitian matrices and to mixed pairs, when one of the matrices if hermitian
and the other is skewhermitian. We also present here applications of the canonical
form to the problems of existence of positive deﬁnite or semideﬁnite (nontrivial)
linear combinations of two hermitian matrices and to the related problem of si-
multaneous diagonalizability. Other applications include treatment of H-expansive
and H-plus-matrices, in particular for the cases when the hermitian matrix H is
singular.
In this and subsequent chapters we will be often concerned with congruence
of matrix pencils or, what is the same, simultaneous congruence of two matrices.
Thus, it will be useful to introduce the concept of matrix pencils congruence in a
general context.
Deﬁnition 8.0.1. Let F be one of R, C, or H.
Two matrix pencils A + tB
and A′ + tB′, where A, B, A′, B′ ∈Fm×m, are said to be F-congruent, or simply
congruent in the case when F = H, if A + tB = S∗(A′ + tB′)S for some invertible
matrix S ∈Fm×m. In this case we also say that the pairs (A′, B′) and (A, B) are
F-simultaneously congruent, or simply simultaneously congruent if F = H.
Clearly, F-congruence of matrix pencils is an equivalence relation.
8.1
CANONICAL FORMS
Deﬁnition 8.1.1. A matrix pencil A+tB, A, B ∈Hn×n, is said to be hermitian
if A = A∗and B = B∗or, equivalently, if A + tB is hermitian for every real t.
Obviously, any matrix pencil which is congruent to a hermitian matrix pencil is
itself hermitian.
Canonical forms for hermitian matrix pencils under strict equivalence and con-
gruence are given by the following theorem. We use the standard real symmetric
m × m matrices
Fm :=


0
· · ·
· · ·
0
1
...
1
0
...
...
0
1
...
1
0
· · ·
· · ·
0


,
(8.1.1)

PENCILS OF HERMITIAN MATRICES
173
Gm :=


0
· · ·
· · ·
1
0
...
0
0
...
...
1
0
...
0
0
· · ·
· · ·
0


=
 Fm−1
0
0
0

.
(8.1.2)
Theorem 8.1.2. (a) Every hermitian matrix pencil A+tB, where A, B ∈Hn×n,
is strictly equivalent to a hermitian matrix pencil of the following form:
0u
⊕

t


0
0
Fε1
0
0
0
Fε1
0
0

+ G2ε1+1


⊕
· · · ⊕

t


0
0
Fεp
0
0
0
Fεp
0
0

+ G2εp+1


⊕
(Fk1 + tGk1) ⊕· · · ⊕(Fkr + tGkr)
⊕
((t + α1) Fℓ1 + Gℓ1) ⊕· · · ⊕
 (t + αq) Fℓq + Gℓq

⊕

0
(t + β1)Fm1
(t + β∗
1)Fm1
0

+

0
Gm1
Gm1
0

⊕
· · · ⊕

0
(t + βs)Fms
(t + β∗
s)Fms
0

+

0
Gms
Gms
0

. (8.1.3)
Here, the numbers ε1 ≤· · · ≤εp and k1 ≤· · · ≤kr are positive integers, αj are real
numbers, βj are nonreal quaternions, and Fm, Gm are the m × m matrices given
by (1.2.3) and (1.2.4).
The form (8.1.3) is uniquely determined by A + tB up to an arbitrary permuta-
tion of the blocks in each of the parts
⊕q
j=1
 (t + αj)Fℓj + Gℓj

and
⊕s
j=1

0
(t + βj)Fmj
(t + β∗
j )Fmj
0

+

0
Gmj
Gmj
0

,
(8.1.4)
and up to replacement of βj in every block

0
(t + βj)Fmj
(t + β∗
j )Fmj
0

+

0
Gmj
Gmj
0

(8.1.5)
by a similar quaternion.
(b) Every hermitian matrix pencil A + tB, where A, B ∈Hn×n, is congruent to

174
CHAPTER 8
a hermitian matrix pencil of the form
0u
⊕

t


0
0
Fε1
0
0
0
Fε1
0
0

+ G2ε1+1


⊕
· · · ⊕

t


0
0
Fεp
0
0
0
Fεp
0
0

+ G2εp+1


⊕
δ1 (Fk1 + tGk1) ⊕· · · ⊕δr (Fkr + tGkr)
⊕
η1 ((t + α1) Fℓ1 + Gℓ1) ⊕· · · ⊕ηq
 (t + αq) Fℓq + Gℓq

⊕

0
(t + β1)Fm1
(t + β∗
1)Fm1
0

+

0
Gm1
Gm1
0

⊕
· · · ⊕

0
(t + βs)Fms
(t + β∗
s)Fms
0

+

0
Gms
Gms
0

. (8.1.6)
Here, ε1 ≤· · · ≤εp, the numbers k1 ≤· · · ≤kr and ℓ1, . . . , ℓq are positive integers,
αj are real numbers, βj are nonreal quaternions, and δ1, . . . , δr, η1, . . . , ηq are signs,
each equal to +1 or −1.
The form (8.1.6) is uniquely determined by A + tB up to an arbitrary permuta-
tion of the blocks in each of the parts
⊕r
j=1(δj(Fkj + tGkj)),
⊕q
j=1
 ηj
 (t + αj) Fℓj + Gℓj

,
and (8.1.4) and up to replacement of βj in every block (8.1.5) by a similar quater-
nion.
The next section is dedicated to the proof of Theorem 8.1.2.
The signs δ1, . . . , δr, η1, . . . , ηq form the sign characteristic of the pencil A+tB.
It associates a sign ±1 to every partial multiplicity of A + tB corresponding to real
eigenvalues and inﬁnity.
Remark 8.1.3. One can take the βj’s in Theorem 8.1.2 to be complex numbers
with positive imaginary parts.
Remark 8.1.4. An alternative version of Theorem 8.1.2 is obtained by using
eGα := FαGαFα =
 0
0
0
Fα−1

(8.1.7)
in place of Gα and
"
0
eGα
eGα
0
#
in place of

0
Gα
Gα
0

(β ̸= β∗).
The veriﬁcation of the alternative version is left as Ex. 8.7.1, and we indicate
only the following relevant, easily veriﬁable equalities:
F2ε+1

t


0
0
Fε
0
0
0
Fε
0
0

+ G2ε+1

=

t


0
0
Fε
0
0
0
Fε
0
0

+ eG2ε+1

F2ε+1;
Fk (Fk + tGk) =

Fk + t eGk

Fk;

PENCILS OF HERMITIAN MATRICES
175
F2m

0
(t + β)Fm
(t + β∗)Fm
0

+

0
Gm
Gm
0

=
 
0
(t + β∗)Fm
(t + β)Fm
0

+
"
0
eGm
eGm
0
#!
F2m,
β ̸= β∗.
We also indicate an explicit congruence of the block

0
(t + βj)Fmj
(t + β∗
j )Fmj
0

+

0
Gmj
Gmj
0

(j = 1, . . . , s)
(8.1.8)
to the analogous block in which βj is replaced by β∗
j . Denoting the block (8.1.8)
by K2mj(βj), we have:

0
Imj
Imj
0

K2mj(βj)

0
Imj
Imj
0

= K2mj
 β∗
j

.
Remark 8.1.5. Another alternative version of Theorem 8.1.2 is obtained by
using −eGm and/or −Gm in place of eGm and/or Gm. This observation is based on
equalities
Ξℓ(1)((t + α)Fℓ−Gℓ)Ξℓ(1)
=
(t + α)Fℓ+ eGℓ,
Ξℓ(1)(Fℓ−tGℓ)Ξℓ(1)
=
Fℓ+ eG,
where Ξℓ(1) is given by (1.2.6). Noting that Ξℓ(1)∗= (−1)ℓ−1Ξℓ(1), we obtain:
Upon replacement of Gℓj, resp. eGℓj, by −Gℓj, resp. −eGℓj, in the block (t+αj)Fℓj +
Gℓj, resp. (t + αj)Fℓj + eGℓj, in (8.1.6), the sign ηj remains the same if ℓj is odd
and reverses to its negative if ℓj is even. This rule applies to the blocks Fki + tGki
as well.
Remark 8.1.6. The blocks η (Fk + tGk) and η ((t + α) Fk + Gk) (η = ±1, α
real), which appear in (8.1.6), can be conveniently represented in a uniﬁed form
(sin θ)Fk −(cos θ)Gk + t((cos θ)Fk + (sin θ)Gk),
(8.1.9)
where 0 ≤θ < 2π.
The parameters θ, α, and η are related by α = tan θ and by the real number
(−1)k−1 cos θ having the sign of η (assuming cos θ ̸= 0; if cos θ = 0, then (8.1.9)
coincides with η (Fk + tGk)). To verify this statement, we present the following
proposition (in a more general form than is actually needed here).
Proposition 8.1.7. Let H1 = [h(1)
ij ]k
i,j=1 and H2 = [h(2)
ij ]k
i,j=1 be real triangular
Hankel matrices:
h(ℓ)
ij =



p(ℓ)
i+j−1
if i + j ≤k + 1,
0
otherwise,
where p(ℓ)
1 , . . . , p(ℓ)
k
are real numbers, ℓ= 1, 2. Assume that p(1)
k
̸= 0 and
x := p(2)
k−1p(1)
k
−p(1)
k−1p(2)
k
̸= 0.
(8.1.10)
Then the canonical form of the real symmetric matrix pencil tH1 + H2, under R-
congruence, is given by
η((t + α)Fk + Gk),
(8.1.11)

176
CHAPTER 8
where α := p(2)
k /p(1)
k
and
η =











sign p(1)
k
if x > 0,
sign p(1)
k
if x < 0 and k odd,
−sign p(1)
k
if x < 0 and k even.
(8.1.12)
The proof will be given in terms of H-hermitian matrices and their canonical
forms, as described later in Chapter 10. For this reason the proof of Proposition
8.1.7 is postponed until Chapter 10.
The alternative form (8.1.9) is just a particular case of Proposition 8.1.7 (as-
suming cos θ ̸= 0), with
p(1)
k
= cos θ,
p(1)
k−1 = sin θ,
p(2)
k
= sin θ,
p(2)
k−1 = −cos θ,
and
p(1)
j
= p(2)
j
= 0
for j = 1, 2, . . . , k −2.
Remark 8.1.8. Note that the canonical forms (8.1.3) and (8.1.6) can be chosen
to involve only real and complex numbers. It turns out that a version of these
canonical forms exists that involves real numbers only. We will provide explicit
formulas. Let β ∈H\R, and write β = µ−qν, where µ, ν ∈R, ν ̸= 0, and q2 = −1.
We will work with the following matrices:
C2m :=
1
√
2

1
1
−q
q

⊕

1
1
−q
q

⊕· · · ⊕

1
1
−q
q

of size 2m × 2m, where m is a positive integer,
D2m := [ e1
e3
. . .
e2m−1
e2
e4
. . .
e2m ] ∈R2m×2m,
and T2m = C2mD2m. Let Z(t, µ, ν) be the real matrix pencil given by (1.2.9). A
computation shows that
T ∗
2mZ2m(t, µ, ν)T2m =
 0m
Km
K∗
m
0m

,
where Km := qGm + (qt + qµ + ν)Fm. Therefore,

Im
0
0
−q−1Im

T ∗
2mZ2m(t, µ, ν)T2m

Im
0
0
q−1Im

=

0m
Gm + (t + β)Fm
Gm + (t + β∗)Fm
0m

,
and so the block (8.1.8) in (8.1.6) can be replaced by a real symmetric block
Z2mj(t, R(β), −q−1V(β)).

PENCILS OF HERMITIAN MATRICES
177
8.2
PROOF OF THEOREM 8.1.2
We start with the proof of Part (a).
Thus, let A + tB a hermitian matrix pencil, and let A0 + tB0 be its Kronecker
form of (7.3.1). Then
A0 + tB0 = P(A + tB)Q
(8.2.1)
for some invertible P, Q ∈Hn×n. Taking conjugate transposes in (8.2.1), we have
A∗
0 + tB∗
0 = Q∗(A + tB)P ∗.
Clearly, A∗
0 + tB∗
0 is also strictly equivalent to A + λB. But
A∗
0 + tB∗
0
=
0v×u ⊕LT
ε1×(ε1+1) ⊕· · · ⊕LT
εp×(εp+1)
⊕
Lη1×(η1+1) ⊕· · · ⊕Lηq×(ηq+1)
⊕
(Ik1 + tJk1(0)T ) ⊕· · · ⊕(Ikr + tJkr(0)T )
⊕
(tIℓ1 + Jℓ1(α1)∗) ⊕· · · ⊕(tIℓs + Jℓs(αs)∗),
which, in view of the similarity between Jm(α)∗and Jm(α∗), as established by the
equality
FmJm(α∗)Fm = (Jm(α))∗,
is strictly equivalent to
0r×u
⊕
LT
ε1×(ε1+1) ⊕· · · ⊕LT
εp×(εp+1)
⊕
Lη1×(η1+1) ⊕· · · ⊕Lηq×(ηq+1)
⊕
(Ik1 + tJk1(0)) ⊕· · · ⊕(Ikr + tJkr(0))
⊕
(tIℓ1 + Jℓ1(α∗
1)) ⊕· · · ⊕(tIℓs + Jℓs(α∗
s)).
(8.2.2)
Now the uniqueness of the Kronecker form implies that r = u; p = q; εj = ηj
for
j = 1, . . . , q and, also, that the number of blocks tIℓu + Jℓu(αt) with αu nonreal is
even and these blocks appear in pairs: tIℓu + Jℓu(αu), tIℓu + Jℓu(α∗
u).
The goal of this analysis is to obtain an hermitian matrix pencil which is strictly
equivalent to (8.2.2). To this end, symmetric expressions can be obtained from the
blocks of (8.2.2) by applying suitable strict equivalence transformations. Thus, for
the singular part of the form (8.2.2), it is easily veriﬁed that
(Lε×(ε+1) ⊕(Fε+1LT
ε×(ε+1)Fε))F2ε+1 = t


0
0
Fε
0
0
0
Fε
0
0

+ G2ε+1.
Similarly, for the regular part:
(Iku + tJku(0))Fku = tGku + Fku;
(tIℓu + Jℓu(α))Fℓu = (t + α)Fℓu + Gℓu,
where α is real; and
((tIℓu + Jℓu(α)) ⊕(tIℓu + Jℓu(α∗)))F2ℓu
=

0
(t + α)Fℓu
(t + α∗)Fℓu
0

+

0
Gℓu
Gℓu
0

(8.2.3)

178
CHAPTER 8
for nonreal α.
Applying these transformations to the blocks in (8.2.2), the form (8.1.3) is
obtained.
The uniqueness statement in Part (a) is immediate from the uniqueness state-
ment in Theorem 7.3.2 and the above analysis.
Next, we prove existence in Part (b). For this a technical lemma will be needed.
Lemma 8.2.1. Let p(t)
be a real scalar polynomial having one of the forms
p(t) = t −a with a > 0, or p(t) = (t −a)(t −a) with a ∈C \ R. Then for every
positive integer m, there exists a scalar polynomial fm(t) with real coeﬃcients such
that t ≡(fm(t))2 modulo a real polynomial multiple of (p(t))m.
Proof. For m = 1, we let f1(t) = √a if p(t) = t −a, and
f1(t) = {|a| + t} {2(|a| + R(a))}−1/2
if p(t) = (t −a)(t −a). Continue by induction on m. If fm(t) has already been
found, then
t = fm(t)2 + u(t)(p(t))m
(8.2.4)
for some real polynomial u(t).
In particular, in view of the form of p(t), (8.2.4)
implies that fm(t) and p(t) are relatively prime. Therefore, there exist polynomials
g(t) and h(t) with real coeﬃcients such that
2g(t)fm(t) + h(t)p(t) = u(t).
Now set
fm+1(t) = fm(t) + g(t)(p(t))m.
□
We now return to the proof of existence in Part (b).
Let A + tB be a hermitian matrix pencil, with matrices of size n × n.
By
Theorem 8.1.2, there exist invertible matrices P, Q ∈Hn×n such that
A + tB = P (A0 + tB0) Q,
(8.2.5)
where A0 +tB0 is the canonical form of (8.1.3). Replacing A+tB by the congruent
pencil (Q∗)−1(A+λB)Q−1, it may (and will) be assumed without loss of generality
that Q = I. Then the equalities A = A∗, B = B∗imply
P(A0 + tB0) = A∗+ tB∗= (A∗
0 + tB∗
0)P ∗= (A0 + tB0)P ∗,
i.e.,
PA0 = A0P ∗,
PB0 = B0P ∗.
(8.2.6)
Therefore, for every polynomial f(t) with real coeﬃcients we also have
f(P)A0 = A0(f(P))∗,
f(P)B0 = B0(f(P))∗.
(8.2.7)
The argument now proceeds in several cases, depending on the nature of eigen-
values of P.
Case 8.2.2. P has only one distinct complex eigenvalue with nonnegative real
part, and this eigenvalue is real.

PENCILS OF HERMITIAN MATRICES
179
Assume ﬁrst that the eigenvalue of P is positive.
Let (t −γ)m, γ > 0, be
the minimal polynomial of P −1. By Lemma 8.2.1 there exists a polynomial fm(t)
with real coeﬃcients such that t = (fm(t))2 modulo a real polynomial multiple
of (t −γ)m. Then P −1 = (fm(P −1))2, and since P −1 itself is a polynomial of P
with real coeﬃcients, we have P −1 = (f(P))2 for some polynomial f(t) with real
coeﬃcients. Let R = f(P), and use (8.2.7) to obtain
R(A + tB)R∗
=
RP(A0 + tB0)R∗= RP(A0 + tB0)(f(P))∗
=
f(P)Pf(P)(A0 + tB0) = A0 + tB0,
(8.2.8)
where the second equality is valid by virtue of (8.2.7), and the existence of (8.1.6)
follows.
Now assume that the eigenvalue of P is negative.
Then, arguing as in the
preceding paragraph, it is found that (−P)−1 = (f(−P))2 for some polynomial
f(t) with real coeﬃcients. As in (8.2.8) it follows that
R(A + tB)R∗= −(A0 + tB0),
where R = f(−P). Thus, to complete the proof of existence of the form (8.1.6) in
Case 8.2.2, it remains to show that each of the blocks
Z1 := t


0
0
Fε
0
0
0
Fε
0
0

+ G2ε+1
(8.2.9)
and
Z2 :=

0
(t + β)Fm
(t + β∗)Fm
0

+

0
Gm
Gm
0

,
β ∈H \ R,
(8.2.10)
is congruent to its negative. This is easily seen because for the block (8.2.10) we
have

Im
0
0
−Im

Z2

Im
0
0
−Im

= −Z2,
and for the block (8.2.9) we have
 Iε
0
0
−Iε+1

Z1
 Iε
0
0
−Iε+1

= −Z1.
(8.2.11)
Case 8.2.3. P has exactly one distinct complex eigenvalue with nonnegative
imaginary part, and this eigenvalue is nonreal.
Let α ̸= α be the complex eigenvalue of P −1 with nonnegative imaginary part.
If g(t) = ((t −α)(t −α))w is the minimal polynomial of P −1, then P −1 satisﬁes
g(P −1) = 0. Using Lemma 8.2.1, we ﬁnd a polynomial gw(t) with real coeﬃcients
such that t ≡(gw(t))2 modulo a real polynomial multiple of g(t). Then P −1 =
(gw(P −1))2. Since P is itself a matrix root of a polynomial with real coeﬃcients,
namely, bg(P) = 0, where
bg(t) = ((t −α−1)(t −(α)−1))w,
it follows that P −1 is also a polynomial of P with real coeﬃcients. Now argue as
in Case 8.2.2 (when the eigenvalue of P is positive).

180
CHAPTER 8
Case 8.2.4. All other possibilities (not covered in Cases 8.2.2 and 8.2.3).
In this case, using the quaternion Jordan form of Theorem 5.5.3, P can be
written in the form
P = S diag (P1, P2, . . . , Pr) S−1,
(8.2.12)
where S ∈Hn×n is invertible and P1, . . . , Pr are matrices of sizes n1×n1, . . . , nr×nr,
respectively, such that
λ ∈σ(Pi) =⇒µλµ−1 /∈σ(Pj)
for j ̸= i and for any µ ∈H \ {0}
(8.2.13)
(i.e., any similarity class of quaternion eigenvalues of P is conﬁned to just one block,
Pj). We also have r ≥2 (the situations when r = 1 are covered in Cases 8.2.2 and
8.2.3). Substituting (8.2.12) in the equality A + tB = P(A0 + tB0), we obtain
 P −1
1
⊕· · · ⊕P −1
r

( eA + t eB) = eA0 + t eB0,
(8.2.14)
where
eA = S−1A(S∗)−1,
eB = S−1B(S∗)−1,
eA0 = S−1A0(S∗)−1,
eB0 = S−1B0(S∗)−1.
Partition the matrix eA :
eA = [Mij]r
i,j=1 ,
where Mij is of the size ni × nj. Since eA and eA0 are hermitian, (8.2.14) implies
P −1
i
Mij = Mij(P ∗
j )−1.
In view of (8.2.13),
σ(P −1
i
) ∩σ((P ∗
j )−1) = ∅
for
i ̸= j.
Now by Theorem 5.11.1 we have Mij = 0 (i ̸= j). In other words,
eA = M11 ⊕· · · ⊕Mrr.
Similarly, eB = N1 ⊕· · · ⊕Nrr, where Nii is of size ni × ni.
Now, induction is used on n to complete the proof that every hermitian matrix
pencil is congruent to a pencil of the form (8.1.6); the basis of induction, when A
and B are scalars, is trivially veriﬁed. Indeed, by the induction hypothesis, each
pencil Mii + tNii is congruent to a pencil of the form (8.1.6); therefore, the same
is true for
eA + t eB = (M11 + tN11) ⊕· · · ⊕(Mrr + tNrr).
This completes the proof of existence of the form (8.1.6).
It remains to prove the uniqueness in Part (b).
It will be proved using the
real matrix representation given by the map χn,n. Let A0 + tB0 be the hermitian
matrix pencil given by (8.1.6), let A′
0 + tB′
0 be a hermitian matrix pencil given by
the same formula (8.1.6) but with possibly diﬀerent signs δ′
1, . . . , δ′
r, η′
1, . . . , η′
q in
place of δ1, . . . , δr, η1, . . . , ηq, respectively, and assume that A0 + tB0 and A′
0 + tB′
0
are congruent:
A′
0 + tB′
0 = S∗(A0 + tB0)S,
S ∈Hn×n invertible.
(8.2.15)

PENCILS OF HERMITIAN MATRICES
181
We have to prove that for every positive integer k the equality
X
j=1,2,...,kr such that kj=k
δ′
j =
X
j=1,2,...,kr such that kj=k
δj
(8.2.16)
holds true, and for every real number α and every positive integer k the equality
X
j=1,2,...,q such that αj=α and ℓj=k
ηj =
X
j=1,2,...,q such that αj=α and ℓj=k
η′
j
(8.2.17)
holds true. We focus on (8.2.16); the consideration of (8.2.17) is completely anal-
ogous. To this end apply the map χn,n to equality (8.2.15). We then obtain that
the real symmetric matrix pencils χn,n (A0)+tχn,n (B0) and χn,n (A′
0)+tχn,n (B′
0)
are R-congruent.
It is easy to see that the part of χn,n (A0) + tχn,n (B0) that
corresponds to the eigenvalue at inﬁnity is R-congruent to
r
X
j=1
δj
 (Fkj + tGkj)⊕4
.
(8.2.18)
Analogously, the part of χn,n (A′
0) + tχn,n (B′
0) corresponding to the eigenvalue at
inﬁnity is R-congruent to
r
X
j=1
δ′
j
 (Fkj + tGkj)⊕4
.
(8.2.19)
It follows that (8.2.18) and (8.2.19) are R-congruent, and the uniqueness of the
canonical form of a pair of real symmetric matrices under R-congruence (see The-
orem 15.2.1) yields the desired equality (8.2.16).
□
We remark that a direct, independent proof of uniqueness can be obtained by
following the arguments in the the proof of the corresponding result in the complex
case in Lancaster and Rodman [92].
8.3
POSITIVE SEMIDEFINITE LINEAR COMBINATIONS
Deﬁnition 8.3.1. If A, B ∈Hn×n are hermitian and aA+bB is positive semidef-
inite for some real numbers a, b not both zero, then we say that the pair (A, B)
admits a positive semideﬁnite linear combination. Similarly, we say that the pair
(A, B) admits a positive deﬁnite linear combination if aA + bB is positive deﬁnite
for some real numbers a, b.
As we shall see later in this section, these properties play a role in the important
problem of simultaneous diagonalization.
We start with an elementary proposition.
Proposition 8.3.2. Let A, B ∈Hn×n be hermitian. Then:
(a) if (A, B) admits a positive semideﬁnite, resp. deﬁnite, linear combination,
then so do all pairs simultaneously congruent to (A, B), i.e., of the form
(S∗AS, S∗BS) for some invertible S ∈Hn×n;

182
CHAPTER 8
(b) if (A, B) admits a positive semideﬁnite, resp. deﬁnite, linear combination,
then so do all pairs of the form (αA + βB, γA + δB), where α, β, γ, δ are real
numbers such that αδ −βγ ̸= 0.
Proof. Part (a) follows from a particular case of the inertia theorem: a her-
mitian matrix is positive semideﬁnite, resp. deﬁnite, if and only if any congruent
matrix is as well. For Part (b), note that
a′(αA + βB) + b′(γA + δB) = (a′α + b′γ)A + (a′β + b′δ)B,
a′, b′, a, b ∈R,
and
aA + bB = a′(αA + βB) + b′(γA + δ)B,
a, b ∈R,
(8.3.1)
where in (8.3.1) a′, b′ are found from the equation
 α
γ
β
δ
  a′
b′

=
 a
b

.
Thus, every linear combination with real coeﬃcients of A and B is that of αA+βB
and γA + δB, and vice versa, and part (b) follows.
□
As an application of Theorem 8.1.2, a criterion for having a positive semideﬁnite
linear combination can now be given.
Theorem 8.3.3. Let A, B ∈Hn×n be hermitian. Then the pair (A, B) admits
a positive semideﬁnite linear combination if and only if the following property holds
true.
(α) For any x ∈Hn×1 such that ⟨Ax, x⟩= ⟨Bx, x⟩= 0, the two vectors Ax and
Bx are linearly dependent over the reals.
We relegate the proof of Theorem 8.3.3 to the next section.
The following result provides a criterion for existence of a positive (or negative)
deﬁnite linear combination.
Theorem 8.3.4. (a) Let A, B ∈Hn×n be hermitian.
Then the pair (A, B)
admits a positive deﬁnite linear combination if and only if the following property
holds true:
⟨Ax, x⟩= ⟨Bx, x⟩= 0,
x ∈Hn×1
=⇒
x = 0.
(8.3.2)
Proof. If C := aA + bB is positive deﬁnite for some real a and b, then (8.3.2)
obviously holds true, because ⟨Cx, x⟩= 0 is possible only if x = 0. Conversely,
if (8.3.2) holds true, then 0 ̸∈WJH
∗(A, B), the joint numerical range of A and
B. Then the convexity (Theorem 3.5.7) and compactness of WJH
∗(A, B) imply by
virtue of the convex separation theorem, 6.6.2, that
WJH
∗(A, B) ⊆{(x1, x2) : ax1 + bx2 ≥c},
where a, b, c are real numbers such that a and b are not both zero and c > 0. Then
aA + bB is positive deﬁnite.
□
It is a well-known fact that if A and B are two complex hermitian (or real
symmetric) matrices and if the pair (A, B) admits a positive deﬁnite linear combi-
nation, then A and B are simultaneously diagonalizable by congruence, i.e., there

PENCILS OF HERMITIAN MATRICES
183
exist real diagonal matrices D1 and D2 such that the hermitian pencil A + tB is
congruent to D1 + tD2 (with the real congruence matrix if A and B are real). The
result is valid also for quaternion hermitian matrices, and can be easily obtained
by inspection of (8.1.3). It generally fails if the positive deﬁniteness hypothesis is
omitted.
Example 8.3.5. Let
A =
 0
1
1
0

,
B =
 1
0
0
0

.
Then there does not exist an invertible S ∈Hn×n such that both S∗AS and S∗BS
are diagonal. Indeed, this follows from the uniqueness of the canonical form of
the matrix pencil A + tB in Theorem 8.1.2, since
 t
1
1
0

represents one of the
primitive canonical blocks in that theorem and, therefore, cannot be simultaneously
congruent to a direct sum of primitive blocks.
□
Theorem 8.1.2(b) leads to results concerning simultaneous diagonalizability with
the positive deﬁniteness hypothesis relaxed.
Theorem 8.3.6. Let A, B ∈Hn×n be hermitian matrices. Assume that there
exists a linear combination
C = αA + βB,
α, β ∈R,
such that C is positive semideﬁnite, and
Ker (C) ⊆Ker (A) ∩Ker (B).
(8.3.3)
Then A and B are simultaneously diagonalizable by congruence, with a quaternion
congruence matrix.
In particular, the hypotheses of Theorem 8.3.6 are satisﬁed if C is positive
deﬁnite.
Proof. We may clearly assume that at least one of α and β is nonzero (otherwise
the theorem is trivial). Applying the transformation of Proposition 8.3.2, we may
further assume that C = B. Finally, take A+tB in the form (8.1.6). The condition
that B is positive semideﬁnite easily implies that
A + tB = 0u×u
⊕
δ1 (Fk1 + tGk1) ⊕· · · ⊕δr (Fkr + tGkr)
⊕
diag (t + α1, . . . , t + αq),
(8.3.4)
where kj ≤2.
If there is a term in (8.3.4) with kj = 2, then Ker (B) is not
contained in Ker (A), a contradiction to (8.3.3). Thus, we must have all kj = 1,
and the right-hand side of (8.3.4) is diagonal.
□
Example 8.3.5 shows that condition (8.3.3) is essential in Theorem 8.3.6.
8.4
PROOF OF THEOREM 8.3.3
It will be convenient to dispose of a particular case ﬁrst.

184
CHAPTER 8
Lemma 8.4.1. Let A, B ∈Hn×n be hermitian with block forms
A =
 D1
0
0
D2

,
B =
 Ip
0
0
−Iq

,
where D1 ∈Hp×p and D2 ∈Hq×q are hermitian. If property (α) is satisﬁed, then
the pair (A, B) admits a positive semideﬁnite linear combination.
Proof. Applying unitary similarity to each of D1 and D2, we may assume that
D1 and D2 are diagonal. The proof proceeds by induction on p and q. The basis
of induction, i.e., p = 0 or q = 0, is trivial (then ±B is positive deﬁnite). The case
p = q = 1 is handled next; thus, let
A =
 d1
0
0
d2

,
B =
 1
0
0
−1

,
d1, d2 ∈R.
If A is positive or negative semideﬁnite, we are done; otherwise, replacing A with −A
and multiplying A by a positive number, we can assume without loss of generality,
that A = diag (1, d), where d < 0. If d ≥−1, then A −B is positive semideﬁnite;
if d < −1, then −A + B is positive semideﬁnite.
Suppose at least one of p and q is larger than 1; say p > 1. We may assume
that the largest eigenvalue of D1, call it γ, is in the top left corner of D1. Replace
A with A −γB. This transformation does not change the property (α) and the
property of having a (nontrivial) positive semideﬁnite linear combination. Then
A =


01×1
0
0
0
eD1
0
0
0
eD2

,
where eD1 is negative semideﬁnite. Applying the induction hypothesis to
eA :=
"
eD1
0
0
eD2
#
,
eB :=
 Ip−1
0
0
−Iq

,
we see that there exist α, β ∈R not both zeros such that α eA + β eB is positive
semideﬁnite. Since eD1 is negative semideﬁnite, it follows that β ≥0. But then
αA + βB is positive semideﬁnite as well.
□
Proof of Theorem 8.3.3.
Assume αA + βB is positive semideﬁnite for some
α, β ∈R, not both zero. If ⟨Ax, x⟩= ⟨Bx, x⟩= 0, then ⟨(αA + βB)x, x⟩= 0, and
in view of positive semideﬁniteness of αA + βB we have (αA + βB)x = 0. So the
R-linear dependence of Ax and Bx follows.
Conversely, assume that (α) holds true. Without loss of generality, it may be
assumed that the hermitian pencil A + tB has the form (8.1.6). Clearly, condition
(α) holds true for every constituent diagonal block in the direct sum (8.1.6).
Now consider whether condition (α) holds true for each of the possible diagonal
blocks in (8.1.6). Clearly, (α) holds true for 0u×u.
Let
A′ + tB′ = λ


0
0
Fε
0
0
0
Fε
0
0

+ G2ε+1,

PENCILS OF HERMITIAN MATRICES
185
and let x = e1 ∈H(2ε+1)×1, the ﬁrst unit coordinate vector.
Then ⟨A′x, x⟩=
⟨B′x, x⟩= 0, but A′x and B′x are linearly independent. Thus, (α) does not hold
true for the pair (A′, B′).
Let
A′ + tB′ = Fk + tGk
or
A′ + tB′ = (t + α)Fk + Gk,
(α ∈R).
Then, using x = e1 again, it follows that (α) does not hold for the pair (A′, B′),
unless k ≤2.
Let
A′ + tB′ =

0
(t + β)Fm + Gm
(t + β)Fm + Gm
0

,
β ∈C \ R.
Then, using x = e1 we see that (α) does not hold for (A′, B′).
Next, consider the four possibilities (8.4.1)–(8.4.4) for the pair (A′, B′). It will
be proved that, in each of the four cases, the pair (A′, B′) does not have property
(α):
A′
=
 0
1
1
0

⊕±
 1
α
α
0

,
B′
=
 1
0
0
0

⊕±
 0
1
1
0

,
α ∈R.
(8.4.1)
A′
=
 0
1
1
0

⊕

0
−1
−1
0

,
B′
=
 1
0
0
0

⊕
 −1
0
0
0

,
α ∈R.
(8.4.2)
A′
=
 1
α
α
0

⊕
 −1
−α
−α
0

,
B′
=
 0
1
1
0

⊕

0
−1
−1
0

,
α ∈R.
(8.4.3)
A′
=

1
α
α
0

⊕±

1
β
β
0

,
B′
=
 0
1
1
0

⊕±
 0
1
1
0

,
α, β ∈R,
α ̸= β.
(8.4.4)
Now, in each of the four cases, a vector x ∈R4 is formulated for which ⟨Ax, x⟩=
⟨Bx, x⟩= 0, and Ax, Bx are R-linearly independent:
x = e2 + e4
if (A′, B′) are given by (8.4.1) or (8.4.4)
(we denote by ep the unit coordinate vector with 1 in the pth position and zeros
elsewhere), and
x = e1 + e3
if (A′, B′) are given by (8.4.2) or (8.4.3).
Using the above analysis and, if necessary, replacing (A, B) by (−A, −B), we

186
CHAPTER 8
see that the pair (A, B) can be taken in one of the forms
A + tB
=
0u×u ⊕δ1 ⊕· · · ⊕δr ⊕η1 (t + α1) ⊕· · · ⊕ηq (t + αq)
⊕
 0
1
1
0

+ t
 1
0
0
0

⊕· · · ⊕
 0
1
1
0

+ t
 1
0
0
0

,
(8.4.5)
where δj and ηj are signs ±1, and αj ∈R, or
A + tB
=
0u×u ⊕δ1 ⊕· · · ⊕δr ⊕η1 (t + α1) ⊕· · · ⊕ηq (t + αq)
⊕
 1
γ
γ
0

+ t
 0
1
1
0

⊕· · ·
⊕
 1
γ
γ
0

+ t
 0
1
1
0

,
(8.4.6)
where δj and ηj are signs ±1 and αj, γ ∈R. Without loss of generality, it may be
assumed that u = 0, i.e., the term 0u×u does not appear in (8.4.5) and (8.4.6).
Suppose ﬁrst that (8.4.5) holds, and assume that at least one of the blocks
ηj (t + αj) and at least one of the blocks
 0
1
1
0

+t
 1
0
0
0

is present. If one of
the signs ηj is −1, say, η1 = −1, then we obtain a contradiction with the hypothesis
that (A, B) satisﬁes property (α), because the pair
(A′, B′) =




q
0
0
0
0
1
0
1
0

,


−1
0
0
0
1
0
0
0
0



,
q ∈R,
(8.4.7)
does not satisfy property (α), as can be easily seen by considering the vector x =
 1
1
−q/2 T . Indeed, ⟨A′x, x⟩= ⟨B′x, x⟩= 0, but A′x =
 q
−q/2
1 T
and B′x =
 −1
1
0 T are linearly independent. Thus, all ηj = 1. But then B
is positive semideﬁnite, and we are done in this case.
If no blocks ηj (t + αj) are present, then B is positive semideﬁnite, and it may
be assumed that no blocks
 0
1
1
0

+ t
 1
0
0
0

are present. We may further
assume (adding if necessary a suitable scalar multiple of B to A), that all the αj’s
are diﬀerent from zero. Now the result follows from Lemma 8.4.1 (with the roles
of A and B interchanged), upon applying a suitable simultaneous congruence to A
and B.
Next, suppose that (8.4.6) holds. Then
B + t(A −γB)
=
tδ1 ⊕· · · ⊕tδr
⊕(η1 + t(η1α1 −η1γ)) ⊕· · · ⊕(ηq + t(ηqαq −ηqγ))
⊕
 0
1
1
0

+ t
 1
0
0
0

⊕· · · ⊕
 0
1
1
0

+ t
 1
0
0
0

.
(8.4.8)
This form can be easily reduced, after a suitable congruence with a diagonal con-
gruence matrix to a form of type (8.4.5), to complete the proof.
□

PENCILS OF HERMITIAN MATRICES
187
8.5
COMPARISON WITH REAL AND COMPLEX CONGRUENCE
Theorem 8.5.1. Let A, B, A′, B′ ∈Rm×m be real symmetric matrices. Then,
the real matrix pencils A + tB and A′ + tB′ are H-congruent if and only if A + tB
and A′ + tB′ are R-congruent.
Proof. The “if” part being trivial, we prove the “only if” part. Let an invertible
matrix S ∈Hm×m be such that equality
S∗(A + tB)S = A′ + tB′
(8.5.1)
holds true, and write S = S0+iS1+jS2+kS3, where S0, S1, S2, S3 are real matrices.
Then (8.5.1) takes the form


ST
0
ST
1
ST
2
ST
3
ST
1
−ST
0
−ST
3
ST
2
ST
2
ST
3
−ST
0
−ST
1
ST
3
−ST
2
ST
1
−ST
0




A + tB
0
0
0
0
A + tB
0
0
0
0
A + tB
0
0
0
0
A + tB


·


S0
S1
S2
S3
S1
−S0
S3
−S2
S2
−S3
−S0
S1
S3
S2
−S1
−S0


=


A′ + tB′
0
0
0
0
A′ + tB′
0
0
0
0
A′ + tB′
0
0
0
0
A′ + tB′

.
(8.5.2)
The matrix
U :=


S0
S1
S2
S3
S1
−S0
S3
−S2
S2
−S3
−S0
S1
S3
S2
−S1
−S0


is invertible. Indeed, the equality Ux = 0, x = [x0 x1 x2 x3]T ∈R4, is equivalent
to S(x0 −ix1 −jx2 −kx3) = 0, and, therefore, in view of the invertibility of S, x
must be the zero vector. Now apply Corollary 15.3.7 to conclude that A + tB and
A′ + tB′ are R-congruent.
□
An analogue of Theorem 8.5.1 holds in the complex case, with the transposes
replaced by conjugate transposes.
Theorem 8.5.2. Let A, B, A′, B′ ∈Cm×m be hermitian matrices. Then, the
complex matrix pencils A + tB and A′ + tB′ are H-congruent if and only if A + tB
and A′ + tB′ are C-congruent.
It will be convenient to prove a lemma ﬁrst.
Lemma 8.5.3. Every complex hermitian matrix pencil A0 + tB0 is C-congruent
to its conjugate A0 + tB0.
Proof. It is easy to see that we need to consider only the case when A0 + tB0
is in the canonical form of complex hermitian matrix pencils, and then we may
assume that A0 + tB0 coincides with a primitive block. The primitive blocks for

188
CHAPTER 8
this canonical form are given in Theorem 15.3.1; they are either real, in which case
the statement of the lemma is trivial, or they have the form
 0
X
X
0

, where X
is a square-size complex matrix, in which case the congruence
 0
I
I
0
  0
X
X
0
  0
I
I
0

=

0
X
X
0

completes the proof.
□
Proof of Theorem 8.5.2. Again, we prove only the “only if” part. Using
the map eωn of (5.6.1) and the fact that eωn is a one-to-one unital homomorphism
of real algebras that preserves the ∗operation, we see that the complex hermitian
2m × 2m matrix pencils
 A + tB
0
0
A + tB

and
 A′ + tB′
0
0
A′ + tB′

are C-congruent. Using Lemma 8.5.3, it follows that
 A + tB
0
0
A + tB

is C-
congruent to
 A′ + tB′
0
0
A′ + tB′

. Now apply Corollary 15.3.7.
□
8.6
EXPANSIVE AND PLUS-MATRICES: SINGULAR H
In this section we focus on H-expansive matrices. We ﬁx a (not necessarily invert-
ible) hermitian matrix H ∈Hn×n, and let [x, y] = ⟨Hx, y⟩= ⟨x, Hy⟩.
Deﬁnition 8.6.1. A matrix A ∈Hn×n is called H-expansive if [Ax, Ax] ≥[x, x]
for all x ∈Hn×1 or, equivalently, if A∗HA −H ≥0.
It turns out that the kernel Ker A := {x ∈Hn×1 : Ax = 0} of an H-expansive
matrix A is H-negative (as long as H is invertible). To prove this, we need the
following auxiliary result.
Lemma 8.6.2. Let A ∈Hn×n be H-expansive. Then there exists a nonsingular
matrix S ∈Hn×n such that
S∗HS = H1 ⊕(−Ip2) ⊕0p3,
S∗A∗HAS = M1 ⊕0p2 ⊕0p3,
(8.6.1)
where M1, H1 ∈Hp1×p1 are nonsingular and p1, p2, p3 are nonnegative integers.
Proof. By Theorem 8.1.2 we may assume that H and A∗HA have the forms
H = H1 ⊕H2 ⊕· · · ⊕Hm,
A∗HA = M1 ⊕M2 ⊕· · · ⊕Mm,
where Mj and Hj have the same size, H1 and M1 are nonsingular, and Mj and Hj,
j > 1, are blocks of one of the following types:
type 1: Hj = εFp and Mj = εFpJp(0) for some positive integer p, ε = ±1;
type 2: p nonnegative integer,
Hj =


0
0
Ip
0
0
0
Ip
0
0

,
Mj =


0
0
0
0
0
Ip
0
Ip
0

∈H(2p+1)×(2p+1);

PENCILS OF HERMITIAN MATRICES
189
type 3: Hj = εFpJp(0) and Mj = εFp for some positive integer p, ε = ±1.
Clearly, since A is H-expansive, each matrix Mj −Hj is positive semideﬁnite.
It is easy to check that this is possible if and only if p = 1 and ε = −1 if Hj and
Mj are of type 1, p = 1 and ε = 1 if Hj and Mj are of type 3, and p = 0 if Hj and
Mj are of type 2. But then, after eventually permuting some blocks, H and A∗HA
have the forms
H = H1 ⊕0p2 ⊕(−Ip3) ⊕0p4,
A∗HA = M1 ⊕0p2 ⊕0p3 ⊕Ip4.
Note that M1 −H1 is still positive semideﬁnite; thus, the number of positive eigen-
values of M1 is larger or equal to the number of positive eigenvalues of H1 (counted
with algebraic multiplicities). See Theorem 5.13.1. From the well-known fact that
the number of positive (resp. negative) eigenvalues of A∗HA is always less or equal
to the number of positive (resp. negative) eigenvalues of H (Theorem 5.13.2), it
follows that blocks of type 3 cannot occur and, hence, A∗HA and H have forms as
in (8.6.1).
□
Corollary 8.6.3. Let H be invertible and let A ∈Hn×n be H-expansive. Then
Ker A is H-negative.
Proof. If A is nonsingular, Ker A is H-negative by deﬁnition. Otherwise, let
y ∈Ker A. Then y ∈Ker A∗HA. Since H is invertible, it follows immediately
from Lemma 8.6.2 and Proposition 4.2.5(b) that y∗Hy < 0.
□
Proposition 8.6.4. Let A ∈Hn×n be H-expansive. Then Ker H is A-invariant.
Proof. Applying a transformation of the form (A, H) 7→(S−1AS, S∗HS), we
may assume that H and A∗HA have the forms as in (8.6.1). Applying one more
transformation on M1 and H1, we may furthermore assume that
H = Ip1 ⊕−Ip2 ⊕−Ip3 ⊕0,
A∗HA =
 M11
M12
M ∗
12
M22

⊕0,
where p1, p2, p3 are nonnegative integers, M11 ∈Hp1×p1, M12 ∈Hp1×p2, M22 ∈
Hp2×p2. Let A be partitioned conformally:
A =


A11
A12
A13
A14
A21
A22
A23
A24
A31
A32
A33
A34
A41
A42
A43
A44

.
With A∗HA −H ≥0 also M11 −Ip1 = A∗
11A11 −A∗
21A21 −A∗
31A31 −Ip1 must be
positive semideﬁnite. This is possible only if A∗
11A11 is positive deﬁnite, i.e., only if
A11 is nonsingular. Next, we show A14 = 0. To see this, assume that A14 is not zero.
Then there exists a matrix P ∈Hp4×p1, where p4 = n−p1−p2−p3, such that A11−
PA14 is singular. (For example, if v ∈Hp4 is such that A14v ̸= 0, choose P such
that PA14v = A11v.)
Applying the transformation (A, H) 7→(P −1AP, P ∗HP)
with
P =


Ip1
0
0
0
0
Ip2
0
0
0
0
Ip3
0
P
0
0
Ip4

,

190
CHAPTER 8
we ﬁnd that the (1, 1)-block of P −1AP is A11 −PA14, whereas P ∗HP = H and
P ∗A∗HAP = A∗HA. This contradicts the fact just mentioned that the (1, 1)-block
of P −1AP must be nonsingular. But A14 = 0 implies that the (4, 4)-block of A∗HA
has the form −A∗
24A24 −A∗
34A34 = 0. This is possible only if A24 = 0 and A34 = 0.
Hence, Ker H is A-invariant.
□
The key result in this section is the following theorem.
Theorem 8.6.5. Let A ∈Hn×n be H-expansive and let M0 ⊆Hn be an A-
invariant subspace which is H-nonnegative. Then there exists an H-nonnegative
A-invariant subspace M ⊇M0 such that dim M = In+ (H) + In0 (H).
Proof.
For the case when H is invertible, the result is proved in Theorem
6.2.6. Thus, consider the case that H is singular. Without loss of generality, we
may assume that H has the form (6.3.1), i.e., H = H1 ⊕0ν, where H1 = Ip ⊕(−Iq).
Then Proposition 8.6.4 implies that A takes the form
A =

A11
0
A21
A22

,
where A11 ∈H(p+q)×(p+q) is easily seen to be H1-expansive. Represent M0 accord-
ing to Lemma 6.3.1:
M0 = Im
 Q0
0
Y0
X0

,
Q0 =
 P0
K0

,
where P ∗
0 P0 = Ip, ∥K0∥≤1, X∗
0X0 = I, and X∗
0Y0 = 0. Then f
M0 = Im Q0
is H1-nonnegative and A11-invariant. By the part already proved, there exists an
In+ (H)-dimensional, H1-nonnegative, and A11-invariant subspace f
M that contains
M0. Let f
M = Im Q for some matrix Q of appropriately chosen dimension, i.e.,
Q0 = QW0 for some matrix W0. Then M := Im(Q ⊕Iν) is H-nonnegative and
A-invariant. Furthermore, it contains M0 and has dimension In+ (H)+In0 (H).
□
Obviously, every H-isometric matrix is H-expansive (recall that a matrix A is
called H-isometric if [Ax, Ax] = [x, x] for every x ∈Hn×1), Thus, as an important
corollary of Theorem 8.6.5, we obtain the following.
Theorem 8.6.6. Let A ∈Hn×n be an H-isometric matrix, and let M0 ⊆Hn be
an A-invariant H-nonnegative, resp. H-nonpositive, subspace. Then there exists an
A-invariant H-nonnegative, resp. H-nonpositive, subspace M such that M ⊇M0
and dim M = In+ (H), resp. dim M = In−(H).
The part of Theorem 8.6.6 concerning H-nonpositive subspaces follows by notic-
ing that A is also expansive with respect to −H and applying Theorem 8.6.5 with
H replaced by −H.
Recall that a matrix A ∈Hn×n is called an H-plus-matrix if [Ax, Ax] ≥0 for
every x ∈Hn×1 such that [x, x] ≥0.
Theorem 8.6.7. Let A be an H-plus-matrix such that the subspace Ran A is
not H-nonnegative, and let M0 ⊆Hn×n be an A-invariant subspace which is H-
nonnegative. Then there exists an H-nonnegative A-invariant subspace M ⊇M0
such that dim M = In+(H) + In0(H).

PENCILS OF HERMITIAN MATRICES
191
Proof. By Proposition 6.2.2, we have [Ax, Ax] ≥µ(A)[x, x] for all x ∈Hn×1,
where µ(A) ≥0 is independent of x. Since Ran A is not H-nonnegative, it follows
that µ(A) > 0. Scaling A, if necessary, we can assume that µ(A) = 1. Then A is
H-expansive, and the result follows from Theorem 8.6.5.
□
Theorem 8.6.8. Let A ∈Hn×n be an H-plus-matrix.
Then there exists an
H-nonnegative A-invariant subspace M such that dim M = In+(H) + In0(H).
Proof. By Proposition 6.2.2, we have [Ax, Ax] ≥µ(A)[x, x] for all x ∈Hn×1
and µ(A) ≥0. If µ(A) = 0, then the range of A is an H-nonnegative subspace, and
we are done by Theorem 4.2.6. Indeed, any maximal H-nonnegative subspace M
containing the range of A is A-invariant. If µ(A) > 0, apply Theorem 8.6.7 with
M0 = {0}.
□
It is instructive to compare Theorem 6.2.6 and 8.6.8. The statement of Theorem
6.2.6 is stronger in the sense that it asserts that every H-nonnegative A-invariant
subspace can be enlarged to an A-invariant subspace which is also maximal H-
nonnegative, whereas Theorem 8.6.8 asserts merely existence of A-invariant sub-
spaces which are maximal H-nonnegative. Of course, Theorem 6.2.6 has stronger
hypothesis, namely, that H is invertible. It is an open question whether or not
the statement of Theorem 6.2.6 holds true without the invertibility hypothesis (cf.
Open Problem 6.2.8).
Deﬁnition 8.6.9. As in Section 6.4 (for the case of invertible H), we say that
a matrix A ∈Hn×n is H-expansive if [Ax, Ax] ≥[x, x] for every x ∈Hn×1. A
matrix B ∈Hn×n is called H-dissipative if R([Bx, x]) ≤0 for every x ∈Hn×1 or,
equivalently, HB + B∗H ≤0.
Lemma 6.4.5 remains valid also for singular (noninvertible) H. So, by using
the linear fractional transformations of Section 6.4, we can obtain results on H-
dissipative matrices from those for H-expansive matrices.
Theorem 8.6.10. Let B be H-dissipative.
(a) Let M0 ⊆Hn×1 be a B-invariant H-nonnegative subspace. Then there exists
a B-invariant maximal H-nonnegative subspace M such that M ⊇M0.
(b) Let M′
0 ⊆Hn×1 be a B-invariant H-nonpositive subspace. Then there exists
a B-invariant maximal H-nonpositive subspace M′ such that M′ ⊇M′
0.
The proof is parallel to that of Theorem 6.4.6, using Theorem 8.6.5 instead of
Corollary 6.4.3. Theorem 8.6.10 extends the result of Theorem 6.4.6 to the case of
singular H.
8.7
EXERCISES
Ex. 8.7.1. Provide complete details for veriﬁcation of the alternative version of
Theorem 8.1.2, Part (b).
Ex. 8.7.2. Verify directly that the pair of matrices (A, B) of Example 8.3.5
cannot be simultaneously diagonalized.

192
CHAPTER 8
Ex.
8.7.3. Describe the canonical form under congruence of the hermitian
pencil A + tB (Theorem 8.1.2) in each of the following situations:
(1) B is positive deﬁnite;
(2) B is positive semideﬁnite;
(3) A and B are positive deﬁnite;
(4) A and B are positive semideﬁnite;
(5) A and B each have only one negative eigenvalue (counted with multiplicities).
Ex. 8.7.4. What can you say about the canonical form (8.1.6) if it is known
that the rank of xA + yB is constant for all real x and y not both zero?
Ex. 8.7.5. Find the canonical forms under strict equivalence and under con-
gruence of Theorem 8.1.2 for the following hermitian pencils:
(a)

Ik
tiFk
−tiFk
−Ik

,
(b) F6 + t


0
0
j
0
0
1
0
0
−j
0
0
0
0
0
0
I3

∈H6×6.
Ex. 8.7.6. Find all H-expansive, all H-dissipative, and all H-plus-matrices for
H =
 1
0
0
0

.
Ex. 8.7.7. Let H = Ip ⊕(−In−p), and let A = diag (a1, . . . , an) ∈Hn×n be a
diagonal matrix.
(a) Find for which values of a1, . . . , an the matrix A is an H-plus-matrix.
(b) For the values of a1, . . . , an obtained in Part (a), ﬁnd all A-invariant maximal
H-nonnegative subspaces.
Ex. 8.7.8. Repeat Ex. 8.7.7 for H = Ip ⊕(−Iq) ⊕0n−p−q.
8.8
NOTES
The material of this chapter (except Sections 8.5 and 8.6) is adapted from Lancaster
and Rodman [92], where the exposition is done for complex hermitian and real
symmetric pencils. In the context of real and complex matrices, the canonical forms
of pairs of hermitian/skewhermitian or symmetric/skewsymmetric matrices, have a
long and distinguished history; we refer the reader to Lancaster and Rodman [92,
93], and especially to Thompson [151] for an extensive bibliography on the subject.
Canonical forms for quaternion hermitian/skewhermitian pencils of matrices, as
well as for symplectic and unitary matrices with respect to sesquilinear, symmetric,
or skewsymmetric forms, have also been known in the literature for some time
(see, e.g., Djokovi´c [33] or Djokovi´c et al. [34]) and recently have been subject to
extensive renewed interest and interpretation: Horn and Sergeichuk [64], Sergeichuk
[142, 143], Rodman [132, 133, 135, 134].
The alternative form (8.1.9) was used in Wall [155] (in the context of real sym-
metric matrix pencils).
Lemma 8.2.1 is attributed to Hua [67].

PENCILS OF HERMITIAN MATRICES
193
In the proof of Theorem 8.1.2 we use the standard approach, as in the proofs of
Theorem 6.1 in Lancaster and Rodman [92] (for complex matrices), Theorem 5.1
in Lancaster and Rodman [93] (for real matrices), and Theorem 8.1(b) in Rodman
[132].
Other characterizations of the positive semideﬁnite linear combination property
and an alternative proof of Theorem 8.3.3 are given by Cheung et al. [26] in the
context of selfadjoint operators on complex Hilbert spaces. See also Lancaster and
Ye [95] and Rodman [136].
Theorem 8.3.4 in the context of complex matrices was proved by Finsler [43]
and later reproved many times using various methods; see, e.g., Au-Yeung [7] and
the survey by Uhlig [152]. The proof of Theorem 8.3.4 based on the convexity of
the numerical range (for complex matrices) is due to Au-Yeung and Poon [10].
A proof of Theorem 8.3.6 for real and complex matrices is given in several texts,
including Gantmacher and Krein [51], Gantmacher [48], and Franklin [45].
The material of Section 8.6 is taken from Mehl et al. [110] (where the presen-
tation is in the framework of real and complex matrices).
Various classes of complex plus-matrices have been studied in van der Mee et
al. [106]. A study of the corresponding classes of quaternion plus-matrices remains
an open problem.

Chapter Nine
Skewhermitian and mixed pencils
Here we present canonical forms under strict equivalence and under congruence
for pencils of quaternion matrices A + tB, where one of the matrices A and B is
skewhermitian and the other can be hermitian or skewhermitian. We treat the case
when both A and B are skewhermitian in Section 9.1 (this is the relatively easy
case). The case when one of A and B is hermitian and the other is skewhermitian
is treated in subsequent sections. Comparisons are made with the corresponding
results for real and complex matrix pencils. We give an application to the canonical
form of a quaternion matrix under congruence.
9.1
CANONICAL FORMS FOR SKEWHERMITIAN MATRIX
PENCILS
Here, it turns out that for matrix pencils of type A + tB, where A and B are
skewhermitian quaternion matrices, the canonical forms of strict equivalence and
of simultaneous congruence are the same. The main result runs as follows:
Theorem 9.1.1. (i) Every matrix pencil A+tB, where A and B are quaternion
skewhermitian n × n matrices, is strictly equivalent to a pencil of skewhermitian
matrices of the form
0u
⊕
⊕p
j=1

t


0
0
Fεj
0
01
0
−Fεj
0
0

+


0
Fεj
0
−Fεj
0
0
0
0
0




⊕
⊕r
j=1


0
0
· · ·
0
0
iβj + ti
0
0
· · ·
0
iβj + ti
i
0
0
· · ·
iβj + ti
i
0
...
...
...
...
...
...
iβj + ti
i
0
· · ·
0
0


kj×kj
⊕
⊕q
j=1


0
0
· · ·
0
0
i
0
0
· · ·
0
i
ti
0
0
· · ·
i
ti
0
...
...
...
...
...
...
i
ti
0
· · ·
0
0


ℓj×ℓj
,
(9.1.1)
where the positive integers, εj’s, satisfy ε1 ≤· · · ≤εp and the quaternions βj’s are
such that the iβj’s all have zero real parts (the case when some or all the βj’s are
zero is not excluded). The subscript in [ · ]p×p indicates the size, p×p, of the matrix
[ · ].

SKEWHERMITIAN AND MIXED PENCILS
195
The form (9.1.1) is uniquely determined by A+tB up to arbitrary permutations
of diagonal blocks in each of the two parts,
⊕r
j=1


0
0
· · ·
0
0
iβj + ti
0
0
· · ·
0
iβj + ti
i
0
0
· · ·
iβj + ti
i
0
...
...
...
...
...
...
iβj + ti
i
0
· · ·
0
0


kj×kj
(9.1.2)
and
⊕q
j=1


0
0
· · ·
0
0
i
0
0
· · ·
0
i
ti
0
0
· · ·
i
ti
0
...
...
...
...
...
...
i
ti
0
· · ·
0
0


ℓj×ℓj
,
and up to replacement of βj in every block in (9.1.2) by a similar quaternion γj
subject to the property that R(iγj) = 0.
(ii) Two quaternion matrix pencils, A1 +tB1 and A2 +tB2, with skewhermitian
A1, B1, A2 and B2 are congruent if and only if they are strictly equivalent.
Thus, the form (9.1.1) is a canonical form for a pair of quaternion skewhermitian
matrices under simultaneous congruence as well as under strict equivalence.
Remark 9.1.2. Note that any nonzero quaternion with zero real part can be
used instead of i in this theorem. Indeed, if α ∈H \ {0} has zero real part, then
α = λ∗iλ for some nonzero quaternion λ (Theorem 2.2.6). Moreover,
λ∗(iβj)λ = (λ∗iλ) (λ−1βjλ),
and if the real part of iβj is zero, then the real part of α ·λ−1βjλ = λ∗(iβj)λ is zero
as well (by the same Theorem 2.2.6).
For the proof of Theorem 9.1.1, it will be convenient to verify ﬁrst the following
independently interesting fact.
Proposition 9.1.3. Let A, B ∈Hm×n. The following statements are equivalent
for the matrix pencil A + tB:
(a) A + tB is strictly equivalent to −A∗−tB∗;
(b) A + tB is strictly equivalent to a pencil A′ + tB′, such that the quaternion
matrices A′ and B′ are skewhermitian;
(c) m = n, and the left indices of A + tB, when arranged in nondecreasing order,
coincide with the right indices of A+tB, also arranged in nondecreasing order.
Proof.
(b) =⇒(a) is easy.
If A + tB = T(A′ + tB′)S for some invertible
matrices S and T, with skewhermitian A′ and B′, then
−A∗−tB∗= −S∗(A′∗+ tB′∗)T ∗= S∗(A′ + tB′)T ∗= S∗T −1(A + tB)S−1T ∗.
(a) =⇒(c). This follows from the uniqueness of the Kronecker form of A + tB
and the easily veriﬁed fact that the left (resp., right) indices of A+tB are the right
(resp., left) indices of −A∗−tB∗.

196
CHAPTER 9
(c) =⇒(b). In view of the Kronecker form of A + tB, we need to consider
only the following cases (leaving aside the trivial block 0u×v, where we must have
u = v):
(i) A + tB = Lε×(ε+1) ⊕LT
ε×(ε+1);
(ii) A + tB = Ik + tJk(0);
(iii) A + tB = tIℓ+ Jℓ(α), where α ∈H.
In Case (ii), let A′ + tB′ = iFk(Ik + tJk(0)). In Case (iii), replacing α by a similar
quaternion, we may assume without loss of generality that iα has zero real part;
then let A′ + tB′ = iFℓ(tIℓ+ Jℓ(α)). In Case (i), use the equality
 Iε+1
0
0
−Fε
 
t


0
0
Fε
0
01
0
−Fε
0
0

+


0
Fε
0
−Fε
01
0
0
0
0




 Fε
0
0
Iε+1

=

0
Lε×(ε+1)
LT
ε×(ε+1)
0

,
and we are done.
□
Proof of Theorem 9.1.1. Part (i) follows by applying the proof of the impli-
cation (c) =⇒(b) of Proposition 9.1.3 to the Kronecker form A0 + tB0 of the given
skewhermitian matrix pencil A + tB. The uniqueness statement in (i) follows from
the uniqueness properties of the Kronecker form A0+tB0. Note that the Kronecker
forms of the blocks
Mℓ:=


0
0
· · ·
0
0
i
0
0
· · ·
0
i
ti
0
0
· · ·
i
ti
0
...
...
...
...
...
...
i
ti
0
· · ·
0
0


ℓ×ℓ
and
Nk(β) :=


0
0
· · ·
0
0
iβ + ti
0
0
· · ·
0
iβ + ti
i
0
0
· · ·
iβ + ti
i
0
...
...
...
...
...
...
iβ + ti
i
0
· · ·
0
0


k×k
,
β ∈H,
are Iℓ+ tGℓand tIk + Jk(β), respectively.
For the proof of existence in Part (ii), we use the same approach as in the proof
of Theorem 8.1.2. The proof boils down to the veriﬁcation that each primitive block
in (9.1.1) is congruent to its negative. This is easy: indeed, for the block
K2ε+1 := t


0
0
Fε
0
0
0
−Fε
0
0

+


0
Fε
0
−Fε
0
0
0
0
0

,

SKEWHERMITIAN AND MIXED PENCILS
197
we have
 Iε
0
0
−Iε+1

K2ε+1
 Iε
0
0
−Iε+1

= −K2ε+1;
for the block Mℓuse (−jI)Mℓ(jI) = −Mℓ; ﬁnally, for the block Nk(β), where the
real part of iβ is zero, we have
(−γI)Nk(β)(γI) = −Nk(β).
Here γ is any quaternion that satisﬁes the equalities γ2 = −1, γiγ = i, γ(iβ)γ = iβ.
The existence of such γ is ensured by Ex. 2.7.14. Indeed, take γ to be a quaternion
with zero real part of norm 1, which is orthogonal to both i and iβ in the sense of
(2.7.1).
Finally, the uniqueness statement in Part (ii) follows from the uniqueness of
the Kronecker form and the Kronecker forms of the blocks Mℓand Nk(β) given
above.
□
9.2
COMPARISON WITH REAL AND COMPLEX
SKEWHERMITIAN PENCILS
For the real skewsymmetric pencils we have the following.
Theorem 9.2.1. Let A+tB, A′+tB′ be real n×n matrix pencils with skewsym-
metric A, A′, B, B′. Then the following statements are equivalent:
(1) A + tB and A′ + tB′ are R-congruent;
(2) A + tB and A′ + tB′ are H-congruent;
(3) A + tB and A′ + tB′ are R-strictly equivalent;
(4) A + tB and A′ + tB′ are H-strictly equivalent.
The result is obtained by combining Theorems 7.6.3, 9.1.1, and 15.2.2.
The situation is somewhat more complicated for complex skewhermitian pencils:
H-congruence generally does not imply C-congruence.
Theorem 9.2.2. The following statements are equivalent for complex skewher-
mitian n × n matrix pencils A + tB and A′ + tB′:
(a) A + tB and A′ + tB′ are C-strictly equivalent;
(b) A + tB and A′ + tB′ are H-strictly equivalent;
(c) A + tB and A′ + tB′ are H-congruent.
Proof. (b) ⇐⇒(c) by Theorem 9.1.1. The implication (a) =⇒(b) is obvious.
To prove (b) =⇒(a), observe that by Theorem 15.3.1 the indices (partial multiplic-
ities) of nonreal eigenvalues in the C-Kronecker form of A + tB come in pairs, one
for an eigenvalue β and the same one for the eigenvalue β (note that A + tB has
the same C-Kronecker form as the hermitian matrix pencil iA + tiB). Of course,
this also holds for the pencil A′ + tB′. Now, suppose (b) holds. By Theorem 7.6.3,
the C-Kronecker form of A + tB is obtained from the C-Kronecker form of A′ + tB′
by replacing some Jordan blocks Jm(α), where α is a complex nonreal eigenvalue,

198
CHAPTER 9
with Jm(α). But since the indices of nonreal eigenvalues come in pairs (see the
observation above), such replacements actually do not alter the C-Kronecker form
of A′ + tB′ (up to permutation of blocks allowed in the C-Kronecker form), and (a)
follows.
□
Using the canonical form for complex hermitian matrix pencils iA + tiB (The-
orem 15.3.1), we have a more precise statement concerning H-congruence versus
C-congruence for complex skewhermitian matrix pencils A + tB.
Theorem 9.2.3. Let A + tB be a skewhermitian n × n complex matrix pencil.
Let λ1, . . . , λu be all the distinct real eigenvalues of A + tB, and we include inﬁnity
in the set {λ1, . . . , λu} if blocks of type I + tJm(0) are present in the C-Kronecker
form of A+tB. Let pλj,s be the number of Jordan blocks Js(λj) in the C-Kronecker
form of A + tB, or the number of blocks I + tJs(0) if λj = ∞. Then the set of all
skewhermitian complex matrix pencils that are H-congruent to A + tB consists of
exactly p disjoint classes F1, . . . , Fp, each class consisting of mutually C-congruent
skewhermitian complex matrix pencils, so that matrix pencils from diﬀerent Fj’s
are not C-congruent. The integer p is given by
p =
u
Y
j=1
 rj
Y
s=1
(pλj,s + 1)
!
.
(9.2.1)
Here rj is the largest size of a Jordan block with eigenvalue λj, or the largest size
of blocks I + tJm(0) if λj = ∞, in the C-Kronecker form of A + tB.
The proof follows considerations analogous to those in the proof of Theorem
5.8.3. We leave the details to the reader.
Note also that the H-Kronecker form of A + tB can be used in Theorem 9.2.3
in place of the C-Kronecker form.
The classes F1, . . . , Fp can be identiﬁed in terms of the canonical form (15.3.1)
of the complex hermitian matrix pencil iA+tiB. For notational simplicity, suppose
λ1 = ∞. Namely, for each pair of integers (j, s), where s = 1, 2, . . . , rj and j =
1, 2, . . . , u such that pλj,s ̸= 0, select an integer qλj,s such that 0 ≤qλj,s ≤pλj,s.
We associate with these selections the form
K0
⊕
⊕{s : pλ1,s̸=0}


(Fs + tGs) ⊕· · · ⊕(Fs + tGs)
|
{z
}
qλ1,s times
⊕(−Fs −tGs) ⊕· · · ⊕(−Fs −tGs)
|
{z
}
pλ1,s−qλ1,s times



⊕⊕u
j=2 ⊕{s : pλj ,s̸=0}



((t −λj)Fs + Gs) ⊕· · · ⊕((t −λj)Fs + Gs)
|
{z
}
qλj ,s times
⊕−((t −λj)Fs + Gs) ⊕· · · ⊕−((t −λj)Fs + Gs)
|
{z
}
pλj ,s−qλj ,s times



,
(9.2.2)

SKEWHERMITIAN AND MIXED PENCILS
199
where K0 is the part of (15.3.1) that consists of blocks other than (t+αj)Fℓj +Gℓj
with real αj and Fki + tGki. Now, a class Fi consists exactly of those complex
skewhermitian pencils A′ + tB′ for which the canonical form of iA′ + tiB′ under
C-congruence is (9.2.2) for a ﬁxed selection of the qλj,s’s.
9.3
CANONICAL FORMS FOR MIXED PENCILS: STRICT
EQUIVALENCE
In this and subsequent sections we study mixed pairs of matrices A, B ∈Hn×n,
where A is hermitian and B is skewhermitian or, equivalently, quaternion ma-
trix pencils A + tB, where A = A∗and B = −B∗, which will be called hermi-
tian/skewhermitian . The case where A is skewhermitian and B is hermitian is
easily reduced to the hermitian/skewhermitian pencils by interchanging the roles
of A and B.
We start with description of primitive forms of hermitian/skewhermitian quater-
nion matrix pencils A + tB. In this description, we use the standard matrices
Fm =


0
· · ·
· · ·
0
1
...
1
0
...
...
0
1
...
1
0
· · ·
· · ·
0


,
Gm =
 Fm−1
0
0
0

,
eGm =
 0
0
0
Fm−1

,
and
Ξm(α) =


0
0
· · ·
0
α
0
0
· · ·
−α
0
...
...
...
...
...
0
(−1)m−2α
· · ·
0
0
(−1)m−1α
0
· · ·
0
0


∈Hm×m,
α ∈H.
(9.3.1)
The notation “q-h-sk” points to the class of pencils under consideration: quaternion
hermitian/skewhermitian.
(q-h-sk0)
a square-size zero matrix, i.e., 0m + t0m.
(q-h-sk1)
G2ε+1 + t


0
0
Fε
0
01
0
−Fε
0
0

.
(q-h-sk2)
Fk + tiGk.
(q-h-sk3)
Gℓ+ tiFℓ.
(q-h-sk4)

0
αFp + Gp
α∗Fp + Gp
0

+t

0
Fp
−Fp
0

, where α ∈H has positive
real part.
(q-h-sk5)
Let β be a nonzero quaternion with zero real part if m is even and a

200
CHAPTER 9
positive real number if m is odd. Then a primitive form is


0
0
· · ·
0
0
β
0
0
· · ·
0
−β
−1
0
0
· · ·
β
−1
0
...
...
...
...
...
...
(−1)m−1β
−1
0
· · ·
0
0


+ tΞm(im)
= Ξm(β) −eGm + tΞm(im).
(9.3.2)
The pencil in (q-h-sk5) is m × m, where m is a positive integer. Note that the
size m × m matrices
Ξm(β) −eGm
and
Ξm(im)
are hermitian and skewhermitian, respectively, for every m and every β subject to
the conditions in (q-h-sk5).
The next theorem is the main result on strict equivalence of mixed hermitian/
skewhermitian matrix pencils.
Theorem 9.3.1. Every hermitian/skewhermitian quaternion matrix pencil A +
tB is strictly equivalent to a direct sum of blocks of types (q-h-sk0)–(q-h-sk5). In
this direct sum, several blocks of the same type and of diﬀerent sizes may be present.
The direct sum is uniquely determined by A and B, up to an arbitrary permutation
of blocks, up to replacement of α with a similar quaternion in any block of type
(q-h-sk4), and up to replacement of β with a similar quaternion in any block of
type (q-h-sk5) of even size.
Note that Gm’s in Theorem 9.3.1 can be replaced by eGm’s, and any nonzero
quaternion with zero real part can be used in place of i. Indeed, given q ∈H \ {0}
with R(q) = 0, we have the equality
(diag (s∗
1, . . . , s∗
n)) (Gn + tiFn) (diag (s1, . . . , sn)) = Gn + tqFn,
where s1, . . . , sn ∈H \ {0} are determined as follows: if n is odd, let s(n+1)/2 be
such that s∗
(n+1)/2is(n+1)/2 = q, and if
s(n+1)/2, s(n+1)/2−1, . . . , s(n+1)/2−k, s(n+1)/2+1, . . . , s(n+1)/2+k
are already determined, where k ∈{0, 1, . . . , (n −1)/2 −1}, we let
s(n+1)/2−(k+1) = (s∗
(n+1)/2+k)−1,
s(n+1)/2+(k+1) = (s∗
(n+1)/2−(k+1)i)q.
In the case n is even, then we deﬁne sn/2 = 1, and if
sn/2, sn/2−1, . . . , sn/2−k, sn/2+1, . . . , sn/2+k
are already determined, where k ∈{0, 1, . . . , n/2 −1}, we let
sn/2+(k+1) = (s∗
n/2−ki)−1q,
sn/2−(k+1) = (s∗
n/2+(k+1))−1.
Analogously, for a given q ∈H \ {0} with R(q) = 0, a diagonal matrix T can be
found such that
T ∗(Fn + tiGn)T = Fn + tqGn.
(9.3.3)

SKEWHERMITIAN AND MIXED PENCILS
201
See Ex. 9.11.8.
Also, the matrix Ξm(β) −eGm, where β is subject to the conditions in Theorem
9.3.1, can be replaced by any one of Ξm(β) ± Gm or Ξm(β) + eGm.
The proof will be patterned on that of Theorem 9.1.1, Part (i). First we state
and prove the analogue of Proposition 9.1.3.
Proposition 9.3.2. Let A, B ∈Hm×n. The following statements (a), (b), and
(c) are equivalent for the matrix pencil A + tB:
(a) A + tB is strictly equivalent to A∗−tB∗;
(b) A + tB is strictly equivalent to a pencil A′ + tB′, such that the quaternion
matrices A′ and B′ are hermitian and skewhermitian, respectively;
(c) (c1) m = n;
(c2) the left indices of A+tB, when arranged in nondecreasing order, coincide
with the right indices, also arranged in nondecreasing order;
(c3) for every eigenvalue α ∈H of A + tB with nonzero real part, the quater-
nion −α is also an eigenvalue of A+tB, and the partial multiplicities of
the eigenvalue α, arranged in nondecreasing order, coincide with those
of the eigenvalue −α, also arranged in nondecreasing order.
Proof. The implication (b) =⇒(a) follows analogously to the implication (b)
=⇒(a) in the proof of Proposition 9.1.3.
(a) =⇒(c). Obviously, if (a) holds, then we must have m = n. Next observe
that if (a) holds for A + tB, then (a) holds for every matrix pencil which is strictly
equivalent to A + tB. Indeed, if
A0 + tB0 = S(A + tB)T,
A + tB = S0(A∗−tB∗)T0
for some matrices A0, B0 ∈Hn×n and some invertible matrices S, S0, T, T0, then
A∗
0 −tB∗
0
=
T ∗(A∗−tB∗)S∗
=
T ∗S−1
0 (A + tB)T −1
0
S∗= T ∗S−1
0 S−1(A0 + tB0)T −1T −1
0
S∗.
Therefore, without loss of generality, we may assume that A+tB is in the Kronecker
form (7.3.1). Now, the Kronecker form of
Lε×(ε+1)(−t)∗=

01×ε
Iε

−t

Iε
01×ε

coincides with Lε×(ε+1)(t)T because of equality
diag (1, −1, 1, . . . , ±1)Lε×(ε+1)(t)T diag (−1, 1, −1, . . . , ±1)
= Lε×(ε+1)(−t)T .
(9.3.4)
This proves (c2) (in view of the uniqueness of the Kronecker form). Also, if α ∈H
has nonzero real part, then the Kronecker form of −tIℓ+ (Jℓ(α))∗is tIℓ+ Jℓ(−α),
and the matrix pencils tIℓ+Jℓ(α) and tIℓ+Jℓ(−α) are not strictly equivalent (since
the quaternions α and −α ar not similar). This proves (c3), again by using the
uniqueness of the Kronecker form.

202
CHAPTER 9
(c) =⇒(b). Since both properties (c) and (b) are easily seen to be invariant
under strict equivalence, it suﬃces to prove this implication for the case when A+tB
is given in a Kronecker form. In turn, we need to consider only the following two
cases:
(1)
A + tB = Lε×(ε+1) ⊕LT
ε×(ε+1);
(2)
A + tB = (tIk + Jk(α)) ⊕(tIk + Jk(−α)),
R(α) ̸= 0.
(9.3.5)
In Case (1), use the equalities
 Lε×(ε+1)(t) ⊕Λε×(ε+1)(−t)T 
F2ε+1 = G2ε+1 + t


0
0
Fε
0
01
0
−Fε
0
0

,
where the ε × (ε + 1) matrix pencil
Λε×(ε+1)(−t) :=


1
−t
0
· · ·
0
0
1
−t
· · ·
0
...
...
...
...
0
0
· · ·
1
−t

∈Hε×(ε+1)
is strictly equivalent to Lε×(ε+1)(−t), which, in turn, is strictly equivalent to
Lε×(ε+1)(t) by virtue of (9.3.4). Indeed,
FεLε×(ε+1)(−t)Fε+1 = Λε×(ε+1)(−t).
(9.3.6)
In Case (2), note that the right-hand side of (9.3.5) is clearly strictly equivalent to

0
tI + J(α)
−tI −Jk(−α)
0

,
and furthermore −Jk(−α) is similar to (J(α))∗.
□
Proof of Theorem 9.3.1. Just repeat the arguments of the proof of (c) =⇒
(b) in Proposition 9.3.2.
□
9.4
CANONICAL FORMS FOR MIXED PENCILS: CONGRUENCE
We now state the main result on congruence of mixed hermitian/skewhermitian
quaternion matrix pencils.
Theorem 9.4.1. Let A + tB be a quaternion hermitian/skewhermitian n × n
matrix pencil. Then A + tB is congruent to a hermitian/skewhermitian pencil of
the form
(A0 + tB0) ⊕⊕r
j=1δj
 Fkj + tiGkj

⊕⊕p
i=1ηi (Gℓi + tiFℓi)
(9.4.1)
⊕
⊕q
u=1ζu









0
0
· · ·
0
0
βu
0
0
· · ·
0
−βu
−1
0
0
· · ·
βu
−1
0
...
...
...
...
...
...
(−1)mu−1βu
−1
0
· · ·
0
0


mu×mu
+ tΞmu(imu)) .
(9.4.2)

SKEWHERMITIAN AND MIXED PENCILS
203
Here, A0 + tB0 is a direct sum of blocks of types (q-h-sk0), (q-h-sk1), (q-h-sk2) for
even k, (q-h-sk3) for odd ℓ, and (q-h-sk4), in which several blocks of the same type
and of diﬀerent and/or the same sizes may be present. Furthermore, the parameters
of the blocks in (9.4.1), (9.4.2) are as follows:
(1) the kj’s are odd positive integers;
(2) the ℓi’s are even positive integers;
(3) the βu’s are positive real numbers if mu is odd, and βu’s are nonzero quater-
nions with zero real parts if mu is even;
(4) the δj, ηi, and ζu are signs ±1.
The subscript in [ · ]x×x designates the size x × x of the matrix [ · ].
The blocks in (9.4.1) and (9.4.2) are uniquely determined by A + tB up to an
arbitrary permutation of constituent matrix pencil blocks in A0 + tB0, up to an
arbitrary permutation of blocks in each of the three parts
⊕r
j=1δj
 Fkj + tiGkj

,
⊕p
i=1ηi (Gℓi + tiFℓi) ,
and (9.4.2), and up to the following replacements:
(a) α can be replaced with a similar quaternion in any block of type (q-h-sk4);
(b) β can be replaced with a similar quaternion in any block of type (9.4.2) of
even size mu × mu.
Conversely, any matrix pencil, which is congruent to a pencil of the form (9.4.1),
(9.4.2) is hermitian/skewhermitian.
Remark 9.4.2. The signs δj, ηi, and ζu form the sign characteristic of the
hermitian/skewhermitian matrix pencil A + tB. Note that the blocks Fk + tiGk,
Gk + tiFk, and
Ξmu(βu) −eGmu + tΞmu(imu),
(9.4.3)
where βu is as in Theorem 9.4.1, are strictly equivalent to I + tJk(0), tI + Jℓ(0),
and tI +Jmu(−imuβu) (if mu is odd), tI +Jmu(imuβu) (if mu is even), respectively.
Thus, the sign characteristic associates a sign ±1 to every odd partial multiplicity at
inﬁnity, to every even partial multiplicity at zero, and to every partial multiplicity
of nonzero eigenvalues with zero real parts, for hermitian/skewhermitian matrix
pencils.
Remark 9.4.3. The strict equivalence in Remark 9.4.2 implies that −βu can
be used in place of βu in some of the blocks (9.4.2) with odd mu. Indeed, (9.4.3)
is strictly equivalent to tI + Jmu(−imuβu), which is, in turn, strictly equivalent to
tI + Jmu(imuβu), as well as to (9.4.3) with βu replaced by −βu. Explicitly, for mu
odd and βu > 0 we have
(diag (−j, j, . . . , −j))(Ξmu(βu) −eGmu + tΞmu(imu))(diag (j, −j, . . . , j))
= −(Ξmu(−βu) −eGmu + tΞmu(imu)).
Thus, replacement of βu by −βu will reverse the sign of ζu.

204
CHAPTER 9
Remark 9.4.4. Noting the equalities
Fm(s1Fm + s2 eGm)Fm = s1Fm + s2Gm,
where s1, s2 are independent variables, we see that ±Gm and ± eGm can be inter-
changed in Theorem 9.4.1. Also note that for m even, we have
diag (1, −1, 1, . . . , −1)(Ξm(β) −eGm + tΞm(im))
× diag (1, −1, 1, . . . , −1) = −(Ξm(β) + eGm + tΞm(im)),
and for m odd,
diag (1, −1, 1, . . . , 1)(Ξm(β) −eGm + tΞm(im))diag (1, −1, 1, . . . , 1)
= Ξm(β) + eGm + tΞm(im).
Analogous equalities hold true for the blocks Fk + tiGk and Gℓ+ tiFℓ. Thus, ±Gm,
resp. ± eGm, can be replaced by ∓Gm, resp. ∓eGm, in Theorem 9.4.1. However,
upon such replacement, the sign in the sign characteristic of blocks of even size will
reverse to its negative, and the sign of blocks of odd size will remain the same (cf.
Remark 8.1.5).
Remark 9.4.5. The quaternion i can be replaced in (9.4.1) by any quaternion
λ with zero real part and norm 1. Indeed, by Theorem 2.3.3 the real dimension
of the solution set of ix −xλ = 0 is equal to 2, so there exists x ∈H such that
ix −xλ = 0 and x has zero real part and norm 1. Now, clearly,
(xI)∗(Fk + tiGk)(xI) = (Fk + tλGk);
(xI)∗(Gℓ+ tiFℓ)(xI) = (Gℓ+ tλFℓ).
The lengthy proof of Theorem 9.4.1 will be relegated to the next two sections.
In this section we present only the next proposition, which justiﬁes the statements
concerning replacements of α’s and β’s, as stipulated in the theorem.
Proposition 9.4.6. (1) If m is even and β1, β2 are similar nonzero quaternions
with zero real part, then the two m × m hermitian/skewhermitian matrix pencils
Υ(1)
j
:=


0
0
· · ·
0
0
βj + t
0
0
· · ·
0
−βj −t
±1
0
0
· · ·
βj + t
±1
0
...
...
...
...
...
...
−βj −t
±1
0
· · ·
0
0


m×m
,
j = 1, 2,
are congruent.
(2) If α1, α2 are similar quaternions with nonzero real parts, then the four 2p×2p
hermitian/skewhermitian matrix pencils
Υ(2)(j, ±) :=

0
±αjFp + Gp
±α∗
jFp + Gp
0

+ t

0
Fp
−Fp
0

,
j = 1, 2,
are congruent.

SKEWHERMITIAN AND MIXED PENCILS
205
Proof. For Part (a), notice that β1 = u∗β2u for some u ∈H with u∗u = 1 (see
Theorem 2.2.6). Then (u∗Im)Υ(1)
2 (uIm) = Υ(1)
1 . For Part (b), let q ∈H \ {0} be
such that qα1q−1 = α2. Then

qI
0
0
(q−1)∗I

Υ(2)(1, ±)

q∗I
0
0
q−1I

= Υ(2)(2, ±).
Also, let X ∈Rp×p be an invertible matrix such that
X(−GpFp)X−1 = GpFp.
(The existence of X is guaranteed because both −GpFp and GpFp are real p × p
matrices of rank p −1 such that (−GpFp)p = (GpFp)p = 0, and any two such
matrices are similar because they have the same real Jordan form.) Then, with
Y = −Fp(X−1)∗Fp, we have
 0
X
Y
0

Υ(2)(j, ±)

0
Y ∗
X∗
0

= Υ(2)(j, ∓),
j = 1, 2.
Statement (b) follows.
□
9.5
PROOF OF THEOREM 9.4.1: EXISTENCE
We start with a preliminary result.
Lemma 9.5.1.
(1) The block Fk+tiGk is congruent to its negative −Fk−tiGk
if k is even.
(2) The block Fk + tiGk is not congruent to its negative if k is odd.
(3) The block Gℓ+ tiFℓis congruent to its negative −Gℓ−tiFℓif ℓis odd.
(4) The block Gℓ+ tiFℓis not congruent to its negative if ℓis even.
(5) The block (q-h-sk5) is not congruent to its negative.
(6) Each of the blocks (q-h-sk1) and (q-h-sk4) is congruent to its negative.
Proof. Parts (1) and (3) follow from the equalities
diag (j, −j, . . . , j, −j) (Fk + tiGk) diag (−j, j, . . . , −j, j) = −(Fk + tiGk)
(9.5.1)
for even k and
diag (j, −j, . . . , −j, j) (Gℓ+ tiFℓ) diag (−j, j, . . . , j, −j) = −(Gℓ+ tiFℓ)
(9.5.2)
for odd ℓ.
Part (2) is obvious because for odd k, the signature of Fk (as a hermitian matrix)
is diﬀerent from the signature of −Fk. Since the signature of Gℓis diﬀerent from
that of −Gℓfor ℓeven, (4) follows as well.
For Part (6) observe the equalities
 Iε
0
0
−Iε+1
 
G2ε+1 +


0
0
tFε
0
01
0
−tFε
0
0




 Iε
0
0
−Iε+1


206
CHAPTER 9
= −

G2ε+1 +


0
0
tFε
0
01
0
−tFε
0
0




and
 Ip
0
0
−Ip
 
0
αFp + Gp + tFp
αFp + Gp −tFp
0
  Ip
0
0
−Ip

=

0
−αFp −Gp −tFp
−αFp −Gp + tFp
0

.
It remains to prove Part (5).
If m is odd, then the hermitian matrix A given
by (9.3.2) has signature diﬀerent from that of −A, and we are done in this case.
Suppose now m is even. It will be convenient to prove a lemma ﬁrst.
Lemma 9.5.2. Let β be a nonzero quaternion with zero real part. For m even,
deﬁne the m × m matrices Φm(β) and Ξm(1) by (1.2.7) and (1.2.6), respectively. If
S and T are m × m quaternion matrices such that the equalities
SΦm(β) = −Φm(β)T,
SΞm(1) = −Ξm(1)T
(9.5.3)
hold true, then the entries of S and T belong to the real subalgebra SpanR {1, β}.
Proof. We may assume without loss of generality, that β = xi, where x ∈R\{0}
(indeed, by Theorem 2.4.4 there exists an automorphism of H that maps any ﬁxed
nonzero quaternion with zero real part to a real multiple of i).
Letting
Xm :=


0
0
· · ·
0
0
0
0
0
· · ·
0
0
−1
0
0
· · ·
0
−1
0
...
...
...
...
...
...
0
−1
0
· · ·
0
0


,
and writing
S = S0 + iS1 + jS2 + kS3,
T = T0 + iT1 + jT2 + kT3,
where S0, S1, S2, S3, T0, T1, T2, T3 ∈Rm×m, the equalities (9.5.3) take the form
(S0 + iS1 + jS2 + kS3)(ixΞm(1) + Xm)
= (−ixΞm(1) −Xm)(T0 + iT1 + jT2 + kT3),
(S0 + iS1 + jS2 + kS3)Ξm(1) = (−Ξm(1))(T0 + iT1 + jT2 + kT3).
Equating the coeﬃcients of j in both sides of each of these equations, and similarly
for k, we obtain
S2Xm + xS3Ξm(1)
=
xΞm(1)T3 −XmT2,
−xS2Ξm(1) + S3Xm
=
−xΞm(1)T2 −XmT3,
(9.5.4)
S2Ξm(1) = −Ξm(1)T2,
S3Ξm(1) = −Ξm(1)T3.
(9.5.5)
Letting
e
X := XmΞm(1)−1,
eT2 = Ξm(1)T2Ξm(1)−1,
eT3 = Ξm(1)T3Ξm(1)−1,

SKEWHERMITIAN AND MIXED PENCILS
207
and using (9.5.5), the ﬁrst equality in (9.5.4) gives
2xS3 = −e
X eT2 −S2 e
X = e
XS2 −S2 e
X.
(9.5.6)
Analogously, the second equality in (9.5.4) gives
−2xS2 = −S3 e
X −e
X eT3 = −S3 e
X + e
XS3.
(9.5.7)
Solve (9.5.6) for S3 and substitute in (9.5.7):
−2xS2 = 1
2x

e
X2S2 −2 e
XS2 e
X + S2 e
X2
.
Iterating this equality and using the easily veriﬁable fact that e
X is nilpotent, i.e.,
e
Xp = 0 for some positive integer p, we obtain S2 = 0, and then S3 = 0. Then also
T2 = T3 = 0, and the lemma is proved.
□
We now return to the proof of Part (5), the case of even m. Without loss of
generality, we take β to be a nonzero real multiple of i: β = ri, where r ∈R \ {0}.
Suppose A + tB are given by (9.3.2), where m is even, and suppose, arguing by
contradiction, that A + tB is congruent (over the quaternions) to −A −tB. By
Lemma 9.5.2, the complex matrix pencils A + tB and −A −tB are C-congruent.
Then the complex hermitian matrix pencils A + t(iB) and −A + t(−iB) are also
C-congruent. However, this is impossible. To see this, let
eA := (iB)−1A = rIm + D,
where the matrix D has −i, i, −i, . . . , i, −i on the ﬁrst superdiagonal and zeros ev-
erywhere else. Clearly, the Jordan form of eA is Jm(r), and both A + t(iB) and
−A+t(−i)B are C-strictly equivalent to tI +Jm(r). By Theorem 15.3.1 in the Ap-
pendix, the canonical form of A + t(iB) under C-congruence is ±((t + r)Fm + Gm);
therefore, the canonical form of −A + t(−i)B must be ∓((t + r)Fm + Gm), and
these canonical forms are not C-congruent by the same Theorem 15.3.1.
This concludes the proof of Lemma 9.5.1.
□
We now are able to prove the existence part of Theorem 9.4.1.
First, con-
sider the scalar case n = 1. If a + tb, where a ∈R, R(b) = 0, is a scalar hermi-
tian/skewhermitian quaternion pencil, then we have the following three possibilities
(excluding the trivial case a = b = 0):
1. a = 0. Then in view of Theorem 2.2.6, the pencil is congruent to ti.
2. b = 0. Obviously, the pencil is congruent to ±1.
3. a, b ̸= 0. Then by the same Theorem 2.2.6, a + tb is congruent to a pencil of
the form ±(β + ti) for some real positive β.
In all possibilities, the conditions of the form (9.4.1), (9.4.2) are fulﬁlled.
In the general case, the statement that every hermitian/skewhermitian quater-
nion matrix pencil is congruent to a pencil of the form (9.4.1), (9.4.2), subject to
the conditions speciﬁed in Theorem 9.4.1, is proved by induction on the size of the
matrices. In the proof, Lemma 9.5.1 and Theorem 5.11.1 are used.

208
CHAPTER 9
Let A + tB be an n × n hermitian/skewhermitian matrix pencil.
l.
Using
Theorem 9.3.1, we ﬁnd invertible matrices P ∈Hn×n and Q ∈Hn×n such that
(A + tB) Q = P (A0 + tB0) ,
(9.5.8)
where A0 + tB0 is a direct sum of blocks of the forms (q-h-sk0)–(q-h-sk5). We will
show that A + tB and A0 + tB0 are φ-congruent for some choice of the signs δj, κj,
ηi, and ζu.
Without loss of generality, assume Q = I (otherwise, consider Qφ(A + tB)Q in
place of A + tB). Then we have
PA0
=
A = A∗= (A0)∗P ∗= A0P ∗,
PB0
=
B = −B∗
0 = −B∗
0P ∗= B0P ∗,
(9.5.9)
and, therefore,
F(P)A0 = A0(F(P)),
F(P)B0 = B0(F(P))∗
(9.5.10)
for every scalar polynomial F(t) with real coeﬃcients.
Several cases with respect to P may occur.
Case 9.5.3. P has only real eigenvalues, there is only one distinct real eigenvalue
of P, and this eigenvalue γ is positive.
From the Jordan form of P it easily follows that for the polynomial with real
coeﬃcients (t −γ−1)n, we have (P −1 −γ−1)n = 0. By Lemma 8.2.1 there exists
a polynomial fn(t) with real coeﬃcients such that t = (fn(t))2 (mod(t −γ−1)n).
Then P −1 = (fn(P −1))2, and since P −1 itself is a polynomial of P with real
coeﬃcients, we have P −1 = (f(P))2 for some polynomial f(t) with real coeﬃcients.
Let R = f(P), and use (9.5.10) to obtain
R(A + tB)R∗
=
RP(A0 + tB0)R∗= RP(A0 + tB0)(f(P))∗
=
f(P)Pf(P)(A0 + tB0) = A0 + tB0,
(9.5.11)
and the existence of the form (9.4.1), (9.4.2) under congruence follows.
Case 9.5.4. P has only real eigenvalues, there is only one distinct real eigenvalue
of P, and this eigenvalue is negative.
Arguing as in the preceding paragraph for −P rather than P, it is found that
(−P)−1 = (f(−P))2 for some polynomial f(t) with real coeﬃcients. As in (9.5.11),
it follows that
R(A + tB)R∗= −(A0 + tB0),
where R = f(−P). Thus, to complete the proof of existence of the form (9.4.1) and
(9.4.2), in Case 9.5.4, it remains to show that each of the blocks of types (q-h-sk1),
(q-h-sk4), Fp + tβGp with p odd, and Gp + tβFp with p even, is congruent to its
negative. This was shown in Lemma 9.5.1.
Case 9.5.5. All eigenvalues of P have equal real parts, and their vector parts
are nonzero and have equal euclidean lengths.

SKEWHERMITIAN AND MIXED PENCILS
209
Let β′ ∈H be an eigenvalue (necessarily nonreal) of P. Denote α = (β′)−1. In
view of the Jordan form of P, for some positive integer w (which may be taken the
largest size of Jordan blocks in the Jordan form of P), we have g(P −1) = 0, where
g(t) = ((t −β′)(t −(β′)∗))w is a polynomial with real coeﬃcients. Using Lemma
8.2.1 again, we ﬁnd a polynomial gw(t) with real coeﬃcients such that t = (gw(t))2
modulo a real polynomial multiple of g(t)). Then P −1 = (gw(P −1))2. Since P is
itself a matrix root of a polynomial with real coeﬃcients, namely, bg(P) = 0, where
bg(t) = ((t −α)(t −(α∗)))w,
it follows that P −1 is also a polynomial of P with real coeﬃcients. Now argue as
in Case 9.5.3.
Case 9.5.6. All other possibilities (not covered in Cases 9.5.3, 9.5.4, and 9.5.5).
In this case, using the Jordan form, P can be written in the form
P = S diag (P1, P2, . . . , Pr) S−1
(9.5.12)
where S is an invertible quaternion matrix and P1, . . . , Pr are matrices of sizes
n1 × n1, . . . , nr × nr, respectively, such that any pair of eigenvalues λ of Pj and µ
of Pi (i ̸= j) satisﬁes the following condition:
either R(λ) ̸= R(µ) or |V(λ)| ̸= |V(µ)|, or both.
(9.5.13)
We also have r ≥2 (the situations when r = 1 are covered in Cases 9.5.3–9.5.5).
Substituting (9.5.12) in the equality A + tB = P(A0 + tB0), we obtain
 P −1
1
⊕· · · ⊕P −1
r

( eA + t eB) = eA0 + t eB0,
(9.5.14)
where
eA
=
S−1A(S∗)−1,
eB = S−1B(S∗)−1,
eA0
=
S−1A0(S∗)−1,
eB0 = S−1B0(S∗)−1.
Partition the matrix eA :
eA = [Mij]r
i,j=1 ,
where Mij is of the size ni × nj. Since eA and eA0 are hermitian, (9.5.14) implies
the equality
P −1
i
Mij = Mij ((Pj)∗)−1 ;
cf. (9.5.9). An elementary calculation shows that if (9.5.13) holds for two nonzero
quaternions λ and µ, then the same property holds for the inverses λ−1 and µ−1.
Note also that in view of Corollary 5.7.2, ν ∈σ(((Pj)∗)−1) if and only if ν ∈σ(P −1
j
).
Thus, as a corollary of these remarks, we have
σ(P −1
i
) ∩σ(((Pj)∗)−1) = ∅
for
i ̸= j.
Now by Theorem 5.11.1 we have Mij = 0 (i ̸= j). In other words,
eA = M11 ⊕· · · ⊕Mrr.
Similarly, eB = N11 ⊕· · · ⊕Nrr, where Nii is of size ni × ni.
Now, induction is used on the size of the matrix pencil A + tB to complete the
proof of existence part in Theorem 9.4.1.
□

210
CHAPTER 9
9.6
PROOF OF THEOREM 9.4.1: UNIQUENESS
In this section we will make extensive use of the canonical form for real symmet-
ric/skewsymmetric matrix pencils (Theorem 15.2.3 in the Appendix).
Let A+tB be a hermitian/skewhermitian quaternion matrix pencil, and assume
that it is congruent to two forms (9.4.1), (9.4.2). We have to prove that the two
forms are obtained from each other by the permutations and replacements indicated
in Theorem 9.4.1. These two forms are congruent; hence, a fortiori strictly equiv-
alent. Therefore, by Theorem 9.3.1 we may assume that the two forms may diﬀer
only in the signs δj, ηk, and ζu. Thus, suppose that the hermitian/skewhermitian
matrix pencil A′ + tB′ given by (9.4.1), (9.4.2) is congruent to the pencil
A′′ + tB′′
:=
(A0 + tB0) ⊕⊕r
j=1δ′
j
 Fkj + tiGkj

⊕
⊕p
i=1η′
i (Gℓi + tiFℓi)
(9.6.1)
⊕
⊕q
u=1ζ′
u


0
0
· · ·
0
0
· · ·
0
0
· · ·
...
...
...
(−1)mu−1(βu + timu)
−1
0
0
0
βu + timu
0
−βu −timu
−1
βu + timu
−1
0
...
...
...
· · ·
0
0


(9.6.2)
for some signs δ′
j, η′
i, and ζ′
u. We need to show that A′′ + tB′′ is obtained from
A′ + tB′ by a permutation of constituent blocks.
To this end we will apply the map χ to the pencils A′ + tB′ and A′′ + tB′′. By
properties (iii) and (iv) of χn,n, we have
χn,n (A′ + tB′) = χn,n (A′) + tχn,n (B′)
is a real symmetric/skewsymmetric 4n × 4n matrix pencil. We describe ﬁrst the
canonical forms of real symmetric/skewsymmetric pencils obtained from the prim-
itive blocks (q-h-sk2), (q-h-sk3), and (q-h-sk5) by the map χ.
We will use the following real symmetric/skewsymmetric matrix pencil in the

SKEWHERMITIAN AND MIXED PENCILS
211
proof of the next lemma. Denote Ξ2 := Ξ2(1) =

0
1
−1
0

; then
Q2m(ν)
:=


0
0
· · ·
0
0
νΞm+1
2
0
0
· · ·
0
−νΞm+1
2
−I2
0
0
· · ·
νΞm+1
2
−I2
0
...
...
...
...
...
...
(−1)m−1νΞm+1
2
−I2
0
· · ·
0
0


+ t


0
0
· · ·
0
Ξm
2
0
0
· · ·
−Ξm
2
0
...
...
...
...
...
0
(−1)m−2Ξm
2
· · ·
0
0
(−1)m−1Ξm
2
0
· · ·
0
0


,
ν > 0.
(9.6.3)
The pencil in (9.6.3) is 2m × 2m, where m is a positive integer.
Lemma 9.6.1. (1) The real symmetric/skewsymmetric matrix pencil χk,k (Fk +
tiGk), where k is odd, is R-congruent to

Fk + t


01
0
0
0
0
F k−1
2
0
−F k−1
2
0




⊕4
.
(2) The real symmetric/skewsymmetric matrix pencil χk,k (Fk + tiGk), where k
is even, is R-congruent to



F2k + t


01
0
0
0
0
0
0
Fk−1
0
0
01
0
0
−Fk−1
0
0






⊕2
.
(9.6.4)
(3) The real symmetric/skewsymmetric matrix pencil χℓ,ℓ(Gℓ+ tiFℓ), where ℓ
is even, is R-congruent to
 
Gℓ+ t
"
0
F ℓ
2
−F ℓ
2
0
#!⊕4
.
(9.6.5)
(4) The real symmetric/skewsymmetric matrix pencil χℓ,ℓ(Gℓ+ tiFℓ), where ℓ
is odd, is R-congruent to

0
Gℓ+ tFℓ
Gℓ−tFℓ
0
⊕2
.
(9.6.6)
(5) The real symmetric/skewsymmetric matrix pencil
χm,m









0
0
· · ·
0
0
β
0
0
· · ·
0
−β
−1
0
0
· · ·
β
−1
0
...
...
...
...
...
...
(−1)m−1β
−1
0
· · ·
0
0


+ tΞm(im)







,
(9.6.7)

212
CHAPTER 9
where m is odd and β is positive real number, is R-congruent to
(−1)(m+1)/2 (Q2m(β) ⊕Q2m(β)) ,
where Q2m(β) is deﬁned by (9.6.3).
(6) The real symmetric/skewsymmetric matrix pencil
χm,m









0
0
· · ·
0
0
β
0
0
· · ·
0
−β
−1
0
0
· · ·
β
−1
0
...
...
...
...
...
...
(−1)m−1β
−1
0
· · ·
0
0


+ tΞm(im)







,
(9.6.8)
where m is even and β is a nonzero quaternion with zero real part, is R-congruent
to (Q2m(|β|))⊕2, where Q2m(|β|) is deﬁned by (9.6.3).
Proof. Part (1). It is easy to see that χk,k (Fk + tiGk) is R-congruent, using a
suitable permutation matrix for the congruence transformation, to
(Fk + t diag (1, −1, 1, −1, . . . , 1)Gk)⊕2
⊕(Fk + t diag (−1, 1, −1, 1, . . . , −1)Gk)⊕2.
In turn,


0
0
· · ·
0
0
ϵ1
0
0
· · ·
0
ϵ2
0
0
0
· · ·
ϵ3
0
0
...
...
...
...
...
...
ϵk
0
· · ·
0
0


(Fk + t diag (1, −1, 1, −1, . . . , 1)Gk)
×


0
0
· · ·
0
0
ϵk
0
0
· · ·
0
ϵk−1
0
0
0
· · ·
ϵk−2
0
0
...
...
...
...
...
...
ϵ1
0
· · ·
0
0


= Fk + t


01
0
0
0
0
F k−1
2
0
−F k−1
2
0

,
where ϵj = ±1 are found from the following system of equations:
ϵ1ϵk
=
ϵ2ϵk−1 = · · · = ϵ(k−1)/2ϵ(k+1)/2 = 1,
(9.6.9)
ϵ2ϵk
=
−1,
ϵ3ϵk−1 = 1, . . . ,
ϵ(k+1)/2ϵ(k+3)/2 = (−1)(k+3)/2. (9.6.10)
It is easy to see that the system (9.6.9), (9.6.10) has a unique solution once the
value of ϵ1 = ±1 is arbitrarily assigned. Analogously, one proves that
Fk + t diag (−1, 1, −1, 1, . . . , −1)Gk
is R-congruent to Fk + t


01
0
0
0
0
F(k−1)/2
0
−F(k−1)/2
0

.
Part (2). By Theorem 15.2.3, it suﬃces to prove that χk,k (Fk + tiGk) is R-
strictly equivalent to (9.6.4) (indeed, it follows from that theorem that a symmetric/

SKEWHERMITIAN AND MIXED PENCILS
213
skewsymmetric real matrix pencil is R-strictly equivalent to (9.6.4) if and only if it
is R-congruent to (9.6.4)). This is easily seen by using R-strict equivalence
χk,k (Fk + tiGk)
7→
Q1χk,k (Fk + tiGk)Q2
with suitable permutation matrices Q1 and Q2.
Part (3). Applying a congruence with a suitable permutation matrix, it is easy
to see that χℓ,ℓ(Gℓ+ tiFℓ) is R-congruent to the matrix


0
0
· · ·
0
I2
teΞ2
0
0
· · ·
I2
teΞ2
0
0
0
· · ·
teΞ2
0
0
...
...
...
...
...
...
I2
teΞ2
0
· · ·
0
0
teΞ2
0
0
· · ·
0
0


2ℓ×2ℓ
⊕


0
0
· · ·
0
I2
teΞ2
0
0
· · ·
I2
teΞ2
0
0
0
· · ·
teΞ2
0
0
...
...
...
...
...
...
I2
teΞ2
0
· · ·
0
0
teΞ2
0
0
· · ·
0
0


2ℓ×2ℓ
,
(9.6.11)
where
eΞ2 := Ξ2(−1) =

0
−1
1
0

.
(9.6.12)
The matrix (9.6.11) is of size 4ℓ× 4ℓand all zeros there are the zero 2× 2 matrices.
Denote by X either of the two equal 2ℓ×2ℓblocks in (9.6.11). Let P be the 2ℓ×2ℓ
permutation matrix whose rows, starting with the top row and going down, are
e1, e4, e5, e8, e9, . . . , e2ℓ−4, e2ℓ−3, e2ℓ, e2, e3, . . . , e2ℓ−2, e2ℓ−1,
where we have denoted by ej the unit row vector with 1 in the jth position and
zeros in all other positions. For typographical convenience, let
Ψℓ=


0
0
· · ·
0
eΞ2
0
0
· · ·
eΞ2
0
...
...
...
...
...
0
eΞ2
· · ·
0
0
eΞ2
0
· · ·
0
0


ℓ×ℓ
.
A computation shows that
PXP ∗= (Gℓ+ tΨℓ) ⊕(Gℓ−tΨℓ) .
Next, observe that
diag (1, −1, 1, . . . −1) · (Gℓ+ tΨℓ) · diag (1, −1, 1, . . . −1) = Gℓ−tΨℓ,

214
CHAPTER 9
and, in turn,
diag (ϵ1, . . . , ϵℓ) · (Gℓ+ tΨℓ) · diag (ϵ1, . . . , ϵℓ) = Gℓ+ t

0
Fℓ/2
−Fℓ/2
0

,
where ϵ1, . . . , ϵℓare signs ±1 that are found from the following equations:
ϵ1ϵℓ
=
−1,
ϵ2ϵℓ−1 = 1,
ϵ3ϵℓ−2
=
−1, . . . , ϵℓ/2ϵ(ℓ/2)+1 = (−1)ℓ/2,
(9.6.13)
ϵ1ϵℓ−1
=
ϵ2ϵℓ−2 = · · · = ϵ(ℓ/2)−1ϵ(ℓ/2)+1 = 1.
(9.6.14)
It is easy to see that the system (9.6.13), (9.6.14) has unique solution once the value
of ϵ1 is arbitrarily assigned.
Part (4). It suﬃces to prove that χℓ,ℓ(Gℓ+ tiFℓ), where ℓis odd, is R-strictly
equivalent to (9.6.6) (indeed, by Theorem 15.2.3, a real symmetric/skewsymmetric
matrix pencil is R-strictly equivalent to (9.6.6) if and only if it is R-congruent to
(9.6.6)). Using the same transformation as in the proof of Part (3), we need to
show that


0
0
· · ·
0
I2
teΞ2
0
0
· · ·
I2
teΞ2
0
0
0
· · ·
teΞ2
0
0
...
...
...
...
...
...
I2
teΞ2
0
· · ·
0
0
teΞ2
0
0
· · ·
0
0


2ℓ×2ℓ
,
with
eΞ =
 0
−1
1
0

,
(9.6.15)
is R-strictly equivalent to

0
Gℓ+ tFℓ
Gℓ−tFℓ
0

.
(9.6.16)
In fact, it is easy to see that the Kronecker form over the reals of both (9.6.15) and
(9.6.16) is (tI + Jℓ(0)) ⊕(tI + Jℓ(0)).
Part (5). Let Ξ2 :=

0
1
−1
0

. It will be convenient to denote also Υ :=
βΞm+1
2
+ t(−Ξm
2 ).
A congruence transformation with a permutation congruence matrix shows that
(9.6.7) is R-congruent to


0
0
· · ·
0
0
Υ
0
0
· · ·
0
−Υ
−I2
0
0
· · ·
Υ
−I2
0
...
...
...
...
...
...
Υ
−I2
0
· · ·
0
0


⊕2
(9.6.17)
if m −1 is not divisible by 4 and to


0
0
· · ·
0
0
−Υ
0
0
· · ·
0
Υ
−I2
0
0
· · ·
−Υ
−I2
0
...
...
...
...
...
...
−Υ
−I2
0
· · ·
0
0


⊕2
(9.6.18)

SKEWHERMITIAN AND MIXED PENCILS
215
if m −1 is divisible by 4 (we assume here that m is odd). We see that the con-
gruence transformation with the congruence matrix F ⊕2m
2
applied to the left-hand
side of (9.6.17) yields the form (9.6.3) with ν = β, whereas the the congruence
transformation with the congruence matrix
(diag (F2, −F2, F2, −F2, · · · , F2))⊕2
applied to the left-hand side of (9.6.18) yields the negative of (9.6.3) with ν = β.
Part (6). Assume m is even. For an automorphism ψ of H, consider the matrix
pencil
W4m(ψ) := χm,m









0
0
· · ·
0
0
ψ(β)
0
0
· · ·
0
−ψ(β)
−1
0
0
· · ·
ψ(β)
−1
0
...
...
...
...
...
...
−ψ(β)
−1
0
· · ·
0
0


+ tΞm(im)







,
where β is a ﬁxed nonzero quaternion with zero real part. Since ψ(β) also has zero
real part, clearly W4m(ψ) is a real symmetric/skewsymmetric pencil. By Proposi-
tion 3.3.2, W4m(ψ), for various ψ’s, are all real unitarily similar to each other and,
in particular, have the same canonical form under real congruence. By Theorem
2.2.6 we see that there exists an automorphism ψ of H such that ψ(β) = qi for
some q > 0. Therefore, we need only to verify (6) for β = qi, where q > 0; in fact,
q = |β|. A congruence with a suitable permutation matrix shows that
χm,m









0
0
· · ·
0
0
qi
0
0
· · ·
0
−qi
−1
0
0
· · ·
qi
−1
0
...
...
...
...
...
...
−qi
−1
0
· · ·
0
0


+ tΞm(im)







is R-congruent to (Q2m(q))⊕2 if m is not divisible by 4 and is R-congruent to
(Q2m(−q))⊕2 if m is divisible by 4. Here, Q2m(±q) is deﬁned by (9.6.3). Finally,
the equality
F ⊕m
2
(Q2m(−q))F ⊕m
2
= Q2m(q)
yields the desired R-congruence.
□
Now we can we easily complete the proof of the uniqueness part of Theorem
9.4.1. Indeed, if A′′ + tB′′ were not obtained from A′ + tB′ by a permutation of
constituent blocks, then by applying the map χ and using Lemma 9.6.1 we obtain
a contradiction with the uniqueness (up to a permutation of constituent primitive
blocks) of the canonical form of the real symmetric/skewsymmetric matrix pencil
χn,n (A′ + tB′) (Theorem 15.2.3).
□
9.7
COMPARISON WITH REAL AND COMPLEX PENCILS:
STRICT EQUIVALENCE
The canonical form for mixed hermitian/skewhermitian quaternion matrix pencils
allows us to sort out the relations between R-congruence and H-congruence of mixed

216
CHAPTER 9
pencils of real matrices, and between C-congruence and H-congruence of mixed
pencils of complex matrices.
For real matrices we have the following.
Theorem 9.7.1. Let A, B, A′, B′ ∈Rm×m be such that
AT = A, (A′)T = A′,
BT = −B, (B′)T = −B′.
Then, the real matrix pencils A+tB and A′ +tB′ are H-congruent, resp. H-strictly
equivalent, if and only if A + tB and A′ + tB′ are R-congruent, resp. R-strictly
equivalent.
Proof.
The statement on strict equivalence is a particular case of Theorem
7.6.3. For congruence, let an invertible matrix S ∈Hm×m be such that equality
S∗(A + tB)S = A′ + tB′
(9.7.1)
holds, and write S = S0 + iS1 + jS2 + kS3, where S0, S1, S2, S3 are real matrices.
Then (9.7.1) takes the form (8.5.2), and the proof is completed as in the proof of
Theorem 8.5.1.
□
The complex case is more involved. In this section we consider the strict equiva-
lence relation. It will be convenient to work with hermitian matrix pencils A+tiB,
where A = A∗, B = −B∗∈Cn×n.
Theorem 9.7.2. Let A, A′ ∈Cn×n be complex hermitian matrices and let
B, B′ ∈Cn×n be complex skewhermitian matrices. Assume that the complex matrix
pencils A + tB and A′ + tB′ are H-strictly equivalent and that the hermitian matrix
pencil A + t(iB) is C-strictly equivalent to the form (15.3.1), where
α1 = α2 = · · · = αq′ = 0,
αw ∈R \ {0} for w = q′ + 1, q′ + 2, . . . , q
for some q′ (the case q′ = 0 is not excluded), and where all the signs δj’s and ηj’s
are taken to be equal to 1.
Then the hermitian pencil A′ + t(iB′) is C-strictly equivalent to a complex her-
mitian matrix pencil of the following form:
0u
⊕

t


0
0
Fε1
0
0
0
Fε1
0
0

+ G2ε1+1


⊕
· · · ⊕

t


0
0
Fεp
0
0
0
Fεp
0
0

+ G2εp+1


⊕
(Fk1 + tGk1) ⊕· · · ⊕(Fkr + tGkr)
⊕
(tFℓ1 + Gℓ1) ⊕· · · ⊕

tFℓq′ + Gℓq′

⊕
 t + κ′
q′+1αq′+1

Fℓq′+1 + Gℓq′+1

⊕· · · ⊕
  t + κ′
qαq

Fℓq + Gℓq

⊕

0
(t + β′
1)Fm1
(t + β′
1)Fm1
0

+

0
Gm1
Gm1
0

⊕
· · · ⊕

0
(t + β′
s)Fms
(t + β′s)Fms
0

+

0
Gms
Gms
0

,
(9.7.2)

SKEWHERMITIAN AND MIXED PENCILS
217
where
for each j = 1, . . . , s, either β′
j = βj or β′
j = −βj,
(9.7.3)
and κ′
q′+1, . . . , κ′
q ∈{1, −1}.
(9.7.4)
Conversely, suppose that A′ + tB′ and A′′ + tB′′ are pencils of the form (9.7.2),
with the parameters
{κ′
q′+1, . . . , κ′
q; β′
1, . . . , β′
s}
subject to (9.7.3) and (9.7.4) for A′ + tB′, and with the corresponding parameters
{κ′′
q′+1, . . . , κ′′
q; β′′
1 , . . . , β′′
s },
again subject to conditions analogous to (9.7.3) and (9.7.4) for A′′ + tB′′. Then the
complex hermitian-skewhermitian matrix pencils A′+t(−i)B′ and A′′+t(−i)B′′ are
H-strictly equivalent.
We illustrate this theorem with an example.
Example 9.7.3. Let A, B, A′, B′ be as in Theorem 9.7.2. Let
A + tB = (t(−i) + α)Fℓ+ Gℓ,
α ∈R.
Multiplying this equality on the right by iFℓ, we see that A+tB has the C-Kronecker
form tIℓ+ Jℓ(iα). Since A′ + tB′ has the same H-Kronecker form as A + tB, by
Theorem 7.6.3 the C-Kronecker form of A′ + tB′ is tIℓ+ Jℓ(iα) or tIℓ+ Jℓ(−iα).
Therefore, A′ + tiB′ is C-strictly equivalent to tiIℓ+ Jℓ(iα) or to tiIℓ+ Jℓ(−iα).
Multiplication on the right by (−i)Fℓyields C-strict equivalence of A′ + tiB′ to
(t + α)Fℓ−iGℓor to (t −α)Fℓ−iGℓ. Finally, note the equalities
diag ((−i)j)ℓ−1
j=0 ((t ± α)Fℓ−iGℓ) diag ((i)ℓ−1−j)ℓ−1
j=0 = (t ± α)Fℓ+ Gℓ,
which produce the desired statement concerning C-strict equivalence of A′ + tiB′
according to Theorem 9.7.2.
□
We prove Theorem 9.7.2 in Section 9.9. Here, we indicate a corollary of this the-
orem describing the situations when strict equivalence of two hermitian/skewhermi-
tian pencils is the same over H and over H. Recall that −α is the eigenvalue of a
matrix pencil tI + Jm(α).
Corollary 9.7.4. Let A, B ∈Cn×n, where A = A∗, B = −B∗. Then the follow-
ing two statements are equivalent:
(1) Every hermitian-skewhermitian complex matrix pencil A′ + tB′, A′ = A′∗,
B′ = −B′∗, which is H-strictly equivalent to A+tB is also C-strictly equivalent
to A + tB;
(2) A + tB has no nonzero pure imaginary eigenvalues.
The cases when zero is an eigenvalue of A + tB and/or there are indices at
inﬁnity are not excluded in Corollary 9.7.4(b).
Proof. Observe that the complex matrix pencils A + tB and A + tiB have the
same indices at inﬁnity and the same partial multiplicities at the eigenvalue zero,
and −α ∈C is an eigenvalue of A + tB if and only if iα is an eigenvalue of A + tiB;
moreover the partial multiplicities of A + tB at its eigenvalue −α coincide with the

218
CHAPTER 9
partial multiplicities of A+tiB at its eigenvalue iα. In view of this observation, the
result of Corollary 9.7.4 is immediate from Theorem 9.7.2.
□
If the conditions of Corollary 9.7.4 do not hold, then we have more than one
class of C-strict equivalence within the class of H-strict equivalence. We compute
the precise number of such classes, as follows.
Let A0 +tB0 be a complex matrix pencil of hermitian matrices, and let (15.3.1)
(with δi and ηj taken to be equal 1) be the canonical form of A0+tB0 under C-strict
equivalence. For every pair of nonzero real numbers (α, −α), where α is taken to
be positive, and every positive integer ℓ, let s(α, ℓ) be the total number of blocks
(t + αj)Fℓj + Gℓj in (15.3.1) with αj = α or αj = −α, and ℓj = ℓ. (We formally
deﬁne s(α, ℓ) = 0 if neither α nor −α is an eigenvalue of A0 + tB0 or if at least one
of α, −α is an eigenvalue of A0 + tB0 but ℓis not a partial multiplicity of A0 + tB0
at ±α.) Deﬁne
s(A0 + tB0) :=
Y
α>0
Y∞
ℓ=1 (s(α, ℓ) + 1).
Theorem 9.7.5. Let A + tB be as in Theorem 9.7.2, and let
Ω(A + tB)
:=
{A′ + tB′ : A′, B′ ∈C, (A′)∗= A′, (B′)∗= −B′,
A′ + tB′ is H −strictly equivalent to A + tB}
be the set of complex hermitian/skewhermitian pencils that are H-strictly equivalent
to A + tB. Then Ω(A + tB) consists of exactly s(A + tiB) disjoint classes so that
the pencils in each class are all C-strictly equivalent and the pencils from diﬀerent
classes are not C-strictly equivalent to each other.
Proof.
By Theorem 9.7.2 we need to show that s(A + tiB) coincides with
the number of mutually C-nonstrictly equivalent hermitian matrix pencils given by
formula (9.7.2), where κ′
q′+1, . . . , κ′
q are arbitrary signs ±1.
Denote by eA+t eB the complex matrix pencil (9.7.2), where all signs κ′
j are taken
to be equal to 1 and β′
j = βj for j = 1, 2, . . . , s.
Fix a pair of nonzero real numbers α, −α, at least one of which is an eigenvalue
of eA + t eB and where α > 0, and let
((t + κ′
1α)Fℓ+ Gℓ)⊕
· · ·
⊕((t + κ′
vα)Fℓ+ Gℓ)
⊕((t + κ′′
1(−α))Fℓ+ Gℓ)⊕
· · ·
⊕((t + κ′′
w(−α))Fℓ+ Gℓ)
(9.7.5)
be the part of (9.7.2) with blocks of size ℓand eigenvalues ±α; for simplicity of
notation, we have changed the subscripts of some of the κ′
j’s in (9.7.2) as well as
renamed some of them as κ′′
j .
Suppose that exactly x of the signs κ′
1, . . . , κ′
v are −1 and exactly y of the signs
κ′′
1, . . . , κ′′
w are −1, where 0 ≤x ≤v; 0 ≤y ≤w. Then the C-Kronecker form (as
well as the H-Kronecker form) of the pencil (9.7.5) is
(U + tV )x,y
:=
(t + α)Fℓ+ Gℓ) ⊕· · · ⊕(t + α)Fℓ+ Gℓ)
|
{z
}
v−x+y times
⊕
(t −α)Fℓ+ Gℓ) ⊕· · · ⊕(t −α)Fℓ+ Gℓ)
|
{z
}
w−y+x times
.
We see that if (x′, y′) is another selection of integers subject to 0 ≤x′ ≤v, 0 ≤y′ ≤
w, then (U +tV )x′,y′ is C-strictly equivalent to (U +tV )x,y precisely when x′ −y′ =

SKEWHERMITIAN AND MIXED PENCILS
219
x−y. Note that x−y can take values in the set {−w, −w+1, . . . , 0, 1, . . . , v}, a total
of v + w + 1 = s(α, ℓ) + 1 values. Thus, we obtain exactly s(α, ℓ) + 1 mutually C-
nonstrictly equivalent matrix pencils from (9.7.5) by all possible selections of signs
κ′
i and κ′′
j . Putting this information together for all pairs (α, −α), where α > 0,
and all integers ℓ, the result of Theorem 9.7.5 follows.
□
9.8
COMPARISON WITH COMPLEX PENCILS: CONGRUENCE
Here we consider comparison of H-congruence and C-congruence of mixed complex
matrix pencils. The sign characteristic plays a key role here. As in Section 9.7,
it will be convenient to work with hermitian matrix pencils A + tiB, where A =
A∗, B = −B∗∈Cn×n.
Theorem 9.8.1. Let A, A′ ∈Cn×n be complex hermitian matrices and let
B, B′ ∈Cn×n be complex skewhermitian matrices. Assume that the complex matrix
pencils A + tB and A′ + tB′ are H-congruent. Let (15.3.1) be the canonical form of
the hermitian pencil A + t(iB) under C-congruence, where
α1 = α2 = · · · = αq′ = 0,
αw ∈R \ {0} for w = q′ + 1, q′ + 2, . . . , q
for some q′ (the case q′ = 0 is not excluded).
Then the canonical form of the hermitian pencil A′ +t(iB′) under C-congruence
has the following structure:
0
⊕

t


0
0
Fε1
0
0
0
Fε1
0
0

+ G2ε1+1

⊕· · · ⊕

t


0
0
Fεp
0
0
0
Fεp
0
0

+ G2εp+1


⊕
δ′
1 (Fk1 + tGk1) ⊕· · · ⊕δ′
r (Fkr + tGkr)
⊕
η′
1 (tFℓ1 + Gℓ1) ⊕· · · ⊕η′
q′

tFℓq′ + Gℓq′

⊕
η′
q′+1
 t + κ′
q′+1αq′+1

Fℓq′+1 + Gℓq′+1

⊕
· · · ⊕η′
q
  t + κ′
qαq

Fℓq + Gℓq

⊕

0
(t + β′
1)Fm1
(t + β′
1)Fm1
0

+

0
Gm1
Gm1
0

⊕
· · · ⊕

0
(t + β′
s)Fms
(t + β′s)Fms
0

+

0
Gms
Gms
0

,
(9.8.1)
where for each j = 1, . . . , s, either β′
j = βj or β′
j = −βj, and
δ′
1, . . . , δ′
r;
η′
1, . . . , η′
q;
κ′
q′+1, . . . , κ′
q
are signs ±1 subject to the following restrictions (1), (2), (3):
(1) δ′
j = δj if kj is odd (j = 1, 2, . . . , r).
(2) η′
w = ηw if ℓw is even (w = 1, 2, . . . , q′).
(3) For every nonzero real eigenvalue α of A + t(iB) and for every index ℓof α,
the following holds: if q′ + 1 ≤w1 < · · · < wk ≤q are all the distinct integers

220
CHAPTER 9
between q′ + 1 and q such that αwj = α and ℓwj = ℓ, j = 1, 2, . . . , k, then
there exists a permutation λ of {1, 2, . . . , k} such that the integers
ℓ+ 1
2((1 + κ′
wλ(j))ℓ+ η′
wλ(j) −ηwj)
(9.8.2)
are even for j = 1, 2, . . . , k.
Conversely, suppose that A′ + tB′ and A′′ + tB′′ are pencils of the form (9.8.1),
with the parameters
{δ′
1, . . . , δ′
r; η′
1, . . . , η′
q; κ′
q′+1, . . . , κ′
q; β′
1, . . . , β′
s}
for A′+tB′ (subject to the restrictions (1), (2), and (3)), and with the corresponding
parameters
{δ′′
1, . . . , δ′′
r ; η′′
1, . . . , η′′
q ; κ′′
q′+1, . . . , κ′′
q; β′′
1 , . . . , β′′
s },
δ′′
j , η′′
k, κ′′
ℓ∈{1, −1},
for A′′ + tB′′ subject to the conditions:
(a) δ′′
j = δj if kj is odd (j = 1, 2, . . . , r);
(b) η′′
w = ηw if ℓw is even (w = 1, 2, . . . , q′);
(c) for every nonzero real eigenvalue α of A + t(iB) and for every index ℓof
α, the following holds: if w1, . . . , wk are the distinct integers between q′ + 1
and q such that αwj = α and ℓwj = ℓ, j = 1, 2, . . . , k, then there exists a
permutation λ of {1, 2, . . . , k} such that the integers
ℓ+ 1
2((1 + κ′′
wλ(j))ℓ+ η′′
wλ(j) −ηwj)
are even for j = 1, 2, . . . , k;
(d) for each j = 1, . . . , s, either β′′
j = βj or β′′
j = −βj.
Then the complex hermitian-skewhermitian matrix pencils A′ + t(−i)B′ and A′′ +
t(−i)B′′ are H-congruent.
Thus, in the structure (9.8.1) there are no restrictions on the signs δ′
j for even
kj and on the signs η′
w for odd ℓw (w = 1, 2, . . . , q).
Note also that condition
(3) determines uniquely the signs η′
w, w = q′ + 1, q′ + 2, . . . , q, once the κ′
w’s are
assigned. More speciﬁcally, the integer (9.8.2) is even if and only if ℓis even and
η′
wλ(j) = ηwj or ℓis odd and η′
wλ(j) = κ′
wλ(j)ηwj.
We give a proof of Theorem 9.8.1 in Section 9.9. Here, we indicate a corollary
(the counterpart of Corollary 9.7.4) of Theorem 9.8.1 describing the situations when
congruence of two hermitian/skewhermitian pencils is the same over H and over C.
Recall that −α is the eigenvalue of a matrix pencil tI + Jm(α).
Corollary 9.8.2. Let A, B ∈Cn×n, where A = A∗, B = −B∗. Then the follow-
ing two statements are equivalent:
(1) Every hermitian-skewhermitian complex matrix pencil A′ + tB′, A′ = A′∗,
B′ = −B′∗which is H-congruent to A + tB is also C-congruent to A + tB.

SKEWHERMITIAN AND MIXED PENCILS
221
(2) A + tB has no nonzero pure imaginary eigenvalues, the partial multiplicities
at the zero eigenvalue are all even, and the indices at inﬁnity are all odd.1
Proof. In view of the observations made in the proof of Corollary 9.7.4, the
result of Corollary 9.8.2 is immediate from Theorem 9.8.1.
□
We formulate the counterpart of Theorem 9.7.5 for C-congruence as an open
problem.
Open Problem 9.8.3. Let A+tB be a hermitian/skewhermitian complex matrix
pencil, and let
Ωc(A + tB)
:=
{A′ + tB′ : A′, B′ ∈C, (A′)∗= A′, (B′)∗= −B′,
A′ + tB′ is H−congruent to A + tB}
be the set of complex hermitian/skewhermitian pencils that are H-congruent to A +
tB. Describe the disjoint classes in Ωc(A + tB) so that the pencils in each class
are all C-congruent to each other and the pencils from diﬀerent classes are not C-
congruent to each other. In particular, ﬁnd a formula for the number of such classes
in terms of the canonical form under C-congruence of the complex hermitian matrix
pencil A + tiB.
9.9
PROOFS OF THEOREMS 9.7.2 AND 9.8.1
It will be convenient to collect separately several facts that will be used in the proof.
Lemma 9.9.1. In the statements (C)–(G) below, η, η′, κ′ are signs ±1 and α is
a nonzero real number. We have:
(A) Fk + t(−i)Gk is not H-congruent to −Fk + tiGk for odd k;
(bA) Fk + t(−i)Gk is H-congruent to −Fk + tiGk for even k;
(B) t(−i)Fℓ+ Gℓis not H-congruent to tiFℓ−Gℓfor even ℓ;
(bB) t(−i)Fℓ+ Gℓis H-congruent to tiFℓ−Gℓfor odd ℓ;
(C) for η′ ̸= η and κ′ = 1, the pencil
η′ ((−it + κ′α) Fℓ+ Gℓ)
(9.9.1)
is not H-congruent to
η ((−it + α) Fℓ+ Gℓ) ;
(9.9.2)
(D) for κ′ = −1, ℓeven, and η′ = η, the pencil (9.9.1) is H-congruent to (9.9.2);
(E) for κ′ = −1, ℓeven, and η′ = −η, the pencil (9.9.1) is not H-congruent to
(9.9.2);
(F) for κ′ = −1, ℓodd and η′ = −η, the pencil (9.9.1) is H-congruent to (9.9.2);
1We point out that Parts (2) in Corollaries 6.2 and 6.4 in Rodman [134] are misstated. To
obtain correct statements, Part (2) in Corollary 6.2, resp. Corollary 6.4, should be replaced by
statement (2) of Corollary 9.7.4, resp. Corollary 9.8.2.

222
CHAPTER 9
(G) for κ′ = −1, ℓodd and η′ = η, (9.9.1) is not H-congruent to (9.9.2);
(H) if β ∈C \ R, then the matrix pencil

0
(−it + β)Fm + Gm
(−it + β)Fm + Gm
0

is H-congruent to

0
(−it −β)Fm + Gm
(−it −β)Fm + Gm
0

.
Proof. Many statements in Lemma 9.9.1 can be proved using Theorem 9.4.1
and Remark 9.4.2. We give a direct proof for most of them.
Statements (A) and (B) follow from the inertia theorem for hermitian quaternion
matrices, because the signature of Fk for odd k is diﬀerent from the signature of
−Fk, and the signature of Gk for even k is diﬀerent from that of −Gk. Statements
(bA) and (bB) follow from the equalities
diag (j, −j, . . . , j, −j) (Fk + tiGk) diag (−j, j, . . . , −j, j) = −(Fk + tiGk)
for even k and
diag (j, −j, . . . , −j, j) (Gℓ+ tiFℓ) diag (−j, j, . . . , j, −j) = −(Gℓ+ tiFℓ)
for odd ℓ.
Statement (C) is a consequence of Theorem 9.4.1; see also Remark
9.4.2. Indeed, it follows that any matrix pencil of mixed hermitian-skewhermitian
quaternion matrices whose H-Kronecker form consists of exactly one Jordan block
and the eigenvalue of this block is nonzero with zero real part is not H-congruent to
its negative. For (D) note the equality (taking η′ = η = 1 without loss of generality)
diag (−j, j, . . . , −j, j) ((−it −α)Fℓ+ Gℓ) diag (j, −j, . . . , j, −j)
= ((−it + α)Fℓ+ Gℓ) ,
where α ∈R \ {0} and ℓis even.
Consider (E). By the already proved statement (D), the matrix pencils
η′ ((−it −α) Fℓ+ Gℓ)
and
η′ ((−it + α) Fℓ+ Gℓ)
are H-congruent; but
η′ ((−it + α) Fℓ+ Gℓ) = −η ((−it + α) Fℓ+ Gℓ) ,
which is not H-congruent to (9.9.2) by statement (C). Statement (F) is proved by
the equality
(diag (j, −j, j, . . . , −j, j)) ((it + α)Fℓ−Gℓ) (diag (−j, j, −j, . . . , j, −j))
= (it + α)Fℓ+ Gℓ,
where ℓis odd. For the proof of (G), observe that the real symmetric matrices
η′κ′αFℓ+ η′Gℓand ηαFℓ+ ηGℓ(the values of the pencils (9.9.1) and (9.9.2) when

SKEWHERMITIAN AND MIXED PENCILS
223
t = 0) have diﬀerent inertia; therefore, these two matrices cannot be H-congruent.
For statement (H), let α and γ be nonzero quaternions satisfying the properties
α(−i)α−1 = i,
α(−1)mγ∗= 1,
and let
X := (diag (α, −α, . . . , (−1)m−1α)) ⊕(diag (γ, −γ, . . . , (−1)m−1γ)) ∈H2m×2m.
Then a straightforward computation shows that
X

0
(−it + β)Fm + Gm
(−it + β)Fm + Gm
0

X∗=

0
(−it −β)Fm + Gm
(−it −β)Fm + Gm
0

.
This completes the proof of Lemma 9.9.1.
□
Statements (C)–(G) of Lemma 9.9.1 can be conveniently summarized as follows.
Corollary 9.9.2. For signs η, η′, κ′ ∈{1, −1}, positive integer ℓ, and α ∈R\{0},
the matrix pencils (9.9.1) and (9.9.2) are H-congruent if and only if the integer
ℓ+ 1
2((1 + κ′)ℓ+ η′ −η)
(9.9.3)
is even, i.e., ℓis even and η′ = η, or ℓis odd and η′ = κ′η.
Proof of Theorem 9.8.1. The direct statement: The pencils A + tB and
A′ +tB′ clearly have the same H-Kronecker form. Therefore, denoting by A′
0 +tB′
0
the C-Kronecker form of A′+tB′ and denoting by A0+tB0 the C-Kronecker form of
A+tB, we obtain, in view of Theorem 7.6.3, that A′
0+tB′
0 follows from A0+tB0 by
replacing some blocks Jℓj(αj) with Jℓj(αj), for nonreal αj. Note that A′
0 + t(iB′
0)
is C-strictly equivalent to A′ + t(iB′), whereas A0 + t(iB0) is C-strictly equivalent
to A + t(iB). Note also that
x = y + iz, y, z ∈R, is an eigenvalue of A0 + t(iB0)
⇐⇒
iy −z is an eigenvalue of A0 + tB0
⇐⇒
iy −z and/or −iy −z is an eigenvalue of A′
0 + tB′
0
⇐⇒
−y + iz and/or y + iz is an eigenvalue of A′
0 + t(iB′
0).
Taking into account Theorem 15.3.1, we see that the canonical form of the complex

224
CHAPTER 9
hermitian pencil A′ + t(iB′) under C-congruence must have the following structure:
0u
⊕

t


0
0
Fε1
0
0
0
Fε1
0
0

+ G2ε1+1


⊕
· · · ⊕

t


0
0
Fεp
0
0
0
Fεp
0
0

+ G2εp+1


⊕
δ′
1 (Fk1 + tGk1) ⊕· · · ⊕δ′
r (Fkr + tGkr)
⊕
η′
1 (tFℓ1 + Gℓ1) ⊕· · · ⊕η′
q′

tFℓq′ + Gℓq′

⊕
η′
q′+1
 t + η′′
q′+1αq′+1

Fℓq′+1 + Gℓq′+1

⊕· · · ⊕η′
q
  t + η′′
q αq

Fℓq + Gℓq

⊕

0
(t + β′
1)Fm1
(t + β′
1)Fm1
0

+

0
Gm1
Gm1
0

⊕
· · · ⊕

0
(t + β′
s)Fms
(t + β′s)Fms
0

+

0
Gms
Gms
0

,
(9.9.4)
where for each j = 1, . . . , s, either β′
j = βj or β′
j = −βj, and
δ′
1, . . . , δ′
r,
η′
1, . . . , η′
q, η′′
q′+1, . . . , η′′
q
are signs ±1. Denote by A1 + tB1 the hermitian pencil (15.3.1), and by A′
1 + tB′
1
the hermitian pencil (9.9.4). Then the pencils A1 + t(−i)B1 and A′
1 + t(−i)B′
1 are
H-congruent. In view of the canonical form for hermitian-skewhermitian quaternion
pencils under H-congruence (Theorem 9.4.1), the direct statement of Theorem 9.8.1
follows from Lemma 9.9.1(A), (B), (H), and from Corollary 9.9.2.
The converse statement follows by analogous arguments using (bA) and (bB) of
Lemma 9.9.1 and Corollary 9.9.2 again.
□
Proof of Theorem 9.7.2. The direct statement: Arguing as in the proof of the
direct statement of Theorem 9.8.1, we see that the canonical form of the complex
hermitian pencil A′ + t(iB′) under C-congruence must have the same structure as
in (9.9.4). Evidently, (9.9.4) is C-strictly equivalent to (9.7.2).
The converse statement: The H-strict equivalence of A′ + t(−i)B′ and A′′ +
t(−i)B′′ follows by observing that every pair of corresponding constituent blocks in
A′ + t(−i)B′ and A′′ + t(−i)B′′ has the same H-Kronecker form.
□
9.10
CANONICAL FORMS FOR MATRICES UNDER
CONGRUENCE
Theorem 9.4.1, together with the obvious observation that every matrix A ∈Hn×n
can be uniquely written in the form A = F + G, where F = F ∗and G = −G∗,
leads to the following canonical form of square-size quaternion matrices under the
congruence transformation A 7→P ∗AP with invertible matrix P ∈Hn×n. Again,
it will be convenient to list the primitive forms ﬁrst; the preﬁx q-c stands for
quaternion congruence.
(q-c0)
0m×m.

SKEWHERMITIAN AND MIXED PENCILS
225
(q-c1)
G2ε+1 +


0
0
Fε
0
01
0
−Fε
0
0

.
(q-c2)
Fk + iGk, where k is an even integer.
(q-c3)
Gℓ+ iFℓ, where ℓis an odd integer.
(q-c4)

0
(α + 1)Fp + Gp
(α∗−1)Fp + Gp
0

, where α ∈H
has positive real part.
Theorem 9.10.1. Let X ∈Hn×n. Then X is congruent to a matrix of the form
A0
⊕
⊕r
j=1δj
 Fkj + iGkj

⊕⊕p
i=1ηi (Gℓi + iFℓi)
(9.10.1)
⊕
⊕q
u=1ζu


0
0
· · ·
0
0
· · ·
0
0
· · ·
...
...
...
(−1)mu−1(βu + imu)
−1
0
0
0
βu + imu
0
−βu −imu
−1
βu + imu
−1
0
...
...
...
· · ·
0
0


mu×mu
. (9.10.2)
Here, A0 is a direct sum of blocks of types (q-c0), (q-c1), (q-c2), (q-c3), and (q-
c4), in which several blocks of the same type and of diﬀerent and/or the same
sizes may be present. Furthermore, the kj’s are odd positive integers, the ℓi’s are
even positive integers, the βu’s are positive real numbers if mu is odd, the βu’s are
nonzero quaternions with zero real parts if mu is even, and the δj’s and ηi’s are
signs 1 or −1.
The blocks in (9.10.1) and (9.10.2) are uniquely determined by X up to a per-
mutation of constituent blocks in A0, up to a permutation of blocks in each of the
three parts
⊕r
j=1δj
 Fkj + iGkj

,
⊕p
i=1ηi (Gℓi + iFℓi) ,
and (9.10.2), up to replacements of α with a similar quaternion in any block of type
(q-c4), and up to replacements of β with a similar quaternion in any block of type
(9.10.2) of even size mu × mu.
The uniqueness statement in Theorem 9.10.1 follows from the uniqueness state-
ment in Theorem 9.8.1 upon the trivial observation that the correspondence be-
tween hermitian/skewhermitian pairs of matrices (F, G), F, G ∈Hn×n, and matrices
A ∈Hn×n given by A = F + G, is one-to-one and onto.
It is instructive to compare H-congruence of complex or real matrices with C-
congruence or R-congruence. The following corollary is immediate from Theorem
8.5.1.

226
CHAPTER 9
Corollary 9.10.2. Two real n × n matrices A and B are R-congruent if and
only if they are H-congruent; in other words, if A = R∗BR for some invertible
R ∈Hn×n, then also A = S∗BS for some invertible S ∈Rn×n.
An analogous result for complex matrices does not hold true. As a direct con-
sequence of Corollary 9.8.2, we have the following.
Corollary 9.10.3. A complex matrix A ∈Cn×n has the property that any com-
plex matrix A′ which is H-congruent to A, is also C-congruent to A, if and only if
the complex hermitian pencil
1
2 (A + A∗) + ti
1
2 (A −A∗)

has no nonzero real eigenvalues, its indices at zero are all even, and its indices at
inﬁnity are all odd.
9.11
EXERCISES
Ex. 9.11.1. Provide details for the proof of Theorem 9.2.3.
Ex. 9.11.2. Show that a matrix A ∈Hn×n is nilpotent if and only if σ(A) = {0}.
Ex.
9.11.3. Describe the canonical form (9.1.1) of the skewhermitian pencil
A + tB under each of the following conditions:
(a) rank (xA + yB) is constant for all x, y ∈R not both zero;
(b) rank A = 1;
(c) rank A = 2.
Ex. 9.11.4. Describe the canonical form for strict equivalence of the hermitian/
skewhermitian pencil A + tB (Theorem 9.3.1), under each of the conditions of Ex.
9.11.3.
Ex. 9.11.5. Describe the canonical form under congruence of the hermitian/
skewhermitian pencil A + tB (Theorem 9.4.1), assuming one of the following hy-
potheses:
(a) A is positive deﬁnite;
(b) A is positive semideﬁnite;
(c) A has only one negative eigenvalue (counted with multiplicities).
Ex. 9.11.6. Find all possible canonical forms under congruence of quaternion
matrices whose ranks do not exceed 2.
Ex.
9.11.7. Consider the following property of a hermitian/skewhermitian
quaternion matrix pencil A + tB:
(A)
If A′ + tB′ is a hermitian/skewhermitian matrix pencil that is strictly
equivalent to A + tB, then in fact A′ + tB′ is congruent to A + tB.
(a) Identify matrix pencils with property (A) in terms of their canonical forms
(9.4.1), (9.4.2).
(b) For every hermitian/skewhermitian matrix pencil A+tB that does not have
property (A), ﬁnd the number of congruence classes within the set of hermitian/
skewhermitian matrix pencils which are strictly equivalent to A + tB.

SKEWHERMITIAN AND MIXED PENCILS
227
Ex. 9.11.8. Provide an explicit expression for the matrix T in (9.3.3).
9.12
NOTES
As indicated in the notes to the preceding chapter (see also references there), the
canonical forms for skewhermitian and mixed (hermitian/skewhermitian) quater-
nion matrix pencils are known in the literature. Exposition in Sections 9.1 and 9.2,
as well as that of Theorem 9.4.1 and its proof, follows Rodman [135].
Results of Sections 9.9 and 9.7 were proved in Rodman [134].
Canonical forms for complex matrices under congruence can be found in Horn
and Sergeichuk [65, 66], Sergeichuk [141], and Lancaster and Rodman [93]. The
canonical form of Theorem 9.10.1 is given in Rodman [135].
Congruences of real, complex, and quaternion matrices have been compared in
Rodman [134].

Chapter Ten
Indeﬁnite inner products: Conjugation
In this chapter we study indeﬁnite inner products deﬁned on Hn×1 of the hermitian-
and skewhermitian-types and matrices having symmetry properties with respect
to one of these indeﬁnite inner products. The hermitian-type inner product is a
function
[·, ·] : Hn×1 × Hn×1 −→H
with the following properties:
(1) Linearity in the ﬁrst argument: [x1α1 + x2α2, y] = [x1, y]α1 + [x2, y]α2 for all
x1, x2, y ∈Hn×1 and all α1, α2 ∈H.
(2) Symmetry: [x, y] = [y, x]∗for all x, y ∈Hn×1.
(3) Nondegeneracy: if x0 ∈Hn×1 is such that [x0, y] = 0 for all y ∈Hn×1, then
x0 = 0.
The skewhermitian-type inner product [·, ·] is deﬁned by properties (1), (3), and
(2′) antisymmetry: [x, y] = −[y, x]∗for all x, y ∈Hn×1.
It follows from (1) and (2), or from (1) and (2′), that
[x, y1α1 + y2α2] = α∗
1[x, y1] + α∗
2[x, y2]
for all x, y1, y2 ∈Hn×1,
α1, α2 ∈H.
Proposition 10.0.1. [·, ·] is an inner product of hermitian-type, resp. skewher-
mitian-type, if and only if there exists a hermitian, resp. skewhermitian, invertible
n × n matrix H such that
[x, y] = ⟨Hx, y⟩= ±⟨x, Hy⟩= y∗Hx,
for all x, y ∈Hn×1,
with the sign +, resp. −. Such a matrix H is uniquely determined by the inner
product.
Proof. The “if” part is straightforward, as is the uniqueness of H. If [·, ·] is an
inner product of hermitian or skewhermitian-type, then for every ﬁxed y ∈Hn×1,
the map x 7→[x, y] is (quaternion) linear; hence, there exists z ∈Hn×1 such that
[x, y] = ⟨x, z⟩for all x ∈Hn×1. One veriﬁes that the map y
7→
z is linear, so
z = Gy for some linear transformation G on Hn×1, which we represent as a matrix
(with respect to the standard basis e1, . . . , en). Now let H = G∗. It is easy to see
that H is invertible and hermitian, resp. skewhermitian, if the inner product is of
hermitian-type, resp. skewhermitian-type.
□
The matrix H of Proposition 10.0.1 is said to be associated with the indeﬁnite
iner product [·, ·]. In the sequel, we will often work with associated matrices rather
than directly with indeﬁnite inner products.

INDEFINITE INNER PRODUCTS: CONJUGATION
229
We develop canonical forms for (H,∗)-hermitian and (H,∗)-skewhermitian ma-
trices, with respect to hermitian-type indeﬁnite inner products, as well as for (H,∗)-
Hamiltonian and (H,∗)-skew-Hamiltonian matrices, with respect to skewhermitian-
type inner products. These forms are based on the canonical forms given in Chap-
ters 8 and 9. Applications are given to invariant semideﬁnite and neutral subspaces
and to boundedness properties of systems of linear diﬀerential equations with cer-
tain symmetries.
10.1
H-HERMITIAN AND H-SKEWHERMITIAN MATRICES
Let [·, ·] be an inner product on Hn×1 of hermitian type. A matrix A ∈Hn×n is
said to be selfadjoint , resp. skewadjoint, with respect to [·, ·] if [Ax, y] = [x, Ay],
resp. [Ax, y] = −[x, Ay], for all x, y ∈Hn×1. If H = H∗is the matrix associated
with [·, ·], then A is selfadjoint, resp. skewadjoint, with respect to [·, ·] if and only
if HA = A∗H, resp.
HA = −A∗H, i.e., A is (H,∗)-hermitian, resp.
(H,∗)-
skewhermitian.
In this chapter, we abbreviate “(H,∗)-hermitian” and “(H,∗)-skewhermitian”
as “H-hermitian” and “H-skewhermitian,” respectively.
Note that A is H-hermitian, resp. H-skewhermitian, if and only if S−1AS is
S∗HS-hermitian, resp. H-skewhermitian, for any invertible S ∈Hn×n.
The canonical forms of H-hermitian and H-skewhermitian matrices are given
as follows. We denote by C+,0 the set of complex numbers with positive imaginary
parts.
Theorem 10.1.1. Let H ∈Hn×n be hermitian and invertible, and let A be H-
hermitian. Denote by dp(A, λ) the number of Jordan blocks in the Jordan form of
A of size p corresponding to the set of mutually similar eigenvalues that contains
the eigenvalue λ of A. Then for every p ≥0 and for every nonreal eigenvalue λ of
A, the number dp(A, λ) is even, and there exists an invertible quaternion matrix S
such that
S∗HS
=
⊕λ∈σ(A)∩C+,0 ⊕p≥1 ⊕
1
2 dp(A,λ)
k=1
⊕F2p
⊕⊕λ∈σ(A)∩R ⊕p≥1 ⊕dp(A,λ)
k=1
η(p)
k,λFp,
(10.1.1)
S−1AS
=
⊕λ∈σ(A)∩C+,0 ⊕p≥1 ⊕
1
2 dp(A,λ)
k=1
" Jp(λ)
0p
0p
Jp(λ)
#
⊕⊕λ∈σ(A)∩R ⊕p≥1 ⊕dp(A,λ)
k=1
Jp(λ),
(10.1.2)
where η(p)
k,λ are signs ±1.
The form (10.1.1), (10.1.2) is uniquely determined by the pair (H, A), up to an
arbitrary permutation of diagonal blocks in each of the two parts
⊕λ∈σ(A)∩C+,0 ⊕p≥1 ⊕
1
2 dp(A,λ)
k=1
⊕F2p
and
⊕λ∈σ(A)∩R ⊕p≥1 ⊕dp(A,λ)
k=1
η(p)
k,λFp
in S∗HS and, simultaneously, in the same permutation of the blocks in S−1AS.

230
CHAPTER 10
Conversely, if H and A are given by formulas (10.1.1), (10.1.2) for some invert-
ible S, then H is hermitian and invertible and A is H-hermitian.
The proof is obtained by applying Theorem 8.1.2 to the pair of hermitian ma-
trices (HA, H). The converse statement is easily veriﬁed.
We note that the canonical form (10.1.2) of the pair (A, H) involves only real and
complex matrices. Using Remark 8.1.8, an alternative form can be given entirely in
terms of real matrices. Another alternative form can be given using Remark 8.1.6.
The signs η(p)
k,λ constitute the sign characteristic of the H-hermitian matrix A.
Thus, the sign characteristic assigns a sign ±1 to every partial multiplicity of every
real eigenvalue of A.
Remark 10.1.2. It is well known, and found in many sources in literature (see,
e.g., Gohberg et al. [52, 53]), that the result of Theorem 10.1.1 is applicable in
the context of complex matrices as well. It can be obtained by applying Theorem
15.3.1 to the pair of complex hermitian matrices (HA, H).
We now give a proof of Proposition 8.1.7 based on the form (10.1.2).
Proof of Proposition 8.1.7.
Note that the matrix A := H−1
1 H2 is H1-
hermitian.
A straightforward computation shows that A is a lower triangular
Toeplitz matrix with p(2)
k /p(1)
k
on the main diagonal and x on the ﬁrst subdiag-
onal. Thus, the Jordan form of A is Jk(p(2)
k /p(1)
k ). Now use the canonical form
(10.1.2) of the H1-hermitian matrix A. The canonical form yields (8.1.11). The
sign η can be determined using the following rule, which may be applied in the case
the H-hermitian k × k matrix A ∈Hk×k has only one Jordan block (necessarily
corresponding to a real eigenvalue) in its Jordan form. Namely, if y1, y2, . . . , yk
is a chain consisting of an eigenvector y1 and associated generalized eigenvectors
y2, . . . , yk of A, then the number y∗
kHy1 = y∗
1Hyk is real and nonzero and its sign
coincides with the sign η in the sign characteristic of the pair (A, H). (This rule is a
particular case of what is known as the second description of the sign characteristic
of the H-hermitian matrix A, for complex matrices; see Gohberg et al. [53, Sections
5.8, 6.3] or Gohberg et al. [52, Section 3.9].) Applying the rule results in formula
(8.1.12).
□
In the canonical form for H-skewhermitian matrices, we will use the standard
matrix
Ξn(in−1) :=


0
0
0
· · ·
0
0
in−1
0
0
0
· · ·
0
−in−1
0
0
0
0
· · ·
in−1
0
0
...
...
...
...
...
...
0
(−1)n−2in−1
0
· · ·
0
0
0
(−1)n−1in−1
0
0
· · ·
0
0
0


.
Note that Ξn(in−1) is hermitian for every positive integer n.
Theorem 10.1.3. Let H ∈Hn×n be hermitian and invertible, and let A ∈Hn×n
be H-skewhermitian: HA = −A∗H. Then there is an invertible quaternion matrix
W such that W −1AW and W ∗HW are block diagonal matrices
W −1AW = A1 ⊕· · · ⊕As,
W ∗HW = H1 ⊕· · · ⊕Hs,
(10.1.3)
where each diagonal block (Aα, Hα) is of one of the following four types:

INDEFINITE INNER PRODUCTS: CONJUGATION
231
(i)
Aα
=
J2n1+1(0) ⊕J2n2+1(0) ⊕· · · ⊕J2np+1(0),
Hα
=
κ1Ξ2n1+1(i2n1) ⊕κ2Ξ2n2+1(i2n2) ⊕· · · ⊕κpΞ2np+1(i2np),
where nj’s are nonnegative integers and κj’s are signs ±1;
(ii)
Aα
=
J2m1(0) ⊕· · · ⊕J2mq(0),
Hα
=
Ξ2m1(i2m1−1) ⊕· · · ⊕Ξ2mq(i2mq−1),
where mj’s are positive integers;
(iii)
Aα
=
Jk1(a + ib) ⊕−(Jk1(a + ib))∗
⊕· · · ⊕Jku(a + ib) ⊕−(Jku(a + ib))∗,
(10.1.4)
Hα
=

0
Ik1
Ik1
0

⊕· · · ⊕

0
Iku
Iku
0

,
(10.1.5)
where a and b are real numbers such that a > 0 and b ≥0, and the numbers
a and b, the total number 2u of Jordan blocks, and the sizes k1, . . . , ku may
depend on (Aα, Hα);
(iv)
Aα
=
Jh1(ib) ⊕Jh2(ib) ⊕· · · ⊕Jht(ib),
Hα
=
η1Ξh1(ih1−1) ⊕· · · ⊕ηtΞht(iht−1),
where b is a positive real number and η1, . . . , ηt are signs ±1. Again, the pa-
rameters b, t, h1, . . . , ht and η1, . . . , ηt may depend on the particular diagonal
block (Aα, Hα).
The form (10.1.3) is uniquely determined by the pair (A, H), up to an arbitrary
permutation of the constituent pairs of blocks (A1, H1), . . . , (As, Hs).
Conversely, if for a pair of quaternion matrices (A, H), formula (10.1.3) holds
for some invertible quaternion matrix W, then H is invertible hermitian and A is
H-skewhermitian.
The proof of Theorem 10.1.3 is obtained by applying Theorem 9.4.1 to the
hermitian/skewhermitian matrix pencil H + tHA and taking advantage of the as-
sumption that H is invertible.
We have a sign characteristic for an H-skewhermitian matrix A: a sign ±1
is attached to every partial multiplicity of A corresponding to nonzero eigenvalues
with zero real parts and to odd partial multiplicities corresponding to the eigenvalue
zero.
The number i can be replaced in Theorem 10.1.3 by any quaternion γ with zero
real part and length one; indeed, to verify that, note the equalities
(−λI)Ξn(in−1)(λI)
=
Ξn((λ−1iλ)n−1),
(−λI)Jn(ib)(λI)
=
Jn((λ−1iλ)b),

232
CHAPTER 10
where λ ∈H has zero real part and length one and b ∈R, and note also that every
γ ∈H with zero real part and length one can be represented in the form γ = λ∗iλ,
for some λ ∈H with zero real part and length one (Theorem 2.5.1(c)).
It is instructive to compare blocks of type (ii) with the corresponding blocks
of Hα-hermitian complex matrices.
If Aα and Hα are n × n complex matrices
such that Hα is invertible and iAα is Hα-hermitian and the Jordan form of iAα
is J2m1(0) ⊕· · · ⊕J2mq(0), then by the canonical form for Hα-hermitian matrices
over C (use Theorem 15.3.1 for the pair of complex hermitian matrices Hα and
Hα(iAα)), there exists a complex invertible matrix W such that
W −1(iAα)W
=
J2m1(0) ⊕· · · ⊕J2mq(0),
W ∗HαW
=
τ1F2m1 ⊕· · · ⊕τqF2mq,
where τ1, . . . , τq are signs ±1. Another transformation with the invertible complex
matrix
W1 := ⊕q
j=1
 diag (1, i, i2, . . . , i2mj−1)

shows that
W −1
1
W −1AαWW1
=
J2m1(0) ⊕· · · ⊕J2mq(0),
W ∗
1 W ∗HαWW1
=
τ1Ξ2m1(i2m1−1) ⊕· · · ⊕τqΞ2mq(i2mq−1).
The signs here are uniquely deﬁned by the pair (Aα, Hα), up to a permutation of
signs corresponding to the Jordan blocks of equal sizes. However, over the quater-
nions the signs are immaterial because there is an invertible quaternion matrix T2m
such that
T −1
2mJ2m(0)T2m = J2m(0),
T ∗
2m(−Ξ2m(i2m−1))T2m = Ξ2m(i2m−1).
Indeed, T2m = jI2m will do.
Open Problem 10.1.4. Develop canonical forms for pairs of matrices (A, H),
where H = H∗∈Hn×n and HA = A∗H. The matrix H is not assumed to be
invertible.
Same problem with respect to pairs of matrices (A, H), where H is hermitian
and HA = −A∗H, or where H = −H and HA = ±A∗H.
Some results in the direction of Open Problem 10.1.4, in the context of complex
H-hermitian matrices, are found in Mehl and Rodman [111].
10.2
INVARIANT SEMIDEFINITE SUBSPACES
In this section we assume that H = H∗∈Hn×n and H is invertible. Our main result
here (the next theorem) asserts the possibility of extending invariant semideﬁnite
subspaces to subspaces that are simultaneously invariant and maximal semideﬁnite.
The invariance here refers to H-hermitian or H-skewhermitian matrices.
Theorem 10.2.1. Let A ∈Hn×n be H-hermitian or H-skewhermitian. Suppose
subspace M ⊆Hn×1 is H-nonnegative, resp.
H-nonpositive, and A-invariant.
Then there is an A-invariant H-nonnegative, resp. H-nonpositive, subspace N ⊆
Hn×1 that contains M and has dimension
dimH N = p,
resp.
dimH N = q,
where p = In+(H), q = In−(H).

INDEFINITE INNER PRODUCTS: CONJUGATION
233
Proof. We consider the case of H-nonnegative subspaces (for the H-nonpositive
subspaces apply the result for −H in place of H).
We use induction on the size n of the matrices H and A, and on the dimension
of M. The case n = 1 is trivial, and the case M = {0} follows by constructions
(which are standard for complex H-hermitian matrices; see, e.g., Gohberg et al.
[53, 52]) using the canonical forms (10.1.2) and (10.1.3).
We provide details of these constructions. Suppose A is H-hermitian. We can
assume without loss of generality, that H and A are given by the right-hand sides of
(10.1.2); indeed, a subspace M is A-invariant H-nonnegative (or H-nonpositive) if
and only if S−1(M) is S−1AS-invariant S∗HS-nonnegative (or S∗HS-nonpositive).
Select the vectors e1, . . . , ep corresponding to each pair F2p,
 Jp(λ)
0p
0p
Jp(λ∗)

(λ
nonreal) of 2p × 2p blocks, and select the vectors e1, . . . , es corresponding to each
pair η(p)
k,λFp, Jp(λ) (λ real) of p × p blocks as follows:
s =













p
2
if p is even,
p + 1
2
if p is odd and η(p)
k,λ = 1,
p −1
2
if p is odd and η(p)
k,λ = −1.
Observe that
In+(Fm) = In−(Fm) = m
2
if m is even,
and
In+(Fm) = m + 1
2
,
In−(Fm) = m −1
2
if m is odd.
Using this observation, it is easy to see that the selected vectors span (over H) an
A-invariant H-nonnegative subspace of dimension p.
Suppose now A is H-skewhermitian, and we may (and do) assume that A and
H are given by the right-hand sides of (10.1.3). Then we select vectors as follows.
For the block of type (ii), select
e1, e2, . . . , em1, e2m1+1, e2m1+2 . . . , e2m1+m2,
. . . , e2m1+···+2mq−1+1, e2m1+···+2mq−1+2, . . . , e2m1+···+2mq−1+mq.
For the block of type (iii) select
e1, e2, . . . , ek1, e2k1+1, e2k1+2 . . . , e2k1+k2,
. . . , e2k1+···+2ku−1+1, e2k1+···+2ku−1+2, . . . , e2k1+···+2ku−1+ku.
To make a selection of vectors in the blocks of type (i) and (iv), we ﬁrst note that
In+ (Ξn(in−1)) = n + 1
2
,
In−(Ξn(in−1)) = n −1
2
(10.2.1)
for n odd; in particular, the ((n + 1)/2, (n + 1)/2)th entry of Ξn(in−1) is equal to
1, and
In+ (Ξn(in−1)) = In−(Ξn(in−1)) = n
2
(10.2.2)

234
CHAPTER 10
for n even. Now, for each pair (J2nj+1(0), κjΞ2nj+1(i2nj)) of blocks as in (i), select
e1, e2, . . . , enj+1 if κj = 1 and select e1, e2, . . . , enj if κj = −1. Finally, for a pair
of blocks (Jhj(ib), ηjΞhj(ihj−1)), as in (iv), we select e1, e2, . . . , ehj/2 if hj is even,
select e1, e2, . . . , e(hj+1)/2 if hj is odd and ηj = 1, and select e1, e2, . . . , e(hj−1)/2
if hj is odd and ηj = −1. The subspace spanned by all the selected vectors is
A-invariant H-nonnegative and has dimension p = In+ (H).
Now proceed to the general case of the proof of Theorem 10.2.1. Using Lemma
4.5.1 and the notation of that lemma, we may assume that
M = SpanH {e1, . . . , en2+n3},
H =


0
0
In2
0
0
In3
0
0
In2
0
0
0
0
0
0
H0

,
A =


A22
A23
A24
A25
A32
A33
A34
A35
0
0
A44
A45
0
0
A54
A55

,
(10.2.3)
where H0 = H∗
0, App ∈Hnp×np for p = 2, 3, 4, 5, and n2 + n3 + n4 + n5 = n. Note
that we must have n1 = 0 because H is assumed to be invertible.
The condition (HA)∗= ±(HA) gives
A32 = 0,
A35 = 0,
A45 = 0,
and (H0A55)∗= ±H0A55. Let
Ip0 ⊕−Iq0
(10.2.4)
be the canonical form of H0 as in Theorem 4.1.6 (note that H0 is invertible because
H is). By the induction hypothesis there exists an A55-invariant H0-nonnegative
subspace M0 of dimension p0. Then the subspace


Hn2
0
0
0

˙+


0
Hn3
0
0

˙+


0
0
0
M0


is clearly A-invariant H-nonnegative and has dimension n2 + n3 + p0. It is easy to
see that n2 + n3 + p0 = In+(H).
□
Remark 10.2.2. It turns out that for an H-skewhermitian matrix A, there
exists an A-invariant maximal H-nonnegative (resp. H-nonpositive) subspace with
additional properties regarding location of the spectrum. Namely, let Λ be a set of
eigenvalues of A such that
λ ∈Λ,
µ ∈H,
µ similar to λ
=⇒
µ ∈Λ
(10.2.5)
and
λ ∈Λ
=⇒
−λ ̸∈Λ.
(10.2.6)
For example, the set of all eigenvalues of A with positive real parts satisﬁes these
conditions. Let MΛ be the sum of root subspaces of A corresponding to the eigen-
values in Λ. Then there exists an A-invariant maximal H-nonnegative subspace
that contains MΛ. Likewise, there exists an A-invariant maximal H-nonpositive
subspace that contains MΛ.
To see that, we may assume that A and H are given by the right-hand sides of
(10.1.3). The canonical form of Theorem 10.1.3 (see Part (iii) there) shows that if
Λ and MΛ are as above, then M is H-neutral. Now apply Theorem 10.2.1.

INDEFINITE INNER PRODUCTS: CONJUGATION
235
It is not generally true that every A-invariant H-positive (or H-negative) sub-
space is contained in an A-invariant H-positive (or H-negative) of dimension p (or
q) as in Theorem 13.3.1, where A is H-hermitian or H-skewhermitian. See Exs.
10.10.1, 10.10.2, and 10.10.3.
10.3
INVARIANT LAGRANGIAN SUBSPACES I
Of particular interest here are Lagrangian subspaces.
Deﬁnition 10.3.1. Given a hermitian or skewhermitian matrix G ∈Hn×n, a
subspace M ⊆Hn×1 is said to be G-Lagrangian if it is G-neutral and has dimension
n/2.
Clearly this deﬁnition makes sense only if n is even. As it follows from Theorems
4.2.6 and 4.2.7 and assuming n is even, G-Lagrangian subspaces exist if and only if
min{In+ (G), In−(G)} + In0 (G) ≥n
2
(10.3.1)
if G is hermitian, and
⌊rank G
2
⌋+ (n −rank G) ≥n
2
if G is skewhermitian. The latter inequality can be rewritten in the form
⌊rank G + 1
2
⌋≤n
2 .
(10.3.2)
We will use the notion of G-Lagrangian subspaces for invertible G (and even n); in
this case, (10.3.2) is always satisﬁed, and (10.3.1) becomes
In+ (G) = In−(G) = n
2 .
(10.3.3)
The following property of Lagrangian subspaces will be very useful.
Proposition 10.3.2. Let
G = ±G∗= G1 ⊕G2 ⊕· · · ⊕Gk ∈Hn×n,
where Gj ∈Hnj×nj, j = 1, 2, . . . , k, and where n is even. Assume G is invertible.
Then a subspace M ⊆Hn×1 of the form
M = M1 ⊕M2 ⊕· · · ⊕Mk :=











x1
x2
...
xk

: x1 ∈M1, . . . , xk ∈Mk









is G-Lagrangian if and only if each Mj is Gj-Lagrangian, for j = 1, 2, . . . , k (in
particular, each nj must be even).
Proof.
First note that each Gj is invertible since G is.
Suppose ﬁrst G is
hermitian; then each Gj is hermitian. If M is G-Lagrangian, then each Mj is

236
CHAPTER 10
Gj-neutral, and because Gj is invertible, dimH (Mj) ≤nj/2 (Theorem 4.2.6(e)).
On the other hand,
k
X
j=1
dimH (Mj) = dimH (M) = n
2 ,
(10.3.4)
so we must have the equality dimH (Mj) = nj/2, and Mj is Gj-Lagrangian. Con-
versely, if each Mj is Gj-Lagrangian, then clearly M is G-neutral, and the ﬁrst
equality in (10.3.4) shows that M is G-Lagrangian.
If G is skewhermitian, the proof is similar (Ex. 10.10.5).
□
If A is H-hermitian or H-skewhermitian, then A does not necessarily have an
invariant H-Lagrangian subspace even when (10.3.3) is fulﬁlled. A necessary and
suﬃcient condition for this to happen is given by the next theorem.
Theorem 10.3.3. Let H ∈Hn×n be hermitian and invertible.
(a) If A ∈Hn×n is H-hermitian, then there exists an A-invariant H-Lagrangian
subspace if and only if for every real eigenvalue λ of A (if any), the number of odd
partial multiplicities corresponding to λ is even, and exactly half of them have sign
−1 (the other half having the sign 1) in the sign characteristic of (A, H).
(b) If A ∈Hn×n is H-skewhermitian, then there exists an A-invariant H-
Lagrangian subspace if and only if for every eigenvalue λ of A with zero real part
(if any), the number of odd partial multiplicities corresponding to λ is even, and
exactly half of them have sign −1 (the other half having the sign 1) in the sign
characteristic of (A, H).
Proof. Note that for any invertible S ∈Hn×n, the subspace M is H-Lagrangian
if and only if S−1(M) is S∗HS-Lagrangian. Therefore, without loss of generality,
we may (and do) assume that the pair (A, H) is given by the canonical form of
Theorem 10.1.1 (if A is H-hermitian) or Theorem 10.1.3 (if A is H-skewhermitian).
Proof of Part (a). In view of Proposition 10.3.2 and the fact that every A-
invariant subspace is the sum of its intersections with the root subspaces of A
(Proposition 5.1.4), we need only consider the situation when Hn×1 is a root sub-
space for A. Two cases can occur. (1) The eigenvalues of A (all similar to each
other) are nonreal; (2) the sole eigenvalue λ of A is real. In the ﬁrst case, the
condition in Part (a) is vacuous, and an A-invariant H-Lagrangian subspace al-
ways exists. Indeed, as in the proof of Theorem 10.2.1, select the vectors e1, . . . , ep
corresponding to each pair F2p,

Jp(λ)
0p
0p
Jp(λ∗)

of 2p × 2p blocks. Then the
subspace spanned by the selected vectors is A-invariant and H-Lagrangian.
Suppose now Case (2) holds true. We prove the “if” direction. Rearranging the
blocks (if necessary) in the canonical form, we assume that
H
=
(Fp1 ⊕−Fp2) ⊕· · · ⊕(Fp2u−1 ⊕−Fp2u)
⊕η2u+1Fp2u+1 ⊕· · · ⊕η2u+vFp2u+v,
A
=
Jp1(λ) ⊕· · · ⊕Jp2u+v(λ),

INDEFINITE INNER PRODUCTS: CONJUGATION
237
where p1, . . . , p2u are odd and p2u+1, . . . , p2u+v are even, and the ηj’s are signs ±1.
(The cases when u = 0 or v = 0 are not excluded.) Select the following vectors in
Hn×1:
e1, . . . , e(p1−1)/2, e(p1+1)/2 + ep1+(p2+1)/2, ep1+1, . . . , ep1+(p2−1)/2, . . . ,
(10.3.5)
ep1+···+p2j+1, . . . , ep1+···+p2j+(p2j+1−1)/2,
(10.3.6)
ep1+···+p2j+(p2j+1+1)/2 + ep1+···+p2j+p2j+1+(p2j+2+1)/2,
(10.3.7)
ep1+···+p2j+p2j+1+1, . . . , ep1+···p2j+p2j+1+(p2j+2−1)/2
(10.3.8)
for j = 1, 2, . . . , u −1, and
ew+1, . . . , ew+p2u+1/2, ew+p2u+1+1, . . . , ew+p2u+1+p2u+2/2, . . . ,
(10.3.9)
ew+p2u+1+···+p2u+v−1+1, . . . , ew+p2u+1+···+p2u+v−1+p2u+v/2,
(10.3.10)
where w = p1 + · · · + p2u. By inspection, we see that the selected vectors (10.3.5),
(10.3.6), (10.3.8), (10.3.9), and (10.3.10) span an A-invariant H-Lagrangian sub-
space.
For the “only if” direction, let
H = ⊕r
j=1ηjFpj,
A = ⊕r
j=1Jpj(λ),
(10.3.11)
where ηj = ±1. If there exists an A-invariant H-Lagrangian subspace, then, in
particular, n is even and we must have In+ (H) = In−(H) = n/2 (cf. (10.3.3)). It
is easily seen that for H given by (10.3.11),
In+ (H) −In−(H) =
X
{j : pj is odd}
ηj,
so the condition in Theorem 10.3.3(a) follows.
The proof of Part (b) follows the same pattern as that of Part (a), using equali-
ties (10.2.1), (10.2.2), and the fact that the ((n+1)/2, (n+1)/2)th entry of Ξn(in−1)
is equal to 1 if n is odd (Ex. 10.10.6).
□
Remark 10.3.4. For any H-skewhermitian matrix A, if there exists an A-invari-
ant H-Lagrangian subspace, then there exists such subspace with additional prop-
erties regarding location of the spectrum, as in Remark 10.2.2. Namely, let Λ be a
set of eigenvalues of A with the properties (10.2.5), (10.2.6). Then there exists an
A-invariant H-Lagrangian subspace M that contains the sum of root subspaces for
A corresponding to the eigenvalues in Λ.
To prove that, it suﬃces to consider the case when the set Λ is maximal (by
inclusion) subject to conditions (10.2.5), (10.2.6). In this case, it turns out that the
restriction A|M has no eigenvalues with nonzero real parts besides those in Λ. To
construct such a subspace M, in view of the canonical form (10.1.3), it suﬃces to
consider two cases: (1) all eigenvalues of A have zero real parts; (2) A and H are
given by the right-hand sides of (10.1.4) and (10.1.5), respectively. In the ﬁrst case
Λ is vacuous, so any A-invariant H-Lagrangian subspace M will do. In the second
case, suppose for example that a + ib ∈Λ, −a + ib ̸∈Λ (if a + ib ̸∈Λ, −a + ib ∈Λ,
the consideration is completely analogous). Select the following vectors:
e1, e2, . . . , ek1, e2k1+1, . . . , e2k1+k2, . . . ,
e2k1+2k2+···+2ku−1+1, e2k1+2k2+···+2ku−1+2, . . . , e2k1+2k2+···+2ku−1+ku.
The selected vectors span in Hn×1, n = 2k1 + · · · + 2ku, a subspace M which is
A-invariant H-Lagrangian, and σ(A|M) consists of all quaternions similar to a+ib.

238
CHAPTER 10
10.4
DIFFERENTIAL EQUATIONS I
In this section we study the system of diﬀerential equations with constant coeﬃ-
cients
Aℓx(ℓ)(t) + Aℓ−1x(ℓ−1)(t) + · · · + A1x′(t) + A0x(t) = 0,
t ∈R,
(10.4.1)
where Aℓ, . . . , A1, A0 ∈Hn×n, and x(t) is an unknown ℓtimes continuously diﬀer-
entiable Hn×1-valued function of the real independent variable t. It will be assumed
in addition that Ak is hermitian if k is odd, Ak is skewhermitian if k is even, and
Aℓis invertible. As in Section 5.14, consider the companion matrix of the system
(10.4.1):
C =


0
In
0
. . .
0
0
0
In
. . .
0
...
...
...
...
...
0
0
0
. . .
In
−A−1
ℓA0
−A−1
ℓA1
−A−1
ℓA2
. . .
−A−1
ℓAℓ−1


∈Hℓn×ℓn.
(10.4.2)
Deﬁne also
G :=


A1
A2
A3
. . .
Aℓ
−A2
−A3
. . .
−Aℓ
0n
A3
A4
. . .
0n
0n
−A4
. . .
. . .
0n
0n
...
...
...
...
...
(−1)ℓ−1Aℓ
0n
0n
. . .
0n


∈Hnℓ×nℓ.
(10.4.3)
Clearly, G is hermitian and invertible, and
(In+ (G), In−(G) =































(nℓ
2 , nℓ
2 )
if ℓis even,
n(ℓ−1)
2
+ In−(Aℓ), n(ℓ−1)
2
+ In+ (Aℓ)

if ℓis odd, ℓ−1
2
is odd,
n(ℓ−1)
2
+ In+ (Aℓ), n(ℓ−1)
2
+ In−(Aℓ)

if ℓis odd, ℓ−1
2
is even.
(10.4.4)
To verify (10.4.4), consider a family of hermitian matrices
Gε =


εA1
εA2
εA3
. . .
Aℓ
−εA2
−εA3
. . .
−Aℓ
0n
εA3
εA4
. . .
0n
0n
−εA4
. . .
. . .
0n
0n
...
...
...
...
...
(−1)ℓ−1Aℓ
0n
0n
. . .
0n


,
0 ≤ε ≤1.
(10.4.5)
Clearly, Gε is hermitian and invertible for all ε ∈[0, 1]. By the continuous depen-
dence of eigenvalues on the entries of the matrix (Theorem 5.2.5), the inertia of Gε

INDEFINITE INNER PRODUCTS: CONJUGATION
239
is constant (i.e., independent of ε). It is not diﬃcult to see that the inertia of G0
is given by the right-hand side of (10.4.4) (Ex. 10.10.4).
The key observation is that the matrix GC is skewhermitian; in other words, C
is G-skewhermitian. The equality GC = −C∗G is veriﬁed by direct computation.
Deﬁnition 10.4.1. We say that the system (10.4.1) is forward, resp. backward,
bounded if all solutions are bounded as t −→+∞, resp. t −→−∞. The system
(10.4.1) is said to be bounded if all solutions are bounded on the real line.
Theorem 10.4.2. The following four statements are equivalent for the system
(10.4.1), where Ak is hermitian if k is odd, Ak is skewhermitian if k is even, and
Aℓis invertible:
(a) the system is forward bounded;
(b) the system is backward bounded;
(c) the system is bounded;
(d) all eigenvalues of C have zero real parts, and for every eigenvalue the geomet-
ric multiplicity coincides with the algebraic multiplicity.
Proof. Clearly (c) implies both (a) and (b). The equivalence of (c) and (d)
follows from Theorem 5.14.2(b) and from the analogue of Theorem 5.14.2(b) for
backward stability (Ex. 5.16.17). It remains to prove that (a) or (b) implies (d).
We show that (a) implies (d); the proof of the statement that (b) implies (d) is
completely analogous. If (a) holds true, then by Theorem 5.14.2(b) all eigenvalues
of C have nonpositive real parts. But then the canonical form (Theorem 10.1.3)
shows that the G-skewhermitian matrix C cannot have eigenvalues with nonzero
real parts. Equality of geometric and algebraic multiplicities for every eigenvalue
follows from the same Theorem 5.14.2(b).
□
In applications, it is often desirable to know if a given system is not only
bounded, but all nearby systems are also bounded. This concept of stable bound-
edness is formally deﬁned as follows.
Deﬁnition 10.4.3. A system of diﬀerential equations (10.4.1) is said to be stably
bounded if there exists ε > 0 such that every system of diﬀerential equations
A′
ℓx(ℓ)(t) + A′
ℓ−1x(ℓ−1)(t) + · · · + A′
1x′(t) + A′
0x(t) = 0,
t ∈R,
(10.4.6)
is bounded, provided the coeﬃcients A′
j ∈Hn×n, j = 0, 1, . . . , ℓ, satisfy the following
conditions:
(1) If k is odd, then A′
k is hermitian.
(2) If k is even, then A′
k is skewhermitian.
(3) The inequalities
∥A′
j −Aj∥< ε,
for j = 0, 1, 2, . . . , ℓ,
hold true.

240
CHAPTER 10
Note that A′
ℓwill be invertible if ∥A′
ℓ−Aℓ∥< ε holds and ε is small enough.
Indeed, this follows, for example, from the continuity of the determinant function
and Theorem 5.9.2(2).
In particular, a stable bounded system is bounded. A suﬃcient condition for
stable boundedness is given next.
Theorem 10.4.4. Assume that every root subspace of the companion matrix C
is either G-positive or G-negative. Then the system (10.4.1) is stably bounded.
Proof. We prove Theorem 10.4.4 in two steps.
Step 1.
We show that (10.4.1) is bounded.
Indeed, the canonical form of
Theorem 10.1.3 applied to the pair (G, C), where C is G-skewhermitian, shows that
all eigenvalues of C have zero real parts (otherwise, the corresponding eigenvectors
are G-neutral) and for every eigenvalue the geometric multiplicity coincides with
the algebraic multiplicity (otherwise, there is a G-neutral eigenvector). Then the
boundedness of (10.4.1) follows from Theorem 10.4.2.
Step 2. We prove the stable boundedness of (10.4.1). Since every root subspace
M of C is either G-positive or G-negative, the same property holds for any subspace
suﬃciently close to M (in the gap metric). Indeed, we have |x∗Gx| > 0 for every
nonzero x ∈M, and therefore, by compactness of the unit sphere SM in M, the
inequality
min
x∈SM {|x∗Gx|} ≥ε > 0
holds true. Now suppose a subspace X ⊆Hn×1 is close to M:
θ(X, M) = ∥PX −PM∥≤δ,
where δ > 0 is small. Take x ∈X, ∥x∥= 1. Then
∥x −PMx∥= ∥PX x −PMx∥≤δ,
(10.4.7)
so for y := PMx we have ∥y∥≥1 −δ and
|y∗Gy| = (1 −δ)2 |(y/(1 −δ))∗G(y/(1 −δ))| ≥(1 −δ)2ε.
On the other hand,
x∗Gx = (x −y)∗G(x −y) + y∗G(x −y) + (x −y)∗Gy + y∗Gy;
hence,
|x∗Gx −y∗Gy| ≤δ2∥G∥+ 2δ∥G∥∥y∥≤δ2∥G∥+ 2δ(1 + δ)∥G∥,
where (10.4.7) was used. Thus,
|x∗Gx| ≥(1 −δ)2ε −(δ2∥G∥+ 2δ(1 + δ)∥G∥),
which can be made ≥ε/2 if δ is suﬃciently small.
This shows that X is also
G-positive (or G-negative, as the case may be).
Let λ0 be the eigenvalue of C in the closed upper half of the complex plane
C+ such that the root subspace M corresponds to λ. The C-invariant subspace
M, being a root subspace, is a Lipschitz function (Theorem 5.2.5), so every nearby

INDEFINITE INNER PRODUCTS: CONJUGATION
241
matrix C′ has an invariant subspace M′ close to M, and every such subspace
is either G-positive or G-negative by the observation in the preceding paragraph.
Moreover, in fact, M′ is the sum of root subspaces for C′ corresponding to the
eigenvalues in C+ which are close to λ, as indicated in the same Theorem 5.2.5. It
follows that every root subspace of C′ is either G-positive or G-negative. By Step 1,
taking C′ to be the companion matrix of a system (10.4.6), we obtain that (10.4.6)
is bounded, if (1), (2), and (3) are satisﬁed with suﬃciently small ε > 0.
□
As the proof of Theorem 10.4.4 shows, if (10.4.1) is stably bounded, then, for
suﬃciently small ε, every system (10.4.6) satisfying (1), (2), and (3) is also stably
bounded (not only bounded as required by the deﬁnition of stable boundedness).
We do not know whether or not the condition of Theorem 10.4.4 is also necessary.
Open Problem 10.4.5. Prove or disprove that if the system of diﬀerential
equations (10.4.1) is stably bounded, then every root subspace of C is either G-
negative or G-positive.
The solution of Open Problem 10.4.5 is aﬃrmative if ℓ= 1, as given in the next
theorem.
Theorem 10.4.6. The system of ﬁrst order diﬀerential equations
A1x′(t) + A0x(t) = 0,
(10.4.8)
where
A1 = A∗
1 ∈Hn×n,
A0 = −A∗
0 ∈Hn×n,
and A1 is invertible, is stably bounded if and only if every root subspace of A−1
1 A0
is either A1-positive or A1-negative.
Proof. The “if” part is contained in Theorem 10.4.4. To prove the “only if”
part, assume that there is a root subspace of A−1
1 A0 that is not A1-positive or
A1-negative, and we will show that the system (10.4.8) is not stably bounded. In
view of Theorem 10.4.2 we may (and do) also assume that all eigenvalues of A−1
1 A0
have zero real parts and for every eigenvalue of A−1
1 A0, the geometric multiplicity
is equal to its algebraic multiplicity. Taking A1 and A−1
1 A0 as in the right-hand
side of (10.1.3), we see that these matrices have the following form:
A1 = η1 ⊕· · · ⊕ηt ⊕κ1 ⊕· · · ⊕κp,
A−1
1 A0 = (ib1) ⊕· · · ⊕(ibt) ⊕0p,
where b1, . . . , bt are positive real numbers and the κi’s, ηj’s are signs ±1.
The
condition that not all root subspaces of A−1
1 A0 are A1-positive or A1-negative
amounts to the following: not all the κi’s are the same, or bj1 = bj2, ηj1 ̸= ηj2 for
some indices j1, j2 (1 ≤j1 < j2 ≤t). For notational simplicity suppose b1 = b2,
η1 = −η2 = 1. Then for nonzero real number y, the matrix
X := A−1
1 A0 +
 0
y
y
0

⊕0t+p−2

is A1-skewhermitian, close to A−1
1 A0 (if y is close to zero), and has eigenvalues
ib1 ± y with nonzero real parts. Thus, the system A1x′(t) + (A1X)x(t) = 0 is not
bounded, and (10.4.8) is not stably bounded.
□

242
CHAPTER 10
10.5
HAMILTONIAN, SKEW-HAMILTONIAN MATRICES:
CANONICAL FORMS
In this section H ∈Hn×n is an invertible skewhermitian matrix.
Recall that
A ∈Hn×n is said to be (H,∗)-Hamiltonian if A∗H = −HA, and (H,∗)-skew-
Hamiltonian if A∗H = HA. In this chapter, we write, for short, H-Hamiltonian and
H-skew-Hamiltonian instead of (H,∗)-Hamiltonian and (H,∗)-skew-Hamiltonian,
respectively.
We begin with canonical forms. The Hamiltonian case is treated in the next
theorem.
Theorem 10.5.1. Let H ∈Hn×n be an invertible skewhermitian matrix, and let
X ∈Hn×n be H-Hamiltonian. Then for some invertible quaternion matrix S, the
matrices S∗HS and S−1XS have, simultaneously, the following form:
S∗HS = ⊕r
j=1ηjΞℓj(iℓj) ⊕⊕s
v=1

0
Fpv
−Fpv
0

⊕⊕q
u=1ζuΞmu(imu),
(10.5.1)
S−1XS = ⊕r
j=1Jℓj(0) ⊕⊕s
v=1
 −Jpv(αv)
0
0
Jpv(αv)

⊕⊕q
u=1Jmu(γu),
(10.5.2)
where ηj, ζu are signs ±1 with the additional condition that ηj = 1 if ℓj is odd,
the complex numbers α1, . . . , αs have positive real parts, and the complex numbers
γ1, . . . , γq are nonzero with zero real parts.
The form (10.5.1), (10.5.2) is uniquely determined by the pair (X, H), up to an
arbitrary simultaneous permutation of primitive blocks in each of the three parts,
 ⊕r
j=1ηjΞℓj(iℓj), ⊕r
j=1Jℓj(0)

,

⊕s
v=1

0
Fpv
−Fpv
0

, ⊕s
v=1
 −Jpv(αv)
0
0
Jpv(αv)

,
and
(⊕q
u=1ζuΞmu(imu), ⊕q
u=1Jmu(γu)) ,
and up to replacements of some αk’s and some γj’s with their complex conjugates.
Conversely, if H, X have the forms (10.5.1), (10.5.2), then H is invertible
skewhermitian, and X is H-Hamiltonian.
Remarks:
(1) A more general form than (10.5.1), (10.5.2) can be given (see Rodman [135]).
Namely, the αk’a and γj’s are allowed to be quaternions (under additional
restriction that iγj is real if mj is odd), and the uniqueness is up to replace-
ments in some primitive blocks, simultaneously in (10.5.1) and (10.5.2) of
the αk’s and the γj’s with similar quaternions. In this connection, note the
formulas
S−1(iJm(λ))S
=
Jm(iλ),
S∗(iFm)S
=
( −Ξm(im)
if m is even,
Ξm(im)
if m is odd,
(10.5.3)
where λ is real, and S = diag (1, −i, (−i)2, . . . , (−i)m−1).

INDEFINITE INNER PRODUCTS: CONJUGATION
243
(2) The formulas
(jI2pν)−1
 −Jpv(αv)
0
0
Jpv(αv)

jI2pν
=
 −Jpv(αv)
0
0
Jpv(αv)

,
(jI2pν)∗

0
Fpv
−Fpv
0

jI2pν
=

0
Fpv
−Fpv
0

make explicit the replacement of αν by its complex conjugate.
(3) The following formulas make explicit the replacement of γu by its complex
conjugate:
S−1Jmu(−γu)S
=
Jmu(γu),
S∗(Ξmu(imu))S
=
( −Ξmu(imu)
if mu is odd,
Ξmu(imu)
if mu is even,
where S = jI. Note that under this replacement ζu reverses to its negative if
mu is odd and remains invariant if mu is even.
(4) The sign characteristic of an H-Hamiltonian matrix A assigns a sign ±1 to
every even partial multiplicity of A corresponding to the zero eigenvalue and
to all partial multiplicities of every nonzero eigenvalue with zero real part.
Proof of Theorem 10.5.1. Note that H is skewhermitian and HA is hermi-
tian. We use the canonical form for the hermitian/skewhernitian pencil HA + tH
under congruence given in Theorem 9.4.1. Because H is invertible, blocks of the
forms (q-h-sk0), (q-h-sk1), (q-h-sk2) cannot appear. We consider separately each
of the blocks (q-h-sk3), (q-h-sk4), (q-h-sk5).
For a block of type (q-h-sk3), we have
H = ηiFℓ,
A = H−1(HA) = (ηiFℓ)−1 · ηGℓ= −iJℓ(0)T ,
(10.5.4)
where the sign η is always +1 if ℓis odd.
We can easily transform, using the
transformation
(H, A)
7→
(W ∗HW, W −1AW)
for some invertible
W ∈Hℓ×ℓ,
(10.5.5)
the pair (H, A) given by (10.5.4) to the pair (ηΞℓ(iℓ), Jℓ(0)) as required in Theorem
10.5.1. Indeed, let
W =


0
0
. . .
0
iℓ−1
0
0
. . .
iℓ−2
0
...
...
...
...
...
1
0
. . .
0
0

,
then a calculation shows that the equalities
W ∗(iFℓ)W = Ξℓ(iℓ),
W −1(−iJℓ(0)T )W = Jℓ(0)
hold true.

244
CHAPTER 10
For block of type (q-h-sk4), we have H =

0
Fp
−Fp
0

, and
A
=
H−1(HA) =

0
Fp
−Fp
0
−1 
0
αFp + Gp
α∗Fp + Gp
0

=

−Jp(α)T
0
0
Jp(α)T

,
where α ∈H has positive real part, and we may take α to be a complex number.
Applying a suitable transformation of type (10.5.5), we obtain the pair

0
Fp
−Fp
0

,
 −Jp(α)
0
0
Jp(α)

,
again as required in Theorem 10.5.1.
Finally, for a block of type (q-h-sk5), we have
H = ζΞm(im),
HA = ζ(Ξm(β) −eGm),
ζ = ±1,
(10.5.6)
where β is a nonzero quaternion with zero real part if m is even and β is a positive
real number if m is odd. We may (and do) assume that, in fact, β is a complex
number, and if necessary we may replace β with −β (see Remark 9.4.3). It will be
convenient to use the following notation:
Γm(λ) :=


0
λ
0
. . .
0
0
0
−λ
. . .
0
...
...
...
...
...
0
0
0
. . .
(−1)mλ
0
0
0
. . .
0


∈Hm×m,
λ ∈H.
A straightforward computation shows that, for H and HA given in (10.5.6), we
have
A
=
βIm + Γm(1)
if m is even, divisible by 4;
A
=
−βIm + Γm(−1)
if m is even, not divisible by 4;
A
=
−iβIm + Γm(i)
if m is odd, m −1 divisible by 4;
A
=
iβIm + Γm(−i)
if m is odd, m −1 not divisible by 4.
In each of these cases, the pair (H, A) can be easily transformed (using the trans-
formation (10.5.5)) to the form
(eζΞm(im), Jm(γ)),
γ ∈C \ {0},
R(γ) = 0,
eζ = ±1.
Indeed, for m even we have
(diag (I2, −I2, . . . , (−1)m/2−1I2)) Ξm(im)
· (diag (I2, −I2, . . . , (−1)m/2−1I2)) = (−1)m/2−1Ξm(im),
(diag (I2, −I2, . . . , (−1)m/2−1I2)) (±(βIm + Γm(1)))
· (diag (I2, −I2, . . . , (−1)m/2−1I2)) = ±Jm(β);

INDEFINITE INNER PRODUCTS: CONJUGATION
245
if m is even but not divisible by 4, a further transformation is needed:
(diag (1, −1, 1, . . . , −1)) Ξm(im) (diag (1, −1, 1, . . . , −1))
=
−Ξm(im),
(diag (1, −1, 1, . . . , −1)) (−Jm(β)) (diag (1, −1, 1, . . . , −1))
=
Jm(−β).
For m odd,
(diag (1, i, 1, i, . . . , 1)) Ξm(im) (diag (1, −i, 1, . . . , 1)) = Ξm(im),
(diag (1, i, 1, i, . . . , 1)) (±(−iβIm + Γm(i))(diag (1, −i, 1, −i, . . . , 1))
= ±Jm(−iβ),
and for the case when m−1 is not divisible by 4, a further transformation is needed,
as in the case m is even, not divisible by 4.
□
Next, consider H-skew-Hamiltonian matrices. The canonical form runs as fol-
lows.
Theorem 10.5.2. Let H ∈Hn×n be skewhermitian and invertible, and let A be
H-skew-Hamiltonian. Then there exists an invertible matrix S ∈Hn×n such that
S−1AS = Jℓ1(β1) ⊕· · · ⊕Jℓq(βq),
S∗HS = iFℓ1 ⊕· · · ⊕iFℓq,
(10.5.7)
where β1, . . . , βq ∈H are such that iβj have zero real parts for j = 1, 2, . . . , q.
The form (10.5.7) is uniquely determined by the pair (A, H), up to a simul-
taneous permutation of the pairs of corresponding primitive blocks in S−1AS and
S∗HS, and up to replacement of each βj by a similar quaternion β′
j subject to the
condition that iβ′
j has zero real part.
Conversely, if A and H are given by the right-hand sides of the equalities in
(10.5.7), then A is H-skew-Hamiltonian.
Note that the canonical form for a pair (A, H), where H = −H∗is invertible and
A is H-skew-Hamiltonian, does not involve sign characteristic. Also, in contrast
with many other canonical forms in the book, the form (10.5.7) cannot be given
generally speaking in terms of real and complex matrices. For example, one can
take β1, . . . , βq ∈SpanR {1, j} or, alternatively β1, . . . , βq ∈SpanR {1, k} .
Proof. We follow the same approach as in the proof of Theorem 10.5.1. Here
H and HA are both skewhermitian. The canonical form for the skewhermitian
matrix pencil HA + tH (Theorem 9.1.1) allows us to consider each primitive block
separately. Taking into account that H is invertible, we have to consider only the
case when
H = iFk,
HA =


0
0
· · ·
0
0
iβ
0
0
· · ·
0
iβ
i
0
0
· · ·
iβ
i
0
...
...
...
...
...
...
iβ
i
0
· · ·
0
0


k×k
,
where β ∈H is such that iβ has zero real part. Thus,
A = H−1(HA) = Jk(β),
and so the pair (H, A) is in the form as required in Theorem 10.5.2.
□

246
CHAPTER 10
Corollary 10.5.3.
(a) A matrix A ∈Hn×n is similar to an H-Hamiltonian
matrix for some invertible skewhermitian H if and only its Jordan structure
is symmetric with respect to negation: if λ ∈σ(A) and λ has nonzero real
part, then also −λ ∈σ(A), and the partial multiplicities of A at λ coincide
with those at −λ.
(b) Any matrix A ∈Hn×n is similar to an H-skew-Hamiltonian matrix for some
invertible skewhermitian H.
Recall that partial multiplicities of A at its eigenvalue λ also include, by deﬁni-
tion, all µ ∈H similar to λ.
The corollary is immediate upon inspection of the canonical forms (10.5.2) and
(10.5.7).
10.6
INVARIANT LAGRANGIAN SUBSPACES II
We assume in this section, as before, that H ∈Hn×n is skewhermitian and invertible
and assume in addition that n is even. Recall that a subspace M ⊆Hn×1 is said to
be H-Lagrangian if M is H-neutral and dim M = n/2. By Theorem 4.2.7(2) (or
(10.3.2)) Lagrangian subspaces always exist.
In this section we give criteria (based on the canonical forms of Section 10.5)
for existence of invariant Lagrangian subspaces for H-Hamiltonian and H-skew-
Hamiltonian matrices.
Theorem 10.6.1. (a) Let X ∈Hn×n be H-Hamiltonian.
Then there exists
an X-invariant H-Lagrangian subspace if and only if every root subspace of X
corresponding to eigenvalues with zero real part (including the zero eigenvalue if X
is not invertible) has even (quaternion) dimension.
(b) Let X ∈Hn×n be H-skew-Hamiltonian. Then there exists an X-invariant
H-Lagrangian subspace if and only if every root subspace for X is even dimensional.
Proof. We start with the proof of Part (b).
The condition is obviously necessary in view of the canonical form (10.5.7) for
the pair (X, H) and Proposition 10.3.2. Now suppose that every root subspace of
X is even dimensional. We may assume that X and H are given by the right-hand
sides of the formulas in (10.5.7):
X = Jℓ1(β1) ⊕· · · ⊕Jℓq(βq),
H = iFℓ1 ⊕· · · ⊕iFℓq,
where β1, . . . , βq ∈SpanR {1, j}. Replacing some of the βj’s by similar quaternions,
we further assume that
βj = aj + jbj,
j = 1, 2, . . . , q,
where aj, bj ∈R,
bj ≥0.
By Proposition 10.3.2 it suﬃces to consider the case when X has only one root
subspace; in other words, given the assumptions on the βj’s, the equalities β1 =
· · · = βq hold true. By our hypothesis, the number of odd indices (counted with
repetitions) among ℓ1, . . . , ℓq is even. Rearranging blocks (if necessary) in (5.3.7),
we let ℓ1, . . . , ℓ2u be odd and ℓ2u+1, . . . , ℓq be even for some nonnegative integer
u. Noting that SpanH
 1
j

is
 i
0
0
i

-neutral, we select the following vectors in
Hn×1:
e1, . . . , e(ℓ1−1)/2, e(ℓ1+1)/2 + jeℓ1+(ℓ2+1)/2, eℓ1+1, . . . , eℓ1+(ℓ2−1)/2,
(10.6.1)

INDEFINITE INNER PRODUCTS: CONJUGATION
247
eℓ1+···+ℓ2j+1, . . . , eℓ1+···+ℓ2j+···+(ℓ2j+1−1)/2,
(10.6.2)
eℓ1+···+ℓ2j+···+(ℓ2j+1+1)/2 + jeℓ1+···+ℓ2j+ℓ2j+1+(ℓ2j+2+1)/2,
(10.6.3)
eℓ1+···+ℓ2j+ℓ2j+1+1, . . . , eℓ1+···+ℓ2j+ℓ2j+1+(ℓ2j+2−1)/2,
(10.6.4)
for j = 1, 2, . . . , u −1, and, setting w = ℓ1 + · · · + ℓ2u,
ew+1, . . . , ew+ℓ2u+1/2, ew+ℓ2u+1+1, . . . , ew+ℓ2u+1+ℓ2u+2/2, . . . ,
(10.6.5)
ew+ℓ2u+1+···+ℓq−1+1, ew+ℓ2u+1+···+ℓq−1+2, . . . , ew+ℓ2u+1+···+ℓq−1+ℓq/2.
(10.6.6)
The selected vectors span an X-invariant H-Lagrangian subspace in Hn×1.
Proof of Part (a).
As in Part (b), by Proposition 10.3.2 and the canonical
form in Theorem 10.5.1, the conditions in Part (a) are necessary for existence of
X-invariant H-Lagrangian subspaces. To prove suﬃciency of these conditions, in
view of Theorem 10.5.1 we may (and do) assume that X and H have one of the
following three forms:
(1)
H = ⊕s
v=1

0
Fpv
−Fpv
0

,
X = ⊕s
v=1
 −Jpv(α)
0
0
Jpv(α)

,
where α ∈C has positive real part;
(2)
H = ⊕v
j=1Ξℓj(iℓj) ⊕⊕q
j=v+1ηjΞℓj(iℓj),
X = ⊕q
j=1Jℓj(0),
where ℓ1, . . . , ℓv are odd, ℓv+1, . . . , ℓq are even, and the ηj’s are signs ±1;
(3)
H = ⊕q
j=1ζjΞmj(imj),
X = ⊕q
j=1Jmj(γ),
where the ζj’s are signs ±1, and γ ∈C has zero real part and positive imagi-
nary part.
In Case (1), an X-invariant H-Lagrangian subspace always exists. To construct
such a subspace, select
e1, e2, . . . , ep1, e2p1+1, e2p1+2, . . . , e2p1+p2, . . . ,
e2p1+···+2pv−1+1, . . . , e2p1+···+2pv−1+pv
and span the subspace in Hn×1 by the selected vectors.
Assume Case (2) holds and assuming that the conditions of Theorem 10.6.1 hold
true, i.e., v = 2u is even, we select the vectors as in (10.6.1)–(10.6.6). Then the
subspace spanned by the selected vectors in Hn×1 is X-invariant and H-Lagrangian.
Finally, assume (3) holds. For simplicity of notation, suppose that m1, . . . mv are
odd and mv+1, . . . , mq are even. Since we assume that the conditions of Theorem
10.6.1(a) hold true, v = 2u is even. Now select the following vectors in Hn×1:
e1, . . . , e(m1−1)/2, e(m1+1)/2 + z1em1+(m2+1)/2, em1+1, . . . , em1+(m2−1)/2,
em1+···+m2j+1, . . . , em1+···+m2j+···+(m2j+1−1)/2,
em1+···+m2j+···+(m2j+1+1)/2 + zj+1em1+···+m2j+m2j+1+(m2j+2+1)/2,

248
CHAPTER 10
em1+···+m2j+m2j+1+1, . . . , em1+···+m2j+m2j+1+(m2j+2−1)/2
for j = 1, 2, . . . , u −1, where zj = 1 if ζ2j−1 = −ζ2j and zj = j if ζ2j−1 = ζ2j, and,
setting w = ℓ1 + · · · + ℓ2u,
ew+1, . . . , ew+ml2u+1/2, ew+m2u+1+1, . . . , ew+m2u+1+m2u+2/2, . . . ,
ew+m2u+1+···+mq−1+1, ew+m2u+1+···+mq−1+2, . . . , ew+m2u+1+···+mq−1+mq/2.
We verify, using Proposition 10.3.2, that the selected vectors span an X-invariant
H-Lagrangian subspace.
□
10.7
EXTENSION OF SUBSPACES
Throughout this section we ﬁx a skewhermitian matrix H ∈Hn×n and an H-
Hamiltonian matrix A ∈Hn×n. Thus, HA is hermitian, and we can consider the
classes of HA-nonnegative, of HA-nonnegative, of maximal HA-nonnegative, and
of maximal HA-nonnegative subspaces. The main results of this section have to do
with extensions of A-invariant H-neutral subspaces to A-invariant maximal HA-
semideﬁnite subspaces.
Remark 10.7.1. Note that for an A-invariant subspace M, if M is H-neutral,
then M is also HA-neutral. The converse generally speaking is false (take A = 0
to provide a counterexample).
However, if A is invertible, then an A-invariant
subspace M is H-neutral if and only if it is HA-neutral. Indeed, suppose M is
HA-neutral; then x∗HAy = 0 for all x, y ∈M. Now take y such that Ay = x to
obtain that x∗Hx = 0 for all x ∈M, and it follows that M is H-neutral (cf. the
polarization identity (3.5.1)).
Theorem 10.7.2. Let N ⊆Hn×1 be an A-invariant H-neutral subspace. Then
there exist A-invariant subspaces L+ and L−such that each of them contains N
and L+ is maximal HA-nonnegative, whereas L−is maximal HA-nonpositive.
Remark 10.7.3. We compute the dimensions of the maximal HA-nonnegative
and maximal HA-nonpositive subspaces in Theorem 10.7.2.
Inspection of the
canonical form of the pair (H, A) as in Theorem 10.5.1 reveals that
In+ (HA) −In−(HA)
=
X
{u : mu
is
odd}
(−ζu · sign(Iγu))
+
X
{j : ℓj
is
even}
ηj
(10.7.1)
and
In0 (HA) = r.
So, denoting by Υ the right-hand side of (10.7.1), we have
In± (HA) = 1
2 (±Υ + n −r),
and the dimension of a maximal HA-nonnegative, resp. maximal HA-nonpositive,
subspace is equal to (1/2) (Υ + n + r), resp. (1/2) (−Υ + n + r).

INDEFINITE INNER PRODUCTS: CONJUGATION
249
We mention in passing that Theorem 10.7.2 also remains valid for singular H;
in other words, for every pair of quaternion matrices H and A such that H = −H∗
and HA = −A∗H. Indeed, if H is singular, then applying a transformation
A, H
7→
S−1AS, S∗HS
(10.7.2)
for a suitable invertible matrix S, we may assume that H has the block form
H =
 H1
0
0
0

, where H1 is invertible and skewhermitian. Because of the equality
HA = −A∗H, we obtain that A has the conformally partitioned block form A =

A11
0
A21
A22

, where A11 is H1-Hamiltonan. Without loss of generality, we may
assume that N ⊇Ker H (otherwise replace N with N +Ker H); then the statement
of Theorem 10.7.2 for A and H is easily reduced to the situation where A and H
are replaced by A11 and H1, respectively.
The subspaces L± of Theorem 10.7.2 may have additional spectral properties.
Since A is an H-Hamiltonian matrix, the canonical form 10.5.2 shows that A is
similar to −A, and, therefore, the set of eigenvalues of A is symmetric relative to
negation: if λ ∈σ (A), then −λ ∈σ (A).
Deﬁnition 10.7.4. A set of eigenvalues S ∈H of A will be called a c-set if the
following four conditions are fulﬁlled:
(1) The eigenvalues in S all have nonzero real parts.
(2) If λ0 ∈S, then all quaternions similar to λ0 are also in S.
(3) If λ0 ∈S, then −λ0 ̸∈S.
(4) S is maximal (in the sense of sets containment) set of eigenvalues of A that
satisﬁes conditions (1), (2), and (3).
This terminology is borrowed from Gohberg et al. [52]. For example, the set
of all eigenvalues of A with positive real parts is a c-set. It may happen (when all
eigenvalues of A have zero real parts) that any c-set is empty.
Theorem 10.7.5. Under the hypotheses of Theorem 10.7.2, assume in addition
that the set S0 of eigenvalues with nonzero real parts of the restriction A|N is such
that
λ0 ∈S0
=⇒
−λ0 ̸∈S0.
Then for every c-set S such that S ⊇S0, there exist subspaces L± as in Theorem
10.7.2, with the additional property that S coincides with the set of eigenvalues with
nonzero real parts of A|L±.
The cases when N = {0} and/or when S0 = ∅are not excluded in Theorems
10.7.2 and 10.7.5.
The case when A is singular presents additional diﬃculty largely due to the fact
that the spectrum of A is symmetric with respect to negation, and zero is the ﬁxed
point of this symmetry.
Open Problem 10.7.6. Are the results of Theorem 10.7.2 and 10.7.5 valid
under the weaker hypothesis that N is A-invariant HA-neutral (rather than A-
invariant H-neutral)?

250
CHAPTER 10
As follows from Remark 10.7.1, the solution to Open Problem 10.7.6 is aﬃrma-
tive if A is invertible.
The proofs of Theorems 10.7.2 and 10.7.5 are given in the next section.
10.8
PROOFS OF THEOREMS 10.7.2 AND 10.7.5
We start with a lemma.
Lemma 10.8.1. Let Z ∈Hm×m be a hermitian matrix partitioned as follows:
Z =


0
0
Q∗
1
0
Q2
K∗
1
Q1
K1
K2

,
where the p × q block Q1 is right-invertible (thus, p ≤q). Then
In±(Z) + In0(Z) = q + In±(Q2) + In0(Q2).
(10.8.1)
Proof. Let Q[−1]
1
be a right inverse of Q1, and let
X = −K∗
1

Q[−1]
1
∗
,
Y = −1
2K∗
2

Q[−1]
1
∗
.
Then


I
0
0
X
I
0
Y
0
I

Z


I
X∗
Y ∗
0
I
0
0
0
I

=


0
0
Q∗
1
0
Q2
0
Q1
0
0

.
In turn, let Q1 = S [Ip 0] T, where S and T are invertible, be a rank decomposition
of Q1 (Proposition 3.2.5), then

T ∗
0
0
S
 

0
0
Ip
0
0
0(q−p)×p
Ip
0p×(q−p)
0



T
0
0
S∗

=

0
Q∗
1
Q1
0

.
Thus,
In±(Z) + In0(Z) = In±(Q2) + In0(Q2) + In±

0
Q∗
1
Q1
0

+ In0

0
Q∗
1
Q1
0

= In±(Q2) + In0(Q2)
+ In±


0
0
Ip
0
0
0(q−p)×p
Ip
0p×(q−p)
0

+ In0


0
0
Ip
0
0
0(q−p)×p
Ip
0p×(q−p)
0

,
and
In±


0
0
Ip
0
0
0(q−p)×p
Ip
0p×(q−p)
0

+ In0


0
0
Ip
0
0
0(q−p)×p
Ip
0p×(q−p)
0


is easily seen to be equal to q.
□

INDEFINITE INNER PRODUCTS: CONJUGATION
251
Proof of Theorems 10.7.2 and 10.7.5. We prove these results only for HA-
nonnegative subspaces (for nonpositive subspaces the proof is analogous, or else use
−H in place of H).
The canonical form of the pair of matrices (A, H) (with invertible H) under the
transformations (10.7.2) given in Theorem 10.5.1 allows us to reduce the proofs to
separate consideration of two cases: (1) A is invertible; (2) A is nilpotent.
Assume ﬁrst that A is invertible. Let bH = HA. Then bH is hermitian and invert-
ible, and A is bH-skewhermitian: bHA = −A∗bH. The subspace N is easily seen to
be bH-neutral. By Theorem 10.2.1 and Remark 10.2.2, there exist bH-nonnegative,
resp. bH-nonpositive, subspaces which are A-invariant and have the required prop-
erties of dimension and location of the spectrum as required in Theorems 10.7.2
and 10.7.5.
Thus, it remains to prove Theorems 10.7.2 and 10.7.5 for nilpotent A. Note that
in this case both S0 and S of Theorem 10.7.5 are vacuous, and in fact Theorem
10.7.5 coincides with Theorem 10.7.2. Therefore, we will prove Theorem 10.7.2
assuming A is nilpotent.
Consider the subspace
N [⊥] := {x ∈Hn×1 |x∗Hy = 0
for all
y ∈N},
the H-orthogonal companion of N. As N is H-neutral, we have N ⊆N [⊥]. Since
A is H-Hamiltonian and N is A-invariant, the subspace N [⊥] is easily seen to be A-
invariant as well. Assuming N ̸= N [⊥], choose an (ordered) euclidean orthonormal
basis
(y1, . . . , yn)
(10.8.2)
in Hn×1 so that the ﬁrst vectors in (10.8.2) form a basis of N, the next vectors in
(10.8.2) form a basis of the euclidean orthogonal complement of N in N [⊥], and the
remaining vectors in (10.8.2) form a basis of the euclidean orthogonal complement
of N [⊥] in Hn×1. With respect to the basis (10.8.2), A has a block form
A =


A11
A12
A22
0
A22
A23
0
0
A33

,
and the corresponding representation of H is
H = [y∗
i Hyj]n
i,j=1 =


0
0
H13
0
H22
H23
−H∗
13
−H∗
23
H33

.
The matrix H22 is skewhermitian and A22 is H22-Hamiltonian and nilpotent.
If dim (Ker (A22)) ≥2, then by Lemma 4.4.2 there exists a nonzero x0 ∈
Ker (A22) which is H22-neutral.
Then the subspace N + SpanH {x0} is clearly
A-invariant and H-neutral. If dim (Ker (A22)) = 1 but A22 ̸= 0, then the canonical
form (10.5.1), (10.5.2) shows that we may suppose that
A22 = Jℓ(0),
H22 = ±Ξℓ(iℓ),
where ℓ≥2. Then the subspace N + SpanH {x1} with x1 an eigenvector of A22, is
A-invariant and H-neutral.

252
CHAPTER 10
We repeat the above procedure with N replaced with N + SpanH {x0} or N +
SpanH {x1} as the case may be. Eventually, we reduce the proof to the case when
N = N [⊥]
or
dimH N = dimH N [⊥] −1.
(10.8.3)
Case 10.8.2. Assume N = N [⊥].
Since
dim N = n −dim N [⊥]
(this equality holds because N [⊥] coincides with the euclidean orthogonal comple-
ment to the dim N-dimensional subspace H(N)), we have
dim N = dim N [⊥] = n
2 ,
(10.8.4)
and n is necessarily even.
Choosing a euclidean orthogonal basis in Hn×1 such that the ﬁrst part of its
elements form a basis in N, we represent A and H in the form
A =

B11
B12
0
B22

,
Bij ∈Hn/2×n/2,
H =

0
H1
−H∗
1
H2

.
(10.8.5)
Here
N = SpanH {e1, . . . , en/2},
the matrix H1 is invertible (because H is so), and H2 is skewhermitian. Applying
a transformation (10.7.2) with S =

I
W1
0
W2

, for suitable W1 and W2, we may
(and do) assume that in fact
H =

0
I
−I
0

.
(10.8.6)
Then, since A is H-Hamiltonian, we have
A =

B11
B12
0
−B∗
11

,
B12
hermitian.
(10.8.7)
Next, with A and H given by (10.8.6) and (10.8.7), we apply a transformation
(10.7.2) with S of the form S =
 U
0
0
V

, where the invertible matrices U and V
are chosen so that U ∗V = I and
U −1B11U =

0r×r
0
C1
C2

,
r = dim (Ker B11) .
The matrix [C1
C2] is clearly right-invertible, the matrix H given by (10.8.6) is
ﬁxed under this transformation, whereas the transformed matrix A (which will be
again denoted by A) is of the form
A =


0
0
D1
D2
C1
C2
D3
D4
0
0
0
−C∗
1
0
0
0
−C∗
2

.

INDEFINITE INNER PRODUCTS: CONJUGATION
253
Thus,
HA =


0
0
0
−C∗
1
0
0
0
−C∗
2
0
0
−D1
−D2
−C1
−C2
−D3
−D4

.
Since HA is hermitian, we have D1 = D∗
1, D4 = D∗
4, D3 = D∗
2. Let M+ be a
maximal (−D1)-nonnegative subspace, and let
L+ = N +


0n/2×1
M+
0(n/2−r)×1

.
By Lemma 10.8.1,
dim L+ = In+ (HA) + In0 (HA).
Also, L+ is clearly HA-nonnegative and A-invariant. This concludes the proof of
Theorems 10.7.2 and 10.7.5 in Case 10.8.2.
Case 10.8.3. Assume dimH N = dimH N [⊥] −1.
Arguing as in Case 10.8.2, we see that n is odd and dimH N = (n −1)/2.
Choosing a euclidean orthogonal basis in Hn×1 such that the ﬁrst (n −1)/2 of its
elements form a basis in N and the ﬁrst (n + 1)/2 of its elements form a basis in
N [⊥], we represent A and H in the form
A
=


B11
B12
B13
0
01×1
B23
0
0
B33

,
where B11, B33 ∈H(n−1)/2×(n−1)/2,
H
=


0
0
H13
0
H22
H23
−H∗
13
−H∗
23
H33

,
where H22 = −H∗
22 ∈H \ {0}.
Here
N = SpanH {e1, . . . , e(n−1)/2},
N [⊥] = SpanH {e1, . . . , e(n+1)/2},
and the matrix H13 is invertible. Applying the transformations as in Case 10.8.2,
we suppose that the matrices A and H have the following form, conformally parti-
tioned:
A
=


0
0
D1
D2
D3
C1
C2
D4
D5
D6
0
0
01×1
D7
D8
0
0
0
0
−C∗
1
0
0
0
0
−C∗
2


,
H
=


0
0
0
I
0
0
0
0
0
I
0
0
H22
H231
H232
−I
0
−H∗
231
0
0
0
−I
−H∗
232
0
0


.

254
CHAPTER 10
Here [C1 C2] ∈Hq×(n−1)/2 is right-invertible (q := rank B1,1). Due to HA being
hermitian, we have −D1 = (H22D7)∗, and
HA =


0
0
0
0
−C∗
1
0
0
0
0
−C∗
2
0
0
0
H22D7
−D∗
4
0
0
(H22D7)∗
G
⋆
−C1
−C2
−D4
⋆
⋆


,
where G = G∗and where we denote by ⋆block entries of no immediate interest.
Let
eA =
 01×1
D7
0
0

,
eH =

01×1
H22D7
(H22D7)∗
G

.
If M is an eA-invariant maximal eH-nonnegative subspace, then, in view of Lemma
10.8.1, the subspace N +


0(n−1)/2×1
M
0q×1

is A-invariant and maximal HA-nonnega-
tive, and we are done.
So it remains to prove existence of an eA-invariant maximal eH-nonnegative sub-
space. If D7 = 0, then any maximal eH-nonnegative subspace will do. If D7 ̸= 0,
then for some invertible S we have D7S = [1 0
. . .
0]. We may replace eA and eH
by
bA :=

1
0
0
S−1

eA

1
0
0
S

=


01×1
1
0
0
01×1
0
0
0
0


and
bH :=
 1
0
0
S∗

eH
 1
0
0
S

=


01×1
H22
0
H∗
22
G11
G∗
21
0
G21
G22

,
where
 G11
G∗
21
G21
G22

= S∗GS, respectively. Let M0 be a maximal G22-nonnegative
subspace; then
M := SpanH {e1} +

02×1
M0

is clearly bA-invariant and bH-nonnegative. We show that M is actually maximal
bH-nonnegative.
Observe the equality


1
0
0
0
1
0
−G21H−1
22
0
I

bH


1
0
−(H∗
22)−1G∗
21
0
1
0
0
0
I

=


0
H22
0
H∗
22
G11
0
0
0
G22

.
Therefore (using the inertia theorem, and cf. (4.2.5)),
In+ ( bH) + In0 ( bH)
=
In+

0
H22
H∗
22
G11

+ In0

0
H22
H∗
22
G11

+In+ (G22) + In0 (G22).

INDEFINITE INNER PRODUCTS: CONJUGATION
255
But it is easily seen that
In+

0
H22
H∗
22
G11

= 1,
In0

0
H22
H∗
22
G11

= 0,
so
In+ ( bH) + In0 ( bH) = In+ (G22) + In0 (G22) + 1 = dim M
(where we have used Theorem 4.2.6(a)), and our claim follows.
10.9
DIFFERENTIAL EQUATIONS II
Consider the system of diﬀerential equations with constant coeﬃcients
Aℓx(ℓ)(t) + Aℓ−1x(ℓ−1)(t) + · · · + A1x′(t) + A0x(t) = 0,
t ∈R,
(10.9.1)
where Aℓ, . . . , A1, A0 ∈Hn×n, and x(t) is an unknown ℓtimes continuously dif-
ferentiable Hn×1-valued function of the real independent variable t. We assume in
addition that Ak is skewhermitian if k is odd, Ak is hermitian if k is even, and Aℓ
is invertible.
The treatment of (10.9.1) is parallel to that of (10.4.1) in Section 10.4. Let-
ting C and G be deﬁned by (10.4.2) and (10.4.3), respectively, we see that G is
skewhermitian and invertible and C is G-Hamiltonian.
The analogue of Theorem 10.4.2 holds true.
Theorem 10.9.1. The following four statements are equivalent for the system
(10.9.1):
(a) The system is forward bounded.
(b) The system is backward bounded.
(c) The system is bounded.
(d) All eigenvalues of C have zero real parts, and for every eigenvalue the geo-
metric multiplicity coincides with the algebraic multiplicity.
The proof is essentially the same as that of Theorem 10.4.2, using the canonical
form of Theorem 10.5.1.
Stable boundedness of a system of diﬀerential equation (10.9.1) is deﬁned anal-
ogously to that of (10.4.1).
Deﬁnition 10.9.2. System (10.9.1) is said to be stably bounded if there exists
ε > 0 such that every system of diﬀerential equations
A′
ℓx(ℓ)(t) + A′
ℓ−1x(ℓ−1)(t) + · · · + A′
1x′(t) + A′
0x(t) = 0,
t ∈R,
(10.9.2)
is bounded, provided the coeﬃcients A′
j ∈Hn×n, j = 0, 1, . . . , ℓ, satisfy the following
conditions:
(1) If k is odd, then A′
k is skewhermitian.
(2) If k is even, then A′
k is hermitian.

256
CHAPTER 10
(3) The inequalities
∥A′
j −Aj∥< ε,
for j = 0, 1, 2, . . . , ℓ,
hold true.
Theorem 10.9.3. Assume that every root subspace of the companion matrix C
is either GC-positive or GC-negative (note that GC is a hermitian matrix). Then
system (10.9.1) is stably bounded.
The proof is essentially the same as that of Theorem 10.4.4.
Note that the
hypotheses of Theorem 10.9.3 imply, in particular, that C is invertible and has
only eigenvalues with zero real parts (cf. the canonical form of Theorem 10.5.1).
In contrast with Theorem 10.4.4, the condition of Theorem 10.9.3 is not nec-
essary for stable boundedness, as the scalar example A1 = i, A0 = 0 shows. The
scalar equation ix′ = 0 is obviously stably bounded, but A−1
1 A0 = 0 is not positive
or negative deﬁnite. However, the question remains for invertible matrices C.
Open Problem 10.9.4. Suppose the system (10.9.1) is stably bounded, and
assume in addition that the companion matrix C is invertible. Does it follow that
every root subspace of C is either GC-positive or GC-negative?
The answer is aﬃrmative for ﬁrst order systems.
Theorem 10.9.5. The system of ﬁrst order diﬀerential equations
A1x′(t) + A0x(t) = 0,
(10.9.3)
where
A1 = −A∗
1 ∈Hn×n,
A0 = A∗
0 ∈Hn×n,
and A1, A0 are invertible, is stably bounded if and only if every root subspace of
A−1
1 A0 is either A0-positive or A0-negative.
Proof. The “if” part is contained in Theorem 10.9.3. To prove the “only if”
part, assume that there is a root subspace of A−1
1 A0 which is not A1-positive or
A1-negative, and we will show that the system (10.9.3) is not stably bounded. In
view of Theorem 10.9.1 we may assume that all eigenvalues of A−1
1 A0 have zero real
parts, and for every eigenvalue of A−1
1 A0 its geometric multiplicity coincides with
the algebraic multiplicity. Using the canonical form of Theorem 10.5.1, we further
assume that A1 and A−1
1 A0 have the form
A1 = ⊕q
u=1ζui,
A−1
1 A0 = ⊕q
u=1γu,
where γu’s are complex numbers with zero real parts and positive imaginary parts
(see remark (3) after Theorem 10.5.1). The condition that there is a root subspace of
A−1
1 A0 which is not A1-positive or A1-negative means that γu1 = γu2 and ζu1 ̸= ζu2
for some indices u1 and u2. For notational simplicity suppose that γ1 = γ2 = ia,
where a > 0, and ζ1 = 1 = −ζ2. Let y be a nonzero real number, and let
eA0 :=
 −a
y
y
a

⊕ζ3iγ3 ⊕· · · ⊕ζqiγq.
Then A−1
1
eA0 has eigenvalues −ia±y with nonzero real parts, and limy →0 eA0 = A0.
By Theorem 10.9.1, the system A1x′(t)+ eA0x(t) = 0 is not bounded; hence, (10.9.3)
is not stably bounded.
□

INDEFINITE INNER PRODUCTS: CONJUGATION
257
10.10
EXERCISES
Ex.
10.10.1. In Exs.
10.10.1, 10.10.2, and 10.10.3, H ∈Hn×n is assumed
to be hermitian and invertible.
Give examples of (H,∗)-hermitian and (H,∗)-
skewhermitian matrices for which there does not exist an A-invariant H-positive
(or H-negative) subspace of dimension In+ (H) (or In−(H)).
Ex. 10.10.2. Show that if A is (H,∗)-hermitian and has only real eigenvalues
with algebraic multiplicity equal to the geometric multiplicity for every eigenvalue,
then every A-invariant H-positive (or H-negative) subspace admits extension to an
A-invariant H-positive (or H-negative) subspace of dimension In+ (H) (or In−(H)).
Ex. 10.10.3. State and prove the statement analogous to Ex. 10.10.2 for (H,∗)-
skewhermitian matrices.
Ex. 10.10.4. For Gε deﬁned by (10.4.2), verify that the inertia of G0 is given
by
(In+ (G0), In−(G0) =

































nℓ
2 , nℓ
2

if ℓis even,
n(ℓ−1)
2
+ In−(Aℓ), n(ℓ−1)
2
+ In+ (Aℓ)

if ℓis odd, ℓ−1
2
is odd,
n(ℓ−1)
2
+ In+ (Aℓ), n(ℓ−1)
2
+ In−(Aℓ)

if ℓis odd, ℓ−1
2
is even.
(10.10.1)
Ex. 10.10.5. Prove, with full detail, Proposition 10.3.2 for the case of skewher-
mitian G.
Ex. 10.10.6. Provide details for the proof of Theorem 10.3.3, Part (b).
Ex. 10.10.7. Let H ∈Hn×n be hermitian of balanced inertia: n is even and
In+ H = In−H = n/2. Show that if A is (H,∗)-hermitian and has at most one
real eigenvalue (perhaps of high multiplicity), then there exists an A-invariant H-
Lagrangian subspace.
Ex. 10.10.8. Let H be as in Ex. 10.10.7. Show that if A is (H,∗)-skewhermitian
and has at most one similarity class of eigenvalues with zero real parts (perhaps of
high multiplicity), then there exists an A-invariant H-Lagrangian subspace.
Ex. 10.10.9. Show that a matrix A ∈Hn×n is (H,∗)-hermitian for some inert-
ible hermitian matrix H if and only if A is similar (over H) to a real matrix.
Ex. 10.10.10. Prove that the following statements are equivalent for A ∈Hn×n:
(a) A is (H,∗)-skewhermitian for some invertible hermitian matrix H;
(b) A is (G,∗)-Hamiltonian for some invertible skewhermitian matrix G;
(c) for every pair (λ, −λ) of eigenvalues of A with nonzero real parts, the partial
multiplicities corresponding to the eigenvalues similar to λ coincide with the
partial multiplicities corresponding to the eigenvalues similar to −λ;

258
CHAPTER 10
(d) A is similar to −A.
Hint: Use the Jordan form of A and the canonical forms of Theorems 10.1.3 and
10.5.1.
Ex. 10.10.11. Show that every matrix A ∈Hn×n is (H,∗)-skew-Hamiltonian
for some skewhermitian invertible matrix H.
Ex. 10.10.12. Consider three matrix pairs:
(a)
A1 =
 λ1
α
0
λ2

,
H1 =
 0
1
1
0

,
λ1, λ2, α ∈H;
(b)
A2 =
 λ1
0
0
λ2

,
H2 =
 1
0
0
−1

,
λ1, λ2 ∈H;
(c)
A3 =


λ1
α1
α2
0
λ2
α3
0
0
λ3

,
H3 =


0
0
1
0
1
0
1
0
0

,
where λj, αj ∈H for j = 1, 2, 3. When is Aj (Hj,∗)-hermitian for j = 1, 2, 3? If it
is, ﬁnd the canonical form of the pair (Aj, Hj).
Ex. 10.10.13. Find the sign characteristic for the pairs in Ex. 10.10.12, assum-
ing that Aj is (Hj,∗)-hermitian.
Ex. 10.10.14. Determine which matrices Aj (j = 1, 2, 3) given in Ex. 10.10.12
are (H,∗)-skewhermitian, and in case they are, ﬁnd the canonical form of the pair
(Aj, Hj).
Ex. 10.10.15. Describe the possible canonical forms of Theorem 10.1.1 if it is
known that the matrix H:
(1) is positive deﬁnite;
(2) has only one negative eigenvalue (counted with multiplicities);
(3) has only two negative eigenvalues (counted with multiplicities).
Ex. 10.10.16. Repeat Ex. 10.10.15, but now with regard to the canonical forms
for (H,∗)-skewhermitian matrices given in Theorem 10.1.3.
Ex. 10.10.17. Describe the possible canonical forms of (H,∗)-Hamiltonian ma-
trices (Theorem 10.5.1) in each of the following cases:
(1) The hermitian matrix HA is positive semideﬁnite.
(2) HA is positive deﬁnite.
(3) HA has only one negative eigenvalue (counted with multiplicities).
Ex. 10.10.18. Show that under the hypotheses of Theorem 10.9.3, every system
(10.9.2) is stably bounded, provided conditions (1), (2), (3) in Section 10.9 are
satisﬁed and ε is suﬃciently small.
Ex. 10.10.19. Let H = −H∗∈Hn×n be invertible, and let A, A′ be two simi-
lar (H,∗)-skew-Hamiltonian matrices. Show that there exists an (H,∗)-symplectic
matrix S such that S−1A′S = A.

INDEFINITE INNER PRODUCTS: CONJUGATION
259
Ex. 10.10.20. Let H be as in Ex. 10.10.19.
(a) Show that the result analogous to Ex. 10.10.19 does not generally hold for
(H,∗)-Hamiltonian matrices.
(b) Find a condition on the Jordan structure of an (H,∗)-Hamiltonian matrix
A that would ensure that for every (H,∗)-Hamiltonian matrix A′ which is similar
to A there exists an (H,∗)-symplectic matrix S such that S−1A′S = A.
Ex. 10.10.21. Let H ∈Hn×n be hermitian and invertible. Is the result anal-
ogous to Ex. 10.10.19 valid for (H,∗)-hermitian matrices? (H,∗)-skewhermitian
matrices? If not, ﬁnd conditions on the Jordan structure of an (H,∗)-hermitian,
resp. (H,∗)-skewhermitian, matrix A that would guarantee that for every (H,∗)-
hermitian, resp. (H,∗)-skewhermitian, matrix A′ which is similar to A, the simi-
larity matrix between A and A′ can be chosen to be (H,∗)-unitary.
Ex. 10.10.22. The system of diﬀerential equations (10.4.1) is said to be poly-
nomially bounded if there exist positive integer m and M1, M2 > 0 such that
∥x(t)∥≤M1|t|m+M2 for every solution x(t) of (10.4.1). State and prove a criterion
for polynomial boundedness in terms of the companion matrix.
Ex. 10.10.23. Repeat Ex. 10.10.22 for the system (10.9.1).
10.11
NOTES
Selfadjoint matrices and operators with respect to real or complex indeﬁnite inner
product have been extensively studied, both in the ﬁnite dimensional and the inﬁ-
nite dimensional settings, and the literature here is voluminous. Among numerous
books and papers on the subject, we mention here only Gohberg et al. [52, 53],
which are dedicated to the ﬁnite dimensional aspects of the theory.
The main results of this chapter (Theorems 10.1.1, 10.1.3, 10.5.1, 10.5.2) are
well known in the literature; see, e.g., Djokovi´c et al. [34]. For Theorem 10.1.1,
see also Karow [78] and Sergeichuk [142]. For comparison with the formulation in
Djokovi´c et al., note that the case we are interested in corresponds to the choice
D = H, ϵ = 1, ρ = −1, with A replaced by A∗, in Djocovi´c et al. Also note that
Djokovi´c et al. work with lower triangular rather than upper triangular Jordan
blocks.
Canonical forms for real and complex Hamiltonian matrices are found in many
sources: Burgoyne and Cushman [23], Faßbender et al.
[41], Lin et al.
[101],
Mehrmann and Xu [113], and Mehl et al. [109].
The presentation of Theorems 10.5.1 and 10.5.2 here is based on Rodman [135].
In the context of real and complex matrices, invariant Lagrangian subspaces
have been the subject of intensive studies. We refer the reader to Freiling et al.
[46], Mehl et al. [107], and Ran and Rodman [125, 126] for more information and
in-depth treatment of real and complex invariant Lagrangian subspaces.
For real matrices, Theorems 10.7.2 and 10.7.5 were proved in Rodman [129, 130].
In connection with Theorem 10.7.2, note that invariant neutral subspaces (under
the additional assumption that H is invertible) have been studied in Lancaster and
Rodman [89].
Formulas similar to (10.8.1) were given in Theorem 2.1 of Alpay and Dym [3].
Ex. 10.10.12 and 10.10.13 are taken from Gohberg et al. [53], where they are
stated in the context of complex matrices.

260
CHAPTER 10
The exposition in Section 10.8 follows, for the most part, Rodman [130], where
it was carried out for real matrices.

Chapter Eleven
Matrix pencils with symmetries: Nonstandard involution
In this chapter the subject matter involves quaternion matrix pencils or, equiva-
lently, pairs of quaternion matrices, with symmetries with respect to a ﬁxed non-
standard involution φ. Here, we provide canonical forms for φ-hermitian pencils,
i.e., pencils of the form A + tB, where A and B are both φ-hermitian. We also
provide canonical forms for φ-skewhermitian pencils. The canonical forms in ques-
tion are with respect to either strict equivalence of pencils or to simultaneous φ-
congruence of matrices. Applications are made to joint φ-numerical ranges of two
φ-skewhermitian matrices and to the corresponding joint φ-numerical cones.
We ﬁx a nonstandard involution φ throughout this chapter and a quaternion
β(φ) such that φ(β(φ)) = −β(φ) and |β(φ)| = 1.
11.1
CANONICAL FORMS FOR φ-HERMITIAN PENCILS
Consider a matrix pencil A + tB, A, B ∈Hn×n.
Deﬁnition 11.1.1. The matrix pencil A+tB is said to be φ-hermitian if Aφ = A
and Bφ = B.
Deﬁnition 11.1.2. Two matrix pencils A + tB and A′ + tB′ are said to be
φ-congruent if
A′ + tB′ = Sφ(A + tB)S
for some invertible S ∈Hn×n.
Clearly, the φ-congruence of matrix pencils is an equivalence relation, and
φ-congruent matrix pencils are strictly equivalent. However, in general, strictly
equivalent matrix pencils need not be φ-congruent. Moreover, any pencil that is
φ-congruent to a φ-hermitian matrix pencil is itself φ-hermitian.
We now present the canonical forms for φ-hermitian matrix pencils under strict
equivalence and φ-congruence. It turns out that these forms are the same.
Theorem 11.1.3. (a) Every φ-hermitian matrix pencil A + tB is strictly equiv-
alent to a φ-hermitian matrix pencil of the form
0u
⊕

t


0
0
Fε1
0
0
0
Fε1
0
0

+ G2ε1+1

⊕· · · ⊕

t


0
0
Fεp
0
0
0
Fεp
0
0

+ G2εp+1


⊕(Fk1 + tGk1) ⊕· · · ⊕(Fkr + tGkr)
⊕((t + α1) Fℓ1 + Gℓ1) ⊕· · · ⊕
 (t + αq) Fℓq + Gℓq

.
(11.1.1)
Here, ε1 ≤· · · ≤εp and k1 ≤· · · ≤kr are positive integers, and αj ∈Inv (φ).
The form (11.1.1) is uniquely determined by A and B up to an arbitrary per-
mutation of the diagonal blocks in the part
((t + α1) Fℓ1 + Gℓ1) ⊕· · · ⊕
 (t + αq) Fℓq + Gℓq

(11.1.2)

262
CHAPTER 11
and up to replacement of αj in each block (t + αj) Fℓj + Gℓj with a quaternion
βj ∈Inv (φ) such that
R(αj) = R(βj)
and
|V(αj)| = |V(βj)|.
(11.1.3)
(b) Every φ-hermitian matrix pencil A + tB is φ-congruent to a φ-hermitian
matrix pencil of the form (11.1.1), with the same uniqueness conditions as in the
part (a).
(c) φ-hermitian matrix pencils are φ-congruent if and only if they are strictly
equivalent.
Note that in (11.1.1) one can replace each Gv by eGv using the formula eGv =
FvGvFv.
Proof. We prove Theorem 11.1.3 by reduction to Theorem 9.1.1.
Proof of (a). Let A + tB be a φ-hermitian matrix pencil and let bA = −β(φ)A,
bB = −β(φ)B.
By Proposition 4.1.7 the pencil bA + t bB is skewhermitian.
By
Theorem 9.1.1, there exist invertible matrices S and T such that
S∗( bA + t bB)T = W,
where W is the canonical from (9.1.1) and where we replace i with −β(φ) (see
Remark 9.1.2). Then S∗(A + tB)T = β(φ)W, again by Proposition 4.1.7, and it is
easily seen that β(φ)W is strictly equivalent to (11.1.1) (with Gv replaced by eGv).
The uniqueness statement in (a) follows from that in Theorem 9.1.1(i).
Part (b) obviously follows from (a) and (c), so it remains to prove (c). Let
A + tB and A1 + tB1 be two strictly equivalent φ-hermitian n × n matrix pencils,
so
Sφ(A + tB)T = A1 + tB1,
S, T ∈Hn×n
are invertible.
Letting
bA = −β(φ)A,
bB = −β(φ)B,
bA1 = −β(φ)A1,
bB1 = −β(φ)B1,
by Proposition 4.1.7 we have
S∗( bA + t bB)T = bA1 + t bB1.
Now by Theorem 9.1.1 the skewhermitian matrix pencils bA + t bB and bA1 + t bB1 are
congruent, so
V ∗( bA + t bB)V = bA1 + t bB1,
V ∈Hn×n
is invertible.
Applying Proposition 4.1.7 once more, we get
Vφ(A + tB)V = A1 + tB1,
and (c) is proved.
□

MATRIX PENCILS WITH SYMMETRIES: NONSTANDARD INVOLUTION
263
11.2
CANONICAL FORMS FOR φ-SKEWHERMITIAN PENCILS
Deﬁnition 11.2.1. A matrix pencil A+tB is called φ-skewhermitian if Aφ = −A
and Bφ = −B.
In this section we consider φ-skewhermitian matrix pencils and their canonical
forms.
We start with the canonical form.
Theorem 11.2.2. Fix β(φ) ∈H such that φ(β(φ)) = −β(φ), |V(β(φ))| = 1.
(a) Every φ-skewhermitian matrix pencil A + tB is strictly equivalent to a φ-
skew- hermitian matrix pencil of the form
0u
⊕

t


0
0
Fε1
0
0
0
−Fε1
0
0

+


0
Fε1
0
−Fε1
0
0
0
0
0




⊕
· · · ⊕

t


0
0
Fεp
0
0
0
−Fεp
0
0

+


0
Fεp
0
−Fεp
0
0
0
0
0




⊕
(β(φ)Fk1 + tβ(φ)Gk1) ⊕· · · ⊕(β(φ)Fkr + tβ(φ)Gkr)
⊕
((t + γ1)β(φ)Fm1 + β(φ)Gm1) ⊕· · · ⊕
 (t + γp)β(φ)Fmp + β(φ)Gmp

⊕

(t + α1)

0
Fℓ1
−Fℓ1
0

+

0
Gℓ1
−Gℓ1
0

⊕
· · · ⊕

(t + αq)

0
Fℓq
−Fℓq
0

+

0
Gℓq
−Gℓq
0

.
(11.2.1)
Here, ε1 ≤· · · ≤εp and k1 ≤· · · ≤kr are positive integers, α1, . . . , αq ∈Inv (φ)\R,
and γ1, . . . , γp are real.
The form (11.2.1) is uniquely determined by A and B up to an arbitrary per-
mutation of the diagonal blocks in each of the parts
⊕p
j=1
 (t + γj)β(φ)Fmj + β(φ)Gmj

and
⊕q
j=1

(t + αj)

0
Fℓj
−Fℓj
0

+

0
Gℓj
−Gℓj
0

(11.2.2)
and up to replacement of αj in each block
(t + αj)

0
Fℓj
−Fℓj
0

+

0
Gℓj
−Gℓj
0

(11.2.3)
with a quaternion βj ∈Inv (φ), which is similar to αj.
(b) Every φ-skewhermitian matrix pencil A + tB is φ-congruent to a matrix

264
CHAPTER 11
pencil (which is also φ-skewhermitian) of the form
0u
⊕

t


0
0
Fε1
0
0
0
−Fε1
0
0

+


0
Fε1
0
−Fε1
0
0
0
0
0




⊕
· · · ⊕

t


0
0
Fεp
0
0
0
−Fεp
0
0

+


0
Fεp
0
−Fεp
0
0
0
0
0




⊕
δ1 (β(φ)Fk1 + tβ(φ)Gk1) ⊕· · · ⊕δr (β(φ)Fkr + tβ(φ)Gkr)
⊕
η1 ((t + γ1)β(φ)Fm1 + β(φ)Gm1)
⊕
· · · ⊕ηp
 (t + γp)β(φ)Fmp + β(φ)Gmp

⊕

(t + α1)

0
Fℓ1
−Fℓ1
0

+

0
Gℓ1
−Gℓ1
0

⊕
· · · ⊕

(t + αq)

0
Fℓq
−Fℓq
0

+

0
Gℓq
−Gℓq
0

.
(11.2.4)
Here εi, kj, αm and γs are as in Part (a), and δ1, . . . , δr and η1, . . . , ηp are signs
±1.
The form (11.2.4) under φ-congruence is uniquely determined by A and B up
to an arbitrary permutation of the diagonal blocks in each of the parts
⊕p
j=1
 ηj
 (t + γj)β(φ)Fmj + β(φ)Gmj

and (11.2.2) and up to replacement of αj in each block (11.2.3) with a similar
quaternion βj ∈Inv (φ).
In connection with the form (11.2.4), note the following φ-congruence:
 β(φ)Iε
0
0
Iε+1
 
t


0
0
Fε
0
0
0
−Fε
0
0

+


0
Fε
0
−Fε
0
0
0
0
0




·
 −β(φ)Iε
0
0
Iε+1

= β(φ)

t


0
0
Fε
0
0
0
Fε
0
0

+


0
Fε
0
Fε
0
0
0
0
0



.
(11.2.5)
The signs δi and ηj of (11.2.4) form the sign characteristic of the φ-skewhermitian
pencil A + tB. In view of the equalities
(β(φ)Fki + tβ(φ)Gki)(−β(φ)Fki) = I + tJki(0)
(11.2.6)
and
(tβ(φ)Fmj + γjβ(φ)Fmj + β(φ)Gmj)(−β(φ)Fmj)
=
tI + Jmj(γj),
γj ∈R,
(11.2.7)
the sign characteristic assigns a sign ±1 to every index corresponding to a real
eigenvalue of A + tB, as well as to every index at inﬁnity. The signs are uniquely
determined by A+tB, up to permutations of signs that correspond to equal indices
of the same real eigenvalue and permutations of signs that correspond to the equal
indices at inﬁnity.

MATRIX PENCILS WITH SYMMETRIES: NONSTANDARD INVOLUTION
265
The proof of this theorem will be given in the next section.
In contrast with φ-hermitian matrix pencils, strictly equivalent φ-skewhermi-
tian matrix pencils need not be φ-congruent.
Theorem 11.2.3. Let A + tB be a φ-skewhermitian n × n quaternion matrix
pencil. Then the strict equivalence class of A + tB consists of not more than 2n
φ-congruence classes. The upper bound 2n is attained for
A + tB = diag ((t + γ1)β(φ), (t + γ2)β(φ), . . . , (t + γn)β(φ)) ,
where γ1, . . . , γn are distinct real numbers.
Proof. Assume that A + tB is strictly equivalent to the form (11.2.1), and
assume that in the form (11.2.1) there are exactly s distinct blocks of the types
β(φ)Fki +tβ(φ)Gki and (t+γi)β(φ)Fmi +β(φ)Gmi. Denote these s distinct blocks
by K1, . . . , Ks, and further assume that the block Kj appears in (11.2.1) exactly
vj times, for j = 1, 2, . . . , s. Deﬁne the integer
q :=
s
Y
j=1
(vj + 1).
Theorem 11.2.2 shows that the strict equivalence class of A + tB contains exactly
q φ-congruence classes. So, the maximal number of φ-congruence classes in a strict
equivalence class of a φ-skewhermitian n × n quaternion matrix pencil is equal to
max{
s
Y
j=1
(vj + 1)},
where the maximum is taken over all tuples of positive integers (v1, . . . , vs) such
that v1 + · · · + vs ≤n. It is easy to see that the maximum is achieved for s = n
and v1 = · · · = vn = 1 (use the elementary inequality v + 1 < (v −u + 1)(u + 1) for
every pair of positive integers u, v such that 1 ≤u ≤v −1).
□
A criterion for φ-skewhermitian matrix pencils with the property that their
strict equivalence to other φ-skewhermitian matrix pencils implies φ-congruence is
given next.
Theorem 11.2.4. Let A+tB be a φ-skewhermitian matrix pencil. Assume that
the following property holds:
rank (A + tB) = rank A = rank B
for all t ∈R.
(11.2.8)
Then a φ-skewhermitian matrix pencil A′ + tB′ is φ-congruent to A + tB if and
only if A′ + tB′ is strictly equivalent to A + tB.
Conversely, if a φ-skewhermitian matrix pencil A + tB has the property that
every φ-skewhermitian matrix pencil that is strictly equivalent to A+tB is actually
φ-congruent to A + tB, then (11.2.8) holds.
The proof of this theorem follows easily from Theorem 11.2.2 by inspection of
(11.2.4). Indeed, property (11.2.8) is equivalent to the absence of blocks
δj(β(φ)Fkj + tβ(φ)Gkj)
and
ηj((t + γj)β(φ)Fmj + β(φ)Gmj)
in (11.2.4).

266
CHAPTER 11
11.3
PROOF OF THEOREM 11.2.2
Part (a) follows from Theorem 8.1.2 by using Proposition 4.1.7 (cf. the proof of
Theorem 11.1.3). For that purpose, we need to verify, apart from evident veriﬁca-
tions, that the matrix pencils
β(φ)W1(t) := β(φ)

0
(t + γ)Fm + Gm
(t + γ∗)Fm + Gm
0

,
γ ∈H \ R,
and
W2(t) :=

0
(t + α)Fm + Gm
(t + α)(−Fm) + (−Gm)
0

,
α ∈Inv (φ) \ R,
are strictly equivalent. This is easy:
 r−1
2
0
0
−r−1
1

W1(t)
 r1
0
0
r2

= W2(t),
where r1, r2 ∈H \ {0} are such that
r−1
1 γ∗r1 = r−1
2 γr2 = α.
Note that for every γ ∈H \ R, one can choose r1 so that r−1
1 γ∗r1 ∈Inv (φ) \ R.
Consider now the existence part of (b). We use again Proposition 4.1.7 and take
advantage of Theorem 8.1.2. Thus, we need only to verify that each constituent
primitive block in (8.1.6), when multiplied by β(φ) on the left, is φ-congruent to
a constituent primitive block in (11.2.4). For this purpose, we may assume that
all signs in (8.1.6) are +1’s. The required veriﬁcation is carried by the formulas
(11.2.5) and
 1
0
0
β(φ)

· β(φ)

0
(t + γ)Fm + Gm
(t + γ∗)Fm + Gm
0

·
 1
0
0
−β(φ)

=

0
(t + α)Fm + Gm
−(t + α)Fm −Gm
0

,
where γ is chosen in Inv (φ) \ R, and α = γ∗.
A proof of uniqueness in Part (b) can be also done using Proposition 4.1.7.
In what follows, we oﬀer a direct and independent proof of the uniqueness part
of Theorem 11.2.2(b). The proof is modeled after the proof of uniqueness of the
canonical form for pairs of complex Hermitian matrices (see, e.g., Section 8 in
Lancaster and Rodman [92, Section 8], or Thompson [151]).
We introduce the following terminology.
Deﬁnition 11.3.1. Let A1 + tB1 be a φ-skewhermitian pencil of the form
(11.2.4). Then we say that
δ1 (β(φ)Fk1 + tβ(φ)Gk1) ⊕· · · ⊕δr (β(φ)Fkr + tβ(φ)Gkr)
is the β(φ)-part of A1 + tB1, and for every real γ,
⊕{j:γj=γ}
 ηj
 (t + γj)β(φ)Fmj + β(φ)Gmj

is the γ-part of A1 + tB1.

MATRIX PENCILS WITH SYMMETRIES: NONSTANDARD INVOLUTION
267
We will write A1 + tB1 in the following form (perhaps, after permutation of
primitive blocks):
A1 + tB1
=
0u ⊕(A1,♭+ tB1,♭)
⊕(A1,♯+ tB1,♯) ⊕(A1,β(φ) + tB1,β(φ))
⊕⊕m
j=1 (A1,γj + tB1,γj),
(11.3.1)
where A1,β(φ) + tB1,β(φ) is the β(φ)-part of A1 + tB1, the block A1,γj + tB1,γj is
the γj-part of A1 + tB1 (the numbers γ1, . . . , γm are real, distinct, and arranged in
increasing order: γ1 < · · · < γm), the part A1,♭+ tB1,♭consists of a direct sum of
blocks
t


0
0
Fεi
0
0
0
−Fεi
0
0

+


0
Fεi
0
−Fεi
0
0
0
0
0

,
i = 1, 2, . . . , p,
and the part A1,♯+ tB1,♯consists of the blocks
(t + αi)

0
Fℓi
−Fℓi
0

+

0
Gℓi
−Gℓi
0

,
i = 1, 2, . . . , q,
where αi ∈Inv (φ) \ R.
Deﬁnition 11.3.2. We say that the form (11.3.1), with the indicated properties,
is an arranged form.
Lemma 11.3.3. Let two φ-skewhermitian matrix pencils A1 +tB1 and A2 +tB2
be given in the arranged forms (11.3.1) and
A2 + tB2
=
0u′ ⊕(A2,♭+ tB2,♭) ⊕(A2,♯+ tB2,♯) ⊕(A2,β(φ) + tB2,β(φ))
⊕⊕m′
j=1 (A2,γ′
j + tB2,γ′
j).
Assume that A1 + tB1 and A2 + tB2 are φ-congruent. Then
u = u′,
m = m′,
γj = γ′
j,
for j = 1, 2, . . . , m,
(11.3.2)
the φ-skewhermitian matrix pencils A1,♭+ tB1,♭and A2,♭+ tB2,♭are φ-congruent,
the φ-skewhermitian matrix pencils A1,♯+ tB1,♯and A2,♯+ tB2,♯are φ-congruent,
the β(φ)-parts A1,β(φ) + tB1,β(φ) and A2,β(φ) + tB2,β(φ) are φ-congruent, and for
each γj, the γj-parts A1,γj + tB1,γj and A2,γj + tB2,γj are φ-congruent.
Proof. Equalities (11.3.2) follow from the uniqueness of the Kronecker form of
A1 + tB1 (which is the same as the Kronecker form of A2 + tB2). For the same
reason, A1,♭+ tB1,♭is strictly equivalent to A2,♭+ tB2,♭, and, therefore, permuting
if necessary the blocks, we may (and will) assume that
A1,♭+ tB1,♭= A2,♭+ tB2,♭.
Analogously, we may assume that
A1,♯+ tB1,♯= A2,♯+ tB2,♯.
Also, the β(φ)-parts of A1 + tB1 and A2 + tB2 are strictly equivalent, as well as
the γj-parts of A1 + tB1 and A2 + tB2, for every ﬁxed γj.

268
CHAPTER 11
For uniformity of notation, we let
M0 + tN0 := 0u,
Mi + tNi
:=
t


0
0
Fεi
0
0
0
−Fεi
0
0

+


0
Fεi
0
−Fεi
0
0
0
0
0

,
i = 1, 2, . . . , p,
(11.3.3)
Mp+i + tNp+i
:=
⊕wi
s=1

(t + αi)

0
Fℓi,s
−Fℓi,s
0

+

0
Gℓi,s
−Gℓi,s
0

,
i = 1, 2, . . . , q,
(11.3.4)
where α1, . . . , αq ∈Inv (φ) \ R are mutually nonsimilar;
Mp+q+1 + tNp+q+1
:=
A1,β(φ) + tB1,β(φ);
M ′
p+q+1 + tN ′
p+q+1
:=
A2,β(φ) + tB2,β(φ);
Mp+q+1+j + tNp+q+1+j
:=
A1,γj + tB1,γj,
j = 1, 2, . . . , m;
M ′
p+q+1+j + tN ′
p+q+1+j
:=
A2,γj + tB2,γj,
j = 1, 2, . . . , m.
Let nj ×nj be the size of Mj +tNj, for j = 0, 1, . . . , p+q +1+m (note that nj ×nj
is also the size of M ′
j + tN ′
j, for j = p + q + 1, p + q + 2, . . . , p + q + 1 + m). Write
T

⊕p+q+1+m
j=0
(Mj + tNj)

=
 ⊕p+q
j=0 (Mj + tNj)

⊕⊕p+q+1+m
j=p+q+1
 M ′
j + tN ′
j

S,
(11.3.5)
S
=
T −1
φ ,
for some invertible T, and partition T and S conformally with (11.3.5):
T = [T (ij)]p+q+1+m
i,j=0
,
S = [S(ij)]p+q+1+m
i,j=0
,
where T (ij) and S(ij) are ni × nj. We then have, from (11.3.5):
T (ij)Mj = MiS(ij);
T (ij)Nj = NiS(ij)
(i, j = 0, . . . , p + q + 1 + m),
(11.3.6)
where in the right-hand sides, Mi and Ni are replaced by M ′
i and N ′
i, respectively,
for i = p + q + 1, p + q + 2, . . . , p + q + m + 1. In particular,
T (0j)Mj = 0,
T (0j)Nj = 0
(j = 0, . . . , p + q + 1 + m),
which immediately implies the equalities T (0j) = 0 for j = p + 1, . . . , p + q + 1 + m.
Since
Ran Mj + Ran Nj = Hnj,
(j = 1, . . . , p),
we also obtain T (0j) = 0 for j = 1, . . . , p. Therefore, in view of the equality S = T −1
φ ,
also S(i0) = 0 for i = 1, . . . , p + q + 1 + m. Now, clearly, the matrices
eT :=
h
T (ij)ip+q+1+m
i,j=1
and
eS :=
h
S(ij)ip+q+1+m
i,j=1
= ( eTφ)−1
(11.3.7)

MATRIX PENCILS WITH SYMMETRIES: NONSTANDARD INVOLUTION
269
are invertible, and we have
eT

⊕p+q+1+m
j=1
(Mj + tNj)

=

⊕p+q
j=1 (Mj + tNj) ⊕⊕p+q+1+m
j=p+q+1
 M ′
j + tN ′
j

eS.
(11.3.8)
Next, consider equalities (11.3.6) for i, j = p + 1, . . . , p + q + 1 + m and i ̸= j
(these hypotheses on i and j will be assumed throughout the present paragraph).
If i, j ̸= p + q + 1, then Ni and Nj are both invertible, and using (11.3.6) we obtain
T (ij)Mj = MiS(ij) = MiN −1
i
T (ij)Nj;
(11.3.9)
hence,
T (ij)MjN −1
j
= MiN −1
i
T (ij)
(if i > p + q + 1, then we use M ′
i and N ′
i in place of Mi and Ni, respectively, in the
right-hand side of (11.3.9)). A computation shows that
MjN −1
j
= ⊕wj−p
s=1
 Jℓj−p,s(αj−p) ⊕Jℓj−p,s(αj−p)

,
for
j = p + 1, . . . , p + q,
and MjN −1
j
= M ′
j(N ′
j)−1 is a Jordan matrix with eigenvalue γj−p−q−1 for j =
p + q + 2, . . . , p + q + 1 + m. By Theorem 5.11.1, we obtain
T (ij) = 0
for i, j ∈{p + 1, . . . , p + q, p + q + 2, . . . , p + q + 1 + m}, i ̸= j.
If i ̸= p + q + 1 but j = p + q + 1, then Ni and Mj are invertible, and (11.3.6) leads
to
T (ij)
=
MiS(ij)M −1
j
= MiN −1
i
T (ij)NjM −1
j
=
(MiN −1
i
)2T (ij)(NjM −1
j
)2
=
· · · = (MiN −1
i
)wT (ij)(NjM −1
j
)w = 0
(11.3.10)
for suﬃciently large positive integer w, because NjM −1
j
is easily seen to be nilpo-
tent. (If i > p + q + 1, then we replace Mi and Ni by M ′
i and N ′
i, respectively, in
(11.3.10).) Finally, if i = p + q + 1 and j ̸= p + q + 1, then, analogously, (11.3.6)
gives
T (ij)
=
N ′
iS(ij)N −1
j
= N ′
i(M ′
i)−1T (ij)MjN −1
j
=
(N ′
i(M ′
i)−1)2T (ij)(MjN −1
j
)2
=
· · · = (N ′
i(M ′
i)−1)wT (ij)(NjM −1
j
)w,
(11.3.11)
and since N ′
i(M ′
i)−1 is nilpotent, the equality T (ij) = 0 follows. Thus, T (ij) = 0
for i, j = p + 1, . . . , p + q + 1 + m and i ̸= j. Analogously, we obtain the equalities
S(ij) = 0 for i, j = p + 1, . . . , p + q + 1 + m and i ̸= j.
Now consider T (ji) and S(ij) for i = 1, . . . , p and j = p + 1, . . . , p + q + 1 + m
(these hypotheses on i and j will be assumed throughout the current paragraph).
Assume ﬁrst j = p + q + 1. Then (11.3.6) gives
T (i,p+q+1)Mp+q+1 = MiS(i,p+q+1),
T (i,p+q+1)Np+q+1 = NiS(i,p+q+1),

270
CHAPTER 11
and therefore
MiS(i,p+q+1)M −1
p+q+1Np+q+1 = NiS(i,p+q+1).
(11.3.12)
Using the form (11.3.3) of Mi and Ni, and equating the bottom rows in the left- and
right-hand sides of (11.3.12), we ﬁnd that the ﬁrst row of S(i,p+q+1) is zero. Then
consideration of the next to the bottom row of (11.3.12) implies that the second
row of S(i,p+q+1) is zero. Continuing in this fashion it is found that the top εi rows
of S(i,p+q+1) consist of zeros. Applying a similar argument to the equality
T (p+q+1,i)Ni = N ′
p+q+1((M ′
p+q+1)−1T (p+q+1,i)Mi),
it is found that the ﬁrst εi columns of T (p+q+1,i) consist of zeros. Now assume
j ̸= p + q + 1. Then (11.3.6) gives
T (ij)Mj = MiS(ij);
T (ij)Nj = NiS(ij),
and consequently
NiS(ij)N −1
j
Mj = MiS(ij).
(11.3.13)
In view of the form (11.3.3) of Mi and Ni, the (εi + 1)th row in the left-hand side
of (11.3.13) is zero.
But (εi + 1)th row in the right-hand side is the negative of
the εith row of S(ij).
So, the εith row of S(ij) is equal to zero. By considering
successively the (εi + 2)th, (εi + 3)th, etc., rows on both sides of (11.3.13), it is
found that the ﬁrst εi rows of S(ij) are zeros. Next, use the equalities
T (ji)Mi = MjS(ji),
T (ji)Ni = NjS(ji),
with Mj, Nj replaced by M ′
j, N ′
j, respectively, if j > p + q + 1, to obtain
T (ji)Mi = MjS(ji) = Mj(N −1
j
T (ji)Ni).
Arguing as above, it follows that the ﬁrst εi columns of T (ji) consist of zeros.
It has been shown that matrix eT has the following form:
eT =


T (11)
· · ·
T (1p)
...
...
...
T (p1)
· · ·
T (pp)
0np+1×ϵ1
∗
· · ·
0np+1×ϵp
∗
0np+2×ϵ1
∗
· · ·
0np+2×ϵp
∗
...
...
...
0np+q+1+m×ϵ1
∗
· · ·
0np+q+1+m×ϵp
∗


(p leftmost block columns), and
eT =


T (1,p+1)
T (1,p+2)
· · ·
T (1,p+q+1+m)
...
...
...
...
T (p,p+1)
T (p,p+2)
· · ·
T (p,p+q+1+m)
T (p+1,p+1)
0
· · ·
0
0
T (p+2,p+2)
· · ·
0
...
...
...
...
0
0
· · ·
T (p+q+1+m,p+q+1+m)



MATRIX PENCILS WITH SYMMETRIES: NONSTANDARD INVOLUTION
271
(q + 1 + m rightmost block columns). Let W = T −1, and partition conformally
with (11.3.1):
W =
h
W (ij)ip+q+1+m
i,j=0
,
f
W := eT −1 =
h
W (ij)ip+q+1+m
i,j=1
.
In view of (11.3.8) we have
f
W

⊕p+q
j=1 (Mj + tNj) ⊕⊕p+q+1+m
j=p+q+1
 M ′
j + tN ′
j

=

⊕p+q+1+m
j=1
(Mj + tNj)

eS−1.
(11.3.14)
A similar argument shows that for i = 1, . . . , p and j = p + 1, . . . , p + q + 1 + m,
the ﬁrst εi columns of W (ji) are zeros.
Also, (11.3.14) implies W (ij) = 0 for
i, j = p + 1, . . . , p + q + 1 + m and i ̸= j, which is proved exactly in the same way
as T (ij) = 0 (i, j = p + 1, . . . , p + q + 1 + m, i ̸= j) was proved using (11.3.8).
Denoting by f
W (ik) the matrix formed by the (εk + 1) rightmost columns of
W (ik) and by eS(kj) the matrix formed by the (εk + 1) bottom rows of S(kj) (here
i, j = p + 1, . . . , p + q + 1 + m; k = 1, . . . , p), we have
W (ik)(Mk + tNk)S(kj) =
h
0
f
W (ik)i  0εk
∗
∗
0εk+1
 
0
eS(kj)

= 0,
(11.3.15)
where the form (11.3.3) of Mk and Nk was used.
Hence, using the properties of W (ij) and S(ij) veriﬁed above, the following
equalities are obtained, where i, j ∈{p + 1, . . . , p + q + 1 + m}:
p+q
X
k=1
W (ik)(Mk + tNk)S(kj) +
p+q+1+m
X
k=p+q+1
W (ik) (M ′
k + tN ′
k) S(kj) = 0
if i ̸= j, and
p+q
X
k=1
W (ik)(Mk + tNk)S(kj)
+
p+q+1+m
X
k=p+q+1
W (ik) (M ′
k + tN ′
k) S(kj)
=
W (ii) (Mi + tNi) S(ii)
(11.3.16)
if i = j. (For i = p + q + 1, . . . , p + q + 1 + m, replace W (ii) (Mi + tNi) S(ii) with
W (ii) (M ′
i + tN ′
i) S(ii) in the right-hand side of (11.3.16).) We now obtain, by a
computation starting with (11.3.14), where (11.3.16) is used:
diag [Mi + tNi]p+q+1+m
i=p+q+1
=
"p+q
X
k=1
W (ik)(Mk + tNk)S(kj)
+
p+q+1+m
X
k=p+q+1
W (ik) (M ′
k + tN ′
k) S(kj)


p+q+1+m
i,j=p+q+1
=
diag
h
W (ii)ip+q+1+m
i=p+q+1 diag [M ′
i + tN ′
i]p+q+1+m
i=p+q+1 diag
h
S(ii)ip+q+1+m
i=p+q+1 .
(11.3.17)

272
CHAPTER 11
Note that the matrix diag [Mi + tNi]p+q+1+m
i=p+q+1
is clearly invertible for some real
value of t; hence, we see from (11.3.17) that the matrices W (ii) and S(ii) are in-
vertible (i = p + q + 1, . . . , p + q + 1 + m).
Since also eS = f
Wφ (see equality
(11.3.7)), the equality (11.3.17) shows that for every i = p+q +1, . . . , p+q +1+m,
the φ-skewhermitian matrix pencils Mi + tNi and M ′
i + tN ′
i are φ-congruent, as
required.
□
We now return to the proof of uniqueness in Theorem 11.2.2(b). Let A + tB
be a φ-skewhermitian matrix pencil that is congruent to two forms (11.2.4). The
uniqueness part of Theorem 11.2.2(a) guarantees that apart from permutations of
blocks, these two forms can possibly diﬀer only in the signs δj and ηk. Lemma
11.3.3 allows us to reduce the proof to cases when either only blocks of the form
δ1 (β(φ)Fk1 + tβ(φ)Gk1) ⊕· · · ⊕δr (β(φ)Fkr + tβ(φ)Gkr)
(11.3.18)
are present or only blocks of the form
η1 ((t + γ)β(φ)Fm1 + β(φ)Gm1)
⊕· · · ⊕ηp
 (t + γ)β(φ)Fmp + β(φ)Gmp

,
γ ∈R
(11.3.19)
are present. We now consider each of these two cases separately.
We start with the form (11.3.19).
So it is assumed that a φ-skewhermitian
matrix pencil A+tB is φ-congruent to (11.3.19), as well as to (11.3.19) with possibly
diﬀerent signs eηj. We have to prove that, in fact, the form
eη1 ((t + γ)β(φ)Fm1 + β(φ)Gm1)
⊕· · · ⊕eηp
 (t + γ)β(φ)Fmp + β(φ)Gmp

,
(11.3.20)
is obtained from (11.3.19) after a permutation of blocks. Write
T
 ⊕p
j=1ηj
 (t + γ)β(φ)Fmj + β(φ)Gmj

=
 ⊕p
j=1eηj
 (t + γ)β(φ)Fmj + β(φ)Gmj

S,
(11.3.21)
where T is an invertible quaternion matrix and S = (Tφ)−1, and partition
T = [Tij]p
i,j=1 ,
S = [Sij]p
i,j=1 ,
where Tij and Sij are mi × mj. Then
Tij
 ηjβ(φ)Fmj

=
(eηiβ(φ)Fmi)Sij,
Tij
 ηj
 γβ(φ)Fmj + β(φ)Gmj

=
(eηi (γβ(φ)Fmi + β(φ)Gmi)) Sij,
for i, j = 1, 2, . . . , p; therefore,
Tij(ηj(γβ(φ)Fmj + β(φ)Gmj))
= (eηi(γβ(φ)Fmi + β(φ)Gmi))eηi(β(φ))−1FmiTij(ηjβ(φ)Fmj).
(11.3.22)
Equality (11.3.22) implies
TijVj = ViTij,
(11.3.23)

MATRIX PENCILS WITH SYMMETRIES: NONSTANDARD INVOLUTION
273
where
Vi = (γFmi + Gmi) F −1
mi = Jmi(γ),
the mi × mi Jordan block with eigenvalue γ. Proposition 5.4.2 shows that Tij has
the form
Tij =
h
0
eTij
i
(if mi ≤mj)
(11.3.24)
or
Tij =
 eTij
0

(if mi ≥mj),
(11.3.25)
where eTij is an upper triangular Toeplitz matrix of size min(mi, mj)×min(mi, mj).
Permuting blocks in (11.3.19) if necessary, it can be assumed that the sizes mj are
arranged in nondecreasing order. Let u < v be indices such that
mi < mu+1 = mu+2 = · · · = mv < mj
for all i ≤u and for all j > v.
Now for u < i ≤v, u < k ≤v, in view of (11.3.21) we obtain the following equality,
where δik is the Kronecker symbol, i.e., δik = 1 if i = k and δik = 0 if i ̸= k:
δikeηiβ(φ)Fmi
=
p
X
j=1
Tij
 ηjβ(φ)Fmj

(Tkj)φ =
u
X
j=1
Tij(ηjβ(φ)Fmj)(Tkj)φ
+
v
X
j=u+1
Tij(ηjβ(φ)Fmj)(Tkj)φ +
p
X
j=v+1
Tij(ηjβ(φ)Fmj)(Tkj)φ.
(11.3.26)
In view of (11.3.25) the lower-left corner in the ﬁrst sum in the right-hand side of
(11.3.26) is zero. Using (11.3.24), it is easily veriﬁed that the lower-left corner in
the third sum is also zero. The lower-left corner in the second sum in the right-hand
side of (11.3.26) is equal to
v
X
j=u+1
tijηjβ(φ)(tkj)φ,
where tij is the entry on the main diagonal of Tij. Thus,
δikeηiβ(φ) =
v
X
j=u+1
tijηjβ(φ)(tkj)φ.
It follows that
eηu+1β(φ) ⊕· · · ⊕eηvβ(φ)
= [tik]v
i,k=u+1 (ηu+1β(φ) ⊕· · · ⊕ηvβ(φ)))

[tik]v
i,k=u+1

φ .
(11.3.27)
Now Theorem 4.1.2(b) guarantees that the two systems of signs {ηu+1, . . . , ηv} and
{eηu+1, . . . , eηv} have the same number of +1s (and also the same number of −1s).
Thus, within each set of blocks of equal size mj, the number of ηj’s which are
equal to +1 (resp. to −1) coincides with the number of eηj’s which are equal to +1
(resp. to −1). This shows that (11.3.20) is indeed obtained from (11.3.19) after a
permutation of blocks.

274
CHAPTER 11
Finally, assume that A + tB is φ-congruent to (11.3.18), and also φ-congruent
to
eδ1 (β(φ)Fk1 + tβ(φ)Gk1) ⊕· · · ⊕eδr (β(φ)Fkr + tβ(φ)Gkr)
with possibly diﬀerent signs eδj, j = 1, 2, . . . , r. Arguing as in the preceding case,
we obtain the equalities
Tij(δjβ(φ)Fkj) = (eδiβ(φ)Fki)Sij,
Tij
 δjβ(φ)Gkj

=

eδiβ(φ)Gki

Sij.
The proof that the form
⊕r
j=1eδj
 Fkj + Gkj

is obtained from (11.3.18) after a permutation of blocks, proceeds from now on in
the same way (letting γ = 0) as the proof that (11.3.19) and (11.3.20) are the same
up to a permutation of blocks.
The proof of the uniqueness part of Theorem 11.2.2(b) is complete.
□
11.4
NUMERICAL RANGES AND CONES
Let φ be a nonstandard involution.
Deﬁnition 11.4.1. For a pair of φ-skewhermitian n × n quaternionic matrices
(A, B), we deﬁne the φ-numerical range
Wφ(A, B) := {(xφAx, xφBx) : x ∈Hn×1,
∥x∥= 1} ⊆H2
and the φ-numerical cone
Cφ(A, B) := {(xφAx, xφBx) : x ∈Hn×1} ⊆H2.
Since φ(xφAx) = −xφAx, we clearly have that
Wφ(A, B) ⊆Cφ(A, B) ⊆{(y1β(φ), y2β(φ)) : y1, y2 ∈R}.
Proposition 11.4.2. The φ-numerical range Wφ(A, B) and cone Cφ(A, B) are
convex.
Indeed, the convexity of Wφ(A, B) is proved in Theorem 3.7.13.
Then the
convexity of Cφ(A, B) follows without diﬃculty (Ex. 11.5.1).
We present a result that characterizes the situations when the φ-numerical cone
is contained in a half-plane bounded by a line passing through the origin.
We
identify here R2β(φ) with R2.
Theorem 11.4.3. The following statements are equivalent for a pair of φ-skew-
hermitian n × n matrices (A, B):
(1) Cφ(A, B) is contained in a half-plane bounded by a line passing through the
origin;
(2) the pencil A + tB is φ-congruent to a pencil of the form β(φ)A′ + tβ(φ)B′,
where A′ and B′ are real symmetric matrices such that some linear combina-
tion (sin µ)A′ + (cos µ)B′, 0 ≤µ < 2π is positive semideﬁnite.

MATRIX PENCILS WITH SYMMETRIES: NONSTANDARD INVOLUTION
275
Proof. Observe that implication (2) =⇒(1) is evident.
We prove (1) =⇒(2). Since Cφ(A, B) = Cφ(SφAS, SφBS) for any invertible
quaternion matrix S, we may (and do) assume without loss of generality, that the
pair (A, B) is given in the canonical form of Theorem 11.2.2(b).
It will be convenient to consider particular blocks ﬁrst.
Claim. Let
A0 =

0
αFℓ+ Gℓ
−αFℓ−Gℓ
0

,
B0 =

0
Fℓ
−Fℓ
0

,
where α ∈Inv (φ) \ R. Then
Cφ(A0, B0) = R2β(φ).
(11.4.1)
To verify (11.4.1), ﬁrst observe that adding a real nonzero multiple of B0 to A0
does not alter the property (11.4.1). Thus, we may assume that the real part of α
is zero. Next, replacing φ by a similar nonstandard involution, if necessary, we may
also assume that β(φ) = k and α = ai + bj for some real a and b not both zero. It
will be proved that
(Rk, 0) ⊆Ω0,
(11.4.2)
where
Ω0 :=




















[φ(y) 0 . . . 0 φ(z)]A0


y
0
...
0
z


, [φ(y) 0 . . . 0 φ(z)]B0


y
0
...
0
z






















,
with x, y ∈H arbitrary, and
(0, Rk) ⊆Ω0.
(11.4.3)
In view of the convexity of Cφ(A0, B0), this will suﬃce to prove (11.4.1). Write
 y
z

= x1 + x2i + x3j + x4k,
x1, x2, x3, x4 ∈R2×1.
Then
[φ(y) 0 . . . 0 φ(z)]B0[y 0 . . . 0 z]T
= (xT
1 + xT
2 i + xT
3 j −xT
4 k)

0
1
−1
0

(x1 + x2i + x3j + x4k),
which, in turn, is equal to
k

xT
1
xT
2
xT
3
xT
4



0
0
0
0
0
0
0
1
0
0
0
0
0
0
−1
0
0
0
0
0
0
1
0
0
0
0
0
0
−1
0
0
0
0
0
0
−1
0
0
0
0
0
0
1
0
0
0
0
0
0
−1
0
0
0
0
0
0
1
0
0
0
0
0
0
0




x1
x2
x3
x4

.
(11.4.4)

276
CHAPTER 11
Analogously,
[φ(y) 0 . . . 0 φ(z)]A0[y 0 . . . 0 z]T
= (xT
1 + xT
2 i + xT
3 j −xT
4 k)

0
ai + bj
−ai −bj
0

(x1 + x2i + x3j + x4k),
which is equal to
k

xT
1
xT
2
xT
3
xT
4



0
0
0
−b
0
a
0
0
0
0
b
0
−a
0
0
0
0
b
0
0
0
0
0
−a
−b
0
0
0
0
0
a
0
0
−a
0
0
0
0
0
−b
a
0
0
0
0
0
b
0
0
0
0
a
0
b
0
0
0
0
−a
0
−b
0
0
0


·


x1
x2
x3
x4

.
(11.4.5)
Assuming x3 = x4 = 0 (if b ̸= 0) or x2 = x4 = 0 (if a ̸= 0) in (11.4.4) and (11.4.5)
shows the inclusion (11.4.2), and assuming x1 = x4 = 0 shows (11.4.3).
Suppose now that statement (1) holds. In view of the claim and taking advan-
tage of formula (11.2.5), we may further assume that A = eAβ(φ) and B = eBβ(φ),
where eA and eB are real symmetric n × n matrices. Now, clearly, statement (2)
follows.
□
Other criteria for Theorem 11.4.3(2) to hold true, in terms of the pair of real
symmetric matrices (A′, B′), can be given using results of Theorem 11.3 in Lan-
caster and Rodman [92] and Theorem 5 in Cheung et al. [26].
Theorem 11.4.4. Let (A, B) be a pair of φ-skewhermitian (quaternion) n × n
matrices. Then Cφ(A, B) is contained in a half-plane bounded by a line passing
through the origin if and only if the pencil A + tB is φ-congruent to a pencil of the
form β(φ)A′ + tβ(φ)B′, where A′ and B′ are real symmetric matrices that satisfy
either one of the following two equivalent conditions:
(a) For every matrix X ∈Rn×2 such that XT X = I2, it holds that some nontrivial
real linear combination of XT A′X and XT B′X is positive semideﬁnite.
(b) For every x ∈Rn×1 such that ⟨A′x, x⟩= ⟨B′x, x⟩= 0, the two vectors A′x
and B′x are R-linearly dependent, and at least one of the following conditions
(i) and (ii) fails:
(i) Ker A′ = Ker B′ has (real) dimension n −2.
(ii) The dimension of Ran (aA′ + bB′) is equal to 2 for every pair a, b ∈R
not both zero.

MATRIX PENCILS WITH SYMMETRIES: NONSTANDARD INVOLUTION
277
11.5
EXERCISES
Ex. 11.5.1. Prove that Cφ(A, B) is convex.
Ex. 11.5.2. Find the canonical form under φ-congruence of Theorem 11.1.3 for
the following φ-hermitian pencils:
(a)

0
β(φ)
−β(φ)
0

+ t
 1
1
1
1

;
(b)


0
0
β(φ)
0
0
0
−β(φ)
0
0

+ t


q
q
q
q
q
q
q
q
q

,
where q ∈H \ {0} is such that β(φ)q = −qβ(φ).
Ex. 11.5.3. Characterize (in terms of structure of the Kronecker form) those
φ-hermitian matrix pencils A + tB that are:
(a) φ-congruent to real symmetric matrix pencils;
(b) φ-congruent to matrix pencils of the form β(φ)A + tβ(φ)B, where A and B
are real skewsymmetric matrices.
Ex. 11.5.4. Find all possible canonical forms of Theorem 11.1.3 if it is known
that rank (xA + yB) ≤2 for all real x, y.
Ex. 11.5.5. Find all possible canonical forms of Theorem 11.1.3 if it is known
that rank (xA + yB) = 3 for all real x, y not both zero.
Ex. 11.5.6. Characterize those φ-skewhermitian matrix pencils A + tB that
are:
(a) φ-congruent to real skewsymmetric matrix pencils;
(b) φ-congruent to matrix pencils of the form β(φ)A + tβ(φ)B, where A and B
are real symmetric matrices.
Ex. 11.5.7. Find the canonical form under φ-congruence of Theorem 11.2.2 for
the following φ-skewhermitian pencils:

0
β(φ)
β(φ)
0

+ t
 aβ(φ)
q
−q
0

,
where q is a nonzero quaternion such that β(φ)q = −qβ(φ), and a is a real param-
eter.
Ex. 11.5.8. For each φ-skewhermitian matrix pencil A+tB of Ex. 11.5.6, verify
whether or not the φ-numerical cone Cφ(A, B) is contained in a half-space bounded
by a line passing through the origin.
Ex. 11.5.9. Let A + tB be a φ-skewhermitian n × n matrix pencil such that A
is invertible and the following property holds true:
(A) Every φ-skewhermitian matrix pencil which is strictly equivalent to A + tB
is, in fact, φ-congruent to A + tB.
Show that then there exists ε > 0 such that every φ-skewhermitian n×n matrix
pencil A′ + tB′ with ∥A′ −A∥+ ∥B′ −B∥< ε also has the property (A). (Cf.
Theorem 11.2.4.)

278
CHAPTER 11
Ex. 11.5.10. Find whether or not the result of Ex. 11.5.9 is valid if:
(a) the condition that A is invertible is omitted;
(b) the condition that A is invertible is replaced by the condition that B is in-
vertible;
(c) the condition that A is invertible is replaced by the condition that B + tA is
invertible for some real t.
Ex.
11.5.11. Let H ∈Hn×n be φ-skewhermitian, not necessarily invertible.
State and prove analogues of Theorems 8.6.5 and 8.6.8 for (H, φ)-expansive and
(H, φ)-plus-matrices.
Ex. 11.5.12. Provide details of the derivation of Theorem 11.2.2 part (a) from
Theorem 8.1.2, by taking advantage of Proposition 4.1.7. Hint: See the proof of
Theorem 11.1.3.
11.6
NOTES
The contents of this chapter are based on Rodman [132]. In particular, formulations
of the main results and the proofs are taken from that paper.
The results of
Theorems 11.1.3 and 11.2.2 (for the case when at least one of the matrices A and
B is invertible) are found in Djokovi´c et al. [34], for example.

Chapter Twelve
Mixed matrix pencils: Nonstandard involutions
The canonical forms of mixed quaternion matrix pencils, i.e., such that one of the
two matrices is φ-hermitian and the other is φ-skewhermitian, are also studied here
with respect to simultaneous φ-congruence. Other canonical forms of mixed matrix
pencils are developed with respect to strict equivalence.
As an application, we
provide canonical forms of quaternion matrices under φ-congruence.
As in the preceding chapter, we ﬁx a nonstandard involution φ throughout this
chapter and a quaternion β(φ) such that φ(β(φ)) = −β(φ) and |β(φ)| = 1.
12.1
CANONICAL FORMS FOR φ-MIXED PENCILS: STRICT
EQUIVALENCE
Deﬁnition 12.1.1. A matrix pencil A + tB, where A, B ∈Hn×n, is said to be
φ-hermitian-skewhermitian, in short φ-hsk, if Aφ = A and Bφ = −B.
In this section we formulate the canonical form for φ-hsk matrix pencils under
strict equivalence.
We start with a list of primitive forms of φ-hsk pencils. We will use the matrices
Ξm (β(φ)) deﬁned in (1.2.6). Thus, (Ξm (β(φ)))φ = Ξm (β(φ)) if m is even, and
(Ξm (β(φ)))φ = −Ξm (β(φ)) if m is odd. The equality
 1
1
1
−1
 
0
β(φ)
β(φ)
0
  1
1
1
−1

=
 2β(φ)
0
0
−2β(φ)

shows that the β(φ)-signature (cf. Theorem 4.1.2(b)) of Ξm (β(φ)) for odd m is
equal to
m −1
2
, m + 1
2
, 0

if m = 4k + 3, k nonnegative integer
(12.1.1)
and to
m + 1
2
, m −1
2
, 0

if m = 4k + 1, k nonnegative integer.
(12.1.2)
We consider the following primitive φ-hsk pencils; the designation (q-φ-h-sk)
stands for quaternion, φ, hermitian, skewhermitian.
(q-φ-h-sk0)
a square-size zero matrix.
(q-φ-h-sk1)
G2ε+1 + t


0
0
Fε
0
01
0
−Fε
0
0

,
ε positive integer.
(q-φ-h-sk2)
Fk + tβ(φ)Gk,
k positive integer.

280
CHAPTER 12
(q-φ-h-sk3)
Gk + tβ(φ)Fk,
k positive integer.
(q-φ-h-sk4)
"
0
αF ℓ
2 + G ℓ
2 + tF ℓ
2
αF ℓ
2 + G ℓ
2 −tF ℓ
2
0
#
,
where ℓis even and α ∈Inv (φ), R(α) > 0.
(q-φ-h-sk5)
 −Ξs−1 (β(φ))
0
0
0

+


0
0
. . .
0
ρ
0
0
. . .
−ρ
0
...
...
...
...
...
0
−ρ
. . .
0
0
ρ
0
. . .
0
0


+ tΞs (β(φ)),
(12.1.3)
where s is odd and ρ is real positive.
(q-φ-h-sk6)
Ξs (β(φ)) +
t








−Ξs−1 (β(φ))
0
0
0

+


0
0
. . .
0
ρ
0
0
. . .
−ρ
0
...
...
...
...
...
0
ρ
. . .
0
0
−ρ
0
. . .
0
0









,
(12.1.4)
where s is even and ρ is real positive.
The matrices eGk and eGℓ/2 can be used in (q-φ-h-sk2), (q-φ-h-sk3), and (q-φ-h-
sk4) in place of Gk and Gℓ/2, respectively.
We also remark that the block (q-φ-h-sk4) is φ-congruent to (q-φ-h-sk4), in
which α is replaced by −α. Thus, in (q-φ-h-sk4) one may replace the condition
R(α) > 0 with the condition R(α) < 0. Indeed, we have

0
Y T
XT
0
 "
0
αF ℓ
2 + G ℓ
2 + tF ℓ
2
αF ℓ
2 + G ℓ
2 −tF ℓ
2
0
#  0
X
Y
0

=
"
0
−αF ℓ
2 + G ℓ
2 + tF ℓ
2
−αF ℓ
2 + G ℓ
2 −tF ℓ
2
0
#
,
(12.1.5)
where X and Y are invertible real ℓ/2 × ℓ/2 matrices such that
Y T (−G ℓ
2 F ℓ
2 )(Y T )−1 = G ℓ
2 F ℓ
2
and
X = −Fℓ/2(Y T )−1Fℓ/2.
Such real matrices X and Y obviously exist because GF = Jℓ/2(0) and Jℓ/2(0) is
similar to its negative. The veriﬁcation of equality (12.1.5) is straightforward.
It will be convenient to denote
Ψs(α) :=
 −Ξs−1 (β(φ))
0
0
0

+


0
0
. . .
0
α
0
0
. . .
−α
0
...
...
...
...
...
0
(−1)s−2α
. . .
0
0
(−1)s−1α
0
. . .
0
0


,

MIXED MATRIX PENCILS: NONSTANDARD INVOLUTIONS
281
where α ∈H \ {0}. Thus, the primitive form (q-φ-h-sk5) is Ψs(ρ) + tΞs (β(φ)), and
the primitive form (q-φ-h-sk6) is Ξs (β(φ)) + tΨs(ρ).
Theorem 12.1.2. Every φ-hsk matrix pencil A + tB is strictly equivalent to a
φ-hsk matrix pencil that is a direct sum of blocks of types (q-φ-h-sk0)–(q-φ-h-sk6).
The direct sum is uniquely determined by A and B up to an arbitrary permuta-
tion of the diagonal blocks and up to a replacement in each block of type (q-φ-h-sk4),
the quaternion α with a similar quaternion α′ ∈Inv (φ).
12.2
PROOF OF THEOREM 12.1.2
We start with preliminary considerations.
Let q = (q1, q2, q3) be a units triple. Write X ∈Hm×n in the form
X = X11 + q1X12 + (X21 + q1X22)q2,
where X11, X12, X21, X22 ∈Rm×n. Then we deﬁne
eω(q)
m,n(X) :=

X11 + iX12
X21 + iX22
−X21 + iX22
X11 −iX12

∈C2m×2n.
(12.2.1)
We will often write eω(q)(X) for eω(q)
m,n(X), with m, n understood form context.
In complete analogy with the map eωn of (5.6.1), the map eω(q)(·) is a unital
homomorphism.
Proposition 12.2.1. The map eω(q)(·) is one-to-one and has the properties:
eω(q)(aX + bY ) = aeω(q)(X) + beω(q)(Y ),
∀X, Y ∈Hm×n,
a, b ∈R;
eω(q)(XY ) = eω(q)(X)eω(q)(Y ),
∀X ∈Hm×n,
Y ∈Hn×p;
eω(q)(X∗) = (eω(q)(X))∗,
∀X ∈Hm×n;
eω(q)(I) = I.
In particular, eω(q) is a homomorphism of real algebras on Hn×n.
As in Theorem 5.7.1, we have the following.
Theorem 12.2.2. If
Jm1(α1) ⊕· · · ⊕Jmr(αr),
α1 = a1 + ib1, . . . , αr = ar + ibr ∈C,
(12.2.2)
is a Jordan form of A ∈Hn×n, then
 Jm1(α1)
0
0
Jm1(α1)

⊕· · · ⊕
 Jmr(αr)
0
0
Jmr(αr)

is the complex Jordan form of eω(q)(A).
For the proof, use the Jordan form (12.2.2) with αj ∈SpanR {1, q1}, for j =
1, 2, . . . , r.
We note the following connection between the action of a nonstandard involution
and the complex representation eω(q).

282
CHAPTER 12
Proposition 12.2.3. Let φ be a nonstandard involution, and let q = (q1, q2, q3)
be a units triple such that
Inv (φ) = SpanR{1, q1, q3}.
Then for the map eω(q)(·) given by (12.2.1), we have
eω(q)(Xφ) = (eω(q)(X))T .
(12.2.3)
The proof is by a straightforward veriﬁcation.
In the next lemma, the properties of the Kronecker form of a mixed pencil are
given.
Lemma 12.2.4. Let A + tB ∈Hn×n, where A is φ-hermitian and B is φ-
skewhermitian. Then:
(1) the left indices of the pencil A + tB, arranged in the nondecreasing order,
coincide with its right indices, also arranged in nondecreasing order;
(2) for every eigenvalue α with nonzero real part of A+tB, the indices of A+tB
that correspond to α are paired with the indices of A + tB that correspond to
−α. In other words, if α is an eigenvalue of A + tB with a nonzero real part,
then −α is also an eigenvalue of A+tB, and for every positive integer k, the
number of blocks tIℓj + Jℓj(αj) in the Kronecker form of A + tB for which
ℓj = k and αj is similar to α coincides with the number of blocks tIℓj +Jℓj(αj)
for which ℓj = k and αj is similar to −α.
Proof. Let (7.3.1) be the Kronecker form of A + tB; thus
S(A + tB)T
=
0u×v ⊕Lε1×(ε1+1) ⊕· · · ⊕Lεp×(εp+1)
⊕LT
η1×(η1+1) ⊕· · · ⊕LT
ηq×(ηq+1)
⊕(Ik1 + tJk1(0)) ⊕· · · ⊕(Ikr + tJkr(0))
⊕(tIℓ1 + Jℓ1(α1)) ⊕· · · ⊕(tIℓs + Jℓs(αs))
(12.2.4)
for some invertible matrices S and T. Let q := (q1, q2, q3) be a units triple such that
Inv (φ) = SpanR{1, q1, q3}. We may (and do) assume without loss of generality, that
α1, . . . , αs ∈SpanR{1, q1}. We now apply the map eω(q) (deﬁned by (12.2.1)) to the
equality (12.2.4). Using the deﬁnition of the map eω(q) and Theorem 12.2.2, after a
permutation of blocks and similarity with a complex similarity matrix (if necessary),
we obtain that the complex pencil eω(q)(A) + teω(q)(B) is strictly equivalent (over C)
to the following complex pencil:
02u×2v
⊕
⊕p
j=1
 Lεj×(εj+1) ⊕Lεj×(εj+1)

⊕
⊕q
j=1

LT
ηj×(ηj+1) ⊕LT
ηj×(ηj+1)

⊕
⊕r
j=1
 (Ikj + tJkj(0)) ⊕(Ikj + tJkj(0))

⊕
⊕s
j=1
 (tIℓj + Jℓj(αj)) ⊕(tIℓj + Jℓj(αj))

.
By Proposition 12.2.3, the matrix eω(q)(A) is (complex) symmetric, and the matrix
eω(q)(B) is skewsymmetric. By Theorem 15.3.6 the left and right indices of eω(q)(A)+
teω(q)(B) coincide, and for every nonzero eigenvalue α ∈C of eω(q)(A) + teω(q)(B),

MIXED MATRIX PENCILS: NONSTANDARD INVOLUTIONS
283
the complex number −α is also an eigenvalue of eω(q)(A)+teω(q)(B), and the indices
of α as the eigenvalue of eω(q)(A) + teω(q)(B) coincide with those of −α. The result
of Lemma 12.2.4 now follows.
□
We are now ready to prove Theorem 12.1.2. Indeed, in view of Lemma 12.2.4 we
may assume that the Kronecker form of the φ-hsk pencil A + tB has the following
form:
0u×u
⊕
⊕p
j=1

Lεj×(εj+1) ⊕LT
εj×(εj+1)

⊕⊕r
j=1
 (Ikj + tJkj(0)

⊕
⊕m
j=1
 tIℓj + Jℓj(αj)

⊕
⊕s
j=m+1
 (tIℓj + Jℓj(αj)) ⊕(tIℓj + Jℓj(−αj))

.
(12.2.5)
Here α1, . . . , αs ∈Inv (φ); the real parts of α1, . . . , αm are equal to zero; and the real
parts of αm+1, . . . , αs are nonzero. It remains to show that each block in (12.2.5) is
strictly equivalent to one of the blocks (q-φ-h-sk0)–(q-φ-h-sk6). This is trivial for
0u×u. For (q-φ-h-sk1), we have
 Lε×(ε+1)(t) ⊕Λε×(ε+1)(−t)T 
F2ε+1 = G2ε+1 + t


0
0
Fε
0
01
0
−Fε
0
0

.
Here the ε × (ε + 1) matrix pencil
Λε×(ε+1)(−t) :=


1
−t
0
· · ·
0
0
1
−t
· · ·
0
...
...
...
...
0
0
· · ·
1
−t

∈Hε×(ε+1)
is strictly equivalent to Lε×(ε+1)(−t), which, in turn, is strictly equivalent to the
pencil Lε×(ε+1)(t). Indeed,
FεLε×(ε+1)(−t)Fε+1 = Λε×(ε+1)(−t),
and
diag (−1, 1, −1, . . . , ±1)Lε×(ε+1)(t)diag (1, −1, 1, . . . , ±1)
= Lε×(ε+1)(−t).
The block Ik + tJk(0) is strictly equivalent to (q-φ-h-sk2):
 diag (1, β(φ)−1, . . . , β(φ)−k+1)

(Ik + tJk(0))Fk
·
 diag (β(φ)k−1, β(φ)k−2, . . . , 1)

= Fk + tβ(φ)Gk.
Analogously, the block tIℓ+ Jℓ(0) is strictly equivalent to a block (q-φ-h-sk3) (of
the same size ℓ× ℓ):
 diag (1, (−β(φ))−1, . . . , (−β(φ))−ℓ+1)

(tIℓ+ Jℓ(0))β(φ)Fℓ
·
 diag ((−β(φ))ℓ−1, (−β(φ))ℓ−2, . . . , 1)

= β(φ)tFℓ+ Gℓ.

284
CHAPTER 12
If the real part of α is nonzero, then (tIℓ/2 +Jℓ/2(α))⊕(tIℓ/2 +Jℓ/2(−α)) is strictly
equivalent to (q-φ-h-sk4):

0
αFℓ/2 + Gℓ/2
αFℓ/2 + Gℓ/2
0

+ t

0
Fℓ/2
−Fℓ/2
0

·

0
−Fℓ/2
Fℓ/2
0

=
 tI + Jℓ/2(α)
0
0
tI −Jℓ/2(α)

,
and note that −Jℓ/2(α) is similar to Jℓ/2(−α). Finally, consider a block tIs +Js(α),
where α ∈Inv (φ) \ {0} and the real part of α is zero. We show that for odd s this
block is strictly equivalent to (q-φ-h-sk5), and for even s it is strictly equivalent to
(q-φ-h-sk6). Indeed, (tIs + Js(α))Ξs (β(φ)) (for s odd) is equal to the matrix
Ψs(αβ(φ)) + tΞs (β(φ)).
Note that αβ(φ) ∈Inv (φ) \ {0}; hence, the transformation
Ψs(αβ(φ)) + tΞs (β(φ))
−→
φ(ω)I · (Ψs(αβ(φ)) + tΞs (β(φ))) · ωI
for a suitable ω ∈H, |ω| = 1, yields the form (12.1.3); see Corollary 4.1.4. For
the case when s is even, we ﬁrst observe that (Js(α))−1 is similar to Js(α−1), and,
therefore, tIs + Js(α) is strictly equivalent to Is + tJs(α−1). (Note that α−1 ∈
Inv (φ)\{0}, and the real part of α−1 is zero as well.) Now, as in the case of odd s,
a suitable transformation of (Is + Js(α−1))Ξs (β(φ)) yields the form (12.1.3). This
completes the proof of the existence part of Theorem 12.1.2.
The proof of uniqueness of Theorem 12.1.2 follows from the uniqueness of the
Kronecker form of the φ-hsk pencil A + tB, and from the proof (given above) that
each block in (12.2.5) is strictly equivalent to one, in fact exactly one, of the blocks
(q-φ-h-sk0)–(q-φ-h-sk6).
12.3
CANONICAL FORMS OF φ-MIXED PENCILS:
CONGRUENCE
We state the main result on the canonical form of φ-hermitian-skewhermitian matrix
pencils under (simultaneous) φ-conruence.
Theorem 12.3.1. Every φ-hsk matrix pencil A + tB is φ-congruent to a φ-hsk
matrix pencil of the form
(A0 + tB0)
⊕
⊕p
j=1εj(Fkj + tβ(φ)Gkj) ⊕⊕q
j=1κj(Gℓj + tβ(φ)Fℓj)
⊕
⊕r
j=1δj(Ψsj(νj) + tΞsj (β(φ)))
⊕
⊕m
j=1µj(Ξwj (β(φ)) + tΨwj(τj)),
where the parameters have the following properties:
(1) The integers kj’s are even, ℓj’s are odd, sj’s are odd, and wj’s are even.
(2) The εj’s, κj’s, δj’s, and µj’s are signs ±1.
(3) The νj and τj are positive reals.

MIXED MATRIX PENCILS: NONSTANDARD INVOLUTIONS
285
(4) The part A0 + tB0 is block diagonal with the diagonal blocks consisting of
blocks of the types (q-φ-h-sk0), (q-φ-h-sk1), (q-φ-h-sk4), (q-φ-h-sk2) of odd
sizes, and (q-φ-h-sk3) of even sizes.
The form (12.3.1) is uniquely determined by the pencil A + tB, up to permutations
of diagonal blocks in each of the ﬁve parts
A0 + tB0,
⊕p
j=1εj(Fkj + tβ(φ)Gkj),
⊕q
j=1κj(Gℓj + tβ(φ)Fℓj),
⊕r
j=1δj(Ψsj(νj) + tΞsj (β(φ))),
and
⊕m
j=1 µj(Ξwj (β(φ)) + tΨwj(τj)),
and up to a replacement in each block of type (q-φ-h-sk4) the quaternion α with a
similar quaternion α′ ∈Inv (φ).
Conversely, if a matrix pencil A + tB is φ-congruent to a pencil of the form
(12.3.1), then A + tB is φ-hsk.
A direct and independent proof of Theorem 12.3.1, with full detail, is given in
Rodman [133]. It follows a general outline of the proof of Theorem 11.2.2(b) (see
also Theorem 8.1 in Rodman [132]); this, in turn, is based on a similar approach for
real and complex matrix pencils with symmetries that was used in many sources;
see, e.g., Thompson [151] and Lancaster and Rodman [92, 89]. The proof in Rodman
[133] is rather lengthy, so we will not reproduce it here. Instead, we will present in
the next section a partial proof (of existence) based on reduction to Theorem 9.4.1.
Remark 12.3.2. The proofs of Theorems 12.1.2 (Section 12.2) and 12.3.1 (as
presented in Rodman [133]) will show that in the form (12.3.1), the blocks of types
(q-φ-h-sk5) and (q-φ-h-sk6) may be replaced by more general types, as follows: Fix
α0, α′
0 ∈H \ {0} such that φ(α0) = α0 and φ(α′
0) = α′
0. Then (12.3.1) can be
replaced by the form
(A0 + tB0)
⊕
⊕p
j=1εj(Fkj + tβ(φ)Gkj) ⊕⊕q
j=1κj(Gℓj + tβ(φ)Fℓj)
⊕
⊕r
j=1δj(Ψsj(xjα0) + tΞsj (β(φ)))
⊕
⊕m
j=1µj(Ξwj (β(φ)) + tΨwj(yjα′
0)),
(12.3.1)
where xj’s and yj’s are real positive numbers, and all other parameters are as in
Theorems 12.1.2 and 12.3.1. The form (12.3.1) is unique up to a permutation of
diagonal blocks.
In particular, one can take α0 = α′
0 = −1 in (12.3.1), which
amounts to the requirement that ρ is real negative in (q-φ-h-sk5) and (q-φ-h-sk6),
rather than real positive.
Remark 12.3.3. Note that for even ℓ, the pencil Gℓ+ tβ(φ)Fℓis φ-congruent
to its negative, −Gℓ+tβ(φ)(−Fℓ). Indeed, if q ∈Inv (φ) is such that q2 = −1, then
qβ(φ) = −β(φ)q, and
(diag (q, −q, . . . , −q))(Gℓ+ tFℓ)(diag (q, −q, . . . , −q)) = −Gℓ−tβ(φ)Fℓ.
Observe that the forms (q-φ-h-sk5) and (q-φ-h-sk6) involve only quaternions
that are real linear combinations of 1 and β(φ). This circumstance allows one to
compare these forms with the canonical forms of complex hermitian-skewhermitian
matrix pencils by means of the real linear map
χ : SpanR {1, β(φ)}
−→
C,
χ(1) = 1,
χ(β(φ)) = i.
(12.3.2)

286
CHAPTER 12
Lemma 12.3.4. (a) Consider the φ-hsk matrix pencil Ψs(ρ)+tΞs (β(φ)), where
ρ be a nonzero real number and s is odd. Then there exists an invertible matrix S
with entries in SpanR {1, β(φ)} such that
Sφ(Ψs(ρ) + tΞs (β(φ)))S = ±(Fs + tβ(φ)FsJs(ρ−1)).
The sign ± depends only on s and on the sign (positive or negative) of ρ.
(b) For the φ-hsk matrix pencil Ξs (β(φ)) + tΨs(ρ), where ρ is a nonzero real
number and s is even, there exists an invertible matrix eS with entries in the algebra
SpanR {1, β(φ)} such that
eSφ(Ξs (β(φ)) + tΨs(ρ))eS = ±(Fs + tβ(φ)FsJs(−ρ)),
(12.3.3)
where the sign ± depends only on s and on the sign of ρ.
Proof. We will prove (b) only, the proof of (a) being completely analogous.
Deﬁne the complex matrices H and G by the equalities
H = χ(Ξs (β(φ))),
iG = χ(Ψs(ρ)),
with the map χ applied entrywise. Then H and G are hermitian and invertible.
Moreover, H−1G is similar to Js(−ρ). Now Theorem 15.3.1 yields existence of an
invertible complex matrix W such that
W ∗(H + tiG)W = ±(Fs + tiFsJs(−ρ)).
(12.3.4)
Letting eS = χ−1(W), formula (12.3.3) follows. To see that the sign in (12.3.4)
depends only on s and on the sign of ρ, we appeal to the perturbation theory of
sign characteristic of a pair of complex or real invertible hermitian matrices (see
Gohberg et al. [53, Theorem 5.9.1] and Rodman [131]).
□
Using Lemma 12.3.4 and Remark 12.3.2, the main result of Theorem 12.3.1 may
be reformulated as follows.
Theorem 12.3.5. Every φ-hsk matrix pencil A + tB is φ-congruent to a φ-hsk
matrix pencil of the form
(A0 + tB0)
⊕
⊕p
j=1εj(Fkj + tβ(φ)Gkj)
⊕
⊕q
j=1κj(Gℓj + tβ(φ)Fℓj)
⊕
⊕r
j=1δj(Fsj + tβ(φ)FsjJsj(τj)),
(12.3.5)
where the parameters have the following properties:
(1) The integers kj’s are even, and ℓj’s are odd.
(2) The εj’s, κj’s, δj’s are signs ±1.
(3) τj are positive reals.
(4) The part A0 + tB0 is block diagonal with the diagonal blocks consisting of
blocks of the types (q-φ-h-sk0), (q-φ-h-sk1), (q-φ-h-sk4), (q-φ-h-sk2) of odd
sizes, and (q-φ-h-sk3) of even sizes.

MIXED MATRIX PENCILS: NONSTANDARD INVOLUTIONS
287
The uniqueness properties of the form (12.3.5) are the same as those of (5.9.1).
The signs εj’s, κj’s, δj’s in (12.3.5) (or in (12.3.1)) constitute the sign charac-
teristic of the φ-hsk pencil A + tB. Thus, the sign characteristic assigns a ± sign
to every Jordan block in the Kronecker form of A + tB with a nonzero eigenvalue
having zero real part, to every Jordan block of A + tB with zero eigenvalue and
odd size, and to every block corresponding to inﬁnity of even size. The unique-
ness statements of Theorems 12.3.1 and 12.3.5 lead naturally to equivalence of sign
characteristics: two sign characteristics are said to be equivalent if for every eigen-
value τβ(φ) of A + tB, 0 ≤τ ≤∞, and for every positive integer k, the number
of signs +1 (or, equivalently, of signs −1) associated with the blocks of size k and
eigenvalue τβ(φ) is the same in both sign characteristics (k is assumed to be odd
if τ = 0 and even if τ = ∞). We obtain from Theorems 12.3.1 and 12.3.5 that two
φ-hsk matrix pencils are φ-congruent if and only if they are strictly equivalent and
have equivalent sign characteristics.
12.4
PROOF OF THEOREM 12.3.1
We prove here the existence part of Theorem 12.3.1 only and refer the reader to
Rodman [133] for a full detailed proof of the theorem.
Let A + tB be a φ-hsk (quaternion) matrix pencil. Set
eA := −β(φ)A,
eB := −β(φ)B.
By Proposition 4.1.7, the matrix pencil eB + t eA is hermitian/skewhermitian. Let
Y := S∗( eB + t eA)S
(12.4.1)
be a canonical form of eB + t eA as in Theorem 9.4.1, and let eY := S∗( eA + t eB)S be
the form obtained from (12.4.1) with the roles of eA and eB interchanged. Then by
Proposition 4.1.7,
β(φ)eY = Sφ(A + tB)S,
and all that remains is to prove that the constituent blocks of β(φ)eY are φ-congruent
to the corresponding blocks in (9.4.1), (9.4.2), or (12.3.5). Ignoring the trivial zero
block, this boils down to the following lemma.
Lemma 12.4.1. (1) The matrix pencils
A1 + tB1 := β(φ)


0
0
Fε
0
01
0
−Fε
0
0

+ tβ(φ)G2ϵ+1
and
A2 + tB2 := G2ε+1 + t


0
0
Fε
0
01
0
−Fε
0
0


are φ-congruent.
(2) The matrix pencils
β(φ)iGk + tβ(φ)Fk
and
± (Gk + tβ(φ)Fk)

288
CHAPTER 12
are φ-congruent if k is even, and
β(φ)iGk + tβ(φ)Fk
and
Gk + tβ(φ)Fk
are φ-congruent if k is odd.
(3) The matrix pencils
β(φ)iFℓ+ tβ(φ)Gℓ
and
± (Fℓ+ tβ(φ)Gℓ)
are φ-congruent if ℓis odd, and
β(φ)iFℓ+ tβ(φ)Gℓ
and
Fℓ+ tβ(φ)Gℓ
are φ-congruent if ℓis even.
(4) For every α ∈H with positive real part, the matrix pencils
β(φ)

0
Fp
−Fp
0

+ tβ(φ)

0
αFp + Gp
α∗Fp + Gp
0

and

0
eαFp + Gp + tFp
eαFp + Gp −tFp
0

are φ-congruent for some eα ∈Inv (φ), which also has positive real part.
(5) The matrix pencils
A3 + tB3 := β(φ)
·







Ξm(im) + t


0
0
. . .
0
β
0
0
. . .
−β
−1
...
...
...
...
...
0
(−1)m−2β
−1
. . .
0
(−1)m−1β
−1
. . .
0
0









(12.4.2)
and
δ(Fm + tβ(φ)FmJm(τ))
are φ-congruent for some choice of δ = ±1. Here β > 0 if m is odd, β ∈H is
nonzero with zero real part if m is even, and τ is a positive number in both cases.
The proof of (5) will show that δ = −1 if m is even or if m is odd and m −1
not divisible by 4 and δ = 1 otherwise.
Proof. Proof of (1). We identify SpanR {1, β(φ)} with C, via identiﬁcation of
β(φ) with the complex imaginary unit. Then φ acts on SpanR {1, β(φ)} as complex
conjugation, and both matrix pencils in (1) are complex hermitian/skewhermitian.
Since both pencils have the same Kronecker form

0
Lϵ×(ϵ+1)(t)
Lϵ×(ϵ+1)(t)
0

,
it follows from Theorem 15.3.1 (applied to the pencils A1 −tβ(φ)B1 and A2 −
tβ(φ)B2) that the matrix pencils in (1) are C-congruent and, therefore, φ-congruent.
Proof of (2). In view of Remark 9.4.5, we may replace i by any quaternion λ
with zero real part and norm 1. (In this respect note that β(φ)α ∈Inv (φ) for all

MIXED MATRIX PENCILS: NONSTANDARD INVOLUTIONS
289
α ∈H with zero real part.) So we replace i with β(φ). Identifying SpanR {1, β(φ)}
with C, and using Theorem 15.3.1 as in the proof of (1), we see that
β(φ)iGk + tβ(φ)Fk
and
Gk + tβ(φ)Fk
are φ-congruent. If, in addition, k is even, then Gk + tβ(φ)Fk is φ-congruent to its
negative by Remark 12.3.3.
The proof of (3) is similar to that of (2), using the equality (analogous to Remark
12.3.3)
diag (q, −q, . . . , q)(Fℓ+ tβ(φ)Gℓ)diag (−q, q, . . . , −q) = −(Fℓ+ tβ(φ)Gℓ)
for an odd size ℓ, where q ∈H is such that φ(q) = q, q2 = −1, and qβ(φ)q = β(φ).
Proof of (4). As it follows from the uniqueness statement of Theorem 9.4.1,
α can be replaced by any similar quaternion.
Thus, we may assume that α ∈
SpanR {1, β(φ)}. As before, we identify SpanR {1, β(φ)} with C and reduce the proof
to C-congruence of complex hermitian/skewhermitian matrix pencils or, equiva-
lently, (upon replacing t with tβ(φ)) to C-congruence of complex hermitian pencils
(Theorem 15.3.1).
Proof of (5).
Consider two cases separately: (a) m is even; (b) m is odd.
Suppose ﬁrst m is even. In view of Theorem 9.4.1, β can be replaced by any similar
quaternion. So, we take β = ±τβ(φ), where τ > 0 and where we take the sign −if
m is divisible by 4 and the sign + if m is not divisible by 4. Let (β(φ), q2, q3) be a
units triple. Then the following equalities hold true:
diag (q2, −q3, . . . , q2, −q3)(A3 + tB3)diag (q2, −q3, . . . , q2, −q3)
= −(Fm + tβ(φ)FmJm(τ))
if m is divisible by 4 and
diag (q2, q3, . . . , q2, q3)(A3 + tB3)diag (q2, q3, . . . , q2, q3)
= −(Fm + tβ(φ)FmJm(τ))
if m is not divisible by 4. This veriﬁes the required φ-congruence.
Next, suppose m is odd. Note that one can replace i in (12.4.2) by any quater-
nion λ similar to α; indeed, if λ = q∗iq for some q ∈H with |q| = 1, and denoting
for convenience the matrix β(φ)−1B3 by Υ, we have
(q∗I)(Ξm(im) + tΥ)(qI) = Ξm(λm) + tΥ.
Thus, upon replacing i with ±β(φ), where the sign chosen so that β(φ)(±β(φ))m =
1, the matrix pencil A3 + tB3 takes the form
A3 + tB3 = Ξ(1) + tβ(φ)Υ.
We now identify SpanR {1, β(φ)} with C and observe that the Kronecker form over
C of A3+tB3 is tI +Jm(α), where α := (β(φ)β)−1, and that of Fm+tβ(φ)FmJm(τ)
is tI + Jm(α′), where α′ := (β(φ)τ)−1. Thus, taking τ = β, we see by Theorem
15.3.1 that the matrix pencil A3 + tB3 is φ-congruent over C (which amounts to
C-congruence) to either Fm + tβ(φ)FmJm(τ) or to −(Fm + tβ(φ)FmJm(τ)).
In
fact, using the canonical form of φ-skewsymmetric matrices (Theorem 4.1.2), one
can easily see that the sign here must be −1 if m −1 is not divisible by 4 and +1
otherwise.
□

290
CHAPTER 12
12.5
STRICT EQUIVALENCE VERSUS φ-CONGRUENCE
In this section we develop some applications of the canonical form of Theorem
12.3.1 regarding the relations between strict equivalence and φ-congruence for φ-
hsk matrix pencils.
Clearly, φ-congruent φ-hsk matrix pencils are strictly equivalent, but the con-
verse is generally false. It turns out that every strict equivalence class contains
only ﬁnitely many φ-congruent classes (when restricted to φ-hsk pencils), and the
number of these can be identiﬁed in terms of the Kronecker form of the pencils.
Theorem 12.5.1. Let A+tB be a φ-hsk matrix pencil, where A, B ∈Hn×n. Let
λ1 = 0,
λ2 = ∞,
λ3 ̸∈{0, ∞}, . . . , λr ̸∈{0, ∞}
be all the distinct eigenvalues of A + tB, including the zero eigenvalue and inﬁnity,
if applicable, having the following properties:
(1) The real parts of λ3, . . . , λr are zeros.
(2) The norms of the vector parts |V(λ3)|, . . . , |V(λr)| are all distinct.
Let k1,1 < · · · < k1,p1 be the distinct odd indices of the eigenvalue 0 of A + tB,
let k2,1 < · · · < k2,p2 be the distinct even indices at inﬁnity of A + tB, and for
j = 3, 4, . . . , r, let kj,1 < · · · < kj,pj be the distinct indices of the eigenvalue λj.
Furthermore, assume that the index kj,m appears qj,m times in the Kronecker form
of A + tB; i.e., there are exactly qj,m blocks tIkj,m + Jkj,m(λj) in the Kronecker
form of A + tB (kj,m is odd if j = 1) for m = 1, 2, . . . , pj and j = 1, 3, 4, . . . , r,
and there are exactly q1,m blocks Ik1,m + tJk1,m(0) of an even size k1,m. Then there
exist
w :=
r
Y
j=1
pj
Y
m=1
(qj,m + 1)
(12.5.1)
φ-hsk mutually pairwise non-φ-congruent pencils A1 + tB1, . . . , Aw + tBw, each of
which is strictly equivalent to A + tB. Moreover, there do not exist w + 1 φ-hsk
mutually pairwise non-φ-congruent pencils, each of which is strictly equivalent to
A + tB.
The proof follows easily from Theorem 12.3.1; indeed, for ﬁxed j and m, as in
Theorem 12.5.1, there are exactly qj,m + 1 mutually nonequivalent ways to assign
the signs corresponding to the blocks of size kj,m with eigenvalue λj in the sign
characteristic of A + tB.
Two corollaries of Theorem 12.5.1 are worthy of separate statements.
Corollary 12.5.2. A φ-hsk quaternion matrix pencil A+tB has the property that
every φ-hsk quaternion pencil that is strictly equivalent to A + tB is automatically
φ-congruent to A + tB if and only if A + tB has no even indices at inﬁnity, no odd
indices corresponding to the zero eigenvalue, and no nonzero eigenvalues having
zero real parts.
Corollary 12.5.3. The maximal number of elements in a set of n×n quaternion
matrix pencils which are strictly equivalent to each other, but mutually pairwise
non-φ-congruent, is equal to 2n.
For the proof of Corollary 12.5.3 observe that for a ﬁxed n, the maximal value
of w in (12.5.1) is 2n, which is attained for any φ-hsk pencil A + tB with n nonreal
eigenvalues with distinct norms of their vector parts.

MIXED MATRIX PENCILS: NONSTANDARD INVOLUTIONS
291
12.6
CANONICAL FORMS OF MATRICES UNDER
φ-CONGRUENCE
As another application of Theorem 12.3.1, we derive a canonical form of matrices
A ∈Hn×n under the φ-congruence relation; recall that two matrices X, Y ∈Hn×n
are called φ-congruent if Y = SφXS for some invertible S ∈Hn×n.
Using the
decomposition
A = B + C,
B = Bφ,
C = −Cφ,
where
B = Aφ + A
2
,
C = −Aφ + A
2
,
the problem reduces to the problem of a canonical form of the φ-hsk pencil B +tC.
It will be convenient to work with suitably modiﬁed blocks (q-φ-h-sk1)–(q-φ-h-
sk6).
(q-φ-h-sk1′)
G2ε+1 +


0
0
Fε
0
01
0
−Fε
0
0

,
ε positive integer.
(q-φ-h-sk2′)
Fk + β(φ)Gk,
k positive integer.
(q-φ-h-sk3′)
Gk + β(φ)Fk,
k positive integer.
(q-φ-h-sk4′)
"
0
(1 + α)F ℓ
2 + G ℓ
2
(−1 + α)F ℓ
2 + G ℓ
2
0
#
,
where ℓis even, α ∈Inv (φ), R(α) > 0.
(q-φ-h-sk5′)
Λs(ρ) :=


0
0
. . .
0
−β(φ)
Υ
0
0
. . .
β(φ)
−Υ
0
...
...
...
...
...
...
(−1)s−1β(φ)
(−1)s−2Υ
. . .
0
0
0
(−1)s−1Υ
0
. . .
0
0
0


,
(12.6.1)
where the size of the matrix is s × s, ρ is real positive, and s may be
even or odd; we have denoted here Υ := ρ + β(φ).
In the forms (q-φ-h-sk1′)–(q-φ-h-sk5′) modiﬁcations are possible, analogous to the
modiﬁcations in the forms (q-φ-h-sk1)–(q-φ-h-sk6) discussed in Section 12.3.
Invoking Theorem 12.3.1, the following result is obtained.
Theorem 12.6.1. Every matrix A ∈Hm×m is φ-congruent to a matrix in the
form
0u×u
⊕
(A0 + tB0) ⊕⊕p
j=1εj(Fkj + β(φ)Gkj)
⊕
⊕q
j=1κj(Gℓj + β(φ)Fℓj) ⊕⊕r
j=1δjΛsj(ρj),
(12.6.2)
where the parameters have the following properties:

292
CHAPTER 12
(1) the integers kj’s are even, and the ℓj’s are odd;
(2) the εj’s, κj’s, and δj’s are signs ±1;
(3) the ρj’s are positive reals;
(4) the part A0 + tB0 is block diagonal with the diagonal blocks consisting of
blocks of the types (q-φ-h-sk1′), (q-φ-h-sk4′), (q-φ-h-sk2′) of odd sizes, and
(q-φ-h-sk3′) of even sizes.
The form (12.6.2) is uniquely determined by the pencil A, up to a permutation of
the diagonal blocks in each of the four parts
A0 + tB0,
⊕p
j=1εj(Fkj + β(φ)Gkj),
⊕q
j=1κj(Gℓj + β(φ)Fℓj),
and
⊕r
j=1 δjΛsj(ρj),
and up to a replacement, in each block of type (q-φ-h-sk4′), the quaternion α with
a similar quaternion α′ ∈Inv (φ).
An alternative statement can be given using Theorem 12.3.5 rather than The-
orem 12.3.1. The only change in Theorem 12.6.1 would be that the block Λsj(ρj)
is replaced with Fsj + β(φ)FsjJsj(ρj).
12.7
COMPARISON WITH REAL AND COMPLEX MATRICES
We compare the strict equivalence and φ-congruence relations for real, complex, and
quaternion matrix pencils and begin with real matrix pencils. It will be eﬃcient to
deal with several symmetries of real matrix pencils at once.
Deﬁnition 12.7.1. We say that complex matrix pencils A + tB and A′ + tB′
are (C,T )-congruent if A + tB = ST (A′ + tB′)S for some invertible complex matrix
S.
Theorem 12.7.2. Fix η = ±1, τ = ±1. Let A, B, A′, B′ ∈Rm×m be such that
AT = ηA, (A′)T = ηA′,
BT = τB, (B′)T = τB′.
(12.7.1)
Then statements (i), (ii), and (iii) are equivalent.
(i) The matrix pencils A + tB and A′ + tB′ are R-strictly equivalent.
(ii) The matrix pencils A + tB and A′ + tB′ are (C,T )-congruent.
(iii) The matrix pencils A+tB and A′+tB′ are φ-congruent for some nonstandard
involution φ, equivalently, for all nonstandard involutions φ.
Proof. Clearly, (iii) implies that A + tB and A′ + tB′ are H-strictly equivalent,
which, in turn, yields (i) (in view of Theorem 7.6.3). To see that (ii) implies (iii),
identify C with SpanR {1, q}, where q ∈H \ {0} has zero real part and φ(q) = q.
It remains to prove that (i) implies (ii). So, assume (i) holds. The canoni-
cal forms of symmetric, skewsymmetric, or mixed symmetric/skewsymmetric real
matrix pencils under R-strict equivalence and under R-congruence are given in The-
orems 15.2.1, 15.2.2, and 15.2.3. It follows from these theorems that there exist
real invertible matrices S and T and pairs of matrices
Aj, Bj ∈Rnj×nj,
n1 + · · · + np = m,

MIXED MATRIX PENCILS: NONSTANDARD INVOLUTIONS
293
such that
ST (A + tB)S
=
⊕p
j=1δj(Aj + tBj),
T T (A′ + tB′)T
=
⊕p
j=1ξj(Aj + tBj),
where for each j, the pair (Aj, Bj) satisﬁes
AT
j = ηAj,
BT
j = τBj,
and δ1, . . . , δp, ξ1, . . . , ξp are signs ±1. It suﬃces to show that
⊕p
j=1δj(Aj + tBj)
and
⊕p
j=1 ξj(Aj + tBj)
are (C,T )-congruent. This is easy:
⊕p
j=1δj(Aj + tBj) =
 ⊕p
j=1Wj
  ⊕p
j=1ξj(Aj + tBj)
  ⊕p
j=1Wj

,
where Wj = Inj if δj = ξj and Wj = iInj if δj ̸= ξj.
□
Consider now comparison with pairs of complex matrices. In the rest of this
section we identify (as usual) C with SpanR {1, i} ⊂H. and we assume that the
nonstandard involution φ is such that β(φ) ∈SpanR {j, k}.
We obviously have the following.
Proposition 12.7.3. If two complex matrix pencils A + tB and A′ + tB′ are
(C,T )-congruent, then they are also φ-congruent (over H).
A key result of this section states that for complex pencils of symmetric or
skewsymmetric matrices, H-strict equivalence is the same as φ-congruence.
Theorem 12.7.4. Fix η = ±1, τ = ±1. Let A, B, A′, B′ ∈Cm×m be such that
AT = ηA,
(A′)T = ηA′,
BT = τB,
(B′)T = τB′.
Then the matrix pencils A + tB and A′ + tB′ are H-strictly equivalent if and only
if they are φ-congruent.
The lengthy proof of Theorem 12.7.4 is relegated to the next section.
A complete characterization of canonical forms of complex symmetric matrix
pencils that are φ-congruent to a ﬁxed complex symmetric matrix pencil in a canon-
ical form is given in the next theorem.
Theorem 12.7.5. Let A + tB be a complex symmetric matrix pencil, and let
0u
⊕

t


0
0
Fε1
0
0
0
Fε1
0
0

+ G2ε1+1


⊕
· · · ⊕

t


0
0
Fεp
0
0
0
Fεp
0
0

+ G2εp+1


⊕
(Fk1 + tGk1) ⊕· · · ⊕(Fkr + tGkr)
⊕
((t + α1) Fℓ1 + Gℓ1) ⊕· · · ⊕
 (t + αq) Fℓq + Gℓq

,
α1, . . . , αq ∈C,
(12.7.2)

294
CHAPTER 12
be its canonical form under (C,T )-congruence (cf. Theorem 15.3.3). Then a complex
symmetric matrix pencil A′+tB′ is φ-congruent to A+tB if and only if the canonical
form of A′ + tB′ under (C,T )-congruence is obtained from (12.7.2) by replacing
some (or none) of the nonreal numbers αj among α1, . . . , αq with their complex
conjugates αj.
For the proof observe that the canonical form of A + tB under φ-congruence is
(12.7.2), up to permutation of blocks and replacement of each αj with its complex
conjugate (see Theorem 11.1.3).
Thus, a φ-congruence class of a complex symmetric matrix pencil A+tB consists
of exactly u classes of (C,T )-congruence, where the number u is computed as follows.
For λ, a complex number with positive imaginary part, and a positive integer ℓ,
let es(λ, ℓ) be the total number of blocks of the form (t + α)Fℓ+ Gℓwith α = λ or
α = λ in the canonical form (12.7.2) of A + tB under (C,T )- congruence. (If there
are no such blocks, we set es(λ, ℓ) = 0.) Then
u =
Y
ℓ>0
Y
{λ∈C : I (λ)>0}
(es(λ, ℓ) + 1).
(12.7.3)
The proof of formula (12.7.3) is obtained in a manner similar to the proof of The-
orem 9.7.5.
We leave it to the reader to formulate and prove results analogous to Theorem
12.7.5 for skewsymmetric complex matrix pencils and mixed matrix pencils—in
other words, symmetric/skewsymmetric (Ex. 12.9.1).
12.8
PROOF OF THEOREM 12.7.4
Clearly, we need to prove only the “only if” part. In the case τ = η = 1, this is
clear in view of fact that φ-hermitian quaternionic pencils are H-strictly equivalent
if and only if they are φ-congruent (Theorem 11.1.3).
Consider the case τ = η = −1. Assume that A + tB and A′ + tB′ are H-strictly
equivalent. Since by Theorem 15.3.2 the relation of C-strict equivalence of complex
skewsymmetric pencils is the same as the relation of (C,T )-congruence, we may
further replace A + tB with its canonical form under (C,T )-congruence. In other
words, we assume that A + tB is given by
0u×u
⊕
⊕p
j=1

t


0
0
Fεj
0
01
0
−Fεj
0
0

+


0
Fεj
0
−Fεj
0
0
0
0
0




⊕
⊕r
j=1

0
Fkj
−Fkj
0

+ t

0
Gkj
−Gkj
0

⊕
⊕q
j=1

(t + αj)

0
Fℓj
−Fℓj
0

+

0
Gℓj
−Gℓj
0

,
(12.8.1)
where αj ∈C. Note the following φ-congruence relations (recall that we assume
β(φ) ∈SpanR(j, k)):
(Skj)φ

0
Fkj + tGkj
−Fkj −tGkj
0

Skj
= (−(β(φ)Fkj + tβ(φ)Gkj)) ⊕(β(φ)Fkj + tβ(φ)Gkj);
(12.8.2)

MIXED MATRIX PENCILS: NONSTANDARD INVOLUTIONS
295
(Sℓj)φ

(t + αj)

0
Fℓj
−Fℓj
0

+

0
Gℓj
−Gℓj
0

Sℓj
= (−
 (t + αj)β(φ)Fℓj + β(φ)Gℓj

) ⊕
 (t + αj)β(φ)Fℓj + β(φ)Gℓj

,
(12.8.3)
αj ∈R,
where
Sm =
1
√
2

β(φ)Im
−β(φ)Im
Im
Im

,
(Sm)φ =
1
√
2

−β(φ)Im
Im
β(φ)Im
Im

.
Comparing with the canonical form under φ-congruence (Theorem 11.2.2), we see
that the canonical form of A + tB under φ-congruence is given by (12.8.1), where
each block

0
Fkj
−Fkj
0

+ t

0
Gkj
−Gkj
0

is replaced with the right-hand side of (12.8.2), and each block
(t + αj)

0
Fℓj
−Fℓj
0

+

0
Gℓj
−Gℓj
0

is replaced with the right-hand side of (12.8.3). In other words, the blocks in the
canonical form of A+tB under φ-congruence that correspond to the real eigenvalues
and to inﬁnity appear in pairs, and in each such pair the two blocks have opposite
signs. Of course, the same property is valid also for the canonical form of A′ + tB′
under φ-congruence. Notice that A + tB and A′ + tB′ have the same canonical
form under H-strict equivalence and that the canonical forms of quaternion φ-
skewhermitian (with respect to a nonstandard involution) matrix pencils under
φ-congruence and under H-strict equivalence can diﬀer only in the signs associated
with blocks corresponding to the real eigenvalues and to the eigenvalue at inﬁnity
(see Theorem 11.2.2). We obtain, therefore, that A + tB and A′ + tB′ have the
same canonical form under φ-congruence. In other words, A + tB and A′ + tB′ are
φ-congruent, as claimed.
Finally, consider the case η = 1, τ = −1 (the remaining case η = −1, τ = 1
can be easily reduced to the case under consideration by interchanging the roles of
A and B). First of all, we will transform the primitive blocks (q-φ-h-sk2)–(q-φ-h-
sk6) into diﬀerent forms using φ-congruence, so that the obtained forms are easily
comparable to the canonical form of quaternion φ-hermitian/skewhermitian matrix
pencils under φ-congruence.
Claim 12.8.1. The block (q-φ-h-sk5) is φ-congruent to
(G + tβ(φ)F) ⊕(−(G + tβ(φ)F)),
(12.8.4)
where we let G = Gℓ/2, F = Fℓ/2, and recall that ℓ/2 is odd.
For the proof of the claim, consider the matrix pencil
 0
G
G
0

+ t

0
−β(φ)F
β(φ)F
0

.
(12.8.5)

296
CHAPTER 12
The matrix pencil (12.8.5) may be considered as a pencil of complex hermitian
matrices under the real linear map Ψ of SpanR {1, β(φ)} onto C via 1
7→1 and
β(φ) 7→i. Transformations of the matrix pencil (12.8.5) of the form

0
G
G
0

+ t

0
−β(φ)F
β(φ)F
0

7→
Sφ
 0
G
G
0

+ t

0
−β(φ)F
β(φ)F
0

S,
where S is an invertible matrix with entries in SpanR {1, β(φ)}, amount to C-
congruences under the map Ψ.
Next, we verify that the canonical form under
C-congruence of (12.8.5), understood as a pencil of complex hermitian matrices, is
equal to
(G + tF) ⊕(−(G + tF)).
(12.8.6)
Indeed, since the C-Kronecker form of (12.8.5) (again, under the map Ψ) is (tI +
Jℓ/2(0)) ⊕(tI + Jℓ/2(0)), it follows from Theorem 15.3.1 that the canonical form of
(12.8.5) under C-congruence is
η1(G + tF) ⊕η2(G + tF),
where η1, η2 are signs ±1.
However, the case η1η2 = 1 is impossible, because
if η1η2 = 1 holds, then for large real values of t, the signature (the diﬀerence
between the number of positive eigenvalues, counted with multiplicities, and the
number of negative eigenvalues, counted with multiplicities) of the hermitian matrix
η1(G + tF) ⊕η2(G + tF) is not zero (this is where the hypothesis that ℓ/2 is odd is
used), whereas for the hermitian matrix (12.8.5) the signature is equal to zero for
all real t, a contradiction with the inertia theorem for hermitian matrices. Thus,
the canonical form of (12.8.5) under C-congruence must be (12.8.6). In particular,
Sφ
 0
G
G
0

S
=
 G
0
0
−G

,
Sφ

0
−β(φ)F
β(φ)F
0

S
=

F
0
0
−F

for some invertible matrix S with entries in SpanR {1, β(φ)}. Thus,
Sφ

0
F
−F
0

S =

β(φ)F
0
0
−β(φ)F

,
and the claim follows.
□
In a completely analogous way, the next claim is veriﬁed:
Claim 12.8.2.
(a) The block (q-φ-h-sk2) is φ-congruent to Fk + tβ(φ)Gk; re-
call that k is odd.
(b) The block (q-φ-h-sk3) is φ-congruent to
(F k
2 + tβ(φ)G k
2 ) ⊕(−(F k
2 + tβ(φ)G k
2 ));
(12.8.7)
recall that k/2 is even.

MIXED MATRIX PENCILS: NONSTANDARD INVOLUTIONS
297
(c) The block (q-φ-h-sk4) is φ-congruent to Gℓ+ tβ(φ)Fℓ; recall that ℓis even.
Our ﬁnal claim concerns (q-φ-h-sk6) with α ∈C \ {0} having zero real part.
Claim 12.8.3. The block

0
αFq + Gq
αFq + Gq
0

+ t

0
Fq
−Fq
0

,
where α ∈C \ {0} has zero real part, is φ-congruent to a block of the form
(A0 + tB0) ⊕(−(A0 + tB0)),
where A0 = (A0)φ ∈Hq×q and B0 = −(B0)φ ∈Hq×q. Moreover, the H-Kronecker
form of the quaternion pencil A0 + tB0 consists of only one Jordan block of size
q × q with eigenvalue α (or any similar eigenvalue).
Proof of Claim 12.8.3. First of all, notice the equality (recall that β(φ) ∈
SpanR {j, k}, α ∈SpanR {i})
αβ(φ) = −β(φ)α.
(12.8.8)
Deﬁne the matrix
Z = diag (1, −1, . . . , (−1)q−1).
(12.8.9)
We obviously have Z−1 = ZT = Z.
Assume ﬁrst that q is odd. Then
Z(FqGq)Z = −FqGq,
Z = FqZFq,
ZGqZ = −Gq.
One now veriﬁes that
 β(φ)Iq
−β(φ)Z
Z
Iq

φ

0
αFq + Gq
αFq + Gq
0

+ t

0
Fq
−Fq
0

·
 β(φ)Iq
−β(φ)Z
Z
Iq

= 2(αβZFq + ZGqβ(φ) −(β(φ)ZFq)t)
⊕2(−αβ(φ)ZFq + ZGqβ(φ) + (β(φ)ZFq)t),
and
Z(−αβ(φ)ZFq + ZGqβ(φ) + (β(φ)ZFq)t)Z
= −(αβ(φ)ZFq + ZGqβ(φ) −(β(φ)ZFq)t).
Assume now that q is even. Then
ZFqZ = −Fq,
ZGqZ = Gq,
where Z is deﬁned by (12.8.9). Since φ is a nonstandard involution, it is easy to
see that there exists γ ∈H such that γ2 = −1, αγ = −γα, and φ(γ) = γ. Now, a
straightforward veriﬁcation shows that
 Iq
γZ
γZ
Iq

φ

0
αFq + Gq
αFq + Gq
0

+ t

0
Fq
−Fq
0
  Iq
γZ
γZ
Iq

= (2(γαZFQ + γZGq −γtZFq)) ⊕(2(γαZFq + γZGq + γtZFq)),

298
CHAPTER 12
and, furthermore (note that (γZ)φ = γZ),
(γZ)(γαZFq + γZGq + γtZFq)(γZ) = −(γαZFq + γZGq −γtZFq).
This completes veriﬁcation of Claim 12.8.3.
□
Assume now that the complex pencils A + tB and A′ + tB′, where A = AT ,
B = −BT , A′ = A′T , and B′ = −B′T , are H-strictly equivalent. Since, by Theorem
15.3.2, the relation of C-strict equivalence of complex symmetric-skewsymmetric
pencils is the same as the relation of (C,T )-congruence, we may replace A + tB
with its canonical form under (C,T )-congruence; in other words, we may assume
that A + tB is a direct sum of primitive blocks of types (q-φ-h-sk0)–(q-φ-h-sk6).
In view of Claims 12.8.1, 12.8.2, and 12.8.3, under the φ-congruence, the blocks
with eigenvalue zero having odd sizes, the blocks with eigenvalue at inﬁnity having
even sizes, and the blocks with nonzero complex eigenvalues having zero real parts
appear in pairs with opposite signs for each of the two blocks in every such pair.
The same statement holds for A′ + tB′ as well.
Now observe that the canonical form under φ-congruence and the canonical
form under H-strict equivalence of a quaternionic matrix pencil
X + tY,
X = Xφ ∈Hm×m,
Y = −Yφ ∈Hm×m,
(12.8.10)
φ nonstandard involution
may diﬀer, apart from a permutation of blocks, only in signs ±1 that attached pre-
cisely to the blocks with eigenvalue zero having odd sizes, the blocks with eigenvalue
at inﬁnity having even sizes, and the blocks with nonzero eigenvalues having zero
real parts (Theorems 12.1.2 and 12.3.1). In view of the statement in the preceding
paragraph, we are done.
□
12.9
EXERCISES
Ex. 12.9.1. State and prove the analogues of Theorem 12.7.5 for skewsymmetric
complex matrix pencils and symmetric/skewsymmetric complex matrix pencils.
Ex. 12.9.2. Consider the following property of a φ-hsk matrix pencil A + tB:
(A) If A′ + tB′ is a φ-hsk matrix pencil which is strictly equivalent to A + tB,
then A′ + tB′ is φ-congruent to A + tB.
Prove that if A + tB has property (A) and if both A and B are invertible, then
there is ε > 0 such that every φ-hsk matrix pencil A′ + tB′ also has property (A),
provided ∥A′ −A∥+ ∥B′ −B∥< ε.
Ex. 12.9.3. Give examples of φ-hsk matrix pencils A + tB for which the result
of Ex. 12.9.2 fails if:
(a) only A is assumed invertible;
(b) only B is assumed invertible.
Ex. 12.9.4. Find all possible canonical forms (12.3.1) (or (12.3.5)) under each
of the following conditions:
(1) rank (xA + yB) = 3 for all real x, y not both zero;

MIXED MATRIX PENCILS: NONSTANDARD INVOLUTIONS
299
(2) rank (xA + yB) ≤2 for all real x and y;
(3) rank B = 2.
Ex. 12.9.5. Find all possible canonical forms (12.3.1) (or (12.3.5)) of the φ-hsk
n × n matrix pencil A + tB under each of the following conditions:
(1) B is β(φ)-positive deﬁnite.
(2) B is β(φ)-positive semideﬁnite.
(3) The β(φ)-inertia of B is (n −2, 1, 1).
Ex. 12.9.6. Let A+tB be a φ-hsk matrix pencil and assume that B is invertible
and the sum of algebraic multiplicities of A + tB corresponding to the eigenvalues
with zero real parts of A + tB does not exceed 2.
(a) Show that there exist not more than 4 mutually pairwise non-φ-congruent
φ-hsk matrix pencils such that each of them strictly equivalent to A + tB.
(b) Prove that there is ε > 0 such that every φ-hsk matrix pencil A′ + tB′ also
has the propery described in Part (a), provided ∥A′ −A∥+ ∥B′ −B∥< ε.
Ex. 12.9.7. State and prove generalization of Ex. 12.9.6 for φ-hsk matrix pencils
A+tB with invertible B and whose sum of algebraic multiplicities corresponding to
the eigenvalues with zero real parts of does not exceed k, for a ﬁxed integer k ≥2.
Ex.
12.9.8. Find the canonical forms of the following φ-hsk matrix pencils
A + tB under strict equivalence and under φ-congruence:
(a)

tβ(φ)
β(φ) + taq
−β(φ) −taq
tβ(φ)

;
(b)


q + tβ(φ)
q
0
q
q + tβ(φ)
0
0
0
tβ(φ)

.
Here a is a real parameter and the nonzero quaternion q is such that β(φ)q =
−qβ(φ).
Ex.
12.9.9. (a)
Find the canonical form of the matrix
 i
j
0
k

under φ-
congruence, where the nonstandard involution φ is such that φ(i) = −i, φ(j) = j,
φ(k) = k.
(b) Repeat (a), but now, with φ such that φ(i) = i, φ(j) = −j, φ(k) = k.
12.10
NOTES
The contents of this chapter are taken from Rodman [133] and [134]. Exposition
and presentation in the chapter, as well as many proofs, follow the two papers.
The results of Theorems 12.1.2 and 12.3.1 were known before; see, e.g., Djokovi´c
et al. [34] (for the case when at least one of the matrices A or B is invertible).

Chapter Thirteen
Indeﬁnite inner products: Nonstandard involution
In this chapter we ﬁx a nonstandard involution φ.
In parallel with Chapter 10, we introduce indeﬁnite inner products deﬁned on
Hn×1 of the symmetric and skewsymmetric types associated with φ and matrices
having symmetry properties with respect to one of these indeﬁnite inner products.
The symmetric-type inner product is a function
[·, ·](φ) : Hn×1 × Hn×1 −→H
(the superscript (φ) indicates that the inner product is associated with φ, in contrast
with the inner product of Chapter 10) with the following properties:
(1′) Linearity in the ﬁrst argument:
[x1α1 + x2α2, y](φ) = [x1, y](φ)α1 + [x2, y](φ)α2
for all x1, x2, y ∈Hn×1 and all α1, α2 ∈H.
(2′) Symmetry: [x, y](φ) = φ([y, x](φ)) for all x, y ∈Hn×1.
(3′) Nondegeneracy: if x0 ∈Hn×1 is such that [x0, y](φ) = 0 for all y ∈Hn×1, then
x0 = 0.
The skewsymmetric-type inner product [·, ·](φ) is deﬁned by the properties (1′), (3′),
and
(2′′) antisymmetry: [x, y](φ) = −φ([y, x](φ)) for all x, y ∈Hn×1.
It follows from (1′) and (2′), or from (1′) and (2′′), that
[x, y1α1 + y2α2](φ) = φ(α1)[x, y1] + φ(α2)[x, y2]
for all x, y1, y2 ∈Hn×1 and all α1, α2 ∈H.
In complete analogy with Proposition 10.0.1 we have the following.
Proposition 13.0.1. [·, ·](φ) is an inner product on Hn×1 of symmetric-, resp.
skewsymmetric-, type if and only if there exists a φ-hermitian, resp. φ-skewhermi-
tian, invertible n × n matrix H such that
[x, y](φ) = yφHx,
for all x, y ∈Hn×1.
Such a matrix H is uniquely determined by the inner product.
The proof is essentially the same as that of Proposition 10.0.1 and is omitted
(Ex. 13.7.1).
In this chapter we study indeﬁnite inner products associated with φ of the
symmetric- and skewsymmetric-type and matrices having symmetry properties with

INDEFINITE INNER PRODUCTS: NONSTANDARD INVOLUTION
301
respect to one of these indeﬁnite inner products. As in Chapter 10, we will often
work with matrices H rather than directly with indeﬁnite inner products.
The development in this chapter is often parallel to that of Chapter 10, but here
the indeﬁnite inner products are with respect to a nonstandard involution, rather
with respect to the conjugation as in Chapter 10.
We develop canonical forms
for (H, φ)-symmetric and (H, φ)-skewsymmetric matrices (when the inner product
is of the symmetric-type), and canonical forms of (H, φ)-Hamiltonian and (H, φ)-
skew-Hamiltonian matrices (when the inner product is of the skewsymmetric-type).
Applications include invariant Lagrangian subspaces and systems of diﬀerential
equations with symmetries.
13.1
CANONICAL FORMS: SYMMETRIC INNER PRODUCTS
Let H ∈Hn×n be an invertible φ-hermitian matrix. Recall that a matrix X ∈
Hn×n is called (H, φ)-symmetric if the equality [x, Xy]H,φ = [Xx, y]H,φ holds for
all x, y ∈Hn×1, where [·, ·]H,φ stands for the inner product of symmetric-type
induced by H: [x, y]H,φ = yφHx, x, y ∈Hn×1. It is easy to see that X is (H, φ)-
symmetric if and only if the equation HX = XφH holds true—in other words,
HX is φ-hermitian. A matrix A ∈Hn×n is said to be (H, φ)-skewsymmetric if the
equality [x, Ay]H,φ = −[Ax, y]H,φ holds true for all x, y ∈Hn×1 or, equivalently, if
HA is φ-skewhermitian.
In this chapter we write H-symmetric and H-skewsymmetric, short for (H, φ)-
symmetric and (H, φ)-skewsymmetric, respectively.
The next theorem describes the canonical form for pairs (H, X), where H is
invertible φ-hermitian and X is H-symmetric. Recall that Inv (φ) stands or the set
(real vector space) of quaternions that are ﬁxed by φ.
Theorem 13.1.1. Let H ∈Hn×n be an invertible matrix such that H = Hφ,
and let A be H-symmetric. Then there exists an invertible matrix S ∈Hn×n such
that the matrices S−1AS and SφHS have the form
SφHS = Fℓ1 ⊕· · · ⊕Fℓq,
S−1AS = Jℓ1(α1) ⊕· · · ⊕Jℓq(αq),
(13.1.1)
where α1, . . . , αq ∈Inv (φ). Moreover, the form (13.1.1) is unique up to an arbitrary
permutation of the diagonal blocks and up to replacement of αj in each block Jℓj(αj)
with a similar quaternion βj ∈Inv (φ).
Conversely, if H and A have the form (13.1.1), then H is φ-hermitian and
invertible and A is H-symmetric.
Proof.
The converse part is checked by a straightforward veriﬁcation.
For
the direct part we apply Theorem 11.1.3 to the φ-pencil HA + tH. Because H is
invertible, only blocks (t + αj)Fℓj + Gℓj can appear in the canonical form (11.1.1)
of HA + tH. Since A = H−1(HA), this leads to formula (13.1.1), but with Jℓj(αj)
replaced with Jℓj(αj)T . Then the transformation
Fℓj 7→(Fℓj)φFℓjFℓj = Fℓj,
Jℓj(αj)T 7→F −1
ℓj Jℓj(αj)T Fℓj = Jℓj(αj)
puts the pair (H, A) in the required form (13.1.1). The uniqueness statement in
Theorem 13.1.1 follows from that of Theorem 11.1.3 applied to the pencil HA +
tH.
□

302
CHAPTER 13
Next, we present the canonical form for H-skewsymmetric matrices. It will be
convenient to identify the primitive forms ﬁrst. As before, we ﬁx β(φ) ∈H, such
that φ(β(φ)) = −β(φ) and |β(φ)| = 1.
(q-φ-H-sk2)
H = εFk, A = β(φ)Jk(0), where ε = 1 if k is odd and ε = ±1
if k is even.
(q-φ-H-sk4)
H =
 0
Fℓ
Fℓ
0

,
A =
 Jℓ(α)
0
0
−Jℓ(α)

,
where α ∈H satisﬁes the conditions φ(α) = α and R(α) > 0.
(q-φ-H-sk5)
H = δFs, A = β(φ)Js(τ), where τ is positive real and δ = ±1.
Alternatively, one can assume that τ is negative real, rather than positive real, in
(q-φ-H-sk5). Indeed, if γ ∈H is such that γ−1β(φ)γ = −β(φ) and |γ| = 1, then
 diag (γ−1, −γ−1, . . . , (−1)s−1γ−1)

β(φ)Js(τ)
·
 diag (γ, −γ, . . . , (−1)s−1γ)

= β(φ)Js(−τ),
(13.1.2)
and
 diag (φ(γ), −φ(γ), . . . , (−1)s−1φ(γ))

Fs
· (diag (γ, −γ, . . . , (−1)sγ)) = (−1)s−1Fs,
where τ > 0. Note that γ2 = −1 and φ(γ) = γ. Thus, the replacement of τ by −τ
in (q-φ-H-sk5) will reverse the sign δ if s is odd and leave the sign δ invariant if s
is even.
Also, the condition R(α) > 0 in (q-φ-H-sk4) may be replaced by R(α) < 0.
This follows from the formulas

0ℓ
diag (1, −1, . . . , (−1)ℓ−1)
diag (1, −1, . . . , (−1)ℓ−1)
0ℓ

·
 Jℓ(−α)
0
0
−Jℓ(−α)

·

0ℓ
diag (1, −1, . . . , (−1)ℓ−1)
diag (1, −1, . . . , (−1)ℓ−1)
0ℓ

=
 Jℓ(α)
0
0
−Jℓ(α)

,

0ℓ
diag (1, −1, . . . , (−1)ℓ−1)
diag (1, −1, . . . , (−1)ℓ−1)
0ℓ
  0
Fℓ
Fℓ
0

·

0ℓ
diag (1, −1, . . . , (−1)ℓ−1)
diag (1, −1, . . . , (−1)ℓ−1)
0ℓ

= (−1)ℓ−1
 0
Fℓ
Fℓ
0

,

INDEFINITE INNER PRODUCTS: NONSTANDARD INVOLUTION
303
and for ℓeven we apply further transformation

I
0
0
−I
 
−

0
Fℓ
Fℓ
0
 
I
0
0
−I

=

0
Fℓ
Fℓ
0

,
 I
0
0
−I
  Jℓ(α)
0
0
−Jℓ(α)
  I
0
0
−I

=
 Jℓ(α)
0
0
−Jℓ(α)

.
Theorem 13.1.2. Let A ∈Hn×n be H-skewsymmetric. Then there exists an
invertible quaternion matrix S such that SφHS and S−1AS have the following
block diagonal form:
SφHS = H1 ⊕H2 ⊕· · · ⊕Hm,
S−1AS = A1 ⊕A2 ⊕· · · ⊕Am,
(13.1.3)
where each pair (Hi, Ai) has one of the forms (q-φ-H-sk2), (q-φ-H-sk4), (q-φ-H-
sk5). Moreover, the form (13.1.3) is uniquely determined by the pair (H, A), up to
a permutation of blocks and up to a replacement in each block of the form (q-φ-H-
sk4), the quaternion α with a similar quaternion α′ such that φ(α′) = α′.
Conversely, if H and A are given by (13.1.3), then H is φ-hermitian and A is
H-skewsymmetric.
Thus, the sign characteristic of a pair (H, A), where A is H-skewsymmetric, at-
taches a sign ±1 to every partial multiplicity of A associated with nonzero eigenval-
ues having zero real parts (if any) and to every even partial multiplicity associated
with the eigenvalue 0 (if A is not invertible).
A technical lemma will be needed for the proof of Theorem 13.1.2, as well as in
later proofs.
Lemma 13.1.3. (a) If x ∈C \ {0}, then there exists a (necessarily invertible)
matrix S ∈Cm×m such that equations
ST (xFm + Gm)S = Fm
(13.1.4)
and
((Jm(x))T )−1S = S(Jm(x−1))T
(13.1.5)
are satisﬁed.
(b) If x ∈R \ {0}, then there exists a matrix S ∈Rn×n such that equations
ST (xFm + Gm)S = ±Fm
(13.1.6)
and (13.1.5) are satisﬁed, with the sign +1 if x > 0 and m odd or if x < 0 and m
even and with the sign −1 if x > 0 and m even or if x < 0 and m odd.
Proof. Proof of Part (a). We exhibit S in the following form: S = [sj,k]m
j,k=1
is lower triangular; sj,k = 0 if j < k, and for j ≥k, the entries sj,k have the form
sj,k = fj,kx−j−k+2s1,1,
fj,k ∈C.
The constants fj,k and s1,1 are to be determined so that equalities (13.1.4) and
(13.1.5) are satisﬁed. We rewrite (13.1.5) as follows:


0
0
· · ·
· · ·
0
−x−2
0
· · ·
· · ·
0
x−3
−x−2
· · ·
· · ·
0
...
...
...
...
...
(−1)m−1x−m
(−1)m−2x−m+1
· · ·
−x−2
0


S

304
CHAPTER 13
= S


0
0
· · ·
· · ·
0
1
0
· · ·
· · ·
0
0
1
· · ·
· · ·
0
...
...
...
...
...
0
0
· · ·
1
0


=


0
0
· · ·
· · ·
0
s2,2
0
· · ·
· · ·
0
s3,2
s3,3
· · ·
· · ·
0
...
...
...
...
...
sm,2
sm,3
· · ·
sm,m
0


.
(13.1.7)
To start with, we let
fj,j = (−1)j−1,
j = 1, 2, . . . , m.
(13.1.8)
It is easy to see that (13.1.7) amounts, in view of (13.1.8), to the equalities
(−1)k−jfj,j + · · · + (−1)fk−1,j = fk,j+1,
for
k > j + 1,
(13.1.9)
which determine uniquely all elements fk,j, provided f1,1 = 1, f2,1, . . . , fm,1 are
given.
To satisfy (13.1.4), we let s1,1 be such that
s2
1,1 = (−1)m−1x2m−3.
(13.1.10)
(Note that if x is real and (−1)m−1x is positive, then s1,1 is real as well.) Then, for
j + k ≥m + 1, the (j, k)th entry in the left-hand side of (13.1.4) coincides with the
(j, k)th entry of Fm. Now, (13.1.4) is satisﬁed if and only if the following property
holds true:
(A) For all j + k ≤m, the (j, k)th entry of the matrix


fm,1 + fm−1,1
fm−1,1 + fm−2,1
· · ·
f2,1 + f1,1
f1,1
fm,2 + fm−1,2
fm−1,2 + fm−2,2
· · ·
f2,2
0
...
...
...
...
...
fm,m−1 + fm−1,m−1
fm−1,m−1
· · ·
0
0
fm,m
0
· · ·
0
0


·


f1,1
0
· · ·
0
0
f2,1
f2,2
· · ·
0
0
...
...
...
...
...
fm−1,1
fm−1,2
· · ·
fm−1,m−1
0
fm,1
fm,2
· · ·
fm,m−1
fm,m


(13.1.11)
is equal to zero.
Note also that the matrix in (13.1.11) is symmetric. We prove by induction on
m that one can chose f2,1, . . . , fm,1 so that (13.1.8), (13.1.9), and (A) hold true. For
m = 2, the choice f2,1 = −1
2 satisﬁes the required properties. Assume by induction
that f2,1, . . . , fm−1,1 have been already selected so that (13.1.8), (13.1.9), and (A)
hold true (with m replaced by m −1). We prove that fm,1 can be selected so that
(13.1.8), (13.1.9), and (A) hold true for m. The equality (13.1.9) implies that
fk,j + fk−1,j = −fk−1,j−1,
for
k > j > 1;
therefore, by the induction hypotheses, the (j, k)th entries in (13.1.11) are zeros,
provided j + k ≤m and j > 1. By the symmetry of the matrix (13.1.11), the same

INDEFINITE INNER PRODUCTS: NONSTANDARD INVOLUTION
305
holds true also for the (j, k)th entries, provided j + k ≤m and k > 1. It remains
to ﬁx fm,1 so that
(fm,1 + fm−1,1)f1,1 + (fm−1,1 + fm−2,1)f2,1
+ · · · + (f2,1 + f1,1)fm−1,1 + f1,1fm,1 = 0,
and we are done with the proof of Part (a).
Part (b). If (−1)m−1x is positive, the result of Part (b) is obtained as a by-
product of the proof of Part (a) (see the remark after (13.1.10)). Suppose now
that (−1)m−1x is negative. Then choose s1,1 ∈R in the proof of Part (a) so that
s2
1,1 = (−1)mx2m−3 to obtain the desired result.
□
Proof of Theorem 13.1.2. The converse part is easily veriﬁed.
We prove the direct part. Note that A is H-skewsymmetric if and only the ma-
trix pencil H +tHA is φ-hermitian/skewhermitian; in this case, the transformation
H 7→SφHS,
A 7→S−1AS,
S ∈H×n,
(13.1.12)
where S is invertible but otherwise arbitrary, is equivalent to φ-congruence of the
pencil H + tHA. Thus, we can use Theorem 12.3.5. It follows that the pair (H, A)
can be brought by a transformation of type (13.1.12) to a direct sum of the forms:
(a) H = εFk, HA = εβ(φ)Gk, where ε = 1 if k is odd and ε = ±1 if k is even;
(b)
H =

0
αFℓ+ Gℓ
αFℓ+ Gℓ
0

,
HA =

0
Fℓ
−Fℓ
0

,
(13.1.13)
where α ∈Inv (φ), R(α) > 0;
(c) H = δFs,
HA = δβ(φ)FsJs(τ), where δ = ±1 and τ > 0.
Forms (a) and (c) clearly lead to (q-φ-H-sk2) and (q-φ-H-sk5), respectively.
We conclude the proof by showing that the pair (H, A) given by (13.1.13) can
be brought by a transformation of type (13.1.12) to the pair ( eH, eA), where
eH =
 0
Fℓ
Fℓ
0

,
eA =
 Jℓ(α−1)
0
0
−Jℓ(α−1)

.
It will be convenient to do this in two steps. First, we show that (13.1.13) can be
brought to the form

eH, A1 :=

−Jℓ(α−1)T
0
0
Jℓ(α−1)T

.
Let q ∈H be a square root of −1 such that α ∈SpanR {1, q} and φ(q) = q.
Identifying SpanR {1, q} with C and using Lemma 13.1.3, we ﬁnd S ∈Hℓ×ℓsuch
that
Sφ(αFℓ+ Gℓ)S = Fℓ
and
S−1(αI + FℓGℓ)−1S = Jℓ(α−1)T .
Then
eH =
 Sφ
0
0
Sφ

H
 S
0
0
S

and
A1 =
 S−1
0
0
S−1

A
 S
0
0
S

.

306
CHAPTER 13
Finally, observe that eH = F2ℓeHF2ℓ,
eA = F2ℓA1F2ℓ.
□
Part (a) and the equivalence of (1) and (2) in Part (b) of the following corollary
are immediate from Theorems 13.1.1 and 13.1.2. The equivalence of (2) and (3) in
Part (b) follows by consideration of the Jordan form of A.
Corollary 13.1.4. (a) Every matrix in Hn×n is H-symmetric for some invertible
φ-hermitian H ∈Hn×n.
(b) The following statements are equivalent for a matrix A ∈Hn×n:
(1) A is H-skewsymmetric for some φ-hermitian invertible H ∈Hn×n.
(2) The partial multiplicities of eigenvalues α and −α of A, where R(α) ̸= 0, are
the same; more precisely, for every positive integer k and every α ∈Hn×n
with R(α) ̸= 0, the number of partial multiplicities of A at α is equal to the
number of partial multiplicities of A at −α, and we set this number to be zero
for all k if α (and then also −α) is not an eigenvalue of A.
(3) A is similar to −A.
13.2
CANONICAL FORMS: SKEWSYMMETRIC INNER
PRODUCTS
In this section we assume that H is φ-skewhermitian and invertible, i.e., we are
working with an indeﬁnite inner product of skewsymmetric-type. Recall that a ma-
trix X ∈Hn×n is called (H, φ)-Hamiltonian if the equality [x, Xy]H,φ = −[Xx, y]H,φ
holds true for all x, y ∈Hn×1, where [·, ·]H,φ stands for the inner product of
skewsymmetric-type induced by H: [x, y]H,φ = yφHx, x, y ∈Hn×1.
It is easy
to see that X is (H, φ)-Hamiltonian if and only if the equation HX = −XφH holds
true; in other words, HX is φ-hermitian.
Deﬁnition 13.2.1. A matrix A ∈Hn×n is said to be (H, φ)-skew-Hamiltonian
if the equality [x, Ay]H,φ = [Ax, y]H,φ holds true for all x, y ∈Hn×1 or, equivalently,
if HA is φ-skewhermitian.
In this chapter we will write H-Hamiltonian and H-skew-Hamiltonian, short for
(H, φ)-Hamiltonian and (H, φ)-skew-Hamiltonian, respectively.
In the next theorem we state the canonical form for Hamiltonian matrices.
Again, we describe ﬁrst the primitive forms.
(q-φ-H-H3)
L = κβ(φ)Fk, A = β(φ)Jk(0), where κ = 1 if k is even, and
κ = ±1 if k is odd.
(q-φ-H-H4)
L =

0
Fℓ
−Fℓ
0

,
A =
 −Jℓ(α)
0
0
Jℓ(α)

,
where α ∈Inv (φ), R(α) > 0.
(q-φ-H-H5)
L = δβ(φ)Fs,
A = β(φ)Js(τ), where δ = ±1 and τ is a
negative real number.

INDEFINITE INNER PRODUCTS: NONSTANDARD INVOLUTION
307
Remark 13.2.2. One can replace “negative” by “positive” in (q-φ-H-H5), as
can be seen from formulas (13.1.2) and
 diag (φ(γ), −φ(γ), . . . , (−1)s−1φ(γ))

(β(φ)Fs)
 diag (γ, −γ, . . . , (−1)s−1γ)

= (−1)s−1(β(φ)Fs),
where γ ∈H is such that γ−1β(φ)γ = −β(φ) and |γ| = 1. Note that the replacement
of τ by its negative in (q-φ-H-H5) will reverse the sign δ if s is even and will leave
the sign invariant if s is odd.
Theorem 13.2.3. Let A ∈Hn×n be H-Hamiltonian, where H ∈Hn×n is in-
vertible and φ-skewhermitian. Then there exists an invertible quaternion matrix S
such that SφHS and S−1AS have the following block diagonal form:
SφHS = L1 ⊕L2 ⊕· · · ⊕Lm,
S−1AS = A1 ⊕A2 ⊕· · · ⊕Am,
(13.2.1)
where each pair (Li, Ai) has one of the forms (q-φ-H-H3), (q-φ-H-H4), (q-φ-H-
H5). Moreover, the form (13.2.1) is uniquely determined by the pair (H, A), up to
an arbitrary simultaneous permutation of blocks and up to a replacement of α in each
block of the form (q-φ-H-H4) with a similar quaternion α′ such that φ(α′) = α′.
Conversely, if H and A are given as in formula (13.2.1), then H is invertible
φ-skewhermitian and A is H-Hamiltonian.
The sign characteristic of an H-Hamiltonian matrix A assigns a sign ±1 to every
partial multiplicity corresponding to a nonzero eigenvalue of A with zero real part
(if any) and to every odd partial multiplicity corresponding to the eigenvalue zero
of A (if A is not invertible).
Proof. The converse part being easily veriﬁed by a straightforward computa-
tion, we focus on the direct part.
Observe that A is H-Hamiltonian if and only if the matrix pencil HA + tH is
φ-hermitian/skewhermitian, and use the canonical form of Theorem 12.3.5. As a
result we see that there is an invertible matrix S ∈Hn×n such that SφHS and
S−1XS have the block diagonal form (13.2.1) with each pair (Li, Ai) having one of
the following three forms:
(a) L = κβ(φ)Fk, A = −β(φ)Jk(0)T , where κ = 1 if k is even and κ = ±1 if k is
odd;
(b) the form (q-φ-H-H4);
(c) L = δβ(φ)FsJs(τ), A = −β(φ)(Js(τ))−1, where τ > 0.
Form (a) can be easily transformed to (q-φ-H-H3) by a transformation (L, A) 7→
(SφLS, S−1AS) for some invertible S. Indeed, we have
(Ξk(β(φ)))φ (β(φ)Fk) Ξk(β(φ))
=
β(φ)Fk,
(Ξk(β(φ)))−1 (−β(φ)Jk(0)T ) Ξk(β(φ))
=
(−1)k−1β(φ)Jk(0),
which for k odd is in the form (q-φ-H-H3) and for k even can be transformed to
that form using Remark 12.3.3.

308
CHAPTER 13
As for (c), we will ﬁnd an invertible matrix S ∈Hs×s such that
Sφ · β(φ)FsJs(τ) · S
=
eδβ(φ)Fs,
(13.2.2)
S−1 · (−β(φ)(Js(τ))−1) · S
=
β(φ)Js(τ −1),
τ ∈R \ {0}, (13.2.3)
where eδ = ±1 (in fact, eδ coincides with the sign of τ). This will complete the proof
of formula (13.2.1).
Let (β(φ), q1, q2) be a units triple. We seek S with entries in SpanR {1, q1}, and
identify SpanR {1, q1} with C. Then
β(φ)S = Sβ(φ),
ST β(φ) = β(φ)S∗,
Sφ = ST ,
S−1β(φ) = β(φ)S
−1,
and equations (13.2.2) and (13.2.3) read
S∗(FsJs(τ))S = eδFs,
S
−1(Js(τ))−1S = −Js(τ −1),
τ < 0.
(13.2.4)
Thus, we seek invertible S ∈Cs×s that satisﬁes (13.2.4). In turn, setting S = iS0,
where S0 ∈Rs×s, (13.2.4) boils down to
ST
0 (τFs + eGs)S0 = ±Fs,
(Js(τ))−1S0 = S0Js(τ −1).
(13.2.5)
We now use Lemma 13.1.3(b), with x = τ and m = s. If S ∈Rs×s is the matrix
satisfying (13.1.6) and (13.1.5), then it is easy to see that S0 := FsSFs satisﬁes
(13.2.5); in verifying this, use the equalities
Fs(Js(y))T Fs = Js(y),
Fs((Js(y))T )−1Fs = (Js(y))−1,
where y ∈H.
The uniqueness statement in Theorem 13.2.3 follows from that of Theorem
12.3.5.
□
Next, we present the canonical form for H-skew-Hamiltonian matrices.
Theorem 13.2.4. Fix β(φ) ∈H such that φ(β(φ)) = −β(φ) and |β(φ))| = 1.
Let H = −Hφ ∈Hn×n be an invertible matrix, and let A be H-skew-Hamiltonian.
Then there exists an invertible matrix S such that the matrices S−1AS and SφHS
have the form
SφHS
=
η1β(φ)Fm1 ⊕· · · ⊕ηpβ(φ)Fmp
⊕

0
Fℓ1
−Fℓ1
0

⊕· · · ⊕

0
Fℓq
−Fℓq
0

,
S−1AS
=
Jm1(γ1)T ⊕· · · ⊕Jmp(γp)T ⊕
 Jℓ1(α1)T
0
0
Jℓ1(α1)T

⊕· · · ⊕
 Jℓq(αq)T
0
0
Jℓq(αq)T

,
(13.2.6)
where η1, . . . , ηp are signs ±1, the quaternions α1, . . . , αq ∈Inv (φ)\R, and γ1, . . . , γp
are real.
Moreover, the form (13.2.6) is unique up to an arbitrary simultaneous permuta-
tion of the diagonal blocks in each of the parts
 ⊕p
j=1
 ηjβ(φ)Fmj

,
⊕p
j=1Jmj(γj)T 

INDEFINITE INNER PRODUCTS: NONSTANDARD INVOLUTION
309
and

⊕q
j=1

0
Fℓj
−Fℓj
0

,
⊕q
j=1
 Jℓj(αj)T
0
0
Jℓj(αj)T

and up to replacements of αj in each block
 Jℓj(αj)T
0
0
Jℓj(αj)T

by a similar
quaternion λj ∈Inv (φ).
Conversely, if H and A are given by formula (13.2.6), then H is φ-skewhermitian
and A is H-skew-Hamiltonian.
Remark 13.2.5. In formula (13.2.6), any of the Jmj(γj)T ’s can be replaced by
Jmj(γj), and any of the (Jℓj(αj)T ) ⊕(Jℓj(αj)T )’s can be replaced by Jℓj(λj) ⊕
Jℓj(λj), where λj ∈Inv (φ) is any quaternion similar to α. Use the equalities
FmjJmj(γj)T Fmj = Jmj(γj),
j = 1, 2, . . . , p,
and similar equalities for the blocks Jℓj(αj)T to verify this claim.
Proof. A is H-skew-Hamiltonian if and only if the matrix pencil HA + tH is
φ-skewhermitian. Now use the canonical form for HA + tH of Theorem 11.2.2.
□
Note that for H-skew-Hamiltonian matrices, the partial multiplicities corre-
sponding to nonreal eigenvalues come in pairs.
Characterizations of matrices that are H-Hamiltonian or H-skew-Hamiltonian
in some indeﬁnite inner product of skewsymmetric-type, are given in the next corol-
lary. Its proof is immediate from Theorems 13.2.3 and 13.2.4.
Corollary 13.2.6. (a) A matrix A ∈Hn×n is H-skew-Hamiltonian for some φ-
skewhermitian invertible H if and only if the partial multiplicities corresponding to
nonreal eigenvalues of A come in pairs. More precisely, for all positive integers k
and for all nonreal α ∈H, the total number of partial multiplicities of A that are
equal to k at eigenvalues similar to α is even, and we set this number to be zero for
all k if α is not an eigenvalue of A.
(b) A matrix A ∈Hn×n is H-Hamiltonian for some φ-skewhermitian invertible
H if and only if the partial multiplicities of eigenvalues α and −α of A, where
R(α) ̸= 0, are the same. More precisely, for all positive integers k and for all α
with R(α) ̸= 0, the total number of partial multiplicities of A that are equal to k at
eigenvalues similar to α coincides with the total number of partial multiplicities of
A that are equal to k at eigenvalues similar to −α.
As we have seen in Corollary 13.1.4, the condition in (b) is equivalent to A being
similar to −A.
13.3
EXTENSION OF INVARIANT SEMIDEFINITE SUBSPACES
Let G ∈Hn×n be φ-skewhermitian. Recall that the β(φ)-inertia of G are deﬁned
by
(In+ (G), In−(G), In0 (G)) = (p, q, n −p −q),
where p and q are taken from the canonical form (4.1.2) for G.

310
CHAPTER 13
Theorem 13.3.1. Let H be φ-skewhermitian and invertible, and let A ∈Hn×n
be (H, φ)-Hamiltonian or (H, φ)-skew-Hamiltonian. Suppose subspace M ⊆Hn×1
is (H, φ)-nonnegative, resp. (H, φ)-nonpositive, and A-invariant. Then there is an
A-invariant (H, φ)-nonnegative, resp. (H, φ)-nonpositive, subspace N ⊆Hn×1 that
contains M and has dimension
dimH N = In+ (H),
resp.
dimH N = In−(H).
Proof. We consider the case of (H, φ)-nonnegative subspaces (for the (H, φ)-
nonpositive subspaces apply the result for −H in place of H). The proof is modeled
after that of Theorem 10.2.1.
We use induction on the size n of the matrices H and A and on the dimension
of M. The case n = 1 is trivial, and the case M = {0} follows by a construction
of an A-invariant H-nonnegative subspace in Step 1 below.
Step 1. The case M = {0}. Assume ﬁrst A is (H, φ)-Hamiltonian. We may
suppose that H and A are given by the right-hand sides of (13.2.1). Note that
In+

0
Fℓ
−Fℓ
0

=
In−

0
Fℓ
−Fℓ
0

= ℓ,
(13.3.1)
In+ (±β(φ)Fk)
=





k
2
if k is even;
k ± 1
2
if k is odd,
(13.3.2)
and
In−(±β(φ)Fk) =





k
2
if k is even;
k ∓1
2
if k is odd.
(13.3.3)
Therefore, to obtain a spanning set for an A-invariant (H, φ)-nonnegative subspace,
we select vectors as follows (cf. selection of vectors in the proof of Theorem 10.2.1):
for each pair of blocks
(Li = κβ(φ)Fk, Ai = β(φ)Jk(0))
in (13.2.1), select e1, . . . , ek if k is even, select e1, . . . , e(k−1)/2 if k is odd and κ = −1,
and select e1, . . . , e(k+1)/2 if k is odd and κ = 1. For each pair of blocks in (13.2.1),

Li =

0
Fℓ
−Fℓ
0

,
Ai =
 −Jℓ(α)
0
0
Jℓ(α)

,
α ∈Inv (φ), R(α) > 0,
select e1, . . . , eℓ. For each pair of blocks in (13.2.1),
(Li = δβ(φ)Fs,
Ai = β(φ)Js(τ)),
δ = ±1,
τ < 0,
select e1, . . . , es if s is even, select e1, . . . , e(s−1)/2 if s is odd and δ = −1, and select
e1, . . . , e(s+1)/2 if s is odd and δ = 1.

INDEFINITE INNER PRODUCTS: NONSTANDARD INVOLUTION
311
Next, assume A is (H, φ)-skew-Hamiltonian. We take A and H in the following
form (Theorem 13.2.4 and Remark 13.2.5):
H
=
η1β(φ)Fm1 ⊕· · · ⊕ηpβ(φ)Fmp
⊕

0
Fℓ1
−Fℓ1
0

⊕· · · ⊕

0
Fℓq
−Fℓq
0

,
A
=
Jm1(γ1) ⊕· · · ⊕Jmp(γp)
⊕

Jℓ1(α1)
0
0
Jℓ1(α1)

⊕· · · ⊕

Jℓq(αq)
0
0
Jℓq(αq)

,
where η1, . . . , ηp ∈{1, −1}, the quaternions α1, . . . , αq ∈Inv (φ)\R, and γ1, . . . , γp ∈
R. We assume also that m1, . . . , mr are odd and mr+1 . . . , mp are even.
Using
(13.3.1), (13.3.2), and (13.3.3), one veriﬁes that the following vectors span an A-
invariant (H, φ)-nonnegative subspace, where we set w = m1 + · · · + mr and u =
m1 + · · · + mp:
e1, e2, . . . , e(m1+η1)/2, em1+1, . . . , em1+(m2+η2)/2, . . . ,
em1+···+mr−1+1, . . . , em1+···+mr−1+(mr+ηr)/2, ew+1, . . . , ew+mr+1/2,
ew+mr+1+1, . . . , ew+mr+1+mr+2/2, . . . ,
ew+mr+1+···+mp−1+1, . . . , ew+mr+1+···+mp−1+mp/2,
eu+1, . . . , eu+ℓ1, eu+2ℓ1+1, . . . , eu+2ℓ1+ℓ2, . . . ,
eu+2ℓ1+···+2ℓq−1+1, eu+2ℓ1+···+2ℓq−1+ℓq.
Step 2. The general case. We proceed as in the proof of Theorem 10.2.1. Using
Lemma 4.5.1, we may assume that M = SpanH {e1, . . . , en2+n3},
H
=


0
0
In2
0
0
β(φ)In3
0
0
−In2
0
0
0
0
0
0
H0

,
and
A
=


A22
A23
A24
A25
A32
A33
A34
A35
0
0
A44
A45
0
0
A54
A55

,
(13.3.4)
where H0 = −(H0)φ, App ∈Hnp×np for p = 2, 3, 4, 5, and n2 + n3 + n4 + n5 = n.
The condition (HA)φ = ±(HA) gives
A21 = 0,
A31 = 0,
A32 = 0,
A35 = 0,
A45 = 0,
and (H0A55)φ = ±H0A55. Letting β(φ)Ip0 ⊕−β(φ)Iq0 be the canonical form of
H0, we obtain by the induction hypothesis that there is an A55-invariant (H0, φ)-
nonnegative subspace M0 of dimension p0. Then the subspace


Hn2
0
0
0

˙+


0
Hn3
0
0

˙+


0
0
0
M0



312
CHAPTER 13
is clearly A-invariant (H, φ)-nonnegative and has the required dimension.
□
Remark 13.3.2. Under the hypotheses of Theorem 10.2.1, consider the case
when A is (H, φ)-Hamiltonian, and let Λ be a set of eigenvalues of A subject to
conditions (10.2.5) and (10.2.6). As in Remark 10.2.2, one can prove that there is
an A-invariant (H, φ)-nonnegative subspace N with the additional property that it
contains the sums of the root subspaces of A corresponding to the eigenvalues in
Λ. Likewise, there exists an A-invariant (H, φ)-nonpositive subspace N with this
property.
For indeﬁnite inner products of symmetric-type, i.e., those generated by invert-
ible φ-hermitian matrices, we have the following result on invariant semideﬁnite
extensions of invariant neutral subspaces.
In the next two theorems we assume that H = Hφ is invertible and A ∈Hn×n
is (H, φ)-skewsymmetric. Note that HA is φ-skewhermitian.
Theorem 13.3.3. Let N ⊆Hn×1 be an A-invariant (H, φ)-neutral subspace.
Then there exist A-invariant subspaces L+ and L−such that each of them con-
tains L, and L+ is maximal (HA, φ)-nonnegative, whereas L−is maximal (HA, φ)-
nonpositive.
As in Remark 10.7.1, one shows that, under the hypotheses of Theorem 13.3.3,
if an A-invariant subspace M is (H, φ)-neutral, then M is also (HA, φ)-neutral;
the converse holds, provided it is assumed in addition that A is invertible.
An analogue of Theorem 10.7.5 is valid as well. Recall the deﬁnition of a c-set
given in Section 10.7.
Theorem 13.3.4. Under the hypotheses of Theorem 13.3.3, assume in addition
that the set S0 of eigenvalues with nonzero real parts of the restriction A|N is such
that
λ0 ∈S0
=⇒
−λ0 ̸∈S0.
Then for every c-set S containing S0 there exist subspaces L± as in Theorem 13.3.3
with the additional property that S is the set of eigenvalues with nonzero real parts
of A|L±.
As in Open Problem 10.7.6, the question is open whether or not the results of
Theorems 13.3.3 and 13.3.4 remain valid under the weaker assumption that N is
A-invariant (HA, φ)-neutral.
The proofs of Theorems 13.3.3 and 13.3.4 are relegated to the next section.
We conclude this section with a formula for the dimensions of maximal (HA, φ)-
nonnegative and maximal (HA, φ)-nonpositive subspaces in terms of the canon-
ical form.
Let H = Hφ ∈Hn×n be invertible, and let A ∈Hn×n be (H, φ)-
skewsymmetric. Then HA is φ-skewhermitian and its β(φ)-inertia is given as fol-
lows, using the canonical form (13.1.3) of the pair (H, A):
(1) In0 (HA) is equal to the number of pairs of blocks (Hi, Ai) of type (q-φ-H-sk2)
in (13.1.3).
(2)
In+ (HA) −In+ (HA) =
X′
εi +
X′′
δj,
(13.3.5)

INDEFINITE INNER PRODUCTS: NONSTANDARD INVOLUTION
313
where the sum P′ is taken over all pairs (Hi, Ai) of type (q-φ-H-sk2) with
even k and the sum P′′ is taken over all pairs (Hj, Aj) of type (q-φ-H-sk5)
with odd s. In (13.3.5), εi is the sign attached to the pair (Hi, Ai), and δj is
the sign attached to the pair (Hj, Aj).
Formulas (1) and (2) are based on the following formulas for the β(φ)-inertia for
φ-skewhermitian matrices:
In0 (εβ(φ)FkJk(0))
=
1;
(13.3.6)
In± (εβ(φ)FkJk(0))
=





k −1
2
if k is odd,
k −1 ± ε
2
if k is even,
(13.3.7)
where ε = ±1;
In+

0
−FℓJℓ(α)
FℓJℓ(α)
0

= In−

0
−FℓJℓ(α)
FℓJℓ(α)
0

= ℓ,
(13.3.8)
where α ∈Inv (φ), R(α) > 0; and
In± (δβ(φ)FsJs(τ)) =





s
2
if s is even,
s + δ
2
if s is odd,
(13.3.9)
where τ is real and positive and δ = ±1.
Veriﬁcation of (13.3.6)–(13.3.9) is left as an exercise (Ex. 13.7.9).
Denoting by Υ the right-hand side of (13.3.5) and by t the number of pairs of
blocks (Hi, Ai) of type (q-φ-H-sk2) in the canonical form of (H, A), it follows that
the dimension of maximal (HA, φ)-nonnegative subspaces is
In0 (HA) + In+ (HA) = t + n + Υ
2
and that of maximal (HA, φ)-nonpositive subspaces is t + (n −Υ)/2.
13.4
PROOFS OF THEOREMS 13.3.3 AND 13.3.4
The proof is modeled (with necessary changes having been made) after the proofs
of Theorems 10.7.2 and 10.7.5. Therefore, we omit some details in the proof.
Lemma 13.4.1. Let Z ∈Hm×m be a φ-skewhermitian matrix partitioned as
follows:
Z =


0
0
−(Q1)φ
0
Q2
−(K1)φ
Q1
K1
K2

,
where the p × q block Q1 is right-invertible (thus p ≤q). Then
In±(Z) + In0(Z) = q + In±(Q2) + In0(Q2).
(13.4.1)
(The inertia here is the β(φ)-inertia of φ-skewhermitian matrices.)

314
CHAPTER 13
The proof is similar to that of Lemma 10.8.1 and is omitted.
We prove Theorems 13.3.3 and 13.3.4 only for HA-nonnegative subspaces (for
nonpositive subspaces the proof is analogous, or else use −H in place of H). The
canonical form of the pair of matrices (A, H) (with invertible H) given in Theorem
13.1.2 allows us to reduce the proofs to separate consideration of two cases: (1) A
is invertible; (2) A is nilpotent.
Assume ﬁrst that A is invertible. Let bH = HA. Then bH is φ-skewhermitian
and invertible, and A is ( bH, φ)-skewsymmetric:
bHA = −Aφ bH. The subspace N
is easily seen to be bH-neutral. By Theorem 13.3.1, there exist ( bH, φ)-nonnegative,
resp. ( bH, φ)-nonpositive, subspaces which are A-invariant and have the properties
of dimension and location of the spectrum, as required in Theorems 13.3.3 and
13.3.4.
Thus, it remains to prove these theorems for nilpotent A. We prove Theorem
13.3.3 assuming A is nilpotent.
Consider the (H, φ)-orthogonal companion of N:
N [⊥,φ] := {x ∈Hn×1 |xφHy = 0
for all
y ∈N},
As N is H-neutral, we have N ⊆N [⊥,φ]. Since A is (H, φ)-skewsymmetric and N
is A-invariant, the subspace N [⊥,φ] is easily seen to be A-invariant as well. Note
also that
dim N = n −dim N [⊥,φ].
(13.4.2)
To verify (13.4.2), represent N [⊥,φ] as the solution set of the system of linear equa-
tions
(zj)φHx = 0,
j = 1, 2, . . . , k,
where z1, . . . , zk is a basis for N.
Assuming N ̸= N [⊥,φ], choose an (ordered) basis
(y1, . . . , yn)
(13.4.3)
in Hn×1 so that the ﬁrst vectors in (13.4.3) form a basis of N, the next vectors
in (13.4.3) form a basis in some complement of N in N [⊥,φ], and the remaining
vectors in (13.4.3) form a basis in some complement of N [⊥,φ] in Hn×1. Then, as
in the proof of Theorems 10.7.2 and 10.7.5, the proof reduces to the case when
N = N [⊥,φ]
or
dimH N = dimH N [⊥,φ] −1.
(13.4.4)
Case 13.4.2. Assume N = N [⊥,φ].
In view of (13.4.2) we have
dim N = dim N [⊥,φ] = n
2 ,
(13.4.5)
and n is necessarily even. Choosing a basis (y′
1, . . . , y′
n) in Hn×1 such that the ﬁrst
part of its elements form a basis in N, we represent A and H in the form
A′
=
 B11
B12
0
B22

,
where
Bij ∈Hn/2×n/2,
H′
=
((y′
i)φHy′
j)n
i,j=1 =

0
H1
(H1)φ
H2

.

INDEFINITE INNER PRODUCTS: NONSTANDARD INVOLUTION
315
Here
N = SpanH {e1, . . . , en/2},
the matrix H1 is invertible, and H2 is φ-hermitian. Applying a transformation
A, H
7→
S−1AS, SφHS
(13.4.6)
with S =
 I
W1
0
W2

for suitable W1 and W2, we may (and do) assume that, in
fact,
H′ =
 0
I
I
0

.
(13.4.7)
Then, since A′ is (H′, φ)-skewsymmetric, we have
A′ =
 B11
B12
0
−(B11)φ

,
B12
is φ-skewhermitian.
(13.4.8)
Now argue as in the proof of Case 10.8.2, using Lemma 13.4.1 in place of Lemma
10.8.1.
Case 13.4.3. Assume dim N = dim N [⊥,φ] −1.
Arguing as in the proof of Case 10.8.3, we represent A and H in the following
form, conformally partitioned:
A
=


0
0
D1
D2
D3
C1
C2
D4
D5
D6
0
0
01×1
D7
D8
0
0
0
0
−(C1)φ
0
0
0
0
−(C2)φ


,
H
=


0
0
0
I
0
0
0
0
0
I
0
0
H22
H231
H232
I
0
(H231)φ
0
0
0
I
(H232)φ
0
0


.
Here
N = SpanH {e1, . . . , e(n−1)/2},
N [⊥,φ]
=
SpanH {e1, . . . , e(n+1)/2},
 D2
D3
D5
D6

∈H(n−1)/2×(n−1)/2,
H22
=
(H22)φ ∈H \ {0},
and [C1 C2] ∈Hq×(n−1)/2 is right-invertible. Due to HA being φ-skewhermitian,
we have D1 = −(H22D7)φ, and
HA =


0
0
0
0
−(C1)φ
0
0
0
0
−(C2)φ
0
0
0
H22D7
−(D4)φ
0
0
−(H22D7)φ
G
⋆
C1
C2
D4
⋆
⋆


,

316
CHAPTER 13
where G = −Gφ and where we denote by ⋆block entries of no immediate interest.
Let
eA =
 01×1
D7
0
0

,
eH =

01×1
H22D7
−(H22D7)φ
G

= −eHφ.
If M is an eA-invariant maximal ( eH, φ)-nonnegative subspace, then, in view of
Lemma 13.4.1, the subspace N +


0(n−1)/2×1
M
0q×1

is A-invariant and maximal
(HA, φ)-nonnegative, and we are done.
It remains, therefore, to prove existence of an eA-invariant subspace which is
maximal ( eH, φ)-nonnegative.
This can be done as in the proof of case 10.8.3,
using the β(φ)-inertia of φ-skewhermitian matrices rather than inertia of hermitian
matrices; to verify that
In+

0
H22
−H22
G11

= 1,
In0

0
H22
−H22
G11

= 0,
where H22 = (H22)φ ∈H \ {0}, G11 = −(G11)φ ∈H, use the equality

aφ
1
bφ
1
 
0
H22
−H22
G11
  a
b
1
1

=
 β(φ)
0
0
−β(φ)

,
where a, b ∈H are found from the equations
−H22b + bφH22 = −β(φ) −G11,
aφH22 = −G11 + H22b.
(Note that the matrix
 a
b
1
1

is necessarily invertible.)
The proofs of Theorems 13.3.3 and 13.3.4 are complete.
□
13.5
INVARIANT LAGRANGIAN SUBSPACES
Of special interest are invariant Lagrangian subspaces, which will be studied in this
section. The exposition here is largely parallel to Sections 10.3 and 10.6, so some
details will be omitted.
As everywhere in this chapter, ﬁx a nonstandard involution φ.
Deﬁnition 13.5.1. Given a φ-hermitian or φ-skewhermitian matrix G ∈Hn×n,
a subspace M ⊆Hn×1 is said to be (G, φ)-Lagrangian if it is (G, φ)-neutral and has
dimension n/2.
Clearly, this deﬁnition makes sense only if n is even. Also, a subspace M is
(G, φ)-Lagrangian if and only if the subspace S−1(M) is (SφGS, φ)-Lagrangian for
all invertible matrices S ∈Hn×n. As before, in Section 4.1, we denote by
(In+(G), In−(G), In0(G))
the β(φ)-inertia of a φ-skewhermitian matrix G. As follows from Theorems 4.2.6
and 4.2.7, and assuming n is even, (G, φ)-Lagrangian subspaces exist if and only if
min{In+ (G), In−(G)} + In0 (G) ≥n
2
(13.5.1)

INDEFINITE INNER PRODUCTS: NONSTANDARD INVOLUTION
317
if G is φ-skewhermitian, and
⌊rank G + 1
2
⌋≤n
2
(13.5.2)
if G is φ-hermitian.
We will use the notion of (G, φ)-Lagrangian subspaces for
invertible G (and even n); in this case, (13.5.2) is always satisﬁed, and (13.5.1)
becomes
In+ (G) = In−(G) = n
2 .
(13.5.3)
The following property of Lagrangian subspaces will be very useful.
Proposition 13.5.2. Let
G = ±Gφ = G1 ⊕G2 ⊕· · · ⊕Gk ∈Hn×n,
where Gj ∈Hnj×nj, j = 1, 2, . . . , k, and where n is even. Assume G is invertible.
Then a subspace M ⊆Hn×1 of the form
M = M1 ⊕M2 ⊕· · · ⊕Mk :=











x1
x2
...
xk

: x1 ∈M1, . . . , xk ∈Mk









is (G, φ)-Lagrangian if and only if each Mj is (Gj, φ)-Lagrangian, for j = 1, 2, . . . , k
(in particular, each nj must be even).
The proof is analogous to that of Proposition 13.5.2 (Ex. 13.7.7).
Next, we consider existence of invariant Lagrangian subpaces. A necessary and
suﬃcient condition for this to happen is given in the next theorem for the case
of inner products of symmetric-type.
As it turns out, existence of Lagrangian
subspaces is generally not suﬃcient.
Theorem 13.5.3. Let H ∈Hn×n be φ-hermitian and invertible.
(a) If A ∈Hn×n is (H, φ)-symmetric, then there exists an A-invariant (H, φ)-
Lagrangian subspace if and only if every root subspace of A is even dimensional.
(b) If A ∈Hn×n is (H, φ)-skewsymmetric, then there exists an A-invariant
(H, φ)-Lagrangian subspace if and only if the following two conditions are satisﬁed:
(1) The root subspace of A corresponding to the zero eigenvalue (if A if not in-
vertible) is even dimensional.
(2) For every nonzero eigenvalue λ of A with zero real part (if any), the number
of odd partial multiplicities corresponding to λ is even and exactly half of them
have sign −1 (the other half having the sign 1) in the sign characteristic of
(A, H).
Proof. Note that for any invertible S ∈H×n, the subspace M is A-invariant
(H, φ)-Lagrangian if and only if S−1(M) is S−1AS-invariant (S∗HS, φ)-Lagrangian.
Therefore, without loss of generality, we may (and do) assume that the pair (A, H)
is given by the canonical form of Theorem 13.1.1 (if A is (H, φ)-symmetric) or
Theorem 13.1.2 (if A is (H, φ)-skewsymmetric).

318
CHAPTER 13
Part (a).
In view of Proposition 13.5.2 and the fact that every A-invariant
subspace is the sum of its intersections with the root subspaces of A (Proposition
5.1.4), we need only to consider the situation when Hn×1 is a root subspace for A.
The condition that n is even is obviously necessary. Assuming n is even, we will
exhibit an A-invariant (H, φ)-Lagrangian subspace. Rearranging (if necessary) the
blocks in the canonical form, we let
H
=
Fℓ1 ⊕· · · ⊕Fℓ2u ⊕Fℓ2u+1 ⊕· · · ⊕Fℓq,
A
=
Jℓ1(α) ⊕· · · ⊕Jℓq(α),
α ∈H,
where ℓ1, . . . , ℓ2u are odd and ℓ2u+1, . . . , ℓq are even. Select the vectors:
e1, . . . , e(ℓ1−1)/2, e(ℓ1+1)/2 + β(φ)eℓ1+(ℓ2+1)/2, eℓ1+1, . . . , eℓ1+(ℓ2−1)/2,
(13.5.4)
eℓ1+···+ℓ2j+1, . . . , eℓ1+···+ℓ2j+···+(ℓ2j+1−1)/2,
(13.5.5)
eℓ1+···+ℓ2j+···+(ℓ2j+1+1)/2 + β(φ)eℓ1+···+ℓ2j+ℓ2j+1+(ℓ2j+2+1)/2,
(13.5.6)
eℓ1+···+ℓ2j+ℓ2j+1+1, . . . , eℓ1+···+ℓ2j+ℓ2j+1+(ℓ2j+2−1)/2
(13.5.7)
for j = 1, 2, . . . , u −1, and, setting w = ℓ1 + · · · + ℓ2u,
ew+1, . . . , ew+ℓ2u+1/2, ew+ℓ2u+1+1, . . . , ew+ℓ2u+1+ℓ2u+2/2, . . . ,
(13.5.8)
ew+ℓ2u+1+···+ℓq−1+1, ew+ℓ2u+1+···+ℓq−1+2, . . . , ew+ℓ2u+1+···+ℓq−1+ℓq/2.
(13.5.9)
The selected vectors span an A-invariant (H, φ)-Lagrangian subspace in Hn×1.
Part (b). In view of the canonical form (13.1.3), we can (and do) assume that
Hn×1 is either a root subspace of A corresponding to an eigenvalue with zero real
part or the sum of two root subspaces of A corresponding to eigenvalues α and −α
with nonzero real part. Three cases can occur:
(i)
H
=
Fℓ1 ⊕· · · ⊕Fℓv ⊕εv+1Fℓv+1 ⊕· · · ⊕εqFℓq,
A
=
β(φ)Jℓ1(0) ⊕· · · ⊕β(φ)Jℓq(0),
where ℓ1, . . . , ℓv are odd, ℓv+1, . . . , ℓq are even, and εj = ±1, j = v + 1, . . . , q;
(ii)
H = ⊕q
j=1(δjFpj),
A = ⊕q
j=1(β(φ)Jpj(τ)),
(13.5.10)
where τ > 0 and the δj’s are signs ±1;
(iii)
H = ⊕s
j=1

0
Fkj
Fkj
0

,
A = ⊕s
j=1
 Jkj(α)
0
0
−Jkj(α)

,
where α ∈Inv (φ) has positive real part.
In Case (i), condition (1) is obviously necessary, and assuming it holds true, we
have that v = 2u is even. We make the selection of vectors as in (13.5.4)–(13.5.9).
Then the selected vectors span an A-invariant (H, φ)-Lagrangian subspace. In Case
(iii), select
e1, e2, . . . , ek1, e2k1+1, . . . , e2k1+k2, e2k1+2k2+1, . . . ,

INDEFINITE INNER PRODUCTS: NONSTANDARD INVOLUTION
319
e2k1+2k2+k3, . . . , e2k1+···+2ks−1+ks.
Again, the selected vectors span an A-invariant (H, φ)-Lagrangian subspace.
Finally, consider Case (ii). Note that HA is φ-skewhermitian. Its β(φ)-inertia
is found by the formula
In+ (HA) + In−(HA) = n,
In+ (HA) −In−(HA) =
X
{j : pj is odd}
δj.
Observe that all A-invariant (H, φ)-Lagrangian subspaces are, clearly, also (HA, φ)-
Lagrangian. Thus, the necessary condition In+ (HA) = In−(HA) for existence of
an A-invariant (H, φ)-Lagrangian subspace boils down to item (2).
Conversely,
suppose (2) holds true. We will construct an A-invariant (H, φ)-Lagrangian sub-
space. Simultaneously rearranging blocks in the formulas for H and A in (13.5.10)
(if necessary), we assume
H = (Fp1 ⊕−Fp2) ⊕· · · ⊕(Fp2u−1 ⊕−Fp2u) ⊕δ2u+1Fp2u+1 ⊕· · · ⊕δqFpq,
where p1, . . . , p2u are odd and p2u+1, . . . , pq are even. Select the vectors in Hn×n, as
in (10.3.5)–(10.3.10). The selected vectors span an A-invariant (H, φ)-Lagrangian
subspace.
□
Remark 13.5.4. For an (H, φ)-skewsymmetric matrix A, if there exists an A-
invariant (H, φ)-Lagrangian subspace, then there exists such a subspace with addi-
tional properties regarding location of the spectrum, as in Remark 10.2.2. Namely,
let Λ be a set of eigenvalues of A with the properties
λ ∈Λ,
µ ∈H,
µ similar to λ
=⇒
µ ∈Λ
(13.5.11)
and
λ ∈Λ
=⇒
−λ ̸∈Λ.
(13.5.12)
Then there exists an A-invariant (H, φ)-Lagrangian subspace M that contains the
sum of root subspaces for A corresponding to the eigenvalues in Λ. We leave it as
an exercise to appropriately select vectors to span such a subspace M (Ex. 13.7.8).
Next, we consider invariant Lagrangian subspaces in the context of indeﬁnite
inner products of skewsymmetric-type. Thus, it will be assumed from now on in
this section that H ∈Hn×n is φ-skewhermitian and invertible and (as before) that
n is even. Then existence of (H, φ)-Lagrangian subspaces is guaranteed.
Theorem 13.5.5. (a) Let X ∈Hn×n be (H, φ)-Hamiltonian. Then there exists
an X-invariant (H, φ)-Lagrangian subspace if and only if every root subspace of X
corresponding to eigenvalues with zero real part (including the zero eigenvalue if X
is not invertible) has even (quaternion) dimension, and for each such eigenvalue,
the signs in the sign characteristic corresponding to the odd multiplicities (if any)
associated with that eigenvalue, sum up to zero.
(b)
Let X ∈Hn×n be (H, φ)-skew-Hamiltonian.
Then there exists an X-
invariant (H, φ)-Lagrangian subspace if and only if every root subspace for X is
even dimensional.
Proof. Part (b). The “only if” part is clear. For the “if” part, in view of the
canonical form (13.2.6) for the pair of matrices (H, X), the proof reduces to two
cases:

320
CHAPTER 13
(i)
H = η1β(φ)Fm1 ⊕· · · ⊕ηpβ(φ)Fmp,
X = Jm1(γ)T ⊕· · · ⊕Jmp(γ)T ,
where γ is real, the ηj’s are signs ±1, and m1 + · · · + mp is even;
(ii)
H
=

0
Fℓ1
−Fℓ1
0

⊕· · · ⊕

0
Fℓq
−Fℓq
0

,
X
=
 Jℓ1(α)T
0
0
Jℓ1(α)T

⊕· · · ⊕
 Jℓq(α)T
0
0
Jℓq(α)T

,
where α ∈Inv (φ) \ R.
In Case (i), argue as in the proof of Part (b) of Theorem 10.6.1.
In Case
(ii), existence of an X-invariant (H, φ)-Lagrangian subspace is evident: one such
subspace is spanned by the following vectors:
e1, e2, . . . , eℓ1, e2ℓ1+1, . . . , e2ℓ1+ℓ2, . . . ,
e2ℓ1+2ℓ2+···+2ℓq−1+1, . . . , e2ℓ1+···+2ℓq−1+ℓq.
(13.5.13)
Part (a). The canonical form of Theorem 13.2.3 for the pair (H, X) will be used.
Thus, we need to consider only the following three cases:
(iii)
H
=

0
Fℓ1
−Fℓ1
0

⊕· · · ⊕

0
Fℓq
−Fℓq
0

,
X
=
 −Jℓ1(α)
0
0
Jℓ1(α)

⊕· · · ⊕
 −Jℓq(α)
0
0
Jℓq(α)

,
where α ∈Inv (φ), R(α) > 0;
(iv)
H = ⊕s
j=1
 κjβ(φ)Fpj

⊕⊕q
j=s+1β(φ)Fpj,
X = ⊕q
j=1β(φ)Jpj(0),
where p1, . . . , ps are odd, ps+1, . . . , pq are even, and the κj’s are signs ±1
(j = 1, 2, . . . , s);
(v)
H
=
⊕v
j=1
 δjβ(φ)Fℓj

⊕⊕q
j=v+1
 δjβ(φ)Fℓj

,
X
=
⊕q
j=1β(φ)Jℓj(τ),
τ < 0,
where ℓ1, . . . , ℓv are odd, ℓv+1, . . . , ℓq are even, and the δj’s are signs ±1
(j = 1, 2, . . . , q).
In case (iii) holds true, the condition in Theorem 13.5.5(a) is vacuous, and
indeed vectors (13.5.13) span an X-invariant (H, φ)-Lagrangian subspace.

INDEFINITE INNER PRODUCTS: NONSTANDARD INVOLUTION
321
Suppose H and X are given as in (iv). For an (H, φ)-Lagrangian subspace to
exist, it is obviously necessary that s = 2u be even. Moreover, for the β(φ)-inertia
In± (H) of H we have
In+ (H) + In−(H) = n,
In+ (H) −In−(H) =
v
X
j=1
κj,
so the necessary condition (13.5.3) holds true if and only if κ1 + · · · + κs = 0.
This proved the “only if” direction of Theorem 13.5.5(a). For the “if” direction, we
rearrange (if necessary) the blocks in (iv) so that κj = (−1)j−1, for j = 1, 2, . . . , s =
2u. Select the vectors in Hn×1 as in (10.3.5)–(10.3.10). By inspection, the selected
vectors span an X-invariant (H, φ)-Lagrangian subspace.
Finally, suppose H and X are given as in (v). As in Case (iv), we verify that the
conditions of Theorem 13.5.5(a) are necessary for existence of an (H, φ)-Lagrangian
subspace. The converse is proved as in Case (iv), by selecting the vectors (10.3.5)–
(10.3.10).
□
A statement analogous to Remark 13.5.4 holds true also for X-invariant (H, φ)-
Lagrangian subspaces, in case H is φ-skewhermitian and invertible and X is (H, φ)-
Hamiltonian.
13.6
BOUNDEDNESS OF SOLUTIONS OF DIFFERENTIAL
EQUATIONS
Consider the system of diﬀerential equations with constant coeﬃcients
Aℓx(ℓ)(t) + Aℓ−1x(ℓ−1)(t) + · · · + A1x′(t) + A0x(t) = 0,
t ∈R,
(13.6.1)
where Aℓ, . . . , A1, A0 ∈Hn×n, and x(t) is an unknown ℓtimes continuously diﬀer-
entiable Hn×1-valued function of the real independent variable t. It will be assumed
in addition that Ak is φ-hermitian if k is odd, Ak is φ-skewhermitian if k is even,
and Aℓis invertible.
In this section we study boundedness properties of solutions of (13.6.1). The
development here is parallel to that of Section 10.4. In particular, we will apply
to (13.6.1) the deﬁnitions of bounded, forward bounded, backward bounded, and
stably bounded systems given in Section 10.4.
As in Section 10.4, consider the companion matrix associated with system
(13.6.1):
C =


0
In
0
. . .
0
0
0
In
. . .
0
...
...
...
...
...
0
0
0
. . .
In
−A−1
ℓA0
−A−1
ℓA1
−A−1
ℓA2
. . .
−A−1
ℓAℓ−1


∈Hℓn×ℓn,
(13.6.2)

322
CHAPTER 13
and deﬁne
G :=


A1
A2
A3
. . .
Aℓ
−A2
−A3
. . .
−Aℓ
0n
A3
A4
. . .
0n
0n
−A4
. . .
. . .
0n
0n
...
...
...
...
...
(−1)ℓ−1Aℓ
0n
0n
. . .
0n


∈Hnℓ×nℓ.
(13.6.3)
Clearly, G is φ-hermitian and invertible.
A computation shows that that the matrix GC is φ-skewhermitian; in other
words, C is (G, φ)-skewsymmetric. This is a key fact that allows us to use the
canonical form (13.1.3) of the pair of matrices (G, C) in the study of (13.6.1).
Theorem 13.6.1. The following four statements are equivalent for the system
(13.6.1), where Ak is φ-hermitian if k is odd, Ak is φ-skewhermitian if k is even,
and Aℓis invertible:
(a) The system is forward bounded.
(b) The system is backward bounded.
(c) The system is bounded.
(d) All eigenvalues of C have zero real parts, and for every eigenvalue the geo-
metric multiplicity coincides with the algebraic multiplicity.
The proof is analogous to the proof of Theorem 10.4.2, and is omitted.
At present, we do not have results concerning stable boundedness of (13.6.1).
Thus, we formulate an open problem.
Open Problem 13.6.2. Develop criteria for stable boundedness of system of
the type (13.6.1), under the hypotheses that Ak is φ-hermitian if k is odd, Ak is
φ-skewhermitian if k is even, and Aℓis invertible.
In particular, is the following statement true?
(A) Assume that the system (13.6.1) is bounded, and for every nonzero eigenvalue
α of C with zero real part (if any), the signs in the sign characteristic of (G, C)
corresponding to the eigenvalues similar to α are all equal. Then (13.6.1) is
stably bounded.
In a similar vein the system of diﬀerential equations (13.6.1) is studied, where
now the following is assumed.
(B) Ak is φ-hermitian if k is even, Ak is φ-skewhermitian if k is odd, and Aℓis
invertible.
In this case, the matrix G deﬁned by (13.6.3) is φ-skewhermitian and C is (G, φ)-
Hamiltonian. The result of Theorem 13.6.1 remains valid, with a proof similar to
that of Theorem 10.4.2.
To state and prove our result on stable boundedness under hypothesis (B), it
will be convenient to introduce the following concept.

INDEFINITE INNER PRODUCTS: NONSTANDARD INVOLUTION
323
Deﬁnition 13.6.3. Given a matrix Y ∈Hn×n and a subspace M ⊆Hn×1, we
say that M is (Y, φ)-deﬁnite if
min{x∈M : ∥x∥=1} |xφY x| > 0.
Some properties of (Y, φ)-deﬁnite subspaces are collected in the next proposition.
Proposition 13.6.4. Let Y ∈Hn×n and M ⊆Hn×1. Then the following state-
ments are equivalent:
(i) M is (Y, φ)-deﬁnite.
(ii) xφY x ̸= 0 for every nonzero x ∈M.
(iii) min{x∈M : ∥x∥≥ε} |xφY x| > 0 for some (equivalently, all) positive ε.
(iv) The subspace S−1(M) is (SφY S, φ)-deﬁnite for some (equivalently, all) in-
vertible matrices S ∈Hn×n.
(v) All nearby subspaces, including M itself, are (Y, φ)-deﬁnite; in other words,
there exists δ > 0, which depends on Y and M only, for ﬁxed φ, such that
every subspace N ⊆Hn×n satisfying θ(M, N) < δ is (Y, φ)-deﬁnite.
Proof. The equivalence of (i) and (iii) and implication (i) ⇒(ii) are easily
seen by scaling the vector x ∈M. To show (ii) ⇒(i), note that by continuity
of the function |xφY x|, x ∈M \ {0}, and by compactness of the unit sphere
{x ∈M : ∥x∥= 1} in M, the inﬁmum
inf{x∈M : ∥x∥=1} |xφY x|
is attained (Theorem 3.10.5); therefore, the inﬁmum cannot be zero in view of (ii).
Hence, M is (Y, φ)-deﬁnite. We leave it to the reader to prove the equivalence of
(i) and (iv).
Clearly (v) implies (i). The opposite implication can be proved as in the proof
of Theorem 10.4.4, Step 2.
□
It turns out that existence of deﬁnite subspaces imposes some restrictions on
the matrix.
Proposition 13.6.5. If M is a (Y, φ)-deﬁnite subspace, then
dimH (Ker (Y −Yφ) ∩M) ≤1.
Proof. Assuming the contrary, let M0 be a two-dimensional subspace in M
such that (Y −Yφ)x = 0 for every x ∈M0. Then
xφY x
=
xφ
1
2(Y + Yφ)

x + xφ
1
2(Y −Yφ)

x
=
xφ
1
2(Y + Yφ)

x,
for all x ∈M0.
(13.6.4)
By Theorem 4.1.2(a), in a suitable basis for M0, the φ-hermitian matrix 1
2(Y +Yφ)
restricted to M0 has one of the three forms: I2, 02, or
 1
0
0
0

. In all three cases,

324
CHAPTER 13
in view of (13.6.4), there exists a nonzero vector x0 ∈M0 such that (x0)φY x0 = 0.
Indeed, this is obvious if
1
2(Y + Yφ) restricted to M0 is 02 or
 1
0
0
0

, and if
1
2(Y + Yφ) restricted to M0 is I2, this follows from Lemma 4.4.1. Existence of such
x0 contradicts the hypothesis that M is (Y, φ)-deﬁnite.
□
We now return to system (13.6.1). Under hypothesis (B), suﬃcient conditions
for stable boundedness are given in the next theorem.
Theorem 13.6.6. Consider system (13.6.1) under hypothesis (B), and let C and
G be deﬁned by (13.6.2) and (13.6.3), respectively. Then the following statements
are equivalent:
(a) Every root subspace of C is (G, φ)-deﬁnite.
(b) System (13.6.1) is bounded, and for every eigenvalue α of C with zero real part,
the signs in the sign characteristic of (G, C) corresponding to the eigenvalues
similar to α are all equal.
Also, each condition (a) or (b) implies:
(c) System (13.6.1) is stably bounded.
If ℓ= 1, then the conditions (a), (b), and (c) are equivalent.
Proof. Proof of (b) ⇒(a). By Theorem 13.6.1 (which holds true under hypoth-
esis (B) as well), all eigenvalues of C have zero real parts, and for every eigenvalue
of C, the geometric and algebraic multiplicities coincide. In view of the canonical
form of the (G, φ)-Hamiltonian matrix C (Theorem 13.2.3), we need only to verify
that for G0 = β(φ)Ik the space Hk×1 is (G0, φ)-deﬁnite. To this end, note the
equality αφβ(φ)α = β(φ)α∗α for all α ∈H, which can be veriﬁed by direct compu-
tation. Therefore, xφG0x = β(φ)∥x∥2 for all x ∈Hk×k, and the (G0, φ)-deﬁniteness
of Hk×1 follows.
Proof of (a) ⇒(b). The canonical form of (G, C) (Theorem 13.2.3) shows that all
eigenvalues of C have zero real parts, and for every eigenvalue of C, the geometric
and algebraic multiplicities coincide (otherwise, there exists an eigenvector x of
C such that xφGx = 0, a contradiction with (a)), so by Theorem 13.6.1 system
(13.6.1) is bounded. Arguing by contradiction, suppose the condition on signs in
(b) is not satisﬁed; we then produce an eigenvector x0 of C such that (x0)φGx0 = 0,
a contradiction with (a). Indeed, in view of the canonical form, we need to consider
only
G0 = β(φ)Ip ⊕−β(φ)Iq,
C0 = −β(φ)τIp+q,
in place of G, C, respectively, where p, q are positive integers and τ is a nonpositive
real number. Then set x0 = e1 + ep+1.
The proof that (b) and/or (a) imply (c) is completely analogous to that of
Theorem 10.4.4, Step 2, and, therefore, is omitted.
Finally, assume ℓ= 1, so (13.6.1) takes the form A1x′ + A0x = 0, where A1 ∈
Hn×n is φ-skewhermitian invertible and A0 ∈Hn×n is φ-hermitian, with C =
A−1
1 A0, G = A1. Suppose (c) holds true. In particular, (13.6.1) is bounded, so by
Theorem 13.6.1 all eigenvalues of C have zero real parts, and for every eigenvalue the
geometric and algebraic multiplicities coincide. Arguing by contradiction, suppose

INDEFINITE INNER PRODUCTS: NONSTANDARD INVOLUTION
325
that there is an eigenvalue α0 of C such that the signs in the sign characteristic of
(C, G) corresponding to α0 are not all equal. We will construct a (G, φ)-Hamiltonian
matrix eC that is arbitrarily close to C and has eigenvalues with nonzero real parts,
thereby contradicting the hypothesis that (13.6.1) is stably bounded. Using the
canonical form for (C, G) (Theorem 13.2.3), it suﬃces to consider two cases:
(1)
G =
 β(φ)
0
0
−β(φ)

,
C = 02;
(2)
G =
 β(φ)
0
0
−β(φ)

,
C =
 −β(φ)τ
0
0
−β(φ)τ

, where τ is real
and negative.
These two cases can be combined as one by assuming τ ≤0. Set
eC =

−β(φ)τ
x
x
−β(φ)τ

,
where x > 0 is close to zero. Then eC is (G, φ)-Hamiltonian and has eigenvalues
with real parts ±x, as required.
□
Open Problem 13.6.7. Are (a), (b), and (c) of Theorem 13.6.6 equivalent if
ℓ> 1?
13.7
EXERCISES
In all exercises in this chapter, φ is a nonstandard involution.
Ex. 13.7.1. Prove Proposition 13.0.1.
Ex.
13.7.2. Show that system (13.6.1), under the hypotheses that Ak is φ-
hermitian if k is odd, Ak is φ-skewhermitian if k is even, and Aℓis invertible,
is stably bounded if it is bounded and all root subspaces of C corresponding to
eigenvalues with zero real parts (if any) are 1-dimensional.
Ex. 13.7.3. Let H be φ-skewhermitian and A be (H, φ)-Hamiltonian. Prove
that a root subspace M of A is (H, φ)-deﬁnite if and only if xφHx ̸= 0 for every
eigenvector x ∈M of A. Hint: Use the canonical form of Theorem 13.2.3.
Ex. 13.7.4. Consider system (13.6.1), and assume hypothesis (B). Show that if
xφGx ̸= 0 for every eigenvector x of C, then the system (13.6.1) is stably bounded.
(G and C are given by (13.6.3) and (13.6.2), respectively.)
Ex. 13.7.5. Consider system (13.6.1), and assume hypothesis (B). Prove that
if every root subspace of C is G-deﬁnite, then every system suﬃciently close to
(13.6.1) and satisfying hypothesis (B) is stably bounded; in other words, there
exists ε > 0 such that the system
A′
ℓx(ℓ)(t) + A′
ℓ−1x(ℓ−1)(t) + · · · + A′
1x′(t) + A′
0x(t) = 0,
A′
0, . . . , A′
ℓ∈Hn×n,
is stably bounded as soon as A′
k is φ-hermitian if k is even, A′
k is φ-skewhermitian
if k is odd, and inequalities ∥A′
k −Ak∥< ε for k = 1, 2, . . . , ℓhold true. (Note that
∥A′
ℓ−Aℓ∥< ε for suﬃciently small ε guarantees invertibility of A′
ℓ.)

326
CHAPTER 13
Ex. 13.7.6. Is the result of Ex. 13.7.5 valid if it assumed that xφGx ̸= 0 for
every eigenvector x of C, instead of assuming that every root subspace of C is
G-deﬁnite?
Ex. 13.7.7. Prove Proposition 13.5.2.
Ex. 13.7.8. Complete the details in the proof of Remark 13.5.4.
Ex. 13.7.9. Verify the formulas (13.3.6), (13.3.7), (13.3.8), and (13.3.9).
Ex. 13.7.10. Let H ∈Hn×n be an invertible φ-hermitian matrix. Prove that if
two (H, φ)-symmetric matrices A and A′ are similar, then A′ = T −1AT for some
(H, φ)-orthogonal matrix T.
Ex. 13.7.11. Determine whether or not the result analogous to Ex. 13.7.10
holds true in each of the following three contexts:
(1) H is an invertible φ-hermitian matrix, and A, A′ are (H, φ)-skewsymmetric
similar matrices.
(2) H is an invertible φ-skewhermitian matrix, and A, A′ are similar (H, φ)-
Hamiltonian matrices.
(3) H is an invertible φ-skewhermitian matrix, and A, A′ are similar (H, φ)-skew-
Hamiltonian matrices.
Ex. 13.7.12. If the result analogous to Ex. 13.7.10 does not generally hold in
one of the situations (1), (2), or (3) of Ex. 13.7.11, then ﬁnd conditions on the
Jordan structure of an (H, φ)-skewsymmetric, resp. (H, φ)-Hamiltonian or (H, φ)-
skew-Hamiltonian, matrix A that would guarantee the following property:
(a) If A′ is an (H, φ)-skewsymmetric, resp. (H, φ)-Hamiltonian or (H, φ)-skew-
Hamiltonian, matrix that is similar to A, then there exists an (H, φ)-ortho-
gonal, resp. (H, φ)-symplectic, matrix T such that A′ = T −1AT.
Ex. 13.7.13. Determine the possible canonical forms (13.2.1) and (13.2.6) under
each of the following four conditions:
(a) H is β(φ)-positive deﬁnite.
(b) The β(φ)-inertia of H is (n −1, 1, 0).
(c) The rank of A is at most 2.
(d) Rank (A) = 3.
Ex. 13.7.14. Find all A-invariant maximal (H, φ)-nonnegative subspaces and
all A-invariant maximal (H, φ)-nonpositive subspaces for each of the following pairs
of matrices (Aj, Hj), j = 1, 2, where β = β(φ) and the nonzero quaternion q is such
that qβ = −βq:
(a)
H1 =


0
0
β
0
β
0
β
0
0

, A1 =


0
0
0
0
q
q
0
q
q

.
(b)
H2 = H1, A2 =


0
0
1
0
0
0
0
0
0

.

INDEFINITE INNER PRODUCTS: NONSTANDARD INVOLUTION
327
Ex. 13.7.15. Suppose that either H ∈Hn×n is φ-skewhermitian and invertible
and A is (H, φ)-skew-Hamiltonian, or H is φ-hermitian and A is (H, φ)-symmetric.
Show that if there exists an A-invariant H-Lagrangian subspace, then for every
ε > 0 there is an (H, φ)-skew-Hamiltonian, resp. (H, φ)-symmetric, matrix A′ for
which no A′-invariant H-Lagrangian subspace exists and such that ∥A′ −A∥< ε.
Hint: Assume H is φ-hermitian, and let H and A be given by the right-hand
sides of the canonical form (13.1.1). Let
A′ = eJℓ1(α1) ⊕Jℓ2(α2) ⊕· · · ⊕Jℓq(αq),
where
eJℓ1(α1) := Jℓ1(α1) +

0(ℓ1−1)×1
0(ℓ1−1)×(ℓ1−1)
ε
01×(ℓ1−1)

,
ε > 0.
What are the eigenvalues of eJℓ1(α1)?
Ex. 13.7.16. Prove that each of the following four sets, for a ﬁxed invertible
H ∈Hn×n, is closed:
{A ∈Hn×n
:
H = Hφ,
A is (H, φ)-symmetric,
and there exist A-invariant H-Lagrangian subspaces}.
{A ∈Hn×n
:
H = Hφ,
A is (H, φ)-skewsymmetric,
and there exist A-invariant H-Lagrangian subspaces}.
{A ∈Hn×n
:
H = −Hφ,
A is (H, φ)-Hamiltonian,
and there exist A-invariant H-Lagrangian subspaces}.
{A ∈Hn×n
:
H = −Hφ,
A is (H, φ)-skew-Hamiltonian,
and there exist A-invariant H-Lagrangian subspaces}.
Hint: Show that if a sequence {Am}∞
m=1 belongs to one of those sets and if eA =
limm→∞Am, then eA also belongs to the set.
Ex. 13.7.17. Are the sets of Ex. 13.7.16 closed if H is not assumed to be ﬁxed?
In other words, determine if the following set is closed:
{A ∈Hn×n
:
for some invertible H = Hφ,
A is (H, φ)-symmetric,
and there exist A-invariant H-Lagrangian subspaces}.
Do the same for three other sets in Ex. 13.7.16.
13.8
NOTES
The material and presentation in this chapter is based on Rodman [132, 133]. In
particular, the proofs are taken from these two papers.
The results of Theorems 13.1.2, 13.1.1, 13.2.4, and 13.2.3 can be found, for
example, in Djokovi´c et al. [34]. Theorem 13.1.1 is a particular case of Theorem
7.5 of Rodman [132]. (The setting of Rodman [132, Theorem 7.5] is more general in
that the symmetry is understood with respect to several nonstandard involutions.)
Theorems 13.1.2 and 13.2.3 are found in Rodman [133, Theorems 6.2 and 6.4,
respectively].
The canonical form for H-skew-Hamiltonian matrices of Theorem 13.2.4 is taken
from Rodman [132, Theorem 8.9].

Chapter Fourteen
Matrix equations
Here, we present applications to polynomial matrix equations, algebraic Riccati
equations, and linear quadratic regulators.
Without attempting to develop in-
depth exposition of the topics (this would take us too far aﬁeld), we present these
applications in basic forms. Maximal invariant semideﬁnite or neutral subspaces
will play a key role.
14.1
POLYNOMIAL EQUATIONS
The approach to studying polynomial equations using companion matrices of Sec-
tion 5.12 extends to polynomial matrix equations. Consider the matrix equation
Zn + An−1Zn−1 + · · · + A1Z + A0 = 0,
(14.1.1)
where A0, . . . , An−1 ∈Hm×m are given and Z ∈Hm×m is the unknown matrix. Let
C be the block companion matrix corresponding to equation (14.1.1):
C =


0
Im
0
0
. . .
0
0
0
Im
0
. . .
0
...
...
...
...
. . .
...
0
0
0
0
. . .
Im
−A0
−A1
−A2
−A3
. . .
−An−1


∈H(mn)×(mn).
Theorem 14.1.1. There exists a one-to-one correspondence between solutions
Z of (14.1.1) and C-invariant subspaces M which are direct complements to the
subspace SpanH {em+1, em+2, . . . , emn} in H(mn)×(mn). The correspondence is given
by the formula
M = Ran







Im
Z
...
Zn−1






.
(14.1.2)
Proof. If Z is a solution of (14.1.1), then we have
C col (Zj)n−1
j=0 = col (Zj)n−1
j=0 Z,
(14.1.3)
which implies that the subspace Ran col (Zj)n−1
j=0 is C-invariant. (Recall that we
use col (Zj)n−1
j=0 for the matrix in the right-hand side of (14.1.2).) Since the matrix

Im
0
col (Zj)n−1
j=1
Imn−m


MATRIX EQUATIONS
329
is invertible, it follows that the subspaces Ran col (Zj)n−1
j=0 and
SpanH {em+1, em+2, . . . , emn} = Ran

0m×m(n−1)
Im(n−1)×m(n−1)

(14.1.4)
are direct complements to each other.
Conversely, if M ⊆Hmn×1 is a subspace as in Theorem 14.1.1, then the condi-
tion that M is a direct complement to (14.1.4) means that
M = Ran
 col (Xj)n
j=1

for some matrices X1, . . . , Xn ∈Hm×m with invertible X1. Multiplying col (Xj)n
j=1
on the right by X−1
1 , we may assume that, in fact, X1 = Im. Now the condition
that M is C-invariant implies that Xj = Xj−1X2 for j = 2, 3, . . . , n, so M has
the form (14.1.2). Now the C-invariance of M means that equality (14.1.3) holds,
and equating the block bottom row in the both sides of (14.1.3), we see that Z is
a solution of (14.1.1).
□
In contrast with the scalar polynomial equation (5.12.3), the matrix equation
(14.1.1) is not guaranteed a solution.
Example 14.1.2. The equation
Z2 =
 0
1
0
0

,
Z ∈H2×2,
(14.1.5)
has no solutions Z. Indeed, if such Z existed, then it must have the only eigenvalue
zero. However, both possibilities for the Jordan form of Z—namely,
 0
1
0
0

and
02×2—lead to a contradiction with (14.1.2).
□
In view of Example 14.1.2, it it of interest to develop criteria, or suﬃcient
conditions, for a matrix polynomial equation (14.1.1) to have solutions. Theorems
14.1.3 and 14.1.5 below provide such results.
Theorem 14.1.3. Assume that the companion matrix C has nm mutually non-
similar eigenvalues. Then equation (14.1.1) has solutions.
The proof of Theorem 14.1.3 is obtained at once by combining Theorem 14.1.1
and the following general result concerning direct complements (take v1, . . . , vmn of
Proposition 14.1.4 to be the eigenvectors of C that correspond to the nm mutually
nonsimilar eigenvalues; see Proposition 5.3.9).
Proposition 14.1.4. Let v1, . . . , vnm be a basis for Hnm×nm. Then for every
subspace M ⊆Hnm×nm, M ̸= Hnm×nm, there are vectors vi1, . . . , vinm−s among
v1, . . . , vnm such that SpanH {vi1, . . . , vinm−s} and M are direct complements to
each other; here s = dim M.
For a proof of Proposition 14.1.4, apply the replacement theorem 3.1.1, with
p = mn and u1, . . . , us a basis for M.
Theorem 14.1.5. (a) If A0, A1 ∈Hm×m are hermitian, then the equation
Z2 + A1Z + A0 = 0,
Z ∈Hm×m,

330
CHAPTER 14
has a solution.
(b) If A0, A1, A2 ∈Hm×m are hermitian, then the equation
Z3 + A2Z2 + A1Z + A0 = 0,
Z ∈Hm×m,
has a solution.
Proof. We start with Part (b). Let
G =


−A1
−A2
−Im
−A2
−Im
0
−Im
0
0

,
C =


0
Im
0
0
0
Im
−A0
−A1
−A2

.
Then G = G∗and (GC)∗= GC. Also,
In+ G = m,
In−G = 2m,
in view of Lemma 10.8.1. By Theorem 10.2.1, there exists an m-dimensional G-
nonnegative C-invariant subspace M.
We claim that M is a direct complement to Span {em+1, em+2, . . . , e3m} in
H3m×1. Suppose not; then there exists a nonzero vector
y =


0
y1
y2

∈M,
y1, y2 ∈Hm×1.
A computation shows that y∗Gy = −y∗
1y1, and since M is G-nonnegative, we must
have y1 = 0. Since M is C-invariant, we now have
Cy =


0
y2
−A2y2

∈M,
and analogous computation shows y2 = 0, a contradiction. Now an application of
Theorem 14.1.1 yields part (b).
Part (a). Here we let
G =
 A1
Im
Im
0

,
C =

0
Im
−A0
−A1

.
We have G = G∗, (GC)∗= GC, and In+ G = In−G = m. By Theorem 10.2.1, there
exists an m-dimensional G-nonnegative C-invariant subspace N. The subspace N
is a direct complement to Span {em+1, . . . , e2m} in H2m×1. Indeed, otherwise we
would have a vector
y =
 0
y1

∈N,
where y1 ∈Hm×1 \ {0}.
Then
Cy =

y1
−A1y1

∈N,
and a computation shows that

y∗
(Cy)∗

G [y
Cy] =

0
y∗
1y1
y∗
1y1
⋆

,
(14.1.6)
where ⋆stands for an entry of no immediate interest. Since y1 ̸= 0, the right-hand
side of (14.1.6) is an indeﬁnite 2×2 hermitian matrix, a contradiction with N being
G-nonnegative. As in the proof of part (b), it remains to apply Theorem 14.1.1.
□

MATRIX EQUATIONS
331
14.2
BILATERAL QUADRATIC EQUATIONS
In the rest of this chapter, we study quadratic matrix equations of the form
ZBZ + ZA −DZ −C = 0,
(14.2.1)
where A ∈Hn×n, B ∈Hn×m, C ∈Hm×n, D ∈Hm×m are given matrices and
solutions Z ∈Hm×n are to be found. Equation (14.2.1) is termed bilateral because
the unknown matrix Z appears on both sides in the expression in the left-hand side
of (14.2.1), in contrast with the unilateral equation (14.1.1), where the unknown
matrix appears only on the right side of the expression in (14.1.1).
Deﬁnition 14.2.1. For any Z ∈Hm×n, we call the n-dimensional subspace
G(Z) := Ran
 In
Z

⊆H(m+n)×1
the graph of Z.
We connect solutions of (14.2.1) with invariant subspaces of the (m+n)×(m+n)
matrix
T =
 A
B
C
D

.
(14.2.2)
Proposition 14.2.2. For any Z ∈Hm×n, the graph of Z is T-invariant if and
only if Z is a solution of (14.2.1).
Proof. If the graph of Z is T-invariant, then
 A
B
C
D
  In
Z

=
 In
Z

X
(14.2.3)
for some matrix X. The ﬁrst block row in this equality gives X = A + BZ, and
substituting into the second block row gives C + DZ = Z(A + BZ); i.e., Z is a
solution of (14.2.1). Conversely, if Z is a solution of (14.2.1), then (14.2.3) holds
with X = A + BZ.
□
Clearly, G(Z1) = G(Z2) holds for Z1, Z2 ∈Hm×n if and only if Z1 = Z2. Thus,
Proposition 14.2.2 establishes a one-to-one correspondence between solutions of
(14.2.1) and certain T-invariant subspace—namely, those T-invariant subspaces
that are graph subspaces or, in other words, T-invariant subspaces that are direct
complements to SpanH {en+1, . . . , em+n} in H(m+n)×1. Although T has invariant
subspaces of every dimension from 0 to m + n, T-invariant subspaces of dimension
n that are graph subspaces need not exist.
Example 14.2.3. Equation (14.1.5) of Example 14.1.2 can be recast in the form
(14.2.1) with
B = I,
A = D = 0,
C =
 0
1
0
0

.
Then
T =


0
0
1
0
0
0
0
1
0
1
0
0
0
0
0
0

.

332
CHAPTER 14
It is easy to see that the Jordan from of T is J4(0), and the unique 2-dimensional T-
invariant subspace is SpanH {e1, e3}, which is not a graph subspace. Thus, (14.1.5)
has no solutions.
□
Note that the set of (m + n) × (m + n)-matrices that have m + n pairwise
nonsimilar eigenvalues is open and dense in H(m+n)×(m+n). Indeed, the openness
of this set follows from the continuous dependence of eigenvalues on the matrix
(Theorem 5.2.5), and the denseness can be easily obtained using the Jordan form
of a matrix T ∈H(m+n)×(m+n) (perturb the diagonal entries in the Jordan form
so that the resulting matrix has m + n pairwise nonsimilar eigenvalues). For such
matrices there exists a basis of eigenvectors in H(m+n)×1 (Proposition 5.3.9). It
follows that generically equation (14.2.1) has a ﬁnite number of solutions, and the
number of solutions does not exceed (m + n)!/(m! n!), the number of ways a set of
n vectors can be chosen from a basis of m + n eigenvectors of T in H(m+n)×1.
14.3
ALGEBRAIC RICCATI EQUATIONS
Here we specialize equation (14.2.1) by introducing certain symmetries and chang-
ing the notation somewhat. The equation we consider now has the form
ZDZ + ZA + A∗Z −C = 0,
(14.3.1)
where all matrices are in Hn×n and D and C are assumed to be hermitian. Equation
(14.3.1) is known as algebraic Riccati equation.
Introduce the matrices
T =
 A
D
C
−A∗

,
K =

0
In
−In
0

,
H =
 −C
A∗
A
D

.
(14.3.2)
Then KT is hermitian and HT is skewhermitian, so T is K-Hamiltonian and (as-
suming H is invertible) H-skewhermitian.
Because of the symmetry of equation (14.3.1), we can say more about T-
invariant subspaces.
Proposition 14.3.1. Let Z be a solution of (14.3.1), with the corresponding
graph subspace G(Z). Then:
(1) Z is hermitian if and only if G(Z) is K-neutral;
(2) the graph G(Z) is H-nonpositive, resp. H-nonnegative, if and only if
(Z∗−Z)(A + DZ) ≤0,
resp. (Z∗−Z)(A + DZ) ≥0;
(3) the graph G(Z) is H-neutral if and only if
(Z∗−Z)(A + DZ) = 0.
Note that the matrix (Z∗−Z)(A + DZ) in (3) is hermitian.
The proof of Proposition 14.3.1 is by straightforward computation; in particular,
the following equality is used:
 I
Z
∗
H
 I
Z

= (Z∗−Z)(A + DZ) + ZDZ + ZA + A∗Z −C.

MATRIX EQUATIONS
333
In view of Proposition 14.2.2, we would like to identify situations when an n-
dimensional T-invariant subspace is a graph subspace. This is the subject matter
of the next lemma.
Deﬁnition 14.3.2. A pair of matrices (A, B), where A ∈Hp×p and B ∈Hp×q,
is said to be controllable if
Hp×1 =
∞
X
j=0
Ran (AjB).
(14.3.3)
This terminology comes from consideration of linear time-invariant control sys-
tems
dx
dt = Ax(t) + Bu(t),
t ≥0
(14.3.4)
(cf. (14.3.16)). One can show that (A, B) is controllable if and only if the state
of system (14.3.4), represented by the vector x(t), can be driven from any position
to any other position in a prescribed period of time by a suitable choice of the
continuous control function u(t). This is a standard fact in the theory of linear
control systems; for a proof in the context of complex matrices; see, e.g., Gohberg
et al. [54, Section 8.2].
Since the degree of the minimal polynomial for A does not exceed 2p (Ex.
5.16.23), condition (14.3.3) is equivalent to
Hp×1 =
2p−1
X
j=0
Ran (AjB).
(14.3.5)
We leave the proof as Ex. 14.4.3. (For a stronger result, see Ex. 14.4.4.)
Lemma 14.3.3. Assume that D is positive semideﬁnite and the pair (A, D)
is controllable. Let L be an n-dimensional T-invariant H-nonpositive subspace of
H2n×1. Then L is a graph subspace.
Proof. For the subspace L, write
L = Ran
 X1
X2

,
where X1, X2 ∈Hn×n.
We are going to prove that X1 is invertible.
Observe that T-invariance of L means
 A
D
C
−A∗
  X1
X2

=
 X1
X2

Q
for some Q ∈Hn×n, i.e.,
AX1 + DX2 = X1Q,
(14.3.6)
CX1 −A∗X2 = X2Q.
(14.3.7)
The H-nonpositivity of L means that the matrix
[X∗
1
X∗
2]
 −C
A∗
A
D
  X1
X2

= X∗
2DX2 + X∗
1A∗X2 + X∗
2AX1 −X∗
1CX1
(14.3.8)

334
CHAPTER 14
is negative semideﬁnite.
Let K = Ker X1. We have for every x ∈K:
x∗X∗
2DX2x + x∗X∗
1A∗X2x + x∗X∗
2AX1x −x∗X∗
1CX1x = x∗X∗
2DX2x,
which is real nonpositive in view of the negative semideﬁniteness of (14.3.8). But
D ≥0, so we must have X2x ∈Ker D—in other words,
X2(K) ⊆Ker D.
(14.3.9)
Further, equation (14.3.6) implies that Q(K) ⊆K. Now equation (14.3.7) gives, for
every x ∈K,
A∗X2x = −CX1x + A∗X2x = −X2Qx ∈X2(K).
Thus,
A∗X2(K) ⊆X2(K) ⊆Ker D.
(14.3.10)
We claim that, more generally,
(A∗)rX2(K) ⊆Ker D,
r = 0, 1, . . . .
(14.3.11)
Indeed, we have already proved (14.3.11) for r = 0 and r = 1. Arguing by induction,
assume (14.3.11) holds true for r −1. Using (14.3.10), it is found that
(A∗)r(X2(K)) = (A∗)r−1(A∗X2(K)) ⊆(A∗)r−1(X2(K)) ⊆Ker D,
and (14.3.11) follows. Now, for every x ∈K, we have


D
DA∗
...
D(A∗)n−1

X2x = 0,
or
(X2x)∗[D
AD
A2D
. . .
An−1D] = 0,
and in view of controllability of (A, D), the equality X2x = 0 is obtained. But the
only vector x for which X1x = X2x = 0 is the zero vector, because otherwise the
dimension of L would be smaller than n, a contradiction with our assumption. So
K = {0}, and X1 is invertible. Now
L = Ran

I
X2X−1
1

is indeed a graph subspace.
□
Recall that T is H-skewhermitian (assuming that H is invertible). Thus, exis-
tence of T-invariant maximal H-nonnegative (or maximal H-nonpositive) subspaces
is ensured by Theorem 10.2.1. Combining this observation, with Proposition 14.3.1
and Lemma 14.3.3, we arrive at a complete description of a class of solutions of
(14.3.1) in terms of invariant subspaces.

MATRIX EQUATIONS
335
Theorem 14.3.4. Assume that D is positive semideﬁnite, the pair (A, D) is
controllable, and
In+ H = In−H = n,
(14.3.12)
where H =
 −C
A∗
A
D

(in particular H is invertible).
Then (14.3.1) admits
solutions Z such that
(Z∗−Z)(A + DZ) ≤0.
(14.3.13)
Moreover, the formula
M = Ran
 I
Z

(14.3.14)
establishes a one-to-one correspondence between solutions Z of (14.3.1) with the
property (14.3.13) and the set of T-invariant maximal H-nonpositive subspaces M.
Specializing to hermitian solutions, we obtain the following.
Theorem 14.3.5. Under the hypotheses of Theorem 14.3.4, equation (14.3.1)
admits hermitian solutions if and only if for every eigenvalue λ of T with zero
real part (if any), the number of odd partial multiplicities corresponding to λ is
even, and exactly half of them have sign −1 in in the sign characteristic of the
pair (T, H). Moreover, formula (14.3.14) establishes a one-to-one correspondence
between hermitian solutions Z of (14.3.1) and the set of T-invariant H-Lagrangian
subspaces M.
Proof. We verify ﬁrst that a solution Z of (14.3.1) satisﬁes
(Z∗−Z)(A + DZ) = 0
(14.3.15)
if and only if Z is hermitian. Indeed, the “if” part is trivial. Conversely, sup-
pose (14.3.15) holds true. By Proposition 14.3.1, G(Z) is H-neutral. Observe the
equality H = −KT. In particular, it follows that T is invertible, and G(Z), being
T-invariant, is also −T −1-invariant. Now, for x, y ∈G(Z), we have
x∗Ky = x∗H(−T −1)y = 0,
because G(Z) is H-neutral. Thus, G(Z) is K-neutral, and by Proposition 14.3.1,
Z is hermitian.
By the same Proposition 14.3.1, solutions Z with the property (14.3.15), or what
is the same, hermitian solutions, are characterized by the property that their graph
subspaces G(Z) are T-invariant H-Lagrangian. Now apply Theorem 10.3.3(b).
□
Algebraic Riccati equations are ubiquitous in many applications—in particular,
in control systems.
Several of these applications are outlined in Lancaster and
Rodman [91]. Here we present just one application—linear quadratic regulator, in
a basic form.
Consider a primitive time-invariant linear control system
dx
dt = Ax(t) + Bu(t),
x(0) = x0,
t ≥0,
(14.3.16)
where A ∈Hn×n, B ∈Hn×m, and x(t) ∈Hn×1 and u(t) ∈Hm×1 are vector valued
functions known as state and input (or control) vectors, respectively. Observe that

336
CHAPTER 14
x(t) is uniquely deﬁned by a given integrable control vector function u(t) and the
initial value x0:
x(t) = eAtx0 +
Z t
0
eA(t−s)Bu(s) ds,
t ≥0.
(14.3.17)
It will be assumed throughout that control functions u(t) are deﬁned for 0 ≤t < ∞
and have the property that u ∈L2
m(0, T) for all T > 0. Here the space L2
m(0, T) is
the quaternion Hilbert space of square integrable functions on (0, T) with values in
Hm×1 equipped with the quaternion-valued inner product
⟨y(t), z(t)⟩L2
m(0,T ) =
Z T
0
(z(t))∗y(t) dt.
Fix two matrices: Q ∈Hn×n positive semideﬁnite and R ∈Hm×m positive
deﬁnite. Deﬁne the quadratic cost functional by
Ju(x0) :=
Z ∞
0
[x(t)∗Qx(t) + u(t)∗Ru(t)] dt.
Here x(t) is given by formula (14.3.17). Thus, Ju(x0) is a function of the control
vector u(t) and the initial value x0, and 0 ≤Ju(x0) ≤∞. The optimal cost at x0
is deﬁned by
bJ(x0) := inf
u Ju(x0),
and an optimal control is then a control vector function u(t) for which the inﬁmum
is attained.
It turns out that, under appropriate hypotheses, the optimal control exists and
can be given by a feedback mechanism; i.e., the optimal control is coupled to the
state vector by an equation of the form u(t) = −Fx(t), for some ﬁxed (i.e., inde-
pendent of t) matrix F ∈Hm×n. For such u(t) the initial value problem (14.3.16)
takes the form
dx
dt
= (A −BF)x(t),
x(0) = x0,
with solution
x(t) = e(A−BF )tx0,
and so
u(t) = −Fe(A−BF )tx0,
t ≥0.
In connection with the linear quadratic regulator problem, introduce the follow-
ing algebraic Riccati equation:
ZBR−1B∗Z −ZA −A∗Z −Q = 0.
(14.3.18)
Here is the main result concerning the linear quadratic regulator.
Theorem 14.3.6. Assume (in addition to the assumptions made above) that
there exists Y ∈Hm×n such that all eigenvalues of A+BY have negative real parts,
and assume that the pair (A∗, Q) is controllable. Then there is a unique positive
semideﬁnite solution Z0 ∈Hn×n of (14.3.18). Moreover:
(a) bJ(x0) = (x0)∗Z0x0 for all x0 ∈Hn×1;

MATRIX EQUATIONS
337
(b) for each x0 there is a unique optimal control u(t); this control is determined
by the feedback matrix F = R−1B∗Z0, and then
u(t) = −R−1B∗Z0e(A−BR−1B∗Z0)tx0,
t ≥0;
(c) all eigenvalues of A −BR−1B∗Z0 have negative real parts;
(d) Z0 is positive deﬁnite;
(e) Z0 is the maximal hermitian solution of (14.3.18): if Z is any hermitian
solution of (14.3.18), then Z0 −Z is positive semideﬁnite.
Theorem 14.3.6 can be proved in the same way as the corresponding result for
complex matrices (see, e.g., Theorem 16.3.3 of Lancaster and Rodman [91]). A
complete proof of the theorem would take us too far aﬁeld; therefore, we omit
details and refer the reader to Lancaster and Rodman [91].
14.4
EXERCISES
Ex. 14.4.1. Show that the solution set of every matrix polynomial equation of
the form (14.1.1) is compact (or empty).
Ex. 14.4.2. Let A ∈Hp×p, B ∈Hp×q, and deﬁne the subspace
C(A, B) :=
∞
X
j=0
Ran (AjB) ⊆Hp×1.
Show that if
Ran (AkB) ⊆
k−1
X
j=0
Ran (AjB)
for some integer k ≥1, then C(A, B) = Pk−1
j=0 Ran (AjB).
Ex. 14.4.3. Prove that (14.3.5) is equivalent to controllability of the pair (A, B).
Hint: Use Ex. 14.4.2.
Ex. 14.4.4. Let A ∈Hp×p, B ∈Hp×q. Prove that the pair (A, B) is controllable
if and only if
Hp×1 =
p−1
X
j=0
Ran (AjB).
Hint: Prove that Ran (ApB) is contained in Pp−1
j=0 Ran (AjB). To this end, use the
Jordan form of A to reduce the proof to the case when A is a complex matrix; then
use the fact that a complex p × p matrix is a root of the characteristic polynomial
of the matrix which is of degree p.
Ex.
14.4.5. For all pairs of matrices A ∈Hp×p, B ∈Hp×q, prove that the
subspace P∞
j=0 Ran (AjB) is A-invariant.
Ex.
14.4.6. Show that (14.3.12) is satisﬁed if both matrices C and D are
positive deﬁnite.

338
CHAPTER 14
14.5
NOTES
Various types of quadratic equations with quaternion coeﬃcients have been treated
extensively in literature; see, e.g., Porter [121], Huang and So [68], and Janovska
and Opfer [73].
Lemma 14.3.3 was proved for complex matrices in ˇCurilov [31] and indepen-
dently by Lancaster and Rodman [88].
Exposition in Section 14.3 (except Theorem 14.3.5) is adapted from Lancaster
and Rodman [91].
Pairs of real matrices (A, H), where H is skewsymmetric and A is (H,T )-
Hamiltonian, play a key role in several important problems of applied analysis—in
particular, Riccati equations (which are ubiquitous in systems and control); see,
e.g., Abou-Kandil et al. [1], Lancaster and Rodman [91], Mehrmann [112], gyro-
scopic vibrating systems in Lancaster et al. [86], Hamiltonian systems, and trans-
fer functions with symmetries and their factorizations (see, e.g., Alpay et al. [2],
Furhmann [47], Lancaster and Rodman [90], and Ran and Rodman [126]). In the
framework of real matrices, invariant subspaces of H-Hamiltonian and symplectic
matrices that have neutrality or deﬁnitiveness properties with respect to the skew
symmetric inner product induced by H have been studied in Rodman [129, 130],
Lancaster and Rodman [89], Freiling et al.
[46], and Mehl et al.
[107].
Many
of these studies were motivated largely by applications to algebraic Riccati equa-
tions. Although of most importance are hermitian solutions of the algebraic Riccati
equations, nonhermitian solutions are also of interest (see, e.g., Abou-Kandil et al.
[1]).
Linear control systems with quaternion coeﬃcients have been studied in Pereira
et al. [117] and Pereira and Vettori [118].
The linear quadratic regulator problem (in the setting of real and complex
matrices) was treated ﬁrst in Kalman [76], and since then the topic has generated
an immense amount of literature.

Chapter Fifteen
Appendix: Real and complex canonical forms
For the reader’s convenience, we state here (without proof, but with references)
canonical forms for real and complex matrices and for pairs of real and complex
matrices, or matrix pencils, with symmetries. All these forms are known, and most
are well-known. Our main sources for the material in this chapter are Gantmacher
[48, 49] and expository papers by Thompson [151] and Lancaster and Rodman
[93, 92]; Thompson [151] also contains an extensive bibliography.
15.1
JORDAN AND KRONECKER CANONICAL FORMS
For Jordan forms, we use the complex and real Jordan blocks (1.2.1) and (1.2.2).
Theorem 15.1.1. Let F = R or F = C. Let A ∈Fn×n. Then there exists an
invertible S ∈Fn×n such that S−1AS has the form
S−1AS = Jm1(λ1) ⊕· · · ⊕Jmp(λp),
λ1, . . . , λp ∈C
(15.1.1)
if F = C or the form
S−1AS
=
Jm1(λ1) ⊕· · · ⊕Jmp(λp)
⊕J2mp+1(µp+1 ± iνp+1) ⊕· · · ⊕J2mq(µq ± iνq)
(15.1.2)
if F = R, where
λ1, . . . , λp, µp+1, νp+1, . . . , µq, νq ∈R
and νp+1, . . . , νq are positive.
The forms (15.1.1) and (15.1.2) are uniquely determined by A, up to an arbi-
trary permutation of the constituent Jordan blocks in the complex case, or up to an
arbitrary permutation of blocks in each of the two parts
⊕p
i=1 Jmi(λi)
and
⊕q
j=p+1 J2mj(µj ± iνj)
in the real case.
In (15.1.2) the cases when p = 0, resp. q = p, are not excluded; in these cases
A has no real eigenvalues, resp. has no nonreal eigenvalues.
Proofs of Theorem 15.1.1 in the complex case can be found in many textbooks
on linear algebra, such as Finkbeiner [42], Smith [147], Horn and Johnson [62],
Lancaster and Tismenetsky [94], Gohberg et al. [55], Gantmacher [48], and Meyer
[114]. In the real case, a complete proof is given in Gohberg et al. [54, Chapter 12],
Shilov [145], and Lancaster and Tismenetsky [94].
Let F = R or F = C.
Deﬁnition 15.1.2. Two matrix pencils A + tB, A′ + tB′, where A, B, A′, B′ ∈
Fm×n, are said to be F-strictly equivalent if PAQ = A′, PBQ = B′ for some
invertible matrices P ∈Fm×m, Q ∈Fn×n.

340
CHAPTER 15
For the Kronecker form, besides Jordan blocks, we also need singular blocks,
given by (1.2.10).
A compete proof of the following theorem is given, e.g., in
Gantmacher [48, 49] and in the Appendix of Gohberg et al. [54] for the complex
case and in Gantmacher [48, Chapter XII] for the real case.
Theorem 15.1.3. Let F = R or F = C. Every pencil A + tB ∈F(t)m×n is
F-strictly equivalent to a matrix pencil in the block diagonal form
0u×v
⊕
Lε1×(ε1+1) ⊕· · · ⊕Lεp×(εp+1) ⊕LT
η1×(η1+1) ⊕LT
ηq×(ηq+1)
⊕
(Ik1 + tJk1(0)) ⊕· · · ⊕(Ikr + tJkr(0))
⊕
(tIℓ1 + Jℓ1(α1)) ⊕· · · ⊕(tIℓs + Jℓs(αs))
(15.1.3)
if F = C and in the block diagonal form
0u×v
⊕
Lε1×(ε1+1) ⊕· · · ⊕Lεp×(εp+1) ⊕LT
η1×(η1+1) ⊕LT
ηq×(ηq+1)
⊕
(Ik1 + tJk1(0)) ⊕· · · ⊕(Ikr + tJkr(0))
⊕
(tIℓ1 + Jℓ1(α1)) ⊕· · · ⊕(tIℓw + Jℓw(αw))
⊕
(tI2ℓw+1 + J2ℓw+1(αw+1 ± iβw+1))
⊕
· · · ⊕(tI2ℓs + J2ℓs(αs ± iβs))
(15.1.4)
if F = R.
In (15.1.3) and (15.1.4) we have
ε1 ≤· · · ≤εp;
η1 ≤· · · ≤ηq;
k1 ≤· · · ≤kr
are positive integers, α1, . . . , αs ∈C if F = C and
α1, . . . , αw, αw+1, . . . , αs ∈R;
βw+1, . . . , βs > 0
if F = R.
Moreover, the integers u, v, and {εi}p
i=1, {ηj}q
j=1, {ky}r
y=1 are uniquely deter-
mined by the pair A, B, and the part
(tIℓ1 + Jℓ1(α1)) ⊕· · · ⊕(tIℓs + Jℓs(αs))
if F = C, or the part
(tIℓ1 + Jℓ1(α1)) ⊕· · · ⊕(tIℓw + Jℓw(αw))
⊕(tI2ℓw+1 + J2ℓw+1(αw+1 ± iβw+1)) ⊕· · · ⊕(tI2ℓs + J2ℓs(αs ± iβs))
if F = R, is uniquely determined by A and B up to an arbitrary permutation of the
diagonal blocks.
The cases when some of the parts in (15.1.3) and (15.1.4) are empty are not
excluded. A similar remark applies to other canonical forms in this chapter.
Two corollaries are noteworthy and will be used extensively in the text.
Corollary 15.1.4. Let F = R or F = C. If A, B ∈Fn×n are such that A⊕s is
similar (over F) to B⊕s for some positive integer s, then A is similar to B (over
F).

APPENDIX: REAL AND COMPLEX CANONICAL FORMS
341
Corollary 15.1.5. Let F = R or F = C. If A + tB and A′ + tB′ are matrix
pencils, where A, B, A′, B′ ∈Fm×n, are such that (A + tB)⊕s and (A′ + tB′)⊕s are
strictly equivalent (over F) for some positive integer s, then A + tB and A′ + tB′
are strictly equivalent (over F) as well.
Proof. The proofs of Corollaries 15.1.4 and 15.1.5 are based on the uniqueness
parts in Theorems 15.1.1 and 15.1.3. We provide details for the case F = C in
Corollary 15.1.4 only; all other cases in Corollaries 15.1.4 and 15.1.5 are treated
analogously. For a ﬁxed λ ∈C and ﬁxed positive integer k, let qλ,k ≥0, resp.
q′
λ,k ≥0, be the number of times the Jordan block Jk(λ) appears in the complex
Jordan form of A, resp. B. The condition that A⊕s is similar to B⊕s implies that
A⊕s and B⊕s have the same Jordan form, and so
sqλ,k = sq′
λ,k.
Thus, qλ,k = q′
λ,k. Consequently, A and B have the same complex Jordan form,
and their similarity (over C) follows.
□
15.2
REAL MATRIX PENCILS WITH SYMMETRIES
In this section we provide canonical forms for pairs of real matrices, either one of
which is symmetric or skewsymmetric, or what is the same, corresponding matrix
pencils.
In addition to the standard matrices Fm, Gm, the following standard
matrices and matrix pencils will be needed here:
Y2m =


0
1
0
0
−1
1
0
0
−1
. . .
1
0
0
−1
0


;
(15.2.1)
Z2m(t, µ, ν) := (t + µ)F2m + νY2m +
 F2m−2
0
0
02

,
where µ ∈R, ν ∈R \ {0}.
We start with the case of two real symmetric matrices.
Theorem 15.2.1. (a) Every matrix pencil A + tB, where A = AT ∈Rn×n,
B = BT ∈Rn×n, is R-congruent to a real symmetric matrix pencil of the form
0u
⊕

t


0
0
Fε1
0
0
0
Fε1
0
0

+ G2ε1+1


⊕
· · · ⊕

t


0
0
Fεp
0
0
0
Fεp
0
0

+ G2εp+1


⊕
δ1 (Fk1 + tGk1) ⊕· · · ⊕δr (Fkr + tGkr)
⊕
η1 ((t + α1) Fℓ1 + Gℓ1) ⊕· · · ⊕ηq
 (t + αq) Fℓq + Gℓq

⊕
Z2m1(t, µ1, ν1) ⊕· · · ⊕Z2ms(t, µs, νs).
(15.2.2)

342
CHAPTER 15
Here, ε1 ≤· · · ≤εp and k1, . . . , kr are positive integers, αj’s and µj’s are real
numbers, ν1, . . . , νs are positive numbers, and δ1, . . . , δr, η1, . . . , ηq are signs, each
equal to +1 or −1.
The form (15.2.2) is uniquely determined by A + tB up to an arbitrary permu-
tation of the blocks in each of the three parts
Z2m1(t, µ1, ν1) ⊕· · · ⊕Z2ms(t, µs, νs),
η1 ((t + α1) Fℓ1 + Gℓ1) ⊕· · · ⊕ηq
 (t + αq) Fℓq + Gℓq

,
and
δ1 (Fk1 + tGk1) ⊕· · · ⊕δr (Fkr + tGkr) .
(b) Every matrix pencil A + tB, where A = AT ∈Rn×n, B = BT ∈Rn×n, is
R-strictly equivalent to a unique (up to a permutation of constituent blocks as in
Part (a)) real symmetric matrix pencil of the form (15.2.2), with all signs taken to
be +1.
The signs δ1, . . . , δr, η1, . . . , ηq form the sign characteristic of the real symmetric
pencil A + tB. Thus, the sign characteristic associates a sign ±1 to every partial
multiplicity of a real eigenvalue and of the eigenvalue at inﬁnity of A + tB.
The result of Theorem 15.2.1 has a long history. We refer the reader to Thomp-
son [151] and Lancaster and Rodman [92] for complete proofs and to Thompson
[151] for an extensive bibliography.
Next, we consider pairs of real skewsymmetric matrices.
Theorem 15.2.2. (i) Every matrix pencil A + tB, where A and B are real
skewsymmetric matrices, is R-strictly equivalent to a pencil of skewsymmetric ma-
trices of the form
0u×u
⊕
⊕p
j=1

t


0
0
Fεj
0
01
0
−Fεj
0
0

+


0
Fεj
0
−Fεj
0
0
0
0
0




⊕
⊕r
j=1

0
Fkj
−Fkj
0

+ t

0
Gkj
−Gkj
0

⊕
⊕q
j=1

(t + αj)

0
Fℓj
−Fℓj
0

+

0
Gℓj
−Gℓj
0

⊕
⊕s
j=1

0
Z2mj(t, µj, νj)
−Z2mj(t, µj, νj)
0

,
(15.2.3)
where the positive integers εj’s satisfy ε1 ≤· · · ≤εp, the numbers αj, µj, νj are all
real, and ν1, . . . , νs are positive.
The form (15.2.3) is uniquely determined by A+tB up to arbitrary permutations
of diagonal blocks in each of the three parts
⊕r
j=1

0
Fkj
−Fkj
0

+ t

0
Gkj
−Gkj
0

,
⊕q
j=1

(t + αj)

0
Fℓj
−Fℓj
0

+

0
Gℓj
−Gℓj
0

,

APPENDIX: REAL AND COMPLEX CANONICAL FORMS
343
and
⊕s
j=1

0
Z2mj(t, µj, νj)
−Z2mj(t, µj, νj)
0

.
(ii) Two matrix pencils A1 + tB1 and A2 + tB2 with skewsymmetric A1, B1, A2
and B2 are R-congruent if and only if they are R-strictly equivalent.
For a complete proof, as well as bibliography, we refer to Thompson [151] and
Lancaster and Rodman [93]. The particular form (15.2.3) is taken from Lancaster
and Rodman [93].
We observe that in the blocks

0
Z2mj(t, µj, νj)
−Z2mj(t, µj, νj)
0

in (15.2.3), νj may be replaced by −νj, the blocks F2mj may be replaced by −F2mj,
the blocks H2mj may be replaced by −H2mj, and the blocks
 F2mj−2
0
0
02

may be replaced by
 −F2mj−2
0
0
02

or by
 02
0
0
±F2mj−2

.
To make this statement explicit, set (we write m instead of mj for brevity)
W2m(x, y, z) = xF2m + yY2m + z

F2m−2
0
0
0

,
f
W2m(x, y, z) = xF2m + yH2m + z

0
0
0
F2m−2

(Y2m is given by (15.2.1)), where x, y, z are independent real variables, and introduce
the skewsymmetric matrix
Q2m = diag

0
1
−1
0

,

0
−1
1
0

, . . . , (−1)m+1

0
1
−1
0

= −QT
2m,
and the symmetric diagonal matrix
D4m =
"
eD2m
0
0
−eD2m
#
,
where
eD2m = diag (1, −1, . . . , 1, −1).
Then we have the following formulas:
D4m

0
W2m(x, y, z)
−W2m(x, y, z)
0

D4m
=

0
W2m(x, −y, z)
−W2m(x, −y, z)
0

;
(15.2.4)

344
CHAPTER 15
 F2m
0
0
F2m
 
0
W2m(x, y, z)
W2m(−x, −y, −z)
0
  F2m
0
0
F2m

=
"
0
f
W2m(x, −y, z)
f
W2m(−x, y, −z)
0
#
;
and
 Q2m
0
0
(−1)m+1Q2m
 
0
W2m(x, y, z)
W2m(−x, −y, −z)
0

·
 −Q2m
0
0
(−1)mQ2m

=

0
W2m(−x, −y, z)
W2m(x, y, −z)
0

.
(15.2.5)
Formulas (15.2.4) and (15.2.5) are taken from Lancaster and Rodman [93]. Other
variations of the canonical from of Theorem 15.2.2 are given in Lancaster and
Rodman [93] as well.
To state the canonical form of real matrix pencils A + tB, where one of the
matrices A, B is symmetric and the other is skewsymmetric, it will be convenient
to identify the primitive canonical real matrix pencils with this symmetry ﬁrst. It
will be assumed that A is symmetric and B is skewsymmetric (the situation when
A is skewsymmetric and B is symmetric can be dealt with by simply interchanging
the roles of A and B). The standard matrix
Ξk := Ξk(1) =


0
0
0
· · ·
0
0
1
0
0
0
· · ·
0
−1
0
0
0
0
· · ·
1
0
0
...
...
...
...
...
...
0
1
0
· · ·
0
0
0
(−1)k−1
0
0
· · ·
0
0
0


= (−1)k−1ΞT
k
(15.2.6)
will be used (cf. (1.2.6)). Thus, Ξk is symmetric if k is odd and skewsymmetric if
k is even.
The primitive canonical real matrix pencils of symmetric/skewsymmetric-type
are as follows; the preﬁx r-sy-sk stands for real, symmetric, skewsymmetric.
(r-sy-sk0)
a square-size zero matrix.
(r-sy-sk1)
 F2ε
0
0
01

+ t


0
0
Fε
0
01
0
−Fε
0
0

.
(r-sy-sk2)
Fk + t


01
0
0
0
0
F k−1
2
0
−F k−1
2
0

,
k odd.

APPENDIX: REAL AND COMPLEX CANONICAL FORMS
345
(r-sy-sk3)
Fk + t


01
0
0
0
0
0
0
F k−2
2
0
0
01
0
0
−F k−2
2
0
0

,
k even and k
2
even.
(r-sy-sk4)
Gℓ+ t
"
0
F ℓ
2
−F ℓ
2
0
#
,
ℓeven.
The matrix eGℓof (1.2.5) can be used in (r-sy-sk4) in place of Gℓ.
(r-sy-sk5)
"
0
G ℓ
2
G ℓ
2
0
#
+ t
"
0
F ℓ
2
−F ℓ
2
0
#
,
ℓeven and
ℓ
2 odd.
(r-sy-sk6)
"
0
αF ℓ
2 + G ℓ
2
αF ℓ
2 + G ℓ
2
0
#
+ t

0
Fℓ/2
−Fℓ/2
0

,
where ℓis even and α ∈R \ {0}.
The matrix eGℓ/2 can be used in (r-sy-sk5) and in (r-sy-sk6) in place of Gℓ/2.
(r-sy-sk7)


0
0
· · ·
0
0
νΞm+1
2
0
0
· · ·
0
−νΞm+1
2
−I2
0
0
· · ·
νΞm+1
2
−I2
0
...
...
...
...
...
...
(−1)m−1νΞm+1
2
−I2
0
· · ·
0
0


+ t


0
0
· · ·
0
Ξm
2
0
0
· · ·
−Ξm
2
0
...
...
...
...
...
0
(−1)m−2Ξm
2
· · ·
0
0
(−1)m−1Ξm
2
0
· · ·
0
0


,
ν > 0.
Here Ξ2 =

0
1
−1
0

. The pencil in (r-sy-sk7) is 2m × 2m, where m is a
positive integer. Note that the size 2m × 2m matrices


0
0
· · ·
0
0
νΞm+1
2
0
0
· · ·
0
−νΞm+1
2
−I2
0
0
· · ·
νΞm+1
2
−I2
0
...
...
...
...
...
...
(−1)m−1νΞm+1
2
−I2
0
· · ·
0
0



346
CHAPTER 15
and


0
0
· · ·
0
Ξm
2
0
0
· · ·
−Ξm
2
0
...
...
...
...
...
0
(−1)m−2Ξm
2
· · ·
0
0
(−1)m−1Ξm
2
0
· · ·
0
0


are symmetric and skewsymmetric, respectively, for every positive integer m
and every real ν.
(r-sy-sk8)

0
J2m(a ± ib)T
J2m(a ± ib)
0

+ t

0
I2m
−I2m
0

,
where a, b > 0. The matrix pencil here is 4m × 4m.
We remark that in (r-sy-sk8), any of the following seven matrices can be used
in place of J2m(a ± ib).
Of course, the transposed matrix should then be used
simultaneously for J2m(a ± ib)T :
J2m(a ± i(−b)),
J2m(−a ± ib),
J2m(−a ± i(−b)),
(J2m(a ± ib))T ,
(J2m(a ± i(−b)))T ,
(J2m(−a ± ib))T ,
(J2m(−a ± i(−b)))T .
To verify this, use: (1) the property that every regular real matrix pencil is R-strictly
equivalent to its transpose (see Corollary 7.3.6 and Theorem 7.6.3, or Corollary 3.4
in Lancaster and Rodman [92] for complex matrix pencils); and (2) the equalities
 X
0
0
(X−1)T
 
0
JT
J
0

+ t

0
I
−I
0
 
XT
0
0
X−1

=

0
J
JT
0

+ t

0
I
−I
0

,
where J is one of the matrices J2m(a ± i(−b)), J2m(−a ± ib), J2m(−a ± i(−b)) and
where the invertible real matrix X is such that XJT X−1 = J. The existence of X
can be established directly, and it also follows from the well-known fact that every
real or complex n×n matrix is similar to its transpose; see, e.g., Horn and Johnson
[62].
One easily veriﬁes that each of the pencils (r-sy-sk0)–(r-sy-sk8) is of mixed type
(symmetric/skewsymmetric).
Theorem 15.2.3. Let A+tB be a real symmetric/skewsymmetric matrix pencil.
(a) A + tB is R-strictly equivalent to a direct sum of blocks of types (r-sy-sk0)–
(r-sy-sk8). In this direct sum, several blocks of the same type and of diﬀerent sizes
may be present.
The direct sum is uniquely determined by A and B, up to an
arbitrary permutation of blocks.
(b) A+tB is R-congruent to a real symmetric/skewsymmetric pencil of the form
(A0 + tB0) ⊕⊕r
j=1δj


Fkj + t


01
0
0
0
0
F kj −1
2
0
−F kj −1
2
0





(15.2.7)

APPENDIX: REAL AND COMPLEX CANONICAL FORMS
347
⊕⊕p
w=1 ηw
 
Gℓw + t
"
0
F ℓw
2
−F ℓw
2
0
#!
⊕⊕q
u=1ζu
(15.2.8)









0
0
· · ·
0
0
νuΞmu+1
2
0
0
· · ·
0
−νuΞmu+1
2
−I2
0
0
· · ·
νuΞmu+1
2
−I2
0
...
...
...
...
...
...
(−1)mu−1νuΞmu+1
2
−I2
0
· · ·
0
0


+t


0
0
· · ·
0
Ξmu
2
0
0
· · ·
−Ξmu
2
0
...
...
...
...
...
0
(−1)mu−2Ξmu
2
· · ·
0
0
(−1)mu−1Ξmu
2
0
· · ·
0
0









.
(15.2.9)
Here, A0 + tB0 is a direct sum of blocks of types (r-sy-sk0), (r-sy-sk1), (r-sy-sk3),
(r-sy-sk5), (r-sy-sk6), and (r-sy-sk8), in which several blocks of the same type and
of diﬀerent and/or the same sizes may be present, and the kj’s are odd positive
integers, the ℓw’s are even positive integers, the νy’s are positive real numbers, and
δj, ηw, ζy are signs ±1.
The blocks in (15.2.7) and (15.2.9) are uniquely determined by A + tB up to
an arbitrary permutation of constituent matrix pencil blocks in A0 + λB0 and an
arbitrary permutation of blocks in each of the three parts
⊕r
j=1δj


Fkj + t


01
0
0
0
0
F kj −1
2
0
−F kj −1
2
0




,
(15.2.8), and (15.2.9).
The special properties of the Kronecker form for real symmetric/skewsymmetric
matrices have been known since the paper by Kronecker [82], and, later, by William-
son [161]; see also Mal’cev [105] for the case when the skewsymmetric matrix is
invertible. The result of part (b) was ﬁrst proved in Williamson [161]; more detailed
expositions are found in Thompson [151] and Lancaster and Rodman [93], where
complete proofs are given. The particular form (15.2.7), (15.2.8), (15.2.9) is taken
from Lancaster and Rodman [93].
The signs δj, ηw, ζy form the sign characteristic of the symmetric/skewsymmetric
real matrix pencil A + tB. It is easy to see that the block
Fkj + t


01
0
0
0
0
F kj −1
2
0
−F kj −1
2
0

,
kj odd,
is R-strictly equivalent to I + tJkj(0), and the block
Gℓw + t
"
0
F ℓw
2
−F ℓw
2
0
#
,
ℓw even,

348
CHAPTER 15
is R-strictly equivalent to tI + Jℓw(0). Moreover, a computation shows that


0
0
· · ·
0
0
νuΞmu+1
2
0
0
· · ·
0
−νuΞmu+1
2
−I2
0
0
· · ·
νuΞmu+1
2
−I2
0
...
...
...
...
...
...
(−1)mu−1νuΞmu+1
2
−I2
0
· · ·
0
0


·


0
0
· · ·
0
Ξmu
2
0
0
· · ·
−Ξmu
2
0
...
...
...
...
...
0
(−1)mu−2Ξmu
2
· · ·
0
0
(−1)mu−1Ξmu
2
0
· · ·
0
0


−1
=


νuΞ2
0
0
. . .
0
(−1)mu−1Ξmu
2
νuΞ2
0
. . .
0
0
(−1)mu−2Ξmu
2
νuΞ2
. . .
0
...
...
...
...
...
0
0
0
. . . (−1)Ξmu
2
νuΞ2


,
which is easily seen to have the real Jordan form J2mu(±νui).
Thus, the sign
characteristic of a symmetric/skewsymmetric real matrix pencil associates a sign
±1 to every odd partial multiplicity at inﬁnity, to every even partial multiplicity at
zero, and to every partial multiplicity corresponding to a pure imaginary eigenvalue
with positive (or negative) imaginary part.
15.3
COMPLEX MATRIX PENCILS WITH SYMMETRIES
In this section we present canonical forms of complex matrix pencils with various
symmetries. We start with the hermitian symmetry.
Theorem 15.3.1. (a) Every matrix pencil A + tB, where A = A∗∈Cn×n,
B = B∗∈Cn×n, is C-congruent to a complex hermitian matrix pencil of the form
0
⊕

t


0
0
Fε1
0
0
0
Fε1
0
0

+ G2ε1+1


⊕
· · · ⊕

t


0
0
Fεp
0
0
0
Fεp
0
0

+ G2εp+1


⊕
δ1 (Fk1 + tGk1) ⊕· · · ⊕δr (Fkr + tGkr)
⊕
η1 ((t + α1) Fℓ1 + Gℓ1) ⊕· · · ⊕ηq
 (t + αq) Fℓq + Gℓq

⊕

0
(t + β1)Fm1
(t + β1)Fm1
0

+

0
Gm1
Gm1
0

⊕
· · · ⊕

0
(t + βs)Fms
(t + βs)Fms
0

+

0
Gms
Gms
0

.(15.3.1)
Here, ε1 ≤· · · ≤εp and k1, . . . , kr are positive integers, αj are real numbers, βj are
complex nonreal numbers with positive imaginary parts, and δ1, . . . , δr, η1, . . . , ηq
are signs, each equal to +1 or −1.

APPENDIX: REAL AND COMPLEX CANONICAL FORMS
349
The form (15.3.1) is uniquely determined by A + tB up to an arbitrary permu-
tation of blocks in each of the three parts
⊕r
j=1
 δj
 Fkj + tGkj

,
⊕q
j=1
 ηj
 (t + αj) Fℓj + Gℓj

,
and
⊕s
j=1

0
(t + βj)Fmj
(t + βj)Fmj
0

+

0
Gmj
Gmj
0

.
(b) Every matrix pencil A + tB, where A = A∗∈Cn×n, B = B∗∈Cn×n,
is C-strictly equivalent to a unique (up to a permutation, as in Part (a)) complex
hermitian matrix pencil of the form (15.3.1), with all signs taken to be +1.
The signs δ1, . . . , δr, η1, . . . , ηq form the sign characteristic of the complex her-
mitian pencil A + tB. Thus, the sign characteristic associates a sign ±1 to every
index of a real eigenvalue and at inﬁnity of A + tB.
We will not consider separately complex skewhermitian matrices because one
can multiply a skewhermitian complex matrix by i to obtain a hermitian matrix
and then apply the results of Theorem 15.3.1.
We study next complex matrix pencils with symmetries with respect to trans-
position. Recall that two complex matrix pencils A + tB and A′ + tB′ are (C,T )-
congruent if A + tB = ST (A′ + tB′)S for some invertible complex matrix S. It
turns out that for complex pencils with symmetries respecting transposition, (C,T )-
congruence is the same as C-strict equivalence:
Theorem 15.3.2. Let A1, B1, A2, B2 ∈Cn×n, and assume that AT
j = η1Aj,
BT
j = η2Bj for j = 1, 2, where η1, η2 are signs ±1. Then the pencils A1 + tB1 and
A2 + tB2 are (C,T )-congruent if and only if they are C-strictly equivalent.
The detailed proof of this theorem, as well as that of Theorems 15.3.3, 15.3.5,
and 15.3.6 below, is found in Thompson [151].
Theorem 15.3.3. Every matrix pencil A + tB, where A and B are complex
symmetric matrices, is (C,T )-congruent to a pencil of symmetric matrices of the
form
0u×u
⊕
⊕s
i=1

0
LT
εi×(εi+1)
Lεi×(εi+1)
0

⊕⊕r
j=1
 Fkj + tGkj

⊕
⊕q
j=1
 (t + αj) Fℓj + Gℓj

,
(15.3.2)
where ε1 ≤· · · ≤εs and k1 ≤· · · ≤kr are positive integers, and α1, . . . , αq ∈C.
The form (15.3.2) is uniquely determined by A + tB up to arbitrary permutation of
blocks in the part
⊕q
j=1
 (t + αj) Fℓj + Gℓj

.
As a corollary we obtain the following.
Corollary 15.3.4. A pencil of complex n × n matrices is C-strictly equivalent
to a pencil of complex symmetric matrices if and only if its left indices, arranged in
nondecreasing order, coincide with its right indices, also arranged in nondecreasing
order.

350
CHAPTER 15
For complex matrices rather than pencils, this result is found in Gantmacher
[48].
Theorem 15.3.5. Every matrix pencil A + tB, where A and B are complex
skewsymmetric matrices, is (C,T )-congruent to a pencil of skewsymmetric matrices
of the form
0u
⊕
⊕p
j=1

t


0
0
Fεj
0
01
0
−Fεj
0
0

+


0
Fεj
0
−Fεj
0
0
0
0
0




⊕
⊕r
j=1

0
Fkj
−Fkj
0

+ t

0
Gkj
−Gkj
0

⊕
⊕q
j=1

(t + αj)

0
Fℓj
−Fℓj
0

+

0
Gℓj
−Gℓj
0

,
(15.3.3)
where the positive integers εj’s satisfy ε1 ≤· · · ≤εp and α1, . . . , αq ∈C.
The form (15.3.3) is uniquely determined by A+tB up to arbitrary permutations
of diagonal blocks in each of the two parts
⊕r
j=1

0
Fkj
−Fkj
0

+ t

0
Gkj
−Gkj
0

and
⊕q
j=1

(t + αj)

0
Fℓj
−Fℓj
0

+

0
Gℓj
−Gℓj
0

.
One can replace Gk by eGk in Theorems 15.3.3 and 15.3.5. Note the general fact
that if a ﬁeld is algebraically closed with characteristic not equal to two, then two
pencils of skewsymmetric matrices are congruent (with respect to transposition)
over the ﬁeld if and only if the pencils are strictly equivalent over the same ﬁeld;
see, e.g., MacDuﬀee [104], Mal’cev [105] (for the case when one matrix is invertible),
Gantmacher [48], or Thompson [151].
To deal with the case of mixed pencils, where one matrix is symmetric and
the other is skewsymmetric, we indicate the primitive forms ﬁrst, with the preﬁx
c-sy-sk (complex symmetric/skewsymmetric). The forms (c-sy-sk0), (c-sy-sk1), (c-
sy-sk2), (c-sy-sk3), (c-sy-sk4), and (c-sy-sk5) are the same as (r-sy-sk0), (r-sy-sk1),
(r-sy-sk2), (r-sy-sk3), (r-sy-sk4), and (r-sy-sk5), respectively. The form (c-sy-sk6)
is
"
0
αF ℓ
2 + G ℓ
2
αF ℓ
2 + G ℓ
2
0
#
+ t
"
0
F ℓ
2
−F ℓ
2
0
#
,
where ℓis even and α ∈C \ {0}. The matrices eGm may be used in place of Gm in
(c-sy-sk4), (c-sy-sk5), and (c-sy-sk6). Note that the forms (c-sy-sk0)–(c-sy-sk6) are
indeed complex symmetric/skewsymmetric pencils.
Theorem 15.3.6. Let A + tB be a complex symmetric/skewsymmetric matrix
pencil: A = AT , B = −BT . Then A + tB is (C,T )-congruent to a direct sum of
blocks of types (c-sy-sk0)–(c-sy-sk6). In this direct sum, several blocks of the same
type and of diﬀerent sizes may be present. The direct sum is uniquely determined
by A and B, up to an arbitrary permutation of blocks.
The following meta-corollary will be useful.

APPENDIX: REAL AND COMPLEX CANONICAL FORMS
351
Corollary 15.3.7. Let F = R or F = C. Fix signs ξ = ±1, η = ±1, and ﬁx an
involutory transformation on matrices X 7→X⋆, where X⋆= XT or X⋆= X∗is
independent of the matrix X (in the real case XT = X∗). Let there be given two
matrix pencils A + tB and A′ + tB′, where A, B, A′, B′ ∈Fn×n and
A = ξA⋆,
B = ηB⋆,
A′ = ξ(A′)⋆,
B′ = η(B′)⋆.
If for some positive integer s, (A + tB)⊕s is (F, ⋆)-congruent to (A′ + tB′)⊕s, then
A + tB is (F, ⋆)-congruent to A′ + tB′
The proof of the meta-corollary follows from the uniqueness of the respective
canonical form for matrix pencils with symmetries, analogously to the proof of the
case F = C in Corollary 15.1.4. We omit details.

This page intentionally left blank 

Bibliography
[1]
H. Abou-Kandil, G. Freiling, V. Ionescu, and G. Jank. Matrix Riccati Equations in
Control and Systems Theory, Birkh¨auser Verlag, Basel, 2003.
[2]
D. Alpay, J. A. Ball, I. Gohberg, and L. Rodman. Realization and factorization for
rational matrix functions with symmetries. Operator Theory: Advances and Appl.
47, 1–60, Birkh¨auser, Basel, 1990.
[3]
D. Alpay and H. Dym. Structured invariant spaces of vector valued rational func-
tions, Hermitian matrices, and a generalization of the Iohvidov laws, Linear Algebra
and Appl. 137/138 (1990), 137–181.
[4]
D. Alpay, A. C. M. Ran, and L. Rodman. Basic classes of matrices with respect to
quaternionic indeﬁnite inner product spaces, Linear Algebra and Appl. 416 (2006),
242–269.
[5]
E. Artin. Geometric Algebra, Princeton University Press, Princeton, NJ, 1957.
[6]
M. Artin. Algebra, Prentice Hall, Englewood Cliﬀs, NJ, 1991.
[7]
Y.-H. Au-Yeung. A theorem on a mapping from a sphere to the circle and the
simultaneous diagonalization of two hermitian matrices, Proc. Amer. Math. Soc. 20
(1969), 545–548.
[8]
———. A simple proof of convexity of the ﬁeld of values deﬁned by two hermitian
forms, Aequationes Math 12 (1975), 82–83.
[9]
———. On the convexity of the numerical range in quaternionic Hilbert space,
Linear and Multilinear Algebra 16 (1984), 93–100.
[10]
Y.-H. Au-Yeung and Y. T. Poon. A remark on the convexity and positive deﬁniteness
concerning Hermitian matrices, Southeast Asian Bull. Math. 3 (1979), 85–92.
[11]
Y. H. Au-Yeung and N. K. Tsing. An extension of the Hausdorﬀ-Toeplitz theorem
on the numerical range, Proc. Amer. Math. Soc. 89 (1983), 215–218.
[12]
———. Some theorems on the generalized numerical ranges, Linear and Multilinear
Algebra 15 (1984), 3–11.
[13]
T. Ya. Azizov and I. S. Iohvidov. Linear Operators in Spaces with an Indeﬁnite
Metric, John Wiley, Chichester, 1989 (translation from Russian).
[14]
G. Berhuy and F. Oggier. An Introduction to Central Simple Algebras and Their
Applications to Wireless Communication, Amer. Math. Soc., Providence, RI, 2013.
[15]
V. Bolotnikov. Private communication.
[16]
Y. Bolshakov, C. V. M. van der Mee, A. C. M. Ran, B. Reichstein, and L. Rodman.
Extensions of isometries in ﬁnite dimensional indeﬁnite scalar product spaces and
polar decompositions, SIAM J. of Matrix Analysis and Appl. 18 (1997), 752–774.
[17]
K. C. Border. Fixed Point Theorems with Applications to Economics and Game
Theory, Cambridge University Press, Cambridge, 1985.

354
BIBLIOGRAPHY
[18]
J. L. Brenner. Matrices of quaternions. Paciﬁc J. Math. 1 (1951), 329–335.
[19]
L. Brickman. On the ﬁeld of values of a matrix, Proc. Amer. Math. Soc. 12 (1961),
61–66.
[20]
E. Brieskorn. Lineare Algebra und Analytische Geometrie, Volume 1 and 2, Vieweg
Verlag, Braunschweig, 1985 and 1993.
[21]
A. M. Bruckner, J. B. Bruckner, and B. S. Thomson. Real Analysis, Prentice Hall,
Upper Saddle River, NJ, 1997.
[22]
A. Bunse-Gerstner, R. Byers, and V. Mehrmann. A quaternion QR algorithm, Nu-
mer. Math. 55 (1989), 83–95.
[23]
N. Burgoyne and R. Cushman. Normal forms for real linear Hamiltonian systems,
Lie Groups: History Frontiers and Applications VII (1977), 483–529, Math Sci.
Press, Brookline, MA.
[24]
L. X. Chen. Inverse matrix and properties of double determinant over quaternion
ﬁeld, Sci. China Ser. A 34 (1991), 528–540.
[25]
———. Deﬁnition of determinant and Cramer solutions over the quaternion ﬁeld,
Acta Mathematics Sinica, New Series 7 (1991), 171–180.
[26]
W.-S. Cheung, C.-K. Li, and L. Rodman. Operators with numerical range in a closed
subspace, Taiwanese J. of Math. 11 (2007), 471–481.
[27]
R. V. Churchill and J. W. Brown. Complex Variables and Applications, 5th ed.,
McGraw-Hill, New York, 1990.
[28]
N. Cohen and S. De Leo. The quaternionic determinant, Electronic J. of Linear
Algebra, 7 (2000), 100–111.
[29]
P. M. Cohn. Skew Fields: Theory of General Division Rings, Cambridge University
Press, New York, 1995.
[30]
J. H. Conway and D. A. Smith. On Quaternions and Octonions: Their Geometry,
Arithmetic, and Symmetry, A. K. Peters, Natick, MA, 2003.
[31]
A. N. ˇCurilov. On the solutions of quadratic matrix equations, Non-linear Vibrations
and Control Theory (Udmurt State University, Izhevsk) 2 (1978), 24–33 (Russian).
[32]
J. Dieudonn´e. Les determinants sur un corp non-commutatif, Bull. Soc. Math.
France 71 (1943), 27–45.
[33]
D. Z. Djokovi´c. Classiﬁcation of pairs consisting of a linear and a semilinear map,
Linear and Multilinear Algebra 20 (1978), 147–165.
[34]
D. Z. Djokovi´c, J. Patera, P. Winternitz, and H. Zassenhaus. Normal forms of el-
ements of classical real and complex Lie and Jordan algebras, J. Math. Phys. 24
(1983), 1363–1374.
[35]
C. H. Edwards, Jr. Advanced Calculus of Several Variables. Academic Press, New
York and London, 1973.
[36]
S. Eilenberg and I. Niven. The ”fundamental theorem of algebra” for quaternions,
Bull. Amer. Math. Soc. 50 (1944), 246–248.
[37]
B. Farb and R. K. Dennis. Noncommutative Algebra, Springer Verlag, New York,
1993.
[38]
D. R. Farenick and B. A. F. Pidkowich. The spectral theorem in quaternions, Linear
Algebra and Appl. 371 (2003), 75–102.

BIBLIOGRAPHY
355
[39]
H. Faßbender and Kh. Ikramov. Several observations on symplectic, Hamiltonian,
and skew-Hamiltonian matrices, Linear Algebra and Appl. 400 (2005), 15–29.
[40]
H. Faßbender, D. S. Mackey, and N. Mackey. Hamilton and Jacobi come full circle:
Jacobi algorithms for structured eigenproblems, Linear Algebra and Appl. 332–334
(2001), 37–80.
[41]
H. Faßbender, D. S. Mackey, N. Mackey, and H. Xu. Hamiltonian square roots of
skew-Hamiltonian matrices, Linear Algebra and Appl. 287 (1999), 125–159.
[42]
D. Finkbeiner. Introduction to Matrices and Linear Transformations, W. H. Free-
man, San Francisco, 1978.
[43]
P. Finsler. ¨Uber das Vorkommen deﬁniter und semideﬁniter Formen in Scharen
quadratischer Formen, Comm. Math. Helv. 9 (1937), 188–192.
[44]
W. Fleming. Functions of Several Variables, Springer Verlag, New York, 1977.
[45]
J. N. Franklin. Matrix Theory, Prentice Hall, Englewood Cliﬀs, NJ, 1968.
[46]
G. Freiling, V. Mehrmann, and H. Xu. Existence, uniqueness, and parametrization
of Lagrangian invariant subspaces, SIAM J. of Matrix Analysis and Appl. 23 (2002),
1045–1069.
[47]
P. A. Fuhrmann. On Hamiltonian rational transfer functions, Linear Algebra and
Appl. 63 (1984), 1–93.
[48]
F. R. Gantmacher. The Theory of Matrices, Vols. 1 and 2, Chelsea, New York, 1959
(translation from Russian).
[49]
———. Applications of the Theory of Matrices, Interscience Publishers, New York,
1959 (translation of part II of the Russian original).
[50]
———. The Theory of Matrices, 3rd. ed., Nauka, Moscow, 1967. (Russian.)
[51]
F. R. Gantmacher and M. G. Krein. Oscillation Matrices and Kernels and Small
Vibrations of Mechanical Systems, rev. ed., AMS Chelsea Publishing, Providence,
RI, 2002 (translation based on the 1941 original).
[52]
I. Gohberg, P. Lancaster, and L. Rodman. Matrices and Indeﬁnite Scalar Products.,
Operator Theory: Advances and Appl. 8, Birkh¨auser, Basel and Boston, 1983.
[53]
———. Indeﬁnite Linear Algebra and Applications, Birkh¨auser, Boston, 2006.
[54]
———. Invariant Subspaces of Matrices with Applications, John Wiley, New York,
1986; republication SIAM, Philadelphia, 2006.
[55]
———. Matrix Polynomials, Academic Press, New York, London, 1982; republica-
tion SIAM, Philadelphia, 2009.
[56]
G. H. Golub and C. F. Van Loan. Matrix Computations, 2nd. ed., The Johns Hopkins
University Press, Baltimore and London, 1989.
[57]
B. Gordon and T. S. Motzkin. On the zeros of polynomials over division rings, Trans.
Amer. Math. Soc. 116 (1965), 218–226.
[58]
R. M. Guralnick and L. S. Levy. Presentations of modules when ideals need not be
principal, Illinois J. Math. 32 (1988), 593–653.
[59]
R. M. Guralnick, L. S. Levy, and C. Odenthal. Elementary divisor theorem for
noncommutative PIDs, Proc. Amer. Math. Soc. 103 (1988), 1003–1011.
[60]
K. G¨urlebeck and W. Spr¨ossig. Quaternionic and Cliﬀord Calculus for Physicists
and Engineers, John Wiley, Chichester, 1997.

356
BIBLIOGRAPHY
[61]
I. N. Herstein. Abstract Algebra, 2nd ed., McMillan, New York, 1990.
[62]
R. A. Horn and C. R. Johnson. Matrix Analysis, Cambridge University Press, Cam-
bridge, 1985.
[63]
———. Topics in Matrix Analysis, Cambridge University Press, Cambridge, 1991.
[64]
R. A. Horn and V. V. Sergeichuk. Canonical matrices of bilinear and sesquilinear
forms, Linear Algebra and Appl. 428 (2008), 193–223.
[65]
———. Canonical forms for complex matrix congruence and *congruence, Linear
Algebra and Appl. 416 (2006) 1010–1032.
[66]
———. Congruences of a square matrix and its transpose, Linear Algebra and Appl.
389 (2004), 347–353.
[67]
L. K. Hua. On the theory of automorphic functions of a matrix variable, II. The
classiﬁcation of hypercircles under the symplectic group, Amer. J. Math. 66 (1944),
531–563.
[68]
L. Huang and W. So. Quadratic formulas for quaternions, Appl. Math. Letters 15
(2002), 533–540.
[69]
T. W. Hungerford. Algebra, Springer Verlag, New York. 1974.
[70]
I. S. Iohvidov, M. G. Krein, and H. Langer. Introduction to the Spectral Theory
of Operators in Spaces with an Indeﬁnite Metric, Mathematical Research, vol. 9,
Akademie-Verlag, Berlin, 1982.
[71]
N. Jacobson. The Theory of Rings. American Mathematical Society, RI, 1943.
[72]
D. Janovska and G. Opfer. A note on the computation of all zeros of simple quater-
nionic polynomial, SIAM J. Numerical Analysis 48 (2010), 244–256.
[73]
———. The algebraic Riccati equations for quaternions, Advances in Applied Clif-
ford Algebras, to appear.
[74]
R. E. Johnson. On the equation χα = γχ + β over an algebraic division ring, Bull.
Amer. Math. Soc. 50 (1944), 202–207.
[75]
M. A. Kaashoek, C. V. M. van der Mee, and L. Rodman. Analytic operator functions
with compact spectrum, II. Spectra pairs and factorization, Integral Equations and
Operator Theory 5 (1982), 791–827.
[76]
R. E. Kalman. Contributions to the theory of optimal control, Boletin Sociedad
Matematica Mexicana 5 (1960), 102–119.
[77]
W. Kaplan. Advanced Calculus, 3rd ed., Addison - Wesley, Reading, MA, 1984.
[78]
M. Karow. Self-adjoint operators and pairs of Hermitian forms over the quaternions,
Linear Algebra and Appl. 299 (1999), 101–117.
[79]
———. Note on the equation ax−xb over the quaternions, unpublished manuscript.
[80]
———. Private communication.
[81]
M. Koecher and R. Remmert. Hamiltons’ quaternions, in Numbers (J. Ewing, ed.),
pp. 189–220, Springer Verlag, New York, 1991 (English translation).
[82]
L. Kronecker. Collected Works, Chelsea, 1968.
[83]
———. ¨Uber die congruenten Transformationen der bilinearer Formen, Monats. der
Akademie der Wissenschaften, Berlin (1874), 397–447.

BIBLIOGRAPHY
357
[84]
J. B. Kuipers. Quaternions and Rotation Sequences, Princeton University Press,
Princeton, 2002.
[85]
P. Lancaster, A. S. Markus, and Q. Ye. Low rank perturbations of strongly deﬁniti-
zable transformations and matrix polynomials, Linear Algebra and Appl. 197/198
(1994), 3–29.
[86]
P. Lancaster, A. S. Markus, and F. Zhou. A wider class of stable gyroscopic systems.
Linear Algebra Appl. 370 (2003), 257–267.
[87]
P. Lancaster, A. S. Markus, and P. Zizler. The order of neutrality for linear operators
on inner product spaces, Linear Algebra and Appl. 259 (1997), 25–29.
[88]
P. Lancaster and L. Rodman. Existence and uniqueness theorems for the algebraic
Riccati equations, International J. of Control 32 (1980), 285–309.
[89]
———. Invariant neutral subspaces for symmetric and skew real matrix pairs, Cana-
dian J. Math. 46 (1994), 602–618.
[90]
———. Minimal symmetric factorizations of symmetric real and complex rational
matrix functions, Linear Algebra and Appl. 220 (1995), 249–282.
[91]
———. Algebraic Riccati Equations, Oxford University Press, New York, 1995.
[92]
———. Canonical forms for hermitian matrix pairs under strict equivalence and
congruence, SIAM Review 47 (2005), 407–443.
[93]
———. Canonical forms for symmetric/skew-symmetric real matrix pairs under
strict equivalence and congruence, Linear Algebra and Appl. 406 (2005), 1–76.
[94]
P. Lancaster and M. Tismenetsky. The Theory of Matrices, 2nd ed., Academic Press,
Orlando, 1985.
[95]
P. Lancaster and Q. Ye. Variational properties and Rayleigh quotient algorithms
for symmetric matrix pencils, Operator Theory: Advances and Appl. 40 (1989),
247–278.
[96]
S. R. Lay. Convex Sets and Their Applications, Dover, Mineola, NY, 2007.
[97]
N. Le Bihan and J. Mars. Singular value decomposition for quaternion matrices: A
new tool for vector-sensor signal processing, Signal Processing 84 (2004), 1177–1199.
[98]
H. C. Lee. Eigenvalues of canonical forms of matrices with quaternion coeﬃcients,
Proc. Royal Irish Acad. 52 Sect. A (1949), 253–260.
[99]
J. M. Lee and D. A. Weinberg. A note on canonical form for matrix congruence.
Linear Algebra and Appl. 249 (1996), 207–215.
[100] L. S. Levy and J. C. Robson. Matrices and pairs of modules, J. of Agebra 29 (1974),
427–454.
[101] W. W. Lin, V. Mehrmann, and H. Xu. Canonical forms for Hamiltonian and sym-
plectic matrices and pencils, Linear Algebra and Appl. 302/303 (1999), 469–533.
[102] T. A. Loring. Factorization of matrices of quaternions, Expositiones Mathematicae,
30 (2012), 250–264.
[103] P. Lounesto. Cliﬀord Algebras and Spinors, 2nd ed., Cambridge University Press,
Cambridge, 2001.
[104] C. C. MacDuﬀee. The Theory of Matrices, Berlin, 1933; Chelsea, New York, 1946.
[105] A. I. Mal’cev. Foundations of Linear Algebra, W. H. Freeman, San Francisco and
London, 1963 (translation from Russian).

358
BIBLIOGRAPHY
[106] C. V. M. van der Mee, A. C. M. Ran, and L. Rodman. Classes of plus-matrices in
ﬁnite dimensional indeﬁnite scalar product spaces, Integral Equations and Operator
Theory 30 (1998), 432–451.
[107] C. Mehl, V. Mehrmann, A. C. M. Ran, and L. Rodman. Perturbation analysis
of Lagrangian invariant subspaces of symplectic matrices, Linear and Multilinear
Algebra 57 (2009), 141–184.
[108] ———. Eigenvalue perturbation theory of classes of structured matrices under
generic sutructured rank one perturbations, Linear Algebra and Appl. 435 (2011),
687–716.
[109] C. Mehl, V. Mehrmann, and H. Xu. Canonical forms for double structured matrices
and pencils, Electronic J. of Linear Algebra 7 (2000), 112–151.
[110] C. Mehl, A. C. M. Ran, and L. Rodman. Semideﬁnite invariant subspaces: de-
generate inner products, Operator Theory: Advances and Appl., Current trends in
operator theory and its applications, 149 (2004), 467–486.
[111] C. Mehl and L. Rodman. Symmetric matrices with respect to sesquilinear forms,
Linear Algebra and Appl. 349 (2002), 55–75.
[112] V. L. Mehrmann. The Autonomous Linear Quadratic Control Problem, Lecture
Notes in Control and Information Sciences 163, Springer-Verlag, Berlin, 1991.
[113] V. Mehrmann and H. Xu. Structured Jordan canonical forms for structured matrices
that are Hermitian, skew-Hermitian, or unitary with respect to an indeﬁnite inner
product, Electronic J. of Linear Algebra 5 (1999), 67–103.
[114] C. D. Meyer. Matrix Analysis and Applied Linear Algebra, SIAM, Philadelphia,
2000.
[115] Y. Nakayama. A note on the elementary divisor theory in non-commutative domains,
Bull. Amer. Math. Soc. 44 (1938), 719–723.
[116] R. Pereira. Quaternionic Polynomials and Behavioral Systems, Doctoral Thesis, Uni-
versidade de Aviero, 2001.
[117] R. Pereira, P. Rocha, and P. Vettori. Algebraic tools for the study of quaternionic
behavioral systems, Linear Algebra and Appl. 400 (2005), 121–140.
[118] R. Pereira and P. Vettori. Stability of quaternionic linear systems, IEEE Trans. on
Automatic Control 31 (2006), 518–523.
[119] A. Pogorui and M. Shapiro. On the structure of the set of zeros of quaternionic
polynomials, Complex Var. Elliptic Funct. 49 (2004), 379–389.
[120] Y. T. Poon. Generalized numerical ranges, joint positive deﬁniteness and multiple
eigenvalues, Proc. of Amer. Math. Soc. 125 (1997), 1625–1634.
[121] R. M. Porter. Quaternionic linear and quadratic equations, J. Natur. Geom. 11
(1997), 101–106.
[122] A. C. M. Ran. Minimal factorizations of selfadjoint rational matrix functions, Inte-
gral Equations and Operator Theory, 6 (1982), 850–869.
[123] A. C. M. Ran and L. Rodman. Stability of invariant maximal semideﬁnite subspaces,
I. Linear Algebra and Appl. 62 (1984), 51–86.
[124] ———. Stability of invariant maximal semideﬁnite subspaces, II. Applications: self-
adjoint rational matrix functions, algebraic Riccati equations, Linear Algebra and
Appl. 63 (1984), 133–173.

BIBLIOGRAPHY
359
[125] ———. Stability of invariant Lagrangian subspaces. I. Operator Theory: Advances
and Appl. 32 (1988), 181–218, Birkh¨auser, Basel.
[126] ———. Stable invariant Lagrangian subspaces: factorization of symmetric rational
matrix functions and other applications. Linear Algebra and Appl. 137/138 (1990),
575–620.
[127] R. von Randow. The involutory antiautomorphisms of the quaternion algebra, Amer-
ican Mathematical Monthly, 74 (1967), 699–700.
[128] R. T. Rockafellar. Convex Analysis,
Princeton University Press, Princeton, NJ,
1970.
[129] L. Rodman. Non-Hermitian solutions of algebraic Riccati equations. Canadian J.
Math. 49 (1997), 840–854.
[130] ———. Invariant subspaces of selfadjoint matrices in skew symmetric inner prod-
ucts, SIAM J. on Matrix Analysis and Appl. 26 (2005), 901–907.
[131] ———. Similarity vs unitary similarity and perturbation analysis of sign character-
istics: Complex and real indeﬁnite inner products, Linear Algebra and Appl. 416
(2006), 945–1009.
[132] ———. Canonical forms for symmetric and skew-symmetric quaternionic matrix
pencils, Operator Theory: Advances and Appl. 176 (2007), 199–254.
[133] ———. Canonical forms for mixed symmetric-skewsymmetric quaternion matrix
pencils, Linear Algebra and Appl. 424 (2007), 184–221.
[134] ———. Comparison of congruences and strict equivalences for real, complex, and
quaternionic matrix pencils with symmetries, Electronic J. of Linear Algebra 16
(2007), 248–283.
[135] ———. Pairs of hermitian and skew-hermitian quaternionic matrices: canonical
forms and their applications, Linear Algebra and Appl. 429 (2008), 981–1019.
[136] ———. Pairs of quaternionic selfadjoint operators with numerical range in a half-
plane, Linear and Multilinear Algebra 56 (2008), 179–184.
[137] ———. Stability of invariant subspaces of quaternion matrices, Complex Analysis
and Operator Theory 6 (2012), 1069–1119.
[138] ———. Strong stability of invariant subspaces of quaternion matrices, Operator
Theory: Advances and Appl. 237 (2013).
[139] ———. Hamiltonian square roots of skew Hamiltonian quaternionic matrices, Elec-
tronic J. of Linear Algebra 17 (2008), 168–191.
[140] ———. Invariant neutral subspaces for Hamiltonian matrices, Electronic J. of Linear
Algebra 27 (2014), 55–99.
[141] V. V. Sergeichuk. Classiﬁcation problems for systems of forms and linear mappings,
Math. USSR-Izv., 31 (1988), 481–501 (translation from Russian).
[142] ———. Classiﬁcation of sesquilinear forms, pairs of Hermitian forms, self-conjugate
and isometric operators over the division ring of quaternions, Math. Notes 49 (1991),
409–414 (translation from Russian).
[143] ———. Canonical matrices of isometric operators on indeﬁnite inner product spaces,
Linear Algebra and Appl. 428 (2008), 154–192.
[144] M. A. Shayman. On the variety of invariant subspaces of a ﬁnite-dimensional linear
operator, Trans.Amer. Math. Soc 274 (1982), 721–747.

360
BIBLIOGRAPHY
[145] G. E. Shilov. An Introduction to the Theory of Linear Spaces, Prentice Hall, Engle-
wood Cliﬀs, 1961 (translation from Russian).
[146] L.-s. Siu. A Study of Polynomials, Determinants, Eigenvalues and Numerical Ranges
over Real Quaternions, M. Ph. Thesis, University of Hong Kong, 1997.
[147] L. Smith. Linear Algebra, Springer Verlag. New York, 1984.
[148] W. So and R. C. Thompson. Convexity of the upper complex plane part of the
numerical range of a quaternionic matrix, Linear and Multilinear Algebra 41 (1996),
303–365.
[149] G. W. Stewart and J.-g. Sun. Matrix Perturbation Theory, Academic Press, Boston,
1990.
[150] E. Study. Zur Theorie der Linearen Gleichungen, Acta Math. 42 (1920), 1–61.
[151] R. C. Thompson. Pencils of complex and real symmetric and skew matrices, Linear
Algebra and Appl. 147 (1991), 323–371.
[152] F. Uhlig. A recurring theorem about pairs of quadratic forms and extensions: a
survey, Linear Algebra and Appl. 25 (1979), 219–237.
[153] J. Vince. Quaternions for Computer Graphics, Springer, London, 2011.
[154] W. R. Wade. An Introduction to Analysis, Prentice Hall, Upper Saddle River, NJ,
1995.
[155] C. T. C. Wall. Stability, pencils, and polytopes, Bull. London Math. Soc. 12 (1980),
401–421.
[156] Z.-X. Wan. Geometry of Matrices, World Scientiﬁc Publishing, River Edge, NJ,
1996.
[157] J. P. Ward. Quaternions and Cayley Numbers, Kluwer Academic Publishers, Dor-
drecht, 1997.
[158] R. Webster. Convexity, Oxford University Press, Oxford, New York Tokyo, 1994.
[159] B. Wie, H. Weiss, and A. Arapostathis. Quarternion feedback regulator for space-
craft eigenaxis rotations, Journal of Guidance, Control, and Dynamics 12 (1989),
375–380.
[160] N. A. Wiegmann. Some theorems on matrices with real quaternion entries, Canadian
J. of Math. 7 (1955), 191–201.
[161] J. Williamson. On the algebraic problem concerning the normal forms of linear
dynamical systems, Amer. J. of Math. 58 (1936), 141–163.
[162] L. A. Wolﬀ. Similarity of matrices in which the elements are real quaternions, Bull.
Amer. Math. Soc. 42 (1936), 737–743.
[163] R. M. W. Wood. Quaternionic eigenvalues. Bull. London Math. Soc. 17 (1985),
137–138.
[164] F. Zhang. Quaternions and matrices of quaternions, Linear Algebra and Appl. 251
(1997), 21–57.
[165] P. Zizler. Deﬁnitizable operators on a Krein space, Bull. Canadian Math.Soc. 38
(1995), 496–506.

Index
β(φ)-inertia, 65
β(φ)-signature, 65
φ-numerical cone, 274
φ-numerical range, 274
joint, 51
algebraic Riccati equation, 332
antiendomorphism, 17
bilateral equation, 331
centralizer, 16
Cholesky decomposition, 32
controllable, 333
cross product, 10
determinant, 113, 115
diﬀerential equations
backward bounded, 128, 239
bounded, 239
forward bounded, 124, 239
forward stable, 124
polynomially bounded, 259
stably bounded, 239, 255
eigenvalue
C-eigenvalue, 86
c-set, 249
left, 90
right, 90
eigenvector
left, 90
right, 90
endomorphism, 17
function
continuous, 60
gap, 56
generalized eigenvectors, 106
graph, 331
Hausdorﬀdistance, 123
inertia, 67
balanced, 66
theorem, 67
inner product, 14, 44
involution, 17
nonstandard, 19
Jordan form, 98
complex, 109
real, 109
Kronecker form, 161
regular part, 162
singular part, 162
left kernel, 162
linear combination
positive deﬁnite, 181
positive semideﬁnite, 181
matrix
(H, φ)-Hamiltonian, 132, 306
(H, φ)-orthogonal, 132
(H, φ)-plus, 147
(H, φ)-skew-Hamiltonian, 132, 306
(H, φ)-skewsymmetric, 132
(H, φ)-symmetric, 132
(H, φ)-symplectic, 132
(H,∗)-Hamiltonian, 132, 242
(H,∗)-hermitian, 132
(H,∗)-skew-Hamiltonian, 132, 242
(H,∗)-skewhermitian, 132
(H,∗)-unitary, 132
H-contractive, 150
H-dissipative, 144, 191
H-expansive, 144, 188, 191
H-isometric, 190
H-plus, 137, 190
β(φ)-negative deﬁnite, 65
β(φ)-negative semideﬁnite, 65
β(φ)-positive deﬁnite, 65
β(φ)-positive semideﬁnite, 65

362
INDEX
φ-congruent, 64, 291
φ-hermitian, 44
φ-normal, 44
φ-skewhermitian, 44
φ-unitary, 44
adjoint, 30
block companion, 328
companion, 119, 124
congruent, 64
diagonal, 92
hermitian, 30
invertible, 30
negative deﬁnite, 67
negative semideﬁnite, 67
nilpotent, 207
normal, 30
orthogonal, 12
positive deﬁnite, 30, 67
positive semideﬁnite, 30, 67
rank, 31
row rank, 61
similar, 97
skewhermitian, 30
strict (H, φ)-plus, 147
strict H-plus, 137
strictly H-contractive, 150
strictly H-expansive, 151
Toeplitz, 95
transposed, 44
unitary, 30
matrix pencil, 161
(C,T )-congruent, 292, 349
C-strictly equivalent, 169
F-congruent, 172
F-strictly equivalent, 339
H-strictly equivalent, 169
R-strictly equivalent, 169
φ-congruent, 261
φ-hermitian, 261
φ-hermitian/skewhermitian, 279
φ-hsk, 279
φ-skewhermitian, 263
β-part, 266
γ-part, 266
arranged form, 267
congruent, 172
eigenvalue, 161
hermitian, 172
sign characteristic, 174
hermitian/skewhermitian, 199
indices
at eigenvalue, 162
at inﬁnity, 161
left, 161
right, 161
partial multiplicities
at eigenvalue, 162
at inﬁnity, 161
strictly equivalent, 161
matrix polynomial, 153
F-equivalent, 157
H-similar, 158
elementary, 153
equivalent, 157
rank, 157
unimodular, 154
metric, 56
minimal polynomial, 84
multiplicity
algebraic, 99
geometric, 99
partial, 99
numerical cone
joint, 62
numerical range, 38
joint, 39
real joint, 52
open cover, 59
optimal control, 336
order of neutrality, 135
Pauli spin matrix, 24
polarization identity, 38, 47
quaternion, 9
congruent, 12
conjugate, 9
norm, 9
real part, 9
similar, 12
vector part, 9
replacement theorem, 28
set
closed, 59
compact, 59
connected, 60
convex, 148

INDEX
363
open, 59
sign characteristic
H-Hamiltonian, 243, 307
H-hermitian, 230
H-skewhermitian, 231
H-skewsymmetric, 303
matrix pencil
φ-hermitian/skewhermitian, 287
φ-skewhermitian, 264
complex hermitian, 349
hermitian/skewhermitian, 203
real symmetric, 342
real symmetric/skewsymmetric,
347
signature, 67
similarity class
complex, 111
quaternion, 111
real, 111
simultaneously congruent, 172
F-simultaneously congruent, 172
simultaneously diagonalizable, 182
singular values, 32
Smith form, 154
C-Smith form, 156
R-Smith form, 156
spectrum, 90
subspace
(H, φ)-negative, 69
(H, φ)-neutral, 69, 133, 146
(H, φ)-nonnegative, 69
(H, φ)-nonpositive, 69
(H, φ)-positive, 69
(H,∗)-negative, 69
(H,∗)-neutral, 69
(H,∗)-nonnegative, 69
(H,∗)-nonpositive, 69
(H,∗)-positive, 69
(Y, φ)-deﬁnite, 323
G-Lagrangian, 235, 246, 316
H-orthogonal companion, 133
φ-orthogonal companion, 44
invariant, 83
maximal, 70, 133
root, 84
skewly linked, 78
Sylvester equation, 14
total divisor, 153
tuple
orthogonal, 29
orthonormal, 29
unilateral equation, 331
unit quaternion, 9
units triple, 17
vector
φ-orthogonal, 44
adjoint, 29
orthogonal, 29

Princeton Series in Applied Mathematics
Chaotic Transitions in Deterministic and Stochastic Dynamical Systems:
Applications of Melnikov Processes in Engineering, Physics, and Neuroscience,
Emil Simiu
Selfsimilar Processes, Paul Embrechts and Makoto Maejima
Self-Regularity: A New Paradigm for Primal-Dual Interior-Point Algorithms,
Jiming Peng, Cornelis Roos, and Tam´as Terlaky
Analytic Theory of Global Bifurcation: An Introduction, Boris Buﬀoni and John
Toland
Entropy, Andreas Greven, Gerhard Keller, and Gerald Warnecke, editors
Auxiliary Signal Design for Failure Detection, Stephen L. Campbell and Ramine
Nikoukhah
Thermodynamics: A Dynamical Systems Approach, Wassim M. Haddad,
VijaySekhar Chellaboina, and Sergey G. Nersesov
Optimization: Insights and Applications, Jan Brinkhuis and Vladimir Tikhomirov
Max Plus at Work, Modeling and Analysis of Synchronized Systems: A Course on
Max-Plus Algebra and its Applications, Bernd Heidergott, Geert Jan Olsder,
and Jacob van der Woude
Impulsive and Hybrid Dynamical Systems: Stability, Dissipativity, and Control,
Wassim M. Haddad, VijaySekhar Chellaboina, and Sergey G. Nersesov
The Traveling Salesman Problem: A Computational Study, David L. Applegate,
Robert E. Bixby, Vasek Chv´atal, and William J. Cook
Positive Deﬁnite Matrices, Rajendra Bhatia
Genomic Signal Processing, Ilya Shmulevich and Edward R. Dougherty
Wave Scattering by Time-Dependent Perturbations: An Introduction, G.F. Roach
Algebraic Curves over a Finite Field, J.W.P. Hirschfeld, G. Korchm´aros, and
F. Torres
Distributed Control of Robotic Networks: A Mathematical Approach to Motion
Coordination Algorithms, Francesco Bullo, Jorge Cort´es, and
Sonia Mart´ınez
Robust Optimization, Aharon Ben-Tal, Laurent El Ghaoui, and
Arkadi Nemirovski
Control Theoretic Splines: Optimal Control, Statistics, and Path Planning, Magnus
Egerstedt and Clyde Martin
Matrices, Moments, and Quadrature with Applications, Gene H. Golub and G´erard
Meurant
Totally Nonnegative Matrices, Shaun M. Fallat and Charles R. Johnson
Matrix Completions, Moments, and Sums of Hermitian Squares, Mih´aly Bakonyi
and Hugo J. Woerdeman
Modern Anti-windup Synthesis: Control Augmentation for Actuator Saturation,
Luca Zaccarian and Andrew W. Teel
Graph Theoretic Methods in Multiagent Networks, Mehran Mesbahi and Magnus
Egerstedt
Stability and Control of Large-Scale Dynamical Systems: A Vector Dissipative
Systems Approach, Wassim M. Haddad and Sergey G. Nersesov
Mathematical Analysis of Deterministic and Stochastic Problems in Complex Media
Electromagnetics, G. F. Roach, I. G. Stratis, and A. N. Yannacopoulos
Topics in Quaternion Linear Algebra, Leiba Rodman

