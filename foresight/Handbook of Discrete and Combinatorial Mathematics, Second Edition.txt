
H A N D B O O K  O F  
DISCRETE AND 
COMBINATORIAL 
MATHEMATICS
SECOND EDITION

DISCRETE  
MATHEMATICS
ITS APPLICATIONS
Series Editors
Miklos Bona
Patrice Ossona de Mendez
Douglas West
R. B. J. T. Allenby and Alan Slomson, How to Count: An Introduction to Combinatorics,  
Third Edition 
Craig P. Bauer, Secret History: The Story of Cryptology
Jürgen Bierbrauer, Introduction to Coding Theory, Second Edition
Katalin Bimbó, Combinatory Logic: Pure, Applied and Typed
Katalin Bimbó, Proof Theory: Sequent Calculi and Related Formalisms
Donald Bindner and Martin Erickson, A Student’s Guide to the Study, Practice, and Tools of 
Modern Mathematics
Francine Blanchet-Sadri, Algorithmic Combinatorics on Partial Words
Miklós Bóna, Combinatorics of Permutations, Second Edition 
Miklós Bóna, Handbook of Enumerative Combinatorics 
Miklós Bóna, Introduction to Enumerative and Analytic Combinatorics, Second Edition 
Jason I. Brown, Discrete Structures and Their Interactions 
Richard A. Brualdi and Drago˘s Cvetkovi´c, A Combinatorial Approach to Matrix Theory and Its 
Applications
Kun-Mao Chao and Bang Ye Wu, Spanning Trees and Optimization Problems
Charalambos A. Charalambides, Enumerative Combinatorics
Gary Chartrand and Ping Zhang, Chromatic Graph Theory
Henri Cohen, Gerhard Frey, et al., Handbook of Elliptic and Hyperelliptic Curve Cryptography
Charles J. Colbourn and Jeffrey H. Dinitz, Handbook of Combinatorial Designs, Second Edition
AND

Titles (continued)
Abhijit Das, Computational Number Theory
Matthias Dehmer and Frank Emmert-Streib, Quantitative Graph Theory:  
Mathematical Foundations and Applications
Martin Erickson, Pearls of Discrete Mathematics
Martin Erickson and Anthony Vazzana, Introduction to Number Theory
Steven Furino, Ying Miao, and Jianxing Yin, Frames and Resolvable Designs: Uses, 
Constructions, and Existence 
Mark S. Gockenbach, Finite-Dimensional Linear Algebra
Randy Goldberg and Lance Riek, A Practical Handbook of Speech Coders
Jacob E. Goodman and Joseph O’Rourke, Handbook of Discrete and Computational Geometry, 
Third Edition
Jonathan L. Gross, Combinatorial Methods with Computer Applications
Jonathan L. Gross and Jay Yellen, Graph Theory and Its Applications, Second Edition
Jonathan L. Gross, Jay Yellen, and Ping Zhang Handbook of Graph Theory, Second Edition
David S. Gunderson, Handbook of Mathematical Induction: Theory and Applications
Richard Hammack, Wilfried Imrich, and Sandi Klavžar, Handbook of Product Graphs,  
Second Edition 
Darrel R. Hankerson, Greg A. Harris, and Peter D. Johnson, Introduction to Information Theory 
and Data Compression, Second Edition
Darel W. Hardy, Fred Richman, and Carol L. Walker, Applied Algebra: Codes, Ciphers, and  
Discrete Algorithms, Second Edition
Daryl D. Harms, Miroslav Kraetzl, Charles J. Colbourn, and John S. Devitt, Network Reliability: 
Experiments with a Symbolic Algebra Environment
Silvia Heubach and Toufik Mansour, Combinatorics of Compositions and Words
Leslie Hogben, Handbook of Linear Algebra, Second Edition
Derek F. Holt with Bettina Eick and Eamonn A. O’Brien, Handbook of Computational Group Theory
David M. Jackson and Terry I. Visentin, An Atlas of Smaller Maps in Orientable and 
Nonorientable Surfaces
Richard E. Klima, Neil P. Sigmon, and Ernest L. Stitzinger, Applications of Abstract Algebra  
with Maple™ and MATLAB®, Second Edition
Richard E. Klima and Neil P. Sigmon, Cryptology: Classical and Modern with Maplets
Patrick Knupp and Kambiz Salari, Verification of Computer Codes in Computational Science  
and Engineering 
William L. Kocay and Donald L. Kreher, Graphs, Algorithms, and Optimization, Second Edition 
Donald L. Kreher and Douglas R. Stinson, Combinatorial Algorithms: Generation Enumeration 
and Search 

Hang T. Lau, A Java Library of Graph Algorithms and Optimization
C. C. Lindner and C. A. Rodger, Design Theory, Second Edition
San Ling, Huaxiong Wang, and Chaoping Xing, Algebraic Curves in Cryptography
Nicholas A. Loehr, Bijective Combinatorics
Nicholas A. Loehr, Combinatorics, Second Edition
Toufik Mansour, Combinatorics of Set Partitions
Toufik Mansour and Matthias Schork, Commutation Relations, Normal Ordering, and Stirling 
Numbers 
Alasdair McAndrew, Introduction to Cryptography with Open-Source Software 
Pierre-Loïc Méliot, Representation Theory of Symmetric Groups  
Elliott Mendelson, Introduction to Mathematical Logic, Fifth Edition 
Alfred J. Menezes, Paul C. van Oorschot, and Scott A. Vanstone, Handbook of Applied 
Cryptography 
Stig F. Mjølsnes, A Multidisciplinary Introduction to Information Security 
Jason J. Molitierno, Applications of Combinatorial Matrix Theory to Laplacian Matrices of Graphs 
Richard A. Mollin, Advanced Number Theory with Applications
Richard A. Mollin, Algebraic Number Theory, Second Edition
Richard A. Mollin, Codes: The Guide to Secrecy from Ancient to Modern Times
Richard A. Mollin, Fundamental Number Theory with Applications, Second Edition
Richard A. Mollin, An Introduction to Cryptography, Second Edition
Richard A. Mollin, Quadratics 
Richard A. Mollin, RSA and Public-Key Cryptography
Carlos J. Moreno and Samuel S. Wagstaff, Jr., Sums of Squares of Integers
Gary L. Mullen and Daniel Panario, Handbook of Finite Fields
Goutam Paul and Subhamoy Maitra, RC4 Stream Cipher and Its Variants 
Dingyi Pei, Authentication Codes and Combinatorial Designs
Kenneth H. Rosen, Handbook of Discrete and Combinatorial Mathematics, Second Edition
Yongtang Shi, Matthias Dehmer, Xueliang Li, and Ivan Gutman, Graph Polynomials
Douglas R. Shier and K.T. Wallenius, Applied Mathematical Modeling: A Multidisciplinary 
Approach
Alexander Stanoyevitch, Introduction to Cryptography with Mathematical Foundations and  
Computer Implementations  
Jörn Steuding, Diophantine Analysis  
Douglas R. Stinson, Cryptography: Theory and Practice, Third Edition
Titles (continued)

Titles (continued)
Roberto Tamassia, Handbook of Graph Drawing and Visualization
Roberto Togneri and Christopher J. deSilva, Fundamentals of Information Theory and Coding 
Design
W. D. Wallis, Introduction to Combinatorial Designs, Second Edition
W. D. Wallis and J. C. George, Introduction to Combinatorics, Second Edition
Jiacun Wang, Handbook of Finite State Based Models and Applications
Lawrence C. Washington, Elliptic Curves: Number Theory and Cryptography, Second Edition
 


DISCRETE MATHEMATICS AND ITS APPLICATIONS
Associate Editors
Douglas R. Shier 
Wayne Goddard
H A N D B O O K  O F  
DISCRETE AND 
COMBINATORIAL 
MATHEMATICS
SECOND EDITION
Editor-in-Chief
Kenneth H. Rosen

CRC Press
Taylor & Francis Group
6000 Broken Sound Parkway NW, Suite 300
Boca Raton, FL 33487-2742
© 2018 by Taylor & Francis Group, LLC
CRC Press is an imprint of Taylor & Francis Group, an Informa business
No claim to original U.S. Government works
Printed on acid-free paper
Version Date: 20170814
International Standard Book Number-13: 978-1-5848-8780-5 (Hardback)
This book contains information obtained from authentic and highly regarded sources. Reasonable efforts have been 
made to publish reliable data and information, but the author and publisher cannot assume responsibility for the validity 
of all materials or the consequences of their use. The authors and publishers have attempted to trace the copyright 
holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this 
form has not been obtained. If any copyright material has not been acknowledged please write and let us know so we may 
rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmitted, or utilized 
in any form by any electronic, mechanical, or other means, now known or hereafter invented, including photocopying, 
microfilming, and recording, or in any information storage or retrieval system, without written permission from the 
publishers.
For permission to photocopy or use material electronically from this work, please access www.copyright.com (http://
www.copyright.com/) or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 
978-750-8400. CCC is a not-for-profit organization that provides licenses and registration for a variety of users. For 
organizations that have been granted a photocopy license by the CCC, a separate system of payment has been arranged.
Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and are used only for 
identification and explanation without intent to infringe.
Visit the Taylor & Francis Web site at
http://www.taylorandfrancis.com
and the CRC Press Web site at
http://www.crcpress.com
MATLAB• is a trademark of The MathWorks, Inc. and is used with permission. The MathWorks does not warrant the 
accuracy of the text or exercises in this book. This book’s use or discussion of MATLAB• software or related products 
does not constitute endorsement or sponsorship by The MathWorks of a particular pedagogical approach or particular 
use of the MATLAB• software.

CONTENTS
ix
CONTENTS
1. FOUNDATIONS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . 1
1.1 Propositional and Predicate Logic
— Jerrold W. Grossman . . . . . . . . . . . . . . . . . . . 12
1.2 Set Theory
— Jerrold W. Grossman . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
1.3 Functions
— Jerrold W. Grossman . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
1.4 Relations
— John G. Michaels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
1.5 Proof Techniques
— Susanna S. Epp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
1.6 Axiomatic Program Veriﬁcation
— David Riley . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
1.7 Logic-Based Computer Programming Paradigms
— Mukesh Dalal . . . . . . . . . . . 70
2. COUNTING METHODS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .85
2.1 Summary of Counting Problems
— John G. Michaels . . . . . . . . . . . . . . . . . . . . . . . . 88
2.2 Basic Counting Techniques
— Jay Yellen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
2.3 Permutations and Combinations
— Edward W. Packel . . . . . . . . . . . . . . . . . . . . . . . .99
2.4 Inclusion/Exclusion
— Robert G. Rieper . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
2.5 Partitions
— George E. Andrews and Andrew V. Sills . . . . . . . . . . . . . . . . . . . . . . . . . . 116
2.6 Burnside/P´olya Counting Formula
— Alan C. Tucker . . . . . . . . . . . . . . . . . . . . . . . . 124
2.7 M¨obius Inversion Counting
— Edward A. Bender . . . . . . . . . . . . . . . . . . . . . . . . . . . .131
2.8 Young Tableaux
— Bruce E. Sagan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .133
3. SEQUENCES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . 139
3.1 Special Sequences
— Thomas A. Dowling and Douglas R. Shier . . . . . . . . . . . . . . 143
3.2 Generating Functions
— Ralph P. Grimaldi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .173
3.3 Recurrence Relations
— Ralph P. Grimaldi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .180
3.4 Finite Differences
— Jay Yellen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
3.5 Finite Sums and Summation
— Victor S. Miller . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
3.6 Asymptotics of Sequences
— Edward A. Bender and Juanjo Ru´e . . . . . . . . . . . . . 202
3.7 Mechanical Summation Procedures
— Kenneth H. Rosen . . . . . . . . . . . . . . . . . . .207
4. NUMBER THEORY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . 217
4.1 Basic Concepts
— Kenneth H. Rosen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .223
4.2 Greatest Common Divisors
— Kenneth H. Rosen . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
4.3 Congruences
— Kenneth H. Rosen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
4.4 Prime Numbers
— Jon F. Grantham and Carl Pomerance . . . . . . . . . . . . . . . . . . . . . 240
4.5 Factorization
— Jon F. Grantham and Carl Pomerance . . . . . . . . . . . . . . . . . . . . . . . . 261
4.6 Arithmetic Functions
— Kenneth H. Rosen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
4.7 Primitive Roots and Quadratic Residues
— Kenneth H. Rosen . . . . . . . . . . . . . . 275
4.8 Diophantine Equations
— Bart E. Goddard . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288

x
CONTENTS
4.9 Diophantine Approximation
— Jeff Shalit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
4.10 Algebraic Number Theory
— Lawrence C. Washington . . . . . . . . . . . . . . . . . . . . . 302
4.11 Elliptic Curves
— Lawrence C. Washington . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .311
5. ALGEBRAIC STRUCTURES
— John G. Michaels . . . . . . . . . . . . . . . . . . . .323
5.1 Algebraic Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
5.2 Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . 331
5.3 Permutation Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343
5.4 Rings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .347
5.5 Polynomial Rings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
5.6 Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .356
5.7 Lattices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .366
5.8 Boolean Algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .369
6. LINEAR ALGEBRA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . 381
6.1 Vector Spaces
— Joel V. Brawley . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .387
6.2 Linear Transformations
— Joel V. Brawley . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
6.3 Matrix Algebra
— Peter R. Turner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .405
6.4 Linear Systems
— Barry Peyton and Esmond Ng . . . . . . . . . . . . . . . . . . . . . . . . . . . . .420
6.5 Eigenanalysis
— R. B. Bapat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .433
6.6 Combinatorial Matrix Theory
— R. B. Bapat and Geir Dahl . . . . . . . . . . . . . . . . . . . 445
6.7 Singular Value Decomposition
— Carla D. Martin . . . . . . . . . . . . . . . . . . . . . . . . . . . 458
7. DISCRETE PROBABILITY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .475
7.1 Fundamental Concepts
— Joseph R. Barr . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 480
7.2 Independence and Dependence
— Joseph R. Barr . . . . . . . . . . . . . . . . . . . . . . . . . 483
7.3 Random Variables
— Joseph R. Barr . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 490
7.4 Discrete Probability Computations
— Peter R. Turner . . . . . . . . . . . . . . . . . . . . . . . 496
7.5 Random Walks
— Patrick Jaillet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 501
7.6 System Reliability
— Douglas R. Shier . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .508
7.7 Discrete-Time Markov Chains
— Vidyadhar G. Kulkarni . . . . . . . . . . . . . . . . . . . . . 518
7.8 Hidden Markov Models
— Narada Warakagoda . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 527
7.9 Queueing Theory
— Vidyadhar G. Kulkarni . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .536
7.10 Simulation
— Lawrence M. Leemis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 543
7.11 The Probabilistic Method
— Niranjan Balachandran . . . . . . . . . . . . . . . . . . . . . . . . 551
8. GRAPH THEORY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . 569
8.1 Introduction to Graphs
— Lowell W. Beineke . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 584
8.2 Graph Models
— Jonathan L. Gross . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 599
8.3 Directed Graphs
— Stephen B. Maurer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .601
8.4 Distance, Connectivity, Traversability, & Matchings
— Edward R. Scheinerman
and Michael D. Plummer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. 612
8.5 Graph Isomorphism and Reconstruction
— Bennet Manvel, Adolfo Piperno,

CONTENTS
xi
and Josef Lauri . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . 625
8.6 Graph Colorings, Labelings, & Related Parameters
— Arthur T. White, Teresa
W. Haynes, Michael A. Henning, Glenn Hurlbert, and Joseph A. Gallian . . . . . . . . . . 631
8.7 Planar Drawings
— Jonathan L. Gross . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .647
8.8 Topological Graph Theory
— Jonathan L. Gross . . . . . . . . . . . . . . . . . . . . . . . . . . . . 654
8.9 Enumerating Graphs
— Paul K. Stockmeyer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 659
8.10 Graph Families
— Maria Chudnovsky, Michael Doob, Michael Krebs, Anthony
Shaheen, Richard Hammack, Sandi Klavˇzar, and Wilfried Imrich . . . . . . . . . . . . . . . . .669
8.11 Analytic Graph Theory
— Stefan A. Burr . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 679
8.12 Hypergraphs
— Andr´as Gy´arf´as . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .684
9. TREES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . 693
9.1 Characterizations and Types of Trees
— Lisa Carbone . . . . . . . . . . . . . . . . . . . . . .696
9.2 Spanning Trees
— Uri Peled . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 705
9.3 Enumerating Trees
— Paul K. Stockmeyer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 711
10. NETWORKS AND FLOWS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .721
10.1 Minimum Spanning Trees
— J. B. Orlin and Ravindra K. Ahuja . . . . . . . . . . . . . . 726
10.2 Matchings
— Douglas R. Shier . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 733
10.3 Shortest Paths
— J. B. Orlin and Ravindra K. Ahuja . . . . . . . . . . . . . . . . . . . . . . . . . 748
10.4 Maximum Flows
— J. B. Orlin and Ravindra K. Ahuja . . . . . . . . . . . . . . . . . . . . . . . .759
10.5 Minimum Cost Flows
— J. B. Orlin and Ravindra K. Ahuja
. . . . . . . . . . . . . . . . . . 769
10.6 Communication Networks
— David Simchi-Levi, Sunil Chopra, and M. Gisela
Bardossy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . 779
10.7 Difﬁcult Routing and Assignment Problems
— Bruce L. Golden, Bharat K.
Kaku, and Xingyin Wang .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. .793
10.8 Small-World Networks
— Vladimir Boginski, Jongeun Kim, and Vladimir
Stozhkov . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . 807
10.9 Network Representations and Data Structures
— Douglas R. Shier . . . . . . . . 826
11. PARTIALLY ORDERED SETS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .843
11.1 Basic Poset Concepts
— Graham Brightwell and Douglas B. West . . . . . . . . . . . 851
11.2 Poset Properties
— Graham Brightwell and Douglas B. West . . . . . . . . . . . . . . . . .864
12. COMBINATORIAL DESIGNS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 879
12.1 Block Designs
— Charles J. Colbourn and Jeffrey H. Dinitz . . . . . . . . . . . . . . . . . . .885
12.2 Symmetric Designs and Finite Geometries — Charles J. Colbourn and
Jeffrey H. Dinitz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . 896
12.3 Latin Squares and Orthogonal Arrays
— Charles J. Colbourn and Jeffrey H.
Dinitz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . 905

xii
CONTENTS
12.4 Matroids
— James G. Oxley . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .913
13. DISCRETE AND COMPUTATIONAL GEOMETRY . . . . . . . . . . . . 925
13.1 Arrangements of Geometric Objects
— Ileana Streinu . . . . . . . . . . . . . . . . . . . . . 933
13.2 Space Filling
— Karoly Bezdek . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .952
13.3 Combinatorial Geometry
— J´anos Pach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .961
13.4 Polyhedra
— Tamal K. Dey . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .970
13.5 Algorithms and Complexity in Computational Geometry
— Jianer Chen . . . .974
13.6 Geometric Data Structures and Searching
— Dina Kravets . . . . . . . . . . . . . . . . 983
13.7 Computational Techniques
— Nancy M. Amato . . . . . . . . . . . . . . . . . . . . . . . . . . . . 992
13.8 Applications of Geometry
— W. Randolph Franklin . . . . . . . . . . . . . . . . . . . . . . . . . 998
14. CODING THEORY
— Alfred J. Menezes, Paul C. van Oorschot, David
Joyner, and Tony Shaska . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1023
14.1 Communication Systems and Information Theory . . . . . . . . . . . . . . . . . . . . . . . . .1027
14.2 Basics of Coding Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1031
14.3 Linear Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1035
14.4 Cyclic Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .1044
14.5 Bounds for Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1053
14.6 Nonlinear Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1056
14.7 Convolutional Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1057
14.8 Quantum Error-Correcting Codes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1062
15. CRYPTOGRAPHY
— Charles C. Y. Lam (Chapter Editor) . . . . . . . . . . . . . . 1069
15.1 Basics of Cryptography
— Charles C. Y. Lam . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1075
15.2 Classical Cryptography
— Charles C. Y. Lam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1079
15.3 Modern Private Key Cryptosystems
— Khoongming Khoo . . . . . . . . . . . . . . . . 1083
15.4 Hash Functions
— Charles C. Y. Lam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1096
15.5 Public Key Cryptography
— Shaoquan Jiang and Charles C. Y. Lam . . . . . . . . 1100
15.6 Cryptographic Mechanisms
— Shaoquan Jiang . . . . . . . . . . . . . . . . . . . . . . . . . . 1112
15.7 High-Level Applications of Cryptography
— Charles C. Y. Lam . . . . . . . . . . . . 1129
16. DISCRETE OPTIMIZATION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1143
16.1 Linear Programming
— Beth Novick . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1148
16.2 Location Theory
— S. Louis Hakimi and Maria Albareda . . . . . . . . . . . . . . . . . . . .1174
16.3 Packing and Covering
— Sunil Chopra and David Simchi-Levi . . . . . . . . . . . . . . 1185
16.4 Activity Nets
— S. E. Elmaghraby . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1195
16.5 Game Theory
— Michael Mesterton-Gibbons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1205
16.6 Sperner’s Lemma and Fixed Points
— Joseph R. Barr . . . . . . . . . . . . . . . . . . . .1217
16.7 Combinatorial Auctions
— Robert W. Day . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1222
16.8 Very Large-Scale Neighborhood Search
— Douglas Altner . . . . . . . . . . . . . . . 1232

CONTENTS
xiii
16.9 Tabu Search
— Manuel Laguna . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1241
17. THEORETICAL COMPUTER SCIENCE . . . . . . . . . . . . . . . . . . . . . . . 1265
17.1 Computational Models
— Wayne Goddard . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1274
17.2 Computability
— William Gasarch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1286
17.3 Languages and Grammars
— Aarto Salomaa . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1290
17.4 Algorithmic Complexity
— Thomas Cormen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1301
17.5 Complexity Classes
— Lane A. Hemaspaandra . . . . . . . . . . . . . . . . . . . . . . . . . . . 1308
17.6 Randomized Algorithms
— Milena Mihail . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1315
18. INFORMATION STRUCTURES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1323
18.1 Abstract Datatypes
— Charles H. Goldberg . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1330
18.2 Concrete Data Structures
— Jonathan L. Gross . . . . . . . . . . . . . . . . . . . . . . . . . . 1339
18.3 Sorting and Searching
— Jianer Chen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1347
18.4 Hashing
— Viera Krnanova Proulx . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1362
18.5 Dynamic Graph Algorithms
— Joan Feigenbaum and Sampath Kannan . . . . .1365
19. DATA MINING . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . 1375
19.1 Data Mining Fundamentals
— Richard Scherl . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1378
19.2 Frequent Itemset Mining and Association Rules
— Richard Scherl . . . . . . . .1384
19.3 Classiﬁcation Methods
— Richard Scherl . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1387
19.4 Clustering
— Daniel Aloise and Pierre Hansen . . . . . . . . . . . . . . . . . . . . . . . . . . . . .1394
19.5 Outlier Detection
— Richard Scherl . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1417
20. DISCRETE BIOINFORMATICS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1425
20.1 Sequence Alignment
— Stephen F. Altschul and Mihai Pop . . . . . . . . . . . . . . . . .1428
20.2 Phylogenetics
— Joseph Rusinko . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1446
20.3 Discrete-Time Dynamical Systems
— Elena Dimitrova . . . . . . . . . . . . . . . . . . . .1455
20.4 Genome Assembly
— Andy Jenkins and Matthew Macauley . . . . . . . . . . . . . . . .1467
20.5 RNA Folding
— Qijun He, Matthew Macauley, and Svetlana Poznanovi´c . . . . . . 1475
20.6 Combinatorial Neural Codes
— Carina Curto and Vladimir Itskov . . . . . . . . . . .1483
20.7 Food Webs and Graphs
— Margaret Cozzens . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1490
BIOGRAPHIES
— Victor J. Katz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1515
INDEX
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . 1541


PREFACE
xv
PREFACE
The importance of discrete and combinatorial mathematics has increased dramatically
within the last few decades. This second edition has been written to update all content
and to broaden the coverage. We have been gratiﬁed by the success of the ﬁrst edition
of the Handbook. We hope that the many readers who have asked for a second edition
will ﬁnd it worth the wait.
The purpose of the Handbook of Discrete and Combinatorial Mathematics is to provide
a comprehensive reference volume for computer scientists, engineers, mathematicians,
as well as students, physical and social scientists, and reference librarians, who need
information about discrete and combinatorial mathematics.
This ﬁrst edition of this book was the ﬁrst resource that presented such information in
a ready-reference form designed for all those who use aspects of this subject in their
work or studies. This second edition is a major revision of the ﬁrst edition. It includes
extensive additions and updates, summarized later in this preface. The scope of this
handbook includes the many areas generally considered to be parts of discrete mathe-
matics, focusing on the information considered essential to its application in computer
science, engineering, and other disciplines. Some of the fundamental topic areas covered
in this edition include:
logic and set theory
graph theory
enumeration
trees
integer sequences
network ﬂows
recurrence relations
combinatorial designs
generating functions
computational geometry
number theory
coding theory
abstract algebra
cryptography
linear algebra
discrete optimization
discrete probability theory
automata theory
data mining
data structures and algorithms
discrete bioinformatics
Format
The material in the Handbook is presented so that key information can be located and
used quickly and easily. Each chapter includes a glossary that provides succinct deﬁni-
tions of the most important terms from that chapter. Individual topics are covered in
sections and subsections within chapters, each of which is organized into clearly identi-
ﬁable parts: deﬁnitions, facts, and examples. Lists of facts include:
• information about how material is used and why it is important
• historical information
• key theorems
• the latest results
• the status of open questions

xvi
PREFACE
• tables of numerical values, generally not easily computed
• summary tables
• key algorithms in simple pseudocode
• information about algorithms, such as their complexity
• major applications
• pointers to additional resources, both websites and printed material.
Facts are presented concisely and are listed so that they can be easily found and un-
derstood. Cross-references linking portions of the Handbook are also provided. Readers
who wish to study a topic further can consult the resources listed.
The material in the Handbook has been chosen for inclusion primarily because it is
important and useful. Additional material has been added to ensure comprehensiveness
so that readers encountering new terminology and concepts from discrete mathematics
in their explorations will be able to get help from this book.
Examples are provided to illustrate some of the key deﬁnitions, facts, and algorithms.
Some curious and entertaining facts and puzzles that some readers may ﬁnd intriguing are
also included. Readers will also ﬁnd an extensive collection of biographies after the main
chapters, highlighting the lives of many important contributors to discrete mathematics.
Each chapter of the book includes a list of references divided into a list of printed resources
and a list of relevant websites.
How This Book Was Developed
The organization and structure of the ﬁrst edition of this Handbook were developed by
a team that included the chief editor, three associate editors, a project editor, and the
editor from CRC Press. This team put together a proposed table of contents which
was then analyzed by members of a group of advisory editors, each an expert in one
or more aspects of discrete mathematics.
These advisory editors suggested changes,
including the coverage of additional important topics. Once the table of contents was
fully developed, the individual sections of the book were prepared by a group of more
than 70 contributors from industry and academia who understand how this material is
used and why it is important. Contributors worked under the direction of the associate
editors and chief editor, with these editors ensuring consistency of style as well as clarity
and comprehensiveness in the presentation of material. Material was carefully reviewed
by authors and our team of editors to ensure accuracy and consistency of style.
For the second edition, a new team was assembled. The ﬁrst goal of this team was to
put together a new table of contents. This involved identifying opportunities for new
chapters and new sections to broaden the scope and appeal of the second edition. With
the help of previous and new contributors, additional material was developed and existing
material was updated and expanded, following the style and maintaining, or improving,
the presentation in the ﬁrst edition.
Changes in the Second Edition
The development of the second edition of this book was a major eﬀort, extending over
many years. Since the ﬁrst edition appeared in 1999, many new discoveries have been
made and new areas have grown in importance. Important changes include:

PREFACE
xvii
• an increase from 17 to 20 chapters and over 360 additional pages
• new chapters on discrete bioinformatics and data mining
• individual chapters on coding theory and on cryptography with expanded coverage
(previously covered in a single chapter)
• new sections on many topics, including
Algebraic Number Theory
Elliptic Curves
Singular Value Decomposition
Hidden Markov Models
Probabilistic Method
Perfect Graphs
Expander Graphs
Small-World Networks
Combinatorial Auctions
Very Large-Scale Neighborhood Search
Tabu Search
Quantum Error-Correcting Codes
Classical Cryptography
Cryptographic Hashing Functions
Cryptographic Mechanisms
Modern Private Key Cryptography
Cryptographic Applications
Association Methods
Classiﬁcation
Clustering
Outlier Analysis
Sequence Alignment
Phylogenetic Trees
Discrete-Time Dynamical Systems
RNA Folding
Food Webs and Competition Graphs
Neural Codes
Genome Assembly
• thousands of updates and additions to existing sections, with major changes or new
subsections including
Primes Numbers
Combinatorial Matrix Theory
Cyclic Codes
Public Key Cryptography
Partitions
Asymptotics of Sequences
Factorization
Distance, Connectivity, Traversability, and Matching
Expander Graphs
Graph Colorings, Labelings, and Related Parameters
Simulation
Communication Networks
Location Theory
Diﬃcult Routing and Assignment Problems
• more than 30 new biographies
• hundreds of new web resources, which have been accessed to verify their availability
in mid-2017
Acknowledgments
First and foremost, we thank Bob Ross, the CRC editor for this book, for his support
and his encouragement in completing this Handbook. We would also like to thank Bob
Stern, our previous editor, for initiating the project of updating the Handbook and for
supporting us patiently over many years. We would also like to thank Wayne Yuhasz,
our original CRC editor, who commissioned the ﬁrst edition. Thanks also go to the staﬀ
at CRC who helped with the production.
We would also like to thank the many people who were involved with this project. First,
we would like to thank the team of advisory editors who helped make this reference

xviii
PREFACE
relevant, useful, unique, and up-to-date. We want to thank our many contributors for
their wonderful contributions and to several who helped edit individual chapters. With
the passing of time, some of our advisory editors and contributors are no longer with
us. We fondly remember and appreciate these colleagues and friends. (We have used the
symbol ⊛to designate these individuals.)
Finally, as Chief Editor, I would like to express my gratitude to my two associate editors
for this edition. Without either of them this new edition would not have been possible.
Both were essential in our quest to bring this Handbook up to date and to extend its
scope. Douglas Shier confronted every problem that got in our way and found a workable
solution. He kept the progress on this project going with his commitment and his abilities.
Wayne Goddard used his masterful LATEX skills to overcome all the typesetting challenges
that confronted us as we migrated to a new typesetting environment.
MATLAB R
⃝is a registered trademark of The MathWorks, Inc. For product information please
contact: The MathWorks, Inc., 3 Apple Hill Drive, Natick, MA, 01760-2098 USA. Tel: 508-647-
7000, Fax: 508-647-7001, E-mail: info@mathworks.com, Web: www.mathworks.com

ADVISORY EDITORIAL BOARD
xix
ADVISORY EDITORIAL BOARD
Andrew Odlyzko — Chief Advisory Editor
University of Minnesota
Stephen F. Altschul
National Institutes of Health
George E. Andrews
Pennsylvania State University
⊛Francis T. Boesch
Stevens Institute of Technology
Ernie Brickell
Intel Corp.
Fan R. K. Chung
Univ. of California at San Diego
Charles J. Colbourn
Arizona State University
Stan Devitt
Oracle
Zvi Galil
Columbia University
Keith Geddes
University of Waterloo
Ronald L. Graham
Univ. of California at San Diego
Ralph P. Grimaldi
Rose-Hulman Inst. of Technology
⊛Frank Harary
New Mexico State University
Alan Hoﬀman
IBM
Bernard Korte
Univeristy of Bonn
Jeﬀrey C. Lagarias
University of Michigan
Carl Pomerance
Dartmouth University
Fred S. Roberts
Rutgers University
Pierre Rosenstiehl
Centre d’Analyse et de Math. Soc.
Francis Sullivan
Institute for Defense Analyses
J. H. van Lint
Eindhoven University of Technology
⊛Scott Vanstone
University of Waterloo
Peter Winkler
Dartmouth University


CONTRIBUTORS
xxi
CONTRIBUTORS
Ravindra K. Ahuja
University of Florida
Maria Albareda
Universitat Polit`ecnica de Catalunya
Daniel Aloise
Universidade Federal do Rio Grande
do Norte
Douglas Altner
MITRE
Stephen F. Altschul
National Center for Biotechnology
Nancy M. Amato
Texas A&M University
George E. Andrews
Pennsylvania State University
Niranjan Balachandran
Indian Institute of Technology, Bombay
R. B. Bapat
Indian Statistical Institute
M. Gisela Bardossy
University of Baltimore
Joseph R. Barr
HomeUnion
Lowell W. Beineke
Indiana University – Purdue University
Fort Wayne
Edward A. Bender
University of California at San Diego
Karoly Bezdek
Cornell University
Vladimir Boginski
University of Florida
Joel V. Brawley
Clemson University
Graham Brightwell
London School of Economics
Stefan A. Burr
City College of New York
Lisa Carbone
Rutgers University
Jianer Chen
Texas A&M University
Sunil Chopra
Northwestern University
Maria Chudnovsky
Princeton University
Charles J. Colbourn
Arizona State University
Thomas Cormen
Dartmouth College
Margaret Cozzens
Rutgers University
Carina Curto
Pennyslvania State University
Geir Dahl
University of Oslo
Mukesh Dalal
i2 Technologies
Robert W. Day
University of Connecticut
Tamal K. Dey
Ohio State University
Elena Dimitrova
Clemson University

xxii
CONTRIBUTORS
Jeﬀrey H. Dinitz
University of Vermont
Michael Doob
University of Manitoba
Thomas A. Dowling
Ohio State University
⊛S. E. Elmaghraby
North Carolina State University
Susanna S. Epp
DePaul University
Joan Feigenbaum
Yale University
W. Randolph Franklin
Rensselaer Polytechnic Institute
Joseph A. Gallian
University of Minnesota Duluth
William Gasarch
University of Maryland
Bart E. Goddard
University of Texas
Wayne Goddard
Clemson University
⊛Charles H. Goldberg
The College of New Jersey
Bruce L. Golden
University of Maryland
Jon F. Grantham
Institute for Defense Analyses
Ralph P. Grimaldi
Rose-Hulman Institute of Technology
Jonathan L. Gross
Columbia University
Jerrold W. Grossman
Oakland University
Andr´as Gy´arf´as
Hungarian Academy of Sciences
⊛S. Louis Hakimi
University of California at Davis
Richard Hammack
Virginia Commonwealth University
Pierre Hansen
HEC Montr´eal
Teresa W. Haynes
East Tennessee State University
Qijun He
Virginia Tech
Lane A. Hemaspaandra
University of Rochester
Michael A. Henning
University of Johannesburg
Glenn Hurlbert
Virginia Commonwealth University
Wilfried Imrich
Montanuniversit¨at Leoben
Vladimir Itskov
Pennsylvania State University
Patrick Jaillet
Massachusetts Institute of Technology
Andy Jenkins
University of Georgia
Shaoquan Jiang
Mianyang Normal University
David Joyner
United States Naval Academy
Bharat K. Kaku
Georgetown University
Sampath Kannan
University of Pennsylvania
Victor J. Katz
University of the District of Columbia
Khoongming Khoo
DSO National Laboratories, Singapore

CONTRIBUTORS
xxiii
Jongeun Kim
University of Florida
Sandi Klavˇzar
University of Ljubljana
Dina Kravets
Retired
Michael Krebs
California State University, Los Angeles
Vidyadhar G. Kulkarni
University of North Carolina
Manuel Laguna
University of Colorado
Charles C. Y. Lam
California State University, Bakersﬁeld
Josef Lauri
University of Malta
Lawrence M. Leemis
The College of William and Mary
Matthew Macauley
Clemson University
Bennet Manvel
Colorado State University
⊛Carla D. Martin
National Security Agency
Stephen B. Maurer
Swarthmore College
Alfred J. Menezes
University of Waterloo
Michael Mesterton-Gibbons
Florida State University
John G. Michaels
SUNY Brockport
Milena Mihail
Georgia Institute of Technology
Victor S. Miller
Institute for Defense Analyses
Esmond Ng
Lawrence Berkeley National Laboratory
Beth Novick
Clemson University
J. B. Orlin
Massachusetts Institute of Technology
James G. Oxley
Louisiana State University
J´anos Pach
EPFL Lausanne and R´enyi Institute
Edward W. Packel
Lake Forest College
⊛Uri Peled
University of Illinois at Chicago
Barry Peyton
Dalton State College
Adolfo Piperno
Sapienza University of Rome
Michael D. Plummer
Vanderbilt University
Carl Pomerance
Dartmouth University
Mihai Pop
University of Maryland
Svetlana Poznanovi´c
Clemson University
Viera Krnanova Proulx
Northeastern University
Robert G. Rieper
William Patterson University
David Riley
University of Wisconsin
Kenneth H. Rosen
Monmouth University
Juanjo Ru´e
Universitat Polit`ecnica de Catalunya

xxiv
CONTRIBUTORS
Joseph Rusinko
Hobart and William Smith Colleges
Bruce E. Sagan
Michigan State University
Aarto Salomaa
University of Turku
Edward R. Scheinerman
Johns Hopkins University
Richard Scherl
Monmouth University
Anthony Shaheen
California State University, Los Angeles
JeﬀShalit
University of Waterloo
Tony Shaska
Oakland University
Douglas R. Shier
Clemson University
Andrew V. Sills
Georgia Southern University
David Simchi-Levi
Northwestern University
Paul K. Stockmeyer
The College of William and Mary
Vladimir Stozhkov
University of Florida
Ileana Streinu
Smith College
Alan C. Tucker
SUNY Stony Brook
Peter R. Turner
United States Naval Academy
Paul C. van Oorschot
Entrust Technologies
Xingyin Wang
Singapore University of Technology
and Design
Narada Warakagoda
Telenor
Lawrence C. Washington
University of Maryland
Douglas B. West
University of Illinois at Urbana–
Champaign
Arthur T. White
Western Michigan University
Jay Yellen
Florida Institute of Technology

1
FOUNDATIONS
1.1 Propositional and Predicate Logic
Jerrold W. Grossman
1.1.1 Propositions and Logical Operations
1.1.2 Equivalences, Identities, and Normal Forms
1.1.3 Predicate Logic
1.2 Set Theory
Jerrold W. Grossman
1.2.1 Sets
1.2.2 Set Operations
1.2.3 Inﬁnite Sets
1.2.4 Axioms for Set Theory
1.3 Functions
Jerrold W. Grossman
1.3.1 Basic Terminology for Functions
1.3.2 Computational Representation
1.3.3 Asymptotic Behavior
1.4 Relations
John G. Michaels
1.4.1 Binary Relations and Their Properties
1.4.2 Equivalence Relations
1.4.3 Partially Ordered Sets
1.4.4 n-ary Relations
1.5 Proof Techniques
Susanna S. Epp
1.5.1 Rules of Inference
1.5.2 Proofs
1.5.3 Disproofs
1.5.4 Mathematical Induction
1.5.5 Diagonalization Arguments
1.6 Axiomatic Program Veriﬁcation
David Riley
1.6.1 Assertions and Semantic Axioms
1.6.2 NOP, Assignment, and Sequencing Axioms
1.6.3 Axioms for Conditional Execution Constructs
1.6.4 Axioms for Loop Constructs
1.6.5 Axioms for Subprogram Constructs
1.7 Logic-Based Computer Programming Paradigms
Mukesh Dalal
1.7.1 Logic Programming
1.7.2 Fuzzy Sets and Logic
1.7.3 Production Systems
1.7.4 Automated Reasoning

2
Chapter 1
FOUNDATIONS
INTRODUCTION
This chapter covers material usually referred to as the foundations of mathematics, in-
cluding logic, sets, and functions. In addition to covering these foundational areas, this
chapter includes material that shows how these topics are applied to discrete mathe-
matics, computer science, and electrical engineering. For example, this chapter covers
methods of proof, program veriﬁcation, and fuzzy reasoning.
GLOSSARY
action: a literal or a print command in a production system.
aleph-null: the cardinality ℵ0 of the set N of natural numbers.
AND: the logical operator for conjunction, also written ∧.
antecedent: in a conditional proposition p →q (“if p then q”), the proposition p (“if-
clause”) that precedes the arrow.
antichain: a subset of a poset in which no two elements are comparable.
antisymmetric: the property of a binary relation R that if aRb and bRa, then a = b.
argument form: a sequence of statement forms, each called a premise of the argument,
followed by a statement form called a conclusion of the argument.
assertion (or program assertion): a program comment specifying some conditions
on the values of the computational variables; these conditions are supposed to hold
whenever program ﬂow reaches the location of the assertion.
asymmetric: the property of a binary relation R that if aRb, then bR/ a.
asymptotic: A function f is asymptotic to a function g, written f(x) ∼g(x), if f(x) ̸=
0 for suﬃciently large x and limx→∞
g(x)
f(x) = 1.
atom (or atomic formula): a simplest formula of predicate logic.
atomic formula: See atom.
atomic proposition: a proposition that cannot be analyzed into smaller parts and
logical operations.
automated reasoning: the process of proving theorems using a computer program
that can draw conclusions that follow logically from a set of given facts.
axiom: a statement that is assumed to be true; a postulate.
axiom of choice: the assertion that given any nonempty collection A of pairwise dis-
joint sets, there is a set that consists of exactly one element from each of the sets in
A.
axiom (or semantic axiom): a rule for a programming language construct prescribing
the change of values of computational variables when an instruction of that construct-
type is executed.
basis step: a proof of the basis premise (ﬁrst case) in a proof by mathematical induc-
tion.
big-oh notation: f is O(g), written f = O(g), if there are constants C and k such that
|f(x)| ≤C|g(x)| for all x > k.

GLOSSARY
3
bijection (or bijective function): a function that is one-to-one and onto.
bijective function: See bijection.
binary relation (from a set A to a set B): any subset of A × B.
binary relation (on a set A): a binary relation from A to A; i.e., a subset of A × A.
body (of a clause A1, . . . , An ←B1, . . . , Bm in a logic program): the literals B1, . . . , Bm
after ←.
cardinal number (or cardinality) of a set: for a ﬁnite set, the number of elements;
for an inﬁnite set, the order of inﬁnity. The cardinal number of S is written |S|.
cardinality: See cardinal number.
Cartesian product (of sets A and B): the set A×B of ordered pairs (a, b) with a ∈A
and b ∈B (more generally, the iterated Cartesian product A1 × A2 × · · · × An is
the set of ordered n-tuples (a1, a2, . . . , an), with ai ∈Ai for each i).
ceiling (of x): the smallest integer that is greater than or equal to x, written ⌈x⌉.
chain: a subset of a poset in which every pair of elements are comparable.
characteristic function (of a set S): the function from S to {0, 1} whose value at x
is 1 if x ∈S and 0 if x /∈S.
clause (in a logic program): a closed formula of the form ∀x1 . . . ∀xs(A1 ∨· · · ∨An ←
B1 ∧· · · ∧Bm).
closed formula: for a function value f(x), an algebraic expression in x.
closure (of a relation R with respect to a property P): the relation S, if it exists, that
has property P and contains R, such that S is a subset of every relation that has
property P and contains R.
codomain (of a function): the set in which the function values occur.
comparable (elements in a poset): elements that are related by the partial order rela-
tion.
complement (of a relation): given a relation R, the relation R where aRb if and only
if aR/ b.
complement (of a set): given a set A in a “universal” domain U, the set A of objects
in U that are not in A.
complement operator: a function [0, 1] →[0, 1] used for complementing fuzzy sets.
complete: the property of a set of axioms that it is possible to prove all true statements.
complex number: a number of the form a + bi, where a and b are real numbers, and
i2 = −1; the set of all complex numbers is denoted C.
composite key: given an n-ary relation R on A1 ×A2 ×· · ·×An, a product of domains
Ai1 ×Ai2 ×· · ·×Aim such that for each m-tuple (ai1, ai2, . . . , aim) ∈Ai1 ×Ai2 ×· · ·×
Aim, there is at most one n-tuple in R that matches (ai1, ai2, . . . , aim) in coordinates
i1, i2, . . . , im.
composition (of relations): for R a relation from A to B and S a relation from B to
C, the relation S ◦R from A to C such that a(S ◦R)c if and only if there exists
b ∈B such that aRb and bSc.
composition (of functions): the function f ◦g whose value at x is f(g(x)).

4
Chapter 1
FOUNDATIONS
compound proposition: a proposition built up from atomic propositions and logical
connectives.
computer-assisted proof : a proof that relies on checking the validity of a large num-
ber of cases using a special purpose computer program.
conclusion (of an argument form): the last statement of an argument form.
conclusion (of a proof): the last proposition of a proof; the objective of the proof is
demonstrating that the conclusion follows from the premises.
condition: the disjunction A1 ∨· · · ∨An of atomic formulas.
conditional statement: the compound proposition p →q (“if p then q”) that is true
except when p is true and q is false.
conjunction: the compound proposition p ∧q (“p and q”) that is true only when p and
q are both true.
conjunctive normal form: for a proposition in the variables p1, p2, . . . , pn, an equiva-
lent proposition that is the conjunction of disjunctions, with each disjunction of the
form xk1 ∨xk2 ∨· · · ∨xkm, where xkj is either pkj or ¬pkj.
consequent: in a conditional proposition p →q (“if p then q”) the proposition q (“then-
clause”) that follows the arrow.
consistent: the property of a set of axioms that no contradiction can be deduced from
the axioms.
construct (or program construct): the general form of a programming instruction
such as an assignment, a conditional, or a while-loop.
continuum hypothesis: the assertion that the cardinal number of the real numbers
is the smallest cardinal number greater than the cardinal number of the natural
numbers.
contradiction: a self-contradictory proposition, one that is always false.
contradiction (in an indirect proof): the negation of a premise.
contrapositive (of the conditional proposition p →q): the conditional proposition
¬q →¬p.
converse (of the conditional proposition p →q): the conditional proposition q →p.
converse relation: another name for the inverse relation.
corollary: a theorem that is derived as an easy consequence of another theorem.
correct conclusion: the conclusion of a valid proof, when all the premises are true.
countable set: a set that is ﬁnite or denumerable.
counterexample: a case that makes a statement false.
deﬁnite clause: clause with at most one atom in its head.
denumerable set: a set that can be placed in one-to-one correspondence with the
natural numbers.
diagonalization proof : any proof that involves something analogous to the diagonal
of a list of sequences.
diﬀerence: a binary relation R −S such that a(R −S)b if and only if aRb is true
and aSb is false.

GLOSSARY
5
diﬀerence (of sets): the set A −B of objects in A that are not in B.
direct proof : a proof of p →q that assumes p and shows that q must follow.
disjoint (pair of sets): two sets with no members in common.
disjunction: the statement p ∨q (“p or q”) that is true when at least one of the two
propositions p and q is true; also called inclusive or.
disjunctive normal form: for a proposition in the variables p1, p2, . . . , pn, an equiva-
lent proposition that is the disjunction of conjunctions, with each conjunction of the
form xk1 ∧xk2 ∧· · · ∧xkm, where xkj is either pkj or ¬pkj.
disproof : a proof that a statement is false.
divisibility lattice: the lattice consisting of the positive integers under the relation of
divisibility.
domain (of a function): the set on which a function acts.
element (of a set): member of the set; the notation a ∈A means that a is an element
of A.
elementary projection function: the function πi : X1 × · · · × Xn →Xi such that
π(x1, . . . , xn) = xi.
empty set: the set with no elements, written ∅or { }.
epimorphism: an onto function.
equality (of sets): the property that two sets have the same elements.
equivalence class: given an equivalence relation on a set A and a ∈A, the subset of A
consisting of all elements related to a.
equivalence relation: a binary relation that is reﬂexive, symmetric, and transitive.
equivalent propositions: two compound propositions (on the same simple variables)
with the same truth table.
existential quantiﬁer: the quantiﬁer ∃x, read “there is an x”.
existentially quantiﬁed predicate: a statement (∃x)P(x) that there exists a value
of x such that P(x) is true.
exponential function: any function of the form bx, b a positive constant, b ̸= 1.
fact set: the set of ground atomic formulas.
factorial (function): the function n! whose value on the argument n is the product
1 · 2 · 3 . . . n; that is, n! = 1 · 2 · 3 . . . n.
ﬁnite: the property of a set that it is either empty or else can be put in a one-to-one
correspondence with a set {1, 2, 3, . . ., n} for some positive integer n.
ﬁrst-order logic: See predicate calculus.
ﬂoor (of x): the greatest integer less than or equal to x, written ⌊x⌋.
formula: a logical expression constructed from atoms with conjunctions, disjunctions,
and negations, possibly with some logical quantiﬁers.
full conjunctive normal form: conjunctive normal form where each disjunction is a
disjunction of all variables or their negations.
full disjunctive normal form: disjunctive normal form where each conjunction is a
conjunction of all variables or their negations.

6
Chapter 1
FOUNDATIONS
fully parenthesized proposition: any proposition that can be obtained using the fol-
lowing recursive deﬁnition: (1) each variable is fully parenthesized; (2) if P and Q
are fully parenthesized, so are (¬P), (P ∧Q), (P ∨Q), (P →Q), and (P ↔Q).
function f : A →B: a rule that assigns to every object a in the domain set A exactly
one object f(a) in the codomain set B.
functionally complete set: a set of logical connectives from which all other connec-
tives can be derived by composition.
fuzzy logic: a system of logic in which each statement has a truth value in the inter-
val [0, 1].
fuzzy set: a set in which each element is associated with a number in the interval [0, 1]
that measures its degree of membership.
generalized continuum hypothesis: the assertion that for every inﬁnite set S there
is no cardinal number greater than |S| and less than |P(S)|.
goal: a clause with an empty head.
graph (of a function): given a function f : A →B, the set {(a, b) | b = f(a)} ⊆A × B.
greatest lower bound (of a subset of a poset): an element of the poset that is a lower
bound of the subset and is greater than or equal to every other lower bound of the
subset.
ground formula: a formula without any variables.
halting function: the function that maps computer programs to the set {0, 1}, with
value 1 if the program always halts, regardless of input, and 0 otherwise.
Hasse diagram: a directed graph that represents a poset.
head (of a clause A1, . . . , An ←B1, . . . , Bm): the literals A1, . . . , An before ←.
identity function (on a set): given a set A, the function from A to itself whose value
at x is x.
image set (of a function): the set of function values as x ranges over all objects of the
domain.
implication: formally, the relation P ⇒Q that a proposition Q is true whenever
proposition P is true; informally, a synonym for the conditional statement p →q.
incomparable: two elements in a poset that are not related by the partial order rela-
tion.
independent: the property of a set of axioms that none of the axioms can be deduced
from the other axioms.
indirect proof : a proof of p →q that assumes ¬q is true and proves that ¬p is true.
induced partition (on a set under an equivalence relation): the collection of equiva-
lence classes under the relation.
induction: See mathematical induction.
induction hypothesis: in a mathematical induction proof, the statement P(xk) in the
induction step.
induction step: in a mathematical induction proof, a proof of the induction premise
“if P(xk) is true, then P(xk+1) is true”.
inductive proof : See mathematical induction.

GLOSSARY
7
inﬁnite (set): a set that is not ﬁnite.
injection (or injective function): a one-to-one function.
instance (of a formula): formula obtained using a substitution.
instantiation: substitution of concrete values for the free variables of a statement or
sequence of statements; an instance of a production rule.
integer: a whole number, possibly zero or negative; i.e., one of the elements in the set
Z = {. . . , −2, −1, 0, 1, 2, . . .}.
intersection: the set A ∩B of objects common to both sets A and B.
intersection relation: for binary relations R and S on A, the relation R ∩S where
a(R ∩S)b if and only if aRb and aSb.
interval (in a poset): given a ≤b in a poset, a subset of the poset consisting of all
elements x such that a ≤x ≤b.
inverse function: for a one-to-one, onto function f : X →Y , the function f −1 : Y →X
whose value at y ∈Y is the unique x ∈X such that f(x) = y.
inverse image (under f : X →Y of a subset T ⊆Y ): the subset {x ∈X | f(x) ∈T },
written f −1(T ).
inverse relation: for a binary relation R from A to B, the relation R−1 from B to A
where bR−1a if and only if aRb.
invertible (function): a one-to-one and onto function; a function that has an inverse.
irrational number: a real number that is not rational.
irreﬂexive: the property of a binary relation R on A that aR/ a, for all a ∈A.
lattice: a poset in which every pair of elements has both a least upper bound and a
greatest lower bound.
least upper bound (of a subset of a poset): an element of the poset that is an upper
bound of the subset and is less than or equal to every other upper bound of the
subset.
lemma: a theorem that is an intermediate step in the proof of a more important theo-
rem.
linearly ordered: the property of a poset that every pair of elements are comparable,
also called totally ordered.
literal: an atom or its negation.
little-oh notation: f is o(g) if limx→∞
 f(x)
g(x)
 = 0.
logarithmic function: a function logb x (b a positive constant, b ̸= 1) deﬁned by the
rule logb x = y if and only if by = x.
logic program: a ﬁnite sequence of deﬁnite clauses.
logically equivalent propositions: compound propositions that involve the same vari-
ables and have the same truth table.
logically implies: A compound proposition P logically implies a compound proposi-
tion Q if Q is true whenever P is true.
loop invariant: an expression that speciﬁes the circumstance under which the loop
body will be executed again.

8
Chapter 1
FOUNDATIONS
lower bound (for a subset of a poset): an element of the poset that is less than or
equal to every element of the subset.
mathematical induction: a method of proving that every item of a sequence of propo-
sitions such as P(n0), P(n0 + 1), P(n0 + 2), . . . is true by showing: (1) P(n0) is true,
and (2) for all n ≥n0, P(n) →P(n + 1) is true.
maximal element (in a poset): an element that has no element greater than it.
maximum element (in a poset): an element greater than or equal to every element.
membership function (in fuzzy logic): a function from elements of a set to [0,1].
membership table (for a set expression): a table used to calculate whether an object
lies in the set described by the expression, based on its membership in the sets
mentioned by the expression.
minimal element (in a poset): an element that has no element smaller than it.
minimum element (in a poset): an element less than or equal to every element.
monomorphism: a one-to-one function.
multi-valued logic: a logic system with a set of more than two truth values.
multiset: an extension of the set concept, in which each element may occur arbitrarily
many times.
mutually disjoint (family of sets): See pairwise disjoint.
n-ary predicate: a statement involving n variables.
n-ary relation: any subset of A1 × A2 × · · · × An.
naive set theory: set theory where any collection of objects can be considered to be
a valid set, with paradoxes ignored.
NAND: the logical connective “not and”.
natural number: a nonnegative integer (or “counting” number); i.e., an element of
N = {0, 1, 2, 3, . . .}. Note: Sometimes 0 is not regarded as a natural number.
negation: the statement ¬p (“not p”) that is true if and only if p is not true.
NOP: pronounced “no-op”, a program instruction that does nothing to alter the values
of computational variables or the order of execution.
NOR: the logical connective “not or”.
NOT: the logical connective meaning “not”, used in place of ¬.
null set: the set with no elements, written ∅or { }.
omega notation: f is Ω(g) if there are constants C and k such that |g(x)| ≤C|f(x)|
for all x > k.
one-to-one (function): a function f : X →Y that assigns distinct elements of the co-
domain to distinct elements of the domain; thus, if x1 ̸= x2, then f(x1) ̸= f(x2).
onto (function): a function f : X →Y whose image equals its codomain; i.e., for every
y ∈Y , there is an x ∈X such that f(x) = y.
OR: the logical operator for disjunction, also written ∨.
pairwise disjoint: the property of a family of sets that each two distinct sets in the
family have empty intersection; also called mutually disjoint.
paradox: a statement that contradicts itself.

GLOSSARY
9
partial function: a function f : X →Y that assigns a well-deﬁned object in Y to some
(but not necessarily all) the elements of its domain X.
partial order: a binary relation that is reﬂexive, antisymmetric, and transitive.
partially ordered set: a set with a partial order relation deﬁned on it.
partition (of a set S): a pairwise disjoint family P = {Ai} of nonempty subsets of S
whose union is S.
Peano deﬁnition: a recursive description of the natural numbers that uses the concept
of successor.
Polish preﬁx notation: the style of writing compound propositions in preﬁx notation
where sometimes the usual operand symbols are replaced as follows: N for ¬, K
for ∧, A for ∨, C for →, E for ↔.
poset: a partially ordered set.
postcondition: an assertion that appears immediately after the executable portion of
a program fragment or of a subprogram.
postﬁx notation: the style of writing compound logical propositions where operators
are written to the right of the operands.
power (of a relation): for a relation R on A, the relation Rn on A where R0 = I,
R1 = R, and Rn = Rn−1 ◦R for all n > 1.
power set: given a set A, the set P(A) of all subsets of A.
precondition: an assertion that appears immediately before the executable portion of
a program fragment or of a subprogram.
predicate: a statement involving one or more variables that range over various domains.
predicate calculus: the symbolic study of quantiﬁed predicate statements.
preﬁx notation: the style of writing compound logical propositions where operators
are written to the left of the operands.
premise: a proposition taken as the foundation of a proof, from which the conclusion
is to be derived.
prenex normal form: the form of a well-formed formula in which every quantiﬁer
occurs at the beginning and the scope is whatever follows the quantiﬁers.
preorder: a binary relation that is reﬂexive and transitive.
primary key: for an n-ary relation on A1, A2, . . . , An, a coordinate domain Aj such
that for each x ∈Aj there is at most one n-tuple in the relation whose jth coordinate
is x.
production rule: a formula of the form C1, . . . , Cn →A1, . . . , Am where each Ci is a
condition and each Ai is an action.
production system: a set of production rules and a fact set.
program construct: See construct.
program fragment: any sequence of program code, from a single instruction to an
entire program.
program semantics (or semantics): the meaning of an instruction or of a program
fragment; i.e., the eﬀect of its execution on the computational variables.

10
Chapter 1
FOUNDATIONS
projection function: a function deﬁned on a set of n-tuples that selects the elements
in certain coordinate positions.
proof (of a conclusion from a set of premises): a sequence of statements (called steps)
terminating in the conclusion, such that each step is either a premise or follows from
previous steps by a valid argument.
proof by contradiction: a proof that assumes the negation of the statement to be
proved and shows that this leads to a contradiction.
proof done by hand: a proof done by a human without the use of a computer.
proper subset: given a set S, a subset T of S such that S contains at least one element
not in T .
proposition: a declarative sentence or statement that is unambiguously either true or
false.
propositional calculus: the symbolic study of propositions.
range (of a function): the image set of a function; sometimes used as synonym for
codomain.
rational number: the ratio a
b of two integers such that b ̸= 0; the set of all rational
numbers is denoted Q.
real number: a number expressible as a ﬁnite (i.e., terminating) or inﬁnite decimal;
the set of all real numbers is denoted R.
recursive deﬁnition (of a function with domain N): a set of initial values and a rule
for computing f(n) in terms of values f(k) for k < n.
recursive deﬁnition (of a set S): a form of speciﬁcation of membership of S, in which
some basis elements are named individually, and in which a computable rule is given
to construct each other element in a ﬁnite number of steps.
reﬁnement of a partition: given a partition P1 = {Aj} on a set S, a partition P2 =
{Bi} on the same set S such that every Bi ∈P2 is a subset of some Aj ∈P1.
reﬂexive: the property of a binary relation R that aRa.
relation (from set A to set B): a binary relation from A to B.
relation (on a set A): a binary relation from A to A.
restriction (of a function): given f : X →Y and a subset S ⊆X, the function f|S
with domain S and codomain Y whose rule is the same as that of f.
reverse Polish notation: postﬁx notation.
rule of inference: a valid argument form.
satisﬁable compound proposition: a compound proposition that is true for at least
one assignment of truth variables to its variables.
scope (of a quantiﬁer): the predicate to which the quantiﬁer applies.
semantic axiom: See axiom.
semantics: See program semantics.
sentence: a well-formed formula with no free variables.
sequence (in a set): a list of objects from a set S, with repetitions allowed; that is, a
function f : N →S (an inﬁnite sequence, often written a0, a1, a2, . . .) or a function
f : {1, 2, . . ., n} →S (a ﬁnite sequence, often written a1, a2, . . . , an).

GLOSSARY
11
set: a well-deﬁned collection of objects.
singleton: a set with one element.
speciﬁcation: in program correctness, a precondition and a postcondition.
statement form: a declarative sentence containing some variables and logical symbols
which becomes a proposition if concrete values are substituted for all free variables.
string: a ﬁnite sequence in a set S, usually written so that consecutive entries are
juxtaposed (i.e., written with no punctuation or extra space between them).
strongly correct code: code whose execution terminates in a computational state sat-
isfying the postcondition, whenever the precondition holds before execution.
subset (of a set S): any set T of objects that are also elements of S, written T ⊆S.
substitution: a set of pairs of variables and terms.
surjection or (surjective function): an onto function.
symmetric: the property of a binary relation R that if aRb then bRa.
symmetric diﬀerence (of relations): for relations R and S on A, the relation R ⊕S
where a(R ⊕S)b if and only if exactly one of the following is true: aRb, aSb.
symmetric diﬀerence (of sets): for sets A and B, the set A⊕B containing each object
that is an element of A or an element of B, but not an element of both.
system of distinct representatives: given sets A1, A2, . . . , An (some of which may
be equal), a set {a1, a2, . . . , an} of n distinct elements with ai ∈Ai for i = 1, 2, . . . , n.
tautology: a compound proposition whose form makes it always true, regardless of the
truth values of its atomic parts.
term (in a domain): either a ﬁxed element of a domain S or an S-valued variable.
theorem: a statement derived as the conclusion of a valid proof from axioms and deﬁ-
nitions.
theta notation: f is Θ(g), written f = Θ(g), if there are positive constants C1, C2,
and k such that C1|g(x)| ≤|f(x)| ≤C2|g(x)| for all x > k.
totally ordered: the property of a poset that every pair of elements are comparable;
also called linearly ordered.
transitive: the property of a binary relation R that if aRb and bRc, then aRc.
transitive closure: for a relation R on A, the smallest transitive relation containing R.
transitive reduction (of a relation): a relation with the same transitive closure as the
original relation and with a minimum number of ordered pairs.
truth table: for a compound proposition, a table that gives the truth value of the
proposition for each possible combination of truth values of the atomic variables in
the proposition.
two-valued logic: a logic system where each statement has exactly one of the two
values: true or false.
union: the set A ∪B of objects in one or both of the sets A and B.
union relation: for R and S binary relations on A, the relation R ∪S where a(R ∪S)b
if and only if aRb or aSb.
universal domain: the collection of all possible objects in the context of the immediate
discussion.

12
Chapter 1
FOUNDATIONS
universal quantiﬁer: the quantiﬁer ∀x, read “for all x” or “for every x”.
universally quantiﬁed predicate: a statement (∀x)P(x) that P(x) is true for every x
in its universe of discourse.
universe of discourse: the range of possible values of a variable, within the context
of the immediate discussion.
upper bound (for a subset of a poset): an element of the poset that is greater than or
equal to every element of the subset.
valid argument form: an argument form such that in any instantiation where all the
premises are true, the conclusion is also true.
Venn diagram: a ﬁgure composed of possibly overlapping circles or ellipses, used to
picture membership in various combinations of the sets.
veriﬁcation (of a program): a formal argument for the correctness of a program with
respect to its speciﬁcations.
weakly correct code: code whose execution results in a computational state satisfy-
ing the postcondition, whenever the precondition holds before execution and the
execution terminates.
well-formed formula (wﬀ): a proposition or predicate with quantiﬁers that bind one
or more of its variables.
well-ordered: the property of a set that every nonempty subset has a minimum ele-
ment.
well-ordering principle: the axiom that every nonempty subset of integers, each
greater than a ﬁxed integer, contains a smallest element.
XOR: the logical connective “not or”.
Zermelo-Fraenkel axioms: a set of axioms for set theory.
zero-order logic: propositional calculus.
1.1
PROPOSITIONAL AND PREDICATE LOGIC
Logic is the basis for distinguishing what may be correctly inferred from a given collec-
tion of facts. Propositional logic, where there are no quantiﬁers (so quantiﬁers range
over nothing) is called zero-order logic. Predicate logic, where quantiﬁers range over
members of a universe, is called ﬁrst-order logic. Higher-order logic includes second-
order logic (where quantiﬁers can range over relations over the universe), third-order
logic (where quantiﬁers can range over relations over relations), and so on. Logic has
many applications in computer science, including circuit design (§5.8.3) and veriﬁcation
of computer program correctness (§1.6). This section deﬁnes the meaning of the sym-
bolism and various logical properties that are usually used without explicit mention. See
[FlPa95], [Me09], and [Mo76].
Here, only two-valued logic is studied; i.e., each statement is either true or false. Multi-
valued logic, in which statements have one of more than two values, is discussed in §1.7.2.

Section 1.1
PROPOSITIONAL AND PREDICATE LOGIC
13
1.1.1
PROPOSITIONS AND LOGICAL OPERATIONS
Deﬁnitions:
A truth value is either true or false, abbreviated T and F, respectively.
A proposition (in a natural language such as English) is a declarative sentence that has
a well-deﬁned truth value.
A propositional variable is a mathematical variable, often denoted by p, q, or r, that
represents a proposition.
Propositional logic (or propositional calculus or zero-order logic) is the study of
logical propositions and their combinations using logical connectives.
A logical connective is an operation used to build more complicated logical expressions
out of simpler propositions, whose truth values depend only on the truth values of the
simpler propositions.
A proposition is atomic or simple if it cannot be syntactically analyzed into smaller
parts; it is usually represented by a single logical variable.
A proposition is compound if it contains one or more logical connectives.
A truth table is a table that prescribes the deﬁning rule for a logical operation. That
is, for each combination of truth values of the operands, the table gives the truth value
of the expression formed by the operation and operands.
The unary connective negation (denoted by ¬) is deﬁned by the following truth table:
p
¬p
T
F
F
T
Note: The negation ¬p is also written p′, p, or ∼p.
The common binary connectives are:
p ∧q
conjunction
p and q
p ∨q
disjunction
p or q
p →q
conditional
if p then q
p ↔q
biconditional
p if and only if q
p ⊕q
exclusive or
p xor q
p ↓q
not or
p nor q
p | q or p ↑q
not and
p nand q
The connective | is called the Sheﬀer stroke. The connective ↓is called the Peirce arrow.
The values of the compound propositions obtained by using the binary connectives are
given in the following table:

14
Chapter 1
FOUNDATIONS
p
q
p ∨q
p ∧q
p →q
p ↔q
p ⊕q
p ↓q
p | q
T
T
T
T
T
T
F
F
F
T
F
T
F
F
F
T
F
T
F
T
T
F
T
F
T
F
T
F
F
F
F
T
T
F
T
T
In the conditional p →q, p is the antecedent and q is the consequent. The conditional
p →q is often read informally as “p implies q”.
Inﬁx notation is the style of writing compound propositions where binary operators
are written between the operands and negation is written to the left of its operand.
Preﬁx notation is the style of writing compound propositions where operators are
written to the left of the operands.
Postﬁx notation (or reverse Polish notation) is the style of writing compound
propositions where operators are written to the right of the operands.
Polish notation is the style of writing compound propositions where operators are writ-
ten using preﬁx notation and where the usual operand symbols are replaced as follows:
N for ¬, K for ∧, A for ∨, C for →, E for ↔. (Jan  Lukasiewicz, 1878–1956)
A fully parenthesized proposition is any proposition that can be obtained using the
following recursive deﬁnition: (1) each variable is fully parenthesized; (2) if P and Q are
fully parenthesized, so are (¬P), (P ∧Q), (P ∨Q), (P →Q), and (P ↔Q).
Facts:
1. The conditional connective p →q represents the following English constructs:
• if p then q
• q if p
• p only if q
• p implies q
• q follows from p
• q whenever p
• p is a suﬃcient condition for q
• q is a necessary condition for p.
2. The biconditional connective p ↔q represents the following English constructs:
• p if and only if q (often written p iﬀq)
• p and q imply each other
• p is a necessary and suﬃcient condition for q
• p and q are equivalent.
3. In computer programming and circuit design, the following notation for logical op-
erators is used: p AND q for p ∧q, p OR q for p ∨q, NOT p for ¬p, p XOR q for p ⊕q,
p NOR q for p ↓q, p NAND q for p | q.
4. Order of operations:
In an unparenthesized compound proposition using only the
ﬁve standard operators ¬, ∧, ∨, →, and ↔, the following order of precedence is typically
used when evaluating a logical expression, at each level of precedence moving from left to
right: ﬁrst ¬, then ∧and ∨, then →, ﬁnally ↔. Parenthesized expressions are evaluated
proceeding from the innermost pair of parentheses outward, analogous to the evaluation
of an arithmetic expression.
5. It is often preferable to use parentheses to show precedence, except for negation
operators, rather than to rely on precedence rules.

Section 1.1
PROPOSITIONAL AND PREDICATE LOGIC
15
6. No parentheses are needed when a compound proposition is written in either preﬁx or
postﬁx notation. However, parentheses may be necessary when a compound proposition
is written in inﬁx notation.
7. The number of nonequivalent logical statements with two variables is 16, because
each of the four lines of the truth table has two possible entries, T or F.
Here are
examples of compound propositions that yield each possible combination of truth values.
(T represents a tautology and F a contradiction. See §1.1.2.)
p
q
T
p ∨q
q →p
p →q
p | q
p
q
p ↔q
T
T
T
T
T
T
F
T
T
T
T
F
T
T
T
F
T
T
F
F
F
T
T
T
F
T
T
F
T
F
F
F
T
F
T
T
T
F
F
T
p
q
p ⊕q
¬q
¬p
p ∧q
p ∧¬q
¬p∧
p ↓q
F
T
T
F
F
F
T
F
F
F
F
T
F
T
T
F
F
T
F
F
F
F
T
T
F
T
F
F
T
F
F
F
F
F
T
T
F
F
F
T
F
8. The number of diﬀerent possible logical connectives on n variables is 22n, because
there are 2n rows in the truth table.
9. The problem of determining whether a compound proposition is satisﬁable, known
as the Propositional Satisﬁability Problem (abbreviated as SAT), is important in many
practical applications, such as in circuit design and in artiﬁcial intelligence, and it is also
important in the study of algorithms.
10. No eﬃcient (polynomial-time) algorithm has been found for solving SAT. Because
it is NP-complete (see Section 16.4.1), if a polynomial-time algorithm could be found for
solving this problem, the famous P versus NP problem would be solved in the aﬃrmative.
Examples:
1. “1+1 = 3” and “Romulus and Remus founded New York City” are false propositions.
2. “1 + 1 = 2” and “The year 1996 was a leap year” are true propositions.
3. “Go directly to jail” is not a proposition, because it is imperative, not declarative.
4. “x > 5” is not a proposition, because its truth value cannot be determined unless the
value of x is known.
5. “This sentence is false” is not a proposition, because it cannot be given a truth value
without creating a contradiction.
6. In a truth table evaluation of the compound proposition p∨(¬p∧q) from the innermost
parenthetic expression outward, the steps are to evaluate ¬p, next (¬p ∧q), and then
p ∨(¬p ∧q):
p
q
¬p
(¬p ∧q)
p ∨(¬p ∧q)
T
T
F
F
T
T
F
F
F
T
F
T
T
T
T
F
F
T
F
F

16
Chapter 1
FOUNDATIONS
7. The statements in the left column are evaluated using the order of precedence indi-
cated in the fully parenthesized form in the right column:
p ∨q ∧r
((p ∨q) ∧r)
p ↔q →r
(p ↔(q →r))
¬q ∨¬r →s ∧t
(((¬q) ∨(¬r)) →(s ∧t))
8. The inﬁx statement p ∧q in preﬁx notation is ∧p q, in postﬁx notation is p q ∧, and
in Polish notation is K p q.
9. The inﬁx statement p →¬(q ∨r) in preﬁx notation is →p ¬ ∨q r, in postﬁx notation
is p q r ∨¬ →, and in Polish notation is C p N A q r.
1.1.2
EQUIVALENCES, IDENTITIES, AND NORMAL FORMS
Deﬁnitions:
A tautology is a compound proposition that is always true, regardless of the truth
values of its underlying atomic propositions.
A contradiction (or self-contradiction) is a compound proposition that is always
false, regardless of the truth values of its underlying atomic propositions. (The term
self-contradiction is used for such a proposition when discussing indirect mathematical
arguments, because “contradiction” has another meaning in that context. See §1.5.)
A contingency is a compound proposition that is neither a tautology nor a contradic-
tion.
A compound proposition is satisﬁable if there is at least one assignment of truth values
for which it is true.
A compound proposition P logically implies a compound proposition Q, written P ⇒
Q, if Q is true whenever P is true. In this case, P is stronger than Q, and Q is weaker
than P.
Compound propositions P and Q are logically equivalent, written P ≡Q, P ⇔Q, or
P iﬀQ, if they have the same truth values for all possible truth values of their variables.
A logical equivalence that is frequently used is sometimes called a logical identity.
A collection C of connectives is functionally complete if every compound proposition
is equivalent to a compound proposition constructed using only connectives in C.
A disjunctive normal expression in the propositions p1, p2, . . . , pn is a disjunction of
one or more propositions, each of the form xk1 ∧xk2 ∧· · · ∧xkm, where xkj is either pkj
or ¬pkj.
A disjunctive normal form (DNF) for a proposition P is a disjunctive normal ex-
pression that is logically equivalent to P.
A conjunctive normal expression in the propositions p1, p2, . . . , pn is a conjunction
of one or more compound propositions, each of the form xk1 ∨xk2 ∨· · · ∨xkm, where xkj
is either pkj or ¬pkj.
A conjunctive normal form (CNF) for a proposition P is a conjunctive normal ex-
pression that is logically equivalent to P.

Section 1.1
PROPOSITIONAL AND PREDICATE LOGIC
17
A compound proposition P using only the connectives ¬, ∧, and ∨has a logical dual
(denoted P ′ or P d), obtained by interchanging ∧and ∨, and interchanging the constant
T (true) and the constant F (false).
The converse of the conditional proposition p →q is the proposition q →p.
The contrapositive of the conditional proposition p →q is the proposition ¬q →¬p.
The inverse of the conditional proposition p →q is the proposition ¬p →¬q.
Facts:
1. P ⇔Q is true if and only if P ⇒Q and Q ⇒P.
2. P ⇔Q is true if and only if P ↔Q is a tautology.
3. The following table lists several logical identities.
name
rule
Commutative laws
p ∧q ⇔q ∧p,
p ∨q ⇔q ∨p
Associative laws
p ∧(q ∧r) ⇔(p ∧q) ∧r,
p ∨(q ∨r) ⇔(p ∨q) ∨r
Distributive laws
p ∧(q ∨r) ⇔(p ∧q) ∨(p ∧r)
p ∨(q ∧r) ⇔(p ∨q) ∧(p ∨r)
DeMorgan’s laws
¬(p ∧q) ⇔(¬p) ∨(¬q),
¬(p ∨q) ⇔(¬p) ∧(¬q)
Excluded middle
p ∨¬p ⇔T
Contradiction
p ∧¬p ⇔F
Double negation law
¬(¬p) ⇔p
Contrapositive law
p →q ⇔¬q →¬p
Conditional as disjunction
p →q ⇔¬p ∨q
Negation of conditional
¬(p →q) ⇔p ∧¬q
Biconditional as implication
(p ↔q) ⇔(p →q) ∧(q →p)
Idempotent laws
p ∧p ⇔p,
p ∨p ⇔p
Absorption laws
p ∧(p ∨q) ⇔p,
p ∨(p ∧q) ⇔p
Dominance laws
p ∨T ⇔T,
p ∧F ⇔F
Exportation law
p →(q →r) ⇔(p ∧q) →r
Identity laws
p ∧T ⇔p,
p ∨F ⇔p
4. There are diﬀerent ways to establish logical identities (equivalences):
• truth tables (showing that both expressions have the same truth values);
• using known logical identities and equivalences to establish new ones;
• taking the dual of a known identity (Fact 7).
5. Logical identities are used in circuit design to simplify circuits. See §5.8.4.
6. Each of the following sets of connectives is functionally complete:
{∧, ∨, ¬},
{∧, ¬},
{∨, ¬},
{ | },
{ ↓}.
However, these sets of connectives are not functionally complete:
{∧},
{∨},
{∧, ∨}.
7. If P ⇔Q is a logical identity, then so is P ′ ⇔Q′, where P ′ and Q′ are the duals
of P and Q, respectively.

18
Chapter 1
FOUNDATIONS
8. Every proposition has a disjunctive normal form and a conjunctive normal form,
which can be obtained by Algorithms 1 and 2.
Algorithm 1:
Disjunctive normal form of proposition P .
write the truth table for P
for each line of the truth table on which P is true, form a “line term”
x1 ∧x2 ∧· · · ∧xn, where xi := pi if pi is true on that line of the truth table
and xi := ¬pi if pi is false on that line
form the disjunction of all these line terms
Algorithm 2:
Conjunctive normal form of proposition P .
write the truth table for P
for each line of the truth table on which P is false, form a “line term”
x1 ∨x2 ∨· · · ∨xn, where xi := pi if pi is false on that line of the truth table
and xi := ¬pi if pi is true on that line
form the conjunction of all these line terms
Examples:
1. The proposition p ∨¬p is a tautology (the law of the excluded middle).
2. The proposition p ∨¬p is a self-contradiction.
3. The proposition (p ∨¬q) ∧(q ∨¬r) ∧(r ∨¬p) is satisﬁable because it is true when
p, q, and r are all false. Note, however, that (p ↔q) ∧(¬p ↔q) is unsatisﬁable, as it is
false for each of the four possible assignments of truth values for p and q.
4. A proof that p ↔q is logically equivalent to (p ∧q) ∨(¬p ∧¬q) can be carried out
using a truth table:
p
q
p ↔q
¬p
¬q
p ∧q
¬p ∧¬q
(p ∧q) ∨(¬p ∧¬q)
T
T
T
F
F
T
F
T
T
F
F
F
T
F
F
F
F
T
F
T
F
F
F
F
F
F
T
T
T
F
T
T
Since the third and eighth columns of the truth table are identical, the two statements
are equivalent.
5. A proof that p ↔q is logically equivalent to (p ∧q) ∨(¬p ∧¬q) can be given by a
series of logical equivalences. Reasons are given at the right.
p ↔q
⇔(p →q) ∧(q →p)
biconditional as implication
⇔(¬p ∨q) ∧(¬q ∨p)
conditional as disjunction
⇔[(¬p ∨q) ∧¬q] ∨[(¬p ∨q) ∧p]
distributive law
⇔[(¬p ∧¬q) ∨(q ∧¬q)] ∨[(¬p ∧p) ∨(q ∧p)]
distributive law
⇔[(¬p ∧¬q) ∨F] ∨[F ∨(q ∧p)]
contradiction
⇔[(¬p ∧¬q) ∨F] ∨[(q ∧p) ∨F]
commutative law
⇔(¬p ∧¬q) ∨(q ∧p)
identity law
⇔(¬p ∧¬q) ∨(p ∧q)
commutative law
⇔(p ∧q) ∨(¬p ∧¬q)
commutative law

Section 1.1
PROPOSITIONAL AND PREDICATE LOGIC
19
6. The proposition p ↓q is logically equivalent to ¬(p ∨q). Its DNF is ¬p ∧¬q, and its
CNF is (¬p ∨¬q) ∧(¬p ∨q) ∧(p ∨¬q).
7. The proposition p|q is logically equivalent to ¬(p ∧q). Its DNF is (p ∧¬q) ∨(¬p ∧
q) ∨(¬p ∧¬q), and its CNF is ¬p ∨¬q.
8. The DNF and CNF for Examples 6 and 7 were obtained by using Algorithm 1 and
Algorithm 2 to construct the following table of terms:
p
q
p ↓q
DNF terms
CNF terms
T
T
F
¬p ∨¬q
T
F
F
¬p ∨q
F
T
F
p ∨¬q
F
F
T
¬p ∧¬q
p
q
p | q
DNF terms
CNF terms
T
T
F
¬p ∨¬q
T
F
T
p ∧¬q
F
T
T
¬p ∧q
F
F
T
¬p ∧¬q
9. The dual of p ∧(q ∨¬r) is p ∨(q ∧¬r).
10. Let S be the proposition in three propositional variables p, q, and r that is true
when precisely two of the variables are true. Then the disjunctive normal form for S is
(p ∧q ∧¬r) ∨(p ∧¬q ∧r) ∨(¬p ∧q ∧r)
and the conjunctive normal form for S is
(¬p ∨¬q ∨¬r) ∧(¬p ∨q ∨r) ∧(p ∨¬q ∨r) ∧(p ∨q ∨¬r) ∧(p ∨q ∨r).
1.1.3
PREDICATE LOGIC
Deﬁnitions:
A predicate is a declarative statement with the symbolic form P(x) or P(x1, . . . , xn)
about one or more variables x or x1, . . . , xn whose values are unspeciﬁed.
Predicate logic (or predicate calculus or ﬁrst-order logic) is the study of state-
ments whose variables have quantiﬁers.
The universe of discourse (or universe or domain) of a variable is the set of possible
values of the variable in a predicate.
An instantiation of the predicate P(x) is the result of substituting a ﬁxed constant
value c from the domain of x for each free occurrence of x in P(x). This is denoted by
P(c).
The existential quantiﬁcation of a predicate P(x) whose variable ranges over a domain
set D is the proposition (∃x ∈D)P(x) or (∃x)P(x) that is true if there is at least one c
in D such that P(c) is true. The existential quantiﬁer symbol ∃is read “there exists” or
“there is”.

20
Chapter 1
FOUNDATIONS
The universal quantiﬁcation of a predicate P(x) whose variable ranges over a domain
set D is the proposition (∀x ∈D)P(x) or (∀x)P(x), which is true if P(c) is true for every
element c in D. The universal quantiﬁer symbol ∀is read “for all”, “for each”, or “for
every”.
The unique existential quantiﬁcation of a predicate P(x) whose variable ranges over
a domain set D is the proposition (∃!x)P(x) that is true if P(c) is true for exactly one c
in D. The unique existential quantiﬁer symbol ∃! is read “there is exactly one”.
The scope of a quantiﬁer is the predicate to which it applies.
A variable x in a predicate P(x) is a bound variable if it lies inside the scope of an
x-quantiﬁer. Otherwise it is a free variable.
A well-formed formula (wﬀ) (or statement) is either a proposition or a predicate
with quantiﬁers that bind one or more of its variables.
A sentence (closed wﬀ) is a well-formed formula with no free variables.
A well-formed formula is in prenex normal form if all the quantiﬁers occur at the
beginning and the scope is whatever follows the quantiﬁers.
A well-formed formula is atomic if it does not contain any logical connectives; otherwise
the well-formed formula is compound.
Higher-order logic is the study of statements that allow quantiﬁers to range over
relations over a universe (second-order logic), relations over relations over a universe
(third-order logic), etc.
Facts:
1. If a predicate P(x) is atomic, then the scope of (∀x) in (∀x)P(x) is implicitly the
entire predicate P(x).
2. If a predicate is a compound form, such as P(x)∧Q(x), then (∀x)[P(x)∧Q(x)] means
that the scope is P(x) ∧Q(x), whereas (∀x)P(x) ∧Q(x) means that the scope is only
P(x), in which case the free variable x of the predicate Q(x) has no relationship to the
variable x of P(x).
3. Universal statements in predicate logic are analogues of conjunctions in propositional
logic. If variable x has domain D = {x1, . . . , xn}, then (∀x ∈D)P(x) is true if and only
if P(x1) ∧· · · ∧P(xn) is true.
4. Existential statements in predicate logic are analogues of disjunctions in propositional
logic. If variable x has domain D = {x1, . . . , xn}, then (∃x ∈D)P(x) is true if and only
if P(x1) ∨· · · ∨P(xn) is true.
5. Adjacent universal quantiﬁers [existential quantiﬁers] can be transposed without
changing the meaning of a logical statement:
(∀x)(∀y)P(x, y) ⇔(∀y)(∀x)P(x, y)
(∃x)(∃y)P(x, y) ⇔(∃y)(∃x)P(x, y)
6. Transposing adjacent logical quantiﬁers of diﬀerent types can change the meaning of
a statement. (See Example 4.)
7. Rules for negations of quantiﬁed statements:
¬(∀x)P(x)
⇔(∃x)[¬P(x)]
¬(∃x)P(x)
⇔(∀x)[¬P(x)]
¬(∃!x)P(x)
⇔¬(∃x)P(x) ∨(∃y)(∃z)[(y ̸= z) ∧P(y) ∧P(z)].

Section 1.1
PROPOSITIONAL AND PREDICATE LOGIC
21
8. Every quantiﬁed statement is logically equivalent to some statement in prenex normal
form.
9. Every statement with a unique existential quantiﬁer is equivalent to a statement that
uses only existential and universal quantiﬁers, according to the rule:
(∃!x)P(x) ⇔(∃x)

P(x) ∧(∀y)[P(y) →(x = y)]

where P(y) means that y has been substituted for all free occurrences of x in P(x), and
where y is a variable that does not occur in P(x).
10. If a statement uses only the connectives ∨, ∧, and ¬, the following equivalences
can be used along with Fact 7 to convert the statement into prenex normal form. The
letter A represents a wﬀwithout the variable x.
(∀x)P(x) ∧(∀x)Q(x)
⇔(∀x)[P(x) ∧Q(x)]
(∀x)P(x) ∨(∀x)Q(x)
⇔(∀x)(∀y)[P(x) ∨Q(y)]
(∃x)P(x) ∧(∃x)Q(x)
⇔(∃x)(∃y)[P(x) ∧Q(y)]
(∃x)P(x) ∨(∃x)Q(x)
⇔(∃x)[P(x) ∨Q(x)]
(∀x)P(x) ∧(∃x)Q(x)
⇔(∀x)(∃y)[P(x) ∧Q(y)]
(∀x)P(x) ∨(∃x)Q(x)
⇔(∀x)(∃y)[P(x) ∨Q(y)]
A ∨(∀x)P(x)
⇔(∀x)[A ∨P(x)]
A ∨(∃x)P(x)
⇔(∃x)[A ∨P(x)]
A ∧(∀x)P(x)
⇔(∀x)[A ∧P(x)]
A ∧(∃x)P(x)
⇔(∃x)[A ∧P(x)].
Examples:
1. The statement (∀x ∈R)(∀y ∈R) [x+y = y +x] is syntactically a predicate preceded
by two universal quantiﬁers.
It asserts the commutative law for the addition of real
numbers.
2. The statement (∀x)(∃y) [xy = 1] expresses the existence of multiplicative inverses for
all numbers in whatever domain is under discussion. Thus, it is true for the positive
real numbers, but it is false when the domain is the entire set of reals, since zero has no
multiplicative inverse.
3. The statement (∀x ̸= 0)(∃y) [xy = 1] asserts the existence of multiplicative inverses
for nonzero numbers.
4. (∀x)(∃y) [x + y = 0] expresses the true proposition that every real number has an
additive inverse, but (∃y)(∀x) [x+y = 0] is the false proposition that there is a “universal
additive inverse” that when added to any number always yields the sum 0.
5. In the statement (∀x ∈R) [x + y = y + x], the variable x is bound and the variable y
is free.
6. “Not all men are mortal” is equivalent to “there exists at least one man who is not
mortal”. Also, “there does not exist a cow that is blue” is equivalent to the statement
“every cow is a color other than blue”.
7. The statement (∀x) P(x) →(∀x) Q(x) is not in prenex form. An equivalent prenex
form is (∀x)(∃y) [P(y) →Q(x)].
8. The following table illustrates the diﬀerences in meaning among the four diﬀerent
ways to quantify a predicate with two variables:

22
Chapter 1
FOUNDATIONS
statement
meaning
(∃x)(∃y) [x + y = 0]
There is a pair of numbers whose sum is zero.
(∀x)(∃y) [x + y = 0]
Every number has an additive inverse.
(∃x)(∀y) [x + y = 0]
There is a universal additive inverse x.
(∀x)(∀y) [x + y = 0]
The sum of every pair of numbers is zero.
9. The statement (∀x)(∃!y) [x + y = 0] asserts the existence of unique additive inverses.
1.2
SET THEORY
Sets are used to group objects and to serve as the basic elements for building more
complicated objects and structures. Counting elements in sets is an important part of
discrete mathematics. General reference books that cover the material of this section are
[Cu16], [FlPa95], [Ha11], and [Ka10].
1.2.1
SETS
Deﬁnitions:
A set is any well-deﬁned collection of objects, each of which is called a member or an
element of the set. The notation x ∈A means that the object x is a member of the set
A. The notation x /∈A means that x is not a member of A.
A roster for a ﬁnite set speciﬁes the membership of a set S as a list of its elements
within braces, i.e., in the form S = {a1, . . . , an}. Order of the list is irrelevant, as is the
number of occurrences of an object in the list.
A deﬁning predicate speciﬁes a set in the form S = {x | P(x)}, where P(x) is a
predicate containing the free variable x. This means that S is the set of all objects x (in
whatever domain is under discussion) such that P(x) is true.
A recursive description of a set S gives a roster B of basic objects of S and a set
of operations for constructing additional objects of S from objects already known to be
in S. That is, any object that can be constructed by a ﬁnite sequence of applications of
the given operations to objects in B is also a member of S. There may also be a list of
axioms that specify when two sequences of operations yield the same result.
The set with no elements is called the null set or the empty set, denoted ∅or { }.
A singleton is a set with one element.
The set N of natural numbers is the set {0, 1, 2, . . .}. (Sometimes 0 is excluded from
the set of natural numbers; when the set of natural numbers is encountered, check to see
how it is being deﬁned.)
The set Z of integers is the set {. . . , −2, −1, 0, 1, 2, . . .}.
The set Q of rational numbers is the set of all fractions a
b , where a is any integer and b
is any nonzero integer.

Section 1.2
SET THEORY
23
The set R of real numbers is the set of all numbers that can be written as terminating
or nonterminating decimals.
The set C of complex numbers is the set of all numbers of the form a + bi, where
a, b ∈R and i = √−1 (i2 = −1).
Sets A and B are equal, written A = B, if they have exactly the same elements:
A = B ⇔(∀x)

(x ∈A) ↔(x ∈B)

.
Set B is a subset of set A, written B ⊆A or A ⊇B, if each element of B is an element
of A:
B ⊆A ⇔(∀x)

(x ∈B) →(x ∈A)

.
Set B is a proper subset of A if B is a subset of A and A contains at least one element
not in B. (The notation B ⊂A is often used to indicate that B is a proper subset
of A, but sometimes it is used to mean an arbitrary subset. Sometimes the proper subset
relationship is written B ⊂
̸=A, to avoid all possible notational ambiguity.)
A set is ﬁnite if it is either empty or else can be put in a one-to-one correspondence with
the set {1, 2, 3, . . ., n} for some positive integer n.
A set is inﬁnite if it is not ﬁnite.
The cardinality |S| of a ﬁnite set S is the number of elements in S.
A multiset is an unordered collection in which elements can occur arbitrarily often, not
just once. The number of occurrences of an element is called its multiplicity.
An axiom (postulate) is a statement that is assumed to be true.
A set of axioms is consistent if no contradiction can be deduced from the axioms.
A set of axioms is complete if it is possible to prove all true statements.
A set of axioms is independent if none of the axioms can be deduced from the other
axioms.
A set paradox is a question in the language of set theory that seems to have no unam-
biguous answer.
Naive set theory is set theory where any collection of objects can be considered to be
a valid set, with paradoxes ignored.
Facts:
1. The theory of sets was ﬁrst developed by Georg Cantor (1845–1918).
2. A = B if and only if A ⊆B and B ⊆A.
3. N ⊂Z ⊂Q ⊂R ⊂C.
4. Every rational number can be written as a decimal that is either terminating or else
repeating (i.e., the same block repeats end-to-end forever).
5. Real numbers can be represented as the points on the number line, and include all
rational numbers and all irrational numbers (such as
√
2, π, e, etc.).
6. There is no set of axioms for set theory that is both complete and consistent.
7. Naive set theory ignores paradoxes.
To avoid such paradoxes, more axioms are
needed.

24
Chapter 1
FOUNDATIONS
Examples:
1. The set {x ∈N | 3 ≤x < 10}, described by the deﬁning predicate 3 ≤x < 10, is
equal to the set {3, 4, 5, 6, 7, 8, 9}, which is described by a roster.
2. If A is the set with two objects, one being the number 5 and the other being the
set whose elements are the letters x, y, and z, then A = {5, {x, y, z}}. In this example,
5 ∈A, but x /∈A, since x is not either member of A.
3. The set E of even natural numbers can be described recursively as follows:
Basic objects: 0 ∈E,
Recursion rule: if n ∈E, then n + 2 ∈E.
4. The liar’s paradox: A person says “I am lying”. Is the person lying or is the person
telling the truth? If the person is lying, then “I am lying” is false, and hence the person
is telling the truth. If the person is telling the truth, then “I am lying” is true, and the
person is lying. This is also called the paradox of Epimenides. This paradox also results
from considering the statement “This statement is false”.
5. The barber’s paradox:
In a small village populated only by men there is exactly
one barber. The villagers follow the following rule: the barber shaves a man if and only
if the man does not shave himself. Question: does the barber shave himself? If “yes”
(i.e., the barber shaves himself), then according to the rule he does not shave himself. If
“no” (i.e., the barber does not shave himself), then according to the rule he does shave
himself. This paradox illustrates a danger in describing sets by deﬁning predicates.
6. Russell’s paradox:
This paradox, named for the British logician Bertrand Russell
(1872–1970), shows that the “set of all sets” is an ill-deﬁned concept. If it really were a
set, then it would be an example of a set that is a member of itself. Thus, some “sets”
would contain themselves as elements and others would not. Let S be the “set” of “sets
that are not elements of themselves”; i.e., S = {A | A /∈A}. Question: is S a member
of itself? If “yes”, then S is not a member of itself, because of the deﬁning membership
criterion. If “no”, then S is a member of itself, due to the deﬁning membership criterion.
One resolution is that the collection of all sets is not a set. (See Chapter 4 of [MiRo91].)
7. Paradoxes such as those in Example 6 led Alfred North Whitehead (1861–1947) and
Bertrand Russell to develop a version of set theory by categorizing sets based on set
types: T0, T1, . . . . The lowest type T0 consists only of individual elements. For i > 0,
type Ti consists of sets whose elements come from type Ti−1. This forces sets to belong
to exactly one type. The expression A ∈A is always false. In this situation Russell’s
paradox cannot happen.
1.2.2
SET OPERATIONS
Deﬁnitions:
The intersection of sets A and B is the set A ∩B = {x | (x ∈A) ∧(x ∈B)}. More
generally, the intersection of any family of sets is the set of objects that are members of
every set in the family. The notation
\
i∈I
Ai = {x | x ∈Ai for all i ∈I}
is used for the intersection of the family of sets Ai indexed by the set I.
Two sets A and B are disjoint if A ∩B = ∅.

Section 1.2
SET THEORY
25
A collection of sets {ai | i ∈I} is disjoint if T
i∈I Ai = ∅.
A collection of sets is pairwise disjoint (or mutually disjoint) if every pair of sets in
the collection are disjoint.
The union of sets A and B is the set A ∪B = {x | (x ∈A) ∨(x ∈B)}. More generally,
the union of a family of sets is the set of objects that are members of at least one set in
the family. The notation
[
i∈I
Ai = {x | x ∈Ai for some i ∈I}
is used for the union of the family of sets Ai indexed by the set I.
A partition of a set S is a pairwise disjoint family P = {Ai} of nonempty subsets whose
union is S.
The partition P2 = {Bi} of a set S is a reﬁnement of the partition P1 = {Aj} of the
same set if for every subset Bi ∈P2 there is a subset Aj ∈P1 such that Bi ⊆Aj.
The complement of the set A is the set A = U −A = {x | x /∈A} containing every
object not in A, where the context provides that the objects range over some speciﬁc
universal domain U. (The notation A′ or Ac is sometimes used instead of A.)
The set diﬀerence is the set A −B = A ∩B = {x | (x ∈A) ∧(x /∈B)}. The set
diﬀerence is sometimes written A \ B.
The symmetric diﬀerence of A and B is the set A ⊕B = {x | (x ∈A −B) ∨(x ∈
B −A)}. This is sometimes written A△B.
The Cartesian product A×B of two sets A and B is the set {(a, b) | (a ∈A) ∧(b ∈B)},
which contains all ordered pairs whose ﬁrst coordinate is from A and whose second
coordinate is from B. The Cartesian product of A1, . . . , An is the set A1×A2×· · ·×An =
Qn
i=1 Ai = {(a1, a2, . . . , an) | (∀i)(ai ∈Ai)}, which contains all ordered n-tuples whose
ith coordinate is from Ai. The Cartesian product A × A × · · · × A is also written An. If
S is any set, the Cartesian product of the collection of sets As, where s ∈S, is the set
Q
s∈S As of all functions f : S →S
s∈S As such that f(s) ∈As for all s ∈S.
The power set of A is the set P(A) of all subsets of A. The alternative notation 2A
for P(A) emphasizes the fact that the power set has 2n elements if A has n elements.
A set expression is any expression built up from sets and set operations.
A set equation (or set identity) is an equation whose left side and right side are both
set expressions.
A system of distinct representatives (SDR) for a collection of sets A1, A2, . . . , An
(some of which may be equal) is a set {a1, a2, . . . , an} of n distinct elements such that
ai ∈Ai for i = 1, 2, . . ., n.
A Venn diagram is a family of n simple closed curves (typically circles or ellipses)
arranged in the plane so that all possible intersections of the interiors are nonempty and
connected. (John Venn, 1834–1923.)
A Venn diagram is simple if at most two curves intersect at any point of the plane.
A Venn diagram is reducible if there is a sequence of curves whose iterative removal
leaves a Venn diagram at each step.

26
Chapter 1
FOUNDATIONS
A membership table is a table used to calculate whether an object lies in the set
described by a set expression, based on its membership in the sets mentioned by the
expression.
Facts:
1. If a collection of sets is pairwise disjoint, then the collection is disjoint. The converse
is false.
2. The following ﬁgure illustrates Venn diagrams for two and three sets.
A
B
A
B
C
3. The following ﬁgure gives the Venn diagrams for sets constructed using various set
operations.
A
B
A
B
U
A
A
B
A
B
C
(A ∩ B) - C
A - B
(A ∪ B)
A ∩ B
A
4. Intuition regarding set identities can be gleaned from Venn diagrams, but it can be
misleading to use Venn diagrams when proving theorems unless great care is taken to
make sure that the diagrams are suﬃciently general to illustrate all possible cases.
5. Venn diagrams are often used as an aid to inclusion/exclusion counting. (See §2.4.)
6. Venn gave examples of Venn diagrams with four ellipses and asserted that no Venn
diagram could be constructed with ﬁve ellipses.
7. Peter Hamburger and Raymond Pippert (1996) constructed a simple, reducible Venn
diagram with ﬁve congruent ellipses. (Two ellipses are congruent if they are the exact
same size and shape, and diﬀer only by their placement in the plane.)
8. Many of the logical identities given in §1.1.2 correspond to set identities, given in the
following table.
name
rule
Commutative laws
A ∩B = B ∩A,
A ∪B = B ∪A
Associative laws
A ∩(B ∩C) = (A ∩B) ∩C
A ∪(B ∪C) = (A ∪B) ∪C
Distributive laws
A ∩(B ∪C) = (A ∩B) ∪(A ∩C)
A ∪(B ∩C) = (A ∪B) ∩(A ∪C)
DeMorgan’s laws
A ∩B = A ∪B,
A ∪B = A ∩B
Complement laws
A ∩A = ∅,
A ∪A = U
Double complement law
A = A
Idempotent laws
A ∩A = A,
A ∪A = A
Absorption laws
A ∩(A ∪B) = A,
A ∪(A ∩B) = A
Dominance laws
A ∩∅= ∅,
A ∪U = U
Identity laws
A ∪∅= A,
A ∩U = A

Section 1.2
SET THEORY
27
9. In a computer, a subset of a relatively small universal domain can be represented by
a bit string. Each bit location corresponds to a speciﬁc object of the universal domain,
and the bit value indicates the presence (1) or absence (0) of that object in the subset.
10. In a computer, a subset of a relatively large ordered datatype or universal domain
can be represented by a binary search tree. (See §18.2.3.)
11. For any two ﬁnite sets A and B, |A ∪B| = |A| + |B| −|A ∩B| (inclusion/exclusion
principle). (See §2.4.)
12. Set identities can be proved by any of the following approaches:
• a containment proof: show the left side is a subset of the right side, and the right
side is a subset of the left side;
• a membership table: construct the analogue of the truth table for each side of the
equation;
• using other set identities.
13. For all sets A, |A| < |P(A)|.
14. Hall’s theorem:
A collection of sets A1, A2, . . . , An has a system of distinct repre-
sentatives if and only if for all k = 1, . . . , n every collection of k subsets Ai1, Ai2, . . . , Aik
satisﬁes |Aii ∪Ai2 ∪· · · ∪Aik| ≥k.
15. If a collection of sets A1, A2, . . . , An has a system of distinct representatives and if
an integer m has the property that |Ai| ≥m for each i, then
• if m ≥n there are at least
m!
(m−n)! systems of distinct representatives;
• if m < n there are at least m! systems of distinct representatives.
16. Systems of distinct representatives can be phrased in terms of 0-1 matrices and
graphs. See §6.6, §8.12.2, and §10.4.3.
Examples:
1. {1, 2} ∩{2, 3} = {2}.
2. The collection of sets {1, 2}, {4, 5}, {6, 7, 8} is pairwise disjoint, and hence disjoint.
3. The collection of sets {1, 2}, {2, 3}, {1, 3} is disjoint, but not pairwise disjoint.
4. {1, 2} ∪{2, 3} = {1, 2, 3}.
5. Suppose that for every positive integer n, [j mod n] = {k ∈Z | k mod n = j}, for
j = 0, 1, . . . , n −1. (See §1.3.1.) Then {[0 mod 3], [1 mod 3], [2 mod 3]} is a partition
of the integers. Moreover, {[0 mod 6], [1 mod 6], . . . , [5 mod 6]} is a reﬁnement of this
partition.
6. Within the context of Z as universal domain, the complement of the set of positive
integers is the set consisting of the negative integers and 0.
7. {1, 2} −{2, 3} = {1}.
8. {1, 2} × {2, 3} = {(1, 2), (1, 3), (2, 2), (2, 3)}.
9. P({1, 2}) = {∅, {1}, {2}, {1, 2}}.
10. If L is a line in the plane, and if for each x ∈L, Cx is the circle of radius 1 centered
at point x, then S
x∈L Cx is an inﬁnite strip of width 2, and T
x∈L Cx = ∅.
11. The ﬁve-fold Cartesian product {0, 1}5 contains 32 diﬀerent 5-tuples, including, for
instance, (0, 0, 1, 0, 1).

28
Chapter 1
FOUNDATIONS
12. The set identity A ∩B = A∪B is veriﬁed by the following membership table. Begin
by listing the possibilities for elements being in or not being in the sets A and B, using
1 to mean “is an element of” and 0 to mean “is not an element of”. Proceed to ﬁnd the
element values for each combination of sets. The two sides of the equation are the same
since the columns for A ∩B and A ∪B are identical:
A
B
A ∩B
A ∩B
A
B
A ∪B
1
1
1
0
0
0
0
1
0
0
1
0
1
1
0
1
0
1
1
0
1
0
0
0
1
1
1
1
13. The collection of sets A1 = {1, 2}, A2 = {2, 3}, A3 = {1, 3, 4} has systems of distinct
representatives, for example {1, 2, 3} and {2, 3, 4}.
14. The collection of sets A1 = {1, 2}, A2 = {1, 3}, A3 = {2, 3}, A4 = {1, 2, 3}, A5 =
{2, 3, 4} does not have a system of distinct representatives since |A1 ∪A2 ∪A3 ∪A4| < 4.
1.2.3
INFINITE SETS
Deﬁnitions:
The Peano deﬁnition for the natural numbers N:
• 0 is a natural number;
• every natural number n has a successor s(n);
• axioms:
⋄0 is not the successor of any natural number;
⋄two diﬀerent natural numbers cannot have the same successor;
⋄if 0 ∈T and if (∀n ∈N)

(n ∈T ) →(s(n) ∈T )

, then T = N.
(This axiomatization is named for Giuseppe Peano, 1858–1932.)
A set is denumerable (or countably inﬁnite) if it can be put in a one-to-one corre-
spondence with the set of natural numbers {0, 1, 2, 3, . . .}. (See §1.3.1.)
A countable set is a set that is either ﬁnite or denumerable. All other sets are un-
countable.
The ordinal numbers (or ordinals) are deﬁned recursively as follows:
• the empty set is the ordinal number 0;
• if α is an ordinal number, then so is the successor of α, written α+ or α+1, which
is the set α ∪{α};
• if β is any set of ordinals closed under the successor operation, then β is an ordinal,
called a limit ordinal.
The ordinal α is said to be less than the ordinal β, written α < β, if α ⊆β (which is
equivalent to α ∈β).
The sum of ordinals α and β, written α + β, is the ordinal corresponding to the well-
ordered set given by all the elements of α in order, followed by all the elements of β
(viewed as being disjoint from α) in order. (See Fact 26 and §1.4.3.)

Section 1.2
SET THEORY
29
The product of ordinals α and β, written α · β, is the ordinal equal to the Cartesian
product α × β with ordering (a1, b1) < (a2, b2) whenever b1 < b2, or b1 = b2 and a1 < a2
(this is reverse lexicographic order).
Two sets have the same cardinality (or are equinumerous) if they can be put into one-
to-one correspondence (§1.3.1). When the equivalence relation “equinumerous” is used
on all sets (§1.4.2), the sets in each equivalence class have the same cardinal number.
The cardinal number of a set A is written |A|. It can also be regarded as the smallest
ordinal number among all those ordinal numbers with the same cardinality.
An order relation can be deﬁned on cardinal numbers of sets by the rule |A| ≤B if
there is a one-to-one function f : A →B. If |A| ≤|B| and |A| ̸= |B|, write |A| < |B|.
The sum of cardinal numbers a and b, written a+b, is the cardinal number of the union
of two disjoint sets A and B such that |A| = a and |B| = b.
The product of cardinal numbers a and b, written ab, is the cardinal number of the
Cartesian product of two sets A and B such that |A| = a and |B| = b.
Exponentiation of cardinal numbers, written ab, is the cardinality of the set AB of all
functions from B to A, where |A| = a and |B| = b.
Facts:
1. Axiom 3 in the Peano deﬁnition of the natural numbers is the principle of mathe-
matical induction. (See §1.5.4.)
2. The ﬁnite cardinal numbers are written 0, 1, 2, 3, . . ..
3. The cardinal number of any ﬁnite set with n elements is n.
4. The ﬁrst inﬁnite cardinal numbers are written ℵ0, ℵ1, ℵ2, . . . , ℵω, . . . .
5. For each ordinal α, there is a cardinal number ℵα.
6. The cardinal number of any denumerable set, such as N, Z, and Q, is ℵ0.
7. The cardinal number of P(N), R, and C is denoted c (standing for the continuum).
8. The set of algebraic numbers (all solutions of polynomials with integer coeﬃcients)
is denumerable.
9. The set R is uncountable (proved by Georg Cantor in late 19th century, using a
diagonal argument). (See §1.5.5.)
10. Every subset of a countable set is countable.
11. The countable union of countable sets is countable.
12. Every set containing an uncountable subset is uncountable.
13. The continuum problem, posed by Georg Cantor (1845–1918) and restated by David
Hilbert (1862–1943) in 1900, is the problem of determining the cardinality |R| of the real
numbers.
14. The continuum hypothesis is the assertion that |R| = ℵ1, the ﬁrst cardinal number
larger than ℵ0. Equivalently, 2ℵ0 = ℵ1. (See Fact 35.) Kurt G¨odel (1906–1978) proved in
1938 that the continuum hypothesis is consistent with various other axioms of set theory.
Paul Cohen (1934–2007) demonstrated in 1963 that the continuum hypothesis cannot be
proved from those other axioms; i.e., it is independent of the other axioms of set theory.
15. The generalized continuum hypothesis is the assertion that 2ℵα = ℵα+1 for all
ordinals α. That is, for inﬁnite sets there is no cardinal number strictly between |S| and
|P(S)|.

30
Chapter 1
FOUNDATIONS
16. The generalized continuum hypothesis is consistent with and independent of the
usual axioms of set theory.
17. There is no largest cardinal number.
18. |A| < |P(A)| for all sets A.
19. Schr¨oder-Bernstein theorem: If |A| ≤|B| and |B| ≤|A|, then |A| = |B|. (This is
also called the Cantor-Schr¨oder-Bernstein theorem.)
20. The ordinal number 1 = 0+ = {∅} = {0}, the ordinal number 2 = 1+ = {0, 1}, etc.
In general, for ﬁnite ordinals, n + 1 = n+ = {0, 1, 2, . . ., n}.
21. The ﬁrst limit ordinal is ω = {0, 1, 2, . . .}.
Then ω + 1 = ω+ = ω ∪{ω} =
{0, 1, 2, . . ., ω}, and so on. The next limit ordinal is ω+ω = {0, 1, 2, . . ., ω, ω+1, ω+2, . . .},
also denoted ω · 2. The process never stops, because the next limit ordinal can always be
formed as the union of the inﬁnite process that has gone before.
22. Limit ordinals have no immediate predecessors.
23. The ﬁrst ordinal that, viewed as a set, is not countable, is denoted ω1.
24. For ordinals the following are equivalent: α < β, α ∈β, α ⊂β.
25. Every set of ordinal numbers has a smallest element; i.e., the ordinals are well-
ordered. (See §1.4.3.)
26. Ordinal numbers correspond to well-ordered sets (§1.4.3). Two well-ordered sets
represent the same ordinal if they can be put into an order-preserving one-to-one corre-
spondence.
27. Addition and multiplication of ordinals are associative operations.
28. Ordinal addition and multiplication for ﬁnite ordinals (those less than ω) are the
same as ordinary addition and multiplication on the natural numbers.
29. Addition of inﬁnite ordinals is not commutative. (See Example 2.)
30. Multiplication of inﬁnite ordinals is not commutative. (See Example 3.)
31. The ordinals 0 and 1 are identities for addition and multiplication, respectively.
32. Multiplication of ordinals is distributive over addition on the left: α(β + γ) =
αβ + αγ. It is not distributive on the right.
33. In the deﬁnition of the cardinal number ab, when a = 2, the set A can be taken
to be A = {0, 1} and an element of AB can be identiﬁed with a subset of B (namely,
those elements of B sent to 1 by the function). Thus 2|B| = |P(B)|, the cardinality of
the power set of B.
34. If a and b are cardinals, at least one of which is inﬁnite, then a + b = a · b = the
larger of a and b.
35. cℵ0 = ℵℵ0
0
= 2ℵ0.
36. The usual rules for ﬁnite arithmetic continue to hold for inﬁnite cardinal arithmetic
(commutativity, associativity, distributivity, and rules for exponents).
Examples:
1. ω1 > ω · 2, ω1 > ω2, ω1 > ωω.
2. 1 + ω = ω, but ω + 1 > ω.
3. 2 · ω = ω, but ω · 2 > ω.
4. ℵ0 · ℵ0 = ℵ0 + ℵ0 = ℵ0.

Section 1.2
SET THEORY
31
1.2.4
AXIOMS FOR SET THEORY
Set theory can be viewed as an axiomatic system, with undeﬁned terms “set” (the uni-
verse of discourse) and “is an element of” (a binary relation denoted ∈).
Deﬁnitions:
The Axiom of choice (AC) states: If A is any set whose elements are pairwise disjoint
nonempty sets, then there exists a set X that has as its elements exactly one element
from each set in A.
The Zermelo-Fraenkel (ZF) axioms for set theory: (The axioms are stated infor-
mally.)
• Extensionality (equality): Two sets with the same elements are equal.
• Pairing: For every a and b, the set {a, b} exists.
• Speciﬁcation (subset): If A is a set and P(x) is a predicate with free variable x,
then the subset of A exists that consists of those elements c ∈A such that P(c)
is true. (The speciﬁcation axiom guarantees that the intersection of two sets
exists.)
• Union: The union of a set (i.e., the set of all the elements of its elements) exists.
(The union axiom together with the pairing axiom implies the existence of the
union of two sets.)
• Power set: The power set (set of all subsets) of a set exists.
• Empty set: The empty set exists.
• Regularity (foundation): Every nonempty set contains a “foundational” element;
that is, every nonempty set contains an element that is not an element of any
other element in the set. (The regularity axiom prevents anomalies such as a
set being an element of itself.)
• Replacement: If f is a function deﬁned on a set A, then the collection of images
{f(a) | a ∈A} is a set. The replacement axiom (together with the union axiom)
allows the formation of large sets by expanding each element of a set into a set.
• Inﬁnity: An inﬁnite set, such as ω (§1.2.3), exists.
Facts:
1. The axiom of choice is consistent with and independent of the other axioms of set
theory; it can be neither proved nor disproved from the other axioms of set theory.
2. The axioms of ZF together with the axiom of choice are denoted ZFC.
3. The following propositions are equivalent to the axiom of choice:
• The well-ordering principle: Every set can be well-ordered; i.e., for every set A
there exists a total ordering on A such that every subset of A contains a smallest
element under this ordering.
• Generalized axiom of choice (functional version): If A is any collection of non-
empty sets, then there is a function f whose domain is A, such that f(X) ∈X
for all X ∈A.
• Zorn’s lemma: Every nonempty partially ordered set in which every chain (totally
ordered subset) contains an upper bound (an element greater than all the other
elements in the chain) has a maximal element (an element that is less than no
other element). (§1.4.3.)

32
Chapter 1
FOUNDATIONS
• The Hausdorﬀmaximal principle: Every chain in a partially ordered set is con-
tained in a maximal chain (a chain that is not strictly contained in another
chain). (§1.4.3.)
• Trichotomy: Given any two sets A and B, either there is a one-to-one function
from A to B, or there is a one-to-one function from B to A; i.e., either |A| ≤|B|
or |B| ≤|A|.
1.3
FUNCTIONS
A function is a rule that associates to each object in one set an object in a second set
(these sets are often sets of numbers). For instance, the expected population in future
years, based on demographic models, is a function from calendar years to numbers.
Encryption is a function from conﬁdential information to apparent nonsense messages,
and decryption is a function from apparent nonsense back to conﬁdential information.
Computer scientists and mathematicians are often concerned with developing methods
to calculate particular functions quickly.
1.3.1
BASIC TERMINOLOGY FOR FUNCTIONS
Deﬁnitions:
A function f from a set A to a set B, written f : A →B, is a rule that assigns to every
object a ∈A exactly one element f(a) ∈B. The set A is the domain of f; the set B
is the codomain of f; the element f(a) is the image of a or the value of f at a. A
function f is often identiﬁed with its graph {(a, b) | a ∈A and b = f(a)} ⊆A × B.
Note: The function f : A →B is sometimes represented by the “maps to” notation
x 7→f(x) or by the variation x 7→expr(x), where expr(x) is an expression in x. The
notation f(x) = expr(x) is a form of the “maps to” notation without the symbol 7→.
The rule deﬁning a function f : A →B is called well-deﬁned since to each a ∈A there
is associated exactly one element of B.
If f : A →B and S ⊆A, the image of the subset S under f is the set f(S) = {f(x) |
x ∈S}.
If f : A →B and T ⊆B, the pre-image or inverse image of the subset T under f is
the set f −1(T ) = {x | f(x) ∈T }.
The image of a function f : A →B is the set f(A) = {f(x) | x ∈A}.
The range of a function f : A →B is the image set f(A). (Some authors use “range”
as a synonym for “codomain”.)
A function f : A →B is one-to-one (1–1, injective, or a monomorphism) if distinct
elements of the domain are mapped to distinct images; i.e., f(a1) ̸= f(a2) whenever
a1 ̸= a2. An injection is an injective function.
A function f : A →B is onto (surjective, or an epimorphism) if every element of the
codomain B is the image of at least one element of A; i.e., if (∀b ∈B)(∃a ∈A) [f(a) = b]
is true. A surjection is a surjective function.

Section 1.3
FUNCTIONS
33
A function f : A →B is bijective (or a one-to-one correspondence) if it is both
injective and surjective; i.e., it is 1–1 and onto. A bijection is a bijective function.
If f : A →B and S ⊆A, the restriction of f to S is the function fS : S →B where
fS(x) = f(x) for all x ∈S. The function f is an extension of fS. The restriction of f
to S is also written f|S.
A partial function on a set A is a rule f that assigns to each element in a subset of A
exactly one element of B. The subset of A on which f is deﬁned is the domain of
deﬁnition of f. In a context that includes partial functions, a rule that applies to all of
A is called a total function.
Given a 1–1 onto function f : A →B, the inverse function f −1 : B →A has the rule
that for each y ∈B, f −1(y) is the object x ∈A such that f(x) = y.
If f : A →B and g : B →C, then the composition is the function g◦f : A →C deﬁned
by the rule (g◦f)(x) = g(f(x)) for all x ∈A. The function to the right of the raised
circle is applied ﬁrst.
Note: Care must be taken since some sources deﬁne the composition (g◦f)(x) = f(g(x))
so that the order of application reads left to right.
If f : A →A, the iterated functions f n : A →A (n ≥2) are deﬁned recursively by the
rule f n(x) = f ◦f n−1(x).
A function f : A →A is idempotent if f ◦f = f.
A function f : A →A is an involution if f ◦f = iA. (See Example 1.)
A function whose domain is a Cartesian product A1 × · · · × An is often regarded as
a function of n variables (also called a multivariate function), and the value of f at
(a1, . . . , an) is usually written f(a1, . . . , an).
An (n-ary) operation on a set A is a function f : An →A, where An = A × · · · × A
(with n factors in the product). A 1-ary operation is called monadic or unary, and a
2-ary operation is called binary.
Facts:
1. The graph of a function f : A →B is a binary relation on A × B. (§1.4.1.)
2. The graph of a function f : A →B is a subset S of A × B such that for each a ∈A
there is exactly one b ∈B such that (a, b) ∈S.
3. In general, two or more diﬀerent objects in the domain of a function might be assigned
the same value in the codomain. If this occurs, the function is not 1–1.
4. If f : A →B is bijective, then: f◦f −1 = iB (Example 1), f −1◦f = iA, f −1 is
bijective, and (f −1)−1 = f.
5. Function composition is associative:
(f◦g)◦h = f◦(g◦h), whenever h: A →B,
g : B →C, and f : C →D.
6. Function composition is not commutative; that is, f◦g ̸= g◦f in general. (See Ex-
ample 12.)
7. Set operations with functions: If f : A →B with S1, S2 ⊆A and T1, T2 ⊆B, then
• f(S1 ∪S2) = f(S1) ∪f(S2);
• f(S1 ∩S2) ⊆f(S1) ∩f(S2), with equality if f is injective;
• f(S1) ⊇f(S1) (i.e., f(A −S1) ⊇B −f(S1)), with equality if f is injective;

34
Chapter 1
FOUNDATIONS
• f −1(T1 ∪T2) = f −1(T1) ∪f −1(T2);
• f −1(T1 ∩T2) = f −1(T1) ∩f −1(T2);
• f −1( T1 ) = f −1(T1) (i.e., f −1(B −T1) = A −f −1(T1));
• f −1(f(S1)) ⊇S1, with equality if f is injective;
• f(f −1(T1)) ⊆T1, with equality if f is surjective.
8. If f : A →B and g : B →C are both bijective, then (g ◦f)−1 = f −1 ◦g−1.
9. If an operation ∗(such as addition) is deﬁned on a set B, then that operation can be
extended to the set of all functions from a set A to B, by setting (f ∗g)(x) = f(x)∗g(x).
10. Numbers of functions:
If |A| = m and |B| = n, the numbers of diﬀerent types of
functions f : A →B are given in the following list:
• all: nm (§2.2.1)
• one-to-one: P(n, m) = n(n −1)(n −2) . . . (n −m + 1) if n ≥m (§2.2.1)
• onto: Pn
j=0(−1)j n
j

(n −j)m if m ≥n (§2.4.2)
• partial: (n + 1)m (§2.3.2)
Examples:
1. The following are some common functions:
• exponential function to base b (for b > 0, b ̸= 1):
the function f : R →R+
where f(x) = bx. (See the following ﬁgure.) (R+ is the set of positive real
numbers.)
• logarithm function with base b (for b > 0, b ̸= 1): the function logb : R+ →R
that is the inverse of the exponential function to base b; that is,
logb x = y if and only if by = x.
• common logarithm function: the function log10 : R+ →R (also written log)
that is the inverse of the exponential function to base 10; i.e., log10 x = y when
10y = x. (See the following ﬁgure.)
• binary logarithm function:
the function log2 : R+ →R (also denoted log
or lg) that is the inverse of the exponential function to base 2; i.e., log2 x = y
when 2y = x. (See the following ﬁgure.)
• natural logarithm function:
the function ln: R+ →R is the inverse of
the exponential function to base e; i.e., ln(x) = y when ey = x, where e =
limn→∞(1 + 1
n)n ≈2.718281828459. (See the following ﬁgure.)
10
8
6
4
2
0
2
4
-4
-2
1
10x
ex
2x
8
10
12
14
6
4
2
1
-4
-2
2
4
0
log2(x)
ln(x)
log(x)

Section 1.3
FUNCTIONS
35
• iterated logarithm: the function log∗: R+ →{0, 1, 2, . . .}, where log∗x is the
smallest nonnegative integer k such that log(k) x ≤1; the function log(k) is
deﬁned recursively by
log(k) x =





x
if k = 0
log(log(k−1) x)
if log(k−1) x is deﬁned and positive
undeﬁned
otherwise.
• mod function: for a given positive integer n, the function f : Z →N deﬁned by
the rule f(k) = k mod n, where k mod n is the remainder when the division
algorithm is used to divide k by n. (See §4.1.2.)
• identity function on a set A: the function iA : A →A such that iA(x) = x for
all x ∈A.
• characteristic function of S: for S ⊆A, the function χS : A →{0, 1} given by
χS(x) = 1 if x ∈S and χS(x) = 0 if x /∈S.
• projection function:
the function πj : A1 × · · · × An →Aj (j = 1, 2, . . . , n)
such that πj(a1, . . . , an) = aj.
• permutation: a function f : A →A that is 1–1 and onto.
• ﬂoor function (sometimes referred to, especially in number theory, as the great-
est integer function):
the function ⌊⌋: R →Z where ⌊x⌋= the greatest
integer less than or equal to x. The ﬂoor of x is also written [x]. (See the
following ﬁgure.) Thus ⌊π⌋= 3, ⌊6⌋= 6, and ⌊−0.2⌋= −1.
• ceiling function:
the function ⌈⌉: R →Z where ⌈x⌉= the smallest integer
greater than or equal to x. (See the following ﬁgure.) Thus ⌈π⌉= 4, ⌈6⌉= 6,
and ⌈−0.2⌉= 0.
2
2
2
2
2
4
4
2
2
2
4
x
x
2. The ﬂoor and ceiling functions are total functions from the reals R to the integers Z.
They are onto, but not one-to-one.
3. Properties of the ﬂoor and ceiling functions (m and n represent arbitrary integers):
• ⌊x⌋= n if and only if n ≤x < n + 1 if and only if x −1 < n ≤x;
• ⌈x⌉= n if and only if n −1 < x ≤n if and only if x ≤n < x + 1;
• ⌊x⌋< n if and only if x < n; ⌈x⌉≤n if and only if x ≤n;
• n ≤⌊x⌋if and only if n ≤x; n < ⌈x⌉if and only if n < x;
• x −1 < ⌊x⌋≤x ≤⌈x⌉< x + 1;
• ⌊x⌋= x if and only if x is an integer;
• ⌈x⌉= x if and only if x is an integer;

36
Chapter 1
FOUNDATIONS
• ⌊−x⌋= −⌈x⌉; ⌈−x⌉= −⌊x⌋;
• ⌊x + n⌋= ⌊x⌋+ n; ⌈x + n⌉= ⌈x⌉+ n;
• the interval [x1, x2] contains ⌊x2⌋−⌈x1⌉+ 1 integers;
• the interval [x1, x2) contains ⌈x2⌉−⌈x1⌉integers;
• the interval (x1, x2] contains ⌊x2⌋−⌊x1⌋integers;
• the interval (x1, x2) contains ⌈x2⌉−⌊x1⌋−1 integers;
• if f(x) is a continuous, monotonically increasing function, and whenever f(x) is
an integer, x is also an integer, then ⌊f(x)⌋= ⌊f(⌊x⌋)⌋and ⌈f(x)⌉= ⌈f(⌈x⌉)⌉;
• if n > 0, then
 x+m
n

=
 ⌊x⌋+m
n

and
 x+m
n

=
 ⌈x⌉+m
n

(a special case of the
preceding fact);
• if m > 0, then ⌊mx⌋= ⌊x⌋+ ⌊x + 1
m⌋+ · · · + ⌊x + m−1
m ⌋.
4. The logarithm function logb x is bijective from the positive reals R+ to the reals R.
5. The logarithm function x 7→logb x is the inverse of the function x 7→bx, if the
codomain of x 7→bx is the set of positive real numbers. If the domain and codomain are
considered to be R, then x 7→logb x is only a partial function, because the logarithm of
a nonpositive number is not deﬁned.
6. All logarithm functions are related according to the following change of base formula:
logb x = loga x
loga b .
7. log∗2 = 1, log∗4 = 2, log∗16 = 3, log∗65536 = 4, log∗265536 = 5.
8. The diagrams in the following ﬁgure illustrate a function that is onto but not 1–1
and a function that is 1–1 but not onto.
A
f
B
onto, not 1-1
A
f
B
1-1, not onto
9. If the domain and codomain are considered to be the nonnegative reals, then the
function x 7→x2 is a bijection, and x 7→√x is its inverse.
10. If the codomain is considered to be the subset of complex numbers with polar
coordinate 0 ≤θ < π, then x 7→√x can be regarded as a total function.
11. Division of real numbers is a multivariate function from R × (R −{0}) to R, given
by the rule f(x, y) = x
y . Similarly, addition, subtraction, and multiplication are functions
from R × R to R.
12. If f(x) = x2 and g(x) = x + 1, then (f ◦g)(x) = (x + 1)2 and (g ◦f)(x) = x2 + 1.
(Therefore, composition of functions is not commutative.)
13. Collatz conjecture: If f : {1, 2, 3, . . .} →{1, 2, 3, . . .} is deﬁned by the rule f(n) = n
2
if n is even and f(n) = 3n + 1 if n is odd, then for each positive integer m there is a
positive integer k such that the iterated function f k(m) = 1. It is not known whether
this conjecture is true.

Section 1.3
FUNCTIONS
37
1.3.2
COMPUTATIONAL REPRESENTATION
A given function may be described by several diﬀerent rules. These rules can then be
used to evaluate speciﬁc values of the function. There is often a large diﬀerence in the
time required to compute the value of a function using diﬀerent computational rules. The
speed usually depends on the representation of the data as well as on the computational
process.
Deﬁnitions:
A (computational) representation of a function is a way to calculate its values.
A closed formula for a function value f(x) is an algebraic expression in the argument x.
A table of values for a function f : A →B with ﬁnite domain A is any explicit repre-
sentation of the set {(a, f(a)) ∈A × B | a ∈A}.
An inﬁnite sequence in a set S is a function from the natural numbers {0, 1, 2, . . .}
to the set S. It is commonly represented as a list x0, x1, x2, . . . such that each xj ∈S.
Sequences are often permitted to start at the index 1 or elsewhere, rather than 0.
A ﬁnite sequence in a set S is a function from {1, 2, . . ., n} to the set S. It is commonly
represented as a list x1, x2, . . . , xn such that each xj ∈S. Finite sequences are often
permitted to start at the index 0 (or at some other value of the index), rather than at
the index 1.
A value of a sequence is also called an entry, an item, or a term.
A string is a representation of a sequence as a list in which the successive entries are
juxtaposed without intervening punctuation or extra spacing.
A recursive deﬁnition of a function f with domain S has two parts: there is a set of
base values (or initial values) B on which the value of f is speciﬁed, and there is a
rule for calculating f(x) for every x ∈S −B in terms of previously deﬁned values of f.
Ackermann’s function (Wilhelm Ackermann, 1896–1962) is deﬁned recursively by
A(x, y, z) =













x + y
if z = 0
0
if y = 0, z = 1
1
if y = 0, z = 2
x
if y = 0, z > 2
A(x, A(x, y −1, z), z −1)
if y, z > 0.
An alternative version of Ackermann’s function, with two variables, is deﬁned recursively
by
A(m, n) =





n + 1
if m = 0
A(m −1, 1)
if m > 0, n = 0
A(m −1, A(m, n −1))
if m, n > 0.
Another alternative version of Ackermann’s function is deﬁned recursively by the rule
A(n) = An(n), where A1(n) = 2n and Am(n) = A(n)
m−1(1) if m ≥2.
The (input-independent) halting function maps computer programs to the set {0, 1},
with value 1 if the program always halts, regardless of input, and 0 otherwise.

38
Chapter 1
FOUNDATIONS
Facts:
1. If f : N →R is recursively deﬁned, the set of base values is frequently the set
{f(0), f(1), . . ., f(j)} and there is a rule for calculating f(n) for every n > j in terms of
f(i) for one or more i < n.
2. There are functions whose values cannot be computed. (See Example 5.)
3. There are recursively-deﬁned functions that cannot be represented by a closed for-
mula.
4. It is possible to ﬁnd closed formulas for the values of some functions deﬁned recur-
sively. See Chapter 3 for more information.
5. Computer software developers often represent a table as a binary search tree (§18.2.3).
6. In Ackermann’s function of three variables A(x, y, z), as the variable z ranges from 0
to 3, A(x, y, z) is the sum of x and y, the product of x and y, x raised to the exponent y,
and the iterated exponentiation of x y times. That is, A(x, y, 0) = x+ y, A(x, y, 1) = xy,
A(x, y, 2) = xy, A(x, y, 3) = xx···x
(y xs in the exponent).
7. The version of Ackermann’s function with two variables, A(x, y), has the following
properties: A(1, n) = n + 2, A(2, n) = 2n + 3, A(3, n) = 2n+3 −3.
8. A(m, n) is an example of a well-deﬁned total function that is computable, but not
primitive recursive. (See §17.2.1.)
Examples:
1. The function that maps each month to its ordinal position is represented by the table
{(Jan, 1), (Feb, 2), . . . , (Dec, 12)}.
2. The function deﬁned by the recurrence relation
f(0) = 0;
f(n) = f(n −1) + 2n −1 for n ≥1
has the closed form f(x) = x2.
3. The function deﬁned by the recurrence relation
f(0) = 0, f(1) = 1;
f(n) = f(n −1) + f(n −2) for n ≥2
generates the Fibonacci sequence 0, 1, 1, 2, 3, 5, 8, . . . (see §3.1.2) and has the closed form
f(n) = (1 +
√
5)n −(1 −
√
5)n
2n√
5
.
4. The factorial function n! is recursively deﬁned by the rules
0! = 1;
n! = n · (n −1)! for n ≥1.
It has no known closed formula in terms of elementary functions.
5. It is impossible to construct an algorithm to compute the halting function.
6. The halting function from the Cartesian product of the set of computer programs
and the set of strings to {0, 1} whose value is 1 if the program halts when given that
string as input and 0 if the program does not halt when given that string as input is
noncomputable.

Section 1.3
FUNCTIONS
39
7. The following is not a well-deﬁned function f : {1, 2, 3, . . .} →{1, 2, 3, . . .}
f(n) =





1
if n = 1
1 + f( n
2 )
if n is even
f(3n −1)
if n is odd, n > 1
since evaluating f(5) leads to the contradiction f(5) = f(5) + 3.
8. It is not known whether the following is a well-deﬁned function f : {1, 2, 3, . . .} →
{1, 2, 3, . . .}
f(n) =





1
n = 1
1 + f( n
2 )
n even
f(3n + 1)
n odd, n > 1.
(See §1.3.1, Example 13.)
1.3.3
ASYMPTOTIC BEHAVIOR
The asymptotic growth of functions is commonly described with various special pieces
of notation and is regularly used in the analysis of computer algorithms to estimate the
length of time the algorithms take to run and the amount of computer memory they
require.
Deﬁnitions:
A function f : R →R or f : N →R is bounded if there is a constant k such that
|f(x)| ≤k for all x in the domain of f.
For functions f, g : R →R or f, g : N →R (sequences of real numbers) the following are
used to compare their growth rates:
• f is big-oh of g (g dominates f) if there exist constants C and k such that
|f(x)| ≤C|g(x)| for all x > k.
Notation: f is O(g), f(x) ∈O(g(x)), f ∈O(g), f = O(g).
• f is little-oh of g if limx→∞
 f(x)
g(x)
 = 0; i.e., for every C > 0 there is a constant k
such that |f(x)| ≤C|g(x)| for all x > k.
Notation: f is o(g), f(x) ∈o(g(x)), f ∈o(g), f = o(g).
• f is big omega of g if there are constants C and k such that |g(x)| ≤C|f(x)|
for all x > k.
Notation: f is Ω(g), f(x) ∈Ω(g(x)), f ∈Ω(g), f = Ω(g).
• f is little omega of g if limx→∞
 g(x)
f(x)
 = 0.
Notation: f is ω(g), f(x) ∈ω(g(x)), f ∈ω(g), f = ω(g).
• f is theta of g if there are positive constants C1, C2, and k such that C1|g(x)| ≤
|f(x)| ≤C2|g(x)| for all x > k.
Notation: f is Θ(g), f(x) ∈Θ(g(x)), f ∈Θ(g), f = Θ(g), f ≈g.
• f is asymptotic to g if limx→∞
g(x)
f(x) = 1.
This relation is sometimes called
asymptotic equality and is denoted f ∼g or f(x) ∼g(x).

40
Chapter 1
FOUNDATIONS
Facts:
1. The notations O( ), o( ), Ω( ), ω( ), and Θ( ) all stand for collections of functions.
Hence the equality sign, as in f = O(g), does not mean equality of functions.
2. The symbols O(g), o(g), Ω(g), ω(g), and Θ(g) are frequently used to represent a
typical element of the class of functions it represents, as in an expression such as f(n) =
n log n + o(n).
3. Growth rates:
• O(g): the set of functions that grow no more rapidly than a positive multiple of g;
• o(g): the set of functions that grow less rapidly than a positive multiple of g;
• Ω(g): the set of functions that grow at least as rapidly as a positive multiple of g;
• ω(g): the set of functions that grow more rapidly than a positive multiple of g;
• Θ(g): the set of functions that grow at the same rate as a positive multiple of g.
4. Asymptotic notation can be used to describe the growth of inﬁnite sequences, since
inﬁnite sequences are functions from {0, 1, 2, . . .} or {1, 2, 3, . . .} to R (by considering the
term an as a(n), the value of the function a(n) at the integer n).
5. The big-oh notation was introduced in 1892 by Paul Bachmann (1837–1920) in the
study of the rates of growth of various functions in number theory.
6. The big-oh symbol is often called a Landau symbol, after Edmund Landau (1877–
1938), who popularized this notation.
7. Properties of big-oh:
• if f ∈O(g) and c is a constant, then cf ∈O(g);
• if f1, f2 ∈O(g), then f1 + f2 ∈O(g);
• if f1 ∈O(g1) and f2 ∈O(g2), then
⋄(f1 + f2) ∈O(g1 + g2)
⋄(f1 + f2) ∈O(max(|g1|, |g2|))
⋄(f1f2) ∈O(g1g2);
• if f is a polynomial of degree n, then f ∈O(xn);
• if f is a polynomial of degree m and g a polynomial of degree n, with m ≥n,
then f
g ∈O(xm−n);
• if f is a bounded function, then f ∈O(1);
• for all a, b > 1, O(loga x) = O(logb x);
• if f ∈O(g) and |h(x)| ≥|g(x)| for all x > k, then f ∈O(h);
• if f ∈O(xm), then f ∈O(xn) for all n > m.
8. Some of the most commonly used benchmark big-oh classes are: O(1), O(log x), O(x),
O(x log x), O(x2), O(2x), O(x!), and O(xx). If f is big-oh of any function in this list,
then f is also big-oh of each of the following functions in the list:
O(1) ⊂O(log x) ⊂O(x) ⊂O(x log x) ⊂O(x2) ⊂O(2x) ⊂O(x!) ⊂O(xx).
The benchmark functions are drawn in the following ﬁgure.
9. Properties of little-oh:
• if f ∈o(g), then cf ∈o(g) for all nonzero constants c;
• if f1 ∈o(g) and f2 ∈o(g), then f1 + f2 ∈o(g);

Section 1.3
FUNCTIONS
41
xx
100,000
10,000
1,000
100
10
2x
5
10
15
20
25
30
x2
x1
x log x
x
log x
1
• if f1 ∈o(g1) and f2 ∈o(g2), then
⋄(f1 + f2) ∈o(g1 + g2)
⋄(f1 + f2) ∈o(max(|g1|, |g2|))
⋄(f1f2) ∈o(g1g2);
• if f is a polynomial of degree m and g a polynomial of degree n with m < n, then
f
g ∈o(1);
• the set membership f(x) ∈L + o(1) is equivalent to f(x) →L as x →∞, where
L is a constant.
10. If f ∈o(g), then f ∈O(g); the converse is not true.
11. If f ∈O(g) and h ∈o(f), then h ∈o(g).
12. If f ∈o(g) and h ∈O(f), then h ∈O(g).
13. If f ∈O(g) and h ∈O(f), then h ∈O(g).
14. If f1 ∈o(g1) and f2 ∈O(g2), then f1f2 ∈o(g1g2).
15. f ∈O(g) if and only if g ∈Ω(f).
16. f ∈Θ(g) if and only if f ∈O(g) and g ∈O(f).
17. f ∈Θ(g) if and only if f ∈O(g) and f ∈Ω(g).
18. If f(x) = anxn + · · · + a1x + a0 (an ̸= 0), then f ∼anxn.
19. f ∼g if and only if
  f
g −1

∈o(1) (provided g(x) = 0 only ﬁnitely often).
Examples:
1. 5x8 + 10200x5 + 3x + 1 ∈O(x8).
2. x3 ∈O(x4), x4 /∈O(x3).
3. x3 ∈o(x4), x4 /∈o(x3).
4. x3 /∈o(x3).
5. x2 ∈O(5x2), x2 /∈o(5x2).
6. sin(x) ∈O(1).
7.
x7−3x
8x3+5 ∈O(x4), x7−3x
8x3+5 ∈Θ(x4).
8. 1 + 2 + 3 + · · · + n ∈O(n2).
9. 1 + 1
2 + 1
3 + · · · + 1
n ∈O(log n).
10. log(n!) ∈O(n log n).
11. 8x5 ∈Θ(3x5).

42
Chapter 1
FOUNDATIONS
12. x3 ∈Ω(x2).
13. 2n + o(n2) ∼2n.
14. Sometimes asymptotic equality does not behave like equality: ln n ∼ln(2n), but
n ̸∼2n and ln n −ln n ̸∼ln(2n) −ln n.
15. π(n) ∼
n
ln n where π(n) is the number of primes less than or equal to n.
16. If pn is the nth prime, then pn ∼n ln n.
17. Stirling’s formula: n! ∼
√
2πn( n
e )n.
1.4
RELATIONS
Relationships between two sets (or among more than two sets) occur frequently through-
out mathematics and its applications. Examples of such relationships include integers
and their divisors, real numbers and their logarithms, corporations and their customers,
cities and airlines that serve them, people and their relatives. These relationships can be
described as subsets of product sets.
Functions are a special type of relation. Equivalence relations can be used to describe
similarity among elements of sets and partial order relations describe the relative size of
elements of sets.
1.4.1
BINARY RELATIONS AND THEIR PROPERTIES
Deﬁnitions:
A binary relation from set A to set B is any subset R of A × B.
An element a ∈A is related to b ∈B in the relation R if (a, b) ∈R, often written aRb.
If (a, b) /∈R, write aR/ b.
A binary relation (relation) on a set A is a binary relation from A to A; i.e., a subset
of A × A.
A binary relation R on A can have the following properties (to have the property, the
relation must satisfy the property for all a, b, c ∈A):
• reﬂexivity:
aRa
• irreﬂexivity:
aR/ a
• symmetry:
if aRb, then bRa
• asymmetry:
if aRb, then bR/ a
• antisymmetry:
if aRb and bRa, then a = b
• transitivity:
if aRb and bRc, then aRc
• intransitivity:
if aRb and bRc, then aR/ c
Binary relations R and S from A to B can be combined in the following ways to yield
other relations:
• complement of R:
the relation R from A to B where aRb if and only if aR/ b
(i.e., ¬(aRb))

Section 1.4
RELATIONS
43
• diﬀerence:
the binary relation R −S from A to B such that a(R −S)b if and
only if aRb and ¬(aSb)
• intersection: the relation R∩S from A to B where a(R∩S)b if and only if aRb
and aSb
• inverse (converse):
the relation R−1 from B to A where bR−1a if and only if
aRb
• symmetric diﬀerence: the relation R ⊕S from A to B where a(R ⊕S)b if and
only if exactly one of the following is true: aRb, aSb
• union:
the relation R ∪S from A to B where a(R ∪S)b if and only if aRb or
aSb.
The closure of a relation R with respect to a property P is the relation S, if it exists,
that has property P and contains R, such that S is a subset of every relation that has
property P and contains R.
A relation R on A is connected if for all a, b ∈A with a ̸= b, either aRb or there are
c1, c2, . . . , ck ∈A such that aRc1, c1Rc2, . . . , ck−1Rck, ckRb.
If R is a relation on A, the connectivity relation associated with R is the relation R′
where aR′b if and only if aRb or there are c1, c2, . . . , ck ∈A such that aRc1, c1Rc2, . . . ,
ck−1Rck, ckRb.
If R is a binary relation from A to B and if S is a binary relation from B to C, then the
composition of R and S is the binary relation S ◦R from A to C where a(S ◦R)c if
and only if there is an element b ∈B such that aRb and bSc.
The nth power (n a nonnegative integer) of a relation R on a set A, is the relation Rn,
where R0 = {(a, a) | a ∈A} = IA (see Example 4), R1 = R and Rn = Rn−1 ◦R for all
integers n > 1.
A transitive reduction of a relation, if it exists, is a relation with the same transitive
closure as the original relation and with a minimal superset of ordered pairs.
Notation:
1. If a relation R is symmetric, aRb is often written a ∼b, a ≈b, or a ≡b.
2. If a relation R is antisymmetric, aRb is often written a ≤b, a < b, a ⊂b, a ⊆b,
a ⪯b, a ≺b, or a ⊑b.
Facts:
1. A binary relation R from A to B can be viewed as a function from the Cartesian
product A × B to the boolean domain {TRUE, FALSE} (often written {T, F}). The
truth value of the pair (a, b) determines whether a is related to b.
2. Under the inﬁx convention for a binary relation, aRb (a is related to b) means
R(a, b) = TRUE; aR/ b (a is not related to b) means R(a, b) = FALSE.
3. A binary relation R from A to B can be represented in any of the following ways:
• a set R ⊆A × B, where (a, b) ∈R if and only if aRb (this is the deﬁnition of R);
• a directed graph DR whose vertices are the elements of A ∪B, with an edge from
vertex a to vertex b if aRb (§8.3.1);
• a matrix (the adjacency matrix for the directed graph DR): if A = {a1, . . . , am}
and B = {b1, . . . , bn}, the matrix for the relation R is the m × n matrix MR
with entries mij where mij = 1 if aiRbj and mij = 0 otherwise.

44
Chapter 1
FOUNDATIONS
4. R is a reﬂexive relation on A if and only if {(a, a) | a ∈A} ⊆R; i.e., R is a reﬂexive
relation on A if and only if IA ⊆R.
5. R is symmetric if and only if R = R−1.
6. R is an antisymmetric relation on A if and only if R ∩R−1 ⊆{(a, a) | a ∈A}.
7. R is transitive if and only if R ◦R ⊆R.
8. A relation R can be both symmetric and antisymmetric: for example, the equality
relation on a set A.
9. For a relation R that is both symmetric and antisymmetric: R is reﬂexive if and only
if R is the equality relation on some set; R is irreﬂexive if and only if R = ∅.
10. The closure of a relation R with respect to a property P is the intersection of all
relations Q with property P such that R ⊆Q, if there is at least one such relation Q.
11. The transitive closure of a relation R is the connectivity relation R′ associated with
R, which is equal to the union S∞
i=1 Ri of all the positive powers of the relation.
12. A transitive reduction of a relation may contain pairs not in the original relation
(Example 8).
13. Transitive reductions are not necessarily unique (Example 9).
14. If R is a relation on A and x, y ∈A with x ̸= y, then x is related to y in the
transitive closure of R if and only if there is a nontrivial directed path from x to y in the
directed graph DR of the relation.
15. The following table shows how to obtain various closures of a relation and gives
the matrices for the various closures of a relation R with matrix MR on a set A where
|A| = n.
relation
set
matrix
reﬂexive closure
R ∪{(a, a) | a ∈A}
MR ∨In
symmetric closure
R ∪R−1
MR ∨MR−1
transitive closure
Sn
i=1 Ri
MR ∨M [2]
R ∨· · · ∨M [n]
R
Here the matrix In is the n × n identity matrix, M [i]
R is the ith boolean power of the
matrix MR for the relation R, and ∨is the join operator (deﬁned by 0 ∨0 = 0 and
0 ∨1 = 1 ∨0 = 1 ∨1 = 1).
16. The following table provides formulas for the number of binary relations with various
properties on a set with n elements.
type of relation
number of relations
all relations
2n2
reﬂexive
2n(n−1)
symmetric
2n(n+1)/2
transitive
no known simple closed formula (§3.1.8)
antisymmetric
2n · 3n(n−1)/2
asymmetric
3n(n−1)/2
irreﬂexive
2n(n−1)
equivalence (§1.4.2)
Bn = Bell number = Pn
k=1
 n
k
	
where
 n
k
	
is a Stirling subset number (§2.5.2)
partial order (§1.4.3)
no known simple closed formula (§3.1.8)

Section 1.4
RELATIONS
45
Algorithm:
1. Warshall’s algorithm, also called the Roy-Warshall algorithm (B. Roy and S. Warshall
described the algorithm in 1959 and 1960, respectively), Algorithm 1, is an algorithm of
order n3 for ﬁnding the transitive closure of a relation on a set with n elements. (Stephen
Warshall, 1935–2006.)
Algorithm 1:
Warshall’s algorithm.
input: M = [mij]n×n = the matrix representing the binary relation R
output: M = the transitive closure of relation R
for k := 1 to n
for i := 1 to n
for j := 1 to n
mij := mij ∨(mik ∧mkj)
Examples:
1. Some common relations and whether they have certain properties are given in the
following table:
set
relation
reﬂexive
symmetric
antisymmetric
transitive
any nonempty set
=
yes
yes
yes
yes
any nonempty set
̸=
no
yes
no
no
R
≤(or ≥)
yes
no
yes
yes
R
< (or >)
no
no
yes
yes
positive integers
is a divisor of
yes
no
yes
yes
nonzero integers
is a divisor of
yes
no
no
yes
integers
congruence mod n
yes
yes
no
yes
any set of sets
⊆(or ⊇)
yes
no
yes
yes
any set of sets
⊂(or ⊃)
no
no
yes
yes
2. If A is any set, the universal relation is the relation R on A × A such that aRb for
all a, b ∈A; i.e., R = A × A.
3. If A is any set, the empty relation is the relation R on A × A where aRb is never
true; i.e., R = ∅.
4. If A is any set, the relation R on A where aRb if and only if a = b is the identity (or
diagonal) relation I = IA = {(a, a) | a ∈A}, which is also written ∆or ∆A.
5. Every function f : A →B induces a binary relation Rf from A to B under the
rule aRfb if and only if f(a) = b.
6. For A = {2, 3, 4, 6, 12}, suppose that aRb means that a is a divisor of b. Then R can
be represented by the set
{(2, 2), (2, 4), (2, 6), (2, 12), (3, 3), (3, 6), (3, 12), (4, 4), (4, 12), (6, 6), (6, 12), (12, 12)}.
The relation R can also be represented by the digraph with the adjacency matrix







1
0
1
1
1
0
1
0
1
1
0
0
1
0
1
0
0
0
1
1
0
0
0
0
1







.

46
Chapter 1
FOUNDATIONS
7. The transitive closure of the relation {(1, 3), (2, 3), (3, 2)} on {1, 2, 3} is the relation
{(1, 2), (1, 3), (2, 2), (2, 3), (3, 2), (3, 3)}.
8. The transitive closure of the relation R = {(1, 2), (2, 3), (3, 1)} on {1, 2, 3} is the
universal relation {1, 2, 3} × {1, 2, 3}. A transitive reduction of R is the relation given by
{(1, 3), (3, 2), (2, 1)}. This shows that a transitive reduction may contain pairs that are
not in the original relation.
9. If R = {(a, b) | aRb for all a, b ∈{1, 2, 3}}, then the relations {(1, 2), (2, 3), (3, 1)} and
{(1, 3), (3, 2), (2, 1)} are both transitive reductions for R. Thus, transitive reductions are
not unique.
1.4.2
EQUIVALENCE RELATIONS
Equivalence relations are binary relations that describe various types of similarity or
“equality” among elements in a set. The elements that look alike or behave in a similar
way are grouped together in equivalence classes, resulting in a partition of the set. Any
element chosen from an equivalence class essentially “mirrors” the behavior of all elements
in that class.
Deﬁnitions:
An equivalence relation on A is a binary relation on A that is reﬂexive, symmetric,
and transitive.
If R is an equivalence relation on A, the equivalence class of a ∈A is the set R[a] =
{b ∈A | aRb}. When it is clear from context which equivalence relation is intended, the
notation for the induced equivalence class can be abbreviated [a].
The induced partition on a set A under an equivalence relation R is the set of equiv-
alence classes.
Facts:
1. A nonempty relation R is an equivalence relation if and only if R ◦R−1 = R.
2. The induced partition on a set A actually is a partition of A; i.e., the equivalence
classes are all nonempty, every element of A lies in some equivalence class, and two classes
[a] and [b] are either disjoint or equal.
3. There is a one-to-one correspondence between the set of all possible equivalence
relations on a set A and the set of all possible partitions of A. (Fact 2 shows how to
obtain a partition from an equivalence relation. To obtain an equivalence relation from
a partition of A, deﬁne R by the rule aRb if and only if a and b lie in the same element
of the partition.)
4. For any set A, the coarsest partition (with only one set in the partition) of A is
induced by the equivalence relation in which every pair of elements are related. The
ﬁnest partition (with each set in the partition having cardinality 1) of A is induced by
the equivalence relation in which no two diﬀerent elements are related.
5. The set of all partitions of a set A is partially ordered under reﬁnement (§1.2.2 and
§1.4.3). This partial ordering is a lattice (§5.7).
6. To ﬁnd the smallest equivalence relation containing a given relation, ﬁrst take the
transitive closure of the relation, then take the reﬂexive closure of that relation, and
ﬁnally take the symmetric closure.

Section 1.4
RELATIONS
47
Examples:
1. For any function f : A →B, deﬁne the relation a1Ra2 to mean that f(a1) = f(a2).
Then R is an equivalence relation. Each induced equivalence class is the inverse image
f −1(b) of some b ∈B.
2. Write a ≡b (mod n) (“a is congruent to b modulo n”) when a, b and n > 0 are
integers such that n | b −a (n divides b −a). Congruence mod n is an equivalence
relation on the integers.
3. The equivalence relation of congruence modulo n on the integers Z yields a partition
with n equivalence classes: [0] = {kn | k ∈Z}, [1] = {1 + kn | k ∈Z}, [2] = {2 + kn |
k ∈Z}, . . . , [n −1] = {(n −1) + kn | k ∈Z}.
4. The isomorphism relation on any set of groups is an equivalence relation. (The same
result holds for rings, ﬁelds, etc.) (See Chapter 5.)
5. The congruence relation for geometric objects in the plane is an equivalence relation.
6. The similarity relation for geometric objects in the plane is an equivalence relation.
1.4.3
PARTIALLY ORDERED SETS
Partial orderings extend the relationship of ≤on real numbers and allow a comparison
of the relative “size” of elements in various sets. They are developed in greater detail in
Chapter 11.
Deﬁnitions:
A preorder on a set S is a binary relation ≤on S that has the following properties for
all a, b, c ∈S:
• reﬂexive: a ≤a
• transitive: if a ≤b and b ≤c, then a ≤c.
A partial ordering (or partial order) on a set S is a binary relation ≤on S that has
the following properties for all a, b, c ∈S:
• reﬂexive: a ≤a
• antisymmetric: if a ≤b and b ≤a, then a = b
• transitive: if a ≤b and b ≤c, then a ≤c.
Notation: The expression c ≥b means that b ≤c. The symbols ⪯and ⪰are often used
in place of ≤and ≥. The expression a < b (or b > a) means that a ≤b and a ̸= b.
A partially ordered set (or poset) is a set with a partial ordering deﬁned on it.
A directed ordering on a set S is a partial ordering that also satisﬁes the following
property: if a, b ∈S, then there is a c ∈S such that a ≤c and b ≤c.
Note: Some authors do not require that antisymmetry hold in the deﬁnition of directed
ordering.
Two elements a and b in a poset are comparable if either a ≤b or b ≤a. Otherwise,
they are incomparable.
A totally ordered (or linearly ordered) set is a poset in which every pair of elements
are comparable.

48
Chapter 1
FOUNDATIONS
A chain is a subset of a poset in which every pair of elements are comparable.
An antichain is a subset of a poset in which no two distinct elements are comparable.
An interval in a poset (S, ≤) is a subset [a, b] = {x | x ∈S, a ≤x ≤b}.
An element b in a poset is minimal if there exists no element c such that c < b.
An element b in a poset is maximal if there exists no element c such that c > b.
An element b in a poset S is a maximum element (or greatest element) if every
element c satisﬁes the relation c ≤b.
An element b in a poset S is a minimum element (or least element) if every element
c satisﬁes the relation c ≥b.
A well-ordered set is a poset (S, ≤) in which every nonempty subset contains a minimum
element.
An element b in a poset S is an upper bound for a subset U ⊆S if every element c
of U satisﬁes the relation c ≤b.
An element b in a poset S is a lower bound for a subset U ⊆S if every element c of U
satisﬁes the relation c ≥b.
A least upper bound for a subset U of a poset S is an upper bound b such that if c is
any other upper bound for U then c ≥b.
A greatest lower bound for a subset U of a poset S is a lower bound b such that if c
is any other lower bound for U then c ≤b.
A lattice is a poset in which every pair of elements, x and y, have both a least upper
bound lub(x, y) and a greatest lower bound glb(x, y) (§5.7).
The Cartesian product of two posets (S1, ≤1) and (S2, ≤2) is the poset with domain
S1 × S2 and relation ≤1× ≤2 given by the rule (a1, a2) ≤1× ≤2 (b1, b2) if and only if
a1 ≤1 b1 and a2 ≤2 b2.
The element c covers another element b in a poset if b < c and there is no element d
such that b < d < c.
A Hasse diagram (cover diagram) for a poset (S, ≤) is a directed graph (§11.1) whose
vertices are the elements of S such that there is an arc from b to c if c covers b, all arcs
are directed upward when drawing the diagram, and arrows on the arcs are omitted.
Facts:
1. R is a partial order on a set S if and only if R−1 is a partial order on S.
2. The only partial order that is also an equivalence relation is the relation of equality.
3. The Cartesian product of two posets, each with at least two elements, is not totally
ordered.
4. In the Hasse diagram for a poset, there is a path from vertex b to vertex c if and only
if b ≤c. (When b = c, it is the path of length 0.)
5. Least upper bounds and greatest lower bounds are unique, if they exist.

Section 1.4
RELATIONS
49
Examples:
1. The positive integers are partially ordered under the relation of divisibility, in which
b ≤c means that b divides c. In fact, they form a lattice (§5.7.1), called the divisibility
lattice. The least upper bound of two numbers is their least common multiple, and the
greatest lower bound is their greatest common divisor.
2. The set of all powers of two (or of any other positive integer) forms a chain in the
divisibility lattice.
3. The set of all primes forms an antichain in the divisibility lattice.
4. The set R of real numbers with the usual deﬁnition of ≤is a totally ordered set.
5. The set of all logical propositions on a ﬁxed set of logical variables p, q, r, . . . is par-
tially ordered under inverse implication, so that B ≤A means that A →B is a tautology.
6. The complex numbers, ordered under magnitude, do not form a poset, because they
do not satisfy the axiom of antisymmetry.
7. The set of all subsets of any set forms a lattice under the relation of subset inclusion.
The least upper bound of two subsets is their union, and the greatest lower bound is
their intersection. Part (a) in the following ﬁgure gives the Hasse diagram for the lattice
of all subsets of {a, b, c}.
8. Part (b) of the following ﬁgure shows the Hasse diagram for the lattice of all positive
integer divisors of 12.
9. Part (c) of the following ﬁgure shows the Hasse diagram for the set {1, 2, 3, 4, 5, 6}
under divisibility.
10. Part (d) of the following ﬁgure shows the Hasse diagram for the set {1, 2, 3, 4} with
the usual deﬁnition of ≤.
(a,b,c)
(a,b)
(a,c)
(b,c)
(a)
(b)
(c)
0
4
6
2
3
12
1
4
2
3
5
6
1
4
3
2
1
(a)
(b)
(c)
(d)
11. Multilevel security policy:
The ﬂow of information is often restricted by using
security clearances.
Documents are put into security classes, (L, C), where L is an
element of a totally ordered set of authority levels (such as “unclassiﬁed”, “conﬁdential”,
“secret”, “top secret”) and C is a subset (called a “compartment”) of a set of subject
areas. The subject areas might consist of topics such as agriculture, Eastern Europe,
economy, crime, and trade. A document on how trade aﬀects the economic structure
of Eastern Europe might be assigned to the compartment {trade, economy, Eastern
Europe}. The set of security classes is made into a lattice by the rule: (L1, C1) ≤(L2, C2)
if and only if L1 ≤L2 and C1 ⊆C2. Information is allowed to ﬂow from class (L1, C1) to
class (L2, C2) if and only if (L1, C1) ≤(L2, C2). For example, a document with security
class (secret, {trade, economy}) ﬂows to both (top secret, {trade, economy}) and (secret,
{trade, economy, Eastern Europe}), but not vice versa. This set of security classes forms
a lattice (§5.7.1).

50
Chapter 1
FOUNDATIONS
1.4.4
n-ARY RELATIONS
Deﬁnitions:
An n-ary relation on sets A1, A2, . . . , An is any subset R of A1 × A2 × · · · × An.
The sets Ai are called the domains of the relation and the number n is called the degree
of the relation.
A primary key of an n-ary relation R on A1 × A2 × · · · × An is a domain Ai such that
each ai ∈Ai is the ith coordinate of at most one n-tuple in R.
A composite key of an n-ary relation R on A1 × A2 × · · · × An is a product of domains
Ai1 × Ai2 × · · · × Aim such that for each m-tuple (ai1, ai2, . . . , aim) ∈Ai1 × Ai2 × · · · ×
Aim, there is at most one n-tuple in R that matches (ai1, ai2, . . . , aim) in coordinates
i1, i2, . . . , im.
The projection function Pi1,i2,...,ik : A1 × A2 × · · · × An →Ai1 × Ai2 × · · · × Aik is
given by the rule
Pi1,i2,...,ik(a1, a2, . . . , an) = (ai1, ai2, . . . , aik).
That is, Pi1,i2,...,ik selects the elements in coordinate positions i1, i2, . . . , ik from the
n-tuple (a1, a2, . . . , an).
The join Jk(R, S) of an m-ary relation R and an n-ary relation S, where k ≤m and
k ≤n, is a relation of degree m + n −k such that
(a1, . . . , am−k, c1, . . . , ck, b1, . . . , bn−k) ∈Jk(R, S)
if and only if
(a1, . . . , am−k, c1, . . . , ck) ∈R and (c1, . . . , ck, b1, . . . , bn−k) ∈S.
Facts:
1. An n-ary relation on sets A1, A2, . . . , An can be regarded as a function R from A1 ×
A2 × · · ·× An to the Boolean domain {TRUE, FALSE}, where (a1, a2, . . . , an) ∈R if and
only if R(a1, a2, . . . , an) = TRUE.
2. n-ary relations are essential models in the construction of database systems.
Examples:
1. Let A1 be the set of all men and A2 the set of all women, in a nonpolygamous society.
Let mRw mean that m and w are presently married. Then each of A1 and A2 is a primary
key.
2. Let A1 be the set of all telephone numbers and A2 the set of all persons. Let nRp
mean that telephone number n belongs to person p. Then A1 is a primary key if each
number is assigned to at most one person, and A2 is a primary key if each person has at
most one phone number.
3. In a conventional telephone directory, the name and address domains can form a
composite key, unless there are two persons with the same name (no distinguishing
middle initial or suﬃx such as “Jr.”) at the same address.
4. Let A = B = C = Z, and let R be the relation on A × B × C such that (a, b, c) ∈R
if and only if a + b = c. The set A × B is a composite key. There is no primary key.

Section 1.5
PROOF TECHNIQUES
51
5. Let A = all students at a certain college, B = all student ID numbers being used at
the college, C = all major programs at the college. Suppose a relation R is deﬁned on
A × B × C by the rule (a, b, c) ∈R means student a with ID number b has major c. If
each student has exactly one major and if there is a one-to-one correspondence between
students and ID numbers, then A and B are each primary keys.
6. Let A = all employee names at a certain corporation, B = all Social Security numbers,
C = all departments, D = all job titles, E = all salary amounts, and F = all calendar
dates. On A×B×C ×D×E ×F ×F let R be the relation such that (a, b, c, d, e, f, g) ∈R
means employee named a with Social Security number b works in department c, has
job title d, earns an annual salary e, was hired on date f, and had the most recent
performance review on date g. The projection P1,5 (projection onto A × E) gives a list
of employees and their salaries.
1.5
PROOF TECHNIQUES
A proof is a derivation of new facts from old ones. A proof makes possible the derivation
of properties of a mathematical model from its deﬁnition, or the drawing of scientiﬁc
inferences based on data that have been gathered. Axioms and postulates capture all
basic truths used to develop a theory. Constructing proofs is one of the principal activities
of mathematicians.
Furthermore, proofs play an important role in computer science—in such areas as veriﬁ-
cation of the correctness of computer programs, veriﬁcation of communications protocols,
automatic reasoning systems, and logic programming.
1.5.1
RULES OF INFERENCE
Deﬁnitions:
A proposition is a declarative sentence that is unambiguously either true or false.
(See §1.1.1.)
A theorem is a proposition derived as the conclusion of a valid proof from axioms and
deﬁnitions.
A lemma is a theorem that is an intermediate step in the proof of a more important
theorem.
A corollary is a theorem that is derived as an easy consequence of another theorem.
A statement form is a declarative sentence containing some variables and logical sym-
bols, such that the sentence becomes a proposition if concrete values are substituted for
all the free variables.
An argument form is a sequence of statement forms.
The ﬁnal statement form in an argument form is called the conclusion (of the argument).
The conclusion is often preceded by the word “therefore” (symbolized ∴).
The statement forms preceding the conclusion in an argument form are called premises
(of the argument).

52
Chapter 1
FOUNDATIONS
If concrete values are substituted for the free variables of an argument form, an argu-
ment of that form is obtained.
An instantiation of an argument is the substitution of concrete values into all free
variables of the premises and conclusion.
A valid argument form is an argument form such that in every instantiation in which
all the premises are true, the conclusion is also true.
A rule of inference is an alternative name for a valid argument form, which is used
when the form is frequently applied.
Facts:
1. Substitution rule:
Any variable occurring in an argument may be replaced by an
expression of the same type without aﬀecting the validity of the argument, as long as
the replacement is made everywhere the variable occurs.
2. The following table gives rules of inference for arguments with compound statements.
name
argument
name
argument
form
form
Modus ponens
p →q
Modus tollens
p →q
(method of aﬃrming)
p
(method of denying)
¬q
∴q
∴¬p
Hypothetical
p →q
Disjunctive
p ∨q
syllogism
q →r
syllogism
¬p
∴p →r
∴q
Disjunctive
p
Dilemma by
p ∨q
addition
∴p ∨q
cases
p →r
q →r
∴r
Constructive
p ∨r
Destructive
¬q ∨¬s
dilemma
p →q
dilemma
p →q
r →s
r →s
∴q ∨s
∴¬p ∨¬r
Conjunctive
p
Conditional
p
addition
q
proof
p ∧q →r
∴p ∧q
∴q →r
Conjunctive
p ∧q
Rule of
given the
simpliﬁcation
∴p
contradiction
contradiction c
¬p →c
∴p
3. The following table gives rules of inference for arguments with quantiﬁers.

Section 1.5
PROOF TECHNIQUES
53
name
argument form
Universal instantiation
(∀x ∈D) Q(x)
∴Q(a) (a any particular element of D)
Generalizing from the
Q(a) (a an arbitrarily chosen element of D)
generic particular
∴(∀x ∈D) Q(x)
Existential speciﬁcation
(∃x ∈D) Q(x)
∴Q(a) (for at least one a ∈D)
Existential generalization
Q(a) (for at least one element a ∈D)
∴(∃x ∈D) Q(x)
4. Substituting R(x) →S(x) in place of Q(x) and z in place of x in generalizing from
the generic particular gives the following inferential rule:
Universal modus
R(a) →S(a) for any particular but arbitrarily chosen a ∈D
ponens:
∴(∀z ∈D) [R(z) →S(z)].
5. The rule of generalizing from the generic particular determines the outline of most
mathematical proofs.
6. The rule of existential speciﬁcation is used in deductive reasoning to give names to
quantities that are known to exist but whose exact values are unknown.
7. A useful strategy for determining whether a statement is true is to ﬁrst try to prove
it using a variety of approaches and proof methods. If this is unsuccessful, the next step
may be to try to disprove the statement, such as by trying to construct or prove the
existence of a counterexample. If this does not work, the next step is to try to prove the
statement again, and so on. This is one of the many ways in which many mathematicians
attempt to develop new results.
Examples:
1. Suppose that D is the set of all objects in the physical universe, P(x) is “x is a human
being”, Q(x) is “x is mortal”, and a is the Greek philosopher Socrates.
argument form
an argument of that form
(∀x ∈D) [P(x) →Q(x)]
∀objects x, (x is a human being) →(x is mortal).
(informally: All human beings are mortal.)
P(a) (for particular a ∈D)
Socrates is a human being.
∴Q(a)
∴Socrates is mortal.
2. The argument form shown below is invalid: there is an argument of this form (shown
next to it) that has true premises and a false conclusion.
argument form
an argument of that form
(∀x ∈D) [P(x) →Q(x)]
∀objects x, (x is a human being) →(x is mortal).
(informally: All human beings are mortal.)
Q(a) (for particular a ∈D)
My cat Bunbury is mortal.
∴P(a)
∴My cat Bunbury is a human being.
In this example, D is the set of all objects in the physical universe, P(x) is “x is a human
being”, Q(x) is “x is mortal”, and a is my cat Bunbury.

54
Chapter 1
FOUNDATIONS
3. The distributive law for real numbers, (∀a, b, c ∈R)[ac + bc = (a + b)c], implies that
2
√
2 + 3
√
2 = (2 + 3)
√
2 (because 2, 3, and
√
2 are particular real numbers).
4. Since 2 is a prime number that is not odd, the rule of existential generalization implies
the truth of the statement “∃a prime number n such that n is not odd”.
5. To prove that the square of every even integer is even, by the rule of generalizing
from the generic particular, begin by supposing that n is any particular but arbitrarily
chosen even integer. The job of the proof is to deduce that n2 is even.
6. By deﬁnition, every even integer equals twice some integer. So if at some stage of a
reasoning process there is a particular even integer n, it follows from the rule of existential
speciﬁcation that n = 2k for some integer k (even though the numerical values of n and
k may be unknown).
1.5.2
PROOFS
Deﬁnitions:
A (logical) proof of a statement is a ﬁnite sequence of statements (called the steps of
the proof) leading from a set of premises to the given statement. Each step of the proof
must either be a premise or follow from some previous steps by a valid rule of inference.
In a mathematical proof, the set of premises may contain any item of previously proved
or agreed upon mathematical knowledge (deﬁnitions, axioms, theorems, etc.) as well as
the speciﬁc hypotheses of the statement to be proved.
A direct proof of a statement of the form p →q is a proof that assumes p to be true
and then shows that q is true.
An indirect proof of a statement of the form p →q is a proof that assumes that ¬q is
true and then shows that ¬p is true. That is, a proof of this form is a direct proof of the
contrapositive ¬q →¬p.
A proof by contradiction assumes the negation of the statement to be proved and
shows that this leads to a contradiction.
Facts:
1. A useful strategy to determine if a statement of the form (∀x ∈D) [P(x) →Q(x)] is
true or false is to imagine an element x ∈D that satisﬁes P(x) and, using this assumption
(and other facts), investigate whether x must also satisfy Q(x). If the answer for all such
x is “yes”, the given statement is true and the result of the investigation is a direct
proof. If it is possible to ﬁnd an x ∈D for which Q(x) is false, the statement is false
and this value of x is a counterexample. If the investigation shows that is not possible
to ﬁnd an x ∈D for which Q(x) is false, the given statement is true and the result of
the investigation is a proof by contradiction.
2. There are many types of techniques that can be used to prove theorems. The following
table describes how to approach proofs of various types of statements.

Section 1.5
PROOF TECHNIQUES
55
statement
technique of proof
p →q
Direct proof : Assume that p is true. Use rules of inference
and previously accepted axioms, deﬁnitions, theorems, and
facts to deduce that q is true.
(∀x ∈D)P(x)
Direct proof :
Suppose that x is an arbitrary element of
D. Use rules of inference and previously accepted axioms,
deﬁnitions, and facts to deduce that P(x) is true.
(∃x ∈D)P(x)
Constructive direct proof : Use rules of inference and pre-
viously accepted axioms, deﬁnitions, and facts to actually
ﬁnd an x ∈D for which P(x) is true.
Nonconstructive direct proof :
Deduce the existence of x
from other mathematical facts without a description of how
to compute it.
(∀x∈D)(∃y∈E)P(x, y)
Constructive direct proof :
Assume that x is an arbitrary
element of D. Use rules of inference and previously accepted
axioms, deﬁnitions, and facts to show the existence of a
y ∈E for which P(x, y) is true, in such a way that y can
be computed as a function of x.
Nonconstructive direct proof :
Assume x is an arbitrary
element of D. Deduce the existence of y from other math-
ematical facts without a description of how to compute it.
p →q
Proof by cases: Suppose p ≡p1∨· · · ∨pk. Prove that each
conditional pi→q is true. The basis for division into cases
is the logical equivalence [(p1∨· · · ∨pk)→q] ≡[(p1→q) ∧
· · · ∧(pk→q)].
p →q
Indirect proof or proof by contraposition:
Assume that
¬q is true (that is, assume that q is false). Use rules of
inference and previously accepted axioms, deﬁnitions, and
facts to show that ¬p is true (that is, p is false).
p →q
Proof by contradiction: Assume that p →q is false (that is,
assume that p is true and q is false). Use rules of inference
and previously accepted axioms, deﬁnitions, and facts to
show that a contradiction results. This means that p →q
cannot be false, and hence must be true.
(∃x ∈D)P(x)
Proof by contradiction: Assume that there is no x ∈D for
which P(x) is true. Show that a contradiction results.
(∀x ∈D)P(x)
Proof by contradiction: Assume that there is some x ∈D
for which P(x) is false. Show that a contradiction results.
p →(q ∨r)
Proof of a disjunction: Prove that one of its logical equiv-
alences (p ∧¬q) →r or (p ∧¬r) →q is true.
p1, . . . , pk are
Proof by cycle of implications: Prove p1 →p2, p2 →p3,
equivalent
. . . , pk−1 →pk, pk →p1. This is equivalent to proving
(p1 →p2) ∧(p2 →p3) ∧· · · ∧(pk−1 →pk) ∧(pk →p1).
Examples:
1. In the following direct proof (see the table in Fact 2, item 2), the domain D is the
set of all pairs of integers, x is (m, n), and the predicate P(m, n) is “if m and n are even,
then m + n is even”.

56
Chapter 1
FOUNDATIONS
Theorem: For all integers m and n, if m and n are even, then m + n is even.
Proof : Suppose m and n are arbitrarily chosen even integers. [m + n must be
shown to be even.]
1.
m = 2r, n = 2s for some integers r and s
(by deﬁnition of even)
2.
m + n = 2r + 2s
(by substitution)
3.
m + n = 2(r + s)
(by factoring out the 2)
4.
r + s is an integer
(it is a sum of two integers)
5. ∴m + n is even
(by deﬁnition of even)
The following partial expansion of the proof shows how some of the steps are justiﬁed by
rules of inference combined with previous mathematical knowledge:
1. Every even integer equals twice some integer:
[∀even x ∈Z (x = 2y for some y ∈Z)]
m is a particular even integer.
∴m = 2r for some integer r.
3. Every integer is a real number: [∀n ∈Z (n ∈R)]
r and s are particular integers.
∴r and s are real numbers.
The distributive law holds for real numbers: [∀a, b, c ∈R (ab + ac = a(b + c))]
2, r, and s are particular real numbers.
∴2r + 2s = 2(r + s).
4. Any sum of two integers is an integer: [∀m, n ∈Z (m + n ∈Z)]
r and s are particular integers.
∴r + s is an integer.
5. Any integer that equals twice some integer is even:
[∀x ∈Z (if x = 2y for some y ∈Z, then x is even.)]
2(r + s) equals twice the integer r + s.
∴2(r + s) is even.
2. A constructive existence proof :
Theorem: Given any integer n, there is an integer m with m > n.
Proof : Suppose that n is an integer. Let m = n+ 1. Then m is an integer and
m > n.
The proof is constructive because it established the existence of the desired integer m by
showing that its value can be computed by adding 1 to the value of n.
3. A nonconstructive existence proof :
Theorem: Given a nonnegative integer n, there is always a prime number p
that is greater than n.
Proof : Suppose that n is a nonnegative integer. Consider n! + 1. Then n! + 1
is divisible by some prime number p because every integer greater than 1 is
divisible by a prime number, and n! + 1 > 1. Also, p > n because when n! + 1
is divided by any positive integer less than or equal to n, the remainder is 1
(since any such number is a factor of n!).
The proof is a nonconstructive existence proof because it demonstrated the existence of
the number p, but it oﬀered no computational rule for ﬁnding it.

Section 1.5
PROOF TECHNIQUES
57
4. A proof by cases:
Theorem: For all odd integers n, the number n2 −1 is divisible by 8.
Proof : Suppose n is an odd integer. When n is divided by 4, the remainder is
0, 1, 2, or 3. Hence n has one of the four forms 4k, 4k + 1, 4k + 2, or 4k + 3
for some integer k. But n is odd. So n ̸= 4k and n ̸= 4k + 2. Thus either
n = 4k + 1 or n = 4k + 3 for some integer k.
Case 1 [n = 4k + 1 for some integer k]: In this case n2 −1 = (4k + 1)2 −1 =
16k2 + 8k + 1 −1 = 16k2 + 8k = 8(2k2 + k), which is divisible by 8 because
2k2 + k is an integer.
Case 2 [n = 4k + 3 for some integer k]: In this case n2 −1 = (4k + 3)2 −1 =
16k2 + 24k + 9 −1 = 16k2 + 24k + 8 = 8(2k2 + 3k + 1), which is divisible by 8
because 2k2 + 3k + 1 is an integer.
So in either case n2−1 is divisible by 8, and thus the given statement is proved.
5. A proof by contraposition:
Theorem: For all integers n, if n2 is even, then n is even.
Proof : Suppose that n is an integer that is not even. Then when n is divided
by 2 the remainder is 1, or, equivalently, n = 2k + 1 for some integer k. By
substitution, n2 = (2k + 1)2 = 4k2 + 4k + 1 = 2(2k2 + 2k) + 1. It follows that
when n2 is divided by 2 the remainder is 1 (because 2k2 + 2k is an integer).
Thus, n2 is not even.
In this proof by contraposition, a direct proof was given of the contrapositive “if n
is not even, then n2 is not even”.
6. A proof by contradiction:
Theorem:
√
2 is irrational.
Proof : Suppose not; that is, suppose that
√
2 were a rational number. By
deﬁnition of rational, there would exist integers a and b such that
√
2 = a
b ,
or, equivalently, 2b2 = a2. Now the prime factorization of the left-hand side of
this equation contains an odd number of factors and that of the right-hand side
contains an even number of factors (because every prime factor in an integer
occurs twice in the prime factorization of the square of that integer). But this
is impossible because the prime factorization of every integer is unique. This
yields a contradiction, which shows that the original supposition was false.
Hence
√
2 is irrational.
7. A proof by cycle of implications:
Theorem: For all positive integers a and b, the following statements are equiv-
alent:
(1) a is a divisor of b;
(2) the greatest common divisor of a and b is a;
(3)
 b
a

= b
a.
Proof : Let a and b be positive integers.
(1) →(2): Suppose that a is a divisor of b. Since a is also a divisor of a, a is a
common divisor of a and b. But no integer greater than a is a divisor of a. So
the greatest common divisor of a and b is a.
(2) →(3): Suppose that the greatest common divisor of a and b is a. Then
a is a divisor of both a and b, so b = ak for some integer k. Then b
a = k, an
integer, and so by deﬁnition of ﬂoor,
 b
a

= k = b
a.

58
Chapter 1
FOUNDATIONS
(3) →(1): Suppose that
 b
a

= b
a. Let k =
 b
a

. Then k =
 b
a

= b
a, and k
is an integer by deﬁnition of ﬂoor. Multiplying the outer parts of the equality
by a gives b = ak, so by deﬁnition of divisibility, a is a divisor of b.
8. A proof of a disjunction:
Theorem: For all integers a and p, if p is prime, then either p is a divisor of a,
or a and p have no common factor greater than 1.
Proof : Suppose a and p are integers and p is prime, but p is not a divisor of a.
Since p is prime, its only positive divisors are 1 and p. So, since p is not a
divisor of a, the only possible positive common divisor of a and p is 1. Hence a
and p have no common divisor greater than 1.
1.5.3
DISPROOFS
Deﬁnitions:
A disproof of a statement is a proof that the statement is false.
A counterexample to a statement of the form (∀x ∈D)P(x) is an element b ∈D for
which P(b) is false.
Facts:
1. The method of disproof by counterexample is based on the following fact:
¬[(∀x ∈D) P(x)] ⇔(∃x ∈D) [¬P(x)].
2. The following table describes how to give various types of disproofs.
statement
technique of disproof
(∀x∈D)P(x)
Constructive disproof by counterexample:
Exhibit a spe-
ciﬁc a ∈D for which P(a) is false.
(∀x∈D)P(x)
Existence disproof : Prove the existence of some a ∈D for
which P(a) is false.
(∃x∈D)P(x)
Prove that there is no a ∈D for which P(a) is true.
(∀x∈D) [P(x) →Q(x)]
Find an element a ∈D with P(a) true and Q(a) false.
(∀x∈D)(∃y∈E) P(x, y)
Find an element a ∈D with P(a, y) false for every y ∈E.
(∃x∈D)(∀y∈E) P(x, y)
Prove that there is no a ∈D for which P(a, y) is true for
every possible y ∈E.
Examples:
1. The statement (∀a, b ∈R) [ a2 < b2 →a < b ] is disproved by the following counter-
example: a = 2, b = −3. Then a2 < b2 (because 4 < 9) but a ̸< b (because 2 ̸< −3).
2. The statement “every prime number is odd” is disproved by exhibiting the counterex-
ample n = 2, since n is prime and not odd.

Section 1.5
PROOF TECHNIQUES
59
1.5.4
MATHEMATICAL INDUCTION
Deﬁnitions:
The principle of mathematical induction (weak form) is the following rule of
inference for proving that all the items in a list x0, x1, x2, . . . have some property P(x):
P(x0) is true
basis premise
(∀k ≥0 ) [if P(xk) is true, then P(xk+1) is true]
induction premise
∴(∀n ≥0) [P(xn) is true].
conclusion
The antecedent P(xk) in the induction premise “if P(xk) is true, then P(xk+1) is true”
is called the induction hypothesis.
The basis step of a proof by mathematical induction is a proof of the basis premise.
The induction step of a proof by mathematical induction is a proof of the induction
premise.
The principle of mathematical induction (strong form) is the following rule of
inference for proving that all the items in a list x0, x1, x2, . . . have some property P(x):
P(x0) is true
basis premise
(∀k ≥0) [if P(x0), P(x1), . . . , P(xk) are all
(strong) induction premise
true, then P(xk+1) is true]
∴(∀n ≥0) [P(xn) is true].
conclusion
The well-ordering principle for the integers is the following axiom:
If S is a
nonempty set of integers such that every element of S is greater than some ﬁxed integer,
then S contains a least element.
Facts:
1. Typically, the principle of mathematical induction is used to prove that one of the
following sequences of statements is true: P(0), P(1), P(2), . . . or P(1), P(2), P(3), . . . .
In these cases the principle of mathematical induction has the form: if P(0) is true and
P(n) →P(n + 1) is true for all n ≥0, then P(n) is true for all n ≥0; or if P(1) is true
and P(n) →P(n + 1) is true for all n ≥1, then P(n) is true for all n ≥1.
2. If the truth of P(n + 1) can be obtained from the previous statement P(n), the weak
form of the principle of mathematical induction can be used. If the truth of P(n + 1)
requires the use of one or more statements P(k) for k ≤n, then the strong form should
be used.
3. Mathematical induction can also be used to prove statements that can be phrased in
the form “For all integers n ≥k, P(n) is true”.
4. Mathematical induction can often be used to prove summation formulas and inequal-
ities.
5. There are alternative forms of mathematical induction, such as the following:
• if P(0) and P(1) are true, and if P(n) →P(n+ 2) is true for all n ≥0, then P(n)
is true for all n ≥0;
• if P(0) and P(1) are true, and if [P(n) ∧P(n + 1)] →P(n + 2) is true for all
n ≥0, then P(n) is true for all n ≥0.

60
Chapter 1
FOUNDATIONS
6. The weak form of the principle of mathematical induction, the strong form of the
principle of mathematical induction, and the well-ordering principle for the integers are
all regarded as axioms for the integers. This is because they cannot be derived from the
usual simpler axioms used in the deﬁnition of the integers. (See the Peano deﬁnition of
the natural numbers in §1.2.3.)
7. The weak form of the principle of mathematical induction, the strong form of the
principle of mathematical induction, and the well-ordering principle for the integers are
all equivalent. In other words, each of them can be proved from each of the others.
8. The earliest recorded use of mathematical induction occurs in 1575 in the book
Arithmeticorum Libri Duo by Francesco Maurolico, who used the principle to prove
that the sum of the ﬁrst n odd positive integers is n2.
Examples:
1. A proof using the weak form of mathematical induction: (In this proof, x0, x1, x2, . . .
is the sequence 1, 2, 3, . . ., and the property P(xn) is the equation 1+2+· · ·+n = n(n+1)
2
.)
Theorem: For all integers n ≥1, 1 + 2 + · · · + n = n(n+1)
2
.
Proof :
Basis Step:
For n = 1 the left-hand side of the formula is 1, and the
right-hand side is 1(1+1)
2
, which is also equal to 1. Hence P(1) is true.
Induction Step: Let k be an integer, k ≥1, and suppose that P(k) is true.
That is, suppose that 1+2+· · ·+k = k(k+1)
2
holds (the induction hypothesis).
It must be shown that P(k + 1) is true: 1 + 2 + · · · + (k + 1) = (k+1)((k+1)+1)
2
,
or, equivalently, that 1 + 2 + · · · + (k + 1) = (k+1)(k+2)
2
. But, by substitution
from the induction hypothesis,
1 + 2 + · · · + (k + 1) = (1 + 2 + · · · + k) + (k + 1)
= k(k+1)
2
+ (k + 1)
= (k+1)(k+2)
2
.
Thus, 1 + 2 + · · · + (k + 1) = (k+1)(k+2)
2
and so P(k + 1) is true.
2. A proof using the weak form of mathematical induction:
Theorem: For all integers n ≥4, 2n < n!.
Proof :
Basis Step: For n = 4, 24 < 4! is true since 16 < 24.
Induction Step:
Let k be an integer, k ≥4, and suppose that 2k < k! is
true. The following shows that 2k+1 < (k + 1)! must also be true:
2k+1 = 2 · 2k < 2 · k! < (k + 1)k! = (k + 1)!.
3. A proof using the weak form of mathematical induction:
Theorem: For all integers n ≥8, n cents in postage can be made using only
3-cent and 5-cent stamps.
Proof :
Let P(n) be the predicate “n cents postage can be made using only
3-cent and 5-cent stamps”.
Basis Step:
P(8) is true since 8 cents in postage can be made using one
3-cent stamp and one 5-cent stamp.

Section 1.5
PROOF TECHNIQUES
61
Induction Step: Let k be an integer, k ≥8, and suppose that P(k) is true.
The following shows that P(k + 1) must also be true. If the pile of stamps
for k cents postage has in it any 5-cent stamps, then remove one 5-cent stamp
and replace it with two 3-cent stamps. If the pile for k cents postage has only
3-cent stamps, there must be at least three 3-cent stamps in the pile (since
k ̸= 3 or 6). Remove three 3-cent stamps and replace them with two 5-cent
stamps. In either case, a pile of stamps for k + 1 cents postage results.
4. A proof using an alternative form of mathematical induction (Fact 5):
Theorem: For all integers n ≥0, Fn < 2n. (Fk are Fibonacci numbers; see
§3.1.2.)
Proof :
Let P(n) be the predicate “Fn < 2n ”.
Basis Step:
P(0) and P(1) are both true since F0 = 0 < 1 = 20 and
F1 = 1 < 2 = 21.
Induction Step:
Let k be an integer, k ≥0, and suppose that P(k) and
P(k+1) are true. Then P(k+2) is also true: Fk+2 = Fk +Fk+1 < 2k +2k+1 <
2k+1 + 2k+1 = 2 · 2k+1 = 2k+2.
5. A proof using the strong form of mathematical induction:
Theorem: Every integer n ≥2 is divisible by some prime number.
Proof : Let P(n) be the sentence “n is divisible by some prime number”.
Basis Step: Since 2 is divisible by 2 and 2 is a prime number, P(2) is true.
Induction Step:
Let k be an integer with k > 2, and suppose that P(i)
is true (the induction hypothesis) for all integers i with 2 ≤i < k. That is,
suppose for all integers i with 2 ≤i < k that i is divisible by a prime number.
(It must now be shown that k is divisible by a prime number.)
Now either the number k is prime or k is not prime. If k is prime, then k is
divisible by a prime number, namely itself. If k is not prime, then k = a · b
where a and b are integers, with 2 ≤a < k and 2 ≤b < k. By the induction
hypothesis, the number a is divisible by a prime number p, and so k = ab is
also divisible by that prime p. Hence, regardless of whether k is prime or not,
k is divisible by a prime number.
6. A proof using the well-ordering principle:
Theorem: Every integer n ≥2 is divisible by some prime number.
Proof : Suppose, to the contrary, that there exists an integer n ≥2 that is
divisible by no prime number. Thus, the set S of all integers ≥2 that are
divisible by no prime number is nonempty.
Of course, no number in S is
prime, since every number is divisible by itself.
By the well-ordering principle for the integers, the set S contains a least element
k. Since k is not prime, there must exist integers a and b with 2 ≤a < k and
2 ≤b < k, such that k = a · b. Moreover, since k is the least element of the
set S and since both a and b are smaller than k, it follows that neither a nor b
is in S. Hence, the number a (in particular) must be divisible by some prime
number p. But then, since a is a factor of k, the number k is also divisible by
p, which contradicts the fact that k is in S. This contradiction shows that the
original supposition is false, or, in other words, that the theorem is true.
7. A proof using the well-ordering principle:

62
Chapter 1
FOUNDATIONS
Theorem: Every decreasing sequence of nonnegative integers is ﬁnite.
Proof : Suppose a1, a2, . . . is a decreasing sequence of nonnegative integers:
a1 > a2 > · · · . By the well-ordering principle, the set {a1, a2, . . .} contains a
least element an. This number must be the last in the sequence (and hence
the sequence is ﬁnite).
If an is not the last term, then an+1 < an, which
contradicts the fact that an is the smallest element.
1.5.5
DIAGONALIZATION ARGUMENTS
Deﬁnitions:
The diagonal of an inﬁnite list of sequences s1, s2, s3, . . . is the inﬁnite sequence whose
jth element is the jth entry of sequence sj.
A diagonalization proof is any proof that involves the diagonal of a list of sequences,
or something analogous to this.
Facts:
1. A diagonalization argument can be used to prove the existence of nonrecursive func-
tions.
2. A diagonalization argument can be used to prove that no computer algorithm can
ever be developed to determine whether an arbitrary computer program, given as input
with a given set of data, will terminate (the Turing Halting Problem).
3. A diagonalization argument can be used to prove that every mathematical theory
(under certain reasonable hypotheses) will contain statements whose truth or falsity is
impossible to determine within the theory (G¨odel’s Incompleteness Theorem).
Example:
1. A diagonalization proof :
Theorem: The set of real numbers between 0 and 1 is uncountable. (Georg
Cantor, 1845–1918).
Proof : Suppose, to the contrary, that the set of real numbers between 0 and
1 is countable. The decimal representations of these numbers can be written
in a list as follows:
0.a11a12a13 . . . a1n . . .
0.a21a22a23 . . . a2n . . .
0.a31a32a33 . . . a3n . . .
...
0.an1an2an3 . . . ann . . .
From this list, construct a new decimal number 0.b1b2b3 . . . bn . . . by specifying
that
bi =
(
5
if aii ̸= 5
6
if aii = 5.
For each integer i ≥1, 0.b1b2b3 . . . bn . . . diﬀers from the ith number in the
list in the ith decimal place, and hence 0.b1b2b3 . . . bn . . . is not in the list.
Consequently, no such listing of all real numbers between 0 and 1 is possible,
and hence, the set of real numbers between 0 and 1 is uncountable.

Section 1.6
AXIOMATIC PROGRAM VERIFICATION
63
1.6
AXIOMATIC PROGRAM VERIFICATION
Axiomatic program veriﬁcation is used to prove that a sequence of programming instruc-
tions achieves its speciﬁed objective. Semantic axioms for the programming language
constructs are used in a formal logic argument as rules of inference. Comments called
assertions, within the sequence of instructions, provide the main details of the argument.
The presently high expense of creating veriﬁed software can be justiﬁed for code that
is frequently reused, where the ﬁnancial beneﬁt is otherwise adequately large, or where
human life is concerned, for instance, in airline traﬃc control. This section presents a
representative sample of axioms for typical programming language constructs.
1.6.1
ASSERTIONS AND SEMANTIC AXIOMS
The correctness of a program can be argued formally based on a set of semantic axioms
that deﬁne the behavior of individual programming language constructs [Ap81], [Fl67],
[Ho69]. (Some alternative proofs of correctness use denotational semantics [Sc86], [St77]
or operational semantics [We72].) In addition, it is possible to synthesize code, using
techniques that permit the axioms to guide the selection of appropriate instructions
[Di76], [Gr13]. Code speciﬁcations and intermediate conditions are expressed in the form
of program assertions.
Deﬁnitions:
An assertion is a program comment containing a logical statement that constrains the
values of the computational variables.
These constraints are expected to hold when
execution ﬂow reaches the location of the assertion.
A semantic axiom for a type of programming instruction is a rule of inference that
prescribes the change of value of the variables of computation caused by the execution
of that type of instruction.
The assertion false represents an inconsistent set of logical conditions.
A computer
program cannot meet such a speciﬁcation.
Given two constraints A and B on computational variables, a statement that B follows
from A purely for reasons of logic and/or mathematics is called a logical implication.
The postcondition for an instruction or program fragment is the assertion that imme-
diately follows it in the program.
The precondition for an instruction or program fragment is the assertion that imme-
diately precedes it in the program.
The assertion true represents the empty set of logical conditions.
Notation:
1. To say that whenever the precondition {Apre} holds, the execution of a program frag-
ment called “Code” will cause the postcondition {Apost} to hold, the following notation
styles can be used:
• Horizontal notation: {Apre} Code {Apost}

64
Chapter 1
FOUNDATIONS
• Vertical notation: {Apre}
Code
{Apost}
• Flowgraph notation:
. . . Apre
Code
. . . Apost
2. Curly braces { . . . } enclose assertions in generic program code. They do not denote
a set.
3. Semantic axioms have a ﬁnite list of premises and a conclusion. They are represented
in the following format:
{Premise 1}
...
{Premise n}
- - - - - - - - - -
{Conclusion}
4. The circumstance that A logically implies B is denoted A ⇒B.
1.6.2
NOP, ASSIGNMENT, AND SEQUENCING AXIOMS
Formal axioms of pure mathematical consequence (no operation, from a computational
perspective) and of straight-line sequential ﬂow are used as auxiliaries to verify correct-
ness, even of sequences of simple assignment statements.
Deﬁnitions:
A NOP (“no-op”) is a (possibly empty) program fragment whose execution does not
alter the state of any computational variables or the sequence of ﬂow.
The Axiom of NOP states
{Apre} ⇒{Apost}
Premise 1
- - - - - - - - - - - - - -
{Apre} NOP {Apost}
Conclusion
Note: The Axiom of NOP is frequently applied to empty program fragments in order to
facilitate a clear logical argument.
An assignment instruction X := E; means that the variable X is to be assigned the
value of the expression E.
In a logical assertion A(X) with possible instances of the program variable X, the result
of replacing each instance of X in A by the program expression E is denoted A(X ←
E).

Section 1.6
AXIOMATIC PROGRAM VERIFICATION
65
The Axiom of Assignment states
{true}
No premises
- - - - - - - - - - - - - -
{A(X ←E)}X := E; {A(X)}
Conclusion
The following Axiom of Sequence provides that two consecutive instructions in the
program code are executed one immediately after the other:
{Apre} Code1 {Amid}
Premise 1
{Amid} Code2 {Apost}
Premise 2
- - - - - - - - - - - - - - - -
{Apre} Code1, Code2 {Apost}
Conclusion
(Commas are used as separators in program code.)
Examples:
1. Example of NOP: Suppose that X is a numeric program variable.
{X = 3} ⇒{X > 0}
mathematical fact
- - - - - - - - - - - - - - -
{X = 3} NOP {X > 0}
by Axiom of NOP
2. Suppose that X and Y are integer-type program variables. The Axiom of Assignment
alone implies correctness of all the following examples:
(a)
{X = 4}
X := X ∗2;
{X = 8}
A(X) is {X = 8}; E is X ∗2; A(X ←E) is {X ∗2 = 8}, which is equivalent to {X = 4}.
(b)
{true}
X := 2;
{X = 2}
A(X) is {X = 2}; E is 2; A(X ←E) is {2 = 2}, which is equivalent to {true}.
(c)
{(−9 < X) ∧(X < 0)}
Y := X;
{(−9 < Y ) ∧(Y < 0)}
A(Y ) is {(−9 < Y ) ∧(Y < 0)}; E is X; A(Y ←E) is {(−9 < X) ∧(X < 0)}.
(d)
{Y = 1}
X := 0;
{Y = 1}
A(X) is {Y = 1}; E is 0; A(X ←E) is {Y = 1}.
(e)
{false}
X := 8;
{X = 2}
A(X) is {X = 2}; E is 8; A(X ←E) is {8 = 2}, which is equivalent to {false}.
3. Examples of sequence:
(a) {X = 1} X := X + 1; {X > 0}
i.
{X = 1} ⇒{X > −1}
mathematics
ii.
{X = 1} NOP {X > −1}
Axiom of NOP
iii.
{X > −1} X := X + 1; {X > 0}
Axiom of Assignment
iv.
{X = 1} NOP, X := X + 1; {X > 0}
Axiom of Sequence ii, iii
v.
{X = 1} X := X + 1; {X > 0}
deﬁnition of NOP.
(b) {Y = a ∧X = b} Z := Y ; Y := X; X := Z; {X = a ∧Y = b}
i.
{Y = a ∧X = b} Z := Y ; {Z = a ∧X = b}
Axiom of Assignment
ii.
{Z = a ∧X = b} Y := X; {Z = a ∧Y = b}
Axiom of Assignment
iii.
{Y = a ∧X = b} Z := Y, Y := X,
Axiom of Sequence
{Z = a ∧Y = b}
i, ii
iv.
{Z = a ∧Y = b} X := Z; {X = a ∧Y = b}
Axiom of Assignment
v.
{Y = a ∧X = b} Z := Y, Y := X, X := Z,
Axiom of Sequence
{X = a ∧Y = b}
iii, iv.

66
Chapter 1
FOUNDATIONS
1.6.3
AXIOMS FOR CONDITIONAL EXECUTION CONSTRUCTS
Deﬁnitions:
A conditional assignment construct is any type of program instruction containing
a logical condition and an imperative clause such that the imperative clause is to be
executed if and only if the logical condition is true. Some types of conditional assignment
contain more than one logical condition and more than one imperative clause.
An if-then instruction if IfCond then ThenCode has one logical condition (which
follows the keyword if) and one imperative clause (which follows the keyword then).
The Axiom of If-then states
{Apre ∧IfCond} ThenCode {Apost}
Premise 1
{Apre ∧¬IfCond} ⇒{Apost}
Premise 2
- - - - - - - - - - - - - - - - - - - - - - - - - - - -
{Apre} if IfCond then ThenCode {Apost}
Conclusion
. . . Apre
. . . Apost
true
IfCond
ThenCode
false
An if-then-else instruction if IfCond then ThenCode else ElseCode has one logical
condition, which follows the keyword if, and two imperative clauses, one after the keyword
then, and the other after the keyword else.
The Axiom of If-then-else states
{Apre ∧IfCond} ThenCode {Apost}
Premise 1
{Apre ∧¬IfCond} ElseCode {Apost}
Premise 2
- - - - - - - - - - - - - - - - - - - - - - - - -
{Apre} if IfCond then ThenCode else ElseCode {Apost}
Conclusion
Examples:
1. If-then:
{true} if X = 3 then Y := X; {X = 3 →Y = 3}

Section 1.6
AXIOMATIC PROGRAM VERIFICATION
67
. . . Apre
true
ThenCode
IfCond
false
ElseCode
. . . Apost
i. {X = 3} Y := X; {X = 3 ∧Y = 3}
Axiom of Assignment
ii. {X = 3 ∧Y = 3} NOP {(X = 3) →(Y = 3)}
Axiom of NOP
(Step ii uses a logic fact: p ∧q ⇒p →q)
iii. {X = 3} Y := X; {X = 3 →Y = 3}
Axiom of Sequence i, ii
(Step iii establishes Premise 1 for Axiom of If-then)
iv. {¬(X = 3)} ⇒{X = 3 →Y = 3}
Logic fact
(Step iv establishes Premise 2 for Axiom of If-then)
v. {true} if X = 3 then Y := X; {X = 3 →Y = 3}
Axiom of If-then iii, iv.
2. If-then-else:
{X > 0}
if (X > Y ) then M := X; else M := Y ;
{(X > 0) ∧(X > Y →M = X) ∧(X ≤Y →M = Y )}
i. {X > 0 ∧X > Y } M := X; {X > 0 ∧(X > Y →M = X) ∧(X ≤Y →M = Y )}
by Axiom of Assignment and Axiom of NOP (establishes Premise 1)
ii. {X > 0 ∧¬(X > Y )} M := Y ; {X > 0 ∧(X > Y →M = X) ∧(X ≤Y →M = Y )}
by Axiom of Assignment and Axiom of NOP (establishes Premise 2)
iii. Conclusion now follows from Axiom of If-then-else.
1.6.4
AXIOMS FOR LOOP CONSTRUCTS
Deﬁnitions:
A while-loop instruction while WhileCond do LoopBody has one logical condition
called the while-condition, which follows the keyword while, and a sequence of instruc-
tions called the loop-body. At the outset of execution, the while condition is tested for
its truth value. If it is true, then the loop body is executed. This two-step process of
test and execute continues until the while condition becomes false, after which the ﬂow
of execution passes to whatever program instruction follows the while-loop.
A loop is weakly correct if whenever the precondition is satisﬁed at the outset of
execution and the loop is executed to termination, the resulting computational state
satisﬁes the postcondition.

68
Chapter 1
FOUNDATIONS
A loop is strongly correct if it is weakly correct and if whenever the precondition is
satisﬁed at the outset of execution, the computation terminates.
The Axiom of While deﬁnes weak correctness of a while-loop (i.e., the axiom ignores
the possibility of an inﬁnite loop) in terms of a logical condition called the loop invariant
denoted “LoopInv” satisfying the following condition:
{Apre} ⇒{LoopInv}
“Initialization” Premise
{LoopInv ∧WhileCond} LoopBody {LoopInv}
“Preservation” Premise
{LoopInv ∧¬WhileCond} ⇒{Apost}
“Finalization” Premise
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
{Apre} while {LoopInv} WhileCond do LoopBody {Apost}
Conclusion
. . . Apre
. . . Looplnv
WhileCond
false
true
LoopBody
Apost . . .
Example:
1. Suppose that J, N, and P are integer-type program variables.
{Apre : J = 0 ∧P = 1 ∧N ≥0}
while {LoopInv : P = 2J ∧J ≤N} (J < N) do
P := P ∗2;
J := J + 1;
endwhile
{Apost : P = 2N}
i.
{Apre : J = 0 ∧P = 1 ∧N ≥0} ⇒{LoopInv : P = 2J ∧J ≤N}
Initialization Premise trivially true by mathematics
ii.
{LoopInv ∧WhileCond : (P = 2J ∧J ≤N) ∧(J < N)}
P := P ∗2;
J := J + 1;
{LoopInv : P = 2J ∧J ≤N}
Preservation Premise proved by using Axiom of Assignment twice
and Axiom of Sequence
iii.
{LoopInv ∧¬WhileCond : (P = 2J ∧J ≤N) ∧¬(J < N)} ⇒{Apost : P = 2N}
Finalization Premise provable by mathematics
iv.
Conclusion now follows from Axiom of While.

Section 1.6
AXIOMATIC PROGRAM VERIFICATION
69
Fact:
1. Proof of termination of a loop is usually achieved by mathematical induction.
1.6.5
AXIOMS FOR SUBPROGRAM CONSTRUCTS
The parameterless procedure is the simplest subprogram construct.
Procedures with
parameters and functional subprograms have somewhat more complicated semantic ax-
ioms.
Deﬁnitions:
A procedure is a sequence of instructions that lies outside the main sequence of in-
structions in a program. It consists of a procedure name, followed by a procedure
body.
A call instruction call ProcName is executed by transferring control to the ﬁrst exe-
cutable instruction of the procedure ProcName.
A return instruction causes a procedure to transfer control to the executable instruction
immediately following the most recently executed call to that procedure. An implicit
return is executed after the last instruction in the procedure body is executed. It is good
programming style to put a return there.
In the following Axiom of Procedure (parameterless), Apre and Apost are the
precondition and postcondition of the instruction call ProcName; ProcPre and ProcPost
are the precondition and postcondition of the procedure whose name is ProcName.
{Apre} ⇒{ProcPre}
“Call” Premise
{ProcPre} ProcBody {ProcPost}
“Body” Premise
{ProcPost} ⇒{Apost}
“Return” Premise
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
{Apre} call ProcName; {Apost}
Conclusion
. . . Apre
. . . ProcPre
ProcBody
. . . ProcPost
. . . Apost

70
Chapter 1
FOUNDATIONS
1.7
LOGIC-BASED COMPUTER PROGRAMMING PARADIGMS
Mathematical logic is the basis for several diﬀerent computer software paradigms. These
include logic programming, fuzzy reasoning, production systems, artiﬁcial intelligence,
and expert systems.
1.7.1
LOGIC PROGRAMMING
A computer program in the imperative paradigm (familiar in languages like C, BASIC,
FORTRAN, and ALGOL) is a list of instructions that describes a precise sequence of
actions that a computer should perform. To initiate a computation, one supplies the
iterative program plus speciﬁc input data to the computer. Logic programming provides
an alternative paradigm in which a program is a list of “clauses”, written in predicate
logic, that describe an allowed range of behavior for the computer. To initiate a com-
putation, the computer is supplied with the logic program plus another clause called a
“goal”. The aim of the computation is to establish that the goal is a logical consequence
of the clauses constituting the logic program. The computer simpliﬁes the goal by exe-
cuting the program repeatedly until the goal becomes empty, or until it cannot be further
simpliﬁed.
Deﬁnitions:
A term in a domain S is either a ﬁxed element of S or an S-valued variable.
An n-ary predicate on a set S is a function P : Sn →{T, F}.
An atomic formula (or atom) is an expression of the form P(t1, . . . , tn), where n ≥0,
P is an n-ary predicate, and t1, . . . , tn are terms.
A formula is a logical expression constructed from atoms with conjunctions, disjunctions,
and negations, possibly with some logical quantiﬁers.
A substitution for a formula is a ﬁnite set of the form {v1/t1, . . . , vn/tn}, where each
vi is a distinct variable, and each ti is a term distinct from vi.
The instance of a formula ψ using the substitution θ = {v1/t1, . . . , vn/tn} is the formula
obtained from ψ by simultaneously replacing each occurrence of the variable vi in ψ by
the term ti. The resulting formula is denoted by ψθ.
A closed formula in logic programming is a program without any free variables.
A ground formula is a formula without any variables at all.
A clause is a formula of the form ∀x1 . . . ∀xs(A1 ∨· · · ∨An ←B1 ∧· · · ∧Bm) with no
free variables, where s, n, m ≥0, and A’s and B’s are atoms. In logic programming, such
a clause may be denoted by A1, . . . , An ←B1, . . . , Bm.
The head of a clause A1, . . . , An ←B1, . . . , Bm is the sequence A1, . . . , An.
The body of a clause A1, . . . , An ←B1, . . . , Bm is the sequence B1, . . . , Bm.
A deﬁnite clause is a clause of the form A ←B1, . . . , Bm or ←B1, . . . , Bm, which
contains at most one atom in its head.
An indeﬁnite clause is a clause that is not deﬁnite.

Section 1.7
LOGIC-BASED COMPUTER PROGRAMMING PARADIGMS
71
A logic program is a ﬁnite sequence of deﬁnite clauses.
A goal is a deﬁnite clause ←B1, . . . , Bm whose head is empty. (Prescribing a goal for
a logic program P tells the computer to derive an instance of that goal by manipulating
the logical clauses in P.)
An answer to a goal G for a logic program P is a substitution θ such that Gθ is a logical
consequence of P.
A deﬁnite answer to a goal G for a logic program P is an answer in which every variable
is substituted by a constant.
Facts:
1. A deﬁnite clause A ←B1, . . . , Bm represents the following logical constructs:
If every Bi is true, then A is also true;
Statement A can be proved by proving every Bi.
2. Deﬁnite answer property:
If a goal G for a logic program P has an answer, then it
has a deﬁnite answer.
3. The deﬁnite answer property does not hold for indeﬁnite clauses. For example, al-
though G = ∃xQ(x) is a logical consequence of P = {Q(a), Q(b) ←}, no ground instance
of G is a logical consequence of P.
4. Logic programming is Turing-complete (§17.5); i.e., any computable function can be
represented using a logic program.
5. Building on the work of logician J. Alan Robinson in 1965, computer scientists Robert
Kowalski and Alain Colmerauer of Imperial College and the University of Marseille-
Aix, respectively, in 1972 independently developed the programming language PROLOG
(PROgramming in LOGic) based on a special subset of predicate logic.
6. The ﬁrst PROLOG interpreter was implemented in ALGOL-W in 1972 at the Uni-
versity of Marseille-Aix. Since then, several variants of PROLOG have been introduced,
implemented, and used in practical applications. The basic paradigm behind all these
languages is called Logic Programming.
7. In PROLOG, the relation “is” means equality.
Examples:
1. The following three clauses are deﬁnite:
P ←Q, R
P ←
←Q, R.
2. The clause P, S ←Q, R is indeﬁnite.
3. The substitution {X/a, Y/b} for the atom P(X, Y, Z) yields the instance P(a, b, Z).
4. The goal ←P to the program {P ←} has a single answer, given by the empty
substitution. This means the goal can be achieved.
5. The goal ←P to the program {Q ←} has no answer. This means it cannot be derived
from that program.
6. The logic program consisting of the following two deﬁnite clauses P1 and P2 computes
a complete list of the pairs of vertices in an arbitrary graph that have a path joining them:
P1. path(V, V ) ←
P2. path(U, V ) ←path(U, W), edge(W, V )

72
Chapter 1
FOUNDATIONS
Deﬁnite clauses P3 and P4 comprise a representation of a graph with nodes 1, 2, and 3,
and edges (1, 2) and (2, 3):
P3. edge(1,2) ←
P4. edge(2,3) ←
The goal G represents a query asking for a complete list of the pairs of vertices in an
arbitrary graph that have a path joining them:
G. ←path(Y, Z)
There are three distinct answers of the goal G to the logic program consisting of
deﬁnite clauses P1 to P4, corresponding to the paths (1, 2), (1, 2, 3), and (2, 3), respec-
tively:
A1. {Y/1, Z/2}
A2. {Y/1, Z/3}
A3. {Y/2, Z/3}
7. The following logic program computes the Fibonacci sequence 0, 1, 1, 2, 3, 5, 8, 13, . . .,
where the predicate fib(N, X) is true if X is the Nth number in the Fibonacci sequence:
fib(0, 0) ←
fib(1, 1) ←
fib(N, X + Y ) ←N > 1, fib(N −1, X), fib(N −2, Y )
The goal “←fib(6, X)” is answered {X/8}, the goal “←fib(X, 8)” is answered
{X/6}, and the goal “←fib(N, X)” has the following inﬁnite sequence of answers:
{N/0, X/0}
{N/1, X/1}
{N/2, X/1}
...
8. Consider the problem of ﬁnding an assignment of digits (integers 0, 1, . . ., 9) to letters
such that adding two given words produces the third given word, as in this example:
S
E
N
D
+
M
O
R
E
M
O
N
E
Y
One solution to this particular puzzle is given by the following assignment:
D = 0, E = 0, M = 1, N = 0, O = 0, R = 0, S = 9, Y = 0.
The following PROLOG program solves all such puzzles:
between(X, X, Z) ←X < Z.
between(X, Y, Z) ←between(K, Y, Z), X is K −1.
val([ ], 0) ←.
val([X|Y ], A) ←val(Y, B), between(0, X, 9), A is 10 ∗B + X.
solve(X, Y, Z) ←val(X, A), val(Y, B), val(Z, C), C is A + B.
The speciﬁc example given above is captured by the following goal:
←solve([D, N, E, S], [E, R, O, M], [Y, E, N, O, M]).

Section 1.7
LOGIC-BASED COMPUTER PROGRAMMING PARADIGMS
73
The predicate between(X, Y, Z) means X ≤Y ≤Z. The predicate val(L, N) means that
the number N is the value of L, where L is the kind of list of letters that occurs on a
line of these puzzles. The notation [X|L] means the list obtained by writing list L after
item X. The predicate solve(X, Y, Z) means that the value of list Z equals the sum of
the values of list X and list Y .
This example illustrates the ease of writing logic programs for some problems where
conventional imperative programs are more diﬃcult to write.
1.7.2
FUZZY SETS AND LOGIC
Fuzzy set theory and fuzzy logic are used to model imprecise meanings, such as “tall”,
that are not easily represented by predicate logic. In particular, instead of assigning
either “true” or “false” to the statement “John is tall”, fuzzy logic assigns a real number
between 0 and 1 that indicates the degree of “tallness” of John. Fuzzy set theory assigns
a real number between 0 and 1 to John that indicates the extent to which he is a member
of the set of tall people. See [Ka86], [Ka92], [KaLa94], [YaFi94], [YaZa94], [Za65], [Zi01].
Deﬁnitions:
A fuzzy set F = (X, µ) consists of a set X (the domain) and a membership function
µ: X →[0, 1]. Sometimes the set is written { (x, µ(x)) | x ∈X } or { µ(x) x | x ∈X }.
The fuzzy intersection of fuzzy sets (A, µA) and (B, µB) is the fuzzy set A ∩B with
domain A ∩B and membership function µA∩B(x) = min(µA(x), µB(x)).
The fuzzy union of fuzzy sets (A, µA) and (B, µB) is the fuzzy set A ∪B with domain
A ∪B and membership function µA∪B(x) = max(µA(x), µB(x)).
The fuzzy complement of the fuzzy set (A, µ) is the fuzzy set ¬A or A with domain A
and membership function µA(x) = 1 −µ(x).
The nth constructor con(µ, n) of a membership function µ is the function µn. That
is, con(µ, n)(x) = (µ(x))n.
The nth dilutor dil(µ, n) of a membership function µ is the function µ1/n. That is,
dil(µ, n)(x) = (µ(x))1/n.
A T-norm operator is a function f : [0, 1]× [0, 1] →[0, 1] with the following properties:
• f(x, y) = f(y, x)
commutativity
• f(f(x, y), z) = f(x, f(y, z))
associativity
• if x ≤v and y ≤w, then f(x, y) ≤f(v, w)
monotonicity
• f(a, 1) = a.
1 is a unit element
The fuzzy intersection A ∩f B of fuzzy sets (A, µA) and (B, µB) relative to the
T-norm operator f is the fuzzy set with domain A ∩B and membership function
µA∩fB(x) = f(µA(x), µB(x)).
An S-norm operator is a function f : [0, 1]×[0, 1] →[0, 1] with the following properties:
• f(x, y) = f(y, x)
commutativity
• f(f(x, y), z) = f(x, f(y, z))
associativity
• if x ≤v and y ≤w, then f(x, y) ≤f(v, w)
monotonicity
• f(a, 1) = 1.

74
Chapter 1
FOUNDATIONS
The fuzzy union A ∪f B of fuzzy sets (A, µA) and (B, µB) relative to the S-norm
operator f is the fuzzy set with domain A ∪B and membership function µA∪fB(x) =
f(µA(x), µB(x)).
A complement operator is a function f : [0, 1] →[0, 1] with the following properties:
• f(0) = 1
• if x < y then f(x) > f(y)
• f(f(x)) = x.
The fuzzy complement ¬fA of the fuzzy set (A, µ) relative to the complement
operator f is the fuzzy set with domain A and membership function µ¬f (x) = f(µ(x)).
A fuzzy system consists of a base collection of fuzzy sets, intersections, unions, com-
plements, and implications.
A hedge is a monadic operator corresponding to linguistic adjectives such as “very”,
“about”, “somewhat”, or “quite” that modify membership functions.
A two-valued logic is a logic where each statement has exactly one of the two values:
true or false.
A multi-valued logic (n-valued logic) is a logic with a set of n (≥2) truth values; i.e.,
there is a set of n numbers v1, v2, . . . , vn ∈[0, 1] such that every statement has exactly
one truth value vi.
Fuzzy logic is the study of statements where each statement has assigned to it a truth
value in the interval [0, 1] that indicates the extent to which the statement is true.
If statements p and q have truth values v1 and v2 respectively, the truth value of p ∨q
is max(v1, v2), the truth value of p ∧q is min(v1, v2), and the truth value of ¬p is 1 −v1.
Facts:
1. Fuzzy set theory and fuzzy logic were developed by Lofti Zadeh in 1965.
2. Fuzzy set theory and fuzzy logic are parallel concepts: given a predicate P(x), the
fuzzy truth value of the statement P(a) is the fuzzy set value assigned to a as an element
of { x | P(x) }.
3. The usual minimum function min(x, y) is a T-norm. The usual real maximum func-
tion max(x, y) is an S-norm. The function c(x) = 1 −x is a complement operator.
4. Several other kinds of T-norms, S-norms, and complement operators have been de-
ﬁned.
5. The words “T-norm” and “S-norm” come from multi-valued logics.
6. The only diﬀerence between T-norms and S-norms is that the T-norm speciﬁes
f(a, 1) = a, whereas the S-norm speciﬁes f(a, 1) = 1.
7. Several standard classes of membership functions have been deﬁned, including step,
sigmoid, and bell functions.
8. Constructors and dilutors of membership functions are also membership functions.
9. The large number of practical applications of fuzzy set theory can generally be di-
vided into three types: machine systems, human-based systems, human-machine systems.
Some of these applications are based on fuzzy set theory alone and some on a variety
of hybrid conﬁgurations involving neurofuzzy approaches, or in combination with neural
networks, genetic algorithms, or case-based reasoning.

Section 1.7
LOGIC-BASED COMPUTER PROGRAMMING PARADIGMS
75
10. The ﬁrst fuzzy expert system that set a trend in practical fuzzy thinking was the
design of a cement kiln called Linkman, produced by Blue Circle Cement and SIRA in
Denmark in the early 1980s. The system incorporates the experience of a human operator
in a cement production facility.
11. The Sendai Subway Automatic Train Operations Controller was designed by Hi-
tachi in Japan.
In that system, speed control during cruising, braking control near
station zones, and switching of control are determined by fuzzy IF-THEN rules that pro-
cess sensor measurements and consider factors related to travelers’ comfort and safety.
In operation since 1986, this most celebrated application encouraged many applications
based on fuzzy set controllers in the areas of home appliances (refrigerators, vacuum
cleaners, washers, dryers, rice cookers, air conditioners, shavers, blood-pressure measur-
ing devices), video cameras (including fuzzy automatic focusing, automatic exposure,
automatic white balancing, image stabilization), automotive (fuzzy cruise control, fuel
injection, transmission and brake systems), robotics, and aerospace.
12. Applications to ﬁnance started with the Yamaichi Fuzzy Fund, which is a fuzzy
trading system. This was soon followed by a variety of ﬁnancial applications world-wide.
13. Research activities will soon result in commercial products related to the use of fuzzy
set theory in the areas of audio and video data compression (such as HDTV), robotic
arm movement control, computer vision, coordination of visual sensors with mechanical
motion, aviation (such as unmanned platforms), and telecommunication.
14. Current status:
Most applications of fuzzy sets and logic are directly related to
structured numerical model-free estimators. Presently, most applications are designed
with linguistic variables, where proper levels of granularity are being used in the evalu-
ations of those variables, expressing the ambiguity and subjectivity in human thinking.
Fuzzy systems capture expert knowledge and through the processing of fuzzy IF-THEN
rules are capable of processing knowledge combining the antecedents of each fuzzy rule,
calculating the conclusions, and aggregating them to the ﬁnal decision.
15. One way to model fuzzy implication A →B is to deﬁne A →B as ¬cA∪f B relative
to some complement operator c and to some S-norm operator f. Several other ways have
also been considered.
16. A fuzzy system is used computationally to control the behavior of an external sys-
tem.
17. Large fuzzy systems have been used in specifying complex real-world control sys-
tems.
The success of such systems depends crucially on the speciﬁc engineering pa-
rameters.
The correct values of these parameters are usually obtained by trial-and-
readjustment.
18. A two-valued logic is a logic that assumes the law of the excluded middle: p ∨¬p is
a tautology.
19. Every n-valued logic is a fuzzy logic.
Examples:
1. A committee consisting of ﬁve people met ten times during the past year. Person A
attended 7 meetings, B attended all 10 meetings, C attended 6 meetings, D attended no
meetings, and E attended 9 meetings. The set of committee members can be described
by the following fuzzy set that reﬂects the degree to which each of the members attended
meetings, using the function µ: {A, B, C, D, E} →[0, 1] with the rule µ(x) =
1
10(number
of meetings attended):
{(A, 0.7), (B, 1.0), (C, 0.6), (D, 0.0), (E, 0.9)},

76
Chapter 1
FOUNDATIONS
which can also be written as
{0.7A, 1.0B, 0.6C, 0.0D, 0.9E}.
Person B would be considered a “full” member and person D a “nonmember”.
2. Four people are rated on amount of activity in a political party, yielding the fuzzy
set
P1 = {0.8A, 0.45B, 0.1C, 0.75D},
and based on their degree of conservatism in their political beliefs, as
P2 = {0.6A, 0.85B, 0.7C, 0.35D}.
The fuzzy union of the sets is
P1 ∪P2 = {0.8A, 0.85B, 0.7C, 0.75D},
the fuzzy intersection is
P1 ∩P2 = {0.6A, 0.45B, 0.1C, 0.35D}
and the fuzzy complement of P1 (measurement of political inactivity) is
P1 = {0.2A, 0.55B, 0.9C, 0.25D}.
3. In the fuzzy set with domain T and membership function
µT (h) =





0
if h ≤170
h−170
20
if 170 < h < 190
1
otherwise
the number 160 is not a member, the number 195 is a member, and the membership
of 182 is 0.6. The graph of µT is given in the following ﬁgure.
µT(h)
h
170
190
1
0
Tall
4. The fuzzy set (T, µT ) of Example 3 can be used to deﬁne the fuzzy set “Tall” =
(H, µH) of tall people, by the rule µH(x) = µT (height(x)) where height(x) is the height
of person x calibrated in centimeters.
5. The second constructor con(µH, 2) of the fuzzy set “Tall” can be used to deﬁne a
fuzzy set “Quite tall”, whose graph is given in the following ﬁgure.
6. The second dilutor dil(µH, 2) of the fuzzy set “Tall” deﬁnes the fuzzy set “Somewhat
tall”, whose graph is given in the following ﬁgure.

Section 1.7
LOGIC-BASED COMPUTER PROGRAMMING PARADIGMS
77
µ(h)
h
170
190
1
0
Quite Tall
µ(h)
h
170
190
1
0
Somewhat Tall
7. The concept of “being healthy” can be modeled using fuzzy logic. The truth value 0.95
could be assigned to “Fran is healthy” if Fran is almost always healthy.
The truth
value 0.4 could be assigned to “Leslie is healthy” if Leslie is healthy somewhat less than
half the time. The truth of the statements “Fran and Leslie are healthy” would be 0.4
and “Fran is not healthy” would be 0.05.
8. Behavior closed-loop control systems: The behavior of some closed-loop control sys-
tems can be speciﬁed using fuzzy logic.
For example, consider an automated heater
whose output setting is to be based on the readings of a temperature sensor. A fuzzy set
“cold” and the implication “very cold →high” could be used to relate the temperature
to the heater settings. The exact behavior of this system is determined by the degree
of the constructor used for “very” and by the speciﬁc choices of S-norm and comple-
ment operators used to deﬁne the fuzzy implication—the “engineering parameters” of
the system.
1.7.3
PRODUCTION SYSTEMS
Production systems are a logic-based computer programming paradigm introduced by
Allen Newell and Herbert Simon in 1975. They are commonly used in intelligent systems
for representing an expert’s knowledge used in solving some real-world task, such as a
physician’s knowledge of making medical diagnoses.
Deﬁnitions:
A fact set is a set of ground atomic formulas. These formulas represent the information
relevant to the system.
A condition is a disjunction A1 ∨· · · ∨An, where n ≥0 and each Ai is a literal.
A condition C is true in a fact set S if:

78
Chapter 1
FOUNDATIONS
• C is empty, or
• C is a positive literal and C ∈S, or
• C is a negative literal ¬A, and B ̸∈S for each ground instance B of A, or
• C = A1 ∨· · · ∨An, and some condition Ai is true in S.
A print command “print(x)”, means that the value of the term x is to be printed.
An action is either a literal or a print command.
A production rule is of the form C1, . . . , Cn →A1, . . . , Am, where n, m ≥1, each Ci
is a condition, each Ai is an action, and each variable in each action appears in some
positive literal in some condition.
The antecedent of the rule C1, . . . , Cn →A1, . . . , Am is C1, . . . , Cn.
The consequent of the rule C1, . . . , Cn →A1, . . . , Am is A1, . . . , Am.
An instantiation of a production rule is the rule obtained by replacing each variable in
each positive literal in each condition of the rule by a constant.
A production system consists of a fact set and a set of production rules.
Facts:
1. Given a fact set S, an instantiation C1, . . . , Cn →A1, . . . , Am of a production rule
denotes the following operation:
if each condition Ci is true in S then
for each Ai:
if Ai is an atom, add it to S
if Ai is a negative literal ¬B, then remove B from S
if Ai is “print(c)”, then print c.
2. In addition to “print”, production systems allow several other system-level com-
mands.
3. OPS5 and CLIPS are currently the most popular languages for writing production
systems. They are available for most operating systems, including UNIX and DOS.
4. To initialize a computation prescribed by a production system, the initial fact set
and all the production rules are supplied as input. The command “run1” non-deter-
ministically selects an instantiation of a production rule such that all conditions in the
antecedent hold in the fact set, and it “ﬁres” the rule by carrying out the actions in the
consequent. The command “run” keeps on selecting and ﬁring rules until no more rule
instantiations can be selected.
5. Production systems are Turing complete.
Examples:
1. The fact set S = {N(3), 3 > 2, 2 > 1} may represent that “3 is a natural number”,
that “3 is greater than 2”, and that “2 is greater than 1”.
2. If the fact set S of Example 1 and the production N(x) →print(x) are supplied as
input, the command “run” will yield the instantiation N(3) →print(3) and ﬁre it to
print 3.
3. The production rule N(x), x > y →¬N(x), N(y) has N(3), 3 > 2 →¬N(3), N(2)
as an instantiation. If operated on fact set S of Example 1, this rule will change S to
{3 > 2, 2 > 1, N(2)}.

Section 1.7
LOGIC-BASED COMPUTER PROGRAMMING PARADIGMS
79
4. The production system consisting of the following two production rules can be used
to add a set of numbers in a fact set:
¬S(x) →S(0)
S(x), N(y) →¬S(x), ¬N(y), S(x + y).
For example, starting with the fact set {N(1), N(2), N(3), N(4)}, this production system
will produce the fact set {S(10)}.
1.7.4
AUTOMATED REASONING
Computers have been used to help prove theorems by verifying special cases. But even
more, they have been used to carry out reasoning without external intervention. De-
veloping computer programs that can draw conclusions from a given set of facts is the
goal of automated reasoning. There are now automated reasoning programs that can
prove results that people have not been able to prove. Automated reasoning can help
in verifying the correctness of computer programs, verifying protocol design, verifying
hardware design, creating software using logic programming, solving puzzles, and proving
new theorems.
Deﬁnitions:
Automated reasoning is the process of using a computer program to draw conclusions
which follow logically from a set of given facts.
Automated theorem proving is the use of automated reasoning to prove theorems.
A proof assistant is a computer program that attempts to prove a theorem with the
help of user input.
A computer-assisted proof is a proof that relies on checking the validity of a large
number of cases using a special-purpose computer program.
A proof done by hand is a proof done by a human without the use of a computer.
Facts:
1. Computer-assisted proofs have been used to settle several well-known conjectures,
including the Four Color Theorem (§8.6.4), the nonexistence of a ﬁnite projective plane
of order 10 (§12.2.3), and the densest packing of unit balls in R3 (§13.2.1).
2. The computer-assisted proofs of both the Four Color Theorem and the nonexistence
of a ﬁnite projective plane of order 10 rely on having a computer verify certain facts
about a large number of cases using special-purpose software.
3. Hardware, system software, and special-purpose program errors can invalidate a
computer-assisted proof.
This makes the veriﬁcation of computer-assisted proofs im-
portant. However, such veriﬁcation may be impractical.
4. Automated reasoning software has been developed for both ﬁrst-order and higher-
order logics. A database of automated reasoning systems can be found at
• http://www-formal.stanford.edu/clt/ARS/systems.html
5. Automated theorem provers in ﬁrst-order logic are computer programs that, when
given a statement in predicate logic (a conjecture), attempt to ﬁnd a proof that the
statement is always true (and hence, a theorem) or that is sometimes false. Generally,
the search for the proof is automatic, that is, without any intervention by a user.

80
Chapter 1
FOUNDATIONS
6. Higher-order logic (HOL) extends ﬁrst-order logic (FOL) by allowing the use of quan-
tiﬁers over functions and predicates. Computer programs designed to ﬁnd proofs of state-
ments in HOL are often proof assistants rather than automated theorem provers. These
systems rely on user input to help guide their operation.
7. The class of problems an automated theorem prover is designed to solve is known as
the problem domain of this solver. Some automated theorem provers, such as most FOL
provers, have a wide problem domain, but other automated theorem provers are built to
solve problems in speciﬁc domains or of a speciﬁc form.
8. Proofs by mathematical induction pose particular problems for automated proof sys-
tems, because of the diﬃculty in searching how to apply an induction hypotheses, the
problem of identifying all cases required for the basis step of a proof, and complications
from the use of recursion. Automated proof systems have been built for mathematical
induction with varying degrees of success. (See Example 4.)
9. Automated reasoning software has been used to prove new results in many areas, in-
cluding settling long-standing, well-known open conjectures (such as the Robbins problem
described in Example 2).
10. Proofs generated by automated reasoning software can usually be checked without
using computers or by using software programs that check the validity of proofs.
11. Proofs done by humans often use techniques ill-suited for implementation in auto-
mated proof software.
12. Automated proof systems rely on proof procedures suitable for computer implemen-
tation, such as resolution and the semantic tableaux procedure. (See [Fi13] or [Wo96]
for details.)
13. The eﬀectiveness of automated proof systems depends on following strategies that
help programs prove results eﬃciently.
14. Restriction strategies are used to block paths of reasoning that are considered to be
unpromising.
15. Direction strategies are used to help programs select the approaches to take next.
16. Look-ahead strategies let programs draw conclusions before they would ordinarily
be drawn following the basic rules of the program.
17. Redundancy-control strategies are used to eliminate some of the redundancy in
retained information.
18. Several eﬀorts have been focused on capturing mathematical knowledge that has
been proved by automated reasoning into a database that can be used in automated
reasoning systems. (See Examples 6-7.)
19. Automated theorem proving and automated reasoning are useful for verifying that
computer programs are correct and that hardware incorporates the functions for which
it is intended. Automated theorem proving can also be used to establish that network
protocols are secure.
20. To learn more about automated reasoning and automated theorem proving, consult
[Fi13] and [RoVo01].
Examples:
1. The OTTER system (now known as Prover9) is an automated reasoning system based
on resolution for ﬁrst-order logic that was developed at Argonne National Laboratory
[Wo96]. It was used to establish many previously unknown results in a wide variety
of areas, including algebraic curves, lattices, Boolean algebra, groups, semigroups, and
logic. A summary of these results can be found at

REFERENCES
81
• http://www.cs.unm.edu/~mccune/prover9
2. The automated reasoning system EQP, developed at Argonne National Laboratory,
settled the Robbins problem in 1996. This problem was ﬁrst proposed in the 1930s by
Herbert Robbins and was actively worked on by many mathematicians. The Robbins
problem can be stated as follows. Can the equivalence
¬(¬p) ⇔p
be derived from the commutative and associative laws for the “or” operator ∨and the
identity
¬(¬(p ∨q) ∨¬(p ∨¬q)) ⇔p?
The EQP system, using some earlier work that established a suﬃcient condition for the
truth of Robbins’ problem, found a 15-step proof of the theorem after approximately 8
days of searching on a UNIX workstation when provided with one of several diﬀerent
search strategies.
3. Isabelle is an interactive theorem prover developed at the University of Cambridge
and Technische Universit¨at M¨unchen. Isabelle can be used to encode ﬁrst-order logic,
higher-order logic, Zermelo-Fraenkel set theory, and other logics. Isabelle’s main proof
method is a higher-order version of resolution. Isabelle is interactive and also features
eﬃcient automatic reasoning tools. It comes with a large library of formally veriﬁed
proofs from number theory, analysis, algebra, and set theory. It can be accessed at
• https://isabelle.in.tum.de/index.html
4. The Boyer-Mayer theorem prover (NQTHM) has had some notable successes with
automating the proofs of theorems using mathematical induction. (See [BoMo79].)
5. The WZ computer-assisted and automated theorem prover, developed by H. Wilf
and D. Zeilberger, has become a valuable tool for proving combinatorial identities. (See
Section 3.7 for details.)
6. The goal of the QED Project, active from 1993 until 1996 was to build a repository
that represents all important, established mathematical knowledge. It was hoped that it
could help mathematicians cope with the explosion of mathematical knowledge and help
in developing and verifying computer systems.
7. The Mizar system, founded by A. Trybulec, has a formal language for writing math-
ematical deﬁnitions and proofs, a proof assistant for mechanically checking proofs in this
language, and a library of formalized theorems, called the Mizar Mathematical Library
(MML), that can be used to help prove new results. In 2014 the MML included more than
10,000 formalized deﬁnitions and more than 50,000 theorems. This library is available
at
• http://mizar.uwb.edu.pl/library
REFERENCES
Printed Resources:
[Ap81] K. R. Apt, “Ten years of Hoare’s logic: a survey—Part 1”, ACM Transactions on
Programming Languages and Systems 3 (1981), 431–483.
[BoMo79] R. S. Boyer and J. S. Moore, A Computational Logic, Academic Press, 1979.

82
Chapter 1
FOUNDATIONS
[Cu16] D. W. Cunningham, Set Theory: A First Course, Cambridge, 2016.
[Di76] E. W. Dijkstra, A Discipline of Programming, Prentice-Hall, 1976.
[DuPr80] D. Dubois and H. Prade, Fuzzy Sets and Systems—Theory and Applications,
Academic Press, 1980.
[Ep10] S. S. Epp, Discrete Mathematics with Applications, 4th ed., Cengage, 2010.
[Fi13] M. Fitting, First-Order Logic and Automated Theorem Proving, 2nd ed., Springer,
2013.
[FlPa95] P. Fletcher and C. W. Patty, Foundations of Higher Mathematics, Cengage,
1995.
[Fl67] R. W. Floyd, “Assigning meanings to programs”, Proceedings of the American
Mathematical Society Symposium in Applied Mathematics 19 (1967), 19–32.
[Gr13] D. Gries, The Science of Programming, Springer, 2013.
[Ha11] P. Halmos, Naive Set Theory, Martino Fine Books, 2011 (reprint of Van Nostrand
1960 edition).
[Ho69] C. A. R. Hoare, “An axiomatic basis for computer programming”, Communica-
tions of the ACM 12 (1969), 576–580.
[Ka10] E. Kamke, Theory of Sets, translated by F. Bagemihl, Dover, 2010.
[Ka86] A. Kandel, Fuzzy Mathematical Techniques with Applications, Addison-Wesley,
1986.
[Ka92] A. Kandel, ed., Fuzzy Expert Systems, CRC Press, 1992.
[KaLa94] A. Kandel and G. Langholz, eds., Fuzzy Control Systems, CRC Press, 1994.
[Kr12] S. G. Krantz, The Elements of Advanced Mathematics, 3rd ed., CRC Press, 2012.
[Ll93] J. W. Lloyd, Foundations of Logic Programming, 2nd ed., Springer, 1993.
[Ma09] V. W. Marek, Introduction to Mathematics of Satisﬁability, CRC Press, 2009
[Me09] E. Mendelson, Introduction to Mathematical Logic, 5th ed., CRC Press, 2009.
[MiRo91] J. G. Michaels and K. H. Rosen, eds., Applications of Discrete Mathematics,
McGraw-Hill, 1991.
[Mo76] J. D. Monk, Mathematical Logic, Springer, 1976.
[ReCl90] S. Reeves and M. Clark, Logic for Computer Science, Addison-Wesley, 1990.
[RoVo01] J. A. Robinson and A. Voronkov, eds., Handbook of Automated Reasoning,
Volumes 1 and 2, MIT Press, 2001.
[Ro12] K. H. Rosen, Discrete Mathematics and Its Applications, 7th ed., McGraw-Hill,
2012.
[Sc86] D. A. Schmidt, Denotational Semantics—A Methodology for Language Develop-
ment, Allyn & Bacon, 1986.
[St77] J. E. Stoy, Denotational Semantics: The Scott-Strachey Approach to Programming
Language Theory, MIT Press, 1977.
[WaHa78] D. A. Waterman and F. Hayes-Roth, Pattern-Directed Inference Systems,
Academic Press, 1978.
[We72] P. Wegner, “The Vienna deﬁnition language”, ACM Computing Surveys 4 (1972),
5–63.

REFERENCES
83
[Wo96] L. Wos, The Automation of Reasoning: An Experiment’s Notebook with OTTER
Tutorial, Academic Press, 1996.
[YaFi94] R. R. Yager and D. P. Filev, Essentials of Fuzzy Modeling and Control, Wiley,
1994.
[YaZa94] R. R. Yager and L. A. Zadeh, eds., Fuzzy Sets, Neural Networks and Soft
Computing, Van Nostrand Reinhold, 1994.
[Za65] L. A. Zadeh, “Fuzzy Sets”, Information and Control 8 (1965), 338–353.
[Zi01] H-J. Zimmermann, Fuzzy Set Theory and Its Applications, 4th ed., Kluwer, 2001.
Web Resources:
http://mizar.uwb.edu.pl/library (Mizar Mathematical Library.)
http://plato.stanford.edu/archives/win1997/entries/russell-paradox (The
online Stanford Encyclopedia of Philosophy’s discussion of Russell’s paradox.)
http://www.austinlinks.com/Fuzzy (Fuzzy Logic Archive: a tutorial on fuzzy logic
and fuzzy systems, and examples of how fuzzy logic is applied.)
http://www.cs.unm.edu/~mccune/prover9 (Prover9 automated theorem prover.)
http://www.cut-the-knot.org/selfreference/russell.shtml (Russell’s paradox.)
http://www-formal.stanford.edu/clt/ARS/systems.html
(A database of existing
mechanized reasoning systems.)
http://www-groups.dcs.st-and.ac.uk/history/HistTopics/Beginnings of set
theory.html (The beginnings of set theory.)
http://www.mcs.anl.gov/research/projects/AR/ (A summary of new results in math-
ematics obtained with Argonne’s Automated Deduction Software.)
http://www.rbjones.com/rbjpub/logic/log025.htm (Information on logic.)
https://isabelle.in.tum.de/index.html (Isabelle proof assistant.)


2
COUNTING METHODS
2.1 Summary of Counting Problems
John G. Michaels
2.2 Basic Counting Techniques
Jay Yellen
2.2.1 Rules of Sum, Product, and Quotient
2.2.2 Tree Diagrams
2.2.3 Pigeonhole Principle
2.2.4 Solving Counting Problems Using Recurrence Relations
2.2.5 Solving Counting Problems Using Generating Functions
2.3 Permutations and Combinations
Edward W. Packel
2.3.1 Ordered Selection: Falling Powers
2.3.2 Unordered Selection: Binomial Coefﬁcients
2.3.3 Selection with Repetition
2.3.4 Binomial Coefﬁcient Identities
2.3.5 Generating Permutations and Combinations
2.4 Inclusion/Exclusion
Robert G. Rieper
2.4.1 Principle of Inclusion/Exclusion
2.4.2 Applying Inclusion/Exclusion to Counting Problems
2.5 Partitions
George E. Andrews and
2.5.1 Partitions of Integers
Andrew V. Sills
2.5.2 Stirling Coefﬁcients
2.6 Burnside/P´olya Counting Formula
Alan C. Tucker
2.6.1 Permutation Groups and Cycle Index Polynomials
2.6.2 Orbits and Symmetries
2.6.3 Color Patterns and Induced Permutations
2.6.4 Fixed Points and Burnside’s Lemma
2.6.5 P´olya’s Enumeration Formula
2.7 M¨obius Inversion Counting
Edward A. Bender
2.7.1 M¨obius Inversion
2.8 Young Tableaux
Bruce E. Sagan
2.8.1 Tableaux Counting Formulas
2.8.2 Tableaux Algorithms

86
Chapter 2
COUNTING METHODS
INTRODUCTION
Many problems in mathematics, computer science, and engineering involve counting
objects with particular properties. Although there are no absolute rules that can be
used to solve all counting problems, many counting problems that occur frequently can
be solved using a few basic rules together with a few important counting techniques.
This chapter provides information on how many standard counting problems are solved.
GLOSSARY
binomial coeﬃcient: the coeﬃcient
 n
k

of xkyn−k in the expansion of (x + y)n.
coloring pattern (with respect to a set of symmetries of a ﬁgure): a set of mutually
equivalent colorings.
combination (from a set S): a subset of S; any unordered selection from S.
A k-
combination from a set is a subset of k elements of the set.
combination coeﬃcient: the number C(n, k) (equal to
 n
k

) of ways to make an un-
ordered choice of k items from a set of n items.
combination-with-replacement (from a set S): any unordered selection with replace-
ment; a multiset of objects from S.
combination-with-replacement coeﬃcient: the number of ways to choose a multi-
set of k items from a set of n items, written CR(n, k).
composition: a partition in which the order of the parts is taken into account.
cycle index: for a permutation group G, the multivariate polynomial PG obtained by
dividing the sum of the cycle structure representations of all the permutations in G
by the number of elements of G.
cycle structure (of a permutation): a multivariate monomial whose exponents record
the number of cycles of each size.
derangement: a permutation on a set that leaves no element ﬁxed.
exponential generating function (for {ak}∞
0 ): the formal sum P∞
k=0 ak xk
k! , or any
equivalent closed-form expression.
falling power: the product xk = x(x−1) . . . (x−k+1) of k consecutive factors starting
with x, each factor decreasing by 1.
Ferrers diagram: a geometric, left-justiﬁed, and top-justiﬁed array of cells, boxes, dots
or nodes representing a partition of an integer, in which each row of dots corresponds
to a part of the partition.
Gaussian binomial coeﬃcient: the algebraic expression
n
k

in the variable q deﬁned
for nonnegative integers n and k by
n
k

= qn−1
q−1 · qn−1−1
q2−1 · · · qn+1−k−1
qk−1
for 0 < k ≤n
and
n
0

= 1.
generating function (or ordinary generating function) for {ak}∞
0 : the formal sum
P∞
k=0 akxk, or any equivalent closed-form expression.
hook (of a cell in a Ferrers diagram): the set of cells directly to the right or directly
below a given cell, together with the cell itself.

GLOSSARY
87
hooklength (of a cell in a Ferrers diagram): the number of cells in the hook of that
cell.
Kronecker delta function: the function δ(x, y) deﬁned by the rule δ(x, y) = 1 if x = y
and 0 otherwise.
lexicographic order: the order in which a list of strings would appear in a dictionary.
M¨obius function: the function µ(m) where
µ(m) =





1
if m = 1
(−1)k
if m is a product of k distinct primes
0
if m is divisible by the square of a prime,
or a generalization of this function to partially ordered sets.
multinomial coeﬃcient: the coeﬃcient
 n
k1 k2 ... km

of xk1
1 xk2
2 . . . xkm
m
in the expan-
sion of (x1 + x2 + · · · + xm)n.
ordered selection (of k items from a set S): a nonrepeating list of k items from S.
ordered selection with replacement (of k items from a set S): a possibly-repeating
list of k items from S.
ordinary generating function (for the sequence {ak}∞
0 ): See generating function.
overpartition: a partition in which at most one occurrence of each integer appearing
as a part is distinguished as being overlined.
partially ordered set (or poset): a set S together with a binary relation ≤that is
reﬂexive, antisymmetric, and transitive, written (S, ≤).
partition: an unordered decomposition of an integer into a sum of positive integers.
Pascal’s triangle: a triangular table with the binomial coeﬃcient
 n
k

appearing in row
n, column k.
pattern inventory: a generating function that enumerates the number of coloring pat-
terns.
permutation: a one-to-one mapping of a set of elements onto itself, or an arrangement
of the set into a list. A k-permutation of a set is an ordered nonrepeating sequence
of k elements of the set.
permutation coeﬃcient: the number of ways to choose a nonrepeating list of k items
from a set of n items, written P(n, k).
permutation group: a nonempty set P of permutations on a set S, such that P is
closed under composition and under inversion.
permutation-with-replacement coeﬃcient: the number of ways to choose a possi-
bly repeating list of k items from a set of n items, written P R(n, k).
poset: See partially ordered set.
probl`eme des m´enages: the problem of ﬁnding the number of ways that married
couples can be seated around a circular table so that no men are adjacent, no women
are adjacent, and no husband and wife are adjacent.
probl`eme des rencontres: given balls 1 through n drawn out of an urn one at a time,
the problem of ﬁnding the probability that ball i is never the ith one drawn.
Stirling cycle number: the number
n
k

of ways to partition n objects into k nonempty
cycles.

88
Chapter 2
COUNTING METHODS
Stirling number of the ﬁrst kind: the coeﬃcient s(n, k) of xk in the polynomial
x(x −1)(x −2) . . . (x −n + 1).
Stirling number of the second kind: the coeﬃcient S(n, k) of xk in the representa-
tion xn = P
k S(n, k)xk of xn as a linear combination of falling powers.
Stirling subset number: the number
n
k
	
of ways to partition n objects into k non-
empty subsets.
symmetry (of a ﬁgure): a spatial motion that maps the ﬁgure onto itself.
tree diagram: a tree that displays the diﬀerent alternatives in some counting process.
unordered selection (of k items from a set S): a subset of k items from S.
unordered selection (of k items from a set S with replacement): a selection of k ob-
jects in which each object in the selection set S can be chosen arbitrarily often and
such that the order in which the objects are selected does not matter.
Young tableau: an array obtained by replacing each cell of a Ferrers diagram by a
positive integer.
2.1
SUMMARY OF COUNTING PROBLEMS
Table 1 lists many important counting problems, gives the number of objects being
counted, together with a reference to the section of this Handbook where details can be
found. Table 2 lists several important counting rules and methods, and gives the types
of counting problems that can be solved using these rules and methods. Notation used
in this table is given at the end of the table.
Table 1: Counting problems.
objects
number of objects
reference
Arranging objects in a row:
n distinct objects
n! = P(n, n) = n(n −1) . . . 2 · 1
§2.3.1
k out of n distinct objects
nk = P(n, k) = n(n−1) . . . (n−k+1)
§2.3.1
some of the n objects are
identical: k1 of a ﬁrst kind, k2
of a second kind, . . . , kj of a
jth kind, and where k1 + k2+
· · · + kj = n
 n
k1 k2 ... kj

=
n!
k1! k2!...kj!
§2.3.2
none of the n objects remains
in its original place (derange-
ments)
Dn = n!
 1−1
1!+ · · · +(−1)n 1
n!

§2.4.2
Arranging objects in a circle (where rotations, but not reﬂections, are equivalent):
n distinct objects
(n −1)!
§2.2.1
k out of n distinct objects
P (n,k)
k
§2.2.1
Choosing k objects from n distinct objects:
order matters, no repetitions
P(n, k) =
n!
(n−k)! = nk
§2.3.1

Section 2.1
SUMMARY OF COUNTING PROBLEMS
89
objects
number of objects
reference
order matters, repetitions
allowed
P R(n, k) = nk
§2.3.3
order does not matter, no
repetitions
C(n, k) =
 n
k

=
n!
k!(n−k)!
§2.3.2
order does not matter,
repetitions allowed
CR(n, k) =
 n+k−1
k

§2.3.3
Subsets:
of size k from a set of size n
 n
k

§2.3.2
of all sizes from a set of size n
2n
§2.3.4
of {1, . . . , n}, without consecu-
tive elements
Fn+2
§3.1.2
Placing n objects into k cells:
distinct objects, distinct cells
kn
§2.2.1
distinct objects, distinct cells,
no cell empty
n
k
	
k!
§2.5.2
distinct objects, identical cells
n
1
	
+
n
2
	
+ · · · +
n
k
	
= Bn
§2.5.2
distinct objects, identical cells,
no cell empty
n
k
	
§2.5.2
distinct objects, distinct cells,
with ki in cell i (i = 1, . . . , n),
where k1 + k2 + · · · + kj = n
 n
k1 k2 ... kj

§2.3.2
identical objects, distinct cells
 n+k−1
n

§2.3.3
identical objects, distinct cells,
no cell empty
 n−1
k−1

§2.3.3
identical objects, identical cells
pk(n)
§2.5.1
identical objects, identical cells,
no cell empty
pk(n) −pk−1(n)
§2.5.1
Placing n distinct objects into k
nonempty cycles
n
k

§2.5.2
Solutions to x1 + · · · + xn = k:
nonnegative integers
 k+n−1
k

=
 k+n−1
n−1

§2.3.3
positive integers
 k−1
n−1

§2.3.3
integers where 0 ≤ai ≤xi for
all i
 k−(a1+···+an)+n−1
n−1

§2.3.3
integers where 0 ≤xi ≤ai for
one or more i
inclusion/exclusion principle
§2.4.2
integers with x1 ≥· · · ≥xn ≥1
pn(k) −pn−1(k)
§2.5.1
integers with x1 ≥· · · ≥xn ≥0
pn(k)
§2.5.1
Solutions to x1 +x2 +· · ·+xn =
n in nonnegative integers where
x1 ≥x2 ≥· · · ≥xn ≥0
p(n)
§2.5.1
Solutions to x1+2x2+3x3+· · · +
nxn = n in nonnegative
integers
p(n)
§2.5.1

90
Chapter 2
COUNTING METHODS
objects
number of objects
reference
Functions from a k-element set to an n-element set:
all functions
nk
§2.2.1
one-to-one functions (n ≥k)
nk =
n!
(n−k)! = P(n, k)
§2.2.1
onto functions (n ≤k)
inclusion/exclusion
§2.4.2
partial functions
 k
0

+
 k
1

n+ · · · +
 k
k

nk = (n + 1)k
§2.3.2
Bit strings of length n:
all strings
2n
§2.2.1
with given entries in k positions
2n−k
§2.2.1
with exactly k 0s
 n
k

§2.3.2
with at least k 0s
 n
k

+
  n
k+1

+ · · · +
 n
n

§2.3.2
with equal numbers of 0s
and 1s
  n
n/2

§2.3.2
Palindromes
2⌈n/2⌉
§2.2.1
with an even number of 0s
2n−1
§2.3.4
without consecutive 0s
Fn+2
§3.1.2
Partitions of a positive integer n into positive summands:
§2.5.1
total number
p(n)
into at most k parts
pk(n)
into exactly k parts
pk(n) −pk−1(n)
into parts each of size ≤k
pk(n)
Partitions of a set of size n:
all partitions
B(n)
§2.5.2
into k parts
n
k
	
§2.5.2
into k parts, each part having
at least two elements
b(n, k)
§3.1.8
Paths:
from (0, 0) to (2n, 0) made up
of line segments from (i, yi) to
(i + 1, yi+1); integer yi ≥0,
yi+1 = yi ± 1
Cn
§3.1.3
from (0, 0) to (2n, 0) made up
of line segments from (i, yi) to
(i+1, yi+1); integer yi > 0 (for
0 < i < 2n), yi+1 = yi ± 1
Cn−1
§3.1.3
from (0, 0) to (m, n) that move
1 unit up or right at each step
 m+n
n

§2.3.2
Permutations of {1, . . . , n}:
all permutations
n!
§2.3.1
with k cycles, all cycles of
length ≥2
d(n, k)
§3.1.8
with k descents
E(n, k)
§3.1.5
with k excedances
E(n, k)
§3.1.5
alternating, n even
(−1)n/2En
§3.1.7

Section 2.1
SUMMARY OF COUNTING PROBLEMS
91
objects
number of objects
reference
alternating, n odd
Tn
§3.1.7
Symmetries of regular ﬁgures:
§2.6
n-gon
2n
tetrahedron
12
cube
24
octahedron
24
dodecahedron
60
icosahedron
60
Coloring regular 2-dimensional & 3-dimensional ﬁgures with ≤k colors:
§2.6
corners of an n-gon, allowing
rotations and reﬂections
1
2n
X
d|n
ϕ(d)k
n
d + 1
2k
(n+1)
2
, n odd
1
2n
X
d|n
ϕ(d)k
n
d + 1
4(k
n
2 + k
(n+2)
2
), n even
corners of an n-gon, allowing
only rotations
1
n
X
d|n
ϕ(d)k
n
d
corners of a triangle, allowing
rotations and reﬂections
1
6[k3 + 3k2 + 2k]
corners of a triangle, allowing
only rotations
1
3[k3 + 2k]
corners of a square, allowing
rotations and reﬂections
1
8[k4 + 2k3 + 3k2 + 2k]
corners of a square, allowing
only rotations
1
4[k4 + k2 + 2k]
corners of a pentagon, allowing
rotations and reﬂections
1
10[k5 + 5k3 + 4k]
corners of a pentagon, allowing
only rotations
1
5[k5 + 4k]
corners of a hexagon, allowing
rotations and reﬂections
1
12[k6 + 3k4 + 4k3 + 2k2 + 2k]
corners of a hexagon, allowing
only rotations
1
6[k6 + k3 + 2k2 + 2k]
corners of a tetrahedron
1
12[k4 + 11k2]
edges of a tetrahedron
1
12[k6 + 3k4 + 8k2]
faces of a tetrahedron
1
12[k4 + 11k2]
corners of a cube
1
24[k8 + 17k4 + 6k2]
edges of a cube
1
24[k12 + 6k7 + 3k6 + 8k4 + 6k3]
faces of a cube
1
24[k6 + 3k4 + 12k3 + 8k2]
Number of sequences of wins/
losses in n+1
2 -out-of-n playoﬀ
series (n odd)
2C(n, n+1
2 )
§2.3.2
Sequences a1, . . . , a2n having n
1s and n −1s; each partial sum
a1 + · · · + ak ≥0
Cn
§3.1.3

92
Chapter 2
COUNTING METHODS
objects
number of objects
reference
Well-formed sequences of paren-
theses of length 2n
Cn
§3.1.3
Well-parenthesized products of
n + 1 variables
Cn
§3.1.3
Triangulations of a convex
(n + 2)-gon
Cn
§3.1.3
Notation:
B(n) or Bn: Bell number
nk = n(n −1) . . . (n −k + 1) = P(n, k):
falling power
b(n, k): associated Stirling number of the
P(n, k) =
n!
(n−k)!: k-permutation
second kind
Cn =
1
n+1
 2n
n

: Catalan number
p(n): number of partitions of n
C(n, k) =
 n
k

=
n!
k!(n−k)!: binomial coeﬃcient
pk(n): number of partitions of n into
at most k summands
d(n, k): associated Stirling number of
p∗
k(n): number of partitions of n into
the ﬁrst kind
exactly k summands
En: Euler number
n
k

: Stirling cycle number
ϕ: Euler phi-function
n
k
	
: Stirling subset number
E(n, k): Eulerian number
Tn: tangent number
Fn: Fibonacci number
Table 2: Methods of counting and the problems they solve.
statement
technique of proof
rule of sum
(§2.2.1)
problems that can be broken into disjoint cases, each of
which can be handled separately
rule of product
(§2.2.1)
problems that can be broken into sequences of independent
counting problems, each of which can be solved separately
rule of quotient
(§2.2.1)
problems of counting arrangements, where the arrange-
ments can be divided into collections that are all of the
same size
pigeonhole principle
(§2.2.3)
problems with two sets of objects, where one set of objects
needs to be matched with the other
inclusion/exclusion
principle (§2.4)
problems that involve ﬁnding the size of a union of sets,
where some or all the sets in the union may have common
elements
permutations
(§2.2.1, §2.3.1, §2.3.3)
problems that require counting the number of selections or
arrangements, where order within the selection or arrange-
ment matters
combinations
(§2.3.2, §2.3.3)
problems that require counting the number of selections or
sets of choices, where order within the selection does not
matter

Section 2.2
BASIC COUNTING TECHNIQUES
93
statement
technique of proof
recurrence relations
(§2.2.4)
problems that require an answer depending on the inte-
ger n, where the solution to the problem for a given size n
can be related to one or more cases of the problem for
smaller sizes
generating functions
(§2.2.5)
problems that can be solved by ﬁnding a closed form for a
function that represents the problem and then manipulat-
ing the closed form to ﬁnd a formula for the coeﬃcients
P´olya counting
(§2.6.5)
problems that require a listing or number of patterns,
where the patterns are not to be regarded as diﬀerent under
certain types of motions (such as rotations and reﬂections)
M¨obius inversion
(§2.7.1)
problems that involve counting certain types of circular
permutations
2.2
BASIC COUNTING TECHNIQUES
Most counting methods are based directly or indirectly on the fundamental principles
and techniques presented in this section. The rules of sum, product, and quotient are the
most basic and are applied more often than any other. The section also includes some
applications of the pigeonhole principle, a brief introduction to generating functions, and
several examples illustrating the use of tree diagrams and Venn diagrams.
2.2.1
RULES OF SUM, PRODUCT, AND QUOTIENT
Deﬁnitions:
The rule of sum states that when there are m cases such that the ith case has ni
options, for i = 1, . . . , m, and no two of the cases have any options in common, the total
number of options is n1 + n2 + · · · + nm.
The rule of product states that when a procedure can be broken down into m steps,
such that there are n1 options for Step 1, and such that after the completion of Step i−1
(i = 2, . . . , m) there are ni options for Step i, the number of ways of performing the
procedure is n1n2 . . . nm.
The rule of quotient states that when a set S is partitioned into equal-sized subsets of
m elements each, there are |S|
m subsets.
An m-permutation of a set S with n elements is a nonrepeating ordered selection of m
elements of S, that is, a sequence of m distinct elements of S. An n-permutation is
simply called a permutation of S.
Facts:
1. The rule of sum can be stated in set-theoretic terms: if sets S1, . . . , Sm are ﬁnite and
pairwise disjoint, then |S1 ∪S2 ∪· · · ∪Sm| = |Si| + |S2| + · · · + |Sm|.
2. The rule of product can be stated in set-theoretic terms: if sets S1, . . . , Sm are ﬁnite,
then |S1 × S2 × · · · × Sm| = |S1| · |S2| · · · · · |Sm|.

94
Chapter 2
COUNTING METHODS
3. The rule of quotient can be stated in terms of the equivalence classes of an equivalence
relation on a ﬁnite set S: if every class has m elements, then there are |S|/m equivalence
classes.
4. Venn diagrams (§1.2.2) are often used as an aid in counting the elements of a
subset, as an auxiliary to the rule of sum. This generalizes to the principle of inclu-
sion/exclusion (§2.4).
5. Counting problems can often be solved by using a combination of counting methods,
such as the rule of sum and the rule of product.
Examples:
1. Counting bit strings:
There are 2n bit strings of length n, since such a bit string
consists of n bits, each of which is either 0 or 1.
2. Counting bit strings with restrictions: There are 2n−2 bit strings of length n (n ≥2)
that begin with two 1s, since forming such a bit string consists of ﬁlling in n−2 positions
with 0s or 1s.
3. Counting palindromes: A palindrome is a string of symbols that is unchanged if the
symbols are written in reverse order, such as rpnbnpr or 10011001. There are k⌈n/2⌉
palindromes of length n where the symbols are chosen from a set of k symbols.
4. Counting the number of variable names:
Determine the number of variable names,
subject to the following rules: a variable name has four or fewer characters, the ﬁrst
character is a letter, the second and third are letters or digits, and the fourth must be X
or Y or Z. Partition the names into four sets, S1, S2, S3, S4, containing names of length
1, 2, 3, and 4 respectively. Then |S1| = 26, |S2| = 26 × 36, |S3| = 26 × 362, and |S4| =
26×362×3. Therefore the total number of names equals |S1|+|S2|+|S3|+|S4| = 135,746.
5. Counting functions:
There are nm functions from a set A = {a1, . . . , am} to a set
B = {b1, . . . , bn}. (Construct each function f : A →B by an m-step process, where Step
i involves selecting the value f(ai).)
6. Counting one-to-one functions: There are n(n−1) . . . (n−m+1) one-to-one functions
from A = {a1, . . . , am} to B = {b1, . . . , bn}. If values f(a1), . . . , f(ai−1) have already
been selected in set B during the ﬁrst i −1 steps, then there are n −i + 1 possible values
remaining for f(ai).
7. Counting permutations: There are n(n−1) . . .(n−m+1) =
n!
(n−m)! m-permutations
of an n-element set. (Each one-to-one function in Example 6 may be viewed as an m-
permutation of B.) (Permutations are discussed in §2.3.)
8. Counting circular permutations:
There are (n −1)! ways to seat n people around
a round table (where rotations are regarded as equivalent, but the clockwise/counter-
clockwise distinction is maintained). The total number of arrangements is n! and each
equivalence class contains n conﬁgurations.
By the rule of quotient, the number of
arrangements is n!
n = (n −1)! .
9. Counting restricted circular permutations: If n women and n men are to be seated
around a circular table, with no two of the same sex seated next to each other, the
number of possible arrangements is n(n −1)!2 .
2.2.2
TREE DIAGRAMS
When a counting problem can be decomposed into cases, a tree can be used to make sure
that every case is counted, and that no case is counted twice.

Section 2.2
BASIC COUNTING TECHNIQUES
95
Deﬁnitions:
A tree diagram is a line-drawing of a tree, often with its branches and/or nodes labeled.
The root represents the start of a procedure and the branches at each node represent
the options for the next step.
Facts:
1. Tree diagrams can be used as an important auxiliary to the rules of sum and product.
2. The objective in a tree-counting approach is often one of the following:
• the number of leaves (endnodes);
• the number of nodes;
• the sum of the path products.
Examples:
1. There are six possible sequences of wins and losses when the home team (H) plays
the visiting team (V) in a best 2-out-of-3 playoﬀ. In the following tree diagram each
edge label indicates whether the home team won or lost the corresponding game, and the
label at each ﬁnal node is the outcome of the playoﬀ. The number of diﬀerent possible
sequences equals the number of endnodes, namely 6.
V  2-0
V  2-1
V  2-1
H  2-1
H  2-1
W
W
W
W
W
L
L
L
L
L
H  2-0
2. Suppose that an experimental process begins by tossing two identical dice. If the
dice match, the process continues for a second round; if not, the process stops at one
round. Thus, an experimental outcome sequence consists of one or two unordered pairs
of numbers from 1 to 6. The three paths in the following tree represent the three diﬀerent
kinds of outcome sequences. The total number of possible outcomes is the sum of the
path products 62 + 6 · 15 + 15 = 141.
6 doubles
15 non-doubles
6 doubles
15 non-doubles
2.2.3
PIGEONHOLE PRINCIPLE
Deﬁnitions:
The pigeonhole principle (Dirichlet drawer principle) states that if n + 1 objects
(pigeons) are placed into n boxes (pigeonholes), then some box contains more than one
object. (Peter Gustav Lejeune Dirichlet, 1805–1859)

96
Chapter 2
COUNTING METHODS
The generalized pigeonhole principle states that if m objects are placed into k boxes,
then some box contains at least
 m
k

objects.
The set-theoretic form of the pigeonhole principle states that if f : S →T , where
S and T are ﬁnite and any two of the following conditions hold, then so does the third:
• f is one-to-one;
• f is onto;
• |S| = |T |.
Examples:
1. Among any group of eight people, at least two were born on the same day of the
week. This follows since there are seven pigeonholes (the seven days of the week) and
more than seven pigeons (the eight people).
2. Among any group of 25 people, at least four were born on the same day of the week.
This follows from the generalized pigeonhole principle with m = 25 and k = 7, yielding
 m
k

=
 25
7

= 4.
3. Suppose that a dresser drawer contains many black socks and blue socks. If choosing
in total darkness, a person must grab at least three socks to be absolutely certain of
having a pair of the same color. The two colors are pigeonholes; the pigeonhole principle
says that three socks (the pigeons) are enough.
4. What is the minimum number of points whose placement in the interior of a 2 × 2
square guarantees that at least two of them are less than
√
2 units apart? Four points
are not enough, since they could be placed near the respective corners of the 2×2 square.
To see that ﬁve is enough, partition the 2 × 2 square into four 1 × 1 squares. By the
pigeonhole principle, one of these 1 × 1 squares must contain at least two of the points,
and these two must be less than
√
2 units apart.
5. In any set of n + 1 positive integers, each less than or equal to 2n, there are at least
two such that one is a multiple of the other. To see this, express each of the n + 1
numbers in the form 2k · q, where q is odd. Since there are only n possible odd values for
q between 1 and 2n, at least two of the n + 1 numbers must have the same q, and the
result follows.
6. Let B1 and B2 be any two bit strings, each consisting of ﬁve ones and ﬁve zeros.
Then there is a cyclic shift of bit string B2 so that the resulting string B′
2 matches B1
in at least ﬁve of its positions. For example, if B1 = 1010101010 and B2 = 0001110101,
then B′
2 = 1000111010 satisﬁes the condition. Observe that there are 10 possible cyclic
shifts of bit string B2. For i = 1, . . . , 10, the ith bit of exactly ﬁve of these strings will
match the ith bit of B1. Thus, there is a total of 50 bitmatches over the set of 10 cyclic
shifts. The generalized pigeonhole principle implies that there is at least one cyclic shift
having
 50
10

= 5 matching bits.
7. Every sequence of n2 + 1 distinct real numbers must have an increasing or decreasing
subsequence of length n + 1. Given a sequence a1, . . . , an2+1, for each aj let dj and ij be
the lengths of the longest decreasing and increasing subsequences beginning with aj. This
gives a sequence of n2 + 1 ordered pairs (dj, ij). If there were no increasing or decreasing
subsequence of length n + 1, then there are only n2 possible ordered pairs (dj, ij), since
1 ≤dj ≤n and 1 ≤ij ≤n. By the pigeonhole principle, at least two ordered pairs must
be identical. Hence there are p and q such that dp = dq and ip = iq. If ap < aq, then
the sequence ap followed by the increasing subsequence starting at aq gives an increasing
subsequence of length greater than iq, a contradiction. A similar contradiction to the
choice of dp follows if aq < ap. Hence a decreasing or increasing subsequence of length
n + 1 must exist.

Section 2.2
BASIC COUNTING TECHNIQUES
97
2.2.4
SOLVING COUNTING PROBLEMS USING RECURRENCE RELATIONS
Certain types of counting problems can be solved by modeling the problem using a
recurrence relation (§3.3) and then working with the recurrence relation.
Facts:
1. The following general procedure is used for solving a counting problem using a re-
currence relation:
• Let an be the solution of the counting problem for the parameter n;
• Determine a recurrence relation for an, together with the appropriate number of
initial conditions;
• Find the particular value of the sequence that solves the original counting problem
by repeated use of the recurrence relation or by ﬁnding an explicit formula for
an and evaluating it at n.
2. There are many techniques for solving recurrence relations which may be useful in
the solution of counting problems. Section 3.3 provides general material on recurrence
relations and contains many examples illustrating how counting problems are solved using
recurrence relations.
Examples:
1. Tower of Hanoi:
The Tower of Hanoi puzzle consists of three pegs mounted on a
board and n disks of diﬀerent sizes. Initially the disks are on the ﬁrst peg in order of
decreasing size. See the following ﬁgure, using four disks. The rules allow disks to be
moved one at a time from one peg to another, with no disk ever placed atop a smaller
one. The goal of the puzzle is to move the tower of disks to the second peg, with the
largest on the bottom. How many moves are needed to solve this puzzle for 64 disks?
Let an be the minimum number of moves to solve the Tower of Hanoi puzzle with n disks.
Transferring the n−1 smallest disks from peg 1 to peg 3 requires an−1 moves. One move
is required to transfer the largest disk to peg 2, and transferring the n −1 disks now
on peg 3 to peg 2, placing them atop the largest disk, requires an−1 moves. Hence the
puzzle with n disks can be solved using 2an−1 + 1 moves. The puzzle for n disks cannot
be solved in fewer steps, since then the puzzle with n−1 disks could be solved using fewer
than an−1 moves. Hence an = 2an−1 +1. The initial condition is a1 = 1. Iterating shows
that an = 2an−1 + 1 = 22an−2 + 2 + 1 = · · · = 2n−1a1 + 2n−2 + · · · + 22 + 2 + 1 = 2n −1.
So 264−1 moves are required to solve this problem for 64 disks. (Example 3 of §3.3.3 and
Example 1 of §3.3.4 provide alternative methods for solving this recurrence relation.)
2. Reve’s puzzle: The Reve’s puzzle is the variation of the Tower of Hanoi puzzle that
follows the same rules as the Tower of Hanoi puzzle, but uses four pegs.
The minimum number of moves needed to solve the Reve’s puzzle for n disks is not
known, but it is conjectured that this number is R(n) = Pk
i=1 i2i−1 −
  k(k+1)
2
−n

2k−1
where k is the smallest integer such that n ≤k(k+1)
2
.

98
Chapter 2
COUNTING METHODS
The following recursive approach, the Frame-Stewart algorithm, gives a method for solv-
ing the Reve’s puzzle by moving n disks from peg 1 to peg 4 in R(n) moves. If n = 1,
move the single disk from peg 1 to peg 4. If n > 1: recursively move the n −k smallest
disks from peg 1 to peg 2 using the Frame-Stewart algorithm; then move the k largest
disks from peg 1 to peg 4 using the 3-peg algorithm from Example 1 on pegs 1, 3, and
4; and ﬁnally recursively move the n −k smallest disks from peg 2 to peg 4 using the
Frame-Stewart algorithm.
3. How many strings of four decimal digits contain an even number of 0s? Let an be the
number of strings of n decimal digits that contain an even number of 0s. To obtain such
a string: (1) append a nonzero digit to a string of n −1 decimal digits that has an even
number of 0s, which can be done in 9an−1 ways; or (2) append a 0 to a string of n −1
decimal digits that has an odd number of 0s, which can be done in 10n−1 −an−1 ways.
Hence an = 9an−1 + (10n−1 −an−1) = 8an−1 + 10n−1. The initial condition is a1 = 9.
It follows that a2 = 8a1 + 10 = 82, a3 = 8a2 + 100 = 756, and a4 = 8a3 + 1,000 = 7,048.
2.2.5
SOLVING COUNTING PROBLEMS USING GENERATING FUNCTIONS
Some counting problems can be solved by ﬁnding a closed form for the function that
represents the problem and then manipulating the closed form to ﬁnd the relevant coef-
ﬁcient.
Facts:
1. Use the following procedure for solving a counting problem by using a generating
function:
• Let an be the solution of the counting problem for the parameter n;
• Find a closed form for the generating function f(x) that has an as the coeﬃcient
of xn in its power series;
• Solve the counting problem by computing an by expanding the closed form and
examining the coeﬃcient of xn.
2. Generating functions can be used to solve counting problems that reduce to ﬁnding
the number of solutions to an equation of the form x1 + x2 + · · · + xn = k, where k is a
positive integer and the xis are integers subject to constraints.
3. There are many techniques for manipulating generating functions (§3.2, §3.3.4) which
may be useful in the solution of counting problems. Section 3.2 contains examples of
counting problems solved using generating functions.
Examples:
1. How many ways are there to distribute eight identical cookies to three children if
each child receives at least two and no more than four cookies. Let cn be the number
of ways to distribute n identical cookies in this way. Then cn is the coeﬃcient of xn in
(x2 + x3 + x4)3, since a distribution of n cookies to the three children is equivalent to a
solution of x1 +x2 +x3 = 8 with 2 ≤xi ≤4 for i = 1, 2, 3. Expanding this product shows
that c8, the coeﬃcient of x8, is 6. Hence there are six ways to distribute the cookies.
2. An urn contains colored balls, where each ball is either red, blue, or black, there are
at least ten balls of each color, and balls of the same color are indistinguishable. Find the
number of ways to select ten balls from the urn so that an odd number of red balls, an even
number of blue balls, and at least ﬁve black balls are selected. If x1, x2, and x3 denote

Section 2.3
PERMUTATIONS AND COMBINATIONS
99
the number of red balls, blue balls, and black balls selected, respectively, the answer is
provided by the number of nonnegative integer solutions of x1 + x2 + x3 = 10 with x1
odd, x2 even, x3 ≥5. This is the coeﬃcient of x10 in the generating function f(x) =
(x+x3+x5+x7+x9+· · · )(1+x2+x4+x6+x8+x10+· · · )(x5+x6+x7+x8+x9+x10+· · · ).
Since the coeﬃcient of x10 in the expansion is 6, there are six ways to select the balls as
speciﬁed.
2.3
PERMUTATIONS AND COMBINATIONS
Permutations count the number of arrangements of objects, and combinations count the
number of ways to select objects from a set. A permutation coeﬃcient counts the number
of ways to arrange a set of objects, whereas a combination coeﬃcient counts the number
of ways to select a subset.
2.3.1
ORDERED SELECTION: FALLING POWERS
Falling powers mathematically model the process of selecting k items from a collection
of n items in circumstances where the ordering of the selection matters and repetition is
not allowed.
Deﬁnitions:
An ordered selection of k items from a set S is a nonrepeating list of k items from S.
The falling power xk is the product x(x −1) . . . (x −k + 1) of k decreasing factors
starting at the real number x.
The number n-factorial, n! (n a nonnegative integer), is deﬁned by the rule 0! = 1,
n! = n(n −1) . . . 3·2·1 if n ≥1.
A permutation of a list is any rearrangement of the list.
A permutation of a set of n items is an arrangement of those items into a list. (Often,
such a list and/or the permutation itself is represented by a string whose entries are in
the list order.)
A k-permutation of a set of n items is an ordered selection of k items from that set.
A k-permutation can be written as a sequence or a string.
The permutation coeﬃcient P(n, k) is the number of ways to choose an ordered
selection of k items from a set of n items: that is, the number of k-permutations.
A derangement of a list is a permutation of the entries such that no entry remains in
the original position.
Facts:
1. The falling power xk is analogous to the ordinary power xk, which is the product
of k constant factors x. The underline in the exponent of the falling power is a reminder
that consecutive factors drop.
2. P(n, k) = nk =
n!
(n−k)!.

100
Chapter 2
COUNTING METHODS
3. For any integer n, nn = n!.
4. The numbers P(n, k) = nk are given in Table 1.
5. A repetition-free list of length n has approximately n!/e derangements.
Table 1: Permutation coeﬃcients P (n, k) = nk.
n\k 0
1
2
3
4
5
6
7
8
9
10
0
1
1
1
1
2
1
2
2
3
1
3
6
6
4
1
4 12
24
24
5
1
5 20
60
120
120
6
1
6 30 120
360
720
720
7
1
7 42 210
840
2,520
5,040
5,040
8
1
8 56 336 1,680
6,720
20,160
40,320
40,320
9
1
9 72 504 3,024 15,120
60,480 181,440
362,880
362,880
10
1 10 90 720 5,040 30,240 151,200 604,800 1,814,400 3,628,800 3,628,800
Examples:
1. (4.2)3 = 4.2 · 3.2 · 2.2 = 29.568.
2. Dealing a row of playing cards: Suppose that ﬁve cards are to be dealt from a deck
of 52 cards and placed face up in a row. There are P(52, 5) = 525 = 52 · 51 · 50 · 49 · 48 =
311,875,200 ways to do this.
3. Placing distinct balls into distinct bins:
k diﬀerently-colored balls are to be placed
into n bins (n ≥k), with at most one ball to a bin. The number of diﬀerent ways to
arrange the balls is P(n, k) = nk. (Think of the balls as if they were numbered 1 to k,
so that placing ball j into a bin corresponds to placing that bin into the jth position of
the list.)
4. Counting ballots:
Voters are asked to rank their three top choices from the eleven
candidates running for oﬃce. A ﬁrst choice vote is worth 3 points, second choice 2 points,
and third choice 1 point. Since a completed ballot is an ordered selection in this situation,
each voter has P(11, 3) = 113 = 11 · 10 · 9 = 990 distinct ways to cast a vote.
5. License plate combinations: The license plates in a state have three letters (from the
upper-case Roman alphabet of 26 letters) followed by four digits. There are P(26, 3) =
15,600 ways to select the letters and P(10, 4) = 5,040 ways to select the digits. By the
rule of product there are P(26, 3)P(10, 4) = 15,600 · 5,040 = 78,624,000 strings.
6. Circular permutations of distinct objects: See Example 8 of §2.2.1. Also see Example
3 of §2.7.1 for problems that allow identical objects.
7. Increasing and decreasing subsequences of permutations: Young tableaux (§2.8) can
be used to ﬁnd the number of permutations of {1, 2, . . ., n} with speciﬁed lengths of their
longest increasing subsequences and longest decreasing subsequences.
2.3.2
UNORDERED SELECTION: BINOMIAL COEFFICIENTS
Binomial coeﬃcients mathematically model the process of selecting k items from a col-
lection of n items in circumstances where the ordering of the selection does not matter,

Section 2.3
PERMUTATIONS AND COMBINATIONS
101
and repetitions are not allowed.
Deﬁnitions:
An unordered selection of k items from a set S is a subset of k items from S.
A k-combination from a set S is an unordered selection of k items.
The combination coeﬃcient C(n, k) is the number of k-combinations of n objects.
The binomial coeﬃcient
 n
k

is the coeﬃcient of xkyn−k in the expansion of (x + y)n.
The extended binomial coeﬃcient (generalized binomial coeﬃcient)
 n
k

is zero
whenever k is negative. When n is a negative integer and k is a nonnegative integer, its
value is (−1)k k−n−1
k

.
The multicombination coeﬃcient C(n: k1, k2, . . . , km), where n = k1 +k2 +· · ·+km,
denotes the number of ways to partition n items into subsets of sizes k1, k2, . . . , km.
The multinomial coeﬃcient
 n
k1 k2 ... km

is the coeﬃcient of xk1
1 xk2
2 . . . xkm
m
in the
expansion of (x1 + x2 + · · · + xm)n.
The Gaussian binomial coeﬃcient is deﬁned for nonnegative integers n and k by
n
k

= qn −1
q −1 · qn−1 −1
q2 −1
· qn−2 −1
q3 −1
· · · qn+1−k −1
qk −1
for 0 < k ≤n
and
n
0

= 1, where q is a variable. (See also §2.5.1.)
Facts:
1. C(n, k) = P (n,k)
k!
= nk
k! =
n!
k!(n−k)! =
 n
k

.
2. Pascal’s recursion:
 n
k

=
 n−1
k−1

+
 n−1
k

, where n > 0 and k > 0.
3. Subsets: There are C(n, k) subsets of size k that can be chosen from a set of size n.
4. The numbers C(n, k) =
 n
k

are given in Table 2.
Table 2: Combination coeﬃcients (binomial coeﬃcients) C(n, k) =
 n
k

.
n\k
0
1
2
3
4
5
6
7
8
9
10
11
12
0
1
1
1
1
2
1
2
1
3
1
3
3
1
4
1
4
6
4
1
5
1
5
10
10
5
1
6
1
6
15
20
15
6
1
7
1
7
21
35
35
21
7
1
8
1
8
28
56
70
56
28
8
1
9
1
9
36
84
126
126
84
36
9
1
10
1
10
45
120
210
252
210
120
45
10
1
11
1
11
55
165
330
462
462
330
165
155
11
1
12
1
12
66
220
495
792
924
792
495
220
66
12
1
Sometimes the entries in Table 2 are arranged into the form called Pascal’s triangle, in
which each entry is the sum of the two numbers diagonally above the number (Pascal’s
recursion, Fact 2).

102
Chapter 2
COUNTING METHODS
1
1
1
1
2
1
1
3
3
1
1
4
6
4
1
1
5
10
10
5
1
1
6
15
20
15
6
1
1
7
21
35
35
21
7
1
1
8
28
56
70
56
28
8
1
1
9
36
84
126 126 84
36
9
1
1
10
45
120 210 252 210 120 45
10
1
5. The extended binomial coeﬃcients satisfy Pascal’s recursion. Their deﬁnition is con-
structed precisely to achieve this purpose.
6. C(n: k1, k2, . . . , km) =
n!
k1!k2!...km! =
 n
k1 k2 ... km

. The number of strings of length n
with ki objects of type i (i = 1, 2, . . ., m) is
n!
k1!k2!...km!.
7. C(n, k) = C(n: k, n −k) = C(n, n −k). That is, the number of unordered selections
of k objects chosen from n objects is equal to the number of unordered selections of n−k
objects chosen from n objects.
8. Gaussian binomial coeﬃcient identities:
•
n
k

=
 n
n−k

;
•
n
k

+
 n
k−1

qn+1−k =
n+1
k

.
9. (1 + x)(1 + qx)(1 + q2x) . . . (1 + qn−1x) = Pn
k=0
n
k

qk(k−1)/2xk.
10. limq→1
n
k

=
 n
k

.
11.
n
k

= a0 + a1q + a2q2 + · · · + ak(n−k)qk(n−k) where each ai is an integer and
Pk(n−k)
i=0
ai =
 n
k

.
Examples:
1. Subsets: A set with 20 elements has C(20, 4) subsets with four elements. The total
number of subsets of a set with 20 elements is equal to C(20, 0)+C(20, 1)+· · ·+C(20, 20),
which is equal to 220. (See §2.3.4.)
2. Nondistinct balls into distinct bins:
Suppose k identically colored balls are to be
placed into n bins (n ≥k), at most one ball to a bin. The number of diﬀerent ways to do
this is C(n, k) = nk
k! . (This amounts to selecting from the n bins the k bins into which
the balls are placed.)
3. Counting ballots: Each voter is asked to identify three choices for trustee from eleven
candidates nominated for the position, without specifying any order of preference. Since
a completed ballot is an unordered selection in this situation, each voter has C(11, 3) =
11·10·9
3!
= 165 distinct ways to cast a vote.
4. Counting bit strings with exactly k 0s:
There are
 n
k

bit strings of length n with
exactly k 0s, since each such bit string is determined by choosing a subset of size k from
the n positions; 0s are placed in these k positions, and 1s in the remaining positions.
5. Counting bit strings with at least k 0s: There are
 n
k

+
  n
k+1

+ · · · +
 n
n

bit strings
of length n with at least k 0s, since each such bit string is determined by choosing a
subset of size k, k + 1, . . ., or n from the n positions; 0s are placed in these positions, and
1s in the remaining positions.

Section 2.3
PERMUTATIONS AND COMBINATIONS
103
6. Counting bit strings with equal numbers of 0s and 1s:
For n even, there are
  n
n/2

bit strings of length n with equal numbers of 0s and 1s, since each such bit string is
determined by choosing a subset of size n
2 from the n positions; 0s are placed in these
positions, and 1s in the remaining positions.
7. Counting strings with repeated letters: The word “MISSISSIPPI” has eleven letters,
with “I” and “S” appearing four times each, “P” appearing twice, and “M” once. There
are C(11: 4, 4, 2, 1) =
11!
4!4!2!1! = 34,650 possible diﬀerent strings obtainable by permuting
the letters. This counting problem is equivalent to partitioning 11 items into subsets of
sizes 4, 4, 2, 1.
8. Counting circular strings with repeated letters: See §2.7.1.
9. Counting paths: The number of paths in the plane from (0, 0) to a point (m, n)
(m, n ≥0) that move one unit upward or one unit to the right at each step is
 m+n
n

.
Using U for “up” and R for “right”, each path can be described by a string of m Rs and
n Us.
10. Playoﬀseries: In a series of playoﬀgames, such as the World Series or Stanley Cup
ﬁnals, the winner is the ﬁrst team to win more than half the maximum number of games
possible, n (odd). The winner must win n+1
2
games. The number of possible win-loss
sequences of such a series is 2C(n, n+1
2 ). For example, in the World Series between teams
A and B, any string of length 7 with exactly 4 As represents a winning sequence for A.
(The string AABABBA means that A won a seven-game series by winning the ﬁrst,
second, fourth, and seventh games; the string AAAABBB means that A won the series
by winning the ﬁrst four games.) There are C(7, 4) ways for A to win the World Series,
and C(7, 4) ways for B to win the World Series.
11. Dealing a hand of playing cards: A hand of ﬁve cards (where order does not matter)
can be dealt from a deck of 52 cards in C(52, 5) = 525
5! = 2,598,960 ways.
12. Poker hands: Table 3 contains the number of combinations of ﬁve cards that form
various poker hands (where an ace can be high or low).
13. Counting partial functions:
There are
 k
0

+
 k
1

n +
 k
2

n2 + · · · +
 k
k

nk partial
functions f : A →B where |A| = k and |B| = n. Each partial function is determined by
choosing a domain of deﬁnition for the function, which can be done, for each j = 0, . . . , n,
in
 k
j

ways. Once a domain of deﬁnition is determined, there are nj ways to deﬁne a
function on that set. (The sum can be simpliﬁed to (n + 1)k.)
14.
3
1

= q3−1
q−1 = 1 + q + q2.
15.
6
2

= q6−1
q−1 · q5−1
q2−1 = q6−1
q2−1 · q5−1
q−1 = (q4 + q2 + 1)(q4 + q3 + q2 + q + 1) = 1 + q + 2q2 +
2q3 + 3q4 + 2q5 + 2q6 + q7 + q8. The sum of these coeﬃcients is 15 =
 6
2

, as Fact 11
predicts.
16. A particle moves in the plane from (0, 0) to (n −k, k) by moving one unit at a time
in either the positive x or positive y direction. The number of such paths where the area
bounded by the path, the x-axis, and the vertical line x = n −k is i units is equal to ai,
where ai is the coeﬃcient of qi in the expansion of the Gaussian binomial coeﬃcient
n
k

in Fact 11.

104
Chapter 2
COUNTING METHODS
Table 3: Number of poker hands.
type of hand
formula
explanation
royal ﬂush (ace, king, queen,
jack, 10 in same suit)
4
4 choices for a suit, and 1 royal
ﬂush in each suit
straight ﬂush (5 cards of ﬁve
consecutive ranks, all in one
suit, but not a royal ﬂush)
 4
1

9
4 choices for a suit, and in each suit
there are 9 ways to get ﬁve cards in
a row
four of a kind (4 cards in one
rank and a ﬁfth card)
 13
1
 48
1

13 choices for a rank, only 1 way to
select the four cards in that rank,
and 48 ways to select a ﬁfth card
full house (3 cards of one
rank, 2 of another rank)
13
 4
3

12
 4
2

13 ways to select a rank for the 3-
of-a-kind, and
 4
3

ways to choose
3 of this rank; 12 ways to select a
rank for the pair, and
 4
2

ways to
get a pair of this rank
ﬂush (5 cards in one suit,
but neither royal nor straight
ﬂush)
4
 13
5

−4·10
4 ways to select a suit,
 13
5

ways to
choose ﬁve cards in that suit; then
subtract royal and straight ﬂushes
straight (5 cards in ﬁve con-
secutive ranks, but not all of
the same suit)
10·45−4·10
10 ways to choose ﬁve ranks in a
row, and 4 ways to choose a card
from each rank; then subtract royal
and straight ﬂushes
three of a kind (3 cards of one
rank, and 2 cards of two dif-
ferent ranks)
13
 4
3
 12
2

42
13 ways to select one rank,
 4
3

ways to choose 3 cards of that rank;
 12
2

ways to pick two other ranks;
and 42 ways to pick a card of each
of those two ranks
two pairs (2 cards in each
of two diﬀerent ranks, and a
ﬁfth card of a third rank)
 13
2
 4
2
 4
2

44
 13
2

ways to select two ranks,
 4
2

ways to choose 2 cards in each of
these ranks, and
 44
1

way to pick a
nonmatching ﬁfth card
one pair (2 cards in one rank,
plus 3 cards from three other
ranks)
13
 4
2
 12
3

43
13 ways to select a rank,
 4
2

ways
to choose two cards in that rank;
 12
3

ways to pick three other ranks,
and 43 ways to pick one card from
each of those ranks
2.3.3
SELECTION WITH REPETITION
Some problems concerning counting the number of ways to select k objects from a set
of n objects permit choices of objects to be repeated. Some of these situations are also
modeled by binomial coeﬃcients.
Deﬁnitions:
An ordered selection with replacement is an ordered selection in which each object
in the selection set can be chosen arbitrarily often.
An ordered selection with speciﬁed replacement ﬁxes the number of times each

Section 2.3
PERMUTATIONS AND COMBINATIONS
105
object is to be chosen.
An unordered selection with replacement is a selection in which each object in the
selection set can be chosen arbitrarily often.
The permutation-with-replacement coeﬃcient P R(n, k) is the number of ways to
choose a possibly repeating list of k items from a set of n items.
The combination-with-replacement coeﬃcient CR(n, k) is the number of ways to
choose a multiset of k items from a set of n items.
Facts:
1. An ordered selection with replacement can be thought of as obtaining an ordered list
of names, obtained by selecting an object from a set, writing its name, placing it back in
the set, and repeating the process.
2. The number of ways to make an ordered selection with replacement of k items from n
distinct items (with arbitrary repetition) is nk. Thus P R(n, k) = nk.
3. The number of ways to make an ordered selection of n items from a set of q distinct
items, with exactly ki selections of object i, is
n!
k1!k2!...kq!.
4. An unordered selection with replacement can be thought of as obtaining a collection
of names, obtained by selecting an object from a set, writing its name, placing it back in
the set, and repeating the process. The resulting collection is a multiset (§1.2.1).
5. The number of ways to make an unordered selection with replacement of k items
from a set of n items is C(n + k −1, k). Thus CR(n, k) = C(n + k −1, k).
Combinatorial interpretation: It is suﬃcient to show that the k-multisets that can be
chosen from a set of n items are in one-to-one correspondence with the bit strings of length
(n+k −1) with k ones. To indicate that kj copies of item j are selected, for j = 1, . . . , n,
write a string of k1 ones, then a “0”, then a string of k2 ones, then another “0”, then a
string of k3 ones, then another “0”, and so on, until after the string of kn−1 ones and
the last “0”, there appears the ﬁnal string of kn ones. The resulting bit string has length
n + k −1 (since it has k ones and n −1 zeros). Every such bit string describes a possible
selection. Thus the number of possible selections is C(n+ k −1, k) = C(n+ k −1, n−1).
6. Integer solutions to the equation x1 + x2 + · · · + xn = k:
• The number of nonnegative integer solutions is C(n+k−1, k) = C(n+k−1, n−1).
[In the combinatorial argument of Fact 5, there are n strings of ones. The ﬁrst
string of ones can be regarded as the value for x1, the second string of ones as
the value for x2, etc.]
• The number of positive integer solutions is C(k −1, n −1).
• The number of nonnegative integer solutions where xi ≥ai for i = 1, . . . , n is
C(n + k −1 −(a1 + · · · + an), n −1) (if a1 + · · · + an ≤k). [Let xi = yi + ai
for each i, yielding the equation y1 + y2 + · · · + yn = k −(a1 + · · · + an) to be
solved in nonnegative integers.]
• The number of nonnegative integer solutions where xi ≤ai for i = 1, . . . , n can
be obtained using the inclusion/exclusion principle. See §2.4.2.
Examples:
1. Distinct balls into distinct bins:
k diﬀerently colored balls are to be placed into n
bins, with arbitrarily many balls to a bin. The number of diﬀerent ways to do this is nk.
(Apply the rule of product to the number of possible bin choices for each ball.)

106
Chapter 2
COUNTING METHODS
2. Binary strings:
The number of sequences (bit strings) of length n that can be
constructed from the symbol set {0, 1} is 2n.
3. Colored balls into distinct bins with colors repeated:
k balls are colored so that k1
balls have color 1, k2 have color 2, . . . , and kq have color q. The number of ways these k
balls can be placed into n distinct bins (n ≥k), at most one per bin, is
P (n,k)
k1!k2!...kq!.
Note: This is more general than Fact 2, since n can exceed the sum of all the kis. If n
equals this sum, then P(n, n) = n! and the two formulas agree.
4. When three dice are rolled, the “outcome” is the number of times each of the numbers
1 to 6 appears. For instance, two 3s and a 5 is an outcome. The number of diﬀerent
possible outcomes is C(6 + 3 −1, 3) =
 8
3

= 56.
5. Nondistinct balls into distinct bins with multiple balls per bin allowed: The number
of ways that k identical balls can be placed into n distinct bins, with any number of balls
allowed in each bin, is C(n + k −1, k).
6. Nondistinct balls into distinct bins with no bin allowed to be empty: The number of
ways that k identical balls can be placed into n distinct bins, with any number of balls
allowed in each bin and no bin allowed to remain empty, is C(k −1, n −1).
7. How many ways are there to choose one dozen donuts when there are 7 diﬀerent
kinds of donuts, with at least 12 of each type available? Order is not important, so a
multiset of size 12 is being constructed from 7 distinct types. Accordingly, there are
C(7 + 12 −1, 12) = 18,564 ways to choose the dozen donuts.
8. The number of nonnegative integer solutions to the equation x1 + x2 + · · · + x7 = 12
is C(7 + 12 −1, 12), since this is a rephrasing of Example 7.
9. The number of nonnegative integer solutions to x1 +x2 +· · ·+x5 = 36, where x1 ≥4,
x3 = 11, and x4 ≥7 is C(17, 3). [It is easiest to think of purchasing 36 donuts, where
at least 4 of type 1, exactly 11 of type 3, and at least 7 of type 4 must be purchased.
Begin with an empty bag, and put in 4 of type 1, 11 of type 3, and 7 of type 4. This
leaves 14 donuts to be chosen, and they must be of types 1, 2, 4, or 5, which is equivalent
to ﬁnding the number of nonnegative integer solutions to x1 + x2 + x4 + x5 = 14.]
2.3.4
BINOMIAL COEFFICIENT IDENTITIES
Facts:
1. Table 4 lists some identities involving binomial coeﬃcients.
2. Combinatorial identities, such as those in Table 4, can be proved algebraically by
using techniques such as substitution, diﬀerentiation, or the principle of mathematical
induction (see Facts 4 and 5); they can also be proved by using combinatorial proofs.
(See Fact 3.)

Section 2.3
PERMUTATIONS AND COMBINATIONS
107
Table 4: Binomial coeﬃcient identities.
Factorial expansion
 n
k

=
n!
k!(n−k)!, k = 0, 1, 2, . . ., n
Symmetry
 n
k

=
  n
n−k

, k = 0, 1, 2, . . ., n
Monotonicity
 n
0

<
 n
1

< · · · <
 n
⌊n/2⌋

, n ≥0
Pascal’s identity
 n
k

=
 n−1
k−1

+
 n−1
k

, k = 0, 1, 2, . . ., n
Binomial theorem
(x + y)n = Pn
k=0
 n
k

xkyn−k, n ≥0
Counting all subsets
Pn
k=0
 n
k

= 2n, n ≥0
Even and odd subsets
Pn
k=0(−1)k n
k

= 0, n ≥0
Sum of squares
Pn
k=0
 n
k
2 =
 2n
n

, n ≥0
Square of row sums
Pn
k=0
 n
k
2 = P2n
k=0
 2n
k

, n ≥0
Absorption/extraction
 n
k

= n
k
 n−1
k−1

, k ̸= 0
Trinomial revision
  n
m
 m
k

=
 n
k
  n−k
m−k

, 0 ≤k ≤m ≤n
Parallel summation
Pm
k=0
 n+k
k

=
 n+m+1
m

, m, n ≥0
Diagonal summation
Pn−m
k=0
 m+k
m

=
  n+1
m+1

, n ≥m ≥0
Vandermonde convolution
Pr
k=0
 m
k
  n
r−k

=
 m+n
r

, m, n, r ≥0
Diagonal sums in Pascal’s
P⌊n/2⌋
k=0
 n−k
k

= Fn+1 (Fibonacci numbers), n ≥0
triangle (§2.3.2)
Other common identities
Pn
k=0 k
 n
k

= n2n−1, n ≥0
Pn
k=0 k2 n
k

= n(n + 1)2n−2, n ≥0
Pn
k=0(−1)kk
 n
k

= 0, n ≥0
Pn
k=0
(
n
k)
k+1 = 2n+1−1
n+1 , n ≥0
Pn
k=0(−1)k (n
k)
k+1 =
1
n+1, n ≥0
Pn
k=1(−1)k−1 (
n
k)
k
= 1 + 1
2 + 1
3 + · · · + 1
n, n > 0
Pn−1
k=0
 n
k
  n
k+1

=
  2n
n−1

, n > 0
Pm
k=0
 m
k
  n
p+k

=
 m+n
m+p

, m, n, p ≥0, n ≥p + m
3. The following give combinatorial interpretations of some of the identities involving
binomial coeﬃcients in Table 4.
• Symmetry:
In choosing a subset of k items from a set of n items, the number
of ways to select which k items to include must equal the number of ways to
select which n −k items to exclude.
• Pascal’s recursion:
In choosing k objects from a list of n distinct objects, the
number of ways that include the last object is
 n−1
k−1

, and the number of ways
that exclude the last object is
 n−1
k

. Their sum is then the total number of
ways to choose k objects from a set of n, namely
 n
k

.
• Binomial theorem:
The coeﬃcient of xkyn−k in the expansion (x + y)n = (x +
y)(x + y) . . . (x + y) equals the number of ways to choose k factors from among
the n factors (x + y) in which x contributes to the resultant term.
• Counting all subsets: Summing the numbers of subsets of all possible sizes yields
the total number of diﬀerent possible subsets.
• Sum of squares:
Choose a committee of size n from a group of n men and n
women. The left side, rewritten as
 n
k
  n
n−k

, describes the process of selecting
committees according to the number of men, k, and the number of women,
n −k, on the committee. The right side gives the total number of committees
possible.

108
Chapter 2
COUNTING METHODS
• Absorption/extraction: From a group of n people, choose a committee of size k
and a person on the committee to be its chairperson. Equivalently, ﬁrst select a
chairperson from the entire group, and then select the remaining k−1 committee
members from the remaining n −1 people.
• Trinomial revision: The left side describes the process of choosing a committee of
size m from n people and then a subcommittee of size k. The right side describes
the process where the subcommittee of size k is ﬁrst chosen from the n people
and then the remaining m −k members of the committee are selected from the
remaining n −k people.
• Vandermonde convolution:
Given m men and n women, form committees of
size r. The summands give the numbers of committees broken down by number
of men, k, and number of women, r −k, on the committee; the right side gives
the total number of committees.
4. The formula for counting all subsets can be obtained from the binomial theorem by
substituting 1 for x and 1 for y.
5. The formula for even and odd subsets can be obtained from the binomial theorem by
substituting 1 for x and −1 for y.
6. A set A of size n has 2n−1 subsets with an even number of elements and 2n−1 subsets
with an odd number of elements. (The even and odd subsets identity in Table 4 shows
that P  n
k

for k even is equal to P  n
k

for k odd. Since the total number of subsets
is 2n, each side must equal 2n−1.)
2.3.5
GENERATING PERMUTATIONS AND COMBINATIONS
There are various systematic ways to generate permutations and combinations of the set
{1, . . ., n}.
Deﬁnitions:
A list of strings from an ordered set is in lexicographic order if the strings are sorted
as they would appear in a dictionary.
If the elements in the strings are ordered by a relation <, string a1a2 . . . am precedes
b1b2 . . . bn if any of the following happens: a1 < b1; there is a positive integer k such that
a1 = b1, . . . , ak = bk and ak+1 < bk+1; or m < n and a1 = b1, . . . , am = bm.
Algorithms:
Here k is a given positive integer less than or equal to n and r(k) is a randomly chosen
integer in the range {1, 2, . . ., k} Algorithms 1, 2, and 5 give ways to generate all per-
mutations, k-permutations, and k-combinations of {1, 2, . . ., n} in lexicographic order.
Algorithms 3, 4, and 6 give ways to randomly generate a permutation, k-permutation,
and k-combination of {1, 2, . . ., n}.
Algorithm 1:
Generate permutations of {1, . . . , n} in lexicographic order
a1a2 . . . an := 1 2 . . . n
while a1a2 . . . an ̸= n n−1 . . . 1
m := the rightmost location such that am is followed by a larger number
a′
1a′
2 . . . a′
m−1 := a1a2 . . . am−1
{Retain everything to the left of am}

Section 2.3
PERMUTATIONS AND COMBINATIONS
109
a′
m := the smallest number larger than am to the right of am
a′
m+1a′
m+2 . . . a′
n := everything else, in ascending order
a1a2 . . . an := a′
1a′
2 . . . a′
n
output a1a2 . . . an
Algorithm 2:
Generate k-permutations of {1, . . . , n} in lexicographic order
a1a2 . . . ak := 1 2 . . . k
while a1a2 . . . ak ̸= n n−1 . . . n −(k −1)
m := the rightmost location such that am is followed by a larger number
a′
1a′
2 . . . a′
m−1 := a1a2 . . . am−1
{Retain everything to the left of am}
a′
m := the smallest number larger than am to the right of am
a′
m+1a′
m+2 . . . a′
k := everything else, in ascending order
a1a2 . . . ak := a′
1a′
2 . . . a′
k
output a1a2 . . . ak
Algorithm 3:
Generate a random permutation of {1, . . . , n}.
a1a2 . . . an := 1 2 . . . n
for i := 0 to n −2
interchange an−i and ar(n−i)
output a1a2 . . . an
Algorithm 4:
Generate a random k-permutation of {1, . . . , n}.
a1a2 . . . an := a random permutation of {1, . . . , n} generated from Algorithm 3
output a1a2 . . . ak
Algorithm 5:
Generate k-combinations of {1, . . . , n} in lexicographic order.
a1a2 . . . ak := 1 2 . . . k
{First combination in lexicographic order}
while a1a2 . . . ak ̸= n−k+1 n−k+2 . . . n
m := the rightmost location among 1, . . . , k such that a number larger than
am but smaller than n is not in the combination
a
′
1a
′
2 . . . a
′
m−1 := a1a2 . . . am−1
{Retain everything to the left of am}
a
′
m := am + 1
a
′
m+1 a
′
m+2 . . . a
′
k := am+2 am+3 . . . am+k−m+1 {Continue consecutively}
output a1a2 . . . ak := a
′
1a
′
2 . . . a
′
k
Algorithm 6:
Generate a random k-combination of {1, . . . , n}.
a1a2 . . . ak := a k-permutation of {1, . . . , n} generated by Algorithm 4
output a1a2 . . . ak
{Ignore the order in which elements are written}
Examples:
1. The lexicographic order for the 3-permutations of {1, 2, 3} is 123, 132, 213, 231, 312,
321.

110
Chapter 2
COUNTING METHODS
2. The lexicographic order of the C(5, 3) = 10 3-combinations of {1, 2, 3, 4, 5} is 123,
124, 125, 134, 135, 145, 234, 235, 245, 345.
3. Generating permutations:
What permutation follows 3142765 in the lexicographic
ordering of the permutations of {1, . . . , 7}? Step 1 of the while-loop of Algorithm 1 leads
to the fourth digit, namely the digit 2, as the ﬁrst digit from the right that has larger
digits following it. Steps 2 and 3 show that the next permutation starts with 3145 since 5
is the smallest digit greater than 2 and following it. Finally, Step 4 yields 2, 6, and 7 (in
numerical order) as the digits that follow. Thus, the permutation immediately following
3142765 is 3145267.
4. Generating combinations:
What 5-combination follows 12478 in the lexicographic
ordering of 5-combinations of {1, . . ., 8}? Step 1 of the while-loop of Algorithm 5 leads
to the third digit, namely the digit 4, as the ﬁrst digit from the right that can be safely
increased by 1. Step 2 shows that the next permutation starts with 125 since the third
digit is increased by 1. Finally, Step 3 yields 6 and 7 as the following digits (add 1 to
the newly-listed previous digit until the new selection of k digits is complete). Thus, the
combination after 12478 is 12567.
2.4
INCLUSION/EXCLUSION
The principle of inclusion/exclusion is used to count the elements in a non-disjoint union
of ﬁnite sets. Many counting problems can be solved by applying this principle to a
well-chosen collection of sets. The techniques involved in this process are best illustrated
with examples.
2.4.1
PRINCIPLE OF INCLUSION/EXCLUSION
The number of elements in the union of two ﬁnite sets A and B is |A| + |B|, provided
that the sets have no element in common. In the general case, however, some elements
in common to both sets have been included in the sum twice. The sum is adjusted to
exclude the double-counting of these common elements by subtracting their number:
|A ∪B| = |A| + |B| −|A ∩B| .
Similarly, the number of elements in the union of three ﬁnite sets is
|A ∪B ∪C| = |A| + |B| + |C| −|A ∩B| −|A ∩C| −|B ∩C| + |A ∩B ∩C|.
The following Venn diagram (§1.2.2) illustrates these two cases. These simple equations
generalize to the case of n sets.
Facts:
1. Inclusion/exclusion principle:
The number of elements in the union of n ﬁnite sets
A1, A2, . . . , An is
|A1 ∪A2 ∪· · · ∪An|
=
P
1≤i≤n
|Ai| −
P
1≤i<j≤n
|Ai ∩Aj| +
P
1≤i<j<k≤n
|Ai ∩Aj ∩Ak|
−· · · + (−1)n+1 |A1 ∩A2 ∩· · · ∩An|

Section 2.4
INCLUSION/EXCLUSION
111
A
B
A
B
C
or, alternatively,
|A1 ∪A2 ∪· · · ∪An| =
nP
k=1
(−1)k+1
P
1≤i1<···<ik≤n
|Ai1 ∩Ai2 ∩· · · ∩Aik| .
Sometimes the inner sum of the alternative formula is denoted Sk.
2. The inclusion/exclusion formula for n sets has 2n −1 terms, one for each possible
nonempty intersection. The coeﬃcient of a term is −1 if the term corresponds to inter-
sections of an even number of sets, and +1 otherwise.
3. The principle is often applied to the complement of a set. Let Ai be the subset of
elements in a universal set U that have property Pi . The number of elements that have
properties Pi1, Pi2, . . . , Pik is often written N(Pi1Pi2 . . . Pik) and the number of elements
that have none of these properties is often written N(P ′
i1P ′
i2 . . . P ′
ik). The number of
elements in U that have none of the properties is
N(P ′
1P ′
2 . . . P ′
n) = |U| −
X
1≤i≤n
N(Pi) +
X
1≤i<j≤n
N(PiPj) −· · · + (−1)nN(P1P2 . . . Pn).
Examples:
1. Of 70 people surveyed, 37 drink coﬀee, 23 drink tea, and 25 drink neither. Find the
number who drink both coﬀee and tea. Using C to represent the set of coﬀee drinkers
and T to represent the set of tea drinkers, the size of C∩T must be found. Since |T ∪C| =
25, the Venn diagram in part (a) of the following ﬁgure shows that |C∪T | = 45. According
to the inclusion/exclusion principle, |C ∩T | = |C| + |T | −|C ∪T | = 37 + 23 −45 = 15,
illustrated in part (b) of the ﬁgure.
45
25
(a)
15
25
(b)
C
T
C
T
8
22
2. Suppose that 16 high-school juniors enroll in Algebra, 17 in Biology, and 30 in Chem-
istry; that 5 students enroll in both Algebra and Biology, 4 in both Algebra and Chem-
istry, and 7 in both Biology and Chemistry; that 3 students enroll in all three; and that
every junior takes at least one of these three subjects. Then the total number of students
in the junior class is 16 + 17 + 30 −(5 + 4 + 7) + 3 = 50.
3. Each of 11 linguists translates at least one of the languages Amharic and Burmese
into English. The numbers who translate only Amharic or Burmese are both odd primes.
More linguists translate Burmese than Amharic. How many can translate Amharic?
Based on experimentation or on an analytic approach, the only possible assignment of
numbers to regions that ﬁts all these facts leads to 6, as shown in the following ﬁgure.

112
Chapter 2
COUNTING METHODS
5
Amharic
3
3
Burmese
4. At a party for 28 people, three kinds of pizza were served: anchovy, broccoli, and
cheese. Everyone ate at least one kind. No two of the seven diﬀerent possible selections
of one or more kinds of pizza were eaten by the same number of partygoers. Each of the
three possible exclusive selections (one kind of pizza only) was eaten by an odd number
of partygoers, and each of the three possible combinations of two kinds of pizza was eaten
by an even number of partygoers. If a total of 18 partygoers ate cheese pizza, how many
ate both anchovy and broccoli?
The answer is 2.
Experimentation or an analytic approach leads to the possible as-
signments of numbers to regions that ﬁt all these facts, shown in the following ﬁgure.
anchovy
2
1
7
broccoli
cheese
3 (or 5)
5 (or 3)
4 (or 6)
6 (or 4)
5. To count the number of ways to select a 5-card hand from a standard 52-card deck
so that the hand contains at least one card from each of the four suits, let A1, A2, A3,
and A4 be the subsets of 5-card hands that do not contain a club, diamond, heart, or
spade, respectively. Then
|Ai| =
 52−13
5

=
 39
5

with
 4
1

choices for i
|Ai ∩Aj| =
 52−26
5

=
 26
5

with
 4
2

choices for i and j
|Ai ∩Aj ∩Ak| =
 52−39
5

=
 13
5

with
 4
3

choices for i, j, and k.
There are
 52
5

possible 5-card hands, so by complementation and the principle of inclu-
sion/exclusion, those that contain at least one card from each suit is
 52
5

−
 4
1
 39
5

+
 4
2
 26
5

−
 4
3
 13
5

= 685,464.
2.4.2
APPLYING INCLUSION/EXCLUSION TO COUNTING PROBLEMS
Deﬁnitions:
A derangement on a set is a permutation that leaves no element ﬁxed. The number of
derangements on a set of cardinality n is denoted Dn.
A rencontre number Dn,k is the number of permutations on a set of n elements that
leave exactly k elements ﬁxed.

Section 2.4
INCLUSION/EXCLUSION
113
Facts:
1. The number of onto functions from an n-element set to a k-element set (n ≥k) is
Pk
j=0(−1)j k
j

(k −j)n
(See Example 3.)
2. The following binomial coeﬃcient identities can all be derived by combinatorial ar-
guments using inclusion/exclusion:
• Pm
k=0(−1)k n
k
  n−k
m−k

= 0
• Pn
k=m(−1)k−m n
k

=
  n−1
m−1

• Pn
k=0(−1)k n
k

( n−k+r−1
r
) =
 r−1
n−1

.
3. Dn = n!(1 −1
1! + 1
2! −· · · + (−1)n 1
n!). (See Example 8.)
4.
Dn
n! →e−1 ≈0.368 as n →∞.
5. Dn = nDn−1 + (−1)n for n ≥1.
6. Dn = (n −1)(Dn−1 + Dn−2) for n ≥2.
7. The following table gives some values of Dn:
n
Dn
n
Dn
n
Dn
n
Dn
1
0
4
9
7
1,854
10
1,334,961
2
1
5
44
8
14,833
11
14,684,570
3
2
6
265
9
133,496
12
176,214,841
8. Dn,0 = Dn.
9. Dn,k =
 n
k

Dn−k
10. The following table gives some values of Dn,k:
n\k
0
1
2
3
4
5
6
7
8
9 10
0
1
1
0
1
2
1
0
1
3
2
3
0
1
4
9
8
6
0
1
5
44
45
20
10
0
1
6
265
264
135
40
15
0
1
7
1,854
1,855
924
315
70
21
0
1
8
14,833
14,832
7,420
2,464
630
112
28
0
1
9
133,496
133,497
66,744
22,260
5,544
1,134
168
36
0 1
10
1,334,961 1,334,960 667,485 222,480 55,650 11,088 1,890 240 45 0
1
Examples:
1. The inclusion/exclusion principle can be used to establish the binomial coeﬃcient
identity
  n
m

=
m
P
k=1
(−1)k+1  n−k
m−k
 n
k

.
Let Ai denote the subset of m-combinations that contain object i.
Thus, the k-fold
intersection Ai1 ∩Ai2 ∩· · · ∩Aik consists of all the m-combinations that contain all the

114
Chapter 2
COUNTING METHODS
objects i1, i2, . . . , ik. Since there are
  n−k
m−k

ways to complete an m-combination in this
intersection, it follows that |Ai1 ∩Ai2 ∩· · ·∩Aik| =
  n−k
m−k

. Since the k objects themselves
can be speciﬁed in
 n
k

ways, it follows that
P
1≤i1<i2<···<ik≤n
|Ai1 ∩Ai2 ∩· · · ∩Aik| =
  n−k
m−k
 n
k

,
k ≤m.
Since A1 ∪A2 ∪· · · ∪An is the set of all m-combinations selected from 1, 2, . . . , n that
contain at least one of the objects 1, 2, . . . , n, it must be the set of all m-combinations.
2. Sieve of Eratosthenes:
The sieve of Eratosthenes (276–194 BCE) is a method for
ﬁnding all primes less than or equal to a given positive integer n. Begin with the list
of integers 2 through n, and delete all multiples of the ﬁrst number in the list, 2, but
not including 2. The ﬁrst integer remaining after 2 is 3; delete all multiples of 3, not
including 3. The ﬁrst integer remaining after 3 is 5; delete all multiples of 5, not including
5. Continue the process. The remaining integers are the primes less than or equal to n.
(See §4.4.2.)
The inclusion/exclusion principle can be used to obtain the number of primes less than
or equal to n. (A number x ≤n is prime if and only if x has a prime factor less than
or equal to ⌊√n⌋.) Let Pi be the property: a number is greater than the ith prime
and divisible by the ith prime. Then the number of primes less than or equal to n is
N(P ′
1P ′
2 . . . P ′
k), where there are k primes less than or equal to ⌊√n⌋. (§2.4.1, Fact 3.)
For example, the number of primes less than or equal to 100 is N(P ′
1P ′
2P ′
3P ′
4) = 99 −
 100
2

−
 100
3

−
 100
5

−
 100
7

+
 100
2·3

+
 100
2·5

+
 100
2·7

+
 100
3·5

+
 100
3·7

+
 100
5·7

−
 100
2·3·5

−
 100
2·3·7

−
 100
2·5·7

−
 100
3·5·7

+

100
2·3·5·7

= 99 −50 −33 −20 −14 + 16 + 10 + 7 + 6 + 4 + 2 −
3 −2 −1 −0 + 0 = 21.
3. Number of onto functions:
The number of onto functions from an n-element set to
a k-element set (n ≥k) is Pk
j=0(−1)j k
j

(k −j)n. The number of onto functions from
an n-element set to a k-element set equals the number of ways that n diﬀerent objects
can be distributed among k diﬀerent boxes with none left empty. Let Ai be the subset
of distributions with box i empty. Then
|Ai| = (k −1)n
with
 k
1

choices for i
|Ai ∩Aj| = (k −2)n
with
 k
2

choices for i and j
...
|Ai1 ∩Ai2 ∩· · · ∩Aik| = (k −k)n
with
 k
k

choices for i1, i2, . . . , ik.
The number of distributions that leave no box empty is then Pk
j=0(−1)j k
j

(k −j)n.
The number of onto functions from an n-element set to a k-element set for some values
of n and k (n ≥k) are given in the following table:
n\k
1
2
3
4
5
6
7
8
9
1
1
2
1
2
3
1
6
6
4
1
14
36
24
5
1
30
150
240
120
6
1
62
540
1560
1800
720
7
1
126
1806
8400
16,800
15,120
5,040
8
1
254
5796
40,824 126,000
191,520
141,120
40,320
9
1
510 18,150 186,480 834,120 1,905,120 2,328,480 1,451,520 362,880

Section 2.4
INCLUSION/EXCLUSION
115
4. There are 584 nonnegative integer solutions to x1 + x2 + x3 + x4 = 20 where x1 ≤8,
x2 ≤10, and x3 ≤5. [Let A1 be the set of solutions where x1 ≥9, A2 the set of solutions
where x2 ≥11, and A3 the set of solutions where x3 ≥6. The ﬁnal answer, obtained
using the inclusion/exclusion principle and the techniques used in the examples of §2.3.3,
is equal to C(23, 3)−|A1∪A2∪A3| = C(23, 3)−(C(14, 3)+C(12, 3)+C(17, 3)−C(3, 3)−
C(8, 3) −C(6, 3) + 0) = 584.]
5. The permutations
 1 2 3
2 3 1

and
 1 2 3
3 1 2

are derangements of 1, 2, 3, but the permutations
 1 2 3
1 2 3

,
 1 2 3
1 3 2

,
 1 2 3
3 2 1

, and
 1 2 3
2 1 3

are not.
6. Probl`eme des rencontres: In the probl`eme des rencontres (matching problem) an urn
contains balls numbered 1 through n, and they are drawn out one at a time. A match
occurs if ball i is the ith ball drawn. The probability that no matches occur when all
the balls are drawn is Dn
n! . The problem was studied by Pierre-R´emond de Montmort
(1678–1719) who studied the card game treize, in which matchings of pairs of cards were
counted when two decks of cards were laid out face-up.
7. Probl`eme des m´enages:
The probl`eme des m´enages, ﬁrst raised by Fran¸cois Lucas
(1842–1891), requires that n married couples be seated around a circular table so that no
men are adjacent, no women are adjacent, and no husband and wife are adjacent. There
are 2n! Pn
1=0(−1)i(n −i)!
 2n−i
i

2n
2n−i ways to seat the people. (There are 2n! ways to
seat the n women. Regardless of how this is done, by the inclusion/exclusion principle
there are Pn
1=0(−1)i(n −i)!
 2n−i
i

2n
2n−i ways to seat the n men.)
8. Determining the number Dn of derangements of {1, . . ., n}: Let Ai be the subset of
permutations that ﬁx object i. The permutations in the subset A1 ∪A2 ∪· · · ∪An are
those that ﬁx at least one object. Then
|Ai| = (n −1)!
with
 n
1

choices for i
|Ai ∩Aj| = (n −2)!
with
 n
2

choices for i and j
...
|Ai1 ∩Ai2 ∩· · · ∩Aik| = (n −k)!
with
 n
k

choices for i1, i2, . . . , ik
Complementation and inclusion/exclusion now yield the formula in Fact 3:
Dn = n! −Pn
k=1(−1)k+1 n
k

(n −k)! = n!Pn
k=0(−1)k 1
k!.
As n becomes large, Dn
n! approaches e−1 ≈0.368 very rapidly.
9. Hatcheck problem:
The hatchecker at a restaurant neglects to place claim checks
on n hats. Each of the n customers is given a randomly selected hat upon exiting. What
is the probability that no one receives the correct hat?
There are n! possible permutations of the n hats, and there are Dn cases in which no
one gets the correct hat. Thus, by Example 8, the probability is approximately e−1,
regardless of the number of diners.
10. Rook polynomials/arrangements of objects with restricted positions: This describes
a family of assignment or matching problems, such as matching applicants to jobs where
some applicants cannot be assigned to certain jobs, the probl`eme des m´enages, and the
probl`eme des rencontres. In terms of matching n applicants to n jobs, set up an n × n
“board of possibilities” where the rows are labeled by the applicants and the columns
are labeled by the jobs. Square (i, j) is a forbidden square if applicant i cannot perform
job j; the remaining squares are allowable squares.
An allowable arrangement is an
arrangement where only allowable squares are chosen, with exactly one square chosen in
each row and column.

116
Chapter 2
COUNTING METHODS
These problems can be rephrased in terms of placing rooks on a chessboard: given a
chessboard with some squares forbidden, ﬁnd the number of ways of placing rooks on
the allowable squares of the chessboard so that no rook can capture any other rook. (In
chess a rook can move any number of squares vertically or horizontally.) For a given
n×n board B, let Ai be the number of ways to place n nontaking rooks on B so that the
rook in row i is on a forbidden square. The total number of ways to place n nontaking
rooks on allowable squares is
n! −|A1 ∪· · · ∪An| = n! −r1(B)(n −1)! + r2(B)(n −2)! −· · · + (−1)nrn(B)0!
where the coeﬃcients ri(B) are the number of ways to place i nontaking rooks on for-
bidden squares of B.
A rook polynomial for an n × n board B is a polynomial of the form
R(x, B) = r0(B) + r1(B)x + r2(B)x2 + · · · + rn(B)xn,
where r0(B) is deﬁned to be 1.
The numbers ri(B) can sometimes be found more easily by using a combination of the
following two reduction techniques:
• R(x, B) = R(x, B1) · R(x, B2), if all forbidden squares of B appear in two disjoint
sub-boards B1 and B2 (the sub-boards B1 and B2 are disjoint if the row labels
of B are partitioned into two parts S1 and S2, the column labels of B are
partitioned into two parts T1 and T2, and B1 is obtained from S1 × T1 and B2
is obtained from S2 × T2).
• R(x, B) = xR(x, B1)+R(x, B2), where there is a square (i, j) of B, B1 is obtained
from B by removing all squares in row i and all squares in column j, and B2 is
obtained from B by making square (i, j) allowable.
It may be necessary to use these techniques repeatedly to obtain boards that are simple
enough that the rook polynomial coeﬃcients can be easily found.
11. Rook polynomials can be used to ﬁnd the number of derangements of n objects.
The forbidden squares of the board B are the squares (i, i). The ﬁrst reduction tech-
nique of Example 10 used repeatedly breaks B into B1, . . . , Bn where Bi consists only of
square (i, i). Then
R(x, B) = R(x, B1)R(x, B2) . . . R(x, Bn) = (1 + x) . . . (1 + x) = (1 + x)n = Pn
i=0
 n
i

xi.
Therefore, the number of derangements is
n! −
 n
1

(n −1)! −
 n
2

(n −2)! + · · · + (−1)n+1 n
n

0!

= n!Pn
k=0(−1)k 1
k!.
2.5
PARTITIONS
Each way to write a positive integer n as a sum of positive integers is called a partition of
n. Similarly, each way to decompose a set S into a family of mutually disjoint nonempty
subsets is called a partition of S. In a cyclic partition of a set, the elements of each
subset are arranged into cycles, and two cyclic partitions in the same family of subsets

Section 2.5
PARTITIONS
117
are distinct if any of the cycle arrangements are diﬀerent. The main concerns are counting
the number of essentially diﬀerent partitions of integers and sets, and with counting cyclic
partitions of sets.
2.5.1
PARTITIONS OF INTEGERS
A positive integer can be decomposed into a sum of positive integers in various ways,
taking into account restrictions on the number of parts or on the properties of the parts.
Deﬁnitions:
A partition of a positive integer n is a representation of n as the sum of positive integers.
The parts are usually written in nonascending order, but order is ignored.
A Ferrers diagram or Young diagram of a partition is an array of boxes, nodes, or
dots into rows of nonincreasing size so that each row represents one part of the partition
(also see §2.8 on Young Tableaux).
The conjugate of a partition is the partition obtained by transposing the rows and
columns of its Ferrers diagram.
Suppose we have two distinguished copies of the positive integers: one ordinary copy and
one copy where each integer k is marked with an overline (¯k). An overpartition of a
positive integer n is a representation of n as a sum of positive integers in which at most
one occurrence of each integer appearing as a part may be overlined.
A composition is a partition in which the order of the parts is taken into account.
A vector partition is a decomposition of an n-tuple of nonnegative integers into a sum
of nonzero n-tuples of nonnegative integers, where order is ignored.
A vector composition is the same as a vector partition, except that order is taken into
account.
Facts:
1. The following table gives the notation for various functions that count partitions:
function
type of partitions counted
p(n)
number of partitions of n
Q(n)
number of partitions of n into distinct parts
O(n)
number of partitions of n into odd parts
pm(n)
number of partitions of n with at most m parts
qm(n)
number of partitions of n with no part larger than m
p(N, M, n)
number of partitions of n into at most M parts, with
each part no larger than N
¯p(n)
number of overpartitions of n
2. p(m, n, n) = qm(n).
3. p(n, m, n) = pm(n).
4. pm(n) = qm(n).
5. O(n) = Q(n).
6. The number of compositions of n into k parts is
 n−1
n−k

=
 n−1
k−1

.

118
Chapter 2
COUNTING METHODS
7. The number of compositions of n is 2n−1.
8. The number of compositions of n + 1 into parts greater than 1 = the number of
compositions of n into odd parts = the number of compositions of n −1 into 1s and 2s
= Fn, where Fn denotes the nth Fibonacci number: F0 = 0, F1 = 1, Fn = Fn−1 + Fn−2
for n ≥2 (§3.1.2).
9. The partition function p(n) satisﬁes these congruences (see [Kn93] for details):
p(5n + 4) ≡0 (mod 5)
p(7n + 5) ≡0 (mod 7)
p(11n + 6) ≡0 (mod 11).
10. The partition functions p(n) and pm(n) satisfy these recurrences:
p(n) −p(n −1) −p(n −2) + p(n −5) + p(n −7) + · · ·
+ (−1)kp(n −k
2(3k −1)) + (−1)kp(n −k
2 (3k + 1)) + · · · = 0, n > 0
pm(n) = pm(n −m) + pm−1(n).
11. The asymptotic behavior of p(n), Q(n), pm(n), and ¯p(n) is as follows (see Chapters
5 and 6 of [An98], [HaRa18], or [Kn93] for details):
p(n) ∼
1
4n
√
3 eπ√
2n/3
as n →∞,
Q(n) ∼
1
4 · 31/4 n−3/4eπ√
n/3
as n →∞,
¯p(n) ∼1
8neπ√n
as n →∞,
pm(n) ∼
nm−1
m!(m −1)!
as n →∞, with m ﬁxed.
12. The following are generating functions (§3.2) for partition functions:
X
n≥0
p(n)qn =
∞
Y
i=1
(1 + qi + qi+i + · · · ) =
∞
Y
i=1
 ∞
X
m=0
qmi
!
=
∞
Y
i=1
1
1 −qi
=
 
∞
X
k=−∞
(−1)kqk(3k−1)/2
!−1
= eπiτ/12
η(τ) ,
where q = e2πiτ and η(τ) is the Dedekind eta function.
X
n≥0
Q(n)qn =
∞
Y
i=1
(1 + qi).
X
n≥0
pm(n)qn =
m
Y
i=1
(1 + qi + qi+i + · · · ) =
m
Y
i=1
 ∞
X
m=0
qmi
!
=
m
Y
i=1
1
1 −qi .
X
n≥0
p(N, M, n)qn =
N
Y
j=1
(1 −qN+M+1−j)
1 −qj
=
QN+M
j=1
(1 −qj)
QN
j=1(1 −qj) QM
j=1(1 −qj)
.

Section 2.5
PARTITIONS
119
X
n≥0
¯p(n)qn =
∞
Y
i=1
1 + qi
1 −qi =
 
∞
X
k=−∞
(−1)kqk2
!−1
=
1
ϑ4(0, q),
where ϑ4(z, q) = P∞
n=−∞(−1)nqn2e2niz is an elliptic theta function
of Jacobi; see Chapter XXI of [WhWa96].
Note:
Even though these expressions for p(N, M, n) look like quotients of polynomials
they are actually just polynomials of degree NM. They are called Gaussian polynomials
or q-binomial coeﬃcients. (See Chapters 1 and 2 of [An98], Chapter 19 of [HaWr08], or
[Ma04] for details. Also see §2.3.2.)
13. The following are additional generating functions for partition functions (see Chap-
ter 2 of [An98] or Section 8.10 of [GaRa04] for details):
∞
X
n=1
p(n)qn = 1 +
∞
X
n=1
qn
(1 −q)(1 −q2) · · · (1 −qn)
= 1 +
∞
X
n=1
qn2
(1 −q)2(1 −q2)2 · · · (1 −qn)2
∞
X
n=1
Q(n)qn = 1 +
∞
X
n=1
qn(n+1)/2
(1 −q)(1 −q2) · · · (1 −qn)
= 1 + q +
∞
X
n=2
qn(1 + q)(1 + q2) · · · (1 + qn−1)
∞
X
n=1
pm(n)qn = 1 +
∞
X
n=1
(1 −qm)(1 −qm+1) · · · (1 −qm+n−1)
(1 −q)(1 −q2) · · · (1 −qn)
qn
∞
X
n=1
¯p(n)qn = 1 +
∞
X
n=1
qn(n+1)/2(1 + 1)(1 + q)(1 + q2) · · · (1 + qn−1)
(1 −q)2(1 −q2)2 · · · (1 −qn)2
.
14. See [GrKnPa94] for an algorithm for generating partitions.
15. The following table gives some values of pm(n). More extensive tables appear in
[GuGwMi58]:
n\m
0
1
2
3
4
5
6
7
8
9
10
0
1
1
1
1
1
1
1
1
1
1
1
1
0
1
1
1
1
1
1
1
1
1
1
2
0
1
2
2
2
2
2
2
2
2
2
3
0
1
2
3
3
3
3
3
3
3
3
4
0
1
3
4
5
5
5
5
5
5
5
5
0
1
3
5
6
7
7
7
7
7
7
6
0
1
4
7
9
10
11
11
11
11
11
7
0
1
4
8
11
13
14
15
15
15
15
8
0
1
5
10
15
18
20
21
22
22
22
9
0
1
5
12
18
23
26
28
29
30
30
10
0
1
6
14
23
30
35
38
40
41
42

120
Chapter 2
COUNTING METHODS
16. The following table gives values for p(n) and Q(n):
n
p(n)
Q(n)
n
p(n)
Q(n)
n
p(n)
Q(n)
0
1
1
17
297
38
34
12,310
512
1
1
1
18
385
46
35
14,883
585
2
2
1
19
490
54
36
17,977
668
3
3
2
20
627
64
37
21,637
760
4
5
2
21
792
76
38
26,015
864
5
7
3
22
1,002
89
39
31,185
982
6
11
4
23
1,255
104
40
37,338
1,113
7
15
5
24
1,575
122
41
44,583
1,260
8
22
6
25
1,958
142
42
53,174
1,426
9
30
8
26
2,436
165
43
63,261
1,610
10
42
10
27
3,010
192
44
75,175
1,816
11
56
12
28
3,718
222
45
89,134
2,048
12
77
15
29
4,565
256
46
105,558
2,304
13
101
18
30
5,604
296
47
124,754
2,590
14
135
22
31
6,842
340
48
147,273
2,910
15
176
27
32
8,349
390
49
173,525
3,264
16
231
32
33
10,143
448
50
204,226
3,658
Examples:
1. The number 4 has ﬁve partitions:
4
3 + 1
2 + 2
2 + 1 + 1
1 + 1 + 1 + 1.
2. The number 4 has eight compositions:
4
1 + 3
3 + 1
2 + 2
2 + 1 + 1
1 + 2 + 1
1 + 1 + 2
1 + 1 + 1 + 1.
3. The number 4 has fourteen overpartitions:
4
¯4
3+1
¯3+1
3+¯1
¯3+¯1
2+2
2+¯2
2+1+1
¯2+1+1
2+1+¯1
¯2+1+¯1
1 + 1 + 1 + 1
1 + 1 + 1 + ¯1
4. The four vector partitions of (2, 1) are
(2, 1)
(2, 0) + (0, 1)
(1, 0) + (1, 0) + (0, 1)
(1, 0) + (1, 1)
5. The partition 5 + 4 + 4 + 2 + 1 + 1 + 1 of 18 has the Ferrers diagram in part (a) of the
following ﬁgure. Its conjugate is the partition 7 + 4 + 3 + 3 + 1, with the Ferrers diagram
in part (b) of the ﬁgure.
6. Identical balls into identical bins: The number of ways that n identical balls can be
placed into k identical bins, with any number of balls allowed in each bin, is given by
pk(n).
7. Identical balls into identical bins with no bin allowed to be empty:
The number
of ways that n identical balls can be placed into k identical bins (n ≥k), with any
number of balls allowed in each bin and no bin allowed to remain empty, is given by
pk(n) −pk−1(n).

Section 2.5
PARTITIONS
121
(a)
(b)
8. Making change: Imagine an unlimited supply of silver coins in all possible unit denom-
inations 1, 2, 3, 4, . . ., and exactly one gold coin for each unit denomination 1, 2, 3, 4, . . ..
Within a given denomination, the coins are only distinguishable by metal content. The
number of ways to “make change” for n units using only the silver coins is p(n), while
the number of ways to make change for n units using the available silver and gold coins
is ¯p(n).
2.5.2
STIRLING COEFFICIENTS
Deﬁnitions:
A cyclic partition of a set is a partition of the set (into disjoint subsets whose union
is the entire set) where the elements of each subset are arranged into cycles. Two cyclic
partitions using the same family of subsets are distinct if any of the cycle arrangements
are diﬀerent.
The Stirling cycle number
n
k

is the number of ways to partition n objects into k
nonempty cycles.
The Stirling number of the ﬁrst kind s(n, k) is the coeﬃcient of xk in the polynomial
x(x −1)(x −2) . . . (x −n + 1). Thus,
n
X
k=0
s(n, k)xk = x(x −1)(x −2) . . . (x −n + 1).
The Stirling subset number
n
k
	
is the number of ways to partition a set of n objects
into k nonempty subsets.
The Stirling numbers of the second kind S(n, k) are deﬁned implicitly by the equa-
tion
xn =
n
X
k=0
S(n, k)x(x −1)(x −2) . . . (x −k + 1).
The Bell number Bn (§3.1.8) is the number of partitions of a set of n objects. (Eric
Temple Bell, 1883–1960)
Facts:
1. s(n, k)(−1)n−k =
n
k

.
2. S(n, k) =
n
k
	
.

122
Chapter 2
COUNTING METHODS
3. The following table gives Stirling numbers of the ﬁrst kind, s(n, k):
n\k 0
1
2
3
4
5
6
7
8
9
10
0
1
1
0
1
2
0
−1
1
3
0
2
−3
1
4
0
−6
11
−6
1
5
0
24
−50
35
−10
1
6
0
−120
274
−225
85
−15
1
7
0
720
−1,764
1,624
−735
175
−21
1
8
0
−5,040
13,068
−13,132
6,769
−1,960
322
−28
1
9
0
40,320 −109,584
118,124 −67,284
22,449 −4,536
546 −36
1
10 0 −362,880 1,026,576 −1,172,700 723,680 −269,325 63,273 −9,450 870 −45
1
4. The following table gives Stirling subset numbers of the second kind, S(n, k) =
n
k
	
:
n\k
0
1
2
3
4
5
6
7
8
9
10
0
1
1
0
1
2
0
1
1
3
0
1
3
1
4
0
1
7
6
1
5
0
1
15
25
10
1
6
0
1
31
90
65
15
1
7
0
1
63
301
350
140
21
1
8
0
1
127
966
1,701
1,050
266
28
1
9
0
1
255
3,035
7,770
6,951
2,646
462
36
1
10
0
1
511
9,330
34,501
42,525
22,827
5,880
750
45
1
5. Bn =
nP
k=1
n
k
	
.
6. The ﬁrst ﬁfteen Bell numbers are:
B1 = 1
B2 = 2
B3 = 5
B4 = 15
B5 = 52
B6 = 203
B7 = 877
B8 = 4,140
B9 = 21,147
B10 = 115,975
B11 = 678,570
B12 = 4,213,597
B13 = 27,644,437
B14 = 190,899,322
B15 = 1,382,958,545.
7. The following table lists some identities involving Stirling numbers:

Section 2.5
PARTITIONS
123
n
k

= (n −1)
n−1
k

+
n−1
k−1

, (k > 0)
cycle number recursion
n
0

=
(
0,
if n ̸= 0
1,
if n = 0
n
k
	
= k
n−1
k
	
+
n−1
k−1
	
, (k > 0)
subset number recursion
n
0
	
=
(
0,
if n ̸= 0
1,
if n = 0
X
k
n
k
 k
m
	
(−1)n−k =
(
0,
if n ̸= m
1,
if n = m
inversion formulas
X
k
n
k
	 k
m

(−1)n−k =
(
0,
if n ̸= m
1,
if n = m
n
1
	
=
n
n
	
= 1
n
2
	
= 2n−1 −1
n
k
	
k! = the number of onto functions
from an n-set to a k-set
n
X
k=0
n
k

= n!
∞
X
n=0
S(n + k, k)xn =
1
(1 −x)(1 −2x) . . . (1 −kx)
∞
X
n=0
s(n, k)xn
n!
= (log(1 + x))k
k!
∞
X
n=0
S(n, k)xn
n!
= 1
k!(ex −1)k
8. The following give combinatorial interpretations of some of the identities involving
Stirling numbers:
• Stirling cycle number recursion: When partitioning n objects into k cycles, there
are
n−1
k−1

ways in which the last object has a cycle to itself. Otherwise, there
are
n−1
k

ways to partition the other n−1 objects into k cycles, and then n−1
choices of a location into which the last object can be inserted.
• Stirling subset number recursion: When partitioning n objects into k nonempty
subsets, there are
n−1
k−1
	
ways in which the last object has a subset to itself.
Otherwise, there are
n−1
k
	
ways to partition the other n −1 objects into k
subsets, and then k choices of a subset into which the last object can be inserted.
• Pn
k=0
n
k

= n! :
The partitions into cycles are in a one-to-one correspondence
with the permutations of n objects, since each permutation can be represented
as a composition of disjoint cycles.
•
n
k

is the number of ways to seat n individuals around k identical circular tables
so that no table is empty.
• Hn = 1
n!
n+1
2

, where Hn is the nth Harmonic number (§3.1.7).

124
Chapter 2
COUNTING METHODS
Examples:
1. x(x−1)(x−2)(x−3) = x4−6x3+11x2−6x, and hence there are
4
2

= 11 permutations
of {1, 2, 3, 4} with 2 cycles:
(12)(34), (13)(24), (14)(23), (1)(234), (1)(324), (2)(134),
(2)(314), (3)(124), (3)(214), (4)(123), (4)(213). Also, s(4, 2) = (−1)4−2 · 11.
2. H3 = 1 + 1
2 + 1
3 = 11
6 . As seen in Example 1,
4
2

= 11. So we have H3 = 1
3!
4
2

.
3. x4 = x(x−1)(x−2)(x−3)+6x(x−1)(x−2)+7x(x−1)+x, and hence there are exactly
4
2
	
= 7 set-partitions of {1, 2, 3, 4} into two blocks:
{1} & {2, 3, 4}, {2} & {1, 3, 4},
{3} & {1, 2, 4}, {4} & {1, 2, 3}, {1, 2} & {3, 4}, {1, 3} & {2, 4}, {1, 4} & {2, 3}.
2.6
BURNSIDE/P ´OLYA COUNTING FORMULA
Burnside’s lemma and P´olya’s formula are used to count the number of “really diﬀerent”
conﬁgurations, such as tic-tac-toe patterns and placement of beads on a bracelet, in which
various symmetries play a role. One of the scientiﬁc applications of P´olya’s formula is
the enumeration of isomers of a chemical compound. From a mathematical perspective,
Burnside/P´olya methods count orbits under a permutation group action. (See §5.3.1.)
2.6.1
PERMUTATION GROUPS AND CYCLE INDEX POLYNOMIALS
Deﬁnitions:
A permutation on a set S is a one-to-one mapping of S onto itself. In this context,
the elements of S are called objects.
A permutation π of a ﬁnite set S is cyclic if there is a subcollection of objects that can
be arranged in a cycle (a1a2 . . . an) so that each object aj is mapped by π onto the next
object in the cycle and every object of S not in this cycle is ﬁxed by π, that is, mapped
to itself.
The tabular form of a permutation π on a ﬁnite set S is a matrix with two rows. In
the ﬁrst row, each object from S is listed once. Below the object a is its image π(a), in
this form:
 
a1
a2
· · ·
an
π(a1)
π(a2)
· · ·
π(an)
!
The cycle decomposition (form) of a permutation π is a concatenation of cyclic per-
mutations whose object subcollections are disjoint and whose product is π. (Sometimes
the 1-cycles are explicitly written and sometimes they are omitted.)
A set P of permutations of a set S is closed under composition if the composition of
each pair of permutations in P is also in P.
A set P of permutations of a set S is closed under inversion if for every permutation
π ∈P, π−1 ∈P.
A permutation group G = (P, S) is a nonempty set P of permutations on a set S such
that P is closed under composition and inversion.

Section 2.6
BURNSIDE/P ´OLYA COUNTING FORMULA
125
The cycle structure of a permutation π is an expression (multivariate polynomial)
of the form xm1
1 xm2
2
. . . xmk
k , where mj is the number of cycles of size j in the cyclic
decomposition of π.
The cycle index of a permutation group G is the multivariate polynomial that is the
sum of the cycle structures of all the permutations in G, divided by the number of
permutations in G.
The cycle index polynomial is written PG(x1, x2, . . . , xn).
(The
notation PG honors George P´olya (1887–1985) who greatly advanced the application of
the cycle index polynomial to counting.)
Facts:
1. Every permutation has a tabular form.
2. The tabular form of a permutation is unique up to the order in which the objects of
the permuted set are listed in the ﬁrst row.
3. Every permutation has a cycle decomposition.
4. The cycle decomposition of a permutation into a product of disjoint cyclic permuta-
tions is unique up to the order of the factors.
5. The collection of all permutations on a set S forms a permutation group.
Examples:
1. The permutation
 a b c d
c d a b

has the cycle decomposition (ac)(bd).
2. The symmetric group Σ3 of all 6 possible permutations on {a, b, c} has the following
elements:
(a)(b)(c), (ab)(c), (ac)(b), (a)(bc), (abc), (acb)
with respective cycle structures
x3
1, x1x2, x1x2, x1x2, x3, x3.
Thus, the cycle index polynomial is
PΣ3 = 1
6
 x3
1 + 3x1x2 + 2x3

.
3. The group Σ4 of all 24 permutations on {a, b, c, d} has the following elements:
(a)(b)(c)(d)
(ab)(c)(d)
(ac)(b)(d)
(ad)(b)(c)
(a)(bc)(d)
(a)(bd)(c)
(a)(b)(cd)
(abc)(d)
(acb)(d)
(abd)(c)
(adb)(c)
(acd)(b)
(adc)(b)
(a)(bcd)
(a)(bdc)
(ab)(cd)
(ac)(bd)
(ad)(bc)
(abcd)
(abdc)
(acbd)
(acdb)
(adbc)
(adcb)
The cycle index polynomial is
PΣ4 =
1
24

x4
1 + 6x2
1x2 + 8x1x3 + 3x2
2 + 6x4

.
2.6.2
ORBITS AND SYMMETRIES
Deﬁnitions:
Given a permutation group G = (P, S), the orbit of a ∈S is the set { π(a) | π ∈P }.
A symmetry of a ﬁgure (or symmetry motion) is a spatial motion of the ﬁgure onto
itself.

126
Chapter 2
COUNTING METHODS
Facts:
1. Given a permutation group G = (P, S), the relation R deﬁned by
aRb ⇐⇒there exists π ∈P such that π(a) = b
is an equivalence relation (§1.4.2), and the equivalence classes under it are precisely the
orbits.
2. The set of all symmetries on a ﬁgure forms a group.
3. The set of symmetries on a polygon induces a permutation group action on its corner
set and a permutation group action on its edge set.
Examples:
1. Acting on the set {a, b, c, d, e} is the following permutation group:
(a)(b)(c)(d)(e), (ab)(c)(d)(e), (a)(b)(cd)(e), and (ab)(cd)(e).
The orbits of this group are {a, b}, {c, d}, {e}. The cycle index is
1
4

x5
1 + 2x3
1x2 + x1x2
2

.
2. A square with corners a, b, c, d (in clockwise order) has eight possible symmetries:
four rotations in the plane around the center of the square and four reﬂections (which
could also be achieved by 180◦spatial rotations out of the plane). See the following
ﬁgure.
a
b
rotations
0°
(a)(b)(c)(d)
horizontal axis
(a  d)(b  c)
(a  b)(c  d)
(a)(c)(b  d)
(b)(d)(a  c)
vertical axis
down-diagonal axis
up-diagonal axis
(a  b  c  d)
(a  c)(b  d)
(a  d  c  b)
90°
180°
270°
reﬂections
c
d
There is only one orbit, {a, b, c, d}, and the cycle index for the group of symmetries of a
square acting on its corner set (the dihedral group D4) is
PD4 = 1
8

x4
1 + 2x4 + 3x2
2 + 2x2
1x2

.
3. A pentagon has ten diﬀerent symmetries. Five are rotations in the plane around
the center of the pentagon: 0◦= (a)(b)(c)(d)(e), 72◦= (abcde), 144◦= (acebd), 216◦=
(adbec), and 288◦= (aedcb); ﬁve are reﬂections (or equivalently, spatial rotations of
180◦out of the plane) around axis lines through a corner and the middle of an op-
posite side: (a)(be)(cd), (b)(ac)(de), (c)(ae)(bd), (d)(ab(ce), and (e)(ad)(bc). See the fol-
lowing ﬁgure. There is only one orbit, {a, b, c, d, e}, and the associated cycle index is
1
10

x5
1 + 4x5 + 5x1x2
2

.
e
a
b
c
d

Section 2.6
BURNSIDE/P ´OLYA COUNTING FORMULA
127
2.6.3
COLOR PATTERNS AND INDUCED PERMUTATIONS
Deﬁnitions:
A coloring of a set S from a set of n colors is a function from S to the set {1, . . ., n},
whose elements are regarded as “colors”. The set of all such colorings is denoted C(S, n).
A corner coloring of a (polygonal or polyhedral) geometric ﬁgure is a coloring of its
set of corners.
An edge coloring of a geometric ﬁgure is a coloring of its set of edges.
Let c1 and c2 be colorings of the set S and let π be a permutation of S. Write π(c1) = c2
if c1(a) = c2(π(a)) for every a ∈S. The correspondence c1 7→c1 ◦π−1 is the map
induced by π on the colorings of S. (The composition c1 ◦π−1 assigns a color to
every object a ∈S, namely the color c1(π−1(a)).
Two corner colorings of a ﬁgure are equivalent if one can be mapped to the other by a
symmetry. Similar deﬁnitions apply to edge colorings and to face colorings.
Two colorings c1 and c2 of a set S are equivalent under a group G = (P, S) if there
is a permutation π ∈P such that π(c1) = c2.
A corner coloring pattern of a ﬁgure with respect to a set of symmetries is a
set of mutually equivalent colorings of the ﬁgure.
Facts:
1. Let G = (P, S) be a permutation group.
Then the induced action of P on the
set C(S, n) of colorings with n colors is a permutation group action.
2. When P acts on the set C(S, n) of colorings of S, the numbers of permuted objects
and orbits, and the cycle index polynomial, are diﬀerent from when P acts on S itself.
3. In permuting the set S of corners of a ﬁgure, a symmetry of a ﬁgure simultaneously
induces a permutation of the set of all its corner colorings. An analogous fact holds for
edge colorings.
Examples:
1. In Example 2 of §2.6.2, a permutation group of eight elements acts on the four corners
of a square. There is only one orbit, and the cycle index is 1
8

x4
1 + 2x4 + 3x2
2 + 2x2
1x2

.
The following ﬁgure shows what happens when the same group acts on the set of black-
white colorings. The permuted set has 16 colorings, there are six orbits, and the cycle
index polynomial is 1
8

x16
1 + 2x2
1x2x3
4 + 3x4
1x6
2 + 2x8
1x4
2

.

128
Chapter 2
COUNTING METHODS
2.6.4
FIXED POINTS AND BURNSIDE’S LEMMA
Deﬁnition:
An element a ∈S is a ﬁxed point of the permutation π if π(a) = a. The set of all ﬁxed
points of π is denoted ﬁx(π).
Facts:
1. The number of ﬁxed points of a permutation π equals the number of 1-cycles in its
cycle decomposition.
2. Burnside’s lemma:
Let G be a group of permutations acting on a set S. Then the
number of orbits induced on S is given by
1
|G|
X
π∈G
|ﬁx(π)|.
Note: The theorem commonly called “Burnside’s lemma” originated with Georg Frobe-
nius (1848–1917). A widely available book by William Burnside (1852–1927) published
in 1911 stated and proved the same result, without mentioning its prior discovery.
3. Evaluation of the sum in Burnside’s lemma is simpliﬁed by using the cycle index
polynomial and Fact 1. For each term in the polynomial, multiply the coeﬃcient by the
exponent of x1, and then sum these products.
4. Special Burnside’s lemma (for colorings):
Let G be a group of permutations acting
on a set S. Then the number of orbits induced on C(S, n) (the set of colorings of S
from a set of n colors) is given by substituting n for each variable in the cycle index
polynomial.
5. The following table gives information on the number of corner coloring patterns of
selected ﬁgures.
ﬁgure
colors
2
3
4
m
triangle
4
11
20
1
6[m3 + 3m2 + 2m]
square
6
21
55
1
8[m4 + 2m3 + 3m2 + 2m]
pentagon
8
39
136
1
10[m5 + 5m3 + 4m]
hexagon
13
92
430
1
12[m6 + 3m4 + 4m3 + 2m2 + 2m]
heptagon
18
198
1,300
1
14[m7 + 7m4 + 6m]
octagon
30
498
4,183
1
16[m8 + 4m5 + 5m4 + 2m2 + 4m]
nonagon
46
1,219
15,084
1
18[m9 + 9m5 + 2m3 + 6m]
decagon
78
3,210
53,764
1
20[m10 + 5m6 + 6m5 + 4m2 + 4m]
tetrahedron
5
15
36
1
12[m4 + 11m2]
cube
23
333
2914
1
24[m8 + 17m4 + 6m2]
Examples:
1. In Example 1 of §2.6.2, the permutation group is
{(a)(b)(c)(d)(e), (ab)(c)(d)(e), (a)(b)(cd)(e), (ab)(cd)(e)}.
The cycle index is 1
4

x5
1 + 2x3
1x2 + x1x2
2

. By Burnside’s lemma and Fact 3 there are
1
4 [1 · 5 + 2 · 3 + 1 · 1] = 12
4 = 3 orbits. The orbits are {a, b}, {c, d}, {e}.

Section 2.6
BURNSIDE/P ´OLYA COUNTING FORMULA
129
2. Example 1 of §2.6.3 shows 16 colorings of the corners of the square with colors
black or white. There are six orbits, and the cycle index for the action on the color-
ings is 1
8

x16
1 + 2x2
1x2x3
4 + 3x4
1x6
2 + 2x8
1x4
2

. By Burnside’s lemma and Fact 3, there are
1
8 [1 · 16 + 2 · 2 + 3 · 4 + 2 · 8] = 48
8 = 6 orbits.
It is simpler to apply the Special Burnside’s lemma to the cycle index for the action
on the square (from Example 2 of §2.6.2),
1
8

x4
1 + 2x4 + 3x2
2 + 2x2
1x2

, which yields
1
8

1 · 24 + 2 · 2 + 3 · 22 + 2 · 22 · 2

= 6 orbits.
3. (Continuing Example 3 of §2.6.2):
The cycle index of the group of symmetries of
the pentagon is
1
10

x5
1 + 4x5 + 5x1x2
2

. By the Special Burnside’s lemma, the number of
m-colorings of the corners of an unoriented pentagon is
1
10

m5 + 4m + 5m3
. For m = 3,
the formula gives
1
10(243 + 12 + 135) = 39 3-coloring patterns of a pentagon.
4. A cube has 24 rotational symmetries, which act on the corners. The identity symme-
try has cycle structure x8
1. There are three additional classes of symmetries, as follows:
(a) Rotations of 90◦, 180◦, or 270◦about an axis line through the middles of opposite
faces, for example, through abcd and efgh in part (a) of the following ﬁgure. A 90◦
rotation, such as (abcd)(efgh), has cycle structure x2
4.
All 270◦rotations have that
same structure. A 180◦rotation, such as (ac)(bd)(eg)(fh), has cycle structure x4
2. There
are three pairs of opposite faces, and so the total contribution to the cycle index of
opposite-face rotations is 6x2
4 + 3x4
2.
a
d
c
g
b
h
e
f
a
d
c
g
b
h
e
f
a
d
c
g
b
h
e
f
(a) rotation about
         opposite faces
(b) rotation about
         opposite edges
(c) rotation about
         opposite corners
(b) Rotating 180◦about an axis line through the middles of opposite edges, for example,
through edges ad and fg in part (b) of the ﬁgure. This rotation, (ad)(bh)(ce)(fg), has
cycle structure x4
2. There are six pairs of opposite edges, and so the total contribution
of opposite-edge rotations is 6x4
2.
(c) Rotating 120◦or 240◦about an axis line through opposite corners, for example, about
the line through corners a and g in part (c) of the ﬁgure. Any 120◦rotation, such as
(a)(bde)(chf)(g), has cycle structure x2
1x2
3. A 240◦rotation has the same structure. There
are four pairs of opposite corners, and so the contribution of opposite-corner rotations is
8x2
1x2
3.
Collect terms to obtain the cycle index
1
24

x8
1 + 6x2
4 + 9x4
2 + 8x2
1x2
3

. Thus, the number
of m-colorings of the corners of an unoriented cube is
1
24

m8 + 6m2 + 9m4 + 8m4
. For
m = 2 and 3, the formula gives 23 2-coloring patterns and 333 3-coloring patterns.
2.6.5
P ´OLYA’S ENUMERATION FORMULA
Deﬁnition:
A pattern inventory is a generating function (§3.2) that enumerates the numbers of
coloring patterns of a given ﬁgure.

130
Chapter 2
COUNTING METHODS
Facts:
1. P´olya’s enumeration formula:
Let G = (P, S) be a permutation group and let
{c1, . . . , cn} be a set of names for n colors for the objects of S.
Then the pattern
inventory with respect to G for the set of all n-colorings of S is given by substituting
(cj
1 + · · · + cj
n) for xj in the cycle index PG(x1, . . . , xm).
Note: This theorem was published in 1937. Essentially the same result was derived by
H. Redﬁeld in 1927.
2. P´olya’s enumeration formula has many applications in enumerating various families
of graphs. This approach was pioneered by F. Harary [HaPa73].
3. P´olya’s enumeration formula has many applications in which some practical question
is modeled as a graph coloring problem.
Examples:
1. The pattern inventory of black-white colorings of the corners of a triangle is
1b3 + 1b2w + 1bw2 + 1w3.
This means there is one coloring pattern with all three corners black, one with two black
corners and one white corner, etc.
2. (Continuing Example 2 of §2.6.4): For corner colorings of the square, the cycle index
is
PD4(x1, x2, x3, x4) = 1
8

x4
1 + 2x2
1x2 + 3x2
2 + 2x4

.
By P´olya’s enumeration formula, the pattern inventory for black-white colorings of the
corners of the square is
PD4[(b + w), (b2 + w2), (b3 + w3), (b4 + w4)]
= 1
8

(b + w)4 + 2(b + w)2(b2 + w2) + 3(b2 + w2)2 + 2(b4 + w4)

= 1
8

8b4 + 8b3w + 16b2w2 + 8bw3 + 8w4
= 1b4 + 1b3w + 2b2w2 + 1bw3 + 1w4.
This pattern inventory may be conﬁrmed by examining the drawing in Example 1
of §2.6.3.
3. (Continuing Example 3 of §2.6.4):
For corner colorings of the pentagon, the cycle
index is
PD5(x1, x2, . . . , x5) =
1
10

x5
1 + 4x5 + 5x1x2
2

.
By P´olya’s enumeration formula, the pattern inventory for black-white colorings of the
corners of the pentagon (conﬁrmable by drawing pictures) is
PD5((b + w), (b2 + w2), (b3 + w3), (b4 + w4), (b5 + w5))
=
1
10

(b + w)5 + 4(b5 + w5) + 5(b + w)(b2 + w2)2
=
1
10

10b5 + 10b4w + 20b3w2 + 20b2w3 + 10bw4 + 10w5
= 1b5 + 1b4w + 2b3w2 + 2b2w3 + 1bw4 + 1w5.
4. (Continuing Example 4 of §2.6.4): For corner colorings of the cube, the cycle index
is
PG(x1, . . . , x4) =
1
24

x8
1 + 6x2
4 + 9x4
2 + 8x2
1x2
3

.

Section 2.7
M ¨OBIUS INVERSION COUNTING
131
By P´olya’s enumeration formula, the pattern inventory for black-white colorings of the
corners of the cube is
PG
 (b + w), (b2 + w2), (b3 + w3), (b4 + w4)

=
1
24

(b + w)8 + 6(b4 + w4)2 + 9(b2 + w2)4 + 8(b + w)2(b3 + w3)2
= b8 + b7w + 3b6w2 + 3b5w3 + 7b4w4 + 3b3w5 + 3b2w6 + bw7 + w8.
5. Organic chemistry:
Two structurally diﬀerent compounds with the same chemical
formula are called isomers. For instance, to two of the six carbons (C) in a ring there
might be attached a hydrogen (H), and to each of the four other carbons some other rad-
ical (R), thereby yielding the chemical formula C6H2R4. The number of diﬀerent isomers
(structurally diﬀerent arrangements of the radicals) is the same as the number of coloring
patterns of a hexagon when two of the corners are “colored” H and four “colored” R.
The cycle index for the symmetries of a hexagon, in terms of corner permutations, is
PD6(x1, . . . , x6) =
1
12

x6
1 + 2x6 + 2x2
3 + 4x3
2 + 3x2
1x2
2

.
Substituting (Hj + Rj) for xj yields a pattern inventory listing the number of isomers of
C6HiR6−i :
1
12

(H + R)6 + 2(H6 + R6) + 2(H3 + R3)2 + 4(H2 + R2)3 + 3(H + R)2(H2 + R2)2
=
1
12

12H8 + 12H5R + 36H4R2 + 36H3R3 + 36H2R4 + 12HR5 + 12R6
= 1H8 + 1H5R + 3H4R2 + 3H3R3 + 3H2R4 + 1HR5 + 1R6.
The three possible coloring patterns corresponding to 3H2R4 are shown in the following
ﬁgure:
H
H
H
R
R
R
R
R
R
R
R
R
R
R
R
H
H
H
2.7
M ¨OBIUS INVERSION COUNTING
M¨obius inversion is an important tool used to solve a variety of counting problems such
as counting how many numbers are relatively prime to some given number (without
individually checking each smaller number) and counting certain types of circular ar-
rangements. It generalizes the principle of inclusion/exclusion. (Augustus Ferdinand
M¨obius, 1790–1868)

132
Chapter 2
COUNTING METHODS
2.7.1
M ¨OBIUS INVERSION
Deﬁnitions:
The Kronecker delta function δ(x, y) is deﬁned by the rule
δ(x, y) =
1
if x = y
0
otherwise.
The M¨obius function is the function µ from the set of positive integers to the set of
integers where
µ(m) =





1
if m = 1
(−1)k
if m = p1p2 . . . pk (the product of k distinct primes)
0
if m is divisible by the square of a prime.
Note: See Chapter 11 for M¨obius functions deﬁned on partially ordered sets.
For a positive integer n, the Euler phi-function φ(n) is the number of positive integers
less than n that are relatively prime to n.
Facts:
1. For the M¨obius function µ deﬁned on the set of positive integers:
• µ is multiplicative: if gcd(m, n) = 1, then µ(mn) = µ(m)µ(n);
• µ is not completely multiplicative: µ(mn) = µ(m)µ(n) is not always true;
• P
d|n
µ(d), where the sum is taken over all positive divisors of n, is 1 if n = 1 and 0
if n > 1.
2. M¨obius inversion formula: If f(n) and g(n) are deﬁned for all positive integers and
f(n) = P
d|n
g(d), then
g(n) =
X
d|n
µ(d)f(n/d).
3. For every positive integer n, n = P
d|n
φ(d).
Examples:
1. Applying the M¨obius inversion formula to the expression given in Fact 3 produces
φ(n) =
X
d|n
µ(d)n
d .
As an illustration, φ(12) = P
d|12
µ(d) 12
d = [µ(1) · 12 + µ(2) · 6 + µ(3) · 4 + µ(4) · 3 + µ(6) ·
2 + µ(12) · 1] = 12 −6 −4 + 0 + 2 + 0 = 4.
2. Circular permutations with repetitions:
Given an alphabet of m letters, how many
circular permutations of length n are possible, if repeated letters are allowed and two
permutations are the same if the second can be obtained from the ﬁrst by rotation? The
problem was ﬁrst solved by Percy A. MacMahon in 1892.

Section 2.8
YOUNG TABLEAUX
133
A circular permutation of length n has a period d, where d|n. (The period of a circular
permutation, viewed as a circular string, is the length of the shortest substring that
repeats end-to-end to give the entire string.) Let g(d) be the number of length d circular
permutations that have period d. A circular permutation of length n can be constructed
from one of length d (where d|n) by concatenating it with itself n
d times. For example,
the circular permutation aabbaabb (where beginning and end are joined) of period four
can be obtained by taking the circular permutation aabb and opening it up at one of
four spots between the letters, to obtain any of four linear strings aabb, abba, bbaa, and
baab. Join one of these to itself, obtaining aabbaabb, abbaabba, bbaabbaa, and baabbaab,
and then join the beginning and the end to form the circular permutation aabbaabb.
For any positive integer k, there are dg(d) linear strings of length k obtained by taking
k
d repetitions of the linear strings of length d that have period d, where d|k. Therefore,
the total number of linear strings of length k where the objects are chosen from m
types is P
d|k dg(d) = mk. Applying the M¨obius inversion formula to mk and g yields
g(k) = 1
k
P
d|k µ(d)mk/d. So the total number of circular permutations of length n where
the elements are chosen from an alphabet of size m is P
d|n g(d), which is equal to
P
k|n
  1
k
P
d|k µ(d)mk/d
.
3. Circular permutations with repetitions and speciﬁed numbers of each type of object:
Suppose there are a total of n objects of t types, with ai of type i (i = 1, . . . , t), where
a1 + · · · + at = n. If a = gcd(a1, . . . , at), these circular permutations can be generated
as in Example 2 by taking a circular permutation of period d (where d|a) with
aid
n
objects of type i (i = 1, . . . , t), breaking it open, and laying it end-to-end
n
d times.
Let g(k) be the number of such circular permutations of length k that have period k.
Then the total number of linear strings of length n with ai objects of type i is given by
P
d|a dg(d) =
n!
a1!...at!. By the M¨obius inversion formula,
g(k) = 1
k
P
d|a µ(d)
(k/d)!
(a1/d)!(a2/d)!...(at/d)!.
Summing g(k) over all divisors of a gives the desired total number of circular permuta-
tions:
P
k|a g(k) = P
k|a

1
k
P
d|a µ(d)
(k/d)!
(a1/d)!...(at/d)!

.
2.8
YOUNG TABLEAUX
Arrays called Young tableaux were introduced by the Reverend Alfred Young (1873–
1940). These arrays are used in combinatorics and the theories of symmetric functions,
which are the subject of this section. Young tableaux are also used in the analysis of
representations of the symmetric group. They make it possible to approach many results
about representation theory from a concrete combinatorial viewpoint.

134
Chapter 2
COUNTING METHODS
2.8.1
TABLEAUX COUNTING FORMULAS
Deﬁnitions:
The hook Hi,j of cell (i, j) in the Ferrers diagram for a partition λ is the set
{ (k, j) ∈λ | k ≥i } ∪{ (i, k) ∈λ | k ≥j };
that is, the set consisting of the cell (i, j), all cells in its row to its right, and all cells in
its column below it.
The hooklength hi,j of cell (i, j) is the number |Hi,j| of cells in its hook.
A Young tableau is an array obtained by replacing each cell of the Ferrers diagram by
a positive integer.
The shape of a Young tableau is the partition corresponding to the underlying Ferrers
diagram. The notation λ ⊢n indicates that λ partitions the number n.
A Young tableau is semistandard (an SSYT) if the entries in each row are weakly
increasing and the entries in each column are strictly increasing.
A semistandard Young tableau of shape λ ⊢n is standard (an SYT) if each number
1, . . . , n occurs exactly once as an entry. The number of SYT of shape λ is denoted fλ.
If G is a group (see §5.2) then an involution is an element g ∈G such that g2 is the
identity. The number of involutions in the symmetric group Sn (or Σn) (the group of all
permutations on the set {1, 2, . . ., n}) is denoted inv(n).
The following table summarizes notation for Young tableaux:
notation
meaning
λ = (λ1, . . . , λl)
partition (with parts λ1 ≥λ2 ≥· · · ≥λl)
λ ⊢n
λ partitions the number n
(i, j)
cell in a Ferrers diagram
Hi,j
hook of cell (i, j)
hi,j
hooklength of hook Hi,j
fλ
number of SYT of shape λ
inv(n)
number of involutions in Sn
Facts:
1. Frame-Robinson-Thrall hook formula (1954): The number of SYT of ﬁxed shape λ
is
fλ =
n!
Q
(i,j)∈λ hi,j
.
2. Frobenius determinantal formula (1900):
The number of SYT of ﬁxed shape λ =
(λ1, . . . , λl) is the determinant
fλ = n!

1
(λi + j −i)!

1≤i,j≤l
.
3. Summations involving the number of SYT:
P
λ⊢n
fλ = inv(n),
P
λ⊢n
f 2
λ = n!.

Section 2.8
YOUNG TABLEAUX
135
4. Young tableaux can be used to ﬁnd the number of permutations with speciﬁed lengths
for their longest increasing subsequences and longest decreasing subsequences [Be71].
Examples:
1. If λ = (3, 2) then a complete list of SYT is
1
2
3
4
5
1
2
4
3
5
1
2
5
3
4
1
3
4
2
5
1
3
5
2
4
2. If λ = (2, 2) then a complete list of SSYT with entries at most 3 is
1
1
2
2
1
1
3
3
2
2
3
3
1
1
2
3
1
2
2
3
1
2
3
3
3. For the partition (3, 2), H1,1 = {(1, 1), (2, 1), (1, 2), (1, 3)}. In the following diagram
each cell of (3, 2) is replaced with its hooklength.
4
3
1
2
1
The hook formula (Fact 1) gives the number of SYT of shape (3, 2): f(3,2) =
5!
4·3·2·12 = 5.
The determinantal formula (Fact 2) gives the same result:
f(3,2) = 5!

1
3!
1
4!
1
1!
1
2!
 = 5.
4. For the partitions of n = 3, f(3) = 1, f(2,1) = 2, f(1,1,1) = 1, so the summation
formulas become
P
λ⊢3
fλ = 4 = inv(3);
P
λ⊢3
f 2
λ = 6 = 3!.
2.8.2
TABLEAUX ALGORITHMS
Deﬁnitions:
An inner corner of a partition λ is a cell (i, j) ∈λ such that (i + 1, j), (i, j + 1) /∈λ.
An outer corner of a partition λ is a cell (i, j) /∈λ such that (i −1, j), (i, j −1) ∈λ.
Facts:
1. The Greene-Nijenhuis-Wilf algorithm (1979) (Algorithm 1) successively ﬁnds inner
corners of a tableau of shape λ, eventually producing a random tableau of the speciﬁed
shape.
Algorithm 1:
Greene-Nijenhuis-Wilf.
input: a shape λI such that λI ⊢n
output: a standard Young tableau of shape λI, uniformly at random
λ := λI

136
Chapter 2
COUNTING METHODS
while λ is nonempty
{Find an inner corner (i, j) ∈λ}
choose (with probability
1
|λ|) any cell (i, j) ∈λ
while the current cell (i, j) is not an inner corner
choose (with probability
1
hi,j ) a pair (i′, j′) ∈Hi,j −{(i, j)}
(i, j) := (i′, j′)
assign label n to inner corner (i, j)
λ := λ −{(i, j)}
2. The Robinson-Schensted algorithm (1938, 1961) (Algorithm 2) constructs a pair
(P, Q) of SYT of the same shape, following a speciﬁed permutation π that guides the
insertion of entries into the two tableaux. At each step i, the entry πi is inserted into
the tableau P whereas i is inserted (at the same position) into Q.
Algorithm 2:
Robinson-Schensted.
input: a permutation π ∈Sn where π =
 
1
2
· · ·
n
π1
π2
· · ·
πn
!
output: a pair (P, Q) of standard Young tableaux of the same shape λ ⊢n
P0 := ∅; Q0 := ∅
for k := 1 to n
r := 1; c := 1; b := πk; Pk := Pk−1; exit := FALSE
while exit = FALSE
{Find next insertion row r in tableau Pk}
while row r(Pk) ̸= ∅and πj > max{row r(Pk)}
r := r + 1
{Find next insertion column c in tableau Pk}
c := 1
while Pk[r, c] ̸= ∅and πk < Pk[r, c]
c := c + 1
{Insert b}
if Pk[r, c] = ∅then
Pk[r, c] := b; exit = TRUE
else
bb := Pk[r, c]; Pk[r, c] := b; b := bb
Qk[r, c] := k
P := Pn; Q := Qn
Examples:
1. The diagrams in the following ﬁgure illustrate a plausible sequence of current cells
chosen as the ﬁrst step of Algorithm 1, in order to ﬁnd an inner corner of a tableau of
shape λ = (5, 5, 5, 2).

REFERENCES
137
c
c
c
λ =  (5,5,5,2)
Pr(c) =
1
17
n = 17
Pr(c) =
1
5
Pr(c) =
1
1
2. The permutation π =
 
1
2
3
4
5
6
7
6
2
3
1
7
5
4
!
yields the following sequence of
tableaux pairs (Pk, Qk) during the execution of Algorithm 2.
6
2
6
2 3
6
1 3
2
6
1 3 7
2
6
1 3
7
5
2
6
1 3
5
7
4
2
6
Ø
P0
P1
P2
P3
P4
P5
P6
P7
1
1
2
1 3
2
1 3
2
4
1 3 5
2
4
1 3
6
5
2
4
1 3
6
7
5
2
4
Ø
Q0
Q1 Q2
Q3
Q4
Q5
Q6
Q7
REFERENCES
Printed Resources:
[An98] G. E. Andrews, The Theory of Partitions, Encyclopedia of Mathematics and its
Applications, Vol. 2, Cambridge University Press, 1998.
[BeGo75] E. A. Bender and J. R. Goldman, “On the applications of M¨obius inversion in
combinatorial analysis”, American Mathematical Monthly 82 (1975), 789–803. (An
expository paper for a general mathematical audience.)
[Be71] C. Berge, Principles of Combinatorics, Academic Press, 1971.
[Co79] D. I. A. Cohen, Basic Techniques of Combinatorial Theory, Wiley, 1979.
[GaRa04] G. Gasper and M. Rahman, Basic Hypergeometric Series, 2nd ed., Cambridge
University Press, 2004.
[GrKnPa94] R. L. Graham, D. E. Knuth, and O. Patashnik, Concrete Mathematics, 2nd
ed., Addison-Wesley, 1994.
[GuGwMi58] H. Gupta, A. E. Gwyther, and J. C. P. Miller, Tables of Partitions, Royal
Society Mathematical Tables, Vol. 4, 1958.
[Ha98] M. Hall, Jr., Combinatorial Theory, 2nd ed., Wiley, 1998. (Section 2.1 of this text
contains a brief introduction to M¨obius inversion.)
[HaPa73] F. Harary and E. M. Palmer, Graphical Enumeration, Academic Press, 1973.
[HaRa18] G. H. Hardy and S. Ramanujan, “Asymptotic formulae in combinatory anal-
ysis”, Proceedings of the London Mathematical Society, Ser. 2, 17 (1918), 75–115.
(Reprinted in Collected Papers of S. Ramanujan, Chelsea, 2000, 276–309.)

138
Chapter 2
COUNTING METHODS
[HaWr08] G. H. Hardy and E. M. Wright, An Introduction to the Theory of Numbers,
6th ed., Oxford University Press, 2008.
[JaKe09] G. D. James and A. Kerber, The Representation Theory of the Symmetric
Group, Encyclopedia of Mathematics and Its Applications, Vol. 16, Cambridge Uni-
versity Press, 2009.
[Kn93] M. I. Knopp, Modular Functions in Analytic Number Theory, 2nd ed., Chelsea,
1993.
[Kn97] D. E. Knuth, The Art of Computer Programming, Vol. 1: Fundamental Algo-
rithms, 3rd ed., Addison-Wesley, 1997.
[Ma99] I. G. Macdonald, Symmetric Functions and Hall Polynomials, 2nd ed., Oxford
University Press, 1999.
[Ma04] P. A. MacMahon, Combinatory Analysis, Vols. I–II, Cambridge University Press,
London, 1916. (Reissued: Dover Publications, 2004.)
[Pa06] E. W. Packel, The Mathematics of Games and Gambling, 2nd ed., Mathematical
Association of America, 2006.
[PoRe11] G. P´olya and R. C. Read, Combinatorial Enumeration of Groups, Graphs, and
Chemical Compounds, Springer-Verlag, 2011.
[Ro12] K. H. Rosen, Discrete Mathematics and Its Applications, 7th ed., McGraw-Hill,
2012.
[Sa01] B. E. Sagan, The Symmetric Group: Representations, Combinatorial Algorithms,
and Symmetric Functions, 2nd ed., Springer, 2001.
[St11] R. P. Stanley, Enumerative Combinatorics, Vol. I, 2nd ed., Cambridge University
Press, 2011. (Chapter 3 of this text contains an introduction to M¨obius functions
and incidence algebras.)
[Tu12] A. Tucker, Applied Combinatorics, 6th ed., Wiley, 2012.
[WhWa96] E. T. Whittaker and G. N. Watson, A Course of Modern Analysis, 4th ed.,
Cambridge University Press, 1996.
[Wi06] H. S. Wilf, generatingfunctionology, 3rd ed., A K Peters, 2006.
Web Resources:
http://theory.cs.uvic.ca (Combinatorial object server.)
http://theory.cs.uvic.ca/amof/ (AMOF: the Amazing Mathematical Object Fac-
tory.)
http://www.cs.sunysb.edu/~algorith/ (The Stony Brook Algorithm Repository; see
Section 1.3 on combinatorial problems.)

3
SEQUENCES
3.1 Special Sequences
Thomas A. Dowling and
3.1.1 Representations of Sequences
Douglas R. Shier
3.1.2 Fibonacci Numbers
3.1.3 Catalan Numbers
3.1.4 Bernoulli Numbers and Polynomials
3.1.5 Eulerian Numbers
3.1.6 Ramsey Numbers
3.1.7 Other Sequences
3.1.8 Miniguide to Sequences
3.2 Generating Functions
Ralph P. Grimaldi
3.2.1 Ordinary Generating Functions
3.2.2 Exponential Generating Functions
3.3 Recurrence Relations
Ralph P. Grimaldi
3.3.1 Basic Concepts
3.3.2 Homogeneous Recurrence Relations
3.3.3 Nonhomogeneous Recurrence Relations
3.3.4 Method of Generating Functions
3.3.5 Divide-and-Conquer Relations
3.4 Finite Differences
Jay Yellen
3.4.1 The Difference Operator
3.4.2 Calculus of Differences: Falling and Rising Powers
3.4.3 Difference Sequences and Difference Tables
3.4.4 Difference Equations
3.5 Finite Sums and Summation
Victor S. Miller
3.5.1 Sigma Notation
3.5.2 Elementary Transformation Rules for Sums
3.5.3 Antidifferences and Summation Formulas
3.5.4 Standard Sums
3.6 Asymptotics of Sequences
Edward A. Bender and
3.6.1 Approximate Solutions to Recurrences
Juanjo Ru´e
3.6.2 Analytic Methods for Deriving Asymptotic Estimates
3.6.3 Generating Function Schemes
3.6.4 Asymptotic Estimates of Multiply-Indexed Sequences
3.7 Mechanical Summation Procedures
Kenneth H. Rosen
3.7.1 Hypergeometric Series
3.7.2 Algorithms to Produce Closed Forms for Sums of Hypergeometric Terms
3.7.3 Certifying the Truth of Combinatorial Identities

140
Chapter 3
SEQUENCES
INTRODUCTION
Sequences of integers occur regularly in combinatorial applications. For example, the
solution to a counting problem that depends on a parameter k can be viewed as the kth
term of a sequence. This chapter provides a guide to particular sequences that arise in
applied settings. Such (inﬁnite) sequences can frequently be represented in a ﬁnite form.
Speciﬁcally, sequences can be expressed using generating functions, recurrence relations,
or by an explicit formula for the kth term of the sequence.
GLOSSARY
antidiﬀerence (of a function f): any function g such that ∆g = f. It is the discrete
analogue of antidiﬀerentiation.
ascent (in a permutation π): any index i such that πi < πi+1.
asymptotic equality (of functions): the function f(n) is asymptotic to g(n), written
f(n) ∼g(n), if f(n) ̸= 0 for suﬃciently large n and limn→∞
g(n)
f(n) = 1.
Bernoulli numbers: the numbers Bn produced by the recursive deﬁnition B0 = 1,
Pn
j=0
 n+1
j

Bj = 0, n ≥1.
Bernoulli polynomials: the polynomial Bm(x) = Pm
k=0
 m
k

Bkxm−k where Bk is the
kth Bernoulli number.
big oh (of the function f): the set of all functions that do not grow faster than some
constant multiple of f, written O(f(n)).
big omega (of the function f): the set of all functions that grow at least as fast as
some constant multiple of f, written Ω(f(n)).
big theta (of the function f): the set of all functions that grow roughly as fast as some
constant multiple of f, written Θ(f(n)).
binomial convolution (of the sequences {an} and {bn}): the sequence whose rth term
is formed by summing products of the form
 r
k

akbr−k.
Catalan number: the number Cn =
1
n+1
 2n
n

.
characteristic equation: an equation derived from a linear recurrence relation with
constant coeﬃcients, whose roots are used to construct solutions to the recurrence
relation.
closed form (for a sum): an algebraic expression for the value of a sum with variable
limits, which has a ﬁxed number of terms; hence the time needed to calculate it does
not grow with the size of the set or interval of summation.
convolution (of the sequences {an} and {bn}): the sequence whose rth term is formed
by summing products of the form akbr−k where 0 ≤k ≤r.
de Bruijn sequence: a circular ordering of letters from a ﬁxed alphabet with p letters
such that each n consecutive letters (wrapping around from the end of the sequence
to the beginning, if necessary) forms a diﬀerent word.
diﬀerence operator: the operator ∆where ∆f(x) = f(x + 1) −f(x) on integer or
real-valued functions. It is the discrete analogue of the diﬀerentiation operator.

GLOSSARY
141
diﬀerence sequence (for the sequence A = { aj | j = 0, 1, . . .}): the sequence ∆A =
{ aj+1 −aj | j = 0, 1, . . . }.
diﬀerence table (for a function f): a table whose kth row is the kth diﬀerence se-
quence for f.
discordant permutation: a permutation that assigns to every element an image dif-
ferent from those assigned by all other members of a given set of permutations.
dissimilar hypergeometric terms: terms in two hypergeometric series such that their
ratio is not a rational function.
divide-and-conquer algorithm: a recursive procedure that solves a given problem by
ﬁrst breaking it into smaller subproblems (of nearly equal size) and then combining
their respective solutions.
doubly hypergeometric: a property of function F(n, k) that F (n+1,k)
F (n,k)
and F (n,k+1)
F (n,k)
are rational functions of n and k.
Eulerian number: the number of permutations of {1, 2, . . ., n} with exactly k ascents.
excedance (of a permutation π): any index i such that πi > i.
exponential generating function (for the sequence a0, a1, a2, . . .): the function f(x)
= a0 + a1x + a2 x2
2! + · · ·
or any equivalent closed form expression.
falling power (of x): the product xn = x(x −1)(x −2) . . . (x −n + 1) of n successive
descending factors, starting with x; the discrete analogue of exponentiation.
Fibonacci numbers: the numbers Fn produced by the recursive deﬁnition F0 = 0,
F1 = 1, Fn = Fn−1 + Fn−2 if n ≥2.
ﬁgurate number: the number of cells in an array of cells bounded by some regular
geometrical ﬁgure.
ﬁrst-order linear recurrence relation with constant coeﬃcients: an equation of
the form C0an + C1an−1 = f(n), n ≥1, with C0 and C1 nonzero real constants.
generating function (for the sequence a0, a1, a2, . . .): the function f(x) = a0 + a1x +
a2x2 + · · ·
or any equivalent closed form expression; sometimes called the ordinary
generating function for the sequence.
geometric series: an inﬁnite series where the ratio between two consecutive terms is
a constant.
Gray code (of size n): a circular ordering of all binary strings of length n in which
adjacent strings diﬀer in exactly one bit.
harmonic number: the sum Hn = Pn
i=1
1
i , which is the discrete analogue of the
natural logarithm.
homogeneous recurrence relation: a recurrence relation satisﬁed by the identically
zero sequence.
hypergeometric series: a series where the ratio of two consecutive terms is a rational
function.
indeﬁnite sum (of the function f): the family of all antidiﬀerences of f.
Lah coeﬃcients: the coeﬃcients resulting from expressing the rising factorial in terms
of the falling factorials.

142
Chapter 3
SEQUENCES
linear recurrence relation with constant coeﬃcients: an equation of the form
C0an + C1an−1 + · · · + Ckan−k = f(n), n ≥k, where the Ci are real constants with
C0 and Ck nonzero.
little oh (of the function f): the set of all functions that grow slower than every con-
stant multiple of f, written o(f(n)).
little omega (of the function f): the set of all functions that grow faster than every
constant multiple of f, written ω(f(n)).
Lucas numbers: the numbers Ln produced by the recursive deﬁnition L0 = 2, L1 = 1,
Ln = Ln−1 + Ln−2 if n ≥2.
nonhomogeneous recurrence relation: a recurrence relation that is not homoge-
neous.
polyomino: a connected conﬁguration of regular polygons (e.g., triangles, squares, or
hexagons) in the plane, generalizing a domino.
power sum: the sum of the kth powers of the integers 1, 2, . . . , n.
radius of convergence (for the series P anxn): the number r (0 ≤r ≤∞) such that
the series converges for all |x| < r and diverges for all |x| > r.
Ramsey number: the number R(m, n) deﬁned as the smallest positive integer k with
the following property: if S is a set of size k and the 2-element subsets of S are
partitioned into two collections, C1 and C2, then there is a subset of S of size m
such that each of its 2-element subsets belong to C1 or there is a subset of S of size
n such that each of its 2-element subsets belong to C2.
recurrence relation: an equation expressing a term of a sequence as a function of prior
terms in the sequence.
rising power (of x): the product xn = x(x + 1)(x + 2) . . . (x + n −1) of n successive
ascending terms, starting with x.
second-order linear recurrence relation with constant coeﬃcients: an equation
of the form C0an + C1an−1 + C2an−2 = f(n), n ≥2, where C0, C1, C2 are real con-
stants with C0 and C2 nonzero.
sequence: a function from {0, 1, 2, . . .} to the real numbers (often the integers).
shift operator: the operator E deﬁned by Ef(x) = f(x + 1) on integer or real-valued
functions.
similar hypergeometric terms: terms in two hypergeometric series such that their
ratio is a rational function.
standardized form (for a sum): a sum over an integer interval, in which the lower
limit of the summation is zero.
Stirling’s approximation formula: the asymptotic estimate
√
2πn(n/e)n for n!.
tangent numbers: numbers generated by the exponential generating function tan x.

Section 3.1
SPECIAL SEQUENCES
143
3.1
SPECIAL SEQUENCES
3.1.1
REPRESENTATIONS OF SEQUENCES
A given inﬁnite sequence a0, a1, a2, . . . can often be represented in a more useful or more
compact form. Namely, there may be a closed form expression for an as a function of n,
the terms of the sequence may appear as coeﬃcients in a simple generating function,
or the sequence may be speciﬁed by a recurrence relation.
Each representation has
advantages, in either deﬁning the sequence or establishing information about its terms.
Deﬁnitions:
A sequence { an | n ≥0 } is a function from the set of nonnegative integers to the real
numbers (often the integers). The terms of the sequence { an | n ≥0 } are the values
a0, a1, a2, . . . .
A closed form for the sequence {an} is an algebraic expression for an as a function of n.
A recurrence relation (§3.3) is an equation expressing a term of a sequence as a function
of prior terms in the sequence. A solution of a recurrence relation is a sequence whose
terms satisfy the relation.
The (ordinary) generating function (§3.2.1) for the sequence {an} is the function
f(x) =
∞
P
i=0
aixi or any equivalent closed form expression.
The exponential generating function (§3.2.2) for the sequence {an} is the function
g(x) =
∞
P
i=0
ai xi
i! or any equivalent closed form expression.
Facts:
1. An important way in which many sequences are represented is by using a recurrence
relation. Although not all sequences can be represented by useful recurrence relations,
many sequences that arise in the solution of counting problems can be so represented.
2. An important way to study a sequence is by using its generating function. Informa-
tion about terms of the sequence can often be obtained by manipulating the generating
function.
Examples:
1. The Fibonacci numbers Fn (§3.1.2) arise in many applications and are given by the
sequence 0, 1, 1, 2, 3, 5, 8, 13, . . .. This inﬁnite sequence can be ﬁnitely encoded by means
of the recurrence relation
Fn = Fn−1 + Fn−2,
n ≥2,
with F0 = 0 and F1 = 1.
Alternatively, a closed form expression for this sequence is given by
Fn =
1
√
5
h
1+
√
5
2
n
−

1−
√
5
2
ni
,
n ≥0.
The Fibonacci numbers can be represented in a third way, via the generating function
f(x) =
x
1−x−x2 . Namely, when this rational function is expanded in terms of powers of

144
Chapter 3
SEQUENCES
x, the resulting coeﬃcients generate the values Fn:
x
1−x−x2 = 0x0 + 1x1 + 1x2 + 2x3 + 3x4 + 5x5 + 8x6 + 13x7 + · · · .
2. Table 1 gives closed form expressions for the generating functions of several com-
binatorial sequences discussed in this Handbook. In this table, r is any real number.
Generating functions for other sequences can be found in §3.2.1, Tables 1 and 2.
Table 1: Generating functions for particular sequences.
sequence
notation
reference
closed form
1, 2, 3, 4, 5, . . .
{n}
1
(1−x)2
12, 22, 32, 42, 52, . . .
{n2}
1+x
(1−x)3
13, 23, 33, 43, 53, . . .
{n3}
1+4x+x2
(1−x)4
1, r, r2, r3, r4, . . .
{rn}
1
1−rx
Fibonacci
Fn
§3.1.2
x
1−x−x2
Lucas
Ln
§3.1.2
2−x
1−x−x2
Catalan
Cn
§3.1.3
1−√1−4x
2x
Harmonic
Hn
§3.1.7
1
1−x ln
1
1−x
Binomial
 m
n

§2.3.2
(1 + x)m
3. Table 2 gives closed form expressions for the exponential generating functions of
several combinatorial sequences discussed in this Handbook. Generating functions for
other sequences can be found in §3.2.2, Tables 4 and 5.
Table 2: Exponential generating functions for particular sequences.
sequence
notation
reference
closed form
1, 1, 1, 1, 1, . . .
{1}
ex
1, r, r2, r3, r4, . . .
{rn}
erx
Derangements
Dn
§2.4.2
e−x
1−x
Bernoulli
Bn
§3.1.4
x
ex−1
Tangent
Tn
§3.1.7
tan x
Euler
En
§3.1.7
sech x
Euler
|En|
§3.1.7
sec x
Stirling cycle number
n
k

§2.5.2
1
k!
h
ln
1
1−x
ik
Stirling subset number
n
k
	
§2.5.2
1
k! [ex −1]k
4. Table 3 gives recurrence relations deﬁning particular combinatorial sequences dis-
cussed in this Handbook.
Table 3: Recurrence relations for particular sequences.
sequence
notation
reference
recurrence relation
Derangements
Dn
§2.4.2
Dn = (n −1)(Dn−1 + Dn−2),
D0 = 1, D1 = 0
Fibonacci
Fn
§3.1.2
Fn = Fn−1 + Fn−2, F0 = 0, F1 = 1
Lucas
Ln
§3.1.2
Ln = Ln−1 + Ln−2, L0 = 2, L1 = 1

Section 3.1
SPECIAL SEQUENCES
145
sequence
notation
reference
recurrence relation
Catalan
Cn
§3.1.3
Cn = C0Cn−1 + C1Cn−2 + · · ·
+ Cn−1C0,
C0 = 1
Bernoulli
Bn
§3.1.4
nP
j=0
 n+1
j

Bj = 0, B0 = 1
Eulerian
E(n, k)
§3.1.5
E(n, k) = (k + 1)E(n −1, k)
+ (n −k)E(n −1, k −1),
E(n, 0) = 1, n ≥1
Binomial
 n
k

§2.3.2
 n
k

=
 n−1
k

+
 n−1
k−1

,
 n
0

= 1, n ≥0
Stirling cycle
n
k

§2.5.2
n
k

= (n −1)
n−1
k

+
n−1
k−1

,
number
0
0

= 1;
n
0

= 0, n ≥1
Stirling subset
n
k
	
§2.5.2
n
k
	
= k
n−1
k
	
+
n−1
k−1
	
,
number
0
0
	
= 1;
n
0
	
= 0, n ≥1
3.1.2
FIBONACCI NUMBERS
Fibonacci numbers form an important sequence encountered in biology, physics, number
theory, computer science, and combinatorics. [BePhHo10], [Gr12], [PhBeHo01], [Va07]
Deﬁnitions:
The Fibonacci numbers F0, F1, F2, . . . are produced by the recursive deﬁnition F0 = 0,
F1 = 1, Fn = Fn−1 + Fn−2, n ≥2.
A generalized Fibonacci sequence is any sequence G0, G1, G2, . . . such that Gn =
Gn−1 + Gn−2 for n ≥2.
The Lucas numbers L0, L1, L2, . . . are produced by the recursive deﬁnition L0 = 2,
L1 = 1, Ln = Ln−1 + Ln−2, n ≥2. (Fran¸cois Lucas, 1842–1891)
Facts:
1. The Fibonacci numbers Fn and Lucas numbers Ln for n = 0, 1, 2, . . ., 50 are shown
in the following table.
2. The Fibonacci numbers were initially studied by Leonardo of Pisa (c. 1170–1250),
who was the son of Bonaccio; consequently these numbers have been called Fibonacci
numbers after Leonardo, the son of Bonaccio (Filius Bonaccii).
3.
lim
n→∞
Fn+1
Fn
= lim
n→∞
Ln+1
Ln
= 1
2(1 +
√
5) ≈1.61803, the golden ratio.

146
Chapter 3
SEQUENCES
n
Fn
Ln
n
Fn
Ln
n
Fn
Ln
0
0
2
17
1,597
3,571
34
5,702,887
12,752,043
1
1
1
18
2,584
5,778
35
9,227,465
20,633,239
2
1
3
19
4,181
9,349
36
14,930,352
33,385,282
3
2
4
20
6,765
15,127
37
24,157,817
54,018,521
4
3
7
21
10,946
24,476
38
39,088,169
87,403,803
5
5
11
22
17,711
39,603
39
63,245,986
141,422,324
6
8
18
23
28,657
64,079
40
102,334,155
228,826,127
7
13
29
24
46,368
103,682
41
165,580,141
370,248,451
8
21
47
25
75,025
167,761
42
267,914,296
599,074,578
9
34
76
26
121,393
271,443
43
433,494,437
969,323,029
10
55
123
27
196,418
439,204
44
701,408,733
1,568,397,607
11
89
199
28
317,811
710,647
45
1,134,903,170
2,537,720,636
12
144
322
29
514,229
1,149,851
46
1,836,311,903
4,106,118,243
13
233
521
30
832,040
1,860,498
47
2,971,215,073
6,643,838,879
14
377
843
31
1,346,269
3,010,349
48
4,807,526,976
10,749,957,122
15
610
1,364
32
2,178,309
4,870,847
49
7,778,742,049
17,393,796,001
16
987
2,207
33
3,524,578
7,881,196
50
12,586,269,025
28,143,753,123
4. Fibonacci numbers arise in numerous applications from various areas. For example,
they occur in models of population growth of rabbits (Example 3), in modeling plant
growth (Example 8), in counting the number of spanning trees of wheel graphs of length
n (Example 12), in counting the number of bit strings of length n without consecutive
0s (Example 13), and in many other contexts.
See [Gr12] and [Va07] for additional
applications of the Fibonacci numbers. The journal Fibonacci Quarterly is devoted to
the study of the Fibonacci numbers and related topics, a tribute to how widely the
Fibonacci numbers arise in mathematics and its applications to other areas. Further
readings about the Fibonacci numbers can be found at
• http://www.maths.surrey.ac.uk/hosted-sites/R.Knott/Fibonacci/
fibrefs.html.
5. Many properties of the Fibonacci numbers were derived by F. Lucas, who also is
responsible for naming them the “Fibonacci” numbers.
6. Binet form (Jacques Binet, 1786–1856): If α = 1
2(1 +
√
5) and β = 1
2(1 −
√
5) then
Fn = αn−βn
√
5
= αn−βn
α−β ,
Fn ∼αn
√
5.
Also,
Ln = αn + βn,
Ln ∼αn.
7. Fn = 1
2(Fn−2 + Fn+1) for all n ≥2. That is, each Fibonacci number is the average
of the terms occurring two places before and one place after it in the sequence.
8. Ln = 1
2(Ln−2 + Ln+1) for all n ≥2. That is, each Lucas number is the average of
the terms occurring two places before and one place after it in the sequence.
9. F0 + F1 + F2 + · · · + Fn = Fn+2 −1 for all n ≥0.
10. F0 −F1 + F2 −· · · + (−1)nFn = (−1)nFn−1 −1 for all n ≥1.
11. F1 + F3 + F5 + · · · + F2n−1 = F2n for all n ≥1.

Section 3.1
SPECIAL SEQUENCES
147
12. F0 + F2 + F4 + · · · + F2n = F2n+1 −1 for all n ≥0.
13. F 2
0 + F 2
1 + F 2
2 + · · · + F 2
n = FnFn+1 for all n ≥0.
14. F1F2 + F2F3 + F3F4 + · · · + F2n−1F2n = F 2
2n for all n ≥1.
15. F1F2 + F2F3 + F3F4 + · · · + F2nF2n+1 = F 2
2n+1 −1 for all n ≥1.
16. If k ≥1 then Fn+k = FkFn+1 + Fk−1Fn for all n ≥0.
17. Cassini’s identity: Fn+1Fn−1 −F 2
n = (−1)n for all n ≥1. (Jean Dominique Cassini,
1625–1712)
18. F 2
n+1 + F 2
n = F2n+1 for all n ≥0.
19. F 2
n+2 −F 2
n+1 = FnFn+3 for all n ≥0.
20. F 2
n+2 −F 2
n = F2n+2 for all n ≥0.
21. F 3
n+2 + F 3
n+1 −F 3
n = F3n+3 for all n ≥0.
22. gcd(Fn, Fm) = Fgcd(n,m). This implies that Fn and Fn+1 are relatively prime, and
that Fk divides Fnk.
23. Fibonacci numbers arise as sums of diagonals in Pascal’s triangle (§2.3.2):
Fn+1 =
⌊n/2⌋
P
j=0
 n−j
j

for all n ≥0.
24. F3n =
nP
j=0
 n
j

2jFj for all n ≥0.
25. The Fibonacci sequence F0, F1, F2, . . . has the generating function
x
1−x−x2 .
26. Fibonacci numbers with negative indices can be deﬁned using the recursive deﬁnition
Fn−2 = Fn −Fn−1. Then F−n = (−1)n−1Fn, n ≥1.
27. The units digits of the Fibonacci numbers form a sequence that repeats after 60
terms. (Joseph Lagrange, 1736–1813)
28. The number of binary strings of length n that contain no consecutive 0s is counted
by Fn+2. (See §3.3.2, Example 12.)
29. L0 + L1 + L2 + · · · + Ln = Ln+2 −1 for all n ≥0.
30. L2
0 + L2
1 + L2
2 + · · · + L2
n = LnLn+1 + 2 for all n ≥0.
31. Ln = Fn−1 + Fn+1, n ≥1. Hence, any formula containing Lucas numbers can be
translated into a formula involving Fibonacci numbers.
32. The Lucas sequence L0, L1, L2, . . . has the generating function
2−x
1−x−x2 .
33. Lucas numbers with negative indices can be deﬁned by extending the recursive
deﬁnition. Then L−n = (−1)nLn for all n ≥1.
34. Fn = Ln−1+Ln+1
5
, n ≥1. Hence, any formula involving Fibonacci numbers can be
translated into a formula involving Lucas numbers.
35. 2n+1Fn+1 =
nP
i=0
2iLi for all n ≥0.
36. If G0, G1, . . . is a sequence of generalized Fibonacci numbers, then Fn = Fn−1G0 +
FnG1 for all n ≥1.
Examples:
1. The Fibonacci number F8 can be computed using the initial values F0 = 0 and F1 = 1
and the recurrence relation Fn = Fn−1 + Fn−2 repeatedly: F2 = F1 + F0 = 1 + 0 = 1,
F3 = F2 + F1 = 1 + 1 = 2, F4 = F3 + F2 = 2 + 1 = 3, F5 = F4 + F3 = 3 + 2 = 5,
F6 = F5 + F4 = 5 + 3 = 8, F7 = F6 + F5 = 8 + 5 = 13, F8 = F7 + F6 = 13 + 8 = 21.

148
Chapter 3
SEQUENCES
2. Each male bee (drone) is produced asexually from a female, whereas each female bee
is produced from both a male and female. The ancestral tree for a single male bee is
shown below. This male has one parent, two grandparents, three great grandparents,
and in general Fk+2 kth-order grandparents, k ≥0.
M
M
M
M
M
F
F
F
F
F
F
F
3. Rabbit breeding:
This problem was originally posed by Fibonacci. A single pair
of immature rabbits is introduced into a habitat. It takes two months before a pair of
rabbits can breed; each month thereafter each pair of breeding rabbits produces another
pair. At the start of months 1 and 2, only the original pair A is present. In the third
month, A as well as their newly born pair B are present; in the fourth month, A, B as
well as the new pair C (progeny of A) are present; in the ﬁfth month, A, B, C as well as
the new pairs D (progeny of A) and E (progeny of B) are present. If Pn is the number
of pairs present in month n, then P1 = 1, P2 = 1, P3 = 2, P4 = 3, P5 = 5. In general,
Pn equals the number present in the previous month Pn−1 plus the number of breeding
pairs in the previous month (which is Pn−2, the number present two months earlier).
Thus Pn = Fn for n ≥1.
4. Let Sn denote the number of subsets of {1, 2, . . ., n} that do not contain consecutive
elements.
For example, when n = 3 the allowable subsets are ∅, {1}, {2}, {3}, {1, 3}.
Therefore, S3 = 5. In general, Sn = Fn+2 for n ≥1.
5. Draw n dots in a line. If each domino can cover exactly two such dots, in how many
ways can (nonoverlapping) dominoes be placed? The following ﬁgure shows the number
of possible solutions for n = 2, 3, 4. To ﬁnd a general expression for Dn, the number
of possible placements of dominoes with n dots, consider the rightmost dot in any such
placement P. If this dot is not covered by a domino, then P minus the last dot determines
a solution counted by Dn−1. If the last dot is covered by a domino, then the last two dots
in P are covered by this domino. Removing this rightmost domino then gives a solution
counted by Dn−2. Taking into account these two possibilities Dn = Dn−1 + Dn−2 for
n ≥3 with D1 = 1, D2 = 2. Thus Dn = Fn+1 for n ≥1.
n = 2
n = 3
n = 4
6. Compositions: Let Tn be the number of ordered compositions (§2.5.1) of the positive
integer n into summands that are odd. For example, 4 = 1 + 3 = 3 + 1 = 1 + 1 + 1 + 1
and 5 = 5 = 1 + 1 + 3 = 1 + 3 + 1 = 3 + 1 + 1 = 1 + 1 + 1 + 1 + 1. Therefore, T4 = 3 and
T5 = 5. In general, Tn = Fn for n ≥1.
7. Compositions: Let Bn be the number of ordered compositions (§2.5.1) of the positive
integer n into summands that are either 1 or 2. For example, 3 = 1+2 = 2+1 = 1+1+1

Section 3.1
SPECIAL SEQUENCES
149
and 4 = 2 + 2 = 1 + 1 + 2 = 1 + 2 + 1 = 2 + 1 + 1 = 1 + 1 + 1 + 1. Therefore, B3 = 3
and B4 = 5. In general, Bn = Fn+1 for n ≥1.
8. Botany: It has been observed in pine cones (and other botanical structures) that
the number of rows of scales winding in one direction is a Fibonacci number while the
number of rows of scales winding in the other direction is an adjacent Fibonacci number.
9. Continued fractions: The continued fraction 1 + 1
1 = 2
1, the continued fraction 1 +
1
1+ 1
1 = 3
2 and the continued fraction 1 +
1
1+
1
1+ 1
1
= 5
3. In general, a continued fraction
composed entirely of 1s equals the ratio of successive Fibonacci numbers.
10. Independent sets on a path: Consider a path graph on vertices 1, 2, . . ., n , with
edges joining vertices i and i + 1 for i = 1, 2, . . . , n −1. An independent set of vertices
(§8.6.3) consists of vertices no two of which are joined by an edge. By an analysis similar
to that in Example 5, the number of independent sets in a path graph on n vertices
equals Fn+2.
11. Independent sets on a cycle: Consider a cycle graph on vertices 1, 2, . . ., n, with
edges joining vertices i and i + 1 for i = 1, 2, . . . , n −1 as well as vertices n and 1. Then
the number of independent sets (§8.6.3) in a cycle graph on n vertices equals Ln.
12. Spanning trees: The number of spanning trees of the wheel graph Wn (§8.1.3) equals
L2n −2.
13. If A is the 2 × 2 matrix
 
1
1
1
0
!
, then An =
 
Fn+1
Fn
Fn
Fn−1
!
for n ≥1.
3.1.3
CATALAN NUMBERS
The sequence of integers called the Catalan numbers arises in counting a variety of
combinatorial structures, such as voting sequences, certain types of binary trees, paths
in the plane, and triangulations of polygons.
Deﬁnitions:
The Catalan numbers C0, C1, C2, . . . satisfy the nonlinear recurrence relation Cn =
C0Cn−1 + C1Cn−2 + · · · + Cn−1C0, n ≥1, with C0 = 1.
(See §3.3.1, Example 9.)
(Eug`ene Catalan, 1814–1894)
Well-formed (or balanced) sequences of parentheses of length 2n are deﬁned
recursively as follows: the empty sequence is well-formed; if sequence A is well-formed
so is (A); if sequences A and B are well-formed so is AB.
Well-parenthesized products of variables are deﬁned recursively as follows: single
variables are well-parenthesized; if A and B are well-parenthesized so is (AB).
Facts:
1. The ﬁrst 12 Catalan numbers Cn are given in the following table.
n
0
1
2
3
4
5
6
7
8
9
10
11
Cn
1
1
2
5
14
42
132
429
1,430
4,862
16,796
58,786
2.
lim
n→∞
Cn+1
Cn
= 4.
3. Catalan numbers arise in a variety of applications, such as in counting binary trees
on n vertices, triangulations of a convex n-gon, and well-formed sequences of n left and
n right parentheses. See the examples below as well as [Gr12] and [MiRo91].

150
Chapter 3
SEQUENCES
4. Cn =
1
n+1
 2n
n

for all n ≥0.
5. The Catalan numbers C0, C1, C2, . . . have the generating function 1−√1−4x
2x
.
6. Cn ∼
4n
√
πn3 .
7. Cn =
 2n
n

−
  2n
n−1

=
 2n−1
n

−
 2n−1
n+1

for all n ≥1.
8. Cn+1 = 2(2n+1)
n+2
Cn for all n ≥0.
Examples:
1. The number of binary trees (§9.1.2) on n vertices is Cn.
2. The number of left-right binary trees (§9.3.3) on 2n + 1 vertices is Cn.
3. The number of ordered trees (§9.1.2) on n vertices is Cn−1.
4. Suppose that a coin is tossed 2n times, coming up heads exactly n times and tails
exactly n times. The number of sequences of tosses in which the cumulative number of
heads is always at least as large as the cumulative number of tails is Cn. For example,
when n = 3 there are C3 = 5 such sequences of six tosses: HTHTHT, HTHHTT,
HHTTHT, HHTHTT, HHHTTT.
5. In Example 4, the number of sequences of tosses in which the cumulative number of
heads always exceeds the cumulative number of tails (until the very last toss) is Cn−1.
For example, when n = 3 there are C2 = 2 such sequences of six tosses: HHTHTT,
HHHTTT.
6. Triangulations: Let Tn be the number of triangulations of a convex n-gon, using
n−3 nonintersecting diagonals.
For instance, the following ﬁgure shows the T5 = 5
triangulations of a pentagon. In general, Tn = Cn−2 for n ≥3.
7. Suppose 2n points are placed in ﬁxed positions, evenly distributed on the circumfer-
ence of a circle. Then there are Cn ways to join n pairs of the points so that the resulting
chords do not intersect. The following ﬁgure shows the C3 = 5 solutions for n = 3.
8. Well-formed sequences of parentheses: The sequence of parentheses (()()) involving
three left and three right parentheses is well-formed, whereas the sequence ( ) ) ( ( ) is
not syntactically meaningful. There are ﬁve such well-formed sequences in this case:
()()(), ()(()), (())(), (()()), ((())).
Notice that if each left parenthesis is replaced by an H and each right parenthesis by a T,
then these ﬁve balanced sequences correspond exactly to the ﬁve coin tossing sequences
listed in Example 4. In general, the number of balanced sequences involving n left and n
right parentheses is Cn.
9. Consider the following procedure composed of n nested for loops:
count := 0
for i1 := 1 to 1

Section 3.1
SPECIAL SEQUENCES
151
for i2 := 1 to i1 + 1
for i3 := 1 to i2 + 1
...
for in := 1 to in−1 + 1
count := count + 1
Then the value of count upon exit from this procedure is Cn.
10. Well-parenthesized products:
The product x1x2x3 (relative to some binary “mul-
tiplication” operation) can be evaluated as either (x1x2)x3 or x1(x2x3). In the former,
x1 and x2 are ﬁrst combined and then the result is combined with x3. In the latter,
x2 and x3 are ﬁrst combined and then the result is combined with x1. Let Pn indicate
the number of ways to evaluate the product x1x2 . . . xn of n variables, using a binary
operation. Note that P3 = 2. In general, Pn = Cn−1. This was the problem originally
studied by Catalan. (See §3.3.1, Example 9.)
11. The numbers 1, 2, . . . , 2n are to be placed in the 2n positions of an 2 × n array
A = (aij). Such an arrangement is monotone if the values increase within each row and
within each column. Then there are Cn ways to form a monotone 2 × n array containing
the entries 1, 2, . . ., 2n. For instance, the following is one of the C4 = 14 monotone 2 × 4
arrays:
A =
 
1
3
5
6
2
4
7
8
!
.
3.1.4
BERNOULLI NUMBERS AND POLYNOMIALS
The Bernoulli numbers are important in obtaining closed form expressions for the sums of
powers of integers. These numbers also arise in expansions involving other combinatorial
sequences.
Deﬁnitions:
The Bernoulli numbers Bn satisfy the recurrence relation
nP
j=0
 n+1
j

Bj = 0 for all n ≥
1, with B0 = 1. (Jakob Bernoulli, 1654–1705)
The Bernoulli polynomials Bm(x) are given by Bm(x) =
m
P
k=0
 m
k

Bkxm−k.
Facts:
1. The ﬁrst 14 Bernoulli numbers Bn are shown in the following table.
n
0
1
2
3
4
5
6
7
8
9
10
11
12
13
Bn
1
−1
2
1
6
0
−1
30
0
1
42
0
−1
30
0
5
66
0
−691
2,730
0
2. B2k+1 = 0 for all k ≥1.
3. The nonzero Bernoulli numbers alternate in sign.
4. Bn = Bn(0).
5. The Bernoulli numbers have the exponential generating function
∞
P
n=0
Bn xn
n! =
x
ex−1.

152
Chapter 3
SEQUENCES
6. The Bernoulli numbers can be expressed in terms of the Stirling subset numbers
(§2.5.2): Bn =
nP
j=0
(−1)jn
j
	 j!
j+1 for all n ≥0.
7. The Bernoulli numbers appear as coeﬃcients in the Maclaurin expansion of tan x,
cot x, csc x, tanh x, coth x, and csch x.
8. The Bernoulli polynomials can be used to obtain closed form expressions for the sum
of powers of the ﬁrst n positive integers. (See §3.5.4.)
9. The ﬁrst 14 Bernoulli polynomials Bm(x) are shown in the following table.
n
Bn(x)
0
1
1
x −1
2
2
x2 −x + 1
6
3
x3 −3
2x2 + 1
2x
4
x4 −2x3 + x2 −1
30
5
x5 −5
2x4 + 5
3x3 −1
6x
6
x6 −3x5 + 5
2x4 −1
2x2 + 1
42
7
x7 −7
2x6 + 7
2x5 −7
6x3 + 1
6x
8
x8 −4x7 + 14
3 x6 −7
3x4 + 2
3x2 −1
30
9
x9 −9
2x8 + 6x7 −21
5 x5 + 2x3 −3
10x
10
x10 −5x9 + 15
2 x8 −7x6 + 5x4 −3
2x2 + 5
66
11
x11 −11
2 x10 + 55
6 x9 −11x7 + 11x5 −11
2 x3 + 5
6x
12
x12 −6x11 + 11x10 −33
2 x8 + 22x6 −33
2 x4 + x2 −
691
2,730
13
x13 −13
2 x12 + 13x11 −143
6 x9 + 286
7 x7 −429
10 x5 + 65
3 x3 −691
210x
10.
R 1
0 Bm(x) dx = 0 for all m ≥1.
11.
dBm(x)
dx
= mBm−1(x) for all m ≥1.
12. Bm+1(x + 1) −Bm+1(x) = (m + 1)xm for all m ≥0.
13. The Bernoulli polynomials have the exponential generating function given by
∞
P
m=0
Bm(x) tm
m! = text
et−1.
3.1.5
EULERIAN NUMBERS
Eulerian numbers are important in counting numbers of permutations with certain num-
bers of increases and decreases.
Deﬁnitions:
Let π = (π1, π2, . . . , πn) be a permutation of {1, 2, . . ., n}.
An ascent of the permutation π is any index i (1 ≤i < n) such that πi < πi+1. A
descent of the permutation π is any index i (1 ≤i < n) such that πi > πi+1.
An excedance of the permutation π is any index i (1 ≤i ≤n) such that πi > i. A
weak excedance of the permutation π is any index i (1 ≤i ≤n) such that πi ≥i.
The Eulerian number E(n, k), also written

n
k

, is the number of permutations of
{1, 2, . . ., n} with exactly k ascents.

Section 3.1
SPECIAL SEQUENCES
153
Facts:
1. E(n, k) is the number of permutations of {1, 2, . . ., n} with exactly k descents.
2. E(n, k) is the number of permutations of {1, 2, . . ., n} with exactly k excedances.
3. E(n, k) is the number of permutations of {1, 2, . . ., n} with exactly k + 1 weak ex-
cedances.
4. The Eulerian numbers can be used to obtain closed form expressions for the sum of
powers of the ﬁrst n positive integers (§3.5.4).
5. Eulerian numbers E(n, k) (1 ≤n ≤10, 0 ≤k ≤8) are given in the following table.
n\k
0
1
2
3
4
5
6
7
8
1
1
2
1
1
3
1
4
1
4
1
11
11
1
5
1
26
66
26
1
6
1
57
302
302
57
1
7
1
120
1,191
2,416
1,191
120
1
8
1
247
4,293
15,619
15,619
4,293
247
1
9
1
502
14,608
88,234
156,190
88,234
14,608
502
1
10
1
1,013
47,840
455,192
1,310,354
1,310,354
455,192
47,840
1,013
6. E(n, 0) = E(n, n −1) = 1 for all n ≥1.
7. Symmetry: E(n, k) = E(n, n −1 −k) for all n ≥1.
8. E(n, k) = (k + 1)E(n −1, k) + (n −k)E(n −1, k −1) for all n ≥2.
9.
n−1
P
k=0
E(n, k) = n! for all n ≥1.
10. Worpitzky’s identity:
xn =
n−1
P
k=0
E(n, k)
 x+k
n

for all n ≥1. (Julius Worpitzky,
1835–1895)
11. E(n, k) =
kP
j=0
(−1)j n+1
j

(k + 1 −j)n for all n ≥1.
12. The Bernoulli numbers (§3.1.4) can be expressed as alternating sums of Eulerian
numbers: Bm =
m
2m(2m−1)
m−2
P
k=0
(−1)kE(m −1, k) for m ≥2.
13. The Stirling subset numbers (§2.5.2) can be expressed in terms of the Eulerian
numbers:
n
m
	
=
1
m!
n−1
P
k=0
E(n, k)
 k
n−m

for n ≥m and n ≥1.
14. The Eulerian numbers have the following (bivariate) generating function in variables
x, t:
∞
P
m=0
∞
P
n=0
E(n, m)xm tn
n! =
1−x
e(x−1)t−x.
Examples:
1. The permutation π = (π1, π2, π3, π4) = (1, 2, 3, 4) has three ascents since 1 < 2 <
3 < 4 and it is the only permutation in S4 with three ascents; note that E(4, 3) =
1. There are E(4, 1) = 11 permutations in S4 with one ascent: (1, 4, 3, 2), (2, 1, 4, 3),
(2, 4, 3, 1), (3, 1, 4, 2), (3, 2, 1, 4), (3, 2, 4, 1), (3, 4, 2, 1), (4, 1, 3, 2), (4, 2, 1, 3), (4, 2, 3, 1),
and (4, 3, 1, 2).

154
Chapter 3
SEQUENCES
2. The permutation π = (2, 4, 3, 1) has two excedances since 2 > 1 and 4 > 2. There
are E(4, 2) = 11 such permutations in S4.
3. The permutation π = (1, 3, 2) has two weak excedances since 1 ≥1 and 3 ≥2. There
are E(3, 1) = 4 such permutations in S3: (1, 3, 2), (2, 1, 3), (2, 3, 1), and (3, 2, 1).
4. When n = 3, Worpitzky’s identity (Fact 10) states that
x3 = E(3, 0)
 x
3

+ E(3, 1)
 x+1
3

+ E(3, 2)
 x+2
3

=
 x
3

+ 4
 x+1
3

+
 x+2
3

.
This is veriﬁed algebraically since
 x
3

+ 4
 x+1
3

+
 x+2
3

= 1
6
 x(x −1)(x −2) + 4(x +
1)x(x −1) + (x + 2)(x + 1)x

= x
6
 x2 −3x + 2 + 4x2 −4 + x2 + 3x + 2

= x
6
 6x2
= x3.
3.1.6
RAMSEY NUMBERS
The Ramsey numbers arise from the work of Frank P. Ramsey (1903–1930), who in 1930
published a paper [Ra30] dealing with set theory that generalized the pigeonhole princi-
ple. (Also see §7.11.1, §8.11.2.) [GrRoSp13], [MiRo91], [RoTe09]
Deﬁnitions:
The Ramsey number R(m, n) is the smallest positive integer k with the following
property: if S is a set of size k and the 2-element subsets of S are partitioned into 2
collections, C1 and C2, then there is a subset of S of size m such that each of its 2-element
subsets belong to C1 or there is a subset of S of size n such that each of its 2-element
sets belong to C2.
The Ramsey number R(m1, . . . , mn; r) is the smallest positive integer k with the
following property: if S is a set of size k and the r-element subsets of S are partitioned
into n collections C1, . . . , Cn, then for some j there is a subset of S of size mj such that
each of its r-element subsets belong to Cj.
The Schur number S(n) is the smallest integer k with the following property: if
{1, . . ., k} is partitioned into n subsets A1, . . . , An, then there is a subset Ai such that
the equation x + y = z has a solution where x, y, z ∈Ai. (Issai Schur, 1875–1941)
Facts:
1. Ramsey’s theorem:
The Ramsey numbers R(m, n) and R(m1, . . . , mn; r) are well
deﬁned for all m, n ≥1 and for all m1, . . . , mn ≥1, r ≥1.
2. Ramsey numbers R(m, n) can be phrased in terms of coloring edges of the complete
graphs Kn: the Ramsey number R(m, n) is the smallest positive integer k such that, if
each edge of Kk is colored red or blue, then either the red subgraph contains a copy of
Km or else the blue subgraph contains a copy of Kn. (See §8.11.2.)
3. Symmetry: R(m, n) = R(n, m).
4. R(m, 1) = R(1, m) = 1 for every m ≥1.
5. R(m, 2) = R(2, m) = m for every m ≥1.
6. The values of few Ramsey numbers are known.
What is currently known about
Ramsey numbers R(m, n), for 3 ≤m ≤10 and 3 ≤n ≤10, and bounds on other Ramsey
numbers are displayed in Table 4. The entries in the body of this table are R(m, n)
(m, n ≤10) when known, or the best known range r1 ≤R(m, n) ≤r2 when not known.
The Ramsey numbers R(3, 3), R(3, 4), R(3, 5), and R(4, 4) were found by A. M. Gleason
and R. E. Greenwood in 1955; R(3, 6) was found by J. G. Kalbﬂeisch in 1966; R(3, 7)

Section 3.1
SPECIAL SEQUENCES
155
was found by J. E. Graver and J. Yackel in 1968; R(3, 8) was found by B. McKay and
Z. Ke Min; R(3, 9) was found by C. M. Grinstead and S. M. Roberts in 1982; R(4, 5) was
found by B. McKay and S. Radziszowski in 1993.
Table 4: Some classical Ramsey numbers.
m\n
3
4
5
6
7
8
9
10
3
6
9
14
18
23
28
36
40-42
4
–
18
25
36-41
49-61
58-84
73-115
92-149
5
–
–
43-49
58-87
80-143
101-216
126-316
144-442
6
–
–
–
102-165
113-298
132-495
169-780
179-1,171
7
–
–
–
–
205-540
217-1,031
241-1,713
289-2,826
8
–
–
–
–
–
282-1,870
317-3,583
≤6,090
9
–
–
–
–
–
–
565-6,588
581-12,677
10
–
–
–
–
–
–
–
798-23,556
Bounds for R(m, n) for m = 3 and 4, with 11 ≤n ≤15:
47 ≤R(3, 11) ≤50
98 ≤R(4, 11) ≤191
52 ≤R(3, 12) ≤59
128 ≤R(4, 12) ≤238
59 ≤R(3, 13) ≤68
133 ≤R(4, 13) ≤291
66 ≤R(3, 14) ≤77
141 ≤R(4, 14) ≤349
73 ≤R(3, 15) ≤87
153 ≤R(4, 15) ≤417
7. If m1 ≤m2 and n1 ≤n2, then R(m1, n1) ≤R(m2, n2).
8. R(m, n) ≤R(m, n −1) + R(n −1, m) for all m, n ≥2.
9. If m ≥3, n ≥3, and if R(m, n −1) and R(m −1, n) are even, then R(m, n) ≤
R(m, n −1) + R(m −1, n) −1.
10. R(m, n) ≤
 m+n−2
m−1

. (Erd˝os and Szekeres, 1935)
11. The Ramsey numbers R(m, n) satisfy the following asymptotic relationship:
√
2
e (1 + o(1))m2m/2 ≤R(m, m) ≤
 2m+2
m+1

· O((log m)−1).
12. There exist constants c1 and c2 such that c1m ln m ≤R(3, m) ≤c2m ln m.
13. The problem of ﬁnding the Ramsey numbers R(m1, . . . , mn; 2) can be phrased in
terms of coloring edges of the complete graphs Kn. R(m1, . . . , mn; 2) is equal to the
smallest positive integer k with the following property: no matter how the edges of Kk
are colored with the n colors 1, 2, . . . , n, there is some j such that Kk has a subgraph Kmj
of color j. (The edges of Kk are the 2-element subsets; Cj is the set of edges of color j.)
14. R(m1, m2; 2) = R(m1, m2).
15. Very little is also known about the numbers R(m1, . . . , mn; 2) if n ≥3.
16. R(2, . . . , 2; 2) = 2.
17. If each mi ≥3, the only Ramsey number whose value is known is R(3, 3, 3; 2) = 17.
18. R(m, r, r, . . . , r; r) = m if m ≥r.
19. R(m1, . . . mn; 1) = m1 + · · · + mn −(n −1).
20. Ramsey theory is a generalization of the pigeonhole principle. In the terminology
of Ramsey numbers, the fact that R(2, . . . , 2; 1) = n + 1 means that n + 1 is the smallest
positive integer with the property that if S has size n + 1 and the subsets of S are
partitioned into n sets C1, . . . , Cn, then for some j there is a subset of S of size 2 such

156
Chapter 3
SEQUENCES
that each of its elements belong to Cj. Hence, some Cj has at least two elements. If S
is a set of n + 1 pigeons and the subset Cj (j = 1, . . . , n) is the set of pigeons roosting
in pigeonhole j, then some pigeonhole must have at least two pigeons in it. The Ramsey
numbers R(2, . . . , 2; 1) give the smallest number of pigeons that force at least two to
roost in the same pigeonhole.
21. Schur’s theorem:
S(k) ≤R(3, . . . , 3; 2) (where there are k 3s in the notation for
the Ramsey number).
22. The following Schur numbers are known: S(1) = 2, S(2) = 5, S(3) = 14.
23. The equation x + y = z in the deﬁnition of Schur numbers has been generalized to
equations of the form x1 + · · · + xn−1 = xn, n ≥4. [BeBr82].
24. Convex sets:
Ramsey numbers play a role in constructing convex polygons. Sup-
pose m is a positive integer and there are n given points, no three of which are collinear.
If n ≥R(m, 5; 4), then a convex m-gon can be obtained from m of the n points [ErSz35].
This paper provided the impetus for the study of Ramsey numbers and suggested the
possibility of its wide applicability in mathematics.
25. It remains an unsolved problem to ﬁnd the smallest integer x (which depends on m)
such that if n ≥x, then a convex m-gon can be obtained from m of the n points.
26. Extensive information on Ramsey number theory, including bounds on Ramsey
numbers, can be found at S. Radziszowski’s web site:
• http://www.cs.rit.edu/~spr/ElJC/eline.html
Examples:
1. If six people are at a party, then either three of these six are mutual friends or three
are mutual strangers. If six is replaced by ﬁve, the result is not true. These facts follow
since R(3, 3) = 6. (See Fact 2. The six people can be regarded as vertices, with a red
edge joining friends and a blue edge joining strangers.)
2. If the set {1, . . . , k} is partitioned into two subsets A1 and A2, then the equation
x + y = z may or may not have a solution where x, y, z ∈A1 or x, y, z ∈A2. If k ≥5,
a solution is guaranteed since S(2) = 5.
If k < 5, no solution is guaranteed—take
A1 = {1, 4} and A2 = {2, 3}.
3.1.7
OTHER SEQUENCES
Additional sequences that regularly arise in discrete mathematics are now described.
⊲Euler Polynomials
Deﬁnition:
The Euler polynomials En(x) have the following exponential generating function:
∞
P
n=0
En(x) tn
n! = 2ext
et+1.
Facts:
1. En(x + 1) + En(x) = 2xn for all n ≥0.
2. The Euler polynomials can be expressed in terms of the Bernoulli numbers (§3.1.4):
En−1(x) = 1
n
nP
k=1
(2 −2k+1)
 n
k

Bkxn−k for all n ≥1.

Section 3.1
SPECIAL SEQUENCES
157
3. The alternating sum of powers of the ﬁrst n integers can be expressed in terms of the
Euler polynomials:
nP
j=1
(−1)n−jjk = 1
2

Ek(n + 1) + (−1)nEk(0)

.
4. The ﬁrst 14 Euler polynomials En(x) are shown in the following table.
n
En(x)
0
1
1
x −1
2
2
x2 −x
3
x3 −3
2x2 + 1
4
4
x4 −2x3 + x
5
x5 −5
2x4 + 5
2x2 −1
2
6
x6 −3x5 + 5x3 −3x
7
x7 −7
2x6 + 35
4 x4 −21
2 x2 + 17
8
8
x8 −4x7 + 14x5 −28x3 + 17x
9
x9 −9
2x8 + 21x6 −63x4 + 153
2 x2 −31
2
10
x10 −5x9 + 30x7 −126x5 + 255x3 −155x
11
x11 −11
2 x10 + 165
4 x8 −231x6 + 2,805
4
x4 −1,705
2
x2 + 691
4
12
x12 −6x11 + 55x9 −396x7 + 1,683x5 −3,410x3 + 2,073x
13
x13 −13
2 x12 + 143
2 x10 −1,287
2
x8 + 7,293
2
x6 −22,165
2
x4 + 26,949
2
x2 −5,461
2
⊲Euler and Tangent Numbers
Deﬁnitions:
The Euler numbers En are given by En = 2nEn( 1
2), where En(x) is an Euler polyno-
mial.
The tangent numbers Tn have the exponential generating function tan x:
∞
P
n=0
Tn xn
n! =
tan x.
Facts:
1. The ﬁrst twelve Euler numbers En and tangent numbers Tn are shown in the following
table.
n
0
1
2
3
4
5
6
7
8
9
10
11
En
1
0
−1
0
5
0
−61
0
1,385
0
−50,521
0
Tn
0
1
0
2
0
16
0
272
0
7,936
0
353,792
2. E2k+1 = T2k = 0 for all k ≥0.
3. The nonzero Euler numbers alternate in sign.
4. The Euler numbers have the exponential generating function
2
et+e−t = sech t.
5. The exponential generating function for |En| is P∞
n=0 |En| tn
n! = sec t.
6. The tangent numbers can be expressed in terms of the Bernoulli numbers (§3.1.4):
T2n−1 = (−1)n−1 4n(4n−1)
2n
B2n for all n ≥1.
7. The tangent numbers can be expressed as an alternating sum of Eulerian numbers
(§3.1.5): T2n+1 =
2n
P
k=0
(−1)n−kE(2n + 1, k) for all n ≥0.

158
Chapter 3
SEQUENCES
8. (−1)nE2n counts the number of alternating permutations in S2n: that is, the number
of permutations π = (π1, π2, . . . , π2n) on {1, 2, . . ., 2n} with π1 > π2 < π3 > π4 < · · · >
π2n.
9. T2n+1 counts the number of alternating permutations in S2n+1.
Examples:
1. The permutation π = (π1, π2, π3, π4) = (2, 1, 4, 3) is alternating since 2 > 1 < 4 > 3.
In all there are (−1)2E4 = 5 alternating permutations in S4: (2, 1, 4, 3), (3, 1, 4, 2),
(3, 2, 4, 1), (4, 1, 3, 2), (4, 2, 3, 1).
2. The permutation π = (π1, π2, π3, π4, π5) = (4, 1, 3, 2, 5) is alternating since 4 > 1 <
3 > 2 < 5. In all there are T5 = 16 alternating permutations in S5.
⊲Harmonic Numbers
Deﬁnition:
The harmonic numbers Hn are given by Hn = Pn
i=1
1
i for n ≥0, with H0 = 0.
Facts:
1. Hn is the discrete analogue of the natural logarithm (§3.4.1).
2. The ﬁrst twelve harmonic numbers Hn are shown in the following table.
n
0
1
2
3
4
5
6
7
8
9
10
11
Hn
0
1
3
2
11
6
25
12
137
60
49
20
363
140
761
280
7,129
2,520
7,381
2,520
83,711
27,720
3. The harmonic numbers can be expressed in terms of the Stirling cycle numbers
(§2.5.2): Hn = 1
n!
n+1
2

, n ≥1.
4.
nP
i=1
Hi = (n + 1)

Hn+1 −1

for all n ≥1.
5.
nP
i=1
iHi =
 n+1
2

Hn+1 −1
2

for all n ≥1.
6.
nP
i=1
 i
k

Hi =
 n+1
k+1

Hn+1 −
1
k+1

for all n ≥1.
7. Hn →∞as n →∞.
8. Hn ∼ln n + γ +
1
2n −
1
12n2 +
1
120n4 , where γ ≈0.577215664901533 denotes Euler’s
constant.
9. The harmonic numbers have the generating function
1
1−x ln
1
1−x.
Example:
1. Fact 8 yields the approximation H10 ≈2.928968257896. The actual value is H10 =
2.928968253968 . . ., so the approximation is accurate to 9 signiﬁcant digits. The ap-
proximation H20 ≈3.597739657206 is accurate to 10 digits, and the approximation
H40 ≈4.27854303893 is accurate to 12 digits.
⊲Gray Codes
Deﬁnition:
A Gray code of size n is an ordering Gn = (g1, g2, . . . , g2n) of the 2n binary strings of
length n such that gk and gk+1 diﬀer in exactly one bit, for 1 ≤k < 2n. Usually it is
required that g2n and g1 also diﬀer in exactly one bit.

Section 3.1
SPECIAL SEQUENCES
159
Facts:
1. Gray codes exist for all n ≥1. Sample Gray codes Gn are shown in the following
table.
n
Gn
1
0
1
2
00
10
11
01
3
000
100
110
010
011
111
101
001
4
0000
1000
1100
0100
0110
1110
1010
0010
0011
1011
1111
0111
0101
1101
1001
0001
5
00000
10000
11000
01000
01100
11100
10100
00100
00110
10110
11110
01110
01010
11010
10010
00010
00011
10011
11011
01011
01111
11111
10111
00111
00101
10101
11101
01101
01001
11001
10001
00001
2. A Gray code of size n ≥2 corresponds to a Hamilton cycle in the n-cube (§8.4.4).
3. Gray codes correspond to an ordering of all subsets of {1, 2, . . ., n} such that adja-
cent subsets diﬀer by the insertion or deletion of exactly one element. Each subset A
corresponds to a binary string a1a2 . . . an where ai = 1 if i ∈A, ai = 0 if i /∈A.
4. A Gray code Gn can be recursively obtained in the following way:
• ﬁrst half of Gn: Add a 0 to the end of each string in Gn−1.
• second half of Gn: Add a 1 to the end of each string in the reversal of Gn−1.
⊲de Bruijn Sequences
Deﬁnitions:
A (p, n) de Bruijn sequence on the alphabet Σ = {0, 1, . . ., p −1} is a sequence
(s0, s1, . . . , sL−1) of L = pn elements si ∈Σ such that each consecutive subsequence
(si, si+1, . . . , si+n−1) of length n is distinct.
Here the addition of subscripts is done
modulo L so that the sequence is considered as a circular ordering. (Nicolaas G. de
Bruijn, 1918–2012)
The de Bruijn diagram Dp,n is a directed graph whose vertices correspond to all
possible strings s1s2 . . . sn−1 of n−1 symbols from Σ. There are p arcs leaving the vertex
s1s2 . . . sn−1, each labeled with a distinct symbol α ∈Σ and leading to the adjacent vertex
s2s3 . . . sn−1α.
Facts:
1. The de Bruijn diagram Dp,n has pn−1 vertices and pn arcs.
2. Dp,n is a strongly connected digraph (§8.3.2).
3. Dp,n is an Eulerian digraph (§8.4.3).
4. There are (p!)pn−1p−n distinct (p, n) de Bruijn sequences.
5. Any Euler circuit in Dp,n produces a (p, n) de Bruijn sequence.
6. de Bruijn sequences exist for all p (with n ≥1). Sample de Bruijn sequences are

160
Chapter 3
SEQUENCES
shown in the following table.
(p, n)
de Bruijn sequence
(2, 1)
01
(2, 2)
0110
(2, 3)
01110100
(2, 4)
0101001101111000
(3, 2)
012202110
(3, 3)
012001110100022212202112102
(4, 2)
0113102212033230
7. A de Bruijn sequence can be generated from an alphabet Σ = {0, 1, . . ., p −1} of p
symbols using Algorithm 1.
Algorithm 1:
Generating a (p, n) de Bruijn sequence.
1. Start with the sequence S containing n zeros.
2. Append the largest symbol from Σ to S so that the newly formed sequence
S′ of n symbols does not already appear as a subsequence of S. Let S = S′.
3. Repeat Step 2 as long as possible.
4. When Step 2 cannot be applied, remove the last n −1 symbols from S.
Examples:
1. The de Bruijn diagram D2,3 is shown in the following ﬁgure. An Eulerian circuit is
obtained by visiting in order the vertices 11, 10, 01, 10, 00, 00, 01, 11, 11. The de Bruijn
sequence 01000111 is obtained by reading oﬀthe edge labels α as this circuit is traversed.
01
00
11
10
0
0
0
0
1
1
1
1
2. When p = 2 and n = 3, the formula in Fact 4 gives
(2!)23−1
23
= 24
23 = 2
distinct (2,3) de Bruijn sequences. These are 01000111 and its reversal 11100010.
⊲Self-generating Sequences
Deﬁnition:
Some unusual sequences deﬁned by simple recurrence relations or rules are informally
called self-generating sequences.

Section 3.1
SPECIAL SEQUENCES
161
Examples:
1. Hofstadter G-sequence: This sequence is deﬁned by a(n) = n −a(a(n −1)), with
initial condition a(0) = 0. The initial terms of this sequence are 0, 1, 1, 2, 3, 3, 4, 4, 5,
6, 7, 8, 8, 9, 9, 10, . . . . It is easy to show this sequence is well deﬁned. A formula for the
nth term of this sequence is a(n) = ⌊(n + 1)µ⌋, where µ = (−1 +
√
5)/2. [Ho79]
2. Variations of the Hofstadter G-sequence about which little is known: These include
the sequence deﬁned by a(n) = n −a(a(a(n −1))) with a(0) = 1, whose initial terms are
0, 1, 1, 2, 3, 4, 4, 5, 5, 6, 7, 7, 8, 9, 10, 10, 11, 12, 13, . . . and the sequence deﬁned by a(n) =
n −a(a(a(a(n −1))) with a(0) = 1, whose initial terms are 0, 1, 1, 2, 3, 4, 5, 5, 6, 6, 7,
8, 8, 9, 10, 11, 11, 12, 13, 14, . . . .
3. The sequence a(n) = a(n −a(n −1)) + a(n −a(n −2)), with a(0) = a(1) = 1, was
also deﬁned by Hofstadter. The initial terms of this sequence are 1, 1, 2, 3, 3, 4, 5, 5, 6,
6, 6, 8, 8, 8, 10, 10, 10, 12, . . . .
4. The intertwined sequence F(n) and M(n) are deﬁned by F(n) = n −F(M(n −1))
and M(n) = n −M(F(n −1)), with initial conditions F(0) = 1 and M(0) = 0. The
initial terms of the sequence F(n) (sometimes called the “female” sequence of the pair)
begins with the terms 1, 1, 2, 2, 3, 3, 4, 5, 5, 6, 6, 7, 8, 8, 9, 9, 10, . . . and the initial terms of
the sequence M(n) (sometimes called the “male” sequence of the pair) begins with the
terms 0, 0, 1, 2, 2, 3, 4, 4, 5, 6, 6, 7, 7, 8, 9, 9, 10, . . ..
5. Golomb’s self-generating sequence:
This sequence is the unique nondecreasing se-
quence a1, a2, a3, . . . with the property that it contains exactly ak occurrences of the
integer k for each integer k. The initial terms of this sequence are 1, 2, 2, 3, 3, 4, 4, 4,
5, 5, 5, 6, 6, 6, 6, . . . .
6. If f(n) is the largest integer m such that am = n where ak is the kth term of Golomb’s
self-generating sequence, then f(n) = Pn
k=1 ak and f(f(n)) = Pn
k=1 kak.
3.1.8
MINIGUIDE TO SEQUENCES
This section lists the numerical values of various integer sequences, classiﬁed according
to the type of combinatorial structure that produces the terms. This listing supplements
many of the tables presented in this Handbook.
A tabulation of over 5,400 integer
sequences is provided in [SlPl95], and over 250,000 entries are available through an online
database. (See Fact 4.)
Deﬁnitions:
The power sum Sk(n) = Pn
j=1 jk is the sum of the kth powers of the ﬁrst n positive
integers. The sum of the kth powers of the ﬁrst n odd integers is denoted Ok(n) =
Pn
j=1(2j −1)k.
The associated Stirling number of the ﬁrst kind d(n, k) is the number of k-cycle
permutations of an n-element set with all cycles of length ≥2.
The associated Stirling number of the second kind b(n, k) is the number of k-block
partitions of an n-element set with all blocks of size ≥2.
The double factorial n!! is the product n(n−2) . . . 6·4·2 if n is an even positive integer
and n(n −2) . . . 5 · 3 · 1 if n is an odd positive integer.

162
Chapter 3
SEQUENCES
The Lah coeﬃcients L(n, k) are the coeﬃcients of xk (§3.4.2) resulting from the ex-
pansion of xn (§3.4.2):
xn =
n
X
k=1
L(n, k)xk .
A permutation π is discordant from a set A of permutations when π(i) ̸= α(i) for all i
and all α ∈A. Usually A consists of the identity permutation ι and powers of the n-cycle
σn = (1 2 . . . n) (see §5.3.1).
A necklace with n beads in c colors corresponds to an equivalence class of functions
from an n-set to a c-set, under cyclic or dihedral equivalence.
A ﬁgurate number is the number of cells in an array of cells bounded by some regular
geometrical ﬁgure.
A polyomino with p polygons (cells) is a connected conﬁguration of p regular polygons
in the plane. The polygons usually considered are either triangles, squares, or hexagons.
Facts:
1. A database of integer sequences was initiated by Neal J. A. Sloane in 1964, while he
was exploring the sequence 1, 8, 78, 944, . . . that arose in determining the average height
of vertices in rooted trees.
2. Sloane published the book [Sl73], which contained over 2,300 integer sequences.
3. Together with Simon Plouﬀe, Sloane published a revised and expanded version of the
book in 1995 [SlPl95]; it contained over 5,400 integer sequences.
4. World Wide Web page: Sequences can be accessed and identiﬁed using a web page
maintained by The OEIS Foundation:
•
http://oeis.org
Sequences are arranged in lexicographic order, after ignoring any initial 0 or 1 terms as
well as the signs of terms, and are uniquely identiﬁed as A plus six digits.
5. Each entry in the following miniguide lists initial terms of the sequence, provides a
brief description, and gives the reference number used in the online database of Fact 4.
Examples:
1. The initial ﬁve terms 1, 2, 6, 20, 70, 252 of an unknown sequence were entered at
the site listed in Fact 4. Several matching sequences were identiﬁed, the ﬁrst of which
(A000984) corresponds to the central binomial coeﬃcients
 2n
n

.
Entering the terms
above, separated by commas, identiﬁes sequences in which the speciﬁed numbers occur
consecutively in the order given.
2. Entering the initial terms 1 1 2 3 5 8 13 21 34 into the OEIS site produced over
10,000 matching sequences. In this case, spaces were used rather than commas to delimit
the terms and so sequences matching the given terms in any order are returned. The
ﬁrst sequence produced was the Fibonacci sequence (A000045); another was the sequence
an =

e
n−1
2 
(A005181).

Section 3.1
SPECIAL SEQUENCES
163
⊲Miniguide to Sequences from Discrete Mathematics
The following miniguide contains a selection of important sequences, grouped by func-
tional problem area (such as graph theory, algebra, number theory). The sequences are
listed in a logical, rather than lexicographic, order within each identiﬁable grouping.
This listing supplements existing tables within the Handbook. References to appropriate
sections of the Handbook are also provided. The notation “Axxxxxx” is the reference
number used in the OEIS database (Fact 4).
Powers of Integers (§3.1.1, §3.5.4)
1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072
2n [A000079]
1, 3, 9, 27, 81, 243, 729, 2187, 6561, 19683, 59049, 177147, 531441, 1594323, 4782969
3n [A000244]
1, 4, 16, 64, 256, 1024, 4096, 16384, 65536, 262144, 1048576, 4194304, 16777216, 67108864
4n [A000302]
1, 5, 25, 125, 625, 3125, 15625, 78125, 390625, 1953125, 9765625, 48828125, 244140625
5n [A000351]
1, 6, 36, 216, 1296, 7776, 46656, 279936, 1679616, 10077696, 60466176, 362797056
6n [A000400]
1, 7, 49, 343, 2401, 16807, 117649, 823543, 5764801, 40353607, 282475249, 1977326743
7n [A000420]
1, 8, 64, 512, 4096, 32768, 262144, 2097152, 16777216, 134217728, 1073741824, 8589934592
8n [A001018]
1, 9, 81, 729, 6561, 59049, 531441, 4782969, 43046721, 387420489, 3486784401, 31381059609
9n [A001019]
1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400, 441, 484
n2 [A000290]
1, 8, 27, 64, 125, 216, 343, 512, 729, 1000, 1331, 1728, 2197, 2744, 3375, 4096, 4913, 5832
n3 [A000578]
1, 16, 81, 256, 625, 1296, 2401, 4096, 6561, 1000014641, 20736, 28561, 38416, 50625, 65536
n4 [A000583]
1, 32, 243, 1024, 3125, 7776, 16807, 32768, 59049, 100000, 161051, 248832, 371293, 537824
n5 [A000584]
1, 64, 729, 4096, 15625, 46656, 117649, 262144, 531441, 1000000, 1771561, 2985984
n6 [A001014]
1, 128, 2187, 16384, 78125, 279936, 823543, 2097152, 4782969, 10000000, 19487171
n7 [A001015]
1, 256, 6561, 65536, 390625, 1679616, 5764801, 16777216, 43046721, 100000000, 214358881
n8 [A001016]
1, 512, 19683, 262144, 1953125, 10077696, 40353607, 134217728, 387420489, 1000000000
n9 [A001017]
1, 3, 6, 10, 15, 21, 28, 36, 45, 55, 66, 78, 91, 105, 120, 136, 153, 171, 190, 210, 231, 253, 276, 300
S1(n) [A000217]

164
Chapter 3
SEQUENCES
1, 5, 14, 30, 55, 91, 140, 204, 285, 385, 506, 650, 819, 1015, 1240, 1496, 1785, 2109, 2470, 2870
S2(n) [A000330]
1, 9, 36, 100, 225, 441, 784, 1296, 2025, 3025, 4356, 6084, 8281, 11025, 14400, 18496, 23409
S3(n) [A000537]
1, 17, 98, 354, 979, 2275, 4676, 8772, 15333, 25333, 39974, 60710, 89271, 127687, 178312
S4(n) [A000538]
1, 33, 276, 1300, 4425, 12201, 29008, 61776, 120825, 220825, 381876, 630708, 1002001
S5(n) [A000539]
1, 65, 794, 4890, 20515, 67171, 184820, 446964, 978405, 1978405, 3749966, 6735950
S6(n) [A000540]
1, 129, 2316, 18700, 96825, 376761, 1200304, 3297456, 8080425, 18080425, 37567596
S7(n) [A000541]
1, 257, 6818, 72354, 462979, 2142595, 7907396, 24684612, 67731333, 167731333, 382090214
S8(n) [A000542]
1, 513, 20196, 282340, 2235465, 12313161, 52666768, 186884496, 574304985, 1574304985
S9(n) [A007487]
3, 6, 14, 36, 98, 276, 794, 2316, 6818, 20196, 60074, 179196, 535538, 1602516, 4799354
Sn(3) [A001550]
4, 10, 30, 100, 354, 1300, 4890, 18700, 72354, 282340, 1108650, 4373500, 17312754
Sn(4) [A001551]
5, 15, 55, 225, 979, 4425, 20515, 96825, 462979, 2235465, 10874275, 53201625, 261453379
Sn(5) [A001552]
6, 21, 91, 441, 2275, 12201, 67171, 376761, 2142595, 12313161, 71340451, 415998681
Sn(6) [A001553]
7, 28, 140, 784, 4676, 29008, 184820, 1200304, 7907396, 52666768, 353815700, 2393325424
Sn(7) [A001554]
8, 36, 204, 1296, 8772, 61776, 446964, 3297456, 24684612, 186884496, 1427557524
Sn(8) [A001555]
9, 45, 285, 2025, 15333, 120825, 978405, 8080425, 67731333, 574304985, 4914341925
Sn(9) [A001556]
1, 5, 32, 288, 3413, 50069, 873612, 17650828, 405071317, 10405071317, 295716741928
Sn(n) [A001923]
1, 28, 153, 496, 1225, 2556, 4753, 8128, 13041, 19900, 29161, 41328, 56953, 76636, 101025
O3(n) [A002593]
1, 82, 707, 3108, 9669, 24310, 52871, 103496, 187017, 317338, 511819, 791660, 1182285
O4(n) [A002309]
1, 244, 3369, 20176, 79225, 240276, 611569, 1370944, 2790801, 5266900, 9351001, 15787344
O5(n) [A002594]
Factorial Numbers
1, 1, 2, 6, 24, 120, 720, 5040, 40320, 362880, 3628800, 39916800, 479001600, 6227020800
n! [A000142]
1, 4, 36, 576, 14400, 518400, 25401600, 1625702400, 131681894400, 13168189440000
(n!)2 [A001044]

Section 3.1
SPECIAL SEQUENCES
165
2, 3, 8, 30, 144, 840, 5760, 45360, 403200, 3991680, 43545600, 518918400, 6706022400
n! + (n −1)! [A001048]
1, 2, 8, 48, 384, 3840, 46080, 645120, 10321920, 185794560, 3715891200, 81749606400
n!!, n even [A000165]
1, 1, 3, 15, 105, 945, 10395, 135135, 2027025, 34459425, 654729075, 13749310575
n!!, n odd [A001147]
1, 1, 2, 12, 288, 34560, 24883200, 125411328000, 5056584744960000
product of n factorials [A000178]
1, 2, 6, 30, 210, 2310, 30030, 510510, 9699690, 223092870, 6469693230, 200560490130
product of ﬁrst n primes [A002110]
Binomial Coeﬃcients (§2.3.2)
1, 3, 6, 10, 15, 21, 28, 36, 45, 55, 66, 78, 91, 105, 120, 136, 153, 171, 190, 210, 231, 253, 276
 n
2

[A000217]
1, 4, 10, 20, 35, 56, 84, 120, 165, 220, 286, 364, 455, 560, 680, 816, 969, 1140, 1330, 1540, 1771
 n
3

[A000292]
1, 5, 15, 35, 70, 126, 210, 330, 495, 715, 1001, 1365, 1820, 2380, 3060, 3876, 4845, 5985, 7315
 n
4

[A000332]
1, 6, 21, 56, 126, 252, 462, 792, 1287, 2002, 3003, 4368, 6188, 8568, 11628, 15504, 20349
 n
5

[A000389]
1, 7, 28, 84, 210, 462, 924, 1716, 3003, 5005, 8008, 12376, 18564, 27132, 38760, 54264, 74613
 n
6

[A000579]
1, 8, 36, 120, 330, 792, 1716, 3432, 6435, 11440, 19448, 31824, 50388, 77520, 116280, 170544
 n
7

[A000580]
1, 9, 45, 165, 495, 1287, 3003, 6435, 12870, 24310, 43758, 75582, 125970, 203490, 319770
 n
8

[A000581]
1, 10, 55, 220, 715, 2002, 5005, 11440, 24310, 48620, 92378, 167960, 293930, 497420, 817190
 n
9

[A000582]
1, 11, 66, 286, 1001, 3003, 8008, 19448, 43758, 92378, 184756, 352716, 646646, 1144066
  n
10

[A001287]
1, 2, 3, 6, 10, 20, 35, 70, 126, 252, 462, 924, 1716, 3432, 6435, 12870, 24310, 48620, 92378
central binomial coeﬃcients
 n
⌊n/2⌋

[A001405]
1, 2, 6, 20, 70, 252, 924, 3432, 12870, 48620, 184756, 705432, 2704156, 10400600, 40116600
central binomial coeﬃcients
 2n
n

[A000984]
1, 3, 10, 35, 126, 462, 1716, 6435, 24310, 92378, 352716, 1352078, 5200300, 20058300
 2n+1
n

[A001700]
Stirling Cycle Numbers/Stirling Numbers of the First Kind (§2.5.2)
1, 3, 11, 50, 274, 1764, 13068, 109584, 1026576, 10628640, 120543840, 1486442880
n
2

[A000254]
1, 6, 35, 225, 1624, 13132, 118124, 1172700, 12753576, 150917976, 1931559552, 26596717056
n
3

[A000399]
1, 10, 85, 735, 6769, 67284, 723680, 8409500, 105258076, 1414014888, 20313753096
n
4

[A000454]
1, 15, 175, 1960, 22449, 269325, 3416930, 45995730, 657206836, 9957703756, 159721605680
n
5

[A000482]

166
Chapter 3
SEQUENCES
1, 21, 322, 4536, 63273, 902055, 13339535, 206070150, 3336118786, 56663366760
n
6

[A001233]
1, 28, 546, 9450, 157773, 2637558, 44990231, 790943153, 14409322928, 272803210680
n
7

[A001234]
2, 11, 35, 85, 175, 322, 546, 870, 1320, 1925, 2717, 3731, 5005, 6580, 8500, 10812, 13566

n
n−2

[A000914]
6, 50, 225, 735, 1960, 4536, 9450, 18150, 32670, 55770, 91091, 143325, 218400, 323680

n
n−3

[A001303]
24, 274, 1624, 6769, 22449, 63273, 157773, 357423, 749463, 1474473, 2749747, 4899622

n
n−4

[A000915]
Stirling Subset Numbers/Stirling Numbers of the Second Kind (§2.5.2)
1, 6, 25, 90, 301, 966, 3025, 9330, 28501, 86526, 261625, 788970, 2375101, 7141686
n
3
	
[A000392]
1, 10, 65, 350, 1701, 7770, 34105, 145750, 611501, 2532530, 10391745, 42355950, 171798901
n
4
	
[A000453]
1, 15, 140, 1050, 6951, 42525, 246730, 1379400, 7508501, 40075035, 210766920, 1096190550
n
5
	
[A000481]
1, 21, 266, 2646, 22827, 179487, 1323652, 9321312, 63436373, 420693273, 2734926558
n
6
	
[A000770]
1, 28, 462, 5880, 63987, 627396, 5715424, 49329280, 408741333, 3281882604, 25708104786
n
7
	
[A000771]
1, 7, 25, 65, 140, 266, 462, 750, 1155, 1705, 2431, 3367, 4550, 6020, 7820, 9996, 12597, 15675

n
n−2
	
[A001296]
1, 15, 90, 350, 1050, 2646, 5880, 11880, 22275, 39325, 66066, 106470, 165620, 249900

n
n−3
	
[A001297]
1, 31, 301, 1701, 6951, 22827, 63987, 159027, 359502, 752752, 1479478, 2757118, 4910178

n
n−4
	
[A001298]
1, 1, 3, 7, 25, 90, 350, 1701, 7770, 42525, 246730, 1379400, 9321312, 63436373, 420693273
maxk
n
k
	
[A002870]
Associated Stirling Numbers of the First Kind (§3.1.8)
3, 20, 130, 924, 7308, 64224, 623376, 6636960, 76998240, 967524480, 13096736640
d(n, 2) [A000276]
15, 210, 2380, 26432, 303660, 3678840, 47324376, 647536032, 9418945536, 145410580224
d(n, 3) [A000483]
2, 20, 210, 2520, 34650, 540540, 9459450, 183783600, 3928374450, 91662070500
d(n, n −3) [A000906]
6, 130, 2380, 44100, 866250, 18288270, 416215800, 10199989800, 268438920750
d(n, n −4) [A000907]
1, 120, 7308, 303660, 11098780, 389449060, 13642629000, 486591585480, 17856935296200
d(2n, n −2) [A001785]
1, 24, 924, 26432, 705320, 18858840, 520059540, 14980405440, 453247114320
d(2n + 1, n −1) [A001784]

Section 3.1
SPECIAL SEQUENCES
167
Associated Stirling Numbers of the Second Kind (§3.1.8)
3, 10, 25, 56, 119, 246, 501, 1012, 2035, 4082, 8177, 16368, 32751, 65518, 131053, 262124
b(n, 2) [A000247]
15, 105, 490, 1918, 6825, 22935, 74316, 235092, 731731, 2252341, 6879678, 20900922
b(n, 3) [A000478]
1, 25, 490, 9450, 190575, 4099095, 94594500, 2343240900, 62199262125, 1764494857125
b(2n, n −1) [A000497]
1, 56, 1918, 56980, 1636635, 47507460, 1422280860, 44346982680, 1446733012725
b(2n + 1, n −1) [A000504]
Lah Coeﬃcients (§3.1.8)
1, 6, 36, 240, 1800, 15120, 141120, 1451520, 16329600, 199584000, 2634508800
L(n, 2) [A001286]
1, 12, 120, 1200, 12600, 141120, 1693440, 21772800, 299376000, 4390848000, 68497228800
L(n, 3) [A001754]
1, 20, 300, 4200, 58800, 846720, 12700800, 199584000, 3293136000, 57081024000
L(n, 4) [A001755]
1, 30, 630, 11760, 211680, 3810240, 69854400, 1317254400, 25686460800, 519437318400
L(n, 5) [A001777]
1, 42, 1176, 28224, 635040, 13970880, 307359360, 6849722880, 155831195520
L(n, 6) [A001778]
Eulerian Numbers (§3.1.5)
1, 4, 11, 26, 57, 120, 247, 502, 1013, 2036, 4083, 8178, 16369, 32752, 65519, 131054, 262125
E(n, 1) [A000295]
1, 11, 66, 302, 1191, 4293, 14608, 47840, 152637, 478271, 1479726, 4537314, 13824739
E(n, 2) [A000460]
1, 26, 302, 2416, 15619, 88234, 455192, 2203488, 10187685, 45533450, 198410786
E(n, 3) [A000498]
1, 57, 1191, 15619, 156190, 1310354, 9738114, 66318474, 423281535, 2571742175
E(n, 4) [A000505]
1, 120, 4293, 88234, 1310354, 15724248, 162512286, 1505621508, 12843262863
E(n, 5) [A000514]
1, 247, 14608, 455192, 9738114, 162512286, 2275172004, 27971176092, 311387598411
E(n, 6) [A001243]
1, 502, 47840, 2203488, 66318474, 1505621508, 27971176092, 447538817472, 6382798925475
E(n, 7) [A001244]
Other Special Sequences (§3.1)
1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6795, 10946, 17711
Fibonacci numbers, n ≥1 [A000045]
1, 3, 4, 7, 11, 18, 29, 47, 76, 123, 199, 322, 521, 843, 1364, 2207, 3571, 5778, 9349, 15127
Lucas numbers, n ≥1 [A000204]
1, 1, 2, 5, 14, 42, 132, 429, 1430, 4862, 16796, 58786, 208012, 742900, 2674440, 9694845
Catalan numbers, n ≥0 [A000108]
1, 3, 11, 25, 137, 49, 363, 761, 7129, 7381, 83711, 86021, 1145993, 1171733, 1195757
numerators of harmonic numbers, n ≥1 [A001008]

168
Chapter 3
SEQUENCES
1, 2, 6, 12, 60, 20, 140, 280, 2520, 2520, 27720, 27720, 360360, 360360, 360360, 720720
denominators of harmonic numbers, n ≥1 [A002805]
1, 1, 1, 1, 1, 5, 691, 7, 3617, 43867, 174611, 854513, 236364091, 8553103, 23749461029
numerators of Bernoulli numbers |B2n|, n ≥0 [A000367]
1, 6, 30, 42, 30, 66, 2730, 6, 510, 798, 330, 138, 2730, 6, 870, 14322, 510, 6, 1919190, 6, 13530
denominators of Bernoulli numbers |B2n|, n ≥0 [A002445]
1, 1, 5, 61, 1385, 50521, 2702765, 199360981, 19391512145, 2404879675441
Euler numbers |E2n|, n ≥0 [A000364]
1, 2, 16, 272, 7936, 353792, 22368256, 1903757312, 209865342976, 29088885112832
tangent numbers T2n+1, n ≥0 [A000182]
1, 1, 2, 5, 15, 52, 203, 877, 4140, 21147, 115975, 678570, 4213597, 27644437, 190899322
Bell numbers, n ≥0 [A000110]
Numbers of Certain Algebraic Structures (§1.4, §5.2)
1, 1, 1, 2, 1, 1, 1, 3, 2, 1, 1, 2, 1, 1, 1, 5, 1, 2, 1, 2, 1, 1, 1, 3, 2, 1, 3, 2, 1, 1, 1, 7, 1, 1, 1, 4, 1, 1, 1, 3
abelian groups of order n [A000688]
1, 1, 1, 2, 1, 2, 1, 5, 2, 2, 1, 5, 1, 2, 1, 14, 1, 5, 1, 5, 2, 2, 1, 15, 2, 2, 5, 4, 1, 4, 1, 51, 1, 2, 1, 14, 1, 2
groups of order n [A000001]
2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 60, 61, 67, 71, 73, 79, 83, 89, 97, 101
orders of simple groups [A005180]
60, 168, 360, 504, 660, 1092, 2448, 2520, 3420, 4080, 5616, 6048, 6072, 7800, 7920, 9828
orders of noncyclic simple groups [A001034]
1, 1, 2, 5, 16, 63, 318, 2045, 16999, 183231, 2567284, 46749427, 1104891746, 33823827452
partially ordered sets on n elements [A000112]
1, 2, 13, 171, 3994, 154303, 9415189, 878222530, 122207703623, 24890747921947
transitive relations on n elements [A006905]
1, 5, 52, 1522, 145984, 48464496, 56141454464, 229148550030864, 3333310786076963968
relations on n unlabeled points [A001173]
1, 2, 1, 2, 3, 6, 9, 18, 30, 56, 99, 186, 335, 630, 1161, 2182, 4080, 7710, 14532, 27594, 52377
binary irreducible polynomials of degree n [A001037]
Permutations (§5.3.1)
by cycles
1, 1, 1, 3, 15, 75, 435, 3045, 24465, 220185, 2200905, 24209955, 290529855, 3776888115
no 2-cycles [A000266]
1, 1, 2, 4, 16, 80, 520, 3640, 29120, 259840, 2598400, 28582400, 343235200, 4462057600
no 3-cycles [A000090]
1, 1, 2, 6, 18, 90, 540, 3780, 31500, 283500, 2835000, 31185000, 372972600, 4848643800
no 4-cycles [A000138]
0, 1, 1, 3, 9, 45, 225, 1575, 11025, 99225, 893025, 9823275, 108056025, 1404728325
no even length cycles [A000246]
discordant (§2.4.2, §3.1.8)
1, 0, 1, 2, 9, 44, 265, 1854, 14833, 133496, 1334961, 14684570, 176214841, 2290792932
derangements, discordant for ι [A000166]
1, 1, 0, 1, 2, 13, 80, 579, 4738, 43387, 439792, 4890741, 59216642, 775596313, 10927434464
menage numbers, discordant for ι and σn [A000179]

Section 3.1
SPECIAL SEQUENCES
169
0, 1, 2, 20, 144, 1265, 12072, 126565, 1445100, 17875140, 238282730, 3407118041
discordant for ι, σn, σ2
n [A000183]
by order
1, 2, 3, 4, 6, 6, 12, 15, 20, 30, 30, 60, 60, 84, 105, 140, 210, 210, 420, 420, 420, 420, 840, 840
max order [A000793]
1, 2, 3, 4, 6, 12, 15, 20, 30, 60, 84, 105, 140, 210, 420, 840, 1260, 1540, 2310, 2520, 4620, 5460
max order, increasing [A002809]
1, 2, 4, 16, 56, 256, 1072, 11264, 78976, 672256, 4653056, 49810432, 433429504, 4448608256
order a power of 2 [A005388]
0, 1, 3, 9, 25, 75, 231, 763, 2619, 9495, 35695, 140151, 568503, 2390479, 10349535, 46206735
order 2 [A001189]
0, 0, 2, 8, 20, 80, 350, 1232, 5768, 31040, 142010, 776600, 4874012, 27027728, 168369110
order 3 [A001471]
0, 0, 0, 6, 30, 180, 840, 5460, 30996, 209160, 1290960, 9753480, 69618120, 571627056
order 4 [A001473]
0, 0, 1, 3, 6, 10, 30, 126, 448, 1296, 4140, 17380, 76296, 296088, 1126216, 4940040, 23904000
odd, order 2 [A001465]
Necklaces (§2.6)
1, 2, 3, 4, 6, 8, 14, 20, 36, 60, 108, 188, 352, 632, 1182, 2192, 4116, 7712, 14602, 27596, 52488
2 colors, n beads [A000031]
1, 3, 6, 11, 24, 51, 130, 315, 834, 2195, 5934, 16107, 44368, 122643, 341802, 956635, 2690844
3 colors, n beads [A001867]
1, 4, 10, 24, 70, 208, 700, 2344, 8230, 29144, 104968, 381304, 1398500, 5162224, 19175140
4 colors, n beads [A001868]
1, 5, 15, 45, 165, 629, 2635, 11165, 48915, 217045, 976887, 4438925, 20346485, 93900245
5 colors, n beads [A001869]
Number Theory (§4.4, §4.6)
2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103
primes [A000040]
0, 1, 2, 2, 3, 3, 4, 4, 4, 4, 5, 5, 6, 6, 6, 6, 7, 7, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 10, 10, 11, 11, 11, 11, 11, 11
number of primes ≤n [A000720]
1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 1, 2, 1, 3, 1, 1, 2, 2, 2, 2, 1, 2, 2, 2
number of distinct primes dividing n [A001221]
2, 3, 5, 7, 13, 17, 19, 31, 61, 89, 107, 127, 521, 607, 1279, 2203, 2281, 3217, 4253, 4423, 9689
Mersenne primes [A000043]
1, 1, 1, 2, 1, 2, 1, 3, 2, 2, 1, 4, 1, 2, 2, 5, 1, 4, 1, 4, 2, 2, 1, 7, 2, 2, 3, 4, 1, 5, 1, 7, 2, 2, 2, 9, 1, 2, 2, 7
number of ways of factoring n [A001055]
1, 1, 2, 2, 4, 2, 6, 4, 6, 4, 10, 4, 12, 6, 8, 8, 16, 6, 18, 8, 12, 10, 22, 8, 20, 12, 18, 12, 28, 8, 30, 16
Euler totient function [A000010]
561, 1105, 1729, 2465, 2821, 6601, 8911, 10585, 15841, 29341, 41041, 46657, 52633, 62745
Carmichael numbers [A002997]
1, 2, 2, 3, 2, 4, 2, 4, 3, 4, 2, 6, 2, 4, 4, 5, 2, 6, 2, 6, 4, 4, 2, 8, 3, 4, 4, 6, 2, 8, 2, 6, 4, 4, 4, 9, 2, 4, 4, 8
number of divisors of n [A000005]

170
Chapter 3
SEQUENCES
1, 3, 4, 7, 6, 12, 8, 15, 13, 18, 12, 28, 14, 24, 24, 31, 18, 39, 20, 42, 32, 36, 24, 60, 31, 42, 40, 56
sum of divisors of n [A000203]
6, 28, 496, 8128, 33550336, 8589869056, 137438691328, 2305843008139952128
perfect numbers [A000396]
Partitions (§2.5.1)
1, 1, 2, 3, 5, 7, 11, 15, 22, 30, 42, 56, 77, 101, 135, 176, 231, 297, 385, 490, 627, 792, 1002, 1255
partitions of n [A000041]
1, 1, 1, 2, 2, 3, 4, 5, 6, 8, 10, 12, 15, 18, 22, 27, 32, 38, 46, 54, 64, 76, 89, 104, 122, 142, 165, 192
partitions of n into distinct parts [A000009]
1, 3, 6, 13, 24, 48, 86, 160, 282, 500, 859, 1479, 2485, 4167, 6879, 11297, 18334, 29601, 47330
planar partitions of n [A000219]
Figurate Numbers (§3.1.8)
polygonal
1, 3, 6, 10, 15, 21, 28, 36, 45, 55, 66, 78, 91, 105, 120, 136, 153, 171, 190, 210, 231, 253, 276
triangular [A000217]
1, 5, 12, 22, 35, 51, 70, 92, 117, 145, 176, 210, 247, 287, 330, 376, 425, 477, 532, 590, 651, 715
pentagonal [A000326]
1, 6, 15, 28, 45, 66, 91, 120, 153, 190, 231, 276, 325, 378, 435, 496, 561, 630, 703, 780, 861, 946
hexagonal [A000384]
1, 7, 18, 34, 55, 81, 112, 148, 189, 235, 286, 342, 403, 469, 540, 616, 697, 783, 874, 970, 1071
heptagonal [A000566]
1, 8, 21, 40, 65, 96, 133, 176, 225, 280, 341, 408, 481, 560, 645, 736, 833, 936, 1045, 1160, 1281
octagonal [A000567]
pyramidal
1, 4, 10, 20, 35, 56, 84, 120, 165, 220, 286, 364, 455, 560, 680, 816, 969, 1140, 1330, 1540, 1771
3-dimensional triangular, height n [A000292]
1, 5, 14, 30, 55, 91, 140, 204, 285, 385, 506, 650, 819, 1015, 1240, 1496, 1785, 2109, 2470, 2870
3-dimensional square, height n [A000330]
1, 6, 18, 40, 75, 126, 196, 288, 405, 550, 726, 936, 1183, 1470, 1800, 2176, 2601, 3078, 3610
3-dimensional pentagonal, height n [A002411]
1, 7, 22, 50, 95, 161, 252, 372, 525, 715, 946, 1222, 1547, 1925, 2360, 2856, 3417, 4047, 4750
3-dimensional hexagonal, height n [A002412]
1, 8, 26, 60, 115, 196, 308, 456, 645, 880, 1166, 1508, 1911, 2380, 2920, 3536, 4233, 5016, 5890
3-dimensional heptagonal, height n [A002413]
1, 5, 15, 35, 70, 126, 210, 330, 495, 715, 1001, 1365, 1820, 2380, 3060, 3876, 4845, 5985, 7315
4-dimensional triangular, height n [A000332]
1, 6, 20, 50, 105, 196, 336, 540, 825, 1210, 1716, 2366, 3185, 4200, 5440, 6936, 8721, 10830
4-dimensional square, height n [A002415]
1, 7, 25, 65, 140, 266, 462, 750, 1155, 1705, 2431, 3367, 4550, 6020, 7820, 9996, 12597, 15675
4-dimensional pentagonal, height n [A001296]
1, 8, 30, 80, 175, 336, 588, 960, 1485, 2200, 3146, 4368, 5915, 7840, 10200, 13056, 16473
4-dimensional hexagonal, height n [A002417]

Section 3.1
SPECIAL SEQUENCES
171
1, 9, 35, 95, 210, 406, 714, 1170, 1815, 2695, 3861, 5369, 7280, 9660, 12580, 16116, 20349
4-dimensional heptagonal, height n [A002418]
Polyominoes (§3.1.8)
1, 1, 2, 5, 12, 35, 108, 369, 1285, 4655, 17073, 63600, 238591, 901971, 3426576, 13079255
squares, n cells [A000105]
1, 1, 1, 3, 4, 12, 24, 66, 160, 448, 1186, 3334, 9235, 26166, 73983, 211297, 604107, 1736328
triangles, n cells [A000577]
1, 1, 3, 7, 22, 82, 333, 1448, 6572, 30490, 143552, 683101, 3274826, 15796897, 76581875
hexagons, n cells [A000228]
1, 1, 2, 8, 29, 166, 1023, 6922, 48311, 346543, 2522572, 18598427, 138462649, 1039496297
cubes, n cells [A000162]
Trees (§9.3)
1, 1, 1, 2, 3, 6, 11, 23, 47, 106, 235, 551, 1301, 3159, 7741, 19320, 48629, 123867, 317955
n unlabeled vertices [A000055]
1, 1, 2, 4, 9, 20, 48, 115, 286, 719, 1842, 4766, 12486, 32973, 87811, 235381, 634847, 1721159
rooted, n unlabeled vertices [A000081]
1, 1, 3, 16, 125, 1296, 16807, 262144, 4782969, 100000000, 2357947691, 61917364224
n labeled vertices [A000272]
1, 2, 9, 64, 625, 7776, 117649, 2097152, 43046721, 1000000000, 25937424601, 743008370688
rooted, n labeled vertices [A000169]
by diameter
1, 2, 5, 8, 14, 21, 32, 45, 65, 88, 121, 161, 215, 280, 367, 471, 607, 771, 980, 1232, 1551, 1933
diameter 4, n ≥5 vertices [A000094]
1, 2, 7, 14, 32, 58, 110, 187, 322, 519, 839, 1302, 2015, 3032, 4542, 6668, 9738, 14006, 20036
diameter 5, n ≥6 vertices [A000147]
1, 3, 11, 29, 74, 167, 367, 755, 1515, 2931, 5551, 10263, 18677, 33409, 59024, 102984, 177915
diameter 6, n ≥7 vertices [A000251]
1, 3, 14, 42, 128, 334, 850, 2010, 4625, 10201, 21990, 46108, 94912, 191562, 380933, 746338
diameter 7, n ≥8 vertices [A000550]
1, 4, 19, 66, 219, 645, 1813, 4802, 12265, 30198, 72396, 169231, 387707, 871989, 1930868
diameter 8, n ≥9 vertices [A000306]
by height
1, 3, 8, 18, 38, 76, 147, 277, 509, 924, 1648, 2912, 5088, 8823, 15170, 25935, 44042, 74427
height 3, n ≥4 vertices [A000235]
1, 4, 13, 36, 93, 225, 528, 1198, 2666, 5815, 12517, 26587, 55933, 116564, 241151, 495417
height 4, n ≥5 vertices [A000299]
1, 5, 19, 61, 180, 498, 1323, 3405, 8557, 21103, 51248, 122898, 291579, 685562, 1599209
height 5, n ≥6 vertices [A000342]
series-reduced
1, 1, 0, 1, 1, 2, 2, 4, 5, 10, 14, 26, 42, 78, 132, 249, 445, 842, 1561, 2988, 5671, 10981, 21209
n vertices [A000014]
1, 1, 0, 2, 4, 6, 12, 20, 39, 71, 137, 261, 511, 995, 1974, 3915, 7841, 15749, 31835, 64540
rooted, n vertices [A001679]

172
Chapter 3
SEQUENCES
0, 1, 0, 1, 1, 2, 3, 6, 10, 19, 35, 67, 127, 248, 482, 952, 1885, 3765, 7546, 15221, 30802, 62620
planted, n vertices [A001678]
Graphs (§8.1, §8.3, §8.4, §8.9)
1, 2, 4, 11, 34, 156, 1044, 12346, 274668, 12005168, 1018997864, 165091172592
n vertices [A000088]
chromatic number
4, 6, 7, 7, 8, 9, 9, 10, 10, 10, 11, 11, 12, 12, 12, 13, 13, 13, 13, 14, 14, 14, 15, 15, 15, 15, 16, 16
surface, connectivity n ≥1 [A000703]
4, 7, 8, 9, 10, 11, 12, 12, 13, 13, 14, 15, 15, 16, 16, 16, 17, 17, 18, 18, 19, 19, 19, 20, 20, 20, 21
surface, genus n ≥0 [A000934]
genus
0, 0, 0, 0, 1, 1, 1, 2, 3, 4, 5, 6, 8, 10, 11, 13, 16, 18, 20, 23, 26, 29, 32, 35, 39, 43, 46, 50, 55, 59, 63
complete graphs, n vertices [A00933]
connected
1, 1, 2, 6, 21, 112, 853,11117, 261080,11716571,1006700565,164059830476,50335907869219
n vertices [A001349]
1, 1, 0, 2, 5, 32, 234, 3638, 106147, 6039504, 633754161, 120131932774, 41036773627286
series-reduced, n vertices [A005636]
1, 1, 3, 5, 12, 30, 79, 227, 710, 2322, 8071, 29503, 112822, 450141, 1867871, 8037472
n edges [A002905]
1, 1, 4, 38, 728, 26704, 1866256, 251548592, 66296291072, 34496488594816
n labeled vertices [A001187]
directed
1, 3, 16, 218, 9608, 1540944, 882033440, 1793359192848, 13027956824399552
n vertices [A000273]
1, 3, 9, 33, 139, 718, 4535, 35979, 363083, 4717687, 79501654, 1744252509, 49872339897
transitive, n vertices [A001930]
1, 1, 2, 4, 12, 56, 456, 6880, 191536, 9733056, 903753248, 154108311168, 48542114686912
tournaments, n vertices [A000568]
1, 4, 29, 355, 6942, 209527, 9535241, 642779354, 63260289423, 8977053873043
transitive, n labeled vertices [A000798]
various
1, 2, 2, 4, 3, 8, 4, 14, 9, 22, 8, 74, 14, 56, 48, 286, 36, 380, 60, 1214, 240, 816, 188, 15506, 464
transitive, n vertices [A006799]
1, 1, 2, 3, 7, 16, 54, 243, 2038, 33120, 1182004, 87723296, 12886193064, 3633057074584
all degrees even, n vertices [A002854]
1, 0, 1, 1, 4, 8, 37, 184, 1782, 31026, 1148626, 86539128, 12798435868, 3620169692289
Eulerian, n vertices [A003049]
1, 0, 1, 3, 8, 48, 383, 6196, 177083, 9305118, 883156024, 152522187830
Hamiltonian, n vertices [A003216]
1, 2, 2, 4, 3, 8, 6, 22, 26, 176, 546, 19002, 389454, 50314870, 2942198546, 1698517037030
regular, n vertices [A005176]
0, 1, 1, 3, 10, 56, 468, 7123, 194066, 9743542, 900969091, 153620333545, 48432939150704
nonseparable, n vertices [A002218]

Section 3.2
GENERATING FUNCTIONS
173
1, 2, 4, 11, 33, 142, 822, 6966, 79853, 1140916, 18681008, 333312451
planar, n vertices [A005470]
3.2
GENERATING FUNCTIONS
Generating functions express an inﬁnite sequence as coeﬃcients arising from a power
series in an auxiliary variable. The closed form of a generating function is a concise way
to represent such an inﬁnite sequence. Properties of the sequence can be explored by
analyzing the closed form of an associated generating function. Two types of generating
functions are discussed in this section—ordinary generating functions and exponential
generating functions. The former arise when counting conﬁgurations in which order is
not important, while the latter are appropriate when order matters.
3.2.1
ORDINARY GENERATING FUNCTIONS
Deﬁnitions:
The (ordinary) generating function for the sequence a0, a1, a2, . . . of real numbers
is the formal power series f(x) = a0 + a1x + a2x2 + · · · = P∞
i=0 aixi or any equivalent
closed form expression.
The convolution of the sequence a0, a1, a2, . . . and the sequence b0, b1, b2, . . . is the
sequence c0, c1, c2, . . . in which ct = a0bt + a1bt−1 + a2bt−2 + · · · + atb0 = Pt
k=0 akbt−k.
Facts:
1. Generating functions are considered as algebraic forms and can be manipulated as
such, without regard to actual convergence of the power series.
2. A rational form (the ratio of two polynomials) is a concise expression for the generat-
ing function of the sequence obtained by carrying out long division on the polynomials.
(See Example 1.)
3. Generating functions are often useful for constructing and verifying identities involv-
ing binomial coeﬃcients and other special sequences. (See Example 10.)
4. Generating functions can be used to derive formulas for the sums of powers of integers.
(See Example 17.)
5. Generating functions can be used to solve recurrence relations. (See §3.3.4.)
6. Each sequence {an} deﬁnes a unique generating function f(x), and conversely.
7. Related generating functions:
Suppose f(x) = P∞
k=0 akxk and g(x) = P∞
k=0 bkxk
are generating functions for the sequences a0, a1, a2, . . . and b0, b1, b2, . . ., respectively.
Table 1 gives some related generating functions.
Table 1: Related generating functions.
generating function
sequence
xnf(x)
0, 0, 0, . . ., 0
|
{z
}
n
, a0, a1, a2, . . .

174
Chapter 3
SEQUENCES
generating function
sequence
f(x) −anxn
a0, a1, . . . , an−1, 0, an+1, . . .
a0 + a1x + · · · + anxn
a0, a1, . . . , an, 0, 0, . . .
f(x2)
a0, 0, a1, 0, a2, 0, a3, . . .
f(x)−a0
x
a1, a2, a3, . . .
f(x)−a0−a1x−···−anxn
xn+1
an+1, an+2, an+3, . . .
f ′(x)
a1, 2a2, 3a3, . . . , kak, . . .
xf ′(x)
0, a1, 2a2, 3a3, . . . , kak, . . .
R x
0 f(t) dt
0, a0, a1
2 , a2
3 , . . . ,
ak
k+1, . . .
f(x)
1−x
a0, a0 + a1, a0 + a1 + a2, a0 + a1 + a2 + a3, . . .
(1 + x)f(x)
a0, a0 + a1, a1 + a2, . . . , ak + ak+1, . . .
(1 + x + x2)f(x)
a0, a0 + a1, a0 + a1 + a2, a1 + a2 + a3, . . .
rf(x) + sg(x)
ra0 + sb0, ra1 + sb1, ra2 + sb2, . . .
f(x)g(x)
a0b0, a0b1 + a1b0, a0b2 + a1b1 + a2b0, . . .
(the convolution of {an} and {bn})
Examples:
1. The sequence 0, 1, 4, 9, 16, . . . of squares of the nonnegative integers has the generating
function 0 + x + 4x2 + 9x3 + 16x4 + · · · . However, this generating function has a concise
closed form expression, namely
x+x2
1−3x+3x2−x3 . Veriﬁcation is obtained by carrying out
long division on the indicated polynomials. This concise form can be used to deduce
properties involving the sequence, such as an explicit algebraic expression for the sum of
squares of the ﬁrst n positive integers. (See Example 17.)
2. The generating function for the sequence 1, 1, 1, 1, 1, . . . is 1+ x+ x2 + x3 + x4 + · · · =
1
1−x. Diﬀerentiating both sides of this expression produces 1+2x+3x2+4x3+· · · =
1
(1−x)2 .
Thus,
1
(1−x)2 is a closed form expression for the generating function of the sequence
1, 2, 3, 4, . . .. (See Table 2.)
3. Table 2 gives closed form expressions for the generating functions of particular se-
quences. In this table, r is an arbitrary real number, Fn is the nth Fibonacci num-
ber (§3.1.2), Ln is the nth Lucas number (§3.1.2), Cn is the nth Catalan number (§3.1.3),
and Hn is the nth harmonic number (§3.1.7).
Table 2: Generating functions for particular sequences.
sequence
closed form
1, 1, 1, 1, 1, . . .
1
1−x
1, 1, . . ., 1, 0, 0, . . . (n 1s)
1−xn
1−x
1, 1, . . . , 1, 1, 0, 1, 1, . . .
1
1−x −xn
(0 following n 1s)
1, −1, 1, −1, 1, −1, . . .
1
1+x
1, 0, 1, 0, 1, . . .
1
1−x2
1, 2, 3, 4, 5, . . .
1
(1−x)2
1, 4, 9, 16, 25, . . .
1+x
(1−x)3
1, r, r2, r3, r4, . . .
1
1−rx
0, r, 2r2, 3r3, 4r4, . . .
rx
(1−rx)2

Section 3.2
GENERATING FUNCTIONS
175
sequence
closed form
0, 1, 1
2, 1
3, 1
4, 1
5, . . .
ln
1
1−x
1
0!, 1
1!, 1
2!, 1
3!, 1
4!, . . .
ex
0, 1, −1
2, 1
3, −1
4, 1
5, . . .
ln(1 + x)
F0, F1, F2, F3, F4, . . .
x
1−x−x2
L0, L1, L2, L3, L4, . . .
2−x
1−x−x2
C0, C1, C2, C3, C4, . . .
1−√1−4x
2x
H0, H1, H2, H3, H4, . . .
1
1−x ln
1
1−x
4. For every positive integer n, the binomial theorem (§2.3.4) states that
(1 + x)n =
 n
0

+
 n
1

x +
 n
2

x2 + · · · +
 n
n

xn =
nP
k=0
 n
k

xk,
so (1+x)n is a closed form for the generating function of
 n
0

,
 n
1

,
 n
2

, . . . ,
 n
n

, 0, 0, 0, . . ..
5. For every positive integer n, the Maclaurin series expansion for (1 + x)−n is
(1 + x)−n = 1 + (−n)x + (−n)(−n−1)x2
2!
+ · · ·
= 1 +
∞
P
k=1
(−n)(−n−1)(−n−2)...(−n−k+1)
k!
xk.
Consequently, (1 + x)−n is the generating function for the sequence
 −n
0

,
 −n
1

,
 −n
2

, . . .,
where
 −n
k

= (−1)k n+k−1
k

is an extended binomial coeﬃcient (§2.3.2).
6. Using Example 5, the expansion of f(x) = (1 −3x)−8 is
(1 −3x)−8 = (1 + y)−8 =
∞
P
k=0
 −8
k

yk =
∞
P
k=0
 −8
k

(−3x)k.
So the coeﬃcient of x4 in f(x) is
 −8
4

(−3)4 = (−1)4 8+4−1
4

(81) =
 11
4

(81) = 26,730.
7. Table 3 gives additional examples of generating functions related to binomial expan-
sions. In this table, m and n are positive integers, and r is any real number.
Table 3: Examples of binomial-type generating functions.
generating function
expansion
(1 + x)n
 n
0

+
 n
1

x +
 n
2

x2 + · · · +
 n
n

xn = Pn
k=0
 n
k

xk
(1 + rx)n
 n
0

+
 n
1

rx +
 n
2

r2x2 + · · · +
 n
n

rnxn = Pn
k=0
 n
k

rkxk
(1 + xm)n
 n
0

+
 n
1

xm +
 n
2

x2m + · · · +
 n
n

xnm = Pn
k=0
 n
k

xkm
(1 + x)−n
 −n
0

+
 −n
1

x +
 −n
2

x2 + · · · = P∞
k=0(−1)k n+k−1
k

xk
(1 + rx)−n
 −n
0

+
 −n
1

rx +
 −n
2

r2x2 + · · · = P∞
k=0(−1)k n+k−1
k

rkxk
(1 −x)−n
 −n
0

+
 −n
1

(−x) +
 −n
2

(−x)2 + · · · = P∞
k=0
 n+k−1
k

xk
(1 −rx)−n
 −n
0

+
 −n
1

(−rx) +
 −n
2

(−rx)2 + · · · = P∞
k=0
 n+k−1
k

rkxk
xn
(1 −x)n+1
 n
n

xn +
 n+1
n

xn+1 +
 n+2
n

xn+2 + · · · = P∞
k=n
 k
n

xk
8. For any real number r, the Maclaurin series expansion for (1 + x)r is
(1 + x)r =
 r
0

1 +
 r
1

x +
 r
2

x2 + · · · ,
where
 r
k

= r(r−1)(r−2)...(r−k+1)
k!
if k > 0 and
 r
0

= 1.

176
Chapter 3
SEQUENCES
9. Using Example 8, the expansion of f(x) = √1 + x is
√
1 + x = (1 + x)1/2 =
 1/2
0

1 +
 1/2
1

x +
 1/2
2

x2 + · · ·
= 1 + 1
2x +
1
2 · −1
2
2! x2 +
1
2 · −1
2 · −3
2
3!
x3 +
1
2 · −1
2 · −3
2 · −5
2
4!
x4 + · · ·
= 1 + 1
2x −1
8x2 + 1
16x3 −
5
128x4 + · · · .
Thus √1 + x is the generating function for the sequence 1, 1
2, −1
8, 1
16, −5
128, . . . .
10. Vandermonde’s convolution identity (§2.3.4) can be obtained from the generating
functions f(x) = (1 + x)m and g(x) = (1 + x)n. First, (1 + x)m(1 + x)n = (1 + x)m+n.
Equating coeﬃcients of xr on both sides of this equation and using Table 3 produces
m
P
k=0
 m
k
  n
r−k

=
 m+n
r

.
11. Twenty identical computer terminals are to be distributed into ﬁve distinct rooms
so each room receives at least two terminals. The number of such distributions is the
coeﬃcient of x20 in the expansion of f(x) = (x2+x3+x4+· · · )5 = x10(1+x+x2+· · · )5 =
x10
(1−x)5 . Thus the coeﬃcient of x20 in f(x) is the coeﬃcient of x10 in (1 −x)−5, which
from Table 3 is
 5+10−1
10

=
 14
10

= 1,001.
12. Suppose in Example 11 that each room can accommodate at most seven terminals.
Now the generating function is g(x) = (x2 + x3 + x4 + x5 + x6 + x7)5 = x10(1 + x + x2 +
x3 + x4 + x5)5 = x10  1−x6
1−x
5. Consequently, the number of allowable distributions is the
coeﬃcient of x10 in
  1−x6
1−x
5 = (1−x6)5(1−x)−5 =

1−
 5
1

x6 +
 5
2

x12 −· · ·−x30 −5
0

+
 −5
1

(−x) +
 −5
2

(−x)2 + · · ·

. This coeﬃcient is
 −5
10

(−1)10 −
 5
1
 −5
4

(−1)4
=
 14
10

−
 5
1
 8
4

= 651.
13. Unordered selections with replacement:
k objects are selected from n distinct
objects, with repetition allowed. For each of the n distinct objects, the power series
1 + x + x2 + · · · represents the possible choices (namely none, one, two, . . .) for that
object. The generating function for all n objects is then
f(x) = (1 + x + x2 + · · · )n = (
1
1−x)n = (1 −x)−n = P∞
k=0
 n+k−1
k

xk.
The number of selections with replacement is the coeﬃcient of xk in f(x), namely
 n+k−1
k

.
14. Suppose there are p types of objects, with ni indistinguishable objects of type i.
The number of ways to pick a total of k objects (where the number of selected objects
of type i is at most ni) is the coeﬃcient of xk in the generating function
pQ
i=1
(1 + x + x2 + · · · + xni).
15. Partitions: Generating functions can be found for p(n), the number of partitions of
the positive integer n (§2.5.1). The number of 1s that appear as summands in a partition
of n is 0 or 1 or 2 or . . ., recorded as the terms in the power series 1 + x + x2 + x3 + · · · .
The power series 1 + x2 + x4 + x6 + · · · records the number of 2s that can appear in a
partition of n, and so forth. For example, p(12) is the coeﬃcient of x12 in
(1 + x + x2 + · · · )(1 + x2 + x4 + · · · ) . . . (1 + x12 + x24 + · · · ) =
12
Q
i=1
1
1−xi ,

Section 3.2
GENERATING FUNCTIONS
177
or in (1 + x+ x2 + · · ·+ x12)(1 + x2 + x4 + · · ·+ x12) . . . (1 + x12). In general, the function
P(x) = Q∞
i=1
1
1−xi is the generating function for the sequence p(0), p(1), p(2), . . . , where
p(0) is deﬁned as 1.
16. The function Pd(x) = (1 + x)(1 + x2)(1 + x3) . . . = Q∞
i=1(1 + xi) generates Q(n),
the number of partitions of n into distinct summands (see §2.5.1). The function Po(x) =
1
1−x ·
1
1−x3 ·
1
1−x5 . . . = Q∞
j=0(1−x2j+1)−1 is the generating function for O(n), the number
of partitions of n with all summands odd (see §2.5.1). Then
Pd(x) = (1 + x)(1 + x2)(1 + x3)(1 + x4) . . .
= 1−x2
1−x · 1−x4
1−x2 · 1−x6
1−x3 · 1−x8
1−x4 . . . =
1
1−x ·
1
1−x3 . . . = Po(x),
so Q(n) = O(n) for every nonnegative integer n.
17. Summation formulas:
Generating functions can be used to produce the formula
12+22+· · ·+n2 = 1
6n(n+1)(2n+1). (See §3.5.4 for an extensive tabulation of summation
formulas.) Manipulating the expansion (1 −x)−1 = 1 + x + x2 + x3 + · · · produces
x d
dx

x d
dx(1 −x)−1
= x(1+x)
(1−x)3 = x + 22x2 + 32x3 + · · · .
So x(1+x)
(1−x)3 is the generating function for the sequence 02, 12, 22, 32, . . . and, by Fact 7,
x(1+x)
(1−x)4 generates the sequence 02, 02 +12, 02 +12+22, 02+12+22+32, . . . . Consequently,
Pn
i=0 i2 is the coeﬃcient of xn in
(x + x2)(1 −x)−4 = (x + x2)
 −4
0

+
 −4
1

(−x) +
 −4
2

(−x)2 + · · ·

.
The answer is then
  −4
n−1

(−1)n−1 +
  −4
n−2

(−1)n−2 =
 n+2
n−1

+
 n+1
n−2

= 1
6n(n + 1)(2n + 1).
18. Catalan numbers: The Catalan numbers (§3.1.3) C0, C1, C2, . . . satisfy the recur-
rence relation Cn = C0Cn−1 + C1Cn−2 + · · ·+ Cn−1C0, n ≥1, with C0 = 1. (See §3.3.1.)
Hence their generating function f(x) = P∞
k=0 Ckxk satisﬁes xf 2(x) = f(x) −1, yielding
f(x) =
1
2x(1 −√1 −4x) =
1
2x(1 −(1 −4x)1/2). (The negative square root is chosen
since the numbers Ci cannot be negative.) Applying Example 8 to (1 −4x)1/2 yields
f(x) =
1
2x

1 −P∞
k=0
 1/2
k

(−4)kxk
=
1
2x

1 −P∞
k=0
−1
2k−1
 2k
k

xk
= P∞
k=0
1
k+1
 2k
k

xk.
Thus Cn =
1
n+1
 2n
n

.
3.2.2
EXPONENTIAL GENERATING FUNCTIONS
Encoding the terms of a sequence as coeﬃcients of
xk
k! is often helpful in obtaining
information about a sequence, such as in counting permutations of objects (where the
order of listing objects is important). The functions that result are called exponential
generating functions.
Deﬁnitions:
The exponential generating function for the sequence a0, a1, a2, . . . of real numbers
is the formal power series f(x) = a0 + a1x + a2 x2
2! + · · · = P∞
i=0 ai xi
i! or any equivalent
closed form expression.
The binomial convolution of the sequence a0, a1, a2, . . . and the sequence b0, b1, b2, . . .
is the sequence c0, c1, c2, . . . in which ct =
 t
0

a0bt+
 t
1

a1bt−1+
 t
2

a2bt−2+· · ·+
 t
t

atb0 =
Pt
k=0
 t
k

akbt−k, the coeﬃcient of xt
t! .

178
Chapter 3
SEQUENCES
Facts:
1. Each sequence {an} deﬁnes a unique exponential generating function f(x), and con-
versely.
2. Related exponential generating functions:
Suppose f(x) = P∞
k=0 ak xk
k! and g(x) =
P∞
k=0 bk xk
k! are exponential generating functions for the sequences a0, a1, a2, . . . and
b0, b1, b2, . . ., respectively. Table 4 gives some related exponential generating functions.
[Here P(n, k) =
 n
k

k! =
n!
(n−k)! is the number of k-permutations of a set with n distinct
objects. (See §2.3.1.)]
Table 4: Related exponential generating functions.
generating function
closed form
xf(x)
0, a0, 2a1, 3a2, . . . , (k + 1)ak, . . .
xnf(x)
0, 0, 0, . . ., 0
|
{z
}
n
, P(n, n)a0, P(n + 1, n)a1, . . . , P(n + k, n)ak, . . .
f ′(x)
a1, a2, a3, . . . , ak, . . .
R x
0 f(t)dt
0, a0, a1, a2, . . .
rf(x) + sg(x)
ra0 + sb0, ra1 + sb1, ra2 + sb2, . . .
f(x)g(x)
 0
0

a0b0,
 1
0

a0b1 +
 1
1

a1b0,
 2
0

a0b2 +
 2
1

a1b1 +
 2
2

a2b0, . . .
(binomial convolution of {ak} and {bk})
Examples:
1. The binomial theorem (§2.3.4) gives
(1 + x)n =
 n
0

+
 n
1

x +
 n
2

x2 +
 n
3

x3 + · · · +
 n
n

xn
= P(n, 0) + P(n, 1)x + P(n, 2) x2
2! + P(n, 3) x3
3! + · · · + P(n, n) xn
n! .
Hence (1 + x)n is the exponential generating function for the sequence P(n, 0), P(n, 1),
P(n, 2), P(n, 3), . . . , P(n, n), 0, 0, 0, . . ..
2. The Maclaurin series expansion for ex is ex = 1+x+ x2
2! + x3
3! +· · · , so the function ex
is the exponential generating function for the sequence 1, 1, 1, 1, . . .. The function e−x =
1−x+ x2
2! −x3
3! +· · · is the exponential generating function for the sequence 1, −1, 1, −1, . . ..
Consequently,
1
2(ex + e−x) = 1 + x2
2! + x4
4! + · · ·
is the exponential generating function for 1, 0, 1, 0, 1, 0, . . ., while
1
2(ex −e−x) = x + x3
3! + x5
5! + · · ·
is the exponential generating function for 0, 1, 0, 1, 0, 1, . . ..
3. The function f(x) =
1
1−x = P∞
i=0xi = P∞
i=0i! xi
i! is the exponential generating func-
tion for the sequence 0!, 1!, 2!, 3!, . . ..
4. Table 5 gives closed form expressions for the exponential generating functions of
particular sequences. In this table,
n
k

is a Stirling cycle number,
n
k
	
is a Stirling subset
number, Bn is the nth Bell number (§2.5.2), and Dn is the number of derangements of
n objects (§2.4.2).

Section 3.2
RECURRENCE RELATIONS
179
Table 5: Exponential generating functions for particular sequences.
sequence
closed form
1, 1, 1, 1, 1, . . .
ex
1, −1, 1, −1, 1, . . .
e−x
1, 0, 1, 0, 1, . . .
1
2(ex + e−x)
0, 1, 0, 1, 0, . . .
1
2(ex −e−x)
0, 1, 2, 3, 4, . . .
xex
P(n, 0), P(n, 1), . . . , P(n, n), 0, 0, . . .
(1 + x)n
0, . . . , 0,
n
n

,
n+1
n

, . . .
1
n!
h
ln
1
(1−x)
in
0, . . . , 0,
n
n
	
,
n+1
n
	
, . . .
1
n! [ex −1]n
B0, B1, B2, B3, B4, . . .
eex−1
D0, D1, D2, D3, D4, . . .
e−x
1−x
5. The number of ways to permute ﬁve of the eight letters in TERMINAL is found using
the exponential generating function f(x) = (1 + x)8. Here each of the eight letters in
TERMINAL is accounted for by the factor (1 + x), where 1 (= x0) indicates the letter
does not occur in the permutation and x (= x1) indicates that it does. The coeﬃcient of
x5
5! in f(x) is
 8
5

5! = P(8, 5) = 6,720.
6. The number of ways to permute ﬁve of the letters in TRANSPORTATION is found
as the coeﬃcient of x5
5! in the exponential generating function f(x) = (1+x+ x2
2! + x3
3! )(1+
x + x2
2! )4(1 + x)3. Here the factor 1 + x + x2
2! + x3
3! accounts for the letter T which can be
used 0, 1, 2, or 3 times. The factor 1 + x + x2
2! occurs four times—for each of R, A, N,
and O. Each of the letters S, P, and I produces the factor (1 + x). The coeﬃcient of x5
in f(x) is found to be 487
3 , so the answer is ( 487
3 )5! = 19,480.
7. The number of ternary sequences (made up of 0s, 1s, and 2s) of length 10 with at
least one 0 and an odd number of 1s can be found using the exponential generating
function
f(x) = (x + x2
2! + x3
3! + · · · )(x + x3
3! + x5
5! + · · · )(1 + x + x2
2! + x3
3! + · · · )
= (ex −1) 1
2(ex −e−x)ex = 1
2(e3x −e2x −ex + 1)
= 1
2
 ∞
P
i=0
(3x)i
i!
−
∞
P
i=0
(2x)i
i!
−
∞
P
i=0
xi
i! + 1

.
The answer is the coeﬃcient of x10
10! in f(x), which is 1
2(310 −210 −110) = 29,012.
8. Suppose in Example 7 that no symbol may occur exactly two times. The exponential
generating function is then f(x) = (1+x+ x3
3! + x4
4! +· · · )3 = (ex −x2
2 )3 = e3x −3
2x2e2x +
3
4x4ex −1
8x6. The number of ternary sequences is the coeﬃcient of x10
10! in f(x), namely
310 −3
2(10)(9)28 + 3
4(10)(9)(8)(7)16 = 28,269.
9. Exponential generating functions can be used to count the number of onto functions
ϕ: A →B where |A| = m and |B| = n. Each such function is speciﬁed by the sequence
of m values ϕ(a1), ϕ(a2), . . . , ϕ(am), where each element b ∈B occurs at least once in
this sequence. Element b contributes a factor (x + x2
2! + x3
3! + · · · ) = (ex −1) to the
exponential generating function f(x) = (ex −1)n. The number of onto functions is the
coeﬃcient of xm
m! in f(x), or n! times the coeﬃcient of xm
m! in (ex−1)n
n!
. From Table 5, the
answer is then n!
m
n
	
. (Also see §2.5.2.)

180
Chapter 3
SEQUENCES
3.3
RECURRENCE RELATIONS
In a number of counting problems, it may be diﬃcult to ﬁnd the solution directly. How-
ever, it is frequently possible to express the solution to a problem of a given size in terms
of solutions to problems of smaller size. This interdependence of solutions produces a
recurrence relation. Although there is no practical systematic way to solve all recurrence
relations, this section contains methods for solving certain types of recurrence relations,
thereby providing an explicit formula for the original counting problem. The topic of re-
currence relations provides the discrete counterpart to concepts in the study of ordinary
diﬀerential equations.
3.3.1
BASIC CONCEPTS
Deﬁnitions:
A recurrence relation for the sequence a0, a1, a2, . . . is an equation relating the term an
to certain of the preceding terms ai, i < n, for each n ≥n0.
The recurrence relation is linear if it expresses an as a linear function of a ﬁxed number
of preceding terms. Otherwise the relation is nonlinear.
The recurrence relation is kth-order if an can be expressed in terms of an−1, an−2, . . . ,
an−k.
The recurrence relation is homogeneous if the zero sequence a0 = a1 = · · · = 0 satisﬁes
the relation. Otherwise the relation is nonhomogeneous.
A kth-order linear homogeneous recurrence relation with constant coeﬃcients is an
equation of the form C0an + C1an−1 + · · · + Ckan−k = 0, n ≥k, where the Ci are real
constants with C0 ̸= 0, Ck ̸= 0. Initial conditions for this recurrence relation specify
particular values for k of the ai (typically a0, a1, . . . , ak−1).
Facts:
1. A kth-order linear homogeneous recurrence relation with constant coeﬃcients can
also be written C0an+k + C1an+k−1 + · · · + Ckan = 0, n ≥0.
2. There are in general an inﬁnite number of solution sequences {an} to a kth-order
linear homogeneous recurrence relation (with constant coeﬃcients).
3. A kth-order linear homogeneous recurrence relation with constant coeﬃcients to-
gether with k initial conditions on consecutive terms a0, a1, . . . , ak−1 uniquely determines
the sequence {an}. This is not necessarily the case for nonlinear relations (see Example
2) or when nonconsecutive initial conditions are speciﬁed (see Example 3).
4. The same recurrence relation can be written in diﬀerent forms by adjusting the
subscripts. For example, the recurrence relation an = 3an−1, n ≥1, can be written as
an+1 = 3an, n ≥0.
Examples:
1. The relation an −a2
n−1 + 2an−2 = 0, n ≥2, is a nonlinear homogeneous recurrence
relation with constant coeﬃcients. If the initial conditions a0 = 0, a1 = 1 are imposed,
this deﬁnes a unique sequence {an} whose ﬁrst few terms are 0, 1, 1, −1, −1, 3, 11, 115, . . ..

Section 3.3
RECURRENCE RELATIONS
181
2. The ﬁrst-order (constant coeﬃcient) recurrence relation a2
n+1 −an = 3, a0 = 1 is
nonhomogeneous and nonlinear. Even though one initial condition is speciﬁed, this does
not uniquely specify a solution sequence. Namely, the two sequences 1, −2, 1, 2, . . . and
1, −2, −1,
√
2, . . . satisfy the recurrence relation and the given initial condition.
3. The second-order relation an+2−an = 0, n ≥0, with nonconsecutive initial conditions
a1 = a3 = 0 does not uniquely specify a solution sequence. Both an = (−1)n + 1 and
an = 2(−1)n + 2 satisfy the recurrence and the given initial conditions.
4. Compound interest: If an initial investment of P dollars is made at a rate of r percent
compounded annually, then the amount an after n years is given by the recurrence relation
an = an−1(1 +
r
100), where a0 = P. [The amount at the end of the nth year is equal
to the amount an−1 at the end of the (n−1)st year plus the interest on an−1, namely
r
100an−1.]
5. Fibonacci sequence:
The Fibonacci numbers satisfy the second-order linear homo-
geneous recurrence relation an −an−1 −an−2 = 0.
6. Bit strings: Let an be the number of bit strings of length n. Then a0 = 1 (the empty
string) and an = 2an−1 if n > 0. [Every bit string of length n −1 gives rise to two bit
strings of length n, by placing a 0 or a 1 at the end of the string of length n −1.]
7. Bit strings with no consecutive 0s: See Example 12 of §3.3.2.
8. Permutations: Let an denote the number of permutations of {1, 2, . . ., n}. Then an
satisﬁes the ﬁrst-order linear homogeneous recurrence relation (with nonconstant coeﬃ-
cients) an+1 = (n + 1)an, n ≥1, a1 = 1. This follows since any n-permutation π can be
transformed into an (n+1)-permutation by inserting the element n+1 into any of the n+1
available positions—either at the beginning or end of π, or between two adjacent elements
of π. To solve for an, repeatedly apply the recurrence relation and its initial condition:
an = nan−1 = n(n −1)an−2 = n(n −1)(n −2)an−3 = · · · = n(n −1)(n −2) . . . 2a1 = n!.
9. Catalan numbers: The Catalan numbers (§3.1.3, §3.3.1) satisfy the nonlinear homo-
geneous recurrence relation Cn −C0Cn−1 −C1Cn−2 −· · · −Cn−1C0 = 0, n ≥1, with
initial condition C0 = 1. Given the product of n + 1 variables x1x2 . . . xn+1, let Cn be
the number of ways in which the multiplications can be carried out. For example, there
are ﬁve ways to form the product x1x2x3x4: ((x1x2)x3)x4, (x1(x2x3))x4, (x1x2)(x3x4),
x1((x2x3)x4), and x1(x2(x3x4)). No matter how the multiplications are performed, there
will be an outermost product of the form (x1x2 . . . xi)(xi+1 . . . xn+1). The number of ways
in which the product x1x2 . . . xi can be formed is Ci−1 and the number of ways in which
the product xi+1 . . . xn+1 can be formed is Cn−i. Thus, (x1x2 . . . xi)(xi+1 . . . xn+1) can
be obtained in Ci−1Cn−i ways. Summing these over the values i = 1, 2, . . ., n yields the
recurrence relation.
10. Tower of Hanoi: See Example 1 of §2.2.4.
11. Onto functions:
The number of onto functions ϕ: A →B can be found by devel-
oping a nonhomogeneous linear recurrence relation based on the size of B. Let |A| = m
and let an be the number of onto functions from A to a set with n elements. Then
an = nm −
 n
1

a1 −
 n
2

a2 −· · · −
  n
n−1

an−1, n ≥2, a1 = 1. This follows since the total
number of functions from A to B is nm and the number of functions that map A onto a
proper subset of B with exactly j elements is
 n
j

aj.
For example, if m = 7 and n = 4, applying this recursion gives a2 = 27 −2(1) = 126,
a3 = 37 −3(1) −3(126) = 1,806, a4 = 47 −4(1) −6(126) −4(1,806) = 8,400. Thus there
are 8,400 onto functions in this case.

182
Chapter 3
SEQUENCES
3.3.2
HOMOGENEOUS RECURRENCE RELATIONS
It is assumed throughout this subsection that the recurrence relations are linear with
constant coeﬃcients.
Deﬁnitions:
A geometric progression is a sequence a0, a1, a2, . . . for which a1
a0 = a2
a1 = · · · = an+1
an
=
· · · = r, the common ratio.
The characteristic equation for the kth-order recurrence relation C0an+C1an−1+· · ·+
Ckan−k = 0, n ≥k, is the equation C0rk + C1rk−1 + · · · + Ck = 0. The characteristic
roots are the roots of this equation.
The sequences {a(1)
n }, {a(2)
n }, . . . , {a(k)
n } are linearly dependent if there exist constants
t1, t2, . . . , tk, not all zero, such that Pk
i=1 tia(i)
n
= 0 for all n ≥0. Otherwise, they are
linearly independent.
Facts:
1. General method for solving a linear homogeneous recurrence relation with constant
coeﬃcients:
First ﬁnd the general solution. Then use the initial conditions to ﬁnd the
particular solution.
2. If the k characteristic roots r1, r2, . . . , rk are distinct, then rn
1 , rn
2 , . . . , rn
k are linearly
independent solutions of the homogeneous recurrence relation. The general solution is
an = c1rn
1 + c2rn
2 + · · · + ckrn
k , where c1, c2, . . . , ck are arbitrary constants.
3. If a characteristic root r has multiplicity m, then rn, nrn, . . . , nm−1rn are linearly
independent solutions of the homogeneous recurrence relation. The linear combination
c1rn + c2nrn + · · · + cmnm−1rn is also a solution, where c1, c2, . . . , cm are arbitrary
constants.
4. Facts 2 and 3 can be used together. If there are k characteristic roots r1, r2, . . . , rk,
with respective multiplicities m1, m2, . . . , mk (where some of the mi can equal 1), the
the general solution is a sum of sums, each of the form appearing in Fact 3.
5. de Moivre’s theorem: For any positive integer n, (cos θ + i sin θ)n = cos nθ + i sinnθ.
This result is used to ﬁnd solutions of recurrence relations when the characteristic roots
are complex numbers. (See Example 10.)
6. Solving ﬁrst-order recurrence relations: The solution of the homogeneous recurrence
relation an+1 = dan, n ≥0, with initial condition a0 = A, is an = Adn, n ≥0.
7. Solving second-order recurrence relations: Let r1, r2 be the characteristic roots asso-
ciated with the second-order homogeneous relation C0an +C1an−1 +C2an−2 = 0. There
are three possibilities:
• r1, r2 are distinct real numbers:
rn
1 and rn
2 are linearly independent solutions of
the recurrence relation. The general solution has the form
an = c1rn
1 + c2rn
2 ,
where the constants c1, c2 are found from the values of an for two distinct values
of n (often n = 0, 1).
• r1, r2 form a complex conjugate pair a ± bi: The general solution is
an = c1(a + bi)n + c2(a −bi)n = (
p
a2 + b2)n(k1 cos nθ + k2 sin nθ),

Section 3.3
RECURRENCE RELATIONS
183
with θ = arctan(b/a). Here (
√
a2 + b2)n cos nθ and (
√
a2 + b2)n sin nθ are lin-
early independent solutions.
• r1, r2 are real and equal:
rn
1 and nrn
1 are linearly independent solutions of the
recurrence relation. The general solution is
an = c1rn
1 + c2nrn
1 .
Examples:
1. The geometric progression 7, 21, 63, 189, . . ., with common ratio 3, satisﬁes the ﬁrst-
order homogeneous recurrence relation an+1 −3an = 0 for all n ≥0.
2. The ﬁrst-order homogeneous recurrence relation an+1 = 3an, n ≥0, does not deter-
mine a unique geometric progression. Any geometric sequence with ratio 3 is a solution;
for example the geometric progression in Example 1 (with a0 = 7), as well as the geo-
metric progression 5, 15, 45, 135, . . . (with a0 = 5).
3. The ﬁrst-order recurrence relation an+1 = 3an, n ≥0, a0 = 7 is easily solved using
Fact 6. The general solution is an = 7(3n) for all n ≥0.
4. Compound interest: If interest is compounded quarterly, how long does it take for an
investment of $500 to double when the annual interest rate is 8%? If an denotes the value
of the investment after n quarters have passed, then an+1 = an + 0.02an = (1.02)an,
n ≥0, a0 = 500. [Here the quarterly rate is 0.08/4 = 0.02 = 2%.] By Fact 6, the
solution is an = 500(1.02)n, n ≥0. The investment doubles when 1000 = 500(1.02)n, so
n =
log 2
log 1.02 ≈35.003. Consequently, after 36 quarters (or 9 years) the initial investment
of $500 (more than) doubles.
5. Population growth:
The number of bacteria in a culture (approximately) triples in
size every hour. If there are (approximately) 100,000 bacteria in a culture after six hours,
how many were there at the start? Deﬁne pn to be the number of bacteria in the culture
after n hours have elapsed. Then pn+1 = 3pn for n ≥0. From Fact 6, pn = p0(3n). So
100,000 = p0(36) and p0 ≈137.
6. Fibonacci sequence:
The Fibonacci sequence 0, 1, 1, 2, 3, 5, 8, 13, . . . arises in varied
applications (§3.1.2). Its terms satisfy the second-order homogeneous recurrence relation
Fn = Fn−1 + Fn−2, n ≥2, with initial conditions F0 = 0, F1 = 1. An explicit formula
can be obtained for Fn using Fact 7. The characteristic equation is r2 −r −1 = 0, with
distinct real roots 1±
√
5
2
. Thus the general solution is
Fn = c1

1+
√
5
2
n
+ c2

1−
√
5
2
n
.
Using the initial conditions F0 = 0, F1 = 1 gives c1 =
1
√
5, c2 = −1
√
5 and the explicit
formula
Fn =
1
√
5
h
1+
√
5
2
n
−

1−
√
5
2
n i
,
n ≥0.
7. Lucas sequence:
Related to the sequence of Fibonacci numbers is the sequence of
Lucas numbers 2, 1, 3, 4, 7, 11, 18, . . . (see §3.1.2). The terms of this sequence satisfy the
same second-order homogeneous recurrence relation Ln = Ln−1 + Ln−2, n ≥2, but with
the diﬀerent initial conditions L0 = 2, L1 = 1. The formula for Ln is
Ln =

1+
√
5
2
n
+

1−
√
5
2
n
,
n ≥0.
8. Random walk: A particle undergoes a random walk in one dimension, along the x-
axis. Barriers are placed at positions x = 0 and x = T . At any instant, the particle moves

184
Chapter 3
SEQUENCES
with probability p one unit to the right; with probability q = 1−p it moves one unit to the
left. Let an denote the probability that the particle, starting at position x = n, reaches
the barrier x = T before it reaches the barrier x = 0. It can be reasoned that an satisﬁes
the second-order recurrence relation an = pan+1 + qan−1 or pan+1 −an + qan−1 = 0. In
this case the two initial conditions are a0 = 0 and aT = 1. The characteristic equation
pr2 −r + q = (pr −q)(r −1) = 0 has roots 1, q
p. When p ̸= q, the roots are distinct and
the ﬁrst case of Fact 7 can be used to determine an; when p = q, the third case of Fact
7 must be used. (Explicit solutions are given in §7.5.2, Fact 10.)
9. The second-order relation an + 4an−1 −21an−2 = 0, n ≥2, has the characteristic
equation r2 + 4r −21 = 0, with distinct real roots 3 and −7. The general solution to the
recurrence relation is
an = c1(3n) + c2(−7)n,
n ≥0,
where c1, c2 are arbitrary constants. If the initial conditions specify a0 = 1 and a1 = 1,
then solving the equations 1 = a0 = c1 + c2, 1 = a1 = 3c1 −7c2 gives c1 = 4
5, c2 = 1
5. In
this case, the unique solution is
an = 4
5(3n) + 1
5(−7)n,
n ≥0.
10. The second-order relation an −6an−1 + 58an−2 = 0, n ≥2, has the characteristic
equation r2 −6r + 58 = 0, with complex conjugate roots r = 3 ± 7i. The general solution
is
an = c1(3 + 7i)n + c2(3 −7i)n,
n ≥0.
Using Fact 5, (3 + 7i)n = [
√
32 + 72(cos θ + i sin θ)]n = (
√
58 )n(cos nθ + i sin nθ), where
θ = arctan 7
3. Likewise (3 −7i)n = (
√
58 )n(cos nθ −i sin nθ). This gives the general
solution
an = (
√
58 )n[(c1 + c2) cos nθ + (c1 −c2)i sin nθ] = (
√
58 )n(k1 cos nθ + k2 sin nθ).
If the initial conditions a0 = 1 and a1 = 1 are speciﬁed, then 1 = a0 = k1, 1 = a1 =
√
58 (cos θ + k2 sin θ), yielding k1 = 1, k2 = −2
7. Thus
an = (
√
58 )n(cos nθ −2
7 sin nθ),
n ≥0.
11. The second-order relation an+2 −6an+1 + 9an = 0, n ≥0, has the characteristic
equation r2 −6r + 9 = (r −3)2 = 0, with the repeated roots 3, 3. The general solution
to this recurrence is
an = c1(3n) + c2n3n,
n ≥0.
If the initial conditions are a0 = 2 and a1 = 4, then 2 = a0 = c1, 4 = 2(3) + c2(1)(3),
giving c1 = 2, c2 = −2
3. Thus
an = 2(3n) −2
3n3n = 2(3n −n3n−1),
n ≥0.
12. For n ≥1, let an count the number of binary strings of length n that contain no
consecutive 0s. Here a1 = 2 (for the two strings 0 and 1) and a2 = 3 (for the strings 01,
10, 11). For n ≥3, a string counted in an ends in either 1 or 0. If the nth bit is 1, then
the preceding n −1 bits provide a string counted in an−1; if the nth bit is 0 then the
last two bits are 10, and the preceding n −2 bits give a string counted in an−2. Thus
an = an−1 + an−2, n ≥3, with a1 = 2 and a2 = 3. The solution to this relation is
simply an = Fn+2, the Fibonacci sequence shifted two places. An explicit formula for an
is obtained using the result in Example 6.

Section 3.3
RECURRENCE RELATIONS
185
13. The third-order recurrence relation an+3 −an+2 −4an+1 + 4an = 0, n ≥0, has the
characteristic equation r3 −r2 −4r + 4 = (r −2)(r + 2)(r −1) = 0, with characteristic
roots 2, −2, and 1. The general solution is given by
an = c12n + c2(−2)n + c31n = c12n + c2(−2)n + c3,
n ≥0.
14. The general solution of the third-order recurrence relation an+3 −3an+2 −3an+1 +
an = 0, n ≥0, is
an = c11n + c2n1n + c3n21n = c1 + c2n + c3n2,
n ≥0.
Here the characteristic roots are 1, 1, 1.
15. The fourth-order relation an+4 + 2an+2 + an = 0, n ≥0, has the characteristic
equation r4 + 2r2 + 1 = (r2 + 1)2 = 0. Since the characteristic roots are ±i, ±i, the
general solution is
an = c1in + c2(−i)n + c3nin + c4n(−i)n
= k1 cos nπ
2 + k2 sin nπ
2 + k3n cos nπ
2 + k4n sin nπ
2 , n ≥0.
3.3.3
NONHOMOGENEOUS RECURRENCE RELATIONS
It is assumed throughout this subsection that the recurrence relations are linear with
constant coeﬃcients.
Deﬁnition:
The kth-order nonhomogeneous recurrence relation has the form C0an + C1an−1 +
· · · + Ckan−k = f(n), n ≥k, where C0 ̸= 0, Ck ̸= 0, and f(n) ̸= 0 for at least one value
of n.
Facts:
1. General solution: The general solution of the nonhomogeneous kth-order recurrence
relation has the form
an = a(h)
n
+ a(p)
n ,
where a(h)
n
is the general solution of the homogeneous relation C0an + C1an−1 + · · · +
Ckan−k = 0, n ≥k, and a(p)
n
is a particular solution for the given relation C0an +
C1an−1 + · · · + Ckan−k = f(n), n ≥k.
2. Given the nonhomogeneous ﬁrst-order relation C0an +C1an−1 = krn, n ≥1, where r
and k are nonzero constants:
• If rn is not a solution of the associated homogeneous relation, then a(p)
n
= Arn
for A a constant.
• If rn is a solution of the associated homogeneous relation, then a(p)
n
= Bnrn for
B a constant.
3. Given the nonhomogeneous second-order relation C0an + C1an−1 + C2an−2 = krn,
n ≥2, where r and k are nonzero constants:
• If rn is not a solution of the associated homogeneous relation, then a(p)
n
= Arn
for A a constant.
• If a(h)
n
= c1rn + c2rn
1 , for r ̸= r1, then a(p)
n
= Bnrn for B a constant.

186
Chapter 3
SEQUENCES
• If a(h)
n
= c1rn + c2nrn, then a(p)
n
= Cn2rn for C a constant.
4. Consider the kth-order nonhomogeneous recurrence relation C0an + C1an−1 + · · · +
Ckan−k = f(n). If f(n) is a constant multiple of one of the forms in the ﬁrst column of
Table 1, then the associated trial solution t(n) is the corresponding entry in the second
column of the table. [Here A, B, A0, A1, . . . , At, r, α are real constants.]
• If no summand of t(n) solves the associated homogeneous relation, then a(p)
n
=
t(n) is a particular solution.
• If a summand of t(n) solves the associated homogeneous relation, then multiply
t(n) by the smallest (positive integer) power of n—say ns—so that no summand
of the adjusted trial solution nst(n) solves the associated homogeneous relation.
Then a(p)
n
= nst(n) is a particular solution.
• If f(n) is a sum of constant multiples of the forms in the ﬁrst column of Table 1,
then (adjusted) trial solutions are formed for each summand using the ﬁrst two
parts of Fact 4. Adding the resulting trial solutions then provides a particular
solution of the nonhomogeneous relation.
Table 1: Trial particular solutions for C0an + C1an−1 + · · · + Ckan−k = f(n).
f(n)
t(n)
c, a constant
A
nt (t a positive integer)
Atnt + At−1nt−1 + · · · + A1n + A0
rn
Arn
sin nα
A sin nα + B cos nα
cos nα
A sin nα + B cos nα
ntrn
rn(Atnt + At−1nt−1 + · · · + A1n + A0)
rn sin nα
rn(A sin nα + B cos nα)
rn cos nα
rn(A sin nα + B cos nα)
Examples:
1. Consider the nonhomogeneous relation an + 4an−1 −21an−2 = 5(4n), n ≥2. The
solution is an = a(h)
n + a(p)
n , where a(h)
n
is the solution of an +4an−1 −21an−2 = 0, n ≥2.
So
a(h)
n
= c1(3n) + c2(−7)n,
n ≥0.
From the third entry in Table 1 a(p)
n
= A(4n) for some constant A. Substituting this
into the given nonhomogeneous relation yields A(4n) + 4A(4n−1) −21A(4n−2) = 5(4n).
Dividing through by 4n−2 gives 16A + 16A −21A = 80, or A = 80/11. Consequently,
an = c1(3n) + c2(−7)n + 80
11(4n),
n ≥0.
If the initial conditions are a0 = 1 and a1 = 2, then c1 and c2 are found using 1 =
c1 + c2 + 80
11, 2 = 3c1 −7c2 + 320
11 , yielding
an = −71
10(3n) + 91
110(−7)n + 80
11(4n),
n ≥0.
2. Suppose the given recurrence relation is an + 4an−1 −21an−2 = 8(3n), n ≥2. Then
it is still true that
a(h)
n
= c1(3n) + c2(−7)n,
n ≥0,

Section 3.3
RECURRENCE RELATIONS
187
where c1 and c2 are arbitrary constants. By the second part of Fact 3, a particular
solution is a(p)
n
= An3n. Substituting a(p)
n
gives An3n+4A(n−1)3n−1−21A(n−2)3n−2 =
8(3n). Dividing by 3n−2 produces 9An+12A(n−1)−21A(n−2) = 72, so A = 12
5 . Thus
an = c1(3n) + c2(−7)n + 12
5 n3n,
n ≥0.
3. Tower of Hanoi: (See Example 1 of §2.2.4.) If an is the minimum number of moves
needed to transfer the n disks, then an satisﬁes the ﬁrst-order nonhomogeneous relation
an −2an−1 = 1,
n ≥1,
where a0 = 0. Here a(h)
n
= c(2n) for an arbitrary constant c, and a(p)
n
= A, using the ﬁrst
entry of Table 1. So A = 2A+1 or A = −1. Hence an = c(2n)−1 and 0 = a0 = c(20)−1
implies c = 1, giving
an = 2n −1,
n ≥0.
4. How many regions are formed if n lines are drawn in the plane, in general position
(no two parallel and no three intersecting at a point)?
If an denotes the number of
regions thus formed, then a1 = 2, a2 = 4, and a3 = 7 are easily determined. A general
formula can be found by developing a recurrence relation for an. Namely, if line n + 1
is added to the diagram with an regions formed by n lines, this new line intersects all
the other n lines. These intersection points partition line n + 1 into n + 1 segments,
each of which splits an existing region in two. As a result, an+1 = an + (n + 1), n ≥1,
a ﬁrst-order nonhomogeneous recurrence relation. Solving this relation with the initial
condition a1 = 1 produces an = 1
2(n2 + n + 2).
3.3.4
METHOD OF GENERATING FUNCTIONS
Generating functions (see §3.2.1) can be used to solve individual recurrence relations as
well as simultaneous systems of recurrence relations. This technique is analogous to the
use of Laplace transforms in solving systems of diﬀerential equations.
Facts:
1. To solve the kth-order recurrence relation C0an+k + · · · + Ckan = f(n), n ≥0, carry
out the following steps:
• Multiply both sides of the recurrence equation by xn+k and sum the result.
• Take this new equation, rewrite it in terms of the generating function f(x) =
P∞
n=0 anxn and solve for f(x).
• Expand the expression found for f(x) in terms of powers of x so that the coeﬃcient
an can be identiﬁed.
2. To solve a system of kth-order recurrence relations, carry out the following steps:
• Multiply both sides of each recurrence equation by xn+k and sum the results.
• Rewrite the system of equations in terms of the generating functions f(x), g(x), . . .
for an, bn, . . ., and solve for these generating functions.
• Expand the expressions found for each generating function in terms of powers of x
so that the coeﬃcients an, bn, . . . can be identiﬁed.

188
Chapter 3
SEQUENCES
Examples:
1. The nonhomogeneous ﬁrst-order relation an+1 −2an = 1, n ≥0, a0 = 0, arises in the
Tower of Hanoi problem (§3.3.3, Example 3). Begin by applying the ﬁrst step of Fact 1:
an+1xn+1 −2anxn+1 = xn+1,
∞
P
n=0
an+1xn+1 −2
∞
P
n=0
anxn+1 =
∞
P
n=0
xn+1.
Then apply the second step of Fact 1:
∞
P
n=0
an+1xn+1 −2x
∞
P
n=0
anxn = x
∞
P
n=0
xn,
(f(x) −a0) −2xf(x) =
x
1−x,
(f(x) −0) −2xf(x) =
x
1−x.
Solving for f(x) gives
f(x) =
x
(1−x)(1−2x) =
1
1−2x −
1
1−x =
∞
P
n=0
(2x)n −
∞
P
n=0
xn =
∞
P
n=0
(2n −1)xn.
Since an is the coeﬃcient of xn in f(x), an = 2n −1, n ≥0.
2. To solve the nonhomogeneous second-order relation an+2 −2an+1 + an = 2n, n ≥0,
a0 = 1, a1 = 2, apply the ﬁrst step of Fact 1:
an+2xn+2 −2an+1xn+2 + anxn+2 = 2nxn+2,
∞
P
n=0
an+2xn+2 −2
∞
P
n=0
an+1xn+2 +
∞
P
n=0
anxn+2 =
∞
P
n=0
2nxn+2.
The second step of Fact 1 produces
∞
P
n=0
an+2xn+2 −2x
∞
P
n=0
an+1xn+1 + x2
∞
P
n=0
anxn = x2
∞
P
n=0
(2x)n,
[f(x) −a0 −a1x] −2x[f(x) −a0] + x2f(x) =
x2
1−2x,
[f(x) −1 −2x] −2x[f(x) −1] + x2f(x) =
x2
1−2x.
Solving for f(x) gives
f(x) =
1
1−2x =
∞
P
n=0
(2x)n =
∞
P
n=0
2nxn.
Thus an = 2n, n ≥0, is the solution of the given recurrence relation.
3. Fact 2 can be used to solve the system of recurrence relations
an+1 = 2an −bn + 2
bn+1 = −an + 2bn −1
for n ≥0, with a0 = 0 and b0 = 1. Multiplying by xn+1 and summing yields
∞
P
n=0
an+1xn+1 = 2x
∞
P
n=0
anxn −x
∞
P
n=0
bnxn + 2x
∞
P
n=0
xn
∞
P
n=0
bn+1xn+1 = −x
∞
P
n=0
anxn + 2x
∞
P
n=0
bnxn −x
∞
P
n=0
xn.

Section 3.3
RECURRENCE RELATIONS
189
These two equations can be rewritten in terms of the generating functions f(x) =
P∞
n=0 anxn and g(x) = P∞
n=0 bnxn as
f(x) −a0 = 2xf(x) −xg(x) + 2x
1
1−x
g(x) −b0 = −xf(x) + 2xg(x) −x
1
1−x.
Solving this system (with a0 = 0, b0 = 1) produces
f(x) =
x(1−2x)
(1−x)2(1−3x) = −3
4
1
1−x + 1
2
1
(1−x)2 + 1
4
1
(1−3x)
= −3
4
∞
P
n=0
xn + 1
2
∞
P
n=0
 −2
n

xn + 1
4
∞
P
n=0
(3x)n
= −3
4
∞
P
n=0
xn + 1
2
∞
P
n=0
 n+1
n

xn + 1
4
∞
P
n=0
3nxn
and
g(x) =
1−4x+2x2
(1−x)2(1−3x) = 3
4
1
1−x + 1
2
1
(1−x)2 −1
4
1
(1−3x)
= 3
4
∞
P
n=0
xn + 1
2
∞
P
n=0
 n+1
n

xn −1
4
∞
P
n=0
3nxn.
It then follows that
an = −3
4 + 1
2(n + 1) + 1
4(3n),
n ≥0
bn = 3
4 + 1
2(n + 1) −1
4(3n),
n ≥0.
3.3.5
DIVIDE-AND-CONQUER RELATIONS
Certain algorithms proceed by breaking up a given problem into subproblems of nearly
equal size; solutions to these subproblems are then combined to produce a solution to
the original problem. Analysis of such “divide-and-conquer” algorithms results in special
types of recurrence relations that can be solved both exactly and asymptotically.
Deﬁnitions:
The time-complexity function f(n) for an algorithm gives the (maximum) number
of operations required to solve any instance of size n. The function f(n) is monotone
increasing if m < n ⇒f(m) ≤f(n) where m and n are positive integers.
A recursive divide-and-conquer algorithm splits a given problem of size n = bk
into a subproblems of size n
b each. It requires (at most) h(n) operations to create the
subproblems and subsequently combine their solutions.
Let S = Sb be the set of integers {1, b, b2, . . .} and let Z+ be the set of positive integers.
If f(n) and g(n) are functions on Z+, then g dominates f on S, written f ∈O(g)
on S, if there are positive constants A ∈R, k ∈Z+ such that |f(n)| ≤A|g(n)| holds for
all n ∈S with n ≥k.
Facts:
1. The time-complexity function f(n) of a recursive divide-and-conquer algorithm is
deﬁned for n ∈S and satisﬁes the recurrence relation
f(1) = c,
f(n) = af(n/b) + h(n),
for n = bk, k ≥1,

190
Chapter 3
SEQUENCES
where a, b, c ∈Z+ and b ≥2.
2. Solving f(n) = af(n/b) + c, f(1) = c:
• If a = 1: f(n) = c(logb n + 1) for n ∈S. Thus f ∈O(logb n) on S. If, in addition,
f(n) is monotone increasing, then f ∈O(logb n) on Z+.
• If a ≥2: f(n) = c(anlogb a −1)/(a −1) for n ∈S. Thus f ∈O(nlogb a) on S. If,
in addition, f(n) is monotone increasing, then f ∈O(nlogb a) on Z+.
3. Let f(n) be any function satisfying the inequality relations
f(1) ≤c,
f(n) ≤af(n/b) + c,
for n = bk, k ≥1,
where a, b, c ∈Z+ and b ≥2.
• If a = 1:
f ∈O(logb n) on S. If, in addition, f(n) is monotone increasing, then
f ∈O(logb n) on Z+.
• If a ≥2:
f ∈O(nlogb a) on S. If, in addition, f(n) is monotone increasing, then
f ∈O(nlogb a) on Z+.
4. Solving for a monotone increasing f(n) where f(n) = af(n/b) + rnd (n = bk, k ≥1),
f(1) = c, where a, b, c, d ∈Z+, b ≥2, and r is a positive real number:
• If a < bd: f ∈O(nd) on Z+.
• If a = bd: f ∈O(nd logb n) on Z+.
• If a > bd: f ∈O(nlogb a) on Z+.
The same asymptotic results hold if inequalities ≤replace equalities in the given recur-
rence relation.
Examples:
1. If f(n) satisﬁes the recurrence relation f(n) = f( n
2 ) + 3, n ∈S2, f(1) = 3, then
Fact 2 gives f(n) = 3(log2 n + 1). Thus f ∈O(log2 n) on S2.
2. If f(n) satisﬁes the recurrence relation f(n) = 4f( n
3 ) + 7, n ∈S3, f(1) = 7, then
Fact 2 gives f(n) = 7(4nlog3 4 −1)/3. Thus f ∈O(nlog3 4) on S3.
3. Binary search:
The binary search algorithm (§18.2.3) is a recursive procedure to
search for a speciﬁed value in an ordered list of n items. Its complexity function satisﬁes
f(n) = f( n
2 ) + 2, n ∈S2, f(1) = 2. Since the complexity function f(n) is monotone
increasing in the list size n, Fact 2 shows that f ∈O(log2 n).
4. Merge sort: The merge sort algorithm (§18.3.4) is a recursive procedure for sorting
the n elements of a list. It repeatedly divides a given list into two nearly equal sublists,
sorts those sublists, and combines the sorted sublists. Its complexity function satisﬁes
f(n) = 2f( n
2 )+(n−1), n ∈S2, f(1) = 0. Since f(n) is monotone increasing and satisﬁes
the inequality relation f(n) ≤2f( n
2 ) + n, Fact 4 gives f ∈O(n log2 n).
5. Matrix multiplication:
The Strassen algorithm is a recursive procedure for multi-
plying two n × n matrices (see §6.3.3). One version of this algorithm requires seven
multiplications of n
2 × n
2 matrices and 15 additions of n
2 × n
2 matrices. Consequently, its
complexity function satisﬁes f(n) = 7f( n
2 ) + 15n2/4, n ∈S2, f(1) = 1. From the third
part of Fact 4, f ∈O(nlog2 7) on Z+. This algorithm requires approximately O(n2.81)
operations to multiply n × n matrices, compared to O(n3) for the standard method.

Section 3.4
FINITE DIFFERENCES
191
3.4
FINITE DIFFERENCES
The diﬀerence and antidiﬀerence operators are the discrete analogues of ordinary diﬀer-
entiation and antidiﬀerentiation. Diﬀerence methods can be used for curve-ﬁtting and
for solving recurrence relations.
3.4.1
THE DIFFERENCE OPERATOR
The diﬀerence operator plays a role in combinatorial modeling analogous to that of the
derivative operator in continuous analysis.
Deﬁnitions:
Let f : N →R.
The diﬀerence operator ∆f(x) = f(x + 1) −f(x) is the discrete analogue of the
diﬀerentiation operator.
The kth diﬀerence of f is the operator ∆kf(x) = ∆k−1f(x+ 1)−∆k−1f(x), for k ≥1,
with ∆0f = f.
The shift operator E is deﬁned by Ef(x) = f(x + 1).
The harmonic sum Hn = Pn
i=1
1
i is the discrete analogue of the natural logarithm
(§3.1.7).
Note:
Most of the results stated in this subsection are also valid for functions on non-
discrete domains. The functional notation that is used for most of this subsection, instead
of the more usual subscript notation for sequences, makes the results easier to read and
helps underscore the parallels between discrete and ordinary calculus.
Facts:
1. Linearity: ∆(αf + βg) = α∆f + β∆g, for all constants α and β.
2. Product rule:
∆(f(x)g(x)) = (Ef(x))∆g(x) + (∆f(x))g(x). This is analogous to
the derivative formula for the product of functions.
3. ∆mxn = 0, for m > n, and ∆nxn = n!.
4. ∆nf(x) =
nP
k=0
(−1)k n
k

f(x + n −k).
5. f(x + n) =
nP
k=0
 n
k

∆kf(x).
6. Leibniz’s theorem: ∆n(f(x)g(x)) =
nP
k=0
 n
k

∆kf(x)∆n−kg(x + k).
7. Quotient rule: ∆

f(x)
g(x)

= g(x)∆f(x)−f(x)∆g(x)
g(x)g(x+1)
.
8. The shift operator E satisﬁes ∆f = Ef −f, written equivalently as E = 1 + ∆.
9. Enf(x) = f(x + n).
10. The equation ∆C(x) = 0 implies that C is periodic with period 1. Moreover, if the
domain is restricted to the integers (e.g., if C(n) is a sequence), then C is constant.

192
Chapter 3
SEQUENCES
Examples:
1. If f(x) = x3 then ∆f(x) = (x + 1)3 −x3 = 3x2 + 3x + 1.
2. The following table gives formulas for the diﬀerences of some important functions.
In this table, the notation xn refers to the nth falling power of x (§3.4.2).
f(x)
∆f(x)
 x
n

  x
n−1

(x + a)n
n(x + a)n−1
xn
 n
1

xn−1 +
 n
2

xn−2 + · · · + 1
ax
(a −1)ax
Hx
x−1 =
1
x+1
sin x
2 sin( 1
2) cos(x + 1
2)
cos x
−2 sin( 1
2) sin(x + 1
2)
3. ∆2f(x) = f(x + 2) −2f(x + 1) + f(x), from Fact 4.
4. f(x + 3) = f(x) + 3∆f(x) + 3∆2f(x) + ∆3f(x), from Fact 5.
5. The shift operator can be used to ﬁnd the exponential generating function (§3.2.2)
for the sequence {ak}, where ak is a polynomial in variable k of degree n.
∞
P
k=0
akxk
k!
=
∞
P
k=0
Ek(a0)xk
k!
=
 ∞
P
k=0
xkEk
k!

a0
= exEa0 = ex(1+∆)a0 = exex∆a0
= ex 
a0 + x∆a0
1!
+ x2∆2a0
2!
+ · · · + xn∆na0
n!

.
For example, if ak = k2 + 1 then
∞
P
k=0
(k2+1)xk
k!
= ex(1 + x + x2).
3.4.2
CALCULUS OF DIFFERENCES: FALLING AND RISING POWERS
Falling powers provide a natural analogue between the calculus of ﬁnite sums and diﬀer-
ences and the calculus of integrals and derivatives. Stirling numbers provide a means of
expressing ordinary powers in terms of falling powers and vice versa.
Deﬁnitions:
The nth falling power of x, written xn, is the discrete analogue of exponentiation and
is deﬁned by
xn = x(x −1)(x −2) . . . (x −n + 1)
x−n =
1
(x + 1)(x + 2) . . . (x + n)
x0 = 1.
The nth rising power of x, written xn, is deﬁned by
xn = x(x + 1)(x + 2) . . . (x + n −1),
x−n =
1
(x −n)(x −n + 1) . . . (x −1),
x0 = 1.

Section 3.4
FINITE DIFFERENCES
193
Facts:
1. Conversion between falling and rising powers:
xn = (−1)n(−x)n = (x −n + 1)n =
1
(x+1)−n ,
xn = (−1)n(−x)n = (x + n −1)n =
1
(x−1)−n ,
x−n =
1
(x+1)n ,
x−n =
1
(x−1)n .
2. Laws of exponents:
xm+n = xm(x −m)n ,
xm+n = xm(x + m)n .
3. Binomial theorem: (x + y)n =
 n
0

xn +
 n
1

xn−1y1 + · · · +
 n
n

yn .
4. The action of the diﬀerence operator on falling powers is analogous to the action of
the derivative on ordinary powers: ∆xn = nxn−1 .
5. There is no chain rule for diﬀerences, but the binomial theorem implies the rule
∆(x + a)n = n(x + a)n−1 .
6. Newton’s theorem: If f(x) is a polynomial of degree n, then
f(x) =
nP
k=0
∆kf(0)
k!
xk .
This is an analogue of Maclaurin’s theorem.
7. If f(x) = xn then ∆kf(0) =
n
k
	
· k!, where
n
k
	
is a Stirling subset number (§2.5.2).
8. Falling powers can be expressed in terms of ordinary powers using Stirling cycle
numbers (§2.5.2):
xn =
nP
k=1
n
k

(−1)n−kxk.
9. Rising powers can be expressed in terms of ordinary powers using Stirling cycle
numbers (§2.5.2):
xn =
nP
k=1
n
k

xk.
10. Ordinary powers can be expressed in terms of falling or rising powers using Stirling
subset numbers (§2.5.2):
xn =
nP
k=1
n
k
	
xk =
nP
k=1
n
k
	
(−1)n−kxk.
Examples:
1. Fact 8, together with Fact 3 of §2.5.2, gives
x0 = x0,
x1 = x1,
x2 = x2 −x1,
x3 = x3 −3x2 + 2x1,
x4 = x4 −6x3 + 11x2 −6x1.

194
Chapter 3
SEQUENCES
2. Fact 10, together with Fact 4 of §2.5.2, gives
x0 = x0 ,
x1 = x1 ,
x2 = x2 + x1 ,
x3 = x3 + 3x2 + x1 ,
x4 = x4 + 6x3 + 7x2 + x1 .
3.4.3
DIFFERENCE SEQUENCES AND DIFFERENCE TABLES
New sequences can be obtained from a given sequence by repeatedly applying the diﬀer-
ence operator.
Deﬁnitions:
The diﬀerence sequence for the sequence A = { aj | j = 0, 1, . . .} is the sequence
∆A = { aj+1 −aj | j = 0, 1, . . . }.
The kth diﬀerence sequence for f : N →R is given by ∆kf(0), ∆kf(1), ∆kf(2), . . . .
The diﬀerence table for f : N →R is the table Tf whose kth row is the kth diﬀerence
sequence for f. That is, Tf[k, l] = ∆kf(l) = ∆k−1f(l + 1) −∆k−1f(l).
Facts:
1. The leftmost column of a diﬀerence table completely determines the entire table, via
Newton’s theorem (Fact 6, §3.4.2).
2. The diﬀerence table of an nth degree polynomial consists of n + 1 nonzero rows
followed by all zero rows.
Examples:
1. If A = 0, 1, 4, 9, 16, 25, . . . is the sequence of squares of integers, then its diﬀerence
sequence is ∆A = 1, 3, 5, 7, 9, . . .. Observe that ∆(x2) = 2x + 1.
2. The diﬀerence table for x3 is given by
0
1
2
3
4
5
· · ·
∆0x3 = x3
0
0
0
6
24
60
· · ·
∆1x3 = 3x2
0
0
6
18
36
· · ·
∆2x3 = 6x1
0
6
12
18
· · ·
∆3x3 = 6
6
6
6
· · ·
∆4x3 = 0
0
0
· · ·
3. The diﬀerence table for x3 is given by
0
1
2
3
4
5
· · ·
∆0x3 = x3
0
1
8
27
64
125
· · ·
∆1x3 = 3x2 + 3x + 1
1
7
19
37
61
· · ·
∆2x3 = 6x + 6
6
12
18
24
· · ·
∆3x3 = 6
6
6
6
· · ·
∆4x3 = 0
0
0
· · ·

Section 3.4
FINITE DIFFERENCES
195
4. The diﬀerence table for 3x is given by
0
1
2
3
4
5
· · ·
∆03x = 3x
1
3
9
27
81
243
. . .
∆13x = 2 · 3x
2
6
18
54
162
. . .
∆23x = 4 · 3x
4
12
36
108
. . .
∆33x = 8 · 3x
8
24
72
. . .
∆43x = 16 · 3x
16
48
. . .
...
...
∆k3x = 2k3x
5. Application to curve-ﬁtting: Find the polynomial p(x) of smallest degree that passes
through the points (0, 5), (1, 5), (2, 3), (3, 5), (4, 17), (5, 45). The diﬀerence table for the
sequence 5, 5, 3, 5, 17, 45 is
5
5
3
5
17
45
. . .
0
−2
2
12
28
. . .
−2
4
10
16
. . .
6
6
6
. . .
0
0
. . .
Newton’s theorem shows that the polynomial of smallest degree is p(x) = 5 −x2 + x3 =
x3 −4x2 + 3x + 5.
3.4.4
DIFFERENCE EQUATIONS
Diﬀerence equations are analogous to diﬀerential equations and many of the techniques
are as fully developed. Diﬀerence equations provide a way to solve recurrence relations.
Deﬁnitions:
A diﬀerence equation is an equation involving the diﬀerence operator and/or higher-
order diﬀerences of an unknown function.
An antidiﬀerence of the function f is any function g such that ∆g = f. The notation
∆−1f denotes any such function.
Facts:
1. Any recurrence relation (§3.3) can be expressed as a diﬀerence equation, and vice
versa, by using Facts 4 and 5 of §3.4.1.
2. The solution to a recurrence relation can sometimes be easily obtained by converting
it to a diﬀerence equation and applying diﬀerence methods.
Examples:
1. To ﬁnd an antidiﬀerence of 10 · 3x, refer to the table given in Example 2 of §3.4.1:
∆−1(10 · 3x) = 5∆−1(2 · 3x) = 5 · 3x + C. (Also see Fact 5 of §3.5.3.)
2. To ﬁnd an antidiﬀerence of 3x, ﬁrst express x as x1 and then use the table found in
Example 2 of §3.4.1: ∆−13x = 3∆−1x1 = 3
2x2 + C = 3
2x(x −1) + C.

196
Chapter 3
SEQUENCES
3. To ﬁnd an antidiﬀerence of x2, express x2 as x2 + x1 and then use the table found
in Example 2 of §3.4.1: ∆−1x2 = ∆−1(x2 + x1) = ∆−1x2 + ∆−1x1 = 1
3x3 + 1
2x2 + C =
1
3x(x −1)(x −2) + 1
2x(x −1) + C.
4. The following are examples of diﬀerence equations:
∆3f(x) + x4∆2f(x) −f(x) = 0,
∆3f(x) + f(x) = x2.
5. To solve the recurrence relation an+1 = an + 5n, n ≥0, with a0 = 2, ﬁrst note that
∆an = 5n. Thus an = ∆−15n = 1
4(5n) + C. The initial condition a0 = 2 now implies
that an = 1
4(5n + 7).
6. To solve the equation an+1 = (nan + n)/(n + 1), n ≥1, the recurrence relation is
ﬁrst rewritten as (n + 1)an+1 −nan = n, which is equivalent to ∆(nan) = n. Thus
nan = ∆−1n = 1
2n2 + C, which implies that an = 1
2(n −1) + C( 1
n).
7. To solve an = 2an−1 −an−2 + 2n−2 + n −2, n ≥2, with a0 = 4, a1 = 5, the
recurrence relation is rewritten as an+2 −2an+1 + an = 2n + n, n ≥0. Now, by applying
Fact 4 of §3.4.1, the left-hand side may be replaced by ∆2an.
If the antidiﬀerence
operator is applied twice to the resulting diﬀerence equation and the initial conditions
are substituted, the solution obtained is
an = 2n + 1
6n3 + c1n + c2 = 2n + 1
6n(n −1)(n −2) + 3.
3.5
FINITE SUMS AND SUMMATION
Finite sums arise frequently in combinatorial mathematics and in the analysis of running
times of algorithms. There are a few basic rules for transforming sums into possibly more
tractable equivalent forms, and there is a calculus for evaluating these standard forms.
3.5.1
SIGMA NOTATION
A complex form of symbolic representation of discrete sums using the uppercase Greek
letter Σ (sigma) was introduced by Joseph Fourier in 1820 and has evolved into several
variations.
Deﬁnitions:
The sigma expression Pb
i=a f(i) has the value f(a) + f(a + 1) + · · · + f(b −1) + f(b)
if a ≤b (a, b ∈Z), and 0 otherwise. In this expression, i is the index of summation
or summation variable, which ranges from the lower limit a to the upper limit b.
The interval [a, b] is the interval of summation, and f(i) is a term or summand of
the summation.
A sigma expression Sn = Pn
i=0 f(i) is in standardized form if the lower limit is zero
and the upper limit is an integer-valued expression.
A sigma expression P
k∈K g(k) over the set K has as its value the sum of all the
values g(k), where k ∈K.

Section 3.5
FINITE SUMS AND SUMMATION
197
A closed form for a sigma expression with an indeﬁnite number of terms is an algebraic
expression with a ﬁxed number of terms, whose value equals the sum.
A partial sum of the (standardized) sigma expression Sn = Pn
i=0 f(i) is the sigma
expression Sk = Pk
i=0 f(i), where 0 ≤k ≤n.
An iterated sum or multiple sum is an expression with two or more sigmas, as exem-
pliﬁed by the double sum Pd
i=c
Pb
j=a f(i, j). Evaluation proceeds from the innermost
sigma outward.
A lower or upper limit for an inner sum of an iterated sum is dependent if it depends
on an outer variable. Otherwise, that limit is independent.
Examples:
1. The sum f(1) + f(2) + f(3) + f(4) + f(5) may be represented as P5
i=1 f(i).
2. Sometimes the summand is written as an expression, such as P50
n=1(n2 + n), which
means the same as P50
n=1 f(n), where f(n) = n2+n. Brackets or parentheses can be used
to distinguish what is in the summand of such an “anonymous function” from whatever
is written to the immediate right of the sigma expression. They may be omitted when
such a summand is very simple.
3. Sometimes the property deﬁning the indexing set is written as part of the Σ, as in
the expressions P
1≤k≤n ak or P
k∈K bk.
4. The right-hand side of the equation Pn
j=0 xj = xn+1−1
x−1
is a closed form for the sigma
expression on the left-hand side.
5. The operational meaning of the multiple sum with independent limits P3
i=1
P4
j=2
i
j
is ﬁrst to expand the inner sum, obtaining the single sum P3
i=1
 i
2 + i
3 + i
4

. Expansion
of the outer sum then yields
 1
2 + 1
3 + 1
4

+
 2
2 + 2
3 + 2
4

+
 3
2 + 3
3 + 3
4

= 13
2 .
6. The multiple sum with dependent limits P3
i=1
P4
j=i
i
j is evaluated by ﬁrst expanding
the inner sum, obtaining
 1
1 + 1
2 + 1
3 + 1
4

+
 2
2 + 2
3 + 2
4

+
 3
3 + 3
4

= 6.
3.5.2
ELEMENTARY TRANSFORMATION RULES FOR SUMS
Sums can be transformed using a few simple rules. A well-chosen sequence of transfor-
mations often simpliﬁes evaluation.
Facts:
1. Distributivity rule:
P
k∈K
cak = c P
k∈K
ak, for any constant c.
2. Associativity rule:
P
k∈K
(ak + bk) = P
k∈K
ak + P
k∈K
bk.
3. Rearrangement rule:
P
k∈K
ak = P
k∈K
aρ(k), if ρ is a permutation of the integers in K.
4. Telescoping for sequences:
For any sequence { aj | j = 0, 1, . . . },
nP
i=m
(ai+1 −ai) =
an+1 −am.
5. Telescoping for functions: For any function f : N →R,
nP
i=m
∆f(i) = f(n+1)−f(m).

198
Chapter 3
SEQUENCES
6. Perturbation method: Given a standardized sum Sn = Pn
i=0 f(i), form the equation
n
X
i=0
f(i) + f(n + 1) = f(0) +
n+1
X
i=1
f(i) = f(0) +
n
X
i=0
f(i + 1).
Algebraic manipulation often leads to a closed form for Sn.
7. Interchanging independent indices of a double sum: When the lower and upper limits
of the inner variable of a double sum are independent of the outer variable, the order of
summation can be changed, simply by swapping the inner sigma, limits and all, with the
outer sigma. That is,
d
X
i=c
b
X
j=a
f(i, j) =
b
X
j=a
d
X
i=c
f(i, j).
8. Interchanging dependent indices of a double sum:
When either the lower or upper
limit of the inner variable j of a double sum of an expression f(i, j) is dependent on the
outer variable i, the order of summation can still be changed by swapping the inner sum
with the outer sum. However, the limits of the new inner variable i must be written as
functions of the new outer variable j so that the entire set of pairs (i, j) over which f(i, j)
is summed is the same as before. One particular case of interest is the interchange
n
X
i=1
n
X
j=i
f(i, j) =
n
X
j=1
j
X
i=1
f(i, j).
Examples:
1. The following summation can be evaluated using Fact 4 (telescoping for sequences):
n
X
i=1
1
i(i + 1) = −
n
X
i=1
 1
i + 1 −1
i

= 1 −
1
n + 1.
2. Evaluate Sn = Pn
i=0 xi, using the perturbation method.
nP
i=0
xi + xn+1 = x0 +
n+1
P
i=1
xi = 1 + x
n+1
P
i=1
xi−1,
Sn + xn+1 = 1 + x
nP
i=0
xi = 1 + xSn,
giving Sn = xn+1−1
x−1 .
3. Evaluate Sn = Pn
i=0 i2i, using the perturbation method.
nP
i=0
i2i + (n + 1)2n+1 = 0 · 20 +
n+1
P
i=1
i2i =
nP
i=0
(i + 1)2i+1,
Sn + (n + 1)2n+1 = 2
n
X
i=0
i2i + 2
n
X
i=0
2i = 2Sn + 2(2n+1 −1),
giving Sn = (n + 1)2n+1 −2(2n+1 −1) = (n −1)2n+1 + 2.
4. Interchange independent indices of a double sum:
3
X
i=1
4
X
j=2
i
j =
4
X
j=2
3
X
i=1
i
j =
4
X
j=2
1
j + 2
j + 3
j

=
4
X
j=2
6
j = 6
4
X
j=2
1
j = 6
1
2 + 1
3 + 1
4

= 13
2 .

Section 3.5
FINITE SUMS AND SUMMATION
199
5. Interchange dependent indices of a double sum:
3
X
i=1
3
X
j=i
i
j =
3
X
j=1
j
X
i=1
i
j =
3
X
j=1
1
j
j
X
i=1
i = 1
1 · 1 + 1
2 · 3 + 1
3 · 6 = 9
2.
3.5.3
ANTIDIFFERENCES AND SUMMATION FORMULAS
Some standard combinatorial functions analogous to polynomials and exponential func-
tions facilitate the development of a calculus of ﬁnite diﬀerences, analogous to the diﬀer-
ential calculus of continuous mathematics. The fundamental theorem of discrete calculus
is useful in deriving a number of summation formulas.
Deﬁnitions:
An antidiﬀerence of the function f is any function g such that ∆g = f, where ∆is the
diﬀerence operator (§3.4.1). The notation ∆−1f denotes any such function.
The indeﬁnite sum of the function f is the inﬁnite family of all antidiﬀerences of f.
The notation P f(x)δx + c is sometimes used for the indeﬁnite sum to emphasize the
analogy with integration.
Facts:
1. Fundamental theorem of discrete calculus:
b
X
k=a
f(k) = ∆−1f(k)

b+1
a
= ∆−1f(b + 1) −∆−1f(a).
Note: The upper evaluation point is one more than the upper limit of the sum.
2. Linearity: ∆−1(αf + βg) = α∆−1f + β∆−1g, for any constants α and β.
3. Summation by parts:
b
X
i=a
f(i)∆g(i) = f(b + 1)g(b + 1) −f(a)g(a) −
b
X
i=a
g(i + 1)∆f(i).
This result, which generalizes Fact 5 of §3.5.2, is a direct analogue of integration by parts
in continuous analysis.
4. Abel’s transformation:
n
X
k=1
f(k)g(k) = f(n + 1)
n
X
k=1
g(k) −
n
X
k=1

∆f(k)
k
X
r=1
g(r)

.
5. The following table gives the antidiﬀerences of selected functions. In this table, Hx
indicates the harmonic sum (§3.4.1), xn is the nth falling power of x (§3.4.2), and
n
k
	
is a Stirling subset number (§2.5.2).
f(x)
∆−1f(x)
f(x)
∆−1f(x)
 x
n

  x
n+1

(x + a)n
(x+a)n+1
n+1
, n ̸= −1
(x + a)−1
Hx+a
ax
ax
(a−1), a ̸= 1
ax
ax
(a−1), a ̸= 1
xax
ax
(a−1)
 x −
a
a−1

, a ̸= 1
xn
nP
k=1
{n
k}
k+1xk+1
(−1)x
1
2(−1)x+1
sin x
−1
2 sin( 1
2 ) cos(x −1
2)
cos x
1
2 sin( 1
2 ) sin(x −1
2)

200
Chapter 3
SEQUENCES
6. The following table gives ﬁnite sums of selected functions.
summation
formula
summation
formula
Pn
k=1 km
(n+1)m+1
m+1
, m ̸= −1
Pn
k=1 km
m
P
j=1
{m
j }(n+1)j+1
j+1
Pn
k=0 ak
an+1−1
a−1 , a ̸= 1
Pn
k=1 kak
(a−1)(n+1)an+1−an+2+a
(a−1)2
, a ̸= 1
Pn
k=1 sin k
sin( n+1
2
) sin( n
2 )
sin( 1
2 )
Pn
k=1 cos k
cos( n+1
2
) sin( n
2 )
sin( 1
2 )
Examples:
1.
nP
k=1
k3 =
nP
k=1
(k1 + 3k2 + k3) =

k2
2 + k3 + k4
4
 
n+1
1
= n2(n+1)2
4
.
2. To evaluate
nP
k=1
k(k + 2)(k + 3), ﬁrst rewrite its summand:
nP
k=1
k(k + 2)(k + 3) = ∆−1[(k + 1 −1)(k + 2)(k + 3)]

n+1
1
=

∆−1(k + 3)3 −∆−1(k + 3)2
n+1
1
=
h
(k+3)4
4
−(k+3)3
3
i 
n+1
1
= (n+4)4
4
−(n+4)3
3
+ 2
= (n+4)(n+3)(n+2)(3n−1)+24
12
.
3.
nP
k=1
k3k = ∆−1(k3k)

n+1
1
= 3k  k
2 −3
4
 
n+1
1
= (2n−1)3n+1+3
4
.
4. Summation by parts can be used to calculate Pn
j=0 jxj, using f(j) = j and ∆g(j) =
xj. Thus g(j) = xj/(x −1), and Fact 3 yields
nP
j=0
jxj = (n+1)xn+1
(x−1)
−0 −
nP
j=0
xj+1
(x−1) = (n+1)xn+1
(x−1)
−
x
x−1
nP
j=0
xj
= (n+1)xn+1
(x−1)
−
x
x−1
xn+1−1
(x−1) = (n+1)(x−1)xn+1−xn+2+x
(x−1)2
.
5. Summation by parts also yields an antiderivative of x3x:
∆−1(x3x) = ∆−1  x∆( 1
2 · 3x)

= 1
2x3x −∆−1( 1
2 · 3x+1 · 1) = 3x   x
2 −3
4

.
3.5.4
STANDARD SUMS
Many useful summation formulas are derivable by combinations of elementary manipu-
lation and ﬁnite calculus. Such sums can be expressed in various ways, using diﬀerent
combinatorial coeﬃcients. (See §3.1.8.)
Deﬁnition:
The power sum Sk(n) = Pn
j=1 jk = 1k +2k +3k +· · ·+nk is the sum of the kth powers
of the ﬁrst n positive integers.

Section 3.5
FINITE SUMS AND SUMMATION
201
Facts:
1. Sk(n) is a polynomial in n of degree k + 1 with leading coeﬃcient 1 k+1. The contin-
uous analogue of this fact is the familiar
R b
a xkdx =
1
k+1(bk+1 −ak+1).
2. The power sum Sk(n) can be expressed using the Bernoulli polynomials (§3.1.4) as
Sk(n) =
1
k + 1[Bk+1(n + 1) −Bk+1(0)].
3. When Sk(n) is expressed in terms of binomial coeﬃcients with the second entry ﬁxed
at k + 1, the coeﬃcients are the Eulerian numbers (§3.1.5).
Sk(n) =
k−1
X
i=0
E(k, i)
 n+i+1
k+1

.
4. When Sk(n) is expressed in terms of binomial coeﬃcients with the ﬁrst entry ﬁxed
at n + 1, the coeﬃcients are products of factorials and Stirling subset numbers (§2.5.2).
Sk(n) =
k
X
i=1
i!
k
i
	 n+1
i+1

.
5. Formulas for the power sums described in Facts 1, 3, and 4 are given in the next
three tables, for small values of k.
summation
formula
Pn
j=1 j
1
2n(n + 1)
Pn
j=1 j2
1
6n(n + 1)(2n + 1)
Pn
j=1 j3
1
4n2(n + 1)2
Pn
j=1 j4
1
30n(n + 1)(2n + 1)(3n2 + 3n −1)
Pn
j=1 j5
1
12n2(n + 1)2(2n2 + 2n −1)
Pn
j=1 j6
1
42n(n + 1)(2n + 1)(3n4 + 6n3 −n2 −3n + 1)
Pn
j=1 j7
1
24n2(n + 1)2(3n4 + 6n3 −n2 −4n + 2)
Pn
j=1 j8
1
90n(n + 1)(2n + 1)(5n6 + 15n5 + 5n4 −15n3 −n2 + 9n −3)
Pn
j=1 j9
1
20n2(n + 1)2(2n6 + 6n5 + n4 −8n3 + n2 + 6n −3)
summation
formula
Pn
j=1 j
 n+1
2

Pn
j=1 j2
 n+1
3

+
 n+2
3

Pn
j=1 j3
 n+1
4

+ 4
 n+2
4

+
 n+3
4

Pn
j=1 j4
 n+1
5

+ 11
 n+2
5

+ 11
 n+3
5

+
 n+4
5

Pn
j=1 j5
 n+1
6

+ 26
 n+2
6

+ 66
 n+3
6

+ 26
 n+4
6

+
 n+5
6

summation
formula
Pn
j=1 j
 n+1
2

Pn
j=1 j2
 n+1
2

+ 2
 n+1
3

Pn
j=1 j3
 n+1
2

+ 6
 n+1
3

+ 6
 n+1
4

Pn
j=1 j4
 n+1
2

+ 14
 n+1
3

+ 36
 n+1
4

+ 24
 n+1
5

Pn
j=1 j5
 n+1
2

+ 30
 n+1
3

+ 150
 n+1
4

+ 240
 n+1
5

+ 120
 n+1
6


202
Chapter 3
SEQUENCES
Examples:
1. To ﬁnd the third power sum S3(n) = Pn
j=1 j3 via Fact 2, use the Bernoulli polynomial
B4(x) = x4 −2x3 + x2 −1
30 from the table found in §3.1.4. Thus
S3(n) = 1
4

B4(x)

n+1
0
= (n+1)4−2(n+1)3+(n+1)2
4
= n2(n+1)2
4
.
2. Power sums can be found using antidiﬀerences and Stirling numbers of both types.
For example, to ﬁnd S3(n) = Pn
x=1 x3 ﬁrst compute
∆−1x3 = ∆−1  3
1
	
x1 +
3
2
	
x2 +
3
3
	
x3
= x2
2 + x3 + x4
4 .
Each term xm is then expressed in terms of ordinary powers of x
x2 =
2
2

x2 −
2
1

x1 = x2 −x,
x3 =
3
3

x3 −
3
2

x2 +
3
1

x1 = x3 −3x2 + 2x,
x4 =
4
4

x4 −
4
3

x3 +
4
2

x2 −
4
1

x1 = x4 −6x3 + 11x2 −6x,
so ∆−1x3 = 1
2(x2 −x) + (x3 −3x2 + 2x) + 1
4(x4 −6x3 + 11x2 −6x) = 1
4(x4 −2x3 + x2).
Evaluating this antidiﬀerence between the limits x = 1 and x = n + 1 gives S3(n) =
1
4n2(n + 1)2. See §3.5.3, Fact 1.
3.6
ASYMPTOTICS OF SEQUENCES
An exact formula for the terms of a sequence may be unwieldy. For example, it is diﬃcult
to estimate the magnitude of the central binomial coeﬃcient
 2n
n

= (2n)!
(n!)2 from the deﬁni-
tion of the factorial function alone. On the other hand, Stirling’s approximation formula
(§3.6.2) leads to the asymptotic estimate
4n
√πn. In applying asymptotic analysis, various
“rules of thumb” help bypass tedious derivations. In practice, these rules almost always
lead to correct results that can be proved by more rigorous methods. In the following
discussions of asymptotic properties, the parameter tending to inﬁnity is denoted by n.
Both the subscripted notation an and the functional notation f(n) are used to denote
a sequence. The notation f(n) ∼g(n) (f is asymptotic to g) means that f(n) ̸= 0 for
suﬃciently large n and limn→∞
g(n)
f(n) = 1.
3.6.1
APPROXIMATE SOLUTIONS TO RECURRENCES
Although recurrences are a natural source of sequences, they often yield only crude
asymptotic information. As a general rule, it helps to derive a summation or a generating
function from the recurrence before obtaining asymptotic estimates.
Facts:
1. Rule of thumb: Suppose a recurrence for some sequence an can be transformed into
a recurrence for a related sequence bn, so that the transformed sequence is approximately
homogeneous and linear with constant coeﬃcients (§3.3). Let ρ be the largest positive
root of the characteristic equation for the homogeneous constant coeﬃcient recurrence.
Then it is probably true that bn+1
bn
∼ρ; i.e., bn grows roughly like ρn.

Section 3.6
ASYMPTOTICS OF SEQUENCES
203
2. Nonlinear recurrences are not covered by Fact 1.
3. Recurrences without ﬁxed degree such as divide-and conquer recurrences (§3.3.5), in
which the diﬀerence between the largest and smallest subscripts is unbounded, are not
covered by Fact 1. See [GrKn07] for appropriate techniques.
Examples:
1. Consider the recurrence Dn+1 = n(Dn +Dn−1) for n ≥1, and deﬁne dn = Dn
n! . Then
dn+1 =
n
n+1dn +
1
n+1dn−1, which is quite close to the constant coeﬃcient recurrence
ˆdn+1 = ˆdn. Since the characteristic root for this latter approximate recurrence is ρ = 1,
Fact 1 suggests that dn+1
dn
∼1, which implies that dn is close to constant. Thus, we
expect the original variable Dn to grow like n!.
Indeed, if the initial conditions are
D0 = D1 = 1, then Dn = n!. With initial conditions D0 = 1, D1 = 0, then Dn is the
number of derangements of n objects (§2.4.2), in which case Dn is the closest integer to
n!
e for n ≥1.
2. The accuracy of Example 1 is unusual. By way of contrast, the number In of invo-
lutions of an n-set (§2.8.1) satisﬁes the recurrence In+1 = In + nIn−1 for n ≥1 with
I0 = I1 = 1. By deﬁning in = In/(n!)1/2, then
in+1 =
in
(n + 1)1/2 +
in−1
(1 + 1/n)1/2 ,
which is nearly the same as the constant coeﬃcient recurrence ˆin+1 = ˆin−1. The charac-
teristic equation ρ2 = 1 has roots ±1, so Fact 1 suggests that in is nearly constant and
hence that In grows like
√
n!. The approximation in this case is not so good, because
In/
√
n! ∼e
√n/(8πen)1/4, which is not a constant.
3.6.2
ANALYTIC METHODS FOR DERIVING ASYMPTOTIC ESTIMATES
Concepts and methods from continuous mathematics can be useful in analyzing the
asymptotic behavior of sequences. The notion of a generating function (§3.2) will play
an important role here.
Deﬁnitions:
The radius of convergence of the series P anxn is the number r such that the series
converges for all |x| < r and diverges for all |x| > r, where 0 ≤r ≤∞.
Given a series A(x) = P anxn, we write [xn]A(x) = an.
The gamma function is the function Γ(x) =
R ∞
0
tx−1e−t dt.
Facts:
1. Stirling’s approximation: n! ∼
√
2πn( n
e )n.
2. Γ(x + 1) = xΓ(x), Γ(n + 1) = n!, and Γ( 1
2) = √π.
3. The radius of convergence of P anxn is given by 1
r = lim supn→∞|an|1/n.
4. From Fact 3, it follows that |an| tends to behave like r−n. Most analytic methods
are reﬁnements of this idea: while the absolute value of the radius of convergence gives
the exponential growth of the coeﬃcients, the nature of the point where the series ceases
to be analytic gives the subexponential growth of the coeﬃcients. See Fact 7.

204
Chapter 3
SEQUENCES
5. The behavior of f(z) near singularities on its circle of convergence determines the
dominant asymptotic behavior of the coeﬃcients of f. Estimates are often based on
Cauchy’s integral formula: an =
H
f(z)z−n−1 dz.
6. Rule of thumb:
Consider the set of values of x for which f(x) = P anxn is either
inﬁnite or undeﬁned, or involves computing a nonintegral power of 0. The absolute value
of the least such x is normally the radius of convergence of f(x). If there is no such x,
then r = ∞.
7. Rule of thumb: As seen in Fact 4, the modulus of the radius of convergence of
f(x) = P anxn determines the exponential growth of an. Additionally, the nature of
the function at the radius of convergence determines the subexponential growth of the
coeﬃcients.
8. Rule of thumb: A very general example of Fact 7 is the following. Suppose that 0 <
r < ∞is the radius of convergence of f(x), that g(x) has a larger radius of convergence,
and that
f(x) −g(x) ∼C

−ln(1 −x
r )
b 
1 −x
r
c
as x →r−
for some constants C, b, and c, where it is not the case that both b = 0 and c is a
nonnegative integer. (Often g(x) = 0.) Then it is probably true that
[xn]f(x) ∼
(
C
 n−c−1
n

(ln n)br−n,
if c ̸= 0,
Cb (ln n)b−1
n
r−n,
if c = 0.
9. Rule of thumb: Let a(x) = d ln f(x)
d ln x
and b(x) = da(x)
d ln x . Suppose that a(rn) = n has a
solution with 0 < rn < r and that b(rn) ∈o(n2). Then it is probably true that
an ∼f(rn)r−n
n
p
2πb(rn)
.
Examples:
1. The number Dn of derangements (§2.4.2) has the exponential generating function
f(x) = P Dn xn
n! = e−x
1−x. Since evaluation for x = 1 involves division by 0, it follows that
r = 1. Since e−x
1−x ∼e−1
1−x as x →1−, take g(x) = 0, C = e−1, b = 0, and c = −1. Fact 8
suggests that Dn ∼n!
e , which is correct.
2. The number bn of left-right binary n-leaved trees has the generating function f(x) =
1
2
 1 −√1 −4x

.
(See §9.3.3.)
In this case r =
1
4 since f( 1
4) requires computing a
fractional power of 0. Take g(x) = 1
2, C = 1
2, b = 0, and c = 1
2 to suspect from Fact 8
that
bn ∼−1
2
n −3
2
n

4n =
−Γ(n −1
2)4n
2Γ(n + 1)Γ(−1
2) ∼4n−1
√
πn3 ,
which is valid. (Facts 1 and 2 have also been used.) This estimate converges rather
rapidly—by the time n = 40, the estimate is less than 0.1% below b40.
3. Since P xn
n! = ex, n! can be estimated by taking a(x) = b(x) = x and rn = n in
Fact 9. This gives 1
n! ∼enn−n
√
2πn , which is Stirling’s asymptotic formula.
4. The number Bn of partitions of an n-set (§2.5.2) satisﬁes P Bn xn
n! = exp(ex −1).
In this case, r = ∞. Since a(x) = xex and b(x) = x(x + 1)ex, it follows that rn is the
solution to rn exp(rn) = n and that b(rn) = (rn + 1)n ∼nrn ∈o(n2). Fact 9 suggests
Bn ∼n! exp(ern −1)
rnn
√2πnrn
= n! exp(n/rn −1)
rnn
√2πnrn
.

Section 3.6
ASYMPTOTICS OF SEQUENCES
205
This estimate is correct, though the estimate converges quite slowly, as shown in the
following table.
n
10
20
100
200
estimate
1.49 × 105
6.33 × 1013
5.44 × 10115
7.01 × 10275
Bn
1.16 × 105
5.17 × 1013
4.76 × 10115
6.25 × 10275
ratio
1.29
1.22
1.14
1.12
Improved asymptotic estimates exist.
5. Analytic methods can sometimes be used to obtain asymptotic results when only
a functional equation is available (see also §3.6.3.) For example, if an is the number
of n-leaved rooted trees in which each non-leaf node has exactly two children (with
left and right not distinguished), the generating function for an satisﬁes f(x) = x +
 f(x)2 + f(x2)

/2, from which it can be deduced that an ∼Cn−3/2r−n, where r =
0.4026975 . . . and C = 0.31877 . . . can easily be computed to any desired degree of accu-
racy. See [BeWi06] for more information.
3.6.3
GENERATING FUNCTION SCHEMES
Speciﬁc general expressions deﬁning a given generating function can be applied to auto-
matically give asymptotic results for their coeﬃcients.
Facts:
1. Rule of thumb: Assume that the generating function A(x) = P anxn satisﬁes the
equation A(x) = xφ(A(x)), where φ(t) is a nonlinear function, analytic at 0 with φ(0) ̸= 0
and [tn]φ(t) ≥0 for all values of n > 0. This is the so-called Lagrange scheme. Then, by
means of the Lagrange inversion formula, we have
an = 1
n[tn−1]φ(t)n.
Sometimes this exact formula is useless, and one can exploit analytic methods to provide
asymptotic estimates for an. More precisely, if R is the radius of convergence of φ(t) and
there exists a solution τ to the equation φ(τ) = τφ′(τ) with 0 < τ < R, then
an ∼
s
φ(τ)
φ′′(τ)
1
√
2πn3
φ(τ)
τ
n
.
2. Rule of thumb: Systems of functional equations have the same asymptotic form as in
Fact 1. Let A1(x), . . . , Ak(x) satisfy a functional system of equations of the form A1(x) =
F1(x, A1(x), . . . , Ak(x)), . . . , Ak(x) = Fk(x, A1(x), . . . , Ak(x)), where the functions Fi
are nonlinear. Additionally, the system is irreducible, namely one cannot simplify it
by eliminating equations. Then, under certain technical but general conditions for the
functions Fi it is probably true that, for some constants Ci, the exponential growth of
the coeﬃcients is of the form
[xn]Ai(x) ∼Ci
1
√
n3 ρ−n.
The constant ρ can be computed analytically as a solution of a system of equations. See
[Dr97] for details.

206
Chapter 3
SEQUENCES
Examples:
1. The Catalan numbers Cn (see §3.1.3) are encoded by the generating function C(x),
which satisﬁes the functional equation C(x) = 1 + xC(x)2. Writing C(x) −1 = U(x) we
obtain U(x) = x(1 + U(x))2. Then φ(t) = (1 + t)2, and Fact 1 applies directly. This
yields τ = 1 and φ(τ)/τ = 4. Hence, by Fact 1
[xn]U(x) = [xn]C(x) =
1
n + 1
2n
n

∼
1
√
πn3 4n.
Compare this result with Example 2 of §3.6.2. Observe that the asymptotic estimate
can be obtained either by applying Stirling’s formula to the Catalan number (expressed
using a binomial coeﬃcient), or directly by using Fact 1.
2. Consider the family of rooted trees where each internal vertex has outdegree equal
to 2, 3 or 4. Then the generating function T (x) associated with this family satisﬁes the
equation T (x) = x(1+T (x)2+T (x)3+T (x)4), where x encodes vertices (see [FlSe09]). In
this case, φ(t) = (1 + t2 + t3 + t4) and the positive solution to the equation φ(τ) = τφ′(τ)
is given by τ = 0.56773 . . .. Finally, the growth constant is φ(τ)/τ = 2.83443 . . ..
3.6.4
ASYMPTOTIC ESTIMATES OF MULTIPLY-INDEXED SEQUENCES
Asymptotic estimates for multiply-indexed sequences are considerably more diﬃcult to
obtain. To begin with, the meaning of a formula such as
n
k

∼
2n
p
πn/2
exp

−(n −2k)2
2n

must be carefully stated, because both n and k are tending to ∞, and the formula is
valid only when this happens in such a way that |2k −n| ∈o(n3/4).
Facts:
1. Most estimates of multiple summations are based on summing over one index at a
time.
2. Some analytic results are available in the research literature [Od95]. In particular,
Drmota [Dr97] shows that Fact 2 in §3.6.3 can be extended in some situations in order
to deal with multiply-indexed sequences.
3. Very recently, some tools arising from multidimensional complex analysis have been
exploited to study such questions [PeWi13].
Example:
1. Consider the number of rooted plane trees, expressed in terms of the number of
vertices n. This sequence is given by the Catalan numbers. We consider also the number
of leaves r.
Denote by tn,r the number of rooted plane trees with n vertices and r
leaves, and write T (x, u) = P tn,rxnur. Then it can be shown that T (x, u) satisﬁes the
functional equation
T (x, u) = xu +
xT (x, u)
1 −T (x, u).
Application of the Lagrange inversion formula (see Fact 1 in §3.6.3) gives
tn,r =
1
n −1
n −1
n −r
n −1
r

.

Section 3.7
MECHANICAL SUMMATION PROCEDURES
207
Using Stirling’s approximation formula we get
tn,r ∼
1
πn2 22n−1 exp

−(n −2k)2
n

.
Such an estimate can be obtained as well by means of the tools of [Dr97] using complex
analytic techniques.
3.7
MECHANICAL SUMMATION PROCEDURES
This section describes mechanical procedures that have been developed to evaluate sums
of terms involving binomial coeﬃcients and related factors. These procedures can not
only be used to ﬁnd explicit formulas for many sums, but can also be used to show
that no simple closed formulas exist for certain sums. The invention of these mechanical
procedures has been a surprising development in combinatorics. The material presented
here is mostly adapted from [PeWiZe96], a comprehensive source for material on this
topic.
3.7.1
HYPERGEOMETRIC SERIES
Deﬁnitions:
A geometric series is a series of the form P∞
k=0 ak where the ratio between two con-
secutive terms is a constant, i.e., where the ratio ak+1
ak
is a constant for all k = 0, 1, 2, . . ..
A hypergeometric series is a series of the form P∞
k=0 tk where t0 = 1 and the ratio
of two consecutive terms is a rational function of the summation index k, i.e., the ratio
tk+1
tk
=
P (k)
Q(k) where P(k) and Q(k) are polynomials in the integer k. The terms of a
hypergeometric series are called hypergeometric terms.
When the numerator P(k) and denominator Q(k) of this ratio are completely factored
to give
P(k)
Q(k) =
(k + a1)(k + a2) . . . (k + ap)
(k + b1)(k + b2) . . . (k + bq)(k + 1) ,
where x is a constant, this hypergeometric series is denoted by
pFq =
ha1
a2
. . .
ap
b1
b2
. . .
bq
; x
i
.
Note:
If there is no factor k + 1 in the denominator Q(k) when it is factored, by
convention the factor k + 1 is added to both the numerator P(k) and denominator Q(k).
Also, a horizontal dash is used to indicate the absence of factors in the numerator or in
the denominator.
The hypergeometric terms sn and tn are similar, denoted sn ∼tn, if their ratio sn/tn
is a rational function of n. Otherwise, these terms are called dissimilar.

208
Chapter 3
SEQUENCES
Facts:
1. A geometric series is also a hypergeometric series.
2. If sn is a hypergeometric term, then
1
sn is also a hypergeometric term. (Equivalently,
if P∞
k=0 sn is a hypergeometric series, then P∞
k=0
1
sn also is.)
3. In common usage, instead of stating that the series P∞
k=0 sn is a hypergeometric
series, it is stated that sn is a hypergeometric term. This means exactly the same thing.
4. If sn and tn are hypergeometric terms, then sntn is a hypergeometric term. (Equiv-
alently, if P∞
k=0 sn and P∞
k=0 tn are hypergeometric series, then P∞
k=0 sntn is a hyper-
geometric series.)
5. If sn is a hypergeometric term and sn is not a constant, then sn+1 −sn is a hyper-
geometric term similar to sn.
6. If sn and tn are hypergeometric terms and sn + tn ̸= 0 for all n, then sn + tn is
hypergeometric if and only if sn and tn are similar.
7. If t(1)
n , t(2)
n , . . . , t(k)
n
are hypergeometric terms with Pk
i=1 t(i)
n = 0, then t(i)
n ∼t(j)
n
for
some i and j with 1 ≤i < j ≤k.
8. A sum of a ﬁxed number of hypergeometric terms can be expressed as a sum of
pairwise dissimilar hypergeometric terms.
9. The terms of a hypergeometric series can be expressed using rising powers an (also
known as rising factorials and denoted by (a)n) (see §3.4.2) as follows:
pFq =
ha1
a2
. . .
ap
b1
b2
. . .
bq
; x
i
=
∞
X
k=0
(a1)k(a2)k . . . (ap)k
(b1)k(b2)k . . . (bq)k
xk
k! .
10. There are a large number of well-known hypergeometric identities (see Facts 12–
17, for example) that can be used as a starting point when a closed form for a sum of
hypergeometric terms is sought.
11. There are many rules that transform a hypergeometric series with one parameter set
into a diﬀerent hypergeometric series with a second parameter set. Such transformation
rules can be helpful in constructing closed forms for sums of hypergeometric terms.
12.
1F1
h1
1; x
i
= ex.
13.
1F0
ha
−; x
i
=
1
(1 −x)a .
14. Gauss’s 2F1 identity: If b is zero or a negative integer or the real part of c −a −b
is positive, then
2F1
ha
b
c
; 1
i
= Γ(c −a −b)Γ(c)
Γ(c −a)Γ(c −b)
where Γ is the gamma function (so Γ(n) = (n −1)! when n is a positive integer).
15. Kummer’s 2F1 identity: If a −b + c = 1, then
2F1
ha
b
c
; −1
i
= Γ( b
2 + 1)Γ(b −a + 1)
Γ(b + 1)Γ( b
2 −a + 1)
and when b is a negative integer, this can be expressed as
2F1
ha
b
c
; −1
i
= 2 cos( πb
2 ) Γ(|b|)Γ(b −a + 1)
Γ( |b|
2 )Γ( b
2 −a + 1)
.

Section 3.7
MECHANICAL SUMMATION PROCEDURES
209
16. Saalsch¨utz’s 3F2 identity: If d + e = a + b + c + 1 and c is a negative integer, then
3F2
ha
b
c
d
e
; 1
i
= (d −a)|c|(d −b)|c|
d|c|(d −a −b)|c| .
17. Dixon’s identity: If 1 + a
2 −b −c > 0, d = a −b + 1, and e = a −c + 1, then
3F2
ha
b
c
d
e
; 1
i
= ( a
2)!(a −b)!(a −c)!( a
2 −b −c)!
a!( a
2 −b)!( a
2 −c)!(a −b −c)! .
The more familiar form of this identity reads
X
k
(−1)ka + b
a + k
a + c
c + k
b + c
b + k

= (a + b + c)!
a!b!c!
.
18. Clausen’s 4F3 identity:
If d is a negative integer or zero and a + b + c −d = 1
2,
e = a + b + 1
2, and a + f = d + 1 = b + g, then
4F3
ha
b
c
d
e
f
g
; 1
i
= (2a)|d|(a + b)|d|(2b)|d|
(2a + 2b)|d|(a)|d|(b)|d| .
Examples:
1. The series P∞
k=0 3·(−5)k is a geometric series. The series P∞
k=0 n2n is not a geometric
series.
2. The series P∞
k=0 tk is a hypergeometric series when tk equals 2k, (k + 1)2,
1
2k+3, or
1
(2k+1)(k+3)!, but is not hypergeometric when tk = 2k + 1.
3. The series P∞
k=0
3k
k!4 equals 0F3
h −
1 1 1; 3
i
since the ratio of the (k+1)st and kth terms
is
3
(k+1)4 .
4. A closed form for Sn = P∞
k=0(−1)k 2n
k
2 can be found by ﬁrst noting that Sn =
2F1
h−2n
−2n
1
; −1
i
since the ratio between successive terms of the sum is −(k−2n)2
(k+1)2 .
This shows that Kummer’s 2F1 identity can be invoked with a = −2n, b = −2n, and
c = 1, producing the equality Sn = 2(−1)n(2n−1)!
n!(n−1)!
= (−1)n 2n
n

.
5. An example of a transformation rule for hypergeometric functions is provided by
2F1
ha
b
c
; x
i
= (1 −x)c−a−b
2F1
hc −a
c −b
c
; x
i
.
3.7.2
ALGORITHMS TO PRODUCE CLOSED FORMS FOR SUMS OF HYPERGEOMETRIC
TERMS
Deﬁnitions:
A function F(n, k) is called doubly hypergeometric if both F (n+1,k)
F (n,k)
and F (n,k+1)
F (n,k)
are
rational functions of n and k.

210
Chapter 3
SEQUENCES
A function F(n, k) is a proper hypergeometric term if it can be expressed as
F(n, k) = P(n, k)
QG
i=1(ain + bik + ci)!
QH
i=1(uin + vik + wi)!
xk
where x is a variable, P(n, k) is a polynomial in n and k, G and H are nonnegative
integers, and all the coeﬃcients ai, bi, ui, and vi are integers.
A function F(n, k) of the form
F(n, k) = P(n, k)
GQ
i=1
(ain + bik + ci)!
HQ
i=1
(uin + vik + wi)!
xk
is said to be well-deﬁned at (n, k) if none of the terms (ain + bik + ci) in the product is
a negative integer. The function F(n, k) is deﬁned to have the value 0 if F is well-deﬁned
at (n, k) and there is a term (uin + vik + wi) in the product that is a negative integer or
P(n, k) = 0.
Facts:
1. If F(n, k) is a proper hypergeometric term, then there exist positive integers L and M
and polynomials ai,j(n) for i = 0, 1, . . . , L and j = 0, 1, . . ., M, not all zero, such that
L
X
i=0
M
X
j=0
ai,j(n)F(n −j, k −i) = 0
for all pairs (n, k) with F(n, k) ̸= 0 and all the values of F(n, k) in this double sum are
well-deﬁned. Moreover, there is such a recurrence with M equal to M ′ = P
s |bs|+P
t |vt|
and L equal to L′ = deg(P) + 1 + M ′(−1 + P
s |as| + P
t |ut|), where the ai, bi, ui, vi
and P come from an expression of F(n, k) as a hypergeometric term as speciﬁed in the
deﬁnition.
2. Sister Celine’s algorithm: This algorithm, developed in 1945 by Sister Mary Celine
Fasenmyer (1906–1996), can be used to ﬁnd recurrence relations for sums of the form
f(n) = P
k F(n, k) where F is a doubly hypergeometric function. The algorithm ﬁnds a
recurrence of the form PL
i=0
PM
j=0 ai,j(n)F(n −j, k −i) = 0 by proceeding as follows:
• Start with trial values of L and M, such as L = 1, M = 1.
• Assume that a recurrence relation of the type sought exists with these values of L
and M, with the coeﬃcients ai,j(n) to be determined, if possible.
• Divide each term in the sum of the recurrence by F(n, k), then reduce each frac-
tion F(n −j, k −i)/F(n, k), simplifying the ratios of factorials so only rational
functions of n and k are left.
• Combine the terms in the sum using a common denominator, collecting the nu-
merator into a single polynomial in k.
• Solve the system of linear equations for the ai,j(n) that results when the coeﬃ-
cients of each power of k in the numerator polynomial are equated to zero.
• If these steps fail, repeat the procedure with larger values of L and M; by Fact 2,
this procedure is guaranteed to eventually work.

Section 3.7
MECHANICAL SUMMATION PROCEDURES
211
3. Gosper’s algorithm: This algorithm, developed by R. W. Gosper, Jr., can be used to
determine, given a hypergeometric term tn, whether there is a hypergeometric term zn
such that zn+1 −zn = tn. When there is such a hypergeometric term zn, the algorithm
also produces such a term.
4. Gosper’s algorithm takes a hypergeometric term tn as input and performs the follow-
ing general steps (for details see [PeWiZe96]):
• Let r(n) = tn+1/tn; this is a rational function of n since t is hypergeometric.
• Find polynomials a(n), b(n), and c(n) such that gcd(a(n), b(n+h)) = 1 whenever h
is a nonnegative integer; this is done using the following steps:
⋄Let r(n) = K · f(n)
g(n) where f(n) and g(n) are monic relatively prime polyno-
mials and K is a constant, let R(h) be the resultant of f(n) and g(n + h)
(which is the product of the zeros of g(n + h) at the zeros of f(n)), and let
S = {h1, h2, . . . , hN} be the set of nonnegative integer zeros of R(h) where
0 ≤h1 < h2 < · · · < hN.
⋄Let p0(n) = f(n) and q0(n) = g(n); then for j = 1, 2, . . . , N carry out the
following steps:
sj(n) := gcd(pj−1(n), qj−1(n + hj));
pj(n) := pj−1(n)/sj(n);
qj(n) := qj−1(n)/sj(n −hj).
• Take a(n) := KpN(n); b(n) := qN(n); c(n) := QN
i=1
Qhi
j=1 si(n −j).
• Find a nonzero polynomial x(n) such that a(n)x(n + 1) −b(n −1)x(n) = c(n) if
one exists; such a polynomial can be found using the method of undetermined
coeﬃcients to ﬁnd a nonzero polynomial of degree d or less, where the degree d
depends on the polynomials a(n), b(n), and c(n). If no such polynomial exists,
then the algorithm fails. The degree d is determined by the following rules:
⋄When deg a(n) ̸= deg b(n) or deg a(n) = deg b(n) but the leading coeﬃcients
of a(n) and b(n) diﬀer, then d = deg c(n) −max(deg a(n), deg b(n)).
⋄When deg a(n) = deg b(n) and the leading coeﬃcients of a(n) and b(n) agree,
d = max(deg c(n)−deg a(n)+1, (B−A)/L) where a(n) = Lnk+Ank−1+· · ·
and b(n −1) = Lnk + Bnk−1 + · · · ; if this d is negative, then no such
polynomial x(n) exists.
• Let zn = tn · b(n −1)x(n)/c(n); it follows that zn+1 −zn = tn.
5. When Gosper’s algorithm fails, this shows that a sum of hypergeometric terms cannot
be expressed as a hypergeometric term plus a constant.
6. Programs in both Maple and Mathematica implementing algorithms described in this
section can be found at the following sites:
• http://www.math.upenn.edu/~wilf/AeqB.html
• http://www.math.rutgers.edu/~zeilberg/programs.html
Examples:
1. The function F(n, k) =
1
5n+2k+2 is a proper hypergeometric term since F(n, k) can
be expressed as F(n, k) = (5n+2k+1)!
(5n+2k+2)!.
2. The function F(n, k) =
1
n2+k3+5 is not a proper hypergeometric term.
3. Sister Celine’s algorithm can be used to ﬁnd a recurrence relation satisﬁed by the
function f(n) = P
k F(n, k) where F(n, k) = k
 n
k

for n = 0, 1, 2, . . .. The algorithm
proceeds by ﬁnding a recurrence relation of the form a(n)F(n, k) + b(n)F(n + 1, k) +

212
Chapter 3
SEQUENCES
c(n)F(n, k + 1) + d(n)F(n + 1, k + 1) = 0. Since F(n, k) = k
 n
k

, this recurrence relation
simpliﬁes to a(n) + b(n) ·
n+1
n+1−k + c(n) · n−k
k
+ d(n) · n+1
k
= 0. Putting the left-hand side
of this equation over a common denominator and expressing it as a polynomial in k, four
equations in the unknowns a(n), b(n), c(n), and d(n) are produced. These equations have
the following solutions: a(n) = t(−1 −1
n), b(n) = 0, c(n) = t(−1 −1
n), d = t, where t is a
constant. This produces the recurrence relation (−1−1
n)F(n, k)+(−1−1
n)F(n, k +1)+
F(n + 1, k + 1) = 0, which can be summed over all integers k and simpliﬁed to produce
the recurrence relation f(n + 1) = 2 · n+1
n f(n), with f(1) = 1. From this it follows that
f(n) = n2n−1.
4. As shown in [PeWiZe96], Sister Celine’s algorithm can be used to ﬁnd an identity for
f(n) = P
k F(n, k) where F(n, k) =
 n
k
 2n
k

(−2)n−k. A recurrence for F(n, k) can be
found using her techniques (which can be carried out using either Maple or Mathematica
software, for example). An identity that can be found this way is: −8(n−1)F(n−2, k −
1)−2(2n−1)F(n−1, k−1)+4(n−1)F(n−2, k)+2(2n−1)F(n−1, k)+nF(n, k) = 0. When
this is summed over all integers k, the recurrence relation nf(n) −4(n −1)f(n −2) = 0
is obtained. From the deﬁnition of f it follows that f(0) = 1 and f(1) = 0. From the
initial conditions and the recurrence relation for f(n), it follows that f(n) = 0 when n is
odd and f(n) =
  n
n/2

when n is even. (This is known as the Reed-Dawson identity.)
5. Gosper’s algorithm can be used to ﬁnd a closed form for Sn = Pn
k=1 k k!.
Let
tn = n n!. Following Gosper’s algorithm gives r(n) =
tn+1
tn
=
(n+1)2
n
, a(n) = n + 1,
b(n) = 1, and c(n) = n. The polynomial x(n) must satisfy (n + 1)x(n + 1) −x(n) = n;
the polynomial x(n) = 1 is such a solution. It follows that zn = n! satisﬁes zn+1−zn = tn.
Hence sn = zn −z1 = n! −1 and Sn = sn+1 = (n + 1)! −1.
6. Gosper’s algorithm can be used to show that Sn = Pn
k=0 k! cannot be expressed as
a hypergeometric term plus a constant. Let tn = n!. Following Gosper’s algorithm gives
r(n) = tn+1
tn
= n + 1, a(n) = n + 1, b(n) = 1, c(n) = 1. The polynomial x(n) must satisfy
(n + 1)x(n + 1) −x(n) = 1 and must have a degree less than zero. It follows that there
is no closed form for Pn
k=0 k! of the type speciﬁed.
3.7.3
CERTIFYING THE TRUTH OF COMBINATORIAL IDENTITIES
Deﬁnitions:
A pair of functions (F, G) is called a WZ pair (after Wilf and Zeilberger) if F(n+1, k)−
F(n, k) = G(n, k + 1) −G(n, k). If (F, G) is a WZ pair, then F is called the WZ mate
of G and vice versa.
A WZ certiﬁcate R(n, k) is a function that can be used to verify the hypergeometric
identity P
k f(n, k) = r(n) by creating a WZ pair (F, G) with F(n, k) = f(n,k)
r(n)
when
r(n) ̸= 0 and F(n, k) = f(n, k) when r(n) = 0 and G(n, k) = R(n, k)F(n, k). When a
hypergeometric identity is proved using a a WZ certiﬁcate, this proof is called a WZ
proof.
Facts:
1. If (F, G) is a WZ pair such that for each integer n ≥0, limk→±∞G(n, k) = 0, then
P
k F(n, k) is a constant for n = 0, 1, 2, . . ..
2. If (F, G) is a WZ pair such that for each integer k, the limit fk = limn→∞F(n, k)
exists and is ﬁnite, for every nonnegative integer n it is the case that limk→±∞G(n, k) =
0, and limL→∞
P
n≥0 G(n, −L) = 0, then P
n≥0 G(n, k) = P
j≤k−1(fj −F(0, j)).

REFERENCES
213
3. An identity P
k f(n, k) = r(n) can be veriﬁed using its WZ certiﬁcate R(n, k) as
follows:
• If r(n) ̸= 0, deﬁne F(n, k) by F(n, k) =
f(n,k)
r(n) , else deﬁne F(n, k) = f(n, k);
deﬁne G(n, k) by G(n, k) = R(n, k)F(n, k).
• Conﬁrm that (F, G) is a WZ pair, i.e., that F(n + 1, k) −F(n, k) = G(n, k + 1) −
G(n, k), by dividing the factorials out and verifying the polynomial identity
that results.
• Verify that the original identity holds for a particular value of n.
4. The WZ certiﬁcate of an identity P
k f(n, k) = r(n) can be found using the following
steps:
• If r(n) ̸= 0, deﬁne F(n, k) to be F(n, k) =
f(n,k)
r(n) , else deﬁne F(n, k) to be
F(n, k) = f(n, k).
• Let f(k) = F(n + 1, k) −F(n, k); provide f(k) as input to Gosper’s algorithm.
• If Gosper’s algorithm produces G(n, k) as output, it is the WZ mate of F and the
function R(n, k) = G(n,k)
F (n,k) is the WZ certiﬁcate of the identity P
k F(n, k) = C
where C is a constant.
If Gosper’s algorithm fails, this algorithm also fails.
Examples:
1. To prove the identity f(n) = P
k
 n
k
2 =
 2n
n

, express it in the form P
k F(n, k) = 1
where F(n, k) =
 n
k
2/
 2n
n

. The identity can be proved by taking the function R(n, k) =
k2(3n−2k+3)
2(2n+1)(n−k+1)2 as its WZ certiﬁcate. (This certiﬁcate can be obtained using Gosper’s
algorithm.)
2. To prove Gauss’s 2F1 identity via a WZ proof, express it in the form P
k F(n, k) = 1
where F(n, k) =
(n+k)!(b+k)!(c−n−1)!(c−b−1)!
(c+k)!(n−1)!(c−n−b−1)!(k+1)!(b−1)!. The identity can then be proved by
taking the function R(n, k) = (k+1)(k+c)
n(n+1−c) as its WZ certiﬁcate. (This certiﬁcate can be
obtained using Gosper’s algorithm.)
REFERENCES
Printed Resources:
[AbSt65] M. Abramowitz and I. A. Stegun, eds., Handbook of Mathematical Functions,
reprinted by Dover, 1965. (An invaluable and general reference for dealing with
functions, with a chapter on sums in closed form.)
[BeWi06] E. A. Bender and S. G. Williamson, Foundations of Combinatorics with Ap-
plications, Dover, 2006. (Section 12.4 contains further discussion of rules of thumb
for asymptotic estimation.)
[BePhHo10] G. E. Bergum, A. N. Philippou, and A. F. Horadam, eds., Applications of
Fibonacci Numbers, Kluwer, 2010.
[BeBr82] A. Beutelspacher and W. Brestovansky, “Generalized Schur numbers”, Combi-
natorial Theory, Lecture Notes in Mathematics 969, Springer-Verlag, 1982, 30–38.
[Br09] R. Brualdi, Introductory Combinatorics, 5th ed., Pearson, 2009.

214
Chapter 3
SEQUENCES
[De10] N. G. de Bruijn, Asymptotic Methods in Analysis, North-Holland, 1958. Third
edition reprinted by Dover Publications, 2010. (This monograph covers a variety of
topics in asymptotics from the viewpoint of an analyst.)
[Dr97] M. Drmota, “Systems of functional equations”, Random Structures & Algorithms
10 (1997), 103–124.
[Dr09] M. Drmota, Random Trees: An Interplay Between Combinatorics and Probability,
Springer-Verlag, 2009. (This book covers the study of asymptotic estimates with
detailed asymptotic results for a wide variety of families of trees.)
[ErSz35] P. Erd˝os and G. Szekeres, “A combinatorial problem in geometry”, Compositio
Mathematica 2 (1935), 463–470.
[FlSaZi91] P. Flajolet, B. Salvy, and P. Zimmermann, “Automatic average-case analysis of
algorithms”, Theoretical Computer Science 79 (1991), 37–109. (Describes computer
software to automate asymptotic average time analysis of algorithms.)
[FlSe09] P. Flajolet and R. Sedgewick, Analytic Combinatorics, Cambridge University
Press, 2009. (Reference for the use of complex analytic techniques in order to analyze
generating functions.)
[GrKnPa94] R. L. Graham, D. E. Knuth, and O. Patashnik, Concrete Mathematics,
2nd ed., Addison-Wesley, 1994. (A superb compendium of special sequences, their
properties and analytical techniques.)
[GrRoSp13] R. Graham, B. Rothschild, and J. Spencer, Ramsey Theory, 2nd ed., Wiley,
2013.
[GrKn07] D. H. Greene and D. E. Knuth, Mathematics for the Analysis of Algorithms,
3rd ed., Birkh¨auser, 2007. (Parts of this text discuss various asymptotic methods.)
[Gr04] R. P. Grimaldi, Discrete and Combinatorial Mathematics, 5th ed., Addison-
Wesley, 2004.
[Gr12] R. P. Grimaldi, Fibonacci and Catalan Numbers, Wiley, 2012.
[Ha75] E. R. Hansen, A Table of Series and Products, Prentice-Hall, 1975. (A reference
giving many summations and products in closed form.)
[Ho79] D. R. Hofstadter, G¨odel, Escher, Bach, Basic Books, 1979.
[Ko01] T. Koshy, Fibonacci and Lucas Numbers with Applications, Wiley, 2001.
[Ko09] T. Koshy, Catalan Numbers with Applications, Oxford University Press, 2009.
[MiRo91] J. G. Michaels and K. H. Rosen, eds., Applications of Discrete Mathematics,
McGraw-Hill, 1991. (Chapters 6, 7, and 8 discuss Stirling numbers, Catalan numbers,
and Ramsey numbers.)
[Mi90] R. E. Mickens, Diﬀerence Equations: Theory and Applications, 2nd ed., Chapman
and Hall/CRC, 1990.
[NiZuMo91] I. Niven, H. S. Zuckerman, and H. L. Montgomery, An Introduction to the
Theory of Numbers, 5th ed., Wiley, 1991.
[Od95] A. M. Odlyzko, “Asymptotic Enumeration Methods”, in Handbook of Combi-
natorics, R. L. Graham, M. Gr¨otschel, and L. Lov´asz (eds.), North-Holland, 1995,
1063–1229. (This is an encyclopedic presentation of methods with a 400+ item bib-
liography.)
[PeWi13] R. Pemantle and M. Wilson, Analytic Combinatorics in Several Variables,
Cambridge University Press, 2013.

REFERENCES
215
[PeWiZe96] M. Petkovˇsek, H. S. Wilf, and D. Zeilberger, A=B, A K Peters/CRC Press,
1996.
[PhBeHo01] A. N. Philippou, G. E. Bergum, and A. F. Horadam, eds., Fibonacci Num-
bers and Their Applications, Mathematics and Its Applications, Vol. 28, Springer,
2001.
[Ra30] F. Ramsey, “On a problem of formal logic”, Proceedings of the London Mathe-
matical Society 30 (1930), 264–286.
[Ri02] J. Riordan, An Introduction to Combinatorial Analysis, Dover, 2002.
[RoTe09] F. S. Roberts and B. Tesman, Applied Combinatorics, 2nd ed., Chapman and
Hall/CRC Press, 2009.
[Ro12] K. H. Rosen, Discrete Mathematics and Its Applications, 7th ed., McGraw-Hill,
2012.
[Sl73] N. J. A. Sloane, A Handbook of Integer Sequences, Academic Press, 1973. (The
original compilation of integer sequences.)
[SlPl95] N. J. A. Sloane and S. Plouﬀe, The Encyclopedia of Integer Sequences, Academic
Press, 1995. (The deﬁnitive reference work on integer sequences.)
[StMc77] D. F. Stanat and D. F. McAllister, Discrete Mathematics in Computer Science,
Prentice-Hall, 1977.
[St11] R. P. Stanley, Enumerative Combinatorics, Vol. I, 2nd ed., Cambridge University
Press, 2011.
[St01] R. P. Stanley, Enumerative Combinatorics, Vol. II, Cambridge University Press,
2001.
[To85] I. Tomescu, Problems in Combinatorics and Graph Theory, translated by R.
Melter, Wiley, 1985.
[Va07] S. Vajda, Fibonacci and Lucas Numbers, and the Golden Section, Dover, 2007.
[Wi05] H. S. Wilf, generatingfunctionology, 3rd ed., CRC Academic Press, 2005.
Web Resources:
http://oeis.org (On-Line Encyclopedia of Integer Sequences.)
http://www.cs.rit.edu/~spr/ElJC/eline.html
(Contains information on Ramsey
numbers.)
http://www.math.rutgers.edu/~zeilberg/programs.html (Contains programs in Maple
implementing algorithms described in §3.7.2.)
http://www.maths.surrey.ac.uk/hosted-sites/R.Knott/Fibonacci/fibrefs.html
(Contains links to sites dealing with Fibonacci numbers as well as a list of relevant
books and articles.)
http://www.math.upenn.edu/~wilf/AeqB.html (Contains programs in Maple and Math-
ematica implementing algorithms described in §3.7.2.)


4
NUMBER THEORY
4.1 Basic Concepts
Kenneth H. Rosen
4.1.1 Numbers
4.1.2 Divisibility
4.1.3 Radix Representations
4.2 Greatest Common Divisors
Kenneth H. Rosen
4.2.1 Introduction
4.2.2 The Euclidean Algorithm
4.3 Congruences
Kenneth H. Rosen
4.3.1 Introduction
4.3.2 Linear and Polynomial Congruences
4.4 Prime Numbers
Jon F. Grantham and
4.4.1 Basic Concepts
Carl Pomerance
4.4.2 Counting Primes
4.4.3 Numbers of Special Form
4.4.4 Pseudoprimes and Primality Testing
4.5 Factorization
Jon F. Grantham and
4.5.1 Factorization Algorithms
Carl Pomerance
4.6 Arithmetic Functions
Kenneth H. Rosen
4.6.1 Multiplicative and Additive Functions
4.6.2 Euler’s Phi-Function
4.6.3 Sum and Number of Divisors Functions
4.6.4 The M¨obius Function and Other Important Arithmetic Functions
4.6.5 Dirichlet Products
4.7 Primitive Roots and Quadratic Residues
Kenneth H. Rosen
4.7.1 Primitive Roots
4.7.2 Index Arithmetic
4.7.3 Quadratic Residues
4.7.4 Modular Square Roots
4.8 Diophantine Equations
Bart E. Goddard
4.8.1 Linear Diophantine Equations
4.8.2 Pythagorean Triples
4.8.3 Fermat’s Last Theorem
4.8.4 Pell’s, Bachet’s, and Catalan’s Equations
4.8.5 Sums of Squares and Waring’s Problem
4.9 Diophantine Approximation
Jeff Shallit
4.9.1 Continued Fractions
4.9.2 Convergents

218
Chapter 4
NUMBER THEORY
4.9.3 Approximation Theorems
4.9.4 Irrationality Measures
4.10 Algebraic Number Theory
Lawrence C. Washington
4.10.1 Basic Concepts
4.10.2 Quadratic Fields
4.10.3 Primes and Unique Factorization in Quadratic Fields
4.10.4 General Number Fields
4.10.5 Properties of Number Fields
4.10.6 Cyclotomic Fields
4.11 Elliptic Curves
Lawrence C. Washington
4.11.1 Basic Concepts
4.11.2 Isomorphisms, Endomorphisms, and Isogenies
4.11.3 Elliptic Curves over Finite Fields
4.11.4 Elliptic Curves over Q
4.11.5 Elliptic Curves over C
INTRODUCTION
This chapter covers the basics of number theory. Number theory, a subject with a long
and rich history, has become increasingly important because of its applications to com-
puter science and cryptography. The core topics of number theory, such as divisibility,
radix representations, greatest common divisors, primes, factorization, congruences, dio-
phantine equations, and continued fractions, are covered here. Algorithms for ﬁnding
greatest common divisors, large primes, and factorizations of integers are described.
There are many famous problems in number theory, including some that have been solved
only recently, such as Fermat’s last theorem, and others that have eluded resolution, such
as the Goldbach conjecture. The status of such problems is described in this chapter. New
discoveries in number theory, such as new large primes, are being made at an increasingly
fast pace. This chapter describes the current state of knowledge and provides pointers
to Internet sources where the latest facts can be found.
GLOSSARY
algebraic integer: a root of a monic polynomial with integer coeﬃcients.
algebraic number: a root of a polynomial with integer coeﬃcients.
arithmetic function: a function deﬁned for all positive integers.
Bachet’s equation: a diophantine equation of the form y2 = x3 +k, where k is a given
integer.
base: the positive integer b, with b > 1, in the expansion n = akbk + ak−1bk−1 + · · · +
a1b + a0, where 0 ≤ai ≤b −1 for i = 0, 1, 2, . . ., k.
binary coded decimal expansion: the expansion produced by replacing each deci-
mal digit of an integer by the four-bit binary expansion of that digit.
binary representation of an integer: the base two expansion of this integer.
Carmichael number: a positive integer that is a pseudoprime to all bases.

GLOSSARY
219
Catalan’s equation: the diophantine equation xm−yn = 1, where solutions in integers
greater than 1 are sought for x, y, m, and n.
Chinese remainder theorem: the theorem that states that given a set of congruences
x ≡ai (mod mi) for i = 1, 2, . . ., n, where the integers mi, i = 1, 2, . . ., n, are pairwise
relatively prime, there is a unique simultaneous solution of these congruences modulo
M = m1m2 . . . mn.
complete system of residues modulo m: a set of integers such that every integer
is congruent modulo m to exactly one integer in the set.
composite: a positive integer that has a factor other than 1 and itself.
congruence class of a modulo m: the set of integers congruent to a modulo m.
congruent integers modulo m: two integers with a diﬀerence divisible by m.
continued fraction: a ﬁnite or inﬁnite expression of the form a0 + 1/(a1 + 1/(a2 + · · · ;
usually abbreviated [a0, a1, a2, . . .].
convergent: a rational fraction obtained by truncating a continued fraction.
coprime (integers): integers that have no positive common divisor other than 1; see
relatively prime.
cyclotomic ﬁeld: a ﬁeld obtained by adjoining a primitive root of unity to the ﬁeld of
rational numbers.
diophantine approximation: the approximation of a number by numbers belonging
to a speciﬁed set, often the set of rational numbers.
diophantine equation: an equation together with the restriction that the only solu-
tions of the equation of interest are those belonging to a speciﬁed set, often the set
of integers or the set of rational numbers.
Dirichlet’s theorem (on primes in arithmetic progressions): the theorem that states
that there are inﬁnitely many primes in each arithmetic progression of the form
an + b, where a and b are relatively prime positive integers.
discrete logarithm of a to the base r modulo m: the integer x such that rx ≡
a (mod m), where r is a primitive root of m and gcd(a, m) = 1.
divides: the integer a divides the integer b, written a | b, if there is an integer c such
that b = ac.
divisor: (1) an integer d such that d divides a for a given integer a, or (2) the positive
integer d that is divided into the integer a to yield a = dq + r with 0 ≤r < d.
elliptic curve: for prime p > 3, the set of solutions (x, y) to the congruence y2 ≡
x3 + ax + b (mod p), where 4a3 + 27b2 ̸≡0 (mod p), together with a special point O,
called the point at inﬁnity.
elliptic curve method (ECM): a factoring technique invented by Lenstra that is
based on the theory of elliptic curves.
Euler phi-function: the function φ(n) whose value at the positive integer n is the
number of positive integers not exceeding n relatively prime to n.
Euler’s theorem: the theorem that states that if n is a positive integer and a is an
integer with gcd(a, n) = 1, then aφ(n) ≡1 (mod n), where φ(n) is the value of the
Euler phi-function at n.
exactly divides: if p is a prime and n is a positive integer, pr exactly divides n, written
pr||n, if pr divides n, but pr+1 does not divide n.

220
Chapter 4
NUMBER THEORY
factor (of an integer n): an integer that divides n.
factorization algorithm: an algorithm whose input is a positive integer and whose
output is the prime factorization of this integer.
Farey series (of order n): the set of fractions h
k , where h and k are relatively prime
nonnegative integers with 0 ≤h ≤k ≤n and k ̸= 0.
Fermat equation: the diophantine equation xn+yn = zn, where n is an integer greater
than 2 and x, y, z are nonzero integers.
Fermat number: a number of the form 22n + 1, where n is a nonnegative integer.
Fermat prime: a prime Fermat number.
Fermat’s last theorem: the theorem that states that if n is a positive integer greater
than 2, then the equation xn + yn = zn has no solutions in integers with xyz ̸= 0.
Fermat’s little theorem: the theorem that states that if p is prime and a is an integer,
then ap ≡a (mod p).
Fibonacci numbers: the sequence of numbers deﬁned by F0 = 0, F1 = 1, and Fn =
Fn−1 + Fn−2 for n = 2, 3, 4, . . ..
fundamental theorem of arithmetic: the theorem that states that every positive
integer has a unique representation as the product of primes written in nondecreasing
order.
Gaussian integers: the set of numbers of the form a + bi, where a and b are integers
and i is √−1.
greatest common divisor gcd (of a set of integers): the largest integer that divides
all integers in the set; the greatest common divisor of the integers a1, a2, . . . , an is
denoted gcd(a1, a2, . . . , an).
hexadecimal representation (of an integer): the base sixteen representation of this
integer.
index of a to the base r modulo m: the smallest nonnegative integer x, denoted
indr a, such that rx ≡a (mod m), where r is a primitive root of m and gcd(a, m) = 1.
integer of an algebraic number ﬁeld: an algebraic integer that belongs to this ﬁeld.
inverse of an integer a modulo m: an integer a such that aa ≡1 (mod m); here
gcd(a, m) = 1.
irrational number: a real number that is not the ratio of two integers.
Jacobi symbol: a generalization of the Legendre symbol.
Kronecker symbol: a generalization of the Legendre and Jacobi symbols.
least common multiple (of a set of integers): the smallest positive integer that is di-
visible by all integers in the set.
least nonnegative residue of a modulo m: the remainder when a is divided by m;
it is the smallest nonnegative integer congruent to a modulo m, written a modm.
Legendre symbol: the symbol
  a
p

that has the value 1 if a is a square modulo p
and −1 if a is not a square modulo p; here p is a prime and a is an integer not
divisible by p.
linear congruential method: a method for generating a sequence of pseudo-random
numbers based on a congruence of the form xn+1 ≡axn + c (mod m).

GLOSSARY
221
Mersenne prime: a prime of the form 2p −1, where p is a prime.
M¨obius function: the arithmetic function µ(n), where µ(n) = 1 if n = 1, µ(n) = 0
if n has a square factor larger than 1, and µ(n) = (−1)s if n is square free and is the
product of s diﬀerent primes.
modulus: the integer m in a congruence a ≡b (mod m).
multiple of an integer a: an integer b such that a divides b.
multiplicative function: a function f such that f(mn) = f(m)f(n) whenever m and n
are relatively prime positive integers.
mutually relatively prime (set of integers): integers with no common factor greater
than 1.
number ﬁeld: a ﬁeld that is a ﬁnite degree extension of Q.
number ﬁeld sieve: a factoring algorithm, currently the best one known for large
numbers with no small prime factors.
octal representation of an integer: the base eight representation of this integer.
one’s complement expansion: an n bit representation of an integer x with |x| <
2n−1, for a speciﬁed positive integer n, in which the leftmost bit is 0 if x ≥0 and 1 if
x < 0, and the remaining n −1 bits are those of the binary expansion of x if x ≥0,
and the complements of the bits in the expansion of |x| if x < 0.
order of an integer a modulo m: the least positive integer t, denoted by ordma,
such that at ≡1 (mod m); here gcd(a, m) = 1.
pairwise relatively prime: integers with the property that each two of them are rel-
atively prime.
palindrome: a ﬁnite sequence that reads the same forward and backward.
partial quotient: a term ai of a continued fraction.
Pell’s equation: the diophantine equation x2 −dy2 = 1, where d is a positive integer
that is not a perfect square.
perfect number: a positive integer whose sum of positive divisors, other than the
integer itself, equals this integer.
periodic base b expansion: a base b expansion in which the terms beyond a certain
point are repetitions of the same block of integers.
powerful integer: an integer n with the property that p2 divides n whenever p is a
prime that divides n.
primality test: an algorithm that determines whether a positive integer is prime.
prime: a positive integer greater than 1 that has exactly two factors, 1 and itself.
prime factorization: the factorization of an integer into primes.
prime number theorem: the theorem that states that the number of primes not ex-
ceeding a positive real number x is asymptotic to
x
log x (where log x denotes the
natural logarithm of x).
prime-power factorization: the factorization of an integer into powers of distinct
primes.
primitive root of an integer n: an integer r such that the least positive residues of
the powers of r run through all positive integers relatively prime to n and less than n.

222
Chapter 4
NUMBER THEORY
probabilistic primality test: an algorithm that determines whether an integer is prime
with a small probability of a false positive result.
pseudoprime to the base b: a composite positive integer n such that bn ≡b (mod n).
pseudo-random number generator: a deterministic method to generate numbers
that share many properties with numbers really chosen randomly.
Pythagorean triple: positive integers x, y, and z such that x2 + y2 = z2.
quadratic ﬁeld: the set of numbers Q(
√
d) = {a+b
√
d | a, b rational} for some square-
free integer d.
quadratic irrational: an irrational number that is the root of a quadratic polynomial
with integer coeﬃcients.
quadratic nonresidue (of m): an integer that is not a perfect square modulo m.
quadratic reciprocity: the law that states that given two odd primes p and q, if at
least one of them is of the form 4n + 1, then p is a quadratic residue of q if and only
if q is a quadratic residue of p and if both primes are of the form 4n + 3, then p is a
quadratic residue of q if and only if q is a quadratic nonresidue of p.
quadratic residue (of m): an integer that is a perfect square modulo m.
quadratic sieve: a factoring algorithm invented by Pomerance in 1981.
rational number: a real number that is the ratio of two integers. The set of rational
numbers is denoted Q.
reduced system of residues modulo m: pairwise incongruent integers modulo m
such that each integer in the set is relatively prime to m and every integer rela-
tively prime to m is congruent to an integer in the set.
relatively prime (integers): two integers with no common divisor greater than 1; see
coprime.
remainder (of the integer a when divided by the positive integer d): the integer r in the
equation a = dq + r with 0 ≤r < d, written r = a mod d.
root (of a function f modulo m): an integer r such that f(r) ≡0 (mod m).
sieve of Eratosthenes: a procedure for ﬁnding all primes less than a speciﬁed integer.
smooth number: an integer all of whose prime divisors are small.
square root (of a modulo m): an integer r whose square is congruent to a modulo m.
square-free integer: an integer not divisible by any perfect square other than 1.
ten most wanted numbers: a collection of large integers on a list, maintained by a
group of researchers, whose currently unknown factorizations are actively sought;
these integers are somewhat beyond the realm of numbers that can be factored using
known techniques.
terminating base-b expansion: a base-b expansion with only a ﬁnite number of nonzero
coeﬃcients.
totient function: the Euler phi-function.
transcendental number: a complex number that cannot be expressed as the root of
an algebraic equation with integer coeﬃcients.
trial division: a factorization technique that proceeds by dividing an integer by suc-
cessive primes.

Section 4.1
BASIC CONCEPTS
223
twin primes: a pair of primes that diﬀer by two.
two’s complement expansion: an n bit representation of an integer x, with −2n−1 ≤
x ≤2n−1 −1, for a speciﬁed positive integer n, in which the leftmost bit is 0 if x ≥0
and 1 if x < 0, and the remaining n −1 bits are those from the binary expansion
of x if x ≥0 and are those of the binary expansion of 2n −|x| if x < 0.
ultimately periodic: a sequence (typically a base-k expansion or continued fraction)
(ai)i≥0 that eventually repeats; that is, there exist k and N such that an+k = an for
all n ≥N.
unit (of a number ﬁeld): an algebraic integer that divides 1 in this ﬁeld.
Waring’s problem: the problem of determining the smallest number g(k) such that
every integer is the sum of g(k) kth powers of integers.
4.1
BASIC CONCEPTS
The basic concepts of number theory include the classiﬁcation of numbers into diﬀerent
sets of special importance, the notion of divisibility, and the representation of integers.
For more information about these basic concepts, see introductory number theory texts,
such as [Ro10].
Unless otherwise speciﬁed, log will indicate the natural logarithm function.
4.1.1
NUMBERS
Deﬁnitions:
The integers are the elements of the set Z = {. . . , −3, −2, −1, 0, 1, 2, 3, . . .}.
The natural numbers are the integers in the set N = {0, 1, 2, 3, . . .}.
The rational numbers are real numbers that can be written as a/b where a and b are
integers with b ̸= 0. Numbers that are not rational are called irrational. The set of
rational numbers is denoted by Q.
The algebraic numbers are real numbers that are solutions of equations of the form
anxn + · · · + a1x + a0 = 0, where ai is an integer for i = 0, 1, . . . , n. Real numbers that
are not algebraic are called transcendental.
Facts:
1. The following table summarizes information and notation about some important
types of numbers.

224
Chapter 4
NUMBER THEORY
name
deﬁnition
examples
natural numbers N
{0, 1, 2, . . .}
0, 43
integers Z
{. . . , −2, −1, 0, 1, 2, . . .}
0, 43, −314
Gaussian integers Z[i]
{a + bi | a, b ∈Z}
3, 4 + 3i, 7i
rational numbers Q
{ a
b | a, b ∈Z; b ̸= 0}
0, 22
7
quadratic irrationals
irrational root of quadratic equation
√
2, 2+
√
5
3
a2x2 + a1x + a0 = 0; all ai ∈Q
irrational numbers
R −Q
√
2, π, e
algebraic numbers Q
root of algebraic equation
i,
√
2,
3q
3
2
anxn + · · · + a0 = 0, n ≥1,
a0, . . . , an ∈Z
algebraic integers A
root of monic algebraic equation
i,
√
2, 1+
√
5
2
xn + an−1xn−1 + · · · + a0 = 0,
n ≥1, a0, a1, . . . , an−1 ∈Z
transcendental numbers
C −Q
π, e, i log 2
real numbers R
completion of Q
0, 1
3,
√
2, π
complex numbers C
R or R[i]
3 + 2i, e + iπ
2. Some sources do not include 0 in the set of natural numbers. So, always check the
deﬁnition of natural numbers used in your sources.
3. A real number is rational if and only if its decimal expansion terminates or is periodic.
(See §4.1.3).
4. The number N 1/m is irrational where N and m are positive integers, unless N is the
mth power of an integer n.
5. The number logb a is irrational, where a and b are positive integers greater than 1, if
there is a prime that divides exactly one of a and b.
6. If x is a root of an equation xm +am−1xm−1 +· · ·+a0 = 0 in which the coeﬃcients ai
(i = 0, 1, . . . , m −1) are integers, then x is either integer or irrational.
7. The set of algebraic numbers is countable (§1.2.3). Hence, almost all real numbers
are transcendental. (However, showing a particular number of interest is transcendental
is usually diﬃcult.)
8. Both e and π are transcendental. The transcendence of e was proven by Hermite in
1873, and π was proven transcendental by Lindemann in 1882. Proofs of the transcen-
dence of e and π can be found in [HaWr08].
9. Gelfond-Schneider theorem: If α and β are algebraic numbers with α not equal to 0
or 1 and β irrational, then αβ is transcendental. (For a proof see [Ba90].)
10. Baker’s linear forms in logarithms: If α1, . . . , αn are nonzero algebraic numbers and
log α1, . . . , log αn are linearly independent over Q, then 1, log α1, . . . , log αn are linearly
independent over Q, where Q is the closure of Q.
(Consult [Ba90] for a proof and
applications of this theorem.)
Examples:
1. The numbers 11
17, −3345
7 , −1, 578
579, and 0 are rational.
2. The number log2 10 is irrational.
3. The numbers
√
2, 1 +
√
2, and 1+
√
2
5
are irrational.

Section 4.1
BASIC CONCEPTS
225
4. The number x = 0.10100100010000 . . ., with a decimal expansion consisting of blocks
where the nth block is a 1 followed by n 0s, is irrational, since this decimal expansion
does not terminate and is not periodic.
5. The decimal expansion of 22
7 is periodic, since 22
7 = 3.142857. However, the decimal
expansion of π neither terminates, nor is periodic, with π = 3.141592653589793 . . ..
6. It is not known whether Euler’s constant γ = lim
n→∞
Pn
k=1
1
k −log n

(where log x
denotes the natural logarithm of x) is rational or irrational.
7. The numbers 2, 1
2,
√
17,
3√
5, and 1 +
6√
2 are algebraic.
8. By the Gelfond-Schneider theorem (Fact 9),
√
2
√
2 is transcendental.
9. By Baker’s linear forms in logarithms theorem (Fact 10), since log2 10 is irrational,
it is transcendental.
4.1.2
DIVISIBILITY
The notion of the divisibility of one integer by another is the most basic concept in
number theory. Introductory number theory texts, such as [HaWr08], [NiZuMo91], and
[Ro10], are good references for this material.
Deﬁnitions:
Suppose a and d are integers with d > 0. Then in the equation a = dq+r with 0 ≤r < d,
a is the dividend, d is the divisor, q is the quotient, and r is the remainder.
Let m and n be integers with m ≥1 and n = dm + r with 0 ≤r < m. Then n mod m,
the value of the mod m function at n, is r, the remainder when n is divided by m.
If a and b are integers and a ̸= 0, then a divides b, written a|b, if there is an integer c
such that b = ac. If a divides b, then a is a factor or divisor of b, and b is a multiple
of a. If a is a positive divisor of b that does not equal b, then a is a proper divisor of
b. The notation a̸ | b means that a does not divide b.
A prime is a positive integer divisible by exactly two distinct positive integers, 1 and
itself. A positive integer, other than 1, that is not prime is called composite.
An integer is square free if it is not divisible by any perfect square other than 1.
An integer n is powerful if whenever a prime p divides n, p2 divides n.
If p is prime and n is a positive integer, then pr exactly divides n, written pr||n, if pr
divides n, but pr+1 does not divide n.
Facts:
1. If a is a nonzero integer, then a|0.
2. If a is an integer, then 1|a.
3. If a and b are positive integers and a|b, then the following statements are true:
• a ≤b;
•
b
a divides b;
• ak divides bk for every positive integer k;
• a divides bc for every integer c.

226
Chapter 4
NUMBER THEORY
4. If a, b, and c are integers such that a|b and b|c, then a|c.
5. If a, b, and c are integers such that a|b and a|c, then a|bm + cn for all integers m and
n.
6. If a and b are integers such that a|b and b|a, then a = ±b.
7. If a and b are integers and m is a nonzero integer, then a|b if and only if ma|mb.
8. Division algorithm:
If a and d are integers with d positive, then there are unique
integers q and r such that a = dq + r with 0 ≤r < d. (Note: The division algorithm is
not an algorithm, in spite of its name.)
9. The quotient q and remainder r when the integer a is divided by the positive integer d
are given by q =
 a
d

and r = a −d
 a
d

, respectively.
10. If a and d are positive integers, then there are unique integers q, r, and e such that
a = dq + er where e = ±1 and −d
2 < r ≤d
2.
11. There are several divisibility tests that are easily performed using the decimal ex-
pansion of an integer. These include:
• An integer is divisible by 2 if and only if its last digit is even. It is divisible by 4 if
and only if the integer made up of its last two digits is divisible by four. More
generally, it is divisible by 2j if and only if the integer made up of the last j
decimal digits of n is divisible by 2j.
• An integer is divisible by 5 if and only if its last digit is divisible by 5 (which
means it is either 0 or 5). It is divisible by 25 if and only if the integer made
up of the last two digits is divisible by 25. More generally, it is divisible by 5j
if and only if the integer made up of the last j digits of n is divisible by 5j.
• An integer is divisible by 3, or by 9, if and only if the sum of the decimal digits
of n is divisible by 3, or by 9, respectively.
• An integer is divisible by 11 if and only if the integer formed by alternately adding
and subtracting the decimal digits of the integer is divisible by 11.
• An integer is divisible by 7, 11, or 13 if and only if the integer formed by succes-
sively adding and subtracting the three-digit integers formed from successive
blocks of three decimal digits of the original number, where digits are grouped
starting with the rightmost digit, is divisible by 7, 11, or 13, respectively.
12. If d|b −1, then n = (ak . . . a1a0)b (this notation is deﬁned in §4.1.3) is divisible by d
if and only if the sum of the base b digits of n, ak + · · · + a1 + a0, is divisible by d.
13. If d|b + 1, then n = (ak . . . a1a0)b is divisible by d if and only if the alternating sum
of the base b digits of n, (−1)kak + · · · −a1 + a0, is divisible by d.
14. If pr||a and ps||b, where p is a prime and a and b are positive integers, then pr+s||ab.
15. If pr||a and ps||b, where p is a prime and a and b are positive integers, then
pmin(r,s)||a + b.
16. There are inﬁnitely many primes. (See §4.4.2.)
17. There are eﬃcient algorithms that can produce large integers that have an extremely
high probability of being prime. (See §4.4.4.)
18. Fundamental theorem of arithmetic:
Every positive integer can be written as the
product of primes in exactly one way, where the primes occur in nondecreasing order in
the factorization.
19. Many diﬀerent algorithms have been devised to ﬁnd the factorization of a positive
integer into primes. Using some recently invented algorithms and the powerful computer
systems available today, it is feasible to factor integers with over 100 digits. (See §4.5.1.)

Section 4.1
BASIC CONCEPTS
227
20. The relative ease of producing large primes compared with the apparent diﬃculty
of factoring large integers is the basis for an important cryptosystem called RSA. (See
Chapter 15.)
Examples:
1. The integers 0, 3, −12, 21, 342, and −1,113 are divisible by 3; the integers −1, 7, 29,
and −1,111 are not divisible by 3.
2. The quotient and remainder when 214 is divided by 6 are 35 and 4, respectively, since
214 = 35 · 6 + 4.
3. The quotient and remainder when −114 is divided by 7 are −17 and 5, respectively,
since −114 = −17 · 7 + 5.
4. With a = 214 and d = 6, the expansion of Fact 10 is 214 = 36 · 6 −2 (so that e = −1
and r = 2).
5. 11 mod 4 = 3, 100 mod 7 = 2, and −22 mod 5 = 3.
6. The following are primes: 2, 3, 17, 101, 641. The following are composites: 4, 9, 91,
111, 1001.
7. The integers 15, 105, and 210 are square free; the integers 12, 99, and 270 are not.
8. The integer 72 is powerful since 2 and 3 are the only primes that divide 72 and 22 = 4
and 32 = 9 both divide 72, but 180 is not powerful since 5 divides 180, but 52 does not.
9. The integer 32,688,048 is divisible by 2, 4, 8, and 16 since 2|8, 4|48, 8|048, and
16|8,048, but it is not divisible by 32 since 32 does not divide 88,048.
10. The integer 723,160,823 is divisible by 11 since the alternating sum of its digits,
3 −2 + 8 −0 + 6 −1 + 3 −2 + 7 = 22, is divisible by 11.
11. Since 33|216, but 34̸ | 216, it follows that 33||216.
4.1.3
RADIX REPRESENTATIONS
The representation of numbers in diﬀerent bases has been important in the development
of mathematics from its earliest days and is extremely important in computer arithmetic.
For further details on this topic, see [Kn97], [Ko93], and [Sc85].
Deﬁnitions:
The base b expansion of a positive integer n, where b is an integer greater than 1, is
the unique expansion of n as n = akbk +ak−1bk−1 +· · ·+a1b+a0; here k is a nonnegative
integer, aj is a nonnegative integer less than b for j = 0, 1, . . ., k, and the initial coeﬃcient
ak ̸= 0. This expansion is written as (akak−1 . . . a1a0)b.
The integer b in the base b expansion of an integer is called the base or radix of the
expansion.
The coeﬃcients aj in the base b expansion of an integer are called the base b digits of
the expansion.
Base 10 expansions are called decimal expansions. The digits are called decimal digits.
Base 2 expansions are called binary expansions. The digits are called binary digits or
bits.
Base 8 expansions are called octal expansions.

228
Chapter 4
NUMBER THEORY
Base 16 expansions are called hexadecimal expansions. The 16 hexadecimal digits are
0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F (where A, B, C, D, E, F correspond to the decimal
numbers 10, 11, 12, 13, 14, 15, respectively).
The binary coded decimal expansion of an integer is the bit string formed by replacing
each digit in the decimal expansion of the integer by the four bit binary expansion of
that digit.
The one’s complement expansion of an integer x with |x| < 2n−1, for a speciﬁed
positive integer n, uses n bits, where the leftmost bit is 0 if x ≥0 and 1 if x < 0,
and the remaining n−1 bits are those from the binary expansion of x if x ≥0 and are
the complements of the bits in the binary expansion of |x| if x < 0. (Note: The one’s
complement representation 11 . . .1, consisting of n 1s, is usually considered to be the
negative representation of the number 0.)
The two’s complement expansion of an integer x with −2n−1 ≤x ≤2n−1−1, for a
speciﬁed positive integer n, uses n bits, where the leftmost bit is 0 if x ≥0 and 1 if x < 0,
and the remaining n−1 bits are those from the binary expansion of x if x ≥0 and are
those of the binary expansion of 2n −|x| if x < 0.
The base b expansion (where b is an integer greater than 1) of a real number x with
0 ≤x < 1 is the unique expansion of x as x = P∞
j=1
cj
bj where cj is a nonnegative integer
less than b for j = 1, 2, ... and for every integer N there is a coeﬃcient cn ̸= b−1 for some
n > N. This expansion is written as (.c1c2c3 . . .)b.
A base b expansion (.c1c2c3 . . .)b terminates if there is a positive integer n such that
cn = cn+1 = cn+2 = · · · = 0.
A base b expansion (.c1c2c3 . . .)b is periodic if there are positive integers N and k such
that cn+k = cn for all n ≥N.
The periodic base b expansion (.c1c2 . . . cN−1cN . . . cN+k−1cN . . . cN+k−1cN . . .)b is
denoted by (.c1c2 . . . cN−1cN . . . cN+k−1)b. The part of the periodic base b expansion
preceding the periodic part is the pre-period and the periodic part is the period,
where the period and pre-period are taken to have minimal possible length.
Facts:
1. If b is a positive integer greater than 1, then every positive integer n has a unique
base b expansion.
2. Converting from base 10 to base b: Take the positive integer n and divide it by b to
obtain n = bq0 + a0, 0 ≤a0 < b. Then divide q0 by b to obtain q0 = bq1 + a1, 0 ≤a1 < b.
Continue this process, successively dividing the quotients by b, until a quotient of zero
is obtained, after k steps.
The base b expansion of n is then (ak−1 . . . a1a0)b.
(See
Algorithm 1.)
Algorithm 1:
Constructing base b expansions.
procedure base b expansion(n: positive integer)
q := n
k := 0
while q ̸= 0
ak := q mod b
q :=
 q
b


Section 4.1
BASIC CONCEPTS
229
k := k + 1
{The base b expansion of n is (ak−1 . . . a1a0)b}
3. Converting from base 2 to base 2k:
Group the bits in the base 2 expansion into
blocks of k bits, starting from the right, and then convert each block of k bits into a
base 2k digit. For example, converting from binary (base 2) to octal (base 8) is done
by grouping the bits of the binary expansion into blocks of three bits starting from the
right and converting each block into an octal digit. Similarly, converting from binary to
hexadecimal (base 16) is done by grouping the bits of the binary expansion into blocks
of four bits starting from the right and converting each block into a hex digit.
4. Converting from base 2k to binary (base 2): convert each base 2k digit into a block of
k bits and string together these bits in the order the original digits appear. For example,
to convert from hexadecimal to binary, convert each hex digit into the block of four bits
that represent this hex digit and then string together these blocks of four bits in the
correct order.
5. Every positive integer can be expressed uniquely as the sum of distinct powers of 2.
This follows since every positive integer has a unique base 2 expansion, with the digits
either 0 or 1.
6. There are ⌊logb n⌋+1 decimal digits in the base b expansion of the positive integer n.
7. The number x with one’s complement representation (an−1an−2 . . . a1a0) can be
found using the equation
x = −an−1(2n−1−1) +
n−2
X
i=0
ai2i.
8. The number x with two’s complement representation (an−1an−2 . . . a1a0) can be
found using the equation
x = −an−1 · 2n−1 +
n−2
X
i=0
ai2i.
9. Two’s complement representations of integers are often used by computers because
addition and subtraction of integers, where these integers may be either positive or
negative, can be performed easily using these representations.
10. Deﬁne a function Lg n by the rule
Lg n =
(
1
if n = 0;
1 + ⌊log2 |n| ⌋
if n ̸= 0.
Then Lg n is the number of bits in the base 2 expansion of n, not counting the sign bit.
(Compare with Fact 6.)
11. The bit operations for the basic operations are given in the following table, adapted
from [BaSh96]. This table displays the number of bit operations used by the standard,
naive algorithms, doing things bit by bit (addition with carries, subtraction with borrows,
standard multiplication by each bit and shifting and adding, and standard division), and
a big-oh estimate for the number of bits required to do the operations using the algorithm
with the currently best known computational complexity. (The function Lg is deﬁned in
Fact 10; the function µ(m, n) is deﬁned by the rule µ(m, n) = m(Lg n)(Lg Lg n) if m ≥n
and µ(m, n) = n(Lg m)(Lg Lg m) otherwise.)

230
Chapter 4
NUMBER THEORY
operations
number of bits for operation
best known complexity
(following naive algorithm)
(sophisticated algorithm)
a ± b
Lg a + Lg b
O(Lg a + Lg b)
a · b
Lg a · Lg b
O(µ(Lg a, Lg b))
a = qb + r
Lg q · Lg b
O(µ(Lg q, Lg b))
12. If b is a positive integer greater than 1 and x is a real number with 0 ≤x < 1,
then x can be uniquely written as x = P∞
j=1
cj
bj where cj is a nonnegative integer less
than b for all j, with the restriction that for every positive integer N there is an integer n
with n > N and cn ̸= b −1 (in other words, it is not the case that from some point on,
all the coeﬃcients are b −1).
13. A periodic or terminating base b expansion, where b is a positive integer, represents
a rational number.
14. The base b expansion of a rational number, where b is a positive integer, either
terminates or is periodic.
15. If 0 < x < 1, x = r
s where r and s are relatively prime positive integers, and s = T U
where every prime factor of T divides b and gcd(U, b) = 1, then the period length of the
base b expansion of x is ordUb (deﬁned in §4.7.1) and the pre-period length is the smallest
positive integer N such that T divides bN.
16. The period length of the base b expansion of
1
m (b and m positive integers greater
than 1) is m −1 if and only if m is prime and b is a primitive root of m. (See §4.7.1.)
Examples:
1. The binary (base 2), octal (base 8), and hexadecimal (base 16) expansions of the
integer 2001 are (11111010001)2, (3721)8, and (7D1)16, respectively.
The octal and
hexadecimal expansions can be obtained from the binary expansion by grouping together,
from the right, the bits of the binary expansion into groups of three bits and four bits,
respectively.
2. The hexadecimal expansion 2FB3 can be converted to a binary expansion by replac-
ing each hex digit by a block of four bits to give 10111110110011. (The initial two 0s in
the four-bit expansion of the initial hex digit 2 are omitted.)
3. The binary coded decimal expansion of 729 is 011100101001.
4. The nine-bit one’s complement expansions of 214 and −113 (taking n = 9 in the
deﬁnition) are 011010110 and 110001110.
5. The nine-bit two’s complement expansions of 214 and −113 (taking n = 9 in the
deﬁnition) are 011010110 and 110001111.
6. By Fact 7 the integer with a nine-bit one’s complement representation of 101110111
equals −1(256 −1) + 119 = −136.
7. By Fact 8 the integer with a nine-bit two’s complement representation of 101110111
equals −256 + 119 = −137.
8. By Fact 15 the pre-period of the decimal expansion of
5
28 has length 2 and the
period has length 6 since 28 = 4 · 7 and ord710 = 6. This is veriﬁed by noting that
5
28 = (.17857142)10.

Section 4.2
GREATEST COMMON DIVISORS
231
4.2
GREATEST COMMON DIVISORS
The concept of the greatest common divisor of two integers plays an important role in
number theory. The Euclidean algorithm, an algorithm for computing greatest common
divisors, was known in ancient times and was one of the ﬁrst algorithms that was studied
for what is now called its computational complexity. The Euclidean algorithm and its
extensions are used extensively in number theory and its applications, including those to
cryptography. For more information about the contents of this section consult [HaWr08],
[NiZuMo91], or [Ro10].
4.2.1
INTRODUCTION
Deﬁnitions:
The greatest common divisor of the integers a and b, not both zero, written gcd(a, b),
is the largest integer that divides both a and b.
The integers a and b are relatively prime (or coprime) if they have no positive divisors
in common other than 1, i.e., if gcd(a, b) = 1.
The greatest common divisor of the integers ai, i = 1, 2, . . ., k, not all zero, written
gcd(a1, a2, . . . , ak), is the largest integer that divides all the integers ai.
The integers a1, a2, . . . , ak are pairwise relatively prime if gcd(ai, aj) = 1 for i ̸= j.
The integers a1, a2, . . . , ak are mutually relatively prime if gcd(a1, a2, . . . , ak) = 1.
The least common multiple of nonzero integers a and b, written lcm(a, b), is the
smallest positive integer that is a multiple of both a and b.
The least common multiple of nonzero integers a1, . . . , ak, written lcm(a1, . . . , ak), is
the smallest positive integer that is a multiple of all the integers ai, i = 1, 2, . . . , k.
The Farey series of order n is the set of fractions h
k in which h and k are integers,
0 ≤h ≤k ≤n, k ̸= 0, and gcd(h, k) = 1, in ascending order, with 0 and 1 included in
the forms 0
1 and 1
1, respectively.
Facts:
1. If d|a and d|b, then d| gcd(a, b).
2. If a|m and b|m, then lcm(a, b)|m.
3. If a is a positive integer, then gcd(0, a) = a.
4. If a and b are positive integers with a < b, then gcd(a, b) = gcd(b mod a, a).
5. If a and b are integers with gcd(a, b) = d, then gcd( a
d, b
d) = 1.
6. If a, b, and c are integers, then gcd(a + cb, b) = gcd(a, b).
7. If a, b, and c are integers with not both a and b zero and c ̸= 0, then gcd(ac, bc) =
|c| gcd(a, b).
8. If a and b are integers with gcd(a, b) = 1, then gcd(a + b, a −b) = 1 or 2. (This
greatest common divisor is 2 when both a and b are odd.)
9. If a, b, and c are integers with gcd(a, b) = gcd(a, c) = 1, then gcd(a, bc) = 1.

232
Chapter 4
NUMBER THEORY
10. If a, b, and c are mutually relatively prime nonzero integers, then gcd(a, bc) =
gcd(a, b) · gcd(a, c).
11. If a and b are integers, not both zero, then gcd(a, b) is the least positive integer of
the form ma + nb where m and n are integers.
12. The probability that two randomly selected integers are relatively prime is
6
π2 . More
precisely, if R(n) equals the number of pairs of integers a, b with 1 ≤a ≤n, 1 ≤b ≤n,
and gcd(a, b) = 1, then R(n)
n2
=
6
π2 + O( log n
n ).
13. If a and b are positive integers, then gcd(2a −1, 2b −1) = 2(a,b) −1.
14. If a, b, and c are integers and a|bc and gcd(a, b) = 1, then a|c.
15. If a, b, and c are integers, a|c, b|c and gcd(a, b) = 1, then ab|c.
16. If a1, a2, . . . , ak are integers, not all zero, then gcd(a1, . . . , ak) is the least positive
integer that is a linear combination with integer coeﬃcients of a1, . . . , ak.
17. If a1, a2, . . . , ak are integers, not all zero, and d|ai holds for i = 1, 2, . . ., k, then
d| gcd(a1, a2, . . . , ak).
18. If a1, . . . , an are integers, not all zero, then the greatest common divisor of these n
integers is the same as the greatest common divisor of the set of n −1 integers made
up of the ﬁrst n −2 integers and the greatest common divisor of the last two. That is,
gcd(a1, . . . , an) = gcd(a1, . . . , an−2, gcd(an−1, an)).
19. If a and b are nonzero integers and m is a positive integer, then lcm(ma, mb) =
m · lcm(a, b)
20. If b is a common multiple of the integers a1, a2, . . . , ak, then b is a multiple of
lcm(a1, . . . , ak).
21. The common multiples of the integers a1, . . . , ak are the integers 0, lcm(a1, . . . , ak),
2 · lcm(a1, . . . , ak), . . . .
22. If a1, a2, . . . , an are pairwise relatively prime integers, then lcm(a1, . . . , an) =
a1a2 . . . an.
23. If a1, a2, . . . , an are integers,
not all zero,
then lcm(a1, a2, . . . , an−1, an)
=
lcm(lcm(a1, a2, . . . , an−1), an).
24. If a = p1a1p2a2 · · · pnan and b = p1b1p2a2 · · · pnbn, where the pi are distinct primes
for i = 1, . . . , n, and each exponent is a nonnegative integer, then
gcd(a, b) = p1min(a1,b1)p2min(a2,b2) . . . pnmin(an,bn),
where min(x, y) denotes the minimum of x and y, and
lcm(a, b) = p1max(a1,b1)p2max(a2,b2) . . . pnmax(an,bn),
where max(x, y) denotes the maximum of x and y.
25. If a and b are positive integers, then ab = gcd(a, b) · lcm(a, b).
26. If a, b, and c are positive integers, then lcm(a, b, c) =
abc · gcd(a, b, c)
gcd(a, b) · gcd(a, c) · gcd(b, c).
27. If a, b, and c are positive integers, then gcd(lcm(a, b), lcm(a, c)) = lcm(a, gcd(b, c))
and lcm(gcd(a, b), gcd(a, c)) = gcd(a, lcm(b, c)).
28. If a
b , c
d, and e
f are successive terms of a Farey series, then c
d = a+e
b+f .
29. If a
b and c
d are successive terms of a Farey series, then ad −bc = −1.
30. If a
b and c
d are successive terms of a Farey series of order n, then b + d > n.

Section 4.2
GREATEST COMMON DIVISORS
233
31. Farey series are named after an English geologist who published a note describing
their properties in the Philosophical Magazine in 1816. The eminent French mathemati-
cian Cauchy supplied proofs of the properties stated, but not proved, by Farey. Also,
according to [Di71], these properties had been stated and proved by Haros in 1802.
Examples:
1. gcd(12, 15) = 3, gcd(14, 25) = 1, gcd(0, 100) = 100, and gcd(3, 39) = 3.
2. gcd(27335472113173, 24355272112133) = 24335272112.
3. lcm(27335472113173, 24355272112133) = 27355472113133173.
4. gcd(18, 24, 36) = 6 and gcd(10, 25, 35, 245) = 5.
5. The integers 15, 21, and 35 are mutually relatively prime since gcd(15, 21, 35) = 1.
However, they are not pairwise relatively prime since gcd(15, 35) = 5.
6. The integers 6, 35, and 143 are both mutually relatively prime and pairwise relatively
prime.
7. The Farey series of order 5 is 0
1, 1
5, 1
4, 1
3, 2
5, 1
2, 3
5, 2
3, 3
4, 4
5, 1
1.
4.2.2
THE EUCLIDEAN ALGORITHM
Finding the greatest common divisor of two integers is one of the most common problems
in number theory and its applications. An algorithm for this task was known in ancient
times by Euclid. His algorithm and its extensions are among the most commonly used
algorithms. For more information about these algorithms see [BaSh96] and [Kn97].
Deﬁnition:
The Euclidean algorithm is an algorithm that computes the greatest common divisor
of two integers a and b with a ≤b, by replacing them with a and b mod a, and repeating
this step until one of the integers reached is zero.
Facts:
1. The Euclidean algorithm: The greatest common divisor of two positive integers can
be computed using the recurrence given in §4.2.1, Fact 4 together with §4.1.2, Fact 3.
The resulting algorithm proceeds by successively replacing a pair of positive integers with
a new pair of integers formed from the smaller of the two integers and the remainder
when the larger is divided by the smaller, stopping once a zero remainder is reached.
The last nonzero remainder is the greatest common divisor of the original two integers.
(See Algorithm 1.)
Algorithm 1:
The Euclidean algorithm.
procedure gcd(a, b: positive integers)
r0 := a; r1 := b
i := 1
while ri ̸= 0
ri+1 := ri−1 mod ri
i := i + 1
{gcd(a, b) is ri−1}

234
Chapter 4
NUMBER THEORY
2. Lam´e’s theorem:
The number of divisions needed to ﬁnd the greatest common
divisor of two positive integers using the Euclidean algorithm does not exceed ﬁve times
the number of decimal digits in the smaller of the two integers. (This was proved by
Gabriel Lam´e (1795–1870).) (See [BaSh96] or [Ro10] for a proof.)
3. The Euclidean algorithm ﬁnds the greatest common divisor of the Fibonacci numbers
(§3.1.2) Fn+1 and Fn+2 (where n is a positive integer) using exactly n division steps. If
the Euclidean algorithm uses exactly n division steps to ﬁnd the greatest common divisor
of the positive integers a and b (with a < b), then a ≥Fn+1 and b ≥Fn+2.
4. The Euclidean algorithm uses O((log b)3) bit operations to ﬁnd the greatest common
divisor of two integers a and b with a < b.
5. The Euclidean algorithm uses O(Lg a·Lg b) bit operations to ﬁnd the greatest common
divisor of two integers a and b.
6. Least remainder Euclidean algorithm: The greatest common divisor of two integers a
and b (with a < b) can be found by replacing a and b with a and the least remainder
of b when divided by a. (The least remainder of b when divided by a is the integer of
smallest absolute value congruent to b modulo a. It equals b mod a if b mod a ≤a
2, and
(b mod a) −a if b mod a > a
2.) Repeating this procedure until a remainder of zero is
reached produces the greatest common divisor of a and b as the last nonzero remainder.
7. The number of divisions used by the least remainder Euclidean algorithm to ﬁnd the
greatest common divisor of two integers is less than or equal to the number of divisions
used by the Euclidean algorithm to ﬁnd this greatest common divisor.
8. Binary greatest common divisor algorithm:
The greatest common divisor of two
integers a and b can also be found using an algorithm known as the binary greatest
common divisor algorithm. It is based on the following reductions: if a and b are both
even, then gcd(a, b) = 2 gcd( a
2, b
2); if a is even and b is odd, then gcd(a, b) = gcd( a
2, b)
(and if a is odd and b is even, switch them); and if a and b are both odd, then gcd(a, b) =
gcd( |a−b|
2
, b). To stop, the algorithm uses the rule that gcd(a, a) = a.
9. Extended Euclidean algorithm:
The extended Euclidean algorithm ﬁnds gcd(a, b)
and expresses it in the form gcd(a, b) = ma + nb for some integers m and n. The two-
pass version proceeds by ﬁrst working through the steps of the Euclidean algorithm to
ﬁnd gcd(a, b), and then working backwards through the steps to express gcd(a, b) as a
linear combination of each pair of successive remainders until the original integers a and b
are reached. The one-pass version of this algorithm keeps track of how each successive
remainder can be expressed as a linear combination of successive remainders. When the
last step is reached both gcd(a, b) and integers m and n with gcd(a, b) = ma + nb are
produced. The one-pass version is displayed as Algorithm 2.
Algorithm 2:
The extended Euclidean algorithm.
procedure gcdex(a, b: positive integers)
r0 := a; r1 := b
m0 := 1; m1 = 0
n0 := 0; n1 := 1
i := 1
while ri ̸= 0
ri+1 := ri−1 mod ri
mi+1 := mi−1 −
 ri−1
ri

mi

Section 4.3
CONGRUENCES
235
ni+1 := ni−1 −
 ri−1
ri

ni
i := i + 1
{gcd(a, b) is ri−1 and gcd(a, b) = mi−1a + ni−1b}
Examples:
1. When the Euclidean algorithm is used to ﬁnd gcd(53, 77), the following steps result:
77 = 1 · 53 + 24,
53 = 2 · 24 + 5,
24 = 4 · 5 + 4,
5 = 1 · 4 + 1,
4 = 4 · 1.
This shows that gcd(53, 77) = 1. Working backwards through these steps to perform the
two-pass version of the Euclidean algorithm gives
1 = 5 −1 · 4
= 5 −1 · (24 −4 · 5) = 5 · 5 −1 · 24
= 5 · (53 −2 · 24) −1 · 24 = 5 · 53 −11 · 24
= 5 · 53 −11 · (77 −1 · 53) = 16 · 53 −11 · 77.
2. The steps of the least-remainder algorithm when used to compute gcd(57, 93) are
gcd(57, 93) = gcd(57, 21) = gcd(21, 6) = gcd(6, 3) = 3.
3. The steps of the binary GCD algorithm when used to compute gcd(108, 194) are
gcd(108, 194) = 2 · gcd(54, 97) = 2 · gcd(27, 97) = 2 · gcd(27, 35)
= 2 · gcd(4, 35) = 2 · gcd(2, 35) = 2 · gcd(1, 35) = 2.
4.3
CONGRUENCES
4.3.1
INTRODUCTION
Deﬁnitions:
If m is a positive integer and a and b are integers, then a is congruent to b modulo m,
written a ≡b (mod m), if m divides a −b. If m does not divide a −b, a and b are
incongruent modulo m, written a ̸≡b (mod m).
A complete system of residues modulo m is a set of integers such that every integer
is congruent modulo m to exactly one of the integers in the set.
If m is a positive integer and a is an integer with a = bm + r, where 0 ≤r ≤m −1, then
r is the least nonnegative residue of a modulo m. When a is not divisible by m, r
is the least positive residue of a modulo m.

236
Chapter 4
NUMBER THEORY
The congruence class of a modulo m is the set of integers congruent to a modulo m
and is written [a]m. Any integer in [a]m is called a representative of this class.
If m is a positive integer and a is an integer relatively prime to m, then a is an inverse
of a modulo m if aa ≡1 (mod m). An inverse of a modulo m is also written a−1 mod m.
If m is a positive integer, then a reduced residue system modulo m is a set of
integers such that every integer relatively prime to m is congruent modulo m to exactly
one integer in the set.
If m is a positive integer, the set of congruence classes modulo m is written Zm. (See
§5.2.1.)
If m is a positive integer greater than 1, the set of congruence classes of elements relatively
prime to m is written Z⋆
m; that is, Z⋆
m = {[a]m ∈Zm | gcd(a, n) = 1}. (See §5.2.1.)
Facts:
1. If m is a positive integer and a, b, and c are integers, then
• a ≡a (mod m);
• a ≡b (mod m) if and only if b ≡a (mod m);
• if a ≡b (mod m) and b ≡c (mod m), then a ≡c (mod m).
Consequently, congruence modulo m is an equivalence relation. (See §1.4.2 and §5.2.1.)
2. If m is a positive integer and a is an integer, then m divides a if and only if a ≡
0 (mod m).
3. If m is a positive integer and a and b are integers with a ≡b (mod m), then
gcd(a, m) = gcd(b, m).
4. If a, b, c, and m are integers with m positive and a ≡b (mod m), then a + c ≡
b + c (mod m), a −c ≡b −c (mod m), and ac ≡bc (mod m).
5. If m is a positive integer and a, b, c, and d are integers with a ≡b (mod m) and
c ≡d (mod m), then ac ≡bd (mod m).
6. If a, b, c, and m are integers, m is positive, d = gcd(c, m), and ac ≡bc (mod m), then
a ≡b (mod m
d ).
7. If a, b, c, and m are integers, m is positive, and c and m are relatively prime, and
ac ≡bc (mod m), then a ≡b (mod m).
8. If a, b, k, and m are integers with k and m positive and a ≡b (mod m), then
ak ≡bk (mod m).
9. Suppose a, b, and m are integers with a ≡b (mod m). If c is an integer, it does not
necessarily follow that ca ≡cb (mod m).
10. If f(x1, . . . , xn) is a polynomial with integer coeﬃcients and a1, . . . , an, b1, . . . , bn
are integers with ai ≡bi (mod m) for all i, then f(a1, . . . , an) ≡f(b1, . . . , bn) (mod m).
11. If a, b, and mi are integers with mi positive and a ≡b (mod mi) for i = 1, 2, . . . , k,
then a ≡b (mod lcm(m1, m2, . . . , mk)).
12. If a and b are integers, mi (i = 1, 2, . . . , k) are pairwise relatively prime positive
integers, and a ≡b (mod mi) for i = 1, 2, . . . , k, then a ≡b (mod m1m2 . . . mk).
13. The congruence class [a]m is the set of integers {a, a ± m, a ± 2m, . . . }.
If a ≡
b (mod m), then [a]m = [b]m. The congruence classes modulo m are the equivalence
classes of the congruence modulo m equivalence relation. (See §5.2.1.)

Section 4.3
CONGRUENCES
237
14. Addition, subtraction, and multiplication of congruence classes modulo m, where
m is a positive integer, are deﬁned by [a]m + [b]m = [a + b]m, [a]m −[b]m = [a −b]m,
and [a]m[b]m = [ab]m. Each of these operations is well deﬁned, in the sense that using
representatives of the congruence classes other than a and b does not change the resulting
congruence class.
15. If m is a positive integer, then (Zn, +), where + is the operation of addition of
congruence classes deﬁned in Fact 14 and in §5.2.1, is an abelian group. The identity
element in this group is [0]m and the inverse of [a]m is [−a]m = [m −a]m.
16. If m is a positive integer greater than 1 and a is relatively prime to m, then a has
an inverse modulo m.
17. An inverse of a modulo m, where m is a positive integer and gcd(a, m) = 1, may
be found by using the extended Euclidean algorithm to ﬁnd integers x and y such that
ax + my = 1, which implies that x is an inverse of a modulo m.
18. If m is a positive integer, then (Z⋆
m, ·), where · is the multiplication operation on
congruence classes, is an abelian group. (See §5.2.1.) The identity element of this group
is [1]m and the inverse of the class [a]m is the class [a]m, where a is an inverse of a modulo
m.
19. If ai (i = 1, . . . , m) is a complete residue system modulo m, where m is a positive
integer, and r and s are integers with gcd(m, r) = 1, then rai + s is a complete system
of residues modulo m.
20. If a and b are integers and m is a positive integer with 0 ≤a < m and 0 ≤b < m,
then (a + b) mod m = a + b if a + b < m, and (a + b) mod m = a + b −m if a + b ≥m.
21. Computing the least positive residue modulo m of powers of integers is important in
cryptology (see Chapter 15). An eﬃcient algorithm for computing bn mod m where n is
a positive integer with binary expansion n = (ak−1 . . . a1a0)2 is to ﬁnd the least positive
residues of b, b2, b4, . . . , b2k−1 modulo m by successively squaring and reducing modulo m,
multiplying together the least positive residues modulo m of b2j for those j with aj = 1,
reducing modulo m after each multiplication.
22. Wilson’s theorem: If p is prime, then (p −1)! ≡−1 (mod p).
23. If n is a positive integer greater than 1 such that (n −1)! ≡−1 (mod n) then n is
prime.
24. Fermat’s little theorem: If p is a prime and a is an integer not divisible by p then
ap−1 ≡1 (mod p).
25. Euler’s theorem: If m is a positive integer and a is an integer relatively prime to m,
then aφ(m) ≡1 (mod m), where φ(m) is the number of positive integers not exceeding m
that are relatively prime to m.
26. If a is an integer and p is a prime that does not divide a, then from Fermat’s little
theorem it follows that ap−2 is an inverse of a modulo p.
27. If a and m are relatively prime integers with m > 1, then aφ(m)−1 is an inverse of a
modulo m. This follows directly from Euler’s theorem.
28. Linear congruential method:
One of the most commonly used methods for gener-
ating pseudo-random numbers is the linear congruential method. It starts with integers
m, a, c, and x0 where 2 ≤a < m, 0 ≤c < m, and 0 ≤x0 ≤m. The sequence of
pseudo-random numbers is deﬁned recursively by
xn+1 = (axn + c) mod m,
n = 0, 1, 2, 3, . . . .
Here m is the modulus, a is the multiplier, c is the increment, and x0 is the seed of the
generator.

238
Chapter 4
NUMBER THEORY
29. Big-oh estimates for the number of bit operations required to do modular addition,
modular subtraction, modular multiplication, modular inversion, and modular exponen-
tiation are summarized in the following table.
name
operation
number of
bit operations
modular addition
(a + b) mod m
O(log m)
modular subtraction
(a −b) mod m
O(log m)
modular multiplication
(a · b) mod m
O(log2 m)
modular inversion
(a−1) mod m
O(log2 m)
modular exponentiation
ak mod m, k < m
O(log3 m)
Examples:
1. 23 ≡5 (mod 9), −17 ≡13 (mod 15), and 99 ≡0 (mod 11), but 11 ̸≡3 (mod 5),
−3 ̸≡8 (mod 6), and 44 ̸≡0 (mod 7).
2. To ﬁnd an inverse of 53 modulo 77, use the extended Euclidean algorithm to obtain
16 · 53 −11 · 77 = 1 (see Example 1 of §4.2.2). This implies that 16 is an inverse of 53
modulo 77.
3. Since 11 is prime, by Wilson’s theorem it follows that 10! ≡−1 (mod 11).
4. 5! ≡0 (mod 6), which provides an impractical veriﬁcation that 6 is not prime.
5. To ﬁnd the least positive residue of 3201 modulo 11, note that by Fermat’s little
theorem 310 ≡1 (mod 11). Hence 3201 = (310)20 · 3 ≡3 (mod 11).
6. Zeller’s congruence:
A congruence can be used to determine the day of the week
of any date in the Gregorian calendar, the calendar used in most of the world.
Let
w represent the day of the week, with w = 0, 1, 2, 3, 4, 5, 6 for Sunday, Monday, Tues-
day, Wednesday, Thursday, Friday, Saturday, respectively. Let k represent the day of
the month. Let m represent the month with m = 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 for Jan-
uary, February, March, April, May, June, July, August, September, October, November,
December, respectively. Let N represent the previous year if the month is January or
February or the current year otherwise, with C the century of N and Y the particular
year of the century of N so that N = 100Y + C. Then the day of the week can be found
using the congruence
w ≡k + ⌊2.6m −0.2⌋−2C + Y +
 Y
4

+
 C
4

(mod 7).
7. January 1, 1900 was a Monday. This follows by Zeller’s congruence with C = 18,
Y = 99, m = 11, and k = 1, noting that to apply this congruence January is considered
the eleventh month of the preceding year.
4.3.2
LINEAR AND POLYNOMIAL CONGRUENCES
Deﬁnitions:
A linear congruence in one variable is a congruence of the form ax ≡b (mod m),
where a, b, and m are integers, m is positive, and x is an unknown.
If f is a polynomial with integer coeﬃcients, an integer r is a solution of the congruence
f(x) ≡0 (mod m), or a root of f(x) modulo m, if f(r) ≡0 (mod m).

Section 4.3
CONGRUENCES
239
Facts:
1. If a, b, and m are integers, m is positive, and gcd(a, m) = d, then the congruence
ax ≡b (mod m) has exactly d incongruent solutions modulo m if d|b, and no solutions
if d̸ | b.
2. If a, b, and m are integers, m is positive, and gcd(a, m) = 1, then the solutions of
ax ≡b (mod m) are all integers x with x ≡ab (mod m).
3. If a and b are positive integers and p is a prime that does not divide a, then the
solutions of ax ≡b (mod p) are the integers x with x ≡ap−2b (mod p).
4. Thue’s lemma:
If p is a prime and a is an integer not divisible by p, then the
congruence ax ≡y (mod p) has a solution x0, y0 with 0 < |x0| < √p, 0 < |y0| < √p.
5. Chinese remainder theorem:
If mi, i = 1, 2, . . . , r, are pairwise relatively prime
positive integers, then the system of simultaneous congruences x ≡ai (mod mi), i =
1, 2, . . ., r, has a unique solution modulo M = m1m2 . . . mr which is given by x ≡
a1M1y1 +a2M2y2 +· · ·+arMryr where Mk = M
mk and yk is an inverse of Mk modulo mk,
k = 1, 2, . . . , r.
6. Problems involving the solution of a system of simultaneous congruences arose in the
writing of ancient mathematicians, including the Chinese mathematician Sun-Tsu, and
in other works by Indian and Greek mathematicians. (See [Di71] for details.)
7. The system of simultaneous congruences x ≡ai (mod mi), i = 1, 2, . . ., r, has a
solution if and only if gcd(mi, mj) divides ai −aj for all pairs of integers (i, j) with
1 ≤i < j ≤r. If a solution exists, it is unique modulo lcm(m1, m2, . . . , mr).
8. If a, b, c, d, e, f, and m are integers with m positive such that gcd(ad −bc, m) = 1,
then the system of congruences ax+ by ≡e (mod m), cx+ dy ≡f (mod m) has a unique
solution given by x ≡g(de−bf) (mod m), y ≡g(af −ce) (mod m) where g is an inverse
of ad −bc modulo m.
9. Lagrange’s theorem: If p is prime, then the polynomial f(x) = anxn + · · ·+ a1x+ a0
with an ̸≡0 (mod p) has at most n roots modulo p.
10. If f(x) = anxn + · · · + a1x + a0, where ai (i = 1, . . . , n) is an integer and p is prime,
has more than n roots modulo p, then p divides ai for all i = 1, . . . , n.
11. If m1, m2, . . . , mr are pairwise relatively prime positive integers with product m =
m1m2 . . . mr, and f is a polynomial with integer coeﬃcients, then f(x) has a root modulo
m if and only if f(x) has a root modulo mi, for all i = 1, 2, . . . , r. Furthermore, if f(x) has
ni incongruent roots modulo mi and n incongruent roots modulo m, then n = n1n2 . . . nr.
12. If p is prime, k is a positive integer, and s is a root of f(x) modulo pk, then
• if p ̸ |f ′(s), then there is a unique root t of f(x) modulo pk+1 with t ≡s (mod pk),
namely t = s+pku where u is the unique solution of f ′(s)u ≡−f(s)/pk (mod p);
• if p|f ′(s) and pk+1|f(s), then there are exactly p incongruent roots of f(x) modulo
pk+1 congruent to s modulo p, given by s + pki, i = 0, 1, . . . , p −1;
• if p|f ′(s) and pk+1̸ | f(s), then there are no roots of f(x) modulo pk+1 that are
congruent to s modulo pk.
13. Finding roots of a polynomial modulo m, where m is a positive integer: First ﬁnd
roots of the polynomial modulo pr for each prime power in the prime-power factorization
of m (Fact 14) and then use the Chinese remainder theorem (Fact 5) to ﬁnd solutions
modulo m.
14. Finding solutions modulo pr reduces to ﬁrst ﬁnding solutions modulo p. In particu-
lar, if there are no roots of f(x) modulo p, there are no roots of f(x) modulo pr. If f(x)

240
Chapter 4
NUMBER THEORY
has roots modulo p, choose one, say r with 0 ≤r < p. By Fact 12, corresponding to r
there are 0, 1, or p roots of f(x) modulo p2.
Examples:
1. There are three incongruent solutions of 6x ≡9 (mod 15) since gcd(6, 15) = 3 and
3|9. The solutions are those integers x with x ≡4, 9, or 14 (mod 15).
2. The linear congruence 2x ≡7 (mod 6) has no solutions since gcd(2, 6) = 2 and 2̸ | 7.
3. The solutions of the linear congruence 3x ≡5 (mod 11) are those integers x with
x ≡3 · 5 ≡4 · 5 ≡9 (mod 11).
4. It follows from the Chinese remainder theorem (Fact 5) that the solutions of the
systems of simultaneous congruences x ≡1 (mod 3), x ≡2 (mod 4), and x ≡3 (mod 5)
are all integers x with x ≡1 · 20 · 2 + 2 · 15 · 3 + 3 · 12 · 3 ≡58 (mod 60).
5. The simultaneous congruences x ≡4 (mod 9) and x ≡7 (mod 15) can be solved
by noting that the ﬁrst congruence implies that x −4 = 9t for some integer t, so that
x = 9t + 4. Inserting this expression for x into the second congruence gives 9t + 4 ≡
7 (mod 15). This implies that 3t ≡1 (mod 5), so that t ≡2 (mod 5) and t = 5u + 2
for some integer u. Hence x = 45u + 22 for some integer u. The solutions of the two
simultaneous congruences are those integers x with x ≡22 (mod 45).
4.4
PRIME NUMBERS
One of the most powerful tools in number theory is the fact that each composite integer
can be decomposed into a product of primes. Primes may be thought of as the building
blocks of the integers in the sense that they can be decomposed only in trivial ways, for
example, 3 = 1 × 3. Prime numbers, once of only theoretical interest, now are important
in many applications, especially in the area of cryptography where large primes play a
crucial role in the area of public key cryptosystems (see Chapter 15). From ancient to
modern times, mathematicians have devoted long hours to the study of primes and their
properties. Even so, many questions about primes have only partially been answered
or remain complete mysteries, including questions that ask whether there are inﬁnitely
many primes of certain forms.
There have been many recent discoveries concerning
prime numbers, such as the discovery of new Mersenne primes or the proof that inﬁnitely
often there are pairs of primes at most a bounded distance apart.. The current state of
knowledge on some of these questions and the latest discoveries are described in this
section.
Additional information about primes can be found in [CrPo05] and [Ri96], as well as the
Prime Pages website http://www.utm.edu/research/primes/.
4.4.1
BASIC CONCEPTS
Deﬁnitions:
A prime is a natural number greater than 1 that is exactly divisible only by 1 and itself.
A composite is a natural number greater than 1 that is not a prime. That is, a composite
may be factored into the product of two natural numbers both smaller than itself.

Section 4.4
PRIME NUMBERS
241
Facts:
1. The number 1 is not considered to be prime.
2. The following table lists the primes less than 10,000. The prime number p10n+k is
found by looking at the row beginning with n.. and at the column beginning with ..k.
..0
..1
..2
..3
..4
..5
..6
..7
..8
..9
2
3
5
7
11
13
17
19
23
1..
29
31
37
41
43
47
53
59
61
67
2..
71
73
79
83
89
97
101
103
107
109
3..
113
127
131
137
139
149
151
157
163
167
4..
173
179
181
191
193
197
199
211
223
227
5..
229
233
239
241
251
257
263
269
271
277
6..
281
283
293
307
311
313
317
331
337
347
7..
349
353
359
367
373
379
383
389
397
401
8..
409
419
421
431
433
439
443
449
457
461
9..
463
467
479
487
491
499
503
509
521
523
10..
541
547
557
563
569
571
577
587
593
599
11..
601
607
613
617
619
631
641
643
647
653
12..
659
661
673
677
683
691
701
709
719
727
13..
733
739
743
751
757
761
769
773
787
797
14..
809
811
821
823
827
829
839
853
857
859
15..
863
877
881
883
887
907
911
919
929
937
16..
941
947
953
967
971
977
983
991
997
1009
17..
1013
1019
1021
1031
1033
1039
1049
1051
1061
1063
18..
1069
1087
1091
1093
1097
1103
1109
1117
1123
1129
19..
1151
1153
1163
1171
1181
1187
1193
1201
1213
1217
20..
1223
1229
1231
1237
1249
1259
1277
1279
1283
1289
21..
1291
1297
1301
1303
1307
1319
1321
1327
1361
1367
22..
1373
1381
1399
1409
1423
1427
1429
1433
1439
1447
23..
1451
1453
1459
1471
1481
1483
1487
1489
1493
1499
24..
1511
1523
1531
1543
1549
1553
1559
1567
1571
1579
25..
1583
1597
1601
1607
1609
1613
1619
1621
1627
1637
26..
1657
1663
1667
1669
1693
1697
1699
1709
1721
1723
27..
1733
1741
1747
1753
1759
1777
1783
1787
1789
1801
28..
1811
1823
1831
1847
1861
1867
1871
1873
1877
1879
29..
1889
1901
1907
1913
1931
1933
1949
1951
1973
1979
30..
1987
1993
1997
1999
2003
2011
2017
2027
2029
2039
31..
2053
2063
2069
2081
2083
2087
2089
2099
2111
2113
32..
2129
2131
2137
2141
2143
2153
2161
2179
2203
2207

242
Chapter 4
NUMBER THEORY
..0
..1
..2
..3
..4
..5
..6
..7
..8
..9
33..
2213
2221
2237
2239
2243
2251
2267
2269
2273
2281
34..
2287
2293
2297
2309
2311
2333
2339
2341
2347
2351
35..
2357
2371
2377
2381
2383
2389
2393
2399
2411
2417
36..
2423
2437
2441
2447
2459
2467
2473
2477
2503
2521
37..
2531
2539
2543
2549
2551
2557
2579
2591
2593
2609
38..
2617
2621
2633
2647
2657
2659
2663
2671
2677
2683
39..
2687
2689
2693
2699
2707
2711
2713
2719
2729
2731
40..
2741
2749
2753
2767
2777
2789
2791
2797
2801
2803
41..
2819
2833
2837
2843
2851
2857
2861
2879
2887
2897
42..
2903
2909
2917
2927
2939
2953
2957
2963
2969
2971
43..
2999
3001
3011
3019
3023
3037
3041
3049
3061
3067
44..
3079
3083
3089
3109
3119
3121
3137
3163
3167
3169
45..
3181
3187
3191
3203
3209
3217
3221
3229
3251
3253
46..
3257
3259
3271
3299
3301
3307
3313
3319
3323
3329
47..
3331
3343
3347
3359
3361
3371
3373
3389
3391
3407
48..
3413
3433
3449
3457
3461
3463
3467
3469
3491
3499
49..
3511
3517
3527
3529
3533
3539
3541
3547
3557
3559
50..
3571
3581
3583
3593
3607
3613
3617
3623
3631
3637
51..
3643
3659
3671
3673
3677
3691
3697
3701
3709
3719
52..
3727
3733
3739
3761
3767
3769
3779
3793
3797
3803
53..
3821
3823
3833
3847
3851
3853
3863
3877
3881
3889
54..
3907
3911
3917
3919
3923
3929
3931
3943
3947
3967
55..
3989
4001
4003
4007
4013
4019
4021
4027
4049
4051
56..
4057
4073
4079
4091
4093
4099
4111
4127
4129
4133
57..
4139
4153
4157
4159
4177
4201
4211
4217
4219
4229
58..
4231
4241
4243
4253
4259
4261
4271
4273
4283
4289
59..
4297
4327
4337
4339
4349
4357
4363
4373
4391
4397
60..
4409
4421
4423
4441
4447
4451
4457
4463
4481
4483
61..
4493
4507
4513
4517
4519
4523
4547
4549
4561
4567
62..
4583
4591
4597
4603
4621
4637
4639
4643
4649
4651
63..
4657
4663
4673
4679
4691
4703
4721
4723
4729
4733
64..
4751
4759
4783
4787
4789
4793
4799
4801
4813
4817
65..
4831
4861
4871
4877
4889
4903
4909
4919
4931
4933
66..
4937
4943
4951
4957
4967
4969
4973
4987
4993
4999
67..
5003
5009
5011
5021
5023
5039
5051
5059
5077
5081
68..
5087
5099
5101
5107
5113
5119
5147
5153
5167
5171

Section 4.4
PRIME NUMBERS
243
..0
..1
..2
..3
..4
..5
..6
..7
..8
..9
69..
5179
5189
5197
5209
5227
5231
5233
5237
5261
5273
70..
5279
5281
5297
5303
5309
5323
5333
5347
5351
5381
71..
5387
5393
5399
5407
5413
5417
5419
5431
5437
5441
72..
5443
5449
5471
5477
5479
5483
5501
5503
5507
5519
73..
5521
5527
5531
5557
5563
5569
5573
5581
5591
5623
74..
5639
5641
5647
5651
5653
5657
5659
5669
5683
5689
75..
5693
5701
5711
5717
5737
5741
5743
5749
5779
5783
76..
5791
5801
5807
5813
5821
5827
5839
5843
5849
5851
77..
5857
5861
5867
5869
5879
5881
5897
5903
5923
5927
78..
5939
5953
5981
5987
6007
6011
6029
6037
6043
6047
79..
6053
6067
6073
6079
6089
6091
6101
6113
6121
6131
80..
6133
6143
6151
6163
6173
6197
6199
6203
6211
6217
81..
6221
6229
6247
6257
6263
6269
6271
6277
6287
6299
82..
6301
6311
6317
6323
6329
6337
6343
6353
6359
6361
83..
6367
6373
6379
6389
6397
6421
6427
6449
6451
6469
84..
6473
6481
6491
6521
6529
6547
6551
6553
6563
6569
85..
6571
6577
6581
6599
6607
6619
6637
6653
6659
6661
86..
6673
6679
6689
6691
6701
6703
6709
6719
6733
6737
87..
6761
6763
6779
6781
6791
6793
6803
6823
6827
6829
88..
6833
6841
6857
6863
6869
6871
6883
6899
6907
6911
89..
6917
6947
6949
6959
6961
6967
6971
6977
6983
6991
90..
6997
7001
7013
7019
7027
7039
7043
7057
7069
7079
91..
7103
7109
7121
7127
7129
7151
7159
7177
7187
7193
92..
7207
7211
7213
7219
7229
7237
7243
7247
7253
7283
93..
7297
7307
7309
7321
7331
7333
7349
7351
7369
7393
94..
7411
7417
7433
7451
7457
7459
7477
7481
7487
7489
95..
7499
7507
7517
7523
7529
7537
7541
7547
7549
7559
96..
7561
7573
7577
7583
7589
7591
7603
7607
7621
7639
97..
7643
7649
7669
7673
7681
7687
7691
7699
7703
7717
98..
7723
7727
7741
7753
7757
7759
7789
7793
7817
7823
99..
7829
7841
7853
7867
7873
7877
7879
7883
7901
7907
100..
7919
7927
7933
7937
7949
7951
7963
7993
8009
8011
101..
8017
8039
8053
8059
8069
8081
8087
8089
8093
8101
102..
8111
8117
8123
8147
8161
8167
8171
8179
8191
8209
103..
8219
8221
8231
8233
8237
8243
8263
8269
8273
8287
104..
8291
8293
8297
8311
8317
8329
8353
8363
8369
8377

244
Chapter 4
NUMBER THEORY
..0
..1
..2
..3
..4
..5
..6
..7
..8
..9
105..
8387
8389
8419
8423
8429
8431
8443
8447
8461
8467
106..
8501
8513
8521
8527
8537
8539
8543
8563
8573
8581
107..
8597
8599
8609
8623
8627
8629
8641
8647
8663
8669
108..
8677
8681
8689
8693
8699
8707
8713
8719
8731
8737
109..
8741
8747
8753
8761
8779
8783
8803
8807
8819
8821
110..
8831
8837
8839
8849
8861
8863
8867
8887
8893
8923
111..
8929
8933
8941
8951
8963
8969
8971
8999
9001
9007
112..
9011
9013
9029
9041
9043
9049
9059
9067
9091
9103
113..
9109
9127
9133
9137
9151
9157
9161
9173
9181
9187
114..
9199
9203
9209
9221
9227
9239
9241
9257
9277
9281
115..
9283
9293
9311
9319
9323
9337
9341
9343
9349
9371
116..
9377
9391
9397
9403
9413
9419
9421
9431
9433
9437
117..
9439
9461
9463
9467
9473
9479
9491
9497
9511
9521
118..
9533
9539
9547
9551
9587
9601
9613
9619
9623
9629
119..
9631
9643
9649
9661
9677
9679
9689
9697
9719
9721
120..
9733
9739
9743
9749
9767
9769
9781
9787
9791
9803
121..
9811
9817
9829
9833
9839
9851
9857
9859
9871
9883
122..
9887
9901
9907
9923
9929
9931
9941
9949
9967
9973
3. Fundamental theorem of arithmetic: Every natural number greater than 1 is either
prime or can be written as a product of prime factors in a unique way, up to the order
of the prime factors.
That is, every composite n can be expressed uniquely as n =
p1p2 . . . pk, where p1 ≤p2 ≤· · · ≤pk are primes. This is sometimes also known as the
unique factorization theorem.
4. The unique factorization of a positive integer n formed by grouping together equal
prime factors produces the unique prime-power factorization n = pa1
1 pa2
2 . . . pak
k .
5. The following table lists the prime-power factorization of all positive integers below
2,500. Numbers appearing in boldface are prime.
0
1
2
3
4
5
6
7
8
9
0
2
3
22
5
2·3
7
23
32
1
2·5
11
22·3
13
2·7
3·5
24
17
2·32
19
2
22·5
3·7
2·11
23
23·3
52
2·13
33
22·7
29
3
2·3·5
31
25
3·11
2·17
5·7
22·32
37
2·19
3·13
4
23·5
41
2·3·7
43
22·11
32·5
2·23
47
24·3
72
5
2·52
3·17
22·13
53
2·33
5·11
23·7
3·19
2·29
59
6
22·3·5
61
2·31
32·7
26
5·13
2·3·11
67
22·17
3·23
7
2·5·7
71
23·32
73
2·37
3·52
22·19
7·11
2·3·13
79
8
24·5
34
2·41
83
22·3·7
5·17
2·43
3·29
23·11
89
9
2·32·5
7·13
22·23
3·31
2·47
5·19
25·3
97
2·72
32·11
10
22·52
101
2·3·17
103
23·13
3·5·7
2·53
107
22·33
109

Section 4.4
PRIME NUMBERS
245
0
1
2
3
4
5
6
7
8
9
11
2·5·11
3·37
24·7
113
2·3·19
5·23
22·29
32·13
2·59
7·17
12
23·3·5
112
2·61
3·41
22·31
53
2·32·7
127
27
3·43
13
2·5·13
131
22·3·11
7·19
2·67
33·5
23·17
137
2·3·23
139
14
22·5·7
3·47
2·71
11·13
24·32
5·29
2·73
3·72
22·37
149
15
2·3·52
151
23·19
32·17
2·7·11
5·31
22·3·13
157
2·79
3·53
16
25·5
7·23
2·34
163
22·41
3·5·11
2·83
167
23·3·7
132
17
2·5·17
32·19
22·43
173
2·3·29
52·7
24·11
3·59
2·89
179
18
22·32·5
181
2·7·13
3·61
23·23
5·37
2·3·31
11·17
22·47
33·7
19
2·5·19
191
26·3
193
2·97
3·5·13
22·72
197
2·32·11
199
20
23·52
3·67
2·101
7·29
22·3·17
5·41
2·103
32·23
24·13
11·19
21
2·3·5·7
211
22·53
3·71
2·107
5·43
23·33
7·31
2·109
3·73
22
22·5·11
13·17
2·3·37
223
25·7
32·52
2·113
227
22·3·19
229
23
2·5·23
3·7·11
23·29
233
2·32·13
5·47
22·59
3·79
2·7·17
239
24
24·3·5
241
2·112
35
22·61
5·72
2·3·41
13·19
23·31
3·83
25
2·53
251
22·32·7
11·23
2·127
3·5·17
28
257
2·3·43
7·37
26
22·5·13
32·29
2·131
263
23·3·11
5·53
2·7·19
3·89
22·67
269
27
2·33·5
271
24·17
3·7·13
2·137
52·11
22·3·23
277
2·139
32·31
28
23·5·7
281
2·3·47
283
22·71
3·5·19
2·11·13
7·41
25·32
172
29
2·5·29
3·97
22·73
293
2·3·72
5·59
23·37
33·11
2·149
13·23
30
22·3·52
7·43
2·151
3·101
24·19
5·61
2·32·17
307
22·7·11
3·103
31
2·5·31
311
23·3·13
313
2·157
32·5·7
22·79
317
2·3·53
11·29
32
26·5
3·107
2·7·23
17·19
22·34
52·13
2·163
3·109
23·41
7·47
33
2·3·5·11
331
22·83
32·37
2·167
5·67
24·3·7
337
2·132
3·113
34
22·5·17
11·31
2·32·19
73
23·43
3·5·23
2·173
347
22·3·29
349
35
2·52·7
33·13
25·11
353
2·3·59
5·71
22·89
3·7·17
2·179
359
36
23·32·5
192
2·181
3·112
22·7·13
5·73
2·3·61
367
24·23
32·41
37
2·5·37
7·53
22·3·31
373
2·11·17
3·53
23·47
13·29
2·33·7
379
38
22·5·19
3·127
2·191
383
27·3
5·7·11
2·193
32·43
22·97
389
39
2·3·5·13
17·23
23·72
3·131
2·197
5·79
22·32·11
397
2·199
3·7·19
40
24·52
401
2·3·67
13·31
22·101
34·5
2·7·29
11·37
23·3·17
409
41
2·5·41
3·137
22·103
7·59
2·32·23
5·83
25·13
3·139
2·11·19
419
42
22·3·5·7
421
2·211
32·47
23·53
52·17
2·3·71
7·61
22·107 3·11·13
43
2·5·43
431
24·33
433
2·7·31
3·5·29
22·109
19·23
2·3·73
439
44
23·5·11
32·72
2·13·17
443
22·3·37
5·89
2·223
3·149
26·7
449
45
2·32·52
11·41
22·113
3·151
2·227
5·7·13
23·3·19
457
2·229
33·17
46
22·5·23
461 2·3·7·11
463
24·29
3·5·31
2·233
467 22·32·13
7·67
47
2·5·47
3·157
23·59
11·43
2·3·79
52·19
22·7·17
32·53
2·239
479
48
25·3·5
13·37
2·241
3·7·23
22·112
5·97
2·35
487
23·61
3·163
49
2·5·72
491
22·3·41
17·29
2·13·19
32·5·11
24·31
7·71
2·3·83
499
50
22·53
3·167
2·251
503
23·32·7
5·101
2·11·23
3·132
22·127
509
51
2·3·5·17
7·73
29
33·19
2·257
5·103
22·3·43
11·47
2·7·37
3·173
52
23·5·13
521
2·32·29
523
22·131
3·52·7
2·263
17·31
24·3·11
232
53
2·5·53
32·59
22·7·19
13·41
2·3·89
5·107
23·67
3·179
2·269
72·11
54
22·33·5
541
2·271
3·181
25·17
5·109
2·3·7·13
547
22·137
32·61
55
2·52·11
19·29
23·3·23
7·79
2·277
3·5·37
22·139
557
2·32·31
13·43
56
24·5·7 3·11·17
2·281
563
22·3·47
5·113
2·283
34·7
23·71
569
57
2·3·5·19
571 22·11·13
3·191
2·7·41
52·23
26·32
577
2·172
3·193

246
Chapter 4
NUMBER THEORY
0
1
2
3
4
5
6
7
8
9
58
22·5·29
7·83
2·3·97
11·53
23·73
32·5·13
2·293
587
22·3·72
19·31
59
2·5·59
3·197
24·37
593
2·33·11
5·7·17
22·149
3·199
2·13·23
599
60
23·3·52
601
2·7·43
32·67
22·151
5·112
2·3·101
607
25·19
3·7·29
61
2·5·61
13·47 22·32·17
613
2·307
3·5·41
23·7·11
617
2·3·103
619
62
22·5·31
33·23
2·311
7·89
24·3·13
54
2·313 3·11·19
22·157
17·37
63
2·32·5·7
631
23·79
3·211
2·317
5·127
22·3·53
72·13
2·11·29
32·71
64
27·5
641
2·3·107
643
22·7·23
3·5·43
2·17·19
647
23·34
11·59
65
2·52·13
3·7·31
22·163
653
2·3·109
5·131
24·41
32·73
2·7·47
659
66
22·3·5·11
661
2·331 3·13·17
23·83
5·7·19
2·32·37
23·29
22·167
3·223
67
2·5·67
11·61
25·3·7
673
2·337
33·52
22·132
677
2·3·113
7·97
68
23·5·17
3·227
2·11·31
683 22·32·19
5·137
2·73
3·229
24·43
13·53
69
2·3·5·23
691
22·173 32·7·11
2·347
5·139
23·3·29
17·41
2·349
3·233
70
22·52·7
701
2·33·13
19·37
26·11
3·5·47
2·353
7·101
22·3·59
709
71
2·5·71
32·79
23·89
23·31 2·3·7·17
5·11·13
22·179
3·239
2·359
719
72
24·32·5
7·103
2·192
3·241
22·181
52·29
2·3·112
727
23·7·13
36
73
2·5·73
17·43
22·3·61
733
2·367
3·5·72
25·23
11·67
2·32·41
739
74
22·5·37 3·13·19
2·7·53
743
23·3·31
5·149
2·373
32·83 22·11·17
7·107
75
2·3·53
751
24·47
3·251
2·13·29
5·151
22·33·7
757
2·379 3·11·23
76
23·5·19
761
2·3·127
7·109
22·191
32·5·17
2·383
13·59
28·3
769
77
2·5·7·11
3·257
22·193
773
2·32·43
52·31
23·97
3·7·37
2·389
19·41
78
22·3·5·13
11·71
2·17·23
33·29
24·72
5·157
2·3·131
787
22·197
3·263
79
2·5·79
7·113 23·32·11
13·61
2·397
3·5·53
22·199
797 2·3·7·19
17·47
80
25·52
32·89
2·401
11·73
22·3·67
5·7·23
2·13·31
3·269
23·101
809
81
2·34·5
811
22·7·29
3·271
2·11·37
5·163
24·3·17
19·43
2·409 32·7·13
82
22·5·41
821
2·3·137
823
23·103
3·52·11
2·7·59
827 22·32·23
829
83
2·5·83
3·277
26·13
72·17
2·3·139
5·167
22·11·19
33·31
2·419
839
84
23·3·5·7
292
2·421
3·281
22·211
5·132
2·32·47
7·112
24·53
3·283
85
2·52·17
23·37
22·3·71
853
2·7·61
32·5·19
23·107
857 2·3·11·13
859
86
22·5·43
3·7·41
2·431
863
25·33
5·173
2·433
3·172
22·7·31
11·79
87
2·3·5·29
13·67
23·109
32·97
2·19·23
53·7
22·3·73
877
2·439
3·293
88
24·5·11
881
2·32·72
883 22·13·17
3·5·59
2·443
887
23·3·37
7·127
89
2·5·89
34·11
22·223
19·47
2·3·149
5·179
27·7 3·13·23
2·449
29·31
90
22·32·52
17·53
2·11·41
3·7·43
23·113
5·181
2·3·151
907
22·227 32·101
91
2·5·7·13
911
24·3·19
11·83
2·457
3·5·61
22·229
7·131
2·33·17
919
92
23·5·23
3·307
2·461
13·71 22·3·7·11
52·37
2·463
32·103
25·29
929
93
2·3·5·31
72·19
22·233
3·311
2·467
5·11·17
23·32·13
937
2·7·67
3·313
94
22·5·47
941
2·3·157
23·41
24·59
33·5·7
2·11·43
947
22·3·79
13·73
95
2·52·19
3·317
23·7·17
953
2·32·53
5·191
22·239 3·11·29
2·479
7·137
96
26·3·5
312
2·13·37
32·107
22·241
5·193
2·3·7·23
967
23·112 3·17·19
97
2·5·97
971
22·35
7·139
2·487
3·52·13
24·61
977
2·3·163
11·89
98
22·5·72
32·109
2·491
983
23·3·41
5·197
2·17·29
3·7·47 22·13·19
23·43
99
2·32·5·11
991
25·31
3·331
2·7·71
5·199
22·3·83
997
2·499
33·37
100
23·53 7·11·13
2·3·167
17·59
22·251
3·5·67
2·503
19·53
24·32·7
1009
101
2·5·101
3·337 22·11·23
1013
2·3·132
5·7·29
23·127
32·113
2·509
1019
102 22·3·5·17
1021
2·7·73 3·11·31
210
52·41
2·33·19
13·79
22·257
3·73
103
2·5·103
1031
23·3·43
1033
2·11·47
32·5·23
22·7·37
17·61
2·3·173
1039
104
24·5·13
3·347
2·521
7·149 22·32·29
5·11·19
2·523
3·349
23·131
1049

Section 4.4
PRIME NUMBERS
247
0
1
2
3
4
5
6
7
8
9
105
2·3·52·7
1051
22·263
34·13
2·17·31
5·211
25·3·11
7·151
2·232
3·353
106
22·5·53
1061
2·32·59
1063
23·7·19
3·5·71
2·13·41
11·97
22·3·89
1069
107
2·5·107 32·7·17
24·67
29·37
2·3·179
52·43
22·269
3·359
2·72·11
13·83
108
23·33·5
23·47
2·541
3·192
22·271
5·7·31
2·3·181
1087
26·17 32·112
109
2·5·109
1091 22·3·7·13
1093
2·547
3·5·73
23·137
1097
2·32·61
7·157
110
22·52·11
3·367
2·19·29
1103
24·3·23
5·13·17
2·7·79
33·41
22·277
1109
111
2·3·5·37
11·101
23·139
3·7·53
2·557
5·223
22·32·31
1117
2·13·43
3·373
112
25·5·7
19·59 2·3·11·17
1123
22·281
32·53
2·563
72·23
23·3·47
1129
113
2·5·113 3·13·29
22·283
11·103
2·34·7
5·227
24·71
3·379
2·569
17·67
114 22·3·5·19
7·163
2·571
32·127 23·11·13
5·229
2·3·191
31·37
22·7·41
3·383
115
2·52·23
1151
27·32
1153
2·577 3·5·7·11
22·172
13·89
2·3·193
19·61
116
23·5·29
33·43
2·7·83
1163
22·3·97
5·233
2·11·53
3·389
24·73
7·167
117 2·32·5·13
1171
22·293 3·17·23
2·587
52·47
23·3·72
11·107
2·19·31 32·131
118
22·5·59
1181
2·3·197
7·132
25·37
3·5·79
2·593
1187 22·33·11
29·41
119
2·5·7·17
3·397
23·149
1193
2·3·199
5·239
22·13·23 32·7·19
2·599 11·109
120
24·3·52
1201
2·601
3·401
22·7·43
5·241
2·32·67
17·71
23·151 3·13·31
121
2·5·112
7·173 22·3·101
1213
2·607
35·5
26·19
1217 2·3·7·29
23·53
122
22·5·61 3·11·37
2·13·47
1223 23·32·17
52·72
2·613
3·409
22·307
1229
123
2·3·5·41
1231
24·7·11
32·137
2·617
5·13·19
22·3·103
1237
2·619
3·7·59
124
23·5·31
17·73
2·33·23
11·113
22·311
3·5·83
2·7·89
29·43
25·3·13
1249
125
2·54
32·139
22·313
7·179 2·3·11·19
5·251
23·157
3·419
2·17·37
1259
126 22·32·5·7
13·97
2·631
3·421
24·79
5·11·23
2·3·211
7·181
22·317
33·47
127
2·5·127
31·41
23·3·53
19·67
2·72·13
3·52·17
22·11·29
1277
2·32·71
1279
128
28·5
3·7·61
2·641
1283 22·3·107
5·257
2·643 32·11·13
23·7·23
1289
129
2·3·5·43
1291 22·17·19
3·431
2·647
5·7·37
24·34
1297
2·11·59
3·433
130
22·52·13
1301 2·3·7·31
1303
23·163
32·5·29
2·653
1307 22·3·109 7·11·17
131
2·5·131 3·19·23
25·41
13·101
2·32·73
5·263
22·7·47
3·439
2·659
1319
132 23·3·5·11
1321
2·661
33·72
22·331
52·53 2·3·13·17
1327
24·83
3·443
133
2·5·7·19
113 22·32·37
31·43
2·23·29
3·5·89
23·167
7·191
2·3·223 13·103
134
22·5·67
32·149
2·11·61
17·79
26·3·7
5·269
2·673
3·449
22·337
19·71
135
2·33·52
7·193
23·132 3·11·41
2·677
5·271
22·3·113
23·59
2·7·97 32·151
136
24·5·17
1361
2·3·227
29·47 22·11·31 3·5·7·13
2·683
1367 23·32·19
372
137
2·5·137
3·457
22·73
1373
2·3·229
53·11
25·43
34·17
2·13·53
7·197
138 22·3·5·23
1381
2·691
3·461
23·173
5·277 2·32·7·11
19·73
22·347
3·463
139
2·5·139
13·107
24·3·29
7·199
2·17·41
32·5·31
22·349
11·127
2·3·233
1399
140
23·52·7
3·467
2·701
23·61 22·33·13
5·281
2·19·37
3·7·67
27·11
1409
141
2·3·5·47
17·83
22·353
32·157
2·7·101
5·283
23·3·59
13·109
2·709 3·11·43
142
22·5·71
72·29
2·32·79
1423
24·89
3·52·19
2·23·31
1427 22·3·7·17
1429
143 2·5·11·13
33·53
23·179
1433
2·3·239
5·7·41
22·359
3·479
2·719
1439
144
25·32·5
11·131
2·7·103 3·13·37
22·192
5·172
2·3·241
1447
23·181 32·7·23
145
2·52·29
1451 22·3·112
1453
2·727
3·5·97
24·7·13
31·47
2·36
1459
146
22·5·73
3·487
2·17·43 7·11·19
23·3·61
5·293
2·733
32·163
22·367 13·113
147
2·3·5·72
1471
26·23
3·491
2·11·67
52·59
22·32·41
7·211
2·739 3·17·29
148
23·5·37
1481 2·3·13·19
1483
22·7·53
33·5·11
2·743
1487
24·3·31
1489
149
2·5·149
3·7·71
22·373
1493
2·32·83
5·13·23
23·11·17
3·499
2·7·107
1499
150
22·3·53
19·79
2·751
32·167
25·47
5·7·43
2·3·251
11·137 22·13·29
3·503
151
2·5·151
1511
23·33·7
17·89
2·757
3·5·101
22·379
37·41 2·3·11·23
72·31

248
Chapter 4
NUMBER THEORY
0
1
2
3
4
5
6
7
8
9
152
24·5·19
32·132
2·761
1523 22·3·127
52·61
2·7·109
3·509
23·191 11·139
153 2·32·5·17
1531
22·383
3·7·73
2·13·59
5·307
29·3
29·53
2·769
34·19
154 22·5·7·11
23·67
2·3·257
1543
23·193
3·5·103
2·773 7·13·17 22·32·43
1549
155
2·52·31 3·11·47
24·97
1553 2·3·7·37
5·311
22·389
32·173
2·19·41
1559
156 23·3·5·13
7·223
2·11·71
3·521 22·17·23
5·313
2·33·29
1567
25·72
3·523
157
2·5·157
1571 22·3·131
112·13
2·787
32·52·7
23·197
19·83
2·3·263
1579
158
22·5·79 3·17·31
2·7·113
1583 24·32·11
5·317
2·13·61
3·232
22·397
7·227
159
2·3·5·53
37·43
23·199
33·59
2·797
5·11·29 22·3·7·19
1597
2·17·47 3·13·41
160
26·52
1601
2·32·89
7·229
22·401
3·5·107
2·11·73
1607
23·3·67
1609
161
2·5·7·23
32·179 22·13·31
1613
2·3·269
5·17·19
24·101 3·72·11
2·809
1619
162
22·34·5
1621
2·811
3·541
23·7·29
53·13
2·3·271
1627 22·11·37 32·181
163
2·5·163
7·233
25·3·17
23·71
2·19·43
3·5·109
22·409
1637 2·32·7·13 11·149
164
23·5·41
3·547
2·821
31·53 22·3·137
5·7·47
2·823
33·61
24·103
17·97
165 2·3·52·11
13·127
22·7·59 3·19·29
2·827
5·331
23·32·23
1657
2·829
3·7·79
166
22·5·83
11·151
2·3·277
1663
27·13
32·5·37
2·72·17
1667 22·3·139
1669
167
2·5·167
3·557 23·11·19
7·239
2·33·31
52·67
22·419 3·13·43
2·839
23·73
168
24·3·5·7
412
2·292 32·11·17
22·421
5·337
2·3·281
7·241
23·211
3·563
169
2·5·132
19·89 22·32·47
1693
2·7·112
3·5·113
25·53
1697
2·3·283
1699
170
22·52·17
35·7
2·23·37
13·131
23·3·71
5·11·31
2·853
3·569
22·7·61
1709
171 2·32·5·19
29·59
24·107
3·571
2·857
5·73 22·3·11·13
17·101
2·859 32·191
172
23·5·43
1721 2·3·7·41
1723
22·431
3·52·23
2·863
11·157
26·33 7·13·19
173
2·5·173
3·577
22·433
1733
2·3·172
5·347
23·7·31
32·193
2·11·79
37·47
174 22·3·5·29
1741
2·13·67
3·7·83
24·109
5·349
2·32·97
1747 22·19·23 3·11·53
175
2·53·7
17·103
23·3·73
1753
2·877
33·5·13
22·439
7·251
2·3·293
1759
176
25·5·11
3·587
2·881
41·43 22·32·72
5·353
2·883 3·19·31 23·13·17
29·61
177
2·3·5·59 7·11·23
22·443
32·197
2·887
52·71
24·3·37
1777
2·7·127
3·593
178
22·5·89
13·137
2·34·11
1783
23·223 3·5·7·17
2·19·47
1787 22·3·149
1789
179
2·5·179
32·199
28·7
11·163 2·3·13·23
5·359
22·449
3·599
2·29·31
7·257
180
23·32·52
1801
2·17·53
3·601 22·11·41
5·192
2·3·7·43
13·139
24·113
33·67
181
2·5·181
1811 22·3·151
72·37
2·907
3·5·112
23·227
23·79 2·32·101 17·107
182
22·5·7·13
3·607
2·911
1823
25·3·19
52·73
2·11·83 32·7·29
22·457
31·59
183
2·3·5·61
1831
23·229 3·13·47
2·7·131
5·367
22·33·17
11·167
2·919
3·613
184
24·5·23
7·263
2·3·307
19·97
22·461
32·5·41
2·13·71
1847 23·3·7·11
432
185
2·52·37
3·617
22·463
17·109 2·32·103
5·7·53
26·29
3·619
2·929 11·132
186
22·3·5·31
1861
2·72·19
34·23
23·233
5·373
2·3·311
1867
22·467
3·7·89
187
2·5·11·17
1871 24·32·13
1873
2·937
3·54
22·7·67
1877
2·3·313
1879
188
23·5·47
32·11·19
2·941
7·269 22·3·157
5·13·29
2·23·41 3·17·37
25·59
1889
189
2·33·5·7
31·61 22·11·43
3·631
2·947
5·379
23·3·79
7·271
2·13·73 32·211
190
22·52·19
1901
2·3·317
11·173
24·7·17
3·5·127
2·953
1907 22·32·53
23·83
191
2·5·191 3·72·13
23·239
1913 2·3·11·29
5·383
22·479
33·71
2·7·137 19·101
192
27·3·5
17·113
2·312
3·641 22·13·37
52·7·11
2·32·107
41·47
23·241
3·643
193
2·5·193
1931 22·3·7·23
1933
2·967
32·5·43
24·112
13·149
2·3·17·19
7·277
194
22·5·97
3·647
2·971
29·67
23·35
5·389
2·7·139
3·11·59
22·487
1949
195
2·3·52·13
1951
25·61 32·7·31
2·977
5·17·23
22·3·163
19·103
2·11·89
3·653
196
23·5·72
37·53 2·32·109
13·151
22·491
3·5·131
2·983
7·281
24·3·41 11·179
197
2·5·197
33·73 22·17·29
1973 2·3·7·47
52·79
23·13·19
3·659
2·23·43
1979
198
22·32·5·11
7·283
2·991
3·661
26·31
5·397
2·3·331
1987
22·7·71 32·13·17

Section 4.4
PRIME NUMBERS
249
0
1
2
3
4
5
6
7
8
9
199
2·5·199
11·181
23·3·83
1993
2·997 3·5·7·19
22·499
1997
2·33·37
1999
200
24·53 3·23·29 2·7·11·13
2003 22·3·167
5·401
2·17·59
32·223
23·251
72·41
201
2·3·5·67
2011
22·503 3·11·61
2·19·53
5·13·31
25·32·7
2017
2·1009
3·673
202
22·5·101
43·47
2·3·337
7·172 23·11·23
34·52
2·1013
2027 22·3·132
2029
203
2·5·7·29
3·677
24·127
19·107 2·32·113
5·11·37
22·509
3·7·97
2·1019
2039
204
23·3·5·17
13·157
2·1021
32·227
22·7·73
5·409 2·3·11·31
23·89
211
3·683
205
2·52·41
7·293 22·33·19
2053
2·13·79
3·5·137
23·257
112·17
2·3·73
29·71
206
22·5·103
32·229
2·1031
2063
24·3·43
5·7·59
2·1033 3·13·53 22·11·47
2069
207
2·32·5·23
19·109
23·7·37
3·691
2·17·61
52·83
22·3·173
31·67
2·1039 33·7·11
208
25·5·13
2081
2·3·347
2083
22·521
3·5·139
2·7·149
2087 23·32·29
2089
209
2·5·11·19 3·17·41
22·523 7·13·23
2·3·349
5·419
24·131
32·233
2·1049
2099
210
22·3·52·7
11·191
2·1051
3·701
23·263
5·421
2·34·13
72·43 22·17·31 3·19·37
211
2·5·211
2111
26·3·11
2113
2·7·151
32·5·47
22·232
29·73
2·3·353 13·163
212
23·5·53 3·7·101
2·1061
11·193 22·32·59
53·17
2·1063
3·709
24·7·19
2129
213
2·3·5·71
2131 22·13·41
33·79
2·11·97
5·7·61
23·3·89
2137
2·1069 3·23·31
214
22·5·107
2141 2·32·7·17
2143
25·67 3·5·11·13
2·29·37
19·113 22·3·179
7·307
215
2·52·43
32·239
23·269
2153
2·3·359
5·431
22·72·11
3·719
2·13·83 17·127
216
24·33·5
2161
2·23·47 3·7·103
22·541
5·433
2·3·192
11·197
23·271 32·241
217
2·5·7·31
13·167 22·3·181
41·53
2·1087
3·52·29
27·17
7·311 2·32·112
2179
218
22·5·109
3·727
2·1091
37·59 23·3·7·13
5·19·23
2·1093
37
22·547 11·199
219
2·3·5·73
7·313
24·137 3·17·43
2·1097
5·439
22·32·61
133
2·7·157
3·733
220
23·52·11
31·71
2·3·367
2203 22·19·29
32·5·72
2·1103
2207
25·3·23
472
221 2·5·13·17 3·11·67
22·7·79
2213
2·33·41
5·443
23·277
3·739
2·1109
7·317
222 22·3·5·37
2221 2·11·101 32·13·19
24·139
52·89
2·3·7·53
17·131
22·557
3·743
223
2·5·223
23·97 23·32·31 7·11·29
2·1117
3·5·149
22·13·43
2237
2·3·373
2239
224
26·5·7
33·83
2·19·59
2243 22·3·11·17
5·449
2·1123
3·7·107
23·281 13·173
225
2·32·53
2251
22·563
3·751
2·72·23
5·11·41
24·3·47
37·61
2·1129 32·251
226
22·5·113 7·17·19 2·3·13·29
31·73
23·283
3·5·151
2·11·103
2267
22·34·7
2269
227
2·5·227
3·757
25·71
2273
2·3·379
52·7·13
22·569 32·11·23
2·17·67
43·53
228 23·3·5·19
2281
2·7·163
3·761
22·571
5·457
2·32·127
2287 24·11·13 3·7·109
229
2·5·229
29·79 22·3·191
2293
2·31·37
33·5·17
23·7·41
2297
2·3·383 112·19
230
22·52·23 3·13·59
2·1151
72·47
28·32
5·461
2·1153
3·769
22·577
2309
231 2·3·5·7·11
2311
23·172
32·257
2·13·89
5·463
22·3·193
7·331
2·19·61
3·773
232
24·5·29
11·211
2·33·43
23·101
22·7·83
3·52·31
2·1163
13·179
23·3·97 17·137
233
2·5·233 32·7·37 22·11·53
2333
2·3·389
5·467
25·73 3·19·41
2·7·167
2339
234 22·32·5·13
2341
2·1171 3·11·71
23·293
5·7·67 2·3·17·23
2347
22·587
34·29
235
2·52·47
2351
24·3·72
13·181 2·11·107
3·5·157
22·19·31
2357 2·32·131
7·337
236
23·5·59
3·787
2·1181
17·139 22·3·197
5·11·43
2·7·132
32·263
26·37 23·103
237
2·3·5·79
2371
22·593 3·7·113
2·1187
53·19
23·33·11
2377
2·29·41 3·13·61
238 22·5·7·17
2381
2·3·397
2383
24·149
32·5·53
2·1193 7·11·31 22·3·199
2389
239
2·5·239
3·797 23·13·23
2393 2·32·7·19
5·479
22·599 3·17·47 2·11·109
2399
240
25·3·52
74
2·1201
33·89
22·601
5·13·37
2·3·401
29·83
23·7·43 3·11·73
241
2·5·241
2411 22·32·67
19·127
2·17·71 3·5·7·23
24·151
2417 2·3·13·31
41·59
242
22·5·112
32·269
2·7·173
2423 23·3·101
52·97
2·1213
3·809
22·607
7·347
243
2·35·5 11·13·17
27·19
3·811
2·1217
5·487 22·3·7·29
2437
2·23·53 32·271
244
23·5·61
2441 2·3·11·37
7·349 22·13·47
3·5·163
2·1223
2447 24·32·17
31·79
245
2·52·72 3·19·43
22·613
11·223
2·3·409
5·491
23·307 33·7·13
2·1229
2459

250
Chapter 4
NUMBER THEORY
0
1
2
3
4
5
6
7
8
9
246 22·3·5·41
23·107
2·1231
3·821
25·7·11
5·17·29
2·32·137
2467
22·617
3·823
247 2·5·13·19
7·353 23·3·103
2473
2·1237 32·52·11
22·619
2477 2·3·7·59
37·67
248
24·5·31
3·827
2·17·73
13·191 22·33·23
5·7·71
2·11·113
3·829
23·311 19·131
249
2·3·5·83
47·53
22·7·89
32·277
2·29·43
5·499
26·3·13
11·227
2·1249 3·72·17
Examples:
1. 6 = 2 × 3.
2. 245 = 5 × 72.
3. 10! = 28 × 34 × 52 × 7.
4. 68,718,821,377 = (217 −1) · (219 −1) (both factors are Mersenne primes; see §4.4.3).
5. The largest prime known is 274,207,281 −1, which has 22,338,618 decimal digits. It is
a Mersenne prime (see Table 2).
4.4.2
COUNTING PRIMES
Deﬁnitions:
The value of the prime counting function π(x) at x, where x is a positive real number,
equals the number of primes less than or equal to x.
The li function is deﬁned by li(x) =
R x
0
dt
ln t, for x ≥2. (The principal value is taken for
the integral at the singularity t = 1.)
Twin primes are primes that diﬀer by exactly 2.
Facts:
1. Euclid (ca. 300 BCE) proved that there are inﬁnitely many primes. He observed that
the product of a ﬁnite list of primes, plus one, must be divisible by a prime not on that
list.
2. Leonhard Euler (1707–1783) showed that the sum of the reciprocals of the primes up
to n tends toward inﬁnity as n tends toward inﬁnity, which also implies that there are
inﬁnitely many primes. (There are many other proofs as well.)
3. There is no useful, exact formula known which will produce the nth prime, given n.
It is relatively easy to construct a useless (that is, impractical) one. For example, let α =
P∞
n=1 pn/22n, where pn is the nth prime. Then the nth prime is ⌊22nα⌋−22n−1⌊22n−1α⌋,
where ⌊x⌋is the greatest integer less than or equal to x.
4. If f(x) is a polynomial with integer coeﬃcients that is not constant, then there are
inﬁnitely many integers n for which |f(n)| is not prime.
5. There are polynomials with integer coeﬃcients with the property that the set of
positive values taken by each of these polynomials as the variables range over the set of
nonnegative integers is the set of prime numbers. The existence of such polynomials has
essentially no practical value for constructing primes. For example, there are polynomials
in 26 variables of degree 25, in 42 variables of degree 5, and in 12 variables of degree
13,697 that have this property. (See [Ri96].)
6.
pn
n log n →1 as n →∞. (This follows from the prime number theorem, Fact 10.)
7. An inexact and rough formula for the nth prime is n log n.

Section 4.4
PRIME NUMBERS
251
8. pn > n log n for all n. (J. B. Rosser)
9. The sieve of Eratosthenes:
Eratosthenes (3rd century BCE) developed a method
(Algorithm 1) for listing all prime numbers less than a ﬁxed bound N. This method
starts from the prime 2 and successively eliminates all of its multiple from the list. It
continues through the list and successively eliminates multiples of the next (prime) value.
Only trial values up to the square root of N need to be tested.
Algorithm 1:
Sieve of Eratosthenes.
make a list of the numbers from 2 to N
i := 1
while i ≤
√
N
i := i + 1
if i is not already crossed out then cross out all proper multiples of i that
are less than or equal to N
{The numbers not crossed out then comprise the primes up to N}
10. Prime number theorem:
π(x), when divided by
x
log x, tends to 1 as x tends to
inﬁnity. That is, π(x) is asymptotic to
x
log x as x →∞.
11. The prime number theorem was ﬁrst conjectured by Carl Friedrich Gauss (1777–
1855) in 1792, and was ﬁrst proved in 1896 independently by Charles de la Vall´ee Poussin
(1866–1962) and Jacques Hadamard (1865–1963). They proved it in the stronger form
|π(x) −li(x)| < c1xe−c2
√log x, where c1 and c2 are positive constants. Their proofs used
functions of a complex variable. The ﬁrst elementary proofs (not using complex variables)
of the prime number theorem were supplied in 1949 by Paul Erd˝os (1913–1996) and Atle
Selberg (1917–2007).
12. Integration by parts shows that li(x) is asymptotic to
x
log x as x →∞.
13. |π(x) −li(x)| < c3xe−c4(log x)3/5(log log x)−1/5 for certain positive constants c3 and c4.
(I. M. Vinogradov and Nikolai Korobov, 1958.)
14. If the Riemann hypothesis (see Open Problem 1) is true, |π(x) −li(x)| is bounded
by √x log x for all x ≥2. Note that conversely, if |π(x) −li(x)| < √x log x for all x ≥2,
then the Riemann hypothesis is true.
15. J. E. Littlewood (1885–1977) showed that π(x) −li(x) changes sign inﬁnitely often.
However, no explicit number x with π(x) −li(x) > 0 is known. Carter Bays and Richard
H. Hudson have shown that such a number x exists below 1.4 · 10316. It is also known
(D. Platt and T. Trudgian) that π(x) < li(x) for all x < 1.39 · 1017.
16. The largest exactly computed value of π(x) is π(1026). This value, computed by D.
B. Staple in 2015, is about 1.56 · 1011 below li(1026). (See the following table.)

252
Chapter 4
NUMBER THEORY
n
π(10n)
≈π(10n) −li(10n)
1
4
−2
2
25
−5
3
168
−10
4
1,229
−17
5
9,592
−38
6
78,498
−130
7
664,579
−339
8
5,761,455
−754
9
50,847,534
−1,701
10
455,052,511
−3,104
11
4,118,054,813
−11,588
12
37,607,912,018
−38,263
13
346,065,536,839
−108,971
14
3,204,941,750,802
−314,890
15
29,844,570,422,669
−1,052,619
16
279,238,341,033,925
−3,214,632
17
2,623,557,157,654,233
−7,956,589
18
24,739,954,287,740,860
−21,949,555
19
234,057, 667, 276, 344, 607
−99,877,775
20
2,220,819,602,560,918,840
−222,744,644
21
21,127,269.486,018,731,928
−597,394,254
22
201,467,286,689,315,906,290
−1,932,355,208
23
1,925,320,391,606,803,968,923
−7,250,186,216
24
18,435,599,767,349,200,867,866
−17,146,907,278
25
176,846,309,399,143,769,411,680
−55,160,980,939
26
1,699,246,750,872,437,141,327, 603
−155,891,678,121
17. Dirichlet’s theorem on primes in arithmetic progressions:
Given coprime inte-
gers a, b with b positive, there are inﬁnitely many primes p ≡a (mod b). G. L. Dirichlet
proved this in 1837.
18. The number of primes p less than x such that p ≡a (mod b) is asymptotic to
1
φ(b)π(x) as x →∞, if a and b are coprime and b is positive.
(Here φ is the Euler
phi-function; see §4.6.2.)
Open Problems:
1. Riemann hypothesis:
The Riemann hypothesis (RH), posed in 1859 by Bernhard
Riemann (1826–1866), is a conjecture about the location of zeros of the Riemann zeta
function, the function of the complex variable s deﬁned by the series ζ(s) = P∞
n=1 n−s
when the real part of s is > 1, and deﬁned by the formula
ζ(s) =
s
s−1 −s R ∞
1 (x −⌊x⌋)x−s−1 dx
in the larger region when the real part of s is > 0, except for the single point s = 1,
where it remains undeﬁned. The Riemann hypothesis asserts that all of the solutions to
ζ(s) = 0 in this larger region lie on the vertical line in the complex number plane with
real part 1
2. Its proof would imply a better error estimate for the prime number theorem.
While believed to be true, it has not been proved. (See Fact 14.)

Section 4.4
PRIME NUMBERS
253
2. Extended Riemann hypothesis:
There is a generalized form of the Riemann hy-
pothesis known as the extended Riemann hypothesis (ERH) or the generalized Riemann
hypothesis (GRH), which also has important consequences in number theory. (For ex-
ample, see §4.4.4.)
3. Hypothesis H: The hypothesis H of Andrzej Schinzel and Wac law Sierpi´nski (1882–
1969) asserts that for every collection of irreducible nonconstant polynomials f1(x), . . . ,
fk(x) with integral coeﬃcients and positive leading coeﬃcients, if there is no ﬁxed integer
greater than 1 dividing the product f1(m) . . . fk(m) for all integers m, then there are
inﬁnitely many integers m such that each of the numbers f1(m), . . . , fk(m) is prime.
The case when each of the polynomials is linear was previously conjectured by L. E.
Dickson, and is known as the prime k-tuples conjecture. The only case of Hypothesis H
that has been proved is the case of a single linear polynomial; this is Dirichlet’s theorem
(Fact 17). The case of the two linear polynomials x and x + 2 corresponds to the twin
prime conjecture (Open Problem 4). Among the many consequences of Hypothesis H is
the assertion that there are inﬁnitely many primes of the form m2 + 1.
4. Twin primes:
It has been conjectured that there are inﬁnitely many twin primes,
that is, pairs of primes that diﬀer by 2.
Let dn denote the diﬀerence between the (n+1)st prime and the nth prime. The sequence
dn is unbounded. The prime number theorem implies that on average dn is about log n.
The twin prime conjecture asks whether dn is 2 inﬁnitely often. Although no proof of
the inﬁnitude of twin primes has been found, some results have recently been proved
about the inﬁnitude of primes that diﬀer by a particular positive integer greater than
two. In 2013 Yitang Zhang surprised the mathematical community when he proved that
there is an integer N with N < 70,000,000 such that inﬁnitely many pairs of primes
diﬀer by N. In later work by the Polymath Project, a collaborative eﬀort to further
mathematical research, Zhang’s bound was reduced to 246. (Under the assumption that
certain hypotheses hold, this bound can be reduced to as little as 6.) The hope is that
this work may eventually lead to a proof of the twin prime conjecture.
5. It is conjectured that dn can be as big as log2 n inﬁnitely often, but not much bigger.
Roger Baker and Glyn Harman have recently shown that dn < n.535 for all large num-
bers n. For the other direction, in 2014 Ford, Green, Konyagin, and Tao showed that
dn > c log n(log log n)(log log log log n)/(log log log n) inﬁnitely often. Several improve-
ments have been made on the constant c, but this ungainly expression has stubbornly
resisted improvement.
6. Christian Goldbach (1690–1764) conjectured that every integer greater than 5 is the
sum of three primes.
7. Goldbach’s conjecture: Christian Goldbach (1690–1764) conjectured that every even
integer greater than 2 is a sum of two primes.
• In 2013 the Goldbach conjecture was veriﬁed up to 4 · 1018 by Olveira e Silva,
Herzog, and Pardi.
• Goldbach also conjectured that all odd integers greater than 7 are the sum of three
primes. This was proved by Harald Helfgott in 2013 and involved a veriﬁcation
of this conjecture for all odd integers n with 7 ≤n < 1027 done by David Platt.
• In 1966 J. R. Chen proved that every suﬃciently large even number is either the
sum of two primes or the sum of a prime and a number that is the product of
two primes.
8. In 2004 B. Green and T. Tao proved the conjecture made in 1770 that the sequence
of prime numbers contains arbitrarily long arithmetic progressions.

254
Chapter 4
NUMBER THEORY
Examples:
1. A method for showing that there are inﬁnitely many primes is to note that the
integer n!+1 must have a prime factor greater than n, so there is no largest prime. Note
that n! + 1 is prime for n = 1, 2, 3, 11, 27, 37, 41, 73, 77, 116, 154, 320, 340, 399, and 427,
but is composite for all positive integers less than 427 not listed.
2. For p a prime, let Q(p) equal one more than the product of the primes not exceeding p.
For example Q(5) = 2·3·5+1 = 31. Then Q(p) is prime for p = 2, 3, 5, 7, 11, 31, 379, 1019,
1021, 2657, 3229, 4547, 4787, 11549, 13649; it is composite for all p < 11213 not in this list.
For example, Q(13) = 2 · 3 · 5 · 7 · 11 · 13 + 1 is composite.
3. There are six primes not exceeding 16, namely 2, 3, 5, 7, 11, 13. Hence π(16) = 6.
4. The expression n2 + 1 is prime for n = 1, 2, 4, 6, 10, . . ., but it is unknown whether
there are inﬁnitely many primes of this form when n is an integer. (See Open Problem 3.)
5. The polynomial f(n) = n2 + n + 41 takes on prime values for n = 0, 1, 2, . . ., 39, but
f(40) = 1681 = 412.
6. Applying Dirichlet’s theorem with a = 123 and b = 1,000, there are inﬁnitely many
primes that end in the digits 123. The ﬁrst such prime is 1,123.
7. The pairs 17, 19 and 191, 193 are twin primes. As of the publication of this book, the
largest known twin primes, 2,996,863,034,895 · 21,290,000 ± 1, have 388,342 decimal digits.
They were found in 2016 by the Twin Prime Search, a dedicated eﬀort for ﬁnding twin
primes begun in 2006, in conjunction with PrimeGrid, a distributed computing project
for ﬁnding world record prime numbers of various types:
• https://www.primegrid.com
4.4.3
NUMBERS OF SPECIAL FORM
Numbers of the form bn ± 1, for b a small number, are often easier to factor or test for
primality than other numbers of the same size. They also have a colorful history.
Deﬁnitions:
A Cunningham number is a number of the form bn ± 1, where b and n are natural
numbers, and b is “small” — 2, 3, 5, 6, 7, 10, 11, or 12. They are named after Allan
Cunningham, who, along with H. J. Woodall, published in 1925 a table of factorizations
of many of these numbers.
A Fermat number Fm is a Cunningham number of the form 22m + 1. (See Table 1.)
A Fermat prime is a Fermat number that is prime.
A Mersenne number Mn is a Cunningham number of the form 2n −1.
A Mersenne prime is a Mersenne number that is prime. (See Table 2.)
The cyclotomic polynomials Φk(x) are deﬁned recursively by the equation xn −1 =
Q
d|n Φd(x).
A perfect number is a positive integer that is equal to the sum of all its proper divisors.
Facts:
1. If Mn is prime, then n is prime, but the converse is not true.

Section 4.4
PRIME NUMBERS
255
2. If b > 2 or n is composite, then a nontrivial factorization of bn −1 is given by
bn −1 = Q
d|n Φd(b), though the factors Φd(b) are not necessarily primes.
3. The number bn + 1 can be factored as the product of Φd(b), where d runs over the
divisors of 2n that are not divisors of n. When n is not a power of 2 and b ≥2, this
factorization is nontrivial.
4. Some numbers of the form bn ± 1 also have so-called Aurifeuillian factorizations,
named after A. Aurifeuille. For more details, see [BrEtal88].
5. The only primes of the form bn −1 (with n > 1) are Mersenne primes.
6. The only primes of the form 2n + 1 are Fermat primes.
7. Fermat numbers are named after Pierre de Fermat (1601–1695), who observed that
F0, F1, F2, F3, and F4 are prime and stated (incorrectly) that all such numbers are
prime. Euler proved this was false, by showing that F5 = 232 + 1 = 641 · 6,700,417.
8. F4 is the largest known Fermat prime. It is conjectured that all larger Fermat numbers
are composite.
9. The smallest Fermat number that has not yet been completely factored is F12 =
2212 + 1, which has a 1187-digit composite factor.
10. As of 2016, we know that Fm is composite for all m with 5 ≤m ≤32. In all, we know
288 values of m for which Fm is composite. The largest such Fm, with m = 3,329,780, was
established as composite in 2014 when it was shown to be divisible by 193 · 23,329,782 + 1.
11. At least one prime factor is known for the Fm in Fact 10 except for F20 and F24.
The complete factorization of Fm is only known for 0 ≤m ≤11.
12. Table 1 provides known factorizations of Fm for 0 ≤m ≤24. Here pk indicates a
k-digit prime, and ck indicates a k-digit composite. All other numbers appearing in the
right column have been proved prime.
13. For up-to-date information about the factorization of Fermat numbers, see
• http://www.maths.dur.ac.uk/users/dzmitry.badziahin/Fermat
%20factoring%20status.html
14. Pepin’s criterion: For m ≥1, Fm is prime if and only if 3(Fm−1)/2 ≡−1 (mod Fm).
15. For m ≥2, every factor of Fm is of the form 2m+2k + 1.
16. Mersenne numbers are named after Marin Mersenne (1588–1648), who made a list
of what he thought were all the Mersenne primes Mp with p ≤257. (The status of Mp
for the primes up to 23 was known prior to Mersenne.) His list consisted of the primes
p = 2, 3, 5, 7, 13, 17, 19, 31, 67, 127, and 257. However, it was later shown that M67
and M257 are composite, while M61, M89, and M107, missing from the list, are prime.
17. It is not known whether there are inﬁnitely many Mersenne primes, nor whether
inﬁnitely many Mersenne numbers with a prime exponent are composite, though it is
conjectured that both are true.
18. Euclid showed that the product of a Mersenne prime 2p −1 with 2p−1 is perfect.
Euler showed that every even perfect number is of this form.

256
Chapter 4
NUMBER THEORY
Table 1: Fermat numbers.
m
factorization of Fm
0
3
1
5
2
17
3
257
4
65,537
5
641 × p7
6
274,177 × p14
7
59,649,589,127,497,217 × p22
8
1,238,926,361,552,897 × p62
9
2,424,833 × 7,455,602,825,647,884,208,337,395,736,200,454,918,783,366,342,657
×p99
10
45,592,577 × 6,487,031,809
×4,659,775,785,220,018,543,264,560,743,076,778,192,897 × p252
11
319,489 × 974,849 × 167,988,556,341,760,475,137
×3,560,841,906,445,833,920,513 × p564
12
114,689 × 26,017,793 × 63,766,529 × 190,274,191,361
×1,256,132,134,125,569
×568,630,647,535,356,955,169,033,410,940,867,804,839,360,742,060,818,433 × c1,133
13
2,710,954,639,361 × 2,663,848,877,152,141,313
×3,603,109,844,542,291,969 × 319,546,020,820,551,643,220,672,513 × c2,391
14
116,928,085,873,074,369,829,035,993,834,596,371,340,386,703,423,373,313 × c4880
15
1,214,251,009 × 2,327,042,503,868,417
×168,768,817,029,516,972,383,024,127,016,961 × c9,808
16
825,753,601 × 188,981,757,975,021,318,420,037,633 × c19,694
17
31,065,037,602,817
×7,751,061,099,802,522,589, 358,967,058,392,886,922,693,580,423,169 × c39,395
18
13,631,489 × 81,274,690,703.860,512,587,777 × c78,884
19
70,525,124,609 × 646,730,219,521 × 37,590,055,514,133,754,286,524,446,080,499,713
×c157,770
20
c315,653
21
4,485,296,422,913 × c631,294
22
64,658,705,994,591,851,009,055,774,868,504,577 × c1,262,577
23
167,772,161 × c2,525,215
24
c5,050,446
19. The Lucas-Lehmer test can be used to determine whether a given Mersenne number
is prime or composite. (See Algorithm 2.)
Algorithm 2:
Lucas-Lehmer test.
p := an odd prime; u := 4; i := 0
while i ≤p −2
i := i + 1
u := u2 −2 mod 2p −1
{If u = 0 then 2p −1 is prime, else 2p −1 is composite}
20. Table 2 lists all known Mersenne primes. An asterisk indicates that a discovery

Section 4.4
PRIME NUMBERS
257
was made as part of the GIMPS program (see Fact 21). As of 2016, the largest known
Mersenne prime is 274,207,281 −1.
As of 2016 we know that there are no additional
Mersenne primes less than the 44th prime listed, but there may be one or more Mersenne
primes between the 44th Mersenne prime and the largest prime on this list. When a new
Mersenne prime is found by computer, there may be other numbers of the form Mp less
than this prime not yet checked for primality. It can take months, or even years, to do
this checking. A new Mersenne prime may even be found this way, as was the case for
the 29th.
Table 2: Mersenne primes.
n
exponent
decimal
year
discoverer (computer used)
digits
discovered
1
2
1
ancient times
2
3
1
ancient times
3
5
2
ancient times
4
7
3
ancient times
5
13
4
1461
anonymous
6
17
6
1588
Cataldi
7
19
6
1588
Cataldi
8
31
10
1750
Euler
9
61
19
1883
Pervushin
10
89
27
1911
Powers
11
107
33
1913
Fauquembergue
12
127
39
1876
Lucas
13
521
157
1952
Robinson (SWAC)
14
607
183
1952
Robinson (SWAC)
15
1,279
386
1952
Robinson (SWAC)
16
2,203
664
1952
Robinson (SWAC)
17
2,281
687
1952
Robinson (SWAC)
18
3,217
969
1957
Riesel (BESK)
19
4,253
1,281
1961
Hurwitz (IBM 7090)
20
4,423
1,332
1961
Hurwitz (IBM 7090)
21
9,689
2,917
1963
Gillies (ILLIAC 2)
22
9,941
2,993
1963
Gillies (ILLIAC 2)
23
11,213
3,376
1963
Gillies (ILLIAC 2)
24
19,937
6,002
1971
Tuckerman (IBM 360/91)
25
21,701
6,533
1978
Noll and Nickel (Cyber 174)
26
23,209
6,987
1979
Noll (Cyber 174)
27
44,497
13,395
1979
Nelson and Slowinski (Cray 1)
28
86,243
25,962
1982
Slowinski (Cray 1)
29
110,503
33,265
1988
Colquitt and Welsh (NEC SX-W)
30
132,049
39,751
1983
Slowinski (Cray X-MP)
31
216,091
65,050
1985
Slowinski (Cray X-MP)
32
756,839
227,832
1992
Slowinski and Gage (Cray 2)
33
859,433
258,716
1994
Slowinski and Gage (Cray 2)
34
1,257,787
378,632
1996
Slowinski and Gage (Cray T94)
35
1,398,269
420,921
1996
Armengaud* (90 MHz Pentium)
36
2,976,221
895,932
1997
Spence* (100 MHz Pentium)

258
Chapter 4
NUMBER THEORY
n
exponent
decimal
year
discoverer (computer used)
digits
discovered
37
3,021,377
909,526
1998
Clarkson* (200 MHz Pentium)
38
6,972,593
2,098,960
1999
Hajratwala* (350 MHz Pentium)
39
13,466,917
4,053,946
2001
Cameron* (800 MHz Athlon)
40
20,996,011
6,320,430
2003
Schafer* (2 GHz Dell Dimension)
41
24,036,583
7,235,733
2004
Findley* (2.4 GHz Pentium)
42
25,964,951
7,816,230
2005
Nowak* (2.4 GHz Pentium)
43
30,402,457
9,152,052
2005
(Cooper and Boone)* (2 GHz Pentium)
44
32,582,657
9,808,358
2006
(Cooper,and Boone)* (3 GHz Pentium)
45
37,156,667
11,185,272
2008
Elvenich* (2.83 GHz Core 2 Duo)
46
42,643,801
12,837,064
2009
Strindmo* (3 GHz Core 2 Duo)
47
43,112,609
12,978,189
2008
Smith* (Dell Optiplex)
48
57,885,161
17,425,170
2013
Cooper* (3 GHz Core 2 Duo)
49
74,207,281
22,338,618
2016
Cooper* (3.6 GHz Intel i7-4790)
21. George Woltman launched the Great Internet Mersenne Prime Search (GIMPS)
in 1996.
GIMPS provides free software for PCs.
GIMPS has played a role in dis-
covering the last 15 Mersenne primes.
Thousands of people participate in GIMPS
over PrimeNet, a virtual supercomputer of distributed PCs, together running more
than 250 Teraﬂops in the quest for Mersenne primes. Consult the GIMPS website at
http://www.mersenne.org for more information about this quest and how to join it.
22. As of 2016, the two smallest composite Mersenne numbers not completely factored
were 2671 −1 and 2683 −1.
23. It is not known whether any odd perfect numbers exist. Odd perfect numbers, if
they exist, must satisfy many diﬀerent conditions that have been proved over the last
130 years. As of 2016, we know that there are none below 10300, a result of R. P. Brent,
G. L. Cohen, and H. J. J. te Riele in 1991. Among other known conditions are that an
odd perfect number must have at least 75 prime factors (proved by K. G. Hare in 2005)
and at least nine distinct prime factors (proved by P. P. Neilsen in 2006).
24. The best reference for the history of the factorization of Cunningham numbers is
[BrEtal88].
25. The current version of the Cunningham table, maintained by Sam Wagstaﬀ, can be
found at https://homes.cerias.purdue.edu/~ssw/cun/index.html.
Examples:
1. The Mersenne number M11 = 211 −1 is not prime since M11 = 23 · 89.
2. To factor 342 = 73 −1 note that 73 −1 = (7 −1) · (72 + 7 + 1) = 6 · 57.
3. To factor 37 + 1 note that 37 + 1 = Φ2(3)Φ14(3) = 4 · 547.
4. An example of an Aurifeuillian factorization is given by 24k−2 + 1 = (22k−1 −2k + 1)·
(22k−1 + 2k + 1).
5. Φ1(x) = x −1 and x3 −1 = Φ1(x)Φ3(x), so Φ3(x) = (x3 −1)/Φ1(x) = x2 + x + 1.
4.4.4
PSEUDOPRIMES AND PRIMALITY TESTING
Finding eﬃcient ways to determine whether a positive integer is prime has fascinated
mathematicians from ancient to modern times. (See [CrPo05] and [Po10].) Many of the

Section 4.4
PRIME NUMBERS
259
most suggested approaches begin by studying the congruence in Fermat’s little theorem
when the modulus is not prime.
This leads to the notions of a pseudoprime and to
probabilistic primality tests. These tests are applied to produce large primes that are
used in public key cryptography (see Chapter 15).
Deﬁnitions:
A pseudoprime to the base b is a composite number n such that bn ≡b (mod n).
A pseudoprime is a pseudoprime to the base 2.
A Carmichael number is a pseudoprime to all bases.
A strong pseudoprime to the base b is an odd composite number n = 2sd+1, with d
odd, and either bd ≡1 (mod n) or b2rd ≡−1 (mod n) for some integer r, 0 ≤r < s.
A witness for an odd composite number n is a base b, with 1 < b < n, to which n is not
a strong pseudoprime. Thus, b is a “witness” to n being composite.
A primality proof is an irrefutable veriﬁcation that an integer is prime.
Facts:
1. By Fermat’s little theorem (§4.3.1), bp−1 ≡1 (mod p) for all primes p and all integers b
that are not multiples of p. Thus, the only numbers n > 1 with bn−1 ≡1 (mod n) are
primes and pseudoprimes to the base b (which are coprime to b). Similarly, the numbers
n which satisfy the strong pseudoprime congruence conditions are the odd primes not
dividing b and the strong pseudoprimes to the base b.
2. The smallest pseudoprime is 341.
3. There are inﬁnitely many pseudoprimes; however, Paul Erd˝os has proved that pseu-
doprimes are rare compared to primes. The same results are true for pseudoprimes to
any ﬁxed base b. (See [CrPo05] or [Ri96] for details.)
4. In 1910, Robert D. Carmichael gave the ﬁrst examples of Carmichael numbers. The
ﬁrst 16 Carmichael numbers are
561 = 3 · 11 · 17
1,105 = 5 · 13 · 17
1,729 = 7 · 13 · 19
2,465 = 5 · 17 · 29
2,821 = 7 · 13 · 31
6,601 = 7 · 23 · 41
8,911 = 7 · 19 · 67
10,585 = 5 · 29 · 73
15,841 = 7 · 31 · 73
29,341 = 13 · 37 · 61
41,041 = 7 · 11 · 13 · 41
46,657 = 13 · 37 · 97
52,633 = 7 · 73 · 103
62,745 = 3 · 5 · 47 · 89
63,973 = 7 · 13 · 19 · 37
75,361 = 11 · 17 · 31
5. If n is a Carmichael number, then n is the product of at least three distinct odd
primes with the property that if q is any one of these primes, then q−1 divides n−1.
6. There are a ﬁnite number of Carmichael numbers that are the product of exactly r
primes with the ﬁrst r−2 primes speciﬁed.
7. If m is a positive integer such that 6m+ 1, 12m+ 1, and 18m+ 1 are all primes, then
(6m + 1)(12m + 1)(18m + 1) is a Carmichael number.
8. In 1994, W. R. Alford (1937–2001), Andrew Granville (born 1962), and Carl Pomer-
ance (born 1944) showed that there are inﬁnitely many Carmichael numbers.
9. There are inﬁnitely many numbers that are simultaneously strong pseudoprimes to
each base in any given ﬁnite set.
Each odd composite n, however, can be a strong
pseudoprime to at most one-fourth of the bases b with 1 ≤b ≤n −1.

260
Chapter 4
NUMBER THEORY
10. Michael Rabin, building upon the notion of a strong pseudoprime introduced by J.
L. Selfridge (born 1927), suggested Algorithm 3 (often referred to as the Miller-Rabin
test).
Algorithm 3:
Strong probable prime test (to a random base).
input: positive numbers n, d, s, with d odd and n = 2sd + 1.
b := a random integer such that 1 < b < n
c := bd mod n
if c = 1 or c = n −1, then declare n a probable prime and stop
compute sequentially c2 mod n, c4 mod n, . . . , c2s−1 mod n
if one of these is n −1, then declare n a probable prime and stop
else declare n composite and stop
11. A “probable prime” is not necessarily a prime, but the chances are good.
The
probability that an odd composite is not declared composite by Algorithm 3 is at most 1
4,
so the probability it passes k independent iterations is at most 4−k. Suppose this test is
applied to random odd inputs n with the hope of ﬁnding a prime. That is, random odd
numbers n (chosen between two consecutive powers of 2) are tested until one is found
that passes each of k independent iterations of the test. Ronald Burthe showed in 1995
that the probability that the output of this procedure is composite is less than 4−k.
12. Gary Miller proved in 1976 that if the extended Riemann hypothesis (§4.4.2) is true,
then every odd composite n has a witness less than c log2 n, for some constant c. Eric
Bach showed in 1985 that one may take c = 2. Therefore, if an odd number n > 1 passes
the strong probable prime test for every base b less than 2 log2 n, and if the extended
Riemann hypothesis is true, then n is prime.
13. In practice, one can test whether numbers under 2.5 · 1010 are prime by a small
number of strong probable prime tests. Pomerance, Selfridge, and Wagstaﬀhave veriﬁed
(1980) that there are no numbers less than this bound that are simultaneously strong
pseudoprimes to the bases 2, 3, 5, 7, and 11. Thus, any number less than 2.5 · 1010 that
passes those strong pseudoprime tests is a prime.
14. Gerhard Jaeschke showed in 1993 that the test described in Fact 13 works far beyond
2.5 · 1010; the ﬁrst number for which it fails is 2,152,302,898,747.
15. Only primes pass the strong pseudoprime tests to all the bases 2, 3, 5, 7, 11, 13,
and 17 until the composite number 341,550,071,728,321 is reached.
16. While pseudoprimality tests are usually quite eﬃcient at recognizing composites,
the task of proving that a number is prime can be more diﬃcult.
17. In 1983, Leonard Adleman, Carl Pomerance, and Robert Rumely developed the
APR algorithm, which can prove that a number n is prime in time proportional to
(log n)c log log log n, where c is a positive constant. See [Co93] and [CrPo05] for details.
18. Recently, Oliver Atkin and Fran¸cois Morain developed an algorithm to prove pri-
mality.
It is diﬃcult to predict in advance how long it will take, but in practice it
has been fast.
One advantage of their algorithm is that, unlike APR, it produces a
polynomial-time primality proof, though the running time to ﬁnd the proof may be a bit
longer. An implementation called ECPP (elliptic curve primality proving) is available
from http://www.lix.polytechnique.fr/Labo/Francois.Morain/Prgms/getecpp.
english.html.

Section 4.5
FACTORIZATION
261
19. In 1987, Carl Pomerance showed that every prime p has a primality proof whose
veriﬁcation involves just c log p multiplications with integers the size of p. It may be
diﬃcult, however, to ﬁnd such a short primality proof.
20. In 2002, Manindra Agrawal, Neeraj Kayal, and Nitin Saxena introduced a test for
determining whether a positive integer is prime that was a major breakthrough because
it works in polynomial time (in terms of the number of digits of the number tested),
it works for all positive integers (unlike some other tests that only work for integers of
a special form), it is deterministic (that is, it always determines whether a number is
prime unlike probabilistic primality tests), and its correctness does not depend on any
unproven hypotheses.
21. The original AKS test determines whether a positive integer n is prime in about
O(log7.5 n) time. Improvements in the test made by Carl Pomerance and H. W. Lenstra
reduce this time complexity to about O(log6 n).
22. The importance of the AKS test is more theoretical than practical at the present
time. It establishes that there is a polynomial-time algorithm for determining whether
a positive integer is prime. However, not even the faster versions of this test run fast
enough to be used to ﬁnd large primes, unlike probabilistic primality tests. (See Fact
18.)
4.5
FACTORIZATION
Determining the prime factorization of positive integers is a question that has been
studied for many years (see [CrPo05] for example). Furthermore, in the past four decades,
this question has become relevant for an extremely important application, the security
of public key cryptosystems. The question of exactly how to decompose a composite
number into the product of its prime factors is a diﬃcult one that continues to be the
subject of much research.
4.5.1
FACTORIZATION ALGORITHMS
Deﬁnition:
A smooth number is an integer all of whose prime divisors are small.
Facts:
1. The simplest algorithm for factoring an integer is trial division, Algorithm 1. While
simple, this algorithm is useful only for numbers that have a fairly small prime factor.
It can be modiﬁed so that after j = 3, the number j is incremented by 2, and there are
other improvements of this kind.
Algorithm 1:
Trial division.
input: an integer n ≥2
output: j (smallest prime factor of n) or statement that n is prime
j := 2

262
Chapter 4
NUMBER THEORY
while j ≤√n
if j|n then print that j is a prime factor of n and stop {n is not prime}
j := j + 1
if no factor is found then declare n prime
2. Many factoring algorithms have been developed in the last half century. Coupled
with the development of modern computers, increasing larger and larger integers can be
factored. Among these factoring algorithms are the quadratic sieve (QS), Pollard p −1
factorization, the number ﬁeld sieve (NFS), the elliptic curve method (ECM), and the
continued fraction method (CFRAC).
3. Currently, one of the fastest algorithms for numbers that are feasible to factor but
do not have a small prime factor is the quadratic sieve (QS), Algorithm 2, invented by
Carl Pomerance in 1981. (For numbers at the far range of feasibility, the number ﬁeld
sieve is faster; see Fact 10.)
Algorithm 2:
Quadratic sieve.
input: n (an odd composite number that is not a power)
output: g (a nontrivial factor of n)
ﬁnd a1, . . . , ak such that each a2
i −n is smooth
ﬁnd a subset of the numbers a2
i −n whose product is a square, say x2
reduce x modulo n
y := the product of the ai used to form the square
reduce y modulo n
{This gives a congruence x2 ≡y2 (mod n); equivalently n|(x2 −y2)}
g := gcd(x −y, n)
if g is not a nontrivial factor then ﬁnd new x and y (if necessary, ﬁnd more ai)
4. The greatest common divisor calculation may be quickly done via the Euclidean
algorithm. If x ̸≡±y (mod n), then g will be a nontrivial factor of n. (Among all
solutions to the congruence x2 ≡y2 (mod n) with xy coprime to n, at least half of them
lead to a nontrivial factorization of n.) Finding the ais is at the heart of the algorithm
and is accomplished using a sieve not unlike the sieve of Eratosthenes, but applied to
the consecutive values of the quadratic polynomial a2 −n. If a is chosen near √n, then
a2 −n will be relatively small, and thus more likely to be smooth. So one sieves the
polynomial a2 −n, where a runs over integers near √n, for values that are smooth.
When enough smooth values are collected, the subset with product a square may be
found via a linear algebra subroutine applied to a matrix formed out of the exponents in
the prime factorizations of the smooth values. The linear algebra may be done modulo 2.
5. The current formulation of QS involves many improvements, the most notable of
them is the multiple polynomial variation of James Davis and Peter Montgomery.
6. In 1994, QS was used to factor a 129-digit composite that was the product of a 64-
digit prime and a 65-digit prime. This number had been proposed as a challenge to those
who would try to crack the famous RSA cryptosystem.
7. In 1985, Hendrik W. Lenstra, Jr. (born 1949) invented the elliptic curve method
(ECM), which has the advantage that, like trial division, the running time is based on

Section 4.5
FACTORIZATION
263
the size of the smallest prime factor. Thus, it can be used to ﬁnd comparatively small
factors of numbers whose size would be prohibitively large for the quadratic sieve. It can
be best understood by ﬁrst examining the p−1 method of John Pollard, Algorithm 3.
8. The Pollard algorithm (Algorithm 3) is successful and eﬃcient if p−1 happens to
be smooth for some prime p|n. If the prime factors p of n have the property that p−1
is not smooth, Algorithm 3 will eventually be successful if a high enough bound B is
chosen, but in this case it will not be any more eﬃcient than trial division, Algorithm 1.
ECM gets around this restriction on the numbers that can be eﬃciently factored by
randomly searching through various mathematical objects called elliptic curve groups,
each of which has p+1−a elements, where |a| < 2√p and a depends on the curve. ECM
is successful when a group is encountered such that p+1−a is a smooth number.
Algorithm 3:
p−1 factorization method.
input: n (composite number), B (a bound)
output: a nontrivial factor of n
b := 2
{Loop on b}
if b|n then stop {b is a prime factor of n}
M := 1
while M ≤B
g := gcd (blcm (1,2,...,M) −1, n)
if n > g > 1 then output g and stop {g is a nontrivial factor of n}
else if g = n then choose ﬁrst prime larger than b and go to beginning of
the b-loop
else M := M + 1
9. As of 2016, prime factors as large as 83 digits have been found using ECM. (After
such a factor is discovered it may turn out that the remaining part of the number is a
prime and the factorization is now complete. This last prime may be very large, as with
the tenth and eleventh Fermat numbers; see Table 1 of §4.4.3. In such cases the success of
ECM is measured by the second largest prime factor in the prime factorization, though
in some sense the method has discovered the largest prime factor as well.)
10. The number ﬁeld sieve (NFS), originally suggested by Pollard for numbers of special
form, and developed for general composite numbers by Buhler, Lenstra, and Pomerance,
is currently the fastest factoring algorithm for very large numbers with no small prime
factors.
11. The number ﬁeld sieve is similar to QS in that one attempts to assemble two squares
x2 and y2 whose diﬀerence is a multiple of n, and this is done via a sieve and linear
algebra modulo 2. However, NFS is much more complicated than QS. Although faster
for very large numbers, the complexity of the method makes it unsuitable for numbers
much smaller than 100 digits. The exact crossover with QS depends a great deal on the
implementations and the hardware employed. The two are roughly within an order of
magnitude of each other for numbers between 100 and 150 digits, with QS having the
edge at the lower end and NFS the edge at the upper end.
12. Part of the NFS algorithm requires expressing a small multiple of the number to be
factored by a polynomial of moderate degree. The running time depends, in part, on the
size of the coeﬃcients of this polynomial. For Cunningham numbers, this polynomial

264
Chapter 4
NUMBER THEORY
can be easy to ﬁnd. (For example, in the notation of §4.4.3, 8F9 = 8(229 + 1) = f(2103),
where f(x) = x5 + 8.) This version is called the special number ﬁeld sieve (SNFS).
The version for general numbers, the general number ﬁeld sieve (GNFS), has somewhat
greater complexity. The greatest success of SNFS has been the factorization of a 180-digit
Cunningham number, while the greatest success of GNFS has been the factorization of
a 232-digit number of no special form and with no small prime factor.
13. See [Co93], [CrPo05], [Po90], and [Po94] for fuller descriptions of the factoring
algorithms described here, as well as others, including the continued fraction (CFRAC)
method. Until the advent of QS, this had been the fastest known practical algorithm.
14. The factorization algorithms QS, ECM, SNFS, and GNFS are fast in practice, but
analyses of their running times depend on heuristic arguments and unproved hypotheses.
The fastest algorithm whose running time has been rigorously analyzed is the class group
relations method (CGRM). It, however, is not practical. It is a probabilistic algorithm
whose expected running time is bounded by ec√log n log log n, where c tends to 1 as n
tends to inﬁnity through the odd composite numbers that are not powers. This result
was proved in 1992 by Lenstra and Pomerance.
15. These algorithms are summarized in Table 1. L(a, b) means that the running time
to factor n is bounded by ec(log n)a(log log n)1−a, where c tends to b as n tends to inﬁnity
through the odd composite non-powers. Running times are measured in the number of
arithmetic steps with integers at most the size of n.
Table 1: Comparison of various factoring methods.
algorithm
year
greatest
running
rigorously
introduced
success
time
analyzed
trial division
antiquity
–
√n
yes
CFRAC
1970
63-digit number
L

1
2,
q
3
2

no
p −1
1974
58-digit factor
–
yes
QS
1981
135-digit number
L( 1
2, 1)
no
ECM
1985
83-digit factor
L( 1
2, 1)
no
SNFS
1988
355-digit number
L

1
3,
3q
32
9

no
CGRM
1992
–
L( 1
2, 1)
yes
GNFS
1993
221-digit number
L

1
3,
3q
64
9

no
16. The running time for trial division in Table 1 is a worst-case estimate, achieved
when n is prime or the product of two primes of the same magnitude.
When n is
composite, trial division will discover the least prime factor p of n in roughly p steps.
The record for the largest prime factor discovered via trial division is not known, nor is
the largest number proved prime by this method, though the feat of Euler of proving that
the Mersenne number 231 −1 is prime, using only trial division and hand calculations,
should certainly be noted. (Euler surely knew, though, that any prime factor of 231 −1
is 1 mod 31, so only 1 out of every 31 trial divisors needed to be tested.)
17. The running time of the p−1 method is about B, where B is the least number such
that for some prime factor p of n, p −1 divides lcm(1, 2, . . . , B).
18. There are variants of CFRAC and GNFS that have smaller heuristic complexity
estimates, but the ones in Table 1 are for the fastest practical version.
19. The running time bound for ECM is a worst-case estimate. It is more appropriate
to measure ECM as a function of the least prime factor p of n. This heuristic complexity

Section 4.6
ARITHMETIC FUNCTIONS
265
bound is ec√log p log log p, where c tends to
√
2 as p tends to inﬁnity.
20. The following table lists the largest hard number factored as a function of time. It
was compiled with the assistance of Sam Wagstaﬀ. It should be remarked that there
is no ﬁrm deﬁnition of a “hard number”. What is meant here is that the number was
factored by an algorithm that is not sensitive to any particular form the number may
have, nor sensitive to the size of the prime factors.
year
method
digits
1970
CFRAC
39
1979
CFRAC
46
1982
CFRAC
54
1983
QS
67
1986
QS
87
1988
QS
102
1990
QS
116
1994
QS
129
1995
GNFS
130
1999
GNFS
155
2003
GNFS
174
2005
GNFS
200
2009
GNFS
232
21. It is unknown whether there is a polynomial-time factorization algorithm. Whether
there are any factorization algorithms that surpass the quadratic sieve, the elliptic curve
method, and the number ﬁeld sieve in their respective regions of superiority is an area
of much current research.
22. A subjective measurement of progress in factorization can be gained by looking at
the “ten most wanted numbers” to be factored. The list is maintained by Sam Wagstaﬀ
and can be found at http://homes.cerias.purdue.edu/~ssw/cun//want131. As of
May 2016, “number one” on this list is 21207 −1.
23. In 1994 Peter Shor invented an algorithm for factoring integers on a quantum com-
puter that runs in polynomial time. More precisely, this algorithm factors a positive
integer n in O(log2 n log log n log log log n) time. At the present time, only rudimentary
quantum computers have been built. Many technological challenges must be solved be-
fore useful quantum computing is feasible. As of 2016, the largest integer that has been
factored using a quantum computer is 56,153; this was done using an algorithm diﬀerent
from Shor’s algorithm.
4.6
ARITHMETIC FUNCTIONS
Functions whose domains are the set of positive integers play an important role in number
theory. Such functions are called arithmetic functions and are the subject of this section.
The information presented here includes deﬁnitions and properties of many important
arithmetic functions, asymptotic estimates on the growth of these functions, and algebraic

266
Chapter 4
NUMBER THEORY
properties of sets of certain arithmetic functions. For more information on the topics
covered in this section, see [Ap76].
4.6.1
MULTIPLICATIVE AND ADDITIVE FUNCTIONS
Deﬁnitions:
An arithmetic function is a function that is deﬁned for all positive integers.
An arithmetic function is multiplicative if f(mn) = f(m)f(n) whenever m and n are
relatively prime positive integers.
An arithmetic function is completely multiplicative if f(mn) = f(m)f(n) for all
positive integers m and n.
If f is an arithmetic function, then P
d|n f(d), the value of the summatory function
of f at n, is the sum of f(d) over all positive integers d that divide n.
An arithmetic function f is additive if f(mn) = f(m) + f(n) whenever m and n are
relatively prime positive integers.
An arithmetic function f is completely additive if f(m, n) = f(m)+f(n) whenever m
and n are positive integers.
Facts:
1. If f is a multiplicative function and n = pa1
1 pa2
2 . . . pas
s is the prime-power factorization
of n, then f(n) = f(pa1
1 )f(pa2
2 ) . . . f(pas
s ).
2. If f is multiplicative, then f(1) = 1.
3. If f is a completely multiplicative function and n = pa1
1 pa2
2 . . . pas
s , then f(n) =
f(p1)a1f(p2)a2 . . . f(ps)as.
4. If f is multiplicative, then the arithmetic function F(n) = P
d|n f(d) is multiplicative.
5. If f is an additive function, then f(1) = 0.
6. If f is an additive function and a is a positive real number, then F(n) = af(n) is
multiplicative.
7. If f is a completely additive function and a is a positive real number, then F(n) =
af(n) is completely multiplicative.
Examples:
1. The function f(n) = n2 is multiplicative. Even more, it is completely multiplicative.
2. The function I(n) =
 1
n

(so that I(1) = 1 and I(n) = 0 if n is a positive integer
greater than 1) is completely multiplicative.
3. The Euler phi-function, the number of divisors function, the sum of divisors function,
and the M¨obius function (deﬁned in subsequent sections) are all multiplicative. None of
these functions is completely multiplicative.
4.6.2
EULER’S PHI-FUNCTION
Deﬁnition:
If n is a positive integer then φ(n), the value of the Euler phi-function at n, is the
number of positive integers not exceeding n that are relatively prime to n. The Euler
phi-function is also known as the totient function.

Section 4.6
ARITHMETIC FUNCTIONS
267
Facts:
1. The Euler φ function is multiplicative, but not completely multiplicative.
2. If p is a prime, then φ(p) = p −1.
3. If p is a positive integer with φ(p) = p −1, then p is prime.
4. If p is a prime and a is a positive integer, then φ(pa) = pa −pa−1.
5. If n is a positive integer with prime-power factorization n = pa1
1 pa2
2 . . . pak
k , then
φ(n) = n Qk
j=1(1 −1
pj ).
6. If n is a positive integer greater than 2, then φ(n) is even.
7. If n has r distinct odd prime factors, then 2r divides φ(n).
8. If m and n are positive integers and gcd(m, n) = d, then φ(mn) = φ(m)φ(n)d
φ(d)
.
9. If m and n are positive integers and m|n, then φ(m)|φ(n).
10. If n is a positive integer, then P
d|n φ(d) = P
d|n φ( n
d ) = n.
11. If n is a positive integer with n ≥5, then φ(n) >
n
6 log log n.
12. Pn
k=1 φ(k) = 3n2
π2 + O(n log n)
13. Pn
k=1
φ(k)
k
= 6n
π2 + O(n log n)
Examples:
1. Table 1 in §4.6.3 displays the values of φ(n) for 1 ≤n ≤1,000.
2. To see that φ(10) = 4, note that the positive integers not exceeding 10 relatively
prime to 10 are 1, 3, 7, and 9.
3. To ﬁnd φ(720), note that φ(720) = φ(24325) = 720(1 −1
2)(1 −1
3)(1 −1
5) = 192.
4.6.3
SUM AND NUMBER OF DIVISORS FUNCTIONS
Deﬁnitions:
If n is a positive integer, then σ(n), the value of the sum of divisors function at n, is
the sum of the positive integer divisors of n.
A positive integer n is perfect if and only if it equals the sum of its proper divisors (or
equivalently, if σ(n) = 2n).
A positive integer n is abundant if the sum of the proper divisors of n exceeds n (or
equivalently, if σ(n) > 2n).
A positive integer n is deﬁcient if the sum of the proper divisors of n is less than n (or
equivalently, if σ(n) < 2n).
The positive integers m and n are amicable if σ(m) = σ(n) = m + n.
If n is a positive integer, then τ(n), the value of the number of divisors function at n,
is the number of positive integer divisors of n.
Facts:
1. The number of divisors function is multiplicative, but not completely multiplicative.
2. The number of divisors function is the summatory function of f(n) = 1; that is,
τ(n) = P
d|n 1.

268
Chapter 4
NUMBER THEORY
3. The sum of divisors function is multiplicative, but not completely multiplicative.
4. The sum of divisors function is the summatory function of f(n) = n; that is, σ(n) =
P
d|n
d.
5. If n is a positive integer with prime-power factorization n = pa1
1 pa2
2 . . . pak
k , then
σ(n) = Qk
j=1(paj+1
j
−1)/(pj −1).
6. If n is a positive integer with prime-power factorization n = pa1
1 pa2
2 . . . pak
k , then
τ(n) = Qk
j=1(aj + 1).
7. If n is a positive integer, then τ(n) is odd if and only if n is a perfect square.
8. If k is an integer greater than 1, then the equation τ(n) = k has inﬁnitely many
solutions.
9. If n is a positive integer, then (P
d|n τ(d))2 = P
d|n τ(d)3.
10. A positive integer n is an even perfect number if and only if n = 2m−1(2m−1)
where m is an integer, m ≥2, and 2m−1 is prime (so that it is a Mersenne prime
(§4.4.3)).
Hence, the number of known even perfect numbers equals the number of
known Mersenne primes.
11. It is unknown whether there are any odd perfect numbers. However, it is known
that there are no odd perfect numbers less than 10300 and that any odd perfect number
must have at least eight diﬀerent prime factors.
12. Pn
k=1 σ(k) = π2n2
12
+ O(n log n).
13. Pn
k=1 τ(k) = n log n + (2γ −1)n + O(√n), where γ is Euler’s constant.
14. If m and n are amicable, then m is the sum of the proper divisors of n, and vice
versa.
Examples:
1. Table 1 lists the values of σ(n) and τ(n) for 1 ≤n ≤1,000.
2. To ﬁnd τ(720), note that τ(720) = τ(24 · 32 · 5) = (4 + 1)(2 + 1)(1 + 1) = 30.
3. To ﬁnd σ(200), note that σ(200) = σ(23 · 52) = 24−1
2−1 · 53−1
5−1 = 15 · 31 = 465.
4. The integers 6 and 28 are perfect; the integers 9 and 16 are deﬁcient; the integers 12
and 945 are abundant.
5. The integers 220 and 284 form the smallest pair of amicable numbers.
Table 1: Values of φ(n), σ(n), τ(n), and µ(n) for 1 ≤n ≤1000.
n
φ
σ
τ
µ
n
φ
σ
τ
µ
n
φ
σ
τ
µ
n
φ
σ
τ
µ
n
φ
σ
τ
µ
1
1
1
1
1
2
1
3
2 −1
3
2
4
2 −1
4
2
7
3
0
5
4
6
2 −1
6
2
12
4
1
7
6
8
2 −1
8
4
15
4
0
9
6
13
3
0
10
4
18
4
1
11
10
12
2 −1
12
4
28
6
0
13
12
14
2 −1
14
6
24
4
1
15
8
24
4
1
16
8
31
5
0
17
16
18
2 −1
18
6
39
6
0
19
18
20
2 −1
20
8
42
6
0
21
12
32
4
1
22
10
36
4
1
23
22
24
2 −1
24
8
60
8
0
25
20
31
3
0
26
12
42
4
1
27
18
40
4
0
28
12
56
6
0
29
28
30
2 −1
30
8
72
8 −1
31
30
32
2 −1
32
16
63
6
0
33
20
48
4
1
34
16
54
4
1
35
24
48
4
1
36
12
91
9
0
37
36
38
2 −1
38
18
60
4
1
39
24
56
4
1
40
16
90
8
0
41
40
42
2 −1
42
12
96
8 −1
43
42
44
2 −1
44
20
84
6
0
45
24
78
6
0
46
22
72
4
1
47
46
48
2 −1
48
16
124 10
0
49
42
57
3
0
50
20
93
6
0
51
32
72
4
1
52
24
98
6
0
53
52
54
2 −1
54
18
120
8
0
55
40
72
4
1
56
24
120
8
0
57
36
80
4
1
58
28
90
4
1
59
58
60
2 −1
60
16
168 12
0
61
60
62
2 −1
62
30
96
4
1
63
36
104
6
0
64
32
127
7
0
65
48
84
4
1

Section 4.6
ARITHMETIC FUNCTIONS
269
n
φ
σ
τ
µ
n
φ
σ
τ
µ
n
φ
σ
τ
µ
n
φ
σ
τ
µ
n
φ
σ
τ
µ
66
20
144
8 −1
67
66
68
2 −1
68
32
126
6
0
69
44
96
4
1
70
24
144
8 −1
71
70
72
2 −1
72
24
195 12
0
73
72
74
2 −1
74
36
114
4
1
75
40
124
6
0
76
36
140
6
0
77
60
96
4
1
78
24
168
8 −1
79
78
80
2 −1
80
32
186 10
0
81
54
121
5
0
82
40
126
4
1
83
82
84
2 −1
84
24
224 12
0
85
64
108
4
1
86
42
132
4
1
87
56
120
4
1
88
40
180
8
0
89
88
90
2 −1
90
24
234 12
0
91
72
112
4
1
92
44
168
6
0
93
60
128
4
1
94
46
144
4
1
95
72
120
4
1
96
32
252 12
0
97
96
98
2 −1
98
42
171
6
0
99
60
156
6
0
100
40
217
9
0
101 100
102
2 −1 102
32
216
8 −1 103 102
104
2 −1 104
48
210
8
0
105
48
192
8 −1
106
52
162
4
1 107 106
108
2 −1 108
36
280 12
0 109 108
110
2 −1
110
40
216
8 −1
111
72
152
4
1 112
48
248 10
0 113 112
114
2 −1 114
36
240
8 −1
115
88
144
4
1
116
56
210
6
0 117
72
182
6
0 118
58
180
4
1 119
96
144
4
1
120
32
360 16
0
121 110
133
3
0 122
60
186
4
1 123
80
168
4
1 124
60
224
6
0
125 100
156
4
0
126
36
312 12
0 127 126
128
2 −1 128
64
255
8
0 129
84
176
4
1
130
48
252
8 −1
131 130
132
2 −1 132
40
336 12
0 133 108
160
4
1 134
66
204
4
1
135
72
240
8
0
136
64
270
8
0 137 136
138
2 −1 138
44
288
8 −1 139 138
140
2 −1
140
48
336 12
0
141
92
192
4
1 142
70
216
4
1 143 120
168
4
1 144
48
403 15
0
145 112
180
4
1
146
72
222
4
1 147
84
228
6
0 148
72
266
6
0 149 148
150
2 −1
150
40
372 12
0
151 150
152
2 −1 152
72
300
8
0 153
96
234
6
0 154
60
288
8 −1
155 120
192
4
1
156
48
392 12
0 157 156
158
2 −1 158
78
240
4
1 159 104
216
4
1
160
64
378 12
0
161 132
192
4
1 162
54
363 10
0 163 162
164
2 −1 164
80
294
6
0
165
80
288
8 −1
166
82
252
4
1 167 166
168
2 −1 168
48
480 16
0 169 156
183
3
0
170
64
324
8 −1
171 108
260
6
0 172
84
308
6
0 173 172
174
2 −1 174
56
360
8 −1
175 120
248
6
0
176
80
372 10
0 177 116
240
4
1 178
88
270
4
1 179 178
180
2 −1
180
48
546 18
0
181 180
182
2 −1 182
72
336
8 −1 183 120
248
4
1 184
88
360
8
0
185 144
228
4
1
186
60
384
8 −1 187 160
216
4
1 188
92
336
6
0 189 108
320
8
0
190
72
360
8 −1
191 190
192
2 −1 192
64
508 14
0 193 192
194
2 −1 194
96
294
4
1
195
96
336
8 −1
196
84
399
9
0 197 196
198
2 −1 198
60
468 12
0 199 198
200
2 −1
200
80
465 12
0
201 132
272
4
1 202 100
306
4
1 203 168
240
4
1 204
64
504 12
0
205 160
252
4
1
206 102
312
4
1 207 132
312
6
0 208
96
434 10
0 209 180
240
4
1
210
48
576 16
1
211 210
212
2 −1 212 104
378
6
0 213 140
288
4
1 214 106
324
4
1
215 168
264
4
1
216
72
600 16
0 217 180
256
4
1 218 108
330
4
1 219 144
296
4
1
220
80
504 12
0
221 192
252
4
1 222
72
456
8 −1 223 222
224
2 −1 224
96
504 12
0
225 120
403
9
0
226 112
342
4
1 227 226
228
2 −1 228
72
560 12
0 229 228
230
2 −1
230
88
432
8 −1
231 120
384
8 −1 232 112
450
8
0 233 232
234
2 −1 234
72
546 12
0
235 184
288
4
1
236 116
420
6
0 237 156
320
4
1 238
96
432
8 −1 239 238
240
2 −1
240
64
744 20
0
241 240
242
2 −1 242 110
399
6
0 243 162
364
6
0 244 120
434
6
0
245 168
342
6
0
246
80
504
8 −1 247 216
280
4
1 248 120
480
8
0 249 164
336
4
1
250 100
468
8
0
251 250
252
2 −1 252
72
728 18
0 253 220
288
4
1 254 126
384
4
1
255 128
432
8 −1
256 128
511
9
0 257 256
258
2 −1 258
84
528
8 −1 259 216
304
4
1
260
96
588 12
0
261 168
390
6
0 262 130
396
4
1 263 262
264
2 −1 264
80
720 16
0
265 208
324
4
1
266 108
480
8 −1 267 176
360
4
1 268 132
476
6
0 269 268
270
2 −1
270
72
720 16
0
271 270
272
2 −1 272 128
558 10
0 273 144
448
8 −1 274 136
414
4
1
275 200
372
6
0
276
88
672 12
0 277 276
278
2 −1 278 138
420
4
1 279 180
416
6
0
280
96
720 16
0
281 280
282
2 −1 282
92
576
8 −1 283 282
284
2 −1 284 140
504
6
0
285 144
480
8 −1
286 120
504
8 −1 287 240
336
4
1 288
96
819 18
0 289 272
307
3
0
290 112
540
8 −1
291 192
392
4
1 292 144
518
6
0 293 292
294
2 −1 294
84
684 12
0
295 232
360
4
1
296 144
570
8
0 297 180
480
8
0 298 148
450
4
1 299 264
336
4
1
300
80
868 18
0
301 252
352
4
1 302 150
456
4
1 303 200
408
4
1 304 144
620 10
0
305 240
372
4
1
306
96
702 12
0 307 306
308
2 −1 308 120
672 12
0 309 204
416
4
1
310 120
576
8 −1
311 310
312
2 −1 312
96
840 16
0 313 312
314
2 −1 314 156
474
4
1
315 144
624 12
0
316 156
560
6
0 317 316
318
2 −1 318 104
648
8 −1 319 280
360
4
1
320 128
762 14
0
321 212
432
4
1 322 132
576
8 −1 323 288
360
4
1 324 108
847 15
0
325 240
434
6
0
326 162
492
4
1 327 216
440
4
1 328 160
630
8
0 329 276
384
4
1
330
80
864 16
1
331 330
332
2 −1 332 164
588
6
0 333 216
494
6
0 334 166
504
4
1
335 264
408
4
1
336
96
992 20
0 337 336
338
2 −1 338 156
549
6
0 339 224
456
4
1
340 128
756 12
0

270
Chapter 4
NUMBER THEORY
n
φ
σ
τ
µ
n
φ
σ
τ
µ
n
φ
σ
τ
µ
n
φ
σ
τ
µ
n
φ
σ
τ
µ
341 300
384
4
1 342 108
780 12
0 343 294
400
4
0 344 168
660
8
0
345 176
576
8 −1
346 172
522
4
1 347 346
348
2 −1 348 112
840 12
0 349 348
350
2 −1
350 120
744 12
0
351 216
560
8
0 352 160
756 12
0 353 352
354
2 −1 354 116
720
8 −1
355 280
432
4
1
356 176
630
6
0 357 192
576
8 −1 358 178
540
4
1 359 358
360
2 −1
360
96 1170 24
0
361 342
381
3
0 362 180
546
4
1 363 220
532
6
0 364 144
784 12
0
365 288
444
4
1
366 120
744
8 −1 367 366
368
2 −1 368 176
744 10
0 369 240
546
6
0
370 144
684
8 −1
371 312
432
4
1 372 120
896 12
0 373 372
374
2 −1 374 160
648
8 −1
375 200
624
8
0
376 184
720
8
0 377 336
420
4
1 378 108
960 16
0 379 378
380
2 −1
380 144
840 12
0
381 252
512
4
1 382 190
576
4
1 383 382
384
2 −1 384 128 1020 16
0
385 240
576
8 −1
386 192
582
4
1 387 252
572
6
0 388 192
686
6
0 389 388
390
2 −1
390
96 1008 16
1
391 352
432
4
1 392 168
855 12
0 393 260
528
4
1 394 196
594
4
1
395 312
480
4
1
396 120 1092 18
0 397 396
398
2 −1 398 198
600
4
1 399 216
640
8 −1
400 160
961 15
0
401 400
402
2 −1 402 132
816
8 −1 403 360
448
4
1 404 200
714
6
0
405 216
726 10
0
406 168
720
8 −1 407 360
456
4
1 408 128 1080 16
0 409 408
410
2 −1
410 160
756
8 −1
411 272
552
4
1 412 204
728
6
0 413 348
480
4
1 414 132
936 12
0
415 328
504
4
1
416 192
882 12
0 417 276
560
4
1 418 180
720
8 −1 419 418
420
2 −1
420
96 1344 24
0
421 420
422
2 −1 422 210
636
4
1 423 276
624
6
0 424 208
810
8
0
425 320
558
6
0
426 140
864
8 −1 427 360
496
4
1 428 212
756
6
0 429 240
672
8 −1
430 168
792
8 −1
431 430
432
2 −1 432 144 1240 20
0 433 432
434
2 −1 434 180
768
8 −1
435 224
720
8 −1
436 216
770
6
0 437 396
480
4
1 438 144
888
8 −1 439 438
440
2 −1
440 160 1080 16
0
441 252
741
9
0 442 192
756
8 −1 443 442
444
2 −1 444 144 1064 12
0
445 352
540
4
1
446 222
672
4
1 447 296
600
4
1 448 192 1016 14
0 449 448
450
2 −1
450 120 1209 18
0
451 400
504
4
1 452 224
798
6
0 453 300
608
4
1 454 226
684
4
1
455 288
672
8 −1
456 144 1200 16
0 457 456
458
2 −1 458 228
690
4
1 459 288
720
8
0
460 176 1008 12
0
461 460
462
2 −1 462 120 1152 16
1 463 462
464
2 −1 464 224
930 10
0
465 240
768
8 −1
466 232
702
4
1 467 466
468
2 −1 468 144 1274 18
0 469 396
544
4
1
470 184
864
8 −1
471 312
632
4
1 472 232
900
8
0 473 420
528
4
1 474 156
960
8 −1
475 360
620
6
0
476 192 1008 12
0 477 312
702
6
0 478 238
720
4
1 479 478
480
2 −1
480 128 1512 24
0
481 432
532
4
1 482 240
726
4
1 483 264
768
8 −1 484 220
931
9
0
485 384
588
4
1
486 162 1092 12
0 487 486
488
2 −1 488 240
930
8
0 489 324
656
4
1
490 168 1026 12
0
491 490
492
2 −1 492 160 1176 12
0 493 448
540
4
1 494 216
840
8 −1
495 240
936 12
0
496 240
992 10
0 497 420
576
4
1 498 164 1008
8 −1 499 498
500
2 −1
500 200 1092 12
0
501 332
672
4
1 502 250
756
4
1 503 502
504
2 −1 504 144 1560 24
0
505 400
612
4
1
506 220
864
8 −1 507 312
732
6
0 508 252
896
6
0 509 508
510
2 −1
510 128 1296 16
1
511 432
592
4
1 512 256 1023 10
0 513 324
800
8
0 514 256
774
4
1
515 408
624
4
1
516 168 1232 12
0 517 460
576
4
1 518 216
912
8 −1 519 344
696
4
1
520 192 1260 16
0
521 520
522
2 −1 522 168 1170 12
0 523 522
524
2 −1 524 260
924
6
0
525 240
992 12
0
526 262
792
4
1 527 480
576
4
1 528 160 1488 20
0 529 506
553
3
0
530 208
972
8 −1
531 348
780
6
0 532 216 1120 12
0 533 480
588
4
1 534 176 1080
8 −1
535 424
648
4
1
536 264 1020
8
0 537 356
720
4
1 538 268
810
4
1 539 420
684
6
0
540 144 1680 24
0
541 540
542
2 −1 542 270
816
4
1 543 360
728
4
1 544 256 1134 12
0
545 432
660
4
1
546 144 1344 16
1 547 546
548
2 −1 548 272
966
6
0 549 360
806
6
0
550 200 1116 12
0
551 504
600
4
1 552 176 1440 16
0 553 468
640
4
1 554 276
834
4
1
555 288
912
8 −1
556 276
980
6
0 557 556
558
2 −1 558 180 1248 12
0 559 504
616
4
1
560 192 1488 20
0
561 320
864
8 −1 562 280
846
4
1 563 562
564
2 −1 564 184 1344 12
0
565 448
684
4
1
566 282
852
4
1 567 324
968 10
0 568 280 1080
8
0 569 568
570
2 −1
570 144 1440 16
1
571 570
572
2 −1 572 240 1176 12
0 573 380
768
4
1 574 240 1008
8 −1
575 440
744
6
0
576 192 1651 21
0 577 576
578
2 −1 578 272
921
6
0 579 384
776
4
1
580 224 1260 12
0
581 492
672
4
1 582 192 1176
8 −1 583 520
648
4
1 584 288 1110
8
0
585 288 1092 12
0
586 292
882
4
1 587 586
588
2 −1 588 168 1596 18
0 589 540
640
4
1
590 232 1080
8 −1
591 392
792
4
1 592 288 1178 10
0 593 592
594
2 −1 594 180 1440 16
0
595 384
864
8 −1
596 296 1050
6
0 597 396
800
4
1 598 264 1008
8 −1 599 598
600
2 −1
600 160 1860 24
0
601 600
602
2 −1 602 252 1056
8 −1 603 396
884
6
0 604 300 1064
6
0
605 440
798
6
0
606 200 1224
8 −1 607 606
608
2 −1 608 288 1260 12
0 609 336
960
8 −1
610 240 1116
8 −1
611 552
672
4
1 612 192 1638 18
0 613 612
614
2 −1 614 306
924
4
1
615 320 1008
8 −1

Section 4.6
ARITHMETIC FUNCTIONS
271
n
φ
σ
τ
µ
n
φ
σ
τ
µ
n
φ
σ
τ
µ
n
φ
σ
τ
µ
n
φ
σ
τ
µ
616 240 1440 16
0 617 616
618
2 −1 618 204 1248
8 −1 619 618
620
2 −1
620 240 1344 12
0
621 396
960
8
0 622 310
936
4
1 623 528
720
4
1 624 192 1736 20
0
625 500
781
5
0
626 312
942
4
1 627 360
960
8 −1 628 312 1106
6
0 629 576
684
4
1
630 144 1872 24
0
631 630
632
2 −1 632 312 1200
8
0 633 420
848
4
1 634 316
954
4
1
635 504
768
4
1
636 208 1512 12
0 637 504
798
6
0 638 280 1080
8 −1 639 420
936
6
0
640 256 1530 16
0
641 640
642
2 −1 642 212 1296
8 −1 643 642
644
2 −1 644 264 1344 12
0
645 336 1056
8 −1
646 288 1080
8 −1 647 646
648
2 −1 648 216 1815 20
0 649 580
720
4
1
650 240 1302 12
0
651 360 1024
8 −1 652 324 1148
6
0 653 652
654
2 −1 654 216 1320
8 −1
655 520
792
4
1
656 320 1302 10
0 657 432
962
6
0 658 276 1152
8 −1 659 658
660
2 −1
660 160 2016 24
0
661 660
662
2 −1 662 330
996
4
1 663 384 1008
8 −1 664 328 1260
8
0
665 432
960
8 −1
666 216 1482 12
0 667 616
720
4
1 668 332 1176
6
0 669 444
896
4
1
670 264 1224
8 −1
671 600
744
4
1 672 192 2016 24
0 673 672
674
2 −1 674 336 1014
4
1
675 360 1240 12
0
676 312 1281
9
0 677 676
678
2 −1 678 224 1368
8 −1 679 576
784
4
1
680 256 1620 16
0
681 452
912
4
1 682 300 1152
8 −1 683 682
684
2 −1 684 216 1820 18
0
685 544
828
4
1
686 294 1200
8
0 687 456
920
4
1 688 336 1364 10
0 689 624
756
4
1
690 176 1728 16
1
691 690
692
2 −1 692 344 1218
6
0 693 360 1248 12
0 694 346 1044
4
1
695 552
840
4
1
696 224 1800 16
0 697 640
756
4
1 698 348 1050
4
1 699 464
936
4
1
700 240 1736 18
0
701 700
702
2 −1 702 216 1680 16
0 703 648
760
4
1 704 320 1524 14
0
705 368 1152
8 −1
706 352 1062
4
1 707 600
816
4
1 708 232 1680 12
0 709 708
710
2 −1
710 280 1296
8 −1
711 468 1040
6
0 712 352 1350
8
0 713 660
768
4
1 714 192 1728 16
1
715 480 1008
8 −1
716 356 1260
6
0 717 476
960
4
1 718 358 1080
4
1 719 718
720
2 −1
720 192 2418 30
0
721 612
832
4
1 722 342 1143
6
0 723 480
968
4
1 724 360 1274
6
0
725 560
930
6
0
726 220 1596 12
0 727 726
728
2 −1 728 288 1680 16
0 729 486 1093
7
0
730 288 1332
8 −1
731 672
792
4
1 732 240 1736 12
0 733 732
734
2 −1 734 366 1104
4
1
735 336 1368 12
0
736 352 1512 12
0 737 660
816
4
1 738 240 1638 12
0 739 738
740
2 −1
740 288 1596 12
0
741 432 1120
8 −1 742 312 1296
8 −1 743 742
744
2 −1 744 240 1920 16
0
745 592
900
4
1
746 372 1122
4
1 747 492 1092
6
0 748 320 1512 12
0 749 636
864
4
1
750 200 1872 16
0
751 750
752
2 −1 752 368 1488 10
0 753 500 1008
4
1 754 336 1260
8 −1
755 600
912
4
1
756 216 2240 24
0 757 756
758
2 −1 758 378 1140
4
1 759 440 1152
8 −1
760 288 1800 16
0
761 760
762
2 −1 762 252 1536
8 −1 763 648
880
4
1 764 380 1344
6
0
765 384 1404 12
0
766 382 1152
4
1 767 696
840
4
1 768 256 2044 18
0 769 768
770
2 −1
770 240 1728 16
1
771 512 1032
4
1 772 384 1358
6
0 773 772
774
2 −1 774 252 1716 12
0
775 600
992
6
0
776 384 1470
8
0 777 432 1216
8 −1 778 388 1170
4
1 779 720
840
4
1
780 192 2352 24
0
781 700
864
4
1 782 352 1296
8 −1 783 504 1200
8
0 784 336 1767 15
0
785 624
948
4
1
786 260 1584
8 −1 787 786
788
2 −1 788 392 1386
6
0 789 524 1056
4
1
790 312 1440
8 −1
791 672
912
4
1 792 240 2340 24
0 793 720
868
4
1 794 396 1194
4
1
795 416 1296
8 −1
796 396 1400
6
0 797 796
798
2 −1 798 216 1920 16
1 799 736
864
4
1
800 320 1953 18
0
801 528 1170
6
0 802 400 1206
4
1 803 720
888
4
1 804 264 1904 12
0
805 528 1152
8 −1
806 360 1344
8 −1 807 536 1080
4
1 808 400 1530
8
0 809 808
810
2 −1
810 216 2178 20
0
811 810
812
2 −1 812 336 1680 12
0 813 540 1088
4
1 814 360 1368
8 −1
815 648
984
4
1
816 256 2232 20
0 817 756
880
4
1 818 408 1230
4
1 819 432 1456 12
0
820 320 1764 12
0
821 820
822
2 −1 822 272 1656
8 −1 823 822
824
2 −1 824 408 1560
8
0
825 400 1488 12
0
826 348 1440
8 −1 827 826
828
2 −1 828 264 2184 18
0 829 828
830
2 −1
830 328 1512
8 −1
831 552 1112
4
1 832 384 1778 14
0 833 672 1026
6
0 834 276 1680
8 −1
835 664 1008
4
1
836 360 1680 12
0 837 540 1280
8
0 838 418 1260
4
1 839 838
840
2 −1
840 192 2880 32
0
841 812
871
3
0 842 420 1266
4
1 843 560 1128
4
1 844 420 1484
6
0
845 624 1098
6
0
846 276 1872 12
0 847 660 1064
6
0 848 416 1674 10
0 849 564 1136
4
1
850 320 1674 12
0
851 792
912
4
1 852 280 2016 12
0 853 852
854
2 −1 854 360 1488
8 −1
855 432 1560 12
0
856 424 1620
8
0 857 856
858
2 −1 858 240 2016 16
1 859 858
860
2 −1
860 336 1848 12
0
861 480 1344
8 −1 862 430 1296
4
1 863 862
864
2 −1 864 288 2520 24
0
865 688 1044
4
1
866 432 1302
4
1 867 544 1228
6
0 868 360 1792 12
0 869 780
960
4
1
870 224 2160 16
1
871 792
952
4
1 872 432 1650
8
0 873 576 1274
6
0 874 396 1440
8 −1
875 600 1248
8
0
876 288 2072 12
0 877 876
878
2 −1 878 438 1320
4
1 879 584 1176
4
1
880 320 2232 20
0
881 880
882
2 −1 882 252 2223 18
0 883 882
884
2 −1 884 384 1764 12
0
885 464 1440
8 −1
886 442 1332
4
1 887 886
888
2 −1 888 288 2280 16
0 889 756 1024
4
1
890 352 1620
8 −1

272
Chapter 4
NUMBER THEORY
n
φ
σ
τ
µ
n
φ
σ
τ
µ
n
φ
σ
τ
µ
n
φ
σ
τ
µ
n
φ
σ
τ
µ
891 540 1452 10
0 892 444 1568
6
0 893 828
960
4
1 894 296 1800
8 −1
895 712 1080
4
1
896 384 2040 16
0 897 528 1344
8 −1 898 448 1350
4
1 899 840
960
4
1
900 240 2821 27
0
901 832
972
4
1 902 400 1512
8 −1 903 504 1408
8 −1 904 448 1710
8
0
905 720 1092
4
1
906 300 1824
8 −1 907 906
908
2 −1 908 452 1596
6
0 909 600 1326
6
0
910 288 2016 16
1
911 910
912
2 −1 912 288 2480 20
0 913 820 1008
4
1 914 456 1374
4
1
915 480 1488
8 −1
916 456 1610
6
0 917 780 1056
4
1 918 288 2160 16
0 919 918
920
2 −1
920 352 2160 16
0
921 612 1232
4
1 922 460 1386
4
1 923 840 1008
4
1 924 240 2688 24
0
925 720 1178
6
0
926 462 1392
4
1 927 612 1352
6
0 928 448 1890 12
0 929 928
930
2 −1
930 240 2304 16
1
931 756 1140
6
0 932 464 1638
6
0 933 620 1248
4
1 934 466 1404
4
1
935 640 1296
8 −1
936 288 2730 24
0 937 936
938
2 −1 938 396 1632
8 −1 939 624 1256
4
1
940 368 2016 12
0
941 940
942
2 −1 942 312 1896
8 −1 943 880 1008
4
1 944 464 1860 10
0
945 432 1920 16
0
946 420 1584
8 −1 947 946
948
2 −1 948 312 2240 12
0 949 864 1036
4
1
950 360 1860 12
0
951 632 1272
4
1 952 384 2160 16
0 953 952
954
2 −1 954 312 2106 12
0
955 760 1152
4
1
956 476 1680
6
0 957 560 1440
8 −1 958 478 1440
4
1 959 816 1104
4
1
960 256 3048 28
0
961 930
993
3
0 962 432 1596
8 −1 963 636 1404
6
0 964 480 1694
6
0
965 768 1164
4
1
966 264 2304 16
1 967 966
968
2 −1 968 440 1995 12
0 969 576 1440
8 −1
970 384 1764
8 −1
971 970
972
2 −1 972 324 2548 18
0 973 828 1120
4
1 974 486 1464
4
1
975 480 1736 12
0
976 480 1922 10
0 977 976
978
2 −1 978 324 1968
8 −1 979 880 1080
4
1
980 336 2394 18
0
981 648 1430
6
0 982 490 1476
4
1 983 982
984
2 −1 984 320 2520 16
0
985 784 1188
4
1
986 448 1620
8 −1 987 552 1536
8 −1 988 432 1960 12
0 989 924 1056
4
1
990 240 2808 24
0
991 990
992
2 −1 992 480 2016 12
0 993 660 1328
4
1 994 420 1728
8 −1
995 792 1200
4
1
996 328 2352 12
0 997 996
998
2 −1 998 498 1500
4
1 999 648 1520
8
0 1000 400 2340 16
0
4.6.4
THE M ¨OBIUS FUNCTION AND OTHER IMPORTANT ARITHMETIC FUNCTIONS
Deﬁnitions:
If n is a positive integer, µ(n), the value of the M¨obius function, is deﬁned by
µ(n) =





1,
if n = 1
0,
if n has a square factor larger than 1
(−1)s,
if n is square free and is the product of s diﬀerent primes.
If n > 1 is a positive integer, with prime-power factorization pa1
1 pa2
2 . . . pam
m , then λ(n), the
value of Liouville’s function at n, is given by λ(n) = (−1)a1+a2+···+am, with λ(1) = 1.
If n is a positive integer with prime-power factorization n = pa1
1 pa2
2 . . . pam
m , then the arith-
metic functions Ωand ω are deﬁned by Ω(1) = ω(1) = 0 and for n > 1, Ω(n) = Pm
i=1 ai
and ω(n) = m. That is, Ω(n) is the sum of exponents in the prime-power factorization
of n and ω(n) is the number of distinct primes in the prime-power factorization of n.
Facts:
1. The M¨obius function is multiplicative, but not completely multiplicative.
2. M¨obius inversion formula: If f is an arithmetic function and F(n) = P
d|n f(d), then
f(n) = P
d|n µ(d)F( n
d ).
3. If n is a positive integer, then φ(n) = P
d|n µ(d) n
d .
4. If f is multiplicative, then P
d|n µ(d)f(d) = Q
p|n(1 −f(p)).
5. If f is multiplicative, then P
d|n µ(d)2f(d) = Q
p|n(1 + f(p)).

Section 4.6
ARITHMETIC FUNCTIONS
273
6. If n is a positive integer, then P
d|n µ(d) =
(
1
if n = 1;
0
if n > 1.
7. If n is a positive integer, then P
d|n λ(d) =
(
1
if n is a perfect square;
0
if n is not a perfect square.
8. In 1897 Mertens showed that | Pn
k=1 µ(k)| < √n for all positive integers n not exceed-
ing 10,000 and conjectured that this inequality holds for all positive integers n. However,
in 1985 Odlyzko and teRiele disproved this conjecture, which went by the name Mertens’
conjecture, without giving an explicit integer n for which the conjecture fails. In 1987
Pintz showed that there is at least one counterexample n with n ≤1065, again without
giving an explicit counterexample n. Finding such an integer n requires more computing
power than is currently available.
9. Liouville’s function is completely multiplicative.
10. The function ω is additive and the function Ωis completely additive.
Examples:
1. Table 1 lists the values of µ(n) for 1 ≤n ≤1,000.
2. µ(12) = 0 since 22|12 and µ(105) = µ(3 · 5 · 7) = (−1)3 = −1.
3. λ(720) = λ(24 · 32 · 5) = (−1)4+2+1 = (−1)7 = −1.
4. Ω(720) = Ω(24 · 32 · 5) = 4 + 2 + 1 = 7 and ω(720) = ω(24 · 32 · 5) = 3.
4.6.5
DIRICHLET PRODUCTS
Deﬁnitions:
If f and g are arithmetic functions, then the Dirichlet product of f and g is the
function f ⋆g deﬁned by (f ⋆g)(n) = P
d|n f(d)g( n
d ).
If f and g are arithmetic functions such that f ⋆g = g ⋆f = I, where I(n) =
 1
n

, then g
is the Dirichlet inverse of f.
Facts:
1. If f and g are arithmetic functions, then f ⋆g = g ⋆f.
2. If f, g, and h are arithmetic functions, then (f ⋆g) ⋆h = f ⋆(g ⋆h).
3. If f, g, and h are arithmetic functions, then f ⋆(g + h) = (f ⋆g) + (f ⋆h).
4. Because of Facts 1–3, the set of arithmetic functions with the operations of Dirichlet
product and ordinary addition of functions forms a ring. (See Chapter 5.)
5. If f is an arithmetic function with f(1) ̸= 0, then there is a unique Dirichlet inverse
of f, which is written as f −1.
Furthermore, f −1 is given by the recursive formulas
f −1(1) =
1
f(1) and f −1(n) = −
1
f(1)
P
d|n
d<n f( n
d )f −1(d) for n > 1.
6. The set of all arithmetic functions f with f(1) ̸= 0 forms an abelian group with
respect to the operation ⋆, where the identity element is the function I.
7. If f and g are arithmetic functions with f(1) ̸= 0 and g(1) ̸= 0, then (f ∗g)−1 =
f −1 ⋆g−1.
8. If u is the arithmetic function with u(n) = 1 for all positive integers n, then µ⋆u = I,
so u = µ−1 and µ = u−1.

274
Chapter 4
NUMBER THEORY
9. If f is a multiplicative function, then f is completely multiplicative if and only if
f −1(n) = µ(n)f(n) for all positive integers n.
10. If f and g are multiplicative functions, then f ⋆g is also multiplicative.
11. If f and g are arithmetic functions and both f and f ⋆g are multiplicative, then g
is also multiplicative.
12. If f is multiplicative, then f −1 exists and is multiplicative.
Examples:
1. The identity φ(n) = P
d|n µ(d) n
d (§4.6.4, Fact 3) implies that φ = µ ⋆N where N is
the multiplicative function N(n) = n.
2. Since the function N is completely multiplicative, N −1 = µN by Fact 9.
3. From Example 1 and Facts 7 and 8, it follows that φ−1 = µ−1 ⋆µN = µ ⋆µN. Hence
φ−1(n) = P
d|n dµ(d).
4.7
PRIMITIVE ROOTS AND QUADRATIC RESIDUES
A primitive root of an integer, when it exists, is an integer whose powers run through
a complete system of residues modulo this integer. When a primitive root exists, it is
possible to use the theory of indices to solve certain congruences. This section provides
the information needed to understand and employ primitive roots.
The question of which integers are perfect squares modulo a prime is one that has been
studied extensively. An integer that is a perfect square modulo n is called a quadratic
residue of n. The law of quadratic reciprocity provides a surprising link between the
answer to the question of whether a prime p is a perfect square modulo a prime q and the
answer to the question of whether q is a perfect square modulo p. This section provides
information that helps determine whether an integer is a quadratic residue modulo a
given integer n.
There are important applications of the topics covered in this section, including applica-
tions to public key cryptography and authentication schemes. (See Chapter 15.)
4.7.1
PRIMITIVE ROOTS
Deﬁnitions:
If a and m are relatively prime positive integers, then the order of a modulo m,
denoted ordma, is the least positive integer x such that ax ≡1 (mod m).
If r and n are relatively prime integers and n is positive, then r is a primitive root
modulo m if ordnr = φ(n). A primitive root modulo m is also said to be a primitive
root of m and m is said to have a primitive root.
If m is a positive integer, then the minimum universal exponent modulo m is the
smallest positive integer λ(m) for which aλ(m) ≡1 (mod m) for all integers a relatively
prime to m.

Section 4.7
PRIMITIVE ROOTS AND QUADRATIC RESIDUES
275
Facts:
1. The positive integer n, with n > 1, has a primitive root if and only if n = 2, 4, pt, or
2pt, where p is an odd prime and t is a positive integer.
2. There are φ(d) incongruent integers modulo p if p is prime and d is a positive divisor
of p −1.
3. There are φ(p −1) primitive roots of p if p is a prime.
4. If the positive integer m has a primitive root, then it has a total of φ(φ(m)) incon-
gruent primitive roots.
5. If r is a primitive root of the odd prime p, then either r or r + p is a primitive root
modulo p2.
6. If r is a primitive root of p2, where p is prime, then r is a primitive root of pk for all
positive integers k.
7. It is an unsettled conjecture (stated by E. Artin) whether 2 is a primitive root of
inﬁnitely many primes. More generally, given any prime p it is unknown whether p is a
primitive root of inﬁnitely many primes.
8. It is known that given any three primes, at least one of these primes is a primitive
root of inﬁnitely many primes [GuMu84].
9. Given a set of n primes, p1, p2, . . . , pn, there are Qn
k=1 φ(pk −1) integers x with
1 < x ≤Qn
k=1 pk such that x is a primitive root of pk for k = 1, 2, . . . , n. Such an
integer x is a called a common primitive root of the primes p1, . . . , pn.
10. Let gp denote the smallest positive integer that is a primitive root modulo p, where
p is a prime. It is known that gp is not always small; in particular it has been shown
by Fridlender and Sali´e that there is a positive constant C such that gp > C log p for
inﬁnitely many primes p [Ri96].
11. Burgess has shown that gp does not grow too rapidly; in particular he showed that
gp ≤Cp
1
4 +ǫ for ǫ > 0, C a constant, and p suﬃciently large [Ri96].
12. The minimum universal exponent modulo the powers of 2 are λ(2) = 1, λ(22) = 2,
and λ(2k) = 2k−2 for k = 3, 4, . . ..
13. If m is a positive integer with prime-power factorization 2kqa1
1 . . . qar
r , where k
is a nonnegative integer, then the least universal exponent of m is given by λ(m) =
lcm(λ(2k), φ(qa1
1 ), . . . , φ(qar
r )).
14. For every positive integer m, there is an integer a such that ordma = λ(m).
15. There are six positive integers m with λ(m) = 2: m = 3, 4, 6, 8, 12, 24.
16. The following table displays the least primitive root ω of each prime less than 10,000.
p
ω
p
ω
p
ω
p
ω
p
ω
p
ω
p
ω
p
ω
p
ω
3
2
5
2
7
3
11
2
13
2
17
3
19
2
23
5
29
2
31
3
37
2
41
6
43
3
47
5
53
2
59
2
61
2
67
2
71
7
73
5
79
3
83
2
89
3
97
5
101
2
103
5
107
2
109
6
113
3
127
3
131
2
137
3
139
2
149
2
151
6
157
5
163
2
167
5
173
2
179
2
181
2
191 19
193
5
197
2
199
3
211
2
223
3
227
2
229
6
233
3
239
7
241
7
251
6
257
3
263
5
269
2
271
6
277
5
281
3
283
3
293
2
307
5
311 17
313 10
317
2
331
3
337 10
347
2
349
2
353
3
359
7
367
6

276
Chapter 4
NUMBER THEORY
p
ω
p
ω
p
ω
p
ω
p
ω
p
ω
p
ω
p
ω
p
ω
373
2
379
2
383
5
389
2
397
5
401
3
409 21
419
2
421
2
431
7
433
5
439 15
443
2
449
3
457 13
461
2
463
3
467
2
479 13
487
3
491
2
499
7
503
5
509
2
521
3
523
2
541
2
547
2
557
2
563
2
569
3
571
3
577
5
587
2
593
3
599
7
601
7
607
3
613
2
617
3
619
2
631
3
641
3
643 11
647
5
653
2
659
2
661
2
673
5
677
2
683
5
691
3
701
2
709
2
719 11
727
5
733
6
739
3
743
5
751
3
757
2
761
6
769 11
773
2
787
2
797
2
809
3
811
3
821
2
823
3
827
2
829
2
839 11
853
2
857
3
859
2
863
5
877
2
881
3
883
2
887
5
907
2
911 17
919
7
929
3
937
5
941
2
947
2
953
3
967
5
971
6
977
3
983
5
991
6
997
7 1009 11 1013
3 1019
2 1021 10
1031 14 1033
5 1039
3 1049
3 1051
7 1061
2 1063
3 1069
6 1087
3
1091
2 1093
5 1097
3 1103
5 1109
2 1117
2 1123
2 1129 11 1151 17
1153
5 1163
5 1171
2 1181
7 1187
2 1193
3 1201 11 1213
2 1217
3
1223
5 1229
2 1231
3 1237
2 1249
7 1259
2 1277
2 1279
3 1283
2
1289
6 1291
2 1297 10 1301
2 1303
6 1307
2 1319 13 1321 13 1327
3
1361
3 1367
5 1373
2 1381
2 1399 13 1409
3 1423
3 1427
2 1429
6
1433
3 1439
7 1447
3 1451
2 1453
2 1459
3 1471
6 1481
3 1483
2
1487
5 1489 14 1493
2 1499
2 1511 11 1523
2 1531
2 1543
5 1549
2
1553
3 1559 19 1567
3 1571
2 1579
3 1583
5 1597 11 1601
3 1607
5
1609
7 1613
3 1619
2 1621
2 1627
3 1637
2 1657 11 1663
3 1667
2
1669
2 1693
2 1697
3 1699
3 1709
3 1721
3 1723
3 1733
2 1741
2
1747
2 1753
7 1759
6 1777
5 1783 10 1787
2 1789
6 1801 11 1811
6
1823
5 1831
3 1847
5 1861
2 1867
2 1871 14 1873 10 1877
2 1879
6
1889
3 1901
2 1907
2 1913
3 1931
2 1933
5 1949
2 1951
3 1973
2
1979
2 1987
2 1993
5 1997
2 1999
3 2003
5 2011
3 2017
5 2027
2
2029
2 2039
7 2053
2 2063
5 2069
2 2081
3 2083
2 2087
5 2089
7
2099
2 2111
7 2113
5 2129
3 2131
2 2137 10 2141
2 2143
3 2153
3
2161 23 2179
7 2203
5 2207
5 2213
2 2221
2 2237
2 2239
3 2243
2
2251
7 2267
2 2269
2 2273
3 2281
7 2287 19 2293
2 2297
5 2309
2
2311
3 2333
2 2339
2 2341
7 2347
3 2351 13 2357
2 2371
2 2377
5
2381
3 2383
5 2389
2 2393
3 2399 11 2411
6 2417
3 2423
5 2437
2
2441
6 2447
5 2459
2 2467
2 2473
5 2477
2 2503
3 2521 17 2531
2
2539
2 2543
5 2549
2 2551
6 2557
2 2579
2 2591
7 2593
7 2609
3
2617
5 2621
2 2633
3 2647
3 2657
3 2659
2 2663
5 2671
7 2677
2
2683
2 2687
5 2689 19 2693
2 2699
2 2707
2 2711
7 2713
5 2719
3
2729
3 2731
3 2741
2 2749
6 2753
3 2767
3 2777
3 2789
2 2791
6
2797
2 2801
3 2803
2 2819
2 2833
5 2837
2 2843
2 2851
2 2857 11
2861
2 2879
7 2887
5 2897
3 2903
5 2909
2 2917
5 2927
5 2939
2
2953 13 2957
2 2963
2 2969
3 2971 10 2999 17 3001 14 3011
2 3019
2
3023
5 3037
2 3041
3 3049 11 3061
6 3067
2 3079
6 3083
2 3089
3
3109
6 3119
7 3121
7 3137
3 3163
3 3167
5 3169
7 3181
7 3187
2
3191 11 3203
2 3209
3 3217
5 3221 10 3229
6 3251
6 3253
2 3257
3

Section 4.7
PRIMITIVE ROOTS AND QUADRATIC RESIDUES
277
p
ω
p
ω
p
ω
p
ω
p
ω
p
ω
p
ω
p
ω
p
ω
3259
3 3271
3 3299
2 3301
6 3307
2 3313 10 3319
6 3323
2 3329
3
3331
3 3343
5 3347
2 3359 11 3361 22 3371
2 3373
5 3389
3 3391
3
3407
5 3413
2 3433
5 3449
3 3457
7 3461
2 3463
3 3467
2 3469
2
3491
2 3499
2 3511
7 3517
2 3527
5 3529 17 3533
2 3539
2 3541
7
3547
2 3557
2 3559
3 3571
2 3581
2 3583
3 3593
3 3607
5 3613
2
3617
3 3623
5 3631 15 3637
2 3643
2 3659
2 3671 13 3673
5 3677
2
3691
2 3697
5 3701
2 3709
2 3719
7 3727
3 3733
2 3739
7 3761
3
3767
5 3769
7 3779
2 3793
5 3797
2 3803
2 3821
3 3823
3 3833
3
3847
5 3851
2 3853
2 3863
5 3877
2 3881 13 3889 11 3907
2 3911 13
3917
2 3919
3 3923
2 3929
3 3931
2 3943
3 3947
2 3967
6 3989
2
4001
3 4003
2 4007
5 4013
2 4019
2 4021
2 4027
3 4049
3 4051 10
4057
5 4073
3 4079 11 4091
2 4093
2 4099
2 4111 12 4127
5 4129 13
4133
2 4139
2 4153
5 4157
2 4159
3 4177
5 4201 11 4211
6 4217
3
4219
2 4229
2 4231
3 4241
3 4243
2 4253
2 4259
2 4261
2 4271
7
4273
5 4283
2 4289
3 4297
5 4327
3 4337
3 4339 10 4349
2 4357
2
4363
2 4373
2 4391 14 4397
2 4409
3 4421
3 4423
3 4441 21 4447
3
4451
2 4457
3 4463
5 4481
3 4483
2 4493
2 4507
2 4513
7 4517
2
4519
3 4523
5 4547
2 4549
6 4561 11 4567
3 4583
5 4591 11 4597
5
4603
2 4621
2 4637
2 4639
3 4643
5 4649
3 4651
3 4657 15 4663
3
4673
3 4679 11 4691
2 4703
5 4721
6 4723
2 4729 17 4733
5 4751 19
4759
3 4783
6 4787
2 4789
2 4793
3 4799
7 4801
7 4813
2 4817
3
4831
3 4861 11 4871 11 4877
2 4889
3 4903
3 4909
6 4919 13 4931
6
4933
2 4937
3 4943
7 4951
6 4957
2 4967
5 4969 11 4973
2 4987
2
4993
5 4999
3 5003
2 5009
3 5011
2 5021
3 5023
3 5039 11 5051
2
5059
2 5077
2 5081
3 5087
5 5099
2 5101
6 5107
2 5113 19 5119
3
5147
2 5153
5 5167
6 5171
2 5179
2 5189
2 5197
7 5209 17 5227
2
5231
7 5233 10 5237
3 5261
2 5273
3 5279
7 5281
7 5297
3 5303
5
5309
2 5323
5 5333
2 5347
3 5351 11 5381
3 5387
2 5393
3 5399
7
5407
3 5413
5 5417
3 5419
3 5431
3 5437
5 5441
3 5443
2 5449
7
5471
7 5477
2 5479
3 5483
2 5501
2 5503
3 5507
2 5519 13 5521 11
5527
5 5531 10 5557
2 5563
2 5569 13 5573
2 5581
6 5591 11 5623
5
5639
7 5641 14 5647
3 5651
2 5653
5 5657
3 5659
2 5669
3 5683
2
5689 11 5693
2 5701
2 5711 19 5717
2 5737
5 5741
2 5743 10 5749
2
5779
2 5783
7 5791
6 5801
3 5807
5 5813
2 5821
6 5827
2 5839
6
5843
2 5849
3 5851
2 5857
7 5861
3 5867
5 5869
2 5879 11 5881 31
5897
3 5903
5 5923
2 5927
5 5939
2 5953
7 5981
3 5987
2 6007
3
6011
2 6029
2 6037
5 6043
5 6047
5 6053
2 6067
2 6073 10 6079 17
6089
3 6091
7 6101
2 6113
3 6121
7 6131
2 6133
5 6143
5 6151
3
6163
3 6173
2 6197
2 6199
3 6203
2 6211
2 6217
5 6221
3 6229
2
6247
5 6257
3 6263
5 6269
2 6271 11 6277
2 6287
7 6299
2 6301 10
6311
7 6317
2 6323
2 6329
3 6337 10 6343
3 6353
3 6359 13 6361 19
6367
3 6373
2 6379
2 6389
2 6397
2 6421
6 6427
3 6449
3 6451
3
6469
2 6473
3 6481
7 6491
2 6521
6 6529
7 6547
2 6551 17 6553 10

278
Chapter 4
NUMBER THEORY
p
ω
p
ω
p
ω
p
ω
p
ω
p
ω
p
ω
p
ω
p
ω
6563
5 6569
3 6571
3 6577
5 6581 14 6599 13 6607
3 6619
2 6637
2
6653
2 6659
2 6661
6 6673
5 6679
7 6689
3 6691
2 6701
2 6703
5
6709
2 6719 11 6733
2 6737
3 6761
3 6763
2 6779
2 6781
2 6791
7
6793 10 6803
2 6823
3 6827
2 6829
2 6833
3 6841 22 6857
3 6863
5
6869
2 6871
3 6883
2 6899
2 6907
2 6911
7 6917
2 6947
2 6949
2
6959
7 6961 13 6967
5 6971
2 6977
3 6983
5 6991
6 6997
5 7001
3
7013
2 7019
2 7027
2 7039
3 7043
2 7057
5 7069
2 7079
7 7103
5
7109
2 7121
3 7127
5 7129
7 7151
7 7159
3 7177 10 7187
2 7193
3
7207
3 7211
2 7213
5 7219
2 7229
2 7237
2 7243
2 7247
5 7253
2
7283
2 7297
5 7307
2 7309
6 7321
7 7331
2 7333
6 7349
2 7351
6
7369
7 7393
5 7411
2 7417
5 7433
3 7451
2 7457
3 7459
2 7477
2
7481
6 7487
5 7489
7 7499
2 7507
2 7517
2 7523
2 7529
3 7537
7
7541
2 7547
2 7549
2 7559 13 7561 13 7573
2 7577
3 7583
5 7589
2
7591
6 7603
2 7607
5 7621
2 7639
7 7643
2 7649
3 7669
2 7673
3
7681 17 7687
6 7691
2 7699
3 7703
5 7717
2 7723
3 7727
5 7741
7
7753 10 7757
2 7759
3 7789
2 7793
3 7817
3 7823
5 7829
2 7841 12
7853
2 7867
3 7873
5 7877
2 7879
3 7883
2 7901
2 7907
2 7919
7
7927
3 7933
2 7937
3 7949
2 7951
6 7963
5 7993
5 8009
3 8011 14
8017
5 8039 11 8053
2 8059
3 8069
2 8081
3 8087
5 8089 17 8093
2
8101
6 8111 11 8117
2 8123
2 8147
2 8161
7 8167
3 8171
2 8179
2
8191 17 8209
7 8219
2 8221
2 8231 11 8233 10 8237
2 8243
2 8263
3
8269
2 8273
3 8287
3 8291
2 8293
2 8297
3 8311
3 8317
6 8329
7
8353
5 8363
2 8369
3 8377
5 8387
2 8389
6 8419
3 8423
5 8429
2
8431
3 8443
2 8447
5 8461
6 8467
2 8501
7 8513
5 8521 13 8527
5
8537
3 8539
2 8543
5 8563
2 8573
2 8581
6 8597
2 8599
3 8609
3
8623
3 8627
2 8629
6 8641 17 8647
3 8663
5 8669
2 8677
2 8681 15
8689 13 8693
2 8699
2 8707
5 8713
5 8719
3 8731
2 8737
5 8741
2
8747
2 8753
3 8761 23 8779 11 8783
5 8803
2 8807
5 8819
2 8821
2
8831
7 8837
2 8839
3 8849
3 8861
2 8863
3 8867
2 8887
3 8893
5
8923
2 8929 11 8933
2 8941
6 8951 13 8963
2 8969
3 8971
2 8999
7
9001
7 9007
3 9011
2 9013
5 9029
2 9041
3 9043
3 9049
7 9059
2
9067
3 9091
3 9103
6 9109 10 9127
3 9133
6 9137
3 9151
3 9157
6
9161
3 9173
2 9181
2 9187
3 9199
3 9203
2 9209
3 9221
2 9227
2
9239 19 9241 13 9257
3 9277
5 9281
3 9283
2 9293
2 9311
7 9319
3
9323
2 9337
5 9341
2 9343
5 9349
2 9371
2 9377
3 9391
3 9397
2
9403
3 9413
3 9419
2 9421
2 9431
7 9433
5 9437
2 9439 22 9461
3
9463
3 9467
2 9473
3 9479
7 9491
2 9497
3 9511
3 9521
3 9533
2
9539
2 9547
2 9551 11 9587
2 9601 13 9613
2 9619
2 9623
5 9629
2
9631
3 9643
2 9649
7 9661
2 9677
2 9679
3 9689
3 9697 10 9719 17
9721
7 9733
2 9739
3 9743
5 9749
2 9767
5 9769 13 9781
6 9787
3
9791 11 9803
2 9811
3 9817
5 9829 10 9833
3 9839
7 9851
2 9857
5
9859
2 9871
3 9883
2 9887
5 9901
2 9907
2 9923
2 9929
3 9931 10
9941
2 9949
2 9967
3 9973 11

Section 4.7
PRIMITIVE ROOTS AND QUADRATIC RESIDUES
279
Examples:
1. Since 21 ≡2, 22 ≡4, and 23 ≡1 (mod 7), it follows that ord72 = 3.
2. The integers 2, 6, 7, 8 form a complete set of incongruent primitive roots modulo 11.
3. The integer 10 is a primitive root of 487, but it is not a primitive root of 4872.
4. There are φ(6)φ(10) = 2 · 4 = 8 common primitive roots of 7 and 11 between 1 and
7 · 11 = 77. They are the integers 17, 19, 24, 40, 52, 61, 68, 73.
5. From Facts 12 and 13 it follows that the minimum universal exponent of 1,200 is
λ(7,200) = λ(25 · 32 · 52) = lcm(23, φ(32), φ(52)) = lcm(8, 6, 20) = 120.
4.7.2
INDEX ARITHMETIC
Deﬁnition:
If m is a positive integer with primitive root r and a is an integer relatively prime to m,
then the unique nonnegative integer x not exceeding φ(m) with rx ≡a (mod m) is the
index of a to the base r modulo m, or the discrete logarithm of a to the base r
modulo m.
The index is denoted indra (where the modulus m is ﬁxed).
Facts:
1. Table 1 displays, for each prime p < 100, the indices of all numbers not exceeding p
using the least primitive root g of p as the base. If gx = y, the table on the left has a y
in position x, while the one on the right has an x in position y.
Table 1: Indices for primes less than 100.
3: N 0 1 2 3 4 5 6 7 8 9
0
2 1
I 0 1 2 3 4 5 6 7 8 9
0 1 2 1
5: N 0 1 2 3 4 5 6 7 8 9
0
4 1 3 2
I 0 1 2 3 4 5 6 7 8 9
0 1 2 4 3 1
7: N 0 1 2 3 4 5 6 7 8 9
0
6 2 1 4 5 3
I 0 1 2 3 4 5 6 7 8 9
0 1 3 2 6 4 5 1
11: N 0
1
2 3 4 5 6 7 8 9
0
10 1 8 2 4 9 7 3 6
1
5
I 0 1 2 3 4
5
6 7 8 9
0 1 2 4 8 5 10 9 7 3 6
1 1
13: N
0
1
2 3 4 5 6
7
8 9
0
12 1 4 2 9 5 11 3 8
1
10
7
6
I
0
1 2 3 4 5
6
7
8 9
0
1
2 4 8 3 6 12 11 9 5
1 10 7 1
17: N 0
1
2
3
4
5
6
7
8
9
0
16 14 1 12 5 15 11 10 2
1
3
7
13 4
9
6
8
I 0 1 2
3
4
5
6
7
8
9
0 1 3 9 10 13 5 15 11 16 14
1 8 7 4 12
2
6
1

280
Chapter 4
NUMBER THEORY
19: N
0
1
2
3
4
5
6
7
8 9
0
18
1
13 2 16 14
6
3 8
1
17 12 15
5
7 11
4
10 9
I
0
1
2
3
4
5
6
7
8
9
0
1
2
4
8 16 13 7 14 9 18
1 17 15 11 3
6
12 5 10 1
23: N 0
1
2
3
4
5
6
7
8
9
0
22
2
16
4
1
18 19
6
10
1
3
9
20 14 21 17
8
7
12 15
2
5 13 11
I
0
1
2
3
4
5
6
7
8
9
0
1
5
2
10
4
20 8 17 16 11
1
9
22 18 21 13 19 3 15
6
7
2 12 14
1
29: N
0
1
2
3
4
5
6
7
8
9
0
28
1
5
2
22
6
12
3
10
1
23 25
7
18 13 27
4
21 11
9
2
24 17 26 20
8
16 19 15 14
I
0
1
2
3
4
5
6
7
8
9
0
1
2
4
8
16
3
6
12 24 19
1
9
18 7 14 28 27 25 21 13 26
2 23 17 5 10 20 11 22 15
1
31: N
0
1
2
3
4
5
6
7
8
9
0
30 24
1
18 20 25 28 12 2
1
14 23 19 11 22 21
6
7
26 4
2
8
29 17 27 13 10
5
3
16 9
3
15
I
0
1
2
3
4
5
6
7
8
9
0
1
3
9
27 19 26 16 17 20 29
1 25 13
8
24 10 30 28 22
4
12
2
5
15 14 11
2
6
18 23
7
21
3
1
37: N
0
1
2
3
4
5
6
7
8
9
0
36
1
26
2
23 27 32
3
16
1
24 30 28 11 33 13
4
7
17 35
2
25 22 31 15 29 10 12
6
34 21
3
14
9
5
20
8
19 18
I
0
1
2
3
4
5
6
7
8
9
0
1
2
4
8
16 32 27 17 34 31
1 25 13 26 15 30 23
9
18 36 35
2 33 29 21
5
10 20
3
6
12 26
3 11 22
7
14 28 19
1
41: N
0
1
2
3
4
5
6
7
8
9
0
40 26 15 12 22
1
39 38 30
1
8
3
27 31 25 37 24 33 16
9
2
34 14 29 36 13
4
17
5
11
7
3
23 28 10 18 19 21
2
32 35
6
4
20
I
0
1
2
3
4
5
6
7
8
9
0
1
6
36 11 25 27 39 29 10 19
1 32 28
4
24 21
3
18 26 33 34
2 40 35
5
30 16 14
2
12 31 22
3
9
13 37 17 20 38 23 15
8
7
4
1
43: N
0
1
2
3
4
5
6
7
8
9
0
42 27
1
12 25 28 35 39
2
1
10 30 13 32 20 26 24 38 29 19
2
37 36 15 16 40
8
17
3
5
41
3
11 34
9
31 23 18 14
7
4
33
4
22
6
21
I
0
1
2
3
4
5
6
7
8
9
0
1
3
9
27 38 28 41 37 25 32
1 10 30
4
12 36 22 23 26 35 19
2 14 42 40 34 16
5
15
2
6
18
3 11 33 13 39 31
7
21 20 17
8
4 24 29
1
47: N
0
1
2
3
4
5
6
7
8
9
0
46 18 20 36
1
38 32
8
40
1
19
7
10 11
4
21 26 16 12 45
2
37
6
25
5
28
2
29 14 22 35
3
39
3
44 27 34 33 30 42 17 31
4
9
15 24 13 43 41 23
I
0
1
2
3
4
5
6
7
8
9
0
1
5
25 31 14 23 21 11
8
40
1 12 13 18 43 27 41 17 38
2
10
2
3
15 28 46 42 22 16 33 24 26
3 36 39
7
35 34 29
4
20
6
30
4
9
45 37 44 32 19
1

Section 4.7
PRIMITIVE ROOTS AND QUADRATIC RESIDUES
281
53: N
0
1
2
3
4
5
6
7
8
9
0
52
1
17
2
47 18 14
3
34
1
48
6
19 24 15 12
4
10 35 37
2
49 31
7
39 20 42 25 51 16 46
3
13 33
5
23 11
9
36 30 38 41
4
50 45 32 22
8
29 40 44 21 28
5
43 27 26
I
0
1
2
3
4
5
6
7
8
9
0
1
2
4
8
16 32 11 22 44 35
1 17 34 15 30
7
14 28
3
6
12
2 24 48 43 33 13 26 52 51 49 45
3 37 21 42 31
9
18 36 19 38 23
4 46 39 25 50 47 41 29
5
10 20
5 40 27
1
59: N
0
1
2
3
4
5
6
7
8
9
0
58
1
50
2
6
51 18
3
42
1
7
25 52 45 19 56
4
40 43 38
2
8
10 26 15 53 12 46 34 20 28
3
57 49
5
17 41 24 44 55 39 37
4
9
14 11 33 27 48 16 23 54 36
5
13 32 47 22 35 31 21 30 29
I
0
1
2
3
4
5
6
7
8
9
0
1
2
4
8
16 32
5
10 20 40
1 21 42 25 50 41 23 46 33
7
14
2 28 56 53 47 35 11 22 44 29 58
3 57 55 51 43 27 54 49 39 19 38
4 17 34
9
18 36 13 26 52 45 31
5
3
6
12 24 48 37 15 30
1
61: N
0
1
2
3
4
5
6
7
8
9
0
60
1
6
2
22
7
49
3
12
1
23 15
8
40 50 28
4
47 13 26
2
24 55 16 57
9
44 41 18 51 35
3
29 59
5
21 48 11 14 39 27 46
4
25 54 56 43 17 34 58 20 10 38
5
45 53 42 33 19 37 52 32 36 31
6
30
I
0
1
2
3
4
5
6
7
8
9
0
1
2
4
8
16 32
3
6
12 24
1 48 35
9
18 36 11 22 44 27 54
2 47 33
5
10 20 40 19 38 15 30
3 60 59 57 53 45 29 58 55 49 37
4 13 26 52 43 25 50 39 17 34
7
5 14 28 56 51 41 21 42 23 46 31
6
1
67: N
0
1
2
3
4
5
6
7
8
9
0
66
1
39
2
15 40 23
3
12
1
16 59 41 19 24 54
4
64 13 10
2
17 62 60 28 42 30 20 51 25 44
3
55 47
5
32 65 38 14 22 11 58
4
18 53 63
9
61 27 29 50 43 46
5
31 37 21 57 52
8
26 49 45 36
6
56
7
48 35
6
34 33
I
0
1
2
3
4
5
6
7
8
9
0
1
2
4
8
16 32 64 61 55 43
1 19 38
9
18 36
5
10 20 40 13
2 26 52 37
7
14 28 56 45 23 46
3 25 50 33 66 65 63 59 51 35
3
4
6
12 24 48 29 58 49 31 62 57
5 47 27 54 41 15 30 60 53 39 11
6 22 44 21 42 17 34
1
71: N
0
1
2
3
4
5
6
7
8
9
0
70
6
26 12 28 32
1
18 52
1
34 31 38 39
7
54 24 49 58 16
2
40 27 37 15 44 56 45
8
13 68
3
60 11 30 57 55 29 64 20 22 65
4
46 25 33 48 43 10 21
9
50
2
5
62
5
51 23 14 59 19 42
4
3
6
66 69 17 53 36 67 63 47 61 41
7
35
I
0
1
2
3
4
5
6
7
8
9
0
1
7
49 59 58 51
2
14 27 47
1 45 31
4
28 54 23 19 62
8
56
2 37 46 38 53 16 41
3
21
5
35
3 32 11
6
42 10 70 64 22 12 13
4 20 69 57 44 24 26 40 67 43 17
5 48 52
9
63 15 34 25 33 18 55
6 30 68 50 66 36 39 60 65 29 61
7
1

282
Chapter 4
NUMBER THEORY
73: N
0
1
2
3
4
5
6
7
8
9
0
72
8
6
16
1
14 33 24 12
1
9
55 22 59 41
7
32 21 20 62
2
17 39 63 46 30
2
67 18 49 35
3
15 11 40 61 29 34 28 64 70 65
4
25
4
47 51 71 13 54 31 38 66
5
10 27
3
53 26 56 57 68 43
5
6
23 58 19 45 48 60 69 50 37 52
7
42 44 36
I
0
1
2
3
4
5
6
7
8
9
0
1
5
25 52 41 59
3
15
2
10
1 50 31
9
45
6
30
4
20 27 62
2 18 17 12 60
8
40 54 51 36 34
3 24 47 16
7
35 29 72 68 48 21
4 32 14 70 58 71 63 23 42 64 28
5 67 43 69 53 46 11 55 56 61 13
6 65 33 19 22 37 39 49 26 57 66
7 38 44
1
79: N
0
1
2
3
4
5
6
7
8
9
0
78
4
1
8
62
5
53 12
2
1
66 68
9
34 57 63 16 21
6
32
2
70 54 72 26 13 46 38
3
61 11
3
67 56 20 69 25 37 10 19 36 35
4
74 75 58 49 76 64 30 59 17 28
5
50 22 42 77
7
52 65 33 15 31
6
71 45 60 55 24 18 73 48 29 27
7
41 51 14 44 23 47 40 43 39
I
0
1
2
3
4
5
6
7
8
9
0
1
3
9
27
2
6
18 54
4
12
1 36 29
8
24 72 58 16 48 65 37
2 32 17 51 74 64 34 23 69 49 68
3 46 59 19 57 13 39 38 35 26 78
4 76 70 52 77 73 61 25 75 67 43
5 50 71 55
7
21 63 31 14 42 47
6 62 28
5
15 45 56 10 30 11 33
7 20 60 22 66 40 41 44 53
1
83: N
0
1
2
3
4
5
6
7
8
9
0
82
1
72
2
27 73
8
3
62
1
28 24 74 77
9
17
4
56 63 47
2
29 80 25 60 75 54 78 52 10 12
3
18 38
5
14 57 35 64 20 48 67
4
30 40 81 71 26
7
61 23 76 16
5
55 46 79 59 53 51 11 37 13 34
6
19 66 39 70
6
22 15 45 58 50
7
36 33 65 69 21 44 49 32 68 43
8
31 42 41
I
0
1
2
3
4
5
6
7
8
9
0
1
2
4
8
16 32 64 45
7
14
1 28 56 29 58 33 66 49 15 30 60
2 37 74 65 47 11 22 44
5
10 20
3 40 80 77 71 59 35 70 57 31 62
4 41 82 81 79 75 67 51 19 38 76
5 69 55 27 54 25 50 17 34 68 53
6 23 46
9
18 36 72 61 39 78 73
7 63 43
3
6
12 24 48 13 26 52
8 21 42
1
89: N
0
1
2
3
4
5
6
7
8
9
0
88 16
1
32 70 17 81 48
2
1
86 84 33 23
9
71 64
6
18 35
2
14 82 12 57 49 52 39
3
25 59
3
87 31 80 85 22 63 34 11 51 24
4
30 21 10 29 28 72 73 54 65 74
5
68
7
55 78 19 66 41 36 75 43
6
15 69 47 83
8
5
13 56 38 58
7
79 62 50 20 27 53 67 77 40 42
8
46
4
37 61 26 76 45 60 44
I
0
1
2
3
4
5
6
7
8
9
0
1
3
9
27 81 65 17 51 64 14
1 42 37 22 66 20 60
2
6
18 54
2 73 41 34 13 39 28 84 74 44 43
3 40 31
4
12 36 19 57 82 68 26
4 78 56 79 59 88 86 80 62
8
24
5 72 38 25 75 47 52 67 23 69 29
6 87 83 71 35 16 48 55 76 50 61
7
5
15 45 46 49 58 85 77 53 70
8 32
7
21 63 11 33 10 30
1

Section 4.7
PRIMITIVE ROOTS AND QUADRATIC RESIDUES
283
97: N
0
1
2
3
4
5
6
7
8
9
0
96 34 70 68
1
8
31
6
44
1
35 86 42 25 65 71 40 89 78 81
2
69
5
24 77 76
2
59 18
3
13
3
9
46 74 60 27 32 16 91 19 95
4
7
85 39
4
58 45 15 84 14 62
5
36 63 93 10 52 87 37 55 47 67
6
43 64 80 75 12 26 94 57 61 51
7
66 11 50 28 29 72 53 21 33 30
8
41 88 23 17 73 90 38 83 92 54
9
79 56 49 20 22 82 48
I
0
1
2
3
4
5
6
7
8
9
0
1
5
25 28 43 21
8
40
6
30
1 53 71 64 29 48 46 36 83 27 38
2 93 77 94 82 22 13 65 34 73 74
3 79
7
35 78
2
10 50 56 86 42
4 16 80 12 60
9
45 31 58 96 92
5 72 69 54 76 89 57 91 67 44 26
6 33 68 49 51 61 14 70 59
4
20
7
3
15 75 84 32 63 24 23 18 90
8 62 19 95 87 47 41 11 55 81 17
9 85 37 88 52 66 39
1
2. If m is a positive integer with primitive root r and a is a positive integer relatively
prime to m, then a ≡rindra (mod m).
3. If m is a positive integer with primitive root r, then indr1 = 0 and indrr = 1.
4. If m > 2 is an integer with primitive root r, then indr(−1) = φ(m)
2
.
5. If m is a positive integer with primitive root r, and a and b are integers relatively
prime to m, then
• indr1 ≡0 (mod φ(m));
• indr(ab) ≡indra + indrb (mod φ(m));
• indrak ≡k · indra (mod φ(m)) if k is a positive integer.
6. If m is a positive integer, and if r and s are both primitive roots modulo m, then
indra ≡indsa · indrs (mod φ(m)).
7. If m is a positive integer with primitive root r, and a and b are integers both relatively
prime to m, then the exponential congruence ax ≡b (mod m) has a solution if and
only if gcd(indra, φ(m)) | indrb. Furthermore, if there is a solution to this exponential
congruence, then there are exactly gcd(indra, φ(m)) incongruent solutions.
8. There are a wide variety of algorithms for computing discrete logarithms, including
those known as the baby-step, giant-step algorithm, the Pollard rho algorithm, the Pollig-
Hellman algorithm, and the index-calculus algorithm. (See [MeVoVa96].)
9. The fastest algorithms known for computing discrete logarithms, relative to a ﬁxed
primitive root, of a given prime p are index-calculus algorithms, which have subexponen-
tial computational complexity. In particular, there is an algorithm based on the number
ﬁeld sieve that runs using Lp( 1
3, 1.923) = O(exp((1.923 + o(1))(log p)
1
3 (log log p)
2
3 )) bit
operations. (See [MeVoVa96].)
10. Many cryptographic methods rely on the intractability of ﬁnding discrete logarithms
of integers relative to a ﬁxed primitive root r of a ﬁxed prime p.
Examples:
1. To solve 3x30 ≡4 (mod 37) take indices to the base 2 (2 is the smallest primitive root
of 37) to obtain ind2(3x30) ≡ind24 = 2 (mod 36). Since ind2(3x30) ≡ind23+30·ind2x =
26 + 30 · ind2x (mod 36), it follows that 30 · ind2x ≡12 (mod 36). The solutions to this
congruence are those x such that ind2(x) ≡4, 10, 16, 22, 28, 34 (mod 36). From the table
of indices (Table 1), the solutions are those x with x ≡16, 25, 9, 21, 12, 28 (mod 37).
2. To solve 7x ≡6 (mod 17) take indices to the base 3 (3 is the smallest primitive
root of 17) to obtain
ind3(7x) ≡ind36 = 15 (mod 16). Since ind3(7x) ≡x · ind37 ≡

284
Chapter 4
NUMBER THEORY
11x (mod 16), it follows that 11x ≡15 (mod 16). Since all the steps in this computation
are reversible, it follows that the solutions of the original congruence are the solutions of
this linear congruence, namely those x with x ≡13 (mod 16).
4.7.3
QUADRATIC RESIDUES
Deﬁnitions:
If m and k are positive integers and a is an integer relatively prime to m, then a is a kth
power residue of m if the congruence xk ≡a (mod m) has a solution.
If a and m are relatively prime integers and m is positive, then a is a quadratic residue
of m if the congruence x2 ≡a (mod m) has a solution. If x2 ≡a (mod m) has no solution,
then a is a quadratic nonresidue of m.
If p is an odd prime and p does not divide a, then the Legendre symbol
  a
p

is 1 if a
is a quadratic residue of p and −1 if a is a quadratic nonresidue of p. This symbol is
named after the French mathematician Adrien-Marie Legendre (1752–1833).
If n is an odd positive integer with prime-power factorization n = pt1
1 pt2
2 . . . ptm
m and a is
an integer relatively prime to n, then the Jacobi symbol
  a
n

is deﬁned by
  a
n

=
m
Q
i=1
  a
pi
ti,
where the symbols on the right-hand side of the equality are Legendre symbols. This
symbol is named after the German mathematician Carl Gustav Jacob Jacobi (1804–
1851).
Let a be a positive integer that is not a perfect square and such that a ≡0 or 1 (mod 4).
The Kronecker symbol (named after the German mathematician Leopold Kronecker
(1823–1891)), which is a generalization of the Legendre symbol, is deﬁned as
•
  a
2

=
(
1
if a ≡1 (mod 8)
−1
if a ≡5 (mod 8)
•
  a
p

= the Legendre symbol
  a
p

if p is an odd prime such that p does not divide a
•
  a
n

=
rQ
j=1
  a
pj
tj if gcd(a, n) = 1 and n =
rQ
j=1
pjtj is the prime factorization of n.
Facts:
1. If p is an odd prime, then there are an equal number of quadratic residues modulo p
and quadratic nonresidues modulo p among the integers 1, 2, . . . , p −1. In particular,
there are p−1
2
integers of each type in this set.
2. Euler’s criterion: If p is an odd prime and a is a positive integer not divisible by p,
then
  a
p

≡a(p−1)/2 (mod p).
3. If p is an odd prime, and a and b are integers not divisible by p with a ≡b (mod p),
then
  a
p

=
  b
p

.
4. If p is an odd prime, and a and b are integers not divisible by p, then
  a
p
  b
p

=
  ab
p

.
5. If p is an odd prime, and a and b are integers not divisible by p, then
  a2
p

= 1.

Section 4.7
PRIMITIVE ROOTS AND QUADRATIC RESIDUES
285
6. If p is an odd prime, then
  −1
p

=
(
1
if p ≡1 (mod 4)
−1
if p ≡−1 (mod 4).
7. If p is an odd prime, then −1 is a quadratic residue of p if p ≡1 (mod 4) and a
quadratic nonresidue of p if p ≡−1 (mod 4). (This is a direct consequence of Fact 6.)
8. Gauss’ lemma: If p is an odd prime, a is an integer with gcd(a, p) = 1, and s is the
number of least positive residues of a, 2a, . . . , p−1
2 a greater than p
2, then
  a
p

= (−1)s.
9. If p is an odd prime, then
  2
p

= (−1)(p2−1)/8.
10. The integer 2 is a quadratic residue of all primes p with p ≡±1 (mod 8) and a
quadratic nonresidue of all primes p ≡±3 (mod 8). (This follows from Fact 9.)
11. Law of quadratic reciprocity: If p and q are odd primes, then
  p
q
  q
p

= (−1)
p−1
2
· q−1
2 .
This law was ﬁrst proved by Carl Friedrich Gauss (1777–1855).
12. Many diﬀerent proofs of the law of quadratic reciprocity have been discovered. By
one count, there are more than 150 diﬀerent proofs.
Gauss published eight diﬀerent
proofs himself.
13. The law of quadratic reciprocity implies that if p and q are odd primes, then
  p
q

=
  q
p

if either p ≡1 (mod 4) or q ≡1 (mod 4), and
  p
q

= −
  q
p

if p ≡q ≡3 (mod 4).
14. If m is an odd positive integer, and a and b are integers relatively prime to m with
a ≡b (mod m), then
  a
m

=
  b
m

.
15. If m is an odd positive integer, and a and b are integers relatively prime to m, then
  ab
m

=
  a
m
  b
m

.
16. If m is an odd positive integer and a is an integer relatively prime to m, then
  a2
m

= 1.
17. If m and n are relatively prime odd positive integers, and a is an integer relatively
prime to m and n, then
  a
mn

=
  a
m
  a
n

.
18. If m is an odd positive integer, then the value of the Jacobi symbol
  a
m

does not
determine whether a is a perfect square modulo m.
19. If m is an odd positive integer, then
  −1
m

= (−1)
m−1
2 .
20. If m is an odd positive integer, then
  2
m

= (−1)
m2−1
8
.
21. Reciprocity law for Jacobi symbols:
If m and n are relatively prime odd positive
integers, then
  m
n
  n
m

= (−1)
m−1
2
n−1
2 .
22. The number of integers in a reduced set of residues modulo n with
  k
n

= 1 equals
the number with
  k
n

= −1.
23. The Legendre symbol
  a
p

, where p is prime and 0 ≤a < p, can be evaluated using
O((log2 p)2) bit operations.
24. The Jacobi symbol
  a
n

, where n is a positive integer and 0 ≤a < n, can be
evaluated using O((log2 n)2) bit operations.
25. Let p be an odd prime. Even though half the integers x with 1 ≤x < p are quadratic
nonresidues of p, there is no known polynomial-time deterministic algorithm for ﬁnding
such an integer. However, picking integers at random produces a probabilistic algorithm
that has 2 as the expected number of iterations done before a nonresidue is found.

286
Chapter 4
NUMBER THEORY
26. Let m be a positive integer with a primitive root. If k is a positive integer and a
is an integer relatively prime to m, then a is a kth power residue of m if and only if
aφ(m)/d ≡1 (mod m) where d = gcd(k, φ(m)). Moreover, if a is a kth power residue
of m, then there are exactly d incongruent solutions modulo m of the congruence xk ≡
a (mod m).
27. If p is a prime, k is a positive integer, and a is an integer with gcd(a, p) = 1, then a
is a kth power residue of p if and only if a(p−1)/d ≡1 (mod p), where d = gcd(k, p −1).
28. The kth roots of a kth power residue modulo p, where p is a prime, can be computed
using a primitive root and indices to this primitive root. This is only practical for small
primes p. (See §4.7.1.)
Examples:
1. The integers 1, 3, 4, 5, and 9 are quadratic residues of 11; the integers 2, 6, 7, 8, and
10 are quadratic nonresidues of 11. Hence
  1
11

=
  3
11

=
  4
11

=
  5
11

=
  9
11

= 1 and
  2
11

=
  6
11

=
  7
11

=
  8
11

=
  10
11

= −1.
2. To determine whether 11 is a quadratic residue of 19, note that using the law of
quadratic reciprocity (Fact 11) and Facts 3, 4, and 10 it follows that
  11
19

= −
  19
11

=
−
  8
11

= −
  2
11
3 = −(−1)3 = 1.
3. To evaluate the Jacobi symbol
  2
45

, note that
  2
45

=
 2
32·5

=
  2
3
2·
  2
5

= (−1)2(−1)
= −1.
4. The Jacobi symbol
  5
21

= 1, but 5 is not a quadratic residue of 21.
5. The integer 6 is a ﬁfth power residue of 101 since 6(101−1)/5 = 620 ≡1 (mod 101).
6. From Example 5 it follows that 6 is a ﬁfth power residue of 101.
The solutions
of the congruence x5 ≡6 (mod 101), the ﬁfth roots of 6, can be found by taking
indices to the primitive root 2 modulo 101.
Since ind26 = 70, this gives ind2x5 =
5 · ind2x ≡70 (mod 100).
The solutions of this congruence are the integers x with
ind2x ≡14 (mod 20).
This implies that the ﬁfth roots of 6 are the integers with
ind2x = 14, 34, 54, 74, and 94.
These are the integers x with x ≡22, 70, 85, 96,
and 30 (mod 101).
7. The integer 5 is not a sixth power residue of 17 since 5
16
gcd(6,16) = 58 ≡−1 (mod 17).
4.7.4
MODULAR SQUARE ROOTS
Deﬁnition:
If m is a positive integer and a is an integer, then r is a square root of a modulo m
if r2 ≡a (mod m).
Facts:
1. If p is a prime of the form 4n + 3 and a is a perfect square modulo p, then the two
square roots of a modulo p are ±a(p+1)/4.
2. If p is a prime of the form 8n + 5 and a is a perfect square modulo p, then the
two square roots of a modulo p are x ≡±a(p+3)/8(mod p) if a(p−1)/4 ≡1 (mod p) and
x ≡±2(p−1)/4a(p+3)/8(mod p) if a(p−1)/4 ≡−1 (mod p).
3. If n is a positive integer that is the product of two distinct primes p and q, and a is a
perfect square modulo n, then there are four distinct square roots of a modulo n. These

Section 4.8
DIOPHANTINE EQUATIONS
287
square roots can be found by ﬁnding the two square roots of a modulo p and the two
square roots of a modulo q and then using the Chinese remainder theorem to ﬁnd the
four square roots of a modulo n.
4. A square root of an integer a that is a square modulo p, where p is an odd prime,
can be found by an algorithm that uses an average of O((log2 p)3) bit operations. (See
[MeVoVa96].)
5. If n is an odd integer with r distinct prime factors, a is a perfect square modulo n,
and gcd(a, n) = 1, then a has exactly 2r incongruent square roots modulo n.
Examples:
1. Using Legendre symbols it can be shown that 11 is a perfect square modulo 19. Using
Fact 1 it follows that the square roots of 11 modulo 19 are given by x ≡±11(19+1)/4 =
±115 ≡±7 (mod 19).
2. There are four incongruent square roots of 860 modulo 11021 = 103 · 107. To ﬁnd
these solutions, ﬁrst note that x2 ≡860 = 36 (mod 103) so that x ≡±6 (mod 103) and
x2 ≡860 = 4 (mod 107) so that x ≡±2 (mod 107). The Chinese remainder theorem
can be used to ﬁnd these square roots. They are x ≡−212, −109, 109, 212 (mod 11021).
3. The square roots of 121 modulo 315 are 11, 74, 101, 151, 164, 214, 241, and 304. As
predicted by Fact 5, 121 = 112 has 23 = 8 square roots modulo 315 = 32 · 5 · 7.
4.8
DIOPHANTINE EQUATIONS
An important area of number theory is devoted to ﬁnding solutions of equations where
the solutions are restricted to belong to the set of integers, or some other speciﬁed
set, such as the set of rational numbers. An equation with the added proviso that the
solutions must be integers (or must belong to some other speciﬁed countable set, such
as the set of rational numbers) is called a diophantine equation. This name comes from
the ancient Greek mathematician Diophantus (ca. 250 A.D.), who wrote extensively on
such equations.
Diophantine equations have both practical and theoretical importance. Their practi-
cal importance arises when variables in an equation represent quantities of objects, for
example. Fermat’s last theorem, which states that there are no nontrivial solutions in
integers n > 2, x, y, and z to the diophantine equation xn + yn = zn has long inter-
ested mathematicians and non-mathematicians alike. This theorem was proved only in
the mid-1990s, even though many brilliant scholars sought a proof during the last three
centuries.
More information about diophantine equations can be found in [Di71], [Gu94], and
[Mo69].
4.8.1
LINEAR DIOPHANTINE EQUATIONS
Deﬁnition:
A linear diophantine equation is an equation of the form a1x1+a2x2+· · ·+anxn = c,
where c, a1, . . . , an are integers and where integer solutions are sought for the unknowns
x1, x2, . . . , xn.

288
Chapter 4
NUMBER THEORY
Facts:
1. Let a and b be integers with gcd(a, b) = d. The linear diophantine equation ax+by = c
has no solutions if d̸ | c.
If d|c, then there are inﬁnitely many solutions in integers.
Moreover, if x = x0, y = y0 is a particular solution, then all solutions are given by
x = x0 + b
dn, y = y0 −a
dn, where n is an integer.
2. A linear diophantine equation a1x1 + a2x2 + · · · + anxn = c has solutions in integers
if and only if gcd(a1, a2, . . . , an)|c. In that case, there are inﬁnitely many solutions.
3. A solution (x0, y0) of the linear diophantine equation ax + by = c where gcd(a, b)|c
can be found by ﬁrst expressing gcd(a, b) as a linear combination of a and b, and then
multiplying by c/ gcd(a, b). (See §4.2.1.)
4. A linear diophantine equation a1x1 + a2x2 + · · · + anxn = c in n variables can be
solved by a reduction method. To ﬁnd a particular solution, ﬁrst let b = gcd(a2, . . . , an)
and let (x1, y) be a solution of the diophantine equation a1x1 + by = c. Iterate this
procedure on the diophantine equation in n −1 variables, a2x2 + a3x3 + · · · + anxn = y
until an equation in two variables is obtained.
5. The solution to a system of r linear diophantine equations in n variables is obtained
by using Gaussian elimination (§6.4.2) to reduce to a single diophantine equation in two
or more variables.
6. If a and b are relatively prime positive integers and n is a positive integer, then the
diophantine equation ax+by = n has a nonnegative integer solution if n ≥(a−1)(b−1).
7. If a and b are relatively prime positive integers, then there are exactly (a−1)(b−1)/2
nonnegative integers n less than ab −a −b such that the equation ax + by = n has a
nonnegative solution.
8. If a and b are relatively prime positive integers, then there are no nonnegative solu-
tions of ax + by = ab −a −b.
Examples:
1. To solve the linear diophantine equation 17x + 13y = 100, express gcd(17, 13) = 1 as
a linear combination of 17 and 13. Using the steps of the Euclidean algorithm, it follows
that 4 · 13 −3 · 17 = 1. Multiplying by 100 yields 100 = 400 · 13 −300 · 17. Since a
particular solution is x = −300, y = 400, all solutions are then given by x = −300 + 13n,
y = 400 −17n, where n ranges over the set of integers.
2. A traveller has exactly $510 in travelers checks where each check is either a $20 or
a $50 check. How many checks of each denomination can there be?
The answer to this question is given by the set of solutions in nonnegative integers to
the linear diophantine equation 20x + 50y = 510. There are inﬁnitely many solutions in
integers, which can be shown to be given by x = −102 + 5n, y = 51 −2n. Since both
x and y must be nonnegative, it follows that n = 21, 22, 23, 24, or 25. Therefore there
could be three $20 checks and nine $50 checks, eight $20 checks and seven $50 checks,
thirteen $20 checks and ﬁve $50 checks, eighteen $20 checks and three $50 checks, or
twenty-three $20 checks and one $50 check.
3. The linear diophantine equation 12x1 + 21x2 + 9x3 + 15x4 = 9 has inﬁnitely many
solutions since gcd(12, 21, 9, 15) = 3, which divides 9.
To ﬁnd a particular solution,
ﬁrst divide both sides of the equation by 3 to get 4x1 + 7x2 + 3x3 + 5x4 = 3. Now
1 = gcd(7, 3, 5), so solve 4x1 + 1y = 3, as in Example 1, to get x1 = 1, y = −1. Next
we solve 7x2 + 3x3 + 5x4 = −1. Since 1 = gcd(3, 5), solve 7x2 + 1z = −1 to get x2 = 1,
z = −8. Finally, solve 3x3+5x4 = −8 to get x3 = −1, x4 = −1. This gives the particular
solution x = (1, 1, −1, −1).

Section 4.8
DIOPHANTINE EQUATIONS
289
4. To solve the following system of linear diophantine equations in integers
x + y + z + w = 100
x + 2y + 3z + 4w = 300
x + 4y + 9z + 16w = 1000,
ﬁrst reduce the system by elimination to
x + y + z + w = 100
y + 2z + 3w = 200
2z + 6w = 300.
The solution to the last equation is z = 150 + 3t, w = −t, where t is an integer. Back-
substitution gives
y = 200 −2(150 + 3t) −3(−t) = −100 −3t
x = 100 −(−100 −3t) −(150 + 3t) −(−t) = 50 + t.
4.8.2
PYTHAGOREAN TRIPLES
Deﬁnitions:
A Pythagorean triple is a solution (x, y, z) of the equation x2 + y2 = z2, where x, y,
and z are positive integers.
A Pythagorean triple is primitive if gcd(x, y, z) = 1.
Facts:
1. Pythagorean triples represent the lengths of sides of right triangles.
2. All primitive Pythagorean triples are given by
x = 2mn, y = m2 −n2, z = m2 + n2
where m and n are relatively prime positive integers of opposite parity with m > n.
3. All Pythagorean triples can be found by taking
x = 2mnt, y = (m2 −n2)t, z = (m2 + n2)t
where t is a positive integer and m and n are as in Fact 2.
4. Given a Pythagorean triple (x, y, z) with y odd, then m and n from Fact 2 can be
found by taking m =
q
z+y
2
and n =
q
z−y
2 .
5. The following table lists all Pythagorean triples with z ≤100.
m
n
x = 2mn
y = m2 −n2
z = m2 + n2
2
1
4
3
5
3
1
6
8
10
3
2
12
5
13
4
1
8
15
17
4
2
16
12
20

290
Chapter 4
NUMBER THEORY
m
n
x = 2mn
y = m2 −n2
z = m2 + n2
4
3
24
7
25
5
1
10
24
26
5
2
20
21
29
5
3
30
16
34
5
4
40
9
41
6
1
12
35
37
6
2
24
32
40
6
3
36
27
45
6
4
48
20
52
6
5
60
11
61
7
1
14
48
50
7
2
28
45
53
7
3
42
40
58
7
4
56
33
65
7
5
70
24
74
7
6
84
13
85
8
1
16
63
65
8
2
32
60
68
8
3
48
55
73
8
4
64
48
80
8
5
80
39
89
8
6
96
28
100
9
1
18
80
82
9
2
36
77
85
9
3
54
72
90
9
4
72
65
97
6. The solutions of the diophantine equation x2 + y2 = 2z2 can be obtained by trans-
forming this equation into
  x+y
2
2 +
  x−y
2
2 = z2, which shows that ( x+y
2 , x−y
2 , z) is a
Pythagorean triple. All solutions are given by x = (m2−n2+2mn)t, y = (m2−n2−2mn)t,
z = (m2 + n2)t where m, n, and t are integers.
7. The solutions of the diophantine equation x2+2y2 = z2 are given by x = (m2−2n2)t,
y = 2mnt, z = m2 + 2n2 where m, n, and t are positive integers.
8. The solutions of the diophantine equation x2 + y2 + z2 = w2 where y and z are even
are given by x = m2+n2−r2
r
, y = 2m, z = 2n, w = m2+n2+r2
r
, where m and n are positive
integers and r runs through the divisors of m2 + n2 less than (m2 + n2)1/2.
9. The solutions of the diophantine equation x2 + y2 = z2 + w2, with x > z, are given
by x = ms+nr
2
, y = ns−mr
2
, z = ms−nr
2
, w = ns+mr
2
, where if m and n are both odd,
then r and s are either both odd or both even.

Section 4.8
DIOPHANTINE EQUATIONS
291
4.8.3
FERMAT’S LAST THEOREM
Deﬁnitions:
The Fermat equation is the diophantine equation xn + yn = zn, where x, y, z are
integers and n is a positive integer greater than 2.
A nontrivial solution to the Fermat equation xn+yn = zn is a solution in integers x, y,
and z in which none of x, y, and z are zero.
Let p be an odd prime and let K = Q(ω) be the degree-p cyclotomic extension of the
rational numbers (§5.6.2). If p does not divide the class number of K (see [Co93]), then p
is said to be regular. Otherwise p is irregular.
Facts:
1. Fermat’s last theorem:
The statement that the diophantine equation xn + yn = zn
has no nontrivial solutions in the positive integers for n ≥3, is called Fermat’s last
theorem. The statement was made more than 300 years ago by Pierre de Fermat (1601–
1665) and resisted proof until recently.
2. Fermat wrote in the margin of his copy of the works of Diophantus, next to the
discussion of the equation x2 +y2 = z2, the following: “However, it is impossible to write
a cube as the sum of two cubes, a fourth power as the sum of two fourth powers and
in general any power the sum of two similar powers. For this I have discovered a truly
wonderful proof, but the margin is too small to contain it.” In spite of this quotation, no
proof was found of this statement until 1994, even though many mathematicians actively
worked on ﬁnding such a proof. Most mathematicians would ﬁnd it shocking if Fermat
actually had found a proof.
3. Fermat’s last theorem was ﬁnally proved in 1994 by Andrew Wiles [Wi95]. Wiles
collected the Wolfskehl Prize, worth approximately $50,000 in 1997 for this proof.
4. That there are no nontrivial solutions of the Fermat equation for n = 4 was demon-
strated by Fermat with an elementary proof using the method of inﬁnite descent. This
method proceeds by showing that for every solution in positive integers, there is a solu-
tion such that the values of each of the integers x, y, and z is smaller, contradicting the
well-ordering property of the set of integers.
5. The method of inﬁnite descent invented by Fermat can be used to show that the more
general diophantine equation x4 + y4 = z2 has no nontrivial solutions in integers x, y,
and z.
6. The diophantine equation x4 −y4 = z2 has no nontrivial solutions, as can be shown
using the method of inﬁnite descent.
7. The sum of two cubes may equal the sum of two other cubes. That is, there are
nontrivial solutions of the diophantine equation x3 +y3 = z3 +w3. The smallest solution
is x = 1, y = 12, z = 9, w = 10.
8. The sum of three cubes may also be a cube. In fact, the solutions of x3+y3+z3 = w3
are given by x = 3a2+5b(a−b), y = 4a(a−b)+6b2, z = 5a(a−b)−3b2, w = 6a2−4b(a+b)
where a and b are integers.
9. Euler conjectured that there were four fourth powers of positive integers whose sum
is also the fourth power of an integer. In other words, he conjectured that there are
nontrivial solutions to the diophantine equation v4 + w4 + x4 + y4 = z4. The ﬁrst such
example was found in 1911 when it was discovered (by R. Norrie) that 304 + 1204 +
2724 + 3154 = 3534.

292
Chapter 4
NUMBER THEORY
10. Euler also conjectured that the sum of the fourth powers of three positive integers
can never be the fourth power of an integer and that the sum of ﬁfth powers of four
positive integers can never be the ﬁfth power of an integer, and so on. In other words,
he conjectured that there were no nontrivial solutions to the diophantine equations w4 +
x4 + y4 = z4, v5 + w5 + x5 + y5 = z5, and so on. He was mistaken. The smallest
counterexamples known are 95,8004 + 217,5194 + 414,5604 = 422,4814 and 275 + 845 +
1105 + 1335 = 1445.
11. If n = mp for some integer m and p is prime, then the Fermat equation can be
rewritten as (xm)p + (ym)p = (zm)p. Since the only positive integers greater than 2
without an odd prime factor are powers of 2 and x4 + y4 = z4 has no nontrivial solutions
in integers, Fermat’s last theorem can be demonstrated by showing that xp + yp = zp
has no nontrivial solutions in integers x, y, and z when p is an odd prime.
12. An odd prime p is regular if and only if it does not divide the numerator of any of
the numbers B2, B4, . . . , Bp−3, where Bk is the kth Bernoulli number. (See §3.1.4.)
13. There is a relatively simple proof of Fermat’s last theorem for exponents that are
regular primes.
14. The smallest irregular primes are 37, 59, 67, 101, 103, 149, and 157.
15. Wiles’ proof of Fermat’s last theorem is based on the theory of elliptic curves. The
proof is based on relating to integers a, b, c, and n that supposedly satisfy the Fermat
equation an + bn = cn the elliptic curve y2 = x(x + an)(x −bn) (called the associated
Frey curve) and deriving a contradiction using sophisticated results from the theory of
elliptic curves.
For more details, see Wiles’ original proof [Wi95], the popular account [Si97], and the
following two sites:
• http://www.scienceandreason.net/flt/flt01.htm
• http://www.pbs.org/wgbh/nova/proof
4.8.4
PELL’S, BACHET’S, AND CATALAN’S EQUATIONS
Deﬁnitions:
Pell’s equation is a diophantine equation of the form x2 −dy2 = 1, where d is a square-
free positive integer. This diophantine equation is named after John Pell (1611–1685).
Bachet’s equation is a diophantine equation of the form y2 = x3 +k. This diophantine
equation is named after Claude Gaspar Bachet (1587–1638).
Catalan’s equation is the diophantine equation xm−yn = 1, where a solution is sought
with integers x > 0, y > 0, m > 1, and n > 1. This diophantine equation is named after
Eug`ene Charles Catalan (1814–1894).
Facts:
1. If x, y is a solution to the diophantine equation x2 −dy2 = n with d square free and
n2 < d, then the rational number x
y is a convergent of the simple continued fraction for
√
d. (See §4.9.2.)
2. An equation of the form ax′2 + bx′ + c = y′2 can be transformed by means of the
relations x = 2ax′ + b and y = 2y′ into an equation of the form x2 −dy2 = n, where
n = b2 −4ac and d = a.

Section 4.8
DIOPHANTINE EQUATIONS
293
3. It is ironic that John Pell apparently had little to do with ﬁnding the solutions to
the diophantine equation x2 −dy2 = 1. Euler gave this equation its name following a
mistaken reference. Fermat conjectured an inﬁnite number of solutions to this equation
in 1657; this was eventually proved by Lagrange in 1768.
4. Let x, y be the least positive solution to x2 −dy2 = 1, with d square free. Then every
positive solution is given by
xk + yk
√
d = (x + y
√
d)k,
where k ranges over the positive integers.
5. The following table gives the smallest positive solutions to Pell’s equation x2−dy2 = 1
with d a square-free positive integer less than 100.
d
x
y
d
x
y
2
3
2
51
50
7
3
2
1
53
66,249
9,100
5
9
4
55
89
12
6
5
2
57
151
20
7
8
3
58
19,603
2,574
10
19
6
59
530
69
11
10
3
61
1,766,319,049
226,153,980
13
649
180
62
63
8
14
15
4
65
129
16
15
4
1
66
65
8
17
33
8
67
48,842
5,967
19
170
39
69
7,775
936
21
55
12
70
251
30
22
197
42
71
3,480
413
23
24
5
73
2,281,249
267,000
26
51
10
74
3,699
430
29
9,801
1,820
77
351
40
30
11
2
78
53
6
31
1,520
273
79
80
9
33
23
4
82
163
18
34
35
6
83
82
9
35
6
1
85
285,769
30,996
37
73
12
86
10,405
1,122
38
37
6
87
28
3
39
25
4
89
500,001
53,000
41
2,049
320
91
1,574
165
42
13
2
93
12,151
1,260
43
3,482
531
94
2,143,295
221,064
46
24,335
3,588
95
39
4
47
48
7
97
62,809,633
6,377,352
6. If k = 0, then the formulae x = t2, y = t3 give an inﬁnite number of solutions to the
Bachet equation y2 = x3 + k.

294
Chapter 4
NUMBER THEORY
7. There are no solutions to Bachet’s equation for the following values of k: −144, −105,
−78, −69, −42, −34, −33, −31, −24, −14, −5, 7, 11, 23, 34, 45, 58, 70.
8. The following table lists solutions to Bachet’s equation for various values of k.
k
x
0
t2 (t any integer)
1
0, −1, 2
17
−1, −2, 2, 4, 8, 43, 52, 5334
−2
3
−4
2, 5
−7
2, 32
−15
1
9. If k < 0, k is square free, k ≡2 or 3 (mod 4), and the class number of the ﬁeld
Q(
√
−k) is not a multiple of 3, then the only solution of the Bachet equation y2 = x3 +k
for x is given by whichever of −(4k ± 1)/3 is an integer. The ﬁrst few values of such k
are 1, 2, 5, 6, 10, 13, 14, 17, 21, and 22.
10. Solutions to the Catalan equation give consecutive integers that are powers of inte-
gers.
11. The Catalan equation has the solution x = 3, y = 2, m = 2, n = 3, so 8 = 23 and
9 = 32 are consecutive powers of integers. The Catalan conjecture is that this is the only
solution.
12. Levi ben Gerson showed in the 14th century that 8 and 9 are the only consecutive
powers of 2 and 3, so that the only solution in positive integers of 3m −2n = ±1 is m = 2
and n = 3.
13. Euler proved that the only solution in positive integers of x3 −y2 = ±1 is x = 2
and y = 3.
14. Lebesgue showed in 1850 that xm −y2 = 1 has no solutions in positive integers
when m is an integer greater than 3.
15. The diophantine equations x3 −yn = 1 and xm −y3 = 1 with m > 2 were shown to
have no solutions in positive integers in 1921, and in 1964 it was shown that x2 −yn = 1
has no solutions in positive integers.
16. R. Tijdeman showed in 1976 that there are only ﬁnitely many solutions in integers
to the Catalan equation xm −yn = 1 by showing that there is a computable constant C
such that for every solution, xm < C and yn < C. However, the enormous size of the
constant C makes it infeasible to establish the Catalan conjecture using this result.
17. In 2002 Catalan’s conjecture was proved by Preda Mih˘ailescu using the theories
of cyclotomic ﬁelds and Galois modules. Consequently, the statement that Catalan’s
equation has exactly one solution (given in Fact 11) is now called Mih˘ailescu’s theorem.
Examples:
1. To solve the diophantine equation x2 −13y2 = 1, note that the simple continued frac-
tion for
√
13 is [3; 1, 1, 1, 1, 6], with convergents 3, 4, 7
2, 11
3 , 18
5 , 119
33 , 137
38 , 256
71 , 393
109, 649
180, . . . .
The smallest positive solution to the equation is x = 649, y = 180. A second solution is
given by (649 + 180
√
13)2 = 842,401 + 233,640
√
13, that is, x = 842,401, y = 233,640.
2. Congruence considerations can be used to show that there are no solutions of Bachet’s
equation for k = 7. Modulo 8, every square is congruent to 0, 1, or 4; therefore if x is

Section 4.8
DIOPHANTINE EQUATIONS
295
even, then y2 ≡7 (mod 8), a contradiction. Likewise if x ≡3 (mod 4), then y2 ≡2
(mod 8), also impossible. So assume that x ≡1 (mod 4). Add one to both sides and
factor to get y2 + 1 = x3 + 8 = (x + 2)(x2 −2x + 4). Now x2 −2x + 4 ≡3 (mod 4), so
it must have a prime divisor p ≡3 (mod 4). Then y2 ≡−1 (mod p), which implies that
−1 is a quadratic residue modulo p. (See §4.7.3.) But p ≡3 (mod 4), so −1 cannot be a
quadratic residue modulo p. Therefore, there are no solutions when k = 7.
4.8.5
SUMS OF SQUARES AND WARING’S PROBLEM
Deﬁnitions:
If k is a positive integer, then g(k) is the smallest positive integer such that every positive
integer can be written as a sum of g(k) kth powers.
If k is a positive integer, then G(k) is the smallest positive integer such that every
suﬃciently large positive integer can be written as a sum of G(k) kth powers.
The determination of g(k) is called Waring’s problem. (Edward Waring, 1741–1793)
Facts:
1. A positive integer n is the sum of two squares if and only if each prime factor of n of
the form 4k + 3 appears to an even power in the prime factorization of n.
2. If m = a2 + b2 and n = c2 + d2, then the number mn can be expressed as the sum of
two squares as follows: mn = (ac + bd)2 + (ad −bc)2.
3. If n is representable as the sum of two squares, then it is representable in 4(d1 −d2)
ways (where the order of the squares and their signs matter), where d1 is the number of
divisors of n of the form 4k + 1 and d2 is the number of divisors of n of the form 4k + 3.
4. An integer n is the sum of three squares if and only if n is not of the form 4m(8k +7),
where m is a nonnegative integer.
5. The positive integers less than 100 that are not the sum of three squares are 7, 15, 23,
28, 31, 39, 47, 55, 60, 63, 71, 79, 87, 92, and 95.
6. Lagrange’s four-square theorem: Every positive integer is the sum of 4 squares, some
of which may be zero. (Joseph Lagrange, 1736–1813)
7. A useful lemma due to Lagrange is the following. If m = a2 + b2 + c2 + d2 and
n = e2 + f 2 + g2 + h2, then mn can be expressed as the sum of four squares as follows:
mn = (ae+bf +cg+dh)2+(af −be+ch−dg)2+(ag−ce+df −bh)2+(ah−de+bg−cf)2.
8. The number of ways n can be written as the sum of four squares is 8(s−s4), where s
is the sum of the divisors of n and s4 is the sum of the divisors of n that are divisible
by 4.
9. It is known that g(k) always exists.
10. For 6 ≤k ≤471,600,000 the following formula holds except possibly for a ﬁnite
number of positive integers k: g(k) =

( 3
2)k
+ 2k −2 where ⌊x⌋represents the ﬂoor
(greatest integer) function.
11. The exact value of G(k) is known only for two values of k, G(2) = 4 and G(4) = 16.
12. From Facts 6 and 11, it follows that G(2) = g(2) = 4.
13. If k is an integer with k ≥2, then G(k) ≤g(k).
14. If k is an integer with k ≥2, then G(k) ≥k + 1.

296
Chapter 4
NUMBER THEORY
15. Hardy and Littlewood showed that G(k) ≤(k −2)2k−1 + 5 and conjectured that
G(k) < 2k + 1 when k is not a power of 2 and G(k) < 4k when k is a power of 2.
16. The best upper bound known for G(k) is G(k) < ck log k for some constant c.
17. The known values and established estimates for g(k) and G(k) for 2 ≤k ≤8 are
given in the following table.
g(2) = 4
G(2) = 4
g(3) = 9
4 ≤G(3) ≤7
g(4) = 19
G(4) = 16
g(5) = 37
6 ≤G(5) ≤17
g(6) = 73
9 ≤G(6) ≤24
143 ≤g(7) ≤3,806
8 ≤G(7) ≤33
279 ≤g(8) ≤36,119
32 ≤G(8) ≤42
18. There are many related diophantine equations concerning sums and diﬀerences of
powers. For instance x = 1, y = 12, z = 9, and w = 10 is the smallest solution to
x3 + y3 = z3 + w3.
4.9
DIOPHANTINE APPROXIMATION
Diophantine approximation is the study of how closely a number θ can be approximated
by numbers of some particular kind. Usually θ is an irrational (real) number, and the
goal is to approximate θ using rational numbers p
q . Standard references are [Ca57] and
[Sc80].
4.9.1
CONTINUED FRACTIONS
Deﬁnitions:
A continued fraction is a (ﬁnite or inﬁnite) expression of the form
a0 +
1
a1 +
1
a2 +
1
a3 +
1
...
The terms a0, a1, . . . are called the partial quotients. If the partial quotients are all
integers, and ai ≥1 for i ≥1, then the continued fraction is said to be simple. For
convenience, the above expression is usually abbreviated as [a0, a1, a2, a3, . . .].
A continued fraction that has an expansion with a block that repeats after some point
is called ultimately periodic. The ultimately periodic continued fraction expansion
[a0, a1, . . . ,
aN, aN+1, . . . , aN+k, aN+1, . . . , aN+k, aN+1, . . .]
is
often
abbreviated
as
[a0, a1, . . . , aN, aN+1, . . . , aN+k]. The terms a0, a1, . . . , aN are called the pre-period and
the terms aN+1, aN+2, . . . , aN+k are called the period.

Section 4.9
DIOPHANTINE APPROXIMATION
297
Facts:
1. Every irrational number has a unique expansion as a simple continued fraction.
2. Every rational number has exactly two simple continued fraction expansions, one
with an odd number of terms and one with an even number of terms. Of these, the one
with the larger number of terms ends with 1.
3. The simple continued fraction for a real number r is ﬁnite if and only if r is rational.
4. The simple continued fraction for a real number r is inﬁnite and ultimately periodic
if and only if r is a quadratic irrational.
5. The simple continued fraction for
√
d, where d a positive integer that is not a square,
is as follows:
√
d = [a0, a1, a2, . . . , an, 2a0 ], where the sequence (a1, a2, . . . , an) is a palin-
drome.
6. The following table illustrates the three types of continued fractions.
type
kind of number
example
ﬁnite
rational
355
113 = [3, 7, 16]
ultimately periodic
quadratic irrational
√
2 = [1, 2, 2, 2, . . .]
inﬁnite, but not ulti-
neither rational nor
π = [3, 7, 15, 1, 292, . . .]
mately periodic
quadratic irrational
7. The continued fraction for a real number can be computed by Algorithm 1.
Algorithm 1:
The continued fraction algorithm.
procedure CFA(x: real number)
i := 0
x0 := x
a0 := ⌊x0⌋
output(a0)
while (xi ̸= ai)
xi+1 :=
1
xi−ai
i := i + 1
ai := ⌊xi⌋
output(ai)
{Return the ﬁnite or inﬁnite sequence (a0, a1, . . .)}
8. The following table gives continued fractions for
√
d, with d square free and d ≤100.
d
√
d
d
√
d
2
[1, 2 ]
53
[7, 3, 1, 1, 3, 14]
3
[1, 1, 2 ]
54
[7, 2, 1, 6, 1, 2, 14]
5
[2, 4 ]
55
[7, 2, 2, 2, 14]
6
[2, 2, 4 ]
56
[7, 2, 14 ]
7
[2, 1, 1, 1, 4]
57
[7, 1, 1, 4, 1, 1, 14]
8
[2, 1, 4 ]
58
[7, 1, 1, 1, 1, 1, 1, 14]
10
[3, 6 ]
59
[7, 1, 2, 7, 2, 1, 14]
11
[3, 3, 6 ]
60
[7, 1, 2, 1, 14]

298
Chapter 4
NUMBER THEORY
d
√
d
d
√
d
12
[3, 2, 6 ]
61
[7, 1, 4, 3, 1, 2, 2, 1, 3, 4, 1, 14]
13
[3, 1, 1, 1, 1, 6]
62
[7, 1, 6, 1, 14]
14
[3, 1, 2, 1, 6]
63
[7, 1, 14 ]
15
[3, 1, 6 ]
65
[8, 16 ]
17
[4, 8 ]
66
[8, 8, 16 ]
18
[4, 4, 8 ]
67
[8, 5, 2, 1, 1, 7, 1, 1, 2, 5, 16]
19
[4, 2, 1, 3, 1, 2, 8]
68
[8, 4, 16 ]
20
[4, 2, 8 ]
69
[8, 3, 3, 1, 4, 1, 3, 3, 16]
21
[4, 1, 1, 2, 1, 1, 8]
70
[8, 2, 1, 2, 1, 2, 16]
22
[4, 1, 2, 4, 2, 1, 8]
71
[8, 2, 2, 1, 7, 1, 2, 2, 16]
23
[4, 1, 3, 1, 8]
72
[8, 2, 16 ]
24
[4, 1, 8 ]
73
[8, 1, 1, 5, 5, 1, 1, 16]
26
[5, 10 ]
74
[8, 1, 1, 1, 1, 16]
27
[5, 5, 10]
75
[8, 1, 1, 1, 16]
28
[5, 3, 2, 3, 10]
76
[8, 1, 2, 1, 1, 5, 4, 5, 1, 1, 2, 1, 16]
29
[5, 2, 1, 1, 2, 10]
77
[8, 1, 3, 2, 3, 1, 16]
30
[5, 2, 10]
78
[8, 1, 4, 1, 16]
31
[5, 1, 1, 3, 5, 3, 1, 1, 10]
79
[8, 1, 7, 1, 16]
32
[5, 1, 1, 1, 10]
80
[8, 1, 16 ]
33
[5, 1, 2, 1, 10]
82
[9, 18 ]
34
[5, 1, 4, 1, 10v]
83
[9, 9, 18v]
35
[5, 1, 10]
84
[9, 6, 18 ]
37
[6, 12 ]
85
[9, 4, 1, 1, 4, 18]
38
[6, 6, 12]
86
[9, 3, 1, 1, 1, 8, 1, 1, 1, 3, 18]
39
[6, 4, 12]
87
[9, 3, 18 ]
40
[6, 3, 12]
88
[9, 2, 1, 1, 1, 2, 18]
41
[6, 2, 2, 12]
89
[9, 2, 3, 3, 2, 18]
42
[6, 2, 12]
90
[9, 2, 18 ]
43
[6, 1, 1, 3, 1, 5, 1, 3, 1, 1, 12]
91
[9, 1, 1, 5, 1, 5, 1, 1, 18]
44
[6, 1, 1, 1, 2, 1, 1, 1, 12]
92
[9, 1, 1, 2, 4, 2, 1, 1, 18]
45
[6, 1, 2, 2, 2, 1, 12]
93
[9, 1, 1, 1, 4, 6, 4, 1, 1, 1, 18]
46
[6, 1, 3, 1, 1, 2, 6, 2, 1, 1, 3, 1, 12]
94
[9, 1, 2, 3, 1, 1, 5, 1, 8, 1, 5, 1, 1, 3, 2, 1, 18]
47
[6, 1, 5, 1, 12]
95
[9, 1, 2, 1, 18]
48
[6, 1, 12]
96
[9, 1, 3, 1, 18]
50
[7, 14 ]
97
[9, 1, 5, 1, 1, 1, 1, 1, 1, 5, 1, 18]
51
[7, 7, 14]
98
[9, 1, 8, 1, 18]
52
[7, 4, 1, 2, 1, 4, 14]
99
[9, 1, 18 ]
9. Continued fraction expansions for certain quadratic irrationals are given in the fol-
lowing table.

Section 4.9
DIOPHANTINE APPROXIMATION
299
d
continued fraction expansion for
√
d
√
n2 −1
[ n −1, 1, 2n −2 ]
√
n2 −2
[ n −1, 1, n −2, 1, 2n −2 ]
√
n2 + 1
[ n, 2n]
√
n2 + 2
[ n, n, 2n]
√
n2 −n
[ n −1, 2, 2n −2 ]
√
n2 + n
[ n, 2, 2n]
√
4n2 + 4
[ 2n, n, 4n]
√
4n2 −n
[ 2n −1, 1, 2, 1, 4n −2 ]
√
4n2 + n
[ 2n, 4, 4n]
√
9n2 + 2n
[ 3n, 3, 6n]
10. Continued fraction expansions for some famous numbers are given in the following
table.
number
continued fraction expansion
π
[ 3, 7, 15, 1, 292, 1, 1, 1, 2, 1, 3, 1, 14, 2, 1, 1, 2, 2, 2, 2, 1, 84, 2, 1, 1, 15, 3, . . .]
γ
[ 0, 1, 1, 2, 1, 2, 1, 4, 3, 13, 5, 1, 1, 8, 1, 2, 4, 1, 1, 40, 1, 11, 3, 7, 1, 7, 1, 1, 5, . . .]
3√
2
[ 1, 3, 1, 5, 1, 1, 4, 1, 1, 8, 1, 14, 1, 10, 2, 1, 4, 12, 2, 3, 2, 1, 3, 4, 1, 1, 2, 14, . . .]
log 2
[ 0, 1, 2, 3, 1, 6, 3, 1, 1, 2, 1, 1, 1, 1, 3, 10, 1, 1, 1, 2, 1, 1, 1, 1, 3, 2, 3, 1, 13, 7, . . .]
e
[ 2, 1, 2, 1, 1, 4, 1, 1, 6, 1, 1, 8, 1, 1, 10, 1, 1, 12, . . .]
e
1
n
[ 1, n −1, 1, 1, 3n −1, 1, 1, 5n −1, 1, 1, 7n −1, . . . ]
e
2
2n+1
[ 1, (6n + 3)k + n, (24n + 12)k + 12n + 6, (6n + 3)k + 5n + 2, 1, 1k≥0 ]
tanh 1
n
[ 0, n, 3n, 5n, 7n, . . . ]
tan 1
n
[ 0, n −1, 1, 3n −2, 1, 5n −2, 1, 7n −2, 1, 9n −2, . . . ]
1+
√
5
2
[ 1, 1, 1, 1, . . .]
Examples:
1. To ﬁnd the continued fraction representation of 62
23, apply Algorithm 1 to obtain
62
23 = 2 + 1
23
16
,
23
16 = 1 + 1
16
7
,
16
7 = 2 + 1
7
2
,
7
2 = 3 + 1
2.
Combining these equations shows that 62
23 = [2, 1, 2, 3, 2]. Since 2 = 1 + 1
1, it also follows
that 62
23 = [2, 1, 2, 3, 1, 1].
2. Applying Algorithm 1 to ﬁnd the continued fraction of
√
6, it follows that
a0 =
√
6

= 2,
a1 =
 √
6+2
2

= 2,
a2 =
√
6 + 2

= 4,
a3 = a1,
a4 = a2, . . . .
Hence
√
6 = [ 2, 2, 4 ].
3. The continued fraction expansion of e is e = [2, 1, 2, 1, 1, 4, 1, 1, 6, . . .]. This expansion
is often abbreviated as [ 2, 1, 2k, 1k≥1 ]. (See [Pe54].)

300
Chapter 4
NUMBER THEORY
4.9.2
CONVERGENTS
Deﬁnitions:
Let p−2 = 0, q−2 = 1, p−1 = 1, q−1 = 0, and deﬁne pn = anpn−1 + pn−2 and qn =
anqn−1 + qn−2 for n ≥0. Then pn
qn = [a0, a1, . . . , an]. The fraction pn
qn is called the nth
convergent.
Facts:
1. pnqn−1 −pn−1qn = (−1)n+1 for n ≥0.
2. Let θ = [ a0, a1, a2, . . . ] be an irrational number. Then
θ −pn
qn
 <
1
an+1q2
n .
3. If n > 1, 0 < q ≤qn, and p
q ̸= pn
qn , then
θ −p
q
 >
θ −pn
qn
.
4. [ . . . , a, b, 0, c, d, . . . ] = [ . . . , a, b + c, d, . . . ].
5. Almost all real numbers have unbounded partial quotients.
6. For almost all real numbers, the frequency with which the partial quotient k occurs
is log2
 1 +
1
k(k+2)

. Hence, the partial quotient 1 occurs about 41.5% of the time, the
partial quotient 2 occurs about 17.0% of the time, etc.
7. For almost all real numbers,
lim
n→∞(a1a2 . . . an)
1
n = K ≈2.68545.
K is called Khintchine’s constant.
8. L´evy’s law: For almost all real numbers,
lim
n→∞(pn)
1
n = lim
n→∞(qn)
1
n = e
π2
12 log 2 .
Examples:
1. We compute the ﬁrst eight convergents to π as follows:
n =
−2
−1
0
1
2
3
4
5
6
7
8
an =
3
7
15
1
292
1
1
1
2
pn =
0
1
3
22
333
355
103,993
104,348
208,341
312,689
833,719
qn =
1
0
1
7
106
113
33,102
33,215
66,317
99,532
265,381
2. In order to ﬁnd a rational fraction p
q in lowest terms that approximates e to within
10−6, we compute the convergents qn until an+1(qn)2 < 10−6:
n =
−2
−1
0
1
2
3
4
5
6
7
8
9
10
an =
2
1
2
1
1
4
1
1
6
1
1
pn =
0
1
2
3
8
11
19
87
106
193
1,264
1,457
2,721
qn =
1
0
1
1
3
4
7
32
39
71
465
536
1,001
Hence, 2721
1001 ≈2.71828171 is the desired fraction.

Section 4.9
DIOPHANTINE APPROXIMATION
301
4.9.3
APPROXIMATION THEOREMS
Facts:
1. Dirichlet’s theorem: If θ is irrational, then
θ −p
q
 <
1
q2
for inﬁnitely many p, q.
2. Dirichlet’s theorem in d dimensions:
If θ1, θ2, . . . , θd are real numbers with at least
one θi irrational, then
θi −pi
q
 <
1
q1+ 1
d
for inﬁnitely many p1, p2, . . . , pd, q.
3. Hurwitz’s theorem: If θ is an irrational number, then
θ −p
q
 <
1
√
5q2
for inﬁnitely many p, q. The constant
√
5 is best possible.
4. Liouville’s theorem: Let θ be an irrational algebraic number of degree n. Then there
exists a constant c (depending on θ) such that
θ −p
q
 >
c
qn
for all rationals p
q with q > 0. The number θ is called a Liouville number if
θ −p
q
 < q−n
has a solution for all n ≥0. An example of a Liouville number is P
k≥1 2−k!.
5. Roth’s theorem:
Let θ be an irrational algebraic number, and let ǫ be any positive
number. Then
θ −p
q
 >
1
q2+ǫ
for all but ﬁnitely many rationals p
q with q > 0.
4.9.4
IRRATIONALITY MEASURES
Deﬁnitions:
Let θ be a real irrational number. Then the real number µ is said to be an irrationality
measure for θ if for every ǫ > 0 there exists a positive real q0 = q0(ǫ) such that
|θ −p
q | > q−(µ+ǫ) for all integers p, q with q > q0.
Fact:
1. The following table shows the best irrationality measures known for some important
numbers.
number θ
measure µ
discoverer
π
7.6064
Salikhov (2008)
π2
5.4413
Rhin and Viola (1996)
ζ(3)
5.5139
Rhin and Viola (2001)
log 2
3.5746
Marcovecchio (2009)
π
√
3
4.2304
Androsenko (2015)

302
Chapter 4
NUMBER THEORY
4.10
ALGEBRAIC NUMBER THEORY
When working on questions involving integers or rational numbers, one is often led to
look at extensions of the rational numbers. These yield rings of algebraic integers, which
have many properties similar to the usual integers. A notable exception is the frequent
lack of unique factorization into primes.
Many classical questions can be rephrased in terms of algebraic number theory, for exam-
ple, factorization of polynomials modulo primes. The number ﬁeld sieve, which is based
on algebraic number theory, is the strongest general purpose factorization algorithm cur-
rently being used. Algebraic number theory is an active subject of research and remains
a basic tool in diophantine equations and in many other areas of mathematics.
Standard references are [La00], [IrRo98], [Mu11], [StTa16], and [Wa97].
4.10.1
BASIC CONCEPTS
Deﬁnitions:
A complex number α is an algebraic number if it is a root of a monic polynomial with
rational coeﬃcients. (A monic polynomial is a polynomial with leading coeﬃcient
equal to 1.)
An algebraic number α is an algebraic integer if it is a root of a monic polynomial
with integer coeﬃcients.
If α is an algebraic number with minimal polynomial f(x) of degree n, then the n −1
other roots of f(x) are called the conjugates of α.
If α and β are algebraic integers and there is an algebraic integer γ such that αγ = β,
then α divides β, written α|β.
An algebraic number α is of degree n if it is a root of a monic polynomial with rational
coeﬃcients of degree n but is not a root of any monic polynomial with rational coeﬃcients
of degree less than n.
The integers of an algebraic number ﬁeld are the algebraic integers that belong to this
ﬁeld.
A number ﬁeld is a ﬁeld that is a ﬁnite degree extension of Q.
An algebraic integer ǫ is a unit if there exists an algebraic integer γ such that ǫγ = 1.
Facts:
1. The integers are the only rational numbers that are algebraic integers.
2. Sums and products of algebraic integers are algebraic integers, so the algebraic inte-
gers in a ﬁeld form a ring.
3. Each element of a number ﬁeld is an algebraic number.
Examples:
1. Two of the most common examples of number ﬁelds are quadratic ﬁelds and cyclo-
tomic ﬁelds. (See §4.10.2 and §4.10.6.)

Section 4.10
ALGEBRAIC NUMBER THEORY
303
2. The numbers 2 +
√
3 and 2 −
√
3 are algebraic integers because they are roots of the
polynomial X2 −4X + 1. They are conjugates of each other. They are units because
(2 +
√
3)(2 −
√
3) = 1.
3. 1/
√
2 is an algebraic number but is not an algebraic integer.
It is a root of the
polynomial is X2 −(1/2), which does not have integer coeﬃcients, and there is no monic
polynomial with integer coeﬃcients that has 1/
√
2 as a root.
4. The minimal polynomial of
3√
2 is X3 −2. The other roots of this polynomial are
ω
3√
2 and ω2 3√
2, where ω is a primitive cube root of unity, so these are the conjugates of
3√
2.
5. Let i = √−1. The ﬁeld Q(i) = {a + bi | a, b ∈Q} is a number ﬁeld of degree 2.
6. The ﬁeld Q(
3√
2) = {a + b
3√
2 + c
3√
4 | a, b, c ∈Q} is a number ﬁeld of degree 3.
7. The algebraic integers in Q(
3√
2) are Z[
3√
2] = {a + b
3√
2 + c
3√
4 | a, b, c ∈Z}.
8. Algebraic numbers arise naturally when solving diophantine equations involving in-
tegers. (See §4.10.4, Example 6.)
4.10.2
QUADRATIC FIELDS
Deﬁnitions:
If d ̸= 1 is a square-free integer, then Q(
√
d) = {a + b
√
d | a and b are rational numbers}
is called a quadratic ﬁeld. If d > 1, then Q(
√
d) is called a real quadratic ﬁeld; if
d < 0, then Q(
√
d) is called an imaginary quadratic ﬁeld.
A number α in Q(
√
d) is a quadratic integer (or an integer when the context is clear)
if α is an algebraic integer.
The integers of Q(√−1) are called the Gaussian integers. (These are the numbers in
Z[i] = {a + bi | a, b are integers}. See §5.4.2.)
If α = a + b
√
d belongs to Q(
√
d), then its conjugate, denoted by α, is the number
a −b
√
d.
If α belongs to Q(
√
d), then the norm of α is the number N(α) = αα.
Facts:
1. The integers of the ﬁeld Q(
√
d), where d is a square-free integer, are the numbers
a + b
√
d when d ≡2 or 3 (mod 4). When d ≡1 (mod 4), the integers of the ﬁeld are the
numbers a+b
√
d
2
, where a and b are integers which are either both even or both odd.
2. If d < 0, d ̸= −1, d ̸= −3, then there are exactly two units, ±1, in Q(
√
d). There
are exactly four units in Q(√−1), namely ±1 and ±i. There are exactly six units in
Q(√−3): ±1, ± −1+√−3
2
, ± −1−√−3
2
.
3. If d > 1, there are inﬁnitely many units in Q(
√
d). Furthermore, there is a unit
ǫ0 > 1, called the fundamental unit of Q(
√
d), such that all units are of the form ±ǫn
0
where n is an integer.
Examples:
1. The conjugate of −2 + 3i in the ring of Gaussian integers is −2 −3i. Consequently,
N(−2 + 3i) = (−2 −3i)(−2 + 3i) = 13.

304
Chapter 4
NUMBER THEORY
2. The number 1 +
√
2 is a fundamental unit of Q(
√
2). Therefore, all units are of the
form ±(1 +
√
2)n where n = 0, ±1, ±2, . . ..
4.10.3
PRIMES AND UNIQUE FACTORIZATION IN QUADRATIC FIELDS
Deﬁnitions:
An integer π in Q(
√
d), not zero or a unit, is irreducible in Q(
√
d) if, whenever π = αβ
where α and β are integers in Q(
√
d), either α or β is a unit.
An integer π in Q(
√
d), not zero or a unit, is prime in Q(
√
d) if, whenever π | αβ where
α and β are integers in Q(
√
d), either π | α or π | β.
If α and β are nonzero integers in Q(
√
d) and α = βǫ where ǫ is a unit, then β is called
an associate of α.
A quadratic ﬁeld Q(
√
d) is a Euclidean ﬁeld with respect to the norm if, given integers
α and β in Q(
√
d) where β is not zero, there are integers δ and γ in Q(
√
d) such that
α = γβ + δ and |N(δ)| < |N(β)|.
A quadratic ﬁeld Q(
√
d) has the unique factorization property if whenever α ̸= 0
is a non-unit integer in Q(
√
d) with two factorizations α = ǫπ1π2 . . . πr = ǫ′π′
1π′
2 . . . π′
s
where ǫ and ǫ′ are units and each πi and π′
j is prime, then r = s and the primes πi and
π′
j can be paired oﬀinto pairs of associates.
Facts:
1. If α is an integer in Q(
√
d) and N(α) is an integer that is prime, then α is irreducible.
2. The integers of Q(
√
d) are a unique factorization domain if and only if every irre-
ducible is prime.
3. A Euclidean quadratic ﬁeld has the unique factorization property.
4. The quadratic ﬁeld Q(
√
d) is Euclidean if and only if d is one of the following integers:
−11, −7, −3, −2, −1, 2, 3, 5, 6, 7, 11, 13, 17, 19, 21, 29, 33, 37, 41, 57, 73.
5. If d < 0, then the imaginary quadratic ﬁeld Q(
√
d) has the unique factorization
property if and only if d = −1, −2, −3, −7, −11, −19, −43, −67, or −163. This theorem
was stated as a conjecture by Gauss in the 19th century and was proved in the 1960s by
Harold Stark and Alan Baker independently.
6. In 1952, the engineer Kurt Heegner (1893–1965) had published a proof of the result of
Baker and Stark, but it was not accepted because it relied on some defective statements
of Heinrich Weber. After the proofs by Baker and Stark, the gaps in Heegner’s proof
were ﬁlled in by Stark, Max Deuring, and Bryan Birch, independently.
7. It is unknown whether inﬁnitely many real quadratic ﬁelds Q(
√
d) have the unique
factorization property.
8. Of the 60 real quadratic ﬁelds Q(
√
d) with 2 ≤d ≤100, exactly 38 have the unique
factorization property, namely those with d = 2, 3, 5, 6, 7, 11, 13 14, 17, 19, 21, 22, 23,
29, 31, 33, 37, 38, 41, 43, 46, 47, 53, 57, 59, 61, 62, 67, 69, 71, 73, 77, 83, 86, 89, 93, 94,
and 97.
9. The Cohen-Lenstra heuristics [CoLe84] predict that, as p runs through the primes
p ≡1 (mod 4), approximately 75.446% of the ﬁelds Q(√p) have the unique factorization
property.

Section 4.10
ALGEBRAIC NUMBER THEORY
305
Examples:
1. The number 2 + i is an irreducible Gaussian integer. This follows since its norm
N(2 + i) = (2 + i)(2 −i) = 5 is a prime integer. Its associates are itself and the three
Gaussian integers (−1)(2 + i) = −2 −i, i(2 + i) = −1 + 2i, and −i(2 + i) = 1 −2i.
2. The integers of Q(√−5) are the numbers of the form a + b√−5, where a and b are
integers. The ﬁeld Q(√−5) does not have the unique factorization property. To see this,
note that 6 = 2 · 3 = (1 + √−5)(1 −√−5) and each of 2, 3, 1 + √−5, and 1 −√−5
are irreducibles in this quadratic ﬁeld. For example, to see that 1 + √−5 is irreducible,
suppose that 1+√−5 = (a+b√−5)(c+d√−5). This implies that 6 = (a2+5b2)(c2+5d2),
which is impossible unless a = ±1, b = 0 or c = ±1, d = 0. Consequently, one of the
factors must be a unit.
4.10.4
GENERAL NUMBER FIELDS
Deﬁnitions:
If I and J are ideals of the ring of algebraic integers of a number ﬁeld, I divides J if
there exists an ideal I′ of this ring such that II′ = J.
Let K be a number ﬁeld of degree n over Q. Choose algebraic integers β1, . . . , βn in K
such that every algebraic integer in K is a linear combination of β1, . . . , βn with integer
coeﬃcients. Form the n × n matrix (aij), where the ijth entry is the trace from K to Q
of βiβj. The discriminant of K, denoted DK, is the determinant of the matrix (aij).
Two nonzero ideals I1 and I2 of the ring of algebraic integers of a number ﬁeld K are in
the same ideal class if there is an element x in the ﬁeld such that I1 = xI2, where the
latter is the set {xi | i ∈I2}.
The set of ideal classes forms a group, called the ideal class group of K.
The order of the ideal class group is called the class number of K, and is often denoted
h or hK.
If α and β are algebraic integers in a number ﬁeld K and I is an ideal of the ring of
integers of K, then α is congruent to β mod I, written α ≡β (mod I), if α −β ∈I.
Let K be a number ﬁeld and let OK be the ring of algebraic integers in K. Let I be a
nonzero ideal of OK. The norm of I, denoted N(I), is the order of OK/I.
Let K be a number ﬁeld of degree n over Q. Write K = Q(α) for some α, and let
m(X) be the minimal polynomial of α. This polynomial has r1 real roots and r2 pairs
of complex roots. The numbers r1 and r2 are called the number of real embeddings
of K and the number of pairs of complex embeddings, respectively.
Facts:
1. Algebraic number theory arose from attempts to generalize the law of quadratic
reciprocity (see §4.7.3) to higher power reciprocity laws.
2. Cubic reciprocity is best stated using the Eisenstein integers, namely the algebraic
integers Z[ω], where ω is a primitive cube root of unity. Let π1 and π2 be two primes
of Z[ω] that are relatively prime to each other and do not divide 3. The cubic residue
symbol

π1
π2

3 is the unique power of ω satisfying
π(N(π2)−1)/3
1
≡
π1
π2

3
(mod π2).

306
Chapter 4
NUMBER THEORY
3. Cubic reciprocity says that if π1 and π2 are congruent to 2 modulo 3, then
π1
π2

3
=
π2
π1

3
.
This was proved by Gotthold Eisenstein (1823–1852), Gauss, and others.
4. Gauss proved a quartic (or biquadratic) reciprocity law in the context of the Gaussian
integers Z[i], where i = √−1.
5. Ernst Kummer (1810–1893) studied pth power reciprocity laws for odd primes p in
the context of the pth cyclotomic ﬁeld Q(ζ) (see §4.10.6), where ζ is a primitive pth root
of unity.
6. Kummer realized that unique factorization can fail in cyclotomic ﬁelds and was thus
led to his theory of ideal numbers (in modern terminology, these are essentially ring
homomorphisms from the ring of integers of a cyclotomic ﬁeld to a ﬁnite ﬁeld; see [Ma77])
and ideal class groups.
7. Richard Dedekind (1831–1916) reinterpreted Kummer’s ideal numbers as ideals in
the ring of algebraic integers.
8. In modern language, Kummer showed that every nonzero ideal is uniquely a product
of prime ideals.
9. The ideal class group is a ﬁnite abelian group.
10. The class number of a number ﬁeld K is 1 if and only if the ring of algebraic integers
in K is a unique factorization domain.
11. The concept of the class number of a number ﬁeld can be useful for ﬁnding solutions
to diophantine equations in integers. See Example 7 and §4.10.6, Example 3.
12. Let K = Q(
√
d), where d ̸= 1 is a square-free integer that can be chosen positive or
negative. The discriminant DK is 4d if d ≡2, 3 (mod 4), and d if d ≡1 (mod 4).
13. If I and J are nonzero ideals of the ring of algebraic integers of a number ﬁeld, then
I divides J if and only if J ⊆I.
14. Let K be a number ﬁeld, let r1 be the number of real embeddings of K, and let
r2 be the number of pairs of complex embeddings. Let WK be the group of roots of
unity contained in K. The Dirichlet unit theorem says that the group of units of the
ring of algebraic integers in K is the direct sum of WK and a free abelian group of rank
r1 + r2 −1.
15. The Minkowski bound says that if K is a number ﬁeld of degree n over Q with r2
pairs of complex embeddings, then each ideal class contains an ideal I with
1 ≤N(I) ≤n!
nn
  4
π
r2 p
| DK |,
where DK is the discriminant of K.
16. The Cohen-Lenstra heuristics [CoLe84] predict that if p is an odd prime, then p
divides the class number of a certain fraction of imaginary quadratic ﬁelds. This fraction
is
1 −Q∞
m=1(1 −pm).
For p = 3, this is 0.43987 . . .. For real quadratic ﬁelds, the predicted fraction is
1 −Q∞
m=2(1 −pm).
For p = 3, this is 0.15981 . . .. Both of these predictions agree well with computed data.

Section 4.10
ALGEBRAIC NUMBER THEORY
307
Examples:
1. If K = Q(
√
−d) is an imaginary quadratic ﬁeld, r1 = 0 and r2 = 1. Therefore,
r1 + r2 −1 = 0, so the group of units consists of only the roots of unity in K.
2. If K = Q(
√
d) is a real quadratic ﬁeld, r1 = 2 and r2 = 0. Therefore, r1 + r2 −1 = 1,
so the group of units is the direct sum of ±1 and an inﬁnite cyclic group. The generator
ǫ0 > 1 for this inﬁnite cyclic group is called the fundamental unit of K.
3. Let a be an integer such that a2 + 3a + 9 is square free. Let ǫ1 and ǫ2 be two of the
roots of X2 −aX −(a+3)X −1. Then the group of units of the ring of algebraic integers
in Q(ǫ1) is the direct sum of ±1 and the rank 2 free abelian group generated by ǫ1 and
ǫ2. The ﬁelds of this form are known as the simplest cubic ﬁelds.
4. The ring of algebraic integers in Q(√−5) is the set of numbers of the form a+b√−5,
where a, b are integers. The ideal I generated by 2 and 1+√−5 and the ideal J generated
by 3 and 1 −√−5 are not principal. Let x = (1 −√−5)/2. Then xI = J, so I and
J are in the same ideal class. The product IJ is by deﬁnition the ideal generated by
the products of the generators of I and J, so it is the ideal generated by 6, 2 −2√−5,
3 + 3√−5, and 6. Since 6 −(2 −2√−5) −(3 + √−5) = 1 −√−5 is in IJ, and all of
these generators of IJ are multiples of 1 −√−5, we ﬁnd that IJ is the principal ideal
generated by 1 −√−5.
5. Let K = Q(√−5). In the deﬁnition of DK, take β1 = 1 and β2 = √−5. Then the
matrix (aij) is
 
2
0
0
−10
!
,
which has determinant DK = −20. The Minkowski bound is 4
√
5/π < 3, so each ideal
class contains an ideal of norm 1 or 2. Only the full ring of algebraic integers has norm
1, and the ideal I in the previous example is the only ideal of norm 2, so there are only
two ideal classes. Therefore, the class number is 2.
6. Suppose we want to ﬁnd all integer solutions of x2 + 1 = y3. Factor the left side as
(x + i)(x −i). It can be shown that x + i and x −i are relatively prime in Z[i], which
has the unique factorization property. Since their product is a cube, each factor is a
unit times a cube in Z[i]. In particular, x + i = (a + bi)3. The imaginary parts yield
1 = b(3a2 −b2), which implies that b = ±1 = (3a2 −b2). This yields a = 0, hence x = 0.
The only integer solution to x2 + 1 = y3 is therefore (x, y) = (0, 1).
7. Suppose we want to ﬁnd all solutions of x2 + 13 = y3.
Factor the left side as
(x + √−13)(x −√−13). It can be shown that the ideals I1 = (x + √−13)Z[√−13] and
I2 = (x −√−13)Z[√−13] are relatively prime, in the sense that they have no common
prime ideal factors. The product I1I2 equals the cube of the ideal yZ[√−13]. Unique
factorization into prime ideals implies that both I1 and I2 must be cubes of ideals.
Therefore, J3 = I1 for some ideal J of Z[√−13]. But I1 is a principal ideal, so J has
order 1 or 3 in the ideal class group. The class number of Q(√−13) is 2, so J2 must also
be principal. It follows that J is principal: J = (a+b√−13)Z[√−13]. Since J3 = I1, and
the units of Z[√−13] are ±1, which are cubes, we must have (a + b√−13)3 = x + √−13.
The coeﬃcients of √−13 yield 1 = b(3a2 −13b2), so b = ±1 = 3a2 −13b2. This yields
a = ±4, and then x = ±70, y = 17.
8. Let ω be a primitive cube root of unity. Let π1 = −3ω −1 and π2 = −6ω −1. Then
π1 ≡π2 ≡2 (mod 3). Since N(π2) = 31, the cubic residue symbol is
π1
π2

3
≡π(31−1)/3
1
= −3725 −18357ω = 1 + (−6ω −1)(2952 −129ω) ≡1 (mod π2).

308
Chapter 4
NUMBER THEORY
Since π2 = 1 + 2π1 ≡1 (mod π1), the cubic residue symbol is

π2
π1

3 = 1.
9. In Example 8, cubic reciprocity plus the fact that π2 ≡1 (mod π1) could be used to
calculate

π1
π2

3:
π1
π2

3
=
π2
π1

3
=
 1
π1

3
= 1.
This implies that π1 is congruent to a cube mod π2. A calculation yields (12 + ω)3 ≡
π1 (mod π2).
4.10.5
PROPERTIES OF NUMBER FIELDS
Deﬁnitions:
The ring of algebraic integers in a number ﬁeld is called monogenic if the ring is of the
form
Z[α] = {a0 + a1α + a2α2 + · · · + amαm | m ≥0, ai ∈Z for 0 ≤i ≤m}.
Let p be a prime number and let K be a number ﬁeld of degree n over Q. Let OK
be the ring of algebraic integers in K. Factor the ideal pOK into prime ideals in OK
as P e1
1 P e2
2 · · · P eg
g , where P1, . . . , Pg are distinct prime ideals of OK and e1, . . . , eg are
positive integers. If some ei > 1 then p ramiﬁes in K. If g = n then p is said to split
completely in K. If g = 1 and e1 = 1, then p is inert in K.
The Dedekind zeta function of a number ﬁeld K is deﬁned to be
ζK(s) =
X
I
N(I)−s,
where s is a complex number with R(s) > 1 and the sum is over the nonzero ideals I of
OK.
Facts:
1. A prime number p ramiﬁes in a number ﬁeld K if and only if p divides the discriminant
DK.
2. If K ̸= Q, then |DK| > 1, so there is always at least one prime number that ramiﬁes
in K.
3. Let K be a number ﬁeld and suppose its ring of integers is Z[α]. Let f(X) be the min-
imal polynomial of α. Let p be a prime and suppose f(X) ≡h1(X)e1h2(X)e2 · · · hg(X)eg
(mod p) is the factorization of f(X) mod p into monic polynomials that are distinct mod
p and irreducible mod p. Then the ideal generated by p factors into prime ideals as
P e1
1 P e2
2 · · · P eg
g
in the ring of integers of K. The prime ideal Pi is generated by p and
hi(α).
4. The ring of algebraic integers in Q(ω,
3√
2), where ω is a primitive cube root of unity,
is Z[α], where α = (1 −
3√
2 + w
3√
4)/(1 −w).
5. Let ω be a primitive cube root of unity. The ring of algebraic integers of the ﬁeld
Q(ω,
3√
d) is not monogenic for any cubefree integers d ̸= ±2, ±4. (See [Ch02].)
6. Let K be a Galois extension of Q of odd prime degree p over Q. If 2p+1 is composite,
then the ring of algebraic integers in K is not monogenic.
If 2p + 1 = ℓis prime,
then the ring is monogenic if and only if K = Q(cos(2π/ℓ)), in which case the ring is
Z[2 cos(2π/ℓ)]. (See [Gr86].)

Section 4.10
ALGEBRAIC NUMBER THEORY
309
7. The number ﬁeld sieve is a method that uses algebraic numbers to produce relations
x2 ≡y2 (mod n) in integers. It is the basis for a very powerful factorization algorithm
that has been used to factor integers of around 200 decimal digits. (See §4.5.1.)
8. The Dedekind zeta function of a number ﬁeld, although deﬁned initially only for
R(s) > 1, has a natural extension to a function analytic in the whole complex plane,
except for a simple pole at 1.
9. The Generalized Riemann Hypothesis predicts that each zero of the Dedekind zeta
function of a number ﬁeld with real part between 0 and 1 must have real part equal to
1/2.
10. The Generalized Riemann Hypothesis implies the truth of Artin’s conjecture (see
§4.7.1) that every integer other than −1 and perfect squares is a primitive root for
inﬁnitely many primes.
Examples:
1. If p ≡1 (mod 4), then p can be written as a sum of two squares: p = a2 + b2.
Therefore, p = (a + bi)(a −bi) in Q(i), which means that p splits completely in Q(i). If
p ≡3 (mod 4), then pZ[i] is a prime ideal, so p is inert. The relation 2 = −i(1 + i)2
written in terms of ideals becomes (2) = (1 + i)2, so 2 ramiﬁes.
2. The polynomial X2 + 5 factors mod 3 as (X + 1)(X + 2). Therefore (Fact 3), the
ideal generated by 3 in Z[√−5] factors as P1P2, where P1 is generated by 1 + √−5 and
3, and P2 is generated by 2 + √−5 and 3.
3. The polynomial X2 +5 is irreducible mod 11. Therefore (Fact 3), the ideal generated
by 11 in Z[√−5] is a prime ideal.
4. The polynomial X2 + 5 factors mod 2 as (X + 1)2. Therefore (Fact 3), the ideal
generated by 2 in Z[√−5] factors as P 2, where P is the ideal generated by 2 and 1+√−5.
4.10.6
CYCLOTOMIC FIELDS
Deﬁnitions:
The nth cyclotomic ﬁeld is the ﬁeld obtained by adjoining a primitive nth root of unity
to the ﬁeld of rational numbers, that is, the ﬁeld Q(ζ), where ζ = e2πi/n.
A prime p is called regular if p does not divide the class number of the pth cyclotomic
ﬁeld. If p divides this class number, p is called irregular.
Facts:
1. Cyclotomic ﬁelds arose in the work of Gauss on constructibility of regular polygons
by compass and straightedge.
2. Cyclotomic ﬁelds also arose as a natural setting for generalizations of quadratic reci-
procity. (See §4.10.4.)
3. Kummer developed much of the theory of cyclotomic ﬁelds in the mid-1800s.
4. The classical proof of Fermat’s last theorem for regular primes by Kummer relied on
his introduction of ideal numbers and ideal class groups to establish a replacement for
the unique factorization property in cyclotomic ﬁelds. See Example 4.
5. Kummer’s work superseded incorrect proofs of Lam´e and Cauchy, who had incorrectly
and implicitly assumed that the pth cyclotomic ﬁeld always has unique factorization.

310
Chapter 4
NUMBER THEORY
6. Extensions of Kummer’s work led to much progress on Fermat’s last theorem through
the 1900s, even though the eventual proof of the theorem was by diﬀerent techniques.
7. The nth cyclotomic ﬁeld has degree φ(n) over Q, where φ is Euler’s function.
8. The ring of algebraic integers in the nth cyclotomic ﬁeld is Z[ζ] = {a0 + a1ζ + · · · +
aφ(n)−1ζφ(n)−1 | ai ∈Z}, where ζ is a primitive nth root of unity. Therefore, this ring
is monogenic.
9. The Kronecker-Weber theorem says that if K is a number ﬁeld such that K/Q is a
Galois extension with abelian Galois group, then K is contained in a cyclotomic ﬁeld.
10. Let n ≥3 be an integer. The nth cyclotomic ﬁeld contains the ﬁeld Q(cos(2π/n)),
and the class number h+
n of this latter ﬁeld divides the class number hn of the nth
cyclotomic ﬁeld.
11. The quotient h−
n = hn/h+
n can be computed fairly quickly. The computation of h+
n
is diﬃcult.
12. The largest prime p for which h+
p has been computed is p = 151 (and h+
151 = 1).
13. Vandiver’s conjecture predicts that if p is prime then p does not divide h+
p . This
has been veriﬁed for all p < 163 · 106.
14. There are inﬁnitely many irregular primes. It has not been proved that there are
inﬁnitely many regular primes. Heuristic models predict that about 61% (= e−1/2) of
primes are regular. This prediction agrees well with computational data.
15. Let p be prime. The numbers ǫa = sin(2πa/p)/ sin(2π/p) for 2 ≤a ≤(p −1)/2 are
units of the ring of algebraic integers of the pth cyclotomic ﬁeld. They, along with −1
and the pth roots of unity, generate a subgroup, called the cyclotomic units, of index h+
p
in the group of units of the ring of algebraic integers of this ﬁeld.
16. Preda Mih˘ailescu’s proof of the Catalan conjecture (§4.8.4) relied heavily on prop-
erties of cyclotomic ﬁelds.
Examples:
1. Let p be an odd prime. The class number hp of the pth cyclotomic ﬁeld is 1 if and
only if p ≤19. The class number of the 23rd cyclotomic ﬁeld is 3.
2. The class number of the 37th cyclotomic ﬁeld is h37 = 37. This factors as h−
37 = 37
and h+
37 = 1. The class number of the 59th cyclotomic ﬁeld is 41241, which factors as
h−
59 = 41241 = 3 · 59 · 233 and h+
59 = 1. Both 37 and 59 are irregular primes.
3. The smallest prime p with h+
p > 1 is p = 163, for which h+
163 = 4.
4. First case of Fermat’s last theorem for regular primes:
Suppose x, y, z are nonzero
integers not divisible by the odd, regular prime p, and xp + yp = zp. Let ζ be a primitive
pth root of unity. Then Qp
j=1(x + ζjy) = zp. If gcd(x, y, z) = 1 then the factors x + ζjy
are pairwise relatively prime. Since their product is a pth power, the ideal generated
by each factor is the pth power of an ideal: (x + ζjy) = Ip
j . If the class number of the
pth cyclotomic ﬁeld is not a multiple of p, then this implies that Ij must be principal,
so x + ζjy is a unit times the pth power of an element of the ﬁeld. This is the same
as can be obtained when there is unique factorization in the pth cyclotomic ﬁeld. The
remainder of the argument is fairly straightforward. See [Wa97].

Section 4.11
ELLIPTIC CURVES
311
4.11
ELLIPTIC CURVES
Elliptic curves have been studied since the time of Diophantus and continue to be a very
active area of research. The most important fact is that the points on an elliptic curve
have a group structure. Elliptic curves arise as diophantine equations, and they played
a crucial role in the proof of Fermat’s last theorem. Elliptic curves over ﬁnite ﬁelds are a
basic tool in cryptography, mostly because of the diﬃculty of the Discrete Log Problem.
4.11.1
BASIC CONCEPTS
Deﬁnitions:
If F is a ﬁeld of characteristic not 2 or 3, an elliptic curve E deﬁned over F is a
curve that can be given by the Weierstrass equation
y2 = x3 + Ax + B,
where A, B ∈F. It is required that the cubic polynomial x3 + Ax + B have no repeated
roots, which is equivalent to 4A3 + 27B2 ̸= 0.
If K is a ﬁeld containing F, then we denote
E(K) = {(x, y) | (x, y) lies on E and x, y ∈K} ∪{∞},
where ∞is the point at inﬁnity. (See Fact 10.)
If E is given by y2 = x3 + Ax + B, then the j-invariant is deﬁned as
j(E) = (1728)4A3/(4A3 + 27B2).
Given points P and Q on an elliptic curve E in Weierstrass form, the group law (or
addition law) is deﬁned as follows. Draw the line through P and Q (the tangent line
to E if P = Q). This line intersects E at a third point R′ (if the line is vertical, R′ = ∞;
see Facts 10 and 11). Reﬂect R′ across the x-axis to get a point R. This operation is
denoted P + Q = R (it is not simply coordinate addition).
If n is a positive integer, then the multiple n of point p is the sum of n copies of P:
i.e., nP = P + P + · · · + P.
A torsion point is a point P on an elliptic curve E such that nP = ∞for some integer
n > 0, where ∞is the point at inﬁnity (see Fact 10).
Facts:
1. Comprehensive coverage of elliptic curves can be found in [Si09], [SiTa15], and [Wa08].
2. Although elliptic curves can be deﬁned over arbitrary ﬁelds, the most important
examples are over ﬁnite ﬁelds, the rational numbers, and the complex numbers.
3. Elliptic curves are not ellipses.
They received their name because of their close
relation with elliptic integrals, which occur in many physical problems, including ﬁnding
the arc length of an ellipse.
4. The addition of points satisﬁes the associative law: (P + Q) + R = P + (Q + R).

312
Chapter 4
NUMBER THEORY
5. If K is a ﬁeld containing the coeﬃcients of E, the addition law makes E(K) into an
abelian group, where the point at inﬁnity is the identity element.
6. The inverse of a point P = (x, y) is −P = (x, −y).
Points can be subtracted:
Q −P = Q + (−P).
7. Addition of points:
Let E be an elliptic curve deﬁned by y2 = x3 + Ax + B. Let
P1 = (x1, y1) and P2 = (x2, y2) be points on E with P1, P2 ̸= ∞. Let P1 + P2 = P3 =
(x3, y3).
• If x1 ̸= x2, then
x3 = m2 −x1 −x2,
y3 = m(x1 −x3) −y1,
where m = y2−y1
x2−x1 .
• If x1 = x2 but y1 ̸= y2, then P1 + P2 = ∞.
• If P1 = P2 and y1 ̸= 0, then
x3 = m2 −2x1,
y3 = m(x1 −x3) −y1,
where m = 3x2
1+A
2y1
.
• If P1 = P2 and y1 = 0, then P1 + P2 = ∞.
• P1 + ∞= P1.
8. The following ﬁgure gives a picture of how addition works for elliptic curves over
the real numbers. For most ﬁelds, this picture cannot be drawn, but it is helpful for
geometric intuition.
P
Q
R
R
9. A curve y2 = x3 +Ax+B can be put into homogeneous form y2z = x3 +Axz2 +Bz3
and the points on the curve are points (x : y : z) in projective space. If z ̸= 0, such a
point can be rescaled to (x/z : y/z : 1), which is a point on the original nonhomogeneous
curve.
10. In homogeneous coordinates, the curve in projective space contains the point (x :
y : z) = (0 : 1 : 0), which is called the point at inﬁnity. In most situations, it is easiest to
regard the point at inﬁnity as a formal symbol ∞that acts as the identity of the group
law. The point at inﬁnity is usually denoted ∞or O.
11. Vertical lines (x = constant) in the plane extend in projective space to contain the
point at inﬁnity.
12. When the characteristic of the ﬁeld is 2 or 3, the generalized Weierstrass form is
used:
y2 + a1xy + a3y = x3 + a2x2 + a4x + a6,

Section 4.11
ELLIPTIC CURVES
313
with a1, a2, a3, a4, a6 ∈F. The reﬂection across the x-axis in the group law is replaced
by the map (x, y) 7→(x, −y −a1x −a3). See [Si09].
13. When the characteristic of the ﬁeld is not 2 or 3, there are situations where the
generalized Weierstrass form y2 + a1xy + a3y = x3 + a2x2 + a4x + a6 is useful, but a
change of variables can be used to put the curve into Weierstrass form.
14. Elliptic curves played a crucial role in the proof of Fermat’s last theorem (§4.8.3).
Suppose an + bn = cn, where a, b, c, n are positive integers and n ≥3. Form the elliptic
curve
y2 = x(x −an)(x + bn).
The fact that the cubic polynomial has discriminant (abc)2n, which is a 2nth power,
forces the elliptic curve to have properties so restrictive that it can be shown that such
a curve cannot exist. Therefore, a, b, c cannot exist.
15. In the 1980s, Hendrik Lenstra developed a method for factorization of integers using
elliptic curves. It is similar to the Pollard p−1 method (see §4.5.1), but has the advantage
that it can use several elliptic curves in parallel. If a set of curves does not produce the
factorization, another set can be used, an aspect that has no analogue for the Pollard
p −1 algorithm. The method is eﬀective for factoring integers of around 50 decimal
digits.
16. ShaﬁGoldwasser and Joe Kilian invented a primality test based on elliptic curves.
It was reﬁned by Oliver Atkin and Fran¸cois Morain and has been used to prove the
primality of integers with more than 20,000 decimal digits. (See §4.4.4, Fact 18.)
17. Suppose we have a curve deﬁned over a ﬁeld of characteristic not 2 by an equation
v2 = au4 + bu3 + cu2 + du + e and suppose we have a point (p, q) lying on the curve.
There is a change of variables that transforms the equation to Weierstrass form. See
[Co92].
18. A nonsingular plane cubic curve can be changed to a curve in Weierstrass form; a
sketch of the procedure is given in [SiTa15]. See Example 4 for a simple example.
19. Let E be an elliptic curve over F, let n be a positive integer not divisible by the
characteristic of F, and let E[n] be the set of points P ∈E(F) (where F is the algebraic
closure of F) that satisfy nP = ∞. Then E[n] ≃Z/nZ ⊕Z/nZ.
20. Let E be deﬁned over a ﬁeld F of characteristic p > 0, let m > 0, and let E[pm] be
the set of points P ∈E(F) that satisfy pmP = ∞. Then E[pm] ≃either Z/pmZ or 0.
The second case happens if and only if E is supersingular (see §4.11.3).
21. Let E be an elliptic curve over F, let n be a positive integer not divisible by the
characteristic of F, and let E(F)[n] be the set of points P ∈E(F) that satisfy nP = ∞.
If E(F)[n] ≃Z/nZ ⊕Z/nZ, then F contains the nth roots of unity. When F = GF(q),
this means that n | q −1.
Examples:
1. The elliptic curve y2 = x3 + 73 contains the points (2, 9) and (3, 10).
The line
through these two points is y = x + 7, which intersects the elliptic curve in (2, 9),
(3, 10), and (−4, 3). The reﬂection across the x-axis of the third point is (−4, −3), so
(2, 9) + (3, 10) = (−4, −3).
2. The line y = (2/3)(x −2) + 9 is tangent to the elliptic curve y2 = x3 + 73 at the
point (2, 9). The line also intersects the curve at (−32/9, 143/27). The reﬂection across
the x-axis is (−32/9, −143/27), so 2(2, 9) = (2, 9) + (2, 9) = (−32/9, −143/27).

314
Chapter 4
NUMBER THEORY
3. The point P = (2, 3) is a torsion point on the elliptic curve y2 = x3+1. The multiples
of this point are
2P = (0, 1),
3P = (−1, 0),
4P = (0, −1),
5P = (2, −3),
6P = ∞.
4. The change of variables x = 12c/(a + b), y = 36(a −b)/(a + b) changes the elliptic
curve y2 = x3 −432 into the Fermat equation of exponent 3, namely, a3 + b3 = c3.
5. The cannonball problem, which goes back at least to Edouard Lucas in 1875, asks
if there is a set of cannonballs that can be put into a square pyramid and also can be
arranged in a square array. If the square pyramid has x layers and the square array is
y × y, the problem asks for integer solutions to y2 = x(x+ 1)(2x+ 1)/6, an equation that
can be transformed into Weierstrass form y2
1 = x2
1 −36x1 with the change of variables
x = (x1 −6)/12, y = y1/72. Besides the trivial solutions x = 0 and x = 1, the only other
nonnegative integral solution is x = 24, corresponding to 4,900 cannonballs.
6. The congruent number problem asks which positive integers occur as areas of right
triangles with rational sides.
Such an integer was called a congruent number by Fi-
bonacci.
The problem of ﬁnding congruent numbers appears in an Arab manuscript
before 1000 CE. For example, 5 occurs as the area of the right triangle with sides 3/2,
20/3, 41/6.
7. Fermat used his method of inﬁnite descent to show that 1 is not a congruent number.
8. Under the assumption of the truth of the conjecture of Birch and Swinnerton-Dyer
(see §4.11.4, Fact 5) for the elliptic curve y2 = x3 −n2x, Jerrold Tunnell gave a simple
criterion for determining whether the integer n is a congruent number.
9. The congruent numbers less than 40 are 5, 6, 7, 13, 14, 15, 20, 21, 22, 23, 24, 28, 29,
30, 31, 34, 37, 38, 39.
10. Finding a rational right triangle of area n is equivalent to ﬁnding an x such that
x −n, x, and x + n are all squares of rational numbers.
11. Nathan Fine [Fi76] showed, implicitly using the theory of elliptic curves, that every
positive rational occurs as the area of some triangle (not necessarily a right triangle)
with rational sides.
12. Several applications of elliptic curves to recreational mathematics are given in
[Ma16].
4.11.2
ISOMORPHISMS, ENDOMORPHISMS, AND ISOGENIES
Deﬁnitions:
Let E1, E2 be elliptic curves deﬁned over F, where Ei is given by y2 = x3 + Aix + Bi for
i = 1, 2. A map φ : E1(F) →E2(F) is said to be given by rational functions (where
F is the algebraic closure of F) if there are rational functions f(x, y) and g(x, y) such
that φ(x, y) = (f(x, y), g(x, y)).
A map given by rational functions is deﬁned over a ﬁeld K if the coeﬃcients of the
rational functions f and g can be chosen to be elements of K.
An endomorphism of an elliptic curve is a homomorphism, given by rational functions,
from an elliptic curve to itself. The zero endomorphism maps every point to the point
∞.

Section 4.11
ELLIPTIC CURVES
315
An elliptic curve E has complex multiplication (CM) if there are endomorphisms of
E that are not given by multiplication by an integer.
Let E1, E2 be elliptic curves deﬁned over F. An isogeny is a homomorphism, given by
rational functions, from E1(F) to E2(F), where F is the algebraic closure of F.
Two elliptic curves Ei, i = 1, 2, given in Weierstrass form by y2
i = x3
i + Aixi + Bi, for
i = 1, 2, are isomorphic over a ﬁeld K (containing all Ai, Bi) if there is a constant
u ∈K such that the change of variables
x2 = u2x1,
y2 = u3y1
maps the points on E1 with coordinates in the algebraic closure of K to those on E2.
Two elliptic curves E1 and E2 deﬁned over a ﬁeld F are said to be twists of each other
if they are isomorphic over the algebraic closure of F.
Facts:
1. If a map is given by rational functions f and g, as in the deﬁnitions, then g2 =
f 3 + A2f + B2 for all x, y ∈F satisfying y2 = x3 + A1x + B1.
2. Let E be an elliptic curve and let n be an integer.
The map P 7→nP is given
by rational functions (for n = 2, see the addition formula (§4.11.1) applied to the case
P1 = P2).
3. Two elliptic curves deﬁned over a ﬁeld F are isomorphic over the algebraic closure of
F if and only if they have the same j-invariant.
4. Let E be deﬁned over F. The endomorphisms of E deﬁned over the algebraic closure
of F form a ring (under addition and composition of functions).
5. If the characteristic of F is 0, this ring of endomorphisms is isomorphic either to Z
or to a subring of the ring of algebraic integers (see §4.10.2) in an imaginary quadratic
ﬁeld.
6. If F is a ﬁnite ﬁeld of characteristic p, this ring of endomorphisms is isomorphic either
to a subring of the ring of algebraic integers (see §4.10.2) in an imaginary quadratic ﬁeld
or to a noncommutative ring (in technical terms: a maximal order in a quaternion algebra
over Q that is ramiﬁed only at p and ∞).
7. A noncommutative endomorphism ring occurs if and only if E is supersingular.
8. The most common situation where E1 and E2 are twists of each other is when Ei is
deﬁned by y2 = x3 + Aix + Bi and there is a nonzero d ∈F such that A2 = d2A1 and
B2 = d3B1, in which case the isomorphism is given by (x, y) 7→(dx, d3/2y).
Examples:
1. The elliptic curves y2
1 = x3
1 +3x1 +7 and y2
2 = x3
2 +12x2 +56 are twists of each other.
They both have the j-invariant 6912/53. The change of variables x2 = 2x1, y2 = 23/2y1
transforms one curve to the other.
2. Let E be the elliptic curve y2 = x3 + x deﬁned over C. The map φ : (x, y) 7→(−x, iy)
gives an endomorphism of E that is not multiplication by an integer, so E has complex
multiplication. It is easy to see that φ4 is the identity. The ring Z[i], with i corresponding
to φ, is the endomorphism ring of E.
3. Let E1 : y2
1 = x3
1 + ax2
1 + bx1 be an elliptic curve over a ﬁeld of characteristic not 2.
We require b ̸= 0 and a2 −4b ̸= 0 in order to have 4A3 + 27B2 ̸= 0 when the curve is

316
Chapter 4
NUMBER THEORY
changed to Weierstrass form. Let E2 be the elliptic curve y2
2 = x3
2 −2ax2
2 + (a2 −4b)x2.
Deﬁne α by
(x2, y2) = α(x1, y1) =

y2
1
x2
1 , y1(x2
1−b)
x2
1

.
Then α is an isogeny from E1 to E2. The kernel of α is {∞, (0, 0)}.
4.11.3
ELLIPTIC CURVES OVER FINITE FIELDS
Deﬁnitions:
Given a point P on an elliptic curve and multiples nP and mP, where m, n are integers
that are not revealed, the computational Diﬃe-Hellman problem is that of ﬁnding
mnP.
Given two points P and Q on an elliptic curve such that Q = nP for some integer n, the
discrete logarithm problem is that of ﬁnding an integer k such that Q = kP.
If E is an elliptic curve deﬁned over a ﬁnite ﬁeld GF(q), the Frobenius map φq maps
a point (x, y) (with x, y in the algebraic closure of GF(q)) to the point (xq, yq).
A supersingular elliptic curve is an elliptic curve deﬁned over a ﬁeld F of character-
istic p that has no points of order p with coordinates in the algebraic closure of F.
Let A, B, C, be groups, and let ∗denote their group operations. A bilinear pairing is
a map
A × B →C,
(a, b) 7→c = ⟨a, b⟩,
such that
⟨a1 ∗a2, b⟩= ⟨a1, b⟩∗⟨a2, b⟩,
⟨a, b1 ∗b2⟩= ⟨a, b1⟩∗⟨a, b2⟩
for all a, a1, a2 ∈A and all b, b1, b2 ∈B. Two common pairings are the Tate-Lichtenbaum
pairing and the Weil pairing (see Fact 7).
Facts:
1. The set of points E(GF(q)) forms a ﬁnite group. It has the structure
Z/mZ
or
Z/mZ × Z/nZ
for some integers m, n.
2. Hasse’s theorem:
Let E be an elliptic curve deﬁned over GF(q) and let N be the
number of points on E with coordinates in GF(q). Then
q + 1 −2√q ≤N ≤q + 1 + 2√q.
3. Let E be an elliptic curve deﬁned over GF(q) and let a = #E(GF(q)). Write
X2 −aX + q = (X −α)(X −β).
Then the number of points on E with coordinates in GF(qn) is
qn + 1 −αn −βn.
4. The Frobenius map φq of an elliptic curve E deﬁned over GF(q) is an endomorphism
of E, and φ2
q −aφq + q is the zero endomorphism, where a = q + 1 −#E(GF(q)).

Section 4.11
ELLIPTIC CURVES
317
5. There is an eﬃcient algorithm due to Schoof-Elkies-Atkin that counts the number of
points on an elliptic curve over a ﬁnite ﬁeld.
6. An elliptic curve over a ﬁnite ﬁeld F of characteristic p is supersingular if and only
if the order of E(F) is congruent to 1 modulo p.
7. Let E be an elliptic curve deﬁned over a ﬁnite ﬁeld F = GF(q) and let n be a positive
integer such that q−1 is a multiple of n. Let E[n] be the set of points P with coordinates
in the algebraic closure of F such that nP = ∞. The Tate-Lichtenbaum pairing is a
nondegenerate bilinear pairing
τn : E[n] × (E(F)/nE(F)) →µn,
where µn is the group of nth roots of unity in F. The Weil pairing is a nondegenerate
bilinear pairing
en : E[n] × E[n] →µn.
8. The Weil pairing and the Tate-Lichtenbaum pairing can be computed eﬃciently by
Miller’s algorithm [Mi04].
9. The Tate-Lichtenbaum pairing (and the Weil pairing) can be used to attack the
discrete logarithm problem. Let E be deﬁned over GF(q). Suppose that P, Q are points
of order ℓon E and kP = Q. Let qn be chosen so that ℓ| qn −1 (the smallest such n > 0
is called the embedding degree). Choose a random point R ∈E(GF(qn)) and compute
α = τℓ(P, R) and β = τℓ(Q, R). Then β = αk, so we have a discrete logarithm problem
in GF(qn). If n is small, it is easier to attack this discrete logarithm problem (for ﬁelds)
than to use methods that work on arbitrary elliptic curves.
Examples:
1. Let E be the elliptic curve y2 = x3 + 4x + 1 deﬁned over GF(5). The points in
E(GF(5)) are ∞, (0, 1), (0, 4), (1, 1), (1, 4), (3, 0), (4, 1), (4, 4).
The addition law yields
(0, 1) + (1, 4) = (3, 0) and 2(1, 1) = (4, 1). There are eight points, so a = 5 + 1 −8 = −2.
The polynomial X2 + 2X + 5 factors as (X + 1 −2i)(X + 1 + 2i). Therefore (see Fact
3), the number of points in E(GF(5n)) is 5n + 1 −(−1 + 2i)n −(−1 −2i)n. The group
E(GF(5)) is cyclic of order 8.
2. Let E be the elliptic curve y2 = x3 + 4x + 1 deﬁned over GF(5). An example of the
discrete logarithm problem is to ﬁnd k such that k(0, 1) = (1, 1). The answer in this case
is k = 5.
3. There is a list of 15 elliptic curves recommended by NIST (see [FIPS186-3]).
4. The elliptic curve E deﬁned by y2 ≡x3 + 1 (mod 5) has six points with coordinates
in Z/5Z. Therefore, it is supersingular. Let ω be a primitive cube root of unity in the
algebraic closure of GF(5). Then α : (x, y) 7→(ωx, y) is an endomorphism of E. Let
φ = φ5 be the Frobenius map. Then φα(x, y) = (ω2x5, y5), but αφ(x, y) = (ωx5, y5).
Therefore, the ring of endomorphisms of E is noncommutative.
4.11.4
ELLIPTIC CURVES OVER Q
Deﬁnition:
Let E be an elliptic curve given in generalized Weierstrass form with integer coeﬃcients.
For each prime p, look at the equation for E modulo p. For all primes not in a ﬁnite

318
Chapter 4
NUMBER THEORY
set SE, this yields a nonsingular curve modulo p. Let ap = p + 1 −#E(GF(p)). The
L-series for E is
L(E, s) =
Y
p∈SE
(Lp(s))−1 Y
p̸∈SE
 1 −app−s + p1−2s−1 =
∞
X
n=1
ann−s,
where the ﬁnitely many factors Lp(s) depend on the properties of E modulo p, and where
s is a complex variable with R(s) > 3/2.
Facts:
1. Mordell-Weil theorem: If F is a ﬁnite extension of Q and E is deﬁned over F, then
E(F) is a ﬁnitely generated abelian group, which means that E(F) ≃Zr ⊕T , where T
is a ﬁnite group. The integer r is called the Mordell-Weil rank of E(F).
2. In 2006, Noam Elkies discovered an elliptic curve deﬁned over Q whose Mordell-Weil
rank is at least 28. It has been conjectured that there are elliptic curves of arbitrarily
large Mordell-Weil rank. On the other hand, Andrew Granville and Bjorn Poonen have
each produced heuristic arguments that indicate that there should be an upper bound
on the rank of elliptic curves.
3. Manjul Barghava and Arul Shankar have proved that the average rank of elliptic
curves deﬁned over Q is at most 7/6.
4. If E is an elliptic curve deﬁned over Q, the L-series of E has a continuation to
an analytic function deﬁned in the whole complex plane. It has a functional equation
relating L(E, s) and L(E, 2 −s). This was conjectured by Taniyama, Shimura, and Weil
and was proved by Wiles, Taylor, Breuil, Conrad, and Diamond.
5. Conjecture of Birch and Swinnerton-Dyer:
Let E be deﬁned over Q. The analytic
rank of E is the order of vanishing of L(E, s) at s = 1 (that is, the degree of the ﬁrst
nonzero term in the Taylor series at s = 1). It is conjectured that the analytic rank of E
equals the Mordell-Weil rank of E(Q). This is known to be true when the analytic rank
is 0 or 1 by work of Victor Kolyvagin and others.
6. Lutz-Nagell theorem: Let E be deﬁned over Q by the equation y2 = x3 + Ax + B
with A, B ∈Z. Let ∞̸= (x, y) ∈E(Q) be a torsion point. Then x, y ∈Z, and either
y = 0 or y2 | 4A3 + 27B2.
7. Mazur’s theorem: Let E be deﬁned over Q. The torsion subgroup of E(Q) has one
of the following forms:
Z/nZ for 1 ≤n ≤10 or n = 12;
Z/nZ × Z/2Z for 1 ≤n ≤4.
Examples:
1. Let E be deﬁned by y2 = x3 + 1. Then 4A3 + 27B2 = 27. Suppose (x, y) is a torsion
point with y ̸= 0. The Lutz-Nagell theorem says that y2 | 27, so y = ±1 or ±3. This
yields the points (0, ±1) and (2, ±3). It can be checked that 3(0, −1) = 3(0, 1) = ∞and
6(2, 3) = 6(2, −3) = ∞, so these are torsion points. The only point with y = 0 is (−1, 0).
Therefore, the torsion subgroup of E(Q) has order 6 and is
{∞, (−1, 0), (0, −1), (0, 1), (2, −3), (2, 3)}.
2. Let E be deﬁned by y2 = x3 −16x + 16. The Lutz-Nagell theorem (Fact 6) implies
that if (x, y) is a torsion point with y ̸= 0, then y2 | 28 · 37, so y = ±2j with 0 ≤j ≤4.
These do not yield torsion points, and neither does y = 0, so the torsion subgroup of
E(Q) is trivial. Therefore, E(Q) ≃Zr for some r. In fact, r = 1 and the points with

REFERENCES
319
rational coordinates are the multiples of the point P = (0, 4). The ﬁrst few multiples of
P are
2P = (4, 4),
3P = (−4, −4),
4P = (8, −20),
5P = (1, −1),
6P = (24, 116),
7P = (−20/9, 172/27),
8P = (84/25, −52/125).
4.11.5
ELLIPTIC CURVES OVER C
Deﬁnition:
Given a lattice L = Zω1 + Zω2 in C, the Weierstrass ℘-function is given by
℘(z) = ℘(z; L) =
1
z2 + P
ω∈L
ω̸=0

1
(z−ω)2 −
1
ω2

.
Facts:
1. If E is deﬁned over the complex numbers C, then E(C) is isomorphic to C/L, where
L is a lattice in C (that is, L is an additive subgroup of C of the form Zω1 + Zω2, where
ω1, ω2 are complex numbers that are linearly independent over the real numbers).
2. The Weierstrass ℘-function is meromorphic in C, has a double pole at each ω ∈L,
and is doubly periodic: ℘(z + ω1) = ℘(z + ω2) = ℘(z) for all z ∈C.
3. Given a lattice L in C, there are constants g2, g3 such that the associated Weierstrass
℘-function satisﬁes
℘′(z)2 = 4℘(z)3 −g2℘(z) −g3
for all z ∈C. This means that there is a map
z 7→(℘(z), 1
2℘′(z))
from C/L to E(C), where E is the elliptic curve deﬁned over C by y2 = x3 −(g2/4)x −
(g3/4). This is an isomorphism of groups.
4. When an elliptic curve with complex multiplication is represented over the complex
numbers as C/L for a lattice L, then the complex multiplication is given by multiplication
by (nonreal) complex numbers α such that αL ⊆L.
REFERENCES
Printed Resources:
[AlWi04] S. Alaca and K. S. Williams, Introductory Algebraic Number Theory, Cam-
bridge University Press, 2004.
[An98] G. E. Andrews, The Theory of Partitions, Encyclopedia of Mathematics and Its
Applications, Vol. 2, Cambridge University Press, 1998.
[Ap76] T. M. Apostol, Introduction to Analytic Number Theory, Springer-Verlag, 1976.
[BaSh96] E. Bach and J. Shallit, Algorithmic Number Theory, Volume 1, Eﬃcient Algo-
rithms, MIT Press, 1996.
[Ba90] A. Baker, Transcendental Number Theory, Cambridge University Press, 1990.

320
Chapter 4
NUMBER THEORY
[Br89] D. M. Bressoud, Factorization and Primality Testing, Springer-Verlag, 1989.
[BrEtal88] J. Brillhart, D. H. Lehmer, J. L. Selfridge, B. Tuckerman, and S. S. Wagstaﬀ, Jr.,
Factorizations of bn ± 1, b = 2, 3, 5, 6, 7, 10, 11, 12 up to High Powers, 2nd ed., Amer-
ican Mathematical Society, 1988.
[Ca57] J. W. S. Cassels, An Introduction to Diophantine Approximation, Cambridge
University Press, 1957.
[Ch02] M-L. Chang, “Non-monogeneity in a family of sextic ﬁelds”, Journal of Number
Theory 97 (2002), 252–268.
[Co93] H. Cohen, A Course in Computational Algebraic Number Theory, Springer-Verlag,
1993.
[CoLe84] H. Cohen and H. W. Lenstra, Jr., “Heuristics on class groups of number
ﬁelds”, Number theory, Noordwijkerhout 1983, Lecture Notes in Mathematics 1068,
Springer, 1984, 33–62.
[Co92] I. Connell, “Addendum to a paper of Harada and Lang”, Journal of Algebra 145
(1992), 463–467.
[CrPo05] R. E. Crandall and C. Pomerance, Primes: A Computational Perspective, 2nd
ed., Springer-Verlag, 2005.
[Da13] A. Das, Computational Number Theory, CRC Press, 2013.
[Di71] L. E. Dickson, History of the Theory of Numbers, Chelsea Publishing Company,
1971.
[Fi76] N. Fine, “On rational triangles”, American Mathematical Monthly 83 (1976),
517–521.
[FIPS186-3] “Digital Signature Standard (DSS)”, Federal Information Processing Stan-
dards Publication 186-3, U.S. Department of Commerce/National Institute of Stan-
dards and Technology, June 2009.
[Gr86] M-N. Gras, “Non monog´en´eit´e de l’anneau des entiers des extensions cycliques de
Q de degr´e premier ℓ≥5”, Journal of Number Theory 23 (1986), 347–353.
[GuMu84] R. Gupta and M. R. Murty, “A remark on Artin’s Conjecture”, Inventiones
Mathematicae 78 (1984), 127–130.
[Gu94] R. K. Guy, Unsolved Problems in Number Theory, 2nd ed., Springer-Verlag, 1994.
[HaWr08] G. H. Hardy and E. M. Wright, An Introduction to the Theory of Numbers,
6th ed., Oxford University Press, 2008.
[IrRo98] K. Ireland and M. Rosen, A Classical Introduction to Modern Number Theory,
2nd ed., Springer, 1998.
[Kn97] D. E. Knuth, The Art of Computer Programming, Volume 2: Seminumerical
Algorithms, 3rd ed., Addison-Wesley, 1997.
[Ko93] I. Koren, Computer Arithmetic Algorithms, Prentice Hall, 1993.
[La00] S. Lang Algebraic Number Theory, 2nd ed., Springer, 2000.
[Ma16] A. J. MacLeod, “Elliptic curves in recreational number theory”, 2016; available
at https://arxiv.org/pdf/1610.03430v1.pdf.
[Ma77] B. Mazur, “Book Review: Ernst Edward Kummer, Collected Papers”, Bulletin
of the American Mathematical Society 83 (1977), 976–988.

REFERENCES
321
[MeVoVa96] A. J. Menezes, P. C. van Oorschot, and S. A Vanstone, Handbook of Applied
Cryptography, CRC Press, 1996.
[Mi04] V. Miller, “The Weil pairing and its eﬃcient calculation”, Journal of Cryptology
17 (2004), 235–161.
[Mo69] L. J. Mordell, Diophantine Equations, Academic Press, 1969.
[Mu11] R. A. Mullin, Algebraic Number Theory, 2nd ed., CRC Press, 2011.
[NiZuMo91] I. Niven, H. S. Zuckerman, and H. L. Montgomery, An Introduction to the
Theory of Numbers, 5th ed., Wiley, 1991.
[Pe54] O. Perron, Die Lehre von den Kettenbr¨uchen, 3rd ed., Teubner Verlagsgesellschaft,
1954.
[Po90] C. Pomerance, ed., Cryptology and Computational Number Theory, Proceedings
of Symposia in Applied Mathematics 42, American Mathematical Society, 1990.
[Po94] C. Pomerance, “The number ﬁeld sieve”, in Mathematics of Computation 1943–
1993: A Half-Century of Computational Mathematics, W. Gautschi (ed.), Proceed-
ings of Symposia in Applied Mathematics 48, American Mathematical Society, 1994,
465–480.
[Po10] C. Pomerance, “Primality testing: variations on a theme of Lucas”, Congressus
Numerantium 201 (2010), 301-312.
[Ri96] P. Ribenboim, The New Book of Prime Number Records, Springer-Verlag, 1996.
[Ro10] K. H. Rosen, Elementary Number Theory and Its Applications, 6th ed., Pearson,
2010.
[Sc80] W. M. Schmidt, Diophantine Approximation, Lecture Notes in Mathematics 785,
Springer, 1980.
[Sc85] N. R. Scott, Computer Number Systems and Arithmetic, Prentice Hall, 1985.
[Si09] J. H. Silverman, The Arithmetic of Elliptic Curves, 2nd ed., Springer, 2009.
[SiTa15] J. H. Silverman and J. T. Tate, Rational Points on Elliptic Curves, 2nd ed.,
Springer, 2015.
[Si97] S. Singh, The Quest to Solve the World’s Greatest Mathematical Problem, Walker
& Co., 1997.
[StTa16] I. Stewart and D. Tall, Algebraic Number Theory and Fermat’s Last Theorem,
4th ed., CRC Press, 2016.
[Wa13] S. S. Wagstaﬀ, Jr., Joy of Factoring, American Mathematical Society, 2013.
[Wa97] L. Washington, Introduction to Cyclotomic Fields, 2nd ed., Springer, 1997.
[Wa08] L. Washington, Elliptic Curves: Number Theory and Cryptography, 2nd ed.,
Chapman & Hall/CRC Press, 2008.
[Wi95] A. J. Wiles, “Modular elliptic curves and Fermat’s last theorem”, Annals of
Mathematics 141 (1995), 443–551.
Web Resources:
http://www-groups.dcs.st-and.ac.uk/~history/HistTopics/Fermat’s last theorem.
html (History of Fermat’s last theorem.)

322
Chapter 4
NUMBER THEORY
http://www.jmilne.org/math/CourseNotes/ant.html (J. S. Milne, “Algebraic num-
ber theory” course notes.)
http://www.mersenne.org (The Great Internet Mersenne Prime Search webpage.)
http://www.numbertheory.org/ntw/number theory.html (The Number Theory web-
page.)
http://www.pbs.org/wgbh/nova/proof (NOVA Online: Proof of Fermat’s last theo-
rem.)
http://www.scienceandreason.net/flt/flt01.htm
(The mathematics of Fermat’s
last theorem.)
http://www.utm.edu/research/primes (Prime number research, records, and resources.)
https://homes.cerias.purdue.edu/~ssw/cun/index.html (The Cunningham Project
website.)

5
ALGEBRAIC STRUCTURES
John G. Michaels
5.1 Algebraic Models
5.1.1 Domains and Operations
5.1.2 Semigroups and Monoids
5.2 Groups
5.2.1 Basic Concepts
5.2.2 Group Isomorphism and Homomorphism
5.2.3 Subgroups
5.2.4 Cosets and Quotient Groups
5.2.5 Cyclic Groups and Order
5.2.6 Sylow Theory
5.2.7 Simple Groups
5.2.8 Group Presentations
5.3 Permutation Groups
5.3.1 Basic Concepts
5.3.2 Examples of Permutation Groups
5.4 Rings
5.4.1 Basic Concepts
5.4.2 Subrings and Ideals
5.4.3 Ring Homomorphism and Isomorphism
5.4.4 Quotient Rings
5.4.5 Rings with Additional Properties
5.5 Polynomial Rings
5.5.1 Basic Concepts
5.5.2 Polynomials over a Field
5.6 Fields
5.6.1 Basic Concepts
5.6.2 Extension Fields and Galois Theory
5.6.3 Finite Fields
5.7 Lattices
5.7.1 Basic Concepts
5.7.2 Specialized Lattices
5.8 Boolean Algebras
5.8.1 Basic Concepts
5.8.2 Boolean Functions
5.8.3 Logic Gates
5.8.4 Minimization of Circuits

324
Chapter 5
ALGEBRAIC STRUCTURES
INTRODUCTION
Many of the most common mathematical systems, including the integers, the rational
numbers, and the real numbers, have an underlying algebraic structure. This chapter
examines the structure and properties of various types of algebraic objects. These objects
arise in a variety of settings and occur in many diﬀerent applications, including counting
techniques, coding theory, information theory, engineering, and circuit design.
GLOSSARY
abelian group: a group in which a ⋆b = b ⋆a for all a, b in the group.
absorption laws: in a lattice a ∨(a ∧b) = a and a ∧(a ∨b) = a.
algebraic element (over a ﬁeld): given a ﬁeld F, an element α ∈K (extension of F)
such that there exists p(x) ∈F[x] (p(x) ̸= 0) such that p(α) = 0. Otherwise α is
transcendental over F.
algebraic extension (of a ﬁeld): given a ﬁeld F, a ﬁeld K such that F is a subﬁeld of
K and all elements of K are algebraic over F.
algebraic integer: an algebraic number that is a zero of a monic polynomial with
coeﬃcients in Z.
algebraic number: a complex number that is algebraic over Q.
algebraic structure: (S, ⋆1, ⋆2, . . . , ⋆n) where S is a nonempty set and ⋆1, . . . , ⋆n are
binary or monadic operations deﬁned on S.
alternating group (on n elements): the subgroup An of all even permutations in Sn.
associative property: the property of a binary operator ⋆that (a ⋆b) ⋆c = a ⋆(b ⋆c).
atom: an element a in a bounded lattice such that 0 < a and there is no element b such
that 0 < b < a.
automorphism: an isomorphism of an algebraic structure onto itself.
automorphism ϕ ﬁxes set S elementwise: ϕ(a) = a for all a ∈S.
binary operation (on a set S): a function ⋆: S × S →S.
Boolean algebra: a bounded, distributive, complemented lattice. Equivalent deﬁni-
tion: (B, +, ·,′ , 0, 1) where B is a set with two binary operations, + (addition) and ·
(multiplication), one monadic operation, ′ (complement), and two distinct elements,
0 and 1, that satisfy the commutative laws (a + b = b + a, a · b = b · a), distributive
laws (a·(b+ c) = (a·b)+ (a·c), a+ (b·c) = (a+ b)·(a+ c)), identity laws (a+ 0 = a,
a · 1 = a), and complement laws (a + a′ = 1, a · a′ = 0).
Boolean function of degree n: a function f : {0, 1}n = {0, 1} × · · ·× {0, 1} →{0, 1}.
bounded lattice: a lattice having elements 0 (lower bound) and 1 (upper bound)
such that 0 ≤a and a ≤1 for all a.
cancellation properties: if ab = ac and a ̸= 0, then b = c (left cancellation prop-
erty); if ba = ca and a ̸= 0, then b = c (right cancellation property).
characteristic (of a ﬁeld): the smallest positive integer n such that 1+1+· · ·+1 = 0 (n
summands). If no such n exists, the ﬁeld has characteristic 0 (or characteristic ∞).

GLOSSARY
325
closure property: a set S is closed under an operation ⋆if the range of ⋆is a subset
of S.
commutative property: the property of an operation ⋆that a ⋆b = b ⋆a.
commutative ring: a ring in which multiplication is commutative.
complemented lattice: a bounded lattice such that for each element a there is an
element b such that a ∨b = 1 and a ∧b = 0.
conjunctive normal form (CNF) (of a Boolean function): a Boolean function writ-
ten as a product of maxterms.
coset: For a subgroup H of group G and a ∈G, a left coset is aH = {ah | h ∈H}; a
right coset is Ha = { ha | h ∈H }.
cycle of length n: a permutation on a set S that moves elements only in a single orbit
of size n.
cyclic group: a group G with an element a ∈G such that G = { an | n ∈Z }.
cyclic subgroup (generated by a): { an | n ∈Z } = {. . . , a−2, a−1, e, a, a2, . . .}, often
written (a), ⟨a⟩, or [a]. The element a is a generator of the subgroup.
degree (of ﬁeld K over ﬁeld F): [K : F] = the dimension of K as a vector space over F.
degree (of a permutation group): the size of the set on which the permutations are
deﬁned.
dihedral group: the group Dn of symmetries (rotations and reﬂections) of a regular
n-gon.
disjunctive normal form (DNF) (of a Boolean function): a Boolean function writ-
ten as a sum of minterms.
distributive lattice: a lattice that satisﬁes a∧(b∨c) = (a∧b)∨(a∧c) and a∨(b∧c) =
(a ∨b) ∧(a ∨c) for all a, b, c in the lattice.
division ring: a nontrivial ring in which every nonzero element is a unit.
dual (of an expression in a Boolean algebra): the expression obtained by interchanging
the operations + and ·, and interchanging the elements 0 and 1, in the original
expression.
duality principle: the principle stating that an identity between Boolean expressions
remains valid when the duals of the expressions are taken.
Euclidean domain: an integral domain with a Euclidean norm deﬁned on it.
Euclidean norm (on an integral domain): given an integral domain I, a function δ: I−
{0} →N such that for all a, b ∈I, δ(a) ≤δ(ab); and for all a, d ∈I (d ̸= 0) there
are q, r ∈I such that a = dq + r, where either r = 0 or δ(r) < δ(d).
even permutation: a permutation that can be written as a product of an even number
of transpositions.
extension ﬁeld (of ﬁeld F): a ﬁeld K such that F is a subﬁeld of K.
ﬁeld: an algebraic structure (F, +, ·) where F is a set closed under two binary operations
+ and ·, (F, +) is an abelian group, the nonzero elements form an abelian group under
multiplication, and the distributive law a · (b + c) = a · b + a · c holds.
ﬁnite ﬁeld: a ﬁeld with a ﬁnite number of elements.
ﬁnitely generated group: a group with a ﬁnite set of generators.

326
Chapter 5
ALGEBRAIC STRUCTURES
ﬁxed ﬁeld (of a set of automorphisms of a ﬁeld): given a set Φ of automorphisms of a
ﬁeld F, the set { a ∈F | aϕ = a for all ϕ ∈Φ }.
free monoid (generated by a set): given a set S, the monoid consisting of all words on
S under concatenation.
functionally complete: property of a set of operators in a Boolean algebra that every
Boolean function can be written using only these operators.
Galois extension (of a ﬁeld F): a ﬁeld K that is a normal, separable extension of F.
Galois ﬁeld: GF(pn) = the algebraic extension Zp[x]/(f(x)) of the ﬁnite ﬁeld Zp where
p is a prime and f(x) is an irreducible polynomial over Zp of degree n.
Galois group (of K over F): the group of automorphisms G(K/F) of ﬁeld K that ﬁx
ﬁeld F elementwise.
group: an algebraic structure (G, ⋆), where G is a set closed under the binary opera-
tion ⋆, the operation ⋆is associative, G has an identity element, and every element
of G has an inverse in G.
homomorphism of groups: a function ϕ: S →T , where (S, ⋆1) and (T, ⋆2) are groups,
such that ϕ(a ⋆1 b) = ϕ(a) ⋆2 ϕ(b) for all a, b ∈S.
homomorphism of rings: a function ϕ: S →T , where (S, +1, ·1) and (T, +2, ·2) are
rings such that ϕ(a +1 b) = ϕ(a) +2 ϕ(b) and ϕ(a ·1 b) = ϕ(a) ·2 ϕ(b) for all a, b ∈S.
ideal: a subring of a ring that is closed under left and right multiplication by elements
of the ring.
identity: an element e in an algebraic structure S such that e ⋆a = a ⋆e = a for all
a ∈S.
improper subgroups (of G): the subgroups G and {e}.
index of H in G: the number of left (or right) cosets of H in G.
integral domain: a commutative ring with unity that has no zero divisors.
inverse of an element a: an element a′ such that a ⋆a′ = a′ ⋆a = e.
involution: a function that is the identity when it is composed with itself.
irreducible element in a ring: a noninvertible element that cannot be written as the
product of noninvertible elements.
irreducible polynomial: a polynomial p(x) of degree n > 0 over a ﬁeld that cannot
be written as p1(x) · p2(x) where p1(x) and p2(x) are polynomials of smaller degrees.
Otherwise p(x) is reducible.
isomorphic: property of algebraic structures of the same type, G and H, that there is
an isomorphism from G onto H, written G ∼= H.
isomorphism: a one-to-one and onto function between two algebraic structures that
preserves the operations on the structures.
isomorphism of groups: for groups (G1, ⋆1) and (G2, ⋆2), a function ϕ: G1 →G2
that is one-to-one, onto G2, and satisﬁes the property ϕ(a ⋆1 b) = ϕ(a) ⋆2 ϕ(b).
isomorphism of permutation groups: for permutation groups (G, X) and (H, Y ), a
pair of functions (α: G→H, f : Y →Y ) such that α is a group isomorphism and f is
a bijection.

GLOSSARY
327
isomorphism of rings: for rings (R1, +1, ·1) and (R2, +2, ·2), a function ϕ: R1 →R2
that is one-to-one, onto R2, and satisﬁes the properties ϕ(a+1 b) = ϕ(a)+2 ϕ(b) and
ϕ(a ·1 b) = ϕ(a) ·2 ϕ(b).
kernel (of a group homomorphism): given a group homomorphism ϕ, the set ϕ−1(e) =
{ x | ϕ(x) = e }, where e is the group identity.
kernel (of a ring homomorphism): given a ring homomorphism ϕ, the set ϕ−1(0) =
{ x | ϕ(x) = 0 }.
Klein four-group: the group under composition of the four rigid motions of a rectangle
that leave the rectangle in its original location.
lattice: a nonempty partially ordered set in which inf{a, b} and sup{a, b} exist for all
a, b. (a ∨b = sup{a, b}, a ∧b = inf{a, b}.) Equivalently, a nonempty set closed under
two binary operations ∨and ∧that satisfy the associative laws, the commutative
laws, and the absorption laws (a ∨(a ∧b) = a, a ∧(a ∨b) = a).
left divisor of zero: a ̸= 0 with b ̸= 0 such that ab = 0.
literal: a Boolean variable or its complement.
maximal ideal: an ideal in a ring R that is not properly contained in any ideal of R
except R itself.
maxterm of the Boolean variables x1, . . . , xn: a sum of the form y1+· · ·+yn where
for each i, yi is equal to xi or x′
i.
minimal polynomial (of an element with respect to a ﬁeld): given a ﬁeld F and α ∈
F, the monic irreducible polynomial f(x) ∈F[x] of smallest degree with f(α) = 0.
minterm of the Boolean variables x1, . . . , xn: a product of the form y1 . . . yn where
for each i, yi is equal to xi or x′
i.
monadic operation: a function from a set into itself.
monoid: an algebraic structure (S, ⋆) such that ⋆is associative and S has an identity.
normal extension of F: a ﬁeld K such that K/F is algebraic and every irreducible
polynomial in F[x] with a root in K has all its roots in K (splits in K).
normal subgroup (of a group): given a group G, a subgroup H ⊆G such that aH =
Ha for all a ∈G.
octic group: See dihedral group.
odd permutation: a permutation that can be written as a product of an odd number
of transpositions.
orbit (of an object a ∈S under permutation σ): {. . . , aσ−2, aσ−1, a, aσ, aσ2, . . .}.
order (of an algebraic structure): the number of elements in the underlying set.
order (of a group element): for an element a ∈G, the smallest positive integer n such
that an = e (na = 0 if G is written additively). If there is no such integer, then a
has inﬁnite order.
p-group: for prime p, a group such that every element has a power of p as its order.
permutation: a one-to-one and onto function σ: S →S, where S is any nonempty set.
permutation group: a collection of permutations on a set of objects that form a group
under composition.

328
Chapter 5
ALGEBRAIC STRUCTURES
polynomial (in the variable x over a ring): an expression of the form p(x) = anxn +
an−1xn−1 + · · · + a1x1 + a0x0 where an, . . . , a0 are elements of the ring.
For a
polynomial p(x), the largest integer k such that ak ̸= 0 is the degree of p(x). The
constant polynomial p(x) = a0 has degree 0, if a0 ̸= 0. If p(x) = 0 (the zero
polynomial), the degree of p(x) is undeﬁned (or −∞).
polynomial ring (over a ring R): R[x] = {p(x) | p(x) is a polynomial in x over R}
with the usual deﬁnitions of addition and multiplication.
prime ideal (of a ring R): an ideal I ̸= R with property that ab ∈I implies that a ∈I
or b ∈I.
proper subgroup (of a group G): any subgroup of G except G and {e}.
quotient group (factor group): for a normal subgroup H of G, the group G/H =
{ aH | a ∈G }, where aH · bH = (ab)H.
quotient ring: for I an ideal in a ring R, the ring R/I = { a + I | a ∈R }, where
(a + I) + (b + I) = (a + b) + I and (a + I) · (b + I) = (ab) + I.
reducible (polynomial): a polynomial that is not irreducible.
right divisor of zero: b ̸= 0 with a ̸= 0 such that ab = 0.
ring: an algebraic structure (R, +, ·) where R is a set closed under two binary operations
+ and · , (R, +) is an abelian group, R satisﬁes the associative law for multiplication,
and R satisﬁes the left and right distributive laws for multiplication over addition.
ring with unity: a ring with an identity for multiplication.
root ﬁeld: a splitting ﬁeld.
semigroup: an algebraic structure (S, ⋆) where S is a nonempty set that is closed under
the associative binary operation ⋆.
separable extension (of ﬁeld F): a ﬁeld K such that every element of K is the root
of a separable polynomial in F[x].
separable polynomial: a polynomial p(x) ∈F[x] of degree n that has n distinct roots
in its splitting ﬁeld.
sign (of a permutation): the value +1 if the permutation has an even number of trans-
positions when the permutation is written as a product of transpositions, and −1
otherwise.
simple group: a group whose only normal subgroups are {e} and G.
skew ﬁeld: a division ring.
splitting ﬁeld (for nonconstant p(x) ∈F[x]): the ﬁeld K = F(α1, . . . , αn) where p(x) =
α(x −α1) . . . (x −αn), α ∈F.
subﬁeld (of a ﬁeld K): a subset F ⊆K that is a ﬁeld using the same operations used
in K.
subgroup (of a group G): a subset H ⊆G such that H is a group using the same group
operation used in G.
subgroup generated by { ai | i ∈S }: for a given group G where ai ∈G for all i in S,
the smallest subgroup of G containing { ai | i ∈S }.
subring (of a ring R): a subset S ⊆R that is a ring using the same operations used
in R.

Section 5.1
ALGEBRAIC MODELS
329
Sylow p-subgroup (of G): a subgroup of G that is a p-group and is not properly con-
tained in any p-group of G.
symmetric group: the group of all permutations on {1, 2, . . ., n} under the operation
of composition.
transcendental element (over a ﬁeld F): given a ﬁeld F and an extension ﬁeld K, an
element of K that is not a root of any nonzero polynomial in F[x].
transposition: a cycle of length 2.
unary operation: See monadic operation.
unit (in a ring): an element with a multiplicative inverse in the ring.
unity (in a ring): a multiplicative identity not equal to 0.
word (on a set): a ﬁnite sequence of elements of the set.
zero (of a polynomial f): an element a such that f(a) = 0.
5.1
ALGEBRAIC MODELS
5.1.1
DOMAINS AND OPERATIONS
Deﬁnitions:
An n-ary operation on a set S is a function ⋆: S × S × · · · × S →S, where the domain
is the product of n factors.
A binary operation on a set S is a function ⋆: S × S →S.
A monadic operation (or unary operation) on a set S is a function ⋆: S →S.
An algebraic structure (S, ⋆1, ⋆2, . . . , ⋆n) consists of a nonempty set S (the domain)
with one or more n-ary operations ⋆i deﬁned on S.
A binary operation can have some of the following properties:
• associative property:
a ⋆(b ⋆c) = (a ⋆b) ⋆c for all a, b, c ∈S;
• existence of an identity element:
there is an element e ∈S such that e⋆a =
a ⋆e = a for all a ∈S (e is an identity for S);
• existence of inverses:
for each element a ∈S there is an element a′ ∈S such
that a′ ⋆a = a ⋆a′ = e (a′ is an inverse of a);
• commutative property:
a ⋆b = b ⋆a for all a, b ∈S.
Example:
1. The most important types of algebraic structures with one binary operation are listed
in the following table. A checkmark means that the property holds.

330
Chapter 5
ALGEBRAIC STRUCTURES
closed
associative
commutative
existence
existence
of identity
of inverses
semigroup
√
√
monoid
√
√
group
√
√
√
√
abelian group
√
√
√
√
√
5.1.2
SEMIGROUPS AND MONOIDS
Deﬁnitions:
A semigroup (S, ⋆) consists of a nonempty set S and an associative binary operation ⋆
on S.
A monoid (S, ⋆) consists of a nonempty set S and an associative binary operation ⋆on
S such that S has an identity.
A nonempty subset T of a semigroup (S, ⋆) is a subsemigroup of S if T is closed under ⋆.
A subset T of a monoid (S, ⋆) with identity e is a submonoid of S if T is closed under ⋆
and e ∈T .
Two semigroups [monoids] (S1, ⋆1) and (S2, ⋆2) are isomorphic if there is a function
ϕ: S1 →S2 that is one-to-one, onto S2, and such that ϕ(a ⋆1 b) = ϕ(a) ⋆2 ϕ(b) for
all a, b ∈S1.
A word on a set S (the alphabet) is a ﬁnite sequence of elements of S.
The free monoid [free semigroup] generated by S is the monoid [semigroup] (S∗, ⋆)
where S∗is the set of all words on a set S and the operation ⋆is deﬁned on S∗by
concatenation: x1x2 . . . xm ⋆y1y2 . . . yn = x1x2 . . . xmy1y2 . . . yn. (S∗, ⋆) is also called the
free monoid [free semigroup] on S∗.
Facts:
1. Every monoid is a semigroup.
2. Every semigroup (S, ⋆) is isomorphic to a subsemigroup of some semigroup of trans-
formations on some set.
Hence, every semigroup can be regarded as a semigroup of
transformations. An analogous result is true for monoids.
Examples:
1. Free semigroups and monoids: The free monoid generated by S is a monoid with the
empty word e = λ (the sequence consisting of zero elements) as the identity.
2. The possible input tapes to a computer form a free monoid on the set of symbols
(such as the ASCII symbols) in the computer alphabet.
3. Semigroup and monoid of transformations on a set S: Let S be a nonempty set and
let F be the set of all functions f : S →S. With the operation ⋆deﬁned by composition,
(f ⋆g)(x) = f(g(x)), (F, ⋆) is the semigroup [monoid] of transformations on S. The
identity of F is the identity transformation e: S →S where e(x) = x for all x ∈S.
4. The set of closed walks based at a ﬁxed vertex v in a graph forms a monoid under
the operation of concatenation. The null walk is the identity. (§8.4.1.)

Section 5.2
GROUPS
331
5. For a ﬁxed positive integer n, the set of all n × n matrices with elements in any ring
with unity (§5.4.1) where ⋆is matrix multiplication (using the operations in the ring) is
a semigroup and a monoid. The identity is the identity matrix.
6. The sets
N = {0, 1, 2, 3, . . .} (natural numbers),
Z = {. . . , −2, −1, 0, 1, 2, . . .} (integers),
Q (the set of rational numbers),
R (the set of real numbers),
C (the set of complex numbers),
where ⋆is either addition or multiplication, are all semigroups and monoids. Using either
addition or multiplication, each semigroup is a subsemigroup of each of those following it
in this list. Likewise, using either addition or multiplication, each monoid is a submonoid
of each of those following it in this list. For example, (Q, +) is a subsemigroup and
submonoid of (R, +) and (C, +). Under addition, e = 0; under multiplication, e = 1.
5.2
GROUPS
5.2.1
BASIC CONCEPTS
Deﬁnitions:
A group (G, ⋆) consists of a set G with a binary operator ⋆deﬁned on G such that ⋆
has the following properties:
• associative property: a ⋆(b ⋆c) = (a ⋆b) ⋆c for all a, b, c ∈G;
• identity property:
G has an element e (identity of G) that satisﬁes e ⋆a =
a ⋆e = a for all a ∈G;
• inverse property: for each element a ∈G there is an element a−1 ∈G (inverse
of a) such that a−1 ⋆a = a ⋆a−1 = e.
If a ⋆b = b ⋆a for all a, b ∈G, the group G is commutative or abelian. (Niels H. Abel,
1802–1829)
The order of a ﬁnite group G, denoted |G|, is the number of elements in the group.
The (external) direct product of groups (G1, ⋆1) and (G2, ⋆2) is the group G1 ×
G2 = {(a1, a2) | a1 ∈G1, a2 ∈G2} where multiplication ⋆is deﬁned by the rule
(a1, a2) ⋆(b1, b2) = (a1 ⋆1 b1, a2 ⋆2 b2). The direct product can be extended to n groups:
G1 × G2 × · · · × Gn. The direct product is also called the direct sum and written
G1 ⊕G2 ⊕· · · ⊕Gn, especially if the groups are abelian. If Gi = G for all i, the direct
product can be written Gn.
The group G is ﬁnitely generated if there are a1, a2, . . . , an ∈G such that every
element of G can be written as aǫ1
k1aǫ2
k2 . . . aǫj
kj where ki ∈{1, . . . , n} and ǫi ∈{1, −1}, for
some j ≥0; where the empty product is deﬁned to be e.
Note:
Frequently the operation ⋆is multiplication or addition.
If the operation is
addition, the group (G, +) is an additive group. If the operation is multiplication, the
group (G, ·) is a multiplicative group.

332
Chapter 5
ALGEBRAIC STRUCTURES
operation ∗
identity e
inverse a−1
additive group
a + b
0
−a
multiplicative group
a · b or ab
1 or e
a−1
Facts:
1. Every group has exactly one identity element.
2. In every group every element has exactly one inverse.
3. Cancellation laws: In all groups,
• if ab = ac then b = c (left cancellation law);
• if ba = ca, then b = c (right cancellation law).
4. (a−1)−1 = a.
5. (ab)−1 = b−1a−1. More generally, (a1a2 . . . ak)−1 = a−1
k a−1
k−1 . . . a−1
1 .
6. If a and b are elements of a group G, the equations ax = b and xa = b have unique
solutions in G. The solutions are x = a−1b and x = ba−1, respectively.
7. The direct product G1 × · · · × Gn is abelian when each group Gi is abelian.
8. |G1 × · · · × Gn| = |G1| · · · · · |Gn|.
9. The identity for G1 × · · · × Gn is (e1, . . . , en) where ei is the identity of Gi. The
inverse of (a1, . . . , an) is (a1, . . . , an)−1 = (a−1
1 , . . . , a−1
n ).
10. The structure of a group can be determined by a single rule (see Example 2) or by
a group table listing all products (see Examples 2 and 3).
Examples:
1. Table 1 displays information on several common groups. All groups listed have inﬁnite
order, except for the following: the group of complex nth roots of unity has order n, the
group of all bijections f : S →S where |S| = n has order n!, Zn has order n, Z∗
n has
order φ(n) (Euler phi-function), Sn has order n!, An has order n!/2, Dn has order 2n,
and the quaternion group has order 8.
All groups listed in the table are abelian except for: the group of bijections, GL(n, R),
Sn, An, Dn, and Q.
Table 1: Examples of groups.
set
operation
identity
inverses
Z, Q, R, C
addition
0
−a
Zn, n a positive integer
coordinatewise
(0, . . . , 0)
−(a1, . . . , an) =
(also Qn, Rn, Cn)
addition
(−a1, . . . , −an)
the set of all complex num-
bers of modulus 1 = {eiθ=
multiplication
ei0 = 1
(eiθ)−1 = e−iθ
cos θ+i sin θ | 0 ≤θ < 2π}
the complex nth roots of
(e2πik/n)−1 =
unity (solutions to zn = 1)
multiplication
1
e2πi(n−k)/n
{e2πik/n | k = 0, 1, . . ., n −1}
R−{0}, Q−{0}, C−{0}
multiplication
1
1/a
R∗(positive real numbers)
multiplication
1
1/a

Section 5.2
GROUPS
333
set
operation
identity
inverses
all rotations of the plane
around the origin; rα =
composition:
counterclockwise rotation
rα2 ◦rα1 =
r0 (the 0◦
r−1
α
= r−α
through an angle of α◦:
rα1+α2
rotation)
rα(x, y) = (x cos α −y sin α,
x sin α + y sin α)
all 1–1, onto functions (bijec-
composition
i: S →S
f −1(y) = x if and
tions) f : S →S where S is
of functions
where i(x) = x
only if f(x) = y
any nonempty set
for all x ∈S
Mm×n = all m × n matrices
matrix
Om×n (zero
−A
with entries in R
addition
matrix)
GL(n, R) = all n × n invert-
A−1
ible, or nonsingular, matrices
matrix
In (identity
with entries in R; (the gene-
multiplication
matrix)
ral linear group)
Zn = {0, 1, . . ., n −1}
(a + b) mod n
0
n −a (a ̸= 0)
−0 = 0
Z∗
n = {k | k ∈Zn, k relative-
ab mod n
1
see Example 2
ly prime to n}, n > 1
Sn = all permutations of
composition of
identity
inverse
{1, 2, . . ., n}; (symmetric
permutations
permutation
permutation
group) (See §5.3.)
An = all even permutations
composition of
identity
inverse
of {1, 2, . . ., n}; (alternating
permutations
permutation
permutation
group) (See §5.3.)
Dn = symmetries (rotations
composition of
rotation
r−1
α
= r−α;
and reﬂections) of a regular
functions
through 0◦
reﬂections are
n-gon; (dihedral group)
their own inverses
Q = quaternion group (see
Example 3)
2. The groups Zn and Z∗
n (see Table 1): In the groups Zn and Z∗
n an element a can be
viewed as the equivalence class {b ∈Z | b mod n = a mod n}, which can be written a
or [a]. To ﬁnd the inverse a−1 of a ∈Z∗
n, use the extended Euclidean algorithm to ﬁnd
integers a−1 and k such that aa−1 + nk = gcd(a, n) = 1. The following are the group
tables for Z2 = {0, 1} and Z3 = {0, 1, 2}:
+
0
1
0
0
1
1
1
0
+
0
1
2
0
0
1
2
1
1
2
0
2
2
0
1
3. Quaternion group: Q = {1, −1, i, −i, j, −j, k, −k} where multiplication is deﬁned by
the following relations:
i2 = j2 = k2 = −1,
ij = −ji = k,
jk = −kj = i,
ki = −ik = j

334
Chapter 5
ALGEBRAIC STRUCTURES
where 1 is the identity. These relations yield the multiplication table shown next. Here,
1−1 = 1, (−1)−1 = −1, x and −x are inverses for x = i, j, k. The group is nonabelian.
·
1
−1
i
−i
j
−j
k
−k
1
1
−1
i
−i
j
−j
k
−k
−1
−1
1
−i
i
−j
j
−k
k
i
i
−i
−1
1
k
−k
−j
j
−i
−i
i
1
−1
−k
k
j
−j
j
j
−j
−k
k
−1
1
i
−i
−j
−j
j
k
−k
1
−1
−i
i
k
k
−k
j
−j
−i
i
−1
1
−k
−k
k
−j
j
i
−i
1
−1
The quaternion group Q can also be deﬁned as the following group of 8 matrices, where
i is the complex number such that i2 = −1 and the group operation is matrix multipli-
cation.
 
1
0
0
1
!
,
 
−1
0
0
−1
!
,
 
−i
0
0
i
!
,
 
i
0
0
−i
!
,
 
0
1
−1
0
!
,
 
0
−1
1
0
!
,
 
0
i
i
0
!
,
 
0
−i
−i
0
!
4. The set {a, b, c, d} with either of the following multiplication tables is not a group. In
the ﬁrst case there is an identity, a, and each element has an inverse, but the associative
law fails: (bc)d ̸= b(cd). In the second case there is no identity (hence inverses are not
deﬁned) and the associative law fails.
·
a
b
c
d
a
a
b
c
d
b
b
d
a
c
c
c
a
b
d
d
d
c
b
a
·
a
b
c
d
a
a
c
b
d
b
d
b
a
c
c
b
d
c
a
d
c
a
d
b
5.2.2
GROUP ISOMORPHISM AND HOMOMORPHISM
Deﬁnitions:
For groups G and H, a function ϕ: G →H such that ϕ(ab) = ϕ(a)ϕ(b) for all a, b ∈G
is a homomorphism. The notation aϕ is sometimes used instead of ϕ(a).
For groups G and H, a function ϕ: G →H is an isomorphism from G to H if ϕ is a
homomorphism that is 1–1 and onto H. In this case G is isomorphic to H, written G ∼=
H.
An isomorphism ϕ: G →G is an automorphism.
The kernel of ϕ is the set {g ∈G | ϕ(g) = e}, where e is the identity of the group G.

Section 5.2
GROUPS
335
Facts:
1. If ϕ is an isomorphism, ϕ−1 is an isomorphism.
2. Isomorphism is an equivalence relation: G ∼= G (reﬂexive); if G ∼= H, then H ∼= G
(symmetric); if G ∼= H and H ∼= K, then G ∼= K (transitive).
3. If ϕ: G →H is a homomorphism, then ϕ(G) is a group (a subgroup of H).
4. If ϕ: G →H is a homomorphism, then the kernel of ϕ is a group (a subgroup of G).
5. If p is prime there is only one group of order p (up to isomorphism), the group (Zp,+).
6. Cayley’s theorem:
If G is a ﬁnite group of order n, then G is isomorphic to a
subgroup of the group Sn of permutations on n objects. (Arthur Cayley, 1821–1895)
The isomorphism is obtained by associating with each a ∈G the map πa : G→G given
by the rule πa(g) = ga for all g ∈G.
7. Zm × Zn is isomorphic to Zmn if and only if m and n are relatively prime.
8. If n = n1n2 . . . nk where the ni are powers of distinct primes, then Zn is isomorphic
to Zn1 × Zn2 × · · · × Znk.
9. Fundamental theorem of ﬁnite abelian groups: Every ﬁnite abelian group G (order
≥2) is isomorphic to a direct product of cyclic groups where each cyclic group has order
a power of a prime. That is, G is isomorphic to Zn1 × Zn2 × · · · × Znk where each cyclic
order ni is a power of some prime. In addition, the set {n1, . . . , nk} is unique.
10. Every ﬁnite abelian group is isomorphic to a subgroup of Z∗
n for some n.
11. Fundamental theorem of ﬁnitely generated abelian groups:
If G is a ﬁnitely gen-
erated abelian group, then there are unique integers n ≥0, n1, n2, . . . , nk ≥2 where
ni+1 | ni for i = 1, 2, . . ., k −1 such that G is isomorphic to Zn × Zn1 × Zn2 × · · · × Znk.
Examples:
1. The following table lists the number of nonisomorphic groups and abelian groups of
all orders from 1 to 60.
order
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15 16 17 18 19 20
groups
1
1
1
2
1
2
1
5
2
2
1
5
1
2
1
14
1
5
1
5
abelian
1
1
1
2
1
1
1
3
2
1
1
2
1
1
1
5
1
2
1
2
order
21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40
groups
2
2
1
15
2
2
5
4
1
4
1
51
1
2
1
14
1
2
2
14
abelian
1
1
1
3
2
1
3
2
1
1
1
7
1
1
1
4
1
1
1
3
order
41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60
groups
1
6
1
4
2
2
1
52
2
5
1
5
1
15
2
13
2
2
1
13
abelian
1
1
1
2
2
1
1
5
2
2
1
2
1
3
1
3
1
1
1
2
2. All groups of order 12 or less are listed by order in the following table.
order
groups
1
{e}
2
Z2
3
Z3
4
Z4, if there is an element of order 4 (group is cyclic)
Z2 × Z2 ∼= Klein four-group, if no element has order 4 (§5.3.2)

336
Chapter 5
ALGEBRAIC STRUCTURES
order
groups
5
Z5
6
Z6, if there is an element of order 6 (group is cyclic)
S3 ∼= D3, if there is no element of order 6 (§5.3.1, §5.3.2)
7
Z7
8
Z8, if there is an element of order 8 (group is cyclic)
Z2 × Z4, if there is an element a of order 4, but none of order 8, and
if there is an element b /∈(a) such that ab = ba and b2 = e
Z2 × Z2 × Z2, if every element has order 1 or 2
D4, if there is an element a of order 4, but none of order 8, and if
there is an element b /∈(a) such that ba = a3b and b2 = e
Quaternion group, if there is an element a of order 4, none of order 8,
and an element b /∈(a) such that ba = a3b and b2 = a2 (§5.2.1)
9
Z9, if there is an element of order 9 (group is cyclic)
Z3 × Z3, if there is no element of order 9
10
Z10, if there is an element of order 10 (group is cyclic)
D5, if there is no element of order 10
11
Z11
12
Z12 ∼= Z3 × Z4, if there is an element of order 12 (group is cyclic)
Z2 × Z6 ∼= Z2 × Z2 × Z3, if group is abelian but noncyclic
D6, if group is nonabelian and has an element of order 6 but none of
order 4
A4, if group is nonabelian and has no element of order 6
The group generated by a and b, where a has order 4, b has order 3,
and ab = b2a
5.2.3
SUBGROUPS
Deﬁnitions:
A subgroup of a group (G, ⋆) is a subset H ⊆G such that (H, ⋆) is a group (with the
same group operation as in G). Write H ≤G if H is a subgroup of G.
If a ∈G, the set (a) = {. . . , a−2 = (a−1)2, a−1, a0 = e, a, a2, . . .} = {an | n ∈Z} is the
cyclic subgroup generated by a. The element a is a generator of G.
G and {e} are improper subgroups of G.
All other subgroups of G are proper
subgroups of G.
Facts:
1. If G is a group, then {e} and G are subgroups of G.
2. If G is a group and a ∈G, the set (a) is a subgroup of G.
3. Every subgroup of an abelian group is abelian.

Section 5.2
GROUPS
337
4. If H is a subgroup of a group G, then the identity element of H is the identity element
of G; the inverse (in the subgroup H) of an element a in H is the inverse (in the group
G) of a.
5. Lagrange’s theorem:
Let G be a ﬁnite group. If H is any subgroup of G, then the
order of H is a divisor of the order of G. (Joseph-Louis Lagrange, 1736–1813)
6. If d is a divisor of the order of a group G, there may be no subgroup of order d. (The
group A4, of order 12, has no subgroup of order 6. See §5.3.2.)
7. If G is a ﬁnite abelian group, then the converse of Lagrange’s theorem is true for G.
8. If G is ﬁnite (not necessarily abelian) and p is a prime that divides the order of G,
then G has a subgroup of order p.
9. If G has order pmn where p is prime and p does not divide n, then G has a subgroup
of order pm, called a Sylow subgroup or Sylow p-subgroup. See §5.2.6.
10. A subset H of a group G is a subgroup of G if and only if the following are all true:
H ̸= ∅; a, b ∈H implies ab ∈H; and a ∈H implies a−1 ∈H.
11. A subset H of a group G is a subgroup of G if and only if H ̸= ∅and a, b ∈H
implies that ab−1 ∈H.
12. If H is a nonempty ﬁnite subset of a group G with the property that a, b ∈H
implies that ab ∈H, then H is a subgroup of G.
13. The intersection of any collection of subgroups of a group G is a subgroup of G.
14. The union of subgroups is not necessarily a subgroup. See Example 12.
Examples:
1. Additive subgroups:
Each of the following can be viewed as a subgroup of all the
groups listed after it: (Z, +), (Q, +), (R, +), (C, +).
2. For n any positive integer, the set nZ = {nz | z ∈Z} is a subgroup of Z.
3. Z2 is not a subgroup of Z4 (the group operations are not the same).
4. The set of odd integers under addition is not a subgroup of (Z, +) (the set of odd
integers is not closed under addition).
5. (N, +) is not a subgroup of (Z, +) (N does not contain its inverses).
6. The group Z6 has the following four subgroups: {0}, {0, 3}, {0, 2, 4}, Z6.
7. Multiplicative subgroups:
Each of the following can be viewed as a subgroup of all
the groups listed after it: (Q −{0}, ·), (R −{0}, ·), (C −{0}, ·).
8. The set of n complex nth roots of unity can be viewed as a subgroup of the set of all
complex numbers of modulus 1 under multiplication, which is a subgroup of (C −{0}, ·).
9. If nd = 360 (n and d positive integers) and rk is the counterclockwise rotation of the
plane about the origin through an angle of k◦, then {rk | k = 0, d, 2d, 3d, . . ., (n −1)d}
is a subgroup of the group of all rotations of the plane around the origin.
10. The set of all n×n nonsingular diagonal matrices is a subgroup of the set of all n×n
nonsingular matrices under multiplication.
11. If n = mk, then {0, m, 2m, . . ., (k−1)m} is a subgroup of (Zn, +) isomorphic to Zk.
12. The union of subgroups need not be a subgroup: {2n | n ∈Z} and {3n | n ∈Z}
are subgroups of Z, but their union is not a subgroup of Z since 2 + 3 = 5 /∈{2n | n ∈
Z} ∪{3n | n ∈Z}.

338
Chapter 5
ALGEBRAIC STRUCTURES
5.2.4
COSETS AND QUOTIENT GROUPS
Deﬁnitions:
If H is a subgroup of a group G and a ∈G, then the set aH = {ah | h ∈H} is a left
coset of H in G. The set Ha = {ha | h ∈H} is a right coset of H in G. (If G is
written additively, the cosets are written a + H and H + a.)
The index of a subgroup H in a group G, written (G: H) or [G: H], is the number of
left (or right) cosets of H in G.
A normal subgroup of a group G is a subgroup H of G such that aH = Ha for all
a ∈G. The notation H ⊳G means that H is a normal subgroup of G.
If H is a normal subgroup of G, the quotient group (or factor group of G modulo
H) is the group G/H = {aH | a ∈G}, where aH · bH = (ab)H.
If G is a group and a ∈G, an element b ∈G is a conjugate of a if b = gag−1 for
some g ∈G.
If G is a group and a ∈G, the set {x | x ∈G, ax = xa} is the centralizer (or
normalizer) of a.
If G is a group, the set {x | x ∈G, gx = xg for all g ∈G} is the center of G.
If H is a subgroup of group G, the set {x | x ∈G, xHx−1 = H} is the normalizer
of H.
Facts:
1. If H is a subgroup of a group G, then the following are equivalent:
• H is a normal subgroup of G;
• aHa−1 = a−1Ha = H for all a ∈G;
• a−1ha ∈H for all a ∈G, h ∈H;
• for all a ∈G and h1 ∈H, there is h2 ∈H such that ah1 = h2a.
2. If group G is abelian, then every subgroup H of G is normal. If G is not abelian, it
may happen that H is not normal.
3. If group G is ﬁnite, then (G: H) = |G|/|H|.
4. {e} and G are normal subgroups of group G.
5. In the group G/H, the identity is eH = H and the inverse of aH is a−1H.
6. Fundamental homomorphism theorem:
If ϕ: G →H is a homomorphism and has
kernel K, then K is a normal subgroup of G and G/K is isomorphic to ϕ(G).
7. If H is a normal subgroup of a group G and ϕ: G →G/H is deﬁned by ϕ(g) = gH,
then ϕ is a homomorphism onto G/H with kernel H.
8. If H is a normal subgroup of a ﬁnite group G, then G/H has |G|/|H| cosets.
9. If H and K are normal subgroups of a group G, then H ∩K is a normal subgroup
of G.
10. For all a ∈G, the centralizer of a is a subgroup of G.
11. The center of a group is a subgroup of the group.
12. The normalizer of a subgroup of group G is a subgroup of G.

Section 5.2
GROUPS
339
13. The index of the centralizer of a ∈G is equal to the number of distinct conjugates
of a in G.
14. If a group G contains normal subgroups H and K such that H ∩K = {e} and
{hk | h ∈H, k ∈K} = G, then G is isomorphic to H × K.
15. If G is a group such that |G| = ab where a and b are relatively prime, and if G
contains normal subgroups H of order a and K of order b, then G is isomorphic to
H × K.
Examples:
1. Z/nZ is isomorphic to Zn, since ϕ: Z →Zn deﬁned by ϕ(g) = g mod n has kernel
nZ.
2. The left cosets of the subgroup H = {0, 4} in Z8 are H + 0 = {0, 4}, H + 1 = {1, 5},
H + 2 = {2, 6}, H + 3 = {3, 7}. The index of H in Z8 is (Z8, H) = 4.
3. {(1), (12)} is not a normal subgroup of the symmetric group S3 (§5.3.1).
5.2.5
CYCLIC GROUPS AND ORDER
Deﬁnitions:
A group (G, ·) is cyclic if there is a ∈G such that G = {an | n ∈Z}, where a0 = e and
a−n = (a−1)n for all positive integers n. If G is written additively, G = {na | n ∈Z},
where 0a = 0 and if n > 0, na = a + a + a + · · · + a (n terms) and −na = (−a) + (−a) +
· · · + (−a) (n terms).
The element a is called a generator of G and the group (G, ·) is written ((a), ·), (a), or
⟨a⟩.
The order of an element a ∈G, written |(a)| or ord(a), is the smallest positive integer n
such that an = e (na = 0 if G is written additively). If there is no such integer, then a
has inﬁnite order.
A subgroup H of a group (G, ·) is a cyclic subgroup if there exists a ∈H such that
H = {an | n ∈Z}.
Facts:
1. The order of an element a is equal to the number of elements in (a).
2. Every group of prime order is cyclic.
3. Every cyclic group is abelian. However, not every abelian group is cyclic: for example,
(R, +) and the Klein four-group.
4. If G is an inﬁnite cyclic group, then G ∼= (Z, +).
5. If G is a ﬁnite cyclic group of order n, then G ∼= (Zn, +).
6. If G is a group of order n, then the order of every element of G is a divisor of n.
7. Cauchy’s theorem: If G is a group of order n and p is a prime that divides n, then G
contains an element of order p. (Augustin-Louis Cauchy, 1789–1857)
8. If G is a cyclic group of order n generated by a, then G = {a, a2, a3, . . . , an} and
an = e. If k and n are relatively prime, then ak is also a generator of G, and conversely.
9. If G is a group and a ∈G, then (a) is a cyclic subgroup of G.
10. Every subgroup of a cyclic group is cyclic.
11. If G is a group of order n and there is an element a ∈G of order n, then G is cyclic
and G = (a).

340
Chapter 5
ALGEBRAIC STRUCTURES
Examples:
1. (Z, +) is cyclic and is generated by each of 1 and −1.
2. (Zn, +) is cyclic and is generated by each element of Zn that is relatively prime to n.
If a ∈Zn, then a has order n/gcd(a, n).
3. (Zp, +), p prime, is a cyclic group generated by each of the elements 1, 2, . . . , p −1.
If a ̸= 0, a has order p.
4. (Z∗
n, ·) is cyclic if and only if n = 2, 4, pk, or 2pk, where k ≥1 and p is an odd prime.
5.2.6
SYLOW THEORY
The Sylow theorems are used to help classify the nonisomorphic groups of a given order
by guaranteeing the existence of subgroups of certain orders.
(Peter Ludvig Mejdell
Sylow, 1832–1918)
Deﬁnitions:
For prime p, a group G is a p-group if every element of G has order pn for some n ≥0.
For prime p, a Sylow p-subgroup (Sylow subgroup) of G is a subgroup of G that is
a p-group and is not properly contained in any p-group in G.
Facts:
1. Sylow’s theorem:
If G is a group of order pm · q where p is a prime, m ≥1, and p
does not divide q, then
• G contains subgroups of orders p, p2, . . . , pm (hence, if prime p divides the order
of a ﬁnite group G, then G contains an element of order p);
• if H and K are Sylow p-subgroups of G, there is g ∈G such that K = gHg−1 (K
is conjugate to H);
• the number of Sylow p-subgroups of G is kp + 1 for some integer k such that
(kp + 1) | q.
2. If G is a group of order pq where p and q are primes and p < q, then G contains a
normal subgroup of order q.
3. If G is a group of order pq where p and q are primes, p < q, and p does not divide
q −1, then G is cyclic.
Examples:
1. Every group of order 15 is cyclic (by Fact 3).
2. Every group of order 21 contains a normal subgroup of order 7 (by Fact 2).
5.2.7
SIMPLE GROUPS
Simple groups arise as a fundamental part of the study of ﬁnite groups and the structure
of their subgroups. An extensive, lengthy search by many mathematicians for all ﬁnite
simple groups ended in 1980 when, as the result of hundreds of articles written by over
one hundred mathematicians, the classiﬁcation of all ﬁnite simple groups was completed.
See [As00] and [Go82] for details.

Section 5.2
GROUPS
341
Deﬁnitions:
A group G ̸= {e} is simple if its only normal subgroups are {e} and G.
A composition series for a group G is a ﬁnite sequence of subgroups H1 = G, H2, . . . ,
Hn−1, Hn = {e} such that Hi+1 is a normal subgroup of Hi and Hi/Hi+1 is simple, for
i = 1, . . . , n −1.
A ﬁnite group G is solvable if it has a sequence of subgroups H1 = G, H2, . . . , Hn−1,
Hn = {e} such that Hi+1 is a normal subgroup of Hi and Hi/Hi+1 is abelian, for
i = 1, . . . , n −1.
A sporadic group is one of 26 nonabelian ﬁnite simple groups that is not an alternating
group or a group of Lie type [Go82].
Facts:
1. Every ﬁnite group has a composition series. Thus, simple groups (the quotient groups
in the series) can be regarded as the building blocks of ﬁnite groups.
2. Some inﬁnite groups, such as (Z, +), do not have composition series.
3. Every abelian group is solvable.
4. An abelian group G is simple if and only if G ∼= Zp where p is prime.
5. If G is a nonabelian solvable group, then G is not simple.
6. Every group of prime order is simple.
7. Every group of order pn (p prime) is solvable.
8. Every group of order pnqm (p, q primes) is solvable.
9. If G is a solvable, simple ﬁnite group, then G is either {e} or Zp (p prime).
10. If G is a simple group of odd order, then G ∼= Zp for some prime p.
11. There is no inﬁnite simple, solvable group.
12. Burnside’s conjecture/Feit-Thompson theorem:
In 1911 William Burnside conjec-
tured that all groups of odd order are solvable. This conjecture was proved in 1963 by
Walter Feit and John Thompson. (See Fact 13.)
13. Every nonabelian simple group has even order. (This follows from the Feit-Thompson
theorem.)
14. The proof of the Burnside conjecture provided the impetus for a massive program
to classify all ﬁnite simple groups. This program, organized by Daniel Gorenstein, led to
hundreds of journal articles and concluded in 1980 when the classiﬁcation problem was
ﬁnally solved (Fact 15). [GoLySo02]
15. Classiﬁcation theorem for ﬁnite simple groups: Every ﬁnite simple group is of one
of the following types:
• abelian: Zp where p is prime (§5.2.1);
• nonabelian:
⋄alternating groups An (n ̸= 4) (§5.3.2);
⋄groups of Lie type, which fall into 6 classes of classical groups and 10 classes
of exceptional simple groups [Ca89];
⋄sporadic groups. There are 26 sporadic groups, listed here from smallest to
largest order. The letters in the names of the groups reﬂect the names of
some of the people who conjectured the existence of the groups or proved
the groups simple. M11 (order 7,920), M12, M22, M23, M24, J1, J2, J3, J4,
HS, Mc, Suz, Ru, He, Ly, ON, .1, .2, .3, M(22), M(23), M(24)′, F5, F3,
F2, F1 (the monster or Fischer-Griess group of order ≈1054).

342
Chapter 5
ALGEBRAIC STRUCTURES
5.2.8
GROUP PRESENTATIONS
Deﬁnitions:
The balanced alphabet on the set X = {x1, . . . , xn} is the set {x1, x−1
1 , . . . , xn, x−1
n },
whose elements are often called symbols.
Symbols xj and x−1
j
of a balanced alphabet are inverses of each other.
A double
inverse (x−1
j )−1 is understood as the identity operator.
A word in X is a string s1s2 . . . sn of symbols from the balanced alphabet on X.
The inverse of a word s = s1s2 . . . sn is the word s−1 = s−1
n . . . s−1
2 s−1
1 .
The free semigroup W(X) has the set of words in X as its domain and string concate-
nation as its product operation.
A trivial relator in the set X = {x1, . . . , xn} is a word of the form xjx−1
j
or x−1
j xj.
A word u is freely equivalent to a word v, denoted u ∼v, if v can be obtained from u
by iteratively inserting and deleting trivial relators, in the usual sense of those string
operations. This is an equivalence relation, whose classes are called free equivalence
classes.
A reduced word is a word containing no instances of a trivial relator as a substring.
The free group F[X] has the set of free equivalence classes of words in X as its domain
and class concatenation as its product operation.
A group presentation is a pair (X : R), where X is an alphabet and R is a set of words
in X called relators. A group presentation is ﬁnite if X and R are both ﬁnite.
A word u is R-equivalent to a word v under the group presentation (X : R), denoted
u ∼R v, if v can be obtained from u by iteratively inserting and deleting relators from R or
trivial relators. This is an equivalence relation, whose classes are called R-equivalence
classes.
The group G(X : R) presented by the group presentation (X : R) has the set of R-
equivalence classes as its domain and class concatenation as its product operation. More-
over, any group G isomorphic to G(X : R) is said to be presented by the group presen-
tation (X : R).
The group G is ﬁnitely presentable if it has a presentation whose alphabet and relator
set are both ﬁnite.
The commutator of the words u and v is the word u−1v−1uv. Any word of this form
is called a commutator.
A conjugate of the word v is any word of the form u−1vu.
Facts:
1. Max Dehn (1911) formulated three fundamental decision problems for ﬁnite presen-
tations:
• word problem:
Given an arbitrary presentation (X : R) and an arbitrary word
w, decide whether w is equivalent to the empty word (i.e., the group identity).
• conjugacy problem:
Given an arbitrary presentation (X : R) and two arbitrary
words w1 and w2, decide whether w1 is equivalent to a conjugate of w2.

Section 5.3
PERMUTATION GROUPS
343
• isomorphism problem:
Given two arbitrary presentations (X : R) and (Y : S),
decide whether they present isomorphic groups.
2. W. W. Boone (1955) and P. S. Novikov (1955) constructed presentations in which
the word problem is recursively unsolvable. This implies that there is no single ﬁnite
procedure that works for all ﬁnite presentations, thereby negatively solving Dehn’s word
problem and conjugacy problem.
3. M. O. Rabin (1958) proved that it is impossible to decide even whether a presentation
presents the trivial group, which immediately implies that Dehn’s isomorphism problem
is recursively unsolvable.
4. The word problem is recursively solvable in various special classes of group pre-
sentations, including the following: presentations with no relators (i.e., free groups),
presentations with only one relator, presentations in which the relator set includes the
commutator of each pair of generators (i.e., abelian groups).
5. The group presentation G(X : R) is the quotient of the free group F[X] by the nor-
malizer of the relator set R.
6. More information on group presentations can be found in [CoMo80], [CrFo08], and
[MaKaSo04].
Examples:
1. The cyclic group Zk has the presentation (x: xk).
2. The direct sum Zr ⊕Zs has the presentation (x, y: xr, ys, x−1y−1xy).
3. The dihedral group Dq has the presentation (x, y: xq, y2, y−1xyx).
5.3
PERMUTATION GROUPS
Permutations, as arrangements, are important tools used extensively in combinatorics
(§2.3 and §2.7). The set of permutations on a given set forms a group, and it is this
algebraic structure that is examined in this section.
5.3.1
BASIC CONCEPTS
Deﬁnitions:
A permutation is a one-to-one and onto function σ: S →S, where S is any nonempty
set. If S = {a1, a2, . . . , an}, a permutation σ is sometimes written as the 2 × n matrix
σ =
 
a1
a2
. . .
an
a1σ
a2σ
. . .
anσ
!
where aiσ means σ(ai).
A permutation σ: S →S is a cycle of length n if there is a subset of S of size n,
{a1, a2, . . . , an}, such that a1σ = a2, a2σ = a3, . . . , anσ = a1, and aσ = a for all other
elements of S. Write σ = (a1 a2 . . . an). A transposition is a cycle of length 2.

344
Chapter 5
ALGEBRAIC STRUCTURES
A permutation group (G, X) is a collection G of permutations on a nonempty set X
(whose elements are called objects) such that these permutations form a group under
composition. That is, if σ and τ are permutations in G, στ is the permutation in G
deﬁned by the rule a(στ) = (aσ)τ. The order of the permutation group is |G|. The
degree of the permutation group is |X|.
The symmetric group on n elements is the group Sn of all permutations on the set
{1, 2, . . ., n} under composition. (See Fact 1.)
An isomorphism from a permutation group (G, X) to a permutation group (H, Y ) is
a pair of functions (α: G→H, f : X→Y ) such that α is a group isomorphism and f is
one-to-one and onto Y .
If σ1 = (ai1 ai2 . . . aim) and σ2 = (aj1 aj2 . . . ajn) are cycles on S, then σ1 and σ2 are
disjoint cycles if the sets {ai1, ai2, . . . , aim} and {aj1, aj2, . . . , ajn} are disjoint.
An even permutation [odd permutation] is a permutation that can be written as a
product of an even [odd] number of transpositions.
The sign of a permutation (where the permutation is written as a product of transposi-
tions) is +1 if it has an even number of transpositions and −1 if it has an odd number
of transpositions.
The identity permutation on S is the permutation ι: S →S such that xι = x for all
x ∈S.
An involution is a permutation σ such that σ2 = ι (the identity permutation).
The orbit of a ∈S under σ is the set {. . . , aσ−2, aσ−1, a, aσ, aσ2, . . .}.
Facts:
1. Symmetric group of degree n:
The set of permutations on a nonempty set X is a
group, where the group operation is composition of permutations: σ1σ2 is deﬁned by
x(σ1σ2) = (xσ1)σ2. The identity is the identity permutation ι. The inverse of σ is the
permutation σ−1, where xσ−1 = y if and only if yσ = x. If |X| = n, the group of
permutations is written Sn, the symmetric group of degree n.
2. Multiplication of permutations is not commutative. (See Examples 1 and 4.)
3. A permutation π is an involution if and only if π = π−1.
4. The number of involutions in Sn, denoted inv(n), is equal to the number of Young
tableaux that can be formed from the set {1, 2, . . ., n}. (See §2.8.)
5. Permutations can be used to ﬁnd determinants of matrices. (See §6.3.)
6. Every permutation on a ﬁnite set can be written as a product of disjoint cycles.
7. Cycle notation is not unique: for example, (1 4 7 5) = (4 7 5 1) = (7 5 1 4) = (5 1 4 7).
8. Every permutation is either even or odd, and no permutation is both even and odd.
Hence, every permutation has a unique sign.
9. Each cycle of length k can be written as a product of k −1 transpositions:
(x1 x2 x3 . . . xk) = (x1 x2)(x1 x3)(x1 x4) . . . (x1 xk).
10. Sn has order n!.
11. Sn is not abelian for n ≥3. For example, (1 2)(1 3) ̸= (1 3)(1 2).
12. The order of a permutation that is a single cycle is the length of the cycle. For
example, (1 5 4) has order 3.

Section 5.3
PERMUTATION GROUPS
345
13. The order of a permutation that is written as a product of disjoint cycles is equal
to the least common multiple of the lengths of the cycles.
14. Cayley’s theorem:
If G is a ﬁnite group of order n, then G is isomorphic to a
subgroup of Sn. (See §5.2.2.)
15. Let G be a group of permutations on a set X (such a group is said to act on X).
Then G induces an equivalence relation R on the set X by the following rule: for a, b ∈X,
aRb if and only if there is a permutation σ ∈G such that aσ = b.
Examples:
1. If σ =
 
1
2
3
4
5
5
1
2
4
3
!
, τ =
 
1
2
3
4
5
4
5
1
3
2
!
, then στ =
 
1
2
3
4
5
2
4
5
3
1
!
and
τσ =
 
1
2
3
4
5
4
3
5
2
1
!
. Note that στ ̸= τσ.
2. All elements of Sn can be written in cycle notation. For example,
σ =
 
1
2
3
4
5
6
7
4
6
3
7
1
2
5
!
= (1 4 7 5)(2 6)(3).
Each cycle describes the orbit of the elements in that cycle. For example, (1 4 7 5)
is a cycle of length 4, and indicates that 1σ = 4, 4σ = 7, 7σ = 5, and 5σ = 1. The
cycle (3) indicates that 3σ = 3. If a cycle has length 1, that cycle can be omitted when
a permutation is written as a product of cycles: (1 4 7 5)(2 6)(3) = (1 4 7 5)(2 6).
3. Multiplication of permutations written in cycle notation can be performed easily. For
example: if σ = (1 5 3 2) and τ = (1 4 3)(2 5), then στ = (1 5 3 2)(1 4 3)(2 5) = (1 2 4 3 5).
(Moving from left to right through the product of cycles, trace the orbit of each element.
For example, 3σ = 2 and 2τ = 5; therefore 3στ = 5.)
4. Multiplication of cycles need not be commutative. For example, (1 2)(1 3) = (1 2 3),
(1 3)(1 2) = (1 3 2), but (1 2 3) ̸= (1 3 2). However, disjoint cycles commute.
5. If the group of permutations G = {ι, (1 2), (3 5)} acts on the set S = {1, 2, 3, 4, 5},
then the partition of S resulting from the equivalence relation induced by G is {{1, 2},
{3, 5}, {4}}. (See Fact 15.)
6. Let group G = {ι, (1 2)} act on X = {1, 2} and group H = {ι, (1 2)(3)} act on
Y = {1, 2, 3}. The permutation groups (G, X) and (H, Y ) are not isomorphic since there
is no bijection between X and Y (even though G and H are isomorphic groups).
5.3.2
EXAMPLES OF PERMUTATION GROUPS
Deﬁnitions:
The alternating group on n elements (n ≥2) is the subgroup An of Sn consisting of
all even permutations.
The dihedral group (octic group) Dn is the group of rigid motions (rotations and
reﬂections) of a regular polygon with n sides under composition.
The Klein four-group (or Viergruppe or the group of the rectangle) is the group
under composition of the four rigid motions of a rectangle that leave the rectangle in its
original location. (Felix Klein, 1849–1925)

346
Chapter 5
ALGEBRAIC STRUCTURES
Given a permutation σ: S →S, the induced pair permutation is the permutation σ(2)
on unordered pairs of elements of S given by the rule σ(2)({x, y}) = {σ(x), σ(y)}.
Given a permutation group G acting on a set S, the induced pair-action group G(2)
is the group of induced pair-permutations {σ(2) | σ ∈G} under composition.
Given a permutation σ: S →S, the ordered pair-permutation is the permutation σ[2]
on the set S × S given by the rule σ[2]((x, y)) = (σ(x), σ(y)).
Given a permutation group G acting on a set S, the ordered pair-action group G[2]
is the group of ordered pair-permutations {σ[2] | σ ∈G} under composition.
Facts:
1. Some common subgroups of Sn are listed in the following table.
subgroup
order
description
symmetric group Sn
n!
all permutations of {1, 2, . . ., n}
alternating group An
n!/2
all even permutations of {1, 2, . . ., n}
dihedral group Dn
2n
rigid motions of regular n-gon in
3-dimensional space (Example 2)
Klein 4-group
4
rigid motions of rectangle in
(subgroup of S4)
3-dimensional space (Example 3)
identity
1
consists only of identity permutation
2. The group An is abelian if n = 2 or 3, and is nonabelian if n ≥4.
3. The group Dn has order 2n. The elements consist of the n rotations and n reﬂections
of a regular polygon with n sides. The n rotations are the counterclockwise rotations
about the center through angles of 360k
n
degrees (k = 0, 1, . . ., n−1). (Clockwise rotations
can be written in terms of counterclockwise rotations.) If n is odd, the n reﬂections are
reﬂections in lines through a vertex and the center; if n is even, the reﬂections are
reﬂections in lines joining opposite vertices and in lines joining midpoints of opposite
sides.
4. The elements of Dn can be written as permutations of {1, 2, . . ., n}. See the following
ﬁgure for the rigid motions in D4 (the rigid motions of the square) and the following table
for the group multiplication table for D4.
1
4
e = 0  CCW
rotation
(1)
2
3
4
3
90  CCW
rotation
(1 2 3 4)
1
2
3
2
180  CCW
rotation
(1 3)(2 4)
4
1
2
1
270  CCW
rotation
(1 4 3 2)
3
4
2
4
reflection in
vertical line
(1 2)(3 4)
1
3
4
1
reflection in
horizontal line
(1 4)(2 3)
3
2
1
2
reflection in
1-3 diagonal
(2 4)
4
3
3
4
reflection in
2-4 diagonal
(1 3)
2
1

Section 5.4
RINGS
347
·
(1)
(1234)
(13)(24)
(1432)
(12)(34)
(14)(23)
(24)
(13)
(1)
(1)
(1234)
(13)(24)
(1432)
(12)(34)
(14)(23)
(24)
(13)
(1234)
(1234)
(13)(24)
(1432)
(1)
(24)
(13)
(14)(23)
(12)(34)
(13)(24)
(13)(24)
(1432)
(1)
(1234)
(14)(23)
(12)(34)
(13)
(24)
(1432)
(1432)
(1)
(1234)
(13)(24)
(13)
(24)
(12)(34)
(14)(23)
(12)(34)
(12)(34)
(13)
(14)(23)
(24)
(1)
(13)(24)
(1432)
(1234)
(14)(23)
(14)(23)
(24)
(12)(34)
(13)
(13)(24)
(1)
(1234)
(1432)
(24)
(24)
(21)(34)
(13)
(14)(23)
(1234)
(1432)
(1)
(13)(24)
(13)
(13)
(14)(23)
(24)
(12)(34)
(1432)
(1234)
(13)(24)
(1)
5. The Klein four-group consists of the following four rigid motions of a rectangle: the
rotations about the center through 0◦or 180◦, and reﬂections through the horizontal or
vertical lines through its center, as illustrated in the following ﬁgure. The following table
is the multiplication table for the Klein four-group.
2
3
1
4
e = 0  CCW
rotation
(1)
4
1
3
2
180  CCW
rotation
(1 3)(2 4)
1
4
2
3
reflection in
vertical line
(1 2)(3 4)
3
2
4
1
reflection in
horizontal line
(1 4)(2 3)
·
(1)
(13)(24)
(12)(34)
(14)(23)
(1)
(1)
(13)(24)
(12)(34)
(14)(23)
(13)(24)
(13)(24)
(1)
(14)(23)
(12)(34)
(12)(34)
(12)(34)
(14)(23)
(1)
(13)(24)
(14)(23)
(14)(23)
(12)(34)
(13)(24)
(1)
6. The Klein four-group is isomorphic to Z∗
8.
7. The induced permutation group S(2)
n
and the ordered-pair-action group S[2]
n are used
in enumerative graph theory. (See §8.9.2.)
8. The induced permutation group S(2)
n
has
 n
2

objects and n! permutations.
9. The ordered-pair-action permutation group S[2]
n has n2 objects and n! permutations.
5.4
RINGS
5.4.1
BASIC CONCEPTS
Deﬁnitions:
A ring (R, +, ·) consists of a set R closed under binary operations + and · such that
• (R, +) is an abelian group; i.e., (R, +) satisﬁes

348
Chapter 5
ALGEBRAIC STRUCTURES
⋄associative property: a + (b + c) = (a + b) + c for all a, b, c ∈R;
⋄identity property: R has an identity element 0 that satisﬁes 0+a = a+0 = a
for all a ∈R;
⋄inverse property: for each a ∈R there is an additive inverse element −a ∈R
(the negative of a) such that −a + a = a + (−a) = 0;
⋄commutative law: a + b = b + a for all a, b ∈R;
• the operation · is associative: a · (b · c) = (a · b) · c for all a, b, c ∈R;
• The distributive properties for multiplication over addition hold for all a, b, c ∈R:
⋄left distributive property: a · (b + c) = a · b + a · c;
⋄right distributive property: (a + b) · c = a · c + b · c.
A ring R is commutative if the multiplication operation is commutative: a · b = b · a
for all a, b ∈R.
A ring R is a ring with unity if there is an identity 1 (̸= 0) for multiplication; i.e.,
1 · a = a · 1 = a for all a ∈R. The multiplicative identity is the unity of R.
An element x in a ring R with unity is a unit if x has a multiplicative inverse; i.e., there
is x−1 ∈R such that x · x−1 = x−1 · x = 1.
Subtraction in a ring is deﬁned by the rule a −b = a + (−b).
Facts:
1. Multiplication a · b is often written ab or a × b.
2. The order of precedence of operations in a ring follows that for real numbers: multi-
plication is to be done before addition. That is, a+bc means a+(bc) rather than (a+b)c.
3. In all rings, a0 = 0a = 0.
4. Properties of subtraction:
−(−a) = a
(−a)(−b) = ab
a(b −c) = ab −ac
(a −b)c = ac −bc
a(−b) = (−a)b = −(ab)
(−1)a = −a (if the ring has unity).
5. The set of all units of a ring is a group under the multiplication deﬁned on the ring.
Examples:
1. The following table gives several examples of rings, together with their associated
identity and unity elements.
set and addition and multiplication operations
0
1
{0}, usual addition and multiplication; (trivial ring)
0
none
Z, Q, R, C, with usual + and ·
0
1
Zn={0, 1, . . ., n −1} (n a positive integer), a+b =
0
1
(a+b) mod n, a·b = (ab) mod n; (modular ring)
Z[
√
2]={a+b
√
2 | a, b ∈Z}, (a+b
√
2)+(c+d
√
2)=
0+0
√
2
1+0
√
2
(a+c)+(b+d)
√
2, (a+b
√
2)·(c+d
√
2)=(ac+2bd)+
(ad+bc)
√
2 [Similar rings can be constructed using
√n (n an integer) if √n not an integer.]
Z[i] = {a + bi | a, b ∈Z}; (Gaussian integers;
0+0i
1+0i
see §5.4.2, Example 2.)

Section 5.4
RINGS
349
set and addition and multiplication operations
0
1
Mn×n(R) = all n × n matrices with entries in a
On
In
ring R with unity, matrix addition and multipli-
(zero
(identity
cation; (matrix ring)
matrix)
matrix)
R = {f | f : A→B} (A any nonempty set and B
f such that
f such that
any ring), (f+g)(x) = f(x)+g(x), (f·g)(x) =
f(x)=0 for
f(x)=1 for
f(x)·g(x); (ring of functions)
all x∈A
all x∈A (if
B has unity)
P(S) = all subsets of a set S, A+B = A∆B =
(A∪B) −(A∩B) (symmetric diﬀerence), A·B =
∅
S
A∩B; (Boolean ring)
{a+bi+cj+dk | a, b, c, d ∈R}, i, j, k in
0+0i+0j+0k
1+0i+0l+0k
quaternion group, elements are added and
multiplied like polynomials using ij = k, etc.;
(ring of real quaternions, §5.2.1)
2. Polynomial rings: For a ring R, the set
R[x] = {anxn + · · · + a1x + a0 | a0, a1, . . . , an ∈R}
forms a ring, where the elements are added and multiplied using the “usual” rules for
addition and multiplication of polynomials.
The additive identity 0 is the constant
polynomial p(x) = 0; the unity is the constant polynomial p(x) = 1 if R has a unity 1.
(See §5.5.)
3. Product rings:
For rings R and S, the set R × S = {(r, s) | r ∈R, s ∈S} forms a
ring, where
(r1, s1) + (r2, s2) = (r1 + r2, s1 + s2);
(r1, s1) · (r2, s2) = (r1r2, s1s2).
The additive identity is (0, 0). Unity is (1, 1) if R and S each have unity 1. Product
rings can have more than two factors: R1 × R2 × · · · × Rk or Rn = R × · · · × R.
5.4.2
SUBRINGS AND IDEALS
Deﬁnitions:
A subset S of a ring (R, +, ·) is a subring of R if (S, +, ·) is a ring using the same
operations + and · that are used in R.
A subset I of a ring (R, +, ·) is an ideal of R if
• (I, +, ·) is a subring of (R, +, ·);
• I is closed under left and right multiplication by elements of R: if x ∈I and
r ∈R, then rx ∈I and xr ∈I.

350
Chapter 5
ALGEBRAIC STRUCTURES
In a commutative ring R, an ideal I is principal if there is r ∈R such that I = Rr =
{xr | x ∈R}. I is the principal ideal generated by r, written I = (r).
In a commutative ring R, an ideal I ̸= R is maximal if the only ideal properly containing
I is R.
In a commutative ring R, an ideal I ̸= R is prime if ab ∈I implies that a ∈I or b ∈I.
Facts:
1. If S is a nonempty subset of a ring (R, +, ·), then S is a subring of R if and only if S
is closed under subtraction and multiplication.
2. An ideal in a ring (R, +, ·) is a subgroup of the group (R, +), but not necessarily
conversely.
3. The intersection of ideals in a ring is an ideal.
4. If R is any ring, then R and {0} are ideals, called trivial ideals.
5. In a commutative ring with unity, every maximal ideal is a prime ideal.
6. Every ideal I in the ring Z is a principal ideal. I = (r) where r is the smallest
positive integer in I.
7. If R is a commutative ring with unity, then R is a ﬁeld (see §5.6) if and only if the
only ideals of R are R and {0}.
8. An ideal in a ring is the analogue of a normal subgroup in a group.
9. The second condition in the deﬁnition of ideal can be stated as rI ⊆I (I is a left
ideal) and Ir ⊆I (I is a right ideal). (If A is a subset of a ring R and r ∈R, then
rA = {ra | a ∈A} and Ar = {ar | a ∈A}.)
Examples:
1. With the usual deﬁnitions of + and · , each of the following rings can be viewed as a
subring of all the rings listed after it: Z, Q, R, C.
2. Gaussian integers:
Z[i] = {a + bi | a, b ∈Z} using the addition and multiplication
of C is a subring of the ring of complex numbers.
3. The ring Z is a subring of Z[
√
2] and Z[
√
2] is a subring of R.
4. Each set nZ (n an integer) is a principal ideal in the ring Z.
5.4.3
RING HOMOMORPHISM AND ISOMORPHISM
Deﬁnitions:
If R and S are rings, a function ϕ: R →S is a ring homomorphism if for all a, b ∈R
• ϕ(a + b) = ϕ(a) + ϕ(b)
(ϕ preserves addition)
• ϕ(ab) = ϕ(a)ϕ(b).
(ϕ preserves multiplication)
Note: ϕ(a) is sometimes written aϕ.
If a ring homomorphism ϕ is also one-to-one and onto S, then ϕ is a ring isomorphism
and R and S are isomorphic, written R ∼= S.
A ring endomorphism is a ring homomorphism ϕ: R →R.
A ring automorphism is a ring isomorphism ϕ: R →R.
The kernel of a ring homomorphism ϕ: R →S is ϕ−1(0) = {x ∈R | ϕ(x) = 0}.

Section 5.4
RINGS
351
Facts:
1. If ϕ is a ring isomorphism, then ϕ−1 is a ring isomorphism.
2. The kernel of a ring homomorphism from R to S is an ideal of the ring R.
3. If ϕ: R →S is a ring homomorphism, then ϕ(R) is a subring of S.
4. If ϕ: R →S is a ring homomorphism and R has unity, then either ϕ(1) = 0 or ϕ(1)
is unity for ϕ(R).
5. If ϕ is a ring homomorphism, then ϕ(0) = 0 and ϕ(−a) = −ϕ(a).
6. A ring homomorphism is a ring isomorphism between R and ϕ(R) if and only if the
kernel of ϕ is {0}.
7. Homomorphisms preserve subrings:
Let ϕ: R →S be a ring homomorphism. If A
is a subring of R, then ϕ(A) is a subring of S. If B is a subring of S, then ϕ−1(B) is a
subring of R.
8. Homomorphisms preserve ideals:
Let ϕ: R →S be a ring homomorphism. If A is
an ideal of R, then ϕ(A) is an ideal of S. If B is an ideal of S, then ϕ−1(B) is an ideal
of R.
Examples:
1. The function ϕ: Z →Zn deﬁned by the rule ϕ(a) = a mod n is a ring homomor-
phism.
2. If R and S are rings, then the function ϕ: R →S deﬁned by the rule ϕ(a) = 0 for
all a ∈R is a ring homomorphism.
3. The function ϕ: Z →R (R any ring with unity) deﬁned by the rule ϕ(x) = x · 1 is a
ring homomorphism. The kernel of ϕ is the subring nZ for some nonnegative integer n,
called the characteristic of R.
4. Let P(S) be the ring of all subsets of a set S (see §5.4.1). If |S| = 1, then P(S) ∼= Z2
with the ring isomorphism ϕ where ϕ(∅) = 0 and ϕ(S) = 1. More generally, if |S| = n,
then P(S) ∼= Zn
2 = Z2 × · · · × Z2.
5. Zn ∼= Z/(n) for all positive integers n. (See §5.4.4.)
6. Zm × Zn ∼= Zmn, if m and n are relatively prime.
5.4.4
QUOTIENT RINGS
Deﬁnitions:
If I is an ideal in a ring R and a ∈R, then the set a + I = {a + x | x ∈I} is a coset
of I in R.
The set of all cosets, R/I = {a + I | a ∈R}, is a ring, called the quotient ring, where
addition and multiplication are deﬁned by the following rules:
• (a + I) + (b + I) = (a + b) + I;
• (a + I) · (b + I) = (ab) + I.
Facts:
1. If R is commutative, then R/I is commutative.
2. If R has unity 1, then R/I has the coset 1 + I as unity.

352
Chapter 5
ALGEBRAIC STRUCTURES
3. If I is an ideal in ring R, the function ϕ: R →R/I deﬁned by the rule ϕ(x) = x + I
is a ring homomorphism, called the natural map. The kernel of ϕ is I.
4. Fundamental homomorphism theorem!for rings: If ϕ is a ring homomorphism and K
is the kernel of ϕ, then ϕ(R) ∼= R/K.
5. If R is a commutative ring with unity and I is an ideal in R, then I is a maximal
ideal if and only if R/I is a ﬁeld (see §5.6).
Examples:
1. For each integer n, Z/nZ is a quotient ring, isomorphic to Zn.
2. See §5.6.1 for Galois rings.
5.4.5
RINGS WITH ADDITIONAL PROPERTIES
Beginning with rings, as additional requirements are added, the following hierarchy of
sets of algebraic structures is obtained:
rings
⊃
commutative
rings with
unity
⊃
integral
domains
⊃
Euclidean
domains
⊃
principal
ideal
domains
Deﬁnitions:
The cancellation properties in a ring R state that for all a, b, c ∈R
if ab = ac and a ̸= 0, then b = c
(left cancellation property)
if ba = ca and a ̸= 0, then b = c
(right cancellation property).
Let R be a ring and let a, b ∈R where a ̸= 0, b ̸= 0. If ab = 0, then a is a left divisor
of zero and b is a right divisor of zero.
An integral domain is a commutative ring with unity that has no zero divisors.
A principal ideal domain (PID) is an integral domain in which every ideal is a
principal ideal.
A division ring is a ring with unity in which every nonzero element is a unit (i.e., every
nonzero element has a multiplicative inverse).
A ﬁeld is a commutative ring with unity such that each nonzero element has a multi-
plicative inverse. (See §5.6.)
A Euclidean norm on an integral domain R is a function δ: R −{0} →{0, 1, 2, . . .}
such that
• δ(a) ≤δ(ab) for all a, b ∈R −{0};
• the following generalization of the division algorithm for integers holds: for all
a, d ∈R where d ̸= 0, there are elements q, r ∈R such that a = dq + r, where
either r = 0 or δ(r) < δ(d).
A Euclidean domain is an integral domain with a Euclidean norm deﬁned on it.

Section 5.5
POLYNOMIAL RINGS
353
Facts:
1. The cancellation properties hold in an integral domain.
2. Every ﬁnite integral domain is a ﬁeld.
3. Every integral domain can be imbedded in a ﬁeld. Given an integral domain R, there
is a ﬁeld F and a ring homomorphism ϕ: R →F such that ϕ(1) = 1.
4. A ring with unity is a division ring if and only if the nonzero elements form a group
under the multiplication deﬁned on the ring.
5. Wedderburn’s theorem: Every ﬁnite division ring is a ﬁeld. (J. H. M. Wedderburn,
1882–1948)
6. Every commutative division ring is a ﬁeld.
7. In a Euclidean domain, if b ̸= 0 is not a unit, then δ(ab) > δ(a) for all a ̸= 0. For
b ̸= 0, b is a unit in R if and only if δ(b) = δ(1).
8. In every Euclidean domain, a Euclidean algorithm for ﬁnding the gcd can be carried
out.
Examples:
1. Some common Euclidean domains are given in the following table.
set
Euclidean norm
Z
δ(a) = |a|
Z[i] (Gaussian integers)
δ(a + bi) = a2 + b2
F (any ﬁeld)
δ(a) = 1
polynomial ring F[x] (F any ﬁeld)
δ(p(x)) = degree of p(x)
2. The following table gives examples of rings with additional properties.
commuta-
integral
principal
Euclidean
division
ring
tive ring
domain
ideal
domain
ring
ﬁeld
with unity
domain
Z
yes
yes
yes
yes
no
no
Q, R, C
yes
yes
yes
yes
yes
yes
Zp (p prime)
yes
yes
yes
yes
yes
yes
Zn (n composite)
yes
no
no
no
no
no
real quaternions
no
no
no
no
yes
no
Z[x]
yes
no
no
no
no
no
Mn×n
no
no
no
no
no
no
5.5
POLYNOMIAL RINGS
5.5.1
BASIC CONCEPTS
Deﬁnitions:
A polynomial in the variable x over a ring R is an expression of the form
f(x) = anxn + an−1xn−1 + · · · + a1x1 + a0x0

354
Chapter 5
ALGEBRAIC STRUCTURES
where an, . . . , a0 ∈R.
For a polynomial f(x) ̸= 0, the largest integer k such that ak ̸= 0 is the degree of f(x),
written deg f(x).
A constant polynomial is a polynomial f(x) = a0. If a0 ̸= 0, f(x) has degree 0. If
f(x) = 0 (the zero polynomial), the degree of f(x) is undeﬁned. (The degree of the
zero polynomial is also said to be −∞.)
The polynomial ring (in one variable x) over a ring R consists of the set
R[x] = {f(x) | f(x) is a polynomial over R in the variable x}
with addition and multiplication deﬁned by the following rules:
• (anxn + · · · + a1x1 + a0x0) + (bmxm + · · · + b1x1 + b0x0)
= anxn + · · · + am+1xm+1 + (an + bn)xn + · · · + (a1 + b1)x1 + (a0 + b0)x0
if n ≥m, and
• (anxn + · · · + a1x1 + a0x0)(bmxm + · · · + b1x1 + b0x0)
= cn+mxn+m + · · · + c1x1 + c0x0
where ci = a0bi + a1bi−1 + · · · + aib0 for i = 0, 1, . . . , m + n.
A polynomial f(x) ∈R[x] of degree n is monic if an = 1.
The value of a polynomial f(x) = anxn + an−1xn−1 + · · · + a1x1 + a0x0 at c ∈R is the
element f(c) = ancn + an−1cn−1 + · · · + a1c + a0 ∈R.
An element c ∈R is a zero of the polynomial f(x) if f(c) = 0.
If R is a subring of a commutative ring S, an element a ∈S is algebraic over R if there
is a nonzero f(x) ∈R[x] such that f(a) = 0.
If p(x) is not algebraic over R, then p(x) is transcendental over R.
A polynomial f(x) ∈R[x] of degree n is irreducible over R if f(x) cannot be written
as f1(x)f2(x) (factors of f(x)) where f1(x) and f2(x) are polynomials over R of degrees
less than n. Otherwise f(x) is reducible over R.
The polynomial ring (in the variables x1, x2, . . . , xn with n > 1) over a ring R is deﬁned
by the rule R[x1, x2, . . . , xn] = (R[x1, x2, . . . , xn−1])[xn].
Facts:
1. Polynomials over an arbitrary ring R generalize polynomials with coeﬃcients in R
or C. The notation and terminology follow the usual conventions for polynomials with
real (or complex) coeﬃcients:
• the elements an, . . . , a0 are coeﬃcients;
• subtraction notation can be used: aixi + (−aj)xj = aixi −ajxj;
• the term 1xi can be written as xi;
• the term x1 can be written x;
• the term x0 can be written 1;
• terms 0xi can be omitted.
2. There is a distinction between a polynomial f(x) ∈R[x] and the function it deﬁnes
using the rule f(c) = ancn + an−1cn−1 + · · · + a1c + a0 for c ∈R. The same function
might be deﬁned by inﬁnitely many polynomials. For example, the polynomials f1(x) =
x ∈Z2[x] and f2(x) = x2 ∈Z2[x] deﬁne the same function: f1(0) = f2(0) = 0 and
f1(1) = f2(1) = 1.

Section 5.5
POLYNOMIAL RINGS
355
3. If R is a ring, R[x] is a ring.
4. If R is a commutative ring, then R[x] is a commutative ring.
5. If R is a ring with unity, then R[x] has the constant polynomial f(x) = 1 as unity.
6. If R is an integral domain, then R[x] is an integral domain. If f1(x) has degree m
and f2(x) has degree n, then the degree of f1(x)f2(x) is m + n.
7. If ring R is not an integral domain, then R[x] is not an integral domain. If f1(x)
has degree m and f2(x) has degree n, then the degree of f1(x)f2(x) can be smaller than
m + n. (For example, in Z6[x], (3x2)(2x3) = 0.)
8. Factor theorem:
If R is a commutative ring with unity and f(x) ∈R[x] has de-
gree ≥1, then f(a) = 0 if and only if x −a is a factor of f(x).
9. If R is an integral domain and p(x) ∈R[x] has degree n, then p(x) has at most n
zeros in R. If R is not an integral domain, then a polynomial may have more zeros than
its degree; for example, x2 + x ∈Z6[x] has four zeros — 0, 2, 3, 5.
5.5.2
POLYNOMIALS OVER A FIELD
Facts:
1. Even though F is a ﬁeld (§5.6.1), F[x] is never a ﬁeld. (The polynomial f(x) = x
has no multiplicative inverse in F[x].)
2. If f(x) has degree n, then f(x) has at most n distinct zeros.
3. Irreducibility over a ﬁnite ﬁeld: If F is a ﬁnite ﬁeld and n is a positive integer, then
there is an irreducible polynomial over F of degree n.
4. Unique factorization theorem:
If f(x) is a polynomial over a ﬁeld F and is not the
zero polynomial, then f(x) can be uniquely factored (ignoring the order in which the
factors are written) as af1(x) · · · fk(x) where a ∈F and each fi(x) is a monic polynomial
that is irreducible over F.
5. Eisenstein’s irreducibility criterion:
If f(x) ∈Z[x] has degree n > 0, if there is
a prime p such that p divides every coeﬃcient of f(x) except an, and if p2 does not
divide a0, then f(x) is irreducible over Q. (Gotthold Eisenstein, 1823–1852)
6. Division algorithm for polynomials:
If F is a ﬁeld with a(x), d(x) ∈F[x] and d(x)
is not the zero polynomial, then there are unique polynomials q(x) (quotient) and r(x)
(remainder) in F[x] such that a(x) = d(x)q(x) + r(x) where deg r(x) < deg d(x) or
r(x) = 0. If d(x) is monic, then the division algorithm for polynomials can be extended
to all rings with unity.
7. Irreducibility over the real numbers R:
If f(x) ∈R[x] has degree at least 3, then
f(x) is reducible. The only irreducible polynomials in R[x] are of degree 1 or 2; for
example x2 + 1 is irreducible over R.
8. Fundamental theorem of algebra (irreducibility over the complex numbers C):
If
f(x) ∈C[x] has degree n ≥1, then f(x) can be completely factored:
f(x) = c(x −c1)(x −c2) . . . (x −cn)
where c, c1, . . . , cn ∈C.
9. If F is a ﬁeld and f(x) ∈F[x] has degree 1 (i.e., f(x) is linear), then f(x) is irre-
ducible.

356
Chapter 5
ALGEBRAIC STRUCTURES
10. If F is a ﬁeld and f(x) ∈F[x] has degree ≥2 and has a zero, then f(x) is reducible.
(If f(x) has a as a zero, then f(x) can be written as (x −a)f1(x) where deg f1(x) =
deg f(x)−1.) The converse is false: a polynomial may have no zeros, but still be reducible.
(See Example 2.)
11. If F is a ﬁeld and f(x) ∈F[x] has degree 2 or 3, then f(x) is irreducible if and only
if f(x) has no zeros.
Examples:
1. In Z5[x], if a(x) = 3x4 + 2x3 + 2x + 1 and d(x) = x2 + 2, then q(x) = 3x2 + 2x + 4
and r(x) = 3x + 3. To obtain q(x) and r(x), use the same format as for long division of
natural numbers, with arithmetic operations carried out in Z5:
3x2 + 2x + 4
x2 + 2

3x4 + 2x3 + 0x2 + 2x + 1
3x4
+ x2
2x3 + 4x2
[−x2 = 4x2 over Z5]
2x3
+ 4x
4x2 + 3x
[2x −4x = −2x = 3x over Z5]
4x2
+ 3
3x + 3
2. Polynomials can have no zeros, but be reducible. The polynomial f(x) = x4+x2+1 ∈
Z2[x] has no zeros (since f(0) = f(1) = 1), but f(x) can be factored as (x2 + x + 1)2.
Similarly, x4 + 2x2 + 1 = (x2 + 1)2 ∈R[x].
5.6
FIELDS
5.6.1
BASIC CONCEPTS
Deﬁnitions:
A ﬁeld (F, +, ·) consists of a set F together with two binary operations, + and ·, such
that
•
(F, +, ·) is a ring;
•
(F −{0}, ·) is a commutative group.
A subﬁeld F of ﬁeld (K, +, ·) is a subset of K that is a ﬁeld using the same operations
as those in K.
If F is a subﬁeld of K, then K is called an extension ﬁeld of F. Write K/F to indicate
that K is an extension ﬁeld of F.
For K an extension ﬁeld of F, the degree of K over F is [K : F] = the dimension of K
as a vector space over F. (See §6.1.3.)
A ﬁeld isomorphism is a function ϕ: F1 →F2, where F1 and F2 are ﬁelds, such that ϕ
is one-to-one, onto F2, and satisﬁes the following for all a, b ∈F1:
• ϕ(a + b) = ϕ(a) + ϕ(b);

Section 5.6
FIELDS
357
• ϕ(ab) = ϕ(a)ϕ(b).
A ﬁeld automorphism is an isomorphism ϕ: F →F, where F is a ﬁeld. The set of all
automorphisms of F is denoted Aut(F).
The characteristic of a ﬁeld F is the smallest positive integer n such that 1+· · ·+1 = 0,
where there are n summands. If there is no such integer, F has characteristic 0 (also
called characteristic ∞).
Facts:
1. Every ﬁeld is a commutative ring with unity.
A ﬁeld satisﬁes all properties of a
commutative ring with unity, and has the additional property that every nonzero element
has a multiplicative inverse.
2. Every ﬁnite integral domain is a ﬁeld.
3. A ﬁeld is a commutative division ring.
4. If F is a ﬁeld and a, b ∈F where a ̸= 0, then ax + b = 0 has a unique solution in F.
5. If F is a ﬁeld, every ideal in F[x] is a principal ideal.
6. If p is a prime and n is any positive integer, then there is exactly one ﬁeld (up to
isomorphism) with pn elements, the Galois ﬁeld GF(pn). (§5.6.1)
7. If ϕ: F →F is a ﬁeld automorphism, then
• −ϕ(a) = ϕ(−a)
• ϕ(a−1) = ϕ(a)−1
for all a ̸= 0.
8. The intersection of all subﬁelds of a ﬁeld F is a ﬁeld, called the prime ﬁeld of F.
9. If F is a ﬁeld, Aut(F) is a group under composition of functions.
10. The characteristic of a ﬁeld is either 0 or prime.
11. Every ﬁeld of characteristic 0 is isomorphic to a ﬁeld that is an extension of Q and
has Q as its prime ﬁeld.
12. Every ﬁeld of characteristic p > 0 is isomorphic to a ﬁeld that is an extension of Zp
and has Zp as its prime ﬁeld.
13. If ﬁeld F has characteristic p > 0, then (a + b)p = ap + bp for all a, b ∈F.
14. If ﬁeld F has characteristic p > 0, f(x) ∈Zp[x], and α ∈F is a zero of f(x), then
αp, αp2, αp3, . . . are also zeros of f(x).
15. If p is not a prime, then Zp is not a ﬁeld since Zp −{0} will fail to be closed under
multiplication. For example, Z6 is not a ﬁeld since 2 ∈Z6 −{0} and 3 ∈Z6 −{0}, but
2 · 3 = 0 /∈Z6 −{0}.
Examples:
1. The following table gives several examples of ﬁelds.

358
Chapter 5
ALGEBRAIC STRUCTURES
set and operations
−a
a−1
charac-
order
teristic
Q, R, C, with usual addition
−a
1/a
0
inﬁnite
and multiplication
Zp = {0, 1, . . ., p−1} (p prime),
p −a
a−1 = b, where
addition and multiplication
(−0 = 0)
ab mod p = 1
p
p
mod p
F[x]/(f(x)), f(x) irreducible
−[a+(f(x))]=
[a+(f(x))]−1=
over ﬁeld F, coset addition and
−a+(f(x))
a−1+(f(x))
varies
varies
multiplication (Example 2)
GF(pn)=Zp[x]/(f(x)), f(x) of
p
pn
degree n irreducible over Zp (p
−[a+(f(x))]=
[a+(f(x))]−1=
prime), addition and multipli-
−a+(f(x))
a−1+(f(x))
cation of cosets (Galois ﬁeld)
2. The ﬁeld F[x]/(f(x)):
If F is any ﬁeld and f(x) ∈F[x] of degree n is irreducible
over F, the quotient ring structure F[x]/(f(x)) is a ﬁeld. The elements of F[x]/(f(x)) are
cosets of polynomials in F[x] modulo f(x), where (f(x)) is the principal ideal generated
by f(x). Polynomials f1(x) and f2(x) lie in the same coset if and only if f(x) is a factor
of f1(x) −f2(x).
Using the division algorithm for polynomials, any polynomial g(x) ∈F[x] can be written
as g(x) = f(x)q(x) + r(x) where q(x) and r(x) are unique polynomials in F[x] and
r(x) has degree < n. The equivalence class g(x) + (f(x)) can be identiﬁed with the
polynomial r(x), and thus F[x]/(f(x)) can be regarded as the ﬁeld of all polynomials in
F[x] of degree < n.
5.6.2
EXTENSION FIELDS AND GALOIS THEORY
Throughout this subsection assume that ﬁeld K is an extension of ﬁeld F.
Deﬁnitions:
For α ∈K, F(α) is the smallest ﬁeld containing α and F, called the ﬁeld extension of
F by α.
For α1, . . . , αn ∈K, F(α1, . . . , αn) is the smallest ﬁeld containing α1, . . . , αn and F,
called the ﬁeld extension of F by α1, . . . , αn.
If K is an extension ﬁeld of F and α ∈K, then α is algebraic over F if α is a root of a
nonzero polynomial in F[x]. If α is not the root of any nonzero polynomial in F[x], then
α is transcendental over F.
A complex number is an algebraic number if it is algebraic over Q.
An algebraic integer is an algebraic number α that is a zero of a polynomial of the
form xn + an−1xn−1 + · · · + a1x + a0 where each ai ∈Z.
An extension ﬁeld K of F is an algebraic extension of F if every element of K is
algebraic over F. Otherwise K is a transcendental extension of F.

Section 5.6
FIELDS
359
An extension ﬁeld K of F is a ﬁnite extension of F if K is ﬁnite-dimensional as a
vector space over F (see Fact 11). The dimension of K over F is written [K : F].
Let α be algebraic over a ﬁeld F. The minimal polynomial of α with respect to F
is the monic irreducible polynomial f(x) ∈F[x] of smallest degree such that f(α) = 0.
A polynomial f(x) ∈F[x] splits over K if f(x) = α(x −α1) . . . (x −αn) where α, α1,
. . . , αn ∈K.
K is a splitting ﬁeld (root ﬁeld) of a nonconstant f(x) ∈F[x] if f(x) splits over K
and K is the smallest ﬁeld with this property.
A polynomial f(x) ∈F[x] of degree n is separable if f(x) has n distinct roots in its
splitting ﬁeld.
K is a separable extension of F if every element of K is the root of a separable
polynomial in F[x].
K is a normal extension of F if K/F is algebraic and every irreducible polynomial in
F[x] with a root in K has all its roots in K (i.e., splits in K).
K is a Galois extension of F if K is a normal, separable extension of F.
A ﬁeld automorphism ϕ ﬁxes set S elementwise if ϕ(x) = x for all x ∈S.
The ﬁxed ﬁeld of a subset A ⊆Aut(F) is FA = {x ∈F | ϕ(x) = x for all ϕ ∈A}.
The Galois group of K over F is the group of automorphisms G(K/F) of K that ﬁx F
elementwise. If K is a splitting ﬁeld of f(x) ∈F[x], G(K/F) is also known as the Galois
group of f(x). (´Evariste Galois, 1811–1832)
Facts:
1. The elements of K that are algebraic over F form a subﬁeld of K.
2. The algebraic numbers in C form a ﬁeld; the algebraic integers form a subring of C,
called the ring of algebraic integers.
3. Every nonconstant polynomial has a unique splitting ﬁeld, up to isomorphism.
4. If f(x) ∈F[x] splits as α(x −α1) . . . (x −αn), then the splitting ﬁeld for f(x) is
F(α1, . . . , αn).
5. If F is a ﬁeld and p(x) ∈F[x] is a nonconstant polynomial, then there is an extension
ﬁeld K of F and α ∈K such that p(α) = 0.
6. If f(x) is irreducible over F, then the ring F[x]/(f(x)) is an algebraic extension of F
and contains a root of f(x).
7. The ﬁeld F is isomorphic to a subﬁeld of any algebraic extension F[x]/(f(x)). The
element 0 ∈F corresponds to the coset of the zero polynomial; all other elements of F
appear in F[x]/(f(x)) as cosets of the constant polynomials.
8. Every minimal polynomial is irreducible.
9. If K is a ﬁeld extension of F and α ∈K is a root of an irreducible polynomial
f(x) ∈F[x] of degree n ≥1, then F(α) = {cn−1αn−1 + · · · + c1α + c0 | ci ∈F for all i}.
10. If K is an extension ﬁeld of F and α ∈K is algebraic over F, then
• there is a unique monic irreducible polynomial f(x) ∈F[x] of smallest degree (the
minimum polynomial) such that f(α) = 0;
• F(α) ∼= F[x]/(f(x));

360
Chapter 5
ALGEBRAIC STRUCTURES
• if the degree of α over F is n, then K = {a0 + a1α + a2α2 + · · · + an−1αn−1 |
a0, a1, . . . , an−1 ∈F}; in fact, K is an n-dimensional vector space over F, with
basis 1, α, α2, . . . , αn−1.
11. If K is an extension ﬁeld of F and x ∈K is transcendental over F, then F(α) ∼=
the ﬁeld of all fractions f(x)/g(x) where f(x), g(x) ∈F[x] and g(x) is not the zero
polynomial.
12. K is a splitting ﬁeld of some polynomial f(x) ∈F[x] if and only if K is a Galois
extension of F.
13. If K is a splitting ﬁeld for separable f(x) ∈F[x] of degree n, then G(K, F) is
isomorphic to a subgroup of the symmetric group Sn.
14. If K is a splitting ﬁeld of f(x) ∈F[x], then
• every element of G(K/F) permutes the roots of f(x) and is completely determined
by its eﬀect on the roots of f(x);
• G(K/F) is isomorphic to a group of permutations of the roots of f(x).
15. If K is a splitting ﬁeld for separable f(x) ∈F[x], then |G(K/F)| = [K : F].
16. For [K : F] ﬁnite, K is a normal extension of F if and only if K is a splitting ﬁeld
of some polynomial in F[x].
17. The fundamental theorem of Galois theory: If K is a normal extension of F, where
F is either ﬁnite or has characteristic 0, then there is a one-to-one correspondence Φ
between the lattice of all ﬁelds K′, where F ⊆K′ ⊆K, and the lattice of all subgroups H
of the Galois group G(K/F):
Φ(K′) = G(K/K′)
and
Φ−1(H) = KH.
The correspondence Φ has the following properties:
• for ﬁelds K′ and K′′ where F ⊆K′ ⊆K and F ⊆K′′ ⊆K
K′ ⊆K′′ ←→Φ(K′′) ⊆Φ(K′).
That is, G(K/K′′) ⊆G(K/K′).
• Φ interchanges the operations meet and join for the lattice of subﬁelds and the
lattice of subgroups:
Φ(K′ ∧K′′) = G(K/K′) ∨G(K/K′′),
Φ(K′ ∨K′′) = G(K/K′) ∧G(K/K′′).
(Note: In the lattice of ﬁelds [groups], A ∧B = A ∩B and A ∨B is the smallest
ﬁeld [group] containing A and B.)
• K′ is a normal extension of F if and only if G(K/K′) is a normal subgroup of
G(K/F).
18. Formulas for solving polynomial equations of degrees 2, 3, or 4:
• second-degree (quadratic) equation ax2 +bx+c = 0: the quadratic formula gives
the solutions −b ±
√
b2 −4ac
2a
;
• third-degree (cubic) equation a3x3 + a2x2 + a1x + a0 = 0:
(1) divide by a3 to obtain x3 + b2x2 + b1x + b0 = 0,

Section 5.6
FIELDS
361
(2) make the substitution x = y −b2
3 to obtain an equation of the form
y3 + cy + d = 0, with solutions y =
3
r
−d
2 +
q
d2
4 + c3
27 +
3
r
−d
2 −
q
d2
4 + c3
27,
(3) use the substitution x = y −b2
3 to obtain the solutions to the original
equation;
• fourth-degree (quartic) equation a4x4 + a3x3 + a2x2 + a1x + a0 = 0:
(1) divide by a4 to obtain x4 + ax3 + bx2 + cx + d = 0,
(2) solve the resolvent equation y3 −bys + (ac −4d)y + (−a2d + 4bd −c2) = 0
to obtain a root z,
(3) solve the pair of quadratic equations:
x2 + a
2x + z
2 = ±
r
a2
4 −b + z

x2 +

a
2z −c

x +

z2
4 −d

to obtain the solutions to the original equation.
19. A general method for solving cubic equations algebraically was given by Nicolo
Fontana (1500–1557), also called Tartaglia. The method is often referred to as Cardano’s
method because Girolamo Cardano (1501–1576) published the method. Ludovico Ferrari
(1522–1565), a student of Cardano, discovered a general method for solving quartic
equations algebraically.
20. Equations of degree 5 or more:
In 1824 Abel proved that the general quintic
polynomial equation a5x5+· · ·+a1x+a0 = 0 (and those of higher degree) are not solvable
by radicals; that is, there can be no formula for writing the roots of such equations using
only the basic arithmetic operations and the taking of nth roots. ´Evariste Galois (1811–
1832) demonstrated the existence of such equations that are not solvable by radicals
and related solvability by radicals of polynomial equations to determining whether the
associated permutation group (the Galois group) of roots is solvable. (See Application 1.)
Examples:
1. C as an algebraic extension of R:
Let f(x) = x2 + 1 ∈R[x] and α = x + (x2 +
1) ∈R[x]/(x2 + 1). Then α2 = −1. Thus, α behaves like i (since i2 = −1). Hence
R[x]/(x2 + 1) = {c1α + c0 | c1, c0 ∈R } ∼= {c1i + c0 | c0, c1 ∈R} = C.
2. Algebraic extensions of Zp:
If f(x) ∈Zp is an irreducible polynomial of degree n,
then the algebraic extension Zp[x]/(f(x)) is a Galois ﬁeld.
3. If f(x) = x4 −2x2 −3 ∈Q[x], its splitting ﬁeld is
Q(
√
3, i) = {a + b
√
3 + ci + di
√
3 | a, b, c, d ∈Q}.
There are three intermediate ﬁelds: Q(
√
3), Q(i), and Q(i
√
3), as illustrated below. The
Galois group G(Q(
√
3, i)/Q) = {e, φ1, φ2, φ3} where
φ1(a + b
√
3 + ci + di
√
3) = a + b
√
3 −ci −di
√
3,
φ2(a + b
√
3 + ci + di
√
3) = a −b
√
3 + ci −di
√
3,
φ3(a + b
√
3 + ci + di
√
3) = a −b
√
3 −ci + di
√
3 = φ2φ1 = φ1φ2,
e(a + b
√
3 + ci + di
√
3) = a + b
√
3 + ci + di
√
3.

362
Chapter 5
ALGEBRAIC STRUCTURES
G(Q(
√
3, i), Q) has the following subgroups:
G = G(Q(
√
3, i), Q) = {e, φ1, φ2, φ3},
H1 = G(Q(
√
3, i), Q(
√
3)) = {e, φ1},
H2 = G(Q(
√
3, i), Q(i)) = {e, φ2},
H3 = G(Q(
√
3, i), Q(i
√
3)) = {e, φ3},
{e} = G(Q(
√
3, i), Q(
√
3, i)).
The correspondence between ﬁelds and Galois groups is shown in the following table and
ﬁgure.
ﬁeld
Galois group
Q(
√
3, i)
{e}
Q(
√
3)
H1
Q(i
√
3)
H3
Q(i)
H2
Q
G
Q(√3,i)
Q(√3)
Q(i√3)
Q(i)
Q
{e}
G
H1
H3
H2
4. Cyclotomic extensions:
The nth roots of unity are the solutions to xn −1 = 0:
1, ω, ω2, . . . , ωn−1, where ω = e2πi/n. The extension ﬁeld Q(ω) is a cyclotomic extension
of Q. If p > 2 is prime, then G(Q(ω), Q) is a cyclic group of order p−1 and is isomorphic
to Z∗
p (the multiplicative group of nonzero elements of Zp).
Applications:
1. Solvability by radicals:
A polynomial equation f(x) = 0 is solvable by radicals if
each root can be expressed in terms of the coeﬃcients of the polynomial, using only the
operations of addition, subtraction, multiplication, division, and the taking of nth roots.
If F is a ﬁeld of characteristic 0 and f(x) ∈F[x] has K as splitting ﬁeld, then f(x) = 0 is
solvable by radicals if and only if G(K, F) is a solvable group. Since there are polynomials
whose Galois groups are not solvable, there are polynomials whose roots cannot be found
by elementary algebraic methods. For example, the polynomial x5 −36x + 2 has the
symmetric group S5 as its Galois group, which is not solvable.
Hence, the roots of
x5 −36x+ 2 = 0 cannot be found by elementary algebraic methods. This example shows
that there can be no algebraic formula for solving all ﬁfth-degree equations.
2. Straightedge and compass constructibility:
Using only a straightedge (unmarked
ruler) and a compass, there is no general method for
• trisecting angles (given an angle whose measure is α, to construct an angle with
measure α
3 );
• duplicating the cube (given the side of a cube C1, to construct the side of a
cube C2 that has double the volume of C1);

Section 5.6
FIELDS
363
• squaring the circle (given a circle of area A, to construct a square with area A);
• constructing a regular n-gon for all n ≥3.
Straightedge and compass constructions yield only lengths that can be obtained by ad-
dition, subtraction, multiplication, division, and taking square roots. Beginning with
lengths that are rational numbers, each of these operations yields ﬁeld extensions Q(a)
and Q(b) where a and b are coordinates of a point constructed from points in Q × Q.
These operations force [Q(a): Q] and [Q(b): Q] to be powers of 2. However, trisecting
angles, duplicating cubes, and squaring circles all yield extensions of Q such that the
degrees of the extensions are not powers of 2. Hence these three types of constructions
are not possible with straightedge and compass.
5.6.3
FINITE FIELDS
Finite ﬁelds have a wide range of applications in various areas of computer science and
engineering: coding theory, combinatorics, computer algebra, cryptology, generation of
pseudorandom numbers, switching circuit theory, and symbolic computation.
Throughout this subsection assume that F is a ﬁnite ﬁeld.
Deﬁnitions:
A ﬁnite ﬁeld is a ﬁeld with a ﬁnite number of elements.
The Galois ﬁeld GF(pn) is the algebraic extension Zp[x]/(f(x)) of the ﬁnite ﬁeld Zp
where p is a prime and f(x) is an irreducible polynomial over Zp of degree n.
(See
Fact 1.)
A primitive element of GF(pn) is a generator of the cyclic group of nonzero elements
of GF(pn) under multiplication.
Let α be a primitive element of GF(pn). The discrete exponential function (with base
α) is the function expα : {0, 1, 2, . . ., pn −2} →GF(pn)∗deﬁned by the rule expαk = αk.
Let α be a primitive element of GF(pn). The discrete logarithm or index function
(with base α) is the function indα : GF(pn)∗→{0, 1, 2, . . ., pn −2} where indα(x) = k if
and only if x = αk.
Let α be a primitive element of GF(pn). The Zech logarithm (Jacobi logarithm) is
the function Z : {1, . . ., pn −1} →{0, . . . , pn −2} such that αZ(k) = 1+αk; if 1+αk = 0,
then Z(k) = 0.
Facts:
1. Existence of ﬁnite ﬁelds:
For each prime p and positive integer n there is exactly
one ﬁeld (up to isomorphism) with pn elements—the ﬁeld GF(pn), also written Fpn.
2. Construction of ﬁnite ﬁelds:
Given an irreducible polynomial f(x) ∈Zp[x] of de-
gree n and a zero α of f(x),
GF(pn) ∼= Zp[x]/(f(x)) ∼= {cn−1αn−1 + · · · + c1α + c0 | ci ∈Zp for all i}.
3. If F is a ﬁnite ﬁeld, then
• F has pn elements for some prime p and positive integer n;
• F has characteristic p for some prime p;

364
Chapter 5
ALGEBRAIC STRUCTURES
• F is an extension of Zp.
4. [GF(pn): Zp] = n.
5. GF(pn) = the ﬁeld of the pn roots of xpn −x ∈Zp[x].
6. The minimal polynomial of α ∈GF(pn) with respect to Zp is
f(x) = (x −α)(x −αp)(x −αp2) . . . (x −αpi)
where i is the smallest positive integer such that αpi+1 = α.
7. If a ﬁeld F has order pn, then every subﬁeld of F has order pk for some k that divides
n.
8. The multiplicative group of nonzero elements of a ﬁnite ﬁeld F is a cyclic group.
9. If a ﬁeld F has m elements, then the multiplicative order of each nonzero element of
F is a divisor of m −1.
10. If a ﬁeld F has m elements and d is a divisor of m −1, then there is an element
of F of order d.
11. Each discrete logarithm function has the following properties:
indα(xy) ≡indαx + indαy (mod pn −1);
indα(xy−1) ≡indαx −indαy (mod pn −1);
indα(xk) ≡k indαx (mod pn −1).
12. The discrete logarithm function indα is the inverse of the discrete exponential func-
tion expα. That is, indαx = y if and only if expαy = x.
13. A discrete logarithm function can be used to facilitate multiplication and division
of elements of GF(pn).
14. The Zech logarithm facilitates the addition of elements αi and αj (i > j) in GF(pn),
since αi + αj = αj(αi−j + 1) = αj · αZ(i−j) = αj+Z(i−j). (Note that the values of the
Zech logarithm function depend on the primitive element used.)
15. There are 1
k
Q
d|k
µ( k
d)pnd irreducible polynomials of degree k over GF(pn), where µ
is the M¨obius function (§2.7).
Examples:
1. If p is prime, Zp is a ﬁnite ﬁeld and Zp ∼= GF(p).
2. The ﬁeld Z2 = F2:
+
0
1
0
0
1
1
1
0
·
0
1
0
0
0
1
0
1
3. The ﬁeld Z3 = F3:
+
0
1
2
0
0
1
2
1
1
2
0
2
2
0
1
·
0
1
2
0
0
0
0
1
0
1
2
2
0
2
1
4. Construction of GF(22) = F4:
GF(22) = Z2[x]/(x2 + x + 1) = {c1α + c0 | c1, c0 ∈Z2} = {0, 1, α, α + 1}

Section 5.6
FIELDS
365
where α is a zero of x2 + x + 1; i.e., α2 + α + 1 = 0. The nonzero elements of GF(pn)
can also be written as powers of α as α, α2 = −α −1 = α + 1, α3 = α · α2 = α(α + 1) =
α2 + α = (α + 1) + α = 2α + 1 = 1.
Thus, GF(22) = {0, 1, α, α2} has the following addition and multiplication tables:
+
0
1
α
α2
0
0
1
α
α2
1
1
0
α2
α
α
α
α2
0
1
α2
α2
α
1
0
·
0
1
α
α2
0
0
0
0
0
1
0
1
α
α2
α
0
α
α2
1
α2
0
α2
1
α
5. Construction of GF(23) = F8:
Let f(x) = x3 + x + 1 ∈Z2[x] and let α be a root of
f(x). Then GF(23) = {c2α2 + c1α + c0 | c0, c1, c2 ∈Z2} where α3 + α + 1 = 0.
The elements of GF(23) (using α as generator) are
0,
α,
α2,
α3 = α + 1
α4 = α2 + α,
α5 = α2 + α + 1,
α6 = α2 + 1,
1 (= α7).
Multiplication is carried out using the ordinary rules of exponents and the fact that
α7 = 1. The following Zech logarithm values can be used to construct the table for
addition: Z(1) = 3, Z(2) = 6, Z(3) = 1, Z(4) = 5, Z(5) = 4, Z(6) = 2, Z(7) = 0. For
example α3 + α5 = α3 · αZ(5−3) = α3 · α6 = α9 = α2.
Using strings of 0s and 1s to represent the elements, 0 = 000, 1 = 001, α = 010,
α+1 = 011, α2 = 100, α2 +α = 110, α2 +1 = 101, α2 +α+1 = 111, yields the following
tables for addition and multiplication:
+
000
001
010
011
100
101
110
111
000
000
001
010
011
100
101
110
111
001
001
000
011
010
101
100
111
110
010
010
011
000
001
110
111
100
101
011
011
010
001
000
111
110
101
100
100
100
101
110
111
000
001
010
011
101
101
100
111
110
001
000
011
010
110
110
111
100
101
010
011
000
001
111
111
110
101
100
011
010
001
000
·
000
001
010
011
100
101
110
111
000
000
000
000
000
000
000
000
000
001
000
001
010
011
100
101
110
111
010
000
010
100
110
011
001
111
101
011
000
011
110
101
111
100
001
010
100
000
100
011
111
110
010
101
001
101
000
101
001
100
010
111
011
110
110
000
110
111
001
101
011
010
100
111
000
111
101
010
001
110
100
011
The same ﬁeld can be constructed using g(x) = x3 + x2 + 1 instead of f(x) = x3 + x + 1
and β as a root of g(x) (β3 + β2 + 1 = 0). The elements (using β as generator) are 0, β,
β2, β3 = β2 + 1, β4 = β2 + β + 1, β5 = β + 1, β6 = β2 + β, 1 (= β7).

366
Chapter 5
ALGEBRAIC STRUCTURES
The polynomial g(x) yields the following Zech logarithm values, which can be used to
construct the table for addition: Z(1) = 5, Z(2) = 3, Z(3) = 2, Z(4) = 6, Z(5) = 1,
Z(6) = 4, Z(7) = 0. This ﬁeld is isomorphic to the ﬁeld deﬁned using f(x) = x3 + x + 1.
6. The following table lists the irreducible polynomials of degree at most 8 in Z2[x].
Each polynomial is represented by the string of its coeﬃcients, beginning with the highest
power. For example, x3 + x + 1 is represented by 1011. For more extensive tables of
irreducible polynomials over certain ﬁnite ﬁelds, see [LiNi94].
degree 1:
10
11
degree 2:
111
degree 3:
1011
1101
degree 4:
10011
11001
11111
degree 5:
100101
101001
101111
110111
111011
111101
degree 6:
1000011
1001001
1010111
1011011
1100001
1100111
1101101
1110011
1110101
degree 7:
10000011
10001001
10001111
10010001
10011101
10100111
10101011
10111001
10111111
11000001
11001011
11010011
11010101
11100101
11101111
11110001
11110111
11111101
degree 8:
100011011
100011101
100101011
100101101
100111001
100111111
101001101
101011111
101100011
101100101
101101001
101110001
101110111
101111011
110000111
110001011
110001101
110011111
110100011
110101001
110110001
110111101
111000011
111001111
111010111
111011101
111100111
111110011
111110101
111111001
5.7
LATTICES
5.7.1
BASIC CONCEPTS
Deﬁnitions:
A lattice (L, ∨, ∧) is a nonempty set L closed under two binary operations ∨(join)
and ∧(meet) such that the following laws are satisﬁed for all a, b, c ∈L:
• associative laws: a ∨(b ∨c) = (a ∨b) ∨c and a ∧(b ∧c) = (a ∧b) ∧c;
• commutative laws: a ∨b = b ∨a and a ∧b = b ∧a;
• absorption laws: a ∨(a ∧b) = a and a ∧(a ∨b) = a.
Lattices L1 and L2 are isomorphic (as lattices) if there is a function ϕ: L1 →L2 that
is one-to-one, onto L2 and preserves ∨and ∧: ϕ(a ∨b) = ϕ(a) ∨ϕ(b) and ϕ(a ∧b) =
ϕ(a) ∧ϕ(b) for all a, b ∈L1.
L1 is a sublattice of lattice L if L1 ⊆L and L1 is a lattice using the same operations
as those used in L.

Section 5.7
LATTICES
367
The dual of a statement in a lattice is the statement obtained by interchanging the
operations ∨and ∧, and interchanging the elements 0 (lower bound) and 1 (upper bound).
(See §5.7.2.)
An order relation ≤can be deﬁned on a lattice so that a ≤b means that a ∨b = b, or,
equivalently, that a ∧b = a. Write a < b if a ≤b and a ̸= b.
Facts:
1. If L is a lattice and a, b ∈L, then a ∧b and a ∨b are unique.
2. Lattices as partially ordered sets:
Every lattice is a partially ordered set using the
order relation ≤. (See §1.4.3; also see Chapter 11 for extended coverage.)
3. Every partially ordered set L in which glb {a, b} and lub {a, b} exist for all a, b ∈L
can be regarded as a lattice by deﬁning a ∨b = lub {a, b} and a ∧b = glb {a, b}.
4. The duality principle holds in all lattices:
If a theorem is the consequence of the
deﬁnition of lattice, then the dual of the statement is also a theorem.
5. Lattice diagrams:
Every ﬁnite lattice can be pictured in a poset diagram (Hasse
diagram), called a lattice diagram.
6. Idempotent laws: a ∨a = a and a ∧a = a for all a ∈L.
Example:
1. The following table gives examples of lattices.
set
∨(join)
∧(meet)
N
a ∨b = lcm{a, b}
a ∧b = gcd{a, b}
N
a ∨b = max {a, b}
a ∧b = min {a, b}
Zn
2
(a1, . . . , an) ∨(b1, . . . , bn) =
(a1, . . . , an) ∨(b1, . . . , bn) =
(max (a1, b1), . . . , max (an, bn))
(min (a1, b1), . . . , min (an, bn))
all subgroups
H1 ∨H2 = the intersection of
of a group G
all subgroups of G containing
H1 ∧H2 = H1 ∩H2
H1 and H2
all subsets of set S
A1 ∨A2 = A1 ∪A2
A1 ∧A2 = A1 ∩A2
5.7.2
SPECIALIZED LATTICES
Deﬁnitions:
A lattice L is distributive if the following are true for all a, b, c ∈L:
• a ∧(b ∨c) = (a ∧b) ∨(a ∧c);
• a ∨(b ∧c) = (a ∨b) ∧(a ∨c).
A lower bound (smallest element, least element) in a lattice L is an element 0 ∈L
such that 0 ∧a = 0 (equivalently, 0 ≤a) for all a ∈L.
An upper bound (largest element, greatest element) in a lattice L is an element
1 ∈L such that 1 ∨a = 1 (equivalently, a ≤1) for all a ∈L.
A lattice L is bounded if L contains a lower bound 0 and an upper bound 1.
A lattice L is complemented if

368
Chapter 5
ALGEBRAIC STRUCTURES
• L is bounded;
• for each a ∈L there is an element b ∈L (called a complement of a) such that
a ∨b = 1 and a ∧b = 0.
An element a in a bounded lattice L is an atom if 0 < a and there is no element b ∈L
such that 0 < b < a.
Facts:
1. Each of the distributive properties in a lattice implies the other.
2. Not every lattice is distributive. (See Example 1.)
3. If a lattice is not distributive, it must contain a sublattice isomorphic to one of the
two lattices in the following ﬁgure.
e
d
b
a
c
b
c
a
d
e
L2
L1
4. Every ﬁnite lattice is bounded: if L = {a1, . . . , an}, then
1 = a1 ∨· · · ∨an
and
0 = a1 ∧· · · ∧an.
5. Some inﬁnite lattices are bounded, while others are not. (See Examples 2 and 3.)
6. In a complemented lattice, complements are not necessarily unique. See the lattice
in Example 4.
7. If L is a ﬁnite, complemented, distributive lattice and a ∈L, then there is exactly
one set of atoms {a1, . . . , ak} such that a = a1 ∨· · · ∨ak.
Examples:
1. Neither lattice in Fact 3 is distributive. For example, in lattice L1, d ∨(b ∧c) = d,
but (d ∨b) ∧(d ∨c) = b. And in L2, d ∨(b ∧c) = d, but (d ∨b) ∧(d ∨c) = a.
2. The lattice (N, ∨, ∧) where a ∨b = max (a, b) and a ∧b = min (a, b) is not bounded;
there is a lower bound (the integer 0), but there is no upper bound.
3. The following inﬁnite lattice is bounded. The element 1 is an upper bound and the
element 0 is a lower bound.
a1
a2
a3
1
0
4. The lattice in Example 3 is complemented, but complements are not unique in that
lattice. For example, the element a1 has a2, a3, . . . as complements.

Section 5.8
BOOLEAN ALGEBRAS
369
5. In lattice L1 of Fact 3, b and c are atoms. In the lattice of all subsets of a set S (see
Example 1), the atoms are the subsets of S of size 1.
5.8
BOOLEAN ALGEBRAS
Boolean algebra is a generalization of the algebra of sets and the algebra of logical
propositions. It forms an abstract model of the design of circuits.
5.8.1
BASIC CONCEPTS
Deﬁnitions:
A Boolean algebra (B, +, ·, ′, 0, 1) consists of a set B closed under two binary opera-
tions, + (addition) and · (multiplication), and one monadic operation, ′ (complementa-
tion), and having two distinct elements, 0 and 1, such that the following laws are true
for all a, b, c ∈B:
• commutative laws: a + b = b + a and a · b = b · a;
• distributive laws: a · (b + c) = (a · b) + (a · c) and a + (b · c) = (a + b) · (a + c);
• identity laws: a + 0 = a and a · 1 = a;
• complement laws: a + a′ = 1 and a · a′ = 0.
(George Boole, 1813–1864)
Notation:
It is common practice to omit the “ · ” symbol in a Boolean algebra, writing ab
instead of a · b. The complement operation is also written using an overline: x′ = x. By
convention, complementation is done ﬁrst, then multiplication, and ﬁnally addition. For
example, a + bc′ means a + (b(c′)).
The dual of a statement in a Boolean algebra is the statement obtained by interchanging
the operations + and ·, and interchanging the elements 0 and 1 in the original statement.
Boolean algebras B1 and B2 are isomorphic (as Boolean algebras) if there is a function
ϕ: B1 →B2 that is one-to-one and onto B2 such that for all a, b ∈B1
• ϕ(a + b) = ϕ(a) + ϕ(b);
• ϕ(ab) = ϕ(a)ϕ(b);
• ϕ(a′) = ϕ(a)′.
An element a ̸= 0 in a Boolean algebra is an atom if the following holds: if xa = x, then
either x = 0 or x = a; that is, if x ≤a, then either x = 0 or x = a (see Fact 1).
The binary operation NAND, written | , is deﬁned by a | b = (ab)′.
The binary operation NOR, written ↓, is deﬁned by a ↓b = (a + b)′.
The binary operation XOR, written ⊕, is deﬁned by a ⊕b = ab′ + a′b.

370
Chapter 5
ALGEBRAIC STRUCTURES
Facts:
1. Every Boolean algebra is a bounded, distributive, complemented lattice where a∨b =
a+b and a∧b = ab. Hence, every Boolean algebra is a partially ordered set (where a ≤b
if and only if a + b = b, or, equivalently, ab = a or a′ + b = 1 or ab′ = 0).
2. The duality principle holds in all Boolean algebras: if a theorem is the consequence
of the deﬁnition of Boolean algebra, then the dual of the theorem is also a theorem.
3. Structure of Boolean algebras: Every ﬁnite Boolean algebra is isomorphic to {0, 1}n
for some positive integer n. Hence every ﬁnite Boolean algebra has 2n elements. The
atoms are the n n-tuples of 0s and 1s with a 1 in exactly one position.
4. If B is a ﬁnite Boolean algebra and b ∈B (b ̸= 0), there is exactly one set of atoms
a1, . . . , ak such that b = a1 + · · · + ak.
5. If a Boolean algebra B has n atoms, then B has 2n elements.
6. The following laws hold in the Boolean algebra B, for all a, b, c ∈B:
• associative laws: a + (b + c) = (a + b) + c and a(bc) = (ab)c;
(Hence there is no ambiguity in writing a + b + c and abc.)
• idempotent laws: a + a = a, aa = a;
• absorption laws: a(a + b) = a, a + ab = a;
• domination (boundedness) laws: a + 1 = 1 and a0 = 0;
• double complement (involution) law: (a′)′ = a;
• De Morgan’s laws: (a + b)′ = a′b′ and (ab)′ = a′ + b′;
• uniqueness of complement: if a + b = 1 and ab = 0, then b = a′.
7. Since every Boolean algebra is a lattice, every ﬁnite Boolean algebra can be pictured
using a partially ordered set diagram. (§11.1)
Examples:
1. {0, 1} is a Boolean algebra, where addition, multiplication, and complementation are
deﬁned in the following tables:
+
0
1
0
0
1
1
1
1
·
0
1
0
0
0
1
0
1
x
x′
0
1
1
0
2. If S is any set, then P(S) (the set of all subsets of S) is a Boolean algebra where
A1 + A2 = A1 ∪A2,
A1 · A2 = A1 ∩A2,
A′ = A
and 0 = ∅and 1 = S.
3. Given n variables, the set of all compound propositions in these variables (identiﬁed
with their truth tables) is a Boolean algebra where
p + q = p ∨q
p · q = p ∧q
p = ¬p
and 0 is a contradiction (the truth table with only values F) and 1 is a tautology (the
truth table with only values T ).
4. If B is any Boolean algebra, then Bn = {(a1, . . . , an) | ai ∈B for all i} is a Boolean
algebra, where the operations are performed coordinatewise:

Section 5.8
BOOLEAN ALGEBRAS
371
(a1, . . . , an) + (b1, . . . , bn) = (a1 + b1, . . . , an + bn);
(a1, . . . , an) · (b1, . . . , bn) = (a1 · b1, . . . , an · bn);
(a1, . . . , an)′ = (a′
1, . . . , a′
n).
In this Boolean algebra 0 = (0, . . . , 0) and 1 = (1, . . . , 1).
5. The statements in each of the following pairs are duals of each other:
a + b = cd,
ab = c + d;
a + (b + c) = (a + b) + c,
a(bc) = (ab)c;
a + 1 = 1, a0 = 0.
5.8.2
BOOLEAN FUNCTIONS
Deﬁnitions:
A Boolean expression in the variables x1, . . . , xn is an expression deﬁned recursively
by
• 0, 1, and all variables xi are Boolean expressions in x1, . . . , xn;
• if E and F are Boolean expressions in the variables x1, . . . , xn, then (EF), (E+F),
and E′ are Boolean expressions in the variables x1, . . . , xn.
A Boolean function of degree n is a function f : {0, 1}n →{0, 1}.
A literal is a Boolean variable or its complement.
A minterm of the Boolean variables x1, . . . , xn is a product of the form y1 . . . yn where
for each i, yi is equal to xi or x′
i.
A maxterm of the Boolean variables x1, . . . , xn is a sum of the form y1 + · · ·+ yn where
for each i, yi is equal to xi or x′
i.
A Boolean function of degree n is in disjunctive normal form (DNF) (or sum-of-
products expansion) if it is written as a sum of distinct minterms in the variables
x1, . . . , xn. (Note: disjunctive normal form is sometimes called full disjunctive normal
form.)
A Boolean function is in conjunctive normal form (CNF) (or product-of-sums
expansion) if it is written as a product of distinct maxterms.
A set of operators in a Boolean algebra is functionally complete if every Boolean
function can be written using only these operators.
Facts:
1. Every Boolean function can be written as a Boolean expression.
2. There are 22n Boolean functions of degree n. Examples of the 16 diﬀerent Boolean
functions with two variables, x and y, are given in the following table.
x y 1 x + y x + y′ x′ + y x|y x y x ⊕y (x ⊕y)′ y′ x′ xy xy′ x′y x ↓y 0
1 1 1
1
1
1
0
1 1
0
1
0
0
1
0
0
0
0
1 0 1
1
1
0
1
1 0
1
0
1
0
0
1
0
0
0
0 1 1
1
0
1
1
0 1
1
0
0
1
0
0
1
0
0
0 0 1
0
1
1
1
0 0
0
1
1
1
0
0
0
1
0

372
Chapter 5
ALGEBRAIC STRUCTURES
3. Every Boolean function (not identically 0) can be written in disjunctive normal form.
Either of the following two methods can be used:
(a) Rewrite the expression for the function so that no parentheses remain. For each
term that does not have a literal for a variable xi, multiply that term by xi +x′
i.
Multiply out so that no parentheses remain. Use the idempotent law to remove
any duplicate terms or duplicate factors.
(b) Make a table of values for the function. For each row where the function has
the value 1, form a minterm that yields 1 in only that row. Form the sum of
these minterms.
4. Every Boolean function (not identically 1) can be written in conjunctive normal form.
Any of the following three methods can be used:
(a) Write the negation of the expression in disjunctive normal form. Use De Mor-
gan’s laws to take the negation of this expression.
(b) Make a table of values for the function. For each row where the function has
the value 0, form a minterm that yields 1 in only that row. Form the sum of
these minterms. Use De Morgan’s laws to take the complement of this sum.
(c) Make a table of values for the function. For each row where the function has
the value 0, form a maxterm that yields 0 in only that row. Form the product
of these maxterms.
5. The following are examples of functionally complete sets, with explanations showing
how any Boolean function can be written using only these operations:
• {+ , · , ′}
disjunctive normal form uses only the operators +, · , and ′ ;
• {+, ′ }
De Morgan’s law (a · b)′ = a′ + b′ allows the replacement of any
occurrence of a · b with an expression that does not use · ;
• {· , ′}
De Morgan’s law a + b = (a′ · b′)′ allows the replacement of any
occurrence of a + b with an expression that does not use + ;
• {|}
write the expression for any function in DNF; use a′ = a | a, a+b =
(a | a) | (b | b), and a · b = (a | b) | (a | b) to replace each occurrence of
′ , + , and · with | ;
• {↓}
write the expression for any function in DNF; use a′ = a ↓a,
a + b = (a ↓b) ↓(a ↓b), and a · b = (a ↓a) ↓(b ↓b) to replace each
occurrence of ′ , + , and · with ↓.
6. The set {+ , ·} is not functionally complete.
Examples:
1. The function f : {0, 1}3 →{0, 1} deﬁned by f(x, y, z) = x(z′ + y′z) + x′ is a Boolean
function in the Boolean variables x, y, z. Multiplying out the expression for this function
yields f(x, y, z) = xz′ + xy′z + x′. In this form the second term, xy′z, is a minterm in
the three variables x, y, z. The ﬁrst and third terms are not minterms: the ﬁrst term,
xz′, does not use a literal for y, and the third term, x′, does not use literals for y and z.
2. Writing a Boolean function in disjunctive normal form:
To write the function f
from Example 1 in DNF using Fact 3(a), replace the terms xz′ and x′ with equivalent
minterms by multiplying these terms by 1 (= a + a′) for each missing variable a:
xz′ = xz′ · 1 = xz′(y + y′) = xyz′ + xy′z′;
x′ = x′ · 1 · 1 = x′(y + y′)(z + z′) = x′yz + x′yz′ + x′y′z + x′y′z′.

Section 5.8
BOOLEAN ALGEBRAS
373
Therefore,
f(x, y, z) = x(z′ + y′z) + x′
= xz′ + xy′z + x′
= xyz′ + xy′z′ + xy′z + x′yz + x′yz′ + x′y′z + x′y′z′.
Alternatively, using Fact 3(b), the table of values for f yields 1 in all rows except the row
in which x = y = z = 1. Therefore minterms are obtained for the other rows, yielding
the same sum of seven minterms.
3. Writing a Boolean function in conjunctive normal form:
Using Fact 4(a) to write
the function f(x, y) = xy′ +x′y in CNF, ﬁrst rewrite the negation of f in DNF, obtaining
f ′(x, y) = xy + x′y′. The negation of f ′ is f ′′(x, y) = f(x, y) = (x′ + y′)(x + y).
Alternatively, using Fact 4(c), the function f has value 0 only when x = y = 1 and
x = y = 0. The maxterms that yield 0 in exactly one of these rows are x′ + y′ and x + y.
This yields the CNF f(x, y) = (x′ + y′)(x + y).
5.8.3
LOGIC GATES
Boolean algebra can be used to model circuitry, with 0s and 1s as inputs and outputs.
The elements of these circuits are gates that implement the Boolean operations.
Facts:
1. The following ﬁgure gives representations for the three standard Boolean operators,
+ , · , and
′ , together with representations for three related operators. (For example,
the AND gate takes two inputs, x and y, and produces one output, xy.)
2. Gates can be extended to include cases where there are more than two inputs. The
ﬁgure of Fact 1 also shows an AND gate and an OR gate with multiple inputs. These
correspond to x1x2 . . . xn and x1 + x2 + · · · + xn. (Since both operations satisfy the
associative laws, no parentheses are needed.)

374
Chapter 5
ALGEBRAIC STRUCTURES
Examples:
1. The gate diagram for a half-adder:
A half-adder is a Boolean circuit that adds two
bits, x and y, producing two outputs:
a sum bit s = (x + y)(xy)′ (s = 0 if x = y = 0 or x = y = 1; s = 1 otherwise);
a carry bit c = xy (c = 1 if and only if x = y = 1).
The gate diagram for a half-adder is given in the following ﬁgure. This circuit is an
example of a multiple output circuit since there is more than one output.
2. The gate diagram for a full-adder:
A full-adder is a Boolean circuit that adds three
bits (x, y, and a carry bit c) and produces two outputs (a sum bit s and a carry bit c′).
The full-adder gate diagram is given in the following ﬁgure.
half
adder
half
adder
c
x
y
s1
c1
c2
c’
s
5.8.4
MINIMIZATION OF CIRCUITS
Boolean expressions that appear to be diﬀerent can yield the same combinatorial circuit.
For example, xyz + xyz′ + x′y and y (as functions of x and y) have the same table of
values and hence yield the same circuit. (The ﬁrst expression can be simpliﬁed to give the
second: xyz+xyz′+x′y = xy(z+z′)+x′y = xy·1+x′y = xy+x′y = (x+x′)y = 1·y = y.)
Deﬁnitions:
A Boolean expression is minimal (as a sum-of-products) if among all equivalent sum-
of-products expressions it has the fewest number of summands, and among all sum-
of-products expressions with that number of summands it uses the smallest number of
literals in the products.
A Karnaugh map for a Boolean expression written in disjunctive normal form is a
diagram (constructed using the following algorithm) that displays the minterms in the
Boolean expression.

Section 5.8
BOOLEAN ALGEBRAS
375
Facts:
1. Minimization of circuits is an NP-hard problem.
2. Don’t care conditions: In some circuits, it may be known that some elements of the
input set for the Boolean function will never be used. Consequently, the values of the
expression for these elements is irrelevant. The values of the circuit function for these
unused elements of the input set are called don’t care conditions, and the values can be
arbitrarily chosen to be 0 or 1. The blocks in the Karnaugh map where the function
values are irrelevant are marked with d. In the simpliﬁcation process of the Karnaugh
map, 1s can be substituted for any or all of the ds in order to cover larger blocks of boxes
and achieve a simpler equivalent expression.
Algorithm:
There is an algorithm for minimizing Boolean expressions by systematically grouping
terms together. When carried out visually, the method uses a Karnaugh map (Maurice
Karnaugh, born 1924). When carried out numerically using bit strings, the method is
called the Quine-McCluskey method (Willard Quine, 1908–2000; Edward McCluskey,
1929–2016).
1. Karnaugh map method:
(a) Write the Boolean expression in disjunctive normal form.
(b) Obtain the Karnaugh map for this Boolean expression.
The layout of the table
depends on the number of variables under consideration.
The grids for Boolean expressions with two variables (x and y), three variables (x, y,
and z), and four variables (w, x, y, and z) are shown in the following ﬁgure. Each square
in each grid corresponds to exactly one minterm—the product of the row heading and
the column heading. For example, the upper right box in the grid of part (a) of the
ﬁgure represents the minterm xy′; the lower right box in the grid of part (c) of the ﬁgure
represents w′xyz′.
y’
x’
y
yz
wx
wx’
w’x’
w’x
yz’ y’z’ y’z
yz y’z y’z’ yz’
x
(a)
(b)
(c)
x’
x
The headings are placed in a certain order—adjacent squares in any row (or column)
diﬀer in exactly one literal in their row headings (or column headings). The ﬁrst and
last squares in any row (or column) are to be regarded as adjacent. (The variable names
can be permuted; for example, in part (b) of the ﬁgure, the row headings can be y and
y′ and the column headings can be xz, xz′, x′z′, and x′z. The column headings could
also have been written in order as yz, y′z, y′z′, yz′ or y′z, y′z′, yz′, yz.)
The Karnaugh map for the Boolean expression is obtained by placing a checkmark in
each square corresponding to a minterm in the expression.
(c) Find the best covering. A geometric version of the distributive law is used to “cover”
groups of the adjacent marked squares, with every marked square covered at least once

376
Chapter 5
ALGEBRAIC STRUCTURES
and each group covered being as large as possible. The possible ways of covering squares
depends on the number of variables.
(For example, working with two variables and using the distributive law, x′y + x′y′ =
x′(y + y′) = x′1 = x′. This corresponds to covering the two boxes in the bottom row
of the ﬁrst 2 × 2 grid in the following ﬁgure and noting that the only literal common to
both boxes is x′.
y
xy
yz
yz
wxz
wz
z
wyz’
wx
wx’
w’x’
w’x
w’x
w’xyz
xz’
xyz’
wx
wx’
w’x’
w’x
wx
wx’
w’x’
w’x
wx
wx’
w’x’
w’x
z
x’
x’y’z’
yz’ y’z’ y’z
yz
y’z y’z’ yz’
yz
y’z y’z’ yz’
yz
y’z y’z’ yz’
yz
y’z y’z’ yz’
yz
yz’ y’z’ y’z
yz
yz’ y’z’ y’z
yz
yz’ y’z’ y’z
x’
1
1
1
x
x’
x
x’
x
x’
x
x’
x
x’
x
x’
z’
y’
y
y’
Similarly, working with three variables, xyz′ + xy′z′ + x′yz′ + x′y′z′ = xz′(y + y′) +
x′z′(y + y′) = xz′ + x′z′ = (x + x′)z′ = z′. This corresponds to covering the four boxes
in the second and third columns of the third 2 × 4 grid in the second row of the ﬁgure
and noting that z′ is the only common literal.)
The following table shows what groups of boxes can be covered, for expressions with 2,
3, and 4 variables. These are the combinations whose expressions can be simpliﬁed to
a single minterm. Examples for 2, 3, and 4 variables are shown in the previous ﬁgure.
(The method is awkward to use when there are more than 4 variables.)
# variables
groups of boxes that can be covered
2
1×1, 1×2, 2×1, 2×2
3
1×1, 1×2, 1×4, 2×1, 2×2, 2×4
4
1×1, 1×2, 1×4, 2×1, 2×2, 2×4, 4×1, 4×2, 4×4
To obtain the minimization, cover boxes according to the following rules:
• cover all marked boxes at least once;
• cover the largest possible blocks of marked boxes;
• do not cover any unmarked box;
• use the fewest blocks possible.
(d) Find the product of common literals for each of the blocks and form the sum of these
products to obtain the minimization.

Section 5.8
BOOLEAN ALGEBRAS
377
2. Quine-McCluskey method:
(a) Write the Boolean expression in disjunctive normal form, and in each summand list
the variables in alphabetical order. Identify with each term a bit string, using a 1 if the
literal is not a complement and 0 if the literal is a complement. (For example, v′wx′yz
is represented by 01011.)
(b) Form a table with the following columns:
column 1: Make a numbered list of the terms and their bit strings, beginning with
the terms with the largest number of uncomplemented variables. (For example,
wxy′z precedes wx′yz′.)
column 2: Make a list of pairs of terms from column 1 where the literals in the
two terms diﬀer in exactly one position. Use a distributive law to add and
simplify the two terms and write the numbers of these terms and the sum of
the terms in the second column, along with its bit string, using “−” in place of
the variable that no longer appears in the sum. (For example, xyz′ and xy′z′
can be combined to yield xz′ with bit string 1−0.)
columns 3, 4, etc.: To obtain column 3 combine the terms in column 2 in pairs
according to the same procedure as that used to construct column 2. Repeat
this process until no more terms can be combined.
(c) Form a table with a row for each of the terms that cannot be used to form terms with
fewer variables and a column for each of the original terms in the disjunctive normal form
of the original expression. Mark the square in the ij-position if the minterm in column
j could be a summand for the term in row i.
(d) Find a set of rows, with as few rows as possible, such that every column has been
marked at least once in at least one row. The sum of the products labeling these rows
minimizes the original expression.
Examples:
1. Simplify w′x′y + w′z(xy + x′y′) + w′x′z′ + w′xyz′ + wx′y′z′ (an expression in four
variables) using a Karnaugh map.
First write the expression in disjunctive normal form:
w′x′y + w′z(xy + x′y′) + w′x′z′ + w′xyz′ + wx′y′z′
= w′x′yz + w′x′yz′ + w′xyz + w′x′y′z + w′x′yz′ + w′x′y′z′ + w′xyz′ + wx′y′z′
= w′x′yz + w′x′yz′ + w′xyz + w′x′y′z + w′x′y′z′ + w′xyz′ + wx′y′z′.
Next, draw its Karnaugh map. See part (a) of the following ﬁgure. A covering is given
in part (b) of the ﬁgure. Note that in order to use larger blocks, some squares have been
covered more than once. Also note that w′x′yz, w′xyz, w′x′yz′, and w′xyz′ are covered
with one 2 × 2 block rather than with two 1 × 2 blocks. In the three blocks the common
literals are w′x′, w′y, and x′y′z′.
Finally, form the sum of these products: w′x′ + w′y + x′y′z′.
2. Minimize w′xy′z + wxyz′ + wx′yz′ + w′x′yz + wxyz + w′x′y′z + w′xyz (an expression
in four variables) using the Quine-McCluskey method.
Step (b) of the Quine-McCluskey method yields the table that follows.

378
Chapter 5
ALGEBRAIC STRUCTURES
yz
wx
wx’
w’x’
w’x
(a)
y’z y’z’ yz’
yz
wx
wx’
w’x’
w’x
(b)
y’z y’z’ yz’
1
wxyz
1111
1, 2
wxy
111−
3, 5, 6, 7
w′z
0−−1
2
wxyz′
1110
1, 3
xyz
−111
3
w′xyz
0111
2, 4
wyz′
1−10
4
wx′yz′
1010
3, 5
w′yz
0−11
5
w′x′yz
0011
3, 6
w′xz
01−1
6
w′xy′z
0101
5, 7
w′x′z
00−1
7
w′x′y′z
0001
6, 7
w′y′z
0−01
The four terms w′z, wxy, xyz, wyz′ were not used in combining terms, so they become
the names of the rows in the next table:
wxyz
wxyz′
w′xyz
wx′yz′
w′x′yz
w′xy′z
w′x′y′z
w′x
√
√
√
√
wxy
√
√
xyz
√
√
wyz′
√
√
There are two ways to cover the seven minterms:
w′x, wxy, wyz′
or
w′x, xyz, wyz′.
This yields two ways to minimize the original expression:
w′x + wxy + wyz′
and
w′z + xyz + wyz′.
REFERENCES
Printed Resources:
[Ar10] M. Artin, Algebra, 2nd ed., Pearson, 2010.
[As00] M. Aschbacher, Finite Group Theory, 2nd ed., Cambridge University Press, 2000.
[BiBa70] G. Birkhoﬀand T. C. Bartee, Modern Applied Algebra, McGraw-Hill, 1970.
[BiMa98] G. Birkhoﬀand S. Mac Lane, A Survey of Modern Algebra, A K Peters/CRC
Press, 1998.
[Bl87] N. J. Bloch, Abstract Algebra with Applications, Prentice-Hall, 1987.
[Ca89] R. W. Carter, Simple Groups of Lie Type, Wiley, 1989.
[Ch08] L. Childs, A Concrete Introduction to Higher Algebra, 3rd ed., Springer-Verlag,
2008.

REFERENCES
379
[CoMo80] H. S. M. Coxeter and W. O. J. Moser, Generators and Relations for Discrete
Groups, 4th ed., Springer-Verlag, 1980.
[CrFo08] R. H. Crowell and R. H. Fox, Introduction to Knot Theory, Dover, 2008.
[Fr02] J. B. Fraleigh, A First Course in Abstract Algebra, 7th ed., Pearson, 2002.
[Go82] D. Gorenstein, Finite Simple Groups: An Introduction to Their Classiﬁcation,
Springer, 1982.
[GoLySo02] D. Gorenstein, R. Lyons, and R. Solomon, The Classiﬁcation of the Finite
Simple Groups, American Mathematical Society, 2002.
[Ha99] M. Hall, Jr., The Theory of Groups, 2nd ed., American Mathematical Society,
1999.
[He75] I. N. Herstein, Topics in Algebra, 2nd ed., Wiley, 1975.
[Hu74] T. W. Hungerford, Algebra, Springer, 1974.
[Ka72] I. Kaplansky, Fields and Rings, 2nd ed., University of Chicago Press, 1972.
[LiNi94] R. Lidl and H. Niederreiter, Introduction to Finite Fields and Their Applications,
revised edition, Cambridge University Press, 1994.
[LiPi98] R. Lidl and G. Pilz, Applied Abstract Algebra, 2nd ed., Springer-Verlag, 1998.
[MaBi99] S. Mac Lane and G. Birkhoﬀ, Algebra, 3rd ed., American Mathematical Society,
1999.
[MaKaSo04] W. Magnus, A. Karrass, and D. Solitar, Combinatorial Group Theory, 2nd
ed., Dover, 2004.
[Mc73] N. H. McCoy, The Theory of Rings, Chelsea, 1973.
[McBe77] N. H. McCoy and T. Berger, Algebra: Groups, Rings and Other Topics, Allyn
& Bacon, 1977.
[McFr12] N. H. McCoy and P. Franklin, Rings and Ideals (Carus Monograph No. 8),
Literary Licensing, LLC, 2012.
[Mc03] R. J. McEliece, Finite Fields for Computer Scientists and Engineers, Kluwer
Academic Publishers, 2003.
[MeEtal10] A. Menezes, I. Blake, X. Gao, R. Mullin, S. Vanstone, and T. Yaghoobian,
Applications of Finite Fields, Kluwer Academic Publishers, 2010.
[Ro99] J. J. Rotman, An Introduction to the Theory of Groups, 4th ed., Springer, 1999.
[ThFe63] J. G. Thompson and W. Feit, “Solvability of groups of odd order,” Paciﬁc
Journal of Mathematics 13 (1963), 775–1029.
[va91] B. L. van der Waerden, Modern Algebra (2 volumes), Springer, 1991.
Web Resources:
http://magma.maths.usyd.edu.au/magma/ (The Magma Computer Algebra System,
successor to CAYLEY, developed by the Computational Algebra Group at the Uni-
versity of Sydney.)
http://www.gap-system.org
(Home page for GAP – Groups, Algorithms and Pro-
gramming, a system for computational discrete algebra.)


6
LINEAR ALGEBRA
6.1 Vector Spaces
Joel V. Brawley
6.1.1 Basic Concepts
6.1.2 Subspaces
6.1.3 Linear Combinations, Independence, Basis, and Dimension
6.1.4 Inner Products, Length, and Orthogonality
6.2 Linear Transformations
Joel V. Brawley
6.2.1 Linear Transformations, Range, and Kernel
6.2.2 Vector Spaces of Linear Transformations
6.2.3 Matrices of Linear Transformations
6.2.4 Change of Basis
6.3 Matrix Algebra
Peter R. Turner
6.3.1 Basic Concepts and Special Matrices
6.3.2 Operations of Matrix Algebra
6.3.3 Fast Multiplication of Matrices
6.3.4 Determinants
6.3.5 Rank
6.3.6 Identities of Matrix Algebra
6.4 Linear Systems
Barry Peyton and
6.4.1 Basic Concepts
Esmond Ng
6.4.2 Gaussian Elimination
6.4.3 LU Decomposition
6.4.4 Cholesky Decomposition
6.4.5 Conditioning of Linear Systems
6.4.6 Pivoting for Stability
6.4.7 Pivoting to Preserve Sparsity
6.5 Eigenanalysis
R. B. Bapat
6.5.1 Eigenvalues and Characteristic Polynomials
6.5.2 Eigenvectors and Diagonalization
6.5.3 Localization
6.5.4 Computation of Eigenvalues
6.5.5 Special Classes
6.6 Combinatorial Matrix Theory
R. B. Bapat and
6.6.1 Matrices of Zeros and Ones
Geir Dahl
6.6.2 Matrices and Graphs
6.6.3 Nonnegative Matrices
6.6.4 Permanents
6.6.5 Matrix Completion and Sign-Solvablity
6.7 Singular Value Decomposition
Carla D. Martin
6.7.1 Basic Concepts

382
Chapter 6
LINEAR ALGEBRA
6.7.2 Computation of the SVD
6.7.3 Solving Linear Systems
6.7.4 Applications
6.7.5 Higher Dimensions
INTRODUCTION
Concepts from linear algebra play an important role in various applications of discrete
mathematics, as in coding theory, computer graphics, marketing, generation of pseudo-
random numbers, graph theory, and combinatorial designs. This chapter discusses fun-
damental concepts of linear algebra, computational aspects, and various applications.
GLOSSARY
access (of a class): The class Ci of vertices has access to class Cj if either i = j or there
is a path from a vertex in Ci to a vertex in Cj.
adjacency matrix (of a graph): a square (0, 1)-matrix whose (i, j) entry is 1 if and
only if vertices i and j are adjacent.
adjoint: See Hermitian adjoint.
algebraic multiplicity: given an eigenvalue, the multiplicity of the eigenvalue as a
root of the characteristic equation.
augmented matrix (of a linear system): the matrix obtained by appending the right-
hand side vector to the coeﬃcient matrix as its rightmost column.
back substitution: a procedure for solving an upper triangular linear system.
basic class (of a matrix): a class such that the Perron root of the corresponding prin-
cipal submatrix equals that of the entire matrix.
basis: an independent spanning set of vectors in a vector space.
characteristic equation: for a square matrix A, the equation pA(λ) = 0, where pA(λ)
is the characteristic polynomial of A.
characteristic polynomial: for a square matrix A, the polynomial (in the indeﬁnite
symbol λ) given by pA(λ) = det(λI −A).
Cholesky decomposition: expressing a matrix A as A = LLT, where L is lower tri-
angular and every entry on the main diagonal of L is positive.
circulant: a matrix in which every row is obtained by a single cyclic shift of the previous
row.
class (of a matrix): a maximal set of row indices such that the corresponding vertices
have mutual access in the directed graph of the matrix.
complete pivoting: an implementation of Gaussian elimination in which a pivot of
largest magnitude is selected at each step.
completion (of a partially speciﬁed matrix): a matrix in which values are chosen for
each unspeciﬁed entry.
condition number: given a matrix A, the number κ(A) = ∥A∥∥A−1∥.
conjugate sequence (of a sequence): the sequence whose nth term is the number of
terms not less than n in the given sequence.

GLOSSARY
383
dependent set: a set of vectors in a vector space that are not independent.
determinant: given an n × n matrix A, det A = P
σ∈Sn sgn(σ) a1σ(1)a2σ(2) . . . anσ(n),
where Sn is the symmetric group on n elements and the coeﬃcient sgn(σ) is the
sign of the permutation σ: 1 if σ is an even permutation and −1 if σ is an odd
permutation.
diagonal matrix: a square matrix with nonzero elements only on the main diagonal.
diagonalizable matrix: a square matrix that is similar to a diagonal matrix.
diﬀerence (of matrices of the same dimensions): the matrix each of whose elements is
the diﬀerence between corresponding elements of the original matrices.
dimension: for a vector space V , the number of vectors in any basis for V .
directed graph (of a matrix A): the graph G(A) with vertices corresponding to the
rows of A and an edge from i to j whenever aij is nonzero.
direct sum (of subspaces): given subspaces U and W, the sum of subspaces in which
U and W have only the zero vector in common.
distance (between vectors): given vectors v and w, the length of the vector v −w.
dominant eigenvalue: given a matrix, an eigenvalue of the matrix of maximum mod-
ulus.
dot product (of real vectors): given real vectors x = (x1, . . . , xn) and y = (y1, . . . , yn),
the number x · y =
nP
i=1
xiyi.
doubly stochastic matrix: a matrix with all entries nonnegative and with all row and
column sums equal to 1.
eigenvalue: given a square matrix A, a scalar λ such that Ax = λx for some nonzero
vector x.
eigenvector: given a square matrix A, a nonzero vector x such that the vector Ax is a
scalar multiple of x.
eigenspace: given a square matrix A, the vector space { x | Ax = λx } for some scalar λ.
exponent (of a matrix): given a matrix A, the least positive integer m, if it exists, such
that Am has all positive entries.
ﬁll: in Gaussian elimination, those nonzero entries created in the triangular factors of
a matrix corresponding to zero entries in the original matrix.
ﬁnal class: given a matrix, a class of the matrix with access to no other class.
ﬂop: a multiply-add operation involving a single multiplication followed by a single
addition.
forward substitution: a procedure for solving a lower triangular linear system.
fully indecomposable matrix: a matrix that is not partly decomposable.
Gaussian elimination: a solution procedure that at each step uses one equation to
eliminate one variable from the system of equations.
geometric multiplicity: the dimension of the eigenspace.
Gerˇsgorin discs: regions in the complex plane that collectively are guaranteed to con-
tain all the eigenvalues of a given matrix.

384
Chapter 6
LINEAR ALGEBRA
growth factor: a ratio that measures how large the entries of a matrix become during
Gaussian elimination.
Hermitian adjoint: given a matrix A, the matrix A∗obtained from the transpose AT
by replacing each entry by its complex conjugate.
Hermitian matrix: a complex matrix whose transpose is its (elementwise) complex
conjugate.
idempotent matrix: a matrix A such that A2 = A.
identity matrix: a diagonal matrix in which each diagonal element is 1.
ill-conditioned system: a linear system Ax = b whose solution x is extremely sensitive
to errors in the data A and b.
independent set: a set of vectors in a vector space that is not dependent.
index of cyclicity: for a matrix, the number of eigenvalues with maximum modulus.
inner product: a ﬁeld-valued function of two vector variables used to deﬁne a notion
of orthogonality (that is, perpendicularity). In real or complex vector spaces it is
also used to introduce length, distance, and convergence.
inverse: given a square matrix A, the square matrix A−1 whose product with the
original matrix is the identity matrix.
invertible matrix: a matrix that has an inverse.
irreducible matrix: a matrix that is not reducible.
isomorphic (vector spaces): vector spaces that are structurally identical.
kernel (of a linear transformation): the set of all vectors that are mapped to the zero
vector by the linear transformation.
Laplacian matrix (of a graph): the diﬀerence between the diagonal matrix associated
with the graph (having vertex degrees on the diagonal) and the adjacency matrix of
the graph.
length (of a vector): the square root of the inner product of the vector with itself.
linear combination (of vectors): given vectors v1, v2, . . . , vt, a vector of the form a1v1+
a2v2 + · · · + atvt, where the ai are scalars.
linear operator: a linear transformation from a vector space to itself.
linear system: a set of m linear equations in n variables x, represented by Ax = b;
here A is the coeﬃcient matrix and b is the right-hand side vector.
linear transformation: a function T from one vector space over F to another vector
space over F satisfying T (au+v) = aT (u)+T (v) for all vectors u, v and all scalars a.
lower triangular matrix: a matrix in which all nonzero elements occur either on or
below the diagonal.
LU decomposition: expressing a matrix A as the product A = LU, where L is unit
lower triangular and U is upper triangular.
Markowitz pivoting: a simple greedy strategy for reducing the number of nonzero
entries introduced during the LU decomposition of a sparse matrix.
matrix (of a linear transformation): given a linear transformation T , a matrix associ-
ated with T that represents T with respect to a ﬁxed basis.

GLOSSARY
385
minimal polynomial: for a matrix A, the monic polynomial q(·) of minimum degree
such that q(A) = 0.
minimum degree algorithm: a version of the Markowitz pivoting strategy for sym-
metric coeﬃcient matrices.
minor: the determinant of a square submatrix of a given matrix.
modulus: the absolute value of a complex number.
nilpotent matrix: a matrix A such that Ak = 0 for some positive integer k.
nonnegative matrix: a matrix with each entry nonnegative.
nonsingular matrix: a matrix that has an inverse.
normal matrix: a matrix A such that AA∗= A∗A (A∗is the Hermitian adjoint of A).
nullity (of a linear transformation): the dimension of the kernel of the linear transfor-
mation.
nullity (of a matrix): the dimension of the null space of the matrix.
null space (of a matrix A): the set of all vectors x for which Ax = 0.
numerically stable algorithm: an algorithm whose accuracy is not greatly harmed
by roundoﬀerrors.
numerically unstable algorithm: an algorithm that can return an inaccurate solu-
tion even when the solution is relatively insensitive to errors in the data.
orthogonal matrix: a real square matrix whose inverse is its transpose.
orthogonal set (of vectors): a set of vectors in which any two distinct vectors have
inner product 0.
orthonormal set (of vectors): a set of unit length orthogonal vectors.
partial pivoting: an implementation of Gaussian elimination which at step k selects
the pivot of largest magnitude in column k.
partly decomposable (matrix): an n × n matrix containing a zero submatrix of size
k × (n −k) for some 1 ≤k ≤n −1.
permanent (of an n × n matrix A): per(A) = P
σ∈Sn a1σ(1)a2σ(2) . . . anσ(n), where Sn
is the symmetric group on n elements.
permutation matrix: a square (0, 1)-matrix in which the entry 1 occurs exactly once
in each row and exactly once in each column.
Perron root: the spectral radius of a nonnegative matrix.
pivot: the coeﬃcient of the eliminated variable in the equation used to eliminate it.
positive deﬁnite matrix: a Hermitian matrix A such that x∗Ax > 0 for all x ̸= 0.
positive matrix: a matrix with each entry positive.
positive semideﬁnite matrix: a Hermitian matrix A such that x∗Ax ≥0 for all x.
power (of a square matrix): the square matrix obtained by multiplying the matrix by
itself the required number of times.
primitive matrix: a matrix with a ﬁnite exponent.
principal minor (of a matrix): the determinant of a principal submatrix of the matrix.

386
Chapter 6
LINEAR ALGEBRA
principal submatrix (of a matrix A): the matrix obtained from A by deleting all but
a speciﬁed set of rows and the same set of columns.
product (of matrices): for an m × n matrix A and an n × p matrix B, the m × p
matrix AB whose ij-entry is the scalar product of row i of A and column j of B.
range (of a linear transformation T ): the set of all vectors w for which T (v) = w has a
solution.
rank (of a linear transformation T ): the dimension of the range of T .
rank (of a matrix): the maximum number of linearly independent rows (or columns) in
the matrix.
reducible matrix: a matrix A with aij = 0 for all i ∈S, j ̸∈S, for some set S.
roundoﬀerrors: the errors associated with storing and computing numbers in ﬁnite
precision arithmetic on a digital computer.
row stochastic matrix: a matrix with all entries nonnegative and row sums 1.
scalar: an element of a ﬁeld.
scalar multiple (of a matrix): the matrix obtained by multiplying each element of the
original matrix by the scalar.
scalar product: See dot product.
sign pattern (of a matrix): the matrix obtained by replacing each entry of the given
matrix by its sign (0, 1, or −1).
similar matrices: square matrices A and B satisfying the equation P −1BP = A for
some invertible matrix P.
singular matrix: a matrix that has no inverse.
singular value decomposition (of a matrix A): the representation A = UΣV T , where
U, V are orthogonal matrices and Σ is a diagonal matrix.
singular values (of a matrix A): the positive square roots of the eigenvalues of AA∗,
where A∗is the Hermitian adjoint of A.
skew-Hermitian matrix: a matrix equal to the negative of its Hermitian adjoint.
skew-symmetric matrix: a matrix equal to the negative of its transpose.
span (of a set of vectors): all vectors obtainable as linear combinations of the given
vectors.
spanning set: a set of vectors in a vector space V whose span equals V .
sparse matrix: a matrix that has relatively few nonzero entries.
spectral radius (of a matrix): the maximum modulus of an eigenvalue of the matrix.
square matrix: a matrix having the same number of rows and columns.
strictly diagonally dominant matrix: a square matrix each of whose diagonal ele-
ments exceeds in modulus the sum of the moduli of all other elements in that row.
strictly totally positive matrix: a matrix with all minors positive.
submatrix (of a matrix A): the matrix obtained from A by deleting all but a certain
set of rows and a certain set of columns.
subspace: a vector space within a vector space.

Section 6.1
VECTOR SPACES
387
sum (of matrices): for two matrices of the same dimensions, the matrix each of whose
elements is the sum of the corresponding elements of the original matrices.
sum (of subspaces): given subspaces U and W, the subspace consisting of all possible
sums u + w where u ∈U and w ∈W.
symmetric matrix: a matrix that equals its transpose.
term rank (of a (0, 1)-matrix): the maximum number of 1s such that no two are in the
same row or column.
tournament matrix: a (0, 1)-matrix A = (aij) with zero diagonal entries and whose
oﬀ-diagonal entries satisfy aij + aji = 1.
trace: given a square matrix, the sum of the diagonal elements of the matrix.
transpose (of a matrix): for a matrix A, the matrix AT whose columns are the rows
of the original matrix.
tridiagonal matrix: a matrix whose nonzero entries are either on the main diagonal
or immediately above or below the main diagonal.
unitary matrix: a square matrix whose inverse is its Hermitian adjoint.
unit triangular matrix: a (lower or upper) triangular matrix having all diagonal en-
tries 1.
upper triangular matrix: a matrix in which all nonzero elements occur either on or
above the main diagonal.
vector: an individual object of a vector space.
vector space: a collection of objects that can be added and multiplied by scalars,
always yielding another object in the collection.
well-conditioned system: a linear system Ax = b whose solution x is relatively in-
sensitive to errors in the data A and b.
(0, 1)-matrix: a matrix with each entry either 0 or 1.
6.1
VECTOR SPACES
The concept of a “vector” comes initially from the physical world, where a vector is a
quantity having both magnitude and direction (for example, force and velocity). The
mathematical concept of a vector space generalizes these ideas, with applications in
coding theory, ﬁnite geometry, cryptography, and other areas of discrete mathematics.
6.1.1
BASIC CONCEPTS
Deﬁnitions:
A vector space over a ﬁeld F (§5.6.1) is a triple (V, ⊕, ·) consisting of a set V and two
operations, ⊕(vector addition) and · (scalar multiplication), such that
• (V, ⊕) is an abelian group (§5.2.1); i.e., ⊕is a function (u, v) →u ⊕v from V × V
to V satisfying

388
Chapter 6
LINEAR ALGEBRA
⋄(u ⊕v) ⊕w = u ⊕(v ⊕w) for all u, v, w ∈V ;
⋄there is a vector 0 such that v ⊕0 = v for all v ∈V ;
⋄for each v ∈V there is −v ∈V such that v ⊕(−v) = 0;
⋄u ⊕v = v ⊕u for all u, v ∈V ;
• the operation · is a function (a, v) →a·v from F ×V to V such that for all a, b ∈F
and u, v ∈V the following properties hold:
⋄a · (b · v) = (ab) · v;
⋄(a + b) · v = (a · v) ⊕(b · v);
⋄a · (u ⊕v) = (a · u) ⊕(a · v);
⋄1 · v = v.
Here, ab and a + b represent multiplication and addition of elements a, b ∈F.
The scalars are the elements of F, the vectors are the elements of V , and the set V
itself is often also called the vector space.
The diﬀerence of two vectors u and v is the vector u −v = u ⊕(−v) where −v is the
negative of v in the abelian group (V, ⊕).
Notation:
While vector addition ⊕and ﬁeld addition + can be quite diﬀerent, it is
customary to use the same notation + for both. It is also customary to write av instead
of a · v, and to use the symbol 0 for the additive identities of the vector space V and the
ﬁeld F.
Facts:
Assume that V is a vector space over F.
1. a0 = 0 and 0v = 0 for all a ∈F and v ∈V .
2. (−1)v = −v for all v ∈V .
3. If av = 0 for a ∈F and v ∈V , then either a = 0 or v = 0.
4. Cancellation property: For all u, v, w ∈V , if u + v = w + v, then u = w.
5. a(u −v) = au −av for all a ∈F and u, v ∈V .
Examples:
1. Force vectors: Forces in the plane can be represented by geometric vectors such as F1
and F2 in part (a) of the following ﬁgure; addition of these vectors is carried out using
the so-called parallelogram law. By introducing a coordinate system and locating the
initial point of each directed line segment at the origin (0, 0), each geometric vector can
be named by its terminal point. Thus, a vector in the plane becomes a pair (x, y) ∈R2 of
real numbers. The parallelogram law of addition translates into componentwise addition
(part (c) of the ﬁgure), while stretching (respectively, shrinking, negating) translates to
componentwise multiplication by a real number r > 1 (respectively, 0 < r < 1, r = −1).
Three-dimensional force vectors are similarly represented using triples (x, y, z) ∈R3.
2. Euclidean space:
Generalizing Example 1, n-dimensional Euclidean space consists
of all n-tuples of real numbers Rn = { (x1, x2, . . . , xn) | xi ∈R }.
3. If F is any ﬁeld, then F n = { (x1, x2, . . . , xn) | xi ∈F } is a vector space, where
addition and scalar multiplication are componentwise:
(x1, x2, . . . , xn) + (y1, y2, . . . , yn) =(x1 + y1, x2 + y2, . . . , xn + yn)
a(x1, x2, . . . , xn) =(ax1, ax2, . . . , axn),
where a ∈F. When F = R, these are the vectors mentioned in Examples 1 and 2.

Section 6.1
VECTOR SPACES
389
F1
F1
F
F
y
x
(a, b)
(c, d)
(a+c, b+d)
F
(-1)F
2F
+
2
F2
1
2
(a)  Addition
(b)  Stretching, shrinking, negating
(c)  Addition of components
4. A vector space over Z2:
V consists of the 128 subsets of the set {1, 2, . . ., 7}
as represented by binary 7-tuples; for example, the subset {1, 4, 5, 7} corresponds to
(1, 0, 0, 1, 1, 0, 1) and the subset {1, 2, 3, 4} to (1, 1, 1, 1, 0, 0, 0). The operations on V are
componentwise addition and scalar multiplication mod 2. In this vector space, the sum
of two members of V corresponds to the symmetric diﬀerence (§1.2.2) of the associated
sets. (This example is a special case of Example 3.)
5. A ﬁnite aﬃne plane over Z5:
V consists of all pairs (x, y) where x, y ∈Z5 and
where addition and scalar multiplication are componentwise modulo 5. This special case
of Example 3 arises in ﬁnite geometry where the 25 members of V are thought of as
“points” and the sets of solutions to equations of the form ax+by = c (where a, b, c ∈Z5
with one of a or b ̸= 0) are viewed as “lines”.
6. Inﬁnite binary sequences:
V consists of all inﬁnite binary sequences { (s1, s2, . . .) |
si ∈Z2 } where addition and multiplication are componentwise mod 2. As in Example 4,
each s ∈V may be viewed as a subset of the positive integers, but each s may also
be viewed as a potential “message” or “data” stream; for example, each group of 7
consecutive members of s could represent a letter in the 7-bit ASCII code.
7. V = F m×n, the set of all m × n matrices over F, is a vector space, where vector
addition is the usual matrix addition and scalar multiplication is the usual scalar-by-
matrix multiplication (§6.3.2). When m = 1, this reduces to Example 3.
8. Let V = E be a ﬁeld and F a subﬁeld. Then V is a vector space over F where vector
addition and scalar multiplication are the addition and multiplication of E. In particular,
the ﬁnite ﬁeld Fq of prime power order q = pn is a vector space over the subﬁeld Fp.
9. Let V = F[x], the set of all polynomials (§5.5.2) over F in an indeterminate x.
Then V is a vector space over F, where addition is ordinary polynomial addition and
scalar multiplication is the usual scalar-by-polynomial multiplication.
10. For a nonempty set X and a given vector space U over F, let V denote the set of
all functions from X to U. The sum f + g of two vectors (functions) f, g ∈V is deﬁned
by (f + g)(x) = f(x) + g(x) for all x ∈X and the scalar multiplication af of a ∈F by
f ∈V is deﬁned by (af)(x) = af(x). (For speciﬁc cases of this general vector space, see
§6.1.2, Examples 13–15.)
6.1.2
SUBSPACES
Deﬁnitions:
A subspace of a vector space V is a nonempty subset W of V that is a vector space
under the addition and scalar multiplication operations inherited from V .

390
Chapter 6
LINEAR ALGEBRA
The sum of two subspaces U, W ⊆V is the set { u+w | u ∈U, w ∈W }. If U ∩W = {0},
their sum is called the direct sum, denoted U ⊕W.
If A is an m × n matrix over F, the null space NS(A) of A is { x ∈F n×1 | Ax = 0 }.
The null space of A is also called the right null space when contrasted with the left
null space LNS(A) deﬁned by { y ∈F 1×m | yA = 0 }.
Facts:
Assume that V is a vector space over F.
1. W ⊆V is a subspace of V if and only if W ̸= ∅and for all a, b ∈F and u, v ∈W,
au + bv ∈W.
2. W ⊆V is a subspace of V if and only if W ̸= ∅and for all a ∈F and u, v ∈W,
u + v ∈W and au ∈W.
3. Every subspace of V contains 0, the zero vector.
4. The sets {0} and V are subspaces of V .
5. The intersection of any collection of subspaces of V is a subspace of V .
6. The sum of any collection of subspaces of V is a subspace of V .
7. Each member of U ⊕W can be expressed as a sum u + w for a unique u ∈U and a
unique w ∈W.
8. The set of solutions to a homogeneous linear equation in the unknowns x1, x2, . . . , xn
is a subspace of F n. Namely, for any ﬁxed (a1, a2, . . . , an) ∈F n, the set W = { x ∈F n |
a1x1 + a2x2 + · · · + anxn = 0 } is a subspace of F n.
9. The set of solutions to any collection of homogeneous linear equations in the un-
knowns x1, x2, . . . , xn is a subspace of F n. In particular, if W is a subspace of F n then
the set of all x = (x1, x2, . . . , xn) ∈F n satisfying a1x1 + a2x2 + · · · + anxn = 0 for all
(a1, a2, . . . , an) ∈W is a subspace of V called the orthogonal complement of W and
denoted W ⊥.
10. The null space NS(A) of an m × n matrix A over F is a subspace of F n×1.
11. The left null space LNS(A) of an m×n matrix A is a subspace of F 1×m and equals
(NS(AT ))T where T denotes transpose.
Examples:
1. The set of all 3-tuples of real numbers of the form (a, b, 2a + 3b) where a, b ∈R is a
subspace of R3. This subspace can also be described as the set of solutions (x, y, z) to
the homogeneous linear equation 2x + 3y −z = 0.
2. The set of all 4-tuples of real numbers of the form (a, −a, 0, b) where a, b ∈R is a
subspace of R4. This subspace can also be described as the set of solutions (x1, x2, x3, x4)
to the pair of equations x1 + x2 = 0 and x3 = 0.
3. For V = Z2
5, the set of all solutions to the equation x + 2y = 0 forms a subspace.
It consists of the ﬁnite set {(0, 0), (3, 1), (1, 2), (4, 3), (2, 4)} and can also be described as
the set of all pairs in V of the form (3a, a). The set S of solutions to x + 2y = 1, namely
{(1, 0), (4, 1), (2, 2), (0, 3), (3, 4)}, is not a subspace of V since for example (1, 0)+(4, 1) =
(0, 1) ̸∈S. However S is a “line” in the aﬃne plane described in Example 5 of §6.1.1.
4. In the vector space V = Z7
2, the set of 7-tuples with an even number of 1s is a
subspace. This subspace can also be described as the collection of all members of V
whose components sum to 0.

Section 6.1
VECTOR SPACES
391
5. Coding theory: In the vector space F n over the ﬁnite ﬁeld F = GF(q), a linear code
(§14.2) is simply any subspace of F n. In particular, an (n, k)-code is a k-dimensional
subspace of F n.
6. Binary codes: A linear binary code is any subspace of the vector space F n where F
is the ﬁnite ﬁeld on two elements, GF(2). Generalizing Example 4, the set of all binary
n-tuples with an even number of 1s is a subspace of F n and so is a linear binary code.
7. Consider the undirected graph (§8.1) in the following ﬁgure, where the edges have
been labeled with the integers {1, 2, . . ., 7}. Associate with this graph the vector space
V = Z7
2 where, as in Example 4 (§6.1.1), each binary 7-tuple is identiﬁed with a subset
of edges. One subspace W of V , called the cycle space of the graph, corresponds to the
(edge-disjoint) union of cycles in the graph. For example, (1, 1, 0, 1, 0, 1, 1) ∈W as it
corresponds to the cycle 1, 2, 6, 7, 4, and so is (1, 1, 1, 0, 1, 1, 1) which corresponds to the
edge-disjoint union of cycles 1, 2, 3 and 5, 6, 7. The sum of these two members of W is
(0, 0, 1, 1, 1, 0, 0) which corresponds to the cycle 3, 4, 5.
1
4
3
5
6
7
2
8. The set of n × n symmetric matrices (§6.3.1) over a ﬁeld F is a subspace of F n×n,
and so is the set of n × n upper triangular matrices (§6.3.1) over F.
9. For an m× m matrix A over F and λ ∈F, the set W = { X ∈F m×n | AX = λX } is
a subspace of F m×n. (This space is related to the eigenspaces of A discussed in §6.5.2.)
10. For a given n × n matrix A over F, the set W = { X ∈F n×n | XA = AX } is a
subspace of F n×n. (This is the space of matrices that commute with A.)
11. Let ﬁeld E be a vector space over subﬁeld F, and let K denote the set of all
elements α ∈E that satisfy a polynomial equation of the form f(α) = 0 for some
nonzero f(x) ∈F[x]. Then K is a subﬁeld of E containing F (the ﬁeld of algebraic
elements of E over F) and consequently is a subspace of E over F. (See §5.6.2.)
12. For each ﬁxed n ≥1, the set of all polynomials of degree ≤n is a subspace of F[x].
(See §6.1.1, Example 9.)
13. In Example 10 of §6.1.1, take X = [a, b] where a, b ∈R with a < b, and take U = R
as a vector space over itself. The resulting V , the set of all real-valued functions on
[a, b], is a vector space. The set C[a, b] of continuous real-valued functions on [a, b] is a
subspace of V .
14. In Example 10 of §6.1.1, take X = {1, 2, . . ., 7} and take U = Z2 as a vector space
over itself. The resulting V , the set of all functions from {1, 2, . . ., 7} to Z2, can be
thought of as the vector space of binary 7-tuples V = Z7
2.
15. In Example 10 of §6.1.1, take both X and U to be vector spaces over F. Then V is
the vector space of all functions from X to U. The collection of those T ∈V satisfying
T (aα + bβ) = aT (α) + bT (β) for all a, b ∈F and α, β ∈X is a subspace of V . (This
space is the space of linear transformations considered in §6.2.)

392
Chapter 6
LINEAR ALGEBRA
6.1.3
LINEAR COMBINATIONS, INDEPENDENCE, BASIS, AND DIMENSION
Deﬁnitions:
If v1, v2, . . . , vt are vectors from a vector space V over F, then a vector w ∈V is a linear
combination of v1, v2, . . . , vt if w = a1v1 + a2v2 + · · · + atvt for some scalars ai ∈F.
The zero vector is considered a linear combination of ∅.
For S ⊆V , the span of S, denoted Span(S), is the set of all (ﬁnite) linear combinations
of members of S; that is, Span(S) consists of all ﬁnite sums a1v1 +a2v2 +· · ·+atvt where
vi ∈S and ai ∈F. (The span of the empty set is taken to be {0}.) Span(S) is also
called the space generated or spanned by S. (See Fact 1.)
The row space RS(A) of an m × n matrix A over F (§6.3.1) is Span(R1, R2, . . . , Rm),
where R1, R2, . . . , Rm are the rows of A viewed as vectors in F 1×n.
The column space CS(A) of A is Span(C1, C2, . . . , Cn), where C1, C2, . . . , Cn are the
columns of A.
A subset S ⊆V is called a spanning set for V if Span(S) = V .
A subset S ⊆V is (linearly) independent if every ﬁnite subset {v1, v2, . . . , vt} of S
has the property that the only scalars a1, a2, . . . , at satisfying a1v1 +a2v2 +· · ·+atvt = 0
are a1 = a2 = · · · = at = 0.
A subset S ⊆V is (linearly) dependent if it is not independent.
A basis for V is an independent spanning set.
A vector space V is ﬁnite dimensional if it has a ﬁnite basis; otherwise, V is inﬁnite
dimensional.
The dimension dim V of a vector space V is the cardinality of any basis for V . (See
Fact 8.)
If B = (v1, v2, . . . , vn) is an ordered basis for V , then the coordinates of v with respect
to B are the scalars a1, a2, . . . , an such that v = a1v1 + a2v2 + · · · + anvn. (See Fact 14.)
The coordinate vector [v]B of v with respect to B (written as a column) is [v]B =
(a1, a2, . . . , an)T where T denotes transpose (§6.3.1).
Note: Some writers distinguish between the coordinates written as a row and as a column,
calling the row (a1, a2, . . . , an) the coordinate vector of v with respect to B and the
column (a1, a2, . . . , an)T the coordinate matrix of v with respect to B.
The row rank of a matrix A over F is dim RS(A), and the column rank of A is
dim CS(A). The rank of A is the size of the largest square submatrix of A with nonzero
determinant (§6.3.4); that is, rank A = r if there exists an r × r submatrix of A whose
determinant is nonzero, and every t × t submatrix of A with t > r has zero determinant.
The nullity of a matrix A is dim NS(A).
Two vector spaces V and U over the same ﬁeld F are isomorphic if there exists a
bijective mapping T : V →U such that T (v + w) = T (v) + T (w) and T (av) = aT (v) for
all v, w ∈V and a ∈F. The mapping T is called an isomorphism.

Section 6.1
VECTOR SPACES
393
Facts:
1. Span(S) is a subspace of V . In particular, RS(A) is a subspace of F 1×n and CS(A)
is a subspace of F m×1.
2. Span(S) is the intersection of all subspaces of V that contain S; thus, Span(S) is the
smallest subspace of V containing S in that it lies inside every subspace of V containing S.
3. A set {v} consisting of a single vector from V is dependent if and only if v = 0.
4. A set of two or more vectors is dependent if and only if some vector in the set is a
linear combination of the remaining vectors in the set.
5. Any superset of a dependent set is dependent, and any subset of an independent set
is independent. (The empty set is independent.)
6. If V has a basis of n elements, then every subset of V with more than n elements is
dependent.
7. If W is a subspace of V then dim W ≤dim V .
8. Every vector space V has a basis, and every two bases for V have the same number
of elements (cardinality). For inﬁnite-dimensional vector spaces, this fact relies on the
axiom of choice (§1.2.4).
9. Every independent subset of V can be extended to a basis for V . More generally, if S
is an independent set, then every maximal independent set containing S is a basis for
V containing S. For inﬁnite-dimensional vector spaces, this fact relies on the axiom of
choice. (An independent set is maximal if every set properly containing it is dependent.)
10. Every spanning set contains a basis for V . More generally, if S is a spanning set,
then every minimal spanning subset of S is a basis for V . For inﬁnite-dimensional vector
spaces, this fact relies on the axiom of choice. (A spanning set is minimal if it contains
no proper subset that spans V .)
11. Rank-nullity theorem: If A is an m × n matrix over F then
• dim RS(A) + dim NS(A) = n;
• dim CS(A) + dim NS(A) = n;
• dim RS(A) + dim LNS(A) = m;
• dim CS(A) + dim LNS(A) = m.
12. For every matrix A, row rank A = column rank A = rank A. Thus, the (maximum)
number of independent rows of A equals the (maximum) number of independent columns.
13. The set of solutions to the m homogeneous linear equations Pn
j=1 aijxj = 0 in n
unknowns has dimension n −r, where r is the rank of the m × n coeﬃcient matrix
A = (aij).
14. If B is a basis for a vector space V (ﬁnite or inﬁnite), then each v ∈V can be
expressed as v = a1v1 + a2v2 + · · · + atvt, where ai ∈F and vi ∈B. If v = b1v1 + b2v2 +
· · · + btvt is another expression for v in terms of elements of B (where possibly some
zero coeﬃcients have been inserted to make the two expressions have equal length), then
ai = bi for i = 1, 2, . . ., t. (If B is ﬁnite, this justiﬁes the deﬁnition of the coordinate
vector [v]B.)
15. If B = (v1, v2, . . . , vn) is an ordered basis for V , then the function T : V →F n×1
deﬁned by T (v) = [v]B is an isomorphism, so V is isomorphic to F n×1.
16. Two vector spaces over F are isomorphic if and only if they have the same dimension.

394
Chapter 6
LINEAR ALGEBRA
Examples:
1. The vector space F n has dimension n.
The standard basis is the ordered basis
(e1, e2, . . . , en) where ei is the vector with 1 in position i and 0s elsewhere. (The spaces
F n, F 1×n, and F n×1 are isomorphic and are often identiﬁed and used interchangeably.)
2. The vector space F m×n of m × n matrices over F has dimension mn; the standard
basis is { Eij | 1 ≤i ≤m, 1 ≤j ≤n } where Eij is the m× n matrix with a 1 in position
(i, j) and 0s elsewhere. It is isomorphic to F mn.
3. The subspace of R3 containing all 3-tuples of the form (a, b, 2a + 3b) has dimen-
sion 2.
One basis for this subspace is B1 = ((1, 0, 2), (0, 1, 3)) and another is B2 =
((1, 1, 5), (1, −1, −1)). The vector w = (5, −1, 7) is in the subspace since w = 5(1, 0, 2) +
(−1)(0, 1, 3) = 2(1, 1, 5) + 3(1, −1, −1). The coordinate vector of w with respect to B1 is
(5, −1)T and the coordinate vector of w with respect to B2 is (2, 3)T.
4. If W is the subspace of V = Z5
2 containing all members of V whose components sum
to 0, then W has dimension 4. In fact W = { (a, b, c, d, a + b + c + d) | a, b, c, d ∈Z2 }.
One ordered basis for this space is ((1, 0, 0, 0, 1), (0, 1, 0, 0, 1), (0, 0, 1, 0, 1), (0, 0, 0, 1, 1)).
5. Binary codes:
More generally, consider the set of all binary n-tuples with an even
number of 1s; this is the linear binary code mentioned in Example 6, §6.1.2.
These
vectors form a subspace W of V = Zn
2 of dimension n −1. A basis for W consists of the
following n −1 vectors, each of which has exactly two 1s: (1, 0, . . . , 1), (0, 1, . . ., 1), . . . ,
(0, 0, . . ., 1, 1). Consequently there are 2n−1 vectors in the code W.
6. The ﬁeld C of complex numbers is two-dimensional as a vector space over R; it has
the ordered basis (1, i), where i = √−1. Any two complex numbers, neither of which is
a real multiple of the other, form a basis.
7. Both C and R are inﬁnite-dimensional vector spaces over the rational ﬁeld Q.
8. The vector space F[x] is an inﬁnite-dimensional space over F; (1, x, x2, x3, . . .) is
an ordered basis. The subspace of all polynomials of degree ≤n has dimension n + 1;
(1, x, x2, . . . , xn) is an ordered basis.
6.1.4
INNER PRODUCTS, LENGTH, AND ORTHOGONALITY
By imposing additional structure on real and complex vector spaces, the concepts of
length, distance, and orthogonality can be introduced. These concepts are motivated by
the corresponding geometric notions for physical vectors. Also, for real vector spaces the
geometric idea of angle can be formulated analytically.
Deﬁnitions:
An inner product on a vector space V over R is a function ⟨·, ·⟩: V × V →R such that
for all u, v, w ∈V and a, b ∈R the following hold:
• ⟨u, v⟩= ⟨v, u⟩;
• ⟨u, u⟩≥0 with equality if and only if u = 0;
• ⟨au + bv, w⟩= a⟨u, w⟩+ b⟨v, w⟩.
An inner product on a vector space V over C is a function ⟨·, ·⟩: V × V →C such that
for all u, v, w ∈V and a, b ∈C the following hold:
• ⟨u, v⟩= ⟨v, u⟩(where bar denotes complex conjugation);
• ⟨u, u⟩≥0 with equality if and only if u = 0;

Section 6.1
VECTOR SPACES
395
• ⟨au + bv, w⟩= a⟨u, w⟩+ b⟨v, w⟩.
Note: The ﬁrst property implies that ⟨u, u⟩is real, so the second property makes sense.
An inner product space is a vector space over R or C on which an inner product is
deﬁned. Such a space is called a real or complex inner product space, depending on its
scalar ﬁeld.
The norm (length) of a vector v ∈V is ∥v∥=
p
⟨v, v⟩.
A vector v ∈V is a unit vector if and only if ∥v∥= 1.
The distance d(v, w) from v to w is d(v, w) = ∥v −w∥.
In a real inner product space, the angle between nonzero vectors v and w is the real
number θ, 0 ≤θ ≤π, such that cos θ =
⟨v, w⟩
∥v∥· ∥w∥.
Two vectors v and w are orthogonal if and only if ⟨v, w⟩= 0.
A subset S ⊆V is an orthogonal set if ⟨v, w⟩= 0 for all v, w ∈S with v ̸= w.
A subset S ⊆V is an orthonormal set if S is an orthogonal set and ∥v∥= 1 for all
v ∈S.
If W is a subspace of an inner product space V , then the orthogonal complement
W ⊥= { v ∈V | ⟨v, w⟩= 0 for all w ∈W }.
Facts:
1. Standard inner product on Rn: The real-valued function deﬁned by ⟨x, y⟩= x1y1 +
x2y2 + · · · + xnyn is an inner product on V = Rn.
2. Standard inner product on Cn:
The complex-valued function deﬁned by ⟨x, y⟩=
x1y1 + x2y2 + · · · + xnyn is an inner product on V = Cn.
3. If A is an n × n real positive deﬁnite matrix (§6.3.2), then the function deﬁned by
⟨x, y⟩= xT Ay is an inner product on Rn. (Here xT denotes the transpose of x.)
4. If H is an n × n complex positive deﬁnite matrix (§6.3.2), then the function deﬁned
by ⟨x, y⟩= y∗Hx is an inner product on Cn. (y∗is the conjugate-transpose of y.)
5. The function ⟨f, g⟩= R b
a f(x)g(x) dx is an inner product on the vector space C[a, b]
of continuous real-valued functions on the interval [a, b].
6. The inner product ⟨·, ·⟩on an inner product space V is an inner product on any
subspace W of V .
7. If W is a subspace of an inner product space V , then the orthogonal complement W ⊥
is a subspace of V and V = W ⊕W ⊥.
8. The norm function satisﬁes the following properties for all scalars a and all vectors
v, w ∈V :
• ∥v∥≥0 with equality if and only if v = 0;
• ∥av∥= |a| · ∥v∥, where |a| denotes the absolute value of a;
• |⟨v, w⟩| ≤∥v∥· ∥w∥(Cauchy-Schwarz inequality);
• ∥v + w∥≤∥v∥+ ∥w∥(triangle inequality);
• if v ̸= 0, then
1
∥v∥v is a unit vector (the normalization of v).

396
Chapter 6
LINEAR ALGEBRA
9. The distance function on a vector space V satisﬁes the following properties for all
v, w, z ∈V :
• d(v, w) ≥0 with equality if and only if v = w;
• d(v, w) = d(w, v);
• d(v, z) ≤d(v, w) + d(w, z) (triangle inequality).
10. For real inner product spaces, two nonzero vectors are orthogonal if and only if the
angle between them is θ = π
2 .
11. An orthogonal set S of nonzero vectors can be converted to an orthonormal set by
normalizing each vector in S.
12. An orthogonal set of nonzero vectors is independent. An orthonormal set is inde-
pendent.
13. If V is an n-dimensional inner product space, any orthonormal set contains at most n
vectors, and any orthonormal set of n vectors is a basis for V .
14. Every subspace W of an n-dimensional space V has an orthonormal (orthogonal)
basis.
15. Gram-Schmidt orthogonalization:
From any ordered basis (w1, w2, . . . , wm) for
a subspace W, an orthonormal basis (u1, u2, . . . , um) for W can be constructed using
Algorithm 1. (J¨orgen Gram, 1850–1916; Erhardt Schmidt, 1876–1959)
Algorithm 1:
Gram-Schmidt orthogonalization process.
input: an ordered basis (w1, w2, . . . , wm)
output: an orthonormal basis (u1, u2, . . . , um)
u1 := 1
a1
w1, where a1 := ∥w1∥
for j := 2 to m
aj := ∥wj −
j−1
X
i=1
⟨wj, ui⟩ui∥
uj := 1
aj

wj −
j−1
X
i=1
⟨wj, ui⟩ui

16. The standard basis is orthonormal with respect to the standard inner product.
17. If (u1, u2, . . . , um) is an orthonormal basis for a subspace W of V and w ∈W, then
w = ⟨w, u1⟩u1 + ⟨w, u2⟩u2 + · · · + ⟨w, um⟩um.
18. Projection vector: Let W be a subspace of a vector space V and let v be a vector
in V .
• There is a unique vector p ∈W nearest to v; that is, the vector p minimizes
∥v −w∥over all w ∈W. This vector p is called the projection of v onto W,
written p = projW (v).
• If (u1, u2, . . . , um) is any orthonormal basis for W, then the projection of v onto W
is given by projW (v) = ⟨v, u1⟩u1 + ⟨v, u2⟩u2 + · · · + ⟨v, um⟩um.
• The vector projW (v) is the unique vector w ∈W such that v −w is orthogonal to
every vector in W.

Section 6.2
LINEAR TRANSFORMATIONS
397
19. Projection matrix:
If V = Rn is equipped with the standard inner product and
(u1, u2, . . . , um) is an orthonormal basis for a subspace W, then the projection of each
x ∈Rn onto W is given by projW (x) = Ax, where A = GGT with G = (u1, u2, . . . , um)
being the n × m matrix with the ui as columns.
20. The projection matrix A is symmetric and satisﬁes A2 = A.
Examples:
Consider the vector space R4 with the standard inner product ⟨x, y⟩= xT y, and let W
be the subspace spanned by the three vectors w1 = (1, 1, 1, 1)T, w2 = (3, 1, 3, 1)T, w3 =
(3, 1, 1, 1)T.
1. ⟨w1, w2⟩= 8 and ∥w1∥= 2.
2. The angle θ between w1 and w2 satisﬁes cos θ =
8
2
√
20 =
2
√
5 (so θ ≈0.4636 radians).
3. The distance from w1 to w2 is d(w1, w2) = ∥w1 −w2∥= ∥(−2, 0, −2, 0)T∥= 2
√
2.
4. The orthogonal complement W ⊥of W is the set of vectors of the form (0, a, 0, −a).
5. The Gram-Schmidt process applied to (w1, w2, w3) yields
u1 =
1
a1 w1 = ( 1
2, 1
2, 1
2, 1
2)T , where a1 = ∥w1∥= 2;
u2 =
1
a2 (w2 −⟨w2, u1⟩u1) =
1
a2 ((3, 1, 3, 1)T −4( 1
2, 1
2, 1
2, 1
2)T )
=
1
a2 (1, −1, 1, −1)T = ( 1
2, −1
2, 1
2, −1
2)T , where a2 = ∥(1, −1, 1, −1)T∥= 2;
u3 =
1
a3 (w3 −⟨w3, u1⟩u1 −⟨w3, u2⟩u2)
=
1
a3
 (3, 1, 1, 1)T −3( 1
2, 1
2, 1
2, 1
2)T −1( 1
2, −1
2, 1
2, −1
2)T 
=
1
a3 (1, 0, −1, 0)T = ( 1
√
2, 0, −1
√
2, 0)T , where a3 = ∥(1, 0, −1, 0)T∥=
√
2.
6. The vector in W that is nearest to v = (3, 6, 3, 4)T is p = projW (v) = ⟨v, u1⟩u1 +
⟨v, u2⟩u2 + ⟨v, u3⟩u3 = 8u1 + (−2)u2 + 0u3 = (3, 5, 3, 5)T. Further, v −p = (0, 1, 0, −1)T
is orthogonal to every vector in W; if u4 = (0,
1
√
2, 0, −1
√
2)T is the normalization of v −p
then (u1, u2, u3, u4) is an orthonormal basis for R4.
7. The projection of any x ∈R4 onto W is given by projW (x) = Ax, where
A = GGT = (u1, u2, u3)(u1, u2, u3)T =





1
0
0
0
0
1
2
0
1
2
0
0
1
0
0
1
2
0
1
2




.
Thus, if x = (3, 6, 3, 4)T, its projection onto W is computed as Ax = (3, 5, 3, 5)T, consis-
tent with the answer found in Example 6.
6.2
LINEAR TRANSFORMATIONS
Linear transformations are special types of functions that map one vector space to an-
other. They are called “linear” because of their eﬀect on the lines of a vector space,

398
Chapter 6
LINEAR ALGEBRA
where by a “line” is meant a set of vectors w of the form w = au + v where u ̸= 0
and v are ﬁxed vectors in the space and a varies over all values in the scalar ﬁeld. Linear
transformations carry lines in one vector space to lines or points in the other.
6.2.1
LINEAR TRANSFORMATIONS, RANGE, AND KERNEL
Deﬁnitions:
Let V and W be vector spaces over the same ﬁeld F. A linear transformation is a
function T : V →W satisfying T (au + v) = aT (u) + T (v) for all u, v ∈V and a ∈F.
The range RT of a linear transformation T is RT = { T (v) | v ∈V }.
The kernel ker T of a linear transformation T is ker T = { v ∈V | T (v) = 0 }.
The rank of T is the dimension of RT . (RT is a subspace of W by Fact 5.)
The nullity of T is the dimension of ker T . (ker T is a subspace of V by Fact 5.)
A linear operator on V is a linear transformation from V to V .
Facts:
1. For any vector spaces V and W over F, the zero function Z : V →W deﬁned by
Z(v) = 0 for all v ∈V is a linear transformation from V to W.
2. For any vector space V over F, the identity function I : V →V deﬁned by I(v) = v
for all v ∈V is a linear operator on V .
3. The following four statements are equivalent for a function T : V →W:
• T is a linear transformation;
• T (u + v) = T (u) + T (v) and T (au) = aT (u) for all u, v ∈V and a ∈F;
• T (au + bv) = aT (u) + bT (v) for all u, v ∈V and a, b ∈F;
• T (Pt
i=1 aivi) = Pt
i=1 aiT (vi) for all ﬁnite subsets {v1, v2, . . . , vt} ⊆V and scalars
ai ∈F.
4. If T : V →W is a linear transformation, then
• T (0) = 0;
• T (−v) = −T (v) for all v ∈V ;
• T (u −v) = T (u) −T (v) for all u, v ∈V .
5. If T : V →W is a linear transformation, then RT is a subspace of W and ker T is a
subspace of V .
6. If T : V →W is a linear transformation, then the rank of T plus the nullity of T
equals the dimension of its domain: dim RT + dim(ker T ) = dim V .
7. If T : V →W is a linear transformation and if the vectors {v1, v2, . . . , vn} span V ,
then {T (v1), T (v2), . . . , T (vn)} span RT .
8. If T : V →W is a linear transformation, then T is completely determined by its
action on a basis for V . That is, if B is a basis for V and f is any function from B to W,
then there exists a unique linear transformation T such that T (v) = f(v) for all v ∈B.
9. A linear transformation T : V →W is one-to-one if and only if ker T = {0}.
10. A linear transformation T : V →W is onto if and only if for every basis B of V , the
set { T (v) | v ∈B } spans W.

Section 6.2
LINEAR TRANSFORMATIONS
399
11. A linear transformation T : V →W is onto if and only if for some basis B of V , the
set { T (v) | v ∈B } spans W.
12. If T : V →W is a bijective linear transformation, then its inverse T −1: W →V is
also a bijective linear transformation.
13. For each ﬁxed m × n matrix A over F, the function T : F n×1 →F m×1 deﬁned by
T (x) = Ax is a linear transformation.
14. Every linear transformation T : F n×1 →F m×1 has the form T (x) = Ax for some
unique m × n matrix A over F.
15. The range RT of the linear transformation T (x) = Ax is equal to the column space
of A, and ker T is equal to the null space of A. (See §6.1.2, §6.1.3.)
16. If T is a linear transformation from V to W and if T (v0) = w0 ∈RT , then the
solution set S to the equation T (v) = w0 is S = { v0 + u | u ∈ker T }.
Examples:
1. The function T : R2×1 →R2×1 given by T
 
x1
x2
!
=
 
x1 −3x2
−2x1 + 6x2
!
is a linear trans-
formation. It has the form T (x) = Ax, where A =
 
1
−3
−2
6
!
. The kernel of T is
{ (3a, a)T | a ∈R } and the range of T is { (b, −2b)T | b ∈R }.
2. For each ﬁxed matrix A ∈F n×n, the function T : F n×n →F n×n deﬁned by T (X) =
AX −XA is a linear transformation whose kernel is the set of matrices commuting
with A.
Speciﬁcally, let n = 2, F = R, and A =
 
1
−3
−2
6
!
.
Then dim R2×2 =
4, and by computation T
" 
1
0
0
0
!#
=
 
0
3
−2
0
!
, T
" 
0
1
0
0
!#
=
 
2
−5
0
−2
!
. Thus,
dim RT ≥2. Since both the identity matrix I and A itself are in ker T , dim(ker T ) ≥2.
By Fact 6, it follows that dim RT = 2 and dim(ker T ) = 2. Therefore (I, A) forms a
basis for ker T , and the matrices
 
0
3
−2
0
!
and
 
2
−5
0
−2
!
are a basis for RT . From
Fact 16, the solutions to T (x) =
 
0
3
−2
0
!
are precisely the set of matrices of the form
 
1
0
0
0
!
+ a
 
1
0
0
1
!
+ b
 
1
−3
−2
6
!
with a, b ∈R.
3. The function E(x1, x2, x3, x4) = (x1, x2, x3, x4, x1+x3+x4, x1+x2+x4, x1+x2+x3),
where xi ∈Z2, is a linear transformation important in coding theory. It represents an
encoding of 4-bit binary vectors into 7-bit binary vectors (“codewords”) before being
sent over a noisy channel (§14.2).
The kernel of the transformation consists of only
the zero vector 0 = (0, 0, 0, 0), and so the transformation is one-to-one. The collection
of codewords (that is, the range of E), is a 16-member, 4-dimensional subspace of Z7
2
having the special property that any two of its distinct members diﬀer in at least three
components. This means that if, during transmission of a codeword, an error is made in
any single one of its components, then the error can be detected and corrected as there
will be a unique codeword that diﬀers from the received vector in a single component.
4. Continuing with Example 3, the linear transformation D(z1, z2, z3, z4, z5, z6, z7) =
(z1 + z3 + z4 + z5, z1 + z2 + z4 + z6, z1 + z2 + z3 + z7) is used in decoding the (binary)

400
Chapter 6
LINEAR ALGEBRA
received vector z. This transformation has the special property that its kernel is precisely
the set of codewords deﬁned in Example 3. Thus, if D(z) ̸= 0, then a transmission error
has been made.
5. For C as a vector space over R and any z0 ∈C, the function T : C →C deﬁned by
T (z) = z0z is a linear operator; in particular, if z0 = cos θ + i sin θ, then T is a rotation
by the angle θ. (T (z) is also a linear operator on C as a vector space over itself.)
6. For any ﬁxed real-valued continuous function g on the interval [a, b], the function T
from the space C[a, b] of continuous functions on [a, b] to the space D[a, b] of continuously
diﬀerentiable functions on [a, b] given by T (f)(x) =
R x
a g(t)f(t)dt is a linear transforma-
tion.
7. For the vector space V of functions p: R →R with continuous derivatives of all
orders, the mapping T : V →V deﬁned by T (p) = p′′ −3p′ + 2p (where p′ and p′′ are the
ﬁrst and second derivatives of p) is a linear transformation. Its kernel is the solution set
to the homogeneous diﬀerential equation p′′ −3p′ + 2p = 0: namely, p(x) = Aex + Be2x,
where A, B ∈R. Since T (x2) = 2−6x+2x2, the set of all solutions to T (p) = 2−6x+2x2
is x2 + Aex + Be2x (by Fact 16).
8. If v0 is a ﬁxed vector in a real inner product space V , then T : V →R given by
T (v) = ⟨v, v0⟩is a linear transformation.
9. For W a subspace of the inner product space V , the projection projW of V onto W
is a linear transformation. (See §6.1.4.)
6.2.2
VECTOR SPACES OF LINEAR TRANSFORMATIONS
Deﬁnitions:
If S and T are linear transformations from V to W, the sum (addition) of S and T is
the function S + T deﬁned by (S + T )(v) = S(v) + T (v) for all v ∈V .
If T is a linear transformation from V to W, the scalar product (scalar multiplica-
tion) of a ∈F by T is the function aT deﬁned by (aT )(v) = aT (v) for all v ∈V .
If T : V →W and S : W →U are linear transformations, then the product (multipli-
cation, composition) of S and T is the function S ◦T deﬁned by (S ◦T )(v) = S(T (v)).
Note: Some writers use the notation vT to denote the image of v under the transformation
T , in which case T ◦S is used instead of S ◦T to denote the product; that is, v(T ◦S) =
(vT )S.
Facts:
1. The sum of two linear transformations from V to W is a linear transformation from
V to W.
2. The product of a scalar and a linear transformation is a linear transformation.
3. If T : V →W and S : W →U are linear transformations, then their product S ◦T is
a linear transformation from V to U.
4. The set of linear transformations from V to W with the operations of addition and
scalar multiplication forms a vector space over F. This vector space is denoted L(V, W).
5. The set L(V, V ) of linear operators on V with the operations of addition, scalar mul-
tiplication, and multiplication forms an algebra with identity over F. Namely, L(V, V )
is a vector space over F and is a ring with identity under the addition and multiplication

Section 6.2
LINEAR TRANSFORMATIONS
401
operations. In addition, a(S ◦T ) = (aS) ◦T = S ◦(aT ) holds for all scalars a ∈F and
all S, T ∈L(V, V ). The identity mapping is the multiplicative identity of the algebra.
6. If dim V = n and dim W = m, then dim L(V, W) = nm.
Examples:
1. Consider L(F n×1, F m×1). If T and S are in L(F n×1, F m×1), then T (x) = Ax and
S(x) = Bx for unique m × n matrices A and B over F. Then (T + S)(x) = (A + B)x,
(aT )(x) = aAx, and in case m = n, (T ◦S)(x) = ABx.
2. Let V = C[a, b] be the space of real-valued continuous functions on the interval [a, b],
and let T and S be linear operators deﬁned by T (f)(x) =
R x
a e−tf(t) dt and S(f)(x) =
R x
a etf(t) dt. Then (T + S)(f)(x) = R x
a (e−t + et)f(t) dt, (cT )(f)(x) = R x
a ce−tf(t) dt, and
(T ◦S)(f)(x) =
R x
a
R t
a es−tf(s) ds dt.
3. Let V be the real vector space of all functions p: R →R with continuous derivatives
of all orders, and let D be the derivative function. Then D: V →V is a linear operator
on V and so is a function such as T = D2 −3D + 2I where D2 = D ◦D and I is the
identity operator on V . The action of T on p ∈V is given by T (p) = p′′ −3p′ + 2p.
6.2.3
MATRICES OF LINEAR TRANSFORMATIONS
Deﬁnitions:
If T : V →W is a linear transformation where dim V = n, dim W = m, and if B =
(v1, v2, . . . , vn) and B′ = (v′
1, v′
2, . . . , v′
m) are ordered bases for V and W, respectively,
then the matrix of T with respect to B and B′ is the m × n matrix [T ]B,B′ whose
jth column is [T (vj)]B′, the coordinate vector (§6.1.3) of T (vj) with respect to B′.
If T : V →V is a linear operator on V , then the matrix of T with respect to B is
the n × n matrix [T ]B,B denoted simply as [T ]B.
Facts:
Assume that T and S are linear transformations from V to W, B and B′ are respective
bases for V and W, and A and B are the matrices deﬁned by A = [T ]B,B′ and B = [S]B,B′.
1. [T (v)]B′ = [T ]B,B′[v]B for all v ∈V ; that is, if y = [T (v)]B′ and x = [v]B, then
y = Ax.
2. ker T = { x1v1 + x2v2 + · · · + xnvn | (x1, x2, . . . , xn)T ∈NS(A) }, where B =
(v1, v2, . . . , vn).
3. T is one-to-one if and only if NS(A) = {0}.
4. RT = { y1v′
1+y2v′
2+· · ·+ymv′
m | (y1, y2, . . . , ym)T ∈CS(A) }, where B′ = (v′
1, v′
2, . . . ,
v′
m).
5. T is onto if and only if CS(A) = F m×1.
6. T is bijective if and only if m = n and A is invertible. In this case, [T −1]B′,B = A−1.
7. [T + S]B,B′ = A + B, [aT ]B,B′ = aA for all a ∈F, and the mapping f from L(V, W)
to F m×n deﬁned by f(T ) = [T ]B,B′ is an isomorphism.
8. If U is a vector space over F, B′′ is a basis for U, and R: W →U is a linear
transformation, then [R ◦T ]B,B′′ = CA where C = [R]B′,B′′; that is, [R ◦T ]B,B′′ =
[R]B′,B′′[T ]B,B′.
9. The algebra L(V, V ) is isomorphic to the matrix algebra F n×n.

402
Chapter 6
LINEAR ALGEBRA
10. If I : V →V is the identity mapping, then [I]B,B = [I]B equals the identity matrix
for any basis B.
11. If A is an m × n matrix over F with B and B′ being arbitrary bases for V and W,
respectively, then there exists a unique linear transformation T : V →W such that
A = [T ]B,B′.
12. Linear transformations are used extensively in computer graphics. (See Example 5.)
Further information can be found in [PoGe89].
Examples:
1. Consider T : R2×1 →R2×1 given by T
 
x1
x2
!
=
 
x1 −3x2
−2x1 + 6x2
!
and the bases B =
(v1, v2) and B′ = (v′
1, v′
2), where v1 = (1, 0)T , v2 = (0, 1)T and v′
1 = (1, 1)T , v′
2 = (2, 1)T .
Since
T (v1) = (1, −2)T = (−5)v′
1 + 3v′
2,
T (v2) = (−3, 6)T = 15v′
1 + (−9)v′
2,
it follows that [T (v1)]B′ = (−5, 3)T and [T (v2)]B′ = (15, −9)T; hence, the matrix of T
relative to B and B′ is [T ]B,B′ =
 
−5
15
3
−9
!
. Similarly, [T ]B,B = [T ]B =
 
1
−3
−2
6
!
,
and [T ]B′,B′ = [T ]B′ =
 
10
5
−6
−3
!
.
2. Consider T of Example 1 where A = [T ]B,B′ =
 
−5
15
3
−9
!
.
Since NS(A) =
{ (3a, a)T | a ∈R } and CS(A) = { (−5b, 3b)T | b ∈R }, Fact 2 gives ker T = { 3av1 +
av2 = (3a, a)T | a ∈R } and Fact 4 gives RT = { (−5b)v′
1 + 3bv′
2 = (b, −2b)T | b ∈R }.
T is not one-to-one since NS(A) ̸= {0} and is not onto since CS(A) ̸= R2×1. (Any one
of the three matrices found in Example 1 could have been used to determine ker T and
RT and to reach these same conclusions.)
3. Consider the linear operator on R2×2 deﬁned by T (X) = AX −XA where A =
 
1
−3
−2
6
!
, and let B = (E11, E12, E21, E22) be the standard basis. (Here, Eij has a 1
in position (i, j) and 0s elsewhere.) Then
T (E11) = AE11 −E11A =
 
0
3
−2
0
!
= 0E11 + 3E12 + (−2)E21 + 0E22,
so (0, 3, −2, 0)T is the ﬁrst column of [T ]B. Similar calculations yield
[T ]B =





0
2
−3
0
3
−5
0
−3
−2
0
5
2
0
−2
3
0




.
The null space of this 4 × 4 matrix is { (5a + b, 3a, 2a, b)T | a, b ∈R }, so that those
matrices X commuting with A (that is, in ker T ) have the form X =
 
5a + b
3a
2a
b
!
.

Section 6.2
LINEAR TRANSFORMATIONS
403
4. Consider C as a vector space over R and the rotation operator in Example 5 of §6.2.1;
namely, T (z) = z0z where z0 = cos θ + i sin θ. If B is the standard basis, B = (1, i), then
the matrix of T relative to B is [T ]B =
 
cos θ
−sin θ
sin θ
cos θ
!
.
5. Computer graphics:
The polygon in part (a) of the following ﬁgure can be rotated
by applying the transformation T in Example 4 to its vertices (−2, −2), (1, −1), (2, 1),
(−1, 3). The matrix of vertex coordinates is
X =
 
−2
1
2
−1
−2
−1
1
3
!
.
(a)
(b)
(c)
For a rotation of π
3 , the matrix of T is
A =
 
1
2
−
√
3
2
√
3
2
1
2
!
and
AX ≈
 
0.732
1.366
0.134
−3.098
−2.732
0.366
2.232
0.634
!
,
producing the rotated polygon shown in part (b) of the ﬁgure. To perform a “zoom in”
operation, the original polygon can be rescaled by 50% by applying the transformation
S
 
x
y
!
=
 
1.5x
1.5y
!
. As the matrix for S relative to the standard basis is D =
 
1.5
0
0
1.5
!
,
the vertex coordinates X are transformed into DX =
 
−3
1.5
3
−1.5
−3
−1.5
1.5
4.5
!
; see
part (c) of the ﬁgure. Reﬂection through the x-axis would involve the transformation
R
 
x
y
!
=
 
x
−y
!
, represented by the diagonal matrix C =
 
1
0
0
−1
!
.
In computer

404
Chapter 6
LINEAR ALGEBRA
graphics, the vertices of an object are actually given (x, y, z) coordinates and three-
dimensional versions of the above transformations can be applied to move and reshape
the object as well as render the scene when the user’s viewpoint is changed.
6.2.4
CHANGE OF BASIS
Deﬁnitions:
Let B = (v1, v2, . . . , vn) and B′ = (v′
1, v′
2, . . . , v′
n) be two ordered bases for V , and let I
denote the identity mapping from V to V . The matrix P = [I]B,B′ is the transition
matrix from B to B′. It is also called the change of basis matrix from basis B to
basis B′.
If A and B are two n × n matrices over a ﬁeld F, then B is similar to A if there exists
an invertible n × n matrix P over F such that P −1BP = A.
Facts:
1. The transition matrix P = [I]B,B′ is invertible; its inverse is P −1 = [I]B′,B.
2. If x = [v]B and y = [v]B′, then y = Px where P = [I]B,B′.
3. When B = B′, the transition matrix P = [I]B,B = [I]B is the n × n identity matrix.
4. If T is a linear operator on V with A and B the matrices of T relative to bases B
and B′, respectively, then B is similar to A. Speciﬁcally, P −1BP = A where P = [I]B,B′.
5. If A and B are similar n × n matrices, then A and B represent the same linear
operator T relative to suitably chosen bases. More speciﬁcally, suppose P −1BP = A,
B = (v1, v2, . . . , vn) is any basis for V , and T is the unique linear transformation with
A = [T ]B.
Then B = [T ]B′ where B′ = (v′
1, v′
2, . . . , v′
n) is the basis for V given by
v′
j = Pn
i=1 p−1
ij vi.
Examples:
1. Consider the R2×1 bases B = (v1, v2) and B′ = (v′
1, v′
2), where v1 = (1, 0)T, v2 =
(0, 1)T and v′
1 = (1, 1)T , v′
2 = (2, 1)T . Since v1 = (−1)v′
1 + v′
2 and v2 = 2v′
1 + (−1)v′
2,
the transition matrix from B to B′ is P = [I]B,B′ =
 
−1
2
1
−1
!
, and its inverse P −1 =
 
1
2
1
1
!
is the transition matrix [I]B′,B. If v = x1v1 +x2v2 where xi ∈R, then by Fact 2,
v = y1v′
1 + y2v′
2 where y1 = (−1)x1 + 2x2 and y2 = x1 + (−1)x2.
2. Consider T : R2×1 →R2×1 given by T
 
x1
x2
!
=
 
x1 −3x2
−2x1 + 6x2
!
, and the same bases
B and B′ speciﬁed in Example 1. The matrix of T with respect to B is [T ]B = A =
 
1
−3
−2
6
!
and the matrix of T with respect to B′ is [T ]B′ = B =
 
10
5
−6
−3
!
. Moreover,
A and B are similar; indeed, as Fact 4 shows, A = P −1BP where P =
 
−1
2
1
−1
!
is
determined in Example 1.

Section 6.3
MATRIX ALGEBRA
405
6.3
MATRIX ALGEBRA
Matrices naturally arise in the analysis of linear systems and in representing discrete
structures. This section studies important types of matrices, their properties, and meth-
ods for eﬃcient matrix computation.
6.3.1
BASIC CONCEPTS AND SPECIAL MATRICES
Deﬁnitions:
The m × n matrix A = (aij) is a rectangular array of mn real or complex numbers aij,
arranged into m rows and n columns.
The transpose of the m × n matrix A = (aij) is the n × m matrix AT = (bij) in which
bij = aji.
The ith row of A, denoted A(i, :), is the array ai1 ai2 . . . ain. The elements in the ith
row can be regarded as a row vector (ai1, ai2, . . . , ain) in Rn or Cn. The jth column
of A, denoted A(:, j), is the array
a1j
a2j
...
amj
which can be identiﬁed with the column vector (a1j, a2j, . . . , amj)T .
A matrix is sparse if it has relatively few nonzero entries.
A submatrix of the matrix A contains the elements occurring in rows i1 < i2 < · · · < ik
and columns j1 < j2 < · · · < jr of A. A principal submatrix of the matrix A contains
the elements occurring in rows i1 < i2 < · · · < ik and columns i1 < i2 < · · · < ik of A.
This principal submatrix has order k and is written A[i1, i2, . . . , ik].
Two matrices A and B are equal if they are both m × n matrices with aij = bij for all
i = 1, 2, . . ., m and j = 1, 2, . . ., n.
The Hermitian adjoint of the m × n matrix A = (aij) is the n × m matrix A∗= (bij)
in which bij is the complex conjugate of aji.
If m = n, the matrix A = (aij) is square with diagonal elements a11, a22, . . . , ann. The
main diagonal contains the diagonal elements of A. An oﬀ-diagonal element is any
aij with i ̸= j. The trace of A, tr A, is the sum of the diagonal elements of A.
The following table deﬁnes special types of square matrices.
matrix
deﬁnition
identity
In = (eij) where eij =
(
1
if i = j
0
if i ̸= j (n × n matrix; each
diagonal entry is 1; each oﬀ-diagonal entry is 0)
diagonal
D = (dij) where dij = 0 if i ̸= j (nonzero entries occur only

406
Chapter 6
LINEAR ALGEBRA
matrix
deﬁnition
on the main diagonal)
lower triangular
L = (lij) where lij = 0 if j > i (nonzero entries occur only on
or below the diagonal)
upper triangular
U = (uij) where uij = 0 if j < i (nonzero entries occur only
on or above the diagonal)
unit triangular
triangular matrix with all diagonal entries 1
tridiagonal
A = (aij) where aij = 0 if |i −j| > 1 (nonzero entries occur
only on or immediately above or below the diagonal)
symmetric
real matrix A for which A = AT
skew-symmetric
real matrix A for which A = −AT
Hermitian
complex matrix A for which A = A∗
skew-Hermitian
complex matrix A for which A = −A∗
Facts:
1. Triangular matrices arise in the solution of systems of linear equations (§6.4).
2. A tridiagonal matrix can be represented as follows, where the diagonal lines represent
the (possibly) nonzero entries.
3. Tridiagonal matrices are particular types of sparse matrices. Such matrices arise in
discretized versions of continuous problems, the solution of diﬀerence equations (§3.3,
§3.4.4), and the solution of eigenvalue problems (§6.5).
4. Sparse matrices frequently arise in the solution of large systems of linear equations
(§6.4), since in many physical models a given variable typically interacts with relatively
few others. Linear systems derived from sparse matrices require less storage space and
can be solved more eﬃciently than those derived from a “dense” matrix.
5. Forming the transpose of a square matrix corresponds to “reﬂecting” the matrix
elements with respect to the main diagonal.
6. Any skew-symmetric matrix A must have aii = 0 for all i.
7. Any Hermitian matrix A must have aii real for all i.
8. If A is real then A∗= AT .
9. The columns of the identity matrix In are the standard basis vectors for Rn (§6.1.3).
10. Viewed as a linear transformation (§6.2), the identity matrix represents the identity
transformation; that is, it leaves all vectors unchanged.
11. Viewed as linear transformations, diagonal matrices with positive diagonal entries
leave the directions of the basis vectors unchanged, but alter the relative scale of the
basis vectors.

Section 6.3
MATRIX ALGEBRA
407
Examples:
1. The 2 × 2 and 3 × 3 identity matrices are I2 =
 
1
0
0
1
!
and I3 =



1
0
0
0
1
0
0
0
1


.
2. The matrix A =



6
0
1
0
2
4
1
4
3


is symmetric.
3. The matrix A =
 
1
2 −3i
2 + 3i
−4
!
is Hermitian.
4. A 2 × 2 diagonal matrix transforms the unit square in R2 into a rectangle with sides
parallel to the coordinate axes. The following ﬁgure shows the eﬀect of the diagonal
matrix
 
3
0
0
2
!
on certain vectors and on the unit square in R2. The standard basis
vectors {(1, 0)T, (0, 1)T } have been transformed to {(3, 0)T , (0, 2)T}.
(0,2)
(0,1)
(1,0)
(3,0)
(3,1)
(3,2)
(1,1)
(1,0.5)
(0.5.1)
(1.5,2)
5. A 3 × 3 diagonal matrix transforms the unit cube into a rectangular parallelepiped.
6. The standard basis vectors are all eigenvectors of a diagonal matrix with the corre-
sponding diagonal elements as their associated eigenvalues (§6.5).
6.3.2
OPERATIONS OF MATRIX ALGEBRA
Deﬁnitions:
The scalar product (dot product) of real vectors x = (x1, x2, . . . , xn) and y =
(y1, y2, . . . , yn) is the number x · y = Pn
i=1 xiyi.
The n × n matrix A is nonsingular (invertible) if there exists an n × n matrix A−1
such that AA−1 = A−1A = I. Any such matrix A−1 is an inverse of A.
An orthogonal matrix is a real square matrix A such that AT A = I.
A unitary matrix is a complex square matrix A such that A∗A = I, where A∗is the
Hermitian adjoint of A (§6.3.1).
A positive deﬁnite matrix is a real symmetric (or complex Hermitian) matrix A such
that x∗Ax > 0 for all x ̸= 0.

408
Chapter 6
LINEAR ALGEBRA
The nonnegative powers of a square matrix A are given by A0 = I, An = AAn−1. If A
is nonsingular then A−n = (A−1)n.
The following table deﬁnes various operations deﬁned on matrices A = (aij) and B =
(bij). (See Facts 1, 2, 5, 6 for restrictions on the sizes of the matrices.)
operation
deﬁnition
sum A + B
A + B = (cij) where cij = aij + bij
diﬀerence A −B
A −B = (cij) where cij = aij −bij
scalar multiple αA
αA = (cij) where cij = αaij
product AB
AB = (cij) where cij = P
k aikbkj
Facts:
1. Matrices of diﬀerent dimensions cannot be added or subtracted.
2. Square matrices of the same dimension can be multiplied.
3. Real or complex matrix addition satisﬁes the following properties:
• commutative: A + B = B + A;
• associative: A + (B + C) = (A + B) + C,
A(BC) = (AB)C;
• distributive: A(B + C) = AB + AC,
(A + B)C = AC + BC;
• α(A + B) = αA + αB, α(AB) = (αA)B = A(αB) for all scalars α.
4. Matrix multiplication is not, in general, commutative—even when both products are
deﬁned. (See Example 3.)
5. The product AB is deﬁned if and only if the number of columns of A equals the
number of rows of B. That is, A must be an m × n matrix and B must be an n × p
matrix.
6. The ijth element of the product C = AB is the scalar product of row i of A and
column j of B:
i  th   row
j th column
i j th element
A
B
C
7. Multiplication by identity matrices of the appropriate dimension leaves a matrix
unchanged: if A is m × n, then ImA = AIn = A.
8. Multiplication by diagonal matrices has the eﬀect of scaling the rows or columns of
a matrix. Pre-multiplication by a diagonal matrix scales the rows:






d11
0
· · ·
0
0
d22
· · ·
0
...
...
...
0
0
· · ·
dnn












a11
· · ·
a1p
a21
· · ·
a2p
...
...
an1
· · ·
anp






=






d11a11
· · ·
d11a1p
d22a21
· · ·
d22a2p
...
...
dnnan1
· · ·
dnnanp






.

Section 6.3
MATRIX ALGEBRA
409
Post-multiplication by a diagonal matrix scales the columns:






a11
· · ·
a1n
a21
· · ·
a2n
...
...
am1
· · ·
amn












d11
0
· · ·
0
0
d22
· · ·
0
...
...
...
0
0
· · ·
dnn






=






d11a11
· · ·
dnna1n
d11a21
· · ·
dnna2n
...
...
d11am1
· · ·
dnnamn






.
9. Any Hermitian matrix can be expressed as A + iB where A is symmetric and B is
skew-symmetric.
10. The inverse of a (nonsingular) matrix is unique.
11. If A is nonsingular, the solution of the system of linear equations (§6.4) Ax = b is
given by (but almost never computed by) x = A−1b.
12. The product of nonsingular matrices A and B is nonsingular, with (AB)−1 =
B−1A−1.
Conversely, if A and B are square matrices with AB nonsingular, then A
and B are nonsingular.
13. For a nonsingular matrix regarded as a linear transformation (§6.2), the inverse
matrix represents the inverse transformation.
14. Sums of lower (upper) triangular matrices are lower (upper) triangular.
15. Products of lower (upper) triangular matrices are lower (upper) triangular.
16. A triangular matrix A is nonsingular if and only if aii ̸= 0 for all i.
17. If a lower (upper) triangular matrix is nonsingular then its inverse is lower (upper)
triangular.
18. Properties of transpose:
• (AT )T = A;
• (A + B)T = AT + BT ;
• (AB)T = BT AT ;
• AAT and AT A are symmetric;
• if A is nonsingular then so is AT ; moreover (AT )−1 = (A−1)T .
19. Properties of Hermitian adjoint:
• (A∗)∗= A;
• (A + B)∗= A∗+ B∗;
• (AB)∗= B∗A∗;
• AA∗and A∗A are Hermitian;
• if A is nonsingular, then so is A∗; moreover (A∗)−1 = (A−1)∗.
20. If A is orthogonal, then A is nonsingular and A−1 = AT .
21. The rows (columns) of an orthogonal matrix are orthonormal with respect to the
standard inner product on Rn (§6.1.4).
22. Products of orthogonal matrices are orthogonal.
23. If A is unitary, then A is nonsingular and A−1 = A∗.
24. The rows (columns) of a unitary matrix are orthonormal with respect to the stan-
dard inner product on Cn (§6.1.4).
25. Products of unitary matrices are unitary.

410
Chapter 6
LINEAR ALGEBRA
26. Positive deﬁnite matrices are nonsingular.
27. All eigenvalues (§6.5) of a positive deﬁnite matrix are positive.
28. Powers of a positive deﬁnite matrix are positive deﬁnite.
29. If A is skew-symmetric, then I + A is positive deﬁnite.
30. If A is nonsingular, then AT A is positive deﬁnite.
Examples:
1. Let A =
 
1
2
3
4
5
6
!
and B =
 
7
8
9
0
1
2
!
.
Then A + B =
 
8
10
12
4
6
8
!
and
A −B =
 
−6
−6
−6
4
4
4
!
.
2. The scalar product of the vectors a = (1, 0, −1) and b = (4, 3, 2) is a · b = (1)(4) +
(0)(3) + (−1)(2) = 2.
3. Let A =
 
1
0
1
1
!
, B =
 
2
3
4
1
!
, and C =
 
1
1
2
0
2
3
!
. Then AB and BA are both
deﬁned with AB =
 
1
0
1
1
!  
2
3
4
1
!
=
 
2
3
6
4
!
, whereas BA =
 
2
3
4
1
!  
1
0
1
1
!
=
 
5
3
5
1
!
. Also, AC is deﬁned but CA is not deﬁned.
4. The matrices A, B of Example 1 cannot be multiplied since A has 3 columns and B
has 2 rows; see Fact 5. However, all the products AT B, ABT , BT A, BAT exist: AT B =



1
4
2
5
3
6



 
7
8
9
0
1
2
!
=



7
12
17
14
21
28
21
30
39


, ABT =
 
1
2
3
4
5
6
! 


7
0
8
1
9
2


=
 
50
8
122
17
!
,
BT A =



7
14
21
12
21
30
17
28
39


, BAT =
 
50
122
8
17
!
. Note that (BT A)T = AT B, as guaranteed
by Fact 18.
5. Multiplication by a diagonal matrix:
 
3
0
0
2
!  
1
2
3
4
5
6
!
=
 
3
6
9
8
10
12
!
and
 
1
2
3
4
5
6
! 


2
0
0
0
3
0
0
0
1


=
 
2
6
3
8
15
6
!
.
6. The 2 × 2 matrix A =
 
a
b
c
d
!
is nonsingular if ∆= ad −bc ̸= 0; in this case
A−1 = 1
∆
 
d
−b
−c
a
!
.
7. The matrix A = 1
9



4
8
1
7
−4
4
−4
1
8


is orthogonal.

Section 6.3
MATRIX ALGEBRA
411
8. If A = 1
2



1
−i
−1 + i
i
1
1 + i
1 + i
−1 + i
0


then A∗= 1
2



1
−i
1 −i
i
1
−1 −i
−1 −i
1 −i
0


.
Since A∗A = I the matrix A is unitary.
9. Every 2 × 2 orthogonal matrix Q can be written as Q =
 
cos θ
−sin θ
sin θ
cos θ
!
for some
real θ. Geometrically, the matrix Q eﬀects a counterclockwise rotation by the angle θ.
10. For the matrix Q in Example 9, Q2 =
 
cos2 θ −sin2 θ
−2 sinθ cos θ
2 sin θ cos θ
cos2 θ −sin2 θ
!
.
Since
this must be the same as a rotation by an angle of 2θ, then Q2 =
 
cos 2θ
−sin 2θ
sin 2θ
cos 2θ
!
.
Equating these two expressions for Q2 gives the double angle formulas of trigonometry.
11. The matrix



4
2i
−3 + i
−2i
−8
6 + 3i
−3 −i
6 −3i
5


is Hermitian. It can be written (see Fact 9)
as A + Bi =



4
0
−3
0
−8
6
−3
6
5


+



0
2i
i
−2i
0
3i
−i
−3i
0


with A symmetric and B skew-
symmetric.
6.3.3
FAST MULTIPLICATION OF MATRICES
A variety of methods have been devised to multiply matrices more eﬃciently than by sim-
ply using the deﬁnition in §6.3.2. This section presents alternative methods for carrying
out matrix multiplication.
Deﬁnitions:
The shift left operation shL(A(i, :), k) rotates elements of row i in matrix A exactly k
places to the left, where data shifted oﬀthe left side of the matrix are wrapped around
to the right side.
The shift up operation shU(B(:, j), k) rotates elements of column j in matrix B exactly
k places up, where data shifted oﬀthe top of the matrix are wrapped around to the
bottom.
These operations can also be applied simultaneously to every row of A or every column
of B, denoted shL(A, k) and shU(B, k) respectively.
Facts:
1. The basic deﬁnition given in §6.3.2 can be used to multiply the m × n matrix A and
the n×p matrix B. The associated algorithm (Algorithm 1) requires O(mnp) operations
(additions and multiplications of individual elements).
Algorithm 1:
Basic matrix multiplication.
input: m × n matrix A, n × p matrix B

412
Chapter 6
LINEAR ALGEBRA
output: m × p matrix C = AB
for i := 1 to m
for j := 1 to p
C(i, j) := 0
for k := 1 to n
C(i, j) := C(i, j) + A(i, k)B(k, j)
2. Matrix multiplication in scalar product form: Algorithm 1 can be rewritten in terms
of the scalar product operation, giving Algorithm 2.
Algorithm 2:
Scalar product form of matrix multiplication.
input: m × n matrix A, n × p matrix B
output: m × p matrix C = AB
for i := 1 to m
for j := 1 to p
C(i, j) := A(i, :) · B(:, j)
3. Algorithm 2 is well suited for fast multiplication on computers designed for eﬃcient
scalar product operations. It requires O(mp) scalar products.
4. Matrix multiplication in linear combination form:
Algorithm 3 carries out matrix
multiplication by taking a linear combination of columns of A to obtain each column of
the product.
Algorithm 3:
Column linear combination form of matrix multiplication.
input: m × n matrix A, n × p matrix B
output: m × p matrix C = AB
for j := 1 to p
C(:, j) := 0
for k := 1 to n
C(:, j) := C(:, j) + B(k, j)A(:, k)
5. The inner loop of Algorithm 3 performs a “vector + (scalar × vector)” operation,
well suited to a vector computer using eﬃciently pipelined arithmetic processing.
6. Algorithm 3 is often used for fast general matrix multiplication on vector machines
since it is based on a natural vector operation. If these vector operations can be performed
on all elements simultaneously, then O(np) vector operations are needed.
7. Access to matrix elements in Algorithm 3 is by column. There are other rearrange-
ments of the algorithm which access matrix information by row.
8. Fast multiplication on array processors: Algorithm 4 multiplies two n×n (or smaller
dimension) matrices on a computer with an n × n array of processors. It uses various
shift operations on the arrays and the array-multiplication operation (∗) of elementwise
multiplication.

Section 6.3
MATRIX ALGEBRA
413
Algorithm 4:
Array processor matrix multiplication.
input: n × n matrices A, B
output: n × n matrix C = AB
{Preshift the matrix arrays}
for i := 1 to n
shL(A(i, :), i −1)
{Shift ith row i −1 places left}
shU(B(:, i), i −1)
{Shift ith column i −1 places up}
C := 0
{Initialize product array}
for k := 1 to n
C := C + A ∗B
shL(A, 1)
shU(B, 1)
9. At each step Algorithm 4 shifts A one place to the left and shifts B one place up
so that components of the array product are correct new terms for the corresponding
elements of C = AB.
Each matrix is preshifted so the ﬁrst step complies with this
requirement.
10. Two n × n matrices can be multiplied in O(n) time using Algorithm 4 on an array
processor.
11. Strassen’s algorithm: Algorithm 5 recursively carries out matrix multiplication for
n × n matrices A and B where n = 2k. The basis of Strassen’s algorithm is partitioning
the two factors into square blocks with dimension half that of the original matrices.
Algorithm 5:
Strassen’s algorithm for 2k × 2k matrices.
procedure Strassen(A, B)
input: 2k × 2k matrices A, B
output: 2k × 2k matrix C = AB
if k = 1 then use Algorithm 6
else
partition A, B into four 2k−1×2k−1 blocks A =
 
A11
A12
A21
A22
!
, B =
 
B11
B12
B21
B22
!
P := Strassen((A11 + A22), (B11 + B22))
Q := Strassen((A21 + A22), B11); R := Strassen(A11, (B12 −B22))
S := Strassen(A22, (B21 −B11)); T := Strassen((A11 + A12), B22)
U := Strassen((A21 −A11), (B11 + B12))
V := Strassen((A12 −A22), (B21 + B22))
C11 := P +S −T +V ;
C12 := R+T ;
C21 := Q+S;
C22 := P −Q+R+U
C :=
 
C11
C12
C21
C22
!
12. Strassen’s algorithm ultimately requires the fast multiplication of 2 × 2 matrices
(Algorithm 6).

414
Chapter 6
LINEAR ALGEBRA
Algorithm 6:
Strassen’s algorithm for 2 × 2 matrices.
input: 2 × 2 matrices A, B
output: 2 × 2 matrix C = AB
p := (a11 + a22)(b11 + b22); q := (a21 + a22)b11; r := a11(b12 −b22)
s := a22(b21 −b11); t := (a11 + a12)b22; u := (a21 −a11)(b11 + b12)
v := (a12 −a22)(b21 + b22)
c11 := p + s −t + v; c12 := r + t; c21 := q + s; c22 := p −q + r + u
13. Algorithm 6 multiplies two 2 × 2 matrices using only 7 multiplications and 18 addi-
tions instead of the normal 8 multiplications and 4 additions. For most modern computers
saving one multiplication at the cost of 14 extra additions would not represent a gain.
14. Strassen’s algorithm can be extended to n× n matrices where n is not a power of 2.
The general algorithm requires O(nlog2 7) ≈O(n2.807) multiplications. Details of this
algorithm and its eﬃciency can be found in [GoVL13].
Examples:
1. This example illustrates Algorithm 4 for 4 × 4 array matrix multiplication.
The
preshift and the ﬁrst array multiplication yield the arrays
a11
a12
a13
a14
a22
a23
a24
a21
a33
a34
a31
a32
a44
a41
a42
a43
b11
b22
b33
b44
b21
b32
b43
b14
b31
b42
b13
b24
b41
b12
b23
b34
a11b11
a12b22
a13b33
a14b44
a22b21
a23b32
a24b43
a21b14
a33b31
a34b42
a31b13
a32b24
a44b41
a41b12
a42b23
a43b34
The next shifts and multiply-accumulate operation produce
a12
a13
a14
a11
a23
a24
a21
a22
a34
a31
a32
a33
a41
a42
a43
a44
b21
b32
b43
b14
b31
b42
b13
b24
b41
b12
b23
b34
b11
b22
b33
b44
a11b11 + a12b21
a12b22 + a13b32
a13b33 + a14b43
a14b44 + a11b14
a22b21 + a23b31
a23b32 + a24b42
a24b43 + a21b13
a21b14 + a22b24
a33b31 + a34b41
a34b42 + a31b12
a31b13 + a32b23
a32b24 + a33b34
a44b41 + a41b11
a41b12 + a42b22
a42b23 + a43b33
a43b34 + a44b44
At subsequent stages the remaining terms get added in to the appropriate elements
of the product matrix. The total cost of matrix multiplication is therefore reduced to
n parallel multiply-accumulate operations plus some communication costs, which for a
typical distributed memory array processor are generally small.
2. Algorithm 6 is illustrated using the matrices A =
 
3
4
−1
2
!
, B =
 
7
3
1
−3
!
. Then
p = 5 · 4 = 20,
q = 1 · 7 = 7,
r = 3 · 6 = 18,
s = 3 · (−6) = −12,
t = 7 · (−3) = −21,
u = (−4) · 10 = −40,
v = 2 · (−2) = −4,
giving the following elements of C = AB: c11 = 20−12+21−4 = 25, c12 = 18−21 = −3,
c21 = 7 −12 = −5, c22 = 20 −7 + 18 −40 = −9.

Section 6.3
MATRIX ALGEBRA
415
6.3.4
DETERMINANTS
Deﬁnitions:
For an n × n matrix A with n > 1, Aij denotes the (n −1) × (n −1) matrix obtained by
deleting row i and column j from A.
The determinant det A of an n × n matrix A can be deﬁned recursively:
• if A = (a) is a 1 × 1 matrix, then det A = a;
• if n > 1, then det A = Pn
j=1(−1)j+1a1j det A1j.
A minor of a matrix is the determinant of a square submatrix of the given matrix. A
principal minor is the determinant of a principal submatrix.
Notation: The determinant of A = (aij) is commonly written using vertical bars:
det A = |A| =

a11
a12
· · ·
a1n
a21
a22
· · ·
a2n
...
...
...
an1
an2
· · ·
ann

.
Facts:
1. Laplace expansion: For any r,
det A =
n
X
j=1
(−1)r+jarj det Arj =
n
X
i=1
(−1)i+rair det Air.
2. If A = (aij) is n × n, then det A = P
σ∈Sn sgn(σ) a1σ(1)a2σ(2) . . . anσ(n). Here Sn is
the set of all permutations on {1, 2, . . ., n}, and sgn(σ) equals 1 if σ is even and −1 if σ
is odd (§5.3.1).
3. det
 
a
b
c
d
!
=

a
b
c
d
 = ad −bc.
4. det



a
b
c
d
e
f
g
h
i


=

a
b
c
d
e
f
g
h
i

= aei + bfg + cdh −afh −bdi −ceg.
5. det AB = det A det B = det BA for all n × n matrices A, B.
6. det AT = det A for all n × n matrices A.
7. det αA = αn det A for all n × n matrices A and all scalars α.
8. det I = 1.
9. If A has two identical rows (or two identical columns), then det A = 0.
10. Interchanging two rows (or two columns) of a matrix changes the sign of the deter-
minant.
11. Multiplying one row (or column) of a matrix by a scalar multiplies its determinant
by that same scalar.
12. Adding a multiple of one row (column) to another row (column) leaves the value of
the determinant unchanged.

416
Chapter 6
LINEAR ALGEBRA
13. If D = (dij) is an n × n diagonal matrix, then det D = d11d22 . . . dnn.
14. If T = (tij) is an n × n triangular matrix, then det T = t11t22 . . . tnn.
15. If A and D are square matrices, then det
 
A
B
0
D
!
= det A det D = det
 
A
0
C
D
!
.
16. A is nonsingular if and only if det A ̸= 0.
17. If A is nonsingular then det(A−1) =
1
det A.
18. If A and D are nonsingular, then det
 
A
B
C
D
!
= det A det(D −CA−1B) =
det D det(A −BD−1C).
19. The determinant of a Hermitian matrix (§6.3.1) is real.
20. The determinant of a skew-symmetric matrix (§6.3.1) of odd size is zero.
21. The determinant of an orthogonal matrix (§6.3.2) is ±1.
22. The n×n symmetric (or Hermitian) matrix A is positive deﬁnite if and only if all its
leading principal submatrices A[1], A[1, 2], . . ., A[1, 2, . . . , n] have a positive determinant.
23. The n × n Vandermonde matrix






1
x1
. . .
xn−1
1
1
x2
. . .
xn−1
2
...
...
...
1
xn
. . .
xn−1
n






has determinant Q
i<j(xj −xi).
24. If the n×n matrix A = (aij) has diagonal elements aii = x and oﬀ-diagonal elements
aij = y, then det A = (x −y)n−1(x −y + ny).
25. The equation of the straight line through points (a1, b1) and (a2, b2) is given by

x
y
1
a1
b1
1
a2
b2
1

= 0.
26. The equation of the circle through points (a1, b1), (a2, b2), (a3, b3) is given by

x2 + y2
x
y
1
a2
1 + b2
1
a1
b1
1
a2
2 + b2
2
a2
b2
1
a2
3 + b2
3
a3
b3
1

= 0.
27. If the three points (a1, b1), (a2, b2), (a3, b3) are listed in counterclockwise order, then
the area of the triangle they form is given by
1
2

a1
b1
1
a2
b2
1
a3
b3
1

.
28. The parallelepiped P = { α1a1 + α2a2 + · · · + αnan | 0 ≤αi ≤1 } spanned by the
vectors a1, a2, . . . , an has volume | det A |, where A has columns a1, a2, . . . , an.
29. Computation: The determinant is (almost) never computed from the deﬁnition or
from Fact 2. Instead it is calculated using Facts 12 and 14. (See Example 1.)

Section 6.3
MATRIX ALGEBRA
417
Examples:
1. Determinants can be calculated by using row operations to create a triangular matrix,
and then applying Fact 14:
det



−1
2
1
0
5
2
3
4
3


= det



−1
2
1
0
5
2
0
10
6


= det



−1
2
1
0
5
2
0
0
2


= −10.
Here the second matrix is obtained from the ﬁrst by adding 3 times row 1 to row 3; the
third matrix is obtained from the second by adding −2 times row 2 to row 3.
2. Determinants can be calculated by using row and column interchanges to obtain a
form with exploitable zeros:
det





4
5
1
6
0
6
0
3
0
5
0
2
3
3
2
4




= −det





4
1
5
6
0
0
6
3
0
0
5
2
3
2
3
4




= det





4
1
5
6
3
2
3
4
0
0
5
2
0
0
6
3




.
Here the second matrix is obtained from the ﬁrst by interchanging columns 2 and 3;
the third matrix is obtained from the second by interchanging rows 2 and 4. The third
matrix has block triangular form, with diagonal blocks A =
 
4
1
3
2
!
and D =
 
5
2
6
3
!
.
By Fact 15, the original determinant equals det A det D = 5 · 3 = 15.
3. The symmetric matrix A =



3
1
0
1
5
3
0
3
4


is positive deﬁnite, since its leading principal
minors (Fact 22) are positive: det(3) = 3 > 0, det
 
3
1
1
5
!
= 14 > 0, and (by Fact 1)
det A = 3 det
 
5
3
3
4
!
−det
 
1
3
0
4
!
= 3 · 11 −4 = 29 > 0.
4. The equation of the line through points (1, 3) and (4, 5) can be found using Fact 25:

x
y
1
1
3
1
4
5
1

=

x
y
1
1
3
1
0
−7
−3

=

x
y −7
3
1
1
2
3
1
0
0
−3

= (2
3x −y + 7
3)(−3) = 0,
giving 2
3x −y + 7
3 = 0 or y = 2
3x + 7
3.
5. By Fact 27, the area of the triangle formed by the points (0, 0), (1, 3), and (4, 5) is
1
2

0
0
1
4
5
1
1
3
1

= 1
2

4
5
1
3
 = 7
2.
6. Cayley’s formula: The determinant of the (n −1) × (n −1) matrix
Tn =






n −1
−1
. . .
−1
−1
n −1
. . .
−1
...
...
...
−1
−1
. . .
n −1







418
Chapter 6
LINEAR ALGEBRA
counts the number of spanning trees of a complete graph. (See §9.2.2.) Using Fact 24,
det Tn = nn−2[n −(n −1)] = nn−2.
6.3.5
RANK
Deﬁnition:
The rank of an m × n matrix A, written rankA, is the size of the largest square nonsin-
gular submatrix of A.
Facts:
1. rank A = rankAT .
2. The rank of A equals the maximum number of linearly independent rows or linearly
independent columns in A.
3. rank(A + B) ≤rankA + rank B.
4. rank AB ≤min{rankA, rank B}.
5. If A is nonsingular then rankAB = rank B and rank CA = rankC.
6. rank A = dim CS(A), where CS(A) is the column space of A and dim V denotes the
dimension of the vector space V . (See §6.1.3.)
7. rank A = dim RS(A), where RS(A) is the row space of A. (See §6.1.3.)
8. An n × n matrix A is nonsingular if and only if rank A = n.
9. Every matrix of rank r can be written as a sum of r matrices of rank 1.
10. If a and b are nonzero n × 1 vectors, then abT is an n × n matrix of rank 1.
11. The rank of a matrix is not always easy to compute.
In the absence of severe
roundoﬀerrors, it can be obtained by counting the number of nonzero rows at the end
of the Gaussian elimination procedure (§6.4.2).
12. System of linear equations:
Consider the system Ax = b, where A is m × n. Let
Ab = (A : b) denote the m × (n + 1) matrix whose (n + 1)st column is the vector b. Then
the system Ax = b has
• a unique solution ⇔rank A = rank Ab = n;
• inﬁnitely many solutions ⇔rank A = rank Ab < n;
• no solution ⇔rankA < rank Ab.
Examples:
1. The matrix A =



1
−1
2
3
4
−1
5
2
3


is singular since det A = 0. However, the submatrix
A[1, 2] =
 
1
−1
3
4
!
has determinant 7 and so is nonsingular, showing that rankA = 2.
The matrix A has two linearly independent rows: row 3 = 2 × (row 1) + (row 2).
Likewise, it has two linearly independent columns: column 3 = (column 1) −(column
2). This again conﬁrms (by Fact 2) that rank A = 2.
2. Consider the system of equations Ax = b, where A is the matrix in Example 1 and
b = (0, 7, 7)T. Since rank A = rank Ab = 2 < 3, this system has inﬁnitely many solutions
x. In fact, the set of solutions is given by { (1 −α, 1 + α, α)T | α ∈R }.

Section 6.3
MATRIX ALGEBRA
419
3. The matrix A =



1
x
x2
x
x2
x3
x2
x3
x4


can be expressed as the product aaT where a is the
column vector (1, x, x2)T . By Fact 10, A has rank 1.
6.3.6
IDENTITIES OF MATRIX ALGEBRA
Facts:
1. Cauchy-Binet formula: If C is m×m and C = AB where A is m×n and B is n×m,
then the determinant of C is given by the sum of all products of order m minors of A
and the corresponding order m minors of B:
det C =
X
1≤s1<s2<···<sm≤n

a1s1
a1s2
· · ·
a1sm
a2s1
a2s2
· · ·
a2sm
...
...
...
ams1
ams2
· · ·
amsm

·

bs11
bs12
· · ·
bs1m
bs21
bs22
· · ·
bs2m
...
...
...
bsm1
bsm2
· · ·
bsmm

.
• If m = n there is only one possible selection; the Cauchy-Binet formula for this
case reduces to det C = det A det B; (see Fact 5, §6.3.4).
• If m > n no possible selections exist (the sum is empty), so det C = 0.
2. Courant-Fischer minimax identity:
If the eigenvalues (§6.5) of an n × n Hermitian
matrix A are ordered so that λ1 ≥λ2 ≥· · · ≥λn, then λk =
max
dim V = k
min
0̸=x∈V
xT Ax
xT x where
V is a linear subspace of Cn.
3. Hadamard’s inequality: This gives an upper bound for the determinant of an n × n
matrix A in terms of the l2 norms (§6.4.5) of its rows (or columns):
• in terms of rows:
(det A)2 ≤Qn
i=1
Pn
j=1 |aij|2
= Qn
i=1 ||A(i, :)||2;
• in terms of columns:
(det A)2 ≤Qn
j=1
Pn
i=1 |aij|2
= Qn
j=1 ||A(:, j)||2.
4. Sherman-Morrison identity: If A is a nonsingular n × n matrix and u, v ∈Rn, then
(A −uvT )−1 = A−1 +
1
1 −vT A−1u (A−1uvT A−1).
5. Woodbury’s identity: If A is nonsingular, then
(A −UV T )−1 = A−1 + A−1U(I −V T A−1U)−1V T A−1.
6. Suppose A is a nonsingular n × n matrix, with S a set of k indices i1 < i2 < · · · < ik
and S the set of remaining indices in {1, 2, . . ., n}. Then the principal minors of A−1 are
related to the principal minors of A via
det A−1[S] =
1
det A det A[S].

420
Chapter 6
LINEAR ALGEBRA
7. Jacobi’s identity:
If the n × n system of linear diﬀerential equations dx
dt = P(t)x
has the linearly independent family of solutions X(:, j)(t) for j = 1, 2, . . ., n, then the
determinant of the (variable) matrix X(t) whose columns are the X(:, j)(t) is given by
det X(t1) = c exp
Z t1
t0
tr P(t) dt

,
where c is a constant and tr P(t) = p11(t)+p22(t)+· · ·+pnn(t) is the trace of the matrix
P(t).
6.4
LINEAR SYSTEMS
The need to ﬁnd solutions of linear systems arises in numerous branches of science and
engineering (physics, biology, chemistry, structural engineering, electrical engineering,
civil engineering) as well as in statistics and applied mathematics. This section discusses
various techniques for the eﬃcient solution of such systems, especially important when
these systems are large and sparse.
6.4.1
BASIC CONCEPTS
This subsection is concerned with representing and solving a system of m linear equations
in n unknowns. Throughout, the focus will be on systems whose data are real numbers.
The extension to linear systems with complex data is straightforward.
Deﬁnitions:
A linear equation in unknowns x1, x2, . . . , xn is an equation of the form Pn
j=1 ajxj = b,
where the coeﬃcients aj ∈R and the right-hand side b ∈R. A solution of this
equation is any set of values x1, x2, . . . , xn satisfying the given equation.
A system of linear equations in unknowns x1, x2, . . . , xn is a collection of m equations
Pn
j=1 aijxj = bi, i = 1, 2, . . ., m where all aij ∈R and all bi ∈R.
A solution of
this system is any set of values x1, x2, . . . , xn satisfying (simultaneously) the m given
equations. The coeﬃcient matrix of this system is the m × n matrix A = (aij), and
the augmented matrix is the m × (n + 1) matrix Ab = (A : b).
A homogeneous system has right-hand sides bi = 0 for all i = 1, 2, . . ., m; otherwise
the system is nonhomogeneous.
Back substitution is a simple and eﬃcient iterative procedure for solving an upper
triangular linear system Ux = b, one unknown at a time.
Forward substitution is a simple and eﬃcient iterative procedure for solving a lower
triangular linear system Lx = b, one unknown at a time.
Facts:
1. A system of m linear equations in the unknowns x1, x2, . . . , xn can be represented by
the linear system Ax = b where A is the m × n coeﬃcient matrix, x = (x1, x2, . . . , xn)T
is the column vector of unknowns, and b = (b1, b2, . . . , bm)T is the column vector of
right-hand sides.

Section 6.4
LINEAR SYSTEMS
421
2. Given the linear system Ax = b, where A is m × n
• the system has no solution when rank A < rank Ab;
• the system has a unique solution when rank A = rank Ab = n;
• the system has inﬁnitely many solutions when rank A = rank Ab < n; in this case
the set of solutions is an aﬃne subspace of dimension n −rank A (§6.1.3).
3. If the square matrix A is nonsingular (§6.3.2), then Ax = b has the unique solution
vector x = A−1b.
4. If the square matrix L = (lij) is lower triangular, then Lx = b has a unique solution
whenever lii ̸= 0 for all i. In this case the solution can be found using forward substitution
(Algorithm 1).
Algorithm 1:
Forward substitution.
input: n × n nonsingular matrix L, n × 1 vector b
output: n × 1 vector x = L−1b
x1 := b1
l11
for i := 2 to n
xi := 1
lii

bi −
i−1
X
j=1
lijxj

5. If the square matrix U = (uij) is upper triangular, then Ux = b has a unique solution
whenever uii ̸= 0 for all i. In this case the solution can be found using back substitution
(Algorithm 2).
Algorithm 2:
Back substitution.
input: n × n nonsingular matrix U, n × 1 vector b
output: n × 1 vector x = U −1b
xn := bn
unn
for i := n −1 down to 1
xi := 1
uii

bi −
n
X
j=i+1
uijxj

Examples:
1. The system of linear equations
x1 + 3x2 + 4x3 = 1
3x1 + 5x2 + 0x3 = 7
corresponds to the linear system Ax = b, where A =
 
1
3
4
3
5
0
!
and b =
 
1
7
!
. Since
rankA = rank Ab = 2 < 3 the system has an inﬁnite number of solutions. In fact the set
of solutions can be expressed as { (4 + 5a, −1 −3a, a)T | a ∈R }. Equivalently, it can
be expressed as the aﬃne subspace { (4, −1, 0)T + a(5, −3, 1)T | a ∈R } of dimension
n −rank A = 3 −2 = 1.

422
Chapter 6
LINEAR ALGEBRA
2. The system of linear equations
5x1 −3x2 + 4x3 = 4
−x2 + 5x3 = 7
3x3 = 6
has the upper triangular coeﬃcient matrix U =



5
−3
4
0
−1
5
0
0
3


. Using Algorithm 2, the
unique solution is x3 = 6
3 = 2, x2 =
1
−1(7 −5 · 2) = 3, x1 = 1
5(4 −(−3) · 3 −4 · 2) = 1.
6.4.2
GAUSSIAN ELIMINATION
Solving a system of linear equations via Gaussian elimination is one of the most common
computations performed by scientists and engineers. Gaussian elimination successively
eliminates variables from the original system, creating a triangular system that is easily
solved (§6.4.1).
Note: This subsection deals only with linear systems Ax = b, where A is a nonsingular
n × n real matrix and b ∈Rn.
Deﬁnitions:
Gaussian elimination is a method for solving systems of linear equations; at each
step, one equation is used to eliminate one variable from the rest of the equations. The
coeﬃcient of the eliminated variable in the eliminated equation is the pivot.
A ﬂop is a multiply-add operation of the form t = s + ab, especially when performed in
ﬂoating point arithmetic on a digital computer.
Roundoﬀerrors are the errors associated with storing and computing numbers in ﬁnite
precision arithmetic on a digital computer.
A numerically stable algorithm is a method whose accuracy is not greatly harmed
by roundoﬀerrors.
A numerically unstable algorithm is a method that may return an inaccurate solution
even when the solution is relatively insensitive to errors in the data.
Facts:
1. Gaussian elimination is easily extended to linear systems for which the data A and b
are complex. Extension to rectangular m × n linear systems is more involved, but not
diﬃcult [GoVL13].
2. In Gaussian elimination, the coeﬃcient of a variable in one of the equations can be
used as a pivot if and only if its value is nonzero.
3. In practice, careful choice of pivots is needed to ensure accuracy or improve eﬃciency
or both. (See Example 2.)
4. Assume freedom at each step to choose any available nonzero pivot. Then Gaussian
elimination succeeds (using exact arithmetic) if and only if A is nonsingular.
5. Gaussian elimination transforms the initial linear system into a second linear system
such that

Section 6.4
LINEAR SYSTEMS
423
• the solutions of the two systems are identical;
• the solution of the second system is easily obtained by back substitution.
6. Ax = b can be solved by Gaussian elimination and back substitution (Algorithm 3),
assuming that at each step there is a nonzero pivot on the main diagonal.
Algorithm 3:
Gaussian elimination and back substitution.
input: n × n nonsingular matrix A, n × 1 vector b
output: n × 1 vector x = A−1b
{Gaussian elimination}
for j := 1 to n −1
{ajj is the pivot}
for i := j + 1 to n
{Eliminate xj from equation i}
compute multiplier m := −aij/ajj
add m × row j to row i
{Back substitution}
use Algorithm 2 on the resulting upper triangular matrix to obtain the values
xn, xn−1, . . . , x1
7. Algorithm 3, implemented to take advantage of created 0s, requires 1
3n3 + O(n2)
ﬂops.
8. To solve Ax = b by computing the inverse and then forming the product A−1b requires
n3 + O(n2) ﬂops.
9. Cramer’s rule: This method for solving Ax = b expresses each entry of the solution
x = (x1, x2, . . . , xn)T as the ratio of two determinants:
x1 = det A1
det A , x2 = det A2
det A , . . . , xn = det An
det A ,
where Ai is obtained from A by substituting column vector b for the ith column of A.
(Gabriel Cramer, 1704–1752)
10. Cramer’s rule is of extremely limited use numerically because
• it requires far more ﬂops than Gaussian elimination and back substitution;
• it is numerically unstable.

424
Chapter 6
LINEAR ALGEBRA
Examples:
1. The following system is solved by ﬁrst applying Gaussian elimination:
x1 + x2 + 2x3 + x4 = 1
2x1 + 4x2 + 5x3 + 4x4 = 5
(−2
1 × equation 1)
x1 + 7x2 + 7x3 + 6x4 = 6
(−1
1 × equation 1)
2x1 + 4x2 + 9x3 + 5x4 = 3
(−2
1 × equation 1)
x1 + x2 + 2x3 + x4 = 1
2x2 + x3 + 2x4 = 3
6x2 + 5x3 + 5x4 = 5
(−6
2 × equation 2)
2x2 + 5x3 + 3x4 = 1
(−2
2 × equation 2)
x1 + x2 + 2x3 + x4 = 1
2x2 + x3 + 2x4 = 3
2x3 −x4 = −4
4x3 + x4 = −2
(−4
2 × equation 3)
x1 + x2 + 2x3 + x4 = 1
2x2 + x3 + 2x4 = 3
2x3 −x4 = −4
3x4 = 6
The solution is then obtained by back substitution:
x4 = 6/3 = 2,
x3 = [−4 + 1 · 2]/2 = −1,
x2 = [3 −1 · (−1) −2 · 2]

2 = 0,
x1 = [1 −1 · 0 −2 · (−1) −1 · 2]

1 = 1.
2. Suppose the following system is solved, rounding all results to three signiﬁcant digits:
0.0001x1 + x2 = 1
0.5x1 + 0.5x2 = 1
(−
0.5
0.0001 × equation 1)
0.0001x1 + x2 = 1
−5000x2 = −5000
Using back substitution produces x2 = 1 and x1 = 0. However, the correct solution to
this simple linear system is x1 = 10000
9999 and x2 = 9998
9999, which to three signiﬁcant digits
becomes x1 = 1 and x2 = 1.
Consequently, simply choosing any nonzero pivot can
produce inaccurate results.
6.4.3
LU DECOMPOSITION
Gaussian elimination can be formulated as LU decomposition of the coeﬃcient matrix.

Section 6.4
LINEAR SYSTEMS
425
Deﬁnitions:
An LU decomposition of a square matrix A expresses A = LU, where L = (lij) is unit
lower triangular and U = (uij) is upper triangular.
A permutation matrix is a square matrix with entries 0 or 1, where the entry 1 occurs
precisely once in each row and once in each column.
Facts:
1. A square matrix has an LU decomposition if and only if every principal submatrix
(§6.3.1) is nonsingular.
2. If P is a permutation matrix, then the product PA rearranges the rows of A and the
product AP rearranges the columns of A.
3. The matrix A is nonsingular if and only if there exists a permutation matrix P such
that PA has an LU decomposition. The LU decomposition of PA is unique.
4. It may be necessary to rearrange the rows of A to avoid a zero pivot.
5. Assume A has an LU decomposition, and consider Gaussian elimination applied
to Ax = b with pivots on the main diagonal.
The following statements express LU
decomposition as a reformulation of Gaussian elimination:
• the entry uij (1 ≤i ≤j ≤n) is the coeﬃcient of xj in equation i after Gaussian
elimination has been completed;
• to eliminate xj from equation i, i > j, Gaussian elimination adds −lij× equation
j to equation i.
6. If A has an LU decomposition, then the linear system Ax = b can be solved as follows
(see Algorithm 4):
• compute the decomposition A = LU;
• solve Ly = b; that is, perform forward substitution;
• solve Ux = y; that is, perform back substitution.
7. It is ineﬃcient to solve a nontrivial sequence of linear systems Ax1 = b1, Ax2 = b2, . . .,
Axp = bp by repeating Gaussian elimination for each system. Only one LU decomposition
is needed, followed by p forward substitution steps and p back substitution steps.
8. An LU decomposition of an n × n matrix requires n2 + O(n) storage locations and
1
3n3 + O(n2) ﬂops.
Algorithm 4:
LU decomposition with forward and back substitution.
input: n × n nonsingular matrix A, n × 1 vector b
output: n × 1 vector x = A−1b
{Compute A = LU}
for k := 1 to n −1
ukk := akk
for i := k + 1 to n
lik := aik/akk; uki := aki
for j := k + 1 to n
for i := k + 1 to n
aij := aij −likukj
{Solve Ly = b; that is, perform forward substitution}

426
Chapter 6
LINEAR ALGEBRA
for i := 1 to n
yi := bi −
i−1
X
j=1
lijyj
{Solve Ux = y; that is, perform back substitution}
for i := n down to 1
xi := 1
uii

yi −
n
X
j=i+1
uijxj

Examples:
1. The following matrix A has no LU decomposition because a11 = 0 (see Fact 1):
A =
 
0
1
2
3
!
.
However, rearranging the rows of A (Fact 4) produces
PA =
 
0
1
1
0
!  
0
1
2
3
!
=
 
2
3
0
1
!
=
 
1
0
0
1
!  
2
3
0
1
!
= LU.
2. The unique LU decomposition of the matrix A in Example 1, §6.4.2 is





1
1
2
1
2
4
5
4
1
7
7
6
2
4
9
5




=





1
0
0
0
2
1
0
0
1
3
1
0
2
1
2
1










1
1
2
1
0
2
1
2
0
0
2
−1
0
0
0
3




.
6.4.4
CHOLESKY DECOMPOSITION
For symmetric positive deﬁnite linear systems, Cholesky decomposition (which exploits
symmetry in the coeﬃcient matrix) is roughly twice as eﬃcient as LU decomposition.
Deﬁnition:
A Cholesky decomposition of A expresses A = LLT, where L is lower triangular and
every entry on the main diagonal of L is positive.
Facts:
1. A matrix has a Cholesky decomposition if and only if it is symmetric and positive
deﬁnite.
2. When A is symmetric and positive deﬁnite, the linear system Ax = b can be solved
as follows:
• compute a Cholesky decomposition A = LLT;
• solve Ly = b; i.e., perform forward substitution;
• solve LTx = y; i.e., perform back substitution.
3. A simple symmetric variant of the standard LU decomposition algorithm is used to
compute Cholesky decomposition [GoVL13, St88].

Section 6.4
LINEAR SYSTEMS
427
4. Cholesky decomposition requires 1
2n2+O(n) storage locations and 1
6n3+O(n2) ﬂops,
in contrast to the n2 + O(n) storage locations and 1
3n3 + O(n2) ﬂops required by LU
decomposition.
Example:
1. The matrix A =



1
−1
3
−1
2
−1
3
−1
14


is clearly symmetric. It is positive deﬁnite since
its principal submatrices (1),
 
1
−1
−1
2
!
, and A have positive determinants. (See §6.3.4,
Fact 22.) Matrix A can be written as A = LLT , where L is the lower triangular matrix



1
0
0
−1
1
0
3
2
1


. To solve the linear system Ax = b with b = (1, 0, 6)T, ﬁrst solve the lower
triangular system Ly = b, yielding y = (1, 1, 1)T. Then solve the upper triangular system
LTx = y, yielding x = (−3, −1, 1)T.
6.4.5
CONDITIONING OF LINEAR SYSTEMS
Errors in the data A and b lead to errors in the solution x. The condition number of A
can be used to bound relative error in the solution in terms of relative errors in the data.
Deﬁnitions:
A (generalized) vector norm on Rn is a real-valued function ∥· ∥satisfying the
following properties for all real scalars a and all vectors x, y ∈Rn:
• ∥x∥≥0 with equality if and only if x = 0;
• ∥ax∥= |a| · ∥x∥, where |a| denotes the absolute value of a;
• ∥x + y∥≤∥x∥+ ∥y∥.
The matrix norm induced by the vector norm ∥· ∥is deﬁned by ∥A∥= max
∥x∥=1 ∥Ax∥.
The condition number of a nonsingular matrix A is the number κ(A) = ∥A∥∥A−1∥.
The larger the condition number of a matrix, the more ill conditioned it is; the smaller
the condition number of a matrix, the more well conditioned it is.
Facts:
1. The deﬁnition of a vector norm given here generalizes that of a vector norm derived
from an inner product space (§6.1.4).
2. The matrix norm induced by a vector norm satisﬁes
• ∥X∥≥0 with equality if and only if X = 0;
• ∥aX∥= |a| · ∥X∥, where |a| denotes the absolute value of a;
• ∥X + Y ∥≤∥X∥+ ∥Y ∥;
• ∥XY ∥≤∥X∥∥Y ∥.
3. κ(A) ≥1.

428
Chapter 6
LINEAR ALGEBRA
4. Consider the linear system Ax = b ̸= 0, where A is nonsingular.
Suppose that
changing from A to A + ∆A and b to b + ∆b changes the solution from x to x + ∆x. If
∥(∆A)∥∥A−1∥< 1, the relative error in x can be bounded in terms of relative errors in
the data:
∥∆x∥
∥x∥
≤

κ(A)
1 −∥(∆A)∥∥A−1∥
∥∆b∥
∥b∥+ ∥∆A∥
∥A∥

.
5. The following are consequences of Fact 4:
• For an ill-conditioned linear system Ax = b, some small errors in A or b can
potentially be ampliﬁed into large errors in x;
• For a well-conditioned linear system Ax = b, where 1 −∥(∆A)∥∥A−1∥is not
approximately zero, all small errors in A or b result in no more than modest
errors in x.
6. Assume A is nonsingular, let Ax = b ̸= 0, and view ˆx as an approximation to the
solution x. Then the residual r = Aˆx −b and the error ˆx −x satisfy
∥ˆx −x∥
∥x∥
≤κ(A)∥r∥
∥b∥.
7. Whenever A is ill conditioned, a small relative residual ∥r∥/∥b∥may not imply a
small relative error ∥ˆx −x∥/∥x∥.
Examples:
1. The standard Euclidean norm (§6.1.4) on Rn deﬁned by ∥x∥2 = (
nP
i=1
x2
i )1/2 is a
(generalized) vector norm.
2. The l1 norm on Rn deﬁned by ∥x∥1 =
nP
i=1
|xi| is a (generalized) vector norm.
3. In coding theory (§14.1), the Hamming distance between two codewords x, y ∈Zn
2
is just ∥x −y∥1.
4. The l∞norm on Rn deﬁned by ∥x∥∞= max
1≤i≤n |xi| is a (generalized) vector norm.
5. The matrix norm induced by ∥x∥1 is given by ∥A∥1 = max
1≤j≤n
nP
i=1
|aij|.
6. The matrix norm induced by ∥x∥∞is given by ∥A∥∞= max
1≤i≤n
nP
j=1
|aij|.
7. The matrix norm induced by ∥x∥2, also called the spectral norm, is given by ∥A∥2 =
max{
√
λ | λ an eigenvalue of AT A }.
8. Consider the linear system Ax = b, where A =
 
1
2
2.001
4
!
and b =
 
2
4
!
. Then
∥A∥∞= 6.001 and ∥A−1∥∞= 3,000, so that κ(A) = 18,003. The solution of Ax = b is
x = (0, 1)T whereas the solution of the slightly perturbed system with ˆb =
 
2
4.001
!
is
x = (1, 0.5)T. Even though the change in the right-hand side is small, the large condition
number allows for radical changes in the solution vector, as seen here.

Section 6.4
LINEAR SYSTEMS
429
6.4.6
PIVOTING FOR STABILITY
Gaussian elimination can be numerically unstable. Numerical stability can be vastly
improved by the addition of pivoting strategies that select large pivots.
Deﬁnitions:
Let a(k)
ij
denote the ij-entry of the current matrix after step k of Gaussian elimination
(or LU decomposition). The growth factor is deﬁned by
max
i,j,k |a(k)
ij |
max
i,j |aij| .
Partial pivoting is a solution strategy which at step k of Gaussian elimination exchanges
row k with the row i ≥k having the entry of largest magnitude in column k.
Complete pivoting is a solution strategy which at step k of Gaussian elimination
exchanges row k and column k with, respectively, the row i ≥k and the column j ≥k
containing the entry of largest magnitude.
Facts:
1. For general coeﬃcient matrices, Gaussian elimination (that is, LU decomposition)
without pivoting is numerically unstable.
2. To improve the numerical stability of Gaussian elimination, it suﬃces to introduce a
pivoting strategy that keeps the growth factor small.
3. For Gaussian elimination with complete pivoting the growth factor is bounded above
by
n1/2 2131/241/3 . . . n1/(n−1)1/2,
which is a relatively slow-growing function of n; hence, Gaussian elimination with com-
plete pivoting is numerically stable.
4. For Gaussian elimination with partial pivoting, the growth factor is bounded above
by 2n−1, and moreover there are contrived examples for which the growth factor is 2n−1.
Hence, Gaussian elimination with partial pivoting can be numerically unstable.
5. In practice, partial pivoting is preferred over complete pivoting for the following two
reasons:
• Despite contrived examples having an exponential growth factor, partial pivoting
limits the growth factor in practice almost as well as complete pivoting;
• Partial pivoting is signiﬁcantly more eﬃcient than complete pivoting; it compares
1
2n2 + O(n) pairs of potential pivots, while complete pivoting compares 1
3n3 +
O(n2) pairs.
Example:
1. LU decomposition applied to the following matrix shows that partial pivoting can
produce a growth factor of 2n−1 (see Fact 4).
Observe that maxi,j |aij| = 1 and
maxi,j,k |a(k)
ij | = unn = 2n−1; hence the growth factor is 2n−1.

430
Chapter 6
LINEAR ALGEBRA













1
0
0
· · ·
0
1
−1
1
0
· · ·
0
1
−1
−1
1
· · ·
0
1
...
...
...
...
...
−1
−1
−1
· · ·
0
1
−1
−1
−1
· · ·
1
1
−1
−1
−1
· · ·
−1
1













6.4.7
PIVOTING TO PRESERVE SPARSITY
Many, if not most, linear systems that arise in practice have relatively few nonzero entries
in the coeﬃcient matrix. Some pivoting strategies aim to preserve many zero entries in
the triangular factors; the LU decomposition algorithm can then save time and space by
omitting zero entries from the computation.
Deﬁnitions:
A matrix is sparse if it has relatively few nonzero entries. The number of nonzero entries
of matrix A is denoted |A|. The ith row of A is denoted A(i, :) and the jth column of A
is denoted A(:, j). (See §6.3.1.)
Fill refers to nonzero entries in the triangular factors whose corresponding positions in
the coeﬃcient matrix are occupied by zeros.
The upper bandwidth and lower bandwidth of a matrix A are given respectively by
ub(A) = max{ (j −i) | aij ̸= 0, i < j }, lb(A) = max{ (i −j) | aij ̸= 0, i > j }.
A banded LU decomposition algorithm stores and computes all entries of L and U
within the band deﬁned by lb(A) and ub(A).
A general sparse LU decomposition algorithm stores and computes only the nonzero
entries in the triangular factors, irrespective of the banded structure.
The Markowitz pivoting strategy for Gaussian elimination chooses at step k from
among all available pivots one that minimizes the product
 |L(:, k)| −1
 |U(k, :)| −1

.
The minimum degree algorithm is a restricted version of the Markowitz pivoting
strategy; it assumes (and preserves) symmetry in the coeﬃcient matrix. At step k of
Gaussian elimination, this algorithm chooses from among the entries on the main diagonal
a pivot that minimizes |L(:, k)|.
Note:
The realistic “no-cancellation” assumption will be made throughout. Namely,
once an entry becomes nonzero during a triangular decomposition, it will be nonzero
upon termination.
Facts:
1. The amount of ﬁll in triangular factors often varies greatly with the choice of pivots.
2. Under the no-cancellation assumption, bandwidth reduction and ﬁll reduction become
combinatorial optimization problems.
3. The following problems are provably intractable (i.e., NP-hard; see §17.5):
• for a symmetric matrix A, ﬁnd a permutation matrix P that minimizes the band-
width lb(PAP T );

Section 6.4
LINEAR SYSTEMS
431
• for a nonsingular matrix A, ﬁnd permutation matrices P and Q such that the LU
decomposition PAQ = LU exists and |L| + |U| is minimum;
• for a symmetric positive deﬁnite matrix A, ﬁnd a permutation matrix P that
minimizes |L|, where L is the Cholesky factor of PAP T .
4. In view of Fact 3, various heuristics are used to reduce bandwidth or to reduce ﬁll.
5. Assume that A has an LU decomposition. Then lb(L) = lb(A) and ub(U) = ub(A).
6. The chief advantage of a banded LU decomposition algorithm over a general sparse
LU decomposition algorithm is its simplicity. The same advantage holds for proﬁle and
skyline methods, both of which are generalizations of the banded approach [GeLi81].
7. For most problems encountered in practice, a banded LU decomposition algorithm,
even if A has been permuted so that lb(A) and ub(A) are minimum, requires much more
space and work than a general sparse LU decomposition algorithm coupled with the
Markowitz pivoting strategy. The same comment applies to proﬁle and skyline methods.
8. Let A be a symmetric positive deﬁnite matrix, and let P be a permutation matrix
with the same number of rows and columns.
• the Cholesky decomposition of PAP T exists and is numerically stable;
• the undirected graph (§8.1) G of the Cholesky factor of PAP T is a chordal graph
and P deﬁnes a perfect elimination ordering of G [GeLi81].
9. General sparse Cholesky decomposition can be handled in a clean, modular fashion:
• using only the positions of nonzeros in A as input, compute a permutation P to
reduce ﬁll in the Cholesky factor of PAP T (using, for example, the minimum
degree algorithm);
• construct data structures to contain the nonzeros of the Cholesky factor;
• after putting the nonzero entries of PAP T into the data structures, compute the
Cholesky factor of PAP T in the provided data structures;
• perform forward and back substitutions to solve the linear system.
10. For symmetric positive deﬁnite matrices arising from two-dimensional and three-
dimensional partial diﬀerential equations, the nested dissection algorithm often com-
putes a more eﬀective ﬁll-reducing permutation than does the minimum degree algorithm
[GeLi81].
11. The interplay between pivoting for stability and pivoting for sparsity complicates
general sparse LU factorization. The best approach is not yet certain.
12. A number of robust and well-tested software packages for solving linear systems are
available at
• http://www.mathworks.com
• http://www.nag.com
• http://www.netlib.org
• http://www.roguewave.com
Examples:
1. For any “arrowhead” matrix there is a pivot sequence that completely ﬁlls the matrix
and another that creates no ﬁll, making it the canonical example used to illustrate Fact 1.
The following is a 4 × 4 arrowhead matrix that ﬁlls in completely. (Here ⋆occupies a

432
Chapter 6
LINEAR ALGEBRA
position that is nonzero in A, • is a ﬁll entry in L or U, and a space is a zero.)
A =





⋆
⋆
⋆
⋆
⋆
⋆
⋆
⋆
⋆
⋆




,
A = LU =





⋆
⋆
⋆
⋆
•
⋆
⋆
•
•
⋆










⋆
⋆
⋆
⋆
⋆
•
•
⋆
•
⋆




.
Reversing the pivot sequence, however, results in no ﬁll:
bA = PAP T =





1
1
1
1










⋆
⋆
⋆
⋆
⋆
⋆
⋆
⋆
⋆
⋆










1
1
1
1




=





⋆
⋆
⋆
⋆
⋆
⋆
⋆
⋆
⋆
⋆




,
bA = bL bU =





⋆
⋆
⋆
⋆
⋆
⋆
⋆










⋆
⋆
⋆
⋆
⋆
⋆
⋆




.
2. The following table illustrates how Fact 7 typically manifests itself in practice. The
four problems arise in ﬁnite element modeling of actual structures. The table records
data for two distinct methods:
• a proﬁle-reducing permutation from the reverse Cuthill-McKee algorithm [GeLi81]
in tandem with a proﬁle factorization algorithm;
• a ﬁll-reducing permutation from the minimum degree algorithm [GeLi81] in tan-
dem with a general sparse factorization algorithm.
Recorded for each method are the number of nonzero entries in the Cholesky factor
(expressed in millions) and the number of ﬂops needed to compute the factor (expressed
in millions).
|L| (×10−6)
No. ﬂops (×10−6)
problem
n
|A|
proﬁle
general
proﬁle
general
reduction
sparse
reduction
sparse
coliseum
1,806
63,454
0.190
0.112
11.803
4.952
winter sports
3,562
159,910
0.538
0.279
44.245
16.352
arena
nuclear power
11,948
149,090
5.908
0.663
2,135.2
70.779
station
76 story
15,439
252,241
2.637
1.417
232.79
142.57
skyscraper

Section 6.5
EIGENANALYSIS
433
6.5
EIGENANALYSIS
Identifying the eigenvalues and eigenvectors of a matrix facilitates the study of com-
plicated systems and the analysis of their behavior over time.
A basis consisting of
eigenvectors yields a particularly simple representation of a linear transformation (§6.2).
Eigenvalues can also provide useful information about discrete structures (§8.10.2).
6.5.1
EIGENVALUES AND CHARACTERISTIC POLYNOMIALS
Deﬁnitions:
A complex number λ is an eigenvalue of the n × n complex matrix A if there exists a
nonzero vector x ∈Cn (an eigenvector of A corresponding to λ) such that Ax = λx.
The characteristic polynomial of the square matrix A is the polynomial pA(λ) =
det(λI −A).
The characteristic equation of A is the equation pA(λ) = 0.
A nilpotent matrix is a square matrix A such that Ak = 0 for some positive integer k.
An idempotent matrix is a square matrix A such that A2 = A.
Let Sk(A) denote the sum of all order k principal minors of the matrix A.
Facts:
1. The characteristic polynomial pA(λ) of an n × n matrix A is a monic polynomial of
degree n in λ.
2. The coeﬃcient of λn−1 in pA(λ) is −tr A.
3. The constant term in pA(λ) is (−1)n det A.
4. pA(λ) = Pn
k=0(−1)kSk(A)λn−k.
5. Similar matrices (§6.2.4) have the same characteristic polynomial.
6. The roots of the characteristic equation are the eigenvalues of A.
7. Cayley-Hamilton theorem: If pA(·) is the characteristic polynomial of A then pA(A)
is the zero matrix.
8. An n × n matrix has n (not necessarily distinct) eigenvalues.
9. The matrix A is singular if and only if 0 is an eigenvalue of A.
10. The characteristic equation of A =
 
a
b
c
d
!
is pA(λ) = λ2 −(a + d)λ + (ad −bc).
11. The eigenvalues of A =
 
a
b
c
d
!
are given by (a + d) ±
p
(a −d)2 + 4bc
2
.
12. If the n × n matrix A has eigenvalues λ1, λ2, . . . , λn then
• Pn
i=1λi = tr A;
• Qn
i=1λi = det A;
• the kth elementary symmetric function P
i1<···<ik λi1 . . . λik equals Sk(A).

434
Chapter 6
LINEAR ALGEBRA
13. The eigenvalues are continuous functions of the entries of a matrix. More precisely,
given an n×n matrix A with eigenvalues λ1, λ2, . . . , λn and ǫ > 0, there exists δ > 0 such
that for any n × n matrix B, with eigenvalues µ1, µ2, . . . , µn and satisfying maxi,j |aij −
bij| < δ, there exists a permutation τ of 1, 2, . . ., n such that |λi−µτ(i)| < ǫ, i = 1, 2, . . . , n.
14. The following table gives the eigenvalues of certain specialized matrices A, whose
eigenvalues are λ1, λ2, . . . , λn. In this table k is any positive integer.
matrix
eigenvalues
diagonal matrix
diagonal elements
upper (or lower) triangular matrix
diagonal elements
AT
eigenvalues of A
A∗
complex conjugates of the
eigenvalues of A
Ak
λk
1, . . . , λk
n
A−k, A nonsingular
λ−k
1 , . . . , λ−k
n
q(A), where q(·) is a polynomial
q(λ1), . . . , q(λn)
SAS−1, S nonsingular
eigenvalues of A
AB, where A is m × n, B is n × m,
eigenvalues of BA; and 0
m ≥n
(m −n times)
(a −b)In + bJn, where Jn is the n × n
a + (n −1)b; and a −b
matrix of all 1s
(n −1 times)
A n × n nilpotent
0 (n times)
A n × n idempotent of rank r
1 (r times); and 0 (n −r times)
Examples:
1. The characteristic polynomial for A =
 
1
4
2
3
!
is pA(λ) =

λ −1
−4
−2
λ −3
 = λ2 −
4λ−5 = (λ+1)(λ−5), so the eigenvalues are λ = −1 and λ = 5. The vector x = (2, −1)T
is an eigenvector for λ = −1 since Ax = (−2, 1)T = −x. The vector x = (1, 1)T is an
eigenvector for λ = 5 since Ax = (5, 5)T = 5x.
2. For the matrix in Example 1,
pA(A) = A2 −4A −5I =
 
9
16
8
17
!
−
 
4
16
8
12
!
−
 
5
0
0
5
!
=
 
0
0
0
0
!
,
as required by Fact 7.
3. The characteristic polynomial of the matrix A =



3
0
2
4
1
4
2
0
3


can be calculated by
using Facts 1–4. Since tr A = 7, det A = 5, and S2(A) =

1
4
0
3
 +

3
2
2
3
 +

3
0
4
1
 = 11,
it follows that pA(λ) = λ3 −7λ2 + 11λ −5.
Thus pA(5) = 0, showing that λ = 5
is an eigenvalue of A. An eigenvector corresponding to λ = 5 is x = (1, 2, 1)T since
Ax = (5, 10, 5)T = 5x.

Section 6.5
EIGENANALYSIS
435
4. The matrix A in Example 3 is nonsingular since pA(0) ̸= 0, so 0 is not an eigenvalue
of A (Fact 9). The inverse of A can be calculated using the Cayley-Hamilton theorem:
A3 −7A2 + 11A −5I = 0, so 5I = A3 −7A2 + 11A = A(A2 −7A + 11I) and I =
A[ 1
5(A2 −7A + 11I)]. Consequently, A−1 = 1
5(A2 −7A + 11I) = 1
5



3
0
−2
−4
5
−4
−2
0
3


.
6.5.2
EIGENVECTORS AND DIAGONALIZATION
Deﬁnitions:
Let λ be an eigenvalue of the n × n (complex) matrix A. The algebraic multiplicity
of λ is its multiplicity as a root of the characteristic polynomial.
The eigenspace of A corresponding to λ is the vector space { x ∈Cn | Ax = λx }.
The geometric multiplicity of λ is the dimension of the eigenspace of A corresponding
to λ.
The square matrix A is diagonalizable if there exists a nonsingular matrix P such that
P −1AP is a diagonal matrix.
The minimal polynomial of the square matrix A is the monic polynomial q(·) of
minimum degree such that q(A) = 0.
The square matrix A is normal if AA∗= A∗A.
The singular values of an n × n matrix A are the (positive) square roots of the eigen-
values of AA∗, written σ1(A) ≤σ2(A) ≤· · · ≤σn(A).
A row stochastic matrix is a matrix with all entries nonnegative and row sums 1.
Facts:
1. The eigenspace corresponding to λ is a subspace of the vector space Cn. Speciﬁcally,
it is the null space (§6.1.2) of the matrix A −λI.
2. Eigenvectors corresponding to distinct eigenvalues are linearly independent.
3. If λ, µ are distinct eigenvalues of A and if Ax = λx and A∗y = µy, then x, y are
orthogonal.
4. The algebraic multiplicity is never less than the geometric multiplicity, but sometimes
it is greater. (See Example 3.)
5. The minimal polynomial is unique.
6. If A can be diagonalized to a diagonal matrix D, then the eigenvalues of A appear
along the diagonal of D.
7. The following conditions are equivalent for an n × n matrix A:
• A is diagonalizable;
• A has n linearly independent eigenvectors;
• the minimal polynomial of A has distinct linear factors;
• the algebraic multiplicity of each eigenvalue of A equals its geometric multiplicity.
8. If the n × n matrix A has n distinct eigenvalues then A is diagonalizable.

436
Chapter 6
LINEAR ALGEBRA
9. If the n × n matrix A has n linearly independent eigenvectors v1, v2, . . . , vn then A
is diagonalizable using the matrix P whose columns are the vectors v1, v2, . . . , vn.
10. Hermitian, skew-Hermitian, and unitary matrices are normal matrices.
11. Spectral theorem for normal matrices:
If A is an n × n normal matrix, then it
can be diagonalized by a unitary matrix. That is, there exists an n × n unitary matrix
U such that U ∗AU = diag(λ1, λ2, . . . , λn), the diagonal matrix with the eigenvalues
λ1, λ2, . . . , λn of A along its diagonal.
12. If A is normal, then it has a spectral decomposition A = Pn
i=1 λiuiu∗
i , where
{u1, u2, . . . , un} is an orthonormal basis for Cn.
13. Diagonalization results for special types of normal matrices are given in the following
table.
matrix A
eigenvalues
diagonalization result
Hermitian
real
Fact 11
real symmetric
real
there exists a real orthogonal P such
that P T AP is diagonal
skew-Hermitian
purely imaginary
Fact 11
real skew-symmetric
purely imaginary
there exists a real orthogonal Q such
that QT AQ is a direct sum of mat-
rices, each of which is a 2 × 2 real
skew-symmetric or null matrix
unitary
all with modulus 1
Fact 11
14. If A, B are normal and commute, they can be simultaneously diagonalized. Namely,
there exists a unitary U such that U ∗AU and U ∗BU are both diagonal.
15. For any square matrix A, the rank of A is never less than the number of nonzero
eigenvalues (counting multiplicities) of A.
16. If A is normal then its rank equals the number of nonzero eigenvalues.
17. Schur’s triangularization theorem: Suppose A is a square matrix. Then there exists
a unitary U such that U ∗AU is upper triangular with the eigenvalues of A on its diagonal.
18. If A, B are square matrices that commute then there exists a unitary U such that
U ∗AU and U ∗BU are both upper triangular.
19. Jordan canonical form:
Let A be an n × n matrix with the distinct eigenvalues
λ1, λ2, . . . , λk having (algebraic) multiplicities r1, r2, . . . , rk respectively. Then there ex-
ists a nonsingular matrix P such that P −1AP = diag(Λ1, Λ2, . . . , Λk), where
Λi =








λi
∗
0
· · ·
0
0
0
λi
∗
· · ·
0
0
...
...
...
...
0
0
0
· · ·
λi
∗
0
0
0
· · ·
0
λi








is an ri ×ri matrix and each ∗is either 0 or 1. Furthermore, the number of 1s is ri minus
the geometric multiplicity of λi.
20. The rank of a square matrix equals the number of nonzero singular values.

Section 6.5
EIGENANALYSIS
437
21. If A is a square matrix and if U and V are unitary, then A and UAV have the same
singular values.
22. Singular value decomposition: If A is an n×n matrix then there exist n×n unitary
matrices U, V such that UAV is diagonal with σ1(A), σ2(A), . . . , σn(A) on the diagonal.
23. QR factorization: If A is an n × n matrix then there exists a unitary matrix Q and
an upper triangular matrix R such that A = QR.
24. The QR factorization of a matrix can be calculated using Gram-Schmidt orthogo-
nalization (§6.1.4).
Examples:
1. Let x, y be vectors of size n×1 and let A = xyT . Then the eigenvalues of A are given
by yT x and 0, the latter with multiplicity n −1.
2. The matrix given in Example 3, §6.5.1 has the characteristic polynomial pA(λ) =
λ3−7λ2+11λ−5 = (λ−1)2(λ−5). The eigenvalues are λ = 1 with algebraic multiplicity 2
and λ = 5 with algebraic multiplicity 1.
For λ = 1 the eigenspace is the null space of A −λI =



2
0
2
4
0
4
2
0
2


. It consists of all
vectors of the form (a, b, −a)T and so is spanned by the linearly independent eigenvectors
(1, 0, −1)T and (0, 1, 0)T . Thus the geometric multiplicity of λ = 1 is 2, the same as its
algebraic multiplicity 1.
As seen in Example 3 of §6.5.1, the eigenvalue λ = 5 has the eigenvector (1, 2, 1)T, linearly
independent of the previous two eigenvectors (Fact 2).
If P =



1
0
1
0
1
2
−1
0
1


is the
matrix containing these eigenvectors then P −1AP = diag(1, 1, 5), thereby diagonalizing
A (Fact 9).
3. By using Maple, the characteristic polynomial of the matrix
A =







7
2
4
0
3
0
6
0
0
0
0
−2
4
0
0
3
2
4
4
3
3
0
2
0
7







is found to be λ5 −28λ4 + 300λ3 −1552λ2 + 3904λ −3840 = (λ −4)3(λ −6)(λ −10), so
the eigenvalue λ = 4 of A has algebraic multiplicity 3. The eigenspace for λ = 4 is the
null space of
A −λI =







3
2
4
0
3
0
2
0
0
0
0
−2
0
0
0
3
2
4
0
3
3
0
2
0
3







,
which is spanned by (1, 0, 0, −1)T and (0, 0, 0, 1, 0)T.
So λ = 4 has geometric mul-
tiplicity 2.
By Fact 7, A is not diagonalizable.
The minimal polynomial of A is
(λ −4)2(λ −6)(λ −10), which has the repeated linear factor λ −4.

438
Chapter 6
LINEAR ALGEBRA
4. The conclusion of Fact 16 need not hold if A is not normal. For example, the matrix
 
0
1
0
0
!
has rank 1 but has no nonzero eigenvalues.
5. Matrix powers:
The matrix A =
 
1
2
1
2
3
4
1
4
!
is a row stochastic matrix and the pow-
ers An of such matrices are important in the analysis of Markov chains (§7.7).
The
eigenvalues of A are λ = 1 and λ = −1
4, with corresponding eigenvectors (1, 1)T and
(2, −3)T.
Thus P −1AP = D = diag(1, −1
4), where P =
 
1
2
1
−3
!
.
Consequently
A = PDP −1, A2 = PDP −1PDP −1 = PD2P −1, and in general An = PDnP −1.
Since Dn = diag(1n, (−1
4)n) = diag(1, αn), the nth power of A can be computed as
An = 1
5
 
3 + 2αn
2 −2αn
3 −3αn
2 + 3αn
!
. Since |α| < 1, An →
 
3
5
2
5
3
5
2
5
!
as n →∞.
6.5.3
LOCALIZATION
Since analytic computation of eigenvalues can be complicated, there are several simple
methods available for (geometrically) estimating the eigenvalues of a matrix.
These
methods can be informative in cases when only the approximate location of eigenvalues
is needed.
Deﬁnitions:
The spectral radius of A, ρ(A), is the maximum modulus of an eigenvalue of A.
Let A be an n × n matrix and let αi = P
j̸=i
|aij|, i = 1, 2, . . ., n.
The Gerˇsgorin discs associated with A are the discs
{ z ∈C
 |z −aii| ≤αi },
i = 1, 2, . . ., n.
The ovals of Cassini associated with A are the ellipses
{ z ∈C
 |z −aii||z −ajj| ≤αiαj },
i ̸= j.
A strictly diagonally dominant matrix is a square matrix A satisfying |aii| > αi for
i = 1, 2, . . ., n.
Facts:
1. ρ(A) is the radius of the smallest disc, centered at the origin of the complex plane,
enclosing all of the eigenvalues of A.
2. ρ(A) ≤min {max
i
P
j |aij|, max
j
P
i |aij|}.
3. The spectral radius of a row stochastic matrix is 1.
4. All the eigenvalues of A are contained in the union of the associated Gerˇsgorin discs.
5. A connected region formed by precisely k ≤n Gerˇsgorin discs contains exactly k
eigenvalues of A.
6. All the eigenvalues of A are contained in the union of the n(n−1)
2
ovals of Cassini
associated with A.

Section 6.5
EIGENANALYSIS
439
Examples:
1. By Fact 2, the spectral radius of the symmetric matrix
A =





8
−2
1
1
−2
−8
−2
−1
1
−2
7
1
1
−1
1
8





is bounded by the maximum absolute row (column) sum 13. Since the eigenvalues of
a real symmetric matrix are real, the spectral radius bound gives the interval [−13, 13]
enclosing all eigenvalues. The Gerˇsgorin discs are the intervals
8 ± 4 = [4, 12], −8 ± 5 = [−13, −3], 7 ± 4 = [3, 11], 8 ± 3 = [5, 11].
The second interval is disjoint from the others, so one eigenvalue is localized in the interval
[−13, −3] while the other three are in the interval [3, 12]. The actual eigenvalues of A
are (approximately) −8.51, 6.31, 7.03, 10.2, consistent with the above intervals. Also, 0
is not in any of the four Gerˇsgorin discs so 0 is not an eigenvalue and A is nonsingular.
Since the eigenvalues of A−1 are the reciprocals of the eigenvalues of A (see §6.5.1), it
follows that the eigenvalues of the symmetric matrix A−1 are localized to the intervals
[−1
3, −1
13] and [ 1
12, 1
3].
2. Using Fact 4, the eigenvalues of the matrix A =



2
1
−1
0
6
2
1
−1
8


are located in the
union of the discs
D1 = { z | |z −2| ≤2 }, D2 = { z | |z −6| ≤2 }, D3 = { z | |z −8| ≤2 }.
Since A and AT have the same eigenvalues, an alternative set of disks can be formed
based on the absolute column sums of A: namely
bD1 = { z | |z −2| ≤1 }, bD2 = { z | |z −6| ≤2 }, bD3 = { z | |z −8| ≤3 }.
Here bD1 is disjoint from both bD2 and bD3, and so one eigenvalue of A is localized to bD1,
and the other two to bD2 ∪bD3. In fact, the eigenvalues of A are 2.24 and 6.88 ± 0.91i,
approximately.
3. The row stochastic matrix A =
 
1
2
1
2
3
4
1
4
!
has Gerˇsgorin discs
D1 = { z | |z −1
2| ≤1
2 } and D2 = { z | |z −1
4| ≤3
4 }.
Since D1 ⊆D2 all eigenvalues must lie in D2.
As seen in Example 5 of §6.5.2, the
eigenvalues of A are 1 and −1
4.
4. Suppose A is strictly diagonally dominant. Then all Gerˇsgorin discs for A reside
in the positive right-half plane so all the eigenvalues must have positive real part. In
particular, 0 is not an eigenvalue and A must be nonsingular.
5. If the n×n matrix A satisﬁes aiiajj > αiαj for all i ̸= j then A must be nonsingular,
since by Fact 6 zero is not an eigenvalue of A. The matrix of Example 2 satisﬁes this
condition since aiiajj ≥12 > 4 = αiαj for all i ̸= j, and so it must be nonsingular.

440
Chapter 6
LINEAR ALGEBRA
6.5.4
COMPUTATION OF EIGENVALUES
The eigenvalues of a matrix can be obtained, in theory, by forming the characteristic
equation and ﬁnding its roots. Since this is not a practical solution method for problems
of realistic size, a variety of iterative techniques have been developed.
Deﬁnitions:
A dominant eigenvalue of a matrix is an eigenvalue with the maximum modulus.
Let U(θ; i, j) be the n × n matrix obtained by replacing the 2 × 2 principal submatrix of
the identity matrix, corresponding to rows i and j, with the rotation matrix
 
cos θ
sin θ
−sin θ
cos θ
!
.
Facts:
1. Power method:
The power method (Algorithm 1) is a simple technique for ﬁnding
the dominant eigenvalue and an associated eigenvector of a nonsingular matrix A having
a unique dominant eigenvalue.
Algorithm 1:
Power method.
input: n × n nonsingular matrix A
output: approximations xk to an eigenvector of A
{Initialization}
choose any vector x0 ∈Cn with ∥x0∥= 1
{Iterative step}
for k := 1 to . . .
xk :=
Axk−1
∥Axk−1∥
2. In Algorithm 1, the kth estimate xk =
Akx0
∥Akx0∥.
3. The sequence xk converges to an eigenvector of A.
4. The sequence ∥Axk∥approaches the dominant eigenvalue.
5. The power method is best suited for large sparse matrices.
6. The rate of convergence of the power method is dictated by the ratio of the largest
to the second largest (in modulus) eigenvalue of A.
The larger this ratio (the more
separated these two eigenvalues in modulus), the faster the convergence of the method.
7. QR method: This method (Algorithm 2) calculates the eigenvalues of a given n × n
matrix A.
Algorithm 2:
QR method.
input: n × n matrix A
output: n × n matrices Ak
{Initialization}

Section 6.5
EIGENANALYSIS
441
A := Q0R0 (a QR factorization of A)
{Iterative step}
for k := 1 to . . .
Ak := Rk−1Qk−1
obtain a QR factorization Ak = QkRk
8. The QR factorization in Algorithm 2 produces a unitary matrix Qk and an upper
triangular matrix Rk. (See §6.5.2, Fact 23.)
9. Under certain conditions (for example, if the eigenvalues of A have distinct moduli)
the sequence Ak in Algorithm 2 converges to an upper triangular matrix whose diagonal
entries are the eigenvalues of A.
10. If A is real then its QR factors are real and can be calculated using real arithmetic.
In this case, if A has nonreal eigenvalues then under certain conditions, the limiting
matrix is block triangular with 1 × 1 and 2 × 2 diagonal blocks.
11. The QR method is not well suited for large sparse matrices since the factors Q, R
can quickly ﬁll with nonzeros.
12. Often as a preparatory step for the QR method the matrix is ﬁrst reduced to
Hessenberg form (upper triangular form in which there may be one nonzero diagonal
below the main diagonal) by using Householder transformations [Da95].
13. The convergence of the QR method can be very slow if the matrix has two eigen-
values that are close in moduli.
14. More eﬀective versions of the QR method are available which make use of certain
shift strategies [GoVL13].
15. Jacobi method: This method (Algorithm 3) ﬁnds the eigenvalues of a real symmet-
ric n × n matrix A having at least one nonzero oﬀ-diagonal entry.
Algorithm 3:
Jacobi method.
input: n × n real symmetric matrix A
output: n × n matrices Ak
{Initialization}
A1 = (a(1)
ij ) := A
{Iterative step}
for k := 1 to . . .
choose r, s (r < s) with |a(k)
rs | as large as possible
deﬁne θ by cot 2θ = a(k)
rr −a(k)
ss
2a(k)
rs
Ak+1 = (a(k+1)
ij
) := U(θ; r, s)T Ak U(θ; r, s)
16. The sequence Ak in Algorithm 3 converges to a real diagonal matrix with the eigen-
values of A on the diagonal.
17. The orthogonal matrix U(θ; r, s) represents a (clockwise) plane rotation by the an-
gle θ.

442
Chapter 6
LINEAR ALGEBRA
18. The Jacobi method is particularly appropriate when A is nearly diagonal, although
in general the QR method exhibits faster convergence.
19. A variant of the Jacobi method, the serial Jacobi method, uses plane rotation pairs
cyclically—for example, (1, 2), (1, 3), . . . , (1, n), (2, 3), . . ., (2, n), . . . .
20. For further information on numerical computation of eigenvalues, see [GoVL13,
Da95].
21. A number of robust and well-tested software packages for carrying out eigensystem
analysis are available at
• http://www.mathworks.com
• http://www.nag.com
• http://www.netlib.org
• http://www.roguewave.com
Examples:
1. The power method, when applied to the matrix in Example 3 of §6.5.1, produces the
following sequence of vectors xk and scalars ∥Axk∥:
k
0
1
2
3
4
5
xk



1
0
0






0.557
0.743
0.371






0.436
0.805
0.403






0.414
0.814
0.407






0.409
0.816
0.408






0.409
0.816
0.408



∥Axk∥
5.385
5.537
5.107
5.021
5.004
5.001
The scalars ∥Axk∥approach the dominant eigenvalue 5 and the vectors xk approach a
multiple of the eigenvector (1, 2, 1)T.
2. The eigenvalues of the matrix A =
 
1
4
2
3
!
can be approximated using the QR
method.
A = Q0R0 with Q0 =
 
−0.447
−0.894
−0.894
0.447
!
and R0 =
 
−2.236
−4.472
0
−2.236
!
.
Then A1 = R0Q0 =
 
5
0
2
−1
!
. Continuing this process produces
A2 =
 
4.862
2.345
0.345
−0.862
!
, A3 =
 
5.023
−1.927
0.073
−1.023
!
, A4 =
 
4.995
2.014
0.014
−0.995
!
.
The sequence Ak approaches an upper triangular matrix with the eigenvalues 5 and −1
on its diagonal.
3. The eigenvalues of the matrix A =



0
1
1
1
4
−3
1
−3
4


can be approximated using the
Jacobi method. The largest oﬀ-diagonal |ars| of A1 = A occurs for r = 2, s = 3 giving θ =
π
4 = 0.7854. Applying the matrix U(θ; 2, 3) =



1
0
0
0
0.7071
0.7071
0
−0.7071
0.7071


produces A2 =

Section 6.5
EIGENANALYSIS
443
U(θ; 2, 3)TA1 U(θ; 2, 3) =



0
0
1.4142
0
7
0
1.4142
0
1


.
The largest magnitude oﬀ-diagonal
entry of A2 is |a13|, giving θ = 0.6155, U(θ; 1, 3) =



0.8165
0
0.5774
0
1
0
−0.5774
0
0.8165


, and A3 =
U(θ; 1, 3)TA2 U(θ; 1, 3) =



−1
0
0
0
7
0
0
0
2


. So the eigenvalues of A are −1, 2, 7.
6.5.5
SPECIAL CLASSES
This section discusses eigenvalues and eigenvectors of specially structured matrices, such
as Hermitian, positive deﬁnite, nonnegative, totally positive, and circulant matrices.
Deﬁnitions:
If x, y ∈Rn, then x majorizes y if Pn
i=1 xi = Pn
i=1 yi and for k = 1, 2, . . . , n −1 the
sum of the k largest components of x is at least as large as the sum of the k largest
components of y. A similar deﬁnition holds for inﬁnite sequences with ﬁnitely many
nonzero terms.
A Hermitian n × n matrix A is positive deﬁnite if x∗Ax > 0 for all nonzero x ∈Cn. It
is positive semideﬁnite if x∗Ax ≥0 for all x ∈Cn.
If A and B are n × n Hermitian matrices then A dominates B in L¨owner order if
A −B is positive semideﬁnite, written A ⪰B.
A matrix is nonnegative (positive) if each of its entries is nonnegative (positive).
The n × n matrix A is reducible if either it is the 1 × 1 zero matrix or there exists a
permutation matrix P such that PAP T is of the form
 
B
0
C
D
!
, where B and D are
square. A matrix is irreducible if it is not reducible.
A strictly totally positive matrix has all of its minors positive.
A circulant matrix has the form








a0
a1
a2
· · ·
an
an
a0
a1
· · ·
an−1
an−1
an
a0
· · ·
an−2
...
...
...
...
a1
a2
a3
· · ·
a0








.
Notation:
Let λ1(A) ≤λ2(A) ≤· · · ≤λn(A) be the eigenvalues of an n × n Hermitian
matrix A.
Facts:
1. Cauchy’s interlacing theorem:
Let A be an n × n Hermitian matrix and let B be a
principal submatrix of A of order n−1. Then
λi(A) ≤λi(B) ≤λi+1(A),
i = 1, 2, . . . , n−1.

444
Chapter 6
LINEAR ALGEBRA
2. Weyl’s theorem:
Let A, B be n × n Hermitian matrices and let j, k be integers
satisfying 1 ≤j, k ≤n.
• If j + k ≥n + 1, then λj+k−n(A + B) ≤λj(A) + λk(B);
• If j + k ≤n + 1, then λj(A) + λk(B) ≤λj+k−1(A + B).
3. Interpretations of the kth smallest eigenvalue of a Hermitian matrix are given in the
following table.
eigenvalue
variational characterization
λ1(A)
min(x∗Ax), minimum over all unit vectors x
λn(A)
max(x∗Ax), maximum over all unit vectors x
λk(A),
min(x∗Ax), minimum over all unit vectors x orthogonal
k = 2, . . . , n
to the eigenspaces of λ1, . . . , λk−1
λn−k(A),
max(x∗Ax), maximum over all unit vectors x orthogonal
k = 1, . . . , n −1
to the eigenspaces of λn−k+1, . . . , λn
4. Schur’s majorization theorem: If A is an n×n Hermitian matrix, then (λ1(A), λ2(A),
. . . , λn(A)) majorizes (a11, a22, . . . , ann). Speciﬁcally, if a11 ≥a22 ≥· · · ≥ann then
kP
i=1
λn−i+1(A) ≥
kP
i=1
aii,
k = 1, 2, . . . , n.
5. Hoﬀman-Wielandt theorem: If A, B are n × n Hermitian matrices, then
nP
i=1
(λi(A + B) −λi(A))2 ≤
nP
i,j=1
|bij|2.
6. Sylvester’s law of inertia: If A is an n×n Hermitian matrix and if X is a nonsingular
n × n matrix, then A and XT AX have the same number of positive eigenvalues as well
as the same number of negative eigenvalues.
7. A Hermitian matrix is positive deﬁnite (positive semideﬁnite) if and only if all its
eigenvalues are positive (nonnegative).
8. If A, B are n × n positive semideﬁnite matrices and A ⪰B, then λi(A) ≥λi(B),
i = 1, 2, . . ., n.
9. If A, B are n × n positive semideﬁnite matrices, then λi+j−n(AB) ≤λi(A)λj(B)
holds for 1 ≤i, j ≤n and i + j ≥n + 1.
10. If A, B are n × n positive semideﬁnite matrices, then
kQ
i=1
λi(AB) ≥
kQ
i=1
λi(A)λi(B),
k = 1, 2, . . . , n.
11. Kantorovich inequality: If A is an n × n positive deﬁnite matrix and if x ∈Cn is a
unit vector, then
(x∗Ax)(x∗A−1x) ≤(λ1(A) + λn(A))2
4λ1(A)λn(A)
.
12. Perron-Frobenius theorem:
If A is an irreducible nonnegative square matrix, then
the spectral radius of A (the Perron root of A) is an eigenvalue of A with algebraic
multiplicity 1 and it has an associated positive eigenvector. If A is positive then the
spectral radius exceeds the modulus of any other eigenvalue.

Section 6.6
COMBINATORIAL MATRIX THEORY
445
13. If A is a nonnegative square matrix, then the spectral radius of A is an eigenvalue
of A and it has an associated nonnegative eigenvector.
14. Let A be an n × n strictly totally positive matrix. Then the eigenvalues of A are
distinct and positive: λ1(A) < λ2(A) < · · · < λn(A). The real eigenvector corresponding
to λn−k has exactly k variations in sign.
15. If A is an n × n strictly totally positive matrix, then (λ1(A), λ2(A), . . . , λn(A))
majorizes (a11, a22, . . . , ann).
16. An (n + 1) × (n + 1) circulant matrix has eigenvalues λj = a0 + a1ωj + a2ω2j +
· · · + anωnj, j = 0, 1, . . ., n with (1, ωj, ω2j, . . . , ωnj), j = 0, 1, . . ., n the corresponding
eigenvectors, where ω = e
2πi
n+1 .
Examples:
1. The matrix A =



0
1
1
1
4
−3
1
−3
4


has eigenvalues −1, 2, and 7 (§6.5.4, Example 3).
The principal submatrix A[1, 2] =
 
0
1
1
4
!
has eigenvalues 2 ±
√
5, which are approxi-
mately equal to −0.2361 and 4.2361. As required by Fact 1, these latter two eigenvalues
interlace those of A: −1 ≤−0.2361 ≤2 ≤4.2361 ≤7. Similarly, the principal submatrix
A[2, 3] =
 
4
−3
−3
4
!
has eigenvalues 1 and 7, which interlace those of A.
2. The matrix in Example 1 has the eigenvalue sequence (−1, 2, 7).
This sequence
majorizes (see Fact 4) the sequence (0, 4, 4) of diagonal elements: 7 ≥4, 7 + 2 ≥4 + 4,
and 7 + 2 −1 ≥4 + 4 + 0.
3. The irreducible matrix A in Example 3 of §6.5.3 is positive with eigenvalues 1 and −1
4.
Thus ρ(A) = 1 and it exceeds the modulus of any other eigenvalue. As required by
Fact 12, there is a positive eigenvector associated with λ = 1, namely (1, 1)T .
4. The matrix A =



2
0
3
1
4
5
2
0
1


is nonnegative with eigenvalues −1 (algebraic multi-
plicity 1) and 4 (algebraic multiplicity 2). So the spectral radius is 4 and (see Fact 13)
λ = 4 must be an eigenvalue. In addition, there is a nonnegative eigenvector associated
with λ = 4, namely (0, 1, 0)T .
6.6
COMBINATORIAL MATRIX THEORY
Matrices and graphs represent two diﬀerent ways of viewing certain discrete structures.
At times a matrix perspective can lend insight into graphical or combinatorial structures.
At other times the graph associated with a matrix can provide useful information about
matrix properties.

446
Chapter 6
LINEAR ALGEBRA
6.6.1
MATRICES OF ZEROS AND ONES
Deﬁnitions:
A (0, 1)-matrix is a matrix with each entry either 0 or 1.
The term rank ρt(A) of a (0, 1)-matrix A is the maximum number of 1s such that no
two are in the same row or column.
An n × n (0, 1)-matrix is partly decomposable if it has a k × (n −k) zero submatrix
for some 1 ≤k ≤n −1; otherwise A is fully indecomposable.
For vectors x = (x1, x2, . . . , xn) and y = (y1, y2, . . . , yn) we say that x is majorized by
y (§6.5.5) if the sum of the k largest components in x is no more than the sum of the k
largest components in y for k ≤n, with equality for k = n. This is denoted by x ⪯y.
A vector x = (x1, x2, . . . , xn) is monotone if x1 ≥x2 ≥· · · ≥xn. If R = (r1, r2, . . . , rm)
is a vector of nonnegative integers with ri ≤n, then its conjugate vector is R∗=
(r∗
1, r∗
2, . . . , r∗
n) where r∗
i is the number of rj with value no less than i.
Thus R∗is
monotone.
For nonnegative integral vectors R = (r1, r2, . . . , rm) and S = (s1, s2, . . . , sn) deﬁne
A(R, S) as the class of (0, 1)-matrices A = (aij) with row sum vector R and column
sum vector S, i.e., Pn
j=1 aij = ri (i ≤m) and Pm
i=1 aij = sj (j ≤n). We may assume
P
i ri = P
j sj. Using permutations of rows and columns one may assume that R and S
are monotone. Then the structure matrix of A(R, S) is the (m + 1) × (n + 1) matrix
T with entries tkl = kl −Pl
j=1 sj + Pm
i=k+1 ri (0 ≤k ≤m, 0 ≤l ≤n).
An n × n (0, 1)-matrix A = (aij) satisfying aii = 0 for i ≤n and aij + aji = 1 for
1 ≤i < j ≤n is called a tournament matrix. Its row sum vector is called the score
vector, and it records the number of wins of each player in a round-robin tournament
where aij = 1 means that Player i won against Player j.
Facts:
1. K¨onig’s theorem:
The term rank ρt(A) of a (0, 1)-matrix A equals the minimum
number of rows and columns required to cover all 1s in the matrix.
2. rank(A) ≤ρt(A).
3. Frobenius-K¨onig theorem:
Let A be an n × n (0, 1)-matrix. Then the term rank of
A is less than n if and only if A has a zero submatrix of size r × s with r + s = n + 1.
4. Let A be an n × n (0, 1)-matrix each of whose row sums and column sums is k.
Then A can be expressed as a sum of k permutation matrices (§6.4.3).
5. Let A be a square (0, 1)-matrix and let B be the matrix obtained from A by replacing
each 0 entry on the main diagonal of A by 1. Then A is irreducible (§6.5.5) if and only
if B is fully indecomposable.
6. Let A, B be n × n fully indecomposable matrices.
Then the matrix obtained by
replacing every nonzero entry in AB by 1 is fully indecomposable.
7. Gale-Ryser theorem: Let R = (r1, r2, . . . , rm) and S = (s1, s2, . . . , sn) be nonnegative
integral vectors with ri ≤n and sj ≤m. Then the class A(R, S) is nonempty (i.e., there
is an m × n (0, 1)-matrix A = (aij) with row sum vector R and column sum vector S) if
and only if S ⪯R∗.
8. The class A(R, S) contains a unique matrix if and only if S = R∗.

Section 6.6
COMBINATORIAL MATRIX THEORY
447
9. Let T = (tkl) be the structure matrix of a nonempty class A(R, S) and let A ∈
A(R, S).
Let A1 be the leading k × l submatrix of A, and let A2 be the submatrix
corresponding to rows k +1, . . ., m and columns l +1, . . ., n. Then tkl equals the number
of zeros in A1 plus the number of ones in A2.
10. Ford-Fulkerson theorem: Suppose R and S are as in Gale-Ryser, with R and S
monotone. Then the class A(R, S) is nonempty if and only if the structure matrix T is
nonnegative.
11. Ryser’s theorem: If A and B are distinct matrices in A(R, S), then B can be
obtained from A by a sequence of interchanges; each such interchange replaces a 2 × 2
identity matrix I2 (a submatrix of A) by the matrix obtained by complementing zeros
and ones, or vice versa.
12. Landau’s theorem: Let R = (r1, r2, . . . , rn) be a monotone nonnegative integral
vector.
Then there exists a tournament matrix with score vector R if and only if
Pk
i=1 ri ≥
 k
2

(1 ≤k ≤n), with equality for k = n.
13. Hardy-Littlewood-P´olya theorem: If x, y ∈Rn, then x ⪯y if and only if there is a
doubly stochastic matrix (§6.6.3) A with x = Ay.
Examples:
1. The following matrix contains a 2 × 4 zero submatrix, occurring in rows 1, 3 and
columns 1, 2, 4, 5. By Fact 3, this means that the matrix must have term rank less than
5. In fact, the matrix has term rank 3. Namely, the starred entries represent a set of 3
entries, no two of which are in the same row or column, and 3 is the largest number of
nonzero entries with this property. Rows 2, 4 and column 3 cover all the 1s in the matrix,
and no smaller number suﬃces, as guaranteed by Fact 1, and ρt(A) = 3.







0
0
1∗
0
0
1∗
1
1
0
1
0
0
1
0
0
1
0
1
1
1∗
0
0
1
0
0







.
2. The matrix
A =





0
1
1
1
1
0
1
1
1
1
0
1
1
1
1
0





has all row and column sums equal to 3. By Fact 4, it can be expressed as the sum of
three permutation matrices. For example
A =





0
0
1
0
0
0
0
1
1
0
0
0
0
1
0
0




+





0
0
0
1
0
0
1
0
0
1
0
0
1
0
0
0




+





0
1
0
0
1
0
0
0
0
0
0
1
0
0
1
0




.
3. Assignment problem:
There are n applicants for n vacant jobs. Each applicant is
qualiﬁed for exactly k ≥1 jobs and for each job there are exactly k qualiﬁed applicants. Is
it possible to assign each applicant to a (distinct) job for which the applicant is qualiﬁed?
To answer this question form the (0, 1)-matrix A where aij = 1 if applicant i is qualiﬁed
for job j, otherwise aij = 0. All row and column sums of A equal k, so (by Fact 4) A

448
Chapter 6
LINEAR ALGEBRA
can be expressed as the sum of k ≥1 permutation matrices. Select any one of these
permutation matrices and use it to deﬁne an assignment of applicants to jobs. Thus it
is possible in this case to ﬁll each job with a diﬀerent qualiﬁed applicant.
4. Let
¯A =







1
1
1
1
1
1
0
0
1
1
1
1
1
0
0
0
1
1
1
1
1
0
0
0
1
1
1
1
0
0
0
0
1
1
1
1
0
0
0
0







,
A =







1
1
1
0
1
0
1
1
1
1
0
1
1
0
1
0
1
1
0
1
0
1
0
1
1
0
1
1
0
1
0
0
0
1
1
0
1
1
0
0







.
Both these matrices have row sum vector R = (6, 5, 5, 4, 4). The column sum vector of ¯A
is S( ¯A) = (5, 5, 5, 5, 3, 1, 0, 0) which is equal to the conjugate R∗of R. The column sum
vector of A is S = (4, 4, 3, 3, 3, 3, 2, 2), so A ∈A(R, S). The majorization S ⪯R∗holds
(Fact 7). ¯A is the unique matrix in the class A(R, R∗).
5. The matrix
A =





0
1
1
1
0
0
1
0
0
0
0
1
0
1
0
0





is a tournament matrix with score vector (3, 1, 1, 1). It may represent a round-robin
tournament where Player 1 wins against all the others while, e.g., Player 2 only defeats
Player 3.
6.6.2
MATRICES AND GRAPHS
Diﬀerent matrices may be used to represent graphs and directed graphs. This makes
it possible to use linear algebra and related ﬁelds, such as optimization, to investigate
graph properties and problems formulated in terms of graphs.
Deﬁnitions:
Let G be a simple graph G with vertices 1, 2, . . . , n and edges e1, e2, . . . , em. The ad-
jacency matrix of G is the square (0, 1)-matrix AG of size n × n where AG(i, j) is 1
if vertices i and j are adjacent, and 0 otherwise. The incidence matrix of G is the
(0, 1)-matrix MI,G of size n × m where MI,G(i, j) is 1 if vertex i is incident with edge ej,
and is 0 otherwise.
The Laplacian matrix LG of G is deﬁned as LG = DG −AG where DG is the diagonal
matrix whose diagonal entry di is the degree of vertex i, and AG is the adjacency matrix
of G.
Let D be a directed graph with vertices 1, 2, . . ., n and arcs e1, e2, . . . , em; we assume no
parallel edges and no loops. The adjacency matrix of D is the square (0, 1)-matrix
AD where AD(i, j) is 1 if there is an arc with tail i and head j, and 0 otherwise. The
incidence matrix of D is the n × m (0, ±1)-matrix MI,D where MI,D(i, j) is 1 (resp.
−1) if vertex i is the head (resp. tail) of arc ej, and is 0 otherwise.
An oriented incidence matrix of a simple graph G is the incidence matrix of the
directed graph D obtained by some orientation of each of its edges.

Section 6.6
COMBINATORIAL MATRIX THEORY
449
A matrix A is called totally unimodular if the determinant of every square submatrix
of A (i.e., a minor) is 0 or ±1.
A polyhedron is a set of the form {x ∈Rn | Ax ≤b} for some real m × n matrix A
and b ∈Rm, i.e., it is the solution set of a linear inequality system.
A polytope P is the convex hull of a ﬁnite set of vectors, say x1, x2, . . . , xk in Rn, so it
consists of all convex combinations Pk
i=1 cixi where ci ≥0 (1 ≤i ≤k), Pk
i=1 ci = 1. An
extreme point of P is a point in P which cannot be written as a convex combination
of other points in P.
The graph G(A) of a symmetric n × n matrix A = (aij) is the graph with vertices
1, 2, . . ., n and an edge {i, j} whenever aij ̸= 0.
Facts:
1. If D is a directed graph, Ak
D = AD · · · AD (k times) is the matrix whose (i, j)th entry
equals the number of directed walks of length k from vertex i to vertex j in the graph G.
2. Two graphs G and G′ on n vertices are isomorphic if and only if there is a permutation
matrix P such that AG = PAG′P T .
3. For a graph G, MI,GM T
I,G = DG + AG.
4. For a directed graph D, MI,DM T
I,D = DG −AG.
5. If G is a graph and MI,D is an oriented incidence matrix of G, then MI,DM T
I,D =
DG −AG = LG.
6. For every graph G the matrix LG is symmetric and positive semideﬁnite, and the
multiplicity of 0 as an eigenvalue of LG equals the number of connected components of
G.
7. If µG is the vector of the eigenvalues of LG (n eigenvalues, if G has n vertices), then
the following majorizations hold: dG ⪯µG ⪯d∗
G, where dG is the degree vector of G and
d∗
G is its conjugate vector (§6.6.1).
8. If a graph G with n vertices is connected then (any) oriented incidence matrix has
rank n −1. More generally, if G has k connected components, then the rank is n −k.
9. Matrix-tree theorem (§8.10.2): For any graph G the absolute value of the determinant
of any (n −1) × (n −1) submatrix of the Laplacian matrix LG equals the number of
spanning trees in G.
10. If D is a directed graph, then its incidence matrix MI,D is totally unimodular.
11. If G is a simple graph, then its incidence matrix MI,G is totally unimodular if and
only if G is bipartite.
12. Every polyhedron is a convex set (so it is closed under taking convex combinations).
13. Farkas-Minkowski-Weyl theorem: A set P is a polytope if and only if P is a bounded
polyhedron.
14. Integrality theorem: If A is totally unimodular, and b, b′, c, c′ are integral vectors
with b ≤b′ and c ≤c′, then the polytope {x | b ≤Ax ≤b′, c ≤c ≤c′} has only integral
extreme points.
15. If G is a bipartite graph, then the matching polytope, deﬁned as the convex hull of
the incidence vectors of matchings (§10.2.1) in G, equals {x | MI,G x ≤1, 0 ≤x ≤1},
where 0 and 1 are the zero vector and the all ones vector, respectively. This follows from
the integrality theorem.
16. The minimum rank of a symmetric matrix A satisfying G(A) = G, where G is a
given matrix, can be computed when G is a tree.

450
Chapter 6
LINEAR ALGEBRA
Examples:
1. Let G be the graph with vertices 1, 2, 3, 4 and edges {1, 2}, {2, 3}, {3, 4}, {1, 4},
{2, 4}. Its adjacency matrix AG, incidence matrix MI,G (with this edge ordering), and
Laplacian matrix LG are
AG =





0
1
0
1
1
0
1
1
0
1
0
1
1
1
1
0




,
MI,G =





1
0
0
1
0
1
1
0
0
1
0
1
1
0
0
0
0
1
1
1




,
LG =





2
−1
0
−1
−1
3
−1
−1
0
−1
2
−1
−1
−1
−1
3




.
The determinant of each 3 × 3 submatrix of LG is ±8, so the number of spanning trees
of G is 8.
2. An oriented incidence matrix for the graph G above is
MI,D =





1
0
0
1
0
−1
1
0
0
1
0
−1
−1
0
0
0
0
1
−1
−1




.
MI,D is totally unimodular while MI,G is not. As G contains an odd cycle, the submatrix
of MI,G consisting of rows 2, 3, 4 and columns 2, 3, 5 has determinant 2 (Fact 11).
3. For the graph G in Example 1, the degree vector is dG = (2, 3, 2, 3), its conjugate is
d∗
G = (4, 4, 2, 0), and the vector of eigenvalues of the Laplacian matrix is µG = (4, 4, 2, 0).
So, in this case µG = d∗
G. This equality always holds for the class of threshold graphs;
such graphs are constructed by starting with the empty graph and successively adding
either isolated vertices or a vertex connected to all previous vertices. The graph G in
Example 1 is a threshold graph.
4. Let D be a directed graph with n vertices and m edges.
Also, let b ∈Rn with
Pn
i=1 bi = 0 and let d ∈Rm be nonnegative. Any vector in the polyhedron PD,b :=
{x | MI,Dx = b, 0 ≤x ≤d} is called a network ﬂow. The variable xe represents the
ﬂow from tail to head of the arc e, and the linear equations assure that the diﬀerence
between the ﬂow into a vertex and the ﬂow out of the same vertex is constant, and equal
to the corresponding entry of b. The vector d represents arc capacities. If b and d are
integral, then the extreme points of PD,b are integral. The linear optimization problem
min{cT x | x ∈PD,b} is known as the minimum cost network ﬂow problem (§10.5.1) and
it has many important applications.
6.6.3
NONNEGATIVE MATRICES
This subsection discusses nonnegative matrices and special classes of nonnegative ma-
trices such as primitive and doubly stochastic matrices. Certain results highlight the
relationship between the Perron root (§6.5.5, Fact 12) and the directed graph of a ma-
trix.
Deﬁnitions:
The directed graph G(A) of an n × n matrix A consists of n vertices 1, 2, . . ., n with
an arc from i to j if and only if aij ̸= 0.

Section 6.6
COMBINATORIAL MATRIX THEORY
451
Vertices i and j of G(A) are equivalent if i = j or if there is a path in G(A) from i to j
and a path from j to i. The corresponding equivalence classes (§1.4.2) of this relation
are the classes of A.
Class Ci has access to class Cj if i = j or if there is a path in G(A) from a vertex in Ci
to a vertex in Cj. A class is ﬁnal if it has access to no other class. Class Ci is basic
if ρ(A[Ci]) = ρ(A), where ρ(·) is the spectral radius (§6.5.3) and A[Ci] is the principal
submatrix of A deﬁned by indices in class Ci.
Let A be an n × n nonnegative irreducible matrix. The number h of eigenvalues of A
of modulus ρ(A) is called the index of cyclicity of A. The matrix A is primitive if
h = 1.
The exponent of A, written exp(A), is the least positive integer m with Am > 0.
A square matrix is doubly stochastic if it is nonnegative and all row and column sums
are 1.
If A is an n × n matrix and σ ∈Sn, the symmetric group on n elements (§5.3.1), then
the set {a1σ(1), a2σ(2), . . . , anσ(n)} is the diagonal of A corresponding to σ.
A diagonal of A is positive if each entry in it is positive.
Matrices A and B of the same size have the same pattern if the following condition
holds: aij = 0 if and only if bij = 0.
The matrix A has a doubly stochastic pattern if there exists a doubly stochastic
matrix B such that A and B have the same pattern.
An n × n matrix is called completely positive if A = BBT for some nonnegative n × k
matrix.
Facts:
1. The matrix A is irreducible if and only if G(A) is strongly connected (§8.3.2).
2. Frobenius normal form:
If the n × n matrix A has k classes, then there exists a
permutation matrix P such that
PAP T =






A11
0
· · ·
0
A21
A22
· · ·
0
...
...
...
Ak1
Ak2
· · ·
Akk






where each Aii, 1 ≤i ≤k, is either irreducible or a 1 × 1 zero matrix.
3. The classes of a nonnegative n × n matrix A are in one-to-one correspondence with
the strong components (§8.3.2) of G(A) and hence can be found in linear time.
4. Let A be an n × n nonnegative matrix. There is a positive eigenvector corresponding
to ρ(A) if and only if the basic classes of A are the same as its ﬁnal classes.
5. Let A be an n × n nonnegative matrix with eigenvalue λ. There exists a nonnegative
eigenvector for λ if and only if there exists a class Ci satisfying both of the following:
• ρ(A[Ci]) = λ;
• if Cj (j ̸= i) is any class that has access to Ci, then ρ(A[Cj]) < ρ(A[Ci]).
6. The n × n nonnegative matrix A is primitive if and only if exp(A) < ∞.
7. A nonnegative irreducible matrix with positive trace is primitive.

452
Chapter 6
LINEAR ALGEBRA
8. Suppose A is an n × n nonnegative irreducible matrix. Let Si be the set of all the
lengths of cycles in G(A) passing through vertex i, and let hi be the greatest common
divisor of all the elements of Si. Then h1 = h2 = · · · = hn and this common value equals
the index of cyclicity of A.
9. Let A be a nonnegative irreducible n × n matrix with p ≥1 nonzero elements on the
main diagonal. Then A is primitive and exp(A) ≤2n −p −1.
10. Let A be a primitive n × n matrix, and let s be the smallest length of a directed
cycle in G(A). Then exp(A) ≤n + s(n −2).
11. Let A be an n × n primitive (0, 1)-matrix, n ≥2. Then exp(A) ≤(n −1)2 + 1.
Equality holds if and only if there exists a permutation matrix P such that
PAP T =








0
1
0
· · ·
0
0
0
1
· · ·
0
...
...
...
...
1
0
0
· · ·
1
1
0
0
· · ·
0








.
12. The set Ωn of n × n doubly stochastic matrices is a compact convex set.
13. Birkhoﬀ-von Neumann theorem:
Every A ∈Ωn can be expressed as a convex
combination of n × n permutation matrices: namely, A = c1P1 + c2P2 + · · · + ctPt for
some permutation matrices P1, P2, . . . , Pt and some positive real numbers c1, c2, . . . , ct
with c1 + c2 + · · · + ct = 1.
14. The following conditions are equivalent for an n × n matrix A:
• A has doubly stochastic pattern;
• There exist permutation matrices P, Q such that PAQ is a direct sum of fully
indecomposable matrices;
• Every nonzero entry of A is contained in a positive diagonal.
15. Let A be an n × n nonnegative idempotent matrix of rank k. Then there exists a
permutation matrix P such that
PAP T =





J
JU
0
0
0
0
0
0
V J
V JU
0
0
0
0
0
0




,
where J is a direct sum of k positive idempotent matrices of rank 1, and where U and
V are arbitrary nonnegative matrices of appropriate sizes.
16. A nonnegative symmetric matrix A of rank k is idempotent if and only if there
exists a permutation matrix P such that
PAP T =
 
J
0
0
0
!
,
where J is a direct sum of k positive symmetric rank one idempotent matrices.
17. Each completely positive n × n matrix is nonnegative, symmetric, and positive
semideﬁnite. For n ≤4 the converse is also true.

Section 6.6
COMBINATORIAL MATRIX THEORY
453
18. The set of n×n completely positive matrices is a closed convex cone and its extreme
rays are generated by the rank one matrices xxT where x is a nonnegative vector in Rn.
19. A graph G is completely positive (meaning that every nonnegative positive semidef-
inite matrix with graph G is completely positive) if and only if G does not have an odd
cycle of length greater than 4.
Examples:
1. The following nonnegative matrix is in Frobenius normal form












5
0
0
0
0
0
0
0
1
1
0
0
0
0
0
2
0
0
0
0
0
2
4
1
0
3
0
0
1
2
0
2
1
0
0
0
1
1
0
0
3
2
0
2
1
0
0
2
3












with four classes C1 = {1}, C2 = {2, 3}, C3 = {4, 5} and C4 = {6, 7}. Class C3 has
access to C1 and C2 while class C4 has access to C2. Classes C1 and C2 are ﬁnal, since
they have access to no other classes. The eigenvalues of A are −2, −1, 1, 2, 3, 5, 5 so
ρ(A) = 5. Classes C1 and C4 are basic since ρ(A[C1]) = ρ(A[C4]) = 5. Since no class
has access to C3, Fact 5 shows there is a nonnegative eigenvector of A for the eigenvalue
ρ(A[C3]) = 3, namely (0, 0, 0, 1, 1, 0, 0)T. However there is no nonnegative eigenvector of
A for ρ(A[C2]) = 2 since class C3 has access to class C2 and ρ(A[C3]) ≥ρ(A[C2]).
2. The directed graph of the matrix
A =







0
1
7
0
0
0
0
0
0
1
0
0
0
1
0
1
0
0
0
0
1
0
0
0
0







is the union of the cycles 1, 2, 5, 1 and 1, 3, 4, 1. The greatest common divisor of the
lengths of all cycles passing through vertex 1 is 3, which by Fact 8 must be the index of
cyclicity. In fact, A has eigenvalues 2, −1 ± i
√
3, 0, 0 and thus there are 3 eigenvalues
with modulus ρ(A) = 2.
3. By Fact 13 every doubly stochastic matrix is a convex combination of permutation
matrices. For example, the doubly stochastic matrix





.4
.3
.3
0
.5
0
.4
.1
.1
.6
0
.3
0
.1
.3
.6





can be expressed as
.2





0
0
1
0
1
0
0
0
0
1
0
0
0
0
0
1




+ .3





0
1
0
0
1
0
0
0
0
0
0
1
0
0
1
0




+ .1





0
0
1
0
0
0
0
1
1
0
0
0
0
1
0
0




+ .4





1
0
0
0
0
0
1
0
0
1
0
0
0
0
0
1




.

454
Chapter 6
LINEAR ALGEBRA
6.6.4
PERMANENTS
The permanent of a matrix is deﬁned as a sum of terms, each corresponding to the
product of elements along a diagonal of the matrix. Permanents arise in the study of
systems of distinct representatives and in other combinatorial problems.
Deﬁnition:
The permanent of the n × n matrix A is per(A) = P
σ∈Sn a1σ(1)a2σ(2) . . . anσ(n),
where Sn is the symmetric group on n elements. (See §5.3.1.)
Facts:
1. The permanent of A is an unsigned version of the determinant of A. (See §6.3.4, Fact
2.)
2. Computing the permanent of a square (0, 1)-matrix is #P-complete.
3. Laplace expansion:
Suppose A is an n × n matrix and Aij is the submatrix of A
obtained by deleting the ith row and the jth column. Then for i = 1, 2, . . . , n
per(A) =
nP
j=1
aij per(Aij).
A similar expansion holds with respect to any column.
4. per(AT ) = per(A).
5. Interchanging two rows (or two columns) of A does not change per(A).
6. Multiplying any row (or column) of A by the scalar α multiplies per(A) by α.
7. Unlike the determinant, the permanent is not multiplicative with respect to matrix
multiplication. (See Example 2.)
8. The permanent of a triangular matrix (§6.3.1) is equal to the product of its diagonal
entries.
9. The permanent of a block diagonal matrix is equal to the product of the permanents
of its diagonal blocks.
10. For each positive integer n the permanent of the n × n matrix






0
1
· · ·
1
1
0
· · ·
1
...
...
...
1
1
· · ·
0






is n!
nP
r=0
(−1)r 1
r! and it represents the number of derangements (§2.4.2) of order n.
11. The permanent of an n×n (0, 1)-matrix A counts the number of assignments (n×n
permutation submatrices) consistent with the 1 entries of A.
12. Minc-B´regman inequality: Let A be an n×n (0, 1)-matrix with row sums r1, r2, . . . ,
rn. Then
per(A) ≤
nQ
i=1
(ri!)1/ri.
13. If A is a nonnegative n × n matrix with row sums r1, r2, . . . , rn then per(A) ≤
r1r2 . . . rn.

Section 6.6
COMBINATORIAL MATRIX THEORY
455
14. Let A be a fully indecomposable nonnegative integral n × n matrix and let s(A)
denote the sum of the entries in A. Then
s(A) −2n + 2 ≤per(A) ≤2s(A)−2n + 1.
15. Alexandroﬀinequality: Let A be a nonnegative n × n matrix and let Ai be the ith
column of A, i = 1, 2, . . . , n. Then
(per(A))2 ≥per(A1, . . . , An−2, An−1, An−1) per(A1, . . . , An−2, An, An).
16. The deﬁnition of the permanent can be extended to m × n matrices with m ≤n by
summing over all permutations in Sm.
17. If A is an m × n (0, 1)-matrix, then per(A) > 0 if and only if A has term rank m.
18. van der Waerden-Egorychev-Falikman inequality: If A is a doubly stochastic n × n
matrix then per(A) ≥n!
nn , and equality holds if and only if A = Jn, the matrix with each
entry 1
n.
Note: This result was ﬁrst conjectured by B. L. van der Waerden in 1926.
Despite
repeated attempts to prove it, the conjecture remained unresolved until ﬁnally established
in 1980 by G. P. Egorychev. The conjecture was also proved independently by D. I.
Falikman in 1981, apart from establishing the uniqueness of the minimizing matrix A.
A self-contained exposition of Egorychev’s proof is given in [Kn81].
19. Let A be the m × n incidence matrix of m subsets of a given n-set X: namely,
aij = 1 if j ∈Xi and aij = 0 otherwise. Then per(A) counts the number of SDRs
(systems of distinct representatives, §1.2.2) selected from the sets X1, X2, . . . , Xm.
Examples:
1. For the matrix
A =



1
0
2
3
1
1
1
5
2


,
evaluation of per(A) by the deﬁnition gives per(A) = 1·1·2+2·3·5+2·1·1+1·1·5 = 39.
Using the Laplace expansion on row 1 gives per(A) = 1 · per
 
1
1
5
2
!
+ 2 · per
 
3
1
1
5
!
=
1 · 7 + 2 · 16 = 39.
2. If A =
 
1
1
0
1
!
and B =
 
1
0
1
1
!
, then C = AB =
 
2
1
1
1
!
. Notice that per(AB) =
3 ̸= 1 · 1 = per(A) per(B).
3. Assignments:
Suppose there are 4 applicants for 4 jobs, where the qualiﬁcations of
each applicant i for each job j can be speciﬁed by the (0, 1)-matrix
A =





0
1
0
1
1
1
0
1
0
0
1
1
1
1
1
0




.
Then the number of diﬀerent assignments of jobs to qualiﬁed applicants (see §6.6.1,
Example 3) equals per(A) = 4. In fact, these are given by those permutations σ where
{(σ(1), σ(2), σ(3), σ(4))} = {(2, 1, 4, 3), (2, 4, 3, 1), (4, 1, 3, 2), (4, 2, 3, 1)}.

456
Chapter 6
LINEAR ALGEBRA
4. M´enage problem:
Suppose that 5 wives are seated around a circular table, leaving
one vacant space between consecutive women. Find the number of ways to seat in these
vacant spots their 5 husbands so that no man is seated next to his wife. Suppose that
the wives occupy positions W1, W2, . . . , W5 listed in a clockwise fashion around the table
and that Xi is the vacant position to the right of Wi. Let A be the 5 × 5 (0, 1)-matrix
where aij = 1 if and only if husband Hi can be assigned to position Xi without violating
the requirements of the problem. Then
A =







0
0
1
1
1
1
0
0
1
1
1
1
0
0
1
1
1
1
0
0
0
1
1
1
0







.
By Fact 11, the number of possible assignments for each ﬁxed placement of wives is
per(A) = 13. (Also see §2.4.2, Example 7.)
5. Count the number of nontaking rooks on a chessboard with restricted positions
(§2.4.2).
Speciﬁcally, suppose that positions (1, 1), (2, 3), (3, 1), (4, 2), (4, 3) of a 4 × 4
chessboard cannot be occupied by rooks. In the remaining positions, 4 rooks are to be
placed so they are nontaking: no two are in the same row or in the same column. This
can be solved (see Fact 11) by ﬁnding all permutations consistent with the 1s in the
matrix
A =





0
1
1
1
1
1
0
1
0
1
1
1
1
0
0
1




.
Here per(A) = 6 is easily found using the Laplace expansion on the ﬁrst column of A, so
there are 6 placements of nontaking rooks.
6.6.5
MATRIX COMPLETION AND SIGN-SOLVABILITY
Matrix completion deals with ﬁlling in missing entries in a partial matrix to obtain a
matrix with certain properties. Applications of this concept can be found in data analysis
and control, for example. The linear algebra of sign-solvability is motivated by qualitative
analysis in economics.
Deﬁnitions:
A real symmetric n × n matrix A is called positive semideﬁnite if xT Ax ≥0 for all
x ∈Rn, i.e., the quadratic form Q(x) = xT Ax = P
i,j aijxixj is never negative.
A real symmetric n × n matrix A is called copositive if xT Ax ≥0 for all nonnegative
x ∈Rn.
A partial matrix is a rectangular array in which some entries are speciﬁed and others
are not. If values are chosen for each unspeciﬁed entry, one obtains a completion of the
partial matrix.
The sign pattern of a matrix A is the matrix obtained from A by replacing each entry
by its sign (0, 1, or −1). The qualitative class Q(A) of A consists of all matrices with
the same sign pattern as A.

Section 6.6
COMBINATORIAL MATRIX THEORY
457
A system of linear equations Ax = b is sign-solvable if for each ˜A ∈Q(A) and ˜b ∈Q(b),
the system ˜Ax = ˜b has a solution and all solutions obtained in this way lie in the same
qualitative class. Thus, changing numerical values, but not the sign, of the data in the
linear system does not change the sign of the solutions.
Matrix A is termed an L-matrix if every matrix in the qualitative class of A has linearly
independent rows. A square L-matrix is called a sign-nonsingular (SNS) matrix.
Facts:
1. The set of n × n copositive matrices is a convex cone which is the dual of the cone of
n × n completely positive matrices.
2. Let A be a partial matrix whose fully speciﬁed principal submatrices are positive
semideﬁnite, and with positive diagonal elements. Then A can be completed to a positive
semideﬁnite matrix if and only the graph of A (corresponding to speciﬁed entries) is
chordal, i.e., every cycle of length at least 4 has a chord.
3. Every partial copositive matrix has a completion that is copositive.
4. The linear system Ax = 0 is sign-solvable if and only AT is an L-matrix.
5. If the linear system Ax = b is sign-solvable, then AT is an L-matrix.
6. A matrix A is an L-matrix if and only if for every diagonal matrix D with diagonal
elements 0, ±1 the matrix DA contains a nonzero column which is unisigned (i.e., it does
not contain both a positive and a negative entry).
7. The recognition problem for L-matrices is NP-complete.
8. A square matrix A is an SNS-matrix if and only if det A ̸= 0 and the determinant of
every matrix in Q(A) has the same sign.
9. Ax = b is sign-solvable if and only if A is an SNS-matrix and each matrix obtained
from A by replacing a column by b is either an SNS-matrix or has an identically zero
determinant (that is, each of the n! terms in the determinant expression is zero).
Examples:
1. The following matrix A is a partial copositive matrix and B is a copositive completion
of A.
A =



8
?
−1
?
2
−1
−1
−1
1


,
B =



8
4
−1
4
2
−1
−1
−1
1


.
2. The linear system Ax = b whose sign patterns are
A′ =
 
1
1
1
−1
!
,
b′ =
 
1
0
!
is sign-solvable. The determinant of every matrix in Q(A) is negative, so A is an SNS-
matrix. Moreover, when ˜A ∈Q(A) and ˜b ∈Q(b), every solution x =
 
x1
x2
!
of ˜Ax = ˜b
satisﬁes that x1 and x2 have the same sign (by the second equation) and actually x1, x2 >
0 (by the ﬁrst equation).
The notion of sign-solvability is motivated by qualitative
problems in economics; see [BrSh09] and [Sa83].
3. A matrix A whose sign pattern is
A′ =



1
0
0
∗
0
−1
0
∗
0
0
1
∗




458
Chapter 6
LINEAR ALGEBRA
where ∗denotes any sign, is an L-matrix. This is seen from the deﬁnition or by Fact 6.
6.7
SINGULAR VALUE DECOMPOSITION
The singular value decomposition (SVD) is a matrix factorization that is used widely in
many applications. Its importance in data analysis as well as in theory has changed the
ﬁeld of linear algebra over the last three decades. In recent years, the SVD and higher-
order equivalents have become even more prominent due to increased computational
memory and speed, as well as the multidimensional nature of many datasets.
6.7.1
BASIC CONCEPTS
Deﬁnitions:
A singular value decomposition (SVD) of a matrix A ∈Rm×n expresses A = UΣV T ,
where U ∈Rm×m, V ∈Rn×n are orthogonal matrices and Σ ∈Rm×n is diagonal. The
diagonal entries σi of Σ also satisfy σ1 ≥σ2 ≥· · · ≥σp ≥0, where p = min(m, n).
The singular values of A are the diagonal entries of Σ.
The left singular vectors of A are the m × 1 column vectors of U.
The right singular vectors of A are the n × 1 column vectors of V .
If the SVD of an m × n matrix A is given by A = UΣV T then the thin (or reduced)
SVD of A (m ≥n) expresses A = U1Σ1V T , where U1 is m × n and contains the ﬁrst n
columns of U and Σ1 is an n × n diagonal matrix containing the n singular values of A.
The matrix p-norm for 1 ≤p ≤∞of a matrix A ∈Rm×n corresponding to the vector
p-norm is the real number
||A||p = sup
n
||Ax||p
||x||p | x ̸= 0, x ∈Rno
.
The Frobenius norm of a matrix A ∈Rm×n, denoted ||A||F , is the real number
||A||F =
qPm
i=1
Pn
j=1 |aij|2.
Facts:
Most of these facts can be found in an advanced linear algebra or matrix theory book.
See [GoVL13, §2.4], [Ma13], [TrBa97, Lectures 4–5] for more details and proofs.
1. Any m×n matrix A has an SVD factorization. If A ∈Cm×n then the SVD is written
as A = UΣV ∗, where U and V are unitary, but Σ ∈Rm×n.
2. The singular values of A are uniquely determined.
3. If A is square and the singular values are distinct, then the left and right singular
vectors are uniquely determined up to (complex) signs.
4. Suppose A = UΣV T is the SVD of A and there are r nonzero singular values. Denote
ui as the ith column of U and vj as the jth column of V . Then

Section 6.7
SINGULAR VALUE DECOMPOSITION
459
• rank(A) = r;
• null(A) = span{vr+1, . . . , vn};
• range(A) = span{u1, . . . , ur};
• null(AT ) = span{ur+1, . . . , um};
• range(AT ) = span{v1, . . . , vr}.
5. If the SVD of A is given by A = UΣV T then
• the nonzero singular values of A are the square roots of the nonzero eigenvalues
of both AAT and AT A;
• the columns of U are the orthonormal eigenvectors of AAT ;
• the columns of V are the orthonormal eigenvectors of AT A.
6. If A is symmetric then the singular values of A are the absolute values of the eigen-
values of A.
7. The image of the unit sphere in Rn under the map A = UΣV T is a hyperellipse
in Rm. The nonzero singular values are the lengths of the principal semi-axes of the
hyperellipse and the left singular vectors are the unit vectors in the direction of the
principal semi-axes.
8. The SVD can be written as a sum of rank-1 matrices:
A =
r
X
i=1
σiuivT
i ,
(1)
where r = rank(A), σi is the ith singular value, and ui, vi are the ith left and right
singular vectors, respectively.
9. If A is m × n and has nonzero singular values σ1, . . . , σr then
• ||A||2 = σ1;
• ||A||F =
p
σ2
1 + σ2
2 + · · · + σ2r;
• min
x̸=0
||Ax||2
||x||2
= σn if m ≥n.
10. If k < r = rank(A) and Ak =
k
X
i=1
σiuivT
i then
•
min
rank(B)=k ||A −B||2 = ||A −Ak||2 = σk+1;
•
min
rank(B)=k ||A −B||F = ||A −Ak||F =
q
σ2
k+1 + · · · + σ2r.
11. If A is an n × n square matrix then | det(A)| =
n
Y
i=1
σi.
12. If A is square then it can be factored into its polar decomposition A = QP, where
Q is orthogonal and P is symmetric and positive deﬁnite. If A = UΣV T is the SVD of
A then the polar decomposition of A is given exactly by A = (UV T )(V ΣV T ).

460
Chapter 6
LINEAR ALGEBRA
Examples:
1. Let A =


3
2
2
3
2
−2

. The SVD of A = UΣV T is
U =


−
√
2/2
1/
√
18
−2/3
−
√
2/2
−1/
√
18
2/3
0
4/
√
18
1/3

, V =
"
−
√
2/2
√
2/2
−
√
2/2
−
√
2/2
#
, Σ =


5
0
0
3
0
0

.
• rank(A) = 2 since there are two nonzero singular values.
• ||A||2 = 5 and ||A||F =
√
52 + 32 =
√
34.
• A can be written as a sum of two rank-1 (outer product) matrices:
A = 5


−
√
2/2
−
√
2/2
0


"
−
√
2/2
−
√
2/2
#T
+ 3


1/
√
18
−1/
√
18
4/
√
18


"
√
2/2
−
√
2/2
#T
.
• The eigenvalues of AT A are λ1 = 25 and λ2 = 9, which are the squares of the
singular values.
2. Let A =


−1
2
2
2
−1
2
2
2
−1

. The singular values of A are σ1 = 3, σ2 = 3, σ3 = 3.
Thus the determinant is the product of the singular values: det(A) = 27.
6.7.2
COMPUTATION OF THE SVD
Mathematically, the SVD of A can be computed using the eigenvalue decomposition of
AT A. This algorithm is not preferred in practice for two reasons. First, the algorithm
is numerically unstable since forming AT A can result in a loss of information [GoVL13,
§5.3.2].
Second, if A is large, forming AT A may be troublesome.
Even if AT A can
be formed, computing its eigenvalue decomposition may be impractical.
In practice,
the main SVD algorithm is iterative and similar to the QR algorithm used to compute
eigenvalues (§6.5.4) without explicit formation of AT A. Here we present an overview of
the common SVD algorithms and refer to [GoVL13], [ClDh13] for details and references.
Deﬁnitions:
A Householder matrix is an m × m matrix P such that
P = I −βvvT ,
β =
2
vT v,
where v ∈Rm is nonzero. A Householder matrix represents a linear transformation that
reﬂects a vector in the hyperplane span{v}⊥.
An upper (lower) bidiagonal matrix B = (bij) is a matrix in which the only nonzero bij
occur when i = j or i = j −1 (i = j + 1).

Section 6.7
SINGULAR VALUE DECOMPOSITION
461
Facts:
1. SVD with Golub-Kahan bidiagonalization: This method (Algorithm 1) computes the
SVD of a matrix A using a two-phase approach; a bidiagonalization phase followed by
an iterative procedure to diagonalize the bidiagonal matrix.
Algorithm 1:
SVD with Golub-Kahan bidiagonalization.
input: m × n matrix A, m ≥n
output: matrices U, V, Σ such that A = UΣV T gives the SVD of A.
1. bidiagonalization. A is brought to upper bidiagonal form (see [GoKa65])
{Initialization}
A1 := A
{Iterative step}
for k := 1 to n
Ak+1 := U T
k Ak
if k ≤n −2
Ak+1 := Ak+1Vk
B := An+1
˜
U1 := U1U2 · · · Un
˜V1 := V1V2 · · · Vn−2
2. diagonalization. Upper bidiagonal B = ˜U T
1 A ˜V1 is diagonalized using a variant
of the QR algorithm (see [GoVL13, §8.6.3])
{Iterative step}
for k := 1 to n −1
B := BVk(θ1; k, k + 1)
B := Uk(θ2; k, k + 1)T B
˜U2 := U1U2 · · · Un−1
˜V2 := V1V2 · · · Vn−1
U := ˜U1 ˜U2
V := ˜V1 ˜V2
Σ := B
2. The bidiagonalization step of Algorithm 1 ﬁnds ˜U1 and ˜V1 simultaneously by implicitly
applying the QR method for the symmetric eigenvalue problem to AT A [GoVL13, §8.6.3].
3. In the bidiagonalization step of Algorithm 1, the orthogonal matrix Uk is a House-
holder matrix that reﬂects the vector Ak([k, k + 1, . . . , n], k) onto a multiple of e1.
Similarly, the orthogonal matrix Vk is a Householder matrix that reﬂects the vector
Ak(k, [k + 1, k + 2, . . . , n]) onto a multiple of e1.
4. In the diagonalization step of Algorithm 1, the orthogonal matrix Vk(θ1; k, k + 1) is
a rotation matrix deﬁned by θ1 such that
yT Vk(θ1; k, k + 1) = α
h
1
0
i
,

462
Chapter 6
LINEAR ALGEBRA
where α ∈R and
yT =



h
t11 −λ
t12
i
for k = 1
h
bk,k+1
bk,k+2
i
for 1 < k < n −1.
In the above, λ is the eigenvalue of the trailing 2 × 2 submatrix of T that is closest to
tnn for T = BT B.
5. In the diagonalization step of Algorithm 1, the orthogonal matrix Uk(θ2; k, k + 1) is
a rotation matrix deﬁned by θ2 such that
Uk(θ2; k, k + 1)T
"
bkk
bk+1,k
#
= β
"
1
0
#
,
where β ∈R.
6. Various improvements to the bidiagonalization step have been proposed and there is
a long history of work related to improving SVD algorithms in general. Specialized SVD
algorithms for structured matrices have also been developed [GoVL13, §8.6], [ClDh13].
7. Two-Sided Jacobi SVD: This method (Algorithm 2) computes the SVD of a matrix
by computing the SVD of 2 × 2 submatrices.
Algorithm 2:
Two-Sided Jacobi SVD.
input: m × n matrix A
output: matrices U, V, Σ such that A = UΣV T gives the SVD of A.
{Initialization}
A1 = (a(1)
ij ) := A
{Iterative step}
for k := 1 to . . . until convergence criteria met
choose r, s (r < s) with |a(k)
rs | as large as possible
deﬁne φ by tan φ = a(k)
rs −a(k)
sr
a(k)
rr + a(k)
ss
B = (bij) := Qk(φ; r, s)T Ak
deﬁne θ by cot 2θ = brr −bss
2brs
Ak+1 = (a(k+1)
ij
) := Jk(θ; r, s)T BJk(θ; r, s)
Σ := Ak+1
U := Q1J1Q2J2 · · · QkJk
V := J1J2 · · · Jk
8. The sequence Ak in Algorithm 2 converges to a matrix with maximum sums of squares
of its diagonal elements. Choosing the largest |ars| works to ensure linear convergence
so that the sums of squares of the oﬀ-diagonal elements decrease at each step. However,
in practice the asymptotic convergence rate of Algorithm 2 is quadratic [GoVL13, §8.5].
The convergence criteria is met when the sums of squares of the oﬀ-diagonals are less
than some prescribed tolerance.
9. The orthogonal matrix Q(φ; r, s) in Algorithm 2 represents a rotation by the angle
φ, chosen so that the 2 × 2 (r, s) submatrix of B = Q(φ; r, s)Ak is symmetric.

Section 6.7
SINGULAR VALUE DECOMPOSITION
463
10. The orthogonal matrix J(θ; r, s) in Algorithm 2 is the same as U(θ; r, s) in Algorithm
3 of §6.5.4, since the 2 × 2 (r, s) submatrix of B in the Two-Sided Jacobi Algorithm is
symmetric.
6.7.3
SOLVING LINEAR SYSTEMS
The SVD is ubiquitous in real-world problems. Its widespread appeal is due to its stability
in computation and ability to provide meaningful solutions of linear systems that may
be ill-conditioned (i.e., close to singular). The SVD is the factorization of choice used to
ﬁnd the best solution to a non-square linear system or systems that are not full rank. It
is also the most precise and numerically stable method to compute the rank of a matrix.
Deﬁnitions:
A rank-deﬁcient matrix A ∈Rm×n has rank(A) < min(m, n).
An underdetermined system of linear equations is a system Ax = b, with A ∈Rm×n,
x ∈Rn, b ∈Rm, where m < n.
An overdetermined system of linear equations is a system Ax = b, with A ∈Rm×n,
x ∈Rn, b ∈Rm, where m > n.
A least squares problem ﬁnds a minimum norm solution to an overdetermined system
of linear equations. That is, given A ∈Rm×n, b ∈Rm, with m > n, solve
min
x∈Rn ||Ax −b||2.
The Moore-Penrose pseudo-inverse of A ∈Rm×n is the unique matrix A+ ∈Rn×m
that satisﬁes
AA+A
=
A,
A+AA+
=
A+,
(AA+)T
=
AA+,
(A+A)T
=
A+A.
The numerical rank of a matrix A ∈Rm×n for some (small) tolerance δ is the number
ˆr such that the computed singular values (σi) of A satisfy
σ1 ≥σ2 ≥· · · ≥σˆr > δ > σˆr+1 ≥· · · ≥σp,
p = min(m, n).
Usually δ = ǫ||A||∞where ǫ is the machine epsilon.
Facts:
1. The most precise and numerically stable method to compute the rank of a matrix is
to compute its numerical rank using the SVD.
2. Let the SVD of A ∈Rm×n be given by A = UΣV T , where r = rank(A), σi is the ith
singular value, ui and vi are the ith left and right singular vectors, respectively.
• The least squares solution ˜x to the overdetermined system Ax = b is
˜x =
r
X
i=1
uT
i b
σi
vi.
• The smallest 2-norm residual is given by
||A˜x −b||2
2 = Pm
i=r+1(uT
i b)2.

464
Chapter 6
LINEAR ALGEBRA
• The smallest 2-norm solution above is also valid if the system is underdetermined.
3. The Moore-Penrose pseudo-inverse A+ ∈Rn×m of A ∈Rm×n is computed using
the SVD of A.
Speciﬁcally, if A = UΣV T is the SVD of A, then A+ = V Σ+U T
where Σ+ ∈Rn×m has diagonal elements 1/σi for i = 1, . . . , r followed by n −r zeros
(r = rank(A)).
• The least squares solution ˜x to the overdetermined system Ax = b can also be
written as ˜x = A+b.
• The pseudo-inverse A+ ∈Rn×m of A ∈Rm×n is the unique Frobenius norm
solution to the problem
min
X∈Rn×m ||AX −Im||F .
• If A is square and nonsingular, then A+ = A−1.
Examples:
1. Suppose A =


3
2
2
3
2
−2

and b =


1
2
2

.
• The Moore-Penrose pseudo-inverse of A is A+ =
1
45
"
7
2
10
2
7
−10
#
.
• The least squares solution to Ax = b is ˜x = A+b =
1
45
"
31
−4
#
.
2. Note that (AB)+ ̸= B+A+ in general, even if A, B are square. For example, let
A =
"
1
0
0
0
#
,
B =
"
1
1
0
1
#
.
Then (AB)+ =
"
1
2
0
1
2
0
#
; however B+A+ =
"
1
0
0
0
#
.
6.7.4
APPLICATIONS
The SVD is utilized in many applications. It plays a major role in statistics where it is
used in Principal Component Analysis and multivariate data analysis. Here we list some
main application areas of the SVD with the understanding that the list is not exhaustive,
but rather it gives a ﬂavor for the types of problems that are well suited for the SVD.
Deﬁnitions:
Dimension reduction is a method that reduces the number of dimensions of a dataset
while maintaining as much information about the dataset as possible.
Let A ∈Rm×n comprise a matrix whose ith column represents m measurements of the
ith variable centered about zero. Then
1
m−1AT A is a covariance matrix, in which the
(i, j)-element measures the covariance between column i and column j of A.

Section 6.7
SINGULAR VALUE DECOMPOSITION
465
Alternating least squares (ALS) is a method that solves a multidimensional least
squares problem such that holding all variables constant but one reduces the problem
to a linear least squares problem (§6.7.3) in that one variable. The multidimensional
problem is solved by alternating which set of variables is held constant with the goal to
eventually converge to the multidimensional least squares solution, though convergence
is not guaranteed.
Examples:
1. Principal Component Analysis (PCA): PCA is a dimension reduction technique that
performs an orthogonal transformation of a dataset A ∈Rm×n in order to reduce its
dimensionality while retaining as much of the variation as possible [Jo02]. PCA computes
the eigenvalue decomposition of the covariance matrix of A. The eigenvectors (principal
components) corresponding to the dominant eigenvalues contain most of the variation
of the data and are uncorrelated. PCA is equivalent to computing the SVD of the data
matrix due to Fact 5, §6.7.1.
2. Latent Semantic Indexing (LSI): Given a set of n documents and a set of m terms
appearing in those documents, set A to be the m × n term-document matrix where the
(i, j)-element of A is the number of times term i appears in document j. LSI uses the
SVD of A in order to correlate related terms in a set of documents [De88], [BeDuOB95],
[Me00, §5.12]. This is often used to ﬁnd relevant documents based on a user query of a set
of terms. Due to the typically large size of A in real-world applications, Fact 10 in Section
§6.7.1 is used to compute the nearest rank-k approximation to A where k << rank(A).
3. Eigenfaces: Given a set of N grayscaled facial images all of size m × n, set A to be
the mn × N matrix where the jth column is the vectorized matrix of pixels of image j.
Eigenfaces is a facial recognition algorithm that uses PCA to project an m × n query
image onto the dominant eigenspaces of the covariance matrix of A. This is equivalent to
computing the SVD of A. See [MuMaHe04], [TuPe91a], [TuPe91b] for more information.
4. Collaborative Filtering: Collaborative ﬁltering is one approach to a class of methods
called recommender systems that work to give suggestions on an item i to a user u.
Recommender systems have become extremely popular recently due to their substantial
improvement to the Netﬂix recommender system publicized by the winner of the Netﬂix
prize [KoBeVo09]. They are now in use in many internet sites, including Amazon.com,
YouTube, Netﬂix, Yahoo, and more. Collaborative ﬁltering is used when explicit feedback
of user ratings towards items is not available and user behavior (“click history”) is used
instead.
Suppose A is the m×n user-item matrix where the (u, i)-entry is a rating rui which indi-
cates the preference by user u of item i (we use the rui terminology to remain consistent
with the collaborative ﬁltering literature). The general model is based on the SVD and
in its most basic form involves ﬁnding (not necessarily orthogonal) matrices X, Y that
solve
min{||AT −Y T X||2
2 | X ∈Rk×m, Y ∈Rk×n},
where k << rank(A). There are many extensions to this basic model, including regular-
ization and addition of bias terms. Since A is typically very large, the SVD cannot be
explicitly computed and ALS is used instead to ﬁnd the best X, Y . The ith column of
Y measures the extent to which item i possesses the k factors, and the uth column of X
measures the extent of interest user u has in items high in the k factors. Therefore, rui
is viewed as an inner product of column i of Y and column u of X. See [KoBe11] for a
detailed treatment of collaborative ﬁltering.
5. Data Mining: The SVD is used in a variety of problems related to clustering and
multivariate analysis of large datasets (see for example [MaKeHo05]). PCA, LSI, and

466
Chapter 6
LINEAR ALGEBRA
Eigenfaces are three examples of data mining techniques, but there are more. The SVD
is also used in graph partitioning for the ratio cut problem and is used in the HITS
Algorithm for web searching and queries [Kl99]. The SVD appears in chemical physics
to obtain approximate solutions to the coupled-cluster equations and is a popular tool
for electronic structure calculations [Ki03]. It is useful in genetics in a PCA context
to understand the dynamics of gene expression and classify genes into groups of simi-
lar regulation and function [AlBrBo00], [HoEtal00]. The SVD has found its way into
political science where it was used to investigate the ideologies of members of Congress
[MaPo12], [PoEtal05], [PoEtal07]. It has also been applied in robotics [BeKu02], ﬁnancial
mathematics [FeEtal11], compressed sensing [XuLi10], and geophysics [Sm94].
6. Cross-Modal Factor Analysis (CFA): CFA is a technique to rotate both optimally and
rigidly one m-element data matrix into another. The rotation is rigid in the sense that
both distances and angles between the elements in the initial data matrix are preserved.
The rotation is optimal in the sense that the sum of the m squared (Euclidean) distances
between the corresponding initial and target data point pairs is minimized. CFA is also
known as the orthogonal Procrustes problem [GoVL13]. In particular, to map an m × n
data matrix B optimally into an m × n data matrix A solve
min
Q∈Rn×n ||A −BQ||2
F ,
subject to QT Q = In.
The optimal Q is Q = UV T where U, V are found from the SVD BT A = UΣV T .
Note that if B is an m × n data matrix and A = BQ for some unknown orthogonal
transformation Q, then the minimum squared Frobenius norm is 0 and CFA may be
applied to recover the matrix Q.
7. Canonical Correlation Analysis (CCA): CCA is a way of measuring correlations be-
tween two groups of random variables, x = (x1, . . . , xm) and y = (y1, . . . , yn) [Ho36],
[Mu82], [BjGo73]. The goal is to ﬁnd linear combinations of the ﬁrst group and linear
combinations of the second group that are maximally correlated. The solution is found
using the SVD of the cross-covariance matrix, in which the (i, j)-element is the covariance
between variables xi and yj.
8. Geometric Applications: Fact 7, §6.7.1 is used in geology to measure the grain sizes
of irregularly shaped crystals. This is done by ﬁnding ellipsoids that approximate the
size and shape of each grain [MaPo12], [Hi06], [Ma98]. The SVD is also used in shape
matching problems; see for example [RuGa06].
9. Compression: The SVD is used in many areas to obtain low-rank approximations to a
dataset using Fact 10, §6.7.1. It is used in signal processing as a noise reduction method,
where a low-rank approximation represents a ﬁltered signal with less noise [Me00, §5.12].
Using the SVD as a noise ﬁlter is also popular in image deblurring [HaNaOL06]. Similarly,
the SVD is used in image processing to obtain a low-rank approximation to an image as
a way to compress the image signal [Ka96].
10. Other Applications: The SVD is used in entanglement, a quantum form of corre-
lation, where it is used to diagonalize the one-particle reduced density matrix to obtain
the natural orbitals (i.e., the singular vectors) and their occupation numbers (i.e., the
singular values) [PaYo01], [ScLoMa01], [MaPo12]. It is also important for theoretical en-
deavors, such as path-following methods for computing curves of equilibria in dynamical
systems [DiGaPa06].

Section 6.7
SINGULAR VALUE DECOMPOSITION
467
6.7.5
Higher Dimensions
More and more applications involve data that are best represented in higher dimensions.
In particular, data compression and PCA are two signiﬁcant motivating areas that have
prompted higher-dimensional equivalents of the matrix SVD. Thus, information is stored
in multidimensional (or multiway) arrays, i.e., tensors, instead of as matrices. Nearly all
the ﬁelds mentioned in §6.7.4 and many others use higher-order tensors for multidimen-
sional data analysis.
Deﬁnitions:
An order-N tensor is a multiway array with N indices:
A = (ai1i2...iN ) ∈RI1×I2×···×IN.
The outer product, denoted by “◦,” of a set of N vectors x1 ∈RI1, x2 ∈RI2, . . . , xN ∈
RIN is the order-N tensor
A = x1 ◦x2 ◦· · · ◦xN ∈RI1×I2×···×IN ,
where each element ai1i2...iN of A is the product of the corresponding vector elements:
ai1i2...iN = x1(i1)x2(i2) . . . xN(iN)
for all 1 ≤ij ≤Ij.
Here xj(ij) refers to the ijth element of the vector xj for j = 1, . . . , N.
If an order-N tensor A can be expressed as an outer product of N vectors, then A is a
rank-one tensor.
The rank of an order-N tensor A is the minimum number of rank-one tensors that sum
to A. For A ∈RI×J×K (illustrated for notational simplicity), its rank decomposition is
of the form
A =
r
X
i=1
xi ◦yi ◦zi,
(2)
where r is minimal, xi ∈RI, yi ∈RJ, and zi ∈RK for i = 1, . . . , r.
The maximum rank of a tensor is the largest attainable rank for that given order and
sized tensor.
The typical rank of a tensor is any rank that occurs with probability greater than zero
(i.e., on a set with positive Lebesgue measure).
Let A be an order-N tensor. The Higher-Order Singular Value Decomposition
(HOSVD) is a sum of rank-one tensors where the corresponding vectors are orthonormal.
If N = 3 and A ∈RI×J×K this is usually written as
A =
I
X
i=1
J
X
j=1
K
X
k=1
σijk(ui ◦vj ◦wk),
(3)
where ui, vj, wk are the ith, jth, kth columns of orthogonal matrices U, V, W, respectively
and σijk are elements from the tensor Σ ∈RI×J×K.

468
Chapter 6
LINEAR ALGEBRA
Facts:
1. A ﬁrst-order tensor is a vector, a second-order tensor is a matrix, a third-order tensor
is a “cube,” and so on.
2. Suppose the SVD of A ∈Rm×n is given by A = UΣV T . Using the outer product
notation, equation (1) can be re-written as
A =
r
X
i=1
σi(ui ◦vi).
(4)
3. The deﬁnition of rank for matrices and order-2 tensors coincide. Indeed, the minimal
r such that a matrix A is a sum of rank-one matrices is given by its SVD.
4. The HOSVD, illustrated for order-3 tensors in (3), is not a rank-revealing decompo-
sition for tensors of order N ≥3.
5. Extending (4) for higher-order tensors deﬁnes the rank of a higher-order tensor. The
extension is easily seen by comparing (2) and (4). In contrast with matrices, the set of
vectors in (2) is not necessarily orthogonal, whereas a rank decomposition for matrices
can be chosen to be the SVD in which case the columns of U and V in (4) are orthonormal.
Furthermore, it can be shown that an orthogonal rank decomposition of the form (2) may
not exist.
6. The HOSVD can be computed directly by computing the SVDs of matrix “ﬂatten-
ings” in each dimension. Therefore, the HOSVD exists for any tensor A [GoVL13, §12.5],
[KoBa09], [DeDeVa00].
7. The maximum rank and typical rank for order-2 tensors (matrices) are identical. If
A ∈Rm×n then the maximum (typical) rank of A is min(m, n).
8. Suppose A is an order-N real-valued tensor, where N ≥3. There are several diﬀer-
ences between tensor rank and matrix rank:
• rank(A) may be diﬀerent when considered over R and C. See [Ma11], [KoBa09]
for examples and explanations. Contrast this with matrices where the rank
(and SVD) is the same when a real-valued matrix is considered over R and C.
• Except in special cases, there is no direct algorithm to determine rank(A), and
in fact the problem is NP-hard [Ha90].
In practice the rank of a tensor is
determined numerically by iterative algorithms; see [KoBa09].
• A may have a diﬀerent maximum and typical rank.
• A may have more than one typical rank, whereas a tensor over C always has one
typical rank.
• If A ∈RI×J×K, only the weak upper bound on its maximum rank is known:
rank(A) ≤min{IJ, IK, JK}.
• If A ∈R2×2×2, then the maximum rank of A is 3 over R and 2 over C. The
typical ranks of A are 2 and 3 (when considered over R).
• If A ∈Rn×n×2, then the maximum rank of A is 3n/2 over R and n over C. The
typical ranks for A are either n or n + 1. See [KoBa09], [CoEtal09] for more
special cases of typical rank.
9. Fact 10, §6.7.1 does not extend to tensors of order N ≥3.
The best low-rank
approximation to a higher-order tensor may not exist.
10. The ordering of the dimensions of the tensor does not aﬀect the rank.

REFERENCES
469
Examples:
1. The set of n×n×n real-valued rank-deﬁcient tensors A over R has nonzero measure.
If A ∈R2×2×2, Monte Carlo simulations show that a rank-2 tensor is generated 79% of
the time and a rank-3 tensor is generated 21% of the time.
2. Suppose A ∈R2×2×2 where the front and back slices of A are deﬁned respectively
by the matrices
"
1
0
0
1
#
and
"
0
1
−1
0
#
.
Then rank(A) = 3 over R:
A =
 "
1
0
#
◦
"
1
0
#
◦
"
1
−1
#!
+
 "
0
1
#
◦
"
0
1
#
◦
"
1
1
#!
+
 "
1
−1
#
◦
"
1
1
#
◦
"
0
1
#!
.
However, rank(A) = 2 over C:
A =
 "
1/
√
2
−i/
√
2
#
◦
"
1/
√
2
i/
√
2
#
◦
"
1
i
#!
+
 "
1/
√
2
i/
√
2
#
◦
"
1/
√
2
−i/
√
2
#
◦
"
1
−i
#!
.
3. The HOSVD is a form of higher-order PCA (see §6.7.4) where the matrices U, V, W in
(3) are thought of as the principal components in each dimension. The tensor Σ = (σijk)
in (3) is called the core tensor and its entries give the interaction between the diﬀerent
components. However, the higher-order PCA is used as a dimension reduction technique
and therefore usually has the following form, illustrated for A ∈RI×J×K
A ≈
P
X
i=1
Q
X
j=1
R
X
k=1
σijk(ui ◦vj ◦wk),
(5)
where P < I, Q < J, R < K and ui, vj, wk are the ith, jth, kth columns of matrices U ∈
RI×P , V ∈RJ×Q, W ∈RK×R, respectively where U, V, W (usually) have orthonormal
columns.
4. The Eigenfaces algorithm (Example 3, §6.7.4) has been generalized for higher-order
tensors as TensorFaces [VaTe02].
REFERENCES
Printed Resources:
[AhMaOr93] R. K. Ahuja, T. L. Magnanti, and J. B. Orlin, Network Flows: Theory,
Algorithms, and Applications, Prentice-Hall, 1993.
[AlBrBo00] O. Alter, P. O. Brown, and D. Botstein, “Singular value decomposition for
genome-wide expression data processing and modeling”, Proceedings of the National
Academy of Sciences USA 97 (2000), 10101–10106.
[An87] T. Ando, “Totally positive matrices”, Linear Algebra and Its Applications 90
(1987), 165–219. (Discusses basic properties of totally positive matrices.)
[An14] H. Anton, Elementary Linear Algebra, 11th ed., Wiley, 2014.
[Ba14] R. B. Bapat, Graphs and Matrices, 2nd ed., Springer, 2014. (Contains an intro-
duction to combinatorial matrix theory and spectral graph theory.)

470
Chapter 6
LINEAR ALGEBRA
[BaRa97] R. B. Bapat and T. E. S. Raghavan, Nonnegative Matrices and Applications,
Encyclopedia of Mathematical Sciences, No. 64, Cambridge University Press, 1997.
[Be97] R. Bellman, Introduction to Matrix Analysis, 2nd ed., Society for Industrial and
Applied Mathematics, 1997.
[BeKu02] C. Belta and V. Kumar, “An SVD-based projection method for interpolation
on SE(3)”, IEEE Transactions on Robotics and Automation 18 (2002), 334–345.
[BePl94] A. Berman and R. J. Plemmons, Nonnegative Matrices in the Mathematical
Sciences, Society for Industrial and Applied Mathematics, 1994. (Second revised
edition of an authentic modern introduction.)
[BeDuOB95] M. W. Berry, S. T. Dumais, and G. W. O’Brien, “Using linear algebra for
intelligent information retrieval”, SIAM Review 37 (1995), 573–595.
[BjGo73] A. Bj¨ork and G. H. Golub, “Numerical methods for computing the angles
between linear subspaces”, Mathematics of Computation 27 (1973), 579–594.
[Br06] R. A. Brualdi, Combinatorial Matrix Classes, Cambridge University Press, 2006.
(Discusses matrix classes such as A(R, S) and doubly stochastic matrices in detail.)
[BrRy91] R. A. Brualdi and H. J. Ryser, Combinatorial Matrix Theory, Cambridge
University Press, 1991. (Discusses combinatorial results pertaining to matrices and
graphs, as well as permanents.)
[BrSh09] R. A. Brualdi and B. L. Shader, Matrices of Sign-Solvable Linear Systems,
Cambridge University Press, 2009. (Treats the concept of sign-solvability in detail.)
[ClDh13] A. K. Cline and I. S. Dhillon, “Computation of the singular value decom-
position”, in Handbook of Linear Algebra, 2nd ed., L. Hogben (ed.), Chapman &
Hall/CRC Press, 2013, Chapter 58.
[CoEtal09] P. Comon, J. M. F. Ten Berge, L. De Lathauwer, and J. Castaing, “Generic
and typical ranks of multi-way arrays”, Linear Algebra and Its Applications 430
(2009), 2997–3007.
[Da10] B. N. Datta, Numerical Linear Algebra and Applications, 2nd ed., Society for
Industrial and Applied Mathematics, 2010.
[De88] S. Deerwester, “Improving information retrieval with latent semantic indexing”,
Proceedings of the 51st Annual Meeting of the American Society for Information
Science 25 (1988), 36–40.
[DeDeVa00] L. De Lathauwer, B. De Moor, and J. Vandewalle, “A multilinear singular
value decomposition”, SIAM Journal on Matrix Analysis and Applications 21 (2000),
1253–1278.
[DiGaPa06] L. Dieci, M. G. Gasparo, and A. Papini, “Path following by SVD”, in Com-
putational Science – ICCS 2006, Lecture Notes in Computer Science 3994, V. N.
Alexandrov, G. D. van Albada, P. M. A. Sloot, and J. Dongarra (eds.), Springer,
2006, 677–684.
[DuErRe89] I. S. Duﬀ, A. M. Erisman, and J. K. Reid, Direct Methods for Sparse Ma-
trices, 2nd ed., Oxford University Press, 1989.
[FeEtal11] D. J. Fenn, M. A. Porter, S. Williams, M. McDonald, N. F. Johnson, and N.
S. Jones, “Temporal evolution of ﬁnancial market correlations”, Physical Review E
84 (2011), 026109.

REFERENCES
471
[Ga59] F. R. Gantmacher, The Theory of Matrices, Volumes I and II, Chelsea, 1959.
(The ﬁrst volume contains an introduction to matrix theory; the second discusses
special topics such as nonnegative matrices and totally positive matrices.)
[GeLi81] A. George and J. W-H. Liu, Computer Solution of Large Sparse Positive Deﬁnite
Systems, Prentice-Hall, 1981.
[GoKa65] G. H. Golub and W. Kahan, “Calculating the singular values and pseudo-
inverse of a matrix”, SIAM Journal on Numerical Analysis 2 (1965), 205–224.
[GoVL13] G. H. Golub and C. Van Loan, Matrix Computations, 4th ed., Johns Hopkins
University Press, 2013. (Discusses elementary and advanced matrix computation
problems; a remarkable blend of theoretical aspects and numerical issues.)
[HaNaOL06] P. C. Hansen, J. G. Nagy, and D. P. O’Leary, Deblurring Images: Matrices,
Spectra, and Filtering, Society for Industrial and Applied Mathematics, 2006.
[Ha90] J. H˚astad, “Tensor rank is NP-complete”, Journal of Algorithms 11 (1990), 644–
654.
[Hi06] M. D. Higgins, Quantitative Textural Measurements in Igneous and Metamorphic
Petrology, Cambridge University Press, 2006.
[Ho13] L. Hogben, Handbook of Linear Algebra, 2nd ed., Chapman & Hall/CRC Press,
2013. (This handbook gives an excellent overview of linear algebra and treats com-
binatorial matrix theory in detail.)
[HoEtal00] N. S. Holter, M. Mitra, A. Maritan, M. Cieplak, J. R. Banavar, and N. V.
Fedoroﬀ, “Fundamental patterns underlying gene expression proﬁles: simplicity from
complexity”, Proceedings of the National Academy of Sciences USA 97 (2000), 8409–
8414.
[HoJo90] R. A. Horn and C. R. Johnson, Matrix Analysis, Cambridge University Press,
1990. (A comprehensive, modern introduction to matrix theory.)
[Ho36] H. Hotelling, “Relations between two sets of variables”, Biometrika 28 (1936),
321–377.
[Jo02] I. Joliﬀe, Principal Component Analysis, 2nd ed., Springer Series in Statistics,
2002.
[Ka96] D. Kalman, “A singularly valuable decomposition: the SVD of a matrix”, The
College Mathematics Journal 27 (1996), 2–23.
[Ki03] T. Kinoshita, “Singular value decomposition approach for the approximate coupled-
cluster method”, Journal of Chemical Physics 119 (2003), 7756–7762.
[Kl99] J. Kleinberg, “Authoritative sources in a hyperlinked environment”, Journal of
the ACM 46 (1999), 604–632.
[Kn81] D. E. Knuth, “A permanent inequality”, American Mathematical Monthly 88
(1981), 731–740.
[KoBa09] T. Kolda and B. Bader, “Tensor decompositions and applications”, SIAM
Review 51 (2009), 455–500.
[KoBe11] Y. Koren and R. M. Bell, “Advances in collaborative ﬁltering”, in Recommender
Systems Handbook, F. Ricci, L. Rokach, B. Shapira, and P. Kantor (eds.), Springer,
2011, 145–186.
[KoBeVo09] Y. Koren, R. M. Bell, and C. Volinsky, “Matrix factorization techniques for
recommender systems”, IEEE Computer 42 (2009), 30–37.

472
Chapter 6
LINEAR ALGEBRA
[MaKeHo05] K. Marida, J. T. Kent, and S. Holmes, Multivariate Analysis, Academic
Press, New York, 2005.
[Ma98] A. D. Marsh, “On the interpretation of crystal size distributions in magmatic
systems”, Journal of Petrology 39 (1998), 553–599.
[MaOlAr11] A. W. Marshall, I. Olkin, and B. C. Arnold, Inequalities: Theory of Majoriza-
tion and Its Applications, 2nd ed., Springer, 2011. (A comprehensive introduction to
majorization theory and its many applications.)
[Ma11] C. D. Martin, “The rank of a 2 × 2 × 2 tensor”, Linear and Multilinear Algebra
59 (2011), 943–950.
[MaPo12] C. D. Martin and M. A. Porter, “The extraordinary SVD”, MAA Monthly 119
(2012), 838–851.
[Ma13] R. Mathias, “Singular values and singular value inequalities”, in Handbook of Lin-
ear Algebra, 2nd ed., L. Hogben (ed.), Chapman & Hall/CRC Press, 2013, Chapter
24.
[Me00] C. D. Meyer, Matrix Analysis and Applied Linear Algebra, Society for Industrial
and Applied Mathematics, 2000.
[Mi84] H. Minc, Permanents, Cambridge University Press, 1984.
[Mi88] H. Minc, Nonnegative Matrices, Wiley, 1988.
[Mu82] K. E. Muller, “Understanding canonical correlation through the general linear
model and principal components”, American Statistician 36 (1982), 342–354.
[MuMaHe04] N. Muller, L. Magaia, and B. M. Herbst, “Singular value decomposition,
eigenfaces, and 3D reconstructions”, SIAM Review 46 (2004), 518–545.
[PaYo01] R. Paˇskauskas and L. You, “Quantum correlations in two-boson wave func-
tions”, Physical Review A 64 (2001), 042310.
[PoGe89] C. K. Pokorny and C. F. Gerald, Computer Graphics: The Principles Behind
the Art and Science, Franklin, Beedle & Associates, 1989.
[PoEtal07] M. A. Porter, P. J. Mucha, M. E. J. Newman, and A. J. Friend, “Community
structure in the United States House of Representatives”, Physica A 386 (2007),
414–438.
[PoEtal05] M. A. Porter, P. J. Mucha, M. E. J. Newman, and C. M. Warmbrand, “A
network analysis of committees in the United States House of Representatives”, Pro-
ceedings of the National Academy of Sciences USA 102 (2005), 7057–7062.
[RuGa06] M. Rudek and P. R. Gardel Kurka, “3-D measurement from images using a
range box”, ABCM Symposium Series in Mechatronics 2 (2006), 629–636.
[Sa83] P. A. Samuelson, Foundations of Economic Analysis, Harvard University Press,
1983.
[ScLoMa01] J. Schliemann, D. Loss, and A. H. MacDonald, “Double-occupancy errors,
adiabaticity, and entanglement of spin qubits in quantum dots”, Physical Review B
63 (2001), 085311.
[Sc86] H. Schneider, “The inﬂuence of the marked reduced graph of a nonnegative matrix
on the Jordan form and on related properties: a survey”, Linear Algebra and Its
Applications 84 (1986), 161–189.
[Sc98] A. Schrijver, Theory of Linear and Integer Programming, Wiley, 1998.

REFERENCES
473
[Sc03] A. Schrijver, Combinatorial Optimization – Polyhedra and Eﬃciency, Springer,
2003. (A main reference on combinatorial optimization and polyhedral combina-
torics.)
[Sm94] C. Small, “A global analysis of midocean ridge axial topography”, Geophysical
Journal International 116 (1994), 64–84.
[St05] G. Strang, Linear Algebra and Its Applications, 4th ed., Cengage Learning, 2005.
[Te91] J. M. F. Ten Berge, “Kruskal’s polynomial for 2×2×2 arrays and a generalization
to 2 × n × n arrays”, Psychometrika 56 (1991), 631–636.
[TrBa97] L. N. Trefethen and D. Bau, III, Numerical Linear Algebra, Society for Indus-
trial and Applied Mathematics, 1997.
[TuPe91a] M. A. Turk and A. P. Pentland, “Eigenfaces for recognition”, Journal of Cog-
nitive Neuroscience 3 (1991), 71–86.
[TuPe91b] M. A. Turk and A. P. Pentland, “Face recognition using Eigenfaces”, Proceed-
ings of Computer Vision and Pattern Recognition 3 (1991), 586–591.
[VaTe02] M. A. O. Vasilescu and D. Terzopoulos, “Multilinear image analysis for facial
recognition”, in ICPR 2002: Proceedings of the 16th International Conference on
Pattern Recognition, 2002, 511–514.
[Wi88] J. H. Wilkinson, The Algebraic Eigenvalue Problem, Oxford University Press,
1988. (An excellent reference on eigenvalues and matrix computations in general.)
[XuLi10] L. Xu and Q. Liang, “Computation of the singular value decomposition”, in
Wireless Algorithms, Systems, and Applications, Lecture Notes in Computer Science
6221, G. Pandurangan, V. S. A. Kumar, G. Ming, Y. Liu, and Y. Li (eds.), Springer,
2010, 338–342.
Web Resources:
http://www.mathworks.com (Home page for MATLAB, a high-level language for dense
and sparse systems, eigenanalysis of dense and sparse matrices, and calculation of
characteristic polynomials.)
http://www.nag.com (Contains Fortran, and C libraries for dense and sparse systems
and eigenanalysis of dense and sparse matrices.)
http://www.netlib.org (Contains LINPACK, a collection of Fortran routines for rela-
tively small dense systems; also contains EISPACK, a collection of Fortran routines
for analyzing eigenvalues and eigenvectors of several classes of matrices.)
http://www.netlib.org (Contains LAPACK/CLAPACK, which supersedes LINPACK
and hosts a collection of Fortran and C routines for dense and banded problems, ideal
for shared-memory vector and parallel processors.)
http://www.roguewave.com/products-services/imsl-numerical-libraries (Con-
tains IMSL, Fortran, and C libraries for dense and sparse systems and for eigenanal-
ysis of dense and banded problems.)
http://www.theory.caltech.edu/~preskill/ph219/chap2 13.pdf/ (Lecture notes for
Physics 219: Quantum Information and Computation by J. Preskill, California In-
stitute of Technology.)


7
DISCRETE PROBABILITY
7.1 Fundamental Concepts
Joseph R. Barr
7.2 Independence and Dependence
Joseph R. Barr
7.2.1 Basic Concepts
7.2.2 Urn Models
7.3 Random Variables
Joseph R. Barr
7.3.1 Distributions
7.3.2 Mean, Variance, and Higher Moments
7.3.3 Generating Functions
7.4 Discrete Probability Computations
Peter R. Turner
7.4.1 Integer Computations
7.4.2 Floating-Point Computations
7.5 Random Walks
Patrick Jaillet
7.5.1 General Concepts
7.5.2 One-Dimensional Simple Random Walks
7.5.3 Generalized Random Walks
7.5.4 Applications of Random Walks
7.6 System Reliability
Douglas R. Shier
7.6.1 General Concepts
7.6.2 Coherent Systems
7.6.3 Calculating System Reliability
7.6.4 Specialized Algorithms for Calculating Reliability
7.7 Discrete-Time Markov Chains
Vidyadhar G. Kulkarni
7.7.1 Markov Chains
7.7.2 Transient Analysis
7.7.3 Classiﬁcation of States
7.7.4 Limiting Behavior
7.7.5 First Passage Times
7.7.6 Branching Processes
7.8 Hidden Markov Models
Narada Warakagoda
7.8.1 Basic Concepts
7.8.2 The Evaluation Problem
7.8.3 The Decoding Problem
7.8.4 The Learning Problem
7.9 Queueing Theory
Vidyadhar G. Kulkarni
7.9.1 Single-Station Queues
7.9.2 General Systems
7.9.3 Special Queueing Systems

476
Chapter 7
DISCRETE PROBABILITY
7.10 Simulation
Lawrence M. Leemis
7.10.1 Input Modeling
7.10.2 Output Analysis
7.10.3 Simulation Languages
7.11 The Probabilistic Method
Niranjan Balachandran
7.11.1 The Basic Method
7.11.2 Dependent Random Choice
7.11.3 The Second Moment
7.11.4 Occurrence of Rare Events: The Local Lemma
7.11.5 Basic Concentration Inequalities
7.11.6 Correlation Inequalities
INTRODUCTION
This chapter discusses aspects of discrete probability that are relevant to mathematics,
computer science, engineering, and other disciplines.
Topics covered include random
variables, important discrete probability distributions, random walks, Markov models,
queues, simulation, and the probabilistic method. Various applications to genetics, tele-
phone network performance, reliability, average-case algorithm analysis, and combina-
torics are presented.
GLOSSARY
absorbing boundary: a boundary that stops the motion of a random walk whose
trajectory comes into contact with it.
all-terminal reliability: the probability that a given network is connected.
antithetic variates: a variance reduction technique, based on negatively correlated
variates, used in the simulation analysis of a given system.
aperiodic state: a state of a Markov chain that is not periodic.
arrival process: the statistical description of the time between successive arrivals to a
queueing system.
average-case complexity (of an algorithm): the average number of operations required
by the algorithm, taken over all problem instances of a given size.
Bernoulli random variable: the discrete random variable X ∈{0, 1} with probability
distribution Pr(X = 0) = 1 −p and Pr(X = 1) = p, for some 0 < p < 1.
binomial random variable: the discrete random variable X ∈{0, 1, . . ., n} with prob-
ability distribution Pr(X = k) =
 n
k

pk(1 −p)n−k, for some 0 < p < 1.
Bose-Einstein model: a probability model in which k indistinguishable balls are ran-
domly placed into n distinguishable urns; several balls are allowed to occupy the
same urn.
boundary: a point or set of points restricting the trajectory of a random walk.
branching process: a special type of Markov chain used to model the growth, and
possible extinction, of populations.
closed class: a communicating class of states of a Markov chain in which transitions
from these states never lead to states outside the class.

GLOSSARY
477
coherent system: a system of components for which increasing the number of operat-
ing components will not degrade the performance of the system.
common random numbers: a variance reduction technique in which alternative sys-
tem conﬁgurations are analyzed using the same set of random numbers.
communicating class: a maximal set of states in a Markov chain that are reachable
from one another by a ﬁnite number of transitions.
conditional probability: the probability Pr(A|B) that the event A occurs, given that
another event B has occurred.
cutset: a minimal set of edges in a graph the removal of which disconnects the graph.
decoding problem (for an HMM): the problem of determining the most likely se-
quence of states that could have generated the observed data of an HMM with
speciﬁed parameters.
density function: a nonnegative real-valued function f(x) that determines the distri-
bution of a continuous random variable X via Pr(a < X < b) =
R b
a f(x) dx.
dependent events: events that are not independent.
discrete-event simulation: a simulation of a time-evolving stochastic process in which
changes to the state of the system can only occur at discrete instants.
discrete-time Markov chain: a probabilistic model of a randomly evolving system
whose future is independent of the past if the present state is known.
distribution (of a random variable): a probability measure associated with the values
attained by the random variable.
elastic boundary: a boundary that could be absorbing or reﬂecting, usually depending
on some given probability.
evaluation problem (for an HMM): the problem of determining the probability of ob-
taining the observed data produced by an HMM with speciﬁed parameters.
event: a subset of the sample space.
expected value (of a random variable): the average value taken on by the random
variable.
experiment: any physically or mentally conceivable action having a measurable result.
extinction probability: in a branching process, the probability that the population
eventually dies out.
Fermi-Dirac model: a probability model in which k indistinguishable balls are ran-
domly placed into n distinguishable urns; at most one ball can occupy each urn.
ﬁrst passage time: the time to ﬁrst visit a given set of states in a Markov chain.
ﬂoating-point arithmetic: the “real number” arithmetic of computers.
ﬂop: a unit for ﬂoating-point computations that is useful in assessing the complexity of
an algorithm.
gambler’s ruin: a one-dimensional random walk in which a gambler wins or loses one
unit at each play of a game, with the game terminating whenever the gambler amasses
a known amount or loses the entire initial stake.
geometric random variable: the discrete random variable X ∈{1, 2, . . .} with prob-
ability distribution Pr(X = k) = (1 −p)k−1p, for some 0 < p < 1.

478
Chapter 7
DISCRETE PROBABILITY
hidden Markov model (HMM): a stochastic model in which (hidden) states follow
a Markov chain, yet one only observes a sequence of state-dependent outcomes.
hypergeometric random variable: the discrete random variable that counts the num-
ber of red balls obtained when randomly selecting a ﬁxed number of balls from an
urn containing a speciﬁed number of red and black balls.
independent events: events in which knowledge of whether one of the events did or
did not occur does not alter the probability of occurrence of any of the other events.
independent random variables: random variables whose joint distribution is the
product of their individual distributions.
irreducible chain: a Markov chain that can visit any state from any other state in a
ﬁnite number of steps.
irrelevant edge: an edge of a two-terminal network not appearing on any simple path
joining the two terminals of the network.
K-cutset: a minimal set of edges in a graph, the removal of which disconnects some
pair of vertices in K.
K-tree: a minimal set of edges in a graph that connects all vertices in K.
learning problem (for an HMM): the problem of ﬁnding parameters of an HMM that
are most likely to have produced the observed data.
machine unit: a measure of the precision of ﬂoating-point arithmetic.
Maxwell-Boltzmann model: a probability model in which k distinguishable balls are
randomly placed into n distinguishable urns; several balls can occupy the same urn.
mincut: a minimal set of components in a coherent system such that the system fails
whenever these speciﬁed components fail.
minpath: a minimal set of components in a coherent system such that the system
operates whenever these speciﬁed components operate.
Monte Carlo simulation: a simulation used to study both deterministic and stochas-
tic phenomenon in which the passage of time is not material.
overﬂow: the result of a ﬂoating-point arithmetic operation that exceeds the available
range of numbers.
parallel system: a system of components that fails only when all components fail.
periodic state: a state of a Markov chain that can only be revisited at multiples of a
certain number d > 1 (the period of the state).
Poisson random variable: the discrete random variable X ∈{0, 1, . . .} with proba-
bility distribution Pr(X = k) = e−λλk
k!
, for some λ > 0.
probability: a numerical value between 0 and 1 measuring the likelihood of occurrence
of an event; the larger the number, the more likely the event.
pseudo-random numbers: numbers generated in a predictable fashion, but that ap-
pear to behave like independent and identically distributed random numbers.
purely multiplicative linear congruential generator: a widely used method of pro-
ducing a stream of pseudo-random numbers.
queue capacity: the maximum number of customers allowed at any time in a queueing
system, either waiting or being served.

GLOSSARY
479
queue discipline: the protocol according to which customers are selected for service
from among those waiting for service.
queueing system: a stochastic process in which customers arrive, await service, and
are served.
queue-length process: a stochastic process describing the number of customers in the
queueing system.
Ramsey number R(k, k): the smallest integer N such that any coloring of the edges
of the complete graph on n ≥N vertices possesses a complete subgraph on k vertices
with all edges colored the same.
random numbers: real numbers generated uniformly over the interval (0, 1).
random variable: a function that assigns a real number to each outcome in the sample
space.
random walk: a stochastic process based on the problem of determining the probable
location of a point subject to random motions.
recurrent state: a state of a Markov chain from which the probability of return to
itself is 1.
recurrent walk: a random walk that returns to its starting location with probability 1.
reﬂecting boundary: a boundary that redirects the motion of a random walk whose
trajectory comes into contact with it.
relative error: the (percent) error in a computation relative to the true value.
reliability: the probability that a given system functions at a random instant of time.
roundoﬀerror: the error resulting from abbreviating a number to the precision of the
machine.
sample size: the number of possible outcomes of an experiment.
sample space: the set of all possible outcomes of an experiment.
series system: a system of components that operates only when all components oper-
ate.
service-time distribution: the statistical distribution of time required to serve a cus-
tomer in a queueing system.
simple path: a path containing no repeated vertices.
simulation: a technique for studying numerically the behavior of complex stochastic
systems and estimating their performance.
single-station queueing system: a system in which customers arrive at a service fa-
cility, wait for service, and depart after service completion.
s-t cutset: a minimal set of edges in a graph the removal of which leaves no s-t path.
stability condition: the set of parameter values for which the queue-length process
(or the waiting-time process) has a steady-state distribution.
steady-state distribution: in a queueing system, the limiting probability distribution
of the number of customers in the system.
stochastic process: a collection of random variables, typically indexed by time (dis-
crete or continuous).

480
Chapter 7
DISCRETE PROBABILITY
structure function: a binary-valued function deﬁned on all subsets of components; its
value indicates whether or not the system operates when the speciﬁed components
all operate.
traﬃc intensity: in a queueing system, the ratio of the maximum arrival rate to the
maximum service rate.
trajectory: the successive positions traced out by a particle undergoing a random walk.
transient state: a state in a Markov chain from which the probability of return to
itself is less than 1.
transient walk: a random walk that is not recurrent.
transition probability: the probability of reaching a speciﬁed state in a Markov chain
by a single transition (step) from a given state.
transition probability matrix: the matrix of one-step transition probabilities for a
Markov chain.
two-terminal network: a network in which two vertices (or terminals) are speciﬁed.
two-terminal reliability: the probability that the speciﬁed vertices of a two-terminal
network are connected by a path of operating edges.
underﬂow: the result of a ﬂoating-point operation that is smaller than the smallest
representable number.
uniform random variable: the continuous random variable X ∈(α, β) with density
function f(x) =
1
β−α.
variance (of a random variable): a measure of dispersion of the random variable, equal
to the average square of the deviation of the random variable from its expected value.
variance reduction techniques: methods for obtaining greater precision for a ﬁxed
amount of sampling.
waiting-time process: a stochastic process describing the time spent in the system by
the customers.
7.1
FUNDAMENTAL CONCEPTS
Deﬁnitions:
An experiment is any physically or mentally conceivable undertaking that results in a
measurable outcome.
The sample space is the set Ωof all possible outcomes of an experiment.
The sample size of an experiment is the number of possible outcomes of the experiment.
An event in the sample space Ωis a subset of Ω.
For a family of events {Aj | j ∈J}, the union S
j∈J
Aj is the set of outcomes belonging to
at least one Aj; the intersection T
j∈J
Aj is the set of all outcomes belonging to every Aj.

Section 7.1
FUNDAMENTAL CONCEPTS
481
The complement A of an event A is the set of outcomes in the sample space not
belonging to A.
The events A and B are disjoint if A ∩B = ∅. The events A1, A2, A3, . . . are pairwise
disjoint if every pair Ai, Aj of distinct events is disjoint.
A probability measure on the sample space Ωis a function Pr from the set of subsets
of Ωinto the interval [0, 1] satisfying
• Pr(Ω) = 1;
• Pr(
∞
S
k=1
Ak) =
∞
P
k=1
Pr(Ak), if the events {Ak} are pairwise disjoint.
A fair (unbiased) coin is a coin that is just as likely to land Heads (H) as it is to land
Tails (T ).
A red/blue spinner is a disk consisting of two sectors, one red with area r and one
blue with area b.
Facts:
1. Pr(∅) = 0.
2. Pr(A) has the interpretation as the long-run proportion of time that the event A
occurs in repeated trials of the experiment.
3. Pr(
nS
k=1
Ak) =
nP
k=1
Pr(Ak), if the n events {Ak} are pairwise disjoint.
4. If all outcomes are equally likely and the sample space has k elements, where k is a
positive integer, then the probability of event A is the number of elements of A divided
by the size of the sample space; that is, Pr(A) = |A|
k .
5. Principle of inclusion-exclusion (simple form): For events A and B,
Pr(A ∪B) = Pr(A) + Pr(B) −Pr(A ∩B).
6. Principle of inclusion-exclusion (general form): For any events A1, A2, . . . , An,
Pr(
nS
r=1
Ar) = P
i
Pr(Ai) −
X
i<j
Pr(Ai ∩Aj) +
X
i<j<k
Pr(Ai ∩Aj ∩Ak) −
· · · + (−1)n+1 Pr(A1 ∩A2 ∩· · · ∩An).
7. Sieve principle: If A1, A2, . . . , An are events, then
Pr( exactly k of the Aj occur ) =
nP
r=k
(−1)r+k r
k

P
j1<j2<···<jr
Pr(Aj1 ∩Aj2 ∩· · · ∩Ajr).
8. Boole’s inequality: If A1, A2, . . . , An are events, then
Pr(
nS
k=1
Ak) ≤
nP
k=1
Pr(Ak).
If A1, A2, . . . is an inﬁnite sequence of events, then
Pr(
∞
S
k=1
Ak) ≤
∞
P
k=1
Pr(Ak).
(George Boole, 1815–1864.)

482
Chapter 7
DISCRETE PROBABILITY
9. Bonferroni’s inequality: If A1, A2, . . . , An are events, then
Pr(
nS
k=1
Ak) ≥
nP
k=1
Pr(Ak) −P
k<j
Pr(Ak ∩Aj).
10. Pr(A ) = 1 −Pr(A).
11. Monotonicity: If A ⊆B, then Pr(A) ≤Pr(B).
12. If A1 ⊆A2 ⊆A3 ⊆· · · is an increasing sequence of events, then
lim
n→∞Pr(An) = Pr(
∞
[
n=1
An).
13. If A1 ⊇A2 ⊇A3 ⊇· · · is a decreasing sequence of events, then
lim
n→∞Pr(An) = Pr(
∞
\
n=1
An).
Examples:
1. The following table gives examples of speciﬁc experiments, their sample spaces, and
the corresponding sample size.
experiment
sample space
sample size
toss a coin
{H, T }
2
toss a coin n times
{(ω1, . . . , ωn) | ωi is H or T }
2n
roll a die
{1, 2, 3, 4, 5, 6}
6
roll a pair of dice
{(1, 1), (1, 2), . . ., (6, 5), (6, 6)}
36
draw a card from a standard deck
{2♣, 2♦, . . ., A♥, A♠}
52
spin a red/blue spinner
{red, blue}
2
2. The following are various events deﬁned for the experiment of rolling a pair of dice
(see the table of Example 1).
sum of dice is 9: A = {(3, 6), (4, 5), (5, 4), (6, 3)}
both dice are multiples of 3: B = {(3, 3), (3, 6), (6, 3), (6, 6)}
sum of dice ≤4: C = {(1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (3, 1)}.
3. The events A = {sum of dice = 9} and C = {sum of dice ≤4} are disjoint. The
events Ai = {sum of the dice is i}, 2 ≤i ≤12, are pairwise disjoint.
4. Random selection of an integer: Let Ω= {1, 2, 3, . . ., n} be the sample space corre-
sponding to the experiment of randomly selecting an integer between 1 and n, and deﬁne
Pr(j) = Pr({j}) = 1
n. By Fact 4, Pr(3 ≤j ≤n) = Pr({3 ≤j ≤n}) = n−2
n .
5. For a red/blue spinner, the sample space is Ω= {red, blue}. If the spinner is equally
likely to land at any location, then Pr(red) =
r
r+b and Pr(blue) =
b
r+b.
6. Toss a fair coin n times and interpret Heads as 1 and Tails as 0. The sample space
Ω= {(ω1, ω2, . . . , ωn) | ωj ∈{0, 1}} consists of all possible 0 -1 sequences of length n.
Since |Ω| = 2n, each probability Pr((ω1, ω2, . . . , ωn)) is assigned the value
1
2n . By Fact 4,
Pr(A) = |A|
2n holds for all A ⊆Ω.
For example, the probability of no tails appearing in four coin tosses is the probability
of event A = {(1, 1, 1, 1)}, so Pr(A) =
1
16. The probability of exactly one tail is the
probability of event B = {(0, 1, 1, 1), (1, 0, 1, 1), (1, 1, 0, 1), (1, 1, 1, 0)}; hence Pr(B) =
4
16 = 1
4. The probability of at least two tails is, using Fact 10, 1 −Pr(A) −Pr(B) =
1 −5
16 = 11
16.

Section 7.2
INDEPENDENCE AND DEPENDENCE
483
7. Derangements:
Let Dn be the set of derangements (§2.4.2) on the n elements
{1, 2, . . ., n} and deﬁne Aj to be the set of all permutations ﬁxing j.
For any per-
mutation σ, Pr(σ) =
1
n!. Also, for j1 < j2 < · · · < jk, Pr(Aj1 ∩Aj2 ∩· · · ∩Ajk) = (n−k)!
n!
and P
j1<j2<···<jk Pr(Aj1 ∩Aj2 ∩· · · ∩Ajk) =
 n
k
 (n−k)!
n!
= 1
k!. By Fact 6, Pr(Sn
r=1 Ar) =
P
i Pr(Ai)−P
i<j Pr(Ai ∩Aj)+ P
i<j<k Pr(Ai ∩Aj ∩Ak)−· · ·+ (−1)n+1 Pr(A1 ∩A2 ∩
· · ·∩An) = 1−1
2! + 1
3! −· · ·+(−1)n+1 1
n!. Hence, Pr(Dn) = 1−1+ 1
2! −1
3! −· · ·+ (−1)n
n!
≈
e−1 ≈0.36788.
8. 5-card stud poker: Five cards are drawn from a well-shuﬄed deck of 52 playing cards.
The sample space consists of the
 52
5

= 2,598,960 possible ﬁve-card hands. The approx-
imate probabilities of various events are displayed in the following table. (See §2.3.2,
Example 12 for further details.) As seen from the probabilities given in the table, ob-
taining a ﬁve-card hand containing three of a kind is approximately ten times more likely
than obtaining a ﬁve-card hand containing a ﬂush, which in turn is approximately ten
times more likely than obtaining a ﬁve-card hand containing four of a kind.
type of hand
example
hand enumeration
probability
one pair
7♥, 7♦, K♣, J♠, 2♥
 13
1
 4
2
 12
3

43
0.42
two pairs
7♠, 7♥, K♦, K♠, 3♣
 13
2
 4
2
 4
2

44
0.048
three of a kind
7♣, 7♥, 7♦, 3♦, 5♠
 13
1
 4
3
 12
2

42
0.021
straight
7♣, 8♠, 9♦, 10♣, J♥
10(45 −4)
0.0039
ﬂush
3♦, 6♦, 7♦, J♦, K♦
4
  13
5

−10

0.0020
full house
3♥, 3♦, 3♠, 7♣, 7♥
13 · 12 ·
 4
3
 4
2

0.0014
four of a kind
A♣, A♦, A♥, A♠, 7♠
13 · 48
0.00024
straight ﬂush
7♦, 8♦, 9♦, 10♦, J♦
 4
1

9
0.000014
royal ﬂush
10♥, J♥, Q♥, K♥, A♥
4
0.0000015
7.2
INDEPENDENCE AND DEPENDENCE
Sequences of independent events are often encountered when an experiment is repeated
(without changes). Independent events correspond, intuitively, to events that do not af-
fect the outcome of one another. The treatment of dependent events requires conditional
probabilities.
7.2.1
BASIC CONCEPTS
Deﬁnitions:
Two events A and B are independent if Pr(A ∩B) = Pr(A) Pr(B).
The n events A1, A2, . . . , An are independent if for all k (2 ≤k ≤n) and j1, j2, . . . , jk
(1 ≤j1 < j2 < · · · < jk ≤n),
Pr(Aj1 ∩Aj2 ∩· · · ∩Ajk) = Pr(Aj1) Pr(Aj2) . . . Pr(Ajk).

484
Chapter 7
DISCRETE PROBABILITY
The inﬁnite collection of events {An | n ≥1} is independent if for all ﬁnite k ≥2 the
events A1, A2, . . . , Ak are independent.
Let B be an event with Pr(B) > 0. The conditional probability of A given B is
Pr(A|B) = Pr(A ∩B)
Pr(B)
.
Facts:
1. If events A and B are independent, then so are A and B, A and B, and A and B.
2. If A1, A2, . . . are independent events, then Pr(
∞
T
k=1
Ak) =
∞
Q
k=1
Pr(Ak).
3. The function φB : A →Pr(A|B) is a probability measure (§7.1).
4. Pr(A ∩B) = Pr(A|B) Pr(B).
5. If A and B are independent, then Pr(A|B) = Pr(A). This equation captures the
notion that for independent events A and B the knowledge that one of the events has
occurred does not aﬀect the probability of the other occurring.
6. Pairwise independence of a collection of events does not necessarily imply that all
events are independent (see Example 3).
7. Law of total probability:
For any event A and any partition of Ωinto the events
B1, B2, . . . , Bn,
Pr(A) =
nP
i=1
Pr(A ∩Bi) =
nP
i=1
Pr(A|Bi) Pr(Bi).
8. Bayes’ formula: For any event A and any partition of Ωinto the events B1, B2, . . . , Bn,
Pr(B1|A) = Pr(B1 ∩A)
Pr(A)
=
Pr(A|B1) Pr(B1)
nP
i=1
Pr(A|Bi) Pr(Bi)
.
(Thomas Bayes, 1702–1761.)
9. Chain rule: For any events A1, A2, . . . , An satisfying Pr(
n−1
T
k=1
Ak) > 0,
Pr(A1 ∩A2 ∩· · · ∩An) = Pr(A1) Pr(A2|A1) Pr(A3|A1 ∩A2) . . . Pr(An |
n−1
T
k=1
Ak).
Examples:
1. Tossing two fair coins:
The sample space for this experiment consists of the four
outcomes HH, HT, T H, T T . For example, the outcome HT means that the ﬁrst coin
turns up Heads and the second Tails. Because both coins are fair, all four outcomes are
equally likely and in particular Pr(HT ) = 1
4. Since Pr(H) = Pr(T ) = 1
2, Pr(HT ) =
1
4 = 1
2 · 1
2 = Pr(H) Pr(T ). Thus, the events “Heads on the ﬁrst coin” and “Tails on the
second coin” are independent.
2. Tossing a fair coin n times:
As in Example 6 of §7.1, let 1 stand for Heads and 0
for Tails.
For each 1 ≤i ≤n select ǫi ∈{0, 1} and deﬁne Ai = {ǫi occurs on the
ith toss}.
Since all outcomes are equally likely, Pr(A1 ∩A2 ∩· · · ∩An) = ( 1
2)n =
1
2 × 1
2 × · · · × 1
2 = Pr(A1) Pr(A2) . . . Pr(An). Also, for all j1, j2, . . . , jk (2 ≤k ≤n),
Pr(Aj1 ∩Aj2 ∩· · · ∩Ajk) = ( 1
2)k =
1
2 × 1
2 × · · · × 1
2 = Pr(Aj1) Pr(Aj2) . . . Pr(Ajk).
Therefore the events A1, A2, . . . , An are independent.

Section 7.2
INDEPENDENCE AND DEPENDENCE
485
3. Let Ω= {a, b, c, d} be a sample space with equiprobable outcomes. Let A = {a, b},
B = {a, c}, and C = {a, d}. Here Pr(A) = Pr(B) = Pr(C) = 1
2. Also Pr(A ∩B) = 1
4 =
1
2 · 1
2 = Pr(A) Pr(B), Pr(A ∩C) = 1
4 = 1
2 · 1
2 = Pr(A) Pr(C), and Pr(B ∩C) = 1
4 =
1
2 · 1
2 = Pr(B) Pr(C). Yet Pr(A ∩B ∩C) = 1
4 ̸= 1
2 · 1
2 · 1
2 = Pr(A) Pr(B) Pr(C). In this
example, any two of the events are independent, but all three are not.
4. Gambler’s fallacy:
Suppose that a fair coin is tossed ﬁve times, turning up Heads
on all ﬁve tosses. What is the probability that the next (sixth) toss turns up Tails? A
common fallacy is to believe that a Tail is more likely to turn up next, since in the long
run 50% of the coins should turn up Tails (and 50% Heads).
The appropriate sample space consists of 26 = 64 equiprobable outcomes, representing
any sequence of six Heads and/or Tails. The required probability is Pr(A|B), where
A = {(H, H, H, H, H, T )} and B = {(H, H, H, H, H, H), (H, H, H, H, H, T )}.
Then
Pr(A|B) =
Pr(A∩B)
Pr(B)
=
Pr(A)
Pr(B) =
1
2.
Consequently, a Tail turning up next is just as
likely as a Head.
5. An urn contains seven blue marbles and ﬁve red marbles. An experiment consists
of drawing (without replacement) a marble at random, observing its color, and then
drawing a second marble at random.
Let Bi be the event “the ith marble drawn is
blue” and let Ri be the event “the ith marble drawn is red”, where i ∈{1, 2}. Then
Pr(B1) =
7
12, Pr(R2|B1) =
5
11, Pr(B1 ∩R2) = Pr(R2|B1) Pr(B1) =
5
11 ·
7
12 =
35
132. By
Fact 7,
Pr(R2) = Pr(R2|R1) Pr(R1) + Pr(R2|B1) Pr(B1) =
4
11 · 5
12 + 5
11 · 7
12 =
55
132.
By Fact 8,
Pr(B1|R2) =
Pr(R2|B1) Pr(B1)
Pr(R2|R1) Pr(R1) + Pr(R2|B1) Pr(B1) =
5
11 · 7
12
4
11 · 5
12 +
5
11 · 7
12
=
7
11.
6. A particular family is known to have two children (one 9 years old, the other 10 years
old). When a census taker comes to the house, a girl answers the doorbell. What is the
probability that the other child is also a girl?
To answer this question, construct the sample space Ω= {(b, b), (b, g), (g, b), (g, g)},
where, for example, the ordered pair (b, g) means that the younger child is a boy and the
older child is a girl. Assume that all four outcomes in the sample space are equiprobable.
The required probability is Pr(A|B), where A = {(g, g)} and B = {(b, g), (g, b), (g, g)}.
Then
Pr(A|B) = Pr(A ∩B)
Pr(B)
=
Pr({(g, g)})
Pr({(b, g), (g, b), (g, g)}) =
1
4
3
4
= 1
3.
7. Genetics:
Genes are responsible for physical traits of all living things. Each gene
is composed of two alleles. Dominant alleles are represented with capital letters and
recessive alleles with lower case letters. The basic discoveries concerning genetics were
made by Gregor Mendel (1822–1884). One of the genes that is responsible for eye color
exhibits two alleles—a dominant one B, for brown eyes, and a recessive one b, for blue
eyes. In a certain population the genotype probabilities are
Pr(an individual has genotype BB) = 0.2
Pr(an individual has genotype Bb) = 0.5
Pr(an individual has genotype bb) = 0.3.

486
Chapter 7
DISCRETE PROBABILITY
Let Eb be the event that an oﬀspring receives a b allele from its mother, and let Fb be the
event that it receives a b allele from its father. Conditioning on the genotype (BB, Bb, bb)
of the oﬀspring produces
Pr(Eb) = Pr(Eb|BB) Pr(BB) + Pr(Eb|Bb) Pr(Bb) + Pr(Eb|bb) Pr(bb)
= 0 × 0.2 + 0.5 × 0.5 + 1 × 0.3 = 0.55;
similarly Pr(Fb) = 0.55.
Let C be the event that the oﬀspring has blue eyes (that is, has genotype bb).
By
independence,
Pr(C) = Pr(Eb ∩Fb) = Pr(Eb) Pr(Fb) = (0.55)2 = 0.3025.
8. In Example 7, let A be the event that the father has blue eyes (has genotype bb). If
the father has blue eyes, then the oﬀspring will have blue eyes (event C) if and only if
it receives a b allele from its mother, giving Pr(C|A) = Pr(Eb) = 0.55. We also have
Pr(C|father is Bb) = Pr(C and mother is Bb|father is Bb)+Pr(C and mother is bb| fa-
ther is Bb) = 0.25 × 0.5 + 0.5 × 0.3 = 0.275 and Pr(C|father is BB) = 0.
The conditional probability that the father has blue eyes if the oﬀspring has blue eyes is
obtained from Fact 8 (interchanging phenotype with genotype when convenient) as
Pr(A|C) =
Pr(C|father has bb) Pr(father has bb)
Pr(C|bb) Pr(bb) + Pr(C|Bb) Pr(Bb) + Pr(C|BB) Pr(BB)
=
0.55 × 0.3
0.55 × 0.3 + 0.275 × 0.5 + 0 ≈0.545.
9. Let’s Make a Deal: A game show contestant is told there is a fabulous prize hidden
behind one of three doors (A, B, or C). The contestant guesses that the prize is behind
door A. At this point the game show host (who knows what is behind each door, and in
particular knows that the prize is not behind door B) opens door B, revealing that the
prize is not there. The contestant is then oﬀered the opportunity to change her guess.
Should she? Intuition might suggest that nothing is to be gained by changing the guess
(the prize, it is argued, is now equally likely to be behind either door A or door C).
Using conditional probabilities, however, shows that it is deﬁnitely worthwhile to now
guess that the prize is behind door C, assuming that the host is known to always open
a door with no prize and to choose randomly if both remaining doors do not hide the
prize.
It is reasonable to assume that the prize is equally likely to be hidden behind each of the
doors. Thus, if HX denotes the event in which the prize is hidden behind door X, then
Pr(HA) = Pr(HB) = Pr(HC) = 1
3. If OX denotes the event that door X is opened,
then Pr(OB|HA) = Pr(OC|HA) = 1
2, whereas Pr(OB|HB) = 0 and Pr(OB|HC) = 1.
By Fact 8,
Pr(HA|OB) = Pr(HA ∩OB)
Pr(OB)
=
Pr(HA) Pr(OB|HA)
Pr(HA) Pr(OB|HA) + Pr(HB) Pr(OB|HB) + Pr(HC) Pr(OB|HC)
=
1
3 · 1
2
1
3 · 1
2 + 0 +
1
3 · 1 = 1
3.

Section 7.2
INDEPENDENCE AND DEPENDENCE
487
Similarly Pr(HC|OB) =
2
3, so it is twice as likely for the prize to be hidden behind
door C as behind door A, given that door B is shown to contain no prize. A web-based
simulation of this situation, in which prizes are randomly hidden behind doors, enables
one to verify experimentally this conclusion; see for example
• http://math.ucsd.edu/~crypto/Monty/monty.html
7.2.2
URN MODELS
Several applications can be viewed as the result of placing balls into urns.
Deﬁnitions:
In the following models, k balls are randomly placed in n distinguishable urns labeled 1
through n.
• Model 1 (Maxwell-Boltzmann):
The balls are distinguishable and multiple
occupancy is permitted.
• Model 2: The balls are distinguishable and multiple occupancy is not permitted.
• Model 3 (Fermi-Dirac):
The balls are indistinguishable and multiple occu-
pancy is not permitted.
• Model 4 (Bose-Einstein): The balls are indistinguishable and multiple occu-
pancy is permitted.
• Model 5: The balls are distinguishable, no urn is allowed to remain empty, and
multiple occupancy is permitted.
Facts:
1. The following table shows, for diﬀerent urn models, the probability of the event
(k1, k2, . . . , kn), in which k1 balls are in urn 1, k2 balls are in urn 2, . . . , kn balls are
in urn n, with the restrictions Pn
j=1 kj = k, kj ≥0. In Model 2, nk is a falling power
(see §3.4.2). In Models 2 and 3, every kj ∈{0, 1} and the models are meaningful only if
k ≤n. In Model 5, every kj ≥1, k ≥n, and
k
n
	
is a Stirling subset number (§2.5.2).
model
sample size
enumeration of (k1, . . . , kn)
probability of (k1, . . . , kn)
1
nk
 k
k1 k2 ... kn

 k
k1 k2 ... kn

n−k
2
nk
k!
 n
k
−1
3
 n
k

1
 n
k
−1
4
 n+k−1
k

1
 n+k−1
k
−1
5
n!
k
n
	
 k
k1 k2 ... kn

 k
k1 k2 ... kn
 n!
k
n
	
2. The Maxwell-Boltzmann model was originally proposed to explain the distribution
of k subatomic particles into n diﬀerent energy states.
It has been replaced by the
Bose-Einstein model (appropriate for particles with integer “spin”, such as photons and
pi mesons) and by the Fermi-Dirac model (appropriate for particles with half-integer
“spin”, such as protons and neutrons).
3. P´olya’s urn scheme (George P´olya, 1887–1985):
In this model, an urn contains b
black and r red balls. At each step one ball is drawn and replaced, and c additional balls
of the same color are placed in that urn. This scheme models the spread of a contagious
disease where an infected person infects c other persons.

488
Chapter 7
DISCRETE PROBABILITY
4. The case c = 0 in P´olya’s urn scheme corresponds to sampling balls with replacement.
5. The case c = −1 in P´olya’s urn scheme corresponds to sampling balls without re-
placement.
6. The following table shows how to calculate several types of probabilities using P´olya’s
urn scheme, where b is the number of black balls, r is the number of red balls, and c is
the number of additional balls added each time.
event
probability
drawing a black
b
b+r
drawing a black then red
br
(b+r)(b+r+c)
drawing in order black, red, black
br(b+c)
(b+r)(b+r+c)(b+r+2c)
drawing k black and n −k red balls
b(b+c)...(b+(k−1)c)r(r+c)...(r+(n−k−1)c)
(b+r)(b+r+c)(b+r+2c)...(b+r+(n−1)c)
in a prescribed order
drawing k black balls in n drawings;
(−b/c
k )(−r/c
n−k)
(
−(b+r)/c
n
)
the order of drawing does not matter
Examples:
1. Partial derivatives:
For analytic functions f, the order in which derivatives are
taken does not matter. As an example, the mixed second partial derivatives fxy and
fyx are equal, as are fxxy and fxyx. Consequently, the number of diﬀerent third-order
partial derivatives of a function of n variables is the number of ways to distribute k = 3
indistinguishable balls into n urns (variables). Each such distribution corresponds to
selecting the number of times each variable occurs in forming the partial derivative.
Using the entry for Model 4 in the table for Fact 1, there are
 n+2
3

third-order partial
derivatives of f. When n = 3 this gives
 5
3

= 10 diﬀerent third-order partial derivatives of
f(x, y, z): namely, fxxx, fyyy, fzzz, fxxy, fxxz, fxyy, fyyz, fxzz, fyzz, fxyz. In general, there
are
 n+k−1
k

diﬀerent kth-order partial derivatives of f.
2. Model 3 provides a model for the occurrence of misprints on the pages of a book.
Here the n urns correspond to the n symbols printed sequentially in the book and k is the
number of misprints. Each symbol is either correct or a misprint, so multiple occupancy
does not occur.
Also, assuming that the misprints are not generated in a systematic fashion, the k balls
can be considered indistinguishable, with misprints equally likely to occur at any location
on the page.
3. Lottery odds:
A lottery is conducted by selecting ﬁve diﬀerent numbers from
1, 2, . . ., 9. This can be viewed using urn Model 3, in which the ﬁve selected numbers
correspond to k = 5 identical balls placed into n = 9 distinguished urns. The number
of such selections, by the table of Fact 1, is
 9
5

= 126. Only one of these 126 selections
matches all the ﬁve winning numbers, so Pr(match 5) =
1
126.
To match exactly four of the ﬁve winning numbers, select the matching numbers in
 5
4

= 5 ways and select the (single) nonmatching number in
 4
1

= 4 ways, giving
Pr(match 4) = 5·4
126 =
20
126.
To match exactly three of the winning numbers, select the matching numbers in
 5
3

= 10
ways and select the two nonmatching numbers in
 4
2

= 6 ways, giving Pr(match 3) =
10·6
126 =
60
126.

Section 7.2
INDEPENDENCE AND DEPENDENCE
489
4. In a number of state lotteries, k = 6 numbers are drawn from 1, 2, . . ., n.
The
following table gives the probability of matching exactly six, exactly ﬁve, and exactly
four of the six winning numbers, for values of n = 35, . . ., 60.
n
match 6
match 5
match 4
35
1/1,623,160
87/811,580
87/23,188
36
1/1,947,792
15/162,316
2,175/649,264
37
1/2,324,784
31/387,464
2,325/774,928
38
1/2,760,681
64/920,227
2,480/920,227
39
1/3,262,623
66/1,087,541
2,640/1,087,541
40
1/3,838,380
17/319,865
561/255,892
41
1/4,496,388
35/749,398
2,975/1,498,796
42
1/5,245,786
108/2,622,893
675/374,699
43
1/6,096,454
111/3,048,227
4,995/3,048,227
44
1/7,059,052
57/1,764,763
10,545/7,059,052
45
1/8,145,060
39/1,357,510
741/543,004
46
1/9,366,819
80/3,122,273
3,900/3,122,273
47
1/10,737,573
82/3,579,191
4,100/3,579,191
48
1/12,271,512
21/1,022,626
4,305/4,090,504
49
1/13,983,816
43/2,330,636
645/665,896
50
1/15,890,700
22/1,324,225
473/529,690
51
1/18,009,460
27/1,800,946
1,485/1,800,946
52
1/20,358,520
69/5,089,630
3,105/4,071,704
53
1/22,957,480
141/11,478,740
3,243/4,591,496
54
1/25,827,165
32/2,869,685
376/573,937
55
1/28,989,675
98/9,663,225
392/644,215
56
1/32,468,436
25/2,705,703
875/1,546,116
57
1/36,288,252
17/2,016,014
2,125/4,032,028
58
1/40,475,358
52/6,745,893
1,105/2,248,631
59
1/45,057,474
53/7,509,579
3,445/7,509,579
60
1/50,063,860
81/12,515,965
4,293/10,012,772
5. Let an urn contain c = 1 red ball and b = 9 black balls. In P´olya’s urn scheme with
c = 1, the probability of obtaining the sequence RRB (two red balls and then a black
ball) is found using conditional probabilities as
Pr(RRB) = Pr(B|RR) Pr(R|R) Pr(R) =
9
12 · 2
11 · 1
10.
Likewise,
Pr(BRR) = Pr(R|BR) Pr(R|B) Pr(B) =
2
12 · 1
11 · 9
10,
and
Pr(RBR) = Pr(R|RB) Pr(B|R) Pr(R) =
2
12 · 9
11 · 1
10.
Thus,
Pr(RRB) = Pr(BRR) = Pr(RBR) =
3
220,

490
Chapter 7
DISCRETE PROBABILITY
agreeing with the value obtained using the table of Fact 6, with k = 1 and n = 3.
The probability of obtaining two red balls and one black ball in some order is then
Pr(RRB) + Pr(BRR) + Pr(RBR) =
9
220.
Using the extended binomial coeﬃcients
(§2.3.2), the corresponding entry in the table of Fact 6 can be veriﬁed for this case,
where k = 1 and n = 3:
Pr( exactly one black ball ) = (
−9
1 )(
−1
2 )
(
−10
3 )
=
(−1)1(
9
1)(−1)2(
2
2)
(−1)3(
12
3 )
=
9
220,
agreeing with the value already found.
7.3
RANDOM VARIABLES
7.3.1
DISTRIBUTIONS
Deﬁnitions:
A random variable X is a real-valued function on a probability space Ω.
The random variable X : Ω→R is discrete if the range of X is ﬁnite or countable.
The real-valued function f : R →R is a density function if
• f(x) ≥0 for all x ∈R;
•
R ∞
−∞f(x) dx = 1.
The random variable X is (absolutely) continuous if there exists a density function f
such that Pr(a < X < b) =
R b
a f(x) dx for all a < b.
The distribution µX of the random variable X is given by µX(B) = Pr(X ∈B) for
every interval B.
The cumulative distribution function of a random variable X is given by F(x) =
Pr(X ≤x).
A random vector is a function X = (X1, . . . , Xk): Ω→Rk.
The joint distribution µX1,...,Xk of the random vector (X1, . . . , Xk) is deﬁned by
µX1,...,Xk(B1, . . . , Bk) = Pr(X1 ∈B1, . . . , Xk ∈Bk) for any k intervals B1, . . . , Bk.
The random variables X1, . . . , Xn are independent if for any intervals B1, . . . , Bn
Pr(X1 ∈B1, . . . , Xn ∈Bn) = Pr(X1 ∈B1) . . . Pr(Xn ∈Bn).
Facts:
1. The cumulative distribution function F(x) is a nondecreasing function of x.
2. limx→∞F(x) = 1, limx→−∞F(x) = 0.
3. Pr(a < X ≤b) = F(b) −F(a) for a < b.
4. If X is a discrete random variable, then P
k Pr(X = k) = 1.
5. If X is a continuous random variable, then
d
dxF(x) = f(x).

Section 7.3
RANDOM VARIABLES
491
6. Some important discrete random variables are described in Table 1. Here q = 1 −p.
Table 1: Discrete random variables.
distribution
description of event (X = k)
range of X
Pr(X = k)
Bernoulli B(1, p)
k = 0 indicates a failure,
0, 1
q
k = 1 indicates a success
p
binomial B(n, p)
k successes in n trials, each with
0, 1, . . ., n
 n
k

pkqn−k
probability p of success
Poisson P(λ)
k arrivals to a counter over a unit
0, 1, 2, . . .
e−λλk
k!
period of time, at average rate λ
geometric G(p)
k trials before ﬁrst success occurs
1, 2, . . .
qk−1p
Pascal NB(r, p)
k trials before rth success occurs
r, r + 1, . . .
 k−1
r−1

qk−rpr
hypergeometric
sample n items from N items, where
0, 1, . . ., n
(m
k)(N−m
n−k )
(N
n)
(N, m, n)
m are defective and N −m are not;
k = number of defectives selected
7. Some important continuous random variables are described in Table 2. Here it is
understood that the density function f(x) = 0 outside the speciﬁed range.
8. If X1 and X2 are independent binomial random variables (see Table 1) with param-
eters n1, p and n2, p, respectively, then X1 + X2 is also a binomial random variable with
parameters n1 + n2, p.
9. If X1 and X2 are independent Poisson random variables (see Table 1) with parameters
λ1 and λ2, respectively, then X1 + X2 is also a Poisson random variable with parameter
λ1 + λ2.
10. If X1 and X2 are independent normal random variables (see Table 2) with parame-
ters µ1, σ2
1 and µ2, σ2
2, respectively, then X1 + X2 is also a normal random variable with
parameters µ1 + µ2, σ2
1 + σ2
2.
Table 2: Continuous random variables.
distribution
range of X
density function f(x)
uniform (α, β)
(α, β)
1
β−α, α < β
exponential (λ)
[0, ∞)
λe−λx, λ > 0
standard normal (0, 1)
(−∞, ∞)
1
√
2πe−x2/2
normal (µ, σ2)
(−∞, ∞)
1
σ
√
2πe−1
2 ( x−µ
σ
)2, σ > 0
gamma Γ(n, λ)
[0, ∞)
λnxn−1e−λx
Γ(n)
, Γ(n) =
R ∞
0
tn−1e−t dt
Cauchy (α)
(−∞, ∞)
α
π(α2+x2), α > 0
beta (p, q)
[0, 1]
Γ(p+q)
Γ(p)Γ(q)xp−1(1 −x)q−1, p, q > 0
chi square χ2(r)
[0, ∞)
1
2r/2Γ(r/2)x
r
2 −1e−x
2 , r > 0
F-distribution Fm,n
[0, ∞)
Γ((m+n)/2)
Γ(m/2)Γ(n/2)
  m
n
m/2
x(m−2)/2
(1+(m/n)x)(m+n)/2
t-distribution tk
(−∞, ∞)
Γ((k+1)/2)
Γ(k/2)
1
√
kπ
1
(1+x2/k)(k+1)/2
Rayleigh R(σ)
[0, ∞)
xe−x2/2σ2
σ2
, σ > 0

492
Chapter 7
DISCRETE PROBABILITY
Examples:
1. A spinner has three sectors—red, white, and blue—with sector areas 0.2, 0.7, and 0.1,
respectively. Deﬁne a random variable X according to the rule X = 1 if the spinner points
on red, X = 2 if it points on white, and X = 3 if it points on blue. The distribution of
the discrete random variable X is displayed in the following table.
event
i
Pr(X = i)
red
1
0.2
white
2
0.7
blue
3
0.1
2. Bernoulli random variable:
Let A ⊂Ωbe a ﬁxed subset, with Pr(A) = p for some
0 < p < 1. Deﬁne the random variable X by X(ω) = 1 for ω ∈A and X(ω) = 0 for ω /∈A.
Often it is said that a success occurs whenever ω ∈A and a failure occurs otherwise.
Then X is a Bernoulli random variable with Pr(X = 1) = p and Pr(X = 0) = 1 −p.
(Jakob Bernoulli, 1654–1705.)
3. Binomial random variable: Suppose that a die is thrown and that the occurrence of
either a one or a six results in a “success”. A single roll of the die constitutes a Bernoulli
trial (Example 2) with probability of success p =
1
3. The number of successes in 10
successive independent trials is a binomial random variable X with parameters n = 10
and p = 1
3. In general, the number of successes X is a discrete random variable with
possible values 0, 1, 2, . . ., n and its distribution is given in Table 1.
4. A dart is thrown at a circular target of radius 1. Assume that the target is never
missed and that any point on the target is as equally likely to be hit as any other
point. Let X be the dart’s distance from the center of the target. Since X can assume
any value between 0 and 1, X is a continuous random variable. For 0 ≤a < b ≤1,
Pr(a < X < b) = Pr(the dart lands in the annulus with radii a and b)= 1
π×(the area of
the annulus with radii a and b)= 1
π(πb2 −πa2) = b2 −a2.
5. Hypergeometric random variable:
A total of n balls are selected from an urn con-
taining N balls, of which m are red and N −m are black. Let X be the number of red
balls selected. Then X is a discrete random variable having the distribution
Pr(X = k) =
 m
k
 N−m
n−k

 N
n

, 0 ≤k ≤m.
6. Multinomial random variable: Cast n identical balls into N labeled boxes in such a
way that the probability that a ball ends up in box j is pj, where PN
j=1 pj = 1. Let Xj
denote the number of balls in box j (1 ≤j ≤N). For a vector (x1, x2, . . . , xN) with
PN
j=1 xj = n, the probability that box 1 contains x1 balls, box 2 contains x2 balls,. . . ,
box N contains xN balls is given by
Pr(X1 = x1, X2 = x2, . . . , XN = xN) =
n!
x1!x2! . . . xN!px1
1 px2
2 . . . pxN
N
=

n
x1 x2 . . . xN

px1
1 px2
2 . . . pxN
N ,
expressed using the multinomial coeﬃcients (§2.3.2).
7. Joint distribution:
Two fair coins are tossed once, resulting in four equally likely
outcomes {(T, T), (T, H), (H, T ), (H, H)}. Let the random variable X be the total num-
ber of heads observed, and let the random variable Y be the number of heads on the

Section 7.3
RANDOM VARIABLES
493
ﬁrst coin minus the number of heads on the second coin.
The joint probability dis-
tribution µX,Y is given by Pr(X = 0, Y = 0) = Pr(X = 1, Y = −1) = Pr(X =
1, Y
= 1) = Pr(X = 2, Y
= 0) =
1
4.
Thus Pr(X = 0) = Pr(X = 2) =
1
4,
Pr(X = 1) =
1
2 and Pr(Y
= 1) = Pr(Y
= −1) =
1
4, Pr(Y
= 0) =
1
2.
Since
Pr(X = 0, Y = 0) =
1
4 ̸=
1
8 = Pr(X = 0) Pr(Y = 0), the variables X and Y are
not independent.
7.3.2
MEAN, VARIANCE, AND HIGHER MOMENTS
Deﬁnitions:
The mean (expected value) EX of a discrete random variable X is given by EX =
P
k
k Pr(X = k).
The mean of a continuous random variable X with density function f is given by EX =
R ∞
−∞xf(x) dx.
The variance Var(X) of a random variable X is Var(X) = E((X −EX)2).
The standard deviation of X is
p
Var(X).
The covariance Cov(X, Y ) of two random variables X and Y is given by Cov(X, Y ) =
E((X −EX)(Y −EY )).
The correlation ρX,Y of two random variables X and Y is ρX,Y =
Cov(X, Y )
p
Var(X) Var(Y )
.
The kth moment of a random variable X is E(Xk).
Facts:
1. The expected value EX of a random variable X measures the “weighted average” of
X or the “center of gravity” of its distribution.
2. E(X + Y ) = EX + EY .
3. E(cX) = cEX for all constants c.
4. E(c) = c for all constants c.
5. If X is a nonnegative integer random variable, then EX =
∞
P
n=0
Pr(X > n).
6. If X is a nonnegative continuous random variable, then EX =
∞
R
0
Pr(X > x) dx.
7. If X and Y are independent, then E(XY ) = (EX)(EY ).
8. If g is a real-valued function and X is a discrete random variable, then E(g(X)) =
P
k
g(k) Pr(X = k).
9. If g is an integrable real-valued function and X is continuous with density f(x), then
E(g(X)) =
R
g(t)f(t) dt.
10. The variance Var(X) of a random variable X measures the “dispersion” of X about
its expected value EX.
11. Var(X) ≥0; Var(X) = 0 if and only if for some constant c, Pr(X = c) = 1.
12. Var(cX) = c2Var(X) for all constants c.

494
Chapter 7
DISCRETE PROBABILITY
13. Var(X + Y ) = Var(X) + Var(Y ) + 2Cov(X, Y ).
14. Var(X + Y ) = Var(X) + Var(Y ) if X and Y are independent.
15. Var(X) = E(X2) −(EX)2.
16. Cov(X, Y ) = 0 if X and Y are independent. The converse is false. (Example 4.)
17. Cov(X, Y ) = E(XY ) −(EX)(EY ).
18. The correlation ρX,Y is a scale-invariant measure of the degree of linear relationship
between two random variables X and Y . Speciﬁcally, ρX,Y = 1 only when Y = aX + b
for some constants a > 0 and b. Similarly, ρX,Y = −1 only when Y = aX + b for some
constants a < 0 and b.
19. |ρX,Y | ≤1.
20. Bienaym´e-Chebyshev’s inequality: Pr(|X−EX| ≥t) ≤Var(X)
t2
, for any value t > 0.
(Ir´en´ee-Jules Bienaym´e, 1796–1878 and Pafnuty Lvovich Chebyshev, 1821–1894.)
21. Kolmogorov’s inequality:
Suppose X1, X2, . . . , Xn are independent random vari-
ables, and let Sk = X1 + X2 + · · · + Xk for 1 ≤k ≤n. Then for any value t > 0
the probability that |Sk −ESk| < t holds for all k = 1, 2, . . ., n is at least 1 −Var(Sn)
t2
.
(Andrey Nikolayevich Kolmogorov, 1903–1987.)
Examples:
1. The random variable X is the number of heads obtained in three tosses of a fair
coin. It follows a binomial distribution (Table 1, §7.3.1), with n = 3 and p = 1
2. Thus
Pr(X = 0) = 1
8, Pr(X = 1) = 3
8, Pr(X = 2) = 3
8, and Pr(X = 3) = 1
8. Using the
deﬁnition of expected value, EX = P
k k Pr(X = k) = 0 · 1
8 + 1 · 3
8 + 2 · 3
8 + 3 · 1
8 = 3
2.
In general, the mean of a binomial distribution with parameters n and p is np; see the
corresponding entry in Table 3 of §7.3.3.
2. The variance of the discrete random variable X in Example 1 can be found using
Var(X) = E((X −EX)2) = E((X −3
2)2) = (0 −3
2)2 · 1
8 + (1 −3
2)2 · 3
8 + (2 −3
2)2 · 3
8 +
(3 −3
2)2 · 1
8· = 3
4. In general, the variance of a binomial distribution with parameters n
and p is np(1 −p); see the corresponding entry in Table 3 of §7.3.3.
3. Suppose X is a Bernoulli random variable with parameter p, so Pr(X = 0) = 1 −p
and Pr(X = 1) = p. Then EX = 0 ·(1 −p)+1 ·p = p and Var(X) = E((X −p)2) = (0 −
p)2·(1−p)+(1−p)2·p = p2(1−p)+p(1−p)2 = p(1−p). Also, E(X2) = 02·(1−p)+12·p = p
and using Fact 15 Var(X) = E(X2) −(EX)2 = p −p2 = p(1 −p), as before.
4. Covariance and independence: In Example 7 of §7.3.1, EX = 0 · 1
4 + 1 · 1
2 + 2 · 1
4 = 1
and EY = −1 · 1
4 + 0 · 1
2 + 1 · 1
4 = 0. Also, E(XY ) = −1 · 1
4 + 0 · 1
2 + 1 · 1
4 = 0. By
Fact 17, Cov(X, Y ) = E(XY ) −(EX)(EY ) = 0 −1 · 0 = 0. In this example, variables X
and Y have zero covariance (and zero correlation); however (see §7.3.1, Example 7) they
are not independent random variables.
5. The moments of the normal random variable X with parameters µ = 0 and σ = 1
are E(X2k) = 1 · 3 · · · (2k −1) and E(x2k−1) = 0 for k ≥1.
6. A manufacturing plant produces ball bearings with an average diameter of 50 mm
and a variance of 11 mm2. Without any further information about the shape of the
distribution of the diameters X, Fact 20 shows that the probability Pr(|X −50| ≥8) of
exceeding the nominal diameter by more than 8 mm is no more than Var(X)
82
= 11
64 = 0.172.
Thus, no more than 17.2% of the ball bearings produced can exceed the stated tolerance.
7. Average-case algorithm analysis:
A simple algorithm for locating an item in an
(unordered) list A = [a1, a2, . . . , an] is called a linear search; it sequentially examines
each entry of list A and compares the given item key with each ak until a match is

Section 7.3
RANDOM VARIABLES
495
found, or until the entire list is searched, in which case key is known not to be in
the list. To obtain the average case complexity of this algorithm, suppose that key is
known to occur in A and that it is equally likely (with probability 1
n) to be at each of
the n positions of A. If key is in fact located at position k of A, then k comparisons
are required by the algorithm.
The expected number of comparisons needed is thus
EX = Pn
k=1 k Pr(X = k) = Pn
k=1 k · 1
n = 1
n
Pn
k=1 k = 1
n
n(n+1)
2
= (n+1)
2
. Consequently,
the average-case complexity of linear search is O(n); see §1.3.3.
7.3.3
GENERATING FUNCTIONS
Deﬁnitions:
The probability generating function of a discrete random variable X is the function
φ(t) = E(tX) = P
k
tk Pr(X = k), deﬁned for |t| ≤1.
The moment generating function of a discrete random variable X is the function
ψ(t) = E(etX) = P
k
etk Pr(X = k), deﬁned for all t such that ψ(t) converges.
The moment generating function of a continuous random variable X with density f
is the function ψ(t) = E(etX) =
R
etxf(x) dx, deﬁned for all t such that ψ(t) converges.
The characteristic function (Fourier transform) of a discrete random variable X is
χ(t) = E(eitX) = P
k
eitk Pr(X = k), deﬁned for all t ∈R.
The characteristic function of a continuous random variable X with density f is
χ(t) = E(eitX) =
R
eitxf(x) dx, deﬁned for all t ∈R.
Facts:
1. The expected value of a random variable X can be expressed in terms of the ﬁrst
derivative of its generating function: EX = φ′(1) = ψ′(0) = −iχ′(0).
2. The variance of a random variable X can be expressed in terms of the ﬁrst and
second derivatives of its generating function: Var(X) = φ′′(1)+φ′(1)−[φ′(1)]2 = ψ′′(0)−
[ψ′(0)]2 = [χ′(0)]2 −χ′′(0).
3.
dk
dtk φ(1) = E(X(X −1)(X −2) . . . (X −k + 1)).
4.
dk
dtk ψ(0) = E(Xk).
5.
dk
dtk χ(0) = ikE(Xk).
6. For any of the three types of generating functions deﬁned, the generating function of
the sum of independent random variables is the product of their respective generating
functions.
Examples:
1. The binomial random variable with parameters n and p is the sum of n independent
Bernoulli random variables with parameter p. The probability generating function for a
Bernoulli random variable is φ(t) = E(tX) = t0(1 −p) + t1p = q + pt, where q = 1 −p.
By Fact 6 the probability generating function for a binomial random variable is [φ(t)]n =
(q + pt)n.
2. Table 3 shows the mean, variance, probability generating function, moment generat-
ing function, and characteristic function of several important discrete distributions. Here
q = 1 −p. An asterisk (∗) signiﬁes that the entry is not available in simple form.

496
Chapter 7
DISCRETE PROBABILITY
Table 3: Moments and generating functions for discrete distributions.
distribution
mean
variance
φ(t)
ψ(t)
χ(t)
Bernoulli B(1, p)
p
pq
q + pt
q + pet
q + peit
Binomial B(n, p)
np
npq
(q + pt)n
(q + pet)n
(q + peit)n
Poisson P(λ)
λ
λ
eλ(t−1)
eλ(et−1)
eλ(eit−1)
geometric G(p)
1
p
q
p2
pt
1−qt
pet
1−qet
peit
1−qeit
Pascal NB(r, p)
r
p
rq
p2
(
pt
1−qt)r
(
pet
1−qet )r
(
peit
1−qeit )r
hypergeometric
mn
N
m(N−m)n(N−n)
N 2(N−1)
∗
∗
∗
(N, m, n)
3. Table 4 shows the mean, variance, moment generating function, and characteristic
function of several important continuous distributions. An asterisk (∗) indicates that the
entry is not available.
Table 4: Moments and generating functions for continuous distributions.
distribution
mean
variance
φ(t)
χ(t)
uniform (α, β)
α+β
2
(β−α)2
12
eβt−eαt
t(β−α)
eiβt−eiαt
it(β−α)
exponential (λ)
1
λ
1
λ2
λ
λ−t
λ
λ−it
standard normal (0, 1)
0
1
et2/2
e−t2/2
normal (µ, σ2)
µ
σ2
eµt+ σ2t2
2
eiµt−σ2t2
2
gamma Γ(n, λ)
n
λ
n
λ2
(
λ
λ−t)n
(
λ
λ−it)n
Cauchy (α)
∞
∞
∞
1
α2 e−α|t|
beta (p, q)
p
p+q
pq
(p+q)2(p+q+1)
∗
∗
chi square χ2(r)
r
2r
(1 −2t)−r/2
(1 −2it)−r/2
F-distribution Fm,n
n
n−2
2n2(m+n−2)
m(n−2)2(n−4)
∗
∗
t-distribution tk
0
k
k−2
∗
∗
Rayleigh R(σ)
p π
2 σ
2(1 −π
4 )σ2
∗
∗
4. The moments of a binomial random variable X can be found from its moment gen-
erating function ψ(t) = (q + pet)n. For example, ψ′(t) = n(q + pet)n−1(pet) and using
Fact 1 produces EX = ψ′(0) = n(q + p)n−1p = np.
5. From Table 4 the moment generating function for the exponential distribution with
parameter λ is ψ(t) =
λ
λ−t. Then ψ′(t) =
λ
(λ−t)2 and ψ′′(t) =
2λ
(λ−t)3 , giving ψ′(0) =
λ
λ2 =
1
λ and ψ′′(0) =
2λ
λ3 =
2
λ2 . By Facts 1 and 2, EX = ψ′(0) =
1
λ and Var(X) =
ψ′′(0) −[ψ′(0)]2 =
2
λ2 −( 1
λ)2 =
1
λ2 .
7.4
DISCRETE PROBABILITY COMPUTATIONS
Many discrete probability computations are much less straightforward than may at ﬁrst
be imagined because of diﬃculties arising from the ﬁniteness of computer arithmetic
systems. Good algorithm design can usually avoid such problems.

Section 7.4
DISCRETE PROBABILITY COMPUTATIONS
497
7.4.1
INTEGER COMPUTATIONS
Enumeration of combinatorial objects, such as permutations and combinations, requires
the computation of integer factorials. In practice, these factorials can only be computed
as integers for small values. Since in most cases the factorials are not themselves the
primary objective of the computation, potential numerical diﬃculties can be overcome
by carefully designed recursive algorithms.
Deﬁnitions:
The N-bit binary representation of the positive integer n is (bN−1bN−2 . . . b1b0)2
where bi ∈{0, 1} and n = PN−1
i=0 bi2i. (See §4.1.3.) Each bi is a binary digit (bit).
The two’s complement representation of the signed integer n is (bN−1bN−2 . . . b1b0)2′
where n = −bN−12N−1 + PN−2
i=0 bi2i.
Integer wraparound is the phenomenon of adding 1 to the largest representable integer
and obtaining the smallest representable integer.
Facts:
1. Signed integers are usually represented in a computer as two’s complement binary
words of a ﬁxed wordlength; commonly 8, 16, or 32 bits are used.
2. A two’s complement integer using N-bit words is interpreted by treating the most
signiﬁcant bit as a coeﬃcient of 2N−1.
3. The range of representable integers in N-bit two’s complement is from −2N−1 =
(10 . . .00)2′ to 2N−1 −1 = (01 . . . 11)2′. Arithmetic operations can generate no carries
beyond this range.
4. Integer wraparound is a consequence of Fact 3 since (in regular binary arithmetic)
(01 . . .11)2 + (00 . . . 01)2 = (10 . . .00)2. Some systems have integer range checking avail-
able to avoid the eﬀect of wraparound.
5. Permutations and combinations (§2.3) are usually expressed in terms of integer fac-
torials.
6. Binomial coeﬃcients (§2.3.2) can be computed using integer arithmetic provided
the result is within the range being used.
Algorithm 1 breaks the computation of a
binomial coeﬃcient into a recursive loop using
 n
k

=
n!
k!(n−k)! =
n(n−1)...(n−k+1)
k(k−1)...1
=
  n
1
   n−1
2

. . .
  n+1−k
k

and the result
 n
k

=
  n
n−k

. (See §2.3.2, Fact 7.)
Algorithm 1:
Integer computation of binomial coeﬃcients.
input: positive integers n, k
output: b =
 n
k

if 2k > n then k := n −k
b := 1
for i := 1 to k
b := [b · (n + 1 −i)] div i
7. By doing the multiplication before the integer division in Algorithm 1, the numerator
necessarily has the appropriate factors to ensure an exact integer result.

498
Chapter 7
DISCRETE PROBABILITY
Examples:
1. For N = 8, 16, and 32 bits, the two’s complement binary integer ranges are given in
the following table.
N
minimum value
maximum value
8
−128
127
16
−32768
32767
32
−2147483648
2147483647
2. For N = 8, the integer 86 has the two’s complement representation (01010110)2′ and
the integer −86 has the two’s complement representation −128 + 42 = (10101010)2′.
3. For 8-bit integers, the eﬀect of integer wraparound is shown by 127 + 1 = −128.
Similarly, we would have 64 × 2 = −128.
4. For 8-bit two’s complement integers, only 1!, 2!, 3!, 4!, 5! can be computed correctly.
Subsequent factorials would generate integer answers—but wrong ones. In particular, 6!
would evaluate to −48. Namely, the 8-bit two’s complement representation of 5! = 120 is
01111000 and 6 = 00000110 so that, with no carries to the left of the 8th bit, 6! = 6 × 5!
is represented by the sum of 11100000 and 11110000 (which are respectively 01111000
shifted 2 and 1 places left.) This sum (again without carries to the left of the leading
bit) is 11010000, which represents (−128) + 64 + 16 = −48.
5. Using 16-bit integers with wraparound, the binomial coeﬃcient
 12
8

cannot be com-
puted directly from its deﬁnition since neither 12! nor 8! can be computed correctly.
Thus Algorithm 1 ﬁnds instead
 12
4

=
 12
8

since 2 × 8 > 12.
This is computed as
  12
1
   11
2
   10
3
   9
4

, with each multiplication being performed before its associated divi-
sion: (12/1) is multiplied by 11, divided by 2, multiplied by 10, divided by 3, multiplied
by 9, and divided by 4. This produces the intermediate results 12, 132, 66, 660, 220, 1980,
495, so that the correct ﬁnal result is obtained without any intermediate computation
exceeding the integer range.
7.4.2
FLOATING-POINT COMPUTATIONS
To compute discrete probabilities (e.g., binomial probabilities), careful attention must
be given to the underlying ﬂoating-point computation model and its properties.
Deﬁnitions:
Let F be the set of numbers representable in a particular ﬂoating-point system, with Ω
the largest positive number in F and ω the smallest positive number in F.
The ﬂoating-point arithmetic operations in F are denoted by ⊕, ⊖, ⊗, ⊘when it is nec-
essary to distinguish them from their real counterparts +, −, ×, /.
The number x is represented in the computer in binary ﬂoating-point form by the
approximation x ≈±f × 2E where the fraction or mantissa f is a binary fraction of
ﬁxed length and the exponent E is an integer within a ﬁxed range. Usually the ﬂoating
point representation is normalized so that f ∈[1, 2).
Floating-point arithmetic is subject to roundoﬀerror, the error introduced by abbre-
viating the representation of a number or an arithmetic result to a ﬁnite wordlength.

Section 7.4
DISCRETE PROBABILITY COMPUTATIONS
499
The usual measure of error for ﬂoating-point computation is relative error, which is
given for an approximation x∗to a quantity x by |x∗−x|
|x|
≈|x∗−x|
|x∗|
.
A ﬂoating-point operation (or ﬂop) is any arithmetic operation performed using
ﬂoating-point arithmetic.
Overﬂow results from a ﬂoating-point operation where the magnitude of the result is
too large for the available range of the ﬂoating-point system being used.
Underﬂow results from a ﬂoating-point operation where the magnitude of the result is
too small for the available range of the ﬂoating-point system being used.
The machine unit µ of a ﬂoating-point system is the smallest positive number that can
be added to 1 and produce a result recognized in the machine as greater than 1: namely,
µ = min{x ∈F | 1 ⊕x > 1}.
Facts:
1. Roundoﬀerrors are propagated in subsequent computations.
2. The two expressions given for relative error are often used interchangeably.
3. Overﬂow and underﬂow result from the ﬁnite range of available exponents. The limits
of these ranges and the details of the implementation vary with both the hardware and
software being used. See [IE08] for the most common implementations.
4. Usually an overﬂow condition terminates a program, while underﬂow results are
normally replaced by 0.
5. Because of the ﬁnite mantissa length (and independent of the rounding rule), most
axioms of the real number system fail for ﬂoating-point arithmetic [St74]. The follow-
ing table summarizes similarities and diﬀerences between the real numbers R and the
ﬂoating-point system F. The second column of the table describes the property, assuming
a, b, c ∈R. If the property fails in F, a brief reason for the failure is also given.
property
description in R
valid in F?
closure +
a + b ∈R
NO: overﬂow
closure ×
a × b ∈R
NO: overﬂow
commutativity
a + b = b + a, a × b = b × a
YES
associativity +
(a + b) + c = a + (b + c)
NO: a = 1, b = c = µ
2
special case
(a + b) −a = b
NO: a = 1, b = µ
2
associativity ×
(a × b) × c = a × (b × c)
NO: roundoﬀ, overﬂow,
or underﬂow
distributive law
a × (b + c) = (a × b) + (a × c)
NO: roundoﬀ, overﬂow,
or underﬂow
existence of zero
(∃0) a + 0 = a
YES
unique negative
∃! (−a) a + (−a) = 0
NO: [−(1 ⊕µ) ⊗a] ⊕a
= 0 if µ × a < ω
existence of one
(∃1) a × 1 = a
YES
zero divisors
a × b = 0 ⇒a = 0 or b = 0
NO: a ⊗b = 0 ⇒
a < √ω or b < √ω
total ordering
a < b or a = b or a > b
YES
order-preservation
a > b ⇒a + c > b + c
NO: roundoﬀ
special case
x > 0 ⇒1 + x > 1
NO: x < µ

500
Chapter 7
DISCRETE PROBABILITY
6. In the previous table, most of the properties that fail in F hold approximately—at
least for arguments of the same sign. These failures are not critical to most computations,
but they can be important for computations such as summing sets of numbers and
evaluating binomial probabilities.
7. The existence of the machine unit µ ensures that some of the order properties of R
will not carry over to F.
8. The machine unit µ is not the same as the smallest representable positive number ω
in F.
9. The relative error in subtraction is essentially unbounded due to cancellation.
10. IEEE arithmetic is required to deliver the same result as if rounding were performed
on the inﬁnite precision computation assuming that the data are exact.
11. A sum of terms of the same sign should generally be summed from smallest to
largest.
12. Improved accuracy in computing a summation is possible by regarding the partial
sums as members of a (reduced) list of summands and always adding the two smallest
terms in the current list. However, the overhead would be prohibitive in most cases.
13. Special care must be taken in computing a summation if its terms are computed
recursively, since the smallest term can underﬂow.
14. There is no completely reliable method for summing terms of mixed sign.
15. For alternating series, special transformations such as Euler’s method can be used
[BuTu92, Chapter 1].
16. Algorithm 2 computes the cumulative sum of binomial probabilities (§7.3.1) using
the deﬁnition B(N, p ; k) = Pk
i=0
 N
i

pi(1 −p)N−i.
Algorithm 2:
Recursive computation of binomial probabilities.
input: positive integers N, k; real number p
output: s = B(N, p ; k)
q := 1 −p
t := qN
s := t
for i := 1 to k
t := t ∗

p
q

∗(N+1−i)
i
s := s + t
17. Algorithm 2 will only work for small values of N.
18. If k is not too large, Algorithm 2 does in fact sum terms from smallest to largest.
19. To compute B(N, p ; k) for large values of k, use the fact that B(N, p ; k) = 1 −
B(N, 1 −p ; N −k −1) and then apply Algorithm 2.
20. If qN underﬂows to 0, then Algorithm 2 returns 0 for all values of k.
21. Algorithm 3 gives an alternative way to calculate the individual binomial probability
term
 N
r

pr(1 −p)N−r. It computes the logarithm of each factor recursively and then
exponentiates this at the end.

Section 7.5
RANDOM WALKS
501
Algorithm 3:
Logarithmic computation of binomial probability terms.
input: positive integers N, r; real number p
output: b =
 N
r

pr(1 −p)N−r
q := 1 −p
t := r ∗ln p + (N −r) ln q
for i := 1 to r
t := t + ln (N + 1 −i) −ln i
b := et
22. Algorithm 3 must be safeguarded to ensure that et underﬂows to 0 for large negative
arguments t.
23. Using logarithms is a frequently applied technique for computing products of many
factors with widely varying magnitudes. It is one step along the way toward using the
symmetric level-index scheme for number representation and arithmetic [ClOlTu89].
Examples:
1. Summations:
If the ﬁrst 224 terms of the harmonic series are summed using IEEE
single precision ﬂoating-point arithmetic, both forward and backward, then the sums
diﬀer by approximately 11%. Speciﬁcally,
 · · ·
  1 ⊕1
2

⊕1
3

⊕· · · ⊕2−24
≈15.40, while
summing the same terms from right-to-left yields 17.23.
2. Binomial probabilities:
The computation of binomial probabilities is thoroughly
discussed in Section 2.6 of [St74] with reference to the speciﬁc case where N = 2000, k =
200, and p = 0.1. Using Algorithm 2 in this case gives the initial value t = 0 and therefore
the ﬁnal result is s = 0. The true value of the ﬁnal probability is approximately 0.5.
3. If the ﬁnal term in Example 2 is computed in IEEE single precision, the binomial
coeﬃcient itself would overﬂow. It is certainly greater than 10200. Also, both (0.1)200
and (0.9)1800 would underﬂow. However the true value of this term is around 0.03.
7.5
RANDOM WALKS
Random walks are special stochastic processes whose applications include various models
of particle motion, covering topics as diverse as crystallography, gambling, stock mar-
kets, biology, genetics, astronomy, and statistical sampling of large graphs. This section
examines an important special class of random walks, sometimes referred to as lattice
random walks, whose trajectories are generated by the summation of independent and
identically distributed discrete random variables.
7.5.1
GENERAL CONCEPTS
Deﬁnitions:
A stochastic process is a collection of random variables, typically indexed by time
(discrete or continuous).

502
Chapter 7
DISCRETE PROBABILITY
A d-dimensional random walk is a stochastic process on the integer lattice Zd whose
trajectories are deﬁned by an initial position S0 = a and the sequence of sums Sn =
a + X1 + X2 + · · · + Xn, n ≥1, where the displacements X1, X2, . . . are independent and
identically distributed random variables on Zd.
A random walk is simple if the values Xi are restricted to the 2d points of Zd of unit
Euclidean distance from the origin. (That is, the random walk proceeds at each time step
to a point one unit from the current point along some coordinate axis.) A symmetric
random walk is a simple walk in which the 2d values of Xi have the same probability.
Random walks that return to the initial position with probability 1 are recurrent;
otherwise they are transient.
An absorbing boundary is a point or a set of points on the lattice that stops the
motion of a random walk whose trajectory comes into contact with it. A reﬂecting
boundary is a point or a set of points that redirects the motion of a random walk. Both
are special cases of an elastic boundary, which stops or redirects the motion depending
on some given probability.
The gambler’s ruin problem is a simple one-dimensional random walk with absorbing
boundaries at values 0 and b. It colorfully illustrates the fortunes of a gambler, who
starts with a dollars and who at each play of a game has a ﬁxed probability of winning
one dollar. The game ends once the gambler has either amassed the amount Sn = b or
goes broke Sn = 0.
For k ∈Zd, the ﬁrst passage time Tk into point k is the ﬁrst time at which the random
walk reaches the point k: namely, Tk = min{i ≥1 | Si = k}. More generally, the hitting
time TA for entering set A ⊆Zd is the ﬁrst time at which the random walk reaches some
point in set A: namely, TA = min{i ≥1 | Si ∈A}.
The following is the basic initial problem of random walks.
• For k ∈Zd, ﬁnd Pr(Sn = k), the probability that a “particle”, executing the ran-
dom walk and starting at point a at time 0, will be at point k at time n.
The following are ﬁrst passage time problems.
• Find the probability Pr(Tk = n) that, starting at point a at time 0, the ﬁrst visit
to point k occurs at time n.
• Find the probability Pr(TA = n) that, starting at point a at time 0, the ﬁrst visit
to A occurs at time n; characterize STA, the point at which A is ﬁrst visited.
Other classical problems in random walks include
• Range problem: Find or approximate the probability distribution and/or the
mean of the number of distinct points visited by a random walk up to time n.
• Occupancy problem: Find or approximate the probability distribution and/or the
mean of the number of times a given point or a set of points has been visited
up to time n.
• Boundary problem: Address all previous problems under absorbing, reﬂecting,
and/or elastic boundary conditions.

Section 7.5
RANDOM WALKS
503
Examples:
1. Coin tossing:
Tossing a coin n times can be viewed as a one-dimensional random
walk (d = 1) on the integers Z. This walk begins at the origin (a = 0) with Xi = 1 if
the result of the ith toss is a Head and Xi = −1 if the result of the ith toss is a Tail.
Since each step Xi is of unit length, this is a simple one-dimensional random walk. If
the tosses are independent events, then Pr(Xi = 1) = p and Pr(Xi = −1) = 1 −p holds
for all i, where 0 < p < 1. The random variable Sn is the cumulative number of Heads
minus the cumulative number of Tails in n tosses. The walk is symmetric if p = 1
2. A
return to the origin means that Sn = 0: that is, the number of Heads and Tails have
equalized after n tosses.
2. Gambler’s ruin:
A gambler repeatedly plays a game of chance, in which a dollar
is won at each turn with probability p and a dollar is lost with probability 1 −p. For
example, suppose the gambler starts with 90 dollars, and stops whenever his current
fortune is 0 (a ruin) or 100 (a positive net gain of 10 dollars). What is the gambler’s
ultimate probability of being ruined? Of success? On average how many expected plays
does it take for the game to be over? What is the expected net gain for the gambler?
If p = 0.5 the answers are 0.1, 0.9, 900, and 0 respectively. If p = 0.45 they are 0.866,
0.134, 765.6, and −76.6 respectively. (See §7.5.2, Example 4.)
7.5.2
ONE-DIMENSIONAL SIMPLE RANDOM WALKS
A number of results are known for random walks in one dimension that take a succession
of unit steps (in either the positive or negative direction).
Deﬁnitions:
The one-dimensional simple random walk (see §7.5.1) corresponds to a particle
moving randomly on the set Z of integers. It begins at the origin at time 0 and at each
time 1, 2, . . . thereafter, moves either one step up (right) with probability p, or one step
down (left) with probability 1 −p. This random walk is symmetric when p = 1
2.
The trajectory of a one-dimensional simple random walk is described by S0 = 0 and
Sn = X1 + X2 + · · · + Xn, n ≥1, where the Xi are independent and have a Bernoulli
distribution (§7.3.1), with Pr(Xi = 1) = p and Pr(Xi = −1) = q = 1 −p for p ∈(0, 1).
Suppose a trajectory is graphically represented by plotting Sn as a function of n, so that
the point (n, k) corresponds to Sn = k. Linking successive points with straight lines
produces a path between points. Deﬁne N(n, k) to be the number of paths from (0, 0)
to (n, k).
In a random walk starting at S0 = a > 0 with absorbing boundaries at 0 and b > a:
• qa is the probability that the random walk will be absorbed at 0;
• pa is the probability that the random walk will be absorbed at b;
• Da is the time until absorption.
Facts:
1. Reﬂection principle:
Let n2 > n1 ≥0, k1 > 0, k2 > 0.
The number of paths
from (n1, k1) to (n2, k2) that touch or cross the x-axis equals the number of paths from
(n1, −k1) to (n2, k2).
2. N(n, k) =
 n
(n+k)/2

, if n+k
2
is an integer in {0, 1, . . ., n}; N(n, k) = 0 otherwise.

504
Chapter 7
DISCRETE PROBABILITY
3. If n ≥1 is ﬁxed and −n ≤k ≤n, then Pr(Sn = k) = N(n, k)p
1
2 (n+k)q
1
2 (n−k).
4. Ballot theorem:
For k > 0, the number of paths from (0, 0) to (n, k) that do not
return to or cross the x-axis is k
nN(n, k).
5. For n ≥1, the ﬁrst return time T0 to the origin satisﬁes
• Pr(T0 > n) = E( |Sn|
n );
• Pr(T0 = 2n) =
1
2n−1
 2n
n

pnqn;
• Pr(T0 > 2n) = Pr(S2n = 0) =
 2n
n

2−2n, if the walk is symmetric.
6. Recurrent walks:
Pr(T0 < ∞) = 1 (the walk is recurrent) if and only if p = q = 1
2.
In this case E(T0) = ∞.
7. For k ̸= 0 and n > 0, Pr(Tk = n) = |k|
n Pr(Sn = k).
8. For k > 0 and n > 0, the maximum value Mn = max{S0, S1, . . . , Sn} satisﬁes
• Mn ≥k if and only if Tk ≤n;
• Pr(Mn ≥k) = Pr(Sn = k) +
P
i≥k+1
[1 + ( q
p)i−k] Pr(Sn = i);
• Pr(Mn = k) = Pr(Sn = k) + Pr(Sn = k + 1), if the walk is symmetric.
9. Arc sine laws: Let Wn be the number of times among {0, 1, . . ., n} at which a random
walk is positive and let Ln be the time of the last visit to 0 up to time n. For a symmetric
random walk,
• Pr(W2n = 2k) = Pr(L2n = 2k) = Pr(S2k = 0) Pr(S2n−2k = 0);
• as n →∞, Pr( W2n
2n ≤x) ≈2
π arcsin √x, for x ∈[0, 1].
10. Gambler’s ruin problem: In this random walk with absorbing boundaries (§7.5.1),
qa is the probability of the gambler (having an initial capital of a) being ruined and pa
is the probability of eventually winning (achieving a total of b). Facts 11–17 refer to the
gambler’s ruin problem.
11. If p ̸= q, qa = (q/p)b−(q/p)a
(q/p)b−1
and pa = 1 −qa.
12. If p = q = 1
2, qa = 1 −a
b and pa = a
b .
13. The expected gain in the gambler’s ruin problem is b(1 −qa) −a, which is 0 if and
only if p = q = 1
2.
14. Pr(Da = n) = b−12np(n−a)/2q(n+a)/2
b−1
P
k=1
cosn−1 πk
b sin πk
b sin πak
b .
15. If p ̸= q, E(Da) =
a
q−p −
b
q−p
1−(q/p)a
1−(q/p)b .
16. If p = q = 1
2, E(Da) = a(b −a).
17. Limiting case of the gambler’s ruin problem: When b = ∞,
• qa = ( q
p)a if p > q, and qa = 1 otherwise;
• Pr(Da = n) = 2np(n−a)/2q(n+a)/2 R 1
0 cosn−1 πx sin πx sin πax dx
= a
n
 n
(n+a)/2

p
1
2 (n−a)q
1
2 (n+a);
• if p < q, E(Da) =
a
q−p;
• if p = q = 1
2, E(Da) = ∞.
18. Random walks with one reﬂecting boundary: Consider a random walk starting at
S0 = a ≥0 with a reﬂecting boundary at 0.

Section 7.5
RANDOM WALKS
505
• The position at time n ≥1 is given by Sn = max{0, Sn−1 + Xn}.
• When p < q and as n →∞, there is a stationary distribution for the random
walk, coinciding with the distribution of M = supi≥0 Si, and given by Pr(M =
k) = (1 −p
q )( p
q )k for all k ≥0.
Examples:
1. A graphical representation of the trajectory for a one-dimensional simple random
walk is shown in the following ﬁgure. Here T0 = 2 and T2 = 4; M3 = 1 and M4 = 2;
W7 = 4; and L7 = 6.
2. The ballot theorem takes its name from the following problem. Suppose that, in a
ballot, candidate A scores x votes and candidate B scores y votes, x > y. What is the
probability that, during the ballot, A is always ahead of B? By Fact 4, the answer is x−y
x+y .
As an illustration, if
x
x+y = 0.52, this probability is 0.04.
3. How much time does a symmetric random walk spend to the left of the origin?
Contrary to intuition, with non-negligible probability, the fraction of time spent to the
left (or to the right) of the origin is near 0 or 1, but not near 1
2.
For example, when n is large, Fact 9 shows that the probability a symmetric random
walk spends at least 97.6% of the time to the left of the origin is approximately 0.1 =
2
π arcsin
√
0.024. Symmetrically, there is a 0.1 probability that it spends at least 97.6% of
the time to the right of the origin. Altogether, with probability 0.2 a symmetric random
walk spends at least 97.6% of the time entirely on one side of the origin.
4. Gambler’s ruin:
Suppose that the probability of winning one dollar is p = 0.45, so
q = 0.55. A gambler begins with an initial stake of a = 90 and will quit whenever the
current winnings reach b = 100. Using Fact 11, the probability of ruin (i.e., losing the
entire original stake) is
qa = (11/9)100−(11/9)90
(11/9)100−1
≈0.866.
The expected net gain is, by Fact 13, b(1 −qa) −a = 100(0.134) −90 = −76.6. The
average duration (number of plays of the game) is found from Fact 15 to be 765.6 plays.
Surprisingly, even though the probability p of winning is only slightly less than 0.5 and
the gambler starts within close reach of the desired goal, the gambler can expect to be
ruined with high probability and the average number of plays of the game is large.
5. The average duration of a fair game in the gambler’s ruin problem is considerably
longer than would be naively expected. When one player has one dollar and the adversary
1,000 dollars, Fact 16 shows that the average duration is 1,000 trials.
7.5.3
GENERALIZED RANDOM WALKS
Two generalizations of one-dimensional random walks are covered here.
In the ﬁrst
case, a one-dimensional walk is now allowed to be based on an arbitrary (as opposed to

506
Chapter 7
DISCRETE PROBABILITY
Bernoulli) distribution. In the second case, a symmetric random walk is considered in
higher dimensions.
Deﬁnitions:
In a one-dimensional random walk on Z starting at S0 = a > 0 with a given boundary
b > a, let ua be the probability that the particle will arrive at some position which is
less than or equal to 0 before reaching any position greater than or equal to b.
Let Rn be the number of distinct points visited by a random walk up to time n.
Facts:
1. If X1, X2, . . . are arbitrarily distributed independent random variables, many basic
qualitative laws are preserved for one-dimensional walks.
• In the case of two absorbing boundaries, the particle will reach one of them with
probability 1.
• In the case of a single absorbing boundary at 0, if E(Xi) ≤0 then the particle
will reach 0 with probability 1.
• An unrestricted walk with E(Xi) = 0 and Var(Xi) < ∞will return to its initial
position with probability 1, and the expected return time is inﬁnite.
2. General ruin problem:
Assume that at each step the particle has probability pk to
move from any point i to i + k, where k ∈Z. The particle starts from position a.
• ua = 1 if a ≤0, and ua = 0 if a ≥b.
• For 0 < a < b, ua = P
i∈Z
uipi−a. This corresponds to a system of b −1 linear
equations in b −1 unknowns that has a unique solution.
3. Local central limit theorem: For a d-dimensional symmetric (simple) random walk,
• | Pr(Sn = k) −2(
d
2πn)d/2e−d|k|2
2n | ≤O(n−(d+2)/2);
• | Pr(Sn = k) −2(
d
2πn)d/2e−d|k|2
2n | ≤|k|−2O(n−d/2).
4. P´olya’s theorem:
For the symmetric random walks in 1 or 2 dimensions, there is
probability 1 that the walk will eventually return to its initial position (recurrent random
walk). In dimension d ≥3 this probability is strictly less than 1. (George P´olya, 1887–
1985.)
5. For symmetric random walks in d = 3 dimensions, the probability of an eventual
return to the initial position is approximately 0.34 and the expected number of returns
is approximately 0.53. The following table gives the approximate return probabilities
Pr(T0 < ∞) for dimensions d ≤10.
d
Pr(T0 < ∞)
3
0.341
4
0.193
5
0.135
6
0.105
7
0.0858
8
0.0729
9
0.0634
10
0.0562

Section 7.5
RANDOM WALKS
507
6. Range problems: As n →∞,
• if d = 1, E(Rn) ≈(8n
π )1/2;
• if d = 2, E(Rn) ≈
πn
log n;
• if d ≥3, E(Rn) ≈cdn for some constant cd.
Example:
1. Absorbing boundaries:
A particle starts at position a ∈{0, 1, 2, 3, 4} on a line and
with equal probabilities moves one or two positions, either to the right or left. Upon
reaching a position x ≤0 or x ≥4, the particle is stopped. This is a form of the general
ruin problem with p−2 = p−1 = p1 = p2 = 1
4. We are interested in the probability ua of
absorption at position x ≤0, given that the particle starts at position a. Using Fact 2,
u0 = 1, u4 = 0 and u1, u2, u3 satisfy the following equations:
u1 = 1
2 + 1
4u2 + 1
4u3
u2 = 1
4 + 1
4u1 + 1
4u3
u3 = 1
4u1 + 1
4u2.
Solving this linear system produces the unique solution u1 =
7
10, u2 =
1
2, u3 =
3
10.
Intuitively, these are reasonable values since starting at the middle position a = 2 it
should be equally likely for the particle to be absorbed at either boundary; starting
at position a = 1 there should be a greater chance of absorption at the left boundary
(probability
7
10) than at the right boundary (probability 1 −7
10 =
3
10).
7.5.4
APPLICATIONS OF RANDOM WALKS
Random walk methodology is central to a number of diverse problem settings. This sec-
tion describes several important applications. Additional examples are found in [BaNi70],
[Be93], and [We94].
Examples:
1. Biological migration: The name “random walk” ﬁrst appears in a query sent by Karl
Pearson (1857–1936) to the journal Nature in 1905. Pearson’s problem refers to a walk
in the plane, with successive steps of length l1, l2, . . . at angles Θ1, Θ2, . . . with respect
to the x-axis, the Θi being chosen randomly. The problem is to ﬁnd, after some ﬁxed
time, the probability distribution of the distance from the initial position. The question
was motivated by a theory of biological migration which Pearson developed at that time,
but soon discarded. Nevertheless, Pearson’s random walk was born and it has since been
applied to many biological models.
2. Biology:
Other, more recent, examples of random walk applications include DNA
sequencing in genetics, bacterial migration in porous media, and molecular diﬀusion. In
the latter example, diﬀusion of molecules occurs as a result of the thermal energy of
the molecules. The motion of the molecules, perturbed through interactions with other
molecules, is then modeled as a random walk. See [Be93] for further details.
3. Physical sciences:
There are many applications in the physical sciences, including
the classical Scher-Montroll model of electrical transport in amorphous semiconductors (a
continuous-time random walk model), models of diﬀusion on tenuously connected struc-
tures such as percolation clusters, inference of molecular structure from data collected in

508
Chapter 7
DISCRETE PROBABILITY
x-ray scattering experiments, conﬁgurational statistics of polymers, and reaction kinetics
in conﬁned geometries. Details can be found in [BaNi70] and [We94].
4. Sequential sampling: A major application of random walks in statistics is in connec-
tion with Wald’s theory of sequential sampling. In this context, the Xi represent certain
characteristics of samples or observations. Measurements are taken as long as the ran-
dom walk remains within a given region. Termination with acceptance or rejection of
the appropriate hypothesis occurs depending on which part of the boundary is reached.
5. Stock prices: One of the early applications of computers in economics was to analyze
economic time series and, in particular, the behavior of stock market prices over time.
It ﬁrst came as a surprise when Kendall found in 1953 that he could not identify any
predictable patterns in stock prices. However, it soon became apparent that random
price movements indicated a well-functioning or eﬃcient market and that a random walk
could be used as a model for the underlying market. See [Ma90]. In fact, at the beginning
of this century, Bachelier had already developed a diﬀusion model for the stock market.
Other macroeconomic time series have also been modeled using random walks.
6. Astronomy: The problem of the mean motion of a planet in the presence of pertur-
bations due to other planets has a very long history (Lagrange). Statistical properties of
perturbed orbits can be analyzed with the help of Pearson’s random walk model. The
escape of comets from the solar system has also been modeled as a random walk among
energy states. Details are provided in [BaNi70].
7.6
SYSTEM RELIABILITY
System reliability involves the study of the overall performance of systems of intercon-
nected components. Examples of such systems are communication, transportation, and
electrical power distribution systems, as well as computer networks.
7.6.1
GENERAL CONCEPTS
Deﬁnitions:
Suppose a given system is composed of a set N = {1, 2, . . ., n} of failure-prone compo-
nents. At any instant of time, each component is found in one of two states: either
operating or failed.
The reliability of component i is the probability pi that component i is operating at
a given instant of time. The unreliability (or failure probability) of component i is
qi = 1 −pi.
At any instant of time, the system is found in one of two states: operating or failed.
The structure function φ is a binary-valued function deﬁned on all subsets S ⊆N.
Speciﬁcally, φ(S) = 1 if the system operates when all components in S operate and all
components of N −S fail; otherwise φ(S) = 0.
The structure function φ is monotone if S ⊆T ⇒φ(S) ≤φ(T ). In words, monotonicity
means that the addition of more operating components to an already functioning system
cannot result in system failure.

Section 7.6
SYSTEM RELIABILITY
509
The structure function φ is nontrivial if φ(∅) = 0 and φ(N) = 1.
A coherent system (N, φ) has a structure function φ that is monotone and nontrivial.
The dual of the system (N, φ) is the system (N, φD), deﬁned by φD(S) = 1 −φ(N −S).
The reliability RN,φ of the system (N, φ) is the probability that the system functions
at a random instant of time: RN,φ = Pr(φ(S) = 1).
The unreliability UN,φ of the system is given by UN,φ = Pr(φ(S) = 0) = 1 −RN,φ.
Facts:
1. System reliability depends not only on the reliability of its individual components,
but also on the manner in which they are interconnected.
2. If the state of any component is statistically independent of the state of any other
component, then the probability that S ⊆N is precisely the set of operating components
is given by prob(S) = Q
i∈S
pi
Q
j̸∈S
qj.
3. The dual of the dual of a system (N, φ) is the original system (N, φ).
Examples:
1. Consider a system built from the set of components N = {1, 2, 3, 4}. Associate with
each component a known weight: w1 = 5, w2 = 7, w3 = 4, w4 = 8. If S ⊆N is the set of
operating components, then the system is considered to operate if P
i∈S wi > 12. Thus
φ(S) = 1 for precisely the following sets S:
{1, 4}, {2, 4}, {1, 2, 3}, {1, 2, 4}, {1, 3, 4}, {2, 3, 4}, {1, 2, 3, 4}.
In all other cases φ(S) = 0. This structure function is nontrivial and monotone, so that
(N, φ) is a coherent system. The reliability of the system is then
RN,φ = p1q2q3p4 + q1p2q3p4 + p1p2p3q4 + p1p2q3p4 + p1q2p3p4 + q1p2p3p4 + p1p2p3p4.
The dual system (N, φD) has the structure function φD, where φD(T ) = 0 for precisely
the following component sets T :
∅, {1}, {2}, {3}, {4}, {1, 3}, {2, 3}.
In all other cases φD(T ) = 1. For example, φD({1, 3}) = 1 −φ({2, 4}) = 1 −1 = 0.
2. For critical ﬁnancial transactions, calculations are carried out simultaneously by three
separate microprocessors. The three results are compared and the result is accepted if
any two of the processors agree (or all three agree). Here the system has components
{1, 2, 3} corresponding to the three microprocessors. A component fails if it gives the
wrong answer, and the system fails if this “majority rule” produces an incorrect (or
inconclusive) answer. Thus, φ(S) = 1 if and only if S is {1, 2}, {1, 3}, {2, 3}, or {1, 2, 3}.
This structure function is nontrivial and monotone, so the system is coherent. If the
microprocessors are identical and each operates independently with probability p, then
Pr(exactly two components work) = 3p2(1 −p), Pr(all components work) = p3, and
RN,φ = 3p2(1 −p) + p3 = 3p2 −2p3.
For example, if p = 0.95 then RN,φ = 0.99275. In this case, even though any single
microprocessor has a 5% failure rate, the system as a whole has only a 0.7% failure rate.

510
Chapter 7
DISCRETE PROBABILITY
3. Telephone network:
The components of this system are individual communication
links (or trunk lines) joining nearby locations. Any telephone call placed between two
distant locations in this system needs to be routed along available communication links.
However, as a result of hardware or software malfunctions, or as a result of overloaded
circuits, certain links may be unavailable at a given instant of time. Thus, a telephone
network can be modeled as a system whose components are subject to failure at random
times. The reliability of the entire system is the probability that the system functions at
a random instant of time; that is, that at any random point in time users can successfully
complete their calls.
7.6.2
COHERENT SYSTEMS
It is assumed throughout this subsection that the system (N, φ) is coherent.
Deﬁnitions:
A minpath P is a minimal set of components such that φ(P) = 1: i.e., φ(P) = 1 and
φ(S) = 0 for all proper subsets S ⊂P. The collection of all minpaths for (N, φ) is
denoted P.
A mincut C is a minimal set of components such that φ(N −C) = 0: i.e., φ(N −C) = 0
and φ(N −S) = 1 for all proper subsets S ⊂C. The collection of all mincuts for (N, φ)
is denoted C.
Let G = (V, E) be an undirected graph with vertex set V and edge set E (§8.1.1).
• A simple path in G is a path that contains no repeated vertices.
• An s-t cutset of G is a minimal set of edges, the removal of which leaves no s-t
path in G.
• A cutset of G is a minimal set of edges, the removal of which disconnects G.
If K ⊆V , a K-tree is a minimal set F of edges in G such that every two vertices of K
are joined by a path in F.
If K ⊆V , a K-cutset is a minimal set of edges in G, the removal of which disconnects
some pair of vertices in K.
Facts:
1. The dual of a coherent system is itself coherent.
2. The structure function of a coherent system (N, φ) can be completely described using
its minpaths P or using its mincuts C. Speciﬁcally
• φ(S) = 1 if and only if S contains some minpath P;
• φ(S) = 0 if and only if N −S contains some mincut C.
3. The minpaths of (N, φ) are the mincuts of the dual (N, φD), and conversely.
4. The mincuts of (N, φ) are the minpaths of the dual (N, φD), and conversely.
5. Every minpath of (N, φ) and every mincut of (N, φ) have nonempty intersection.
6. If P is a minimal set of components that has nonempty intersection with every mincut
of (N, φ), then P is a minpath of (N, φ).
7. If C is a minimal set of components that has nonempty intersection with every
minpath of (N, φ), then C is a mincut of (N, φ).
8. A K-tree has the topology of a tree (§9.1.1) whose leaf vertices are in K.

Section 7.6
SYSTEM RELIABILITY
511
Examples:
1. Series system: This system (N, φ) operates only when all components of N operate.
See the following ﬁgure. General characteristics are listed in Table 1.
1
2
3
n
Table 1: Characteristics of series and parallel systems.
series
parallel
structure
φ(S) = 0, if S ⊂N
φ(S) = 1, if S ̸= ∅
function
φ(N) = 1
φ(∅) = 0
minpaths
P1 = N
P1 = {1}, P2 = {2}, . . . , Pn = {n}
mincuts
C1 = {1}, C2 = {2}, . . ., Cn = {n}
C1 = N
reliability
p1p2 . . . pn
1 −(1 −p1)(1 −p2) . . . (1 −pn)
unreliability
1 −p1p2 . . . pn
(1 −p1)(1 −p2) . . . (1 −pn)
dual
parallel, n components
series, n components
2. Parallel system:
This system (N, φ) fails only when all components of N fail. See
the following ﬁgure. General characteristics are listed in Table 1.
1
n
2
3
3. k-out-of-n success system:
This system (N, φ) operates only when at least k out of
the n components operate. The following ﬁgure illustrates a 2-out-of-3 success system.
General characteristics are listed in Table 2. The special case k = 1 gives a parallel
system; k = n gives a series system.
1
2
3
1
2
3
4. k-out-of-n failure system: This system (N, φ) fails only when at least k out of the n
components fail. This is the same as an (n −k + 1)-out-of-n success system. General
characteristics are listed in Table 2. The special case k = 1 gives a series system; k = n
gives a parallel system.
5. Two-terminal network:
Two vertices s, t of an undirected graph G = (V, E) are
speciﬁed and a message is to be sent from vertex s to vertex t. Assume that only the
edges are failure-prone, so N = E. The system operates when there exists some path of
operating edges joining s to t in the graph.
• structure function: φ(S) = 1 if there exists a path from s to t in the subgraph
deﬁned by edges S; φ(S) = 0 otherwise;

512
Chapter 7
DISCRETE PROBABILITY
• minpaths: all simple s-t paths of G;
• mincuts: all s-t cutsets of G;
• reliability: RN,φ = the probability that a message sent from s will arrive at t =
P{prob(S) | S contains some simple s-t path};
• unreliability: UN,φ = P{prob(N −S) | S contains some s-t cutset}.
Table 2: Characteristics of k-out-of-n systems.
k-out-of-n success
k-out-of-n failure
structure
φ(S) = 1 if |S| ≥k
φ(S) = 0 if |S| ≤n −k
function
φ(S) = 0 if |S| < k
φ(S) = 1 if |S| > n −k
minpaths
S ⊆N with |S| = k
S ⊆N with |S| = n −k + 1
mincuts
S ⊆N with |S| = n −k + 1
S ⊆N with |S| = k
reliability
P{prob(S) | |S| ≥k}
P{prob(N −S) | |S| < k}
unreliability
P{prob(S) | |S| < k}
P{prob(N −S) | |S| ≥k}
dual
k-out-of-n failure
k-out-of-n success
6. All-terminal network: A message is to be disseminated among all vertices V in the
undirected graph G = (V, E). The system operates when the operating edges in the
graph allow all vertices to mutually communicate.
• structure function: φ(S) = 1 if the subgraph deﬁned by vertices V and edges S
is connected; φ(S) = 0 otherwise;
• minpaths: all spanning trees (§9.2) of G;
• mincuts: all cutsets of G;
• reliability: RN,φ = probability that G is connected = P{prob(S) | S contains
some spanning tree of G};
• unreliability: UN,φ = P{prob(N −S) | S contains some cutset of G}.
7. All-terminal network, equal operating probabilities:
Suppose the undirected graph
G = (V, E) has n = |V | vertices and m = |E| edges, and suppose each edge has the same
operating probability p. Let q = 1 −p be the common edge failure probability. Then the
all-terminal reliability R(G, p) can be calculated as
R(G, p) = Pm
i=0 Fipm−iqi,
where Fi counts the number of operating system states in which exactly m −i edges are
functioning.
• 0 ≤Fi ≤
 m
i

for i = 0, 1, . . . , m;
• Fi = 0 for i > m −n + 1;
• Fi =
 m
i

for i < c, where c is the minimum cardinality of a cutset of G;
• Sperner’s bound: iFi ≤(m −i + 1)Fi−1.
8. K-terminal network:
A message is to be disseminated among a ﬁxed subset K of
vertices in the undirected graph G = (V, E). The system operates when the operating
edges of the graph allow all vertices in K to mutually communicate.
• structure function: φ(S) = 1 if the subgraph deﬁned by vertices K and edges S
is connected; φ(S) = 0 otherwise;

Section 7.6
SYSTEM RELIABILITY
513
• minpaths: all K-trees of G;
• mincuts: all K-cutsets of G;
• reliability: RN,φ = probability that K is connected = P{prob(S) | S contains
some K-tree of G};
• unreliability: UN,φ = P{prob(N −S) | S contains some K-cutset of G};
• special cases: K = {s, t} gives the two-terminal network problem; K = V gives
the all-terminal network problem.
9. Examples 5–8 are deﬁned in terms of undirected networks. The two-terminal, all-
terminal, and K-terminal reliability problems described in these examples can also be
deﬁned for directed networks.
10. Consider the coherent system (N, φ) on components N = {1, 2, 3, 4, 5} with min-
paths P1 = {1, 2}, P2 = {1, 5}, P3 = {3, 5}, P4 = {2, 3, 4}. To illustrate Fact 2, notice that
φ({3, 5}) = 1 and φ({2, 3, 5}) = 1 since P3 is a minpath. Also, φ({2, 5}) = 0 since {2, 5}
contains no minpath. By Fact 7, C1 = {1, 3} is a mincut for this system since C1 has
nonempty intersection with each of P1, P2, . . . , P4 and since neither {1} nor {3} has this
property. Likewise, C2 = {2, 5} and C3 = {1, 4, 5} are mincuts for this system. Fact 4
shows that the dual (N, φD) has as its minpaths the mincuts of (N, φ): namely, {1, 3},
{2, 5}, and {1, 4, 5}. This means φD({1, 3, 4}) = 1 since {1, 3, 4} contains the minpath
{1, 3}. Alternatively, from the deﬁnition φD({1, 3, 4}) = 1 −φ({2, 5}) = 1 −0 = 1.
7.6.3
CALCULATING SYSTEM RELIABILITY
Four general approaches can be used to calculate the reliability RN,φ of a coherent system
(N, φ). These are state-space enumeration, inclusion-exclusion, disjoint products, and
factoring.
Notation:
Let P = {P1, P2, . . . , Pm} be the minpaths and let C = {C1, C2, . . . , Cr} be the mincuts
of the coherent system (N, φ).
• Ei is the event that all components of minpath Pi operate (with no stipulation
as to the states of the other components);
• Fi is the event that all components of Ci fail (with no stipulation as to the states
of the other components);
• EiEj denotes the event that both Ei and Ej occur;
• (N, φ+i) is the system derived from (N, φ) in which component i always works;
• (N, φ−i) is the system derived from (N, φ) in which component i always fails.
Facts:
1. Calculation of the reliability RN,φ is in general quite diﬃcult; speciﬁcally, it is a
#P-complete problem (see §17.5.3).
2. RN,φ = P{prob(S) | φ(S) = 1}.
3. RN,φ = P{prob(S) | S contains some P ∈P}.
4. RN,φ = 1 −UN,φ = 1 −P{prob(N −S) | S contains some C ∈C}.
5. State-space enumeration:
System reliability can be found by enumerating all oper-
ating (or all failed) states of the system, using Facts 2–4.

514
Chapter 7
DISCRETE PROBABILITY
6. RN,φ = Pr(E1 ∪E2 ∪· · · ∪Em).
7. UN,φ = Pr(F1 ∪F2 ∪· · · ∪Fr).
8. Applying the inclusion-exclusion principle (§2.4) to Fact 6 produces
RN,φ = P
i Pr(Ei) −P
i<j Pr(EiEj) + · · · + (−1)m+1 Pr(E1E2 . . . Em).
9. Applying the inclusion-exclusion principle (§2.4) to Fact 7 produces
UN,φ = P
i Pr(Fi) −P
i<j Pr(FiFj) + · · · + (−1)r+1 Pr(F1F2 . . . Fr).
10. Inclusion-exclusion: This approach calculates system reliability using Facts 8–9.
11. RN,φ = Pr(E1) + Pr(E1E2) + · · · + Pr(E1E2 . . . Em−1Em).
12. UN,φ = Pr(F1) + Pr(F 1F2) + · · · + Pr(F 1F 2 . . . F r−1Fr).
13. Disjoint products: This approach calculates system reliability using the law of total
probability (§7.2.1). (See Facts 11–12.)
14. RN,φ = piRN,φ+i + (1 −pi)RN,φ−i.
15. Factoring:
Rather than requiring an enumeration of the minpaths or mincuts of
the system, this method (based on Fact 14) concentrates on the state of an individual
component i: it is either operating (with probability pi) or failed (with probability qi =
1 −pi).
16. The factoring method is applied most productively when the system (N, φ) has
additional structure. For example, this approach can be used to determine the reliability
of k-out-of-n systems and two-terminal networks. (See §7.6.4.)
Examples:
1. State-space enumeration: Consider the coherent system (N, φ) with N = {1, 2, 3, 4}
and minpaths P1 = {2, 3}, P2 = {1, 2, 4}, P3 = {1, 3, 4}. By Fact 2 of §7.6.2, the oper-
ating states of the system are {2, 3}, {1, 2, 3}, {1, 2, 4}, {1, 3, 4}, {2, 3, 4}, and {1, 2, 3, 4}.
By Fact 3, RN,φ = q1p2p3q4 + p1p2p3q4 + p1p2q3p4 + p1q2p3p4 + q1p2p3p4 + p1p2p3p4.
2. Inclusion-exclusion: Consider the coherent system on N = {1, 2, 3, 4} with minpaths
P1 = {1, 2}, P2 = {2, 4}, P3 = {1, 3, 4}. Event E1 has probability p1p2, event E1E2 has
probability p1p2p4, etc. Fact 8 gives
RN,φ = p1p2 + p2p4 + p1p3p4 −p1p2p4 −p1p2p3p4 −p1p2p3p4 + p1p2p3p4
= p1p2 + p2p4 + p1p3p4 −p1p2p4 −p1p2p3p4.
3. Disjoint products: Fact 11 is applied to the coherent system on N = {1, 2, 3, 4, 5, 6}
with minpaths P1 = {1, 5}, P2 = {1, 3, 6}, P3 = {2, 4, 5}, P4 = {2, 6}. For simplicity
of notation, let the event {e operates} be denoted by e, and let the event {e fails} be
denoted by ¯e. Identities of set theory (see §1.2.2) can then be used to obtain
Pr(E1) = p1p5;
Pr(E1E2) = Pr((¯1 ∪¯5)136) = Pr(¯5136) = p1p3q5p6;
Pr(E1E2E3) = Pr((¯1 ∪¯5)(¯1 ∪¯3 ∪¯6)245) = Pr(¯1(¯1 ∪¯3 ∪¯6)245)
= Pr(¯1245) = q1p2p4p5;
Pr(E1E2E3E4) = Pr((¯1 ∪¯5)(¯1 ∪¯3 ∪¯6)(¯2 ∪¯4 ∪¯5)26)
= Pr((¯1 ∪¯5)(¯1 ∪¯3)(¯4 ∪¯5)26)
= Pr((¯1 ∪¯3¯5)(¯4 ∪¯5)26)
= Pr((¯1¯4 ∪¯1¯5 ∪¯3¯5)26).

Section 7.6
SYSTEM RELIABILITY
515
Since the events ¯1¯4, ¯1¯5, and ¯3¯5 above are not disjoint, Fact 11 can be reapplied to this
new union of events, yielding
Pr(E1E2E3E4) = Pr((¯1¯4 ∪4¯1¯5 ∪1¯3¯5)26)
= Pr(¯1¯426 ∪4¯1¯526 ∪1¯3¯526)
= q1p2q4p6 + q1p2p4q5p6 + p1p2q3q5p6.
The ﬁnal expression for system reliability is then
RN,φ = p1p5 + p1p3q5p6 + q1p2p4p5 + q1p2q4p6 + q1p2p4q5p6 + p1p2q3q5p6.
7.6.4
SPECIALIZED ALGORITHMS FOR CALCULATING RELIABILITY
The general methods for calculating system reliability discussed in §7.6.3 can often be
streamlined when the system has a special structure. This section describes algorithms
for calculating the reliability of series-parallel, k-out-of-n, and certain network systems.
Deﬁnitions:
The two-terminal reliability of a network G is the probability Rst(G) that vertices s
and t are connected in G.
The all-terminal reliability of a network G is the probability RV (G) that G is con-
nected.
If G is a two-terminal network with speciﬁed vertices s and t, then an irrelevant edge
is one not appearing in any simple s-t path of G.
Let GS = (VS, ES) be the subgraph of a directed graph G = (V, E) induced by edges
S ⊆E. If GS is acyclic and has no irrelevant edges, the domination of GS is dS =
(−1)|ES|−|VS|+1; in all other cases, deﬁne dS = 0.
Deﬁne fk(n) to be the reliability of a k-out-of-n success system having the components
N = {1, 2, . . ., n} and corresponding reliabilities p1, p2, . . . , pn.
Facts:
1. A parallel system has reliability f1(n) = 1 −(1 −p1)(1 −p2) . . . (1 −pn).
2. A series system has reliability fn(n) = p1p2 . . . pn.
3. Series-parallel system: If a system is constructed from series and parallel subsystems
(with no component appearing in more than one subsystem), then the reliability of the
overall system is calculated by successively applying Facts 1 and 2.
4. Applying the factoring approach of §7.6.3 to a k-out-of-n success system gives fk(n) =
pnfk−1(n −1) + (1 −pn)fk(n −1).
5. k-out-of-n system: Repeated application of Facts 1, 2, and 4 produces the reliability
of any k-out-of-n success system. Since a k-out-of-n failure system is the same as an
(n −k + 1)-out-of-n success system, the reliability of any k-out-of-n failure system is
found in a similar way.
6. For a two-terminal undirected network G, the system (N, φ−e) corresponds to the
two-terminal network G −e with edge e deleted.
7. For a two-terminal undirected network G, the system (N, φ+e) corresponds to the
two-terminal network G/e in which edge e is contracted.

516
Chapter 7
DISCRETE PROBABILITY
8. Two-terminal undirected network: Algorithm 1 is a recursive procedure that calcu-
lates Rst(G). Based on the factoring approach of §7.6.3, it splits the initial reliability
calculation for G into calculations for the smaller networks G −e and G/e.
Algorithm 1:
Two-terminal reliability for undirected networks.
procedure Rst(G)
perform series and parallel reductions on G, producing network H
if H consists of the single edge (s, t) then return the reliability of (s, t)
else
select an edge e from H
let pe be the reliability of e in H
return pe Rst(H/e) + (1 −pe) Rst(H −e)
9. For the sake of eﬃciency, Algorithm 1 carries out any applicable series and parallel
reductions before selecting an edge on which to factor.
10. To avoid redundant calculations in Algorithm 1, edge e should be chosen from H
so that H/e and H −e do not contain irrelevant edges.
11. If G is a two-terminal directed network, then Rst(G) = P
S dS
Q
i∈S pi.
12. The expression in Fact 11 is obtained by using the inclusion-exclusion expansion
(§7.6.3, Fact 8) applied to the simple s-t paths of G. Remarkably, a number of terms in
this expansion cancel one another and the remaining coeﬃcients are either +1 or −1.
13. In an undirected network G = (V, E), the minpaths for the all-terminal problem are
the spanning trees of G.
14. All-terminal reliability: Algorithm 2 calculates RV (G) using the disjoint-products
expansion (§7.6.3, Fact 11), applied to the spanning trees in lexicographic (dictionary)
order. Here each term of the expansion reduces to a single product involving pi and
qi = 1 −pi.
Algorithm 2:
All-terminal reliability for undirected networks.
input: undirected network G
output: RV (G)
let T1, T2, . . . , Tm be the spanning trees of G, listed in lexicographic order
for k := 1 to m
Sk := Tk
Fk := ∅
for j := 1 to k −1
Fk := Fk ∪min{r | r ∈Tj −Tk}
gk := Q
i∈Sk pi
Q
i∈Fk qi
RV (G) := Pm
k=1 gk
Examples:
1. A system on four components is built up from series and parallel subsystems. Subsys-
tem A has components 1 and 2 in series and subsystem B consists of component 3. Sub-
system C has these two subsystems in parallel and its reliability is 1−(1−p1p2)(1−p3) =

Section 7.6
SYSTEM RELIABILITY
517
p1p2 + p3 −p1p2p3. The entire system is constructed from subsystem D (component
4 alone) in series with subsystem C, so it has reliability p4(p1p2 + p3 −p1p2p3) =
p1p2p4 + p3p4 −p1p2p3p4.
2. To calculate the reliability of a 2-out-of-3 success system with components 1, 2, 3,
Facts 1 and 2 are ﬁrst used to obtain
f1(2) = 1 −(1 −p1)(1 −p2) = p1 + p2 −p1p2,
f2(2) = p1p2.
Fact 4 then gives the system reliability
RN,φ = f2(3) = p3f1(2) + (1 −p3)f2(2) = p1p2 + p1p3 + p2p3 −2p1p2p3.
3. The two-terminal bridge network G is shown in the following ﬁgure, with s = a and
t = d.
1
a
b
c
d
2
3
4
5
No series or parallel reductions can be performed on G, so factoring with respect to edge
e = 3 produces the networks G1 = G/e and G2 = G −e shown in the next ﬁgure.
1
a
d
2
G1
4
5
1
a
b
c
d
2
4
5
G2
Since both G1 and G2 are series-parallel networks,
Rst(G1) = [1 −(1 −p1)(1 −p2)][1 −(1 −p4)(1 −p5)]
= (p1 + p2 −p1p2)(p4 + p5 −p4p5),
Rst(G2) = [1 −(1 −p1p4)(1 −p2p5)] = p1p4 + p2p5 −p1p2p4p5.
Algorithm 1 then produces
Rst(G) = p3(p1 + p2 −p1p2)(p4 + p5 −p4p5) + (1 −p3)(p1p4 + p2p5 −p1p2p4p5)
= p1p4 + p2p5 + p1p3p5 + p2p3p4 −p1p3p4p5 −p2p3p4p5 −p1p2p3p4
−p1p2p3p5 −p1p2p4p5 + 2p1p2p3p4p5.
4. Consider a directed version of the two-terminal network in the ﬁgure of Example 3,
in which there are oppositely directed edges 3 = (b, c) and 6 = (c, b). Edges 1 and 2
are directed out of s = a, while edges 4 and 5 are directed into t = d.
The cyclic
subgraph deﬁned by edges S = {1, 2, 3, 4, 5, 6} has dS = 0. Also the subgraph deﬁned by
S = {1, 2, 3, 4} has the irrelevant edges 2 and 3, so that dS = 0. On the other hand, S =
{1, 2, 3, 5} deﬁnes an acyclic network without irrelevant edges, giving dS = (−1)4−4+1 =
−1 and the term −p1p2p3p5. Similarly, S = {1, 4} produces dS = (−1)2−3+1 = +1 and
the term +p1p4.

518
Chapter 7
DISCRETE PROBABILITY
After generating all acyclic networks without irrelevant edges, Fact 11 is applied to obtain
Rst(G) = p1p4 + p2p5 + p1p3p5 + p2p4p6 −p1p2p4p5 −p1p3p4p5
−p1p2p4p6 −p1p2p3p5 −p2p4p5p6 + p1p2p3p4p5 + p1p2p4p5p6.
5. The bridge network G in Example 3 has eight spanning trees, given in lexicographic
order by T1 = {1, 2, 4}, T2 = {1, 2, 5}, T3 = {1, 3, 4}, T4 = {1, 3, 5}, T5 = {1, 4, 5},
T6 = {2, 3, 4}, T7 = {2, 3, 5}, T8 = {2, 4, 5}. Applying Algorithm 2 gives
S1 = {1, 2, 4},
F1 = ∅,
g1 = p1p2p4
S2 = {1, 2, 5},
F2 = {4},
g2 = p1p2q4p5
S3 = {1, 3, 4},
F3 = {2},
g3 = p1q2p3p4
S4 = {1, 3, 5},
F4 = {2, 4},
g4 = p1q2p3q4p5
...
...
...
S8 = {2, 4, 5},
F8 = {1, 3},
g8 = q1p2q3p4p5
Summing these eight terms then yields
RV (G) = p1p2p4 + p1p2q4p5 + p1q2p3p4 + p1q2p3q4p5 + · · · + q1p2q3p4p5.
6. The all-terminal reliability of the bridge network G given in Example 3 can also be
calculated by determining the coeﬃcients Fi discussed in Example 7 of §7.6.2. Here we
assume that all edges operate with probability p and fail with probability q = 1 −p.
Notice that c = 2 is the minimum cardinality of a cutset in G. Therefore, F0 =
 5
0

= 1
and F1 =
 5
1

= 5. At the other extreme, Fi = 0 for i > m −n + 1 = 2. The only
remaining coeﬃcient is F2, obtained by noting that G remains connected whenever any
three of the ﬁve edges operate, except for {1, 2, 3} and {3, 4, 5}, so that F2 =
 5
2

−2 = 8.
This gives R(G, p) = p5 + 5p4q + 8p3q2.
7.7
DISCRETE-TIME MARKOV CHAINS
Many physical systems evolve randomly in time, e.g., the population of a country, the
value of a company’s stock, the number of customers waiting at a checkout counter,
and the functional state of a machine subject to failures and repairs. A discrete-time
Markov chain can be used to model such situations when the set of possible states of
the system is ﬁnite (or countable) and the system changes state at discrete time points.
Such Markov chain models ﬁnd applications in diverse ﬁelds, such as biology, computer
science, inventory, production, queueing systems, and demography. In addition, many
recursive algorithms can be viewed as a manifestation of an underlying discrete-time
Markov chain.
7.7.1
MARKOV CHAINS
Deﬁnitions:
A sequence of random variables {Xn | n ≥0} is a (discrete-time) Markov chain
(DTMC) on a (countable) state space S if Xn ∈S for all n ≥0 and Xn+1 depends

Section 7.7
DISCRETE-TIME MARKOV CHAINS
519
(probabilistically) on the previous states of the system only via Xn:
Pr(Xn+1 = j | Xn = i, Xn−1 = in−1, ..., X0 = i0) = Pr(Xn+1 = j | Xn = i),
for all i0, i1, ..., in−1, i, j ∈S.
A Markov chain {Xn | n ≥0} is time-homogeneous if
Pr(Xn+1 = j | Xn = i) = pij, for all n ≥0.
Note:
Only time-homogeneous discrete-time Markov chains will be considered in this
and later sections.
The matrix P = (pij) is the (one-step) transition probability matrix of the discrete-
time Markov chain.
The initial distribution for a DTMC is the vector a = (ai), where ai = Pr(X0 = i) for
i ∈S.
The transition diagram of a DTMC is the directed graph (§8.3.1) G = (V, E), where
V = S is the state space and E = {(i, j) ∈S × S | pij > 0}.
A stochastic matrix M = (mij) has mij ≥0 for all i, j and P
j mij = 1 for all i.
Facts:
1. The ﬁrst systematic study of Markov chains was carried out by Andrei Andreevich
Markov (1856–1922); this work initiated the study of stochastic processes (sequences of
random variables).
2. A DTMC on state space S is completely described by the initial distribution a and
the transition probability matrix P.
3. The transition probability matrix P is a stochastic matrix.
Examples:
1. Consider a DTMC on the set S = {1, 2, 3, 4, 5, 6} with the following transition prob-
ability matrix
P =









0.4
0.6
0
0
0
0
0.7
0.3
0
0
0
0
0
0
0
1
0
0
0
0
1
0
0
0
0
0
0
0
1
0
0.1
0.1
0.1
0.1
0.1
0.5









To completely describe this DTMC, it is also necessary to specify the initial distribution.
For example, a = (0, 0, 0, 0, 0, 1) means that the system starts oﬀin state 6.
2. Simple random walk with absorbing states: This is a DTMC on S = {0, 1, 2, ..., N}
with transition probabilities pi,i+1 = p, pi,i−1 = 1 −p = q, 1 ≤i ≤N −1, where
0 ≤p ≤1 is a given number. Also, p0,0 = pN,N = 1, meaning that states 0 and N are
absorbing: once the DTMC visits these states it cannot leave them. This Markov model
is also (more colorfully) known as the gambler’s ruin problem (§7.5.1, Example 2). The
transition diagram of this DTMC is given in the following ﬁgure.
3. Simple random walk with reﬂecting states: This is a variant of Example 2, in which
the boundary states 0 and N are reﬂecting: namely, p0,1 = pN,N−1 = 1. The transition
diagram of this DTMC is shown next.

520
Chapter 7
DISCRETE PROBABILITY
4. Weather:
A simpliﬁed model of the daily weather results in a DTMC. Suppose
that each day is either sunny (0) or rainy (1) and that tomorrow’s weather depends
only on today’s weather. Speciﬁcally, suppose that a rainy day follows a sunny day with
probability 0.3 and a sunny day follows a rainy day with probability 0.4. This is a DTMC
with state space S = {0, 1} and transition probability matrix
P =
 
0.7
0.3
0.4
0.6
!
.
5. Urns:
Urn B contains nine black and one white ball, while Urn R contains six red
and four white balls. Balls are successively drawn with replacement from an urn. If the
ball drawn is colored, the drawing continues from the same urn. If the ball drawn is
white, the drawing continues from the other urn. Deﬁne the state of the system to be
the urn being sampled, so S = {B, R}. This is a DTMC with transition probabilities
pBB = 0.9, pBR = 0.1, pRR = 0.6, pRB = 0.4.
6. Ehrenfest diﬀusion model: Suppose that there are M molecules in a vessel, separated
into two chambers by a membrane, across which molecules can pass. A state of the system
at any instant is given by (k1, k2), where there are k1 molecules in the ﬁrst chamber and
k2 = M −k1 in the second chamber. Transitions from the current state (k1, k2) occur by
the movement of a single molecule from the ﬁrst chamber to the second, resulting in state
(k1 −1, k2 +1), or from the second chamber to the ﬁrst, resulting in state (k1 +1, k2 −1).
In the Ehrenfest model of this process, the probability of transition from (k1, k2) to
(k1 −1, k2 + 1) is given by k1
M , whereas the probability of transition to (k1 + 1, k2 −1) is
k2
M = 1−k1
M . This quantiﬁes the idea that if more molecules are present in (say) chamber
1, then it is more likely for some molecule to transfer next from chamber 1 to chamber 2.
This is a DTMC with state space S = {(0, M), (1, M −1), . . . , (M, 0)} and the transition
probabilities speciﬁed.
7.7.2
TRANSIENT ANALYSIS
Transient analysis of a DTMC involves the computation of Pr(Xn = j), the probability
of the Markov chain being in state j after n steps.
Deﬁnitions:
For i, j ∈S the n-step transition probability p(n)
ij
is the probability of being in state j
after n ≥0 steps, if the Markov chain starts in state i: p(n)
ij
= Pr(Xn = j | X0 = i).
The n-step transition probability matrix is given by P (n) = (p(n)
ij ).

Section 7.7
DISCRETE-TIME MARKOV CHAINS
521
Facts:
1. Pr(Xn = j) = P
i∈S
Pr(X0 = i) Pr(Xn = j | X0 = i) = P
i∈S
aip(n)
ij .
2. Chapman-Kolmogorov equations: P (n+m) = P (n)P (m) = P (m)P (n) for all m, n ≥0.
3. If P n denotes the nth power of P, then P (n) = P n, n ≥0.
4. If a is the initial distribution of a DTMC, the (absolute) probabilities Pr(Xn = j)
are the entries of the vector aP n.
Examples:
1. For Example 4 of §7.7.1, the two-step transition probability matrix is
P (2) = P 2 =
 
0.7
0.3
0.4
0.6
!  
0.7
0.3
0.4
0.6
!
=
 
0.61
0.39
0.52
0.48
!
.
Note that P (2) is again a stochastic matrix. To illustrate, if Friday is sunny then the
conditional probability that the following Sunday is sunny is given by p(2)
00 = 0.61.
2. A general two-state DTMC on S = {0, 1} can be represented by the stochastic
transition probability matrix
P =
 
1 −p
p
q
1 −q
!
.
Direct calculation gives the two-step transition probability matrix
P (2) =
 
(1 −p)2 + pq
p(2 −p −q)
q(2 −p −q)
(1 −q)2 + pq
!
,
which can be rewritten as
P (2) =
1
p + q
 
q + p(1 −p −q)2
p −p(1 −p −q)2
q −q(1 −p −q)2
p + q(1 −p −q)2
!
.
In general,
P (n) =
1
p + q
 
q + p(1 −p −q)n
p −p(1 −p −q)n
q −q(1 −p −q)n
p + q(1 −p −q)n
!
.
3. Limiting probabilities: Suppose in Example 2 that 0 < p < 1 and 0 < q < 1, so that
|1 −p −q| < 1. From the ﬁnal expression obtained in Example 2, it is seen that P (n)
tends to the limiting matrix
1
p + q
 
q
p
q
p
!
.
Consequently, if a = (a1, a2) is any initial distribution, then the limiting probabilities aP n
approach
1
p + q

a1
a2
  
q
p
q
p
!
=
1
p + q
 
q
p
q
p
!
since a1 + a2 = 1.
For example, the limiting probability of being in state 0 is
q
p+q ,
independent of the initial state of the DTMC. (See §7.7.4.)

522
Chapter 7
DISCRETE PROBABILITY
7.7.3
CLASSIFICATION OF STATES
Deﬁnitions:
State j ∈S is accessible from state i ∈S (written i →j) if it is possible to make a
sequence of transitions leading from state i to state j: that is, p(n)
ij
> 0 for some n ≥0.
States i, j ∈S communicate (written i ↔j) if they are mutually accessible from one
another: i →j and j →i.
Set C ⊆S is a (maximal) communicating class if
• i, j ∈C ⇒i ↔j;
• i ∈C, i ↔j ⇒j ∈C.
A communicating class C is closed if transitions from the states of C never lead to states
outside C: i ∈C, j /∈C ⇒j is not accessible from i.
A DTMC is irreducible if i ↔j for all i, j ∈S; otherwise it is reducible.
For j ∈S deﬁne
• Tj = min{n > 0 | Xn = j};
• fj = Pr(Tj < ∞| X0 = j);
• fj(n) = Pr(Tj = n | X0 = j);
• mj = E(Tj | X0 = j).
State j ∈S is recurrent if return to that state is certain: fj = 1; if fj < 1 then state j
is transient.
A recurrent state j ∈S is positive recurrent if mj < ∞and null
recurrent if mj = ∞.
A recurrent state j has period d if d is the largest integer satisfying P∞
n=0 fj(nd) = 1.
If d = 1 then state j is aperiodic.
Facts:
1. Generally, all classes that are not closed can be lumped into a single set T of tran-
sient states. Thus, the state space of a DTMC can be partitioned into closed classes
C1, C2, . . . , CK and the set T .
2. State j is accessible from state i if and only if there is a directed path from vertex i
to vertex j in the transition diagram of the DTMC.
3. A set of states C is a communicating class if and only if the corresponding set of
vertices forms a strongly connected component (§8.3.2) in the transition diagram.
4. Tarjan [Ta72] describes an algorithm to ﬁnd the strongly connected components,
which runs in time linear in the number of arcs in the transition diagram (i.e., the
number of nonzero entries of P).
5. Transience, positive recurrence, and null recurrence are class properties:
• if i is transient and i ↔j then j is transient;
• if i is positive recurrent and i ↔j then j is positive recurrent;
• if i is null recurrent and i ↔j then j is null recurrent.
In other words, states in a communicating class are all simultaneously transient or null
recurrent or positive recurrent.

Section 7.7
DISCRETE-TIME MARKOV CHAINS
523
6. By Fact 5, a communicating class or an irreducible DTMC can be termed positive re-
current, null recurrent, or transient if all of its states are positive recurrent, null recurrent,
or transient.
7. A ﬁnite communicating class is positive recurrent if it is closed, and transient other-
wise.
8. A ﬁnite state irreducible DTMC is positive recurrent.
9. Null recurrent states do not occur in a ﬁnite state DTMC.
10. Establishing recurrence or transience in an inﬁnite state DTMC is a diﬃcult task
and has to be done on a case-by-case basis.
11. We have not deﬁned period for a transient state since for such a state the concept
is not needed. Some references do however deﬁne period for all states.
12. The period of state j is the greatest common divisor of all integers n ≥0 such that
p(n)
jj > 0.
13. The period of state j is the greatest common divisor of all the lengths of the directed
cycles in the transition diagram that contain state j.
14. Periodicity is a class property: if i has period d and i ↔j, then j has period d.
15. The period of a state in a ﬁnite irreducible DTMC is at most equal to the number
of states in the DTMC.
16. By Fact 14, a recurrent communicating class or a recurrent irreducible DTMC can
be termed periodic if all states in it are periodic with d > 1, else it is termed aperiodic.
Examples:
1. For the DTMC in Example 1 of §7.7.1, it is seen that 1 →2, 2 →1, and 1 ↔2.
However, 3 is not accessible from 1. The communicating classes are C1 = {1, 2}, C2 =
{3, 4}, C3 = {5}, C4 = {6}. This DTMC is reducible. Classes C1, C2, C3 are closed,
but C4 is not. States 1, 2, 3, 4, 5 are positive recurrent and state 6 is transient. Classes
C1, C2, C3 are positive recurrent.
2. Consider the random walk in Example 2 of §7.7.1 with 0 < p < 1. There are three
communicating classes: C1 = {0}, C2 = {1, 2, ..., N −1}, and C3 = {N}. This DTMC
is reducible. Here C1 and C3 are closed, while C2 is not. States 0 and N are positive
recurrent. The rest are transient.
3. For the DTMC in Example 1 of §7.7.1, states 3 and 4 have period 2; states 1, 2, and 5
are aperiodic. A period is not associated with state 6 since it is transient. Classes {1, 2}
and {5} are aperiodic, while the class {3, 4} is periodic with period 2.
4. For the DTMC in Example 2 of §7.7.1, states 0 and N are aperiodic. Period is not
deﬁned for the rest of the states as they are transient.
5. Example 3 of §7.7.1 is an irreducible chain. All states are positive recurrent and have
period 2.
7.7.4
LIMITING BEHAVIOR
To establish possible equilibrium conﬁgurations of DTMCs, it is necessary to study the
behavior of the n-step transition probabilities Pr(Xn = j | X0 = i) as n →∞.

524
Chapter 7
DISCRETE PROBABILITY
Facts:
1. Let {Xn | n ≥0} be an irreducible DTMC with transition probability matrix P and
ﬁnite state space S. Then there exists a unique solution π = (πj) to the equations
π = πP,
X
j∈S
πj = 1.
2. The long run fraction of the visits to state j is given by πj, regardless of the initial
state. Also, mj, the expected time between two consecutive visits to state j, is
1
πj .
3. If the DTMC is aperiodic, then limn→∞Pr(Xn = j | X0 = i) = πj for all i ∈S.
4. Let {Xn | n ≥0} be a ﬁnite state reducible DTMC with K closed communicating
classes C1, C2, . . . , CK and the set of transient states T . Then limn→∞Pr(Xn = j | X0 =
i) = πij, where
(a) if j ∈T , then πij = 0;
(b) if i and j belong to diﬀerent closed classes, then πij = 0;
(c) if i and j belong to the same closed class Cr, then πij = πj, where {πj} are
the limiting probabilities calculated by using Fact 1 for the irreducible DTMC
formed by the states in Cr;
(d) if i ∈T and j ∈Cr, then πij = αirπj, where {πj} are as in (c) and αir is the
probability that the DTMC eventually visits the class Cr starting from state i.
5. In Fact 4(c), if Cr is periodic then limiting probabilities do not exist and πj is
interpreted as the long run fraction of the time the DTMC spends in state j starting
from state i.
6. The {αir} in Fact 4(d) are given by the unique solution to
αir = P
j∈Cr
pij + P
j∈T
pijαjr.
Examples:
1. The DTMC in Example 3 of §7.7.1 is irreducible and periodic with d = 2. Using
Fact 1, its limiting behavior is described by the equations
π0 = qπ1
π1 = π0 + qπ2
πi = pπi−1 + qπi+1
for 2 ≤i ≤N −2
πN−1 = pπN−2 + πN
πN = pπN−1
and PN
j=0 πj = 1. Solving these equations gives πj = ρj
c , where
ρ0 = 1
ρj =
( p
q )j
p
for 1 ≤j ≤N −1
ρN =
p
q
N−1
and the normalizing constant is c = PN
j=0 ρj.
This DTMC is periodic and hence
these πj represent the long run fraction of the time the DTMC spends in state j. Here
limn→∞Pr(Xn = j | X0 = i) does not exist since the probabilities under question keep
oscillating with period 2.

Section 7.7
DISCRETE-TIME MARKOV CHAINS
525
2. For the DTMC in Example 1 of §7.7.1, C1 = {1, 2}, C2 = {3, 4}, C3 = {5}, and
T = {6}. Therefore, π1 =
7
13, π2 =
6
13, π3 = 1
2, π4 = 1
2, π5 = 1, α61 = 2
5, α62 = 2
5,
α63 = 1
5. By Fact 4 the limiting matrix (πij) is given by









7
13
6
13
0
0
0
0
7
13
6
13
0
0
0
0
0
0
1
2
1
2
0
0
0
0
1
2
1
2
0
0
0
0
0
0
1
0
14
65
12
65
1
5
1
5
1
5
0









.
States 3 and 4 are periodic, and hence the third and fourth columns need to be interpreted
as the long run fraction of the time the discrete-time Markov chain spends in those states.
3. The Ehrenfest diﬀusion model (§7.7.1, Example 6) is an irreducible DTMC with
period 2. The solution π (Fact 1) is given by πj =
 M
j

2−M (0 ≤j ≤M). The binomial
distribution (§7.3.1) describes the long run fraction of time the system spends in each
state.
7.7.5
FIRST PASSAGE TIMES
Deﬁnitions:
Let {Xn | n ≥0} be a DTMC on state space S with transition probability matrix P.
Let A ⊆S be a given subset of states.
The ﬁrst passage time TA into set A is the ﬁrst time at which the Markov chain reaches
some state in set A; i.e., TA = min{n ≥0 | Xn ∈A}.
For i ∈S, let αi = Pr(TA < ∞| X0 = i) and let τi = E(TA | X0 = i).
Facts:
1. The {αi} are given by the unique solution to
αi = P
j∈S
pijαj
with the boundary conditions αi = 1 if i ∈A and αi = 0 if no state in A is accessible
from i.
2. If αi = 1 for all i ∈S, then {τi} are given by the unique solution to
τi = 1 + P
j∈S
pijτj
with the boundary condition τi = 0 if i ∈A.
Examples:
1. Consider the DTMC in §7.7.1, Example 2 with A = {0}. The equations of Fact 1 are
αi = qαi−1 + pαi+1, 1 ≤i ≤N −1 with the boundary conditions α0 = 1 and αN = 0. If
q ̸= p, the solution is given by
αi =
( q
p )i−( q
p )N
1−( q
p )N ,
0 ≤i ≤N.
If q = p, the solution is αi = 1 −i
N .

526
Chapter 7
DISCRETE PROBABILITY
2. Consider the DTMC in §7.7.1, Example 2 with A = {0, N}. In this case αi = 1 for all
i. The equations of Fact 2 are τi = 1 + qτi−1 + pτi+1, 1 ≤i ≤N −1, with the boundary
conditions τ0 = 0 and τN = 0. If q ̸= p, the solution is given by
τi =
i
q−p −
N
q−p
1−( q
p )i
1−( q
p )N ,
0 ≤i ≤N.
If q = p, the solution is given by τi = i(N −i).
7.7.6
BRANCHING PROCESSES
Branching processes are a special type of Markov chain used to study the growth (and
possible extinction) of populations in biology and sociology as well as particles in physics.
Deﬁnitions:
Suppose {Yni | n, i ≥1} are independent and identically distributed random variables
having common probability distribution function pk = Pr(Yni = k), k ≥0, with mean m
and variance σ2. Then the DTMC {Xn | n ≥0} is a branching process if X0 = 1,
Xn+1 =
Xn
P
i=1
Yni.
A branching process is stable if m < 1, critical if m = 1, and unstable if m > 1.
The extinction probability of a branching process is the probability that the popula-
tion becomes extinct, where X0 = 1.
Facts:
1. It is convenient to think of the random variable Xn as the number of individuals in
the nth generation and the random variable Yni as the number of oﬀspring of the ith
individual in the nth generation.
2. The transient behavior of the branching process is given by
E(Xn) = mn,
Var(Xn) =



nσ2,
if m = 1
σ2mn−1 mn −1
m −1 ,
if m ̸= 1.
3. State 0 is absorbing for a branching process. Absorption in state 0 is certain if and
only if m ≤1, while the expected time until extinction (i.e., absorption in state 0) is
ﬁnite if and only if m < 1.
4. The probability of extinction ρ in an unstable branching process is given by the
unique solution in (0, 1) to the equation
ρ =
∞
P
n=0
pnρn.
5. The expected total number of individuals ever born in a stable branching process
until it becomes extinct is given by
E(
∞
P
n=0
Xn) =
1
1 −m.
6. There is no simple expression for the expected time until absorption for a general
stable branching process.

Section 7.8
HIDDEN MARKOV MODELS
527
Examples:
1. The branching process with p0 = 1
2, p1 = 1
8, p2 = 3
8 has mean m = 7
8 < 1 and is
stable. With probability 1, the population will die out.
2. The branching process with p0 = 1
4, p1 = 1
4, p2 = 1
2 has mean m = 5
4 > 1 and is
unstable. The probability of extinction ρ0 is found as the smallest positive root of the
equation ρ = 1
4 + 1
4ρ + 1
2ρ2. The roots of this equation are 1
2 and 1, so the probability
of extinction is ρ0 = 1
2. If the initial population is X0 = 10 instead of X0 = 1, then the
probability that the initial population eventually becomes extinct is ρ10
0 =
1
1024.
7.8
HIDDEN MARKOV MODELS
A Hidden Markov Model (HMM) is a statistical tool used in modeling discrete-time
stochastic systems. The theory of HMMs assumes that the system under consideration
has only a ﬁnite number of states. Still the HMM is a very powerful tool because of
its dual stochastic nature (i.e., the probabilistic modeling of both inter-state transitions
and intra-state processes). This property makes it suitable for modeling systems that
change their statistical properties over time. HMMs are widely used in speech recognition
and have found applications in several other areas such as bioinformatics, gene sequence
modeling, economic/ﬁnancial analysis, speech synthesis, and web usage analysis.
7.8.1
BASIC CONCEPTS
Deﬁnitions:
A Hidden Markov Model (HMM) consists of a ﬁnite set S = {1, 2, . . ., N} of states,
where each state j ∈S is associated with a (generally multidimensional) probability
distribution bj(o). In a particular state, at a given time t ≥1, an outcome or observation
ot can be generated, according to the associated probability distribution.
Transitions among the states are governed by a set of transition probabilities aij =
Pr(qt+1 = j|qt = i), where qt ∈S is a random variable denoting the state at time t. The
matrix A = (aij) is the transition probability matrix of the HMM.
The initial state distribution of the HMM is the vector π = (πi), where πi = Pr(q1 =
i), i ∈S.
While a transition from qt to the next state qt+1 can in general depend on all past states,
the model is simpliﬁed by truncating the dependency to only the past k states. This gives
a kth-order HMM in which Pr(qt+1 = j|qt = it, qt−1 = it−1, . . . , qt−k+1 = it−k+1) =
Pr(qt+1 = j|qt = it, qt−1 = it−1, . . . , q1 = i1), where i1, i2, . . . , it, j ∈S. If k = 1, this
HMM has the Markov property:
aij = Pr(qt+1 = j|qt = i) = Pr(qt+1 = j|qt = it, qt−1 = it−1, . . . , q1 = i1),
for all i1, i2, . . . , it, j ∈S.
The HMM is stationary or time homogeneous if Pr(qs+1 = j|qs = i) = Pr(qt+1 =
j|qt = i) for all s, t ≥1.

528
Chapter 7
DISCRETE PROBABILITY
The HMM has the conditional independence property if Pr(O|q1, q2, . . . , qT , λ) =
QT
t=1 Pr(ot|qt, λ), where O = (o1, o2, . . . , oT ) is a sequence of observations, qt ∈S, and
λ denotes the parameter set of the HMM.
The HMM is a Discrete Hidden Markov Model (DHMM) if all the observations it
can generate can be drawn from a ﬁnite set. That is, ot ∈{ν1, ν2, . . . , νM} for all t ≥1,
where νk, k ∈M = {1, 2, . . ., M}, denotes the kth observation symbol in the alphabet.
Then the probability distribution associated with each state can be represented by the
ﬁnite set of probabilities bj(o) = bjk = Pr(o = νk|qt = j), where j ∈S, k ∈M, and o is
the current observation. The matrix B = (bjk) is termed the observation probability
matrix of the DHMM.
The HMM is a Continuous Density Hidden Markov Model (CDHMM) if it can
generate all the observations within a given (ﬁnite or inﬁnite) interval. In this case the
observation probability distribution can be modeled analytically using a density function
such as a multidimensional Gaussian distribution
N(µ, Σ, o) = (2π)−D
2 |Σ|−1
2 exp
 −1
2(o −µ)T Σ−1(o −µ)

where D is the dimension of the observation vector, µ is the distribution mean, and Σ is
the distribution covariance matrix.
Facts:
1. A discrete-time Markov chain (DTMC) [see §7.7] can be extended to an HMM by
associating a probability distribution with each state of the DTMC.
2. Unlike a DTMC, in the case of an HMM, only the outcome ot, not the state qt, is
visible to an external observer and therefore states are “hidden” to the outside. This
accounts for the name Hidden Markov Model.
3. A DHMM can be completely deﬁned by the 3-tuple λ = (A, B, π), where A, B, and π
are the transition probability matrix, the observation probability matrix, and the initial
state distribution, respectively.
4. For a CDHMM a weighted sum of a ﬁnite number M of Gaussians is often used to
model the distributions bj(o) as
bj(o) =
M
X
m=1
cjmN(µjm, Σjm, o)
for all j ∈S, where cjm, m ∈M, is the weighting coeﬃcient.
5. A CDHMM can be completely deﬁned by the 5-tuple λ = (A, C, µ∗, Σ∗, π), where A
and π are as in Fact 3, and the weighting coeﬃcient matrix, the set of mean vectors, and
the set of covariance matrices are given by C = (cjm), µ∗= {µjm|j ∈S, m ∈M}, and
Σ∗= {Σjm|j ∈S, m ∈M}.
6. The initial state probabilities satisfy PN
i=1 πi = 1.
7. The transition probabilities satisfy aij ≥0, i, j ∈S and PN
j=1 aij = 1, i ∈S.
8. The observation probabilities for a DHMM satisfy bjk ≥0, j ∈S, k ∈M and
PM
k=1 bjk = 1, j ∈S.
9. The weighting coeﬃcients for a CDHMM satisfy cjm ≥0, j ∈S, m ∈M and
PM
m=1 cjm = 1, j ∈S.

Section 7.8
HIDDEN MARKOV MODELS
529
Examples:
1. Suppose that an experimenter tosses one of three (possibly biased) coins and reports
to the observer only the outcome Heads (H) or Tails (T ). This situation can be modeled
as a DHMM in which the state j refers to coin j, which has probability pj of producing
Heads. The experimenter successively chooses coins according to a DTMC with transition
probabilities aij, where i, j ∈{1, 2, 3}. For coin j the observation probabilities for H and
T are bj(H) = pj and bj(T ) = 1 −pj, respectively. This DHMM has N = 3 states and
M = 2 observation symbols.
2. Suppose that there are N urns, each containing some mix of M colored balls. At
each time step, a ball is randomly selected from one of the urns and its color is revealed
to the observer. The ball is then returned to that urn. However, the observer does not
know what urn was sampled.
This situation can be modeled as a DHMM in which the state corresponds to the urn
sampled, with state transition probabilities indicating the next urn sampled. For a given
urn, the mixture of colors dictates the observation probabilities for the M possible colors.
Notice that the states (urns) are hidden from view, and one only is allowed to observe
the color drawn.
3. Suppose that we are interested in determining overall annual temperatures (Hot or
Cold) at a speciﬁc location at some point in the distant past. However, we do not have
records of the temperatures (say over T years) but do have samples of trees harvested
(and preserved) over that time period. In particular, we can observe the size of tree
rings o1, o2, . . . , oT over that time period, where each ot ∈{small, medium, large}. It is
reasonable that temperature (Hot, Cold) may follow a 2-state DTMC, with transition
probabilities indicating that hot years are more likely to follow hot years, and similarly
for cold years. Likewise, the size of tree rings may be presumed to be correlated with
overall annual temperature. This situation can be modeled as a DHMM, since we only
observe the size of tree rings, but not the state of the system (temperature).
As a speciﬁc example, the following ﬁgure shows the state transition diagram for the
Markov chain on states Hot and Cold. Below this are the corresponding columns of the
matrix B = (bjk). The observations o1, o2, o3 indicate respectively the ring sizes (small,
medium, large).
4. A variety of additional applications, together with reference sources, are given in the
following table.

530
Chapter 7
DISCRETE PROBABILITY
application
references
biological sequencing
[DuEtal98]
computer vision
[BuCa01]
economic analysis
[BhHa04]
handwriting recognition
[GuScDe97]
power systems
[SaGu93]
protein folding
[StEtal11]
speech recognition
[GaYo07]
speech synthesis
[ToEtal13]
web usage
[SaTa12]
7.8.2
THE EVALUATION PROBLEM
This subsection discusses the problem of determining the likelihood of obtaining the ob-
served data, given the speciﬁcation of the parameters of some given HMM. This provides
a measure for scoring how well a particular model matches the observation sequence. For
simplicity, the focus here is on DHMMs, with speciﬁed parameter set λ = (A, B, π).
Deﬁnitions:
Assume that a given sequence of observations O = (o1, o2, . . . , oT ) has been generated by
a given HMM with parameter set λ. The evaluation problem consists of determining
the probability Pr(O|λ).
A state Q of the system is given by Q = (q1, q2, . . . qT ) and the set of all system states
is denoted Ω. Direct evaluation of Pr(O|λ) is obtained by enumerating all states of
the system: Pr(O|λ) = P
Q∈ΩPr(O, Q|λ) = P
Q∈ΩPr(O|Q, λ) Pr(Q|λ).
The forward algorithm for computing Pr(O|λ) makes use of the forward variables
αt(i) = Pr(o1, o2, . . . , ot, qt = i|λ), which can be interpreted as the probability of the
partial observation sequence o1, o2, . . . , ot, when it terminates at the state i.
The backward algorithm for computing Pr(O|λ) makes use of the backward vari-
ables βt(i) = Pr(ot+1, ot+2, . . . , oT |qt = i, λ), which can be interpreted as the probability
of the partial observation sequence ot+1, ot+2, . . . , oT , given that the current state is i.
Facts:
1. In the direct evaluation method, Pr(Q|λ) can be calculated using π and A.
2. In the direct evaluation method, Pr(O|Q, λ) can be calculated using B.
3. Since the state space Ωhas size O(N T ), direct evaluation of Pr(O|λ) has O(T N T )
computational complexity and so is feasible only if N and T are very small.
4. The forward variables αt(j) satisfy the recursive relationship
αt+1(j) = bj(ot+1)
N
X
i=1
αt(i)aij, j ∈S, t ∈{1, 2, . . ., T −1},
with α1(j) = πjbj(o1), j ∈S.

Section 7.8
HIDDEN MARKOV MODELS
531
5. The conditional probability of the observation sequence given the HMM is related to
the forward variables through
Pr(O|λ) =
N
X
i=1
Pr(o1, o2, . . . , oT , qT = i|λ) =
N
X
i=1
αT (i).
6. The forward algorithm, shown as Algorithm 1, is based on Facts 4 and 5.
Algorithm 1:
Forward algorithm.
input: A, B, π
output: Pr(O|λ)
for j := 1 to N
α1(j) := πjbj(o1)
for t := 1 to T −1
for j := 1 to N
αt+1(j) := bj(ot+1) PN
i=1 αt(i)aij
Pr(O|λ) := PN
i=1 αT (i)
7. The computational complexity of the forward algorithm is O(N 2T ), which is linear
in T compared to the exponential complexity of direct evaluation (Fact 3).
8. The backward variables βt(i) satisfy the recursive relationship
βt(i) =
N
X
j=1
βt+1(j)aijbj(ot+1), i ∈S, t ∈{T −1, T −2, . . . , 1},
with βT (i) = 1, i ∈S.
9. The conditional probability of the observation sequence given the HMM is related to
the backward variables through
Pr(O|λ) =
N
X
i=1
πibi(o1)β1(i)
since
πibi(o1)β1(i)
= Pr(q1 = i) Pr(o1|q1 = i) Pr(o2, o3, . . . , oT |q1 = i, λ)
= Pr(o1, o2, . . . , oT , q1 = i|λ).
10. The backward algorithm, shown as Algorithm 2, is based on Facts 8 and 9.
Algorithm 2:
Backward algorithm.
input: A, B, π
output: Pr(O|λ)
for i := 1 to N
βT (i) := 1
for t := T −1 to 1
for i := 1 to N
βt(i) := PN
j=1 βt+1(j)aijbj(ot+1)
Pr(O|λ) := PN
i=1 πibi(o1)β1(i)

532
Chapter 7
DISCRETE PROBABILITY
11. The computational complexity of the backward algorithm is also O(N 2T ).
12. Using the deﬁnitions of αt(i) and βt(i) together with the conditional independence
assumption, it can be shown that
αt(i)βt(i) = Pr(O, qt = i|λ), i ∈S, t ∈{1, 2, . . ., T }.
13. Fact 11 gives another way to calculate Pr(O|λ), using both forward and backward
variables:
Pr(O|λ) =
N
X
i=1
Pr(O, qt = i|λ) =
N
X
i=1
αt(i)βt(i), t ∈{1, 2, . . . , T }.
Examples:
1. We use the direct evaluation approach to calculate the probability of observing the
sequence (medium, large, small) for Example 3 in §7.8.1. For ease of representation, the
observations are encoded using small = 1, medium = 2, and large = 3, so the observed
sequence is 2, 3, 1. Also, we encode the states as Hot = 1 and Cold = 2. In this problem,
the initial state probabilities π = (0.8, 0.2) are used.
To illustrate, for state Q = Hot Hot Cold = 112, we calculate Pr(Q|λ) = Pr(112|λ) =
(0.8)(0.7)(0.3) = 0.168 and Pr(O|Q, λ) = Pr(231|Q, λ) = (0.4)(0.5)(0.6) = 0.12. Con-
sequently, Pr(O, Q|λ) = Pr(O|Q, λ) Pr(Q|λ) = (0.12)(0.168) = 0.02016. Similar calcu-
lations are shown in the following table. Adding the entries in the last column gives
Pr(O|λ) = 0.03628.
Q
Pr(Q|λ)
Pr(O|Q, λ)
Pr(O, Q|λ)
111
0.392
0.02
0.00784
112
0.168
0.12
0.02016
121
0.096
0.004
0.000384
122
0.144
0.024
0.003456
211
0.056
0.015
0.00084
212
0.024
0.09
0.00216
221
0.048
0.003
0.000144
222
0.072
0.018
0.001296
2. We illustrate the application of the forward algorithm to the same problem studied
in Example 1.
α1(1) = π1b1(2) = (0.8)(0.4) = 0.32, α1(2) = π2b2(2) = (0.2)(0.3) = 0.06;
α2(1) = b1(3)[α1(1)a11 + α1(2)a21] = 0.5[(0.32)(0.7) + (0.06)(0.4)] = 0.124;
α2(2) = b2(3)[α1(1)a12 + α1(2)a22] = 0.1[(0.32)(0.3) + (0.06)(0.6)] = 0.0132;
α3(1) = b1(1)[α2(1)a11 + α2(2)a21] = 0.1[(0.124)(0.7) + (0.0132)(0.4)] = 0.009208;
α3(2) = b2(1)[α2(1)a12 + α2(2)a22] = 0.6[(0.124)(0.3) + (0.0132)(0.6)] = 0.027072.
As a result, Pr(O|λ) = 0.009208 + 0.027072 = 0.03628.
3. We illustrate the application of the backward algorithm to the same problem studied
in Example 1.
β3(1) = 1, β3(2) = 1;
β2(1) = β3(1)a11b1(o3) + β3(2)a12b2(o3) = 1(0.7)(0.1) + 1(0.3)(0.6) = 0.25;
β2(2) = β3(1)a21b1(o3) + β3(2)a22b2(o3) = 1(0.4)(0.1) + 1(0.6)(0.6) = 0.4;

Section 7.8
HIDDEN MARKOV MODELS
533
β1(1) = β2(1)a11b1(o2)+β2(2)a12b2(o2) = (0.25)(0.7)(0.5)+(0.4)(0.3)(0.1) = 0.0995;
β1(2) = β2(1)a21b1(o2) + β2(2)a22b2(o2) = (0.25)(0.4)(0.5) + (0.4)(0.6)(0.1) = 0.074.
Then Pr(O|λ) = π1b1(o1)β1(1)+π2b2(o1)β1(2) = (0.8)(0.4)(0.0995)+(0.2)(0.3)(0.074) =
0.03628.
7.8.3
THE DECODING PROBLEM
This subsection discusses the problem of ﬁnding a state sequence that best explains the
observed data, given the speciﬁcation of the parameters of an HMM. The focus here is
on DHMMs, with parameter set λ = (A, B, π).
Deﬁnitions:
Given an HMM with parameter set λ and a sequence of observations O = (o1, o2, . . . , oT ),
the decoding problem deals with ﬁnding a state sequence Q = (q1.q2, . . . , qT ) that
maximizes the conditional probability Pr(Q|O, λ).
The direct approach to the decoding problem proceeds by calculating Pr(Q|O, λ) for
all possible state sequences and picking one that achieves the highest value.
The Viterbi algorithm for solving the decoding problem makes use of the auxiliary
variable
δt(i) =
max
q1,q2,...,qt−1 Pr(q1, q2, . . . , qt−1, qt = i, o1, o2, . . . , ot|λ),
which is the largest joint probability that the partial observation sequence and the state
sequence can have up to time t, when the current state is i.
Facts:
1. Since the observation sequence O is given, the decoding problem can be solved by
ﬁnding a state sequence Q that maximizes the joint probability Pr(Q, O|λ).
2. Since the state space Ωhas size O(N T ), the direct approach has exponential com-
plexity.
3. From the Markov property and the conditional independence property of the HMM,
the following recursion holds:
δt+1(j) = bj(ot+1)[
max
i=1,...,N δt(i)aij ], j ∈S, t ∈{1, 2, . . ., T −1},
with δ1(j) = πjbj(o1), j ∈S.
4. The Viterbi algorithm (Algorithm 3), based on Facts 1 and 3, produces a state
sequence Q∗that maximizes the joint probability Pr(Q, O|λ).
Algorithm 3:
Viterbi algorithm.
input: A, B, π, O
output: state Q∗= (q∗
1, q∗
2, . . . , q∗
T ) that maximizes Pr(Q, O|λ), and the maximum
joint probability p∗.
for j := 1 to N
δ1(j) := πjbj(o1)
ψ1(j) := 0

534
Chapter 7
DISCRETE PROBABILITY
for t := 1 to T −1
for j := 1 to N
δt+1(j) := bj(ot+1)[ max i=1,...,N δt(i)aij ]
ψt+1(j) := argmax i=1,...,N δt(i)aij
p∗:= max j=1,...,N δT (j)
q∗
T := argmax j=1,...,N δT (j)
for t := T to 2
{Backtrack to ﬁnd the state Q∗}
q∗
t−1 := ψt(q∗
t )
Q∗= (q∗
1, q∗
2, . . . , q∗
T )
5. Algorithm 3 uses the predecessor array ψ in order to retrieve a state sequence that
achieves the maximum joint probability p∗.
6. Algorithm 3 has O(N 2T ) complexity.
7. Calculations in the Viterbi algorithm are quite similar to those in the forward algo-
rithm, except that the max operator is applied within the main loop as opposed to the
summation operator.
8. Algorithm 3 can be interpreted as a search in an acyclic network whose nodes are
formed by the states of the HMM at each time instant t ∈{1, 2, . . ., T }.
Examples:
1. We use the direct evaluation approach to calculate a state sequence Q∗that maximizes
the joint probability Pr(Q, O|λ), where O = 231. The last column of the table shown
in Example 1 of §7.8.2 displays the joint probabilities Pr(Q, O|λ). The maximum such
probability is then 0.02016, achieved by the state sequence Q∗= 112, which corresponds
to the sequence of temperatures Hot Hot Cold.
2. We illustrate the application of the Viterbi algorithm to the same problem studied
in Example 1.
δ1(1) = π1b1(2) = (0.8)(0.4) = 0.32, δ1(2) = π2b2(2) = (0.2)(0.3) = 0.06;
δ2(1) = b1(3) max [δ1(1)a11, δ1(2)a21] = 0.5 max [0.224, 0.024] = 0.112, ψ2(1) = 1;
δ2(2) = b2(3) max [δ1(1)a12, δ1(2)a22] = 0.1 max [0.096, 0.036] = 0.0096, ψ2(2) = 1;
δ3(1) = b1(1) max [δ2(1)a11, δ2(2)a21] = 0.1 max [0.0784, 0.00384] = 0.00784,
ψ3(1) = 1;
δ3(2) = b2(1) max [δ2(1)a12, δ2(2)a22] = 0.6 max [0.0336, 0.00576] = 0.02016,
ψ3(2) = 1.
So the maximum joint probability is p∗= max [δ3(1), δ3(2)] = max [0.00784, 0.02016)] =
0.02016. Also q∗
3 = 2, q∗
2 = ψ3(q∗
3) = ψ3(2) = 1, q∗
1 = ψ2(q∗
2) = ψ2(1) = 1, giving
Q∗= 112.
7.8.4
THE LEARNING PROBLEM
This subsection discusses the problem of optimizing the model parameters λ = (A, B, π)
of an HMM to best describe a given sequence of observations. This problem arises in
“training” an HMM in order to mimic the observed behavior of a system.

Section 7.8
HIDDEN MARKOV MODELS
535
Deﬁnitions:
Let the HMM have parameter set λ = (A, B, π) and suppose the sequence of observations
O = (o1, o2, . . . , oT ) is given. The learning problem deals with adjusting the model
parameters λ in order to maximize L = Pr(O|λ).
While there is no known way to analytically solve this problem, there are iterative
procedures that adjust the model parameters so that L is locally maximized.
The
Baum-Welch algorithm is such a procedure which makes use of the auxiliary vari-
ables ξt(i, j) = Pr(qt = i, qt+1 = j|O, λ) and γt(i) = Pr(qt = i|O, λ).
Here ξt(i, j) is the probability of being in state i at time t and in state j at time t + 1,
whereas γt(i) is the a posteriori probability of being in state i at time t, given the
observation sequence and the model.
Facts:
1. The Baum-Welch algorithm, also known as the forward-backward algorithm, can be
derived using arguments of occurrence counting between qt and qt+1.
2. The Baum-Welch algorithm is a special case of the well-known Expectation Maxi-
mization (EM) method. It is guaranteed to converge to at least a local maximum.
3. The following relationship holds:
ξt(i, j) = Pr(qt = i, qt+1 = j, O|λ)
Pr(O|λ)
=
αt(i)aijβt+1(j)bj(ot+1)
PN
ℓ=1
PN
m=1 αt(ℓ)aℓmβt+1(m)bm(ot+1)
,
where i, j ∈S and t ∈{1, 2, . . ., T −1}.
4. The relationship between γt(i) and ξt(i, j) is given by
γt(i) =
N
X
j=1
ξt(i, j), i ∈S, t ∈{1, 2, . . ., T −1}.
5. Using Facts 3 and 4, the Baum-Welch algorithm can be constructed as shown in
Algorithm 4.
Algorithm 4:
Baum-Welch algorithm.
input: λ = (A, B, π), O
output: λ = (A , B, π) that maximizes Pr(O|λ)
initialize λ0 to a set of random values
while Pr(O|λ) −Pr(O|λ0) > ǫ
run the forward algorithm to ﬁnd αt(i) for all i ∈S and t ∈{1, 2, . . ., T }
run the backward algorithm to ﬁnd βt(i) for all i ∈S and t ∈{1, 2, . . ., T }
for i := 1 to N
for j := 1 to N
for t := 1 to T −1
ξt(i, j) :=
αt(i)aijβt+1(j)bj(ot+1)
PN
ℓ=1
PN
m=1 αt(ℓ)aℓmβt+1(m)bm(ot+1)
for i := 1 to N
for t := 1 to T −1
γt(i) := PN
j=1 ξt(i, j)

536
Chapter 7
DISCRETE PROBABILITY
for i := 1 to N
πi := γ1(i)
for i := 1 to N
for j := 1 to N
aij :=
P{ξt(i, j) : t = 1, . . . , T −1}
P{γt(i) : t = 1, . . . , T −1}
for j := 1 to N
for k := 1 to M
bj(k) :=
P{γt(j) : t = 1, . . . , T −1; Ot = νk}
P{γt(j) : t = 1, . . . , T −1}
A := (aij), B := (bj(k)), π := (πi)
λ := (A , B, π), λ0 := λ
λ := λ
return λ
7.9
QUEUEING THEORY
Queueing theory provides a set of tools for the analysis of systems in which customers
arrive at a service facility.
It has its origins in the works of A. K. Erlang (starting
in 1908) in telephony. Since then it has found many applications in diverse areas such as
manufacturing, inventory systems, computer science, analysis of algorithms, and telecom-
munications. Although queueing theory uses the terminology of servers providing service
to customers, in actual applications the customers may be people, jobs, computational
steps, or messages, and the servers may be human beings, machines, telephone circuits,
communication channels, or computers.
7.9.1
SINGLE-STATION QUEUES
The simplest queueing system is a single-station queue in which customers arrive, wait for
service, and depart after service completion. In this and subsequent sections we restrict
ourselves to single-station queues.
Deﬁnitions:
A queueing system consists of a set of customers, who arrive at a service facility
according to a speciﬁed arrival process. If a server is available then the customer is
served immediately, with the length of time required to carry out the service determined
by a service-time distribution. If a server is not free, the customer joins the queue
and is later served according to a service discipline, which speciﬁes the order in which
customers are selected for service from the queue. Throughout, the service discipline is
assumed to be First-Come-First-Served (FCFS). Alternative service disciplines include
Last-Come-First-Served, randomly, or according to a tiered priority scheme.
The queue capacity is the maximum number of customers allowed in the system,

Section 7.9
QUEUEING THEORY
537
either being served or awaiting service. Unless otherwise speciﬁed, the queue capacity is
assumed to be inﬁnite.
In a single-station queueing system, customers arrive, wait for service, and depart after
service completion.
An exponential distribution with parameter λ is a density function (§7.3.1) having
the form f(x) = λe−λx for x ≥0.
An arrival process is Poisson if the interarrival times (times between successive arrivals)
are independent and identically distributed exponential random variables.
A random variable has an Erlang distribution with phase parameter k if it is the sum
of k ≥1 independent and identically distributed exponential random variables.
Facts:
1. If the random variable X has an exponential distribution with parameter λ, then
E(X) = 1
λ and Var(X) =
1
λ2 .
2. If the arrival process is Poisson with parameter λ, then the number of customers
arriving in an interval of time of length x is a Poisson random variable (§7.3.1) with
parameter λx.
3. The Erlang distribution is a special type of gamma distribution (§7.3.1).
4. Kendall’s notation: A single-station queueing system can be described by the 5-tuple
interarrival-time distribution/service-time distribution/number of servers/waiting room
capacity/service discipline.
5. The following symbols are standard in describing queueing systems according to the
scheme in Fact 4.
• M — exponential (M for Memoryless);
• Ek — Erlang with phase parameter k;
• D — deterministic (constant);
• G — general.
6. More complicated queueing systems can consist of networks of queues, multiple types
of customers, and priority schemes.
7. A list of over 300 books on queueing theory can be found at
• http://web2.uwindsor.ca/math/hlynka/qbook.html
8. A compilation of queueing theory software can be found at
• http://web2.uwindsor.ca/math/hlynka/qsoft.html
Examples:
1. A single-station queueing system is depicted by the schematic diagram in the following
ﬁgure. Here customers randomly join the system (according to the arrival process), wait
for service in the waiting room, are served (which takes a random amount of time), and
then depart from the system.
arrivals
waiting room
departures

538
Chapter 7
DISCRETE PROBABILITY
2. An M/G/3/10/LCFS system has Poisson arrivals (exponential interarrival times),
general service times, three servers, room for ten customers (including those in service),
and Last-Come-First-Served service discipline.
3. An M/M/1 queue has Poisson arrivals, exponential service times, a single server,
inﬁnite waiting room, and FCFS service discipline.
4. Airplane landings: The landing of aircraft at an airport can be viewed as a queueing
system in which the aircraft are the customers and the runways are the servers. Aircraft
arrive according to a certain stochastic arrival process, and the length of time to land
follows a certain service-time distribution. Those aircraft that are unable to land must
join the queue of circling aircraft, awaiting service (normally according to a FCFS dis-
cipline, except in the case of an emergency landing, which would be a type of priority
scheme).
5. Communication network:
Messages arrive according to a Poisson process at rate λ
per second and are to be transmitted over a particular data link. The time required to
transmit a single message is exponentially distributed, with average duration 1
µ seconds.
Messages waiting to be sent are stored in an input buﬀer.
If the buﬀer has inﬁnite
capacity, then this system is an M/M/1 queue. If the input buﬀer has ﬁnite capacity c,
then this system is an M/M/1/c queue.
6. Banking: Customers arriving at a bank form a single common queue and are served
by the s available tellers in FCFS order. If arrivals are Poisson and the length of time
to service a customer is exponential, this system can be modeled as an M/M/s queue.
7. Call center:
A call center has c operators (servers) available to provide technical
assistance to customers calling a central telephone number. Calls are routed to an avail-
able agent if one is free; otherwise the caller receives a busy signal. If arrivals are Poisson
and the length of time spent on a call follows an arbitrary distribution, then this is an
M/G/c/c queue. It is also known as a loss system, since any calls to the system receive a
busy signal when all servers are occupied, and hence these calls are “lost” to the system.
7.9.2
GENERAL SYSTEMS
This section presents results applicable to single-station queues with general arrival pat-
terns, service-time distributions, and queue disciplines.
Deﬁnitions:
For a single-station queueing system, deﬁne
• An = the arrival time of the nth customer to the system;
• Sn = the service time of the nth customer;
• Dn = the departure time of the nth customer;
• A(t) = the total number of arrivals up to and including time t;
• D(t) = the total number of departures up to and including time t;
• X(t) = the total number of customers waiting in the system at time t.
The stochastic process {X(t) | t ≥0} is the queue-length process.
The state distribution following an arbitrary departure is πj = lim
n→∞Pr(X(D+
n ) = j), for
j ≥0.

Section 7.9
QUEUEING THEORY
539
The state distribution prior to an arbitrary arrival is π∗
j = lim
n→∞Pr(X(A−
n ) = j), for
j ≥0.
The state distribution at an arbitrary time point, or steady-state distribution, is
pj = lim
t→∞Pr(X(t) = j), for j ≥0.
The queue-length process (or the queueing system) is stable if the steady-state distri-
bution {pj | j ≥0} exists and
∞
P
j=0
pj = 1.
The waiting time of the nth customer is Wn = Dn −An; it includes the service time.
The steady-state expected waiting time is W = lim
n→∞
1
n
nP
k=1
Wk, if the limit exists.
The long-run arrival rate is λ = lim
n→∞
n
An , if the limit exists.
The steady-state expected number in the system is L = lim
t→∞
1
t
R t
0 X(u) du, if the limit
exists.
Facts:
1. The number of customers in the system at any time equals the total number of
arrivals up to that time minus the number of departures up to that time: that is, X(t) =
A(t) −D(t) for t ≥0.
2. A sample path of the queue-length process {X(t) | t ≥0} is piecewise constant, with
upward jumps at points of arrival and downward jumps at points of departure.
3. Suppose all the jumps in the sample paths of {X(t) | t ≥0} are of size ±1, with
probability 1. If either πj or π∗
j exists, then πj = π∗
j for all j ≥0.
4. PASTA (Poisson Arrivals See Time Averages): If {A(t) | t ≥0} is Poisson and, for
every s ≥0, {A(t) | t ≥s} is independent of {X(u) | 0 ≤u < s}, then pj = π∗
j for all
j ≥0.
5. Little’s law (J. D. C. Little, 1961): L = λW.
Examples:
1. Suppose arrivals to a system occur deterministically, every three minutes, and service
times are deterministic, each taking two minutes. Since every arriving customer is served
immediately, either X(t) = 0 (no customers) or X(t) = 1 (a single customer). Every
arrival ﬁnds an empty system and every departure leaves an empty system: π0 = 1 = π∗
0,
π1 = 0 = π∗
1, as required by Fact 3. (The steady-state distribution does not exist.)
2. On average λ = 24 customers per hour arrive at a copy shop. Typically, there are
L = 9 customers in the store at any time. Using Little’s law, W = L
λ = 0.375 hour so
that each customer spends on average 0.375 hours (or 22.5 minutes) in the shop.
3. The steady-state queue length or waiting time in a queueing system can be reduced
by increasing the service rate. Suppose the long-run arrival rate doubles, but the service
rate is increased so that the steady-state expected waiting time remains the same. Then
by Little’s law the steady-state expected queue length will also double.
4. Machine repair: A single machine is either working or being repaired. Suppose that
the average time between breakdowns is exponentially distributed with mean 1
λ and the
time to repair the machine is is exponentially distributed with mean
1
µ. This is then
an M/M/1/1 queueing system with a single customer, corresponding to a broken down
machine. X(t) = 0 signiﬁes that the machine is working, and X(t) = 1 signiﬁes that the

540
Chapter 7
DISCRETE PROBABILITY
machine is being repaired. A sample path of this system is shown in the following ﬁgure,
with the machine initially working. Over a long period of time, after N breakdowns and
subsequent repairs, the machine is working for N( 1
λ) units of time and is being repaired
for N( 1
µ) units of time. The long run proportion of time the machine is working is then
N( 1
λ)
N( 1
λ) + N( 1
µ) =
µ
λ + µ.
This value also turns out to be the steady-state probability of ﬁnding the system in state
0, with the machine working.
7.9.3
SPECIAL QUEUEING SYSTEMS
This section summarizes analytical results about special types of single-station queues.
Notation:
•
1
λ = expected interarrival time;
•
˜A(·) = Laplace transform of the interarrival-time density;
•
1
µ = expected service time;
•
˜B(·) = Laplace transform of the service-time density;
• σ2 = variance of the service time;
• s = number of servers;
• ρ =
λ
sµ = traﬃc intensity.
The probability generating function (§7.3.3) for the steady-state distribution {pj} of a
queueing system is φ(z) =
∞
P
j=0
pjzj, |z| < 1.
The Laplace transform for the waiting-time distribution f(w) of a queueing system is
ψ(s) =
R ∞
0
e−swf(w) dw, Re(s) > 0.
Facts:
1. The M/M/1 queue is stable if ρ < 1. The following results hold when the queue is
stable.
pj = (1 −ρ)ρj = πj = π∗
j
L =
ρ
1−ρ
W =
1
µ−λ.

Section 7.9
QUEUEING THEORY
541
2. The M/M/1/K queue is always stable. Assume ρ ̸= 1.
pj =
1−ρ
1−ρK+1 ρj,
0 ≤j ≤K
π∗
j =
pj
1−pK = πj,
0 ≤j ≤K −1
L =
ρ
1−ρ( 1−ρK
1−ρK+1 −KpK)
W =
1
µ−λ( 1−ρK
1−ρK+1 −KpK).
If ρ = 1, the above formulas reduce to
pj =
1
K+1 ,
0 ≤j ≤K
π∗
j = πj = 1
K ,
0 ≤j ≤K −1
L = K
2
W = K
2µ .
3. The M/M/s queue is stable if ρ < 1. The following results hold when the queue is
stable.
p0 =
s−1
P
n=0
(sρ)n
n!
+ ss
s!
ρs
1−ρ
−1
pj =
(
(sρ)j
j! p0,
0 ≤j < s
ss
s! ρjp0,
j ≥s
πj = π∗
j = pj,
j ≥0
L = ρ(s +
ps
(1−ρ)2 )
W =
1
sµ(s +
ps
(1−ρ)2 )
E(number of busy servers) = ρ.
4. The M/M/∞queue is always stable.
pj = e−(λ/µ) (λ/µ)j
j!
,
j ≥0
πj = π∗
j = pj,
j ≥0
L = λ
µ
W = 1
µ.
5. The M/G/1 queue is stable if ρ < 1. The following results hold when the queue is
stable.
p0 = 1 −ρ,
πj = π∗
j = pj
φ(z) = (1 −ρ) (1−z) ˜
B(λ(1−z))
˜
B(λ(1−z))−z
ψ(s) = (1 −ρ)
s ˜
B(s)
s−λ(1−˜
B(s))
L = ρ + ρ2+λ2σ2
2(1−ρ)
W = 1
µ + λ((1/µ)2+σ2)
2(1−ρ)
.
The last four equations are the various forms of the Pollaczek-Khintchine formula. No
closed form results are available for M/G/c queues for 2 ≤c < ∞.

542
Chapter 7
DISCRETE PROBABILITY
6. The M/G/c/c queue, also called a loss system, is always stable. The main result is
pj =
ρj/j!
cP
n=0
ρn/n!
,
0 ≤j ≤c.
7. The M/G/∞queue is always stable.
pj = e−(λ/µ) (λ/µ)j
j!
,
j ≥0
πj = π∗
j = pj,
j ≥0
L = λ
µ
W = 1
µ.
8. The G/M/1 queue is stable if ρ < 1. When the queue is stable there is a unique
solution α ∈(0, 1) to α = ˜A(µ(1 −α)). The following results hold when the queue is
stable.
π∗
j = (1 −α)αj = πj,
j ≥0
p0 = 1 −ρ
pj = ρπ∗
j−1,
j ≥1
L =
ρ
1−α
W = 1
µ
1
(1−α).
The G/M/c queue can be analytically solved, but the results are complicated.
9. The G/M/∞queue is always stable.
L = λ
µ
variance of number in system = λ
µ(1 −λ
µ) + λ/µ
2
˜
A(µ)
1−˜
A(µ).
Examples:
1. At a drop-in legal clinic, the lawyer sees four clients during a typical (eight hour)
day. Each client’s case requires on average 1.5 hours of the lawyer’s time. If arrivals are
Poisson and service times are exponentially distributed, then this is an M/M/1 queue
with λ = 4
8 = 1
2 customers per hour and 1
µ = 3
2. Here ρ = λ
µ = 3
4 < 1, so the queue is
stable. Using Fact 1, p0 = 1 −ρ = 1
4, so there is probability 1
4 that the lawyer is idle.
The expected number of clients in the clinic is L =
ρ
1−ρ = 3 and the average wait of a
client is W =
1
µ−λ = 6 hours.
2. Customers arrive at a service station according to a Poisson process with rate 10 per
hour. The manager has two options: (a) employ a single fast server who can service
a customer in ﬁve minutes on average, or (b) employ two slow servers each taking ten
minutes on average to serve a customer. Assume that the service times are exponential.
Which option should the manager implement to minimize the expected waiting time in
steady state?
Under (a) the system is an M/M/1 queue with λ = 10, µ = 12. Since ρ = 10
12 < 1, the
system is stable. By Fact 1, W =
1
12−10 = 0.5 hours. Under (b) the system is an M/M/2
queue with λ = 10 and µ = 6. The system is stable since ρ =
10
2·6 < 1. From Fact 3,
p0 =
1
11, p2 =
25
198, and W =
6
11 = 0.55 hours. Thus option (a) is better. In general, it is
better to employ a few fast servers than many slow servers with the same overall service
capacity.

Section 7.10
SIMULATION
543
3. A system manager has the option of using one of three possible servers in a single-
server system. The service times under the ﬁrst server are exponential with mean of
six minutes. Under the second server they are uniformly distributed over [4, 8] minutes.
Under the third they are constant, equal to six minutes. The customers arrive according
to a Poisson process with rate 8 per hour. Which server should be chosen to minimize
the expected waiting time in steady state?
The mean service time is six minutes; i.e., µ = 10 per hour, for all three servers. However,
the variances σ2 are diﬀerent. This M/G/1 system is stable under all three servers since
ρ =
8
10 < 1. For server one, σ2 = 0.01 (hours)2 and W = 0.5 hours. For the second server,
σ2 =
1
2700 = 0.000370 (hours)2 and W =
83
270 = 0.31 hours. For the third server, σ2 = 0.0
(hours)2 and W =
3
10 = 0.3 hours. Thus, it is best to use the server with constant service
times. In general, reducing the variance of the service times has a beneﬁcial eﬀect on the
system.
4. A small business wants to install a telephone system with multiple lines, though
without any capacity for call queueing. This is to be done to ensure that 95% of the
calls made to the business get answered. Suppose that the arrival process is Poisson with
λ = 10 calls per hour, and that the average call lasts ﬁve minutes. This is an M/G/c/c
loss system (Fact 6), and it is necessary to ﬁnd the smallest value of c such that pc ≤0.05.
Using Fact 6 with ρ = 10
12 = 5
6 and c = 1 gives p1 =
ρ
1+ρ = 0.45. Similar calculations
give p2 = 0.16 for c = 2 and p3 = 0.042 for c = 3. Consequently, three lines are needed
to ensure the stipulated grade-of-service requirement.
7.10
SIMULATION
Simulation is a technique for numerically estimating the performance of a complex
stochastic system when analytic solution is not feasible.
This section discusses both
discrete-event and Monte Carlo simulation.
In discrete-event simulation models, the
passage of time plays a key role, as changes to the state of the system occur only at
certain points in simulated time. For example, queueing and inventory systems can be
studied by discrete-event simulation models. Monte Carlo simulation models do not,
however, require the passage of time. Such models are useful in estimating eigenvalues,
estimating π, and estimating the quantiles of a mathematically intractable test statistic
in hypothesis testing. Simulation has been described [BrFoSc87] as “driving a model of
a system with suitable inputs and observing the corresponding outputs.” Accordingly,
the following three subsections discuss input modeling, output analysis, and simulation
programming languages.
7.10.1
INPUT MODELING
This subsection addresses three key issues in constructing a simulation model:
• determining a source of randomness to drive the probabilistic aspects of the model;
• input model selection to determine the appropriate probabilistic models to drive
the simulation;

544
Chapter 7
DISCRETE PROBABILITY
• random variate generation algorithms that transform random numbers to random
variates.
Deﬁnitions:
Random numbers are real numbers generated uniformly over the interval (0, 1).
A random number generator is any mechanism or algorithm for generating random
numbers.
Pseudo-random numbers are values generated deterministically, but that appear to
behave like independent and identically distributed random numbers.
Let m be a large prime integer. A purely multiplicative linear congruential gen-
erator (§4.3.1) produces a stream of pseudo-random numbers { xi
m | i ≥1} based on the
recursive relationship xi+1 = axi (mod m), where a is an integer multiplier between 1
and m −1, and x0 is an integer seed between 1 and m −1.
An input model characterizes the stochastic elements of a discrete-event simulation
model.
A trace-driven input model generates a process that is identical to the collected data
values without relying on a parametric model.
A random variate is a realization of a random variable.
Facts:
1. Stochastic simulations typically derive their source of randomness from random num-
bers. That is, inputs to the simulated system need to be generated according to a speciﬁed
probability model, a task that can be accomplished by suitably transforming (uniform)
random numbers.
2. Desirable properties for random number generators include: uniformity, indepen-
dence, speed, minimal memory requirements, ease of implementation, portability across
various computer systems, reproducibility, and multiple stream capability.
3. Although numerous methods have been proposed for generating random numbers,
multiplicative linear congruential generators are typically used to produce a stream of
pseudo-random numbers.
4. Due to the prevalence of 32-bit computer architecture, m is often chosen to be 231−1,
which is prime.
5. A full period generator, which cycles through all m −1 possible xi values prior to
repeating, is obtained by selecting a to be a primitive root modulo m. (See §4.7.1.)
6. Software for pseudo-random number generators can be found at
• http://random.mat.sbg.ac.at/software
• http://www.taygeta.com/random.html
7. Additional information on the theoretical and empirical performance of a variety of
pseudo-random number generators is available at
• http://random.mat.sbg.ac.at/generators
• http://random.mat.sbg.ac.at/tests
8. If the multiplier a is chosen so that it is “modulus-compatible” with m, then potential
overﬂow can be averted for large values of m. Two values of a that are often used with
m = 231 −1 are a = 75 = 16,807 and a = 48,271 [PaMi88].
9. More detail concerning random number generation is given by [LE94] and [LE12].

Section 7.10
SIMULATION
545
10. Successful input modeling for a discrete-event simulation requires a close match
between the input model and the true underlying probabilistic mechanism associated
with the system.
11. One of the ﬁrst steps in determining an appropriate input model for an element
of a discrete-event simulation is to assess whether the observations are independent and
identically distributed.
12. An input model can be speciﬁed in several ways: e.g., using a cumulative distribu-
tion function, joint probability density function, hazard function, intensity function, or
variate-generation algorithm.
13. Potential input models for simulation are described in references [BaNe03], [FoEtal11],
[JoKoBa94] and at
• http://www.math.wm.edu/~leemis/chart/UDR/UDR.html
14. Many input models rely on parametric probabilistic models such as the binomial,
normal, and Weibull distributions. Maximum likelihood is typically used to estimate
parameters of these models.
15. B´ezier curves [FlWi93] oﬀer a unique combination of the parametric and nonpara-
metric approaches. After an initial distribution is ﬁtted to the dataset, the modeler
decides whether diﬀerences between the empirical and ﬁtted models represent sampling
variability (chance variation) or an aspect of the distribution that should be included in
the input model.
16. Multivariate distributions (e.g., the multivariate normal distribution with mean µ
and variance-covariance matrix Σ) are considered by [Jo87].
17. Once an input model has been chosen, random variate generation algorithms are
used to transform random numbers to random variates from the input model.
18. Devroye [De86] gives algorithms for converting random numbers to random variates
associated with input models chosen to drive the simulation.
19. Techniques commonly used for generating random variates from univariate proba-
bility distributions are: inverse transformation, composition, acceptance/rejection, and
special properties.
20. Algorithm 1, which shows the inversion method for generating a univariate random
variate X, is based on the probability integral transformation. It is assumed that the
cumulative distribution function F(x) for the input model of interest has the inverse
F −1(U).
Algorithm 1:
Inverse transformation method.
input: cumulative distribution function F
output: a random variate X from this distribution
generate U uniformly over (0, 1)
X := F −1(U)
21. Other topics in random variate generation include table methods, generating from
multivariate distributions, random sampling, estimating integrals, and generating pro-
cesses correlated in time.

546
Chapter 7
DISCRETE PROBABILITY
Examples:
1. Suppose that a sequence of arrival times (e.g., of customers at a bank) is collected over
a 24-hour time period. A trace-driven input model for the arrival process is generated
by having arrivals occur at the same times as the observed values.
2. Let t1, t2, . . . , tn be the arrival times to a queue collected on the time interval (0, c].
If the times between arrivals are independent and identically distributed, a parametric
or nonparametric model can be ﬁtted to the data. In the former case, parameters are
often estimated by maximizing the likelihood function [La15]
L(θ) =
n
Y
i=1
f(xi, θ),
where xi = ti −ti−1 for i = 1, 2, . . ., n, t0 ≡0, θ = (θ1, θ2, . . . , θp) is a vector of unknown
parameters, and f(xi, θ) is the probability density function of the interarrival times.
3. If the interarrival times to a queue (as in Example 2) are not independent and iden-
tically distributed, then a nonstationary point process might be considered, such as a
nonhomogeneous Poisson process, where the arrival rate λ(t; θ) varies over time. One
parametric model is the power law process, with intensity function λ(t; λ, κ) = λκκtκ−1
for t > 0, where λ and κ are positive parameters. The likelihood function for the sin-
gle realization on (0, c] is L(λ, κ) = (Qn
i=1 λ(ti; λ, κ)) exp
 −
R c
0 λ(t; λ, κ) dt

. Maximum
likelihood estimators can be determined by maximizing L(λ, κ) or its logarithm with
respect to λ and κ. Conﬁdence regions for the unknown parameters can be found by
using asymptotic properties of the likelihood ratio statistic or the observed information
matrix [La15]. As with all statistical modeling, goodness-of-ﬁt tests should be performed
in order to assess the model adequacy.
4. Weibull distribution: The Weibull distribution has cumulative distribution function
F(x) = 1 −e−(λx)κ for x > 0.
The inverse cumulative distribution function is F −1(y) = [−ln(1 −y)]1/κ
λ
for 0 < y < 1.
Algorithm 1 can be used to generate a Weibull variate according to
X := [−ln(1 −U)]1/κ
λ
.
5. M/M/1 queue:
To simulate the operation of a single-server queue (§7.9.1) having
Poisson arrivals at arrival rate λ and exponential service times with service rate µ, ex-
ponentially generated variates are needed for the interarrival times {In} and the service
times {Sn}. These are available as a special case of the Weibull distribution (Example
4) with κ = 1 and can be generated using In = −ln(1 −Un)
λ
and Sn = −ln(1 −Vn)
µ
,
with the {Un}, {Vn} generated uniformly over (0, 1).
A concrete example is provided in the following table, which shows one simulated run of
an M/M/1 queue with λ = 0.5 and µ = 0.7. The table shows, in successive columns,
the following values for each customer n: the interarrival time In, the arrival time An,
the service time Sn, the beginning time of service Bn, the departure time Dn, and the
sojourn time Wn = Dn −An. Notice that customers 1, 4, 7, 8, 9 are served immediately
and incur no waiting time in the queue.

Section 7.10
SIMULATION
547
customer
In
An
Sn
Bn
Dn
Wn
1
5.44
5.44
0.78
5.44
6.22
0.78
2
0.61
6.05
2.77
6.22
8.99
2.94
3
0.35
6.40
0.96
8.99
9.95
3.55
4
4.12
10.52
2.42
10.52
12.94
2.42
5
0.54
11.06
0.88
12.94
13.82
2.76
6
2.07
13.13
0.87
13.82
14.69
1.56
7
6.82
19.95
0.86
19.95
20.81
0.86
8
2.19
22.14
0.76
22.14
22.90
0.76
9
4.09
26.23
3.31
26.23
29.54
3.31
10
0.02
26.25
0.01
29.54
29.55
3.30
7.10.2
OUTPUT ANALYSIS
Once a veriﬁed and validated simulation model has been developed, a modeler typically
wants to estimate measures of performance associated with outputs of the model. Al-
though there are often several performance measures of interest, a single measure of
performance θ (e.g., the mean waiting time in a queue) is studied here. This subsection
discusses using point estimation to compute an estimate for θ, determining a conﬁdence
interval for the point estimate, and applying variance reduction techniques to obtain
more precise point estimates.
Deﬁnitions:
Suppose {Yi} is the output stochastic process. If the output stochastic process con-
sists of independent observations obtained from a population with cumulative distribu-
tion function FY , the pth quantile of FY is the value yp such that FY (yp) = p. The
median of FY corresponds to p = 0.5.
The sample mean of the observations Y1, Y2, . . . , Yn is given by Y = 1
n
nP
i=1
Yi.
If the values Y1, Y2, . . . , Yn are rearranged so that Y(1) ≤Y(2) ≤· · · ≤Y(n) then Y(i) is
the ith order statistic.
The mean µY of the process is the asymptotic mean of the output process {Yi}.
The variance σ2
Y of the process is the asymptotic mean of the output process {(Yi−Y )2}.
The probability Pr(A) of event A is the asymptotic mean of the output process {I(A)},
where I is the 0-1 indicator function for event A.
The output process Y1, Y2, . . . is covariance stationary if, for ﬁnite mean µ and ﬁnite
variance σ2 > 0, E(Yi) = µ, i = 1, 2, . . ., Var(Yi) = σ2, i = 1, 2, . . ., and Cov(Yi, Yi+j) is
independent of i, for j = 1, 2, . . ..
Variance reduction techniques are strategies for obtaining greater precision for a
ﬁxed amount of sampling.
Time-persistent statistics are often calculated in discrete-event simulations. These
statistics fundamentally diﬀer from statistics that are based on observations Y1, Y2, . . . , Yn.
Time-persistent statistics account for the value of a performance measure of interest (for
example, the number of customers in a waiting line) and also the length of time that the
value was held.

548
Chapter 7
DISCRETE PROBABILITY
Facts:
1. The three most common measures of performance to be estimated in Monte Carlo
simulation and discrete-event simulation are means, probabilities, and quantiles.
2. Point estimates for µY , σ2
Y , Pr(A) are typically given by the associated sample
statistics, that is, the sample mean, the sample variance, and the fraction of occurrences
of the event A.
3. A simple estimator of yp = F −1
Y (p) is Y(s), where s = ⌊p(n + 1)⌋. This estimator can
be improved (with respect to bias) by estimating F −1
Y (p) with the linear combination
(1 −α)Y(s) + αY(s+1), where α = p(n + 1) −⌊p(n + 1)⌋.
4. Sample means, variances, and quantiles can also be estimated for time-persistent
statistics [Ne13].
5. Replication:
This is one of the simplest methods of interval estimation, in which
several runs of a simulation model are used. Classical conﬁdence intervals based on the
central limit theorem for the measures of interest are often applied to the output.
6. The presence of autocorrelation among observations (e.g., the waiting times of adja-
cent customers in a queue) signiﬁcantly complicates the statistical analysis of simulation
output from a single run.
7. To analyze a single simulation run with autocorrelation present, techniques have been
developed for determining interval estimates whose actual coverage is close to the stated
coverage. For many of these techniques, the output is assumed to be covariance station-
ary. These techniques include batch means, overlapping batch means, standardized time
series, regeneration, spectral analysis, and autoregression [La15].
8. Common random numbers:
This is a variance reduction technique in which two
or more alternative system conﬁgurations are analyzed using the same set of random
numbers for particular purposes (e.g., generating service times). Using common random
numbers ensures that the output diﬀerences are due to the conﬁgurations rather than
the sampling variability in the random numbers.
9. Antithetic variates:
This is a second variance reduction technique, applicable to
the analysis of a single system. If the random numbers {Ui} are used for a particular
purpose in one simulation run, then using {1 −Ui} in a second run typically induces
a negative correlation between the outputs of the two runs. Thus, the average of the
output measures from the two runs will have a reduced variance.
10. There are a variety of variance reduction techniques. See Wilson [Wi84] for a de-
tailed discussion.
11. Other topics in output analysis include initialization bias detection, ranking and
selection, comparing alternative system designs, experimental design, and optimization.
Examples:
1. Conﬁdence intervals for expected waiting times: Let X1, X2, . . . , Xn be the averages
of the waiting times of customers in a single-server queue from n independent replications
of a discrete-event simulation model. A 100(1−α)% conﬁdence interval for µ, the steady-
state mean waiting time, is
X −tα/2,n−1
s
√n < µ < X + tα/2,n−1
s
√n ,
where X is the sample mean, s is the sample standard deviation, and tα/2,n−1 is the
1
1−α/2
quartile of the t distribution with n −1 degrees of freedom. Each replication must be
“warmed up” to avoid initialization bias. The asymptotic normality of X1, X2, . . . , Xn is

Section 7.10
SIMULATION
549
assured by the central limit theorem and independence is based on the use of independent
random number streams.
2. M/M/1 queue:
The simulation in Example 5 of §7.10.1 was executed so that the
ﬁrst 200 customer wait times were collected.
The measure of performance θ for the
system is the steady-state expected customer wait time. The initial conditions for each
replication are an empty system and an idle server. The stopping time for each replication
is when the 200th customer departs. Running this simulation experiment for n = 100
replications gave X = 4.72 and for n = 500 replications gave X = 4.76. For this simple
queueing system, the steady-state analytical solution is W =
1
µ−λ = 5.0 (§7.9.3, Fact 1).
These averages are biased low because the early waiting times have a lower expected
value than the subsequent waiting times as a result of the initial conditions. To improve
these point estimates, the system was permitted to warm up for the ﬁrst 100 customers
and the average waiting time was then calculated for the last 100 customers. In this
case, rerunning the simulation gave the improved estimates X = 5.20 for n = 100 and
X = 4.93 for n = 500.
3. Common random numbers: Law [La15, pp. 596–597] compares the M/M/1 and the
M/M/2 queueing models with a utilization of ρ = 0.9 using the waiting times in the
queue of the ﬁrst 100 customers. With n = 100 independent replications of each system,
the two models are compared in four ways:
• independent runs (I);
• arrival streams using common random numbers (A);
• service times using common random numbers (S);
• arrival streams and service times using common random numbers (A & S).
Common random numbers is a variance reduction technique that feeds identical interar-
rival and/or service times into the two diﬀerent queueing models to increase the likeli-
hood that observed diﬀerences in the waiting times are due to the system conﬁgurations
(M/M/1 versus M/M/2) rather than sampling error. The mean half-widths of the con-
ﬁdence intervals (α = 0.10) reported for this example are 0.70(I), 0.49(A), 0.49(S), and
0.04(A & S).
7.10.3
SIMULATION LANGUAGES
This subsection considers the history and features of simulation programming languages
developed over the years.
Facts:
1. The use of a general-purpose simulation programming language (SPL) expedites
model development, input modeling, output analysis, and animation. In addition, SPLs
have accelerated the use of simulation as an analysis tool by bringing down the cost of
developing a simulation model.
2. In a history of the development of SPLs from 1955 to 1986, Nance [Na93] deﬁnes six
requirements that an SPL must meet:
• random number generation;
• random variate generation;
• list processing capabilities so that objects can be created, altered, and deleted;
• statistical analysis routines;

550
Chapter 7
DISCRETE PROBABILITY
• summary report generators;
• a timing executive or event calendar to model the passage of time.
3. SPLs may take the form of
• a set of subprograms in a general purpose language (GPL) such as Fortran or C
that can be called to meet these six requirements;
• a preprocessor that converts statements or symbols to lines of code in a GPL;
• a conventional programming language.
4. The following table shows a division of the historical record into ﬁve distinct periods,
including the names of several languages that came into existence in each period.
period
characteristics
languages
1955–1960
period of search
GSP
1961–1965
the advent
CLP, CSL, DYNAMO, GASP, GPSS,
MILITRAN, OPS, QUIKSCRIPT,
SIMSCRIPT, SIMULA, SOL
1966–1970
formative period
AS, BOSS, Q-GERT, SLANG, SPL
1971–1978
expansion period
DRAFT, HOCUS, PBQ, SIMPL
1979–1986
consolidation and
INS, SIMAN, SLAM
regeneration
5. The General Purpose System Simulator (GPSS) was ﬁrst developed on various IBM
computers in the early 1960s. Algol-based SIMULA was also developed in the 1960s and
had features that were ahead of its time. These included abstract data types, inheritance,
the co-routine concept, and quasi-parallel execution.
6. SIMSCRIPT was developed by the RAND Corporation with the purpose of decreasing
model and program development times. SIMSCRIPT models are described in terms of
entities, attributes, and sets. The syntax and program organization were inﬂuenced by
Fortran.
7. The Control and Simulation Language (CSL) takes an “activity scanning” approach
to language design, where the activity is the basic descriptive unit.
8. The General Activity Simulation Program (GASP), in common with several other
languages, used ﬂow-chart symbols to bridge the gap between personnel unfamiliar with
programming and programmers unfamiliar with the application area. Although originally
written in Algol, GASP provided Fortran subroutines for list-processing capabilities (e.g.,
queue insertion).
9. GASP was a forerunner to both the Simulation Language for Alternative Modeling
(SLAM) and SIMulation ANalysis (SIMAN) languages.
10. SLAM was the ﬁrst language to include three modeling perspectives in one language:
network (process orientation), discrete-event, and continuous (state variables).
11. SIMAN was the ﬁrst major SPL executable on an IBM PC.
12. Simulation software has mushroomed, with numerous packages and languages avail-
able both for general purpose and application-speciﬁc simulations. Special purpose and
integrated packages are widespread and available on desktop computers. The 2015 survey
[Sw15] compares 55 products, having a wide range of features and capabilities.
13. A recent trend has been the addition of animation to intelligently view simulation
output. Several web-based simulation tools can be found at

Section 7.11
THE PROBABILISTIC METHOD
551
• http://www.dcs.ed.ac.uk/home/hase/simjava/applets/index.html
• http://www.vissim.com
• http://www.anylogic.com
• http://www.um.es/fem/EjsWiki/index.php/Main/WhatIsEJS
14. Software for carrying out Monte Carlo simulation can be found at the site
• http://random.mat.sbg.ac.at/software
15. A site that writes code for random variate generation for various probability distri-
butions can be found at
• http://statistik.wu-wien.ac.at/projects/anuran/index.html
16. Winter Simulation Conference proceeding papers on simulation languages, model-
ing, analysis, applications, etc. are available at
• http://www.informs-sim.org
7.11
THE PROBABILISTIC METHOD
The Probabilistic Method is a powerful technique for proving the existence of combina-
torial structures with certain desired properties. This method seems ubiquitous because
it provides a technique to deal with the “local-global” problem. More speciﬁcally, when
one is confronted with the task of constructing a ﬁnite set structure that satisﬁes a cer-
tain combinatorial condition at every element, the total number of choices often grows
exponentially, so it is very diﬃcult to decide which choices are good. The probabilistic
paradigm provides what one may call conditions for building a global patch from all the
local data. The basic premise is the following: Deﬁne a suitable probability space and
then prove that the probability of making no “bad choice” is positive. In particular, its
power is most visible when it is used to prove deterministic statements. We describe here
brieﬂy some of the main aspects of the probabilistic method. There are numerous books,
monographs, and lecture notes on the subject including [AlSp08], [MoRe00], [Sp87].
7.11.1
THE BASIC METHOD
Deﬁnitions:
Suppose (Ω, Pr) is a probability on a ﬁnite set Ωand let X : Ω→R be a random variable
on Ω. The expectation of X is given by E(X) = P
ω∈ΩX(ω) Pr(ω).
The Ramsey number R(k, k) is the least integer N such that for every two-coloring
of the edges of the complete graph Kn with n ≥N, there is a monochrome complete
subgraph on k vertices.
A hypergraph is said to be n-uniform if each edge size is n. We say that a hypergraph
H = (V, E) has Property B (after Felix Bernstein, 1878–1956) if each vertex in V can
be assigned one of two colors so that no edge E ∈E is monochromatic, i.e., for no edge
E ∈E is it the case that every vertex of E has been assigned the same color.
A set B ⊂R is called sum-free if the sum of any two elements in B does not lie in B.

552
Chapter 7
DISCRETE PROBABILITY
The girth of a graph is the size of a smallest cycle in the graph. If the graph is acyclic
then its girth is deﬁned to be ∞.
Facts:
1. Suppose (Ω, Pr) is a probability on the ﬁnite set Ωand X, Y : Ω→R are random
variables on Ω. Then
E(X + Y )
=
E(X) + E(Y ),
E(λX)
=
λE(X), for real λ.
This is expressed by saying that expectation is linear.
2. If X is a random variable deﬁned on a probability space (Ω, Pr), then Pr(X ≥
E(X)) > 0 and Pr(X ≤E(X)) > 0. In particular, with positive probability, there exist
ω, ω′ ∈Ωsuch that X(ω) ≥E(X) and X(ω′) ≤E(ω′).
3. Markov’s inequality: If X : Ω→R+ is a nonnegative valued random variable, then
for any a > 0
Pr(X > a) ≤E(X)
a
.
Examples:
We include several examples that illustrate how the method basically works.
These
examples span several areas of discrete mathematics, including additive combinatorics,
extremal combinatorics, combinatorial geometry, and graph theory.
1. Lower bound for R(k, k):
We shall show that R(k, k) > 2k/2 [Er47]. To do so,
color each edge of Kn red or blue independently, and with equal probability. Then the
probability that there exists a monochrome Kk under this coloring is at most
 n
k

2−(k
2)+1
since there are
 n
k

distinct k subsets of {1, 2, . . ., n} and for each ﬁxed k subset, the
probability that all the edges incident with those vertices bear the same color is 2−(k
2)+1.
Now, a routine calculation shows that for n = 2k/2 this expression is less than one under
a random two-coloring. In particular, with positive probability, no k-subset of the vertex
set induces a monochromatic Kk.
2. Colored hats and a guessing game:
There are n friends standing in a circle so
that everyone can see everybody else. On each person’s head a randomly chosen hat—
either black or white—is placed. After they have had a look at each other, they must
simultaneously shout, “my hat is black”,“my hat is white”, or “I don’t know the color
of my hat”.
The friends may make any strategy prior to the placement of the hats
but subsequent to that, they are not allowed to communicate in any manner. They are
awarded a grand prize if at least one person gets the color of her hat correct, and no one
gets her hat color wrong. In the latter case, they are all punished. The friends need to
devise a strategy that increases the probability of their getting the reward.
One strategy for the beating the 50-50 odds is the following. Denoting white and black
by 1 and 0 respectively, any conﬁguration of hats on their heads is a point in {0, 1}n.
Suppose there exists a set L ⊂{0, 1}n such that for every element (x1, x2, . . . , xn) ∈W =
{0, 1}n\L there is an element (y1, y2, . . . , yn) ∈L such that the set {i | xi ̸= yi} has size 1.
In this case we say that the set L is desirable. Here is a strategy for the n friends: Person i
knows xj for all j ̸= i, so if there is a unique value of xi so that (x1, x2, . . . , xn) ∈W then
person i declares that her hat color is xi. It then follows that the probability of getting the
award by following this strategy is |W|/2n. Thus to maximize this probability, they need
a “small” desirable set L. This can be obtained by making a random choice. Pick a set X
by choosing each element of {0, 1}n independently with probability p (to be determined).

Section 7.11
THE PROBABILISTIC METHOD
553
If Y is the set of elements which diﬀer from the chosen elements in at least two coordinates
then E(|Y |) = 2n(1 −p)n+1, so the set L = X ∪Y satisﬁes the required condition.
Furthermore, by linearity of expectation E(|L|) = E(|X| + |Y |) = 2n(p + (1 −p)n+1).
Minimizing this over p ∈[0, 1] we get E(|L|) ≤2nO

log n
n

. In particular, the n friends
can achieve a success rate of 1 −O

log n
n

. One can also prove similar results when the
friends are assigned hats that may take any one of q diﬀerent colors, for any q ≥2. See
[Al08] for other related results.
3. Property B in uniform hypergraphs:
Denote by m(n) the least number of edges in
an n-uniform hypergraph which does not have Property B. Then we have [Er63], [Er64]
2n−1 ≤m(n) ≤O(n22n).
For the lower bound, we need to show that any n-uniform hypergraph with fewer than
2n−1 edges is 2-colorable. To see that, given such a hypergraph H, randomly and uni-
formly and independently color each vertex red or blue. Then if N denotes the number of
monochrome edges, then E(N) = P
E∈E Pr(E is monochrome) < 1 since the probability
that any given edge is monochrome is precisely 1/2n−1. Thus there is some choice of
vertex coloring such that no edge is monochrome.
For the upper bound, we pick a random hypergraph as follows.
Let V be the set
{1, 2, . . ., v} and choose m edges from the set of all n-subsets of V , independently at
random: speciﬁcally, pick an edge E with probability 1/
 v
n

, and repeat this process over
m rounds. Here v, m will be chosen later. Now, ﬁx a partition Π of V into two parts of
sizes a, b. Then the probability that an edge that is not monochrome for Π is selected
for H equals
 a
n

+
 b
n

 v
n

≥2
 v/2
n

 v
n
 ,
so the probability that there exists a partition Π such that the corresponding coloring
for H has no monochrome edges is at most
(# of 2-colorings of V ) · (1 −P)m ≤2ve−P m,
where P denotes the quantity
2(v/2
n )
(v
n) . Some calculation and optimizing for m shows that
for v = n2/2 and n large enough we have m ≤n2
2 ln(2) · 2n−1 · e1+o(1). It is conjectured
that m(n)/2n = Θ(n). The best known result is m(n)/2n = Ω(
q
n
log n). [RaSr00]
4. Sum-free subsets of a given set: Given a set B ⊂N of size n, how large a sum-free
subset can it contain? We will show that B contains a sum-free subset of size greater
than |B|/3. [Er65], [AlKl90]
Pick a prime p of the form 3k +2 which is larger than twice the maximum absolute value
of elements in B. By our choice of p, all of the elements in B are distinct mod p. Now,
consider the sets xB = {xb | b ∈B} in Z/pZ, and let N(x) = |[k + 1, 2k + 1] ∩xB| .
Now note in order to ﬁnd a sum-free subset of B of the desired size, it suﬃces to ﬁnd
some x ∈Z/pZ such that N(x) > n/3 since the set {k + 1, k + 2, . . . , 2k + 1} in Z/pZ
is sum-free. To ﬁnd such an x, pick x ∈(Z/pZ) \ {0} uniformly at random. Then
E(N(x)) = P
b∈B Pr (x · b ∈[k + 1, 2k + 1]) = n · k+1
3k+1 > n/3.
This again implies that there exists some x such that N(x) > n/3 and that completes
the proof.

554
Chapter 7
DISCRETE PROBABILITY
Another way to state this result is as follows. The best constant c for which every set of
integers of size n contains a sum-free subset of size at least cn is at least as large as 1/3.
It has been shown that in fact 1/3 is the best possible constant [EbGrMa14].
Alterations:
Sometimes the probabilistic method may not produce the desired object itself but can
prove the existence of an object which is very “close” to what is desired.
1. Improvement on R(k, k): Color the edges of the complete graph Kn red or blue with
equal probability, and independently for distinct edges. Then the expected number of
monochrome copies of Kk is m =
 n
k

2−(k
2)+1. Thus there is a coloring of the edges in
which there are at most m monochrome copies of Kk. Now, from each such monochrome
copy, delete a vertex; then the resulting graph on n −m vertices has no monochrome
Kk. Thus we get R(k, k) > n −
 n
k

2−(
k
2)+1. Optimizing this for n (for ﬁxed k) we get
R(k, k) > ( 1
e + o(1))k2k/2.
2. A conjecture of Danzer-Gr¨unbaum in combinatorial geometry: Ludwig Danzer and
Branko Gr¨unbaum [DaGr62] conjectured the following: “Any conﬁguration of 2n points
in Rn contains some three points that form a non-acute angle”. In fact, they managed
a proof of the same for dimensions 2, 3. Following [ErFu83], we give a proof that re-
futes this conjecture for all n ≥40. To do so, we try to pick a subset of vertices of
the n-dimensional hypercube, and try to avoid right angles. Since the vertices of the
hypercube are in one to one correspondence with the set of subsets of {1, 2 . . ., n}, we
can denote the vertices picked instead by subsets of {1, 2, . . ., n}. Now pick each sub-
set of {1, 2, . . ., n} independently with probability p for some p to be determined. Let
X = #{{P, Q, R} | ∠PQR = π
2 }. Suppose the sets P, Q, R form a right angle. If we
denote by VP the vector in Rn corresponding to the set (vertex) P of the cube, then
⟨VP −VQ, VR −VQ⟩= 0 where ⟨·, ·⟩is the usual dot product in Rn. This implies that
∅= ¯P ∩Q ∩¯R and ∅= P ∩¯Q ∩R. Therefore {P, Q, R} is a right angle triple if and
only if P ∩R ⊂Q ⊂P ∪R. Hence if the set of points picked is S, then E(|S|) = 2np
and it is not diﬃcult to see that E(|X|) = 6np3
2 . Now if we remove one of the sets from
each triple of X that appear among the sets chosen in S then the resulting set S′ has
no right angles either, so there is a subset S′ of expected size at least 2np −6np3
2
with
no right angles. Optimizing for p, we get E(|S′|) ≥1
3
q
2
3

2
√
3
n
, so there is a set of size
Ω(( 2
√
3)n) in the n-dimensional hypercube in which no three points form a right angle.
For n ≥40, this exceeds 2n.
For more recent and better bounds (including deterministic constructions) for large sets
with only acute angles, and other generalizations, see [Be06].
3. Graphs with arbitrarily large girth and large chromatic number: Given k, g ≥3
there exist graphs G with χ(G) > k and girth greater than g. The sketch of the proof
of this result, due to Erd˝os [Er59], is as follows. Take the vertex set V = {1, 2, . . ., n}
and for each pair i, j select the corresponding edge with probability p = n−1+θ for some
0 < θ < 1/g, independently for each pair. By the choice of p it follows that the probability
that this graph has an independent set of size greater than O(n1−θ log n) is o(1). Also,
if X denotes the number of cycles in G of size at most g then E(X) = o(n), so with
high probability (probability greater than 1
2) X < n/2. Hence with positive probability
there exists a graph for which the number of cycles of size at most g is less than n/2 and
the size of maximum independent set in G is at most O(n1−θ log n). Fix such a graph
and remove a vertex from each small cycle (i.e., a cycle of size at most g). The resulting
graph has no small cycles, at least n/2 vertices, and χ(G) > Ω

nθ
log n

.

Section 7.11
THE PROBABILISTIC METHOD
555
7.11.2
DEPENDENT RANDOM CHOICE
For some instances of proving the existence of a combinatorial structure with certain
desired properties, it is better to choose a diﬀerent object randomly and then pick a
relevant object associated with the randomly chosen object to be our desired object. Since
we do not choose our actual objects of interest randomly but rather in some dependent
manner, this method is referred to as the method of Dependent Random Choice.
Deﬁnitions:
For a graph H, ex(n; H) denotes the maximum number of edges in a graph on n vertices
without a copy of H.
For a graph H, the Ramsey number R(H) denotes the minimum number of vertices
N such that every two coloring of the edges of KN admits a monochrome copy of H.
The n-dimensional hypercube Qn is the graph whose vertices are all the binary se-
quences of length n; two vertices are adjacent if and only if they diﬀer in exactly one
coordinate.
Examples:
1. Suppose H is bipartite with vertex partition A ∪B and let |A| = a, |B| = b. In
[AlKrSu03], the authors prove that if every vertex of B has degree at most r, then
ex(n; H) = O(n2−1
r ). To see this, we shall try to embed each vertex of H into any graph
G with Cn2−1/r edges for some absolute constant C. First we show that G contains a
subset of vertices A0 of size at least a + b with the property that every r-subset of A0
has at least a + b common neighbors in G. To achieve this, pick a subset T ⊂V (G) over
t rounds, at random (with repetition, and uniformly over each round). Consider the set
of common neighbors of T which we denote by N ∗(T ). If d denotes the average degree
of the vertices of G, then
E(|N ∗(T )|) = P
v∈V Pr(v ∈N ∗(T )) = P
v∈V

d(v)
n
t
≥
dt
nt−1 .
Let Y denote the number of r-subsets U of N ∗(T ) such that U has fewer than a + b
common neighbors. If U is a subset of size r with fewer than a + b neighbors, the event
U ⊂N ∗(T ) occurs only if every element of T was picked from among the common
neighbors of U, so Pr(U ⊂N ∗(T )) ≤( a+b
n )t, and hence by linearity of expectation,
E(Y ) ≤
 n
r

( a+b
n )t. So, E(|N ∗(T )| −Y ) ≥
dt
nt−1 −
 n
r

( a+b
n )t. Thus it follows that there
exists a set A0 ⊂N ∗(T ) of size ≥
dt
nt−1 −
 n
r

( a+b
n )t such that every r-subset of A0 has at
least a+b common neighbors in G. Now since V (H) = A∪B, we embed the vertices of A
into A0 arbitrarily. To embed the vertices of B, let B = {v1, v2, . . . , vb} and suppose that
we have already embedded v1, . . . , vi−1 into A0. Let W denote the set of neighbors of vi
in A (in the graph H). Since A has been embedded into A0, this gives a set U ⊂V (G)
of size at most r corresponding to the set of neighbors of vi. But every U of size at most
r has at least a + b neighbors in G, of which at most a + i −1 (with i < b) vertices are
already assigned to some vertex of A ∪B. Hence, vi can be embedded into A0 as well.
To complete the proof, some routine calculations show that for C > 1
2(a + (a+b)r
r!
)
1
r we
have |A0| ≥a + b.
2. The Ramsey number R(Qn):
Burr and Erd˝os [BuEr75] conjectured that R(Qn) =
O(2n).
This conjecture is yet unsettled, but we show here (following [FoSu11]) that
R(Qn) ≤23n. Set N = 23n and consider the denser of the two colors that appear in

556
Chapter 7
DISCRETE PROBABILITY
a two coloring of the edges of KN. There are at least 1
2
 N
2

edges of this color, so the
average degree of the graph G induced by edges of this color is at least 2−4/3N. Set
t = 3n/2, m = 2n and a = 2n−1. Then
dt
N t−1 −
 N
n
   m
N
t ≥2−4/3N −N n−tmt/n! ≥2n−1.
As in the previous proof, it follows that the graph G contains a set U of size 2n−1 with
the property that every n vertices of U have at least 2n common neighbors. Since Qn is
an n-regular bipartite graph with parts of size 2n−1, it follows that G contains Qn.
3. For additional examples, see the survey paper [FoSu11].
7.11.3
THE SECOND MOMENT
Deﬁnitions:
The variance of the random variable X is Var(X) = E(X −E(X))2.
A set of positive integers {x1, x2, . . . , xk} has distinct sums if the quantities P
i∈S xi
are all distinct for all subsets S ⊆{1, 2, . . ., k}.
Facts:
1. Chebyshev’s inequality: For a random variable X : Ω→R and any λ > 0
Pr(|X −E(X)| > λ) ≤Var(X)
λ2
.
2. If X is a nonnegative integer-valued random variable, then
Pr(X = 0) ≤Var(X)
E(X)2 .
3. If Var(X) = o(E(X)2), then asymptotically almost surely X ∼E(X).
Examples:
1. Erd˝os posed the question of estimating the maximum size f(n) of a set {x1, x2, . . . , xk}
with distinct sums and xk ≤n. We shall show that f(n) ≤log2 n + 1
2 log2 log2 n + O(1).
Suppose X = {x1, x2, . . . , xk} is a maximum sized set with distinct sums. Pick a random
subset S of {1, 2, . . ., k} by selecting each element independently with probability 1/2.
This subset yields the random sum XS = P
i∈S xi with E(XS) = 1
2(x1 + x2 + · · · + xk).
Also, Var(XS) = 1
4(x2
1 + x2
2 + · · · + x2
k) ≤n2k
4 , so by Chebyshev’s inequality we have
Pr(|XS −E(XS)| < λ) ≥1 −n2k
4λ2 . Since X has distinct sums and there are 2k distinct
subsets of {x1, x2, . . . , xk}, it follows that for each integer r we have that Pr(XS = r) is
either
1
2k or 0, depending on whether r appears as a sum of some subset of X or not.
This observation coupled with Chebyshev’s inequality gives
1 −n2k
4λ2 ≤Pr(|XS −E(XS)| < λ) ≤2λ+1
2k .
Optimizing for λ gives the desired result.
2. Given a set X of at least 4k2 distinct residue classes modulo a prime p, there exists an
integer a such that {ax (mod p) | x ∈X} intersects every cyclic interval in {0, 1, . . ., p −
1} of length at least p/k (see [AlKrNe95], [AlPe92]). To see this, we ﬁrst note that if
some shift aX + b satisﬁes this property, then so does the set aX.

Section 7.11
THE PROBABILISTIC METHOD
557
We start with a simple observation. Let I1, I2, . . . , I2k be a (ﬁxed) covering of {0, 1, . . ., p−
1} by 2k intervals of length ⌈p/2k⌉each. Suppose that a set Y intersects each of these in-
tervals Ij; then in particular, since every cyclic interval J of size at least p/k must contain
some Ij it follows that Y ∩J ̸= ∅. Select a ∈{1, 2, . . ., p−1} and b ∈{0, 1, . . ., p−1} uni-
formly and independently, and let Xa,b = {ax+b | x ∈X}. Here the arithmetic is modulo
p. Fix an interval Ij and consider the random variables Xj
x (for x ∈X) which equal one
if ax + b ∈Ij and zero otherwise. Setting Nj = P
x∈X Xj
x = |Xa,b ∩Ij| we see that
E(Nj) = P
x∈X Pr(ax + b ∈Ij) ≥2k. Furthermore, Var(Nj) = P
x∈X Var(Xx) ≤2k.
The equality in the above holds because the random variables Xj
x (as x varies over X) are
pairwise independent, though they are not all mutually independent. Hence, by Cheby-
shev’s inequality, Pr(Nj = 0) < 1/2k. In particular, the probability that Nj = 0 for
some j is at most P
j Pr(Nj = 0), and this is strictly less than 1. Consequently, there
exist a, b such that Xa,b intersects every Ij, and this completes the proof.
7.11.4
OCCURRENCE OF RARE EVENTS: THE LOCAL LEMMA
In all the examples seen so far, the probability of the “bad” event(s) is in fact negligi-
ble. But in order to prove that a “good” event occurs, we only need to show that the
probability of the bad event is strictly less than one.
Deﬁnitions:
Suppose E1, E2, . . . , EN are events in a probability space (Ω, Pr). A graph D = (V, E)
with vertex set V = {Ei | i = 1, . . . , N} is called a dependency digraph of the events
Ei if each Ei is independent of the set {Ej | (Ei, Ej) ̸∈E}. In words, Ei is independent
of all the events Ej whenever Ei and Ej are nonadjacent in D. We write i ∼j (resp.
i ̸∼j) to denote (Ei, Ej) ∈D (resp. (Ei, Ej) /∈D).
Suppose graph G admits a partition V = V1 ∪V2 ∪· · · ∪Vr of its vertex set. We say a set
I is an independent transversal for this partition if I contains exactly one element
from each Vi and is independent in G.
Fix k ≥2. For a k coloring c : R →{1, 2, . . ., k} of R and a ﬁxed subset T ⊂R we
say that T is multicolored with respect to c if every color appears in the coloring
induced on T .
A proper k-coloring of (the vertices of) a graph G = (V, E) is a map C : V →
{1, 2, . . ., k} such that for adjacent vertices x and y we have C(x) ̸= C(y).
For any
integer β > 1 a proper vertex coloring C of G is β-frugal if for any vertex v and any
color c there are at most β neighbors of v that are colored c.
Facts:
1. Lov´asz Local Lemma - symmetric case:
Suppose E1, E2, . . . , EN are events in a
probability space (Ω, Pr) and D is a dependency graph for these events. Let d denote
the maximum vertex degree in D, and let 0 < p < 1 be such that Pr(Ei) ≤p for all i. If
ep(d + 1) ≤1, then
Pr
 E1 ∧E2 ∧· · · ∧EN

> 0.
In particular, with positive probability, none of the events Ei occur. Here, e ≈2.71828
denotes the Euler constant. [ErLo75]
2. Lov´asz Local Lemma - general case:
Suppose E1, E2, . . . , EN are events in a prob-
ability space (Ω, Pr) and D is a dependency graph for these events. Suppose there are

558
Chapter 7
DISCRETE PROBABILITY
real numbers xi, i = 1, . . . , N satisfying 0 ≤xi < 1 so that for each event Ei we have
Pr(Ei) ≤xi
Q
j̸∼i(1 −xj).
Then
Pr
 E1 ∧E2 ∧· · · ∧EN

≥QN
i=1(1 −xi) > 0.
Again, with positive probability, none of the events Ei occur. [ErLo75]
Examples:
1. Independent transversals:
Suppose the graph G admits a vertex partition V =
V1 ∪V2 ∪· · · ∪Vr and the maximum vertex degree in G equals d. Suppose further that
each set Vi has size at least 2ed. Then there exists an independent transversal in G for
the partition (V1, V2, . . . , Vr). [Al88]
To see this, pick vi ∈Vi independently and uniformly for each i. For each edge (x, y),
let Exy denote the event that both vertices x and y are selected. Our chosen set is an
independent transversal if and only if for each edge (x, y) ∈E(G), the event Exy does
not occur. Consider the graph D whose vertex set consists of the edges of G in which two
edges are adjacent in D if and only if they intersect. Then D is a dependency graph for
the events Exy for each (x, y) ∈E(G). To use the local lemma, note that the probability
that both vertices of any ﬁxed edge e ∈E(G) are included in the transversal is at most
1/(2ed)2, and since each vertex in G has degree at most d, the maximum degree of D is
at most 2d. Hence by the local lemma, it follows that with positive probability, none of
the events Exy occur, since e(2d+1)
(2ed)2 ≤1.
2. A question of Straus:
Does there exist m(k) > 0 such that for any given ﬁxed
S of size m, there exists a k-coloring of R such that every translate x + S, x ∈R is
multicolored? It turns out that m(k) = (3 + o(1))k log k suﬃces. [ErLo75]
To see this, ﬁrst let X be a ﬁnite set of real numbers and let X = S
x∈X(x + S). Now
color every element of X from {1, 2, . . ., k} independently and uniformly. We claim that
Pr(∧x∈X((x + S) is not multicolored)) > 0. Fix x and denote the event that x + S is
not multicolored by Ex. Then
Pr(Ex) ≤k
 1 −1
k
m .
Moreover, Ex is independent of all Ey as long as (x + S) ∩(y + S) = ∅. Hence by the
local lemma, we are done provided ekm2  1 −1
k
m ≤1. It is a routine calculation to see
that this holds if m > (3 + o(1))k log k. Hence it follows that for every ﬁnite set X, there
exists a color choice for the elements of X such that each translate x + S is multicolored.
This coupled with a compactness argument shows that the same result holds for X = R.
3. Frugal colorings:
Suppose G is a regular graph of degree ∆, i.e., every vertex of
G has degree ∆. Then for ∆suﬃciently large, there is a proper β-frugal coloring of G
using at most O(∆1+ 1
β ) colors [HiMoRe97]. To see this, as always, color each vertex
uniformly and independently from the set of colors {1, 2, . . ., N} where N = m∆1+ 1
β
for some suitable constant m that shall be determined later. For each edge uv we con-
sider the event Auv that both u and v have been assigned the same color. Likewise,
for each independent set v1, v2, . . . , vβ+1 of vertices that are also neighbors of some ver-
tex x in G, we consider the event B(v1, v2, . . . , vβ+1) that all the vertices v1, . . . , vβ+1
have been assigned the same color.
This random coloring describes a frugal color-
ing if none of the events Auv or B(v1, v2, . . . , vβ+1) occur. Note that each event Auv
is independent of all other Awz as long as {u, v} ∩{w, z} = ∅and independent of

Section 7.11
THE PROBABILISTIC METHOD
559
B(v1, v2, . . . , vβ+1) if {v1, v2, . . . , vβ+1} ∩{u, v} = ∅.
Similarly, B(v1, v2, . . . , vβ+1) is
independent of B(w1, w2, . . . , wβ+1) if {v1, v2, . . . , vβ+1} ∩{w1, w2, . . . , wβ+1} = ∅.
Let x = 2/N and y = 2/N β. Following a few routine calculations, it follows that
Pr(Auv) ≤x(1 −x)2∆−2(1 −y)2∆(∆
β)
and
Pr(B(v1, v2, . . . , vβ+1)) ≤y(1 −x)(β+1)∆(1 −y)(β+1)∆(∆
β),
for m = 12, so by the general version of the local lemma the proof follows.
To see how good the above bound is, consider the following bipartite graphs Gβ, for
any integer β > 1. The vertex set consists of the 1-dimensional subspaces (points) and
(β + 1)-dimensional subspaces (β + 1 ﬂats) of a (β + 2)-dimensional vector space over
a ﬁnite ﬁeld of q elements. A point p and a β + 1 ﬂat Π are adjacent in Gβ if p is a
subspace of Π. Note that no color can be used more than β times in a β-frugal coloring.
Since any β + 1 distinct points together lie in a (β + 1)-dimensional ﬂat, the number of
colors needed is at least (qβ+1 +qβ +· · ·+1)/β. Since the maximum degree in Gβ equals
qβ + qβ−1 + · · · + q + 1, the number of colors needed in a β-frugal coloring is at least
Ω(∆1+1/β). Hence the result above is tight up to the constant factor C.
7.11.5
BASIC CONCENTRATION INEQUALITIES
Deﬁnitions:
For a hypergraph H, a 2-coloring χ of the vertices of H, and an edge e ∈E(H) the
discrepancy D(e) of the edge e is the absolute value of the diﬀerence between the
number of vertices in the two color classes. The discrepancy of H with respect to χ
denoted D(H, χ) is deﬁned as the maximum D(e) as e ranges over the edges in H. The
discrepancy of the hypergraph H is D(H) = minχ D(H, χ) where χ ranges over all
2-colorings of H.
For graphs G1 = (V1, E1) and G2 = (V2, E2) the Cartesian product of G1 and G2
denoted G1□G2 is the graph with vertex set V1 × V2 where vertices (u1, u2) and (v1, v2)
are adjacent if either u1 = v1 and (u2, v2) ∈E2 or (u1, v1) ∈E1 and u2 = v2. The
Cartesian product of graphs is associative and commutative.
For vertices u, v in a graph G, the distance dG(u, v) between u and v in G is the length
of a shortest u-v path in G. If u and v lie in diﬀerent connected components, then we
write dG(u, v) = ∞. For a subset of vertices U, dG(x, U) = minu∈U dG(x, u).
Facts:
1. Chernoﬀbounds [Ch52]:
Suppose X is a binomial random variable B(n, p), and
λ = E(X) = np. Then for t ≥0
Pr(X ≥E(X) + t)
≤
exp

−
t2
2(λ+t/3)

,
Pr(X ≥E(X) −t)
≤
exp

−t2
2λ

.
2. Chernoﬀbounds - general case:
If Xi ∼Ber(pi), i = 1, . . . , n and Xi are mutually
independent Bernoulli random variables deﬁned on Ωwith parameters pi, then the same
inequalities as above hold for X = Pn
i=1 Xi with λ = E(X) = Pn
i=1 pi.

560
Chapter 7
DISCRETE PROBABILITY
3. Simple concentration bound: Suppose X1, X2, . . . , Xn are independent random vari-
ables in (Ω, Pr) and suppose f(x1, x2, . . . , xn) : Rn →R is a real-valued function that
satisﬁes the Lipschitz condition, i.e., |f(x1, x2, . . . , xi, . . . , xn)−f(x1, x2, . . . , x′
i, . . . , xn)| ≤
ci for each i and for some ci > 0. If X = f(X1, X2, . . . , Xn), then
Pr(|X −E(X)| > t) ≤2 exp

−
t2
2 Pn
i=1 c2
i

.
4. Azuma’s inequality [Az67]:
Let X0, X1, . . . , Xn be a martingale such that |Xi+1 −
Xi| ≤ci for each i and some reals ci > 0. Then for λ > 0
Pr(Xn −X0 > λ)
≤
exp

−
λ2
2 Pn
i=1 c2
i

,
Pr(Xn −X0 < −λ)
≤
exp

−
λ2
2 Pn
i=1 c2
i

.
Examples:
1. Discrepancy: Let H be a k-uniform hypergraph with k edges.
Color each ver-
tex uniformly and independently using one of the two colors.
We shall show that
with positive probability, this random coloring χ satisﬁes D(H, χ) ≤√8k log k, so that
D(H) ≤√8k log k. For each edge e, we have by the Chernoﬀbounds that Pr(D(e) >
2t) ≤2e−t2/(3k/2) <
1
k for t = √k log k. Hence the probability that D(e) exceeds 2t
for some edge e ∈H is less than 1, so that with positive probability, χ has the desired
property.
2. A result of Erd˝os, Silverman, and Stein [ErSiSt83]:
Let Πn be a projective plane
of order n. There exists a subset S of points, and absolute constants k, K such that
k log n ≤|L ∩S| ≤K log n for every line L in Πn.
To see this, choose S at random, with each point x placed in S with probability p = f(n)
n+1 ,
for some f(n) to be determined later. Fix a line L, and let SL = |S ∩L|. Note that
E(SL) = (n + 1)p = f(n). By the Chernoﬀbound, Pr[|SL −f(n)| > f(n)] < 2e−f(n)/3.
Since Πn contains n2 + n + 1 lines,
Pr (there exists L such that |Sl −f(n)| > f(n)) < 4e−f(n)/3n2.
Therefore, if ef(n)/3 > 4n2, the desired S exists. It is easy to check that f(n) = 3 log 4n2
works for the aforementioned argument.
3. Consider the graph Gr,n = K□n
r
, the r-fold Cartesian product of the complete graph
Kr. For ǫ > 0, let W ⊂Gr,n be a subset of size at least ǫrn and let U be the set of vertices
of Gr,n whose distance from W is greater than 2√n log n. We claim that |U| ≤Crn
n2
for
some absolute constant C that depends only on ǫ.
Suppose x is a vertex of Gr,n obtained by choosing v1, v2, . . . , vn uniformly and indepen-
dently among the vertices of Kr. Consider the Doob martingale process {Yi}i=0,1,...,n of
exposing one vi at a time, i.e., consider the random variables Yi = E(d(x, W)|v1, v2, . . . , vi)
for i = 0, 1, . . ., n (here by convention, Y0 = E(d(x, W))) where we write d(x, W) =
dG(x, W) for simplicity. {Yi} is a martingale with ci = 1 for each i since altering vi
changes the value of Yi−1 by at most one. By Azuma’s inequality, we have Pr(d(x, W)−
E(d(x, W)) < −c√n) ≤e−c2/2. Setting c <
p
2 log(1/ǫ) we see that Pr(d(x, W) <
E(d(x, W) −c√n) < ǫ. However, note that since |W| ≥ǫrn, we have Pr(d(x, W) =
0) ≥ǫ, so in particular, E(d(x, W) <
p
2 log(1/ǫ)n. On the other hand, we also have
Pr(d(x, W) −E(d(x, W)) > c√n) ≤e−c2/2, so setting c = 2√log n we get Pr(d(x, W) >
2√n log n) ≤O(1/n2). This completes the proof.

Section 7.11
THE PROBABILISTIC METHOD
561
7.11.6
CORRELATION INEQUALITIES
Deﬁnitions:
A family of subsets A of {1, 2, . . ., n} is called an upset if whenever A ∈A and A ⊂A′
then A′ ∈A as well.
A family of subsets A of {1, 2, . . ., n} is called a downset if
whenever A ∈A and A′ ⊂A then A′ ∈A as well.
Suppose L is a ﬁnite distributive lattice, i.e., a lattice in which for any x, y, z ∈L we
have (x ∧y) ∨z = (x ∨z) ∧(y ∨z). A function µ : L →R+ is called log-supermodular
if µ(x)µ(y) ≤µ(x ∨y)µ(x ∧y).
A real-valued function f deﬁned on a ﬁnite distributive lattice is called increasing if
f(x) ≤f(y) whenever x ≤y. Similarly, a function f is called decreasing if f(x) ≥f(y)
whenever x ≤y.
Facts:
1. Four function theorem [AhDa78]:
Suppose α, β, γ, δ are nonnegative real-valued
functions deﬁned on the set of all subsets of {1, 2, . . ., n} and suppose for any sets A, B ⊂
{1, 2, . . ., n}
α(A)β(B) ≤γ(A ∪B)δ(A ∩B)
holds. Then for any families of sets A, B
α(A)β(B) ≤γ(A ∪B)δ(A ∩B),
where for any function φ, φ(A) denotes the sum P
A∈A φ(A).
2. Four function theorem - lattice version: Suppose α, β, γ, δ are nonnegative real-valued
functions deﬁned on the distributive lattice L and suppose that for any a, b ∈L
α(a)β(b) ≤γ(a ∨b)δ(a ∧b).
Then for any subsets A, B ⊂L
α(A)β(B) ≤γ(A ∨B)δ(A ∧B),
where A ∨B = {a ∨b | a ∈A, b ∈B} and A ∧B = {a ∧b | a ∈A, b ∈B}.
3. FKG inequality [FoKaGi71]: Let L be a ﬁnite distributive lattice and let µ : L →R+
be a log-supermodular function. Then for any two increasing functions f, g
 P
x∈L µ(x)f(x)
  P
x∈L g(x)µ(x)

≤
 P
x∈L µ(x)f(x)g(x)
  P
x∈L µ(x)

.
4. Correlation inequalities:
Suppose A, B, C, D are families of subsets of {1, 2, . . ., n}
with A, B being upsets and C, D being downsets. Pick a random subset R of {1, 2, . . ., n}
by picking each i ∈{1, 2, . . ., n} independently with probability 0 ≤pi < 1. Then
Pr(R ∈A ∩B)
≥
Pr(R ∈A) · Pr(A ∈B),
Pr(R ∈C ∩D)
≥
Pr(R ∈C) · Pr(R ∈D),
Pr(R ∈A ∩C)
≤
Pr(R ∈A) · Pr(R ∈C).

562
Chapter 7
DISCRETE PROBABILITY
Examples:
1. Suppose 0 < α1 ≤α2 ≤· · · ≤αn and 0 < β1 ≤β2 ≤· · · ≤βn are increasing
sequences of positive real numbers. Then
n P
i αiβn+1−i ≤(P
i αi) (P
i βi) ≤n P
i αiβi.
To prove the ﬁrst inequality, we use induction on n; if the inequality holds for n −1,
then applying the inequality for the sequences ai = (α1, α2, . . . , αi−1, αi+1, . . . , αn) and
bi = (β1, β2, . . . , βn−i, βn−i+2, . . . , βn) and summing these inequalities as i ranges over
1, 2, . . ., n, we get
P
i(A −αi)(B −βn+1−i) ≥(n −1)2 P
i αiβn+1−i,
where A = P
i αi, B = P
i βi. This inequality simpliﬁes to (n −2)AB + P
i αiβn+1−i ≥
(n −1)2 P
i αiβn+1−i, which establishes the inductive hypothesis. For the second in-
equality, consider the distributive lattice L = {1, 2, . . ., n} with i ∨j = max{i, j}, i ∧j =
min{i, j}. It is easy to check that α(i) = αi, β(i) = βi, γ(k) = max{αiβj | i ∨j = k},
δ(i) = 1 satisfy the hypotheses of the four function theorem. Furthermore, since the se-
quences αi, βi are nonnegative and increasing we have γ(i) ≤αiβi. Now the four function
theorem gives AB ≤P
i nγ(i) ≤n P
i αiβi which is what we seek.
2. Suppose A is a family of subsets of {1, 2, . . ., n} such that for any A, B ∈A both
A ∩B ̸= ∅and A ∪B ̸= {1, 2, . . ., n} hold. Then |A| ≤2n−2. To see why, let C be a
maximal intersecting family containing A, and let D be a maximal family containing A
with no two sets having {1, 2, . . ., n} as their union. Note that the maximality of C and
D implies that C is an upset and D is a downset. Since A ⊂C ∩D, by the correlation
inequalities
|A| ≤|C ∩D| ≤|C|·|D|
2n
= 2n−2.
This is also clearly best possible, e.g., consider the family A = {A | 1 ∈A, n /∈A}.
3. Suppose A1, A2, . . . , Ak (for k ≥1) are intersecting families on {1, 2, . . ., n}. Then
how large can | Sk
i=1 Ai| be? Clearly, if we set Ai to be the intersecting family of all
subsets of {1, 2, . . ., n} containing i, then Sk
i=1 Ai consists of all those sets that contain
at least one of the elements of {1, . . . , k}, so clearly | Sk
i=1 Ai| = 2n −2n−k. We claim
that this is indeed best possible. For k = 1 this follows since for any set A ⊂{1, 2, . . ., n}
we must have |A∩{A, A}| ≤1, so an intersecting family of {1, 2, . . ., n} contains at most
2n−1 sets. In general, let B = Sk−1
i=1 Ai. Since each Ai is an intersecting family, we may
assume that each Ai is an upset since adding larger sets will not violate the property
that Ai is intersecting. Since each Ai is an upset, it follows that B is an upset as well.
Also, without loss of generality, assume that A := Ak is a maximal intersecting family,
so |A| = 2n−1. Let us pick a set T uniformly at random from {1, 2, . . ., n}. Then
Pr(T ∈A ∪B)
=
Pr(T ∈A) + Pr(T ∈B) −Pr(T ∈A ∩B)
≤
1
2 + Pr(T ∈B) −1
2 Pr(T ∈B)
=
1
2 + 1
2 Pr(T ∈B)
≤
1
2 + 1
2
 1 −
1
2k−1

= 1 −1
2k ,
where the ﬁrst inequality is a consequence of the correlation inequality applied to the
upsets A, B, and the last inequality follows by induction on k. But this translates as
| S Ai| ≤2n(1 −1
2k ) = 2n −2n−k.

REFERENCES
563
REFERENCES
Printed Resources:
[AhDa78] R. Ahlswede and D. E. Daykin, “An inequality for the weights of two families of
sets, their unions and intersections”, Zeitschrift f¨ur Wahrscheinlichkeitstheorie verw.
Gebiete 43 (1978), 183–185.
[AlFi02] D. Aldous and J. A. Fill, Reversible Markov Chains and Random Walks on
Graphs, unﬁnished monograph, 2002.
[Al88] N. Alon, “The linear arboricity of graphs”, Israel Journal of Mathematics 62
(1988), 311–325.
[Al08] N. Alon, “Problems and results in extremal combinatorics II”, Discrete Mathe-
matics 308 (2008), 4460–4472.
[AlKl90] N. Alon and D. J. Kleitman, “Sum-free subsets”, in A Tribute to Paul Erd˝os, A.
Baker, B. Bollob´as, and A. Hajnal (eds.), Cambridge University Press, 13–26, 1990.
[AlKrSu03] N. Alon, M. Krivelevich, and B. Sudakov, “Tur´an numbers of bipartite graphs
and related Ramsey-type questions”, Combinatorics, Probability and Computing 12
(2003), 477–494.
[AlKrNe95] N. Alon, I. Kriz, and J. Neˇsetˇril, “How to color shift hypergraphs”, Studia
Scientiarum Mathematicarum Hungarica 30 (1995), 1–11.
[AlPe92] N. Alon and Y. Peres, “Uniform dilations”, Geometric and Functional Analysis
2 (1992), 1–28.
[AlSp08] N. Alon and J. Spencer, The Probabilistic Method, 3rd ed., Wiley, 2008.
[Az67] K. Azuma, “Weighted sums of certain dependent variables”, Tohoku Mathematical
Journal 3 (1967), 357–367.
[BaNe03] N. Balakrishnan and V. B. Nevzorov, A Primer on Statistical Distributions,
Wiley, 2003.
[BaNi70] M. N. Barber and B. W. Ninham, Random and Restricted Walks: Theory and
Applications, Gordon and Breach, 1970.
[BaPr87] R. E. Barlow and F. Proschan, Mathematical Theory of Reliability, Society for
Industrial and Applied Mathematics, 1987.
[Be93] H. C. Berg, Random Walks in Biology, Princeton University Press, 1993.
[Be06] D. Bevan, “Sets of points determining only acute angles and some related colouring
problems”, Electronic Journal of Combinatorics 13 (2006), # R12.
[BhHa04] R. Bhar and S. Hamori, Hidden Markov Models: Applications to Financial
Economics, Springer, 2004.
[Bo01] B. Bollob´as, Random Graphs, 2nd ed., Vol. 73, Cambridge Studies in Advanced
Mathematics, Cambridge University Press, 2001.
[BrFoSc87] P. Bratley, B. L. Fox, and L. E. Schrage, A Guide to Simulation, 2nd ed.,
Springer-Verlag, 1987.
[BuTu92] J. L. Buchanan and P. R. Turner, Numerical Methods and Analysis, McGraw-
Hill, 1992.

564
Chapter 7
DISCRETE PROBABILITY
[BuCa01] H. Bunke and T. Caelli, Hidden Markov Models: Applications in Computer
Vision, World Scientiﬁc Publishing, 2001.
[BuEr75] S. A. Burr and P. Erd˝os, “On the magnitude of generalized Ramsey numbers
for graphs”, Inﬁnite and Finite Sets I, Colloq. Math. Soc. Janos Bolyai 10, North-
Holland, 214–240, 1975.
[Ch52] H. Chernoﬀ, “A measure of asymptotic eﬃciency for tests of a hypothesis based
on the sum of observations”, Annals of Mathematical Statistics 23 (1952), 493–507.
[ClOlTu89] C. W. Clenshaw, F. W. J. Olver, and P. R. Turner, “Level-index arithmetic:
an introductory survey”, in Numerical Analysis and Parallel Processing, Lecture
Notes in Mathematics 1397 (1989), Springer-Verlag, 95–168.
[Co87] C. J. Colbourn, The Combinatorics of Network Reliability, Oxford University
Press, 1987.
[Co81] R. B. Cooper, Introduction to Queueing Theory, North-Holland, 1981.
[DaGr62] L. Danzer and B. Gr¨unbaum, “¨Uber zwei probleme bez¨uglich konvexer K¨orper
von P. Erd˝os und von V. L. Klee”, Mathematische Zeitschrift 79 (1962), 95–99.
[De86] L. Devroye, Non-Uniform Random Variate Generation, Springer-Verlag, 1986.
[DoSn84] P. G. Doyle and J. L. Snell, Random Walks and Electric Networks, Mathemati-
cal Association of America, 1984. (An excellent treatment of the connection between
random walks and electrical networks.)
[DuEtal98] R. Durbin, S. Eddy, A. Krogh, and G. Mitchison, Biological Sequence Analy-
sis: Probabilistic Models of Proteins and Nucleic Acids, Cambridge University Press,
1998.
[EbGrMa14] S. Eberhard, B. Green, and F. Manners, “Sets of integers with no large
sum-free subset”, Annals of Mathematics 180 (2014), 621–652.
[Er47] P. Erd˝os, “Some remarks on the theory of graphs”, Bulletin of the American
Mathematical Society 53 (1947), 292–294.
[Er59] P. Erd˝os, “Graph theory and probability”, Canadian Journal of Mathematics 11
(1959), 34–38.
[Er63] P. Erd˝os, “On a combinatorial problem”, Nordisk Matematisk Tidskrift 11 (1963),
220–223.
[Er64] P. Erd˝os, “On a combinatorial problem II”, Acta Mathematica Academiae Scien-
tiarum Hungaricae 15 (1964), 445–447.
[Er65] P. Erd˝os, “Extremal problems in number theory”, Proceedings of Symposia in
Pure Mathematics (AMS) Vol. VIII (1965), 181–189.
[ErFu83] P. Erd˝os and Z. F¨uredi, “The greatest angle among n points in the d-dimensional
Euclidean space”, Annals of Discrete Mathematics 17 (1983), 275–283.
[ErLo75] P. Erd˝os and L. Lov´asz, “Problems and results on 3-chromatic hypergraphs and
some related questions”, in Inﬁnite and Finite Sets, A. Hajnal, R. Rado, and V. T.
Sos (eds.), North-Holland, 609–628, 1975.
[ErSiSt83] P. Erd˝os, R. Silverman, and A. Stein, “Intersection properties of families
containing sets of nearly the same size”, Ars Combinatoria 15 (1983), 247–259.
[Fe68] W. Feller, An Introduction to Probability Theory and Its Applications, Vol. I, 3rd
ed., Wiley, 1968. (A classic text with extensive coverage of probability theory, the
combinatorics of simple random walks, and Markov chains.)

REFERENCES
565
[Fe71] W. Feller, An Introduction to Probability Theory and Its Applications, Vol. II,
2nd ed., Wiley, 1971. (A companion to the ﬁrst volume with a treatment of random
walks on continuous space and more advanced topics.)
[Fi03] G. S. Fishman, Monte Carlo: Concepts, Algorithms, and Applications, Springer-
Verlag, 2003. (A comprehensive and integrated treatment of Monte Carlo methods
and their applications.)
[FlWi93] M. Flanigan-Wagner and J. R. Wilson, “Using univariate B´ezier distributions
to model simulation input processes”, in Proceedings of the 1993 Winter Simulation
Conference, 1993, 365–373.
[FoEtal11] C. Forbes, M. Evans, N. Hastings, and B. Peacock, Statistical Distributions,
4th ed., Wiley, 2011.
[FoKaGi71] C. M. Fortuin, P. W. Kastelyen, and J. Ginibre, “Correlation inequalities on
some partially ordered sets”, Communications in Mathematical Physics 22 (1971),
89–103.
[FoSu11] J. Fox and B. Sudakov, “Dependent random choice”, Random Structures &
Algorithms 38 (2011), 68–99.
[GaYo07] M. Gales and S. Young, “The application of hidden Markov models in speech
recognition”, Foundations and Trends in Signal Processing 1 (2007), 195–304.
[Ga12] N. Gautam, Analysis of Queues: Methods and Applications, CRC Press, 2012.
[GuScDe97] I. Guyon, M. Schenkel, and J. Denker, “Overview and synthesis of on-line
cursive handwriting recognition techniques”, in Handbook of Character Recognition
and Document Image Analysis, H. Bunke and P. S. P. Wang (eds.), World Scientiﬁc
Publishing, 1997, Chapter 7.
[Ha13] M. Harchol-Balter, Performance Modeling and Design of Computer Systems:
Queueing Theory in Action, Cambridge University Press, 2013.
[GrHa85] D. Gross and C. M. Harris, Fundamentals of Queueing Theory, 2nd ed., Wiley,
1985.
[HeSo82] D. P. Heyman and M. J. Sobel, Stochastic Models in Operations Research, Vol.
1, McGraw-Hill, 1982.
[HiMoRe97] H. Hind, M. Molloy, and B. Reed, “Colouring a graph frugally”, Combina-
torica 17 (1997), 469–482.
[Ho63] W. Hoeﬀding, “Probability inequalities for sums of bounded random variables”,
Journal of the American Statistical Association 58 (1963) 13–30.
[IE08] IEEE, Binary Floating-Point Arithmetic, IEEE Standard 754-2008, IEEE, 2008.
[JaLuRu00] S. Janson, T.  Luczak, and A. Ruci´nski, Random Graphs, Wiley, 2000.
[Jo87] M. E. Johnson, Multivariate Statistical Simulation, Wiley, 1987.
[JoKoBa94] N. L. Johnson, S. Kotz, and N. Balakrishnan, Continuous Univariate Distri-
butions, Volume 1, 2nd ed., Wiley, 1994.
[KeSn60] J. G. Kemeny and L. J. Snell, Finite Markov Chains, Van Nostrand, 1960.
[Kl75] L. Kleinrock, Queueing Systems, Vol. I: Theory, Wiley, 1975.
[Kl66] D. J. Kleitman, “Families of non-disjoint subsets”, Journal of Combinatorial The-
ory A 1 (1966), 153–155.

566
Chapter 7
DISCRETE PROBABILITY
[Ku11] V. G. Kulkarni, Introduction to Modeling and Analysis of Stochastic Systems,
2nd ed., Springer, 2011.
[Ku16] V. G. Kulkarni, Modeling and Analysis of Stochastic Systems, 3rd ed., Chapman-
Hall, 2016.
[La15] A. M. Law, Simulation Modeling and Analysis, 5th ed., McGraw-Hill, 2015.
[La91] G. Lawler, Intersections of Random Walks, Birkh¨auser, 1991. (A monograph on
the mathematical analysis of problems dealing with the non-intersection of paths of
random walks. Sophisticated mathematical methods.)
[LE94] P. L’Ecuyer, “Uniform random number generation”, Annals of Operations Re-
search 53 (1994), 77–120.
[LE12] P. L’Ecuyer, “Random number generation” in Handbook of Computational Statis-
tics, 2nd ed., J. E. Gentle, W. Haerdle, and Y. Mori (eds.), Springer-Verlag, 35–71,
2012.
[Lo93] L. Lov´asz, “Random walks on graphs: a survey”, in Combinatorics, Paul Erd¨os is
Eighty, Bolyai Society, Mathematical Studies, Vol. 2 (1993), 1–46.
[Ma90] B. G. Malkiel, A Random Walk Down Wall Street, W. W. Norton, 1990.
[MoRe00] M. Molloy and B. Reed, Graph Colouring and the Probabilistic Method,
Springer-Verlag, 2000.
[Na93] R. E. Nance, “A history of discrete event simulation programming languages”,
ACM SIGPLAN Notices 28 (1993), 149–175.
[Ne13] B. L. Nelson, Foundations and Methods of Stochastic Simulation: A First Course,
Springer, 2013.
[PaMi88] S. K. Park and K. W. Miller, “Random number generators: good ones are hard
to ﬁnd”, Communications of the ACM 31 (1988), 1192–1201.
[RaSr00] J. Radhakrishnan and A. Srinivasan, “Improved bounds and algorithms for
hypergraph two-coloring”, Random Structures & Algorithms 16 (2000), 4–32.
[Ro14] S. M. Ross, Introduction to Probability Models, 11th ed., Academic Press, 2014.
[SaTa12] N. Sain and S. Tamrakar, “Web usage mining & pre-fetching based on hidden
Markov model and fuzzy clustering”, International Journal of Computer Science and
Information Technologies 3 (2012), 4874–4877.
[SaGu93] L. Satish and B. I. Gururaj, “Use of hidden Markov models for partial discharge
pattern classiﬁcation”, IEEE Transactions on Dielectrics and Electrical Insulation 28
(1993) 172–182.
[Sc90] B. Schmeiser, “Simulation experiments”, in Handbooks in OR & MS, D. P. Hey-
man and M. J. Sobel (eds.), Elsevier, 1990, 296–330.
[Sh91] D. R. Shier, Network Reliability and Algebraic Structures, Clarendon Press, 1991.
[Sp87] J. H. Spencer, Ten Lectures on the Probabilistic Method, SIAM, 1987.
[Sp76] F. Spitzer, Principles of Random Walks, 2nd ed., Springer-Verlag, 1976. (A classic
monograph on mathematical properties of lattice random walks.)
[St74] P. H. Sterbenz, Floating-Point Computation, Prentice-Hall, 1974.
[StEtal11] J. Stigler, F. Ziegler, A. Gieseke, J. C. M. Gebhardt, and M. Rief, “The
complex folding network of single calmodulin molecules”, Science 334 (2011), 512–
516.

REFERENCES
567
[Sw15] J. J. Swain, “Simulated worlds”, OR/MS Today 42/6 (2015), 36–49.
[Ta72] R. E. Tarjan, “Depth-ﬁrst search and linear graph algorithms”, SIAM Journal on
Computing 1 (1972), 146–160.
[ToEtal13] K. Tokuda, Y. Nankaku, T. Toda, H. Zen, J. Yamagishi, and K. Oura, “Speech
synthesis based on hidden Markov models”, Proceedings of the IEEE 101 (2013),
1234–1252.
[We94] G. H. Weiss, Aspects and Applications of the Random Walk, North Holland,
1994. (A treatment of random walks and their applications with a physical scientist’s
perspective.)
[Wi84] J. R. Wilson, “Variance reduction techniques for digital simulation”, American
Journal of Mathematical and Management Sciences 4 (1984), 277–312.
[Wo89] R. W. Wolﬀ, Stochastic Modeling and the Theory of Queues, Prentice-Hall, 1989.
Web Resources:
https://math.dartmouth.edu/~doyle/docs/walks/walks.pdf (A 2006 online version
of the Doyle and Snell book on random walks and electrical networks.)
http://math.ucsd.edu/~crypto/Monty/monty.html (Simulation of the Let’s Make a
Deal game show, in which prizes are randomly hidden behind doors.)
http://random.mat.sbg.ac.at/generators/
(Information on a variety of pseudo-
random number generators.)
http://random.mat.sbg.ac.at/software (Software for Monte Carlo simulation and
for pseudo-random number generators.)
http://random.mat.sbg.ac.at/tests (Tests for uniform pseudo-random number gen-
erators.)
http://statistik.wu-wien.ac.at/projects/anuran/index.html (Code generation
for non-uniform random variates.)
http://web2.uwindsor.ca/math/hlynka/qbook.html
(Provides an extensive list of
books on queueing theory.)
http://web2.uwindsor.ca/math/hlynka/qsoft.html (Compilation of software on queue-
ing theory.)
http://ws3.atv.tuwien.ac.at/eurosim/
(Lists a number of commercial and free-
ware/shareware simulation packages.)
http://www.anylogic.com (Integrated modeling and simulation development environ-
ment.)
http://www.dcs.ed.ac.uk/home/hase/simjava/applets/index.html (Java-based dis-
crete event simulation applets.)
http://www.informs-sim.org (Archive of Winter Simulation Conference proceedings,
an excellent source for simulation information.)
http://www.math.wm.edu/~leemis/chart/UDR/UDR.html
(Interactive chart contain-
ing univariate probability distributions.)
http://www.stat.berkeley.edu/~aldous/RWG/book.html (Recompiled monograph of
Aldous and Fill on reversible Markov chains and graphs.)

568
Chapter 7
DISCRETE PROBABILITY
http://www.um.es/fem/EjsWiki/index.php/Main/WhatIsEJS (Visual simulation en-
vironment.)
http://www.taygeta.com/random.html (Software for pseudo-random number genera-
tors.)
http://www.vissim.com (Graphical language for simulation development.)

8
GRAPH THEORY
8.1 Introduction to Graphs
Lowell W. Beineke
8.1.1 Varieties of Graphs and Graph Models
8.1.2 Graph Operations
8.1.3 Special Graphs and Graph Families
8.1.4 Graph Representation and Computation
8.2 Graph Models
Jonathan L. Gross
8.2.1 Attributes of a Graph Model
8.3 Directed Graphs
Stephen B. Maurer
8.3.1 Digraph Models and Representations
8.3.2 Paths, Cycles, and Connectedness
8.3.3 Orientations
8.3.4 Directed Acyclic Graphs
8.3.5 Tournaments
8.4 Distance, Connectivity, Traversability, & Matchings
8.4.1 Walks, Distance, and Cycle Rank
Edward R. Scheinerman
8.4.2 Connectivity
Edward R. Scheinerman
8.4.3 Euler Trails and Tours
Edward R. Scheinerman
8.4.4 Hamilton Cycles and Paths
Edward R. Scheinerman
8.4.5 Matchings and Factors
Michael D. Plummer
8.5 Graph Isomorphism and Reconstruction
8.5.1 Isomorphism Invariants
Bennet Manvel & Adolfo Piperno
8.5.2 Isomorphism Testing
Bennet Manvel & Adolfo Piperno
8.5.3 Graph Reconstruction
Bennet Manvel & Josef Lauri
8.6 Graph Colorings, Labelings, & Related Parameters
8.6.1 Vertex Colorings
Arthur T. White
8.6.2 Edge Colorings
Arthur T. White
8.6.3 Cliques and Independence
Arthur T. White
8.6.4 Map Colorings
Arthur T. White
8.6.5 More Graph Colorings
Arthur T. White
8.6.6 Domination in Graphs
Teresa W. Haynes & Michael A. Henning
8.6.7 Graph Pebbling
Glenn Hurlbert
8.6.8 Graph Labelings
Joseph A. Gallian
8.7 Planar Drawings
Jonathan L. Gross
8.7.1 Characterizing Planar Graphs
8.7.2 Numerical Planarity Criteria
8.7.3 Planarity Algorithm
8.7.4 Crossing Numbers and Thickness
8.7.5 Stereographic Projection
8.7.6 Geometric Drawings

570
Chapter 8
GRAPH THEORY
8.8 Topological Graph Theory
Jonathan L. Gross
8.8.1 Closed Surfaces
8.8.2 Drawing Graphs on Surfaces
8.8.3 Combinatorial Representation of Graph Imbeddings
8.8.4 Genus and Crosscap Number
8.9 Enumerating Graphs
Paul K. Stockmeyer
8.9.1 Counting Labeled Graphs and Multigraphs
8.9.2 Counting Unlabeled Graphs and Multigraphs
8.9.3 Counting Labeled Digraphs and Tournaments
8.9.4 Counting Unlabeled Digraphs and Tournaments
8.10 Graph Families
8.10.1 Perfect Graphs
Maria Chudnovsky
8.10.2 Spectral Graph Theory
Michael Doob
8.10.3 Algebraic Graphs and Automorphisms
Michael Doob
8.10.4 Expander Graphs
Michael Krebs & Anthony Shaheen
8.10.5 Product Graphs
Richard Hammack, Sandi Klavˇzar, Wilfried Imrich
8.11 Analytic Graph Theory
Stefan A. Burr
8.11.1 Extremal Graph Theory
8.11.2 Ramsey Theory for Graphs
8.11.3 Probabilistic Graph Theory
8.12 Hypergraphs
Andr´as Gy´arf´as
8.12.1 Hypergraphs as a Generalization of Graphs
8.12.2 Hypergraphs as General Combinatorial Structures
8.12.3 Numerical Invariants of Hypergraphs
INTRODUCTION
A graph is conceptually a set of points and a set of lines (possibly curved) joining one
point to another (or to itself). Graph theory has its origins in many disciplines. Graphs
are natural mathematical models of physical situations in which the points represent
either objects or locations and the lines represent connections. Graphs are also used
to model sociological and abstract situations in which each line represents a relationship
between the entities represented by the points. Applications of graphs are wide-ranging—
in areas such as circuit design, communications networks, ecology, engineering, operations
research, counting, probability, set theory, information theory, and sociology.
This chapter contains an extensive treatment of the various properties of graphs. Further
topics in graph theory are covered in Chapter 9 (Trees) and in Chapter 10 (Networks
and Flows).
GLOSSARY
acyclic digraph: a digraph containing no directed cycles.
acyclic graph: a graph containing no cycles.
adjacency matrix (of a digraph): the square matrix A with A[i, j] = the number of
edges from vertex vi to vertex vj.

GLOSSARY
571
adjacency matrix (of a graph): the square matrix A with A[i, j] = the number of
edges between vertices vi and vj.
adjacent edges: two edges with a common endpoint.
adjacent vertex (in a digraph) from [to] a vertex u: a vertex v such that there is an
arc from u to v [to u from v].
adjacent vertices: two vertices that are endpoints of the same edge.
admittance matrix: See Laplacian.
algebraic speciﬁcation (of a graph): a speciﬁcation that uses group elements in the
vertex and edge names and uses the group operation in the incidence rule.
almost every (a.e.) graph has property P : the statement that the probability that
a random n-vertex graph has property P approaches 1 as n →∞.
antichain: a hypergraph in which no edge contains any other edge.
antihole: an induced subgraph whose complement is isomorphic to the cycle Ck with
k ≥4.
antimagic graph: a graph whose q edges can be labeled with distinct integers from 1
to q such that the sums of the labels of the edges incident to each vertex are distinct.
arc: another name for a directed edge of a graph.
articulation point: synonym for cutpoint.
attachment of a bridge of a subgraph: for a bridge B of a subgraph H, a vertex of
B ∩H.
attribute (of the edge-set or vertex-set): any additional feature, such as length, cost,
or color, that enables a graph to model a real problem.
automorphism: for a graph or digraph, an isomorphism from the graph or digraph to
itself.
automorphism group: the collection Aut(G) of all automorphisms of a graph or di-
graph G under the operation of composition.
basis (for a digraph): a set of vertices V ′ of the digraph such that every vertex not in V ′
is reachable from V ′ and no proper subset of V ′ has this property.
Berge graph: a graph that has no odd hole and no odd antihole.
bipartite: property of a graph that its vertices can be partitioned into two subsets,
called “parts”, so that no two vertices within the same part are adjacent.
block: in a graph, a maximal nonseparable subgraph.
bond: a minimal disconnecting set of edges.
boundary (of a region of a graph imbedded in a surface): for region R, the subgraph
containing all vertices and edges incident on R; denoted by ∂R.
boundary (of a set of vertices): for F a subset of vertices in a graph, the set of edges
that have one endpoint inside F and one endpoint outside F; denoted by ∂F.
bouquet: a graph Bn with one vertex and n self-loops.
bridge (edge): a cut-edge.
cactus: a connected graph in which every block is either an edge or a cycle.

572
Chapter 8
GRAPH THEORY
Cartesian product: for graphs G and H, the graph G □H that has vertex set VG×VH
and edge set

(g, h)(g′, h′) | gg′ ∈EG and h = h′, or, g = g′ and hh′ ∈EH
	
.
caterpillar: a tree that contains a path such that every edge has one or both endpoints
in that path.
Cayley graph (or digraph): a graph that depicts a group with a prescribed set of
generators; the vertices represent group elements, and the edges or arcs (often “col-
ored” by the generators) represent the product rule.
cellular imbedding: an imbedding such that every region is equivalent to the interior
of a (unit) disk.
center: in a connected graph, the set of vertices of minimum eccentricity.
chain: a simple hypergraph in which, given any pair of edges, one edge contains the
other.
characteristic polynomial (of a graph): the characteristic polynomial of its adjacency
matrix.
chromatic index (of a graph or hypergraph): See edge chromatic number.
chromatic number (of a graph): the minimum number χ(G) of colors needed to color
the vertices of a graph G so that no vertex is adjacent to a vertex of the same color.
chromatic number (of a hypergraph): the smallest number χ(H) of independent sets
required to partition the vertex set of H.
chromatic number (of a map): the minimum number χ(M) of colors needed to color
the regions of the map M so that no color meets itself across an edge.
chromatic number (of a surface): the largest map chromatic number χ(S) taken over
all maps on the surface S.
chromatically n-critical graph: an n-chromatic graph G such that χ(G −e) = n −1
no matter what edge e is removed.
circuit: synonym for a closed walk, a closed trail, or a cycle, depending on the context.
clique (in a graph): a complete subgraph.
clique (in a hypergraph): a simple hypergraph such that every pair of edges has non-
empty intersection.
clique number (of a graph): the number ω(G) of vertices of a largest clique in the
graph G.
clique number (of a hypergraph): the largest number ω(H) of edges of any partial
clique in the hypergraph H.
clique partition number: for a hypergraph H, the smallest number cp(H) of cliques
required to partition the edge set.
closed walk (trail or path): a walk, trail, or path whose origin and terminus are the
same.
n-colorable graph: a graph having a vertex coloring using at most n colors.
n-colorable map: a map having a coloring using at most n colors.
comparability graph: a graph that admits a transitive orientation.
complement (of a graph): the graph G with the same vertex set as G, but in which
two vertices are adjacent if and only if they are not adjacent in G.

GLOSSARY
573
complete bipartite graph: a bipartite graph Kr,s whose vertex set has two parts, of
sizes r and s, respectively, such that every vertex in one part is adjacent to every
vertex in the other part.
complete graph: the simple graph Kn with n vertices in which every pair of vertices
is adjacent.
complete hypergraph: the simple n-vertex hypergraph K∗
n in which every subset of
vertices is an edge.
complete multipartite (or k-partite) graph: a k-partite simple graph such that ev-
ery pair of vertices from diﬀerent parts is joined by an edge; denoted by Kn1,...,nk,
where n1, . . . , nk are the sizes of the parts.
complete r-uniform hypergraph: the simple n-vertex hypergraph Kr
n in which every
r-element subset is an edge.
complete set of invariants: a set of invariants that determine a graph or digraph up
to isomorphism.
component: for a graph, a maximal connected subgraph.
connected: property of a graph that each pair of vertices is joined by a path.
connectivity: See vertex connectivity.
contraction: for a graph, the result of a sequence of elementary contractions.
contraction, elementary (of a graph): the operation of shrinking an edge to a point,
so that its endpoints are merged, without otherwise changing the graph.
contraction, elementary (of a simple graph): replacing two adjacent vertices u and v
by one vertex adjacent to all other vertices to which u or v were adjacent.
converse: for a digraph, the digraph obtained by reversing the direction of every arc.
corona (of a graph): for a graph H, the graph obtained by adding for each vertex v
of H a new vertex v′ and the edge vv′; denoted by H ◦K1.
crosscap: a subportion of a surface that forms a M¨obius band.
crosscap number (of a nonorientable surface): for a nonorientable surface S, the max-
imum number γ(S) of disjoint crosscaps one can ﬁnd on the surface. The nonori-
entable surface of crosscap number k is denoted Nk.
crossing number: for a graph G, the minimum number ν(G) of edge-crossings taken
over all normalized planar drawings of G.
cube graph: See hypercube graph.
cut-edge: for a graph G, an edge e such that G −e has more components than G.
cut-vertex (or cutpoint): for a graph G, a vertex v such that G −v has more com-
ponents than G.
cycle: a closed path of positive length. See also k-cycle.
cycle, directed: a closed directed walk in which all the vertices except the ﬁrst and
last are distinct.
cycle graph: a graph Cn with n vertices that is 2-regular and connected.
cycle rank: for a connected graph G, the number β1(G) of edges in the complement of
a spanning tree for G; that is, |EG| −|VG| + 1.
DAG: an acronym for directed acyclic graph.

574
Chapter 8
GRAPH THEORY
degree (of a vertex in a graph): for a vertex v, the number deg(v) of instances of v as
an endpoint; that is, the number of proper edges incident on v plus twice the number
of loops at v.
degree (of a hypergraph vertex): for a vertex x, the number deg(x) of hypergraph edges
containing x.
degree sequence of a graph: the sequence of the degrees of its vertices, usually sorted.
deleting an edge from a graph: given a graph G and an edge e of G, the operation
that results in the subgraph G −e, which contains all the vertices of G and all edges
except e.
deleting a vertex from a graph: given a graph G and a vertex v of G, the operation
that results in the subgraph G −v, which contains all vertices of G except v and all
the edges of G except those incident with v.
diameter: for a connected graph, the maximum distance between two of its vertices.
diconnected digraph: See strongly connected digraph.
digraph (or directed graph): a graph in which every edge is directed.
dipole: the graph Dn with two vertices and a multi-edge of multiplicity n joining them.
direct product: for graphs G and H, the graph G × H that has vertex set VG × VH
and edge set

(g, h)(g′, h′) | gg′ ∈EG and hh′ ∈EH
	
.
directed cycle, path, trail, walk: See cycle, path, etc.
directed graph: See digraph.
direction (on an edge): a sense of forward progression from one end to the other, usu-
ally marked by an arrowhead.
disconnected (digraph): a digraph whose underlying graph is disconnected.
disconnecting set of edges (in a connected graph): a set whose removal yields a non-
connected graph.
disconnecting set of vertices (in a connected graph): a set whose removal yields a
nonconnected graph.
distance (in a connected graph): for two vertices v and w, the length d(v, w) of a short-
est path between them.
distance (in a connected digraph): for two vertices v and w, the length d(v, w) of a
shortest directed path between them.
dodecahedral graph: the 1-skeleton of the dodecahedron, which is a 3-dimensional
polyhedron whose 12 faces are all pentagons; this graph has 20 vertices, each of
degree 3, and 30 edges.
dominating set (of a graph): a set S of vertices such that every vertex not in S is
adjacent to a vertex in S.
domination number (of a graph): the minimum cardinality γ(G) of a dominating set
in graph G.
downset: a simple hypergraph in which every subset of every edge is also an edge of
the hypergraph.
dual graph imbedding: a new graph imbedding obtained by placing a dual vertex in
the interior of each existing (“primal”) region and by drawing a dual edge through
each existing (“primal”) edge connecting the dual vertices on its opposite sides.

GLOSSARY
575
dual (of a hypergraph): for a hypergraph H, the hypergraph H∗whose incidence ma-
trix is the transpose of the incidence matrix M(H).
eccentricity (of a vertex): for a vertex v in a connected graph, the maximum distance
from v to another vertex.
edge: a line, either joining one vertex to another or joining a vertex to itself; an element
of the second constituent set of a graph.
edge chromatic number (of a graph): for a graph G, the smallest number n such
that G is n-edge colorable, written χ1(G).
edge chromatic number (of a hypergraph): for a hypergraph H, the smallest number
q(H) of matchings required to partition the edge set of H.
n-edge colorable: property of a graph that it has an edge coloring using at most n
colors.
edge coloring: an assignment of colors to the edges of a graph so that adjacent edges
receive diﬀerent colors. See also n-edge colorable.
edge connectivity: the cardinality κ′(G) of a smallest disconnecting set of edges in
graph G. See also k-edge-connected.
edge cut: See disconnecting set.
edge independence number: the cardinality α1(G) of a largest independent set of
edges in graph G.
edge-complement: See complement.
edge-deleted subgraph: any subgraph obtained from a graph by removing a single
edge.
edge-reconstructible graph: a graph that is uniquely determined by its collection of
edge-deleted subgraphs.
edge-reconstructible invariant: an invariant that is uniquely determined by the col-
lection of edge-deleted subgraphs of a graph.
eigenvalue (of a graph): a number λ such that Ax = λx for some nonzero vector x,
where A is the adjacency matrix.
eigenvector (of a graph): a nonzero vector x such that Ax = λx, where A is the adja-
cency matrix.
embedding: See imbedding.
empty graph: sometimes, a graph with no edges; other times, a graph with no vertices
or edges. See null graph.
endpoints (of an edge): the vertices that are joined by the edge.
Euler characteristic: for a surface S, the invariant χ(S) given by 2 −2g for the ori-
entable surface of genus g, and 2 −k for the nonorientable surface of crosscap num-
ber k.
Euler tour: a closed Euler trail.
Euler trail: a trail that contains all the edges of the graph.
Eulerian graph: a graph that has an Euler tour.
expander family: a sequence of graphs, all of the same regularity and whose orders
grow to inﬁnity, for which there exists ǫ > 0 such that the isoperimetric constant is
at least ǫ for all the graphs.

576
Chapter 8
GRAPH THEORY
exterior region: in a planar graph drawing, the region that extends to inﬁnity.
extremal graph: for a set G of graphs and an integer n, an n-vertex graph with ex(G; n)
edges that contains no member of G.
extremal number: for a set G of graphs, the greatest number ex(G; n) of edges in any
n-vertex simple graph that does not contain some member of G as a subgraph.
face: for an imbedding of a graph in a surface, a region plus its boundary.
1-factorization: a partition of the edges of a graph into 1-factors.
forest: a graph without a cycle.
four color theorem: the fact that every planar map can be properly colored with at
most four colors, proved in 1976.
general graph: another name for a graph that might have loops.
generating set (for a group): a subset of group elements such that every group element
is a product of generators.
genus (of a graph): the minimum genus of a surface in which the graph has a cellular
imbedding.
genus (of an orientable surface): for a surface S, the maximum number γ(S) of disjoint
handles one can ﬁnd on the surface; the orientable surface of genus g is denoted Sg.
girth: for a graph, the number of edges in a shortest cycle, if there is at least one cycle;
undeﬁned if the graph has no cycles.
graceful graph: a graph for which there exists an injection f from the vertices to the
set {0, 1, . . ., q}, where q is the number of edges, such that when each edge xy is
assigned the label |f(x) −f(y)|, the resulting edge labels are distinct.
graph: a set V of vertices and a set E of edges such that all the endpoints of edges in E
are contained in V , written G = (V, E), (VG, EG), or (V (G), E(G)).
graph model: any conﬁguration with underlying graph structure, and possibly some
additional attributes on its edges and/or vertices, such as length, direction, or cost.
graph sum: for graphs G and H, the graph G + H whose vertex set and edge set are,
respectively, the disjoint union of the vertex sets and the edge sets of G and H.
graphical sequence: a sequence of nonnegative integers such that there is a simple
graph for which it is the degree sequence.
Gray code: a cyclic ordering of all 2k bitstrings of length k, such that each bitstring
diﬀers from the next in exactly one bit entry.
Hamilton cycle: a spanning cycle; that is, a cycle including each vertex of a graph
exactly once.
Hamilton path: a path that includes all the vertices of a graph.
Hamiltonian graph: a graph that contains a Hamiltonian cycle.
harmonious graph: a graph for which there is an injection f from the vertices to Zq,
where q is the number of edges, such that when each edge xy is assigned the label
f(x) + f(y) (mod q), the resulting edge labels are distinct; when the graph is a tree,
exactly one label may be used on two vertices.
head (of an arc): the vertex the arc goes to.

GLOSSARY
577
Hoﬀman polynomial (of a graph): a polynomial p(x) of minimum degree such that
p(A) = J, where A is the adjacency matrix and J is the matrix with every entry
equal to 1.
hole: an induced subgraph that is isomorphic to the cycle Ck with k ≥4.
homeomorphic graphs: two graphs that can both be obtained from the same graph
by a sequence of edge subdivisions.
hypercube graph: a graph Qd whose 2d vertices can be labeled with the bitstrings of
length d, so that two vertices are adjacent if and only if their labels diﬀer in exactly
one bit.
hypergraph: a ﬁnite set V of “vertices” together with a ﬁnite collection E of “edges”
(sometimes, “hyperedges”), which are arbitrary subsets of V , written H = (V, E).
icosahedral graph: the 1-skeleton of the icosahedron, which is a 3-dimensional poly-
hedron whose 20 faces are triangles; this graph has 12 vertices, each of degree 5, and
30 edges.
imbedding (of a graph in a surface): a drawing so that there are no edge-crossings;
also embedding.
incidence matrix (of a digraph with no self-loops): the matrix MI with MI[i, j] = 0
if vertex vi is not an endpoint of arc ej, MI[i, j] = 1 if vi is the head of arc ej, and
MI[i, j] = −1 if vi is the tail of arc ej.
incidence matrix (of a graph): the matrix MI with MI[i, j] = 0 if vertex vi is not
an endpoint of edge ej, MI[i, j] = 1 if ej is a proper edge with endpoint vi, and
MI[i, j] = 2 if ej is a loop at vi.
incidence matrix (of a hypergraph): for a hypergraph, the matrix [mi,j] where mi,j =
1 if vertex xj is in edge ei, and mi,j = 0 otherwise.
incidence rule: a rule specifying the endpoints of every edge of a graph.
incident edge (from [to] a digraph vertex): for a vertex u in a digraph, an arc e such
that u is the tail [head] of e.
incident edge (in a graph): for a vertex u in a graph, an edge e such that u is an
endpoint of e.
incident-edge table (for a graph): a table that lists, for each vertex, the edges having
that vertex as an endpoint.
in-degree: for a vertex v, the number of arcs with head v.
independent set (in a graph): for a graph G, a subset of either V (G) or E(G) such
that no two elements are adjacent in G.
independent set (of hypergraph vertices): a set of vertices that does not (completely)
contain any edge of the hypergraph.
independence number (of a graph): the number α(G) of vertices in the largest inde-
pendent subset in G.
independence number (of a hypergraph): the maximum number α(H) of vertices that
form an independent set in H.
induced subgraph (on a vertex subset): the subgraph of a graph G containing every
edge of G that joins two vertices of the prescribed vertex subset.

578
Chapter 8
GRAPH THEORY
intersection graph (for a family of subsets): for a family F = {Sj} of sets, the graph
with vertex set F such that there is an edge between each pair of subsets Si and Sj
whose intersection is nonempty.
intersection graph (of a hypergraph): for a hypergraph H, the simple graph I(H)
whose vertices are the edges of H, such that two vertices of I(H) are adjacent if and
only if the corresponding edges of H have nonempty intersection.
interval graph: the intersection graph of a family of subintervals of [0, 1].
invariant: a parameter or property of graphs that is preserved by isomorphisms.
irreducible tournament: a tournament with no bipartition V1, V2 of the vertices such
that all arcs between V1 and V2 go from V1 to V2.
isolated point: a vertex of a graph that is not the endpoint of any edge.
isomorphic (pair of graphs): a pair of graphs with identical mathematical structure;
formally, a pair of graphs such that there is an isomorphism from one to the other.
isomorphism (of digraphs): an isomorphism of the underlying graphs of two digraphs
such that the edge-correspondence preserves direction.
isomorphism (of graphs): for graphs G and H, a pair of bijections fV : VG →VH and
fE : EG →EH such that for every edge e ∈EG, the endpoints of e are mapped onto
the endpoints of fE(e).
isomorphism (of simple graphs): a bijection between the vertices of two graphs such
that a pair of vertices is adjacent in one graph if and only if the corresponding pair
of vertices is adjacent in the other graph.
isomorphism type: for a graph [digraph] G, the class of all graphs [digraphs] isomor-
phic to G.
isoperimetric constant: for graph G with vertex set VG, the minimum ratio |∂F|/|F|
taken over all F ⊆VG such that 0 < |F| ≤|VG|/2; denoted h(G).
join: for graphs G and H, the graph G ∗H obtained by adding to the disjoint union
G + H an edge from each vertex in G to each vertex in H.
k-connected: property of a graph G that the smallest size of a disconnecting set of
vertices is at least k; that is, κ(G) ≥k.
k-cycle: a cycle of length k.
k-edge-connected: property of a graph G that κ′(G) ≥k.
k-partite graph: a graph whose vertex set can be partitioned into at most k parts in
such a way that each edge joins diﬀerent parts. Equivalent to a k-colorable graph.
k-regular: property of a graph or hypergraph that all its vertices have degree k.
king: a vertex in a digraph that can reach all other vertices by paths of length 1 or 2.
Kuratowski graphs: the complete graph K5 and the complete bipartite graph K3,3.
labeled graph: in applied graph theory, any graph in which the vertices and/or edges
have been assigned labels; in pure graph theory, a graph in which standard labels
v1, v2, . . . , vn have been assigned to the vertices.
λ of a graph: for a connected d-regular graph, the maximum |λi| taken over all eigen-
values λi that are not ±d.
Laplacian (of a graph G): the matrix D −A where D is the diagonal matrix with the
degree sequence of G on the diagonal and where A is the adjacency matrix.

GLOSSARY
579
length (of a walk): the number of edge-steps in the sequence that speciﬁes the walk.
lexicographic product: for graphs G and H, the graph G ◦H that has vertex set
VG × VH and edge set

(g, h)(g′, h′) | (gg′ ∈EG), or (g = g′ and hh′ ∈EH)
	
.
line: synonym for edge, or refers to what is modeled by an edge.
line graph: for a graph G, the graph L(G) whose vertices correspond to the edges of G,
with two vertices being adjacent in L(G) whenever the corresponding edges have a
common endpoint in G.
linear extension ordering: a consecutive labeling v1, v2, . . . , vn of the vertices of a
digraph such that, if there is an arc from vi to vj, then i < j.
link: See proper edge.
loop (or self-loop): an edge joining a vertex to itself.
magic graph: a connected graph whose edges can be labeled with distinct positive
integers such that for each vertex v the sum of the labels of all edges incident with v
is the same.
map: an imbedding of a graph on a surface.
map chromatic number: See chromatic number of a map.
map coloring: an assignment of colors to the regions of a map so that adjacent regions
receive diﬀerent colors.
matching: a set of pairwise disjoint edges in a graph or hypergraph.
matching number: in a graph, the maximum number of pairwise disjoint edges of the
graph; in a hypergraph H, the maximum number ν(H) of pairwise disjoint edges
of H; that is, the cardinality of the largest partial of H that forms a matching.
minor: for a graph G, any graph that can be obtained from G by a sequence of edge
deletions and contractions.
M¨obius band: the surface obtained from a rectangular sheet by pasting the left side
to the right with a half-twist.
multi-arc: two or more arcs, all of which have the same head and the same tail.
multi-edge: a set of at least two edges, all of which have the same endpoints.
multigraph: a graph with multi-edges.
neighbor: for a vertex, any vertex adjacent to it.
node: a vertex, or refers to what is modeled by a vertex.
nonorientable surface: a surface such that some subportion forms a M¨obius band.
nonorientable surface of crosscap number k: the surface Nk obtained by adding k
crosscaps to a sphere.
nonplanar: property of a graph that it cannot be drawn in the plane without crossings.
nonseparable: property of a connected graph that it has no cut-vertex.
normal: property of a hypergraph H that q(H) = ∆(H).
normalized drawing: the usual way a graph is drawn, avoiding pathological con-
trivances such as overloaded crossings (i.e., more than two edges).
null graph: (usually) a graph with no vertices or edges.

580
Chapter 8
GRAPH THEORY
obstruction to n-coloring: synonym for a chromatically (n + 1)-critical graph, since
a chromatically (n + 1)-critical subgraph prevents n-chromaticity.
octahedral graph: the 1-skeleton of the 3-dimensional octahedron, or sometimes, a
generalization of this graph.
1-skeleton (of a polyhedron): the graph whose vertices and edges are, respectively, the
vertices and edges of that polyhedron.
open: property of a walk, trail, or path that its ﬁnal vertex is diﬀerent from its initial
vertex.
order (of a graph): for a graph G, the cardinality |VG| of the vertex set.
order (of a hypergraph edge): the number of vertices in the edge.
orientable surface: any surface obtainable from a sphere by adding handles.
orientable surface of genus g: the surface Sg obtained by attaching g handles to a
sphere.
orientation: an assignment of a direction to every edge of a graph, making it a digraph.
origin (of a walk): the initial vertex of the walk.
out-degree: for a vertex v, the number of arcs with tail v.
partial: for a hypergraph H = (V, E), a hypergraph H′ = (V, E′) such that E′ ⊆E.
path: a trail in which all of its vertices are diﬀerent, except that the initial and ﬁnal
vertices may be the same. See also u,v-path.
path, directed: a directed trail in which no vertex is repeated.
pebbling number (of a graph): the maximum value π(G) of the rooted pebbling num-
ber over all roots of the graph G.
pebbling step from u to v: for an edge uv, the removal of two pebbles from u and
placement of one pebble on v.
perfect graph: a graph such that every induced subgraph has vertex chromatic number
equal to its clique number.
perfect matching (or 1-factor): a matching that covers all the vertices of a graph.
Petersen graph: a 3-regular 10-vertex graph that looks like a 5-cycle joined by its
vertices to the vertices of a 5-pointed star drawn in its interior.
planar: property of a graph that it can be drawn in the plane without crossings.
Platonic graph: the 1-skeleton of a Platonic solid.
Platonic solid: any of ﬁve 3-dimensional polyhedra whose sides are all identical regular
polygons.
polyhedron: a generalization of a polygon to higher dimensions; usually a solid 3-
dimensional ﬁgure subtended by planes.
proper edge (or link): an edge with two distinct endpoints.
pseudograph: synonym for a graph with loops.
r-partite hypergraph: an r-uniform hypergraph whose vertex set can be partitioned
into r blocks so that each edge intersects each block in exactly one vertex.
r-uniform: property of a uniform hypergraph that r is the common edge-order.
radius: for a connected graph G, the minimum eccentricity among the vertices of G.

GLOSSARY
581
Ramanujan graph: A d-regular graph G such that λ(G) ≤2
√
d −1.
Ramsey number, classical: the number r(m, n), which is the smallest positive inte-
ger k such that every simple graph with k vertices either contains Km as a subgraph
or has a set of n independent vertices.
Ramsey number: the number R(G, H), which is the smallest positive integer k such
that, if the edges of Kk are bipartitioned into red and blue classes, then either the
red subgraph contains a copy of G or the blue subgraph contains a copy of H.
random graph on n vertices: an n-vertex graph generated by a probability distri-
bution, in which each edge is as likely to occur as any of the others.
reachable vertex (from vertex u): a vertex v such that there is a u, v-path.
reconstructible: property of a graph that it is uniquely determined by its collection
of vertex-deleted subgraphs.
reconstructible invariant: an invariant that is uniquely determined by the collection
of vertex-deleted subgraphs of a graph.
reducible: property of a digraph that its vertex set can be partitioned into a disjoint
union V1 ∪V2 so that all arcs joining V1 and V2 go from V1 to V2.
region: for a graph imbedded in a surface, a maximal expanse of surface containing no
vertex and no part of any edge of the graph.
regular: property of a graph or hypergraph that all its vertices have the same degree.
See also k-regular.
representation (of a graph): a graph description, such as a drawing, from which a
formal speciﬁcation can be constructed and labeled with the vertex names and edge
names, so as to obtain a graph that conforms to the incidence rule for the graph.
rooted pebbling number (of a graph): for a connected graph with root vertex r, the
minimum number t so that from every starting conﬁguration of t pebbles it is possible
to move a pebble to r via pebbling steps.
rotation system (of an imbedding): a list of the cyclic orderings of the incidence of
edges at each vertex.
Schreier graph: a graph that depicts the cosets of some subgroup of a group with some
set of generators; the vertices represent cosets, and the edges (often “color-coded”
for the generators) represent the product rule.
self-complementary: property of a graph that it is isomorphic to its complement.
self-loop: an edge that joins a vertex to itself; see loop.
simple digraph: a digraph that has no self-loops and no pair of arcs with the same
tail and head.
simple graph: a graph with no loops or multi-edges.
simple hypergraph: a hypergraph with no repeated edges.
sink: a digraph vertex with out-degree zero.
source: a digraph vertex with in-degree zero.
spanning subgraph: a subgraph of a graph G that includes all vertices of G.
speciﬁcation (of a graph): a list of its vertices and a list of its edges, with the incidence
rule for determining the endpoints of every edge.

582
Chapter 8
GRAPH THEORY
spectrum (of a graph): the multiset of its eigenvalues.
strong component: in a digraph, a maximal subdigraph that is strongly connected.
strong orientation: for a graph, an assignment of a direction to every edge making it
a strongly connected digraph.
strong product: for graphs G and H, the graph G ⊠H that has vertex set VG × VH
and edge set

(g′, h)(g′, h′) | (g = g′ or gg′ ∈EG) and (h = h′ or hh′ ∈EH)
	
.
strong tournament: a tournament in which there is a directed path from every vertex
to every other vertex.
strongly connected: property of a digraph that every vertex is reachable from every
other vertex.
strongly regular graph (with parameters (n, k, r, s)): an n-vertex, k-regular graph
in which every adjacent pair of vertices is mutually adjacent to r other vertices, and
in which every pair of nonadjacent vertices is mutually adjacent to s other vertices.
subdivision (of an edge): the operation of inserting a new vertex into the interior of
the edge, thereby splitting it into two edges.
subdivision: given a graph, any new graph obtained by subdividing one or more edges
one or more times.
subgraph: given a graph G, a graph whose vertices and edges are all in G.
supermagic graph: a connected graph whose edges can be labeled with consecutive
positive integers such that for each vertex v the sum of the labels of all edges incident
with v is the same for all v.
tail (of an arc): the vertex the arc goes from.
terminus (of a walk): the last vertex of the walk.
tetrahedral graph: another name for the complete graph K4, resulting from the fact
that it is equivalent to the 1-skeleton of the 4-sided Platonic solid called a tetrahe-
dron.
thickness: for a graph G, the minimum number θ(G) of planar subgraphs whose union
is G.
topological sort (or topsort): an algorithm that assigns a linear extension ordering
to a DAG.
tough graph: a connected graph G such that for every nonempty set S of vertices, the
number of components of the graph G −S does not exceed |S|.
total dominating set (of a graph): a set S of vertices such that every vertex is adja-
cent to a vertex in S.
total domination number: the minimum cardinality γt(G) of a total dominating set
in graph G.
tournament: a digraph with exactly one arc between each pair of distinct vertices.
trail: a walk in which no edge occurs more than once.
trail, directed: a directed walk in which no arc is repeated.
transitive: property of a digraph that whenever it contains an arc from u to v and an
arc from v to w, it also contains an arc from u to w.

GLOSSARY
583
transitive orientation: for a graph, an assignment of a direction to every edge, making
it a transitive digraph.
transmitter: in a digraph, a vertex that has an arc to every other vertex.
transversal: in a hypergraph, a set of vertices that has nonempty intersection with
every edge of the hypergraph.
transversal number: the minimum number τ(H) of vertices taken over all transversals
of H.
tree: a connected graph without a cycle.
trivial graph: the graph with one vertex and no edge.
Tur´an graph: the n-vertex k-partite simple graph Tk(n) with the maximum number
of edges.
u,v-path: a path whose origin is the vertex u and whose terminus is the vertex v.
underlying graph: for a digraph, the graph obtained from the digraph by stripping
the directions oﬀall the arcs.
uniform: property of a hypergraph that all edges have the same number of vertices.
See also r-uniform.
unilaterally connected (or unilateral): property of a digraph that for every pair of
vertices u, v, there is either a uv-path or a vu-path.
upset: a simple hypergraph in which every superset of every edge is also an edge of the
hypergraph.
valence: a synonym for degree (adapted from molecular bonds in chemistry).
vertex: a point; an element of the ﬁrst constituent set of a graph.
vertex coloring: an assignment of colors to the vertices of a graph so that adjacent
vertices receive diﬀerent colors.
(vertex) connectivity: the smallest number κ(G) of vertices whose removal discon-
nects the graph; by convention, κ(Kn) = n −1.
vertex cut: See disconnecting set.
vertex-deleted subgraph: any subgraph obtained from a graph by removing a single
vertex and all of its incident edges.
vertex invariant: a property at a vertex that is preserved by every isomorphism.
walk: an alternating sequence v0, e1, v1, . . . , er, vr of vertices and edges where consecu-
tive edges are adjacent, so that each edge ei joins vertices vi−1 and vi.
walk, directed: an alternating sequence of vertices and arcs v0, e1, v1, e2, . . . , en, vn
where the arcs align head to tail, so that each vertex is the head of the preced-
ing arc and the tail of the subsequent arc.
weakly connected (or weak) digraph: a digraph whose underlying graph is con-
nected.
weighted graph: a graph model in which each edge is assigned a number called the
weight or the cost.
wheel graph: an (n + 1)-vertex graph Wn that “looks like” a wheel whose rim is an
n-cycle and whose hub vertex is joined by spokes to all the vertices on the rim.

584
Chapter 8
GRAPH THEORY
8.1
INTRODUCTION TO GRAPHS
Graphs are highly adaptable mathematical structures, and can be represented on a com-
puter so that as new applications arise, existing algorithms can be reused without rewrit-
ing. This section provides some of the basic terminology and operations needed for the
study of graphs and describes several useful families of graphs.
8.1.1
VARIETIES OF GRAPHS AND GRAPH MODELS
Due to the vast breadth of the usefulness of graphs, the terminology varies widely, not
only from one type of graph to another, but also from one application to another. The
table in Fact 1 gives synonyms for several terms. Deﬁnitions for undirected graphs are
given ﬁrst, followed by deﬁnitions for directed graphs. Because much of the terminology
is similar for the two types of graph, only terms that are diﬀerent for directed graphs are
given.
Deﬁnitions:
A graph G = (V, E) is a set V of vertices and a set E of edges (both sets are ﬁnite and
V is nonempty unless speciﬁed otherwise) such that each edge is associated with either
two vertices or one vertex twice. It is sometimes denoted (VG, EG) or (V (G), E(G)).
A vertex is usually conceptualized as a point. Abstractly, it is a member of the ﬁrst of
the two sets that form a graph.
An edge is usually conceptualized as a line segment or curve, either joining one vertex
to another or joining a vertex to itself. Abstractly, it is a member of the second of the
two sets that form a graph.
The number of vertices of a graph is often called its order. The number of edges of a
graph is sometimes called its size.
A proper edge (or link) is an edge that joins one vertex to another.
A loop (or self-loop) is an edge that joins a vertex to itself.
The endpoints of an edge are the vertices that the edge joins. A loop has only one
endpoint.
An edge e is incident with a vertex v if v is an endpoint of e.
An incidence rule speciﬁes the endpoints of the edges.
A simple graph is a graph G that has no loops and in which no two edges have the
same endpoints. In a simple graph, an edge with endpoints v and w can be regarded as
the pair {v, w} and is often denoted simply as vw.
A graph with no edges is called a null graph. If further, V is empty, then the result is
the empty graph. (Note that some researchers invert the meanings of null graph and
empty graph.)
The trivial graph has just one vertex and no edges.
Vertices v and w are adjacent if there is an edge whose endpoints are v and w.

Section 8.1
INTRODUCTION TO GRAPHS
585
Two edges are adjacent if they have a common endpoint.
A neighbor of a vertex is any vertex to which it is adjacent.
An attribute of the edge set or vertex set of a graph is a feature such as length, cost,
or color sometimes attached to graphs.
A graph model is a graph which may have attributes on its edges or vertices. The
vertices and edges of the model may represent arbitrary objects and relationships from
the context of the application.
A weighted graph is a graph in which each edge is assigned a number called the weight
or cost.
A node is sometimes a synonym for a vertex and sometimes refers to whatever is modeled
by a vertex in a graph model.
A line is sometimes a synonym for an edge and sometimes refers to whatever is modeled
by an edge in a graph model.
If at least two edges have the same endpoints, the set of all edges with these endpoints
is called a multi-edge. If r is the number of these edges, this is a sometimes called an
edge of multiplicity r. A graph with a multi-edge is sometimes said to have multiple
edges or parallel edges.
A multigraph is another name for a graph with multi-edges but no loops, used for
emphasis when the context is largely restricted to simple graphs.
A pseudograph (or general graph) is another name for a graph in which loops and
multi-edges are permitted, used for emphasis when the context is largely restricted to
loop-free graphs.
The degree of vertex v, deg(v), is the number of proper edges plus twice the number of
loops incident with v. Thus, in a drawing, it is the number of edge-endings at v.
The valence of a vertex is a synonym for degree adapted from terminology in chemistry.
A vertex of degree 0 is called an isolated vertex.
A vertex of degree 1 is called a leaf or an end vertex.
A regular graph is a graph in which all vertices have the same degree.
It is called
k-regular if that degree is k.
The degree sequence of a graph is the sequence of the degrees of its vertices, usually
given in increasing or decreasing order.
A graphical sequence is a sequence of nonnegative integers that is the degree sequence
of some simple graph.
A digraph or directed graph D = (V, A) is a set V of vertices and a set A of arcs
(both sets are ﬁnite and V is nonempty unless speciﬁed otherwise) such that each arc
is associated with an ordered pair of vertices (which may be the same). It is sometimes
denoted (VD, AD) or (V (D), A(D)).
A direction on an edge is an ordering for its endpoints so that the edge goes from one
endpoint and to the other. Any edge, including a loop, can be directed by giving it a
sense of forward progression; for example, in a graph drawing, by placing an arrow on
the edge.

586
Chapter 8
GRAPH THEORY
An arc is a directed edge.
The tail of an arc is the vertex at which the arc originates. The head is the vertex at
which the arc terminates.
Two arcs are parallel if they have the same head and the same tail. They are opposite
if the tail of each is the head of the other.
A strict digraph has no parallel arcs. In set theory terms, it is the graph of a relation.
A simple digraph has no loops and no parallel arcs. In set theory terms, it is the graph
of an irreﬂexive relation.
The out-degree of vertex v, denoted od(v) or deg+(v), is the number of arcs having
v as tail. The in-degree of vertex v, denoted id(v) or deg−(v), is the number of arcs
having v as head.
A vertex with in-degree 0 and positive out-degree is called a source or transmitter.
A vertex with out-degree 0 and positive in-degree is called a sink or receiver.
The degree pair of vertex v is the ordered pair (od(v), id(v)).
The degree sequence of a digraph is the sequence of the degree pairs of its vertices,
usually given in increasing or decreasing order of the out-degree.
A digraphical sequence is a sequence of ordered pairs of nonnegative integers that is
the degree sequence of some simple digraph.
Facts:
1. The following table lists some graph theory synonyms.
vertex: point, node
edge: line, link
loop: self-loop
neighbor: adjacent vertex
arc: directed edge
degree: valence
number of vertices: order
number of edges: size
nonsimple graph: pseudograph, general graph
loop-free nonsimple graph: multigraph
2. The following table lists some of the varieties of graphs and digraphs.
graph variety
loops allowed?
multi-edges allowed?
simple graph
NO
NO
multigraph
NO
YES
general graph
YES
YES
pseudograph
YES
YES
digraph
YES
YES
strict digraph
YES
NO*
simple digraph
NO
NO*
*at most one arc in each direction between two vertices

Section 8.1
INTRODUCTION TO GRAPHS
587
3. In a drawing of a graph, the degree of a vertex v equals the number of edge-ends at v.
The degree of v need not equal the number of edges at v, since each loop contributes
twice toward the degree.
4. In every graph, the sum of the degrees equals twice the number of edges. From this
it follows that the sum of the degrees of all vertices is even.
5. In every graph the number of vertices of odd degree is even.
6. The name handshaking lemma is commonly applied to various elementary results
about the degrees of simple graphs, especially Facts 4 and 5.
7. In every simple graph with at least two vertices, there is a pair of vertices with the
same degree.
8. Havel’s theorem:
A sequence of nonnegative integers is graphical if and only if the
sequence obtained by deleting the largest entry d and subtracting 1 from each of the
d next largest entries is graphical. (V. Havel, 1955)
9. A nonincreasing sequence of nonnegative integers d1, d2, . . . , dn is graphical if and
only if its sum is even, and for k = 1, 2, . . . , n,
kP
i=1
di ≤k(k −1) +
nP
i=k+1
min{k, di}.
(P. Erd˝os and T. Gallai, 1960)
10. In every digraph, the sum of the in-degrees equals the sum of the out-degrees, and
this is equal to the number of arcs.
11. A sequence of ordered pairs of nonnegative integers is digraphical if and only if the
sequence obtained by replacing the largest ﬁrst entry d by 0 and subtracting 1 from each
of the d largest second entries among the other pairs is digraphical.
12. A sequence of ordered pairs of nonnegative integers at most n−1, say (a1, b1), (a2, b2),
. . . , (an, bn), with the ai nonincreasing is digraphical if and only if Pn
i=1 ai = Pn
i=1 bi,
and for k = 1, 2, . . ., n,
kP
i=1
ai ≤
kP
i=1
min{k −1, bi} +
nP
i=k+1
min{k, bi}.
(D. L. Fulkerson, 1960)
Examples:
1. The following ﬁgure gives examples of the various varieties of graphs. (See the table
in Fact 2.)
graph/pseudograph
simple graph
multigraph
2. The following ﬁgure gives examples of the various varieties of digraphs. (See the table
in Fact 2.)

588
Chapter 8
GRAPH THEORY
digraph
strict digraph
simple digraph
3. Computer programming ﬂowchart (always a digraph): Each vertex represents some
programmed operation or decision, and each arc represents the ﬂow of control to the
next operation or decision.
no
yes
yes
no
D divides N
?
PRINT
N is
not
prime
PRINT
N is
prime
Input N
D := 2
D :=  D+1
D>   N
4. Model for social networks: Each vertex represents a person in the network, and each
edge represents a form of interaction between the persons represented by its endpoints.
This is illustrated by the following graph.
5. Model for road networks (most edges undirected): Each vertex represents either an
intersection of two roads or the end of a dead-end street. The absence of an endpoint
in the illustration indicates that the road continues beyond what is shown. Direction on
an edge may be used to indicate a one-way road, with undirected edges being two-way
roads.
South Mill
Buchak
Stuart
Woodmere
Amherst
Brian’s Way
Brian’s Way
6. In the following graph with vertex set V = {v1, v2, v3, v4, v5} and edge set E =
{e1, e2, e3, e4, e5, e6, e7}, the vertex v5 is an isolated vertex, and the degree sequence is
(0, 3, 3, 4, 4). The edge e7 is a loop, and the three edges e4, e5, and e6 form a multi-edge.

Section 8.1
INTRODUCTION TO GRAPHS
589
v1
v2
v3
v4
v5
e7
e1
e3
e2
e5 e4
e6
7. Deleting the isolated vertex and the self-loop in Example 6 and then reducing the
multi-edge to a single edge yields the following simple graph, whose degree sequence is
(1, 2, 2, 3).
v1
v2
v3
v4
e1
e3
e2
e4
8. One possible choice of edge directions for the graph of Example 6 yields this digraph.
v1
v2
v3
v4
v5
e7
e1
e3
e2
e5 e4
e6
9. The sequence (1, 2, 2, 3, 4, 5) is not graphical, by Fact 4, because its sum is odd.
10. Havel’s reduction (Fact 8) of the sequence (2, 2, 2, 3, 4, 5) is (1, 1, 1, 2, 3). Havel’s
reduction of that sequence is (0, 0, 1, 1).
Since (0, 0, 1, 1) is the degree sequence of a
graph with four vertices, two of which are isolated and two of which are joined by an
edge, it follows from Havel’s theorem that the sequence (2, 2, 2, 3, 4, 5) is graphical.
11. The sequence (2, 1), (2, 0), (1, 1), (1, 3), is not digraphical since the sum of the ﬁrst
entries does not equal the sum of the second entries.
12. The reduction of the sequence (3, 1), (1, 2), (1, 2), (1, 1) using Fact 11 is the sequence
(0, 1), (1, 1), (1, 1), (1, 0). Since the second sequence is digraphical, being the degree se-
quence of the directed path of length 3, the ﬁrst sequence is also digraphical.
8.1.2
GRAPH OPERATIONS
Deﬁnitions:
A graph H = (VH, EH) is a subgraph of a graph G = (VG, EG) if its vertex set and
edge set are subsets of VG and EG, respectively.
A spanning subgraph of a graph G is a subgraph that contains all the vertices of G.
The induced subgraph on a nonempty set S of vertices in a graph G is the subgraph
whose vertex set is S and whose edge set consists of all edges of G having both endpoints
in S. It may be denoted by ⟨S⟩or G[S] and is the maximal subgraph of G with vertex
set S.

590
Chapter 8
GRAPH THEORY
An induced subgraph of G is a subgraph H such that every edge of G that joins two
vertices of H is also an edge of H.
Deleting an edge e from a graph G results in the subgraph G −e that contains all the
vertices of G and all the edges of G except for e.
Deleting a set Y of edges from a graph G results in the subgraph G−Y that contains
all the vertices of G and all the edges of G except for those in Y .
Deleting a vertex v from a graph G results in the subgraph G −v that contains all
the vertices of G except v and all the edges of G except those incident with v. It is the
induced subgraph ⟨VG −{v}⟩.
Deleting a set S of vertices from a graph G results in the subgraph G−S that contains
all the vertices of G except those in S and all the edges of G except those incident with
vertices in S. It is the induced subgraph ⟨VG −S⟩.
Contracting an edge e in a graph G means shrinking the edge to a point, so that its
endpoints are merged, without changing the rest of the graph. The resulting graph is
denoted G/e (or G · e or G ↓e). To construct G/e from G, delete the edge e from the
edge set and replace all instances of its endpoints in the vertex set and incidence rule by
a new vertex.
A minor of a graph G is any graph that can be obtained from G by a sequence of edge
deletions and contractions and vertex deletions.
The graph union G ∪H has as its vertices and edges those vertices and edges that are
in G or H.
The graph intersection G ∩H has as its vertices and edges those vertices and edges
that are both in G and in H.
The graph sum (or disjoint union) G + H consists of the disjoint unions of the vertex
sets and of the edge sets of the graphs G and H.
The iterated graph sum nG is the sum of n disjoint copies of G.
The join G ∗H is obtained by adding to G + H an edge from each vertex in G to each
vertex in H.
The Cartesian product G □H (or sometimes G × H) has as its vertices the product
VG × VH and as its edges (VG × EH) ∪(EG × VH). The endpoints of the edge (u, d) are
the vertices (u, x) and (u, y), where x and y are the endpoints of d in H, and those of
the edge (e, w) are (u, w) and (v, w), where u and v are the endpoints of e.
An isomorphism f : G →H (of graphs) establishes their structural equivalence. It is
given by a pair of bijections fV : VG →VH and fE : EG →EH such that if u and v are
the endpoints of edge e in graph G, then fV (u) and fV (v) are the endpoints of fE(e) in
graph H. The vertex function and the edge function can both be denoted f without the
subscript. (See §8.5.)
Two graphs are isomorphic if there is an isomorphism between them. This means that
they are essentially the same graph except for the names of their vertices and edges.
A graph mapping f : G →H (of graphs) is a pair of functions fV : VG →VH and
fE : EG →EH such that if u and v are the endpoints of edge e in G, then f(u) and f(v)
are the endpoints of f(e) in H. Such a pair of functions is said to preserve incidence.
The vertex function and the edge function can both be denoted f without the subscript.

Section 8.1
INTRODUCTION TO GRAPHS
591
An automorphism of a graph G is an isomorphism of G to itself.
The automorphism group Aut(G) is the group of all automorphisms of graph G.
Subdividing an edge e is the operation of inserting a new vertex in the interior of an
edge. Combinatorially, this is achieved by deleting e and adding a new vertex adjacent
to the endpoints of e.
Two graphs are homeomorphic if there is a graph from which they can both be obtained
by a sequence of edge subdivisions.
The complement G of a simple graph G has the same vertex set as G, with two vertices
being adjacent in G if and only if they are not adjacent in G.
A self-complementary graph is a graph that is isomorphic to its complement.
The line graph L(G) of a graph G has vertices corresponding to the edges of G, with
two vertices in L(G) being adjacent whenever the corresponding edges are adjacent in G.
Facts:
1. If a graph J is isomorphic to a subgraph of a graph G, then it is commonly said
that J “is” a subgraph of G, even though VJ and EJ might not be subsets of VG and EG.
2. A graph is a subgraph of its union with any other graph.
3. The intersection of two graphs is a subgraph of each of them.
4. A graph mapping is the combinatorial counterpart of what is topologically a contin-
uous function from one graph to the other.
5. A graph isomorphism is a graph mapping for which both the vertex function and the
edge function are bijections.
6. There is a self-complementary graph of order n if and only if n ≡0 or 1 (mod 4).
7. The automorphism group Aut(G) of any simple graph is isomorphic to the automor-
phism group Aut(G) of its complement.
8. A connected graph G is isomorphic to its line graph if and only if G is a cycle (§8.1.3).
9. If two connected graphs have isomorphic line graphs, then either they are isomorphic
to each other or they are K3 and K1,3 (see §8.1.3).
10. Aut(Kn) is isomorphic to the symmetric group Sn (see §5.3.1).
11. Aut(Cn) is isomorphic to the dihedral group Dn (see §5.3.2).
Examples:
1. The dark subgraph spans the following graph, because it contains every vertex of the
given graph.
2. The Cartesian product C4 □K2 (see §8.1.3) is illustrated as follows:

592
Chapter 8
GRAPH THEORY
=
3. The join K2 ∗P3 (see §8.1.3) is illustrated as follows:
=
4. The following two graphs are homeomorphic, but not isomorphic.
5. The graphs K3,3 and 2K3 (see §8.1.3) are complements of each other.
6. The path P4 and the cycle C5 (see §8.1.3) are self-complementary.
7. The line graph L(K4) (see §8.1.3) is isomorphic to the octahedral graph K2,2,2.
8.1.3
SPECIAL GRAPHS AND GRAPH FAMILIES
Deﬁnitions:
Note: Many of the following graphs are drawn in Figures 1 and 2.
The bouquet Bn is the graph with one vertex and n loops.
The dipole Dn is the graph with two vertices and an edge of multiplicity n joining them.
The complete graph Kn is the simple graph with n vertices in which every pair of
vertices is adjacent.
The n-path Pn consists of a sequence of n vertices v1, v2 . . . , vn and the n−1 edges joining
successive vertices in the sequence; that is, the n −1 edges are v1v2, v2v3, . . . , vn−1vn.
A path is a graph that is an n-path for some n ≥1.
The n-cycle Cn consists of a sequence of n vertices v1, v2, . . . , vn and the n edges joining
successive vertices cyclically; that is, the n edges are v1v2, v2v3, . . . , vn−1vn, vnv1.
A cycle is a graph which is an n-cycle for some n > 0.
The n-wheel Wn is the join of K1 and the n-cycle Cn.
A graph is bipartite if its vertices can be partitioned into two subsets (the parts, or
partite sets) so that no two vertices in the same part are adjacent.
The complete bipartite graph Kr,s is the simple bipartite graph in which the two
parts have cardinalities r and s, such that every vertex in one part is adjacent to every
vertex in the other part.

Section 8.1
INTRODUCTION TO GRAPHS
593
The complete r-partite graph Kn1,n2,...,nr has r disjoint subsets of vertices of orders
n1, n2, . . . , nr, with two vertices adjacent if and only if they lie in diﬀerent subsets. If
the r sets all have t vertices, this graph is sometimes denoted Kr(t).
A graph G is connected if for each pair of vertices in G, there is a path in G from one
to the other.
A tree is a connected graph without any cycles as subgraphs. (See Chapter 9.) A forest
is a graph without any cycles as subgraphs.
The Kuratowski graphs are the graphs K5 and K3,3.
The Petersen graph is the graph constructed from two disjoint 5-cycles on the vertices
v1, v2, v3, v4, v5 and w1, w2, w3, w4, w5 by adding the edges v1w2, v2w4, v3w1, v4w3, v5w5.
(See Figure 2.)
A Platonic solid is a regular 3-dimensional polyhedron.
The 1-skeleton of a polyhedron is the graph that has as its vertices and edges those of
the polyhedron.
The tetrahedral graph is the 1-skeleton of the 4-sided Platonic solid called a tetrahe-
dron (its faces are triangles). It has 4 vertices, each of degree 3, and 6 edges.
The cube graph Q3 is the 1-skeleton of the 6-sided Platonic solid called a cube (its
faces are squares). It has 8 vertices, each of degree 3, and 12 edges.
The d-dimensional hypercube graph Qd is a graph with 2d vertices that can be labeled
with the 2d bitstrings of length d so that two vertices are adjacent if and only if their
labels diﬀer in exactly one bit.
The octahedral graph O3 is the 1-skeleton of the 8-sided Platonic solid called an
octahedron (its faces are triangles). It has 6 vertices, each of degree 4, and 12 edges.
The generalized octahedral graph On is the graph that can be obtained from the
complete graph K2n by removing n mutually nonadjacent edges. It is isomorphic to
Kn(2).
The dodecahedral graph is the 1-skeleton of the 12-sided Platonic solid called a do-
decahedron (its faces are pentagons). It has 20 vertices, each of degree 3, and 30 edges.
The icosahedral graph is the 1-skeleton of the 20-sided Platonic solid called an icosa-
hedron (its faces are triangles). It has 12 vertices, each of degree 5, and 30 edges.
The intersection graph of a ﬁnite collection F = {S1, S2, . . . , Sk} of subsets of some
set has as its vertices the subsets themselves, with an edge between each pair of subsets
whose intersection is nonempty.
An interval graph is any graph isomorphic to the intersection graph of some collection
of intervals on the real line.
Facts:
1. In a computer program, the trivial graph is often used as the initial value of a graph-
valued variable, similar to how an integer-valued variable is initialized to zero. As the
program runs, the graph-valued variable can be modiﬁed by adding vertices and edges.
2. Bouquets and dipoles are fundamental building blocks for graphs constructed by
topological techniques.
3. Every path is a tree.

594
Chapter 8
GRAPH THEORY
4. A graph is bipartite if and only if it has no cycles of odd length.
5. Every tree is bipartite.
6. The hypercube graphs Qn can be deﬁned recursively as follows: Q0 = K1, Qn =
K2 □Qn−1 for n > 0.
7. The hypercube graph Qn is bipartite and is isomorphic to the lattice of subsets of a
set of n elements. (See §5.7.1.)
8. The octahedral graphs On can be deﬁned recursively as follows: O0 = K2, On =
K2 ∗On−1 for n > 0.
9. There are exactly ﬁve Platonic graphs: the tetrahedral graph K4, the cube Q3, the
octahedral graph O3, the dodecahedral graph, and the icosahedral graph.
Examples:
1. Figure 1 shows some of the classes of graphs that occur most often in general con-
structions.
2. Figure 2 shows some of the graphs that occur most often as special examples.
8.1.4
GRAPH REPRESENTATION AND COMPUTATION
To apply a computer to graph-theoretic computations, it is necessary to specify the
underlying graph completely and without ambiguity. The representations are also useful
in their own right.
Deﬁnitions:
A speciﬁcation of a graph is a list of its vertices, a list of its edges, and the incidence
rule for determining the endpoints of every edge.
An endpoint table for a graph is a tabular description of the incidence rule, that gives
the endpoints of every edge. In a digraph or partially directed graph, the tail and head
of each arc are distinguished.
An incident-edge table for a graph is a tabular description of the incidence rule, that
gives for each vertex v, a list of the edges having v as an endpoint.
If the graph is
directed, this list is partitioned into two sublists, according to whether v is tail or head.
A representation of a graph G is a graph description, such as a drawing, from which
a formal speciﬁcation could be constructed and labeled with the vertex names and edge
names from G, so as to obtain a graph that conforms to the incidence rule for G.
The incidence matrix of a graph (without loops) G with vertices v1, v2, . . . , vn and
edges e1, e2, . . . , em is the n × m matrix MI with
MI[i, j] =
(
0
if vi is not an endpoint of ej
1
if vi is an endpoint of ej.
(In case loops are present, if ej is a loop at vi, then usually MI[i, j] = 2, but sometimes
MI[i, j] = 1 even though this violates the properties that every column-sum equals 2 and
every row-sum equals the degree of the corresponding vertex.)

Section 8.1
INTRODUCTION TO GRAPHS
595
Figure 1: Some fundamental inﬁnite classes of graphs.
dipoles
D1
D2
D3
D4
D5
complete
graphs
K1
K2
K3
K4
K5
paths
P2
P3
P4
P5
P6
complete
bipartite
graphs
K3,3
K3,4
K3,5
K1,1
K2,2
K1,2
K1,3
K2,4
K2,3
cycles
C1
C2
C3
C4
C5
wheels
W1
W2
W3
W4
W5
bouquets
B1
B2
B3
B4
B5
complete
tripartite
graphs
K1,1,1
K1,1,2
K1,2,2
K1,2,3

596
Chapter 8
GRAPH THEORY
hypercubes
small
trees
Q0
Q1
Q2
Q4
Q3
Figure 2: Some special graphs.
K5
K3,3
tetrahedral graph
cube graph
octahedral graph
dodecahedral graph
icosahedral graph
Kuratowski
graphs
Petersen
graph
Platonic
graphs

Section 8.1
INTRODUCTION TO GRAPHS
597
The adjacency matrix of a loop-free graph G with vertices v1, v2, . . . , vn is the n × n
matrix A with
A[i, j] = the number of edges between vi and vj if i ̸= j.
If there are loops, then A[i, i] is usually deﬁned to be the number of loops at vi.
A normalized drawing of a graph represents each vertex as a distinct point in the
plane and each edge as a possibly curved line between endpoints, such that
• the interior of an edge does not contain any vertex;
• at most two edges cross at any point of the plane;
• two edges cross each other at most once;
• each edge crossing is normal, not a tangency.
A complete set of operations on graphs is a set from which all other operations can
be constructed. The operations in a complete set are primitive if none can be derived
from the other operations.
A graph computation package is a computer software system that represents graphs
and includes a complete set of operations.
Facts:
1. Despite the redundancy, an incident-edge table is often used with an endpoint table in
computer software, since it facilitates fast searching at the cost of relatively little space.
2. If a graph is simple, then its edges can be represented as endpoint pairs vw. Thus,
the graph can be speciﬁed as a list of endpoint pairs and a list of isolated vertices.
3. If a graph is simple, then its incident-edge table can be represented as a table that
gives the list of neighbors of every vertex.
4. If A is the adjacency matrix of graph G, then the (i, j)-entry of Ak is the number of
walks (see §8.4.1) of length k from vi to vj in G.
5. Matrix-tree theorem:
Let G be a graph, and let A be its adjacency matrix and D
the diagonal matrix of the degrees of its vertices. Then the value of every cofactor of
D −A equals the number of spanning trees of G. (G. R. Kirchhoﬀ, 1847)
6. Given the incidence matrix of G, it is possible to obtain the incidence matrix for a
subgraph H of G by deleting all rows and columns corresponding to vertices and edges
that are not in H.
7. The most commonly used complete set of operations is adding a vertex, deleting a
vertex, adding an edge, and deleting an edge.
8. Graph computation packages are built into mathematical computation systems such
as Maple and Mathematica. They usually include display operations.
Examples:
1. The following normalized drawing, endpoint table, incident-edge table, incidence ma-
trix and adjacency matrix all specify the same graph G.
v1
v2
v3
v4
v5
e7
e1
e3
e2
e5 e4
e6

598
Chapter 8
GRAPH THEORY
endpoint table
e1
e2
e3
e4
e5
e6
e7
v1
v2
v2
v3
v3
v3
v1
v2
v3
v4
v4
v4
v4
incident-edge table
v1
e1
e7
v2
e1
e2
e3
v3
e2
e4
e5
e6
v4
e3
e4
e5
e6
v5
MI =






e1
e2
e3
e4
e5
e6
e7
v1
1
0
0
0
0
0
2
v2
1
1
1
0
0
0
0
v3
0
1
0
1
1
1
0
v4
0
0
1
1
1
1
0
v5
0
0
0
0
0
0
0






A =






v1
v2
v3
v4
v5
v1
1
1
0
0
0
v2
1
0
1
1
0
v3
0
1
0
3
0
v4
0
1
3
0
0
v5
0
0
0
0
0






2. The following normalized drawing, list of endpoint pairs, lists-of-neighbors table,
incidence matrix and adjacency matrix all specify the same simple graph H. It is a
spanning subgraph of the graph G of Example 1, but it is not an induced subgraph.
v1
v2
v3
v4
e1
e3
e2
e4
endpoint pairs:
v1v2, v2v3, v2v4, v3v4
lists-of-neighbors
v1 :
v2
v2 :
v1
v3
v4
v3 :
v2
v4
v4 :
v2
v3
MI =




e1
e2
e3
e4
v1
1
0
0
0
v2
1
1
1
0
v3
0
1
0
1
v4
0
0
1
1




A =




v1
v2
v3
v4
v1
0
1
0
0
v2
1
0
1
1
v3
0
1
0
1
v4
0
1
1
0




3. Squaring and cubing the adjacency matrix of Example 2 provides an illustration of
Fact 4.
A2 =




v1
v2
v3
v4
v1
1
0
1
1
v2
0
3
1
1
v3
1
1
2
1
v4
1
1
1
2




A3 =




v1
v2
v3
v4
v1
0
3
1
1
v2
3
2
4
4
v3
1
4
2
3
v4
1
4
3
2




For instance, the three walks of length 3 from v4 to v3 are as follows:
⟨v4, e3, v2, e3, v4, e4, v3⟩, ⟨v4, e4, v3, e4, v4, e4, v3⟩, ⟨v4, e4, v3, e2, v2, e2, v3⟩.
4. As an illustration of the Kirchhoﬀmatrix-tree theorem of Fact 5, observe that the
graph of Example 2 has the following three spanning trees.

Section 8.2
GRAPH MODELS
599
v1
v1
v1
e1
e1
e1
e3
e3
e4
e4
e2
e2
v3
v3
v3
v2
v2
v2
v4
v4
v4
The value of the (2, 2)-cofactor of the matrix D −A is also equal to 3:
D −A =




v1
v2
v3
v4
v1
1
−1
0
0
v2
−1
3
−1
−1
v3
0
−1
2
−1
v4
0
−1
−1
2




cofactor =

1
0
0
0
2
−1
0
−1
2

= 3.
8.2
GRAPH MODELS
Modeling with graphs is one of the main ways in which discrete mathematics has been
applied to real world problems. This section gives a list of some of the ways in which
graphs are used as mathematical models. Further information can be found in for exam-
ple [ArGr06] and [Ro76].
8.2.1
ATTRIBUTES OF A GRAPH MODEL
Deﬁnitions:
A mathematical representation of a physical or behavioral phenomenon is a corre-
spondence between the parts and processes of that phenomenon and a mathematical
system of objects and functions.
A model of a physical or behavioral phenomenon is the mathematical object or function
assigned to that phenomenon under a mathematical representation.
Modeling is the mathematical activity of designing models and comprehensive mathe-
matical representations of physical and behavioral phenomena.
A graph model is a mathematical representation that involves a graph.
Examples:
Table 1 gives many examples of graph models. Each example states what the vertices
and edges (or arcs) represent and where in the Handbook details can be found.
Table 1: Directory of graph models.
subject area
vertex attributes and meaning
reference
and application
edge/arc attributes and meaning
computer programming
vertex labels are program steps
§8.1.1
ﬂowcharts
edge directions show ﬂow
social organization
vertices are persons
§8.1.1
social networks
edges represent interactions

600
Chapter 8
GRAPH THEORY
subject area
vertex attributes and meaning
reference
and application
edge/arc attributes and meaning
civil engineering
vertices are road intersections
§8.1.1,
road networks
edges are roads
§8.3.1
operations research
vertices are activities
§8.3.1
scheduling
arcs show operational precedence
sociology
vertices are individuals
§8.3.1
hierarchical dominance
arcs show who reports to whom
computer programming
vertices are subprograms
§8.3.1
subprogram-calling diagram
arcs show calling direction
ecology
vertices are species
§8.3.1
food webs
arcs show who eats whom
operations research
vertices are activities
§8.3.1,
scheduling
edges are activity conﬂicts
§8.6.1
genealogy
vertices are family members
§8.3.1
family trees
arcs show parenthood
set theory
vertices are elements
§8.3.1
binary relations
arcs show relatedness
probabilistic analysis
vertices are process states
§8.3.2
Markov models
edges are state transitions
traﬃc control
vertices are intersections
§8.3.3
assigning one-way streets
edges are streets
partially ordered sets
vertices are elements
§8.3.4
Hasse diagrams
arcs show covering relation
computer engineering
vertices are computational nodes
§8.4.2
communications networks
arcs are communications links
operations research
vertices are supply and demand nodes
§8.4.2
transportation networks
arcs are supply lines
walking tours
vertices are land masses
§8.4.3
Seven Bridges of K¨onigsberg
edges are bridges
postal delivery routing
vertices are street intersections
§8.4.3
Chinese Postman Problem
edges are streets
information theory
vertices are binary strings
§8.4.4
Gray codes
edges are single-bit changes
radio broadcasting
vertices are broadcast stations
§8.6.1
assignment of frequencies
edges are potential interference
chemistry
vertices are chemicals
§8.6.1
preventing explosions
edges are co-combustibility
cartography
regions are countries
§8.6.4
map-coloring
edges are shared borders

Section 8.3
DIRECTED GRAPHS
601
subject area
vertex attributes and meaning
reference
and application
edge/arc attributes and meaning
highway construction
vertices are road intersections
§8.7.1
avoiding overcrossings
edges are roads
electrical network boards
vertices are circuit components
§8.7.1
avoiding insulation
edges are wires
VLSI computer chips
vertices are circuit components
§8.7.4
minimizing layering
edges are wires
information management
vertices are data records
§18.1.4
binary search trees
edges are decisions
computer operating systems
vertices are jobs
§18.1.5
priority trees
edges are priority relations
physical chemistry
vertices are atoms
§9.3.2
counting isomers
edges are molecular bonds
network optimization
edges are connections
§10.1.1
min-cost spanning trees
edge-labels are costs
bipartite matching
parts are people and jobs
§10.2.2
personnel assignment
edges are job-capabilities
network optimization
vertices are locations
§10.3.1
shortest path
edge-labels are distances
traveling salesman routing
vertices are locations
§10.7.1
shortest complete tour
edge-labels are distances
8.3
DIRECTED GRAPHS
Assigning directions to the edges of a graph is useful in modeling, and is natural whenever
order is important, e.g., in a hierarchical structure or a one-way road system. Also, any
graph may be viewed as a digraph, by replacing each edge with two directed edges,
one in each direction. Many graph problems are best solved as special cases of digraph
problems, for instance, ﬁnding shortest paths, maximum ﬂows, and connectivity.
8.3.1
DIGRAPH MODELS AND REPRESENTATIONS
Most terminology applies equally well to graphs and digraphs. The deﬁnitions below
are special to digraphs or take on a somewhat diﬀerent meaning for digraphs. Further,
where it is clear that only digraphs are being discussed, “directedness” is often an implicit
attribute of an “edge”, “path”, and other terms.
Deﬁnitions:
A directed graph, or digraph, consists of a set V of vertices and a set E of directed
edges or arcs, and an incidence function that assigns to each edge a tail and a head.

602
Chapter 8
GRAPH THEORY
The tail of an arc is the vertex it leaves, and the head is the vertex it enters. An arc
with tail u and head v is often denoted by uv.
A simple digraph has no self-loops or multi-arcs.
The underlying graph of a digraph is the graph obtained from the digraph by re-
placing every directed edge by an undirected edge.
The out-degree of vertex v, denoted δ+(v), is the number of arcs with tail at v.
The in-degree of vertex v, denoted δ−(v), is the number of arcs with head at v.
A digraph is transitive if whenever it contains an arc from u to v and an arc from v
to w, it also contains an arc from u to w.
The adjacency matrix of a digraph is the matrix A = [aij] where aij is the number of
arcs from vi to vj.
The incidence matrix of a digraph with no self-loops is the matrix M = [bij], where
bij = +1 if vi is the tail of ej, bij = −1 if vi is the head of ej, and bij = 0 otherwise.
(There is no standard convention for self-loops.)
Facts:
1. Simple-digraph terminology: In a context focusing on simple digraphs, there is often
a diﬀerent convention:
• “digraph” refers to a simple digraph;
• a directed graph with multi-arcs is called a multidigraph;
• a directed graph with self-loops is called a pseudodigraph;
2.
Alternative “path” terminology:
There is an alternative convention in which a
(directed) “path” may use vertices and arcs more than once, but an “elementary path”
does not repeat arcs, and a “simple path” does not repeat vertices (and, hence, does not
repeat arcs either). See §8.3.2.
3. A digraph is frequently represented by an arc list, in which each arc is represented
by an ordered pair uv, where u is its tail and v is its head. There is a separate entry for
each arc, so that uv occurs as often as the number of such arcs. A list of the isolated
vertices plus such an arc list completely speciﬁes a digraph.
4. Another common speciﬁcation of a digraph is the lists-of-neighbors representation.
For each vertex u, there is a list, which contains the head of each arc whose tail is u.
Thus a vertex v occurs in that list as many times as there are arcs from u to v.
5. The incidence matrix is another way to represent a digraph. Since all but one or two of
the entries in every column are zero, the incidence matrix is an ineﬃcient representation.
6. The adjacency matrix is a way to specify a digraph when there is no reason to identify
the arcs by name.
7. A digraph can be represented by a 2 × |E| incidence table in which the tail and head
of arc e appear in column e. Direction on an arc can be indicated by a convention as to
whether tail or head appears in the ﬁrst row.
8. The row-sum in a directed adjacency matrix equals the out-degree of the correspond-
ing vertex. The column-sum equals the in-degree.
9. In a digraph, the sum of the in-degrees, the sum of the out-degrees, and the number
of arcs are all equal to each other; that is, P
v∈V δ−(v) = P
v∈V δ+(v) = |E|.

Section 8.3
DIRECTED GRAPHS
603
Examples:
1. The following arc list, incidence table, list-of-neighbors, and adjacency matrix all
represent the digraph G.
arc list:
uv, vv, vw, xw, xw, ux, xu
incidence table:
e1
e2
e3
e4
e5
e6
e7
u
v
v
x
x
u
x
v
v
w
w
w
x
u
lists-of-neighbors:
u : v, x
v : v, w
w : ∅
x : w, w, u
adjacency matrix:




u
v
w
x
u
0
1
0
1
v
0
1
1
0
w
0
0
0
0
x
1
0
2
0




2. Civil Engineering: A road network in which at least some of the roads are one-way
can be modeled by a digraph. The vertices are road junctures; each two-way road is
represented by a pair of arcs, one in each direction. Loops are allowed, and they may
represent “circles” that occur in housing developments and in industrial parks. Similarly,
multi-arcs may occur.
3. Operations Research:
A large project consists of many smaller tasks with a prece-
dence relation—some tasks must be completed before certain others can begin.
The
vertices represent tasks, and there is an arc from u to v if task u must be completed
before v can begin. For instance, in the following ﬁgure it is necessary both that food is
loaded and the cabin is cleaned before passengers are loaded.
4. Sociology and Sociobiology:
A business (or army, or society, or ant colony) has a
hierarchical dominance structure. The vertices are the employees (soldiers, citizens, ants)
and there is an arc from u to v if u dominates v. If the chain of command is unique, with
a single leader, and if only arcs representing immediate authority are included, then the
result is a rooted tree. (See §9.1.2.)
5. Computer Software Design:
A large program consists of many subprograms, some
of which can invoke others. Let the vertices of D be the subprograms, and let there be

604
Chapter 8
GRAPH THEORY
an arc from u to v if subprogram u can invoke subprogram v. Then the call graph D
encapsulates all possible ways control can ﬂow within the program.
Directed cycles
represent indirect recursion, and serve as a warning to the designer to ensure against
inﬁnite loops. See the following ﬁgure, where subprogram 2 can call itself indirectly.
Main
Program
Proc  1
Proc  4
Proc  3
Proc  2
6. Ecology: A food web is a simple digraph in which vertices represent species and in
which there is an arc from u to v if species u eats species v. The following ﬁgure shows
a small food web.
frog
spider
grackle
cherry tree
beetle
7. Operations Research:
A sequence of books must be printed and bound, using one
press and one binding machine. Suppose that book i requires time pi for printing and
time bi for binding. It is desired to print the books in such an order that the binding
machine is never idle: when it ﬁnishes one book, the next book should already be printed.
The vertices of a digraph D can represent the books. There is an arc from book i to
book j if pj ≤bi. Then any path through all the vertices corresponds to a permissible
ordering.
8. Genealogy: A “family tree” is a digraph where the orientation is traditionally given
not by arrows but by the direction down for later generations. Despite the name, a family
tree is usually not a tree, since people commonly marry distant cousins, knowingly or
unknowingly.
9. Binary Relations: To any binary relation R on a set V (see §1.4.1) a digraph D(V, R)
can be associated: the vertices are the elements of V , and there is an arc from u to v if
(u, v) ∈R. Conversely, every digraph without multiple arcs deﬁnes a binary relation on
its vertices. The relation R is transitive if and only if the digraph D(V, R) is transitive.
8.3.2
PATHS, CYCLES, AND CONNECTEDNESS
Deﬁnitions:
A directed walk is a sequence of arcs such that the head of one arc is the tail of the
next arc.
The length of a directed walk is the number of arcs in the sequence.
A closed directed walk is a directed walk that begins and ends at the same vertex.

Section 8.3
DIRECTED GRAPHS
605
A directed trail is a directed walk in which no arc is repeated.
A directed path is a directed trail in which no vertex is repeated.
A directed cycle is a closed directed trail in which no vertices are repeated, except the
initial and ﬁnal vertex.
Vertex v is reachable from vertex u if there is a directed path from u to v.
A basis for a digraph is a set of vertices V ′ such that every vertex not in V ′ is reachable
from V ′ and such that no proper subset of V ′ has this property.
The distance from a vertex u to a vertex v in a digraph is the length of a shortest
directed path from u to v.
A digraph is strongly connected (or diconnected, or strong) if every vertex is reach-
able from every other vertex.
A digraph is unilaterally connected (or unilateral) if for every pair of vertices u
and v, there is either a uv-path or a vu-path.
A digraph is weakly connected (or weak) if the underlying graph is connected.
The digraph is disconnected if the underlying graph is disconnected.
A strong component of a digraph is a maximal subgraph that is strongly connected.
A digraph is reducible if the vertex set can be partitioned into a disjoint union V1 ∪V2
so that all arcs joining V1 and V2 go from V1 to V2.
The condensation D∗of a digraph D is the simple digraph whose vertices are the
strong components {V1, V2, . . . , Vk} of D, with an arc ViVj ∈ED∗if and only if there is
an arc vv′ in D such that v ∈Vi and v′ ∈Vj.
The converse of a digraph D is obtained by reversing the directions of all the arcs
of D.
The directional dual of a theorem about digraphs is the statement obtained by replac-
ing each property in the theorem statement by its converse.
Facts:
1. Using a pencil on a drawing of a digraph, a directed walk can be traversed by following
the arrows without lifting the pencil from the graph.
2. Distance in digraphs need not be symmetric. That is, the distance from u to v might
be diﬀerent from the distance from v to u.
3. If A is the adjacency matrix of D, then the ij entry of An is the number of n-arc
walks from vi to vj.
4. Let δ+ be the smallest out-degree of a simple digraph D. If δ+ > 0, then D has a
cycle of length at least δ+ + 1.
5. Let δ−be the smallest in-degree of a simple digraph D. If δ−> 0, then D has a
cycle of length at least δ−+ 1.
6. The directional dual of a theorem about digraphs is a theorem about digraphs.
7. Fact 5 is the directional dual of Fact 4.
8. A digraph D is Eulerian (see §8.4.3) if and only if the underlying graph is connected
and in-degree equals out-degree at every vertex.
9. A digraph D has an Euler uv-trail (where u ̸= v) if the following conditions hold:

606
Chapter 8
GRAPH THEORY
• the out-degree of vertex u exceeds the in-degree by one; that is, d+(u) = d−(u)+1;
• the in-degree of v exceeds the out-degree by one; that is, d−(v) = d+(v) + 1;
• at every other vertex, the in-degree equals the out-degree; that is, d−(w) = d+(w)
for all w ̸= u, v.
Other Euler-type results for graphs generalize to digraphs as well.
10. Let δ be the minimum of all in- and out-degrees of simple digraph D. If δ ≥|V |/2 >
1, then D contains a Hamilton cycle (see §8.4.4).
11. The strong components of a digraph D partition the vertices of D, but not the
arcs. However, the maximal unilateral subgraphs do partition the arcs. If V1, V2 are the
vertex sets of two strong components of D, then all arcs between V1 and V2 face the same
way—either from V1 or to V1. See the following ﬁgure.
D
D *
12. The condensation of any digraph is an acyclic digraph (see §8.3.4). See the ﬁgure
for the previous fact.
13. A digraph is reducible if and only if its condensation has at least two vertices.
14. A digraph is unilateral if and only if its condensation is a path.
15. A set V ′ is a basis of a digraph D if and only if V ′ consists of one vertex from each
strong component of D that has in-degree 0 in D∗. Thus, every basis of a digraph has
the same number of vertices.
16. The eigenvalues of a digraph D are the union (counting multiplicities) of the eigen-
values of its strong components. (See §8.10.2.)
Examples:
1. Let u, v be vertices of an n-vertex digraph D with adjacency matrix A.
If v is
reachable from u, then some uv-path has length ≤n −1. Thus, D is strong if and only
if every entry of Pn−1
k=0 Ak is positive. There are more computationally eﬃcient tests
for diconnectivity: the Floyd-Warshall algorithm (see §10.3.3) and directed depth-ﬁrst
search (see §9.2.1).
2. Let M be an arbitrary square matrix. Computation of the eigenvalues of M can
sometimes be sped up as follows. Create matrix A by replacing each nonzero entry of M
by a ‘1’, and then let D be the digraph with adjacency matrix A. The eigenvalues of M
are the union of the eigenvalues of the minors of M indexed by the strong components
of D.
If M is sparse (few nonzeros), then digraph D will usually have many small
components and this approach will be eﬃcient.
3. Markov models:
Let V represent a set of states and E the possible transitions of a
Markov process (see §7.7). Then walks through D represent “histories” that the process
can follow.

Section 8.3
DIRECTED GRAPHS
607
8.3.3
ORIENTATIONS
There are many natural questions concerning when the edges of an undirected graph
could be assigned directions so as to obtain a certain sort of digraph. For instance, when
can a graph be oriented to obtain a strong digraph? An application of this last question
is to determine when a set of roads could all be made one-way, while keeping all points
reachable from all others.
Deﬁnitions:
An orientation of a graph is an assignment of directions to its edges, thereby making
it a digraph.
An orientation of a graph is strong if, for each pair of vertices u, v, there is a directed
path from u to v and a directed path from v to u.
An orientation of a graph is transitive if, whenever there is an arc from u to v and an
arc from v to w, there is also an arc from u to w.
A graph that admits a transitive orientation is called a comparability graph.
A cut-edge (or bridge) of a graph is an edge whose removal would increase the number
of components (see §8.4.1).
A 2-edge-connected graph G is connected and has no cut-edge.
A generalized circuit in a graph is a closed walk (see §8.4.1) that uses each edge at
most once in each direction.
A triangular chord for a closed walk (see §8.4.1) u1, u2, . . . , uk, u1 is a proper edge that
joins two vertices exactly two apart on the walk.
Facts:
1. Let χ(G) be the chromatic number (see §8.6.1) of graph G. Then every orientation
of G has a path of length at least χ(G) −1.
2. A graph G has a strong orientation if and only if G is 2-edge-connected. (H. Robbins,
1939)
3. A graph G is a comparability graph if and only if every generalized circuit of G of
odd length > 3 has a triangular chord.
4. Algorithms 1 and 2 give ways of creating a strong orientation in a 2-edge-connected
graph.
Algorithm 1:
Naive algorithm for creating a strong orientation.
{This algorithm is good to use by hand for small graphs}
input: a 2-edge-connected graph G
output: a strong orientation of G
H := any cycle in G
direct H
while some vertex of G is not in directed subgraph H
v := a vertex not in H
ﬁnd two edge-disjoint paths from v to H

608
Chapter 8
GRAPH THEORY
{Two such paths exist because G is 2-edge-connected}
direct one path from v to H and the other from H to v
H := H with these two subgraphs added
orient any remaining edges arbitrarily
Algorithm 2:
Better algorithm for creating a strong orientation.
{A good algorithm for large graphs or for computer implementation}
input: a 2-edge-connected graph
output: a strong orientation
select an arbitrary vertex as root
construct the depth-ﬁrst search spanning tree from that root {See §9.2.1.}
orient the tree edges downward from the root
orient all back edges upward toward the root
orient all cross edges arbitrarily
Examples:
1. In the ﬁgure below the digraph D is a weak transitive orientation of the graph G and
D′ is a strong nontransitive orientation.
G
D
D’
2. The following graph is not transitively orientable, and x, u, v, y, v, w, z, w, u, x are the
vertices of a generalized circuit without a triangular chord.
x
u
v
w
y
z
3. Traﬃc control: The ﬂow of traﬃc on crowded city streets can sometimes be improved
by making streets one-way. When this is done, it is necessary that a car can travel legally
between any two locations. Assigning directions to the edges of the graph representing
the street grid is an orientation of this graph, and cars can travel legally between any
two points if and only this graph has a strong orientation. Consequently, by Robbins’
theorem (Fact 2), to make all the streets one-way without losing mutual accessibility of
locations, it is necessary and suﬃcient that the grid of streets be 2-edge-connected.

Section 8.3
DIRECTED GRAPHS
609
8.3.4
DIRECTED ACYCLIC GRAPHS
Deﬁnitions:
A digraph is acyclic if it has no directed cycles. A directed acyclic graph is sometimes
called a DAG.
A source of a digraph is a vertex of in-degree zero.
A sink of a digraph is a vertex of out-degree zero.
A linear extension ordering of the n vertices of a digraph is a consecutive labeling
v1, v2, . . . , vn so that, if there is an arc from vi to vj, then i < j. (See also §11.2.5.)
A topological sort, or topsort, is an algorithm that assigns a linear extension ordering
to a DAG.
Facts:
1. Every DAG has at least one source, and by duality, at least one sink.
2. Every DAG has a unique basis (see §8.3.2), namely, the set of all its sources.
3. Topsort yields a linear ordering for the vertices that makes the adjacency matrix of
a DAG upper-triangular.
4. Doing a preliminary topsort permits some optimization problems about paths to be
solved subsequently by a single algorithmic pass through the vertices in the topsort order;
see, for example, §16.4.1 (critical paths).
5. Algorithm 3 provides a simple algorithm for topological sort.
Algorithm 3:
Naive topological sort.
{Construct a linear extension ordering for a DAG}
input: a DAG D
output: a numbering of the vertices in a topsort order
H := D; k := 1
while VH ̸= ∅
vk := a vertex of H of in-degree 0 {This exists; see Fact 1}
H := H −vk {Remaining graph is still a DAG}
k := k + 1
Examples:
1. In the following digraph vertex w is a source and vertex z is a sink. It is a DAG, even
though the underling graph has cycles. Labeling the vertices either in the order w, x, y, z
or w, y, x, z is a linear extension ordering.
w
x
z
y

610
Chapter 8
GRAPH THEORY
2. Consider any digraph whose vertices represent instantaneous events, and whose arcs
go from earlier events to later events. Any such digraph is acyclic. Conversely, any
digraph whose vertices represent procedural steps and whose arcs represent required
precedence can be scheduled (using a topological sort) so that arcs do in fact go forward
in time.
3. The Hasse diagram of a poset (see §11.1.1) is a DAG, as is the entire graph of a poset
(arc from u to v if and only if u ≥v).
8.3.5
TOURNAMENTS
Deﬁnitions:
A tournament is a digraph with exactly one arc between each pair of distinct vertices.
An n-tournament has n vertices.
The score vector of a tournament is the sequence of out-degrees of the vertices (number
of arcs leaving each vertex), usually in ascending order.
A tournament is regular if every vertex has the same out-degree.
A tournament is strong if there is a directed path between each pair of vertices in both
directions.
A tournament is transitive if, whenever there is an arc from u to v and from v to w,
there is also an arc from u to w.
A tournament is irreducible if there is no bipartition V1, V2 of the vertices such that all
arcs between V1 and V2 go from V1 to V2.
Vertex u of a tournament dominates vertex v if there is an arc from u to v.
A transmitter in a digraph is a vertex that has an arc to every other vertex.
A king in a digraph is a vertex from which there is a path of length 1 or 2 to all other
vertices.
A single-elimination competition is a contest from which a competitor is eliminated
after the ﬁrst loss.
Facts:
1. Every tournament has a Hamilton path (see §8.4.4), in fact an odd number of them.
2. The following statements are equivalent for any n-tournament T :
• T is strong;
• T is irreducible;
• T has a Hamilton cycle;
• T has cycles of all lengths 3, 4, . . ., n;
• Every vertex of T is on cycles of all lengths 3, 4, . . . , n.
3. Almost all tournaments are strong, in the sense that, as n →∞, the fraction of
labeled n-tournaments that are strong approaches 1.
4. The following are equivalent for a tournament:
• the tournament is transitive;
• the tournament contains no cycles;

Section 8.3
DIRECTED GRAPHS
611
• the tournament contains no 3-cycles;
• the tournament is a total (i.e., linear) order;
• the tournament has a unique Hamilton path.
5. Every tournament has a king.
6. The king of a tournament is unique if and only if it is a transmitter. Otherwise, there
are at least three kings.
7. In a tournament, almost every vertex is a king, for as n →∞, the fraction of n-
tournaments in which every vertex is a king approaches 1.
8. Score vector characterizations: A nondecreasing sequence S of nonnegative integers
s1, s2, . . . , sn is the score vector of an n-tournament if and only if
kP
i=1
si ≥
 k
2

,
for k = 1, 2, . . . , n−1,
and
nP
i=1
si =
 n
2

.
Or equivalently, if and only if the sequence S′ obtained by deleting any one si and
reducing the largest remaining n −si −1 terms by 1 is a score vector of an (n−1)-
tournament.
9. The second characterization of Fact 8 leads to a recursive algorithm to construct a
tournament having a speciﬁed score vector. See Example 4.
10. A nonnegative integer sequence s1 ≤s2 ≤· · · ≤sn is the score vector of a strong
n-tournament if and only if
kP
i=1
si >
 k
2

,
for k = 1, 2, . . . , n−1,
and
nP
i=1
si =
 n
2

.
11. The two tournaments in the following ﬁgure are isomorphic as unlabeled tourna-
ments, but distinct as labeled tournaments.
z
x
y
z
y
x
There are 2(
n
2) distinct labeled tournaments, because for each pair of vertices {u, v},
there are two choices for which way to direct the edge. If cn is the number of distinct
unlabeled n-tournaments, then
cn > 2(n
2)
n!
and
lim
n→∞
cn
2(n
2) /n! = 1.
See §8.9.3 and §8.9.4.
12. When a tournament is acyclic, it corresponds to a unique total ordering (Fact 4),
so the ranking is clear. However, almost all tournaments are strong (Fact 3). Moreover,
in a large tournament, almost every vertex is a king (Fact 7). These are reasons why it
is diﬃcult to give a satisfactory general method to rank tournaments.
13. Scheduling tournaments: To speed up the play of an n-tournament, games can be
scheduled in parallel. If n is even, then at most n/2 of the
 n
2

games can be played
simultaneously, so at least n −1 rounds are needed. However, if n is odd, then only
(n −1)/2 games can be played simultaneously, so at least n rounds are needed. In fact,
this minimum number of rounds can be obtained, and several methods of scheduling
tournaments, subject to various additional conditions, have been devised. See [Mo15].

612
Chapter 8
GRAPH THEORY
Examples:
1. A round-robin sports tournament in which there are no ties is a tournament in the
mathematical sense deﬁned above. However, a single-elimination competition (e.g., most
tennis tournaments) is not a tournament as deﬁned above.
2. It has been observed that in every small ﬂock of hens, every pair of hens establish a
dominance relation—the weaker of the two allows the stronger to peck her. Thus, this
pecking order is a tournament.
3. In a “paired comparison experiment”, a subject is asked to state a preference in each
pair chosen from n items. This amounts to a tournament, where there is an arc ij if
item i is preferred to item j.
4. Is there a tournament on vertices (a, b, c, d, e) with respective scores (1, 2, 2, 2, 3)?
Deleting e according to the second part of Fact 8 leaves vertices (a, b, c, d) with scores
(1, 2, 2, 1). Next deleting d leaves (a, b, c) with scores (1, 1, 1). The obvious tournament
with such a score vector is a 3-cycle. Next reinsert vertex d, making it dominate vertex a
only. Then reinsert vertex e, making it dominate a, b, c. This 5-tournament has the
speciﬁed score vector (1, 2, 2, 2, 3).
5. Ranking real tournaments: Ranking teams by their order along a Hamilton path (see
Fact 1) is rarely satisfactory, because that order is unique only for transitive tournaments
(Fact 4); in most cases, there are a great many Hamilton paths. Ranking by score vector
usually creates ties, and a team with few wins may deserve a better rank if those teams
it beats have many wins.
So one may consider the second-order score vector, where
each team’s score is the sum of the out-degrees of the teams it beats.
This can be
continued to nth-order score vectors. There is a limit ranking obtained this way (often
quite satisfactory), related to the eigenvalues of the digraph. See [Mo15] for more detail
and references.
8.4
DISTANCE, CONNECTIVITY, TRAVERSABILITY, &
MATCHINGS
Graphs often serve as models for transportation and communication network problems.
Movement from one node to another in the network corresponds to the graph-theoretic
notion of a walk, while the connectivity of a graph is a measure of resistance to a com-
munications cutoﬀ.
8.4.1
WALKS, DISTANCE, AND CYCLE RANK
Deﬁnitions:
A walk in a graph is an alternating sequence v0, e1, v1, . . . , er, vr of vertices and edges in
which each edge ei joins vertices vi−1 and vi. Such a walk is also called a v0, vr-walk.
The length of a walk is the number of occurrences of edges in it. An edge that occurs
more than once is counted each time it occurs.
A trail is a walk in which all of the edges are diﬀerent.
A walk or trail is open if its ﬁnal vertex is diﬀerent from its initial vertex.

Section 8.4
DISTANCE, CONNECTIVITY, TRAVERS., & MATCHINGS
613
A walk or trail is closed if its ﬁnal vertex is the same as its initial vertex.
A path is a trail in which all the vertices are diﬀerent. A path from v0 to vr is called a
v0, vr-path. (The word “path” also refers to a type of graph; see §8.1.3.)
A cycle is a closed trail of positive length in which all the vertices are diﬀerent except for
the initial and ﬁnal vertex. (The word “cycle” also refers to a type of graph; see §8.1.3.)
A graph is connected if each pair of vertices is joined by a path.
A component of a graph is a maximal connected subgraph of the graph.
The vertex v is reachable from vertex u in a graph if there is a u, v-path in the graph.
An isolated vertex of a graph is a vertex with no incident edges.
The distance d(v, w) between two vertices v and w of a graph is the length of a shortest
path between them, with d(v, v) = 0, and d(v, w) = ∞if there is no path between v
and w.
The diameter of a connected graph is the maximum distance between two of its vertices.
The eccentricity of a vertex v of a connected graph is the greatest distance from v to
another vertex.
The radius of a connected graph is the minimum eccentricity among all the vertices of
the graph.
The center of a connected graph is the set of vertices of minimum eccentricity.
The cycle rank (or ﬁrst Betti number), denoted by β1(G), of a connected graph
G = (V, E) is |E| −|V | + 1.
Facts:
1. Alternative terminology: Sometimes “circuit” is used to mean what is here called a
closed trail.
2. In a simple graph, a walk may be represented as a string of vertices v0v1 . . . vr, without
mentioning the edges.
3. The distance function on the vertex set of any connected graph G is a metric; i.e.,
the following rules hold for all vertices u, v, and w in G:
• d(v, w) ≥0, with equality if and only if v = w;
• d(w, v) = d(v, w);
• d(u, w) ≤d(u, v) + d(v, w), with equality if and only if v is on a shortest path
from u to w.
4. There are polynomial-time algorithms for ﬁnding a shortest path between vertices.
(See §10.3.2.)
5. A graph is connected if and only if it has a spanning tree.
6. A graph is disconnected if and only if there is a partition of its vertex set into
nonempty sets A and B so that no edge has one end in A and the other in B.
7. The relation “is reachable from” is an equivalence relation on the vertex set. The
equivalence classes of this relation induce the components.
8. A graph is connected if every vertex is reachable from every other vertex.

614
Chapter 8
GRAPH THEORY
9. In a simple graph, the minimum length of a cycle is at least 3. In a general graph,
a self-loop is a 1-cycle, and a 2-cycle is formed by a pair of vertices joined by a pair of
parallel edges.
10. The cycle rank β1(G) of a connected graph G equals the number of edges in the
complement of a spanning tree for G.
11. The cycle rank β1(G) of a connected graph G is equal to the rank of a vector space
over Z2 whose domain is the set of cycles of G.
12. The cycle rank of a connected planar graph G equals the number of regions in a
plane drawing of G, minus the exterior region.
13. The following table gives the cycle rank of some families of graphs.
graph
cycle rank
bouquet Bn
n
dipole Dn
n −1
complete graph Kn
(n−2)(n−1)
2
complete bipartite graph Km,n
(m −1)(n −1)
cycle graph Cn
1
wheel Wn
n
hypercube Qn
(n −2)2n + 1
any tree
0
Example:
1. The following connected graph has diameter 3 and radius 2. The vertices in its center
are indicated by solid dots.
2. The cycle rank of the following connected graph is 3. Observe that there are three
edges in the complement of the indicated spanning tree.
3. The following disconnected graph has three components, one of which is an isolated
vertex.

Section 8.4
DISTANCE, CONNECTIVITY, TRAVERS., & MATCHINGS
615
8.4.2
CONNECTIVITY
Deﬁnitions:
A cut-vertex of a graph G (or cut-point or articulation point) is a vertex v such
that G −v has more components than G. (In topological analysis of nonsimple graphs,
sometimes a vertex attached to a self-loop is also considered to be a cut-vertex.)
A nonseparable graph is a connected graph with no cut-vertices.
A block of a graph is a maximal nonseparable subgraph.
An cut-edge of a graph G is an edge e such that G −e has more components than G
(in which case there is just one more).
A disconnecting set of vertices (or cutset) in a connected graph is a set of vertices
whose removal yields a disconnected graph.
A disconnecting set of edges in a connected graph is a set of edges whose removal
yields a disconnected graph.
The zeroth Betti number β0(G) of a graph G is the number of components in G.
Elsewhere this is sometimes denoted by c(G) or ω(G).
The (vertex) connectivity κ(G) is the number of vertices in a smallest disconnecting
set of vertices. By convention, κ(Kn) = n −1.
The edge connectivity κ′(G) is the number of edges in a smallest disconnecting set of
edges.
A graph is k-connected if κ(G) ≥k.
A graph is k-edge-connected if κ′(G) ≥k.
Facts:
1. A vertex is a cut-vertex if and only if it lies on all paths between two other vertices.
2. Every nontrivial graph has at least two vertices that are not cut-vertices.
3. An edge is a cut-edge if and only if it is not contained in any cycle.
4. For any edge e of a graph G, β0(G) + 1 ≥β0(G −e) ≥β0(G).
5. For any vertex v of a graph G, β0(G −v) ≥β0(G); however, β0(G −v) may be
arbitrarily greater than β0(G).
6. Let G be a 2-connected graph. Then for any two vertices, there is a cycle containing
those vertices.
7. Let G be a 2-connected graph. Then for any two edges, there is a cycle containing
those edges.
8. The following statements are equivalent for a connected graph G with at least three
vertices:
• G is nonseparable;
• every pair of vertices lies on a cycle;
• every pair of edges lies on a cycle;
• given any three vertices u, v, and w, there is a path from u to w containing v;
• given any three vertices u, v, and w, there is a path from u to w not containing v.

616
Chapter 8
GRAPH THEORY
9. Menger’s theorem (for vertex connectivity):
A graph with at least k + 1 vertices is
k-connected if and only if every pair of vertices is joined by k paths which are internally
disjoint (i.e., disjoint except for their origin and terminus). (Menger, 1927)
10. Menger’s theorem (for edge connectivity): A graph is k-edge-connected if and only
if every pair of vertices is joined by k edge-disjoint paths. (Ford and Fulkerson, 1956;
also Elias, Feinstein, and Shannon, 1956)
11. For any graph G, the vertex connectivity is no more than the edge connectivity, and
the edge connectivity is no more than the minimum degree. That is, κ(G) ≤κ′(G) ≤
δ(G), where δ(G) denotes the minimum degree.
12. Furthermore, for any positive integers a ≥b ≥c, there exists a simple graph G for
which δ(G) = a, κ′(G) = b, and κ(G) = c. (Chartrand and Harary, 1968)
13. The following table gives the vertex connectivity and edge connectivity of some
families of graphs.
graph
κ
κ′
complete graph Kn
n −1
n −1
complete bipartite graph Km,n
min(m, n)
min(m, n)
cycle graph Cn
2
2
wheel Wn
3
3
hypercube Qn
n
n
Examples:
1. The following graph G has cut-vertices u and v. The blocks are illustrated at the
right.
u
v
G
2. In the following graph, vertices u and v form a disconnecting set.
u
v
3. Communication networks:
A communication network can be modeled as a graph
with vertices representing the nodes and with undirected edges representing direct two-
way communications links between nodes. In order that all pairs of nodes be in commu-
nication, the graph must be connected. Vertex connectivity and edge connectivity are
measures of network reliability.
4. Transportation networks:
Low connectivity in transportation networks results in
“bottlenecks”, in which many diﬀerent shipments must all pass through a small number
of nodes. High connectivity implies (by Menger’s theorem) several alternative routes
between nodes.

Section 8.4
DISTANCE, CONNECTIVITY, TRAVERS., & MATCHINGS
617
5. Menger’s theorem implies that a 2-connected graph has two disjoint paths between
each pair of vertices. It does not imply that for any path between two vertices, there
must be a second such path disjoint from the ﬁrst, as indicated in the following graph.
There are two disjoint paths from the leftmost vertex to the rightmost, but there is no
such path disjoint from the one indicated by thick edges.
6. The following shows a graph G with κ(G) = 2 and κ′(G) = 3. On the left there are
two internally-disjoint paths between the upper-left vertex and the lower-right vertex,
and on the right there are three edge-disjoint paths.
7. The following graph illustrates Fact 12, with κ = 2, κ′ = 3, δ = 4.
8.4.3
EULER TRAILS AND TOURS
Deﬁnitions:
An Euler trail in a graph [digraph] is a trail that contains all the edges [arcs] of the
graph.
An Euler tour or Euler circuit in a graph or digraph is a closed Euler trail.
A graph or digraph is Eulerian if it has an Euler tour.
Facts:
1. Seven bridges of K¨onigsberg problem:
In Kaliningrad, Russia, two branches of the
River Pregel meet and ﬂow past an island into the Baltic Sea. In 1736, when this was
the town of K¨onigsberg in East Prussia, there were seven bridges joining the banks of
the river, the headland, and the island, as illustrated at the left in the following picture.
The celebrated Swiss mathematician Leonhard Euler (1707–1783) was asked whether it
was possible to cross all seven bridges without recrossing any of them. In the earliest
known paper on graph theory, Euler proved it is impossible, because the graph at the
right has no Euler trail.

618
Chapter 8
GRAPH THEORY
B
A
C
D
A
C
B
D
2. Euler’s work on the seven bridges of K¨oningsberg problem is commonly described as
the founding of graph theory and also as the founding of topology.
3. A connected graph is Eulerian if and only if every vertex has even degree. The tour
may begin/end at any vertex.
4. A connected digraph has a directed Euler tour if and only if the in-degree of every
vertex v equals its out-degree.
5. A connected graph has an Euler trail between distinct vertices u and v if and only if
u and v are the only vertices of odd degree.
6. A connected graph (digraph) is Eulerian if and only if there exists a collection of
cycles (directed cycles) whose edges partition the edge set of the graph.
7. A connected planar (see §8.7.1) graph is Eulerian if and only if its dual (see §8.8.2)
is bipartite.
8. A graph G can be oriented to have a directed Euler tour if and only if it is an Eulerian
graph. (Traversing an Euler tour provides an orientation.)
9. The following table indicates which members of several families of graphs are Eulerian.
graph
Eulerian?
bouquet Bn
for all n
dipole Dn
for even n
complete graph Kn
for odd n
complete bipartite graph Km,n
for m and n both even
cycle graph Cn
for all n
wheel Wn
never
hypercube Qn
for even n
tree
only if trivial
10. Algorithm 1 gives a recursive method for ﬁnding an Eulerian tour on an Eulerian
graph.
Algorithm 1:
Recursive algorithm for ﬁnding an Eulerian tour.
input: a connected graph G, all of whose vertices have even degree
output: an Euler tour of G
C := a cycle in the graph G; place C on the cycle-queue Q
partition the edge-complement G −E(C) into components H1, H2, . . . , Hk
recursively run this algorithm on each component Hi
{So far, EG has been completely partitioned into the cycles on Q}
merge the elements of Q into an Euler tour for G, by traversing the cycle C
and splicing in the tours found for the components Hi whenever possible

Section 8.4
DISTANCE, CONNECTIVITY, TRAVERS., & MATCHINGS
619
11. Fleury’s algorithm for ﬁnding an Euler tour or trail is given in Algorithm 2.
Algorithm 2:
Fleury’s algorithm for ﬁnding an Euler tour/trail.
input: a connected graph G, an initial vertex v, and a ﬁnal vertex w; if v ̸= w,
then every vertex except v and w must have even degree (if v = w, then all
degrees must be even)
output: an Euler trail whose origin is v and whose terminus is w
{Find trail edge with origin v}
if deg(v) > 1 then e := any edge incident at v which is not a cut-edge
else { deg(v) = 1 }
e := the unique edge incident at v
u := the other endpoint of e
recursively ﬁnd an Euler trail from u to w in G −e
prepend the edge e to the trail found in the recursive step
{This yields the required Euler trail of G}
Examples:
1. The following is an Eulerian graph and one of its Euler tours.
>
2. Chinese postman problem (due to Guan Meigu, 1962): A letter carrier begins at the
post oﬃce, traverses every street in his territory at least once, and then returns to the
post oﬃce. His objective is to walk as little as possible. Each edge of a graph representing
the street conﬁguration is labeled with the length of the corresponding block. If the graph
is Eulerian, then an Euler tour gives an optimal solution. Otherwise, some edges must
be retraced. Polynomial-time algorithms to solve this problem are known. See §10.2.3.
3. For every letter in an arbitrary n-letter alphabet A, there is a string starting and
ending with that letter, in which every possible substring of two letters appears consecu-
tively exactly once. To see this, consider the digraph D with the letters of A as vertices
and one arc for each ordered pair. (This is called a de Bruijn digraph.) The digraph D
is connected and, at each of the n vertices, in-degree = out-degree = n, which implies
that it is Eulerian. Thus, the sequence of vertices encountered on a closed Euler tour
from any vertex to itself yields the speciﬁed string.
In the following ﬁgure, e1, e2, . . . , e9 are the arcs of an Euler cycle and the associated
string is the sequence of vertices, aabbccacba. This result generalizes to substrings of any
ﬁxed length, also using Euler tours.

620
Chapter 8
GRAPH THEORY
e1
e2
e3
e4
e5
e7
e8
e9
e6
b
a
c
8.4.4
HAMILTON CYCLES AND PATHS
Deﬁnitions:
A Hamilton cycle in a graph [digraph] is a cycle [directed cycle] that includes all
vertices of the graph.
A graph or digraph is Hamiltonian if it contains a Hamilton cycle.
A Hamilton path in a graph [digraph] is a path [directed path] that includes all vertices
of the graph.
A theta graph is a subdivision of the complete bipartite graph K2,3. Thus, the graph
comprises three internally disjoint paths joining the two degree-3 vertices.
A Gray code is a cyclic ordering of all 2k length-k bitstrings, such that each bitstring
diﬀers from the next in exactly one bit entry.
A 1-tough (or simply tough) graph is a connected graph G such that no matter what
nonempty, proper vertex subset S is removed, the resulting number of components of
G −S is no more than |S|.
Facts:
1. The concept of a Hamilton cycle ﬁrst arose in a puzzle within the Icosian Game,
invented by Sir William Rowan Hamilton (1805–1865), an Irish mathematician. This
puzzle involved a dodecahedron whose 20 vertices were labeled with world capitals. It
required ﬁnding a complete tour of these 20 capitals.
2. The recognition problem for Hamiltonian graphs is NP-complete. Thus, unlike the
case with Eulerian graphs, there is no easy test to decide whether a graph is Hamiltonian
(unless P = NP). However, many of the following facts provide criteria that are often
helpful in deciding.
3. A Hamiltonian graph has no cut-point. (Thus, it is 2-connected.)
4. The previous fact has this generalization: Let G be a Hamiltonian graph and let
S ⊆VG. Then the graph G −S has at most |S| components. That is, Hamiltonian
graphs are tough.
5. Bipartite Hamiltonian graphs have an equal number of vertices in the two parts of
the bipartition.
6. If a simple graph has n ≥3 vertices and minimum degree at least n/2, then it is
Hamiltonian. (Dirac, 1952)
7. If a simple graph has n ≥3 vertices, and if every pair of nonadjacent vertices u and v
satisﬁes the inequality deg(u) + deg(v) ≥n, then it is Hamiltonian. (Ore, 1960)

Section 8.4
DISTANCE, CONNECTIVITY, TRAVERS., & MATCHINGS
621
8. Suppose that a simple graph with n ≥3 vertices has degree sequence d1 ≤d2 ≤· · · ≤
dn, and that for every i with 1 ≤i ≤n/2 either di > i or dn−i ≥n −i. Then the graph
is Hamiltonian.
9. Every simple graph with n ≥3 vertices and at least (n2 −3n + 6)/2 edges is Hamil-
tonian.
10. Every graph with at least three vertices whose connectivity (κ) is at least as large
as its independence number (α) is a Hamiltonian graph.
11. Every 4-connected planar graph is Hamiltonian.
12. If the edges of the complete graph Kn are assigned directions, then the resulting
digraph always has a Hamilton directed path.
13. The edges of the complete graph K2n+1 can be partitioned into n Hamilton cycles.
14. Theta graphs are non-Hamiltonian and every non-Hamiltonian graph contains a
theta subgraph.
15. A graph G is non-Hamiltonian if there is an independent set containing more than
half the vertices of G.
16. “Almost all” graphs are Hamiltonian. That is, of the exactly 2n(n−1)/2 simple graphs
on n (labeled) vertices, the proportion that are Hamiltonian tends to 1 as n →∞.
17. Suppose that a simple graph is constructed by the following process: start with n
vertices and no edges; until the minimum degree is 2, a possible edge is chosen uniformly
at random from among the edges not already in the graph, and added to the graph.
With probability tending to 1 as n →∞, the resulting graph is Hamiltonian.
18. The following table indicates which members of several families of graphs are Hamil-
tonian.
graph
Hamiltonian?
bouquet Bn
for all n ≥1
dipole Dn
for all n ≥2
complete graph Kn
for all n ≥3
complete bipartite graph Km,n
when m = n
cycle graph Cn
for all n ≥1
wheel Wn
for all n ≥2
hypercube Qn
for all n ≥2
Examples:
1. Finding a Hamilton cycle in the dodecahedral graph (see §8.1.3), as illustrated below,
is equivalent to solving Hamilton’s Icosian Game puzzle. An example of a Hamilton cycle
in this graph is: RST V WXHJKLMNPCDFGBZQR.
R
W
S 
T
V
X
H
G
Z
Q
C
B
P
N
M
D
F
K
L
J

622
Chapter 8
GRAPH THEORY
2. The following graph has the Hamilton cycle acefdba.
a
c
d
b
f
e
3. The following graph is non-Hamiltonian, by Fact 15, since the vertices u, v, w, and x
form an independent set.
r
u
v
w
x
s
t
4. The 10-cycle C10 (§8.1.3) is an example of a graph that satisﬁes none of the suﬃcient
conditions in Facts 6–10 above for Hamiltonicity, but is nonetheless Hamiltonian.
5. The traveling salesman problem (see §10.7.1) is to ﬁnd a minimum-cost Hamilton
cycle in a complete graph whose edges are labeled with costs.
6. Information theory — Gray codes: In information theory, a cyclic ordering of the 2n
length-n bitstrings such that each bitstring diﬀers from its predecessor in exactly one
bit is called a Gray code. This corresponds to a Hamilton cycle in the n-dimensional
hypercube Qn.
The following ﬁgure shows a Hamilton cycle in the 3-cube giving the Gray code 000 →
001 →011 →111 →101 →100 →110 →010 →000.
010
011
110
100
111
101
000
001
7. The Petersen graph (see §8.1.3) is tough but not Hamiltonian.
8. The following graph is tough but not Hamiltonian.

Section 8.4
DISTANCE, CONNECTIVITY, TRAVERS., & MATCHINGS
623
8.4.5
MATCHINGS AND FACTORS
Deﬁnitions:
A matching in a graph is a set of disjoint edges.
A perfect matching (or 1-factor) is a matching which covers all the vertices of the
graph.
A graph H is a factor of a graph G if H is a spanning subgraph of G.
A factor which is k-regular is called a k-factor.
If f is a function on V (G), then a spanning subgraph H is called an f-factor if degH(v) =
f(v), for all v ∈V (G). Of course, if f(v) = k for every vertex, then f is just a k-factor.
An [a, b]-factor of a graph G is an f-factor such that a ≤f(v) ≤b, for all v ∈V (G).
If f and g are functions on V (G), then a (g, f)-factor is a spanning subgraph H such
that g(v) ≤degH(v) ≤f(v), for all v ∈V (G).
A path factor is a spanning subgraph where each component is a path.
An F -factor is a spanning subgraph where each component is either a K2 or an odd
cycle.
A 1-factorization of a graph is a partition of the edges into edge-disjoint 1-factors.
A graph has the odd-cycle property if every pair of odd cycles either has a common
vertex or is joined by an edge.
A graph G is k-tough if c(G −S) ≤|S|/k for all cutsets S ⊆V (G), where c(G −S)
denotes the number of components of G −S.
Facts:
1. Petersen’s theorem: A 3-regular graph with at most one cut-edge contains a 1-factor
and hence also a 2-factor. (J. Petersen, 1891)
2. If ν(G) is the size of a largest matching in G and τ(G) is the minimum number of
vertices necessary to cover all the edges of G, then ν(G) ≤τ(G). K¨onig’s theorem says
that if G is bipartite, ν(G) = τ(G). (D. K¨onig, 1931)
3. Hall’s theorem: Let G be a bipartite graph with vertex partition V (G) = A ∪B.
Then G has a matching of A onto B if and only if (i) |A| = |B|, and (ii) |N(X)| ≥|X| for
all X ⊆A, where N(X) denotes the neighborhood of set X. (In fact, Hall’s and K¨onig’s
theorems are equivalent.) (P. Hall, 1935)
4. Tutte’s 1-factor theorem: A graph G has a 1-factor if and only if for each S ⊆V (G),
it holds that co(G −S) ≤|S|, where co(G −S) denotes the number of odd components
of G −S. (W. Tutte, 1947)
5. If a graph of even order is r-regular and has the odd-cycle property, then it has a
1-factor.
6. If G is k-connected and contains a 1-factor and if |V (G)| is suﬃciently large, then G
has at least k! 1-factors.
7. If G is connected and has a unique 1-factor, then G has a cut-edge belonging to this
1-factor.

624
Chapter 8
GRAPH THEORY
8. If G is an r-regular (r −1)-edge-connected graph of even order, then G has a 1-factor
not containing any r −1 prescribed edges. (And hence G has a 1-factor containing any
prescribed edge.)
9. If G is a connected graph of even order whose automorphism group acts transitively
on V (G), then G has a 1-factor containing any given edge.
10. A 3-regular graph has exponentially many 1-factors.
11. Counting the number of perfect matchings has applications in crystal physics and
organic chemistry.
12. The ﬁrst polynomial-time algorithm for matching in an arbitrary graph was formu-
lated by Edmonds [Ed65] and is now called the Blossom Algorithm. (See §10.2.3.)
13. Currently the fastest algorithm for matching in an arbitrary graph with n vertices
and m edges is O(m√n).
14. If a graph G of order n is k-tough and kn is even, then G has a k-factor.
15. Let G be a connected graph of order n and let k be an integer ≥3 such that kn is
even and n ≥4k −3. If d(u) ≥k for all vertices, and max{d(u), d(v)} ≥n/2 for all pairs
of non-adjacent vertices u and v, then G has a k-factor.
16. Let G be a graph with the odd-cycle property and with a k-factor, and let r be an
integer, 1 ≤r ≤k. Then G has an r-factor if r|V (G)| is even.
17. If G is a graph and G−v has a k-factor for all v ∈V (G), then G also has a k-factor.
18. Tutte’s f-factor theorem: Graph G has an f-factor if and only if the following two
conditions hold for all disjoint sets D, S ⊆V (G):
(i) f(D)−f(S)+dG−D(S)−qG(D, S, f) ≥0, where qG(D, S, f) denotes the number
of components C of G −(D ∪S) such that eG(V (C), S) + f(V (C)) is odd;
(ii) f(D) −f(S) + dG−D(S) −qG(D, S, f) ≡f(V (G)) (mod 2).
(Here eG(A, B) denotes the number of edges in G joining vertex sets A and B.)
19. If G is r-regular, then G has a [k, k + 1]-factor, for all k, 0 ≤k ≤r.
20. The graph G has a (g, f)-factor if and only if both the following conditions hold:
(i) eG(V (C), S) + f(V (C)) is odd;
(ii) f(D) −g(S) + degG−D(S) −ˆqG(D, S, g, f) ≥0, for all pairs of disjoint sets
D, S ⊆V (G), where ˆqG(D, S, g, f) denotes the number of components C of G −(D ∪S)
having g(v) = f(v) for all v ∈V (C).
21. If G is a simple graph, then G has a path factor if and only if i(G −S) ≤|2S|, for
all S ⊆V (G). Here i(G −S) denotes the number of isolated vertices in G −S.
22. Let FACT(H) denote the problem of ﬁnding a factor where each component is
isomorphic to H. If H = K2, then FACT(H) is just the problem of ﬁnding a perfect
matching and hence is polynomial-time. However, if H has a component with at least
three vertices, then FACT(H) becomes NP-complete.
23. A graph G has an F-factor if and only if |N(S)| ≥|S|, for every independent set
S ⊆V (G).
(This result can be viewed as a generalization of Hall’s Theorem to the
non-bipartite case.)
24. There is a polynomial-time algorithm for ﬁnding an F-factor or showing that none
exists.
25. 1-factorization conjecture: Every r-regular graph of even order n with r ≥n/2
admits a 1-factorization.
(Or stated diﬀerently, all such graphs have edge chromatic
number r.)

Section 8.5
GRAPH ISOMORPHISM AND RECONSTRUCTION
625
26. The 1-factorization conjecture is “asymptotically true” in that every r-regular graph
of even order n with r ≥( 1
2 + o(1))n has a 1-factorization.
8.5
GRAPH ISOMORPHISM AND RECONSTRUCTION
Deciding whether two graph descriptions actually specify structurally identical graphs
is called isomorphism testing. Polynomial-time algorithms for isomorphism testing are
known only for certain special classes of graphs. However, there are heuristic algorithms
to test isomorphism of reasonable-sized graphs. The related problem of reconstructing a
graph from its vertex-deleted subgraphs is also still unsettled.
8.5.1
ISOMORPHISM INVARIANTS
Deﬁnitions:
An isomorphism between two simple graphs G and H is a bijection f : VG →VH be-
tween their vertex sets that preserves adjacency: (u, v) ∈EG if and only if (f(u), f(v)) ∈
EH, for any pair of vertices u, v ∈VG.
In full generality, a graph isomorphism between two graphs G and H is a pair of
bijections fV : VG →VH and fE : EG →EH such that for every edge e ∈EG, the
endvertices of e are mapped by fV onto the endvertices of fE(e). Note: Except when
confusion will result, the same notation f can be used for both fV and fE.
A digraph isomorphism is an isomorphism that preserves all arcs.
Two graphs are isomorphic if there is an isomorphism from one to the other.
The isomorphism type of a graph (digraph) G is the class of all graphs (digraphs)
isomorphic to G.
A graph invariant is a property of graphs such that two isomorphic graphs have the
same value with regard to it.
A vertex invariant is a property of a vertex that is preserved by isomorphism: if
f : VG →VH is an isomorphism, v ∈VG and p is a property of v, then the vertex
f(v) ∈VH has the same value as v with regard to p.
An automorphism is an isomorphism from a graph to itself.
The set of all automorphisms of a graph G forms a group under composition called the
automorphism group Aut(G). (See §5.2.2.)
Facts:
1. Two graphs G and H are isomorphic if there is a bijection f : VG →VH such that
for every vertex pair u, v ∈VG the number of edges joining u and v equals the number
joining their images f(u), f(v) ∈VH.
2. Graph invariants are used to distinguish between nonisomorphic graphs.
3. Most graph invariants are either too hard to compute or not strong enough at dis-
tinguishing similar but nonisomorphic graphs.

626
Chapter 8
GRAPH THEORY
4. Vertex invariants are often used to partition the vertices of a graph into equivalence
classes under graph automorphism, in order to discover the automorphism group of the
graph.
5. Graphs have many isomorphism invariants, including: elementary invariants (such
as the degree sequence), structural invariants (such as the girth), topological invariants
(such as genus), chromatic invariants (such as the edge chromatic number), and algebraic
invariants (such as eigenvalues).
Examples:
1. The following ﬁgure illustrates an isomorphism f of simple graphs.
1
2
f(4)
f(2)
f(1)
f(3)
3
4
2. The following ﬁgure illustrates an isomorphism f of nonsimple graphs.
f(3)
c
e
d
4
3
b
2
a
1
f(b)
f(d)
f(e)
f(4)
f(2)
f(c)
f(a)
f(1)
3. The following two digraphs are not isomorphic. Even though there are six diﬀerent
isomorphisms of their underlying graphs, none of them preserves the direction of all the
edges.
4. The next three graph drawings all look diﬀerent. The following table shows some of
their isomorphism invariants, from which it may be concluded that graph B cannot be
isomorphic to either graph A or graph C, but that A and C might be isomorphic.
A
B
C
A
B
C
# vertices
6
6
6
# edges
9
9
9
degree seq.
3, 3, 3, 3, 3, 3
3, 3, 3, 3, 3, 3
3, 3, 3, 3, 3, 3
connectivity
3
3
3
girth
4
3
4
genus
1
0
1
chromatic #
2
3
2

Section 8.5
GRAPH ISOMORPHISM AND RECONSTRUCTION
627
To construct an isomorphism between graphs A and C, assign labels 0, 1, 2, 3, 4, 5
cyclically to the vertices of A. Then assign labels 0, 2, and 4 to the top three vertices of
C, and 1, 3, and 5 to the bottom three.
8.5.2
ISOMORPHISM TESTING
Deﬁnitions:
An isomorphism test for graphs is an algorithm that accepts two graphs as input
and outputs “yes” to indicate the decision that they are isomorphic or “no” to indicate
that they are not isomorphic. Unless the context explicitly mentions the possibility of
error, it is implicitly understood that the decision is correct.
The eigenvalues of a graph are the eigenvalues of its adjacency matrix. (See §8.10.2
and §6.5.)
The average genus of a graph is the average genus of the imbedding surface, taken over
all cellular imbeddings of that graph. (See §8.8.3.)
An equitable partition for a graph G is a partition V1, . . . , Vn of its vertex set and a set
of numbers {d i,j | 1 ≤i, j ≤n} such that every vertex in Vi is adjacent to exactly di,j
vertices in Vj.
A devil’s pair for an isomorphism-testing approach is a pair of nonisomorphic graphs
that the approach fails to distinguish.
Facts:
1. No polynomial-time isomorphism testing algorithm is known. Moreover, it is not
known whether isomorphism testing is an NP-complete problem.
2. Any two randomly selected graphs are almost always not isomorphic.
3. An algorithm that calculates the order of the automorphism group of a graph can be
used as a subprogram to test isomorphism of two graphs G and H, as follows:
if |Aut(G)| ̸= |Aut(H)| then G and H are not isomorphic
else
if |Aut(G ∪H)| = 2|Aut(G)|2 then G and H are isomorphic
else G and H are not isomorphic
Note that the converse is true. Thus the problem of graph isomorphism and the prob-
lem of computing the order of the automorphism group of a graph are computationally
equivalent.
4. Another algebraic approach to isomorphism testing is based on eigenvalues. A devil’s
pair for simply comparing eigenvalues appears in Example 4.
5. One topological approach to isomorphism testing dissects each graph into planar
components (§8.7) and combines known eﬃcient tests for isomorphism of planar graphs
with careful study of possible interconnections.
6. Another topological approach to isomorphism testing is based on the genus distribu-
tion (§8.8.4), taken over all cellular imbeddings. Although calculating the genus distri-
bution by brute force would be tedious, one can estimate it by random sampling. Any
pair of trees is a trivial devil’s pair, but trees are easily tested by another isomorphism
algorithm.

628
Chapter 8
GRAPH THEORY
7. The best known practical isomorphism algorithm is “NAUTY” (an acronym for No
AUTomorphisms, Yes?) created by B. D. McKay. This backtrack algorithm repeatedly
reﬁnes an initial vertex partition. At each stage of the reﬁnement, a cell of size greater
than 1 is broken into two parts, one a single vertex, and the coarsest equitable partition
is found. The discrete partitions generated in this way correspond to labelings of the
graph, organized so as to determine the automorphism group. An isomorphism invariant
criterion is used to pick one of these labelings as the “canonical” one used for isomorphism
testing. This algorithm can very quickly check isomorphism for most graphs.
8. To certify that two graphs are isomorphic, one can give a vertex bijection that realizes
the isomorphism. Deciding whether a bijection between the vertex sets of two graphs is
the vertex function of a graph isomorphism can be achieved in polynomial time.
9. If graph isomorphism is NP-complete, then the complexity hierarchy between P and
NP collapses, which is considered unlikely. (See §17.5.)
10. The following table shows the best known time bounds for checking isomorphism
in various classes of graphs. Almost all of these bounds have been achieved using an
algebraic approach.
class of graphs (on n vertices)
time bound
graphs
exp O((log n)c)
trees
O(n)
planar graphs
O(n)
graphs of genus g
nO(g)
cubic graphs
O(n3 log n)
graphs with max degree ≤d
nO(d)
tournaments
nO(log n)
11. Algorithm 1 gives a naive O(n2 n!) method for testing whether or not two graphs
are isomorphic.
Algorithm 1:
Naive graph isomorphism-testing algorithm.
input: simple graphs G, H
if |VG| ̸= |VH| or |EG| ̸= |EH| then print “NO” and stop
for each bijection f : VG →VH
for each pair u, v ∈VG
if u, v adjacent and f(u), f(v) not adjacent then print “NO” and stop
if u, v not adjacent and f(u), f(v) adjacent then print “NO” and stop
print “YES” and stop
Examples:
1. The labeling of the following two isomorphic graphs indicates the correspondence
between vertices. This is the Petersen graph.

Section 8.5
GRAPH ISOMORPHISM AND RECONSTRUCTION
629
0
0
4
3
2
7
5
9
1
6
8
4
3
8
7
6
5
9
2
1
2. One devil’s pair for isomorphism testing by degree sequence is
G
H
3. Another devil’s pair for isomorphism testing by degree sequence is
G
H
4. The following graphs are a devil’s pair for isomorphism testing by simple comparison
of eigenvalues. They both have characteristic polynomial λ6 −7λ4 −4λ3 + 7λ2 + 4λ −1.
Yet they cannot be isomorphic because their degree sequences are diﬀerent.
5. A 3-connected devil’s pair for simply comparing average genus is shown below. Both
graphs shown have 8 imbeddings of genus 0, 536 of genus 1, 3416 of genus 2, and 1224
of genus 3.
8.5.3
GRAPH RECONSTRUCTION
The question of whether a graph is reconstructible from its subgraphs is one of the most
beguiling unsolved problems in graph theory.
Deﬁnitions:
A vertex-deleted subgraph of a graph G is a subgraph G −v obtained by removing
a single vertex v and all of its incident edges.
An edge-deleted subgraph of a graph G is a subgraph G −e obtained by removing a
single edge e.
The vertex-deleted subgraph collection of a graph G is the multi-set of all vertex-
deleted subgraphs G −v. The number of times a graph appears in the collection equals

630
Chapter 8
GRAPH THEORY
the number of diﬀerent vertices whose removal yields that graph. Thus, the cardinality
of the collection equals the number of vertices of G.
The edge-deleted subgraph collection of a graph G is the multi-set of all edge-
deleted subgraphs G −e. The number of times a graph appears in the collection equals
the number of diﬀerent edges whose removal yields that graph. Thus, the cardinality of
the collection equals the number of edges of G.
A reconstructible graph is a graph G such that no other graph has the same vertex-
deleted subgraph collection as G.
An edge-reconstructible graph is a graph G such that no other graph has the same
edge-deleted subgraph collection as G.
A reconstructible invariant is a graph invariant such that all graphs with the same
vertex-deleted subgraph collection have the same value with respect to this invariant.
An edge-reconstructible invariant is a graph invariant such that all graphs with the
same edge-deleted subgraph collection have the same value with respect to this invariant.
Conjectures:
1. The graph reconstruction conjecture (P. Kelly and S. Ulam, 1941):
Every graph
with more than two vertices is reconstructible.
2. The edge reconstruction conjecture:
Every graph with at least four edges is edge
reconstructible.
3. Halin’s conjecture: If two (possibly inﬁnite) graphs with more than two vertices have
the same vertex-deleted subgraph collection, then each graph is a subgraph of the other.
4. Ramachandran’s conjecture:
Every digraph D is reconstructible from the vertex-
deleted subdigraphs D −v together with the additional information consisting of the in-
and out-degree of v.
Facts:
1. The graph reconstruction conjecture implies the edge reconstruction conjecture, and
both are implied by Halin’s conjecture.
2. The graph reconstruction conjecture does not hold for graphs on two vertices, because
K2 and K2 have identical sets of deleted subgraphs.
3. The edge reconstruction conjecture does not hold for graphs on four edges, because
K3 + K1 (disjoint union) and K1,3 have identical collections of edge-deleted subgraphs.
4. Computer search has veriﬁed the reconstruction conjecture for graphs with eleven or
fewer vertices.
5. The following table lists some invariants and types of graphs which are known to be
reconstructible.
both edge-reconstructible and reconstructible
other edge-reconstructible
invariants
graphs
graphs
number of vertices
regular
more edges than non-edges
number of edges
disconnected
only two vertex degrees
degree sequence
trees
no induced K1,3 subgraph
connectivity
outerplanar
large with Hamilton path
characteristic polynomial
cacti
2 log2(2 max deg) ≤avg deg
maximal planar

Section 8.6
COLORINGS, LABELINGS, & RELATED PARAMETERS
631
6. If graph F has fewer vertices than graph G, then the number of subgraphs of G
isomorphic to F is reconstructible.
7. The reconstruction conjecture is not true for directed graphs in general, because
nonreconstructible tournaments of arbitrarily large size are known, but Ramachandran’s
conjecture holds for all known nonreconstructible tournaments.
8. Inﬁnite graphs are not reconstructible in general, but Halin’s conjecture holds for all
known nonreconstructible inﬁnite pairs.
9. Almost every graph is uniquely determined by any three vertex-deleted subgraphs.
Example:
1. The following ﬁgure shows a graph (at the left) and its collection of vertex-deleted
subgraphs.
8.6
GRAPH COLORINGS, LABELINGS, & RELATED
PARAMETERS
The vertex set of a graph can be colored so that adjacent vertices are colored diﬀerently.
Similarly, the edges of a graph without self-loops can be colored so that adjacent edges are
colored diﬀerently. If a graph is imbedded in a surface so that there are no self-adjacent
regions, then the regions can be colored so that adjacent regions receive diﬀerent colors.
These entertaining concepts have many important applications, including assignment
and scheduling problems. We consider these and related questions here.
8.6.1
VERTEX COLORINGS
Deﬁnitions:
A (proper) vertex k-coloring (or k-coloring) of a simple graph G is a function
f : VG →{1, . . ., k} such that adjacent vertices are assigned diﬀerent numbers. Often,
the set {1, . . . , k} is regarded as a set of colors.
A coloring of a graph is a k-coloring for some integer k.
An improper coloring of a graph permits two adjacent vertices to be colored the same.
A graph is k-colorable (or k-vertex colorable) if it has a k-coloring.
The chromatic number or (vertex chromatic number) χ(G) of a graph G is the
minimum number k such that G is k-colorable; that is, χ(G) is the smallest number of
colors needed to color the vertices of G so that no adjacent vertices have the same color.
A graph G is k-chromatic if χ(G) = k.

632
Chapter 8
GRAPH THEORY
A graph G is chromatically k-critical if G is k-chromatic and if χ(G −e) = k −1 for
each edge e of G.
An elementary contraction of a simple graph G on the edge e, denoted G · e, is
obtained by replacing the edge e and its two endpoints by one vertex adjacent to all the
other vertices to which the endpoints were adjacent.
The chromatic polynomial of the graph G is the function πG(t) whose value at the
integer t is the number of diﬀerent functions VG →{1, . . ., t} that are proper colorings
of G.
A list assignment L of a graph is an assignment of a set Lv of allowable colors at each
vertex v.
A graph is k-choosable if, for every list assignment L with |Lv| ≥k for all v, it is
possible to choose for each vertex v an element of Lv such that the result is a proper
coloring.
The choice number ch(G) is the minimum k such that graph G is k-choosable.
Facts:
1. A direct way to calculate the chromatic number of a reasonably small graph is in two
steps. First derive an upper bound for the number of colors needed, either by ﬁnding a
coloring by trial and error or by using the greedy coloring algorithm. Then prove that
one fewer colors would be insuﬃcient. This could be achieved by an exponential-time
exhaustion algorithm, or by ﬁnding an insightful proof for the particular graph.
2. χ(G) = 2 if and only if the graph is bipartite and its edgeset is nonempty.
3. The odd cycles are the 3-critical graphs.
4. The odd wheels W2n+1, n ≥1, are 4-critical.
5. χ(G) ≤1 + ∆(G), where ∆denotes maximum degree.
6. Brooks’ theorem:
If G is a connected graph which is neither an odd cycle nor a
complete graph, then χ(G) ≤∆(G).
7. χ(G) ≤1 + max δ(G′), where δ denotes minimum degree, and where the maximum
is taken over all induced subgraphs G′ of G.
8. χ(G) ≤m(G), where m(G) is the maximum length of a path in G.
9. The Four Color Theorem (Appel and Haken, 1976): If G is planar, then χ(G) ≤4.
That is, every planar graph has a proper coloring of its vertices with 4 or fewer colors.
10. Hadwiger’s conjecture:
If G is a connected graph with χ(G) = n, then Kn is a
minor of G; it is known to be true for n ≤5.
11. Nordhaus-Gaddum inequalities:
If G is a graph with |V (G)| = p and G is its
edge-complement, then
• 2√p ≤χ(G) + χ(G) ≤p + 1;
• p ≤χ(G) · χ(G) ≤(p + 1)2/4.
12. The greedy coloring algorithm (Algorithm 1) produces a vertex coloring of a graph G
whose vertices are ordered. (It is called “greedy” because once a color is assigned, it is
never changed.)

Section 8.6
COLORINGS, LABELINGS, & RELATED PARAMETERS
633
Algorithm 1:
Greedy coloring algorithm.
input: a graph G with vertex list v1, v2, . . . , vn
c := 0 {Initialize color at “color 0”}
while some vertex still has no color
c := c + 1 {Get the next unused color}
for i := 1 to n {Assign the new color to as many vertices as possible}
if vi is uncolored and no neighbor of vi has color c then assign color c to vi
13. The number of colors used by the greedy coloring algorithm depends on the ordering
of the vertices of G. At least one of the orderings of the vertices of G yields χ(G), but
the number of colors used can exceed χ(G) arbitrarily.
14. There is no known polynomial-time algorithm for ﬁnding χ(G) exactly. Deciding
whether a graph has at most a particular chromatic number is NP-complete, if that
number is at least 3.
15. The following table gives the chromatic and edge chromatic numbers (see §8.6.2) of
the graphs in some common families.
graph G
χ(G)
χ1(G)
path Pn, n ≥3
2
2
cycle Cn, n even, n ≥2
2
2
cycle Cn, n odd, n ≥3
3
3
wheel Wn, n even, n ≥4
3
n
wheel Wn, n odd, n ≥3
4
n
complete graph Kn, n even, n ≥2
n
n −1
complete graph Kn, n odd, n ≥3
n
n
complete bipartite graph Km,n, m, n ≥1
2
max{m, n}
bipartite G, at least one edge
2
∆(G)
Petersen graph
3
4
complete k-partite Km1,...,mk, mi ≥1
k
max{m1, . . . , mk}
16. The choice number is at least the chromatic number.
17. If G is connected but neither complete nor an odd cycle, ch(G) ≤∆(G).
18. The choice number of Km,m is (1 + o(1)) log2 m.
19. Every planar graph is 5-choosable.
20. For every edge e of a simple graph G, πG(t) = πG−e(t) −πG·e(t).
21. The chromatic polynomial πG(t) of a graph with n vertices and m edges is a poly-
nomial in t of degree n, whose leading term is tn, whose next term is −mtn−1, and whose
constant term is 0.
22. The following table gives the chromatic polynomials of some graphs.
graph
πG(t)
n-vertex tree
t(t −1)n−1
cycle graph Cn
(t −1)n + (−1)n(t −1)
wheel Wn
t(t −2)n−1 + (−1)n−1t(t −2)
complete graph Kn
tn = t(t −1)(t −2) . . . (t −n + 1)

634
Chapter 8
GRAPH THEORY
Examples:
1. Time scheduling:
Let classes at a school be modeled by the vertices of a simple
graph G, with two vertices adjacent if and only if there is at least one student in both
of the corresponding classes. Then χ(G) gives the minimum number of time periods for
scheduling the classes so as to accommodate all the students.
2. Assignment of radio frequencies:
If the vertices of a graph G represent radio sta-
tions, with two stations adjacent precisely when their broadcast areas overlap, then χ(G)
determines the minimum number of transmission frequencies required to avoid broadcast
interference.
3. Separating combustible chemical combinations: Let the vertices of graph G represent
diﬀerent kinds of chemicals needed in some manufacturing process. An edge joins each
pair of chemicals that might explode if they are combined. The chromatic number of
this graph is the number of diﬀerent storage areas required so that no two chemicals that
mix explosively are stored together.
4. Proceeding in the direct way, as described in Fact 1, to color the graph in the following
ﬁgure quickly yields its chromatic number. Applying the greedy coloring algorithm, with
the vertices considered in cyclic order around the 8-cycle, yields a 3-coloring. Since this
graph contains an odd cycle (a 5-cycle), it cannot be 2-colored. Thus, χ = 3.
5. In the following ﬁgure vertex colorings are indicated for the cycle graphs C3, C4,
and C5; in each case three colors are used. Note that χ(C3) = χ(C5) = 3, whereas
χ(C4) = 2 (since the vertex colored “3” could have been colored “1”).
1
1
1
1
3
2
2
2
2
2
3
3
C3
C4
C5
6. The following ﬁgure shows three chromatically 4-critical graphs.
A
B
C
7. A 3-coloring of graph A in the ﬁgure of Example 6 would necessarily give some color
to three diﬀerent vertices. Two of these vertices would have to be adjacent (because
the edge-complement contains no 3-cycle). Thus, a 3-coloring could not be proper, and
hence χ = 4.
8. A 3-coloring of graph B in the ﬁgure of Example 6 would need three diﬀerent colors
on the outer 5-cycle. These would force the use of three diﬀerent colors on the points of
the central 5-star. This would force the use of a fourth color on the central vertex. Thus,
χ = 4.

Section 8.6
COLORINGS, LABELINGS, & RELATED PARAMETERS
635
9. The choice number of K3,3 is 3. To see that ch(K3,3) > 2, consider the assignment
{a, b}, {a, c}, and {b, c} to the vertices in one partite set, and the same in the other
partite set.
8.6.2
EDGE COLORINGS
Deﬁnitions:
An edge coloring of a graph is an assignment of colors to its edges such that adjacent
edges receive diﬀerent colors.
A graph G is k-edge colorable if there is an edge coloring of G using at most k colors.
The edge chromatic number χ1(G) (or χ′(G)) of a graph G is the minimum k such
that G is k-edge colorable. If χ1(G) = k, then G is edge k-chromatic.
Chromatic index is a synonym for edge chromatic number.
A graph is edge-chromatically k-critical if it is edge k-chromatic and χ1(G −e) =
χ1(G) −1 for every edge e of G.
For a graph G, the line graph L(G) has as vertices the edges of G, with two vertices
adjacent in L(G) if and only if the corresponding edges are adjacent in G.
Facts:
1. Every edge coloring of a graph G can be interpreted as a vertex coloring of the
associated line graph L(G). Thus, χ1(G) = χ(L(G)).
2. ∆(G) ≤χ1(G).
3. Vizing’s theorem: If G is a simple graph, then χ1(G) ≤∆(G) + 1.
4. Vizing’s general theorem: If G is a general graph whose maximum edge multiplicity
is µ, then χ1(G) ≤∆(G) + µ.
5. Either χ1(G) = ∆(G) (G is of class one) or χ1(G) = ∆(G) + 1 (G is of class two).
6. χ1(Km,n) = χ(L(Km,n)) = χ(Km □Kn) = max{m, n}, if m, n ≥1.
7. If G is bipartite, then χ1(G) = ∆(G).
8. χ1(Kn) = n if n is odd (n ̸= 1); χ1(Kn) = n −1 if n is even.
9. If G is planar and ∆(G) ≥8, then χ1(G) = ∆(G).
10. If G is 3-regular and Hamiltonian, then χ1(G) = ∆(G).
11. If G is regular with |VG| odd and |EG| > 0, then χ1(G) = ∆(G) + 1.
12. The greedy edge-coloring algorithm (Algorithm 2) produces an edge-coloring of a
graph G, whose vertices are ordered. The number of colors it assigns depends on the
vertex ordering, and it is not necessarily the minimum possible.
(It is equivalent to
applying the greedy vertex-coloring algorithm to the line graph.)
Algorithm 2:
Greedy edge-coloring algorithm.
input: a graph G with edge list e1, e2, . . . , en
c := 0 {Initialize color at “color 0”}
while some edge still has no color

636
Chapter 8
GRAPH THEORY
c := c + 1 {Get the next unused color}
for i := 1 to n
{Assign the new color to as many edges as possible}
if ei is uncolored and no neighbor of ei has color c then assign color c to ei
Examples:
1. The following three graphs are all edge 3-chromatic. None of them is edge-chromati-
cally 3-critical.
Since each graph has a vertex of degree three, no 2-edge-coloring is
possible.
2. The following graph is 5-edge-chromatic. Since there are 14 edges, a 4-edge-coloring
would have to give the same color to four of them. For this edge-coloring to be proper,
these four edges would have to have no endpoints in common. That is impossible, because
the graph has only seven vertices.
3. The Petersen graph is edge-chromatically 4-critical.
4. Exam scheduling:
Suppose that each student at a university is to be examined
orally by each of their professors at the end of the term. Then the minimum number of
examination periods required is the edge chromatic number of the bipartite graph with
vertices representing students and professors, and edges connecting students with their
professors.
5. Wiring electrical network boards: A number of relays, switches, and other electronic
devices D1, D2, . . . , Dn on a relay panel are to be connected into a network. The con-
necting wires are twisted into a cable, with those connected to D1 emerging at one point,
those connected to D2 at another, and so forth. The wires emerging from the same point
must be colored diﬀerently, so that they can be distinguished. The least number of colors
required to color the wires is the edge chromatic number of the associated network.
6. The following nonsimple graph illustrates Vizing’s general theorem. Its highest edge
multiplicity is 3, its maximum degree is 6, and its edge chromatic number is 9.

Section 8.6
COLORINGS, LABELINGS, & RELATED PARAMETERS
637
8.6.3
CLIQUES AND INDEPENDENCE
Deﬁnitions:
A clique of a graph G is a complete subgraph of G. It is maximal if it is contained in
no larger complete subgraph of G.
The clique number ω(G) of a graph G is the number of vertices of a largest clique of G.
A subset W of V (G) (or D of E(G)) is independent if no two elements of W (respec-
tively D) are adjacent.
The vertex independence number α(G) of G is the size of a largest independent set
of vertices in G.
The edge independence number α1(G) of a graph G is the size of a largest indepen-
dent set of edges in G. It is also called the matching number.
Facts:
1. The independence number of a graph is equal to the clique number of its edge-comple-
ment, and vice versa. That is, α(G) = ω(G) and ω(G) = α(G).
2. The chromatic number of a graph is at least as large as the clique number: χ(G) ≥
ω(G).
3. For each positive integer n, there is a graph G with chromatic number n and clique
number equal to 2; that is, G contains no triangles.
4. If no induced subgraph of a graph is isomorphic to P4, then its chromatic number
equals its clique number and the greedy algorithm (§8.6.1, Algorithm 1) always produces
a coloring with the minimum number of colors.
5.
|V (G)|
α(G) ≤χ(G) ≤|V (G)| + 1 −α(G).
6. If |E(G)| > ∆(G) · α1(G), then χ1(G) = ∆(G) + 1.
Examples:
1. The following graph has three maximal cliques—of sizes 2, 3, and 4. Thus, its clique
number is 4.
2. If 1 ≤m ≤n, then ω(Km,n) = 2, α(Km,n) = n, and α1(Km,n) = m.
3. Deﬁne Kn(m) to be the graph whose edge complement is nKm, the disjoint union
of n copies of Km. Then ω(Kn(m)) = n, α(Kn(m)) = m, and α1(Kn(m)) =
 mn
2

.
8.6.4
MAP COLORINGS
Deﬁnitions:
An orientable surface S is a surface homeomorphic to a sphere with g ≥0 handles
attached and is denoted by Sg.

638
Chapter 8
GRAPH THEORY
A nonorientable surface S is a surface homeomorphic to a sphere with k ≥1 crosscaps
attached and is denoted by Nk. (See §8.8.1.)
The Euler characteristic of a surface S is 2 −2g if S is homeomorphic to Sg, and
2 −k if S is homeomorphic to Nk. The most usual notation for Euler characteristic
in mathematics is χ(S). However, since that is used for the chromatic number, ad hoc
notation such as eu(S) is sometimes used in chromatic graph theory.
A map on a surface is an imbedding of a graph on that surface.
A map coloring is an assignment of colors to the regions of a map so that adjacent
regions (those sharing a one-dimensional boundary portion) receive diﬀerent colors.
A map M is n-colorable if there is a map coloring of M using at most n colors.
The chromatic number χ(M) of a map M is the minimum n such that M is n-colorable.
The chromatic number χ(S) of a surface S is the largest chromatic number χ(M) for
all maps M on S.
The (empire) chromatic number χ(S, c) for a surface S is the largest χ(M) for all
maps M on S, where now a country has at most c ≥1 components (regions) and all
components of a ﬁxed country are colored alike, but adjacent components of diﬀerent
countries must receive diﬀerent colors. (Thus χ(S) = χ(S, 1).)
Facts:
1. A region coloring can be regarded as a vertex coloring of the dual graph (see §8.8.2).
From this perspective, χ is the largest value of χ(G) for all graphs G imbeddable on S.
2. By stereographic projection (see §8.7.5), χ(S0) gives the chromatic number of the
plane.
3. Let G be a planar cubic block; then χ1(G) = 3.
4. Let M be a plane map whose graph G is connected and bridgeless. Then χ(M) = 2
if and only if G is Eulerian.
5. Let M be a plane map for a cubic connected bridgeless graph G; then χ(M) = 3 if
and only if the dual graph is Eulerian.
6. If G is a plane graph without triangles, then χ(G) = 3. (Gr¨otzsch, 1958)
7. The Four Color Theorem (Appel and Haken, 1976): χ(S0) = 4. That is, every map
on a sphere or plane can be colored with 4 or fewer colors.
8. The Heawood map coloring theorem (Ringel and Youngs, 1968): For g > 0,
χ(Sg) =
7 + √1 + 48g
2

.
9. The nonorientable Heawood map coloring theorem (Ringel, 1954): For k > 0,
χ(Nk) =
7 +
√
1 + 24k
2

except that χ(N2) = 6.
10. χ(S, c) ≤
$
6c + 1 +
p
(6c + 1)2 −24eu(S)
2
%
.
11. χ(S0, c) = 6c for c ≥2; χ(S1, c) = 6c + 1 for c ≥1; and χ(N1, c) = 6c for c ≥1.

Section 8.6
COLORINGS, LABELINGS, & RELATED PARAMETERS
639
12. History of the four color problem:
In 1852, Francis Guthrie asked whether four
colors suﬃce to color every planar map. Arthur Cayley in 1878 was ﬁrst to mention the
problem in print. In 1879, A. B. Kempe, a London barrister, published a “proof” of the
four color conjecture: every planar map is 4-colorable. In 1890 Percy Heawood found
an error in Kempe’s argument. A correct proof was established by Kenneth Appel and
Wolfgang Haken in 1976.
13. Concepts in the Haken-Appel proof of the four color theorem:
Appel and Haken
found an “unavoidable” set with 1476 graphs, which means that at least one of these
graphs must be a subgraph of any minimum counterexample to the four color conjecture.
A method called “discharging” is used to ﬁnd an unavoidable set. Using a computer,
they proved that each of these graphs is “reducible”, which means that it cannot be a
subgraph of a minimum counterexample.
14. A simpliﬁed proof of the four color theorem can be found at the website:
http://www.math.gatech.edu/~thomas/FC/ftpinfo.html
Examples:
1. Let M be the tetrahedral map, i.e., an imbedding of K4 in S0. By Fact 4, χ(M) ̸= 2,
since K4 is not Eulerian. By Fact 5, χ(M) ̸= 3, since the dual graph (isomorphic to K4)
is also not Eulerian. Thus, χ(M) = 4 = χ(S0).
2. Cartography: If countries on Earth are allowed two components, but no more, then
by Fact 11 a map might require twelve colors, but no more.
3. By Fact 8, χ(S1) = 7. The dual map of the following ﬁgure imbeds K7 in the torus.
To obtain the torus, paste the left side of the rectangular sheet directly to the right side,
and then paste the top to the bottom with a 2
7 twist.
E
A
B
C
D
E
F
G
F
G
A
B
C
D
8.6.5
MORE GRAPH COLORINGS
This subsection deals with the proper coloring of the vertices of a graph where each
vertex is assigned more than one color (or label), or colorings of the vertices and edges
simultaneously.
Deﬁnitions:
A (proper) k-tuple coloring of a graph is an assignment of a set of k distinct colors to
each vertex of a graph so that adjacent vertices have disjoints sets.
The k-tuple chromatic number χk(G) of a graph G is the smallest number of colors
such that G has a k-tuple coloring.
The fractional chromatic number χf(G) of a graph G is the minimum of the ratio
χk(G)/k taken over all k.
A total coloring of a graph is an assignment of colors to both the vertices and edges
such that two elements receive diﬀerent colors if they are adjacent or incident.
The total chromatic number χT (G) is the minimum number of colors needed for a
total coloring of graph G.

640
Chapter 8
GRAPH THEORY
Facts:
1. χk(G) ≤kχ(G).
2. If the clique number ω(G) (see §8.6.3) of G is equal to χ(G), then χk(G) = kχ(G).
3. χk(Kn) = nk.
4. If G is bipartite (with at least one edge), then χk(G) = 2k.
5. χf(G) ≤χ(G).
6. χT (Kn) = n if n is even, and χT (Kn) = n + 1 if n is odd.
7. The total coloring conjecture is that χT (G) ≤∆(G) + 2 for all graphs G.
Examples:
1. Multiple channel assignment:
Several cities each need to have four broadcast fre-
quencies assigned to them (a generalization of §8.6.1, Example 2). The 4-tuple chromatic
number χ4(G) is the minimum number of frequencies needed so that there is no broadcast
interference.
2. χ2(C5) = 5, as illustrated in the following ﬁgure.
In fact it can be shown that
χf(C5) = 5/2.
1,2
3,4
4,5
2,3
1,5
3. Exam scheduling: Each ﬁnal exam at a school is given in two parts, with each part
requiring one ﬁnal exam period. If a graph G is constructed by using the courses as
vertices, with an edge joining v and w if there is a student taking courses v and w,
then χ2(G) gives the minimum number of periods required to schedule all the exams so
no student has a conﬂict.
8.6.6
DOMINATION IN GRAPHS
Domination has applications in areas such as network design, social sciences, optimiza-
tion, and bioinformatics. For more information see the books [HaHeSl98a], [HaHeSl98b],
and [HeYe13].
Deﬁnitions:
A dominating set of a graph is a set S of vertices such that every vertex not in S is
adjacent to a vertex in S.
A minimal dominating set is a dominating set that contains no dominating set as a
proper subset.
The domination number γ(G) is the minimum cardinality of a (minimal) dominating
set in a graph G, and the upper domination number Γ(G) is the maximum cardinality
of a minimal dominating set.

Section 8.6
COLORINGS, LABELINGS, & RELATED PARAMETERS
641
A total dominating set of a graph is a set S of vertices such that every vertex is
adjacent to a vertex in S. Thus, a total dominating set S is a dominating set with the
additional property that the subgraph induced by the set S contains no isolated vertex.
A total dominating set S is minimal if it contains no total dominating set as a proper
subset.
The total domination number γt(G) of a graph G is the minimum cardinality of a
total dominating set, and the upper total domination number Γt(G) of G is the
maximum cardinality of a minimal total dominating set.
An independent dominating set is a set that is both a dominating set and an inde-
pendent set.
The independent domination number, i(G), and the (vertex) independence
number, α(G), are the minimum and maximum cardinalities, respectively, of a maximal
independent set in G.
Let S be a set of vertices in a graph G. A vertex v ∈S is S-irredundant if v has no
neighbor in S or there exists a vertex w /∈S that is adjacent to v but to no other vertex
of S. The set S is an irredundant set if every vertex in it is S-irredundant.
The irredundance number of G, ir(G), is the minimum cardinality of a maximal
irredundant set, while the upper irredundance number of G, IR(G), is the maximum
cardinality of an irredundant set.
The corona H ◦K1 of a graph H is the graph obtained from H by adding for each vertex
v of H a new vertex v′ and the pendant edge vv′.
Facts:
1. If S is a dominating set, then so too is every superset of S. However, not every subset
of S is necessarily a dominating set.
2. Every maximal independent set is a minimal dominating set, and every minimal
dominating set is a maximal irredundant set. Thus ir(G) ≤γ(G) ≤i(G) ≤α(G) ≤
Γ(G) ≤IR(G).
3. Determining whether a graph has domination number at most k is NP-complete.
4. If G is a graph of order n, then 1 ≤γ(G) ≤n. Equality of the lower bound is attained
if and only if the maximum degree of G is n −1, and equality holds for the upper bound
if and only if G is the empty graph, that is, a set of isolated vertices.
5. If G = (V, E) is a graph with no isolated vertex and D is a minimal dominating set
of G, then V \ D is a dominating set of G.
6. Ore’s bound. If G is a graph of order n with no isolated vertex, then γ(G) ≤n/2.
7. If G is a graph of order n with no isolated vertex, then γ(G) = n/2 if and only if the
components of G are the cycle C4 or the corona H ◦K1 for any connected graph H.
8. For every regular graph G of order n with no isolated vertex, Γ(G) ≤n/2.
9. If graph G has n vertices and m edges and does not contain isolated vertices, then
γ(G) ≤(1 −b)n/2 + bm for all b ≥0. For example, taking b = 0, we have Ore’s upper
bound of γ(G) ≤n/2. Taking b = 1/3, we have that γ(G) ≤(n + m)/3.
10. If G is a connected graph of order n ≥8 with δ(G) ≥2, then γ(G) ≤2n/5, and
this bound is tight. (Here δ(G) denotes the minimum degree.)
11. If G is a graph of order n and δ(G) ≥3, then γ(G) ≤3n/8, and this bound is tight.

642
Chapter 8
GRAPH THEORY
12. If G is a connected graph of order n ≥14 with δ(G) ≥2 and no induced 4-cycle
and no induced 5-cycle, then γ(G) ≤3n/8.
13. Vizing’s conjecture. For any graphs G and H, γ(G □H) ≥γ(G)γ(H). (Here □
denotes Cartesian product.)
14. For any graphs G and H, Γ(G □H) ≥Γ(G)Γ(H).
15. If G is a connected graph of order n ≥3, then γt(G) ≤2n/3.
16. If G is a connected graph of order n ≥11 with δ(G) ≥2, then γt(G) ≤4n/7.
17. If G is a graph of order n with δ(G) ≥3, then γt(G) ≤n/2.
18. If G is a graph of order n with δ(G) ≥4, then γt(G) ≤3n/7.
19. If G is a graph of order n with δ(G) ≥5, then γt(G) ≤
  4
11 + 1
72

n. The conjectured
bound is γt(G) ≤4n/11.
20. If G is a graph of order n with δ(G) ≥1, then γt(G) ≤
  1+ln δ
δ

n.
21. If G is a connected graph of order n, girth g ≥3 with δ(G) ≥2, then
γt(G) ≤n
2 + max

1,
n
2(g + 1)

,
and this bound is sharp.
22. For every k-regular graph G of order n with no isolates, Γt(G) ≤n/(2 −1/k).
23. For any isolate-free graphs G and H, 2γt(G □H) ≥γt(G)γt(H).
24. For any isolate-free graphs G and H, 2Γt(G □H) ≥Γt(G)Γt(H).
25. For every 0 < ǫ′ < ǫ and p = (1 + ǫ′)
q
1
n(2 ln n), almost every graph G ∈G(n, p)
has
 1
2
√
2 −ǫ
 p
n ln(n) < γ(G) ≤γt(G) <
 1
√
2 + ǫ
 p
n ln(n).
Examples:
1. For the Petersen graph G below, the sets S1 = {2, 5, 7}, S2 = {1, 3, 6, 10}, and
S3 = {1, 2, 3, 4, 5} are all minimal dominating sets. Hence, the Petersen graph contains
minimal dominating sets of cardinalities 3, 4, and 5. No set of two vertices dominates all
ten vertices in the graph, and so the set S1 is a dominating set of minimum cardinality
and γ(G) = 3. Every minimal dominating set in the Petersen graph has cardinality at
most 5, implying that Γ(G) = 5.
1
2
3
4
5
6
7
8
9
10
2. For the Petersen graph G above, it holds that γt(G) = 4 and the closed neighborhoods
N[v] corresponding to the ten vertices v in the Petersen graph form the ten minimum
total dominating sets of G. Further, Γt(G) = 6 and the set {1, 3, 5, 8, 9, 10} is an example
of a minimal total dominating set of G of size 6.

Section 8.6
COLORINGS, LABELINGS, & RELATED PARAMETERS
643
8.6.7
GRAPH PEBBLING
Deﬁnitions:
A conﬁguration C is an assignment of pebbles to the vertices of a connected graph G.
The value C(v) denotes the number of pebbles at vertex v. The size of C is deﬁned as
|C| = P
v∈V (G) C(v).
For an edge uv, if u has at least two pebbles on it, then a pebbling step from u to v
is to remove two pebbles from u and place one pebble on v. That is, if C is the original
conﬁguration, then the resulting conﬁguration C′ has C′(u) = C(u)−2, C′(v) = C(v)+1,
and C′(x) = C(x) for all x ∈V (G) −{u, v}.
A conﬁguration C is r-solvable if it is possible from C to place a pebble on vertex r
via pebbling steps; it is r-unsolvable otherwise. The conﬁguration C is solvable if it
is r-solvable for every vertex r.
For a connected graph G and a root vertex r, the rooted pebbling number π(G, r) is
deﬁned to be the minimum number t so that every conﬁguration C of size t is r-solvable.
The pebbling number π(G) is the maximum value of π(G, r) over all r.
A graph G is of Class 0 if π(G) equals its order.
A sequence of paths P = (P[1], . . . , P[h]) is a maximum r-path partition of a rooted
tree (T, r) if P forms a partition of E(T ), r is a leaf of P[1] and, for all 1 ≤i ≤h, P[i]
is a maximum-length path in T −Si−1
j=1 E(P[j]) among all such paths with one endpoint
in Si−1
j=1 P[j].
For m ≥2t + 1, the Kneser graph K(m, t) has as vertices all t-element subsets of
{1, 2, . . ., m} and edges between every pair of disjoint sets.
Facts:
1. Graph pebbling arose in combinatorial number theory. It has since produced the
following more general result. If g1, . . . , gn is a sequence of elements of an abelian group Γ
of size n, then there is a nonempty subsequence (gk)k∈K such that P
k∈K ak = 0Γ and
P
k∈K 1/|gk| ≤1, where |g| denotes the order of the element g in Γ and 0Γ is the identity
element in Γ.
2. Every rooted graph (G, r) has π(G, r) ≥max{n(G), 2ecc(r)}, where ecc(r) is the
maximum distance of a vertex from r, and π(G, r) ≤2n(G)−1.
3. The path on n vertices has pebbling number π(Pn) = 2n−1.
4. The cycle has pebbling number π(C2k) = 2k and π(C2k+1) = ⌈(2k+2 −1)/3⌉for all
k ≥1.
5. If (T, r) is a rooted tree, then π(T, r) = Ph
i=1 2li −h + 1, where li is the length of
path P[i] in a maximum r-path partition P of T .
6. Complete graphs, complete bipartite graphs other than stars, cubes, the Petersen
graph, and split graphs with minimum degree at least 3 are all Class 0.
7. For any constant c > 0 there is an integer t0 such that the Kneser graph K(2t + s, t)
is Class 0 when t > t0 and s ≥c(t/ log t)1/2.
8. If G is a graph with n vertices and e edges, then G is Class 0 when e ≥
 n−1
2

+ 2,
while G being Class 0 implies that κ(G) ≥2, e ≥⌊3n/2⌋, and the girth of G is at most
1 + 2 log n.

644
Chapter 8
GRAPH THEORY
9. If diam(G) = 2, then π(G) ≤n(G) + 1. If also κ(G) ≥3, then G is Class 0.
10. If diam(G) = d and κ(G) ≥22d+3, then G is Class 0.
11. There exists a Class 0 graph G on at most n ≥3 vertices with girth at least
⌊
p
(log n)/2 + 1/4 −1/2⌋.
12. Let Gn,p be the random graph on n vertices with edge probability p (see §8.11.3,
§10.8.2). If p ≫(n log n)1/d/n for ﬁxed d, then almost surely Gn,p is Class 0.
13. Graham’s Conjecture states that every pair of connected graphs G and H satisfy
π(G □H) ≤π(G)π(H). This is known to be true when G and H are both complete
graphs, both trees, both cycles, both complete bipartite graphs with at least 15 vertices
per part, or both connected graphs on n vertices with minimum degree at least k ≥
212n/k+15.
14. Deciding whether a conﬁguration on a graph is solvable is NP-complete, even when
restricted to the classes of diameter-two graphs or planar graphs, but is in P when
restricted to the classes of complete graphs, trees, diameter-two planar graphs, or outer-
planar graphs.
15. Deciding whether π(G) ≤k is ΠP
2 -complete (see §17.5.1).
16. π(G) can be calculated in polynomial time when G is a tree, a diameter-two graph,
a split graph, or a 2-path.
Examples:
1. Two r-unsolvable conﬁgurations (of maximum size, right) on the path P7 are shown
below.
1
1
0
0
2
0
7
r
3
0
0
0
0
0
15
r
2. A maximum-sized r-unsolvable conﬁguration on a tree is shown below.
255
31
15
7
3
3
1
1
1
1
1
r
3. An r-solvable (and solvable) conﬁguration is shown below.
1
0
4
1
r

Section 8.6
COLORINGS, LABELINGS, & RELATED PARAMETERS
645
8.6.8
GRAPH LABELINGS
Deﬁnitions:
A graph with q edges is graceful if there exists an injection f from the vertices to the
set {0, 1, . . ., q} such that, when each edge xy is assigned the label |f(x) −f(y)|, the
resulting edge labels are distinct.
A graph with q edges is harmonious if there is an injection f from the vertices to the
group of integers modulo q such that when each edge xy is assigned the label f(x)+f(y)
(mod q), the resulting edge labels are distinct. When a graph is a tree, exactly one label
may be used on two vertices.
A connected graph is called magic if there is a labeling of the edges with distinct positive
integers such that for each vertex v the sum of the labels of all edges incident with v
is the same for all v. A magic labeling is called supermagic if the set of edge labels
consists of consecutive positive integers.
A graph with q edges is called antimagic if its edges can be labeled with distinct integers
from 1 to q such that the sums of the labels of the edges incident to each vertex are
distinct.
Remarks:
1. In 1967 A. Rosa introduced graceful labelings as tools for decomposing the complete
graph into isomorphic subgraphs.
2. In 1980 R. Graham and N. Sloane introduced harmonious labelings as a modular
version of additive bases problems stemming from error-correcting codes.
3. Determining whether a graph has a harmonious labeling is NP-complete.
4. “Almost all” graphs are not graceful or harmonious.
5. The conjecture made in 1968 by Ringel and Kotzig that all trees are graceful is still
open.
6. The conjecture made in 1980 by Graham and Sloane that all trees are harmonious is
still open.
7. J. Sedl´aˇcek introduced magic labelings of graphs in 1963.
8. B. M. Stewart introduced supermagic labelings of graphs in 1967 as an extension of
the classic concept of an n × n magic square in number theory, which corresponds to a
supermagic labeling of Kn,n.
9. In 1990 Hartsﬁeld and Ringel introduced antimagic graphs. Their conjectures that
every tree except P2 is magic and every connected graph except P2 is magic are still
open.
10. There are scores of variations on graceful, harmonious, magic, and antimagic label-
ings.
Examples:
1. Here are graceful and harmonious labelings of the wheel W5.

646
Chapter 8
GRAPH THEORY
2
0
10
7
3
9
2
8
5
1
7
10
3
4
6
9
1
0
2
4
6
8
1
3
5
7
9
2
6
0
4
8
2. The following table speciﬁes conditions for which some common graphs are graceful
and harmonious.
Graph
Graceful
Harmonious
trees
at most 35 vertices
at most 31 vertices
caterpilars
all
all
Cn
iﬀn ≡0, 3 (mod 4)
iﬀn odd
wheels Wn
all
all
grids Pm □Pn
all
iﬀ(m, n) ̸= (2, 2)
Kn
iﬀn ≤4
iﬀn ≤4
Km,n
all
iﬀm or n = 1
Cm □Pn
n = 2 or m even
n odd; n = 2, m ̸= 4; m = 4, n ≥3
Cm □Cn
m ≡0 (mod 4), n even
m = 4, n > 1
n-cube
all
iﬀn ≥4
3. Here are magic and antimagic labelings of the wheel W5.
19
19
19
19
19
19
2
3
5
8
1
6
10
4
7
11
30
6
12
18
24
20
2
4
6
8
10
3
5
7
9
1
4. The following table speciﬁes conditions for which some common magic and antimagic
graphs. An M indicates the graph is magic; SPM indicates it is supermagic.
Graph
Magic
Antimagic
Pn
M iﬀn = 2
n ≥3
Cn
M none
all
Kn
M if n = 2 or n ≥5
n ≥3
wheels Wn
M n ≥4
all
Km,n
M if m = n ≥3
all except K1,1
Cm □Cn
SPM if m = n or m, n even
all
n-cube
SPM n = 1 or n > 2 even
all

Section 8.7
PLANAR DRAWINGS
647
8.7
PLANAR DRAWINGS
Planarity is an important consideration in physical networks of any kind, because it
is usually less expensive to fabricate a planar network. For instance, overpasses are a
costly feature in highway design. Moreover, it is less complicated to manufacture a planar
electrical network than a nonplanar network.
8.7.1
CHARACTERIZING PLANAR GRAPHS
A graph cannot be drawn without edge-crossings in the plane if it “contains” either the
complete graph K5 or the complete bipartite graph K3,3. Conversely, every graph that
“contains” neither of those two graphs can be drawn without crossings.
Deﬁnitions:
A graph imbedding (or embedding) is a drawing with no crossings at all.
A graph is planar if it has an imbedding in the plane.
A graph is nonplanar if no imbedding in the plane is possible.
A drawing of a graph is normalized if there are no crossings, or if each crossing is a
point where the interior of one edge crosses the interior of one other edge. (Edges may
be drawn either straight or with curves.)
The graphs K5 and K3,3 are called the Kuratowski graphs.
Facts:
1. The graphs K5 and K3,3 are both nonplanar. See Examples 4 and 5 for proofs that
they are not planar.
2. Kuratowski planarity theorem:
A graph is planar if and only if it has no subgraph
homeomorphic (see §8.1.2) to K5 or to K3,3.
Examples:
1. The drawings of Q3, K5, and K3,3 in the following ﬁgure all have crossings. However,
the graph Q3 is planar, because it can be redrawn without any crossings.
2. The drawings of Q3 and K5 in the ﬁgure of Example 1 are normalized, but the
drawing of K3,3 is not normalized, because three lines go through the same point.
3. The Petersen graph does not contain K3,3 itself as a subgraph. However, if the two
edges depicted by broken lines in the following ﬁgure are discarded, then the resulting
graph is homeomorphic to K3,3, so the Petersen graph is not planar.

648
Chapter 8
GRAPH THEORY
4. Nonplanarity of K5:
To draw the complete graph on the vertices v1, v2, v3, v4, v5
in the plane, one might as well start by drawing the 4-cycle v1, v2, v3, v4, which (by the
Jordan curve theorem) separates the plane. Next draw the edges between v1 and v3 and
between v2 and v4. To avoid crossing each other, one of these edges must go inside the
4-cycle and the other outside, as shown in the following ﬁgure. The net result so far is
that there are four 3-sided regions, each with three vertices on its boundary. Thus, no
matter which region is to contain the vertex v5, that vertex cannot be joined to more
than three other vertices without crossing the boundary.
v1
v2
v3
v4
5. Nonplanarity of K3,3:
To form a planar drawing of the complete bipartite graph
on the parts {v1, v3, v5} and {v2, v4, v6}, one might as well start by drawing the 6-cycle
v1, v2, v3, v4, v5, v6, which separates the plane. Next draw the edges between v1 and v4
and between v2 and v5. To avoid crossing each other, one of these edges must go inside
the 6-cycle and the other outside. The net result so far is shown in the following ﬁgure.
It is now clear that v3 and v6 cannot be joined without crossing some other edge.
v1
v6
v5
v4
v3
v2
6. Civil engineering:
Suppose that a number of towns are to be joined by a network
of highways.
If the network is planar, then the cost of bridges for underpasses and
overpasses can be avoided.
7. Electrical networks:
A planar electrical network with bare wires joining the nodes
can be placed directly onto a ﬂat board. Otherwise, insulation is needed to prevent short
circuits at wire crossings.
8.7.2
NUMERICAL PLANARITY CRITERIA
Certain numerical relationships are true of all planar graphs. One way to show that a
graph is nonplanar is to show that it does not satisfy one of these relations.

Section 8.7
PLANAR DRAWINGS
649
Deﬁnitions:
A region of an imbedded graph is, informally, a piece of what results when the surface
is cut open along all the edges. That is, a maximal subsurface containing no vertex and
no part of any edge of the graph.
The boundary of a region R of an imbedded graph is the subgraph containing all
vertices and edges incident on R. It is denoted ∂R.
A face of an imbedded graph is a region plus its boundary.
The exterior region of a planar graph drawing is the region that extends to inﬁnity.
The girth of a graph is the number of edges in a shortest cycle. The girth is undeﬁned
if the graph has no cycles.
Facts:
1. Euler polyhedral equation:
Let G = (V, E) be a connected graph imbedded in the
plane with face set F. Then |V | −|E| + |F| = 2.
2. Edge-face inequality:
Let G = (V, E) be a simple, connected graph imbedded in a
surface with face set F. Then 2|E| ≥3|F|.
3. Edge-face inequality (strong version): Let G = (V, E) be a connected graph, but not
a tree, imbedded in a surface with face set F. Then 2|E| ≥girth(G) · |F|.
4. Let G = (V, E) be a simple, connected planar graph. If G is planar then 3|V |−|E| ≥6.
5. Let G = (V, E) be a connected planar graph that is not a tree. Then (|V | −2) ·
girth(G) ≥|E| · (girth(G) −2).
6. Let G = (V, E) be a simple, connected, bipartite graph that is not a tree. If G is
planar then |E| ≤2 · |V | −4.
Examples:
1. In the planar imbedding of the following ﬁgure, |V | = 4, |E| = 6, and |F| = 4. Thus,
|V | −|E| + |F| = 4 −6 + 4 = 2. (The “exterior” region counts as a face.)
2. Fact 4 implies that K5 is nonplanar.
3. Fact 5 implies that the Petersen graph, whose girth equals 5, is nonplanar.
4. Fact 6 implies that K3,3 is nonplanar.
8.7.3
PLANARITY ALGORITHM
Deﬁnitions:
A bridge B of a subgraph H in a graph G is a maximal connected subgraph B of
the edge-complement G −H such that for any two edges d and e of B, there is a path
starting with d and terminating in e, none of whose internal vertices lies in B ∩H.
An attachment of a bridge is a vertex both in the bridge and the subgraph. (That is,
an attachment is a vertex in which the bridge meets the rest of the graph.)

650
Chapter 8
GRAPH THEORY
Facts:
1. Call two edges in the complement of a subgraph H of a graph G “related” if they are
both contained in a path in G that has no vertices of H in its interior. Then the bridges
of H are the induced subgraphs on the equivalence classes of edges under this relation.
2. Informally, a bridge is a subgraph obtained from one of the “pieces” that result by
deleting H from G by reattaching the endpoints to the edges that attach to H. See
Example 1.
3. The time needed to test planarity by searching directly for subdivided copies of K5
and K3,3 is an exponential function of the number of vertices.
4. J. Hopcroft and R. Tarjan developed a planarity-testing algorithm that can be exe-
cuted in time proportional to the number of vertices (“linear time”).
5. None of the linear-time planarity algorithms is easy to describe and implement. How-
ever, Algorithm 1 below is easily implemented, and its running time is satisfactory for
reasonably large graphs.
6. Algorithm 1 can be implemented to run in time approximately proportional to the
square of the number of vertices (“quadratic time”).
Algorithm 1:
Easy planarity-testing for graph G.
input: a simple, connected graph G
G0 := an arbitrary cycle in G; draw G0 in the plane; j := 0
{Grow a sequence of nested subgraphs G0, G1, . . . until all of G has been drawn
in the plane; if this does not happen, then G is nonplanar}
while Gj ̸= G {This possible exit implies G is planar} and
 ∀B ∈bridges (Gj)

 ∀v ∈attachments (B)
 ∃region R of Gj in plane

v ∈∂R {This possible
exit implies G is nonplanar} do
{While-loop body says how to grow subgraph Gj+1}
if
 ∃B ∈bridges (Gj)
 ∀v ∈attachments (B)
 ∃! region R of Gj
 
v ∈∂R

then {Case 1 — a forced move exists}
select a path P between two attachments of B
obtain subgraph Gj+1 by drawing path P in region R
else {Case 2 — no forced move exists}
select any bridge, and ﬁnd two regions for its attachments
select any path between two attachments of that bridge
draw that path into either region to obtain Gj+1
j := j + 1
Example:
1. The following ﬁgure shows a subgraph and its three bridges B1B2, B3. The subgraph
H is the dark cycle. Attachments of the bridges are the vertices along the dark cycle.
B2
B1
B3
H

Section 8.7
PLANAR DRAWINGS
651
2. Suppose that the ﬁgure in Example 1 occurred in the execution of Algorithm 1. At
the next iteration of the while-loop body, suppose that bridge B2 is selected, and suppose
that a path in B2 is drawn outside the dark cycle. Then, on the following iteration of the
while-loop body, bridge B3 would be a forced choice, and a path from B3 would have to
be drawn inside the dark cycle. Eventually, bridge B1 would have to be drawn outside
the dark cycle, thereby yielding a planar drawing of the entire graph.
8.7.4
CROSSING NUMBER AND THICKNESS
Deﬁnitions:
The crossing number of graph G, denoted ν(G), is the minimum number of edge-
crossings possible in a normalized drawing of G in the plane.
The thickness of graph G, denoted θ(G), is the minimum number of planar graphs
whose union is G.
Facts:
1. ν(Kn) ≤1
4 ·
 n
2

·
 n−1
2

·
 n−2
2

·
 n−3
2

.
2. For all integers n ≤12, ν(Kn) = 1
4 ·
 n
2

·
 n−1
2

·
 n−2
2

·
 n−3
2

.
3. It is conjectured that equality in Fact 2 holds for all positive integers.
4. ν(Km,n) ≤
 m
2

·
 m−1
2

·
 n
2

·
 n−1
2

.
5. For all integers m and n such that min(m, n) ≤6, ν(Km,n) =
 m
2

·
 m−1
2

·
 n
2

·
 n−1
2

.
6. Zarankiewicz’s conjecture:
The equation of Fact 5 holds for all positive integers m
and n.
7. θ(Kn) =
 n+7
6

except that θ(K9) = θ(K10) = 3.
8. θ(Qn) =
 n+1
4

.
9. θ(G) ≥
j
|E|
3|V |−6
k
for all simple graphs.
Examples:
1. Fact 1 implies that ν(K6) ≤3. Thus, it is possible to draw K6 with at most three
crossings.
2. Computer engineering:
Facts 7 and 8 yield lower bounds for the minimum number
of layers needed for a multi-layer layout of an electronic interconnection network whose
architecture is a complete graph or a hypercube graph, respectively.
8.7.5
STEREOGRAPHIC PROJECTION
Deﬁnitions:
A continuous one-to-one function from one subset of Euclidean space onto another is
a topological equivalence if its inverse is continuous. (Informally, this means that
either subset could be reshaped into the other without tearing, but only by compressing,
stretching, and twisting.)

652
Chapter 8
GRAPH THEORY
The stereographic projection adds a single point to a plane and thereby closes the
“hole at inﬁnity” and converts it into a sphere, as follows. Start with a sphere in 3-
space, tangent at its south pole S to the plane z = 0 at the origin (0, 0, 0), as shown in
the following ﬁgure; and through each point x of the sphere draw a ray from the north
pole N, extending to the point f(x) at which it meets the plane.
N
S
x
f(x)
(0,0,0)
Facts:
1. The correspondence x →f(x) from the sphere minus its north pole onto the plane is
a topological equivalence. In other words, the sphere minus a point could be stretched
apart at the missing point and ﬂattened out so that it covers the plane.
2. Any planar imbedding can be transformed into an imbedding in the sphere, which
is a closed surface, by using the inverse of stereographic projection and closing up the
pinhole. This eliminates the inelegant nuisance of having one “special” region with a
hole.
8.7.6
GEOMETRIC DRAWINGS
Geometric drawing of graphs is a topic in computational geometry. Unlike ordinarily
planarity and topological graph theory, its concerns include the exact coordinates in the
plane of the images of the vertices and the edges.
Deﬁnitions:
A straight-line drawing of a graph is a drawing in which each edge is represented by
a single straight line segment.
An orthogonal drawing of a graph is a drawing in which each edge is represented by
a chain of horizontal and vertical line segments.
A polyline drawing of a graph is a drawing in which each edge is represented by a
polygonal path, that is, by a chain of line segments with arbitrary slope.
A bend in a polyline drawing is a junction point of two line segments belonging to the
same edge.
A grid drawing of a graph is a polyline drawing in which vertices, crossings, and bends
have integer coordinates.
The area of a graph drawing is the area of the convex hull of the drawing.
A distance-ranked partition of a graph G with respect to a nonempty vertex subset S
has cells Cj for j = 0, 1, . . . . Vertex v is in cell Cj if and only if its shortest path to every
vertex of S has length j.
A distance-ranked drawing of a graph G with respect to a nonempty vertex subset S
has the cells of its distance-ranked partition organized into columns from left to right
according to ascending distance from S.

Section 8.7
PLANAR DRAWINGS
653
Facts:
1. Straight-line and orthogonal drawings are special cases of polyline drawings.
2. Polyline drawings can approximate drawings with curved edges.
3. Computer systems that support general polyline drawings are more complicated than
systems that support only straight-line drawings.
4. Many graph drawing problems involve a trade-oﬀbetween competing objectives, such
as the desire to minimize both the area and the number of edge-crossings.
5. The area required for a planar polyline grid drawing of an n-vertex planar graph
is O(n2).
6. The area required for a planar orthogonal grid drawing of an n-vertex planar graph
is O(n2).
7. The area required for a planar straight line grid drawing of an n-vertex planar graph
is O(n2).
8. Every planar graph of maximum degree 4 has an orthogonal planar drawing whose
total number of bends is at most 2n + 2.
9. Every planar graph of maximum degree 4 has an orthogonal planar drawing such
that the maximum number of bends in an edge is at most 2.
Examples:
1. The following ﬁgure shows a nonplanar straight line drawing of the octahedron K2,2,2
and a planar polyline drawing of that same graph.
2. The following ﬁgure shows two orthogonal grid drawings of the octahedron K2,2,2.
Whereas the lefthand drawing has two edges with three bends, the maximum number of
bends in any edge of the middle drawing is two. The righthand drawing has the smallest
total number of bends and the smallest area of the three drawings.
3. The following ﬁgure shows a distance-ranked drawing of the cube graph Q3 with
respect to the vertex 000.
100
110
000
001
011
010
101
111

654
Chapter 8
GRAPH THEORY
8.8
TOPOLOGICAL GRAPH THEORY
Topological graph theory mainly involves placing graphs on closed surfaces.
Special
emphasis is given to placements that are minimum with respect to some kind of cost
or that are highly symmetric. Minimization helps to control the cost of manufacturing
networks, and symmetry facilitates the task of routing information through a network.
8.8.1
CLOSED SURFACES
Holes in a surface can be closed oﬀby operations like stereographic projection (see §8.7.5).
This enables topological graph theory to focus on drawings in closed surfaces.
Deﬁnitions:
Adding a handle to a surface is accomplished in two steps (illustrated in Example 1):
(a) punch two disk-like holes into the surface; and (b) reclose the surface by installing a
tube that runs from one hole to the other.
An orientable surface is deﬁned recursively to be either the sphere S0, or a surface
that is obtained from an orientable surface by adding a handle. (See Example 2 for the
construction.)
The genus of an orientable surface is the number of handles one must add to the
sphere to obtain it. Thus, the surface obtained by adding g handles to S0 has genus g.
It is denoted Sg.
The torus is the surface S1 of genus 1.
A M¨obius band is the surface obtained by pasting the left side of a rectangular sheet
to the right with a half-twist. A paper ring with a half-twist is a commonplace model of
the M¨obius band. (See Example 3.)
Adding a crosscap to a surface is accomplished by the following two steps: (a) punch
one disk-like hole into the surface; and (b) reclose the hole by matching its boundary to
the boundary of a M¨obius band.
The nonorientable surface Nk is obtained by adding k crosscaps to the sphere. The
sphere is sometimes regarded as the “surface with crosscap number 0” and denoted N0,
even though it is orientable. (See Example 4.)
The subscript k is called the crosscap number of the surface Nk.
The surfaces N1 and N2 are called the projective plane and the Klein bottle, respec-
tively.
Facts:
1. Classiﬁcation of closed surfaces: Every closed surface is equivalent to exactly one of
the surfaces Sg (g ≥0) or Nk (k ≥1).
2. Adding a handle to the nonorientable surface Nk is equivalent to adding two crosscaps.
That is, the resulting surface is Nk+2.
3. If a loop is drawn around each handle of Sg and if these g loops are then cut open,
the result is a (non-closed) surface that can be stretched and ﬂattened out into a subset
of the plane.

Section 8.8
TOPOLOGICAL GRAPH THEORY
655
4. The subscript g equals the maximum number of closed curves on Sg that can be cut
open without disconnecting that surface.
5. The subscript k equals the maximum number of closed curves on Nk that can be cut
open without disconnecting that surface.
6. No closed nonorientable surface can be imbedded in R3.
7. Network layouts:
The surfaces actually used for computer interconnection network
layouts and other practical purposes rarely have graceful curved shapes, because among
other reasons, that would obstruct miniaturization and ease of manufacture. Moreover,
such surfaces usually have holes. However, the classiﬁcation theorem and the closing of
holes reduce the topology of the layout problems to placing graphs on closed surfaces.
Examples:
1. Adding a handle is achieved by punching two holes and connecting them with a tube,
as illustrated.
2. To construct the sequence of all orientable surfaces from the sphere S0, each successive
handle is added at the right of the previous surface.
S0
S1
S2
3. The M¨obius band is constructed by giving a half-twist to a rectangular strip and then
pasting the ends together.
4. To construct the sequence of all nonorientable surfaces from the projective plane N1,
each successive crosscap is added by cutting a hole in the previous surface and capping
it with a M¨obius band.
N2
N3
N1

656
Chapter 8
GRAPH THEORY
8.8.2
DRAWING GRAPHS ON SURFACES
Deﬁnitions:
A ﬂat-polygon representation of a surface S is a drawing of a ﬂat polygon with
markings to match the sides into pairs such that when the sides are pasted together as
the markings indicate, the resulting surface S is obtained. (Certain special ﬂat-polygon
representations are called fundamental polygon representations.) (See Example 1.)
An imbedding (or embedding) of a graph is a drawing with no edge-crossings.
A face of an imbedding means a region plus its boundary. The set of all faces is de-
noted F.
The Euler characteristic of an imbedding of a graph G = (V, E) is the number
|V | −|E| + |F|.
A ﬂat-polygon drawing of a graph on a surface has some graph edges drawn in two
or more segments, so that one segment runs from one endpoint of an edge to a side of
the ﬂat polygon and another segment runs from the other endpoint to the corresponding
position on the matched side of the ﬂat polygon. Sometimes there are also some interior
edge segments running between polygon sides. (Flat-polygon drawings are best used for
small graphs.)
Imbedding modiﬁcation (or “surgery”) on a surface means adding handles and cross-
caps to the surface and then drawing one or more edges that traverse the new handles
and crosscaps.
The (Poincar`e) duality construction (see Example 3) is as follows:
• insert into the interior of each (primal) face f a single dual vertex f ∗;
• through each primal edge e draw a dual edge e∗; if edge e lies on the intersection
of two primal faces f and f ′ (possibly f = f ′), then the dual edge e∗joins the
dual vertices f ∗and f ′∗;
• the dual graph is the graph G∗=
 { f ∗| f ∈F }, { e∗| e ∈E }

;
• the dual imbedding is the resulting imbedding G∗→S.
Facts:
1. Every closed surface has a ﬂat-polygon representation. This makes it possible to
draw a picture in the plane of any graph imbedding in any surface.
2. Euler polyhedral equation for orientable surfaces:
Let G = (V, E) be a connected
graph, cellularly imbedded (see §8.8.3) into the surface Sg with face set F. Then
|V | −|E| + |F| = 2 −2g = χ(Sg).
3. Euler polyhedral equation for nonorientable surfaces: Let G = (V, E) be a connected
graph, cellularly imbedded into the surface Nk with face set F. Then
|V | −|E| + |F| = 2 −k = χ(Nk).
4. Edge-face inequality:
Let G = (V, E) be a simple, connected graph imbedded in a
surface with face set F. Then
2|E| ≥3|F|.

Section 8.8
TOPOLOGICAL GRAPH THEORY
657
5. Edge-face inequality, strong version: Let G = (V, E) be a connected graph, but not
a tree, imbedded in a surface with face set F. Then
2|E| ≥girth(G) · |F|.
6. The most frequently used method for constructing the imbeddings of a recursively
constructable sequence of graphs is with voltage graphs. (See §8.10.3.)
7. For each surface Sg, there is a ﬁnite list Lg of graphs analogous to the Kuratowski
graphs K5 and K3,3. That is, a graph can be an imbedding in Sg if and only if it contains
no subgraph homeomorphic to a graph in Lg. (Robertson and Seymour, 1990)
Examples:
1. Flat-polygon representations of the double-torus and the Klein bottle are illustrated
in the following diagrams.
2
2
1
1
N2
S2
2
2
1
1
4
4
3
3
2. In the imbedding K5 →S1 illustrated below, edges c and d cross through ﬂat-polygon
sides 2 and 1, respectively. The “outer region” is actually 8-sided, with boundary circuit
⟨a, d, b, c, f, d, e, c⟩. Two pairs of sides of this region are pasted together. The appearance
of this single region as four subregions at the corners of the ﬂat polygon is a side-eﬀect
of the particular representation and not a true feature of the imbedding.
a
b
f
c
2
2
1
1
e
d
3. The Poincare duality construction is illustrated in the following diagram.

658
Chapter 8
GRAPH THEORY
8.8.3
COMBINATORIAL REPRESENTATION OF GRAPH IMBEDDINGS
Deﬁnitions:
The rotation (in “edge-format”) at v is obtained from a ﬂat-polygon drawing of a
graph by the following sequence of steps: (a) label one end of each edge + and the other
end −, or put an arrow on each edge so that the head faces toward the + end; and (b) at
each vertex, traverse a small circle centered at that vertex, and record the cyclically
ordered list of edge-ends encountered; this list is the rotation.
The vertex-format of a rotation is obtained by replacing each edge-end in the edge-
format by the vertex at the other end of that edge. The vertex format is used only for
simple graphs.
A rotation system is a complete list of rotations, that is, one for every vertex. If the
surface is orientable, it is assumed that the traversals of the small circles around the
vertices are in a consistent direction, that is, all clockwise or else all counterclockwise.
An imbedding is cellular (or a “2-cell imbedding”) if every region is planar and has
connected boundary.
Facts:
1. Two cellular imbeddings of a graph are equivalent if and only if they have the same
rotation system.
2. If a cellular graph imbedding is represented as a rotation system, then the regions
can be reconstructed algorithmically.
Example:
1. An imbedding K4 →S1 and both formats of its rotation system are shown next.
2
2
1
1
V1
V2
V3
V4
d
e
c
b
a
f
edge - format
vertex - format
V1. a - b - c -
V2. a + d - e -
V3. c + f + e +
V4. b + f - d +
V1.
V2.
V3.
V4.
V2
V1
V1
V1
V4
V4
V4
V3
V3
V3
V2
V2
8.8.4
GENUS AND CROSSCAP NUMBER
Deﬁnitions:
The minimum genus (often, simply genus) γmin(G) of a connected graph G is the
minimum integer g such that there is an imbedding of G into the surface Sg.
The maximum genus γmax(G) of a connected graph G is the maximum integer g such
that there is a cellular imbedding of G into the surface Sg.

Section 8.9
ENUMERATING GRAPHS
659
The minimum crosscap number (often, simply crosscap number) γmin(G) is the
minimum integer k such that there is an imbedding of G into Nk. Thus, a planar graph
has minimum crosscap number zero.
The maximum crosscap number γmax(G) is the maximum integer k such that there is
a cellular imbedding of G into Nk. Thus, a planar graph has maximum crosscap number
zero.
Facts:
1. The genus of any planar graph is 0.
2. γmin(G) ≥|E| −3|V | + 6
6
if G is simple.
3. γmin(G) ≥|E| −2|V | + 4
4
if G is simple and bipartite.
4. γmin(Kn) =
(n −3)(n −4)
12

. (Ringel and Youngs, 1968)
5. γmin(Km,n) =
(m −2)(n −2)
4

. (Ringel, 1965)
6. γmin(Qn) =
(m −2)(n −2)
4

. (Ringel, 1955)
7. γmin(G) ≥
|E| −3|V | + 6
3

for every simple graph G.
8. γmin(G) ≥
|E| −2|V | + 4
2

for every simple bipartite graph G.
9. γmin(Kn) =
(n −3)(n −4)
6

, except that γmin(K7) = 3. (Ringel, 1959)
10. Many genus and crosscap number formulas can be derived by using voltage graphs
or current graphs (see §8.10.4). [GrTu12]
11. For any genus g such that γmin(G) ≤g ≤γmax(G), there is at least one imbedding
of the graph G in the surface Sg.
12. For any crosscap number k such that γmin(G) ≤k ≤γmax(G), there is at least one
imbedding of the graph G in the surface Nk.
13. The problem of calculating the minimum genus of a graph is NP-hard. (Thomassen,
1989)
14. The problem of calculating the maximum genus of a graph is solvable in polynomial
time. (Furst, Gross, and McGeoch, 1988)
8.9
ENUMERATING GRAPHS
It is often important to know how many graphs there are with some desired property;
for example, this is used in computer science to analyze the requirements of algorithms,
and in chemistry to catalogue chemical molecules with various shapes.
Many of the
techniques for counting graphs are based on the master theorem in the historic 1937

660
Chapter 8
GRAPH THEORY
work of George P´olya [P´oRe87]. Frank Harary and others exploited this master the-
orem in counting graphs, multigraphs, digraphs, and similar graphical structures. An
exhaustive survey of results in graph enumeration can be found in [HaPa73]. Alterna-
tively, if you know the ﬁrst few terms of a graph-counting sequence, you can likely ﬁnd
more terms, references, and further information in the On-Line Encyclopedia of Integer
Sequences [OEIS].
8.9.1
COUNTING LABELED GRAPHS AND MULTIGRAPHS
When counting graphs, it is important to distinguish between the enumeration of labeled
graphs and that of unlabeled graphs. Labeled graphs are relatively easy to count, usually
requiring only factorials, exponentials, and binomial coeﬃcients.
Deﬁnitions:
A labeled graph or labeled multigraph is a graph or multigraph with standard labels
(commonly v1, v2, . . . , vn) assigned to the vertices. Two labeled graphs or multigraphs
with the same set of labels are considered the same only if there is an isomorphism from
one to the other that preserves the labels.
Examples:
1. The following ﬁgure shows the three isomorphically distinct graphs with 4 vertices
and 3 edges. There are 4 essentially diﬀerent ways to label each of the ﬁrst two and 12
ways to label the third. Thus there are 20 diﬀerent labeled graphs with 4 vertices and 3
edges.
2. The next ﬁgure shows the three isomorphically distinct loopless multigraphs that
together with the graphs in the previous ﬁgure form the six diﬀerent multigraphs with
4 vertices and 3 edges. There are 6 essentially diﬀerent ways to label the ﬁrst and third
graphs in this ﬁgure and 24 ways to label the middle graph. Thus the multigraphs in
these two ﬁgures represent the total of 56 labeled loopless multigraphs with 4 vertices
and 3 edges.
Facts:
1. The number of labeled graphs with n vertices and m edges is the binomial coeﬃcient
 n
2

m

. These numbers form sequence A084546 in [OEIS]. See Table 1.

Section 8.9
ENUMERATING GRAPHS
661
Table 1: Labeled graphs with n vertices and m edges.
m

n
1
2
3
4
5
6
7
8
0
1
1
1
1
1
1
1
1
1
1
3
6
10
15
21
28
2
3
15
45
105
210
378
3
1
20
120
455
1,330
3,276
4
15
210
1,365
5,985
20,475
5
6
252
3,003
20,349
98,280
6
1
210
5,005
54,264
376,740
7
120
6,435
116,280
1,184,040
8
45
6,435
203,490
3,108,105
9
10
5,005
293,930
6,906,900
10
1
3,003
352,716
13,123,110
11
1,365
352,716
21,474,180
12
455
293,930
30,421,755
13
105
203,490
37,442,160
14
15
116,280
40,116,600
total
1
2
8
64
1,024
32,768
2,097,152
268,435,456
2. For m >
 n
2

/2, the number of labeled graphs with n vertices and m edges is the
same as the number of labeled graphs with n vertices and
 n
2

−m edges.
3. The total number of labeled graphs with n vertices is 2(
n
2). This is sequence A006125
in [OEIS]. See Table 1.
4. The number Cn of connected labeled graphs with n vertices can be determined from
the following recurrence system.
C1 = 1,
and
Cn = 2
 n
2

−1
n
n−1
X
k=1
k
 n
k

2
 n−k
2

Ck
for n > 1.
This is sequence A001187 in [OEIS]. See Table 2.
Table 2: Connected labeled graphs with n vertices.
n
1
2
3
4
5
6
7
8
Cn
1
1
4
38
728
26,704
1,866,256
251,548,592
5. Asymptotically, most labeled graphs are connected. Thus the sequence Cn satisﬁes
Cn ∼2(n
2).
6. The number of labeled loopless multigraphs with n vertices and m edges is the bi-
nomial coeﬃcient
 m+
 n
2

−1
m

. When n = 1, this expression should be interpreted as 1
when m = 0 and 0 otherwise. See Table 3. These numbers form the sequence A098568
in [OEIS].

662
Chapter 8
GRAPH THEORY
Table 3: Labeled loopless multigraphs with n vertices and m edges.
m

n
1
2
3
4
5
6
7
8
0
1
1
1
1
1
1
1
1
1
1
3
6
10
15
21
28
2
1
6
21
55
120
231
406
3
1
10
56
220
680
1,771
4,060
4
1
15
126
715
3,060
10,626
31,465
5
1
21
252
2,002
11,628
53,130
201,376
6
1
28
462
5,005
38,760
230,230
1,107,568
8.9.2
COUNTING UNLABELED GRAPHS AND MULTIGRAPHS
Unlike the enumeration of labeled graphs, the enumeration of unlabeled graphs requires
rather sophisticated counting techniques, often utilizing permutation group theory and
generating functions.
Deﬁnitions:
The symmetric group Sn is the group of all n! permutations γ acting on the set
Xn = {1, 2, . . ., n}.
The order of a permutation group is the number of permutations it contains. The degree
of a permutation group is the number of objects being permuted. The symmetric group
Sn has order n! and degree n.
The cycle index Z(G) of a permutation group G of order m and degree d is a polynomial
in variables a1, a2, . . . , ad given by the formula
Z(G) = 1
m
X
γ∈G
d
Y
k=1
ajk(γ)
k
,
where jk(γ) is the number of cycles of length k in the permutation γ. For example, for
G = S3 = {(1)(2)(3), (123), (132), (1)(23), (2)(13), (3)(12)}, the symmetric group of
order 6 and degree 3, the cycle index is Z(G3) = 1
6
 a3
1 + 2a3 + 3a1a2

.
The pair permutation γ(2) induced by the permutation γ acting on the set Xn is the
permutation acting on unordered pairs of distinct elements of Xn deﬁned by the rule
γ(2) ({x1, x2}) = {γ(x1), γ(x2)} .
The symmetric pair group S(2)
n
induced by the symmetric group Sn is the permutation
group

γ(2)  γ ∈Sn
	
. This group has order n! and degree n(n −1)/2.
Facts:
1. The cycle index Z(S(2)
n ) of the symmetric pair group, used in counting graphs with
n vertices, is
Z
 S(2)
n

= 1
n!
X
(j)
n!
Q
k
kjkjk!
Y
k
a
k(
jk
2 )
k
(akak−1
2k )j2kakj2k+1
2k+1
Y
r<s
agcd(r,s)jrjs
lcm(r,s)
,

Section 8.9
ENUMERATING GRAPHS
663
where lcm(r, s) and gcd(r, s) are the least common multiple and greatest common divisor
of r and s, respectively. The sum is taken over all partitions (j) = j1, j2, . . . , jn of the
integer n as an unordered sum of parts, where jk is the number of parts of size k. For
example, in the partition of 7 as 2 + 2 + 3 we have j2 = 2, j3 = 1, and j1 = j4 = j5 =
j6 = j7 = 0. Explicit formulas for Z(S(2)
n ) for small values of n are
Z
 S(2)
1

= 1
Z
 S(2)
2

= a1
Z
 S(2)
3

= 1
3!
 a3
1 + 3a1a2 + 2a3

Z
 S(2)
4

= 1
4!
 a6
1 + 9a2
1a2
2 + 8a2
3 + 6a2a4

Z
 S(2)
5

= 1
5!
 a10
1 + 10a4
1a3
2 + 20a1a3
3 + 15a2
1a4
2 + 30a2a2
4 + 20a1a3a6 + 24a2
5

Z
 S(2)
6

= 1
6!
 a15
1 + 15a7
1a4
2 + 40a3
1a4
3 + 60a3
1a6
2 + 180a1a2a3
4 + 120a1a2a2
3a6
+ 144a3
5 + 40a5
3 + 120a3a2
6

2. Let Gn,m denote the number of graphs with n vertices and m edges, and let gn(x)
be the generating function for n-vertex graphs, so that
gn(x) =
 n
2

X
m=0
Gn,mxm.
P´olya’s enumeration theorem states that this generating function gn(x) can be obtained
from the cycle index Z(S(2)
n ) by replacing each variable ai with 1 + xi. See Table 4.
These numbers form sequence A008406 in [OEIS].
3. For m >
 n
2

/2, the number of graphs with n vertices and m edges is the same as the
number of graphs with n vertices and
 n
2

−m edges.
4. The total number Gn of graphs with n vertices is obtained from the cycle index
Z(S(2)
n ) by replacing each variable ai with the number 2. See Table 4. This is sequence
A000088 in [OEIS].
5. Asymptotically, the sequence Gn satisﬁes Gn ∼2(n
2)/n!.
Table 4: Graphs with n vertices and m edges.
m

n
1
2
3
4
5
6
7
8
0
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
1
2
2
2
2
2
3
1
3
4
5
5
5
4
2
6
9
10
11
5
1
6
15
21
24
6
1
6
21
41
56
7
4
24
65
115

664
Chapter 8
GRAPH THEORY
m

n
1
2
3
4
5
6
7
8
8
2
24
97
221
9
1
21
131
402
10
1
15
148
663
11
9
148
980
12
5
131
1,312
13
2
97
1,557
14
1
65
1,646
Total
1
2
4
11
34
156
1,044
12,346
6. The enumeration of connected graphs requires an auxiliary sequence An deﬁned re-
cursively by
A1 = 1,
and
An = nGn −
n−1
X
k=1
Ak · Gn−k
for n > 1.
This sequence 1, 3 ,7, 27, 106, 681, 5972, 88963, . . . is sequence A003083 in [OEIS]. The
number Kn of connected graphs with n vertices can then be computed as
Kn = 1
n
X
d|n
µ(d)An/d,
where the sum is over all divisors of n and µ is the M¨obius function deﬁned by
µ(n) =





1
if n = 0
0
if m2|n for some m > 1
(−1)k
if n is the product of k distinct primes.
See Table 5. The sequence Kn is sequence A001349 in [OEIS].
Table 5: Connected graphs with n vertices.
n
1
2
3
4
5
6
7
8
Kn
1
1
2
6
21
112
853
11,117
7. Asymptotically, most graphs are connected. Thus the sequence Kn satisﬁes Kn ∼
2(n
2)/n!.
8. Let Mn,k denote the number of loopless multigraphs with n vertices and k edges, and
let mn(x) be the generating function for n-vertex loopless multigraphs, so that
mn(x) =
 n
2

X
m=0
Mn,kxk.
P´olya’s enumeration theorem states that this generating function mn(x) can be obtained
from the cycle index Z(S(2)
n ) by replacing each variable ai with the inﬁnite series 1+xi +
x2i + x3i + · · · . See Table 6. These numbers form sequence A192517 in [OEIS]. (Also,
column n = 3 is sequence A001399, column n = 4 is sequence A003082, column n = 5 is
sequence A014395, and column n = 6 is sequence A014396.)

Section 8.9
ENUMERATING GRAPHS
665
Table 6: Loopless multigraphs with n vertices and m edges.
m

n
1
2
3
4
5
6
0
1
1
1
1
1
1
1
1
1
1
1
1
2
1
2
3
3
3
3
1
3
6
7
8
4
1
4
11
17
21
5
1
5
18
35
52
6
1
7
32
76
132
7
1
8
48
149
313
8
1
10
75
291
741
9
1
12
111
539
1,684
10
1
14
160
974
3,711
8.9.3
COUNTING LABELED DIGRAPHS AND TOURNAMENTS
Deﬁnitions:
A labeled digraph is a digraph with distinct labels, typically v1, v2, . . . , vn, assigned to
its vertices. Two labeled digraphs with the same set of labels are considered the same
only if there is an isomorphism from one to the other that preserves the labels.
A tournament (or round-robin tournament) is a digraph in which, for each pair u,
v of distinct vertices, either there exists an arc from u to v or an arc from v to u but not
both.
A digraph is strong (or strongly connected) if for each pair u, v of vertices, there
exist directed paths from u to v and from v to u. A strong tournament is also called an
irreducible tournament.
Examples:
1. The following ﬁgure shows the four isomorphically distinct digraphs with 3 vertices
and 3 arcs. The last two are tournaments. There are 6 essentially diﬀerent ways to
label each of the ﬁrst three digraphs and 2 ways to label the fourth. Thus there are 20
diﬀerent labeled digraphs with 3 vertices and 3 arcs. Only the last digraph is strong—an
irreducible tournament.
2. The next ﬁgure shows the four isomorphically distinct tournaments with 4 vertices.
There are 24 essentially diﬀerent ways to label the ﬁrst and last tournaments, and 8 ways
to label each of the middle two. Thus there are 64 diﬀerent labeled tournaments with 4
vertices. Only the last tournament is strong.

666
Chapter 8
GRAPH THEORY
Facts:
1. The number of labeled digraphs with n vertices and m arcs is the binomial coeﬃcient
 n(n−1)
m

. See Table 7. These numbers form sequence A123554 in [OEIS].
Table 7: Labeled digraphs with n vertices and m arcs.
m

n
1
2
3
4
5
0
1
1
1
1
1
1
2
6
12
20
2
1
15
66
190
3
20
220
1,140
4
15
495
4,845
5
6
792
15,504
6
1
924
38,760
7
792
77,520
8
495
125,970
9
220
167,960
10
66
184,756
Total
1
4
64
4,096
1,048,576
2. For m > n(n −1)/2, the number of labeled digraphs with n vertices and m arcs is
the same as the number of labeled digraphs with n vertices and n(n −1) −m arcs.
3. The total number of labeled digraphs with n vertices is 2n(n−1). See Table 7. This is
sequence A053763 in [OEIS].
4. The number of labeled tournaments with n vertices is 2(n
2), the same as the number
of labeled graphs with n vertices. See Table 8. This is sequence A006125 in [OEIS].
5. The number c
Sn of strong labeled tournaments with n vertices can be computed from
the recursive formula
c
S1 = 1,
and
c
Sn = 2(n
2) −
n−1
X
k=1
n
k

2(n−k
2 )c
Sk
for n > 1.
See Table 8. This is sequence A054946 in [OEIS].
Table 8: Labeled tournaments and strong tournaments with n vertices.
n
Labeled Tournaments
Strong Labeled Tournaments
1
1
1
2
2
0
3
8
2
4
64
24
5
1,024
544
6
32,768
22,320
7
2,097,152
1,677,488
8
268,435,456
236,522,496

Section 8.9
ENUMERATING GRAPHS
667
6. Asymptotically, most labeled tournaments are strong. Thus the sequence c
Sn counting
strong labeled tournaments satisﬁes c
Sn ∼2(
n
2).
8.9.4
COUNTING UNLABELED DIGRAPHS AND TOURNAMENTS
As with unlabeled graphs, the enumeration of unlabeled digraphs and tournaments re-
quires rather sophisticated counting techniques, often utilizing permutation group theory
and generating functions. (See §8.9.2.)
Deﬁnitions:
The ordered pair permutation γ[2] induced by the permutation γ acting on the set
Xn = {1, 2, . . . , n} is the permutation acting on ordered pairs of distinct elements of Xn
deﬁned by the rule
γ[2] ((x1, x2)) = (γ(x1), γ(x2)) .
The reduced ordered pair group S[2]
n
induced by the symmetric group Sn is the
permutation group

γ[2]  γ ∈Sn
	
. This group has order n! and degree n(n −1).
Facts:
1. The cycle index Z(S[2]
n ) of the reduced ordered pair group, used in counting digraphs
with n vertices, is
Z(S[2]
n ) = 1
n!
X
(j)
n!
Q
k
kjkjk!
Y
k
a
(k−1)jk+2k(
jk
2 )
k
Y
r<s
a2 gcd(r,s)jrjs
lcm(r,s)
,
where lcm(r, s) and gcd(r, s) are the least common multiple and greatest common divisor
of r and s, respectively. The sum is taken over all partitions (j) = j1, j2, . . . , jn of the
integer n as an unordered sum of parts, where jk is the number of parts of size k. (See §2.5
for a discussion of partitions.) Explicit formulas for Z(S[2]
n ) for small values of n are
Z
 S[2]
1

= 1
Z
 S[2]
2

= 1
2!
 a2
1 + a2

Z
 S[2]
3

= 1
3!
 a6
1 + 3a3
2 + 2a2
3

Z
 S[2]
4

= 1
4!
 a12
1 + 6a2
1a5
2 + 8a4
3 + 3a6
2 + 6a3
4

Z
 S[2]
5

= 1
5!
 a20
1 + 10a6
1a7
2 + 20a2
1a6
3 + 15a10
2 + 30a5
4 + 20a2a2
3a2
6 + 24a4
5

Z
 S[2]
6

= 1
6!
 a30
1 + 15a12
1 a9
2 + 40a6
1a8
3 + 45a2
1a14
2 + 90a2
1a7
4 + 120a3
2a4
3a2
6
+ 144a6
5 + 15a15
2 + 90a2a7
4 + 40a10
3 + 120a5
6

.
2. Let Dn,m denote the number of digraphs with n vertices and m arcs, and let dn(x)
be the generating function for n-vertex digraphs, so that
dn(x) =
n(n−1)
X
m=0
Dn,mxm.

668
Chapter 8
GRAPH THEORY
P¨olya’s enumeration theorem states that this generating function dn(x) can be obtained
from the cycle index Z(S[2]
n ) by replacing each variable ai with 1+xi. See Table 9. These
numbers form sequence A052283 in [OEIS].
Table 9: Digraphs with n vertices and m arcs.
m

n
1
2
3
4
5
0
1
1
1
1
1
1
1
1
1
1
2
1
4
5
5
3
4
13
16
4
4
27
61
5
1
38
154
6
1
48
379
7
38
707
8
27
1,155
9
13
1,490
10
5
1,670
Total
1
3
16
218
9,608
3. For m > n(n −1)/2, the number of digraphs with n vertices and m arcs is the same
as the number of digraphs with n vertices and n(n −1) −m arcs.
4. The total number Dn of digraphs with n vertices is obtained from the cycle index
Z(S[2]
n ) by replacing each variable ai with the number 2. See Table 9. This is sequence
A000273 in [OEIS].
5. Asymptotically, the sequence Dn satisﬁes Dn ∼2n(n−1)/n!.
6. The number Tn of tournaments with n vertices is given by the formula
Tn = 1
n!
X′
(j)
n!
Q
k
kjkjk!2D(j),
where the sum is over all partitions (j) of n into odd size parts, and where
D(j) = 1
2
 n
X
r=1
n
X
s=1
gcd(r, s)jrjs −
n
X
k=1
jk
!
.
See Table 10. This is sequence A000568 in [OEIS].
7. The number Sn of strong tournaments with n vertices can be determined by the
recurrence relation
S1 = 1,
and
Sn = Tn −
n−1
X
k=1
Tn−kSk
for n > 1,
where Tn is the number of tournaments from Fact 6 above. See Table 10. Note that
there are no strong tournaments with exactly two vertices. This is sequence A051337
in [OEIS].
8. The sequences Tn and Sn satisfy Tn ∼Sn ∼2(n
2)/n!.

Section 8.10
GRAPH FAMILIES
669
Table 10: Tournaments and strong tournaments with n vertices.
n
Tournaments
Strong Tournaments
1
1
1
2
1
0
3
2
1
4
4
1
5
12
6
6
56
35
7
456
353
8
6,880
6,008
9
191,536
178,133
10
9,733,056
9,355,949
11
903,753,248
884,464,590
12
154,108,311,168
152,310,149,735
8.10
GRAPH FAMILIES
This section discusses some graph families and the theory behind them: perfect graphs,
eigenvalues, algebraically-deﬁned graphs, expander graphs, and product graphs.
8.10.1
PERFECT GRAPHS
The class of perfect graphs was introduced by Claude Berge in 1961. Around the same
time Berge made two conjectures about this class of graphs that motivated a great deal
of research in the ﬁeld. Both conjectures have by now been solved, and are known as the
Weak and Strong Perfect Graph Theorems. Below is a brief discussion of perfect graphs
and of some recent developments on the topic. For more information see the appropriate
section in [GrYeZh15].
Deﬁnitions:
An induced subgraph of G is a subgraph H such that every edge of G that joins two
vertices of H is also an edge of H.
A graph G is called perfect if for every induced subgraph H of G, χ(H) = ω(H) (where
χ denotes the chromatic number and ω the clique number; see §8.6.3).
A hole in a graph is an induced subgraph that is isomorphic to the cycle Ck with k ≥4,
and k is the length of the hole. A hole is odd if k is odd, and even otherwise.
An antihole in a graph is an induced subgraph whose complement is isomorphic to Ck
with k ≥4, and k is the length of the antihole. An antihole is odd if k is odd, and
even otherwise.
A graph is called Berge if it has no odd hole and no odd antihole.

670
Chapter 8
GRAPH THEORY
A Berge graph G is called basic if: (a) G or its complement is bipartite; (b) G or its
complement is the line graph of a bipartite graph; or (c) G is a double split graph.
An even pair is a pair of vertices {u, v} such that every induced path from u to v has
even length (in particular, v and v are nonadjacent). The operation of contracting
even pair {u, v} means replacing {u, v} by a vertex w such that w is adjacent precisely
to all vertices that were adjacent to at least one of u or v.
A graph G is called even contractile if one can repeatedly contract an even pair and
end up with the complete graph on ω(G) vertices.
A prism is an induced subgraph consisting of two triangles {a1, a2, a3} and {b1, b2, b3}
and three disjoint paths P1, P2, P3, where Pi is from ai to bi, and for 1 ≤i < j ≤3 the
only edges between V (Pi) and V (Pj) are aiaj and bibj. A prism is odd if P1, P2, P3 all
have odd length, and even if P1, P2, P3 all have even length.
Results:
1. An odd hole has ω = 2 and χ = 3; an odd antihole of length k has ω = (k −1)/2 and
χ = (k + 1)/2. Therefore a perfect graph cannot contain either.
2. Weak Perfect Graph Theorem: A graph is perfect if and only if its complement is
perfect. [Lo72]
3. Strong Perfect Graph Theorem: A graph is perfect if and only if it has no odd hole
and no odd antihole. [ChEtal06]
4. The strong theorem implies the weak.
5. There is a polynomial-time algorithm to test if an input graph is perfect.
6. If a graph is restricted to being perfect, then some algorithmic problems, such as
calculating the size of the largest clique or ﬁnding an optimal coloring, that are NP-
complete in general, can be solved in polynomial time.
7. If G is Berge and G′ is formed by contracting an even pair, then G′ is Berge and
ω(G′) = ω(G).
8. If a prism is Berge, then either it is an odd prism or an even prism.
9. It is conjectured that if a Berge graph has no odd prism and no antihole of length at
least six, then it is even contractile.
10. It is known that if a Berge graph has no prism and no antihole of length at least
six, then it is even contractile. Also, if a Berge graph has no odd prism and no hole of
length four, then it has an even pair.
8.10.2
SPECTRAL GRAPH THEORY
In this subsection we consider only undirected graphs.
Deﬁnitions:
The characteristic polynomial of a graph G is the characteristic polynomial p(x) of
its adjacency matrix AG; that is, p(x) = det(xI −AG).
An eigenvalue (or characteristic value) of a matrix A is a number λ such that
Ax = λx, for some nonzero vector x; the vector x is an eigenvector (or characteristic
vector).

Section 8.10
GRAPH FAMILIES
671
An eigenvector of a graph is an eigenvector of its adjacency matrix, and an eigenvalue
of a graph an eigenvalue of its adjacency matrix.
The spectrum of a graph is the spectrum of its adjacency matrix; that is, the multiset
of eigenvalues.
The Laplacian (or admittance matrix) of a graph G is the matrix DG −AG,
where DG is the diagonal matrix with the degree sequence of G on the diagonal and AG
is the adjacency matrix.
A connected graph G is strongly regular with parameters (n, k, r, s) if
• |VG| = n;
• G is k-regular, with k > 0;
• every pair of adjacent vertices is mutually adjacent to r other vertices;
• every pair of nonadjacent vertices is mutually adjacent to s other vertices.
The Hoﬀman polynomial of a graph is a polynomial p(x) of minimum degree such
that p(AG) = J, where AG is the adjacency matrix and J is the square matrix with every
entry equal to 1.
A cospectral pair of graphs is a pair of nonisomorphic graphs that have the same
spectrum.
Facts:
1. The eigenvalues of a graph are independent of the particular labeling of the vertices;
thus, two isomorphic graphs have the same spectrum.
2. All the eigenvalues of a graph are real. This is a special case of the well-known linear
algebra result that the eigenvalues of any Hermitian matrix are real.
3. From linear algebra, it follows that the characteristic polynomial of a graph G satisﬁes
the equation p(x) = Qn
i=1(x −λi), where λ1, . . . , λn are the eigenvalues of G.
4. If a graph is connected, then its largest eigenvalue has multiplicity 1. This eigen-
value has a corresponding eigenvector with all positive entries, which is the only such
eigenvector.
5. If λ is the largest eigenvalue of a graph and µ is another eigenvalue, then λ ≥|µ|;
moreover, −λ is an eigenvalue if and only if the graph is bipartite.
6. A graph is bipartite if and only if its spectrum is symmetric with respect to 0; that
is, λ is an eigenvalue if and only if −λ is also an eigenvalue.
7. The largest eigenvalue of a k-regular graph is k, and it has multiplicity equal to
the number of connected components. The sum of the coordinates of an eigenvector
corresponding to any other eigenvalue is 0.
8. The (i, j)th entry of the kth power Ak
G of the adjacency matrix of a graph G is the
number of walks of length k starting at vertex vi and terminating at vj.
9. If λ1, λ2, . . . , λn are the eigenvalues of a graph G, then Pn
i=1 λ2
i = 2|EG| where EG
is the edge-set of G. Also, Pn
i=1 λ3
i = 6T where T is the number of triangles in G.
10. If p(x) = xn +an−1xn−1 +an−2xn−2 +· · ·+a1x+a0 is the characteristic polynomial
of a graph G, then an−1 = 0, −an−2 is the number of edges, and −an−3 is twice the
number of triangles.
11. The set of eigenvalues of the disjoint sum G+H is the union of the sets of eigenvalues
of G and H. The multiplicity of λ as an eigenvalue of G+H is the sum of the multiplicity
of λ as an eigenvalue of G and the multiplicity of λ as an eigenvalue of H.

672
Chapter 8
GRAPH THEORY
12. The eigenvalues of the Cartesian product G □H are {λi +λj | λi an eigenvalue of G
and λj an eigenvalue of H }. The multiplicity of λi + λj as an eigenvalue of G □H is the
product of the multiplicity of λi as an eigenvalue of G and λj as an eigenvalue of H.
13. If G is a k-regular graph and G its complement, then λ < k is an eigenvalue of G
if and only if −λ −1 is an eigenvalue of G. In this case λ and −λ −1 have the same
multiplicities.
14. If λ is an eigenvalue of G with multiplicity m, then −λ −1 is an eigenvalue of G
with multiplicity m −1, m, or m + 1.
15. If G has n vertices and λ1 ≥λ2 ≥· · · ≥λn as eigenvalues, and H is an induced
subgraph with n −1 vertices and eigenvalues µ1 ≥µ2 ≥· · · ≥µn−1, then λ1 ≥µ1 ≥
λ2 ≥µ2 ≥· · · ≥µn−1 ≥λn. (See §6.5.5.)
16. The eigenvalues of a line graph L(G) are greater than or equal to −2. Equality is
attained unless every connected component of G is a tree or has exactly one circuit, that
circuit being odd.
17. If G is a k-regular graph with λ > −k as an eigenvalue, then λ+k−2 is an eigenvalue
of L(G).
18. A graph has a Hoﬀman polynomial if and only if it is regular and connected.
19. A regular connected graph has exactly three distinct eigenvalues if and only if it is
strongly regular.
20. Matrix-tree theorem: If Mi is formed by deleting the ith row and column from the
Laplacian of G, then det(Mi) is independent of the choice of i and is equal to the number
of spanning trees of G.
Examples:
1. The edgeless graph on n vertices has one eigenvalue, namely 0 with multiplicity n.
2. The eigenvalues of the complete graph Kn are n −1 and −1 with respective multi-
plicities of 1 and n −1. For instance, the characteristic polynomial of K4 is

x
−1
−1
−1
−1
x
−1
−1
−1
−1
x
−1
−1
−1
−1
x

= (x + 1)3 (x −3) .
3. The eigenvalues of the complete bipartite graph Km,n are √mn, 0, and −√mn, with
respective multiplicities of 1, mn −2, and 1.
4. The eigenvalues of the Petersen graph are 3, 1, and −2, with respective multiplici-
ties 1, 5, and 4.
5. The eigenvalues of the n-path Pn are {2 cos kπ
n+1 | k = 1, 2, . . . , n}, each with multi-
plicity 1.
6. The eigenvalues of the n-cycle Cn, are {2 cos 2kπ
n
| k = 1, 2, . . . , n}. The eigenvalue 2,
and the eigenvalue −2 when n is even, have multiplicity 1; all other eigenvalues have
multiplicity 2.
7. The eigenvalues of the hypercube Qd are d, d−2, d−4, . . ., −d+2, −d, with respective
multiplicities
 d
0

,
 d
1

,
 d
2

, . . . ,
  d
d−1

,
 d
d

.
8. The eigenvalues of the line graph L(Kn) are 2n −4, n −4, and −2, with respective
multiplicities 1, n −1, and n(n−3)
2
.

Section 8.10
GRAPH FAMILIES
673
9. The eigenvalues of the line graph L(Km,n) are m + n −2, m −2, n −2, and −2, with
respective multiplicities 1, n −1, m −1, and (m −1)(n −1).
10. If G is strongly regular with parameters (n, k, r, s), then its eigenvalues are k and
1
2
 r −s ±
p
(r −s)2 −4(s −k)

.
11. The smallest pair of cospectral graphs is K1,4 and C4 + K1, both with spectrum
{−2, 0, 0, 0, 2}. See the following ﬁgure. Observe that K1,4 is connected and that C4 +
K1 is not, and that the two graphs have diﬀerent degree sequences. This implies that
connectedness and degree sequences cannot be determined from spectral properties alone.
K1,4
C4 + K1
8.10.3
ALGEBRAIC GRAPHS AND AUTOMORPHISMS
Deﬁnitions:
A generating subset for a group Γ is a subset Σ of group elements such that every
group element is a product of elements of Σ. (Note: The group identity is the empty
product.)
The Cayley digraph for group Γ and generating set Σ has as vertices the elements
of Γ, with an arc σγ from the vertex γ to the vertex γ′ if and only if γσ = γ′.
If Γ is a group and Σ a symmetric set (meaning γ ∈Γ if and only if γ−1 ∈Γ), then
the undirected Cayley graph for Γ and Σ is the graph obtained by removing all arc
directions from the Cayley digraph, and by collapsing each opposite pairs of arcs to a
single edge.
The Schreier graph [Schreier digraph] for a group A with generating set x1, . . . , xr
and subgroup B has as its vertices the cosets of B in A. For each coset Bb and each
generator xj, there is an edge between vertices Bb and Bbxj [from Bb to Bbxj].
An algebraic speciﬁcation of a graph is a generalization of Cayley graphs and Schreier
graphs. It uses elements of a group as all or part of the names of the vertices and edges,
and the group operation in the incidence rule.
A voltage graph is a form of algebraic speciﬁcation in which the vertices and edges are
speciﬁed as a set of one or more symbols with subscripts, ranging over group elements.
Its usual form is a digraph with vertex labels and edge labels.
The automorphism group Aut(G) of a graph G is the set of all automorphisms of
graph G, under the operation of functional composition. (See §8.5.1.)
A graph is vertex-transitive if for all vertices u and v there is an automorphism that
maps u to v.
Facts:
1. Every Cayley graph is regular and vertex-transitive.
2. Every Cayley graph [digraph] is a Schreier graph [digraph].

674
Chapter 8
GRAPH THEORY
3. A simple graph G and its edge-complement G have the same automorphism group.
4. An automorphism ϕ of a graph G induces an automorphism ϕ on the line graph
L(G).
5. If G is a connected simple graph with at least 4 vertices, then G and its line
graph L(G) have isomorphic automorphism groups.
6. If the G graph has adjacency matrix A, and if the permutation ϕ of VG has per-
mutation matrix P, then ϕ is the vertex map of an automorphism of G if and only if
PA = AP.
7. If all eigenvalues of a graph G have multiplicity 1, then every automorphism has
order at most 2.
8. Frucht’s theorem:
Let Γ be any ﬁnite group. Then there exists a graph G whose
automorphism group is isomorphic to Γ. It can be constructed by modifying a Cayley
digraph for Γ.
9. Every regular graph of even degree can be speciﬁed as a Schreier graph. (J. Gross,
1977)
10. The graph speciﬁed by a voltage graph is topologically a covering space of the
voltage graph. (J. Gross, 1974)
Examples:
1. The n-vertex edgeless graph and the complete graph Kn have the symmetric group Sn
as their automorphism group. They are the only n-vertex graphs with this automorphism
group.
2. The automorphism group of the complete bipartite graph Km,n is Sn × Sm if n ̸= m
and is the wreath product [Ro99] Sn ≀S2 if m = n.
3. The automorphism group of the n-path Pn (with n > 1) is S2.
4. The automorphism group of the n-cycle Cn is the dihedral group Dn of order 2n. For
instance, the 4-cycle C4 with vertices a, b, c, and d (in cyclic order), has the following
vertex automorphisms:
(a)(b)(c)(d)
(a b c d)
(a c)(b d)
(a d c b)
(a b)(c d)
(a d)(b c)
(a)(c)(b d)
(b)(d)(a c).
5. The Cayley digraph of the group S3 with generating set {(1 2 3), (1 2)} is illustrated
in the following ﬁgure.
(1)(2)(3)
(1 2)(3)
(1 3)(2)
(1)(2 3)
(1 3 2)
(1 2  3)
6. The Petersen graph is an example of a graph that is vertex-transitive but not a Cayley
graph. It has the following algebraic speciﬁcation:
V = {vj, wj | j = 0, 1, 2, 3, 4};
E = {xj (uj →u(j+1) mod 5), yj (vj →v(j+2) mod 5), zj (uj →vj) | j = 0, 1, 2, 3, 4}.
With appropriate labeling, it corresponds to the voltage graph on the right.

Section 8.10
GRAPH FAMILIES
675
u0
u1
u2
u3
u4
v0
v1
v2
v3
v4
x0
x1
x2
x3
x4
y0
y1
y2
y3
y4
z0
z1
z2
z3
z4
0
z
u
v
2
1
x
y
8.10.4
EXPANDER GRAPHS
Expander graphs are sparse graphs where every small set of vertices is well-connected to
the rest of the graph. For more information on expander graphs, and for proofs of facts
stated below, see [HoLiWi06], [KrSh11], [Lu94], and the references contained within. All
graphs in this subsection are ﬁnite and undirected.
Deﬁnitions:
If F is a subset of vertices in a graph, the boundary of F, denoted by ∂F, is the set of
edges that have one endpoint inside F and one endpoint outside F.
The isoperimetric constant of graph X with vertex set V is deﬁned as
h(X) = min
|∂F|
|F|
| F ⊆V, 0 < |F| ≤|V |/2

.
It is also known as the Cheeger constant or expansion constant of X.
An expander family is a sequence (Xn) of graphs, all of the same regularity and whose
orders grow to inﬁnity, for which there exists ǫ > 0 such that h(Xn) ≥ǫ for all n.
The eigenvalues of graph G are the eigenvalues of its adjacency matrix AG. (See §8.10.2.)
Since AG is symmetric, all its eigenvalues are real. We use the convention that λ1(X) ≥
λ2(X) ≥· · · ≥λn(X).
If X is a d-regular graph, then the spectral gap of X is d −λ2(X).
If X is a regular graph of order n, we deﬁne λ(X) as max{|λ2(X)|, |λn(X)|} if X is not
bipartite, and max{|λ2(X)|, |λn−1(X)|} if X is bipartite.
A d-regular graph X is Ramanujan if λ(X) ≤2
√
d −1.
Let G be a ﬁnite group and let Γ be a subset of G. Given a ﬁnite-dimensional unitary
representation π: G →GL(V ) of G, deﬁne κ(G, Γ, π) to be the minimum, over all unit
vectors v ∈V and all γ ∈Γ, of the quantity ∥π(γ)v −v∥. The Kazhdan constant of
the pair (G, Γ), denoted by κ(G, Γ), is deﬁned to be the minimum, over all nontrivial
irreducible unitary representations π of G, of κ(G, Γ, π).
Facts:
1. Expander graphs are used in the design of algorithms, computer networks, and error-
correcting codes, as well as in cryptography. They have also been used inside proofs of
complexity theory results.

676
Chapter 8
GRAPH THEORY
2. If X is a d-regular graph, then
d −λ2(X)
2
≤h(X) ≤
p
(d + λ2(X))(d −λ2(X)).
3. Let (Xn) be a sequence of d-regular graphs with |Xn| →∞. Then (Xn) is an expander
family if and only if there exists ǫ > 0 such that the spectral gap is at least ǫ for all n.
4. Let (Xn) be a sequence of d-regular Ramanujan graphs with d ≥3 and |Xn| →∞.
Then (Xn) is an expander family.
5. There exist inﬁnite families of regular bipartite Ramanujan graphs of every degree
bigger than 2. (Marcus, Spielman, Srivastava, 2015).
6. Let (Xn) be a sequence of d-regular graphs with |Xn| →∞. Then for all ǫ > 0 there
is some Xn with λ(Xn) > 2
√
d −1 −ǫ. That is, the bound given in the deﬁnition of
Ramanujan graphs cannot be lowered.
7. Let d be a positive integer and let (Gn) be a sequence of ﬁnite groups with |Gn| →∞.
For each n, let Xn be an (undirected) Cayley graph for Gn with generating set Γn chosen
so that Xn is d-regular. (See §8.10.3.) Then (Xn) forms an expander family if and only
if there exists ǫ > 0 such that κ(Gn, Γn) ≥ǫ for all n. (It is known that this cannot hold
if all the groups are abelian.)
8. Let d ≥3 and let 0 < α < 1 be a real number such that 24/d < (1 −α)1−α(1 + α)1+α.
In a certain random graph model, as n →∞, the probability that h(X) ≥(1 −α)d/2 for
a randomly chosen d-regular graph X of order n goes to 1.
9. Let d be a positive integer and ǫ > 0. In a certain random graph model, as n →∞,
the probability that λ2(X) ≤2
√
d −1 + ǫ for a randomly chosen d-regular graph X of
order n goes to 1.
Examples:
1. Let X = (VX, EX) and Y = (VY , EY ) be regular graphs, where the degree of X equals
the order of Y . In this example, multiple edges and loops are allowed, and multiple edges
are treated as distinct elements. For each v from VX, deﬁne the set Ev consisting of all
edges e ∈EX incident to v, and choose some bijection Lv : VY →Ev. Deﬁne the zig-zag
product X z⃝Y to be the graph with vertex set VX × VY where the number of edges
between (x1, y1) and (x2, y2) is equal to the number of ordered pairs (z1, z2) ∈EY × EY
such that y1 is an endpoint of z1, y2 is an endpoint of z2, and Lx1(z1(y1)) = Lx2(z2(y2)).
2. If G is a graph with adjacency matrix A, we denote by G2 the graph with adjacency
matrix A2. Let W be a nonbipartite d-regular graph of order d4 with λ(W)/d ≤1/5. (It
can be shown that such a graph W exists.) Deﬁne the sequence Wn by W1 = W 2 and
Wn+1 = W 2
n z⃝W. Then the sequence of graphs (Wn) forms an expander family.
3. Construct a family (Xn) of 8-regular graphs as follows. Let the vertex set of Xn be
Zn ×Zn. Let the neighbors of (x, y) be the vertices (x, y+2x), (x+2y, y), (x, y+2x+1),
(x + 2y + 1, y), (x, y −2x), (x −2y, y), (x, y −2x −1), and (x −2y −1, y) (arithmetic
modulo n). Then λ(Xn) ≤5
√
2 < 8. Hence, (Xn) is an expander family by Fact 3.
8.10.5
PRODUCT GRAPHS
Graph products provide a convenient language for the description of structures, but
they are also algebraic objects with deep results and challenging open problems. Here

Section 8.10
GRAPH FAMILIES
677
we describe only the four standard products, although there are many other products
that are useful for special purposes, e.g., in the construction of expanders. For more
information consult the handbook [HaImKl11].
Deﬁnitions:
Given graphs G and H, the Cartesian product G □H, the direct product G × H,
the strong product G⊠H, and the lexicographic product G◦H have as vertex sets
the Cartesian product V (G) × V (H). Edges are as follows:
E(G □H) =

(g, h)(g′, h′) | gg′ ∈E(G) and h = h′, or, g = g′ and hh′ ∈E(H)
	
,
E(G × H) =

(g, h)(g′, h′) | gg′ ∈E(G) and hh′ ∈E(H)
	
,
E(G ⊠H) = E(G □H) ∪E(G × H),
E(G ◦H) =

(g, h)(g′, h′) | gg′ ∈E(G), or g = g′ and hh′ ∈E(H)
	
.
If • ∈{ □, ⊠, ×, ◦} and A = G • H, then G and H are factors of A with respect to •.
If a graph cannot be represented as a product G•H of two graphs on at least two vertices
each, then it is prime with respect to •.
Facts:
1. The standard products are associative in the sense that the map ((g, h), k) 7→(g, (h, k))
is an isomorphism (G • H) • K →G • (H • K).
2. The Cartesian, direct, and strong products are commutative. In general G ◦H and
H ◦G are not isomorphic.
3. If G ◦H and H ◦G are isomorphic, then G and H are either both complete, both
edgeless, or both lexicographic powers of the same graph X.
4. If + represents disjoint union, then G • (H + K) = G • H + G • K for each of the
standard products.
5. The one-vertex graph K1 is a unit for the Cartesian, the strong and the lexicographic
products; that is, G = G □K1 = G ⊠K1 = G ◦K1 for any graph G.
6. The direct product has no unit in the class of simple graphs. If one allows loops,
then its unit is the one-vertex graph with a loop.
7. The Cartesian and the strong products of two graphs are connected if and only if
both factors are connected.
8. The direct product of two graphs is connected if and only if both factors are connected
and at most one is bipartite.
9. If G is nontrivial, then G ◦H is connected if and only if G is connected. (But if
G = K1, then G ◦H is connected if and only if H is connected.)
10. Every nontrivial connected graph has a unique representation as a Cartesian product
of prime graphs, up to isomorphisms and the order of the factors. The same result holds
for the strong product.
11. Unique prime factorization over the direct product involves the class Γ0 of ﬁnite
graphs that admit loops. Every connected nonbipartite graph on more than one vertex
factors uniquely (up to isomorphism and the order of the factors) into primes with respect
to the direct product in Γ0.
12. Although connected bipartite graphs do not necessarily factor uniquely into primes,
such a factoring always contains exactly one prime bipartite factor, which is unique.
(R. Hammack and O. Puﬀenberger, 2015)

678
Chapter 8
GRAPH THEORY
13. Despite the fact that prime factorization over the direct product need not be unique
for bipartite graphs, we have the following cancellation laws:
(i) If G × H is isomorphic to G′ × H and G, G′, H are bipartite, then G is isomorphic
to G′.
(ii) If G × H is isomorphic to G′ × H and H has an odd cycle, then G is isomorphic
to G′.
In general, G × H being isomorphic to G′ × H requires that V (G′) = V (G) and E(G′) =
{xα(y) | xy ∈E(G)}, where α is a permutation of V (G) satisfying α(x)α−1(y) ∈E(G)
for all xy ∈E(G).
14. A Cartesian, strong, direct, or lexicographic product has a transitive automorphism
group if and only if every factor has a transitive automorphism group.
15. Given a connected graph of order n and size m, its prime factors over the Cartesian
product can be computed in time O(m).
16. Given a connected graph, its prime factors over the strong product can be computed
in time O(ma∆), where m is the size, a the arboricity, and ∆the maximum degree.
17. Over the direct product, the prime factors of a connected nonbipartite graph of
order n, size m, and maximum degree ∆can be computed in O(m min(n2, ∆3)) time.
18. The problem of ﬁnding a nontrivial factoring of a graph over the lexicographic
product is polynomially equivalent to the graph isomorphism problem.
19. The automorphism group of a connected graph is isomorphic to the automorphism
group of the disjoint union of its prime factors over the Cartesian product. The same
holds: (i) For the strong product, if in addition no two vertices of the graph have iden-
tical closed neighborhoods. (ii) For the direct product (in the class Γ0 of ﬁnite graphs
that admit loops), if the graph is nonbipartite and no two vertices have identical open
neighborhoods.
20. If G and H are graphs on at least two vertices, then the connectivity κ(G □H) is
min{κ(G)|V (H)|, κ(H)|V (G)|, δ(G) + δ(H)}.
Open problems:
1. Hedetniemi’s conjecture:
χ(G × H) = min{χ(G), χ(H)}, where χ is the chromatic
number.
This is only known to be true if G or H has chromatic number up to 4.
(See §8.6.1)
2. Vizing’s conjecture:
γ(G □H) ≥γ(G)γ(H), where γ is the domination number.
(See §8.6.6.)
3. Graham’s conjecture:
p(G □H) ≤p(G)p(H), where p is the pebbling number.
(See §8.6.7.)
Examples:
1. The ﬁgure below shows the four products whose factors are the ﬁve-cycle and the
path of length two.
C5
P3
Cartesian product
C5 ×
◦
P3
Direct product
C5
P3
Strong product
C5
P3
Lexicographic product

Section 8.11
ANALYTIC GRAPH THEORY
679
2. Hamming graphs are Cartesian products of complete graphs. Thus hypercubes are
Hamming graphs where every factor is a K2.
3. Some direct products can be represented as Cartesian products. For example, K2×K4
= Q3, C2k+1 × C2k+1 = C2k+1 □C2k+1, and Km × Kn is the complement of Km □Kn.
4. A median graph is a graph with the property that for any three vertices u, v, w, there
is exactly one vertex z that is on a shortest u,v-path, a shortest v,w-path, and a shortest
w,u-path. These graphs play a role in genetics. Every median graph is the retract of an
hypercube: that is, the image of an adjacency-preserving mapping α of some hypercube
into itself, where α2 = α.
5. Large powers of (small) graphs with respect to the direct product can be used to
model the internet graph and similar networks.
6. The Shannon capacity of a graph G is the limit as n →∞of the nth root of the
independence number of the strong product of n copies of G. It measures the amount
of information involving an alphabet V (G) that can be accurately transmitted across
a channel, where E(G) is the set of pairs of symbols that are easily “confoundable” in
transmission.
7. If G = Γ(A, S) and G′ = Γ(A′, S′) are Cayley graphs, where A and A′ are groups with
generating sets S and S′, then G◦G′ is the Cayley graph Γ
 A×A′, ({1}×S′)∪(S ×A′)

.
8.11
ANALYTIC GRAPH THEORY
Analytic graph theory involves three diﬀerent perspectives on the properties of graphs
that are suﬃciently “dense”. One analysis is what must happen in a simple graph when
the number of edges is suﬃciently large. A second analysis is what must happen in at
least one of the parts of a partition of the edges of a graph. The third analysis is what
happens with a high probability when a graph is randomly chosen according to some
distribution.
8.11.1
EXTREMAL GRAPH THEORY
We focus on the part of extremal graph theory that investigates the number of edges an
n-vertex simple graph must have in order to guarantee that it contains a certain graph
or type of graph.
Deﬁnitions:
The extremal number ex(G; n) for a set G of graphs is the greatest number of edges in
any simple graph with n vertices that does not contain some member of G as a subgraph.
Also, the notation ex(G; n) is used when G consists of just one graph G.
An extremal graph for a set G of graphs and an integer n is a graph with n vertices
and ex(G; n) edges that contains no member of G.
The Tur´an graph Tk(n) is the n-vertex k-partite simple graph with the maximum
number of edges.
The Tur´an number tk(n) is the number of edges in the Tur´an graph Tk(n).

680
Chapter 8
GRAPH THEORY
Facts:
1. If ex(G; n) =
 n
2

, then no graph with n vertices contains any member of G.
2. The Tur´an graph Tk(n) is the unique complete k-partite graph with the property
that the numbers of vertices in any two of its parts diﬀer by at most 1. In the special
case k = 2, T2(n) = K⌊n/2⌋,⌈n/2⌉. More generally, if n = tk + r, where 0 ≤r < k, then
there are r parts of size t + 1 and k −r parts of size t.
3. The Tur´an number tk(n) equals
 n
2

−t(n−k+r)
2
, where n = tk + r, with 0 ≤r < k. If
k = 2, this simpliﬁes: t2(n) = ⌊n2
4 ⌋.
4. Tur´an’s theorem:
ex(Kk; n) = tk−1(n); furthermore, Tk−1(n) is the only extremal
graph for Kk and n.
5. Let χ = χ(G) (chromatic number of G, §8.6.1) and p = |G|. Then ex(G; n) is given
by

1 −
1
χ−1
  n
2

+ O(n2−1/p). Furthermore, all the extremal graphs diﬀer from the
Tur´an graph Tχ−1(n) by adding and deleting O(n2−1/p) edges, and the minimum degree
of all such graphs is (1 −
1
χ−1)n + O(n2−1/p). (Erd˝os, Simonovits)
6. Fact 5 is also true for ex(G; n), where χ is the smallest chromatic number among the
members of G, and p is the smallest order among these members.
7. ex(G; n) = O(n) if and only if G contains a (tree or) forest.
8. There exists a number t0 such that, for t > t0, every tree T of order t satisﬁes the
inequality ex(T ; n) ≤n(t−2)
2
for every n ≥t + 1.
9. ex(C4; n) = 1
2(n3/2) + O(n4/3). (The exponent 4
3 can be slightly improved.)
10. ex(C2m; n) = O(n1+1/m). This is known to be sharp only for 2m = 4, 6, 10, but is
conjectured to be sharp for all m.
11. The ratio
ex(G;n)
(n
2)
is monotone nonincreasing; that is, for every set G and for all
m ≤n, ex(G;m)
(m
2)
≥ex(G;n)
(n
2)
.
12. The following table summarizes some facts that apply as the number of edges grows.
# edges
what must occur, but not for
smaller # edges
what must occur if n is large
enough
n
some cycle
 3n−1
2

some even cycle
3n −5
two disjoint cycles
t2(n) + 1 =
 n2
4

+ 1
some odd cycle (i.e., χ ≥3),
C3, . . . , C⌊(n+3)/2⌋
Ks,s + e for ﬁxed s
t2(n) + m, m ﬁxed
m
 n
2

copies of C3; for ﬁxed s
Ks,s plus m extra edges
tk(n) + 1
Kk; also, χ ≥k
tk(n) + m, m ﬁxed
for ﬁxed s, Ks,n plus m extra
edges
 n
2

−n + 3
a Hamilton cycle
Examples:
1. ex(K2; n) = 0. The extremal graph is the edgeless graph.
2. ex(P2; n) =
 n
2

. The extremal graph is a perfect matching.

Section 8.11
ANALYTIC GRAPH THEORY
681
3. ex(K1,r; n) =
 (r−1)n
2

. If (r −1)n is even, then any (r −1)-regular graph is an
extremal graph. If (r −1)n is odd, then any graph with one vertex of degree r −2 and
all the others of degree r −1 is extremal.
4. ex(K3; n) =
 n2
4

. The Tur´an graph T2(n) is the only extremal graph.
5. The Tur´an graph T3(10) is the 3-partite graph K3,3,4. It has 33 edges, which is more
than any other 3-partite graph on 10 vertices. Thus, ex(K4; 10) = 33.
8.11.2
RAMSEY THEORY FOR GRAPHS
If the edges of a “dense” graph are partitioned into two parts, then at least one of the
parts must still be fairly dense. Ramsey theory, which can also be studied in connection
with many mathematical objects other than graphs, relies on this idea. (Also see §3.1.6.)
Deﬁnitions:
The (classical) Ramsey number r(m, n) is the smallest positive integer k such that
every k-vertex graph contains either the complete graph Km or n mutually nonadjacent
vertices.
The Ramsey number r(G, H) is the smallest positive integer k such that, if the edges
of Kk are partitioned into red and blue classes, then either the red subgraph con-
tains a copy of G or else the blue subgraph contains a copy of H.
Sometimes r(G)
denotes r(G, G).
The Ramsey number r(G1, . . . , Gs) is the smallest number k such that in any s-fold
partition of the edgeset of Kk, there is an index j such that the jth part contains the
graph Gj.
A k-canonical coloring of a complete graph is an edge-coloring in which the vertices
can be partitioned into k or fewer parts, such that the color of each edge depends only
on the two parts to which its endpoints belong.
The arrows notation F →(G, H) (“F arrows (G, H)”) means that if the edges of
the graph F are partitioned into red edges and blue edges, then either the red subgraph
contains a copy of G or else the blue subgraph contains a copy of H. When G = H, the
notation F →G is used. The notation F →(G1, . . . , Gk) means that k edge colors are
involved.
Facts:
1. r(Km, Kn) = r(m, n).
2. r(G, H) = r(H, G). That is, Ramsey numbers are symmetric.
3. r(Kn, K1) = r(K1, Kn) = 1 for every n ≥1.
4. r(Kn, K2) = r(K2, Kn) = n for every n ≥1.
5. r(Km, Kn) ≤r(Km, Kn−1) + r(Km−1, Kn) for all m, n ≥2.
6. r(Km, Kn) ≤
 m+n−2
m−1

. (Erd˝os and Szekeres, 1935)
7. If n ≥3, then 2n/2 ≤r(Kn, Kn) ≤
 2n−2
n−1

< 4n.
8. The previous fact can be improved: O(n2n/2) ≤r(Kn, Kn) ≤
 2n−2
n−1

· O(1/ log n).
9. There exist constants c1 and c2 such that c1n ln n ≤r(K3, Kn) ≤c2n ln n.
10. A 1-canonical coloring assigns every edge the same color.

682
Chapter 8
GRAPH THEORY
11. A 2-canonical coloring consists of two complete edge-monochromatic subgraphs,
such that all edges joining them are of the same color.
12. If χ(G) = χ and |VH| = n, then r(G, H) ≥(χ −1)(n −1) + 1. This fact is based on
a (χ −1)-canonical coloring.
13. If T is an n-vertex tree, then r(Km, T ) = (m −1)(n −1) + 1. In other words, the
lower bound in the immediately preceding fact determines the Ramsey number.
14. Except for r(C3, C3) = r(C4, C4) = 6, the Ramsey numbers r(Cm, Cn) and r(Pm, Cn)
are determined by the best possible 2-canonical colorings, which are easy to ﬁnd.
15. For every choice of graphs G1, G2, . . . , Gk, there exists a graph F for which F →
(G1, . . . , Gk). In particular, the Ramsey number r(G1, . . . , Gk) is well deﬁned.
16. If m, n ≥3, the values of only nine classical Ramsey numbers are known:
r(3, 3) = 6
r(3, 4) = 9
r(3, 5) = 14
r(3, 6) = 18
r(3, 7) = 23
r(3, 8) = 28
r(3, 9) = 36
r(4, 4) = 18
r(4, 5) = 25.
17. Estimates on some other Ramsey numbers are given in §3.1.6. In addition to the
nine exact results, only one other nontrivial Ramsey number for complete graphs is
known:
r(K3, K3, K3) = 17.
8.11.3
PROBABILISTIC GRAPH THEORY
Probabilistic graph theory takes two basic directions.
It studies random graphs for
themselves, and it uses random graphs in deriving graph-theoretical results that are not
themselves probabilistic.
Deﬁnitions:
In Model 1, the random graph Gn,p has n labeled vertices v1, . . . , vn, and the proba-
bility of any pair of vertices being joined by an edge is p, where all these edge probabilities
are mutually independent.
In Model 2, the random graph Gn,e has n labeled vertices v1, . . . , vn, and exactly
e edges, and each such labeled graphs occurs with the same probability 1/
 N
e

, where
N =
 n
2

.
We say almost every (a.e.) graph has some property P under either Model 1 or
Model 2, if the probability that a random graph has property P approaches 1 as n →∞;
where the probability p stays constant under Model 1, but how e varies with n under
Model 2 must be speciﬁed. If no model is explicitly speciﬁed, then Model 1 with p = 1/2
is implicit.
Facts:
1. The number of labeled graphs in the probability space for Model 1 is 2(
n
2).
2. While Model 2 is sometimes considered more natural, it is in practice usually easier to
work with Model 1. Fortunately, Model 1 with p =
e
N behaves very similarly to Model 2
in most cases, so that facts about Model 1 usually lead easily to facts about Model 2 as
well.

Section 8.11
ANALYTIC GRAPH THEORY
683
3. In Model 1, a graph with e edges occurs with probability pe(1 −p)(n
2)−e. If p = 1/2,
then every labeled graph on n vertices has the same probability 2−(n
2).
4. Random graphs can be used to prove theorems about graphs, especially existence
theorems. (See Example 1.)
5. Let b = 1
p and d = 2 logb
en
2 logb n = 1, where e = 2.718 . . ., not the number of edges.
Then for every positive ǫ < 1
2 the clique number of a.e. graph is either ⌊d −ǫ⌋or ⌊d + ǫ⌋,
where these two values are usually the same when ǫ is small. This means that the clique
number is determined for a.e. graph, unless d is close to an integer, in which case there
are two possible values.
6. If p ≫1/n (for example if p is constant), the chromatic number of G(n, p) is
  1
2 + o(1)

n
log2(np) log2

1
1−p

.
7. Moreover, if p = n−α for ﬁxed α > 1/2, then in Model 1, there exists an f(n, p) so
that for almost every graph, f(n, p) ≤χ ≤f(n, p) + 1. That is, the chromatic number χ
takes on one of two consecutive values.
8. Almost every graph in Model 1 has its connectivity and its edge connectivity equal
to its minimum degree. Furthermore, the common value of these three parameters is
pn −(2p(1 −p)n log n)
1
2 + o(n log n)
1
2 .
9. Generating a random graph Gn,p under Model 1 is straightforward, as indicated by
Algorithm 1.
Algorithm 1:
Generate random graph Gn,p (per Model 1).
initialize graph with vertex list v1, v2, . . . , vn
for i := 1 to n −1
for j := i + 1 to n
join vertices vi and vj with probability p
10. To generate a random graph Gn,e under Model 2, the possible edges are placed
in bijective correspondence with the integers 1, . . . ,
 n
2

according to the rule f(i, j) =
 n
2

−
 n−i+1
2

+j. Also, the e-combinations of the integers 1, . . . ,
 n
2

are placed in bijective
correspondence with the integers 1, . . . ,
 (
n
2)
e

according to the lexicographic ordering of
those e-combinations (see §2.3.5). This yields Algorithm 2.
Algorithm 2:
Generate random graph Gn,e (per Model 2).
initialize graph with vertex list v1, v2, . . . , vn
generate random integer r ∈
n
1, . . . ,
 (
n
2)
e
o
convert r to an e-combination C in

1, . . . ,
 n
2
	
convert e-combination C to e edges in Gn,e
11. For every ﬁxed s, almost every graph contains the complete graph Ks. Moreover,
for every ﬁxed graph H, almost every graph contains H.
12. Here is a table of some properties of almost every n-vertex graph:

684
Chapter 8
GRAPH THEORY
p under Model 1
e under Model 2
property of almost every graph
o( 1
n)
o(n)
no cycles
2c
n , 0 < c < 1
2
cn, 0 < c < 1
2
cycles are possible, and the largest compo-
nent has order ≈ln n
1
n
n
2
some cycle exists, and the largest compo-
nent has order Θ(n2/3)
2c
n , c > 1
2
cn, c > 1
2
the largest component has order c′n
c ln n
n , c < 1
c
2n ln n, c < 1
the graph is disconnected
c ln n
n , c > 1
c
2n ln n, c > 1
the graph is connected and Hamiltonian
Examples:
1. Using random graphs to prove theorems:
Here is a proof that the Ramsey number
r(Kn, Kn) (see §8.11.2) is greater than 2n/2 for all n ≥3. Consider a random red-blue
edge-coloring of KN for some N > n with p(red) = 1/2. The probability that any given
Kn occurring within this 2-colored KN is entirely red is 2
−(n
2). Of course, the probability
that it is colored blue is the same. Thus, the probability that the given subgraph Kn is
monochromatic in either color is 2
1−(n
2). Since there are
 N
n

diﬀerent copies of Kn in
the colored KN, the expected number of monochromatic Kn is
 N
n

· 2
1−(n
2).
With the choice of N = ⌊2n/2⌋, this expectation is
 N
n

· 2
1−(n
2) < 21+n/2
n!
·
N n
2n2/2 < 1, i.e.,
less than 1. Therefore there must be some coloring with no monochromatic Kn at all.
2. Other examples in which random graphs are analyzed can be found in the section on
the probabilistic method (see §7.11).
8.12
HYPERGRAPHS
In ordinary graph theory, an edge of a simple graph can be regarded as a pair of vertices.
In hypergraph theory, an “edge” can be regarded as an arbitrary subset of vertices. In
this sense, hypergraphs are a natural generalization of graphs. Their systematic study
was initiated by C. Berge. They have evolved into a unifying combinatorial concept.
8.12.1
HYPERGRAPHS AS A GENERALIZATION OF GRAPHS
Deﬁnitions:
A hypergraph H = (V, E) is a ﬁnite set V of “vertices” together with a ﬁnite multiset E
of “edges” (sometimes, “hyperedges”), which are arbitrary subsets of V .
The order of a hypergraph edge is its cardinality.
A partial hypergraph (or simply a partial) of the hypergraph H = (V, E) is a hyper-
graph H′ = (V, E′) such that E′ ⊆E. This generalizes a spanning subgraph.
A hypergraph H = (V, E) is simple if E has no repeated edges.

Section 8.12
HYPERGRAPHS
685
The incidence matrix of a hypergraph H = (V, E) with E = {e1, e2, . . . , em} and
V = {x1, x2, . . . , xn} is the m × n matrix M(H) = [mi,j] with mi,j = 1 if xj ∈ei and 0
otherwise.
The dual hypergraph of the hypergraph H is the hypergraph H∗whose incidence
matrix is the transpose of the incidence matrix M(H). This concept of duality from
block design theory diﬀers from the dual of a graph embedded on a surface.
The degree deg(x) of a hypergraph vertex x is the number of hypergraph edges con-
taining x.
A hypergraph is regular if all vertices have the same degree. If t is the common value
of the degrees, then the hypergraph is t-regular.
A hypergraph is uniform if all edges have the same number of vertices. If r is the
common value, then the hypergraph is r-uniform.
The complete hypergraph K∗
n has all subsets of n vertices as edges, so that it has 2n
edges.
The complete r-uniform hypergraph Kr
n is the simple hypergraph of order n with
all r-element subsets as edges, so that it has
 n
r

edges.
The intersection graph I(H) of the hypergraph H is a simple graph whose vertices
are the edges of H. Two vertices of I(H) are adjacent if and only if the corresponding
edges of H have nonempty intersection.
An independent set of vertices in a hypergraph is a set of vertices that does not
(completely) contain any edge of the hypergraph.
Facts:
1. How to draw a hypergraph: First draw the vertices and the hyperedges of order 2, as
if they were vertices and edges, respectively, of a graph. Then shade triangular regions
corresponding to hyperedges of order 3.
Higher-order hyperedges and hyperedges of
order 1 can be indicated by drawing enclosures around their vertices.
2. Every hypergraph satisﬁes the generalized Euler equation for degree-sum:
X
x∈V
deg(x) =
X
e∈E
|e|.
3. Every simple graph is a 2-uniform simple hypergraph.
4. The intersection graph of a hypergraph generalizes the line graph L(G) of a graph G.
(See §8.1.2.)
5. Every graph is the intersection graph of some hypergraph.
6. Every graph of order n is isomorphic to the intersection graph of a hypergraph of
order at most

n2/4

.
7. When a graph G is regarded as a hypergraph, its dual is a hypergraph whose inter-
section graph is G.
Examples:
1. The hypergraph H = (V, E) with V = {a, b, c, d} and E = {ab, bc, bd, acd, c} can be
illustrated as follows:

686
Chapter 8
GRAPH THEORY
a
ab
b
bc
bd
acd
d
c
2. The hypergraph of Example 1 has the following incidence matrix:
a
b
c
d
ab
1
1
0
0
acd
1
0
1
1
bc
0
1
1
0
bd
0
1
0
1
c
0
0
1
0
3. The dual of the hypergraph of Example 1 has the following incidence matrix:
v (ab)
v (acd)
v (bc)
v (bd)
v (c)
e (a)
1
1
0
0
0
e (b)
1
0
1
1
0
e (c)
0
1
1
0
1
e (d)
0
1
0
1
0
This dual hypergraph may be illustrated as follows:
v(ab)
v(bd)
v(c)
v(bc)
e(a)
e(c)
e(d)
e(b)
v(acd)
4. The hypergraph of Example 1 has the following intersection graph:
ab
acd
c
bc
bd
8.12.2
HYPERGRAPHS AS GENERAL COMBINATORIAL STRUCTURES
Deﬁnitions:
A transversal (or cover or blocking set) in a hypergraph is a set of vertices that has
nonempty intersection with every edge of the hypergraph.
A system of distinct representatives (SDR) in a hypergraph H = (V, E) with
E = {e1, e2, . . . , em} is a transversal of m distinct vertices x1, x2, . . . , xm such that xi ∈ei
for i = 1, . . . , m.

Section 8.12
HYPERGRAPHS
687
Hall’s condition on a hypergraph is that, for each t = 1, . . . , m the union of every
subset of t edges have at least t vertices. Thus, each partial must have at least as many
vertices as edges.
A matching in a hypergraph is a set of pairwise disjoint edges.
An antichain is a hypergraph in which no edge contains any other edge.
A chain is a simple hypergraph in which, given any pair of edges, one edge contains the
other.
A symmetric chain in an n-vertex hypergraph H is a chain with edges of order n
2 −
t, . . . , n
2 + t for some t ≥0.
A downset (or ideal) is a simple hypergraph in which every subset of every edge is also
an edge of the hypergraph.
An upset (or ﬁlter) is a simple hypergraph in which every superset of every edge is also
an edge of the hypergraph.
A hypergraph clique is a simple hypergraph such that every pair of edges has nonempty
intersection.
An r-partite hypergraph is an r-uniform hypergraph whose vertex set can be partitioned
into r blocks such that each edge intersects each block in exactly one vertex.
A hypergraph is unimodular if the determinant of every square submatrix of its inci-
dence matrix is equal to 0, 1, or −1.
A hypergraph is an interval hypergraph if its n vertices can be labeled 1, 2, . . . , n so
that each edge is labeled by consecutive numbers.
Facts:
1. Hall’s condition is necessary and suﬃcient for the existence of an SDR in a hyper-
graph.
2. Sperner’s lemma: If the hypergraph H with n vertices and m edges is an antichain,
then m ≤
 n
⌊n/2⌋

.
3. If the hypergraph H with mi edges of order i for i = 1, . . . , n is an antichain, then
nP
i=0
mi
 n
i
−1 ≤1.
4. The complete hypergraph K∗
n can be partitioned into symmetric chains.
5. Kleitman’s lemma: Let D and U be hypergraphs on the same n vertices. Let D be
a d-edge downset and U a u-edge upset. And let D and U have m common edges. Then
du ≥2nm.
6. An n-vertex hypergraph clique has at most 2n−1 edges.
7. An r-uniform n-vertex hypergraph clique n has at most
 n−1
r−1

edges if n ≥2r.
8. In any r-uniform hypergraph H, the maximum size r-partite partial hypergraph
contains at least r!
rr of the edges of H.
9. Let H be an n-vertex, m-edge hypergraph clique, such that each pair of distinct edges
intersect in exactly one vertex. Then m ≤n. (de Bruijn and Erd˝os)
10. Fisher’s inequality:
Let H be an n-vertex, m-edge hypergraph clique such that
each pair of edges intersects in λ vertices. Then m ≤n.
11. Modular intersection theorem:
Let L be a set of s integers, and let p be a prime
number.
Let H be an r-uniform hypergraph such that r /∈L mod p and that the
intersection size for each pair of distinct edges is in L mod p. Then m ≤
 n
s

.

688
Chapter 8
GRAPH THEORY
Examples:
1. The Fano plane (see §12.1.1) is the hypergraph with (using mod 7 arithmetic):
V = {1, 2, . . ., 7} and E = { {1 + i, 2 + i, 4 + i} | 1 ≤i ≤7 } .
2. A block design is a regular, uniform hypergraph such that each pair of vertices is con-
tained in precisely λ edges. Block designs often provide extremal examples in problems
of hypergraph theory.
3. A matroid (see §12.4.1) can be regarded as a hypergraph such that under every
nonnegative weighting of the vertices, a greedy algorithm would ﬁnd an edge of maximum
weight.
8.12.3
NUMERICAL INVARIANTS OF HYPERGRAPHS
Calculating formulas for the values of some standard numerical invariants of hypergraphs
tends to be quite diﬃcult, even for complete hypergraphs. Two famous examples are
Lov´asz’s proof of the Kneser conjecture and Baranyai’s proof of the factorization theorem.
Deﬁnitions:
The maxdegree ∆(H) is the largest degree of any vertex in the hypergraph H.
The chromatic number χ(H) is the smallest number of independent sets required to
partition the vertex set of H. To ensure the existence of such partitions it is assumed
that H does not contain any edges with just one vertex.
The independence number α(H) is the maximum number of vertices which form an
independent set in H.
The chromatic index q(H) is the smallest number of matchings required to partition
the edges of H.
A hypergraph H is normal if q(H) = ∆(H).
The transversal number τ(H) is the minimum cardinality (i.e., number of vertices),
taken over all transversals of H.
The matching number ν(H) is the maximum number of pairwise disjoint edges of H,
i.e., the cardinality of the largest partial of H which forms a matching.
The clique partition number cp(H) is the smallest number of cliques required to
partition the edge set of H.
The clique number ω(H) is the largest number of edges of any partial clique in the
hypergraph H.
Facts:
1. Many hypergraph invariants are representable as graph invariants. In particular,
ω(H) = ω(I(H)),
ν(H) = α(I(H)),
q(H) = χ(I(H)),
cp(H) = χ(I(H))
where G denotes the edge-complement of a graph G.
2. Every hypergraph H satisﬁes the following two min ≥max relations:
q(H) ≥cp(H) ≥∆(H),
τ(H) ≥cp(H) ≥ν(H).

REFERENCES
689
3. A hypergraph H is normal if and only if τ(H′) = ν(H′) for all partials H′ of H.
(Lov´asz, 1972)
4. The following relations hold in every n-vertex hypergraph H:
τ(H) = n −α(H),
χ(H) ≥
n
α(H),
χ(H) + α(H) ≤n + 1.
5. The parameters χ(H) and τ(H) can be approximated by greedy algorithms.
6. The Kneser conjecture that cp(Kr
2r+k) = k + 2 was proved by topological methods.
(Lov´asz and B´ar´any, 1978)
7. The factorization theorem that q(Kr
kr) =
 kr−1
r−1

was proved by using network ﬂows.
(Baranyai, 1975)
8. Hypergraphs in the following classes are known to be bicolorable (i.e., χ(H) = 2):
normal hypergraphs (including unimodular hypergraphs), r-uniform hypergraphs with
size at most 2r−1, r-uniform hypergraphs in which each edge intersects at most 2r−3
other edges (proved by probabilistic methods), ﬁnite planes of order at least three.
Examples:
1. Consider the hypergraph H of §8.12.1, Example 1 with V = {a, b, c, d} and E =
{ab, bc, bd, acd, c}. The maximum degree ∆(H) is 3, since vertex c has degree 3. The
chromatic number χ(H) is 4, since every pair of vertices lies in some edge, so all four
vertices must get diﬀerent colors. The independence number α(H) is 1, since every pair
of vertices lies in some edge. The chromatic index q(H) is 4, using the matching c, ab.
a
ab
b
bc
bd
acd
d
c
The hypergraph H is not normal, since q(H) = 4, but ∆(H) = 3.
The transversal
number τ(H) is 2, using the transversal b, c. The matching number ν(H) is 2, using the
matching ab, c.
2. The Fano plane (§8.12.2, Example 1) has the following parameters: ω = q =
7,
∆= τ = χ = 3, α = 4, ν = cp = 1.
REFERENCES
Printed Resources:
[AgGr06] G. Agnarsson and R. Greenlaw, Graph Theory: Modeling, Applications, and
Algorithms, Pearson, 2006.
[BeWi04] L. W. Beineke and R. J. Wilson, Topics in Algebraic Graph Theory, Cambridge
University Press, 2004.
[BeWi15] L. W. Beineke and R. J. Wilson, Topics in Chromatic Graph Theory, Cam-
bridge University Press, 2015.
[Bi94] N. Biggs, Algebraic Graph Theory, 2nd ed., Cambridge University Press, 1994.
[Bo85] B. Bollob´as, Random Graphs, Academic Press, 1985.

690
Chapter 8
GRAPH THEORY
[BoHe77] J. A. Bondy and R. L. Hemminger, “Graph reconstruction — a survey”, Journal
of Graph Theory 1 (1977), 227–268.
[BoMu08] J. A. Bondy and U. S. R. Murty, Graph Theory, Springer, 2008.
[Br13] A. Bretto, Hypergraph Theory: An Introduction, Springer, 2013.
[ChLeZh15] G. Chartrand, L. Lesniak, and P. Zhang, Graphs & Digraphs, 6th ed., Chap-
man and Hall/CRC, 2015.
[ChEtal06] M. Chudnovsky, N. Robertson, P. Seymour, and R. Thomas, “The strong
perfect graph theorem”, Annals of Mathematics 164 (2006), 51–229.
[CvRoSi10] D. M. Cvetkovi´c, P. Rowlinson, and S. Simi´c, An Introduction to the Theory
of Graph Spectra, Cambridge University Press, 2010.
[Ed65] J. Edmonds, “Paths, trees and ﬂowers”, Canadian Journal of Mathematics 17
(1965), 449–467.
[Eu36] L. Euler, “Solutio problematis ad geometriam situs pertinentis”, Commentarii
academiae scientiarum Petropolitanae 8 (1736), 128–140.
[GaJo79] M. R. Garey and D. S. Johnson, Computers and Intractability: A Guide to the
Theory of NP-completeness, W. H. Freeman, 1979.
[GrTu12] J. L. Gross and T. W. Tucker, Topological Graph Theory, Dover, 2012.
[GrYe05] J. L. Gross and J. Yellen, Graph Theory and Its Applications, Chapman and
Hall/CRC, 2005.
[GrYeZh13] J. L. Gross, J. Yellen, and P. Zhang, Handbook of Graph Theory, 2nd ed.,
Chapman and Hall/CRC, 2013.
[Ha35] P. Hall, “On representatives of subsets”, Journal of the London Mathematical
Society 10 (1935), 26–30.
[HaImKl11] R. Hammack, W. Imrich, and S. Klavˇzar, Handbook of Product Graphs, 2nd
ed., CRC Press, 2011.
[HaPa73] F. Harary and E. M. Palmer, Graphical Enumeration, Academic Press, 1973.
[HaHeSl98a] T. W. Haynes, S. T. Hedetniemi, and P. J. Slater, Fundamentals of Domi-
nation in Graphs, Marcel Dekker, 1998.
[HaHeSl98b] T. W. Haynes, S. T. Hedetniemi, and P. J. Slater, Domination in Graphs:
Advanced Topics, Marcel Dekker, 1998.
[HeYe13] M. A. Henning and A. Yeo, Total Domination in Graphs, Springer, 2013.
[HoLiWi06] S. Hoory, N. Linial, and A. Wigderson, “Expander graphs and their appli-
cations”, Bulletin of the American Mathematical Society 43 (2006), 439–561.
[ImKlRa08] W. Imrich, S. Klavˇzar, and D. F. Rall, Topics in Graph Theory: Graphs and
Their Cartesian Product, A K Peters, 2008.
[K¨oScTo93] J. K¨obler, U. Sch¨oning, and J. Tor´an, The Graph Isomorphism Problem: Its
Structural Complexity, Birkh¨auser, 1993.
[KrSh11] M. Krebs and A. Shaheen, Expander Families and Cayley Graphs: A Beginner’s
Guide, Oxford University Press, 2011.
[Lo72] L. Lov´asz, “Normal hypergraphs and the perfect graph conjecture”, Discrete Math-
ematics 2 (1972), 253–267.

REFERENCES
691
[Lu94] A. Lubotzky, Discrete Groups, Expanding Graphs, and Invariant Measures, Birk-
h¨auser, 1994.
[McPi14] B. D. McKay and A. Piperno, “Practical graph isomorphism, II”, Journal of
Symbolic Computation 60 (2014), 94–112.
[Mo15] J. W. Moon, Topics on Tournaments, Dover, 2015.
[Na78] C. St. J. A. Nash-Williams, “The reconstruction problem”, in Selected Topics
in Graph Theory, L. W. Beineke and R. J. Wilson (eds.), Academic Press, 1978,
205–236.
[PeSk03] S. Pemmaraju and S. Skiena, Implementing Discrete Mathematics, Cambridge
University Press, 2003.
[P´oRe87] G. P´olya and R. C. Read, Combinatorial Enumeration of Groups, Graphs, and
Chemical Compounds, Springer-Verlag, 1987.
[Re98] R. C. Read, Atlas of Graphs, Oxford University Press (Clarendon Press), 1998.
[Ro76] F. S. Roberts, Discrete Mathematical Models with Applications to Social, Biolog-
ical, and Environmental Problems, Prentice-Hall, 1976.
[Ro11] K. H. Rosen, Discrete Mathematics and Its Applications, 7th ed., McGraw-Hill,
2011.
[Ro99] J. J. Rotman, An Introduction to the Theory of Groups, 4th ed., Springer, 1999
[Ta97] R. Tamassia, “Graph drawing”, in Handbook of Discrete and Computational
Geometry, J. E. Goodman and J. O’Rourke (eds.), CRC Press, 1997, 815–832.
[Th98] R. Thomas, “An update of the four-color theorem”, Notices of the American
Mathematical Society 45 (1998), 848–859.
[Tu47] W. T. Tutte, The factorization of linear graphs, Journal of the London Mathe-
matical Society 22 (1947), 107–111.
[We01] D. B. West, Introduction to Graph Theory, 2nd ed., Prentice-Hall, 2001.
Web Resources:
[OEIS] oeis.org (The On-Line Encyclopedia of Integer Sequences)
www.cs.stonybrook.edu/~algorith/ (The Stony Brook Algorithm Repository)
www.math.gatech.edu/~thomas/FC (Information on the Four Color Theorem.)
Graphviz.org (graph theory software.)


9
TREES
9.1 Characterizations and Types of Trees
Lisa Carbone
9.1.1 Properties of Trees
9.1.2 Roots and Orderings
9.1.3 Tree Traversal
9.1.4 Inﬁnite Trees
9.2 Spanning Trees
Uri Peled
9.2.1 Depth-ﬁrst and Breadth-ﬁrst Spanning Trees
9.2.2 Enumeration of Spanning Trees
9.3 Enumerating Trees
Paul K. Stockmeyer
9.3.1 Counting Generic Trees
9.3.2 Counting Trees in Chemistry
9.3.3 Counting Trees in Computer Science

694
Chapter 9
TREES
INTRODUCTION
A tree is a connected graph containing no cycles.
Trees have applications in a wide
variety of disciplines, particularly computer science. For example, they can be used to
construct searching algorithms for ﬁnding a particular item in a list, to store data, to
model decisions and their outcomes, or to design networks.
GLOSSARY
ancestor (of a vertex v in a rooted tree): any vertex on a path to v from the root.
m-ary tree: a rooted tree in which every internal vertex has at most m children.
backtrack: a pair of successive edges in a walk where the second edge is the same as
the ﬁrst, but traversed in the opposite direction.
balanced tree: a rooted m-ary tree of height h such that all leaves of the tree have
height h or h−1.
bihomogeneous tree: a tree (usually inﬁnite) in which there are exactly two values
for the vertex degrees.
binary search tree (BST): a type of binary tree used to represent a table of data,
which is eﬃciently accessed by storage and retrieval algorithms.
binary tree: an ordered rooted tree in which each vertex has at most two children,
that is, a possible “left child” and a possible “right child”; an only child must be
designated either as a left child or a right child (this usage is standard for computer
science); an m-ary tree in which m = 2 (in pure graph theory).
bounded tree: a (possibly inﬁnite) tree of ﬁnite diameter.
breadth-ﬁrst search: a method for visiting all the vertices of a graph in a sequence,
in order of their proximity to a designated starting vertex.
caterpillar: a tree that contains a path such that every edge has one or both endpoints
in that path.
center (of a tree): the set of vertices of minimum eccentricity.
child (of a vertex v in a rooted tree): a vertex such that v is its immediate ancestor.
chord: for a graph G with a spanning tree T , an edge e of G such that e /∈T .
complete binary tree: a binary tree where every parent has two children and all leaves
are at the same depth.
decision tree: a rooted tree in which every internal vertex represents a decision and
each path from the root to a leaf represents a sequence of decisions.
depth (of a vertex in a rooted tree): the number of edges in the unique path from the
root to that vertex.
depth-ﬁrst search: a method for visiting every vertex of a graph by progressing as far
as possible from the most recently visited vertex, before doing any backtracking.
descendant (of a vertex v in a rooted tree): a vertex that follows v on a path away
from the root.
diameter (of a tree): the maximum distance between two vertices in the tree.

GLOSSARY
695
distance (between two vertices in a tree): the number of edges in the unique (simple)
path between these vertices.
eccentricity (of a vertex in a tree): the length of the longest simple path beginning at
that vertex.
ﬁnite tree: a tree with a ﬁnite number of vertices and edges.
forest: a graph with no cycles.
full m-ary tree: a rooted tree in which every internal vertex has exactly m children.
fundamental cycle: in a connected graph G, the unique cycle created by adding the
edge e ∈EG not in T to a spanning tree T .
fundamental edge-cut: in a connected graph G, the partition-cut ⟨X1, X2⟩where X1
and X2 are the vertex sets of the two components of T −e for some edge e in a
spanning tree T for G.
fundamental system of cycles: in a connected graph G, the set of fundamental cycles
corresponding to the various edges of G −T , where T is a spanning tree for G.
fundamental system of edge-cuts: in a connected graph G, the set of fundamental
edge-cuts that result from removal of an edge from a spanning tree T for G.
height (of a rooted tree): the maximum of the levels of its vertices.
homogeneous: property of an (inﬁnite) tree that every vertex has the same degree.
n-homogeneous: property of an (inﬁnite) tree that every vertex has degree n.
inﬁnite tree: a tree with an inﬁnite number of vertices and edges.
inorder traversal (of an ordered rooted tree): a recursive listing of all vertices starting
with the vertices of the ﬁrst subtree of the root, next the root vertex itself, and then
the vertices of the other subtrees as they occur from left to right.
internal vertex (of a rooted tree): a vertex with children.
isomorphism (of trees): for trees X and Y , a pair of bijections fV : VX →VY and
fE : EX →EY such that if u and v are the endpoints of an edge e in the tree X,
then fV (u) and fV (v) are the endpoints of the edge fE(e) in the tree Y (see §8.1).
isomorphism (of rooted trees): for rooted trees (T1, r1) and (T2, r2), a tree isomor-
phism f : T1 →T2 that takes r1 to r2.
labeled tree: a tree with labels such as v1, v2, . . . , vn assigned to its vertices.
leaf : in a rooted tree, a vertex that has no children.
left child (of a vertex in an ordered, rooted binary tree): the ﬁrst child of that vertex.
left-right tree: Synonym for full binary tree.
left subtree (of an ordered, rooted binary tree): the subtree rooted at a left child.
level (of a vertex in a rooted tree): the length of the unique path from the root to this
vertex.
locally ﬁnite tree: a tree in which the degree of every vertex is ﬁnite.
nth level (of a rooted tree): the set of all vertices at depth n.
ordered tree: a rooted tree in which the children of each internal vertex are linearly
ordered.

696
Chapter 9
TREES
parent (of a vertex v, other than the root, in a rooted tree): a vertex that is the im-
mediate predecessor of v on the unique path away from the root to v.
partition-cut of a graph: given a partition of the set of vertices of G into X1 and X2,
the set ⟨X1, X2⟩of edges of G that have one endpoint in X1 and the other in X2.
postorder traversal (of an ordered rooted tree): a recursive listing of vertices starting
with the vertices of subtrees as they occur from left to right, followed by the root.
preorder traversal (of an ordered rooted tree): a recursive listing of vertices starting
with the root, then the vertices of the ﬁrst subtree, followed by the vertices of other
subtrees as they occur from left to right.
reduced tree: a tree with no vertices of degree 2.
reduced walk: a walk in a graph without backtracking.
regular: Synonym for homogeneous.
right child (of a vertex in an ordered rooted binary tree): the second child of that ver-
tex.
right subtree (of an ordered, rooted binary tree): the subtree rooted at the right child.
rooted tree: a tree in which one vertex is designated as the “root”.
siblings (in a rooted tree): vertices with the same parent.
spanning tree (of a connected graph): a subgraph that is a tree and contains all the
vertices of the graph.
subtree: a subgraph of a tree that is also a tree.
tree: a connected graph with no cycles.
tree edge: for a graph G with a spanning tree T , an edge e of G such that e ∈T .
tree traversal: a walk that visits all the vertices of a tree.
9.1
CHARACTERIZATIONS AND TYPES OF TREES
9.1.1
PROPERTIES OF TREES
For trees, as with other graphs, there is a wide variety of terminology in use from one
application or specialty to another.
Deﬁnitions:
A graph is acyclic if it contains no subgraph isomorphic to a cycle Cn (see §8.1.3).
A forest is an acyclic graph.
A tree is an acyclic connected graph.
(Note: Unless stated otherwise, all trees are
assumed to be ﬁnite, i.e., to have a ﬁnite number of vertices.)
A walk is an alternating sequence v0, e1, v1, . . . , er, vr of vertices and edges in which each
edge ei joins vertices vi−1 and vi.

Section 9.1
CHARACTERIZATIONS AND TYPES OF TREES
697
A path (sometimes called a simple path) is a walk where all the vertices are diﬀerent.
The eccentricity of a vertex is the length of the longest (simple) path beginning at that
vertex.
The center of a tree contains all vertices with minimum eccentricity.
An end vertex of a tree is a vertex of degree 1.
A caterpillar is a tree that contains a path such that every edge has one or both
endpoints in that path.
Facts:
1. A (ﬁnite) tree with at least two vertices has at least two end vertices.
2. A connected graph with n vertices is a tree if and only if has exactly n −1 edges.
3. A graph is a tree if and only if there is a unique (simple) path between any two
vertices.
4. A graph is a forest if and only if every edge is a cut-edge. (See §8.4.2.)
5. Trees are bipartite. Hence, every tree can be colored using two colors.
6. The center of a tree consists of either only one vertex or two adjacent vertices.
Examples:
1. The following tree has 9 vertices and 9 −1 = 8 edges.
2. The following graph is a forest with three components.
3. The center of the following tree consists of the two adjacent vertices a and b.
a
b
4. The following tree is a caterpillar.

698
Chapter 9
TREES
5. Neither of the following graphs is a tree.
One contains a 3-cycle, and the other
contains a 1-cycle (i.e., a self-loop).
9.1.2
ROOTS AND ORDERINGS
Adding some extra structure to trees adapts them to applications in many disciplines,
especially computer science.
Deﬁnitions:
A rooted tree (T, r) is a tree T with a distinguished vertex r (the root), in which all
edges are implicitly directed away from the root.
Two rooted trees (T1, r1) and (T2, r2) are isomorphic as rooted trees if there is an
isomorphism f : T1 →T2 (see §8.1.2) that takes r1 to r2.
A child of a vertex v in a rooted tree is a vertex that is the immediate successor of v on
a path away from the root.
A descendant of a vertex v in a rooted tree is v itself or any vertex that is a successor
of v on a path away from the root. A proper descendant of a vertex v in a rooted tree
is any descendant except v itself.
The parent of a vertex v in a rooted tree is a vertex that is the immediate predecessor
of v on the path to v away from the root.
The parent function of a rooted tree T maps the root of T to the empty set and maps
every other vertex to its parent.
An ancestor of a vertex v in a rooted tree is v itself or any vertex on the path to v away
from the root. A proper ancestor of a vertex v in a rooted tree is any ancestor except
v itself.
Siblings in a rooted tree are vertices with the same parent.
An internal vertex in a rooted tree is a vertex with children.
A leaf in a rooted tree is a vertex that has no children.
The depth of a vertex in a rooted tree is the number of edges in the unique path from
the root to that vertex.
The nth level in a rooted tree is the set of all vertices at depth n.
The height of a rooted tree is the maximum depth over all vertices.
An ordered tree is a rooted tree in which the children of each internal vertex are linearly
ordered.
A left sibling of a vertex v in an ordered tree is a sibling that precedes v in the ordering
of v and its siblings. A right sibling of a vertex v in an ordered tree is a sibling that
follows v in the ordering of v and its siblings.

Section 9.1
CHARACTERIZATIONS AND TYPES OF TREES
699
A plane tree is a geometric realization (drawing) of an ordered tree such that the left-
to-right order of the children of each vertex in the drawing is consistent with the linear
ordering of the corresponding vertices in the tree.
In the level ordering of the vertices of an ordered tree, u precedes v under any of these
circumstances:
• if the depth of u is less than the depth of v;
• if u is a left sibling of v;
• if the parent of u precedes the parent of v.
Two ordered trees (T1, r1) and (T2, r2) are isomorphic as ordered trees if there is a
rooted tree isomorphism f : T1 →T2 that preserves the ordering at every vertex.
An m-ary tree is a rooted tree such that every internal vertex has at most m children.
A full m-ary tree is a rooted tree such that every internal vertex has exactly m children.
A (pure) binary tree is a rooted tree such that every internal vertex has at most two
children. This meaning of “binary tree” occurs commonly in graph theory.
A binary tree is a 2-ary tree such that every child, even an only child, is distinguished as
left child or right child. This meaning of “binary tree” occurs commonly in computer
science and in permutation groups.
The principal subtree at a vertex v of a rooted tree comprises all descendants of v and
all edges incident to these descendants. It is itself a rooted tree with v designated as its
root.
The left subtree of a vertex v in a binary tree is the principal subtree at the left child.
The right subtree of v is the principal subtree at the right child.
A balanced tree of height h is a rooted m-ary tree in which all leaves are of height h
or h−1.
A complete binary tree is a binary tree in which every parent has two children and
all leaves are at the same depth.
A complete m-ary tree is an m-ary tree in which every parent has m children and all
leaves are at the same depth.
A decision tree is a rooted tree in which every internal vertex represents a decision and
each path from the root to a leaf represents a sequence of decisions.
A preﬁx code for a ﬁnite set X = {x1, . . . , xn} is a set {c1, . . . , cn} of binary strings
in X (called codewords) such that no codeword is a preﬁx of any other codeword.
A Huﬀman code for a set X with a probability measure Pr (see §7.1) is a preﬁx code
{c1, . . . , cn} such that Pn
j=1 len(cj)Pr(xj) is minimum among all preﬁx codes, where
len(cj) measures the length of cj in bits.
A Huﬀman tree for a set X with a probability measure Pr is a tree constructed by
Huﬀman’s algorithm to produce a Huﬀman code for (X, Pr).
Facts:
1. Plane trees are usually drawn so that vertices of the same level in the corresponding
ordered tree are at the same vertical position in the plane.
2. A rooted tree can be represented by its vertex set plus its parent function.

700
Chapter 9
TREES
3. The concept of ﬁnite binary tree also has the following recursive deﬁnition: (basis)
an ordered tree with only one vertex is a binary tree; (recursion) an ordered tree with
more than one vertex is a binary tree if the root has at most two children and if both its
principal subtrees are binary trees.
4. A full m-ary tree with k internal vertices has mk +1 vertices and (m−1)k +1 leaves.
5. A full m-ary tree with k vertices has (k−1)/m internal vertices and ((m−1)k+1)/m
leaves.
6. There are at most mh leaves in any m-ary tree of height h.
7. A binary search tree is a special kind of binary tree used to implement a random
access table with O(n) maintenance and retrieval algorithms. (See §18.2.3.)
8. A balanced binary tree can be used to implement a priority queue with O(n) enqueue
and dequeue algorithms. (See §18.2.4.)
9. Algorithm 1, due to D. Huﬀman in 1951, constructs a Huﬀman tree.
Algorithm 1:
Find a Huﬀman code.
input: the probabilities Pr(x1), . . . , Pr(xn) on a set X
output: a Huﬀman code for (X, Pr)
initialize F to be a forest of isolated vertices, labeled x1, . . . , xn, each to be
regarded as a rooted tree
assign weight Pr(xj) to the rooted tree xj, for j = 1, . . . , n
repeat until forest F is a single tree
choose two rooted trees, T and T ′, of smallest weights in forest F
replace trees T and T ′ in forest F by a tree with a new root whose left subtree
is T and whose right subtree is T ′
label the new edge to T with a 0 and the new edge to T ′ with a 1
assign weight w(T ) + w(T ′) to the new tree
return tree F
{The Huﬀman code word for xi is the concatenation of the labels on the unique
path from the root to xi}
Examples:
1. The following is a rooted tree (T, d) with root d.
c
g
e
f
b
d
a
vertex
a
b
c
d
e
f
g
parent
d
d
d
∅
c
b
c
2. The following is a 2-ary tree of height 4.

Section 9.1
CHARACTERIZATIONS AND TYPES OF TREES
701
3. A balanced binary tree is shown next.
4. The following tree is rooted at vertex r. Vertices d and e are children of vertex b.
Vertex f is a descendant of f, d, b, and r, but f is not a descendant of vertex a. Vertex a
is the parent of c, which is the only proper descendant of vertex a. Vertices d and e are
siblings, but c is not a sibling of d or of e. The leaves are c, e, f; the internal vertices are
a, b, r, d.
c
a
r
b
d
f
e
5. The following two rooted trees are isomorphic as graphs, but they are considered to
be diﬀerent as rooted trees, because there is no graph isomorphism from one to the other
that maps root to root.
6. The following two plane trees are isomorphic as rooted trees, but they are not iso-
morphic as ordered rooted trees, because there is no rooted tree isomorphism from one
to the other that preserves the child ordering at every vertex.
7. A complete binary tree of height 2 and a complete 3-ary tree of height 2 are shown
next.

702
Chapter 9
TREES
8. The iterative construction of a Huﬀman tree for the set X = {a, b, c, d, e, f} with
respective probabilities {0.08, 0.10, 0.12, 0.15, 0.20, 0.35} would proceed as follows:
.08
.18
.27
.20
.18
0
1
a
a
b
b
0
1
0
1
a
forest  0
forest  1
forest  2
b
c
d
c
d
e
f
c
d
e
f
e
f
.10
.12
.15
.20
.35
.12
.15
.20
.35
.35
.38
forest 3
forest 4
forest 5
0
0
0
c
d
e
b
a
1
1
0
0
e
b
a
1
1
0
0
0
e
b
a
1
1
1
1
f
d
c
0
0
1
1
1
f
d
c
0
0
1
f
.27
.35
.38
.62
1.0
The resulting codes are 000 for a, 001 for b, 100 for c, 101 for d, 01 for e, and 11 for f.
So the most frequently used objects in X are represented by the shortest codes.
9.1.3
TREE TRAVERSAL
Ordered rooted trees can be used to store data or arithmetic expressions involving num-
bers, variables, and operations. A tree traversal algorithm gives a systematic method for
accessing the information stored in the tree.
Deﬁnitions:
A boundary walk of a plane tree is a walk around the boundary of the single region of
the plane imbedding of the tree, starting at the root.
A backtrack along a walk in a graph is an instance . . . , u, e, v, e, u, . . . of two consecutive
edge-steps in which an edge-step traverses the same edge as its predecessor, but in the
opposite direction.

Section 9.1
CHARACTERIZATIONS AND TYPES OF TREES
703
A preorder traversal of an ordered rooted tree T lists the vertices of T (or their labels)
so that each vertex v is followed by all the vertices, in preorder, in its principal subtrees,
respecting their left-to-right order.
A postorder traversal of an ordered rooted tree T lists the vertices of T (or their
labels) so that each vertex v is preceded by all the vertices, in postorder, in its principal
subtrees, respecting their left-to-right order.
An inorder traversal of an ordered rooted tree T lists the vertices of T (or their labels)
so that each vertex v is preceded by all the vertices, in inorder, in its ﬁrst principal
subtree and so that v is followed by the vertices, in inorder, of its other principal subtrees,
respecting their left-to-right order.
Preﬁx (or Polish) notation is the form of an arithmetic expression obtained from a
preorder traversal of a binary tree representing this expression.
Postﬁx (or reverse Polish) notation is the form of an arithmetic expression obtained
from a postorder traversal of a binary tree representing this expression.
Inﬁx notation is the form of an arithmetic expression obtained from an inorder traversal
of a binary tree representing this expression. A left parenthesis is written immediately
before writing the left principal subtree of each vertex, and a right parenthesis is written
immediately after writing the right principal subtree.
The universal address system of an ordered rooted tree is a labeling in which the root
is labeled 0 and for each vertex with label x, its m children are labeled x.1, x.2, . . . , x.m,
from left to right.
In the level order of the vertices of an ordered tree T , vertex u precedes vertex v if u is
nearer the root, or if u and v are at the same level and u and v have ancestors u′ and v′
that are siblings and u′ precedes v′ in the ordering of T .
A bijective assignment of labels from an ordered set (such as alphabetic strings or the
integers) to the vertices of an ordered tree is sorted if the level order of these labels is
either ascending or descending.
Facts:
1. The preorder traversal of a plane tree is obtained by a counterclockwise traversal of
the boundary walk of the plane region, that is, starting downward toward the left. As
each vertex of the tree is encountered for the ﬁrst time along this walk, it is recorded in
the preorder.
2. The postorder traversal of a plane tree is obtained by a counterclockwise traversal of
the boundary walk of the plane region, that is, starting downward toward the left. As
each vertex of the tree is encountered for the last time along this walk, it is recorded in
the postorder. Algorithm 2 uses this to ﬁnd the parents of all vertices.
3. The inorder traversal of a plane tree is obtained by a counterclockwise traversal of
the boundary walk of the plane region, that is, starting downward toward the left. As
each interior vertex of the tree is encountered for the second time along this walk, it is
recorded in the inorder. An end vertex is recorded whenever it is encountered for the
only time.
4. Two nonisomorphic ordered trees with sorted vertex labels can have the same preorder
but not the same postorder.

704
Chapter 9
TREES
Algorithm 2:
Parent-ﬁnder for the postorder of a plane tree.
input: the postorder vp(1), . . . , vp(n) of a plane tree with sorted vertex labels and
a vertex vj
output: the parent of vj
scan the postorder until vj is encountered
continue scanning until some vertex vi is encountered such that i < j
return (vi)
Examples:
1. The following plane tree has preorder a b e f h c d g i j k, postorder e h f b c i j k g d a,
and inorder e b h f a c i g j k d.
e
b
a
c
f
h
i
j
k
g
d
2. The following binary tree represents the arithmetic expression that has inﬁx form
(x + y)/(x −2), preﬁx form / + x y −x 2, and postﬁx form x y + x 2 −/.
x
y
x
2
+
/
-
9.1.4
INFINITE TREES
Deﬁnitions:
An inﬁnite tree is a tree with an inﬁnite number of vertices and edges.
The diameter of a tree is the maximum distance between two distinct vertices in the
tree.
A bounded tree is a tree of ﬁnite diameter.
The degree of a vertex is the number of edges incident with it.
A locally ﬁnite tree is a tree in which the degree of every vertex is ﬁnite.
A homogeneous tree is a tree in which every vertex has the same degree.
An n-homogeneous tree is a tree in which every vertex has degree n.
A bihomogeneous tree is a nonhomogeneous tree with a partition of the vertices into
two subsets, such that all vertices in the same subset have the same degree.

Section 9.2
SPANNING TREES
705
Facts:
1. K¨onig’s lemma: An inﬁnite tree that is locally ﬁnite contains an inﬁnitely long path.
Examples:
1. Suppose that two ﬁnite bitstrings are considered adjacent if one bitstring can be
obtained from the other by appending a 0 or a 1 at the right. The resulting graph is the
inﬁnite bihomogeneous tree, in which the empty string λ has degree 2 and all other ﬁnite
bitstrings have degree 3.
111
11
1
  λ
0
00
000
001
010
110
101
01
10
011
100
2. Consider the set of all ﬁnite strings on the alphabet {a, a−1, b, b−1} containing no
instances of the substrings aa−1, a−1a, bb−1, or b−1b. Suppose that two such strings
are considered to be adjacent if and only if one of them can be obtained from the other
by appending one of the alphabet symbols at the right. Then the resulting graph is a
4-homogeneous tree.
3. Consider as vertices the set of inﬁnite bitstrings with at most two 1s. Suppose two
such bitstrings are regarded as adjacent if they diﬀer in only one bit and that bit is a
rightmost 1 for one of the two bitstrings. This graph is a bounded tree of diameter four.
9.2
SPANNING TREES
A spanning tree of a graph G is a subgraph of G that is a tree and contains every
vertex of G. Spanning trees are very useful in searching the vertices of a graph and in
communicating from any given vertex to the other vertices. Minimum spanning trees are
covered in §10.1.
9.2.1
DEPTH-FIRST AND BREADTH-FIRST SPANNING TREES
Deﬁnitions:
A spanning tree of a graph G is a tree that is a subgraph of G and that contains every
vertex of G.
A tree edge of a graph G with a spanning tree T is an edge e such that e ∈T .
A chord of a graph G with a spanning tree T is an edge e such that e /∈T .
A back edge of a digraph G with a spanning tree T is a chord that joins one of its
endpoints to an ancestor in T . A forward edge is a chord that joins one of its endpoints
to a descendant in T . A cross edge is a chord that is neither a back edge nor a forward
edge.

706
Chapter 9
TREES
The fundamental cycle of a chord e with respect to a given spanning tree T of a
graph G consists of the edge e and the unique path in T joining the endpoints of e.
A depth-ﬁrst search (DFS) of a graph G is a way to traverse every vertex of a con-
nected graph by constructing a spanning tree, rooted at a given vertex r. Each stage of
the DFS traversal seeks to move to an unvisited neighbor of the most recently visited
vertex, and backtracks only if there is none available. See Algorithm 1.
A depth-ﬁrst search tree is the spanning tree constructed during a depth-ﬁrst search.
Backtracking during a depth-ﬁrst search means retreating from a vertex with no un-
visited neighbors back to its parent in the DFS-tree.
A breadth-ﬁrst search (BFS) of a graph G is a way to traverse every vertex of a
connected graph by constructing a spanning tree, rooted at a given vertex r.
After
the BFS traversal visits a vertex v, all of the previously unvisited neighbors of v are
enqueued, and then the traversal removes from the queue whatever vertex is at the front
of the queue, and visits that vertex. See Algorithm 2.
A breadth-ﬁrst search tree is the spanning tree constructed during a breadth-ﬁrst
search.
The fundamental system of cycles of a connected graph G associated with a spanning
tree T is the set of fundamental cycles corresponding to the various edges of G −T .
Given two vertex sets X1 and X2 that partition the vertex set of a graph G, the
partition-cut ⟨X1, X2⟩is the set of edges of G that have one endpoint in X1 and
the other in X2.
The fundamental edge-cut of a connected graph G associated with removal of an edge
e from a spanning tree T is the partition-cut ⟨X1, X2⟩where X1 and X2 are the vertex
sets of the two components of T −e.
The fundamental system of edge-cuts of a connected graph G associated with a
spanning tree T is the set of fundamental edge-cuts that result from removal of an edge
from the tree T .
Algorithm 1:
Depth-ﬁrst search spanning tree.
input: a connected n-vertex graph G and a starting vertex r
output: the edge set ET of a spanning tree and an array X[1..n] listing VG in
DFS order
initialize all vertices as unvisited and all edges as unused
ET := ∅; loc := 1
dfs(r)
procedure dfs(u)
mark u as visited
X[loc] := u
loc := loc + 1
while vertex u has any unused edges
e := next unused edge at u

Section 9.2
SPANNING TREES
707
mark e as used
w := the other endpoint of edge e
if w is unvisited then
add e to ET
dfs(w)
Algorithm 2:
Breadth-ﬁrst search spanning tree.
input: a connected n-vertex graph G and a starting vertex r.
output: the edge set ET of a spanning tree and an array X[1..n] listing VG in
BFS order
initialize all vertices as unvisited and all edges as unused
ET := ∅; loc := 1; Q := r {Q is a queue}
while Q ̸= ∅
x := front(Q)
remove x from Q
bfs(x)
procedure bfs(u)
mark u as visited
X[loc] := u
loc := loc + 1
while vertex u has any unused edges
e := next unused edge at u
mark e as used
w := the other endpoint of edge e
if w is unvisited then
add e to ET
add w to the back of Q
Facts:
1. Every connected graph has at least one spanning tree.
2. A connected graph G has k edge-disjoint spanning trees if and only if for every
partition of VG into m nonempty subsets, there are at least k(m −1) edges connecting
vertices in diﬀerent subsets.
3. Let T and T ′ be spanning trees of a graph G and e ∈T −T ′. Then there exists an
edge e′ ∈T ′ −T such that both T −e ∪{e′} and T ′ −e′ ∪{e} are spanning trees of G.
4. In the column vector space of the incidence matrix of G over Z2, every edge set can
be represented as a sum of column vectors. Let T be a spanning tree of G. Then each
cycle C can be written in a unique way as a linear combination of the fundamental cycles
of whatever chords of T occur in C.

708
Chapter 9
TREES
5. Depth-ﬁrst search on an n-vertex, m-edge graph runs in O(m) time.
6. DFS-trees are used to ﬁnd the components, cutpoints, blocks, and cut-edges of a
graph.
7. The unique path in the BFS-tree T of a graph G from its root r to a vertex v is a
shortest path in G from r to v.
8. Breadth-ﬁrst search on an n-vertex, m-edge graph runs in O(m) time.
9. A BFS-tree in a simple graph has no back edges.
10. Dijkstra’s algorithm (see §10.3.2) constructs a spanning tree T in an edge-weighted
graph such that for each vertex v, the unique path in T from a speciﬁed root r to v
is a minimum-cost path in the graph from r to v. When all edges have unit weights,
Dijkstra’s algorithm produces the BFS tree.
11. The level order of the vertices of an ordered tree is the order in which they would
be traversed in a breadth-ﬁrst search of the tree.
12. The fundamental cycle of an edge e with respect to a spanning tree T such that
e /∈T consists of edge e and those edges of T whose fundamental edge-cuts contain e.
13. The fundamental edge-cut with respect to removal of edge e from a spanning tree T
consists of edge e and those edges of EG −ET whose fundamental cycles contain e.
Examples:
1. Consider the following graph and spanning tree and a digraph on the same vertex
and edge set. The tree edges are a, b, c, e, f, h, k, l, and the chords are d, g, i, j. Chord d
is a forward edge, chord i is a back edge, and chords g and j are cross edges.
a
b
e
d
g
f
c
h
l
i
j
k
a
b
e
d
g
f
c
h
l
i
j
k
2. In the graph of Example 1, the fundamental cycles of the chords d, g, i, and j are
{d, b, e}, {g, f, c, a, b, e}, {i, h, l}, and {j, f, h, l}, respectively. The non-fundamental cycle
{a, d, g, c, f} is the sum (mod 2) of the fundamental cycles of chords d and g.
3. A spanning tree and its fundamental system of cycles are shown next.
u 
a 
v
b 
   c 
d
x 
e 
w
u
b 
   c
x 
e 
w
u 
a 
v
 
   c 
d
 
 
w
4. A spanning tree and its fundamental system of edge-cuts are shown next.
u 
 
v
 
    c 
d
x 
e 
w
u
b
x 
e 
w
u 
a 
v
 
 
d
 
 
w
u 
a 
v
b 
    c 
d
x 
e 
w
5. In the graph of Example 1 suppose that the local order of adjacencies at each vertex
is the alphabetic order of the edge labels. Then the construction of the DFS-tree is shown
in the following ﬁgure.

Section 9.2
SPANNING TREES
709
1
6
2
d
e
3
4
7
9
8
5
a
f
c
h
l
i
j
k
g
b
1
6
2
d
e
3
4
7
9
8
5
a
f
c
h
l
i
j
k
g
b
1
6
2
d
e
3
4
7
9
8
5
a
f
c
h
l
i
j
k
g
b
1
6
2
d
e
3
4
7
9
8
5
a
f
c
h
l
i
j
k
g
b
1
6
2
d
e
3
4
7
9
8
5
a
f
c
h
l
i
j
k
g
b
1
6
2
d
e
3
4
7
9
8
5
a
f
c
h
l
i
j
k
g
b
1
6
2
d
e
3
4
7
9
8
5
a
f
c
h
l
i
j
k
g
b
1
6
2
d
e
3
4
7
9
8
5
a
f
c
h
l
i
j
k
g
b
6. In the graph of Example 1 suppose that the local order of adjacencies at each vertex
is the alphabetic order of the edge labels. Then the construction of the BFS-tree is shown
in the following ﬁgure.
1
3
2
d
e
4
5
7
9
8
6
a
f
c
h
l
i
j
k
g
b
1
3
2
d
e
4
5
7
9
8
6
a
f
c
h
l
i
j
k
g
b
1
3
2
d
e
4
5
7
9
8
6
a
f
c
h
l
i
j
k
g
b
1
3
2
d
e
4
5
7
9
8
6
a
f
c
h
l
i
j
k
g
b
1
3
2
d
e
4
5
7
9
8
6
a
f
c
h
l
i
j
k
g
b
1
3
2
d
e
4
5
7
9
8
6
a
f
c
h
l
i
j
k
g
b
1
3
2
d
e
4
5
7
9
8
6
a
f
c
h
l
i
j
k
g
b
1
3
2
d
e
4
5
7
9
8
6
a
f
c
h
l
i
j
k
g
b
9.2.2
ENUMERATION OF SPANNING TREES
Deﬁnitions:
The number of spanning trees τ(G) of a graph G counts two spanning trees T1 and T2
as diﬀerent if their edge sets are diﬀerent, even if there is an automorphism of G mapping
T1 onto T2.
The degree matrix D(G) of an n-vertex graph G with vertex degree sequence d1, . . . , dn
is the n × n diagonal matrix in which the elements of the main diagonal are the degrees
d1, . . . , dn (and the oﬀ-diagonal elements are 0s).

710
Chapter 9
TREES
Facts:
1. Cayley’s formula: τ(Kn) = nn−2, where Kn is the complete graph.
2. τ(Km,n) = mn−1nm−1, where Km,n is the complete bipartite graph.
3. τ(Is ∗Kn−s) = nn−2 (1 −s/n))s−1, where Is is the edgeless graph on n vertices
and “∗” denotes the join (see §8.1.2).
4. τ(Wn) =

3+
√
5
2
n
+

3−
√
5
2
n
−2, where Wn denotes the wheel with n rim vertices.
5. Matrix-tree theorem:
For each s and t, the value τ(G) equals (−1)s+t times the
determinant of the matrix obtained by deleting row s and column t from D(G) −A(G),
where A(G) is the adjacency matrix for G.
6. For each edge e of a graph G, τ(G) = τ(G −e) + τ(G/e), where “−e” denotes edge
deletion and “/e” denotes edge contraction.
7. The number of spanning trees of Kn with degrees d1, . . . , dn equals
 n−2
d1−1,...,dn−1

(see §2.3.2). In this formula, the vertices are distinguishable (labeled) and are given
their degrees in advance, and the only question is how to realize them with edges.
Examples:
1. τ(K3) = 33−2 = 3. Each of the three spanning trees is a path on two edges, as
illustrated below. Also, τ(K4) = 44−2 = 16.
2. τ(K2,n) = n2n−1. To conﬁrm this, let X = {x1, x2} and |Y | = n. The spanning tree
contains a path of length 2 joining x1 to x2, whose middle vertex in Y can be chosen
in n ways. For each of the remaining n −1 vertices of Y , there is a choice as to which of
x1 and x2 is its neighbor (not both, since that would create a cycle).
3. τ(I3 + K2) = 53  1 −3
5
2 = 20.
4. τ(W4) =

3+
√
5
2
4
+

3−
√
5
2
4
−2 = 45.
5. To illustrate the matrix-tree theorem, consider the following graph G. Then
1
2
4
3
D(G) −A(G) =





3
−1
−1
−1
−1
2
0
−1
−1
0
2
−1
−1
−1
−1
3




.
Deleting row 2 and column 3, for example, yields
τ(G) = (−1)2+3

3
−1
−1
−1
0
−1
−1
−1
3

= 8.
The eight spanning trees of G are

Section 9.3
ENUMERATING TREES
711
1
2
4
3
1
2
4
3
1
2
4
3
1
2
4
3
1
2
4
3
1
2
4
3
1
2
4
3
1
2
4
3
6. The recursive formula τ(G) = τ(G−e)+τ(G/e) is illustrated with the same graph G
of the previous example and with e = v1v4. In the computation G is drawn instead of
writing τ(G), and similarly with the other graphs. This yields
1
2
3
4
=
1
2
3
4
+
3
1⋅4
2
7. Let the vertices of K5 be v0, v1, v2, v3, v4. The number of spanning trees of K5 in which
the degrees of v0, v1, v2, v3, v4 are 3, 2, 1, 1, 1, respectively, is given by the multinomial
coeﬃcient
 5−2
3−1,2−1,1−1,1−1,1−1

=
3!
2!·1!·0!·0!·0! =
6
2·1·1·1·1 = 3. The three trees in question
are
v0
v1
v0
v0
v1
v1
v4
v4
v4
v3
v2
v3
v2
v3
v2
9.3
ENUMERATING TREES
Tree counting began with Arthur Cayley in 1889, who wanted to enumerate the saturated
hydrocarbons. George P´olya developed an extensive theory in 1937 for counting families
of organic chemicals, which was used by Richard Otter in 1948 in his solution of the
speciﬁc problem of counting saturated hydrocarbons. Tree counting formulas are used
in computer science to estimate running times in the design of algorithms.
9.3.1
COUNTING GENERIC TREES
When counting generic trees, we must be careful to distinguish among labeled trees,
rooted trees, unlabeled trees, and various other objects.
While labeled trees can be
counted easily, unlabeled trees, both rooted and unrooted, are typically counted using
generating functions.
Deﬁnitions:
A labeled tree is a tree in which distinct labels, typically v1, v2, . . . , vn, have been
assigned to the vertices. Two labeled trees with the same set of labels are considered the
same only if there is an isomorphism from one to the other that preserves the labels.

712
Chapter 9
TREES
A rooted tree is a tree in which one vertex, the root, is distinguished. Two rooted trees
are considered the same only if there is an isomorphism from one to the other that maps
the root of the ﬁrst to the root of the second.
A rooted labeled tree is a labeled tree in which one vertex, the root, is distinguished.
Two rooted labeled trees with the same set of labels are considered the same only if there
is an isomorphism from one to the other that preserves the labels and maps the root of
the ﬁrst to the root of the second.
A reduced tree (or irreducible tree) is a tree with no vertices of degree 2.
Examples:
1. The ﬁgure below shows the three isomorphically distinct trees with ﬁve vertices.
There are 60 essentially diﬀerent ways to label each of the ﬁrst two and ﬁve essentially
diﬀerent ways to label the third. Thus there are 125 diﬀerent labeled trees with ﬁve
vertices.
2. There are three essentially diﬀerent ways to root the ﬁrst tree in the ﬁgure of Ex-
ample 1, four essentially diﬀerent ways to root the second, and two essentially diﬀerent
ways to root the third. Thus there are nine rooted (unlabeled) trees with ﬁve vertices.
3. Each of the 125 labeled trees discussed in Example 1 can be rooted at any of its ﬁve
vertices, yielding 625 possible rooted labeled trees.
4. The third tree in the ﬁgure of Example 1 is the only reduced tree with ﬁve vertices.
Facts:
1. The number of labeled trees with n vertices is nn−2. See Table 1. This is sequence
A000272 in [OEIS].
2. The number of rooted labeled trees with n vertices is nn−1. See Table 1. This is
sequence A000169 in [OEIS].
Table 1: Labeled trees and rooted labeled trees with n vertices.
n
Labeled Trees
Rooted Labeled Trees
1
1
1
2
1
2
3
3
9
4
16
64
5
125
625
6
1,296
7,776
7
16,807
117,649
8
262,144
2,097,152
9
4,782,969
43,046,721
10
100,000,000
1,000,000,000
11
2,357,947,691
25,937,424,601
12
61,917,364,224
743,008,370,688

Section 9.3
ENUMERATING TREES
713
n
Labeled Trees
Rooted Labeled Trees
13
1,792,160,394,037
23,298,085,122,481
14
56,693,912,375,296
793,714,773,254,144
15
1,946,195,068,359,375
29,192,926,025,390,625
16
72,057,594,037,927,936
1,152,921,504,606,846,980
3. Let Rn denote the number of (unlabeled) rooted trees with n vertices, and let r(x)
be the generating function for rooted trees, so that
r(x) =
∞
X
n=1
Rnxn = x + x2 + 2x3 + 4x4 + 9x5 + 20x6 + · · · .
The coeﬃcients Rn of this generating function can be determined by means of the recur-
rence relation
r(x) = x
∞
Y
k=1
(1 −xk)−Rk.
See Table 2. This is sequence A000081 in [OEIS].
Table 2: Rooted trees, trees, and reduced trees with n vertices.
n
Rooted Trees
Trees
Reduced Trees
1
1
1
1
2
1
1
1
3
2
1
0
4
4
2
1
5
9
3
1
6
20
6
2
7
48
11
2
8
115
23
4
9
286
47
5
10
719
106
10
11
1,842
235
14
12
4,766
551
26
13
12,486
1,301
42
14
32,973
3,159
78
15
87,811
7,741
132
16
235,381
19,320
249
17
634,847
48,629
445
18
1,721,159
123,867
842
19
4,688,676
317,955
1,561
20
12,826,228
823,065
2,988
21
35,221,832
2,144,505
5,671
22
97,055,181
5,623,756
10,981
23
268,282,855
14,828,074
21,209
24
743,724,984
39,299,897
41,472

714
Chapter 9
TREES
n
Rooted Trees
Trees
Reduced Trees
25
2,067,174,645
104,636,890
81,181
26
5,759,636,510
279,793,450
160,176
27
16,083,734,329
751,065,460
316,749
28
45,007,066,269
2,023,443,032
629,933
29
126,186,554,308
5,469,566,585
1,256,070
30
354,426,847,597
14,830,871,802
2,515,169
4. Let Tn denote the number of trees with n vertices, and let t(x) be the generating
function for trees, so that
t(x) =
∞
X
n=1
Tnxn = x + x2 + x3 + 2x4 + 3x5 + 6x6 + · · · .
The coeﬃcients Tn of this generating function t(x) can be determined from the generating
function r(x) for rooted trees in Fact 3 above by using the formula
t(x) = r(x) −1
2
 r2(x) −r(x2)

.
See Table 2. This is sequence A000055 in [OEIS].
5. Counting reduced trees requires an auxiliary sequence Qn with generating function
q(x), so that
q(x) =
∞
X
k=1
Qkxk = x + x3 + x4 + 2x5 + 3x6 + 6x7 + 10x8 + · · · .
The coeﬃcients Qi of this generating function can be determined from the recurrence
relation
q(x) =
x
1 + x
∞
Y
k=1
(1 −xk)−Qk.
This is sequence A001678 in [OEIS].
6. Let Hn denote the number of reduced trees with n vertices, and let h(x) be the
generating function for reduced trees, so that
h(x) =
∞
X
n=1
Hnxn = x + x2 + x4 + x5 + 2x6 + 2x7 + 4x8 + · · · .
The coeﬃcients Hn of this generating function h(x) can be determined from the auxiliary
function q(x) in Fact 5 above by using the formula
h(x) = (1 + x)q(x) −
1 + x
2

q2(x) +
1 −x
2

q(x2).
See Table 2. Note that there are no reduced trees with exactly 3 vertices. This is sequence
A000014 in [OEIS].

Section 9.3
ENUMERATING TREES
715
9.3.2
COUNTING TREES IN CHEMISTRY
Deﬁnitions:
A 1-4 tree is a tree in which each vertex has degree 1 or 4.
A 1-rooted 1-4 tree is a 1-4 tree rooted at a vertex of degree 1.
Remarks:
1. The 1-4 trees model many types of organic chemical molecules such as saturated
hydrocarbons or alkanes.
These molecules have the chemical formula CnH2n+2 and
consist of n carbon atoms of valence 4 and 2n + 2 hydrogen atoms of valence 1.
2. The 1-rooted 1-4 trees model the monosubstituted hydrocarbons such as the alcohols
with the chemical formula CnH2n+1OH and consisting of n carbon atoms, 2n+1 hydrogen
atoms, and an OH group.
3. It is convenient when counting alcohols to include the water molecule HOH as an
honorary alcohol.
Examples:
1. The ﬁgure below shows the three diﬀerent 1-4 trees with ﬁve vertices of degree 4 and
12 vertices of degree 1.
H
H
H
H
H
H
H
H
H
H
H
H
C
C
H
H
H
H
H
H
H
C
H
H
H
C
H
H
H
C
C
C
C
C
C
H
C
H
H
H
C
C
H
H
H
H
C
H
H
H
C
2. The ﬁrst 1-4 tree in Example 1 can be rooted at a vertex of degree 1 in three essentially
diﬀerent ways, the second in four essentially diﬀerent ways, and the third in essentially
only one way. Thus there are eight diﬀerent 1-rooted 1-4 trees with ﬁve vertices of degree
4.
Facts:
1. A 1-4 tree with n vertices of degree 4 always has 2n + 2 vertices of degree 1.
2. Let An denote the number of 1-rooted 1-4 trees with n vertices of degree 4, and let
a(x) be the generating function for the number of 1-rooted 1-4 trees, so that
a(x) =
∞
X
n=0
Anxn = 1 + x + x2 + 2x3 + 4x4 + 8x5 + 17x6 + · · · .
The coeﬃcients An of this generating function a(x) can be determined from the recur-
rence relation
a(x) = 1 + x
6
 a3(x) + 3a(x)a(x2) + 2a(x3)

.
See Table 3. This is sequence A000598 in [OEIS].

716
Chapter 9
TREES
Table 3: 1-Rooted 1-4 trees and 1-4 trees with n vertices of degree 4.
n
1-Rooted 1-4 Trees
1-4 trees
1
1
1
2
1
1
3
2
1
4
4
2
5
8
3
6
17
5
7
39
9
8
89
18
9
211
35
10
507
75
11
1,238
159
12
3,057
355
13
7,639
802
14
19,241
1,858
15
48,865
4,347
16
124,906
10,359
17
321,198
24,894
18
830,219
60,523
19
2,156,010
148,284
20
5,622,109
366,319
21
14,715,813
910,726
22
38,649,152
2,278,658
23
101,821,927
5,731,580
24
269,010,485
14,490,245
25
712,566,567
36,797,588
26
1,891,993,344
93,839,412
27
5,034,704,828
240,215,803
28
13,425,117,806
617,105,614
29
35,866,550,869
1,590,507,121
30
95,991,365,288
4,111,846,763
3. Counting (unrooted) 1-4 trees requires ﬁrst counting 1-4 trees rooted at a vertex of
degree 4. Let Zn be the number of 4-rooted 1-4 trees with n vertices of degree 4, and let
z(x) be the generating function for the number of 4-rooted 1-4 trees, so that
z(x) =
∞
X
n=1
Znxn = x + x2 + 2x3 + 4x4 + 9x5 + 18x6 + · · · .
The coeﬃcients Zn of this generating function z(x) can be obtained by using the formula
z(x) =
x
24
 a4(x) + 6a2(x)a(x2) + 8a(x)a(x3) + 3a2(x2) + 6a(x4)

,
where a(x) is the generating function for 1-rooted 1-4 trees from Fact 2. This is sequence
A000678 in [OEIS].

Section 9.3
ENUMERATING TREES
717
4. Let Bn denote the number of (unrooted) 1-4 trees with n vertices of degree 4, and
let b(x) be the generating function for 1-4 trees, so that
b(x) =
∞
X
n=0
Bnxn = 1 + x + x2 + x3 + 2x4 + 3x5 + 5x6 + · · · .
The coeﬃcients Bn of this generating function b(x) can be determined from the functions
a(x) and z(x) from Facts 2 and 3, respectively, by using the formula
b(x) = z(x) + a(x) −1
2
 a2(x) −a(x2)

.
See Table 3. This is sequence A000602 in [OEIS].
9.3.3
COUNTING TREES IN COMPUTER SCIENCE
Deﬁnitions:
A binary tree consists of a root vertex and at most two principal subtrees that are
themselves binary trees.
Each principal subtree must be speciﬁed as either the left
subtree or the right subtree. The root vertex of a binary tree is joined by an edge to the
root of each principal subtree.
An ordered tree consists of a root vertex and a sequence t1, t2, . . . , tm of m ≥0 principal
subtrees that are themselves ordered trees.
The children of the root vertex of an ordered tree or a binary tree are the roots of the
principal subtrees.
A full binary tree (sometimes called a left-right tree) is a binary tree in which each
vertex has either 0 or 2 children.
Examples:
1. The following ﬁgure shows the ﬁve binary trees with three vertices.
2. The next ﬁgure shows the ﬁve ordered trees with four vertices.
3. The following ﬁgure shows the ﬁve full binary trees with seven vertices.

718
Chapter 9
TREES
Remarks:
1. In computer science, trees are usually drawn with the root at the top.
2. Binary trees are easily represented in a computer. Other types of trees are often
converted into binary trees for computer representation.
3. Ordered trees are used to represent structures such as family trees, showing all descen-
dants of a person represented by the root. The roots of the principal subtrees represent
the children of the root person, in order of birth.
4. Full binary trees are frequently used to represent arithmetic expressions, in which the
leaves correspond to numbers and the other vertices represent binary operations such as
+ , −, × , or ÷ .
Facts:
1. The number of binary trees with n vertices is the Catalan number Cn (see §3.1.3)
given by the formula
Cn =
1
n + 1
2n
n

=
(2n)!
(n + 1)! n!
for n ≥0.
See Table 4. This is sequence A000108 in [OEIS].
Table 4: The Catalan numbers.
n
Cn
n
Cn
1
1
17
129,644,790
2
2
18
477,638,700
3
5
19
1,767,263,190
4
14
20
6,564,120,420
5
42
21
24,466,267,020
6
132
22
91,482,563,640
7
429
23
343,059,613,650
8
1,430
24
1,289,904,147,324
9
4,862
25
4,861,946,401,452
10
16,796
26
18,367,353,072,152
11
58,786
27
69,533,550,916,004
12
208,012
28
263,747,951,750,360
13
742,900
29
1,002,242,216,651,368
14
2,674,440
30
3,814,986,502,092,304
15
9,694,845
31
14,544,636,039,226,909
16
35,357,670
32
55,534,064,877,048,198
2. The number of ordered trees with n vertices is the Catalan number Cn−1. See Table 4.
3. The number of full binary trees with 2n + 1 vertices is also Cn. See Table 4.

REFERENCES
719
REFERENCES
Printed Resources:
[Ca89] A. Cayley, A theorem on trees, Quarterly Journal of Pure and Applied Mathe-
matics 23 (1889), 376–378.
[GrYeZh13] J. Gross, J. Yellen, and P. Zhang, Handbook of Graph Theory, 2nd ed.,
Chapman and Hall/CRC, 2013.
[HaPa73] F. Harary and E. M. Palmer, Graphical Enumeration, Academic Press, 1973.
[P´oRe87] G. P´olya and R. C. Read, Combinatorial Enumeration of Groups, Graphs, and
Chemical Compounds, Springer-Verlag, 1987.
[Ro11] K. H. Rosen, Discrete Mathematics and Its Applications, 7th ed., McGraw-Hill,
2011.
[Se80] J. P. Serre, Trees, Springer-Verlag, 1980.
[Ta83] R. E. Tarjan, Data Structures and Network Algorithms, SIAM, 1983.
[We01] D. B. West, Introduction to Graph Theory, 2nd ed., Prentice-Hall, 2001.
Web Resources:
[OEIS] oeis.org (The On-Line Encyclopedia of Integer Sequences.)
www.cs.stonybrook.edu/~algorith/ (The Stony Brook Algorithm Repository.)


10
NETWORKS AND FLOWS
10.1 Minimum Spanning Trees
J. B. Orlin and
10.1.1 Basic Concepts
Ravindra K. Ahuja
10.1.2 Algorithms for Minimum Spanning Trees
10.1.3 Applications
10.2 Matchings
Douglas R. Shier
10.2.1 Basic Concepts
10.2.2 Matchings in Bipartite Networks
10.2.3 Matchings in Nonbipartite Networks
10.2.4 Stable Matchings
10.2.5 Applications
10.3 Shortest Paths
J. B. Orlin and
10.3.1 Basic Concepts
Ravindra K. Ahuja
10.3.2 Algorithms for Single-Source Shortest Paths
10.3.3 Algorithms for All-Pairs Shortest Paths
10.3.4 Parallel Algorithms
10.3.5 Applications
10.4 Maximum Flows
J. B. Orlin and
10.4.1 Basic Concepts
Ravindra K. Ahuja
10.4.2 Algorithms for Maximum Flows
10.4.3 Applications
10.5 Minimum Cost Flows
J. B. Orlin and
10.5.1 Basic Concepts
Ravindra K. Ahuja
10.5.2 Algorithms for Minimum Cost Flows
10.5.3 Applications
10.6 Communication Networks
David Simchi-Levi
10.6.1 Capacitated Minimum Spanning Tree Problem
Sunil Chopra and
10.6.2 Capacitated Concentrator Location Problem
M. Gisela Bardossy
10.6.3 Network Design Problem
10.6.4 Models for Survivable Networks
10.6.5 Connected Facility Location Problem
10.6.6 Regenerator Location Problem
10.7 Difﬁcult Routing and Assignment Problems
Bruce L. Golden
10.7.1 Traveling Salesman Problem
Bharat K. Kaku and
10.7.2 Vehicle Routing Problem
Xingyin Wang
10.7.3 Quadratic Assignment Problem
10.8 Small-World Networks
Vladimir Boginski
10.8.1 Basic Concepts
Jongeun Kim and
10.8.2 Random Graph Models
Vladimir Stozhkov

722
Chapter 10
NETWORKS AND FLOWS
10.8.3 Watts-Strogatz Model
10.8.4 Kleinberg’s Geographical Small-World Model
10.8.5 Power-Law Random Networks and Their Small-World Properties
10.8.6 Evolving Random Networks with Small-World Properties
10.8.7 Applications
10.9 Network Representations and Data Structures
Douglas R. Shier
10.9.1 Network Representations
10.9.2 Tree Data Structures
INTRODUCTION
The vertices and edges of a graph often have quantitative information associated with
them, such as supplies and demands (for vertices), and distance, length, capacity, and
cost (for edges). Relative to such networks, a number of discrete optimization problems
arise in a variety of disciplines: statistics, electrical engineering, operations research,
combinatorics, and computer science. Typical applications include designing least cost
telecommunication systems, maximizing throughput in a manufacturing system, ﬁnding
a minimum cost route or set of routes for delivery vehicles, and distributing electricity
from a set of supply points to meet customer demands at minimum cost. In this chapter,
a number of classical network optimization problems are studied and algorithms are
described for their exact or approximate solution. Models and analysis of small-world
networks are also presented.
GLOSSARY
adjacency matrix: a 0-1 matrix whose (i, j) entry indicates the absence or presence,
respectively, of an arc joining vertex i to vertex j in a graph.
adjacency set: the set of arcs emanating from a speciﬁed vertex.
alternating path (in a matching): a path with edges that are alternately free and
matched.
arc list: a list of the arcs of a graph, presented in no particular order.
assignment (from a set S to a set T ): a bijective function from S onto T .
augmenting path (in a ﬂow network): a directed path between two speciﬁed vertices
in which each arc has a positive residual capacity.
augmenting path (in a matching): an alternating path between two free vertices.
backbone network: a collection of devices that interconnect vertices at which message
exchanges occur in a communication network.
Barab´asi-Albert model: a model of network formation in which a new vertex is (pref-
erentially) joined to existing vertices based on their current degree in the network.
blossom: an odd length cycle formed by adding an edge joining two even vertices on
an alternating path.
blocking pair (relative to a matching): a currently unmatched pair of individuals, each
of which prefers one another to their current mate in the matching.

GLOSSARY
723
capacitated concentrator location problem: an optimization problem in which a
minimum cost conﬁguration of concentrators and their connections to terminals are
sought so that each concentrator’s total capacity is not exceeded.
capacitated minimum spanning tree: a minimum cost collection of subtrees joined
to a speciﬁed root vertex, in which the total amount of demand generated by each
subtree is bounded above by a constant.
capacitated network: a network in which each arc is assigned a capacity.
capacity (of an arc): the maximum amount of material that can ﬂow along the arc.
capacity (of a cut [S, S]): the sum of the capacities of arcs (i, j) with i ∈S and j ∈S.
capacity (of a path): the smallest capacity of any arc on the path.
capacity assignment problem: an optimization problem in which links of diﬀerent
capacities are to be installed at minimum cost to support a number of point-to-point
communication demands.
clustering coeﬃcient (of a graph): a measure of the probability that two random
neighbors of a vertex are neighbors of each other.
complete matching: in a bipartite graph G = (X ∪Y, E), a matching M in which
each vertex of X is incident with an edge of M.
composite (hybrid) method: a heuristic algorithm that combines elements of both
construction methods and improvement methods.
conﬁguration model: a model that generates graphs by randomly connecting half-
edges in a graph having a given degree sequence.
connected facility location problem: an optimization problem in which certain fa-
cilities are to be opened and customers assigned to them at minimum total cost.
construction method: a heuristic algorithm that builds a feasible solution, starting
with a trivial conﬁguration.
cost (of a ﬂow): P
(i,j) cijxij, where cij is the cost and xij is the ﬂow on arc (i, j).
cut (in a graph): the set of edges [S, S] in the graph joining vertices in S to vertices in
the complementary set S.
degree distribution (of a graph): the proportion of vertices having each degree in the
graph.
directed network: a vertex set V and an arc set E, where each directed arc has an
associated cost, length, weight, or capacity.
directed out-tree: a tree rooted at vertex s such that the unique path in the tree from
vertex s to every other vertex is a directed path.
distance label: an estimate (in particular, an upper bound) on the shortest path length
from the source vertex to each network vertex.
Dorogovtsev-Mendes model: a small-world network model that constructs a regular
(low-dimensional) lattice and then adds an extra vertex, randomly connected to
existing vertices in the lattice.
even vertex (in an alternating path P): a vertex on P that is reached using an even
number of edges of P, starting from the origin vertex of P.
exact algorithm: a procedure that produces a veriﬁable optimal solution to every
problem instance.

724
Chapter 10
NETWORKS AND FLOWS
ﬂow: a feasible assignment of material that satisﬁes ﬂow conservation and arc capacity
restrictions.
forward star: a compact representation of a graph in which information about arcs
leaving a vertex is stored using consecutive locations of an array.
free edge (in a matching): an edge that does not appear in the matching.
free vertex (in a matching): a vertex that is not incident with any matched edge.
generalized random graph model: a model that generates graphs by adding edges
with probability proportional to the product of the given vertex degrees.
heuristic algorithm: a procedure that produces a feasible, though not necessarily op-
timal, solution to every problem instance.
improvement method: a heuristic algorithm that starts with a suboptimal solution
(often randomly generated) and attempts to improve it.
Jackson-Rogers model: a two-step model of network formation in which a new vertex
is joined randomly to a set of parental vertices and then is joined to some of the
neighbors of these parental vertices.
Kleinberg small-world model: a modiﬁed version of the Newman-Watts model in
which edges are randomly added, based on the network distance between their asso-
ciated vertices.
Klemm-Egu´ıluz model: a model of network formation in which a new vertex is joined
either to an existing vertex or to a dynamically changing set of active vertices.
length (of a path): the sum of all costs appearing on the arcs of the path.
linear assignment problem (LAP): an optimization problem in which an assign-
ment is sought that minimizes an appropriate set-up cost.
link capacity: an upper bound on the amount of traﬃc that a communication link can
carry at any one time.
linked adjacency list: a collection of singly-linked lists used to represent a graph.
local access network: a network used to transfer traﬃc between the backbone net-
work and the end users.
matched edge (in a matching): an edge that appears in the matching.
matched vertex (in a matching): a vertex that is incident with a matched edge.
matching (in a graph): a set of pairwise nonadjacent edges in the graph.
mate (of a matched vertex): in a matching, the other endpoint of the matched edge
incident with the given vertex.
maximum ﬂow (in a network): a ﬂow in the network having maximum value.
maximum size matching: a matching having the largest size.
maximum spanning tree (of a network): a spanning tree of the network with maxi-
mum cost.
maximum weight matching: a matching having the largest weight.
metaheuristic: a general-purpose heuristic procedure (such as tabu search, simulated
annealing, genetic algorithms, or neural networks) for solving diﬃcult optimization
problems.
minimum cost ﬂow (in a network): a ﬂow in the network having minimum cost.

GLOSSARY
725
minimum cut (in a network): a cut in the network having minimum capacity.
minimum spanning tree (of a network): a spanning tree of the network with mini-
mum cost.
negative cycle: a directed cycle of negative cost (or length).
network design problem: an optimization problem in which a minimum cost assign-
ment of diﬀerent capacity cables to edges is sought that satisﬁes given point-to-point
demands and that respects all edge capacities.
Newman-Watts model: a small-world network model that constructs a regular (low-
dimensional) lattice and then adds a random number of extra edges.
odd vertex (in an alternating path P): a vertex on P that is reached using an odd
number of edges of the path P, starting from the origin vertex of P.
perfect matching: a matching in a graph in which each vertex of the graph is incident
with exactly one edge of the matching.
power-law distribution: a distribution in which the proportion pk of vertices having
degree k is given by pk = Ck−β for some ﬁxed parameter β.
predecessor: relative to a rooted tree, the vertex preceding a given vertex on the unique
path from the root to the given vertex.
preﬂow: a relaxation of ﬂow where inﬂow into a vertex can be greater than its outﬂow.
pseudoﬂow: a relaxation of ﬂow where inﬂow into a vertex need not be equal to its
outﬂow.
quadratic assignment problem (QAP): an optimization problem in which an as-
signment is sought that minimizes the sum of set-up and interaction costs.
random graph: a graph obtained by randomly adding edges between a ﬁxed number
of vertices.
reduced cost of arc (i, j): relative to given vertex potentials π, the quantity cπ
ij = cij −
π(i) + π(j).
regenerator location problem: an optimization problem in which a minimum cost
placement of regenerators is sought so that all terminal vertices can communicate
with one another without loss of signal quality.
regular lattice: a graph obtained by placing regularly-spaced vertices and then joining
each vertex to its closest neighbors.
residual capacity (of an arc): the maximum additional ﬂow (with respect to a given
ﬂow) that can be sent on an arc.
residual network: a network consisting of arcs with positive residual capacity.
s-t cut: a cut [S, S] in which s ∈S and t ∈S.
savings: the reduction in cost from joining two vertices directly compared to joining
both to a central vertex.
shortest path: a directed path between speciﬁed vertices having minimum total cost
(or length).
size (of a matching): the number of edges in the matching.
stable matching: a matching that contains no blocking pairs.
stable partners: individuals that are matched in some stable matching.

726
Chapter 10
NETWORKS AND FLOWS
survivable network: a network that can survive failures in some of its vertices or edges
and still transfer a prespeciﬁed amount of traﬃc.
traveling salesman problem (TSP): an optimization problem in which a ﬁxed set
of cities must be visited in some order at minimum total cost.
two-phase method: a heuristic algorithm that implements a cluster ﬁrst/route second
philosophy.
undirected network: a vertex set V and an edge set E, where each undirected edge
has an associated cost, length, weight, or capacity.
value of a ﬂow: the total ﬂow leaving the source vertex.
vehicle routing problem (VRP): an optimization problem in which a given set of
customers must be serviced at minimum total cost, using a ﬂeet of vehicles having
ﬁxed capacity.
vertex potential: a quantity π(i) associated with each vertex i of a network.
Watts-Strogatz model: a small-world network model that constructs a regular (low-
dimensional) lattice and then repositions a random number of its edges.
weight (of a matching): the sum of the weights of edges in the matching.
10.1
MINIMUM SPANNING TREES
In an undirected network, the minimum spanning tree problem is the problem of iden-
tifying a spanning tree of the network that has the smallest possible sum of edge costs.
This problem arises in a number of applications, both as a stand-alone problem and as
a subproblem in more complex problem settings. It is assumed throughout this section
that the network is connected.
10.1.1
BASIC CONCEPTS
Deﬁnitions:
An undirected network is a weighted graph (§8.1.1) G = (V, E), where V is the set
of vertices, E is the set of undirected edges, and each edge (i, j) ∈E has an associated
cost (or weight, length) cij. Let n = |V | and m = |E|.
If T = (V, F) is a spanning tree (§9.2) of G = (V, E), then every edge in F ⊆E is a tree
edge and every edge in E −F is a nontree edge (or chord).
A minimum spanning tree (MST) of G is a spanning tree of G for which the sum of
the edge costs is minimum.
A maximum spanning tree of G is a spanning tree of G for which the sum of the edge
costs is maximum.
A cut of G = (V, E) is a partition of the vertex set V into two parts, S and S = V −S.
Each cut deﬁnes the set of edges [S, S] ⊆E having one endpoint in S and the other
endpoint in S.

Section 10.1
MINIMUM SPANNING TREES
727
Facts:
1. Every spanning tree T of a network G with n vertices contains exactly n −1 edges,
and every two vertices of T are connected by a unique path.
2. Adding an edge to a spanning tree of G produces a unique cycle, called a fundamental
cycle (§9.2.1).
3. Every cut [S, S] is a disconnecting set of edges (§8.4.2). However, not every discon-
necting set of edges can be represented as a cut [S, S]; see Example 4.
4. Removing an edge from a spanning tree of G produces two subtrees, on vertex sets S
and S, respectively. The associated cut [S, S] is called a fundamental cut.
5. Path optimality conditions: A spanning tree T ∗is a minimum spanning tree of G if
and only if for each nontree edge (k, l) of G, cij ≤ckl holds for all tree edges (i, j) in the
fundamental cycle determined by edge (k, l).
6. Cut optimality conditions:
A spanning tree T ∗is a minimum spanning tree of G if
and only if for each tree edge (i, j) ∈T ∗, cij ≤ckl holds for all nontree edges (k, l) in the
fundamental cut determined by edge (i, j).
7. If all edge costs are diﬀerent, then the minimum spanning tree is unique.
8. The minimum spanning tree can be unique even if some of the edge costs are equal;
see Example 3.
9. Adding a constant to all edge costs of an undirected network does not change the
minimum spanning tree(s) of the network. Thus, it is suﬃcient to have an algorithm
that works when all edge costs are positive.
10. Multiplying each edge cost of an undirected network by −1 converts a maximum
spanning tree into a minimum spanning tree, and vice versa. Thus, it is suﬃcient to have
algorithms to ﬁnd a minimum spanning tree.
Examples:
1. Part (a) of the following ﬁgure shows an undirected network G, with costs indicated
on each edge. Part (b) shows a spanning tree T ∗of G.
Adding the nontree edge (3, 5) to T ∗produces the fundamental cycle [3, 1, 2, 5, 3]; see
part (c). Since each tree edge in this cycle has cost no more than that of the nontree
edge (3, 5), the path optimality condition is satisﬁed by edge (3, 5). Similarly, it can be
veriﬁed that the other nontree edges, namely (2, 3), (4, 5), and (5, 6), satisfy the path

728
Chapter 10
NETWORKS AND FLOWS
optimality conditions, establishing by Fact 5 that T ∗is a minimum spanning tree. By
Fact 7 this is the unique minimum spanning tree.
2. For the tree edge (1, 2) in part (b) of the ﬁgure of Example 1, the fundamental cut
[S, S] formed by deleting edge (1, 2) has S = {1, 3} and S = {2, 4, 5, 6}; see part (d) of the
ﬁgure. This cut contains two nontree edges, (2, 3) and (3, 5). Since each such nontree edge
has cost greater than or equal to that of the tree edge (1, 2), the cut optimality condition
is satisﬁed for edge (1, 2). Similarly, it can be veriﬁed that the other tree edges, namely
(1, 3), (2, 4), (2, 5), and (4, 6), satisfy the cut optimality conditions, establishing by Fact
6 that T ∗is a minimum spanning tree.
3. The undirected network in part (a) of the following ﬁgure has four vertices and ﬁve
edges, with the edge cost shown beside each edge. This network contains eight spanning
trees, which are listed in the table in part (b) of the ﬁgure. The spanning tree T5 achieves
the minimum cost 7 among all the spanning trees and so is a minimum spanning tree.
In fact, T5 is the unique minimum spanning tree, even though the edge costs are not all
distinct. See Fact 8.
1
2
4
3
(a)
2
2
3
3
5
Spanning Tree
Edges
Total
Cost
T1
T2
T3
T4
T5
T6
T7
T8
10
10
8
8
7
10
10
9
(1,2), (1,3) (2,4)
(1,2), (1,3) (3,4)
(1,2), (2,3) (2,4)
(1,2), (2,3) (3,4)
(1,2), (2,4) (3,4)
(1,3), (2,3) (2,4)
(1,3), (2,3) (3,4)
(1,3), (2,4) (3,4)
(b)
4. The set of edges F = {(1, 2), (2, 4), (3, 4)} is a disconnecting set in the network G of
Example 3, since removal of these edges disconnects G. However, there is no partition
of the vertex set of G into nonempty sets S and S for which F = [S, S].
10.1.2
ALGORITHMS FOR MINIMUM SPANNING TREES
There are several greedy algorithms for constructing minimum spanning trees, based on
the optimality conditions in §10.1.1, Facts 5 and 6. Each of these algorithms myopically
(greedily) adds an edge to the current conﬁguration based on only local information;
nonetheless, these procedures are guaranteed to produce a minimum spanning tree.
Deﬁnitions:
The nearest neighbor operation takes as input a tree T ∗having vertex set S and
produces a minimum cost edge (i, j) in the cut [S, S ]; i.e., cij = min{cab | a ∈S, b /∈S}.
The merge operation takes as input an edge (i, j) whose two endpoints i and j belong
to disjoint trees Ti and Tj and combines the trees into Ti ∪Tj ∪{(i, j)}.
The graph G = (V, E) is assumed connected, with n vertices and m edges.
Facts:
1. Kruskal’s algorithm:
This greedy algorithm (Algorithm 1) is based on the path
optimality conditions (§10.1.1) and builds a minimum spanning tree by examining edges

Section 10.1
MINIMUM SPANNING TREES
729
of E one by one in nondecreasing order of their costs. The edge being examined is added
to the current forest if its addition does not create a cycle. (J. B. Kruskal, 1928–2010)
Algorithm 1:
Kruskal’s algorithm.
input: connected undirected network G
output: minimum spanning tree T ∗
order the edges (i1, j1), (i2, j2), . . . , (im, jm) so that ci1j1 ≤ci2j2 ≤· · · ≤cimjm
T ∗:= ∅
for k := 1 to m
if T ∗∪{(ik, jk)} does not contain a cycle then T ∗:= T ∗∪{(ik, jk)}
2. Kruskal’s algorithm can be terminated once n −1 edges have been added to T ∗.
3. Kruskal’s algorithm can be implemented in O(m log n) time. The bottleneck opera-
tion is sorting of the edge costs.
4. Algorithm 1 was independently discovered by Kruskal (1956) and by H. Loberman
and A. Weinberger (1957).
5. Prim’s algorithm:
This algorithm (Algorithm 2) is based on the cut optimality
conditions (§10.1.1). It maintains a single tree T ∗, which initially consists of an arbitrary
vertex i0. At each iteration, the algorithm adds the least cost edge emanating from T ∗
until a spanning tree is obtained. (R. C. Prim, born 1921)
Algorithm 2:
Prim’s algorithm.
input: connected undirected network G, vertex i0
output: minimum spanning tree T ∗
T ∗:= the tree consisting of vertex i0
while |T ∗| < n −1
(i, j) := nearest neighbor(T ∗)
T ∗:= T ∗∪{(i, j)}
6. Algorithm 2 was ﬁrst proposed in 1930 by V. Jarn´ık. Later it was independently
discovered by Prim (1957) and by E. W. Dijkstra (1959).
7. Running times of several implementations of Prim’s algorithm are shown in the fol-
lowing table. See [AhMaOr93] for a discussion of these implementations.
data structure
running time
binary heap
O(m log n)
d-heap
O(m logd n), with d = max{2, ⌈m
n ⌉}
Fibonacci heap
O(m + n log n)
8. Sollin’s algorithm:
This greedy algorithm (Algorithm 3) is also based on the cut
optimality conditions (§10.1.1). It starts with a forest of n trees, each consisting of a
single vertex, and builds a minimum spanning tree by repeatedly adding edges to the
current forest. At each iteration a least cost edge emanating from each tree is added,
leading to the merging of certain trees.

730
Chapter 10
NETWORKS AND FLOWS
Algorithm 3:
Sollin’s algorithm.
input: connected undirected network G
output: minimum spanning tree T ∗
T ∗:= forest of all vertices of G, but no edges
while |T ∗| < n −1
let T1, T2, . . . , Tp be the trees in the forest T ∗
for k := 1 to p
(ik, jk) := nearest neighbor(Tk)
for k := 1 to p
if ik and jk belong to diﬀerent trees then
merge(ik, jk)
T ∗:= T ∗∪{(ik, jk)}
9. Each iteration of Algorithm 3 reduces the number of trees in the forest T ∗by at least
half.
10. Sollin’s algorithm performs O(log n) iterations and can be implemented to run in
O(m log n) time; see [AhMaOr93].
11. A variation of Sollin’s algorithm that runs in time O(m log log n) can be found in
[Ya75].
12. The origins of Algorithm 3 can be traced to O. Bor˚uvka (1926), who ﬁrst formulated
the minimum spanning tree problem in the context of electric power networks. This
algorithm was independently proposed in 1938 by G. Choquet for points in a metric
space and by G. Sollin in 1961 for arbitrary networks.
13. Sollin’s algorithm can be easily parallelized in EREW (exclusive-read, exclusive-
write) PRAM (parallel random access machine). (See §17.1.4.) This algorithm assigns a
processor to each edge and each vertex of the network [KiLe88].
14. Chong, Han, and Lam [ChHaLa01] developed an algorithm that runs in O(log n)
time using m parallel processors. This running time is best possible.
15. Computational studies have found that the Prim and Sollin algorithms consistently
outperform Kruskal’s algorithm. Prim’s algorithm is faster when the network is dense,
whereas Sollin’s algorithm is faster when the network is sparse.
16. An excellent discussion of the history of the minimum spanning tree problem is
provided in [GrHe85].
17. The fastest deterministic algorithms for the minimum spanning tree problem are
due to Chazelle [Ch00] and to Pettie and Ramachandran [PeRa02]. Chazelle’s algorithm
runs in O(mα(n, m)) time where α(n, m) is the inverse Ackermann function, which for
all practical purposes is less than 5.
Pettie and Ramachandran show how to ﬁnd a
minimum spanning tree in O(T ∗(n, m)) time, where T ∗(n, m) is the minimum number
of edge-weight comparisons needed to determine the solution.
18. Karger, Klein, and Tarjan [KaKlTa95] developed a randomized algorithm for the
minimum spanning tree problem that runs in O(m) time.
19. Computer codes that implement spanning tree algorithms can be found at
• http://www3.cs.stonybrook.edu/~algorith/files/minimum-spanning-tree.
shtml

Section 10.1
MINIMUM SPANNING TREES
731
20. An important variant of the minimum spanning tree problem places constraints on
the number of edges incident with a vertex in a candidate spanning tree. Such degree-
constrained minimum spanning trees are investigated in [GlKl75] and [Vo89].
21. Another variant is the capacitated minimum spanning tree problem, which arises
in the design of local access telecommunication networks. In this problem, a feasible
spanning tree is one rooted at a speciﬁed central vertex such that the total traﬃc (number
of calls) generated by each subtree connected to the central vertex does not exceed a
known capacity. A feasible spanning tree having minimum total cost is then sought.
(See §10.6.1 for further details.)
Examples:
1. For the network shown in part (a) of the following ﬁgure, ordering edges by nonde-
creasing cost produces the following sequence of edges: (2, 4), (3, 5), (3, 4), (2, 3), (4, 5),
(1, 2), (1, 3). Kruskal’s algorithm adds the edges (2, 4), (3, 5), (3, 4) to T ∗; discards the
edges (2, 3) and (4, 5); then adds the edge (1, 2) to T ∗and terminates. Part (b) of the
ﬁgure shows the resulting minimum spanning tree, having total cost 80.
1
1
2
2
4
4
3
5
3
5
35
35
15
(a)
(b)
15
20
20
30
25
40
10
10
2. Prim’s algorithm (Algorithm 2) is applied to the network in part (a) of the ﬁgure
for Example 1, starting with the initial vertex i0 = 3. The minimum cost edge out of
vertex 3 is (3, 5), so T ∗= {(3, 5)}. Next, the minimum cost edge emanating from T ∗
is (3, 4), giving T ∗= {(3, 5), (3, 4)}. Subsequent iterations add the edges (2, 4) and (1, 2),
producing the minimum spanning tree T ∗= {(3, 5), (3, 4), (2, 4), (1, 2)}. Starting from
any other initial vertex i0 would give the same result.
3. To apply Sollin’s algorithm (Algorithm 3) to the network in part (a) of the ﬁgure for
Example 1, begin with a forest containing ﬁve trees, each consisting of a single vertex.
Part (a) of the following ﬁgure shows the least cost edge emanating from each of these
trees. One iteration of Algorithm 3 produces the two trees shown in part (b) of the
following ﬁgure. The least cost edge emanating from either of these two trees is (3, 4).
Adding this edge completes the minimum spanning tree shown in part (c) of the ﬁgure.
35
1
2
4
5
5
1
1
2
4
5
3
4
2
(a)
(b)
(c)
3
3
10
15
15
20
35
10
35
10
20
15
10.1.3
APPLICATIONS
Minimum spanning tree problems arise both directly and indirectly. For direct appli-
cations, the points in a given set are to be connected using the least cost collection of

732
Chapter 10
NETWORKS AND FLOWS
edges. For indirect applications, creative modeling of the original problem recasts it as
a minimum spanning tree problem.
Applications:
1. Designing physical systems:
A minimum cost network is to be designed to connect
geographically dispersed system components. Each component is represented by a vertex,
with potential network connections between vertices represented by edges.
A cost is
associated with each edge.
2. Examples of Application 1 occur in the following:
• Connect terminals in cabling the panels of electrical equipment in order to use
the least total cost of wire.
• Construct a pipeline network to connect a number of towns using the smallest
possible total cost of pipeline.
• Link isolated villages in a remote region, which are connected by roads but not
yet by telephone service. The problem is to determine along which stretches of
roads to place telephone lines to link every pair of villages, using the minimum
total miles of installed lines.
• Construct a digital computer system, composed of high-frequency circuitry, when
it is important to minimize the length of wires between diﬀerent components
to reduce both capacitance and delay line eﬀects.
• Connect a number of computer sites by high-speed lines. Each line is available for
leasing at a certain monthly cost, and a conﬁguration is required that connects
all the sites at minimum overall cost.
• Design a backbone network of high-capacity links that connect switching devices
to support internet traﬃc. A minimum cost backbone network that maintains
acceptable throughput is required.
3. Clustering:
Objects having k measurable characteristics are to be clustered into
groups of “similar” objects. First, construct an undirected network, where each object is
represented by a vertex and every two distinct vertices are joined by an edge. The cost
of edge (i, j) is the distance (in k-dimensional space) between the k-vectors for objects i
and j. Applying Kruskal’s algorithm to this network then yields a hierarchy of partitions
of the vertex set; each partition is deﬁned by the trees comprising the forest obtained at
each iteration of Kruskal’s algorithm. Such a hierarchy is then used to deﬁne clusters of
the original objects. This application has been used to represent multidimensional gene
expression data [XuOlXu02].
4. Computation of minimum spanning trees sometimes arises as a subproblem in a larger
optimization problem. For example, one heuristic approach to the traveling salesman
problem (§10.7.1) involves the calculation of minimum spanning trees.
5. Optimal message passing:
An intelligence service has agents operating in a non-
friendly country. Each agent knows some of the other agents and has in place procedures
for arranging a rendezvous with someone he knows. For each such possible rendezvous,
say between agent i and agent j, any message passed between these agents will fall into
hostile hands with a certain probability pij. The group leader wants to transmit a conﬁ-
dential message among all the agents while maximizing the probability that no message
is intercepted.
If the agents are represented by vertices and each possible rendezvous by an edge, then
in the resulting graph G a spanning tree T is required that maximizes the probability
that no message is intercepted, given by Π(i,j)∈T (1 −pij). Such a tree can be found by

Section 10.2
MATCHINGS
733
deﬁning the cost of each edge (i, j) as log(1 −pij) and solving a maximum spanning tree
problem.
6. All-pairs minimax path problem:
In this variant of the shortest path problem
(see §10.3.1), the value of a path P is the maximum cost of an edge in P. The all-
pairs minimax path problem is to determine a minimum value path between every pair
of vertices in a network G. It can be shown that if T ∗is a minimum spanning tree of G,
then the unique path in T ∗between any pair of vertices is also a minimax path between
that pair of vertices.
7. Examples of Application 6 arise in the following contexts:
• Determine the trajectory of a spacecraft that keeps the maximum temperature of
the surface as small as possible.
• When traveling through a desert, select a route that minimizes the length of the
longest stretch between rest areas.
• A person traveling in a wheelchair desires a route that minimizes the maximum
ascent along the path segments of the route.
8. Measuring homogeneity of bimetallic objects: In this application minimum spanning
trees are used to determine the degree to which a bimetallic object is homogeneous in
composition. First, the composition of the bimetallic object is measured at a set of sample
points. A network is then constructed with vertices corresponding to the sample points
and with an edge connecting physically adjacent sample points. The cost of edge (i, j)
is the product of the physical (Euclidean) distance between sample points i and j, and a
homogeneity factor between 0 and 1. The homogeneity factor is 0 if the composition of
the corresponding samples is identical, and is 1 if the composition is very diﬀerent. This
cost structure gives greater weight to two points if they have diﬀerent compositions and
are far apart. Then the cost of the minimum spanning tree provides an overall measure
of the homogeneity of the object.
9. Additional applications, with reference sources, are given in the following table.
application
references
two-dimensional storage schemes
[AhMaOr93], [AhEtal95]
chemical physics
[AhMaOr93]
manufacturing
[EvMi92]
network design
[AhMaOr93]
network reliability
[AhEtal95]
pattern classiﬁcation
[AhMaOr93], [GrHe85]
image segmentation
[FeHu04]
automatic speech recognition
[GrHe85]
numerical taxonomy
[GrHe85]
handwriting recognition
[TaRo04]
10.2
MATCHINGS
In an undirected network, the maximum matching problem is to ﬁnd a set of nonadjacent
edges that has the largest total size or weight. This discrete optimization problem arises
in a number of applications, often involving the optimal pairing of a set of objects.

734
Chapter 10
NETWORKS AND FLOWS
10.2.1
BASIC CONCEPTS
Deﬁnitions:
Let G = (V, E) be an undirected network with vertex set V and edge set E (see §10.1.1).
Assume that G contains neither loops nor multiple edges. Each edge e = (i, j) ∈E has
an associated weight we = wij. Let n = |V | and m = |E|.
The degree of vertex v ∈V in G is the number of edges in G that are incident with v,
written deg(v). (See §8.1.1.)
A matching in G = (V, E) is a set M ⊆E of pairwise nonadjacent edges (§8.1.1).
A perfect matching in G = (V, E) is a matching M in which each vertex of V is
incident with exactly one edge of M.
A vertex cover in G = (V, E) is a set S of vertices such that every edge of G is incident
with some vertex in S.
The size (cardinality) of a matching M is the number of edges in M, written |M|.
The weight of a matching M is wt(M) = P
e∈M we.
A maximum size matching of G is a matching M having the largest size |M|.
A maximum weight matching of G is a matching M having the largest weight wt(M).
Relative to a matching M in G = (V, E), edges e ∈M are matched edges, while edges
e ∈E −M are free edges. Vertex v is matched if it is incident with a matched edge;
otherwise vertex v is free.
Every matched vertex v has a mate, the other endpoint of the matched edge incident
with v.
With respect to a matching M, the weight wt(P) of path P is the sum of the weights
of the free edges in P minus the sum of the weights of the matched edges in P.
An alternating path has edges that are alternately free and matched. An augmenting
path is an alternating path that starts and ends at a free vertex.
Facts:
1. Matchings are useful in a wide variety of applications, such as assigning personnel
to jobs, target tracking, crew scheduling, snowplowing streets, scheduling on parallel
machines, among others (see §10.2.5).
2. In a matching M, each vertex of G has degree 0 or 1 relative to the edges in M. In
a perfect matching M, each vertex of G has degree 1 relative to the edges in M.
3. Any matching M of G contains 2|M| matched vertices and n −2|M| free vertices.
4. If M is any matching in G, then |M| ≤
 n
2

.
5. Weak duality theorem: The size of any vertex cover for G is an upper bound on the
size of any matching in G. As a result, the minimum size of a vertex cover is at least as
large as the size of a maximum matching.
6. Every augmenting path has an odd number of edges.
7. If M is a matching and P is an augmenting path with respect to M, then the
symmetric diﬀerence (§1.2.2) M∆P is a matching of size |M| + 1.

Section 10.2
MATCHINGS
735
8. If M is a matching and P is an augmenting path with respect to M, then wt(M∆P)
= wt(M) + wt(P).
9. Augmenting path theorem:
M is a maximum size matching if and only if there is
no augmenting path with respect to M.
10. Fact 9 was obtained independently by C. Berge (1957) and also by R. Z. Norman
and M. O. Rabin (1959). This result was also recognized in an 1891 paper of J. Petersen.
11. Suppose M is a matching having maximum weight among all matchings of a ﬁxed
size k. If P is an augmenting path of maximum weight, then M∆P is a matching having
maximum weight among all matchings of size k + 1.
12. Suppose paths P1, P2, . . . , Pk are obtained as in Fact 11 by augmenting along a
maximum weight path. Then wt(P1) ≥wt(P2) ≥· · · ≥wt(Pk).
13. Weighted augmenting path theorem: M is a maximum weight matching if and only
if there is no augmenting path with respect to M that has positive weight.
14. The number of perfect matchings of the complete graph (§8.1.3) K2n on 2n vertices
is (2n −1)! ! = 1 · 3 · 5 . . . (2n −1).
15. An historical perspective on the theory of matchings is found in [Pl92].
Examples:
1. The graph G in the following ﬁgure has six vertices and eight edges. The highlighted
edges display a matching M of size 2. By Fact 4, the size of any matching in G is at most
3. Since the set S = {1, 6} of vertices form a vertex cover for G of size 2, Fact 5 shows
that M is actually a maximum size matching and S is a minimum size vertex cover.
2
5
4
3
1
6
2. Part (a) of the following ﬁgure displays a network G with the weight we shown next
to each edge e.
1
6
7
5
3
5
1
2
5
4
3
1
6
2
4
2
4
1
6
7
5
1
2
5
4
3
1
6
(a
5
3
(
)
b)
The matching M1 = {(1, 2), (3, 5)} of size 2 is also shown, with the matched edges
highlighted. The mate of vertex 1 is vertex 2, and the mate of vertex 5 is vertex 3.
The weight of M1 is wt(M1) = 7.
Relative to the matching M1, vertices 4 and 6
are free vertices, and an augmenting path P from 4 to 6 is given by the set of edges
P = {(1, 4), (1, 2), (2, 3), (3, 5), (5, 6)}. Here wt(P) = 1 + 4 + 3 −2 −5 = 1 and (as
guaranteed by Fact 6) path P has an odd number of edges.
The matching M2 =
M1∆P = {(1, 2), (3, 5)} ∆{(1, 4), (1, 2), (2, 3), (3, 5), (5, 6)} = {(1, 4), (2, 3), (5, 6)} is a
perfect matching and is highlighted in part (b) of the ﬁgure. There are no free vertices
relative to matching M2 and no augmenting paths, so M2 is a maximum size match-
ing of G. There are other maximum size matchings, such as {(1, 4), (2, 5), (3, 6)} and
{(1, 2), (4, 5), (3, 6)}.

736
Chapter 10
NETWORKS AND FLOWS
3. Part (a) of the following ﬁgure shows a matching M1 of size 1, with wt(M1) = 7. Since
edge (2, 5) has maximum weight among all edges, M1 is a maximum weight matching
of size 1. Relative to M1 the augmenting path P1 = {(1, 5), (2, 5), (2, 3)} has weight
6 + 4 −7 = 3, whereas the augmenting path {(3, 6)} has weight 1. It can be veriﬁed
that P1 is a maximum weight augmenting path relative to M1.
By Fact 11, M2 =
M1∆P1 = {(1, 5), (2, 3)} is a maximum weight matching of size 2 in the network, with
wt(M2) = wt(M1) + wt(P1) = 7 + 3 = 10; see part (b) of the ﬁgure.
1
6
7
5
3
5
1
2
5
4
3
1
6
1
6
7
5
1
2
5
4
3
1
6
(a
2
4
2
4
5
3
(
)
b)
Relative to M2 there are several augmenting paths between the free vertices 4 and 6:
Q1 = {(1, 4), (1, 5), (5, 6)},
wt(Q1) = 1 + 3 −6 = −2,
Q2 = {(1, 4), (1, 5), (2, 5), (2, 3), (3, 6)},
wt(Q2) = 1 + 7 + 1 −6 −4 = −1,
Q3 = {(4, 5), (1, 5), (1, 2), (2, 3), (3, 6)},
wt(Q3) = 5 + 2 + 1 −6 −4 = −2.
The maximum weight augmenting path is Q2 and so (by Fact 11) M3 = M2∆Q2 =
{(1, 4), (2, 5), (3, 6)} is a maximum weight matching of size 3 in the network with wt(M3) =
9. Overall, the maximum weight matching in G is M2 as expected, since all augmenting
paths relative to M2 have negative weight (see Fact 12).
4. Paired kidney donations: To address the severe shortage of organs for use in kidney
transplants, some hospitals have organized a paired kidney donation program. While a
relative of a patient would gladly donate a kidney to that patient, it may not be possible
because of incompatibilities in blood type or unfavorable antibody reactions.
This situation can be illustrated using the following graph G. Each of the eight vertices
in G represents a pair of incompatible donor-recipients. Two vertices are connected by
an edge if the donors D of each pair are compatible matches for the recipients R of the
other pair. For example, the edge (1, 2) indicates that D1 can donate a kidney to R2 and
that D2 can donate a kidney to R1. A largest number of compatible kidney exchanges
then corresponds to a maximum size matching in G. By Fact 4, there can be at most four
compatible matches. However, S = {1, 3, 6} is a vertex cover for G, meaning a maximum
size matching can have size at most 3 (Fact 5). Consequently, at most three compatible
matches are possible in this situation—for example, the pairs (1, 2), (3, 7), (6, 8).
2
4
1
7
8
5
3
6

Section 10.2
MATCHINGS
737
10.2.2
MATCHINGS IN BIPARTITE NETWORKS
In this section, algorithms are described for ﬁnding maximum size and maximum weight
matchings in bipartite networks (§8.1.3). Bipartite networks arise in a number of appli-
cations, such as in assigning personnel to jobs or tracking objects over time. Moreover,
the algorithms developed for the case of bipartite networks are considerably simpler than
those needed for the case of general networks (§10.2.3).
Deﬁnitions:
Let G = (X ∪Y, E) be a bipartite network with n vertices and m edges, and edge weights
wxy.
If S ⊆X then Γ(S) = {y ∈Y | (x, y) ∈E for some x ∈X} is the set of vertices in Y
adjacent to some vertex of X.
A complete matching from X to Y in G = (X ∪Y, E) is a matching M in which each
vertex of X is incident with an edge of M.
The directed two-terminal ﬂow network G′ associated with G = (X∪Y, E) is deﬁned
by adding new vertices s and t, as well as arcs (s, x) for each x ∈X and arcs (y, t) for
each y ∈Y . All other arcs (x, y) of G′ correspond to edges (x, y) of G where x ∈X and
y ∈Y . Every arc of G′ has capacity 1.
Facts:
1. Hall’s theorem:
G = (X ∪Y, E) has a complete matching from X to Y if and only
if |Γ(S)| ≥|S| holds for every S ⊆X. In words, a complete matching exists precisely
when every set of vertices in X is adjacent to at least an equal number of vertices in Y .
(Philip Hall, 1904–1982.)
2. K¨onig’s theorem: For a bipartite network G, the maximum size of a matching in G
equals the minimum size of a vertex cover for G. (D´enes K¨onig, 1884–1944.)
3. Suﬃcient condition for a complete matching: Suppose there exists some k such that
deg(x) ≥k ≥deg(y) holds in G = (X ∪Y, E) for all x ∈X and y ∈Y . Then G has a
complete matching from X to Y .
4. There is a one-to-one correspondence between matchings of size k in G and integral
ﬂows (§10.4.1) of value k in the associated two-terminal ﬂow network G′.
5. A maximum ﬂow in G′, and thereby a maximum size matching of G, can be found
in O(m√n) time.
6. Suppose that costs are added to the two-terminal ﬂow network G′, using cij = 0 if
i = s or j = t, and cij = −wij otherwise. By starting with the ﬂow (§10.5.1) x = 0, the
successive shortest path algorithm (§10.5.2) can be applied to G′ to produce a minimum
cost ﬂow. This will identify (via Fact 4) a matching with maximum weight.
7. Bipartite matching algorithm: This method (Algorithm 1), which is based on Fact 9
of §10.2.1, produces a maximum size matching of the bipartite network G = (X ∪Y, E).
Each iteration involves a modiﬁed breadth-ﬁrst search of G (see §9.2.1), starting with
the free vertices in the set X. All vertices of G are structured into levels that alternate
between free and matched edges.

738
Chapter 10
NETWORKS AND FLOWS
Algorithm 1:
Bipartite matching algorithm.
input: undirected bipartite network G = (X ∪Y, E)
output: maximum size matching M
M := ∅
while true
let S1 consist of all free vertices of X
mark all vertices of S1 as seen, the remaining vertices as unseen
while there are unseen vertices of G
S2 := {y | (x, y) ∈E, x ∈S1, y unseen}
if some y ∈S2 is free then
an augmenting path P to y has been found
mark all remaining vertices of G as seen
else mark all vertices of S2 as seen
S1 := {x | (y, x) ∈M, y ∈S2, x unseen}
mark all vertices of S1 as seen
if an augmenting path P has been found then M := M∆P
else terminate with matching M
8. Algorithm 1 can be implemented to run in O(nm) time.
9. Bipartite weighted matching algorithm: This method (Algorithm 2), which is based
on Fact 13 of §10.2.1, produces a maximum weight matching of G = (X ∪Y, E). Each
iteration develops a longest path tree in G, rooted at the set of free vertices in X. The
tentative largest weight of a path from a free vertex in X to vertex j is maintained in
the label d(j).
Algorithm 2:
Bipartite weighted matching algorithm.
input: undirected bipartite network G = (X ∪Y, E), weights we
output: maximum weight matching M
M := ∅
while true
let S1 consist of all free vertices of X
d(j) := 0 for j ∈S1, d(j) := −∞otherwise
while S1 ̸= ∅
S2 := ∅
for (x, y) ∈E −M with x ∈S1
if d(x) + wxy > d(y) then
d(y) := d(x) + wxy, S2 := S2 ∪{y}
S1 := ∅
for (y, x) ∈M with y ∈S2
if d(y) −wyx > d(x) then
d(x) := d(y) −wyx, S1 := S1 ∪{x}
let y be a free vertex with maximum label d(y)
let P be the associated augmenting path to y

Section 10.2
MATCHINGS
739
if d(y) > 0 then M := M∆P
else terminate with matching M
10. Algorithm 2 can be implemented to run in O(nm) time.
Examples:
1. Drug testing:
A drug company wishes to test n antibiotics using n volunteer pa-
tients. Some of the patients have known allergic reactions to certain of these antibiotics.
To determine whether there is a feasible assignment of the n diﬀerent antibiotics to n
diﬀerent patients, construct the bipartite network G = (X ∪Y, E), where X is the set of
antibiotics and Y is the set of patients. An edge (i, j) ∈E exists when patient j is not
allergic to antibiotic i. A complete matching of G is then sought.
2. Part (a) of the following ﬁgure shows a bipartite graph G with X = {1, 2, 3, 4} and
Y = {a, b, c, d}.
4
3
1
2
a
b
c
d
3
1
2
a
b
c
d
(a
(
)
b)
Using Fact 1, there cannot be a complete matching from X to Y : if S = {1, 2, 4} then
Γ(S) = {a, c} and |Γ(S)| < |S|. There is, however, a matching of size 3: for example,
{(1, a), (3, b), (4, c)}. This matching has maximum size, since by Fact 2 there is a vertex
cover {3, a, c} of size 3.
3. Part (b) of the ﬁgure in Example 2 shows a bipartite graph G with X = {1, 2, 3}
and Y = {a, b, c, d}. Since deg(x) ≥2 ≥deg(y) holds for all x ∈X and y ∈Y , there
must exist a complete matching from X to Y . One such complete matching is given by
{(1, a), (2, c), (3, b)}.
4. Algorithm 1 can be used to ﬁnd a maximum size matching in the bipartite graph
shown in part (a) of the following ﬁgure.
4
3
1
2
a
b
c
(a)
4
3
1
2
a
b
c
(b)
4
3
1
2
a
b
c
(c)
4
3
1
2
a
b
c
(d)
Relative to the initial empty matching, all vertices of X are free so S1 = {1, 2, 3, 4},
giving S2 = {a, b, c}. In particular, vertex a ∈S2 is free and an augmenting path to a is
P = {(1, a)}. The resulting matching is M = {(1, a)}, shown in part (b) of the ﬁgure.
The second iteration of Algorithm 1 starts with S1 = {2, 3, 4}, giving S2 = {a, b, c}. An
augmenting path to the free vertex b is P = {(2, b)}, resulting in M = {(1, a), (2, b)}; see
part (c) of the ﬁgure.

740
Chapter 10
NETWORKS AND FLOWS
At the next iteration, S1 = {3, 4} and S2 = {a, b}. Since both vertices of S2 are now
matched, the algorithm continues with S1 = {1, 2} and S2 = {c}.
Since c ∈S2 is
free, with augmenting path P = {(3, a), (a, 1), (1, c)}, the new matching produced is
M = {(1, c), (2, b), (3, a)}; see part (d) of the ﬁgure.
The fourth iteration produces S1 = {4}, S2 = {b}; S1 = {2}, S2 = {a, c}; and ﬁnally S1 =
{1, 3}, S2 = ∅. No further augmenting paths are found, and the algorithm terminates
with the maximum size matching M = {(1, c), (2, b), (3, a)}.
5. Algorithm 2 can be used to ﬁnd a maximum weight matching in the bipartite network
shown in part (a) of the following ﬁgure.
3
1
2
a
b
c
(a)
4
1
5
5
6
4
3
1
2
a
b
c
(b)
4
1
5
5
6
4
3
1
2
a
b
c
(c)
4
1
5
5
6
4
3
1
2
a
b
c
(d)
4
1
5
5
6
4
Relative to the initial empty matching, all vertices of X are free so S1 = {1, 2, 3}, with
d(1) = d(2) = d(3) = 0. The labels on vertices a, b, c are updated to d(a) = 6, d(b) = 4,
d(c) = 5, giving S2 = {a, b, c}. Since M = ∅no further updates occur. The free vertex a
has maximum label, and the associated path P1 = {(3, a)} has wt(P1) = 6. The resulting
matching M = {(3, a)} is shown in part (b) of the ﬁgure; it represents the largest weight
matching of size 1, with wt(M) = 6.
The second iteration starts with S1 = {1, 2}. The current labels on vertices a, b, c are
then updated to d(a) = 4, d(b) = 4, d(c) = 5, so S2 = {a, b, c}. Using the matched
edge (a, 3), vertex 3 has its label updated to d(3) = −2 and S1 = {3}.
No further
updates occur, and free vertex c with maximum label d(c) = 5 is selected. This label
corresponds to the augmenting path P2 = {(2, c)}, with wt(P2) = 5. The new matching
is M = {(2, c), (3, a)}, with wt(M) = 11; see part (c) of the ﬁgure.
At the third iteration, S1 = {1} and vertices a, b receive updated labels d(a) = 4, d(b) = 1.
Subsequent updates produce d(3) = −2, d(c) = 3, d(2) = −2, d(b) = 2.
Finally, the free vertex b is selected with d(b) = 2, corresponding to the augmenting path
P3 = {(1, a), (a, 3), (3, c), (c, 2), (2, b)} with wt(P3) = 2. This produces the maximum
weight matching M = {(1, a), (2, b), (3, c)}, with wt(M) = 13; see part (d) of the ﬁgure.
As predicted by Fact 12 of §10.2.1, the weights of the augmenting paths are nonincreasing:
wt(P1) ≥wt(P2) ≥wt(P3).
10.2.3
MATCHINGS IN NONBIPARTITE NETWORKS
This section covers matchings in more general (nonbipartite) networks. Algorithms for
constructing maximum size and maximum weight matchings are considerably more in-
tricate than for bipartite networks. The important new concept is that of a “blossom”
in a network.
Deﬁnitions:
Suppose P is an alternating path from a free vertex s in network G = (V, E). Then a
vertex v on P is even (outer) if the subpath Psv of P joining s to v has even length.
Vertex v on P is odd (inner) if Psv has odd length.

Section 10.2
MATCHINGS
741
Suppose P is an alternating path from a free vertex s to an even vertex v and edge
(v, w) ∈E joins v to another even vertex w on P. Then P ∪{(v, w)} contains a unique
cycle, called a blossom.
A shrunken blossom results when a blossom B is collapsed into a single vertex b,
whereby every edge (x, y) with x ̸∈B and y ∈B is transformed into the edge (x, b). The
reverse of this process gives an expanded blossom.
Facts:
1. Every blossom B has odd length 2k + 1 and contains k matched edges, for some
k ≥1.
2. A bipartite network contains no blossoms.
3. Edmonds’ theorem: Suppose network GB is formed from G by collapsing blossom B.
Then GB contains an augmenting path if and only if G does. (J. Edmonds, born 1965.)
4. General matching algorithm: This method (Algorithm 3), based on Fact 9 of §10.2.1,
produces a maximum size matching of G. At each iteration, a forest of trees is grown,
rooted at the free vertices of G, to ﬁnd an augmenting path. As encountered, blossoms B
are shrunk, with the search continued in the resulting network GB.
Algorithm 3:
General matching algorithm.
input: undirected network G = (V, E)
output: maximum size matching M
M := ∅
{Start iteration}
mark all free vertices as even
mark all matched vertices as unreached
mark all free edges as unexamined
while there are unexamined edges (v, w) and no augmenting path is found
mark (v, w) as examined
{Case 1}
if v is even and w is unreached then
mark w as odd and its mate z as even
extend the forest by (v, w) and the matched edge (w, z)
{Case 2}
if v, w are even and they belong to diﬀerent subtrees then
an augmenting path has been found
{Case 3}
if v, w are even and they belong to the same subtree then
a blossom B has been found
shrink B to an even vertex b
if an augmenting path P has been found then
M := M∆P
go to {Start iteration}
else terminate with matching M
5. Algorithm 3 was initially proposed by Edmonds [Ed65a] with a time bound of O(n4).

742
Chapter 10
NETWORKS AND FLOWS
6. An improved implementation of Algorithm 3 runs in O(nm) time.
7. There are other algorithms for maximum size matchings in nonbipartite networks:
• an algorithm of Gabow [Ga76], which runs in time O(n3);
• an algorithm of Micali and Vazirani [MiVa80], which runs in O(m√n) time.
Computer codes for these algorithms can be found at
• ftp://dimacs.rutgers.edu/pub/netflow/matching/
8. General weighted matching algorithms:
More complicated algorithms are required
for solving weighted matching problems. The ﬁrst such algorithm, also involving blos-
soms, was developed by Edmonds [Ed65b] and has a time bound of O(n4).
9. Improved algorithms exist for the weighted matching problem, with running times
O(n3), O(nm log n), and O(nm + n2 log n) respectively. Computer code for the ﬁrst of
these algorithms can be found at
• ftp://dimacs.rutgers.edu/pub/netflow/matching/
Examples:
1. In part (a) of the following ﬁgure, P = {(1, 2), (2, 3), (3, 4), (4, 5)} is an alternating
but not augmenting path, relative to the matching M = {(2, 3), (4, 5)}.
3
1
2
4
5
6
(a)
1
2
b
(b)
B
6
Relative to this path, vertices 1, 3, 5 are even while vertices 2, 4 are odd. Since (3, 5) is an
edge joining two even vertices on P, the blossom B = {(3, 4), (4, 5), (5, 3)} is formed. On
the other hand, Q = {(1, 2), (2, 3), (3, 5), (5, 4), (4, 6)} is an augmenting path relative to
M so that M∆Q = {(1, 2), (3, 5), (4, 6)} is a matching of larger size—in fact a matching
of maximum size. Notice that relative to path Q, vertices 1, 3, 4 are even while vertices
2, 5, 6 are odd.
2. Shrinking the blossom B relative to path P in part (a) of the ﬁgure of Example 1
produces the network GB shown in part (b) of that ﬁgure; note that vertices 1, b are
even while vertices 2, 6 are odd. The path P B = {(1, 2), (2, b), (b, 6)} is now augmenting
in GB. By expanding P B so that (2, 3) remains matched and (4, 6) remains free, the
augmenting path Q = {(1, 2), (2, 3), (3, 5), (5, 4), (4, 6)} in G is obtained.
3. Algorithm 3 is applied to the nonbipartite network shown in part (a) of the following
ﬁgure. Suppose the matching M = {(3, 4), (6, 8)} of size 2 is already available.
Iteration 1: The free vertices 1, 2, 5, 7 are marked as even, and the matched vertices 3, 4,
6, 8 are marked as unreached. The initial forest consists of the isolated vertices 1, 2, 5, 7.
• If the free edge (2, 3) is examined, then Case 1 applies, so vertex 3 is marked odd
and vertex 4 even; the free edge (2, 3) and the matched edge (3, 4) are added
to the forest.
• If the free edge (4, 7) is examined, then Case 2 applies, and the augmenting
path P = {(2, 3), (3, 4), (4, 7)} is found.
Using P the new matching M =
{(2, 3), (4, 7), (6, 8)} of size 3 is obtained; see part (b) of the ﬁgure.

Section 10.2
MATCHINGS
743
3
1
2
4
5
6
7
8
(a
(
)
b)
(c)
(d)
3
1
2
4
5
6
7
8
3
1
2
4
5
b
3
1
2
4
5
6
7
8
Iteration 2: The forest is initialized with the free (even) vertices 1, 5.
• If the free edge (1, 2) is examined, then Case 1 applies, so vertex 2 is marked odd
and vertex 3 even; edges (1, 2) and (2, 3) are added to the forest.
• Examining in turn the free edges (3, 4) and (7, 6) makes 4, 6 odd vertices and 7, 8
even. Edges (3, 4), (4, 7), (7, 6), (6, 8) are then added to the subtree rooted at 1.
• If edge (7, 8) is examined, Case 3 applies so the blossom B = {(7, 6), (6, 8), (8, 7)} is
detected and shrunk to the even vertex b; part (c) of the ﬁgure shows the result-
ing GB. The current subtree rooted at 1 now becomes {(1, 2), (2, 3), (3, 4), (4, b)}.
• If the free edge (b, 5) is examined, then Case 2 applies and the augmenting path
{(1, 2), (2, 3), (3, 4), (4, b), (b, 5)} is found in GB. The corresponding augment-
ing path in G is P = {(1, 2), (2, 3), (3, 4), (4, 7), (7, 8), (8, 6), (6, 5)}. Forming
M∆P produces the new matching {(1, 2), (3, 4), (7, 8), (5, 6)}, a maximum size
matching; see part (d) of the ﬁgure.
10.2.4
STABLE MATCHINGS
The stable matching problem, formulated on a bipartite network G = (X∪Y, E), incorpo-
rates preferences of the items/individuals in X for those in Y and likewise the preferences
of the items/individuals in Y for those in X. In a stable matching there is no incentive
for an unmatched pair (x, y) to both ﬁnd it desirable to switch their existing mates for
one another. This is an important model for assignment problems with preferences (e.g.,
the assignment of medical residents to hospitals, students to schools, and organ donors
to patients).
Deﬁnitions:
Let G be a complete bipartite graph (§8.1.3) on the vertex set X ∪Y , where X and
Y are disjoint sets with |X| = |Y | = n and (x, y) is an edge for every x ∈X and y ∈Y .
For each x ∈X, let ≻x be an ordering of the elements of Y ; for each y ∈Y , let ≻y be
an ordering of the elements of X.
Suppose M is a perfect matching in G = (X ∪Y, E). Then w is the mate M(v) of
v ∈X ∪Y if (v, w) ∈M. The pair (x, y) ∈X × Y is a blocking pair or instability if

744
Chapter 10
NETWORKS AND FLOWS
(x, y) ̸∈M, yet y ≻x M(x) and x ≻y M(y). In other words, each of x and y prefer one
another to their current mate.
A perfect matching M of G is stable if there are no blocking pairs relative to M.
Individuals x and y are stable partners if (x, y) ∈M for some stable matching M.
Facts:
1. A stable matching in G = (X ∪Y, E) always exists and can be found by using
Algorithm 4, called the Gale-Shapley algorithm [GaSh62].
Algorithm 4:
Gale-Shapley algorithm.
input: complete bipartite graph G on X ∪Y , with |X| = |Y |
output: stable matching M of G
let all x ∈X and y ∈Y be free
while there is some free x
select such an x
let y be the greatest element of Y (according to ≻x) that has not rejected x
if y is free, tentatively match y with x
add (x, y) to M
else
if x ≻y M(y) then
make M(y) free and tentatively match y with x
else
permanently record that y has now rejected x
2. Each iteration of Algorithm 4 involves a free vertex x “proposing” to its most-
preferred choice y ∈Y that has not yet rejected it. Once a proposal has been rejected, it
is never reconsidered. However, the individuals in Y can always “trade up” if a proposal
is made from a more preferred partner.
3. Surprisingly, Algorithm 4 always terminates and will do so with a stable matching.
Moreover, the outcome is the same regardless of the order in which the free vertices in
X are processed.
4. Algorithm 4 terminates after at most n2 iterations and can be implemented to run
in O(n2) time.
5. Algorithm 4 ﬁnds the best stable matching for X. That is, every x ∈X is matched
with its most preferred stable partner.
6. Algorithm 4 ﬁnds the worst stable matching for Y . That is, every y ∈Y is matched
with its least preferred stable partner.
7. By reversing the roles of X and Y in Algorithm 4, we obtain a best stable matching
for Y .
8. While formulated for a complete bipartite graph G with |X| = |Y |, the stable match-
ing problem can be generalized to an arbitrary bipartite graph. In this case, edges of
G indicate acceptable partners. Stability of M now means that no x, y that are not
matched with each other prefer one another to their current partners M(x), M(y). A
stable matching may no longer be perfect.

Section 10.2
MATCHINGS
745
9. A number of applications (e.g., assigning students to schools) require the possibility
of multiple partners. For example, in the school assignment context, each student would
be assigned to a single school, but schools can admit many students. This generalization
allows each y ∈Y to have an integer quota qy ≥1. In fact, this situation can be reduced
to a standard matching problem by making qy copies of y, and by letting each copy have
the same preference ordering over X as the original y.
10. Since the stable matching produced by the Gale-Shapley algorithm greatly advan-
tages the individuals in X and greatly disadvantages the individuals in Y (see Facts 5–6),
work has proceeded to ﬁnd more equitable stable matchings [GuIr89].
11. The 2012 Nobel Prize in Economics was awarded to Lloyd S. Shapley and Alvin
E. Roth for their pioneering work on stable matchings and applications to important
societal problems.
Examples:
1. The following table shows the preferences (in descending order) of the individuals in
X = {1, 2, 3, 4} for those in Y = {a, b, c, d} and likewise the preferences of the individuals
in Y for those in X.
1
d
b
c
a
2
a
d
b
c
3
a
c
b
d
4
c
b
d
a
a
1
3
2
4
b
4
2
1
3
c
2
1
3
4
d
4
2
1
3
The matching {(1, d), (2, b), (3, c), (4, a)} is shown in bold in the following ﬁgure. This
matching is not stable since (4, b) (shown as dashed) is a blocking pair for the matching.
Namely, b ≻4 a and 4 ≻b 2.
4
3
1
2
d
b
c
a
2. The matching {(1, b), (2, d), (3, a), (4, c)} is stable. To verify this, observe that 3 and
4 are matched with their top choice, so they cannot be part of a blocking pair. Suppose 1
was matched with a higher choice than b, namely d; however, d prefers the current mate
2 to 1 so (1, d) is not a blocking pair. Similarly, the only possible blocking pair involving
2 is (2, a), but a prefers the current mate 3 to 2.
3. The matching {(1, c), (2, d), (3, a), (4, b)} is also stable. This is the only other stable
matching for the given set of preferences.
4. Applying Algorithm 4 to the instance in Example 1 results in the following. First,
1 proposes to d and is tentatively accepted; 2 proposes to a and is tentatively accepted.
Next, 3 proposes to a; since 3 is more favored by a than its current partner 2, a accepts
the proposal and rejects 2. Then 4 proposes to c and is tentatively accepted. The current
matching is now {(1, d), (3, a), (4, c)}. Now 2 proposes to d, the most-preferred of the
remaining candidates, who accepts the proposal and rejects 1. Then 1 proposes to b, the
most-preferred of the remaining candidates, who accepts the proposal.

746
Chapter 10
NETWORKS AND FLOWS
At this point no member of X is free and so the algorithm terminates with the matching
{(1, b), (2, d), (3, a), (4, c)}. This is the stable matching identiﬁed in Example 2. Notice
that this matching pairs 3 and 4 with their ﬁrst choices, while 1 and 2 are paired with
their second choices.
5. As indicated in Example 3, the only other stable matching for the given preferences
is the matching {(1, c), (2, d), (3, a), (4, b)}. Notice that this matching pairs 3 with its
ﬁrst choice, 2 and 4 with their second choice, and 1 with its third choice. This veriﬁes
Fact 5, showing that the stable matching found by Algorithm 4 matches each member
of X with its most preferred stable partner.
6. By reversing the roles of X and Y in Algorithm 4, we obtain the second stable
matching {(1, c), (2, d), (3, a), (4, b)}. This matching, which pairs b with its ﬁrst choice,
and a, c, d with their second choice, is the best stable matching for Y . (See Fact 6.)
10.2.5
APPLICATIONS
Matching problems, in both bipartite and nonbipartite networks, are useful models in
a number of applied areas. This section presents some representative applications of
matchings.
Applications:
1. Linear assignment problem:
There are n applicants to be assigned to n jobs, with
each job being ﬁlled with exactly one applicant. The weight wij measures the suitability
of applicant i for job j. Finding a valid assignment with the best overall weight is a
weighted matching problem on the bipartite network G = (X ∪Y, E), where X is the set
of applicants and Y is the set of jobs.
2. Personnel assignment: Pairs of pilots are to be assigned to a ﬂeet of aircraft serving
international routes. Pilots i and j are considered compatible if they are ﬂuent in a
common language and have comparable ﬂight training.
Form the network G whose
vertices represent the pilots and with edges placed between compatible pairs of pilots.
The problem of ﬂying the largest number of aircraft with compatible pilots can then be
solved as a maximum size matching problem on G.
3. Other examples of Application 2 occur in assigning police oﬃcers sharing beats,
matching pairs of compatible roommates, and assigning pairs of employees with comple-
mentary skills to speciﬁc projects.
4. Pruned chessboards:
Several squares (2k in all) are removed from an n × n chess-
board, yielding the pruned chessboard P. Is it then possible to cover the squares of P
using nonoverlapping dominoes, with no squares left uncovered? This can be formulated
as a matching problem on the bipartite network G = (R∪B, E), where R is the set of red
squares and B is the set of black squares in P. An edge joins r ∈R to b ∈B if squares r
and b share a common side. Each set of nonoverlapping dominoes on P corresponds to
a matching in G.
All squares of P can be covered using nonoverlapping dominoes if and only if the max-
imum size matching in G has size n2
2 −k. More generally, a maximum size matching
in G explicitly provides a way to cover the maximum number of squares of P using
nonoverlapping dominoes.
5. Target tracking: The movements of n objects (such as submarines or missiles) are to
be followed over time. The locations of the set of objects are known at two distinct times,

Section 10.2
MATCHINGS
747
though without identiﬁcation of the individual objects. Suppose X = {x1, x2, . . . , xn}
and Y = {y1, y2, . . . , yn} represent the spatial coordinates of the objects detected at
times t and t + ∆t. If ∆t is suﬃciently small, then the Euclidean distance between a
given object’s position at these two times should be relatively small. To aid in identifying
the objects (as well as their velocities and directions of travel), a pairing between set X
and set Y is desired that minimizes the sum of Euclidean distances.
This can be formulated as a maximum weight matching problem on the complete bipartite
network G = (X ∪Y, E), where the edge (i, j) indicates pairing position xi with position
yj. The weight of this edge is the negative of the Euclidean distance between xi and
yj. A maximum weight matching of size n in G then provides an optimal (minimum
distance) pairing of all observations at the two times t and t + ∆t.
6. Crew scheduling: Bus drivers are hired to work two four-hour shifts each day. Union
rules require a certain minimum amount of time between the shifts that a driver can work.
There are also costs associated with getting the driver between the ending location of
the ﬁrst shift and the starting location of the second shift.
The problem of optimally combining pairs of shifts that satisfy union regulations and
incur minimum total cost can be formulated as a maximum weight matching problem.
Namely, deﬁne the network G with vertices representing each shift that must be covered
and edges between pairs of compatible shifts (satisfying union regulations). The weight
of edge (i, j) is the negative of the cost of assigning a single driver to shifts i and j.
It is convenient also to add edges (i, i) to G to represent the possibility of needing a
part-time driver to cover a single shift; edge (i, i) is given a suﬃciently large negative
weight to discourage single-shift assignments unless absolutely necessary. A maximum
weight matching in the network G then provides a minimum cost pairing of shifts for the
bus drivers.
7. Snowplowing streets:
The streets of an area of a city are to be plowed by a single
snowplow. Let G be the network representing the street system of the city, with vertices
representing street intersections and edges representing streets. Associated with each
street (i, j) is its length cij.
If all vertices of G have even degree, then G is an Eulerian graph (§8.4.3) and a circuit
that traverses each edge (street) exactly once can be found using the algorithms in §8.4.3.
Otherwise, a closed walk of G that covers each street at least once is needed, and one
with minimum total length P cij is desired. Let N be the set of vertices of G having
odd degree; by Fact 4 of §8.1.1, |N| is an even integer 2k. Form the complete network
H = (N, E) in which the weight of edge (i, j) is the negative of the shortest path distance
(§10.3.1) between vertices i and j in G. Determine a maximum weight (perfect) matching
M of size k in H. For each (i, j) in M, add the edges of the shortest path between i and
j to the network G, forming the network G′. Every vertex of G′ now has even degree,
and an Euler circuit of G′ provides a minimum cost traversal of the city streets.
This problem is known as the (undirected) Chinese postman problem. A directed version
of the problem is discussed in §10.5.3, Application 3.
8. Sparse matrices:
In solving large sparse systems of linear equations, it is advan-
tageous to ﬁrst reorder the rows and columns of the given n × n coeﬃcient matrix A.
Speciﬁcally, it is desired to place as many nonzero entries on the diagonal of the permuted
matrix [Du81]. This can be viewed as a maximum size matching problem on a bipartite
graph G = (X ∪Y, E) associated with the coeﬃcient matrix A. Here X contains the n
row indices, Y contains the n column indices, and edge (i, j) exists whenever aij ̸= 0.
9. Stable matchings:
A celebrated application of the stable matching model can be
found in the National Resident Matching Program (NRMP), which assigns graduating

748
Chapter 10
NETWORKS AND FLOWS
medical students (residents) to hospitals. The algorithm developed for this centralized
matching program (in use from 1952) was very similar in operation to the Gale-Shapley
algorithm and produced hospital-optimal stable matchings. Roth and Peranson [RoPe99]
helped to redesign the NRMP matching algorithm, now a variant of the resident-optimal
Gale-Shapley algorithm.
10. Additional applications, with reference sources, are given in the following table.
application
references
medical residents assignment
[AhMaOr93]
school bus driver assignment
[AhMaOr93]
oil well drilling
[AhMaOr93], [Ge95], [LoPu86]
chemical bonds
[AhMaOr93]
inventory depletion
[AhMaOr93]
scheduling on machines
[AhMaOr93], [LoPu86]
ranks of matrices
[AhMaOr93]
doubly stochastic matrices
[LoPu86]
nonnegative matrices
[LoPu86]
basketball conference scheduling
[EvMi92]
major league umpire scheduling
[EvMi92]
project scheduling
[Ge95]
plotting street maps
[Ge95]
10.3
SHORTEST PATHS
The shortest path problem requires ﬁnding paths of minimum cost (or length) from
a speciﬁed source vertex to every other vertex in a directed network.
Shortest path
problems lie at the heart of network ﬂows (§10.4–§10.5). They are important both to
researchers and to practitioners because
• they arise frequently in application settings where material is to be sent between
speciﬁed points as quickly, as cheaply, or as reliably as possible;
• they arise as subproblems when solving many combinatorial and network opti-
mization problems;
• they can be solved very eﬃciently.
10.3.1
BASIC CONCEPTS
Deﬁnitions:
A directed network is a weighted graph G = (V, E), where V is the set of vertices
and E is the set of arcs (directed edges). Each arc (i, j) ∈E has an associated cost (or
weight, length) cij. It is possible that certain of the cij are negative. Let n = |V | and
m = |E|.

Section 10.3
SHORTEST PATHS
749
The adjacency set A(i) for vertex i is the set of all arcs incident from i, written
A(i) = {(i, j) | (i, j) ∈E}.
A directed path (§8.3.2) P has length Σ(i,j)∈P cij.
A directed cycle (§8.3.2) W for which Σ(i,j)∈W cij < 0 is called a negative cycle.
A shortest path from vertex s to vertex j is a directed s-j path having minimum length.
A directed out-tree is a tree rooted at vertex s in which all arcs are directed away
from the root s.
A shortest path tree is an out-tree T ∗rooted at vertex s with the property that the
directed path in T ∗from s to any other vertex j is a shortest s-j path.
A vector d(·) is called a vector of distance labels if for every vertex j ∈V , d(j) is the
length of some directed path from the source vertex s to vertex j, with d(s) = 0. If these
labels are the lengths of shortest s-j paths, they are called shortest path distances.
The directed path P = [i0, i1, . . . , ir] from vertex i0 to vertex ir can be represented using
predecessor indices: pred(i1) = i0, pred(i2) = i1, . . . , pred(ir) = ir−1.
Facts:
1. Shortest paths are useful in a wide variety of applications, such as in eﬃcient routing
of messages and distribution of goods, developing optimal investment strategies, schedul-
ing personnel, and approximating piecewise linear functions (see §10.3.5).
2. If P = [s, i1, . . . , ir] is a shortest path from s to ir then Q = [s, i1, . . . , ik] is a shortest
path from s to ik for each 1 ≤k < r.
3. Shortest path optimality conditions:
The vector d(·) of distance labels represents
shortest path distances if and only if d(j) ≤d(i) + cij for all (i, j) ∈E.
4. If the network contains a negative cycle accessible from vertex s, then distance labels
satisfying the conditions in Fact 3 do not exist.
5. If the network does not contain any negative cycle, then (unique) distance labels
satisfying the conditions in Fact 3 always exist. Furthermore, there is a shortest path
tree T ∗realizing these shortest path distances.
Examples:
1. In the directed network of the following ﬁgure, arc costs are shown along each arc.
Part (b) lists the nine paths from vertex 1 to vertex 6, together with their lengths.
Path P4, with length 10, is the (unique) shortest path joining these two vertices. This
path can be represented using the predecessor indices:
pred(6) = 4, pred(4) = 3,
pred(3) = 2, pred(2) = 1. By Fact 2, the subpath Q = [1, 2, 3, 4] of P4 is a shortest
path from vertex 1 to vertex 4.
1
2
4
6
5
3
4
5
2
Path
P1
P2
P3
P4
P5
P6
P7
P8
P9
11
14
13
10
13
12
12
14
15
[1, 2, 4, 6]
[1, 2, 5, 6]
[1, 2, 5, 4, 6]
[1, 2, 3, 4, 6]
[1, 2, 3, 5, 6]
[1, 2, 3, 5, 4, 6]
[1, 3, 4, 6]
[1, 3, 5, 4, 6]
[1, 3, 5, 6]
Length
1
6
1
7
3
4
(a)
(b)
4

750
Chapter 10
NETWORKS AND FLOWS
2. In the directed network of the following ﬁgure, arc costs are shown along each arc
and a set of distance labels is shown at each vertex. Part (b) gives paths from vertex
s = 1 whose lengths equal the corresponding distance labels. These distance labels do
not satisfy the optimality conditions of Fact 3 because for the arc (3, 5), d(5) > d(3)+c35.
The out-tree T in this ﬁgure deﬁned by predecessor indices pred(2) = 5, pred(3) = 1,
pred(4) = 2, pred(5) = 3 has distance labels d = (0, 5, 5, 25, 0). It is a shortest path tree
rooted at vertex 1 since the optimality conditions of Fact 3 are satisﬁed: namely
5 ≤0 + 10 for arc (1, 2),
5 ≤5 + 10 for arc (2, 3),
0 ≤25 + 15 for arc (4, 5).
1
2
4
5
3
0
10
10
20
30
15
5
5
5
-5
45
(a)
s
10
2
3
4
5
j
d(j)
(b)
path
10
5
30
45
[1,2]
[1,3]
[1,2,4]
[1,2,4,5]
10.3.2
ALGORITHMS FOR SINGLE-SOURCE SHORTEST PATHS
This subsection discusses algorithms for ﬁnding shortest path trees from a given source
vertex s in a directed network G with n vertices and m arcs.
Facts:
1. Label-correcting algorithm:
A general label-correcting algorithm (Algorithm 1) is
based on the shortest path optimality conditions (§10.3.1, Fact 3) and is a very popular
algorithm to solve shortest path problems with arbitrary arc costs (L. R. Ford, 1956 and
R. E. Bellman, 1958).
Algorithm 1:
Label-correcting algorithm.
input: directed network G, source vertex s
output: shortest path tree T ∗rooted at s
d(s) := 0
pred(s) := 0
d(j) := ∞for all j ∈V −{s}
LIST := {s}
while LIST ̸= ∅
remove a vertex i from LIST
for each (i, j) ∈A(i)
if d(j) > d(i) + cij then
d(j) := d(i) + cij
pred(j) := i
if j /∈LIST then add j to LIST
2. Algorithm 1 maintains a list, LIST, of vertices with the property that if an arc (i, j)
violates the optimality condition, then LIST must contain vertex i. If LIST is empty, then

Section 10.3
SHORTEST PATHS
751
the current distance labels are optimal. Otherwise some vertex i is removed from LIST
and the arcs of A(i) are scanned. If an arc (i, j) ∈A(i) violates the optimality condition,
then d(j) is updated appropriately.
3. When Algorithm 1 terminates, the nonzero predecessor indices deﬁne a shortest path
tree T ∗rooted at the source vertex: namely, T ∗= {(pred(i), i) | i ∈V −{s}}.
4. Convergence:
In Algorithm 1, vertices in LIST can be selected in any order and
the algorithm still converges ﬁnitely. If all arc costs are integers whose magnitudes are
bounded by a constant C, then the algorithm performs O(n2C) iterations and can be
implemented to run in O(nmC) time, regardless of the order in which vertices from LIST
are selected.
5. Queue implementation: Suppose in Algorithm 1 that LIST is maintained as a queue
(§18.1.2); that is, vertices in LIST are examined in a ﬁrst-in-ﬁrst-out (FIFO) order. This
speciﬁc implementation examines no vertex more than n −1 times and runs in O(nm)
time. This is the best strongly polynomial-time algorithm to solve the shortest path
problem with arbitrary arc costs.
6. Deque implementation: Suppose in Algorithm 1 that LIST is maintained as a deque
or two-way list (§18.1.2). Speciﬁcally, vertices are removed from the front of the deque,
but vertices are added either at the front or at the rear. If the vertex has been in LIST
earlier, the algorithm adds it to the front; otherwise, it adds the vertex to the rear.
Empirical studies have found that the deque implementation is one of the most eﬃ-
cient algorithms to solve the shortest path problem in practice even though it is not a
polynomial-time algorithm.
7. Negative cycle detection: The queue implementation (Fact 5) of the label-correcting
algorithm can be used to detect the presence of a negative cycle. To do so, record the
number of times that the algorithm examines each vertex. If the algorithm examines
a vertex more than n −1 times, there must exist a negative cycle. In this case, the
subgraph formed by the arcs (pred(i), i) will contain a negative cycle.
8. Goldberg [Go95] developed a scaling algorithm that determines a negative cost cycle
in O(√nm log C) time. It is the fastest running time for a large range of values of C.
9. Cherkassky et al. [ChGoRa96], [ChEtal09] have carried out careful computational
analyses of algorithms for single-source shortest path problems.
10. Dijkstra’s algorithm (1959):
Dijkstra’s algorithm (Algorithm 2) is a popular al-
gorithm for solving shortest path problems with nonnegative arc costs (E. W. Dijkstra,
1930–2002).
Algorithm 2:
Dijkstra’s algorithm.
input: directed network G with cij ≥0, source vertex s
output: shortest path tree T ∗rooted at s
d(s) := 0
pred(s) := 0
d(j) := ∞for all j ∈V −{s}
LIST := V
while LIST ̸= ∅
{Vertex selection}
let i ∈LIST be a vertex for which d(i) = min{d(j) | j ∈LIST}
remove vertex i from LIST

752
Chapter 10
NETWORKS AND FLOWS
{Distance update}
for each (i, j) ∈A(i)
if d(j) > d(i) + cij then
d(j) := d(i) + cij
pred(j) := i
11. Algorithm 2 performs two steps repeatedly: vertex selection and distance update.
The vertex selection step chooses a vertex i with smallest distance label in LIST for
examination. The distance update step scans each arc (i, j) ∈A(i) and updates the
distance label d(j), if necessary, to restore the optimality condition for arc (i, j).
12. Whenever a vertex is selected for examination in Algorithm 2, its distance label is
the shortest path distance from s; consequently, each vertex is examined only once.
13. Using a simple array or linked list representation of LIST, vertex selections take a
total of O(n2) time and distance updates take a total of O(m) time. This implementation
of Algorithm 2 runs in O(n2) time.
14. By using more sophisticated data structures, the eﬃciency of Dijkstra’s algorithm
can be improved. Currently, two of the best implementations use
• Fibonacci heaps, giving O(m + n log n) running time [FrTa84];
• radix heaps, giving O(m + n(log C)1/2) running time [AhEtal90].
15. Thorup [Th99] developed an algorithm that solves the shortest path problem on
undirected networks in O(m) time.
16. Goldberg [Go01] gave an implementation of Dijkstra’s algorithm that solves the
shortest path problem in O(m) expected time if the arc lengths are positive and uniformly
distributed.
17. A very fast implementation of Dijkstra’s algorithm is due to R. Dial [Di69], and it
runs in O(m + nC) time.
18. A comprehensive discussion of several implementations of Dijkstra’s algorithm and
the label-correcting algorithm is presented in [AhMaOr93].
19. Computer codes that implement algorithms for single-source shortest paths can be
found at
• http://www.avglab.com/andrew/soft.html
• http://www.dis.uniroma1.it/challenge9/
20. A useful extension of the shortest path problem involves ﬁnding the k shortest
paths in a network. The case k = 1 corresponds to a shortest path. More generally,
the kth shortest path is one having the kth smallest length among all paths from s to t.
Several algorithms for solving the problem of ﬁnding the k shortest paths are discussed
in [EvMi92].
Examples:
1. The following ﬁgure illustrates three iterations of the label-correcting algorithm ap-
plied to Example 2 of §10.3.1. LIST is maintained as a queue.
In the ﬁrst iteration, the source vertex s = 1 is examined, and the distance labels of
vertices 2 and 3 are decreased to 10 and 5, respectively. At this point, LIST = [2, 3].

Section 10.3
SHORTEST PATHS
753
2
4
5
3
1
10
∞
∞
∞
∞
∞
∞
∞
10
10
30
10
30
10
5
5
0
20
0
0
0
0
5
5
5
-5
(a)
(b)
(d)
(c)
2
4
5
3
1
2
4
5
3
1
2
4
5
3
1
In the second iteration, vertex 2 is removed from LIST and examined. The distance
label of vertex 4 decreases to 30, while the distance label of vertex 3 remains unchanged,
giving LIST = [3, 4].
Next, vertex 3 is removed from LIST and examined, triggering a reduction of the distance
label of vertex 5 to 0. At this point the current out-tree, deﬁned by the predecessor indices
pred(·), consists of arcs (1, 2), (2, 4), (1, 3), and (3, 5).
2. Part (b) of the following ﬁgure shows the application of Dijkstra’s algorithm to the
directed network in part (a) with nonnegative arc costs and s = 1.
Shown at each
iteration are the current distance labels, the vertex selected, and the resulting distance
updates. Upon termination, the shortest path lengths d = (0, 6, 4, 9, 7) are realized by
the optimal tree T ∗having arcs (1, 3), (3, 2), (2, 4), and (2, 5).
2
4
5
3
1
4
1
3
7
s
4
5
(a)
(b)
2
2
iteration  
labels  
select  
updates
1 
(0,∞,∞,∞,∞) 
1 
d(2) = 7,  d(3) = 4
2 
(0,7,4,∞,∞) 
3 
d(2) = min{7,6} = 6
 
 
 
d(5) = 9
3 
(0,6,4,∞,9) 
2 
d(4) = 9
 
 
 
d(5) = min{9,7} = 7
4 
(0,6,4,9,7) 
5 
d(3) = min{4,9} = 4
5 
(0,6,4,9,7) 
4 
d(5) = min{7,13} = 7
10.3.3
ALGORITHMS FOR ALL-PAIRS SHORTEST PATHS
This section discusses algorithms for ﬁnding shortest path distances between every pair
of vertices in a directed network with n vertices and m arcs.

754
Chapter 10
NETWORKS AND FLOWS
Deﬁnitions:
Suppose G = (V, E) is a directed network with vertex set V and arc set E, and let cij
be the cost of arc (i, j) ∈E.
The n × n arc length matrix U = (uij) is deﬁned as follows:
uij =





0
if i = j
cij
if (i, j) ∈E
∞
if i ̸= j and (i, j) /∈E.
Let dij be the length of a shortest path from vertex i to vertex j, with dii = 0.
Deﬁne the n×n matrix Dk = (dk
ij), where dk
ij is the length of a shortest path from vertex
i to vertex j subject to the condition that the path contains no more than k arcs.
Deﬁne minsum matrix multiplication C = A ⊗B by cij = min1≤p≤n{aip + bpj}.
Also, deﬁne A⊗k = A ⊗A ⊗· · · ⊗A (k times).
In the directed path [i0, i1, . . . , ir] from i0 to ir, the vertices i1, i2, . . . , ir−1 are called
internal vertices. Let dk[i, j] be the length of a shortest path from vertex i to vertex j
subject to the condition that this path uses only 1, 2, . . . , k −1 as internal vertices.
The n × n matrix D[k] contains the entries dk[i, j].
Facts:
1. The length of a shortest path containing at most k arcs can be expressed in terms of
shortest path lengths involving at most k −1 arcs. Namely, for all vertices i and j
• d1
ij = uij;
• dk
ij = min
1≤p≤n{dk−1
ip
+ upj} for 2 ≤k ≤n −1;
• if there is no negative cycle, then dn−1
ij
= dij.
2. Dk = U ⊗k for all 1 ≤k ≤n −1.
3. For any pair of vertices i and j, the following conditions hold:
• d1[i, j] = uij;
• dk+1[i, j] = min{dk[i, j], dk[i, k] + dk[k, j]}, 1 ≤k ≤n;
• if there is no negative cycle then dn+1[i, j] = dij.
4. The all-pairs shortest path problem can be solved by applying n times either Algo-
rithm 1 or Algorithm 2 of §10.3.2, considering each vertex once as a source.
5. Specialized algorithms are available to solve the all-pairs shortest path problem: the
matrix multiplication algorithm (Fact 6) and the Floyd-Warshall algorithm (Fact 8).
6. Matrix multiplication algorithm:
This algorithm (Algorithm 3), based on Facts 1
and 2, computes the shortest path distances between all vertex pairs by multiplying two
matrices repeatedly, using minsum matrix multiplication.
Algorithm 3:
Matrix multiplication algorithm.
input: directed network G on n vertices
output: shortest distance matrix D = (dij)
form the n × n arc length matrix U
compute D := U ⊗(n−1)

Section 10.3
SHORTEST PATHS
755
7. If there is no negative cycle, then Algorithm 3 ﬁnds all shortest path distances using
O(log n) matrix multiplications, each of which takes O(n3) time. Hence this algorithm
runs in O(n3 log n) time and requires O(n2) space. Zwick [Zw02] showed how the matrix
algorithm could be sped up using fast matrix multiplication.
8. Floyd-Warshall algorithm: This approach (Algorithm 4) calculates all-pairs shortest
path distances in a directed network G and is based on computing conditional shortest
path lengths d[i, j].
Algorithm 4:
Floyd-Warshall algorithm.
input: directed network G on n vertices
output: shortest distance matrix D = (d[i, j])
for all (i, j) ∈V × V
d[i, j] := ∞
for all i ∈V
d[i, i] := 0
for all (i, j) ∈E d[i, j] := cij
for k := 1 to n
for (i, j) ∈V × V
if d[i, j] > d[i, k] + d[k, j] then d[i, j] := d[i, k] + d[k, j]
9. If there is no negative cycle, the Floyd-Warshall algorithm correctly computes the
matrix of shortest path distances. A single n × n array D is used to implement the
algorithm.
10. Algorithm 4 can be used to detect (and identify) negative cycles by monitoring
whenever d[i, i] < 0 occurs for some vertex i.
11. Algorithm 4 runs in O(n3) time and requires O(n2) space.
12. If the underlying network is dense, that is, m = Ω(n2), then the O(n3) time bound
for Algorithm 4 is as good as any other discussed in §10.3.2 or §10.3.3.
13. Algorithm 4 was ﬁrst discovered by B. Roy in 1959 in the context of determining
the transitive closure of a graph; this same algorithm was independently discovered by
S. Warshall in 1962. The method was generalized to computing all-pairs shortest paths
by R. W. Floyd, also in 1962.
Examples:
1. The matrix multiplication algorithm is applied to the directed network in the follow-
ing ﬁgure.
4
1
10
3
2
5
4
6
3
6
5
3
4
4
By Facts 1 and 2, the matrix D4 is the matrix of shortest path distances.
D1 =







0
4
5
∞
∞
∞
0
6
3
10
∞
∞
0
4
∞
∞
∞
3
0
6
∞
∞
∞
4
0







D2 =







0
4
5
7
14
∞
0
6
3
9
∞
∞
0
4
10
∞
∞
3
0
6
∞
∞
7
4
0








756
Chapter 10
NETWORKS AND FLOWS
D4 =







0
4
5
7
13
∞
0
6
3
9
∞
∞
0
4
10
∞
∞
3
0
6
∞
∞
7
4
0







.
2. Algorithm 4 is illustrated using the network shown in Example 1.
D[1] =







0
4
5
∞
∞
∞
0
6
3
10
∞
∞
0
4
∞
∞
∞
3
0
6
∞
∞
∞
4
0







D[3] =







0
4
5
7
14
∞
0
6
3
10
∞
∞
0
4
∞
∞
∞
3
0
6
∞
∞
∞
4
0







D[5] =







0
4
5
7
13
∞
0
6
3
9
∞
∞
0
4
10
∞
∞
3
0
6
∞
∞
7
4
0







.
It can be veriﬁed that D[2] = D[1], D[4] = D[3], and D[6] = D[5]. Consequently, the
matrix D[5] above gives all shortest path distances.
10.3.4
PARALLEL ALGORITHMS
Parallel implementations of certain shortest path algorithms are described here relative
to an EREW (exclusive-read, exclusive-write) PRAM (parallel random-access machine).
For details of EREW PRAM, see §17.1.4.
Facts:
1. Label-correcting algorithm: The parallel implementation of the label-correcting algo-
rithm (§10.3.2) associates a processor with each arc and with each vertex of the network.
This algorithm maintains a distance label for each vertex, appropriately initialized. Sup-
pose the distance labels are d(i) at the beginning of an iteration. During the iteration,
the processor attached to each arc (i, j) computes a temporary label d′(i, j) = d(i) + cij
in O(1) time. Then the processor associated with vertex j examines incoming arcs at
vertex j and sets d(j) := min{d′(i, j) | 1 ≤i ≤n}.
2. Using a parallel preﬁx operation, the distance labels can be updated in O(log n)
time. The label-correcting algorithm performs O(n) iterations and so its running time is
O(n log n) using O(m) processors.
3. Matrix multiplication algorithm:
The matrix multiplication algorithm of §10.3.3
solves the all-pairs shortest path problem by performing O(log n) matrix multiplications.
4. Unlike a sequential computer, where matrix multiplication takes O(n3) time, a par-
allel computer can perform matrix multiplication in O(log n) time using O(n3) proces-
sors [Le92]. Consequently, this all-pairs shortest path algorithm runs in O(log2 n) time
using O(n3) processors.

Section 10.3
SHORTEST PATHS
757
10.3.5
APPLICATIONS
Shortest path problems arise in a variety of applications, both as stand-alone models and
as subproblems in more complex problem settings. Shortest path problems also arise in
surprising ways that on the surface might not appear to involve networks at all. This
subsection presents several models based on determining shortest paths.
Applications:
1. Distribution:
Material needs to be shipped by truck from a central warehouse to
various retailers at minimum cost. The underlying network is an undirected road network,
with edges representing the roads joining various cities (vertices). The cost of an edge
is the per unit shipping cost. Solving the single-source shortest path problem provides a
least-cost shipping pattern for the material.
2. Telephone routing: A call is to be routed from a speciﬁed origin to a speciﬁed desti-
nation. Here the underlying network is the telephone system, with vertices representing
individual users (or switching centers). Since a direct connection between the origin ver-
tex s and the destination vertex t may not be available, one practice is to route the call
along a path having the minimum number of arcs (i.e., involving the smallest number
of switching centers). This means ﬁnding a shortest path with unit lengths on all arcs.
Alternatively, each arc can be provided with a measure of delay, and routing can take
place along a timewise shortest path from s to t.
3. Salesperson routing:
A salesperson is to travel by air from city A to city B. The
commission obtained by visiting each city along the way can be estimated. An optimal
itinerary can be found by solving a shortest path problem on the underlying airline
network, represented as a directed network of nonstop routes (arcs) connecting cities
(vertices). Each arc (i, j) is given the net cost cij = fij −rj, where fij is the cost of
the ﬂight from city i to city j and rj is the commission obtained by visiting city j. A
shortest path from A to B identiﬁes an optimal itinerary.
4. Investment strategy:
An investor has a ﬁxed amount to invest at the beginning of
the year. A variety of diﬀerent ﬁnancial opportunities are available for investing during
the year, with each such opportunity assumed to be available only at the start of each
month. Construct the directed network having a vertex for each month as well as a ﬁnal
vertex t = 13. The arc (i, j) corresponds to an investment opportunity beginning in
month i and maturing at the start of month j, with its weight cij being the negative of
the proﬁt earned for the duration of the investment. An optimal investment strategy is
identiﬁed by a shortest path from vertex 1 to vertex t.
5. Equipment replacement: A job shop must periodically replace its capital equipment
because of machine wear. As the machine ages, it breaks down more frequently and so
becomes more expensive to operate. Also, as a machine ages its salvage value decreases.
Let cij denote the cost of buying a particularly important machine at the beginning of
period i, plus the cost of operating the machine over the periods i, i + 1, . . ., j −1, minus
the salvage cost of the machine at the beginning of period j. The problem is to design a
replacement plan that minimizes the cost of buying, selling, and operating the machine
over a planning horizon of n years, assuming that the job shop must have exactly one
machine in service at all times.
This problem can be formulated as a shortest path problem on a network G with vertices
i = 1, 2, . . ., n+1; G contains an arc (i, j) with cost cij for all i < j. There is a one-to-one
correspondence between directed paths in G from vertex 1 to vertex n+1 and equipment

758
Chapter 10
NETWORKS AND FLOWS
replacement plans. The following ﬁgure gives a sample network with n = 5. The path
[1, 3, 6] corresponds to buying the equipment at the beginning of periods 1 and 3. A
shortest path from vertex 1 to vertex n + 1 identiﬁes an optimal replacement plan.
6. Paragraph problem:
The document processing program TEX uses an optimization
procedure to decompose a paragraph into several lines so that when lines are left- and
right-justiﬁed, the appearance of the paragraph will be the most attractive. Suppose that
a paragraph consists of words i = 1, 2, . . ., n. Let cij denote the attractiveness of a line if
it begins with the word i and ends with the word j −1. The program TEX uses formulas
to compute the value of each cij. Given the cij, the decision problem is to decompose the
paragraph into several lines of text in order to maximize the total attractiveness (of all
lines). This problem can be formulated as a shortest path problem in a manner similar
to Application 5.
7. Tramp steamer problem: A ship travels from port to port carrying cargo and passen-
gers. A voyage of the steamer from port i to port j earns pij units of proﬁt and requires
tij ≥0 units of time. Here it is assumed that P
(i,j)∈W tij > 0 for every directed cycle
W in G. The captain of the ship would like to know whether there exists a tour (di-
rected cycle) W for which the daily proﬁt is greater than a speciﬁed threshold µ0; that is,
P
(i,j)∈W pij/P
(i,j)∈W tij > µ0. By writing this inequality as P
(i,j)∈W (µ0tij −pij) < 0,
it is seen that there is a tour W with mean daily proﬁt exceeding µ0 if and only if G
contains a negative cost directed cycle W. The shortest path label-correcting algorithm
can be used to detect the presence (or absence) of negative cycles (see §10.3.2, Fact 7).
8. System of diﬀerence constraints:
In some linear programming applications (§16.1)
with constraints of the form Ax ≤b, the m × n constraint matrix A contains one +1
and one −1 in each row, with all other entries being zero. Suppose that the kth row
has a +1 entry in column jk and a −1 entry in column ik; entries in the vector b have
arbitrary signs. This linear program deﬁnes the following set of m diﬀerence constraints
in n variables x = (x(1), x(2), . . . , x(n)): x(jk) −x(ik) ≤b(k) for each k = 1, 2, . . . , m.
The problem is to determine whether this system of diﬀerence constraints has a feasible
solution, and if so, to obtain one.
Associate a graph G with this system of diﬀerence constraints; G has n vertices corre-
sponding to the n variables, and the arc (ik, jk) of length b(k) results from the constraint
x(jk) −x(ik) ≤b(k). These constraints are identical with the optimality conditions for
the shortest path problem in G, and they can be satisﬁed if and only if G contains no
negative cycle. In this case the shortest path distances give a solution x satisfying the
constraints.
9. Examples of Application 8 are also found in telephone operator scheduling, just-in-
time scheduling, analyzing the consistency of measurements, and the scaling of data.
10. Maximin paths: In a network with capacities (that is, upper bounds on the amount
of material that can be sent on each arc), the capacity of a path is the smallest capacity
on any of its constituent arcs. A common problem in such networks is to ﬁnd a path
from vertex s to vertex t having the maximum capacity. This represents a path along
which the maximum amount of material can ﬂow. Such a maximin path can be found
eﬃciently by adapting Dijkstra’s shortest path algorithm.
11. Additional applications, with reference sources, are given in the following table.

Section 10.4
MAXIMUM FLOWS
759
application
references
approximating piecewise linear functions
[AhMaOr93], [AhEtal95]
DNA sequence alignment
[AhEtal95]
molecular conﬁrmation
[AhMaOr93], [AhEtal95]
robot design
[AhMaOr93], [AhEtal95]
scaling of matrices
[AhMaOr93], [AhEtal95]
knapsack problems
[AhMaOr93], [AhEtal95], [EvMi92]
personnel planning
[AhMaOr93]
production lot sizing
[EvMi92]
transportation planning
[EvMi92]
single-crew scheduling
[AhMaOr93]
dynamic facility location
[AhMaOr93]
UAV path planning
[KiHe03]
social networks
[Ne01]
10.4
MAXIMUM FLOWS
The maximum ﬂow problem involves sending the maximum amount of material from a
speciﬁed source vertex s to another speciﬁed sink vertex t, subject to capacity restrictions
on the amount of material that can ﬂow along each arc. A closely related problem is the
minimum cut problem, which is to ﬁnd a set of arcs with smallest total capacity whose
removal separates s and t.
10.4.1
BASIC CONCEPTS
Deﬁnitions:
Let G = (V, E) be a directed network with vertex set V and arc set E (see §10.3.1).
Each arc (i, j) ∈E has an associated capacity uij ≥0. Such a network is called a
capacitated network. Let n = |V | and m = |E|.
Suppose s is a speciﬁed source vertex and t is a speciﬁed sink vertex. Then a (feasible)
ﬂow is a function x = (xij) deﬁned on arcs (i, j) ∈E satisfying
• mass balance constraints:
P
{j|(i,j)∈E}
xij =
P
{j|(j,i)∈E}
xji
for all i ∈V −{s, t};
• capacity constraints: 0 ≤xij ≤uij for all (i, j) ∈E.
The arc (i, j) is saturated in ﬂow x if xij = uij.
The value of ﬂow x is v =
P
{j|(s,j)∈E}
xsj, the total ﬂow leaving the source vertex.
A maximum ﬂow is a ﬂow having maximum value.
A cut [S, S] partitions the vertex set V into two subsets S and S = V −S, and consists
of all arcs with one endpoint in S and the other in S. Arcs directed from S to S are
forward arcs, and the set of forward arcs is denoted by (S, S). Arcs directed from S to
S are backward arcs, and the set of backward arcs is denoted by (S, S).

760
Chapter 10
NETWORKS AND FLOWS
The cut [S, S] is an s-t cut if s ∈S and t ∈S. The capacity of the s-t cut [S, S] is
u[S, S] =
P
(i,j)∈(S,S)
uij.
A minimum cut is an s-t cut having minimum capacity.
Facts:
1. The ﬂow xij on arc (i, j) can represent the number of cars (per hour) traveling along
a highway segment, the rate at which oil is pumped through a section of pipe in a
distribution system, or the number of messages per unit time that can be sent along a
data link in a communication system.
2. The mass balance constraints ensure that for all vertices i (other than the source or
sink), the total ﬂow out of i equals the total ﬂow into i.
3. The capacity constraints ensure that the ﬂow on an arc does not exceed its stated
capacity.
4. Maximum ﬂows arise in a variety of practical problems involving the ﬂow of goods,
vehicles, and messages in a network. Maximum ﬂows can also be used to study the con-
nectivity of graphs, the covering of chessboards, the selection of representatives, winning
records in tournaments, matrix rounding, and staﬀscheduling (see §10.4.3).
5. For any s-t ﬂow x, the ﬂow out of s equals the ﬂow into t; that is,
X
{j|(s,j)∈E}
xsj = v =
X
{j|(j,t)∈E}
xjt.
6. Removal of the arcs in the s-t cut Z = [S, S] from G separates vertex s from vertex t:
namely, there is no s-t path in G −Z.
7. Let [S, S] be any s-t cut in the network. Then the value of the ﬂow x is given by
v =
X
(i,j)∈(S,S)
xij −
X
(j,i)∈(S,S)
xji.
That is, the net ﬂow across each s-t cut is the same and equals v.
8. Weak duality theorem:
The value of every s-t ﬂow is less than or equal to the
capacity of every s-t cut in the network.
9. If x is some s-t ﬂow whose value equals the capacity of some s-t cut [S, S], then x is
a maximum ﬂow and [S, S] is a minimum cut.
10. Max-ﬂow min-cut theorem:
The maximum value of the ﬂow from vertex s to
vertex t in a capacitated network equals the minimum capacity among all s-t cuts. (L.
R. Ford and D. R. Fulkerson, 1956.)
11. A systematic study of ﬂows in networks was ﬁrst carried out by Ford and Fulkerson
[FoFu62]. They developed their max-ﬂow min-cut theorem when working on a classiﬁed
project for Frank S. Ross and Ted Harris.
The goal of the project was to evaluate
interdiction strategies on the capacity of the Eastern European rail network to support a
conventional war. Their 1955 classiﬁed report “Fundamentals of a method for evaluating
rail net capacities” was declassiﬁed in 1999.
Examples:
1. Part (a) of the following ﬁgure shows a ﬂow network with s = 1 and t = 6; capacities
are indicated along each arc. The function x given in part (b) satisﬁes the mass balance
constraints and the capacity constraints, and hence is a feasible ﬂow. Relative to this

Section 10.4
MAXIMUM FLOWS
761
ﬂow, arc (1, 2) is not saturated since x12 = 6 < 7 = u12; on the other hand, arc (3, 5) is
saturated since x35 = 3 = u35. The ﬂow has value v = x12+x13 = 6+2 = 8. Here the ﬂow
into vertex 6 is x46 +x56 = 5+3 = 8 = v, as guaranteed by Fact 5. The ﬂow value across
the s-t cut [S, S] with S = {1, 3} is x12 + x34 + x35 −x23 = 6 + 3 + 3 −4 = 8. Similarly,
the ﬂow value across the s-t cut [S, S] with S = {1, 2, 3, 5} is x24 + x34 + x56 −x45 =
2 + 3 + 3 −0 = 8. (See Fact 7.) This ﬂow is not, however, a maximum ﬂow.
7
1
5
2
4
6
4
4
9
6
3
8
3
3
(a)
(b)
5
6
1
2
2
4
6
0
4
3
5
3
2
3
3
5
2. In Example 1, the s-t cut [S, S] with S = {1, 2, 3, 5} has capacity u24 + u34 + u56 =
5+3+9 = 17. Thus the value of any ﬂow in the network is bounded above (see Fact 8) by
17. The s-t cut [S, S] with S = {1, 3} has capacity u12 + u34 + u35 = 7 + 3 + 3 = 13. This
cut capacity 13 provides an improved upper bound on the value of a ﬂow. In particular,
the ﬂow deﬁned in part (b) of the previous ﬁgure has value v = 8 ≤13.
3. The following ﬁgure shows another feasible ﬂow x′ in the network of Example 1.
For x′, the ﬂow value across the s-t cut [S, S] with S = {1, 2, 3} is v = x24 + x34 + x35 =
5+3+3 = 11, which equals the s-t cut capacity u[S, S] = u24+u34+u35 = 5+3+3 = 11.
By Fact 9, x′ is a maximum ﬂow and S = {1, 2, 3} deﬁnes a minimum cut [S, S].
7
1
5
2
4
6
2
2
5
6
3
4
3
3
5
10.4.2
ALGORITHMS FOR MAXIMUM FLOWS
There are two main classes of maximum ﬂow algorithms: augmenting path algorithms
and preﬂow-push algorithms. Both types of algorithms work on an auxiliary network
(called the residual network) associated with the current solution.
Deﬁnitions:
Let G = (V, E) be a directed network with n = |V | and m = |E|. Let s and t be the
speciﬁed source and sink, and let U be the largest of the arc capacities uij in G.
Let x = (xij) be a function deﬁned on the arcs (i, j) of G. Relative to x, the outﬂow
from vertex i and inﬂow to vertex i are given, respectively, by
out(i) =
X
{j|(i,j)∈E}
xij,
in(i) =
X
{j|(j,i)∈E}
xji.
The excess of vertex i is e(i) = in(i) −out(i).
A preﬂow is any x = (xij) satisfying

762
Chapter 10
NETWORKS AND FLOWS
• relaxed mass balance constraints: e(i) ≥0 for all i ∈V −{s, t};
• capacity constraints: 0 ≤xij ≤uij for all (i, j) ∈E.
Vertex i is active if e(i) > 0.
Given a ﬂow (or a preﬂow) x, the residual capacity rij of the arc (i, j) ∈E is the
maximum additional ﬂow that can be sent from i to j using arcs (i, j) and (j, i).
The residual network G(x) with respect to ﬂow x consists of those arcs of G having
positive residual capacity.
An augmenting path is a directed path from vertex s to vertex t in G(x).
The capacity of a directed path in G(x) is the minimum residual arc capacity appearing
on the path.
A set of distance labels with respect to a preﬂow (or ﬂow) x is a function d: V →
{0, 1, 2, . . .} satisfying
• d(t) = 0;
• d(i) ≤d(j) + 1 for every arc (i, j) in the residual network G(x).
An arc (i, j) in the residual network G(x) is admissible with respect to the distance
labels d(·) if d(i) = d(j) + 1.
Facts:
1. The maximum ﬂow problem on an undirected network can be converted to a maxi-
mum ﬂow problem on a directed network. Namely, replace every undirected edge (i, j)
of capacity uij by two oppositely directed arcs (i, j) and (j, i), each with capacity uij.
2. The residual capacity rij = (uij −xij) + xji. The ﬁrst term uij −xij represents
the unused capacity of arc (i, j); the second term xji represents the amount of ﬂow on
arc (j, i) that can be canceled to increase ﬂow from vertex i to vertex j.
3. The capacity of an augmenting path is always positive.
4. Augmenting path property: A ﬂow x is a maximum ﬂow if and only if the residual
network G(x) contains no augmenting path.
5. Augmenting path algorithm:
A general augmenting path algorithm (Algorithm 1)
is based on Fact 4. It identiﬁes augmenting paths and sends ﬂows on these paths until
the residual network contains no such path.
Algorithm 1:
Augmenting path algorithm.
input: directed network G, source vertex s, sink vertex t
output: maximum ﬂow x
x := 0
while G(x) contains a directed path from s to t
identify an augmenting path P in G(x)
δ := min{rij | (i, j) ∈P}
augment δ units of ﬂow along P and update G(x)
recover an optimal ﬂow x from the ﬁnal residual network G(x)
6. Integrality property:
For networks with integer capacities, Algorithm 1 starts with
the zero ﬂow and augments by an integral ﬂow at each iteration. Hence the maximum
ﬂow problem with integral capacities always has an optimal integer ﬂow.

Section 10.4
MAXIMUM FLOWS
763
7. An augmenting path in G(x) can be identiﬁed by any search procedure that starts
at vertex s and identiﬁes all vertices reachable from s by directed paths (§8.3.2).
8. Augmenting the ﬂow along P by δ decreases the residual capacities of arcs in P by δ
and increases the residual capacities of the reversals of arcs in P by δ.
9. At the last iteration of Algorithm 1, let S be the set of vertices reachable from s.
Then t ∈S and [S, S] is a minimum cut.
10. Upon termination of Algorithm 1, an optimal ﬂow x can be reconstructed from the
ﬁnal G(x) using Fact 2. Speciﬁcally, let (i, j) ∈E. If uij −rij ≥0 then set xij = uij −rij
and xji = 0; otherwise, set xji = rij −uij and xij = 0.
11. Algorithm 1 was independently discovered by L. R. Ford and D. R. Fulkerson (1956)
and by P. Elias, A. Feinstein, and C. E. Shannon (1956).
12. The distance label d(i) is a lower bound on the length (number of arcs) of a shortest
(directed) path from vertex i to vertex t in the residual network.
13. If some vertex j satisﬁes d(j) ≥n, then vertex j is separated from the sink vertex
in the residual network.
14. Algorithm 1 runs in pseudopolynomial time O(nmU) for networks with integer
arc capacities. The algorithm may not terminate ﬁnitely for networks with irrational
capacities.
15. Two speciﬁc implementations of Algorithm 1 run in polynomial time:
• By augmenting ﬂow along a shortest path, the number of augmentations is reduced
to O(nm), and using very sophisticated data structures this algorithm can be
implemented to run in O(nm log n) time;
• By augmenting ﬂow along a path with maximum residual capacity, the number
of augmentations is O(m log U) and this algorithm can be implemented to run
in O(nm log U) time.
16. Preﬂow-push algorithm:
The preﬂow-push algorithm (Algorithm 2) maintains a
preﬂow at every step and pushes ﬂow on individual arcs instead of along augmenting
paths. The basic operation is to select an active vertex and try to remove its excess by
pushing ﬂow to neighbors that are “closer” to the sink.
Algorithm 2:
Preﬂow-push algorithm.
input: directed network G, source vertex s, sink vertex t
output: maximum ﬂow x
compute the shortest path lengths d(·) to vertex t
d(s) := n; x := 0; xsj := usj for all arcs (s, j) ∈E
while the network contains an active vertex
select an active vertex i and push relabel(i)
recover an optimal ﬂow x from the ﬁnal residual network G(x)
procedure push relabel(i)
if the network contains an admissible arc (i, j) then
push δ := min{e(i), rij} units of ﬂow from i to j
else d(i) := min{d(j) + 1 | (i, j) ∈E and rij > 0}
17. The shortest path lengths calculated in Algorithm 2 represent the minimum number

764
Chapter 10
NETWORKS AND FLOWS
of arcs in a path to vertex t and can be eﬃciently found by carrying out a breadth-ﬁrst
search relative to t (§9.2.1).
18. In Algorithm 2, if the active vertex currently being examined has an admissible arc
(i, j), then increasing the ﬂow on (i, j) by δ decreases rij by δ and increases rji by δ.
Also, e(i) is decreased by δ and e(j) is increased by δ.
19. In Algorithm 2, if the active vertex currently being examined has no admissible arc,
then after its distance label is increased, at least one admissible arc is created.
20. The preﬂow-push algorithm can be implemented to run in O(n2m) time. Variations
of this algorithm with improved worst-case complexity are described in [AhOrTa89].
21. The highest-label preﬂow-push algorithm [GoTa86] is a speciﬁc implementation of
Algorithm 2 that always examines vertices with the largest distance label. This O(n2√m)
implementation is currently one of the fastest algorithms to solve the maximum ﬂow
problem in practice.
22. Algorithm 2 can be implemented to run in O(nm log(n2/m)) time using a dynamic
tree data structure.
23. Hochbaum [Ho98] introduced the pseudoﬂow algorithm, which is a variant of the
preﬂow-push algorithm. Implementations of the pseudoﬂow algorithm are among the
fastest for solving the maximum ﬂow problem.
24. King, Rao, and Tarjan [KiRaTa94] developed a preﬂow-push algorithm that runs
in O(nm) time whenever m > n1+ǫ for some ﬁxed ǫ > 0.
25. Goldberg and Rao [GoRa98] developed an augmenting path algorithm that has the
best time bound if U is not too large relative to n. The running time when m ≤n4/3 is
O(m3/2 log n log U). The running time when m > n4/3 is O(n2/3m log(n2/m) log U).
26. Cheriyan and Mehlhorn [ChMe99] developed a preﬂow-push algorithm that runs in
O(n3/ log n) time. This is the best running time if m is proportional to n2.
27. Orlin [Or13] developed an augmenting path algorithm that runs in O(nm) time when
m < n1.06. When combined with the King, Rao, and Tarjan algorithm, this establishes
that a maximum ﬂow can be obtained in O(nm) time. Orlin also showed how to solve
the maximum ﬂow problem in O(n2/ log n) time when m is proportional to n.
28. A series of papers starting with Christiano et al. [ChEtal11] and leading to a paper
by Kelner et al. [KeEtal14] developed ǫ-optimal solutions for the maximum ﬂow problem
on undirected graphs. The algorithms rely on the theory of electrical ﬂows. The best of
these algorithms runs in nearly linear time.
29. The books [AhMaOr93] and [CoEtal01] discuss additional versions of augmenting
and preﬂow-push algorithms, as well as specializations of these algorithms to unit capac-
ity networks, bipartite networks, and planar networks.
30. Goldberg and Tarjan [GoTa14] provide a survey of algorithms for the maximum
ﬂow problem and discuss the history of running time improvements.
31. Computer codes for solving maximum ﬂow/minimum cut problems can be found at
• ftp://dimacs.rutgers.edu/pub/netflow/maxflow/
• http://www.avglab.com/andrew/soft.html
• http://riot.ieor.berkeley.edu/Applications/Pseudoflow/maxflow.html
Examples:
1. Part (a) of the following ﬁgure illustrates a network G with capacities shown next
to each arc. A feasible ﬂow from vertex s = 1 to vertex t = 4 is displayed in part (b);
this ﬂow has value v = x12 + x13 = 3. Every path in G from s to t contains a saturated

Section 10.4
MAXIMUM FLOWS
765
arc: paths [1, 2, 4] and [1, 2, 3, 4] have the saturated arc (1, 2), and path [1, 3, 4] has the
saturated arc (3, 4). Consequently, no additional ﬂow can be pushed in the “forward”
direction from s to t. Yet, the current ﬂow x is not a maximum ﬂow.
2
4
1
3
3
(a)
3
1
1
1
2
4
1
3
3
(b)
2
1
0
1
2
4
1
3
3
(c)
2
1
1
1
1
2
4
1
3
3
(d)
3
0
1
1
To ﬁnd additional ﬂow from s to t, the residual network G(x) is constructed; see part (c)
of the ﬁgure. An augmenting path in part (c) is P = [1, 3, 2, 4] with (residual) capacity
δ = 1. Adding the ﬂow on P to that in part (b) produces the new ﬂow x′ in part (d);
notice that the ﬂow on arc (2, 3) in x has been canceled in this process. The resulting
ﬂow x′ has ﬂow value v = 4. Since the s-t cut [S, S] with S = {1, 2, 3} has capacity
u24 + u34 = 4 = v, the ﬂow x′ is a maximum ﬂow and S = {1, 2, 3} deﬁnes a cut having
minimum capacity.
2. The following ﬁgure illustrates three iterations of the augmenting path algorithm
(Algorithm 1). Part (a) of the ﬁgure shows a network with capacities indicated on each
arc. Here s = 1 and t = 6. Initially the ﬂow x = 0, so the residual network is identical
to the original network with rij = uij for every arc (i, j).
7
1
5
2
4
6
4
4
9
6
3
8
3
3
(a)
(b)
(d)
(c)
5
2
5
1
5
2
4
6
4
4
9
6
3
8
3
3
5
2
5
5
1
5
2
4
6
4
5
6
4
3
6
3
3
3
3
5
2
5
4
1
5
2
4
6
4
6
4
3
6
2
1
4
3
3
5
Suppose that the algorithm identiﬁes path P 1 = [1, 2, 4, 6] as the augmenting path. The
algorithm augments δ = min{r12, r24, r46} = min{7, 5, 6} = 5 units of ﬂow along P 1.
This augmentation changes the residual capacities only of arcs in P 1 (or their reverse
arcs), yielding the new residual network in part (b).
In the second iteration, suppose the algorithm identiﬁes path P 2 = [1, 3, 5, 6] as the next
augmenting path. Then ﬂow is increased by δ = min{8, 3, 9} = 3 units along P 2; part
(c) shows the residual network after the second augmentation.

766
Chapter 10
NETWORKS AND FLOWS
A third augmentation with δ = 1 occurs along path P 3 = [1, 3, 4, 6] in part (c), giving
the residual network shown in part (d).
3. The following ﬁgure illustrates three iterations of the preﬂow-push algorithm on the
ﬂow network with capacities given in part (a). Here s = 1 and t = 4; in addition, the pair
(e(i), d(i)) is shown beside each vertex i. Part (b) of the ﬁgure gives G(x) corresponding
to the initial preﬂow x with x12 = 2 and x13 = 4.
Suppose that the algorithm selects vertex 2 for the push/relabel operation. Then arc (2, 4)
is the only admissible arc and the algorithm pushes δ = min{e(2), r24} = min{2, 1} = 1
unit along this arc; part (c) gives the residual network at this stage.
Suppose that the algorithm again selects vertex 2. Since no admissible arc emanates from
this vertex, the algorithm performs a relabel operation and gives vertex 2 a new distance
label: d(2) = min{d(3) + 1, d(1) + 1} = min{2, 5} = 2. The new residual network is the
same as the one shown in part (c) except that d(2) = 2 instead of 1.
In the third iteration, suppose that vertex 3 is selected. Then δ = min{e(3), r34} =
min{4, 5} = 4 units are pushed along the arc (3, 4); part (d) gives the residual network
at the end of this iteration.
(0,1)
(0,2)
(0,1)
(0,0)
2
(a)
(d)
(c)
(b)
3
4
5
1
4
3
1
2
(4,1)
(0,4 )
(0,1)
(0,0)
2
3
4
5
1
4
3
1
2
(4,1)
(0,4)
(1,1)
(1,0)
2
3
4
5
1
4
3
1
2
(0,1)
(0,4)
(1,2)
(5,0)
2
3
4
1
4
1
4
3
1
2
10.4.3
APPLICATIONS
A variety of applied problems can be modeled using maximum ﬂows or minimum cuts.
The max-ﬂow min-cut theorem (§10.4.1, Fact 10) can also be used to deduce a number
of max-min duality results in combinatorial theory. This section discusses a number of
such applications.
Applications:
1. Distribution network:
Oil needs to be shipped from a reﬁnery to a storage facility
using the pipelines of an underlying distribution network. Here the reﬁnery corresponds
to a particular vertex s in the distribution network, and the storage facility corresponds
to another vertex t. The capacity of each arc is the maximum amount of oil per unit

Section 10.4
MAXIMUM FLOWS
767
time that can ﬂow along it. The maximum ﬂow rate from the source vertex s to the sink
vertex t is determined by the value of a maximum s-t ﬂow.
2. Other examples of Application 1 occur in transportation networks, electrical power
networks, and telecommunication networks.
3. System of distinct representatives:
Given is a collection of sets X1, X2, . . . , Xm
which are subsets of a given n-set X. A system of distinct representatives (§1.2.2) for
the collection is sought, if one exists.
To solve this problem, set up the bipartite network (V1 ∪V2, E) in which there is a vertex
of V1 for each set Xi and a vertex of V2 for each element of X. An arc (i, j) of inﬁnite
capacity joins i ∈V1 to j ∈V2 if j ∈Xi. Add a source vertex s joined by arcs of unit
capacity to each i ∈V1, and a sink vertex t with arcs of unit capacity joining each j ∈V2
to t. Then a system of distinct representatives exists if and only if the maximum ﬂow
in this constructed network has value m. In this case, those arcs (i, j), with i ∈V1 and
j ∈V2, having ﬂow xij = 1 identify a system of distinct representatives selected from the
m sets.
4. Feasible ﬂow problem: This problem involves ﬁnding a ﬂow x in G = (V, E) so that
the net ﬂow at each vertex is a speciﬁed value b(i), where P
i∈V b(i) = 0. That is, a
ﬂow x on the arcs of network G is required, satisfying
• mass balance constraints:
P
{j|(i,j)∈E}
xij −
P
{j|(j,i)∈E}
xji = b(i) for all i ∈V ;
• capacity constraints: 0 ≤xij ≤uij for all (i, j) ∈E.
This can be modeled as a maximum ﬂow problem. Construct the augmented network G′
by adding a source vertex s and a sink vertex t to G. For each vertex i with b(i) > 0, an
arc (s, i) is added to E with capacity b(i); for each vertex i with b(i) < 0, an arc (i, t)
is added to E with capacity −b(i). Then solve a maximum ﬂow problem from vertex s
to vertex t in G′. It can be proved that the feasible ﬂow problem for G has a solution if
and only if the maximum ﬂow in G′ saturates all arcs emanating from vertex s in G′.
5. Application 4 frequently arises in distribution problems.
For example, a known
amount of merchandise is available at certain ports and is required at other ports in
known quantities. Also the maximum quantity of merchandise that can be shipped on a
particular sea route is speciﬁed. Determining whether it is possible to satisfy all of the
demands by using the available supplies is a feasible ﬂow problem.
6. Graph connectivity:
In a directed graph G, the arc connectivity κ′
ij of vertices i
and j is the minimum number of arcs whose removal from G leaves no directed path
from i to j. The arc connectivity κ′(G) is the minimum number of arcs whose removal
from G separates some pair of vertices (see §8.4.2). The arc connectivity of a graph is an
important measure of the graph’s reliability or stability. Since κ′(G) = min{κ′
ij | (i, j) ∈
V × V, i ̸= j}, the arc connectivity of a graph can be computed by determining the arc
connectivity of n(n −1) pairs of vertices. As a matter of fact, the arc connectivity of G
can be found by determining only n −1 arc connectivities.
The arc connectivity κ′
ij can be found by applying the max-ﬂow min-cut theorem (§10.4.1)
to the network obtained from G by setting the capacity of each arc (i, j) to 1. In such
a unit capacity network, the maximum i-j ﬂow value equals the maximum number of
arc-disjoint paths from vertex i to vertex j, and the minimum i-j cut capacity equals the
minimum number of arcs required to separate vertex i and vertex j. This shows that the
maximum number of arc-disjoint paths from vertex i to vertex j equals the minimum
number of arcs whose removal disconnects all paths from vertex i to vertex j. (This
result is a variation of Menger’s theorem in §8.4.2; it was independently discovered by

768
Chapter 10
NETWORKS AND FLOWS
Ford and Fulkerson and by Elias, Feinstein, and Shannon.) Consequently, κ′
ij equals the
maximum i-j ﬂow value in the network, and the arc connectivity κ′(G) can be determined
by solving n −1 maximum ﬂow problems in a unit capacity network.
7. Tournaments: Consider a round-robin tournament between n teams, assuming each
team plays against every other team c times and no game ends in a draw. It is claimed
that αi for 1 ≤i ≤n is the number of victories accrued by the ith team at the end of
the tournament. Verifying whether the nonnegative integers α1, α2, . . . , αn are possible
winning records for the n teams can be modeled as a feasible ﬂow problem.
Deﬁne a directed network G = (V, E) with vertex set V = {1, 2, . . ., n} and arc set
E = {(i, j) ∈V × V | i < j}. Let xij, i < j, represent the number of times team i
defeats team j. The total number of times team i defeats teams i + 1, i + 2, . . . , n is
P
{j|(i,j)∈E}xij.
Since the number of times team i defeats a team j < i is c −xji,
it follows
that the total number of times that team i defeats teams 1, 2, . . ., i −1 is
(i −1) c −P
{j|(j,i)∈E}xji. However, there are two constraints:
• The total number of wins αi of team i must equal the total number of times it
defeats teams 1, 2, . . . , n, giving
P
{j|(i,j)∈E}
xij −
P
{j|(j,i)∈E}
xji = αi −(i −1)c for all i ∈V ;
• A possible winning record must also satisfy 0 ≤xij ≤c for all (i, j) ∈E.
Consequently, {αi} deﬁne a possible winning record if these two constraints have a feasi-
ble solution x. Let b(i) = αi−(i−1)c. Since P
i∈V αi and P
i∈V (i−1)c are both equal to
cn(n−1)
2
, the total number of games played, it follows that P
i∈V b(i) = 0. The problem
of ﬁnding a feasible solution to the two constraints is then a feasible ﬂow problem.
8. Matchings and covers:
The max-ﬂow min-cut theorem can also be used to prove
a max-min result concerning matchings and covers in a directed bipartite graph G =
(V1 ∪V2, E). (See §8.1.3.) The subset E′ ⊆E is a matching (§10.2.1) if no two arcs in E′
are incident with the same vertex. The subset V ′ ⊆V1 ∪V2 is a vertex cover if every arc
in E is incident to at least one vertex in V ′. Create the network G′ from G by adding
vertices s and t, as well as arcs (s, i) with capacity 1 for all i ∈V1 and arcs (j, t) with
capacity 1 for all j ∈V2. All other arcs of G′ correspond to arcs of G and have inﬁnite
capacity. Then each matching of cardinality v deﬁnes a ﬂow of value v in G′, and each
s-t cut of capacity v induces a corresponding vertex cover with v vertices. Application
of the max-ﬂow min-cut theorem establishes the desired result. Namely, in a bipartite
graph G = (V1 ∪V2, E), the maximum cardinality of any matching equals the minimum
cardinality of any vertex cover of G.
9. 0-1 matrices:
Suppose A = (aij) is a 0-1 matrix. Associate with A the directed
bipartite graph G = (V1 ∪V2, E), where V1 is the set of row indices and V2 is the set
of column indices. Place an arc (i, j) ∈E whenever aij = 1. A matching in G now
corresponds to a set of “independent” 1s in the matrix A: i.e., no two of these 1s are in
the same row or the same column. Also, a vertex cover of G corresponds to a set of rows
and columns in A that collectively cover all the 1s in the matrix. Applying the result
in Application 8 shows that the maximum number of independent 1s in A equals the
minimum number of lines (rows and/or columns) needed to cover all the 1s in A. This
result is known as K¨onig’s theorem (§6.6.1).
10. Additional applications, with reference sources, are given in the following table.

Section 10.5
MINIMUM COST FLOWS
769
application
references
matrix rounding
[AhMaOr93], [AhEtal95]
distributed computing
[AhMaOr93], [AhEtal95]
network reliability
[AhMaOr93], [AhEtal95]
open pit mining
[AhMaOr93], [AhEtal95]
building evacuation
[AhMaOr93]
covering sports events
[AhEtal95]
nurse staﬀscheduling
[AhMaOr93], [AhEtal95]
bus scheduling
[AhEtal95]
machine scheduling
[AhMaOr93], [AhEtal95]
tanker scheduling
[AhMaOr93], [AhEtal95]
bottleneck assignment
[FoFu62]
selecting freight-handling terminals
[AhEtal95]
site selection
[EvMi92]
material-handling systems
[EvMi92]
decompositions of partial orders
[FoFu62]
matrices with prescribed row/column sums
[FoFu62]
computer vision
[Ro99]
10.5
MINIMUM COST FLOWS
The minimum cost ﬂow problem involves determining the least cost shipment of a com-
modity through a capacitated network in order to satisfy demands at certain vertices
using supplies available at other vertices. This problem generalizes both the shortest
path problem (§10.3) and the maximum ﬂow problem (§10.4).
10.5.1
BASIC CONCEPTS
Deﬁnitions:
Let G = (V, E) be a directed network with vertex set V and arc set E (see §10.3.1).
Each arc (i, j) ∈E has an associated cost cij and a capacity uij ≥0. Let n = |V | and
m = |E|.
Each vertex i ∈V has an associated supply/demand b(i). If b(i) > 0, then vertex i is a
supply vertex; if b(i) < 0, then vertex i is a demand vertex.
A (feasible) ﬂow is a function x = (xij) deﬁned on arcs (i, j) ∈E satisfying
• mass balance constraints:
P
{j|(i,j)∈E}
xij −
P
{j|(j,i)∈E}
xji = b(i) for all i ∈V ,
• capacity constraints: 0 ≤xij ≤uij for all (i, j) ∈E,
where P
i∈V
b(i) = 0.
The cost of ﬂow x is
P
(i,j)∈E
cijxij.

770
Chapter 10
NETWORKS AND FLOWS
A minimum cost ﬂow is a ﬂow having minimum cost.
A pseudoﬂow is a function x = (xij) satisfying the arc capacity constraints; it may
violate the mass balance constraints.
The residual network G(x) corresponding to a ﬂow (or pseudoﬂow) x is deﬁned in the
following manner. Replace each arc (i, j) ∈E by two arcs (i, j) and (j, i). Arc (i, j) has
cost cij and residual capacity rij = uij −xij, and arc (j, i) has cost −cij and residual
capacity rji = xij. The residual network consists only of arcs with positive residual
capacity.
The potential of vertex i is a quantity π(i) associated with the mass balance constraint
at vertex i. With respect to a given set of vertex potentials, the reduced cost of an arc
(i, j) in the residual network G(x) is cπ
ij = cij −π(i) + π(j).
The cost of path P in G(x) is c(P) =
P
(i,j)∈P
cij; its reduced cost is cπ(P) =
P
(i,j)∈P
cπ
ij.
A negative cycle is a directed cycle W in G(x) for which c(W) < 0.
Facts:
1. The mass balance constraints ensure that the net ﬂow out of each vertex i is equal to
b(i). Thus, if there is excess ﬂow out of vertex i, then b(i) > 0 and i is a supply vertex. If
b(i) < 0, then more ﬂow enters i than leaves i, meaning that vertex i is a demand vertex.
2. Minimum cost ﬂows arise in practical problems involving the least cost routing of
goods, vehicles, and messages in a network. Minimum cost ﬂows can also be used in
models of warehouse layout, production and inventory problems, scheduling of personnel,
automatic classiﬁcation of chromosomes, and racial balancing of schools. (See §10.5.3.)
3. Let {π(i) | i ∈V } be any set of vertex potentials.
• If P is a path from i to j in G(x), then cπ(P) = c(P) −π(i) + π(j).
• If W is a cycle in G(x), then cπ(W) = c(W).
4. Negative cycle optimality conditions:
A feasible ﬂow x is a minimum cost ﬂow if
and only if the residual network G(x) contains no negative cycle.
5. Reduced cost optimality conditions: A feasible ﬂow x is a minimum cost ﬂow if and
only if some set of vertex potentials π satisﬁes cπ
ij ≥0 for every arc (i, j) in G(x).
6. Complementary slackness optimality conditions: A feasible ﬂow x is a minimum cost
ﬂow if and only if there exist vertex potentials π such that for every arc (i, j) ∈E
• if cπ
ij > 0, then xij = 0;
• if cπ
ij < 0, then xij = uij;
• if 0 < xij < uij, then cπ
ij = 0.
Examples:
1. In the ﬂow network of part (a) of the following ﬁgure, b(i) is shown next to each
vertex i and (cij, uij) is shown next to each arc (i, j).
The function x = (xij) given in part (b) satisﬁes the mass balance constraints for each
vertex. For example, the ﬂow out of vertex 2 is x24 = 6 and the ﬂow into vertex 2 is
x12 + x32 = 5, so that ﬂow out minus ﬂow in equals 6 −5 = 1 = b(2). Also the capacity
constraints for all arcs are satisﬁed: e.g., x12 = 4 ≤5 = u12.
Thus x is a feasible
ﬂow, with cost 163. The residual network G(x) corresponding to the ﬂow x is shown in
part (c). Selected arcs of G(x) are labeled with their cost and residual capacity. The
directed cycle W = [1, 2, 3, 1] in G(x) has cost 11 −9 −10 = −8 and so W is a negative
cycle. By Fact 4, this ﬂow x is not a minimum cost ﬂow.

Section 10.5
MINIMUM COST FLOWS
771
1
-3
(a)
(b)
(c)
9
-7
(8, 6)
(9, 3)
(11, 5)
(12, 5)
(10, 9)
4
2
1
3
(-8, 6)
(-9, 1)
(11, 1)
(12, 4)
(-10, 5)
4
2
1
3
1
9
-3
-7
5
1
4
6
1
4
2
1
3
2. Part (a) of the following ﬁgure shows another feasible ﬂow x′ for the network in
Example 1, with cost 155.
The corresponding residual network G(x′) is given in part (b), in which each arc is labeled
with its cost and its residual capacity. Using the vertex potentials π = (0, −14, −10, −22),
the reduced cost of arc (2, 1) in the residual network is cπ
21 = −11−(−14)+0 = 3; likewise
cπ
32 = 9 −(−10) −14 = 5. The remaining reduced costs are found to be zero, so cπ
ij ≥0
for all arcs (i, j) in G(x). By Fact 5, x′ is a minimum cost ﬂow for the given network.
3. Alternatively, the optimality of the ﬂow x′ in part (a) of the ﬁgure for Example 2
can be veriﬁed using Fact 6. As in Example 2, let π = (0, −14, −10, −22). Arc (3, 2) of
the original network G in part (a) of the ﬁgure of Example 1 has positive reduced cost
cπ
32 = 9 −(−10) −14 = 5 and x′
32 = 0. Arc (1, 2) has cπ
12 = 11 −0 −14 = −3 < 0 and
x′
12 = u12. The remaining arcs (1, 3), (2, 4), (3, 4) have zero reduced cost. Consequently,
the complementary slackness optimality conditions are satisﬁed and the ﬂow x′ achieves
the minimum cost.
10.5.2
ALGORITHMS FOR MINIMUM COST FLOWS
A variety of algorithms are available to solve the minimum cost ﬂow problem. Three
algorithms are described in this section: the cycle-canceling algorithm, the successive
shortest path algorithm, and the network simplex algorithm.
Deﬁnitions:
Let G = (V, E) be a directed network with n = |V | and m = |E|; let U denote the largest
arc capacity and let C denote the largest arc cost (in absolute value) in G.

772
Chapter 10
NETWORKS AND FLOWS
For a given pseudoﬂow x = (xij), the imbalance of vertex i ∈V is e(i) = b(i) +
P
{j|(j,i)∈E}
xji −
P
{j|(i,j)∈E}
xij.
An excess vertex is one with a positive imbalance, and a deﬁcit vertex is one with a
negative imbalance.
A spanning tree solution x = (xij) consists of a spanning tree T of G = (V, E) in
which each nontree arc (i, j) has either xij = 0 or xij = uij.
A spanning tree solution is feasible if the mass balance constraints and capacity con-
straints are satisﬁed.
Facts:
1. Cycle-canceling algorithm: The cycle-canceling algorithm (Algorithm 1) is based on
the negative cycle optimality conditions (§10.5.1, Fact 4). It starts with a feasible ﬂow
and successively augments ﬂow along negative cycles in the residual network until there
is no negative cycle.
Algorithm 1:
Cycle-canceling algorithm.
input: directed network G
output: minimum cost ﬂow x
establish a feasible ﬂow x in the network
while G(x) contains a negative cycle
identify a negative cycle W
δ := min{rij | (i, j) ∈W}
augment δ units of ﬂow along W and update G(x)
recover an optimal ﬂow x from the ﬁnal residual network G(x)
2. As shown in §10.4.3, an initial feasible ﬂow can be found by solving a maximum ﬂow
problem.
3. Integrality property:
For problems with integer arc capacities and integer vertex
supplies/demands, Algorithm 1 starts with an integer ﬂow, at each iteration augments
by an integral amount of ﬂow, and thus produces an optimal ﬂow that is integer. Thus
any minimum cost ﬂow problem with integer supplies, demands, and capacities always
has an optimal solution that is integer.
4. A negative cycle W in the residual network can be identiﬁed in O(nm) time by using
a queue implementation of the label-correcting algorithm (§10.3.2, Fact 5).
5. Augmenting the ﬂow along W by δ decreases the residual capacities of arcs in W by
δ and increases the residual capacities of the reversals of arcs in W by δ.
6. Upon termination of Algorithm 1, an optimal ﬂow x can be reconstructed from the
ﬁnal G(x); see §10.4.2, Fact 10.
7. For problems with integer supplies, demands, and arc capacities, the cycle-canceling
algorithm runs in pseudopolynomial time O(nm2CU).
8. If ﬂow is augmented along a negative cycle W in G(x) that minimizes the ratio
1
|W|
P
(i,j)∈W cij among all directed cycles in G(x), then this implementation runs in
polynomial time [GoTa88].

Section 10.5
MINIMUM COST FLOWS
773
9. Successive shortest path algorithm:
The successive shortest path algorithm (Algo-
rithm 2) starts with the pseudoﬂow x = 0. It proceeds by selecting an excess vertex k
and a deﬁcit vertex l, and then augmenting ﬂow along a minimum cost path from vertex
k to vertex l in G(x).
Algorithm 2:
Successive shortest path algorithm.
input: directed network G
output: minimum cost ﬂow x
x := 0
e(i) := b(i) for all i ∈V
initialize V + := {i | e(i) > 0} and V −:= {i | e(i) < 0}
while V + ̸= ∅
select a vertex k ∈V + and a vertex l ∈V −
identify a shortest path P in G(x) from vertex k to vertex l
δ := min{e(k), −e(l), min{rij | (i, j) ∈P}}
augment δ units of ﬂow along P
update e, G(x), V +, and V −
recover an optimal ﬂow x from the ﬁnal residual network G(x)
10. If in Algorithm 2 reduced costs cπ
ij are used instead of arc costs cij, then Dijkstra’s
algorithm (§10.3.2, Algorithm 2) can be applied to determine a shortest path P in the
residual network.
11. Augmenting the ﬂow along P by δ decreases the residual capacities of arcs in P by
δ and increases the residual capacities of the reversals of arcs in P by δ. It also decreases
e(k) by δ and increases e(l) by δ.
12. The solution maintained by the successive shortest path algorithm always satisﬁes
the reduced cost optimality conditions (§10.5.1, Fact 5). The ﬁnal solution is in addition
feasible, and so is an optimal solution of the minimum cost ﬂow problem.
13. For problems with integer supplies, demands, and arc capacities, the shortest aug-
menting path algorithm runs in pseudopolynomial time.
14. Several implementations of the shortest augmenting path algorithm run in polyno-
mial or even strongly polynomial time. Orlin [Or88] describes an implementation running
in O(m log n(m+n log n)) time, currently the fastest strongly polynomial-time algorithm
to solve the minimum cost ﬂow problem.
15. If a minimum cost ﬂow problem has an optimal solution, then it has an optimal
spanning tree solution.
16. Given a spanning tree solution x, with ﬂows on nontree arcs (i, j) speciﬁed (at
either 0 or uij), the ﬂows on the tree arcs are uniquely determined by the mass balance
constraints.
17. Given a spanning tree solution x, vertex potentials π can be determined such that
• π(1) = 0;
• cπ
ij = 0 for all tree arcs (i, j).
18. Complementary slackness optimality conditions:
Suppose x is a feasible spanning
tree solution with vertex potentials determined as in Fact 17. Then x is a minimum cost
ﬂow if

774
Chapter 10
NETWORKS AND FLOWS
• cπ
ij ≥0 for all nontree arcs (i, j) with xij = 0;
• cπ
ij ≤0 for all nontree arcs (i, j) with xij = uij.
19. Network simplex algorithm:
The network simplex algorithm (Algorithm 3) is a
specialized version of the well-known linear programming simplex method (§16.1.3). It
maintains a spanning tree solution and at each iteration transforms the current spanning
tree solution into an improved spanning tree solution until optimality is reached.
Algorithm 3:
Network simplex algorithm.
input: directed network G
output: minimum cost ﬂow x
determine an initial spanning tree solution with associated tree T
let x be the ﬂow and π the corresponding vertex potentials
while a nontree arc violates the complementary slackness optimality conditions
select an entering arc (k, l) violating its optimality condition
add arc (k, l) to T , augment the maximum possible ﬂow in the cycle thus
created, and determine the leaving arc (p, q)
update the tree T , the ﬂow x, and the vertex potentials π
20. Using appropriate data structures, the network simplex algorithm can be imple-
mented very eﬃciently. The network simplex algorithm is one of the fastest algorithms
to solve the minimum cost ﬂow problem in practice.
21. The network simplex algorithm has a exponential worst-case time bound. Orlin
[Or97] provided the ﬁrst polynomial-time implementations of the (generic) network sim-
plex algorithm.
22. Detailed descriptions of Algorithms 1–3, as well as several other algorithms for
ﬁnding minimum cost ﬂows, can be found in [AhMaOr93].
23. Computer codes for solving the minimum cost ﬂow problem can be found at
• ftp://dimacs.rutgers.edu/pub/netflow/mincost/
• http://www.avglab.com/andrew/soft.html
• http://lemon.cs.elte.hu/trac/lemon
Examples:
1. The following ﬁgure illustrates the cycle-canceling algorithm. Part (a) of this ﬁgure
depicts the given ﬂow network, with b(i) shown for each vertex i and (cij, uij) for each
arc (i, j). Part (b) shows the residual network corresponding to the ﬂow x12 = x24 = 3
and x13 = x34 = 1.
In the ﬁrst iteration, suppose the algorithm selects the negative cycle [2, 3, 4, 2] with
cost −1. Then δ = min{r23, r34, r42} = min{2, 4, 3} = 2 units of ﬂow are augmented
along this cycle. Part (c) shows the modiﬁed residual network.
In the next iteration, the algorithm selects the cycle [1, 3, 4, 2, 1] with cost −2 and aug-
ments δ = 1 unit of ﬂow. Part (d) depicts the updated residual network which contains
no negative cycle, so the algorithm terminates. From part (d), an optimal ﬂow pattern
is deduced: x12 = x13 = x23 = 2 and x34 = 4.
2. The successive shortest path algorithm is illustrated using the ﬂow network in part (a)
of the ﬁgure for Example 1. The initial residual network G(x) for x = 0 is the same as

Section 10.5
MINIMUM COST FLOWS
775
(-3, 3)
(1, 2)
(2, 1)
(-2, 3)
(1, 4)
(-1, 1)
(-2, 1)
(3, 3)
(2, 4)
(1, 5)
(1, 2)
(2, 2)
(2, 1)
4
2
1
3
(-3, 1)
(3, 2)
(-1,2)
(2, 1)
(-2, 3)
(1, 2)
(-1, 3)
(-2, 1)
(2, 1)
4
2
1
3
(3, 3)
(-1, 2)
(2, 2)
(-2, 2)
(1, 1)
(-1, 4)
(-2, 2)
4
2
1
3
0
4
0
(a)
(b)
(d)
(c)
-4
4
2
1
3
that of part (a). Initially, the imbalances are e = (4, 0, 0, −4), so that V + = {1} and
V −= {4}, giving k = 1 and l = 4. The shortest path from vertex 1 to 4 in G(x) is [1, 3, 4],
and the algorithm augments δ = 2 units of ﬂow along this path. The following ﬁgure
shows the residual network after this augmentation, as well as the updated imbalance at
each vertex.
(3, 3)
(2, 4)
(1, 3)
(-1, 2)
(1, 2)
(-2, 2)
0
2
0
(a)
-2
4
2
1
3
(3, 3)
(2, 2)
(-2, 2)
(1, 1)
(-1, 4)
(-1, 2)
(-2, 2)
0
0
0
(b)
0
4
2
1
3
The sets V + and V −do not change, so again k = 1 and l = 4. The shortest path
from vertex 1 to vertex 4 is now [1, 2, 3, 4], and the algorithm augments δ = 2 units of
ﬂow along this path. Part (b) of the ﬁgure shows the resulting residual network. Now
V + = V −= ∅and the algorithm terminates.
3. The following ﬁgure illustrates the network simplex algorithm. Part (a) of this ﬁgure
depicts the given ﬂow network, with b(i) shown for each vertex i and (cij, uij) for each
arc (i, j).
A feasible spanning tree solution is shown in part (b) of the ﬁgure; each nontree arc
(dashed line) has ﬂow at either its lower or upper bound.
The unique ﬂows on the
tree arcs (solid lines) are determined by the mass balance constraints. A set of vertex
potentials (obtained using Fact 17) are also shown in part (b). Relative to these potentials
π, the reduced costs for the nontree arcs are given by cπ
23 = 2 −(−3) −2 = 3, cπ
35 =
4 −(−2) −5 = 1, cπ
54 = 5 −(−5) −8 = 2, and cπ
46 = 3 −(−8) −9 = 2. Since arc (3, 5),
with ﬂow at its upper bound, violates the optimality conditions of Fact 18, it is added
to the current tree producing the cycle [1, 2, 5, 3, 1]. The maximum ﬂow that can be sent
along this cycle without violating the capacity constraints is 1 unit, which forces the ﬂow
on arc (2, 5) to its upper bound. Arc (2, 5) is then removed from the current tree and
arc (3, 5) is added to the current tree.

776
Chapter 10
NETWORKS AND FLOWS
Part (c) of the ﬁgure gives the new ﬂow as well as the new vertex potentials. Since
cπ
46 = 3 −(−8) −10 = 1, arc (4, 6) is added to the spanning tree, forming the cycle
[1, 3, 5, 6, 4, 2, 1]. The maximum ﬂow that can be sent along this cycle without violating
the capacity constraints is 1 unit, which forces arc (3, 5) out of the tree.
Part (d) gives the new ﬂow as well as the new vertex potentials. Since the complementary
slackness optimality conditions are satisﬁed, the current ﬂow is optimal.
10.5.3
APPLICATIONS
Minimum cost ﬂow problems arise in many industrial settings and scientiﬁc domains,
often in the form of distribution or routing problems. The minimum cost ﬂow problem
also has less transparent applications, several of which are presented in this section.
Applications:
1. Distribution: A common application of the minimum cost ﬂow problem involves
the distribution at minimum cost of a product from manufacturing plants (with known
supplies) to warehouses (with known demands). A similar scenario applies to the dis-
tribution of goods from warehouses to retailers as well as the ﬂow of raw materials and
intermediate goods through various machining stations in a production line.
2. Routing: The routing of cars through an urban street network and the routing of
calls through a telephone system can be modeled using minimum cost ﬂows. In either
case, the items (cars, calls) must be sent from certain speciﬁed origins to other speciﬁed
destinations, with capacity constraints on the total ﬂow on each arc (road, communication
link). This is done to minimize total (or average) delay in the system.
3. Directed Chinese postman problem: Leaving from the post oﬃce, a mail carrier needs
to visit all houses on a postal route, delivering and collecting letters, and then return
to the post oﬃce. The carrier would like to cover this route by traveling the minimum
possible distance. (See also §8.4.3.) In this variation, known as the directed Chinese
postman problem, each street is assumed to be directed, so the problem is deﬁned on a
directed network G = (V, E) whose arcs (i, j) have an associated nonnegative length cij.
It is desired to ﬁnd a directed walk (§8.3.2) of minimum length that starts at some vertex

Section 10.5
MINIMUM COST FLOWS
777
(the post oﬃce), visits each arc of the network at least once, and returns to the starting
vertex. In an optimal walk, some arcs may be traversed more than once. If xij represents
the number of times arc (i, j) is traversed, then this problem can be formulated as
minimize:
P
(i,j)∈E
cijxij
subject to:
P
{j|(i,j)∈E}
xij −
P
{j|(j,i)∈E}
xji = 0 for all i ∈V ,
xij ≥1 for all (i, j) ∈E.
This problem is a minor variant of the minimum cost ﬂow problem where each arc has a
lower bound of one unit of ﬂow. From an optimal ﬂow x∗for this problem, an optimal
tour can be constructed in the following manner. First, replace each arc (i, j) with x∗
ij
copies of the arc, each carrying a unit ﬂow. Next, decompose the resulting network into
a set of directed cycles. Finally, connect the directed cycles to form a closed walk.
4. Optimal loading of a hopping airplane: A small commuter airline uses a plane with
capacity of at most p passengers on a “hopping ﬂight”, as shown in part (a) of the
following ﬁgure. The ﬂight visits the cities 1, 2, 3, . . ., n in a ﬁxed sequence. The plane
can pick up passengers at any city and drop them oﬀat any other city. Let bij denote the
number of passengers available at city i who want to go to city j, and let fij denote the
fare per passenger from city i to city j. The airline would like to determine the number
of passengers that the plane should carry between various origins and destinations in
order to maximize the total fare per trip while never exceeding the capacity of the plane.
Part (b) of the following ﬁgure shows a minimum cost ﬂow formulation of this hopping
plane ﬂight problem. The network displays data only for those arcs with nonzero cost
or ﬁnite capacity.
Any arc without a displayed cost has zero cost; any arc without
a displayed capacity has inﬁnite capacity. For example, three types of passengers are
available at vertex 1: those whose destination is vertex 2, vertex 3, or vertex 4. These
three types of passengers are represented by the vertices 1-2, 1-3, and 1-4 with supplies
b12, b13, and b14. A passenger available at any such vertex, say 1-3, either boards the
plane at its origin vertex by ﬂowing through the arc (1-3, 1), thus incurring a cost of
−f13 units, or never boards the plane, represented by ﬂowing through the arc (1-3, 3).
1-4
1
2
3
n
1
2
3
4
2-3
1-3
1-2
2-4
3-4
b14
–f14
–f13
cost
capacity
–f12
0
p
(b)
(a)
p
p
–f23
–f24
–f34
b24
b34
b23
b13
– b12
– b13 – b23
– b14 – b24 – b34
b12
. . .

778
Chapter 10
NETWORKS AND FLOWS
5. Leveling mountainous terrain:
In building road networks through hilly or moun-
tainous terrain, civil engineers must determine how to distribute earth from high points
to low points of the terrain to produce a leveled roadbed. To model this, construct a
terrain graph, an undirected graph G whose vertices represent locations with a demand
for earth (low points) or locations with a supply of earth (high points). An edge of G
indicates an available route for distributing the earth, and the cost of this edge is the cost
per truckload of moving earth between the corresponding two locations. The following
ﬁgure shows a portion of a sample terrain graph. A leveling plan for a terrain graph is
a ﬂow (set of truckloads) that meets the demands at vertices (levels the low points) by
the available supplies (earth obtained from high points) at minimum trucking cost. This
can be solved as a minimum cost ﬂow problem on the terrain graph.
6
–7
3
5
–3
10
15
4
5
6. Additional applications, with reference sources, are given in the following table.
application
references
medical tomography
[AhMaOr93], [AhEtal95]
automatic chromosome classiﬁcation
[AhMaOr93], [AhEtal95]
racial balancing of schools
[AhMaOr93], [AhEtal95]
controlled matrix rounding
[AhMaOr93], [AhEtal95]
building evacuation
[AhMaOr93], [AhEtal95]
just-in-time scheduling
[AhMaOr93], [AhEtal95]
telephone operator scheduling
[AhMaOr93], [AhEtal95]
nurse staﬀscheduling
[AhMaOr93]
machine scheduling
[AhMaOr93]
production scheduling
[EvMi92]
equipment replacement
[AhMaOr93]
microdata ﬁle merging
[AhEtal95], [EvMi92]
warehouse layout
[AhMaOr93], [AhEtal95]
facility location
[AhMaOr93], [AhEtal95]
determining service districts
[AhMaOr93], [AhEtal95], [EvMi92]
capacity expansion
[AhMaOr93]
vehicle ﬂeet planning
[AhMaOr93]

Section 10.6
COMMUNICATION NETWORKS
779
10.6
COMMUNICATION NETWORKS
Modern communication networks consist of two main components. Using high-capacity
links, the backbone network interconnects switching centers and gateway vertices that
carry and direct traﬃc through the communication system. Local access networks trans-
fer traﬃc between the backbone network and the end users. This section presents several
optimization models used in the design of such communication networks.
10.6.1
CAPACITATED MINIMUM SPANNING TREE PROBLEM
The capacitated minimum spanning tree problem arises in the design of local access
tree networks in which end users generate and retrieve data from other sources, always
through a speciﬁed control center (e.g., a communication switch of the backbone net-
work). In this problem, user sites are to be interconnected at minimum cost by means
of subtrees, which are in turn connected to the control center. The total traﬃc in each
subtree is limited by a capacity constraint.
Deﬁnitions:
Let N = {1, 2, . . ., n} be a set of terminals and let 0 denote a speciﬁed control center.
The complete undirected graph G = (V, E) has vertex set V = N ∪{0} and contains all
possible edges between distinct vertices of V (§8.1.3).
The cost of connecting distinct vertices i, j ∈V is cij = ce, where e = (i, j) ∈E.
The demand wi at vertex i ∈N is the amount of traﬃc to be transmitted to the control
center.
Relative to a spanning tree T (§9.2.1) of G, vertex j is a root vertex if it is adjacent to
vertex 0. Vertex i is assigned to root vertex j if j is on the unique path in T joining i
to the control center. The set of all vertices assigned to j deﬁnes the subtree Tj of T .
This subtree has demand D(Tj) = P
i∈Tj
wi.
A capacitated minimum spanning tree (CMST) is a spanning tree T of G composed
of subtrees Tj1, Tj2, . . . , Tjr such that
•
P
e∈T
ce is minimum;
• the demand in each Tj is at most Q, a speciﬁed capacity.
For j ∈N, let yj ∈{0, 1} indicate whether vertex j is a root vertex.
For i, j ∈N, let xij ∈{0, 1} indicate whether vertex i is assigned to root vertex j, and
let zij ∈{0, 1} indicate whether edge (i, j) belongs to the spanning tree T .
Given a vector z, the subgraph G(N, Ez) of G induced by z has vertex set N and edges
e ∈Ez if ze > 0. Similarly, given a vector (z, y), the subgraph induced by (z, y),
written G(V, Ezy), has vertex set V ; it contains every edge of Ez plus each edge from j
to 0 where yj > 0.
Relative to a given vector z, C(i, j) denotes the set of all i-j cuts [S, S] in the graph
G(N, Ez). (See §10.4.1.)

780
Chapter 10
NETWORKS AND FLOWS
If S ⊆V , then E(S) = {(i, j) ∈E | i, j ∈S} contains all edges between vertices of S.
For I ⊂N, let b(I) be the minimum number of subtrees needed to pack all terminals in
I. That is, b(I) is the optimal solution to the bin packing problem (§16.3.2) with bins
of capacity Q and items of size wi for every i ∈I.
A set S ⊂N is a cover if P
i∈S wi > Q. If also (P
i∈S wi) −wk ≤Q for all k ∈S,
then S is a minimal cover.
Facts:
1. The CMST problem has the following 0-1 integer linear programming formulation
(§16.1.8):
minimize:
n−1
P
i=1
nP
j=i+1
cijzij +
nP
j=1
c0jyj
subject to:
nP
j=1
xij = 1, for all i ∈{1, 2, . . ., n}
nP
i=1
wixij ≤Qyj, for all j ∈{1, 2, . . ., n}
xij ≤yj, for all i, j ∈{1, 2, . . ., n}
xij ≤P
e∈K
ze, for all i, j ∈{1, 2, . . ., n} (i ̸= j) and for all K ∈C(i, j)
P
e
ze + P
j
yj = n
xij ∈{0, 1}, for all i, j ∈{1, 2, . . ., n}
yj ∈{0, 1}, for all j ∈{1, 2, . . ., n}
zij ∈{0, 1}, for all i, j ∈{1, 2, . . ., n} (i ̸= j).
2. In Fact 1,
• the ﬁrst set of constraints ensures that each vertex is assigned to a root vertex;
• the second set of constraints ensures that the ﬂow through any root vertex is no
more than the capacity Q;
• the third set of constraints ensures that vertex i can be assigned to vertex j only
if j is a root vertex;
• the fourth set of constraints ensures that if vertex i is assigned to root vertex j,
then there must be a path between i and j;
• the ﬁfth set of constraints guarantees that G(V, Ezy) is a tree.
3. Savings heuristic:
This greedy heuristic (Algorithm 1) begins with n components,
each a single vertex, and successively merges pairs of components to reduce the total cost
by the largest amount.
4. The quantity suv computed in Algorithm 1 represents the savings in joining sub-
trees Tu and Tv to one another, compared to joining both to vertex 0.
5. The savings heuristic, developed by Esau and Williams [EsWi66], was one of the ﬁrst
heuristics developed for the CMST problem.
6. The savings heuristic, surprisingly eﬀective in practice, has attracted signiﬁcant at-
tention. Relatively simple adjustments have been proposed to improve its performance
while maintaining its simplicity.
• Dai and Fujino [DaFu00] use component-oriented savings computations in order
to increase its eﬃciency;

Section 10.6
COMMUNICATION NETWORKS
781
• Bruno and Laporte [BrLa02] propose enhancements to the procedure to connect
subtrees;
•
¨Oncan and Altinel [¨OnAl09] introduce parametric improvements which control
edge selections and combine distance and demand information in its savings
criterion.
Algorithm 1:
Savings heuristic.
input: undirected network G, control center 0, capacity limit Q
output: an approximate capacitated minimum spanning tree T ∗
U := {1, 2, . . ., n}
Tu := {u} for u ∈U
while true
for u ∈U
compute fu, the minimum cost of connecting the control center 0 to com-
ponent Tu
S := ∅
for u, v ∈U (u ̸= v)
if D(Tu ∪Tv) ≤Q then
compute suv, the diﬀerence between max{fu, fv} and the minimum cost
of connecting Tu to Tv
if suv > 0 then S := S ∪{(u, v)}
if S = ∅then return
else
choose u0, v0 such that su0v0 = max{suv | (u, v) ∈S}
merge Tu0 and Tv0, creating a new subtree indexed by min{u0, v0}, and
update U appropriately
7. Optimal tour partitioning heuristic:
This heuristic (Algorithm 2), developed by
Altinkemer and Gavish [AlGa88], is based on ﬁnding a traveling salesman tour (§10.7.1)
in a certain derived graph.
Algorithm 2:
Optimal tour partitioning heuristic.
input: undirected network G, control center 0, capacity limit Q
output: an approximate capacitated minimum spanning tree T ∗
ﬁnd a traveling salesman tour on the vertex set V = N ∪{0}
let 0 = x(0), x(1), . . . , x(n) be an ordering of the vertices on the tour
construct the directed graph H with vertex set V and arc costs Cjk:
if j < k and Pk
i=j+1 wx(i) ≤Q then Cjk := cx(0),x(j+1) + Pk−1
i=j+1 cx(i),x(i+1)
else Cjk := ∞
ﬁnd a shortest path P from x(0) to x(n) in H
use P = [x(0), x(u), x(v), . . . , x(t), x(n)] to deﬁne T ∗via the subtrees
{x(0), x(1), x(2), . . . , x(u)}, {x(0), x(u+1), x(u+2), . . . , x(v)}, . . . ,
{x(0), x(t+1), x(t+2), . . . , x(n)}

782
Chapter 10
NETWORKS AND FLOWS
8. In Algorithm 2, every path from x(0) to x(n) in the directed graph H generates a
collection of subtrees satisfying the capacity restriction.
9. The performance of Algorithm 2 depends on the initial traveling salesman tour cho-
sen. If an optimal traveling salesman tour is used, then the worst-case relative error
bound of the algorithm is 4 −4
Q. That is, bZ/Z∗≤4 −4
Q, where bZ is the cost of the
heuristic solution generated and Z∗is the cost of the optimal design.
10. Exact algorithms:
A number of exact algorithms are based on mathematical pro-
gramming approaches:
• Gavish [Ga85] develops a Lagrangian relaxation based algorithm and uses it to
solve problems with homogeneous (unit) demands;
• Araque, Hall, and Magnanti [ArHaMa90] derive valid inequalities and facets for
the CMST problem;
• Hall [Ha96] and Bienstock, Deng, and Simchi-Levi [BiDeSi94] develop additional
valid inequalities and facets and use them in a branch-and-cut algorithm;
• Gouveia and Martins [GoMa99], [GoMa00], [GoMa05] propose branch-and-cut
algorithms over hop-indexed formulations with tight lower bounds for unitary
demand problems;
• Uchoa et al. [UcEtal08] present a robust branch-cut-and-price algorithm over an
arc formulation strengthened by an exponential number of constraints on q-
arb structures. Various powerful cuts are implemented without increasing the
complexity of the problem and which improve upon previous formulations.
11. The CMST formulation given in Fact 1 can be improved by adding the various
inequalities listed in Facts 12–15.
12. Knapsack inequalities: Let S be a minimal cover. For every l ∈N, the inequality
P
i∈S
xil ≤(|S| −1)yl
is valid for the CMST problem.
13. Subtour elimination inequalities:
For any I ⊂N, let P = {S1, S2, . . . , S|I|} be a
partition of N −I into |I| subsets, some of which may be empty. For every i ∈I, let Si
be the unique subset from P associated with it. Then
P
e∈E(I)
ze + P
j∈I
yj + P
i∈I
P
j∈Si
xij ≤P
i∈I
P
j∈N
xij
is valid for the CMST problem.
14. Generalized subtour elimination inequalities: For any I ⊂N, the inequality
P
e∈E(I)
ze ≤|I| −b(I)
is valid for the CMST problem.
15. Cluster inequalities:
Consider p sets of vertices S1, S2, . . . , Sp ⊂N with p ≥3. If
the conditions
• S0 =
pT
i=1
Si ̸= ∅, and Si −S0 ̸= ∅for i = 1, 2, . . . , p
•
P
i∈Sk∪Sl
wi > Q for all 1 ≤k < l ≤p

Section 10.6
COMMUNICATION NETWORKS
783
are satisﬁed, then
pP
i=1
P
e∈E(Si)
ze ≤
pP
i=1
|Si| −2p + 1
is valid for the CMST problem.
16. Metaheuristics: Since 1995 there has been growing interest in metaheuristics with
the aim of solving larger, more diﬃcult instances. The methods proposed include neigh-
borhood exploration [AhOrSh01], [AhOrSh03], [AmDoVo96], [SoDuRi03], tabu search
[ShEtal97], ant colony optimization [ReLa06], ﬁlter-and-fan algorithms [ReMa11], and
genetic algorithms [RuEtal15].
Examples:
1. The following ﬁgure presents data for a problem involving n = 5 terminals and a
control center 0. Part (a) gives the cost cij of constructing each edge (i, j) as well as
the demand wi at each vertex i. The objective is to construct a minimum cost set of
subtrees connected to vertex 0, in which the demand generated by any subtree is at most
Q = 150. Part (b) shows a feasible capacitated spanning tree T , which contains two root
vertices (at 2 and 3). The total demand in subtree T2 is w2 + w4 + w5 = 150 ≤Q and
the total demand in subtree T3 is w1 + w3 = 95 ≤Q. The spanning tree T has total cost
21, the sum of the displayed edge costs cij.
4
5
1
3
0
2
cij
c0i
j
i
wi
 
2 
3 
4 
5 
1 
2 
4 
3 
4 
8 
40
2 
 
5 
2 
3 
5 
35
3 
 
 
6 
5 
7 
55
4 
 
 
  
4 
6 
50
5 
 
 
 
 
6 
65
(a)
(b)
5
2
3
7
4
2. Algorithm 1 is applied to the problem data in the ﬁgure of Example 1. To begin, ﬁve
subtrees are selected, each a single vertex and each joined to the control center 0. Thus,
f1 = 8, f2 = 5, f3 = 7, f4 = 6, and f5 = 6. Then s12 = 8 −2 = 6, s13 = 8 −4 = 4, . . . ,
s35 = 7 −5 = 2, and s45 = 6 −4 = 2.
The largest savings occurs for (1, 2) so T1
and T2 are merged, giving the new tree T1 with root vertex 2 and the single edge (1, 2).
Next, the fu and suv are updated.
For example, f1 = min{8, 5} = 5, f3 = 7, and
s13 = 7 −min{c13, c23} = 7 −4 = 3. The largest savings is found to be s14, so T1 and T4
are merged, giving the new tree T1 with root vertex 2 and edges (1, 2) and (2, 4). At
the next stage T3 and T5 are merged, giving the new tree T3 with root vertex 5 and
the single edge (3, 5). Since no further merging can take place (without violating the
capacity constraint), the savings heuristic terminates with the spanning tree shown in
the following ﬁgure (see next page), having total cost 20.
10.6.2
CAPACITATED CONCENTRATOR LOCATION PROBLEM
The capacitated concentrator location problem is frequently used to locate concentrators
in local access networks and switching centers in the backbone network. In either case,

784
Chapter 10
NETWORKS AND FLOWS
5
2
2
5
5
6
0
3
4
1
2
concentrators of ﬁxed capacity are to be located at a subset of possible sites.
Each
given terminal of the network is to be connected to exactly one concentrator, so that the
concentrator’s capacity is not exceeded. A feasible conﬁguration having minimum total
cost is then sought.
Deﬁnitions:
Let N = {1, 2, . . ., n} be a speciﬁed set of terminals, where terminal i uses wi units of
capacity. Let M = {1, 2, . . ., m} be a set of possible sites for concentrators, each of
ﬁxed capacity Q.
The bipartite graph G = (V, E) has vertex set V = N ∪M and edges (i, j) for all possible
assignments connecting terminal i ∈N to a concentrator at site j ∈M.
If a concentrator is located at site j, the set-up cost is vj, for j ∈M. The connection
cost of connecting terminal i to a concentrator at site j is cij, for i ∈N and j ∈M.
The capacitated concentrator location problem (CCLP) involves ﬁnding locations
for concentrators and an assignment of terminals to concentrators such that
• the sum of set-up and connection costs is minimum;
• the total capacity required by the terminals assigned to each concentrator is at
most Q.
For j ∈M, let yj ∈{0, 1} indicate whether a concentrator is located at site j.
For i ∈N and j ∈M, let xij ∈{0, 1} indicate whether terminal i is connected to a
concentrator at site j.
Facts:
1. There are two variants of capacity constraints, based on (a) each concentrator’s pro-
cessing capacity, and (b) the number of connection ports. Han and Raja [HaRa03] study
an extended version of the problem with both types of constraints.
2. The classical capacitated concentrator location problem with only processing capac-
ity constraints (i.e., variant (a)) is also known as the single source capacitated facility
location problem.
3. The classical CCLP has the following 0-1 integer linear programming formulation
(§16.1.8):
minimize:
nP
i=1
m
P
j=1
cijxij +
m
P
j=1
vjyj
subject to:
m
P
j=1
xij = 1, for all i ∈N
nP
i=1
wixij ≤Qyj, for all j ∈M
xij ≤yj, for all i ∈N, j ∈M
xij ∈{0, 1}, for all i ∈N, j ∈M
yj ∈{0, 1}, for all j ∈M.

Section 10.6
COMMUNICATION NETWORKS
785
4. In Fact 3,
• the ﬁrst set of constraints ensures that each terminal is connected to exactly one
concentrator;
• the second set of constraints ensures that the concentrator’s capacity is not ex-
ceeded;
• third set of constraints ensures that terminal i can be assigned to a concentrator
at site j only if a concentrator is located at site j.
5. In variant (b), wi = 1, for all i ∈N and Q is the number of connection ports.
6. A number of algorithms have been proposed for this problem, most of which are
based on a Lagrangian relaxation approach, while some are based on polyhedral analysis
[NeWo99].
7. Yang, Chu, and Chen [YaChCh12] present a cut-and-solve algorithm which they
apply to large sized instances.
8. Guastaroba and Speranza [GuSp14] extend the kernel search framework with variants
based on variable ﬁxing to binary integer linear programming models and apply it to this
problem with high-quality results. The heuristic is based on optimally solving a sequence
of subproblems, where each subproblem is restricted to a subset of the decision variables.
The subsets of decision variables are constructed starting from the optimal values of the
linear relaxation.
Examples:
1. The following table shows the data for a problem with four terminals i and three
possible sites j for locating concentrators. Let the capacity of any concentrator be Q = 30
for variant (a) and Q = 2 for variant (b).
j
i
cij
 
1 
2 
3 
wi
1 
8 
6 
4 
11
2 
2 
4 
3 
12
3 
5 
5 
3 
 9
4 
3 
2 
6 
 8
vj 
9 
7 
8
2. For variant (a), one feasible conﬁguration is to connect terminals 2, 3, 4 to a concen-
trator at site 2, and to connect terminal 1 to a concentrator at site 3. See part (a) of the
following ﬁgure.
The concentrator at site 2 has total capacity w2+w3+w4 = 29 ≤30 and the concentrator
at site 3 has total capacity w1 = 11 ≤30. The connection cost is c22 + c32 + c42 + c13 =
15 and the set-up cost is v2 + v3 = 15, giving a total cost of 30.
Another feasible
conﬁguration, shown in part (b), is to connect terminals 2 and 4 to a concentrator at
site 1, and to connect terminals 1 and 3 to a concentrator at site 3. The connection cost
is 12 and the set-up cost is 17, giving the smaller total cost 29.
3. For variant (b) only the second conﬁguration is feasible.

786
Chapter 10
NETWORKS AND FLOWS
10.6.3
NETWORK DESIGN PROBLEM
Fiber-optic and opto-electronic cable technologies, together with traditional copper ca-
bles, provide many possible choices for link capacities and oﬀer economies of scale. In the
capacity assignment problem, a point-to-point communication demand is given between
various pairs of vertices of the (typically, backbone) network. The objective is to install
links of several types (capacities) to transfer all communication demand without violat-
ing link capacities and to do so at minimum total cost. The special case involving two
types of transmission media, also referred to in the literature as the two-facility network
design problem or the network loading problem, is discussed here.
Deﬁnitions:
Let G = (V, E) be an undirected graph with vertex set V and edge set E.
Each communication demand is represented by a commodity k ∈K, where K is the
set of commodities. Commodity k ∈K has a required ﬂow in G of dk units between its
origin vertex O(k) and its destination vertex D(k).
Two types of cables can be installed: low capacity cables have capacity L, and high
capacity cables have capacity H. Let ae (be) denote the installation cost for each low
capacity (high capacity) cable on edge e ∈E.
The network design problem (NDP) involves ﬁnding a mix of low and high capacity
cables for each edge of G such that
• the total installation cost is minimum;
• all communication demands dk are met;
• the ﬂow on each edge does not exceed its installed capacity.
Let xe = xij (ye = yij) be the number of low capacity (high capacity) cables installed on
edge e = (i, j).
Let f k
ij be the amount of commodity k that ﬂows from i to j on edge (i, j).
Facts:
1. The NDP has the following mixed integer linear programming formulation (§16.1.8):

Section 10.6
COMMUNICATION NETWORKS
787
minimize:
P
e∈E
(aexe + beye)
subject to:
P
j∈V
f k
j,O(k) −P
j∈V
f k
O(k),j = −dk, for all k ∈K
P
j∈V
f k
j,D(k) −P
j∈V
f k
D(k),j = dk, for all k ∈K
P
j∈V
f k
ji −P
j∈V
f k
ij = 0, for all k ∈K and for all i ∈V \ {O(k), D(k)}
P
k∈K
(f k
ij + f k
ji) ≤Lxij + Hyij, for all (i, j) ∈E
xe, ye ≥0 integer, for all e ∈E
f k
ij, f k
ji ≥0, for all (i, j) ∈E and for all k ∈K.
2. In Fact 1,
• the ﬁrst three sets of constraints are the standard mass balance constraints (see
§10.4.1);
• the next set of constraints enforces the capacity constraint on the total ﬂow
through edge e = (i, j).
3. The NDP becomes harder to solve when more than one type of facility is involved,
due to the complexity of the cost structure. The two-facility NDP is strongly NP-hard
as it contains the ﬁxed-charge NDP and the Steiner tree problem as special cases.
4. Various polyhedral approaches for network design problems are discussed in [BiGu95]
and [MaMiVa95]. Hamid and Agarwal [HaAg15] consider 3-partitions of the original
graph and derive a family of facets that signiﬁcantly strengthen the linear formulation
and reduce the integrality gap.
Example:
1. In the network G of the following ﬁgure, the costs (ae, be) are shown for each edge e;
here L = 2 and H = 5. There are k = 3 communication demands (commodities): d1 = 12
between vertices 1 and 4, d2 = 10 between vertices 2 and 5, and d3 = 9 between vertices 1
and 5. A feasible assignment of ﬂows and capacities to edges is displayed in part (b)
of the ﬁgure. For instance, edge (1, 3) carries 7 units of commodity 1 and 9 units of
commodity 3, for a total ﬂow of 16 units. There are three high capacity cables and one
low capacity cable installed on this edge giving a total capacity of 3H + L = 17, at a
cost of 3 · 5 + 1 · 3 = 18. The total installation cost for this assignment is 114.
fij
k
  e 
1 
2 
3 
xe 
ye
(1,2) 
5 
 
 
0 
1
(1.3) 
7 
 
9 
1 
3
(2,3) 
 
4 
 
0 
1
(2,4) 
5 
6 
 
3 
1
(3,4) 
7 
 
3 
0 
2
(3,5) 
 
4 
6 
0 
2
(4,5) 
 
6 
3 
0 
2
2
1
3
4
5
(a)
(b)
(3,5)
(7,15)
(5,9)
(4,7)
(4,9)
(5,9)
(3,7)

788
Chapter 10
NETWORKS AND FLOWS
10.6.4
MODELS FOR SURVIVABLE NETWORKS
The introduction of ﬁber-optic technology has provided high capacity links and makes
it possible to design communication networks with low-cost sparse topologies. Unfortu-
nately, sparse networks are very vulnerable; a failure in one edge or vertex can disconnect
many users from the rest of the network. This is the prime motivation for studying the
design of survivable networks.
Deﬁnitions:
Let G = (V, E) be an undirected graph with vertex set V and edge set E.
The cost of establishing edge e ∈E is given by ce. The cost of a subnetwork H = (V, F)
of G is P
e∈F
ce.
Associated with every vertex s ∈V is a corresponding number rs, indicating a desired
level of redundancy.
A spanning subnetwork H = (V, F) of G is said to satisfy the edge (vertex) connec-
tivity requirement if for every distinct pair s, t ∈V there are at least rst = min{rs, rt}
edge-disjoint (vertex-disjoint) paths between s and t in H.
Deﬁne xe, for e = (i, j), to be the number of edges connecting vertex i to vertex j.
Facts:
1. The problem of designing a minimum cost subnetwork that satisﬁes all edge connec-
tivity requirements has the following integer programming formulation (§16.1.8):
minimize:
P
e∈E
cexe
subject to:
P
e∈[S,S]
xe ≥
max
(i,j)∈[S,S]
rij, for all S ⊂V, S ̸= ∅
xe ≥0 integer, for all e ∈E.
2. The model in Fact 1, analyzed by Goemans and Bertsimas [GoBe93], allows multiple
edges connecting the same two vertices.
3. Gr¨otschel, Monma, and Stoer [GrMoSt92] analyze a related survivability model in
which multiple edges are forbidden. In this case, xe is restricted to be 0 or 1 in the
formulation of Fact 1.
Examples:
1. Part (a) of the following ﬁgure shows a network G having four vertices and six edges;
the cost ce of each edge e is also displayed.
(a)
5
4
3
7
7
2
2
2
3
4
1
3
4
1
3
4
1
6
(b)
(c)
Suppose that the speciﬁed redundancies are r1 = 1 and r2 = r3 = r4 = 2. The spanning
subnetwork H shown in part (b), with cost 22, satisﬁes the vertex connectivity require-
ment. For example, there are min{r3, r4} = 2 vertex-disjoint paths joining vertices 3

Section 10.6
COMMUNICATION NETWORKS
789
and 4 in H: namely, [3, 2, 4] and [3, 1, 4]. Also, there are min{r2, r4} = 2 vertex-disjoint
paths joining vertices 2 and 4: [2, 4] and [2, 3, 1, 4].
2. Part (c) of the ﬁgure for Example 1 shows another spanning subnetwork H′ with
cost 20 that satisﬁes the stated vertex connectivity requirement. For example, there are
min{r2, r4} = 2 vertex-disjoint paths joining vertices 2 and 4 in H′: [2, 4] and [2, 3, 4].
Notice that there is min{r1, r3} = 1 path joining 1 and 3 in H′, but not two such
vertex-disjoint paths.
10.6.5
CONNECTED FACILITY LOCATION PROBLEM
In designing the last mile of telecommunication networks, the ﬁber-to-the-curb strategy
can be modeled as the connected facility location problem. Namely, ﬁber optic cables
run to a cabinet serving a neighborhood; end users connect to this cabinet using copper
connections; and expensive switching devices are installed in these cabinets. The rele-
vant problem is to minimize the overall cost of the network by determining positions of
cabinets, deciding which customers to connect to them, and how to reconnect cabinets
among each other and to the backbone.
Deﬁnitions:
Let G = (V, E) be an undirected graph with vertex set V and edge set E.
The vertex set V is partitioned into three disjoint sets D, F, S: D is the set of demand
vertices, F is the set of potential facility vertices, and S is the set of potential Steiner
vertices. Thus, V = D ∪F ∪S.
Facility opening costs ci ≥0 are incurred for each open facility i ∈F. Assignment
costs aij ≥0 are incurred for assigning customer j ∈D to a facility i ∈F. Core edge
costs bij ≥0 are incurred for any edge (i, j) ∈E(F ∪S) if it is used by the Steiner
tree T connecting open facilities.
The vertices in S may be viewed as pure Steiner vertices and can only be used in the
tree T as Steiner vertices, while the vertices in F may be used as Steiner vertices in the
tree T with no facility opening cost incurred if no customers are assigned to them.
The ﬁnal network cost is given by P
i∈Z ci + P
(i,j)∈E(T ) bij + P
j∈D ai(j)j, where i(j) is
the facility serving demand vertex j, Z is the set of open facilities, and T is a Steiner
tree connecting the open facilities.
The connected facility location (ConFL) problem involves ﬁnding a subset of open
facilities such that
• each customer is assigned to exactly one open facility;
• a Steiner tree connects all open facilities;
• the total network cost is minimized.
Let xe = xij ∈{0, 1} (ye = yij ∈{0, 1}) indicate whether an assignment edge (core edge)
is installed on edge (i, j).
Let zi ∈{0, 1} indicate whether there is an open facility at vertex i.
Specify vertex r ∈D as the root vertex for the Steiner tree and designate a commodity
k for each demand vertex that has origin r and destination k ∈D.

790
Chapter 10
NETWORKS AND FLOWS
Let f k
ij be the amount of commodity k that ﬂows from i to j on edge (i, j) for demand
vertex k ∈D.
Facts:
1. Problem ConFL has the following mixed integer programming formulation (§16.1.8):
minimize:
P
i∈F cizi + P
(i,j)∈E(S∪F ) bijyij + P
i∈F, j∈D aijxij
subject to:
P
i∈F xij = 1, for all j ∈D
xij ≤zi, for all i ∈F and for all j ∈D
P
j∈V
f k
ji −P
j∈V
f k
i,j = −1, for i = r and for all k ∈D
P
j∈V
f k
ji −P
j∈V
f k
ij = 1, for i = k and for all k ∈D
P
j∈V
f k
ji −P
j∈V
f k
ij = 0, for i ∈V \ {r, k} and for all k ∈D
f k
ij ≤yij, for all (i, j) ∈E(S ∪F) and for all k ∈D
f k
ij ≤xij, for all i ∈F, for all j ∈D, and for all k ∈D
xij ∈{0, 1}, for all (i, j) ∈E
yij ∈{0, 1}, for all (i, j) ∈E(S ∪F)
zl ∈{0, 1}, for all l ∈S ∪F
f k
ij ≥0, for all (i, j) ∈E and for all k ∈D.
2. In Fact 1,
• the ﬁrst two sets of constraints enforce that each customer is assigned to an open
facility;
• the next set of constraints are the standard mass balance constraints (§10.4.1);
• the last two sets of constraints ensure that ﬂow is only routed through edges on
the tree.
3. The ConFL problem encompasses a family of network problems that combine facility
location and network design; namely, the rent-or-buy problem, the Steiner tree star
problem, and the general Steiner tree star problem.
4. Various approximation algorithms, heuristics and exact methods have been proposed.
5. Bardossy and Raghavan [BaRa10] propose a dual-based local heuristic to obtain both
a high-quality heuristic solution and a tight lower bound. In the ﬁrst stage the problem
is modeled as a directed Steiner tree problem with a unit degree constraint at the root
vertex. Then a dual-ascent procedure is applied to obtain a lower bound and a feasible
solution to the problem. In the second stage a local search procedure is used to improve
the feasible solution.
6. Gollowitzer and Ljubic [GoLj11] present various mixed integer programming formu-
lations and compare the models in terms of their bounds and computational time.
Example:
1. Consider the network G with vertices D = {1, 2, 3, 4, 5, 6, 7}, F = {8, 9, 10, 11}, and
S = {12, 13}, as seen in part (a) of the following ﬁgure. Edges of G are deﬁned by a
complete bipartite graph between facilities and demand vertices, and a complete graph
between facilities and potential Steiner nodes. The assignment costs aij and the core
edge costs bij are shown in part (c) and part (d), respectively. The facility opening cost
is ci = 1 for all facility vertices. A feasible assignment of customers to facilities and the
connection of open facilities is displayed in part (b) of the ﬁgure. The overall cost for

Section 10.6
COMMUNICATION NETWORKS
791
this solution is 22. The total assignment cost is 10, the total core edge cost for T is 9,
and the total facility opening cost is 3. Vertex 9 plays the role of a Steiner vertex and
so does not incur an opening cost. This solution turns out to be optimal.
10.6.6
REGENERATOR LOCATION PROBLEM
In optical telecommunication networks, an optical signal can only travel a maximum
distance before its quality deteriorates to the point that it must be regenerated by a re-
generator on the network. As the cost of a regenerator is high, the goal is to deploy as few
regenerators as possible in the network, while ensuring that all vertices can communicate
with one another.
Deﬁnitions:
Let G = (N, E) be a communication graph with vertex set N and edge set E.
Let S ⊆N be the set of candidate locations where regenerators can be placed, and
let T ⊆N be the set of terminal vertices that must communicate with one another.
Each edge e = (i, j) ∈E represents a simple path between i and j. In other words, the
length of the path does not exceed the maximum distance that can be traveled by the
optical signal such that no regenerator is necessary between them.
The installation cost for placing a regenerator at location i ∈S is ci.
The regenerator location problem (RLP) involves locating regenerators in the can-
didate locations S such that

792
Chapter 10
NETWORKS AND FLOWS
• the total regenerator installation cost is minimum;
• all terminal vertices can communicate with one another.
Let xi ∈{0, 1} indicate whether a regenerator is located at vertex i.
Specify a vertex r ∈T as the source vertex for all communications. Deﬁne a commodity
k for each terminal vertex k ∈T \ {r} that has origin r and destination k.
Let yrj ∈{0, 1} indicate whether there is any ﬂow from r to j on edge (r, j).
Let f k
ij be the amount of commodity k that ﬂows from i to j on edge (i, j) for terminal
vertex k.
Facts:
1. The RLP has the following mixed integer linear programming formulation (§16.1.8):
minimize:
P
i∈S
cixi
subject to:
P
j∈N
f k
ji −P
j∈N
f k
ij = −1, for i = r and for all k ∈T \ {r}
P
j∈N
f k
ji −P
j∈N
f k
ij = 1, for i = k and for all k ∈T \ {r}
P
j∈N
f k
ji −P
j∈N
f k
ij = 0, for i ∈N \ {r, k} and for all k ∈T \ {r}
f k
ij ≤xj, for all (i, j) ∈E and for all k ∈T
f k
rj ≤yrj, for all j ∈N \ {r} and for all k ∈T
P
(r,j)∈E
yrj ≤1 + Mxr
xi ∈{0, 1}, for all i ∈S
yrj ∈{0, 1}, for all (r, j) ∈E
f k
ij ≥0, for all (i, j) ∈E and for all k ∈T.
2. In Fact 1,
• the ﬁrst three sets of constraints are the standard mass balance constraints (see
§10.4.1);
• the next set of constraints enforces the installation of a regenerator for passing
communications;
• the following sets of constraints ensure that the source vertex r has unit degree or
a regenerator installed. Here M is a suﬃciently large constant with M ≥|T |−2.
3. The regenerator location problem is NP-hard.
4. Chen, Ljubic, and Raghavan [ChLjRa15] establish that the RLP is equivalent to the
vertex-weighted directed Steiner forest problem.
Using this fact, they derive various
integer and mixed integer programming formulations. They also propose construction
heuristics combined with local search.
5. When N = S = T , Chen, Ljubic, and Raghavan [ChLjRa10] show that the RLP is
equivalent to a maximum leaf spanning tree problem.
Example:
1. In the communication graph G of the following ﬁgure, S = {1, 3, 6}, T = {1, 2, 4, 5},
and the regenerator installation cost is 1 for all locations.
A feasible installation of
regenerators that ensures communication between all terminal vertices is displayed in part
(b) of the ﬁgure, where # indicates the location of regenerators. The total regenerator
installation cost is 2.

Section 10.7
DIFFICULT ROUTING AND ASSIGNMENT PROBLEMS
793
10.7
DIFFICULT ROUTING AND ASSIGNMENT PROBLEMS
An exact algorithm for a combinatorial optimization problem is a procedure that pro-
duces a veriﬁable optimal solution to every instance of this problem. A heuristic algo-
rithm produces a feasible (although not necessarily optimal) solution to each problem
instance. This section discusses exact and heuristic approaches to three classical com-
binatorial optimization problems: the traveling salesman problem, the vehicle routing
problem, and the quadratic assignment problem. These three problems have in common
the goal of minimizing the cost of movement or travel, generally of people or of materials.
10.7.1
TRAVELING SALESMAN PROBLEM
In the traveling salesman problem, a salesman starts out from a home city and is to visit
in some order a speciﬁed set of cities, returning home at the end. This journey is to be
designed to incur the minimum total cost (or distance). While the traveling salesman
problem has attracted the attention of many mathematicians and computer scientists, it
has resisted attempts to develop an eﬃcient solution algorithm.
Deﬁnitions:
Let G = (V, E) be a complete graph (§8.1.3) with V = {1, 2, . . ., n} the set of vertices
and E the set of all edges joining pairs of distinct vertices.
Each edge (i, j) ∈E has an associated cost or distance cij.
The distance between set S ⊆V and vertex j ̸∈S is dS(j) = min{cij | i ∈S}.
A Hamilton cycle or tour in G is a cycle passing through each vertex i ∈V exactly
once. (See §8.4.4.)
The cost of a cycle C is
P
(i,j)∈C
cij.
The traveling salesman problem (TSP) requires ﬁnding a Hamilton cycle in G of
minimum total cost.
The costs (distances) satisfy the triangle inequality if cij ≤cik + ckj holds for all
distinct i, j, k ∈V .

794
Chapter 10
NETWORKS AND FLOWS
In a Euclidean TSP, each vertex i corresponds to a point xi in R2 and cij is the distance
between xi and xj, relative to the standard real inner product (§6.1.4).
Tour construction procedures generate an approximately optimal TSP tour from the
costs cij.
Tour improvement procedures attempt to ﬁnd a smaller cost tour, given an initial
(often random) tour.
Composite (hybrid) procedures construct a starting tour from one of the tour con-
struction procedures and then attempt to ﬁnd a smaller cost tour using one or more of
the tour improvement procedures.
A k-change or k-opt exchange of a given tour is obtained by deleting k edges from
the tour and adding k other edges to form a new tour. A tour is k-optimal (k-opt) if
it is not possible to improve the tour via a k-change.
Metaheuristics are general-purpose procedures (such as tabu search, simulated anneal-
ing, genetic algorithms, or neural networks) for heuristically solving diﬃcult optimization
problems; these general methodologies for searching complex solution spaces can be spe-
cialized to handle speciﬁc types of optimization problems.
Facts:
1. The TSP is possibly the most well-known network optimization problem, and it
serves as a prototype for diﬃcult combinatorial optimization problems in the theory of
algorithmic complexity (§17.5.2).
2. The ﬁrst use of the term “traveling salesman problem” in a mathematical context
appears to have occurred in 1931–1932.
3. There are numerous applications of the TSP: drilling of printed circuit boards, cluster
analysis, sequencing of jobs, x-ray crystallography, archaeology, cutting stock problems,
robotics, and order-picking in a warehouse (see Examples 7–11).
4. There are (n−1)!
2
diﬀerent Hamilton cycles in the complete graph G. This means that
brute force enumeration of all Hamilton cycles to solve the TSP is not practical. (See
Example 1.)
5. The TSP is an NP-hard optimization problem (§17.5.2).
This remains true even
when the distances satisfy the triangle inequality or represent Euclidean distances.
6. If certain edges (i, j) of G are missing, then cij can be assigned a suﬃciently large
value M; e.g., one can set M to be greater than the sum of the n largest edge costs. The
TSP can then be solved on the complete graph G. If the (exact) solution obtained has
any edges with cost M, then there is no Hamilton cycle in the original graph.
7. Asymmetric traveling salesman problem: Certain applications require ﬁnding a min-
imum cost directed Hamilton cycle in a directed network H; here it is not required that
cij = cji holds for all arcs (i, j) of H. This asymmetric (directed) TSP can be transformed
into a TSP problem on an undirected network [J¨uReRi95].
8. A seminal paper of G. B. Dantzig, D. R. Fulkerson, and S. M. Johnson (1954) solved
a 49-city TSP to optimality by adding cutting planes (§16.1.8) to a linear programming
relaxation of the problem.
9. Although ingenious exact algorithms for the TSP have been proposed by numerous
authors, most encounter problems with storage and/or running time for cases with more
than ﬁve hundred vertices.

Section 10.7
DIFFICULT ROUTING AND ASSIGNMENT PROBLEMS
795
10. Exact approaches to the TSP are computationally intensive, especially for large
networks. Thus a large number of heuristic approaches have been developed to produce
useful, but not necessarily optimal, solutions to the TSP.
11. The wealth of TSP heuristics can be categorized into four broad classes: tour con-
struction procedures, tour improvement procedures, composite procedures, and meta-
heuristics.
12. Nearest neighbor heuristic:
This construction method (Algorithm 1) builds up a
tour by successively adding new vertices that are closest to a growing path.
Algorithm 1:
Nearest neighbor heuristic.
input: undirected network G = (V, E), costs cij
output: a traveling salesman tour
i0 := any vertex of G {the starting vertex}
W := V −{i0}
P := ∅
v := i0
while W ̸= ∅
let k ∈W be such that cvk = min{cvj | j ∈W}
add (v, k) to P
W := W −{k}
v := k
add (k, i0) to the path P to produce a tour
13. Using appropriate data structures, Algorithm 1 can be implemented to run in O(n2)
time.
14. Suppose zNN is the cost of a tour constructed by the nearest neighbor heuristic
and zOPT is the cost of an optimal TSP tour. Then there are examples for which
zNN
zOPT
is Θ(log n). This means that the cost of the tour produced by Algorithm 1 cannot be
bounded above by a constant times the cost of an optimal TSP tour.
15. Nearest insertion heuristic:
This construction method (Algorithm 2) builds up a
tour from smaller cycles by successively adding a vertex that is closest to the current
cycle C. The new vertex is inserted between two successive vertices in the cycle, in the
best possible way.
Algorithm 2:
Nearest insertion heuristic.
input: undirected network G = (V, E), costs cij
output: a traveling salesman tour
i := any vertex of G {the starting vertex}
j := subscript such that cij = min{cir | r ∈V −{i}}
S := {i, j}
C := {(i, j), (j, i)}
while S ̸= V
let k be such that dS(k) = min{dS(r) | r ∈V −S}
S := S ∪{k}

796
Chapter 10
NETWORKS AND FLOWS
ﬁnd an edge (u, v) ∈C so cuk + ckv −cuv = min{cxk + cky −cxy | (x, y) ∈C}
add (u, k) and (k, v) to C, and remove (u, v) from C
16. Using appropriate data structures, Algorithm 2 can be implemented to run in O(n2)
time.
17. Suppose zNI is the cost of a tour constructed by the nearest insertion heuristic and
zOPT is the cost of an optimal TSP tour. If the values cij satisfy the triangle inequality,
then
zNI
zOPT ≤2 holds for all TSP instances.
18. Clarke and Wright savings heuristic:
This construction method (Algorithm 3)
builds up a tour by successively adding an edge (i, j) having the largest savings sij, the
beneﬁt from directly connecting vertices i and j compared with joining each directly to
a central vertex.
Algorithm 3:
Clarke and Wright savings heuristic.
input: undirected network G = (V, E), costs cij
output: a traveling salesman tour
select any vertex (for example, 1) as the starting vertex
compute sij = c1i + c1j −cij for distinct i, j ∈V −{1}
order the savings si1j1 ≥si2j2 ≥· · · ≥sitjt
P := ∅
k := 0
while |P| < n −2
k := k + 1
if P ∪{(ik, jk)} is a vertex-disjoint union of paths then add (ik, jk) to P
connect the endpoints of P to vertex 1, forming a tour
19. Using appropriate data structures, Algorithm 3 can be implemented to run in
O(n2 log n) time.
20. Christoﬁdes’ heuristic:
This construction method (Algorithm 4) builds up a tour
from a minimum spanning tree to which are added certain other small cost edges. It is
assumed that the costs satisfy the triangle inequality.
Algorithm 4:
Christoﬁdes’ heuristic.
input: undirected network G, costs cij
output: a traveling salesman tour
T := minimum spanning tree of G with respect to the costs cij (see §10.1)
let S contain all odd-degree vertices in T
ﬁnd a minimum cost perfect matching M (§10.2) relative to vertices S of G and
using the costs cij
obtain a closed trail C by adding M to the edges of T
remove all edges but two incident with vertices of degree greater than 2 by ex-
ploiting the triangle inequality, transforming C into a tour

Section 10.7
DIFFICULT ROUTING AND ASSIGNMENT PROBLEMS
797
21. Using appropriate data structures, Algorithm 4 can be implemented to run in O(n3)
time.
22. Suppose zC is the cost of a tour constructed by Christoﬁdes’ heuristic and zOPT is
the cost of an optimal TSP tour. If the cij satisfy the triangle inequality, then
zC
zOPT ≤3
2
holds for all TSP instances.
23. The following table [J¨uReRi95] compares several of the most popular tour construc-
tion procedures on a set of 30 Euclidean TSPs from the literature with known optimal
solutions. These problems range in size from 105 to 2,392 vertices. Surprisingly, the
savings heuristic is the best tour construction heuristic of those tested. These results are
consistent with those of other studies.
heuristic
average percent above optimality
nearest neighbor
24.2
nearest insertion
20.0
Christoﬁdes
19.5
modiﬁed nearest neighbor
18.6
cheapest insertion
16.8
random insertion
11.1
farthest insertion
9.9
savings
9.8
modiﬁed savings
9.6
24. The best known tour improvement heuristics for the TSP involve edge exchanges
(Algorithm 5). Often the initial tour is chosen randomly from the set of all possible
tours.
Algorithm 5:
General edge-exchange heuristic.
input: undirected network G, initial tour
output: a traveling salesman tour
repeat improve the tour using an allowable edge exchange
until no additional improvement can be made
25. Specialized versions of Algorithm 5 typically use 2-opt exchanges, 3-opt exchanges,
and more complicated Lin-Kernighan [J¨uReRi95] edge exchanges. Such exchange tech-
niques have been used to generate excellent solutions to large-scale TSPs in a reasonable
amount of time.
26. Edge-exchange procedures are typically more expensive computationally than tour
construction procedures.
27. Tour improvement procedures typically require a “downhill move” (i.e., a strict
reduction in cost) in order for edge exchanges to be made. As a result, they terminate
with a local minimum solution.
28. Since the 2-opt exchange procedure is weaker than the 3-opt procedure, Algorithm 5
will generally terminate at an inferior local optimum using 2-opt exchanges instead of
3-opt exchanges. The Lin-Kernighan procedure will generally terminate with a better
local optimum than will a 3-opt exchange procedure.
29. In practice, it often makes sense to apply a composite procedure. The strategy is
to get a good initial solution rapidly (by tour construction), which is then improved by
an edge-exchange procedure.

798
Chapter 10
NETWORKS AND FLOWS
30. The following table [J¨uReRi95] compares several composite procedures on the same
sample problems described in Fact 23. In each case, the initial tour is constructed using
the nearest neighbor heuristic (Algorithm 1). The improvement procedures include 2-
opt, 3-opt, two variants of Lin-Kernighan, and iterated Lin-Kernighan. Iterated Lin-
Kernighan is the most computationally burdensome of the edge-exchange procedures,
but it consistently obtains results that are within 1% of optimality.
heuristic
average percent above optimality
2-opt
8.3
3-opt
3.8
Lin-Kernighan (variant 1)
1.9
Lin-Kernighan (variant 2)
1.5
Iterated Lin-Kernighan
0.6
31. Metaheuristics:
Unlike Algorithm 5 (which permits only downhill moves), meta-
heuristics [OsKe95] allow the possibility of nonimproving moves. For example, uphill
moves can be accepted either randomly (simulated annealing) or based upon determinis-
tic rules (threshold accepting). Memory can be incorporated in order to prevent revisiting
local minima already evaluated and to encourage discovering new ones (tabu search, see
§16.9).
Other metaheuristics such as evolutionary strategies, genetic algorithms, and neural net-
works have also been applied to the TSP. To date, neural networks and tabu search have
been less successful than the other approaches.
32. For a detailed history of the TSP, see the ﬁrst chapter of [LaEtal85] as well as the
very accessible exposition in [Co12].
33. The article [La10] provides a concise guide to the most eﬀective exact and heuristic
approaches for the TSP.
34. Software for the traveling salesman problem can be found in [LoPu07].
35. A library of sample problems, with their best known solutions, is available at
• http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95
36. To date, the largest non-trivial instance solved to optimality has 85,900 cities (solved
in 2006).
37. For exact algorithms, we refer interested readers to [ApEtal07].
Examples:
1. Brute force enumeration: Suppose that a TSP solution is required for the complete
graph G on n = 25 cities. By Fact 4, there are 24!
2
≈3.1 × 1023 Hamilton tours in
the graph G. Even with a supercomputer that is capable of ﬁnding and evaluating each
such tour in one nanosecond (10−9 seconds), it would take over 9.8 million years of
uninterrupted computations to determine an optimal TSP tour. This example illustrates
how quickly brute force enumeration of Hamilton tours becomes impractical.
2. Part (a) of the following ﬁgure shows the costs cij for a ﬁve city TSP. An initial tour
can be constructed using the nearest neighbor heuristic (Algorithm 1). Let the initial
vertex be i0 = 1, so W = {2, 3, 4, 5}. The closest vertex of W to 1 is 2, with c12 = 1,
so edge (1, 2) is added to the current path. A closest vertex of W = {3, 4, 5} to 2 is 5,
so edge (2, 5) is added to the path. Continuing in this way, edges (5, 3) and edge (3, 4)
are added, giving the path P = [1, 2, 5, 3, 4] and the tour [1, 2, 5, 3, 4, 1] with total cost
1 + 3 + 2 + 3 + 5 = 14. This tour is displayed in part (b).

Section 10.7
DIFFICULT ROUTING AND ASSIGNMENT PROBLEMS
799
j
i
cij
 
2 
3 
 4 
 5
1 
1 
2 
 5 
 4
2 
  
3 
 4 
 3
3 
  
  
 3 
 2
4 
 
 
 
 3
1
2
5
3
4
5
1
3
2
3
(a)
(b)
3. Suppose that the nearest insertion heuristic (Algorithm 2) is applied to the problem
data in part (a) of the ﬁgure for Example 2, starting with the initial vertex i = 1. The
nearest vertex to i is j = 2, giving the initial cycle C = {(1, 2), (2, 1)}. The closest vertex
to this cycle is k = 3, producing the new cycle C = {(1, 2), (2, 3), (3, 1)}. Relative to
S = {1, 2, 3}, dS(4) = 3 and dS(5) = 2, so vertex 5 will next be added to the cycle.
Since c15 + c52 −c12 = 6, c25 + c53 −c23 = 2, and c15 + c53 −c13 =
4, vertex 5
is inserted between vertices 2 and 3 in the current cycle, giving C = {(1, 2), (2, 5),
(5, 3), (3, 1)}. Finally, vertex 4 is added between vertices 2 and 5, producing the tour
C = {(1, 2), (2, 4), (4, 5), (5, 3), (3, 1)} with total cost 12.
4. The savings heuristic (Algorithm 3) can alternatively be applied to the problem
speciﬁed in part (a) of the ﬁgure for Example 2. The savings s23 = c12 + c13 −c23 =
1 + 2 −3 = 0. Similarly, s24 = 2, s25 = 2, s34 = 4, s35 = 4, and s45 = 6. This produces
the ordered list of edges [(4, 5), (3, 4), (3, 5), (2, 4), (2, 5), (2, 3)]. Considering edges in turn
from this list gives the path P = [3, 4, 5, 2]. Adding edges from the endpoints of P to
vertex 1 produces the tour [1, 3, 4, 5, 2, 1] with total cost 12.
5. Christoﬁdes’ heuristic (Algorithm 4) is now applied to the problem given in part (a)
of the ﬁgure for Example 2. A minimum spanning tree T consists of the following edges:
(1, 2), (1, 3), (3, 5), (3, 4); see part (a) of the following ﬁgure. Vertices 2, 3, 4, 5 have odd
degree and {(2, 4), (3, 5)} constitutes a minimum cost perfect matching on these vertices.
Adding these edges to those of T produces the multi-graph in part (b) of the following
ﬁgure. Replacing edges (4, 3) and (3, 5) having aggregate cost 5 by the single edge (4, 5)
of cost 3 produces the tour in part (c), having total cost 12.
1
1
2
4
3
5
2
1
1
2
3
2
2
2
2
1
4
3
3
4
5
2
4
5
3
2
1
4
3
2
(c)
(b)
(a)
6. To illustrate edge exchanges, consider the tour of cost 14 in part (b) of the ﬁgure
for Example 2. Removal of edges (1, 4) and (3, 5) disconnects the cycle into two disjoint
paths. Join the endpoints of one path to the endpoints of the other with edges (1, 3)
and (4, 5) to create a new tour [1, 2, 5, 4, 3, 1] of smaller cost 12. No further pairwise
exchanges reduce the cost of this tour, so this tour is a 2-opt local minimum solution.
7. Delivery routes:
A delivery truck must visit a set of customers in a city and then
return to the central garage after completing the route. Determining an optimal (i.e.,
shortest time) delivery route can be modeled as a traveling salesman problem on a city

800
Chapter 10
NETWORKS AND FLOWS
street network. Here the vertices represent the customer locations and the cost cij of
edge (i, j) is the driving time between locations i and j.
8. Printed circuit boards:
One application of the TSP occurs in fabricating printed
circuit boards.
Holes at a number of ﬁxed locations have to be drilled through the
board. The objective is to minimize the total time needed to move the drilling head from
position to position. Here the vertices i correspond to the locations of the holes as well
as the starting position of the drill. The cost cij represents the time required to move
the drilling head from i and reposition it at j. A minimum cost traveling salesman tour
gives an optimal way of sequencing the drilling of the holes.
9. Order-picking:
In a warehouse, a customer order requires a certain subset of the
items stored there. A vehicle must be sent to pick up these items and then return to the
central dispatch location. Here the vertices are the locations of the items as well as the
central dispatch location. The costs are the times needed to move the vehicle from one
location to the other. A minimum cost traveling salesman tour then gives an optimal
order in which to retrieve items from the warehouse.
10. Job sequencing: In a factory, materials must be processed by a series of operations
on a machine. The set-up time between operations varies depending on the order in
which the operations are scheduled. Determining an optimal ordering that minimizes
the total set-up time can be formulated as a traveling salesman problem.
11. Additional applications, with reference sources, are given in the following table.
application
references
dating archaeological ﬁnds
[AhEtal95]
DNA mapping
[AhEtal95]
x-ray crystallography
[J¨uReRi95]
engine design
[AhEtal95]
robotics
[J¨uReRi95]
clustering
[AhEtal95], [LaEtal85]
cutting stock problems
[HoPaRi13]
aircraft route assignment
[HoPaRi13]
computer wiring
[EvMi92], [J¨uReRi95], [LaEtal85],
genome sequence mapping
[AgEtal00]
fuel-optimal imaging strategies
[BaMcBe00]
power cables
[SlMaKa97]
10.7.2
VEHICLE ROUTING PROBLEM
Private ﬁrms and public organizations that distribute goods or provide services to cus-
tomer locations rely on a ﬂeet of vehicles. Given demands for service at numerous points
in a transportation network, the vehicle routing problem requires determining which cus-
tomers are to be serviced by each vehicle and the order in which customers on a route
are to be visited.
Deﬁnitions:
Let G = (V, E) be a complete graph with V = {1, 2, . . ., n} the set of vertices and E the
set of all edges joining pairs of distinct vertices (§8.1.3).

Section 10.7
DIFFICULT ROUTING AND ASSIGNMENT PROBLEMS
801
Vertex 1 is the central depot, whereas the other vertices represent customer loca-
tions. Customer location i has a known demand wi.
Each edge (i, j) ∈E has an associated distance or cost cij.
There are also available a number of vehicles, each having the same capacity Q.
A route is sequence of customers visited by a vehicle that starts and ends at the central
depot.
The vehicle routing problem (VRP) requires partitioning the set of customers into
a set of delivery routes such that
• the total distance traveled by all vehicles is minimum;
• the total demand generated by the customers assigned to each route is ≤Q.
In a construction heuristic for the VRP, subtours are joined as long as the resulting
subtour does not violate the vehicle capacity.
An improvement heuristic employs successive edge exchanges that reduce the total
distance without violating any vehicle capacity constraint.
A two-phase heuristic implements a cluster ﬁrst/route second philosophy, in which
customers are ﬁrst partitioned into groups Gk with P
i∈Gk
wi ≤Q, after which a minimum
distance sequencing of customers is found within each group.
Facts:
1. The TSP (§10.7.1) is a special case of the VRP in which there is a single vehicle with
unlimited capacity.
2. The VRP is an NP-hard optimization problem (§17.5.2).
3. VRPs with more than 100 vertices are diﬃcult to solve to optimality. The state-of-
the-art exact method is branch-and-cut-and-price.
4. Most solution strategies for large VRPs are heuristic in nature, involving construction,
improvement, and two-phase methods as well as metaheuristics.
5. In 1959 G. B. Dantzig and J. H. Ramser ﬁrst formulated the general vehicle rout-
ing problem and developed a heuristic solution procedure. This solution technique was
applied to a problem involving the delivery of gasoline to service stations.
6. The Clarke and Wright savings heuristic (§10.7.1) is a construction approach, origi-
nally proposed for the VRP. Algorithm 6 outlines this heuristic, which begins with each
customer served by a diﬀerent vehicle and successively combines routes in order of non-
increasing savings sij = c1i + c1j −cij to form a smaller set of feasible routes.
Algorithm 6:
Clarke and Wright savings heuristic.
input: undirected network G = (V, E), costs cij, capacity limit Q
output: a set of delivery routes
Ri := route consisting of edges (1, i) and (i, 1) for i ∈V −{1}
compute sij = c1i + c1j −cij for distinct i, j ∈V −{1}
order the savings si1j1 ≥si2j2 ≥· · · ≥sitjt
for k := 1 to t
if Rik and Rjk have combined demand at most Q then merge Rik and Rjk

802
Chapter 10
NETWORKS AND FLOWS
7. In two-phase methods, a minimum distance ordering of customers within each spec-
iﬁed cluster of vertices can be found by solving a TSP (§10.7.1).
8. In the past two decades, metaheuristics such as simulated annealing, tabu search,
genetic algorithms, record-to-record travel, and iterated neighborhood search have been
applied quite successfully to VRPs.
9. [LaRoVi14] compared the performances of a selected set of successful metaheuristics.
In terms of objective function values, the parallel algorithm that combined a heuristic
local search improvement with integer programming [GrGoWa11] performed best on both
CMT (50 to 200 customers) and GWKC (240 to 483 customers) benchmark instances.
10. Extensions to the basic VRP include modiﬁcations for asymmetric distances (cij
need not equal cji), heterogeneous ﬂeets, constraints on the distance of every route, mul-
tiple depots, time windows for deliveries, split deliveries, arc routing, inventory routing,
prize collecting routing, pick-up and delivery, and multi-objective routing.
11. A survey of 25 commercial software products for vehicle routing problems is avail-
able in [HaPa16]. This survey discusses interfaces with mapping systems, cloud-based
solutions, computer platforms supported, extensions to the basic VRP that are incorpo-
rated, and signiﬁcant installations of the product for industrial customers.
12. A list of free vehicle routing software can be found in [WaEtal16].
13. Datasets and best-known solutions for VRPs are available at
• http://neo.lcc.uma.es/vrp/
Examples:
1. The following table gives the data for a VRP involving six customers in which vehicle
capacity is 820. The route [1, 2, 4, 6, 1] is not feasible since the total demand of customers
on this route is w2+w4+w6 = 486+326+24 = 836 > 820. Since P7
i=2 wi = 1967, at least
⌈1967/820⌉= 3 routes will be needed to service all demands. The routes [1, 5, 2, 6, 1],
[1, 3, 1], and [1, 4, 7, 1] constitute a feasible set of routes with (respective) demands 800,
541, and 626. In this feasible solution, the total distance traveled by the ﬁrst vehicle
is c15 + c52 + c26 + c61 = 131, by the second is c13 + c31 = 114, and by the third is
c14 + c47 + c71 = 181, for a total distance of 426.
customer
2
3
4
5
6
7
demand
486
541
326
290
24
300
cij
2
3
4
5
6
7
1
19
57
51
49
4
92
2
51
10
53
25
53
3
49
18
30
47
4
50
11
38
5
68
9
6
94
2. The Clarke and Wright heuristic is applied to the problem speciﬁed in the table of
Example 1. For instance, s35 = c13 + c15 −c35 = 57 + 49 −18 = 88. The largest savings
occurs for s57 = 132 and w5 + w7 = 590 ≤820, so the initial routes [1, 5, 1] and [1, 7, 1]
are merged to produce the feasible route [1, 5, 7, 1] with distance 150. The next largest
savings occur for s47 = 105, s37 = 102, and s35 = 88; however, neither customer 3
nor customer 4 can be inserted into the route [1, 5, 7, 1] without exceeding the vehicle

Section 10.7
DIFFICULT ROUTING AND ASSIGNMENT PROBLEMS
803
capacity. The next largest savings is s24 = 60, giving the new feasible route [1, 2, 4, 1]
with demand 812 and distance 80. Continuing in this fashion eventually ﬁnds s36 = 31
and constructs the route [1, 3, 6, 1] with demand 565 and distance 91. This feasible set
of three routes has total distance 150 + 80 + 91 = 321, smaller than that for the feasible
solution given in Example 1.
10.7.3
QUADRATIC ASSIGNMENT PROBLEM
The quadratic assignment problem deals with the relative location of facilities that in-
teract with one another in some manner. The objective is to minimize the total cost of
interactions between facilities, with distance often used as a surrogate for measures such
as dollar cost, fatigue, or inconvenience.
Deﬁnitions:
There are n facilities to be assigned to n predeﬁned locations, where each location can
accommodate any one facility.
The ﬁxed cost cij is the cost of assigning facility i to location j.
The ﬂow fij is the level of interaction between facilities i and j.
The distance dij between locations i and j is the per unit cost of interaction between
the two locations. Typically, it is measured using the rectilinear or Euclidean distance
between the locations.
An assignment is a bijection ρ from the set of facilities onto the set of locations.
The linear assignment problem (LAP) is the problem of ﬁnding an assignment ρ
that minimizes P
i ci,ρ(i).
The quadratic assignment problem (QAP) is the problem of ﬁnding an assignment ρ
that gives the minimum value zQAP of P
i ci,ρ(i) + P
i,p fipdρ(i),ρ(p).
In some partial assignment for the QAP, let F be the set of facilities (possibly empty)
that have already been assigned and L be the set of locations having assigned facilities.
Facts:
1. The following table gives a variety of situations that can be formulated using the
QAP model.
facilities
interaction
departments in a manufacturing plant
ﬂow of materials
departments in an oﬃce building
ﬂow of information, movement of people
departments in a hospital
movement of patients and medical staﬀ
buildings on a campus
movement of students and staﬀ
electronic component boards
connections
computer keyboard keys
movement of ﬁngers
runners in a relay team
transfer time of a baton
airport terminals and gates
ﬂow of passengers with connecting ﬂights
turbine blades
slight variations in blade weights, that
need to be balanced

804
Chapter 10
NETWORKS AND FLOWS
2. The interdependence of facilities due to interactions between them leads to the
quadratic nature of the objective function in the QAP.
3. If the facilities are independent of each other (there are no interactions between
them), the QAP reduces to the LAP, which can be solved in polynomial time (§10.2.2).
4. The TSP is a special case of the QAP (see Example 4).
5. The QAP is an NP-hard optimization problem (§17.5.2).
6. Exact solution of the QAP is limited to fairly small problems, generally of size 30 or
smaller.
7. A lower bound on completions of a partial assignment for the QAP is given by
min P
i∈F
ci,ρ(i) + P
i∈F
P
p∈F
fipdρ(i),ρ(p)
+ P
i∈F
P
p/∈F
 fipdρ(i),ρ(p) + fpidρ(p),ρ(i)

+ P
i/∈F
ci,ρ(i) + P
i/∈F
P
p/∈F
fipdρ(i),ρ(p).
The ﬁrst two terms in this expression are the known ﬁxed and interaction costs of as-
signments already made; the third term captures the interaction costs between assigned
facilities and those yet to be assigned; and the last two terms represent the ﬁxed and
interaction costs of assignments not yet made.
8. A minimum value z∗can be calculated for the last three terms in the lower bound
expression of Fact 7 by solving a LAP such that each cost term is a lower bound on the
incremental costs that would be incurred if facility i /∈F is assigned to location j /∈L.
9. Gilmore-Lawler lower bound: This lower bound for zQAP is given by
P
i∈F
ci,ρ(i) + P
i∈F
P
p∈F
fipdρ(i),ρ(p) + z∗,
where z∗is found as in Fact 8.
10. The Gilmore-Lawler lower bound allows the QAP to be solved using a branch-and-
bound (implicit enumeration) technique (§16.1.8).
11. The following table provides alternative tighter lower bounds for exact solution
of the QAP. Here SDP refers to Semideﬁnite Programming and RLT refers to the
Reformulation-Linearization Technique.
lower bounds
references
linear programming based
[ReRaDr95]
quadratic programming based
[AnBr01]
SDP based
[Ro04]
lift-and-project SDP
[BuVa06]
level-1 RLT
[HaGr98], [ShAd99]
level-2 RLT
[AdEtal07]
level-3 RLT
[HaEtal12]
level-2 RLT interior point
[RaEtal02]
bundle method
[ReSo07]
12. There are several ways to linearize the QAP by deﬁning additional variables and
constraints. However, none of the linearizations proposed so far has proved to be com-
putationally eﬀective.

Section 10.7
DIFFICULT ROUTING AND ASSIGNMENT PROBLEMS
805
13. Heuristic methods for solving the QAP can be classiﬁed as limited enumeration,
construction methods, improvement methods, hybrid methods, and metaheuristics. A
survey of exact and heuristic solution methods for the QAP is found in [KuHe87]; exper-
imental comparisons of heuristic approaches appear in [BuSt78] and [Li81].
14. Limited enumeration: There are two distinct approaches for limiting the search for
an optimal QAP solution using a branch-and-bound approach:
• The search can be curtailed by placing a limit on the computation time or the
number of subproblems examined.
Since an optimal solution is often found
fairly early in a branch-and-bound procedure, especially if a good branching
rule is available, this approach may ﬁnd an optimal (or a near-optimal) solution
while saving on the signiﬁcant cost of proving optimality.
• The gap between the lower and upper bound is largest at higher levels of a branch-
and-bound tree. Thus a relatively large gap can be used to fathom subproblems
at higher levels, and this gap can be decreased gradually as the search reaches
lower levels of the tree.
15. Construction methods:
These heuristics start with an empty assignment and add
assignments one at a time until a complete solution is obtained. The rule used to choose
the next assignment can employ various viewpoints:
• a local view: select a facility having the maximum interaction with a facility
already assigned; locate it to minimize the cost of interaction between facilities;
• a global view: take into account assignments already made as well as future
assignments to be made.
16. Suppose that k assignments have already been made. Using statistical properties,
the expected value for the completion of the partial assignment is given by the following
expression, whose terms are analogous to those in Fact 7:
EV = P
i∈F
ci,ρ(i) + P
i∈F
P
p∈F
fipdρ(i),ρ(p)
+
P
i∈F
P
p/∈F
P
j /∈L
 fipdρ(i),j + fpidj,ρ(i)

n −k
+
P
i/∈F
P
j /∈Lcij
n −k
+
P
i,p/∈F
fip
  P
j,q/∈L
djq

(n −k) (n −k −1).
The low computational requirements of computing EV make this a good choice to guide
a construction heuristic [GrWh70].
17. Improvement methods:
These heuristics start with a suboptimal solution (often
randomly generated) and attempt to improve it through partial changes in the assign-
ments. Several important issues arise in designing an improvement heuristic:
• type of exchange:
The choices are pairwise, triple, or higher-order exchanges.
The use of pairwise exchanges has been found to be the most eﬀective in terms
of solution quality and computational burden. Higher-order exchanges can be
beneﬁcial but are generally used in a limited way because of the signiﬁcant
increase in computation time.
• scope of exchange: The procedure can use a local approach that considers only the
exchange of adjacent facilities, or a global approach that considers all possible
exchanges. Current computing capabilities allow the use of a global approach,
which has been found to be more eﬀective.

806
Chapter 10
NETWORKS AND FLOWS
• choice of exchange: The procedure can eﬀect an exchange as soon as an improving
move is found, or can evaluate all possible exchanges and choose the best. The
ﬁrst improvement option is more common.
• order of evaluation: The possible exchanges can be evaluated in a random or some
predetermined order. This is relevant only if the “ﬁrst improvement” approach
is used, as is often the case. One simple but eﬀective solution is to consider
facilities in the ﬁxed order of decreasing total interactions, so that exchanges
with potentially large savings are evaluated ﬁrst.
18. Hybrid methods:
Unlike improvement procedures, which tend to get trapped at
local minima, hybrid methods use multiple restarts from a set of diversiﬁed solutions.
Hybrid procedures combine the power of improvement routines with diversiﬁed solutions
obtained through construction methods.
19. Metaheuristics:
In recent years, metaheuristics such as simulated annealing, tabu
search, and genetic algorithms have been developed to help improvement procedures
avoid the trap of local minima and have been applied with success to the QAP. Meta-
heuristics have been able to ﬁnd the best known solutions for the commonly used bench-
mark problems in the literature and remain an active area of research on the QAP.
20. A recent survey of the QAP is provided in [Dr15]. The most successful metaheuristic
so far for solving the QAP is the hybrid genetic algorithm.
21. Research papers, software, datasets, test instances, and solutions can be found in
QAPLIB at
• http://anjos.mgi.polymtl.ca/qaplib/
Examples:
1. The following table gives the data cij, fij for a QAP with four facilities and four
locations.
cij
1
2
3
4
1
1
3
2
1
2
2
1
4
3
3
4
2
4
4
4
3
1
2
2
fij
1
2
3
4
1
0
1
3
4
2
1
0
2
1
3
3
2
0
3
4
4
1
3
0
The ﬁxed locations 1, 2, 3, 4 occur at equally-spaced points along a line, with unit dis-
tances between successive points, so that dij = |i −j|. For the assignment ρ speciﬁed by
ρ(1) = 1, ρ(2) = 4, ρ(3) = 2, ρ(4) = 3 the ﬁxed cost is c11 + c24 + c32 + c43 = 8. Because
the ﬂows and distances are symmetric, the interaction cost is 2(f12d14 +f13d12 +f14d13 +
f23d42 + f24d43 + f34d23) = 44. The total cost of assignment ρ is then 8 + 44 = 52.
2. The assignment in Example 1 can be improved by a pairwise exchange. Namely,
instead of assigning facilities 1 and 2 (respectively) to locations 1 and 4, they are assigned
to the interchanged locations 4 and 1. This gives σ(1) = 4, σ(2) = 1, σ(3) = 2, σ(4) = 3.
Then the ﬁxed cost incurred is c14 + c21 + c32 + c43 = 7 and the interaction cost is
2(f12d41 + f13d42 + f14d43 + f23d12 + f24d13 + f34d23) = 40. The total cost 47 is lower
than that for the assignment ρ in Example 1. In fact σ is an optimal QAP assignment.
3. The QAP arises in designing the layout of a manufacturing facility. A number of
products are to be made in this facility and diﬀerent products require diﬀerent operations
in given sequences for completion. These operations are performed by n departments:
e.g., turning, milling, drilling, heat treatment, and assembly. Knowing the sequence of
operations and the volume of each product to be produced, it is possible to calculate the

Section 10.8
SMALL-WORLD NETWORKS
807
ﬂow fij from any department i to another department j. There are n physical locations,
with distance dij between locations i and j. The ﬁxed cost of assigning department i
to location j is cij, representing the cost of building foundations and installing support
equipment (cables, pipes) for the machines. Then the objective is to assign departments
to locations in order to minimize the sum of ﬁxed and interaction costs.
4. The TSP (§10.7.1) can be formulated as a special case of the QAP, where the n cities
correspond to locations and a position number (facility) in the tour is to be associated
with each city. Let f12 = f23 = · · · = fn1 = 1 and fij = 0 otherwise. The distance dij
represents the cost of traveling between cities i and j, and let all ﬁxed costs cij be zero.
Then a solution to this QAP gives an optimal labeling of cities with their positions in
an optimal TSP tour.
10.8
SMALL-WORLD NETWORKS
The concept of a “small-world network” is well known in both the scientiﬁc literature
and in popular culture. It is often associated with the term “six degrees of separation”,
which indicates the relatively small number of steps in a chain linking random individuals.
Small-world networks are neither random, nor perfectly ordered. Rather they display the
important characteristics of small average distance between vertices, small diameter, and
a high degree of clustering. Many social networks exhibit the small-world property, as
well as a number of other naturally occurring networks (the structure of the Internet,
ecological food webs, transportation networks, and biological networks). This section
presents an introduction to the structure and dynamics of small-world networks.
10.8.1
BASIC CONCEPTS
This subsection provides important graph deﬁnitions and concepts useful in quantita-
tively characterizing network structures that arise in nature and society. It allows one
to compare and contrast random, structured, and small-world networks.
Deﬁnitions:
Let G = (V, E) be a simple undirected graph, with n = |V | and m = |E|. The vertex
subset S ⊆V forms an induced subgraph G[S] = (S, E[S]) if E[S] = {(i, j) | i, j ∈
S, (i, j) ∈E}. The largest connected component of G and the corresponding induced
subgraph are indicated by S ⊆V and G[S], respectively.
The open neighborhood of vertex i ∈V is denoted N(i) = {j | (i, j) ∈E}.
The degree of vertex i is di = |N(i)| and ¯d = 1
n
P
i∈V di is the average degree of G.
A d-regular graph is a graph in which the degree of each vertex is equal to d.
The triangle degree of vertex i is ti = |{(j, k) | j, k ∈N(i), (j, k) ∈E}|: namely, the
number of triangles containing vertex i.
The distance between vertices i, j ∈V is the length l(i, j) of a shortest path be-
tween them. The diameter of G is the largest distance between any two vertices in
G: diam(G) = maxi,j∈V l(i, j).

808
Chapter 10
NETWORKS AND FLOWS
Note: If G is not connected, we consider the diameter of the largest component diam(G[S])
instead.
The average distance in G is deﬁned as l = l(G) =
P
i,j∈V, i<j
l(i, j)/
 n
2

.
The clustering coeﬃcient C of a graph is
C =
3 × (number of triangles)
number of connected triples,
where a connected triple consists of three vertices connected by two or more edges.
The local clustering coeﬃcient Ci of vertex i is
Ci =
number of triangles connected to vertex i
number of connected triples centered on vertex i
and the Watts-Strogatz clustering coeﬃcient CWS is the average of the local clus-
tering coeﬃcients:
CWS = 1
n
X
i∈V
Ci.
Facts:
1. Many empirical studies of practical small-world networks imply that distances in the
network are considered to be “small” when they are orders of magnitude smaller than
the order n of the network [Ba16]. Hence, network distances can be considered “small”
when l(G) = O(ln n) and diam(G) = O(ln n).
2. The number of distinct triangles in G is equal to 1
3
P
i∈V ti.
3. The clustering coeﬃcient C is a measure of transitivity: namely, the probability that
two random neighbors of a vertex are neighbors of each other [Ne10].
4. The factor 3 in the numerator of C arises since each triangle produces three closed
triples.
5. Equivalently, C can be deﬁned in terms of ti and di:
C =
P
i∈V ti
P
i∈V
 di
2
.
6. The alternative clustering coeﬃcient CWS was proposed by Watts and Strogatz
[WaSt98].
7. Since the local clustering coeﬃcient can be alternatively represented as Ci = ti/
 di
2

,
CWS can also be equivalently expressed in terms of ti and di:
CWS = 1
n
X
i∈V
ti
 di
2
.
8. The clustering coeﬃcients satisfy 0 ≤C ≤1 and 0 ≤CWS ≤1.
9. The clustering coeﬃcients C and CWS are both good measures of network transitivity.
Random networks display low transitivity, and thereby both C and CWS for such networks
usually converge to zero as n →∞. In contrast, these clustering coeﬃcients for a network
with high transitivity usually fall within the interval [0.1, 1].
10. Whereas random networks tend to have a small average distance and a small cluster-
ing coeﬃcient, small-world networks have a small average distance and a large clustering
coeﬃcient.

Section 10.8
SMALL-WORLD NETWORKS
809
Examples:
1. The following graph on ﬁve vertices has the degree sequence d1 = 2, d2 = d3 = d4 =
3, d5 = 1 and the triangle degrees t1 = 1, t2 = t3 = 2, t4 = 1, t5 = 0.
By Fact 5, the clustering coeﬃcient C is computed as
C =
P5
i=1 ti
P5
i=1
 di
2
 = 1 + 2 + 2 + 1 + 0
1 + 3 + 3 + 3 + 0 = 0.6.
By Fact 7, the local clustering coeﬃcients are C1 = 1, C2 = C3 = 2
3, C4 = 1
3, C5 = 0
and so CWS is computed as
CWS = 1
5
P5
i=1 Ci = 1
5
 1 + 2
3 + 2
3 + 1
3 + 0

=
8
15 ≈0.53.
Notice that C > CWS holds in this example.
2. In general, there is no order relation between these two deﬁnitions of the clustering
coeﬃcient (C and CWS). For instance, suppose we delete vertex 5 from the graph in
Example 1 and consider the graph eG = G[V \{5}].
Then C( eG) = 3/4 = 0.75 and
CWS( eG) = 5/6 ≈0.83, yielding C < CWS for the graph eG.
3. Although C need not be equal to CWS, there are types of graphs where the equality
C = CWS holds. For example, every regular graph satisﬁes C = CWS. Namely, for any
d-regular graph G with tG triangles in total
C = CWS = 3tG
n
 d
2
.
There are graphs that are not regular, but still satisfy C = CWS, as occurs in the following
two graphs. In the ﬁrst graph C = CWS = 1/4, while in the second graph C = CWS =
1/12. Thus, regularity is a suﬃcient but not a necessary condition for C = CWS.
10.8.2
RANDOM GRAPH MODELS
The classical random graph model was mostly motivated by an observation that social
connections are formed with omnipresent randomness.
Although this random graph
model is not suﬃcient to explain the properties of social and other real-world networks,
it still plays a signiﬁcant role in studying the small-world phenomena.

810
Chapter 10
NETWORKS AND FLOWS
Deﬁnitions:
In the Gn,m random model, n vertices are connected by m randomly placed edges.
In the Gn,p random model, each pair of vertices has an edge connecting them with a
ﬁxed probability p.
Let nk be the number of vertices having degree k in a simple graph G = (V, E). The
degree distribution of G is given by pk = nk/n for k = 0, 1, . . ., n −1.
A discrete random variable ξ follows a binomial distribution B(N, p) with parameters
N and p if
Pr(ξ = k) =
N
k

pk(1 −p)N−k for k = 0, 1, . . ., N.
A discrete random variable η follows a Poisson distribution P(λ) with parameter λ if
Pr(η = k) = λk
k! e−λ for k = 0, 1, 2, . . ..
Facts:
1. Erd˝os and R´enyi [ErR´e60] proposed the Gn,m random model, while Gilbert [Gi59]
proposed the Gn,p random model. Both Gn,m and Gn,p are commonly referred as to
Erd˝os-R´enyi random graphs or uniform random graphs.
2. The Gn,m and Gn,p models behave identically for p = m/
 n
2

under several conditions.
While this subsection focuses on the more popular Gn,p random graph model, most of
the results obtained for Gn,p carry over to Gn,m [Ne03].
3. Since pk is the fraction of vertices with degree k in G, the degree distribution of G
reﬂects the probability that a randomly selected vertex of a graph has a speciﬁed degree.
4. Binomial degree distribution:
The degree distribution of Gn,p is binomial with the
parameters n −1 and p [Ne10]: namely,
pk =
n −1
k

pk(1 −p)n−k−1 for k = 0, 1, . . ., n −1.
Since E[B(N, p)] = Np (see §7.3.3), the average degree ¯d is well estimated by (n −1)p.
5. Poisson degree distribution: For large sparse random graphs, the degree distribution
of Gn,p is well approximated by a Poisson distribution with mean ¯d. Speciﬁcally, if n
tends to inﬁnity and ¯d ≪n, then it can be shown [Ba16] that for k ≪n
pk ≈
¯dk
k! e−¯d .
Hence Gn,p graphs are also referred to as Poisson random networks. Although ¯d ≪n, ¯d
still can be suﬃciently large and can tend to inﬁnity (but at a much slower rate than n).
6. Size of a random network:
While the number of edges of Gn,m is constant and
equal to m, the number of edges of Gn,p is random but densely concentrated around its
expectation E[m(Gn,p)] = p
 n
2

, due to the properties of the binomial distribution.
7. Approximate equivalence of the models: In the limit of large n, models Gn,m and Gn,p
exhibit very similar properties when m is close to p
 n
2

. Particularly, if p(1 −p)n2 →∞
and a certain property Q holds for Gn,m for all consecutive values of m in the range
p
 n
2

−
p
p(1 −p)n < m < p
 n
2

+
p
p(1 −p)n, then Q holds for Gn,p as well.
The
converse holds for a rich class of convex properties [Bo98].

Section 10.8
SMALL-WORLD NETWORKS
811
8. Order of the giant component: The giant connected component emerges in a uniform
random graph after an average degree passes the critical point ¯d = 1. Poisson networks
with 1 < ¯d < ln n/n are said to be in a supercritical regime. They have a single giant
component containing a fraction u = nS/n of all vertices, which can be approximated as
u ≈−W(−¯de−¯d)
¯d
,
where W is the principal branch of the Lambert W-function [Ne10]. For random graphs
with ¯d > ln n/n (connected regime) the giant connected component contains all vertices
making S = V and u = 1.
9. Diameter: The following results hold for connected Erd˝os-R´enyi (Poisson) networks.
• Simple calculations can yield a good approximation for the diameter of a uniform
random graph [Ba16], [Ne10]:
diam(Gn,p) = Θ
ln n
ln ¯d

.
• If np →∞for an Erd˝os-R´enyi graph Gn,p, then the following result holds asymp-
totically almost surely [ChLu04]:
diam(Gn,p) = (1 + o(1)) ln n
ln np.
• For a Poisson random graph Gn,p with ¯d = λ > 1 (thus, p ≈λ/n), the equality
diam(Gn,p) = ln n
ln λ + 2 ln n
ln λ⋆
+ Op(1)
is valid, where λ⋆is a “dual”-parameter of λ given by the formula λ⋆=
−W(−λe−λ) and Op(1) denotes a sequence of random variables Xn bounded
in probability [JaLuRu00]. This result is a stronger form of the result obtained
by Fernholz and Ramachandran [FeRa07] for Poisson random graphs.
10. Clustering coeﬃcient: The clustering coeﬃcient of a uniform random graph is C = p
or, equivalently, C = O(1/n). For large sparse networks, C converges to 0 as n →∞.
However, Albert and Barab´asi [AlBa00] argued that clustering coeﬃcients in real-world
networks tend not to follow the results obtained for uniform random graphs.
11. Although uniform random graphs exhibit small distances (logarithmic in the order
of the network), their clustering coeﬃcient tends to zero as n increases. The latter fact is
the main ﬂaw of this model in the context of small-world networks. This ﬂaw is addressed
by the models presented in the next section.
10.8.3
WATTS-STROGATZ MODEL
The Watts-Strogatz model [WaSt98] combines the small diameter and high clustering
properties. This subsection summarizes the main characteristics of this model and its
extensions.

812
Chapter 10
NETWORKS AND FLOWS
Deﬁnitions:
A lattice (or grid) is a graph whose vertices correspond to integer coordinate points in
an n-dimensional Euclidean space and whose edges connect every vertex to its c closest
vertices.
The Watts-Strogatz model (W-S model, GWS
c,p ) generates networks using the following
steps:
1. Deﬁne the number of vertices n, the lattice parameter c, and the rewiring pa-
rameter p ∈[0, 1], where n ≫c ≫ln n ≫1 and c must be even.
2. Place n vertices in a “ring” and connect each vertex to its c closest vertices. Note
that the resulting graph is a one-dimensional lattice with periodic boundary
conditions.
3. “Rewire” each edge (i, j) with probability p. This means that we shift one of its
ends j to a new position randomly selected from the rest of the vertices in the
ring (ensuring that no two vertices are joined by more than one edge and no
vertex has a self-loop), whereas the other end i remains at the same position.
The Newman-Watts model (N-W model, GNW
c,p ) was proposed by Newman and Watts
[NeWa99] in order to analyze the W-S model theoretically. The N-W model is a mod-
iﬁcation of the W-S model, obtained by slightly revising the W-S network generation
algorithm. Instead of rewiring, the generation algorithm for the N-W model adds extra
edges to the underlying lattice. Namely, the algorithm generates a new edge with prob-
ability p for each edge in the initial lattice. The endpoints of each newly generated edge
are chosen uniformly at random.
The Dorogovtsev-Mendes model (D-M model, GDM
n,p ) was proposed by Dorogovtsev
and Mendes [DoMe00] as an alternative small-world network model. The D-M model is
built by adding an extra vertex as a “central point” and connecting it with each vertex in
the underlying lattice with probability p. The edges emanating from the central vertex
have weights 1/2 as opposed to unit weights of the edges on the initial ring.
Facts:
1. The assumptions stated in the W-S model include sparsity (n ≫c), connectivity
(c ≫ln n), and a proper size (ln n ≫1) of the network.
2. Consider the W-S model. If p →0, then ¯l(GWS
c,p ) ∼n/(2c) ≫1 and C(GWS
c,p ) ≈
CWS(GWS
c,p ) ∼(3(c −2))/(4(c −1)), which is approximately 3/4 for large values of c. If
p →1, then ¯l(GWS
c,p ) ≈¯l(Gn,p) ∼ln n/ ln c and C(GWS
c,p ) ≈CWS(GWS
c,p ) ≈C(Gn,p) ∼c/n.
In particular, small average distance and high clustering are observed simultaneously
(i.e., the network becomes “small-world”) for small values of p, usually 0.001 < p < 0.01.
3. The N-W model is equivalent to the W-S model if p is small, which is the case when
the W-S model becomes a small-world network.
4. In the N-W model, as well as in the W-S model, the average distance increases
linearly with the network size n for p = 0, whereas it increases logarithmically with n for
p = 1. Newman et al. [NeMoWa00] employed a mean-ﬁeld approximation to show that
the transition between these growth rates occurs in the region where the average number
of added shortcuts s = ncp/2 is about one. More speciﬁcally, as n →∞, s ≫1 and c, p
are ﬁxed the average distance between vertices increases logarithmically with n, since
¯l(GNW
c,p ) ≈ln(ncp)
c2p
= Θ (ln n) .

Section 10.8
SMALL-WORLD NETWORKS
813
Thus, the N-W model shows that the addition of only a small fraction of random shortcuts
to a large underlying lattice can yield a small-world eﬀect.
5. Newman [Ne10] estimated the clustering coeﬃcient of the N-W model in the limit of
large n:
C(GNW
c,p ) ≈
3(c −2)
4(c −1) + 8cp + 4cp2 .
Therefore, C(GNW
c,p ) ≈3/(4(1 + p)2) for suﬃciently large c. In other words, the N-W
model with reasonably small values of p exhibits high clustering.
6. Asymptotic bounds on the diameter: Bollob´as and Chung [BoCh88] derived a strong
asymptotic result for the diameter of a cycle plus a random matching, which can be
considered as a rough approximation of the N-W model with parameters c = 2 and
p = 1/2 (for such parameters the size of a random matching and an estimated number
of added shortcuts are both equal to n/2). If G is a cycle on n vertices plus a random
matching, then
log2 n −θ < diam(G) ≤log2 n + log2 ln n + θ
asymptotically almost surely (that is, with probability tending to 1 as n →∞), where θ
is a small constant (at most 10).
7. Dorogovtsev and Mendes [DoMe00] obtained an asymptotic expression for ¯l as p →0
and n →∞in the D-M model:
¯l(GDM
n,p )
n
∼2ρ −3 + (ρ + 3)e−ρ
ρ2
,
where ρ = np is ﬁxed. Furthermore, consider any function ρ(n) →∞, which implies
¯l(GDM
n,p ) ∼2n/ρ. Thus, if one chooses any p(n) = Θ(1/ lnn), then
¯l(GDM
n,p ) = Θ(ln n).
Hence, with a proper choice of p(n), the D-M model exhibits small-world distances as
n →∞.
Examples:
1. The following ﬁgure illustrates a W-S network generated with n = 16 vertices, placed
in a ring with c = 2. The four shaded vertices are selected for rewiring; one end of
an edge incident to each such vertex is randomly linked to another vertex, giving the
“middle” edges shown in the picture.
2. The following ﬁgure illustrates an N-W network generated with n = 16 vertices,
placed in a ring with c = 2. Four new edges (shown highlighted) are randomly generated
and added to this ring.

814
Chapter 10
NETWORKS AND FLOWS
3. The following ﬁgure illustrates a D-M network generated with n = 16 vertices, placed
in a ring with c = 2. An extra vertex is added to the network and connected to some
other randomly selected vertices in the ring. The added edges are shown highlighted.
10.8.4
KLEINBERG’S GEOGRAPHICAL SMALL-WORLD MODEL
This subsection considers a natural generalization of the W-S small-world network model,
referred to as the geographical small-world model, for which there exists a decentralized
algorithm capable of ﬁnding short paths in logarithmic time with high probability.
Deﬁnitions:
In an r-dimensional lattice, l1-distances are measured by d(u, v) = Pr
k=1 |uk −vk|, where
u = (u1, . . . , ur) and v = (v1, . . . , vr).
Kleinberg’s small-world model, which is a modiﬁed version of the N-W model, is
obtained by adding random directed edges to an r-dimensional lattice, based on l1-
distance. In the underlying lattice, each vertex has a directed link to any other vertex
within lattice l1-distance ξ, for a user-deﬁned constant ξ ≥1; such neighbors are called
local contacts. Also q long-range connections (u, v) for each vertex u are generated with
probability proportional to d(u, v)−α for each vertex v; here α is a user-deﬁned parameter.
Local search (or, equivalently, decentralized search) L in Kleinberg’s model is de-
signed to imitate Milgram’s small-world experiment (see §10.8.7). Suppose that a starting
vertex i is given a message that it must forward to a target vertex j. At each iteration, a
current vertex should choose which of its neighbors to send the message to. Meanwhile, it
uses only local information, i.e., it knows only geographical distances between its neigh-
bors and the target vertex j. In Kleinberg’s decentralized algorithm every current vertex
connects to a neighbor that is geographically closest to vertex j.
These consecutive
choices determine a path from i to j via local search.

Section 10.8
SMALL-WORLD NETWORKS
815
The expected delivery time TA of a search algorithm A is the expected number of
steps taken by the algorithm to deliver the message over a generated network.
A particular version of Kleinberg’s model is based upon probabilities determined by rank
rather than by physical distances. The rank of a vertex v with respect to a vertex u is
deﬁned as the number of other vertices that are geographically closer to u than to v: i.e.,
ranku(v) = |{w | d(w, u) < d(v, u)}|.
Facts:
1. Kleinberg’s geographical small-world model [Kl00] was originally constructed on a
two-dimensional lattice (r = 2). If no speciﬁc information about r is given, this original
model with r = 2 will be assumed and denoted GK
α,q,ξ.
2. If α is large, then long-range edges have a small chance of being created, whereas
if α is close to 0, then long-range edges may appear in the network with a reasonably
high probability. When α = 0, Kleinberg’s model generates additional edges according
to a uniform distribution, which is equivalent to the directed N-W model on the two-
dimensional lattice (we treat every undirected edge (u, v) in the underlying lattice of the
N-W model as two directed edges (u, v) and (v, u)).
3. For every graph G and algorithm A, the inequality TA(G) ≥¯l(G) holds.
4. In large Kleinberg networks, local search can be carried out most eﬃciently when
α = 2, as supported by the following theoretical results [Kl00]:
• For Kleinberg’s model with 0 ≤α < 2, arbitrary q ≥1, ξ ≥1, and any decentral-
ized algorithm A
TA(GK
α,q,ξ) = Ω(n(2−α)/3).
As a consequence, despite the fact that there exist paths between every pair
of vertices whose lengths are O(ln n) for the two-dimensional N-W model (the
case α = 0), there is no decentralized algorithm capable of ﬁnding those chains
in logarithmic time.
• For Kleinberg’s model with α = 2, q = 1, ξ = 1, and the decentralized algorithm
L (Kleinberg’s local search)
TL(GK
α,q,ξ) = O(ln2 n).
• For Kleinberg’s model with α > 2, arbitrary q ≥1, ξ ≥1, and any decentralized
algorithm A
TA(GK
α,q,ξ) = Ω(n(α−2)/(α−1)).
In computational experiments on Kleinberg networks with several hundred million ver-
tices, local search performed most eﬃciently for values of α in the range [1.5, 2]. Moreover,
as the network size increases, the most “eﬃcient” value for α approaches 2 [Kl00].
5. Rank-based friendship is a concept based on the assumption that a directed edge
(u, v) is randomly generated with probability puv proportional to ranku(v)−β, where β
is a predetermined parameter. If vertices are uniformly distributed in a Euclidean two-
dimensional space according to the l1-norm, then ranku(v) is approximately 2d(u, v)2.
Thus, a rank-based friendship network with β = 1 is approximately the same as Klein-
berg’s network with α = 2, since puv ∝d(u, v)−2 for both models.
6. Blog network:
Liben-Nowell et al. [LiEtal05] employed the rank-based friendship
principle to analyze roughly 500,000 users of the blogging site LiveJournal.
In their
case, the rank of a target user v with respect to a primary user u indicates how far v

816
Chapter 10
NETWORKS AND FLOWS
lives from u in comparison with other users connected with u via LiveJournal. Liben-
Nowell et al. investigated the following question: if we consider all vertex pairs (u, v) with
ranku(v) = x for some ﬁxed integer x ≥1, what fraction f(x) of these pairs are actual
friends? They computationally found that f(x) follows a power law (see §10.8.5) with
exponent in the range (−1.15, −1.2). This experimental ﬁnding supports the statement
(see Facts 4–5) that β = 1 is an adequate choice for the rank-based friendship model.
7. Facebook network: Backstrom et al. [BaSuMa10] also considered rank-based friend-
ship. They analyzed the Facebook network at that time and estimated the exponent β
to be very close to 1. In their case, the bulk of the distribution was approximated by
x−0.95. These results are consistent with those of Liben-Nowell et al. and further support
the formula puv ∝ranku(v)−1 as the best choice for the rank-based friendship model.
8. Regarding Kleinberg’s model based upon an r-dimensional lattice with r > 2, Easley
and Kleinberg [EaKl10] pointed out that local search is eﬃcient for networks built by
adding long-range contacts to lattices in r > 2 dimensions, when the exponent α is equal
to r.
Example:
1. The following ﬁgure illustrates an undirected version of Kleinberg’s small-world model
generated with n = 16 vertices, placed in a ring with ξ = 1. (Note that any instance of
the directed Kleinberg model can be easily transformed into an instance of the undirected
Kleinberg model by drawing an edge (u, v) if either the edge (u, v) or (v, u) exists.) New
edges were randomly generated with probability puv ∝d(u, v)−2 for each vertex pair
(u, v). As a result, four new edges (shown highlighted) were added to this ring. The
corresponding distances for the four pairs are 2, 2, 2, 3, which are shorter than the ones
of the N-W model instance described in §10.8.3, Example 2 (which are 3, 4, 5, 8).
10.8.5
POWER-LAW RANDOM NETWORKS AND THEIR SMALL-WORLD PROPERTIES
In contrast to the Erd˝os-R´enyi model, most real-world networks have degree distributions
substantially diﬀerent from the Poisson distribution that uniform graphs exhibit for large
n. Several empirical studies suggest that many real-world networks follow a power-law
degree distribution [AiChLu02], [BaAl99], [Ne03]. These observations motivated work on
developing more realistic random network models that mitigate the shortcomings of the
Erd˝os-R´enyi model. This subsection reviews the facts associated with random network
models having general degree distributions, as well as the important special cases of
power-law random networks.

Section 10.8
SMALL-WORLD NETWORKS
817
Deﬁnitions:
Given the number of vertices n and degree sequence {d1, d2, . . . , dn}, the conﬁguration
model generates networks using the following steps:
1. For each vertex i ∈V , attach di “half-edges” (also called “stubs”).
2. Randomly connect the ends of the half-edges together, one pair at a time.
The generalized random graph model generates graphs in which vertices i, j ∈V
are connected with edge probability
pij = wiwj
2m ,
where {w1, w2, . . . , wn} is a given sequence of expected degrees chosen from a distribution
of interest.
The power-law degree distribution is deﬁned by
pk = Ck−β for k = kmin, . . . , kmax,
where pk is the fraction of vertices with degree k, C is a normalization factor, β is the
decay exponent, and kmin, kmax are (respective) lower and upper limits on the possible
values of k.
The pure power-law degree distribution is given by
pk = Ck−β for k ≥1,
where C = 1/ζ(β) (the Riemann zeta function).
The power-law degree distribution with exponential cutoﬀis given by
pk = Ck−βe−k/κ for k ≥1,
where C is a normalization constant and κ is an exponential cutoﬀparameter.
Remark: The three deﬁned versions of power laws are discrete distributions and should
not be confused with continuous power-law distributions.
Notation: Ggen
β
denotes a graph produced by the generalized random graph model with
an expected degree sequence {wi} sampled from the pure power-law distribution having
parameter β.
Gpl
β denotes a theoretical power-law graph with parameter β, that is,
a graph with a power-law degree distribution with an exponent β that is assumed to
exist; it is used only in theoretical derivations. Gconf
β
denotes a graph produced by the
conﬁguration model with a degree sequence that follows the pure power-law distribution
with parameter β. Gconf
β,κ denotes a graph generated by the conﬁguration model with an
expected degree sequence drawn from the power-law distribution with exponential cutoﬀ
and parameters β, κ.
Facts:
1. The conﬁguration model is one of the most popular generalized random network
models.
This process generates every possible topology having the given parameters
with equal probability [Ne10].
2. Graphs generated by the conﬁguration model can contain loops and multi-edges.
The common way to transform such graphs into simple ones is to delete any loops and
multi-edges. As n becomes large, the density of loops and multi-edges tends to 0 for the
conﬁguration model, and the degree distribution of the resulting simple graphs is close
to the given one. Since we focus on the behavior of graphs for large n, such self-loops
and multi-edges will be disregarded.

818
Chapter 10
NETWORKS AND FLOWS
3. The paramount property of the conﬁguration model is that the probability pij of
edge (i, j) appearing is given by
pij = didj
2m as n →∞.
4. The property given in Fact 3 makes the conﬁguration model similar to the generalized
random graph model, which was proposed by Chung and Lu [ChLu04].
5. Despite the fact that the ﬁnal degree sequence of the generalized random graph model
is not exactly equal to the desired degree sequence in general, this model has notable
calculation advantages. As in the case of the conﬁguration model, we ignore self-loops
and multi-edges produced by the generalized random graph model.
6. For the power-law degree distribution with exponential cutoﬀ, κ is typically a ﬁxed
large positive constant, but it can also be represented as a function of n.
7. The normalization constant C for the power-law degree distribution with exponential
cutoﬀturns out to be [Liβ(e−1/κ)]−1, where Lin(x) is the nth polylogarithm of x.
8. Small distances in Ggen
β :
Chung and Lu [ChLu04] showed that distances in power-
law graphs generated via the generalized random graph model are small (or even “ultra-
small”).
• For a power-law random graph Ggen
β
with exponent β > 3 and average degree
¯d > 1, both the average distance and diameter are Θ(ln n) almost surely. More
precisely, when n becomes very large
¯l(Ggen
β ) = (1 + o(1))(ln n/ ln ˜d),
diam(Ggen
β ) = Θ(ln n),
hold almost surely, where ˜d = P
i∈V w2
i / P
i∈V wi denotes the second-order
average degree. These equalities are corroborated by the approximation results
obtained by Cohen and Havlin [CoHa03]: ¯l(Gpl
β ) ∝ln n for β > 3 and n ≫1.
• For β = 3 a power-law random graph Ggen
β
has
diam(Ggen
β ) = Θ(ln n)
almost surely and
¯l(Ggen
β ) = Θ(ln n/ ln ln n)
as n →∞. The respective approximation results by Cohen and Havlin support
the latter formula: ¯l(Gpl
β ) ≈ln n/ ln ln n for β = 3 assuming ln ln n ≫1.
• Consider a power-law random graph Ggen
β
with exponent 2 < β < 3, average
degree ¯d > 1, and maximum degree ∆(Ggen
β ) satisfying ln ∆≫ln n/ ln ln n.
Then almost surely
diam(Ggen
β ) = Θ(ln n)
and
¯l(Ggen
β ) ≤(2 + o(1))[ln ln n/ ln(1/(β −2))]
as n →∞.
9. Small distances in Gconf
β
: Newman et al. [NeStWa01] derived approximation results
for the pure power-law distribution realized via the conﬁguration model as n →∞.
Assuming that Gconf
β
is connected and β > 3
¯l(Gconf
β
) ≈
ln n + ln[ζ(β)/ζ(β −1)]
ln[ζ(β −2)/ζ(β −1) −1] + 1.

Section 10.8
SMALL-WORLD NETWORKS
819
More precisely, for suﬃciently large n, the average distance in pure power-law graphs
grows logarithmically: i.e., ¯l(Gconf
β
) ≈Θ(ln n).
10. Small distances in Gconf
β,κ :
Dorogovtsev et al. [DoMeSa03] obtained the following
approximation results for power-law graphs with exponential cutoﬀgenerated by the
conﬁguration model.
• For a graph Gconf
β,κ with exponent 2 < β < 3 and large but ﬁnite cutoﬀparameter
κ, the average distance is Θ(ln n). More speciﬁcally, for large n
¯l(Gconf
β,κ ) ∼e1/κ
3 −β ln n.
• For a graph Gconf
β,κ
with exponent 2 < β < 3 and cutoﬀκ ∝nǫ, the average
distance is ultrasmall: i.e., ¯l(Gconf
β,κ ) ∼Θ(ln ln n). Speciﬁcally, as n →∞
¯l(Gconf
β,κ ) ∼
ln ln n
ln(1/(β −2)),
which matches the results of Chung and Lu for Ggen
β
with 2 < β < 3.
• Graphs Gconf
β,κ with exponent β = 3 and a cutoﬀparameter that is polynomial in
n (i.e., κ ∝nǫ) have average distance
¯l(Gconf
β,κ ) ∼(1 + ǫ) ln n
ln ln n
for large n. This coincides with the results of Chung and Lu for Ggen
β
with
β = 3.
11. Clustering in power-law graphs:
For any graph generated by the conﬁguration
model using a given degree distribution, the clustering coeﬃcient can be directly esti-
mated as follows [Ne10]:
C = 1
n
[ ¯d2 −¯d]2
( ¯d)3
.
Here ¯d and ¯d2 are the ﬁrst (average degree) and second moments of the degree distribu-
tion. If both of these moments are bounded, then the clustering coeﬃcient converges to
zero as n →∞. This is not the case for power-law graphs and even for graphs with a
degree distribution having a power-law tail. Usually, their second moment diverges while
the average degree is ﬁxed or bounded.
Example:
1. To illustrate Fact 11, let Gpl
β be a graph having a power-law distribution with pa-
rameter β for k ≥kmin and an arbitrary distribution for k < kmin. If one cuts it oﬀ
(in a simple graph) at k = n, this results in the following approximation of the second
moment for 2 < β < 3:
¯d2(Gpl
β ) ∼n3−β.
Therefore, as n becomes very large, the clustering coeﬃcient grows as
C(Gpl
β ) ∼1/nβ−2,
which still converges to zero, but at a much slower rate (especially for values of β close
to 2) than the clustering coeﬃcient of a uniform random network.

820
Chapter 10
NETWORKS AND FLOWS
10.8.6
EVOLVING RANDOM NETWORKS WITH SMALL-WORLD PROPERTIES
Models of network growth (formation) are an important class of random network models
whose primary goal is to mimic growth processes that may take place in modern dy-
namically changing real-world networks, such as the World Wide Web, the Internet, and
social networks. Such growth models include Price’s model [Pr76], uniform attachment
[BaAlJe99], preferential attachment [BaAl99] (the well-known Barab´asi-Albert model)
and its generalizations [Ne03], a variety of hybrid models of uniform and preferential
attachment [DoMe01], [JaRo07], [V´a03], and several nonlinear preferential attachment
models [JeN´eBa03], [KrReLe00]. In general, it is diﬃcult to theoretically calculate di-
ameters and shortest path lengths for growing network models; however, some of these
models have been shown to generate networks that exhibit small-world properties. These
models are reviewed in this subsection.
Deﬁnitions:
Price’s model generates directed graphs. A new vertex is added at each time period
and its out-degree is deﬁned according to a ﬁxed distribution with mean µ. That newly
appearing vertex is connected to already existing vertices chosen at random with proba-
bility proportional to their current degree plus a positive parameter µ0.
In the Barab´asi-Albert model, a new vertex is added to the growing network at
each time period; it is connected to µ already existing vertices, selected with probability
proportional to their current degree. The initial state of the Barab´asi-Albert model is
a clique of cardinality µ at time 0. Note that the Barab´asi-Albert model is similar to
Price’s model, but it generates undirected graphs.
The Klemm-Egu´ıluz model has as its initial state a clique of cardinality µ at time 0.
Each vertex of the network can be either in the active state or in the inactive state. There
are always µ active vertices in the network at the end of every time period. At each step, a
new vertex joins the growing network and becomes active. This new vertex is connected to
the existing network with µ edges. Each of these edges is connected either to a randomly
chosen vertex (with probability p) or to one of the µ active vertices (with probability
1 −p). In the former case, random vertices are chosen with probability proportional to
their vertex degree at time t. At the end of each iteration, one of the active vertices is
deactivated. Vertex i is selected for deactivation with probability inversely proportional
to its degree at time t: i.e., pi ∝di(t)−1.
The Jackson-Rogers model adds, at each step, a new vertex to the existing net-
work. This new vertex identiﬁes µ1 “parent vertices” uniformly and forms a directed
(undirected) link with each of them with probability p1. Furthermore, µ2 vertices are
uniformly selected from the out-neighborhoods (neighborhoods) of the “parent vertices”
and directed (undirected) links from the new vertex to the chosen µ2 vertices are formed
with probability p2 each. It is worth noting that µ2 vertices are selected independently
of whether a certain “parent vertex” has been linked to the new vertex. The initial state
of the Jackson-Rogers model is a clique on (µ1 + µ2 + 1) vertices. Henceforth, we will
concentrate on the undirected version of the Jackson-Rogers model.
Notation:
GBA
µ (t) denotes a graph generated at time t by the Barab´asi-Albert model
with parameter µ. GKE
µ,p(t) denotes a graph generated at time t by the Klemm-Egu´ıluz
model with parameters µ and p. GJR
µ,ρ(t) denotes an undirected graph generated at time
t by the undirected version of the Jackson-Rogers model, where µ = p1µ1 + p2µ2 and
ρ = p1µ1/p2µ2.

Section 10.8
SMALL-WORLD NETWORKS
821
Facts:
1. Price’s model [Pr76] is a network formation model aiming at simulating citation
networks, which are naturally directed. Price called the respective formation mechanism
cumulative advantage. He was one of the ﬁrst to discover a power law governing the out-
degree distribution. Indeed, Price’s model with parameters µ0 and µ exhibits a power-law
out-degree distribution with the exponent β = 2 + µ0/µ in the limit of large n.
2. The Barab´asi-Albert model [BaAl99], also known as the preferential attachment
model, is related to Price’s model.
The main diﬀerence is that the Barab´asi-Albert
model considers undirected edges, while the Price model considers directed edges.
3. The Klemm-Egu´ıluz model [KlEg02b] represents a hybrid between the highly clus-
tered dynamic model [KlEg02a] and the Barab´asi-Albert model. The cases p = 0 and
p = 1 correspond to the highly clustered model and the Barab´asi-Albert model, respec-
tively. The Klemm-Egu´ıluz model is often considered for the values of 0 < p ≪1.
4. The Jackson-Rogers model [JaRo07] is based on the idea of “meeting-based forma-
tion”, and it represents a crossover between uniform attachment (p1 = 1, p2 = 0) and
preferential attachment (p1 = 0, p2 = 1, µ1 = µ2 ≥1) mechanisms. In this model µ is
the expected number of edges formed at each iteration and ρ is the ratio of the numbers
of edges formed using these two attachment mechanisms.
5. Small distances in GBA
µ : Bollob´as and Riordan [BoRi04] discovered the most signif-
icant result about the diameter of the preferential attachment model for a slight mod-
iﬁcation of the Barab´asi-Albert model. Here the network formation process is allowed
to add multiple edges between pairs of vertices and self-loops for single vertices. This
model is a very close approximation to the preferential attachment model.
• If a graph GBA
µ
is formed using preferential attachment with µ = 1, then the
diameter of its largest component S is
diam(GBA
µ [S]) = Θ(ln n)
asymptotically almost surely, as n →∞. More precisely,
Pr[(1/γ −ǫ) ln n ≤diam(GBA
µ [S]) ≤(1/γ + ǫ) ln n] →1
as n →∞, where ǫ is some positive real number and γ is the solution of
γe1+γ = 1.
• If a graph GBA
µ
is formed via preferential attachment with µ ≥2, then asymptot-
ically almost surely GBA
µ
is connected and has diameter smaller than ln n:
diam(GBA
µ ) = Θ
 ln n
ln ln n

as n →∞. Speciﬁcally,
Pr[(1 −ǫ) ln n/ ln ln n ≤diam(GBA
µ ) ≤(1 + ǫ) ln n/ ln ln n] →1
as n →∞, where ǫ is a small positive real number.
These results show that the Barab´asi-Albert model generates graphs with small diameter.
6. Clustering in GBA
µ :
Klemm and Egu´ıluz [KlEg02b] derived an approximate expres-
sion for the average local clustering of the preferential attachment model. Namely, based
on estimations for the Barab´asi-Albert model, they showed
CWS(GBA
µ ) = Θ((ln2 n)/n)

822
Chapter 10
NETWORKS AND FLOWS
as n →∞. More rigorously, CWS(GBA
µ ) ∼µ(ln2 n)/(8n), which still converges to zero for
large n but is larger than the average local clustering of a random network CWS(Gn,p) ∼
¯d/n by a factor of O(ln2 n). Thus, the Barab´asi-Albert model exhibits some transitivity,
which is higher than that of random networks, but might still not be suﬃcient to model
real-life small-world networks.
7. Small distances in GKE
µ,p: Klemm and Egu´ıluz only managed to empirically estimate
average distances in their model for diﬀerent values of p ∈[0, 1]. They simulated 100
independent realizations of graphs having order n = 104 and average degree ¯d = 20 (with
µ = 10) for each value of p.
• Computational experiments suggested that the average path length in the simu-
lated graphs GKE
µ,p with p = 0 grows linearly, that is
¯l(GKE
10,0) ∝n,
which is similar to the behavior of regular one-dimensional lattices.
• Klemm-Egu´ıluz graphs generated for intermediate values of p are expected to
produce average distances that grow logarithmically, since ¯l(GKE
µ,1) ∝ln n/ ln ln n
for µ ≥2. Indeed, computational experiments conﬁrmed that
¯l(GKE
10,0.1) ∝ln n
for suﬃciently large n. Thus, empirical evidence suggests that crossover Klemm-
Egu´ıluz graphs exhibit small average distances.
8. Clustering in GKE
µ,p:
The average local clustering coeﬃcient of the Klemm-Egu´ıluz
model with p = 1 is proportional to (ln2 n)/n for large n because GKE
µ,1 = GBA
µ . By
contrast, the average local clustering coeﬃcient for the highly clustered model (p = 0)
reaches the asymptotic value 5/6. More precisely,
CWS(GKE
µ,0) ≈5
6 −
7
30µ + O
 1
µ2

,
i.e., the Klemm-Egu´ıluz model has very high transitivity for relatively large values of the
parameter µ [KlEg02b]. Computational experiments also showed that for the crossover
model with p = 0.1 the average local clustering coeﬃcient is lower than 5/6 but still
reaches an asymptotic value greater than zero.
Therefore, the crossover model with
values 0 < p ≪1 is believed to exhibit not only short average distances but also high
transitivity. This conclusion is supported by the empirical evidence obtained by Klemm
and Egu´ıluz.
9. Small distances in GJR
µ,ρ:
Jackson and Rogers [JaRo07] conducted computational
experiments on several real-world networks (ﬁtted to undirected Jackson-Rogers graphs)
supporting their conjecture that
diam(GJR
µ,ρ) = O(ln n)
for various combinations of the model parameters.
10. Clustering in GJR
µ,ρ:
The Jackson-Rogers model exhibits high clustering under the
following conditions: µ must be an integer, ρ > 1, and if p1 ≥ρ, then µ1 and p2µ2/µ1
both must be positive integers. Under such assumptions the average local clustering
coeﬃcient of the model converges to the ﬁxed number
CWS(GJR
µ,ρ) =
6p1
(1 + ρ)[(3µ −2)(ρ −1) + 2µρ]
as n →∞, thus conﬁrming that the Jackson-Rogers model has high transitivity, which
is consistent with small-world characteristics.

Section 10.8
SMALL-WORLD NETWORKS
823
Example:
1. The following six graphs illustrate the growing procedure of the Barab´asi-Albert
model with µ = 2. The process starts with a clique of cardinality µ = 2. Then the model
sequentially adds a new vertex (shown highlighted) and connects it to two other vertices
which are selected with probability proportional to their current degree. The diagrams
show the succession of graphs for n = 2, 3, 4, 5, 10, 20 vertices.
10.8.7
APPLICATIONS
Small-world networks arise in a variety of applications, ranging from manmade techno-
logical networks to naturally occurring networks in biology, economics, and society. This
subsection presents several examples of small-world networks.
1. Milgram’s small-world experiment:
The social psychologist Stanley Milgram pio-
neered the study of path lengths in real-world social networks through several sets of
small-world experiments. Probably the most well-known of these experiments are the
1967 Kansas study [Mi67] and the 1969 Nebraska study [TrMi69]. In both experiments,
a target person in the Boston area was chosen, and a set of randomly selected individuals
in Nebraska (or Kansas) were asked to mail a letter to someone they know who would
be most likely to have some relation to the target person. The mean number of inter-
mediaries in the network paths between “starters” and the target turned out to be 5.2,
which transforms into the average distance 6 if rounded up. Over 20 years later, this
result was popularized by John Guare’s 1990 play “Six Degrees of Separation” [Gu90]
and the movie with the same title. Since then, the concepts of small-world networks and
six degrees of separation have been well appreciated both in the scientiﬁc literature and
in popular culture.
2. Dodds’ small-world experiment:
After Milgram’s work, small-world experiments
were repeated and extended in several studies. One such study was conducted by Dodds
et al. [DoMuWa03] using email messages. The researchers selected more than 60,000
participants who were given instructions in order to reach one of 18 target persons from
13 diﬀerent countries. Every “starter” was provided some personal information about
the assigned target. This experiment was conducted on a signiﬁcantly larger scale than

824
Chapter 10
NETWORKS AND FLOWS
Milgram’s study as a result of the power of modern internet technology (24,000 email
chains were initiated). The results of Dodds’ experiment corroborated the fundamental
conclusions of Milgram’s studies.
The average path length of successfully completed
chains was 4.05, smaller than Milgram’s value 5.2. Besides, Dodds’ experiment accounted
for uncompleted chains and it estimated that starters could reach their targets in a
median of ﬁve to seven steps.
3. Collaboration/coauthorship networks:
Collaboration/coauthorship networks repre-
sent prominent examples of real-life small-world networks. Perhaps the most well-known
such networks are the collaboration network of mathematics researchers (where two re-
searchers are connected by a link if they have ever written a joint paper) and the network
of Hollywood actors (where two actors are connected if they have appeared in the same
movie). In these networks, the role of a central vertex is traditionally given to Paul Erd˝os
and to Kevin Bacon, respectively. The shortest distance from each vertex to the central
vertex is traditionally referred to as that individual’s Erd˝os number or Bacon number.
It turns out that the values of the Erd˝os number and Bacon number for all vertices in
the respective collaboration networks are small (many mathematicians have an Erd˝os
number of at most 5, and the average Bacon number for all actors in the database is
about 3), which conﬁrms the small-world structure of these networks.
Coauthorship networks for other scientiﬁc disciplines can be similarly analyzed. The
following table summarizes characteristics of such networks, demonstrating that these
networks exhibit small average distances and diameters, as well as high clustering co-
eﬃcients (compared to what would be observed if edges in these networks were added
uniformly and randomly). This conﬁrms that coauthorship networks do exhibit small-
world properties.
Biology
Physics
Computer
Mathematics
Economics
Science
number of
1,520,251
52,909
11,994
253,339
81,217
authors (n)
log number of
14.2
10.9
9.4
12.4
11.3
authors (ln n)
mean # authors
3.75
2.53
2.22
1.45
1.56
per paper
average #
18.10
9.70
3.59
3.90
1.67
collaborators ( ¯d)
size of largest
1,395,693
44,337
6,396
208,200
33,027
component (nS)
average
4.6
5.9
9.7
7.6
9.5
distance (¯lS)
diameter
24
20
31
27
29
(diamS)
average edge
1 × 10−5
2 × 10−4
3 × 10−4
2 × 10−5
2 × 10−5
probability (p)
clustering
0.066
0.430
0.496
0.150
0.136
coeﬃcient (C)

Section 10.8
NETWORK REPRESENTATIONS AND DATA STRUCTURES
825
4. Large-scale real-world networks: Adamic [Ad99] showed that the World Wide Web
(WWW) network has a small-world topology. She considered a graph that was a repre-
sentation of the WWW at the site level, based on data collected by Jim Pitkow at Xerox
PARC. The graph contained around 50 million documents and 259,794 sites. After the
deletion of leaf vertices, it was reduced to an undirected graph of order n = 153,127.
Adamic estimated the average path length ¯l to be 3.1 and the average local clustering
coeﬃcient CWS to be 0.1078, much larger than the average local clustering coeﬃcient
of the respective random graph (that would be 2.3 × 10−4). In a second experiment,
Adamic considered the respective directed graph or, more precisely, its largest strongly
connected component on nS = 64,826 vertices. Here ¯l(G[S]) ≈4.3 (average directed
distance) and CWS ≈0.081, in contrast to 1.05 × 10−3 for the corresponding random
graph. Hence, this study provided strong evidence supporting the conjecture about the
small-world nature of the WWW.
In a related study, Albert et al. [AlJeBa99] also investigated the average shortest path
length in the World Wide Web, in which web pages (URLs) are vertices and hyperlinks
are directed edges. Using local connectivity measurements, they constructed a topolog-
ical model of the WWW. They employed the ﬁnite size scaling procedure to assess the
average distance in the WWW by measuring distances in samples of increasing size. They
calculated ¯l ≈0.35 + 0.89 lnn, which implies that the average path length in the WWW
is around 18.59, since the size of the WWW was estimated to be 8 × 108 at that time.
Currently its size is believed to be ≈1012, and so the average distance can be predicted
to be close to 25. These results do not contradict Adamic’s conclusions because they
dealt with diﬀerent objects in the WWW. Since every site includes many web pages, the
order of Albert’s graph was substantially greater than the order of Adamic’s graph, thus
making the estimated ¯l much smaller for the latter graph. Unlike Adamic’s study, Albert
et al. did not study transitivity properties of the WWW.
Leskovec and Horwitz [LeHo08] constructed a network based on one month (June 2006)
of communication activities from the Microsoft Instant Messenger network. The initial
dataset contained 240 million users, but they extracted 180 million users who had par-
ticipated in at least one conversation during the selected time period. The undirected
communication network contained 1.3 × 109 edges among active users, where an edge
was constructed for each pair of individuals who communicated. The largest connected
component appeared to include 99.9% of all the vertices and, more importantly, it was
estimated to have ¯l(G[S]) ≈6.6. In addition, Leskovec and Horwitz found that the graph
was well clustered with CWS ≈0.137, which was rather large for such a massive network.
One of the most extensive studies of the small-world nature of social networks was con-
ducted by Backstrom et al. [BaEtal12]. They analyzed the dataset of all Facebook users
active as of May 2011. The resulting graph, with n ≈7.21 × 108 and m ≈6.9 × 1010
(symmetric friendship links were considered as its edges), was shown to have O(ln n)
average distance. In particular, they found that ¯l ≈4.74, utilizing the diﬀusion-based
algorithm HyperANF to do estimations. Moreover, they coined the term “four degrees of
separation”, referring to the 3.74 intermediaries used on average. Recently, researchers at
Facebook updated the estimation of the average shortest path length obtained by Back-
strom et al. Edunov et al. [EdEtal16] analyzed the Facebook network, which had became
larger (containing approximately twice as many users compared to May 2011) and more
interconnected. Their estimation, done via the Flajolet-Martin algorithm, showed that
the average distance in the Facebook graph was about 4.57, meaning 3.57 intermediaries
on average (this led to the term “three and a half degrees of separation”).

826
Chapter 10
NETWORKS AND FLOWS
10.9
NETWORK REPRESENTATIONS AND DATA STRUCTURES
To carry out network optimization algorithms eﬃciently, careful attention needs to paid
to the design of the data structures supporting these algorithms. There are alternative
ways to represent a network—diﬀering in their storage requirements and their eﬃcacy
in executing certain fundamental operations.
These representations need to incorpo-
rate both the topology of the underlying graph and also any quantitative information
present in the network (such as cost, length, capacity, demand, or supply). Standard
representations of networks, and trees in particular, are discussed in this section.
10.9.1
NETWORK REPRESENTATIONS
There are various ways to represent networks, just as there are various ways to represent
graphs (§8.1.4, §8.3.1). In addition it is necessary to incorporate quantitative information
associated with the vertices and edges (or arcs) of the network. While the description here
concentrates on directed networks, extensions to undirected networks are also indicated.
Deﬁnitions:
Let G = (V, E) be a directed graph (§8.3.1) with vertex set V = {1, 2, . . ., n} and arc set
E. Deﬁne m = |E| to be the number of arcs in G.
The adjacency set A(i) = {(i, j) | (i, j) ∈E} for vertex i is the set of arcs emanating
from i. (See §10.3.1.)
The adjacency matrix for G is the 0-1 matrix AG = (aij) having aij = 1 if (i, j) ∈E
and aij = 0 if (i, j) /∈E. (See also §6.6.2 and §8.3.1.)
The arc list for G (see §8.3.1) can be implemented using two arc-length arrays FROM
and TO:
• For each arc (i, j) ∈E there is a unique 1 ≤k ≤m satisfying FROM(k) = i and
TO(k) = j.
• Arcs are listed in the parallel FROM and TO arrays in no particular order.
The linked adjacency list for G is given by a vertex-length array START and a singly-
linked list ARCLIST of arc records:
• START(i) points to the ﬁrst record for vertex i in this list, corresponding to a
speciﬁed ﬁrst element of A(i).
• Each arc (i, j) ∈A(i) has an associated arc record, which contains the ﬁelds
TO and NEXT. Speciﬁcally, ARCLIST.TO gives the adjacent vertex j, and
ARCLIST.NEXT points to the next arc record in A(i).
If there is no such
following record, ARCLIST.NEXT = null.
The forward star for G is given by a vertex-length array START and an arc-length
array TO, with the latter in one-to-one correspondence with arcs (i, j) ∈E:
• START(i) gives the position in array TO of the ﬁrst arc leaving vertex i.
• The arcs of A(i) are found in the consecutive positions START(i), START(i) + 1,
. . . , START(i + 1) −1 of array TO. If arc (i, j) corresponds to position k of
TO, then TO(k) = j.

Section 10.9
NETWORK REPRESENTATIONS AND DATA STRUCTURES
827
• By convention, an additional dummy vertex n+1 is added, with START(n+1) =
m + 1.
Facts:
1. An undirected graph can be represented by replacing each undirected edge (i, j) by
two oppositely directed arcs (i, j) and (j, i).
2. The adjacency matrix, the arc list, the linked adjacency list, and the forward star
are four standard representations of a directed (or undirected) graph.
3. The linked adjacency list and forward star structures are commonly used implemen-
tations of the lists-of-neighbors representation (§8.3.1).
4. The following table shows the (worst-case) computational eﬀort required to carry out
certain fundamental operations on G: ﬁnding an arc, deleting an arc (once found), adding
an arc, and scanning the adjacency set of an arbitrary vertex i. Here αi = |A(i)| ≤n.
representation
ﬁnd arc
delete arc
add arc
scan A(i)
adjacency matrix
O(1)
O(1)
O(1)
O(n)
arc list
O(m)
O(1)
O(1)
O(m)
linked adjacency list
O(αi)
O(1)
O(1)
O(αi)
forward star
O(αi)
O(n + m)
O(n + m)
O(αi)
5. The storage requirements of the four representations are given in the following table
for both directed and undirected graphs. For the last two representations, each undirected
edge appears twice: once in each direction.
representation
storage
storage
exploit
(directed)
(undirected)
sparsity?
adjacency matrix
n2
n2
2
no
arc list
2m
2m
yes
linked adjacency list
n + 2m
n + 4m
yes
forward star
n + m
n + 2m
yes
6. As seen in the table of Example 5, all representations other than the adjacency matrix
representation can exploit sparsity in the graph G. That is, the storage requirements are
sensitive to the actual number of arcs and the computations will generally proceed more
rapidly when G has relatively few arcs.
7. Quantitative data for network vertices (such as supply and demand) can be stored
in an associated vertex-length array, thus supplementing the standard graph representa-
tions.
8. Quantitative data for network arcs (such as cost, length, capacity, and ﬂow) can be
accommodated as follows:
• For the adjacency matrix representation, costs (or lengths) cij can be imbedded in
the matrix AG itself. Namely, redeﬁne AG = (aij) so that aij = cij if (i, j) ∈E,
whereas aij is an appropriate special value if (i, j) /∈E. For instance, in the
shortest path problem (§10.3.1), aij = ∞can be used to signify that (i, j) /∈E.
Additional n × n arrays would be needed, however, to represent more than one
type of arc data.
• For the arc list representation, additional arrays parallel to the arrays FROM and
TO can be used to store quantitative arc data.

828
Chapter 10
NETWORKS AND FLOWS
• For the linked adjacency list representation, additional ﬁelds within the arc record
can be used to store quantitative arc data.
• For the forward star representation, additional arrays parallel to the array TO
can be used to store quantitative arc data.
9. The arc list representation is best suited for arc-based processing of a network, such
as occurs in Kruskal’s minimum spanning tree algorithm (§10.1.2).
10. The arc list representation is a convenient form for the input of a network to an
optimization algorithm. Often this external representation is converted within the algo-
rithm to a more suitable internal representation (linked adjacency list or forward star)
before executing the steps of the optimization algorithm.
11. The linked adjacency list and forward star representations are best suited to carrying
out vertex-based explorations of a graph, such as a breadth-ﬁrst search or a depth-ﬁrst
search (§9.2.1). It is also ideal for carrying out Prim’s minimum spanning tree algorithm
(§10.1.2) as well as most shortest path algorithms (§10.3.2).
12. Especially in the case of undirected graphs, the linked adjacency list and forward
star representations can be enhanced by use of an additional arc-length array MIRROR.
The array MIRROR allows one to move from the location of arc (i, j) to the location of
arc (j, i) in constant time.
13. The linked adjacency list is typically used when the structure of the graph can
dynamically change (as by addition/deletion of arcs or vertices). On the other hand, the
forward star representation is appropriate for static graphs, in which the graph structure
does not change.
Examples:
1. A directed graph G with ﬁve vertices and eight arcs is shown in the following ﬁgure.
2
5
4
3
1
The 5 × 5 adjacency matrix for G is given by
AG =






1
2
3
4
5
1
0
1
1
0
0
2
0
0
1
1
0
3
0
0
0
0
0
4
1
0
1
0
1
5
0
0
1
0
0






2. An arc list representation of the directed graph in the ﬁgure of Example 1 is given in
the following table.
FROM
1
2
1
4
2
4
4
5
TO
2
4
3
1
3
5
3
3
3. The following ﬁgure shows a linked adjacency list representation of the directed graph
of Example 1. The symbol ⊙is used to indicate a null pointer.

Section 10.9
NETWORK REPRESENTATIONS AND DATA STRUCTURES
829
1
5
3
3
1
2
3
4
5
2
3
4
3
4. The following ﬁgure shows a forward star representation of the directed graph in
Example 1. Since A(3) = ∅, it is necessary to set START(3) = START(4) = 5. For
example, the arcs in A(4) are associated with positions [START(4), . . . , START(5) −1] =
[5, 6, 7] of the TO array. Similarly, the single arc emanating from vertex 5 is associated
with position [START(5), . . . , START(6) −1] = [8] of the TO array.
1
2
3
4
5
6
1
3
5
5
8
9
2
3
4
3
3
5
1
3
1
2
3
4
5
6
7
8
10.9.2
TREE DATA STRUCTURES
Since trees are important objects in optimization problems, as well as useful data struc-
tures in their own right (see §9.1), additional representations and features of trees are
given here.
Deﬁnitions:
If T is a rooted tree with root r (§9.1.2), then the predecessor function pred : V →V
is deﬁned by pred(r) = 0, and pred(j) = i if vertex i is the parent of j in T . (See §10.3.1.)
The principal subtree Tj rooted at vertex j is the subgraph of T induced by all de-
scendants of j (including j). (See §9.1.2.)
The cardinality card(j) of vertex j in a rooted tree T is the number of vertices in its
principal subtree Tj.
The least common ancestor least(i, j) of vertices i and j in a rooted tree is the vertex
of largest depth (§9.1.2) that is an ancestor of both i and j.
Facts:
1. A rooted tree is uniquely speciﬁed by the mapping pred(·).
2. In a rooted tree with root r, depth(r) = 0 and depth(j) = 1 + depth(pred(j)) for
j ̸= r.

830
Chapter 10
NETWORKS AND FLOWS
3. In a rooted tree, height(i) = 0 if i is a leaf.
If i is not a leaf, then height(i) =
1 + max{height(j) | j a child of i}.
4. In a rooted tree, card(i) = 1 if i is a leaf. If i is not a leaf, card(i) = 1 + P{card(j) |
j a child of i}.
5. The predecessor, depth, height, and cardinality of a rooted tree T can all be calculated
while carrying out a preorder or postorder traversal (§9.1.3) of T :
• The predecessor and depth can be calculated while advancing from the current
vertex to an unvisited vertex.
• The height and cardinality can be updated when retreating from a vertex all of
whose children have been visited.
6. The depth of a vertex is a monotone increasing function on each path from the root.
7. The height and cardinality are monotone decreasing functions on each path from the
root.
Examples:
1. The following ﬁgure shows a tree T rooted at vertex 1. Here the vertices have been
numbered according to a preorder traversal of T . The accompanying table gives the
predecessor, depth, height, and cardinality of each vertex of T .
2
5
4
3
1
6
7
8
9
vertex
1
2
3
4
5
6
7
8
9
pred
0
1
2
2
1
5
6
6
1
depth
0
1
2
2
1
2
3
3
1
height
3
1
0
0
2
1
0
0
0
card
9
3
1
1
4
3
1
1
1
2. Certain applications (such as cycle detection in the network simplex algorithm,
§10.5.2) require ﬁnding the least common ancestor least(i, j) of vertices i and j in a
rooted tree.
3. The calculation of least(i, j) can be carried out eﬃciently, in O(n) time, by using
Algorithm 1.
4. Algorithm 1 is based on Fact 6 and employs two auxiliary data structures, the pre-
decessor and depth functions. It repeatedly backs up from a vertex of larger depth until
the least common ancestor is found.
Algorithm 1:
Least common ancestor.
input: rooted tree T , vertices i and j
output: least common ancestor of i and j

REFERENCES
831
procedure least(i, j)
while i ̸= j
if depth(i) > depth(j) then i := pred(i)
else if depth(i) < depth(j) then j := pred(j)
else i := pred(i), j := pred(j)
return i
REFERENCES
Printed Resources:
[Ad99] L. A. Adamic, “The small world web” in Research and Advanced Technology for
Digital Libraries, S. Abiteboul and A-M. Vercoustre (eds.), Springer, 1999, 443–452.
[AdEtal07] W. Adams, M. Guignard, P. Hahn, and W. Hightower, “A level-2 reformulation-
linearization technique bound for the quadratic assignment problem”, European
Journal of Operational Research 180 (2007), 983–996.
[AgEtal00] R. Agarwala, D. L. Applegate, D. Maglott, G. D. Schuler, and A. A. Schaf-
ﬂer,“A fast and scalable radiation hybrid map construction and integration strategy”,
Genome Research 10 (2000), 350–364.
[AhMaOr93] R. K. Ahuja, T. L. Magnanti, and J. B. Orlin, Network Flows: Theory,
Algorithms, and Applications, Prentice-Hall, 1993.
[AhEtal95] R. K. Ahuja, T. L. Magnanti, J. B. Orlin, and M. R. Reddy, “Applications
of network optimization” in Network Models, M. Ball, T. Magnanti, C. Monma, and
G. Nemhauser (eds.), North-Holland, 1995, 1–83.
[AhEtal90] R. K. Ahuja, K. Mehlhorn, J. B. Orlin, and R. E. Tarjan, “Faster algorithms
for the shortest path problem”, Journal of the ACM 37 (1990), 213–223.
[AhOrSh01] R. K. Ahuja, J. B. Orlin, and D. Sharma, “Multi-exchange neighborhood
structures for the capacitated minimum spanning tree problem”, Mathematical Pro-
gramming 91 (2001), 71–97.
[AhOrSh03] R. K. Ahuja, J. B. Orlin, and D. Sharma, “A composite very large-scale
neighborhood structure for the capacitated minimum spanning tree problem”, Op-
erations Research Letters 31 (2003), 185–194.
[AhOrTa89] R. K. Ahuja, J. B. Orlin, and R. E. Tarjan, “Improved time bounds for the
maximum ﬂow problem”, SIAM Journal on Computing 18 (1989), 939–954.
[AiChLu02] W. Aiello, F. Chung, and L. Lu, “Random evolution in massive graphs” in
Handbook of Massive Data Sets, J. Abello, P. M. Pardalos, and M. G. C. Resende
(eds.), Springer, 2002, 97–122.
[AlBa00] R. Albert and A-L. Barab´asi, “Topology of evolving networks: local events and
universality”, Physical Review Letters 85 (2000), 5234.
[AlJeBa99] R. Albert, H. Jeong, and A-L. Barab´asi, “Internet: diameter of the world-wide
web”, Nature 401/6749 (1999), 130–131.
[AlGa88] K. Altinkemer and B. Gavish, “Heuristics with constant error guarantees for
topological design of local access tree networks”, Management Science 34 (1988),
331–341.

832
Chapter 10
NETWORKS AND FLOWS
[AmDoVo96] A. Amberg, W. Domeschke, and S. Voss, “Capacitated minimum spanning
trees: algorithms using intelligent search”, Combinatorial Optimization: Theory and
Practice 1 (1996), 9–33.
[AnBr01] K. M. Anstreicher and N. W. Brixius, “A new bound for the quadratic assign-
ment problem based on convex quadratic programming”, Mathematical Program-
ming 89 (2001), 341–357.
[ApEtal07] D. L. Applegate, R. E. Bixby, V. Chvatal, and W. J. Cook, eds., The Traveling
Salesman Problem: A Computational Study, 2nd ed., Princeton University Press,
2007.
[ArHaMa90] G. J. R. Araque, L. Hall, and T. Magnanti, “Capacitated trees, capacitated
routing and associated polyhedra”, CORE discussion paper 1990061, 1990.
[BaEtal12] L. Backstrom, P. Boldi, M. Rosa, J. Ugander, and S. Vigna, “Four degrees
of separation”, Proceedings of the 4th Annual ACM Web Science Conference, ACM,
2012, 33–42.
[BaSuMa10] L. Backstrom, E. Sun, and C. Marlow, “Find me if you can: improving
geographical prediction with social and spatial proximity”, Proceedings of the 19th
International Conference on World Wide Web, ACM, 2010, 61–70.
[BaMcBe00] C. A. Bailey, T. W. McLain, and R. W. Beard, “Fuel saving strategies
for separated spacecraft interferometry”, AIAA Guidance, Navigation, & Control
Conference, American Institute of Aeronautics & Astronautics, 2000, A00-37143.
[Ba16] A-L. Barab´asi, Network Science, Cambridge University Press, 2016.
[BaAl99] A-L. Barab´asi and R. Albert, “Emergence of scaling in random networks”,
Science 286 (1999), 509–512.
[BaAlJe99] A-L. Barab´asi, R. Albert, and H. Jeong, “Mean-ﬁeld theory for scale-free
random networks”, Physica A 272 (1999), 173–187.
[BaRa10] M. G. Bardossy and S. Raghavan, “Dual-based local search for the connected
facility location and related problems”, INFORMS Journal on Computing 22 (2010),
584–602.
[BiDeSi94] D. Bienstock, Q. Deng, and D. Simchi-Levi, “A branch-and-cut algorithm for
the capacitated minimum spanning tree problem”, working paper, Columbia Univer-
sity, 1994.
[BiG¨u95] D. Bienstock and O. G¨unl¨uk, “Computational experience with a diﬃcult mixed-
integer multicommodity ﬂow problem”, Mathematical Programming 68 (1995), 213–
237.
[Bo98] B. Bollob´as, Random Graphs, Springer, 1998.
[BoCh88] B. Bollob´as and F. R. K. Chung, “The diameter of a cycle plus a random
matching”, SIAM Journal on Discrete Mathematics 1 (1988), 328–333.
[BoRi04] B. Bollob´as and O. Riordan, “The diameter of a scale-free random graph”,
Combinatorica 24 (2004), 5–34.
[BrLa02] G. Bruno and G. Laporte, “A simple enhancement of the Esau-Williams heuris-
tic for the capacitated minimum spanning tree problem”, Journal of the Operational
Research Society 53 (2002), 583–586.
[BuVa06] S. Burer and D. Vandenbusssche, “Solving lift-and-project relaxations of binary
integer programs”, SIAM Journal on Optimization 16 (2006), 726–750.

REFERENCES
833
[BuSt78] R. Burkard and K. Stratmann, “Numerical investigations on quadratic assign-
ment problems”, Naval Research Logistics Quarterly 25 (1978), 129–148.
[Ch00] B. Chazelle, “A minimum spanning tree algorithm with inverse-Ackermann type
complexity”, Journal of the ACM 47 (2000), 1028–1047.
[ChLjRa10] S. Chen, I. Ljubic, and S. Raghavan, “The regenerator location problem”,
Networks 55 (2010), 205–220.
[ChLjRa15] S. Chen, I. Ljubic, and S. Raghavan, “The generalized regenerator location
problem”, INFORMS Journal on Computing 27 (2015), 204–220.
[ChMe99] J. Cheriyan and K. Mehlhorn, “An analysis of the highest-level selection rule
in the preﬂow-push max-ﬂow algorithm”, Information Processing Letters 69 (1999),
239–242.
[ChEtal09] B. V. Cherkassky, L. Georgiadis, A. Goldberg, R. E. Tarjan, and R. F. Wer-
neck, “Shortest-path feasibility algorithms: an experimental evaluation”, Journal of
Experimental Algorithmics 14 (2009), Section 2, Article 7.
[ChGoRa96] B. V. Cherkassky, A. V. Goldberg, and T. Radzik, “Shortest paths algo-
rithms: theory and experimental evaluation”, Mathematical Programming, Series A
73 (1996), 129–174.
[ChHaLa01] K. W. Chong, Y. Han, and T. W. Lam, “Concurrent threads and optimal
parallel minimum spanning trees algorithm”, Journal of the ACM 48 (2001), 297–323.
[ChEtal11] P. Christiano, J. A. Kelner, A. Madry, D. A. Spielman, and S-H. Teng, “Elec-
trical ﬂows, Laplacian systems, and faster approximation of maximum ﬂow in undi-
rected graphs”, Proceedings of the 43rd ACM Symposium on the Theory of Com-
puting, ACM, 2011, 273–282.
[ChLu04] F. R. K. Chung and L. Lu, “The average distance in a random graph with
given expected degrees”, Internet Mathematics 1 (2004), 91–113.
[CoHa03] R. Cohen and S. Havlin, “Scale-free networks are ultrasmall”, Physical Review
Letters 90 (2003), 058701.
[CoKeTa94] R. Cole, P. Kelvin, and R. E. Tarjan, “A linear-work parallel algorithm
for ﬁnding minimum spanning trees”, Proceedings of the 6th ACM Symposium on
Parallel Algorithms and Architectures, 1994, 11–15.
[Co12] W. J. Cook, In Pursuit of the Traveling Salesman: Mathematics at the Limits of
Computation, Princeton University Press, 2012.
[CoEtal01] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction to
Algorithms, 2nd ed., MIT Press and McGraw-Hill, 2001.
[DaFu00] H. K. Dai and S. Fujino, “On designing constrained local access networks”, Pro-
ceedings of the 2000 International Symposium on Parallel Architectures, Algorithms,
and Networks (ISPAN ’00), IEEE Computer Society, 2000, 167–176.
[Di69] R. Dial, “Algorithm 360: shortest path forest with topological ordering”, Commu-
nications of the ACM 12 (1969), 632–633.
[DoMuWa03] P. S. Dodds, R. Muhamad, and D. J. Watts, “An experimental study of
search in global social networks”, Science 301 (2003), 827–829.
[DoMe00] S. N. Dorogovtsev and J. F. F. Mendes, “Exactly solvable small-world net-
work”, Europhysics Letters 50 (2000), 1–7.

834
Chapter 10
NETWORKS AND FLOWS
[DoMe01] S. N. Dorogovtsev and J. F. F. Mendes, “Scaling properties of scale-free evolv-
ing networks: continuous approach”, Physical Review E 63 (2001), 056125.
[DoMeSa03] S. N. Dorogovtsev, J. F. F. Mendes, and A. N. Samukhin, “Metric structure
of random networks”, Nuclear Physics B 653 (2003), 307–338.
[Dr15] Z. Drezner, “The quadratic assignment problem” in Location Science, G. Laporte,
S. Nickel, and F. Saldanha da Gama (eds.), Springer, 2015, 345–363.
[EaKl10] D. Easley and J. Kleinberg, Networks, Crowds, and Markets, Cambridge Uni-
versity Press, 2010.
[Ed65a] J. Edmonds, “Paths, trees, and ﬂowers”, Canadian Journal of Mathematics 17
(1965), 449–467.
[Ed65b] J. Edmonds, “Maximum matching and a polyhedron with 0, 1-vertices”, Journal
of Research of the National Bureau of Standards B-69 (1965), 125–130.
[EdEtal16] S. Edunov, C. Diuk, I. O. Filiz, S. Bhagat, and M. Burke, “Three and a half
degrees of separation”, available at https://research.facebook.com/blog/three-
and-a-half-degrees-of-separation/, 2016.
[ErR´e60] P. Erd˝os and A. R´enyi, “On the evolution of random graphs”, Publication of
the Mathematical Institute of the Hungarian Academy of Sciences 5 (1960), 17–61.
[EsWi66] L. R. Esau and K. C. Williams, “On teleprocessing system design”, IBM Sys-
tems Journal 5 (1966), 142–147.
[EvMi92] J. R. Evans and E. Minieka, Optimization Algorithms for Networks and Graphs,
Marcel Dekker, 1992.
[FeHu04] P. F. Felzenszwalb and D. P. Huttenlocher, “Eﬃcient graph-based image seg-
mentation”, International Journal of Computer Vision 59 (2014), 167–181.
[FeRa07] D. Fernholz and V. Ramachandran, “The diameter of sparse random graphs”,
Random Structures & Algorithms 31 (2007), 482–516.
[FoFu62] L. R. Ford and D. R. Fulkerson, Flows in Networks, Princeton University Press,
1962.
[FrTa84] M. L. Fredman and R. E. Tarjan, “Fibonacci heaps and their uses in improved
network optimization algorithms”, Proceedings of the 25th IEEE Symposium on
Foundations of Computer Science, 1984, 338–346. Full paper in Journal of the ACM
34 (1987), 596–615.
[Ga76] H. N. Gabow, “An eﬃcient implementation of Edmonds’ algorithm for maximum
matching on graphs”, Journal of the ACM 23 (1976), 221–234.
[GaEtal86] H. N. Gabow, Z. Galil, T. Spencer, and R. E. Tarjan, “Eﬃcient algorithms for
ﬁnding minimum spanning trees in undirected and directed graphs”, Combinatorica
6 (1986), 109–122.
[GaSh62] D. Gale and L. S. Shapley, “College admissions and the stability of marriage”,
American Mathematical Monthly 69 (1962), 9–15.
[Ga85] B. Gavish, “Augmented Lagrangian based algorithms for centralized network
design”, IEEE Transactions on Communications COM-33 (1985), 1247–1257.
[Ga91] B. Gavish, “Topological design of telecommunication networks—local access de-
sign methods”, Annals of Operations Research 33 (1991), 17–71.
[GeHeLa94] M. Gendreau, A. Hertz, and G. Laporte, “A tabu search heuristic for the
vehicle routing problem”, Management Science 40 (1994), 1276–1290.

REFERENCES
835
[Ge95] A. M. H. Gerards, “Matching” in Network Models, M. Ball, T. Magnanti, C.
Monma, and G. Nemhauser (eds.), North-Holland, 1995, 135–224.
[Gi59] E. N. Gilbert, “Random graphs”, The Annals of Mathematical Statistics 30 (1959),
1141–1144.
[GlKl75] F. Glover and D. Klingman, “Finding minimum spanning trees with a ﬁxed
number of links at a node”, in Combinatorial Programming: Methods and Applica-
tions, B. Roy et al. (eds.), 1975, 191–201.
[GoBe93] M. X. Goemans and D. J. Bertsimas, “Survivable networks, linear programming
relaxations and the parsimonious property”, Mathematical Programming 60 (1993),
145–166.
[Go95] A. V. Goldberg, “Scaling algorithms for the shortest paths problem”, SIAM Jour-
nal on Computing 24 (1995), 494–504.
[Go01] A. V. Goldberg, “A simple shortest path algorithm with linear average time”,
Algorithms–ESA 2001, Lecture Notes in Computer Science 2161, Springer, 2001,
230–241.
[GoRa98] A. V. Goldberg and S. Rao, “Beyond the ﬂow decomposition barrier”, Journal
of the ACM 45 (1998), 783–797.
[GoTa86] A. V. Goldberg and R. E. Tarjan, “A new approach to the maximum ﬂow
problem”, Proceedings of the 18th ACM Symposium on the Theory of Computing,
1986, 136–146. Full paper appears in Journal of the ACM 35 (1988), 921–940.
[GoTa88] A. V. Goldberg and R. E. Tarjan, “Finding minimum-cost circulations by
canceling negative cycles”, Proceedings of the 20th ACM Symposium on the Theory
of Computing, 1988, 7–18. Full paper appears in Journal of the ACM 36 (1989),
873–886.
[GoTa14] A. V. Goldberg and R. E. Tarjan, “Eﬃcient maximum ﬂow algorithms”, Com-
munications of the ACM 57/8 (2014), 82–89.
[GoLj11] S. Gollowitzer and I. Ljubic, “MIP models for connected facility location: a
theoretical and computational study”, Computers & Operations Research 38 (2011),
435–449.
[GoMa99] L. Gouveia and P. Martins, “The Capacitated Minimal Spanning Tree Prob-
lem: an experiment with a hop-indexed model”, Annals of Operations Research 86
(1999), 271–294.
[GoMa00] L. Gouveia and P. Martins, “A hierarchy of hop-indexed models for the Ca-
pacitated Minimum Spanning Tree Problem”, Networks 35 (2000), 1–16.
[GoMa05] L. Gouveia and P. Martins, “The capacitated minimum spanning tree problem:
revisiting hop-indexed formulations”, Computers & Operations Research 32 (2005),
2435–2452.
[GrHe85] R. L. Graham and P. Hell, “On the history of the minimum spanning tree
problem”, Annals of the History of Computing 7 (1985), 43–57.
[GrWh70] G. Graves and A. Whinston, “An algorithm for the quadratic assignment
problem”, Management Science 17 (1970), 453–471.
[GrGoWa11] C. Groer, B. Golden, and E. Wasil, “A parallel algorithm for the vehicle
routing problem”, INFORMS Journal on Computing 23 (2011), 315–330.

836
Chapter 10
NETWORKS AND FLOWS
[GrMoSt92] M. Gr¨otschel, C. L. Monma, and M. Stoer, “Computational results with a
cutting plane algorithm for designing communication networks with low-connectivity
constraints”, Operations Research 40 (1992), 309–330.
[Gu90] J. Guare, Six Degrees of Separation: A Play, Vintage Books, 1990.
[GuSp14] G. Guastaroba and M. G. Speranza, “A heuristic for BILP problems: The Single
Source Capacitated Facility Location Problem”, European Journal of Operational
Research 238 (2014), 438–450.
[GuIr89] D. Gusﬁeld and R. W. Irving, The Stable Marriage Problem: Structure and
Algorithms, MIT Press, 1989.
[HaGr98] P. Hahn and T. Grant, “Lower bounds for the quadratic assignment problem
based upon a dual formulation”, Operations Research 46 (1998), 912–922.
[HaEtal12] P. M. Hahn, Y-R. Zhu, M. Guignard, W. L. Hightower, and M. J. Saltz-
man, “A level-3 reformulation-linearization technique-based bound for the quadratic
assignment problem”, INFORMS Journal on Computing 24 (2012), 202–209.
[Ha96] L. Hall, “Experience with a cutting plane algorithm for the capacitated spanning
tree problem”, INFORMS Journal on Computing 8 (1996), 219–234.
[HaPa16] R. Hall and J. Partyka, “Higher expectations drive performance”, OR/MS
Today 43/1 (2016), 40–47.
[HaAg15] F. Hamid and Y. K. Agarwal, “Solving the two-facility network design problem
with 3-partition facets”, Networks 66 (2015), 11–32.
[HaRa03] B. T. Han and V. T. Raja, “A GRASP heuristic for solving an extended
capacitated concentrator location problem”, International Journal of Information
Technology and Decision Making 2 (2003), 597–617.
[HaRo55] T. E. Harris, and F. S. Ross, “Fundamentals of a method for evaluating rail
net capacities”, Research Memorandum RM-1573, The RAND Corporation, 1955.
[Ho98] D. S. Hochbaum, “The pseudoﬂow algorithm and the pseudoﬂow-based simplex
for the maximum ﬂow problem”, in Integer Programming and Combinatorial Opti-
mization, Lecture Notes in Computer Science 1412, Springer, 1998, 325–337.
[HoPaRi13] K. L. Hoﬀman, M. Padberg, and G. Rinaldi, “Traveling salesman problem”
in Encyclopedia of Operations Research and Management Science, S. I. Gass and M.
C. Fu (eds.), Springer, 2013, 1573–1578.
[JaRo07] M. O. Jackson and B. W. Rogers, “Meeting strangers and friends of friends: how
random are social networks?”, The American Economic Review 97 (2007), 890–915.
[JaLuRu00] S. Janson, T. Luczak, and A. Rucinski, Random Graphs, John Wiley & Sons,
2000.
[JeN´eBa03] H. Jeong, Z. N´eda, and A-L. Barab´asi, “Measuring preferential attachment
in evolving networks”, Europhysics Letters 61 (2003), 567–572.
[J¨uReRi95] M. J¨unger, G. Reinelt, and G. Rinaldi, “The traveling salesman problem”
in Network Models, M. Ball, T. Magnanti, C. Monma, and G. Nemhauser (eds.),
North-Holland, 1995, 225–330.
[KaKlTa95] D. R. Karger, P. N. Klein, and R. E. Tarjan, “A randomized linear-time
algorithm to ﬁnd minimum spanning trees”, Journal of the ACM 42 (1995), 321–
328.

REFERENCES
837
[KeEtal14] J. A. Kelner, Y. T. Lee, L. Orecchia, and A. Sidford, “An almost-linear-time
algorithm for approximate max ﬂow in undirected graphs, and its multicommod-
ity generalizations”, Proceedings of the 25th ACM-SIAM Symposium on Discrete
Algorithms, SIAM, 2014, 217–226.
[KiHe03] J. Kim and J. Hespanha, “Discrete approximations to continuous shortest-path:
application to minimum-risk path planning for groups of UAVs”, Proceedings of the
42nd IEEE Conference on Decision and Control, IEEE, 2003, 1734–1740.
[KiLe88] G. A. P. Kindervator and J. K. Lenstra, “Parallel computing in combinatorial
optimization”, Annals of Operations Research 14 (1988), 245–289.
[KiRaTa94] V. King, S. Rao, and R. Tarjan, “A faster deterministic maximum ﬂow
algorithm”, Journal of Algorithms 17 (1994), 447–474.
[Kl00] J. Kleinberg, “The small-world phenomenon: an algorithmic perspective”, Pro-
ceedings of the 32nd Annual ACM Symposium on the Theory of Computing, ACM,
2000, 163–170.
[KlTa13] J. Kleinberg and E. Tardos, Algorithm Design, Pearson Education Limited,
2013.
[KlEg02a] K. Klemm and V. M. Egu´ıluz, “Highly clustered scale-free networks”, Physical
Review E 65 (2002), 036123.
[KlEg02b] K. Klemm and V. M. Egu´ıluz, “Growing scale-free networks with small-world
behavior”, Physical Review E 65 (2002), 057102.
[KnGo97] D. E. Knuth and M. Goldstein, Stable Marriage and Its Relation to Other Com-
binatorial Problems: An Introduction to the Mathematical Analysis of Algorithms,
American Mathematical Society, 1997.
[KrReLe00] P. L. Krapivsky, S. Redner, and F. Leyvraz, “Connectivity of growing random
networks”, Physical Review Letters 85 (2000), 4629.
[KuHe87] A. Kusiak and S. Heragu, “The facility layout problem”, European Journal of
Operational Research 29 (1987), 229–251.
[La10] G. Laporte, “A concise guide to the traveling salesman problem”, Journal of the
Operational Research Society 61 (2010), 35–40.
[LaRoVi14] G. Laporte, S. Ropke, and T. Vidal, “Heuristics for the vehicle routing prob-
lem” in Vehicle Routing: Problems, Methods, and Applications, 2nd ed., P. Toth and
D. Vigo (eds.), SIAM, 2014, 87–116.
[LaEtal85] E. L. Lawler, J. K. Lenstra, A. H. G. Rinnooy Kan, and D. B. Shmoys, eds.,
The Traveling Salesman Problem: A Guided Tour of Combinatorial Optimization,
Wiley, 1985.
[Le92] F. T. Leighton, Introduction to Parallel Algorithms and Architectures, Morgan
Kaufmann, 1992.
[LeHo08] J. Leskovec and E. Horvitz, “Planetary-scale views on a large instant-messaging
network”, Proceedings of the 17th International Conference on World Wide Web,
ACM, 2008, 915–924.
[LiEtal05] D. Liben-Nowell, J. Novak, R. Kumar, P. Raghavan, and A. Tomkins, “Geo-
graphic routing in social networks”, Proceedings of the National Academy of Sciences
USA 102 (2005), 11623–11628.
[Li81] R. Liggett, “The quadratic assignment problem: an experimental evaluation of
solution strategies”, Management Science 27 (1981), 442–458.

838
Chapter 10
NETWORKS AND FLOWS
[LoPu07] A. Lodi and A. P. Punnen, “TSP software” in The Traveling Salesman Problem
and Its Variations, G. Gutin and A. P. Punnen (eds.), Springer, 2007, 737–749.
[LoPu86] L. Lov´asz and M. D. Plummer, Matching Theory, North-Holland, 1986.
[MaMiVa95] T. L. Magnanti, P. Mirchandani, and R. Vachani, “Modeling and solving the
two-facility capacitated network loading problem”, Operations Research 43 (1995),
142–157.
[MiVa80] S. Micali and V. V. Vazirani, “An O(
p
|V |·|E|) algorithm for ﬁnding maximum
matching in general graphs”, Proceedings of the 21st Symposium on Foundations of
Computer Science, 1980, 17–27.
[Mi67] S. Milgram, “The small world problem”, Psychology Today 1 (1967), 61–67.
[NeWo99] G. L. Nemhauser and L. A. Wolsey, Integer and Combinatorial Optimization,
Wiley, 1999.
[Ne01] M. E. J. Newman, “Scientiﬁc collaboration networks. II. Shortest paths, weighted
networks, and centrality”, Physical Review E 64 (2001), 016132.
[Ne03] M. E. J. Newman, “The structure and function of complex networks”, SIAM
Review 45 (2003), 167–256.
[Ne10] M. E. J. Newman, Networks: An Introduction, Oxford University Press, 2010.
[NeMoWa00] M. E. J. Newman, C. Moore, and D. J. Watts, “Mean-ﬁeld solution of the
small-world network model”, Physical Review Letters 84 (2000), 3201.
[NeStWa01] M. E. J. Newman, S. H. Strogatz, and D. J. Watts, “Random graphs with
arbitrary degree distributions and their applications”, Physical Review E 64 (2001),
026118.
[NeWa99] M. E. J. Newman and D. J. Watts, “Scaling and percolation in the small-world
network model”, Physical Review E 60 (1999), 7332.
[¨OnAl09] T. ¨Oncan and I. K. Altinel, “Parametric enhancements of the Esau-Williams
heuristic for the capacitated minimum spanning tree problem”, Journal of the Op-
erational Research Society 60 (2009), 259–267.
[Or88] J. B. Orlin, “A faster strongly polynomial minimum cost ﬂow algorithm”, Pro-
ceedings of the 20th ACM Symposium on the Theory of Computing, ACM, 1988,
377–387. Full paper in Operations Research 41 (1993), 338–350.
[Or97] J. B. Orlin, “A polynomial time primal network simplex algorithm for minimum
cost ﬂows”, Mathematical Programming 78 (1997), 109–129.
[Or13] J. B. Orlin, “Max ﬂows in O(nm) time, or better”, Proceedings of the 45th ACM
Symposium on the Theory of Computing, ACM, 2013, 765–774.
[OsKe95] I. Osman and J. Kelly, eds., Meta-Heuristics: Theory and Applications, Kluwer,
1995.
[PeRa02] S. Pettie and V. Ramachandran, “An optimal minimum spanning tree algo-
rithm”, Journal of the ACM 49 (2002), 16–34.
[Pl92] M. D. Plummer, “Matching theory—a sampler: from D´enes K¨onig to the present”,
Discrete Mathematics 100 (1992), 177–219.
[Pr76] D. De Solla Price, “A general theory of bibliometric and other cumulative advan-
tage processes”, Journal of the American Society for Information Science 27 (1976),
292–306.

REFERENCES
839
[RaEtal02] K. G. Ramakrishnan, M. Resende, B. Ramachandran, and J. Pekny, “Tight
QAP bounds via linear programming” in Combinatorial and Global Optimization, P.
M. Pardalos, A. Migdalas, and R. Burkard (eds.), World Scientiﬁc Publishing, 2002,
297–303.
[Re93] C. Reeves, ed., Modern Heuristic Techniques for Combinatorial Problems, Wiley,
1993.
[ReMa11] C. Rego and F. Mathew, “A ﬁlter-and-fan algorithm for the capacitated min-
imum spanning tree problem”, Computers and Industrial Engineering 60 (2011),
187–194.
[ReLa06] M. Reimann and M. Laumanns, “Savings based ant colony optimization for the
capacitated minimum spanning tree problem”, Computers & Operations Research
33 (2006), 1794–1822.
[ReSo07] F. Rendl and R. Sotirov, “Bounds for the quadratic assignment problem using
the bundle method”, Mathematical Programming 109 (2007), 505–524.
[ReRaDr95] M. Resende, K. Ramakrishnan, and Z. Drezner, “Computational experi-
ments with the lower bound for the quadratic assignment problem based on linear
programming”, Operations Research 43 (1995), 781–791.
[RoPe99] A. E. Roth and E. Peranson, “The redesign of the matching market for Amer-
ican physicians: some engineering aspects of economic design”, American Economic
Review 89 (1999), 748–780.
[RoSo91] A. E. Roth and M. Sotomayor, Two-Sided Matching: A Study in Game-Theoretic
Modeling and Analysis, Cambridge University Press, 1991.
[Ro04] F. Roupin, “From linear to semideﬁnite programming: an algorithm to obtain
semideﬁnite relaxations for bivalent quadratic problems”, Journal of Combinatorial
Optimization 8 (2004), 469–493.
[Ro99] S. Roy, “Stereo without epipolar lines: a maximum-ﬂow formulation”, Interna-
tional Journal of Computer Vision 34 (1999), 147–161.
[RuEtal15] E. Ruiz, M. Albareda-Sambola, E. Fern´andez, and M. G. C. Resende, “A
biased random-key genetic algorithm for the capacitated minimum spanning tree
problem”, Computers & Operations Research 57 (2015), 95–108.
[ShEtal97] Y. M. Sharaiha, M. Gendreau, G. Laporte, and I. H. Osman, “A tabu search
algorithm for the capacitated shortest spanning tree problem”, Networks 29 (1997),
161–171.
[ShAd99] H. D. Sherali and W. P. Adams, A Reformulation-Linearization Technique for
Solving Discrete and Continuous Nonconvex Problems, Springer, 1999.
[SlMaKa97] T. H. Sloane, F. Mann, and H. Kaveh, “Powering the last mile: an alterna-
tive to powering FITL”, 19th International Telecommunications Energy Conference,
1997, 536–543.
[SoDuRi03] M. C. de Souza, C. Duhamel, and C. C. Ribeiro, “A GRASP heuristic for
the capacitated minimum spanning tree problem using a memory-based local search
strategy”, Applied Optimization 86 (2003), 627–658.
[TaRo04] E. Tapia and R. Rojas, “Recognition of on-line handwritten mathematical
expressions using a minimum spanning tree construction and symbol dominance”, in
Graphics Recognition. Recent Advances and Perspectives, Springer, 2004, 329–340.
[Ta83] R. E. Tarjan, Data Structures and Network Algorithms, SIAM, 1983.

840
Chapter 10
NETWORKS AND FLOWS
[Th99] M. Thorup, “Undirected single-source shortest paths with positive integer weights
in linear time”, Journal of the ACM 46 (1999), 362–394.
[TrMi69] J. Travers and S. Milgram, “An experimental study of the small world problem”,
Sociometry 32 (1969), 425–443.
[UcEtal08] E. Uchoa, R. Fukasawa, J. Lysgaard, A. Pessoa, M. P. de Arag˜ao, and D.
Andrade, “Robust branch-cut-and-price for the Capacitated Minimum Spanning Tree
problem over a large extended formulation”, Mathematical Programming 112 (2008),
443–472.
[V´a03] A. V´azquez, “Growing network with local rules: preferential attachment, clustering
hierarchy, and degree correlations”, Physical Review E 67 (2003), 056104.
[Vo89] A. Volgenant, “A Lagrangean approach to the degree-constrained minimum span-
ning tree problem”, European Journal of Operational Research 39 (1989), 325–331.
[WaEtal16] X. Wang, M. Battarra, B. Golden, and E. Wasil, “Vehicle routing and schedul-
ing” in Routledge Handbook of Transportation, D. Teodorovic (ed.), Taylor & Fran-
cis, 2016, 238–256.
[WaSt98] D. J. Watts and S. H. Strogatz, “Collective dynamics of ‘small-world’ networks”,
Nature 393 (1998), 440–442.
[XuOlXu02] Y. Xu, V. Olman, and D. Xu, “Clustering gene expression data using a
graph-theoretic approach: an application of minimum spanning trees”, Bioinformat-
ics 18 (2002), 536–545.
[YaChCh12] Z. Yang, F. Chu, and H. Chen, “A cut-and-solve based algorithm for the
single-source capacitated facility location problem”, European Journal of Operational
Research 221 (2012), 521–532.
[Ya75] A. Yao, “An O(|E| log log |V |) algorithm for ﬁnding minimum spanning trees”,
Information Processing Letters 4 (1975), 21–23.
[Zw02] U. Zwick, “All pairs shortest paths using bridging sets and rectangular matrix
multiplication”, Journal of the ACM 49 (2002), 289–317.
Web Resources:
ftp://dimacs.rutgers.edu/pub/netflow/matching/ (Computer code in C for solving
weighted matching problems; computer codes in C, Pascal, and Fortran for ﬁnding
maximum size matchings in nonbipartite networks.)
ftp://dimacs.rutgers.edu/pub/netflow/maxflow/ (Computer codes for solving max-
imum ﬂow and minimum cut problems.)
ftp://dimacs.rutgers.edu/pub/netflow/mincost/ (Computer codes for solving min-
imum cost ﬂow problems.)
http://anjos.mgi.polymtl.ca/qaplib/ (Problem instances, solutions, software, and
research papers for the QAP.)
http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/ (A library of sample
problems related to the traveling salesman problem, with their best known solutions.)
http://lemon.cs.elte.hu/trac/lemon
(Computer codes for solving minimum cost
ﬂow problems.)
http://neo.lcc.uma.es/vrp/ (Datasets and solution methods for vehicle routing prob-
lems and variants.)

REFERENCES
841
http://oracleofbacon.org
(Site for determining the Bacon number for a speciﬁed
actor.)
http://riot.ieor.berkeley.edu/Applications/Pseudoflow/maxflow.html
(Com-
puter codes for solving maximum ﬂow and minimum cut problems.)
http://www.ams.org/mathscinet/collaborationDistance.html (Website for calcu-
lating the minimum distance between mathematical researchers.)
http://www.avglab.com/andrew/soft.html
(Computer codes to solve single-source
shortest path, maximum ﬂow, minimum cut, and minimum cost ﬂow problems.)
http://www.cs.sunysb.edu/~algorith/ (The Stony Brook Algorithm Repository; see
listed Sections 1.4 and 1.5 on Graph Problems.)
http://www.dis.uniroma1.it/challenge9/
(Computer codes to solve single-source
shortest path problems.)
http://www.mat.uc.pt/~eqvm/cientificos/fortran/codigos.html (Fortran code for
implementing Kruskal’s algorithm and Prim’s algorithm for minimum spanning trees;
Fortran code for implementing the label-correcting algorithm and Dijkstra’s algo-
rithm for shortest paths.)
http://www.netlib.org/toms/479 (Fortran code for implementing Prim’s algorithm.)
http://www.netlib.org/toms/562 (Fortran code for implementing the label-correcting
algorithm for shortest paths.)
http://www.netlib.org/toms/613 (Fortran code for implementing Prim’s algorithm.)
http://www.oakland.edu/enp/ (Website for the Erd˝os Number Project.)
http://www3.cs.stonybrook.edu/~algorith/files/minimum-spanning-tree.shtml
(Computer codes for solving minimum spanning tree problems.)
https://research.facebook.com/blog/three-and-a-half-degrees-of-separation
(Article about three and a half degrees of separation in the Facebook graph.)


11
PARTIALLY ORDERED SETS
11.1 Basic Poset Concepts
Graham Brightwell and
11.1.1 Comparability
Douglas B. West
11.1.2 Chains, Antichains, and Poset Operations
11.1.3 Rank, Ideals, and Filters
11.1.4 Lattices
11.1.5 Distributive and Modular Lattices
11.2 Poset Properties
Graham Brightwell and
11.2.1 Poset Partitions
Douglas B. West
11.2.2 LYM Property
11.2.3 Rankings, Semiorders, and Interval Orders
11.2.4 Application to Social Choice
11.2.5 Linear Extensions and Dimension
11.2.6 Posets and Graphs

844
Chapter 11
PARTIALLY ORDERED SETS
INTRODUCTION
Partially ordered sets play important roles in a wide variety of applications, including
the design of sorting and searching methods, the scheduling of tasks, the study of social
choice, and the study of lattices. This chapter covers the basic concepts involving par-
tially ordered sets, the various types of partially ordered sets, the fundamental properties
of these sets, and their important applications.
A table of notation used in the study of posets is given following the glossary.
GLOSSARY
antichain: a subset of a poset in which no two distinct elements are comparable.
atom: in a poset, an element of height 1.
atomic lattice: a lattice such that every element is a join of atoms (or equivalently,
such that the atoms are the only join-irreducible elements).
auxiliary graph (of a simple graph G): the graph G′ whose vertices are the edges of G,
with vertex e1 adjacent to vertex e2 in G′ if and only if e1 and e2 are adjacent edges
in G, but do not lie on a 3-cycle in G.
biorder representation (on a digraph D): a pair of real-valued functions f, g on the
vertex set VD such that u →v is an arc if and only if f(u) > g(v).
bipartite poset: a poset of height at most 2.
Boolean algebra: the poset whose domain is all subsets of a given set, partially ordered
by inclusion.
Borda consensus function (on a set of social choice proﬁles): the consensus function
that ranks the alternatives by their Borda count.
Borda count (of an alternative social choice x): the sum, over all individual rankings,
of the number of alternatives x “beats”.
bounded poset: a poset with both a unique minimal element and a unique maximal
element.
u,v-bypass (in a directed graph): a u,v-path of length at least two such that there is
also an arc from u to v.
Cartesian product (of two posets P = (X, R) and P ′ = (X′, R′)): the poset P ×P ′ =
(X × X′, S), such that (x, x′)S(y, y′) if and only if xRy and x′R′y′.
chain: a subset of a poset in which every two elements are comparable.
k-chain: a chain of size k, i.e., a chain on k elements.
chain-product: the Cartesian product of a collection of chains.
comparability digraph (of a poset (X, R)): the simple digraph whose vertex set is
the domain X and which has an arc from x to y if and only if x ≤y.
comparability graph (of a poset (X, R)): the simple graph whose vertex set is the
domain X and which has an edge joining distinct vertices x and y if and only if
x ≤y.

GLOSSARY
845
comparability invariant (for posets): an invariant f such that f(P) = f(Q) when-
ever posets P and Q have the same comparability graph.
comparable elements (in a poset (X, R)): elements x and y such that either (x, y) ∈
R or (y, x) ∈R.
consecutive chain (in a ranked poset): a chain whose elements belong to consecutive
ranks.
consensus function (on a set of social choice proﬁles): a function that assigns to each
possible proﬁle P = {Pi | i ∈I} on a set of alternatives a linear ordering (ties allowed)
of those alternatives.
consensus ranking (on a set of social choice proﬁles): the linear ordering of the alter-
natives assigned by the consensus function.
cover graph (of a poset (X, R)): the graph with vertex set X and edge set consisting
of the pairs satisfying the cover relation.
cover relation (of a poset (X, R)): the relation on X consisting of the pairs (x, y) such
that x > y in R and such that there is no “intermediate” element z with x > z > y.
cover diagram: a synonym for the Hasse diagram.
critical pair (in a poset): an ordered incomparable pair that cannot be made compa-
rable by adding any other single incomparable pair as a relation.
dependent edge (in an acyclic directed graph): an arc from u to v such that the graph
contains a u,v-bypass.
dimension (of a poset): the minimum number of chains in a realizer of the poset.
distributive lattice: a lattice in which the meet operator distributes over the join
operator, so that x ∧(y ∨z) = (x ∧y) ∨(x ∧z) for all x, y, z.
divisor lattice: the poset D(n) of divisors of n, in which x ≤y means that y is an
integer multiple of x.
downset (in a poset): a subposet I such that if x ∈I and if y < x, then y ∈I; also
called an ideal.
dual (of a poset P = (X, R)): the poset P ∗= (X, S) such that x ≤y in S if and only
if y ≤x in R.
extension (of a poset P = (X, R)): a poset Q = (X, S) such that R ⊆S; meaning that
xRy implies xSy.
k-family (in a poset): a subposet containing no chain of size k + 1.
Ferrers digraph: a digraph having a biorder representation.
ﬁlter (generated by an element x in a poset P): the upset U[x] = {y ∈P | y ≥x}.
ﬁlter (generated by a subset in a poset P): given a subset A of P, the upset U[A] =
S
x∈A U[x].
ﬁlter (in a poset): a subposet whose domain is the set-theoretic complement of the
domain of an ideal.
forbidden subposet description (of a class of posets): a characterization of the class
as the class of all posets that does not contain any of the posets in a speciﬁed col-
lection.
geometric lattice: an atomic, upper semimodular lattice of ﬁnite height.

846
Chapter 11
PARTIALLY ORDERED SETS
greatest lower bound (glb) (of elements x and y in a poset): a common lower bound
z such that every other common lower bound z′ satisﬁes the inequality z ≥z′. Such
an element, if it exists, is denoted x ∧y.
graded poset: a poset in which all maximal chains have the same length.
Hasse diagram (of a poset): a straight-line drawing of the cover graph in the plane so
that the lesser element of each adjacent pair is below the greater.
height (of a poset): the maximum size of a chain in that poset.
height (of an element x of a poset): the maximum length h(x) of a chain that has x as
its maximal element.
ideal (generated by an element x in a poset P): the downset D[x] = {y ∈P | y ≤x}.
ideal (generated by a subset A in a poset P): the downset D[A] = S
x∈A D[x].
ideal (in a poset): a subposet I such that if x ∈I and if y < x, then y ∈I.
incomparability graph (of a poset P): the edge-complement of the associated com-
parability graph G(P).
incomparable pair (in a poset (X, R)): a pair x, y ∈X such that neither x ≤y nor
y ≤x in R.
integer partition: a nonincreasing nonnegative integer sequence having ﬁnitely many
nonzero terms, with trailing zeros added as needed for comparison.
intersecting family: a collection of subsets of a set such that every pair of members
has nonempty intersection.
intersection (of partial orderings P = (X, R) and Q = (X, S) on the set X): the poset
(X, R ∩S) that includes the comparisons present in both.
intersection (of posets (X, R) and (X, S)): the poset (X, R ∩S).
interval (in a poset): the subposet which contains all elements z such that x ≤z ≤y.
interval order: a poset in which there is an assignment to its members of real intervals,
such that x < y if and only if the interval for y is totally to the right of the interval
for x.
interval representation (of a poset P): a collection of real intervals corresponding to
an interval order for P.
isomorphic (posets): posets P = (X, R) and Q = (Y, S) such that there is a poset
isomorphism P →Q.
isomorphism (of lattices): an order-preserving bijection from one lattice to another
that also preserves greatest lower bounds and least upper bounds of pairs.
isomorphism (of posets): a bijection from one poset to another that preserves the order
relation.
join: given {x, y}, another name for the least upper bound x ∨y.
join-irreducible element (in a lattice): a nonzero element that cannot be expressed
as the join of two other elements.
Jordan-Dedekind chain condition: the condition for a poset that every interval has
ﬁnite length.
lattice: a poset in which every pair of elements has both a greatest lower bound and a
least upper bound.

GLOSSARY
847
lattice (of bounded sequences): the set L(m, n) of length-m real sequences a1, . . . , am
such that 0 ≤a1 ≤· · · ≤am ≤n.
lattice (of order ideals in a poset P = (X, R)): the set J(P) of order ideals of P, or-
dered by inclusion.
least upper bound (lub) (of elements x and y in a poset): a common upper bound z
such that every other common upper bound z′ satisﬁes the inequality z′ ≥z. Such
an element, if it exists, is denoted x ∨y.
length (of a chain): the number of cover relations in the chain; in other words, one less
than the number of elements in the chain.
length (of a poset): the length of a longest chain, which is one less than the height of
that poset. (Sometimes height is used synonymously with length.)
lexicographic ordering (of the Cartesian product of posets): the ordering for the Car-
tesian product of the domains in which (x1, x2) ≤(y1, y2) if and only if x1 < y1 or
x1 = y1 and x2 ≤y2; this is not the usual ordering of the Cartesian product of
posets.
linear extension (of a poset): an extension of the poset that is a chain.
linear order: See total order.
linearly ordered set: a poset in which every pair of elements is comparable.
linear sum (of two disjoint posets P and P ′): the poset in which all the elements of
poset P lie “below” all those of poset P ′.
locally ﬁnite poset: a poset in which every interval is ﬁnite.
lower bound (of elements x and y in a poset): an element z such that x ≥z and y ≥
z.
lower semimodular lattice: a lattice whose dual is upper semimodular.
majority rule property (for a consensus function): the property that x will be pre-
ferred to y if and only if a majority of the individuals prefer x to y.
maximal element (in a poset): an element such that no other element is greater.
meet (of elements x and y): another name for the greatest lower bound x ∧y.
meet-irreducible element (of a lattice): a nonzero element that cannot be expressed
as the meet of two other elements.
minimal element (in a poset): an element such that no other element is less.
minimum realizer encoding (of a poset): a poset that lists for each element its po-
sition on each extension in a minimum realizer.
modular lattice: a lattice in which x ∧(y ∨z) = (x ∧y) ∨z for all x, y, z such that
z ≤x.
module (in a graph G): a vertex subset U ⊆VG such that each vertex outside U is
adjacent to all or none of the vertices in U.
k-norm (of a sequence a = {ai}): the sum P
i min{k, ai}, whose value is commonly de-
noted mk(a).
k-norm of a chain partition: the k-norm of its sequence of chain sizes.
normalized matching property (for a graded poset): the property stating that for
every rank k and every subset A of rank Pk, the set A∗of elements in the rank Pk+1
that are comparable to at least one element of A satisﬁes the inequality
|A∗|
Nk+1 ≥|A|
Nk .

848
Chapter 11
PARTIALLY ORDERED SETS
order module in a poset: a set S of elements such that every element outside S is
above all of S, below all of S, or incomparable to all of S.
order-preserving mapping (from poset P = (X, R) to poset Q = (Y, S)): a function
f : X →Y such that f(x) ≤f(y) whenever x ≤y in P.
order relation (on a set X): a relation R such that (X, R) is a partially ordered set.
partially ordered set: a pair P = (X, R) consisting of a set X and a relation R that
is reﬂexive, antisymmetric, and transitive.
partition lattice: the poset Πn of partitions of the set [n] = {1, . . ., n}, where π < σ
if π is a reﬁnement of σ.
permutation graph: a graph whose vertices can be placed in 1-1 correspondence with
the elements of a permutation of [n] = {1, . . . , n}, such that vi is adjacent to vj if
and only if the larger of {i, j} comes ﬁrst in the permutation.
planar poset: a poset with a Hasse diagram that has no edge-crossings.
plurality consensus function (on a set of social choice proﬁles): the consensus func-
tion in which the winner(s) is (are) the alternative(s) appearing in the greatest num-
ber of top ranks, after which the winner(s) is (are) deleted and the procedure is
repeated to select the next rank of the consensus ranking, etc.
poset: a partially ordered set.
proﬁle (on a set of alternative social choices): a set P = {Pi | i ∈I} of linear rankings
(ties allowed) of the alternatives, one for each member of a set I of “individuals”
participating in the decision process.
quasi-transitive orientation (on a simple graph G): an assignment of directions to
the edges of G so that whenever there is an xy-arc and a yz-arc, there is also an arc
between x and z.
rank (of a graded poset P): the length r(P) of any maximal chain in P.
rank function (on a poset): an integer-valued function r on the elements of the poset
so that “y covers x” implies that r(y) = r(x) + 1.
ranked poset: a poset having a rank function.
kth rank of a ranked poset: the subset Pk of elements for which r(x) = k.
rank parameters (of a subset F of elements in a ranked poset P): the numbers fk =
|F ∩Pk|.
ranking: a poset P whose elements are partitioned into ranks P1, . . . , Pk such that two
elements are incomparable in the poset if and only if they belong to the same rank.
realizer (of a poset P): a set of linear extensions of P whose intersection is P.
reﬁnement (of a set partition σ): replacement of each block B ∈σ by some partition
of B.
regular covering (of a poset by chains): a multiset of maximal chains such that for
each element x the fraction of the chains containing x is
1
Nr(x) , where Nr(x) is a
Whitney number.
self-dual poset: a poset isomorphic to its dual.
semimodular lattice: an upper semimodular lattice.
semiorder: a poset on which there is a real-valued function f and a real number δ > 0
such that x < y if and only if f(y) −f(x) > δ.

GLOSSARY
849
shadow (of a family of sets F): the collection of sets containing every set that is ob-
tainable by selecting a set in F and deleting one of its elements.
size (of a ﬁnite poset): the number of elements.
Sperner property (for a graded poset): the property that some single rank is a max-
imum antichain.
k-Sperner property (for a graded poset): the property that the poset has a maximum
k-family consisting of k ranks.
standard k-chain: the poset {1, . . . , k}, under the usual ordering of the integers, writ-
ten k.
standard example of an n-dimensional poset: the subposet Sn of the Boolean al-
gebra 2n induced by the singletons and their complements.
strict Sperner property: the property of a graded poset that all maximum antichains
are single ranks.
strong Sperner property: the property that a graded poset P is k-Sperner for all
k ≤r(P).
Steinitz exchange axiom: for a closure operator σ: 2E →2E, the rule that p /∈σ(A)
and p ∈σ(A ∪q) imply q ∈σ(A ∪p).
sublattice (of a lattice): a subposet that contains the meet and join of every pair of its
elements.
submodular height function (in a lattice): a height function h such that h(x ∧y) +
h(x ∨y) ≤h(x) + h(y) for all x, y.
subposet (of a poset (X, R)): a poset (Y, S) such that Y ⊆X and S = R ∩(Y × Y ).
subset lattice: the Boolean algebra 2n, that is, the Cartesian product of n copies of
the standard 2-chain.
subspace lattice: the set Ln(q) of subspaces of an n-dimensional vector space over a
q-element ﬁeld, partially ordered by set inclusion.
symmetric chain (in a ranked poset P): a chain that has an element of rank r(P)−k
whenever it has an element of rank k.
symmetric chain decomposition (of a ranked poset): a partition of that poset into
symmetric consecutive chains.
symmetric chain order: a poset with a symmetric chain decomposition.
topological ordering (of an acyclic digraph): a linear extension of the poset it repre-
sents.
topological sort: an algorithm that arranges the elements of a partially ordered set
into a total ordering that is compatible with the original partial ordering.
total order (of a set): an order relation in which each pair of distinct elements is com-
parable.
transitive orientation (on a simple graph): an assignment of directions to the edges
of a simple graph G so that whenever there is an xy-arc and a yz-arc, there is also
an xz-arc.
triangular chord (for a walk x1, . . . , xk in an undirected graph): an edge between ver-
tices xi−1 and xi+1, two apart on the walk.
upper bound (of elements x and y in a poset): an element z with x ≤z and y ≤z.

850
Chapter 11
PARTIALLY ORDERED SETS
upper semimodular lattice: a lattice in which whenever x covers x∧y, it is also true
that x ∨y covers y.
upset (in a poset): a ﬁlter.
weak order: a ranking, i.e., a poset P whose elements are partitioned into ranks
P1, . . . , Pk such that two elements are incomparable if and only if they belong to
the same rank.
kth Whitney number (of a ranked poset P): the cardinality |Pk| of the kth rank;
written Nk(P).
width (of a poset): the maximum size of an antichain in the poset.
Young lattice: the lattice of integer partitions under component-wise ordering.
poset notation
notation
meaning
y ≥x
x ≤y
x < y
x ≤y and x ̸= y
x||y
x ̸≤y and y ̸≤x
0
minimal element in a bounded poset
1
maximal element in a bounded poset
[x, y]
the interval {z | x ≤z ≤y}
k
standard k-chain
P1 + P2
disjoint union of posets
P1 ⊕P2
linear sum of two posets
P1 × P2
Cartesian product of two posets
P n
iterated Cartesian product of n copies of P
P ∗
dual of poset P
D(n)
divisibility poset of the integer n
r(P)
rank of a graded poset P
Nk(P)
kth Whitney number (= cardinality of kth rank) of P
w(P)
width of P (= maximum size of an antichain)
D[x]
downset (ideal) {y | y ≤x}
D(x)
downset (ideal) {y | y < x}
U[x]
upset (ﬁlter) {y | y ≥x}
U(x)
upset (ﬁlter) {y | y > x}
x ∨y
lub of x and y
x ∧y
glb of x and y

Section 11.1
BASIC POSET CONCEPTS
851
11.1
BASIC POSET CONCEPTS
11.1.1
COMPARABILITY
The integers and the real numbers are totally ordered sets, since every pair of distinct
elements can be compared. In a partially ordered set, some pairs of elements may be
incomparable. For example, under the containment relation, the sets {1, 2} and {1, 3}
are incomparable.
Deﬁnitions:
A partial ordering (or order relation) R on a set X is a binary relation that is
• reﬂexive: for all x ∈S, xRx;
• antisymmetric: for all x, y ∈S, if xRy and yRx, then x = y;
• transitive: for all x, y, z ∈S, if xRy and yRz, then xRz.
Note:
x ≤y or x ≤P y are often written in place of xRy or (x, y) ∈R. Also, y ≥x
means x ≤y. The notation ⪯is sometimes used in place of ≤. See the table following
the glossary for further poset notation.
A partially ordered set (or poset) P = (X, R) is a pair consisting of a set X, called
the domain, and a partial ordering R on X. Writing x ∈P means that x ∈X. The
notation (X, ≤) is often used instead of (X, R) to designate a poset.
The size of a ﬁnite poset P is the number of elements in the domain.
The elements x and y are comparable (related) in P if either x ≤y or y ≤x (or both,
in which case x = y).
A totally ordered set (or linearly ordered set) is a poset in which every element is
comparable to every other element.
The elements x and y are incomparable (unrelated) if they are not comparable. Writ-
ing x ∥y indicates incomparability.
Element x is less than element y, written x < y, if x ≤y and x ̸= y. (The notation ≺
is sometimes used in place of <.)
Element x is greater than element y, written x > y, if x ≥y and x ̸= y.
An element x of a poset is minimal if the poset has no element less than x.
An element x of a poset is maximal if the poset has no element greater than x.
A poset is bounded if it has both a unique minimal element (denoted “0”) and a unique
maximal element (denoted “1”).
The comparability digraph D(P) of a poset P = (X, R) is the digraph with vertex
set X, such that there is an arc from x to y if and only if x ≤y.
The comparability graph G(P) of a poset P = (X, R) is the simple graph with vertex
set X, such that xy ∈EG if and only if x and y are comparable in P, where x ̸= y.

852
Chapter 11
PARTIALLY ORDERED SETS
The incomparability graph of a poset P is the edge-complement of the comparability
graph G(P).
The induced poset of an acyclic digraph D is the poset whose elements are the vertices
of D and such that x ≤y if and only if there is a directed path from x to y.
The element y covers the element x in a poset if x < y and there is no intermediate
element z such that x < z < y.
The cover graph of poset P is the graph with vertex set X such that x and y are
adjacent if and only if one of them covers the other in P.
A Hasse diagram (or cover diagram or diagram) of poset P is a straight-line drawing
of the cover graph in the plane such that the lesser element of each pair satisfying the
cover relation is lower in the drawing.
A poset is planar if it has a Hasse diagram without edge-crossings.
A subposet of P = (X, ≤) is a subset Y ⊆X with the relation x ≤y in Y if and only
if x ≤y in X.
The interval [x, y] in poset P is the subposet that contains all elements z such that
x ≤z ≤y.
A poset P is locally ﬁnite if every interval in P has ﬁnitely many elements.
An order-preserving mapping from poset P = (X, ≤P ) to poset Q = (Y, ≤Q) is a
function f : X →Y such that x ≤P x′ implies f(x) ≤Q f(x′).
An isomorphism of posets P = (X, R) and Q = (Y, S) is a bijection f : X →Y that
preserves the order relation: whenever x1 ≤P x2, then f(x1) ≤Q f(x2).
Isomorphic posets are posets P = (X, R) and Q = (Y, S) such that there is a poset
isomorphism P →Q. This is sometimes indicated informally by writing P = Q.
Poset Q = (Y, S) is contained in (or imbeds in) poset P = (X, R) if Q is isomorphic
to a subposet of P.
A poset P is Q-free if P does not contain a poset isomorphic to Q.
Facts:
1. Every ﬁnite nonempty poset has a minimal element and a maximal element.
2. The comparability digraph D(P) of a poset P is an acyclic digraph.
3. The minimal elements of a poset P induced by a digraph D are the sources of D;
that is, they are the vertices at which every arc points outward.
4. The maximal elements of a poset P induced by a digraph D are the sinks of D; that
is, they are the vertices at which every arc points inward.
5. The element y covers the element x in a poset P induced by a digraph D if and only
if there is an arc in digraph D from x to y and there is no other directed path from x to
y.
6. Suppose that the poset P is induced from an acyclic digraph D. Then the compara-
bility digraph of P is the transitive closure of D.
7. Two diﬀerent posets cannot have the same Hasse diagram, but they may have the
same cover graph or the same comparability graph.
8. There is a polynomial-time algorithm to check whether a graph G is a comparability
graph, but the problem of deciding whether there exists a poset for which G is the cover
graph is NP-complete.

Section 11.1
BASIC POSET CONCEPTS
853
Examples:
1. Any collection of subsets of the same set forms a poset when the subsets are partially
ordered by the usual inclusion relation X ⊆Y .
2. Boolean algebra:
The Boolean algebra on a set X is the poset consisting of all the
subsets of X, ordered by inclusion.
3. The Boolean algebra on the set {a, b, c} has the following Hasse diagram. The only
maximal element is {a, b, c}. The only minimal element is ∅.
{a,b}
{a,c}
{a,b,c}
{b,c}
{c}
{b}
{a}
ф
4. There are ﬁve diﬀerent isomorphism types of posets of size three, whose Hasse dia-
grams are as follows.
a
b
c
a
b
c
b
a
c
a
b
a
b
c
5. There are 16 diﬀerent isomorphism types of posets of size four, whose Hasse diagrams
are as follows.
a
b
c
d
a c
d
b
b
a
c
d
c
d
b
a
a
c
d
b
c
d
b
a
c
c
b
d
a
d
c
b
a
b
c
d
a
b
d
c
a
b
c
d
b
a
c
d
b
a
a
c
b
d
a
b
c
d
c
d
a
a
b
d
6. Divisibility poset:
The divisibility poset on the set I of positive integers, de-
noted D(I), has the relation x ≤y if y is an integer multiple of x. A number y covers a
number x if and only if the quotient y
x is prime.
7. The set D(n) of divisors of n forms a subposet of D(I), for any positive integer n.
The set D(n) is identical to the interval [1, n] in D(I). For instance, the following ﬁgure
is the Hasse diagram of D(24) = [1, 24].
8
4
2
1
3
6
12
24

854
Chapter 11
PARTIALLY ORDERED SETS
8. The interval [3, 30] in D(I) has domain {3, 6, 12, 15, 30}.
The interval [2, 24] has
domain {2, 4, 6, 8, 12, 24}.
9. The poset D(I) is inﬁnite, but locally ﬁnite.
10. The Boolean algebra of all subsets of an inﬁnite set is not a ﬁnite poset. Nor is it
locally ﬁnite, since each interval from a ﬁnite set to an inﬁnite superset is inﬁnite.
11. The poset D(6) is isomorphic to the poset of subsets of {a, b}, as seen next.
6
3
1
2
{a}
{b}
ø
{a,b}
12. To generalize Example 11, if p1, . . . , pn are distinct primes, then the divisibility
poset D(p1 . . . pn) is isomorphic to the poset of subsets of a set of n objects.
13. The partitions of a set form a poset under reﬁnement, as illustrated for the set
{a, b, c, d}. A notation such as ab-c-d means a partition of the set {a, b, c, d} into the
subsets {a, b}, {c}, {d}.
a-b-c-d
ab-c-d
abc-d
ab-cd
abd-c
ac-bd
acd-b
ad-bc
a-bcd
abcd
ac-b-d
ad-b-c
a-bc-d
a-bd-c
a-b-cd
14. The 6-cycle is the comparability graph of exactly one (isomorphism type of) poset,
which has the following Hasse diagram.
15. The 6-cycle is the cover graph of seven posets, all of which are planar. They have
the following Hasse diagrams.
11.1.2
CHAINS, ANTICHAINS, AND POSET OPERATIONS
Deﬁnitions:
A chain is a subset S of mutually comparable elements of a poset P, or sometimes the
subposet of P formed by such a subset.

Section 11.1
BASIC POSET CONCEPTS
855
The length of a ﬁnite chain C is |C| −1, i.e., the number of edges in the Hasse diagram
of that chain, regarded as a poset.
A k-chain is a chain of size k, i.e., a chain on k elements.
The standard k-chain k is a ﬁxed k-chain, presumed to be disjoint from other objects
in the universe of discourse.
The height of a poset P is the maximum size of a chain in P.
The height of an element x in a poset P is the maximum length h(x) of a chain in P
that has x as its maximal element.
A bipartite poset is a poset of height at most 2.
A chain-product (or grid) is the Cartesian product of a collection of chains.
An antichain (or clutter or Sperner family) is a subset S of pairwise incomparable
elements of a poset P, or sometimes the subposet of P formed by such a subset.
A chain or antichain is maximal if it is contained in no other chain or antichain.
A chain or antichain in a ﬁnite poset is a maximum chain or antichain if it is one of
maximum size.
The disjoint union of two posets P = (X, R) and P ′ = (X′, R′) with X ∩X′ = ∅is the
poset (X ∪X′, R ∪R′), denoted P + P ′.
The linear sum of two posets P = (X, R) and P ′ = (X′, R′) with X ∩X′ = ∅is the
poset (X ∪X′, R ∪R′ ∪(X × X′)), denoted P ⊕P ′. (This puts all of poset P “below”
poset P ′.)
The Cartesian product P × P ′ (or direct product or product) of two posets P =
(X, R) and P ′ = (X′, R′) is the poset (X × X′, S), such that (x, x′)S(y, y′) if and only if
xRy and x′R′y′.
The iterated Cartesian product of n copies of a poset P = (X, ≤), written P n, is the
set of n-tuples in P, such that (x1, . . . , xn) ≤(y1, . . . , yn) if and only if xj ≤yj for all
j = 1, . . . , n.
The lexicographic ordering of the Cartesian product P1 × P2 of the domains of two
posets is the partial ordering in which (x1, x2) ≤(y1, y2) if and only if x1 < y1, or x1 = y1
and x2 ≤y2.
The dual of a poset P, denoted P ∗, is the poset on the elements of P deﬁned by the
relation y ≤P ∗x if and only if x ≤P y.
A self-dual poset is a poset that is isomorphic to its dual.
Facts:
1. Every k-chain is isomorphic to the linear sum 1 ⊕1 ⊕· · · ⊕1 of k copies of 1.
2. Every antichain of size k is isomorphic to the disjoint union 1 + 1+ · · ·+ 1 of k copies
of 1.
3. The chains are characterizable as the class of (1 + 1)-free posets.
4. The cover graph of a chain is a path.
5. The comparability graph of a chain of size n is the complete graph Kn.
6. The antichains are the class of 2-free posets.

856
Chapter 11
PARTIALLY ORDERED SETS
7. The comparability graph of an antichain has no edges.
8. The maximum size of a chain in a ﬁnite poset P equals the minimum number of
antichains needed to cover the elements of P, that is, the minimum number of antichains
whose union equals the domain of poset P.
9. The bipartite posets are precisely the 3-free posets.
10. The bipartite posets are the posets whose comparability graph and cover graph are
the same.
11. Every maximal chain of a ﬁnite poset P extends from a minimal element of P to a
maximal element of P, and successive pairs on a maximal chain satisfy the cover relation
of P.
12. The Cartesian product of two posets is a poset.
13. A poset and its dual have the same comparability graph and the same cover graph.
14. The Hasse diagram of the dual of a poset P can be obtained from the Hasse diagram
of P either by reﬂecting through the horizontal axis or by rotating 180 degrees.
15. The set of order-preserving maps from a poset P to a poset Q forms a poset, denoted
by QP , under “coordinate-wise ordering”: f ≤g in QP if and only if f(x) ≤Q g(x) for
all x ∈P.
Examples:
1. The following ﬁgure shows: (a) a 3-chain, (b) an antichain of width 4, and (c) a
bipartite poset.
(a)
(b)
(c)
2. The poset 23 is not planar, even though it has a planar cover graph. However, deleting
its minimal element or maximal element leaves a planar subposet.
1
1
0
23
23-0
0
23-1
3. The cover graph of the poset 2n is isomorphic to the n-dimensional cube, whose
vertices are the bitstrings of length n, with bitstrings adjacent if they diﬀer in one
position. Each bit encodes the possible presence of an element of the set of which Bn is
the Boolean algebra (§5.8.1).
4. The interval in the Boolean algebra 2n between an element of rank k and an element
of rank l ≥k is isomorphic to the poset 2l−k.
5. Every maximal chain in the Boolean algebra 2n has size n + 1 and length n, and
there are n! such chains. There are maximal antichains as small as one element.
6. In general, the poset D(n) is isomorphic to a chain product, one factor for each prime
divisor of n. The elements of D(n) can be encoded as integer vectors {a1, . . . , an | 0 ≤
ai < ei}, where n is a product of distinct primes with powers e1, . . . , en, and a ≤b if and
only if ai ≤bi for all i.

Section 11.1
BASIC POSET CONCEPTS
857
7. The Hasse diagrams for two possible partial orderings on the Cartesian product of
the domains of two posets are shown in the following ﬁgure.
a
c
b
y
x
bx
by
Q
P
PxQ
PxQ, lex
ax
cx
ay
cy
bx
by
ax
ay
cx
cy
8. The two posets M5 = 1 ⊕(1 + 1 + 1) ⊕1 and N5 = 1 ⊕(2 + 1) ⊕1 are used in §11.1.5
in a forbidden subposet description.
M5
N5
9. The following posets are self-dual.
a
b
e
b
a
c
d
e
c
d
11.1.3
RANK, IDEALS, AND FILTERS
Deﬁnitions:
A graded poset is a poset in which all maximal chains have the same length.
The rank r(P) of a graded poset P is the length of any maximal chain.
A rank function r on a poset P is an assignment of integers to the elements so that
the relation y covers x implies that r(y) = r(x) + 1.
A ranked poset is a poset having a rank function.
The kth rank of a ranked poset P is the subset Pk of elements x for which r(x) = k.
The kth rank parameter of a subset of elements F in a ranked poset P is the cardinality
|F ∩Pk| of the number of elements of F in the kth rank of P.
The kth Whitney number Nk(P) of a ranked poset P is the cardinality |Pk| of the kth
rank.
The length of a poset P is the length of a longest chain in P, which is one less than
the height of P. Note: Sometimes “height” is used synonymously with length.
The Jordan-Dedekind chain condition for a poset is that every interval has ﬁnite
length.
The width of a poset P, denoted w(P), is the maximum size of an antichain in P.

858
Chapter 11
PARTIALLY ORDERED SETS
An ideal (or downset, order ideal, or hereditary family) in a poset P is a subposet I
such that if x ∈I and y < x, then y ∈I.
A ﬁlter (or upset or dual ideal) in a poset P is a subposet F whose domain is the
set-theoretic complement of the domain of an ideal.
The ideal generated by an element x in a poset P is the downset D[x] = {y ∈P |
y ≤x}. The related notation D(x) means the downset {y ∈P | y < x}.
The ideal generated by a subset A in a poset P is the downset D[A] = S
x∈A D[x].
The related notation D(A) means the downset S
x∈A D(x).
The ﬁlter generated by an element x in a poset P is the upset U[x] = {y ∈P | y ≥x}.
The related notation U(x) means the upset {y ∈P | y > x}.
The ﬁlter generated by a subset A in a poset P is the upset U[A] = S
x∈A U[x]. The
related notation U(A) means the upset S
x∈A U(x).
A forbidden subposet description of a class of posets is a characterization of that
class as the class of all posets that does not contain any of the posets in a speciﬁed
collection. (This generalizes the concept of Q-free.)
Facts:
1. A graded poset has a rank function, in which the rank of each element is deﬁned to
be its height.
2. If posets P1, P2 have rank functions r1, r2, then the Cartesian product P = P1 × P2
is ranked, so that the element x = (x1, x2) has rank r(x) = r1(x1) + r2(x2).
3. In a Cartesian product of ﬁnite ranked posets P1 and P2, the Whitney numbers for
the Cartesian product P = P1 × P2 satisfy the equation Nk(P) = P
i Ni(P1)Nk−i(P2).
4. The Boolean algebra on a set X of cardinality n is isomorphic to 2n, the Cartesian
product of n copies of 2. This poset isomorphism type is often denoted Bn.
5. The Boolean algebra on a set X of cardinality n is a graded poset, with rank function
r(S) = |S|, and with Whitney numbers Nk(2n) =
 n
k

.
6. The sequence of Whitney numbers on the Boolean algebra on a set X of cardinal-
ity n is symmetric, since
 n
k

=
  n
n−k

.
It is also unimodal, since the sequence rises
monotonically to the maximum and then falls monotonically.
7. Sperner’s theorem: The only maximum antichains in the Boolean algebra 2n are the
middle ranks (one such rank if n is even, two if n is odd). Thus the width of 2n is
 n
⌊n/2⌋

.
8. The maximal elements of an ideal form an antichain, as do the minimal elements of
a dual ideal; these yield natural bijections between the set of antichains in a poset P and
the sets of ideals or dual ideals of P.
9. The divisibility poset D(I) on the integers satisﬁes the Jordan-Dedekind chain con-
dition.
Examples:
1. In the poset P of partitions of {a, b, c, d} under inverse reﬁnement, illustrated next,
the Whitney numbers are N1(P) = 1, N2(P) = 6, N3(P) = 7, and N4(P) = 1.
2. In the poset of partitions of {a, b, c, d} under inverse reﬁnement, the ideal D[ac-bd] is
the set {ac-bd, a-bd-c, ac-b-d, a-b-c-d}, and the ideal D(ac-bd) is the set {a-bd-c, ac-b-d,
a-b-c-d}.

Section 11.1
BASIC POSET CONCEPTS
859
a-b-c-d
ab-c-d
abc-d
ab-cd
abd-c
ac-bd
acd-b
ad-bc
a-bcd
abcd
ac-b-d
ad-b-c
a-bc-d
a-bd-c
a-b-cd
3. In the poset of partitions of {a, b, c, d} under inverse reﬁnement, the ﬁlter U[a-bd-c]
is the set {a-bd-c, abd-c, ac-bd, a-bcd, abcd}, and the ﬁlter U(a-bd-c) is the set {abd-c,
ac-bd, a-bcd, abcd}.
4. In the graded poset D(n) of divisors of n, the rank r(x) is the sum of the exponents
in the prime power factorization of x. The Whitney numbers of D(n) are symmetric
because the divisors x and n
x have “complementary” ranks. If n is a product of k distinct
primes, then D(n) ∼= 2k.
5. The following Hasse diagram corresponds to an ungraded poset, because the lengths
of its maximum chains diﬀer, i.e., they are length 2 and length 3.
6. In the divisibility poset D(I), the subposet D(n) of divisors of n is a ﬁnite ideal, and
the non-multiples of n form an inﬁnite ideal, whose complement is the inﬁnite ﬁlter U(n)
of numbers that are divisible by n.
11.1.4
LATTICES
Lattices are posets with additional properties that capture some aspects of the intersec-
tion and the union of sets and (more generally) of the greatest common divisor and least
common multiple of positive integers. (See also §5.7.)
Deﬁnitions:
A (common) upper bound for elements x, y in a poset is an element z such that x ≤z
and y ≤z.
A least upper bound (or lub, pronounced “lub”) for elements x, y in a poset is a
common upper bound z such that every other common upper bound z′ satisﬁes the
inequality z ≤z′. Such an element, if it exists, is denoted x ∨y.
The join of x and y is the lub x ∨y.
A (common) lower bound for elements x, y in a poset is an element z such that x ≥z
and y ≥z.
A greatest lower bound (or glb, pronounced “glub”) for elements x, y in a poset is
a common lower bound z such that every other common lower bound z′ satisﬁes the
inequality z ≥z′. Such an element, if it exists, is denoted x ∧y.

860
Chapter 11
PARTIALLY ORDERED SETS
The meet of x and y is the glb x ∧y.
A lattice is a poset in which every pair of elements has both a lub and a glb.
A lattice is bounded if it has both a unique minimal element (denoted “0”) and a unique
maximal element (denoted “1”).
A nonzero element of a lattice L is join-irreducible (or simply irreducible) if it can-
not be expressed as the lub of two other elements. The subposet formed by the join-
irreducible elements of L is denoted by P(L).
A nonzero element of a lattice L is meet-irreducible if it cannot be expressed as a glb
of two other elements. The subposet formed by the meet-irreducible elements of L is
denoted by Q(L).
A complement of an element x of a lattice is an element x such that x ∨x = 1 and
x ∧x = 0.
A complemented lattice is a lattice in which every element has a complement.
A lattice isomorphism is an order-preserving bijection from one lattice to another that
also preserves glbs and lubs.
An atom of a poset is an element of height 1.
A lattice is atomic if every element is a lub of atoms (or equivalently, if the atoms are
the only join-irreducible elements).
A sublattice of a lattice L is a subposet P such that x ∧y and x ∨y are in P for all x
and y ∈P.
The divisor lattice is the poset D(n) of the positive integer divisors of n, in which
x ≤y means that y is an integer multiple of x.
The subset lattice is the Boolean algebra 2n, that is, the Cartesian product of n copies
of the standard 2-chain.
The subspace lattice Ln(q) is the set of subspaces of an n-dimensional vector space
over a q-element ﬁeld, partially ordered by set inclusion.
The lattice of (order) ideals J(P), for any poset P = (X, R), is the set of order ideals
of P, ordered by inclusion.
The lattice of bounded sequences L(m, n) has as members the length-m real se-
quences a1, . . . , am such that 0 ≤a1 ≤· · · ≤am ≤n.
An integer partition is a nonincreasing nonnegative integer sequence having ﬁnitely
many nonzero terms, with trailing zeros added as needed for comparison.
The Young lattice is the lattice of integer partitions under component-wise ordering.
A reﬁnement of a set partition σ replaces each block B ∈σ by some partition of B.
The partition lattice is the poset Πn of partitions of the set [n] = {1, . . ., n}, where
π < σ if π is a reﬁnement of σ.
Facts:
1. If z ≤x in a lattice, then x ∧(y ∨z) ≥(x ∧y) ∨z.
2. 4-point lemma:
If each of the elements z, w is less than or equal to each of the
elements x, y in a lattice, then z ∨w ≤x ∧y.

Section 11.1
BASIC POSET CONCEPTS
861
3. An element z is a least upper bound for x and y if and only if it is a unique minimal
element among their common upper bounds.
4. Every ﬁnite lattice is bounded.
5. Every chain-product is a lattice.
6. If a locally ﬁnite poset P with a unique maximal element 1 also has a well-deﬁned glb
operation, then P is a lattice.
7. Not all lattices are ranked.
In particular, the lattice of integer partitions under
dominance ordering is unranked.
8. Every interval in a lattice is a sublattice, but not every sublattice is an interval.
9. In the subspace lattice Ln(q), the Whitney numbers (§11.1.3) satisfy the equation
Nk(Ln(q)) = (qn −1)(qn−1 −1) . . . (qn−k+1 −1)
(qk −1)(qk−1 −1) . . . (q1 −1)
.
10. In the subspace lattice Ln(q), the Whitney number Nk(Ln(q)) equals the Gaussian
coeﬃcient
n
k

q (§2.3.2), which appears in algebraic identities and in analogues of results
on subsets.
11. The Whitney number Nk(Πn) of partitions of the set [n] into n −k blocks is the
Stirling subset number
 n
n−k
	
(§2.5.2). This has no closed formula, but the inclusion-
exclusion principle yields
n
t
	
= Pt
i=0(−1)i (t−i)n
t!
.
Examples:
1. The poset speciﬁed by the following Hasse diagram is a lattice.
a
d
b
c
f
0
e
1
2. The poset speciﬁed by the following Hasse diagram is not a lattice. Although every
pair of elements has a common upper bound, none of the three common upper bounds
for the elements c and d is a least upper bound.
1
b
d
0
c
a
3. Two 5-element lattices that occur as subposets of 23 but not as sublattices of 23 are
M5 = 1 ⊕(1 + 1 + 1) ⊕1 and N5 = 1 ⊕(2 + 1) ⊕1.
4. In the divisor lattice D(n), a ∧b = gcd(a, b) and a ∨b = lcm(a, b).
5. The join-irreducible elements of the divisibility lattice D(I) are the powers of primes.

862
Chapter 11
PARTIALLY ORDERED SETS
6. In the subset lattice 2n, a ∧b = a ∩b and a ∨b = a ∪b.
7. The join-irreducible elements of the subset lattice 2n are the singleton sets.
8. The subspace lattice Ln(q) is a graded lattice, with rank r(U) = dim U.
9. In the subspace lattice Ln(q), the meet of subspaces U and V is their intersection
U ∩V , and the join is the unique minimal subspace containing their union.
10. In the lattice of order ideals J(P), glb and lub are given by intersection and union.
Hence J(P) is a sublattice of the Boolean lattice 2|P |; equality holds if and only if P is
an antichain.
11. The lattice J(P) of ideals of a poset P = (X, R) is ﬁnite, with 1J(P ) = X and
0J(P ) = ∅. It is graded, with rank function r(I) = |I|.
12. By the correspondence between ideals of a poset P = (X, R) and their antichains
of maximal elements, the lattice J(P) of ideals is also a lattice on the antichains of P.
The corresponding ordering on antichains is A ≤B if every element of A is less than or
equal to some element of B.
13. The lattice L(m, n) of bounded sequences is a sublattice of n + 1m, and L(m, n) =
J(m × n) = L(n, m). The natural isomorphism maps a sequence a ∈L(m, n) to the
order ideal of m × n generated by {(m + 1 −i, ai) | ai > 0}.
14. The lattice L(m, n) is a sublattice of the Young lattice.
15. In the partition lattice Πn, 13|4|2|5 < 123|45; the order of the blocks and the order
of elements within each block are irrelevant.
16. The partition lattice Πn is a graded poset, with 1Πn = [n] and 0Πn = 1|2| · · · |n.
The common reﬁnement of π and σ with the fewest blocks is the greatest lower bound
(meet) of π and σ.
17. The lattice Π3 is isomorphic to the lattice M5.
18. In the ordering on antichains of a poset P deﬁned by A ≤B if every element
of A is less than or equal to some element of B, the maximum antichains of P induce a
sublattice.
11.1.5
DISTRIBUTIVE AND MODULAR LATTICES
Deﬁnitions:
A lattice L is distributive if glb distributes over lub in L, that is, if x ∧(y ∨z) =
(x ∧y) ∨(x ∧z) for all x, y, z ∈L.
A lattice L is modular if x ∧(y ∨z) = (x ∧y) ∨z for all x, y, z ∈L such that z ≤x.
A lattice L is (upper) semimodular if for all x, y ∈L, “x covers x ∧y” implies “x ∨y
covers y”.
A lattice L is lower semimodular if the reverse implication holds (equivalently, if the
dual lattice L∗is semimodular).
The height function h of a lattice L is a submodular height function if h(x ∧y) +
h(x ∨y) ≤h(x) + h(y) for all x, y ∈L.
A lattice is geometric if it is atomic, semimodular, and has ﬁnite height.
A closure operator on the subsets of a set E is a function σ: 2E →2E that maps
each set to a superset of itself, is order-preserving with respect to set inclusion, and is
“idempotent”: σ(σ(A)) = σ(A).

Section 11.1
BASIC POSET CONCEPTS
863
The closed subsets of a set E, with respect to a closure operator σ: 2E →2E, are the
sets with σ(A) = A.
The Steinitz exchange axiom for a closure operator σ: 2E →2E is the rule that
p /∈σ(A) and p ∈σ(A ∪q) imply q ∈σ(A ∪p).
Facts:
1. The smallest nondistributive lattices are M5 = 1⊕(1+1+1)⊕1 and N5 = 1⊕(2+1)⊕1,
which are illustrated in §11.1.2, Example 8.
2. A lattice is distributive if and only if it occurs as a sublattice of 2n for some n.
3. Every sublattice of a distributive lattice is a distributive lattice.
4. The product of distributive lattices L1 and L2 is a distributive lattice, with (x1, x2)∧
(y1, y2) = (x1 ∧y1, x2 ∧y2) and (x1, x2) ∨(y1, y2) = (x1 ∨y1, x2 ∨y2).
5. In a lattice L, distributivity and the dual property that x ∨(y ∧z) = (x ∨y) ∧(x ∨z)
for all x, y, z ∈L are equivalent. Hence the dual of a distributive lattice is a distributive
lattice.
6. A lattice L is modular if and only if c ∈[a∧b, a] implies a∧(b∨c) = c for all a, b ∈L
(equivalently, if c ∈[b, b ∨d] implies b ∨(c ∧d) = d for all b, d ∈L).
7. Let µa : L →L be the operation “take the glb with a”, and let νb : L →L be the
operation “take the lub with b”. A lattice L is modular if and only if for all a, b ∈L, the
intervals [a∧b, a] and [b, a∨b] are isomorphic sublattices of L, with lattice isomorphisms
given by νb and µa.
8. If y covers x in a semimodular lattice L, then for all z ∈L, x ∨z = y ∨z or x ∨z is
covered by y ∨z.
9. A lattice L with a lower bound is semimodular if and only if the following is true:
the height function of L is submodular and in each interval the maximal chains all have
the same length.
10. A lattice is modular if and only if it does not have N5 as a sublattice.
11. Every distributive lattice is modular, because in a distributive lattice x ∧(y ∨z) =
(x ∧y) ∨(x ∧z) = (x ∧y) ∨z if z ≤x.
12. A modular lattice is distributive if and only if it does not have M5 as a sublattice.
13. Given a closure operator, the closed sets form a lattice under inclusion with meet
and lub given by intersection and closure of the union, respectively.
14. If a closure operator σ satisﬁes the Steinitz exchange axiom, then the lattice of
closed sets is semimodular.
15. The lattice Ln(q) is semimodular. (This follows from the previous fact.)
16. A poset is a geometric lattice if and only if it is the lattice of closed sets of a
matroid, ordered by inclusion (§12.4). (The span operator in a matroid, which adds
to X every element whose addition to X does not increase the rank, is a closure operator
that satisﬁes the Steinitz exchange axiom.)
17. A geometric lattice is distributive if and only if it has the form 2n, and the corre-
sponding matroid is the free matroid, in which all subsets of the elements are independent.
18. A complemented distributive lattice is a Boolean algebra.
Examples:
1. Among nondistributive lattices, the lattice M5 is modular, and the lattice N5 is not
(which explains the notation).

864
Chapter 11
PARTIALLY ORDERED SETS
2. The subspace lattices Ln(q) are not distributive.
3. The partition lattice Πn is semimodular but not modular for n > 3. The lattice Ln(q)
is semimodular.
4. The partition lattice Πn is geometric, and it is the lattice of closed sets of the cycle
matroid of the complete graph Kn.
5. For n ≥3, the lattice Π(n) is not distributive.
6. The Boolean lattice 2n, the divisor lattice D(N), the lattice J(P) of order ideals of
a poset, and the bounded sequence lattice L(m, n) are distributive.
11.2
POSET PROPERTIES
11.2.1
POSET PARTITIONS
Deﬁnitions:
A chain partition of a poset is a partition of the domain of that poset into chains.
The k-norm of a sequence x = {xi} of real numbers is the sum P
i min{k, xi}, whose
value is commonly denoted mk(x).
The k-norm of a chain partition C of a poset, denoted mk(C), means the k-norm of
the sequence of sizes of the chains in the partition.
A k-family in a poset P is a subposet containing no chain of size k + 1. The size of a
maximum k-family in P is denoted by dk(P).
A partition of a poset P into chains is k-saturated if mk(C) = dk(P).
A chain in a ranked poset is symmetric if it has an element of rank r(P) −k whenever
it has an element of rank k.
A chain is consecutive if its elements belong to consecutive ranks.
A symmetric chain decomposition of P is a partition of P into symmetric consecutive
chains.
A symmetric chain order is a poset with a symmetric chain decomposition.
A graded poset has the Sperner property if some single rank is a maximum antichain.
A graded poset has the strict Sperner property if all maximum antichains are single
ranks.
A poset P has the k-Sperner property if it has a maximum k-family consisting of k
ranks.
A poset has the strong Sperner property if it is k-Sperner for all k ≤r(P).
A graded poset P satisﬁes the normalized matching property if for every k and
every subset A of Pk, the set A∗of elements in Pk+1 that are comparable to at least
one element of A satisﬁes the inequality
|A∗|
Nk+1 ≥|A|
Nk , where Nk and Nk+1 are Whitney
numbers.

Section 11.2
POSET PROPERTIES
865
A regular covering by chains is a multiset of maximal chains such that for each x ∈P
the fraction of the chains containing x is
1
Nr(x) .
To obtain the bracket representation of a subset S of [n] = {1, . . . , n}, ﬁrst represent
the subset S as a length-n “parenthesis-vector”, in which the jth bit is a right parenthesis
if j ∈S and a left parenthesis if j /∈S. Then wherever possible, recursively, match a left
parenthesis to the nearest unmatched right parenthesis that is separated from it only by
previously matched entries.
An on-line partitioning algorithm processes the elements of a poset as they are “re-
vealed”. Once an element is assigned to a cell, it remains there; there is no backtracking
to change earlier decisions.
Facts:
1. Dilworth’s theorem: If P is a ﬁnite poset, then the width of P equals the minimum
number of chains needed to cover the elements of P.
2. Dilworth’s theorem also holds for inﬁnite posets of ﬁnite width.
3. The 1-families are the antichains.
4. Every k-family is a union of k antichains.
5. A k-family in P can be transformed into an antichain in P × k of the same size, and
vice versa, and hence dk(P) = w(P × k).
6. The discussion of saturated partitions is generally restricted to ﬁnite posets.
7. If αk is the number of chains of size at least k in a k-saturated chain partition of P,
then ∆k(P) ≥αk ≥∆k+1(P), where ∆k(P) = dk(P) −dk−1(P) for k ≥1.
8. Littlewood-Oﬀord problem:
Let A = {a1, . . . , an} be a set of vectors in Rd, with
each vector having length at least 1. Let R1, . . . , Rk be regions in Rd of diameter at most
1. Then of the 2n subsets of A, the number whose sum lies in S
i Ri is at most dk(2n).
9. Greene-Kleitman (GK) theorem: For every ﬁnite poset P and every k ≥0, there is
a chain partition of P that is both k-saturated and (k + 1)-saturated.
10. The GK theorem is best possible, since there are inﬁnitely many posets for which
no chain partition is both k-saturated and l-saturated for any nonconsecutive nontrivial
values for k, l; the smallest has six elements (see the following illustration). The GK
theorem extends in various ways to directed graphs.
1
b
d
0
c
a
11. Dilworth’s theorem is the special case of the GK theorem for k = 0 (every chain
partition is 0-saturated).
12. Every product of symmetric chain orders is a symmetric chain order.
13. The lattice of bounded sequences L(m, n) (§11.1.4) has a symmetric chain decom-
position if min{m, n} ≤4. It is not known whether L(m, n) in general has a symmetric
chain decomposition.

866
Chapter 11
PARTIALLY ORDERED SETS
14. The lattice L(m, n) has the Sperner property.
15. The partition lattice Π(n) fails to satisfy the Sperner property if n is suﬃciently
large.
16. The Boolean lattice 2n and the subspace lattice Ln(q) satisfy the strict Sperner
property.
17. Every symmetric chain order has the strong Sperner property, and a symmetric
chain decomposition is k-saturated for all k.
18. The class of graded posets that have the strong Sperner property and a symmetric
unimodal sequence of Whitney numbers is closed under Cartesian product.
19. When Nk ≤Nk+1, the normalized matching property implies Hall’s condition for
the existence of a matching saturating Pk in the bipartite graph of the relations between
the two levels.
20. Two subsets of the Boolean lattice in [n] are on the same chain of the “bracketing
decomposition” if and only if they have the same bracketing representation. This provides
an explicit symmetric chain decomposition of 2n. This generalizes for multisets (D(N)).
21. Dedekind’s problem:
This is the problem of computing the total number of an-
tichains in the Boolean algebra 2n. By using the bracketing decomposition, this number
is calculated to be at most 3(
n
⌊n/2⌋). Asymptotically, for even n, the number is
2( n
n/2)e(
n
n/2−1)[2−n/2 + n22−n−5 −n2−n−4(1 + o(1))].
The exact values for n ≤7 are 3; 6; 20; 168; 7,581; 7,828,354; and 2,414,682,040,998.
The stated estimate gives 7,996,118 for n = 6.
22. Universal set sequences:
A universal set sequence on a set S is a sequence that
contains every subset of S as a consecutive subsequence. The bracketing decomposition
yields a universal set sequence on [n] of length asymptotic to 2
π2n.
23. If two sets, x and y, are chosen independently according to a probability distribu-
tion on the Boolean lattice 2n, then the probability that x is contained in y is at least
 n
⌊n/2⌋
−1.
24. There is an on-line algorithm that partitions posets of height k into
 k+1
2

antichains.
This is best possible, even for 2-dimensional posets.
25. There is an on-line algorithm that partitions posets of width k into 5k−1
4
chains.
26. It is impossible to design an on-line algorithm that partitions every poset of width
k into fewer than
 k+1
2

chains.
27. There is an on-line algorithm to partition every poset of width 2 into ﬁve chains,
and this is best possible.
Examples:
1. A symmetric chain decomposition of the Boolean lattice 23 is shown next.
2. A regular chain covering of the Boolean lattice 23 is shown next.

Section 11.2
POSET PROPERTIES
867
3. The poset 3 × 4 has a regular covering by six chains, using two chains twice and two
other chains once each.
4. The poset 3 × 4 satisﬁes the Sperner property but not the strict Sperner property,
since it has a maximum antichain (size three) that is not conﬁned to a single rank.
5. Dilworth’s theorem can be used to demonstrate the following result: Any sequence
of n2 + 1 real numbers contains a monotone subsequence of length n + 1 (Erd˝os and
Szekeres, 1935).
Let the given sequence be a1, a2, . . . , an2+1.
Deﬁne the poset P =
{(k, ak) | k = 1, . . . , n2 + 1}, with (i, ai) ≤(j, aj) ⇔i < j and ai ≤aj. Assume then
that the given sequence does not contain a monotone nondecreasing subsequence of length
n + 1. Since a chain in P corresponds to a monotone nondecreasing subsequence, each
chain can contain at most n elements, and so we require at least n + 1 chains to cover
P. Since an antichain corresponds to a monotone nonincreasing subsequence, Dilworth’s
theorem assures us that there exists an antichain of size at least n+1 and so a monotone
nonincreasing subsequence of length at least n + 1.
11.2.2
LYM PROPERTY
Deﬁnitions:
The LYM inequality for a family F in a ranked poset P is the inequality P
x∈F
1
Nr(x) ≤1,
where Nr(x) is a Whitney number.
A poset P is an LYM order (or satisﬁes the LYM property) if every antichain F ⊆P
satisﬁes the LYM inequality.
Facts:
1. The LYM property was discovered independently for 2n by Lubell, Yamamoto, and
Meshalkin.
2. The LYM property, the normalized matching property, and the existence of a regular
covering by chains are equivalent.
3. The LYM property implies the Sperner property and also implies the strong Sperner
property (but not the strict Sperner property).

868
Chapter 11
PARTIALLY ORDERED SETS
4. Every LYM order that has symmetric unimodal Whitney numbers has a symmetric
chain decomposition. In particular, Ln(q) is a symmetric chain order.
5. It is not known whether every LYM poset has a chain decomposition that is k-
saturated for all k.
6. A product of LYM orders may fail the LYM property.
7. A product of LYM orders whose sequence of Whitney numbers is log-concave is an
LYM order with a log-concave sequence of Whitney numbers. (A sequence {an} is log-
concave if a2
n ≥an−1an+1 for all n.)
8. The divisor lattice D(N) is an LYM order, which follows from the previous fact.
9. The partition lattice Π(n) is an LYM order if and only if n < 20.
10. The Boolean lattice 2n and the subspace lattice Ln(q) have regular coverings by
chains and hence are LYM orders.
11. If {λx} is an assignment of real-valued weights to the elements of an LYM poset P,
then for every subset G ⊂P and every regular covering C of P,
X
x∈G
λx
Nr(x)
≤max
C∈C {
X
y∈C∩G
λy}.
Example:
1. The lattice L(m, n) of bounded sequences is an LYM order if and only if min{m, n} ≤
2 or (m, n) = (3, 3).
11.2.3
RANKINGS, SEMIORDERS, AND INTERVAL ORDERS
A chain names the “better” of any pair according to a single scale. Realistically, some
comparisons may yield indiﬀerence. Several families of “chain-like” partial orders suc-
cessively relax the requirements on indiﬀerence.
Deﬁnitions:
A poset P is a ranking or weak order if its elements are partitioned into ranks
P1, . . . , Pk such that two elements are incomparable if and only if they belong to the
same rank.
A poset P is a semiorder if there is a real-valued function f and a ﬁxed real number
δ > 0 (δ may be taken to be 1) such that x < y if and only if f(y) −f(x) > δ. The
pair (f, δ) is a semiorder representation of the poset P.
A poset P is an interval order if there is an assignment of real intervals to its members
such that x < y if and only if the interval for y is totally to the right of the interval for x.
The collection of intervals is called an interval representation of the poset P.
A biorder representation on a digraph D is a pair of real-valued functions f, g on the
vertex set VD such that u →v is an arc if and only if f(u) > g(v).
A Ferrers digraph (or Ferrers relation or biorder) is a digraph having a biorder
representation. (Also see §2.5.1.)

Section 11.2
POSET PROPERTIES
869
Facts:
1. Rankings model a single criterion of comparison with “ties” allowed, as in voting.
2. A poset is a ranking if and only if its comparability graph is a complete multipartite
graph.
3. A ranking assigns a score f(z) to each element z such that x < y if and only if
f(x) < f(y).
4. The forbidden subposet characterization of a ranking is 1 + 2.
5. Semiorders were introduced to model intransitivity of indiﬀerence; a diﬀerence of a
few grains of sugar in a coﬀee cup or a few dollars in the price of a house is not likely to
aﬀect one’s attitude, but pounds of sugar or thousands of dollars will. The threshold δ
in a semiorder representation indicates a “just-noticeable diﬀerence”.
6. A poset is a semiorder if and only if its incomparability graph is a unit interval graph,
that is, an interval graph (§8.1.3) such that all intervals are of unit length.
7. An interval representation of a semiorder P with semiorder representation (f, δ) is
obtained by setting Ix = [f(x) −δ
2 + ǫ, f(x) + δ
2 −ǫ].
8. Scott-Suppes theorem:
The forbidden subposet characterization of a semiorder is
{1 + 3, 2 + 2}.
9. The number of nonisomorphic semiorders on an n-element set is the Catalan number
Cn =
1
n+1
 2n
n

(§3.1.3).
10. Interval orders model a situation where the value assigned to an element is imprecise.
11. The incomparability graph of an interval order is an interval graph.
12. Every poset whose incomparability graph is an interval graph is an interval order.
This follows from the forbidden subposet characterization of interval orders.
13. The forbidden subposet characterization of an interval order is 2 + 2.
14. A poset P is an interval order if and only if both the collections of “upper holdings”
U(x) = {y ∈P | y > x} and “lower holdings” D(x) = {y ∈P | y < x} form chains
under inclusion, in which case the number of distinct nonempty upper holding sets and
distinct nonempty lower holding sets is the same. Construction of these chains yields a
fast algorithm to compute a representation for an interval order or semiorder.
15. The strict comparability digraph of an interval order is a Ferrers digraph, with f(x)
and g(x) denoting the left and right endpoints of the interval assigned to x. This is
the strict comparability digraph of a poset because f(x) ≤g(x) for all x. The “upper
holdings” and “lower holdings” for an interval order become predecessor and successor
sets for a Ferrers digraph.
16. For a digraph D with adjacency matrix A(D), the following are equivalent:
• D has a biorder representation (and is a Ferrers digraph);
• A(D) has no 2-by-2 submatrix that is a permutation matrix;
• the successor sets of D are ordered by inclusion;
• the predecessor sets of D are ordered by inclusion;
• the rows and columns of A(D) can be permuted independently so that to the left
of a 1 is a 1.
17. The greedy algorithm is an optimal on-line algorithm for partitioning an interval
order into the minimum number of antichains. It uses at most 2h −1 antichains to
recursively partition an interval order of height h, and this is best possible.

870
Chapter 11
PARTIALLY ORDERED SETS
18. There is an on-line algorithm to partition every interval order of width k into 3k −2
chains, and this is best possible. Equivalently, the maximum number of colors needed
for on-line coloring of interval graphs with clique size k is 3k −2.
19. No on-line partitioning algorithm colors all trees with a bounded number of colors.
20. “Universal” interval orders:
Since the ordering of the interval endpoints is all
that matters, interval representations may be restricted to have integer endpoints. The
poset I[0, n] or In denotes the interval order whose interval representation consists of all
intervals with integer endpoints in {0, . . ., n}.
21. Every ﬁnite interval order is a subposet of some I[0, n].
Examples:
1. The following Hasse diagram represents a poset that is a ranking. Its three ranks are
indicated by the levels in the diagram.
2. The following Hasse diagram represents a poset that is a semiorder: for instance,
with δ = 1, deﬁne f(a) = 2, f(b) = 1.3, f(c) = 0.8, and f(d) = 0. It is not a ranking, by
Fact 4, because 1 + 2 is a subposet. The interval representation of its incomparability
graph is at the right.
a
b
d
c
d
c
a
b
3. The following Hasse diagram represents a poset that is an interval order. The interval
representation of its incomparability graph is at the right. By Fact 8, it is not a semi-
order.
a
b
d
c
d
b
a
c
4. The skill of a tennis player may vary from day to day, leading to use of an inter-
val [ax, bx] to represent player x. In this case player x always beats player y if ax > by.
5. The interval order I[0, 3] is not a semiorder.
11.2.4
APPLICATION TO SOCIAL CHOICE
When there are more than two candidates for a public oﬃce, it is not obvious what is the
“best” way to select a winner. Any rule has its pluses and minuses, from the standpoint
of public policy. Social choice theory analyzes the eﬀect of various rules for deciding the
outcomes of preferential rankings.

Section 11.2
POSET PROPERTIES
871
Deﬁnitions:
A proﬁle on a set A of “alternatives” (e.g., candidates for a public oﬃce) is a set
P = {Pi | i ∈I} of linear rankings (ties allowed) of A, one for each member of a set I of
“individuals” (e.g., voters).
A consensus function (or social choice function) is a function φ that assigns to each
possible proﬁle P = {Pi | i ∈I} on a set A of alternatives a linear order (ties allowed) of
A called the consensus ranking for P.
A consensus function upholds majority rule provided it ensures that x is preferred to
y if and only if a majority of the individuals prefer x to y.
Plurality is the consensus function in which the winner(s) is (are) the alternative(s)
appearing in the greatest number of top ranks, after which the winner(s) is (are) deleted
and the procedure is repeated to select the next rank of the consensus ranking, etc.
The Borda count of an alternative x is the sum, over individual rankings, of the num-
ber of alternatives x “beats”.
The resulting Borda consensus function ranks the
alternatives by their Borda count.
Facts:
1. Plurality can elect someone ranked last by a majority.
2. Condorcet’s paradox:
Some proﬁles have no decisive consensus (i.e., producing a
single winner) that upholds majority rule.
3. The Borda count is subject to abuse.
4. Arrow’s impossibility theorem: No consensus function exists that satisﬁes the follow-
ing four axioms, which were formulated in an attempt to develop a consensus function φ
that avoids the diﬃculties cited in the facts above:
• monotonicity: If a > b in φ(P) and if proﬁle P ′ agrees with proﬁle P except for
moving alternative a upward in some or all rankings, then a > b in φ(P ′).
• independence of irrelevant alternatives:
If proﬁles P and P ′ agree within a set
A′ ⊆A, then φ(P) and φ(P ′) have the same restriction to A′. This axiom
implies that votes for extraneous alternatives do not aﬀect the determination
of the consensus ranking among the alternatives within the subset A′.
• nondegeneracy: Given a, b ∈A, there is a proﬁle P such that a > b in φ(P). This
axiom implies that the structure of the outcome is independent of renaming the
alternatives.
• nondictatorship: There is no i ∈I such that a > b in Pi implies a > b in φ(P).
Examples:
1. Suppose that A = {a, b, c } is the set of alternatives, and suppose that the proﬁle
consists of the three rankings a > b > c, c > a > b, and b > c > a. Then for each
alternative, there is another alternative that is preferred by 2
3 of the population.
2. The U.S. presidential election of 1912 had three candidates:
Wilson (W), Roo-
sevelt (R), and Taft (T). It is estimated that 45% of the voters ranked W > R > T , that
30% ranked R > T > W, and that 25% ranked T > R > W. Wilson won the election,
garnering a plurality of the popular vote, but a majority of the population preferred
Roosevelt to Wilson. Moreover, 55% regarded Wilson as the least desirable candidate.
3. Consider a close election, with four individuals preferring x to y to all other alterna-
tives. A ﬁfth individual prefers y to x. If there are enough other alternatives, the ﬁfth
individual can throw a Borda-count election to y by placing x at the bottom.

872
Chapter 11
PARTIALLY ORDERED SETS
11.2.5
LINEAR EXTENSIONS AND DIMENSION
By adding additional comparison pairs to a partial ordering on a set, ultimately a total
ordering is obtained. Each of the many ways to do this is called an extension of the
original partial ordering.
Deﬁnitions:
An extension of a poset P = (X, R) is a poset Q = (X, S) such that R ⊆S (i.e., xRy
implies xSy).
A linear extension of a poset P is an extension of P that is a chain.
A topological sort is an algorithm that accepts a ﬁnite poset as input and produces a
linear extension of that poset as output.
A topological ordering of an acyclic digraph is a linear extension of the poset arising
from it.
The intersection of two partial orderings P = (X, R) and Q = (X, S) on the same
set X is the poset (X, R ∩S) that includes the relations common to both.
A realizer of a poset P is a set of linear extensions of P whose intersection is P.
The order dimension (or dimension) of P, written dim(P), is the minimum cardinality
of a realizer of P.
The standard example Sn of an n-dimensional poset is the subposet of 2n induced by
the singletons and their complements.
An alternating k-cycle in a poset P is a sequence of ordered incomparable pairs
{(xi, yi)}k
i=1 such that yi ≤xi+1, where subscripts are taken modulo k.
A critical pair (or unforced pair) in a poset P is an ordered incomparable pair that
cannot be made comparable by adding any other single incomparable pair as a relation.
A linear extension L of a poset P puts Y over X, where X and Y are disjoint subposets,
if y is above x in L whenever (x, y) is an incomparable pair with x ∈X, y ∈Y .
Given a subposet Q ⊆P, an upper extension of Q is a linear extension of P that puts
P −Q over Q.
Given a subposet Q ⊆P, a lower extension of Q is a linear extension of P that puts
P −Q below Q.
The minimum realizer encoding of a poset lists for each element its position on each
extension in a minimum realizer.
The probability space on the set of all linear extensions of a (ﬁnite) poset P is obtained
by taking each linear extension to be equally likely. The notation Pr(x < y) denotes the
proportion of linear extensions in which element x comes below element y.
Facts:
1. Every poset is the intersection of all its linear extensions, from which it follows that
the concept of dimension is well deﬁned.
2. Given incomparable elements x and y in a poset P, there is a linear extension of P
in which x appears above y.

Section 11.2
POSET PROPERTIES
873
3. The chains are the only posets of dimension 1.
4. Every antichain has dimension 2, because the intersection of a linear order and its
dual is an antichain.
5. Topological sort is used to organize activities having a precedence ordering into a
sequential schedule.
6. The list of minimal forbidden subposets for dimension 2 consists of ten isolated ex-
amples and seven one-parameter families.
7. If Q is a subposet of P, then dim(Q) ≤dim(P).
8. The dimension of a product of k chains (each of size at least 2) is k.
9. A poset has dimension at most k if and only if it imbeds in a product of k chains.
10. The dimension of a poset P equals the minimum integer n such that P is a subposet
of Rn.
11. The standard example Sn is a bipartite poset whose comparability graph is obtained
from the complete bipartite graph Kn,n by deleting a complete matching.
12. The minimum realizer encoding of an n-element poset of dimension k takes only
O(kn log n) bits, instead of the O(n2) bits of the order relation. Thus, posets of small
dimension have concise representations.
13. In the sense of Fact 12, the dimension of a poset may be regarded as a measure of
its “space complexity”.
14. The dimension of a poset P equals the minimum number of linear extensions con-
taining all the critical pairs of P.
15. The dimension of a poset P is equal to the chromatic number of the hypergraph
whose vertex set is the set of critical pairs and whose edges are the sets of critical pairs
forming minimal alternating cycles.
16. The cover graph of the standard example Sn (of an n-dimensional poset) is Kn,n-
(1-factor).
17. If X and Y are disjoint subposets of a poset P, then P has a linear extension L
putting Y over X if and only if P contains no 2 + 2 with minimal elements in Y and
maximal elements in X.
18. dim(P) ≤w(P). A realizer of size w(P) can be formed by taking upper extensions
of the chains in a partition of P into w(P) chains.
19. dim(P) ≤|P |
2 . The standard example Sn shows that this bound is the best possible.
20. One-point removal theorem: For every x ∈P, dim(P) ≤1 + dim(P −x).
21. For every poset P, there exist four elements {x, y, z, w} such that dim(P) ≤2 +
dim(P −{x, y, z, w}). It is conjectured that, for every poset P, there exist two elements
{x, y} such that dim(P) ≤1 + dim(P −{x, y}).
22. A poset has dimension 2 if and only if the complement of its comparability graph is
also a comparability graph; thus there is a polynomial-time algorithm to decide whether
a poset has dimension 2. However, recognizing posets of dimension k is NP-complete for
every ﬁxed k at least 3.
23. If P is a ﬁnite poset that is not a chain, then P has a pair of elements x, y such that
0.2764 ≃1
2 −
1
2
√
5 ≤Pr(x < y) ≤1
2 +
1
2
√
5 ≃0.7236.
24. The 1
3- 2
3 conjecture: This conjecture states that there is always a pair of elements, x
and y, such that 1
3 ≤Pr(x < y) ≤2
3.

874
Chapter 11
PARTIALLY ORDERED SETS
25. The traditional name topological sort is commonly used in applications. However,
a topological sort is not a sort in the standard meaning of that word. Nor is it directly
related to what mathematicians call topology.
26. Algorithm 1 carries out a topological sort on a given poset; also see §8.3.4.
Algorithm 1:
Topological sort.
input: a ﬁnite poset (X = {x1, . . . , xn}, ≤)
output: a compatible total ordering A = x1 ≤x2 ≤· · · ≤xn of the elements of X
for j := 1 to n
xj := a minimal element of X
X := X −{xj}
Examples:
1. The following poset has the linear extensions abc and acb and it is the intersection of
these extensions. Thus its dimension is 2.
a
c
b
2. The following poset has six linear extensions: abcd, acbd, acdb, cdab, cadb, and cabd.
Since it is the intersection of abcd and cdab, its dimension is 2.
a
b
d
c
3. The bipartite poset S3 whose comparability graph and cover graph is the 6-cycle
1, 2, 3, 1, 2, 3 has dimension 3. The realizer {231123, 132213, 123312} establishes the upper
bound. Every realizer must have an extension with 1 below 1, one with 2 below 2, and
one with 3 below 3. No two of these can occur in the same linear extension, so the
dimension is at least 3.
4. More generally, for the elements i ∈[n] of the standard example Sn, a realizer must
include distinct linear extensions in which the singleton {i} appears above its comple-
ment, and any n such extensions suﬃce.
5. For the standard example Sn of dimension n, the critical pairs are {¯i, i}; this reﬂects
the fact that, in a realizer, the extensions need to put i above ¯i, for each i. Each pair of
critical pairs forms a minimal alternating cycle. Viewing the minimal alternating cycles
as edges creates a hypergraph, namely the complete graph Kn, with chromatic number n.
6. Let N be the bipartite poset with minimal elements a and b and maximal elements c
and d, in which a lies below c, and b lies below c and d.
This poset has ﬁve linear
extensions, namely a < b < c < d, a < b < d < c, b < a < c < d, b < a < d < c, and
b < d < a < c. Thus Pr(a < b) = 2
5.
7. Application of posets to sorting:
The objective of a sort is to arrange the elements
of a set X into a sequence by posing sequential queries of the form: “is x < y true?”.
At any time, the state of cumulative knowledge is representable by a poset P = (X, R),
such that the linear extensions of P are remaining candidates for the ﬁnal sequence
order. A desirable query substantially reduces the number of candidates for extensions

Section 11.2
POSET PROPERTIES
875
no matter whether the answer is yes or no, most especially ﬁnding a pair, x and y, such
that Pr(x < y) is close to 1
2. Thus, Fact 23 shows that the worst case time to sort, in
the presence of partial information given by a poset P, is Ω(log |P|).
8. Application of posets to searching:
The objective of searching a poset P in which
item s(x) is stored at location x is to determine whether a target item α is present in P.
Each step of the search probes a location and compares its value against the target item.
The worst case requires determining for each x ∈P whether the item at location x is
greater or less than α, so the searching problem is the problem of identifying the downset
Dα = {x ∈P | s(x) < α}. A probe of location x splits the remaining possible downsets
into those that contain x and those that do not. The former remain as candidates if
s(x) < α; the latter remain if s(x) > α. A hypothetical adversary would arrange the
value s(x) so that the response would leave the larger portion of the ideals. Thus, the
number c(P) of probes required in the worst case is at least ⌈log2 i(P)⌉, where i(P)
denotes the number of ideals in poset P.
11.2.6
POSETS AND GRAPHS
From the graph-theoretic viewpoint, a comparability graph is by deﬁnition a simple graph
(§8.1.1) that has a transitive orientation. Comparability graphs are perfect graphs, which
motivates most study of comparability graphs.
Deﬁnitions:
A transitive orientation on a simple graph G is an assignment of directions to the
edges so that whenever there is an xy-arc and a yz-arc, there is also an xz-arc.
A quasi-transitive orientation on a simple graph G is an assignment of directions to
the edges so that whenever there is an xy-arc and a yz-arc, there is also an arc between x
and z.
A triangular chord for a walk x1, . . . , xk in an undirected graph G is an edge between
vertices xi−1 and xi+1, two apart on the walk.
The auxiliary graph for a simple graph G is the graph G′ whose vertices are the edges
of G, with vertex e1 adjacent to vertex e2 in G′ if and only if edges e1 and e2 are adjacent
in graph G but do not lie on a 3-cycle.
A module in a graph G is a vertex subset U such that each vertex outside U is adjacent
to all or none of the vertices in U.
An order module in a poset P (or autonomous set) is a set S of elements such that
every element outside S is above all of S, below all of S, or incomparable to all of S.
A comparability invariant for posets is an invariant f such that f(P) = f(Q) whenever
posets P and Q have the same comparability graph.
A permutation graph is a graph whose vertices can be placed in 1-1 correspondence
with the elements of a permutation of [n] = {1, . . . , n} such that vi is adjacent to vj if
and only if the larger of i and j comes ﬁrst in the permutation.
A u,v-bypass in a directed graph is a u,v-path of length at least two such that there is
also an arc from u to v.
A dependent edge in an acyclic directed graph is an arc from u to v such that the
graph contains a u,v-bypass.

876
Chapter 11
PARTIALLY ORDERED SETS
Facts:
1. For a simple graph G, the following are equivalent:
• G has a transitive orientation;
• G has a quasi-transitive orientation;
• every closed odd walk of G has a triangular chord;
• the auxiliary graph G′ is bipartite.
The implications from top to bottom are straightforward, as is the proof that if the
auxiliary graph G′ is bipartite then G has a quasi-transitive orientation. The proof that
if G has a quasi-transitive orientation then G has a transitive orientation takes more work.
The last characterization gives an algorithm to decide whether G is a comparability graph
in O(n3) time, where n is the number of vertices. The proof is constructive, so a transitive
orientation can also be obtained.
2. In any graph, the set of all vertices, the singleton sets of vertices, and the empty set
are always modules.
3. Modules yield a forbidden subgraph characterization of comparability graphs. The
minimal forbidden induced subgraphs consist of eight inﬁnite families and ten special
examples.
4. If two partial orders have the same comparability graph, then one can be transformed
into the other by a sequence of moves involving reversing all the relations inside an order
module S, i.e., by replacing the partial order induced on S by its dual, and preserving
all relations between S and its complement.
5. Let f be a poset invariant such that f(P) = f(P ∗) for all posets P, and such that, if
poset Q is obtained from P by replacing a module in P with another module having the
same value of f, then f(Q) = f(P). Then the invariant f is a comparability invariant.
6. Height, width, dimension, and number of linear extensions are all comparability
invariants.
7. A graph is the complement of a comparability graph if and only if it is the intersection
graph of the curves representing a collection of continuous real-valued functions on [0, 1].
8. The following conditions are equivalent for a graph G:
• G is a permutation graph (adjacency representing the inversions of a permutation);
• G is the comparability graph of a 2-dimensional partial order;
• G and G are comparability graphs.
9. Isomorphism of permutation graphs can be tested in O(n2) time. Some NP-complete
scheduling problems become polynomial when the poset of precedence constraints is 2-
dimensional.
10. A directed graph corresponds to the diagram of some partial order if and only if it
contains no cycles or bypasses.
11. Every graph that is the cover graph of some poset is triangle-free.
12. If a graph has chromatic number less than its girth, then it is the cover graph
of some poset. In particular, a 3-chromatic graph is a cover graph if and only if it is
triangle-free.
13. It is NP-complete to decide whether a 4-chromatic graph is a covering graph.
14. The smallest triangle-free graph that is not a cover graph is the 4-chromatic Gr¨otzsch
graph with 11 vertices.

Section 11.2
POSET PROPERTIES
877
15. The maximum number of dependent edges among the orientations of a graph G is
equal to the cycle rank β(G) = |E| −|V | + 1.
16. If a graph G has chromatic number less than its girth (§8.7.2), then for all i such
that 0 ≤i ≤β(G), the graph has an acyclic orientation with exactly i dependent edges.
17. The cover graph of a modular lattice is bipartite.
18. A modular lattice is distributive if and only if its cover graph does not contain the
complete bipartite graph K2,3.
19. The subgraph of the cover graph of 22k+1 induced by the k-sets and (k + 1)-sets is
a vertex-transitive (k + 1)-regular bipartite graph. The graph is known to contain cycles
using more than 80% of its vertices. The Erd˝os revolving door conjecture asserts that
this graph is Hamiltonian.
20. Gallai-Milgram theorem: The vertices of a digraph D can be covered using at most
α(D) disjoint paths, where α(D) is the maximum size of an independent set in D.
21. Dilworth’s theorem (§11.2.1) is the special case of the Gallai-Milgram theorem for
comparability digraphs.
Examples:
1. A transitive orientation for a bipartite graph can be obtained by assigning all the
edge directions from one part to the other, as shown here for K3,3:
2. An odd cycle of length ≥5 has no quasi-transitive orientation (see Fact 1).
3. Inserting a triangular chord into a 5-cycle permits the resulting graph to have a
transitive orientation, as shown in the following ﬁgure.
4. The following ﬁgure shows a graph and its auxiliary graph.
a
e
f
d
b
c
b
a
e
d
f
c
5. Any subset of either part of a complete bipartite graph is a module, since the other
vertices in its part are not adjacent to any vertex in the module, and the vertices in the
other part of the bipartition are each adjacent to all the vertices in the module.
6. Deleting a 1-factor from Kn,n, for n ≥3, yields a graph with no module other than
the complete set of vertices, the singletons, and the empty set.

878
Chapter 11
PARTIALLY ORDERED SETS
REFERENCES
Printed Resources:
[An87] I. Anderson, Combinatorics of Finite Sets, Oxford University Press, 1987.
[Bi95] G. Birkhoﬀ, Lattice Theory, 3rd ed., American Mathematical Society, 1995.
[Bo98] B. Bollob´as, Combinatorics: Set Systems, Hypergraphs, Families of Vectors, and
Combinatorial Probability, Cambridge University Press, 1998.
[BoMo94] V. Bouchitt´e and M. Morvan, eds., Orders, Algorithms and Applications,
Springer, 1994.
[DaPr02] B. A. Davey and H. A. Priestley, Introduction to Lattices and Order, 2nd ed.,
Cambridge University Press, 2002.
[Fi85] P. C. Fishburn, Interval Orders and Interval Graphs: A Study of Partially Ordered
Sets, Wiley, 1985.
[Ri82] I. Rival, ed., Proceedings of the Symposium on Ordered Sets, Reidel Publishing,
1982.
[Ri85] I. Rival, ed., Graphs and Order, Reidel Publishing, 1985.
[Ri86] I. Rival, ed., Combinatorics and Ordered Sets, American Mathematical Society,
1986.
[Ri89] I. Rival, ed., Algorithms and Order, Kluwer Academic Press, 1989.
[Ro12] K. H. Rosen, Discrete Mathematics and Its Applications, 7th ed., McGraw-Hill,
2012.
[Sc03] B. Schr¨oder, Ordered Sets: An Introduction, Birkh¨auser, 2003.
[Tr92] W. T. Trotter, Combinatorics and Partially Ordered Sets: Dimension Theory, The
Johns Hopkins University Press, 1992.

12
COMBINATORIAL DESIGNS
12.1 Block Designs
Charles J. Colbourn
12.1.1 Balanced Incomplete Block Designs
and Jeffrey H. Dinitz
12.1.2 Isomorphism and Automorphism
12.1.3 Subdesigns
12.1.4 Resolvable Designs
12.1.5 t-Designs and Steiner Systems
12.1.6 Pairwise Balanced Designs
12.1.7 Group Divisible Designs and Transversal Designs
12.2 Symmetric Designs and Finite Geometries
Charles J. Colbourn
12.2.1 Finite Geometries
and Jeffrey H. Dinitz
12.2.2 Symmetric Designs
12.2.3 Projective and Afﬁne Planes
12.2.4 Hadamard Designs and Matrices
12.2.5 Difference Sets
12.3 Latin Squares and Orthogonal Arrays
Charles J. Colbourn
12.3.1 Latin Squares
and Jeffrey H. Dinitz
12.3.2 Mutually Orthogonal Latin Squares
12.3.3 Orthogonal Arrays
12.4 Matroids
James G. Oxley
12.4.1 Basic Deﬁnitions and Examples
12.4.2 Alternative Axiom Systems
12.4.3 Duality
12.4.4 Fundamental Operations
12.4.5 Characterizations
12.4.6 The Greedy Algorithm

880
Chapter 12
COMBINATORIAL DESIGNS
INTRODUCTION
In broad terms, the study of combinatorial designs is the study of the structure of collec-
tions of subsets of a ﬁnite set when these collections of subsets satisfy certain prescribed
properties. In particular, a block design has the property that every one of these subsets
has the same size k and every pair of points in the set is in exactly the same number
of these subsets. Latin squares are also fundamental in this area and can be thought of
in this context, but they are commonly thought of as n × n arrays with the property
that each cell contains one element from an n-set and each row and each column contain
each element exactly once. Some of the questions of general interest include: existence of
designs, enumeration of nonisomorphic designs, and the study of subdesigns of designs.
Matroids generalize a variety of combinatorial objects, such as matrices and graphs.
These structures arise naturally in a variety of combinatorial contexts and provide a
framework for the study of many problems in combinatorial optimization and graph
theory.
Much of the information in §12.1–12.3 is condensed from [CoDi07], which provides a
comprehensive treatment of combinatorial designs. The main source for the material
in §12.4 is [Ox11].
GLOSSARY
aﬃne plane: a set of points and a set of subsets of points (called lines) such that every
two points lie on exactly one line, if a point does not lie on a line L there is exactly
one line through the point that does not intersect L, and there are three points that
are not collinear.
aﬃne space (of dimension n): the set AG(n, q) of all cosets of subspaces of an n-
dimensional vector space over a ﬁeld of order q.
automorphism (a design D): an isomorphism from D onto D.
balanced incomplete block design (BIBD): given a ﬁnite set X (of points), a col-
lection of subsets (called blocks) of X of the same size such that every point belongs
to the same number of blocks, and each pair of points belongs to the same number of
blocks. The BIBD is described by ﬁve parameters: size of X, number of blocks, num-
ber of blocks to which every element of X belongs, size of each block, and number
of blocks to which each pair of distinct points belongs.
basis (for a matroid): a maximal independent set in the matroid.
basis axioms: a set of axioms that speciﬁes the set of bases of a matroid.
binary matroid: a matroid that is isomorphic to a vector matroid of a matrix over the
ﬁeld GF(2).
biplane: symmetric design in which every pair of distinct points belongs to exactly two
blocks.
block: each of the subsets in a design.
circuit: a minimal dependent set in a matroid.
circuit axioms: a set of axioms that speciﬁes the set of circuits of a matroid.
closed set: in a matroid, a subset of its ground set that is equal to its closure.

GLOSSARY
881
closed under duality: the property of a class of matroids that the dual of a matroid
in the class is also in the class.
closure (of a subset of the ground set in a matroid): given a subset X of the ground
set E in a matroid, the set of all points x ∈E such that the rank of X ∪{x} is equal
to the rank of X.
closure axioms: a set of axioms that speciﬁes the properties that a closure operator
of a matroid must satisfy.
closure operation: the mapping K →B(K), where K is a set of positive integers and
B(K) is the set of positive integers v for which there exists a (v, K)-PBD.
cobasis (of a matroid): a basis of the dual of a matroid.
cocircuit (of a matroid): a circuit of the dual of a matroid.
cographic matroid: a matroid isomorphic to the cocyle matroid of a graph.
coindependent set (of a matroid): an independent set of the dual of a matroid.
coloop (of a matroid): a loop of the dual of a matroid.
combinatorial geometry: a simple matroid.
complete set of mutually orthogonal Latin squares: a set of n −1 mutually or-
thogonal Latin squares of side n.
conjugate: Let L be an n×n Latin square on symbol set E3, with rows indexed by the
elements of the n-set E1 and columns indexed by the elements of the n-set E2. Let
T = {(x1, x2, x3) | L(x1, x2) = x3}. Let {a, b, c} = {1, 2, 3}. The (a, b, c)-conjugate
of L, L(a,b,c), has rows indexed by Ea, columns by Eb, and symbols by Ec, and is
deﬁned by L(a,b,c)(xa, xb) = xc for each (x1, x2, x3) ∈T .
connected: the property of a matroid that it cannot be written as the direct sum of
two nonempty matroids.
cycle matroid (of a graph): the matroid on the edge-set of the graph whose circuits
are the cycles of the graph.
t-design: a t-(v, k, λ) design.
t-(v, k, λ) design: a pair (X, A) where X is a set of v elements (points), A is a family
of k-subsets (blocks) of X, and every t-subset of X occurs in exactly λ blocks.
development (of a diﬀerence set D): the incidence structure dev(D) whose points are
the elements of G and whose blocks are the translates D + g = {d + g | d ∈D}.
dual (of an incidence structure): the incidence structure obtained by interchanging the
roles of points and lines.
dual (of a matroid): given a matroid M, the matroid on the same set as M whose bases
are the complements of the bases of M.
equivalent (Latin squares): two Latin squares L and L′ of side n are equivalent if there
are three bijections, from the rows, columns, and symbols of L to the rows, columns,
and symbols, respectively, of L′, that map L to L′.
Fano plane (or projective plane of order 2): the (7, 7, 3, 3, 1) design with point set
X = {0, . . ., 6} and the block set A = {013, 124, 235, 346, 450, 561, 602}.
ﬂat: closed set.
t-ﬂat: a subspace of projective dimension t of a projective space; a coset of a subspace
of aﬃne dimension t of an aﬃne space.

882
Chapter 12
COMBINATORIAL DESIGNS
k-GDD: a group divisible design with λ = 1 and K = {k}.
graphic matroid: a matroid that is isomorphic to the cycle matroid of some graph.
ground set: the set of points of a matroid.
group divisible design (or (K, λ)-GDD): given an integer λ and a set of positive
integers K, a triple (X, G, A) where X is a set (of points), G is a partition of X into
at least two subsets (called groups), A is a family of subsets of X (called blocks)
such that: if A in A, then |A| ∈K, a group and a block contain at most one common
point, and every pair of points from distinct groups occurs in exactly λ blocks.
group-type (or type): for a group divisible design, the multiset {|G| : G ∈G}.
Hadamard design: a symmetric (4n −1, 2n −1, n −1) design.
Hadamard matrix: an n × n matrix H with all entries ±1 that satisﬁes HTH = nI.
hyperplane: a subspace of projective dimension n−1 of a projective space of projective
dimension n; a coset of a subspace of aﬃne dimension n−1 of an aﬃne space of aﬃne
dimension n; a maximal nonspanning set of a matroid.
idempotent: the property of a Latin square (or partial Latin square) that for all i, cell
(i, i) is occupied by i.
imbedded Latin square: An n × n partial Latin square P is imbedded in a Latin
square L if the upper n × n left corner of L agrees with P.
incidence matrix (of a (v, b, r, k, λ) design): the b × v matrix with (i, j)-entry equal
to 1 if the ith block contains the jth element, and 0 otherwise.
incidence structure: the structure (V, B, I) consisting of a ﬁnite set V of points, a
ﬁnite set B of lines, and an incidence relation I between them.
independent set: any set in a special collection of subsets of the ground set in a
matroid.
index: the number of blocks to which each pair of distinct points in a design belongs.
isomorphism (of block designs (V, B) and (W, C)): a bijection ψ: (V, B) →(W, C) un-
der which ψ(B) occurs as a block in C the same number of times that B occurs as a
block in B.
isomorphism (of matroids): a bijection between the ground sets of two matroids that
preserves independence.
isotopic: equivalent.
Kirkman’s schoolgirl problem: the problem of arranging ﬁfteen schoolgirls in ﬁve
subsets of size 3 for a walk on each of seven days so that every pair of girls walks
together exactly once.
Kirkman triple system: a (v, 3, 1) resolvable BIBD, together with a resolution of it.
Kronecker product: for m × p matrix M = (mij) and n × q matrix N = (nij), the
mn × pq matrix given by
M × N =




m11N
m12N
· · ·
m1pN
...
...
...
mm1N
mm2N
· · ·
mmpN



.

GLOSSARY
883
Latin rectangle: a k × n (k < n) array in which each cell contains a single element
from an n-set such that each element occurs exactly once in each row and at most
once in each column.
Latin square: a Latin square of side n is an n × n array in which each entry contains
a single element from a set S of size n such that each element occurs exactly once in
each row and exactly once in each column.
line: a subspace of projective dimension 1 of a projective space; a coset of a subspace
of aﬃne dimension 1 of an aﬃne space.
loop (in a matroid): an element e of the matroid such that {e} is a circuit.
matroid: an ordered pair M = (E(M), I(M)) where E (the ground set) is a ﬁnite set
and I is a collection of subsets (independent sets) of E such that: the empty set is
independent; every subset of an independent set is independent; and if X and Y are
independent and |X| < |Y |, then there is an element e in Y −X such that X ∪{e}
is independent.
matroid representable over a ﬁeld: a matroid that is isomorphic to the vector ma-
troid of some matrix over the ﬁeld F.
multiplier (of a diﬀerence set D in a group G): an automorphism ϕ of G such that
ϕ(D) = D + g for some g ∈G.
mutually orthogonal: the property of a set of Latin squares that every two are or-
thogonal.
orthogonal: property of two n × n Latin squares A = (aij) and B = (bij) that all n2
ordered pairs (aij, bij) are distinct.
orthogonal array (of size N with k constraints, s levels, and strength t): a k × N ar-
ray with entries from a set of s ≥2 symbols, having the property that in every t× N
submatrix every t × 1 column vector appears the same number of times.
pairwise balanced design (PBD): for a set K of positive integers, a design (v, K, λ)
consisting of an ordered pair (X, A) where X is a set of size v and A is a collection of
subsets of X with the property that every pair of elements of X occurs in exactly λ
blocks, and for every block A ∈A, |A| ∈K; a pairwise balanced design is called a
(v, K)-PBD when λ = 1.
parallel class: a collection of blocks that partition the point set of a design.
parallel elements: in a matroid, two elements that form a circuit.
partial Latin square: an n × n array with cells, each of which is either empty or else
contains exactly one symbol, such that no symbol occurs more than once in any row
or column.
partial transversal (of length k): in a Latin square, a set of k cells, each from a dif-
ferent row and each from a diﬀerent column, such that no two contain the same
symbol.
paving matroid: a matroid such that the number of elements in every circuit is at
least as large as the rank of the matroid.
PBD-closure: for a set K of positive integers, the set B(K) = {v | there exists a
(v, K)-PBD}.
planar: the property of a matroid that it is isomorphic to the cycle matroid of a planar
graph.

884
Chapter 12
COMBINATORIAL DESIGNS
plane: a subspace of projective dimension 2 of a projective space; a coset of a subspace
of aﬃne dimension 2 of an aﬃne space.
projective plane: a ﬁnite set (of points) and a set of subsets of points (called lines)
such that every two points lie on exactly one line, every two lines intersect in exactly
one point, and there are four points with no three collinear; equivalently, a symmetric
(n2 + n + 1, n + 1, 1) design.
projective space (of dimension n): for a ﬁeld F of order q and an (n+ 1)-dimensional
vector space S over F, the set PG(n, q) of all subspaces of S.
rank (of a matroid): the rank of the ground set of the matroid.
rank (of a set in a matroid): the cardinality of every maximal independent subset of
the set.
rank axioms: a set of axioms that speciﬁes the properties that a rank function on a
matroid must satisfy.
reduced Latin square: a Latin square such that the elements in the ﬁrst row and the
elements in the ﬁrst column occur in natural order.
regular matroid: a matroid that is representable over all ﬁelds.
replication number: the number of blocks to which each point in a design belongs.
representable over a ﬁeld: the property of a matroid that it is isomorphic to a vector
matroid of some matrix over the ﬁeld.
resolution: a partition of the family of blocks of a balanced incomplete block design
into parallel classes.
resolvable: the property of a balanced incomplete block design that it has at least one
resolution.
simple matroid: a matroid that has no loops or parallel elements.
simple (t-design): a t-design that contains no repeated blocks.
spanning set (of a matroid): for matroid M, a subset of the ground set E of rank r(M).
Steiner triple system: a balanced incomplete block design in which each block has
three elements and each pair of points occurs in exactly one block; that is, a (v, 3, 1)
design.
subdesign: a collection of points and blocks in a block design that is itself a block
design.
subsquare: for k < n, a Latin square of side k whose rows and columns are chosen
from a Latin square of side n.
symmetric block design: a (v, b, r, k, λ) design where the number of points v equals
the number of blocks b.
ternary matroid: a matroid isomorphic to a vector matroid of a matrix over GF(3).
transversal: in a Latin square of side n, a set of n cells, one from each row and column,
containing each of the n symbols exactly once.
transversal design: a k-GDD having k groups of size n and uniform block size k.
transversal matroid: given a family of sets, the matroid whose independent sets are
partial transversals of this family.
type: See group-type.

Section 12.1
BLOCK DESIGNS
885
uniform matroid: the matroid with 1, 2, . . . , n as ground set, and all subsets of size
less than a speciﬁed number as independent sets.
(v, k, λ) design: a BIBD with parameters (v, b, r, k, λ).
(v, k, λ; n) diﬀerence set (of order n = k −λ): a k-subset D of a group G (of order v)
where every nonzero element of G has exactly λ diﬀerences d −d′ with elements
from D.
vector matroid: the matroid on the columns of a matrix whose independent sets are
the linearly independent sets of columns.
void design: a BIBD with at most one element.
12.1
BLOCK DESIGNS
12.1.1
BALANCED INCOMPLETE BLOCK DESIGNS
Deﬁnitions:
A balanced incomplete block design (BIBD) with parameters (v, b, r, k, λ) is a pair
(X, A), where X is a set, A is a collection of subsets of X, the ﬁve parameters are non-
negative integers, either v ∈{0, 1} (the void designs) or v > k > 0, and the parameters
represent the following:
• v (order): the size of X (elements of X are points, varieties, or treatments);
• b (block number): the number of elements of A (elements of A are blocks);
• r (replication number): the number of blocks to which every point belongs;
• k (block size): the common size of each block;
• λ (index): the number of blocks to which every pair of distinct points belongs.
Note: A BIBD is often referred to as a design. Diﬀerent notations are used for balanced
incomplete block designs: (v, b, r, k, λ) BIBD, (v, k, λ) BIBD and Sλ(2, k, v).
In this
chapter (v, k, λ) design will be used. See Fact 6.
A Steiner triple system is a (v, v(v−1)
6
, v−1
2 , 3, 1) design, i.e., a BIBD in which each
block has size 3 and each pair of points occurs in exactly one block. A Steiner triple
system is denoted STS(v) or S(2, 3, v). (Jakob Steiner, 1796–1863)
The incidence matrix of a (v, b, r, k, λ) design is the b×v matrix A = (aij) deﬁned by
aij =
(
1
if the ith block contains the jth point
0
otherwise.
Facts:
1. Balanced incomplete block designs are used in the design of experiments when the
total number (v) of objects to be tested is greater than the number (k) that can be
tested at any one time. They are used to design experiments where the subjects must be
divided into subsets (blocks) of the same size to receive diﬀerent treatments, such that

886
Chapter 12
COMBINATORIAL DESIGNS
each subject is tested the same number of times and every pair of subjects appears in
the same number of subsets.
2. Designs are useful in many areas, such as coding theory, cryptography, group testing,
and tournament scheduling. Detailed coverage of these and other applications of designs
can be found in Chapter V of [CoDi07].
3. The word “balanced” refers to the fact that λ remains constant. If λ changes de-
pending on the pair of points chosen, the design is not balanced.
4. The word “incomplete” refers to the fact that k < v, that is, the size of each block is
less than the number of varieties.
5. Necessary conditions for existence: If there is a (v, b, r, k, λ) design for particular v,
b, r, k, and λ, then the parameters must satisfy
• vr = bk;
• λ(v −1) = r(k −1);
• b ≥v. (Fisher’s inequality, 1940) (Ronald A. Fisher, 1890–1962)
6. If a (v, b, r, k, λ) design exists, r =
λ(v−1)
k−1
and b =
λv(v−1)
k(k−1) .
In view of these
two relationships, (v, b, r, k, λ) designs are commonly referred to simply by the three
parameters — v, k, λ — as a (v, k, λ) design.
7. Necessary conditions for existence:
If there is a (v, k, λ) design for particular v, k,
and λ, then
• λ(v −1) ≡0 (mod k −1);
• λv(v −1) ≡0 (mod k(k −1)).
8. Existence of (v, k, λ) designs:
• (v, 3, λ) design:
exists for all v satisfying the necessary conditions given in Fact
5, namely
⋄if λ ≡2 or 4 (mod 6) and v ≡0 or 1 (mod 3)
⋄if λ ≡1 or 5 (mod 6) and v ≡1 or 3 (mod 6)
⋄if λ ≡3 (mod 6) and v ≡1 (mod 2)
⋄if λ ≡0 (mod 6) and v ̸= 2;
• (v, 4, λ) design: exists for all v and λ satisfying the necessary conditions given in
Fact 5;
• (v, 5, λ) design:
exists for all v satisfying the necessary conditions given in Fact
5 except for the case v = 15, λ = 2;
• (v, 6, λ) design:
exists for all v satisfying the necessary conditions given in Fact
5, if λ > 1;
• (v, 6, 1) design:
exists for all v ≡1 or 6 (mod 15), v ≥31, v ̸∈{36, 46}, with 29
possible exceptions, the largest being 801. (The ﬁrst few open cases are 51, 61,
81, and 166.)
9. Existence of Steiner triple systems:
A Steiner triple system with v points exists if
and only if v ≡1 or 3 (mod 6). (Kirkman)
10. Wilson’s asymptotic existence theorem: Given k and λ, there exists a v0(k, λ) such
that a (v, k, λ) design exists for all v ≥v0(k, λ) that satisfy the necessary conditions
given in Fact 5 and make b and r integral. It is known that v0(k, λ) < exp(exp(kk2)).
11. Assume that (V, B) is a (v, k, λ) design. Let B = {V −B | B ∈B}. Then (V, B) is
a
 v, v −k, λ (v−k)(v−k−1)
k(k−1)

design, the complement of (V, B).

Section 12.1
BLOCK DESIGNS
887
12. Given two Steiner triple systems with v1 and v2 points, respectively, a Steiner triple
system with v1v2 points can be constructed as follows: Let an STS(v1) be deﬁned on the
point set {x1, . . . , xv1} and an STS(v2) be deﬁned on the point set {y1, . . . , yv2}. Deﬁne
an STS(v1v2) on the point set {zij | 1 ≤i ≤v1, 1 ≤j ≤v2} where zmnzpqzrs is a block
in STS(v1v2) if and only if one of the following holds:
• m = p = r and ynyqys is a block in STS(v2);
• n = q = s and xmxpxr is a block in STS(v1);
• xmxpxr is a block in STS(v1) and ynyqys is a block in STS(v2).
13. The following table lists diﬀerent types of block designs and their features.
size of
#
name
block
subset
times
other
size
covered
covered
properties
balanced incomplete block design
k
2
λ
BIBD §12.1.1
pairwise balanced design PBD
various
2
λ
also called a lin-
§12.1.6
ear space if λ = 1
Steiner triple system STS
3
2
1
§12.1.1, §12.1.5
Kirkman triple system KTS
3
2
1
resolvable
§12.1.4
resolvable balanced incomplete
k
2
λ
resolvable
block design RBIBD §12.1.4
projective plane PG(2, q)
q + 1
2
1
#points = #blocks
§12.2.3
= q2 + q + 1
aﬃne plane AG(2, q) §12.2.3
q
2
1
resolvable
symmetric design SBIBD
k
2
λ
#points = #blocks
§12.2.2
t-design t-(v, k, λ) §12.1.5
k
t ≥2
λ
Steiner system S(t, v, k) §12.1.5
k
t ≥2
1
Examples:
1. The following is a (4, 4, 3, 3, 2) design: X = {a, b, c, d}, blocks {abc, abd, acd, bcd}.
2. Aﬃne plane of order 3 [a (9, 3, 1) design]:
Here the point set is X = {0, . . ., 8} and
the block set is A = {012, 345, 678, 036, 147, 258, 048, 156, 237, 057, 138, 246}. Also see
§12.2.3. This design is known as AG(2, 3). This is a Steiner triple system.
3. Each of the following is a Steiner triple system (a (v, 3, 1) design). In each of the
following a set of base blocks Bi = {bi1, bi2, bi3} in the group Zv is given. To get all the
blocks of the design, take all distinct translates Bi + g = {bi1 + g, bi2 + g, bi3 + g}, for all
g ∈Zv, for each of the base blocks Bi.
v = 7: {0, 1, 3} (mod 7)
[Fano plane]
v = 15: {0, 1, 4} {0, 2, 8} {0, 5, 10} (mod 15) [The last base block has only 5 (= v
3)
distinct translates. This is a short orbit and occurs for all orders v ≡3 (mod 6).]
v = 19: {0, 1, 4} {0, 2, 9} {0, 5, 11} (mod 19)

888
Chapter 12
COMBINATORIAL DESIGNS
v = 21: {0, 1, 3} {0, 4, 12} {0, 5, 11} {0, 7, 14} (mod 21)
v = 25: {0, 1, 3} {0, 4, 11} {0, 5, 13} {0, 6, 15} (mod 25)
v = 27: {0, 1, 3} {0, 4, 11} {0, 5, 15} {0, 6, 14} {0, 9, 18} (mod 27)
v = 31: {0, 1, 3} {0, 4, 11} {0, 5, 15} {0, 6, 18} {0, 8, 17} (mod 31)
v = 33: {0, 1, 3} {0, 4, 10} {0, 5, 18} {0, 7, 19} {0, 8, 17} {0, 11, 22} (mod 33)
v = 37: {0, 1, 3} {0, 4, 9} {0, 6, 21} {0, 7, 18} {0, 8, 25} {0, 10, 24} (mod 37)
v = 39: {0, 1, 3} {0, 4, 9} {0, 6, 20} {0, 7, 18} {0, 8, 23} {0, 10, 22} {0, 13, 26} (mod 39).
4. Fano plane or projective plane of order 2, PG(2, 2): A (7, 7, 3, 3, 1) design with point
set X = {0, . . . , 6} and block set A = {013, 124, 235, 346, 450, 561, 602}, shown in the
following ﬁgure. (Often, as here, a block {a, b, c} is written as abc.) Also see §12.2.3.
(Gino Fano, 1871–1952)
The incidence matrix of the Fano plane is












1
1
0
1
0
0
0
0
1
1
0
1
0
0
0
0
1
1
0
1
0
0
0
0
1
1
0
1
1
0
0
0
1
1
0
0
1
0
0
0
1
1
1
0
1
0
0
0
1












12.1.2
ISOMORPHISM AND AUTOMORPHISM
Deﬁnitions:
Two designs (V, B) and (W, C) are isomorphic if there is a bijection ψ: V →W under
which ψ(B) = {ψ(x) | x ∈B} occurs as a block in C the same number of times that B
occurs as a block in B. Such a bijection is an isomorphism.
An automorphism of a design D is an isomorphism from D onto D.
The automorphism group of a design D is the set of all automorphisms for D with
composition as the group operation.
Facts:
1. Nonisomorphic Steiner triple systems of order v have been enumerated for v ≤19.
Up to isomorphism, there are unique designs of order 3, 7, and 9; there are precisely
two nonisomorphic designs of order 13, and 80 of order 15. At that point, an explosion
occurs: the number of nonisomorphic STS(19) is 11,084,874,829.
2. The number of nonisomorphic STS(v) is at least (e−5v)
v2/6 for large v. (Wilson)

Section 12.1
BLOCK DESIGNS
889
3. The following table lists the parameter sets (v, k, λ) that satisfy the necessary condi-
tions for the existence of a block design, with r ≤15 and 3 ≤k ≤v
2. The parameter sets
are ordered lexicographically across the rows of the table by r, k and λ (in this order).
The column N contains the number of pairwise nonisomorphic (v, k, λ) designs or the
best known lower bound for this number. The notation “?” indicates that no design
with these parameters is known to exist, but that existence has not been ruled out.
v
k
λ
N
v
k
λ
N
v
k
λ
N
7
3
1
1
9
3
1
1
13
4
1
1
6
3
2
1
16
4
1
1
21
5
1
1
11
5
2
1
13
3
1
2
7
3
2
4
10
4
2
3
25
5
1
1
31
6
1
1
16
6
2
3
15
3
1
80
8
4
3
4
15
5
2
0
36
6
1
0
43
7
1
0
22
7
2
0
15
7
3
5
9
3
2
36
25
4
1
18
13
4
2
2,461
9
4
3
11
21
6
2
0
49
7
1
1
57
8
1
1
29
8
2
0
19
3
1
11,084,874,829
10
3
2
960
7
3
3
10
28
4
1
≥4,747
10
5
4
21
46
6
1
0
16
6
3
18,920
28
7
2
8
64
8
1
1
73
9
1
1
37
9
2
4
25
9
3
78
19
9
4
6
21
3
1
≥6.2×107
6
3
4
4
16
4
2
≥2.2×106
41
5
1
≥15
21
5
2
≥22,998
11
5
4
4,393
51
6
1
?
21
7
3
3,809
36
8
2
0
81
9
1
7
91
10
1
4
46
10
2
0
31
10
3
151
12
3
2
242,995,846
12
4
3
≥17,172,470
45
5
1
≥16
12
6
5
11,603
45
9
2
≥16
100
10
1
0
111
11
1
0
56
11
2
≥5
23
11
5
1,106
25
3
1
≥1014
13
3
2
≥1,897,376
9
3
3
22,521
7
3
4
35
37
4
1
≥51,402
19
4
2
≥423
13
4
3
≥3,702
10
4
4
13,769,944
25
5
2
≥118,884
61
6
1
?
31
6
2
≥72
21
6
3
≥236
16
6
4
≥111
13
6
5
19,072,802
22
8
4
0
33
9
3
≥3,375
55
10
2
0
121
11
1
≥1
133
12
1
≥1
67
12
2
0
45
12
3
≥3,752
34
12
4
0
27
3
1
≥1011
40
4
1
≥106
66
6
1
≥1
14
7
6
15,111,019
27
9
4
≥2.45×108
40
10
3
?
66
11
2
≥2
144
12
1
?
157
13
1
?
79
13
2
≥2
53
13
3
0
40
13
4
≥1,108,800
27
13
6
208,310
15
3
2
≥685,521
22
4
2
≥7,921
8
4
6
2,310
15
5
4
≥896
36
6
2
≥5
15
6
5
≥117
85
7
1
?
43
7
2
≥4
29
7
3
≥1
22
7
4
≥3,393
15
7
6
≥57,810
78
12
2
0
169
13
1
≥1
183
14
1
≥1
92
14
2
0
31
3
1
≥6×1016
16
3
2
≥1013
11
3
3
≥436,800
7
3
5
109
6
3
6
6
16
4
3
≥6×105
61
5
1
≥10
31
5
2
≥1
21
5
3
≥109
16
5
4
≥294
13
5
5
≥76
11
5
6
≥127
76
6
1
≥1
26
6
3
≥1
16
6
5
≥25
91
7
1
≥2
16
8
7
≥9×107
21
9
6
≥104
136
10
1
?
46
10
3
?
28
10
5
≥3
56
12
3
≥4
91
13
2
0
196
14
1
0
211
15
1
0
106
15
2
0
71
15
3
≥72
43
15
5
0
36
15
6
≥25,634

890
Chapter 12
COMBINATORIAL DESIGNS
12.1.3
SUBDESIGNS
Deﬁnition:
Let Y be a subset of w points in a (v, k, λ) design. If every block of the BIBD contains
0, 1, or k of the points in Y , then a (w, k, λ) design is obtained by taking those blocks
that contain k points from Y . This BIBD on w points is a subdesign, called a (w, k, λ)
subdesign.
Facts:
1. If there is a (v, k, 1) design containing a (w, k, 1) subdesign, then v ≥(k −1)w + 1.
(The parameter lists (v, k, 1) and (w, k, 1) must satisfy the necessary conditions of §12.1.1,
Fact 6.)
2. In the cases k = 3 and k = 4, the necessary conditions of §12.1.1, Fact 5 for the
presence of a subdesign are suﬃcient. That is, in the case of k = 3, for all v ≥2w + 1,
with both v, w ≡1 or 3 mod 6), there exists a (v, 3, 1) design that contains a (w, 3, 1)
subdesign. In the case k = 4, for all v ≥3w + 1, with both v, w ≡1 or 4 (mod 12) there
exists a (v, 4, 1) design that contains a (w, 4, 1) subdesign.
Example:
1. A construction for a Steiner triple system of order 2v+1 given a Steiner triple system
of order v:
A variant of this construction dates back at least to Thomas P. Kirkman
in 1847. The original STS(v) is a subdesign of the resulting STS(2v + 1).
Let (X, A) be an STS(v) with X = {x0, x1, . . . , xv−1}. For each i = 0, 1, . . . , v −1, let
Fi = {{x+i, −x+i} | x ∈Zv, x ̸= 0}∪{i, ∞}. Then for each i = 0, , . . . , v −1, construct
the triples {a, b, xi} where {a, b} ∈Fi. The set of all such triples in addition to the
original triples in A is the desired STS(2v + 1) on the point set X ∪{0, 1, . . ., v, ∞}.
For v = 7, the following STS(15) is obtained. The last row of triples is an STS(7).
{0, ∞, x0} {1, 6, x0} {2, 5, x0} {3, 4, x0}
{1, ∞, x1} {2, 0, x1} {3, 6, x1} {4, 5, x1}
{2, ∞, x2} {3, 1, x2} {4, 0, x2} {5, 6, x2}
{3, ∞, x3} {4, 2, x3} {5, 1, x3} {6, 0, x3}
{4, ∞, x4} {5, 3, x4} {6, 2, x4} {0, 1, x4}
{5, ∞, x5} {6, 4, x5} {0, 3, x5} {1, 2, x5}
{6, ∞, x6} {0, 5, x6} {1, 4, x6} {2, 3, x6}
{x0, x1, x3} {x1, x2, x4} {x2, x3, x5} {x3, x4, x6} {x4, x5, x0} {x5, x6, x1} {x6, x0, x2}.
12.1.4
RESOLVABLE DESIGNS
Deﬁnitions:
A parallel class is a collection of blocks that partition the point set.
A resolution of a BIBD is a partition of the family of blocks into parallel classes. A
resolution contains exactly r parallel classes.
A BIBD is resolvable, denoted RBIBD, if it has at least one resolution.

Section 12.1
BLOCK DESIGNS
891
A (v, 3, 1) RBIBD, together with a resolution of it, is a Kirkman triple system, written
KTS(v).
Facts:
1. Necessary conditions for existence of a (v, k, λ) RBIBD are
• λ(v −1) ≡0 (mod (k −1));
• v ≡0 (mod k).
2. If a (v, k, λ) RBIBD exists, then b ≥v + r −1 where b is the number of blocks.
When b = v + r −1 (or equivalently, r = k + λ) the RBIBD has the property that two
nonparallel lines intersect in exactly k2
v points. (R. C. Bose, 1901–1987)
3. A KTS(v) exists if and only if v ≡3 (mod 6).
4. The following table summarizes the current state of knowledge concerning the exis-
tence of resolvable designs.
For the values of k and λ given, the number of parameter sets (v, k, λ) satisfying all
necessary conditions for the existence of a resolvable (v, k, λ) design for which the ex-
istence of a resolvable (v, k, λ) design is not known is given under the column headed
“exceptions”. The column headed “largest possible exception” gives the largest v satis-
fying the necessary conditions for the existence of a resolvable (v, k, λ) design for which
a resolvable (v, k, λ) design is not known.
k
λ
exceptions
largest possible
exception
3
1
none
3
2
6
4
1
none
4
3
none
5
1
645
5
2
15
395
5
4
10,15
195
6
5
none
6
10
none
7
6
14
462
8
1
24,480
8
7
1,488
Example:
1. Kirkman’s schoolgirl problem: In 1850, Kirkman posed the following: ﬁfteen young
ladies in a school walk out three abreast for seven days in succession; it is required to
arrange them daily, so that no two walk twice abreast. (Thomas P. Kirkman, 1806–1895)
This is equivalent to ﬁnding a resolution of some (15, 3, 1) design (or a KTS(15)). Seven
nonisomorphic solutions exist; the following is one solution to Kirkman’s schoolgirl prob-
lem:

892
Chapter 12
COMBINATORIAL DESIGNS
Monday
Tuesday
Wednesday
Thursday
Friday
Saturday
Sunday
9,10,12
10,11,13
11,12,14
12,13,15
13,14,9
14,15,10
15,9,11
15,8,1
9,8,2
10,8,3
11,8,4
12,8,5
13,8,6
14,8,7
13,2,7
14,3,1
15,4,2
9,5,3
10,6,4
11,7,5
12,1,6
11,3,6
12,4,7
13,5,1
14,6,2
15,7,3
9,1,4
10,2,5
14,4,5
15,5,6
9,6,7
10,7,1
11,1,2
12,2,3
13,3,4
12.1.5
t-DESIGNS AND STEINER SYSTEMS
Deﬁnitions:
A t-(v, k, λ) design (also denoted Sλ(t, k, v) and written t-design) is a pair (X, A)
that satisﬁes the following properties:
• X is a set of v elements (called points);
• A is a family of subsets (blocks) of X, each of cardinality k;
• every t-subset of distinct points occurs in exactly λ blocks.
A t-design is simple if it contains no repeated blocks.
A Steiner system is a t-(v, k, 1) design.
A Steiner triple system, denoted STS(v), is a (v, 3, 1) design. (See §12.1.1.)
A Steiner quadruple system, denoted SQS(v), is a 3-(v, 4, 1) design.
Facts:
1. A (v, k, λ) design (a BIBD) is a 2-(v, k, λ) design.
2. If s < t, then a t-(v, k, λ) design is also an s-(v, k, µ) design, where µ = λ(
v−s
t−s)
(k−s
t−s).
3. As a consequence of Fact 2, a t-(v, k, λ) design only exists if λ
 v−s
t−s

≡0 (mod
 k−s
t−s

)
for every 0 ≤s ≤t; these are the divisibility conditions.
4. t-(v, k, λ) designs exist for all t. A t-(v, t + 1, ((t + 1)!)2t+1) design exists if v ≥t + 1
and v ≡t (mod [(t + 1)!]2t+1). (Teirlinck)
5. If a t-(v, k, λ) design exists, where t = 2s is even, then the number of blocks b ≥
 v
s

.
(This generalizes Fisher’s inequality, §12.1.1, Fact 5.)
6. When λ = 1, until 2014 t-designs were known only for t ≤5, and the construction of a
6-(v, k, 1) design was one of the outstanding open problems in the theory of combinatorial
designs. In 2014, Keevash established that for every t and k, t-(v, k, 1) designs exist when
v satiﬁes the divisibility conditions (from Fact 3) and v is suﬃciently large.
7. Much less is known about the existence of t-(v, k, λ) designs with t ≥3 compared to
BIBDs:
• For t = 3, several inﬁnite families are known.
• For every prime power q and d ≥2, there exists a 3-(qd+1, q+1, 1) design, known
as an inversive geometry. When d = 2, these designs are known as inversive
planes.
• A 3-(v, 4, 1) design (Steiner quadruple system) exists if and only if v ≡2 or
4 (mod 6).

Section 12.1
BLOCK DESIGNS
893
Examples:
1. The following is a 3-(8, 4, 1) design:
X = {∞, 0, 1, 2, 3, 4, 5, 6}
A = {{0, 1, 3, ∞}, {1, 2, 4, ∞}, {2, 3, 5, ∞}, {3, 4, 6, ∞}, {4, 5, 0, ∞},
{5, 6, 1, ∞}, {6, 0, 2, ∞}, {2, 4, 5, 6}, {3, 5, 6, 0}, {4, 6, 0, 1},
{5, 0, 1, 2}, {6, 1, 2, 3}, {0, 2, 3, 4}, {1, 3, 4, 5}}
2. Simple t-designs (t = 4, 5):
For t = 4 or 5 and v ≤30, the only t-(v, k, 1) designs
known to exist are those having the following parameters:
4-(11, 5, 1)
5-(12, 6, 1)
4-(23, 7, 1)
5-(24, 8, 1)
4-(27, 6, 1)
5-(28, 7, 1).
3. Simple t-designs (t = 6): For t = 6 and v ≤20, the only t-(v, k, λ) designs known to
exist are those having the following parameters:
6-(14, 7, 4)
6-(19, 7, 4)
6-(19, 7, 6)
6-(20, 9, 112)
6-(22, 7, 8)
6-(20,10,7m), m ∈{48, 58, 62, 63, 67, 68}.
12.1.6
PAIRWISE BALANCED DESIGNS
Deﬁnitions:
Given a set K of positive integers and a positive integer λ, a pairwise balanced design,
written (v, K, λ)-PBD, is an ordered pair (X, A) where X is a set (of points) of size v
and A is a collection of subsets (blocks) of X such that
• every pair of elements of X occurs together in exactly λ blocks;
• for every block A ∈A, |A| ∈K.
When λ = 1, λ can be omitted from the notation and the design is called a (v, K)-PBD
or a ﬁnite linear space.
Given a set K of positive integers, let B(K) denote the set of positive integers v for
which there exists a (v, K)-PBD. The mapping K →B(K) is a closure operation on
the set of subsets of the positive integers, as it satisﬁes the following properties:
• K ⊆B(K);
• K1 ⊆K2 ⇒B(K1) ⊆B(K2);
• B(B(K)) = B(K).
The set B(K) is the closure of the set K.
If K is any set of positive integers, then K is PBD-closed (or closed) if B(K) = K.
If K is a closed set, then there exists a ﬁnite subset J ⊆K such that K = B(J). This
set J is a generating set for the PBD-closed set K.
If J is a generating set for K and if s ∈J is such that J −{s} is also a generating set
for K, then s is inessential in K; otherwise s is essential.
A basis is a generating set consisting of essential elements.

894
Chapter 12
COMBINATORIAL DESIGNS
Facts:
1. A (v, k, λ) design is a special case of a PBD in which the blocks are only permitted
to be of one size, k.
2. Necessary conditions for existence:
The existence of a (v, K)-PBD (with v > 0)
implies
• v ≡1 (mod α(K))
• v(v −1) ≡0 (mod β(K))
where α(K) is the greatest common divisor of the integers {k −1 | k ∈K} and β(K) is
the greatest common divisor of the integers {k(k −1) | k ∈K}.
3. Asymptotic existence: Given K, there exists a constant ck such that a (v, K)-PBD
exists for all v ≥ck that satisfy the necessary conditions of Fact 2. The constant ck is, in
general, unspeciﬁed. In practice, considerable further work is usually required to obtain
a concrete upper bound on ck.
Examples:
1. The following is a (10, {3, 4})-PBD:
{1, 2, 3, 4}, {1, 5, 6, 7}, {1, 8, 9, 10}, {2, 5, 8}, {2, 6, 9}, {2, 7, 10}
{3, 5, 10}, {3, 6, 8}, {3, 7, 9}, {4, 5, 9}, {4, 6, 10}, {4, 7, 8}
2. The following table lists closures of some subsets of {3, 4, . . ., 8}. From Fact 3, for
a given set K there are only a ﬁnite number of values of v (satisfying the necessary
conditions) for which there does not exist a (v, K)-PBD. These exceptional cases are
listed in this table for some small sets K. Since 7 ∈B(3), it is not necessary to include 7
in the list of sets whose closures are given, when 3 is present. Genuine exceptions (values
of v satisfying the necessary conditions for the existence of a (v, K)-PBD for which it
has been proven that no such design can exist) are shown in boldface, while possible
exceptions (neither existence or nonexistence of a (v, K)-PBD is known) are shown in
normal type.
subset K
necessary
exceptions
conditions
3
1, 3 mod 6
−
3,4
0, 1 mod 3
−
3,5
1 mod 2
−
3,6
0, 1 mod 3
4, 10, 12, 22
3,8
N (natural
4, 5, 6, 10, 11, 12, 14, 16, 17, 18, 20, 23, 26, 28, 29,
numbers)
30, 34, 35, 36, 38
3,4,5
N
6, 8
3,4,6
0, 1 mod 3
−
3,4,8
N
5, 6, 11, 14, 17
3,5,6
N
4, 8, 10, 12, 14, 20, 22
3,5,8
N
4, 6, 10, 12, 14, 16, 18, 20, 26, 28, 30, 34
3,6,8
N
4, 5, 10, 11, 12, 14, 17, 20, 23
3,4,5,6
N
8
3,4,5,8
N
6
3,4,6,8
N
5, 11, 14, 17

Section 12.1
BLOCK DESIGNS
895
subset K
necessary
exceptions
conditions
3,5,6,8
N
4, 10, 14, 20
3,4,5,6,8
N
−
4
1, 4 mod 12
−
4,5
0, 1 mod 4
8, 9, 12
4,6
0, 1 mod 3
7, 9, 10, 12, 15, 18, 19, 22, 24, 27, 33, 34, 39, 45, 46,
51, 87
4,7
1 mod 3
10, 19
4,8
0, 1 mod 4
5, 9, 12, 17, 20, 21, 24, 33, 41, 44, 45, 48, 53, 60, 65,
69, 77, 89
4,5,6
N
7, 8, 9, 10, 11, 12, 14, 15, 18, 19, 23
4,5,7
N
6, 8, 9, 10, 11, 12, 14, 15, 18, 19, 23, 26, 27, 30, 39,
42, 51, 54
4,5,8
0, 1 mod 4
9, 12
4,6,7
0, 1 mod 3
5, 9, 10, 12, 15, 19, 24, 27, 33, 45, 87
4,6,8
N
5, 7, 9, 10, 11, 12, 14, 15, 17, 18, 19, 20, 22, 23, 24,
26, 27, 33, 34, 35, 39, 41, 47, 50, 51, 53, 59, 62, 65,
71, 77, 87, 95, 110, 131, 170
4,7,8
N
5, 6, 9, 10, 11, 12, 14, 15, 17, 18, 19, 20, 21, 23, 24,
26, 27, 30, 33, 35, 38, 39, 41, 42, 44, 45, 47, 48, 51,
54, 59, 62, 65, 66, 69, 74, 75, 77, 78, 83, 87, 89, 90, 102,
110, 114, 126, 131, 138, 143, 150, 162, 167, 174, 186
4,5,6,7
N
8, 9, 10, 11, 12, 14, 15, 18, 19, 23
4,5,6,8
N
7, 9, 10, 11, 12, 14, 15, 18, 19, 23
4,5,7,8
N
6, 9, 10, 11, 12, 14, 15, 18, 19, 23, 26, 27, 30, 42, 51
4,6,7,8
N
5, 9, 10, 11, 12, 14, 15, 17, 18, 19, 20, 23, 24, 26,
27, 33, 35, 41, 65, 77, 131
4,5,6,7,8
N
9, 10, 11, 12, 14, 15, 18, 19, 23
5,6
0, 1 mod 5
10, 11, 15, 16, 20, 35, 40, 50, 51, 80
12.1.7
GROUP DIVISIBLE DESIGNS AND TRANSVERSAL DESIGNS
Deﬁnitions:
A group divisible design (or (K, λ)-GDD) is a triple (X, G, A) where X is a set (of
points), G is a partition of X into at least two subsets (called groups), A is a family of
subsets of X (called blocks) such that
• if A in A, then |A| ∈K;
• a group and a block contain at most one common point;
• every pair of points from distinct groups occurs in exactly λ blocks.
If λ = 1, a (K, λ)-GDD is often denoted by K-GDD. If K = {k}, a K-GDD is written
k-GDD.

896
Chapter 12
COMBINATORIAL DESIGNS
The group-type (or type) of a GDD is the multiset {|G| | G ∈G}. Usually an “expo-
nential notation” is used to describe the type of a GDD: a GDD of type tu1
1 tu2
2 . . . tuk
k
is
a GDD where there are ui groups of size ti for 1 ≤i ≤k.
A transversal design TD(k, n) is a k-GDD of type nk (that is, one having k groups of
size n and uniform block size k).
Fact:
1. The existence of a TD(k, n) is equivalent to the existence of k−2 mutually orthogonal
Latin squares of side n. (§12.3.2.)
12.2
SYMMETRIC DESIGNS AND FINITE GEOMETRIES
12.2.1
FINITE GEOMETRIES
Deﬁnitions:
A ﬁnite incidence structure (V, B, I) consists of a ﬁnite set V of points, a ﬁnite set B
of lines, and an incidence relation I between them.
(Equivalently, a ﬁnite incidence
structure is a pair (V, B), where B = { {v | (v, b) ∈I} | b ∈B }. In this case, lines are
sets of points.)
The dual incidence structure is obtained by interchanging the roles of points and lines.
Let F be a ﬁnite ﬁeld, and let S be an (n+1)-dimensional vector space over F. The set of
all subspaces of S is the projective space of projective dimension n over F. When F
is the Galois ﬁeld GF(q) (see §5.6.3), the projective space of projective dimension n is
denoted PG(n, q).
Subspaces of projective dimensions 0, 1, 2, and n are points, lines, planes, and hy-
perplanes, respectively; in general, subspaces of projective dimension t are t-ﬂats.
PGt(n, q) denotes the incidence structure of points and t-ﬂats in PG(n, q) (incidence
is just containment as subspaces).
Often, PG1(n, q) is denoted PG(n, q) (taking the
structure of points and lines as the natural geometry of the underlying space).
Let S be an n-dimensional vector space over a ﬁnite ﬁeld F. The set of all cosets of
subspaces of S is the aﬃne space of aﬃne dimension n over F. When F is the
Galois ﬁeld GF(q), the aﬃne space of aﬃne dimension n is denoted AG(n, q).
Cosets of subspaces of (aﬃne) dimension 0, 1, 2, and n−1 of an aﬃne space of aﬃne
dimension n are points, lines, planes, and hyperplanes, respectively.
In general,
cosets of subspaces of aﬃne dimension t are t-ﬂats. AGt(n, q) denotes the incidence
structure of points and t-ﬂats in AG(n, q) (incidence is containment). Often, AG1(n, q)
is denoted AG(n, q) (taking the structure of points and lines as the natural geometry of
the underlying space).
Note:
The term ﬁnite geometry often just means ﬁnite incidence structure. However,
incidence structures are often too unstructured to be of much (geometric) interest. Hence
the term is sometimes reserved to cover only incidence structures satisfying additional
axioms such as those given in §12.2.3, Fact 3 for projective planes.

Section 12.2
SYMMETRIC DESIGNS AND FINITE GEOMETRIES
897
Facts:
1. Projective geometries: For q a prime power and 1 ≤t < n, PGt(n, q) is a

qn+1−1
q−1 , qt+1−1
q−1 , (qn−1−1)(qn−2−1)...(qn−t−1−1)
(qt−1−1)(qt−2−1)...(q−1)

design.
2. Aﬃne geometries: For q a prime power and 1 ≤t < n, AGt(n, q) is a

qn, qt, (qn−1−1)(qn−2−1)...(qn−t−1−1)
(qt−1−1)(qt−2−1)...(q−1)

design.
12.2.2
SYMMETRIC DESIGNS
Deﬁnitions:
A (v, b, r, k, λ) block design is symmetric if the number of points equals the number of
blocks, that is, v = b.
A symmetric design with λ = 2 is a biplane. The parameters of a biplane are v =
 k
2

+1,
k = k, λ = 2.
Facts:
1. In a symmetric (v, k, λ) design, r = k.
2. If a symmetric (v, k, λ) design exists, then
• if v is even, then k −λ is a perfect square;
• if v is odd, then the Diophantine equation x2 = (k −λ)y2 + (−1)(v−1)/2λz2 has
a solution in integers, not all of which are zero.
(Bruck-Ryser-Chowla; the
theorem is often referred to as BRC.)
3. For every positive integer k there is a symmetric (2k+2−1, 2k+1−1, 2k−1) block de-
sign.
4. If p is prime and k is a positive integer, there is a symmetric (p2k + pk + 1, pk + 1, 1)
block design.
5. In a symmetric design any two blocks intersect in exactly λ points.
6. The dual incidence structure obtained by interchanging the roles of points and blocks
is also a BIBD with the same parameters (hence the term symmetric).
7. The dual of a symmetric design need not be isomorphic to the original design.
8. Given a symmetric (v, k, λ) design, and a block A of this design, if the points not
in A are deleted from all blocks which intersect A, the design obtained is the derived
design. Its parameters are (k, v −1, k −1, λ, λ −1).
9. Given a symmetric (v, k, λ) design, and given a block A of this design, delete the
block A, and delete all points in A from all other blocks. The resulting design is the
residual design, and has parameters (v −k, v −1, k, k −λ, λ). (See Example 1.)
10. Any (v −k, v −1, k, k −λ, λ) design is a quasi-residual design.
11. Any quasi-residual BIBD with λ = 1 or 2 is residual (Hall-Connor); but for λ = 3,
there are examples of quasi-residual BIBDs that are not residual.
12. If there is a symmetric (v, k, λ) design with n = k −λ, then 4n−1 ≤v ≤n2 +n+1.
When v = 4n −1 it is a Hadamard design; when v = n2 + n + 1 it is a projective plane.
13. The only known biplanes have parameters (7, 4, 2), (11, 5, 2), (16, 6, 2), (37, 9, 2),
(56, 11, 2), and (79, 13, 2).

898
Chapter 12
COMBINATORIAL DESIGNS
14. The only known symmetric designs with λ = 3 have parameters (11, 6, 3), (15, 7, 3),
(25, 9, 3), (31, 10, 3), (45, 12, 3), and (71, 15, 3).
15. Although inﬁnitely many symmetric designs with λ = 1 are known, there is no other
value of λ for which this is known to be true.
Example:
1. In the symmetric (15, 7, 3) design in the following table, if the block b0 is removed
and if all the points in b0 are removed from the blocks b1, . . . , b14, the resulting design is
the residual design. It has parameters (8, 4, 3) and its blocks are given on the right.
b0 :
0
1
2
3
4
5
6
b1 :
0
1
2
7
8
9
10
b2 :
0
1
2
11
12
13
14
b3 :
0
3
4
7
8
11
12
b4 :
0
3
4
9
10
13
14
b5 :
0
5
6
7
8
13
14
b6 :
0
5
6
9
10
11
12
b7 :
1
3
5
7
9
11
13
b8 :
1
3
6
7
10
12
14
b9 :
1
4
5
8
10
11
14
b10 :
1
4
6
8
9
12
13
b11 :
2
3
5
8
10
12
13
b12 :
2
3
6
8
9
11
14
b13 :
2
4
5
7
9
12
14
b14 :
2
4
6
7
10
11
13
12.2.3
PROJECTIVE AND AFFINE PLANES
Deﬁnitions:
A projective plane is a ﬁnite set of points and a set of subsets of points (called lines)
such that
• every two points lie on exactly one line;
• every two lines intersect in exactly one point;
• there are four points with no three collinear.
An aﬃne plane is a set of points and a set of subsets of points (called lines) such that
• every two points lie on exactly one line;
• if a point does not lie on a line L, there is exactly one line through the point that
does not intersect L;
• there are three points that are not collinear.
Facts:
1. A ﬁnite projective plane is a symmetric (n2 +n+1, n+1, 1) design, for some positive
integer n, called the order of the projective plane. The projective plane has n2 + n + 1
points and n2 + n + 1 lines.
Each point lies on n + 1 lines and every line contains
exactly n + 1 points.

Section 12.2
SYMMETRIC DESIGNS AND FINITE GEOMETRIES
899
2. Principle of duality:
Given any statement about ﬁnite projective planes that is a
theorem, the dual statement (obtained by interchanging “point” and “line” and inter-
changing “point lying on a line” with “line passing through a point”) is a theorem.
3. Any symmetric design with λ = 1 is a projective plane.
4. The existence of a projective plane of order n is equivalent to the existence of a set
of n−1 mutually orthogonal Latin squares (MOLS) of side n.
5. Existence of projective planes: Very little is known about the existence of projective
planes:
• There exists a projective plane of order pk whenever p is prime and k is a positive
integer. (See Fact 10.)
• There is no projective plane known for any order n that is not a power of a prime.
The smallest open order is 12.
• There is no projective plane of order 10 or any n ≡6 (mod 8).
• There exist projective planes for every order q2 and q3 when q is a prime power
that cannot be constructed by the method of Fact 9; such planes are called
nondesarguesian.
• There are four nonisomorphic projective planes of order 9, three of which are
nondesarguesian.
• The following table summarizes the known facts about the existence and number
of projective planes of order n, for 1 ≤n ≤12:
order
2
3
4
5
6
7
8
9
10
11
12
number of projective planes
1
1
1
1
0
1
1
4
0
≥1
?
6. The proof by Lam, Thiel, and Swiercz in 1989 that there is no projective plane of
order 10 involved great amounts of computer power and time. For details, see [CoDi07].
7. The existence of a projective plane of order n is equivalent to the existence of an
aﬃne plane of order n.
8. A ﬁnite aﬃne plane is a (n2, n, 1) design, for some positive integer n. The aﬃne plane
has n2 points and n2 + n lines. Each point lies on n + 1 lines and every line contains
exactly n points. The integer n is the order of the aﬃne plane.
9. Any aﬃne plane of order n has the property that the lines can be partitioned into
n + 1 parallel classes each containing n lines and hence is a resolvable block design.
10. A direct construction of a projective plane of every order q = pk, when p is prime
and k is a positive integer: Consider the three-dimensional vector space F3
q over GF(q).
This vector space contains q3−1
q−1 = q2 + q + 1 1-dimensional subspaces (lines through
the origin (0, 0, 0)) and an equal number of 2-dimensional subspaces (planes through the
origin). Now construct an incidence structure where the points are the 1-dimensional
subspaces, the lines are the 2-dimensional subspaces, and a point is on a line if the
1-dimensional subspace (associated with the point) is contained in the 2-dimensional
subspace (associated with the line). This structure satisﬁes the axioms and thus is a
projective plane (of order q). Projective planes such as this, coming from ﬁnite ﬁelds via
the construction in this example, are desarguesian planes.
11. A construction of a projective plane of order n from an aﬃne plane of order n: To
construct the projective plane of order n from the aﬃne plane of order n, use the fact that
the lines of the aﬃne plane can be partitioned into n+1 parallel classes each containing n
lines. To each line in the ith parallel class adjoin the new symbol ∞i. Add one new line,

900
Chapter 12
COMBINATORIAL DESIGNS
namely {∞1, ∞2, . . . , ∞n+1}. Now each line contains n + 1 points, there are n2 + n + 1
total points, and each pair of points is on a unique line—this is the projective plane of
order n.
12. A construction of an aﬃne plane of order n from a projective plane of order n:
The aﬃne plane of order n is the residual design of the projective plane of order n. See
§12.2.2, Fact 9 for the construction.
Examples:
1. The Fano plane (§12.1.1, Example 3) is the projective plane of order 2.
2. The aﬃne plane of order 2 is given in part (a) of the following ﬁgure. The set of
points is {1, 2, 3, 4}. The six lines are
{1, 2}
{3, 4}
{1, 3}
{2, 4}
{1, 4}
{2, 3}.
The three parallel classes are
{{1, 2}, {3, 4}}
{{1, 3}, {2, 4}}
{{1, 4}, {2, 3}}.
1
3
2
4
2
8
4
5
6
1
3
7
9
(b)
(a)
3. The aﬃne plane of order 3 is given in part (b) of the ﬁgure of Example 2. The set of
points is {1, 2, 3, . . ., 9}. The twelve lines (listed in order in four parallel classes of three
lines each) are
{1, 2, 3}, {4, 5, 6}, {7, 8, 9}
{1, 4, 7}, {2, 5, 8}, {3, 6, 9}
{1, 5, 9}, {6, 2, 7}, {4, 8, 3}
{3, 5, 7}, {2, 4, 9}, {8, 6, 1}.
4. Using the construction in Fact 11 on the aﬃne plane of order 3 (Example 3) yields
the projective plane of order 3 with thirteen points {1, 2, 3, 4, 5, 6, 7, 8, 9, ∞1, ∞2, ∞3, ∞4}
and thirteen lines:
{1, 2, 3, ∞1}
{4, 5, 6, ∞1}
{7, 8, 9, ∞1}
{1, 4, 7, ∞2}
{2, 5, 8, ∞2}
{3, 6, 9, ∞2}
{1, 5, 9, ∞3}
{6, 2, 7, ∞3}
{4, 8, 3, ∞3}
{3, 5, 7, ∞4}
{2, 4, 9, ∞4}
{8, 6, 1, ∞4}
{∞1, , ∞2, ∞3, ∞4}.

Section 12.2
SYMMETRIC DESIGNS AND FINITE GEOMETRIES
901
12.2.4
HADAMARD DESIGNS AND MATRICES
Deﬁnitions:
A Hadamard matrix H of order n is a square n × n matrix all of whose entries are ±1
that satisﬁes the property that H TH = nI where I is the n × n identity matrix and H T
is the transpose of H.
If M = (mij) is an m × p matrix and N = (nij) is an n × q matrix, the Kronecker
product is the mn × pq matrix M × N given by
M × N =






m11N
m12N
· · ·
m1pN
m21N
m22N
· · ·
m2pN
...
...
...
mm1N
mm2N
· · ·
mmpN






.
A Hadamard design of order 4n is a symmetric (4n −1, 2n −1, n −1) design. The
dimension of the Hadamard design is n.
Facts:
1. A necessary condition for the existence of a Hadamard matrix of order n is that
n = 1, n = 2, or n ≡0 (mod 4).
2. HH T = nI = H TH.
3. The rows [columns] of a Hadamard matrix are pairwise orthogonal (when considered
as vectors of length n).
4. If a row of a Hadamard matrix is multiplied by −1, the result is a Hadamard matrix.
Similarly, if a column of a Hadamard matrix is multiplied by −1, the result is a Hadamard
matrix.
5. By multiplying rows and columns of a Hadamard matrix by −1, a Hadamard matrix
can be obtained where the ﬁrst row and column consist entirely of +1s. A Hadamard
matrix of this type is normalized.
6. In a normalized Hadamard matrix of order 4n, every row and column (except the
ﬁrst) contains +1 and −1 exactly 2n times each.
7. Of all n × n matrices with entries from {−1, +1}, a Hadamard matrix H has the
maximal determinant. | det H| = nn/2.
8. The Kronecker product of two Hadamard matrices is a Hadamard matrix. Thus, if
there are Hadamard matrices of order m and n, then there is a Hadamard matrix of
order mn.
9. If there exist Hadamard matrices of orders 4m, 4n, 4p, 4q, then there exists a
Hadamard matrix of order 16mnpq. (Craigen, Seberry, and Zhang)
10. Let q be a positive integer. Then there exists a Hadamard matrix of order 2sq for
every s ≥⌊2 log2(q −3)⌋. (Seberry)
11. A Hadamard design of order 4n exists if and only if a Hadamard matrix of order 4n
exists. See the construction in Fact 15.
12. Hadamard’s conjecture: The fundamental question concerning Hadamard matrices
remains the existence question. The Hadamard conjecture is that there exist Hadamard
matrices of order 4n for all n ≥1. This remains unproved.

902
Chapter 12
COMBINATORIAL DESIGNS
13. Currently the smallest order for which the existence of a Hadamard matrix is open
is 668. Because of Fact 8 and the existence of a Hadamard matrix of order 2, if q is
an odd number and there exists a Hadamard matrix of order 2sq, then there exists a
Hadamard matrix of order 2tq for all t ≥s.
The following table gives values of t such that a Hadamard matrix of order 2tn is known,
for odd n < 999 (n is obtained by adding the three indices of the entry). The notation
“.” indicates t = 2.
000
100
200
300
400
500
600
700
800
900
00 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 .
10 . . . . . . . . . . . . . . . 3 . . . . . . . . 3 . . 3 . . . . . . . . . . . 4 . . . . . . . . 3 3
20 . . . . . . . . . . . 3 . . . . . . . . . . . . . . 3 . . . . . . . . 3 . . . . . 3 . . . . . . . .
30 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 . 3 . . . . . . . . 5 . . . . 3 . 4 . . .
40 . . . . . . . . . . . . . . . . . . 3 . . 3 . . . . . . . . . 3 . 3 . . . . . . . . . . . . . . 3 .
50 . . . . . . . . . . 3 . . . . . . . . 3 . . . . . . . . . . . . . . 4 3 . . . . . . . . 3 . . 3 . .
60 . . . . . . . . 3 . . . . . . . . . . . . . . . . . . . . . . . . . 3 . . . . . . 3 . . . . . . . .
70 . . . . . . . . . 3 . . . . . . . . . . . . . . 3 3 3 . . . . . . . . . . . . . . . . . . 5 . . . .
80 . . . . . . . . . . . 3 . . . . . . . . . . . 3 . . . . . . . . . . . . . . 3 3 . 3 . . . . . . . .
90 . . . . . 3 . . . . . . . . . . . . . . 3 . . . . . . . . 5 . . . . . . . . . . . . . . . 3 . . . .
1 3 5 7 9 1 3 5 7 9 1 3 5 7 9 1 3 5 7 9 1 3 5 7 9 1 3 5 7 9 1 3 5 7 9 1 3 5 7 9 1 3 5 7 9 1 3 5 7 9
14. A general construction for Hadamard matrices of order q+1 when q is an odd prime
power and q ≡3 (mod 4):
• construct a q × q matrix C = (cij) indexed by the elements of the ﬁeld GF(q) by
letting
cij =
(
1,
if i −j is a square in GF(q)
−1,
if i −j is not a square in GF(q);
• construct a Hadamard matrix H of order q + 1 from C by adding a ﬁrst column
of all −1s and then a top row of all 1s.
This method is used in Example 1 to construct the Hadamard matrix of order 12.
15. Constructing Hadamard designs from Hadamard matrices, and vice versa: Assume
that there exists a Hadamard matrix of order 4n. Let H be a normalized Hadamard
matrix of this order. Remove the ﬁrst row and column of H and replace every −1 in the
resulting matrix by a 0. The ﬁnal (4n −1) × (4n −1) matrix can be shown to be the
incidence matrix of a (4n −1, 2n −1, n −1) Hadamard design.
This process can be reversed to construct a Hadamard matrix from a Hadamard design.
Example:
1. The smallest examples of Hadamard matrices are the following:

1

,
 
1
1
1
−1
!
,

Section 12.2
SYMMETRIC DESIGNS AND FINITE GEOMETRIES
903





−1
1
1
1
1
−1
1
1
1
1
−1
1
1
1
1
−1




,














1
1
1
1
1
1
1
1
1
−1
1
−1
1
−1
1
−1
1
1
−1
−1
1
1
−1
−1
1
−1
−1
1
1
−1
−1
1
1
1
1
1
−1
−1
−1
−1
1
−1
1
−1
−1
1
−1
1
1
1
−1
−1
−1
−1
1
1
1
−1
−1
1
−1
1
1
−1














,























1
1
1
1
1
1
1
1
1
1
1
1
−1
1
1
−1
1
1
1
−1
−1
−1
1
−1
−1
−1
1
1
−1
1
1
1
−1
−1
−1
1
−1
1
−1
1
1
−1
1
1
1
−1
−1
−1
−1
−1
1
−1
1
1
−1
1
1
1
−1
−1
−1
−1
−1
1
−1
1
1
−1
1
1
1
−1
−1
−1
−1
−1
1
−1
1
−1
1
1
1
1
−1
1
−1
−1
−1
1
−1
1
1
−1
1
1
−1
1
1
−1
−1
−1
1
−1
1
1
−1
1
−1
1
1
1
−1
−1
−1
1
−1
1
1
1
−1
−1
1
1
1
−1
−1
−1
1
−1
1
1
−1
1
−1
1
1
1
−1
−1
−1
1
−1
1























.
12.2.5
DIFFERENCE SETS
Note: In this section, only diﬀerence sets in abelian groups are considered.
Deﬁnitions:
Let G be an additively written group of order v. A k-subset D of G is a (v, k, λ; n)
diﬀerence set of order n = k −λ if every nonzero element of G has exactly λ
representations as a diﬀerence d −d′ (d, d′ ∈D). The diﬀerence set is abelian, cyclic,
etc., if the group G has the respective property.
The development of a diﬀerence set D is the incidence structure dev(D) whose points
are the elements of the group G and whose blocks are the translates D + g = {d + g |
d ∈D}, g ∈G.
A multiplier of a diﬀerence set D in a group G is an automorphism ϕ of G such that
ϕ(D) = D + g for some g ∈G. If ϕ is a multiplier and ϕ(h) = th for all h ∈G, then t is
a numerical multiplier.
Facts:
1. Both the group G itself and G −{g} (for an arbitrary g ∈G) are (v, v, v; 0) and
(v, v −1, v −2; 1) diﬀerence sets. In the table of Example 2, these trivial diﬀerence sets
are excluded.
2. The complement of a (v, k, λ; n) diﬀerence set is again a diﬀerence set with parameters
(v, v −k, v −2k + λ; n). Therefore only k ≤v
2 (the case k = v
2 is actually impossible) is
considered.

904
Chapter 12
COMBINATORIAL DESIGNS
3. The existence of a (v, k, λ; n) diﬀerence set is equivalent to the existence of a sym-
metric (v, k, λ) design D admitting G as a point regular automorphism group; that is,
for any two points p and q, there is a unique group element g which maps p to q. The
design D is isomorphic with dev(D).
4. There are many symmetric designs which do not have diﬀerence set representations.
5. Since diﬀerence sets can yield symmetric designs, the parameters v, k, and λ must
satisfy the trivial necessary conditions for the existence of a symmetric design (λ(v−1) =
k(k −1)) and must also satisfy the Bruck-Ryser-Chowla condition (§12.2.2, part 2 of
Fact 2).
6. If ϕ is a multiplier of the diﬀerence set D, then there is at least one translate D + g
of D which is ﬁxed by ϕ.
7. The multiplier theorem: Let D be an abelian (v, k, λ; n) diﬀerence set. If p is a prime
that satisﬁes (p, v) = 1, p|n, and p > λ, then p is a numerical multiplier.
8. The multiplier conjecture:
Every prime divisor p of n that is relatively prime to v
is a multiplier of a (v, k, λ; n) diﬀerence set; that is, the condition p > λ in Fact 7 is
unnecessary.
Examples:
1. A (11, 5, 2; 3) diﬀerence set in the group Z11 is {1, 3, 4, 5, 9}.
2. The following table lists abelian diﬀerence sets of order n ≤15. (See Fact 1.) One
diﬀerence set for each abelian group is listed.
In general, there will be many more
examples. There are no other groups or parameters with n ≤15 for which the existence
of a diﬀerence set is undecided.
In the column “group” the decomposition of the group as a product of cyclic subgroups
is given. If the group is cyclic, the integers modulo the group order are used to describe
the diﬀerence set.
n
v
k
λ
group
diﬀerence set
2
7
3
1
(7)
1 2 4
3
13
4
1
(13)
0 1 3 9
3
11
5
2
(11)
1 3 4 5 9
4
21
5
1
(21)
3 6 7 12 14
4
16
6
2
(8)(2)
(00) (10) (11) (20) (40) (61)
(4)2
(00) (01) (10) (12) (20) (23)
(4)(2)2
(000) (010) (100) (101) (200) (211)
(2)4
(0000) (0010) (1000) (1001) (1100) (1111)
4
15
7
3
(3)(5)
0 1 2 4 5 8 10
5
31
6
1
(31)
1 5 11 24 25 27
5
19
9
4
(19)
1 4 5 6 7 9 11 16 17
6
23
11
5
(23)
1 2 3 4 6 8 9 12 13 16 18
7
57
8
1
(57)
1 6 7 9 19 38 42 49
7
27
13
6
(3)3
(001) (011) (021) (111) (020) (100) (112) (120) (121)
(122) (201) (202) (220)
8
73
9
1
(73)
1 2 4 8 16 32 37 55 64
8
31
15
7
(31)
1 2 3 4 6 8 12 15 16 17 23 24 27 29 30
9
91
10
1
(91)
0 1 3 9 27 49 56 61 77 81
9
45
12
3
(3)2(5)
(000) (001) (002) (003) (010) (020) (101) (112) (123)
(201) (213) (222)

Section 12.3
LATIN SQUARES AND ORTHOGONAL ARRAYS
905
n
v
k
λ
group
diﬀerence set
9
40
13
4
(40)
1 2 3 5 6 9 14 15 18 20 25 27 35
9
36
15
6
(4)(3)2
(010) (011) (012) (020) (021) (022) (100) (110) (120)
(200) (211) (222) (300) (312) (321)
(2)2(3)2
(0010) (0011) (0012) (0020) (0021) (0022) (0100) (0110)
(0120) (1000) (1011) (1022) (1100) (1112) (1121)
9
35
17
8
(35)
0 1 3 4 7 9 11 12 13 14 16 17 21 27 28 29 33
11
133
12
1
(133)
1 11 16 40 41 43 52 60 74 78 121 128
11
43
21
10
(43)
1 4 6 9 10 11 13 14 15 16 17 21 23 24 25 31 35 36 38 40
41
12
47
23
11
(47)
1 2 3 4 6 7 8 9 12 14 16 17 18 21 24 25 27 28 32 34 36 37
42
13
183
14
1
(183)
0 2 3 10 26 39 43 61 109 121 130 136 141 155
15
59
29
14
(59)
1 3 4 5 7 9 12 15 16 17 19 20 21 22 25 26 27 28 29 35 36
41 45 46 48 49 51 53 57
12.3
LATIN SQUARES AND ORTHOGONAL ARRAYS
12.3.1
LATIN SQUARES
Deﬁnitions:
A Latin square of side n is an n×n array in which each entry contains a single element
from an n-set S, such that each element of S occurs exactly once in each row and exactly
once in each column.
A Latin square of side n (on the set {1, 2, . . ., n} or on the set {0, 1, . . ., n −1}) is
reduced or in standard form if in the ﬁrst row and column the elements occur in
increasing order.
Let L be an n × n Latin square on symbol set E3, with rows indexed by the elements of
the n-set E1 and columns indexed by the elements of the n-set E2. Let T = {(x1, x2, x3) |
L(x1, x2) = x3} and {a, b, c} = {1, 2, 3}. The (a, b, c)-conjugate of L, L(a,b,c), has rows
indexed by Ea, columns by Eb, and symbols by Ec, and is deﬁned by L(a,b,c)(xa, xb) = xc
for each (x1, x2, x3) ∈T .
The transpose of a Latin square L, denoted LT, is the Latin square which results from L
when the roles of rows and columns are exchanged; that is, LT(i, j) = L(j, i).
A Latin square L of side n is symmetric if L(i, j) = L(j, i) for all 1 ≤i ≤n, 1 ≤j ≤n.
A Latin square L of side n is idempotent if L(i, i) = i for all 1 ≤i ≤n.
A transversal in a Latin square of side n is a set of n cells, one from each row and
column, containing each of the n symbols exactly once.
A partial transversal of length k in a Latin square of side n is a set of k cells, each
from a diﬀerent row and each from a diﬀerent column, such that no two contain the same
symbol.

906
Chapter 12
COMBINATORIAL DESIGNS
Two Latin squares L and L′ of side n are equivalent (or isotopic) if there are three
bijections, from the rows, columns, and symbols of L to the rows, columns, and symbols,
respectively, of L′, that map L to L′.
Two Latin squares L and L′ of side n are main class isotopic if L is isotopic to some
conjugate of L′.
Let k < n. If in a Latin square L of side n the k2 cells deﬁned by k rows and k columns
form a Latin square of side k, then the cells are a Latin subsquare of L.
An n × n array L with cells that are either empty or contain exactly one symbol is a
partial Latin square if no symbol occurs more than once in any row or column.
A partial Latin square is symmetric (or commutative) if whenever cell (i, j) is occupied
by x, cell (j, i) is also occupied by x, for every 1 ≤i ≤n, 1 ≤j ≤n.
A partial Latin square is idempotent if cell (i, i) is occupied by i, for all i.
A Latin rectangle is a k × n (k < n) array in which each cell contains a single element
from an n-set such that each element occurs exactly once in each row and at most once
in each column.
An n×n partial Latin square P is said to be imbedded in a Latin square L if the upper
n × n left corner of L agrees with P.
Facts:
1. The multiplication table of any (multiplicative) group is a Latin square.
2. For each positive integer k a reduced Latin square can be constructed using the
following format:
1
2
3
· · ·
k−1
k
2
3
4
· · ·
k
1
3
4
5
· · ·
1
2
...
...
...
...
...
...
k−1
k
1
· · ·
k−3
k−2
k
1
2
· · ·
k−2
k−1
3. Every Latin square has 1, 2, 3, or 6 distinct conjugates.
4. A symmetric Latin square of even side can never be idempotent.
5. A symmetric idempotent Latin square of side n is equivalent to a 1-factorization of
the complete graph Kn+1 on n + 1 points. A Latin square of side n is equivalent to a
1-factorization of the complete graph Kn,n.
6. Every idempotent Latin square has a transversal (the main diagonal).
7. Some Latin squares have no transversals. One such class of Latin squares is composed
of the addition tables of the integers modulo 2n for every n ≥1, or in general the addition
table of any group that has a unique element of order 2.
8. Every Latin square of side n has a partial transversal of length k where
k ≥max{n −√n, n −15(log n)2}. (P. W. Shor)
9. A Latin square of side n with a proper subsquare of side k exists if and only if
k ≤⌊n
2 ⌋.

Section 12.3
LATIN SQUARES AND ORTHOGONAL ARRAYS
907
10. There exists a Latin square of side n with no proper subsquares if n ̸= 2a3b or if
n = 3, 9, 12, 16, 18, 27, 81, 243.
11. A partial Latin square of side n with at most n−1 ﬁlled cells can always be completed
to a Latin square of side n.
12. A k×n (k < n) Latin rectangle can always be completed to a Latin square of side n.
13. Let L be a partial Latin square of order n in which cell (i, j) is ﬁlled if and only if
i ≤r and j ≤s. Then L can be completed to a Latin square of order n if and only if
N(i) ≥r + s −n for i = 1, 2, . . . , n, where N(i) denotes the number of elements in L
that are equal to i. (H. Ryser)
14. A partial n × n Latin square can be imbedded in a t × t Latin square for every
t ≥2n.
15. An n × n partial symmetric Latin square can be imbedded in a t × t symmetric
Latin square for every even t ≥2n.
16. The number of distinct Latin squares, the number of main classes, and the number
of equivalence classes of Latin squares of side n go to inﬁnity as n →∞. For 1 ≤n ≤3,
there is one main class and one equivalence class.
The number of main classes and
equivalence classes of Latin squares of side 4 ≤n ≤10 are given in the following table:
n
4
5
6
7
8
9
10
main
2
2
12
147
283,657
19,270,853,541
34,817,397,894,749,939
equiv.
2
2
22
563
1,676,257
115,618,721,533
208,904,371,354,363,006
Examples:
1. A 4×4 Latin square on {1, 2, 3, 4}, where 1, 2, 3, 4 represent four brands of tires, gives
a way to test each brand of tire on each of the four wheel positions on each of four cars:
the i, j-entry of the Latin square is the brand of tire to be tested on wheel position i of
car j.
2. A Latin square of side 8 on the symbols 0, 1, . . . , 7:
0
1
2
3
4
5
6
7
1
0
3
4
5
6
7
2
2
3
5
0
6
7
4
1
3
4
0
7
1
2
5
6
4
5
6
1
7
0
2
3
5
6
7
2
0
3
1
4
6
7
4
5
2
1
3
0
7
2
1
6
3
4
0
5
3. A Latin square of side 4 and its six conjugates:
1
4
2
3
2
3
1
4
4
1
3
2
3
2
4
1
(1,2,3)-conjugate
1
2
4
3
4
3
1
2
2
1
3
4
3
4
2
1
(2,1,3)-conjugate
1
3
2
4
2
4
1
3
4
2
3
1
3
1
4
2
(3,2,1)-conjugate

908
Chapter 12
COMBINATORIAL DESIGNS
1
2
4
3
3
4
2
1
2
1
3
4
4
3
1
2
(2,3,1)-conjugate
1
3
4
2
3
1
2
4
2
4
3
1
4
2
1
3
(1,3,2)-conjugate
1
3
2
4
3
1
4
2
4
2
3
1
2
4
1
3
(3,1,2)-conjugate
4. The following gives the main classes of Latin squares of sides 4, 5, and 6. No two
Latin squares listed are main class isotopic.
n = 4:
0
1
2
3
1
0
3
2
2
3
0
1
3
2
1
0
0
1
2
3
1
0
3
2
2
3
1
0
3
2
0
1
n = 5:
0
1
2
3
4
1
2
3
4
0
2
3
4
0
1
3
4
0
1
2
4
0
1
2
3
0
1
2
3
4
1
0
3
4
2
2
3
4
0
1
3
4
1
2
0
4
2
0
1
3
n = 6:
0
1
2
3
4
5
1
0
3
2
5
4
2
3
4
5
0
1
3
2
5
4
1
0
4
5
0
1
2
3
5
4
1
0
3
2
0
1
2
3
4
5
1
0
3
2
5
4
2
3
4
5
0
1
3
2
5
4
1
0
4
5
0
1
3
2
5
4
1
0
2
3
0
1
2
3
4
5
1
0
3
4
5
2
2
3
0
5
1
4
3
4
5
0
2
1
4
5
1
2
0
3
5
2
4
1
3
0
0
1
2
3
4
5
1
0
3
4
5
2
2
3
0
5
1
4
3
4
5
0
2
1
4
5
1
2
3
0
5
2
4
1
0
3
0
1
2
3
4
5
1
0
3
4
5
2
2
3
1
5
0
4
3
4
5
1
2
0
4
5
0
2
3
1
5
2
4
0
1
3
0
1
2
3
4
5
1
0
3
4
5
2
2
3
4
5
0
1
3
4
5
2
1
0
4
5
0
1
2
3
5
2
1
0
3
4
0
1
2
3
4
5
1
0
3
2
5
4
2
4
0
5
1
3
3
5
1
4
0
2
4
2
5
0
3
1
5
3
4
1
2
0
0
1
2
3
4
5
1
0
3
2
5
4
2
4
0
5
1
3
3
5
1
4
0
2
4
2
5
1
3
0
5
3
4
0
2
1
0
1
2
3
4
5
1
0
3
2
5
4
2
4
0
5
1
3
3
5
1
4
2
0
4
3
5
1
0
2
5
2
4
0
3
1
0
1
2
3
4
5
1
0
3
2
5
4
2
4
0
5
3
1
3
5
4
0
1
2
4
2
5
1
0
3
5
3
1
4
2
0
0
1
2
3
4
5
1
0
3
4
5
2
2
3
1
5
0
4
3
5
4
1
2
0
4
2
5
0
1
3
5
4
0
2
3
1
0
1
2
3
4
5
1
2
0
4
5
3
2
0
1
5
3
4
3
5
4
1
0
2
4
3
5
2
1
0
5
4
3
0
2
1
5. The following are a Latin square of side 7 with a subsquare of side 3 (3 × 3 square in

Section 12.3
LATIN SQUARES AND ORTHOGONAL ARRAYS
909
upper left corner) and a Latin square of order 12 with no proper subsquares.
1
2
3
4
5
6
7
2
3
1
6
4
7
5
3
1
2
7
6
5
4
4
7
5
1
3
2
6
7
5
6
3
2
4
1
6
4
7
5
1
3
2
5
6
4
2
7
1
3
1
2
3
4
5
6
7
8
9
a
b
c
2
3
4
5
6
1
8
9
a
b
c
7
3
1
5
2
7
8
4
a
6
c
9
b
4
5
6
7
1
9
b
c
8
3
2
a
5
6
2
8
a
7
9
b
c
4
1
3
6
c
8
1
3
a
2
7
b
9
4
5
7
8
1
a
c
b
5
4
2
6
3
9
8
9
b
3
4
c
a
6
5
1
7
2
9
b
7
c
2
5
1
3
4
8
a
6
a
7
c
b
9
4
6
1
3
2
5
8
b
4
a
9
8
3
c
2
7
5
6
1
c
a
9
6
b
2
3
5
1
7
8
4
6. The partial Latin square
1
·
2
·
3
1
4
1
·
is imbedded in the Latin square
1
4
2
3
2
3
1
4
4
1
3
2
3
2
4
1
.
12.3.2
MUTUALLY ORTHOGONAL LATIN SQUARES
Deﬁnitions:
Two Latin squares A = (aij) and B = (bij) of order n are orthogonal if the n2 ordered
pairs (aij, bij) (1 ≤i, j ≤n) are distinct. (The relation of orthogonality is symmetric.)
A set of Latin squares {A1, . . . , Ak} is a set of mutually orthogonal Latin squares
(MOLS) if Ai and Aj are orthogonal for all i, j ∈{1, . . ., k} (i ̸= j). The maximum
number of MOLS of order n is written N(n). It is customary to deﬁne N(0) = N(1) = ∞.
A set of n−1 MOLS of side n is a complete set of MOLS.
Facts:
1. If n ≥2, then N(n) ≤n−1.
2. If n is a prime power, then N(n) = n−1.
3. N(n) ≥2 for all n ≥3, except n = 6. (Bose-Parker-Shrikhande)
4. N(n) ≥3 for all n ≥4 except for n = 6 and possibly n = 10. The table that follows
gives the best known lower bounds for N(n) for 0 ≤n ≤499. Add the row and column
indices to obtain the order.
0
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15 16
17 18 19
0 ∞∞
1
2
3
4
1
6
7
8
2
10
5
12
4
4
15
16
5
18
20 4
5
3
22
7
24 4 26
5
28
4
30 31
5
4
5
8
36
4
5
40 7
40
5
42
5
6
4 46
8
48
6
5
5
52
5
6
7
7
5
58
60 5
60
5
6
63
7
5 66
5
6
6
70
7
72
5
7
6
6
6
78
80 9
80
8
82
6
6
6
6
7
88
6
7
6
6
6
6
7
96
6
8

910
Chapter 12
COMBINATORIAL DESIGNS
0
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15 16
17 18 19
100 8 100 6 102 7
7
6 106
6
108 6
6
13 112 6
7
6
8
6
6
120 7 120 6
6
6 124 6 126 127
7
6 130 6
7
6
7
7
136 6 138
140 6
7
6
10 10
7
6
7
6
148 6 150 7
8
8
7
6
156 7
6
160 9
7
6 162 6
7
6 166
7
168 6
8
6 172 6
6
14
9
6 178
180 6 180 6
6
7
9
6 10
6
8
6 190 7 192 6
7
6
196 6 198
200 7
8
6
7
6
8
6
8
14
11 10 210 6
7
6
7
7
8
6
10
220 6
12
6 222 13
8
6 226
6
228 6
7
7 232 6
7
6
7
6 238
240 7 240 6 242 6
7
6 12
7
7
6 250 6
12
9
7 255 256 6
12
260 6
8
8 262 7
8
7 10
7
268 7 270 15 16
6 13 10 276 6
9
280 7 280 6 282 6
12 6
7
15 288 6
6
6 292 6
6
7
10 10 12
300 7
7
7
7
15 15 6 306
7
7
7 310 7 312 7 10
7
316 7
10
320 15 15
6
16
8
12 6
7
7
9
6 330 7
8
7
6
8
336 6
7
340 6
10 10 342 7
7
6 346
6
348 8
12 18 352 6
9
7
9
6 358
360 8 360 6
7
7
10 6 366 15
15
7
15
7 372 7 15
7
13
7 378
380 7
12
7 382 15 15 7 15
7
388 7
16
7
8
7
7
8
396 7
7
400 15 400 7
15 11
8
7 15
8
408 7
13
8
12 10 9
18
15
7 418
420 7 420 7
15
7
16 6
7
7
10
6 430 15 432 6 15
6
18
7 438
440 7
15
7 442 7
13 7 11
15 448 7
15
7
7
7 15
7
456 7
16
460 7 460 7 462 15 15 7 466
8
8
7
15
7
15 10 18
7
15
6 478
480 15 15
6
15
8
7
6 486
7
15
6 490 6
16
6
7
15
15
6 498
5. N(n) →∞as n →∞. (Chowla, Erd˝os, Straus)
6. N(n × m) ≥min{N(n), N(m)}. (MacNeish)
7. The existence of n −1 MOLS of order n is equivalent to the existence of a projective
plane of order n (an (n2 + n + 1, n + 1, 1) design) and an aﬃne plane of order n (an
(n2, n, 1) design). (§12.2.3)
8. The existence of a set of k −2 mutually orthogonal Latin squares of order n is
equivalent to the existence of a transversal design TD(k, n). (§12.1.7)
9. A set of k −2 MOLS of order n is equivalent to an OA(n, k) (§12.3.3).
10. Constructing a complete set of MOLS of order q for q a prime power: A complete
set of MOLS of order q for q a prime power can be constructed as follows:
• for each α ∈GF(q) −{0}, deﬁne the Latin square Lα(i, j) = i + αj, where
i, j ∈GF(q) and the algebra is performed in GF(q).
The set of Latin squares {Lα | α ∈GF(q) −{0}} is a set of q −1 MOLS of side q.
11. Let nk be the largest order for which the existence of k MOLS is unknown. So if
n > nk, then there exist at least k MOLS of order n. See the following table:
k
nk
k
nk
k
nk
k
nk
k
nk
2
6
5
60
8
2,766
11
7,222
14
7,874
3
10
6
74
9
3,678
12
7,286
15
8,360
4
22
7
570
10
5,804
13
7,288

Section 12.3
LATIN SQUARES AND ORTHOGONAL ARRAYS
911
12. Constructing a set of r MOLS of size mn×mn from a set of r MOLS of size m×m
and a set of r MOLS of size n×n: Let A1, . . . , Ar and B1, . . . , Br be two sets of MOLS,
where each Ai = (a(i)
xy) is of size m × m and each Bi = (b(i)
xy) is of size n × n. Construct
a set C1, . . . , Cr of mn × mn MOLS as follows: for each k = 1, . . . , r, let
Ck =






D(k)
11
D(k)
12
· · ·
D(k)
1m
D(k)
21
D(k)
22
· · ·
D(k)
2m
...
...
...
D(k)
m1
D(k)
m2
· · ·
D(k)
mm






where
D(k)
ij
=






(a(k)
ij , b(k)
11 )
(a(k)
ij , b(k)
12 )
· · ·
(a(k)
ij , b(k)
1n )
(a(k)
ij , b(k)
21 )
(a(k)
ij , b(k)
22 )
· · ·
(a(k)
ij , b(k)
2n )
...
...
...
(a(k)
ij , b(k)
n1 )
(a(k)
ij , b(k)
n2 )
· · ·
(a(k)
ij , b(k)
nn)






.
Note: In 1782 Leonhard Euler considered the following problem:
A very curious question, which has exercised for some time the ingenuity of
many people, has involved me in the following studies, which seem to open a
new ﬁeld of analysis, in particular in the study of combinations. The question
revolves around arranging 36 oﬃcers to be drawn from six diﬀerent ranks and
at the same time from six diﬀerent regiments so that they are also arranged
in a square so that in each line (both horizontal and vertical) there are six
oﬃcers of diﬀerent ranks and diﬀerent regiments.
A solution to Euler’s problem would be equivalent to a pair of orthogonal Latin squares
of order 6, the symbol set of the ﬁrst consisting of the six ranks and the symbol set of
the second consisting of the six regiments. Euler convinced himself that his problem was
incapable of solution and goes even further:
I have examined a very great number of tables . . . and I do not hesitate to
conclude that one cannot produce an orthogonal pair of order 6 and that the
same impossibility extends to 10, 14, . . . and in general to all the orders which
are unevenly even.
Euler was proven correct in his claim that an orthogonal pair of order 6 does not exist
[G. Tarry, 1900]; however, in 1960 Euler was shown to be wrong for all orders greater
than 6. (See the Bose-Parker-Shrikhande theorem, Fact 3.)
Examples:
1. Two mutually orthogonal Latin squares of side 3:
1
2
3
2
3
1
3
1
2
1
2
3
3
1
2
2
3
1
2. Three mutually orthogonal Latin squares of side 4:
1
2
3
4
4
3
2
1
2
1
4
3
3
4
1
2
1
2
3
4
3
4
1
2
4
3
2
1
2
1
4
3
1
2
3
4
2
1
4
3
3
4
1
2
4
3
2
1

912
Chapter 12
COMBINATORIAL DESIGNS
3. Two MOLS of order 10:
0
4
1
7
2
9
8
3
6
5
8
1
5
2
7
3
9
4
0
6
9
8
2
6
3
7
4
5
1
0
5
9
8
3
0
4
7
6
2
1
7
6
9
8
4
1
5
0
3
2
6
7
0
9
8
5
2
1
4
3
3
0
7
1
9
8
6
2
5
4
1
2
3
4
5
6
0
7
8
9
2
3
4
5
6
0
1
8
9
7
4
5
6
0
1
2
3
9
7
8
0
7
8
6
9
3
5
4
1
2
6
1
7
8
0
9
4
5
2
3
5
0
2
7
8
1
9
6
3
4
9
6
1
3
7
8
2
0
4
5
3
9
0
2
4
7
8
1
5
6
8
4
9
1
3
5
7
2
6
0
7
8
5
9
2
4
6
3
0
1
4
5
6
0
1
2
3
7
8
9
1
2
3
4
5
6
0
9
7
8
2
3
4
5
6
0
1
8
9
7
12.3.3
ORTHOGONAL ARRAYS
Deﬁnition:
An orthogonal array of size N, with k constraints (or of degree k), s levels (or of
order s), and strength t, denoted OA(N, k, s, t), is a k × N array with entries from a set
of s ≥2 symbols, having the property that in every t × N submatrix, every t × 1 column
vector appears the same number λ = N
st of times. The parameter λ is the index of the
orthogonal array.
Note: An OA(N, k, s, t) is also denoted by OAλ(t, k, s); in this notation, if t is omitted
it is understood to be 2, and if λ is omitted it is understood to be 1.
Facts:
1. An OAλ(k, v) is equivalent to a transversal design TDλ(k, v).
2. OA1(t, k, s) are known as MDS codes in coding theory.
3. An OAλ(k, n) exists only if k ≤
j
λv2−1
v−1
k
(Bose-Bush bound).
Generally one is
interested in ﬁnding the largest k for which there exists an OAλ(k, n) (for a given λ
and n).
The following table gives the best known upper bounds and lower bounds for the largest k
for which there exists a OAλ(k, n), for 1 ≤n ≤18 and 2 ≤λ ≤10. Entries for which
the upper and lower bounds match are shown in boldface.

Section 12.4
MATROIDS
913
λ\n
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
2
3
7
11 15 19 23 27 31 35 39
43
47
51
55
59
63
67
71
3
4
7
13 16 22 25 31 34 40
43
49
52
58
61
67
70
76
79
13 10
13 25
31
13
49
14
25
37
49
25
4
5
9
14 21 25 30 37 41 46
53
57
62
69
73
78
85
89
94
13
10 17 13
37
21
13
61
21
57
22
37
37
5
6
11 17 23 31 36 42 48 56
61
67
73
81
86
92
98
106 111
8
21
16 18 26 21
18
26
26
21
43
81
36
91
6
3
13 20 27 34 43 49 56 63
70
79
85
92
99
106 115 121 128
7
9
13
8
12
9
13 11
12
9
19
11
17
13
25
11
19
7
8
15 23 30 38 46 57 64 72
79
87
95
106 113 121 128 136 144
10 29 12 19
29 29
19
29
50
29
36
38
29
64
8
9
17 26 34 43 52 61 73 81
90
98
107 116 125 137 145 154 162
10 33 10 17 57
22
41
33
33
26
65
57
41
73
9 10 19 29 38 48 58 68 78 91 100 110 119 129 139 149 159 172 181
28 37 19 55 28 73
37
37
109
50
55
55
82
73
10
9
21 32 42 53 64 75 86 97 111 121 132 142 153 164 175 186 197
4
10 12 10 10 20 12 11 12
12
11
28
12
12
12
19
12
30
12.4
MATROIDS
Linearly independent sets of columns in a matrix and acyclic sets of edges in a graph
share many similar properties.
Hassler Whitney (1907–1989) aimed to capture these
similarities when he deﬁned matroids in 1935.
These structures arise naturally in a
variety of combinatorial contexts. Moreover, they are precisely the hereditary families of
sets for which a greedy strategy produces an optimal set.
12.4.1
BASIC DEFINITIONS AND EXAMPLES
Deﬁnitions:
A matroid M (also written (E, I) or (E(M), I(M)) ) is a ﬁnite set E (the ground set
of M) and a collection I of subsets of E (independent sets) such that
• the empty set is independent;
• every subset of an independent set is independent (I is hereditary);
• if X and Y are independent and |X| < |Y |, then there is e ∈Y −X such that
X ∪{e} is independent.
Subsets of E that are not in I are dependent.
A basis of a matroid is a maximal independent set. The collection of bases of M is
denoted B(M).

914
Chapter 12
COMBINATORIAL DESIGNS
A circuit of a matroid is a minimal dependent set. The collection of circuits of M is
denoted C(M).
Matroids M1 and M2 are isomorphic (M1 ∼= M2) if there is a one-to-one function ϕ
from E(M1) onto E(M2) that preserves independence; that is, a subset X of E(M1) is
in I(M1) if and only if ϕ(X) is in I(M2).
For a matroid M with ground set E and A ⊆E, all maximal independent subsets of A
have the same cardinality, called the rank of A, written r(A) or rM(A). The rank r(M)
of M is r(E).
A spanning set of a matroid M is a subset of the ground set E of rank r(M).
A hyperplane of a matroid M is a maximal nonspanning set.
The closure cl(X) (or σ(X)) of X is {x ∈E | r(X ∪{x}) = r(X)}.
A set X is a closed set or ﬂat if cl(X) = X.
A loop of M is an element e such that {e} is a circuit.
If {f, g} is a circuit, then f and g are parallel elements.
Matroid M is a simple matroid (or combinatorial geometry) if it has no loops and
no parallel elements.
A paving matroid is a matroid M in which all circuits have at least r(M) elements.
Various classes of matroids are deﬁned in the following table.
matroid
ground set
independent
bases
circuits
M
E(M)
sets I(M)
B(M)
C(M)
uniform
{I ⊆E :
{B ⊆E :
{C ⊆E :
matroid, Um,n
{1, 2, . . . , n}
|I| ≤m}
|B| = m}
|C| = m + 1}
(0 ≤m ≤n)
M(G), cycle
E(G), the
{I ⊆E(G) |
for connected
edge-sets of
matroid of
edges of G
I contains
G: edge-sets of
cycles
graph G
no cycle}
spanning trees
M[A], vector
column
{I ⊆E | I labels
labels of max-
labels of min-
matroid of
labels
a linearly inde-
imal linearly
imal linearly
matrix A
of A
pendent multiset
independent
dependent multi-
over ﬁeld F
of columns}
sets of columns
sets of columns
transversal
partial transvers-
matroid, M(A),
als of A: sets
maximal partial
minimal sets
of family A =
E
{xi1, . . . , xik},
transversals
that are not par-
(A1, A2, . . . , Am)
i1 < . . . < ik
of A
tial transversals
where Aj ⊆E
and xij ∈Aij
Matroid M is in the speciﬁed class if M satisﬁes the indicated condition:
• graphic: M ∼= M(G) for some graph G;
• planar: M ∼= M(G) for some planar graph G;
• representable over F: M ∼= M[A] for some matrix A over the ﬁeld F;
• binary: representable over GF(2), the 2-element ﬁeld;

Section 12.4
MATROIDS
915
• ternary: representable over GF(3);
• regular: representable over all ﬁelds.
Facts:
1. If a matroid M is graphic, then M ∼= M(G) for some connected graph G.
2. Whitney’s 2-isomorphism theorem:
Two graphs have isomorphic cycle matroids if
and only if one can be obtained from the other by performing a sequence of the following
operations:
• choose one vertex from each of two components and identify the chosen vertices;
• produce a new graph from which the original can be recovered by applying the
previous operation;
• in a graph that can be obtained from the disjoint union of two graphs G1 and G2
by identifying vertices u1 and v1 of G1 with vertices u2 and v2 of G2, twist the
graph by identifying, instead, u1 with v2 and u2 with v1.
3. If A′ is obtained from the matrix A over the ﬁeld F by elementary row operations,
deleting or adjoining zero rows, permuting columns, and multiplying columns by nonzero
scalars, then M[A′] ∼= M[A].
The converse of this holds if and only if F is GF(2)
or GF(3).
4. If a matroid M is representable over F and r(M) ≥1, then M ∼= M[Ir(M)|D],
where Ir(M)|D consists of an r(M)×r(M) identity matrix followed by some other matrix
D over F.
5. A matroid M is regular if and only if M can be represented over the real numbers by
a totally unimodular matrix (a matrix for which all subdeterminants are 0, 1, or −1).
6. A matroid M is regular if and only if M is both binary and ternary.
7. The smallest matroids not representable over any ﬁeld have eight elements.
8. Conjecture: For all n ≥9, more than half of all matroids on {1, 2, . . ., n} are paving.
9. The following table lists the numbers of nonisomorphic matroids, simple matroids,
and binary matroids with up to nine elements:
|E(M)|
0
1
2
3
4
5
6
7
8
9
matroids
1
2
4
8
17
38
98
306
1,724
383,172
simple
1
1
1
2
4
9
26
101
950
376,467
binary
1
2
4
8
16
32
68
148
342
848
Examples:
1. Let M be the matroid with E(M) = {1, 2, . . ., 6} and C(M) = {{1}, {5, 6}, {3, 4, 5},
{3, 4, 6}}. Then B = {{2, 3, 4}, {2, 3, 5}, {2, 3, 6}, {2, 4, 5}, {2, 4, 6}}. The following ﬁg-
ure shows that M is graphic and binary since M = M(G1) = M(G2) and M = M[A] with
A being interpreted over GF(2); M is regular since M = M[A] when A is interpreted over
any ﬁeld F. Also M is transversal since M = M(A) where A = ({2}, {3, 4}, {4, 5, 6}).

916
Chapter 12
COMBINATORIAL DESIGNS
1
3
2
4
A =
G1
5
6
1
3
2
4
G2
5
6
1
0
0
0
2
1
0
0
3
0
1
0
4
0
0
1
5
0
1
1
6
0
1
1
2. Fano and non-Fano matroids:
Given a ﬁnite set E of points in the plane and a
collection of lines (subsets of E with at least three elements), no two of which share more
than one common point, there is a matroid with ground set E whose circuits are all sets
of three collinear points and all sets of four points no three of which are collinear. Two
such matroids are shown in the following ﬁgure. Each has ground set {1, 2, . . ., 7}. On
the right is the non-Fano matroid F −
7 . It diﬀers from the Fano matroid F7 on the left
by the collinearity of 4, 5, and 6 in the latter.
The matrix in this ﬁgure represents F7 over all ﬁelds of characteristic 2, and represents F −
7
over all other ﬁelds. F7 is binary but non-ternary; F −
7 is ternary but non-binary. Both
are non-uniform, non-regular, non-graphic, and non-transversal.
1
3
F7
F7
2
4
5
7
6
1
3
2
4
5
7
6
1
1
0
0
2
0
1
0
3
0
0
1
4
1
1
0
5
1
0
1
6
0
1
1
7
1
1
1
12.4.2
ALTERNATIVE AXIOM SYSTEMS
Matroids can be characterized by many diﬀerent axiom systems. Some examples of these
systems follow. Throughout, E is assumed to be a ﬁnite set and 2E stands for the set of
subsets of E.
Deﬁnitions:
Circuit axioms:
A subset C of 2E is the set of circuits of a matroid on E if and only
if C satisﬁes
• ∅̸∈C;
• no member of C is a proper subset of another;
• circuit elimination: if C1, C2 are distinct members of C and e ∈C1 ∩C2, then C
has a member C3 such that C3 ⊆(C1 ∪C2) −{e}.
Note: The circuit elimination axiom can be strengthened to the following:
• strong circuit elimination:
if C1, C2 ∈C, f ∈C1 −C2, and e ∈C1 ∩C2, then C
has a member C3 such that f ∈C3 ⊆(C1 ∪C2) −{e}.
Basis axioms: A subset B of 2E is the set of bases of a matroid on E if and only if

Section 12.4
MATROIDS
917
• B is nonempty;
• if B1, B2 ∈B and x ∈B1 −B2, then there is an element y ∈B2 −B1 such that
(B1 −{x}) ∪{y} ∈B.
Rank axioms: A function r from 2E into the nonnegative integers is the rank function
of a matroid on E if and only if, for all subsets X, Y , Z of E
• 0 ≤r(X) ≤|X|;
• if Y ⊆Z, then r(Y ) ≤r(Z);
• submodularity: r(X ∪Y ) + r(X ∩Y ) ≤r(X) + r(Y ).
Closure axioms:
A function cl from 2E into 2E is the closure operator of a matroid
on E if and only if, for all subsets X and Y of E
• X ⊆cl(X);
• if X ⊆Y , then cl(X) ⊆cl(Y );
• cl(cl(X)) = cl(X);
• MacLane-Steinitz exchange:
if x ∈E and y ∈cl(X ∪{x}) −cl(X), then x ∈
cl(X ∪{y}).
Fact:
1. If M is a matroid with ground set E and I ⊆E, the following statements are
equivalent:
• I is an independent set of M;
• no circuit of M contains I;
• some basis of M contains I;
• r(I) = |I|;
• for every element e of I, e /∈cl(I −{e}).
12.4.3
DUALITY
Deﬁnitions:
For a matroid M, let B∗(M) = {E(M) −B | B ∈B(M)}. Then B∗(M) is the set of
bases of a matroid M ∗, called the dual of M, whose ground set is also E(M).
Bases, circuits, loops, and independent sets of M ∗are called cobases, cocircuits,
coloops, and coindependent sets of M.
For a graph G, the cocycle matroid (or bond matroid) of G is the dual of M(G) and
is denoted by M ∗(G).
A matroid M is cographic if M ∼= M ∗(G) for some graph G.
A class of matroids is closed under duality if the dual of every member of the class is
also in the class.

918
Chapter 12
COMBINATORIAL DESIGNS
Facts:
1. For all matroids M, (M ∗)∗= M.
2. For all matroids M, the rank function of M ∗is given by r∗(X) = |X|−r(M)+r(E −
X).
3. The cocircuits of every matroid M are the minimal sets having nonempty intersection
with every basis of M.
4. The cocircuits of every matroid M are the minimal nonempty sets C∗such that
|C∗∩C| ̸= 1 for every circuit C of M.
5. For every graph G, the circuits of M ∗(G) are the minimal edge cuts of G.
6. A graphic matroid is cographic if and only if it is planar.
7. The following classes of matroids are closed under duality: uniform matroids, ma-
troids representable over a ﬁxed ﬁeld F, planar matroids, and regular matroids. The
classes of graphic and transversal matroids are not closed under duality.
8. The following are special sets and their complements in a matroid M and M ∗:
X
basis of M
independent set of M
circuit of M
E −X
basis of M ∗
spanning set of M ∗
hyperplane of M ∗
Example:
1. The following are duals of some basic examples:
matroid
dual
Um,n
Un−m,n
M(G) (G planar)
M(G∗), where G∗is any dual of G
M[Ir|D] ([Ir|D] an r × n matrix)
M[−DT |In−r], same order of column labels
as [Ir|D]
12.4.4
FUNDAMENTAL OPERATIONS
Deﬁnitions:
Three basic constructions for matroids M, M1, and M2 are deﬁned in the following
table. M\T and M/T are also written as M|(E −T ) and M.(E −T ) and are called the
restriction and contraction of M to E −T ; M\{e} and M/{e} are written as M\e
and M/e.
matroid
I
C
rank
M\T
{I ⊆E(M) −T |
{C ⊆E(M) −T |
rM\T (X) =
(deletion of
I ∈I(M)}
C ∈C(M)}
rM(X)
T from M)
M/T
{I ⊆E(M) −T |
minimal nonempty
rM/T (X) =
(contraction
I ∪BT ∈I(M) for
members of
rM(X ∪T )−
of T from M)
some BT in B(M|T )}
{C−T | C ∈C(M)}
rM(T )
M1 ⊕M2
rM1⊕M2(X) =
(direct sum
{I1 ∪I2 | Ij ∈I(Mj)}
C(M1) ∪C(M2)
r1(X ∩E(M1))+
of M1 and M2)
r2(X ∩E(M2))

Section 12.4
MATROIDS
919
Matroid N is a minor of matroid M if N can be obtained from M by a sequence of
deletions and contractions. The minor N is proper if N ̸= M.
A matroid is connected if it cannot be written as the direct sum of two nonempty
matroids (matroids with nonempty ground sets).
Facts:
In each of the following, M, M1, and M2 are matroids.
1. M\X\Y = M\(X ∪Y ) = M\Y \X; M/X/Y = M/(X ∪Y ) = M/Y/X; and
M\X/Y = M/Y \X.
2. M1 ⊕M2 = M2 ⊕M1.
3. (M/T )∗= M ∗\T ; and (M\T )∗= M ∗/T . (Deletion and contraction are dual opera-
tions.)
4. The scum theorem:
Every minor of M can be written as M\X/Y for some in-
dependent set Y and coindependent set X. (The name derives from the fact that an
isomorphic copy of every simple minor of a matroid occurs at (that is, ﬂoats to) the top
of the lattice.) (D. A. Higgs) [CrRo70]
5. The following are equivalent:
• M is connected;
• M ∗is connected;
• every two distinct elements of M are in a circuit;
• there is no proper nonempty subset T of E(M) such that M\T = M/T ;
• there is no proper nonempty subset T of E(M) such that r(T ) + r(E(M) −T ) =
r(M);
• there is no proper nonempty subset T of E(M) such that r(T ) + r∗(T ) = |T |.
6. If M is connected, then M is uniquely determined by the set of circuits containing
some ﬁxed element of E(M).
7. If M is connected and e ∈E(M), then M\e or M/e is connected.
8. F7 ⊕F −
7 is not representable over any ﬁeld.
Examples:
1. Um,n\e = Um,n−1 unless m = n when Um,n\e = Um−1,n−1.
2. Um,n/e = Um−1,n−1 unless m = 0 when Um,n/e = Um,n−1.
3. M(G)\e = M(G\e) where G\e is obtained from G by deleting the edge e.
4. M(G)/e = M(G/e) where G/e is obtained from G by contracting the edge e.
5. M[A]\e is the vector matroid of the matrix obtained by deleting column e from A.
6. If e corresponds to a standard basis vector in A, then M[A]/e is the vector matroid
of the matrix obtained by deleting both the column e and the row containing the one
of e.
12.4.5
CHARACTERIZATIONS
Many matroid results characterize various classes of matroids. Some examples of such
results appear below. The Venn diagram in the following ﬁgure indicates the relationship
between certain matroid classes.

920
Chapter 12
COMBINATORIAL DESIGNS
Graphic
Planar
Cographic
Binary
Ternary
Representable
Regular
Matroids
Deﬁnitions:
Let M1 and M2 be two binary matroids such that E(M1) ∩E(M2) = T , M1|T = M2|T ,
and no cocircuit of M1 or M2 is contained in T . The 2-sum and 3-sum of M1 and M2
are matroids on (E(M1)∪E(M2))−T whose ﬂats are those sets F −T such that F ∩E(Mi)
is a ﬂat of Mi for i = 1, 2. The 2-sum occurs when |T | = 1, |E(Mi)| ≥3, and T is not a
loop of Mi, and the 3-sum occurs when |E(Mi)| ≥7 and T is a 3-element circuit of Mi.
Facts:
1. The following are equivalent for a matroid M:
• M is uniform;
• every circuit of M has r(M) + 1 elements;
• every circuit of M meets every cocircuit of M.
2. The following are equivalent for a matroid M:
• M is binary;
• for every circuit C and every cocircuit C∗, |C ∩C∗| is even;
• for every circuit C and every cocircuit C∗, |C ∩C∗| ̸= 3;
• for all C1, C2 ∈C, (C1 −C2) ∪(C2 −C1) is a disjoint union of circuits.
3. The class of regular matroids is the class of matroids that can be constructed by
direct sums, 2-sums, and 3-sums from graphic matroids, cographic matroids, and copies
of R10 (the matroid that is represented over GF(2) by the ten 5-tuples with exactly three
ones). (This last fact is the basis of a polynomial-time algorithm to determine whether
a real matrix is totally unimodular.)
4. Excluded-minor theorems: Many classes of matroids are minor-closed; that is, every
minor of a member of the class is also in the class. Such classes can be characterized by
listing their excluded minors (those matroids that are not in the class but have all their
proper minors in the class). Some important examples of such results are given in the
following table. The class of transversal matroids is not minor-closed since a contraction
of a transversal matroid need not be transversal.
class
excluded minors
class
excluded minors
binary
U2,4
ternary
U2,5, U3,5, F7, F ∗
7
uniform
U0,1 ⊕U1,1
graphic
U2,4, F7, F ∗
7 , M ∗(K5), M ∗(K3,3)
paving
U0,1 ⊕U2,2
regular
U2,4, F7, F ∗
7

REFERENCES
921
12.4.6
THE GREEDY ALGORITHM
Deﬁnitions:
For a ﬁnite set E, let I be a subset of 2E satisfying the ﬁrst two axioms for independent
sets in the deﬁnition of matroid (§12.4.1). Let w be a real-valued function on E. For
X ⊆E, deﬁne the weight of X by w(X) = P
x∈X w(x), and let w(∅) = 0.
Facts:
1. Matroids have an important relationship to the greedy algorithm (Algorithm 1) that
makes them important in optimization problems.
Algorithm 1:
The greedy algorithm for (I, w).
X0 := ∅; j := 0
while E −Xj contains an element e such that Xj ∪{e} ∈I
ej+1 := an element e of maximum weight such that Xj ∪{e} ∈I
Xj+1 := Xj ∪{ej+1}
j := j + 1
BG := Xj
2. I (a subset of 2E satisfying the ﬁrst two axioms for independent sets in the deﬁnition
of matroid) is the set of independent sets of a matroid on E if and only if, for all real-
valued weight functions w on E, the set BG produced by the greedy algorithm is a
maximal member of I of maximum weight.
Example:
1. Let G be a connected graph with each edge e having a cost c(e). Deﬁne w(e) = −c(e).
Then the greedy algorithm is just Kruskal’s algorithm (§10.1.2) and BG is the edge-set
of a spanning tree of minimum cost.
REFERENCES
Printed Resources:
[An93] I. Anderson, Combinatorial Designs: Construction Methods, Ellis Horwood, 1993.
[BeJuLe86] T. Beth, D. Jungnickel, and H. Lenz, Design Theory, Cambridge University
Press, 1986.
[CoDi07] C. J. Colbourn and J. H. Dinitz, eds., Handbook of Combinatorial Designs, 2nd
ed., Chapman and Hall/CRC Press, 2007. (A comprehensive source of information
on combinatorial designs.)
[CrRo70] H. H. Crapo and G.-C. Rota, On the Foundations of Combinatorial Theory:
Combinatorial Geometries, MIT Press, 1970.
[DiSt92] J. H. Dinitz and D. R. Stinson, eds., Contemporary Design Theory: A Collection
of Surveys, John Wiley & Sons, 1992.
[KeD´e15] A. D. Keedwell and J. D´enes, Latin Squares and Their Applications, North
Holland, 2015.

922
Chapter 12
COMBINATORIAL DESIGNS
[Ox11] J. G. Oxley, Matroid Theory, 2nd ed., Oxford University Press, 2011. (The main
source for §12.4; contains references for all stated results.)
[Re89] A. Recski, Matroid Theory and Its Applications in Electric Network Theory and
in Statics, Springer-Verlag, 1989.
[St04] D. R. Stinson, Combinatorial Designs: Constructions and Analysis, Springer, 2004.
[We10] D. J. A. Welsh, Matroid Theory, Dover, 2010.
[Wh08] N. White, ed., Theory of Matroids, Cambridge University Press, 2008.
Web Resources:
http://designtheory.org (Information on combinatorial, computational, and statis-
tical aspects of design theory.)
http://gams.nist.gov/ (Guide to Available Mathematical Software at the National
Institute of Science and Technology: a cross-index and virtual repository of math-
ematical and statistical software components of use in computational science and
engineering; contains a program for ﬁnding t-designs.)
http://lib.stat.cmu.edu/designs/ (The Designs Archive at Statlib: has some very
useful programs for making orthogonal arrays.)
http://link.springer.com/journal/10623 (Designs, Codes and Cryptography has a
site with its table of contents.)
http://neilsloane.com/gosset/ (GOSSET: A general purpose program for designing
experiments.)
http://neilsloane.com/hadamard/index.html
(Contains an extensive collection of
Hadamard matrices.)
http://neilsloane.com/oadir/index.html
(Contains an extensive collection of or-
thogonal arrays.)
http://onlinelibrary.wiley.com/journal/10.1002/(ISSN)1520-6610
(Journal of
Combinatorial Designs has a site with its table of contents.)
http://userhome.brooklyn.cuny.edu/skingan/matroids/software.html (Contains
an open source, interactive, extensible software system for experimenting with ma-
troids.)
http://www-m10.ma.tum.de/foswiki/pub/Lehrstuhl/PublikationenJRG/21 Orient
edMatroids.pdf (Source information on oriented matroids.)
https://www.ccrwest.org/cover.html and https://www.ccrwest.org/cover/table
.html (La Jolla Covering Repository: contains coverings C(v, k, t) with v ≤32,
k ≤16, t ≤8, and less than 5,000 blocks.)
http://www.cecm.sfu.ca/organics/papers/lam/paper/html/paper.html (Informa-
tion on the search for a ﬁnite projective plane of order 10.)
http://www.cems.uvm.edu/~dinitz/hcd.html (Handbook of Combinatorial Designs
has a website that lists new results in design theory that have been discovered since
its 2007 publication.)
http://www.gap-system.org/Packages/design.html (DESIGN is a package for con-
structing, classifying, partitioning and studying block designs. )
https://www.math.LSU.edu/~oxley/survey4.pdf (Contains the 45-page paper “What
is a Matroid?” by James Oxley.)

REFERENCES
923
http://www.matroidunion.org/ (A blog for and by the matroid community.)
http://www.netlib.org/ (Netlib Repository: a collection of mathematical software,
papers, and databases.)
http://www.utu.fi/fi/yksikot/sci/yksikot/mattil/opiskelu/kurssit/Document
s/comb2.pdf (Notes from Ian Anderson and Iiro Honkala for a “Short Course in
Combinatorial Designs”; 39 pages, revised 2012.)


13
DISCRETE AND COMPUTATIONAL
GEOMETRY
13.1 Arrangements of Geometric Objects
Ileana Streinu
13.1.1 Point Conﬁgurations
13.1.2 Line and Hyperplane Arrangements
13.1.3 Pseudoline Arrangements
13.1.4 Oriented Matroids
13.2 Space Filling
Karoly Bezdek
13.2.1 Packing
13.2.2 Covering
13.2.3 Tiling
13.3 Combinatorial Geometry
J´anos Pach
13.3.1 Convexity
13.3.2 Incidences
13.3.3 Distances
13.3.4 Coloring
13.4 Polyhedra
Tamal K. Dey
13.4.1 Geometric Properties of Polyhedra
13.4.2 Triangulations
13.4.3 Face Numbers
13.5 Algorithms and Complexity in Computational Geometry
Jianer Chen
13.5.1 Convex Hulls
13.5.2 Triangulation Algorithms
13.5.3 Voronoi Diagrams and Delaunay Triangulations
13.5.4 Arrangements
13.5.5 Visibility
13.6 Geometric Data Structures and Searching
Dina Kravets
13.6.1 Point Location
13.6.2 Range Searching
13.6.3 Ray Shooting and Lines in Space
13.7 Computational Techniques
Nancy M. Amato
13.7.1 Parallel Algorithms
13.7.2 Randomized Algorithms
13.7.3 Parametric Search
13.7.4 Finite Precision
13.7.5 Degeneracy Avoidance
13.8 Applications of Geometry
W. Randolph Franklin
13.8.1 Mathematical Programming

926
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
13.8.2 Polyhedral Combinatorics
13.8.3 Computational Convexity
13.8.4 Motion Planning in Robotics
13.8.5 Convex Hull Applications
13.8.6 Nearest Neighbor
13.8.7 Computer Graphics
13.8.8 Mechanical Engineering Design and Manufacturing
13.8.9 Layout Problems
13.8.10 Graph Drawing and Visualization
13.8.11 Geographic Information Systems
13.8.12 Geometric Constraint Solving
13.8.13 Implementations
INTRODUCTION
This chapter outlines the theory and applications of various concepts arising in two
rapidly growing, interrelated areas of geometry: discrete geometry (which deals with
topics such as space ﬁlling, arrangements of geometric objects, and related combinatorial
problems) and computational geometry (which deals with the many aspects of the de-
sign and analysis of geometric algorithms). A more extensive treatment of discrete and
computational geometry can be found in [T´oORGo17].
GLOSSARY
anti-aliasing: the ﬁltering out of high-frequency spatial components of a signal, to
prevent artifacts, or aliases, from appearing in the output image.
aperiodic (prototile): a prototile in d-dimensional Euclidean space such that the pro-
totile admits a tiling of the space, yet all such tilings are nonperiodic.
arrangement (of lines in the plane): the planar straight-line graph whose vertices are
the intersection points of the lines and whose edges connect consecutive intersection
points on each line (it is assumed that all lines intersect at a common point at
inﬁnity).
arrangement graph: a graph associated with a Euclidean or projective line arrange-
ment, or a big circle arrangement.
aspect ratio (of a simplex): the ratio of the radius of the circumscribing sphere to the
radius of the inscribing sphere of the simplex; for a triangulation, the largest aspect
ratio of a simplex in the triangulation.
basis (of a point conﬁguration in Rd): a subset of the point conﬁguration that is a sim-
plex of the ambient space Rd.
basis (of a vector conﬁguration in Rd): a subset of the vector conﬁguration that is a
basis of the ambient space Rd.
big-circle arrangement: the intersection of a central plane arrangement with the unit
sphere in R3.
boundary (of a polyhedron): the vertices, edges, and higher-dimensional facets of the
polyhedron.

GLOSSARY
927
cell (of a line arrangement): a connected component of the complement in R2 of the
union of the points on the lines.
centerpoint (of a point conﬁguration P of size n): a point q, not necessarily in P, such
that for any hyperplane containing q there are at least

n
d+1

points in each semi-
space induced by the hyperplane.
central hyperplane arrangement (in Rd): a ﬁnite set of central hyperplanes, not all
of them going through the same point.
central plane arrangement (in R3): a ﬁnite set of central planes.
chain: a planar straight-line graph with vertices v1, . . . , vn and edges {v1, v2}, {v2, v3},
. . . , {vn−1, vn}.
chirotope: for an ordered point conﬁguration, the set of all signed bases of the con-
ﬁguration; for an ordered vector conﬁguration, the set of all signed bases of the
conﬁguration.
circuit (of a set of labeled vectors V = {v1, . . . , vn}): the signed set C = (C+, C−),
where C+ = { j | αj > 0 } and C−= { j | αj < 0 }, of indices of the non-
null coeﬃcients αj in a minimal linear dependency V ′ = {vi1, . . . , vik} of V with
Pk
j=1 αjvij = 0.
class library: in an object-oriented computer language, a set of new data types and
operations on them, activated by sending a data item a message.
closed half-space: the set of all points on a hyperplane and the points on one side of
the same hyperplane.
cluster of rank 3 hyperline sequences (associated with a vector conﬁguration): the
ordered set of stars, one for each point in the conﬁguration.
cluster of stars (associated with a point conﬁguration): the ordered set of stars, one
for each point in the conﬁguration.
cocircuit (of a labeled vector conﬁguration V ): a signed set C = (C+, C−) of the set
{1, 2, . . ., n}, induced by a subset of d −1 vectors spanning a central hyperplane h.
For an arbitrary orientation of h, C+ is the set of indices of elements in V lying
in h+ and C−is the set of indices of elements in V lying in h−.
computational convexity: the study of high-dimensional convex bodies.
contraction (on element i in a rank d central plane arrangement): the arrangement ob-
tained by identifying hi with Rd−1 and intersecting it with all the other hyperplanes
to obtain a rank d−1 arrangement with one fewer element.
convex: the property of a subset of a Euclidean space that for every pair of points in
the set the linear segment joining them is contained in the set.
convex body: a closed and bounded convex set with nonempty interior.
convex d-polyhedron: the intersection of a ﬁnite number of closed half-spaces in Rd.
convex decomposition (of a polyhedron): its partition into interior disjoint convex
pieces.
convex hull (of a set of points): the smallest convex set containing the given set of
points.
convex polygon: a polytope in the plane.
convex polyhedron: the intersection of a ﬁnite number of half-spaces.

928
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
convex polytope: a bounded convex polyhedron.
convex position: the property of a set of points that it is the vertex set of a polytope.
convex set: a subset of d-dimensional Euclidean space such that for every pair of dis-
tinct points in the set, the segment with these two points as endpoints is contained
in the set.
covering: a family of convex bodies in d-dimensional Euclidean space such that each
point belongs to at least one of the convex bodies.
cyclic d-polytope: the convex hull of a set of n ≥d + 1 points on the moment curve
in Rd. The moment curve in Rd is deﬁned parametrically by x(t) = (t, t2, . . . , td).
Davenport-Schinzel sequence (of order s): a sequence of characters over an alpha-
bet of size n such that no two consecutive characters are the same, and for any pair
of characters, a and b, there is no alternating subsequence of length s+ 2 of the form
. . . a . . . b . . . a . . . b . . . .
deletion: the removal of a point (vector, line, etc.) from a conﬁguration and recording
the oriented matroid (chirotope, circuits, cluster of stars, etc.) only for the remaining
points.
density (of a covering): the common value (if it exists) of the lower density and upper
density of the covering.
density (of a packing): the common value (if it exists) of the lower density and upper
density of the packing.
dual polytopes: two polytopes P and Q such that there exists a one-to-one corre-
spondence δ between the set of faces of P and Q, where two faces f1, f2 ∈P satisfy
f1 ⊂f2 if and only if δ(f1) ⊃δ(f2) in Q.
duality transformation: a mapping of points to lines and lines to points that preserves
incidences.
Euclidean hyperplane arrangement (in Rd): a ﬁnite set of aﬃne hyperplanes, not
all of them going through the same point.
Euclidean line arrangement: a ﬁnite set of planar lines, not all of them going through
the same point.
Euclidean pseudoconﬁguration of points: a pair consisting of a planar set of points
and a pseudoline arrangement, such that for every pair of distinct points there exists
a unique pseudoline incident with them.
k-face: an open set of dimension k that is part of the boundary of a polyhedron; 0-faces,
1-faces, and (d−1)-faces of a d-polyhedron are called vertices, edges, and facets.
face vector (of a d-polyhedron): the d-dimensional vector (f0, f1, . . . , fd−1), where fi
is the number of i-dimensional faces of the d-polyhedron.
face-to-face (tiling): a tiling of d-dimensional Euclidean space by convex d-polytopes
such that the intersection of any two tiles is a face of each tile, possibly the (improper)
empty face.
general position: the property of a set of vectors in Rd that every subset of d elements
is a basis; property of a set of points in Rd that every subset of d elements is a simplex.
genus (of a manifold 3-polyhedron): the genus number of its boundary, if the boundary
is a 2-manifold.

GLOSSARY
929
geographic information system (GIS): an information system designed to capture,
store, manipulate, analyze, and display spatial or geographically-referenced data.
geometric constraint solving: the problem of locating a set of geometric elements
given a set of constraints between them.
graphical user interface (GUI): a mechanism that allows a user to interactively con-
trol a computer program with a bitmapped display by using a mouse or pointer to
select menu items, move sliders or valuators, etc.
Grassmann-Pl¨ucker relations (rank 3): the
identities
[123][145] −[124][135] +
[125][134] = 0 satisﬁed by the determinants [ijk] = det(vi, vj, vk), for any ﬁve vectors
vi, 1 ≤i ≤5.
half-space: one of the two connected components of the complement of a hyperplane.
ham-sandwich cut: a hyperplane that simultaneously bisects d point conﬁgurations
in d-dimensional Euclidean space.
hyperplane: in d dimensions the set of all points on a (d−1)-dimensional plane.
hyperplane arrangement: the partitioning of the Euclidean space Rd into connected
regions of diﬀerent dimensions (vertices, edges, etc.) by a ﬁnite set of hyperplanes.
isogonal (tiling): a tiling such that the group of symmetries acts transitively on the
vertices of the tiles.
isomorphic (vector or point conﬁgurations): conﬁgurations having the same order type,
after possibly relabeling their elements.
isotoxal (tiling): a tiling such that the group of symmetries acts transitively on the
edges of the tiles.
lattice (tiling): a tiling of d-dimensional Euclidean space by translates of a tile such
that the corresponding translation vectors form a d-dimensional lattice.
k-level (in a given nonvertical arrangement of n lines): the lower boundary of the set
of points in R2 having exactly k lines above and n−k below.
line arrangement: the partitioning of the plane into connected regions (cells, edges,
and vertices) induced by a ﬁnite set of lines.
lower density (of a covering C): ν(C) =
lim inf
R→+∞
P
Ki∩BR̸=∅Vol(Ki)
Vol(BR)
, where each Ki is a
convex body in the covering C of d-dimensional Euclidean space and BR is the closed
ball of radius R centered at the origin.
lower density (of a packing P): δ(P) =
lim inf
R→+∞
P
Ki⊂BR Vol(Ki)
Vol(BR)
, where each Ki is a
convex body in the packing P of d-dimensional Euclidean space and BR is the closed
ball of radius R centered at the origin.
lower envelope (of a nonvertical line arrangement): the half-plane intersection of the
half-planes below the lines of the arrangement.
manifold d-polyhedron: a polyhedron whose boundary is topologically the same as
a (d−1)-manifold; i.e., every point on the boundary of a manifold d-polyhedron has
a small neighborhood that looks like an open d-ball.
mathematical programming: the large-scale optimization of an objective function
of many variables subject to constraints.
minor (of an oriented matroid given by hyperline sequences): an oriented matroid ob-
tained by a sequence of deletions and/or contractions.

930
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
monohedral (tiling): a tiling T of d-dimensional Euclidean space in which all tiles are
congruent to one ﬁxed set T , the (metrical) prototile of T .
nonconvex polyhedron: the union of a set of convex polyhedra such that the under-
lying space is connected and nonconvex.
non-manifold d-polyhedron: a d-polyhedron that does not have a manifold bound-
ary.
nonperiodic (tiling): a tiling such that its group of symmetries contains no translation
other than the identity.
normal (tiling): a tiling of d-dimensional Euclidean space by convex polytopes such that
there exist positive real numbers r and R such that each tile contains a Euclidean
ball of radius r and is contained in a Euclidean ball of radius R.
oracle: an algorithm that gives information about a convex body.
order type (of a vector or point conﬁguration): the collection of all semi-spaces of the
conﬁguration.
oriented matroid: a pair M = (n, L), where L, the set of covectors of M, is a subset
of {+, −, 0}n and satisﬁes the properties: 0 ∈L; if X ∈L, then −X ∈L; if X, Y ∈L,
then X ◦Y ∈L; if X, Y ∈L and i ∈S(X, Y ) = {i | Xi = −Yi ̸= 0}, then there is
Z ∈L such that Zi = 0; for each j /∈S(X, Y ), Zj = (X ◦Y )j = (Y ◦X)j.
oriented matroid given by a chirotope: an abstract set of points labeled {1, . . . , n},
together with a function satisfying the chirotope axioms.
packing: a family of convex bodies in d-dimensional Euclidean space such that no two
have an interior point in common.
parallel algorithm: an algorithm that concurrently uses more than one processing
element during its execution.
parallel random access machine (PRAM): a synchronous machine in which each
processor is a sequential RAM, and processors communicate using a shared memory.
parametric search: an algorithmic technique for solving optimization problems.
periodic (tiling): a tiling of d-dimensional Euclidean space such that the group of all
symmetries of the tiling contains translations in d linearly independent directions.
planar straight-line graph: a planar graph such that each edge is a straight line.
point conﬁguration (of dimension d): a ﬁnite set of points aﬃnely spanning Rd.
point location problem: the problem of determining which region of a given subdi-
vision of Rd contains a given point.
polar-duality of vectors and central planes (in R3): a mapping associating with a
vector v in R3 an oriented central plane h having v as its normal vector, and vice
versa.
polyhedron: the intersection of a ﬁnite number of closed half-spaces in d-dimensional
Euclidean space.
polytope: a bounded polyhedron.
d-polytope: a convex d-polyhedron for which there exists a d-dimensional cube con-
taining it inside; that is, a bounded convex d-polyhedron.
H-polytope: a polytope deﬁned as the intersection of d half-spaces in d-dimensional
Euclidean space.

GLOSSARY
931
V-polytope: a polytope deﬁned as the convex hull of d points in d-dimensional Eu-
clidean space.
projective line arrangement: a ﬁnite set of projective lines in the projective plane.
prototile: the single tile used repeatedly in a monohedral tiling.
pseudoline arrangement: a ﬁnite collection of simple planar curves that intersect
pairwise in exactly one point, where they cross.
Radon partition (of a set of labeled points P): a signed set C = (C+, C−) of points
of P such that the convex hull of the points in C+ intersects the convex hull of the
points in C−.
randomized algorithm: an algorithm that makes random choices during its execution.
range counting problem: the problem of counting the number of points of a given
set of points that lie in a query range.
range emptiness problem: the problem of determining if a query range contains any
points of a given set of points.
range reporting problem: the problem of determining all the points of a given set of
points that lie in a query range.
rank 3 hyperline sequence (associated with some vector v ∈V ⊆R3): an alternat-
ing circular sequence of subsets of indices, obtained by rotating an oriented central
plane in counterclockwise order around the line through v.
ray: a half-line that is directed away from its endpoint.
ray shooting problem: the problem of determining the ﬁrst object in a set of geomet-
ric objects that is hit by a query ray.
real random access machine (real RAM): a model of computation in which values
can be arbitrarily long real numbers, and all standard operations such as +, −, ×,
and ÷ can be performed in unit time regardless of operand length.
realizable (pseudoline arrangement): a pseudoline arrangement isomorphic to a line
arrangement.
reﬂex edges: edges of a nonconvex 3-polyhedron that subtend an inner dihedral angle
greater than 180◦.
regular (polygon): a polygon with all sides congruent and all interior angles equal.
regular (polytope): a d-polytope (d > 0) with all its facets regular (d−1)-polytopes
that are combinatorially equivalent; a regular 0-polytope is a vertex.
regular (tiling): a monohedral tiling of the plane with a regular polygon as prototile.
reorientation (of a vector conﬁguration V = {v1, . . . , vn}): a vector conﬁguration V ′
= {v′
1, . . . , v′
n} such that each v′
i is equal to vi or −vi.
semiregular (polyhedron): a convex polyhedron with each face a regular polygon, but
where more than one regular polygon can be used as a face.
semiregular (tiling): a tiling of the plane using n prototiles with the same numbers of
polygons around each vertex.
semi-space (of a conﬁguration induced by a hyperplane): the set of indices of the con-
ﬁguration lying on one side of the hyperplane.
semi-space (of a vector or point conﬁguration): a semi-space induced by some hyper-
plane.

932
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
k-set (of a point conﬁguration): a semi-space of the conﬁguration of size k.
d-dimensional simplex (or d-simplex): a d-polytope with d + 1 vertices.
simplicial complex: a triangulation of a polyhedron such that for any two simplices
in the triangulation, either the intersection of the simplices is empty or is a face of
both simplices.
simplicial polytope: a polytope in which all faces are simplices.
(standard aﬃne) pseudo polar-duality: the association between an x-monotone
pseudoline arrangement L = {l1, . . . , ln} given in slope order and a pseudo con-
ﬁguration of points (P, L′), P = {p1, . . . , pn}, given in increasing order of the x-
coordinates and with L′ being x-monotone, satisfying the property that the cluster
of stars associated with L and to P are the same.
straight-line dual: given the Voronoi diagram of a set {p1, . . . , pn} of points in the
plane, the planar straight-line graph whose vertices are the points in the set, with
two vertices pi and pj adjacent if and only if the regions V (pi) and V (pj) share a
common edge.
strictly convex: the property of a convex set that its boundary contains no line seg-
ment.
symmetry (of a tiling): a Euclidean motion that maps each tile of the tiling onto a tile
of the tiling.
tile: an element of a tiling.
tiling (of Euclidean d-space): a countable family T of closed topological d-cells of Rd
that cover Rd without gaps and overlaps.
triangulation (of a d-polyhedron): a convex decomposition in which each convex piece
of the decomposition is a d-simplex.
triangulation (of a simple polygon): an augmentation of the polygon with noninter-
secting diagonal edges connecting vertices of the polygon such that in the resulting
planar straight-line graph every bounded face is a triangle.
uniform chirotope: a chirotope function that takes nonzero values on all d-tuples.
upper density (of a covering C): ν(C) =
lim sup
R→+∞
P
Ki∩BR̸=∅Vol(Ki)
Vol(BR)
, where each Ki is a
convex body in the covering C of d-dimensional Euclidean space and BR is the closed
ball of radius R centered at the origin.
upper density (of a packing P): δ(P) =
lim sup
R→+∞
P
Ki⊂BR Vol(Ki)
Vol(BR)
, where each Ki is a
convex body in the packing P of d-dimensional Euclidean space and BR is the closed
ball of radius R centered at the origin.
upper envelope (of a nonvertical line arrangement): the half-plane intersection of the
half-planes above the lines of the arrangement.
vector conﬁguration (of dimension d): a ﬁnite set of vectors spanning Rd.
visibility graph: given n nonintersecting line segments in the plane, the graph whose
vertices are the endpoints of the line segments, with two vertices adjacent if and only
if they are visible from each other.
visibility problem: the problem of ﬁnding what is visible, given a conﬁguration of
objects and a viewpoint.

Section 13.1
ARRANGEMENTS OF GEOMETRIC OBJECTS
933
Voronoi cell (with center ci): the convex polyhedral set Vi = { x ∈Rd : |x −ci| =
minj |x−cj| }, where c1, c2, . . . are centers of unit balls in a packing of d-dimensional
Euclidean space.
Voronoi diagram (of the set of points {p1, . . . , pn} in d-dimensional Euclidean space):
the partition of d-dimensional Euclidean space into convex polytopes V (pi) such that
V (pi) is the locus of points that are closer to pi than to any other point in pj.
zone (of a line in an arrangement): the set of cells of the arrangement intersected by
the line.
zonotope: the vector (Minkowski) sum of a ﬁnite number of line segments.
13.1
ARRANGEMENTS OF GEOMETRIC OBJECTS
A wide range of applied ﬁelds (statistics, computer graphics, robotics, geographical
databases) depend on solutions to geometric problems: polygon intersection, visibil-
ity computations, range searching, shortest paths among obstacles, just to name a few.
These problems typically start with “consider a ﬁnite set of points (or lines, segments,
curves, hyperplanes, polygons, polyhedra, etc.)”. The combinatorial properties of these
sets, or arrangements, of objects (incidence, order, partitioning, separation, convexity)
set the foundations for the algorithms developed in the ﬁeld of computational geometry.
In this chapter attention is focused on the most studied and best understood arrange-
ments of geometric objects: points, lines, and hyperplanes. Introducing the concepts
relies on linear algebra. The combinatorial properties studied belong, however, to a rel-
atively new ﬁeld, the theory of oriented matroids, which has sometimes been described
as linear algebra without coordinates.
Several fundamental types of questions are asked about these arrangements. The most
basic is the classiﬁcation problem, whose goal is to ﬁnd combinatorial parameters allowing
the partitioning of the (uncountable) set of all possible arrangements of n objects into
a ﬁnite number of equivalence classes. Examples of such structures for point and line
arrangements include semi-spaces, Radon partitions, chirotopes, hyperline sequences, etc.
They satisfy simple properties known as axiomatic systems for oriented matroids, which
lead to the deﬁnition of an abstract class of objects generalizing ﬁnite point and vector
sets. In dimension 2, oriented matroids can be visualized topologically as pseudoline
arrangements. The numerous deﬁnitions needed to introduce arrangements and oriented
matroids will be complemented in this section by the most important facts, such as
counting the number of ﬁnite point, line, and pseudoline arrangements, deciding when a
pseudoline arrangement is equivalent to a line arrangement, and basic algorithmic results.
13.1.1
POINT CONFIGURATIONS
The simplest geometric objects are points in some d-dimensional space. Most of the
other objects of interest for applications of computational geometry (sets of segments,
polygons, polyhedra) are built on top of, and inherit, geometric structure from sets of
points.

934
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
The setting for computational geometry problems is the Euclidean (aﬃne) space Rd
and most of its fundamental concepts (convexity, proximity) belong here in a natural
way. However, some standard techniques, such as polarity and duality, as well as the
abstraction to oriented matroids, are better explained in the context of vector spaces.
Several categories of concepts are introduced in this section, and these are developed
and used in the subsequent subsections: vector and point conﬁgurations, hyperplanes
and half-spaces, convexity, and some combinatorial parameters associated with vector
or point conﬁgurations, relevant to applications (in statistics, pattern recognition or
computational geometry): signed bases, semi-spaces, k-sets, centerpoints.
Deﬁnitions:
The (standard) real vector space of dimension d is the vector space Rd = {x | x =
(x1, . . . , xd), xi ∈R}, with vector addition x + y = {x1 + y1, . . . , xd + yd} and scalar
multiplication αx = {αx1, . . . , αxd}. A vector in Rd is a d-dimensional vector.
A linear combination of a set of vectors {v1, . . . , vn} is a vector of the form Pn
i=1 αivi,
for coeﬃcients α1, . . . , αn ∈R.
A linearly independent set of vectors is a set of vectors {v1, . . . , vk} such that a linear
combination of them equals the zero vector (Pk
i=1 αivi = 0) if and only if αi = 0 for all
i = 1, . . . , k.
A basis of Rd is a maximal set of linearly independent vectors, i.e., one that is no longer
independent if a new element is added.
A basis is an ordered basis if it is given as an ordered set.
The sign of an ordered basis is the sign of the determinant of the d × d matrix with
columns given in order by the vectors of the ordered basis.
A linearly dependent set of vectors V = {v1, . . . , vk} is a set of vectors for which there
exists a linear combination with at least one nonzero coeﬃcient yielding the 0 vector;
i.e., Pk
i=1 αivi = 0 with some αi ̸= 0.
The linear space spanned by a set of vectors V = {v1, . . . , vk}, vi ∈Rd, is the set of all
linear combinations of vectors of V .
A linear k-dimensional subspace of Rd (k ≤d) is the set of all linear combinations
of k linearly independent vectors v1, . . . , vk in Rd.
A line through v ∈Rd is the 1-dimensional linear subspace of Rd induced by v ̸= 0.
Euclidean space of dimension d is Rd seen as an aﬃne space.
It is sometimes
identiﬁed with the d-dimensional aﬃne hyperplane xd+1 = 1 in Rd+1.
A (d-dimensional) point is an element of Rd seen as a Euclidean space.
An aﬃne combination of a set of points {p1, . . . , pn} is a point of the form Pn
i=1 αipi,
with αi ∈R and Pn
i=1 αi = 0.
An aﬃnely independent set of points is a set of points {p1, . . . , pk} such that no
point is an aﬃne combination of the others.
A simplex of Rd is a maximal set of aﬃnely independent vectors. It is an ordered
simplex if it is given as an ordered set.
The extended matrix of an ordered simplex {p1, . . . , pd+1} is the (d + 1) × (d + 1)
matrix with its ith column equal to (pi, 1).

Section 13.1
ARRANGEMENTS OF GEOMETRIC OBJECTS
935
The sign of an ordered simplex is the sign of the determinant of the extended matrix of
the simplex.
An aﬃnely dependent set of points P = {p1, . . . , pk} is a set of points such that one
of the points is an aﬃne combination of the others.
The aﬃne space spanned by a set of points P = {p1, . . . , pk}, with pi ∈Rd, is the set
of all aﬃne combinations of points of P. It is an aﬃne subspace of Rd.
The aﬃne k-dimensional subspace of Rd (k ≤d) is the set of all aﬃne combinations
of k aﬃnely independent points p1, . . . , pk in Rd.
A linear function is a function h: Rd →R such that h(x1, . . . , xd) = Pd
i=1 aixi +ad+1.
A linear function is homogeneous if ad+1 = 0.
The aﬃne hyperplane induced by a linear function h is the set h0 = {x ∈Rd | h(x) =
0}.
An aﬃne hyperplane is a central hyperplane if h is a homogeneous linear function.
An oriented hyperplane is a hyperplane, together with a choice of a positive side for
the hyperplane. This amounts to choosing a (homogeneous or aﬃne) linear function h
to generate it, together with all those of the form αh, α > 0.
A reorientation of an oriented hyperplane is a swapping of the negative and positive
sides of the hyperplane (or, changing the generating linear function h to −h).
The open half-spaces induced by an oriented hyperplane h are h+ = {x | h(x) > 0}
(the positive side of h0) and h−= {x | h(x) < 0} (the negative side of h0). The sets
h+, h0, and h−form a partition of Rd: Rd = h+ ∪h−∪h0, and h+, h−, and h0 are
pairwise disjoint.
The closed half-spaces induced by h are h+ ∪h0 and h−∪h0.
A convex combination of a set of points {p1, . . . , pn} is a point of the form Pn
i=1 αipi
with αi ∈R, αi > 0, and Pn
i=1 αi = 1.
The segment with endpoints p1 ̸= p2 is the set of all convex combinations of p1 and p2.
A set of points {p1, . . . , pk} is convexly independent if no point is a convex combination
of the others. The points are also said to be in convex position.
A convex set in Rd is a set S ⊆Rd such that if p1 and p2 are distinct points in S, then
the segment with endpoints p1 and p2 is contained in S.
The convex hull of a ﬁnite set of points P is the set of all convex combinations of points
of P.
A convex polytope is the convex hull of a ﬁnite set of points. Its boundary consists of
faces of dimension 0 (vertices), 1 (edges), . . . , d −1 (facets).
The face description of a convex polytope is a data structure storing information about
all the faces and their incidences.
A convex polygon is the convex hull of a ﬁnite set of points in R2.
A vector conﬁguration [point conﬁguration] of dimension d is a ﬁnite set of n vectors
{v1, . . . , vn} (vi ∈Rd) spanning Rd [points {p1, . . . , pn} (pi ∈Rd) aﬃnely spanning Rd].
A conﬁguration is labeled if its elements are given as an ordered set. (It may be given
as the set of columns of a d × n matrix with real entries.)

936
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
The rank of a vector conﬁguration [point conﬁguration] in Rd is the number d [d + 1].
A set of vectors [points] in Rd is in general position if every subset of d elements is a
basis [simplex].
An aﬃne conﬁguration (or acyclic vector conﬁguration) is a conﬁguration with a
central hyperplane containing all the vectors of the conﬁguration on one side.
A reorientation of a vector conﬁguration V = {v1, . . . , vn} is a vector conﬁguration
V ′ = {v′
1, . . . , v′
n} with each v′
i equal to either vi or −vi.
A reorientation class is the set of all labeled vector conﬁgurations which are reorien-
tation equivalent.
A point conﬁguration P ⊂Rd induced by an acyclic vector conﬁguration
V ⊂Rd+1 contained in a half-space h+ is the set of all points p obtained as follows: take
the aﬃne plane h′ in h+ parallel to h and tangent to the unit sphere Sd in Rd+1; the
intersection of the line through vector v ∈V with the plane h′ is a point p ∈h′. Rd is
identiﬁed with the aﬃne plane h′.
A semi-space of a vector conﬁguration [point conﬁguration] V induced by an oriented
central hyperplane [aﬃne hyperplane] h is the set of indices of the elements in V lying
on one side of the hyperplane.
A semi-space of a vector conﬁguration [point conﬁguration] V is a semi-space
induced by some hyperplane h.
The order type of a vector or point conﬁguration V is the collection of all semi-spaces
of V .
Isomorphic vector or point conﬁgurations are conﬁgurations having the same order
type, after possibly relabeling their elements.
A k-set of a point conﬁguration P is a semi-space of P of size k (0 ≤k ≤n).
A centerpoint of a point conﬁguration P of size n is a point q, not necessarily in P, such
that if h is a hyperplane containing q there are at least

n
d+1

points in each semi-space
induced by h.
A ham-sandwich cut is a hyperplane that simultaneously bisects d point conﬁgurations
P1, P2, . . . , Pd in Rd.
Facts:
1. A basis of the vector space Rd has d elements and a simplex of the aﬃne space Rd
has d + 1 elements.
2. The rank of the d × n matrix associated with an n-vector conﬁguration in Rd is d;
the rank of the matrix associated with a point conﬁguration in Rd, extended with a row
of 1s, is d + 1.
3. The determinant of a d × d matrix whose columns are a basis of Rd and the deter-
minant of the (d+1) × (d+1) extended matrix of a simplex in Rd are nonnull.
4. If {v1, . . . , vd} is a basis, then for any vector vd+1, {v1, . . . , vd, vd+1} is linearly de-
pendent.
5. The intersection of linear subspaces [aﬃne subspaces, convex subspaces] of Rd is
linear [aﬃne, convex].
6. The intersection of k central hyperplanes [aﬃne hyperplanes] in Rd (k ≤d) is a linear
subspace [aﬃne subspace]. Its dimension is at least d −k.

Section 13.1
ARRANGEMENTS OF GEOMETRIC OBJECTS
937
7. Every aﬃne subspace of Rd is a convex set.
8. Carath´eodory’s theorem:
Each point in the convex hull of a set of points P ⊂Rd
lies in the convex hull of a subset of P with at most d + 1 points.
9. Radon’s theorem: Each set of at least d+ 2 points in Rd can be partitioned into two
disjoint sets whose convex hulls intersect in a nonempty set.
10. Helly’s theorem: Let S be a ﬁnite family of n convex sets in Rd (n ≥d+1). If every
subfamily of d + 1 sets in S has a nonempty intersection, then there is a point common
to all the sets in S.
11. Every point conﬁguration admits a centerpoint.
12. For every d conﬁgurations of points in Rd, there exists a ham-sandwich cut.
13. Upper bound theorem: The number of facets of the convex hull of an n-point
conﬁguration in Rd is O(n⌊d/2⌋). This bound is obtained for conﬁgurations of points on
the moment curve P = {p(t) | t ∈{t1, . . . , tn} ⊆R}, where p(t) = (t, t2, . . . , td) ∈Rd.
14. The number of semi-spaces of a rank d+1 vector or point conﬁguration of n elements
is O(nd). The maximum is attained for points in general position.
15. For d = 2, let ek(n) denote the maximum number of k-sets of any planar n-point
conﬁguration. Then Ω(n log k) ≤ek(n) ≤O(nk
1
3 ).
16. Erd˝os-Szekeres problem: If c(k) is the maximum number of planar points in general
position such that no k are in convex position, then 2k−2 ≤c(k) ≤
 2n−4
n−2

.
17. The face description of the convex hull of a point set of size n in Rd can be computed
optimally in O(n log n) time if d = 2 or d = 3, and O(n⌊d
2 ⌋) time for d > 3.
18. A ham-sandwich cut in dimension 2 can be found in linear time.
Examples:
1. The conﬁguration of points in the following ﬁgure is given by the columns of the
matrix
 
0 −1
3
1
2
0
1
0
1
2
!
. It is not in general position: the three points 1, 4, and 5 are
collinear. The extended matrix is



0 −1
3
1
2
0
1
0
1
2
1
1
1
1
1


. Because det



0 −1
3
0
1
0
1
1
1


< 0, the
simplex 123 is negative. Some semi-sets are: {3} (1-set), {2, 5} (2-set), {1, 3, 4} (3-set),
etc. The convex hull is {1, 3, 5, 2}.
3
4
(-1,1)
1
2
5
(0,0)
(1,1)
(2,2)
(3,0)

938
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
2. The two conﬁgurations of points from the ﬁgure of Example 1 and part (a) of the
following ﬁgure are isomorphic, but those in parts (a) and (b) of the following ﬁgure are
not. This can be seen because, for example, they have diﬀerent numbers of points on
their convex hulls.
5
2
3
1
4
(a)
5
2
3
1
4
(b)
3. The grey point in the following ﬁgure is a centerpoint of the point conﬁguration of
black points. Some of the separating lines have been shown: they have at least 1
3 of the
points on each side.
4. The line in the following ﬁgure is a ham-sandwich cut: it simultaneously bisects the
black and the white points.
13.1.2
LINE AND HYPERPLANE ARRANGEMENTS
Line arrangements and aﬃne point conﬁgurations in the plane are related via polar-
duality, a transformation which is better understood in terms of 3-dimensional vectors

Section 13.1
ARRANGEMENTS OF GEOMETRIC OBJECTS
939
and central planes or using the projective and spherical models. Several types of com-
binatorial data can be directly translated from the primal setting to the polar. As a
consequence, theorems and algorithms on line arrangements follow directly from their
counterparts on point conﬁgurations, and vice versa. This powerful tool has been used
successfully in computational geometry for the design of eﬃcient algorithms.
It also
generalizes to higher dimensions, where hyperplane arrangements are polar-dual to point
conﬁgurations in Rd.
Deﬁnitions:
A (Euclidean) line in R2 is an aﬃne subspace of dimension 1.
A line is induced
by a linear function l(x, y) = ax + by + c and any of its multiples of the form αl. A
line is oriented if a direction has been chosen for it. Its induced half-spaces are called
half-planes and denoted by l+ and l−.
A nonvertical line is a line given by an equation of the form y = ax + b, where a is
called the slope and b the y-intercept of the line. It is oriented in increasing order of the
x-coordinates of its points and its induced half-planes are above/below it.
A (Euclidean) line arrangement is a set L = {l1, . . . , ln} of planar lines, not all of
them going through the same point; that is, Tn
i=1 li = ∅. If the lines are oriented, this is
an arrangement of oriented lines. It is labeled if it is given as an ordered set.
A line arrangement is in general position if no three lines have a point in common.
An x-monotone curve is a curve intersecting each vertical line in exactly one point.
A half-plane intersection is the planar region lying in the intersection of a ﬁnite set
of half-planes. It is described by the (circular) list of the lines incident to its boundary.
An upper envelope [lower envelope] of a nonvertical line arrangement is the boundary
of the half-plane intersection of the half-planes above [below] the lines of the arrangement.
The k-level in a nonvertical arrangement of n lines (1 ≤k ≤n) is the lower boundary
of the set of points in R2 having exactly k lines above and n −k below.
The cell of a line arrangement is a connected component of the complement in R2 of
the union of the points on the lines.
The zone of a line l in an arrangement L (l /∈L) is the set of cells of the arrange-
ment L intersected by l.
A central plane arrangement in R3 is a ﬁnite set of central planes. The arrangement
is oriented if the planes are oriented.
An acyclic (or aﬃne) central plane arrangement in R3 is a central plane arrange-
ment such that there is a point in R3 that lies on the positive side of all these planes.
The (standard) line arrangement induced by a central plane arrangement is the
arrangement of the lines of intersection of the central planes with an aﬃne plane h in R3
that is not parallel with any plane of the arrangement. If the central planes are oriented,
an orientation is induced on the lines by keeping the positive side of a line within the
positive side of the corresponding plane.
A big circle is the intersection of the unit sphere S2 in R3 with a central plane. If the
plane is oriented, the circle is given an orientation so that the positive side of the plane
lies on the left of the circle.
A big-circle arrangement is the intersection of a central plane arrangement with the
unit sphere S2 in R3. It is oriented if the planes are oriented.

940
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
The big-circle arrangement induced by a central plane arrangement is the ar-
rangement of the big circles of intersection of the central planes with the sphere S2 in
R3. It is oriented if the planes are oriented.
The projective plane P 2 is the sphere S2 in R3 with the antipodal points identiﬁed.
A projective line is the projective curve induced by identifying the antipodal points of
a big circle on S2.
A projective line arrangement is a ﬁnite set of projective lines in the projective plane
P 2.
The projective line arrangement induced by a central plane arrangement is the
projective arrangement obtained by the antipodal point identiﬁcation of the big-circle
arrangement on S2 induced by the central plane arrangement.
An arrangement graph is a graph associated with a Euclidean or projective line ar-
rangement, or a big-circle arrangement. Its vertices correspond to intersection points
of lines (or circles) and its edges correspond to line (or arc) segments between two
intersection points.
Note: For the Euclidean case, typically only the bounded line segments are considered as
edges (but by adding extra dummy vertices “at inﬁnity”, the inﬁnite extremities of each
line among the edges can be included). For the Euclidean or spherical case, if the lines
are oriented, the arrangement graph is directed, with the edges oriented to be compatible
with the orientation of the lines or circles.
Isomorphic arrangements are arrangements having isomorphic arrangement graphs.
(This applies to Euclidean lines, big-circles (oriented or not), and projective lines.)
A polar-duality of vectors and central planes in R3 is a mapping D associating
a vector v ∈R3 with an oriented central plane having v as its normal vector, and vice
versa.
A polar-duality of points and lines in the aﬃne space R2 is any mapping D associat-
ing a point p ∈R2 with an oriented line l in R2 and vice versa, by the following general
procedure: map the points to vectors via some imbedding of R2 as an aﬃne plane in R3,
apply the polar-duality of vectors and central planes, and then intersect the polar central
planes with some aﬃne plane (identiﬁed with R2) to get lines.
A (standard) aﬃne polar-duality is a mapping D between nonvertical lines and points
in R2, associating the point (a, −b) with the line y = ax + b, and vice versa.
A Euclidean hyperplane [central hyperplane] arrangement in Rd is a ﬁnite set
H = {h1, . . . , hn} of aﬃne hyperplanes [central hyperplanes], not all of them going
through the same point. If the hyperplanes are oriented, the arrangement is oriented.
The following are generalizations to an arbitrary aﬃne space Rd [vector space Rd+1] of
previously deﬁned concepts in aﬃne dimension 2 [vector space R3]:
• arrangements of big (d−1)-spheres on Sd generalize big-circle arrangements
on S2;
• projective arrangements of hyperplanes in P d generalize projective arrange-
ments of lines in P 2;
• the polar-duality between vectors and central hyperplanes in Rd+1 associates
with a vector the hyperplane normal to it;

Section 13.1
ARRANGEMENTS OF GEOMETRIC OBJECTS
941
• the face lattice of a hyperplane (central, aﬃne, projective) or sphere arrange-
ment, a data structure storing information on faces and their incidences, gen-
eralizes the arrangement graph, and is used to deﬁne isomorphism of arrange-
ments.
• the k-level in an aﬃne arrangement of nonvertical hyperplanes is the lower bound-
ary of the set of points having exactly k hyperplanes above them.
Facts:
1. A bounded cell in a Euclidean line arrangement is a convex polygon.
2. The k-level of a nonvertical line arrangement is an x-monotone piecewise linear curve
incident with vertices and lines of the arrangement.
3. The upper envelope is the 0-level of an arrangement of nonvertical lines.
4. In a simple big-circle arrangement, every pair of big circles intersects in exactly two
points, which are antipodal on the sphere.
5. The arrangement graphs of planar line arrangements or spherical big-circle arrange-
ments are planar imbedded graphs.
The arrangement graph of a projective line ar-
rangement is projective-planar. The faces or cells of these graphs are the connected
components of the complement of the union of lines or circles.
6. The association among central plane arrangements, big-circle arrangements, and pro-
jective arrangements preserves isomorphisms. The standard association of an aﬃne line
arrangement and big-circle arrangement to an acyclic plane arrangement preserves iso-
morphisms.
7. In a simple Euclidean arrangement of n lines, the number of vertices is
 n
2

, the
number of segments (bounded or unbounded) is n2, and the number of cells (bounded
or unbounded) is
 n
2

+ n + 1. No nonsimple arrangement exceeds these values.
8. Zone theorem: The total number of edges (bounded or unbounded) in a zone of an
arrangement of n lines is at most 6n.
9. D(D(v)) = v and D(D(h)) = h, for every vector v and hyperplane h.
10. Incidence preserving:
If v ∈h, then D(h) ∈D(v), for every vector v and hyper-
plane h.
11. Orientation preserving:
If v ∈h+, then D(h) ∈D(v)+, for every vector v and
hyperplane h.
12. Basic properties of the standard aﬃne polar-dual transformation:
• polar-duality preserves above/below properties: if a point p is above line l, then
the polar line D(p) is below the polar point D(l);
• the polar-dual of a conﬁguration of points P is an arrangement of nonvertical lines
L = D(P), and vice versa;
• the polar-dual of a set of points given in increasing order of their x-coordinates is
a set of lines given in increasing order of their slopes;
• the polar-dual of the set of points on the convex hull of P is the set of lines
on the upper and lower envelopes of the polar arrangement D(P); convex hull
computation dualizes to half-plane intersection;
• semi-spaces of P dualize to vertices, edges, and cells of the polar arrangement
D(P);
• isomorphic arrangements of lines dualize to isomorphic conﬁgurations of points;

942
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
• the polar-duals of lines pipj inducing (k−1)-sets and k-sets in a point conﬁguration
P = {p1, . . . , pn} are the vertices li ∩lj on levels k and n−k of the polar-dual
arrangement L = {l1, . . . , ln}.
13. The polar-dual of an acyclic vector conﬁguration in R3 is an acyclic central plane
arrangement.
14. The upper envelope of a line arrangement can be computed optimally in O(n log n)
time.
15. The arrangement graph of a line arrangement can be computed in O(n2) time and
space.
16. The face incidence lattice of a hyperplane arrangement in Rd can be computed
in O(nd) time and space.
17. The standard polar-duality is ubiquitously used in computational geometry. For
example, it is used to derive algorithms for half-plane intersection from convex hull algo-
rithms, to translate between line slope and point x-coordinate selection, and to compute
the visibility graph in O(n2) time using the polar-dual arrangement graph. See [Ed87]
(Chapter 12) for a collection of such problems.
18. In computational geometry, k-levels are related to the furthest k-neighbors Voronoi
diagrams via a lifting transformation that reduces the computation of dimension d
Voronoi diagrams to dimension d + 1 arrangements of hyperplanes.
19. k-levels in arrangements, as polar-duals to k-sets, have an abundance of applications
in statistics.
Examples:
1. The following ﬁgure shows a line arrangement in general position. The arrangement
graph has 10 vertices corresponding to the black points (which could be labeled with
pairs of indices of lines such as 12, 13, etc.), 15 edges corresponding to the bounded line
segments such as (12, 13), and 2×5 = 10 unbounded edges. The upper envelope (0-level),
whose list of lines is {1, 2, 3, 5}, is shown highlighted. The dashed piecewise linear curve
is the 2-level.
1
0-level = upper envelope
2
3
4
5
2-level
2-level
2. The zone of line 3 in the line arrangement {1, 2, 4, 5} from the ﬁgure of Example 1 is
depicted in the following ﬁgure. It has ﬁve cells (one bounded, four unbounded), whose
boundaries sum up to 12 segments.

Section 13.1
ARRANGEMENTS OF GEOMETRIC OBJECTS
943
1
2
3
4
5
3. The line arrangements in the ﬁgure for Example 1 and part (a) of the following ﬁgure
are isomorphic. Those in parts (a) and (b) of the following ﬁgure are not isomorphic.
1
2
3
4
5
(a)
1
2
3
4
5
(b)
4. The following ﬁgure illustrates the standard polar-duality. The arrangement is polar-
dual to the conﬁguration of points in the ﬁgure of §13.1.1, Example 1; hence the lines are
given by the equations 1: y = 0, 2: y = −x−1, 3: y = 3x, 4: y = x−1 and 5: y = 2x−2.
In the primal conﬁguration, point 2 is above line 13. In the polar-dual, line 2 is below
the intersection point of lines 1 and 3.
1
2
3
4
5
13.1.3
PSEUDOLINE ARRANGEMENTS
Pseudoline arrangements represent a natural generalization of line arrangements, retain-
ing incidence and orientation properties, but not straightness. They provide a topological
representation for rank 3 oriented matroids (see §13.1.4), which in turn abstract combi-
natorial properties of vector conﬁgurations and oriented line arrangements.
Deﬁnitions:
A pseudoline is a planar curve (which may be given an orientation). It is x-monotone
if the curve is x-monotone.
A pseudoline arrangement is a ﬁnite collection of simple planar curves that intersect
pairwise in exactly one point, where they cross, and not all of which have a point in

944
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
common.
It is labeled if the pseudolines are given in a ﬁxed order {l1, . . . , ln} and
oriented if the pseudolines are oriented.
A pseudoline arrangement is realizable or stretchable if it is isomorphic to a line
arrangement.
Note:
The following terms, deﬁned in §13.1.1 and §13.1.2 for line arrangements, have
straightforward generalizations to pseudoline arrangements:
open half-planes, general
position, cell, arrangement graph, isomorphism, upper/lower envelope, k-level, zone.
Facts:
1. Every arrangement of pseudolines is isomorphic to an arrangement of x-monotone
piecewise linear pseudolines (a wiring diagram such as shown in the ﬁgure of Example
2).
2. The arrangement graph of a pseudoline arrangement in general position has
 n
2

vertices, n2 edges, and
 n
2

+ n + 1 faces.
3. The number of edges in a zone of a pseudoline in a pseudoline arrangement is at most
6n.
4. Let ek(n) be the number of edges on the k-level of a pseudoline arrangement. Then
Ω(n log k) ≤ek(n) ≤O(nk
1
3 ).
5. The logarithm of the number of isomorphism classes of pseudoline arrangements
is Θ(n2). The same number for line arrangements is Θ(n log n).
6. There exist nonstretchable pseudoline arrangements.
7. It is NP-hard to decide whether a pseudoline arrangement is stretchable.
8. Pseudoline stretchability is decidable in PSPACE.
9. Assume that a predicate is given for deciding when the intersection point of two
pseudolines is above or below a third pseudoline. Then the algorithms for computing the
upper envelopes, half-space intersection, or the arrangement graph of a line arrangement
can be adapted to work for pseudoline arrangements.
10. In computational geometry, some algorithmic solutions can be found by reducing the
problem to structures behaving like pseudolines — for example, computing the boundary
of a union or intersection of unit circles in R2, all having at least one point in common.
11. It is an open problem whether better algorithms can be devised by making explicit
use of the straightness of the lines in geometric problems. So far, the only problem where
an explicit gap in eﬃciency between lines and pseudolines has been displayed is in the
number of comparisons needed to sort the x-coordinates of the vertices of line versus
x-monotone pseudoline arrangements.
Examples:
1. The pseudoline arrangement in the following ﬁgure is stretchable, because it is iso-
morphic to the line arrangement in the ﬁgure of §13.1.2, Example 1.

Section 13.1
ARRANGEMENTS OF GEOMETRIC OBJECTS
945
1
2
3
4
5
2. The following ﬁgure shows a standard way of representing a pseudoline arrangement
as an x-monotone piecewise linear curve arrangement called a wiring diagram.
The
arrangement is the same as in the ﬁgure of Example 1.
1
2
3
4
5
3. A nonstretchable pseudoline arrangement: The theorem of Pappus in plane geometry
states that if the points 1, 2, 3 and 4, 5, 6 are collinear and in this order on two lines,
then the three intersection points of the pairs of lines 7 = (15, 24), 8 = (16, 34), and 9 =
(26, 35) are also collinear. See the following ﬁgure. The perturbed arrangement obtained
by replacing the line through 7, 8, 9 with the dashed pseudoline is not stretchable, since
it violates the theorem of Pappus.
1
2
3
4
5
7
9
6
8
13.1.4
ORIENTED MATROIDS
General oriented matroids are abstractions of vector conﬁgurations and oriented hyper-
plane arrangements. Aﬃne oriented matroids model the corresponding situation in aﬃne
spaces. They capture in various types of data structures (semi-spaces, chirotopes, Radon

946
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
partitions, arrangement graphs, hyperline sequences) combinatorial information about
n-element conﬁgurations. This forms the basis of the classiﬁcation of all n-point sets
into a ﬁnite number of equivalence classes. Each data structure satisﬁes a set of simple
properties, or axioms, which characterize a wider class of objects collectively referred to
as oriented matroids.
To simplify the exposition, in some cases only the axiomatization corresponding to points
in general position will be presented. Not all oriented matroids arise geometrically from
vector sets, but they do have a topological representation via pseudohyperplane arrange-
ments.
In rank 3, aﬃne oriented matroids are modeled by pseudoline arrangements.
Many geometric algorithms working with line arrangements or, by polarity, point conﬁg-
urations, make use of no more than oriented matroid properties and can be extended to
pseudolines.
As potential applications, oriented matroids lay the foundations for a rigorous theory of
geometric program veriﬁcation and testing.
Deﬁnitions:
Let En = {1, . . . , n} and En = {1, . . ., n}∪{1, . . . , n}. Triplets (i, j, k) ∈E
3
n are denoted
ijk.
A signed set X = (X+, X−) is a partition of a ﬁnite set X into a positive part X+
and a negative part X−. That is, X = X+ ∪X−and X+ ∩X−= ∅. In En a signed
set may be denoted as a signed sequence of indices, such as 1 2 3 4 for ({1, 3, 4}, {2}).
The complement of a signed set X = (X+, X−) is the set −X = (X−, X+).
The support of a signed set X = (X+, X−) is the unsigned set X.
The size of a signed set X = (X+, X−) is the size of the support of X.
The signed double covering of a ﬁnite set X is the set X = X+ ∪X−, where X+ = X
and X−= {x | x ∈X} is a signed distinct copy of X (its elements called negated
elements), X+ ∩X−= ∅. If x ∈X−, then x is the corresponding nonnegated element
in X+.
A basis of a vector conﬁguration [point conﬁguration] V ⊂Rd is a subset of V , identiﬁed
by a d-set of indices, which is a basis [simplex] of the ambient space Rd. A signed basis
is an ordered basis together with its sign.
The chirotope of an ordered vector conﬁguration [point conﬁguration] is the set of all
signed bases of V .
An alternating function is a function f : Ed
n →R such that the sign of f(i1, . . . , id)
is preserved under an even permutation and negated under an odd permutation of the
d-tuple (i1, . . . , id).
An antisymmetric function is a function f : E
d
n →R such that its sign changes when
one of the parameters is negated. [For example, f(i1, i2, . . . , id) = −f(i1, i2, . . . , id).]
The (rank 3) Grassmann-Pl¨ucker relations are the identities
[1 2 3][1 4 5] −[1 2 4][1 3 5] + [1 2 5][1 3 4] = 0
satisﬁed by the determinants [i j k] = det(vi, vj, vk), for any ﬁve vectors vi, 1 ≤i ≤5.
The (rank d) Grassmann-Pl¨ucker relations are the identities
[i1 . . . id−21 2][i1 . . . id−23 4] −[i1 . . . id−21 3][i1 . . . id−22 4] +
[i1 . . . id−21 4][i1 . . . id−22 3] = 0

Section 13.1
ARRANGEMENTS OF GEOMETRIC OBJECTS
947
satisﬁed by the determinants [i1 . . . id−2j k] = det(vi1, . . . , vid−2, vj, vk), for any d + 2
vectors vij (1 ≤j ≤d−2) and vi (1 ≤i ≤4).
Chirotope axioms (rank d): A function χ: E
d
n →{−1, 0, +1} is a chirotope of rank d
if it satisﬁes the following conditions:
• χ is alternating and antisymmetric;
• for any d + 2 generic points i1 . . . id−21 2 3 4, the signs χ(i1 . . . id−2j k) of the six
triplets involved in the Grassmann-Pl¨ucker relations are such that equality is
possible.
A uniform chirotope is a chirotope function that takes nonzero values on all d-tuples.
Chirotope axioms (uniform, rank 3):
A function χ: E
3
n →{−1, +1} is a uniform
chirotope of rank 3 if it satisﬁes the following conditions:
• χ is alternating and antisymmetric;
• for any ﬁve generic points a, b, i, j, k, if χ(a b i) = χ(a b j) = χ(a b k) = +1 and
χ(a i j) = χ(a j k) = +1, then χ(a i k) = +1.
The chirotope χ is aﬃne if, in addition, it satisﬁes the following axiom:
• for any four points i, j, k, l, if χ(i j k) = χ(i k l) = χ(i l j) = +1, then χ(j k l) = +1.
An oriented matroid given by a chirotope is an abstract set of points labeled
{1, . . ., n}, together with a function χ satisfying the chirotope axioms.
The circuit of a set of labeled vectors V = {v1, . . . , vn} is the signed set C = (C+, C−),
where C+ = {j | αj > 0} and C−= {j | αj < 0}, of indices of the nonnull coeﬃcients αj
in a minimal linear dependency V ′ = {vi1, . . . , vik} of V with Pk
j=1 αjvij = 0. If C is a
circuit, its complement −C is also a circuit.
A Radon partition of a set of labeled points P is a signed set C = (C+, C−) of points
of P such that the convex hull of the points in C+ intersects the convex hull of the points
in C−.
A minimal Radon partition (or circuit) is a Radon partition whose support is mini-
mal with respect to set inclusion.
An oriented matroid of an ordered vector [point] conﬁguration given by its circuits is
the set of all circuits of V .
A set C of signed subsets of En satisﬁes the circuit axioms if
• ∅/∈C;
• if C ∈C then −C ∈C;
• (minimality): if C = (C+, C−) is a circuit, then no subset of the support of C is
the support of another circuit;
• (exchange): if C1 and C2 are two circuits such that C1 ̸= −C2 and e ∈C+
1 ∩C−
2 ,
then there exists another circuit D such that e ̸∈D, D+ ⊂C+
1 ∪C+
2 , and
D−⊂C−
1 ∪C−
2 .
The oriented matroid given by its circuits is an abstract set En together with a set
of signed sets C satisfying the circuit axioms.

948
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
A cocircuit of a labeled vector conﬁguration V is a signed set C = (C+, C−) of En,
induced by a subset of d −1 vectors spanning a central hyperplane h. For an arbitrary
orientation of h, C+ is the set of indices of elements in V lying in h+ and C−is the set
of indices of elements in V lying in h−.
The cocircuit axioms are the conditions obtained from the circuit axioms by replacing
“circuit” with “cocircuit”.
An oriented matroid given by its cocircuits is an abstract set En, together with a
set of signed sets C satisfying the cocircuit axioms.
A circular sequence of period k is a doubly inﬁnite sequence (qi)i∈Z with qi = qi+k
for all i ∈Z.
A signed permutation of a set S is a permutation of S whose elements are also assigned
a sign; for example, 1 3 4 2, where 3 is negative and 1, 2, and 4 are positive.
An alternating circular sequence is a circular sequence (qi)i∈Z
with half-period k,
deﬁned with elements from a signed double covering qi ∈X and satisfying the property
qi = qi+k for all i ∈Z.
A representation of an alternating circular sequence can be obtained from any of its
subsequences of k consecutive elements (half period) {q1, . . . , qk}.
A star (or rank 3 hyperline sequence) associated with a point pi ∈P ⊆R2 [vector
vi ∈V ⊆R3] is an alternating circular sequence of subsets of indices in En obtained
by rotating an oriented line [oriented central plane] in counterclockwise order around pi
[the line through vector vi] and recording the successive positions where it coincides with
lines [central planes] deﬁned by pairs of points (pi, pj) with pj ∈P \{pi} [vectors (vi, vj),
with vj ∈V \ {vi}]. If a point pj is encountered by the rotating line in the positive
direction from pi, it will be recorded as a positive index, otherwise it will be recorded
as a negative index. When the points are not in general position, several may become
simultaneously collinear with the rotating line, and they are recorded as one subset Li
j.
The sequence is denoted by a half-period sI = (Li
1, Li
2, . . . , Li
ki), where Li
j ⊂En \ {i, i}.
A cluster of stars (or rank 3 hyperline sequences) associated with a point (or
vector) conﬁguration P is the ordered set of n stars s1, . . . , sn, one for each point
pi ∈P.
A uniform cluster of stars is a cluster of stars corresponding to a set of points in
general position. (Each star is a sequence of individual indices.)
An oriented matroid of a vector (or point) set V given by its cluster of stars
is the cluster of stars associated with V .
A star (or rank 3 hyperline sequence) associated with an element ci of a big-
circle arrangement C = {c1, . . . , cn} on S2 is an alternating circular sequence of subsets
of indices in En obtained by traversing the oriented big circle in its given direction and
recording in order the intersections of ci with the other big circles cj (j ̸= i). Each
intersection is recorded as a signed index j: positive if cj crosses ci from left to right,
negative otherwise.
The cluster of stars (or rank 3 hyperline sequences) associated with a big-circle
arrangement is the set of n stars s1, . . . , sn, one for each circle ci ∈C.
The cluster of stars associated with an oriented central plane arrangement
in R3 [line arrangement in R2] is the cluster of stars of the big-circle arrangement

Section 13.1
ARRANGEMENTS OF GEOMETRIC OBJECTS
949
associated with the central plane arrangement [to the central plane arrangement induced
by the line arrangement via the imbedding of R2 as the plane z = 1 in R3].
The cluster of stars associated with a pseudoline arrangement [a pseudocon-
ﬁguration of points] is the generalization from straight lines to pseudolines obtained
by recording the order of the vertices of the arrangement along a pseudoline (positive or
negative according to whether the line crossing at that vertex comes from right or left)
[the circular counterclockwise order of the pseudolines incident with a point].
A cluster of stars permutation is an ordered set of alternating circular sequences
s1, . . . , sn with the property that the representative half-period of sequence si is a signed
permutation of the set En \ {i}.
A chirotope function associated with a set of cluster of stars permutations
s1, . . . , sn is a function χ: E3
n →{−1, +1} deﬁned by χ(i j k) = +1 if, in the ith se-
quence si and in a half period of it where both j and k occur positively, j occurs before k.
Otherwise χ(i j k) = −1.
A set En, together with an ordered set of alternating circular sequences s1, . . . , sn, satis-
ﬁes the cluster of stars axioms (uniform, rank 3) if the set of sequences are cluster
of star permutations whose associated chirotope function is alternating.
A (uniform, rank 3) oriented matroid given by its cluster of stars is a set En
together with n alternating sequences satisfying the cluster of stars axioms.
An abstract set En, together with a set of nd−2 (uniform) alternating sequences (in-
dexed by (d−2)-tuples (i1, . . . , id−2)), is an oriented matroid given by its hyper-
line sequences (uniform, rank d) if the chirotope function χ: Ed
n →{−1, +1}
associated with it is alternating.
[The function χ: Ed
n →{−1, +1} is deﬁned by
χ(i1 . . . id−2j k) = +1 if in the star indexed by i1, . . . , id−2 and in a half period where
both j and k occur positively, j occurs before k. Otherwise χ(i1 . . . id−2j k) = −1.]
Deletion is the removal of a point (vector, line, etc.) from a conﬁguration and recording
the oriented matroid (chirotope, circuits, cluster of stars, etc.) only for the remaining
points.
In a rank d central plane arrangement, the contraction on element i is obtained by
identifying hi with Rd−1 and intersecting it with all the other hyperplanes to obtain a
rank d−1 arrangement with one less element.
The oriented matroid obtained by a one-element deletion in the hypersequence
representation is the matroid obtained by removing the element from all the hyperline
sequences of the original oriented matroid, and discarding all hyperline sequences whose
labels contain that element.
The oriented matroid obtained by a one-element contraction in the hyperse-
quence representation is the matroid obtained by retaining only the hyperline sequences
whose labels contain the element, and dropping it from the labels.
A rank 2 contraction of a cluster of stars is one of the stars.
A minor of an oriented matroid given by hyperline sequences is an oriented matroid
obtained by a sequence of deletions and/or contractions.
A (Euclidean) pseudoconﬁguration of points is a pair (P, L) with P = {p1, . . . , pn}
a planar set of points and L = {l1, . . . , lm} a pseudoline arrangement, such that for every
pair of distinct points (pi, pj) there exists a unique pseudoline lij ∈L incident with them.

950
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
If a pseudoline arrangement is intersected with a vertical line lv (x = −M), for some
very large constant M, and all the vertices of the arrangement lie to the right of lv, then
the order in which the pseudolines in L cross vh (decreasing by the y-coordinates of the
crossings) is the (increasing) slope order of the pseudolines.
The (standard aﬃne) pseudo polar-duality is the association between an x-mono-
tone pseudoline arrangement L = {l1, . . . , ln} given in slope order and a pseudo conﬁgu-
ration of points (P, L′), P = {p1, . . . , pn}, given in increasing order of the x-coordinates
and with L′ being x-monotone, satisfying the property that the cluster of stars associated
with L and to P is the same.
Facts:
1. Cocircuits correspond to semi-spaces, when the deﬁning hyperplane is incident with
d −1 independent elements of the conﬁguration.
2. In the rank d uniform oriented matroid associated with a vector conﬁguration in
general position in Rd, all the d-tuples are bases, all the (d+1)-tuples are supports of
circuits, and all the (n−d+1)-tuples are supports of cocircuits.
3. The oriented matroid associated with an aﬃne vector conﬁguration V and the aﬃne
oriented matroid associated with the aﬃne point conﬁguration induced by V are the
same.
4. The chirotope function χ associated with the cluster of stars of a vector or point
conﬁguration is an alternating and antisymmetric function.
5. The two given systems of chirotope axioms are equivalent (for the uniform case).
6. The hyperline sequences of a contraction by one element of a central plane arrange-
ment are the induced rank (d−1) contraction by that element of the set of rank d hyperline
sequences of the original arrangement.
7. The induced rank (d−1) contraction of a set of rank d hyperline sequences is a rank
(d−1) set of hyperline sequences.
8. A minor of an oriented matroid (given by its hyperline sequences) is an oriented
matroid.
9. For two labeled vector (or point) conﬁgurations V1 and V2, the following statements
are equivalent:
• V1 and V2 have the same chirotope;
• V1 and V2 have the same order type;
• V1 and V2 have the same hyperline sequences;
• V1 and V2 have the same minors.
Moreover, for any reorientation of V1 or V2:
• V1 and V2 have the same oriented matroid given by circuits;
• V1 and V2 have the same oriented matroid given by cocircuits.
This justiﬁes the unique name oriented matroid for the equivalence class of vector con-
ﬁgurations with the same chirotope (or clusters, order type, etc.), and for a reorientation
class of an oriented matroid.
10. For a labeled vector conﬁguration V in Rd+1, the following statements are equiva-
lent:
• V is acyclic (aﬃne);

Section 13.1
ARRANGEMENTS OF GEOMETRIC OBJECTS
951
• there is a labeled point conﬁguration P in Rd whose oriented matroid (or order
type or chirotope) is the same as the oriented matroid of V .
11. Multiplying the elements of a vector conﬁguration by a positive scalar yields a vector
conﬁguration with the same oriented matroid. In particular, for any vector conﬁguration
V in Rd+1, there exists an equivalent vector conﬁguration on the d-sphere.
12. For any vector conﬁguration, there exists a reorientation of some of its vectors which
makes it aﬃne.
13. The set of circuits of an aﬃne (acyclic) vector conﬁguration does not contain the
positive cycle (En, ∅).
14. Two projective line arrangements with the same arrangement graph have polar-dual
conﬁgurations of points in the same reorientation class (and this can be generalized to
arbitrary dimension d).
15. If a labeled vector conﬁguration in R3 and a labeled oriented arrangement of central
planes are polar-dual, then they have the same hyperline sequences (and this can be
generalized to arbitrary dimension d).
16. The number of oriented matroids of rank d and size n is 2O(nd−1).
17. The number of realizable oriented matroids of rank d and size n is 2O(n log n).
18. Folkman-Lawrence topological representation theorem:
Every oriented matroid of
rank d can be represented as a (d−1)-pseudosphere arrangement on Sd.
19. Every aﬃne oriented matroid of rank 3 can be represented as a pseudoline arrange-
ment and as a (polar-dual) pseudoconﬁguration of points.
20. There exist nonrealizable oriented matroids of any rank.
21. Realizable oriented matroids cannot be characterized by a ﬁnite set of excluded
minors.
Examples:
1. The function χ(i, j, k) = sign det(vi, vj, vk), for vl ∈V = {v1, . . . , vn} ⊂R3, is
alternating antisymmetric.
2. The following ﬁgure shows an example of a point conﬁguration in general position,
together with the connecting lines.
1
2
3
4
5
Its oriented matroid is given by
• chirotope: 1 2 3−, 1 2 4−, 1 2 5+, 1 3 4+, 1 3 5+, 2 3 4+, 2 3 5+, 3 4 5+. The other
signed triplets are computed by antisymmetry and alternation.
• minimal Radon partitions: 1 2 3 4, 1 2 3 5, 1 2 4 5, 1 3 4 5, 2 3 4 5 and their comple-
ments.

952
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
• cocircuits: 3 4 5, 2 4 5, 2 3 5, 2 3 4, 1 4 5, 1 3 5, 1 3 4, 1 2 5, 1 2 4, 1 2 3 and their
complements.
• cluster of stars: 1: 3 4 2 5,
2: 1 5 3 4,
3: 1 4 5 2,
4: 5 2 1 3, and 5: 1 2 3 4. (An
arbitrary half-period was chosen for the circular sequences.)
3. The oriented matroid given as a cluster of stars for the line arrangement in the ﬁgure
of §13.1.2, Example 1 (or for the pseudoline arrangement in the ﬁgure of §13.1.3, Example
1) is
1: 2 3 5 4,
2: 1 3 5 4,
3: 1 2 5 4,
4: 5 1 2 3,
5: 4 1 2 3.
The orientation of the lines is assumed to be in increasing order of the x-coordinates. Its
minor by the deletion of element 3 is 1: 2 5 4, 2: 1 5 4, 4: 5 1 2, and 5: 4 1 2. Its contraction
on point 3 is 1 2 5 4.
13.2
SPACE FILLING
Discrete geometry is the study of discrete arrangements of objects in Euclidean and non-
Euclidean spaces. This subject dates back to the early 1610s when J. Kepler studied
packings of Euclidean space with equally-sized spheres, formulating a conjecture about
the greatest possible density of such a packing that was unsettled until the late 1990s.
The modern systematic development of the subject began in the late 1940s with the
work of L. Fejes T´oth. The Hungarian school he founded focused mainly on packing and
covering problems. Two areas from discrete geometry, dense sphere packings and tilings,
occupy a substantial part of this section on space ﬁlling. Both areas have been active
areas of research for the past 75 years.
13.2.1
PACKING
The most important results on packings are centered around density and contact num-
bers, including kissing numbers, foam problems, and soft packings.
Deﬁnitions:
A convex body in Rd is a compact convex subset of Rd with nonempty interior.
A family P = {K1, K2, . . .} of the convex bodies K1, K2, . . . in Rd forms a packing
of Rd if no two of the convex bodies K1, K2, . . . have an interior point in common.
Let P = {K1, K2, . . .} be a packing of Rd and let BR be the closed ball of radius R
centered at the origin in Rd. The lower density and upper density of P are deﬁned
by
δ(P) = lim inf
R→+∞
P
Ki⊂BR Vol(Ki)
Vol(BR)
and
δ(P) = lim sup
R→+∞
P
Ki⊂BR Vol(Ki)
Vol(BR)
.
If δ(P) = δ(P) = δ(P), then δ(P) is called the density of P.
For a convex body K ⊂Rd, let δ(K) denote the largest (upper) density of packings by
congruent copies of K in Rd. In particular,

Section 13.2
SPACE FILLING
953
δT (K) = the largest (upper) density of packings by translates of K in Rd;
δL(K) = the largest (upper) density of packings by lattice translates of K in Rd.
Let P = {Bd
1, Bd
2, . . .} be a packing of unit balls in Rd with centers c1, c2, . . . .
The
Voronoi cell with center ci is the convex polyhedral set Vi = {x ∈Rd | |x −ci| =
minj |x −cj|}.
The kissing number problem asks for the maximum number k(d) of nonoverlapping
unit balls that can touch a unit ball in the d-dimensional Euclidean space Rd.
The Hadwiger number (or translative kissing number) of the convex body K in
Rd, denoted by H(K), is the largest number of nonoverlapping translates of the convex
body K that can all touch K.
The isoperimetric quotient of the convex body K in Rd is iq(K) = (Svol(bd(K)))d
(Vol(K))d−1 ,
where Svol(bd(K)) denotes the (d −1)-dimensional surface volume of the boundary
bd(K) of K and Vol(K) stands for the d-dimensional volume of K.
The contact graph of a ﬁnite unit ball packing in Rd is the (simple) graph with vertices
corresponding to the packing elements and where two vertices are connected by an edge
if and only if the corresponding two packing elements touch each other.
The number of edges of a contact graph is called the contact number of the underlying
unit ball packing.
The contact number problem asks for the largest contact number, that is, for the
maximum number c(n, d) of edges that a contact graph of n nonoverlapping unit balls
can have in Rd. Analogously, the largest contact number of packings by n translates of
a convex body K in Rd is denoted by c(K, n, d).
Let T be a tiling of R3 into convex polyhedra Pi, i = 1, 2, . . ., each containing a unit ball
Pi containing the closed 3-dimensional ball Bi having radius 1 for i = 1, 2, . . .. Assuming
that there is a ﬁnite upper bound for the diameters of the convex cells in T , we say that T
is a normal tiling of R3 with the underlying packing P of the unit balls Bi, i = 1, 2, . . . .
The (lower) average surface area s(T ) of the cells in T is deﬁned by
s(T ) = lim inf
L→∞
P
{i|Bi⊂CL} sarea(Pi ∩CL)
card{i | Bi ⊂CL}
,
where CL denotes the cube centered at the origin o with edges parallel to the coordinate
axes of R3 and with edge length L; here sarea(S) and card(S) denote the surface area and
cardinality of the set S. (Note that s(T ) is independent of the choice of the coordinate
system of R3.)
Let P = {ci+Bd | i = 1, 2, . . . with |cj −ck| ≥2 for all 1 ≤j < k} be an arbitrary inﬁnite
packing of unit balls in Rd. For any d ≥2 and λ ≥0 let Pλ = S+∞
i=1 (ci + (1 + λ)Bd)
denote the outer parallel domain of P = S+∞
i=1 (ci + Bd) having outer radius λ and let
¯δd(Pλ) = lim sup
R→+∞
Vol(Pλ ∩RBd)
Vol(RBd)
be the (upper) density of the outer parallel domain Pλ assigned to the unit ball packing
P in Rd. Finally, let

954
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
¯δd(λ) = sup
P
¯δd(Pλ)
be the largest density of the outer parallel domains of unit ball packings having outer
radius λ in Rd. Then we say that the family {ci +(1+λ)Bd | i = 1, 2, . . . } of closed balls
of radii 1 + λ is a packing of soft balls with penetrating constant λ if P = {ci + Bd |
i = 1, 2, . . .} is a unit ball packing of Rd.
In particular, ¯δd(Pλ) is the (upper) density of the soft ball packing {ci + (1 + λ)Bd |
i = 1, 2, . . .} with ¯δd(λ) the largest density of packings of soft balls of radii 1 + λ with
penetrating constant λ.
Facts:
1. There are two major problems concerning density:
• Given a convex body K ⊆Rd, ﬁnd eﬃcient packings with congruent copies of K;
i.e., ﬁnd packings with congruent copies of K in Rd having “relatively high”
density.
• Find a “good” upper bound for δ(K).
2. Sphere packing in n-dimensional space, for n large, is important in designing codes
that are eﬃcient and unlikely to contain errors when data is transmitted.
3. If K is a convex body in Rd, 2ζ(d)
 2d
d

≤δL(K) ≤δT (K) ≤δ(K), where ζ(d) =
1 + 1
2d + 1
3d + · · · denotes Riemann’s zeta function.
4. If K is a centrally symmetric convex body in Rd, ζ(d)
2d−1 ≤δL(K) ≤δT (K) ≤δ(K).
5. Facts 3 and 4 have been improved slightly for diﬀerent classes of convex bodies and
subclasses of centrally symmetric convex bodies. See [DaRo47], [ElOdRu91], and [Sc63].
6. For each d, (d −1) ζ(d)
2d−1 ≤δL(Bd) ≤δT (Bd) = δ(Bd).
7. For every convex domain D, δL(D) = δT (D).
8. For every centrally symmetric convex domain D, δL(D) = δT (D) = δ(D).
9. There exists an ellipsoid C in R3 for which δL(C) < δ(C).
10. The class of convex bodies C ⊆Rd for which δL(C), δT (C), and δ(C) can be
determined (for a given d) is very small.
11. The densest packing of unit balls in R3 has density δ(B3) =
π
√
18 = 0.7404..., which
is attained by the “cannonball packing” [Ha05], [Ha12]. This fact was conjectured by
Johannes Kepler in the 17th century. It was proved by Thomas Hales in 1998. His proof
was based on examining a large number of cases, following an approach ﬁrst conceived by
Fejes T´oth in 1953. Hales’s proof by exhaustion relied on complex computer calculations,
and, after extensive examination over many years, was generally believed to be complete.
To remove any doubt, Hales and a large team embarked on an eﬀort to produce a formal
proof of the conjecture. After more than 11 years, Hales and 21 collaborators succeeded
in completing a formal proof using the proof assistants Isabelle and HOL Light.
12. No packing of unit balls in R8 has density greater than
π4
384 = 0.2536..., the density
of the E8-lattice packing [Vi16].
13. The Leech lattice achieves the optimal sphere packing density in R24 with δ(B24) =
π12
12! = 0.001929... and it is the only periodic packing in R24 with that density [CoEtal16].

Section 13.2
SPACE FILLING
955
14. The Cohn-Elkies linear programming bound [CoEl03] improves on all upper bounds
for δ(Bd) obtained earlier in dimensions 4 ≤d ≤128 [CoZh14]. On the other hand,
for d ≥129 the best bounds are 2−(1+o(1))d ≤δ(Bd) ≤2−(0.5990...+o(1))d. See [KaLe78],
[Va11], and [Ve13].
15. The known values of k(d) are k(2) = 6 (trivial), k(3) = 12 ([ScVa53]), k(4) = 24
([Mu08]), k(8) = 240 ([OdSl79]), and k(24) = 196,560 ([OdSl79]). In general, k(d) ≤
20.401d(1+o(1)) [KaLe78].
16. c(n, 2) = ⌊3n −√12n −3⌋holds for all n ≥2 [Ha74]. On the other hand, one
has 6n −7.863n
2
3 < c(n, 3) < 6n −0.926n
2
3 for all n ≥2, and c(n, d) <
1
2k(d) n −
1
2d δ(Bd)−d−1
d n
d−1
d
<
1
220.401d(1+o(1))n −
1
2d 20.599(1+o(1))(d−1)n
d−1
d
for all d ≥4, n ≥2
[Be13].
17. Let K be a convex domain diﬀerent from a parallelogram in R2.
Then for all
n ≥2, one has c(K, n, 2) = ⌊3n −√12n −3⌋. If K is a parallelogram, then c(K, n, 2) =
⌊4n −√28n −12⌋holds for all n ≥2 [Br96]. On the other hand, if K is a convex body
in Rd, d ≥3, then
c(K, n, d) ≤H(K)
2
n −
1
2dδT (K)
d−1
d
d
s
iq(Bd)
iq(K) n
d−1
d
≤3d −1
2
n −
d√ωd
2d+1 n
d−1
d ,
where ωd =
π
d
2
Γ( d
2 +1) is the volume of the unit ball Bd [Be10].
18. Let T be an arbitrary normal tiling of R3. Then the average surface area of the
cells in T is always at least
24
√
3: i.e., s(T ) ≥24
√
3 = 13.8564... [Be13].
19. Let T d = conv{t1, t2, . . . , td+1} be a regular d-simplex of edge length 2 in Rd, d ≥2,
and let 0 ≤λ <
q
2d
d+1 −1. Then ¯δd(λ) ≤
Vol(T d∩(∪d+1
i=1 ti+(1+λ)Bd))
Vol(T d)
< 1 holds [BeL´a15].
Open Questions and Conjectures:
1. Determine the asymptotic behavior of δ(Bd) as d →+∞.
2. Determine the asymptotic behavior of c(n, 3) as n →+∞.
3. Foam problem of unit ball packings:
If the Euclidean 3-space is partitioned into
convex cells each containing a unit ball, how should the shapes of the cells be designed
to minimize the average surface area of the cells? In particular, if T denotes the Voronoi
tiling of an arbitrary unit ball packing in R3, then prove or disprove that s(T ) ≥12
√
2 =
16.9705....
4. Soft dodecahedral conjecture: Let D ⊂R3 be a regular dodecahedron circumscribed
by the unit ball B3. Then, for λ with 0 < λ <
√
3 tan π
5 −1 = 0.258408 . . ., we set
¯τ3(λ) = Vol(D∩(1+λ)B3)
Vol(D)
, where
√
3 tan π
5 is the circumradius of D. The soft dodecahedral
conjecture asserts that if 0 < λ <
√
3 tan π
5 −1 = 0.258408 . . ., then ¯δ3(λ) ≤¯τ3(λ).
13.2.2
COVERING
Deﬁnitions:
A family C = {Ki | i ∈I} of convex bodies in Rd forms a covering of Rd (that is,
covers Rd) if each point of Rd belongs to at least one convex body of C.

956
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
The lower density and upper density of a covering C are
ν(C) = lim inf
R→+∞
P
Ki∩BR̸=∅Vol(Ki)
Vol(BR)
and
ν(C) = lim sup
R→+∞
P
Ki∩BR̸=∅Vol(Ki)
Vol(BR)
,
where BR denotes the closed ball of radius R centered at the origin in Rd.
If ν(C) = ν(C) = ν(C), then ν(C) is called the density of C.
For a convex body K ⊆Rd, we deﬁne these quantities:
ν(K) = the smallest (lower) density of coverings of Rd by congruent copies of K;
νT (K) = the smallest (lower) density of coverings of Rd by translates of K;
νL(K) = the smallest (lower) density of coverings of Rd by lattice translates of K.
A homothety is an aﬃne transformation of Rd of the form x 7→t + λx, where t ∈Rd
and λ is a nonzero real number.
The image t + λK of a convex body K under a homothety is said to be its homothetic
copy (or simply a homothet). A homothetic copy is positive if λ > 0 and negative
otherwise. Furthermore, a homothetic copy with 0 < λ < 1 is called a smaller positive
homothet. Let α(K) denote the covering number of K, the minimum number of
smaller positive homothets of K with a union containing K.
Given 0 < k < d deﬁne a k-codimensional cylinder C in Rd as a set which can be
presented in the form C = H + B, where H is a k-dimensional linear subspace of Rd and
B is a measurable set (called the base) in the orthogonal complement H⊥of H.
For a given convex body K and a k-codimensional cylinder C = H + B, we deﬁne the
cross-sectional volume crvK(C) of C with respect to K by
crvK(C) = Vold−k(C ∩H⊥)
Vold−k(PH⊥K) = Vold−k(PH⊥C)
Vol d−k(PH⊥K) =
Vold−k(B)
Vol d−k(PH⊥K),
where PH⊥: Rd →H⊥denotes the orthogonal projection of Rd onto H⊥. Notice that
for every invertible aﬃne map T : Rd →Rd one has crvK(C) = crvT K(T C).
Facts:
1. There are two major problems concerning covering:
• Given a convex body K ⊆Rd, ﬁnd eﬃcient coverings of Rd with congruent copies
of K; that is, ﬁnd coverings of Rd by congruent copies of K having “relatively
small” density.
• Find a “good” lower bound for ν(K). (This is a highly nontrivial task for most
of the convex bodies K ⊆Rd.)
2. If K is a convex body in Rd, then
ν(K) ≤νT (K) ≤d(ln d) + d(ln ln d) + 4d
and
νT (K) ≤νL(K) ≤d(log2 log2 d)+c
for some constant c.
3. If Bd denotes the d-dimensional closed unit ball in Rd, then
ν(Bd) = νT (Bd) ≤νL(Bd) ≤cd(ln d)
1
2 log2(2πe)
for some constant c.

Section 13.2
SPACE FILLING
957
4. Take a regular simplex inscribed in a unit ball in Rd and draw unit balls around each
vertex. Let τd be the ratio of the sum of the volumes of the portions of these balls lying
in the regular simplex to the volume of the regular simplex. Then τd ≤ν(Bd).
5. τd ∼
d
e3/2 . (Thus, Facts 2 and 3 give strong estimates for ν(Bd). Moreover, for d = 1
and 2, the lower bound τd is sharp.)
6. The thinnest lattice covering of Rd by unit balls has been determined up to dimension
5 only. The following table lists the optimal lattices. See [CoSl93].
dimension
1
2
3
4
5
thinnest lattice covering
Z
A2
A
∗
3
A
∗
4
A
∗
5
7. If K is a convex domain in R2, then α(K) ≤4. If K is a convex body in R3, then
α(K) ≤16. Furthermore, if P is a convex polyhedron with aﬃne symmetry in R3, then
α(P) ≤8 [Be10].
8. If K is a convex body in Rd, d ≥2, then α(K) ≤Vol(K−K)
Vol(K) (d ln d+d ln ln d+5d+1) ≤
 2d
d

(d ln d + d ln ln d + 5d + 1). (Moreover, for suﬃciently large d, 5d can be replaced
by 4d.)
Furthermore, if B[X] ⊂Rd, d ≥3 is an intersection of d-dimensional unit
balls centered at the points of X ⊂Rd having diameter diam(X) ≤1, then α(B[X]) <
5d
3
2 (4 + ln d)
  3
2
 d
2 [Be13].
9. Let K be a convex body in Rd.
Let C1, . . . , CN be k-codimensional cylinders in
Rd, 0 < k < d, such that K ⊂SN
i=1 Ci. Then PN
i=1 crvK(Ci) ≥
1
(
d
k). Moreover, if K is
an ellipsoid and C1, . . . , CN are 1-codimensional cylinders in Rd such that K ⊂SN
i=1 Ci,
then PN
i=1 crvK(Ci) ≥1 [Be10].
10. The sum of the base areas of ﬁnitely many (1-codimensional) cylinders covering a 3-
dimensional convex body is always at least one third of the minimum area 2-dimensional
projection of the body [Be10].
11. If K is a centrally symmetric convex body and C1, . . . , CN are (d−1)-codimensional
cylinders (i.e., planks) in Rd such that K ⊂SN
i=1 Ci, then PN
i=1 crvK(Ci) ≥1 [Ba91].
Open Questions and Conjectures:
1. Hadwiger-Levi covering conjecture (1955):
If K is an arbitrary convex body in
Rd, d ≥3, then α(K) ≤2d.
2. Bang’s aﬃne plank conjecture (1951): Let K be a convex body in Rd, d ≥2, and let
C1, . . . , CN be (d−1)-codimensional cylinders (i.e., planks) in Rd such that K ⊂SN
i=1 Ci.
Then prove that PN
i=1 crvK(Ci) ≥1.
3. Bang’s cylinder covering problem (1951): Prove or disprove that the sum of the base
areas of ﬁnitely many (1-codimensional) cylinders covering a 3-dimensional convex body
is at least half of the minimum area 2-dimensional projection of the body.
13.2.3
TILING
Only a “diagonal” view of the basic deﬁnitions and theorems of this area will be given.
See [GrSh90] for additional material.

958
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
Deﬁnitions:
A tiling T of Euclidean d-space Rd is a countable family of closed topological d-cells of
Rd, the tiles of T , which cover Rd without gaps and overlaps.
A monohedral tiling is a tiling T of Rd in which all tiles are congruent to one ﬁxed
tile T , the (metrical) prototile of T . In this case, T admits the tiling T .
A regular polygon is a polygon with all sides congruent and all interior angles equal.
A regular tiling is a monohedral tiling of the plane (R2) with a regular polygon as
prototile.
A semiregular tiling is a tiling of the plane using n prototiles with the same numbers
of polygons around each vertex.
A semiregular polyhedron is a convex polyhedron with each face a regular polygon,
but where more than one regular polygon can be used as a face.
A tiling T of Rd by convex polytopes is normal if there exist positive real numbers r
and R such that each tile contains a Euclidean ball of radius r and is contained in a
Euclidean ball of radius R.
A face-to-face tiling is a tiling T by convex d-polytopes such that the intersection of
any two tiles is a face of each tile, possibly the (improper) empty face. When d = 2, such
a tiling is an edge-to-edge tiling.
A lattice tiling, with lattice L, is a tiling T by translates of a single tile T such that
the corresponding translation vectors form a d-dimensional lattice L in Rd.
A Euclidean motion σ of Rd is a symmetry of a tiling T if σ maps (each tile of) T
onto (a tile of) T . The set of all symmetries of T (a group under composition) is the
symmetry group S(T ) of T .
A periodic tiling is a tiling T of Rd such that S(T ) contains translations in d linearly
independent directions. A tiling T is nonperiodic if S(T ) contains no translation other
than the identity.
An isohedral tiling is a tiling T such that S(T ) acts transitively on the tiles of T .
An isogonal tiling is a tiling T such that S(T ) acts transitively on the vertices of T .
An isotoxal tiling is a tiling T such that S(T ) acts transitively on the edges of T .
Let T and T ′ be tilings of Rd with symmetry groups S(T ) and S(T ′). Let Φ: Rd →Rd
be a homeomorphism that maps T onto T ′. Φ is compatible with a symmetry σ of
T if there exists a symmetry σ′ of T ′ such that σ′Φ = Φσ. Φ is compatible with S(T )
if Φ is compatible with each σ in S(T ). The tilings T and T ′ of Rd are homeomeric,
or of the same homeomeric type, if there exists a homeomorphism Φ: Rd →Rd that
maps T onto T ′ such that Φ is compatible with S(T ) and Φ−1 is compatible with S(T ′).
A prototile T in Rd is aperiodic if T admits a tiling of Rd, yet all such tilings are
nonperiodic. In general, a set S of prototiles in Rd is said to be aperiodic if S admits
a tiling of Rd, yet all such tilings are nonperiodic.
Facts:
1. There are three monohedral edge-to-edge tilings of Rd with regular polygons; the
prototile must be a triangle, a square, or a hexagon. These are illustrated in the following
ﬁgure.

Section 13.2
SPACE FILLING
959
2. There are eight semiregular tilings of Rd. These tilings use two or three prototiles.
3. Shapes that are not regular polygons [polyhedra] can be used in monohedral tilings
of R2 [R3].
4. Any triangle can be used in a monohedral tiling of the plane. (Join two to form
a parallelogram and tile a strip using these parallelograms. Repeat this process with
parallel strips to tile the plane.) See the following ﬁgure.
5. Any quadrilateral can be used in a monohedral tiling of the plane. (Take a second
copy of the quadrilateral and rotate it 180 degrees. Join the two to form a hexagon. Use
the hexagons to tile the plane.) See the following ﬁgure.
6. Any pentagon with a pair of parallel sides can be used in a monohedral tiling of the
plane.
7. There are at least fourteen types of convex pentagons that can be used in a monohe-
dral tiling of the plane. It is not known if there are more.
8. There are three types of convex hexagons that can be used in a monohedral tiling
of the plane. Assume that the hexagon has vertices a, b, c, d, e, f in clockwise order, as
shown in the following ﬁgure. The prototile must be of one of the following forms:
• sum of angles at a, b, c is 360◦; length of {a, f} = length of {c, d};
• sum of angles at a, b, e is 360◦; length of {a, f} = length of {d, e} and length of
{b, c} = length of {e, f};
• angles at a, b, and c are each equal to 120◦; length of {a, b} = length of {a, f},
length of {c, b} = length of {c, d}, and length of {e, d} = length of {e, f}.

960
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
f
a
b
c
d
e
9. No convex polygon with more than six sides can be used as prototile in a monohedral
tiling of R2.
10. Of the ﬁve regular polyhedra (tetrahedron, hexahedron (cube), octahedron, dodec-
ahedron, icosahedron), only the tetrahedron and cube can be used as a prototile in a
regular tiling of R3.
11. If T is a tiling of Rd with convex tiles, then each tile in T is a convex d-polyhedron.
12. If T is a tiling of Rd with compact convex tiles, then each tile in T is a convex
d-polytope.
13. The following classiﬁcation results have a long history. See [GrSh90].
• There exist precisely 11 distance edge-to-edge isogonal plane tilings, the tiles of
which are convex regular polygons (called Archimedean tilings).
• There exist precisely 81 homeomeric types of normal isohedral plane tilings. Pre-
cisely 47 of these can be realized by a normal isohedral edge-to-edge tiling with
convex polygonal tiles.
• There exist precisely 91 homeomeric types of normal isogonal plane tilings. Pre-
cisely 63 types can be realized by normal isogonal edge-to-edge tilings with
convex polygonal tiles.
• There exist precisely 26 homeomeric types of normal isotoxal plane tilings. Pre-
cisely six types can be realized by a normal isotoxal edge-to-edge tiling with
convex polygonal tiles.
14. Let T be a convex d-polytope. If T tiles Ed by translation, then T admits (uniquely)
a face-to-face lattice tiling of Rd. Such a tile T is called a parallelotope. This result is
not true for nonconvex polytopes.
15. Several aperiodic sets have been found in the plane. Some of them, such as the
Wang tiles and Penrose tiles, possess several highly interesting properties [GrSh90].
16. Progress has been made for aperiodic tilings in higher dimensions via dynamical
systems [Ra95a].
Open Questions:
1. Extend the classiﬁcation problems to higher dimensions.
(At present, this looks
hopeless.)
2. Classify all convex d-polytopes which are prototiles of monohedral tilings of Rd.
(This problem is not even solved for the plane.) However, under suitable restrictions the
complexity of the problem changes. (See Fact 4.)
3. For d ≥5, determine whether each d-parallelotope is a Voronoi cell (see §13.2.1) for
some d-lattice. (This is known to be true for 1 ≤d ≤4.)

Section 13.3
COMBINATORIAL GEOMETRY
961
13.3
COMBINATORIAL GEOMETRY
This section studies geometric results involving combinatorics in the areas of convexity,
incidences, distances, and colorings.
In some cases the problems themselves have a
combinatorial ﬂavor, while in other cases their solution requires combinatorial tools.
13.3.1
CONVEXITY
In this subsection, questions of two diﬀerent kinds are studied. Most of them belong to
geometric transversal theory, a subject originating in Helly’s theorem. Another group
of problems grew out of the Erd˝os-Szekeres theorem, which turned out to be a starting
point of Ramsey theory.
Deﬁnitions:
A subset C of d-dimensional Euclidean space (d-space) Rd is convex if the following is
true: for any pair of points in C, the straight-line segment connecting them is entirely
contained in C.
A convex set is strictly convex if its boundary contains no line segment.
A convex body is a compact (i.e., bounded and closed) convex set with nonempty
interior.
A polytope is a bounded convex body that can be obtained as the intersection of ﬁnitely
many closed half-spaces. (§13.1.1.)
A convex polygon is a polytope in the plane.
A vertex of a polytope P is a point q ∈P, for which there is a hyperplane (§13.1.1) H
such that H ∩P = {q}.
A point set is in convex position if it is the vertex set of a polytope.
The convex hull of a set S ⊆Rd is the smallest convex set containing S.
A family C = {C1, C2, . . .} of sets in d-space is said to be intersecting if all members of
C have a point in common.
A set T ⊆Rd is a transversal of a family C of sets if T ∩Ci is nonempty for every i.
If C has a k-element transversal (|T | = k), its members can be pierced by k points.
Two sequences P = {p1, . . . , pn} and Q = {q1, . . . , qn} of points in Rk have the same
order type if, for all 1 ≤i1 < i2 < · · · < ik+1 ≤n, the orientations of the simplices in-
duced by {pi1, . . . , pik+1} and {qi1, . . . , qik+1} are the same. This order type is nontrivial
if P and Q are not contained in any hyperplane of Rk.
A k-ﬂat (an oriented k-dimensional plane) F intersects a sequence of d-dimensional
convex bodies C = {C1, . . . , Cn} consistently with the above order type if there are
xi ∈F ∩Ci such that the sequences X = {x1, . . . , xn} and P have the same order type.
Facts:
1. The convex hull of a set S is the intersection of all convex sets containing S.

962
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
2. For any set S ⊂Rd of ﬁnitely many points, not all of which lie in the same hyperplane,
the convex hull of S is a polytope. In particular, if S has d + 1 points, then its convex
hull is a simplex whose vertices are the elements of S.
3. The convex hull of the vertex set of any convex polytope P is identical with P.
4. Helly’s theorem: If a family C of at least d + 1 convex bodies in Rd has the property
that every d + 1 of its members have a point in common, then C is intersecting (i.e., all
its members have a point in common) [He23].
5. Carath´eodory’s theorem: If the convex hull of a set S ⊆Rd contains a point p, then
there exists a subset of S with at most d + 1 elements whose convex hull contains p.
6. Let S be a compact set in Rd with the property that for every (d+ 1)-element subset
T ⊂S, there is a point s ∈S such that each segment connecting s to an element of T
lies in S. Then S has a point such that every segment connecting it to an element of S
is entirely contained in S [Kr46].
7. Any set of (k −1)(d + 1) + 1 points in Rd can be partitioned into k parts whose
convex hulls have a point in common [Ra21], [Tv66].
8. Let C be any family of convex bodies in Rd with the property that the volume of the
intersection of any 2d of them is at least 1. Then the volume of the intersection of all
members of C is at least a positive constant depending only on d [B´aKaPa82].
9. For any ǫ > 0 and for any d there is a δ > 0 satisfying the following condition:
if C is a family of n (> d + 1) convex bodies in Rd having at least ǫ
  n
d+1

intersecting
(d+1)-tuples, then C has at least δn members with a point in common [Ka84].
10. For any d < q ≤p, there exists k = k(p, q, d) satisfying the following condition: if C
is a family of convex bodies in Rd such that every subfamily of C of size p contains q
members with a point in common, then C can be pierced by k points [AlKl92].
11. A sequence C = {C1, . . . , Cn} of convex bodies in Rd has a hyperplane transversal
if and only if for some 0 ≤k ≤d −1, there is a nontrivial k-dimensional order type
of n points such that every (k+2)-member subfamily of C can be met by a suitable k-ﬂat
consistently with that order type [PoWe90].
12. If Sk is any family of k-dimensional linear subspaces of Rd with the property that
any
 k+l
l

of them can be intersected by an l-dimensional subspace, then all members of
Sk can be intersected by an l-dimensional subspace. (Two subspaces intersect each other
if they have at least one point in common, diﬀerent from the origin.)
13. Any set of ﬁve points in the plane, no three of which are on a line, has four elements
in convex position.
14. Erd˝os-Szekeres theorem: For every k > 2, there exists a smallest integer n(k) with
the property that every set of at least n(k) points in the plane, no three of which are
on a line, contains k points in convex position. If k = 3, 4, 5, 6, then n(k) = 2k−2 + 1.
(These are the only known values.) Furthermore, 2k−2 + 1 ≤n(k) ≤
 2k−5
k−2

[ErSz35],
[T´oVa98], and, asymptotically, n(k) ≤2(1+o(1))k [Su17].
Examples:
1. Let S = {(1, 0, 0), (0, 1, 0), (0, 0, 1)}. The convex hull of S ⊂R3 is a triangular region,
which is not a convex body in 3-space because its interior is empty.
2. Let S = {(0, 0), (1, 0), (0, 1)}. The convex hull of S ⊂R2 is a triangular region, which
is a convex body (polygon) in the plane.
3. Let S = {(x, y) | 0 ≤x ≤2, 0 ≤y ≤2} and S′ = {(x, y) | 0 ≤x ≤3, 0 ≤y ≤3}.
The family of all axis-parallel unit squares lying in S is intersecting because each of them

Section 13.3
COMBINATORIAL GEOMETRY
963
contains the point (1, 1). The family of axis-parallel unit squares in S′ can be pierced by
four points: (1, 1), (1, 2), (2, 1), (2, 2).
4. In the line, {1, 3, 4, 2} and {0, 4, 25, 3} have the same (1-dimensional) order type. The
3-dimensional closed unit balls centered at (0, 1, 5), (0, 0, 9.6), (0, 0, 9.4), (1, 0, 7) are met
by the z-axis consistently with the above order type, because these balls contain the
points (0, 0, 5), (0, 0, 9), (0, 0, 10), and (0, 0, 7), respectively, and the order type of this
sequence along the z-axis is the same as the 1-dimensional order type of {1, 3, 4, 2}.
13.3.2
INCIDENCES
This subsection studies the structure (and number) of incidences between a set of points
and a set of lines (or planes, spheres, etc.). The starting point of many investigations in
this ﬁeld was the Sylvester-Gallai theorem.
Deﬁnitions:
Given a point set P and a set L of lines (or k-ﬂats, spheres, etc.) in Euclidean d-space Rd,
a point p ∈P and a line l ∈L are incident with each other, if p ∈l.
Given a set L of lines in the plane, a point incident with precisely two elements of L
is called an ordinary crossing. Given a set of points P ⊆Rd, a hyperplane passing
through precisely d elements of P is called an ordinary hyperplane (for d = 2, an
ordinary line).
Given a set of points P, a Motzkin hyperplane is a hyperplane h such that all but
one element of h ∩P lie in a (d−2)-ﬂat.
A family Γ of curves in the plane has d degrees of freedom if there exists an integer s
such that
• no two curves in Γ have more than s points in common;
• for any d points, there are at most d curves in Γ passing through all the points.
A family of pseudolines is a family of simple curves in the plane with the property that
every two of them meet precisely once.
A family of pseudocircles is a family of simple closed curves in the plane with the
property that every two of them meet in at most two points.
Facts:
1. Sylvester-Gallai theorem:
Every ﬁnite set of points in the plane, not all of which
are on a line, determines an ordinary line. In dual version: every ﬁnite set of straight
lines in the plane, not all of which pass through the same point, determines an ordinary
crossing.
2. For every ﬁnite set of points in Euclidean d-space, not all of which lie on a hyperplane,
there exists a Motzkin hyperplane [Ha65], [Ha80].
3. Every set of n points in d-space, not all of which lie on a hyperplane, determine at
least n distinct hyperplanes.
4. In 3-space, every set of n noncoplanar points determines at least 2n
5 Motzkin hyper-
planes.
5. If n is suﬃciently large, then every set of n noncocircular points in the plane deter-
mines at least
 n−1
2

distinct circles, and this bound is best possible [El67].

964
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
6. Every set of n (>7) noncollinear points in the plane determines at least 6n
13 ordinary
lines. This bound is sharp for n = 13 and false for n = 7 [CsSa93].
7. If n is suﬃciently large, then the number of ordinary lines is at least n/2 if n is even,
and at least 3⌊n/4⌋if n is odd [GrTa13]. This proves an old conjecture of Dirac and
Motzkin.
8. There is a positive constant c such that every set of n points in the plane, not all
on a line, has an element incident with at least cn connecting lines. Moreover, any set
of n points in the plane, no more than n −k of which are on the same line, determines
at least c′kn distinct connecting lines, for a suitable constant c′ > 0. According to the
d = 2 special case of Fact 4, due to de Bruijn-Erd˝os, for k = 1 the number of distinct
connecting lines is at least n. For k = 2, the corresponding bound is 2n −4 (for n ≥10)
[Be83], [SzTr83].
9. Every set of n noncollinear points in the plane always determines at least 2⌊n
2 ⌋lines
of diﬀerent slopes. Furthermore, every set of n points in the plane, not all on a line,
permits a spanning tree, all of whose n −1 edges have diﬀerent slopes [Un82], [Ja87].
10. The number of incidences between a set P of points and a set L of lines can be
obtained by summing over all l ∈L the number of points in l belonging to P, or,
equivalently, by summing over all p ∈P the number of lines in L passing through p.
11. Let Γ be a family of curves in the plane with d degrees of freedom.
Then the
maximum number of incidences between n points in the plane and m elements of Γ is
O(nd/(2d−1)m(2d−2)/(2d−1) + n + m)
(see [PaSh98]). From the most important special case, when Γ is the family of all straight
lines (d = 2), it follows that for any set P of n points in the plane, the number of distinct
straight lines containing at least k elements of P is O( n2
k3 + n
k ) [SzTr83]. This bound is
asymptotically tight. The same result holds for pseudolines.
For d > 2, the above bound is probably not tight: slightly better results are known for
the number of incidences between n points and a family Γ of m algebraic curves with
bounded degree [ShShSo16].
12. The maximum number of incidences between n points and m spheres in R3 is
O(n
4
7 m
9
7 β(n, m) + n2),
where β(n, m) = o(log(nm)) is an extremely slowly growing function.
If no three spheres contain the same circle, then the following better bound is obtained:
O(n
3
4 m
3
4 + n + m).
Neither of these estimates is known to be asymptotically tight [ClEtal90], [Za13].
13. The maximum number of collinear triples determined by n points in the plane, no
four of which are on a line, is at most n2
6 −O(n) [BuGrSl79]. More precisely, if n is
suﬃciently large, the number of collinear triples is at most ⌊1
6n(n −3)⌋+ 1, and this
bound is tight [GrTa13].
14. If M(n) denotes the minimum number of diﬀerent midpoints of the
 n
2

line segments
determined by n points in convex position in the plane, then [ErFiF¨u91] showed
 n
2

−
 n(n+1)(1−e−1/2)
4

≤M(n) ≤
 n
2

−
 n2−2n+12
20

.

Section 13.3
COMBINATORIAL GEOMETRY
965
Examples:
1. Let P be a set of seven points in the plane, consisting of the vertices, the centroid
(the point of intersection of the medians), and the midpoints of all sides of an equilateral
triangle. Then P determines three ordinary lines (the lines connecting the midpoints of
two sides).
2. Let P be a 4k-element set in the plane that can be obtained from the vertex set
{v1, v2, . . . , v2k} of a regular 2k-gon by adding the intersection of the line at inﬁnity
with every line vivj.
Then the set P determines precisely 2k ordinary lines: every
line connecting some vi to the intersection point of vi−1vi+1 and the line at inﬁnity
(1 ≤i ≤2k, the indices are taken modulo 2k).
(It can be achieved by a suitable
projective transformation that no point of P is at inﬁnity, and the number of ordinary
lines remains |P |
2 = 2k.)
3. Let P be a set of n ≥4 points lying on two noncoplanar lines in 3-space so that there
are at least two points on each line. Not all points of P are coplanar, but P does not
determine any ordinary plane.
4. The family of all straight lines in the plane and the family of all unit circles both have
two degrees of freedom. The family of all circles with arbitrary radii has three degrees of
freedom. The family of the graphs of all polynomials of one variable and degree d has d
degrees of freedom.
5. Let P be an n
1
2 × n
1
2 part of the integer grid; i.e.,
P = {(i, j) | 1 ≤i ≤n
1
2 , 1 ≤j ≤n
1
2 }.
Let k =
  cm
n1/2
1/3 > 2, where c > 0 is a suﬃciently small constant. For every 1 ≤s <
r ≤k and for every 1 ≤i ≤r, 1 ≤j ≤n1/2
2 , consider the line passing through (i, j) and
(i+ r, j + s). If c is suﬃciently small, then the number of these lines is at most m. There
is a constant c′ > 0 such that the total number of incidences between these lines and the
elements of P is at least c′n
2
3 m
2
3 . (See the case d = 2 of Fact 11.)
13.3.3
DISTANCES
The systematic study of the distribution of the
 n
2

distances determined by n points was
initiated by Erd˝os. Given a set of n points P = {p1, p2, . . . , pn}, let g(P) denote the
number of distinct distances determined by P, and let f(P) denote the number of times
that the unit distance occurs between two elements of P. That is, f(P) is the number
of pairs pipj, i < j, such that |pi −pj| = 1. In [Er46], Erd˝os raised the following general
questions: What is the minimum of g(P) and what is the maximum of f(P) over all
n-element subsets of Euclidean d-space or of any other ﬁxed metric space?
Deﬁnitions:
For any point set P in a metric space, the unit distance graph of P is the graph G(P)
whose vertex set is P and two points (vertices) are connected by an edge if and only if
their distance is 1.
Let P be a ﬁnite set of points in a metric space. If the distance between two points
p, q ∈P is minimum, then p and q form a closest pair.
A point q ∈P is a nearest neighbor of p ∈P if no point of P is closer to p than q.

966
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
A set P in a metric space is a separated set if the minimum distance between the points
of P is at least 1.
The diameter of a ﬁnite set of points in a metric space is the maximum distance between
two points of the set.
A point q ∈P is a farthest neighbor of p ∈P if no point of P is farther from p than q.
A set of points in the plane is said to be in general position if no three are on a line
and no four on a circle.
Facts:
1. f(P) is equal to the number of edges of G(P).
2. If p and q form a closest pair in P, then q is a nearest neighbor of p and p is a nearest
neighbor of q.
3. If the distance between p and q is equal to the diameter of P, then q is a farthest
neighbor of p and p is a farthest neighbor of q.
4. The maximum number of times that the unit distance can occur among n points
in the plane is O(n4/3). Conjecture: the asymptotically best bound is O(n1+c/ log log n)
[SpSzTr84].
5. The maximum number of times that the unit distance can occur in a separated set
of n ≤3 points is ⌊3n −√12n −3⌋[Ha74].
6. The maximum number of times that the unit distance can occur in a set of n points
in the plane with unit diameter is n [HoPa34].
7. For any set of n > 3 points in the plane, the total number of farthest neighbors of
all elements is at most 3n −3 if n is even, and at most 3n −4 if n is odd. These bounds
cannot be improved [EdSk89].
8. The maximum number of times that the unit distance can occur among n points in
convex position in the plane is O(n log n). For n > 15, the best known lower bound is
2n −7 [F¨u90], [EdHa91].
9. In 2015 L. Guth and N. Katz showed that the minimum number of distinct distances
determined by n points in the plane is Ω(n/ log n) (see [GuKa15]). It was conjectured
by Erd˝os [Er46] that the best bound is Ω
 n
√log n

.
10. The minimum number of distinct distances determined by n > 3 points in convex
position in the plane is
 n
2

[Al63].
11. The minimum number of distinct distances determined by n > 3 points in the plane,
no three of which are on a line, is at least
 n−1
3

. Conjecture: the best possible bound
is
 n
2

.
12. The minimum number of distinct distances determined by n points in general po-
sition in the plane is O(n1+c/√log n), for some positive constant c. However, it is not
known whether this function is superlinear in n [ErEtal93].
13. There are arbitrarily large noncollinear ﬁnite point sets in the plane such that all
distances determined by them are integers, but there exists no inﬁnite set with this
property.
14. In an n-element planar point set, the maximum number of noncollinear triples that
determine the same angle is O(n2 log n), and this bound is asymptotically tight [PaSh90].
15. Let f3(n) denote the maximum number of times that the unit distance can occur
among n points in R3. Then
Ω(n
4
3 log log n) ≤f3(n) ≤n
3
2 ,

Section 13.3
COMBINATORIAL GEOMETRY
967
where the lower bound was proved in [Er46] and the upper bound is a slight improvement
of a result in [ClEtal90], obtained in [KaEtal12] and [Za13].
16. The maximum number of times that the unit distance can occur in a set of n ≥4
points in R3 with unit diameter is 2n −2 [Gr56].
17. If n is suﬃciently large, then for any set of n points in R3, the total number of
farthest neighbors of all elements is at most n2
4 + 3n
2 + 3 if n is even, at most n2
4 + 3n
2 + 9
4
if n ≡1 (mod 4), and at most n2
4 + 3n
2 + 13
4 if n ≡3 (mod 4). These bounds cannot be
improved [Cs96].
18. Let fd(n) denote the maximum number of times that the unit distance can occur
among n points in Rd. If d ≥4 is even, then
fd(n) = n2
2

1 −
1
⌊d
2 ⌋

+ n −O(d).
If d ≥5 is odd, then
fd(n) = n2
2

1 −
1
⌊d
2 ⌋

+ Θ(n
4
3 ) [Er60], [ErPa90].
19. Let Φd(n) denote the maximum of the total number of farthest neighbors of all
points over all n-element sets in Rd. For every d ≥4,
Φd(n) = n2
1 −
1
⌊d
2 ⌋+ o(1)

[ErPa90].
Examples:
1. Let P be the vertex set of a regular n-gon (n > 3) in the plane. Then g(P), the
number of distinct distances determined by P, is equal to ⌊n
2 ⌋. The number of times
that the diameter of P is realized is equal to n if n is odd, and n
2 if n is even.
2. Take a regular hexagon of side length k and partition it into 6k2 equilateral triangles
with unit sides. Let P denote the union of the vertex sets of these triangles. Then P is
a separated set, and |P| = n = 3k2 + 3k + 1. The number of times that the minimum
(unit) distance occurs between two elements of P is 9k2 + 3k = 3n −√12n −3.
3. Let P denote an n
1
2 ×n
1
2 part of the integer grid; i.e., let P = {(x, y) | 1 ≤x, y ≤n
1
2 }.
It follows from classical number-theoretic results that there exists an integer k ( n
16 ≤k ≤
n
8 ) that can be written as the sum of two squares in 2n
c
log log n diﬀerent ways, for a
constant c > 0. Thus, for every (x, y) ∈P, the number of points (x′, y′) ∈P satisfying
(x −x′)2 + (y −y′)2 = k is at least 2n
c
log log n . In other words, the distance k
1
2 occurs
n1+
c
log log n times among the elements. By proper scaling, an n-element point set P ′ is
obtained in which the unit distance occurs n1+
c
log log n times. That is, f(P ′) = n1+
c
log log n .
It can also be shown that the number of distinct distances determined by P ′ satisﬁes
g(P ′) = g(P) =
c′n
√log n for a suitable positive constant c′.
4. Lenz’ construction:
Let C1, . . . , C⌊d
2 ⌋be circles of radius
1
√
2 centered at the origin
of Rd, and assume that the supporting planes of these circles are mutually orthogonal.
Choose ni points on Ci, where ni =

n/⌊d
2⌋

or ni =

n/⌊d
2⌋

, so that P
i ni = n. It is
clear that any pair of points belonging to diﬀerent circles Ci are at unit distance from
each other. Hence, this point system determines at least
n2
2

1 −
1
⌊d
2 ⌋

+ n −O(d)
unit distances.

968
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
5. Let p1, p2, p3, p4 be the vertices of a regular tetrahedron with side length 1 in R3. The
locus of points in 3-space lying at unit distance from both p1 and p2 is a circle passing
through p3 and p4. Choose distinct points p5, p6, . . . , pn on the shorter arc of this circle
between p3 and p4. An n-element point set in R3 is obtained with diameter 1 and in
which the diameter occurs 2n −2 times.
13.3.4
COLORING
One of the oldest problems in graph theory is the Four Color Problem (§8.6.4). This
problem has attracted much interest among professional and amateur mathematicians,
and inspired a lot of research about colorings, including Ramsey theory [GrRoSp90] and
the study of chromatic numbers, polynomials, etc. In this section, some coloring problems
are discussed in a geometric setting.
Deﬁnitions:
A coloring of a set with k colors is a partition of the set into k parts. Two points that
belong to the same part are said to have the same color.
The chromatic number of a graph G is the minimum number of colors χ(G) needed
to color the vertices of G so that no two adjacent vertices have the same color.
The chromatic number of a metric space is the chromatic number of the unit
distance graph of the space; that is, the minimum number of colors needed to color all
points of the space so that no two points of the same color are at unit distance.
The polychromatic number of a metric space is the minimum number of colors χ
needed to color all points of the space so that for each color class Ci (1 ≤i ≤χ) there is
a distance di with the property that no two points of this color are at distance di from
each other.
A point set P in Rd is k-Ramsey if for any coloring of Rd with k colors, at least one of
the color classes has a subset congruent to P. If for every k, there exists d(k) such that
P is k-Ramsey in Rd(k), then P is called Ramsey.
A point set P ′ is called a homothetic copy (or a homothet) of P, if P and P ′ are
similar to each other and they are in parallel position.
Facts:
1. The minimum number of colors needed for coloring the plane so that no two points
at unit distance receive the same color is at least 4 and at most 7. That is, the chromatic
number of the plane is between 4 and 7 [JeTo95], [So09].
2. The following table contains the best known upper and lower bounds on the chromatic
numbers of various metric spaces. (Sd−1(r) denotes the sphere of radius r in d-space,
where the distance between two points is the length of the chord connecting them.) See
[So09].

Section 13.3
COMBINATORIAL GEOMETRY
969
space
lower bound
upper bound
line
2
2
plane
4
7
rational points of plane
2
2
3-space
6
15
rational points of R3
2
2
S2(r), 1
2 ≤r ≤
√
3−
√
3
2
3
4
S2(r),
√
3−
√
3
2
≤r ≤
1
√
3
3
5
S2(r), r ≥
1
√
3
4
7
S2  1
√
2

4
4
rational points of R4
4
4
rational points of R5
8
< ∞
Rd
(1 + o(1))(1.2)d
(3 + o(1))d
Sd−1(r), r ≥1
2
d
< ∞
3. Lower bound for the chromatic numbers of n-space:
The chromatic number of n-
space is at least n + 2 for n ≥2 (D. Raiskii) [So09].
4. Improved lower bounds for speciﬁc n:
The chromatic number of 3-space is at least
6 (O. Nechushtan), the chromatic number of 4-space is at least 7 (K. Cantwell), the
chromatic number of 5-space is at least 9 (K. Cantwell), and the chromatic number of
6-space is at least 11 (J. Cibulka). See [So09].
5. Lower bounds for chromatic numbers of Qn for small n: In 2008 J. Cibulka showed
that χ(Q5) ≥8 and χ(Q7) ≥15. M. Mann and D. Cherkashin, A. Kulikov, and A.
Raigorodskii [ChKuRa15] demonstrated that χ(Q6) ≥10, χ(Q8) ≥16, and χ(Q9) ≥22,
respectively.
6. The polychromatic number of the plane is at least 4 and at most 6 [So94].
7. For any ﬁnite d-dimensional point conﬁguration P and for any coloring of d-space
with ﬁnitely many colors, at least one of the color classes will contain a homothetic copy
of P. The corresponding statement is false if “homothetic copy of P” is replaced by
“translate of P”.
8. A necessary condition for a ﬁnite set P to be Ramsey is that it be spherical; i.e., all
its points lie on a sphere [GrRoSp90].
9. The following conditions are suﬃcient for a ﬁnite set P to be Ramsey:
• P is the vertex set of a right parallelepiped;
• P is the set of points in d-space with exactly k (k < d) nonzero coordinates having
values x1, . . . , xk in this order, where x1, . . . , xk
is an arbitrary sequence of
nonzero reals;
• P is the vertex set of a regular n-gon;
• P is a subset of a Ramsey set;
• P is the Cartesian product of two Ramsey sets [FrR¨o86], [FrR¨o90].
10. It follows from the ﬁrst two and the last two conditions of Fact 9 that all “triangles”
are Ramsey. Moreover, given any nondegenerate point conﬁguration (“simplex”) S, there
is a constant c(S) > 1 such that for every k < cd(S), S is k-Ramsey in d-space.

970
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
Examples:
1. Let G be a graph on the vertex set {v1, . . . , v7}, whose edges are v1v2, v1v3, v1v4,
v1v5, v2v3, v2v6, v3v6, v4v5, v4v7, v5v7, and v6v7. The chromatic number of G is 4.
2. The graph G of Example 1 can be embedded in the plane so that if two of its
vertices are connected by an edge, then the corresponding points in the plane are at
unit distance. In other words, G is a subgraph of the unit distance graph of the plane.
(In every such imbedding, the points corresponding to {v1, v2, v3, v6} and {v1, v4, v5, v7}
form two rhombi of side length 1 that share a vertex.) Hence, the chromatic number of
the plane is at least 4.
3. Let P be a 2-element point set in Euclidean space. For every positive integer k, P
is k-Ramsey in k-space. (To see this, consider a regular simplex in k-space, whose side
length is equal to the distance between the elements of P. Any coloring of Rk induces a
coloring of the vertices of this simplex and, by the pigeonhole principle, one can always
ﬁnd two vertices that get the same color. They form a 2-element set congruent to P.
Thus, P is Ramsey.)
13.4
POLYHEDRA
This section presents basic properties of polyhedra, commonly known as (planar) solids.
Any application such as geometric modeling that models the 3-dimensional world of
objects must deal with polyhedra. Basic geometric and combinatorial properties of poly-
hedra as well as their convex decompositions and triangulations are discussed.
13.4.1
GEOMETRIC PROPERTIES OF POLYHEDRA
Deﬁnitions:
A (d-1)-dimensional plane is the solution set of the linear equation a1x1 + a2x2 +
· · · + adxd = ad+1, where a1, a2, . . . , ad+1 are constants and x1, . . . , xd are d variables.
A hyperplane in d-dimensional Euclidean space Rd is the set of all points on a (d−1)-
dimensional plane.
A closed half-space in Rd is the set of all points on the hyperplane together with the
points on one side of the same hyperplane.
A convex d-polyhedron is the intersection of a ﬁnite number of closed half-spaces
in Rd.
A nonconvex polyhedron is the union of a set of convex polyhedra such that the
underlying space is connected and nonconvex.
A k-face, part of the boundary of a polyhedron, lies on at least d −k hyperplanes
forming the boundary. In particular, 0-faces, 1-faces, and (d−1)-faces of a d-polyhedron
are vertices, edges, and facets, respectively.
A polytope (d-polytope) is a convex d-polyhedron that is contained in the interior of
some d-dimensional cube: that is, a bounded convex d-polyhedron.

Section 13.4
POLYHEDRA
971
A d-polytope is regular if all its facets are regular (d−1)-polytopes that are combinato-
rially equivalent. A vertex is a regular 0-polytope.
Two polytopes P and Q are dual polytopes if there exists a one-to-one correspondence δ
between the set of faces of P and Q such that two faces f1, f2 ∈P satisfy f1 ⊂f2 if and
only if δ(f1) ⊃δ(f2) in Q.
A manifold (d-manifold) is a topological space that is locally homeomorphic to Rd
everywhere.
A manifold d-polyhedron is a polyhedron whose boundary is topologically the same
as a (d−1)-manifold. That is, every point on the boundary of a manifold d-polyhedron
has a small neighborhood that looks like Rd.
A non-manifold d-polyhedron is a d-polyhedron whose boundary is not a manifold.
A manifold 3-polyhedron has genus g if its boundary is a 2-manifold with genus g. A
2-manifold surface has genus g if every set of g + 1 circular cuts separate the surface, but
not all sets of g circular cuts do.
Edges in a 3-polyhedron are reﬂex edges if the inner angle subtended by two faces
meeting at that edge is greater than 180◦.
Facts:
1. Every polytope has a dual polytope.
2. Every polytope is the convex hull of its vertices.
3. A k-face is an open set of dimension k.
4. Curvature: The curvature κv of a manifold 3-polyhedron at a vertex v is
κv = 2π−P
i θi
2π
,
where θi is the angle between two consecutive edges incident with v. Intuitively, curvature
at a vertex measures its “sharpness”.
5. Gauss-Bonnet theorem: P
v
κv = 2 −2g.
6. Angle sums:
Let f be a face of a polytope P and let p be an interior point of f.
The angle at f is measured as the fraction of P covered by a suﬃciently small (d−1)-
dimensional sphere centered at p. If αk is the sum of angles at all k-dimensional faces,
then
d−1
P
k=0
(−1)kαk = (−1)d−1 (Gram’s formula, [Gr67]).
7. A 3-polyhedron is convex if and only if it does not have any reﬂex edges.
Examples:
1. Tetrahedra and cubes are manifold 3-polyhedra with genus 0. These are displayed
next.
2. Two cubes meeting at a single edge, or two tetrahedra meeting at a single vertex,
form non-manifold 3-polyhedra. See the following ﬁgure.

972
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
3. A cube has genus 0, but a cube with a cubical through-hole is a manifold 3-polyhedron
with genus 1. This is illustrated in the following ﬁgure.
4. A cube and a octahedron (bipyramid) are dual to each other; a tetrahedron is dual
to itself.
5. There are ﬁve regular polytopes in three dimensions: tetrahedron, cube (hexahedron),
octahedron, dodecahedron, icosahedron. They are also called Platonic solids. See the
following ﬁgure.
6. There is a circular cut for a toroidal surface that does not separate it, though any
two circular cuts always separate it.
13.4.2
TRIANGULATIONS
A complex domain is decomposed into simple parts for computational simplicity in many
applications. For example, in ﬁnite element methods often a domain is triangulated into
simplices.
Deﬁnitions:
A simplex (d-dimensional simplex or d-simplex) is a d-polytope with d+1 vertices.
A triangulation of a d-polyhedron is a convex decomposition in which each convex piece
of the decomposition is a d-simplex.
A polyhedron is triangulated with Steiner points if the vertex set of simplices in the
triangulation is strictly a superset of the set of vertices of the polyhedron. This type of
triangulation uses extra points (other than the vertices of the polyhedron) as vertices.
A triangulation of a polyhedron is a simplicial complex if for every two simplices σ1, σ2
in the triangulation, σ1 ∩σ2 is either empty or a face of both simplices.
A convex decomposition of a polyhedron is its partition into convex pieces that have
disjoint interiors.
The aspect ratio of a simplex is the ratio of the radius of the circumscribing sphere
to the radius of the inscribing sphere of the simplex.
The aspect ratio of a triangulation is the largest aspect ratio of a simplex in the
triangulation.

Section 13.4
POLYHEDRA
973
Facts:
1. Every d-polytope can be triangulated without Steiner points.
2. Every d-polytope with n faces can be triangulated into O(n) simplices in O(n) time
and space.
3. There are nonconvex 3-polyhedra that can’t be triangulated without Steiner points.
4. The problem of deciding if a nonconvex 3-polyhedron can be triangulated without
Steiner points or not is NP-complete [RuSe92].
5. The problem of decomposing a polyhedron into the minimum number of convex pieces
is NP-hard.
6. Every polyhedron can be decomposed into disjoint convex pieces by repeatedly slicing
the polyhedron through reﬂex edges [BaDe92], [Ch84].
7. There is a class of polyhedra with n edges, of which r are reﬂex, that requires at
least Ω(n + r2) convex pieces for its decomposition.
These polyhedra have two sets
of parallel edges which are created as reﬂex edges. Two such sets are placed on two
hyperbolic paraboloids with an angle of almost 90◦between them.
These polyhedra
require at least Ω(n + r2) convex pieces for their decomposition [Ch84].
8. Every manifold 3-polyhedron can be triangulated into O(n+r2) tetrahedra in O((n+
r2) log r) time [ChPa90].
9. There exists a polynomial-time algorithm that produces a triangulation of any 3-
polyhedron with an aspect ratio and size that are within a constant factor of the optimal
[MiVa92].
Examples:
1. A triangle is a 2-simplex; a tetrahedron is a 3-simplex.
2. Part (a) of the following ﬁgure shows a tetrahedron with a bad aspect ratio; part (b)
shows a tetrahedron with a good aspect ratio.
(a)
(b)
3. The Sch¨onhardt polyhedron is a nonconvex 3-polyhedron that cannot be triangulated
without Steiner points. This polyhedron can be constructed out of a prism whose base
and top facets are equilateral triangles. Twist the top triangle, keeping the base ﬁxed.
This destroys the planarity of vertical facets. To maintain the planarity, triangulate these
facets appropriately [RuSe92].
13.4.3
FACE NUMBERS
In many cases the complexity of algorithms dealing with polyhedra depends on the
number of faces they contain. Therefore, combinatorial bounds on these numbers play a
signiﬁcant role in analyzing these algorithms.

974
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
Deﬁnitions:
A cyclic d-polytope is the convex hull of a set of n (n ≥d + 1) points on the moment
curve in Rd, x(t) = (t, t2, . . . , td).
A face vector of a d-polyhedron P is the d-dimensional vector (f0, f1, . . . , fd−1), where
fi = fi(P) is the number of i-dimensional faces of P.
A simplicial polytope is a polytope in which all faces are simplices.
Facts:
1. For 2k ≤d, every k vertices of a cyclic polytope deﬁne a (k−1)-face.
2. Euler’s relation: For any d-polytope,
d−1
P
i=0
(−1)ifi = 1 −(−1)d.
3. For a manifold 3-polyhedron with genus g,
2P
i=0
(−1)ifi = 2 −2g.
4. The edges on the boundary of a manifold 3-polyhedron with genus 0 form a planar
graph. By the property of planarity, the number of vertices, edges, and facets of such
polyhedra are within a constant factor of each other.
5. Dehn-Sommerville equations:
The face vectors of simplicial polytopes satisfy the
following equations for −1 ≤k ≤d −2 with f−1 = 1:
Ek
d :
d−1
P
j=k
(−1)j j+1
k+1

fj = (−1)d−1fk.
In particular, E−1
d
is Euler’s relation.
6. Upper bound theorem:
For any d-polytope P with n vertices, fi(P) = O(n⌊d
2 ⌋) for
1 ≤i ≤d −1.
7. Optimality of cyclic polytopes:
Cyclic polytopes achieve the upper bound since
they have
 n
k

= Ω(nk) (k−1)-faces for 2k ≤d. This implies that they have Ω(n⌊d/2⌋)
 d
2

-faces.
Examples:
1. The 3-dimensional cube (d = 3) has f0 = 8, f1 = 12, and f2 = 6; thus, by Fact 2,
f0 −f1 + f2 = 2.
2. A 3-dimensional cube with a cubical through-hole (g = 1) has f0 = 16, f1 = 24, and
f2 = 10; thus, by Fact 3, f0 −f1 + f2 = 0.
13.5
ALGORITHMS AND COMPLEXITY IN COMPUTATIONAL
GEOMETRY
Computational geometry studies eﬃcient algorithms for solving geometric problems and
has applications in computer graphics, robotics, VLSI design, computer-aided design,
pattern recognition, statistics, and other ﬁelds. The study of computational geometry
uses concepts and results from classical geometry, topology, combinatorics, as well as
standard techniques from design and analysis of computer algorithms. See [PrSh85] and
[T´oORGo17].

Section 13.5
ALGORITHMS AND COMPLEXITY
975
13.5.1
CONVEX HULLS
Finding eﬃcient algorithms for the construction of convex hulls has been a central topic
in computational geometry. Several eﬃcient algorithms for constructing boundaries of
convex hulls of sets of points in the plane have been developed.
Deﬁnition:
The convex hull of a set of points in Rd is the smallest convex set containing the points.
Algorithms:
1. Finding boundaries of convex hulls by rotational sweeping:
• Graham Scan:
Given a set S of n points in the plane, Algorithm 1 scans the
points rotationally around a ﬁxed point and eliminates those that are not hull
vertices. The remaining points are the vertices of the boundary of the convex
hull of S. The running time of Graham Scan is O(n log n), which is dominated
by the sorting of the points. The remaining steps take only linear time.
Algorithm 1:
Graham Scan.
input: a ﬁnite set S of points in the plane
output: the vertices of the boundary of the convex hull of S
p0 := the point in S with the minimum y-coordinate
sort remaining points by polar angle around p0; append the point p0 to the
end of the sorted list; let the resulting list be (p1, p2, . . . , pn), where pn = p0.
H[1] := pn; H[2] := p1; j := 2
for i := 2 to n
while the path {H[j −1], H[j], pi} does not form a left turn
j := j −1
j := j + 1
H[j] := pi
{H[1], H[2], . . ., H[j] is the boundary of the convex hull}
• Jarvis’ March:
Given a set S of n points in the plane, Jarvis’ March algorithm
constructs the boundary of the convex hull by “marching around” the outer
perimeter of S. This method is also called “gift-wrapping”. Jarvis’ March runs
in time O(hn), where h is the number of vertices of the convex hull, which in
the worst case is n.
2. Divide-and-conquer algorithms:
• QuickHull: This algorithm recursively constructs a chain on the boundary of the
convex hull, connecting two hull vertices u and v. It ﬁrst ﬁnds a hull vertex w
on the chain (for example, w is the farthest point from the line uv). Then the
subchains connecting u and w, w and v, respectively, are constructed recursively
and are concatenated [PrSh85]. QuickHull runs practically fast, but in the worst
case the running time of QuickHull is O(n2).

976
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
• MergeHull:
This algorithm ﬁrst partitions the set S of points into two subsets
S1 and S2 of equal size and then recursively constructs the boundaries of the
convex hulls CH(S1) and CH(S2). Finally, CH(S1) and CH(S2) are “merged”
into the convex hull of the set S [PrSh85].
The boundary of the convex hull for S is the same as the boundary of the
convex hull for the hull vertices of CH(S1) and CH(S2). Thus, to construct
the boundary of CH(S), ﬁrst sort the hull vertices of CH(S1) and CH(S2) (this
sorting can be done in linear time), then apply the linear scan of Graham Scan
to construct CH(S). Therefore, the boundary of the convex hull CH(S) can
be constructed from CH(S1) and CH(S2) in linear time. The running time of
MergeHull is O(n log n).
3. Other methods:
• Incremental method:
The incremental method for constructing the boundary
of the convex hull of a set of points in the plane adds one point at a time
to an already constructed boundary of a convex hull. This method has time
complexity O(n log n) [PrSh85]. An advantage of this method is that it can be
generalized to construct boundaries of convex hulls in higher dimensions [Ed87].
• Algorithm by Kirkpatrick and Siedel based on the prune-and-search method: This
algorithm partitions a given set of points in the plane into two linearly separable
subsets of equal size, ﬁnds the two edges of the boundary of the convex hull
that “bridge” these two subsets, and recursively constructs the subchains on
the boundary of the convex hull between these two bridges. This method has
time complexity O(n log h), where h is the number of vertices on the boundary
of the convex hull [Ya90].
Facts:
1. The problem of ﬁnding convex hulls is at least as hard as sorting.
The lower
bound Ω(n log n) of sorting on comparison decision trees also applies to the convex hull
problem. This lower bound Ω(n log n) can be extended to a more general computation
model, the bounded-degree algebraic decision trees.
2. An O(n log n) time algorithm for constructing the boundary of the convex hull of
a set of points in R3 has been developed, which is a generalization of the MergeHull
algorithm.
3. For dimension d > 3, the convex hull of n points in Rd can have up to O(n⌊d
2 ⌋)
faces. An algorithm based on the incremental method has been proposed to construct
the convex hull for a set of n points in Rd in time O(n⌊d+1
2
⌋) [Ya90]. An optimal algorithm
of time O(n⌊d
2 ⌋) has been developed recently by Chazelle. (See the bibliography of [Mu94]
for a reference.)
Examples:
1. The following ﬁgure shows a set S of 7 points in the plane and the convex hull of S.
A
B
E
G
F
D
C
C
A
B
G
F

Section 13.5
ALGORITHMS AND COMPLEXITY
977
2. The convex hull of the set {(0, 0, 0), (0, 0, 1), (0, 1, 0), (0, 1, 1), (1, 0, 0), (1, 0, 1),
(1, 1, 0), (1, 1, 1)} in R3 is the surface of the unit cube together with its interior.
13.5.2
TRIANGULATION ALGORITHMS
Triangulation plays an important role in many applications. On a triangulated planar
straight-line graph, many problems can be solved more eﬃciently. Triangulation of a
set of points arises in numerical interpolation of bivariate data and in the ﬁnite element
method.
Deﬁnitions:
A planar straight-line graph (PSLG) is a planar graph such that each edge is a
straight line.
A triangulation of a simple polygon P is an augmentation of P with nonintersecting
diagonal edges connecting vertices of P such that in the resulting PSLG, every bounded
face is a triangle.
A triangulation of a PSLG G is an augmentation of G with nonintersecting edges
connecting vertices of G so that every point in the interior of the convex hull of G is
contained in a face that is a triangle. In particular, the PSLG to be triangulated can be
simply n discrete points in the plane.
A chain is a PSLG with vertices v1, . . . , vn and edges {v1, v2}, {v2, v3}, . . . , {vn−1, vn}.
A chain is monotone if there is a straight line L such that every line perpendicular to L
intersects the chain in at most one point.
A simple polygon is monotone if its boundary can be decomposed into two monotone
chains.
Two vertices v and u in a polygon are visible from each other if the open line segment uv
is entirely in the interior of the polygon.
Facts:
1. Every simple polygon can be triangulated.
2. Every triangulation of a simple polygon with n vertices has n −2 triangles and n −3
diagonals.
3. Given a simple polygon with n vertices, there is a diagonal that divides the polygon
into two polygons that have at most
 2n
3

+ 1 vertices.
4. For a history of triangulations, see [OR87].
5. Simple polygons can be triangulated in O(n) time using an algorithm developed by
Chazelle [Ch91].
6. PSLGs can be triangulated in O(n log n) time. (See the triangulation of a general
PSLG algorithm — item 3 in the following list of algorithms.) This is optimal because
a lower bound Ω(n log n) has been derived for the time complexity of triangulation of
a PSLG.
Algorithms:
1. Triangulation of a monotone polygon: A monotone polygon P can be triangulated in
linear time based on the following greedy method. Observe that the monotone polygon P

978
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
is triangulated if nonintersecting edges are added so that no two vertices are visible from
each other.
If necessary, rotate the polygon so that it is monotone with respect to the y-axis. Sort
the vertices of P by decreasing y-coordinate. (This sorting can be done in linear time by
merging the two monotone chains of P.) Move through the sorted list, and for a vertex
v, examine each vertex u lower than v, in the sorted order, and add an edge between
vertices v and u as long as u is visible from v. The edge addition process for the vertex
v stops at a lower vertex that is not visible from v. Then move to the next vertex and
perform the edge addition process. Note that once an edge is added between vertices v
and u, then no vertices between v and u in the sorted list are visible from a vertex that is
lower than u. Therefore, such vertices can be ignored in the later edge addition process.
The edge addition process for all vertices can be performed in linear time if a stack is
used to hold the sorted list.
2. Triangulation of a simple polygon: Given a general simple polygon P, partition P in
time O(n log n) into monotone polygons, then apply the previous linear-time algorithm
to triangulate each monotone polygon. This gives a triangulation of a simple polygon in
time O(n log n).
3. Triangulation of a general PSLG: To triangulate a general PSLG G, ﬁrst add edges
to G so that each face is a simple polygon (no nonconsecutive edges intersect), then apply
Chazelle’s linear-time algorithm (Fact 5) to triangulate each face.
The complexity of Chazelle’s algorithm can be avoided since there is an eﬃcient algorithm
that adds edges to a PSLG so that each face is a monotone polygon. To do this, observe
that in a PSLG G every face is a monotone polygon if and only if each vertex (except the
highest one) has a higher neighbor and each vertex (except the lowest one) has a lower
neighbor. Thus, to make each face of G a monotone polygon, check each vertex of G and
for those that do not have desired neighbors, add proper edges to them. This process
can be accomplished in time O(n log n) using the plane sweeping method [PrSh85]. Now,
the simpler linear-time algorithm for triangulating a monotone polygon (see item 1) is
applied to triangulate each face.
Examples:
1. The following ﬁgure illustrates a simple polygon and two of its triangulations.
2. In part (a) of the following ﬁgure the chain is monotone (with respect to any horizontal
line); the chain in part (b) is not monotone.
(a)
(b)

Section 13.5
ALGORITHMS AND COMPLEXITY
979
13.5.3
VORONOI DIAGRAMS AND DELAUNAY TRIANGULATIONS
Deﬁnitions:
Given a set S = {p1, . . . , pn} in Rd, the Voronoi diagram Vor(S) of S is a partition of
Rd into n convex polytopes (Voronoi cells or Dirichlet cells) V (p1), . . . , V (pn) such
that the region V (pi) is the locus of points that are closer to pi than to any other point
in S.
Given the Voronoi diagram Vor(S) of a set S = {p1, . . . , pn} of points in the plane, the
straight-line dual D(S) of Vor(S) is a PSLG whose vertices are the points in S and in
which two vertices pi and pj in D(S) are connected if and only if the regions V (pi) and
V (pj) share a common edge.
The PSLG D(S) is a triangulation of the set S, called the Delaunay triangulation of
S.
Facts:
1. The Voronoi diagram of a set of n points in the plane can be constructed in time
O(n log n).
2. The Delaunay triangulation of a set S of points in the plane has the property that the
circuit with the three vertices of a triangle of the triangulation on its boundary contains
no other point of the set S. This property makes the Delaunay triangulation useful in
interpolation applications.
3. The convex hull problem in the plane can be reduced in linear time to the Voronoi
diagram problem in the plane: a point p in a set S is a hull vertex if and only if V (p) is
unbounded, and two hull vertices pi and pj are adjacent if and only if the two unbounded
regions V (pi) and V (pj) share a common edge. Thus, the O(n log n) time algorithm for
constructing Voronoi diagrams (Algorithm 2) is optimal.
4. The Voronoi diagram problem for n points in Rd can be reduced in linear time to the
convex hull problem for n points in Rd+1 [Ed87]. Thus, the Voronoi diagram of a set of
n points in Rd can be constructed in time O(n⌊(d+1)/2⌋) based on the optimal algorithm
for constructing the convex hull of a set of n points in Rd+1.
Algorithms:
1. Construction of Voronoi diagrams in the plane:
The Voronoi diagram of a set of
points in R2 can be constructed using the divide-and-conquer method of Algorithm 2.
To eﬃciently partition a set S into a left subset and a right subset of equal size in
each recursive construction, pre-sort the set S by x-coordinate. To merge the Voronoi
diagrams Vor(SL) and Vor(SR) into the Voronoi diagram Vor(S), add the part of Vor(S)
that is missing in Vor(SL) and Vor(SR) and then delete the part of Vor(SL) and Vor(SR)
that does not appear in Vor(S).
Algorithm 2:
Construction of Voronoi diagrams.
input: a set S of points in the plane
output: the Voronoi diagram of S
if |S| < 4 then construct Vor(S) directly and stop
else

980
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
partition S into two equal size subsets SL (left subset) and SR (right subset)
separated by a vertical line
construct Vor(SL) and Vor(SR) recursively;
merge Vor(SL) and Vor(SR) into Vor(S);
2. Voronoi diagrams and geometric optimization problems: An O(n log n) time optimal
algorithm can be derived via the Voronoi diagram for the problem of ﬁnding for each
point in a set S of n points in the plane the nearest point in S. This is so because each
point p in S and its nearest neighbor correspond to two regions in Vor(S) that share a
common edge. This also implies an O(n log n) time optimal algorithm for the problem
of ﬁnding the closest pair in a set of n points in the plane.
The Voronoi diagram can be used to design an O(n log n) time optimal algorithm for
constructing a Euclidean minimum spanning tree for a set S of n points in the plane be-
cause edges of any Euclidean minimum spanning tree must be contained in the Delaunay
triangulation D(S) of S. This algorithm implies an O(n log n) time approximation algo-
rithm for the Euclidean traveling salesman problem which produces a traveling salesman
tour of length at most twice the optimum.
Example:
1. The left half of the following ﬁgure illustrates a set of six points in the plane and the
Voronoi diagram for the set. The right half shows the Delaunay triangulation of the set.
13.5.4
ARRANGEMENTS
Deﬁnition:
Given n lines in the plane, the arrangement of the lines in the plane is the PSLG whose
vertices are the intersection points of the lines and whose edges connect consecutive
intersection points on each line (it is assumed that all lines intersect at a common point
at inﬁnity).
Facts:
1. The arrangement of n lines in the plane partitions the plane into a collection of O(n2)
faces, edges, and vertices.
2. The arrangement of n lines can be constructed in O(n2) time (Algorithm 3), which
is optimal.
3. An arrangement can be represented by a doubly-connected-edge-list in which the
edges incident with a vertex can be traversed in clockwise order in constant time per
edge [PrSh85].

Section 13.5
ALGORITHMS AND COMPLEXITY
981
4. Applications of arrangements include ﬁnding the smallest-area triangle among n
points, constructing Voronoi diagrams, and half-plane range query.
5. The arrangement of n hyperplanes in Rd can be deﬁned similarly, which partitions Rd
into O(nd) faces of dimension at most d.
6. Algorithm 3 can be generalized to construct the arrangement of n hyperplanes in Rd
in O(nd) time, which is optimal [Ed87].
Algorithm:
1. Constructing the arrangement of a set of lines: Algorithm 3 constructs the arrange-
ment A of a set H of n lines L1, . . . , Ln in the plane by the incremental method.
To traverse the faces of A that intersect the line Li, start from a face F that has the point
pi on its boundary, and traverse the boundary of F until an edge e is encountered such
that e intersects Li at a point q. A new vertex q is introduced in A and the adjacencies
of the two ends of the edge e are updated. Then reverse the traversing direction on the
edge e and start traversing the face that shares the edge e with F, and so on. The total
number of edges traversed in this process in order to insert the line Li is bounded by
O(i) [Mu94].
Algorithm 3:
Incremental method for constructing the arrangement of a
set H of n lines.
input: a set H of n lines L1, L2, . . . , Ln in the plane
output: the arrangement A of the set H
A := L1;
for i := 2 to n
ﬁnd the intersection point pi of Li and L1
starting from pi, traverse the faces of A that intersect Li and update the vertex
set and edge set of A
Example:
1. The following ﬁgure shows an arrangement of four lines in the plane. The graph has
seven vertices (including the vertex at inﬁnity), 16 edges (of which eight are unbounded),
and 11 regions (of which eight are unbounded).
13.5.5
VISIBILITY
Visibility problems are concerned with determining what can be viewed from a given
point (or points) in the plane or 3-dimensional space. See [OR87], [OR93], [Sh92].

982
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
Deﬁnitions:
The visibility problem is the problem of ﬁnding what is visible, given a conﬁguration
of objects and a viewpoint.
Given n nonintersecting line segments in the plane, the visibility graph is the graph
whose vertices are the endpoints of the line segments, with two vertices adjacent if and
only if they are visible from each other; i.e., there is an edge joining a and b if and only
if the open line segment ab does not intersect any other line segments.
A star polygon is a polygon with an interior point p such that each point on the polygon
is visible from p.
Facts:
1. Visibility problems have important applications in computer graphics and robotics
and have served as motivation for research in computational geometry.
2. Constructing the visibility graph for a set of n nonintersecting line segments is a crit-
ical component of the shortest path problem in the plane. The visibility graph problem
can be solved in optimal time O(n2) [Ya90].
3. Art gallery theorems: Given a simple polygon with n vertices,
• there is a set S of
 n
3

vertices of the polygon such that each point on or inside
the polygon is visible from a point in S;
• there is a set S of
 n
3

points on the polygon such that each point on or outside
the polygon is visible from a point in S;
• there is a set S of
 n
2

vertices of the polygon such that each point on, inside, or
outside the polygon is visible from a point in S.
In each case the number given is the best possible.
Algorithms:
1. Given n line segments in the plane, compute the sequence of subsegments that are
visible from the point y = −∞, that is, by using parallel rays. The problem can be solved
by a modiﬁed version of the plane sweeping algorithm for computing all intersection
points of the line segments. The algorithm has worst-case time complexity O(n2 log n).
2. An alternative algorithm is based on the divide-and-conquer approach: arbitrarily
partition the set of the n line segments into two equal size halves, solve both subproblems,
and merge the results.
Note that the merging step amounts to computing the minimum of two piecewise (not
necessarily continuous) linear functions, which can be easily done in time linear in the
number of pieces if it is recursively assumed that the pieces are sorted by x-coordinate.
For a set of n arbitrary line segments in the plane, in the worst case, the number of
subsegments visible from the point y = −∞is bounded by O(nα(n)) [Ya90], where α(n),
the inverse of Ackermann’s function (§1.3.2), is a monotonically increasing function that
grows so slowly that for practical purposes it can be treated as a constant. Therefore, the
merging step runs in time O(nα(n)). Consequently, the time complexity of the algorithm
based on the divide-and-conquer method is O(nα(n) log n).
3. Three-dimensional visibility:
Given a set of disjoint opaque polyhedra in R3, ﬁnd
the part of the set that is visible from the viewpoint z = −∞(that is, with parallel rays).
The problem can be solved in time O(n2 log n) by a modiﬁed plane sweeping algorithm
for computing the intersection points of the line segments that are the projections of the
edges of the polyhedra on the xy-plane. Optimal algorithms of time complexity O(n2)
have been developed based on line arrangements [Do94], [Ya90].

Section 13.6
GEOMETRIC DATA STRUCTURES AND SEARCHING
983
Examples:
1. The following ﬁgure shows three line segments and the visibility graph with six ver-
tices determined by the line segments. The edges of the visibility graph are shown as
dotted lines.
2. Surveillance problems:
A variety of problems require that “guards” be posted at
points of a polygon so that corners and/or edges are visible. (See the art gallery theorems
of Fact 3.)
3. Hidden surface removal: An important problem in computer graphics is the problem
of ﬁnding (and removing) the portions of shapes in 3-dimensional space that are hidden
from view, when the object is viewed from a given point. (See the three-dimensional
visibility algorithm.)
13.6
GEOMETRIC DATA STRUCTURES AND SEARCHING
This section describes the use of data structures for searching, or querying, among a
set S of geometric objects. For each of the following problems, there are algorithms
that perform a single search in time proportional to n, the total complexity of all the
geometric objects in S. These single-search algorithms use minimal data structures and
minimal preprocessing time. When the application searches multiple times among the
elements of the same set S, it becomes more eﬃcient to preprocess the objects in S into
a data structure that would allow a faster searching procedure.
This section presents four fundamental searching problems in computational geometry
for which clever data structures reduce the search time to O(logk
2 n), where k is a small
constant (often equal to 1 for problems in the plane and 3-dimensional space). This
section covers only the static versions of these problems; that is, S never changes. The
dynamic versions allow deletions from S and/or insertions into S in between queries. In
the dynamic versions of the problem, in addition to polylogarithmic query time, the goal
is to keep the update time polylogarithmic. The dynamic versions are, as a rule, much
more diﬃcult.
13.6.1
POINT LOCATION
Deﬁnition:
Let p be a point and S a subdivision of Rd. (S can be a single geometric object, such
as a polytope, or can be a general subdivision of Rd.) The point location problem is
the problem of determining which region of S contains p.

984
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
Examples:
1. Locate a point in the subdivision of the space induced by an arrangement of a set of
hyperplanes. (See §13.5.4.)
2. Locate a point in a subdivision all of whose regions are convex (a convex subdivision).
For example, an arrangement of hyperplanes is a convex subdivision.
3. Search for nearest neighbors in Voronoi diagrams. (See §13.5.3.)
4. Range searching. (See §13.6.2.)
5. Ray shooting. (See §13.6.3.)
Facts:
1. Point location in straight-line subdivision using triangulation hierarchy:
Given an
n-vertex triangulation R′ = (V ′, T ′), where V ′ is the set of vertices and T ′ is the set
of triangles, an (n+3)-vertex enclosed triangulation R = (V, T ) is the triangulation R′
together with the triangulation of the region between U and the convex hull of R′, where
U = (VU, TU) is a triangle that contains R′ in its interior. The triangulation-hierarchy of
[Ki83] consists of a sequence of triangulations R = ⟨R1, R2, . . . , Rc log2 n⟩, where R1 = R,
Rc log2 n = U, and Ri is created from Ri−1 as follows (see Example 1):
• remove from Vi−1 −VU a set X of independent (that is, nonadjacent) vertices
and remove from Ti−1 the set Z of all triangles incident with any vertex in X:
Vi = Vi−1 −X, Ti = Ti−1 −Z;
• retriangulate any polygons in Ri = (Vi, Ti).
2. Algorithm 1 produces a hierarchy for planar subdivision. With minor modiﬁcations
(for example, “triangles” become tetrahedrons), it can be used for subdivisions in R3.
It can be proven that |Tc log2 n| = 1 for some constant c, and that |τ(t)| is a constant for
every t.
3. Algorithm 1 runs in O(n) time and produces a triangulation hierarchy that takes O(n)
space.
Algorithm 1:
Computing the triangulation hierarchy.
input: planar straight-line subdivision S
output: triangulation hierarchy of S
compute triangulation R′ of S; compute enclosed triangulation R of R′; choose a
small constant k; R1 := R; i := 1
while |Ti| > 1
i := i + 1; Ri := Ri−1; mark all vertices in Vi having degree < k
while there exists some marked vertex v
P := (VP , EP ) (polygon consisting of vertices adjacent to v; that is,
VP := {vj | (v, vj, vk) ∈Ti}; EP := {(vj, vk) | (v, vj, vk) ∈Ti})
remove v from Vi; remove all the triangles incident with v from Ti; that is,
Trem := {(v, vj, vk) | vj ∈VP }; Ti := Ti −Trem
compute the triangulation RP of P
for each triangle t in RP
τ(t) := the set of triangles in Trem that overlap with t
create a pointer from t to every triangle in τ(t)
unmark v and any marked vertices in VP

Section 13.6
GEOMETRIC DATA STRUCTURES AND SEARCHING
985
4. Algorithm 2 carries out point location, given a triangulation hierarchy, and takes
O(log2 n) time.
Algorithm 2:
Performing point location.
input: a point q and a triangulation hierarchy R
output: triangle that contains q
check if Rc log2 n contains q
i := c log2 n −1; t := U
while i ≥1
determine the triangle t′ in τ(t) that contain q using pointers from t to τ(t)
t := t′; i := i −1
5. The following table shows the complexity of various point location algorithms. The
number m denotes the number of regions (or cells) in the subdivision S; n denotes the
total combinatorial complexity of S.
query
preprocessing
dimension
subdivision type
time
space
time
2
convex subdivision
O(log2 n)
O(n)
O(n)
3
simple polytope
O(log2 n)
O(n)
O(n)
3
convex subdivision
O(log2
2 n)
O(n log2
2 n)
O(n log2
2 n)
d
arrangement of n
O(log2 n)
O(nd)
O(nd)
hyperplanes
subdivision of m
d
(d−1)-simplices
O(log3
2 m)
O(md−1+ǫ + n)
O(md−1+ǫ
with a total of n
+ n log2 m)
faces, ǫ > 0
Example:
1. Part (a) of the ﬁgure seen on the next page shows triangulation R1 of Fact 1 (the
vertices that are removed from R1 are circled). Part (b) shows triangulation R2 (dotted
lines are edges in retriangulation). Part (c) gives a list of the pointers from triangles
in R2 to triangles in R1.
13.6.2
RANGE SEARCHING
Deﬁnitions:
The range counting problem is the problem of counting the number of points in a
given set S ⊆Rd that lie in a given query range q.
The range reporting problem is the problem of determining all points in a given
set S ⊆Rd that lie in a given query range q.
The range emptiness problem is the problem of determining if a given query range q
contains any points from a given set S ⊆Rd.

986
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
(a)
t5
t6
t13
t12
t11
t10
t8
t9
t1
t7
t4
t3
t2
(b)
t4
t3
t2
t16
t14
t15
t1
t18
t17
t14
t15
t16
t17
t18
t8, t10
t8, t9, t10, t11
t5, t12
t5, t6, t7, t12, t13
t7, t13
(c)
Facts:
1. The following table gives information on various range searching algorithms. The
integer n is the number of points in S; ǫ is an arbitrarily small positive constant. When
the query is reporting, the query time has an additive factor of k, which is the size of
the output.
preprocessing
dim
range type
query time
space
time
2
orthogonal
O(log2 n)
O(n log2+ǫ
2
n)
O(n log2 n)
2
convex polygon
O(√n log2 n)
O(n)
O(n1+ǫ)
2
convex polytope
O(n2/3 log2
2 n)
O(n log2 n)
O(n1+ǫ)
d
convex polytope
O(logd+1
2
n)
O(nd)
O(nd(log2 n)ǫ)
for n ≤m ≤nd
O(n1−1/d)
O(n)
O(n1+ǫ)
O((n/m1/d) logd+1
2
n)
O(m1+ǫ)
O(m1+ǫ)
d
half-space
O(log2 n)
O(nd/ logd
2 n)
O(nd/ logd−ǫ
2
n)
for n ≤m ≤nd
O(n/m1/d)
O(m)
O(n1+ǫ+m(log2 n)ǫ)
2. The following table gives information on various range reporting algorithms.

Section 13.6
GEOMETRIC DATA STRUCTURES AND SEARCHING
987
preprocessing
dim
range type
query time
space
time
2
half-plane ﬁxed-
O(log2 n + k)
O(n)
O(n log2 n)
radius circle
2
orthogonal
O(log2 n + k)
O(n logǫ
2 n)
O(n log2 n)
3
half-space
O(log2 n + k)
O(n log2 n)
O(n log3
2 n log2 log2 n)
d
half-space
O(log2 n + k)
O(n⌊d
2 ⌋+ǫ)
O(n⌊d
2 ⌋+ǫ)
n ≤m ≤n⌊d
2 ⌋
O(n1−1/⌊d
2 ⌋+ǫ+k)
O(n)
O(n)
O(
n
m1/d log2 n+k)
O(m1+ǫ)
O(m1+ǫ)
d
orthogonal
O(logd−1
2
n + k)
O( n logd−1
2
n
log2 log2 n)
O(n logd−1
2
n)
O(dn1−1
d + k)
O(dn)
O(dn log2 n)
3. Orthogonal range searching in R2 can be carried out using range trees.
4. The range tree is deﬁned recursively by Algorithm 3.
Each node stores a sub-
set of points organized into a threaded binary search tree by the y-coordinates of the
points. The left child contains half the parent’s points, in particular those with lesser
x-coordinates; the right child contains the other half of the parents’ points with greater
x-coordinates.
Each node also stores the range of x-coordinates of its points. For simplicity, all coor-
dinates of all points are assumed to be distinct. It is also assumed that all points of
S = {(x1, y1), (x2, y2), . . . , (xn, yn)} have been presorted by their x-coordinate so that
x1 < x2 < · · · < xn.
5. The running time of Algorithm 3 is O(n log2
2 n) and the space taken by the range
tree is O(n log2 n). The running time can be improved by a log2 n factor. Essentially the
same procedure can be used to build range trees in any dimension.
Algorithm 3:
Computing the range tree.
procedure RangeT ree(S = {(x1, y1), (x2, y2), . . . , (xn, yn)}: set of points,
T : pointer to root of a range tree)
if S = ∅then return
else store the interval [x1, xn] in T.int
store BinarySearchT ree(S) in T.y
RangeT ree({(x1, y1), . . . , (x n
2 , y n
2 )}, T.left child)
RangeT ree({(x n
2 +1, y n
2 +1), . . . , (xn, yn)}, T.right child)
procedure BinarySearchT ree(S′ = {(x1, y1), . . . , (xn, yn)}: set of points)
sort the points of S′ by y-coordinate so that y1 < y2 < · · · < yn
create a threaded balanced binary search tree B for S′:
store point (xi, yi) in the ith leftmost leaf ℓi
ℓi.next := ℓi+1 {connect the leaves into a linked list}
ℓi.key := yi
for each node v, v.key := min{ℓi.key | ℓi ∈subtree(v.right child)}

988
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
Algorithm 4:
Orthogonal range reporting using range trees
procedure OrthoRangeSearching(q = [x1, x2] × [y1, y2]: rectangle in the plane,
T : pointer to root of range tree)
if T = NIL then return
else if T.int ⊆[x1, x2] then SearchAll(T.y, [y1, y2])
if [x1, x2]∩∈T.left child.int ̸= ∅then
OrthoRangeSearching(q, T.left child)
if [x1, x2]∩∈T.right child.int ̸= ∅then
OrthoRangeSearching(q, T.right child)
procedure SearchAll(v: pointer to root of binary tree, [y1, y2]: query interval)
while v is not a leaf
if y1 < v.key then v = v.left child
else v := v.right child
if v.key < y1 then v = v.next
while v ̸= NIL and v.key < y2
output point stored at v
v := v.next
6. Orthogonal range reporting proceeds as follows down the range tree. If the range
of the current node x is a subset of the x range of the query, then all the points in the
node’s binary search tree with y-coordinate in the y range of the query are output. If the
x range of the query overlaps the x range of the left child, then the algorithm proceeds
recursively to the left child. If the x range of the query overlaps the x range of the right
child, then the algorithm proceeds recursively to the right child.
7. Algorithm 4 takes O(log2
2 n+k) time, where k is the number of reported points. This
running time can be improved to O(log2 n + k).
Examples:
1. Orthogonal range search:
The query range q is a Cartesian product of intervals on
diﬀerent coordinate axes.
2. Bounded distance search: The query range q is a sphere in Rd.
3. Other typical search domains are half-spaces and simplices.
4. Machine learning: Points are labeled as positive or negative examples of a concept,
and range query determines the relative number of positive and negative examples in the
range (thus enabling the range to be classiﬁed as either a positive or negative example
of the concept).
5. Multikey searching in databases:
Records identiﬁed by a d-tuple of keys can be
viewed as a point in Rd, and the range query on records corresponds to an orthogonal
range query.
6. Part (a) of the following ﬁgure shows a set of eight points in R2: a, b, . . . , h and a
query range q. Part (b) shows the associated range tree.

Section 13.6
GEOMETRIC DATA STRUCTURES AND SEARCHING
989
10
5
(a)
(b)
c
e
h
f
g
d
a
b
00
[2,6]
[2,4]
[2]
[2,14]
[9,14]
[12,14]
[14]
[12]
[11]
[9]
[9,11]
[5,6]
[6]
7
4
11
12
8
8
8
5
2
5
11
4
b
a
d
c
b
g
a
d
f
h
c
e
g f
h
e
e
e
d
d
c
c
b
a
b
a
f
f
g
g
h
h
8
8
2
2
7
7
12
12
11
11
5
5
1
1
4
12
11
8
2 7
7
8
12
12
12
7
5
4
2
1
4
4
1
5
11
11
[5]
[4]
5
10
15
q = [0.5,  13.5]x[1.5,  8.5]
13.6.3
RAY SHOOTING AND LINES IN SPACE
Deﬁnitions:
A ray r is a half-line that is directed away from its endpoint; that is, it satisﬁes the
equation r = p + λ⃗v, λ ≥0, where p is the starting point of r and ⃗v is the direction
of r.
Given a set S of geometric objects in Rd and a query ray r, the ray shooting problem
is the problem of determining the ﬁrst object in S that is hit by r, that is, the object
s ∈S whose intersection with r is closer to p than the intersection between r and any
other object in S.
A polyhedron is axis-parallel if each of its edges is parallel to a coordinate axis.
Facts:
1. Applications of ray shooting include hidden surface removal, visibility questions and
ray tracing in computer graphics, and computing shortest paths in the presence of ob-
stacles in robotics.
2. The following table gives information on various ray shooting algorithms.

990
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
query
preprocess-
dim
subdivision type
time
space
ing time
2
simple polygon
O(log2 n)
O(n)
O(n)
2
line segments
O(log2 n)
O(n2α2(n))
O(n2α2(n))
O(√n log2 n)
O(n log2
2 n)
O(n log2
2 n)
3, ﬁx p
axis-parallel polyhedra
O(log2 n)
O(n log2 n)
O(n log2 n)
3, ﬁx p
polyhedra
O(log2 n)
O(n2α(n))
O(n2α(n))
for any ǫ > 0,
O(n1+ǫ/√m)
O(m1+ǫ)
O(m1+ǫ)
n ≤m ≤n2
3, ﬁx ⃗v
axis-parallel polyhedra
O(log2 n ×
O(n log2 n)
˜O(n log2
2 n)
(log2 log2 n)2)
for any ǫ > 0
O(log2 n)
O(n1+ǫ)
O(n1+ǫ)
3, ﬁx ⃗v
polyhedra
O(log2 n)
O(n3+ǫ)
O(n3+ǫ)
for any ǫ > 0,
O(n1+ǫ/m1/3)
O(m1+ǫ)
O(m1+ǫ)
n ≤m ≤n3
3
axis-parallel polyhedra
O(log2 n)
O(n2+ǫ)
O(n2+ǫ)
3
polyhedra
O(log2 n)
O(n4+ǫ)
˜O(n4+ǫ)
Algorithm:
1. Ray shooting from a ﬁxed point among planar nonintersecting segments:
For sim-
plicity, assume that the ﬁxed point p in the plane is at the origin. Deﬁne two relations
using the same notation: for two points qj and qk, qj ≺qk if qj makes a smaller polar
angle with respect to the origin than does qk; for two nonintersecting segments sj and
sk, sj ≺sk if for every ray r that starts at the origin and crosses both sj and sk, r
crosses sj before crossing sk. Segment (qj, qk) starts at qj and ends at qk if qj ≺qk. A
null segment is denoted s∞; that is, a query ray hitting s∞does not intersect any of the
given segments.
Algorithm 5, VisibilityMap, creates an array I of nonoverlapping angle intervals, sorted
by their polar angle, with the property that consecutive entries in I have diﬀerent “small-
est” segments according to the “≺” relation.
This algorithm uses a technique called sweep-plane: the algorithm sweeps the polar
coordinates originating at p with a ray, stopping the sweep-ray at all the angles where the
sweep-ray intersects a segment endpoint. The set S′ stores all the segments intersected
by the current sweep-ray; S′ is organized as a binary search tree ordered by the “≺”
relation on the segments.
When the sweep-ray encounters a segment endpoint that
starts a segment, the segment is added to S′; when the sweep-ray encounters a segment
endpoint that ends a segment, the segment is removed from S′. At every stop of the
sweep-ray, if the smallest (under the “≺” relation) segment of S′ is diﬀerent from the
sweep-ray’s last stop, a new interval is added to I. See Example 3.
Algorithm 5, VisibilityMap, takes O(n log2 n) time and can be used for ray shooting
among simple polygons. The visibility map consists of array I and takes O(n) space.
The problem is harder if the segments are allowed to intersect.
Algorithm 6, RayShoot, takes O(log2 n) time.

Section 13.6
GEOMETRIC DATA STRUCTURES AND SEARCHING
991
Algorithm 5:
Computing visibility map.
procedure V isibilityMap(p: ﬁxed origin point, S: set of n segments)
sort endpoints of all segments by their polar angles so that q1 ≺q2 ≺· · · ≺q2n
no of intervals := 0; S′ := ⟨s∞⟩
{S′ is a binary search tree containing segments ordered by the “≺” relation}
for i = 1 to 2n
first := the “smallest” (under the “≺” relation) segment in S′
if qi starts a segment sj then insert sj into S′
if qi ends a segment sj then remove sj from S′
if first ̸= smallest segment in S′ then
no of intervals := no of intervals + 1
I[no of intervals].angle := polar angle of qi
I[no of intervals].name := the smallest segment in S′
Algorithm 6:
Ray shooting using the visibility map.
procedure RayShoot(r = (p,⃗v): query ray, I: visibility map)
consider ⃗v as a polar angle
do a binary search among I[∗].angle to ﬁnd the segment I[k].name such that
I[k].angle ≤⃗v and I[k + 1].angle > ⃗v
Examples:
1. S can be a single object, such as a simple polygon in the plane.
2. S can be a collection of objects, such as a set of polyhedra in 3-dimensional space.
3. The following ﬁgure illustrates the sweep-plane technique, with rays originating from
point p. The thick lines in this ﬁgure are the segments in S and the thin lines are the
boundaries between intervals in the visibility map for S. The intervals are labeled by
their names.
I[5]=s2
I[4]=s3
I[6]=s4
I[7]=s2
I[8]=s∞
I[9]=s6
I[10]=s8
I[11]=s6
s7
s6
s8
s5
s4
s3
s2
I[15]=s10
I[3]=s2
I[1]=s1
I[2]=s∞
s1
s10
s11
s12
p
s9
I[14]=s∞
I[12]=s∞
I[13]=s9

992
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
13.7
COMPUTATIONAL TECHNIQUES
This section describes some techniques used in the design of geometric algorithms.
13.7.1
PARALLEL ALGORITHMS
The goal of parallel computing is to solve problems faster than would be possible on
a sequential machine — through the use of parallel algorithms. The complexity of a
parallel algorithm is given in terms of its time and the number of processors used [J´a92].
The work of a parallel algorithm is bounded above by the processor-time product, the
product of the number of processors and the time.
Deﬁnitions:
A parallel algorithm is an algorithm that concurrently uses more than one processing
element during its execution.
A parallel machine is a computer that can execute multiple operations concurrently.
A parallel random access machine (PRAM) is a synchronous machine in which
each processor is a sequential RAM and processors communicate using a shared memory.
Depending upon whether concurrent accesses are allowed to the shared memory cells, a
PRAM is either exclusive read (ER) or concurrent read (CR), and either exclusive
write (EW) or concurrent write (CW).
Facts:
1. Parallel divide-and-conquer:
Parallel algorithms can be obtained using the divide-
and-conquer paradigm. The subproblems resulting from the divide phase are solved in
parallel, and the combining phase of the algorithm is parallelized. In traditional divide-
and-conquer algorithms, the problem is partitioned into two subproblems.
2. Many-way divide-and-conquer: Sometimes faster parallel algorithms can be obtained
by partitioning the original problem into multiple, smaller subproblems, often referred to
as many-way divide-and-conquer [J´a92]. The solution to the original problem is obtained
from the solutions to the subproblems.
3. Cascading divide-and-conquer: Some divide-and-conquer algorithms can be speeded
up by pipelining (cascading) the work performed in the recursive applications as follows.
Consider a binary tree representing the solutions to the recursive computations of the
original divide-and-conquer algorithm, where leaves represent terminal subproblems. To
obtain a faster algorithm, information is passed from a child to its parent before the
solution to the child’s subproblem is completely known. The parent then does some
precomputation so that the solution to its problem can be computed as soon as the
solutions of both of its children’s problems are available.
Typically, the information
passed from a child to its parent is a constant sized sample of the child’s current state.
Often, such algorithms run in time proportional to the height of the recursion tree.
4. Cole ﬁrst used pipelining to design a work-optimal O(log n) time parallel version of
merge sort. Atallah, Cole, and Goodrich used this strategy to solve several geometric
problems including the 3-dimensional maxima problem and computing the visibility of a
polygon from a point [AtGo93].

Section 13.7
COMPUTATIONAL TECHNIQUES
993
Algorithms:
1. Using parallel divide-and-conquer to compute the convex hull CH(S) of a set S of n
points in the plane:
In Algorithm 1 the points in S are presorted by x-coordinate so
that the division in the second step can be accomplished in constant time. The sorting
takes O(log n) time using O(n log n) work [J´a92]. The tangents needed in the third step
can be computed in constant time using |SL| + |SR| processors on a CREW PRAM
[AtGo93]. Thus, as there are O(log n) recursive calls, Algorithm 1 runs in O(log n) time
using O(n log n) work, which is both worst-case work-optimal and time-optimal for the
CREW PRAM.
Algorithm 1:
ConvexHull(S: a set presorted by x-coordinate).
if |S| ≤3 then construct CH(S) directly
else
partition S into two subsets of equal size, SL (the left subset) and SR (the
right subset), which are separated by a vertical line
in parallel, recursively construct CH(SL) and CH(SR)
construct CH(S) from CH(SL), CH(SR), and common tangents between them
2. Computing the convex hull of a 3-dimensional point set:
The convex hull of a 3-
dimensional point set can be computed using a two-way divide-and-conquer algorithm
similar to Algorithm 1. The running time of this algorithm is O(log2 n) because the
combining in the last step of Algorithm 1 takes O(log n) time since it is more complex
than in the planar case. However, a faster algorithm can be designed using many-way
divide-and-conquer. The point set is partitioned into O(n
1
2 ) groups, each of size O(n
1
2 ),
and then, even though the combining still takes O(log n) time, a total time of O(log n)
can be obtained.
13.7.2
RANDOMIZED ALGORITHMS
Randomization is a powerful technique that has been used in algorithms for many geo-
metric problems.
Deﬁnition:
A randomized algorithm is an algorithm that makes random choices during its exe-
cution.
Facts:
1. Randomized algorithms are often faster, simpler, and easier to generalize to higher
dimensions than deterministic algorithms.
2. For many problems, eﬃcient algorithms can be obtained by processing the input
objects in a particular order or by grouping them into equal-sized subsets. Although
signiﬁcant computation may be required to exactly determine an appropriate order or a
good partition into subsets, in many cases simple random choices can be used instead.
3. Randomized incremental methods: One of the simplest ways to construct a geometric
structure is incrementally. In an incremental construction algorithm, the input objects
are inserted one at a time and the current structure is updated after each addition. The

994
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
desired structure is obtained after all input objects have been inserted. Although the cost
of some updates could be very large, in many cases it can be shown that if the objects
are inserted in random order, then the amount of work for each update is expected to be
small and the expected running time of the algorithm will be small as well. Thus, the
expectation of the running time depends not on the input distribution but on the ratio
of good to bad insertion sequences.
4. The power of randomization in incremental algorithms was ﬁrst noted by Clarkson
and Shor, and by Mulmuley. Randomized incremental algorithms have been proposed for
constructing many geometric structures including convex hulls, Delaunay triangulations,
trapezoidal decompositions, and Voronoi diagrams [Mu94].
5. Randomized divide-and-conquer: In randomized divide-and-conquer a random sub-
set of the input is used to partition the original problem into subproblems. Then, as in
any divide-and-conquer algorithm, the subproblems are solved, perhaps recursively, and
their solutions are combined to obtain the solution to the original problem. Ideally, the
partition produces subproblems of nearly equal size and the sum of the subproblem sizes
roughly equals the input size. This ideal can almost be achieved for many geometric
problems. For example, with probability at least 1
2, O(r) subproblems of size O( n log r
r
)
can often be obtained, where n is the number of input objects and r is the size of the
random subset.
6. The fact that a random sample of the input items can often be used to produce sub-
problems of almost equal size was ﬁrst shown by Clarkson, and by Haussler and Welzl.
Randomized divide-and-conquer algorithms are known for many geometric problems in-
cluding answering closest-point queries, constructing arrangements, triangulating point
sets, and constructing convex hulls [Mu94].
Examples:
1. Incrementally constructing the intersection of a set H of n halfplanes in the plane:
First, a random permutation (h1, h2, . . . , hn) of H is formed. Let Ii denote the intersec-
tion of halfplanes h1 through hi. During the ith iteration of the algorithm, Ii is computed
from Ii−1 by removing the portion of Ii−1 that is not contained in hi.
To make this update easier, a vertex of the current intersection Ii−1 that is not contained
in hj is maintained, for all j ≥i; such a vertex is said to conﬂict with hj.
Given
a conﬂicting vertex v for hi, the portion of Ii−1 that must be removed is determined
by traversing its boundary in both directions from v until reaching vertices that are
contained in hi.
Since In has size O(n), the amortized cost of each update is O(1)
and the algorithm spends a total of O(n) time updating the intersection. After Ii is
computed, the conﬂicting vertex for hj is updated, for all j > i. It can be shown that
the total cost of maintaining the conﬂicting vertices in the algorithm is O(n log n).
2. Using randomized divide-and-conquer to construct the intersection I(S) of a set S
of n halfplanes, each of which contains the origin:
See Algorithm 2. In Step 2, the
intersection of the halfplanes in the sample is used to create O(r) triangles, each of
which corresponds to a subproblem. For technical reasons, the region corresponding to
each subproblem should have constant descriptive complexity.
Often, this condition is achieved by triangulating the resulting structure.
Next, the
input is distributed to the subproblems. Usually, this is done by ﬁnding the objects that
intersect, or conﬂict with, a subproblem’s region.
Then, the subproblems are solved, recursively if desired, and their solutions are combined
to form the solution to the original problem.
In the intersection algorithm the ﬁnal
solution is obtained by “gluing” the subproblem solutions together.

Section 13.7
COMPUTATIONAL TECHNIQUES
995
If r is some suitably chosen constant, then Algorithm 2 runs in expected time O(n log n).
Algorithm 2:
Intersect(S: a set of n halfplanes).
R := a random sample of size r chosen from S.
compute the intersection I(R) and triangulate it by connecting each vertex of
I(R) to the origin to obtain T (R)
for each triangle t ∈T (R), determine its conﬂict list c(t) (the set of halfplanes
in S whose bounding lines intersect t)
for each triangle t ∈T (R), compute the intersection of the halfplanes in c(t) re-
stricted to t {This may be done recursively}
13.7.3
PARAMETRIC SEARCH
Deﬁnition:
Parametric search is an algorithmic technique for solving optimization problems by
using an algorithm for solving an associated decision problem as a subroutine.
Algorithm:
1. Parametric search is a powerful algorithmic technique that can be applied to a diverse
range of optimization problems. It is best explained in terms of an abstract optimization
problem. Consider two versions of this problem:
• a search problem P, which can be parametrized by some real, nonnegative value t
and whose solution is a value t∗that satisﬁes the optimization criteria;
• a decision problem D(t), whose input includes a real, nonnegative value t and
whose solution is either YES or NO.
In parametric search, the search problem P is solved using an algorithm As for the
decision problem D(t). In order to apply parametric search, the points corresponding
to YES answers for D(t) must form a connected, possibly unbounded, interval in [0, ∞).
Then, assuming that D(0) = YES, the search problem P is to ﬁnd the largest value of t
for which D(t) = YES.
The basic idea of parametric search is to use As to ﬁnd t∗. This is done by simulating As,
but using a variable (parameter) instead of a value for t. Assume that As can detect
if t = t∗, and that the computation in As consists of comparisons, each of which tests
the sign of a bounded degree polynomial in t and in the original input. During each
comparison in the simulation:
• the roots of the appropriate polynomial are computed,
• As is run on each root,
• t∗is located between two consecutive roots.
Thus, each comparison in the simulation reduces the interval known to contain t∗, which
is originally [0, ∞).

996
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
Facts:
1. In most geometric applications it can be shown that at some point the simulation
will test a root that is equal to t∗.
2. If Ts denotes the worst-case running time of As, the total cost of the parametric search
is O(T 2
s ) since there are O(Ts) operations in the simulation of As, and each comparison
operation in this simulation takes O(Ts) time.
3. With a parallel algorithm Ap to solve the decision problem D(t), the (sequential)
parametric search can be done faster. Suppose that Ap runs in Tp parallel time steps using
p processors. Then, Ap performs at most p independent comparison operations during
each parallel time step. The parametric search simulates Ap sequentially as follows. To
simulate a parallel time step of Ap:
• compute the O(p) roots of the at most p bounded degree polynomials correspond-
ing to independent comparisons in the parallel time step;
• sort these roots;
• use a binary search to locate t∗between two consecutive roots. (The sequential
algorithm As is used to evaluate the roots during the binary search.)
The cost of simulating each parallel time step is O(p log p + Ts log p), and the total cost
of the parametric search is O(Tp(p log p + Ts log p)).
4. Parametric search was originally proposed by Megiddo. It has been used for many
geometric problems including ray shooting, slope selection, and computing the diameter
of a 3-dimensional point set [Mu94].
Example:
1. Answering ray shooting queries in a convex polytope Q: In the search problem P, a
ray r is given originating at a point p contained in Q, and the face of Q hit by r needs
to be found. The decision problem D(t) is the polytope membership problem: given a
point t, determine if t lies in Q. The value t∗sought by the parametric search is the
point where r intersects Q. The ray shooting problem can be solved in O(log2 n) time
by parametric search [Mu94] (since, given an appropriate data structure, the polytope
membership problem can be solved sequentially in O(log n) time).
13.7.4
FINITE PRECISION
Geometric algorithms are usually designed assuming the real random access machine
model of computation. In this model, values can be arbitrarily long real numbers, and
all standard operations such as +, −, ×, and ÷ can be performed in unit time regardless
of operand length.
In reality, however, computers have ﬁnite precision and can only
approximate real numbers. Several techniques have been suggested for dealing with this
problem [Fo93].
Facts:
1. Algorithms directly designed for a discrete domain: This approach can dramatically
increase the complexity of the algorithm and has not gained wide use.
2. Floating point numbers:
Floating point numbers provide a convenient and eﬃcient
way to approximate real numbers. Unfortunately, naively rounding numbers in geometric
algorithms can create serious problems such as topological inversions. There are cases
when ﬂoating point arithmetic can safely be used. For example, for certain inputs the

Section 13.7
COMPUTATIONAL TECHNIQUES
997
result of the ﬂoating point operation will be unambiguous. Some algorithms have been
shown to be suﬃciently stable using ﬂoating point arithmetic.
However, no general
method is known for designing stable algorithms.
3. Exact arithmetic: In exact arithmetic, numbers are represented by vectors of integers
and all primitive operations are guaranteed to give correct answers. Integer arithmetic is
suﬃcient for many geometric algorithms since symbolic or algebraic numbers are rarely
needed, and in many cases homogeneous coordinates can remove the need for rational
numbers.
However, since exact representations can have large bit complexity, exact
arithmetic can be expensive — typically increasing the cost of arithmetic operations
by an order of magnitude.
This cost can be decreased somewhat by optimizing the
expressions and computations involving exact arithmetic.
4. A combination of ﬂoating point and exact arithmetic:
First, the operation is per-
formed using ﬂoating point arithmetic. Then, if the result is ambiguous, an exact com-
putation is performed.
5. Adaptive-precision arithmetic: In adaptive-precision arithmetic each number is ap-
proximated by an interval whose endpoints require lower precision. If the exact com-
putation using the approximation is ambiguous, the method iterates using smaller and
smaller intervals with higher precision endpoints.
6. Although no clear consensus has been reached, a combination of the above strategies
may yield the best results.
13.7.5
DEGENERACY AVOIDANCE
To simplify exposition by reducing the number of cases that must be considered, many
geometric algorithms assume that the input is in general position. The general position
assumption depends on the problem. For example, in problems involving planar point
sets, the assumption might be that no two points have the same x-coordinate, that no
three points lie on the same line, or that no four points lie on the same circle.
Deﬁnitions:
A set of objects is in general position if the objects satisfy certain speciﬁed conditions.
A set of input objects that violates the general position assumption is degenerate.
Facts:
1. Perturbation:
Several schemes have been proposed that apply small perturbations
to transform the input so that it does not contain degeneracies [Fo93]. The object of
perturbation schemes is to allow the design of simpler algorithms which may validly
assume the input is in general position.
Algorithms using perturbation schemes may not always produce correct output. For ex-
ample, in a convex hull algorithm, points on the boundary could potentially be perturbed
into the interior of the polytope and vice versa. Also, perturbation schemes can aﬀect,
perhaps adversely, output-sensitive algorithms whose running times depend on the size
of the output they produce.
2. Deal with degeneracies directly:
Algorithms that deal with degeneracies in a prob-
lem speciﬁc manner have been designed for problems such as triangulating point sets.
Although this approach is not as general and usually leads to more complex algorithms
than those employing perturbation schemes, it can provide superior performance.

998
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
3. Symbolically perturbing the coordinates of each input point (Edelsbrunner, M¨ucke):
This is done by adding to each coordinate a suitable power of a small, positive real number
represented by a symbolic parameter ǫ. Then, since values are now polynomials in ǫ, the
arithmetic operations in the algorithm are replaced by polynomial arithmetic operations.
Geometric primitives are implemented symbolically, typically by evaluating a sequence of
determinants. Assuming that the dimension of the problem is ﬁxed, this scheme increases
the running time of the algorithm by at most a constant factor. However, the overhead
incurred can in fact be quite large.
13.8
APPLICATIONS OF GEOMETRY
Geometry overlays the entire computing spectrum, having applications in almost every
area of science and engineering, including astrophysics, molecular biology, mechanical
design, ﬂuid mechanics, computer graphics, computer vision, geographic information
systems, robotics, multimedia, and mechanical engineering.
The growing availability of large geometric databases is a major driver of the increase
in applications. GIS mapping databases containing most of the world’s roads enable
route planning applications. Airborne LIDAR creates terrain elevation databases. That
enables observer visibility computation, with applications ranging from radio tower siting
to visual nuisance mitigation. Laser scanners produce 3D point clouds recording the
surfaces of objects from buildings down to sculptures and even people.
The ensuing
applications include deducing the structure of those objects’ surfaces.
13.8.1
MATHEMATICAL PROGRAMMING
Deﬁnition:
Mathematical programming is the large-scale optimization of an objective function
(such as cost) of many variables subject to constraints, such as supplies and capacities.
Facts:
1. Mathematical programming includes both linear programming (continuous, integer,
and network) and nonlinear programming (quadratic, convex, general continuous, and
general integer).
2. Applications include transportation planning and transshipment, factory production
scheduling, and even determining a least-cost but adequate diet.
3. A modeling language, such as AMPL, is often used in applications [FoGaKe93].
Example:
1. Linear programming in low dimensions:
This is a special case of linear program-
ming since there are algorithms whose time is linear in the number of constraints but
exponential in the number of dimensions.
Application:
1. Find the smallest enclosing ball of a set of points or a set of balls in arbitrary dimen-
sion [We91]. This uses a randomized incremental algorithm employing the move-to-front
heuristic.

Section 13.8
APPLICATIONS OF GEOMETRY
999
13.8.2
POLYHEDRAL COMBINATORICS
Polyhedra have been important to geometry since the classiﬁcation of regular polyhedra
in classical times.
Fact:
1. The following are some of the many possible operations that can be performed on
polygons and polyhedra:
• Boolean operations, such as intersection, union, and diﬀerence;
• point location of new query points in a preprocessed set of polygons, which may
partition a larger region;
• range search of a preprocessed set of points to ﬁnd those inside a new query
polygon;
• decomposition (or triangulation) of a polygon into triangles or a polyhedron into
tetrahedra (§13.4.2). A simple n-gon is always decomposable into n−2 triangles,
in linear time, and all triangulations have exactly n −2 triangles. However,
some polyhedra in R3 cannot be partitioned intro tetrahedra without additional
Steiner points. Also, diﬀerent triangulations of the same polyhedron may have
diﬀerent numbers of tetrahedra.
Applications:
1. Aperiodic tilings and quasicrystals:
Tilings (in the plane) and crystallography (in
3-dimensional space) are classic applications of polygons and polyhedra. A recent de-
velopment is the study of aperiodic (Penrose) tilings [Ga77] and quasicrystals [Ap94].
These are locally but not globally symmetric under 5-fold rotations, quasi-periodic with
respect to translations, and self-similar. The 1991 Nobel Prize in Chemistry was awarded
for the discovery of actual quasicrystals:
• http://www.nobelprize.org/nobel prizes/chemistry/laureates/2011/
Large-scale symmetries, such as 5-fold, that are impossible in traditional crystallography,
can be visible with X-ray diﬀraction. Tilings can be constructed by projecting simple
objects from, say, R5. One application is the surface reinforcement of soft metals.
2. Error-correcting codes: Some error-correcting codes can be visualized with polytopes
as follows. Assume that the goal is k-bit symbols, where any error of up to b bits can
be detected. The possible symbols are some of the 2k vertices of the hypercube in k-
dimensional space. The set of symbols must contain no two symbols less than b + 1
distance apart, where the metric is the number of diﬀerent bits. If errors of up to c bits
are to be correctable, then no two symbols can be closer than 2c + 1.
Similarly, in quantum computing, a quantum error-correcting code can be designed using
Cliﬀord groups and binary orthogonal geometry [CaEtal97].
13.8.3
COMPUTATIONAL CONVEXITY
Deﬁnitions:
An H-polytope is a polytope deﬁned as the intersection of m half-spaces in Rn.

1000
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
A V-polytope is a polytope deﬁned as the convex hull of m points in Rn.
A zonotope is the vector (Minkowski) sum of a ﬁnite number of line segments.
Computational convexity is the study of high-dimensional convex bodies.
An oracle is an algorithm that gives information about a convex body.
Facts:
1. Computational complexity is related to linear programming, polyhedral combina-
torics, and the algorithmetic theory of polytopes and convex bodies.
2. In contrast to computational geometry, computational complexity considers convex
structures in normed vector spaces of ﬁnite but not restricted dimension. If the body
under consideration is more complex than a polytope or zonotope, it may be repre-
sented as an oracle. Here the body is a black-box, and all information about it, such
as membership, is supplied by calls to the oracle function. Typical algorithms involve
volume computation, either deterministically, or by Monte Carlo methods, perhaps after
decomposition into simpler bodies, such as simplices.
3. When the dimension n is ﬁxed, the volume of V-polytopes and H-polytopes can be
computed in polynomial time.
4. There does not exist a polynomial-space algorithm for the exact computation of the
volume of H-polytopes (where n is part of the input).
5. Additional information on computational convexity can be found at
• http://dimacs.rutgers.edu/TechnicalReports/TechReports/1994/94-31.ps
13.8.4
MOTION PLANNING IN ROBOTICS
In Computer Assisted Manufacturing, both the tools and the parts being assembled must
often be moved around each other in a cluttered environment. Their motion should be
planned to avoid collisions, and then to minimize cost.
Deﬁnition:
A Davenport-Schinzel sequence of order s over an alphabet of size n, or DS(n, s), is
a sequence of characters such that:
• no two consecutive characters are the same;
• for any pair of characters, a and b, there is no alternating subsequence of length
s + 2 of the form . . . a . . . b . . . a . . . b . . . .
Facts:
1. Practical general motion, path planning, is solvable with Davenport-Schinzel se-
quences [ShAg95]. Upper bounds on λs(n), the length of the longest DS(n, s), determine
upper bounds on the complexity of the lower envelopes of certain functions.
For example, given n points in the plane that are moving with positions that are polyno-
mials of degree s in time, the number of times that the closest pair of points can change
is λ2s(
 n
2

).
2. Visibility graphs (§13.5.5) are useful in ﬁnding a shortest path between two points in
the plane, in the presence of obstacles.

Section 13.8
APPLICATIONS OF GEOMETRY
1001
3. The problem of moving a ﬁnite object in the presence of obstacles may also be mapped
into a conﬁguration space (or C-space) problem of moving a corresponding point in a
higher dimension. If translational and rotational motion in 2-dimensional (respectively
3-dimensional) is allowed, then the C-space is 3-dimensional (respectively 6-dimensional).
4. Articulated objects, multiple simultaneous motion, and robot hands also increase the
number of degrees of freedom.
5. Current problems:
• representation of objects, since although planar, faceted models are simpler, the
objects should be algebraic surfaces, and even if they are planar, in C-space
their corresponding versions will be curved;
• grasping, or placing a minimal number of ﬁngers to constrain the object’s motion;
• sequence planning of the assembly of a collection of parts;
• autonomous navigation of robots in unstructured environments.
13.8.5
CONVEX HULL APPLICATIONS
Facts:
1. The convex hull (§13.5.1) is related to the Voronoi diagram (§13.5.3) since a convex
hull problem in Rk is trivially reducible to a Voronoi diagram problem in Rk, and a
Voronoi diagram problem in Rk is reducible to a convex hull problem in Rk+1.
2. The deﬁnition of convex hull is not constructive, in that it does not lead to a method
for ﬁnding the convex hull. Nevertheless, there are many constructive algorithms and im-
plementations. One common implementation is QuickHull (§13.5.1), a general dimension
code for computing convex hulls, Delaunay triangulations, Voronoi vertices, furthest-site
Voronoi vertices, and half-space intersections [BaDoHu96].
3. Alpha-shapes are a useful generalization of convex hulls [Ed05]. An intuitive deﬁni-
tion is as follows. For a given α, form a surface by rolling around the given point set
with a sphere of radius α, then smooth out the curved regions. The speciﬁcation α = ∞
gives the convex hull of the point set. The smaller the value of α, the more closely the
original points are followed. One application is inferring a surface from a 3D point set.
A good introduction to the topic is found at
• http://graphics.stanford.edu/courses/cs268-16-fall/Handouts/
AlphaShapes/as fisher.pdf
Examples:
1. Mathematics:
• determining the principal components of spectral data;
• studying circuits of matroids that form a Hilbert base;
• studying the neighbors of the origin in the R8 lattice.
2. Biology and medicine:
• classifying molecules by their biological activity;
• determining the shapes of left ventricles for electrical analysis of the heart.
3. Engineering:

1002
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
• computing support structures for objects in layered manufacturing in rapid pro-
totyping [StBrEa95].
By supporting overhanging material, these structures
prevent the object from toppling while partially built.
• designing nonlinear controllers for controlling vibration;
• ﬁnding invariant sets for delta-sigma modulators;
• classifying handwritten digits;
• analyzing the training sets for a multilayer perceptron model;
• determining the operating characteristics of process equipment;
• navigating robots;
• creating 6-dimensional wrench spaces to measure the stability of robot grasps;
• building micromagnetic models with irregular grain structures;
• building geographical information systems;
• simulating a spatial database system to evaluate spatial tesselations for indexing;
• producing virtual reality systems;
• performing discrete simulations of incompressible viscous ﬂuids using vortex meth-
ods;
• modeling subduction zones of tectonic plates and studying ﬂuid ﬂow and crystal
deformation;
• computing 3-dimensional unstructured meshes for computational ﬂuid dynamics.
13.8.6
NEAREST NEIGHBOR
Variants of the problem of ﬁnding the nearest pair of a set of points have applications in
ﬁelds from handwriting recognition to astrophysics.
Facts:
1. Fixed search set, varying query point: A ﬁxed set P of n points in Rd is preprocessed
so that the closest point p ∈P can be found for each query point q. The search time
per query can range from log n (if d = 2) to n
d
2 (for large d). The Voronoi diagram is
commonly used in low dimensions. However, because of the Voronoi diagram’s complexity
in higher dimensions, hierarchical search structures, bucketing, and probabilistic methods
perhaps returning approximate answers are common.
2. Moving points:
The points in P may be moving and the close pairs of points over
time is of interest.
Examples:
1. Fixed search sets, varying query point:
• Character recognition in document processing: Each representative character is
deﬁned by a vector of features. Each new, unknown character must be mapped
to the closest representative character in feature space.
• Color map optimization in computer graphics:
Many frame buﬀers allow only
the 256 colors in the current color map to be displayed simultaneously, from a
palette of 224 possible colors. Thus, each color in a new image must be mapped
to the closest color in the color map.
A related problem is the problem of
determining what colors to use in the color map.

Section 13.8
APPLICATIONS OF GEOMETRY
1003
• Clustering algorithms for speech and image compression in multimedia systems:
As in the color map problem, a large number of points must be quantized down
to a smaller set.
2. Moving points:
• Simulation of star motion in astrophysics:
Calculating the gravitational attrac-
tion between every pair of stars is too costly, so only close pairs are individually
calculated. Otherwise the stars are grouped, and the attraction between close
groups is calculated. The groups may themselves be grouped hierarchically.
• Molecular modeling: In molecular modeling, close pairs of atoms will be subject
to van der Waals forces.
• Air traﬃc control:
Air traﬃc controllers wish to know about pairs of aircraft
closer than a minimum safe distance. Here the metric is nonuniform; small
vertical separations are more tolerable than small horizontal separations.
• During path planning in robotics and numerically controlled machining, unin-
tended close pairs of objects must also be avoided.
13.8.7
COMPUTER GRAPHICS
Computer graphics may be divided into modeling of surfaces, and simulation of the
models. The latter includes rendering a scene and its light sources to generate synthetic
imagery with respect to some viewpoint. Rendering involves visibility, or determining
which parts of the surfaces are visible, and shading them according to some lighting
model.
Deﬁnitions:
Anti-aliasing refers to ﬁltering out high-frequency spatial components of a signal, to
prevent artifacts, or aliases, from appearing in the output image. In graphics, a high
frequency may be an object whose image is smaller than one pixel or a sharp edge of an
object.
A GUI (graphical user interface) is a mechanism that allows a user to interactively
control a computer program with a bitmapped display by using a mouse or pointer to
select menu items, move sliders or valuators, and so on. The keyboard is only occasionally
used. A GUI contrasts with typing the program name followed by options on a command
line, or by preparing a text ﬁle of commands for the program. A GUI is easier and more
intuitive to use, but can slow down an expert user.
Examples:
1. Visibility: Visibility algorithms may be object-space, where the visible parts of each
object are determined, or image-space, where the color of each pixel in the frame buﬀer
is determined. The latter is often simpler, but the output has less meaning, since it is
not referred back to the original objects. Techniques include ray tracing and radiosity.
• Ray tracing: Ray tracing extends a line from viewpoint through each pixel of the
frame buﬀer until the ﬁrst intersecting object. If that surface is a mirror, then
the line is reﬂected from the surface and continues in a diﬀerent direction until
it hits another object (or leaves the scene). If the object is glass, then both a
reﬂecting and a refracting line are continued, with their colors to be combined
according to Fresnel’s law.

1004
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
One geometry problem here is that of sampling for subpixel averaging. The
goal is to color a square pixel of a frame buﬀer according to the fraction of its
area occupied by each visible object. Given a line diagonally crossing a pixel,
the fraction of the pixel covered by that face must be obtained for anti-aliasing.
If the edges of two faces intersect in this pixel, each face cannot be handled
independently, for example with an anti-aliased Bresehnam algorithm. If this
is done badly, then it is very obvious in the ﬁnal image as a possible fringe of a
diﬀerent color around the border of the object [Mi96].
The solution is to pick a small set of points in the pixel (typically 9, 16, or 64
points), determine which visible object projects to each point, and combine
those colors. The problem is then to select a set of sampling points in the pixel,
such that given a subset region, the number of points in it approximates its
area. Four possible methods, from worst to best, are to
⋄pick the points independently and uniform randomly;
⋄use a nonrandom uniform distribution;
⋄start with the above distribution, then jitter the points, or perturb each one
slightly;
⋄use simulated annealing to improve the point distribution.
• Radiosity:
Radiosity partitions the scene into facets, computes a form factor of
how much light from each facet will impinge on each other, and solves a system
of linear equations to determine each facet’s brightness. This models diﬀuse
lighting particularly well.
• Windowing systems: Another visibility problem is designing the appropriate data
structure for representing the windows in a GUI, so that the window that is in
front at any particular pixel location can be determined, in order to receive the
input focus.
• Radio wave propagation: The transmission of radio waves, as from cellular tele-
phones, which are reﬂected and absorbed by building contents, is another ap-
plication of visibility [Fo96].
2. Computer vision: Applications of geometry to vision include model-based recognition
(or pattern matching), and reconstruction or recovery of 3-D structure from 2-D images,
such as stereopsis, and structure from motion.
In recognition, a model of an object
is transformed into a sensor-based coordinate system and the transformation must be
recovered. In reconstruction, the object must be determined from multiple projections.
3. Medical image shape reconstruction:
Various medical imaging methods, such as
computer tomography, produce data in the form of successive parallel slices through the
body.
The basic step in reconstructing the 3-dimensional object from these slices in
order to view it involves joining the corresponding vertices and edges of two polygons in
parallel planes by triangles to form a simple polyhedron. However, there exists a pair of
polygons that cannot be so joined [GiORSu96].
13.8.8
MECHANICAL ENGINEERING DESIGN AND MANUFACTURING
Geometry is very applicable in CAD/CAM, such as in the design and manufacture of
automobile bodies and parts, aircraft fuselages and parts such as turbine blades, and
ship hulls and propellers. The current leading edge of CAD/CAM is additive manu-
facturing, also known as 3D printing.

Section 13.8
APPLICATIONS OF GEOMETRY
1005
Examples:
1. Representations:
How should mechanical parts be represented?
One problem is
that geometric descriptions are verbose compared to 2-dimensional descriptions, such
as draftings, since those assume certain things that the users will ﬁll in as needed, but
which must be explicit in the geometric description. Currently, it is possible to additively
manufacture objects that cannot be represented so that their properties can be analyzed.
The problem is objects containing lattices with billions of cells and nonhomogeneous
(graded) materials. See George Allen’s comments in
• http://www.3dcadworld.com/why-cad-is-hard-geometric-problems/
• http://www.cs.technion.ac.il/gdm2014/Presentations/GDM2014 allen.pdf
Possible representation methods include the following:
• constructive solid geometry:
Primitive objects, such as cylinders and blocks,
are combined with the regularized Boolean operators union, intersection, and
diﬀerence.
• faceted boundary representation: The object is a polyhedron with a boundary of
planar faces.
• exact boundary representation:
The object is deﬁned by boundary “faces”, but
now each face can be curved, such as a NURBS (Non-Uniform Rational B-
Spline), or an implicit piecewise quadric, Dupin cyclide (a quartic surface that
is good for blending two quadric surfaces), or supercyclide.
The possible methods can be evaluated with the following criteria:
• robustness against numerical errors;
• elegance;
• accuracy in representing complex, curved, shapes, especially blends between the
two surfaces at the intersection of two components;
• ease of explicitly obtaining geometry such as the boundary;
• eﬃciency when implemented on GPUs, which prefer simple, regular, representa-
tions.
2. Mesh generation:
A mesh is the partition of a polyhedron into, typically, tetra-
hedra or hexahedra to facilitate ﬁnite element modeling. A good mesher conforms to
constraints, can change scale over a short distance, has no unnecessary long thin ele-
ments, and has fewer elements when possible. In some applications, periodic remeshing
is required.
If the elements are tetrahedra, then a Delaunay criterion that the circumsphere of each
tetrahedron contains no other vertices may be used. However, this is inappropriate in
certain cases, such as just exterior to an airfoil, where a (hexahedral) element may have
an aspect ratio of 100,000:1. This raises numerical computation issues.
Applications of meshing outside mechanical design include computational ﬂuid dynamics,
contouring in GIS, terrain databases for real-time simulations, and Delaunay applications
in general.
Some mesh generation and additive manufacturing programs include the
following.
• Triangle, a 2-dimensional quality mesh generator and Delaunay triangulator. Tri-
angle generates exact Delaunay triangulations, constrained Delaunay triangu-
lations, conforming Delaunay triangulations, Voronoi diagrams, and triangular
meshes. The latter can be generated with no small or large angles, and are thus
suitable for ﬁnite element analysis.

1006
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
https://www.cs.cmu.edu/~quake/triangle.html
• TetGen, a quality tetrahedral mesh generator and a 3D Delaunay triangulator.
http://wias-berlin.de/software/tetgen/
• MeshLab is an open-source system for processing and editing 3D triangular meshes.
It provides a set of tools for editing, cleaning, healing, inspecting, rendering,
texturing, and converting meshes. It oﬀers features for processing raw data
produced by 3D digitization tools/devices and for preparing models for 3D
printing.
http://www.meshlab.net/
• ImplicitCAD is an open-source, programmatic CAD environment.
http://www.implicitcad.org/
• Autodesk Netfabb is an excellent commercial product with a comprehensive range
of connected tools, including design optimization and printing.
https://www.netfabb.com/
• Slic3r is a widely used free open and ﬂexible toolchain for 3D printers. It intro-
duced many features such as multiple extruders, brim, microlayering, bridge
detection, command line slicing, variable layer heights, sequential printing (one
object at time), honeycomb inﬁll, mesh cutting, object splitting into parts,
AMF support, avoiding crossing perimeters, and distinct extrusion widths.
http://slic3r.org/
3. Minimizing workpiece setup in numerically controlled (NC) machining:
In 4- and
5-axis numerically controlled machining, in order to machine all the faces, the workpiece
must be repeatedly dismounted, recalibrated, and remounted. This setup can take much
more time than the actual machining. Minimizing the number of setups by maximizing
the number of faces that can be machined in one setup is a visibility problem harder than
ﬁnding an optimal set of observers to cover some geographic terrain. Exact solutions are
NP-hard; approximate solutions use geometric duality, topological sweeping, and eﬃcient
construction and searching of polygon arrangements on a sphere.
4. Dimensional tolerancing:
Tolerancing refers to formally modeling the relationships
between mechanical function and geometric form while assigning and analyzing dimen-
sional tolerances to ensure that parts assemble interchangeably [SrVo93]. A tolerance
may be speciﬁed parametrically, as a variation in a parameter, such as the width of a
rectangle, or as a zone that the object’s boundary must remain in. The latter is more
general but must be restricted to prohibit pathologies, such as the object’s boundary
being not connected.
Tolerance synthesis attempts to optimize the tolerances to minimize the manufacturing
cost of an object, considering that, while large tolerances are cheaper to manufacture,
the resulting product may function poorly [Sk96].
Unsolved Problems:
The following problems are perpetually on the list of those needing more research.
1. Blending between two surfaces in mechanical design, especially at the ends of the
blend, where these surfaces meet others. (A blending surface smooths the intersection
of two surfaces by being tangent to them, each along a curve.)

Section 13.8
APPLICATIONS OF GEOMETRY
1007
2. Variational design of a class of objects subject to constraints. Well-designed con-
straint systems may have multiple solutions; the space must be searched for the correct
one. Labeling derivative entities, such as the edge resulting from the intersection of two
inputs, is an issue partly because this edge may not exist for some parameter values.
3. Generally formalizing the semantics of solid modeling [Ho96].
4. Updating simplifying assumptions, such as the linearity of random access memory,
and points being in general position, which were useful in the past, but which cause
problems now.
5. Accounting for dependencies between geometric primitives, and maintaining topo-
logical consistency.
6. Designing robust algorithms when not only is there numerical roundoﬀduring the
computation, but also the input data are imprecise, for example, with faces not meeting
properly.
7. Better 3-dimensional anti-aliasing to remove crevices and similar database errors
before rapid prototyping.
8. There still remains a need for many features in geometry implementations, such as
more geometric primitives at all levels, default visualization or animation easily callable
for each data structure, more rapid prototyping with visualization, a visual debugger for
geometric software, including changing objects online, and generally more interactivity,
not just data-driven programs.
13.8.9
LAYOUT PROBLEMS
The eﬃcient layout of objects has wide-ranging applications in geometry.
Examples:
1. Textile part layout:
The clothing industry cuts parts from stock material after
performing a tight, nonoverlapping, layout of the parts, in order to minimize the costs
of expensive material. Often, because the cloth is not rotationally symmetric, the parts
may be translated, but not rotated.
Therefore, geometric algorithms for minimizing
the overlap of translating polygons are necessary. Since this problem is PSPACE-hard,
heuristics must be used [Da95], [LiMi95].
2. VSLI layout: Both laying out circuits and analyzing the layouts represent important
problems. The masks and materials of a VLSI integrated circuit design are typically
represented as rectangles, mostly isothetic, although 45 degrees or more general angles
of inclination for the edges are becoming common. The rectangles of diﬀerent layers
may overlap. One integrated circuit may be 50MB of data before its hierarchical data
structure is ﬂattened, or 2GB after. For further details, see [WeHa11].
Geometry problems include the following.
• design rule veriﬁcation:
It is necessary to check that objects are separated by
the proper distances and that average metal densities are appropriate for the
fabrication process.
• polygon simpliﬁcation: A design described by a complex set of polygons may per-
haps be optimized into a smaller set of isothetic polygons (with only horizontal
and vertical sides), such that the symmetric diﬀerence from the original design
is as small as possible.

1008
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
• logic veriﬁcation: The electrical circuit is determined by the graph extracted from
the adjacency information of the rectangles, and whether it matches the original
logic design is determined. A subproblem is determining devices (transistors),
which occur when rectangles of two particular diﬀerent layers overlap.
• capacitance:
This depends on the closeness of the component rectangles, which
might be overlapping or separated, representing two conductors.
• PPC (Process Proximity Correction): This means to correct the eﬀect that, when
etching a circuit, a rectangle’s edges are displaced outward, possibly causing it
to come too close to another rectangle, and change the circuit.
13.8.10
GRAPH DRAWING AND VISUALIZATION
The classic ﬁeld of graph drawing and visualization (see [KaWa01], [Ta14], and [TaTo95])
aims automatically to display a graph, emphasizing fundamental properties such as sym-
metry while minimizing the ratio between longest and shortest edges, number of edge
crossings, etc. Applications include advanced GUIs, visualization systems, databases,
showing the interactions of individuals and groups in sociology, illustrating connections
between components in software engineering, and visualization of biological networks,
including phylogenetic trees, metabolic networks, protein-protein interaction networks,
and gene regulatory networks.
Facts:
1. Graph G can be drawn as the 1-skeleton of a convex polytope in R3 if and only if G
is planar and 3-connected (Steinitz). See [Gr67].
2. Given a 3-connected planar graph, the graph can be drawn as a convex polyhedron in
R3 using O(n) volume while requiring the vertices to be at least unit distance apart, which
allows them to be visually distinguished. This can be done in O(n1.5) time [ChGoTa96].
3. A special language GraphML is available for representing graphs and drawings of
these graphs.
4. Three of the most widely used software systems for constructing graph drawings are
the Open Graph Drawing Framework (OGDF), the GDTookit, and the Public Imple-
mentation of a Graph Algorithm Library and Editor PIGALE, available at
• http://www.ogdf.net
• http://www.dia.uniroma3.it/~gdt
• http://pigale.sourceforge.net
5. There are a variety of hardware and software systems available for visualization of
3-dimensional graph drawings. See [La01] and the conference sites
• http://algo.math.ntua.gr/~gd2016/
• http://www.diagrams-conference.org/2016/
13.8.11
GEOGRAPHIC INFORMATION SYSTEMS
A map (§8.6.4) is a planar graph. Minimally, it contains vertices, edges, and polygons.
However, a sequence of consecutive edges and 2-vertices is often called a chain (or poly-
line), and its interior vertices points. For example, if each polygon is one nation, then
the southern border of Canada with the USA is one chain.

Section 13.8
APPLICATIONS OF GEOMETRY
1009
Deﬁnition:
A geographic information system (GIS) is an information system designed to cap-
ture, store, manipulate, analyze, and display spatial or geographically-referenced data.
Facts:
Typical simple geometric operations are given in Facts 1–6. More complex ones are given
in Facts 7–9.
1. Projecting data from one map projection to another, and determining the appropriate
projection:
Since the earth is not a developable surface, no projection meets all the
following criteria simultaneously: equal-area, equidistant (preserving distances from one
central point to every other point), conformal (preserving all angles), and azimuthal
(correctly showing the compass angle from one central point to every other point). Since
a projection that meets any one criterion exactly is quite bad in the others, the most
useful projections tend to be compromises, such as the recent Robinson projection [Da95].
2. Rubber-sheeting, or nonlinear stretching, to align a map with calibration points, and
for edge joining of adjacent map sheets or databases, which may have slightly diﬀerent
coordinate systems.
3. Generalizing or reducing the number of points in a chain while preserving certain
error properties.
4. Topological cleanup so that edges that are supposed to meet at one vertex do so,
the boundary of each polygon is a closed sequence of vertices and polylines, adjacency
information is correct, and so on.
5. Choice of the correct data structure.
Should elevation data be represented in a
gridded form (as an array of elevations) or should a triangulated irregular network (TIN)
be used (the surface is partitioned into triangles)?
6. Zone of inﬂuence calculation:
For example, ﬁnd all the national monuments within
ten miles of the highway.
7. Overlaying:
Overlaying two maps to produce a third, where one polygon of the
overlay map will be those points that are all from the same two polygons of the two
input maps is one of the most complex operations in a GIS. If only the area or other
mass property of the overlay polygons is desired, then it is not necessary completely
to ﬁnd the overlay polygons ﬁrst; it is suﬃcient to ﬁnd the set of vertices and their
neighborhoods of each overlay polygon [FrEtal94].
8. Name placement:
Consider a cartographic map containing point features such as
cities, line features such as rivers, and area features such as states. The name placement
problem involves locating the features’ names so as to maximize readability and aesthetics
[FrAh84]. Eﬃcient solutions become more important as various mapping packages now
produce maps on demand. The techniques also extend to labelling CAD drawings, such
as piping layouts and wiring diagrams.
9. Viewsheds and visibility indices:
Consider a terrain database, and an observer and
target, both of which may be some distance above the terrain. The observer can see
the target if and only if a line between them does not intersect the terrain. Note that
if they are at diﬀerent heights above the terrain, then this relation is not necessarily
commutative.
The (not necessarily connected) polygon of possible targets visible by a particular ob-
server is his viewshed. The viewshed’s area is the observer’s visibility index. In order to
site observers optimally, the visibility index for each possible observer in the database

1010
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
may be required. Calculating this exactly for an n×n gridded database takes time O(n5)
so sampling techniques are used [FrRa94].
Implementations:
1. GRASS GIS, also known as GRASS (Geographic Resources Analysis Support Sys-
tem), is a free and open-source Geographic Information System (GIS) software suite
widely used used for geospatial data management and analysis, image processing, graph-
ics and maps production, spatial modeling, and visualization.
• https://grass.osgeo.org/
2. The Open Source Geospatial Foundation (OSGeo) was created to support the collab-
orative development of open-source geospatial software and to promote its widespread
use.
• http://www.osgeo.org/
13.8.12
GEOMETRIC CONSTRAINT SOLVING
Applications of geometric constraint solving include mechanical engineering, molecular
modeling, geometric theorem proving, and surveying.
Deﬁnition:
Geometric constraint solving is the problem of locating a set of geometric elements
given a set of constraints among them.
Fact:
1. The problem may be under-constrained, with an inﬁnite number of solutions, or
over-constrained, with no solutions without some relaxation.
Examples:
1. A receptor is a rigid cavity in a protein, which is the center of activity for some
reaction. A ligand is a small molecule that may bind at a receptor. The activity level of
a drug may depend on how the ligand ﬁts the receptor, which is made more complicated
by the protein molecule’s bending.
2. In CAD/CAM, there may be constraints such as the speciﬁcation that opposite sides
of a feature must be parallel. For example, commercial systems like Pro/Engineer allow
the user to freehand-sketch a part, and then apply constraints such as right angles, to
snap the drawing to ﬁt. Then the user is required to add more constraints until the part
is well constrained.
3. Molecular modeling:
There is often a lock-and-key relationship between a ﬂexible
protein molecule’s receptor and the ligand that it binds. In addition to geometrically
matching the ﬁtted shapes, the surface potentials of the molecules is also important. This
ﬁtting problem, called molecular docking, is important in computer-aided drug design.
Generally, a heuristic strategy is used to move the molecules to achieve no overlap between
the two molecules while maximizing their contact area [IePa95].

Section 13.8
APPLICATIONS OF GEOMETRY
1011
13.8.13
IMPLEMENTATIONS
One major application of geometry is in implementations of geometric software packages,
either as class libraries and subroutine packages callable from user programs, or as stand-
alone systems, which the user prepares input data ﬁles for and directs with either input
command ﬁles or a GUI. These implementations are useful both for creating or modeling
geometric objects and for visualizing them. This section describes some implementations
of general usefulness across several applications.
Deﬁnitions:
In an object-oriented computer language, a class library is a set of new data types
(classes) and operations on them, activated by sending a message to an object, or data
item.
(For example, a plane object may respond to a message to rotate itself.
The
internal representation of an object is private, and it may be accessed only by sending it
a message.)
In C++, class libraries often use template metaprogramming. Here, class templates
are expanded at compile time into code that often runs much more eﬃciently than
traditional subroutine libraries.
This trades oﬀcompilation time for execution time,
especially for complicated classes with eﬃcient specialized cases. For example, consider
a matrix that is known at compile time to be of size 3×3, which is common in geometric
applications. Here, specialized code is generated for any loop iterating over the rows of
the matrix, probably using loop unrolling. Even better, expressions with several matrix
operations chained together become code with no temporary matrix storage allocations,
and with all the code located together in one routine and available for global optimization.
Example libraries:
1. Boost.Geometry (also known as the Generic Geometry Library, GGL) is a part of
the collection of Boost C++ Libraries. It deﬁnes concepts, primitives, and algorithms
for solving geometry problems.
• http://www.boost.org/doc/libs/1 63 0/libs/geometry/doc/html/geometry/
introduction.html
Boost.Geometry contains a dimension-agnostic, coordinate-system-agnostic and scal-
able kernel, based on concepts, meta-functions, and tag dispatching. Supported algo-
rithms include area, length, perimeter, centroid, convex hull, intersection (clipping),
within (point in polygon), distance, envelope (bounding box), simplify, and transform.
Boost.Geometry can be used wherever geometry plays a role, such as in mapping and GIS,
game development, computer graphics and widgets, robotics, and astronomy. However,
the development has been mostly GIS-oriented.
2. Computational Geometry Algorithms Library (CGAL) is a software project that pro-
vides access to eﬃcient and reliable geometric algorithms in the form of a C++ library.
CGAL is used in various areas needing geometric computation, such as geographic infor-
mation systems, computer aided design, molecular biology, medical imaging, computer
graphics, and robotics.
The library oﬀers data structures and algorithms like triangulations, Voronoi diagrams,
Boolean operations on polygons and polyhedra, point set processing, arrangements of
curves, surface and volume mesh generation, geometry processing, alpha shapes, convex
hull algorithms, shape analysis, AABB and KD trees, among others.

1012
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
CGAL is a massive project that started in 1996 as a consortium of several existing
projects.
It was originally funded by the European Union’s information technologies
program ESPRIT.
• http://www.cgal.org/
3. Geogram is a C++ programming library of geometric algorithms, with eﬃcient graph-
ics for surfacic and volumetric meshes.
• http://homepages.loria.fr/BLevy/GEOGRAM/
Example stand-alone systems:
1. Wolfram|Alpha is an engine for carrying out dynamic computations, searching for
answers, and providing knowledge, based on the Mathematica system. Alpha can answer
questions such as “How many baseballs ﬁt in a Boeing 747?”, where the answer makes a
reasonable assumption about the packing density, and states it. More technical questions,
such as intersecting lines, computing properties of polytopes, and drawing fractals, are
also possible.
• https://www.wolframalpha.com/about.html
• https://www.wolfram.com/mathematica/
2. Blender is a free and open-source 3D creation suite. It supports modeling, rigging,
animation, simulation, rendering, compositing and motion tracking, video editing, and
game creation.
• https://www.blender.org/
3. Several very eﬃcient geometry and GIS programs for processing large datasets are
available at
• www.ecse.rpi.edu/Homepages/wrf/Software
Here is a sampling of the supported software:
• PinMesh is a very fast algorithm with the facility to preprocess a polyhedral mesh
in order to perform 3D point location queries [MaEtal16].
• TiledVS allows one to eﬃciently compute terrain viewsheds on large raster ter-
rains using small amounts of real memory with a custom virtual memory man-
ager [FeEtal16].
• UPLAN is an eﬃcient algorithm for path planning on road networks having polyg-
onal constraints [MaEtal15b].
• EMFlow and RWFlood eﬃciently compute hydrography (water ﬂow) on very large
(50000×50000) terrains containing basins [GoEtal15]; the novel component was
to raise the ocean and compute how it ﬂowed over sills to ﬂood interior basins.
• EPUG-Overlay allows one to compute the overlay of two GIS maps (plane graphs)
in parallel using rational numbers, with the ability to process maps with tens
of millions of vertices and hundreds of thousands of polygons in a few minutes
[MaEtal15a].
• NearptD is a very fast parallel nearest neighbor algorithm and implementation,
which has processed 107 points in E6 and 184 · 106 points in E3.

REFERENCES
1013
REFERENCES
Printed Resources:
[AgEtal96] P. K. Agarwal, B. Aronov, J. Pach, R. Pollack, and M. Sharir, “Quasi-planar
graphs have a linear number of edges”, in Graph Drawing ’95, Lecture Notes in
Computer Science 1027, Springer-Verlag, 1996, 1–7.
[AjEtal82] M. Ajtai, V. Chv´atal, M. M. Newborn, and E. Szemer´edi, “Crossing-free
subgraphs”, Annals of Discrete Mathematics 12 (1982), 9–12.
[AkAl89] J. Akiyama and N. Alon, “Disjoint simplices and geometric hypergraphs”, Com-
binatorial Mathematics, Annals of the New York Academy of Sciences 555 (1989),
1–3.
[AlKl92] N. Alon and D. Kleitman, “Piercing convex sets and the Hadwiger-Debrunner
(p, q)-problem”, Advances in Mathematics 96 (1992), 103–112.
[Al63] E. Altman, “On a problem of Erd˝os”, American Mathematical Monthly 70 (1963),
148–157.
[Ap94] Aperiodic 1994, International Conference on Aperiodic Crystals, Les Diablerets,
Switzerland, 1994.
[ArEtal91] B. Aronov, B. Chazelle, H. Edelsbrunner, L. Guibas, M. Sharir, and R.
Wenger, “Points and triangles in the plane and halving planes in space”, Discrete &
Computational Geometry 6 (1991), 435–442.
[AtGo93] M. J. Atallah and M. T. Goodrich, “Deterministic parallel computational ge-
ometry”, in Synthesis of Parallel Algorithms, J. H. Reif (ed.), Morgan Kaufmann,
1993, 497–536.
[BaKe92] A. Bachem and W. Kerns, Linear Programming Duality, An Introduction to
Oriented Matroids, Springer-Verlag, 1992.
[BaDe92] C. Bajaj and T. K. Dey, “Convex decomposition of polyhedra and robustness”,
SIAM Journal on Computing 21 (1992), 339-364.
[Ba91] K. Ball, “The plank problem for symmetric bodies”, Inventiones Mathematicae
104 (1991), 535–543.
[B´a82] I. B´ar´any, “A generalization of Carath´eodory’s theorem”, Discrete Mathematics
40 (1982), 141–152.
[B´aF¨uLo90] I. B´ar´any, Z. F¨uredi, and L. Lov´asz, “On the number of halving planes”,
Combinatorica 10 (1990), 175–183.
[B´aKaPa82] I. B´ar´any, M. Katchalski, and J. Pach, “Quantitative Helly-type theorems”,
Proceedings of the American Mathematical Society 86 (1982), 109–114.
[BaDoHu96] C. B. Barber, D. P. Dobkin, and H. T. Huhdanpaa, “The Quickhull algo-
rithm for convex hulls”, ACM Transactions on Mathematical Software 22 (1996),
469–483.
[Be83] J. Beck, “On the lattice property of the plane and some problems of Dirac, Motzkin
and Erd˝os in combinatorial geometry”, Combinatorica 3 (1983), 281–297.
[Be10] K. Bezdek, Classical Topics in Discrete Geometry, CMS Books in Mathematics,
Springer, 2010.

1014
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
[Be13] K. Bezdek, Lectures on Sphere Arrangements – the Discrete Geometric Side,
Fields Institute Monographs, Volume 32, Springer, 2013.
[BeKu90] A. Bezdek and W. Kuperberg, “Examples of space-tiling polyhedra related to
Hilbert’s Problem 18, Question 2”, in Topics in Combinatorics and Graph Theory,
R. Bodendiek and R. Henn (eds.), Physica-Verlag, 1990, 87–92.
[BeL´a15] K. Bezdek and Zs. L´angi, “Density bounds for outer parallel domains of unit ball
packings”, Proceedings of the Steklov Institute of Mathematics 288 (2015), 209–225.
[BjEtal93] A. Bj¨orner, M. Las Vergnas, B. Sturmfels, N. White, and G. Ziegler, Oriented
Matroids, Cambridge University Press, 1993. (A comprehensive monograph on the
theory of oriented matroids.)
[Bo93] J. Bokowski, “Oriented matroids”, in Handbook of Convex Geometry, Vol. A, P.
M. Gruber and J. M. Wills (eds.), North Holland, 1993, 555–602.
[BoSt89] J. Bokowski and B. Sturmfels, Computational Synthetic Geometry, Lecture
Notes in Mathematics 1355, Springer-Verlag, 1989.
[BoF¨u84] E. Boros and Z. F¨uredi, “The number of triangles covering the center of an
n-set”, Geometria Dedicata 17 (1984), 69–77.
[Br96] P. Brass, “Erd˝os distance problems in normed spaces”, Computational Geometry
6 (1996), 195–214.
[BuGrSl79] S. Burr, B. Gr¨unbaum, and N. Sloane, “The orchard problem”, Geometria
Dedicata 2 (1979), 397–424.
[CaEtal97] A. R. Calderbank, E. Rains, P. W. Shor, and N. J. A. Sloane, “Quantum error
correction and orthogonal geometry”, Physical Review Letters 78 (1997), 405–408.
[Ch84] B. Chazelle, “Convex partitions of polyhedra: a lower bound and worst-case op-
timal algorithm”, SIAM Journal on Computing 13 (1984), 488–507.
[Ch91] B. Chazelle, “Triangulating a simple polygon in linear time”, Discrete & Compu-
tational Geometry 6 (1991), 485–524.
[ChPa90] B. Chazelle and L. Palios, “Triangulating a nonconvex polytope”, Discrete &
Computational Geometry 5 (1990), 505–526.
[ChKuRa] D. Cherkashin, A. Kulikov, and A. Raigorodskii, “On the chromatic numbers
of small-dimensional Euclidean spaces”, arXiv:1512.03472.
[ChGoTa95] M. Chrobak, M. T. Goodrich, and R. Tamassia, “On the volume and reso-
lution of 3-dimensional convex graph drawing”, in Electronic Proceedings of the 5th
MSI-Stony Brook Workshop on Computational Geometry, 1995.
[ChGoTa96] M. Chrobak, M. T. Goodrich, and R. Tamassia, “Convex drawings of graphs
in two and three dimensions (preliminary version)”, Proceedings of the 12th Annual
Symposium on Computational Geometry, ACM, 1996, 319–328.
[ClEtal90] K. Clarkson, H. Edelsbrunner, L. Guibas, M. Sharir, and E. Welzl, “Com-
binatorial complexity bounds for arrangements of curves and spheres”, Discrete &
Computational Geometry 5 (1990), 99–160.
[CoEl03] H. Cohn and N. Elkies, “New upper bounds on sphere packings I”, Annals of
Mathematics 157 (2003), 689–714.
[CoEtal16] H. Cohn, A. Kumar, S. D. Miller, D. Radchenko, and M. Viazovska, “The
sphere packing problem in dimension 24”, preprint, 2016, arXiv:1603.06518.

REFERENCES
1015
[CoZh14] H. Cohn and Y. Zhao, “Sphere packing bounds via spherical codes,” Duke
Mathematics Journal 163 (2014), 1965–2002.
[CoSl93] J. H. Conway and N. J. A. Sloane, Sphere Packings, Lattices and Groups,
Springer-Verlag, 1993.
[CsSa93] J. Csima and E. Sawyer, “There exist 6n/13 ordinary points”, Discrete & Com-
putational Geometry 9 (1993), 187–202.
[Cs96] G. Csizmadia, “Furthest neighbors in space”, Discrete Mathematics 150 (1996),
81–88.
[Da95] K. Daniels,“Containment algorithms for nonconvex polygons with applications to
layout”, Ph.D. thesis, Harvard University, 1995.
[de93] M. de Berg, Ray Shooting, Depth Orders and Hidden Surface Removal, Springer-
Verlag, 1993.
[DeEd94] T. K. Dey and H. Edelsbrunner, “Counting triangle crossings and halving
planes”, Discrete & Computational Geometry 12 (1994), 281–289.
[Do94] S. E. Dorward, “A survey of object-space hidden surface removal”, International
Journal of Computational Geometry and Its Applications 4 (1994), 325–362.
[Ed87] H. Edelsbrunner, Algorithms in Combinatorial Geometry, Springer-Verlag, 1987.
(Monograph on combinatorial and algorithmic aspects of point conﬁgurations and
hyperplane arrangements.)
[Ed05] H. Edelsbrunner, “Smooth surfaces for multi-scale shape representation”, in Foun-
dations of Software Technology and Theoretical Computer Science, P. S. Thiagarajan
(ed.), Lecture Notes in Computer Science 1026, Springer, 1996, 391–412.
[EdHa91] H. Edelsbrunner and P. Hajnal, “A lower bound on the number of unit distances
between the points of a convex polygon”, Journal of Combinatorial Theory A 56
(1991), 312–316.
[EdSk89] H. Edelsbrunner and S. Skiena, “On the number of furthest neighbor pairs in
a point set”, American Mathematical Monthly 96 (1989), 614–618.
[El67] P. D. T. A. Elliott, “On the number of circles determined by n points”, Acta
Mathematica Academiae Scientiarum Hungaricae 18 (1967), 181–188.
[Er46] P. Erd˝os, “On sets of distances of n points”, American Mathematical Monthly 53
(1946), 248–250.
[Er60] P. Erd˝os, “On sets of distances of n points in Euclidean space”, Magyar Tu-
dom´anyos Akad ´mia K¨ozlem´enyei 5 (1960), 165–169.
[ErFiF¨u91] P. Erd˝os, P. Fishburn, and Z. F¨uredi, “Midpoints of diagonals of convex
n-gons”, SIAM Journal on Discrete Mathematics 4 (1991), 329–341.
[ErEtal93] P. Erd˝os, Z. F¨uredi, J. Pach, and Z. Ruzsa, “The grid revisited”, Discrete
Mathematics 111 (1993), 189–196.
[ErPa90] P. Erd˝os and J. Pach, “Variations on the theme of repeated distances”, Com-
binatorica 10 (1990), 261–269.
[ErSz35] P. Erd˝os and G. Szekeres, “A combinatorial problem in geometry”, Compositio
Mathematica 2 (1935), 463–470.
[FeKu93] G. Fejes T´oth and W. Kuperberg, “Packing and covering with convex sets”, in
Handbook of Convex Geometry, P. M. Gruber and J. M. Wills (eds.), North-Holland,
1993, 799–860.

1016
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
[FeGo17] S. Felsner and J. E. Goodman, Pseudoline arrangements, in C. D. T´oth, J.
O’Rourke, and J. E. Goodman (eds.), Handbook of Discrete and Computational
Geometry, 3rd ed., CRC Press, 2017, Chapter 5.
[FeEtal16] C. R. Ferreira, M. V. A. Andrade, S. V. G. Magalh˜aes, and W. R. Franklin,
“An eﬃcient external memory algorithm for terrain viewshed computation”, ACM
Transactions on Spatial Algorithms and Systems 2 (2016), article 6.
[Fo93] S. Fortune, “Progress in computational geometry”, in Directions in Geometric
Computing, R. Martin (ed.), Information Geometers Ltd., 1993, 81–128.
[Fo96] S. Fortune, “A beam tracing algorithm for prediction of indoor radio propagation”,
in Applied Computational Geometry Towards Geometric Engineering, M. C. Lin and
D. Manocha (eds.), Lecture Notes in Computer Science 1148, Springer, 1996, 37–40.
[FoGaKe93] R. Fourer, D. M. Gay, and B. W. Kernighan, AMPL: A Modeling Language
for Mathematical Programming, Duxbury Press/Wadsworth Publishing Co., 1993.
[FrR¨o86] P. Frankl and V. R¨odl, “All triangles are Ramsey”, Transactions of the American
Mathematical Society 297 (1986), 777–779.
[FrR¨o90] P. Frankl and V. R¨odl, “A partition property of simplices in Euclidean space”,
Journal of the American Mathematical Society 3 (1990), 1–7.
[FrRa94] W. R. Franklin and C. Ray, “Higher isn’t necessarily better: visibility algorithms
and experiments”, in Advances in GIS Research: 6th International Symposium on
Spatial Data Handling, T. C. Waugh and R. G. Healey (eds.), 1994, 751–770.
[FrEtal94] W. R. Franklin, V. Sivaswami, D. Sun, M. Kankanhalli, and C. Narayanaswa-
mi, “Calculating the area of overlaid polygons without constructing the overlay”,
Cartography and Geographic Information Systems 21 (1994), 81–89.
[FrAh84] H. Freeman and J. Ahn, “A system for automatic name placement”, 4th Jeru-
salem Conference on Information Technology (JCIT); Next Decade in Information
Technology, IEEE Computer Society Press, 1984, 134–143.
[F¨u90] Z. F¨uredi, “The maximum number of unit distances in a convex n-gon”, Journal
of Combinatorial Theory A 55 (1990), 316–320.
[Ga77] M. Gardner, “Mathematical recreations”, Scientiﬁc American 236 (Jan. 1977),
110–121.
[GiORSu96] C. Gitlin, J. O’Rourke, and V. Subramanian, “On reconstructing polyhedra
from parallel slices”, International Journal of Computational Geometry & Applica-
tions 6 (1996), 103–122.
[GoEtal15] T. L. Gomes, S. V. G. Magalh˜aes, M. V. A. Andrade, W. R. Franklin, and
G. C. Pena, “Eﬃciently computing the drainage network on massive terrains using
external memory ﬂooding process”, Geoinformatica 19 (2015), 671–692.
[GrRoSp90] R. Graham, B. Rothschild, and J. Spencer, Ramsey Theory, 2nd ed., Wiley,
1990.
[GrTa13] B. Green and T. Tao, “On sets deﬁning few ordinary lines”, Discrete & Com-
putational Geometry 50 (2013), 409–468.
[GrWi93] P. M. Gruber and J. M. Wills, eds., Handbook of Convex Geometry, Vols. A
and B, North Holland, 1993.
[Gr56] B. Gr¨unbaum, “A proof of V´azsonyi’s conjecture”, Bulletin of the Research Coun-
cil of Israel, Section A 6 (1956), 77–78.

REFERENCES
1017
[Gr67] B. Gr¨unbaum, Convex Polytopes, Wiley, 1967.
[Gr72] B. Gr¨unbaum, Arrangements and Spreads, CBMS Regional Conference Series in
Mathematics 10, American Mathematical Society, 1972.
[GrSh90] B. Gr¨unbaum and G. C. Shephard, Tilings and Patterns, Freeman, 1990.
[GuEtal95] P. Gupta, R. Janardan, J. Majhi, and T. Woo, “Eﬃcient geometric algorithms
for workpiece orientation in 4- and 5-axis NC-machining”, Electronic Proceedings of
the 5th MSI-Stony Brook Workshop on Computational Geometry, 1995.
[GuKa15] L. Guth and N. H. Katz “On the Erd˝os distinct distances problem in the
plane”, Annals of Mathematics 181 (2015), 155–190.
[Ha94] T. C. Hales, “The status of the Kepler conjecture”, The Mathematical Intelligencer
16 (1994), 47–58.
[Ha05] T. C. Hales, “A proof of the Kepler conjecture”, Annals of Mathematics 162
(2005), 1065–1185.
[Ha12] T. C. Hales, Dense Sphere Packings: A Blueprint for Formal Proofs, Cambridge
University Press, 2012.
[Ha65] S. Hansen, “A generalization of a theorem of Sylvester on lines determined by a
ﬁnite set”, Mathematica Scandinavica 16 (1965), 175–180.
[Ha80] S. Hansen, “On conﬁgurations in 3-space without elementary planes and on the
number of ordinary planes”, Mathematica Scandinavica 47 (1980), 181–194.
[Ha74] H. Harborth, “L¨osung zu problem 664A”, Elemente der Mathematik 29 (1974),
14–15.
[He23] E. Helly, “¨Uber Mengen konvexer K¨orper mit gemeinschaftlichen Punkten”, Jahres-
bericht der Deutschen Mathematiker-Vereinigung 32 (1923), 175–176.
[Ho96] C. M. Hoﬀmann, “How solid is solid modeling?” in Applied Computational Ge-
ometry Towards Geometric Engineering, M. C. Lin and D. Manocha (eds.), Lecture
Notes in Computer Science 1148, Springer, 1996, 1–8.
[HoPa34] H. Hopf and E. Pannwitz, “Aufgabe Nr. 167”, Jahresbericht der Deutschen
Mathematiker-Vereinigung 43 (1934), 114.
[IePa95] D. Ierardi and S. Park, “Rigid molecular docking by surface registration at
multiple resolutions”, Electronic Proceedings of the 5th MSI-Stony Brook Workshop
on Computational Geometry, 1995.
[J´a92] J. J´aJ´a, An Introduction to Parallel Algorithms, Addison-Wesley, 1992.
[Ja87] R. Jamison, “Direction trees”, Discrete & Computational Geometry 2 (1987),
249–254.
[JeTo95] T. R. Jensen and B. Toft, Graph Coloring Problems, Wiley-Interscience, 1995.
[KaLe78] G. A. Kabatiansky and V. I. Levenshtein, “On bounds for packings on a sphere
and in space” (in Russian), Problemy Peredachi Informacii 14 (1978), 3–25; English
translation in Problems of Information Transmission 14 (1978), 1–17.
[Ka84] G. Kalai, “Intersection patterns of convex sets”, Israel Journal of Mathematics
48 (1984), 161–174.
[KaEtal12] H. Kaplan, J. Matouˇsek, Z. Safernov´a, and M. Sharir, “Unit distances in three
dimensions”, Combinatorics, Probability and Computing 21 (2012), 597–610.

1018
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
[K´aPaT´o98] G. K´arolyi, J. Pach, and G. T´oth, “Ramsey-type results for geometric
graphs”, Discrete & Computational Geometry 20 (1998), 375–388.
[KaWa01] M. Kaufmann and D. Wagner, eds., Drawing Graphs: Methods and Models,
Lecture Notes in Computer Science 2025, Springer-Verlag, 2001.
[Ki83] D. Kirkpatrick, “Optimal search in planar subdivisions”, SIAM Journal on Com-
puting 12 (1983), 28–35.
[Kn92] D. E. Knuth,
Axioms and Hulls, Lecture Notes in Computer Science 606,
Springer-Verlag, 1992.
[Ko93] P. Komj´ath, “Set theoretic constructions in Euclidean spaces”, in New Trends in
Discrete and Computational Geometry, J. Pach (ed.), Springer-Verlag, 1993.
[Kr46] M. A. Krasnoselski˘ı, “Sur un crit`ere pour qu’un domain soit ´etoil´e”, (Russian,
with French summary), Matematicheski˘i Sbornik, N. S. 19 (1946), 309–310.
[Ku79] Y. Kupitz, “Extremal Problems in Combinatorial Geometry”, Aarhus University
Lecture Notes Series 53, Aarhus University, 1979.
[La01] B. Landgraf, “3D graph drawing”, in Drawing Graphs: Methods and Models, M.
Kaufmann and D. Wagner (eds.), Lecture Notes in Computer Science 2025, Springer-
Verlag, 2001, 172–192.
[Le03] F. T. Leighton, Complexity Issues in VLSI: Optimal Layouts for the Shuﬄe-
Exchange Graph and Other Networks, The MIT Press, 2003.
[LiMi95] Z. Li and V. Milenkovic, “Compaction and separation algorithms for noncon-
vex polygons and their applications”, European Journal of Operational Research 84
(1995), 539–561.
[LiMa96] M. C. Lin and D. Manocha, eds., Applied Computational Geometry Towards
Geometric Engineering, Lecture Notes in Computer Science 1148, Springer, 1996.
[Lo71] L. Lov´asz, “On the number of halving lines”, Annales Universitatis Scientarium
Budapest, E¨otv¨os, Sectio Mathematica 14 (1971), 107–108.
[MaEtal15a] S. V. G. Magalh˜aes, M. V. A. Andrade, W. R. Franklin, and W. Li, “Fast
exact parallel map overlay using a two-level uniform grid”, Proceedings of the 4th
International ACM SIGSPATIAL Workshop on Analytics for Big Geospatial Data,
ACM, 2015, 45–54.
[MaEtal15b] S. V. G. Magalh˜aes, M. V. A. Andrade, W. R. Franklin, and W. Li, “Fast
path planning under polygonal obstacle constraints”, 4th GIS-focused Algorithm
Competition, GISCUP, 2015, Winner (2nd place).
[MaEtal16] S. V. G. Magalh˜aes, M. V. A. Andrade, W. R. Franklin, and W. Li, “PinMesh
– Fast and exact 3D point location queries using a uniform grid”, Computers &
Graphics 58 (2016), 1–11.
[Ma93] J. Matouˇsek, “Geometric Range Searching”, Technical Report, FB Mathematik
und Informatik, Freie Universit¨at Berlin, 1993.
[Mc80] P. McMullen, “Convex bodies which tile space by translation”, Mathematika 27
(1980), 113–121.
[MeN¨a95] K. Mehlhorn and S. N¨aher, “LEDA: a platform for combinatorial and geometric
computing”, Communications of the ACM 38 (1995), 96–102.
[Mi96] J. S. B. Mitchell, “On some applications of computational geometry in manu-
facturing and virtual environments”, in Applied Computational Geometry Towards

REFERENCES
1019
Geometric Engineering, M. C. Lin and D. Manocha (eds.), Lecture Notes in Com-
puter Science 1148, Springer, 1996, 37–40.
[MiVa92] S. A. Mitchell and S. A. Vavasis, “Quality mesh generation in three dimensions”,
Proceedings of the 8th Annual Symposium on Computational Geometry, ACM, 1992,
212–221.
[Mu94] K. Mulmuley, Computational Geometry: An Introduction Through Randomized
Algorithms, Prentice Hall, 1994.
[Mu08] O. R. Musin, “The kissing number in four dimensions”, Annals of Mathematics
168 (2008), 1–32.
[OdSl79] A. M. Odlyzko and N. J. A. Sloane, “New bounds on the number of unit spheres
that can touch a unit sphere in n-dimensions”, Journal of Combinatorial Theory A
26 (1979), 210–214.
[OR87] J. O’Rourke, Art Gallery Theorems and Algorithms, Oxford University Press,
1987.
[OR93] J. O’Rourke, “Computational Geometry Column 18”, International Journal of
Computational Geometry and Its Applications 3 (1993), 107–113.
[OR94] J. O’Rourke, Computational Geometry in C, Cambridge University Press, 1994.
[Pa93] J. Pach, ed., New Trends in Discrete and Computational Geometry, Springer-
Verlag, 1993.
[PaAg95] J. Pach and P. K. Agarwal, Combinatorial Geometry, Wiley, 1995.
[PaShSz94] J. Pach, F. Shahrokhi, and M. Szegedy, “Applications of crossing numbers”,
10th ACM Symposium on Computational Geometry, 1994, 198–202.
[PaSh90] J. Pach and M. Sharir, “Repeated angles in the plane and related problems”,
Journal of Combinatorial Theory A 59 (1990), 12–22.
[PaSh98] J. Pach and M. Sharir, “On the number of incidences between points and
curves”, Combinatorics, Probability and Computing 7 (1998), 121–127.
[PaStSz92] J. Pach, W. Steiger, and M. Szemer´edi, “An upper bound on the number of
planar k-sets”, Discrete & Computational Geometry 7 (1992), 109–123.
[PaT¨o94] J. Pach and J. T¨or˝ocsik, “Some geometric applications of Dilworth’s theorem”,
Discrete & Computational Geometry 12 (1994), 1–7.
[PoWe90] R. Pollack and R. Wenger, “Necessary and suﬃcient conditions for hyperplane
transversals”, Combinatorica 10 (1990), 307–311.
[PrSh85] F. P. Preparata and M. I. Shamos, Computational Geometry: An Introduction,
Springer-Verlag, 1985.
[Ra95a] C. Radin, “Aperiodic tilings in higher dimensions”, Proceedings of the American
Mathematical Society 123 (1995), 3543–3548.
[Ra21] J. Radon, “Mengen konvexer K¨orper, die einen gemeinsamen Punkt enthalten”,
Mathematische Annalen 83 (1921), 113–115.
[Ra95b] V. T. Rajan, “Computational geometry problems in an integrated circuit design
and layout tool”, Electronic Proceedings of the 5th MSI-Stony Brook Workshop on
Computational Geometry, 1995.
[Ri96] J. Richter-Gebert, Realization Spaces of Polytopes, Lecture Notes in Mathematics
1643, Springer-Verlag, 1996.

1020
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
[RiZi97] J. Richter-Gebert and G. M. Ziegler, “Oriented matroids”, in Handbook of
Discrete and Computational Geometry, 3rd ed., C. D. T´oth, J. O’Rourke, and J. E.
Goodman (eds.), CRC Press, 2017, Chapter 6.
[RuSe92] J. Ruppert and R. Seidel, “On the diﬃculty of triangulating three-dimensional
nonconvex polyhedra”, Discrete & Computational Geometry 7 (1992), 227–253.
[Sc91] P. Schorn, “Implementing the XYZ GeoBench: a programming environment for
geometric algorithms”, in Computational Geometry – Methods, Algorithms and Ap-
plications, Lecture Notes in Computer Science 553, Springer-Verlag, 1991, 187–202.
[ScVa53] K. Sch¨utte and B. L. Van Der Waerden, “Das Problem der dreizehn Kugeln”,
Mathematische Annalen 125 (1953), 325–334.
[ShAg95] M. Sharir and P. K. Agarwal, Davenport-Schinzel Sequences and Their Geo-
metric Applications, Cambridge University Press, 1995.
[ShShSo16] M. Sharir, A. Sheﬀer, and N. Solomon, “Incidences with curves in Rd ”,
Electronic Journal of Combinatorics 23 (2016), #P4.16.
[Sh92] T. C. Shermer, “Recent results in art galleries”, Proceedings of the IEEE 80
(1992), 1384–1399.
[Sk96] V. Skowronski, “Synthesizing tolerances for optimal design using the Taguchi
quality loss function”, Ph.D. thesis, Rensselaer Polytechnic Institute, 1996.
[So94] A. Soifer, “Six-realizable set x6”, Geombinatorics 3 (1994), 140–145.
[So09] A. Soifer, The Mathematical Coloring Book, Springer, 2009.
[SpSzTr84] J. Spencer, E. Szemer´edi, and W. T. Trotter, “Unit distances in the Euclidean
plane”, in Graph Theory and Combinatorics, B. Bollob´as (ed.), Academic Press,
1984, 293–303.
[SrVo93] V. Srinivasan and H. B. Voelcker, eds., Proceedings of the 1993 International
Forum on Dimensional Tolerancing and Metrology, American Society of Mechanical
Engineers, Center for Research and Technology Development, and Council on Codes
and Standards, 1993.
[StBrEa95] P. Stucki, J. Bresenham, and R. Earnshaw, eds., “Rapid prototyping tech-
nology”, IEEE Computer Graphics and Applications 15 (1995), 17–55.
[Su17] A. Suk, “On the Erd˝os-Szekeres convex polygon problem”, Journal of the American
Mathematical Society, 2017.
[Sz97] L. A. Sz´ekely, “Crossing numbers and hard Erd˝os problems in discrete geometry”,
Combinatorics, Probability and Computing 7 (1997), 353–358.
[SzTr83] E. Szemer´edi and W. T. Trotter, “Extremal problems in discrete geometry”,
Combinatorica 3 (1983), 381–392.
[Ta14] R. Tamassia, ed., Handbook of Graph Drawing and Visualization, CRC Press,
2014.
[TaTo95] R. Tamassia and I. G. Tollis, eds., Graph Drawing, Lecture Notes in Computer
Science 894, Springer-Verlag, 1995.
[T´oORGo17] C. D. T´oth, J. O’Rourke, and J. E. Goodman, eds., Handbook of Discrete
and Computational Geometry, 3rd ed., CRC Press, 2017. (An extensive, comprehen-
sive reference source in discrete and computational geometry.)
[T´oVa98] G. T´oth and P. Valtr, “Note on the Erd˝os-Szekeres theorem”, Discrete & Com-
putational Geometry 19 (1998), 457–459.

REFERENCES
1021
[Tv66] H. Tverberg, “A generalization of Radon’s theorem”, Journal of the London Math-
ematical Society 41 (1966), 123–128.
[Un82] P. Ungar, “2N noncollinear points determine at least 2N directions”, Journal of
Combinatorial Theory A 33 (1982), 343–347.
[Va11] S. Vance, “Improved sphere packing lower bounds from Hurwitz lattices”, Ad-
vances in Mathematics 227 (2011), 2144–2156.
[Ve13] A. Venkatesh, “A note on sphere packings in high dimension,” International Math-
ematics Research Notices 7 (2013), 1628–1642.
[Vi16] M. S. Viazovska, “The sphere packing problem in dimension 8,” preprint, 2016,
arXiv:1603.04246.
[We91] Emo Weltz, “Smallest enclosing disks (balls and ellipsoids)”, in New Results and
New Trends in Computer Science, Lecture Notes in Computer Science 555, Springer-
Verlag, 1991, 359–370.
[WeHa11] N. Weste and D. Harris, CMOS VLSI Design: A Circuits and Systems Per-
spective, 4th ed., Pearson, 2011.
[Ya90] F. F. Yao, “Computational geometry”, in Handbook of Theoretical Computer
Science, Vol. A, J. van Leeuwen (ed.), The MIT Press, Elsevier, 1990, Chapter 7.
[Za13] J. Zahl, “An improved bound on the number of point-surface incidences in three
dimensions”, Contributions to Discrete Mathematics 8 (2013), 100–121.
[Zi94] G. M. Ziegler, Lectures on Polytopes, Springer-Verlag, 1994.
[Zi98] G. M. Ziegler, “Oriented matroids today”, Electronic Journal of Combinatorics,
Dynamic Survey DS#4, 1998.
[ˇZiVr92] R. ˇZivaljevi´c and S. Vre´cica, “The colored Tverberg’s problem and complexes
of injective functions”, Journal of Combinatorial Theory A 61 (1992), 309–318.
Web Resources:
http://cs.brown.edu/cgc/ (Center for Geometric Computing, Brown University.)
http://cs.brown.edu/stc/ (NSF Graphics and Visualization Center.)
http://dimacs.rutgers.edu/TechnicalReports/TechReports/1994/94-31.ps
(On
the Complexity of Some Basic Problems in Computational Convexity: II. Volume
and Mixed Volumes, DIMACS Technical Report 94-31.)
http://graphdrawing.org/index.html (Graph drawing resources.)
http://jeffe.cs.illinois.edu/compgeom/compgeom.html
(JeﬀErickson’s Compu-
tational Geometry pages.)
http://www.algorithmic-solutions.com/leda/index.htm (LEDA C++ algorithms
for geometric computations.)
http://www-cgrl.cs.mcgill.ca/%7Egodfried/teaching/cg-web.html
(Computa-
tional geometry resources.)
http://www.cs.cmu.edu/~quake/triangle.html
(Triangle: A Two-dimensional Qual-
ity Mesh Generator and Delaunay Triangulator.)
http://www.cs.mcgill.ca/~fukuda/soft/polyfaq/polyfaq.html (FAQs in polyhe-
dral computation.)

1022
Chapter 13
DISCRETE AND COMPUTATIONAL GEOMETRY
http://www3.cs.stonybrook.edu/~algorith/major section/1.6.shtml (The Stony
Brook Algorithm Repository, which includes Section 1.6 on Computational Geome-
try.)
http://www.geom.uiuc.edu/apps/quasitiler/about.html (QuasiTiler 3.0.)
http://www.ics.uci.edu/~eppstein/geom.html (David Eppstein’s Geometry in Ac-
tion pages, a collection of many applications of discrete and computational geometry.)
https://polymake.org/doku.php
(Polymake, open-source software for research in
polyhedral geometry.)
https://www.math.uci.edu/research/mathematical-visualization (Mathematical
visualization site.)

14
CODING THEORY
Alfred J. Menezes
Paul C. van Oorschot
David Joyner and
Tony Shaska
14.1 Communication Systems and Information Theory
14.1.1 Basic Concepts
14.1.2 Entropy
14.1.3 The Noiseless Coding Theorem
14.1.4 Channels and Channel Capacity
14.2 Basics of Coding Theory
14.2.1 Fundamental Concepts
14.2.2 Maximum Likelihood Decoding
14.2.3 The Noisy Channel Coding Theorem
14.3 Linear Codes
14.3.1 Introduction
14.3.2 Syndrome Decoding
14.3.3 Constructing New Codes from Old
14.3.4 Hamming Codes
14.3.5 Reed-Muller Codes
14.3.6 Weight Enumerators
14.4 Cyclic Codes
14.4.1 Introduction
14.4.2 Interleaving
14.4.3 BCH Codes
14.4.4 Reed-Solomon Codes
14.4.5 Codes from Incidence Matrices of Graphs
14.5 Bounds for Codes
14.5.1 Constraints on Code Parameters
14.5.2 Perfect Codes
14.6 Nonlinear Codes
14.6.1 Nordstrom-Robinson Code
14.6.2 Preparata Codes
14.7 Convolutional Codes
14.7.1 Background
14.7.2 Shift Registers
14.7.3 Encoding
14.7.4 Turbo Codes

1024
Chapter 14
CODING THEORY
14.8 Quantum Error-Correcting Codes
14.8.1 Quantum Codes
14.8.2 Quantum Algebraic Geometry Codes
INTRODUCTION
This chapter deals with techniques for the eﬃcient and reliable transmission of data over
communications channels that may be subject to non-malicious errors. The general topic
areas related to these techniques are information theory and coding theory. Information
theory is concerned with the mathematical theory of communication, and includes the
study of redundancy and the underlying limits of communications channels. Coding the-
ory, in its broadest sense, deals with the translation between source data representations
and the corresponding representative symbols used to transmit source data over a com-
munications channel, or store this data. Error-correcting coding is the part of coding
theory that adds systematic redundancy to messages to allow transmission errors not
only to be detected, but also to be corrected.
GLOSSARY
AG code (algebraic geometry code): a code deﬁned in terms of the divisors of an
algebraic curve.
analog channel: a channel that is continuous in amplitude and time.
BCH code: a code arising from a special family of cyclic codes.
binary symmetric channel (BSC): a memoryless channel with binary input and
output alphabets, and ﬁxed probability p that a symbol is transmitted incorrectly.
burst error: a vector whose only nonzero entries are among a string of successive com-
ponents, the ﬁrst and last of which are nonzero.
capacity of a channel: a measure of the ability of a channel to transmit information
reliably.
check symbols: the positions in a codeword that provide redundancy.
code: a map from the set of words to the set of all ﬁnite strings of elements in a
designated alphabet.
codeword: a string produced when a code is applied to a word.
coding theory: the subject concerned with the translation between source data rep-
resentations and the corresponding representative symbols used to transmit source
data over a communications channel.
complete maximum likelihood decoding (CMLD): the decoding scheme that de-
codes a received n-tuple to the unique codeword of minimum distance from this
n-tuple, if such a codeword exists. Otherwise, the scheme arbitrarily decodes the
n-tuple to one of the codewords closest to this n-tuple.
convolutional code: a code in which the encoder has memory, so that an n-tuple
produced by the encoder not only depends on the message k-tuple u, but also on
some message k-tuples produced prior to u.
coset: the set C + x = {c + x | c ∈C} determined by a word x, given a code C.

GLOSSARY
1025
coset leader: a coset member of smallest Hamming weight.
cyclic code: a linear code in which every cyclic shift of a codeword is also a codeword.
data compression: the transformation of data into a representation which is more
compact yet maintains the information content of the original data.
dual code (of a code): the orthogonal complement of the code.
entropy: a measure of the amount of information provided by an observation of a
random variable.
equivalent codes: codes for which there is a ﬁxed permutation of the coordinate po-
sitions which transform one code to the other.
erasure: a transmission error whose position is known, but whose transmission symbol
for that position is not.
error-correction coding: coding that adds systematic redundancy to messages to al-
low transmission errors to be detected and corrected.
error-detection coding: coding that adds systematic redundancy to messages to allow
transmission errors to be detected (but not necessarily corrected).
extended code: the code obtained by adding a parity check symbol to each codeword
of a code.
generator matrix for a code: a matrix whose rows form a basis for that code.
generator polynomial: a monic polynomial of least degree in a cyclic code.
Golay code: a particular perfect code.
Hamming code: a perfect single-error correcting code.
Hamming distance between two n-tuples: the number of coordinate positions in
which they diﬀer.
Hamming distance of a code: the smallest Hamming distance over all pairs of dis-
tinct codewords in that code.
Hamming weight of an n-tuple: the number of nonzero coordinates.
incomplete maximum likelihood decoding (IMLD): the decoding scheme that de-
codes a received n-tuple to a unique codeword such that the distance between the
n-tuple and the codeword is minimum if such a codeword exists. If no such codeword
exists, then the scheme reports that errors have been detected, but no correction is
possible.
information symbols: the positions within a codeword that provide information on
the message sent.
information theory: the mathematical theory of communication concerned with both
the study of redundancy and the underlying limits of communication channels.
linear code: a subspace of the set of n-tuples with entries from a ﬁnite ﬁeld.
list decoding: a decoding algorithm which outputs a list of possible codewords “near”
the received word, one of which is correct.
low-density parity check code: a linear code whose parity check matrix contains
relatively few nonzero elements.
memoryless source: a source for which the probability of a particular word being
emitted at any point in time is ﬁxed.

1026
Chapter 14
CODING THEORY
message: a ﬁnite string of source words.
minimum error probability decoding (MED): the decoding scheme that decodes
a received n-tuple r to a codeword c for which the conditional probability Pr(c is
sent | r is received), c ∈C, is largest.
nonlinear code: a code which lacks the structure of a vector space over a ﬁnite ﬁeld.
Nordstrom-Robinson code: a special nonlinear code.
parity check bit: a bit added to a bit string so that the total number of 1s in the
extended string is even.
parity check matrix (for a code): a generator matrix for the dual code of the code.
perfect code: a code of distance d for which every word is within distance t =
 d−1
2

of some codeword.
Preparata code: a code from an inﬁnite family of nonlinear codes that have eﬃcient
encoding and decoding algorithms.
punctured code: the code obtained by removing any column of a generator matrix of
a linear code.
qubit (quantum bit): the analog in quantum computation of a bit in classical com-
putation.
Reed-Muller code: a code from a particular family of linear codes.
Reed-Solomon code: a linear code from a special family of BCH codes.
self-dual code: a linear code that is equal to its dual code.
self-orthogonal code: a linear code that is contained in its dual code.
shortened code: the set of all codewords in a linear code which are 0 in a ﬁxed coor-
dinate position with that position deleted.
stabilizer code: a device used to help construct quantum codes from some classical
codes.
syndrome (of a word x): the vector xHT , where H is a parity check matrix for a linear
code C.
systematic code: a linear code that has a generator matrix of the form [Ik | A] (alter-
natively, of the form [A | Ik]).
turbo code: a special type of code built using convolutional codes and an interleaver
which permutes the original bits before sending them to the second encoder.
uniquely decodable code: a code for which every string of symbols is the image of
at most one message.

Section 14.1
COMMUNICATION SYSTEMS & INFORMATION THEORY
1027
14.1
COMMUNICATION SYSTEMS AND INFORMATION THEORY
14.1.1
BASIC CONCEPTS
Deﬁnitions:
A communication system, as illustrated in the following ﬁgure, is modeled as a data
source providing either continuous or discrete output, a source encoder transforming
source data into binary digits (bits), a channel encoder, and a channel.
Source
Source
Encoder
Encryption
Channel
Encoder
Modulator
Channel
Demodulator
Channel
Decoder
Decryption
Source
Decoder
Receiver
In many communication systems the channel is analog, that is, continuous in amplitude
and time, in which case a modulator/demodulator (modem) is required to transform
between analog channel data and discrete encoder/decoder data.
The source encoder, the aim of which is to minimize the number of bits required to
represent source data while still allowing subsequent reconstruction, typically includes
data compression to remove unnecessary redundancy.
The objective of the channel encoder is to maximize the rate at which information can
be reliably conveyed by the channel, in the presence of disruptive channel noise.
Coding theory is the study of the translation between source data representations and
the corresponding representative symbols (coded data) used to transmit source data over
a communication channel.
Error-correction coding, located in the channel encoder, adds systematic redundancy
to messages to allow transmission errors not only to be detected but also to be corrected.
14.1.2
ENTROPY
Deﬁnitions:
Information theory is concerned with a mathematical theory of communication and
includes the study of redundancy and the underlying limits of communication channels.
Let X be a random variable that takes on a ﬁnite set of values x1, x2, . . . , xn with
probability Pr(X = xi) = pi, where 0 ≤pi ≤1 for each i, 1 ≤i ≤n, and Pn
i=1 pi = 1.
Also, let Y be a random variable that takes on a ﬁnite set of values.

1028
Chapter 14
CODING THEORY
The entropy (or uncertainty) of X is deﬁned to be H(X) = −Pn
i=1 pi log2 pi, where
pi log2 pi = 0 if pi = 0.
The joint entropy of X and Y is deﬁned to be
H(X, Y ) = −P
x,y Pr(X= x, Y = y) log2 Pr(X= x, Y = y).
If X and Y are random variables, the conditional entropy of X given Y = y is
H(X | Y = y) = −P
x Pr(X= x | Y = y) log2 Pr(X= x | Y = y).
The conditional entropy of X given Y (or the equivocation of Y about X) is
H(X | Y ) = −P
y Pr(Y = y) H(X | Y = y). (The summation indices x and y range over
all values of X and Y , respectively.)
Facts:
1. Useful books that cover information theory include [Ha80], [HaHaJo97], [Mc77], and
[We98].
2. Information theory provides a theoretical basis for many results in error-correcting
codes and cryptography, and provides theoretical bounds useful as metrics for evaluating
conjectures in both areas.
3. The entropy of X is a measure of the amount of information provided by an obser-
vation of X.
4. The entropy of X is also useful for approximating the number of bits required to
encode the elements of X.
5. If X and Y are random variables, then
• 0 ≤H(X) ≤log2 n;
• H(X) = 0 if and only if pi = 1 for some i, and pj = 0 for all j ̸= i (that is, there
is no uncertainty of the result);
• H(X) = log2 n if and only if pi = 1
n for each i, 1 ≤i ≤n (that is, all outcomes
are equally likely);
• H(X, Y ) ≤H(X) + H(Y );
• H(X, Y ) = H(X) + H(Y ) if and only if X and Y are independent.
6. The quantity H(X | Y ) measures the amount of uncertainty remaining about X af-
ter Y has been observed.
7. If X and Y are random variables, then
• H(X | Y ) ≥0;
• H(X | X) = 0;
• H(X, Y ) = H(Y ) + H(X | Y );
• H(X | Y ) ≤H(X);
• H(X | Y ) = H(X) if and only if X and Y are independent.
Examples:
1. If X is the random variable on the set {x1, x2, x3, x4} with p1 = 0.4, p2 = 0.3,
p3 = 0.2, and p4 = 0.1, then the entropy of X is
H(X) = −(0.4 log2 0.4 + 0.3 log2 0.3 + 0.2 log2 0.2 + 0.1 log2 0.1) ≈1.84644.

Section 14.1
COMMUNICATION SYSTEMS & INFORMATION THEORY
1029
2. If X is the random variable on the same set {x1, x2, x3, x4} with p1 = p2 = p3 =
p4 = 1
4, then the entropy of X is
H(X) = −4( 1
4 log2
1
4) = 2.
This situation, with equal probability outcomes, achieves the maximum entropy log2 n =
log2 4 = 2.
14.1.3
THE NOISELESS CODING THEOREM
Deﬁnitions:
A source is a stream of words from a set W = {w1, w2, . . . , wM}.
Let Xi denote the ith word produced by a source. The source is said to be memoryless
if for each word wj ∈W, the probability Pr(Xi = wj) = pj is independent of i, that is,
the Xi are independent and identically distributed random variables.
The entropy of a memoryless source is H = −PM
j=1 pj log2 pj.
A code is a map f from W to A∗, the set of all ﬁnite strings of elements of A where A
is a ﬁnite set called the alphabet.
For each source word wj ∈W, the string f(wj) is a codeword.
The length of the codeword f(wj), denoted |f(wj)|, is the number of symbols in the
string.
A message is any ﬁnite string of source words. If m = v1v2 . . . vr is a message, then its
encoding is obtained by concatenation: f(m) = f(v1)f(v2) . . . f(vr).
The average length of a code f is PM
j=1 pj|f(wj)|.
A code is uniquely decodable if every string from A∗is the image of at most one
message.
A preﬁx code is a code such that there do not exist distinct words wi and wj such that
f(wi) is an initial segment, or preﬁx, of f(wj).
Facts:
1. Preﬁx codes are uniquely decodable.
2. Preﬁx codes have the advantage of being instantaneous. That is, they can be decoded
online without looking at future codewords.
3. Kraft’s inequality: A preﬁx code f : W →A∗with codeword length |f(wi)| = li for
i = 1, 2, . . ., M exists if and only if PM
j=1 n−lj ≤1, where n is the size of the alphabet A.
4. Macmillan’s inequality:
If a uniquely decodable code f : W →A∗with codeword
lengths l1, l2, . . . , lM exists, then PM
j=1 n−lj ≤1, where n is the size of the alphabet A.
5. A uniquely decodable code with prescribed word lengths exists if and only if a preﬁx
code with the same word lengths exists. As a result, attention can be restricted to preﬁx
codes.
6. Shannon’s noiseless coding theorem:
For a memoryless source of entropy H, any
uniquely decodable code for the source into an alphabet of size n must have average
length at least
H
log2 n. Moreover, there exists such a code having average length less than
1 +
H
log2 n.

1030
Chapter 14
CODING THEORY
7. For a memoryless source, a preﬁx code with smallest possible average length can be
constructed by the Huﬀman coding algorithm. (See §9.1.2.)
Examples:
1. The code that maps the letters A, B, C, D to 1, 01, 001, 0001, respectively, is a preﬁx
code on this set of four letters.
2. The code that maps the letters A, B, C, D to 11, 111, 11111, 111111, respectively,
is not a preﬁx code since the code for A forms the ﬁrst part of the code for B (and
for the codes for C and D as well). It is also not uniquely decodable since a bit string
can correspond to more than one string of the letters A, B, C, D. For example, 11111
corresponds to AB, BA, and C.
14.1.4
CHANNELS AND CHANNEL CAPACITY
Deﬁnitions:
A channel is a medium that accepts strings of symbols from a ﬁnite alphabet A =
{a1, . . . , an} and produces strings of symbols from a ﬁnite alphabet B = {b1, . . . , bm}.
Let Xi denote the ith input symbol and let Yi denote the ith output symbol. The channel
is said to be memoryless if the probability Pr(Yi = bj | Xi = ak) = pjk (for 1 ≤j ≤m
and 1 ≤k ≤n) is independent of i.
A binary symmetric channel (BSC) is a memoryless channel with input and output
alphabets {0, 1}, and probability p that a symbol is transmitted incorrectly. The prob-
ability p is called the symbol error probability of the channel. See the following ﬁgure.
1  p
1  p
1
Output
0
0
p
p
1
Input
A q-ary symmetric channel is a memoryless channel with input and output alphabets
each of size q and such that the probability that an error occurs on symbol transmission is
a constant p. Furthermore, if an error does occur then each of the q−1 symbols diﬀerent
from the correct symbol is equally likely to be received.
The capacity of a binary symmetric channel with symbol error probability p is C(p) =
1 + p log2 p + (1 −p) log2(1 −p).
Facts:
1. The capacity of a communications channel is a (unitless) measure of its ability to
transmit information reliably.
2. The capacity of a BSC with symbol error probability p is a monotone decreasing
function of p for 0 ≤p ≤1
2, with 1 ≥C(p) ≥0. Moreover, C(0) = 1 and C( 1
2) = 0.

Section 14.2
BASICS OF CODING THEORY
1031
Example:
1. The capacity of a BSC with symbol error probability 0.01 is given by
C(0.01) = 1 + 0.01 log2(0.01) + 0.99 log2(0.99) ≈0.92.
14.2
BASICS OF CODING THEORY
Coding theory is the subject devoted to the theory of error-correcting codes.
Error-
correcting codes were invented to correct errors over unreliable transmission links. With
digital communications and digital storage media ubiquitous in the modern world, error-
correcting codes have grown in importance.
Advances in error-correcting codes have
made it possible to transmit information across the solar system using weak transmitters
and to store data robustly on storage media so that it is resistant to damage, such as
scratches on a compact disk.
Error-correcting codes work by encoding data as strings of symbols, such as bit strings,
that contain redundant information that helps identify which codeword may have been
sent when a string of symbols, potentially diﬀerent from the string sent, is received.
Coding theory is an active research area, with new and better codes being devised at a
steady pace.
14.2.1
FUNDAMENTAL CONCEPTS
Deﬁnitions:
Let A be any ﬁnite set (called an alphabet), and let An denote the set of all n-tuples with
entries in A. A block code of length n containing M codewords over the alphabet A
is a subset of An of size M. Such a block code is called an (n, M)-code over A.
The Hamming distance d(x, y) between two n-tuples x and y ∈An is the number of
entries in which they diﬀer.
Let C be an (n, M)-code over A. The Hamming distance of C is the smallest Hamming
distance over all pairs of distinct codewords in C. If C has Hamming distance d, then C
is sometimes referred to as an (n, M, d)-code.
The information rate (or rate) of an (n, M)-code over an alphabet of size q is R =
logq M
n
.
Suppose that a codeword c from a block code is transmitted and r is received. The error
vector is e = r −c (formed by componentwise subtraction). The number of errors is
the number of nonzero components in e.
A code is said to detect t errors if the decoder is capable of detecting any pattern of t
or fewer errors per codeword that may be introduced by the channel.
A code is said to correct t errors if the decoder is capable of correcting any pattern of t
or fewer errors per codeword that may be introduced by the channel.
Suppose that a codeword c from a block code is transmitted and r is received.
An
erasure is a coordinate within r which is unknown.

1032
Chapter 14
CODING THEORY
A code is said to be t-erasure correcting if the decoder is capable of correcting any
pattern which contains at most t erasures.
A burst error of length t is a vector whose only nonzero entries are among a string of
successive entries, considered cyclically or non-cyclically, the ﬁrst and last of which are
nonzero.
A code is said to be t-cyclic burst error correcting if the decoder is capable of
correcting any pattern which is a cyclic burst of length t.
If C1 and C2 are two (n, M)-codes over an alphabet A, then C1 and C2 are said to
be equivalent codes if there is a ﬁxed permutation of the coordinate positions which
transform one code to the other.
The parity check bit of a bit string is 0 if there is an even number of bits in the string
and is 1 if there is an odd number of bits in the string.
Facts:
1. Richard Hamming wrote one of the pioneering papers of coding theory in 1948,
published in [Ha50].
2. Lester Hill, around the time of his paper [Hi27] from the 1920s, wrote an unpublished
paper entitled “The checking of the accuracy of transmittal of telegraphic communica-
tions by means of operations in ﬁnite algebraic ﬁelds” [ToChJo12]. In this paper, Hill
introduced a very special class of BCH codes for the purpose of error detection.
3. Some of the many introductory-level books in coding theory are [Ba97], [Bi05], [Gu-
RuSu15], [JoKi11], [Mo05], [Pr92], [Ro06], [Ro96], and [Ye08]. For more extensive treat-
ments, see [Be15], [Bl03], [JoZi15], [LiCo04], [Ma03], [NeRaSl06], [PeWe72], [PlHuBr98],
[TsVl91], [TsVlNo07], [vL99], and especially [HuPl03] and [MaSl77], which contain ex-
tensive bibliographies.
4. The Error Correcting Codes (ECC) home page provides free software implementing
several important error-correcting codes:
• http://www.eccpage.com
5. The main objective of coding theory is the design of codes such that
• an eﬃcient algorithm is known for encoding messages;
• an eﬃcient algorithm is known for decoding;
• the error-correcting capability of the code is high;
• the information rate of the code is high.
6. For applications in which a two-way communications channel is available (for exam-
ple, a telephone circuit), it is sometimes economical to use error detection and retrans-
mission upon error, in a so-called automatic repeat request (ARQ) strategy, rather than
so-called forward error correction (FEC) techniques capable of actually correcting errors
at the cost of more complex decoding equipment. This is not an option when the com-
munications channel is eﬀectively one-way or unperturbed source data is not available
for retransmission (for example in CD-ROM storage and deep-space communications
systems).
7. For any n-tuples x, y, z ∈An, the Hamming distance satisﬁes
• d(x, y) ≥0 with equality if and only if x = y;
• d(x, y) = d(y, x);

Section 14.2
BASICS OF CODING THEORY
1033
• d(x, y) + d(y, z) ≥d(x, z).
8. The information rate R of a block code measures the fraction of information of the
code which is non-redundant; the information rate R satisﬁes the inequality 0 < R ≤1.
9. When a word r is received, the decoder must make some decision. This decision may
be one of the following:
• no errors have occurred; accept r as a codeword;
• errors have occurred; correct r to a codeword c;
• errors have occurred; no correction is possible.
10. Let C be an (n, M, d)-code.
• If used only for error detection, C can detect d−1 errors.
• If used for error correction, C can correct
 d−1
2

errors.
• If used for erasure correction, C can correct d−1 erasures.
11. Let C be an (n, M, d)-code, and let s, t, u ≥0.
• If d ≥2s + t + 1, then C can simultaneously correct s errors and detect s + t
errors.
• If d ≥2s + t + 1, then C can simultaneously correct s errors and t erasures.
• If d ≥2s + t + u + 1, then C can simultaneously correct s errors and t erasures
and detect s + u errors.
12. Equivalent codes have the same distance, and hence the same error-correcting ca-
pabilities.
13. Adding a parity check bit to a bit string of length n produces a bit string of length
n + 1 with an even number of 0s.
14. Diﬀerent families of error-correcting codes have been, and continue to be, designed
to meet the particular requirements of applications.
One type of requirement is the
ability to correct speciﬁc types of errors. For many applications, it is suﬃcient to assume
that errors in diﬀerent positions occur independently of each other. This assumption
follows immediately when using a memoryless channel.
15. In other applications, errors may not be independent of each other. For example,
when signals are sent over radio channels, including those from deep space, interference
can produce errors in a run of bits.
Similarly, damage to storage media, such as a
compact disk, can produce errors that come in clusters. These types of errors may be
better modeled using burst errors and are better handled by cyclic codes (§14.4.1), in
particular Reed-Solomon codes (§14.4.4), interleaved Reed-Solomon codes (see [HuPl03]
for more information), and ﬁre codes (see [Bl03]).
Examples:
1. The code produced by adding a parity check bit to each bit string of length n can
detect a single error. (It detects an odd number of errors, but not an even number of
errors; no error correction is possible using this code.) For example, suppose the bit
string 0111 is received where the code word sent is a bit string of length three with a
parity check bit added. Since 0111 contains three 1s, it cannot be a codeword. Hence,
an error was made in transmission. This error cannot be corrected. To see this, note
that if exactly one bit error was made in the transmission, any of the codewords 0110,
0101, 0011, and 1111 could have been sent.

1034
Chapter 14
CODING THEORY
2. C = {0100011, 1010101, 1101111} is a (7, 3, 3)-code over the binary alphabet. Note
that d = 3 is the minimum Hamming distance between codewords (namely, the ﬁrst and
third codewords). The information rate of C is R = log2 3
7
≈0.226.
3. The binary repetition code of length n is the code C = {00 . . .0, 11 . . .1}. The code
has distance n, and so can correct
 n−1
2

errors. If used only for error detection, then C
can detect n−1 errors. Although the error-correcting capabilities of C are very good, its
information rate R = 1
n is very poor.
14.2.2
MAXIMUM LIKELIHOOD DECODING
Deﬁnitions:
Suppose C is an (n, M, d)-code. Diﬀerent decoding schemes can be used to recover a
codeword from a transmitted bit string received with possible errors. These schemes
include
• Minimum Error Probability Decoding (MED): If an n-tuple r is received,
then correct r to a codeword c for which the conditional probability Pr(c is sent
| r is received), c ∈C, is largest.
• Incomplete Maximum Likelihood Decoding (IMLD):
If an n-tuple r is
received, and there is a unique codeword c ∈C such that d(r, c) is a minimum,
then correct r to c.
If no such c exists, then report that errors have been
detected, but no correction is possible.
• Complete Maximum Likelihood Decoding (CMLD):
If an n-tuple r is
received, and there is a unique codeword c ∈C such that d(r, c) is a minimum,
then correct r to c. Otherwise, arbitrarily select one of the codewords c ∈C
that is the closest to r, and correct r to c.
Facts:
1. For any ﬁxed probability distribution of the source messages, the probability of a
decoding error, given that an n-tuple r is received, is minimized by MED among all
decoding schemes.
2. MED has the disadvantage that the decoding algorithm depends on the probability
distribution of the source messages. The decoding strategy that is used in practice is
CMLD.
3. Suppose that the probability that a symbol is transmitted incorrectly in a q-ary
symmetric channel is p, where 0 < p < q−1
q . Let r be a received word and c1, c2 ∈C with
d(c1, r) = d1 and d(c2, r) = d2. Let Pr(r | c) denote the probability that r is received,
given that c was sent. Then Pr(r | c1) ≤Pr(r | c2) if and only if d1 ≥d2.
4. CMLD chooses a codeword c for which the conditional probability Pr(r is received |
c is sent), c ∈C, is largest.
5. If all source messages are equally likely, then CMLD performs in exactly the same
way as MED.
Examples:
1. For the binary symmetric channel, Maximum Likelihood Decoding agrees with Near-
est Neighbor Decoding (where a received word v is corrected to the codeword c closest
to it in the Hamming distance).

Section 14.3
LINEAR CODES
1035
2. C = {010101, 101010, 111111} is a (6, 3, 3)-code over the binary alphabet. The code
is transmitted over a binary symmetric channel. The received word v = 011111 has
respective distances 2, 4, 1 to the three codewords of C and so is corrected to the codeword
c = 111111 using Maximum Likelihood Decoding.
14.2.3
THE NOISY CHANNEL CODING THEOREM
Deﬁnitions:
Let C be an (n, M)-code, where each word occurs with equal probability. Let ri be the
probability of making an incorrect decision using complete maximum likelihood decoding
given that the ith codeword was transmitted. The error probability of the code C is
PC =
1
M
PM
j=1 rj.
Let parameters n and M be ﬁxed. Deﬁne P ∗(n, M, p) to be the smallest error probabil-
ity PC of any (n, M)-code using a BSC with symbol error probability p.
Facts:
1. Shannon’s noisy channel coding theorem:
Let C(p) denote the capacity of a BSC
with symbol error probability p, and deﬁne the quantity Mn = 2⌊Rn⌋. If 0 < R < C(p),
then P ∗(n, Mn, p) →0 as n →∞.
2. By Shannon’s noisy channel coding theorem, arbitrarily reliable communication with
a ﬁxed information rate is possible on a channel provided that the information rate is
less than the channel capacity. Unfortunately, all known proofs of the theorem are non-
constructive and do not give bounds on word lengths. So the good codes promised by
the theorem may have extremely large word lengths.
14.3
LINEAR CODES
Linear codes are an important type of codes with a particular type of structure.
In
particular, a linear code is a code that is a subspace of a ﬁnite-dimensional vector space
over a ﬁnite ﬁeld. The main advantages of using linear codes arise from the eﬃcient
procedures for correcting errors. These procedures are based on matrix computations
that can be carried out easily and rapidly.
14.3.1
INTRODUCTION
Deﬁnitions:
Let GF(q)n denote the vector space of all n-tuples having components from the ﬁnite
ﬁeld GF(q) (§5.6.3). The elements of GF(q)n are called vectors or words.
An [n, k]-linear code C over GF(q) is a k-dimensional subspace of GF(q)n over GF(q).
More precisely, C is a linear block code, but the qualiﬁcation “block” is generally
omitted. The code C is referred to as an [n, k, d]-code, where n is the length of the
code, k is the dimension of the subspace, and d is the distance.

1036
Chapter 14
CODING THEORY
The Hamming weight of a word v ∈GF(q)n is the number of nonzero coordinates in
v.
Let C be an [n, k]-code over GF(q). A generator matrix G for C is a k × n matrix
with entries from GF(q) whose rows form a basis for C.
If an [n, k]-code C has a generator matrix of the form G = [Ik | A], then C is called a
systematic code, and the generator matrix G is said to be in standard form.
The ﬁrst k positions within a systematic [n, k]-code are referred to as the information
symbols for the code. The last n −k positions are referred to as the check symbols.
Let x = (x1, x2, . . . , xn) and y = (y1, y2, . . . , yn) be two vectors in GF(q)n. The inner
product of x and y is the ﬁeld element x ◦y = Pn
i=1 xiyi. If x ◦y = 0, x and y are
orthogonal.
Let C be an [n, k]-code over GF(q). The orthogonal complement of C, denoted C⊥
(read “C perp”), is the set of vectors orthogonal to every vector in C:
C⊥= {x ∈GF(q)n | x ◦y = 0 for all y ∈C}.
C⊥is usually called the dual code of C.
A parity check matrix for an [n, k]-code C is a generator matrix for C⊥.
A linear code C is self-orthogonal if C ⊆C⊥. It is self-dual if C = C⊥.
Facts:
1. Both [HuPl03] and [MaSl77] are excellent books on linear codes.
2. Square parentheses (used to denote an [n, k]-code or an [n, k, d]-code) denote that a
code is linear, while round brackets (used to denote an (n, M)-code or an (n, M, d)-code
as deﬁned in §14.2.1) are used for all codes, linear or not.
3. An [n, k]-code over GF(q), the ﬁnite ﬁeld of q elements, is an (n, qk)-block code.
4. The information rate of an [n, k]-code is R = k
n.
5. The distance of a linear code C is the minimum Hamming weight of a nonzero vector
in C.
6. A linear code is often described by a generator matrix.
7. If G is a generator matrix for a code, then any matrix obtained from G by applying
a sequence of elementary row operations is also a generator matrix for that code.
8. Let C be an [n, k]-code over GF(q). Then there exists an equivalent systematic code
C′ with generator matrix [Ik | A], where Ik is the k × k identity matrix, and A is a
k × (n −k) matrix with entries from GF(q).
9. If G is a generator matrix for an [n, k]-code C, then C = {mG | m ∈GF(q)k}. The
source messages can be taken to be the elements of GF(q)k, and hence encoding is simply
multiplication by G. Systematic codes are advantageous because if G is in standard form
and c = mG is the codeword corresponding to a message m, then the ﬁrst k components
of c are identically m.
10. Within a systematic code, the information symbols provide information about the
message while the check symbols provide the redundancy which makes error correction
possible.
11. If C is an [n, k]-code over GF(q), then C⊥is an [n, n −k]-code over GF(q).
12. If C is an [n, k]-code over GF(q), then the dual code of C⊥is C itself.

Section 14.3
LINEAR CODES
1037
13. There are many important special types and families of linear codes, including
Hamming codes (§14.3.4), Golay codes (§14.5.2), Reed-Muller codes (§14.3.5), and cyclic
codes (§14.4.1). Among cyclic codes, BCH codes form an important class (§14.4.3) and
among BCH codes there is an important class of codes known as Reed-Solomon codes
(§14.4.4).
14. Reed-Muller codes were used by the Mariner 9 spacecraft on its mission to Mars.
A Golay code was used by the Voyager 2 on its mission to Jupiter and Saturn. A Reed-
Solomon code was used by the Voyager 2 on its mission to Uranus. (See §15.9 in [HuPl03]
for more details on these applications.)
15. Algorithm 1 uses linear algebra to construct a parity check matrix for a linear code
from a generator matrix.
Algorithm 1:
Constructing a parity check matrix H from a generator
matrix G.
G′ := the reduced row echelon form of G {use elementary row operations}
A := the k × (n −k) matrix obtained from G′ by deleting the (pivot) leading
columns of G′
H := the (n −k) × n matrix H obtained by placing, in order, the rows of −A in
the columns of H which correspond to the leading columns of G′, and
placing in the remaining n −k columns of H, in order, the columns of the
(n −k) × (n −k) identity matrix In−k
16. Parity check matrices:
Let C be an [n, k]-code over GF(q) with a generator ma-
trix G, and let H be a parity check matrix for C.
• A vector x ∈GF(q)n belongs to C if and only if xHT = 0; it follows that
GHT = 0.
• If G = [Ik | A] is a generator matrix for C, then H = [−AT | In−k] is a parity
check matrix for C.
• C has distance at least s if and only if every set of s −1 columns of H are
linearly independent over GF(q); in other words, the distance of C is equal to
the smallest number of columns of H that are linearly dependent over GF(q).
17. Let C be an [n, k]-code with generator matrix G. C is self-orthogonal if and only if
GGT = 0.
18. Let C be an [n, k]-code with generator matrix G. C is self-dual if and only if it is
self-orthogonal and k = n
2 (and hence n is even).
19. A low-density parity check matrix (LDPC) is a linear code whose parity check
matrix has relatively few nonzero elements.
Examples:
1. Let C be a binary [7, 4]-code with generator matrix
G =





0
0
1
0
1
0
1
1
1
0
0
1
0
1
0
0
1
0
0
1
1
1
1
1
0
1
1
1




.

1038
Chapter 14
CODING THEORY
Elementary row operations yield the reduced row echelon form of G:
G′ =





1
1
0
0
0
1
0
0
0
1
0
0
1
0
0
0
0
0
1
1
0
0
0
0
0
0
0
1




.
The leading columns of G′ are columns 1, 3, 5, and 7, giving
A =





1
0
1
0
0
1
0
0
1
0
0
0




.
Hence, the following parity check matrix is obtained
H =



1
1
0
0
0
0
0
0
0
0
1
0
0
0
1
0
1
0
1
1
0


.
2. The extended Hamming code of order 3 is a binary [8, 4, 4]-code with generator matrix
G =





1
0
0
0
1
1
0
1
0
1
0
0
1
0
1
1
0
0
1
0
0
1
1
1
0
0
0
1
1
1
1
0




.
The code is self-dual since GGT = 0.
14.3.2
SYNDROME DECODING
Syndrome decoding is a general decoding technique for linear codes that is useful if the
information rate of the code is high. Let C be an [n, k, d]-code over GF(q) with parity
check matrix H.
Deﬁnitions:
For any x ∈GF(q)n, the coset of C determined by x is the set C + x = {c + x | c ∈C}.
For any x ∈GF(q)n, the syndrome of x is the vector xHT .
A coset leader of a coset of C is one of the coset members of smallest weight.
Facts:
1. The coset determined by 0 is C.
2. For all x ∈GF(q)n, x ∈C + x.
3. For all x, y ∈GF(q)n, if y ∈C + x, then C + y = C + x, that is, each word in a coset
determines that coset.
4. The cosets of C partition GF(q)n into qn−k cosets, each of size qk.
5. A syndrome is a vector of length n −k.

Section 14.3
LINEAR CODES
1039
6. Two vectors x1 and x2 ∈GF(q)n are in the same coset of C if and only if they have
the same syndrome, that is, x1HT = x2HT .
7. A vector x ∈GF(q)n is a codeword if and only if its syndrome is 0.
8. Suppose that a codeword c is transmitted and r is received.
If e = r −c, then
rHT = eHT , which means that the error vector is in the same coset as the received
word. By maximum likelihood decoding, the decoder should choose a vector of smallest
weight in this coset as the error vector.
9. The fact that there is a one-to-one correspondence between syndromes and coset
leaders leads to syndrome decoding, a decoding algorithm for linear codes, which is
described as Algorithm 2.
Algorithm 2:
Syndrome decoding for linear codes.
precomputation: set up a one-to-one correspondence between coset leaders and
syndromes; let r be a received word and H the parity check matrix
compute the syndrome s = rHT of r
ﬁnd the coset leader e associated with s
correct r to r −e
Example:
1. Consider the binary [5, 2]-code C with generator matrix
G =
 
1
0
0
0
1
0
1
1
1
1
!
and parity check matrix
H =



0
1
1
0
0
0
1
0
1
0
1
1
0
0
1


.
The eight cosets of C are
{00000, 10001, 01111, 11110}
{10000, 00001, 11111, 01110}
{01000, 11001, 00111, 10110}
{00100, 10101, 01011, 11010}
{00010, 10011, 01101, 11100}
{11000, 01001, 10111, 00110}
{10100, 00101, 11011, 01010}
{01100, 11101, 00011, 10010}.
The following is a list of coset leaders and their syndromes:
coset leader
00000
10000
01000
00100
00010
11000
10100
01100
syndrome
000
001
111
100
010
110
101
011
If the word r = 01101 is received, compute the syndrome 01101 · HT = 010, which
corresponds to a coset leader e = 00010. Hence, r is corrected to r −e = 01111.
14.3.3
CONSTRUCTING NEW CODES FROM OLD
There are several methods for modifying a linear code to produce a new linear code.
Some of these methods are extending a code, puncturing a code, and shortening a code.

1040
Chapter 14
CODING THEORY
Deﬁnitions:
If C is a linear code of length n over the ﬁeld GF(q), then the extended code C of C
is C = {(c1, c2, . . . , cn, cn+1) | (c1, c2, . . . , cn) ∈C, Pn+1
i=1 ci = 0}. The symbol cn+1 is
called the overall parity check symbol.
If C is a linear code over GF(q), the code obtained by removing any column of a generator
matrix of C is called a punctured C, denoted C∗.
If C is a linear code of length n, a shortened code C′ of C is a linear code of length
n −1 which equals the set of all codewords in C having 0 in a ﬁxed coordinate position,
with that position deleted.
Facts:
1. If C is an [n, k, d]-code over GF(q) with generator matrix G and parity check ma-
trix H, then
• C is an [n + 1, k, d]-code over GF(q);
• if C is a binary code, then d =
(
d,
if d is even
d + 1,
if d is odd;
• a generator matrix for C is G, which is obtained by adding a column to G in such
a way that the sum of the elements of each row of G is 0;
• a parity check matrix for C is H, where H =








1
1
1
1
· · ·
1
0
H
0
...
0








.
2. Puncturing a code is the reverse process to extending a code.
3. If C is an [n, k, d]-code over GF(q), then C∗is a linear code over GF(q) of length
n −1, dimension k or k −1, and distance d or d −1.
4. If C is an [n, k, d]-code over GF(q), k ≥2, and C has at least one codeword for which
the deleted position has a nonzero entry, then C′ is an [n −1, k −1, d′]-code over GF(q),
with d′ ≥d.
14.3.4
HAMMING CODES
Deﬁnition:
A Hamming code of order r over Fq, denoted Hr(q), is an [n, k]-code where n = qr−1
q−1
and k = n −r, with a parity check matrix whose columns are nonzero and such that no
two columns are scalar multiples of each other.
Facts:
1. A decoding algorithm for Hamming codes is shown in Algorithm 3.
2. In the binary case (q = 2), the Hamming code Hr(2) has a parity check matrix whose
columns consist of all nonzero binary vectors of length r, each used exactly once.
3. Hr(q) has distance 3, and so is a 1-error correcting code.
4. Any two binary Hamming codes of order r are equivalent.

Section 14.3
LINEAR CODES
1041
5. Hr(q) is a perfect code (§14.5.2).
Algorithm 3:
Decoding algorithm for Hamming codes.
H := a parity check matrix for a Hamming code Hr(q)
r := a received word
compute the syndrome s = rHT of r
if s = 0 then accept r as the transmitted word
else
compare sT with the columns of H
if sT = αhi (where hi is the ith column of H) and α ∈Fq then
the error vector e is the vector with α in position i and 0s elsewhere
correct r to c = r −e
Example:
1. Consider H3(2), the binary Hamming code of order 3. The code has length n = 7
and dimension k = 4, and a parity check matrix is
H =



1
0
0
1
1
0
1
0
1
0
1
0
1
1
0
0
1
0
1
1
1


.
If the received word is r = 1011101, compute the syndrome s = 1011101 · HT = 001,
which is the third column of H. Hence e = 0010000, and correct r to 1001101.
14.3.5
REED-MULLER CODES
Deﬁnition:
A ﬁrst-order Reed-Muller code, denoted R(1, m), is the binary [2m, m + 1]-code
whose generator matrix is formed by adding a single row entirely of 1s to the m × 2m
submatrix whose columns consist of all binary m-tuples.
A kth-order Reed-Muller code, denoted R(k, m), is the binary [2m, s]-code where
s = Pk
i=0
 m
k

whose generator matrix consists of a single row entirely of 1s and the rows
of the submatrices B1, . . . , Bk which have the following form. B1 is the m × 2m matrix
whose columns are all distinct binary m-tuples and Bi for i = 2, . . . , k is the
 m
i

× 2m
matrix where each row is formed from an i-subset of the rows of B1 by making an entry
1 if and only if the corresponding entries in each member of the i-subset is 1.
Facts:
1. A ﬁrst-order Reed-Muller code R(1, m) has weight 2m−1.
2. In practice, the columns in the generator matrix of a ﬁrst-order Reed-Muller code
are listed according to a proper ordering. This may be seen as normal lexigraphic order
of 0-1 strings, but from right to left. Alternatively, it may be seen as the natural order of
the integers in little-endian binary representation. The matrix in Example 1 shows this
ordering.
3. First-order Reed-Muller codes can be decoded using Algorithm 4.

1042
Chapter 14
CODING THEORY
Algorithm 4:
Decoding algorithm for ﬁrst-order Reed-Muller codes.
H2m := a Hadamard matrix of order 2m in standard form
G := a generator matrix for the Reed-Muller code
r := (r0, . . . , r2m−1) a received word
compute the vector R := (R0, . . . , R2m−1) where Ri = (−1)ri
compute the vector ˆR := RH2m
let ˆR = ( ˆR0, . . . , ˆR2m−1); ﬁnd a component ˆRj with maximum magnitude
suppose j has the binary representation j = Pm
i=1 ji2i−1 with ji ∈{0, 1}
if ˆRj > 0 then decode r to Pm
i=1 ji⃗vi where ⃗vi is the ith row of G (discounting
the row of 1s)
if ˆRj ≤0 then decode r to ⃗1+Pm
i=1 ji⃗vi where ⃗vi is the ith row of G (discounting
the row of 1s)
4. In Algorithm 4, the computation of RH2m can be speeded up by using a Fast
Hadamard Transform.
5. The ﬁrst-order Reed-Muller code R(1, 5) was used to transmit photographs from the
Mariner 9 spacecraft. It had good error-correcting capabilities of correcting 7 errors and
detecting 8. However, it had a very low information rate and relatively few codewords.
6. The kth order Reed-Muller code may also be viewed as a subspace of the vector
space V of linear functionals f : GF(2)m →GF(2). By considering linear functionals as
polynomials in x1, . . . , xn, the set of the characteristic functionals χu = Qm
i=1(xi +1+ui)
for all u = (u1, . . . , um) forms a basis for V . Note that χu has the property that χu(u) =
1 but χu(v) = 0 for all v ̸= u.
The kth-order Reed-Muller code is the subspace of
functionals of total degree at most k.
A bijection can be explicitly deﬁned between
this representation and the ordinary representation by taking f 7→(c0, . . . , c2m−1) if
f = P2m−1
i=0
ciχui where ui is the m-tuple representing the binary value of i.
7. The kth-order Reed-Muller code can also be represented as the code whose generator
matrix is the incidence matrix between the points and the (m −k)-ﬂats of an aﬃne
geometry of dimension m over GF(2).
Examples:
1. Consider the ﬁrst-order Reed-Muller code R(1, 3).
This code has length n = 8,
dimension k = 4, and distance d = 4. Using the proper ordering, the generator matrix
for R(1, 3) is
G =





1
1
1
1
1
1
1
1
0
1
0
1
0
1
0
1
0
0
1
1
0
0
1
1
0
0
0
0
1
1
1
1




.
2. Suppose the vector r = (1, 0, 1, 1, 0, 1, 1, 0) is received. Then we can compute R =
(−1, 1, −1, −1, 1, −1, −1, 1). Multiplying R by the Hadamard matrix

Section 14.3
LINEAR CODES
1043
H8 =














1
1
1
1
1
1
1
1
1
−1
1
−1
1
−1
1
−1
1
1
−1
−1
1
1
−1
−1
1
−1
−1
1
1
−1
−1
1
1
1
1
1
−1
−1
−1
−1
1
−1
1
−1
−1
1
−1
1
1
1
−1
−1
−1
−1
1
1
1
−1
−1
1
−1
1
1
−1














gives us the vector ˆR = (−2, −2, 2, 2, −2, −2, 2, −6). The component ˆR7 = −6 has largest
magnitude. Since 7 = 1+(1)21+1(22), and ˆR7 < 0, we decode r to c = (1, 0, 0, 1, 0, 1, 1, 0).
3. The second-order Reed-Muller code R(2, 3) would have n = 8, k = 7, and d = 2. It
would have generator matrix
G =












1
1
1
1
1
1
1
1
0
1
0
1
0
1
0
1
0
0
1
1
0
0
1
1
0
0
0
0
1
1
1
1
0
0
0
1
0
0
0
1
0
0
0
0
0
1
0
1
0
0
0
0
0
0
1
1












.
14.3.6
WEIGHT ENUMERATORS
Deﬁnitions:
Let C be an (n, M)-code and let Ai be the number of codewords of weight i in C, for
i = 0, 1, . . ., n. The vector (A0, A1, . . . , An) is called the weight distribution of C.
Let C be an [n, k]-code over GF(q) with weight distribution (A0, A1, . . . , An).
The
weight enumerator of C is deﬁned to be the polynomial WC(z) = Pn
i=0 Aizi.
Facts:
1. Let C be an [n, k]-code over GF(q), and let the symbol error probability on the q-ary
symmetric channel be p. If C is used only for error detection, then the probability of an
error going undetected is Pn
i=0 Ai
 p
q−1
i(1 −p)n−i.
2. MacWilliams’ identity:
Let C be an [n, k]-code over GF(q) with dual code C⊥.
Then
WC⊥(z) =
1
qk [1 + (q −1)z]n WC
 1−z
1+(q−1)z

.
Examples:
1. The weight distribution of a binary Hamming code of length n satisﬁes the recurrence
A0 = 1, A1 = 0,
(i + 1)Ai+1 + Ai + (n −i + 1)Ai−1 =
 n
i

,
i ≥1.
2. The weight enumerator of the Golay code (§14.5.2) is
1 + 253z7 + 506z8 + 1288z11 + 1288z12 + 506z15 + 253z16 + z23.

1044
Chapter 14
CODING THEORY
14.4
CYCLIC CODES
Cyclic codes are an important type of linear code which have an additional algebraic
structure imposed upon them. They are not only subspaces of ﬁnite-dimensional vector
spaces, but ideals within a special type of polynomial ring. This additional structure
enables the use of shifting and polynomial multiplication in addition to ordinary vector
addition. One of the main advantages of cyclic codes is their ability to correct errors
which occur in bursts.
14.4.1
INTRODUCTION
Deﬁnitions:
A linear code C of length n is cyclic if whenever (a0, a1, a2, . . . , an−1) is a codeword
in C, then the cyclic shift (an−1, a0, a1, . . . , an−2) is also a codeword in C.
Let g(x) be a polynomial in GF(q)[x]/(xn −1). The ideal generated by g(x), namely
{a(x)g(x) | a(x) ∈GF(q)[x]/(xn −1)}, is called the code generated by g(x) and is
denoted ⟨g(x)⟩.
Let C be a nonzero cyclic code in GF(q)[x]/(xn −1). A monic polynomial g(x) of least
degree in C is called a generator polynomial of C.
The polynomial h(x) = xn −1
g(x)
is called the check polynomial of C.
Let H be a parity check matrix for a cyclic code. If r is a received word, the syndrome
polynomial of r is the polynomial s(x) corresponding to the syndrome s = rHT .
Facts:
1. The study of cyclic codes is facilitated by the attachment of some additional algebraic
structure to the vector space GF(q)n.
2. If the vector (a0, a1, a2, . . . , an−1) in GF(q)n is identiﬁed with the polynomial a0 +
a1x + a2x2 + · · · + an−1xn−1, then
• the ring GF(q)[x]/(xn −1) can be viewed as a vector space over GF(q);
• the vector spaces GF(q)n and GF(q)[x]/(xn −1) are isomorphic;
• multiplication of a polynomial in GF(q)[x]/(xn −1) by x corresponds to a cyclic
shift of the corresponding vector;
• a linear code C in the vector space GF(q)n is cyclic if and only if C is an ideal in
the ring GF(q)[x]/(xn −1).
3. An ideal may contain many elements which will generate the ideal. One of these
generators is singled out as the generator.
4. If g(x) is a generator polynomial of a cyclic code C, then g(x) generates C; that is,
⟨g(x)⟩= C.
5. The following are consequences of the fact that the ring GF(q)[x]/(xn −1) is a
principal ideal domain (i.e., each ideal is generated by a monic polynomial, called the
generator polynomial).
Here C is a nonzero cyclic code in GF(q)[x]/(xn −1) with
generator polynomial g(x).

Section 14.4
CYCLIC CODES
1045
• The generator polynomial of C is unique.
• g(x) divides xn −1 in GF(q)[x].
• If the degree of g(x) is n−k, that is, g(x) = g0 +g1x+g2x2 +· · ·+gn−kxn−k (and
gn−k = 1), then a basis for C is {g(x), xg(x), x2g(x), . . . , xk−1g(x)}; hence C
has dimension k and a generator matrix for C is








g0
g1
g2
· · ·
· · ·
gn−k
0
0
· · ·
0
0
g0
g1
· · ·
· · ·
gn−k−1
gn−k
0
· · ·
0
0
0
g0
· · ·
· · ·
gn−k−2
gn−k−1
gn−k
· · ·
0
...
...
...
...
...
...
...
0
0
· · ·
0
g0
· · ·
· · ·
gn−k








.
6. Any c(x) ∈C can be written uniquely as c(x) = f(x)g(x) in the ring GF(q)[x], where
f(x) ∈GF(q)[x] has degree less than k. Hence, encoding a message polynomial f(x)
consists simply of polynomial multiplication by g(x).
7. The dual code C⊥is also cyclic.
8. Let h(x) = h0 + h1x + h2x2 + · · · + hkxk = xn −1
g(x)
be in GF(q)[x].
Then the
reciprocal polynomial h∗(x) = xkh( 1
x) of h(x) is a generator of C⊥. (In fact, ( 1
h0 )h∗(x)
is the generator polynomial of C⊥.) Hence, a parity check matrix for C is








hk
hk−1
hk−2
· · ·
· · ·
h0
0
0
· · ·
0
0
hk
hk−1
· · ·
· · ·
h1
h0
0
· · ·
0
0
0
hk
· · ·
· · ·
h2
h1
h0
· · ·
0
...
...
...
...
...
...
0
0
· · ·
0
hk
· · ·
· · ·
h0








.
9. A cyclic code of length n over GF(q) is characterized by its generator polynomial.
10. There is a one-to-one correspondence between cyclic codes in GF(q)n and monic
polynomials in GF(q)[x] which divide xn −1.
11. The following table gives the complete factorization of xn −1 over GF(2) for some
small values of odd n, 1 ≤n ≤31.

1046
Chapter 14
CODING THEORY
n
factorization of xn −1 over GF(2)
1
1 + x
3
(1 + x)(1 + x + x2)
5
(1 + x)(1 + x + x2 + x3 + x4)
7
(1 + x)(1 + x + x3)(1 + x2 + x3)
9
(1 + x)(1 + x + x2)(1 + x3 + x6)
11
(1 + x)(1 + x + x2 + · · · + x10)
13
(1 + x)(1 + x + x2 + · · · + x12)
15
(1 + x)(1 + x + x2)(1 + x + x2 + x3 + x4)(1 + x + x4)(1 + x3 + x4)
17
(1 + x)(1 + x + x2 + x4 + x6 + x7 + x8)(1 + x3 + x4 + x5 + x8)
19
(1 + x)(1 + x + x2 + · · · + x18)
21
(1 + x)(1 + x + x2)(1 + x2 + x3)(1 + x + x3)(1 + x2 + x4 + x5 + x6)·
·(1 + x + x2 + x4 + x6)
23
(1 + x)(1 + x + x5 + x6 + x7 + x9 + x11)(1 + x2 + x4 + x5 + x6 + x10 + x11)
25
(1 + x)(1 + x + x2 + x3 + x4)(1 + x5 + x10 + x15 + x20)
27
(1 + x)(1 + x + x2)(1 + x3 + x6)(1 + x9 + x18)
29
(1 + x)(1 + x + x2 + · · · + x28)
31
(1 + x)(1 + x2 + x5)(1 + x3 + x5)(1 + x + x2 + x3 + x5)(1 + x + x2 + x4 + x5)·
·(1 + x + x3 + x4 + x5)(1 + x2 + x3 + x4 + x5)
12. If C is an [n, k]-cyclic code generated by g(x), then another parity check matrix for
C is the matrix H whose ith column is xi mod g(x), for i = 0, 1, . . . , n −1.
13. If r(x) is the polynomial corresponding to the received word r, then the syndrome
polynomial of r is simply s(x) = r(x) mod g(x).
14. A cyclic code can correct cyclic burst errors of length t if and only if all cyclic burst
errors of length t or less have diﬀerent syndrome polynomials. Algorithm 1 shows how
burst errors can be corrected. In the algorithm, g(x) is the generator polynomial for an
[n, k]-cyclic code which can correct burst errors of length t.
Algorithm 1:
Decoding algorithm using burst error correction in cyclic
codes.
suppose a codeword c is transmitted and r is received
for i = 0 to n −1
compute the syndrome polynomial si(x) = xir(x) mod g(x)
if si has a (non-cyclic) burst of length t or less then
compute the error polynomial e(x) = xn−isi(x)
correct r(x) to c(x) = r(x) −e(x)
break
received word cannot be decoded
Examples:
1. Over GF(2), the factorization of x7 −1 is x7 −1 = (1 + x)(1 + x + x3)(1 + x2 + x3).
The monic divisors of x7 −1 are

Section 14.4
CYCLIC CODES
1047
g1(x) = 1
g2(x) = 1 + x
g3(x) = 1 + x + x3
g4(x) = 1 + x2 + x3
g5(x) = (1 + x)(1 + x + x3) = 1 + x2 + x3 + x4
g6(x) = (1 + x)(1 + x2 + x3) = 1 + x + x2 + x4
g7(x) = (1 + x + x3)(1 + x2 + x3) = 1 + x + x2 + x3 + x4 + x5 + x6
g8(x) = 1 + x7
The polynomial g5(x) generates the binary [7, 3]-cyclic code
C = {0000000, 1011100, 0101110, 0010111, 1001011, 1100101, 1110010, 0111001}.
A generator matrix for C is
G =



1
0
1
1
1
0
0
0
1
0
1
1
1
0
0
0
1
0
1
1
1


.
A parity check matrix for C is
H =





1
1
0
1
0
0
0
0
1
1
0
1
0
0
0
0
1
1
0
1
0
0
0
0
1
1
0
1




.
2. The cyclic code in Example 1 has distance d = 4 and so can correct 1 error, but is
also a 2-cyclic burst error correcting code. Suppose the the word r = 1110100 is received.
Running Algorithm 1 yields the results
i
si(x)
si
0
x + x3
0101
1
1 + x3
1001
2
1 + x + x2 + x3
1111
3
1 + x
1100
The syndrome is a (non-cyclic) burst of length 2.
The error polynomial is e(x) =
x7−3s3(x) = x4 + x5. The received word r is decoded to the codeword c = r −e =
1110100 −0000110 = 1110010.

1048
Chapter 14
CODING THEORY
14.4.2
INTERLEAVING
Deﬁnitions:
Let C be an [n, k]-code and consider s codewords ci = (ci,1, ci,2, . . . , ci,n) ∈C. Place
these codewords in a matrix as follows.






c1,1
c1,2
· · ·
c1,n
c2,1
c2,2
· · ·
c2,n
...
...
...
...
cs,1
cs,2
· · ·
cs,n






.
The word (c1,1, c2,1, . . . , cs,1, c1,2, c2,2, . . . , cs,2, . . . , c1,n, c2,n, . . . , cs,n) obtained by expand-
ing the matrix entry-by-entry column-wise is said to be made by interleaving the s
codewords together.
The [ns, ks]-code C∗obtained by interleaving all possible s-tuples of codewords from C
is said to have been created by interleaving C to a depth of s.
Suppose instead of interleaving the codewords, we use the entries in each column of the
above matrix as information symbols within an [m, s]-code ˆC. The result would be a
sequence of codewords from ˆC. This process is referred to as cross-interleaving C and
ˆC.
Facts:
1. If an [n, k]-code C has distance d, then the code C∗obtained by interleaving C to a
depth of s has distance ds.
2. If an [n, k]-code C can correct cyclic burst errors of length t, then the code C∗
obtained by interleaving C to a depth of s can correct cyclic bursts of length st.
3. Suppose G is a generator matrix for an [n, k]-code C and let C∗be the code obtained
by interleaving C to a depth of s. A generator matrix G∗for C∗can be constructed by
interleaving each of the k rows of G separately with s −1 copies of the zero codeword in
each of the s possible ways. The resulting ks vectors would form the rows of G∗.
4. If C is a cyclic code with generator polynomial g(x), then C∗, the code obtained
interleaving C to a depth of s, is also cyclic and its generator polynomial is g(xs).
5. In practice, after cross-interleaving C and ˆC, the resulting codewords in ˆC are often
interleaved themselves to some speciﬁed depth.
Examples:
1. Let C be the binary [7, 4]-code with generator matrix
G =





1
0
0
0
1
1
1
0
1
0
0
1
0
1
0
0
1
0
0
1
1
0
0
0
1
1
1
0




.
Interleaving the three codewords 1000111, 1010100, and 0111000 would produce the
interleaved codeword 110 001 011 001 110 100 100.

Section 14.4
CYCLIC CODES
1049
2. Suppose that we wish to cross-interleave the three codewords from Example 1 with
the binary [6, 3]-code with generator matrix
G =



1
0
0
1
1
1
0
1
0
0
1
1
0
0
1
1
1
0


.
Using each group of three symbols as information symbols, we get the seven new code-
words 110100, 001110, 011101, 001110, 110100, 100111, and 100111. In practice, we can
now interleave these codewords to a desired depth.
14.4.3
BCH CODES
Deﬁnitions:
Let β be a primitive nth root of unity in an extension ﬁeld of GF(q). Let g(x) be the
least common multiple of the minimal polynomials over GF(q) of βa, βa+1, . . . , βa+δ−2
where a is an integer. The cyclic code of length n over GF(q) with generator polynomial
g(x) is called a BCH code (after its discoverers: R. C. Bose, D. Ray-Chaudhuri, and
A. Hocquenghem) with designed distance δ.
If a = 1 in the deﬁnition of a BCH code, the code is called narrow-sense. If n = qm −1
for some positive integer m (that is, β is primitive in GF(qm)), the code is primitive.
Facts:
1. BCH codes are special types of cyclic codes, discovered by A. Hocquenghem in 1959
and independently by R. C. Bose and D. K. Ray-Chaudhuri in 1960.
2. BCH bound:
Let C be a BCH code over GF(q) with designed distance δ. Then C
has distance at least δ.
3. Algorithm 2 is one method for decoding BCH codes. In the algorithm, let g(x) be a
generator polynomial for a BCH code over GF(q) of designed distance δ and length n.
Hence g(x) = lcm{mi(x) | a ≤i ≤a + δ −2}, where mi(x) is the minimal polynomial of
βi over GF(q), and β is a primitive nth root of unity in an extension ﬁeld GF(qm).
Algorithm 2:
Decoding algorithm for BCH codes.
suppose a codeword c is transmitted and r is received
compute Sj = r(βa+j) for j = 0, 1, . . ., δ −2 and form the polynomial S(z) =
Pδ−2
j=0 Sjzj
use the extended Euclidean algorithm to calculate the greatest common divisor
of S(z) and zδ−1 in the ring GF(qm)[z]; stop as soon as the remainder ri(z)
has degree < δ−1
2 ; this yields polynomials si(z) and ti(z) such that
si(z)zδ−1 + ti(z)S(z) = ri(z); σ(z) := ti(z); w(z) := ri(z)
ﬁnd B, the set of roots of σ(z) in GF(qm)
for each γ ∈B, set Eγ = −γ−1w(γ)
σ′(γ)
, where σ′(z) denotes the formal derivative
of σ(z)
the error vector is e = (e0, e1, . . . , en−1), where ei =
(
0,
if β−i ̸∈B,
Eγ,
if β−i = γ ∈B
decode r to r −e

1050
Chapter 14
CODING THEORY
4. In Algorithm 2 the roots comprising B will lie in the subgroup of GF(qm)∗generated
by β.
5. If it is assumed that the number of errors is l ≤⌊δ−1
2 ⌋, then the decoding in Algo-
rithm 2 is correct.
6. There are more eﬃcient ways of obtaining σ(z) and w(z) than by using the Eu-
clidean algorithm; for example, by using the Berlekamp-Massey algorithm (see §5.4.2 in
[HuPl03]).
7. There exists an analogue to Algorithm 2 for decoding BCH codes in which both errors
and erasures occur. See Chapter 5 of [HuPl03] for a description and references.
Examples:
1. Consider the ﬁnite ﬁeld GF(33) generated by a root α of the primitive polynomial
f(x) = 1 + 2x2 + x3 ∈GF(3)[x]. A table of powers of α is given in the following table.
i
αi
i
αi
i
αi
0
1
9
2 + 2α + 2α2
18
1 + α
1
α
10
1 + 2α + α2
19
α + α2
2
α2
11
2 + α
20
2 + 2α2
3
2 + α2
12
2α + α2
21
1 + 2α + 2α2
4
2 + 2α + α2
13
2
22
1 + α + α2
5
2 + 2α
14
2α
23
2 + α + 2α2
6
2α + 2α2
15
2α2
24
1 + 2α
7
1 + α2
16
1 + 2α2
25
α + 2α2
8
2 + α + α2
17
1 + α + 2α2
The element β = α2 is a primitive 13th root of unity in GF(33). If mi(x) denotes the
minimal polynomial of βi over GF(3), then
m0(x) = 2 + x
m1(x) = 2 + 2x + 2x2 + x3
m2(x) = 2 + 2x + x3
m4(x) = 2 + x + 2x2 + x3
m7(x) = 2 + x2 + x3.
Since m1(x) = m3(x), the polynomial
g(x) = lcm(m0(x), m1(x), m2(x), m3(x)) = m0(x)m1(x)m2(x)
has among its roots the elements β0, β1, β2, and β3. Hence g(x) is a generator polynomial
for a BCH code over F3 of designed distance δ = 5 and length n = 13.
2. Using the BCH code in Example 1, suppose that the decoder received the word
r = (220 021 110 2110). The following steps follow Algorithm 2 to decode r:
• Compute S0 = r(β0) = 1, S1 = r(β1) = α14, S2 = r(β2) = α23, and S3 = r(β3) =
α16. This gives S(z) = 1 + α14z + α23z2 + α16z3.

Section 14.4
CYCLIC CODES
1051
• Applying the extended Euclidean algorithm in GF(33)[z] to S(z) and z4 yields
i
si(z)
ti(z)
ri(z)
deg ri(z)
−1
1
0
z4
4
0
0
1
1 + α14z + α23z2 + α16z3
3
1
1
α17 + α23z
α17 + α16z + α13z2
2
2
α3 + α16z
α15 + α3z + α13z2
α15 + α16z
1
Stop, since deg(r2(z)) <
δ−1
2
= 2.
Hence, σ(z) = α15 + α3z + α13z2 and
w(z) = α15 + α16z.
• By trying all possibilities, obtain the set B = {β5, β9} of roots of σ(z).
• Compute Eβ5 = −β−5 w(β5)
σ′(β5) = 2 and Eβ9 = −β−9 w(β9)
σ′(β9) = 2.
• Hence, the error vector is e = (000 020 002 0000) and the word r is decoded to
(220 001 111 2110).
14.4.4
REED-SOLOMON CODES
Deﬁnition:
A Reed-Solomon (RS) code C is a primitive BCH code of length n = q−1 over GF(q).
If C has dimension k then we call C an [n, k]-RS code.
Facts:
1. Reed-Solomon codes are special types of BCH codes, and hence they have the same
encoding and decoding algorithms.
2. Reed-Solomon codes are important because, for a ﬁxed n and k, no linear code can
have greater distance.
3. Reed-Solomon codes are useful for correcting burst errors. (A binary burst of length b
is a bit string whose only nonzero entries are among b successive components, the ﬁrst
and last of which are nonzero.)
4. A Reed-Solomon code was used to encode the data transmissions from the Voyager 2
spacecraft during its encounter with Uranus in January, 1986.
5. If C is an [n, k]-RS code over GF(q) with designed distance δ, then the generator
polynomial for C has the form g(x) = (x −βa)(x −βa+1) . . . (x −βa+δ−2), where β is a
primitive element of GF(q).
6. If C is an [n, k]-RS code over GF(q) with designed distance δ, then the distance of
C is exactly δ = n −k + 1.
7. A generalized [n, k]-Reed-Solomon code consists of all the codewords having the form
(f(α1), . . . , f(αn)) for all polynomials f ∈GF(q)[x] of degree < k where α1, . . . , αn are
ﬁxed distinct elements from GF(q). Codewords are sometimes represented as polynomi-
als in GF(q)[x] of degree < k.
8. Generalized Reed-Solomon codes can be decoded by a curve-ﬁtting algorithm pro-
posed by Sudan. This algorithm gives a way of decoding generalized RS-codes beyond
the normal error-correction capabilities of an RS-code. It uses a method called list de-
coding, which produces a list of possible candidates to which the transmitted codeword
should correspond. An excellent introduction to list decoding can be found in [Gu04].

1052
Chapter 14
CODING THEORY
9. It is possible to associate codewords within an RS-code to their polynomial represen-
tatives in a generalized RS-code. Suppose c = (c1, . . . , cn) is a codeword in an RS-code
such that the cis are distinct. Setting αi = ci gives the association. A polynomial f
in the general representation can be converted to an n-tuple by evaluating it at the n
elements ci. An n-tuple (ˆc1, . . . , ˆcn) can be converted to a polynomial f of degree < k
by applying Lagrange interpolation given the relations ˆci = f(ci).
10. Error correction in compact disks (developed by Philips and Sony) uses a code
known as the Cross-Interleaved Reed-Solomon Code (CIRC). The CIRC code is obtained
by cross-interleaving two Reed-Solomon codes, one a [28, 24]-RS code and the other a
[32, 28]-RS code.
Examples:
1. Consider the ﬁnite ﬁeld GF(5) generated by β = 2. Then g(x) = (x −β)(x −β2) =
(x −2)(x −4) = x2 + 4x + 3 generates a [4, 2]-RS code over GF(5) with distance δ = 4.
2. The code in Example 1 may be considered as a generalized RS-code by considering
all polynomials of degree < 2 over GF(q) and using the ﬁxed elements (1, 2, 4, 3). Using
this representation, the codeword (1, 2, 4, 3) is associated with the polynomial x while
the codeword (0, 4, 2, 3) is associated with the polynomial 1 + 4x.
14.4.5
CODES FROM INCIDENCE MATRICES OF GRAPHS
Deﬁnition:
For any prime p, consider the row span C over GF(p) of the |V | × |E| incidence matrix
of a connected graph Γ = (V, E). This class of codes arising from incidence matrices
of graphs has been investigated by a number of researchers, for example, Dankelmann,
Key, and Rodrigues [DaKeRo13].
Recall an incidence structure D = (P, B, J), with point set P, block set B, and incidence
J, is a t-(v, k, λ) design if |P| = v, every block b ∈B is incident with precisely k
points, and every t distinct points are together incident with precisely λ blocks. The
code CF (D) of the design D over the ﬁnite ﬁeld F is the space spanned by the
incidence vectors of the blocks over F.
An edge cut of a connected graph Γ is a set S ⊂E such that Γ −S = (V, E −S) is
disconnected. The edge connectivity λ(Γ) is the minimum cardinality of an edge cut.
The minimum and maximum degrees of the vertices of Γ are denoted by δ(Γ) and ∆(Γ).
If λ(Γ) = δ(Γ) and, in addition, the only edge sets of cardinality λ(Γ) whose removal
disconnects Γ are the sets of edges incident with a vertex of degree δ(Γ), then Γ is called
super-λ.
Let Γ = (V, E) be a connected graph and let G be an incidence matrix for Γ. Denote by
Cp(G) the row-span of G over GF(p). Note that C2(G) is also known as the cut space
of Γ.
Facts:
1. We have dim(C2(G)) = |V | −1. For odd p, dim(Cp(G)) = |V | if Γ has a closed path
of odd length (i.e., if Γ is not bipartite), and dim(Cp(G)) = |V | −1 if Γ has no closed
path of odd length (i.e., if Γ is bipartite).
2. Let Γ = (V, E) be a connected graph and let G be a |V | × |E| incidence matrix for
Γ. Then

Section 14.5
BOUNDS FOR CODES
1053
• C2(G) is a [|E|, |V | −1, λ(Γ)]2-code;
• if Γ is super-λ, then C2(G) is a [|E|, |V | −1, δ(Γ)]2-code, and the minimum words
are the rows of G of weight δ(Γ).
3. Let Γ = (V, E) be a connected bipartite graph and let G be a |V | × |E| incidence
matrix for Γ. Then
1. Cp(G) is a [|E|, |V | −1, λ(Γ)]p-code;
2. if Γ is super-λ, then Cp(G) is a [|E|, |V |−1, δ(Γ)]p-code, and the minimum words
are the nonzero scalar multiples of the rows of G of weight δ(Γ).
Examples:
1. Let Γ = K5 denote the complete graph on ﬁve vertices and let G be an incidence
matrix for Γ. Then C2(G) is a [10, 4, 4]-code and its dual is a [10, 6, 3]-code.
2. Let Γ = K7 denote the complete graph on seven vertices and let G be an incidence
matrix for Γ. Then C2(G) is a [21, 6, 6]-code and its dual is a [21, 15, 3]-code.
14.5
BOUNDS FOR CODES
How many codewords can a code have if its codewords are n-tuples of elements of GF(q)
and it has distance d? Although this is a diﬃcult question for all but special sets of values
of n, q, and d, there are several diﬀerent useful bounds on M, the number of codewords
in the code. There are also special types of codes, called perfect codes, that achieve the
maximum number of codewords possible, given values of n, q, and d.
14.5.1
CONSTRAINTS ON CODE PARAMETERS
Deﬁnitions:
Let Aq(n, d) be the maximum M for which there exists an (n, M, d)-code over GF(q). A
code that achieves this bound is called optimal.
Let Vq(n, d) be the number of words in GF(q)n that have distance at most d from a ﬁxed
word.
An [n, k, d]-code for which k = n −d + 1 is called a maximum distance separable
(MDS code), or an [n, k]-MDS code.
Facts:
1. Little is known about Aq(n, d) except for some speciﬁc values of q, n, and d.
2. For all n ≥1, Aq(n, 1) = qn and Aq(n, n) = q.
3. For all n ≥2, Aq(n, d) ≤qAq(n −1, d).
4. If d is even, then A2(n, d) = A2(n −1, d −1).
5. Vq(n, d) = Pd
i=0
 n
i

(q −1)i.
6. Hamming bound (or sphere-packing bound): If t =
 d−1
2

, then Aq(n, d) ≤
qn
Vq(n,t).
7. Singleton bound:
Aq(n, d) ≤qn−d+1.
Hence, for any [n, k, d]-code over GF(q),
k ≤n −d + 1.

1054
Chapter 14
CODING THEORY
8. Manin result: There exists a continuous decreasing function αq : [0, 1] →[0, 1] such
that
• αq is strictly decreasing on [0, q−1
q ];
• αq(0) = 1;
• if q−1
q
≤x ≤1, then αq(x) = 0;
• The set Σq of all accumulation points of sequences (dC/nC, kC/nC), with C a
linear code over GF(q), is given by
Σq = {(δ, R) ∈[0, 1]2 | 0 ≤R ≤αq(δ)}.
The Singleton bound, for example, gives an upper estimate for αq(δ).
9. Gilbert-Varshamov bound:
• Aq(n, d) ≥
qn
Vq(n,d−1);
• if Vq(n−1, d−2) < qn−k, then there exists an [n, k, d]-linear code over GF(q); thus,
if k is the largest integer for which this inequality holds, then Aq(n, d) ≥qk.
10. Asymptotic Gilbert-Varshamov bound:
αq(x) ≥1 −x logq(q −1) −x logq(x) −(1 −x) logq(1 −x).
In other words, for each ﬁxed ǫ > 0, there exists an [n, k, d]-code C (which may depend
on ǫ) with
R(C) ≥1 −δ(C) logq(q −1) −δ(C) logq(δ(C)) −(1 −δ(C)) logq(1 −δ(C)) −ǫ.
11. For thirty years, the asymptotic version of the Gilbert-Varshamov bound was be-
lieved to be the best possible lower bound for good codes. In 1982, using some sophis-
ticated ideas from algebraic geometry, it was proved that the Gilbert-Varshamov bound
can be improved. A good survey of these results appears in [TsVl91], [TsVlNo07].
12. Let C be an [n, k]-MDS code. If G is a generator matrix for C, then any k columns
of G are linearly independent.
13. If C is an [n, k]-MDS code, then C⊥is also an MDS code.
14. Johnson bound: If d = 2t + 1, then
A2(n, d) ≤
2n
tP
i=0
 n
i

+
1
⌊
n
t+1⌋
 n
t
  n−t
t+1 −
 n−t
t+1
 .
This is an improvement of the Hamming bound (Fact 6) for binary codes.
Examples:
1. The [13, 10, 3] Hamming code over GF(3) meets the Singleton bound.
2. The Reed-Solomon codes are MDS codes.

Section 14.5
BOUNDS FOR CODES
1055
14.5.2
PERFECT CODES
Deﬁnitions:
An (n, M, d)-code over GF(q) is said to be perfect if it meets the Hamming bound, that
is, M =
qn
Vq(n,t), where t =
 d−1
2

.
The binary Golay code is a [23, 12, 7]-code over GF(2) with generator matrix G =
[I12 | A], where
A =























1
1
0
1
1
1
0
0
0
1
0
1
0
1
1
1
0
0
0
1
0
1
0
1
1
1
0
0
0
1
0
1
1
1
1
1
0
0
0
1
0
1
1
0
1
1
0
0
0
1
0
1
1
0
1
1
0
0
0
1
0
1
1
0
1
1
0
0
0
1
0
1
1
0
1
1
1
0
0
1
0
1
1
0
1
1
1
0
0
1
0
1
1
0
1
1
1
0
0
1
0
1
1
0
1
1
1
0
0
0
0
1
1
0
1
1
1
0
0
0
1
1
1
1
1
1
1
1
1
1
1
1























.
The ternary Golay code is an [11, 6, 5]-code over F3 = {0, 1, 2} with generator matrix
G = [I6 | B], where
B =









1
1
1
1
1
0
1
2
2
1
1
0
1
2
2
2
1
0
1
2
2
2
1
0
1
1
2
2
1
0









.
Facts:
1. A necessary condition for a code to be perfect is that d be odd.
2. The binary Golay code is a perfect code.
3. The extended binary Golay code is a [24, 12, 8]-code that is self-dual.
4. The ternary Golay code is a perfect code.
5. The set of all perfect codes over GF(q), determined in 1973 by A. Tiet¨av¨ainen, con-
sists of the following:
• the linear code consisting of all words in GF(q)n;
• the binary repetition codes of odd lengths;
• the Hamming codes and all codes with the same parameters as them;
• the binary Golay code and all codes equivalent to it;
• the ternary Golay code and all codes equivalent to it.
6. There do exist perfect codes with the same parameters as the Hamming codes, but
which are not equivalent to them.

1056
Chapter 14
CODING THEORY
Examples:
1. The extended binary Golay code is a [24, 12, 8]-code that is self-dual.
2. The binary Hamming [7, 4, 3] code is perfect.
14.6
NONLINEAR CODES
Although linear codes are studied and used extensively, there are several important types
of nonlinear codes. In particular, there are nonlinear codes with eﬃcient encoding and
decoding algorithms, as well as nonlinear codes that are important for theoretical reasons.
14.6.1
NORDSTROM-ROBINSON CODE
Deﬁnitions:
Permute the coordinates of the extended binary Golay code so that one of the weight 8
codewords is 1111111100 . . .0, and call this new code C′. For each of the 8-bit words
00000000, 10000001, 01000001, 00100001, 00010001, 00001001, 00000101, 00000011, there
are exactly 32 codewords in C′ that begin with that word. The extended Nordstrom-
Robinson code is the code whose codewords are obtained from these 256 words by
deleting the ﬁrst 8 coordinate positions. The Nordstrom-Robinson code is obtained
by puncturing the last digit of the extended Nordstrom-Robinson code.
Facts:
1. The extended Nordstrom-Robinson code is a binary (16, 256, 6)-nonlinear code.
2. The Nordstrom-Robinson code is a binary (15, 256, 5)-nonlinear code.
3. The Johnson bound (§14.5.1, Fact 14) yields A2(15, 5) ≤256, and hence it follows
that A2(15, 5) = 256. On the other hand, it has been proved that no linear code of
length 15 and distance 5 has more codewords than the binary 2-error correcting BCH
code, which has 128.
14.6.2
PREPARATA CODES
Deﬁnitions:
The Preparata codes are an inﬁnite family of nonlinear codes that have eﬃcient en-
coding and decoding algorithms. Let β be a primitive element of GF(2)m, and label
the elements of GF(2)m as αi = βi, 0 ≤i ≤2m −2, and α2m−1 = 0. For a subset
X ⊆GF(2)m, let χ(X) denote the characteristic vector of X; that is, χ(X) is a binary
vector of length 2m whose ith coordinate is 1 if αi ∈X and is 0 otherwise, for each
0 ≤i ≤2m −1.
If m ≥3 is odd, the extended Preparata code P(m) is the set of words of the form
(χ(X), χ(Y )), where X and Y are subsets of GF(2)m such that
• |X| and |Y | are even;
• P
x∈X x = P
y∈Y y;

Section 14.7
CONVOLUTIONAL CODES
1057
• P
x∈X x3 +
 P
x∈X x
3 = P
y∈Y y3.
The Preparata code P(m) is obtained from P(m) by puncturing the coordinate cor-
responding to the ﬁeld element 0 in the ﬁrst half of each codeword.
Facts:
1. If m ≥3 is odd, then P(m) is a binary nonlinear code with parameters n = 2m+1,
M = 22m+1−2m−2, d = 6.
2. If m ≥3 is odd, then P(m) is a binary nonlinear code with parameters n = 2m+1 −1,
M = 22m+1−2m−2, d = 5.
3. P(3) is the same as the Nordstrom-Robinson code.
4. The Preparata codes can be viewed as linear codes over Z4.
14.7
CONVOLUTIONAL CODES
Convolutional codes are a powerful class of error-correcting codes. They work diﬀerently
than block codes do. Instead of grouping message symbols into blocks for encoding, check
digits are interwoven within streams of information symbols. Convolutional codes can
be considered to have memory, since n symbols of information are encoded using these
n symbols and previous information symbols.
14.7.1
BACKGROUND
Deﬁnitions:
The ﬁgure in §14.1.1 can be used to distinguish two approaches to decoding. For a hard
decision decoder, the demodulator maps received coded data symbols into the set of
transmitted data symbols (for example, 0 and 1). In contrast, the demodulator of soft
decision decoders may pass extra information to the decoder (for example, three bits
of information for each received channel data symbol, indicating the degree of conﬁdence
in its being a 0 or a 1).
Facts:
1. Convolutional codes were introduced by P. Elias in 1955, and are widely used in
practice today.
2. Convolutional codes are used extensively in radio and satellite links and have been
used by NASA for deep-space missions since the late 1970s.
3. There are linear codes that diﬀer from block codes in that the codewords do not have
constant length.
4. Convolutional codes also diﬀer from block codes in that the n-tuple produced by an
encoder depends not only on the message k-tuple u, but also on some message k-tuples
produced prior to u; that is, the encoder has memory.
5. Soft decision decoding typically allows performance improvements.
6. Hard and soft decision techniques can be used in both block and convolutional codes,
although soft decision techniques can typically be used to greater advantage in convolu-
tional codes.

1058
Chapter 14
CODING THEORY
7. Theoretical results, particularly with respect to BCH codes, position block codes as
superior to convolutional codes.
8. The minimum distances of BCH codes are typically much larger than the correspond-
ing free distances (§14.7.3) of comparable convolutional codes.
9. Decoding techniques for block codes are generally applicable only to q-ary (or binary)
symmetric channels, which are an appropriate model for only a relatively small fraction
of channels that arise in practice.
10. Eﬃcient decoding of BCH codes requires hard-decision decoding, which suﬀers in-
formation loss relative to soft-decision strategies, precipitating a performance penalty.
The resulting performance of the BCH decoder is signiﬁcantly inferior to that for a
comparable convolutional code, despite the BCH codes being inherently more powerful.
Consequently, convolutional codes are used in a majority of practical applications, due
to their relative simplicity and performance, and the large number of communication
channels which beneﬁt from soft decoding techniques.
11. A recently developed class of codes, known as turbo codes, are built using convolu-
tional codes. The basic idea behind a turbo encoder is to combine two simple convolu-
tional encoders. Input to the encoder is a block of bits. The two constituent encoders
generate parity bits and the information bits are sent unchanged. The key innovation
is an interleaver, which permutes the original information bits before they are provided
as input to the second encoder. The permutation causes input sequences which produce
low-weight codewords for one encoder to generally produce high-weight codewords for
the other encoder. See [HeWi99] for information on turbo codes.
Example:
1. In the simplest version of soft decision decoding, known as the binary erasure channel
(and usually classiﬁed as a hard-decision technique), the demodulator output is one of
three values: 0, 1, or “erasure” (indicating that neither a 0 nor a 1 was clearly recognized).
14.7.2
SHIFT REGISTERS
Deﬁnitions:
An m-stage shift register is a hardware device that consists of m delay elements
(or ﬂip-ﬂops), each having one input and one output, and a clock which controls the
movement of data. During each unit of time, the following operations are performed:
• a new input bit and the contents of some of the delay elements are added modulo 2
to form the output bit;
• the content of each delay element (with the exception of the last delay element)
is shifted one position to the right;
• the new input bit is fed into the ﬁrst delay element.
The generator of an m-stage shift register is a polynomial g(x) = 1 + g1x+ g2x2 + · · ·+
gmxm ∈GF(2)[x], where gi = 1 if the contents of the ith delay element are involved in
the modulo 2 sum that produces the output, and gi = 0 otherwise.
Facts:
1. Assume that the initial contents of a shift register are all 0s. Suppose that a shift
register has generator g(x). Let the input stream u0, u1, u2, . . . be described by the formal

Section 14.7
CONVOLUTIONAL CODES
1059
power series u(x) = u0 + u1x + u2x2 + · · · over GF(2). (If the input stream is ﬁnite of
length t, let ui = 0 for i ≥t.) Similarly, let the output stream c0, c1, c2, . . . be described
by the formal power series c(x) = c0+c1x+c2x2+· · · over GF(2). Then c(x) = u(x)g(x).
2. In some applications, some of the delay elements are added modulo 2 to the input
element. This type of register is referred to as a linear feedback shift register (LFSR).
Examples:
1. Shift-example:
Suppose that the delay elements of the 4-stage shift register in the
following ﬁgure initially contain all 0s.
Input
Output
D1
D2
D3
D4
If the input stream to the register is 11011010 (from left to right), the updated contents
of the delay elements and the output bits are summarized in the following table:
time
input
D1
D2
D3
D4
output
0
–
0
0
0
0
–
1
1
1
0
0
0
1
2
1
1
1
0
0
0
3
0
0
1
1
0
1
4
1
1
0
1
1
1
5
1
1
1
0
1
1
6
0
0
1
1
0
0
7
1
1
0
1
1
1
8
0
0
1
0
1
0
2. The generator of the shift register in Example 1 is g(x) = 1 + x + x4.
14.7.3
ENCODING
Note: Throughout this subsection assume that the initial contents of a shift register are
all 0s.
Deﬁnitions:
An (n, 1, m)-convolutional code with generators g1(x), g2(x), . . . , gn(x) ∈GF(2)[x]
(m = max(deg gi(x))) contains all codewords of the form c(x) = (c1(x), c2(x), . . . , cn(x)),
where ci(x) = u(x)gi(x), and u(x) = u0 + u1x + u2x2 + · · · represents the input stream.
The system memory of the code is m.
A convolutional code is catastrophic if a ﬁnite number of channel errors can cause an
inﬁnite number of decoding errors.
The rate of an (n, k, m)-convolutional code is k
n.

1060
Chapter 14
CODING THEORY
The free distance dfree of a convolutional code is the minimum weight of all nonzero
output streams.
A Recursive Systematic Convolutional (RSC) code is a convolutional code which
allows some of the output bits to to be fed back into the input to make the code systematic
in the sense that the data bits within a codeword come before the redundancy bits.
Facts:
1. A convolutional code is linear.
2. Convolutional codes are not block codes since the codewords have inﬁnite length.
They are, however, similar to block codes, and in fact can be viewed as block codes over
certain inﬁnite ﬁelds.
3. An (n, 1, m)-convolutional code can be described by a single shift register with n
outputs, where ci(x) is the output of the single-output shift register with generator gi(x)
when u(x) is the input. In practice, c1(x), c2(x), . . . , cn(x) are interleaved to produce one
output stream.
4. Let C be an (n, 1, m)-convolutional code with generators g1(x), g2(x), . . . , gn(x). Let
G(x) = Pn
i=1 xi−1gi(xn).
If the message is u(x), then the corresponding interleaved
codeword is c(x) = G(x)u(xn).
5. The Viterbi algorithm is a maximum likelihood decoding algorithm for convolutional
codes [LiCo04]. For an algebraic treatment of convolutional codes, see [Pi88].
6. If gcd(g1(x), g2(x), . . . , gn(x)) = 1 in GF(2)[x], then C is not catastrophic.
7. An (n, k, m)-convolutional code can be described by k multi-output shift registers,
each of maximum length m. The message is divided into k streams, each stream being
the input to one of the k shift registers. There are n output streams, each formed using
some or all of the shift registers.
8. The free distance of a convolutional code is a measure of the error-correcting capa-
bility of the code, and is a concept analogous to the distance of a block code.
9. In contrast to block codes, there are few algebraic constructions known for convolu-
tional codes.
10. The convolutional codes used in practice are usually those found by a computer
search designed to maximize the free distance among all encoders with ﬁxed parameters n,
k, and m. The following table lists the best codes with a rate of 1
2 (n = 2, k = 1). The
polynomials g1(x) and g2(x) are represented by their coeﬃcients, listed from low order
to high order.
m
g1(x)
g2(x)
dfree
2
101
111
5
3
1101
1111
6
4
10011
11101
7
5
110101
101111
8
6
1011011
1111001
10
7
11100101
10011111
10
8
101110001
111101011
12
9
1001110111
1101100101
12
10
10011011101
11110110001
14
11
100011011101
101111010011
15
12
1000101011011
1111110110001
16

Section 14.7
CONVOLUTIONAL CODES
1061
11. While a convolutional code is constructed with a shift register, an RSC is constructed
with an LFSR.
12. An RSC code can be shown to be equivalent to a convolutional code. But because
they are systematic, data bits can be sent directly to the output.
This is generally
referred to as the data stream. The redundancy bits, meanwhile, are processed through
the LFSR. This is generally referred to as the parity stream.
Examples:
1. Consider the (2, 1, 3)-convolutional code with generators g1(x) = 1 + x3 and g2(x) =
1 + x + x3. The code can be described by the shift register of the following ﬁgure. The
message u(x) = 1 + x3 + x4, corresponding to the bit string 10011, gets encoded to
c(x) = (u(x)g1(x), u(x)g2(x)) = (1 + x4 + x6 + x7, 1 + x + x5 + x6 + x7), or in interleaved
form to c = 11 01 00 00 10 01 11 11 00 00 00 . . . .
Message
C1
C2
2. Suppose that the input stream contains an inﬁnite number of 1s, and the output
stream has only ﬁnitely many 1s. If the channel introduces errors precisely in the positions
of these 1s, then the resulting all-zero output stream will be decoded by the receiver to
m(x) = 0.
3. The following ﬁgure is a shift register encoder for a (3, 2, 3)-convolutional code C.
C1
C3
I1
I2
C2
If the input stream is u = 1011101101, it is ﬁrst divided using alternating bits into
two streams I1 = 11110 and I2 = 01011. The three output streams are c1 = 10010,
c2 = 00011 and c3 = 01010, and the interleaved output is c = 100 001 000 111 010.

1062
Chapter 14
CODING THEORY
14.7.4
TURBO CODES
Deﬁnitions:
A turbo code is the code formed by parallel concatenation of two RSC codes whose
output consists of the following three streams:
• the data stream;
• the parity stream from the ﬁrst RSC;
• the parity stream from the second RSC where the input data stream, within each
input block of a given length m, has been permuted.
Facts:
1. The permutation involved in the input stream for the second RSC code is usually
taken to be pseudorandom in practice. When this is the case, any nonzero input will
produce a parity stream with large Hamming weight in at least one of the two RSCs.
2. Turbo codes (along with LDPC) are the codes which are closest to achieving Shan-
non’s noiseless coding theorem.
3. If the input stream consists of blocks of length m and the parity streams output
blocks of length n1 and n2 respectively, then turbo codes have a rate of m/(m+n1 +n2).
4. In order to increase the rate of a turbo code, some of the elements in the output
stream may be deleted. This is known as puncturing the code. When decoding, these
symbols are reconstructed using a soft-decision decoder.
5. Turbo codes generally suﬀer from low rates as well as high encoding and decoding
complexity.
6. Turbo codes were initially used in deep-space applications, 3G telephony, and wireless
standards.
14.8
QUANTUM ERROR-CORRECTING CODES
14.8.1
QUANTUM CODES
Let q = pn, for some prime p.
Let H be the q-dimensional Hilbert space (i.e., a q-
dimensional complex linear space).
Deﬁnitions:
A quantum system is given by the tensor product space V := H⊗n = H⊗H⊗· · ·⊗H.
An element of V is called a quantum state and denoted by |ψ⟩and a quantum code Q
is a qk-dimensional subspace of V . A quantum state |ψ⟩∈Q of k particles is transmitted
over a noisy channel and we then receive a quantum state ¯
|ψ⟩∈V of n particles.
Let q = 2. A qubit is the short form for quantum bit and the analog of a bit in classical
computation. The possible states of a quantum bit are the states of the vector space C2.
The standard basis of this state space is denoted |0⟩=
 1
0

, |1⟩=
 0
1

. The general state

Section 14.8
QUANTUM ERROR-CORRECTING CODES
1063
of a qubit is a linear combination |ψ⟩= α |0⟩+ β |1⟩, with α, β ∈C and |α|2 + |β|2 = 1.
For q > 2 the analogue of qubits is qudits.
The Pauli group for H is deﬁned as
G = ⟨XiZj | i, j ∈Fq⟩,
where
X =








0
0
· · ·
0
1
1
0
· · ·
0
0
0
1
· · ·
0
0
...
...
...
...
...
0
0
· · ·
1
0








,
Z =






1
0
· · ·
0
0
ωp
· · ·
0
...
...
...
...
0
0
· · ·
ωp−1
p






.
The Pauli group on V of length n is the tensor product of the Pauli group on single
qubits:
Gn = ⟨U1 ⊗· · · ⊗Un | Ui ∈G for i = 1, . . . , n⟩.
As usual, we will call the elements of Gn operators in Gn.
Stabilizer codes are helpful devices that make possible construction of quantum codes
from classical codes. Let V denote the qubit state space and let Gn be the Pauli group
on n qubits. Let H ≤Gn and denote by VH the subspace of V ﬁxed by H:
VH = {v ∈V | gv = v, ∀g ∈Gn}.
Subsequently we assume that H is abelian and −I ̸∈H. The subspace VH is called the
stabilizer code C(H) of H.
Since it suﬃces to look at a set of generators, we can represent a stabilizer code in an
easier way. A generator matrix G of a stabilizer code is an l×2n matrix G(X|Z), where
the ﬁrst n components represent the X errors and the second n components represent
the Z errors.
For an operator E ∈Gn, we denote its adjoint by E† and for any subgroup H ≤Gn, its
normalizer will be denoted N(H); see [Sh08] for details.
The weight wt of an operator U1 ⊗· · · ⊗Un is the number of elements Ui that are not
equal to the identity.
Facts:
1. VH is non-trivial if and only if H is abelian and −I ̸∈H.
2. Let {Ei} be a set of operators in Gn such that E†
j Ek ̸∈N(H) \ H for all j and k.
Then {Ej} is a correctable set of errors for the code C(H). For a proof, see [AsKn01].
3. Let H be an abelian subgroup of Gn. If a state |ψ⟩is in the +1 eigenspace of a set
of generators {g1, . . . , gl} of H, it is an eigenstate of all elements in H.
4. The generator matrix deﬁnes an [[n, k, d]] quantum error-correcting code with k =
n −l.
5. Let E ∈Gn be an error operator. Then
• if E ∈H, then E is a codeword;
• if E ∈Gn\N(H), then the error is detectable and can be corrected with the most
likely error principle;

1064
Chapter 14
CODING THEORY
• if E ∈N(H)\H, then the error cannot be detected and therefore is not cor-
rectable.
6. Fact 5 allows us to introduce the distance of a stabilizer code. The distance d of a
quantum stabilizer code C is the minimum weight of all normalizer elements that are not
in the stabilizer:
d = min{wt(x) | x ∈N(H)\H}.
7. The stabilizer code of the normalizer N(H) is equal to the dual code C⊥s with respect
to the symplectic inner product ⟨, ⟩. Consequently
d = min{wt(x) | x ∈C⊥s\C}.
8. Quantum codes can be constructed from classical codes [AsKn01]. Speciﬁcally, let
C ⊂F 2n
q
be an (n + k)-dimensional subspace such that C⊥s ⊂C. Then there exists a
quantum code Q ⊂H⊗n of dimension qk and minimum distance d = dim C \ C⊥s.
14.8.2
QUANTUM ALGEBRAIC GEOMETRY CODES
Deﬁnitions:
Let F/Fq be an algebraic function ﬁeld in one variable. Let P1, . . . , Pn be places of degree
1 and let D = P1 +· · ·+Pn. Furthermore let G be a divisor with supp(G)∩supp(D) = ∅.
Then the Goppa code (respectively AG code) CL ⊆Fn
q is deﬁned by
CL(D, G) = {(f(P1), . . . , f(Pn)) | f ∈L(G)} ⊆Fn
q .
Deﬁne the following linear evaluation map
ϕ : L(G) →Fn
q
f 7→(f(P1), . . . , f(Pn)).
Facts:
1. The Goppa code is given by CL(D, G) = ϕ(L(G)).
2. The code CL(D, G) is a linear [n, k, d]-code with parameters
k = dim G −dim(G −D),
d ≥n −deg G =: ddes.
The parameter ddes is called the designed distance of the Goppa code. See [ElSh11],
[Sh08], [TsVl91], and [TsVlNo07] for details.
3. Assume deg G < n and let g be the genus of F/Fq. Then the following hold [St93].
• ϕ : L(G) →CL(D, G) is injective and CL(D, G) is an [n, k, d]-code with
k
=
dim G ≥deg G + 1 −g
d
≥
n −deg G.
• If in addition 2g −2 < deg G < n, then
k
=
deg G + 1 −g.

REFERENCES
1065
• If (f1, . . . , fk) is a basis of L(G), then a generator matrix for CL(D, G) is
M
=




f1(P1)
· · ·
f1(Pn)
...
...
fk(P1)
· · ·
fk(Pn)



.
4. Let D = P1+· · ·+Pn be a divisor, where the Pis are places of degree 1 of an algebraic
function ﬁeld F/Fq. Furthermore let G be a divisor with supp(G) ∩supp(D) = ∅and
deﬁne the code CΩ(D, G) by
CΩ(D, G) = {(resP1(ω), . . . , resPn(ω) | ω ∈ΩF (G −D)} ⊆Fn
q ,
where resPi(w) is the residue of w at Pi. In other words, if t ∈F is a Pi-prime element
and w ∈F has a Pi-adic expansion w = P∞
j=n ajtj with n ∈Z and aj ∈F, then
resPi(w) = a−1. Then the code CΩ(D, G), where D and G are as above, has the following
properties:
• CL(D, G)⊥= CΩ(D, G);
• CΩ(D, G) = a·CL(D, H) with H = D−G+(η) where η is a diﬀerential, vPi(η) =
−1 for i = 1, . . . , n, and a = (resP1(η), . . . , resPn(η);
• CL(D, G)⊥= a · CL(D, H).
5. One can use diﬀerentials with special properties to help construct a self-orthogonal
code [St93, Prop. VII.1.2]. Namely, let x and y be elements of F such that vPi(y) = 1,
vPi(x) = 0, and x(Pi) = 1 for i = 1, . . . , n. Then the diﬀerential η = x · dy
y satisﬁes
vPi(η) = −1 and resPi(η) = 1 for i = 1, . . . , n.
REFERENCES
Printed Resources:
[AsKn01] A. Ashikhmin and E. Knill, “Nonbinary quantum stabilizer codes”, IEEE
Transactions on Information Theory 47 (2001), 3065–3072.
[Ba97] J. Baylis, Error-Correcting Codes: A Mathematical Introduction, Chapman &
Hall, 1997.
[Be15] E. R. Berlekamp, Algebraic Coding Theory, revised ed., World Scientiﬁc Press,
2015.
[Bi05] J. Bierbrauer, Introduction to Coding Theory, Chapman & Hall/CRC Press, 2005.
[Bl03] R. E. Blahut, Algebraic Codes for Data Transmission, Cambridge University Press,
2003.
[DaKeRo13] P. Dankelmann, J. D. Key, and B. G. Rodrigues, “Codes from incidence
matrices of graphs”, Designs, Codes and Cryptography 68 (2013), 373–393.
[Eb13] W. Ebeling, Lattices and Codes, 3rd ed., Springer Spektrum, 2013.
[ElSh11] A. Elezi and T. Shaska, “Quantum codes from superelliptic curves”, Albanian
Journal of Mathematics 5 (2011), 175–191.
[ElSh15] A. Elezi and T. Shaska, “Weight distributions, zeta functions and Riemann
hypothesis for linear and algebraic geometry codes”, in Advances on Superelliptic
Curves and Their Applications, L. Beshaj, T. Shaska, and E. Zhupa (eds.), IOS
Press, 2015, 328–359.

1066
Chapter 14
CODING THEORY
[Gu04] V. Guruswami, List Decoding of Error-Correcting Codes, Lecture Notes in Com-
puter Science 3282, Springer, 2004.
[GuRuSu15] V. Guruswami, A. Rudra, and M. Sudan, Essential Coding Theory, available
at http://www.cse.buffalo.edu/faculty/atri/courses/coding-theory/book/,
2015.
[Ha50] R. W. Hamming, “Error detecting and error correcting codes”, Bell System Tech-
nical Journal 29 (1950), 147–160.
[Ha80] R. W. Hamming, Coding and Information Theory, Prentice-Hall, 1980.
[HaHaJo97] D. Hankerson, G. A. Harris, and P. D. Johnson, Jr., Introduction to Infor-
mation Theory and Data Compression, CRC Press, 1997.
[HeWi99] C. Heegard and S. B. Wicker, Turbo Coding, Kluwer, 1999.
[Hi27] L. S. Hill, “The role of prime numbers in the checking of telegraphic communica-
tions”, Telegraph and Telephone Age (April 1 and July 16, 1927) 151–154, 323–324.
[HuPl03] W. C. Huﬀman and V. Pless, Fundamentals of Error-Correcting Codes, Cam-
bridge University Press, 2003.
[JoZi15] R. Johannesson and K. Zigangirov, Fundamentals of Convolutional Coding, 2nd
ed., Wiley, 2015.
[JoKi11] D. Joyner and J-L. Kim, Selected Unsolved Problems in Coding Theory, Birkh¨a-
user, 2011.
[LiCo04] S. Lin and D. J. Costello, Jr., Error Control Coding, 2nd ed., Pearson, 2004.
[Ma03] D. MacKay, Information Theory, Inference, and Learning Algorithms, Cambridge
University Press, 2003.
[MaSl77] F. J. MacWilliams and N. J. A. Sloane, The Theory of Error-Correcting Codes,
North-Holland, 1977.
[Mc77] R. J. McEliece, The Theory of Information and Coding, Addison-Wesley, 1977.
[Mo05] T. Moon, Error Correction Coding: Mathematical Methods and Algorithms, Wi-
ley, 2005.
[NeRaSl06] G. Nebe, E. M. Rains, and N. J. A. Sloane, Self-Dual Codes and Invariant
Theory, Springer-Verlag, 2006.
[PeWe72] W. W. Peterson and E. J. Weldon, Jr., Error-Correcting Codes, MIT Press,
1972.
[Pi88] P. Piret, Convolutional Codes, MIT Press, 1988.
[PlHuBr98] V. Pless, W. C. Huﬀman, and R. A. Brualdi, Handbook of Coding Theory,
Elsevier, 1998.
[Pr92] O. Pretzel, Error-Correcting Codes and Finite Fields, Clarendon Press, 1992.
[Ro96] S. Roman, Introduction to Coding and Information Theory, Springer-Verlag, 1996.
[Ro06] R. Roth, An Introduction to Coding Theory, Cambridge University Press, 2006.
[Sh08] T. Shaska, “Quantum codes from algebraic curves with automorphisms”, Con-
densed Matter Physics 11 (2008), 383–396.
[St93] H. Stichtenoth, Algebraic Function Fields and Codes, Springer-Verlag, 1993.
[ToChJo12] J. Torres, C. Christensen, and D. Joyner, “Hill’s error-detection codes”,
Cryptologia 36 (2012), 88–103.

REFERENCES
1067
[TsVl91] M. A. Tsfasman and S. G. Vlˇadut¸, Algebraic-Geometric Codes, Mathematics
and its Applications, Kluwer Academic Publishers, 1991.
[TVN07] M. Tsfasman, S. Vlˇadut¸, and D. Nogin, Algebraic Geometric Codes: Basic
Notions, American Mathematical Society, 2007.
[vL99] J. H. van Lint, Introduction to Coding Theory, third ed., Springer-Verlag, 1999.
[VavO89] S. A. Vanstone and P. C. van Oorschot, An Introduction to Error Correcting
Codes with Applications, Kluwer Academic Publishers, 1989.
[We98] R. B. Wells, Applied Coding and Information Theory for Engineers, Pearson,
1998.
[Ye08] R. Yeung, Information Theory and Network Coding, Springer-Verlag, 2008.
Web Resources:
http://www.codetables.de/ (Provides bounds on the parameters of various types of
codes.)
http://www.eccpage.com (The Error Correcting Codes (ECC) Home Page.)


15
CRYPTOGRAPHY
Charles C. Y. Lam, Chapter Editor
This chapter is a major extension and update of the sections on cryptography in the ﬁrst
edition written by Alfred J. Menezes and Paul C. van Oorshot.
15.1 Basics of Cryptography
Charles C. Y. Lam
15.1.1 Basic Concepts
15.1.2 Security of Cryptosystems
15.2 Classical Cryptography
Charles C. Y. Lam
15.3 Modern Private Key Cryptosystems
Khoongming Khoo
15.3.1 Block Ciphers
15.3.2 Feistel Ciphers
15.3.3 The Advanced Encryption Standard (AES)
15.3.4 Modes of Operation
15.3.5 Stream Ciphers
15.4 Hash Functions
Charles C. Y. Lam
15.4.1 Basic Concepts
15.4.2 Construction and Methods
15.4.3 Families of Hash Functions
15.5 Public Key Cryptography
Shaoquan Jiang and
15.5.1 Introduction to Public Key Cryptosystems
Charles C. Y. Lam
15.5.2 RSA Cryptosystem
15.5.3 ElGamal Cryptosystem
15.5.4 Elliptic Curve Cryptosystem
15.5.5 NTRU Cryptosystem
15.5.6 Cramer-Shoup Cryptosystem
15.5.7 Paillier Cryptosystem
15.5.8 Goldwasser-Micali Cryptosystem
15.5.9 Identity-Based Encryption
15.5.10 Attribute-Based Encryption
15.6 Cryptographic Mechanisms
Shaoquan Jiang
15.6.1 Digital Signature
15.6.2 Message Authentication Code
15.6.3 Key Exchange
15.6.4 Homomorphic Encryption
15.6.5 Secret Sharing
15.6.6 Conference Key Distribution

1070
Chapter 15
CRYPTOGRAPHY
15.6.7 Oblivious Transfer
15.6.8 Secure Multiparty Computation
15.7 High-Level Applications of Cryptography
Charles C. Y. Lam
15.7.1 Digital Cash
15.7.2 Electronic Voting
15.7.3 Secure Cloud Computing
INTRODUCTION
The history of protecting the secrecy of information goes back to ancient times. For
example, the ancient Romans used a secret code to send messages that could not be read
by their enemies. The evolution of secret codes ties with the historical development of
human intelligence and improvements in technology.
In modern times, there is a constant need to protect information from unauthorized
access and from malicious actions. The ﬁeld of cryptography is devoted to methods that
oﬀer such protection.
Sending secret messages, authenticating messages, distributing
secret keys, sharing secrets, electronic cash, and electronic voting are only some of the
applications addressed by modern cryptography.
Symmetric key cryptography and public key cryptography are the two major classes of
cryptography. Since prehistoric times, encryption conducted between two parties can be
seen as a communication mechanism with a secret key shared between two communi-
cating parties, where the knowledge of the encryption key implies the knowledge of the
corresponding decryption key. In the 1970s, through the demonstration of a practical
method by Whitﬁeld Diﬃe and Martin Hellman, and an independent classiﬁed discovery
within Britain’s GCHQ (Government Communications Headquarters), public key cryp-
tography came to life. The discovery led to constructions of many new products that are
used in the digital age.
The study of cryptography spans various ﬁelds from mathematics, computer science, and
engineering, to non-science areas such as business and law. The goal of mathematical
cryptography is to ensure the security of the most fundamental cryptographic compo-
nents.
GLOSSARY
active adversary: an adversary that threatens the integrity or authentication of stored
or transmitted data.
adaptive chosen-ciphertext attack security (CCA2): given the challenge cipher-
text C in CCA1 security, the attacker can continue the decryption requests adap-
tively, except for C itself, and is unable to recover the plaintext.
advanced encryption standard (AES): a 128-bit block cipher adopted as a stan-
dard in the United States and widely used for commercial applications.
aﬃne cipher: a cipher that encrypts x ∈Z26 by sending it to ax+b mod 26, using the
key (a, b), where a, b ∈Z26 and gcd(a, b) = 1.
attribute-based encryption: a public key encryption scheme where the user’s secret
key and the ciphertext both depend on attributes such as membership and position
title.

GLOSSARY
1071
authentication: corroboration that a party, or the origin of data, is as claimed.
bitcoin: a widely-used cryptocurrency.
bitcoin mining: a process in ensuring data integrity in bitcoin transactions that re-
quires a search of a nonce that produces a desired hash result.
block cipher: a cipher that breaks up the plaintext into blocks of a ﬁxed length and
encrypts each block via a secret bijective transformation.
Caesar cipher: a cipher that shifts each letter forward three positions in the alphabet,
wrapping the letters at the end of the alphabet around to the beginning letters.
(Sometimes, all shift ciphers are called Caesar ciphers.)
certiﬁcation authority (CA): a trusted authority that veriﬁes the identity and RSA
public key of a party, and signs the data.
chosen-ciphertext attack: an attack in which the adversary has some chosen cipher-
text and its corresponding plaintext.
chosen-plaintext attack: an attack in which the adversary has some chosen plaintext
and its corresponding ciphertext.
cipher: an encryption scheme.
cipher block chaining (CBC) mode: a mode of operation of an n-bit block cipher
in which plaintext is processed n bits at a time, an initialization block is used, and to
encrypt each successive n-bit block the bitwise XOR of the block with the encrypted
version of the previous block is formed and the resulting n-bit block is encrypted by
the block cipher.
cipher feedback (CFB) mode: a mode of operation of an n-bit block cipher in which
plaintext is processed t bits at a time, 1 ≤t ≤n, and in which the ciphertext depends
on the current block and previous blocks.
ciphertext: encrypted plaintext.
ciphertext-only attack: an attack in which the adversary has possession of some ci-
phertext and nothing else.
ciphertext space: the set of all possible ciphertexts.
cloud computing: an internet-based model for shared computing resources and data.
computational security: the amount of computational eﬀort required by the best
currently-known attacks to defeat a system.
conference key distribution: a mechanism whereby a trusted authority assigns a
piece of secret data to each user so that any desired group of users can compute
a common secret key.
conﬁdentiality: preventing protected data from becoming available in an intelligible
form to unauthorized parties.
counter mode: a mode of operation of a n-bit block cipher in which the secret keystream
is formed by encrypting an incremental counter initialized by an initial value (IV).
Cramer-Shoup cryptosystem: a public key cryptosystem based on the decisional
Diﬃe-Hellman problem.
cryptanalysis: the use of mathematical, statistical, and other techniques to defeat
cryptographic protection mechanisms, including plaintext from ciphertext without
knowledge of the key.

1072
Chapter 15
CRYPTOGRAPHY
cryptocurrency: a class of digital cash for which cryptography is used in secure trans-
actions and in the creation of additional currency.
cryptographic primitive: a basic algorithm used to build cryptographic protocols.
cryptographic protocol: a protocol that performs security functions and applies cryp-
tographic primitives.
cryptography: the science and study of protecting data from malicious or unauthorized
actions, including access to, manipulation, impersonation, and forgery.
cryptology: the study of both cryptography and cryptanalysis. (Note that “cryptog-
raphy” is often used in place of “cryptology”.)
cryptosystem (or cryptographic system): a system composed of a space of plain-
text messages, a space of ciphertext messages, a key space, and families of enciphering
and deciphering functions indexed with the keys.
data encryption standard (DES): a 64-bit block cipher adopted as a standard in
the United States and which is widely used for commercial applications.
data integrity: the assurance that data has not been subjected to unauthorized ma-
nipulation, possibly including assurances regarding uniqueness and timeliness.
data origin authentication: corroboration that the origin of data is as claimed.
decryption: the process of recovering plaintext from ciphertext with knowledge of the
key.
diﬀerential analysis: an attack on symmetric key ciphers that distinguishes the right
key from wrong guesses by observing the statistical distribution of diﬀerences in the
input plaintext and diﬀerences in the output ciphertext.
digital cash: a digital equivalent of cash that achieves all the functionality and security
(and more) of physical cash.
digital signature: the digital analogue of a handwritten signature, in the form of a
value dependent on some secret known only to the signer and on the message being
signed.
digital signature algorithm (DSA): a digital signature scheme adopted as a United
States standard and which is widely used for commercial applications.
digram cipher: a cipher that replaces pairs of plaintext characters (digrams) with
other digrams.
electronic code book (ECB) mode: a mode of operation of an n-bit block cipher in
which plaintext is encrypted n bits at a time.
electronic voting: a mechanism to provide accurate, auditable, fast, and secure voting
over a communications network.
ElGamal cryptosystem: a public key cryptosystem based on the discrete logarithm
problem.
elliptic curve cryptosystem: a public key cryptosystem based on the elliptic curve
discrete logarithm problem.
encryption: the process of mapping plaintext to ciphertext designed to render data
unintelligible to all but the intended recipient.
Enigma machine: a rotor-based encryption machine developed before World War II
and used extensively by the German military during the war with modiﬁcations.

GLOSSARY
1073
entity authentication: corroboration that a party’s identity is as claimed.
Feistel cipher: a block cipher structure introduced by Horst Feistel of IBM in the
1970s and used in the construction of many ciphers.
frequency analysis: a technique to analyze a classical cipher by studying the frequency
of letters or groups of letters in a ciphertext.
Goldwasser-Micali cryptosystem: the ﬁrst provably secure public key cryptosystem.
hash function: a function that maps arbitrary length bit strings to small ﬁxed length
outputs that is easy to compute and has preimage resistance, weak collision resis-
tance, and/or strong collision resistance.
Hill cipher: a matrix-based cipher using an invertible n × n matrix A over Z26 as its
key, which encrypts plaintext by splitting it into blocks of size n and then sending
the plaintext block m = (m1, m2, . . . , mn)′ as the m-tuple Am.
homomorphic encryption: an encryption mechanism that allows certain computa-
tions to be carried out on ciphertext in such a way that decryption of the result
yields the same quantity as obtained by performing the computations on the original
plaintext.
identity-based encryption: a public key cryptosystem that uses an arbitrary string,
such as email address or name, as the public key.
Kerckhoﬀ’s assumption: the statement that the security of a system should rest en-
tirely in the secret key—the adversary is assumed to have complete knowledge of the
rest of the cryptographic mechanism(s).
key: a secret number or other signiﬁcant information which parametrizes an encryption
or decryption algorithm.
key exchange (or key agreement): a mechanism that allows two parties to securely
share a secret key so that no one else can obtain a copy.
key schedule: an algorithm that derives a series of subkeys from an input key used for
encryption rounds in a symmetric key encryption scheme.
keyspace: the set of all possible keys of a cryptosystem.
keystream generator: an algorithm to generate an inﬁnitely long stream of characters
to be used as keys starting with a secret seed value.
known-plaintext attack: an attack in which the adversary has some plaintext and its
corresponding ciphertext.
linear cryptanalysis: an attack on symmetric key ciphers that distinguishes the right
key from wrong key guesses by observing the linear approximation of input plaintext
bits and output ciphertext bits.
linear feedback shift register (LFSR): a shift register whose input is characterized
by a linear recursive function of the output.
message authentication code (MAC): a keyed function FK(·) acting on messages
m such that when m and FK(m) are transmitted, the intended recipient will be
assured of the authenticity of m.
message space: the set of all possible plaintexts.
non-adaptive chosen-ciphertext attack security (CCA1): an attacker that can
inquire multiple times the plaintext for any adaptively chosen ciphertext, yet is
unable to recover the plaintext for a new challenge ciphertext C.

1074
Chapter 15
CRYPTOGRAPHY
nonce: a pseudorandom number generated for one-time use.
non-repudiation: the ability to ensure that a party to a communication cannot deny
the authenticity of their signature or the sending of a message that they originated.
NTRU cryptosystem: a lattice-based public key cryptosystem.
oblivious transfer: a protocol in which one party has a set of bit strings as input
and the other party can choose a string to be transferred but the ﬁrst party cannot
determine which string this is.
one-time pad: a stream cipher in which the secret key has length equal to that of the
plaintext and is used only once for encryption.
output feedback (OFB) mode: a mode of operation of an n-bit block cipher in which
a message is split into blocks of t bits for processing, 1 ≤t ≤n, and in which error
propagation is avoided.
Paillier cryptosystem: a public key cryptosystem based on the intractability of com-
puting n-th residue classes.
passive adversary: an adversary that does not interrupt, alter, or insert any trans-
mitted or stored data.
perfect secrecy (or unconditional security): a security criteria in which an adver-
sary is assumed to have unlimited computational resources.
plaintext: a message to be encrypted.
polyalphabetic substitution cipher: a family of monoalphabetic substitution ciph-
ers using diﬀerent substitution mappings in various locations of the plaintext.
polygram substitution cipher: a cipher that substitutes groups of characters with
other groups of characters.
probabilistic encryption: an encryption scheme that involves the use of randomness
in the encryption algorithm, so that the same message is encrypted into diﬀerent
ciphertexts when diﬀerent randomness parameters are chosen.
product cipher: a combination of two or more simpler ciphers.
provable security: security in which the diﬃculty of defeating the method is essen-
tially as diﬃcult as solving a well-known, supposedly diﬃcult problem.
public key certiﬁcate: data that binds together a party’s identiﬁcation and public
key.
public key system: a cryptosystem in which each user has a pair of encryption (public)
and decryption (private) keys.
quantum attack: an attack on cryptographic primitives using quantum computing
methods.
quantum cryptography: cryptographic methods employing the science of quantum
mechanical properties.
round function: a subroutine in a symmetric key encryption scheme that is run mul-
tiple times to obtain the ciphertext.
RSA cryptosystem: a public key cryptosystem in which encryption is based on mod-
ular exponentiation with a modulus that is the product of two large primes, and is
based on the diﬃculty of factoring the product of two large primes and the ease of
generating large primes.

Section 15.1
BASICS OF CRYPTOGRAPHY
1075
scytale: an ancient encryption device that writes on a piece of parchment wrapped
around a cylinder.
secret sharing: a mechanism that allows a group of users to share a secret such that
any group with more than a threshold number of them can recover the secret, whereas
fewer members are unable to obtain any information about the secret.
secure hash algorithm (SHA): a family of cryptographic hash functions adopted as
a standard in the United States and widely used for commercial applications.
secure multiparty computation: a protocol π such that, given n parties P1, . . . , Pn
where each Pi has a private input xi, π realizes a function f(x1, . . . , xn) = (o1, . . . , on)
such that Pi obtains the output oi, i = 1, . . . , n.
self-synchronizing stream cipher: a stream cipher capable of re-establishing proper
decryption automatically after loss of synchronization, with at most a ﬁxed number
of plaintexts unrecoverable.
shift cipher: a cipher that replaces each plaintext letter by the letter shifted a ﬁxed
number of positions in the alphabet, with letters at the end of the alphabet shifted
to the beginning of the alphabet.
steganography: the art of concealing the existence of data without the transformation
of data.
substitution cipher: a cipher that replaces each plaintext character by a ﬁxed substi-
tute according to a permutation of the alphabet.
stream cipher: a symmetric key cipher that encrypts individual characters of a plain-
text message using a keystream generator and a family of encryption functions in-
dexed by keys.
symmetric key system: a cryptosystem in which knowing the encryption function
corresponding to a key allows a user to easily determine the corresponding decryption
function.
synchronous stream cipher: a stream cipher in which the keystream is generated
independently of the message.
transposition cipher: a cipher that divides plaintext into blocks of a ﬁxed size and
rearranges the characters in each block according to a ﬁxed permutation.
unconditional security (or perfect secrecy): a security criteria for which an adver-
sary is assumed to have unlimited computational resources.
Vernam cipher: a one-time pad.
Vigen`ere cipher: a cipher with a t-tuple (k1, . . . , kt) as its key that encrypts plaintext
messages in blocks of size t so that the ith letter in a block is shifted ki positions in
the alphabet, modulo 26.
15.1
BASICS OF CRYPTOGRAPHY
The word cryptography comes from Greek words that translate as “hidden writing”.
Until the mid 20th century, these Greek words completely described the subject, when
all activities were devoted to making messages secret, to recovering secret messages,

1076
Chapter 15
CRYPTOGRAPHY
and to preventing unauthorized attempts to ﬁnd original messages from secret messages.
However, during the last ﬁfty years, cryptography has expanded to cover all aspects of
protecting data from a wide range of malicious activities. A particular vocabulary now
describes diﬀerent parts of cryptography, the processes of making messages secret and
recovering these messages, and attacks on secret messages.
15.1.1
BASIC CONCEPTS
Deﬁnitions:
Cryptography is the science and study of protecting data from malicious or unautho-
rized actions, including access to, manipulation, impersonation, and forgery.
Cryptanalysis is the use of mathematical, statistical, and other techniques to defeat
cryptographic protection mechanisms.
Cryptology is the study of both cryptography and cryptanalysis, although “cryptogra-
phy” is often used in place of “cryptology”.
Steganography is the art of concealing the existence of data without transforming the
data.
A cipher is a method whereby a message in some source language (the plaintext) is
transformed by a mapping, called an encryption algorithm, to yield an output, called
the ciphertext, which is unintelligible to all but an authorized recipient.
A recipient of an encrypted message is able to recover the plaintext from the ciphertext
by use of a corresponding decryption algorithm.
A key is a secret number or other signiﬁcant information which parametrizes an encryp-
tion or decryption algorithm.
The message space (or plaintext space) M is the set of all possible plaintexts, the
ciphertext space C consists of all possible ciphertexts, and the keyspace K consists of
all possible keys.
An encryption algorithm E is a family of mappings parametrized by a key k ∈K,
such that each value k deﬁnes a mapping Ek ∈E, where E is the set of all invertible
mappings from M to C. A speciﬁc plaintext message m is mapped by Ek to a ciphertext
c = Ek(m).
The set D of decryption algorithms consists of all invertible mappings from C back
to M, such that for each encryption key k ∈K, there is some mapping D ∈D such
that Df(k)(Ek(m)) = m for all m ∈M, where f(k) is some key dependent on k and
Df(k) is the decryption algorithm corresponding to the decryption key f(k). For so-called
symmetric key systems, this decryption key f(k) is equal to k itself.
A cryptographic primitive is a basic algorithm used to build cryptographic protocols.
Hash functions and encryption schemes are examples of cryptographic primitives.
A cryptosystem is a ﬁve-tuple (M, C, K, E, D) consisting of the following:
• a set of possible plaintexts M;
• a set of possible ciphertexts C;
• a set of possible keys K;

Section 15.1
BASICS OF CRYPTOGRAPHY
1077
• a family of encryption algorithms E and the corresponding family of decryption
rules D, parametrized by the keyspace K.
Probabilistic encryption involves the use of randomness in the encryption algorithm,
so that the same message is encrypted into diﬀerent ciphertexts when diﬀerent random-
ness parameters are chosen.
Conﬁdentiality means preventing conﬁdential data from being available in an intelligi-
ble form to unauthorized parties.
Detection of data manipulation by unauthorized parties (including alteration, insertion,
deletion, substitution, delay, and replay) is called data integrity; it should be noted
that encryption alone does not guarantee data integrity.
Authentication is corroboration that a party’s identity is as claimed (entity authenti-
cation), or that the origin of data is as claimed (data origin authentication); related
to this is the assurance that data has not been subjected to unauthorized manipulation
(cf. data integrity), possibly including assurances regarding uniqueness and timeliness.
The provision for the resolution of disputes related to digital signatures is called non-
repudiation. Digital signatures can be used as the basis of authorization of certain
actions. Disputes may occasionally arise subsequently due to either false denials (repu-
diated signatures) or fraudulent claims (forged signatures).
A digital signature is intended to be the digital analogue of a handwritten signature; it
should be a number dependent on some secret known only to the signer, and, additionally,
on the content of the message being signed.
Facts:
1. Many excellent books on cryptography are available. Those new to the subject can
consult [Bu13], [DeKn15], [Ma03], [PaPe09], and [TrWa06]. Books with more historical
content include [MevOVa10] and [Sc07]. Textbooks with comprehensive coverage at a
more advanced level are [KaLi14] and [St05].
2. Cryptography diﬀers from steganography in that while the former involves use of
techniques to secure data (for example, codes and ciphers), the latter involves the use of
techniques which obscure the existence of data itself (for example, invisible ink, secret
compartments, and the use of subliminal channels).
3. Cryptographic mechanisms can be used to support a number of fundamental security
services, including conﬁdentiality, data integrity, authentication, and non-repudiation.
4. The traditional objectives of conﬁdentiality and authentication (although not both
required in all cases) lead to the following requirements for a cryptosystem:
• Fundamental requirement. (To maintain secrecy of key) it should be infeasible
for an adversary to deduce the key k given one or more plaintext-ciphertext
pairs (m, c);
• Conﬁdentiality requirement. (To maintain conﬁdentiality) it should be infeasible
for an adversary to deduce the plaintext m corresponding to any given cipher-
text c;
• Authentication requirement. (To prevent forgery or substitution) it should be
infeasible for an adversary to deduce a ciphertext c′ corresponding to any mes-
sage m′ of his choosing, or corresponding to any other (meaningful) message m.

1078
Chapter 15
CRYPTOGRAPHY
15.1.2
SECURITY OF CRYPTOSYSTEMS
Deﬁnitions:
Adversaries are either passive or active. Passive adversaries are a threat to conﬁden-
tiality; they do not interrupt, alter, or insert any transmitted or stored data. Active
adversaries additionally threaten integrity and authentication.
There are many models under which one can assume a cryptanalyst is able to attack a
cryptographic system. The following types of attack can be hypothesized for increasingly
powerful adversaries:
• ciphertext-only: the adversary has possession only of some ciphertext;
• known-plaintext:
the adversary has some plaintext and its corresponding ci-
phertext;
• chosen-plaintext:
the adversary has some plaintext of his choosing and its
corresponding ciphertext;
• chosen-ciphertext:
the adversary has some ciphertext of his choosing and its
corresponding plaintext.
The most stringent measure of the security of a cryptographic algorithm is uncondi-
tional security, where an adversary is assumed to have unlimited computational re-
sources, and the question is whether there is enough information available to defeat the
system. Unconditional security for encryption systems is called perfect secrecy.
To measure an adversary’s uncertainty in the key after observing n ciphertext characters
C, Shannon deﬁned the key equivocation function Q(n) = H(K|C1C2 . . . Cn), the
conditional entropy (§14.1.2) of the key K given the ciphertext messages C1, C2, . . . , Cn;
this measures the amount of uncertainty that remains in the key K after observing the
ciphertexts C1, C2, . . . , Cn. Shannon deﬁned the unicity distance of the cipher to be
the ﬁrst value n = n0 for which Q(n) ≈0.
A cryptographic method is said to be provably secure if the diﬃculty of defeating it can
be shown to be essentially as diﬃcult as (that is, polynomially equivalent to) solving a
well-known and supposedly diﬃcult (typically number-theoretic) problem, such as integer
factorization or the computation of discrete logarithms. (Thus, “provable” here means
provable subject to as yet unproved assumptions.)
A proposed technique is said to be computationally secure if the (perceived) level of
computation required to defeat it exceeds, by a comfortable margin, the computational
resources of the hypothesized adversary.
Facts:
1. It is a standard cryptographic assumption that an adversary will have access to
ciphertext.
2. Kerckhoﬀ’s assumption:
The security of a system should rest entirely in the secret
key — the adversary is assumed to have complete knowledge of the rest of the crypto-
graphic mechanism(s).
3. In determining whether the security of a particular cryptosystem is adequate for a
particular application, the powers and resources of the anticipated adversary must be
taken into account.
Potential adversaries may have powers ranging from minimal to
unlimited.

Section 15.2
CLASSICAL CRYPTOGRAPHY
1079
4. The security of a cryptographic algorithm can be measured according to several
diﬀerent metrics, including unconditional security, provable security, and computational
security.
5. Let M, C, and K be random variables ranging over the message space M, ciphertext
space C, and keyspace K. Unconditional security for encryption systems can be speciﬁed
by the condition H(M|C) = H(M); that is, the uncertainty in the plaintext, after
observing the ciphertext, is equal to the a priori uncertainty about the plaintext —
observation of the ciphertext provides no information (whatsoever) to an adversary.
6. A necessary condition for an encryption scheme to be unconditionally secure is that
the key should be at least as long as the plaintext. The one-time pad (§15.3.5) is an
example of an unconditionally secure encryption algorithm.
7. In general, encryption schemes do not oﬀer perfect secrecy, and each ciphertext char-
acter observed decreases the uncertainty in the encryption key k used.
8. Let n0 be the unicity distance of a cipher. After observing n0 characters, the key
uncertainty is zero, meaning an information-theoretic adversary can narrow the set of
possible keys down to a single candidate, thus defeating the cipher.
9. The computational security measures the amount of computational eﬀort required,
by the best currently-known attacks, to defeat a system.
15.2
CLASSICAL CRYPTOGRAPHY
The ancient Greeks and Romans were known to use cryptography. Some of the earliest
uses of ciphers include the Scytale and the Caesar cipher. While all of the systems are
now considered insecure, they provide insights into the evolution of cryptography.
The approaches in cryptography and cryptanalysis became more systematic in the 19th
century. With the development of electromechanical devices, encryption machines were
introduced after World War I. The Enigma machine was one of the most famous encryp-
tion machines used in World War II. The ciphers up to this point were all restricted to
the encryption of individual characters, or groups of characters in the plaintext.
The invention of transistors after World War II led to the computer age where plaintext
messages were no longer encoded in pure characters in a speciﬁc language. Encryption
methods have taken on a much more mathematical approach since then.
Deﬁnitions:
Let P = C = Zn be the plaintext and the ciphertext character set for some integer n.
Let K ∈Zn. The encryption-decryption pair eK, dK deﬁned by
eK(x)
=
x + K mod n,
dK(y)
=
y −K mod n
is a shift cipher.
Suppose the plaintext and the ciphertext character set A are the same. A simple (mono-
alphabetic) substitution cipher is a permutation π over A, where the message is
mapped one character at a time. Decryption is done by applying π−1 to the ciphertext.

1080
Chapter 15
CRYPTOGRAPHY
Given the alphabet of size n, the aﬃne cipher uses the encryption function ea,b(x) =
ax + b mod n, where gcd(a, n) = 1.
A digram cipher replaces pairs of plaintext characters (digrams) with other digrams.
A polygram substitution cipher involves substituting groups of characters with other
groups of characters.
Let t be a positive integer. Let π be a permutation on {1, 2, . . ., t}. For a plaintext
m = (m1, m2, . . . , mt), the permutation cipher maps m to eπ(m1, m2, . . . , mt) =
(mπ(1), mπ(2), . . . , mπ(t)).
A polyalphabetic substitution cipher is a family of monoalphabetic substitution
ciphers using diﬀerent substitution mappings at various locations of the plaintext.
A simple Vigen`ere cipher of period t over an n-character alphabet has a key k1k2 . . . kt,
where encryption of plaintext m = m1m2m3 . . . to ciphertext c = c1c2c3 . . . is speciﬁed
by ci = mi + ki mod n. The key is reused every t characters (that is, the index i of k is
taken modulo t). The cipher is an example of a polyalphabetic substitution cipher.
Let m be an n-character message in vector form m = (m1, m2, . . . , mn)′.
The Hill
cipher (1929) is an n-gram substitution cipher deﬁned by an invertible n × n matrix A
over Z26 as the key, with encryption c = Am. Decryption is therefore m = A−1c.
Facts:
1. For a plaintext character set Zn, there are n distinct shift ciphers. That is, the size
of the keyspace is n.
2. The shift cipher can be easily compromised using exhaustive key search, by trying
all keys K, 0 ≤K ≤n −1, until meaningful text appears.
3. The Caesar Cipher, purportedly used by Julius Caesar, is a shift cipher with K = 3.
4. In the English language with 26 characters, there are 26! ≈4 × 1026 possible monoal-
phabetic substitution ciphers, making exhaustive search infeasible.
5. The monoalphabetic substitution cipher can be broken easily by frequency analysis,
which makes use of statistical properties of letters in the English language.
6. The aﬃne cipher is a polygram substitution cipher.
7. Al-Kindi, an Arab mathematician, published around AD 800 the book Risalah ﬁ
Istikhraj al-Mu’amma on cryptography. The book was the ﬁrst publication that described
cryptanalytic techniques, including frequency analysis.
8. Ibn al-Durayhim (AD 1312–1361) gave detailed descriptions of eight ciphers, includ-
ing substitution and transposition ciphers (see Scytale, Fact 15).
9. The ﬁrst polyalphabetic substitution cipher was invented by Leon Battista Alberti
circa 1467.
10. Cryptography was widely employed across Europe during and after the Renais-
sance. One of the most famous stories involved the use of cryptanalysis by Sir Francis
Walsingham, leading to the execution of Mary, Queen of Scots.
11. In the English alphabet of 26 characters, there are 262 digrams, and thus there are
(262)! possible keys in the digram cipher.
12. The Playfair Cipher (1854) is a digram cipher where an alphabet of 25 letters is
arranged in a 5 × 5 array. In the English alphabet, this can be done by either equating
I and J, or omitting Q or X to reduce the number of letters from 26 to 25. Encryption
is done by encrypting pairs of plaintext letters as follows:

Section 15.2
CLASSICAL CRYPTOGRAPHY
1081
• If the two letters are in diﬀerent rows and columns in the table, then the two letters
form two corners of a rectangle, and are replaced by the letters representing the
other two corners of the rectangle, where the ﬁrst encrypted letter is in the
same column as the ﬁrst letter of the plaintext.
• If the two letters are in the same row, then each letter is replaced by the letter
to the immediate right in the table, where a wrap around to the ﬁrst column
occurs when needed.
• If the two letters are in the same column, then each letter is replaced by the letter
immediately below it. The cipher defeats basic frequency analysis, but is easily
broken by analysis of frequencies of pairs of letters.
13. The Hill Cipher can be easily broken by a known-plaintext attack through solving
a system of linear equations.
14. The permutation cipher keeps the plaintext characters unchanged, and uses a per-
mutation to alter the positions of the characters in the plaintext.
15. The Scytale was used by the Greeks, especially by the Spartans, during military
campaigns. To operate, a piece of parchment is wrapped around a cylinder. The message
is then written on the parchment. The recipient wraps the parchment on a cylinder of
the exact speciﬁcation to retrieve the message. The ciphertext is a result of a special
case of permutation cipher called transposition cipher due to the regular reordering of
plaintext characters.
16. Two approaches for the cryptanalysis of the Vigen`ere cipher are the Kasiski test,
based on ﬁnding repeated strings in cipher text, and the index of coincidence, which
examines the probability that two letters in the ciphertext are the same.
• Kasiski (1863), and independently Babbage in 1846 who kept it as a military
secret, discovered that repeated strings in the plaintext that are apart by mul-
tiples of the length of the key are encrypted in the same way. Therefore, by
examining repeated patterns, candidate key lengths k can be deduced. Suppose
k is the actual key length, let c = c1c2c3 . . . cnk be the ciphertext, and let
c1
=
c1ck+1c2k+1 . . . c(n−1)k+1,
c2
=
c2ck+2c2k+2 . . . c(n−1)k+2,
...
ck
=
ckc2kc3k . . . cnk.
Since every kth character is encrypted using the same method, each ci should
have a letter frequency distribution similar to regular English text.
• Index of coincidence (Friedman, 1920). Suppose c = c1c2 . . . cn is a ciphertext
of n characters. The index of coincidence EI(c) is the probability that two
random elements of c are identical. In regular English text, the probability is
approximately 0.065, whereas for a completely random string, the probability
is approximately 0.038. The method involves organizing the ciphertext in a
manner similar to the Kasiski test and measures the index of coincidence for
each string ci. The correct key length guess should reveal index of coincidence
values similar to those of English text.
17. The Jeﬀerson disk, a polyalphabetic substitution cipher invented by Thomas Jeﬀer-
son in 1795, uses rotating disks. A solid cylinder of length 6 inches is sliced into 36 disks.
Each disk is numbered, where the 26 letters of the alphabet are inscribed on the disk

1082
Chapter 15
CRYPTOGRAPHY
around the edge in a distinct and random ordering. Each disk has a hole in the middle
that allows them to be stacked and mounted on an axle. Encryption is done in blocks of
36 letters. To encrypt, each of the 36 wheels is rotated individually to align the message
in one row. Then, any of the other 25 rows can be used as the ciphertext. To decrypt,
the recipient arranges the cylinders in the agreed-upon order, aligns the ciphertext in
one row, then checks the other 25 rows until meaningful message appears. Since there
are 36 disks, the total number of possible orderings is 36! ≈3.72 × 1041.
18. A rotor machine is a design for mechanical automation of polyalphabetic substi-
tution ciphers. It consists of a number of wired wheels called rotors. Each rotor has
connectors on each face of the wheel, wired internally so that an individual rotor repre-
sents a ﬁxed monoalphabetic substitution cipher by mapping a character at its input face
to another one at its output face. A plaintext character input to the ﬁrst rotor generates
an output that serves as an input to the second rotor. This process continues until the
ciphertext character outputs from the last rotor.
• When the rotors are ﬁxed, the combination of rotors produces only a monoalpha-
betic substitution cipher.
• The rotors are designed to move with each plaintext character encipherment.
This results in a polyalphabetic substitution cipher. A basic design mechanism
involves rotating one step of the ﬁrst rotor until it completes a single revolution,
then the second rotor will move one step, and so on.
• The encryption key includes the order of the rotors and their respective positions.
• The ﬁrst rotor machine was built by Dutch naval oﬃcers Theo A. van Hengel and
R. P. C. Spengler in 1915. Independent inventions have also been attributed to
E. H. Hebern, A. G. Damm, H. A. Koch, and A. Scherbius.
• E. H. Hebern founded the Hebern Electric Code, Inc. in 1921 to become the ﬁrst
U.S. cipher machine company. The company went bankrupt in 1926.
• A. Scherbius ﬁled a rotor machine patent in 1918. Scherbius later designed and
marketed the Enigma machine.
• Scherbius developed the Enigma machine in successive versions. The company
Chiﬀriermaschinen AG was set up in 1923 to produce and market these ma-
chines. The initial Model A (1923) was replaced by Model B (1925). In 1926,
a reﬂector and indicator lamps were introduced in Model C. In 1927, Enigma
D, with improvements that allowed wheels to be interchangeable, received wide
commercial success.
• The Enigma machine as used in World War II consists of versions of similar design
with a choice of wheels, and other enhancements such as plugboards to increase
the complexity of cryptanalysis.
• Alan Turing, who worked at the British Government Code and Cypher School
(GC&CS) at Bletchley Park during the war, provided many of the ideas that led
to the construction of the Bombe machine for the cryptanalysis of the Enigma.
19. Many other codes were used in World War II. Some notable ones include the Amer-
ican Codetalkers using Native American languages as a basis for a substitution cipher,
the British Typex which is an adaptation of the Enigma machine, and the German Fish
cipher [Tu00], similar to the Enigma, but only used between the German High Command
and Army Group commanders in the ﬁeld.
20. Some references on the history of classical cryptography include The Code Book
[Si00], The Codebreakers [Ka96], Handbook of Applied Cryptography [MevOVa10], and
Introduction to Cryptography with Coding Theory [TrWa06].

Section 15.3
MODERN PRIVATE KEY CRYPTOSYSTEMS
1083
Examples:
1. The shift cipher can be used on any alphabet by a proper mapping from the alphabet
to some Zn. In English, we can map A ↔0, B ↔1, . . ., Z ↔25 over Z26. For example,
with key K = 4, the word “QUIZ” is encoded to (16, 20, 8, 25), and is encrypted to
(20, 24, 12, 3), or “UYMD”.
2. A permutation cipher of the English alphabet such as
plaintext
abcdefghijklmnopqrstuvwxyz
ciphertext UXJAGRDEYLNKOPBCTWVQMIHFZS
maps the word “king” to “NYPD”.
3. Let A = 0, B = 1, C = 2, . . . Consider the aﬃne cipher e3,14(x) = 3x + 14 mod 26.
Then the word “QUEEN” is encrypted to “KWAAB”.
4. Consider the permutation cipher on nine symbols π = (135)(86)(2497), where the
symbols indicate the positions of characters in the message. The word “XYLOPHONE”
is then encrypted to “POXYLNEHO”.
5. Using the mapping A ↔0, B ↔1, C ↔2, . . . , Z ↔25, the Vigen`ere cipher with key
“MIKE” encrypts the message “SENDMORETROOPS” to “EMXHYWBIFZYSBA”.
15.3
MODERN PRIVATE KEY CRYPTOSYSTEMS
Classical encryption can be performed by hand (e.g., Caesar cipher) or by mechanical
means (e.g., Enigma cipher). Modern ciphers refer to ciphers invented since the 1970s
which rely upon the power of modern computers. For the most commonly used modern
private key cryptosystems, plaintext messages and ciphertext messages are bit strings of
a speciﬁed length. These cryptosystems are designed so that encryption and decryption
can be carried out rapidly using complicated functions that are designed to be resistant
to attack. This section covers the basic principles of such cryptosystems and provides
information about the most widely used private key systems of today and of the past
three decades.
15.3.1
BLOCK CIPHERS
Deﬁnitions:
A block cipher breaks up the plaintext into blocks of a ﬁxed length and encrypts each
block via a secret bijective transformation.
The secret bijective transform of a block cipher is deﬁned by a key schedule and a
round function. The key schedule expands the secret key K into r distinct roundkeys
K1, K2, ..., Kr. To perform encryption, the round function (denoted by φ) is iteratively

1084
Chapter 15
CRYPTOGRAPHY
applied r times on the plaintext x0 and the roundkeys Ki to produce the ciphertext xr.
Keyschedule(K)
7→
K1, K2, K3, . . . , Kr
x0
=
Plaintext
x1
=
φ(K1, x0)
x2
=
φ(K2, x1)
x3
=
φ(K3, x2)
· · ·
xr
=
φ(Kr, xr−1)
Ciphertext
=
xr.
r is called the number of rounds of the block cipher and is carefully chosen to
ensure suﬃcient security without sacriﬁcing throughput.
The round function φ mixes the intermediate text with the roundkeys by performing the
following transformations in sequence:
• Substitution/Confusion: To provide (local) nonlinearity. This is usually real-
ized by several 4-bit or 8-bit nonlinear substitution tables (also called S-boxes)
acting in parallel.
• Permutation/Diﬀusion: To spread the nonlinear eﬀect across the whole block.
This is usually realized by a bit permutation or a linear matrix product over
GF(2).
A product cipher combines two or more simpler ciphers (transformations) such that
the resulting cipher is more secure than its components. Thus a block cipher can be seen
as a product cipher which combines many simpler transformations (where confusion and
diﬀusion are alternately applied over many rounds) to perform encryption.
Facts:
1. The substitution cipher is an example of a block cipher where the plaintext is broken
up into individual English alphabets and each alphabet is encrypted through a secret
substitution table.
2. Modern block ciphers encrypt block sizes of 64 bits or 128 bits.
3. It is not possible to represent the secret bijection of modern block sizes by a single
substitution table. Thus modern block ciphers are instantiated by a sequence of simpler
substitution tables and permutation transforms which mixes the plaintext with the secret
key to produce a ciphertext.
4. Some prominent examples of block ciphers include the Data Encryption Standard
(DES, §15.3.2) [Fi77] and the Advanced Encryption Standard (AES, §15.3.3) [DaRi13].
5. The product cipher was introduced by Claude Shannon in his paper “Communication
theory of secrecy systems” in 1949.
15.3.2
FEISTEL CIPHERS
Deﬁnitions:
The Feistel cipher is a block cipher structure introduced by Horst Feistel of IBM in
the 1970s. It is used to construct many ciphers including the Data Encryption Standard
(DES).

Section 15.3
MODERN PRIVATE KEY CRYPTOSYSTEMS
1085
The round function of a Feistel cipher (denoted by φK) divides the input into two
equal halves (L, R) and is deﬁned by
φK(L, R) = (R, L ⊕F(K, R)),
where K is the roundkey and F is a function mapping the right half R and the roundkey
K to be XORed with the left half L. For an r-round Feistel cipher, a swap is performed
on the left and right halves of the ﬁnal output to produce the ciphertext, that is,
EK(P) = swap(φKr ◦· · · ◦φK2 ◦φK1(P)).
where swap(L, R) = (R, L) for equal halves (L, R).
Facts:
1. One advantage of the Feistel cipher is that it is an involution. That is, the encryption
circuit can be re-used for decryption when the roundkeys are fed in reverse order. Thus
it saves implementation resources as only one copy of the cipher needs to be implemented
for both encryption and decryption.
2. One of the earliest and most well-known Feistel ciphers is the Data Encryption Stan-
dard (DES), proposed in 1977 [Fi77]. DES resulted from an IBM submission to a 1974
request by the U.S. National Bureau of Standards (NBS) (which has now become the
National Institute of Standards and Technology, NIST) soliciting encryption algorithms
for the protection of computer data.
3. The early success of DES made Feistel a well-researched structure with many more
secure Feistel ciphers being designed later. Some examples include the ciphers KASUMI
[Ca05], Camellia [AoEtal00], its parallelizable variant p-Camellia [YaKhPo13], and the
ﬁnalists MARS and Twoﬁsh of the AES competition [Ae97] (see §15.3.3). These ciphers
still make use of the basic Feistel structure but use more secure components, larger
keylength and blocklength, or use a larger number of rounds.
4. The basic Feistel cipher with two sub-blocks has also been extended to generalized
Feistel structures with four sub-blocks. They include the ciphers Skipjack [Sk98], CLE-
FIA [ShEtal07], SMS4 [DiLe08], its parallelizable variant p-SMS4 [YaKhPo13], and the
ﬁnalist RC6 of the AES competition [Ae97] (see §15.3.3). For illustrative purposes, we
can illustrate the round function of SMS4 as an example of a generalized Feistel structure:
φK(A, B, C, D) = (B, C, D, A ⊕F(K, B ⊕C ⊕D)).
5. DES was used by the U.S. government to protect binary coded data during trans-
mission and storage in computer systems and networks. It was also used by the banking
industry and businesses to protect ﬁnancial transactions for commercial data security.
DES and its more secure variant Triple-DES (which encrypts the plaintext three times
using a longer key) were also part of the SSH/TLS suite of ciphers to protect internet
traﬃc.
6. A high-level description of DES is the following:
• DES processes plaintext blocks of 64 bits and produces ciphertext blocks of 64
bits. The encryption mapping EK is parameterized by a secret 56-bit key K.
Since decryption requires that the mapping be invertible, EK is a bijection.
• Encryption of each 64-bit block proceeds in sixteen stages or rounds. The 56-bit
key K is used to create sixteen 48-bit subkeys Ki, one for each round.
• Within each round, eight ﬁxed, carefully selected 6-to-4 bit substitution mappings
(S-boxes) Si, collectively denoted S, are used.

1086
Chapter 15
CRYPTOGRAPHY
• The initial 64-bit plaintext is divided into two 32-bit halves, L0 and R0. Each
round is functionally equivalent, taking 32-bit inputs Li−1 and Ri−1 from the
previous round and producing outputs Li and Ri for 1 ≤i ≤16, according to
Li = Ri−1, Ri = Li−1 ⊕F(Ri−1, Ki), F(Ri−1, Ki) = P(S(E(Ri−1 ⊕Ki))).
Here E is a ﬁxed expansion permutation mapping 32 bits to 48 bits (all bits
are used once, some are taken twice), and P is another ﬁxed permutation on
32-bits. An initial permutation (IP) precedes the ﬁrst round, and its inverse is
applied following the last round.
• Decryption makes use of the same algorithm and same key, except the subkeys
are applied to the internal rounds in the reverse order.
7. DES was used by ﬁnancial institutions through the 1990s, but in the late 1990s it
became apparent that it is not suﬃciently secure. With advancements in computing
technologies, brute force search of the DES keyspace became cheaper and easier (as out-
lined in Facts 8–10). Furthermore, several methods, known as Diﬀerential Cryptanalysis
and Linear Cryptanalysis (see Facts 11–12), were discovered and were shown to be more
eﬃcient than brute force.
8. The keylength of DES (56 bits) is too short. The original proposal for DES had a
keylength of 64 bits. But later, eight bits of the secret key were converted to parity
bits making the equivalent keylength 56 bits. The complexity of exhaustive search is
256 and this was already criticized early on by researchers to be insuﬃcient protection
against brute force search. Diﬃe and Hellman postulated in 1977 that a custom machine
consisting of 106 custom VLSI chips costing $20,000,000 could search the entire DES
keyspace in about one day.
9. In 1998, the Electronic Frontier Foundation built a dedicated DES Cracker machine
(DEEP CRACK) costing $250,000 which could ﬁnd a DES key in 56 hours [De98].
10. In 2006, re-conﬁgurable parallelized hardware called COPACOBANA was built to
break ciphers with keylengths up to 64 bits, including DES [G¨uEtal08]. It takes longer
to ﬁnd a DES key (on average 8.7 days) but is much cheaper at $10,000. With the ad-
vancement in computing technologies, brute force search on DES became faster, cheaper,
and easier.
11. Diﬀerential Cryptanalysis (DC) is an attack on DES that distinguishes the right
key from wrong key guesses by observing the statistical distribution of diﬀerences in
the input plaintext and diﬀerences in the output ciphertext. It can recover the secret
key with 247 chosen plaintext-ciphertext pairs [BiSh91], which is faster than exhaustive
search.
However, the enormous volume of chosen plaintext-ciphertext pairs required
diminishes the value of this attack.
12. Linear Cryptanalysis (LC) on DES distinguishes the right key from wrong key
guesses by observing the linear approximation of input plaintext bits and output cipher-
text bits. It can recover the key with 243 known plaintext-ciphertext pairs [Ma93], which
is faster than exhaustive search. Similar to DC, even though plaintext-ciphertext pairs
are not required to be chosen, the enormous volume of pairs required still diminishes the
value of this attack.
13. Diﬀerential and linear cryptanalysis on DES can be thwarted by changing keys be-
fore the required number of plaintext-ciphertext pairs needed for cryptanalysis is trans-
mitted. For example, we can change the key after every 232 ciphertexts are transmitted.
14. The short keylength of DES allows for practical key recovery with just a few
plaintext-ciphertext pairs. One countermeasure to the short keylength of DES is to use

Section 15.3
MODERN PRIVATE KEY CRYPTOSYSTEMS
1087
Triple-DES which encrypts the plaintext three times with longer keys. In the following
description, K1, K2, K3 are 56-bit keys, PT is the plaintext, and CT is the ciphertext.
• Two-Key Triple-DES (112-bit secret key):
CT = DESK1(DES−1
K2(DESK1(PT ))).
• Three-Key Triple-DES (168-bit secret key):
CT = DESK3(DES−1
K2(DESK1(PT ))).
Note that the decryption function DES−1 is used for the second pass to allow for back-
ward compatibility with single-key DES. This is because DES with single-key K can be
deﬁned as two-key Triple-DES with key (K1, K2) = (K, K) or three-key Triple-DES with
key (K1, K2, K3) = (K, K, K).
15. Another way to counter the short keylength of DES is to use DES-X. It is deﬁned
as
CT = DESK2(PT ⊕K1) ⊕K3,
where PT and CT are the plaintext and ciphertext, K2 is a 56-bit DES key, and K1 and
K3 are 64-bit keys to mask the input and output of DES. The total keylength for DES-X
is 184 bits.
16. Meet-in-the-middle (MITM) attacks can be applied to Triple-DES and DES-X to
reduce the complexity of brute force search, but their security is still better than DES.
Two-key triple DES can be attacked with 232 known plaintexts, 288 time complexity,
and 256 memory [vOWi90]. Three-key Triple-DES can be attacked with three known
plaintexts, 2112 time complexity, and 256 memory [MevOVa10]. The equivalent keylength
of DES-X is shown to be 88 bits under a distinguishing attack with 230 known plaintexts
[KiRo01]. See [MevOVa10] for a summary of various MITM attacks on Triple-DES and
[KiRo01] for a proof of the security of DES-X against MITM.
17. Although Triple-DES and DES-X are considered to be secure, neither has suﬃ-
ciently high throughput to be used for modern applications. Because of this, in 1997
NIST launched a competition for a new encryption standard, to be named the Advanced
Encryption Standard (AES).
15.3.3
THE ADVANCED ENCRYPTION STANDARD (AES)
In 1997, NIST organized a competition to choose a replacement for DES called the
Advanced Encryption Standard (AES) [Ae97]. The competition called for ciphers with a
128-bit blocklength and keylengths of 128, 192, and 256 bits, secure against known attacks
including diﬀerential and linear cryptanalysis, and which provided excellent performance
when implemented in hardware and software.
The process of determining which of the 15 submissions provided the best trade-oﬀ
between security and performance took three years. Finally, in 2000 the winner was
declared to be Rijndael, the submission of the Belgian cryptographers Joan Daemen and
Vincent Rijmen. After winning the competition, the name AES replaced their original
name Rijndael.

1088
Chapter 15
CRYPTOGRAPHY
Deﬁnitions:
The AES cipher takes in a 128-bit plaintext and three possible keylengths of 128, 192,
and 256 bits. The internal state can be viewed as a 4-by-4 array of bytes which is ﬁrst
ﬁlled with the 128-bit plaintext. A key schedule expands the secret key of 128, 192,
or 256 bits into 11, 13, and 15 roundkeys, respectively. A prewhitening roundkey
is XORed to the plaintext and then the round function is repeatedly applied for R =
10, 12, or 14 times for AES-128, AES-192, or AES-256. Each round of AES uses these
operations:
• SubBytes: Every byte is nonlinearly transformed by a Substitution Box (S-
Box). The S-Box is aﬃne equivalent to the power function x254 on GF(28);
x254 is the multiplicative inverse of x when x ̸= 0, and 0 when x = 0. The
ﬁnite ﬁeld representation for GF(28) is given by GF(2)[a]/(f(a)), where f(a) =
a8 + a4 + a3 + a + 1 is an irreducible polynomial.
• ShiftRows: The ShiftRows operation shifts row r of the AES state array by r−1
bytes to the left.
• MixColumns: The MixColumns operation multiplies each column of the state
array by a right circulant matrix over GF(28). The ﬁrst row of the matrix is
given by (a, a + 1, 1, 1) where GF(28) is also deﬁned by GF(2)[a]/(f(a)), where
f(a) = a8 + a4 + a3 + a + 1.
• AddRoundKey: This operation XORs the ith subkey ki to the ith round.
Facts:
1. A complete description of DES can be found in [DaRi13].
2. The AES cipher execution framework is shown in Algorithm 1.
Algorithm 1:
AES cipher algorithm.
input: R, plaintext
output: ciphertext
state := plaintext
AddRoundKey(state, k0)
for i = 1 to R −1
SubBytes(state)
ShiftRows(state)
MixColumns(state)
AddRoundKey(state, ki)
SubBytes(state)
ShiftRows(state)
AddRoundKey(state, kR)
ciphertext := state
3. The ﬁnal round transformation does not have the MixColumns operation.
4. After adoption of AES as the new international block cipher standard by NIST, many
banks and ﬁnancial institutions migrated their encryption algorithm from DES to AES.
5. AES is also used by the IPSec (Internet Protocol Security) standard to protect IP
communications. Other applications include TLS/SSH encryption for internet security
and WPA/WPA2 encryption for WiﬁProtection Access.

Section 15.3
MODERN PRIVATE KEY CRYPTOSYSTEMS
1089
6. The NSA also stipulated AES as part of its suite B algorithms to protect both
unclassiﬁed and most classiﬁed information for the U.S. government.
7. AES is based on the concept of a Substitution Permutation Network (SPN). An SPN
starts with a roundkey XOR layer followed by an S-Box substitution layer that provides
confusion, followed by a diﬀusion layer designed to spread the local S-box eﬀect to the
entire block.
8. The SubBytes operation in AES has the lowest diﬀerential and linear probabilities
which defend AES against diﬀerential [BiSh91] and linear [Ma93] cryptanalysis (two
attacks which broke DES). The AES SubBytes is a special case of a power function xk
on GF(2n); see [Ca10b] for other power functions with good cryptographic properties.
9. The MixColumns matrix is a 4 × 4 Maximal Distance Separable (MDS) matrix over
GF(28). These matrices have optimal branch number 5. The branch number of a matrix
is the minimum number of nonzero bytes in the input and output, when ranged among
all nonzero inputs. The maximal possible branch number for an n × n matrix is n + 1,
in which case the matrix will be called an MDS matrix. An MDS matrix ensures a high
number of active S-boxes in a block cipher to prevent diﬀerential and linear cryptanalysis.
10. In general, an n × n MDS matrix over GF(2k) can be formed by reducing the
generator matrix of a [2n, n, n + 1]-MDS code over GF(2k) to standard form [I|A]. Then
A will be an MDS matrix.
11. An equivalent formulation for an n × n MDS matrix A is that every submatrix of A
must be nonsingular. In particular, this means that all entries of an MDS matrix have
to be nonzero.
12. The AES ShiftRows is an example of an optimal diﬀusion structure [DaRi13]. It
spreads the eﬀect of MixColumns across the whole 128-bit state and ampliﬁes the diﬀu-
sion eﬀect of MixColumns from ﬁve active S-boxes every two rounds to 25 active S-boxes
every four rounds. This gives very strong protection against DC and LC, where the dif-
ferential and linear characteristic probability over four rounds of AES is already 2−150,
rendering both attacks infeasible [DaRi13].
13. Due to the algebraic nature of the components used by AES, it is possible to embed
the AES cipher into a bigger cipher BES (Big Encryption System) such that BES can
be expressed as a simple set of linear and quadratic equations over GF(28) [MuRo02].
14. It was claimed that the XSL (eXtreme Sparse Linearization) attack [CoPi02] can
then be applied on the BES equations to break AES with complexity 2100. However,
it was shown in [LiKh07] that there were too many linearly dependent equations in the
XSL attack on BES for it to work. Despite this, there may be some other way to solve
the simple set of equations produced by the BES embedding.
15. As of 2016, the most successful attacks on AES are the related-key diﬀerential
cryptanalysis [BiKhNi09] and biclique attacks [BoKhRe11]. However, they are still not
yet practical.
• Related-key diﬀerential cryptanalysis (RK-DC) is a diﬀerential attack where not
just the plaintext, but also the secret key has a nonzero input diﬀerence. If the
key diﬀerence is chosen carefully, then there is a higher chance to ﬁnd a good
diﬀerential path such that during the AddRoundKey operation, the roundkey
diﬀerences cancel the intermediate state diﬀerences to result in fewer active
bytes. RK-DC can reduce the attack complexity on AES-256 from 2256 to 2130
but needs 235 related keys, which are extremely unlikely to be available in most
practical applications, except perhaps when AES-256 is used in a hash function
mode of operation.

1090
Chapter 15
CRYPTOGRAPHY
• The biclique attack is a variant of the meet-in-the-middle attack (MITM) that
uses a biclique structure to extend the number of rounds that can be attacked
by MITM. The attack on AES-128, 192, and 256 has complexity 2126.1, 2189.7,
and 2254.4, respectively. Although they are the best single-key attacks on AES,
their complexity is too close to exhaustive search to be practical.
16. For applications that use AES-256 in a hash function mode of operation, the key
schedule needs to be strengthened to resist related-key diﬀerential attacks. Such work
has been carried out in [Ni10] and [ChEtal11], where the main cipher remains intact
but the key schedule is modiﬁed so that high probability related-key diﬀerential paths,
crucial for a successful attack, cannot be found.
15.3.4
MODES OF OPERATION
With large messages, it is necessary to break up the message into smaller blocks for
encryption. Therefore, to use block ciphers, there is a need to employ an encryption
mode of operation such that messages can be broken up in blocks, and encrypted in
sequence. Various modes are introduced with distinct beneﬁts.
Deﬁnitions:
An initial vector (IV) is a random value generated and sent in the clear with the
ciphertext in some block cipher modes.
The Electronic Code Book (ECB) mode encrypts the plaintext block by block:
cr = EK(mr),
for 1 ≤r ≤l.
In the Cipher Block Chaining (CBC) mode, the previous ciphertext block is XORed
to the current plaintext block before encryption:
c1 = EK(m1 ⊕IV ), cr = EK(mr ⊕cr−1),
for 2 ≤r ≤l.
The Output FeedBack (OFB) mode is a stream cipher mode where the IV is re-
peatedly encrypted, and the most signiﬁcant t bits of the output blocks form the secret
keystream to be XORed to the plaintext mr to form the ciphertext cr:
s0 = EK(IV ), sr = EK(sr−1), cr = mr ⊕MSBt(sr−1),
for 1 ≤r ≤l.
The Cipher FeedBack (CFB) mode is a variant of the OFB mode. The ciphertext
is still formed by XORing the plaintext to the secret internal state. However, the se-
cret internal state evolution is formed by repeatedly encrypting a concatenation of the
previous secret state shifted left by t bits and the ciphertext:
s0 = IV, sr = (sr−1 ≪t) ⊕cr−1, cr = mr ⊕MSBt(EK(sr−1)),
for 1 ≤r ≤l.
Like the OFB mode, the Counter Mode is one where the secret keystream is formed
by encrypting an incremental counter initialized by IV:
cr = mr ⊕EK(IV + r −1),
for 1 ≤r ≤l.

Section 15.3
MODERN PRIVATE KEY CRYPTOSYSTEMS
1091
Facts:
1. ECB is not secure because the same plaintext blocks are mapped to the same ci-
phertext blocks. For example, if we want to encrypt the pixels of a secret treasure map,
it only maps the natural color of a map to diﬀerent secret colors, but the outline and
secret locations would still be visible. That is why we need other modes of operation
that conceal the relationship between plaintext blocks.
2. Error propagation.
Counter mode and OFB have 1-bit propagation, i.e., a 1-bit
error in the ciphertext during transmission causes a 1-bit error in the plaintext after
decryption. CBC and CFB have 1-bit plus 1-block propagation, i.e., a 1-bit error in the
ciphertext causes a 1-bit error in the corresponding plaintext block and a 1-block error
in the subsequent block after decryption [Dw01].
3. Performance. CBC needs more resources to implement because it needs both en-
cryption and decryption, whereas OFB, CFB, and counter mode just need encryption.
Counter mode is faster than the other three modes because it can be parallelized. CFB
allows for t-bit block slip recovery.
4. Security against IV misuse. If IV is repeated, counter mode and OFB are totally
broken because the same keystream KS would be repeated for two diﬀerent encryptions
and we have
CT = PT ⊕KS and CT ′ = PT ′ ⊕KS ⇒CT ⊕CT ′ = PT ⊕PT ′.
Thus the plaintext XOR relation can be deduced from the ciphertext. This weakness
only aﬀects the ﬁrst block of CFB. CBC is not aﬀected as diﬀerent plaintexts will give
diﬀerent ciphertexts even when the IV is repeated. Thus many software implementations
use CBC mode because software random number generation is less reliable and IV could
repeat. However, it is important to protect the integrity of the IV in CBC mode, or else
an adversary can selectively manipulate the ﬁrst message block by manipulating the bits
of the IV. This situation may be avoided by encrypting the IV.
5. Counter mode is combined with message authentication functions CMAC and GMAC
to form the CCM and GCM authenticated encryption modes, respectively [Dw04], [Dw07].
15.3.5
STREAM CIPHERS
Deﬁnitions:
A keystream is a stream of secret random characters that are combined with the plain-
text to form the ciphertext.
A stream cipher is a symmetric key cipher that encrypts individual characters of a
plaintext message, or small units. It is deﬁned by a keystream generator and an encryp-
tion rule.
The keystream generator takes in a short secret seed (usually the secret key K) and
expands it into an inﬁnitely long stream of keystream characters, i.e., Key Gen(K) =
z1, z2, z3, . . ..
The encryption rule ez(x) deﬁnes how the plaintext character xi is combined with
the keystream character zi. There is a corresponding decryption rule dz(x) such that
dz(ez(x)) = x.
The one-time pad is a stream cipher where the secret key is of bitlength equal to
the plaintext and never re-used for encryption. Thus there is no need for a keystream

1092
Chapter 15
CRYPTOGRAPHY
generator as the keystream Z is just the secret key K. The encryption rule is ez(x) =
x ⊕z. The decryption rule is dz(y) = y ⊕z.
A synchronous stream cipher is a stream cipher in which the keystream is generated
independently of the message.
A self-synchronizing stream cipher is a stream cipher capable of re-establishing
proper decryption automatically after loss of synchronization, with only a ﬁxed number
of plaintexts unrecoverable.
Many stream ciphers use Linear Feedback Shift Registers (LFSR) in their construc-
tion (see §14.7.2). An n-word LFSR is deﬁned over a ﬁnite ﬁeld F with an initial state
(s0, s1, . . . , sn−1), si ∈F, and a feedback relation
si+n = ci+n−1si+n−1 + ci+n−2si+n−2 + · · · + cisi, i ≥0.
In stream cipher applications, the LFSR is usually deﬁned over the ﬁnite ﬁeld F = GF(2)
or GF(2m).
Facts:
1. The one-time pad can be proven to be unconditionally secure but it suﬀers the draw-
back that a large quantity of keying material has to be pre-shared to encrypt long mes-
sages. Thus in practice, stream ciphers usually generate a long unpredictable keystream
from a short key to be XORed with the plaintext for encryption.
2. An important advantage of stream ciphers is that they are lighter and faster than
block ciphers in hardware implementations.
This is because stream ciphers process
smaller chunks of data, and thus need fewer resources to store and process the inter-
mediate states.
They also have smaller error propagation (than certain block cipher
modes such as CFB, CBC) where a one-character error in the ciphertext only causes a
one-character error in the plaintext.
3. When the encryption rule is ez(x) = x ⊕z, it is insecure to use the same keystream
zi to encrypt two diﬀerent plaintext streams xi and x′
i. This is because the ciphertext
streams will be ci = xi ⊕zi and c′
i = x′
i ⊕zi. And this allows the adversary to deduce
the plaintext relation xi ⊕x′
i from the ciphertext relation ci ⊕c′
i because they are equal.
4. To prevent the repeated keystream weakness, an initialization vector IV is mixed into
the key generation and sent in the clear to the receiver for decryption. Now even when the
sender uses the same secret key K to encrypt two sessions, he can mix it with two distinct
initial vectors IV, IV ′ to produce distinct keystreams Key Gen(K, IV ) = z1, z2, z3, . . .
and Key Gen(K, IV ′) = z′
1, z′
2, z′
3, . . ..
5. Care must be taken not to repeat the initial vector IV when using stream ciphers
or else it is completely broken. It is also not that easy to convert a stream cipher to an
authenticated encryption (AE) scheme, whereas numerous AE modes of operation for
block ciphers such as CCM, GCM, OCB exist.
6. The Vigen`ere cipher can be seen as a stream cipher. The keystream generator is a
cyclical repetition of the secret key:
Key Gen(k1, k2, . . . , ks) = k1, k2, . . . , ks, k1, k2, . . . , ks, . . . .
The encryption rule is ez(x) = x + z (mod 26), where the English alphabet A, B, . . . , Z
is mapped to 0, 1, . . ., 25.

Section 15.3
MODERN PRIVATE KEY CRYPTOSYSTEMS
1093
7. Another stream cipher that works on the English alphabet is the autokey cipher. The
keystream generator only uses the secret key once in the beginning and then the plaintext
is appended to it. If the plaintext stream is m1, m2, . . . , mn, then the keystream is
Key Gen(k1, k2, . . . , ks) = k1, k2, . . . , ks, m1, m2, . . . , mn−s.
The English alphabet is encoded as in the Vigen`ere cipher and the encryption rule is
ez(x) = x + z (mod 26).
8. On modern computers, the plaintext characters used are binary bits and the encryp-
tion rule is usually the XOR operation ez(x) = x⊕z. Most of the complexity and security
of the cipher is provided by the keystream generation.
9. Early stream ciphers from the 1950s were based on Linear Feedback Shift Registers
(LFSRs). A straightforward way to encrypt is to use an LFSR to expand a short key
into a long keystream and XOR it to the plaintext.
10. The LFSR structure, however, is insecure as the cryptosystem is linear and can
be easily broken by Gaussian elimination, or more eﬃciently by the Berlekamp-Massey
algorithm (even when the LFSR feedback taps are secret) [Ma69]. The Berlekamp-Massey
algorithm is derived from the Berlekamp-Welch algorithm for decoding BCH codes.
11. To be more secure, the LFSR output can be ﬁltered by a nonlinear Boolean function
in one of two conﬁgurations [Ru12]:
• Combinatorial Generator. This stream cipher consists of n LFSRs and a nonlinear
Boolean function f(x) mapping n bits to 1 bit. At each clock cycle, 1 bit is
extracted from each LFSR to make up an n-bit input vector, which is passed
through the Boolean function f(x) to produce 1 output keystream bit. The
keystream is XORed with the plaintext to form the ciphertext.
• Filter Function Generator.
This stream cipher consists of one LFSR and one
Boolean function f(x) mapping n bits to 1 bit. At each clock cycle, n bits are
extracted from the LFSR and passed as input to the Boolean function f(x) to
produce 1 output keystream bit. The keystream is XORed with the plaintext
to form the ciphertext.
12. The combinatorial and ﬁlter function generator stream ciphers are secure against
traditional attacks such as correlation attack, divide and conquer attack, and linear span
attack if the ﬁlter function f(x) is chosen to have the following good cryptographic
properties:
• Balance. The Boolean function f(x) has an equal number of 0s and 1s in its
output. This is to prevent the keystream from having any bias which might
leak information about the plaintext.
• Low Linear Bias. We want the linear bias of f(x) given by
ǫ = | Pr(f(x) = l(x)) −1
2|
to be low for all linear functions l(x). This is to defend against fast correlation
attacks which make use of soft decoding techniques from LDPC (low-density
parity-check), convolutional, and turbo codes [AgEtal12] to recover the stream
cipher secret state. See §14.3 and §14.7.
• High Correlation Immunity. This property requires f(x) to have linear bias ǫ = 0
for all linear approximations with few linear terms. This is to defend against
divide and conquer correlation attacks on combinatorial generators [Ca10a].

1094
Chapter 15
CRYPTOGRAPHY
• High Algebraic Degree.
We need f(x) to have high algebraic degree, i.e., its
Boolean expression involves monomials that are products of many variables.
This is to defend against algebraic attacks and the Berlekamp-Massey attack
on keystreams with low linear complexity. Furthermore, for additional resis-
tance to algebraic attacks, we also require f(x) to have high algebraic immunity
(explained below).
13. It was believed that stream ciphers with high algebraic degree are not susceptible to
algebraic attacks. However, the fast algebraic attack invented in 2003 managed to break
such ciphers [Co03], [CoMe03].
14. Algebraic Attack on Stream Ciphers. The algebraic attack from [CoMe03] works on
the ﬁlter function f(x) in stream ciphers. The attack ﬁnds low degree Boolean functions
g(x) such that f(x)g(x) also has low degree. Then the keystream equation f(Lt(s)) = zt,
which may have high degree, will be transformed to the low degree equation
f(Lt(s))g(Lt(s)) = ztg(Lt(s)).
This low degree equation can be solved by algebraic methods such as linearization, eX-
treme Linearization (XL), or Gr¨obner basis ﬁnding algorithms.
15. Algebraic Immunity. To resist the algebraic attacks from [CoMe03] and [Co03] on
stream ciphers, we need f(x) to have high algebraic immunity. That means we want all
Boolean functions g(x) such that g(x)f(x) = 0 for all x and all functions g(x) such that
g(x)(f(x) + 1) = 0 for all x to have high algebraic degree.
16. Some practical applications of stream ciphers include the following:
• SNOW 3G stream cipher for UEA2/UIA2 telecommunications encryption. The
SNOW cipher takes in a 128-bit key and a 128-bit initialization vector IV .
It is a ﬁlter function generator that maintains its secret state in a 16-word
LFSR deﬁned over GF(232). The nonlinear combiner is a ﬁnite state machine
that consists of three 32-bit registers that make use of the AES SubBytes and
MixColumns operations for computation. It builds on the success of AES by
retaining some of the good cryptographic properties of AES components but
aims to be faster by computing each 32-bit word with fewer clock cycles. As of
2015, there is one published analysis on SNOW 3G which shows some related
key distinguishing properties [KiYo11], but is not a break of the cipher.
• RC4 stream cipher for WEP encryption to protect WiFi traﬃc. The RC4 cipher
takes in variable length keys between 40 to 2048 bits. It maintains a key de-
pendent secret permutation table on 256 bytes, which is iteratively updated by
a pseudorandom generation algorithm to produce a keystream of secret bytes.
This is XORed with the plaintext to produce the ciphertext. The original speci-
ﬁcation of RC4 is only deﬁned based on a secret key with no initialization vector
IV . However, the usage in WEP requires a fresh IV for every packet trans-
mitted. The WEP designers used RC4 with a 128-bit key, where a 24-bit IV is
concatenated with a 104-bit secret key. However, the weak design of this key
schedule enabled Fluhrer, Mantin, and Shamir to launch a related-key attack
(same 104-bit secret key but diﬀerent 24-bit chosen IV s) on WEP encryption
to recover the secret key in practical time [FlMaSh01].
• E0 stream cipher for Bluetooth encryption. The E0 stream cipher takes in a 128-
bit key, 48-bit Bluetooth address, and a 26-bit master counter. These are used
to initialize four LFSRs of lengths 25, 31, 33, 39 bits and a ﬁnite state machine
(FSM) that consists of two pairs of 2-bit delay elements. At each clock cycle, 1

Section 15.3
MODERN PRIVATE KEY CRYPTOSYSTEMS
1095
bit is taken from the FSM and each of the four LFSRs to be XORed to form a
keystream bit. This is then XORed with the plaintext to form the ciphertext.
The best attack on E0 to date is a conditional correlation attack by Lu, Meier,
and Vaudenay [LuMeVa05]. It uses the ﬁrst 24 bits of 223.8 frames (each frame
is 2745 bits long) to recover the secret key with 238 complexity.
• A5/1 stream cipher for GSM encryption. The A5/1 stream cipher takes in a 64-
bit key and a 22-bit frame number, which are used to initialize three LFSRs of
lengths 19, 22, and 23 bits. The LFSRs are clocked in a stop-and-go fashion
based on majority logic. One clocking bit is taken from each LFSR and a ma-
jority vote is computed. If the clocking bit of an LFSR agrees with the majority
bit, that LFSR is clocked. Thus at each clock cycle, either two or three LFSRs
are clocked. The short 64-bit key length of A5/1 makes it vulnerable to brute
force attack based on just a few frames of known plaintext, e.g., by using the
COPACOBANA search engine [G¨uEtal08]. With more known plaintext, there
are faster attacks such as the correlation attack by Maximov et al. [MaJoBa04],
where A5/1 can be broken in less than one minute based on a few seconds of
conversation.
17. The emergence of algebraic attacks and other prominent breaks on stream cipher
standards such as A5/1 and A5/2 (used in GSM encryption) prompted the call for better
design/standardization of stream ciphers [Es04]. This prompted the call for a stream
cipher competition ESTREAM, organized by EU-ECRYPT, to develop secure stream
cipher standards.
18. The ESTREAM competition was held by EU-ECRYPT from 2005 to 2008 [RoBi08].
The stream cipher entries fall under two proﬁles: proﬁle 1 for high throughput software
(faster than AES) and proﬁle 2 for lightweight hardware implementations in constrained
devices.
There were 34 submissions of which seven were chosen to be stream cipher
standards after scrutiny by the international cryptographic community.
19. The OFB and counter modes of a block cipher expand the secret key into a long
keystream (independently of the plaintext) by repeated applications of the block cipher.
Thus they are examples of synchronous stream ciphers.
20. The CFB mode of a block cipher is an example of a self-synchronizing stream cipher.
CFB mode with t-bit feedback can recover from a bit slip of t bits. That is, when t con-
secutive bits of data are lost during transmission, the ciphertext will decrypt incorrectly
for b/t iterations, where b is the blocklength, and then resume correct decryption after
the misaligned content is completely ﬂushed from the block cipher input register.
Examples:
1. Consider an autokey cipher with secret key “iloveyou”= {8, 11, 14, 21, 4, 24, 14, 20}.
It maps the plaintext “todayisagoodday”= {19, 14, 3, 0, 24, 8, 18, 0, 6, 14, 14, 3, 3, 0, 24} as
follows:
plaintext
:
19, 14, 3, 0, 24, 8, 18, 0, 6, 14, 14, 3, 3, 0, 24
keystream
:
8, 11, 14, 21, 4, 24, 14, 20, 19, 14, 3, 0, 24, 8, 18
ciphertext
:
1, 25, 17, 21, 2, 6, 6, 20, 25, 2, 17, 3, 1, 8, 16.
The keystream is formed by appending the plaintext after the key and the ciphertext is
formed by adding the plaintext to the keystream modulo 26. The ciphertext converted
to the alphabet is “bzrvcgguzcrdbiq”.
2. Consider a simple 9-bit LFSR cipher over GF(2) deﬁned by the feedback relation
si+9 = si+4 + si. Suppose the secret key is 011001010 and the plaintext is the 16-bit

1096
Chapter 15
CRYPTOGRAPHY
string 1100111010110111. To perform encryption, the secret key is expanded to a 16-bit
keystream 0110010100011010. It is then XORed to the plaintext to form the ciphertext
1010101110101101.
3. One example of the algebraic attack of [CoMe03] is on the Toyocrypt cipher. The
cipher is a nonlinear ﬁlter function generator where a 128-bit LFSR is ﬁltered by a 128-bit
Boolean function deﬁned by
f(s0, s1, . . . , s127) = s127 + P62
i=0 sisα(i) + s10s23s32s42+
s2s9s12s18s20s23s25s26s28s33s38s41s42s51s53s59 + Q62
i=0 si,
where α(i) is a permutation of the indices i ∈{63, 64, . . ., 125}. This ﬁlter function is
not easy to attack directly with an algebraic attack because its degree is 63, which is
high.
However, when f(s) is multiplied by the linear polynomial g(s) = s23 + 1, we see that
f(s)g(s) becomes a cubic polynomial
f(s)g(s) = s127(s23 + 1) + P62
i=0 sisα(i)(s23 + 1).
This is because the last three monomials of f(s) disappear when multiplied by (s23 + 1),
as s23 appears in them and s2
23 = s23 for Boolean variables. In this way, the degree of
the keystream equation is reduced from 63 to 3. By performing a linearization attack
(treating each monomial as a new variable), one can solve the resulting linear system
for the key in 251.6 time using 217.4 keystream bits. In [Co03], an improvement is made
to this attack where the keystream independent part of the linearized equations is pre-
processed by the Berlekamp-Massey algorithm and stored in memory, leading to a faster
online attack. There was a further enhancement [ZhEtal09] to the algebraic attacks of
[CoMe03] and [Co03] by considering the attack in the re-synchronization scenario, which
results in further reduction of the attack complexity to a few seconds on a PC to recover
the secret key under four resynchronizations, each producing 128 keystream bits.
15.4
HASH FUNCTIONS
Conventional hash functions in non-cryptographic applications are commonly used to
map data of arbitrary size to a ﬁxed size. Cryptographic hash functions require additional
properties and are used for the purposes of data integrity and message authentication.
15.4.1
BASIC CONCEPTS
Deﬁnitions:
A hash function h maps arbitrary length bit strings to small ﬁxed length (for example,
128-bit or 512-bit) outputs called hash-values.
A collision is a pair of distinct bit strings mapped to the same output by a hash function.
A hash function is preimage-resistant if given any y in the range of h (for which a cor-
responding input is not known), it is computationally infeasible to ﬁnd any preimage x∗
such that h(x∗) = y.

Section 15.4
HASH FUNCTIONS
1097
A hash function is weak collision-resistant if given any one input x, it is computa-
tionally infeasible to ﬁnd a second preimage x∗̸= x such that h(x) = h(x∗).
A hash function is strong collision-resistant if it is computationally infeasible to ﬁnd
any two distinct inputs x and x∗such that h(x) = h(x∗).
A one-way hash function (OWHF) is a function h that maps arbitrary length inputs
to ﬁxed length outputs, and has the properties of preimage-resistance and weak collision-
resistance.
A collision-resistant hash function (CRHF) is a function h that maps arbitrary
length inputs to ﬁxed length outputs, and has the property of strong collision-resistance.
An n-bit hash function is said to have ideal security if the following properties hold:
• Given a hash output, producing both a preimage and a second preimage given a
ﬁrst, requires approximately 2n operations;
• Producing a collision requires approximately 2n/2 operations.
Facts:
1. The basic idea is that a hash-value serves as a compact representative image (some-
times called a digital ﬁngerprint or message digest) of the input string, and can be used
as if it were uniquely identiﬁable with that string.
2. The problem of checking the integrity of the potentially large original input is reduced
to verifying that of a small, ﬁxed-size hash-value.
3. A hash-value should be uniquely identiﬁable with a single input in practice, and
collisions should be computationally diﬃcult to ﬁnd.
4. While the utility of hash functions is widespread, the most common cryptographic
uses are with digital signatures and for data integrity.
5. Regarding digital signatures, long messages are typically hashed ﬁrst, and then the
hash-value is signed rather than signing individual blocks of the original message. Ad-
vantages of this oversigning the individual blocks of the original message directly include
eﬃciency with respect to both time and space.
6. Regarding data integrity, hash functions together with appropriate additional tech-
niques can be used to verify the integrity of data. Speciﬁc integrity applications include
virus protection and software distribution.
7. MACs (§15.6.2) are a special class of hash functions which take in addition to message
input a secret key as a second input, allowing for the veriﬁcation of both data integrity
and data origin authentication.
8. Given a hash function h and an input x, h(x) should be easy to compute.
9. The complete speciﬁcation of h is usually assumed to be publicly available.
10. Collision-resistance is required for applications such as digital signatures and data
integrity. Otherwise an adversary might ﬁnd two messages x and x′ that have the same
hash-value, obtain a signature on x, and claim it as a signature on x′.
11. Depending on the intended application and the susceptibility of the environment to
certain attacks, weak or strong collision-resistance may be required.
12. There are no known instances of functions that have been proven to be one-way,
that is, for which it can be proven (without assumptions) that ﬁnding a preimage is
diﬃcult. However, it would be most surprising if such functions indeed did not exist.
All instances of “one-way functions” given to date should thus properly be qualiﬁed as
“conjectured” or “candidate” one-way functions.

1098
Chapter 15
CRYPTOGRAPHY
13. Particular hash functions diﬀer in the nature of the compression function and pre-
processing of the input.
14. A typical usage for data integrity is as follows:
• The hash-value corresponding to a particular input is computed at some point in
time;
• The integrity of this hash-value is then protected in some manner;
• At a subsequent point in time, to verify the input data has not been altered, the
hash-value is recomputed, using purportedly the same input, and compared for
equality with the original hash-value.
15.4.2
CONSTRUCTION AND METHODS
In general, hash functions are built by processing small blocks of data iteratively. Two
common construction methods account for the majority of existing secure hash functions.
Facts:
1. Merkle-Damg˚ard construction: Most hash functions process ﬁxed-size blocks of the
input iteratively as follows:
• A prespeciﬁed starting value or initializing value (IV) is deﬁned.
• The hash input x = x1x2 . . . xt of arbitrary ﬁnite length is divided into ﬁxed-
length n-bit blocks xi. This preprocessing typically involves appending extra
bits (padding) as necessary to extend the input to an overall bit length that is
a multiple of the blocklength n. The padding also often includes a partial block
indicating the bit length of the unpadded input.
• Each block xi is then used as input to a simpler hash function f, called an m-bit
compression function, which computes a new intermediate result of some ﬁxed
bit length m as a function of the previous m-bit intermediate result (initially
the IV) and the block xi. Letting Hi denote the partial result after the ith
stage, the hash h(x) of an input x = x1x2 . . . xt is deﬁned as follows:
H0 = IV ;
Hi = f(Hi−1, xi), 1 ≤i ≤t;
h(x) = Ht.
• Hi−1 serves as the chaining variable between stages i −1 and i.
2. Matyas-Meyer-Oseas construction:
This construction works with any block cipher
such as AES. Let E be an n-bit block cipher parameterized by a symmetric key k. Let
g be a function that maps an n-bit string to a key k suitable for E. Fix an initial value
IV . The following algorithm is then an n-bit hash function which, given any input string
x, outputs an n-bit hash h(x):
• Divide x into n-bit blocks and pad if necessary by some method such that all
blocks are complete, yielding a padded message of t n-bit blocks x1x2 . . . xt;
• Deﬁne h(x) = Ht, where
H0 = IV ;
Hi = Eg(Hi−1)(xi) ⊕xi, 1 ≤i ≤t.
This is believed to be a one-way hash function requiring 2n operations to ﬁnd a preimage,
and 2n/2 operations to ﬁnd a collision.
For underlying ciphers, such as DES, which
have relatively small blocklength (for example, with blocks of no more than 64 bits),
this is not a collision-resistant hash function since 232 operations is well within current
computational capability.

Section 15.4
HASH FUNCTIONS
1099
15.4.3
FAMILIES OF HASH FUNCTIONS
Families of unkeyed hash functions were introduced over the years as cryptographic
primitives.
These functions were updated in response to increases in computational
power, new attacks, as well as industrial needs.
Facts:
1. The MD Message-Digest Algorithm Family was proposed by Rivest et al. MD2 was
proposed in 1989 [Ka92]. The input is in blocks of 128 bits with an output size of the
same length. MD2 is considered historic. In CVE-2009-2409 [Cv09], security updates
were issued to disable MD2 in Firefox, GnuTLS before 2.6.4 and 2.7.4, OpenSSL 0.9.8
through 0.9.8k, and other products that support MD2 with X.509 certiﬁcates.
2. MD4 was an update in 1990 [Ri92]. The input block size is 512 bits with an output
size of 128 bits. As of 2011, MD4 was considered “historic” [TuCh11].
3. MD5 was proposed in 1992 as a replacement of MD4. The algorithm follows a Merkle-
Damg˚ard construction with the same input and output parameters as MD4. In 2013,
Xie, Liu, and Feng demonstrated a collision attack in 218 operations [XiLiFe13]. It is
advised that MD5 not be used [Vu08].
4. MD6 was ﬁrst published in 2008 [Ri08] by Rivest et al. as a proposal for the NIST
SHA-3 competition. It uses a Merkle tree-like structure to make use of parallel com-
putation of hashes for long inputs. MD6 did not advance to the second round of the
competition.
5. The Secure Hash Algorithm (SHA) is a family of hash functions published by NIST.
6. SHA-0, published in 1993, produces a hash of 160 bits with inputs in 512-bit blocks.
The algorithm follows a Merkle-Damg˚ard construction. It was quickly withdrawn and
replaced by SHA-1 in 1995. In 2008, Manuel and Peyrin [MaPe08] showed that a SHA-0
collision can be found in 233.6 operations.
7. SHA-1 has the same input and output parameters as SHA-0, and follows a Merkle-
Damg˚ard construction. In 2005, Wang et al. [WaYu05] presented a collision attack with
complexity 263. In 2011, NIST [BaRo11] declared that “SHA-1 shall not be used for
digital signature generation after December 31, 2013.”
8. A SHA-1 collision was demonstrated in 2017 with a complexity of 263.1, or 100 GPU
years, at a cost of $110,000 by researchers at CWI Amsterdam and Google Research
[StEtal17].
9. SHA-2, published in 2001, is itself a family of six hash functions with various in-
put/output sizes. These functions follow a Merkle-Damg˚ard construction with a Davies-
Meyer compression function. SHA-224 and SHA-256 have block sizes of 512 bits and
output a digest (hash value) of 224 and 256 bits, respectively, in 64 rounds. SHA-384,
SHA-512, SHA-512/224, and SHA-512/256 have block sizes of 1024 bits and output di-
gests of size 384, 512, 224, and 256 bits, respectively in 80 rounds. Reduced-rounds
attacks were found in 2012 [BiEtal11], [KhReSa12].
10. SHA-3 is a result of a NIST call for an alternative to SHA-2. Submissions were
accepted until the end of 2008. The standard was ultimately released in August 2015
[Fi15].
The algorithm uses a sponge construction [BeEtal07]. SHA3-224, SHA3-256,
SHA3-384, SHA3-512 have block sizes of 1152, 1088, 832, 576 and output digests of size
224, 256, 384, and 512, respectively.

1100
Chapter 15
CRYPTOGRAPHY
15.5
PUBLIC KEY CRYPTOGRAPHY
The invention of public key cryptography in the 1970s signiﬁcantly changed the ﬁeld
of cryptography. As a result, many security services can now be realized. In a private
key system, the encryption and decryption keys are the same, or the knowledge of the
encryption key implies an easy derivation of the decryption key. On the other hand, in
a public key cryptosystem there is a public key portion and a private key portion. The
public key is not kept secret and allows everyone to use it to encrypt messages to the
owner of the private key. The private key is known only to the receiver and is used for
decryption of the ciphertext.
A number of public key cryptosystems were proposed in the past 40 years. Soon after
their appearance, cryptanalysts worked hard to break them. Most systems were broken
quickly. Those that have withstood sustained attacks are generally believed to meet the
security requirements of a public key cryptosystem. However, researchers still continue to
look for ways to break them and sometimes succeed quite a while after the systems were
ﬁrst proposed. In the following, we introduce the signiﬁcant systems that have survived
existing attacks and appeared hard to break, and that are also reasonably eﬃcient.
15.5.1
INTRODUCTION TO PUBLIC KEY CRYPTOSYSTEMS
Deﬁnitions:
Let Ep and Ds be the encryption and decryption algorithms in a public key cryptosystem
with a public key p and a private key s, respectively. To encrypt plaintext M, one can
generate the ciphertext C = Ep(M). Upon receiving ciphertext C, the receiver, using
the private key s, decrypts by evaluating M ′ = Ds(C). Given the public key p, it should
be hard to compute the private key s.
Non-adaptive chosen-ciphertext attack security (CCA1): an attacker that can
inquire multiple times the plaintext for any adaptively chosen ciphertext is unable to
recover the plaintext for a new challenge ciphertext C. Here non-adaptivity means that
no query is based on C (i.e., no decryption query is allowed after C is given).
Adaptive chosen-ciphertext attack security (CCA2): given the challenge cipher-
text C in CCA1 security, the attacker can continue the decryption requests adaptively,
except for C itself, and is unable to recover the plaintext.
Facts:
1. The invention of public key cryptography is credited to Diﬃe and Hellman [DiHe76].
However, it was ﬁrst conceived in 1970 by James H. Ellis in secret work at the U.K.’s Gov-
ernment Communications Headquarters (GCHQ). This earlier discovery was not made
public until 1997.
2. In 1978 Ron Rivest, Adi Shamir, and Leonard Adleman invented the ﬁrst public key
cryptosystem that has withstood attack, the RSA cryptosystem (see §15.5.2). However,
in 1997 it was revealed that in 1973 Cliﬀord Cocks discovered the equivalent of the RSA
cryptosystem in secret work at the GCHQ.
3. For a cryptosystem to be useful, it is necessary that M ′ = M. In addition, given the
ciphertext C and public key p, it should be hard to derive the plaintext M. That is, the
cryptosystem should be resistant to ciphertext-only attack.

Section 15.5
PUBLIC KEY CRYPTOGRAPHY
1101
4. CCA2 is a desired property, but can be diﬃcult to achieve in practice.
5. Since the late 1970s many diﬀerent public key cryptosystems have been developed.
The mostly widely used are the RSA cryptosystem (§15.5.2), the ElGamal cryptosys-
tem (§15.5.3), and the elliptic curve cryptosystem (§15.5.4). Other important public key
cryptosystems are the NTRU cryptosystem (§15.5.5), the Cramer-Shoup cryptosystem
(§15.5.6), the Paillier cryptosystem (§15.5.7), the Goldwasser-Micali probabilistic cryp-
tosystem (§15.5.8), identity-based encryption (§15.5.9), and attribute-based encryption
(§15.5.10).
6. It will be possible to quickly break many, but not all, public key cryptosystems once
quantum computing becomes practical. This is noted in the coverage of the public key
cryptosystems in this section.
15.5.2
RSA CRYPTOSYSTEM
The RSA cryptosystem is the best known and most widely used public key cryptosystem
for secure data transmission. Its suitability as a public key cryptosystem is based on the
diﬃculty of factoring large integers compared with the ease of ﬁnding large primes.
Facts:
1. Key Generation.
• Randomly select two large and distinct primes p and q. Compute n = pq and
φ(n) = (p −1)(q −1).
• Select an arbitrary value e with 1 < e < φ(n) so that gcd(e, φ(n)) = 1.
• Compute d with 1 < d < φ(n) such that ed ≡1 (mod φ(n)).
• The public key is (n, e) and the private key is d.
2. Encryption. To encrypt a plaintext M ∈{0, 1, . . ., n −1}, compute C = M e mod n.
3. Decryption. To decrypt M from C, compute M = Cd mod n.
4. The correctness of RSA follows from elementary number theory, in particular from
Euler’s theorem (see §4.3.1).
5. Some computational notes:
• The public exponent e should be coprime with φ(n) = (p −1)(q −1) and can be
checked using the Euclidean algorithm (see §4.2.2).
• The secret exponent d can be computed from (e, φ(n)) using the extended Eu-
clidean algorithm (see §4.2.2).
• To compute M e mod n and Cd mod n, many fast modular exponentiation algo-
rithms such as the square-and-multiply algorithm are available [MevOVa10].
6. The hardness of factoring is the security basis of RSA. Some particular concerns with
the choice of parameters are the following:
• When |p −q| is small, n will suﬀer from Fermat factoring attack.
• When |p −q| < n1/4, Coppersmith [Co97] presented an attack to factor n.
• If p −1 or q −1 has small factors only, then n can be factored using Pollard’s
p −1 algorithm [Po74]; when p + 1 or q + 1 has small factors only, then n can
be factored using Williams’ p + 1 algorithm [Wi82].

1102
Chapter 15
CRYPTOGRAPHY
7. One might attempt to use a small d for better decryption eﬃciency. However, this us-
age is vulnerable to the factoring attacks by Wiener [Wi90], Boneh and Durfee [BoDu99],
and Howgrave-Graham and Seifert [HoSe99]. Hinek and Lam [HiLa10] showed that in
practice, three instances of RSA encryption are suﬃcient to factor n when d < n0.33 with
at least 93% success rate. Therefore, it is advisable not to use small d.
8. Due to the nature of factoring algorithms, p and q need to be of similar size to keep
the system secure.
9. RSA assumption and factoring assumption.
• Security against ciphertext-only attack requires that it be hard to derive M from
C and (n, e). This is known as the RSA assumption.
• Given n, if one can ﬁnd p and q, then one can easily compute d from (n, e). So
the security of RSA needs the assumption that it is hard to factor n. This is
known as the factoring assumption, and is stronger than the RSA assumption.
10. Under a generic computation model over the ring Zn, the factoring assumption is
equivalent to the RSA assumption [AgMa09]. This gives evidence for the widely accepted
belief that they are equivalent in the normal computation model.
11. Under the RSA assumption, RSA is secure against ciphertext-only attack. However,
it is not CCA2 secure due to its multiplicative property.
12. To generate the keys e and d, one must ﬁrst generate two primes p and q of similar
bitlengths. There are several eﬃcient probabilistic primality tests available, such as the
Miller-Rabin test and the Solovay-Strassen test (§4.4).
13. There are several general purpose factoring algorithms. See §4.5 for details.
14. In 1991, RSA Laboratories began the RSA Factoring Challenge to stimulate research
in factoring products of two large primes. One goal of this challenge has been to provide
guidance as to the minimum recommended size of an RSA modulus n = pq.
15. As of 2016, the largest factored RSA modulus n has bitlength 768 (RSA-768).
Therefore, the choice of n needs to be signiﬁcantly larger. Currently, a 2048-bit (617
decimal digits) modulus is recommended for general applications and a 3072-bit (925
decimal digits) modulus for high-security applications.
16. If quantum computers are developed, RSA will be completely broken, since the
factoring problem can be eﬃciently solved on quantum computers. However, building
such computers does not seem realistic in the near future.
17. An important application of RSA is the digital envelope. A digital envelope is a
framework for data encryption, in which the data is encrypted under a secret key using
a symmetric key cryptosystem such as AES, while this secret key is encrypted using a
public key cryptosystem, such as RSA. The details are described in PKCS #7 [Ka98]. A
well-known application of the digital envelope is Pretty Good Privacy (PGP), a system
for email security.
18. An important use of the RSA system is to guarantee the authenticity of a message
using a digital signature. See §15.6.1 for details.
Examples:
1. Suppose that p = 53 and q = 83, so that n = 53 · 83 = 4399 and φ(n) = (53 −
1)(83 −1) = 4624, and further suppose that e = 23. Then (n, e) = (4399, 23) is a valid
public key for RSA because gcd(φ(n), e) = gcd(4624, 23) = 1, as can be veriﬁed using
the Euclidean algorithm. Using the extended Euclidean algorithm, it can be shown that
23−1 mod 4624 = 927. Hence, the exponent in the private key corresponding to the

Section 15.5
PUBLIC KEY CRYPTOGRAPHY
1103
public key (4399, 23) is d = 927. (Note that for practical use, the primes p and q need
to be hundreds of digits long.)
2. In Example 1, to encrypt M = 19, compute C = M e mod n = 2556. In order to
decrypt C = 2556, compute M = Cd mod n = 19.
15.5.3
ELGAMAL CRYPTOSYSTEM
The ElGamal cryptosystem is developed from the Diﬃe-Hellman key-exchange protocol.
The system can be deﬁned over any cyclic group.
Its security is based on both the
discrete logarithm problem and the computational Diﬃe-Hellman problem.
Facts:
1. The ElGamal cryptosystem was invented by Taher ElGamal [El85] in 1985.
2. Key Generation.
• Let p be a large prime. Let g be a generator of the multiplicative group Z∗
p.
• Randomly select d with 1 ≤d ≤p −2, and compute h = gd mod p.
• The public key is (p, g, h) and the private key is d.
3. Encryption. To encrypt a plaintext M ∈⟨g⟩, randomly select k with 1 ≤k ≤p −2
and then compute X = gk mod p and Y = Mhk mod p. The ciphertext is C = (X, Y ).
4. Decryption. To decrypt (X, Y ), the receiver computes M ′ = Y X−d mod p.
5. The correctness of the algorithm follows from the fact that Y M −1 = hk = gkd = Xd.
6. Given p, g, and h, if one can determine the private key d, then the ciphertext can be
decrypted. Thus, the security of ElGamal encryption depends on the hardness of ﬁnding
d from (p, g, h). This is called the Discrete Logarithm Problem (DLP).
7. The ciphertext-only security of ElGamal encryption requires that it be hard to de-
termine Z = hk mod p from (p, g, h, X). This is called the Computational Diﬃe-Hellman
Problem (CDHP). It is widely believed that the CDHP can only be solved by ﬁrst solving
the DLP.
8. The assumption that the DLP is hard does not hold in every ﬁnite cyclic group. For
instance, the DLP is easy in the additive group Zn.
9. The DLP is insecure under a quantum attack.
Examples:
1. Let p = 3767 and choose g = 7. Let d = 15. Then h = 715 mod 3767 = 3467.
The public key is (p, g, h) = (3767, 7, 3467) and the private key is d = 15. Note that in
practice, the parameters should be larger.
2. In Example 1, to encrypt message M = 125, choose a random value k = 87, compute
X = 787 mod 3767 = 3113 and Y = Mhk mod 3767 = 125 · 346787 mod 3767 = 2379.
Thus, the ciphertext is (3113, 2379). To decrypt the ciphertext (3113, 2379), compute
M ′ = Y X−d mod 3767 = 2379 · 3113−15 mod 3767 = 2379 · 464 mod 3767 = 125.

1104
Chapter 15
CRYPTOGRAPHY
15.5.4
ELLIPTIC CURVE CRYPTOSYSTEM
The elliptic curve cryptosystem is a adaptation of the ElGamal cryptosystem over a
group of elliptic curves over a ﬁnite ﬁeld. A signiﬁcant advantage of this system is that
it requires a signiﬁcantly smaller key size compared to other systems in order to achieve
the same level of computational security.
Facts:
1. Elliptic curve cryptography (ECC) is the elliptic curve analogue of discrete logarithm
cryptography. It was invented by Neal Koblitz [Ko87] and Victor Miller [Mi85] in 1985.
(See §4.11 for the basics of elliptic curve group operations.)
2. Key Generation.
• Select a large prime p and elliptic curve group E(Fp).
• Let P be a point of prime order n in E(Fp) and randomly select an integer d,
1 ≤d ≤n −1. Compute Q = dP.
• The public key is (p, E, n, P, Q) and the secret key is d.
3. Encryption. To encrypt message M ∈⟨P⟩, select a random integer k from [1, n −1];
the ciphertext is C = (C1, C2) = (kP, M + kQ).
4. Decryption. To decrypt (C1, C2), compute M = C2 −dC1.
5. Decryption is correct because C2 −dC1 = M + kQ −dkP = M + k(dP) −dkP = M.
6. There are a number of standards on the use of elliptic curve cryptography, covering
the choice of curves and various implementation issues.
• ANSI X9.62 (2005) speciﬁes Elliptic Curve Digital Signature Algorithm (ECDSA).
The standard includes methods and criteria for public and private key genera-
tion, and procedural controls required for the secure use of the algorithm with
these keys.
• ANSI X9.63 (2011) speciﬁes key agreement and key transport schemes using el-
liptic curves for use in symmetric schemes.
• IEEE P1363 (2000, 2004) includes key agreement, signature, and encryption
schemes.
• NIST FIPS 186-4 (2013), the Digital Signature Standard (DSS) speciﬁes DSA,
RSA, and ECDSA.
• NSA Suite B (2005) includes a set of cryptographic algorithms proposed by the Na-
tional Security Agency (NSA). Included are ECDSA and Elliptic Curve Diﬃe-
Hellman Key Agreement (ECDH). In 2015, NSA announced preliminary plans
for a future transition to quantum-resistant algorithms.
7. The security of ECC is based on the diﬃculty of the Elliptic Curve Discrete Logarithm
Problem (ECDLP): namely, given point P ∈E(Fq) of prime order n and a point Q ∈⟨P⟩,
ﬁnd an integer ℓsuch that Q = ℓP.
• Using Pollard’s rho attack. the ECDLP can be solved in √n time.
• The index-calculus attack, which normally applies to the discrete logarithm prob-
lem over a ﬁnite ﬁeld, is not generally applicable to the ECDLP.
• In special cases, one can compute an isomorphism from ⟨P⟩to a cyclic group G,
where the discrete logarithm problem can be solved by algorithms running in
subexponential or faster time. The following attacks have been discovered:

Section 15.5
PUBLIC KEY CRYPTOGRAPHY
1105
• Prime-ﬁeld-anomalous curve. A curve E of prime order p deﬁned over
Fp is isomorphic to the additive group F+
p . The attack was found in
1998 [Se98], [Sm99], [TaEtal98].
• Weil and Tate pairing attacks. When E is deﬁned over Fq and gcd(n, q) =
1, there is an isomorphism between E and the multiplicative subgroup
of order n of F∗
qk of some extension ﬁeld Fqk. However, k is very large
in general and so the attack is easily circumvented.
• Weil descent. This attack reduces the ECDLP in an elliptic curve de-
ﬁned over a binary ﬁeld F2m to the discrete logarithm problem in the
Jacobian of a hyperelliptic curve deﬁned over a proper subﬁeld of F2m.
Thus far, the attack is eﬀective only in certain elliptic curves over F2m
where m is composite. Therefore, for cryptographic purposes, curves
deﬁned over F2m where m is composite should be avoided [Fr01], [GaH-
eSm02], [JaMeSt01], [MaMeTe02], [MeQu01], [MeTe06], [MeTeWe04].
8. Since ECC is based on the elliptic curve discrete logarithm problem, ECC is insecure
against quantum attacks.
Examples:
1. Let p = 23 and consider E(F23) : y2 = x3 + x + 1. Let P = (3, 10) and d = 3. Then
Q = 2P + P = (7, 12) + (3, 10) = (−4, 5).
2. In Example 1, to encrypt M = (9, 7), choose k = 2, whence C1 = kP = (7, 12) and
C2 = kQ + M = 2(7, 12) + (9, 7) = (−6, 3) + (9, 7) = (3, 13). To decrypt (C1, C2) =
((7, 12), (3, 13)), compute dC1 = 3(7, 12) = (−6, 3). Thus, C2 −dC1 = (3, 13) −(−6, 3) =
(3, 13) + (−6, 20) = (9, 7) = M.
15.5.5
NTRU CRYPTOSYSTEM
NTRU is a cryptosystem based on lattices. It is one of the very few cryptosystems that
is currently resistant to quantum attacks.
Facts:
1. NTRU is a lattice-based public key cryptosystem proposed by J. Hoﬀstein, J. Pipher,
and J. Silverman in 1996.
2. For a prime N, let R = Z[x]/(xN −1). Let F(x), G(x) ∈R with F(x) = PN−1
i=0 Fixi
and G(x) = PN−1
j=0 Gjxj. Then multiplication ⊙in R can be implemented as F(x) ⊙
G(x) = H(x) = PN−1
k=0 Hkxk, where Hk = P
i+j=k mod N FiGj.
3. Key Generation.
• Select a large prime N, and coprime integers p and q with q ≫p.
• Choose random polynomials f(x), g(x) ∈R with coeﬃcients in {−1, 0, 1}. Com-
pute fp(x), fq(x) such that f(x)⊙fp(x) = 1 mod p and f(x)⊙fq(x) = 1 mod q.
• Deﬁne h(x) = pfq(x) ⊙g(x) mod q. Then the public key is (N, p, q, h) and the
private key is (f(x), fp(x)).
4. Encryption. Consider message m(x) ∈R with coeﬃcients in {−1, 0, 1}. Generate a
random polynomial r(x) ∈R with small coeﬃcients (relative to q) and compute c(x) =
r(x) ⊙h(x) + m(x) mod q. The ciphertext is c(x).

1106
Chapter 15
CRYPTOGRAPHY
5. Decryption.
To decrypt ciphertext c(x), the receiver computes A(x) = f(x) ⊙
c(x) mod q so that the coeﬃcients of A(x) are in [−q/2, q/2]. Recover the plaintext
m′(x) = fp(x) ⊙A(x) mod p.
6. The correctness of the decryption can be demonstrated as follows. Notice that
A(x) = f(x) ⊙r(x) ⊙(pfq ⊙g(x)) + f(x) ⊙m(x) mod q.
Simplifying, we have A(x) = pr(x)⊙g(x)+f(x)⊙m(x) mod q. Since p and the coeﬃcients
of r(x), f(x), g(x), m(x) are small relative to q, it holds that A(x) = pr(x)⊙g(x)+f(x)⊙
m(x). Hence, m′(x) = fp(x) · A(x) mod p = fp(x) ⊙f(x) ⊙m(x) mod p = m(x).
7. The security of NTRU relies on the diﬃculty of ﬁnding fq(x) and g(x) from h(x).
8. NTRU security is based on the hardness of the Shortest Vector Problem (SVP),
ﬁnding the shortest vector in a lattice.
9. Currently, the most powerful attack on NTRU is the Lenstra-Lenstra-Lov´asz (LLL)
algorithm.
However, the attack can be prevented when the security parameters are
suﬃciently large.
10. NTRU is not known to be compromised by any quantum algorithm. This is an
advantage over the RSA, ElGamal, and ECC schemes.
Example:
1. Let N = 11, p = 3, q = 31, f(x) = x8 −x7 + x3 + x −1, g(x) = x3 −x + 1. We
have fq(x) = 8x10 + 11x9 + 20x8 + 10x7 + 28x6 + 26x5 + 30x3 + 17x2 + 11x + 26 and
fp(x) = 2x10 + 2x9 + 2x8 + x6 + 2x4 + x3 + x2 + 2. Thus, h(x) = pfq(x) ⊙g(x) mod q =
21x10 + 26x9 + 15x8 + 8x7 + 3x6 + 5x5 + 5x4 + 24x3 + 11x2 + 19x + 21. The public key
is (N, p, q, h) and the private key is (f(x), fp(x)).
2. In Example 1, to encrypt the message m(x) = x10 −x9 +x8 +x+1, choose a random
polynomial r(x) = x9 + x8 + x + 1 and compute the ciphertext c(x) = r(x) ⊙h(x) +
m(x) mod q = 16x10 + 18x9 + 4x8 + 27x7 + 18x6 + 2x5 + 9x4 + 12x3 + 9x2 + 8x + 16. To
decrypt c(x), compute A(x) = f(x)⊙c(x) mod q = −5x10+3x9+2x8−2x6 +2x5+3x4+
4x3 −x2 + 2x + 7. Thus, m′(x) = fp(x) ⊙A(x) mod p = x10 −x9 + x8 + x + 1 = m(x).
15.5.6
CRAMER-SHOUP CRYPTOSYSTEM
The Cramer-Shoup cryptosystem is an extension of the ElGamal cryptosystem. It is the
ﬁrst practical scheme that is proven to be CCA2 secure without random oracles.
Facts:
1. The ﬁrst practical CCA2-secure public key cryptosystem was proposed by R. Cramer
and V. Shoup [CrSh98] in 1998.
2. The security is based on the Decisional Diﬃe-Hellman (DDH) assumption: it is hard
to distinguish (g, h, gx, hx) from (g, h, gx, hy), where g, h ∈Z∗
p have large prime order q
and x, y are randomly selected from Zq.
3. Key Generation.
• Let p be a large prime. Let G be a subgroup of Z∗
p of large prime order q. (All
arithmetic in G is performed modulo p.) Randomly select generators g1, g2 ∈G.
Let H be a hash function.
• Randomly select x1, x2, y1, y2, z ∈Zq and compute c = gx1
1 gx2
2 , d = gy1
1 gy2
2 , h =
gz
1.

Section 15.5
PUBLIC KEY CRYPTOGRAPHY
1107
• The public key is (p, g1, g2, c, d, h, H) and the private key is (x1, x2, y1, y2, z).
4. Encryption. To encrypt the plaintext m ∈G, choose a random r ∈Zq and compute
u1 = gr
1, u2 = gr
2, e = mhr, α = H(u1, u2, e), v = crdrα. The ciphertext is (u1, u2, e, v).
5. Decryption. To decrypt the ciphertext (u1, u2, e, v), the receiver ﬁrst computes α =
H(u1, u2, e) and veriﬁes whether v = ux1+y1α
1
ux2+y2α
2
. If the veriﬁcation fails, he rejects;
otherwise, he outputs m′ = e/uz
1.
6. The veriﬁcation of the decryption step follows, since ux1+y1α
1
ux2+y2α
2
= ux1
1 ux2
2
·
(uy1
1 uy2
2 )α = crdrα = v and e = mhr = mgzr
1 = muz
1.
7. CCA2 security is attained because to pass the veriﬁcation the encrypter has to con-
struct u1, u2, v that are consistent with a single r. To achieve this, he needs to know
r (hence the plaintext e/hr). Thus, the decryption requests in a CCA2 attack are not
useful.
8. Plaintext Awareness (PA) refers to the property that one must know the underlying
plaintext when creating a ciphertext. This property is useful to guarantee some privacy
properties such as deniability.
The plaintext awareness of this scheme was shown in
[De06]. The PA property of more general schemes appeared in [JiWa10].
9. The Cramer-Shoup cryptosystem has CCA2 security, but it assumes DDH. This
assumption does not hold under quantum attacks.
10. It is important to verify that u1, u2, e, v have prime order q. Otherwise, the scheme
may be vulnerable to attacks in some circumstances.
Examples:
1. Let p = 23, g1 = 4, g2 = 9. Let x1 = 2, x2 = 3, y1 = 5, y2 = 6, z = 7. Compute
c = gx1
1 gx2
2
= 3, d = gy1
1 gy2
2 = 13, h = gz
1 = 8. Then the public key is (p, g1, g2, c, d, h) =
(23, 4, 9, 3, 13, 8) and the private key is (x1, x2, y1, y2, z) = (2, 3, 5, 6, 7).
2. In Example 1, to encrypt the message M = 5, select the random value r = 8 and
compute u1 = gr
1 = 9, u2 = gr
2 = 13, e = mhr = 20. Suppose α = H(u1, u2, e) = 9
and so v = crdrα = 13. Thus, the ciphertext is (u1, u2, e, v) = (9, 13, 20, 13). To decrypt
the ciphertext (9, 13, 20, 13), note that ux1+y1α
1
ux2+y2α
2
= 13, which is consistent with v.
Decryption proceeds by evaluating m = e/uz
1 = 20/97 = 5.
15.5.7
PAILLIER CRYPTOSYSTEM
The Paillier cryptosystem is an encryption system that has been proposed as a primitive
in electronic voting and cloud computing.
Facts:
1. The system was proposed by P. Paillier in 1999 [Pa99].
2. The security is based on the intractability of computing nth residue classes.
3. Key Generation.
• Randomly select two large primes p, q of similar size so that gcd(pq, (p−1)(q−1)) =
1. Let n = pq and λ = (p −1)(q −1).
• Randomly select g ∈Z∗
n2 such that the order of g is a multiple of n. This can be
checked by determining if gcd(L(gλ mod n2), n) = 1, where L(x) = x−1
n .
• The public key is (n, g) and the private key is (p, q).

1108
Chapter 15
CRYPTOGRAPHY
4. Encryption. To encrypt the message m ∈Zn, randomly select r ∈Z∗
n. Compute the
ciphertext c = gm · rn mod n2.
5. Decryption. To decrypt the ciphertext c ∈Zn2, compute
m′ = L(cλ mod n2)
L(gλ mod n2) mod n.
6. The correctness of the algorithm makes use of the fact that the order of g is a multiple
of n in Z∗
n2, together with using the binomial theorem on (1 + n)x mod n2. Details can
be found in [Pa99].
7. The ciphertext is double the size of the message. This is a considerable practical
disadvantage.
8. The system is additively homomorphic (§15.6.4).
9. The Paillier cryptosystem is not secure under a quantum attack.
Examples:
1. Let p = 149, q = 179. Then n = pq = 26671, λ = (p −1)(q −1) = 26344. Choose
g = 5.
Note that gcd(L(gλ mod n2), n) = gcd(1213, 26671) = 1.
The public key is
therefore (n, g) = (26671, 5) and the private key is (p, q) = (149, 179).
2. In Example 1, let the message be m = 67. Randomly choose r = 81. The ciphertext
is c = gm · rn mod n2 = 567 · 8126671 mod 266712 = 577656191.
3. To decrypt the ciphertext 577656191, compute
m′ = L(57765619126344 mod 266712)
L(526344 mod 266712)
mod 26671 = 1258
1213 mod 26671 = 67.
Note that the denominator above was calculated in Example 1 during the key setup
stage.
15.5.8
GOLDWASSER-MICALI CRYPTOSYSTEM
The Goldwasser-Micali scheme is a probabilistic public key cryptosystem that is also the
ﬁrst provably secure system.
Facts:
1. The system was proposed by S. Goldwasser and S. Micali in 1982 [GoMi82].
2. The security is based on the intractability of the Quadratic Residuosity Problem.
Namely, let n ≥3 be an odd composite integer and let Jn be the set of all a ∈Z∗
n having
Jacobi symbol 1. Given b ∈Jn, determine whether or not b is a quadratic residue modulo
n.
3. Key Generation.
• Randomly select two large primes p and q of roughly the same size. Compute
n = pq.
• Select y ∈Zn such that the Legendre symbol
  y
p

=
  y
q

= −1, so that the Jacobi
symbol
  y
n

= 1.
• The public key is (n, y) and the private key is (p, q).

Section 15.5
PUBLIC KEY CRYPTOGRAPHY
1109
4. Encryption. Let the message m be represented as a binary string m = m1m2 . . . mt.
• For each bit mi, randomly select xi ∈Z∗
n. If mi = 1, then compute ci = yx2
i mod
n; otherwise, compute ci = x2
i mod n.
• The ciphertext is c = (c1, c2, . . . , ct).
5. Decryption. To decrypt the ciphertext c = (c1, c2, . . . , ct),
• For each ci, decide whether it is a quadratic residue modulo n by evaluating the
Legendre symbol ei =
  ci
p

. If ei = 1, then set m′
i = 0. Otherwise, set m′
i = 1.
• The decrypted message is m′ = m′
1m′
2 . . . m′
t.
6. Correctness. When mi = 0, ci is a quadratic residue modulo n. However, when
mi = 1, ci is a pseudosquare modulo n and is revealed when computing the Legendre
symbol modulo p.
7. The system is insecure under a quantum attack due to the ability to factorize n.
8. The system was developed to demonstrate a provably secure system. Similar to the
Paillier system in §15.5.7, message expansion is a practical drawback, in this case with
expansion by a factor of log2 n.
15.5.9
IDENTITY-BASED ENCRYPTION
Identity-based encryption uses some unique information about the identity of a user as
the public key, thus eliminating the need to manage public key certiﬁcates. It uses some
central authority to distribute the secret key to the user.
Facts:
1. Identity-based encryption (IBE) is a public key cryptosystem that uses an arbitrary
string (e.g., email address or name) as the public key. It was ﬁrst proposed by A. Shamir
[Sh84b] in 1984. The motivation of this system is to remove the task of managing public
key certiﬁcates. The ﬁrst fully functional construction was proposed by D. Boneh and
M. Franklin [BoFr01] in 2001.
2. Let G1 and G2 be multiplicatively-written cyclic groups of prime order p. A bilinear
pairing ˆe on (G1, G2) is an eﬃciently computable map ˆe : G1 × G1 →G2 satisfying
• Bilinearity: ∀g, h ∈G1, ∀a, b ∈Zp, ˆe(ga, hb) = ˆe(g, h)ab; and
• Non-degeneracy:
ˆe(g, h) = 1 implies that g = 1 or h = 1.
3. Key Generation. The system generator determines p, G1, G2, ˆe, chooses a generator
g of G1, chooses two hash functions G : {0, 1}∗→{0, 1}n and H : {0, 1}∗→G1, a
random s ∈Zp, and sets Ppub = gs. The system parameters are
(p, g, G1, G2, ˆe, G, H, Ppub).
The system master key is s.
4. Extract. For any identity ID, the system generates its private key as dID = H(ID)s.
5. Encryption. To encrypt M ∈{0, 1}n under identity ID, randomly select r ∈Zp and
compute the ciphertext as
(C1, C2) =
 gr, M ⊕G[ˆe(H(ID)r, Ppub)]

.

1110
Chapter 15
CRYPTOGRAPHY
6. Decryption. To decrypt (C1, C2) for identity ID, compute
M ′ = C2 ⊕G[ˆe(dID, C1)].
7. The correctness of this scheme can be easily seen. In the encryption step,
ˆe(H(ID)r, Ppub) = ˆe(H(ID), g)rs = ˆe(H(ID)s, gr) = ˆe(dID, C1),
where the last value is the quantity evaluated in the decryption step.
8. Security relies on the Computational Bilinear Diﬃe-Hellman (CBDH) assumption:
given (ga, gb, gc), it is hard to compute ˆe(g, g)abc. If this assumption does not hold, then
from gr, Ppub = gs and H(ID) = gw (for unknown w), an attacker can compute ˆe(g, g)rsw
which is ˆe(H(ID)r, Ppub). This can then be used to recover M.
9. The Boneh-Franklin scheme assumes that the output of hash functions G and H is
random so that no information about the input can be deduced from the output. Such
idealized hash functions are called random oracles.
10. Although this assumption is reasonable in practice, it is known that security in the
random oracle model does not necessarily hold when the random oracle is replaced by a
concrete hash function.
11. CCA2-secure IBE without random oracles was constructed by Boneh and Boyen
[BoBo04].
12. The Boneh-Franklin scheme is not secure against an adaptive chosen ciphertext
(CCA2) attack. Given the ciphertext (C1, C2), one can easily deduce the ciphertext for
the message M ⊕1. However, the scheme can be converted to a CCA2-secure scheme by
a Fujisaki-Okamoto transformation [FuOk99].
13. Although IBE can eliminate the need to certify public keys, it still does not remove
the need for public key management. For instance, if a user’s private key is leaked, an
oﬃcial revocation is still needed.
14. Just as with RSA, ElGamal, and elliptic curve schemes, the Boneh-Franklin scheme
is broken quickly using quantum computers.
15.5.10
ATTRIBUTE-BASED ENCRYPTION
While identity-based encryption uses one unique informational item about a user as the
public key, attribute-based encryption uses multiple attributes of a user to determine
access to decryption.
Facts:
1. Attribute-based encryption is a type of public key encryption where the user’s secret
key and the ciphertext both depend on attributes.
• Attribute examples: membership, position title, age, color, height.
• Motivation: one can decrypt a ciphertext only if the attributes for the secret key
satisfy the attribute constraints embedded in the ciphertext.
• Example: to decrypt a ciphertext for the female professors of a mathematics
department, the decryption key needs to contain attributes of “female”, “pro-
fessor”, and “mathematics department”.

Section 15.5
PUBLIC KEY CRYPTOGRAPHY
1111
• Collusion-resistant security: if an attacker collects many keys, then he can decrypt
a ciphertext only if one of these keys satisﬁes the attribute restriction of the
ciphertext. In the previous example, a female student and a male mathematics
professor cannot jointly decrypt the ciphertext.
• Application: in cloud access control, encrypted data is stored in a cloud server.
A user who retrieves a ciphertext can decrypt it if and only if he has a key
satisfying the encryption policy.
2. The concept of attribute-based encryption was ﬁrst proposed by Sahai and Waters
[SaWa05] and by Goyal et al. [GoEtal06]. In the former, a ciphertext can be decrypted
if and only if the number of common attributes embedded in the decryption key and the
ciphertext is greater than or equal to a threshold. The latter generalized the scheme:
a ciphertext can be decrypted if and only if the attributes in the ciphertext satisfy the
access structure embedded in the key.
3. The following are some basic notions:
• Let e : G1 ×G1 →G2 be a bilinear pairing, where G1 and G2 are multiplicatively-
written groups of prime order p.
• The universe of attributes is the set {1, . . ., n}. Each user u has a subset of at-
tributes Au ⊆{1, . . ., n}.
• A ciphertext C is prepared using an attribute set B; user u can decrypt C only if
|Au ∩B| ≥d for a ﬁxed threshold d.
• Lagrange interpolation. For a polynomial q(x) ∈Zp[x] of degree d −1 and a set
S ⊆Zp −{0} of size d, we have q(0) = P
i∈S q(i)λi, where λi = Q
j∈S\{i}
j
j−i.
4. The Sahai-Waters attribute-based encryption scheme [SaWa05] is described in the
following facts.
5. System Setup. The system manager selects p, G1, G2, ˆe and a generator g of G1. Then
he selects t1, . . . , tn, y randomly from Zp and deﬁnes h1 = gt1, h2 = gt2, . . . , hn = gtn,
Y = e(g, g)y. The system parameters are (p, G1, G2, ˆe, h1, . . . , hn, Y ) and the master key
is (t1, . . . , tn, y).
6. User Key Generation. For user u, randomly select a polynomial q(x) ∈Zp[x] such
that q(0) = y. For each i ∈Au, compute Di = g
q(i)
ti . Then the private key for u is
{Di}i∈Au.
7. Encryption. To encrypt m ∈G2 with attribute set B, the sender randomly selects
s ∈Zp and computes the ciphertext
(B, C0, {Ci}i∈B) = (B, mY s, {hs
i}i∈B).
8. Decryption. To decrypt a ciphertext (B, C0, {Ci}i∈B) for user u where |Au ∩B| ≥d,
user u ﬁnds a subset S ⊆Au ∩B of size d and computes
m′ =
C0
Q
i∈S ˆe(Di, Ci)λi .
9. The correctness of the decryption is immediate. Note that C0 = mY s. The denomi-
nator in the decryption step can be simpliﬁed as follows:
Q
i∈S ˆe(Di, Ci)λi = Q
i∈S ˆe(g
q(i)
ti , gsti)λi = ˆe(g, g)s P
i∈S q(i)λi = ˆe(g, g)sq(0) = Y s.

1112
Chapter 15
CRYPTOGRAPHY
10. The idea behind security is the following. To decrypt m, one needs to compute Y s.
If |Au ∩B| < d, user u does not have suﬃciently many shares to recover q(0) = y in the
exponentiation [es(g, g)]q(0) = Y s (due to s in Y s, the recovery Y s′ in other ciphertexts
will be useless). Hence, decryption will be infeasible. Any two users u1, u2 will have
diﬀerent polynomials q1(x), q2(x) when generating their private keys. Hence, it is not
productive for them to collude.
11. Security relies on the Decisional Bilinear Diﬃe-Hellman (DBDH) assumption, which
states that it is hard to distinguish (g, ga, gb, gc, ˆe(g, g)abc) from (g, ga, gb, gc, ˆe(g, g)r),
where a, b, c, r are uniformly random in Zp. This assumption does not hold under quan-
tum attacks.
15.6
CRYPTOGRAPHIC MECHANISMS
The cryptosystems described in the previous sections work with several core crypto-
graphic applications that provide immediate, basic services in communications security.
These core products are often components in high-level applications.
15.6.1
DIGITAL SIGNATURE
The notion of digital signature was invented by W. Diﬃe and M. Hellman in 1976.
The authors conjectured about the existence of the mechanism. The use of a digital
signature is to demonstrate the authenticity of an electronic document. The ﬁrst practical
algorithm was demonstrated through the RSA digital signature scheme.
Deﬁnitions:
A digital signature is an electronic analogue of traditional handwritten signatures.
When using public key methods, a digital signature has a private signing key s for a
signer and a public veriﬁcation key v for a veriﬁer. The signing procedure is described
by a signing algorithm S and the veriﬁcation procedure is described by a veriﬁcation
algorithm V.
A signature scheme is secure against a public key only attack if no attacker can forge
a signature only with V .
A signature scheme is secure against a known-message attack if an attacker cannot
forge a signature of a new message after given some messages and their corresponding
signatures.
A signature scheme is secure against a chosen-message attack if an attacker cannot
forge a signature of a new message when he adaptively chooses arbitrary messages and
receives their corresponding signatures.
Facts:
1. Signatures must be veriﬁable in the sense that, should a dispute arise as to whether a
party signed a document (caused by either a lying signer trying to repudiate a signature
it did create, or a fraudulent claimant), an unbiased third party can resolve the matter
equitably, without requiring access to the signer’s secret information (private key).

Section 15.6
CRYPTOGRAPHIC MECHANISMS
1113
2. Most of today’s digital signature algorithms employ techniques from public key cryp-
tography. It is possible to use symmetric key cryptography to achieve this goal, but it
requires the use of a trusted third party, or new keying materials for each signature.
3. A secure digital signature should be resistant to the three types of attacks: public
key only attack, known-message attack, and chosen-message attack.
4. There is a digital signature scheme for each of the public key schemes given in §15.5.
There are also other approaches as described in this section.
5. RSA signature scheme. The plain RSA signature scheme is dual to the RSA public
key cryptosystem.
• Key generation. The signing key is d, and the veriﬁcation key is (n, e), which are
generated in the same manner as in RSA encryption.
• Signing. To sign a message m ∈Zn, the signer computes the signature as sig =
md mod n.
• Verifying. To verify the signature sig, check if sige ≡m (mod n).
6. Given an arbitrary message m, since d is unknown, it is hard to forge the signature
md mod n. However, it is easy to forge a signature for some messages. For instance, sig
is a valid signature of message sige mod n since (sige)d ≡sig (mod n).
7. To avoid this forging attack, one should ﬁrst hash the message m. That is, if H :
{0, 1}∗→{0, . . ., n −1} is a hash function, the actual signature of m is deﬁned as
sig = H(m)d mod n. The veriﬁcation is modiﬁed accordingly. Assuming that H is a
random oracle and that the RSA assumption holds, the hashed RSA signature is proven
secure against chosen-message attacks.
8. The ElGamal digital signature scheme is a randomized signature algorithm that
generates digital signatures on binary messages with arbitraty length. It requires a hash
function h : {0, 1}∗→Zp, where p is a large prime number.
• Key Generation. As in the ElGamal cryptosystem, the public key is (p, g, h) and
the private key is d.
• Signing.
To sign a message m ∈{0, 1}∗, the signer randomly selects k with
1 ≤k ≤p −2 with gcd(k, p −1) = 1, and then computes X = gk mod p and
s = k−1(h(m) −dX) mod p −1. The signature is (X, s).
• Verifying. To verify the signature (X, s), compute v1 = hX · Xs mod p and v2 =
gh(m) mod p. Accept the signature if and only if v1 = v2.
9. The correctness of the ElGamal signature algorithm can be easily seen since ks ≡
h(m) −dX (mod p −1). That is, h(m) ≡dX + ks (mod p −1) so that v2 ≡gh(m) ≡
gdX+ks ≡(gd)X · (gk)s ≡hX · Xs ≡v1 (mod p −1).
10. The security of the ElGamal scheme is based on the discrete logarithm assumption
in ⟨g⟩.
11. The Digital Signature Algorithm (DSA) was adopted in 1994 by the U.S. government
as a signature standard [Fi13]. It is a variant of the ElGamal signature scheme and is
described as follows:
• Setup. The system generator selects a large prime p of bitlength at least 3072
such that p −1 has a 256-bit prime factor q. It also selects an element g ∈Z∗
p
of order q.
• Key generation. Select x from Zq randomly and compute h = gx mod p. The
signing key is x and the veriﬁcation key is (p, q, g, h).

1114
Chapter 15
CRYPTOGRAPHY
• Signing. To sign a message m with signing key x, the signer randomly selects k
from Zq and computes
r = (gk mod p) mod q and s = k−1(H(m) + xr) mod q.
The signature of m is (r, s).
• Verifying. To verify a signed message (m, (r, s)), let w = s−1 mod q. Check that
r
?= [(gH(m)hr)w mod p] mod q.
If the equality holds, the signature is accepted; otherwise, it is rejected.
12. The correctness of the signature can be easily seen since
(gH(m)hr)w ≡g(H(m)+rx)w ≡gk
(mod p).
13. The security of DSA is based on the discrete logarithm assumption in ⟨g⟩.
14. An adaptation of elliptic curve cryptography to DSA is described in ANSI X9.62 as
the Elliptic Curve Digital Signature Algorithm (ECDSA).
15. The short signature scheme known as BLS signature was proposed by D. Boneh, B.
Lynn, and H. Shacham [BoLySh04]. The scheme is built using a bilinear pairing.
Namely, let G1 and G2 be multiplicatively-written cyclic groups of prime order p. Let g
be a generator of G1 and let ˆe : G1 × G1 →G2 be a bilinear pairing (see §15.5.9). The
BLS scheme is speciﬁed as follows:
• Setup. The system generator selects p, G1, G2, g and a hash function H : {0, 1}∗→
G1.
• Key generation. Select x ∈Zq randomly and compute h = gx. The veriﬁcation
key is (G1, G2, p, g, h) and the signing key is x.
• Signing. To sign a message m, the signer computes σ = H(m)x.
• Verifying.
To verify the signed message (m, σ), a veriﬁer checks if ˆe(σ, g)
?=
ˆe(H(m), h). If equality holds, the signature is accepted; otherwise, it is rejected.
16. The correctness of the BLS signature follows from bilinearity of ˆe.
17. It was proven in [BoLySh04] that BLS is secure against chosen-message attack,
assuming that H is a random function and that the CDH assumption in G1 holds.
Examples:
1. RSA Signature. Using the RSA key pair (n, e) = (4399, 23) and d = 927, to sign
the message m = 19, compute sig = md mod n = 19927 mod 4399 = 4155. To verify the
signature sig = 4155, compute sige ≡415523 ≡19 ≡m (mod 4399).
2. ElGamal Signature. Let the ElGamal parameters be (p, g, h) = (3767, 7, 3467) and
d = 15. Suppose we need to sign the message m where h(m) = 125. Randomly choose
the value k = 87. Note that gcd(87, 3766) = 1. Compute X = 787 mod 3767 = 3113 and
s = k−1(h(m) −dX) mod p −1 = 87−1(125 −15 · 3113) mod 3766 = 3274. The signature
is (3113, 3274). To verify, compute v1 = hX ·Xs mod p = 34673113 ·31133274 mod 3767 =
3030, and v2 = gh(m) mod p = 7125 mod 3767 = 3030. Note that v1 = v2.
3. DSA Key Setup. Let p = 15464731. Since p −1 = 15464730 = 2 · 3 · 5 · 17 · 30323,
let q = 30323. Note that 2 is a generator of Z∗
p. Then g = 22·3·5·17 = 2241624 has
order q. Consider x = 5. Then h = gx mod p = 6922386. The signing key is x = 5 and
the veriﬁcation key is (p, q, g, h) = (15464731, 30323, 2241624, 6922386). Note that DSA
parameters have much larger speciﬁed bitlength.

Section 15.6
CRYPTOGRAPHIC MECHANISMS
1115
4. DSA Signing. Suppose the message m has hash value H(m) = 10628. The signer
randomly selects k = 14 ∈Zq and computes r = (gk mod p) mod q = 5120962 mod
q = 26698 and s = k−1(H(m) + xr) mod q = 14626. The signature for m is (r, s) =
(26698, 14626).
5. DSA Veriﬁcation.
To verify the signed message m having the signature (r, s) =
(26698, 14626), evaluate w = s−1 mod q = 18375. Then [(gH(m)hr)w mod p] mod q =
5120962 mod q = 26698 = r.
15.6.2
MESSAGE AUTHENTICATION CODE
Message authentication code, also called keyed hash function, is a cryptographic primitive
that protects the authenticity and integrity of a message.
Deﬁnitions:
A message authentication code (MAC) scheme is a keyed function FK(·) acting on
messages m such that when m and FK(m) are transmitted, the intended recipient will
be assured of the authenticity of m.
If a computationally bounded attacker has obtained the MACs of n messages of its choice
but still cannot forge the MAC of a new message with probability ǫ, the system is called
an (n, ǫ)-secure MAC.
If ǫ is negligible, and n and the runtime of an attacker are polynomially bounded, then
the MAC scheme is existentially unforgeable.
A universal2 hash function from S to V is a keyed hash function HK such that for
any pair of distinct inputs x, y, HK(x) = HK(y) holds for at most
1
|V | of all possible
keys.
Facts:
1. In a MAC scheme, the sender and the receiver share the secret key K. This primitive
was introduced by E. Gilbert, F. MacWilliams, and N. Sloane [GiMaSl74].
2. The security notion for MAC schemes is similar to that of digital signature schemes.
An attacker may request the MACs of some messages of its choosing, and its objective
is to forge the MAC of some new message.
3. Onetime MAC. Let p be a large prime and let the key K = (a, b) be chosen randomly
from Z2
p. Consider the MAC scheme Fa,b(x) = ax + b mod p, where a, b ∈Zp.
• It is immediate that this scheme is (1, 1/p)-secure.
• This is a special case of a universal2 hash function used as a MAC by Wegman
and Carter [WeCa81].
4. HMAC. This is a message authentication code proposed in 1996 by M. Bellare, R.
Canetti, and H. Krawczyk [BeCaKr96]. It is standardized as FIPS PUB 198-1.
Let HMACK(m) = H
 (K ⊕opad)||H((K ⊕ipad)||m)

, with H an iterated hash function
(e.g., MD5, SHA-1, SHA-256), K is the secret key, opad is the constant 0x5c5c5c...5c5c,
and ipad is the constant 0x363636...3636 such that opad and ipad have the same length
as K.
5. CBC-MAC. This is a message authentication code obtained from the CBC encryption
mode. The construction is as follows. Let K = (K1, K2) be the secret key and let (E, D)
be a block cipher of blocklength ℓ.

1116
Chapter 15
CRYPTOGRAPHY
• Suppose m is the message to be authenticated. The sender divides m into ℓ-bit
blocks m1||m2|| . . . ||mt.
• Set V0 = 0. For i = 0, . . . , t, compute Vi = EK1(Vi−1 ⊕mi).
• Deﬁne the MAC of m to be EK2(Vt).
6. The plain CBC-MAC is simply Vt. However, this construction is not secure. Indeed,
given the MAC σ of m1||m2|| . . . ||mt and the MAC σ′ of m′
1||m′
2||| . . . ||m′
s, one can
compute m′′
1 = Vt ⊕m′
1. Then the MAC of
m1|| . . . ||mt||m′′
1||m′
2||m′
3|| . . . ||m′
s
is σ′, which follows from the fact m′′
1 ⊕σ = 0 ⊕m′
1.
7. An alternative patch to the plain CBC-MAC is to prepend the length of m to the
input. The problem with this variant is that the message length must be known before
the processing begins.
15.6.3
KEY EXCHANGE
Two communicating parties wishing to exchange messages using symmetric key encryp-
tion must ﬁrst establish an encryption key. Key exchange is a mechanism that allows
two parties to securely share a secret key so that no one else can obtain a copy.
Facts:
1. A shared key is required whenever a symmetric key system is used. When a key is
used for just one communcations session, such a key is called a session key.
2. The idea of key exchange in the modern era originated in the landmark Diﬃe-Hellman
paper [DiHe76].
3. Generally, key exchange is assumed to take place over a public channel, therefore,
any key exchange protocol must assume that all messages are available to any person
including the adversary. The ultimate goal of the adversary is to compromise the secret
key shared between communicating parties.
4. The security model describes what information the adversary can obtain and what
actions it can take. Two types of security are considered: passive security and active
security. In passive security, an adversary can observe messages transmitted over the
channel but cannot modify them. In active security, an adversary can modify, delete,
and replay messages transmitted over the channel. In both cases, the goal is to compute
the key shared between the legitimate communicating parties.
5. Diﬃe-Hellman key exchange. Malcolm J. Williamson of GCHQ came up with the
Diﬃe-Hellman key exchange in 1974.
His work was unknown to the public until its
declassiﬁcation in 1997.
Sender (A) and Receiver (B) agree on large primes p and q, where q divides p −1, and
an element g of order q in Z∗
p. The steps of this protocol are as follows:
• A selects x randomly, where 1 ≤x ≤q −1, and sends gx to B.
• Upon receiving gx, B selects a random value y, 1 ≤y ≤q −1, and sends gy to A.
Then B computes the shared key as sk = (gx)y = gxy.
• Upon receiving gy, A computes the shared key as sk = (gy)x = gxy.

Section 15.6
CRYPTOGRAPHIC MECHANISMS
1117
6. The protocol is passively secure if the Computational Diﬃe-Hellman (CDH) assump-
tion holds. That is, it is computationally infeasible to compute gxy from gx and gy.
However, the protocol is not actively secure.
7. Nonetheless, the Diﬃe-Hellman key agreement protocol serves as a basic mechanism
for constructing actively-secure key establishment protocols.
8. The Internet Key Exchange (IKE) protocol [HaCa98] is used in IPsec standards. Its
security against active attacks has been proven by R. Canetti and H. Krawczyk [CaKr02].
Let p, q be large primes with q | p −1 and let g be an element of order q in Z∗
p. Suppose
(Sig, Ver) is a signature scheme and let MAC be a message authentication code. Let
FK(·) : {0, 1}∗→{0, 1}2λ be a keyed function (called a pseudorandom function) whose
outputs appear random to an attacker who does not know K. The steps of this protocol
are the following:
• Setup. Each user i generates a signing key si and veriﬁcation key vi of (Sig, Ver).
Then si is the private key and vi is the public key for user i.
• Protocol. The key exchange between user i and user r proceeds as follows:
⋄User i selects a random x, 1 ≤x ≤q −1, and sends gx and a session-id s to
user r.
⋄User r selects a random y, 1 ≤y ≤q −1, computes (k0, k1) = Fgxy(0), and
sends
(s, gy, Sigsr(1||s||gx||gy), MACk1(1||s||r)) = (s, β, σ, τ)
to user i.
⋄Upon receiving (s, β, σ, τ), user i computes (k′
0, k′
1) = Fβx(0) and veriﬁes
that Ver vr(σ) = 1 and τ = MACk′
1(1||s||r). If the veriﬁcation passes, user
i deﬁnes k′
0 as the shared key and sends
(s, Sigsi(0||s||gy||gx), MACk′
1(0||s||i)) = (s, σ′, τ ′)
to user r; otherwise, user i rejects.
⋄Upon receiving (s, σ′, τ ′), user r veriﬁes that σ′ and τ ′ are valid. If so, user
r accepts k0 as the shared key; otherwise, user r rejects.
9. The IKE protocol is essentially an authenticated variant of the Diﬃe-Hellman key
exchange.
10. The protocol guarantees that gy and gx originated from the legitimate parties. gy
and gx are cryptographically bound in σ′ and σ to guarantee that both parties see the
same (gy, gx).
11. The purpose of the session id s is to prevent attacks that combine data from diﬀerent
sessions.
12. The MAC is used to ensure that the other party has indeed computed the session
key. This is called explicit authentication.
13. Password-based key exchange. Passwords are probably the most popular mechanism
for access control. Naturally, key exchange using a password as the user’s long-term secret
is useful.
14. M. Bellare, D. Pointcheval, and P. Rogaway [BePoRo00] proposed a provably secure
password-based key exchange protocol that is based on the encrypted key exchange
(EKE) technique of Bellovin and Merritt [BeMe92].
Let π be a password shared between Alice and Bob.
Let p, q be large primes with
q | p −1, and let g be an element of order q in Z∗
p. Let Eπ(·) : G →G be a symmetric

1118
Chapter 15
CRYPTOGRAPHY
key encryption scheme with decryption algorithm Dπ(·), and let H be a hash function.
The steps for the password-based key exchange protocol are as follows:
• Party A selects x randomly from Zq and sends (A, Eπ(gx)) = (A, β) to party B.
• On receiving (A, β), B recovers gx = Dπ(β), randomly selects y ∈Zq, deﬁnes
k = H(A, B, gx, gy, gxy), and sends (B, Eπ(gy), H(k, 1)) = (B, γ, τ1) to A.
• On receiving (B, γ, τ1), A recovers gy = Dπ(γ), computes k = H(A, B, gx, gy, gxy),
and checks if τ1 = H(k, 1). If yes, then A sends τ2 = H(k, 2) to B and deﬁnes
the session key to be sk = H(k, 0); otherwise, A rejects.
• On receiving τ2, B veriﬁes that τ2 = H(k, 2). If yes, A deﬁnes the session key to
be sk = H(k, 0); otherwise, B rejects.
15. This protocol is actively secure. The security proof relies on the assumption that
H is a random oracle and E is an ideal cipher. Researchers have made progress towards
removing such assumptions (see [GeLi03], [GrKa10], [JiGo04], [KaOsYu01]).
15.6.4
HOMOMORPHIC ENCRYPTION
Homomorphic encryption is an encryption mechanism that allows certain computations
to be carried out on ciphertext in such a way that decryption of the result yields the
same quantity as would be obtained if the computations were performed on the original
plaintext.
Deﬁnition:
An encryption scheme is additively homomorphic if the ciphertext of m1 + m2 can
be obtained from ciphertexts of m1 and m2.
An encryption scheme is multiplicatively homomorphic if the ciphertext of m1m2
can be obtained from ciphertexts of m1 and m2.
An encryption scheme is fully homomorphic if it is both multiplicatively and additively
homomorphic.
Facts:
1. RSA cryptosystem is multiplicatively homomorphic. Given ciphertexts c1 = me
1 mod
n and c2 = me
2 mod n, the ciphertext of m = m1·m2 mod n is c = c1·c2 mod n. However,
RSA is not additively homomorphic.
2. ElGamal cryptosystem is multiplicatively homomorphic.
Given ciphertexts c1 =
(gr, m1hr) and c2 = (gs, m2hs), the ciphertext of m = m1 · m2 is c = (gr+s, m1m2hr+s).
ElGamal is not additively homomorphic.
3. Paillier cryptosystem is additively homomorphic. Given ciphertexts c1 = gm1rn
1 and
c2 = gm2rn
2 , then c1c2 = gm1+m2(r1r2)n.
4. It had long been an open question whether a fully homomorphic public key encryp-
tion cryptosystem existed. The existence of such a public key cryptosystem would be
important because when such a cryptosystem is used, any computation can be formu-
lated as a circuit of addition and multiplication operations, whereby a third party can
compute any function of arbitrary ciphertexts.
5. In 2009, C. Gentry solved this open question by constructing the ﬁrst fully homo-
morphic public key cryptosystem using lattice-based cryptography [Ge09]. Although this
solved the open question, his system is far too slow to be practical.

Section 15.6
CRYPTOGRAPHIC MECHANISMS
1119
6. An application of fully homomorphic encryption is cloud computing (§15.7.3).
7. All currently studied fully homomorphic cryptosystems are public key based.
8. In 2010, a second fully homomorphic cryptosystem using integer-based cryptography
was developed by M. van Dijk, C. Gentry, S. Halevi, and V. Vaikuntanathan [vDEtal10].
Like Gentry’s original cryptosystem, this approach is also impractical.
9. After the development of the ﬁrst two fully homomorphic cryptosystems, the search
for a practical fully homomorphic cryptosystem continued. Much progress has been made
with the development of a second generation of fully homomorphic cryptosystems, but
as of 2017 work remains before there is a fully homomorphic cryptosystem that can be
used in real-world applications.
10. In commercial applications, it may be desirable to chain certain services together
while keeping the secrecy of the data intact. Homomorphic encryption can be used to
achieve privacy where information is processed in the encrypted form.
Example:
1. Homomorphic encryption can be used in health care where digital patient data from
multiple sources can be stored securely while computations are being performed without
violating a patient’s right to conﬁdentiality.
15.6.5
SECRET SHARING
Secret sharing is a mechanism that allows a group of users to share a secret such that a
suﬃcient number of them can recover the secret whereas an insuﬃcient number of users
is unable to obtain any information about the secret.
Deﬁnition:
In a (t, n)-secret sharing scheme, each of n users receives a share of a secret s so
that any t or more of them can recover s, but any group of fewer than t parties cannot
determine any information about s.
Facts:
1. Secret sharing was independently invented by Shamir [Sh79] and Blakley [Bl79].
2. Shamir’s secret sharing scheme. Let p be a prime; the secret is s ∈Fp.
• Sharing.
The dealer chooses a random polynomial f(x) of degree t −1 with
constant term s. That is, f(x) = a0 + a1x + · · · + at−1xt−1 where ai for i ≥1
is uniformly random over Fp and a0 = s. The share for party i is si = f(i).
• Recovery. When at least t parties wish to recover the secret s, they jointly do the
following
⋄Choose some arbitrary t parties i1, . . . , it from the set of colluders.
⋄Since sij = f(ij) for j = 1, . . . , t, they use Lagrange interpolation to obtain
f(x) = Pt
j=1 sj
Q
1≤k≤t,k̸=j(x−ik)
Q
1≤k≤t,k̸=j(ij−ik).
They then recover
s = f(0) = Pt
j=1 sj
Q
1≤k≤t,k̸=j ik
Q
1≤k≤t,k̸=j(ik−ij).

1120
Chapter 15
CRYPTOGRAPHY
• Security. If fewer than t parties (say, parties 1, . . . , t −1) wish to recover s, they
can form
sj = a0 + a1j + · · · + at−1jt−1, j = 1, . . . , t −1.
Notice that (s1, . . . , st) uniquely recovers (a0, . . . , at−1) and vice versa. Hence,
(s1, . . . , st) is in 1-1 correspondence with (a0, . . . , at−1). Since the latter is uni-
formly random over Ft
p, so too is the former. Hence, given (s1, . . . , st−1), a0 is
uniformly random over Fp due to the randomness of st, and thus (s1, . . . , st−1)
reveals nothing about s.
3. When t = n, an (n, n)-secret sharing is one where any n parties can recover the secret
while the absence of one or more parties will leak no information about s. In this case, a
simpler scheme exists whereby the dealer chooses n −1 random shares s1, . . . , sn−1 and
deﬁnes the share sn = s −s1 −· · · −sn−1 mod p.
4. Blakley’s secret sharing scheme. The scheme uses a t-dimensional projective space
PG(t, Fq). Let the secret be a random point P ∈PG(t, Fq). The scheme is described as
follows:
• Sharing. To share a secret P ∈PG(t, Fq) among n users, the dealer randomly
chooses n hyperplanes that contain P. Each user receives one of these hyper-
planes as his share.
• Recovery. If any v ≥t users want to recover P, t of the users are selected to
combine their shares to form t linear equations, the solution of which determines
P.
• Correctness. In the recovery, the t equations must be linearly independent in
order to recover P. This can be satisﬁed with high probability when n is not
too large while q is large.
• Security. Any set of fewer than t users can only form a linear system with fewer
than t equations. This determines a proper subspace of PG(t, Fq), but cannot
determine P.
5. Compared to Shamir’s scheme, the Blakley sharing scheme is not perfectly secure.
That is, fewer than t users can still obtain some information about the secret P compared
to randomly guessing P. The correctness also does not hold with probability 1 as there
is a nonzero probability that some subset of fewer than t shares (i.e., hyperplanes) can
linearly generate another particular share.
Examples:
1. Shares of a missile launching code can be given to a list of 12 generals, where at least
ﬁve of them need to collaborate to reveal the launch code.
2. A private company can use secret sharing to protect unauthorized fund transfers over
a certain threshold by giving shares of the authorization code to multiple members of
management.
15.6.6
CONFERENCE KEY DISTRIBUTION
Among the members of a group of users, there may be a need for a subgroup to perform
secure communications. Conference key distribution was invented to achieve this purpose.

Section 15.6
CRYPTOGRAPHIC MECHANISMS
1121
Deﬁnitions:
A conference key distribution scheme is a mechanism whereby a trusted authority
assigns a piece of secret data to each user so that any desired group of users can compute
a common secret key (called a group key).
If a group of users compute their group key interactively, then the scheme is called an
interactive key distribution (key agreement or group key exchange); otherwise it is
called a non-interactive key distribution (or key pre-distribution).
If the total number of users is n, then a non-interactive conference distribution scheme
(see Fact 2) secure against t colluding users is called (n, t)-secure.
If the scheme is secure against computationally unbounded attackers, then it is called
unconditionally secure; otherwise, it is called computationally secure.
Facts:
1. Diﬃe-Hellman key exchange is an interactive key distribution scheme for a group of
size two.
2. In a non-interactive scenario, security requires that colluding users should not be able
to compute a group key when none of the colluding users is a group member.
• To ensure that an eﬃcient construction is possible, the number of colluding users
is usually bounded by a threshold t.
3. In an interactive scenario, security is more complex, and considers both passive attack
and active attacks. An attacker can corrupt a number of users, and thereby obtain the
secret data of these users.
• In a passive attack, an attacker can only observe the transmitted messages.
• In an active attack, an attacker can impersonate legitimate users and modify
messages sent by users.
• In both cases, the attacker’s objective is to learn a conference key that he is not
supposed to know.
4. Blom non-interactive key distribution. Blom [Bl84] proposed a non-interactive confer-
ence key distribution scheme without public key components. His scheme is for subgroups
of size two and is secure against a collusion of size k. Suppose there are n users.
• Key setup. The key setup for users is based on the (k + 1)-by-n generator matrix
G of an (n, k + 1)-MDS code over a ﬁnite ﬁeld Fq. Let D be a (k + 1) × (k + 1)
purely random but symmetric matrix over Fq. Then the secret key Ki for user
i is deﬁned as the ith column of DG.
• Conference key computation. The conference key kij for users i and j is the entry
(i, j) of the square matrix (DG)T G = GT DG. This is the inner product of the
ith column of DG and the jth column of G. User i can compute kij since i
possesses Ki and G is public. On the other hand, user j can compute kij as
the inner product of the ith column of G and the jth column of DG.
• Security.
Blom’s scheme is secure against a collusion of size k.
The collud-
ers possess k columns of DG. However, since G is the generator matrix of an
(n, k + 1)-MDS code, any k + 1 columns of G are linearly independent. This
independence does not change by left multiplying with a full-ranked square ma-
trix D (ignoring the negligible probability that rank(D) < k + 1). Hence, Ki is
independent of any collusion of size k.

1122
Chapter 15
CRYPTOGRAPHY
5. Polynomial interpolation non-interactive key distribution. Blundo et al. [BlEtal98]
extended Blom’s scheme for conferences of arbitrary size t. Their scheme is based on
polynomial representations.
For security against collusions of size k, their scheme is
described as follows:
• Key setup. Let σ denote an arbitrary permutation over {1, . . ., t}. Then deﬁne
P(x1, . . . , xt) = P
0≤j1≤···≤jt≤k aj1···jt
P
σ xj1
σ(1) · · · xjt
σ(t),
where aj1···jt is uniformly random over the ﬁnite ﬁeld Fq. Note that P(x1, . . . , xt)
is symmetric. The secret key for user i is deﬁned as Ki = P(i, x2, . . . , xt) for
i = 1, . . . , n.
• Conference key computation. The conference key for a group G = {i1, . . . , it} is
kG = P(i1, . . . , it). User iℓcan compute kG = P(iℓ, i1, . . . , iℓ−1, iℓ+1, . . . , it).
The scheme works because (ℓ, 1, . . . , ℓ−1, ℓ+ 1, . . . , t) is a permutation of
(1, . . . , t) and P(x1, . . . , xt) is symmetric.
• Security. (n, k)-security requires that any collusion group {i∗
1, . . . , i∗
k} ⊂{1, . . ., n}
is unable to compute a conference key kG, when no member in G colluded.
⋄Suppose G = {i1, . . . , it}. Let P0(x1, . . . , xt) be any symmetric polynomial
of degree ≤k such that P0(i∗
u, x2, . . . , xt) = Ki∗u for u = 1, . . . , k.
⋄In addition, note that A(x1, . . . , xt) = Qk
j=1
Qt
v=1(xv −i∗
j) is symmetric
with degree k and satisﬁes A(i∗
j, x2, . . . , xt) = 0.
⋄Then, for any α ∈Fq, P0(x1, . . . , xt) + αA(x1, . . . , xt) can serve as a can-
didate of P(x1, . . . , xt) (in view of colluders {i∗
1, . . . , i∗
k}), because when
evaluated at i∗
j, it yields the result Ki∗
j .
⋄However, it has a conference key P0(i1, . . . , it) + α Qk
j=1
Qt
v=1(iv −i∗
j) for
G, which is uniformly random over Fq (due to the randomness of α) as
iv ̸= i∗
j. Thus, the colluders have no information about kG.
6. Fiat-Naor scheme. Fiat and Naor [FiNa93] proposed a non-interactive key distribu-
tion scheme. This scheme is designed for n users {1, . . ., n} and is unconditionally secure
against w colluders.
• Key setup. Let P1, . . . , Pν be all the subsets of {1, . . ., n} of size at least n−w. Note
that ν = Pw
i=0
 n
i

. Associate each Pi with a random key kPi. The secret key
for user j is Kj = {kPi | j ∈Pi, i = 1, . . . , ν}. Thus, Kj has size Pw
i=0
 n−1
i

.
• Conference key computation. For any group G ⊆{1, . . . , n}, the conference key is
kG = P
i:G⊆Pi kPi. Since every member j ∈G knows kPi for any Pi containing
G, user j can compute kG.
• Security. The Fiat-Naor scheme is secure against w colluders.
⋄Suppose the colluders are {i1, . . . , iw}. Then, for any G not containing any
colluders, kG = P
j:G⊆Pj kPj is independent of {Ki1, . . . , Kiw}.
⋄Notice that there exists t such that Pt = {1, . . ., n}−{i1, . . . , iw} and hence
kPt is independent of the keys of colluders. Since ij ̸∈G for any j, G ⊆Pt.
Thus, kPt appears in the sum of kG. So kG is independent of the keys of
colluders.
7. Tripartite key distribution from pairing. Joux [Jo00] presented a one-round compu-
tationally secure key distribution scheme for groups of size three. It is based on a bilinear
pairing ˆe : G1 × G1 →G2, where G1, G2 are groups of prime order p.

Section 15.6
CRYPTOGRAPHIC MECHANISMS
1123
• Key setup.
Let g ∈G1 be a generator.
Then each user i randomly selects
ai, 1 ≤ai ≤q −1, as its private key and publishes Qi = gai as its public key.
• Conference key computation. For any group {i, j, ℓ}, the conference key is k{i,j,ℓ} =
ˆe(g, g)aiajaℓ. User i can compute this key using ai as ˆe(Qj, Qℓ)ai = ˆe(g, g)aiajaℓ.
• Security.
The security of Joux’s scheme requires the Computational Bilinear
Diﬃe-Hellman (CBDH) assumption.
8. Burmester-Desmedt interactive key distribution scheme.
Burmester and Desmedt
[BuDe94] proposed an interactive key distribution scheme. Their scheme is a generaliza-
tion of the Diﬃe-Hellman key exchange to the multiparty setting. It provides a way to
compute the conference key for an entire group of users. However, it is insecure when
used to compute a group key for a proper subgroup using the same key setup.
Let p and q be large primes with q | p −1. Let g ∈Z∗
p be an element of order q.
• Key setup. For i = 0, . . . , n −1, user i randomly selects a secret key ai, 1 ≤ai ≤
q −1, and publishes the public key Ai = gai.
• Conference key computation. User i computes Xi = (Ai+1/Ai−1)ai and broad-
casts Xi to group members, where index arithmetic is performed modulo n when
necessary. The group key is deﬁned as k = ga0a1+···+an−2an−1+an−1a0. For any
user j, notice that Aℓai
i−1Xℓ−1
i
= gai−1ai × A(ℓ−1)ai
i+1
for each i. Therefore, user j
can compute k by computing
Aajn
j−1Xn−1
j+1 Xn−2
j+2 · · · X1
j−2
(6)
= gaj−1aj × Aaj+1(n−1)
j
Xn−2
j+2 · · · X1
j−2
= gaj−1aj+ajaj+1 × Aaj+2(n−2)
j+1
Xn−3
j+3 · · · X1
j−2
...
= gaj−1aj+ajaj+1+···+aj−2aj−1 = ga0a1+···+an−1a0.
• Security.
The Burmester-Desmedt scheme is secure against a passive outsider
attack. The attacker knows all of X1, A1, . . . , Xn−1, An−1.
⋄In a passive attack, the attacker cannot compute ga0a1+···+an−1a0. Oth-
erwise, from Eq. (6), the attacker can compute Aaj
j−1 = gaj−1aj. Since
A1, . . . , An−1 is computationally independent of ga0a1, . . . , gan−1a0 and hence
is computationally independent of gaj−1aj, X1, . . . , Xn−1, the attacker must
compute gaj−1aj solely based on X1, . . . , Xn−1. However, this is impossible
because they are statistically independent.
⋄Since conference key distribution is usually used to help a subgroup derive
a conference key, one might think that the Burmester-Desmedt scheme
can also do the same as key derivation is completely compatible with any
subgroup. However, once a user i is involved in a group key computation
for a group (say, {1, . . . , n}), user i can compute Aaj
j−1 using the group key
and Eq. (6). Then user i can compute the conference key of any group that
involves users j −1 and j (and not including user i).

1124
Chapter 15
CRYPTOGRAPHY
15.6.7
OBLIVIOUS TRANSFER
There are a variety of reasons why someone may wish to send one of many pieces of
information to a receiver without knowing which piece has been transferred. This task
can be done using what is known as oblivious transfer.
Deﬁnitions:
Oblivious transfer (OT) is a protocol in which Alice has a set of bit strings as her
input and Bob can choose only one string to be transferred to him and Alice cannot
determine which string this is.
When Alice’s set has k elements, the protocol is called OTk
1.
In a k-out-of-n oblivious transfer, Alice has a set of n bit strings as her input, where
Bob chooses k strings to be transferred.
Facts:
1. Oblivious transfer originated in Rabin’s work [Ra81], where he described a method
to send a message m from Alice to Bob with a success probability of 1
2, while Alice does
not know if Bob received the message or not.
• Alice generates RSA parameters p, q, n, e, d, and sends n, e, me to Bob.
• Bob chooses a random value x ∈Z∗
n and evaluates c = x2 mod n. Note that since
x ∈Z∗
n, there are four square roots of c mod n. He sends c to Alice.
• With the knowledge of p, q, Alice ﬁnds a square root y of c easily, and sends y to
Bob.
• If y is not x or −x, then Bob can factor n by evaluating gcd(x −y, n), and thus
solve for d to recover m. Otherwise, Bob cannot recover m from me. Since
there are four square roots, Bob recovers m with probability 1
2.
2. A key requirement of oblivious transfer is that Alice cannot learn which element Bob
has chosen while Bob cannot learn any information about the remaining elements in the
set.
3. Three constructions of Naor et al. [NaPi01], [NaPi05] are the simplest among all
provably secure OT schemes.
4. The one-out-of-two oblivious transfer protocol (OT2
1) is due to Naor and Pinkas
[NaPi01].
5. Alice has two messages M0, M1.
Bob has a bit σ and wishes to learn Mσ.
The
protocol proceeds as follows:
• Let p, q be large primes with q | p −1 and let g ∈Z∗
p be an element of order q.
Bob randomly selects a, b, r ∈Zq and sends A = ga, B = gb, Cσ = gab, and
C1−σ = gr to Alice.
• Alice veriﬁes that C0 ̸= C1. Then she randomly selects s0, s1, t0, t1 ∈Zq, and
computes and sends
W0 = (As0gt0, M0Cs0
0 Bt0), W1 = (As1gt1, M1Cs1
1 Bt1)
to Bob.
• Upon receiving W0 = (α0, β0) and W1 = (α1, β1), Bob computes Mσ = βσ/αb
σ.

Section 15.6
CRYPTOGRAPHIC MECHANISMS
1125
6. Correctness. Notice that Cσ = Ab. Thus, (Asσgtσ)b = Csσ
σ Btσ and Mσ = βσ/αb
σ.
7. Alice’s privacy. Since C0 ̸= C1, C¯σ is not the Diﬃe-Hellman value corresponding to
(A, B), where ¯σ = 1 −σ. Therefore, r
a ̸= b
1. Thus, Cs¯σ
¯σ Bt¯σ is independent of At¯σgt¯σ.
That is, M¯σ is independent of Alice’s knowledge.
8. The Decisional Diﬃe-Hellman (DDH) assumption states that it is hard to distinguish
(ga, gb, gab) from (ga, gb, gr), when a, b, r are randomly selected.
9. Bob’s privacy. σ is computationally independent of Alice’s knowledge (A, B, C0, C1)
since distinguishing C0 from C1 given (A, B) is the DDH problem.
10. One-out-of-many oblivious transfer (OTk
1). The OTk
1 extension of the OT2
1 protocol
originated in [NaPi01]. Suppose Alice has messages M0, . . . , Mk−1, and Bob has an index
I ∈{0, . . . , k −1} and wishes to learn MI. The protocol proceeds as follows:
• Let p, q be large primes with q | p−1 and let g ∈Z∗
p be an element of order q. Bob
randomly selects a, b, rj ∈Zq for j ̸= I and computes A = ga, B = gb, CI =
gab, Cj = grj, ∀j ̸= I. Finally, he sends (A, B, C0, . . . , Ck−1) to Alice.
• Alice veriﬁes Ci ̸= Cj for i ̸= j. Then she computes and sends
Wi = (Asigti, MiCsi
i Bti), ∀i = 0, . . . , k −1
to Bob, where si, ti are randomly selected from Zq.
• Upon receiving Wi = (αi, βi), i = 0, . . . , k −1, Bob computes MI = βI/αb
I.
11. Correctness and privacy analyses for Alice and Bob are similar to OT2
1 in Fact 10.
12. OTk
1 from a general setup assumption. This construction of OTk
1 is due to Naor
and Pinkas [NaPi05]. The scheme, built from an OT2
1 protocol and a one-way function,
is described in the following facts.
13. Let FK(·) be a pseudorandom function, i.e., a keyed function such that without the
knowledge of K, one cannot distinguish outputs of FK(·) from random values, even if
one can adaptively learn the outputs of many other inputs. Pseudorandom functions can
be constructed from one-way functions.
14. Suppose Alice has input (M0, . . . , M2ℓ−1) and Bob has an index I and wishes to
learn MI. Let a0 · · · aℓ−1 be the binary representation of I. The protocol proceeds as
follows:
• Alice prepares ℓpairs of secret keys for function F: (K0
i , K1
i ), i = 0, . . . , ℓ−1.
For each 0 ≤J ≤2ℓ−1, let the binary representation of J be j0 · · · jℓ−1. Alice
computes CJ = MJ ⊕
 ⊕ℓ−1
i=0 FK
ji
i (J)

and sends (C0, . . . , C2ℓ−1) to Bob.
• For each i = 0, . . . , ℓ−1, Alice and Bob jointly execute OT2
1: Alice has input
(K0
i , K1
i ) and Bob has ai as input. As a result, for each i, Bob learns Kai
i .
• Bob computes MI = CI ⊕FK
ai
i (I).
15. Correctness. As I = a0 · · · aℓ−1, it follows that CI = MI ⊕
 ⊕ℓ
i=0 FKai
i (I)

by
deﬁnition of CI.
16. Alice’s privacy. Notice that FKb
i (0), . . . , FKb
i (2ℓ−1) are computationally indepen-
dent for any i and bit b when Kb
i is not given. In the OT2
1 protocol, for each i, Bob
can learn at most one of K0
i , K1
i (assume it is ai). Then, for any J ̸= I = a0 · · · a2ℓ−1,
there exists at least one i, ji ̸= ai, and hence Kji
i
is not learned by Bob. Thus CJ is
computationally independent of Bob’s view, and therefore Bob cannot learn MJ for any
J ̸= I.

1126
Chapter 15
CRYPTOGRAPHY
17. Bob’s privacy. Bob uses I only in the execution of OT2
1 for each i. That is, he
inputs ai. However, by Bob’s privacy in OT2
1, Alice learns nothing about ai. Thus,
Bob’s privacy is guaranteed.
15.6.8
SECURE MULTIPARTY COMPUTATION
Deﬁnition:
A secure multiparty computation is a protocol π such that given n parties P1, . . . , Pn,
where each Pi has a private input xi, π realizes a function f(x1, . . . , xn) = (o1, . . . , on)
such that Pi obtains the output oi, i = 1, . . . , n.
Facts:
1. The requirements of secure multiparty computation include
• Correctness. After jointly executing π, each Pi should obtain oi in f(x1, . . . , xn).
• Security. After executing π, Pi cannot obtain any information about (x1, . . . , xn)
beyond (oi, xi). That is, besides the protocol output oi and input xi, Pi cannot
learn anything from the messages he sees during the execution of π.
2. A set of parties A ⊆{P1, . . . , Pn} might collude and are called corrupted parties.
Security requires that they cannot obtain anything from the execution of π beyond
{(oi, xi)}Pi∈A.
3. Semi-honest vs. malicious security. In semi-honest security, corrupted parties faith-
fully follow the speciﬁcation of π during the execution and their goal is to determine
some information about the inputs of honest parties. In malicious security, corrupted
parties may arbitrarily deviate from the speciﬁcation of π and might send inconsistent
messages in order to obtain information about the inputs of honest parties.
4. Randomized function vs. deterministic function.
The function value is a random
variable for any randomized function f(x1, . . . , xn). However, the randomness parameter
of each party can be included in the input. That is, if Pi uses randomness parameter ri,
then his input can be considered to be xi|ri. The function f then becomes deterministic.
Thus, we only need to consider deterministic functions.
5. Function represented by a circuit. Any (vector) Boolean function f(x1, . . . , xn) can
be represented using a set of basic logic gates. For example, {AND, OR, NOT} is a
complete set for this purpose, and so is {NAND}.
6. The origins of secure multiparty computation stem from Yao’s millionaires’ problem
[Ya82], in which two millionaires Alice and Bob wish to ﬁnd out who is richer without
revealing their wealth.
7. Garbled circuit. Let C be a Boolean circuit realizing the function to be computed.
For simplicity, assume the circuit consists of XOR and AND gates. Assume the circuit
inputs are z1, . . . , zm. A gate has two inputs and an output. The input of a gate comes
from a circuit input or an output of another gate. A circuit input or a gate is called
a node. We label all nodes from 1 to N for some integer N. For each node i, select a
pair of random keys (k0
i , k1
i ). Let (E, D) be a symmetric encryption scheme. For a gate
node s, assume its inputs come from nodes i and j. Then we associate this gate with the
ciphertext table as follows.
• Let g : {0, 1} × {0, 1} →{0, 1} be a gate function for gate s. Deﬁne

Section 15.6
CRYPTOGRAPHIC MECHANISMS
1127
cs
0,0 = Ek0
i (Ek0
j (kg(0,0)
s
)),
cs
0,1 = Ek0
i (Ek1
j (kg(0,1)
s
))
cs
1,0 = Ek1
i (Ek0
j (kg(1,0)
s
)),
cs
1,1 = Ek1
i (Ek1
j (kg(1,1)
s
)).
A circuit with a ciphertext table at each gate is called a garbled circuit. The
following ﬁgure provides an example.
Note that for any input (α, β) of gate s, cs
α,β is a double encryption of one
of the random keys at this gate that corresponds to its output bit g(α, β).
Then, given (kα
i , kβ
j ) and {cs
00, cs
01, cs
10, cs
11} (in a random order), we can compute
kg(α,β)
s
using the fact that Dkβ
j (Dkα
i (cα′,β′)) equals kg(α,β)
s
if (α′, β′) = (α, β)
and equals ⊥(meaning a decryption error) otherwise.
• Anyone who is given kz1
1 , . . . , kzm
m (but not the zis) can evaluate the garbled circuit
in a gate-by-gate manner and ﬁnally obtain the key kC(z1,...,zm)
N
(assume that
node N is the last gate to be evaluated in C).
• From kC(z1,...,zm)
N
, one cannot obtain C(z1, . . . , zm). But given k0
N and k1
N, one
can determine it.
8. Yao’s two-party computation protocol. Suppose that the parties 1 and 2 have inputs
(x1, . . . , xn) and (y1, . . . , yn), respectively, where xi, yi ∈{0, 1}. The protocol proceeds
as follows:
• Party 1 takes a pair of secret keys (k0
i , k1
i ) for each node i in circuit C for i =
1, . . . , N, where N is the total number of nodes in C.
• For each gate s, party 1 computes a table cs
00, cs
01, cs
10, cs
11 and shuﬄes them ran-
domly. This generates a garbled circuit of C.
• Party 1 sends G(C) and kx1
1 , . . . , kxn
n
(assuming the input nodes of party 1 are
1, . . . , n) to party 2.
• At this moment, party 2 needs to further obtain ky1
n+1, . . . , kyn
2n in order to decrypt
the ciphertext table for each gate and ﬁnally obtain kC(x1,...,xn,y1,...,yn)
N
.

1128
Chapter 15
CRYPTOGRAPHY
• Toward this end, let the input nodes for party 2 be n + 1, . . . , 2n.
For each
i = 1, . . . , n, party 1 with input (k0
n+i, k1
n+i) and party 2 with input yi jointly
execute a 1-out-of-2 oblivious transfer to send kyi
n+i to party 2.
• Upon receiving kxi
i
and kyi
n+i for i = 1, . . . , n, party 2 evaluates the garbled circuit
of C and obtains kC(x1,...,xn,y1,...,yn)
N
(assuming gate N is the ﬁnal gate to be
evaluated in C), and sends it to party 1.
• Party 1 obtains C(x1, . . . , xn, y1, . . . , yn) by comparing kC(x1,...,xn,y1,...,yn)
N
(from
party 2) with k0
N and k1
N, and sends the output to party 2.
9. Passive security of party 1. Notice that kxi
i
is independent of xi. Thus, prior to the
ﬁnal output from party 1, the input x1, . . . , xN is independent of the messages seen by
party 2. Thus, during the entire protocol, party 2 can only obtain information about
(x1, . . . , xn) through C(x1, . . . , xn, y1, . . . , yn).
10. Passive security of party 2. Party 1 obtains y1, . . . , yn only through the 1-out-of-
2 oblivious transfers and C(x1, . . . , xn, y1, . . . , yn) in the ﬁnal step.
However, by the
security of oblivious transfers, party 1 obtains no knowledge about y1, . . . , yn. Hence,
the obtained information is covered by C(x1, . . . , xn, y1, . . . , yn).
11. GMW multiparty computation protocol was invented by Goldreich, Micali, and
Wigderson [GoMiWi87].
Consider circuit C with NOT and AND gates. For i = 1, . . . , m, suppose party i has
input xi = (xi1, . . . , xin) ∈{0, 1}n. The protocol proceeds as follows:
• Each party i shares input bit xiℓwith all parties. Speciﬁcally, he generates uni-
formly random bits xj
iℓso that Pm
j=1 xj
iℓ= xiℓand sends {xj
iℓ}n
ℓ=1 to party j in
private. Thus, each party j obtains a share xj
iℓfor each input bit xiℓ.
• All parties jointly evaluate the gates in C in a top-down manner, where only the
top gates are attached to the input wires of C. If the input(s) of a gate g is
shared among all parties, then g is evaluated, with the result shared among all
parties as follows:
⋄if g is a NOT gate with input z, then by assumption, z is shared among all
parties. Assume party i obtains a share zi. Then, party 1 ﬂips z1 and all
other parties keep their shares unchanged. Since ¯z1 + z2 + · · ·+ zm = ¯z, the
NOT gate is evaluated with sharing.
⋄if g is an AND gate with inputs x, y, then by assumption, x, y are shared
and party i holds the share xi, yi for x, y, respectively. The parties want to
jointly compute z = x · y with sharing z = z1 + · · · + zm. Notice that
(Pm
i=1 xi) (Pm
i=1 yi) = Pm
i=1 xiyi + P
1≤i<j≤m(xiyj + xjyi).
(7)
Since party i owns xi, yi, he can compute xiyi locally and deﬁne it as zii.
Then (xiyj + xjyi) can be jointly computed by parties i and j with sharing
zij + zji = (xiyj + xjyi) via OT4
1 as follows:
Party i selects zij uniformly randomly and tries to help party j
derive zji.
Notice that zji = zij for xjyj = 00; zji = zij + yi
for xjyj = 10; zji = zij + xi for xjyj = 01; zji = zij + xi + yi for
xjyj = 11. Then party 1 with input (zij, zij+yi, zij+xi, zij+yi+xi)
and party 2 with input (xj, yj) execute OT4
1, thereby allowing party
2 to obliviously obtain zji = zij + (xiyj + xjyi).
Finally, each party i deﬁnes zi = Pm
j=1 zij. This yields a shared evaluation
of x · y.

Section 15.7
HIGH-LEVEL APPLICATIONS OF CRYPTOGRAPHY
1129
• After evaluating all gates of C, each party i obtains a share oj
i for party j’s output
gate. Then, all parties send their shares oj
i, i = 1, . . . , m to party j. Party j
obtains his circuit output oj = Pm
i=1 oj
i.
12. Correctness. For each gate g, the sharing for its output is achieved. As seen in the
previous fact, each gate is computed correctly. Consequently, C is properly evaluated.
13. Privacy of party i. We need to guarantee that xi is not leaked beyond C(x1, . . . , xm).
• Evaluating each gate does not leak any information. Namely, if g is a NOT gate,
this is true because each output share is computed locally only. If g is an AND
gate, xi is not leaked in any sense. Recall that the inputs are used only in OT4
1,
whereas OT4
1 does not leak any information beyond zji = zij + (xiyj + xjyi).
Further, since zij is selected by party i in a uniform random manner, xi is
independent of zji.
• In the output stage, party i obtains oi
j from every party j, whereas the oi
j, j =
1, . . . , m, are jointly uniformly random subject to oi = Pm
j=1 oi
j. Thus, it does
not leak more than oi. The privacy of party i is achieved.
15.7
HIGH-LEVEL APPLICATIONS OF CRYPTOGRAPHY
The cryptosystems and cryptographic mechanisms described earlier in this chapter are
often components of high-level applications. In this section, three important high-level
applications are described: digital cash, electronic voting, and secure cloud computing.
15.7.1
DIGITAL CASH
Digital cash aims to achieve all the functionality and security (and more) of paper cash
in an electronic world.
Deﬁnitions:
According to the European Banking Authority, virtual currency is “a digital repre-
sentation of value that is neither issued by a central bank or a public authority, nor
necessarily attached to a ﬁat currency, but is accepted by natural or legal persons as a
means of payment and can be transferred, stored, or traded electronically”. Cryptogra-
phy is not necessarily used in the framework.
Cryptocurrency is a class of unregulated digital cash in which cryptography is used in
secure transactions and in the creation of additional currency.
A nonce is a pseudorandom number generated for one-time use.
Facts:
1. The following are some desirable properties of digital cash suggested in the literature
[Ch83], [ChFiNa90], [Wa97]:
• Anonymity. Transactions should be anonymous to third parties.

1130
Chapter 15
CRYPTOGRAPHY
• Peer-to-peer transactions. Transactions do not need to involve a trusted third
party, nor a central clearance system.
• Oﬄine. Transactions can be conducted oﬄine.
• Proof of payment. Payment schemes should provide proof of payment.
• Fraud detection. There is the ability to determine fraud, such as double spending.
• Protection from theft. A stop payment occurs if cash is stolen.
2. Much of the research in digital cash involves two issues: maintaining anonymity and
preventing double spending. Bitcoin is one recent and high-proﬁle digital cash system.
3. Bitcoin is a cryptocurrency invented in 2008 by an unknown person under the alias
of Satoshi Nakamoto. In 2009, the system was released as open-source software.
4. The legal status of bitcoin varies from country to country, from being purely illegal
to unregulated, and to being considered as a commodity.
5. General information on bitcoin:
• The main cryptographic primitives used are ECDSA and SHA-256.
• Bitcoin is a peer-to-peer version of electronic cash. No ﬁnancial institution is
involved in any transaction.
• An electronic coin is deﬁned to be a chain of digital signatures.
• A timestamp server mechanism is used to verify a chain of transactions. A block
is a chain of transactions together with information from a previously veriﬁed
block and a nonce with predeﬁned properties.
• As the chain of transactions grows, the electronic coin takes up memory. A Merkle
tree-like structure is used to reduce the space required.
• The hash function SHA-256 is used extensively both for signing the hash of trans-
actions and for providing proof-of-work (see Fact 8) to prevent fraud. ECDSA
is used for the signing of the hash.
• Bitcoin does not provide anonymity since all transactions are recorded in a public
chain. However, anonymity can be partially achieved by keeping public keys
anonymous.
• As of November 2016, bitcoin had a market capitalization of US $11 billion.
6. The operations of the bitcoin network follow six steps:
• Every new transaction is broadcast to all nodes.
• Each node then collects new transactions into a block by concatenating the trans-
actions.
• Each node competes to ﬁnd a proof-of-work (see Fact 8) for its block.
• A node that ﬁnds a proof-of-work broadcasts the block to all nodes.
• Nodes accept the block when all transactions are veriﬁed and not already spent.
• Nodes accept the block by working on creating the next block in the chain, using
the hash of the previous block to begin a new block.
7. In a transaction from user A to user B, user A combines the previous transaction and
B’s public key, and signs its hash using A’s signing key. The result is then transmitted
to B and publicly announced. Any other user can verify the transaction by verifying A’s
signature.

Section 15.7
HIGH-LEVEL APPLICATIONS OF CRYPTOGRAPHY
1131
8. To further reinforce integrity of data, a timestamp mechanism is used to hash a chain
of transactions as proof of existence of data. The timestamp server uses a proof-of-work
system: the timestamp requires the hashing of the previous timestamp hash, together
with a nonce and a chain of transactions, resulting in a block; the choice of nonce is
restricted such that when hashed, the result must begin with a required number of zero
bits. This process is called mining.
9. It is possible that two timestamps on the same set of transactions can be produced at
similar times. In this case, the longest block chain is recognized. In the case that there
is a tie, then it is broken when the next proof-of-work is found for one of the chains.
10. Since proof-of-work requires computing power from contributors, there is an incen-
tive in the form of new bitcoins that are oﬀered to those who provided the proof-of-work
service.
11. Bitcoin security.
• The proof-of-work system is used to prevent fraud. As with any cryptographically
secure hash function, the amount of time (work) required to ﬁnd a nonce doubles
with each additional required zero bit.
• Double spending is avoided by recording the series of transactions which is avail-
able to all users.
• For a malicious user to nullify a past transaction, the attacker must re-do the
proof-of-work of the block, all subsequent blocks, and surpass the work of the
honest nodes. The probability of being able to accomplish this diminishes ex-
ponentially as more blocks are added.
• Bitcoin can be proven to be secure when there are more honest nodes than mali-
cious nodes.
• The bitcoin wallet, which is a ﬁle that contains a collection of private signing keys
for use in transactions, is not encrypted by default, and is vulnerable to theft.
15.7.2
ELECTRONIC VOTING
Electronic voting was developed to fulﬁll a need for an accurate, auditable, fast and
secure voting system. Suggested voting systems include polling place electronic voting
and remote internet voting [Jo], [Mo00].
Deﬁnitions:
Electronic voting is voting using electronic means to assist in vote casting or counting
votes.
Blinding is a technique wherein a supplier can provide a service to a client without
knowing the actual content of the input or the output. In the case of voting, a voter
uses the technique to conceal the vote, which is then submitted to an administrator for
signing. The voter reveals the vote anonymously at a later stage.
Facts:
1. Some cryptographically-relevant criteria for an electronic voting system include the
following [Mo00]:
• Eligibility and Authentication. Only registered voters can vote.
• Uniqueness. Participants can vote at most once.

1132
Chapter 15
CRYPTOGRAPHY
• Accuracy. Votes should be recorded correctly.
• Integrity. The system prevents votes from being modiﬁed, forged, or deleted.
• Veriﬁability and Auditability. The system has the ability to verify that votes are
accounted for, with reliable and authentic election records.
• Reliability. The system should work despite numerous failures.
• Secrecy and Non-Coercibility. No one should be able to determine how an indi-
vidual voted. Voters should not be able to prove how they voted.
2. Blind Signatures Based Voting (Fujioka-Okamoto-Ohta). This voting scheme was
published in 1993 [FuOkOh92]. It uses blind signatures, where an administrator signs
the concealed vote submitted by the voter.
The scheme consists of six major steps:
preparation, administration, voting, collecting, opening, and counting.
• Preparation. A voter ﬁlls in a ballot using a key k, attaches a random ID string
to the ballot, and blinds the vote using a blinding technique. The voter then
sends the blinded vote to the administrator.
• Administration. The administrator, after verifying the validity of the voter, signs
the blinded vote and sends it back to the voter.
• Voting.
The voter unblinds the signature and sends the administrator-signed
anonymous ballot to the collector.
• Collecting.
The collector checks the administrator’s signature and enters the
encrypted vote into a list.
• Opening. The voter checks if the ballot is in the list and sends key k through an
anonymous channel.
• Counting. The collector opens the vote using key k.
3. The scheme is proven to satisfy eligibility and authentication, uniqueness, accuracy,
integrity, veriﬁability, and secrecy.
4. Cryptographic Counter Based Voting (Katz-Myers-Ostrovsky). A cryptographic n-
counter is a triple of algorithms (G, D, T ), with n diﬀerent counting states, such that
using any public key cryptosystem, we have
• G is a probabilistic key generation algorithm, where a k-bit input generates a
public key-private key pair (pk, sk) and a string s0. The secret key implicitly
deﬁnes an associated set of states Ssk, where s0 ∈Ssk. The states represent
the count.
• D is the deterministic decryption algorithm that takes in secret key sk and a
string s, and outputs an integer i ∈Zn if s ∈Ssk.
Otherwise, D outputs
contradiction.
• T is a probabilistic algorithm known as the transition algorithm. It takes the
public key pk, string s, and an integer i ∈Zn, and outputs string s′.
5. A high-level voting protocol is described in the following steps. Assume that there
are n voters, where voters are voting in sequence.
• Setup. The authorities run the key generation algorithm, and then announce the
public key pk and the initial string s0 to all voters.
• Voting. The collector carries the current total. Let si be the counter after the ith
vote. For the (i + 1)st vote, the voter takes the current counter si, public key
pk, the desired vote, and updates the state to si+1 using the transition function.

Section 15.7
HIGH-LEVEL APPLICATIONS OF CRYPTOGRAPHY
1133
• Tally. The authorities decrypt the last counter.
6. A simple voting protocol where a voter’s choice is limited to {0, 1} is illustrated using
quadratic residuosity in [KaMyOs01]. The scheme is proven to satisfy secrecy, reliability,
and veriﬁability.
7. In a mix-net voting scheme [BoGo02], [ChPe92], [JaJuRi02], voters submit signed,
encrypted votes. A sequence of independent authorities called mixers shuﬄe the votes
and perform further encryption. After a chain of mixers has performed the operations,
the shuﬄed votes are completely decrypted. Due to the secret shuﬄing, the votes become
anonymous, thus protecting secrecy. A method of providing shuﬄe-and-decrypt proofs
also provides veriﬁability.
8. The Paillier cryptosystem [Pa99], a homomorphic encryption system (§15.6.4) that
satisﬁes E(m1)E(m2) = E(m1 + m2), can be used in voting where encrypted votes are
multiplied and then decrypted to reveal the total tally [BaEtal01].
15.7.3
SECURE CLOUD COMPUTING
Cloud computing emerged from the development of high-speed, high-capacity networks
and low-cost computers. The proliferation of computer technology has enabled sharing of
computing and data resources across the internet. This shared resources model enables
businesses and researchers to avoid expensive initial infrastructure costs, and it maximizes
the use of computational resources across a network.
Deﬁnition:
Cloud computing is an internet-based model that allows shared computing resources
and data to electronic devices on demand. The goal is to enable ubiquitous, convenient,
on-demand network access to a shared pool of conﬁgurable computing resources that can
be rapidly provisioned and released with minimal management eﬀort or service provider
interaction [MeGr11].
Facts:
1. Since resources are shared, it is essential that conﬁdentiality and auditability are
provided throughout the service [ArEtal10].
2. In the interest of data privacy, cryptographic separation is needed in which compu-
tations and data are concealed in such a way that they appear intangible to the outsider
[Ry13], [SuKa11], [ZiLe12]. One can store the encrypted data on a cloud server and the
cloud server can run operations such as keyword search, or perform arithmetic on the
data, without decrypting the data. This mechanism eliminates the need to download
data from the cloud, thereby reducing security risk.
3. One solution to protect data privacy in cloud computing is through the use of homo-
morphic encryption [TeHaGh12], [TeHa14]; see §15.6.4. Paillier [Pa99] and Goldwasser-
Micali [GoMi82] cryptosystems can be used in applications where the operation is ad-
dition.
However, both systems involve message expansion and thus have a practical
disadvantage. The RSA and ElGamal systems can be used for multiplication requests.

1134
Chapter 15
CRYPTOGRAPHY
REFERENCES
Printed Resources:
[AgMa09] D. Aggarwal and U. Maurer, “Breaking RSA generically is equivalent to factor-
ing” in Advances in Cryptology–Eurocrypt 2009, Lecture Notes in Computer Science
5479, Springer, 2009, 36–53.
[AgEtal12] M. ˚Agren et al., “A survey on fast correlation attacks”, Cryptography and
Communications 4 (2012), 173–202.
[AoEtal00] K. Aoki et al., “Camellia: a 128-bit block cipher suitable for multiple platforms
–design and analysis” in Selected Areas in Cryptography–SAC 2000, Lecture Notes
in Computer Science 2012, Springer, 2000, 39–56.
[ArEtal10] M. Armbrust et al., “A view of cloud computing”, Communications of the
ACM 53 (2010), 50–58.
[BaRo11] E. Barker and A. Roginsky, “Transitions: recommendation for transitioning
the use of cryptographic algorithms and key lengths”, NIST Special Publication 800-
131A, 2011.
[BaEtal01] O. Baudron, P. Fouque, D. Pointcheval, J. Stern, and G. Poupard, “Practical
multi-candidate election system”, Proceedings of the 20th Annual ACM Symposium
on Principles of Distributed Computing, ACM, 2001, 274–283.
[BeCaKr96] M. Bellare, R. Canetti, and H. Krawczyk, “Keying hash functions for message
authentication” in Advances in Cryptology–Crypto ’96, Lecture Notes in Computer
Science 1109, Springer, 1996, 1–15.
[BePoRo00] M. Bellare, D. Pointcheval, and P. Rogaway, “Authenticated key exchange
secure against dictionary attacks” in Advances in Cryptology–Eurocrypt 2000, Lec-
ture Notes in Computer Science 1807, Springer, 2000, 139–155.
[BeMe92] S. Bellovin and M. Merritt, “Encrypted key exchange: password-based proto-
cols secure against dictionary attacks” in Symposium on Research in Security and
Privacy, IEEE, 1992, 72–84.
[BeEtal07] G. Bertoni et al., “Sponge functions” in ECRYPT Hash Workshop, 2007.
[BiSh91] E. Biham and A. Shamir, “Diﬀerential cryptanalysis of DES-like cryptosys-
tems”, Journal of Cryptology 4 (1991), 3–72.
[BiEtal11] A. Biryukov et al., “Second-order diﬀerential collisions for reduced SHA-256”
in Advances in Cryptology–Asiacrypt 2011, Lecture Notes in Computer Science 7073,
Springer, 2011, 270–287.
[BiKhNi09] A. Biryukov, D. Khovratovich, and I. Nikoli´c, “Distinguisher and related-key
attack on the full AES-256” in Advances in Cryptology–Crypto 2009, Lecture Notes
in Computer Science 5677, Springer, 2009, 231–249.
[Bl79] G. Blakley,“Safeguarding cryptographic keys”, Proceedings of the AFIPS 1979
National Computer Conference 48 (1979), 313–317.
[Bl84] R. Blom, “An optimal class of symmetric key generation systems” in Advances in
Cryptology–Eurocrypt ’84, Lecture Notes in Computer Science 209, Springer, 1984,
335–338.
[BlEtal98] C. Blundo et al., “Perfectly secure key distribution for dynamic conferences”,
Information and Computation 146 (1998), 1–23.

REFERENCES
1135
[BoKhRe11] A. Bogdanov, D. Khovratovich, and C. Rechberger, “Biclique cryptanaly-
sis of the full AES” in Advances in Cryptology–Asiacrypt 2011, Lecture Notes in
Computer Science 7073, Springer, 2011, 344–371.
[BoBo04] D. Boneh and X. Boyen, “Secure identity based encryption without random
oracles” in Advances in Cryptology–Crypto 2004, Lecture Notes in Computer Science
3152, Springer, 2004, 443–459.
[BoDu99] D. Boneh and G. Durfee, “Cryptanalysis of RSA with private key d less than
N 0.292 ” in Advances in Cryptology–Eurocrypt ’99, Lecture Notes in Computer Sci-
ence 1592, Springer, 1999, 1–11.
[BoFr01] D. Boneh and M. Franklin, “Identity-based encryption from the Weil pairing”
in Advances in Cryptology–Crypto 2001, Lecture Notes in Computer Science 2139,
Springer, 2001, 213–229.
[BoGo02] D. Boneh and P. Golle, “Almost entirely correct mixing with applications to
voting” in Proceedings of the 9th ACM Conference on Computer and Communica-
tions Security, ACM, 2002, 68–77.
[BoLySh04] D. Boneh, B. Lynn, and H. Shacham, “Short signatures from the Weil pair-
ing”, Journal of Cryptology 17 (2004), 297–319.
[Bu13] J. Buchmann, Introduction to Cryptography, Springer Science & Business Media,
2013.
[BuDe94] M. Burmester and Y. Desmedt, “A secure and eﬃcient conference key distribu-
tion system” in Advances in Cryptology–Eurocrypt ’94, Lecture Notes in Computer
Science 950, Springer, 1994, 275–286.
[CaKr02] R. Canetti and H. Krawczyk, “Security analysis of IKE’s signature based key-
exchange protocol” in Advances in Cryptology–Crypto 2002, Lecture Notes in Com-
puter Science 2442, Springer, 2002, 143–161.
[Ca05] C. De Canni`ere, “Kasumi/Misty1” in Encyclopedia of Cryptography and Security,
H. C. A. van Tilborg (ed.), Springer, 2005, 322–323.
[Ca10a] C. Carlet, “Boolean functions for cryptography and error correcting codes” in
Boolean Models and Methods in Mathematics, Computer Science, and Engineering,
Y. Crama and P. L. Hammer (eds.), Cambridge University Press, 2010, 257–397.
[Ca10b] C. Carlet, “Vectorial Boolean functions for cryptography” in Boolean Models
and Methods in Mathematics, Computer Science, and Engineering, Y. Crama and
P. L. Hammer (eds.), Cambridge University Press, 2010, 398–469.
[Ch83] D. Chaum, “Blind signatures for untraceable payments” in Advances in Cryptol-
ogy, D. Chaum (ed.), Springer, 1983, 199–203.
[ChFiNa90] D. Chaum, A. Fiat, and M. Naor, “Untraceable electronic cash” in Advances
in Cryptology–Crypto ’88, Lecture Notes in Computer Science 403, Springer, 1990,
319–327.
[ChPe92] D. Chaum and T. Pedersen, “Wallet databases with observers” in Advances
in Cryptology–Crypto ’92, Lecture Notes in Computer Science 740, Springer, 1992,
89–105.
[ChEtal11] J. Choy et al., “AES variants secure against related-key diﬀerential and
boomerang attacks” in Information Security Theory and Practice, Lecture Notes
in Computer Science 6633, Springer, 2011, 191–207.

1136
Chapter 15
CRYPTOGRAPHY
[Co97] D. Coppersmith, “Small solutions to polynomial equations, and low exponent
RSA vulnerabilities”, Journal of Cryptology 10 (1997), 233–260.
[Co03] N. Courtois, “Fast algebraic attacks on stream ciphers with linear feedback” in
Advances in Cryptology–Crypto 2003, Lecture Notes in Computer Science 2729,
Springer, 2003, 176–194.
[CoMe03] N. Courtois and W. Meier, “Algebraic attacks on stream ciphers with linear
feedback” in Advances in Cryptology–Eurocrypt 2003, Lecture Notes in Computer
Science 2656, Springer, 2003, 345–359.
[CoPi02] N. Courtois and J. Pieprzyk, “Cryptanalysis of block ciphers with overdeﬁned
systems of equations” in Advances in Cryptology–Asiacrypt 2002, Lecture Notes in
Computer Science 2501, Springer, 2002, 267–287.
[CrSh98] R. Cramer and V. Shoup, “A practical public key cryptosystem provably secure
against adaptive chosen ciphertext attack” in Advances in Cryptology–Crypto ’98,
Lecture Notes in Computer Science 1462, Springer, 1998, 13–25.
[DaRi13] J. Daemen and V. Rijmen, The Design of Rijndael: AES–The Advanced En-
cryption Standard, Springer Science & Business Media, 2013.
[DeKn15] H. Delfs and H. Knebl, Introduction to Cryptography, 3rd ed., Springer, 2015.
[De06] A. Dent, “The Cramer-Shoup encryption scheme is plaintext aware in the stan-
dard model” in Advances in Cryptology–Eurocrypt 2006, Lecture Notes in Computer
Science 4004, Springer, 2006, 289–307.
[DiHe76] W. Diﬃe and M. Hellman, “New directions in cryptography”, IEEE Transac-
tions on Information Theory 22 (1976), 644–654.
[DiLe08] W. Diﬃe and G. Ledin, “SMS4 encryption algorithm for wireless networks” in
IACR Cryptology ePrint Archive Report 2008/329.
[Dw01] M. Dworkin, “Recommendation for block cipher modes of operation: methods
and techniques”, NIST Special Publication 800-38A, 2001.
[Dw04] M. Dworkin, “Recommendation for block cipher modes of operation: the CCM
mode for authentication and conﬁdentiality”, NIST Special Publication 800-38C,
2004.
[Dw07] M. Dworkin, “Recommendation for block cipher modes of operation: Galois/coun-
ter mode (GCM) and GMAC”, NIST Special Publication 800-38D, 2007.
[El85] T. ElGamal, “A public key cryptosystem and a signature scheme based on discrete
logarithms”, IEEE Transactions on Information Theory 31 (1985), 469–472.
[FiNa93] A. Fiat and M. Naor, “Broadcast encryption” in Advances in Cryptology–
Crypto ’93, Lecture Notes in Computer Science 773, Springer, 1993, 480–491.
[Fi77] FIPS PUB 46, “Data Encryption Standard,” Federal Information Processing Stan-
dards Publication, 1977.
[Fi13] FIPS PUB 186-4, “Digital Signature Standard (DSS)”, Federal Information Pro-
cessing Standards Publication, 2013.
[Fi15] FIPS PUB 202, “SHA-3 Standard: Permutation-Based Hash and Extendable-
Output Functions”, Federal Information Processing Standards Publication, 2015.
[FlMaSh01] S. Fluhrer, I. Mantin, and A. Shamir, “Weaknesses in the key scheduling
algorithm of RC4” in Selected Areas of Cryptography–SAC 2001, Lecture Notes in
Computer Science 2259, Springer, 2001, 1–24.

REFERENCES
1137
[Fr01] G. Frey, “Applications of arithmetical geometry to cryptographic constructions”
in Finite Fields and Applications, D. Jungnickel and H. Niederreiter (eds.), Springer,
2001, 128–161.
[FuOk99] E. Fujisaki and T. Okamoto, “Secure integration of asymmetric and symmet-
ric encryption schemes” in Advances in Cryptology–Crypto ’99, Lecture Notes in
Computer Science 1666, Springer, 1999, 537–554.
[FuOkOh92] A. Fujioka, T. Okamoto, and K. Ohta, “A practical secret voting scheme
for large scale elections” in Advances in Cryptology–Auscrypt ’92, Lecture Notes in
Computer Science 718, Springer, 1992, 244–251.
[GaHeSm02] P. Gaudry, F. Hess, and N. Smart, “Constructive and destructive facets of
Weil descent on elliptic curves”, Journal of Cryptology 15 (2002), 19–46.
[GeLi03] R. Gennaro and Y. Lindell, “A framework for password-based authenticated key
exchange” in Advances in Cryptology–Eurocrypt 2003, Lecture Notes in Computer
Science 2656, Springer, 2003, 524–543.
[Ge09] C. Gentry, “Fully homomorphic encryption using ideal lattices”, Proceedings of
the 41st Annual ACM Symposium on Theory of Computing, ACM, 2009, 169–178.
[GiMaSl74] E. N. Gilbert, F. J. MacWilliams, and N. J. A. Sloane, “Codes which detect
deception”, Bell System Technical Journal 53 (1974), 405–424.
[GoMiWi87] O. Goldreich, S. Micali, and A. Wigderson, “How to play any mental game
or a completeness theorem for protocols with honest majority”, Proceedings of the
19th Annual ACM Symposium on Theory of Computing, ACM, 1987, 218–229.
[GoMi82] S. Goldwasser and S. Micali, “Probabilistic encryption & how to play mental
poker keeping secret all partial information”, Proceedings of the 14th Annual ACM
Symposium on Theory of Computing, ACM, 1982, 365–377.
[GoEtal06] V. Goyal et al., “Attribute-based encryption for ﬁne-grained access control
of encrypted data”, Proceedings of the 13th ACM Conference on Computer and
Communications Security, ACM, 2006, 89–98.
[GrKa10] A. Groce and J. Katz, “A new framework for eﬃcient password based authen-
ticated key exchange”, Proceedings of the 17th ACM Conference on Computer and
Communications Security, ACM, 2010, 516–525.
[G¨uEtal08] T. G¨uneysu et al., “Cryptanalysis with COPACOBANA”, IEEE Transactions
on Computers 57 (2008), 1498–1513.
[HaCa98] D. Harkins and D. Carrel, The Internet Key Exchange (IKE), RFC 2409,
Internet Engineering Task Force, 1998.
[HiLa10] M. J. Hinek and C. Lam, “Common modulus attacks on small private exponent
RSA and some fast variants (in practice)”, Journal of Mathematical Cryptology 4
(2010), 58–93.
[HoSe99] N. Howgrave-Graham and J. P. Seifert, “Extending Wiener’s attack in the
presence of many decrypting exponents” in Secure Networking–CQRE [Secure]’99,
Lecture Notes in Computer Science 1740, Springer, 1999, 153–166.
[JaMeSt01] M. Jacobson, A. Menezes and A. Stein, “Solving elliptic curve discrete loga-
rithm problems using Weil descent”, Journal of the Ramanujan Mathematical Society
16 (2001), 231–260.
[JaJuRi02] M. Jakobsson, A. Juels, and R. Rivest, “Making mix nets robust for electronic
voting by randomized partial checking” in USENIX Security Symposium, 2002, 339–
353.

1138
Chapter 15
CRYPTOGRAPHY
[JiGo04] S. Jiang and G. Gong, “Password based key exchange with mutual authenti-
cation” in Selected Areas in Cryptography–SAC 2004, Lecture Notes in Computer
Science 3357, Springer, 2004, 267–279.
[JiWa10] S. Jiang and H. Wang, “Plaintext-awareness of hybrid encryption” in Topics in
Cryptology–CT-RSA 2010, Lecture Notes in Computer Science 5985, Springer, 2010,
57–72.
[Jo00] A. Joux, “A one round protocol for tripartite Diﬃe-Hellman” in International Al-
gorithmic Number Theory Symposium–ANTS-IV 2000, Lecture Notes in Computer
Science 1838, Springer, 2000, 385–393.
[Ka96] D. Kahn, The Codebreakers: The Comprehensive History of Secret Communica-
tion from Ancient Times to the Internet, Simon and Schuster, 1996.
[Ka92] B. Kaliski, “The MD2 message-digest algorithm”, RFC 1319, Network Working
Group, 1992.
[Ka98] B. Kaliski, “PKCS #7: Cryptographic message syntax version 1.5”, RFC 2315,
Network Working Group, 1998.
[KaLi14] J. Katz and Y. Lindell, Introduction to Modern Cryptography, CRC Press,
2014.
[KaMyOs01] J. Katz, S. Myers, and R. Ostrovsky, “Cryptographic counters and appli-
cations to electronic voting” in Advances in Cryptology–Eurocrypt 2001, Lecture
Notes in Computer Science 2045, Springer, 2001, 78–92.
[KaOsYu01] J. Katz, R. Ostrovsky, and M. Yung, “Eﬃcient password authenticated key
exchange using human-memorable passwords” in Advances in Cryptology–Eurocrypt
2001, Lecture Notes in Computer Science 2045, Springer, 2001, 475–494.
[KhReSa12] D. Khovratovich, C. Rechberger, and A. Savelieva, “Bicliques for preimages:
attacks on Skein-512 and the SHA-2 family” in Fast Software Encryption–FSE 2012,
Lecture Notes in Computer Science 7549, Springer, 2012, 244–263.
[KiRo01] J. Kilian and P. Rogaway, “How to protect DES against exhaustive key search
(an analysis of DESX)”, Journal of Cryptology 14 (2001), 17–35.
[KiYo11] A. Kircanski and A. Youssef, “On the sliding property of SNOW 3G and SNOW
2.0”, IET Information Security 5 (2011), 199–206.
[Ko87] N. Koblitz, “Elliptic curve cryptosystems”, Mathematics of Computation 48
(1987), 203–209.
[LiWa14] Y. Li and M. Wang, ”Constructing advanced encryption S-boxes for lightweight
cryptography with Feistel Structure” in Cryptographic Hardware and Embedded
Systems–CHES 2014, Lecture Notes in Computer Science 8731, Springer, 2014, 127–
146.
[LiKh07] C. Lim and K. Khoo, “An analysis of XSL applied to BES” in Fast Software
Encryption–FSE 2007, Lecture Notes in Computer Science 4593, Springer, 2007,
242–253.
[LuMeVa05] Y. Lu, W. Meier, and S. Vaudenay, “The conditional correlation attack: a
practical attack on Bluetooth encryption” in Advances in Cryptology–Crypto 2005,
Lecture Notes in Computer Science 3621, Springer, 2005, 97–117.
[MaPe08] S. Manuel and T. Peyrin, “Collisions on SHA-0 in one hour” in Fast Software
Encryption–FSE 2008, Lecture Notes in Computer Science 5086, Springer, 2008,
16–35.

REFERENCES
1139
[Ma03] W. Mao, Modern Cryptography: Theory and Practice, Prentice Hall Professional
Technical Reference, 2003.
[Ma69] J. Massey, “Shift-register synthesis and BCH decoding”, IEEE Transactions on
Information Theory 15 (1969), 122–127.
[Ma93] M. Matsui, “Linear cryptanalysis method for DES cipher” in Advances in Crypto-
logy–Eurocrypt ’93, Lecture Notes in Computer Science 765, Springer, 1993, 386–397.
[MaMeTe02] M. Maurer, A. Menezes, and E. Teske,“Analysis of the GHS Weil descent
attack on the ECDLP over characteristic two ﬁnite ﬁelds of composite degree”, LMS
Journal of Computation and Mathematics 5 (2002), 127–174.
[MaJoBal04] A. Maximov, T. Johansson, and S. Babbage, “An improved correlation
attack on A5/1” in Selected Areas in Cryptography–SAC 2004, Lecture Notes in
Computer Science 3357, Springer, 2004, 1–18.
[MeQu01] A. Menezes and M. Qu, “Analysis of the Weil descent attack of Gaudry, Hess
and Smart” in Topics in Cryptology–CT-RSA 2001, Lecture Notes in Computer
Science 2020, Springer, 2001, 308–318.
[MeTe06] A. Menezes and E. Teske, “Cryptographic implications of Hess’s generalized
GHS attack”, Applicable Algebra in Engineering, Communication and Computing
16 (2006), 439–460.
[MeTeWe04] A. Menezes, E. Teske, and A. Weng, “Weak ﬁelds for ECC” in Topics in
Cryptology–CT-RSA 2004, Lecture Notes in Computer Science 2964, Springer, 2004,
366–386.
[MevOVa10] A. Menezes, P. van Oorschot, and S. Vanstone. Handbook of Applied Cryp-
tography, CRC Press, 2010.
[MeHe78] R. Merkle and M. Hellman, “Hiding information and signatures in trapdoor
knapsacks”, IEEE Transactions on Information Theory 24 (1978), 525–530.
[Mi85] V. Miller, “Uses of elliptic curves in cryptography” in Advances in Cryptology–
Crypto ’85, Lecture Notes in Computer Science 218, Springer, 1985, 417–462.
[Mo00] C. D. Mote, Jr., “Report of the national workshop on internet voting: issues and
research agenda” in Proceedings of the 2000 Annual National Conference on Digital
Government Research, Digital Government Society of North America, 2000, 1–59.
[MuRo02] S. Murphy and M. Robshaw, “Essential algebraic structure within the AES”
in Advances in Cryptology–Crypto 2002, Lecture Notes in Computer Science 2442,
Springer, 2002, 1–16.
[NaPi01] M. Naor and B. Pinkas, “Eﬃcient oblivious transfer protocols”, Proceedings
of the 12th Annual ACM-SIAM Symposium on Discrete Algorithms, SIAM, 2001,
448–457.
[NaPi05] M. Naor and B. Pinkas, “Computationally secure oblivious transfer”, Journal
of Cryptology 18 (2005), 1–35.
[Ni10] I. Nikolic, “Tweaking AES” in Selected Areas in Cryptography–SAC 2010, Lecture
Notes in Computer Science 6544, Springer, 2010, 198–210.
[PaPe09] C. Paar and J. Pelzl, Understanding Cryptography: A Textbook for Students
and Practitioners, Springer Science & Business Media, 2009.
[Pa99] P. Paillier, “Public-key cryptosystems based on composite degree residuosity
classes” in Advances in Cryptology–Crypto ’99, Lecture Notes in Computer Science
1592, Springer, 1999, 223–238.

1140
Chapter 15
CRYPTOGRAPHY
[Po74] J. M. Pollard, “Theorems of factorization and primality testing”, Proceedings of
the Cambridge Philosophical Society 76 (1974), 521–528.
[Ra81] M. O. Rabin, “How to exchange secrets by oblivious transfer”, Technical Report
TR-81, Aiken Computation Laboratory, 1981.
[Ri08] R. Rivest, “The MD6 hash function”, Invited talk at Advances in Cryptology–
Crypto 2008; available at http://people.csail.mit.edu/rivest/.
[RiShAd78] R. Rivest, A. Shamir, and L. Adleman, “A method for obtaining digital
signatures and public-key cryptosystems”, Communications of the ACM 21 (1978),
120–126.
[RoBi08] M. Robshaw and O. Billet, eds., New Stream Cipher Designs–The eSTREAM
Finalists, Lecture Notes in Computer Science 4986, Springer, 2008.
[Ru12] R. Rueppel, Analysis and Design of Stream Ciphers, Springer Science & Business
Media, 2012.
[Ry13] M. Ryan, “Cloud computing security: the scientiﬁc challenge, and a survey of
solutions”, Journal of Systems and Software 86 (2013), 2263–2268.
[SaWa05] A. Sahai and B. Waters,“Fuzzy identity-based encryption” in Advances in
Cryptology–Eurocrypt 2005, Lecture Notes in Computer Science 3494, Springer,
2005, 457–473.
[Sc07] B. Schneier, Applied Cryptography: Protocols, Algorithms, and Source Code in
C, John Wiley & Sons, 2007.
[Se98] I. Semaev, “Evaluation of discrete logarithms in a group of p-torsion points of an
elliptic curve in characteristic p”, Mathematics of Computation 67 (1998), 353–356.
[Sh79] A. Shamir, “How to share a secret”, Communications of the ACM 22 (1979),
612–613.
[Sh84a] A. Shamir, “A polynomial-time algorithm for breaking the basic Merkle-Hellman
cryptosystem”, IEEE Transactions on Information Theory 30 (1984), 699–704.
[Sh84b] A. Shamir, “Identity-based cryptosystems and signature schemes” in Advances
in Cryptology–Crypto ’84, Lecture Notes in Computer Science 196, Springer, 1984,
47–53.
[ShEtal07] T. Shirai et al., “The 128-bit blockcipher CLEFIA (extended abstract)” in Fast
Software Encryption–FSE 2007, Lecture Notes in Computer Science 4593, Springer,
2007, 181–195.
[Si00] S. Singh, The Code Book: The Science of Secrecy from Ancient Egypt to Quantum
Cryptography, Anchor, 2000.
[Sm99] N. Smart, “The discrete logarithm problem on elliptic curves of trace one”, Jour-
nal of Cryptology 12 (1999), 193–196.
[StEtal17] M. Stevens, E. Bursztein, P. Karpman, A. Albertini, and Y. Markov, “The
ﬁrst collision of full SHA-1”; available http://shattered.io.
[St05] D. Stinson, Cryptography: Theory and Practice, 3rd ed., CRC Press, 2005.
[SuKa11] S. Subashini and V. Kavitha, “A survey on security issues in service delivery
models of cloud computing”, Journal of Network and Computer Applications 34
(2011), 1–11.

REFERENCES
1141
[TaEtal98] S. Takakazu et al., “Fermat quotients and the polynomial time discrete log
algorithm for anomalous elliptic curves”, Commentarii Mathematici Universitatis
Sancti Pauli, Rikkyo Daigaku Sugaku Zasshi 47 (1998), 81–92.
[TeHa14] M. Tebaa and S. El Hajji, “Secure cloud computing through homomorphic
encryption”, preprint, 2014; available at http://arxiv.org/abs/1409.0829.
[TeHaGh12] M. Tebaa, S. El Hajji, and A. El Ghazi, “Homomorphic encryption applied
to the cloud computing security”, Proceedings of the World Congress on Engineering,
Vol. I, 2012, 536–539.
[TrWa06] W. Trappe and L. Washington, Introduction to Cryptography with Coding
Theory, 2nd ed., Pearson Education, 2006.
[Tu00] W. T. Tutte, “Fish and I” in Coding Theory and Cryptography, D. Joyner (ed.),
Springer, 2000, 9–17.
[vDEtal10] M. van Dijk, C. Gentry, S. Halevi, and V. Vaikuntanathan, “Fully homo-
morphic encryption over the integers” in Advances in Cryptology–Eurocrypt 2010,
Lecture Notes in Computer Science 6110, Springer, 2010, 24–43.
[vOWi90] P. van Oorschot and M. Wiener, “A known plaintext attack on two-key triple
encryption” in Advances in Cryptology–Eurocrypt ’90, Lecture Notes in Computer
Science 473, Springer, 1990, 318–325.
[WaYu05] X. Wang and H. Yu, “How to break MD5 and other hash functions” in Ad-
vances in Cryptology–Eurocrypt 2005, Lecture Notes in Computer Science 3494,
Springer, 2005, 19–35.
[Wa97] P. Wayner, Digital Cash: Commerce on the Net, Academic Press Professional,
Inc., 1997.
[WeCa81] M. Wegman and J. L. Carter, “New hash functions and their use in authen-
tication and set equality”, Journal of Computer and System Sciences 22 (1981),
265–279.
[Wi90] M. Wiener, “Cryptanalysis of short RSA secret exponents”, IEEE Transactions
on Information Theory 36 (1990), 553–558.
[Wi82] H. C. Williams, “A p + 1 method of factoring”, Mathematics of Computation 39
(1982), 225–234.
[XiLiFe13] T. Xie, F. Liu, and D. Feng, “Fast collision attack on MD5”, IACR Cryptology
ePrint Archive, Report 2013/170.
[Ya82] A. Yao,“Protocols for secure computations”, Proceedings of the 23rd Annual Sym-
posium on Foundations of Computer Science, IEEE, 1982, 160–164.
[YaKhPo13] H. Yap, K. Khoo, and A. Poschmann,“Parallelisable variants of Camellia
and SMS4 block cipher: p-Camellia and p-SMS4”, International Journal of Applied
Cryptography 3 (2013), 1–20.
[ZhEtal09] A. Zhang et al., “Extensions of the cube attack based on low degree annihila-
tors” in Cryptology and Network Security, Lecture Notes in Computer Science 5888,
Springer, 2009, 87–102.
[ZiLe12] D. Zissis and D. Lekkas, “Addressing cloud computing security issues”, Future
Generation Computer Systems 28 (2012), 583–592.

1142
Chapter 15
CRYPTOGRAPHY
Web Resources:
[Ae97] http://csrc.nist.gov/archive/aes/pre-round1/aes 9709.htm (Request for
candidate algorithm nominations for the Advanced Encryption Standard.)
[Cv09] http://cve.mitre.org/cgi-bin/cvename.cgi?name=cve-2009-2409(Common
Vulnerabilities and Exposures, CVE-2009-2409.)
[De98] https://w2.eff.org/Privacy/Crypto/Crypto misc/DESCracker/HTML/199807
16 eff des faq.html (Frequently Asked Questions (FAQ) about the Electronic Fron-
tier Foundation’s DES Cracker machine.)
[Es04] http://www.ecrypt.eu.org/stream/ (eSTREAM, the ECRYPT Stream Cipher
Project.)
[Ja] http://jablon.org/passwordlinks.html (Site for research papers on password-
based cryptography.)
[Jo] http://www.umic.pt/images/stories/publicacoes1/final report.pdf (A Re-
port on the Feasibility of Internet Voting, California Internet Voting Task Force,
January, 2000.)
[Li] http://kodu.ut.ee/~lipmaa/crypto/ (Helger Limpmaa’s cryptology pointers.)
[MeGr11] http://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication
800-145.pdf (Peter Mell and Timothy Grance, The NIST Deﬁnition of Cloud Com-
puting.)
[P1363] http://grouper.ieee.org/groups/1363/ (IEEE P1363 Standard Speciﬁca-
tions for Public Key Cryptography.)
[PGP] http://openpgp.org/ (OpenPGP mail encryption standard.)
[Ri92] http://tools.ietf.org/html/rfc1320 (R. Rivest, The MD4 Message-Digest
Algorithm.)
[Sk98] http://cryptome.org/jya/skipjack-spec.htm (Speciﬁcations for the SKIP-
JACK and KEA algorithms.)
[TuCh11] http://tools.ietf.org/html/rfc6150 (Sean Turner and Lily Chen, MD4 to
Historic Status.)
[Vu08] http://www.kb.cert.org/vuls/id/836068 (Note documenting MD5 vulnerabil-
ity to collision attacks.)

16
DISCRETE OPTIMIZATION
16.1 Linear Programming
Beth Novick
16.1.1 Basic Concepts
16.1.2 Tableaus
16.1.3 Simplex Method
16.1.4 Interior Point Methods
16.1.5 Duality
16.1.6 Sensitivity Analysis
16.1.7 Goal Programming
16.1.8 Integer Programming
16.2 Location Theory
S. Louis Hakimi and
16.2.1 p-Median and p-Center Problems
Maria Albareda
16.2.2 p-Medians and p-Centers on Networks
16.2.3 Algorithms for Location on Networks
16.2.4 Capacitated Location Problems
16.2.5 Facilities in the Plane
16.2.6 Obnoxious Facilities
16.2.7 Equitable Locations
16.3 Packing and Covering
Sunil Chopra and
16.3.1 Knapsacks
David Simchi-Levi
16.3.2 Bin Packing
16.3.3 Set Covering and Partitioning
16.4 Activity Nets
S. E. Elmaghraby
16.4.1 Deterministic Activity Nets
16.4.2 Probabilistic Activity Nets
16.4.3 Complexity Issues
16.5 Game Theory
Michael Mesterton-Gibbons
16.5.1 Noncooperative Games
16.5.2 Matrix and Bimatrix Games
16.5.3 Characteristic-Function Games
16.5.4 Applications
16.6 Sperner’s Lemma and Fixed Points
Joseph R. Barr
16.6.1 Sperner’s Lemma
16.6.2 Fixed-Point Theorems
16.7 Combinatorial Auctions
Robert W. Day
16.7.1 Basic Concepts
16.7.2 Winner Determination and Bid Language
16.7.3 Price Determination and Incentive Compatibility
16.7.4 Demand Revelation

1144
Chapter 16
DISCRETE OPTIMIZATION
16.8 Very Large-Scale Neighborhood Search
Douglas Altner
16.8.1 Basic Concepts
16.8.2 Variable-Depth Neighborhood Search Algorithms
16.8.3 Cyclic Exchange Neighborhood Search Algorithms
16.8.4 Other VLSN Search Algorithms
16.9 Tabu Search
Manuel Laguna
16.9.1 Basic Concepts
16.9.2 Long-Term Memory
16.9.3 Strategic Oscillation
16.9.4 Path Relinking
INTRODUCTION
This chapter discusses various topics in discrete optimization, especially those that arise
in applying operations research techniques to applied problems.
A fundamental tool
is linear programming and its extensions to problems in which certain variables must
assume integer values. These techniques are useful in devising solution techniques for
a variety of problems in which a given resource must be optimally utilized subject to
constraints. The chapter also presents several metaheuristic approaches for approximate
solution of diﬃcult discrete optimization problems.
GLOSSARY
active constraint: an inequality satisﬁed with equality by a given vector.
balanced matrix: a 0-1 matrix having no square submatrix of odd order with exactly
two 1s in each row and column.
basic feasible solution (of an LP): a basic solution that is also a feasible solution.
basic solution (of an LP): a solution obtained by setting certain nonbasic variables to
zero and solving for the remaining basic variables.
bid language: a format for communicating lists of combinatorial bids.
bin packing problem: an optimization problem in which a given set of items is to be
packed using the fewest number of bins.
bounded LP: a linear programming problem having a ﬁnite optimal solution.
capacitated location problem: a location problem in which bounds are placed on
the amount of demand that can be handled by individual facilities.
p-center: a set of p locations for facilities that minimizes the maximum distance from
any demand point to its closest facility.
characteristic function: a mapping from the set of all coalitions to the nonnegative
real numbers.
characteristic-function game: a model for distributing a cooperative beneﬁt fairly
among players when the concept of fairness is based on the bargaining strengths of
coalitions that could form if the players had not already agreed to cooperate.
coalition: any subset of the players in a game.
combinatorial auction: a situation in which multiple items are auctioned, and each
bidder can submit several bids (each of which may contain several items).

GLOSSARY
1145
complete information: a situation arising when a game’s structure is known to all
players.
convex hull: the smallest convex set containing a given set of points.
convex set: a set containing the line segment joining any two of its points.
CPM model: a deterministic activity net with strict precedence among the activities.
critical path: a sequence of activities that determines the completion time of a project.
criticality index: the probability that a given path (activity) is (lies on) a critical path
of a project.
cutting plane: a constraint that can be added to an existing set of constraints without
excluding any feasible integer solution.
decision variables: the unknowns in an optimization problem.
demand point: a point in a metric space that is a source of demand for the service
provided by the facilities.
deterministic activity net: a directed network in which all the parameters (such as
duration, resource requirements, precedence) are known deterministically.
dual LP: a minimization LP problem associated with a given maximization LP prob-
lem.
equilibrium: a strategy combination from which no player has a unilateral incentive
to depart.
facility: a place where a service (or product) is provided.
facility location: a point in a metric space where a facility is located.
feasible direction: a direction that preserves feasibility in a suﬃciently small neigh-
borhood of a given feasible solution.
feasible LP: an LP with a nonempty feasible region.
feasible region: the set of all feasible solutions to a given LP.
feasible solution: a vector that satisﬁes the given set of constraints.
ﬁxed point (of a function f): a point x such that f(x) = x.
ﬂoat: in a deterministic activity net, a measure of the ﬂexibility available in scheduling
an activity without delaying the project completion time.
forward auction: an auction with several buyers and one seller.
frequency-based memory: used in TS to expand the current neighborhood and ex-
amine unvisited regions.
c-game: a characteristic-function game.
GAN model: a probabilistic activity net with conditional progress and probabilistic
realization of activities.
general position: a set of points x1, x2, . . . , xp+1 ∈Rn such that the vectors x2 −x1,
x3 −x1, . . . , xp+1 −x1 are linearly independent.
GERT model: a probabilistic activity net with exclusive-or branching.
goal programming (GP) problem: an LP having multiple objective functions.
improving direction: a feasible direction that improves the objective function value.

1146
Chapter 16
DISCRETE OPTIMIZATION
imputation: a distribution among players of the cooperative beneﬁt in a c-game.
infeasible LP: an LP with an empty feasible region.
integer programming (IP) problem: a linear programming problem in which some
of the decision variables are required to be integers.
interior point method: a technique for solving an LP that iteratively moves through
the interior of the feasible region.
knapsack problem: an optimization problem in which items are to be selected to
maximize the total beneﬁt without exceeding the capacity of the knapsack.
linear programming (LP) problem: an optimization problem involving the selec-
tion of decision variables that optimize a given linear function and that satisfy linear
inequality constraints.
location problem: an optimization problem in which p facilities are to be established
to minimize the cost of meeting known demands arising from n locations.
LP relaxation: the linear programming problem obtained by dropping the integrality
requirements of an IP.
p-median: a set of p locations for facilities that minimizes the total (transportation)
cost of satisfying all demands.
metric space: a set of points on which a distance function has been deﬁned.
mixed strategy: a probability distribution over a set of pure strategies.
move: an operation that modiﬁes a given solution x to obtain a “nearby” solution x′
to an optimization problem.
neighbor (of a solution x): another solution x′ to an optimization problem reached by
a valid move from x.
neighborhood (of a solution x): the set of all neighbors of x.
neighborhood search algorithm: an iterative algorithm that begins with a solution
to an optimization problem and iteratively moves to an improving solution if one
exists.
noncooperative game: a mathematical model of strategic behavior in the absence of
binding agreements.
normalized characteristic function: a mapping from the set of all coalitions to [0, 1].
nucleolus: a c-game solution concept based on minimizing the dissatisfaction of the
most dissatisﬁed coalitions.
objective function: the function associated with a given optimization problem that is
to be maximized or minimized.
optimal solution: a feasible solution to an optimization problem achieving the largest
(or perhaps smallest) value of the objective function.
packing: a subset of items from a given list that can be placed in a bin of speciﬁed
capacity.
path relinking: a search strategy in TS that chooses moves from initiating solutions
that incorporate attributes of guiding solutions.
payoﬀfunction: a mapping from the set of feasible strategy combinations to Rn, where
n is the number of players.

GLOSSARY
1147
perfect information: a situation arising when the history of a game is known to all
players.
PERT model: a probabilistic activity net with strict precedence and activity durations
that are known only in probability.
pivot: a move from a given basic solution of an LP to one diﬀering in only one active
constraint.
players: a collection of interacting decisionmakers.
polyhedron: the set of points satisfying a given ﬁnite set of linear inequalities.
probabilistic activity net: a directed network in which some or all of the parameters,
including the realization of the activities, are probabilistically known.
pure strategy: a plan of action available to a player.
recency-based memory: used in TS to obtain a modiﬁed neighborhood that excludes
(makes tabu) recently visited solutions.
reduced cost: the unit change in the objective function incurred by increasing the
value of a given decision variable.
redundant constraint: a constraint that can be removed from a given set of con-
straints without changing the set of feasible solutions.
set cover: a family of subsets such that each of a speciﬁed list of elements is contained
in at least one subset.
set covering problem: an optimization problem in which a minimum cost set cover is
needed.
set partition: a family of subsets such that each of a speciﬁed list of elements is con-
tained in exactly one subset.
set partitioning problem: an optimization problem in which a minimum cost set par-
tition is needed.
Shapley value: a c-game solution concept based on players’ marginal worths to coali-
tions on joining, assuming all orders of formation are equally likely.
p-simplex: the convex hull of a collection of p + 1 points in general position.
simplex method: a technique for solving an LP that moves from vertex to neighboring
vertex along the boundary of the feasible region.
simplicial subdivision (of a simplex): a decomposition of the simplex into a collection
of simplices that intersect only along entire common faces.
slack variables: the components of b −Ax∗where x∗is a feasible solution to an LP
with constraints Ax ≤b, x ≥0.
solution: an equilibrium or set of equilibria in a noncooperative game, or an imputation
or set of imputations in a c-game.
strategic behavior: behavior such that the outcome of an individual’s actions depends
on actions yet to be taken by others.
strategic oscillation: a search strategy in TS that repeatedly approaches and crosses
the feasibility boundary from diﬀerent directions.
strategy combination: a vector of strategies, one for each player.
tableau: a table storing all information pertinent to a given basic solution for an LP.

1148
Chapter 16
DISCRETE OPTIMIZATION
tabu search (TS): a metaheuristic solution methodology that employs adaptive mem-
ory and responsive exploration of the solution space.
tabu tenure: the number of iterations that an attribute remains tabu-active in TS,
thus preventing the revisiting of recently examined solutions.
totally unimodular matrix: a 0-1 matrix such that every square submatrix has de-
terminant 0, +1, or −1.
unbounded LP: a linear programming problem that is not bounded.
variable-depth neighborhood search algorithm: a neighborhood search algorithm
that moves to a new solution in a neighborhood of variable depth at each iteration.
very large-scale neighborhood: a neighborhood that has very large size with respect
to the problem input.
very large-scale neighborhood search algorithm: a neighborhood search algorithm
that selects from a very large-scale neighborhood at each iteration.
vertex (of a feasible region): given a feasible region S, a point x ∈S ⊆Rn deﬁned by
the intersection of exactly n linearly independent constraints.
Vickrey-Clarke-Groves (VCG) mechanism: an eﬃcient auction mechanism in which
each bidder has an incentive to be honest about his/her valuation of the items.
winner determination problem: the problem of ﬁnding a set of winning bids to max-
imize total monetary value in an auction.
16.1
LINEAR PROGRAMMING
Linear programming (LP) involves the optimization of a linear function under linear in-
equality constraints. Applications of this model are widespread, including problems aris-
ing in marketing, ﬁnance, inventory, capital budgeting, computer science, transportation,
and production. Algorithms are available that, in practice, solve LP problems eﬃciently.
16.1.1
BASIC CONCEPTS
Deﬁnitions:
A linear programming (LP) problem is an optimization problem that can be written
maximize:
cx
subject to:
eAx ≤eb
(1)
where eA is a given q × n matrix, c is a given row vector of length n, and eb is a given
column vector of length q. The decision variables of problem (1) are represented by
the column vector x of length n.
A feasible solution is a vector x satisfying eAx ≤eb. The feasible region is the subset
of all feasible solutions in Rn. If no feasible solution exists (so that the feasible region is
empty), the LP problem is infeasible; otherwise it is feasible.

Section 16.1
LINEAR PROGRAMMING
1149
Each of the q inequalities in eAx ≤eb is a constraint. A constraint is redundant if
removing it from (1) doesn’t change the feasible region.
For a feasible solution x, the function z = cx is the objective function, with cx the
objective value of x. When the objective value z∗= cx∗is also maximum, then the
feasible x∗is an optimal solution. If the objective value can be made arbitrarily large
over the feasible region, the LP problem is unbounded. Otherwise it is bounded.
A vector y is a feasible direction at x∗if there is some τ > 0 such that eA(x∗+ λy) ≤eb
for all 0 ≤λ ≤τ. If cy > 0 also holds, then y is an improving direction.
A constraint of the system eAx ≤eb that is satisﬁed with equality by a feasible solution x∗
is active at x∗.
A set of constraints {aix ≤bi | i = 1, 2, . . . , k} is linearly independent if the vectors
{a1, a2, . . . , ak} are linearly independent (see §6.1.3).
A vertex is a feasible solution with n linearly independent active constraints. A vertex
with more than n active constraints is degenerate. An LP problem with a degenerate
vertex is degenerate.
A set S is convex if the line segment joining any two of its points is contained in S: i.e.,
λx + (1 −λ)y ∈S holds for all x, y ∈S and 0 ≤λ ≤1.
Let L be the line segment connecting the two vertices x1 and x2. Then x1 and x2 are
adjacent if for all points y ̸= x1, x2 on L and all feasible y1 and y2, the only way y can
equal 1
2y1 + 1
2y2 is if y1 and y2 are also on L. In this case, L is an edge.
Facts:
1. Linear programming models arise in a wide variety of applications, which typically
involve the allocation of scarce resources in the best possible way. A sample of such
application areas, with reference sources, is given in the following table.
application
references
production scheduling and inventory control
[Ch83], [Ga85]
tanker scheduling
[BaJaSh90]
airline scheduling
[Ga85]
cutting stock problems
[BaJaSh90], [Ch83]
workforce planning
[Ga85]
approximation of data
[Ch83]
matrix games
[Ch83]
blending problems
[BaJaSh90]
petroleum reﬁning
[Ga85]
capital budgeting
[BaJaSh90]
military operations
[Ga85]
land use planning
[Ga85]
agriculture
[Ga85]
banking and ﬁnance
[Ga85]
environmental economics
[Ga85]
health care
[Ga85]
marketing
[Ga85]
public policy
[Ga85]

1150
Chapter 16
DISCRETE OPTIMIZATION
2. The general concepts of linear programming were ﬁrst developed by G. B. Dantzig
in 1947 in connection with military planning problems for the U.S. Air Force. Earlier,
in 1939, L. V. Kantorovich formulated and solved a particular type of LP problem in
production planning.
3. The term “linear programming” conveys its historical origins and purpose: it is a
mathematical model involving linear constraints and a linear objective function, used
for the optimal planning (programming) of operations.
4. Form (1) of an LP naturally occurs in the selection of levels for production activities
that maximize proﬁt subject to constraints on the utilization of the given resources.
5. These transformations on an LP do not change feasible (or optimal) solutions:
• constraints:
change the sense of an inequality by multiplying both sides by −1;
or replace aix = bi with aix ≤bi and −aix ≤−bi; or replace aix ≤bi with
aix + si = bi and si ≥0;
• variables:
for xj unrestricted, set xj := x′
j −x′′
j with x′
j, x′′
j ≥0; or for xj ≤0,
set xj := −x′
j with x′
j ≥0;
• objective function:
change a minimization (maximization) problem to a maxi-
mization (minimization) problem by setting c := −c.
6.
Farkas’ lemma:
Suppose eA is a q × n matrix and c is an n-row vector. Then the
following are equivalent:
• cy ≥0 for all y ∈Rn such that eAy ≥0;
• there exists some u ∈Rq such that u ≥0, c = u eA.
This result is important in establishing the optimality conditions for linear programming
problems; it can also be applied to show the existence (and uniqueness) of solutions
to linear models of economic exchange and stationary distributions in ﬁnite Markov
chains (§7.7). (J. Farkas, 1847–1930)
7. A feasible solution with an improving direction cannot be optimal for (1).
8. A feasible solution with no improving direction is always optimal for (1).
9. If a feasible solution to (1) has an improving direction y and if eAy ≤0 then the LP
problem is unbounded.
10. Each LP problem is either infeasible, unbounded, or has an optimal solution. This
need not be the case for nonlinear optimization problems.
11. Form (1) of an LP is helpful for understanding the geometric properties of an LP.
12. For algorithmic purposes the following form, form (2), of an LP is preferred:
maximize:
cx
subject to:
Ax ≤b
x ≥0.
(2)
Here A is an m × n matrix.
13. The most general form of an LP problem is
maximize (or minimize):
dx1 + ex2 + fx3
subject to:
Ax1 + Bx2 + Cx3 ≤a
Dx1 + Ex2 + Fx3 ≥b
Gx1 + Hx2 + Kx3 = c
x1 ≥0, x2 ≤0, x3 unrestricted.

Section 16.1
LINEAR PROGRAMMING
1151
In this formulation A, B, C, D, E, F, G, H, and K are matrices; a, b, and c are column
vectors; and d, e, and f are row vectors.
14. The feasible region of an LP problem is a convex set.
15. Equivalence of forms:
The general form in Fact 13 is equivalent to both form (1)
and form (2) in the following sense: any of these three forms can be transformed into
another using the operations of Fact 5. Each form possesses the same set of feasible (or
optimal) solutions.
16. An excellent glossary of linear programming terms, as well as concepts in general
mathematical optimization, can be found at
• http://glossary.computing.society.informs.org/ver2/mpgwiki/
17. A comprehensive introduction to building linear programming models and a discus-
sion of various applications are found in Williams [Wi13].
Examples:
1. Feed mix:
A manufacturer produces a special feed for farm animals. To ensure
that the feed is nutritionally balanced, each bag of feed must supply at least 1250 mg of
Vitamin A, 250 mg of Vitamin B, 900 mg of Vitamin C, and 232.5 mg of Vitamin D.
Three diﬀerent grains (1, 2, 3) are blended to create the ﬁnal product. Each ounce of
Grain 1 supplies 2, 1, 5, 0.6 mg of Vitamins A, B, C, D, respectively. Each ounce of
Grain 2 provides 3, 1, 3, 0.25 mg of Vitamins A, B, C, D, while each ounce of Grain 3
provides 7, 1 mg of Vitamins A, D. The costs (per ounce) of the constituent grains are
41, 35, and 96 cents for Grains 1, 2, and 3, respectively.
The manufacturer wants to determine the minimum cost mix of grains that satisﬁes all
four nutritional requirements. If xi is the number of ounces of Grain i that are blended
in the ﬁnal product, then the manufacturer’s problem is modeled by the following LP:
minimize:
0.41x1 + 0.35x2 + 0.96x3
subject to:
2x1 + 3x2 + 7x3 ≥1250
x1 + x2 ≥250
5x1 + 3x2 ≥900
0.6x1 + 0.25x2 + x3 ≥232.5
x1, x2, x3 ≥0.
Each constraint in this LP corresponds to a nutritional requirement. It turns out that
the optimal solution to the LP is x∗
1 = 200.1, x∗
2 = 49.9, x∗
3 = 100.01 with z∗= 195.5.
Note that the amount of Vitamin C supplied by this solution is in excess of 900 mg, while
the other vitamins are supplied in exactly the minimum amounts.
2. The LP in Example 1 is not in either form (1) or form (2). However, using Fact 5 it
can be transformed into form (2), giving the equivalent representation
maximize:
−0.41x1 −0.35x2 −0.96x3
subject to:
−2x1 −3x2 −7x3 ≤−1250
−x1 −x2 ≤−250
−5x1 −3x2 ≤−900
−0.6x1 −0.25x2 −x3 ≤−232.5
x1, x2, x3 ≥0.

1152
Chapter 16
DISCRETE OPTIMIZATION
3. The following ﬁgure shows the feasible region of the LP problem
maximize:
−x1
subject to:
−x2 ≤0
(A)
−x1 −x2 ≤−4
(B)
−x1 + x2 ≤4
(C)
−3x1 + 5x2 ≤30
(D)
−x1 + 3x2 ≤22
(E)
x
A
D
B
E
(5,9)
(6,5)
(0,4)
(4,0)
2
x 1
This LP has n = 2 decision variables x1, x2 and it has the vertices (4, 0), (0, 4), and (5, 9).
Vertex (x1, x2) = (0, 4) is the optimal solution, achieving the maximum objective value
z = 0. Thus, the LP is bounded, even though its feasible region is not bounded. Con-
straint (D) is redundant, since dropping it doesn’t change the feasible region. Vertex (5, 9)
is degenerate, since 3 > n constraints are active at this vertex. All vectors are feasible
directions at (6, 5). At vertex (5, 9), the direction (1, −1) is feasible, but the direction
(1, 1) is not. Vertices (0, 4) and (5, 9) are adjacent, as are (4, 0) and (0, 4).
4. Farkas’ lemma: The row vectors a1, a2 of eA =
 
2
4
5
2
!
are shown in the next ﬁgure.
C
y
Y
Y
a1
a2
2
y1
The set Y = {y | eAy ≥0} is the region bounded by the two dashed lines. Notice that
if c = u eA for some u ≥0 then c must lie in the cone C bounded by the vectors a1
and a2. Geometrically, any c ∈C makes an acute angle with every y ∈Y , hence cy ≥0.
Conversely, any c making an acute angle with every y ∈Y must be in C.

Section 16.1
LINEAR PROGRAMMING
1153
5. Fact 10 is illustrated using the following LP problem:
maximize:
−x1 −x2
subject to:
−2x1 + x2 ≤−1
−x1 −2x2 ≤−2
x1, x2 ≥0 .
This LP has the optimal solution (x∗
1, x∗
2) = ( 4
5, 3
5). Suppose the objective function is
changed to z = x1−x2. Then (x1, x2) = (a, 0) is feasible for a ≥2 with objective value a.
Thus z can be made arbitrarily large and the LP is unbounded. On the other hand, if
the second constraint is changed to 4x1 −x2 ≤−1, the feasible region is empty and the
LP is infeasible.
6. Product mix: A company manufactures n types of a product, using m shops. Type j
requires aij machine-hours in shop i. There is a limitation of bi machine-hours for shop i
and the sale of each type j unit brings the company a proﬁt cj. The optimization problem
facing the company is given by an LP problem in form (2). Namely, if xj is the number
of units produced of type j, then the optimization problem is
maximize:
nP
j=1
cjxj
subject to:
nP
j=1
aijxj ≤bi,
i = 1, . . . , m
xj ≥0,
j = 1, . . . , n.
7. Transportation:
A product stored at m warehouses needs to be shipped to satisfy
demands at n markets. Warehouse i has a supply of si units of the product, and market j
has a demand of dj units. The cost of shipping a unit of product from warehouse i to
market j is cij.
The problem is to determine the number of units xij to ship from
warehouse i to market j in order to satisfy all demands while minimizing cost:
maximize:
m
P
i=1
nP
j=1
cijxij
subject to:
nP
j=1
xij ≤si,
i = 1, . . . , m
m
P
i=1
xij = dj,
j = 1, . . . , n
xij ≥0,
i = 1, . . . , m, j = 1, . . . , n.
This is an LP in the form speciﬁed by Fact 13. Using the transformations in Fact 5, this
optimization problem can alternatively be expressed as an LP in the form (1).
16.1.2
TABLEAUS
Deﬁnitions:
Suppose that an LP is expressed in form (2) of §16.1.1, with A an m × n matrix. A
tableau is any table
u
z
D
f
with the following properties:
• D is an m×(m+n) matrix with entries dtj; z is a real number; u is an (m+n)-row
vector; and f is an m-column vector.

1154
Chapter 16
DISCRETE OPTIMIZATION
• Associated with the tableau is a partition ΣB, ΣN of the integers 1, . . . , m + n.
The set ΣB, with cardinality m, is the basic set and ΣN is the nonbasic set.
• For every row index t = 1, . . . , m, there is a column of D equal to zero in all
coordinates except for the tth coordinate, which equals 1. The index of this
column is ϕ(t) where ϕ is a function from {1, . . . , m} to ΣB associated with the
tableau.
•
u
z
D
f
can be obtained from
−c
0
0
A
I
b
(where ΣB = {n + 1, . . . , m + n} and
ϕ(t) = n+t, t = 1, . . . , m) by performing the following pivot operation a ﬁnite
number of times:
P1. choose a row index t∗∈{1, . . ., m} and a column index j∗∈ΣN with
dt∗j∗̸= 0;
P2. multiply row t∗by 1/(dt∗j∗);
P3. add appropriate multiples of row t∗to all other rows to make uj∗= 0 and
to make dtj∗= 0 for all t ̸= t∗;
P4. remove j∗from ΣN and place it in ΣB; remove ϕ(t∗) from ΣB and place
it in ΣN; set ϕ(t∗) = j∗.
In the pivot operation, ϕ(t∗), before replacement, is the index of the leaving variable
and j∗is the index of the entering variable.
The set of variables {xi | i ∈ΣB} are the basic variables and the remaining variables
are the nonbasic variables.
A basic solution is a vector x∗with its basic variables deﬁned by x∗
i = ft where
t = ϕ−1(i); its nonbasic variables have x∗
i = 0. If f ≥0 then x∗is a basic feasible
solution (BFS).
The basis matrix B is the m×m matrix consisting of the columns of [A I ] correspond-
ing to the basic variables; the nonbasis matrix N is the m × n matrix corresponding
to the nonbasic variables. Let cB [cN] denote the vector of basic [nonbasic] components
of c. Let xB [xN] denote the vector of basic [nonbasic] components of x.
The reduced cost of nonbasic variable xj is the negative of uj in the associated tableau.
The slack variables are given by (xn+1, xn+2, . . . , xn+m) = b −Ax.
Facts:
1. Every BFS of (2) corresponds to a vertex of (1), with q = m+n, eA =
"
A
−I
#
, eb =
"
b
0
#
.
2. In the absence of degeneracy the correspondence in Fact 1 is one-to-one; otherwise it
is many-to-one.
3. Every LP problem (2) with an optimal solution has an optimal solution that is a
vertex. Since the number of vertices is ﬁnite, LP problems are combinatorial in nature;
that is, an LP can be solved in theory by enumerating its vertices and then selecting one
with maximum objective value.
4. Let x∗be a BFS of (2). All information pertinent to x∗is contained in its tableau
which (after possibly permuting the ﬁrst m + n columns) is
0
cBB−1N −cN
cBB−1b
I
B−1N
B−1b
.

Section 16.1
LINEAR PROGRAMMING
1155
Here cBB−1b is the objective value z of x∗. The value of the basic variable x∗
i is the tth
component of B−1b, where i = ϕ(t). Every nonbasic variable has value 0.
5. A tableau expresses the set of equations below, called a dictionary [Ch83]:
xB = B−1b −B−1NxN
z = cBB−1b + (cN −cBB−1N)xN.
6. The reduced costs of the nonbasic variables are given by the vector cN −cBB−1N.
Basic variables have zero reduced cost.
7. Column i of B−1 is identical to dm+i, the column of D associated with the slack
variable xn+i.
Examples:
1. When slack variables x4, x5, x6 are added to the LP
maximize:
3x1 + 4x2 + 4x3
subject to:
3x1 −x3 ≤5
−9x1 + 4x2 + 3x3 ≤12
−6x1 + 2x2 + 4x3 ≤2
x1, x2, x3 ≥0
the following equivalent LP is formed:
maximize:
3x1 + 4x2 + 4x3 + 0x4 + 0x5 + 0x6
subject to:
3x1 −x3 + x4 = 5
−9x1 + 4x2 + 3x3 + x5 = 12
−6x1 + 2x2 + 4x3 + x6 = 2
x1, x2, . . . , x6 ≥0.
The associated tableau, with ΣB = {4, 5, 6} and ΣN = {1, 2, 3}, is then
−3
−4
−4
0
0
0
0
3
0
−1
1
0
0
5
−9
4
3
0
1
0
12
−6
2
4
0
0
1
2
Here ϕ(1) = 4, ϕ(2) = 5, ϕ(3) = 6. The basic variables are x4 = 5, x5 = 12, x6 = 2 and
the nonbasic variables are x1 = 0, x2 = 0, x3 = 0. The basic feasible solution associated
with this tableau is x = (0, 0, 0, 5, 12, 2)T with objective value z = 0. The nonbasic
variables x1, x2, x3 have reduced costs 3, 4, 4, respectively.
2. A pivot is now performed on the tableau in Example 1 using t∗= 3 and j∗= 2,
so the entering variable is x2 and the leaving variable is x6. The resulting tableau (a)
follows, where ΣB = {2, 4, 5} and ϕ(1) = 4, ϕ(2) = 5, ϕ(3) = 2. The corresponding BFS
is x = (0, 1, 0, 5, 8, 0)T with objective value z = 4. If a pivot is performed on (a) using
t∗= 1 and j∗= 1, then tableau (b) results. Here ϕ(1) = 1, ϕ(2) = 5, ϕ(3) = 2 and the
new BFS is x = ( 5
3, 6, 0, 0, 3, 0)T with objective value z = 29.
tableau (a)
−15
0
4
0
0
2
4
3
0
−1
1
0
0
75
3
0
−5
0
1
−2
8
−3
1
2
0
0
1
2
1
tableau (b)
0
0
−1
5
0
2
29
1
0
−1
3
1
3
0
0
5
3
0
0
−4
−1
1
−2
3
0
1
1
1
0
1
2
6

1156
Chapter 16
DISCRETE OPTIMIZATION
For tableau (b), the basis matrix B corresponds to columns 1, 5, 2 of [A I ], namely
B =



3
0
0
−9
1
4
−6
0
2


. From Fact 7, the inverse matrix B−1 =



1
3
0
0
−1
1
−2
1
0
1
2


consists of
columns 4, 5, 6 in tableau (b).
16.1.3
SIMPLEX METHOD
The simplex method is in practice remarkably eﬃcient and it is widely used for solving
LP problems.
The solution idea dates back to J. B. J. Fourier (1768–1830); it was
developed and popularized in 1947 by G. B. Dantzig (1914–2005). This section presents
two descriptions of the same algorithm: the ﬁrst is geometrically intuitive, the second is
closer to its actual implementation.
Facts:
1. Simplex algorithm I: This method (Algorithm 1) solves a linear programming prob-
lem in form (1) of §16.1.1. Assuming that an initial vertex is known, this algorithm
travels from vertex to vertex along improving edges until an optimal vertex is reached
or an unboundedness condition is detected. In Algorithm 1, the rows of eA are denoted
a1, a2, . . . , aq and eb has the corresponding components b1, b2, . . . , bq.
Algorithm 1:
Simplex algorithm — form (1).
input: LP in form (1), initial vertex x0
output: an optimal vertex or an indication of unboundedness
k := 0
ﬁnd a subsystem Bx ≤r of (1) consisting of n linearly independent constraints
active at xk
S := list containing the indices of these active constraints
{Main loop}
if u ≡cB−1 ≥0 then xk is an optimal solution — stop
else {an improving direction}
i∗:= the smallest index such that ui∗< 0
y := column i∗of −B−1
if eAy ≤0 then the LP problem is unbounded — stop
else {move to next vertex (possibly same as last)}
j∗:= smallest index j attaining minimum λ ≡min
 bj−ajxk
ajy
| j /∈S, ajy > 0
	
xk+1 := xk + λy
S[i∗] := j∗; update B
k := k + 1
{Continue with next iteration of main loop}
2. Simplex algorithm II: This method (Algorithm 2) solves a linear programming prob-
lem in form (2) of §16.1.1, assuming that b ≥0. It proceeds by successively identifying
nonbasic variables having positive reduced cost and pivoting them into the current basis
in a way that maintains a basic feasible solution (BFS).

Section 16.1
LINEAR PROGRAMMING
1157
Algorithm 2:
Simplex algorithm — form (2).
input: LP in form (2), with b ≥0
output: an optimal BFS or an indication of unboundedness
begin with the initial tableau:
−c
0
0
A
I
b , where ΣB = {n + 1, . . . , m + n}
and ϕ(t) = n + t, t = 1, . . . , m
x0 := (xB, xN) where xB = b ≥0 and xN = 0
k := 0
{Main loop}
if uj ≥0 for all j ∈ΣN then xk is an optimal solution — stop
else {select entering variable}
j∗:= the smallest index with uj∗< 0
if dtj∗≤0 for t = 1, . . . , m then the LP is unbounded — stop
else
t∗:= an index t achieving the minimum
min
 ft
dtj∗| t = 1, . . . , m; dtj∗> 0
	
(if there are several such t∗, make ϕ(t∗) as small as possible)
do a pivot with entering index j∗, leaving index ϕ(t∗)
set component ϕ(t) of xk+1 to ft for t = 1, . . . , m and the remaining com-
ponents of xk+1 to zero
k := k + 1
{Continue with next iteration of main loop}
3. There are examples for which Algorithm 1 requires exponential running time, and
similarly for Algorithm 2.
4. In practice the number of iterations of Algorithms 1 and 2 is proportional to the
number of constraints m and grows slowly with the number of variables n.
5. There is a one-to-one correspondence between the vertex xk of Algorithm 1 and the
BFS xk of Algorithm 2, when eA is set to
"
A
−I
#
and eb is set to
"
b
0
#
.
6. Interchanging a basic and a nonbasic variable in Algorithm 2 corresponds to inter-
changing a nonactive and an active constraint in Algorithm 1.
7. In the absence of degeneracy, the objective value strictly increases at each step (in
both algorithms). The method of breaking ties by choosing the smallest index prevents
cycling and ensures termination in ﬁnite time. In practice, though, cycling is rare and
other rules are used.
8. When a vertex is not known in Algorithm 1 (when b ̸≥0 in Algorithm 2) a preliminary
LP problem, Phase I, can be solved to get an initial vertex (a starting tableau).
9. The revised simplex method is a variation of Algorithm 2. Instead of maintaining
the entire tableau at each step only B−1 is kept. Columns of [ A I ] are brought in from
storage as needed to ﬁnd j∗and t∗. This method is good for sparse matrices A with
many columns.
10. LP problems can be submitted online for solution using the NEOS server at

1158
Chapter 16
DISCRETE OPTIMIZATION
• http://www.neos-server.org/neos
11. A survey of software packages for solving linear programming problems is described
in [Fo15]. Virtually all of these products run on PCs. In many cases, the LP solvers are
linked to more general modeling packages that provide a single environment for carrying
out the formulation, solution, and analysis of LP problems.
12. Many software packages are available to model and to solve LP problems.
–The AMPL and GAMS modeling languages are described at
• http://www.ampl.com
• http://www.gams.com
–Solver software (LINDO API and MATLAB) can be found at
• http://www.lindo.com
• http://www.mathworks.com/products/optimization
–Integrated solver and modeling environments are described at
• http://www.aimms.com
• http://optimization-suite.coin-or.org
• http://www.cplex.com
• http://www.gurobi.com
• http://www.lindo.com
Examples:
1. The LP in Example 1 of §16.1.2 can be placed in the form (1) with
eA =









3
0
−1
−9
4
3
−6
2
4
−1
0
0
0
−1
0
0
0
−1









,
eb =









5
12
2
0
0
0









,
c = (3 4 4).
If x0 = (0, 1, 0)T then constraints 3, 4, 6 are active at x0 and S = [3, 4, 6]. Thus
B =



−6
2
4
−1
0
0
0
0
−1


,
B−1 =



0
−1
0
1
2
−3
2
0
0
−1


,
u = cB−1 = (2 −15 4).
Here i∗= 2, y = (1, 3, 0)T , eAy = (3, 3, 0, −1, −3, 0)T, and eb −eAx0 = (5, 8, 0, 0, 1, 0).
Then λ = min{ 5
3, 8
3} = 5
3 and j∗= 1. The new vertex is x1 = x0 + λy = ( 5
3, 6, 0)T and S
is updated to S = [3, 1, 6], so B now contains rows 3, 1, 6 of eA. Additional iterations of
Algorithm 1 can then be carried out using the updated S and B.
2. The same LP can alternatively be solved using Algorithm 2. For illustration, suppose
that the tableau (a) from Example 2 (§16.1.2) is given, corresponding to the BFS x1 =
(0, 1, 0, 5, 8, 0)T and ΣB = {2, 4, 5}. Here u = (−15, 0, 4, 0, 0, 2) and j∗= 1 is chosen.
The minimum ratio test gives min{ 5
3, 8
3} =
5
3 and t∗= 1. The next pivot produces
tableau (b) in Example 2 (§16.1.2), with ΣB = {1, 2, 5} and x2 = ( 5
3, 6, 0, 0, 3, 0)T. Here
u = (0, 0, −1, 5, 0, 2) so a further pivot is performed using j∗= 3 and t∗= 3, giving the

Section 16.1
LINEAR PROGRAMMING
1159
tableau below. Since u ≥0 the BFS x3 = ( 11
3 , 0, 6, 0, 27, 0)T is an optimal solution to
the LP, with optimal objective value z∗= 35.
0
1
0
6
0
5
2
35
1
1
3
0
2
3
0
1
6
11
3
0
4
0
3
1
0
27
0
1
1
1
0
1
2
6
16.1.4
INTERIOR POINT METHODS
There are numerous interior point methods for solving LP problems. In contrast to the
simplex method, which proceeds from vertex to vertex along edges of the feasible region,
these methods move through the interior of the feasible region. In particular this section
discusses N. Karmarkar’s “projective scaling” algorithm (1984).
Deﬁnitions:
The norm of x ∈Rn is given by ∥x∥=
p
x2
1 + x2
2 + · · · + x2n. (See §6.1.4.)
Let e denote the row vector of n 1s.
The LP problem
minimize:
z = cx
subject to:
Ax = 0
ex = 1
x ≥0
(3)
is in standard form for Karmarkar’s method if 1
n e is a feasible vector and if the optimal
objective value is z∗= 0.
The n × n diagonal matrix diag(x1, x2, . . . , xn) has diagonal entries x1, x2, . . . , xn. (See
§6.3.1.)
The unit simplex in n dimensions is Sn = {x ∈Rn | ex = 1, x ≥0}.
If x is feasible to (3), Karmarkar’s centering transformation Tx : Sn →Sn is
Tx(x) =
diag(x)−1x
e diag(x)−1x.
The projection of a vector v onto the subspace X ≡{x ∈Rn | eAx = 0} is the unique
vector p ∈X for which (v −p)T x = 0 for all x ∈X. (See §6.1.4.)
Karmarkar’s potential function for (3) is f(x) =
nP
j=1
ln
  cx
xj

.
Facts:
1. Any LP problem can be transformed into form (3); see [BaJaSh90], [Sc86] for details.
2. The centering transformation Tx is one-to-one and onto.
3. The inverse of the centering transformation is
T −1
x (y) =
diag(x) y
e diag(x) y .

1160
Chapter 16
DISCRETE OPTIMIZATION
4. The transformation Tx places x at the center of the transformed unit simplex: Tx(x) =
1
n e.
5. The transformation Tx maps the feasible region of (3) to
Y = {y ∈Rn | A diag(x) y = 0, ey = 1, y ≥0}.
6. W = {w ∈Rn | A diag(x) w = 0, ew = 0, w ≥0} is the set of all feasible directions
for Y .
7. The projection of v onto W is [I −P T (PP T )−1P]v, where P =
 
A diag(x)
1 1 · · · 1
!
.
8. Karmarkar’s algorithm:
This method (Algorithm 3) moves through the interior of
the feasible region of (3), transforming the problem at each iteration to place the current
point at the “center” of the transformed region.
Algorithm 3:
Karmarkar’s method.
input: LP in form (3)
output: an optimal solution to (3)
x0 := e
n
k := 0
{Main loop}
{Test for optimality within ǫ}
if cxk < ǫ then stop
else {ﬁnd new point y in transformed unit simplex}
P :=
 
A diag(xk)
1 1 · · · 1
!
cP := [I −P T (PP T )−1P] diag(xk) cT
yk := e
n −

θ
√
n(n−1)

cP
∥cP ∥
{ﬁnd new feasible point in the original space}
xk+1 := T −1
xk (yk)
k := k + 1
{Continue with next iteration of main loop}
9. In Algorithm 3, ǫ > 0 is a ﬁxed tolerance chosen arbitrarily small. The parameter θ
is a constant, 0 < θ < 1, associated with convergence of the algorithm. The value θ = 1
4
ensures the convergence of Algorithm 3.
10. There is a positive constant δ with f(xk) −f(xk+1) ≥δ for all iterations k of
Karmarkar’s method. To ensure this inequality diag(xk) c, rather than c, is projected
onto the space of feasible directions W.
11. For large problems, Karmarkar’s method requires many fewer iterations than does
the simplex method.
12. Letting L be the maximum number of bits needed to represent any number asso-
ciated with the LP problem, the running time of Karmarkar’s algorithm is polynomial,
namely O(n3.5L2).
13. The earliest polynomial-time algorithm for LP problems is the ellipsoid method,
proposed by L. G. Khachian in 1979. (See [Ch83], [Sc86].)

Section 16.1
LINEAR PROGRAMMING
1161
14. The ellipsoid method has worst-case complexity O(n6L2), where L is deﬁned in
Fact 12. Because its calculations require high precision, this method is very ineﬃcient in
practice.
15. Karmarkar’s polynomial-time algorithm was announced in 1984 and it has proven
to be seriously competitive with the simplex method. Typically, Karmarkar’s algorithm
reduces the objective function by fairly signiﬁcant amounts at the early iterations, often
converging within 50 iterations regardless of the problem size.
16. More eﬃcient interior point methods have been developed and have proven to be
competitive with, and often superior to, the best simplex packages, especially for large
LP problems [RoTeVi10], [Wr87].
17. LP problems can be submitted online for solution by several solvers that implement
interior-point methods using the NEOS server at
• http://www.neos-server.org/neos
18. The LP solver packages mentioned in Fact 12 of §16.1.3 all support solution by
interior point algorithms.
Example:
1. In the following LP the vector x = ( 1
3, 1
3, 1
3)T is feasible and the problem has the
optimal objective value z∗= 0, achieved for x∗= (0, 2
3, 1
3)T .
minimize:
x1
subject to:
x1 + x2 −2x3 = 0
x1 + x2 + x3 = 1
x1, x2, x3 ≥0.
Karmarkar’s algorithm is started with x0 = ( 1
3, 1
3, 1
3)T , yielding cx0 = 1
3. For illustrative
purposes the value θ = 0.9 is used throughout. Since A = (1 1 −2) the matrix P =
 
1
3
1
3
−2
3
1
1
1
!
, giving cP = ( 1
6, −1
6, 0) and y0 = (0.0735, 0.5931, 0.3333)T = x1. The new
objective value is cx1 = 0.0735. Additional iterations of Algorithm 3 are tabulated in
the following table, showing convergence to the optimal x∗= (0, 2
3, 1
3)T after just a few
iterations.
k
0
1
2
3
4
xk



0.3333
0.3333
0.3333






0.0735
0.5931
0.3333






0.0056
0.6611
0.3333






0.0004
0.6663
0.3333






0.0000
0.6666
0.3333



yk



0.0735
0.5931
0.3333






0.0349
0.5087
0.4564






0.0333
0.4852
0.4814






0.0333
0.4835
0.4832






0.0333
0.4833
0.4833



cxk
0.3333
0.0735
0.0056
0.0004
0.0000
16.1.5
DUALITY
Associated with every LP problem is its dual problem, which is important in devising
alternative solution procedures for the original LP. The dual also provides useful infor-
mation for conducting postoptimality analyses on the given LP.

1162
Chapter 16
DISCRETE OPTIMIZATION
Deﬁnitions:
Associated with every LP problem is another LP problem, its dual. The original problem
is called the primal.
The dual of an LP in form (2)
maximize:
cx
subject to:
Ax ≤b
x ≥0
is deﬁned to be the LP
minimize:
ub
subject to:
uA ≥c
u ≥0.
(4)
The components u1, u2, . . . , um of u are the dual variables.
Facts:
1. To ﬁnd the dual of an arbitrary LP problem either transform it (§16.1.1, Fact 5) into
form (2) or use the following table:
primal
dual
maximization problem
minimization problem
unrestricted variable
equality constraint
nonnegative variable
≥constraint
nonpositive variable
≤constraint
equality constraint
unrestricted variable
≤constraint
nonnegative variable
≥constraint
nonpositive variable
2. The dual of the dual LP is the primal LP.
3. Weak duality theorem:
For any feasible solution x to the primal and any feasible
solution u to the dual cx ≤ub.
4. Strong duality theorem:
If x∗is an optimal solution to (2) then there exists an
optimal solution u∗for (4) and cx∗= u∗b.
5. A given primal LP and its associated dual LP can only produce certain combinations
of outcomes, as speciﬁed in the following table. For example, if one problem is unbounded
then the other must be infeasible.
primal
dual
optimal
optimal
infeasible
unbounded
unbounded
infeasible
infeasible
infeasible
6. Let x∗be an optimal BFS of the primal LP (2), with the corresponding tableau
u
z
D
f . Then u is an optimal BFS of the dual LP (4).
7. Complementary slackness: An optimal dual (primal) variable u∗
i (x∗
j) can be nonzero
only if it corresponds to a primal (dual) constraint active at x∗(u∗).

Section 16.1
LINEAR PROGRAMMING
1163
8. Economic interpretation: Suppose in the LP (2) that bi is the amount of resource i
available to a ﬁrm maximizing its proﬁt. Then the optimal dual variable u∗
i is the price
the ﬁrm should be willing to pay (over and above its market price) for an extra unit of
resource i.
9. Dual simplex algorithm:
This approach (Algorithm 4) can be used when a basic
solution for (2) is known that is not necessarily feasible but which has nonnegative
reduced costs (i.e., it is a dual feasible basic solution). The main idea of the algorithm
is to start with the dual feasible basic solution and to maintain dual feasibility at each
pivot. An optimal BFS is found once primal feasibility is achieved.
Algorithm 4:
Dual simplex algorithm.
input: LP in form (2), dual feasible basic solution x0
output: an optimal BFS or an indication of infeasibility
associate with x0 = (xB, xN) = (f, 0) the tableau
u
z
D
f , where u ≥0
k := 0
{Main loop}
{Optimality test}
if f ≥0 then xk is an optimal solution — stop
else
t∗:= the smallest index with ft∗< 0
if dt∗j ≥0 for all j then the LP is infeasible — stop
else
j∗:= the smallest index attaining the maximum
max
 uj
dt∗j | j = 1, . . . , m + n; dt∗j < 0
	
do a pivot with entering index j∗, leaving index ϕ(t∗)
set component ϕ(t) of xk+1 to ft for t = 1, . . . , m and the remaining
components of xk+1 to zero
k := k + 1
{Continue with next iteration of main loop}
10. The dual simplex method was devised in 1954 by C. E. Lemke (1920–2004).
11. C++ code that implements the dual simplex algorithm can be found at
• http://www.codeforge.com/article/214515
Examples:
1. Using the table of Fact 1, the dual of
maximize:
5x1 −7x2
subject to:
x1 + 3x2 −x3 + x4 ≤−1
2x1 + x2 −4x3 −x4 ≥3
x1 + x2 −3x3 + 2x4 = 2
x2 ≥0, x4 ≤0, x1, x3 unrestricted

1164
Chapter 16
DISCRETE OPTIMIZATION
is
minimize:
−u1 + 3u2 + 2u3
subject to:
u1 + 2u2 + u3 = 5
3u1 + u2 + u3 ≥−7
−u1 −4u2 −3u3 = 0
u1 −u2 + 2u3 ≤0
u1 ≥0, u2 ≤0, u3 unrestricted.
2. The LP of §16.1.2, Example 1 has the dual
minimize:
5u1 + 12u2 + 2u3
subject to:
3u1 −9u2 −6u3 ≥3
4u2 + 2u3 ≥4
−u1 + 3u2 + 4u3 ≥4
u1, u2, u3 ≥0 .
The optimal solution to the primal LP (see §16.1.3, Example 2) is x∗= ( 11
3 , 0, 6)T with
optimal objective value z∗= 35. The associated tableau has u = (0, 1, 0, 6, 0, 5
2). The
optimal dual variables for (4) are recovered from the reduced costs of the slack variables
x4, x5, and x6, so that u∗= (6, 0, 5
2).
As guaranteed by Fact 4, the optimal dual
objective value 5u∗
1 + 12u∗
2 + 2u∗
3 = 30 + 5 = 35 = z∗. The complementary slackness
conditions in Fact 7 hold here: the second primal constraint holds with strict inequality
(x∗
5 = 27 > 0), so the second dual variable u∗
2 = 0; also, the second dual constraint holds
with strict inequality (u∗
5 = 1 > 0), so the second primal variable x∗
2 = 0.
3. Using the transformations in Fact 5 of §16.1.1, the LP problem
minimize:
2x1 + 3x2 + 4x3
subject to:
2x1 −x2 + 3x3 ≥4
x1 + 2x2 + x3 ≥3
x1, x2, x3 ≥0
can be written in form (2), with the corresponding tableau (a) below. Since u ≥0 the
current solution x4 = −4, x5 = −3 is dual feasible but not primal feasible. Algorithm 4
can then be applied, giving t∗= 1 and j∗= 1. The variable x4 leaves the basis and
the variable x1 enters, giving tableau (b) and the new basic but not feasible solution
x = (2, 0, 0, 0, −1)T with z = 4.
One additional dual simplex pivot achieves primal
feasibility and produces the optimal solution x∗= ( 11
5 , 2
5, 0, 0, 0)T with z∗= 28
5 .
tableau (a)
2
3
4
0
0
0
−2
1
−3
1
0
−4
−1
−2
−1
0
1
−3
tableau (b)
0
4
1
1
0
−4
1
−1
2
3
2
−1
2
0
2
0
−5
2
1
2
−1
2
1
−1
16.1.6
SENSITIVITY ANALYSIS
Since the data to an LP are often estimates or can vary over time, the analysis of many
problems requires studying the behavior of the optimal LP solution to changes in the
input data. This form of sensitivity analysis typically uses the solution of the original
LP as a starting point for solving the altered LP.

Section 16.1
LINEAR PROGRAMMING
1165
Deﬁnitions:
The original tableau for the LP problem (2) is
−c
0
0
A
I
b .
The ﬁnal tableau for the optimal basic solution x∗(possibly after a permutation of the
columns 1, . . . , m + n) is
u
z
D
f
= 0
cBB−1N −cN
cBB−1b
I
B−1N
B−1b
.
Row 0 of a tableau refers to the row u of associated dual variables.
A tableau is suboptimal if some entries of row 0 are negative. A tableau is infeasible
if some entries of column f are negative.
Let aj be the column of [A I ] associated with variable xj and let dj be the column of D
associated with variable xj.
Facts:
1. The formulas in Table 1 show how to construct an updated tableau T ′ from the ﬁnal
tableau T of an LP problem:
• if T ′ is suboptimal, reoptimize using the simplex method starting with T ′;
• if T ′ is infeasible, reoptimize using the dual simplex method starting with T ′;
• otherwise, T ′ corresponds to an optimal BFS for the altered problem.
Table 1: Formulas for constructing the updated tableau T ′.
change in
possible changes
tableau updates
LP data
in tableau
change in cs, s nonbasic:
only entry s of
us := us −∆
cs := cs + ∆
row 0 can change
update cB
change in cs, s basic:
row 0 and z
uj := cBB−1aj−cj, j nonbasic
cs := cs + ∆
can change
z := cBB−1b
f := f + ∆(dn+r)
change in br:
decision variables
update b
br := br + ∆
and z can change
z := cBB−1b
change nonbasic column s:
update as and cs
as := eas
tableau column s and
us := cBB−1as −cs
cs := ecs
us can change
ds := B−1as
add a new column
new tableau column ℓ
uℓ:= cBB−1aℓ−cℓ
aℓwith cost cℓ
and new uℓ
dℓ:= B−1aℓ
2. Ranging:
Table 2 shows how to calculate the (maximal) ranges over which the
current basis B remains optimal. In the “range” column of Table 2, b and cB refer to
entries of T , rather than T ′.
3. When bi is changed within the allowable range (Table 2), the change in the objective
value is −∆times the reduced cost of the slack variable associated with row i.
4. To add a new constraint aℓx ≤bℓto the original LP do the following:

1166
Chapter 16
DISCRETE OPTIMIZATION
• add a new (identity) column to the tableau corresponding to the slack variable of
the new constraint;
• add a new row ℓto the tableau corresponding to the new constraint;
• for each basic j with dℓj ̸= 0, multiply row i = ϕ−1(j) by −dℓj and add to row ℓ;
• if the updated fℓ< 0 use the dual simplex method to reoptimize.
5. For changes in more than one component of c, or in more than one right-hand side b,
use the “100% rule”:
• objective function changes: If all changes occur in variables j with uj > 0, the
current solution remains optimal as long as each cj is within its allowable range
(Table 2). Otherwise, let ∆cj be the change to cj. If ∆cj ≥0 set rj := ∆cj
∆U ,
else set rj := −∆cj
∆L , where ∆U, ∆L are computed from Table 2. If P rj ≤1,
the current solution remains optimal (if not, the rule tells nothing).
• right-hand side changes: If all changes are in constraints not active at x∗, the
current basis remains optimal as long as each bi is within its allowable range
(Table 2). Otherwise, let ∆bi be the change to bi. If ∆bi ≥0 set ri := ∆bi
∆U , else
set ri := −∆bi
∆L , where ∆U, ∆L are computed from Table 2. If P ri ≤1, the
current solution remains optimal (if not, the rule tells nothing).
Table 2: Ranges over which current basis is optimal.
change in LP data
range
change in cs, s nonbasic:
∆≤us
cs := cs + ∆
∆L ≤∆≤∆U, where p = sth row of B−1
change in cs, s basic:
∆L = max{ cj−cBB−1aj
paj
| paj > 0, j nonbasic}
cs := cs + ∆
∆U = min{ cj−cBB−1aj
paj
| paj < 0, j nonbasic}
∆L ≤∆≤∆U, where q = rth column of B−1
change in br:
∆L = max{ −(B−1b)i
qi
| qi > 0}
br := br + ∆
∆U = min{ −(B−1b)i
qi
| qi < 0}
Examples:
1. The LP problem
maximize:
3x1 + 4x2 + 4x3
subject to:
3x1 −x3 ≤5
−9x1 + 4x2 + 3x3 ≤12
−6x1 + 2x2 + 4x3 ≤2
x1, x2, x3 ≥0
has the ﬁnal tableau T
0
1
0
6
0
5
2
35
1
1
3
0
2
3
0
1
6
11
3
0
4
0
3
1
0
27
0
1
1
1
0
1
2
6

Section 16.1
LINEAR PROGRAMMING
1167
corresponding to the optimal BFS x∗= ( 11
3 , 0, 6, 0, 27, 0)T with z∗= 35. The associated
basis matrix B contains columns 1, 5, 3 and the inverse basis matrix is B−1, where
B =



3
0
−1
−9
1
3
−6
0
4


,
B−1 =



2
3
0
1
6
3
1
0
1
0
1
2


.
If the nonbasic objective coeﬃcient c2 is changed to 4 + ∆, the current BFS remains
optimal for ∆≤u2 = 1, that is, for c2 ≤5.
If the basic objective coeﬃcient c1 is
changed to 3 + ∆, then p = ( 2
3, 0, 1
6) and ∆L = max{ −1
1/3, −6
2/3, −5/2
1/6 } = −3. This gives
−3 ≤∆, so the current BFS remains optimal over the range c1 ≥0. If, however, c3
is changed to the value 2, meaning ∆= −2, the current basis with ΣB = {1, 5, 3} will
no longer be optimal. Using Table 2 the vector cB is updated to cB = (3, 0, 2) and the
nonbasic uj are computed as u2 = −1, u4 = 4, u6 = 3
2. The updated u = (0, −1, 0, 4, 0, 3
2)
and z = 23 are inserted in tableau T . Since u2 < 0 a simplex pivot with j∗= 2 and
t∗= 3 is performed, leading to the new optimal solution x∗= ( 5
3, 6, 0, 0, 3, 0)T with
z∗= 29.
2. Suppose that the right-hand side b1 in the original LP of Example 1 is changed to
b1 = 3, corresponding to the change ∆= −2. From Table 1 f is updated to ( 11
3 , 27, 6)T −
2( 2
3, 3, 1)T = ( 7
3, 21, 4)T, giving the optimal BFS x∗= ( 7
3, 0, 4, 0, 21, 0)T.
Since b =
(3, 12, 2)T the objective value found from Table 1 is z = 23. Notice that the change in
objective value is ∆z = 23 −35 = −12, which is the same as −∆times the reduced cost
−u4 of x4: namely, −12 = 2 · (−6). To determine the range of variation of b1 so that the
basis deﬁned by ΣB = {1, 5, 3} remains unchanged, Table 2 is used. Here q = ( 2
3, 3, 1)T
and ∆L = max {−11/3
2/3 , −27
3 , −6
1} = −11
2 . Thus −11
2 ≤∆so that the current basis is
optimal for b1 ≥−1
2.
3. If the new constraint 3x1 + 2x2 −x3 ≤4 is added to the LP in Example 1, the (pre-
vious) optimal solution x∗= ( 11
3 , 0, 6, 0, 27, 0)T is no longer feasible. Using Fact 4, a new
row and column are added to the tableau T , giving tableau (a) below. By adding (−3)
times row 1 and +1 times row 3 to the last row, a new tableau (b) is produced corre-
sponding to the basic set ΣB = {1, 5, 3, 7}. Since b4 < 0 the dual simplex algorithm is
then used with t∗= 4 and j∗= 4, producing a new tableau that is primal feasible, with
the new optimal BFS (3, 0, 5, 1, 24, 0, 0)T and objective value 29.
tableau (a)
0
1
0
6
0
5
2
0
35
1
1
3
0
2
3
0
1
6
0
11
3
0
4
0
3
1
0
0
27
0
1
1
1
0
1
2
0
6
3
2
−1
0
0
0
1
4
tableau (b)
0
1
0
6
0
5
2
0
35
1
1
3
0
2
3
0
1
6
0
11
3
0
4
0
3
1
0
0
27
0
1
1
1
0
1
2
0
6
0
2
0
−1
0
0
1
−1
4. One example of the practical use of sensitivity analysis occurred in the airline indus-
try. When the price of aviation fuel was relatively high, and varied by airport location,
a linear programming model was successfully used to determine an optimal strategy for
refueling aircraft. The key idea is that it might be more economical to take on extra fuel
at an enroute stop if the fuel cost savings for the remainder of the ﬂight are greater than
the extra fuel burned because of the excess weight of additional fuel. A linear program-
ming model of this situation ended up saving millions of dollars annually. An important
feature was providing pilots with ranges of fuel prices for each airport location, with
associated optimal policies for taking on extra fuel based on the cost range.

1168
Chapter 16
DISCRETE OPTIMIZATION
5. Another example of the beneﬁcial use of sensitivity analysis occurred in a 1997 study
to assess the eﬀectiveness of mandatory minimum-length sentences for reducing drug
use. One ﬁnding of the study was that the longer sentences become more eﬀective than
conventional enforcement only when it costs more than $30,000 to arrest a drug dealer.
Thus, rather than producing a single optimal policy, this study identiﬁed conditions
(parameter ranges) under which each alternative policy is to be preferred.
16.1.7
GOAL PROGRAMMING
Goal programming refers to a multicriteria decision-making problem in which a given
LP problem can have multiple objectives or goals. This technique is useful when it is
impossible to satisfy all goals simultaneously. For example, a model for optimizing the
operation of an oil reﬁnery might seek not only to minimize production cost, but also
to reduce the amount of imported crude oil and the amount of oil having a high sulfur
content. In another instance, the routing of hazardous waste might consider minimizing
not only the total distance traveled but also the number of residents living within ten
miles of the selected route.
Deﬁnitions:
A goal programming (GP) problem has linear constraints that can be written
Ax ≤b
Hx + ex −x = h
x ≥0, ex ≥0, x ≥0
and objective functions
G1 :
minimize z1 = c1ex1 + d1x1
G2 :
minimize z2 = c2ex2 + d2x2
...
Gℓ:
minimize zℓ= cℓexℓ+ dℓxℓ
where A is an m × n matrix and H is an ℓ× n matrix.
The value hk is the target value of the kth goal. Goal k is satisﬁed if (Hx)k = hk
holds for a given vector x of decision variables.
The variables ex are the underachievement variables while the variables x are the
overachievement variables.
Facts:
1. In a GP problem, the aim is to ﬁnd decision variables that approximately satisfy the
given goals, which is achieved by jointly minimizing the magnitudes of the underachieve-
ment and overachievement variables.
2. Assuming ck > 0 and dk > 0, then goal k is satisﬁed by making zk = 0.
3. If all ct and dt are positive then for each k = 1, . . . , ℓat most one of exk, xk will be
positive in an optimal solution.
4. One important case of a GP problem has ck = dk = 1 for k = 1, . . . , ℓ, making the
objective to (approximately) satisfy all constraints Hx = h.

Section 16.1
LINEAR PROGRAMMING
1169
5. When the relative importance of G1, . . . , Gℓis known precisely, an ordinary LP can
be used with the objective function being a weighted sum of z1, . . . , zℓ.
6. Preemptive goal programming: Here the goals are prioritized G1 ≫G2 ≫· · · ≫Gℓ,
meaning that goal G1 is the most important and goal Gℓis the least important. Solutions
are sought that satisfy the most important goal. Among all such solutions, those are
retained that best satisfy the second highest goal, and so forth.
7. Goal programming simplex method:
The simplex method (§16.1.3, Algorithm 2)
can be extended to preemptive GP (minimization) problems, with the following modiﬁ-
cations:
• ℓ“objective rows” are maintained in the tableau instead of just one.
• Let i∗be the highest-priority index with zi∗> 0 for which there exists a nonba-
sic j∗with ui∗
j∗> 0 and with ui
j∗≥0 for any (higher-priority) objective row
i < i∗. If there is no such i∗, stop; else pick j∗corresponding to the most
positive ui∗
j∗.
• All ℓobjective rows are updated when a pivot is performed.
• At completion, if the solution fails to satisfy all goals, then every nonbasic vari-
able that would decrease the objective value zi if it entered the basis, would
increase zi′ for some higher-priority goal Gi′, i′ < i.
8. An optimization package (LiPS) that implements goal programming can be found at
• http://sourceforge.net/projects/lipside/
16.1.8
INTEGER PROGRAMMING
Integer programming problems are LPs in which some of the variables are constrained to
be integers. Such problems more accurately model a wide range of application areas, in-
cluding capital budgeting, facility location, manufacturing, scheduling, logical inference,
physics, engineering design, environmental economics, and VLSI circuit design. However,
integer programming problems are much more diﬃcult to solve than LPs.
Deﬁnitions:
Let Zn [Zn
+] denote the set of all n-vectors with all components integers [nonnegative
integers], and let Rn [Rn
+] denote the set of all n-vectors with all components real numbers
[nonnegative real numbers].
A pure integer programming (IP) problem is an optimization problem of the form
maximize:
zIP = cx
subject to:
Ax ≤b
x ∈Zn
+
(5)
where A is an m × n matrix, b is an m-column vector, and c is an n-row vector.
A 0-1 IP problem is an IP with each xj ∈{0, 1}.
A mixed integer programming (MIP) problem is of the form
maximize:
zMIP = cx + hy
subject to:
Ax + Gy ≤b
x ∈Zn
+, y ∈Rp
+

1170
Chapter 16
DISCRETE OPTIMIZATION
where A is an m × n matrix, G is an m × p matrix, b is an m-column vector, c is an
n-row vector, and h is a p-row vector.
For IP problem (5), the feasible region is S ≡{x ∈Zn
+ | Ax ≤b}.
A polyhedron is a set of points in Rn satisfying a ﬁnite set of linear inequalities.
If X is a ﬁnite set of points in Rn, the convex hull of X is
conv (X) ≡{P λixi | xi ∈X, P λi = 1, λi ≥0}.
The LP relaxation of (5) is the linear programming problem
maximize:
zLP = cx
subject to:
Ax ≤b
x ≥0.
(6)
More generally, a relaxation of (5) is any problem max{cx | x ∈T }, where S ⊂T .
The problem max{cx | ¯Ax ≤¯b, x ∈Zn
+} is a formulation for (5) if it contains exactly
the same set of feasible integral points as (5). If the feasible region of the LP relaxation
of one formulation is strictly contained in another, the ﬁrst is a tighter formulation.
Suppose ex is a feasible solution to (6) but not to (5). A cutting plane is any inequality
πx ≤π0 satisﬁed by all points in conv (S) but not by ex.
A family S of subsets of S is a separation of S if S
Sk∈S Sk = S; a separation is usually
a partition of the set S.
A lower bound z for zIP is an underestimate of zIP .
Facts:
1. zIP ≤zLP. More generally, any relaxation of (5) has an optimal objective value at
least as large as zIP .
2. If x′ is feasible to (5) then z′ = cx′ satisﬁes z′ ≤zIP .
3. The feasible region of an LP problem is a polyhedron, and every polyhedron is the
feasible region of some LP problem.
4. The set conv (S) is a polyhedron, so there is an LP problem max{cx | eAx ≤eb, x ≥0}
with the feasible region conv (S).
5. An optimal solution to the LP in Fact 4 is an optimal solution to (5). However,
ﬁnding all necessary constraints, called facets, of this LP is extremely diﬃcult.
6. IP is an NP-hard optimization problem (§17.5.2). Consequently, such problems are
harder to solve in practice than LPs. The inherent complexity of solving IPs stems from
the nonconvexity of their feasible region, which makes it diﬃcult to verify the optimality
of a proposed optimal solution in an eﬃcient manner.
7. Formulation of an IP is critical: achieving problem tightness is more important than
reducing the number of constraints or variables appearing in the formulation.
8. An excellent introduction to building integer linear programming models is found in
Williams [Wi13].
9. Solution techniques for (5) usually involve some preliminary operations that improve
the formulation, called preprocessing, followed by an iterative use of heuristics to quickly
ﬁnd feasible solutions.

Section 16.1
LINEAR PROGRAMMING
1171
10. Popular solution techniques for solving (5) include cutting plane methods (Fact 11),
branch-and-bound techniques (Fact 13), and (hybrid) branch-and-cut methods.
11. Cutting plane method:
This approach (Algorithm 5) proceeds by ﬁrst ﬁnding an
optimal solution x to a relaxation R of the original problem (5). If x is not optimal, a
cutting plane is added to the constraints of the current relaxation and the new LP is
then solved. This process is repeated until an optimal solution is found.
Algorithm 5:
Cutting plane algorithm for (5).
input: IP in form (5)
output: an optimal solution x∗with objective value z∗
let R be the LP relaxation: max{cx | Ax ≤b, x ≥0}
{Main loop}
optimally solve problem R, obtaining x
if x ∈Zn
+ then stop with x∗:= x and z∗:= cx
else
ﬁnd a cutting plane πx ≤π0 with πx > π0 and πx ≤π0 for all feasible
solutions of (5)
modify R by adding the constraint πx ≤π0
{Continue with next iteration of main loop}
12. General methods for ﬁnding cutting planes for IP or MIP problems are relatively
slow. Cutting plane algorithms using facets for speciﬁc classes of IP problems are better,
since facets make the “deepest” cuts.
13. Branch-and-bound method: This approach (Algorithm 6) decomposes the original
problem P into subproblems or nodes by breaking S into subsets. Each subproblem Pj
is implicitly investigated (and possibly discarded) until an optimal one is found.
In
this algorithm z∗
j is the optimal value of problem Pj, zj is the optimal value of the
relaxation Rj of Pj, and zj is the best known lower bound for z∗
j .
Algorithm 6:
Branch-and-bound algorithm for (5).
input: IP in form (5)
output: an optimal solution x0 with objective value z0
let P be the problem: max{cx | x ∈S}
P0 := P; S0 := S; z0 := −∞; z0 := +∞
put P0 on the list of live nodes
{Branching}
if no live node exists then go to {termination}
else select a live node Pj
{Bounding}
solve a relaxation Rj of Pj
if Rj is infeasible then discard Pj and go to {branching}
if zj = +∞then go to {separation}
{zj is ﬁnite}
if zj ≤z0 then discard node Pj and go to {branching}

1172
Chapter 16
DISCRETE OPTIMIZATION
if zj = zj then update z0 := max{z0, zj} and discard any node Pi for
which zi ≤z0
{Separation}
choose a separation S∗
j of Sj forming new live nodes and go to {branching}
{Termination}
if z0 = −∞then problem (5) is infeasible
if z0 is ﬁnite then z0 is the optimal objective value and the associated x0 is
an optimal solution
14. In Algorithm 6 the optimal value zj of relaxation Rj is an upper bound for z∗
j . Also
z0 is the objective value for the best known feasible solution to (5).
15. LP relaxations are often used in the bounding portion of Algorithm 6.
16. There are specializations of Algorithm 6 for 0-1 IP problems and for MIP problems.
17. Branch-and-bound tends to be a computationally expensive solution method. Usu-
ally it is applied only when other methods appear to be stalling.
18. In the survey [Fo15] of linear programming software, many of the packages listed
will handle IP problems as well.
When available, these extensions to binary and/or
integer-valued variables are indicated by the survey.
19. Several software packages that solve IP and MIP problems can be found at
• http://www.aimms.com
• http://optimization-suite.coin-or.org
• http://www.cplex.com
• http://www.gurobi.com
• http://www.lindo.com
• www.mathworks.com/products/optimization
Examples:
1. The next ﬁgure shows the convex hull of feasible solutions to the IP
maximize:
−x1 + 1
2x2
subject to:
x2 ≤4
−x1 −x2 ≤−5
2
8x1 + x2 ≤24
−3x1 + 4x2 ≤10
x1, x2 ≥0
x1, x2 integers.
Here S = {(1, 2), (1, 3), (2, 1), (2, 2), (2, 3), (2, 4), (3, 0)}.
The optimal solution occurs
at (1, 3), with zIP = 1
2. The feasible region of the LP relaxation is also shown, with
the optimal LP value zLP =
5
4 attained at (0, 5
2); a cutting plane is depicted by the
dashed line.

Section 16.1
LINEAR PROGRAMMING
1173
For this problem, conv (S) is deﬁned by the following constraints:
−x1 −x2 ≤−3
−x1 + x2 ≤2
−x1+ ≤−1
4x1 + x2 ≤12
x2 ≤4
x1, x2 ≥0.
All of these constraints are facets except for x2 ≤4 and the nonnegativity constraints,
which are redundant.
2. The following IP has the feasible region S = {(0, 0), (1, 1), (2, 2), (3, 3), (4, 4)} and the
optimal solution occurs at (4, 4) with zIP = 8:
maximize:
x1 + x2
subject to:
2x1 −2x2 ≤1
−7x1 + 8x2 ≤4
x1, x2 ≥0
x1, x2 integers.
The LP relaxation has a feasible region deﬁned by vertices (0, 0), ( 1
2, 0), (0, 1
2), (8, 15
2 ),
so its optimal solution occurs at (8, 15
2 ) with zLP = 31
2 . Consequently, the LP solution
is a poor approximation to the optimal IP solution. Moreover, simply rounding the LP
solution gives either (8, 7) or (8, 8), both of which are infeasible to the given IP problem.
3. The following IP can be solved using Algorithm 6:
maximize:
3x1 + 3x2 −8x3
subject to:
−3x1 + 6x2 + 7x3 ≤8
6x1 −3x2 + 7x3 ≤8
x1, x2, x3 ≥0
x1, x2, x3 integers.
The initial problem P0 has an LP relaxation R0 that is obtained by removing the integer
restrictions; solving this LP gives x = (2.667, 2.667, 0) with z = 16. A separation is
achieved by creating the two subproblems P1 and P2; the constraint x1 ≤2 is appended
to P0, creating P1, while the constraint x1 ≥3 is appended to P0, creating P2. Now
the live nodes are P1 and P2. Solving the LP relaxation R1 gives x = (2, 2.333, 0) with
z = 13. New subproblems P3 and P4 are obtained from P1 by appending the constraints
x2 ≤2 and x2 ≥3, respectively. Now the live nodes are subproblems P2, P3, P4. The

1174
Chapter 16
DISCRETE OPTIMIZATION
LP relaxation R2 of P2 is infeasible, as is the LP relaxation R4 of P4. Solving the LP
relaxation R3 gives the feasible integer solution x = (2, 2, 0) with z = 12. Since there are
no more live nodes, this represents the optimal solution to the stated problem.
4. Fixed-charge problem:
Find optimal levels of n activities to satisfy m constraints
while minimizing total cost. Each activity j has per unit cost cj. In addition, there is a
startup cost dj for certain undertaken activities j.
This problem can be modeled as a MIP problem, with a real variable xj for the level of
each activity j. If activity j has a startup cost, introduce the additional 0-1 variable yj,
equal to 1 when xj > 0 and 0 otherwise. For example, this condition can be enforced by
imposing the constraints Mjyj ≥xj, yj ∈{0, 1}, where Mj is a known upper bound on
the value of xj. The objective is then to minimize z = cx + dy.
5. Queens problem:
On an n × n chessboard, the task is to place as many nontaking
queens as possible.
This problem can be formulated as a 0-1 IP problem, having binary variables xij. Here
xij = 1 if and only if a queen is placed in row i and column j of the chessboard. The
objective function is to maximize z = P
i
P
j xij and there is a constraint for each row,
column, and diagonal of the chessboard. Such a constraint has the form P
(i,j)∈S xij ≤1,
where S is the set of entries in the row, column, or diagonal. For example, one optimal
solution of this IP for the 7 × 7 chessboard is the assignment x16 = x24 = x37 = x41 =
x53 = x65 = x72 = 1, with all other xij = 0.
16.2
LOCATION THEORY
Location theory is concerned with locating a ﬁxed number of facilities at points in some
space. The facilities provide a service (or product) to the customers whose locations and
levels of demand (for the service) are known. The object is to ﬁnd locations for the facil-
ities to optimize some speciﬁed criterion, e.g., the cost of providing the service. Interest
in location theory has grown very rapidly because of its variety of applications to such
ﬁelds as operations research, city planning, geography, economics, electrical engineering,
and computer science.
16.2.1
p-MEDIAN AND p-CENTER PROBLEMS
Deﬁnitions:
A metric space is a space S consisting of a set of points with a real-valued function
d(x, y) deﬁned on all pairs of points x, y ∈S with the following properties (§6.1.4):
• d(x, y) = d(y, x) ≥0 for all x, y ∈S;
• d(x, y) = 0 if and only if x = y;
• d(x, z) ≤d(x, y) + d(y, z) for all x, y, z ∈S.
The value d(x, y) is called the distance between points x, y ∈S.
There are p facilities that are to be located at some set Xp = {x1, x2, . . . , xp} of p points
in the (metric) space S. The elements of Xp are the facility locations.

Section 16.2
LOCATION THEORY
1175
The facilities are to provide a service to the customers whose positions are given by a set
V = {v1, v2, . . . , vn} of n points in S. The points in V are the demand points and the
level of demand at vi ∈V is given by w(vi) ≥0.
For x ∈S and Xp ⊆S, let d(x, Xp) be the minimum distance from x to a point of Xp:
d(x, Xp) = min
xi∈Xp{d(x, xi)}.
Suppose Xp is a candidate set of points in S for locating the p facilities. The following
two objective functions are deﬁned on Xp ⊆S:
• F(Xp) =
nP
i=1
w(vi)d(vi, Xp);
• H(Xp) = max
1≤i≤n{w(vi)d(vi, Xp)}.
The set Xm
p ⊆S is a p-median if F(Xm
p ) ≤F(Xp) for all possible Xp ⊆S.
The set Xc
p ⊆S is a p-center if H(Xc
p) ≤H(Xp) for all possible Xp ⊆S.
Facts:
1. It is customary to assume that the demand at vi is satisﬁed by its closest facility.
Then w(vi)d(vi, Xp) indicates the total (transportation) cost associated with having the
demand at vi ∈S satisﬁed by its closest facility in the candidate set Xp.
2. F(Xp) represents the total transportation cost of satisfying the demands if the facil-
ities are located at Xp.
3. H(Xp) represents the cost (or unfairness) associated with a farthest demand point
not being in close proximity to any facility.
4. A p-median formulation is designed for locating p facilities in a manner that minimizes
the average cost of serving the customers.
5. A p-center formulation is designed for locating p emergency facilities (e.g., police,
ﬁre, and ambulance services), in which the maximum time to respond to an emergency
is to be made as small as possible.
6. The p-median and p-center problems are only interesting if p < n; otherwise, it is
possible to locate at least one facility at each demand point, thereby reducing F(Xp) or
H(Xp) to 0.
Examples:
1. Suppose that a single warehouse is to be located in a way to service n retail outlets
at minimum cost. Here w(vi) is the number of shipments made per week to the outlet at
location vi. This can be modeled as a 1-median problem, since the objective is to locate
the single warehouse to minimize the total distance traveled by delivery vehicles.
2. A new police station is to be located within a portion of a city to serve residents of
that area. Neighborhoods in that area can be taken as the demand points, and locating
the police station can be formulated as a 1-center problem. Here, the maximum distance
from the source of an emergency is critical so the police station should be located to
minimize the maximum distance from a neighborhood. The weights at each demand
point might be taken to be equal, or in some situations diﬀering weights could signify
conversion factors that translate distance into some other measure such as the value of
residents’ time.
3. Statistics:
Suppose that n given data values x1, x2, . . . , xn are viewed as points
placed along the real line. If the distance between points xi and xj is their absolute

1176
Chapter 16
DISCRETE OPTIMIZATION
diﬀerence |xi −xj|, then a 1-median of this set of points (unweighted customer locations)
is a point (facility) ˆx that minimizes Pn
i=1 |xi −ˆx|. In fact, ˆx corresponds to a median
of the n data values. If the distance between points xi and xj is their squared diﬀerence
(xi −xj)2, then a 1-median is a point ¯x minimizing Pn
i=1(xi −¯x)2, which is precisely
the mean of the n data values. Alternatively, for either distance measure the 1-center of
this set of points turns out to correspond to the midrange of the dataset: namely, the
1-center is the point located halfway between the largest and the smallest data values.
16.2.2
p-MEDIANS AND p-CENTERS ON NETWORKS
Deﬁnitions:
A network is a weighted graph G = (V, E) with vertex set V = {v1, v2, . . . , vn} and
edge set E, where m = |E|; see §8.1.1.
The weight of vertex v ∈V represents the demand at v and is denoted by w(v) ≥0.
The length of edge e ∈E represents the cost of travel (or distance) across e and is
denoted by ℓ(e) > 0. Each edge is assumed to be a line segment joining its end vertices.
A point on a network G is any point along any edge of G. The precise location of the
point x on edge e = (u, v) is indicated by the distance of x from u or v.
If x and y are any two points on G, the distance d(x, y) is the length of a shortest path
between x and y in G, where the length of a path is the sum of the lengths of the edges
(or partial edges) in the path.
Facts:
1. Network G with the above deﬁnition of distance constitutes a metric space (§16.2.1).
2. p-median theorem: Given a positive integer p and a network G = (V, E), there exists
a set of p vertices Vp ⊆V such that F(Vp) ≤F(Xp) for all possible sets Xp of p points
on G. That is, a p-median can always be found that consists entirely of vertices.
3. A p-median of a network G can be found by a ﬁnite search through all possible
 n
p

choices of p vertices out of n. This is still a formidable task, but if p is a small number
(say p < 5) it is certainly manageable.
4. The p-median theorem also holds if the cost of satisfying the demand at vi ∈V
is fi(d(vi, Xp)), instead of w(vi) · d(vi, Xp), provided that fi : R+ →R+ is a con-
cave nondecreasing function for all i = 1, 2, . . . , n. (R+ denotes the set of nonnegative
real numbers.) In this case, the objective function for the p-median problem becomes
F(Xp) = Pn
i=1 fi(d(vi, Xp)).
5. Each point x of a p-center Xc
p of network G = (V, E) is a point on some edge e such
that for some pair of distinct vertices u and v ∈V , w(u)d(u, x) = w(v)d(v, x); i.e., the
point x is the “center” of a shortest path from u to v in G that passes through edge e.
6. There are at most n2 predetermined choices of points on each edge of G that could
be potential points in Xc
p; thus there are n2m predetermined choices for points in Xc
p.
7. A p-center Xc
p of network G can be found by examining all possible
 n2m
p

choices of
p points out of n2m. Even for small values of p, this is a formidable task.

Section 16.2
LOCATION THEORY
1177
Examples:
1. In the following network the levels of demand are given at the vertices and the lengths
of the edges are shown on the edges. The 1-median of this network is at the vertex labeled
x1. The total transportation cost is F({x1}) = 35.
2
1
3
4
2
2
2
2
2
2
1
3
3
3
3
3
1
4
x1
2
2. A tree network T is shown in the following ﬁgure, with the vertex demands and
edge lengths displayed. A 2-median of T is the set of vertices X2 = {x1, x2}, with total
transportation cost F(X2) = 25. If x1 is kept ﬁxed and t is an arbitrary point along edge
e, then {x1, t} also constitutes a 2-median of T . This is consistent with Fact 2, which
only states that there is a p-median that is a subset of V , not that every p-median occurs
in this way.
4
2
1
1
1
1
1
e
x2
4
1
2
1
2
2
2
x1
3
3. A 1-center XC
1 is found for the network shown in the left portion of the following
ﬁgure. For illustration, suppose that the 1-center is along the edge (u1, u2), thereby
limiting the search to candidate points on this edge. Let X(x) be an arbitrary point
along edge (u1, u2), parametrized by the scalar x = d(u1, X(x)). Note that x ∈[0, 3]
with X(0) = u1 and X(3) = u2.
u1
u2
u3
u4
3.5
1
3
1
1.5
2
2
2
X1
c
3
8/7
1
2
3
4
5
6
7
0
1
2
3
8/7
x
w(u3) d(u3,X(x ))
w(u4) d(u4,X (x))
w(u1) d(u1,X (x))
w(u2) d(u2,X(x ))

1178
Chapter 16
DISCRETE OPTIMIZATION
The right portion of the ﬁgure shows plots of w(ui)d(ui, X(x)) as a function of x for
i = 1, 2, . . ., 4. The plot of D(x) ≡max1≤i≤4 w(ui)d(ui, X(x)) is highlighted in bold
and D(x) assumes its minimum value when x = 8
7. The 1-center of the network in part
(a) is then located along edge (u1, u2) a distance of 8
7 from u1. Note that for this value
of x, w(u4)d(u4, X(x)) = w(u3)d(u3, X(x)), consistent with Fact 5. In general, Xc
1 of a
network is not necessarily a unique point; however, here Xc
1 is unique and H(Xc
1) = 30
7 .
4. Transportation planners are trying to decide where to locate a single school bus
stop along a major highway. Situated along a one-mile stretch of the highway are eight
communities. The table below gives the number of school-age students in each community
who ride the bus on a daily basis. The distance of each community from the westernmost
edge of the one-mile stretch of highway is also shown.
community
1
2
3
4
5
6
7
8
# students
9
4
8
11
5
3
5
11
distance (mi)
0.0
0.2
0.3
0.4
0.6
0.7
0.9
1.0
The data of this problem can be represented by an undirected path (§8.1.3) with each
vertex v corresponding to a community and w(v) being the number of students from that
community riding the bus. Edges join adjacent communities and have a length given
by the diﬀerence in distance entries from the table. To minimize the total (weighted)
distance traveled by the students, a 1-median is sought. By Fact 2, only vertex locations
need to be considered. For example, situating the bus stop at vertex 3 incurs a cost of
F({3}) = 9(0.3) + 4(0.1) + 8(0) + 11(0.1)+ 5(0.3) + 3(0.4) + 5(0.6) + 11(0.7) = 17.6. The
minimum cost is incurred for vertex 4, with F({4}) = 16.2, so that the bus stop should
be located at community 4.
16.2.3
ALGORITHMS FOR LOCATION ON NETWORKS
Algorithms for ﬁnding p-medians and p-centers of a network G can be devised that are
feasible for small values of p. For general p, however, there are no eﬃcient methods known
for arbitrary networks G. Specialized (and eﬃcient) algorithms are available when G is
a tree.
Deﬁnitions:
Let T be a tree (§9.1.1) with vertex weights w(v).
If T ′ is a subnetwork of T , deﬁne the total weight of T ′ by W(T ′) =
P
v∈V (T ′)
w(v).
For v ∈V (T ), let Tv1, Tv2, . . . , Tvd(v) be the components of T −v, where d(v) is the
degree (§8.1.1) of vertex v. Deﬁne Mv =
max
1≤i≤d(v){W(Tvi)}.
A leaf vertex of T is a vertex of degree 1.
Facts:
1. For a network G = (V, E) with n vertices and m edges, the p-median and p-center
problems can be solved in O(np+1) and O(mpnp log2 n) time, respectively [Ta88].
2. Bhattacharya and Shi [BhSh14] provide an improved algorithm for solving the p-
center problem with worst-case complexity O(mpnp/22log∗n log n) when p ≥3, where
log∗n denotes the iterated logarithm of n. When p = 2, the algorithm’s running time is
improved to O(m2n log2 n).

Section 16.2
LOCATION THEORY
1179
3. If p is an independent input variable (i.e., p could grow with n), then both the p-
center and p-median problems are NP-hard [KaHa79a], [KaHa79b]. Thus it is highly
unlikely that an algorithm will be found with running time polynomial in n, m, and p.
4. Considerable success has been reported in solving large p-median problems using
heuristic methods that do not necessarily guarantee optimal solutions. A survey of vari-
ous heuristic approaches as well as multiobjective extensions is presented in [DaMa15].
5. Exact and heuristic algorithms for variants of the p-center problem are reviewed in
[CaLaYa15].
6. Algorithms of complexities O(n2p) and O(n log n) for the p-median and p-center
problems on tree networks have been reported by Tamir [Ta96] and by Frederickson and
Johnson [MiFr90, Chapter 7].
7. Vertex u is a 1-median of a tree network T if and only if Mu ≤1
2W(T ).
8. 1-median of a tree: This algorithm (Algorithm 1) is based on Fact 7. The main idea
is to repeatedly remove a leaf vertex, conﬁning the problem to a smaller tree T ′.
Algorithm 1:
1-median of a tree.
input: tree T
output: 1-median ev
T ′ := T ; W0 := P
v∈V (T ′) w(v); W(v) := w(v) for each v ∈V (T ′)
{Main loop}
if T ′ consists of a single vertex ev then stop
else
ev := a leaf vertex of T ′
if W(ev) ≥1
2W0 then stop
else
u := the vertex adjacent to ev in T ′
W(u) := W(u) + W(ev)
T ′ := T ′ −ev
{Continue with next iteration of main loop}
9. Algorithm 1 can be implemented to run in O(n) time.
10. Let T be a tree network with w(v) = c for all v ∈V (T ). Then the 1-center of T is
the unique middle point of a longest path in T .
11. Select any vertex v0 in a tree network T . Let v1 be a farthest vertex from v0, and
let v2 be a farthest vertex from v1. Then the path from v1 to v2 is a longest path in T .
12. 1-center of a tree: This algorithm (Algorithm 2) applies to “unweighted” trees, in
which there are identical weights at each vertex. It is based on Facts 10 and 11.
Algorithm 2:
1-center of an unweighted tree.
input: tree T with w(v) = c for all v ∈V (T )
output: 1-center x
ﬁnd a longest path P in T (using Fact 11)
let u1 and u2 be the end vertices of P
ﬁnd the middle point of this path: i.e., the point x such that d(x, u1) = d(x, u2)

1180
Chapter 16
DISCRETE OPTIMIZATION
13. Algorithm 2 can be implemented to run in O(n) time.
Examples:
1. Suppose that the vertices of the tree T in Example 2 of §16.2.2 are labeled v1, v2, . . . , v8
in order from top to bottom and left to right at each height. Algorithm 1 can be applied
to ﬁnd the 1-median of T . First, the leaf vertex v1 is selected and since W(v1) = 1 is
less than 1
2W0 = 15
2 , its weight is added to vertex v3. The following table shows the
progress of the algorithm, which eventually identiﬁes vertex v4 as the 1-median of T . As
guaranteed by Fact 7, Mv4 = max{6, 1, 3, 4} ≤15
2 .
W(vi)
iteration
ev
1
2
3
4
5
6
7
8
0
1
1
1
1
3
4
2
2
1
v1
–
1
2
1
3
4
2
2
2
v2
–
–
2
2
3
4
2
2
3
v5
–
–
2
5
–
4
2
2
4
v6
–
–
6
5
–
–
2
2
5
v3
–
–
–
11
–
–
2
2
2. Consider the tree T used in Example 1 whose vertices are labeled as in that example.
Algorithm 2 can be applied to ﬁnd the 1-center of T , with all vertex weights being 1.
First, select v1 and ﬁnd a farthest vertex from it, namely v5. A farthest vertex from v5
is then v8, giving a longest path P = [v5, v4, v7, v8] in T . The midpoint x of P, located 1
2
unit from v4 along edge (v4, v5), is then the 1-center of T . If instead the longest path
Q = [v6, v3, v4, v5] in T had been identiﬁed, then the same midpoint x would be found.
16.2.4
CAPACITATED LOCATION PROBLEMS
Deﬁnitions:
Let Xp = {x1, . . . , xp} be a set of locations for p facilities in the metric space S with n
demand points V = {v1, . . . , vn} ⊆S where w(vi) ≥0 is the demand at vi ∈V .
For each vi ∈V and xj ∈Xp, let w(vi, xj) be the portion of the demand at vi satisﬁed
by the facility at xj.
Let W(xj) be the sum of the demands satisﬁed by (or allocated to) the facility at xj. In
a capacitated location problem, upper (and/or lower) bounds are placed on W(xj).
Given the positive integer p and positive constant α, two versions of the capacitated
p-median (CPM) problem in network G = (V, E) are deﬁned:
(a) Find a set of locations Xp such that F(Xp) = P
vi∈V w(vi)d(vi, Xp) is min-
imized subject to W(xj) ≤α for all xj ∈Xp. Here it is assumed that the
demands are satisﬁed by their closest facility, and in the case of ties, a demand,
say at v, may be allocated in an arbitrary way among the closest facilities to v.
(b) Find Xp and {w(vi, xj) | vi ∈V and xj ∈Xp} to minimize
pP
j=1
nP
i=1
w(vi, xj)d(vi, xj)

Section 16.2
LOCATION THEORY
1181
subject to
nP
i=1
w(vi, xj) ≤α,
j = 1, 2, . . . , p
pP
j=1
w(vi, xj) = w(vi),
i = 1, 2, . . . , n.
Facts:
1. Capacitated facility location problems occur in several applied settings, including
• the location of manufacturing plants (with limited output) to serve customers;
• the location of landﬁlls (with limited capacity), which receive solid waste from
the members of a community;
• the location of concentrators in a telecommunication network, where each con-
centrator bundles messages received from individual users and can handle only
a certain amount of total message traﬃc.
2. Version (a) of CPM may not have a solution if α is too small.
3. If α is suﬃciently large in version (a) and CPM has a solution, there may not exist
a solution consisting entirely of vertices of G. See Example 1.
4. Version (b) of the CPM has a solution consisting entirely of vertices of G. This was
shown by J. Levy; see [HaMi79].
Examples:
1. Suppose p = 2 and α = 3 in the following network G. A solution to version (a) of
the CPM problem consists of the points X2 = {x1, x2} with W(x1) = W(x2) = 3 and
F(X2) = 9. It is easy to see that the choice of any two vertices for X2 would violate the
allocation constraint to one facility.
2
4
2
2
2
1
1
x1
3
3
1
x2
2. If p = 2 and α = 3 for the network G given in Example 1, then version (b) of the CPM
problem has a solution containing only vertices of G. Suppose that the top two vertices
in the ﬁgure are v1 and v2 (from left to right) and the bottom two vertices are v3 and v4.
Then X2 = {v1, v2} is an optimal solution, where all the demand w(v3) = 1 is allocated
to v1 whereas the demand w(v4) = 2 is equally split between v1 and v2. (Here, not all
demand from v4 is sent to its closest facility v2.) In this solution, W(v1) = W(v2) = 3
and F(X2) = 7.

1182
Chapter 16
DISCRETE OPTIMIZATION
16.2.5
FACILITIES IN THE PLANE
The p-median and p-center problems can be deﬁned in the plane R2. Several measures
of distance are commonly considered for these location problems in the plane.
Deﬁnitions:
Let x = (x1, x2) and y = (y1, y2) be points of S = R2.
The Euclidean (ℓ2) distance between x and y is d(x, y) = [(x1 −y1)2 + (x2 −y2)2]1/2.
The rectilinear (ℓ1) distance between x and y is d(x, y) = |x1 −y1| + |x2 −y2|.
The (generalized) Weber problem is the p-median problem in R2 with the ℓ2 measure
of distance.
The unweighted Euclidean 1-center problem is the 1-center problem in R2 with
the ℓ2 measure of distance and with w(vi) = c for all vi ∈V .
Facts:
1. No polynomial-time algorithm for the Weber problem, even when p = 1, has been
discovered. The history of this problem, and generalizations, are discussed in [We93].
2. In practice, an iterative method due to Weiszfeld [We37], [WePl09] has been shown
to be highly successful for the Weber problem with p = 1.
3. Further information on the Weiszfeld algorithm and extensions can be found in [Pl11].
Recent algorithmic improvements are discussed in [Dr15] and [RoVa13].
4. The p-median and p-center problems in R2 with either ℓ1 or ℓ2 as the measure of
distance have been proven to be NP-hard if p is an input variable [MeSu84].
5. The unweighted Euclidean 1-center problem is equivalent to ﬁnding the center of the
smallest (radius) circle that encloses all points in V .
6. The following table provides a summary of time complexity results of the best known
algorithms for location problems in the plane [Me83], [MeSu84].
p arbitrary
p = 1
NP-hard if p is an input variable
p-median
under ℓ1 or ℓ2
unknown complexity under ℓ1 or ℓ2
unknown complexity if p is ﬁxed
p-center
NP-hard if p is an input variable
O(n log2 n) under ℓ2
under ℓ1 or ℓ2
O(n) under ℓ2 in the unweighted case
unknown complexity for ﬁxed
O(n) under ℓ1 for both the weighted
p > 1
and unweighted cases
7. A uniﬁed treatment of location problems in Euclidean space studies the ordered
median location problem, which generalizes both the p-median and p-center problems
[BlPuEl14], [NiPu05].
Examples:
1. The ﬂoor plan of a factory contains existing machines A, B, C at the coordinate
locations a = (0, 4), b = (2, 0), c = (5, 2). A new central storeroom, to house materials
needed by the machines, is to be placed at some location x = (x1, x2) on the factory ﬂoor.
Because the aisles of the factory ﬂoor run north-south and east-west, transportation

Section 16.2
LOCATION THEORY
1183
between the storeroom and the machines must take place along these perpendicular
directions.
For example, the distance between the storeroom and the machine C is
|x1 −5| + |x2 −2|. Management wants to locate the storeroom so that the weighted
sum of distances between the new storeroom and each machine is minimized, taking into
account that the demand for material by machine A is twice the demand by machine B,
and demand for material by machine C is three times that by machine B. This is a
weighted 1-median problem in the plane with the ℓ1 measure of distance. The point (3, 2)
is an optimal location for the storeroom. In fact, for any 2 ≤u ≤5 the point (u, 2) is
also an optimal location.
2. Suppose that the ℓ2 distance measure is used instead in Example 1. Then a weighted
1-median is a location x = (x1, x2) that minimizes
2
q
x2
1 + (x2 −4)2 +
q
(x1 −2)2 + x2
2 + 3
p
(x1 −5)2 + (x2 −2)2.
The minimizing point in this case is (5, 2), which is the unique optimal location for the
storeroom. On the other hand, if the demands for material are the same for all three
machines, then the unweighted 1-median occurs at the unique location (2.427, 1.403).
16.2.6
OBNOXIOUS FACILITIES
In the preceding subsections, it has been assumed that the consumers at vi wish to be
as close as possible to a facility. That is, the facilities are desirable. In contrast, this
subsection discusses location problems where the facilities are undesirable or obnoxious.
Deﬁnitions:
For vi ∈V , w(vi)d(vi, Xp) represents the utility (in contrast to cost) associated with
having an obnoxious facility located at distance d(vi, Xp) from vi.
The following two obnoxious facility location problems are deﬁned:
(a) ﬁnd Xp ⊆S to maximize F(Xp) =
nP
i=1
w(vi)d(vi, Xp);
(b) ﬁnd Xp ⊆S to maximize G(Xp) = min
1≤i≤n w(vi)d(vi, Xp).
If space S is a network G = (V, E), for each edge e = (u, v) ∈E, let x(e) = x be the
point on e such that w(u)d(u, x) = w(v)d(v, x) ≡w(e).
Facts:
1. If S is a network, problem (a) may not have a solution that is a subset of vertices
(see Example 2).
2. Suppose S is a network G with w(vi) > 0 for all vi ∈V ; further assume that at most
one point of Xp can be on any particular edge. Renumber the m edges of G so that
w(e1) ≥w(e2) ≥· · · ≥w(em). Then x(e1), x(e2), . . . , x(ep) is a solution to problem (b).
3. Additional results on this subject can be found in [BrCh89].
Examples:
1. In the location of obnoxious facilities, the distance to a closest facility is to be made
as large as possible. This type of problem arises in siting nuclear power plants, sewage
treatment facilities, and landﬁlls, for example.

1184
Chapter 16
DISCRETE OPTIMIZATION
2. In the following network, an optimal solution X1 = {x1} to problem (a) when p = 1
is the midpoint of any edge, with F(X1) = 5. However, if the facility is located at any
vertex v then F({v}) = 4.
1
1
1
2
2
1
1
x 1
16.2.7
EQUITABLE LOCATIONS
The p-median problem is a widely used model for locating public or private facilities.
However, it may leave some demand points (communities) too far from their closest
facility and thus might be perceived as inequitable. To remedy this situation, the p-
median problem can be modiﬁed in several ways.
Deﬁnitions:
Suppose S is a metric space, V ⊆S, w(vi) ≥0 for all vi ∈V , and p is a positive integer.
Let w′(vi) = w(vi)/ Pn
j=1 w(vj) and deﬁne F ′(Xp) = Pn
i=1 w′(vi)d(vi, Xp), Z(Xp) =
Pn
i=1 w′(vi)[d(vi, Xp) −F ′(Xp)]2.
The following three equitable facility location problems are deﬁned:
(a) given a constant β, ﬁnd a set of p points Xp ⊆S to minimize
F(Xp) = Pn
i=1 w(vi)d(vi, Xp)
subject to
d(vi, Xp) ≤β,
for all vi ∈V ;
(b) given a constant α, 0 < α < 1, ﬁnd a set of p points Xp ⊆S to minimize
αF(Xp) + (1 −α)H(Xp);
(c) ﬁnd Xp ⊆S to minimize Z(Xp).
Facts:
1. Since the objective function in (b) is a linear combination of the objective functions
for the p-median and p-center problems, the solution X∗
p is called a centdian.
2. F ′(Xp) = F(Xp)/ Pn
j=1 w(vj) is the mean distance to the consumers given that the
facilities are located at Xp.
3. Z(Xp) is the variance of the distance to the consumers given that the facilities are
located at Xp.
4. Additional results are discussed in [HaMi79], [Ma86].
5. Other models that introduce equity criteria for facility location models are discussed
in [Og09].

Section 16.3
PACKING AND COVERING
1185
Example:
1. The following ﬁgure shows a tree network T on vertices a, b, . . . , g, with the edge
lengths displayed. Suppose that all vertex weights are 1. Then the 1-median of T is
located at vertex c, while the 1-center of T is located at the point x one unit from vertex
d along (d, g). These locations can be calculated using Algorithms 1 and 2 from §16.2.3.
It can be veriﬁed that the centdian of T is at point x for 0 ≤α ≤1
6, at vertex d for
1
6 ≤α ≤1
2, and at vertex c for 1
2 ≤α ≤1.
a
c
b
e
d
g
f
2
1
2
1
1
5
16.3
PACKING AND COVERING
Many practical problems can be formulated as either packing or covering problems. In
packing problems, known activities are given each of which requires several resources for
its completion. The problem is to select a most valuable set of activities to undertake
without using any common resources. Such a problem arises, for example, in scheduling
as many computational activities as possible on a set of machines (resources) that cannot
be used simultaneously for more than one activity. In covering problems, a speciﬁed set
of tasks must be performed, and the objective is to minimize the resources required
to perform the tasks. For example, a number of delivery trucks (resources) operating
on overlapping geographical routes need to be dispatched to pick up items at customer
locations (tasks). The fewest number of trucks are to be sent so that each customer
location is “covered” by at least one of the dispatched trucks. Both exact and heuristic
solution algorithms for packing and covering problems are discussed in this section.
16.3.1
KNAPSACKS
The knapsack problem arises when a single critical resource is to be optimally allocated
among a variety of options. Speciﬁcally, there are available a number of items, each of
which consumes a known amount of the resource and contributes a known beneﬁt. Items
are to be selected to maximize the total beneﬁt without exceeding the given amount of
the resource. Knapsack problems arise in many practical situations involving cutting
stock, cargo loading, and capital budgeting.
Deﬁnitions:
Let N = {1, 2, . . ., n} be a given set of n items. Utilizing item j consumes (requires)
aj > 0 units of the given resource and confers the beneﬁt cj > 0.

1186
Chapter 16
DISCRETE OPTIMIZATION
The knapsack problem (KP) is the following 0-1 integer linear programming problem:
maximize:
P
j∈N
cjxj
subject to:
P
j∈N
ajxj ≤b
xj ∈{0, 1}.
(1)
It is assumed that aj ≤b for all j ∈N. Let z(b) denote the optimal objective value
in (1) for a given integer b.
The LP relaxation (§16.1.8) of (1) is the linear programming problem:
maximize:
P
j∈N
cjxj
subject to:
P
j∈N
ajxj ≤b
0 ≤xj ≤1.
(2)
Let zLP denote the optimal objective value to the LP relaxation (2).
Let N 1 and N 0 be the set of variables taking values 1 and 0, respectively, in the optimal
solution to (2).
Let λ∗be the dual variable (§16.1.5) associated with the knapsack
inequality in the optimal solution to (2).
A cover is a set S ⊆N such that P
j∈S aj > b. The cover S is minimal if no proper
subset of S is a cover.
A branch-and-bound tree for KP is a tree T whose nodes correspond to subproblems
obtained by ﬁxing certain variables of (1) to either 0 or 1. The root of T corresponds
to the original problem (1).
A node t of T is speciﬁed by its level k = 0, 1, . . . , n and the index set Nt ⊆{1, . . ., k}
of variables currently ﬁxed to 1.
Associated with the set Nt is the current beneﬁt zt = P
j∈Nt cj and the available amount
of resource bt = b −P
j∈Nt aj.
Facts:
1. Formulation (1) is a 0-1 integer linear programming problem with a single (knapsack)
constraint. It expresses the optimization problem in which a subset of the n items is to
be selected to maximize the total beneﬁt without exceeding the available amount of the
given resource (the capacity of the knapsack).
2. KP is an NP-hard optimization problem (§17.5.2).
3. KP can be solved in polynomial time for ﬁxed b.
4. Given a rational ǫ > 0, a 0-1 vector x∗can be found satisfying P
j∈N ajx∗
j ≤b
and P
j∈N cjx∗
j ≥(1 −ǫ)z(b) in time polynomially bounded by 1
ǫ and by the sizes of
a = (a1, . . . , an), c = (c1, . . . , cn), and b.
5. If the coeﬃcients aj can be ordered such that each coeﬃcient is an integer multiple
of the previous one, then KP can be solved in polynomial time.
6. If aj−1 ≥aj + · · · + an holds for j = 2, . . . , n then KP can be solved in polynomial
time.
7. Greedy heuristic: This heuristic (Algorithm 1) for the KP processes the variables xj
in nonincreasing order of cj
aj , making each variable equal to 1 if possible.

Section 16.3
PACKING AND COVERING
1187
Algorithm 1:
Greedy heuristic for the KP.
input: KP with c1
a1 ≥c2
a2 ≥· · · ≥cn
an
output: feasible solution x = (x1, x2, . . . , xn)
for k := 1 to n
xk := 1
if Pk
j=1 ajxj > b then xk := 0
8. Suppose zH is the objective value for the solution x produced by Algorithm 1. Then
z(b) ≥zH ≥1
2z(b).
9. Algorithm 1 is most eﬀective if the coeﬃcients aj are small relative to b.
10. The LP relaxation (2) can be solved explicitly by ﬁlling the knapsack in turn with
items j in order of nonincreasing
cj
aj , ignoring the integer restriction. The solution x
obtained has at most one fractional component.
11. Core heuristic: This heuristic (Algorithm 2) for the KP ﬁrst solves the LP relaxation
(2) as in Fact 10, in which at most one variable xk can be fractional. A smaller knapsack
problem is then solved, by setting to 0 any variable xj with index j suﬃciently greater
than k and by setting to 1 any variable xj with index j suﬃciently smaller than k.
Algorithm 2:
Core heuristic for the KP.
input: KP with c1
a1 ≥c2
a2 ≥· · · ≥cn
an
output: feasible solution x = (x1, x2, . . . , xn)
{Solve LP relaxation}
ﬁnd the smallest value k such that Pk
j=1 aj ≥b
{Solve restricted KP}
select any r > 0
xj := 1 for j ≤k −r
xj := 0 for j ≥k + r
solve to optimality the smaller knapsack problem:
maximize Pk+r−1
j=k−r+1 cjxj
subject to Pk+r−1
j=k−r+1 ajxj ≤b −Pk−r
j=1 aj
xj ∈{0, 1}
12. Algorithm 2 is eﬀective if the number of variables n is large since values of r between
10 and 25 give very good approximations in most cases. For further details see [BaZe80].
13. z(b) ≤zLP.
14. If zl is the objective value of a feasible solution to (1), then zl ≤z(b).
15. Node t of the branch-and-bound tree T corresponds to a subproblem having a
nonempty set of feasible solutions if and only if bt ≥0. When this holds, zt is a lower
bound for z(b).
16. An upper bound on the objective value over the subproblem corresponding to node
t is zu
t = ⌊z∗
t ⌋, where z∗
t = zt + max{Pn
j=k+1 cjxj | Pn
j=k+1 ajxj ≤bt, 0 ≤xj ≤1}.
17. Implicit enumeration:
This is an exact technique based on the branch-and-bound
method (§16.1.8). It is implemented using a branch-and-bound tree T , with the following
speciﬁcations:

1188
Chapter 16
DISCRETE OPTIMIZATION
• The initial tree T consists of the root t, with lower bound zl on z(b) obtained
using the greedy heuristic. An upper bound for node t is zu
t = zLP .
• If zu
t ≤zl, then node t is discarded since it cannot provide a better solution.
• If zu
t > zl, there are three cases (where node t is at level k of T ):
⋄ak+1 < bt: If k + 1 < n, create a new node with xk+1 = 1. If k + 1 = n an
optimal solution for node t has xn = 1. Since this solution is feasible for
KP, set zl = zu
t and discard node t.
⋄ak+1 = bt: An optimal solution for node t (and a feasible solution for KP)
is obtained by setting xk+1 = 1 and xj = 0 for j > k + 1. Set zl = zu
t and
discard node t.
⋄ak+1 > bt:
Discard the (infeasible) node with xk+1 = 1 and create a new
node with xk+1 = 0.
• To backtrack from node t let Nt = {j1, . . . , jr} ⊆{1, . . ., k} with j1 < · · · < jr.
If k /∈Nt retreat to level jr and set xjr = 0. If k = jr retreat to level jr−1 and
set xjr−1 = 0.
18. Variable ﬁxing: Given zLP, zl, N 1, N 0, and λ∗, the following tests can be used to
ﬁx variables and reduce the size of the knapsack problem:
• If k ∈N 1 and zLP −(ck −λ∗ak) ≤zl, then ﬁx xk = 1.
• If k ∈N 0 and zLP + (ck −λ∗ak) ≤zl, then ﬁx xk = 0.
• Given k ∈N 1 deﬁne
zk
LP =
X
j∈N 1−{k}
cj + max
n
X
j∈N−N 1
cjxj

X
j∈N−N 1
ajxj ≤b, 0 ≤xj ≤1
o
.
If zk
LP ≤zl, then xk can be ﬁxed to 1.
• Given k ∈N 0 deﬁne
zk
LP = ck + max
n
X
j∈N−N 0
cjxj

X
j∈N−N 0
ajxj ≤b −ak, 0 ≤xj ≤1
o
.
If zk
LP ≤zl, then xk can be ﬁxed to 0.
19. Minimal cover inequality:
If S is a minimal cover then each feasible solution x
to KP satisﬁes P
j∈S xj ≤|S| −1.
20. Lifted minimal cover inequality:
The minimal cover inequality can be further
strengthened. Without loss of generality, assume that a1 ≥a2 ≥· · · ≥an and S = {j1 <
j2 < · · · < jr}. Let µh = Ph
k=1 ajk for h = 1, . . . , r and deﬁne λ = µr −b ≥1. Then
each feasible solution x to KP satisﬁes P
j∈N−S αjxj + P
j∈S xj ≤|S| −1, where
• if µh ≤aj ≤µh+1 −λ then αj = h;
• if µh+1 −λ + 1 ≤aj ≤µh+1 −1 then (a) αj ∈{h, h + 1}, and (b) there is at least
one lifted minimal cover inequality with αj = h + 1.
21. Computer codes for solving knapsack problems can be found at
• http://www.diku.dk/~pisinger/codes.html
22. Further details on the material in this section are available in [KePfPi04], [NeWo88],
and [Sc86].
23. Algorithms for the multidimensional 0-1 knapsack problem are discussed in [FrHa05].

Section 16.3
PACKING AND COVERING
1189
Examples:
1. Investment problem:
An investor has $50,000 to place in any combination of ﬁve
available investments (1, 2, 3, 4, 5). All investments have the same maturity but are issued
in diﬀerent denominations and have diﬀerent (one-year) yields, as shown in the table.
investment
1
2
3
4
5
denomination ($)
10,000
20,000
30,000
10,000
20,000
yield (%)
20
14
18
9
13
Let variable xj = 1 if Investment j is selected and xj = 0 if it is not. The interest earned
for Investment 1 is (0.20)10,000 = 2,000; the values of the other investments are found
similarly. Then the investor’s problem is the knapsack problem
maximize:
2,000x1 + 2,800x2 + 5,400x3 + 900x4 + 2,600x5
subject to:
10,000x1 + 20,000x2 + 30,000x3 + 10,000x4 + 20,000x5 ≤50,000
which has the optimal solution x1 = x3 = x4 = 1, x2 = x5 = 0 with maximum interest
of $8,300.
2. Consider the knapsack problem in 0-1 variables x
maximize:
30x1 + 8x2 + 16x3 + 20x4 + 12x5 + 9x6 + 5x7 + 3x8
subject to:
10x1 + 3x2 + 7x3 + 9x4 + 6x5 + 5x6 + 3x7 + 2x8 ≤27.
Here the variables xj are indexed in nonincreasing order of cj
aj . The optimal solution to
the LP relaxation (2) is x1 = x2 = x3 = 1, x4 = 7
9, xj = 0 otherwise, with zLP = 69 5
9.
The greedy heuristic (Algorithm 1) gives the feasible solution x1 = x2 = x3 = x5 = 1,
xj = 0 otherwise, with zH = 66. Using r = 3, the core heuristic gives x1 = x2 = x4 =
x6 = 1, xj = 0 otherwise. This solution is optimal, with objective value 67.
3. Consider the knapsack problem in 0-1 variables x
maximize:
x1 + x2 + x3 + x4 + x5 + x6
subject to:
10x1 + 8x2 + 4x3 + 3x4 + 3x5 + 2x6 ≤11.
The set S = {3, 4, 5, 6} is a minimal cover which gives the lifted minimal cover inequality
3x1 + 2x2 + x3 + x4 + x5 + x6 ≤3. Adding this inequality and solving the resulting linear
program gives x4 = x5 = x6 = 1, xj = 0 otherwise. This solution is optimal.
4. General knapsack problem:
The general (or unbounded) knapsack problem allows
the decision variables xj to be any nonnegative integers, not just 0 and 1. For example,
an individual can select any number of a given item to pack in a knapsack, subject to
the given weight restriction.
16.3.2
BIN PACKING
Minimizing the number of copies of a resource required to perform a speciﬁed set of
tasks can be formulated as a bin packing problem. It is assumed that no such task can
be split between two diﬀerent units of the resource. For example, this type of problem
arises in allocating a set of customer loads to (identical) trucks, with no load being split
between two trucks. Also, the scheduling of heterogeneous tasks on identical machines
can be viewed as a bin packing problem. Namely, ﬁnd the fewest number of machines of
capacity C such that each task is executed on one of the machines and the total capacity
of jobs assigned to any machine does not exceed C.

1190
Chapter 16
DISCRETE OPTIMIZATION
Deﬁnitions:
The positive integer C denotes the bin capacity.
Let L = (p1, p2, . . . , pn) be a list of n items, where item pi has an integer size s(pi) ≤C.
A subset P ⊆L is a packing if P
pi∈P s(pi) ≤C.
The gap of a packing P is given by the quantity C −P
pi∈P s(pi).
The bin packing problem is the problem of ﬁnding the minimum number of bins (each
of capacity C) needed to pack all items so that the gap in each bin is nonnegative. The
minimum number of bins needed for the list L is denoted b∗(L).
Facts:
1. The bin packing problem is an NP-hard optimization problem (§17.5.2).
2. First ﬁt (FF) method: In this heuristic algorithm, item pi (i = 1, 2, . . . , n) is placed
in the ﬁrst bin into which it ﬁts. A new bin is started only when pi will not ﬁt into any
nonempty bin.
3. Let bF F (L) denote the number of bins produced by the FF algorithm for a list L.
Then bF F (L) ≤min
 17
10b∗(L)

, 1.75b∗(L)
	
.
4. First ﬁt decreasing (FFD) method:
In this heuristic algorithm, the items are ﬁrst
ordered by decreasing size so that s(p1) ≥s(p2) ≥· · · ≥s(pn). Then the FF algorithm
is applied to the reordered list.
5. Let bF F D(L) denote the number of bins produced by the FFD algorithm for a list L.
Then bF F D(L) ≤min
 11
9 b∗(L) + 3, 1.5b∗(L)
	
.
6. If all item sizes are of the form C( 1
k)j, j ≥0, for some ﬁxed positive integer k, then
bF F D(L) = b∗(L).
7. If the item sizes are uniformly distributed on [0, a] with 0 < a ≤C
2 , then asymptoti-
cally bF F D(L)
b∗(L)
→1.
8. Modiﬁed ﬁrst ﬁt decreasing (MFFD) method: This heuristic method (Algorithm 3)
produces a packing using relatively few bins. After the initial phase of packing the largest
sized items LA, let an “A-bin” denote one containing only a single item from LA.
Algorithm 3:
MFFD algorithm for bin packing.
input: list L, bin capacity C
output: a packing of L
partition L into the three sublists LA = {pi | s(pi) ∈( 1
3C, C]},
LD = {pi | s(pi) ∈( 11
71C, 1
3C]}, LX = {pi | s(pi) ∈(0, 11
71C]}
pack the sublist LA using the FFD algorithm.
{Pack as much of LD into A-bins as possible}
1. let bin Bj be the A-bin with the currently largest gap; if the two smallest
unpacked items in LD will not ﬁt together in Bj, go to 4
2. place the smallest unpacked item pi from LD in Bj
3. let pk be the largest unpacked item in LD that will now ﬁt in Bj; place pk
in Bj and go to 1
4. combine the unpacked portion of LD with LX and add these items to the
packing using FFD

Section 16.3
PACKING AND COVERING
1191
9. Let bMF F D(L) denote the number of bins produced by the MFFD algorithm for a
list L. Then asymptotically, as b∗(L) gets large, bMF F D(L) ≤1.183b∗(L).
10. Best ﬁt (BF) method: In this heuristic algorithm, item pi is placed in the bin into
which it will ﬁt with the smallest gap left over. Ties are broken in favor of the lowest
indexed bin.
11. Best ﬁt decreasing (BFD) method:
In this heuristic algorithm, the items are ﬁrst
ordered so that s(p1) ≥s(p2) ≥· · · ≥s(pn). Then the BF algorithm is applied to the
reordered list.
12. Asymptotic worst-case bounds for BF [BFD] are the same as those for FF [FFD].
In practice the BF version performs somewhat better.
13. For a ﬁxed number of d diﬀerent item sizes, the bin packing problem can be solved
in polynomial time [GoRo14].
14. Further details on the material covered in this section are found in [CoEtal13] and
[CoGaJo84].
Examples:
1. Television commercials are to be assigned to station breaks.
This is a bin pack-
ing problem where the duration of each station break is C and the duration of each
commercial is s(pi).
2. Material such as cable, lumber, or pipe is supplied in a standard length C. Demands
for pieces of the material are for arbitrary lengths s(pi) not exceeding C. The objective is
to use the minimum number of standard lengths to supply a given list of required pieces.
This is also a bin packing problem.
3. A set of independent tasks with known execution times s(pi) are to be executed
on a collection of identical processors. Determining the minimum number of processors
needed to complete all tasks by the deadline C is a bin packing problem.
4. Consider the list L = (4, . . . , 4, 7, . . . , 7, 8, . . ., 8, 13, . . ., 13), consisting of twelve 4s
and six each of 7s, 8s, and 13s. Each bin has capacity C = 24. Either FF (or BF) when
applied to L results in a packing with twelve bins: two bins are packed as (4, 4, 4, 4, 4, 4),
two as (7, 7, 7), two as (8, 8, 8), and six as (13).
5. If FFD (or BFD) is applied to the list in Example 4, a packing with ten bins results:
six bins are packed as (13, 8), two as (7, 7, 7), and two as (4, 4, 4, 4, 4, 4).
6. If MFFD is applied to the list in Example 4, then LA contains the six 13s and LD
contains the remaining items. Packing LA using FFD results in six A-bins, each con-
taining a single 13 and having gap 11.
Steps 1–3 of Algorithm 3 result in six bins
packed as (13, 7, 4), and Step 4 yields two bins packed as (8, 8, 8) and one bin packed as
(4, 4, 4, 4, 4, 4). This is an optimal solution since all nine bins are completely packed.
16.3.3
SET COVERING AND PARTITIONING
Set covering or set partitioning problems arise when a speciﬁed set of tasks must be
performed while minimizing the cost of resources used. Such problems arise in scheduling
ﬂeets of vehicles or aircraft, locating ﬁre stations in an urban area, political redistricting,
and fault testing of electronic circuits.
Deﬁnitions:
Let e denote the column vector of all 1s.

1192
Chapter 16
DISCRETE OPTIMIZATION
Let A = (aij) be a 0-1 incidence matrix and let c = (cj) be a row vector of costs.
The set Aj = {i | aij = 1} contains all rows covered by column j.
The set covering (SC) problem is the 0-1 integer linear programming problem
minimize:
cx
subject to:
Ax ≥e
xj ∈{0, 1}.
Let v∗be the optimal objective value to this problem.
The set partitioning (SP) problem has the same form as the set covering problem
except the constraints are Ax = e.
The LP relaxation of SC or SP is obtained by replacing the constraints xj ∈{0, 1} by
0 ≤xj ≤1. Let vLP be the optimal objective value to the LP relaxation.
The matrix A is totally unimodular if the determinant of every square submatrix of A
is 0, +1, or −1.
The matrix A is balanced if A has no square submatrix of odd order, containing exactly
two 1s in each row and column.
The matrix A is in canonical block form if, by reordering, its columns can be parti-
tioned into t nonempty subsets B1, . . . , Bt such that for each block Bj there is some row
i of A with aik = 1 for all k ∈Bj and aik = 0 for k ∈∪t
l=j+1Bl. The rows of A are then
ordered so that the row deﬁning Bj becomes the jth row for j = 1, . . . , t.
Facts:
1. Formulation SC expresses the problem of selecting a set of columns (sets) that to-
gether cover all rows (elements) at minimum cost. In formulation SP, the covering sets
are required to be disjoint.
2. Both SC and SP are NP-hard optimization problems (§17.5.2).
3. Checking whether a set partitioning problem is feasible is NP-hard.
4. In many instances (including bin packing, graph partitioning, and vehicle routing)
the LP relaxation of the set covering (partitioning) formulation of the problem is known
to give solutions very close to optimality.
5. For the bin packing and vehicle routing problems (see Examples 2–3) v∗≤4
3⌈vLP ⌉.
6. If A is totally unimodular or balanced, then the polyhedra {x | Ax ≥e, 0 ≤xj ≤1}
and {x | Ax = e, 0 ≤xj ≤1} have only integer extreme points (vertices). In this
case, SC and SP can be solved in polynomial time using linear programming.
7. Checking whether a given matrix A is totally unimodular or balanced can be done
in polynomial time.
8. Every 0-1 matrix that is totally unimodular is also balanced. The converse, however,
is not true (see Example 4).
9. The matrix A is totally unimodular if and only if each collection of columns of A can
be split into two parts so that the sum of the columns in one part minus the sum of the
columns in the other part is a vector with entries 0, +1, −1.
10. Greedy heuristic:
This heuristic (Algorithm 4) for the set covering problem suc-
cessively chooses columns that have smallest cost per covered row.

Section 16.3
PACKING AND COVERING
1193
Algorithm 4:
Greedy heuristic for the set covering problem.
input: 0-1 m × n matrix A, costs c
output: feasible set cover x
M 1 := {1, 2, . . ., m}; N 1 := {1, 2, . . ., n}; k := 1
{Main loop}
select jk ∈N k to minimize
cj
|Aj∩Mk|
N k+1 := N k −{jk}
obtain M k+1 from M k by deleting all rows containing a 1 in column jk
if M k+1 = ∅then xj := 1 for j /∈N k+1 and xj := 0 otherwise
else k := k + 1
{Continue with next iteration of main loop}
11. Randomized greedy heuristic: This heuristic for the set covering problem is similar
to Algorithm 4 except that at iteration k the column jk ∈N k is selected at random from
among columns j satisfying
cj
|Aj∩Mk| ≤(1 + α) min

cr
|Ar∩Mk|
 r ∈N k	
, where α ≥0.
12. Whereas Algorithm 4 is run only once, the randomized greedy heuristic is repeated
several times and the best solution is selected.
13. Implicit enumeration:
This exact approach (Algorithm 5) for SP works well for
dense matrices. In this algorithm, S is the index set of the variables ﬁxed at 1, z is the
associated objective value, and R is the set of rows satisﬁed by S. Also z∗denotes the
objective value of the best feasible solution found so far.
Algorithm 5:
Implicit enumeration method for SP.
input: 0-1 matrix A, costs c
output: optimal set of columns S (if any)
place A in canonical block form with blocks Bj
order the columns within Bj by nondecreasing ct/ P
i ait
S := ∅; R := ∅; z := 0, z∗:= ∞
1. r := min{i | i /∈R}; set a marker in the ﬁrst column of Br
2. examine all columns of Br in order starting from the marked column
if column j is found with aij = 0 for all i ∈R and z + cj < z∗then go to 3
if Br is exhausted then go to 4
3. S := S ∪{j}; R := R ∪{i | aij = 1}; z := z + cj
if all rows are included in R then z∗:= z and go to 4 else go to 1
4. if S = ∅then terminate with the best solution found (if any)
else let k := the last index included in S
S := S −{k}; update z and R
Br := the block to which column k belongs
move the marker in Br forward by one column and go to 2
14. Other implicit enumeration approaches to set partitioning and set covering are dis-
cussed in [BaPa76].

1194
Chapter 16
DISCRETE OPTIMIZATION
15. Cutting plane methods:
Cutting plane methods (§16.1.8) have been used success-
fully to solve large set partitioning and set covering problems. For details regarding an
implementation used to solve crew scheduling problems see [HoPa93].
16. Further details on the material in this section can be found in [GaNe72], [Ho97],
[KoVy12], [NeWo88], and [Sc86].
Examples:
1. Crew scheduling problem: An airline must cover a given set of ﬂight segments with
crews. There are speciﬁed work rules that restrict the assignment of crews to ﬂights.
The objective is to cover all ﬂights at minimum total cost. The rows of the matrix A
correspond to the ﬂights that an airline has to cover. The columns of A are the incidence
vectors of ﬂight “rotations”: sequences of ﬂight segments for each ﬂight that begin and
end at individual base locations and that conform to all applicable work rules.
The
objective is to minimize crew costs. This problem can be formulated as either a set
covering or set partitioning problem.
2. Bin packing:
The bin packing problem (§16.3.2) can be formulated as a set parti-
tioning problem. The rows of the matrix A correspond to the items and the columns are
incidence vectors of any feasible packing of items to a bin. The cost of each variable is
1 if the number of bins is to be minimized. In general, a weighted version can also be
formulated where diﬀerent bins have diﬀerent costs.
3. Vehicle routing: Given are a set of customers and the quantity that is to be supplied
to each from a warehouse. A ﬂeet of trucks of a speciﬁed capacity is available. The
objective is to service all the customers at minimum cost. The rows of the matrix A
correspond to the customers and the columns are incidence vectors of feasible assignments
of customers to trucks (a bin packing problem). The cost of each variable is the cost of
the corresponding assignment of customers to the truck. This problem can be formulated
as either a set covering or set partitioning problem.
4. The following matrix A is not totally unimodular, since det(A) = −2. This can also
be seen using Fact 9. If A has columns Cj then (C1 + C2) −(C3 + C4) = (0, 2, 0, 0)T has
an entry greater than one in absolute value. However, A is a balanced matrix.
A =





1
1
1
1
1
1
0
0
1
0
1
0
1
0
0
1





5. There are four requests R1, R2, R3, R4 for information stored in a database, which
is composed of ﬁve large ﬁles {1, 2, 3, 4, 5}. Request R1 can be fulﬁlled by retrieving
ﬁles 1, 3, or 4; request R2 by retrieving ﬁles 2 or 3; request R3 by retrieving ﬁles 1
or 5; and request R4 by retrieving ﬁles 4 or 5. The lengths of the ﬁles are 7, 3, 12, 7, 6
(gigabytes), respectively, and the time to retrieve each ﬁle is proportional to its length.
Filling all requests in the minimum amount of time is then a set covering problem, with
costs c = (7, 3, 12, 7, 6) and incidence matrix
A =





1
0
1
1
0
0
1
1
0
0
1
0
0
0
1
0
0
0
1
1




.
Applying the greedy heuristic (Algorithm 4) produces j1 = 2, j2 = 5, and j3 = 1, giving
x = (1, 1, 0, 0, 1) with total cost 16. This is an optimal solution to the SC problem.

Section 16.4
ACTIVITY NETS
1195
16.4
ACTIVITY NETS
Activity nets are important tools in the planning, scheduling, and control of projects.
In particular, the CPM (Critical Path Method) and PERT (Program Evaluation and
Review Technique) models are widely used in the management of large projects, such as
those occurring in construction, shipbuilding, aerospace, computer system design, urban
planning, marketing, and accounting.
16.4.1
DETERMINISTIC ACTIVITY NETS
The scheduling of large complex projects can be aided by modeling as a directed net-
work of activities having known durations and resource requirements, with the network
structure deﬁning the activity precedences. The commonly used critical path method is
described as well as extensions that address constrained resources, ﬁnancial considera-
tions, and project compression.
Deﬁnitions:
A project is deﬁned by a set of activities that are related by precedence relations.
An activity consumes time and resources to accomplish, whereas a dummy activity
consumes neither.
Activity u (strictly) precedes activity v, written u ≺v, if activity u must be completed
before activity v can be initiated.
A project can be represented using a directed acyclic network G (§8.3.4).
In the activity-on-node (AoN) representation of a project, the network G contains
a node for each activity and the arcs of G represent the precedence relations between
nodes (activities).
In the activity-on-arc (AoA) representation of a project, the network G contains an
arc for each activity and the nodes of G represent certain events. Precedence relations
are described by the network arcs, possibly requiring the use of dummy arcs (dummy
activities). In the AoA representation, the network is assumed to have no multiple arcs
joining the same pair of nodes, so an activity can be unambiguously referred to by (i, j)
for some nodes i and j, with the corresponding activity duration being aij.
Network G is a deterministic activity net if the precedence relations and the param-
eters associated with the activities are known deterministically. Such a network is also
referred to as a Critical Path Method (CPM) model.
An initial node of G has no entering directed arcs; a terminal node has no exiting
directed arcs.
Generalized precedence relations (GPRs) relax the necessity of a strict precedence
between activities. They can be speciﬁed in the form of certain lead or lags between a
pair of activities, commonly by start-to-start, ﬁnish-to-ﬁnish, start-to-ﬁnish, and
ﬁnish-to-start relations.
The optimal project compression problem is that of achieving a target project
completion time with least cost, or alternatively minimizing the duration of the project
subject to a speciﬁed budget constraint.

1196
Chapter 16
DISCRETE OPTIMIZATION
The complex interaction between the required resources and the duration of an activity
is assumed to be given by the functional relationship ca = φ(ya), where ya is the duration
of activity a, ℓa ≤ya ≤ua, and ca is its cost. The upper limit ua is the normal duration
and the lower limit ℓa is the crash duration of activity a.
Facts:
1. The CPM model arose out of the need to solve industrial scheduling problems; the
original work was jointly sponsored by Dupont and Sperry-Rand in the late 1950s.
2. In the AoA representation, the network can be assumed to have a single initial node
and a single terminal node. These conditions can in general be guaranteed, possibly
through the introduction of dummy arcs.
3. Suppose that the AoA representation of a network has n nodes, with initial node 1
and terminal node n. Then the nodes can always be renumbered (topologically sorted)
such that each arc leads from a smaller numbered node to a larger numbered one. (See
§8.3.4.)
4. In the AoA representation, the earliest time of realization of node j, written tj(E), is
determined recursively from tj(E) = maxi∈B(j){ti(E) + aij} and t1(E) = 0, where B(j)
is the set of nodes immediately preceding node j.
5. Suppose the time of realization of node n is speciﬁed as tn(L) ≥tn(E). The lat-
est time of realization of node i, written ti(L), is determined recursively from ti(L) =
minj∈A(i){tj(L) −aij}, where A(i) is the set of nodes immediately succeeding node i.
6. tj(L) ≥tj(E) holds for any node j. The diﬀerence tj(L) −tj(E) ≥0 is called the
node slack for j.
7. For each activity (i, j) there are four activity ﬂoats corresponding to the diﬀerences
tj(X) −ti(Y ) −aij, where X, Y ∈{E, L}:
• total ﬂoat: T F(i, j) = tj(L) −ti(E) −aij;
• safety ﬂoat: SF(i, j) = tj(L) −ti(L) −aij;
• free ﬂoat: FF(i, j) = tj(E) −ti(E) −aij;
• interference ﬂoat: IF(i, j) = tj(E) −ti(L) −aij.
8. T F(i, j), SF(i, j), and FF(i, j) are always nonnegative, whereas IF(i, j) can be
negative, indicating infeasibility of realization under the speciﬁed conditions (all activities
succeeding node j are accomplished as early as possible and all activities preceding node
i are accomplished as late as possible).
9. A critical activity (i, j) has total ﬂoat T F(i, j) = 0. If tn(L) = tn(E), then the set of
critical activities contains at least one path from node 1 to node n, which represents a
longest path in the network from node 1 to node n. Such a path is called a critical path
(CP).
10. Floats play an important role in both resource allocation and activity scheduling,
since ﬂoats give a measure of the ﬂexibility in scheduling activities during project execu-
tion without delaying the project completion time.
11. The problems of optimal resource allocation and activity scheduling subject to the
known precedence constraints are NP-hard optimization problems (§17.5.2).
12. Practical solutions to optimal resource allocation and activity scheduling problems
are based on heuristics.
Virtually all heuristics used in practice rely on ranking the
activities according to their ﬂoat (T F, SF, FF, IF).

Section 16.4
ACTIVITY NETS
1197
13. The measure T F is the only ﬂoat in the AoA mode that is representation-invariant;
this measure is the same in both modes of representation and in all AoA models of the
same project.
14. The SF, FF, IF measures are representation-dependent: they do indeed depend on
the structure of the AoA and they may also vary from their AoN values.
15. A simple redeﬁnition of ti(E) for nodes i with all outgoing arcs dummy and tj(L)
for nodes j with all incoming arcs dummy reestablishes the invariance of the activity
ﬂoats to the mode of representation [ElKa90].
16. A plethora of “oﬀ-the-shelf” project planning and control software packages for PCs
are currently available [DeHe90], [KoPa01]. The review [DeHe90] also outlines criteria
against which a software package should be judged.
17. An extensive listing of commercial and noncommercial software for project planning
can be found at
• https://wikipedia.org/wiki/Comparison of project management software
18. GPRs aﬀord the ﬂexibility of modeling relations that are present among activities
in many practical situations, gained at a price in computational eﬀort and interpretation
of results. The concepts of criticality and ﬂoat of an activity take on a new meaning,
since activities may be “compressed” (speeded up) or “expanded” (slowed down) from
their “normal” durations [ElKa92].
19. Considerations of resource availabilities are important in project planning and its
dynamic control. The most common planning criteria are
• minimization of the project duration;
• smoothing of resource usage;
• minimization of the maximum resource utilization;
• minimization of the cost of resource usage;
• maximization of the present value of the project.
20. In the presence of limited resources, the “critical path” may no longer be a “path”,
in the sense of a connected chain of activities. What emerges is the concept of a critical
sequence of activities [El77], which need not form a connected chain in the network. (See
Example 4.)
21. The scheduling of activities related by arbitrary precedence relations subject to
resource availabilities is an NP-hard problem. Consequently, such problems are typically
approached by integer programming techniques (§16.1.8) or heuristics (e.g., simulated
annealing, tabu search, genetic algorithms, neural nets).
22. The book [SlWe89] discusses project scheduling under constrained resources; in
particular, Chapter 5 of Part I evaluates various heuristics that have been proposed.
Also [HeDeDe98] gives a review of recent contributions to this area.
23. Typically, resources are available in one or several units or may be acquired at a cost.
Mathematical models (large-scale integer linear programs) abound for the minimization
of the project duration [El77]. Various branch-and-bound approaches have been proposed
for these models [DeHe92], [Sp94].
24. Heuristic procedures, let alone optimization algorithms, for activity scheduling un-
der the other criteria mentioned in Fact 19 are not generally available.
25. In the CPM model of activity networks there is little problem in deﬁning the cost
of an activity, and subsequently the cost of the project.

1198
Chapter 16
DISCRETE OPTIMIZATION
26. Generally, there are two streams of cash ﬂow (from the contractor’s point of view):
an in-stream representing payments to the contractor by the owner, and an out-stream
representing payments by the contractor in the execution of the activities.
27. From the owner’s point of view there is only one stream of cash ﬂows: namely,
payments to the contractor for work accomplished. Given a particular schedule for the
activities, the two streams of cash ﬂow can be easily obtained. The problem is then
scheduling activities to maximize the net present value (NPV) of the project.
28. Issues concerning the NPV of a project are equally important to those interested
in bidding on a proposed project and those who are committed to carry out an already
agreed-upon project. Succinctly stated, the problem is to determine the dates of the
deliverables in order to maximize the NPV.
29. Suppose that the function φ(ya) is nonincreasing over the interval [ℓa, ua]. Reference
[El77] gives a treatment of linear, convex, concave, and discrete functions, while [ElKa92]
discusses the case in which φ(ya) is piecewise linear and convex over the interval [ℓa, ua].
Examples:
1. Construction planning:
The construction of a house involves carrying out the nine
activities listed in the following table.
Their durations and (immediate) predecessor
activities are also indicated.
activity
duration (days)
predecessors
foundation/frame
12
—
wiring/plumbing
4
foundation/frame
sheetrock
7
wiring/plumbing
interior paint
2
sheetrock, windows
carpet
3
interior paint
roof
3
foundation/frame
siding
7
roof
windows
2
siding
exterior paint
2
windows
An AoA representation of this project is shown in the following ﬁgure. It is necessary to
use a dummy activity to ensure that the given precedences are faithfully depicted. The
nodes have been numbered in topological order, with node 1 the initial node and node 9
the terminal node. The longest path from node 1 to node 9 is [1, 2, 4, 5, 6, 7, 8, 9] with
length 29, corresponding to a project completion time of 29 days.
2. A project is composed of the four activities a, b, c, d with precedence relations a ≺c,
a ≺d, and b ≺d. The AoN representation of this project is shown in part (a) of the
following ﬁgure. The AoA representation is shown in part (b) of the ﬁgure, where the
nodes have been numbered in topological order (Fact 3). The dummy activity joining
nodes 2 and 3 is needed to maintain the integrity of the precedence relations. Activity
durations are indicated on the arcs of part (b) of the ﬁgure.

Section 16.4
ACTIVITY NETS
1199
3. The earliest and latest event times tj(E), tj(L) are shown next to each node j in
part (b) of the ﬁgure of Example 2, where t4(L) = 14, which is 2 units more than
t4(E) = 12. Here all nodes have the same slack of 2, which provides no information
on the various activity ﬂoats, given in the following table. Since T F(i, j) > 0 for all
activities (i, j), there are no critical activities and no critical path, since a small delay in
any activity will not delay the completion time of the project. The critical path can be
determined if instead t4(L) = t4(E) = 12. Then the critical path is given by [1, 2, 3, 4].
activity
T F
SF
FF
IF
(1, 2)
2
0
0
−2
(1, 3)
5
3
3
1
(2, 4)
3
1
1
−1
(3, 4)
2
0
0
−2
4. The following ﬁgure gives a project with six activities in AoN representation. There
is a single resource, with availability of 6 units. The duration of each activity and the
required quantity of the resource are indicated next to each activity (node).
1
3
1
3
4
5
6
2
5
4
6
2
4
3
5
6
6
5
4
3
2
1
6
5
4
3
2
1
resource
resource
1
3
5
7
1
3
5
7 8
9
11
(1,3)
(2,5)
(1,4)
(2,3)
(1,5)
(4,3)
duration
resource
(a)
(b)
(c)
2
2
1
The CP (based solely on durations) is [1, 3, 5, 6] of duration 5. If the integrity of the CP
is maintained as long as possible, then activity 4 must be inserted before activity 6 (thus
breaking the continuity of the CP), which is then followed by activity 2, as shown in part

1200
Chapter 16
DISCRETE OPTIMIZATION
(b) of the ﬁgure. The total duration of the project under this schedule is 11 time units.
Now consider the schedule shown in part (c) of the ﬁgure, in which the CP is split after
activity 1; the total duration of the project is thereby reduced to only 8 time units.
5. The project of the following ﬁgure is shown in AoA mode, with the duration of each
activity written beside each arc.
The payment shown next to a node is the income
accrued (if positive) or expense incurred (if negative) at the time of realization of that
event (node). The CP is [1, 3, 4] with duration 11. Ignoring the time value of money (i.e.,
assuming a discount factor β = 0) gives 1,000 as the estimate of project proﬁt. Assuming
a discount factor β = 0.99 and that activities are done as early as possible to maintain
the CP, the estimate of project proﬁtability shrinks to −5,000(.99)2 + 3,000(.99)8 +
3,000(.99)11 = 553.75.
2
4
3
1
2
8
3
8
4
(-5000)
(+3000)
(+3000)
Now suppose that the schedule of activities is modiﬁed as follows: delay activity (1, 2)
to complete at time t2 = 4 (instead of 2); do activity (1, 3) as early as possible to
complete at time t3 = 8; and do activity (2, 4) as early as possible (after the realization
of node 2) to complete at time t4 = 12.
Then the project proﬁtability increases to
−5,000(.99)4 + 3,000(.99)8 + 3,000(.99)12 = 624.41. Note that the increase in project
proﬁtability comes as a consequence of ignoring the CP, and in fact delaying the project
beyond its normal duration.
6. A project involving ﬁve activities is shown in the following ﬁgure in AoA mode. Each
activity (arc) a is labeled with (ua, ℓa, ka) where ka is the marginal cost of reducing dura-
tion from the normal time ua. Next to each node j is its earliest time of realization tj(E)
under normal activity durations.
2
4
3
1
(3,1,3)
(6,1,3)
(5,2,1)
(4,2,1)
(2,0,1)
0
3
11
5
The next table summarizes the breakpoints of the resulting piecewise linear cost function.
duration
marginal
cumulative
breakpoint
(t4)
cost
cost
1
11
1
0
2
10
2
1
3
9
3
3
4
8
4
6
5
4
5
22
6
3
∞
27

Section 16.4
ACTIVITY NETS
1201
The function itself is shown in the following ﬁgure. With the complete cost function
in hand it is easy to answer various questions. For example, the least additional cost
required to reduce the project duration from its normal value 11 to 7 is seen to be 10.
Alternatively, if 6 additional units of money are available, then the maximum reduction
achievable in the project duration is 3 units of time (from 11 to 8).
30
20
10
0
0
5
10
t4
cumulative  cost
16.4.2
PROBABILISTIC ACTIVITY NETS
The CPM model can be extended to incorporate uncertainty or randomness.
If the
durations of activities are random variables, then the network is a PERT (Program
Evaluation and Review Technique) model.
Alternatively, the very undertaking of an
activity may be determined by chance and this consideration has led to the development
of GAN (Generalized Activity Network) models.
Deﬁnitions:
A probabilistic activity net is a directed network in which some or all of the param-
eters, including the realization of the activities, are probabilistically known.
In a PERT model, activity durations are random variables. The duration of activity a
has expected value µa and variance σ2
a.
Let P(τ) be the probability that the project is completed by time τ.
The criticality index of a path Q in the network is the probability that Q is a critical
path in any realization of the project.
The criticality index of an activity a is the probability that a lies on a critical path in
any realization of the project.
A GAN model is a probabilistic activity net with conditional progress and probabilistic
realization of activities.
If X is a standard normal deviate (§7.3.1), then its (cumulative) distribution function is
denoted by Φ(x) = Pr(X ≤x).

1202
Chapter 16
DISCRETE OPTIMIZATION
Facts:
1. The original PERT model evolved in the late 1950s from the U.S. Navy’s eﬀorts to
plan and accelerate the Polaris submarine missile project.
2. A detailed account of the original PERT model, its analysis, and the criticisms levied
against it is found in [El77, Chapter 3].
3. Estimation of the exact probability distribution function (pdf) of the project duration
is an extremely diﬃcult problem due to the nonindependence of the paths leading from
the initial node to the terminal node.
4. The original PERT model suggested substituting µa for each activity duration and
then proceeding with the standard CPM calculations to determine a critical path Q∗in
the resulting deterministic network.
5. The pdf of the duration of the project can then be approximated using a normal
distribution having mean bµQ∗= P
a∈Q∗µa and variance bσ2
Q∗= P
a∈Q∗σ2
a. The normal
approximation increases in validity as the number of activities in the path Q∗increases.
6. The probability P(τ) of project completion by time τ can be approximated using
bP(τ) = Φ((τ −bµQ∗)/ bσQ∗).
7. The value bµQ∗always underestimates the exact mean project duration (often, seri-
ously). No equivalent statement can be made about the variance estimate bσ2
Q∗except
that it is often a gross approximation of the exact variance.
8. PERT analysis goes one step further and uses an approximation to the expected
value and the variance of each activity, based on the assumption that each activity
duration follows a beta distribution (§7.3.1). In particular, the variance is approximated
by
1
36(range)2. These additional assumptions render the procedure even more suspect.
9. An immediate consequence of randomness in the activity durations is that (virtually)
any path can be the CP in some realization of the project. Thus, the criticality index of
a path and the criticality index of an activity are more meaningful concepts. See [Wi92]
for a critique of the latter.
10. In general, it is extremely diﬃcult to determine the exact values of the criticality
indices analytically. Monte Carlo sampling is typically used to estimate these values
[BaCoPr95].
11. Since the early days of PERT, signiﬁcant strides have been made in estimating
the various parameters in the PERT model. The approaches can be classiﬁed into the
categories of exact, approximating, and bounding procedures. See [El89], [Ka92].
12. The concept of a uniformly directed cutset has been used to evaluate some common
network performance criteria under the assumption of exponentially distributed activity
durations [KuAd86]. Attempts to extend the concept to applications in optimal resource
allocation have had limited success thus far.
13. The restriction of GANs to “exclusive-or” type nodes renders the network a graphi-
cal representation of a semi-Markov process. The resulting GERT (Graphical Evaluation
and Review Technique) model has been expanded into SLAM II, an extremely powerful
discrete event simulation language.
14. The analysis of stochastic activity nets with exclusive-or type nodes (STEOR-nets)
is thoroughly discussed in [Ne90].

Section 16.4
ACTIVITY NETS
1203
Examples:
1. The following ﬁgure shows a project with six activities whose durations are random
variables that assume discrete values with equal probabilities. For example, activity (1, 2)
has duration 1, 2, or 5 with probability 1
3 each.
2
4
3
1
(1,2,5)
(1,4,5)
(1,6,12)
(0,1,10)
(2,8)
(3,5)
The exact distribution of project completion time (secured by complete enumeration of
the 324 realizations) is shown in the following table, from which it is seen that the true
mean project duration is µ = 12.315 and the true standard deviation is σ = 2.5735. The
probability that the project duration is no more than 12 time units is P(12) = 0.4815.
The PERT estimates of these same parameters, based on the deterministic critical path
[1, 2, 3, 4], are bµQ∗= 10, bσQ∗= 1, and bP(12) = 0.9772.
duration
frequency
relative
(t4)
frequency
17
36
0.1111
15
12
0.0370
14
48
0.1481
13
72
0.2222
12
42
0.1296
11
30
0.0926
10
36
0.1111
9
28
0.0864
8
10
0.0309
7
6
0.0185
6
2
0.0062
5
2
0.0062
324
1.0000
2. The paths from initial node 1 to terminal node 4 for the project in the ﬁgure of
Example 1 are Q1 = [1, 2, 4], Q2 = [1, 2, 3, 4], Q3 = [1, 4], Q4 = [1, 3, 4]. The following
table lists the frequency and relative frequency that each path Qi, or combination of
paths, is a critical path. The path criticality indices are then easily determined from this
table. For example, the criticality index of Q1 is 0.3951 (= 0.3457+0.0309+0.0185), and
of Q4 is 0.2592 (= 0.1975+0.0185+0.0432). The criticality index of each activity can be
easily determined from the criticality indices of the paths. For instance, the criticality
index of activity (1, 2), which lies on paths Q1 and Q2, is 0.7284 (= 1 −0.0741 −0.1975).

1204
Chapter 16
DISCRETE OPTIMIZATION
t4
Q1
Q2
Q3
Q4
Q1, Q2
Q1, Q4
Q2, Q3
Q2, Q4
17
36
15
12
14
36
12
13
30
6
24
6
6
12
12
24
6
11
6
18
6
10
8
24
4
9
10
16
2
8
2
6
2
7
2
2
2
6
2
5
2
freq.
112
90
24
64
10
6
4
14
rel. freq.
.3457
.2778
.0741
.1975
.0309
.0185
.0123
.0432
16.4.3
COMPLEXITY ISSUES
Facts:
1. The AoN representation of a project is essentially unique.
2. The AoA representation is not unique because of the necessity to introduce dummy
activities (e.g., to maintain the integrity of the precedence relations).
3. Construction of the AoA representation can be carried out with diﬀerent objectives
in mind: to minimize the number of nodes, to minimize the number of dummy activities,
or to minimize the complexity index of the resulting AoA network [MiKaSt93].
4. Analytical solutions to optimization problems for project networks often proceed by
conditioning upon certain activities, and then removing the conditioning through either
enumeration or multiple integration.
Minimizing the computing eﬀort then involves
minimizing the number of activities on which such conditioning takes place.
5. If the network is series-parallel then no conditioning is required and its analysis is
straightforward, though it may be computationally demanding.
6. If the network is not series-parallel, then the minimum number of activities for con-
ditioning can be secured by the optimal node reduction procedure of [BeKaSt92], which
has polynomial complexity.
7. Patterson [Pa83] collected a set of 110 standard test problems, useful for compar-
ing alternative solution procedures. These problems have been supplanted by a more
recent sets of test problems [KaSpDr92], [KoSp97]. Benchmark problems for resource-
constrained project scheduling problems are available at
• http://www.om-db.wi.tum.de/psplib/library.html
8. Several measures of the complexity of a project network were proposed in the 1960s,
with questionable validity. The signiﬁcance of the complexity index [BeKaSt92] in ac-
counting for the diﬃculty in analysis is discussed in [DeHe96].

Section 16.5
GAME THEORY
1205
16.5
GAME THEORY
Games, mathematical models of conﬂict or bargaining, can be classiﬁed in three ways:
by mood of play (noncooperative or cooperative), by ﬁeld of application (e.g., biology
or economics), and by mathematical structure (e.g., discrete, continuous, or diﬀerential).
Correspondingly, game theory is a vast and diverse subject with diﬀerent traditions in
each of many specialties. This section discusses discrete games, in which ﬁnitely many
strategies are available to ﬁnitely many players. Combinatorial and other games form
largely separate disciplines to which appropriate references appear in §16.5.4.
16.5.1
NONCOOPERATIVE GAMES
This section discusses noncooperative games involving a ﬁnite number of players. Collu-
sion among the players is not allowed in these types of games. Such games can model a
wide variety of situations, as indicated in §16.5.4.
Deﬁnitions:
An n-player game Γ in extensive form consists of
• a set {1, . . . , n} ∪{0} of n decisionmakers (or players) augmented by a ﬁctitious
player, called 0 (or chance), whose actions are random;
• a tree, in which each nonterminal vertex represents a decision point for some
player, whose possible actions correspond to arcs emanating from the vertex;
• a payoﬀfunction that assigns an n-vector to each terminal vertex;
• a partition of the nonterminal vertices into n + 1 vertex sets, one for each player
and for chance;
• a subpartition of each player’s vertex set into subsets (information sets), such
that no vertex follows another in the same subset and all vertices in a subset
are followed by the same number of arcs;
• a probability distribution on arcs emanating from any chance vertex.
A subgame of Γ is a game whose tree is a subtree of the tree for Γ. A subgame is
proper if the information set that contains its root contains no other vertices.
A game is ﬁnite if its tree is ﬁnite.
A game has perfect information if all information sets contain a single vertex; otherwise,
it has imperfect information.
A game has complete information if all players know the entire extensive form including
all terminal payoﬀs; otherwise it has incomplete information.
A pure strategy is a function that maps each of a player’s information sets to an
emanating arc.
An n-person game in normal (or strategic) form consists of a set N = {1, 2, . . ., n}
of players, a set Sk of possible pure strategies for each k ∈N, and a payoﬀfunction
f = (f1, f2, . . . , fn) that assigns fk(w) to Player k for every pure strategy combination
w = (w1, w2, . . . , wn), where wk ∈Sk. Payoﬀs are computed by taking expected values
over distributions associated with chance vertices in the corresponding extensive form.

1206
Chapter 16
DISCRETE OPTIMIZATION
Let D ⊆S1 × S2 × · · · × Sn be the set of all possible pure strategy combinations w.
Let w || wk denote the joint pure strategy combination that is identical to w except for
the strategy of Player k:
w || wk = (w1, . . . , wk−1, wk, wk+1, . . . , wn).
w∗∈D is a Nash equilibrium pure strategy combination (or simply equilibrium)
if, for every k ∈N, fk(w∗) ≥fk(w∗|| wk) holds for all wk ∈Sk. (John F. Nash, 1928–
2015) Let E denote the set of all such equilibria.
For k ∈N deﬁne the function mk that minimizes fk(w) over components of w that k
does not control:
mk(wk) =
min
{w | wk=wk} fk(w).
If ewk maximizes mk(wk), then ewk is a max-min strategy for k and efk = mk( ewk) is
the corresponding max-min payoﬀ.
Let D∗= {w ∈D | fk(w) ≥efk for all k ∈N}.
The strategy combination w is individually rational for all players if w ∈D∗.
The combination w ∈D is group rational (or Pareto-optimal) if no w ∈D exists
such that fk(w) ≥fk(w) for all k ∈N and fi(w) > fi(w) for some i ∈N.
Let P denote the set of all Pareto-optimal w. The set P ∗= P ∩D∗is the bargaining
set and each w ∈P ∗is a cooperative strategy combination.
An equilibrium is subgame perfect if its restriction to any proper subgame is also an
equilibrium. Let ES denote the set of subgame perfect equilibria.
Facts:
1. Information sets are constructed so that in making a decision a player knows the
identity of the information set, but not the particular vertex of the set at which the
decision is being made.
2. At an equilibrium w∗∈D, no k ∈N has a unilateral incentive to depart from (w∗)k
if each j ∈N, j ̸= k, holds fast to (w∗)j.
3. w || wk = w.
4. Diﬀerent equilibria can yield identical outcomes.
5. The bargaining set can also be deﬁned with “threat” strategies in lieu of max-min
(or “security”) strategies as criteria of individual rationality. Context determines which
deﬁnition is apt.
6. If E is a singleton, or if all elements of E yield the same outcome (see Example 7),
then the game is usually regarded as solved.
7. In general, however, E may either be empty or yield a multiplicity of outcomes (see
Example 8).
8. A suﬃcient condition for E ̸= ∅in a ﬁnite game is that information be perfect
(although E need not be computable by all players unless information is also complete).
The above condition is not necessary; see Examples 7 and 8.
9. If E yields a multiplicity of outcomes, then an equilibrium selection criterion is nec-
essary. One criterion is to reduce E to E ∩P ∗, thus preferring cooperative equilibria (of
a noncooperative game) to noncooperative equilibria. Another criterion is to reduce E
to E ∩ES.

Section 16.5
GAME THEORY
1207
10. Rationales for the above criteria are discussed in [Me93]. Other equilibrium selection
criteria are discussed in [Fr90] and [My91].
11. The equilibrium selection problem is one of the important unsolved problems of
game theory; see [BiKiTa93].
Examples:
1. A university (Player 3) must oﬀer a faculty position to either or both of two indi-
viduals, a distinguished researcher (Player 1) and a younger colleague in the same area
(Player 2), each of whom can say either YES or NO to an oﬀer but cannot communicate
with the other. The payoﬀto Player i = 1, 2 (in well-being) is σi (> 0) for an oﬀer,
bi (> σi) for an appointment, and Bi (> bi) if both are appointed. To the university,
hiring Player 1 alone is worth 4 (in prestige); but hiring both merits 3, hiring neither
is worth 2, and hiring Player 2 alone merits zero, because appointing Player 2 prevents
the appointment of another distinguished researcher. The university hides from each
candidate whether it has made an oﬀer to the other. The extensive form of this game
is shown in the following ﬁgure. Each player has a single information set (denoted by a
rectangle). There are no chance vertices and no proper subgames. The payoﬀs to Players
1, 2, and 3 are indicated by the 3-vector at each terminal vertex of the tree.
3
1
σ1
0
2
b1
0
4
0
σ2
2
0
b2
0
σ1
σ2
2
σ1
b2
0
b1
σ2
4
B1
B2
3
1
2
2
2
2. Suppose in Example 1 that the university now reveals to whom it has made an oﬀer.
Also, the university need not oﬀer the position to either candidate this year, in which
case a single individual is appointed next year and chance decides with equal probability
which current candidate the appointee matches in caliber, giving the university a payoﬀ
of 0.5(4) + 0.5(0) = 2. The extensive form of this game is shown in the following ﬁgure.
Player 1 has information sets I, J; Player 2 has information sets K, L. There is a single
chance vertex. Information sets I, J, K each contain the root of a proper subgame.
3. The ﬁgures of Examples 1 and 2 are ﬁnite games of imperfect information since in
both cases Player 2 has an information set with more than one vertex. Each game has
incomplete information if players know only their own terminal payoﬀs.

1208
Chapter 16
DISCRETE OPTIMIZATION
σ1
σ2
2
σ1
b2
0
b1
σ2
4
B1
B2
3
2
2
2
1
3
1
2
0
σ2
2
0
b2
0
σ1
0
2
b1
0
4
0
4
0
0
0
0
0.5
0
0.5
1
I
J
K
L
4. In the ﬁgure of Example 2, Player 2 can say YES or NO at each of K or L. Hence
Player 2 has four possible strategies: YKNL (yes if K, no if L), NKYL, an unconditional
YES, and an unconditional NO. Likewise, Player 1 has four strategies: YINJ, NIYJ,
YES, and NO.
5. The following table depicts the strategic form of the ﬁgure of Example 1 as a 3-
dimensional array.
The strategy sets are S1 = {YES, NO}, S2 = {YES, NO}, and
S3 = {BOTH, 1 ONLY, 2 ONLY}. The payoﬀfunction is deﬁned by f1(YES, NO, 1
ONLY) = b1, f2(NO, NO, 2 ONLY) = σ2, f3(YES, YES, BOTH) = 3, etc. Player 1’s
strategies correspond to rows, Player 2’s strategies correspond to columns, and Player 3’s
strategies correspond to arrays.
YES
NO
YES
NO
YES
NO
YES


B1
B2
3




b1
σ2
4


YES


b1
0
4




b1
0
4


YES


0
b2
0




0
σ2
2


NO


σ1
b2
0




σ1
σ2
2


NO


σ1
0
2




σ1
0
2


NO


0
b2
0




0
σ2
2


BOTH
1 ONLY
2 ONLY
6. The following table depicts the strategic form of the ﬁgure of Example 2 as a 3-
dimensional array. Player 1’s strategies correspond to rows, Player 2’s strategies corre-
spond to columns, and Player 3’s strategies correspond to arrays. The strategy sets now
are S1 = {YES, YINJ, NIYJ, NO}, S2 = {YES, YKNL, NKYL, NO}, and S3 = {BOTH,
1 ONLY, 2 ONLY, NEITHER}. The sets S1, S2 contain more strategies than in Example
5 because Players 1 and 2 have better information: the game is less imperfect. The payoﬀ
to Player 3 from NEITHER is an expectation over arcs emanating from the game’s single
chance vertex.

Section 16.5
GAME THEORY
1209
YES
YKNL
NKYL
NO
YES
YKNL
NKYL
NO
YES


B1
B2
3




b1
σ2
4




B1
B2
3




b1
σ2
4


YES


b1
0
4




b1
0
4




b1
0
4




b1
0
4


YINJ


B1
B2
3




b1
σ2
4




B1
B2
3




b1
σ2
4


YINJ


σ1
0
2




σ1
0
2




σ1
0
2




σ1
0
2


NIYJ


σ1
b2
0




σ1
σ2
2




σ1
b2
0




σ1
σ2
2


NIYJ


b1
0
4




b1
0
4




b1
0
4




b1
0
4


NO


σ1
b2
0




σ1
σ2
2




σ1
b2
0




σ1
σ2
2


NO


σ1
0
2




σ1
0
2




σ1
0
2




σ1
0
2


BOTH
1 ONLY
YES
YKNL
NKYL
NO
YES
YKNL
NKYL
NO
YES


0
b2
0




0
b2
0




0
σ2
2




0
σ2
2


YES


0
0
2




0
0
2




0
0
2




0
0
2


YINJ


0
b2
0




0
b2
0




0
σ2
2




0
σ2
2


YINJ


0
0
2




0
0
2




0
0
2




0
0
2


NIYJ


0
b2
0




0
b2
0




0
σ2
2




0
σ2
2


NIYJ


0
0
2




0
0
2




0
0
2




0
0
2


NO


0
b2
0




0
b2
0




0
σ2
2




0
σ2
2


NO


0
0
2




0
0
2




0
0
2




0
0
2


2 ONLY
NEITHER
7. {YES, YES, 1 ONLY} and {YES, NO, 1 ONLY} are the equilibria of Example 1;
both yield the same outcome, namely, Player 1 is hired without an oﬀer to Player 2.
8. Example 2 has 14 equilibria: namely, (YINJ, YES, BOTH), (YINJ, NKYL, BOTH),
and all strategy combinations of the form (YES, · , 1 ONLY), (NIYJ, · , 1 ONLY), or
(NO, · , NEITHER), where · denotes any of the four strategies of Player 2. Eight of these
14 equilibria correspond to the equilibrium outcome of Example 1, whereas the other six
correspond to two diﬀerent outcomes.
9. For Example 1, m3(BOTH) = 0 = m3(2 ONLY) and m3(1 ONLY) = 2, implying
ew3 = 1 ONLY and ef3 = 2. For k ≤2, mk(YES) = 0 = mk(NO), implying efk = 0. So
D∗= D −{(YES, YES, 2 ONLY), (NO, YES, 2 ONLY), (NO, YES, BOTH)}.

1210
Chapter 16
DISCRETE OPTIMIZATION
10. For Example 1, P = {(YES, YES, BOTH), (YES, NO, BOTH)} = P ∗.
11. In Example 2, the equilibria (YINJ, YES, BOTH) and (YINJ, NKYL, BOTH) are
not subgame perfect because in the subgame beginning at J they would require Player 1
to say NO, which would be irrational. (Player 1’s threat to say NO, unless Player 3
makes an oﬀer to BOTH, is not credible because Player 3 has a ﬁrst mover advantage.)
12. While reducing E to E ∩P ∗eliminates equilibria of the form (NO, · , NEITHER)
in Example 2, it is also possible that E and P are disjoint (as in Example 1).
13. While reducing E to E ∩ES eliminates equilibria of the form (YINJ, YES, BOTH)
and (YINJ, NKYL, BOTH) in Example 2, it is also possible that E = ES (as in Exam-
ple 1, where there are no proper subgames).
16.5.2
MATRIX AND BIMATRIX GAMES
This subsection discusses two-player noncooperative games. Such games can be repre-
sented in normal form by a pair of matrices.
Deﬁnitions:
Suppose S1 = {1, . . . , r} and S2 = {1, . . ., s}.
The r×s payoﬀmatrices A = (aij) and B = (bij), with aij = f1(i, j) and bij = f2(i, j),
deﬁne a bimatrix game.
The game is zero-sum if aij + bij = 0 for all i ∈S1, j ∈S2. The game is symmetric if
r = s and B = AT . In either case, the game is completely determined by A and is called
a matrix game.
For Player 1, i ∈S1 is dominated by i′ ∈S1 if ai′j ≥aij for all j ∈S2, with strict
inequality for at least one j. For Player 2, j ∈S2 is dominated by j′ ∈S2 if bij′ ≥bij
for all i ∈S1 with strict inequality for at least one i.
Let ek denote the k-dimensional vector in which every entry is 1, and let Xk denote the
(k−1)-dimensional unit simplex Xk = {(x1, . . . , xk) | xek = 1, x ≥0}.
A mixed strategy for Player 1 is a vector p = (p1, . . . , pr) ∈Xr, where pi is the
probability that Player 1 selects i ∈S1.
Similarly, a mixed strategy for Player 2 is
q = (q1, . . . , qs) ∈Xs, where qj is the probability that Player 2 selects j ∈S2.
In a mixed strategy combination (p, q) ∈Xr × Xs, the expected payoﬀs to Players 1
and 2, respectively, are given by φ1(p, q) = pAqT and φ2(p, q) = pBqT .
The pair (p∗, q∗) ∈Xr × Xs is a Nash equilibrium mixed strategy combination,
or simply an equilibrium in mixed strategies, if φ1(p∗, q∗) ≥φ1(p, q∗) for all p ∈Xr
and φ2(p∗, q∗) ≥φ2(p∗, q) for all q ∈Xs. If the game is zero-sum, then p∗is called an
optimal strategy for Player 1 and q∗is called an optimal strategy for Player 2.
Facts:
1. Every bimatrix game has at least one equilibrium in mixed strategies.
2. All equilibria in mixed strategies of a zero-sum game yield the same expected payoﬀs,
v to Player 1 and −v to Player 2; v is known as the value of the game.
3. The value v of a zero-sum game and a pair (p∗, q∗) of optimal strategies can always
be computed by solving a dual pair of linear programming (LP) problems (§16.1). The

Section 16.5
GAME THEORY
1211
primal LP problem ﬁnds p to maximize v subject to AT p ≥v1s, p ∈Xr, whereas the
dual LP problem ﬁnds q to minimize v subject to Aq ≤v1r, q ∈Xs.
4. Player 1 can achieve the value v of a zero-sum game with a mixed strategy that
attaches zero probability to any dominated pure strategy. Likewise, Player 2 can achieve
value −v by playing dominated pure strategies with zero probability.
5. Graphical methods can be used to compute eﬃciently all equilibria of zero-sum games
where r = 2 or s = 2, or of matrix games (of either type) where r = s = 3; see [Dr81],
[Ow13], and [Pe08]. There is no general method for computing all equilibria.
6. The deﬁnition of mixed strategy and the existence of equilibria are readily extended
to n-player games. This result was one of the fundamental contributions to game theory
for which John Nash was awarded the 1994 Nobel Prize in Economic Science.
Examples:
1. Two advertising agencies are involved in a campaign to promote competing beverages.
The payoﬀs of various promotional strategies are shown in the following table.
j
1
2
i
old
new
1
old
0
−2
2
new
−2
−1
3
diet
3
−3
The promotional strategies for the ﬁrst agency are: stress the old formula, advertise a
new formula, or advertise a diet drink. The second agency has the possible strategies:
stress the old formula, or advertise a new formula. The payoﬀs in this case indicate the
net change in millions of sales gained (by Advertiser 1). For example, if the ﬁrst agency
promotes a diet drink while the other agency promotes the old formula, three million
more drinks will be sold. On the other hand, if the other agency happens to promote
the new formula, then the ﬁrst agency will end up losing three million unit sales to the
second agency.
2. An investor has just taken possession of jewels worth $45,000 and must store them
for the night in one of two locations (A, B). The safe in location A is relatively secure,
with a probability
1
15 of being opened by a thief. The safe at location B is not as secure
as the safe at location A, and has a probability 1
5 of being opened. A notorious thief
is aware of the jewels, but doesn’t know where they will be stored. Nor is it possible
for the thief to visit both locations in one evening.
This is a (symmetric) zero-sum
game between the investor (Player 1), who selects where to keep the jewels and the thief
(Player 2), who decides which safe to try. If the investor puts the jewels in the most
secure location (A) and the jewel thief goes to this location, the expected loss in this case
is
1
15(−45,000) + 14
15(0) = −3,000. The other entries of the payoﬀmatrix in the following
table are computed similarly, and are expressed in thousands of dollars (to the investor).
j
1
2
i
old
new
A
B
1
A
−3
0
2
B
0
−9
No pure strategy combination is a Nash equilibrium, since it is always tempting for one
player to defect from the current strategy. However, there is a Nash equilibrium mixed

1212
Chapter 16
DISCRETE OPTIMIZATION
strategy combination: p∗= ( 3
4, 1
4) = q∗, with value v = −$2,250 to the investor. The
mixed strategy p∗is found by solving the following linear program, in which Player 1
wants to ﬁnd the largest value of v so that he is guaranteed to receive at least v (regardless
of what Player 2 does). The associated optimal dual LP solution gives q∗.
maximize:
v
subject to:
−3p1 + 0p2 ≥v
0p1 −9p2 ≥v
p1 + p2 = 1
p1, p2 ≥0
3. The zero-sum game of chump is played between two camels, a dromedary (Player 1)
and a bactrian (Player 2).
Player k must simultaneously ﬂash Fk humps and guess
that its opponent will ﬂash Gk. Possible strategies (Fk, Gk) satisfy 0 ≤F1, G2 ≤1 and
0 ≤F2, G1 ≤2. If both players are right or wrong, then the game is a draw; if one is
wrong and the other is right, then the ﬁrst pays F1 + F2 piasters to the second. The
following table shows the strategy sets and corresponding payoﬀs aij to Player 1.
j
1
2
3
4
5
i
(0, 0)
(0, 1)
(1, 0)
(1, 1)
(2, 0)
(2, 1)
(0, 0)
0
0
−1
0
−2
0
1
(0, 1)
0
0
0
1
−2
0
2
(0, 2)
0
0
−1
0
0
2
3
(1, 0)
1
0
0
−2
0
−3
4
(1, 1)
0
−1
2
0
0
−3
5
(1, 2)
0
−1
0
−2
3
0
The ﬁrst row and column can be deleted from the full payoﬀmatrix because (0, 0) is
dominated by (0, 1) for both players (Fact 4). Thus it suﬃces to analyze the reduced
payoﬀmatrix in which r = s = 5. The value of the game is −4
21 for Player 1 ( 4
21 for
Player 2). Optimal strategies p∗= ( 2
21, 4
7, 1
7, 4
21, 0) and q∗= ( 4
7, 4
21, 2
21, 1
7, 0) are found by
linear programming (Fact 3). Note that strategies i = 5 and j = 5 have zero probability
at this equilibrium, despite being undominated.
4. The symmetric game of four ways [Me01] is played by two left-turning motorists who
arrive simultaneously from opposite directions at a 4-way junction. Each has three pure
strategies: the ﬁrst is to go, the second to wait, and the third a conditional strategy
of going only if the other appears to be waiting. It takes 2 seconds for one motorist
to cross the junction while the other waits.
If initially both either go or wait, then
both motorists incur an extra “posturing” delay of either 3 or 2 seconds, respectively.
Also, the one who ultimately waits is equally likely to be either player. For example,
a11 = 0.5 × (−3 −2) + 0.5 × (−3) = −4 and a22 = 0.5 × (−2 −2) + 0.5 × (−2) = −3.
This game has the payoﬀmatrix



−4
0
0
−2
−3
−2
−2
0
−4


.
There are inﬁnitely many equilibria in mixed strategies; these are described in the fol-
lowing table, where 0 ≤a ≤1 and 1
2 ≤b ≤1.

Section 16.5
GAME THEORY
1213
p∗
q∗
(1, 0, 0)
(0, a, 1 −a)
(0, a, 1 −a)
(1, 0, 0)
(0, 1, 0)
(b, 0, 1 −b)
(b, 0, 1 −b)
(0, 1, 0)
1
11(6, 2, 3)
1
11(6, 2, 3)
16.5.3
CHARACTERISTIC-FUNCTION GAMES
When there exists a binding agreement among all players to cooperate, attention shifts
from strategies to the bargaining strengths of coalitions. These strengths are assumed to
be measured in terms of a freely transferable beneﬁt (e.g., money or time) and players
are assumed to seek a fair distribution of the total beneﬁt available. Also, without loss
of generality, the beneﬁt of cooperation will be taken as the savings in costs.
Deﬁnitions:
A coalition is a subset S of N = {1, . . . , n}; equivalently, S ∈2N.
The cost associated with coalition S is denoted c(S).
Let R+ denote the set of nonnegative reals. The characteristic function ν : 2N →R+
assigns to each S its cooperative beneﬁt, using ν(S) = max {0, P
i∈S
c({i}) −c(S)}.
A characteristic-function game, or c-game, is the pair Γ = (N, ν).
The game Γ is inessential if ν(N) = 0. If ν(N) > 0 then the game is essential, with
normalized characteristic function ν : 2N →[0, 1] deﬁned by ν(S) = ν(S)
ν(N).
The game Γ is convex if ν(S ∪T ) ≥ν(S) + ν(T ) −ν(S ∩T ) for all S, T ∈2N.
Let X = Xn be the (n−1)-dimensional unit simplex (§16.5.2). Any x ∈X is called an
imputation; it allocates xi of the total normalized beneﬁt ν(N) = 1 to Player i. An
imputation is unreasonable if it allocates more to some i ∈N than the maximum that
i could contribute to any coalition T −{i} by joining it.
The reasonable set is XRS = {x ∈X | xi ≤max
T [ν(T ) −ν(T −{i})] for all i ∈N}.
For any x ∈X and S ∈2N, the excess of coalition S at x is e(S, x) = ν(S) −P
i∈S
xi.
The core of Γ is C = {x ∈X | e(S, x) ≤0 for all S ∈2N}.
The marginal worth of Player i to the coalition T −{i} is ν(T ) −ν(T −{i}).
The Shapley value of a c-game is the imputation xS = (xS
1 , xS
2 , . . . , xS
n) deﬁned by
xS
i = 1
n!
P
T ∈Πi(|T | −1)! (n −|T |)! (ν(T ) −ν(T −{i})), where Πi = {T ∈2N | T ⊇{i}}.
Facts:
1. An imputation is both individually rational and group rational (see §16.5.1).
2. Convexity is a suﬃcient (but not necessary) condition for the core to exist.
3. If C ̸= ∅then C ⊆XRS.

1214
Chapter 16
DISCRETE OPTIMIZATION
4. If C contains a single imputation, then the c-game is usually regarded as solved.
5. In general, C may either be empty (see Example 1) or contain inﬁnitely many impu-
tations (see Example 2).
6. If C contains inﬁnitely many imputations, then there are several ways to single one
out as the solution to the c-game. One approach is to deﬁne a “center” of C, which leads
to the important concept of the nucleolus [Me01].
7. Every c-game solution concept assumes that players have agreed to enter coalition N.
If its order of formation were known, players could be allocated their marginal worths;
in general, however, this order of formation (and hence marginal worth) is a random
variable.
8. If all orders of formation of N are equally likely, then the probability that Player i
enters N by joining the coalition T −{i} is (|T |−1)!(n−|T |)!
n!
.
9. The Shapley value distinguishes a single imputation as the solution of a c-game
by allocating to players the expected values of their marginal worths, based on the
assumption that all orders of formation of N are equally likely.
10. xS ∈XRS.
11. xS ∈C if Γ is convex.
Examples:
1. In the c-game log-hauling [Me01], three lone drivers of pickup trucks discover a pile
of 150 logs too heavy for any one to lift. Players 1, 2, and 3 can haul up to 45, 60,
and 75 logs, respectively. Thus ν({1, 2}) = 105, ν({1, 3}) = 120, ν({2, 3}) = 135, and
ν({1, 2, 3}) = 150 so that ν({1, 2}) =
7
10, ν({1, 3}) = 4
5, and ν({2, 3}) =
9
10. This c-
game is not convex; for example, if S = {1, 2} and T = {2, 3}, then 1 = ν(S ∪T ) <
ν(S) + ν(T ) −ν(S ∩T ) = 8
5. Also, C = ∅.
2. The c-game car pool [Me01] is played by three co-workers whose oﬃce is d miles from
their residential neighborhood, shown in the following ﬁgure.
d miles
1 mile
1
office
2
3

Section 16.5
GAME THEORY
1215
Driving to work costs $k per mile, and the shortest route is always used. The beneﬁt
of cooperation is car pool savings, giving the characteristic function displayed in the
following table.
S
c(S)
ν(S)
ν(S)
e(S, x) for d = 1
∅
0
0
0
0
{1}
(4 + d)k
0
0
−x1
{2}
(3 + d)k
0
0
−x2
{3}
(3 + d)k
0
0
x1 + x2 −1
{1, 2}
(4 + d)k
(3 + d)k
3+d
3+2d
4
5 −x1 −x2
{1, 3}
(6 + d)k
(1 + d)k
1+d
3+2d
x2 −3
5
{2, 3}
(6 + d)k
dk
d
3+2d
x1 −4
5
{1, 2, 3}
(7 + d)k
(3 + 2d)k
1
0
Because x3 = 1 −x1 −x2 (≥0), a set of imputations is determined by its projection onto
x3 = 0. In these terms, for d = 1, X is the largest triangle in the following ﬁgure, XRS
is the shaded hexagon, and C is the shaded quadrilateral. Here C ⊂XRS ⊂X because
the c-game is convex (for all d ≥0).
1.0
0.5
1.0
0.5
x1
x2
3. For the c-game in Example 2, it is easy enough to locate a center for C; see the ﬁgure
for Example 2, where the nucleolus is marked by a dot.
4. In Example 1, the six possible formation orders of N are 123, 132, 213, 231, 312, 321.
Thus, the Shapley value is the imputation xS = ( 17
60, 1
3, 23
60); see the following table.

1216
Chapter 16
DISCRETE OPTIMIZATION
i
T ∈Πi
ν(T ) −ν(T −{i})
probability i enters N
xS
i
by joining T −{i}
1
{1}
0
1
3
17
60
{1, 2}
7
10
1
6
{1, 3}
4
5
1
6
{1, 2, 3}
1
10
1
3
2
{2}
0
1
3
1
3
{1, 2}
7
10
1
6
{2, 3}
9
10
1
6
{1, 2, 3}
1
5
1
3
3
{3}
0
1
3
23
60
{1, 3}
4
5
1
6
{2, 3}
9
10
1
6
{1, 2, 3}
3
10
1
3
5. By a calculation very similar to that laid out in the table of Example 4, the Shapley
value for Example 2 is the imputation xS = ( 7
15, 11
30, 1
6). Because the c-game is convex,
xS ∈C. This is illustrated in the previous ﬁgure, where xS is marked by a cross.
16.5.4
APPLICATIONS
Discrete (noncooperative or characteristic-function) games have numerous applications
and merge with other categories of games not examined here.
The references in the
following table provide sources for the deﬁnitions, concepts, and applications of such
games. This table also lists some representative areas of application of game theory.
category and references
selected applications
remarks
characteristic function
airport landing fees,
utility is usually assumed to be
games [GoGaFi10], [Me93],
voting, water resour-
transferable: in essence, play-
[Me01], [Ow13], [Pe08], [Wa88]
ces
ers value beneﬁts identically
classical game theory
microeconomics, par-
economic (as opposed to evolu-
[LuRa57], [vNMo53]
lor games
tionary) game theory
combinatorial games
chess, go, nim, other
two players; complete, perfect
[AlNoWo07], [Gu91]
parlor games
information; no chance moves;
zero-sum
continuous games
duels, military com-
a discrete game with mixed
[Dr81], [Fr90]
bat, oligopoly theory
strategies is a special case of
a continuous game
cooperative games in strategic
wage bargaining,
agreements among players are
form (as opposed to c-games)
motoring behavior
binding
[Fr90], [Me01], [Pe08]
diﬀerential games
resource
extension of optimal control
[BaHa94], [DoEtal00], [Me93]
management
theory

Section 16.6
SPERNER’S LEMMA AND FIXED POINTS
1217
category and references
selected applications
remarks
economic game theory
microeconomics
equilibria are the result of ra-
[Fr90], [GoGaFi10], [My91]
tional thought processes
evolutionary game theory
animal behavior
equilibria are the result of nat-
[BrRy13], [Cr03], [Ma82],
ural selection or equivalent
[Me01], [Sa10], [Si10]
populational processes
iterated games
rationality of cooper-
often inﬁnitely many iterations
[Fr90], [Me01], [Si10]
ation
resource games
ﬁsheries, forestry, wa-
discrete, continuous, and diﬀer-
[Me93]
ter resources
erential games are all used
symmetric matrix games
evolutionary game
dynamical systems theory pro-
[BrRy13], [Cr03], [Ma82],
theory
vides a rationale for strategic
[Me01], [Sa10], [Si10]
equilibrium
zero-sum matrix games
military science
[Dr81], [Pe08]
16.6
SPERNER’S LEMMA AND FIXED POINTS
A ﬁxed point of a function from a set X to itself is a point of X that is mapped into
itself. Brouwer (1912) proved that every continuous mapping f on the unit ball has a
ﬁxed point. Sperner (1928) gave an elegant proof of Brouwer’s ﬁxed-point theorem using
a combinatorial lemma known today as Sperner’s lemma. This lemma has a number of
applications to economics, nonlinear programming, and game theory.
16.6.1
SPERNER’S LEMMA
Sperner’s lemma is a combinatorial result applicable to certain triangulations of a p-
dimensional convex set, in which the vertices of the triangulation are given labels from
{1, 2, . . ., p + 1}.
Deﬁnitions:
The p + 1 points x1, x2, . . . , xp+1 ∈Rn are said to be in general position if the vectors
x2−x1, x3−x1, . . . , xp+1−x1 are linearly independent (§6.1.3).
The set C ⊆Rn is convex if for all x, y ∈C and 0 ≤λ ≤1, λx + (1 −λ)y ∈C.
The convex hull of a ﬁnite set of points v1, . . . , vp+1 ∈Rn is the set ⟨v1, . . . , vp+1⟩=
p+1
P
i=1
λivi

p+1
P
i=1
λi = 1, λi ≥0
	
.
A p-simplex σ is the convex hull of p + 1 points x1, . . . , xp+1 ∈Rn in general position.
The vertices of the p-simplex σ = ⟨x1, . . . , xp+1⟩are the points x1, . . . , xp+1.
The
face τ = ⟨xj1, . . . , xjk⟩of σ is the simplex spanned by the subset {xj1, . . . , xjk} of
{x1, . . . , xp+1}. Write τ ≺σ when τ is a face of σ.

1218
Chapter 16
DISCRETE OPTIMIZATION
A simplicial complex K is a collection of simplices satisfying
• if σ ∈K and τ ≺σ then τ ∈K;
• if σ, τ ∈K intersect, their intersection is a face of each.
The p-skeleton of a simplicial complex K is the set of all simplices of dimension p or
less. The 0-skeleton is the vertex set, denoted V (K).
A simplicial subdivision F of a simplex σ is a collection of simplices {τj | 1 ≤j ≤m}
satisfying
• σ = Sm
j=1 τj;
• the intersection of any two τj is either empty or a face of each.
A simplicial subdivision F′ of a simplicial complex K is a reﬁnement of the simplicial
subdivision F of K if every simplex of F is a union of simplices of F′.
Given a simplicial subdivision F of the p-simplex σ = ⟨x1, . . . , xp+1⟩, a proper labeling
of F is a mapping ℓ: V (F) →{1, 2, . . ., p + 1} satisfying
• ℓ(xm) = m for m = 1, . . . , p + 1;
• if vertex v lies on a face ⟨xk1, . . . , xkq⟩of σ, then ℓ(v) ∈{k1, . . . , kq}.
Here {1, 2, . . ., p + 1} is the label set, and if ℓ(v) = k then v receives the label k.
A distinguished simplex is a p-simplex that receives all p + 1 labels 1 through p + 1.
Facts:
1. The convex hull ⟨v1, . . . , vp+1⟩is the intersection of all convex sets containing the
points v1, . . . , vp+1.
2. The dimension of any p-simplex is p.
3. A p-simplex contains 2p+1 −1 simplices of dimension p or less.
4. Sperner’s lemma (1928):
Every properly labeled subdivision of a simplex σ has an
odd number of distinguished simplices. (E. Sperner, 1906–1980)
5. Algorithm 1 gives a method for ﬁnding a distinguished triangle in a properly labeled
subdivision of a triangle T . Each iteration of the outer loop starts at a distinguished
1-simplex and traces out a path, terminating either at a distinguished 2-simplex or at an
outer edge of T .
Algorithm 1:
Distinguished simplex of a 2-simplex.
input: properly labeled subdivision of triangle T
output: a distinguished triangle of T
{Outer loop}
ﬁnd a distinguished 1-simplex τ along the bottom of T
{Inner loop}
repeat
if the unique triangle containing τ is distinguished then stop
else proceed to a neighboring triangle whose common edge is distinguished
until either a distinguished triangle is found or the search leads to the bottom
edge of T
continue outer loop with a new distinguished 1-simplex τ

Section 16.6
SPERNER’S LEMMA AND FIXED POINTS
1219
6. Since there is an odd number of distinguished 1-simplices along the bottom of T
(Fact 4) and since each “failed” outer loop iteration produces a path joining two such
distinguished 1-simplices, Algorithm 1 must eventually produce a path terminating at a
distinguished 2-simplex.
Examples:
1. A 0-simplex is a point, a 1-simplex is a line segment, and a 2-simplex is a triangle
(interior included).
A 3-simplex includes the vertices, edges, faces, and interior of a
tetrahedron. See the following ﬁgure.
0-simplex
1-simplex
2-simplex
3-simplex
2. The 0-skeleton of a simplex σ is its vertex set; the 1-skeleton of σ is the edge set of σ
including their endpoints; if σ is a 3-simplex, the 2-skeleton is the union of the faces of σ.
3. Part (a) of the following ﬁgure shows a simplicial subdivision of a 2-simplex. The
subdivision in part (b) of the ﬁgure is not simplicial because τ1 ∩τ3 is not a face of the
simplex τ3.
(a)
(b)
τ 2
τ1
τ3
4. The following ﬁgure shows a proper labeling of a simplicial subdivision of a 1-simplex.
A distinguished 1-simplex is a subinterval that receives both the labels 1 and 2. In this
example, there are ﬁve such 1-simplices, an odd number (as guaranteed by Fact 4).
1
2
1
1
2
2
2
1
5. The following ﬁgure shows a proper labeling of a simplicial subdivision of a 2-simplex.
There is one distinguished 2-simplex, receiving all three labels, which is shown shaded
in the ﬁgure. If the vertex in the interior of the triangle is instead labeled 3, then there
will be three distinguished 2-simplices, still an odd number.

1220
Chapter 16
DISCRETE OPTIMIZATION
3
3
2
2
2
1
1
1
1
1
6. Several possible paths from executing Algorithm 1 are displayed in the following
ﬁgure. The rightmost path terminates in a bottom edge, while the leftmost path leads to
a distinguished triangle. Note that there are three distinguished triangles in this example,
an odd number as required by Sperner’s lemma.
1
1
1
1
1
1
3
2
3
2
2
2
2
2
2
2
2
1
1
1
2
2
2
2
2
2
1
1
Start
Start
16.6.2
FIXED-POINT THEOREMS
Fixed-point theorems have applicability to a number of problems in economics, as well
as to game theory and optimization.
Deﬁnitions:
The point x ∈B is a ﬁxed point of the mapping f : B →B if f(x) = x.
The mapping f deﬁned on a subset X of a normed space B is a contraction if there is
some 0 ≤β < 1 such that ∥f(x) −f(y)∥≤β∥x −y∥for all x, y ∈X.
The function F is a set mapping on X if F(x) is a nonempty subset of X for all x ∈X.
The set mapping F is convex if F(x) is a convex subset of X for all x ∈X.
Facts:
1. Fixed-point theorems can be used to demonstrate the existence of economic equilibria,
solutions to a system of nonlinear equations, and Nash equilibria in two-person nonzero-
sum games.
2. Brouwer’s ﬁxed-point theorem I: Every continuous mapping f : σ →σ where σ is a
p-simplex has a ﬁxed point. (L. E. J. Brouwer, 1881–1966)

Section 16.6
SPERNER’S LEMMA AND FIXED POINTS
1221
3. For a simplex σ = ⟨x1, x2, . . . , xp+1⟩and a continuous mapping f : σ →σ, let
f(Pp+1
k=1 λkxk) = Pp+1
k=1 µkxk. Then Pp+1
k=1 µk = Pp+1
k=1 λk = 1.
4. Relative to the mapping f : σ →σ, deﬁne Tj = {Pp+1
k=1 λkxk | µj ≤λj}. Then a ﬁxed
point of f is any point belonging to T p+1
j=1 Tj.
5. Suppose an interior vertex v of a subdivision F of σ is labeled with j provided that
v ∈Tj, and suppose a vertex v belonging to a face ⟨xk1, xk2, . . . , xkt⟩is labeled with any
one of the labels k1, k2 . . . , kt. Then a ﬁxed point of f occurs in a distinguished simplex
of σ.
6. Algorithm 2, based on Sperner’s lemma (§16.6.1), produces a sequence of points
converging to a ﬁxed point of a p-simplex σ.
Algorithm 2:
Fixed point of a p-simplex.
input: function f deﬁned on a p-simplex σ
output: ﬁxed point x∗∈σ
construct a sequence of subdivisions {Fn | n ≥1} such that Fn+1 reﬁnes Fn
label the vertex set of Fn as in Fact 5
for each subdivision Fn ﬁnd a distinguished simplex τn
T τn contains the desired ﬁxed point x∗
7. Brouwer’s ﬁxed-point theorem II: Every continuous mapping from a convex compact
set B ⊆Rn into itself has a ﬁxed point.
8. Contraction mapping theorem: Every contraction f : X →X has a ﬁxed point. The
ﬁxed point is the limit of the sequence {f(xn) | n ≥0}, where x0 is an arbitrary element
of X and xn+1 = f(xn).
9. Kakutani’s ﬁxed-point theorem: Let X ⊆Rn be a convex compact set and suppose
that F is a convex mapping on X. If the graph {(x, y) | y ∈F(x)} ⊆R2n is closed, then
there exists a point x∗∈X such that x∗∈F(x∗).
10. Schauder’s ﬁxed-point theorem: Every continuous mapping f on a convex compact
subset X in a normed space B has a ﬁxed point.
11. Reference [Bo89] gives applications of ﬁxed-point theorems to determining market
equilibria, maximal elements of binary relations, solutions to complementarity problems,
as well as solutions to various types of games (cooperative and noncooperative).
Examples:
1. The real-valued function f(x) = 1 −x is a mapping from the 1-simplex σ = [0, 1] to
itself. It is not a contraction since |f(x)−f(y)| = |(1 −x)−(1 −y)| = 1 ·|x−y| holds for
all x, y ∈σ so β ≥1. The function f has a ﬁxed point at x = 1
2. However, the iterative
procedure in Fact 8 will not generally locate this ﬁxed point. For example, using x0 = 1
4
produces the sequence x1 = f(x0) = 3
4, x2 = f(x1) = 1
4, x3 = f(x2) = 3
4, and so forth,
with no limiting value.
2. The following ﬁgure shows a real-valued function f : σ →σ deﬁned over the 1-simplex
σ = [0, 1]. This function has three ﬁxed points, identiﬁed by the intersection of the graph
of f with the dashed line y = x. The sets Tj of Fact 4 relative to x1 = 0, x2 = 1 are also
indicated in the ﬁgure, and it is veriﬁed that T1∩T2 contains the three ﬁxed points of f. A
subdivision of σ into ﬁve subintervals is shown in the ﬁgure. Using Fact 5, the associated
vertices (at x = 0.0, 0.2, 0.4, 0.6, 0.8, 1.0) receive the labels 1, 2, 1, 1, 2, 2 respectively, and
so there are three distinguished simplices (each containing a ﬁxed point).

1222
Chapter 16
DISCRETE OPTIMIZATION
T2
T1
0
0.2
0.4
0.6
0.8
1.0
1.0
0
3. The real-valued function f(x) =
1
4+x2 is a mapping from R to itself. It can also be
shown to be a contraction mapping with β =
1
16 < 1. If the iterative procedure in Fact 8
is applied using x0 = 1, then x1 = 0.2, x2 = 0.24752, x3 = 0.24623, x4 = 0.24627, and
x5 = 0.24627, yielding the (approximate) ﬁxed point x∗= 0.24627.
4. Perron’s theorem:
This theorem (§6.5.5), which assures that every positive matrix
has a positive eigenvalue-eigenvector pair, can be proved using the ﬁxed-point theorem
in Fact 2. Let A = (aij) be an n × n matrix, with all aij > 0. The set σ = {x ∈
Rn | Pn
k=1 xk = 1, xk ≥0} is an (n−1)-simplex, and the continuous function deﬁned
by f(x) =
Ax
∥Ax∥1 maps σ into itself. (Here ∥w∥1 is the l1 norm of vector w; see §6.4.5.)
By Fact 2, f has a ﬁxed point ¯x, so that A¯x = ∥A¯x∥1¯x. Since at least one component
of ¯x is positive and A is positive, the vector A¯x has positive components. It then follows
that the eigenvalue ∥A¯x∥1 is positive and that the corresponding eigenvector ¯x has all
positive components.
16.7
COMBINATORIAL AUCTIONS
A combinatorial auction (CA) describes a market with several items for sale in which
bidders make bids on combinations of items rather than individual items. The speciﬁed
combination is either won in its entirety or the bid is not accepted. Although initially
proposed in 1982 by Rassenti, Smith, and Bulﬁn [RaSmBu82], the widespread use of
CAs in a variety of real-world settings began only later, in the early 21st century. The
most prominent CAs have occurred in industrial procurement and in the sale of radio
spectrum licenses to telecommunication companies (with individual auctions in some in-
stances generating billions of dollars in revenue). The following subsections discuss basic
concepts as well as four issues arising in combinatorial auctions: winner determination,
bid language, incentive compatibility, and demand revelation. A few suggested exercises
are included sporadically.

Section 16.7
COMBINATORIAL AUCTIONS
1223
16.7.1
BASIC CONCEPTS
Deﬁnitions:
A combinatorial auction (CA) describes a situation in which multiple items are auc-
tioned, and each bidder can submit several bids (each may contain several items). The
set of bidders is I = {1, 2, . . . , i, . . . , |I|}, the set of bids is J = {1, 2, . . ., j, . . . , |J|},
and the set of items for sale is K = {1, 2, . . ., k, . . . , |K|}. In addition, there is a ﬁxed
supply ck ≥1 of identical, indivisible copies of item k in the auction.
Each bid j consists of a monetary bid amount bj and an associated (unique) bidder ij.
An integer quantity akj of each item k must be received by bidder ij at the conclusion
of the auction for bid j to be considered accepted.
Bidder i’s valuation for a set of items S is denoted vi(S).
Bidder i’s bid on a set of items S is denoted bi(S).
A forward auction is one with several buyers and one seller, in contrast to a reverse
auction having several sellers and one buyer.
Except where explicitly noted, this section will focus on forward CAs.
Facts:
1. CAs are also sometimes referred to as bundle auctions or package auctions.
2. A market using combinatorial bids among several buyers and several sellers is called a
combinatorial exchange (CE), or a combinatorial double auction. These two-way markets
have diﬀerent theoretical properties and are less frequently discussed in the literature or
used in practice.
3. It is convenient to write the (row) vector of all bid amounts as b and the (column)
vector of available supplies as c. Also the |K| × |J| matrix A = (akj), indexed by items
and bids, indicates the quantity of item k speciﬁed in bid j.
4. For small numerical examples with unique items (i.e., ck = 1 for all k), capital letters
are often used as item names. For convenience, set braces and commas can be dropped
in the expression for a bidder’s valuation; e.g., bidder i’s valuation for items A and B
will be written vi(AB) rather than as vi({A, B}).
5. The valuation function is typically assumed to be weakly monotonic: i.e., v(T ) ≥
v(S), for all S ⊆T . This condition is usually called free disposal.
6. Bidders are often assumed to have an overall utility that is quasi-linear in money:
i.e., receiving the set of items S and paying Pi dollars provides the net utility ui(S) =
vi(S) −Pi to bidder i.
Examples:
1. Government procurement: The Chilean government procures half a billion dollars in
meal services annually using thousands of package bids [KiOlWe14]. Bus route services
in London have been bought via a CA with a ﬁrst-price rule (i.e., pay-as-bid) [CaPe06].
2. Industrial procurement:
Using expressive commerce software platforms based on
combinatorial procurement auctions, the ﬁrm CombineNet conducted $35 billion in rev-
enue in its ﬁrst 447 business-to-business auctions over ﬁve years. Auction items included
direct goods (e.g., food ingredients, steel, chemicals, ﬁbers), indirect goods (e.g., oﬃce
supplies, furnishing, computers), and services (e.g., transportation, consulting, advertis-
ing) [Sa13].

1224
Chapter 16
DISCRETE OPTIMIZATION
3. Spectrum allocation:
Using a combinatorial clock auction (CCA) format, which
combines iterative bidding with a ﬁnal sealed-bid auction, governments have auctioned
the rights to use telecommunications frequencies around the world, including those in the
United Kingdom, the Netherlands, Denmark, Ireland, Switzerland, Australia, Austria,
Slovenia, Canada, and Slovakia. Revenues were around $19.5 billion for just ten auctions
conducted in 2012 through 2014.
4. Airport arrival and departure slots: The FAA adopted a core-selecting combinatorial
auction to allocate airport landing slots in 2008, but the auction was canceled for political
reasons [CoCoOt09].
5. Electricity auctions: In the U.S. and Europe, regional wholesale markets for electri-
cal power generation are governed by a complex sequence of contracts sold at auction,
including long-term futures contracts for generation, “day-ahead” markets, and spot
markets, some of which include CA concepts in their designs [Sh12].
6. Oﬀ-shore wind energy tracts:
To facilitate the development of wind farms on the
U.S. Outer Continental Shelf, CAs were developed that combine both price and non-price
attributes into a multi-phase CA design [AuCr11].
7. Real estate auctions:
A new multistory building in Amsterdam used a CA to sell
a mixture of residential, oﬃce, dining (etc.) space, subject to municipal constraints, in
order to maximize rent [GoEtAl14].
8. Suppose we have a set of |K| = 3 unique items {A, B, C} for sale, with |J| = 5 bids
from |I| = 5 bidders. (When a bidder bids on only one bundle, she is called single-
minded.) Bidders submit bids b1(AB) = 15, b2(C) = 7, b3(AC) = 12, b4(B) = 8, and
b5(ABC) = 16. Since there are unique items, c = (1, 1, 1)T. Bid amounts are collected
as b = (15, 7, 12, 8, 16) with the bundle structure yielding
A =



1
0
1
0
1
1
0
0
1
1
0
1
1
0
1


.
16.7.2
WINNER DETERMINATION AND BID LANGUAGE
This subsection studies the winner determination problem (WDP), namely, that of ﬁnd-
ing a set of winning bids to maximize total monetary value.
In order to formulate
appropriate optimization problems, it is necessary to deﬁne the underlying bid language
used to communicate bid information.
Deﬁnitions:
The OR bid language is a format for communicating bids in which a bidder submits a
list of combinatorial bids, and the auctioneer is free to accept any subset of them (Bid1
OR Bid2 OR Bid3 . . .), subject to supply constraints over all accepted bids. By contrast,
in the XOR bid language the auctioneer is constrained to accept at most one of the
bids made by the same bidder (Bid1 XOR Bid2 XOR Bid3 . . .).
The |I| × |J| binary matrix B = (bij) consists of entries in which bij = 1 if and only if
ij = i. Thus, the unit entries of row i of B identify those bids made by bidder i.
Facts:
1. The winner determination problem for the OR bid language (WDP-OR) can be
formulated as the following 0-1 integer programming problem (see §16.1.8) in the binary

Section 16.7
COMBINATORIAL AUCTIONS
1225
decision variables x: namely, xj = 1 if and only if bid j is accepted by the auctioneer in
the ﬁnal allocation of auction items to bidders. Here it is assumed that all akj and ck
values are nonnegative.
maximize:
bx
subject to:
Ax ≤c
x ∈{0, 1}|J|.
(1)
2. The (feasibility) constraints in (1) state that no more than ck copies of item k can
be allocated; i.e., each row enforces P
j∈J akjxj ≤ck for each item k.
3. Under the OR bid language, a bidder cannot express preferences for substitute items.
(See Example 1.)
4. WDP-OR is an NP-hard optimization problem for an arbitrary set of bids. This
remains true if bj = 1 for all submitted bids and if each bid is restricted to be on sets of
three or fewer items [RoPeHa98].
5. Assume ck = 1 for all k. Suppose the permissible subsets to be bid on form a tree
structure: if S1 and S2 are both allowed to be bid on, then either one set contains the
other or they are disjoint. Then WDP-OR is solvable in O(n2) time [RoPeHa98].
6. Assume ck = 1 for all k, and that permissible packages include only contiguous
intervals from the list of items K. The resulting matrix A will exhibit the consecutive
ones property and WDP-OR is solvable in O(n2) time. This describes the auctioning of
real estate along a coastline with bids only on contiguous land. The case of a closed loop
of coastline around an island can be solved in O(n3) time [RoPeHa98].
7. Since the OR bid language cannot express preferences for substitute items (e.g., I wish
to have exactly one of these items and would refuse/throw away any additional items),
the more expressive XOR bid language may be preferred, which can communicate any
value function over subsets.
8. The winner determination problem for the XOR bid language (WDP-XOR) can be
formulated as the following 0-1 integer programming problem (see §16.1.8) in the binary
decision variables x: namely, xj = 1 if and only if bid j is accepted by the auctioneer in
the ﬁnal allocation of auction items to bidders.
maximize:
bx
subject to:
Ax ≤c
Bx ≤1
x ∈{0, 1}|J|.
(2)
9. Each of the second set of constraints from (2) is indexed by a unique bidder i and
can be written as P
j∈J|ij=i xj ≤1.
10. A solution to WDP-XOR is referred to as eﬃcient, based on its connection to the
First Fundamental Welfare Theorem of Economics. When all bids are truthful, a solution
to WDP-XOR corresponds exactly to the classical notion of allocative eﬃciency.
11. WDP-XOR is an NP-hard optimization problem for an arbitrary set of bids. This
remains true if bj = 1 for all submitted bids and each bid is restricted to be on sets of
two or fewer items [vHMu01].
12. If each bidder submits a bid on every subset of K (communicating at least 2|K| −1
nonempty subsets), then both WDP-OR and WDP-XOR can be solved in polynomial
time by dynamic programming. However, even for relatively small |K| the burden of
communicating so many bids becomes prohibitive [RoPeHa98].

1226
Chapter 16
DISCRETE OPTIMIZATION
13. Both WDP-OR and WDP-XOR do not admit a polynomial-time approximation
scheme (unless P = NP) [BeFu99].
14. Among the most-studied special cases is that of unit demand, in which each bidder
wants to consume at most one item, and so is willing to bid only on single-item bundles.
This setting is often called the assignment game. When bidders are restricted to bid for
each item individually, formulation (2) becomes totally unimodular and WDP is hence
polynomially solvable. (See also §16.7.3, Fact 3 and §16.7.4, Fact 3.)
15. Early work in CAs looked at forming larger bids through the combination of atomic
bids on items or subsets using logical connectives like OR, XOR, AND, etc. Various
combinations such as OR-of-XOR and XOR-of-OR have been studied.
16. The desire to pay $500 for any n of the items from a particular subset cannot be
written eﬃciently (as the size of the subset grows) with simple logical connectives, unless
an explicit “n-of-the-set{}” operator is introduced [BoHo01].
Examples:
1. To illustrate Fact 3, suppose that vi(A) = vi(B) = vi(AB) = 10. In the OR bidding
language, the bidder cannot express this preference to the auctioneer.
If the bidder
submits bi(A) = bi(B) = 10, WDP-OR always allows the auctioneer to award both bids
at a total value of 20, ignoring any bid bi(AB) below 20.
2. Returning to Example 8 of §16.7.1, the solutions to both WDP-OR and WD-XOR
are identical (because bidders are all single-minded) with winning bids b1(AB) = 15
and b2(C) = 7.
Now suppose instead that the ﬁrst two bids come from one bidder
and the remaining from a second bidder (i.e., change the problem to bids b1(AB) = 15,
b1(C) = 7, b2(AC) = 12, b2(B) = 8, and b2(ABC) = 16). While WDP-OR remains
essentially unchanged (except that the two winning bids are from the same bidder), for
WDP-XOR we must employ the matrix
B =
 
1
1
0
0
0
0
0
1
1
1
!
as in (2), to enforce a single winning bid per bidder. The WDP-XOR solution is to award
b2(ABC) = 16, as the auction can no longer accept the ﬁrst and second bids together, or
the third and fourth bids together, both of which are better solutions under WDP-OR.
3. The XOR-language has been used in practice in several spectrum license CCAs in
Europe. Adjacent licenses with similar transmission properties were grouped into cat-
egories and sold in a ﬁrst CA to allocate the number of licenses won in each of a few
categories, with a second “assignment stage” CA used to determine a particular contigu-
ous assignment of bandwidth won in the ﬁrst auction. This two-stage design helped to
set prices on similar items, and to reduce both the communication burden (number of
bundles to bid on) and the computational burden of winner determination.
4. Combining the ﬂexibility/communication beneﬁts of OR and the richer expression
as in XOR, an important bid language introduced in the early CA literature is OR∗,
which consists of OR bids in which a bidder could place ﬁctitious items into her bids, or
equivalently, allow bidders to insert XOR-type constraints [Ni06].
5. For many practical auction applications, devising a manageable domain-speciﬁc bid
language (format to succinctly express relevant packages without an explosion of com-
munication requirements) can be a nontrivial task. Several papers take this approach for
a speciﬁc application area. For example, [GoEtAl15] look at packages of TV ad slots.
6. The 2014 Canadian spectrum license auction used an “XOR of base + OR” bidding
language in a CCA format.

Section 16.7
COMBINATORIAL AUCTIONS
1227
• http://www.ic.gc.ca/eic/site/smt-gst.nsf/eng/sf10726.html
For each base + OR collection of bids conveyed, if a bidder receives the base package,
then the auctioneer may also accept any of the following OR package bids. At most one
base + OR collection could be accepted from each bidder (an XOR structure). This
allowed for many more packages to be covered by bids, important because the number
of licenses in the auction (spanning several regions and frequency categories) made the
number of packages to consider quite large.
16.7.3
PRICE DETERMINATION AND INCENTIVE COMPATIBILITY
For simplicity, this subsection focuses on the XOR language.
Deﬁnitions:
For any auction, Pi will denote the total ﬁnal payment of bidder i.
Where convenient, pk will represent the unit price of item k.
Bidder i’s surplus is her observed or inferred proﬁt, given by si = bi(Si) −Pi, where Si
is her winning package, emphasizing that this is not her true payoﬀor proﬁt if she does
not bid truthfully, i.e., does not bid bi(S) = vi(S). Typically, CAs satisfy individual
rationality, in which si ≥0 for all i.
An allocation and bundle-pricing scheme are in competitive equilibrium if and only
if (i) the allocation is eﬃcient when bids are replaced by prices, and (ii) a bidder prefers
the bundle and price assigned to her relative to any other bundle at its prescribed price.
The bundle she receives, possibly empty, is said to be in her demand set at these prices.
The classical concept of a Walrasian equilibrium, also applied to continuous rather than
discrete commodities, demands that all items with a positive price are sold.
An auction is dominant-strategy incentive compatible (DSIC) if the strategy to
set bi(S) = vi(S) for all S provides at least as much utility to each bidder i as any other
bidding strategy she could choose.
A subset of bidders is called a coalition. The optimal objective value of WDP solved
over coalition C ⊆I (by ignoring the bids of others) is written as WDP(C). When bids
are true values, this gives the value of the coalitional welfare function, written w(C).
The Vickrey-Clarke-Groves (VCG) mechanism is the eﬃcient auction mechanism
in which each bidder pays Pi = bi(Si)−WDP(I)+WDP(I\i). The quantity WDP(I)−
WDP(I\i) is called the VCG discount.
A welfare function exhibits the buyers-are-substitutes condition if and only if w(I) −
w(I\C) ≥P
i∈C w(I) −w(I\i) for all C ⊂I.
If the buyers-are-substitutes property holds for every subset of I, the welfare function is
said to exhibit buyer-submodularity on I.
The outcome of an auction is blocked if there is some coalition of bidders who would
each agree to an alternative outcome that would also be preferred by the seller (because
of greater total payments). As in cooperative game theory, an unblocked outcome is said
to be in the core.
An eﬃcient auction that selects payments Pi in the core when bids are treated as truthful
is called core-selecting [DaMi08].

1228
Chapter 16
DISCRETE OPTIMIZATION
Facts:
1. An allocation is supported by competitive equilibrium prices if and only if it is eﬃ-
cient, but the prices may be nonlinear in items and non-anonymous (i.e., diﬀerent bidders
may see diﬀerent prices for the same bundle). See [MiPa07] for the computation of uni-
versal competitive equilibrium prices.
2. Whenever the LP relaxation of formulation (2) yields an integral solution, the LP dual
provides Walrasian equilibrium prices. In particular, the dual variables associated with
item capacities yield linear prices for each item that can be added to form anonymous
bundle prices. When the LP relaxation solution value strictly exceeds the integer solution
value, competitive equilibrium linear item prices do not exist. (Exercise: Prove these
statements using LP duality theory and complementary slackness. Hint: ﬁrst show that
dual variables associated with matrix B equal the surplus amounts si.)
3. In the assignment game context (see §16.7.2, Fact 14), [Le83] showed that among
all optimal solutions to the LP dual of (2), those that minimize payments are the VCG
prices.
4. When item prices pk exist that form a competitive equilibrium, the auction adding
those prices to form bundle payments is core-selecting. (Exercise: Prove this fact. Use
Fact 2 and Fact 11.)
5. The VCG mechanism is the unique eﬃcient DSIC individually rational payment rule
[GrLa77].
6. Bidders-are-substitutes is a necessary and suﬃcient condition for VCG to be sup-
ported in competitive equilibrium [BiOs02].
7. When the VCG outcome is in the core, it is the unique bidder-dominant point (strictly
preferred by all winners) in the core [AuMi02].
8. When the VCG outcome is not in the core, there is no bidder-dominant outcome.
Payments may be as low as zero despite large counter-bids, and VCG is vulnerable to
collusive manipulation and false-name bidding [AuMi02]. (See Example 2.)
9. Buyer-submodularity of the welfare function guarantees that the VCG outcome is in
the core [AuMi02].
10. If the substitutes (also called goods-are-substitutes or gross substitutes) condition
holds for all bidders (a property discussed in the economics literature in which a price
increase on one good cannot cause a necessary drop in demand of a diﬀerent good), then
buyer-submodularity holds [AuMi02]. (See Example 1.)
11. Core-selecting payments can be found via constraint generation. Given a candidate
set of payments for a particular eﬃcient solution, solving WDP with bids lowered by the
current surplus will ﬁnd the most violated core constraint [DaRa07]. With winning bid
amounts b∗
i for winning bidders i ∈W, the core constraint for any coalition C can be
written as P
i∈W\C Pi ≥WDP(C) −P
i∈C b∗
i .
12. When the VCG outcome is in the core, a bidder-optimal core-selecting algorithm
arrives at the VCG outcome.
13. In a CA, pay-as-bid is always a core-selecting outcome (so the core is always
nonempty.) The core in a CE may be empty.
14. In a CE, VCG may violate budget balance: VCG payments exceed revenue, requir-
ing outside funds.
Examples:
1. Let b1(A) = 4, b1(B) = 3, and b1(AB) = 6; b2(A) = 3, b2(B) = 4, and b2(AB) = 5.
Here the bidders treat the items as substitutes (Fact 10), and so the VCG outcome is

Section 16.7
COMBINATORIAL AUCTIONS
1229
in the core (Fact 9). (Exercise: Determine winning bids and compute VCG payments.
Attempt a few non-truthful strategies by a single bidder to see that there is no beneﬁt.)
2. Let b1(A) = b2(B) = b3(AB) = 20. VCG yields zero payments and is not in the core.
(See Fact 8.) If v1(A) < 20 this illustrates successful collusive or false-name bidding for
VCG [AuMi02]. Any individually rational outcome with P1+P2 = 20 is a bidder-optimal
core-selecting outcome.
3. Let b1(AB) = b2(AC) = b3(BC) = 10 and b4(ABC) = 12. Linear item prices that
form a competitive equilibrium do not exist (assuming individual rationality).
VCG
and bidder-optimal core-selection both have P4 = 10, but this cannot be decomposed
into equilibrium item prices. (Exercise: Prove the non-existence of item prices for this
numerical example.)
4. Let b1(A) = b2(B) = b3(C) = 20, b4(AB) = 28, b5(AC) = 26, and b7(A) = b8(B) =
b9(C) = 10. Minimum-revenue core-selecting payments are given by P1 = 16, P2 = 12,
and P3 = 10, which if taken as item prices form a Walrasian equilibrium. However, VCG
payments P1 = P2 = P3 = 10 are not in the core [DaRa07]. (Exercise: Demonstrate that
VCG is not in the core for this example.)
16.7.4
DEMAND REVELATION
Among the most important properties of traditional auctions is their ability to elicit
information about the demand in a market through an iterative process (e.g., calling
successively higher prices for a single good). With CAs, however, it is impossible in
many cases to guarantee truthful demand revelation through an iterative process (i.e.,
to implement VCG by calling out market prices and asking for demand reports). Here
we mention some facts and heuristic techniques concerning iterative auction procedures
in the CA setting.
Deﬁnitions:
An iterative CA consisting of simultaneous single-item auctions, in which item prices are
announced for each item in successive rounds, with price increases on items experiencing
excess demand, is called a simultaneous ascending auction (SAA) or simultaneous
multi-round auction (SMRA).
The exposure problem occurs in a SAA when a bidder feels the need to keep bidding
on a set of complementary items, exposing her to the risk of getting an incomplete (less
valuable) subset. It may also refer to the situation in which a bidder must actively bid
on substitute items, not knowing which one she might get, exposing her to the risk of
winning more than one when only one is desired.
Similar to the SAA process, ascending linear-item prices over several rounds have also
been used as the clock phase of a multi-phase CCA auction. In this case the bids in
each round are translated into XOR package bids to be used in a subsequent sealed-
bid WDP called the supplementary bidding phase. Bidders can place additional
supplementary bids beyond those revealed in the clock phase.
Because a SAA or clock phase is intended to reveal demand information for the market
as a whole, activity rules are often used to assure that bidders reveal their demand
in early rounds, rather than withholding information about the packages of interest, or
their value, until later rounds.

1230
Chapter 16
DISCRETE OPTIMIZATION
Among the simplest activity rules applied in practice is an eligibility points system.
Each item is assigned a number of eligibility points prior to the auction, and bidding is
restricted such that the total of eligibility points for the package bid on by a bidder is
not allowed to go up from one round to the next successive auction rounds.
Under a revealed preference activity rule, each bidder is forced to bid self-consistently
according to an assumed utility functional form.
Straightforward bidding occurs in an iterative auction when a bidder always truthfully
reports her packages of highest surplus at the current prices. This may also be called
myopic bidding, as a bidder bids without regard to any potential future rounds, as if
the current round will be the last.
Iterative CAs with nonlinear prices have been proposed based on real-time computation
of a winning level and deadness level for any package of interest. If a bidder submits
a package bid just above the current winning level, then it is guaranteed to become a
winning bid (at least until another later bid might knock it out). Any bid below the
deadness level can never become winning (no matter what bids are submitted in the
future). A bid between the winning level and deadness level of a bundle will not become
winning immediately, but may become winning with the help of other bids in future
rounds.
Facts:
1. There is no ascending-price auction using linear and anonymous item prices that is
guaranteed to implement the VCG outcome, even when attention is restricted to bidder
preferences satisfying the substitutes property [GuSt00].
2. If the substitutes condition holds (see §16.7.3, Fact 10), then the SAA with straight-
forward bidding closely approximates a competitive equilibrium (dependent on the bid
increment). With at least three bidders and one bidder who violates the substitutes
property, no competitive linear item prices exist [Mi00]. Eﬃciency can be quite low for
an iterative linear-price auction in the worst case. For example, [BiShZi13] give a
2
|K|+1
upper bound.
3. In the assignment game context (see §16.7.2, Fact 14), [DeGaSo86] among others
provide a primal-dual algorithm related to the Hungarian algorithm that implements
the VCG auction iteratively. The auctioneer announces prices and bidders report which
items maximize their utility at the stated prices (myopic bidding). An assignment is
attempted at each round with prices rising on over-demanded items until an assignment
is found.
4. With |K| = 1 and diminishing marginal values for the single item, buyers-are-
substitutes holds and VCG can be implemented with an ascending auction [Au04].
5. Assuming buyer-submodularity and straightforward bidding, the ascending proxy
auction [AuMi02] and iBundle(3) [PaUn00] terminate with minimal competitive equilib-
rium payments corresponding to the VCG outcome. (This does not guarantee a linear
item price decomposition of VCG, however.)
6. Assuming concave utility functions that are additively separable in money and items,
[BiEtAl11] consider a seller selling bundles corresponding to bases of a matroid, with
applications in scheduling and network formation markets. They provide an ascending
auction that terminates at the VCG outcome in such a setting.
7. Eligibility point activity rules have been used in several spectrum license auctions,
but are considered somewhat easy to manipulate. Consequently, there has been a move
towards revealed-preference activity rules.

Section 16.7
COMBINATORIAL AUCTIONS
1231
8. For the standard (most simple) revealed-preference activity rule, assume quasi-linear
preferences and no budget constraints, and consider any two auction rounds t1 and
t2, with respective item-price vectors pt1 and pt2. If a given bidder bids on packages
deﬁned by vectors at1 and at2, then regardless of her hidden values for these bundles,
self-consistency demands that (pt2 −pt1)(at2 −at1) ≤0. Bids violating this inequality will
be rejected according to the activity rule [AuCtMi06]. (Exercise: Prove that a truthful
bidder who knows her valuation function will never violate this activity rule under these
assumptions.)
9. The previously mentioned revealed-preference rule may be too restrictive in practice,
as budget constraints may be important and/or learning about preferences (due to im-
perfect knowledge of one’s own valuation function) may be desired. So relaxed versions
of this concept have been explored [AuBa14].
10. [AdGu05] introduced the concepts of winning levels and deadness levels for the
OR language, which were later extended by [PeZiBi13] to the XOR setting, where the
computation of deadness levels is shown to be ΠP
2 -complete.
Examples:
1. Let bidder 1’s bids be given by [(8 for A) XOR (8 for B)] OR [(8 for C) XOR (8
for D)]; bidder 2’s bids are [(6 for A) XOR (2 for C)] OR (6 for B); bidder 3’s bids
are [(2 for B) XOR (6 for D)] OR (6 for C). In one eﬃcient allocation, bidder 1 gets
{A, D}, bidder 2 gets {B}, and bidder 3 gets {C}, with VCG payments of 12, 2, and 2,
respectively. However, the minimal Walrasian equilibrium prices are given by (6, 6, 6, 6).
This example demonstrates preferences that satisfy the substitutes property yet cannot
achieve the VCG outcome in anonymous linear prices. (See Fact 1.)
2. Assume quasi-linear preferences and no budget constraints. Suppose at auction round
1 we have p1 = (1, 5) with submitted bid a1 = (1, 1)T . Then suppose that at some later
auction round 2 we have p2 = (3, 5) with a2 = (2, 0)T .
This violates the standard
revealed-preference activity rule (Fact 8) since (p2 −p1)(a2 −a1) > 0. In other words,
from round 1 to 2 the price of bundle (1, 1)T increased by 2, while the price of (2, 0)T
increased by 4; if you prefer the former bundle in round 1, you will continue to prefer it
after it experiences a smaller price increase.
3. Suppose that in the previous example, the bidder has v((1, 1)T ) = 12 and v((2, 0)T ) =
7, but now assume he has a budget of 7. He would prefer the ﬁrst bundle under both
p1 and p2, but cannot pay 8 for it under p2, so instead demands the latter bundle. This
shows a weakness of revealed-preference rules when budgets are taken into consideration.
4. Consider an iterative CA in which bidders submit package bids in turn, after querying
the auctioneer for the current winning level and deadness level of any bundle(s) of interest
(see Fact 10). Suppose the following bids have already been made by ﬁve diﬀerent bidders:
b1(B) = 5, b2(C) = 5, b3(AD) = 50, b4(AB) = 30, and b5(CD) = 40. Bidders 4 and
5 are currently winning. If a sixth bidder now queries the auctioneer about her bundle
of interest, {B, C}, a winning level of 20 is reported; bidding at least one bid increment
above 20 will assure winning, because added to bidder 3’s bid it will exceed the current
winning bids of bidders 4 and 5, i.e., b3(AD)+b6(BC) > b4(AB)+b5(CD). The reported
deadness level for {B, C} would be 10; any amount not exceeding 10 results in the bid
being rejected in favor of b1(B) + b2(C) = 10.

1232
Chapter 16
DISCRETE OPTIMIZATION
16.8
VERY LARGE-SCALE NEIGHBORHOOD SEARCH
There are numerous optimization problems of interest that are too diﬃcult to solve to
optimality in a reasonable amount of time. A practical remedy is to employ heuristics
that can quickly compute good quality solutions. Neighborhood search algorithms, also
called local search algorithms, represent a wide class of heuristics where at each iteration
the algorithm searches for a better quality solution in a “neighborhood” of the current
solution.
A critical issue in neighborhood search is deﬁning the neighborhood itself.
As a rule of thumb, the larger the neighborhood, the better the quality of a locally
optimal solution.
However, the larger the neighborhood, the longer will be the time
required to search the neighborhood explicitly. A large neighborhood will not produce
an eﬀective heuristic unless one can search the large neighborhood eﬃciently. This section
concentrates on neighborhood search algorithms where the size of the neighborhood is
“very large” with respect to the size of the input data, yet the neighborhood can be
implicitly searched in an eﬃcient manner.
16.8.1
BASIC CONCEPTS
Deﬁnitions:
A combinatorial optimization problem (COP) is an optimization problem of the
form: minimize {f(S) | S ∈F} where F ⊆2E is a ﬁnite set of feasible solutions.
In this context, E = {e1, e2, . . . , en} is a ﬁnite set of elements called the ground set,
f : F →R is the objective function, and 2E denotes the set of all subsets of E. For
example, in the Traveling Salesman Problem (TSP) of §10.7.1, E corresponds to the
edges in the graph, F corresponds to the set of feasible TSP tours, and f(S) corresponds
to the total length of the tour S ∈F.
A neighborhood function N is a point-to-set map N : F →2F. Given a feasible
solution S ∈F, the set N(S) is called the neighborhood of the solution S. The size
of a neighborhood is the cardinality of the set N(S).
An operation that modiﬁes a solution Si to obtain a solution Si+1 ∈N(Si) is called a
move. If f(Si+1) < f(Si), then the move is improving.
A solution S∗∈F is locally optimal with respect to a neighborhood function N if and
only if f(S∗) ≤f(S) for all S ∈N(S∗).
A neighborhood search algorithm, also called a local search algorithm, is an
iterative algorithm that begins with a solution Si−1 at the ith iteration, and moves to
an improving solution Si ∈N(Si−1) if one exists. Otherwise, the algorithm terminates
with Si−1.
A very large-scale neighborhood is a neighborhood that has “very large” size with
respect to the problem input. Typically, these neighborhoods are exponentially large,
but this term is often used informally to describe neighborhoods that are too large to
explicitly search in practice.
A very large-scale neighborhood (VLSN) search algorithm is a neighborhood
search algorithm that searches a very large-scale neighborhood at each iteration.

Section 16.8
VERY LARGE-SCALE NEIGHBORHOOD SEARCH
1233
Facts:
1. Algorithm 1 describes a generic neighborhood search algorithm, in which the initial
solution S0 is input. The subroutine BestNeighbor(Si−1, N, f) returns the best solution,
according to the objective function f, from the neighborhood N(Si−1).
Algorithm 1:
Generic neighborhood search.
input: S0
output: best solution found by neighborhood search
i ←0
repeat
i ←i + 1
Si ←BestNeighbor(Si−1, N, f)
until f(Si) ≥f(Si−1)
return Si−1
2. The term VLSN search was originally coined in the survey paper by Ahuja et al.
[AhEtal02].
3. Surveys of VLSN search techniques can be found in [AhEtal02], [AhEtal07], and
[PiRo10]. A survey of VLSN search techniques, designed speciﬁcally for the Traveling
Salesman Problem, can be found in De˘ineko and Woeginger [DeWo00].
4. VLSN search algorithms have been used to solve a wide range of well-structured
and complex real-world problems: vehicle routing problems [ErOrSt06], facility location
problems [AhEtal04b], generalized knapsack problems [CuAh05], and parallel machine
scheduling problems [FrNeSc04].
5. VLSN search algorithms have also been developed for many industrial applications:
airline ﬂeet assignment [AhEtal07a], [AhEtal04a], railroad freight blocking [AhJhLi07],
intermodal load planning [NeJhAh09], and steel manufacturing [DaEtal07].
Examples:
Although the focus is on VLSN search algorithms for NP-hard COPs, the next three
illustrations of VLSN search are for polynomially solvable optimization problems.
1. Column generation for linear programming:
The simplex method (see §16.1) can
be viewed as a neighborhood search algorithm. A basic feasible solution is determined
from the set of basic columns B. A move (or pivot) exchanges one column from B with
a column from N, the set of nonbasic columns. The neighborhood of a basic feasible
solution is the set of all basic feasible solutions that may be obtained by a single pivot.
The simplex method can be interpreted as a VLSN search when the set of columns is too
large to be represented explicitly and where column generation is used to determine the
entering variable.
2. Cycle canceling for the minimum cost ﬂow problem:
Consider the minimum cost
ﬂow problem (see §10.5) for a network G = (V, A), where each vertex v ∈V has a
supply/demand b(v) and where the unit cost of sending ﬂow on edge (i, j) is cij. For
a given feasible ﬂow x, the neighborhood of x consists of all feasible ﬂows that can be
obtained from x by sending ﬂow around a negative cost cycle in the residual network
induced by x [AhMaOr93].
3. Augmenting path algorithm for the maximum cardinality matching problem:
Con-
sider the maximum cardinality matching problem (see §10.2) for an undirected graph

1234
Chapter 16
DISCRETE OPTIMIZATION
G = (V, A). A matching is a collection of edges with no two edges incident with a common
vertex. Relative to the path P = [v1, . . . , v2k] in G, let A1(P) = {(v2j−1, v2j) | 1 ≤j ≤k}
and A2(P) = {(v2j, v2j+1) | 1 ≤j ≤k −1}. The path P is augmenting with respect to
the matching M if M ∩A1(P) = ∅and A2(P) ⊆M. To augment along the path P is to
replace M by (M\A2(P))∪A1(P). The neighborhood of M consists of all matchings that
can be obtained from M via augmenting paths. Berge [Be57] showed that if M is not
a maximum matching, then there is an augmenting path with respect to M. Edmonds
[Ed65] developed the blossom algorithm (see §10.2.3) for determining an augmenting
path if one exists. The blossom algorithm is widely considered to be one of the most
elegant algorithms in all of combinatorial optimization.
16.8.2
VARIABLE-DEPTH NEIGHBORHOOD SEARCH ALGORITHMS
Neighborhoods based on exchanging two or three elements can often be explicitly searched
in a reasonable amount of time.
Exchanging a larger number of elements may lead
to neighborhoods containing better local optimal solutions, but they typically cannot
be searched eﬃciently.
Variable-depth search algorithms are local search algorithms
that (in principle) consider neighborhoods where a variable number of elements can be
exchanged; to make the search practical, heuristics are used. There are variable-depth
search algorithms for a large number of applications that attain near optimal solutions
eﬃciently. Such algorithms are the focus of this subsection.
Deﬁnitions:
Let Si and Si+1 be solutions of a COP with ground set E. The move from solution Si
to solution Si+1 is a k-exchange if |Si\Si+1| = |Si+1\Si| = k for a positive integer k.
A neighborhood where |Si\Si+1| = |Si+1\Si| ≤k for all Si+1 ∈N(Si) is a k-exchange
neighborhood of Si. A k-exchange neighborhood is said to have depth k.
A variable-depth neighborhood search algorithm begins with a solution Si−1 at the
ith iteration, and moves to an improving solution Si in the |E|-exchange neighborhood
of Si−1 if an improvement can be determined. Otherwise, the algorithm terminates with
Si−1. Typically, at each iteration, a variable-depth neighborhood search algorithm only
partially explores the k-exchange neighborhood for each k ∈{1, 2, . . ., |E|}, thus avoiding
the Ω(|E|k) lower bound from explicitly searching the entire neighborhood.
An ejection chain is a variable-depth neighborhood search algorithm that iteratively
generates a structured alternating sequence of additions/deletions S1, S2, . . . , Sk where
• |S1| = |Sj| for all odd values of j, and
• |S2| = |Sj| for all even values of j.
Facts:
1. The ﬁrst variable-depth neighborhood search algorithm was developed by Kernighan
and Lin [KeLi70] for a set of graph partitioning problems.
2. Ejections chains were introduced by Glover [Gl96] for solving the TSP. Glover origi-
nally based ejection chains by extending and generalizing the ideas of Lin and Kernighan
[LiKe73].
3. Many variable-depth search techniques, particularly those for vehicle routing prob-
lems, rely on ejection chains at each iteration in order to search the neighborhood.

Section 16.8
VERY LARGE-SCALE NEIGHBORHOOD SEARCH
1235
Examples:
1. A traveling salesman tour [1, 2, 3, 4, 5, 1] is shown in part (a) of the following ﬁgure.
Part (b) shows a 2-exchange of this tour in which edges (2, 3) and (4, 5) are replaced by
edges (2, 4) and (3, 5), forming the new tour [1, 2, 4, 3, 5, 1].
1
2
3
4
5
(a)
1
2
3
4
5
(b)
2. In the context of the vehicle routing problem (see §10.7.2), the following ﬁgure shows
in part (a) a feasible assignment of two vehicles that service nine customers from a central
depot at location 0. Part (b) indicates the result of a 1-exchange in which customers 5
and 9 are interchanged, again resulting in a feasible solution.
2
3
4
5
1
0
7
6
8
9
(a)
2
3
4
5
1
0
7
6
8
9
(b)
3. The maximum cut problem (MAX CUT) is deﬁned as follows. Given an undirected
graph G = (V, A) where each edge e ∈A has weight we, partition the vertices V into two
subsets X and Y to maximize the sum of the weights of the edges in the cut (X, Y ) =
{(i, j) | i ∈X and j ∈Y }. Let f(X, Y ) denote the sum of the weights in the cut (X, Y ).
Algorithm 2 describes a generic variable-depth search algorithm for MAX CUT, given
the initial solution S0 as input. The ith iteration of the algorithm starts with a solution
Si−1 and either moves to a new solution Si where f(Si) > f(Si−1) or maintains the
solution Si−1 if no better solution is discovered within the ith iteration. Within each
iteration of the variable-depth search, there are as many as |V | rounds. Here Mj denotes
the set of vertices that may not be moved during round j + 1, and (Xj, Yj) denotes the
cut constructed during round j.
The subroutine flip(Xj−1, Yj−1, v) returns a new cut (Xj, Yj), where vertex v has been
moved from the subset containing it to the other subset. For example, if v ∈Xj−1, then
Xj = Xj−1\v and Yj = Yj−1 ∪{v}.
Algorithm 2:
Generic variable-depth search for MAX CUT
input: S0
output: best solution found by variable-depth search
i ←0
repeat
i ←i + 1

1236
Chapter 16
DISCRETE OPTIMIZATION
M0 ←∅
(X0, Y0) ←Si−1
for j = 1 to |V | do
v∗
j ←argmax v∈V \Mj−1{f(flip(Xj−1, Yj−1, v))}
Mj ←Mj−1 ∪{v∗
j }
(Xj, Yj) ←flip(Xj−1, Yj−1, v∗
j )
end for
Si ←{(Xk, Yk) | k = argmax j∈{1,...,|V |}f(Xj, Yj)}
until f(Si) ≤f(Si−1)
return Si−1
4. Additional applications, with reference sources, are given in the following table.
application
references
clustering
[DoPe94]
generalized assignment problem
[YaIbGl04], [YaYaIb99]
graph partitioning
[FiMa82], [KeLi70]
machine scheduling
[AgEtal07], [BrEtal11]
staﬀscheduling
[Do98]
traveling salesman problem
[He00], [JoMc97], [LiKe73]
traveling umpire problem
[TrYi07]
vehicle routing problem
[Gl96], [NaBr09], [Re98], [XuKe96]
16.8.3
CYCLIC EXCHANGE NEIGHBORHOOD SEARCH ALGORITHMS
The cyclic exchange neighborhood was developed for solving minimum cost partition
problems. Suppose that S = {S1, S2, . . . , SK} is a partition of E. Roughly speaking, a
move in a cyclic exchange neighborhood permits one element to be transferred into Sj
and permits another element to be transferred out of Sj, for several of the subsets Sj of
the partition S. We discuss cyclic exchange neighborhoods and reference applications to
a wide range of settings.
Deﬁnitions:
Let E be the ground set for a COP. The collection of subsets {S1, S2, . . . , SK} is a K-
partition of E if no subset is empty, the sets are pairwise disjoint, and the union of the
sets is E.
Suppose that c is a function deﬁned on the subsets of E; that is, c(E′) is the cost of
subset E′ of E.
A partitioning problem is a COP where the objective is to ﬁnd a partition T =
{T1, T2, . . . , TK} of E that minimizes PK
j=1 c(Tj).
Let S = {S1, S2, . . . , SK} and T = {T1, T2, . . . , TK} be two K-partitions of E. We say
that T is a 1-neighbor of S if |Tj\Sj| ≤1 and if |Sj\Tj| ≤1 for each j = 1, . . . , K. If
T is a 1-neighbor of S, then T is obtained from S via a 1-transfer.

Section 16.8
VERY LARGE-SCALE NEIGHBORHOOD SEARCH
1237
Suppose that S = {S1, S2, . . . , SK} is a K-partition and that T = {T1, T2, . . . , TK} is
obtained from S via a 1-transfer. From S and T , we can construct the (S, T )-transfer
graph, having vertex set E. For each k = 1, . . . , K, if Tk is obtained from Sk by inserting
element i and deleting element j (that is, Tk = (Sk\i) ∪{j}), then (i, j) is an edge of the
(S, T )-transfer graph.
If the (S, T )-transfer graph consists of a single cycle, we say that T is obtained from
S via a cyclic exchange. Speciﬁcally, if the cycle is [i1, . . . , ik, i1], then T is obtained
from S as follows: for each j = 1, . . . , k, one inserts ij into the subset of S containing
ij+1 and then deletes ij+1 from that subset. (Here, ik+1 is equivalent to i1.) Similarly,
if the (S, T )-transfer graph consists of a single path, we say that T is obtained from S
via a path exchange. The cyclic exchange neighborhood (resp., path exchange
neighborhood) of a K-partition S consists of all K-partitions that can be obtained
from S by a single cyclic (resp., path) exchange.
To ﬁnd an improving cyclic exchange starting with the K-partition S = {S1, S2, . . . , SK}
we create a new graph called the improvement graph. The vertex set of the improve-
ment graph is E. For every pair i, j of elements of E that are in diﬀerent subsets of the
partition S, there is an edge (i, j) in the improvement graph. Suppose Sk is the part of
S that contains element j. Then the edge (i, j) is interpreted as a transfer of element i
into Sk and a transfer of element j out of Sk. Let S′
k = (Sk\{j}) ∪{i} be the resulting
subset. The cost of the edge (i, j) in the improvement graph is cij = c(S′
k)−c(Sk), which
is the net change in cost of transferring element i into Sk and transferring element j out.
A cycle C = [i1, . . . , ik, i1] in the improvement graph is subset-disjoint with respect to
a K-partition S if no two elements of C belong to the same subset of S.
Facts:
1. Cyclic exchange neighborhoods were introduced by Thompson and Orlin [ThOr89]
and Thompson and Psaraftis [ThPs93].
2. Finding a minimum cost subset-disjoint cycle is NP-hard [ThOr89], but it is one of
those NP-hard problems that is relatively easy to solve in practice. Thus, heuristics are
used to search the improvement graph in practice. Eﬀective heuristics for constructing
negative cost subset-disjoint cycles are presented in Thompson and Psaraftis [ThPs93]
and in Ahuja et al. [AhOrSh01].
3. The improvement graphs and the transfer graphs are closely related. Suppose that
C is a subset-disjoint cycle in the improvement graph for partition S, and that T is the
partition obtained by performing the cyclic exchange induced by C. Then the (S, T )-
transfer graph consists of the cycle C.
4. Symmetry: if T is a 1-neighbor of S, then S is a 1-neighbor of T .
5. If T is a 1-neighbor of S, then the (S, T )-transfer graph is a disjoint union of cycles
and paths because each vertex of the graph has at most one incoming edge and at most
one outgoing edge.
6. Suppose S is a K-partition, and let G be the resulting improvement graph. There is a
one-to-one correspondence between cyclic exchanges from S and subset-disjoint directed
cycles in G.
7. Suppose S is a K-partition, and let G be the resulting improvement graph. The cost
of a subset-disjoint cycle C is the net change in objective value obtained after performing
the induced cyclic transfer.
8. In practice, it is often much faster to obtain the current improvement graph by
updating the improvement graph from the previous iteration rather than constructing a

1238
Chapter 16
DISCRETE OPTIMIZATION
new improvement graph from scratch.
9. A path exchange may be transformed into a cyclic exchange by adding dummy ver-
tices. This allows one to use a single uniﬁed search algorithm to identify improving cyclic
exchanges and path exchanges.
10. For K-partitions, the cyclic exchange neighborhood is of size O(nK), where n =
|E|. When K is permitted to grow with n, the size of the neighborhood is typically
exponential.
Examples:
1. The following ﬁgure illustrates a cyclic exchange. Here, vertex e1 will be transferred
from subset T1 to subset T2. Vertex e4 will be transferred from subset T2 to subset T4.
Vertex e10 will be transferred from subset T4 to T5. Finally, the cyclic exchange will be
completed by transferring vertex e13 from subset T5 to subset T1.
e10
e1
e11
e13
e12
e2
e3
e4
e5
e6
e7
e8
e9
T1
T2
T4
T3
T5
2. The following ﬁgure illustrates a cyclic exchange for a VRP [AgEtal03]. In general, a
cyclic exchange for VRP transfers customer demands in a circular manner among several
routes. This example illustrates a “3-cyclic, 2-exchange” scheme where two customers
from each of three routes are transferred to the next route in the cycle. Here there are
A1
A2
A3
A4
A5
B1
B2
B3
B4
B5
C1
C2
C3
D1
D2
D3
D4
(a)
A1
A2
A3
A4
A5
B1
B2
B3
B4
B5
C1
C2
C3
D1
D2
D3
D4
(b)
four routes I1, . . . , I4 and the customers being served by each route are I1 = {A1, . . . , A5},
I2 = {B1, . . . , B5}, I3 = {C1, . . . , C3}, and I4 = {D1, . . . , D4}. This 3-cyclic 2-exchange
involves only routes I1, I2, and I3. Two customers {A1, A3} from route I1 are transferred
to route I2; two customers {B1, B5} from route I2 are moved to route I3; two customers
{C2, C3} from I3 are transferred to I1; route I4 is left unchanged. Let S = {I1, . . . , I4}
and S′ = {I′
1, . . . , I′
4} represent the set of routes before and after a cyclic exchange occurs,

Section 16.8
VERY LARGE-SCALE NEIGHBORHOOD SEARCH
1239
and let f(I) represent the optimal cost of route I. The cost of this cyclic exchange is
P4
i=1[f(I′
i) −f(Ii)].
3. Capacitated minimum spanning tree problem (CMST):
The CMST (see §10.6.1)
is the problem of choosing a minimum weight spanning tree subject to the following
constraint: if one deletes the root vertex and its incident edges, then each remaining
connected component has at most K vertices, where K is part of the input. This problem
models applications in which the goal is to develop a minimum cost computer network
connecting n computers to a single hub (or server), where bandwidth is limited. Ahuja
et al. [AhOrSh01], [AhOrSh03] used cyclic exchanges to obtain solutions to a set of
benchmark instances for the CMST. In each benchmark instance, they either obtained
or improved upon the best previously reported solution.
4. Facility location problems:
In facility location problems (see §16.2), the goal is
to determine the location of a collection of facilities, and also to assign customers to
facilities, to minimize the sum of the distances from customers to facilities. Ahuja et
al. [AhEtal04b] developed a cyclic exchange VLSN search algorithm for a capacitated
facility location problem.
Scaparra et al. [ScPaSc04] created an algorithm based on
the cyclic exchange neighborhood for solving the closely related capacitated vertex p-
center problem. In both cases, the algorithms based on cyclic exchanges produced nearly
optimal solutions and were fast on benchmark instances.
5. Additional applications, with reference sources, are given in the following table.
application
references
generalized assignment problem
[YaEtal04]
intermodal load planning
[NeJhAh09]
inventory routing
[SiEtal05]
machine scheduling
[FrNeSc04]
multiple knapsack problems
[CuAh05]
network design
[AmScSc09]
production planning
[AhEtal07b]
quadratic assignment problem
[AhEtal07c]
timetabling
[AbEtal06]
vehicle routing problem
[AgEtal03], [ThPs93]
weapons target assignment
[AhEtal08]
16.8.4
OTHER VLSN SEARCH ALGORITHMS
In designing very large-scale neighborhood search algorithms, researchers have drawn
inspiration and techniques from more than 50 years of research and algorithmic devel-
opment in mathematical programming and computer science algorithms. In this sub-
section, we describe a diverse collection of approaches for developing very large-scale
neighborhood search algorithms.
Examples:
1. Neighborhoods based on combining independent moves: Compounded independent
move (CIM) neighborhoods are neighborhoods based on simultaneously executing two
or more moves whose eﬀects on the objective value are independent of one another; that
is, the change in the objective value due to the compound move is guaranteed to be the

1240
Chapter 16
DISCRETE OPTIMIZATION
sum of the changes due to the individual moves. CIM neighborhoods were introduced
by Congram et al. [CoPoVe02] for a machine scheduling problem and by Potts and van
de Velde [PoVe95] for the TSP. Their techniques searched a CIM neighborhood with
dynamic programming, and they referred to their approach as dynasearch. Ergun et
al. [ErOrSt06] and Gendreau et al. [GeEtal06a] searched CIM neighborhoods to solve
vehicle routing problems. Taillard [Ta03] used CIM neighborhoods for solving clustering
problems. Dror and Levy [DrLe86] used CIM neighborhoods for solving the inventory
routing problem.
2. Neighborhoods based on context-free grammars:
Bompadre and Orlin [BoOr05]
showed how to use context-free grammars (CFGs) to generate exponentially large neigh-
borhoods for sequencing problems. Moreover, for some sequencing problems such as the
TSP and the linear ordering problem (see §16.9.1), they developed an algorithm that
takes the grammar and the problem data as input and then ﬁnds the minimum cost
neighbor in polynomial time. In addition to presenting an innovative use of CFGs, this
paper demonstrates how a single algorithm can be used to search a wide range of diﬀerent
exponentially sized neighborhoods.
3. Neighborhoods based on mathematical programming:
Suppose that x is a solution
to a mathematical programming problem. For a given subset I of indices, we say that
another feasible solution y is I-adjacent to x if yi = xi for i ∈I, and we say that y is in
the I-neighborhood of x. I-neighborhoods are conveniently searched using mathematical
programming software. This neighborhood can be used in constraint programming (e.g.,
Pesant and Gendreau [PeGe99] and Davenport et al. [DaEtal07]) as well as in integer
programming (e.g., Danna et al. [DaRoLe05]).
For example, the relaxation induced
neighborhood search (RINS) algorithm of Danna et al. has been implemented in the
CPLEX Mixed Integer Programming (MIP) solver.
4. Neighborhoods based on polynomially solvable special cases: Researchers have devel-
oped a substantial literature on polynomial-time algorithms for special cases of NP-hard
COPs. Many of the special cases are neighborhood-inducing, and lead to very large-scale
neighborhoods that can be searched in polynomial time. For example, Carlier and Villon
[CaVi90] presented a polynomial-time algorithm for ﬁnding an optimal pyramidal tour,
that is, a sequence i1, . . . , in such that for some value k, the indices in the subsequence
i1, . . . , ik are in increasing order, and the indices in the subsequence ik, . . . , in are in
decreasing order. For a given tour T , its pyramidal neighborhood is the set of pyramidal
tours that would result if one ﬁrst relabeled the vertices (or cities) so that T = [1, . . . , n].
Therefore, any algorithm for ﬁnding a minimum cost pyramidal tour can be used for
ﬁnding a minimum cost pyramidal neighbor.
As another example, for a given ﬁxed value K, there is a polynomial-time dynamic
program for ﬁnding an optimal tour [i1, . . . , in] subject to the constraint that ij < j + K
for each j. Simonetti and Balas [SiBa96] developed a very large-scale neighborhood that
uses this dynamic program as the search engine.
5. Neighborhoods that exploit local structure: Ahuja et al. [AhJhLi07] created a neigh-
borhood tailored to a railroad blocking problem. Blocking problems, which also arise in
parcel delivery, trucking and air transportation, model how to consolidate a large number
of shipments into blocks to reduce the total handling costs. The VLSN search algorithm
devised by Ahuja et al. iteratively improves solutions by permitting changes to the block-
ing at one rail station at a time.

Section 16.9
TABU SEARCH
1241
16.9
TABU SEARCH
Tabu search is a metaheuristic solution methodology applicable to general optimization
problems. It is distinguished by its use of adaptive (ﬂexible) memory and responsive
exploration of the solution space. In this way the search can learn to exploit speciﬁc
areas of the solution space as well as explore new terrain.
16.9.1
BASIC CONCEPTS
Tabu search starts from an initial solution and explores the solution space by making
transformations to move from one solution to the next. The use of ﬂexible memory pro-
motes good move combinations and solution attributes by modifying the neighborhood
searched at each step.
Deﬁnitions:
Tabu search (TS) is a search technique that proceeds iteratively from one (feasible)
solution to another until a chosen termination criterion is satisﬁed. It can be applied
to optimization problems of the form: minimize {f(x) | x ∈X}, where the set X
summarizes the constraints on the decision variables x.
Each solution x ∈X has an associated neighborhood N(x) ⊆X. The solution x′ ∈
N(x) is reached from x by a move operation; in this case, x′ is a neighbor of x.
The move value v is the change in the objective function value f from the current
solution to the solution after the move: i.e., v = f(x′) −f(x). An improving move
has v < 0 while a nonimproving move has v ≥0. A best neighbor of x (according
to the objective function value) is a neighbor x′ ∈N(x) having the smallest value f(x′)
over all neighbors of x.
Recency-based (short-term) memory is used in TS to obtain a modiﬁed neighbor-
hood N ∗(x) ⊆N(x) by eliminating (making tabu) recently visited solutions.
Facts:
1. Tabu search was ﬁrst proposed by Fred Glover [Gl86]. The most cited reference is
the book Tabu Search by Fred Glover and Manuel Laguna [GlLa97].
2. The word tabu (or taboo) comes from the Polynesian language Tongan, in which it
indicates objects that cannot be touched because they are sacred.
3. Whereas the steepest descent procedure only makes improving moves, TS allows the
choice of a best neighbor x′ of x even when the move is nonimproving.
4. By using short-term strategies, it is possible that a solution x can be visited more
than once, but it is likely that the corresponding reduced neighborhood N ∗(x) will be
diﬀerent each time.
5. TS memory structures are based on recording attributes as well as the most recent
history of the search trajectory (recency-based memory).
6. Selected attributes that occur in solutions recently visited are tabu-active, and solu-
tions that contain tabu-active elements, or particular combinations of these attributes,
are classiﬁed as tabu. This prevents certain solutions from the recent past from belonging
to N ∗(x) and hence from being revisited.

1242
Chapter 16
DISCRETE OPTIMIZATION
7. The number of iterations that an attribute remains tabu-active (the tabu tenure)
can be static (ﬁxed throughout the search) or dynamic (vary either systematically or
probabilistically).
8. Aspiration criteria can be used to override tabu activation rules. The most common
aspiration criterion consists of removing a tabu classiﬁcation from a trial move when the
move yields a solution better than the best obtained so far.
9. The overall structure of tabu search, employing short-term memory, is displayed in
Algorithm 1. The solutions xnext chosen during the Intensiﬁcation Phase are either not
classiﬁed tabu (as shown in Algorithm 1) or their tabu status has been overridden by the
aspiration criteria (not shown in Algorithm 1).
Algorithm 1:
Short-term memory tabu search.
input: initial solution x0 ∈X
output: best solution xbest found during the search
xnow = xbest = x0
repeat Intensiﬁcation Phase
choose xnext ∈N ∗(xnow)
xnow ←xnext
if f(xnow) < f(xbest) then xbest ←xnow
update recency-based memory and tabu status
until termination criteria satisﬁed
Examples:
1. Linear ordering problem (LOP):
Given a matrix of weights, the LOP consists of
ﬁnding a permutation x of the columns (and rows) of the matrix in order to maximize
the sum of the weights in the upper triangle. Let x = (1, 2, 3, 4) be a solution to a LOP
of size 4, deﬁned by the following table.
1
2
3
4
1
0
12
−5
3
2
6
0
3
4
3
−8
7
0
−2
4
4
0
3
0
The objective function value f(x) = 12 −5 + 3 + 3 + 4 −2 = 15. If a move is deﬁned as
the swap of two elements in the permutation, where each element represents the index
of the same row and column, then the swap (1, 2) results in the following table.
2
1
3
4
2
0
6
3
4
1
12
0
−5
3
3
7
−8
0
−2
4
0
4
3
0

Section 16.9
TABU SEARCH
1243
The neighborhood of x consists of the following solutions x′.
move
x′
f(x′)
(1, 2)
(2, 1, 3, 4)
9
(1, 3)
(3, 2, 1, 4)
10
(1, 4)
(4, 2, 3, 1)
8
(2, 3)
(1, 3, 2, 4)
19
(2, 4)
(1, 4, 3, 2)
20
(3, 4)
(1, 2, 4, 3)
20
2. Knapsack problem: Here we seek to maximize the overall proﬁt obtained by placing
a subset of items in a knapsack with total weight capacity 100. Each item has a known
proﬁt and weight. A formulation in terms of 0-1 variables xj is
max 67x1 + 500x2 + 98x3 + 200x4 + 120x5 + 312x6 + 100x7 + 200x8 + 180x9 + 100x10
s.t. 5x1 + 45x2 + 9x3 + 19x4 + 12x5 + 32x6 + 11x7 + 23x8 + 21x9 + 14x10 ≤100
xj ∈{0, 1}
An initial solution x0 can be found using a greedy heuristic: namely, successively select
items in decreasing order of their bang-for-buck ratio (proﬁt/weight), continuing until
adding one more item exceeds the capacity of the knapsack. The following table shows
these ratios; notice that the variables x1, . . . , x10 are already indexed by decreasing ratios.
This greedy heuristic successively chooses items 1, 2, 3, 4, 5 for the initial solution, giving
x0 = (1, 1, 1, 1, 1, 0, 0, 0, 0, 0) with total proﬁt 985 and total weight 90.
item
1
2
3
4
5
6
7
8
9
10
proﬁt
67
500
98
200
120
312
100
200
180
100
weight
5
45
9
19
12
32
11
23
21
14
ratio
13.40
11.11
10.89
10.53
10.00
9.75
9.09
8.70
8.57
7.14
3. We continue with the previous example. Suppose that a move is deﬁned as a “ﬂip”
move: i.e., the value of a single variable xj is changed from either zero to one or from
one to zero. The neighborhood N(x0) of the current solution x0 is then shown in the
following table. The move column indicates the new value of the ﬂipped variable, while
the neighbor column shows the indices of the variables set to 1 as a result of the move.
j
move
neighbor
proﬁt
weight
1
x1 = 0
{2, 3, 4, 5}
918
85
2
x2 = 0
{1, 3, 4, 5}
485
45
3
x3 = 0
{1, 2, 4, 5}
887
81
4
x4 = 0
{1, 2, 3, 5}
785
71
5
x5 = 0
{1, 2, 3, 4}
865
78
6
x6 = 1
{1, 2, 3, 4, 5, 6}
1297
122
7
x7 = 1
{1, 2, 3, 4, 5, 7}
1085
101
8
x8 = 1
{1, 2, 3, 4, 5, 8}
1185
113
9
x9 = 1
{1, 2, 3, 4, 5, 9}
1165
111
10
x10 = 1
{1, 2, 3, 4, 5, 10}
1085
104

1244
Chapter 16
DISCRETE OPTIMIZATION
Suppose the search is not allowed to visit infeasible solutions. Then there is no improving
move in the neighborhood of the current solution (recall that we have a maximization
problem, not a minimization problem). If we adopt the best-improving strategy, then
the ﬁrst move in the table, changing the value of x1 to zero, is the best one in N(x0),
giving xnext = (0, 1, 1, 1, 1, 0, 0, 0, 0, 0) with a proﬁt of 918.
4. We show how short-term memory can be incorporated into the previous 0-1 knapsack
example. Let the attribute of interest be the index of the variable chosen to change
values.
Any move that includes such an index is classiﬁed as tabu, and we set the
tabu tenure to be three iterations. Using this memory structure, we determine the next
solution xnext.
Subject to the constraint that x1 = 0 and considering only a single
“ﬂip” move, a best neighbor of xnow = (0, 1, 1, 1, 1, 0, 0, 0, 0, 0) turns out to be xnext =
(0, 1, 1, 1, 1, 0, 1, 0, 0, 0), achieved by changing the value of x7. The corresponding proﬁt is
1018 with a weight of 96. The following table shows shows ten iterations of the procedure
using this short-term memory structure. The tabu active column lists the indices of the
tabu active variables; each index remains tabu for three iterations. The move column
indicates the ﬂipped variable, while the solution column shows the solution after the
move has been executed.
The resulting proﬁt and weight are shown in the last two
columns. The best solution found after ten iterations occurs in the second iteration and
has a proﬁt of 1018.
iteration
tabu active
move
solution
proﬁt
weight
0
{1, 2, 3, 4, 5}
985
90
1
1
{2, 3, 4, 5}
918
85
2
1
7
{2, 3, 4, 5, 7}
1018
96
3
7 1
3
{2, 4, 5, 7}
920
87
4
3 7 1
5
{2, 4, 7}
800
75
5
5 3 7
8
{2, 4, 7, 8}
1000
98
6
8 5 3
7
{2, 4, 8}
900
87
7
7 8 5
3
{2, 3, 4, 8}
998
96
8
3 7 8
4
{2, 3, 8}
798
77
9
4 3 7
9
{2, 3, 8, 9}
978
98
10
9 4 3
8
{2, 3, 9}
778
75
5. An alternative neighborhood deﬁnition for the 0-1 knapsack problem could be one in
which the values of two variables are ﬂipped. Instead of 10 neighbors, this move generates
45 neighbors, corresponding to all combinations of two variables. A potential short-term
memory structure for this neighborhood could be as follows.
• attributes: indices of the variables chosen to change values
• tabu move: one that includes at least one tabu active variable
• tabu tenure: three iterations for a variable ﬂipping from 1 to 0, and two iterations
for a variable ﬂipping from 0 to 1
The tabu tenure recognizes that in a typical knapsack problem there will be more vari-
ables outside the knapsack (i.e., with their values set to zero) than those inside the
knapsack (i.e., with their values set to one). Therefore, if a variable leaves the knapsack,
the tabu tenure keeps this item outside longer than when an item enters the knapsack.
6. The LOP in Example 1 provides a few choices for a short-term memory design
in a tabu search that employs swap moves (i.e., exchanges of the positions of two
rows/columns). One of those designs is as follows.

Section 16.9
TABU SEARCH
1245
• attributes: indices of the elements (rows/columns) chosen to exchange positions
• tabu move: one that includes at least one tabu active variable
• tabu tenure: two iterations for the element exchanging to the lower position, and
three iterations for the element moving to the higher position in the permutation
This tabu tenure structure identiﬁes that elements moving to the earlier position in the
permutation make a larger contribution to the objective function value. Arguably, these
elements have moved to their “preferred” position and therefore there is no need to force
them to stay there.
7. Additional applications, with reference sources, are given in the following table.
application
references
maritime transportation
[KoFa10], [KoFaLa10]
chemical industry
[LiEtal05], [LiMi04a], [LiMi04b]
satellite range scheduling
[ZuAmGi08]
conservation area network design
[CiBaSa10]
high level synthesis
[SeSiRo11]
real-world routing
[ArHeSp06], [GeEtal06b], [GeEtal08],
[OpLø08], [PaEtal07]
forestry
[CaEtal03]
scheduling
[AlEtal04], [HoLe10]
DNA sequencing
[BlEtal00], [BlEtal04], [BlGlKa04]
logistics
[ScSø09], [Su12]
16.9.2
LONG-TERM MEMORY
Long-term memory plays an important role in creating the right balance between in-
tensiﬁcation and diversiﬁcation of a tabu search. Intensiﬁcation strategies are based on
modifying choice rules to encourage move combinations and solution features historically
found good. They may also initiate a return to attractive regions to search them more
thoroughly. Diversiﬁcation, on the other hand, encourages the search process to examine
unvisited regions and the generation of solutions that diﬀer in signiﬁcant ways from those
previously seen.
Deﬁnitions:
Frequency-based (long-term) memory is used in TS to obtain a modiﬁed neigh-
borhood N ∗(x) ⊇N(x) in order to expand the neighborhood and examine unvisited
regions.
Frequencies consist of ratios, whose numerators represent either transition counts or
residence counts. A transition count is the number of iterations where an attribute
changes (enters or leaves) the solutions visited. A residence count is the number of
iterations where an attribute belongs to the solutions visited. The denominators generally
represent one of three types of quantities: (1) the total number of occurrences of all events
represented by the numerators (such as the total number of iterations), (2) the sum (or
average) of the numerators, or (3) the maximum numerator value.

1246
Chapter 16
DISCRETE OPTIMIZATION
Facts:
1. The use of long-term memory does not require long solution runs before its beneﬁts
become visible.
2. The chance of ﬁnding still better solutions as time grows—in the case where an
optimal solution has not already been found—is enhanced by using long-term TS memory
in addition to short-term memory.
3. Residence frequencies and transition frequencies sometimes convey related informa-
tion, but in general they carry diﬀerent implications.
4. A high residence frequency may indicate that an attribute is highly attractive if the
domain consists of high quality solutions, or may indicate the opposite if the domain
consists of low quality solutions.
5. Frequency-based memory is used to deﬁne penalty and incentive values to modify the
evaluation of moves and therefore determine which moves are selected. In a minimization
problem, the modiﬁed move value function has the following mathematical form:
v′ = v(1 + pq).
Here v is the original move value, p is the penalty factor, q is the frequency ratio, and
v′ is the modiﬁed move value. When p = 0 the original move value is not modiﬁed. A
value p > 0 penalizes the move and v′ makes it less attractive. A value p < 0 provides
an incentive and v′ makes the move more attractive.
6. Penalty/incentive functions based on frequency ratios are also used in restarting
procedures that employ greedy choices. Penalty/incentive functions bias the selections
and induce diversiﬁcation.
7. Algorithm 2 shows how a restarting procedure based on long-term memory may be
superimposed on the short-term memory tabu search in Algorithm 1.
Algorithm 2:
Tabu search with long-term memory.
input: initial solution x0 ∈X
output: best solution xbest found during the search
xnow = xbest = x0
repeat Diversiﬁcation Phase
reset recency-based memory
if xnow = ∅then xnow ←diversiﬁed starting solution
repeat Intensiﬁcation Phase
choose xnext ∈N ∗(xnow)
xnow ←xnext
if f(xnow) < f(xbest) then xbest ←xnow
update recency-based memory and tabu status
update frequency-based memory
until termination criteria satisﬁed
until termination criteria satisﬁed
Example:
1. Suppose that in the knapsack problem described in §16.9.1, Example 2 we deﬁne the
residency ratio q as the number of times an item was in the knapsack divided by the

Section 16.9
TABU SEARCH
1247
current number of visited solutions. We then restart the search by erasing the short-term
memory and generating a solution with the following function:
r′ = r(1 −q),
where r is the original bang-for-buck ratio. This function reduces the attractiveness of
elements with large q values. The new starting solution is built by giving priority to the
elements with large r′ and thus inducing diversiﬁcation.
16.9.3
STRATEGIC OSCILLATION
Strategic oscillation provides a means to achieve an eﬀective interplay between inten-
siﬁcation and diversiﬁcation over the intermediate to long term. Strategic oscillation
operates by orienting moves in relation to the feasibility boundary, which represents a
point where a search would normally stop. Instead of stopping when this boundary is
reached, however, the rules for selecting moves are modiﬁed to permit the region deﬁned
by the feasibility boundary to be crossed. The approach then proceeds for a speciﬁed
depth beyond the oscillation boundary, and turns around.
The oscillation boundary
again is approached and crossed, this time from the opposite direction, and the method
proceeds to a new turning point.
Deﬁnitions:
The process of repeatedly approaching and crossing the feasibility boundary from diﬀer-
ent directions creates an oscillatory behavior, and so characterizes the strategic oscil-
lation method.
The implementation of strategic oscillation entails the selection of a proximity measure
and the oscillation amplitude. The proximity measure assigns a numerical value to
moving toward or away from the feasibility boundary. Proximity measures are deﬁned
in reference to the problem constraints. The oscillation amplitude is controlled by
a speciﬁed number of moves m. If a move is made that causes the search to cross the
feasibility boundary, then the search is allowed to make m −1 additional moves in the
same direction (i.e., moving away from the feasibility boundary) before turning around.
Facts:
1. In combinatorial optimization problems that require the selection of k elements, the
rule to delete elements from the solution will typically be diﬀerent in character from the
one used for adding elements. In other words, one rule is not simply the inverse of the
other.
2. Rule diﬀerences are features of strategic oscillation that provide an enhanced heuristic
vitality. The application of diﬀerent rules may be accompanied by crossing a boundary
to diﬀerent depths on diﬀerent sides. An option is to approach and retreat from the
boundary while remaining on a single side, without crossing (i.e., electing a crossing of
“zero depth”).
3. In both one-sided and two-sided oscillation approaches it is frequently important to
spend additional search time in regions close to the feasibility boundary, and especially
to spend time at the boundary itself.
4. In problems for which feasibility is determined by a set of constraints, vector-valued
functions can be used to control the oscillation.
In this case, controlling the search
by bounding this function can be viewed as manipulating a parameterization of the
constraint set.

1248
Chapter 16
DISCRETE OPTIMIZATION
Examples:
1. In a combinatorial optimization problem consisting of selecting k out of a given
number of elements, a proximity measure t may be the diﬀerence between the number
of elements that have been selected and k. Feasible solutions are those for which t = 0.
Solutions with t > 0 have too many elements and solutions with t < 0 have too few.
Moves can be designed to keep the search at t = 0 or to create an oscillation pattern
around t = 0.
2. The feasibility boundary in the knapsack problem is deﬁned by the capacity of the
knapsack, which is 100 in §16.9.1, Example 2. To create simple oscillation around the
feasible boundary, the search is allowed to cross to the infeasible region and m is set to
1. The proximity measure t is deﬁned as the diﬀerence between the total weight of the
items in the knapsack and the knapsack capacity. Therefore, t > 0 indicates an infeasible
solution while t ≤0 corresponds to a feasible solution. The procedure operates with the
following rules:
• When approaching the feasibility boundary from the feasible region, select the
best non-tabu feasible move. If no feasible move is available, then select the
non-tabu move that improves proﬁt the most.
• When approaching the feasibility boundary from the infeasible region, select the
non-tabu move that removes the variable with the smallest bang-for-buck ratio.
The short-term memory structure is the same as the one deﬁned in §16.9.1. Therefore,
the move selections have to be done by considering the tabu classiﬁcations. The following
table shows 10 iterations of TS with strategic oscillation. The initial solution is given by
the bang-for-buck heuristic.
iteration
tabu active
move
solution
proﬁt
weight
0
{1, 2, 3, 4, 5}
985
90
1
6
{1, 2, 3, 4, 5, 6}
1297
122
2
6
5
{1, 2, 3, 4, 6}
1177
110
3
5 6
4
{1, 2, 3, 6}
977
91
4
4 5 6
8
{1, 2, 3, 6, 8}
1177
114
5
8 4 5
6
{1, 2, 3, 8}
865
82
6
6 8 4
9
{1, 2, 3, 8, 9}
1045
103
7
9 6 8
3
{1, 2, 8, 9}
947
94
8
3 9 6
4
{1, 2, 4, 8, 9}
1147
113
9
4 3 9
8
{1, 2, 4, 9}
947
90
10
8 4 3
3
{1, 2, 3, 4, 9}
1045
99
The oscillation can be observed in the weight column. It is noteworthy to discuss the
move made in the last iteration. The move is selected even though the move is classiﬁed
tabu (note that 3 appears in the tabu active list). The initial solution is the best-known
feasible solution at that point and it has an objective function value of 985. Therefore, an
aspiration criterion that overrides the tabu status of a move can be invoked (see §16.9.1,
Fact 8). The criterion states that the tabu classiﬁcation can be overridden if the move
leads to a solution that is better than the best solution found so far.

Section 16.9
TABU SEARCH
1249
16.9.4
PATH RELINKING
A useful integration of intensiﬁcation and diversiﬁcation strategies occurs in the approach
called path relinking. This approach generates new solutions by exploring trajectories
that connect reference solutions—by starting from one of these solutions, called an ini-
tiating solution, and generating a path in the neighborhood space that leads toward the
other solutions, called guiding solutions. This is accomplished by selecting moves that
introduce attributes contained in the guiding solutions.
Deﬁnition:
Path relinking is a strategy that seeks to incorporate attributes of high quality solu-
tions, by creating inducements to favor these attributes in the moves selected. However,
instead of using an inducement that merely encourages the inclusion of such attributes,
the path relinking approach subordinates all other considerations to the goal of choosing
moves that introduce the attributes of the guiding solution, in order to create a “good
attribute composition” in the current solution.
Facts:
1. At each step, path relinking chooses the best move, according to the change in the
objective function value, from the restricted set of moves that incorporate a maximum
number of the attributes in the guiding solutions.
2. Path relinking can be used to generate intensiﬁcation strategies by choosing reference
solutions that lie in a common region or that share common features.
3. Diversiﬁcation strategies based on path relinking characteristically select reference
solutions that come from diﬀerent regions or that exhibit contrasting features.
4. Membership in the reference set is determined by setting a threshold that is connected
to the objective function value of the best solution found during the search.
5. The initiating solution can be used to give a beginning partial construction, by spec-
ifying particular attributes as a basis for the remaining constructive steps. Constructive
neighborhoods can be viewed as a feasibility-restoring mechanism since a null or partially
constructed solution does not satisfy all conditions to qualify as feasible.
6. Path relinking can make use of destructive neighborhoods, where an initial solution
is “overloaded” with attributes donated by the guiding solutions, and such attributes
are progressively stripped away or modiﬁed until reaching a set with an appropriate
composition.
7. Path relinking consists of the following steps:
• Identify the neighborhood structure and associated solution attributes for path
relinking (possibly diﬀerent from those of other TS strategies applied to the
problem).
• Select a collection of two or more reference solutions, and identify which mem-
bers will serve as the initiating solution and the guiding solution(s). (Reference
solutions can be infeasible, such as “incomplete” or “overloaded” solution com-
ponents treated by constructive or destructive neighborhoods.)
• Move from the initiating solution toward the guiding solution(s), generating one
or more intermediate solutions.

1250
Chapter 16
DISCRETE OPTIMIZATION
Example:
1. Consider the following two reference solutions for the knapsack problem in §16.9.1,
Example 2.
solution
1
2
3
4
5
6
7
8
9
10
proﬁt
weight
1
0
1
1
0
1
0
1
0
1
0
998
98
2
1
1
1
1
1
0
0
0
0
0
985
90
These solutions were found with a TS that used simple moves consisting of ﬂipping one
variable at a time. Both of these solutions are feasible. Suppose that solution 2 is chosen
as the initiating solution and that solution 1 is the guiding solution. The relinking process
starts by identifying the values that are common to both solutions. These correspond to
variables 2, 3, and 5. Swap moves are used during the path relinking process in order to
keep the intermediate solutions close to the feasibility boundary. A swap in this context
corresponds to adding one item to the knapsack while taking another item out of the
knapsack. This means that a variable that is set to 1 in the initiating solution and to 0
in the guiding solution must be paired with a variable that is set to 0 in the initiating
solution and to 1 in the guiding solution. The pairs in our example are (1, 7), (1, 9), (4, 7),
and (4, 9). The next table shows the intermediate solutions that result from these swaps.
swap
1
2
3
4
5
6
7
8
9
10
proﬁt
weight
(1, 7)
0
1
1
1
1
0
1
0
0
0
1018
96
(1, 9)
0
1
1
1
1
0
0
0
1
0
1098
106
(4, 7)
1
1
1
0
1
0
1
0
0
0
885
82
(4, 9)
1
1
1
0
1
0
0
0
1
0
965
92
The best feasible swap is (1, 7) and the search moves to the intermediate solution with
proﬁt 1018 and weight 96. After the move, there is only one swap left to reach the guiding
solution, namely (4, 9). In this case, the process produced only one intermediate solution;
however, in general, path relinking visits more solutions during the transformation of the
initiating solution into the guiding solution.
REFERENCES
Printed Resources:
[AbEtal06] S. Abdullah, S. Ahmadi, E. K. Burke, and M. Dror, “Investigating Ahuja-
Orlin’s large neighborhood search approach for examination timetabling”, OR Spec-
trum 29 (2006), 351–372.
[AdGu05] G. Adomavicius and A. Gupta, “Toward comprehensive real-time bidder sup-
port in iterative combinatorial auctions”, Information Systems Research 16 (2005),
169–185.
[AgEtal03] R. Agarwal, R. K. Ahuja, G. Laporte, and Z. J. Shen, “A composite very large-
scale neighborhood search algorithm for the vehicle routing problem”, in Handbook
of Scheduling: Algorithms, Models and Performance Analysis, J. Y-T. Leung (ed.),
CRC Press, 2003, Chapter 49.
[AgEtal07] R. Agarwal, ¨O. Ergun, J. B. Orlin, and C. N. Potts, “Solving parallel machine
scheduling problems with very large-scale neighborhood search”, working paper, In-
dustrial and Systems Engineering School, Georgia Institute of Technology, 2007.

REFERENCES
1251
[AhEtal02] R. K. Ahuja, ¨O. Ergun, J. B. Orlin, and A. P. Punnen, “A survey of very large-
scale neighborhood search techniques”, Discrete Applied Mathematics 123 (2002),
75–102.
[AhEtal07] R. K. Ahuja, ¨O. Ergun, J. B. Orlin, and A. P. Punnen, “Very large-scale
neighborhood search: theory, algorithms, and applications”, in Approximation Al-
gorithms and Metaheuristics, T. F. Gonzalez (ed.), Chapman & Hall/CRC, 2007,
Chapter 20.
[AhEtal04a] R. K. Ahuja, J. Goodstein, J. Liu, A. Mukherjee, and J. B. Orlin, “A neigh-
borhood search algorithm for the combined through ﬂeet assignment model with
time windows”, Networks 44 (2004), 160–171.
[AhEtal07a] R. K. Ahuja, J. Goodstein, A. Mukherjee, J. B. Orlin, and D. Sharma,
“A very large-scale neighborhood search algorithm for the combined through-ﬂeet
assignment model”, INFORMS Journal on Computing 19 (2007), 416–428.
[AhEtal07b] R. K. Ahuja, W. Huang, H. E. Romeijn, and D. Romero Morales, “A heuris-
tic approach to the multi-period single-sourcing problem with production and inven-
tory capacities and perishability constraints”, INFORMS Journal on Computing 19
(2007), 14–26.
[AhJhLi07] R. K. Ahuja, K. C. Jha, and J. Liu, “Solving real-life railroad blocking prob-
lems”, Interfaces 37 (2007), 404–419.
[AhEtal07c] R. K. Ahuja, K. C. Jha, J. B. Orlin, and D. Sharma, “A very large-scale
neighborhood search algorithm for the quadratic assignment problem”, INFORMS
Journal on Computing 19 (2007), 646–657.
[AhEtal08] R. K. Ahuja, A. Kumar, K. C. Jha, and J. B. Orlin, “Exact and heuristic algo-
rithms for the weapon-target assignment problem”, Operations Research 55 (2008),
1136–1146.
[AhMaOr93] R. K. Ahuja, T. L. Magnanti, and J. B. Orlin, Network Flows: Theory,
Algorithms and Applications, Prentice Hall, 1993.
[AhEtal04b] R. K. Ahuja, J. B. Orlin, S. Pallottino, M. P. Scaparra, and M.G. Scutell`a, “A
multi-exchange heuristic for the single-source capacitated facility location problem”,
Management Science 50 (2004), 749–760.
[AhOrSh01] R. K. Ahuja, J. B. Orlin, and D. Sharma, “Multi-exchange neighborhood
structures for the capacitated minimum spanning tree problem”, Mathematical Pro-
gramming 91 (2001), 71–97.
[AhOrSh03] R. K. Ahuja, J. B. Orlin, and D. Sharma, “A composite very large-scale
neighborhood structure for the capacitated minimum spanning tree problem”, Op-
erations Research Letters 31 (2003), 185–194.
[AlNoWo07] M. H. Albert, R. J. Nowakowski, and D. Wolfe, Lessons in Play: An Introduc-
tion to Combinatorial Game Theory, A K Peters/CRC Press, 2007. (An introduction
to combinatorial game theory.)
[AlEtal09] D. Aloise, A. Deshpande, P. Hansen, and P. Popat, “NP-hardness of Euclidean
sum-of-squares clustering”, Machine Learning 75 (2009), 245–248.
[AlEtal04] A. C. F. Alvim, C. C. Ribeiro, F. Glover, and D. J. Aloise, “A hybrid improve-
ment heuristic for the one-dimensional bin packing problem”, Journal of Heuristics
10 (2004), 205–229.

1252
Chapter 16
DISCRETE OPTIMIZATION
[AmScSc09] D. Ambrosino, A. Sciomachen, and M. G. Scutell`a, “A heuristic based on
multi-exchange techniques for a regional ﬂeet assignment location-routing problem”,
Computers & Operations Research 36 (2009), 442–460.
[ArHeSp06] C. Archetti, A. Hertz, and M. G. Speranza, “A tabu search algorithm for the
split delivery vehicle routing problem”, Transportation Science 40 (2006), 64–73.
[Au04] L. M. Ausubel, “An eﬃcient ascending-bid auction for multiple objects”, Ameri-
can Economic Review 94 (2004), 1452–1475.
[AuBa14] L. M. Ausubel and O. V. Baranov, “Market design and the evolution of the
combinatorial clock auction”, American Economic Review 104 (2014), 446–451.
[AuCr11] L. M. Ausubel and P. Cramton, “Multiple factor auction design for wind rights”,
Report to Bureau of Ocean Energy Management, Regulation and Enforcement, 2011.
[AuCtMi06] L. Ausubel, P. Cramton, and P. Milgrom, “The clock-proxy auction: a
practical combinatorial auction design”, in Combinatorial Auctions, P. Cramton,
Y. Shoham, and R. Steinberg (eds.), MIT Press, 2006, Chapter 5.
[AuMi02] L. M. Ausubel and P. R. Milgrom, “Ascending auctions with package bidding”,
Frontiers of Theoretical Economics 1 (2002), 1–42.
[BaPa76] E. Balas and M. W. Padberg, “Set partitioning: a survey”, SIAM Review 18
(1976), 710–760.
[BaZe80] E. Balas and E. Zemel, “An algorithm for large zero-one knapsack problems”,
Operations Research 28 (1980), 1130–1154.
[BaCoPr95] M. O. Ball, C. J. Colbourn, and J. S. Provan, “Network reliability” in Net-
work Models, M. Ball, T. Magnanti, C. Monma, and G. Nemhauser (eds.), North-
Holland, 1995, 673–762.
[BaHa94] T. Basar and A. Haurie (eds.), Advances in Dynamic Games and Applications,
Birkh¨auser, 1994.
[BaJaSh90] M. S. Bazaraa, J. J. Jarvis, and H. D. Sherali, Linear Programming and
Network Flows, 2nd ed., Wiley, 1990.
[BeKaSt92] W. W. Bein, J. Kamburowski, and M. F. M. Stallmann, “Optimal reduction
of two-terminal directed acyclic graphs”, SIAM Journal on Computing 21 (1992),
1112–1129.
[Be57] C. Berge, “Two theorems in graph theory”, Proceedings of the National Academy
of Sciences USA 43 (1957), 842–844.
[BeFu99] P. Berman and T. Fujito, “On approximation properties of the independent set
problem for low degree graphs”, Theory of Computing Systems 32 (1999), 115–132.
[BhSh14] B. Bhattacharya and Q. Shi,“Improved algorithms to network p-center location
problems”, Computational Geometry 47 (2014), 307–315.
[BiShZi13] M. Bichler, P. Shabalin, and G. Ziegler, “Eﬃciency with linear prices? A
game-theoretical and computational analysis of the combinatorial clock auction”,
Information Systems Research 24 (2013), 394–417.
[BiEtAl11] S. Bikhchandani, S. de Vries, J. Schummer, and R. Vohra, “An ascending
Vickrey auction for selling bases of a matroid”, Operations Research 59 (2011), 400–
413.
[BiOs02] S. Bikhchandani and J. M. Ostroy, “The package assignment model”, Journal
of Economic Theory 107 (2002), 377–406.

REFERENCES
1253
[BiKiTa93] K. Binmore, A. Kirman, and P. Tani (eds.), Frontiers of Game Theory, MIT
Press, 1993.
[BlPuEl14] V. Blanco, J. Puerto, and S. El Haj Ben Ali, “Revisiting several problems
and algorithms in continuous location with ℓτ norms”, Computational Optimization
and Applications 58 (2014), 563–595.
[BlEtal04] J. Blazewicz, P. Formanowicz, M. Kasprzak, W. T. Markiewicz, and A. Swierc,
“Tabu search algorithm for DNA sequencing by hybridization with isothermic li-
braries”, Computational Biology and Chemistry 28 (2004), 11–19.
[BlEtal00] J. Blazewicz, P. Formanowicz, M. Kasprzak, W. T. Markiewicz, and J. Weglarz,
“Tabu search for DNA sequencing with false negatives and false positives”, European
Journal of Operational Research 125 (2000), 257–265.
[BlGlKa04] J. Blazewicz, F. Glover, and M. Kasprzak, “DNA sequencing—tabu and
scatter search combined”, INFORMS Journal on Computing 16 (2004), 232–240.
[BoOr05] A. Bompadre and J. B. Orlin, “Using grammars to generate very large-scale
neighborhoods for the traveling salesman problem and other sequencing problems”,
Proceedings of the Eleventh International Conference on Integer Programming and
Combinatorial Optimization (IPCO), 2005, 437–451.
[Bo89] K. C. Border, Fixed Point Theorems with Applications to Economics and Game
Theory, Cambridge University Press, 1989.
[BoHo01] C. Boutilier and H. H. Hoos, “Bidding languages for combinatorial auctions”,
Proceedings of the Seventeenth International Joint Conference on Artiﬁcial Intelli-
gence, 2001, 1211–1217.
[BrCh89] M. L. Brandeau and S. S. Chiu, “An overview of representative problems in
location research”, Management Science 35 (1989), 645–674.
[BrRy13] M. Broom and J. Rycht´aˇr, Game-Theoretical Models in Biology, CRC Press,
2013.
[BrEtal11] T. Brueggemann, J. Hurink, T. Vredeveld, and G. Woeginger, “Exponential
size neighborhoods for makespan minimization scheduling”, Naval Research Logistics
58 (2011), 795–803.
[CaLaYa15] H. Calik, M. Labb´e, and H. Yaman, “p-Center Problems”, in Location Sci-
ence, G. Laporte, S. Nickel, and F. Saldanha da Gama (eds.), Springer, 2015, 79–92.
[CaPe06] E. Cantillon and M. Pesendorfer, “Auctioning bus routes: the London experi-
ence”, in Combinatorial Auctions, P. Cramton, Y. Shoham, and R. Steinberg (eds.),
MIT Press, 2006, Chapter 22.
[CaVi90] J. Carlier and P. Villon, “A new heuristic for the traveling salesman problem”,
RAIRO - Operations Research 24 (1990), 245–253.
[CaEtal03] F. Caro, M. Constantino, I. Martins, and A. Weintraub, “A 2-opt tabu search
procedure for the multiperiod forest harvesting problem with adjacency, green-up,
old growth and even ﬂow constraints”, Forest Science 49 (2003), 738–751.
[Ch83] V. Chv´atal, Linear Programming, Freeman, 1983.
[CiBaSa10] M. Ciarleglio, J. W. Barnes, and S. Sarkar, “ConsNet—A tabu search ap-
proach to the spatially coherent conservation area network design problem”, Journal
of Heuristics 16 (2010), 537–557.

1254
Chapter 16
DISCRETE OPTIMIZATION
[CoEtal13] E. G. Coﬀman, J. Csirik, G. Galambos, S. Martello, and D. Vigo, “Bin packing
approximation algorithms: survey and classiﬁcation”, in Handbook of Combinatorial
Optimization, P. M. Pardalos, D. Z. Du, and R. L. Graham (eds.), Springer, 2013,
455–531.
[CoGaJo84] E. G. Coﬀman, M. R. Garey, and D. S. Johnson, “Approximation algorithms
for bin-packing: an updated survey”, in Algorithm Design for Computer System
Design, G. Ausiello, M. Lucertini, and P. Seraﬁni (eds.), Springer-Verlag, 1984, 49–
106.
[CoCoOt09] J. P. Cohen, C. C. Coughlin, and L. S. Ott, “Auctions as a vehicle to reduce
airport delays and achieve value capture”, Federal Reserve Bank of St. Louis Review
91 (2009), 569–587.
[CoPoVe02] R. K. Congram, C. N. Potts, and S. L. van de Velde, “An iterated dynasearch
algorithm for the single machine total weighted tardiness scheduling problem”, IN-
FORMS Journal on Computing 14 (2002), 52–67.
[Cr03] R. Cressman, Evolutionary Dynamics and Extensive Form Games, MIT Press,
2003.
[CuAh05] C. B. Cunha and R. K. Ahuja, “Very large-scale neighborhood search for the
K-constrained multiple knapsack problem”, Journal of Heuristics 11 (2005), 465–481.
[DaRoLe05] E. Danna, E. Rothberg, and C. Le Pape, “Exploring relaxation induced
neighborhoods to improve MIP solutions”, Mathematical Programming 102 (2005),
71–90.
[DaMa15] M. S. Daskin and K. L. Maass, “The p-median problem”, in Location Science,
G. Laporte, S. Nickel, and F. Saldanha da Gama (eds.), Springer, 2015, 21–45.
[DaEtal07] A. Davenport, J. Kalagnanam, C. Reddy, S. Siegel, and J. Hou, “An applica-
tion of constraint programming to generating detailed operations schedules for steel
manufacturing”, Proceedings of Constraint Programming, 2007, 64–75.
[DaMi08] R. Day and P. Milgrom, “Core-selecting package auctions”, International Jour-
nal of Game Theory 36 (2008), 393–407.
[DaRa07] R. W. Day and S. Raghavan, “Fair payments for eﬃcient allocations in public
sector combinatorial auctions”, Management Science 53 (2007), 1389–1406.
[DeWo00] V. G. De˘ineko and G. J. Woeginger, “A study of exponential neighborhoods
for the Travelling Salesman Problem and for the Quadratic Assignment Problem”,
Mathematical Programming 87 (2000), 519–542.
[DeGaSo86] G. Demange, D. Gale, and M. Sotomayor, “Multi-item auctions”, Journal of
Political Economy 94 (1986), 863–872.
[DeHe92] E. Demeulemeester and W. S. Herroelen, “A branch-and-bound procedure for
the multiple resource-constrained project scheduling problem”, Management Science
38 (1992), 1803–1818.
[DeHe96] B. De Reyck and W. S. Herroelen, “On the use of the complexity index as
a measure of complexity in activity networks”, European Journal of Operational
Research 91 (1996), 347–366.
[DeHe90] J. De Wit and W. S. Herroelen, “An evaluation of microcomputer-based soft-
ware packages for project management”, European Journal of Operational Research
49 (1990), 102–139.

REFERENCES
1255
[DoEtal00] E. Dockner, S. Jørgensen, N. V. Long, and G. Sorger, Diﬀerential Games in
Economics and Management Science, Cambridge University Press, 2000.
[DoPe94] U. Dorndorf and E. Pesch, “Fast clustering algorithms”, ORSA Journal of
Computing 6 (1994), 141–153.
[Do98] K. A. Dowsland, “Nurse scheduling with tabu search and strategic oscillation”,
European Journal of Operational Research 106 (1998), 393–407.
[Dr81] M. Dresher, The Mathematics of Games of Strategy: Theory and Applications,
Dover, 1981. (First published in 1961.)
[Dr15] Z. Drezner, “The fortiﬁed Weiszfeld algorithm for solving the Weber problem”,
IMA Journal of Management Mathematics 26 (2015), 1–9.
[DrLe86] M. Dror and L. Levy, “A vehicle routing improvement algorithm comparison
of a greedy and a matching implementation for inventory routing”, Computers &
Operations Research 13 (1986), 33–45.
[Ed65] J. Edmonds, “Paths, trees and ﬂowers”, Canadian Journal of Mathematics 17
(1965), 449–467.
[El77] S. E. Elmaghraby, Activity Networks: Project Planning and Control by Network
Models, Wiley, 1977.
[El89] S. E. Elmaghraby, “The estimation of some network parameters in the PERT
model of activity networks: review and critique”, in R. S lowinski and J. Weglarz
(eds.), Advances in Project Scheduling, Elsevier, 1989, 371–432.
[ElKa90] S. E. Elmaghraby and J. Kamburowski, “On project representation and activity
ﬂoats”, Arabian Journal of Science and Engineering 15 (1990), 627–637.
[ElKa92] S. E. Elmaghraby and J. Kamburowski, “The analysis of activity networks
under generalized precedence relations (GPRs)”, Management Science 38 (1992),
1245–1263.
[ErOrSt06] ¨O. Ergun, J. B. Orlin, and A. Steele-Feldman, “Creating very large-scale
neighborhoods out of smaller ones by compounding moves”, Journal of Heuristics 12
(2006), 115–140.
[FiMa82] C. M. Fiduccia and R. M. Mattheyses, “A linear time heuristic for improv-
ing network partitions”, Proceedings of ACM IEEE Nineteenth Design Automation
Conference, IEEE Computer Society, 1982, 175–181.
[Fo15] R. Fourer, “Linear programming: software survey”, OR/MS Today 42/3 (2015),
52–63.
[FrMcWh92] R. L. Francis, L. F. McGinnis, Jr., and J. A. White, Facility Layout and
Location: An Analytical Approach, Prentice-Hall, 1992.
[FrNeSc04] A. Frangioni, E. Necciari, and M. G. Scutell`a, “Multi-exchange algorithms
for minimum makespan machine scheduling problems”, Journal of Combinatorial
Optimization 8 (2004), 195–220.
[FrHa05] A. Fr´eville and S. Hanaﬁ,“The multidimensional 0-1 knapsack problem—bounds
and computational aspects”, Annals of Operations Research 139 (2005), 195–227.
[Fr90] J. W. Friedman, Game Theory with Applications to Economics, 2nd ed., Oxford
University Press, 1990.
[GaNe72] R. S. Garﬁnkel and G. L. Nemhauser, Integer Programming, Wiley, 1972.

1256
Chapter 16
DISCRETE OPTIMIZATION
[Ga85] S. I. Gass, Linear Programming: Methods and Applications, 5th ed., Boyd &
Fraser, 1985.
[GeEtal06a] M. Gendreau, F. Guertin, J-Y. Potvin, and R. Seguin, “Neighborhood search
heuristics for a dynamic vehicle dispatching problem with pick-ups and deliveries”,
Transportation Research Part C 14 (2006), 157–174.
[GeEtal06b] M. Gendreau, M. Iori, G. Laporte, and S. Martello, “A tabu search algorithm
for a routing and container loading problem”, Transportation Science 40 (2006), 342–
350.
[GeEtal08] M. Gendreau, M. Iori, G. Laporte, and S. Martello, “A tabu search heuristic
for the vehicle routing problem with two-dimensional loading constraints”, Networks
51 (2008), 4–18.
[Gl86] F. Glover, “Future paths for integer programming and links to artiﬁcial intelli-
gence”, Computers & Operations Research 13 (1986), 533–549.
[Gl90] F. Glover, “Tabu search: a tutorial”, Interfaces 20 (1990), 74–94.
[Gl96] F. Glover, “Ejection chains, reference structures, and alternating path algorithms
for traveling salesman problems”, Discrete Applied Mathematics 65 (1996), 223–253.
[GlLa97] F. Glover and M. Laguna, Tabu Search, Springer, 1997.
[GlLa02] F. Glover and M. Laguna, “Tabu search”, in Handbook of Applied Optimization,
P. M. Pardalos and M. G. C. Resende (eds.), Oxford University Press, 2002, 194–208.
[GlLa13] F. Glover and M. Laguna, “Tabu search”, in Handbook of Combinatorial Op-
timization, 2nd ed., D. Z. Du and P. M. Pardalos (eds.), Springer, 2013, 3261–3362.
[GlLaMa07] F. Glover, M. Laguna, and R. Mart´ı, “Principles of tabu search”, in Approx-
imation Algorithms and Metaheuristics, T. Gonzalez (ed.), Chapman & Hall/CRC,
2007, Chapter 23.
[GoRo14] M. X. Goemans and T. Rothvoß, “Polynomiality for bin-packing with a con-
stant number of item types”, Proceedings of the 25th Annual ACM-SIAM Sympo-
sium on Discrete Mathematics, 2014, 830–839.
[GoEtAl15] A. Goetzendorﬀ, M. Bichler, P. Shabalin, and R. W. Day, “Compact bid
languages and core pricing in large multi-item auctions”, Management Science 61
(2015), 1684–1703.
[GoGaFi10] J. Gonz´alez-D´ıaz, I. Garc´ıa-Jurado, and M. G. Fiestras-Janeiro, An Intro-
ductory Course on Mathematical Game Theory, American Mathematical Society,
2010.
[GoEtAl14] D. R. Goossens, S. Onderstal, J. Pijnacker, and F. C. R. Spieksma, “Solids:
a combinatorial auction for real estate”, Interfaces 44 (2014), 351–363.
[GrLa77] J. Green and J-J. Laﬀont, “Characterization of satisfactory mechanisms for the
revelation of preferences for public goods”, Econometrica 45 (1977), 427–438.
[GuSt00] F. Gul and E. Stacchetti, “The English auction with diﬀerentiated commodi-
ties”, Journal of Economic Theory 92 (2000), 66–95.
[Gu91] R. Guy, ed., Combinatorial Games, Proceedings of Symposia in Applied Mathe-
matics, American Mathematical Society, Volume 43, 1991.
[HaMi79] G. Y. Handler and P. B. Mirchandani, Location on Networks: Theory and
Algorithms, MIT Press, 1979.

REFERENCES
1257
[He00] K. Helsgaun, “An eﬀective implementation of the Lin-Kernighan traveling sales-
man heuristic”, European Journal of Operational Research 126 (2000), 106–130.
[HeDeDe98] W. S. Herroelen, B. De Reyck, and E. Demeulemeester, “Resource-con-
strained project scheduling: a survey of recent developments”, Computers & Opera-
tions Research 25 (1998), 279–302.
[HoLe10] S. C. Ho and J. M. Y. Leung, “Solving a manpower scheduling problem for
airline catering using metaheuristics”, European Journal of Operational Research
202 (2010), 903–921.
[Ho97] D. S. Hochbaum, Approximation Algorithms for NP-Hard Problems, PWS Pub-
lishing Company, 1997.
[HoPa93] K. Hoﬀman and M. W. Padberg, “Solving airline crew scheduling problems by
branch-and-cut”, Management Science 39 (1993), 657–682.
[JoMc97] D. S. Johnson and L. A. McGeoch, “The travelling salesman problem: a case
study in local optimization”, in Local Search in Combinatorial Optimization, E. H.
L. Aarts and J. K. Lenstra (eds.), Wiley, 1997, 311–336.
[Ka92] J. Kamburowski, “Bounding the distribution of project duration in PERT net-
works”, Operations Research Letters 12 (1992), 17–22.
[KaSpDr92] J. Kamburowski, A. Sprecher, and A. Drexl, “Characterization and gener-
ation of a general class of resource-constrained project scheduling problems: easy
and hard instances”, Research Report N0301, Institut f¨ur Betriebswirtschaftslehere,
Christian-Albrechts Universit¨at zu Kiel, 1992.
[KaHa79a] O. Kariv and S. L. Hakimi, “An algorithmic approach to network location
problems. I: the p-centers”, SIAM Journal on Applied Mathematics 37 (1979), 513–
538.
[KaHa79b] O. Kariv and S. L. Hakimi, “An algorithmic approach to network location
problems. II: the p-medians”, SIAM Journal on Applied Mathematics 37 (1979),
539–560.
[KePfPi04] H. Kellerer, U. Pferschy, and D. Pisinger, Knapsack Problems, Springer Sci-
ence & Business Media, 2004.
[KeLi70] B. Kernighan and S. Lin, “An eﬃcient heuristic procedure for partitioning
graphs”, Bell System Technical Journal 49 (1970), 291–307.
[KiOlWe14] S. W. Kim, M. Olivares, and G. Y. Weintraub, “Measuring the performance
of large-scale combinatorial auctions: a structural estimation approach”, Manage-
ment Science 60 (2014), 1180–1201.
[KoPa01] R. Kolisch and R. Padman, “An integrated survey of deterministic project
scheduling”, Omega 29 (2001), 249–272.
[KoSp97] R. Kolisch and A. Sprecher, “PSPLIB – A project scheduling problem library”,
European Journal of Operational Research 96 (1997), 205–216.
[KoFa10] J. E. Korsvik and K. Fagerholt, “A tabu search heuristic for ship routing and
scheduling with ﬂexible cargo quantities”, Journal of Heuristics 16 (2010), 117–137.
[KoFaLa10] J. E. Korsvik, K. Fagerholt, and G. Laporte, “A tabu search heuristic for
ship routing and scheduling”, Journal of the Operational Research Society 61 (2010),
594–603.
[KoVy12] B. Korte and J. Vygen, Combinatorial Optimization, 3rd edition, Springer,
2012.

1258
Chapter 16
DISCRETE OPTIMIZATION
[KuAd86] V. G. Kulkarni and V. G. Adlakha, “Markov and Markov-regenerative PERT
networks”, Operations Research 34 (1986), 769–781.
[Le83] H. B. Leonard, “Elicitation of honest preferences for the assignment of individuals
to positions”, Journal of Political Economy 91 (1983), 461–479.
[LiEtal05] B. Lin, S. Chavali, K. Camarda, and D. C. Miller, “Computer-aided molecular
design using tabu search”, Computers & Chemical Engineering 29 (2005), 337–347.
[LiKe73] S. Lin and B. Kernighan, “An eﬀective heuristic algorithm for the traveling
salesman problem”, Operations Research 21 (1973), 498–516.
[LiMi04a] B. Lin and D. C. Miller, “Solving heat exchanger network synthesis problems
with tabu search”, Computers & Chemical Engineering 28 (2004), 1451–1464.
[LiMi04b] B. Lin and D. C. Miller, “Tabu search algorithm for chemical process opti-
mization”, Computers & Chemical Engineering 28 (2004), 2287–2306.
[LuRa57] R. D. Luce and H. Raiﬀa, Games and Decisions, Wiley, 1957. (Covers much
the same ground as [vNMo53] but at a more elementary level.)
[Ma86] O. Maimon, “The variance equity measures in location decisions on trees”, Annals
of Operations Research 6 (1986), 147–160.
[MaPiTo99] S. Martello, D. Pisinger, and P. Toth, “Dynamic programming and strong
bounds for the 0-1 knapsack problem”, Management Science 45 (1999), 414–424.
[MaTo90] S. Martello and P. Toth, Knapsack Problems: Algorithms and Computer Im-
plementations, Wiley, 1990.
[Ma82] J. Maynard Smith, Evolution and the Theory of Games, Cambridge University
Press, 1982. (The founding treatise on evolutionary game theory.)
[Me83] N. Megiddo, “Linear-time algorithms for linear programming in R3 and related
problems”, SIAM Journal on Computing 12 (1983), 759–776.
[MeSu84] N. Megiddo and K. J. Supowit, “On the complexity of some common geometric
location problems”, SIAM Journal on Computing 13 (1984), 182–196.
[Me93] M. Mesterton-Gibbons, “Game-theoretic resource modeling”, Natural Resource
Modeling 7 (1993), 93–147. (A survey of game theory and applications accessible to
the general reader.)
[Me01] M. Mesterton-Gibbons, An Introduction to Game-Theoretic Modelling, 2nd ed.,
American Mathematical Society, 2001. (A uniﬁed introduction to classical and evo-
lutionary game theory.)
[MiKaSt93] D. J. Michael, J. Kamburowski, and M. Stallmann, “On the minimum dummy-
arc problem”, RAIRO Recherche Op´erationnelle 27 (1993), 153–168.
[Mi00] P. Milgrom, “Putting auction theory to work: the simultaneous ascending auc-
tion”, Journal of Political Economy 108 (2000), 245–272.
[MiFr90] P. B. Mirchandani and R. L. Francis (eds.), Discrete Location Theory, Wiley,
1990.
[MiPa07] D. Mishra and D. Parkes, “Ascending price Vickrey auctions for general valu-
ations”, Journal of Economic Theory 132 (2007), 335–366.
[My91] R. B. Myerson, Game Theory: Analysis of Conﬂict, Harvard University Press,
1991. (General introduction to economic game theory.)

REFERENCES
1259
[NaBr09] Y. Nagata and O. Br¨aysy, “A powerful route minimization heuristic for the
vehicle routing problem with time windows”, Operations Research Letters 37 (2009),
333–338.
[NeJhAh09] A. K. Nemani, K. C. Jha, and R. K. Ahuja, “The load planning problem at an
intermodal railroad terminal”, working paper, Industrial and Systems Engineering,
University of Florida, 2009.
[NeWo88] G. L. Nemhauser and L. A. Wolsey, Integer and Combinatorial Optimization,
Wiley, 1988.
[Ne90] K. Neumann, Stochastic Project Networks: Temporal Analysis, Scheduling, and
Cost Minimization, Lecture Notes in Economics and Mathematical Systems 344,
Springer-Verlag, 1990.
[NiPu05] S. Nickel and J. Puerto, Location Theory: A Uniﬁed Approach, Springer, 2005.
[Ni06] N. Nisan, “Bidding languages”, in Combinatorial Auctions, P. Cramton, Y. Shoham,
and R. Steinberg (eds.), MIT Press, 2006, Chapter 9.
[Og09] W. Ogryczak, “Inequality measures and equitable locations”, Annals of Opera-
tions Research 167 (2009), 61–86.
[OpLø08] J. Oppen and A. Løkketangen, “A tabu search approach for the livestock
collection problem”, Computers & Operations Research 35 (2008), 3213–3229.
[Ow13] G. Owen, Game Theory, 4th ed., Emerald Group Publishing Limited, 2013. (An
overview of the mathematical theory of games.)
[PaEtal07] D. C. Paraskevopoulos, P. P. Repoussis, C. D. Tarantilis, G. Ioannou, and G.
P. Prastacos, “A reactive variable neighborhood tabu search for the heterogeneous
ﬂeet vehicle routing problem with time windows”, Journal of Heuristics 14 (2007),
247–254.
[PaUn00] D. C. Parkes and L. H. Ungar, “Iterative combinatorial auctions: theory and
practice”, Proceedings of the 17th National Conference on Artiﬁcial Intelligence,
2000, 74–81.
[Pa83] J. H. Patterson, “Exact and heuristic solution procedures for the constrained-
resource, project scheduling problem: Volumes I, II, and III”, Research Monograph,
privately circulated, 1983.
[PeGe99] G. Pesant and M. Gendreau, “A constraint programming framework for local
search methods”, Journal of Heuristics 5 (1999), 255–279
[Pe08] H. Peters, Game Theory: A Multi-Leveled Approach, Springer, 2008.
[PeZiBi13] I. Petrakis, G. Ziegler, and M. Bichler, “Ascending combinatorial auctions
with allocation constraints: on game theoretical and computational properties of
generic pricing rules”, Information Systems Research 24 (2013), 768–786.
[PiRo10] D. Pisinger and S. Ropke, “Large neighborhood search”, in Handbook of Meta-
heuristics, M. Gendreau and J-Y. Potvin (eds.), Springer, 2010, 399–419.
[Pl11] F. Plastria, “The Weiszfeld algorithm: proof, amendments, and extensions”, in
Foundations of Location Analysis, H. A. Eiselt and V. Marianov. (eds.), Springer,
2011, 357–389.
[PoVe95] C. N. Potts and S. L. van de Velde, “Dynasearch: iterative local improvement by
dynamic programming: Part I, the traveling salesman problem”, Technical Report,
University of Twente, 1995.

1260
Chapter 16
DISCRETE OPTIMIZATION
[RaSmBu82] S. J. Rassenti, V. L. Smith, and R. L. Bulﬁn, “A combinatorial auction
mechanism for airport time slot allocation”, The Bell Journal of Economics 13 (1982),
402–417.
[Re98] C. Rego, “A subpath ejection method for the vehicle routing problem”, Manage-
ment Science 44 (1998), 1447–1459.
[RoVa13] A. M. Rodr´ıguez-Ch´ıa and C. Valero-Franco, “On the global convergence of a
generalized iterative procedure for the minisum location problem with ℓp distances
for p > 2”, Mathematical Programming 137 (2013), 477–502.
[RoTeVi10] C. Roos, T. Terlaky, and J-Ph. Vial, Interior Point Methods for Linear Op-
timization, 2nd ed., Springer, 2010.
[RoPeHa98] M. H. Rothkopf, A. Pekeˇc, and R. M. Harstad, “Computationally manage-
able combinational auctions”, Management Science 44 (1998), 1131–1147.
[Sa13] T. Sandholm, “Very-large-scale generalized combinatorial multi-attribute auctions:
lessons from conducting $60 billion of sourcing”, in The Handbook of Market Design,
N. Vulkan, A. E. Roth, and Z. Neeman (eds.), Oxford University Press, 2013, Chapter
16.
[Sa10] W. H. Sandholm, Population Games and Evolutionary Dynamics, MIT Press,
2010.
[ScPaSc04] M. P. Scaparra, S. Pallottino, and M. G. Scutell`a, “Large-scale local search
heuristics for the capacitated vertex p-center problem”, Networks 43 (2004), 241–255.
[ScSø09] P. Schittekat and K. Sørensen, “Supporting 3PL decisions in the automotive
industry by generating diverse solutions to a large-scale location-routing problem”,
Operations Research 57 (2009), 1058–1067.
[Sc10] U. Schneider, “A tabu search tutorial based on a real-world scheduling problem”,
Central European Journal of Operations Research 19 (2010), 467–493.
[Sc86] A. Schrijver, Theory of Linear and Integer Programming, Wiley, 1986.
[SeSiRo11] M. Sevaux, A. Singh, and A. Rossi, “Tabu search for multiprocessor schedul-
ing: application to high level synthesis”, Asia-Paciﬁc Journal of Operational Research
28 (2011), 201–212.
[Sh12] G. B. Shebl´e, Computational Auction Mechanisms for Restructured Power Indus-
try Operation, Springer, 2012.
[Si10] K. Sigmund, The Calculus of Selﬁshness, Princeton University Press, 2010.
[SiBa96] N. Simonetti and E. Balas, “Implementation of a linear time algorithm for cer-
tain generalized traveling salesman problems”, Proceedings of the 5th International
Conference on Integer Programming and Combinatorial Optimization (IPCO), 1996,
316–329.
[SiEtal05] S. Sindhuchao, H. E. Romeijn, E. Ak¸cali, and R. Boondiskulchok, “An in-
tegrated inventory-routing system for multi-item joint replenishment with limited
vehicle capacity”, Journal of Global Optimization 32 (2005), 93–118.
[SlWe89] R. S lowinski and J. Weglarz, eds., Advances in Project Scheduling, Elsevier,
1989.
[Sp94] A. Sprecher, Resource-Constrained Project Scheduling: Exact Methods for the
Multi-Mode Case, Lecture Notes in Economics and Mathematical Systems 409,
Springer-Verlag, 1994.

REFERENCES
1261
[Su12] M. Sun, “A tabu search heuristic procedure for the capacitated facility location
problem”, Journal of Heuristics 18 (2012), 91–118.
[Ta03] E. D. Taillard, “Heuristic methods for large centroid clustering problems”, Journal
of Heuristics 9 (2003), 51–73.
[Ta88] A. Tamir, “Improved complexity bounds for center location problems on networks
using dynamic data structures”, SIAM Journal on Discrete Mathematics 1 (1988),
377–396.
[Ta96] A. Tamir, “An O(pn2) algorithm for the p-median and related problems on tree
graphs”, Operations Research Letters 19 (1996), 59–64.
[ThOr89] P. M. Thompson and J. B. Orlin, “The theory of cyclic transfers”, Operations
Research Center Working Paper, MIT, 1989.
[ThPs93] P. M. Thompson and H. N. Psaraftis, “Cyclic transfer algorithms for multive-
hicle routing and scheduling problems”, Operations Research 41 (1993), 935–946.
[TrYi07] M. A. Trick and H. Yildiz, “Bender’s cuts guided large neighborhood search for
the traveling umpire problem”, in Integration of AI and OR Techniques in Constraint
Programming for Combinatorial Optimization Problems, P. Van Hentenryck and L.
Wolsey (eds.), Springer, 2007, 332–345.
[vHMu01] S. van Hoesel and R. M¨uller, “Optimization in electronic markets: examples
in combinatorial auctions”, Netnomics 3 (2001), 23–33.
[vNMo53] J. von Neumann and O. Morgenstern, Theory of Games and Economic Behav-
ior, 3rd ed., Princeton University Press, 1953. (The founding treatise on economic
game theory.)
[Wa88] J. Wang, The Theory of Games, Oxford University Press, 1988. (An overview of
the mathematical theory of games emphasizing bimatrix games and c-games.)
[We37] E. Weiszfeld, “Sur le point pour lequel la somme des distances de n points donn´es
est minimum”, Tohoku Mathematical Journal 43 (1937), 355–386.
[WePl09] E. Weiszfeld and F. Plastria, “On the point for which the sum of the distances
to n given points is minimum”, Annals of Operations Research 167 (2009), 7–41.
[We93] G. Wesolowsky, “The Weber problem: history and perspective”, Location Science
1 (1993), 5–23.
[Wi13] H. P. Williams, Model Building in Mathematical Programming, 5th ed., Wiley,
2013.
[Wi92] T. M. Williams, “Criticality of stochastic networks”, Operations Research 43
(1992), 353–357.
[Wr87] S. J. Wright, Primal-Dual Interior-Point Methods, SIAM, 1987.
[XuKe96] J. Xu and J. P. Kelly, “A network ﬂow-based tabu search heuristic for the
vehicle routing problem”, Transportation Science 30 (1996), 379–393.
[YaIbGl04] M. Yagiura, T. Ibaraki, and F. Glover, “An ejection chain approach for the
generalized assignment problem”, INFORMS Journal on Computing 16 (2004), 133–
151.
[YaEtal04] M. Yagiura, S. Iwasaki, T. Ibaraki, and F. Glover, “A very large-scale neigh-
borhood search algorithm for the multi-resource generalized assignment problem”,
Discrete Optimization 1 (2004), 87–98.

1262
Chapter 16
DISCRETE OPTIMIZATION
[YaYaIb99] M. Yagiura, T. Yamaguchi, and T. Ibaraki, “A variable-depth search algo-
rithm for the generalized assignment problem”, in Metaheuristics: Advances and
Trends in Local Search Paradigms for Optimization, S. Voss, S. Martello, I. H. Os-
man, and C. Roucairol (eds.), Kluwer, 1999, 459–471.
[ZuAmGi08] N. Zuﬀerey, P. Amstutz, and P. Giaccari, “Graph colouring approaches for
a satellite range scheduling problem”, Journal of Scheduling 11 (2008), 263–277.
Web Resources:
http://akira.ruc.dk/~keld/research/LKH/ (Helsgaun’s implementation of the Lin-
Kernighan heuristic (LKH) for solving the traveling salesman problem. Helsgaun’s
LKH has produced optimal solutions for all TSPLIB instances that have been solved
to optimality. Furthermore, Helsgaun’s LKH has improved the best known solutions
for a series of large-scale instances with unknown optima, among these a 1,904,711-
city instance that is known as the “World TSP”.)
http://crcv.ucf.edu/source/K-Means (MATLAB and C codes for the k-means algo-
rithm.)
http://glossary.computing.society.informs.org/ver2/mpgwiki/ (Excellent glos-
sary of linear programming terms, as well as concepts in general mathematical opti-
mization.)
http://optimization-suite.coin-or.org (COIN-OR open source software for opti-
mization problems.)
http://people.brunel.ac.uk/~mastjjb/jeb/info.html (OR-Library, a collection of
test datasets for a wide variety of operations research problems, including those for
the generalized assignment problem, the traveling salesman problem, and various
kinds of knapsack problems and vehicle routing problems.)
http://sourceforge.net/projects/lipside
(LiPS package for linear, integer, and
goal programming problems.)
http://wikipedia.org/wiki/Comparison of project management software (A com-
prehensive survey of models, data, and algorithms for project scheduling problems.)
http://www.aimms.com (AIMMS integrated solver and modeling environment.)
http://www.ampl.com (Modeling language for optimization problems.)
http://www.codeforge.com/article/214515 (C++ code for the dual simplex method.)
http://www.cplex.com/ (Software package CPLEX to solve LP and IP/MIP problems.)
http://www.cs.nott.ac.uk/~rxq/data.htm
(Test instances of diﬀerent timetabling
problems maintained by the Department of Computer Science and Information Tech-
nology at the University of Nottingham.)
http://www.di.unipi.it/optimize (Test instances for diﬀerent optimization problems
produced and maintained by the Operations Research Group in the Department of
Computer Science at the University of Pisa.)
http://www.diku.dk/~pisinger/codes.html (Optimization codes in C for knapsack
problems.)
http://www.gams.com (High-level modeling platform for optimization problems.)
http://www.gurobi.com
(Integrated solver and modeling environment for LP and
IP/MIP problems.)

REFERENCES
1263
http://www.ic.gc.ca/eic/site/smt-gst.nsf/eng/sf10726.html (Licensing frame-
work for broadband radio service in Canada, setting out details of the auction for-
mat.)
http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/ (TSPLIB, a widely ref-
erenced library of benchmark instances of the symmetric traveling salesman problem,
the Hamiltonian cycle problem, the asymmetric traveling salesman problem, the se-
quential ordering problem, and the capacitated vehicle routing problem.)
http://www.lindo.com/ (Software packages LINDO/LINGO to solve LP problems and
IP/MIP problems.)
http://www.mathworks.com/products/optimization (MATLAB toolbox for solving
optimization problems.)
http://www.neos-server.org/neos (Online solver for LPs using simplex and interior
point methods.)
http://www.netlib.org/toms/632 (Fortran code for solving knapsack problems.)
http://www.om-db.wi.tum.de/psplib/library.html (Contains benchmark problem
sets for various types of resource-constrained project scheduling problems.)


17
THEORETICAL COMPUTER
SCIENCE
17.1 Computational Models
Wayne Goddard
17.1.1 Finite State Machines
17.1.2 Pushdown Automata
17.1.3 Turing Machines
17.1.4 Parallel Computational Models
17.2 Computability
William Gasarch
17.2.1 Recursive Functions and Church’s Thesis
17.2.2 Recursive Sets and Decidable Problems
17.3 Languages and Grammars
Aarto Salomaa
17.3.1 Alphabets
17.3.2 Languages
17.3.3 Grammars and the Chomsky Hierarchy
17.3.4 Regular and Context-free Languages
17.3.5 Combinatorics on Words
17.4 Algorithmic Complexity
Thomas Cormen
17.4.1 Overview of Complexity
17.4.2 Worst-Case and Average-Case Analysis
17.4.3 Lower Bounds
17.5 Complexity Classes
Lane A. Hemaspaandra
17.5.1 Oracles and the Polynomial Hierarchy
17.5.2 Reducibility and NP-Completeness
17.5.3 Probabilistic Turing Machines
17.6 Randomized Algorithms
Milena Mihail
17.6.1 Overview and General Paradigms
17.6.2 Las Vegas and Monte Carlo Algorithms

1266
Chapter 17
THEORETICAL COMPUTER SCIENCE
INTRODUCTION
Theoretical computer science is concerned with modeling computational problems and
solving them algorithmically. It strives to distinguish what can be computed from what
cannot. If a problem can be solved by an algorithm, it is important to know the amount
of space and time needed.
GLOSSARY
abelian square: a word having the pattern xxp, where xp is a permutation of x.
acceptance probability (of input word by a probabilistic TM): the sum of the prob-
abilities over all acceptance paths of computation.
Ackermann function: a very rapidly growing function that is recursive, but not prim-
itive recursive.
algorithm: a ﬁnite list of instructions designed to accomplish a speciﬁed computation
or other task.
alphabet: a ﬁnite set of symbols.
ambiguous context-free grammar: a grammar whose language has a string having
two diﬀerent leftmost derivations.
analysis of an algorithm: an estimation of its cost of execution, especially of its run-
ning time.
antecedent of a production α →β: the string α that precedes the arrow.
average-case running time: the expected running time of an algorithm, usually ex-
pressed asymptotically in terms of the input size.
Backus-Naur (or Backus normal) form (BNF): a metalanguage used to specify
computer language syntax.
busy beaver function: the function BB(n) whose value is the maximum number of 1s
that an n-state Turing machine can print and still halt.
busy beaver machine, n-state: an n-state Turing machine on the alphabet Σ =
{#, 1} that accepts an input tape ﬁlled with blanks (#s) and halts after placing
a maximum number of 1s on the tape.
cellular automaton (n-dimensional): an interconnection network in which there is
a processor at each integer lattice point of n-dimensional Euclidean space, and each
processor communicates with its immediate neighbors.
characteristic function (of a language): the function on strings that has value yes for
elements in the language, and no otherwise.
characteristic function (of a set): the function whose value is 1 for elements of the
set, and 0 otherwise.
Chomsky hierarchy: four classes of grammars, with increasing restrictions.
Chomsky normal form (for a production rule): the form A →BC where B and C
are nonterminals or the form A →a where a is a terminal.

GLOSSARY
1267
Church’s thesis (or the Church-Turing thesis): the premise that the intuitive no-
tion of what is computable or partially computable should be formally deﬁned as
computable by a Turing machine.
code (for an alphabet): a language C of nonempty words, such that whenever a word w
can be written as a concatenation of words in C, the write-up is always unique.
That is, if w = x1 . . . xm = y1 . . . yn where xi, yj ∈C, then m = n and xi = yi for
i = 1, . . . , m.
code indicator (of a language): the sum of the code indicators of all words in the
language.
code indicator (of a word w ∈V ∗): the number ci(w) = |V |−|w|.
collapse (of the polynomial hierarchy to the ith rank): the circumstance that PH =
Σp
i , for some i ≥0.
common PRAM (or CRCWcom): a CRCW PRAM model in which concurrent writes
to the same location are permitted if all processors write the same data.
comparison sort: a sorting algorithm that uses only comparisons between keys to
determine the sorted order.
complement (of language L over alphabet V ): the language L, where complementa-
tion is taken with respect to V ∗.
C-complete language (where C is a class of languages): a language A such that A is
C-hard and A ∈C.
complexity (of an algorithm): an asymptotic measure of the number of operations or
the running time needed for a complete execution; sometimes, a measure of the total
amount of computational space needed.
complexity (of a function): usually, the minimum complexity of any algorithm repre-
senting the function; sometimes, the length or complicatedness of the list of instruc-
tions.
complexity (of a function), Kolmogorov-Chaitin type: a measure of the minimum
complicatedness of any algorithm representing the function, usually according to the
number of instructions in the algorithm (and not related to its running time).
coNP: the complexity class Πp
1, which contains every language A such that A ∈Σp
1.
concatenation (of two languages L1 and L2): the set {xy | x ∈L1, y ∈L2}, de-
noted L1L2.
concatenation (of two strings): the result of appending the second string to the right
end of the ﬁrst.
consequent (of a production α →β): the string β that follows the arrow.
context-free (or type 2) grammar: a grammar in which the antecedent of each pro-
duction is a single nonterminal.
context-sensitive (or type 1) grammar: a grammar where every production α →β
(except possibly S →λ) has the form α = uAv and β = uxv.
CRCW concurrent read concurrent write: a PRAM model in which both concur-
rent reads from and concurrent writes to the same location are allowed.
CREW concurrent read exclusive write: a PRAM model where concurrent reads
are allowed, but not concurrent writes to the same location.

1268
Chapter 17
THEORETICAL COMPUTER SCIENCE
cube (over an alphabet): a word having the pattern xxx.
derivation (of the string y from the string x): a sequence of substitutions, according
to the production rules, that transforms string y into string z. The notation x =⇒∗y
means that such a derivation exists.
deterministic ﬁnite automaton (DFA): a model of a computer for deciding mem-
bership in a set.
emptiness problem for grammars: deciding whether the language generated by a
grammar is empty.
empty string: the string of length zero, that is, the string with no symbols; often
written λ.
equivalence problem for grammars: deciding whether two grammars are equiva-
lent.
equivalent automata: automata that accept the same language.
equivalent grammars: grammars that generate the same language.
EREW exclusive read exclusive write: a PRAM model in which neither concur-
rent reads from nor concurrent writes to the same location are allowed.
existential lower bound (for an algorithm): a lower bound for its number of execu-
tion steps that holds for at least one input.
existential lower bound (for a problem): a lower bound for every algorithm that can
solve that problem.
ﬁnite automaton: a DFA or an NFA.
ﬁnite-state machine: a ﬁnite automaton or ﬁnite transducer.
ﬁnite-state machine with output: another name for a ﬁnite transducer.
ﬁniteness problem (for a grammar): deciding whether the language generated by that
grammar is ﬁnite.
ﬁnite transducer: a model of a computer for calculating a function, like a ﬁnite au-
tomaton, except that it also produces an output string each time it reads an input
symbol.
free monoid (generated by an alphabet): the set of all strings composable from sym-
bols in the alphabet, with the operation of string concatenation.
frequency (of a symbol in a string): the number of occurrences of the symbol in the
string.
Game of Life: a 2-dimensional cellular automaton designed by John H. Conway.
G¨odel numbering (of a set): a method for encoding Turing machines as products of
prime powers; more generally, a similar one-to-one recursive function on an arbitrary
set whose image in N is a recursive set.
grammar: a set of production rules of the form α →β that allow one to derive strings
by repeated substitution.
halting problem: the problem of designing an algorithm capable of deciding which
computations P(x) halt and which do not, where P is a computer program (or
Turing machine) and x is a possible input to P.
C-hard language (C a class of languages): a language A such that every language in
class C is polynomial-time reducible to A.

GLOSSARY
1269
Hilbert’s tenth problem: the (undecidable) problem of determining for an arbitrary
multivariate polynomial equation p(x1, . . . , xn) = 0 whether there exists a solution
consisting of integers.
inclusion problem for languages: deciding whether one language is included in an-
other.
inherently ambiguous context-free language: a context-free language such that
every context-free grammar for the language is ambiguous.
input size: the quantity of data supplied as input to a computation.
interconnection network model: a parallel computation model as a digraph in which
each vertex represents a processor, and in each phase, each processor communicates
with its neighbors and makes a computation.
inverse of a morphism: for morphism h: V ∗→U ∗, the mapping h−1 : U ∗→2V ∗
deﬁned for x ∈U ∗by h−1(x) = {y ∈V ∗| h(y) = x}.
Kleene closure (or Kleene star): of a language L, the set of all iterated concatena-
tions of zero or more words in L, denoted L∗.
language (accepted by a machine, e.g., ﬁnite automaton, PDA, or Turing): the set of
all accepted strings.
language (generated by the grammar G): the language L(G) of words consisting of ter-
minal symbols derivable from the start symbol.
language (over an alphabet V ): a subset of the free monoid V ∗; that is, a set of strings.
Las Vegas algorithm: an algorithm that always produces correct output, whose run-
ning time is a random variable.
Las Vegas to Monte Carlo transformation: the Monte Carlo algorithm obtained
by running the Las Vegas scheme for kE[T ] steps and halting, where E[T ] is the
expected Las Vegas running time.
leftmost derivation: a derivation in which at each step the leftmost nonterminal is
replaced.
leftmost language (generated by the grammar G): the language of strings of termi-
nals with leftmost derivations from the start symbol.
length set (of a language L): the set {|x| | x ∈L}.
length-increasing (or type 1) grammar: a grammar in which the consequent β of
each production α→β (except S→λ, if present) is at least as long as its antecedent α.
linear grammar: a context-free grammar where each consequent contains at most one
nonterminal.
Mealy machine: a ﬁnite transducer whose output function always produces a single
symbol.
membership problem (for a grammar G): given an input string x, deciding whether
x ∈L(G).
mirror image (of a language L): the language obtained by reversing every string in L.
Monte Carlo algorithm: an algorithm that has a bounded number of computational
steps and produces incorrect output with some low probability.
Monte Carlo to Las Vegas transformation: the Las Vegas algorithm of repeatedly
running that Monte Carlo algorithm until correct output occurs.

1270
Chapter 17
THEORETICAL COMPUTER SCIENCE
Moore machine: a Mealy machine such that for every state k and every pair of input
symbols s1 and s2, the outputs τ(k, s1) and τ(k, s2) are the same.
morphism (from the alphabet V to the alphabet U): a function s: V →U.
nondeterministic ﬁnite automaton (NFA): a model like a DFA, but there may be
several diﬀerent states to which a transition is possible, instead of only one.
nondeterministic polynomial-time computation on a TM: a computation such
that there exists a polynomial function p(n) such that for any input of size n there
is a computational path on the TM whose length is at most p(n) steps.
nondeterministic Turing machine: a model like a deterministic Turing machine, ex-
cept that the transition function ∆maps each state-symbol pair to a set of state-
symbol-direction triples.
nonterminal (in a grammar): a symbol that may be replaced when a production is
applied.
nontrivial family of languages: a family that contains at least one language diﬀerent
from ∅and {λ}.
NP: the complexity class of languages nondeterministically TM-decidable in polynomial
time.
NP-complete language: a language A such that A is NP-hard and A ∈NP.
NP-complete problem: a decision problem equivalent to deciding membership in an
NP-complete language.
NP-hard language: a language A such that every language in complexity class NP is
polynomial-time reducible to A.
oracle (for a language): a machine state that decides whether or not a given string is
in that language.
oracle Turing machine: like a Turing machine, except equipped with a special second
tape on which it can write a string to query an oracle for a ﬁxed language L.
P: the complexity class of languages deterministically TM-decidable in polynomial time.
palindrome: a string that is identical to its reverse.
parallel computation model: a computational model that permits more than one
instruction to be executed simultaneously, instead of requiring that they be executed
sequentially.
parsing a string: in theoretical computer science, a derivation.
partial function: an incomplete rule that assigns values to some elements in its domain
but not necessarily to all of them.
partial function φM induced by a TM M: the rule that associates to each input v
for which M’s computation halts the output φM(v), and is otherwise undeﬁned.
partial recursive function: a partial function derivable from the constant zero func-
tions ζn(x1, . . . , xn) = 0, the successor function σ(n) = n + 1, and the projection
functions πn
i (x1, . . . , xn) = xi, using multivariate composition, multivariate primitive
recursion, and unbounded minimalization.
pattern (over an alphabet V ): a string of variables over that alphabet; regarded as
present in a particular word w ∈V ∗if there exists an assignment of strings from
V + to the variables in that pattern such that the word formed thereby is a substring
of w.

GLOSSARY
1271
polynomial hierarchy (PH): the union of the complexity classes Σp
n, for n ≥0.
polynomial-space computation (of a function by a TM M): a computation by M of
that function such that there exists a polynomial function p(n) such that for every
input of size n, the calculation workspace takes at most p(n) positions on the tape.
polynomial-time computation (of a function by a TM M): a computation by M of
that function such that there exists a polynomial function p(n) such that for every
input of size n, the calculation takes at most p(n) steps.
positive closure (or Kleene plus): Of a language L, the set of all iterated concate-
nations of words in L excluding the empty word, denoted L+.
nth power of a language: the set of all iterated concatenations w1w2 . . . wn where
each wi is a word in the language.
PRAM memory conﬂict: the conﬂict that occurs when more than one processor at-
tempts concurrently to write to or read from the same global memory register.
PRAM parallel random access machine: a model of parallel computation as a set
of global memory registers and a set of processors, each with access to its own local
registers.
primitive recursion: a restricted way of deﬁning f(n + 1) in terms of f(n).
primitive recursive function: any function derivable from the constant zero func-
tions ζk(x1, . . . , xk) = 0, the successor function σ(n) = n + 1, and the projection
functions πn
i (x1, . . . , xn) = xi, using multivariate composition and multivariate prim-
itive recursion.
probabilistic Turing machine: a nondeterministic Turing machine M with exactly
two choices of a next state at each step, both with probability 1
2 and independent of
all previous choices.
production rule (in a grammar): a rule for making a substitution in a string.
projection function, n-place: a function πn
i (x1, . . . , xn) = xi that maps an n-tuple
to its ith coordinate.
PSPACE: the complexity class of languages TM-decidable in polynomial space.
pumping lemma: any one of several results in formal language theory concerned with
rewriting strings; used in impossibility proofs.
pushdown automaton (PDA): a (possibly nondeterministic) ﬁnite automaton equip-
ped with a stack.
random access machine (RAM): a computation model with several arithmetic reg-
isters and an inﬁnite number of memory registers.
randomized algorithm: an algorithm that makes random choices during its execution,
guided by the output of a random (or pseudo-random) number generator.
recursive language: a language with a decidable membership question.
recursive set: a set whose characteristic function is recursive.
recursively enumerable set: a set that is either empty or the image of a recursive
function.
reducibility in polynomial-time (of language A to language B): the existence of a
polynomial-time computable function f such that x ∈A if and only if f(x) ∈B, for
each string x in the alphabet of language A; denoted by A ≤p
m B.

1272
Chapter 17
THEORETICAL COMPUTER SCIENCE
reduction: a strategy for solving a problem by transforming its natural form of input
into the input form for another problem, solving that other problem on the trans-
formed input, and transforming the answer back into the original problem domain.
regular expression (over an alphabet V ): a string w in the symbols of V and the
special set

ǫ, ), (, +, ∗
	
such that w ∈V or w = ǫ, or (continuing recursively)
w = (αβ), (α + β), or α∗, where α and β are regular expressions.
regular (or type 3) grammar: a grammar such that every production α →β has
antecedent α ∈N and consequent β ∈T ∪T N ∪{λ}.
regular language: a language that can be obtained from elements of its alphabet using
ﬁnitely many times the operations of union, concatenation, and Kleene star.
regularity problem (for grammars): deciding whether L(G) is a regular language.
reverse (of the string x): the string xR obtained by writing x backwards.
running time: the number of primitive operation steps executed by an algorithm, usu-
ally expressed in big-O asymptotic notation (or sometimes Θ-notation) as a formula
based on the input size.
space complexity (of an algorithm): a measure of the amount of computational space
needed in the execution, relative to the size of the input.
sparse language: a language A for which there is a polynomial function p(n) such that
for every n ∈N, there are at most p(n) elements of length n in A.
square (over an alphabet): the pattern xx, or any word having that pattern.
square-free word: a word having no subwords with the pattern xx.
start symbol (in a grammar): a designated nonterminal from which every word of the
language is generated.
state diagram (for a ﬁnite automaton): a labeled digraph whose vertices represent the
states and whose arcs represent the transitions.
string (accepted by a ﬁnite automaton): a string that can cause the automaton to end
up in an accepting state, immediately after the last transition.
string (accepted by a PDA): an input string that can lead to the stack being empty
and the PDA being in an accepting state after the last transition.
string (accepted by a TM M): a string w such that M halts on input w.
string: a ﬁnite sequence of symbols from some alphabet.
substitution (for the alphabet V in the alphabet U): a mapping s: V −→2U∗, which
means that each symbol b ∈V may be replaced by any of the strings in the set s(b);
extends to strings of V ∗.
terminal (in a grammar): a symbol that cannot be replaced by other symbols.
time complexity (of an algorithm): a function representing the number of operations
or the running time needed, using the size of the input as its argument.
total function: a partial function deﬁned on all of its domain, i.e., a function.
tractable problem: a problem that can be solved by an algorithm with polynomial-
time complexity.
λ-transition (in an NFA): a transition that could occur without reading any symbols
of the input string.

GLOSSARY
1273
transition table (for a DFA): a table whose rows are indexed by the states and whose
columns are indexed by the symbols, such that the entry in row r and column c is
the state to which the DFA moves if it reads symbol c while in state r.
trapping state (of a ﬁnite automaton): a non-accepting state from which every out-
ward arc is a self-loop.
trio: a nontrivial family of languages closed under λ-free morphisms, inverse morphisms,
and intersection with regular languages.
Turing-acceptable language: a language that has a TM M that accepts it.
Turing-computable function: a function f such that there is a TM M with f = φM.
Turing-decidable language: a language whose characteristic function is Turing- com-
putable.
Turing machine (TM): an automaton whose head can move one character in either
direction and that can replace the symbol it reads by a diﬀerent symbol.
Turing-p-reducibility (of language A to language B): the existence of a determinis-
tic oracle TM M B that decides language A in polynomial time. Notation: A ≤p
T B.
Turing’s test (of whether a given computer can think): are its responses to a series of
written questions indistinguishable from human responses (by a person who does not
know who gave the responses)?
type 0 grammar: a grammar with no restrictions.
type 1 grammar: a length-increasing grammar, or equivalently, a context-sensitive
grammar.
type 2 grammar: a context-free grammar.
type 3 grammar: a regular grammar.
unambiguous context-free language: a context-free language that has a context-
free grammar that is not ambiguous.
unbounded minimalization: a way of using a function or partial function to deﬁne a
new function or partial function.
uncomputable function: a function whose values cannot be calculated by a Turing
machine (or by a computer program).
undecidable problem: a decision problem whose answers cannot be given by a Turing
machine (or by a computer program).
universal Turing machine: a TM that can simulate every TM.
unsolvable problem: a problem that cannot be decided by a recursive function. Equiv-
alently, an undecidable problem.
variable (over an alphabet V ): a symbol not in V whose values range over V ∗.
word: usually a ﬁnite sequence of symbols (same as string), sometimes a countably
inﬁnite sequence.
word equation (over an alphabet V ): an expression α = β, such that α and β are
words containing letters from V and some variables over V .
word inequality: the negation of a word equation, commonly written as α ̸= β.
worst-case running time: the maximum number of execution steps of an algorithm,
usually expressed in big-O asymptotic notation (or sometimes Θ-notation) as a for-
mula based on the input size.

1274
Chapter 17
THEORETICAL COMPUTER SCIENCE
17.1
COMPUTATIONAL MODELS
The objective of a computer, no matter what input/output or memory devices are at-
tached, is ultimately to make logical decisions and to calculate the values of functions.
Any decision problem can be represented as recognizing whether an input string is in a
speciﬁed subset; calculating a function amounts to accepting an input string and pro-
ducing an output string. At this fundamental level, the fundamental models in Table 1
can serve as the theoretical basis for all sequential computers.
Table 1: Fundamental computational models.
model
description
comment
DFA = (K, s, F, Σ, δ)
K = set of states, start at
s ∈K, accepting states F ⊆K,
input alphabet Σ,
transition fn δ: K × Σ →K
deterministic
ﬁnite
automaton: scans a
tape once;
decides
whether to accept
recognizes
regular
lan-
guages
NFA = (K, s, F, Σ, ∆)
K, s, F, Σ like DFA,
transition fn ∆: K ×Σ∗→P(K)
nondeterministic ﬁ-
nite automaton
equivalent power to DFA
(K, s, ΣI, ΣO, δ, τ)
K = set of states, start at
s ∈K, in-alphabet ΣI,
out-alphabet ΣO,
transition fn δ: K × ΣI →K,
output fn δ: K × ΣI →Σ∗
O
Finite
transducer:
also called “ﬁnite-
state machine with
output”
Special cases: Mealy ma-
chine writes one output
symbol per input symbol;
in Moore machine, output
symbol depends only on
state before transition
PDA = (K, s, F, Σ, Γ, ∆)
K, s, F, Σ as DFA, stack
alphabet Γ, transition fn
∆: K × Σ∗× Γ∗→P(K × Γ∗)
Pushdown automa-
ton:
uses stack for
storage,
nondeter-
ministic
recognizes
context-free
languages
TM = (K, s, h, Σ, δ)
K = set of states, start at
s ∈K, halting state h /∈K,
alphabet Σ, transition fn
δ: K ×Σ →K ×Σ×{L, R}∪{h}
Turing
machine:
has
two-way
tape
with
rewritable
symbols
decides membership in re-
cursive
sets;
equivalent
power to nondeterministic
version
17.1.1
FINITE-STATE MACHINES
Deﬁnitions:
If Σ is a set of symbols, then Σ∗is the set of all (ﬁnite) strings made from symbols of Σ.
A deterministic ﬁnite automaton, abbreviated DFA, models a computer for decision-
making as a 5-tuple (K, s, F, Σ, δ) such that
• K is a ﬁnite set of states;
• s ∈K (starting state);
• F ⊆K (accepting states);

Section 17.1
COMPUTATIONAL MODELS
1275
• Σ is a ﬁnite alphabet of symbols;
• δ : K × Σ →K (transition function).
A DFA is also known as a (deterministic) ﬁnite-state recognizer.
The computer model for a DFA consists of a logic box, programmed by the transition
function δ. It has a read-only head that examines an input tape while moving in only
one direction. Whenever it reads symbol c on the tape while in state q, the automaton
switches into state δ(q, c) and moves to the next symbol. The string is considered to be
accepted if the automaton is in an accepting state after the last transition.
x
F
reading head
q5
q0
q1
q2
q3
y
y
y
y
x
x
x
q4
A nondeterministic ﬁnite automaton, or NFA, is a 5-tuple (K, s, F, Σ, ∆) like a
DFA, except that ∆is a function from K × Σ∗to P(K) (the set of all subsets of K).
The computer model for an NFA is like the computer model for a DFA. However, if it
reads string u on the input string while in state q, the automaton may switch to any of
the states in the set ∆(q, u) and move to the symbol after u. The string is considered to
be accepted if it is possible for the automaton to be in an accepting state after the last
transition.
A ﬁnite automaton is either a DFA or an NFA.
A state diagram for a DFA is a digraph with vertex set K such that for each state q and
each symbol c there is an arc from vertex q to vertex δ(q, c), labeled with c. (Sometimes
a single arc is labeled with multiple symbols, instead of drawing multiple arcs between
the same pair of states.) The starting state is designated by an entering arrow and the
accepting states by double circles. A state diagram for an NFA is similar, except that for
each state p in ∆(q, u) there is an arc from vertex q to vertex p labeled with the string u,
which may be the empty string λ.
A transition table for a DFA is a table with rows indexed by the states in K and
columns indexed by the symbols in Σ, such that the entry in row q and column c is δ(q, c).
The starting-state row label is marked with a “>” and the accepting-state row labels are
underscored.
A conﬁguration for a ﬁnite automaton is a pair (q, w) with q ∈K and w ∈Σ∗. It
signiﬁes that the automaton is in state q with the head at the ﬁrst symbol of string w.
(Since the head moves in only one direction, a common assumption is that it consumes
each symbol as it reads.)
The DFA conﬁguration (q, w) yields the conﬁguration (q′, w′) in one step if deleting
the initial symbol, call it c, of the string w gives the string w′, and δ(q, c) = q′. Notation:
(q, w) ⊢M (q′, w′).
The NFA conﬁguration (q, w) yields the conﬁguration (q′, w′) in one step if there
is an initial preﬁx u on the string w whose deletion gives the string w′, and q′ ∈∆(q, u).

1276
Chapter 17
THEORETICAL COMPUTER SCIENCE
The conﬁguration (q, w) yields the conﬁguration (q′, w′) if there is a sequence of con-
ﬁgurations (q, w) = (q0, w0), (q1, w1), . . . , (qn, wn) = (q′, w′) such that (qi−1, wi−1) ⊢M
(qi, wi), for i = 1, . . . , n. Notation: (q, w) ⊢∗
M (q′, w′).
A string w is accepted by ﬁnite automaton M if there is an accepting state q such that
(s, w) ⊢∗
M (q, λ). That is, machine M accepts string w if, starting in state s at the ﬁrst
symbol, its transition sequence can ultimately lead to an accepting state, immediately
after its last transition.
The language accepted by a ﬁnite automaton M is the set of all strings accepted
by M. It is denoted L(M).
Finite automata M1 and M2 are equivalent if L(M1) = L(M2); that is, if they accept
the same language.
A trapping state of a ﬁnite automaton is a non-accepting state where every outward
arc is a self-loop. To simplify state diagrams, trapping states are often not explicitly
drawn.
A (deterministic) ﬁnite transducer, also known as a ﬁnite-state machine with
output, models a function-calculating computer as a 6-tuple (K, s, ΣI, ΣO, δ, τ) such
that
• K is a ﬁnite set of states;
• s ∈K (starting state);
• ΣI is a ﬁnite alphabet of input symbols;
• ΣO is a ﬁnite alphabet of output symbols;
• δ: K × ΣI →K (transition function);
• τ : K × ΣI →Σ∗
O (output function).
A Mealy machine is a ﬁnite transducer whose output function always produces a single
symbol.
A Moore machine is a Mealy machine such that for every state k and every pair of
input symbols s1 and s2, the outputs τ(k, s1) and τ(k, s2) are the same.
A ﬁnite-state machine is a ﬁnite automaton or a ﬁnite transducer.
Facts:
1. Finite-state machines are the design plan of many practical electronic control devices,
for instance in wristwatches or automobiles.
2. The phrase “ﬁnite-state machine” refers to a ﬁnite-state model that may or may not
have output capacity and that may or may not be nondeterministic.
3. For every NFA, there is an equivalent DFA. (M. Rabin and D. Scott, 1959)
4. In software design, NFAs are commonly used in preference to DFAs because they
often achieve the same task with fewer states.
5. The nondeterminism of an NFA is that possibly u = λ, or that possibly there are two
states p and p′ in ∆(q, u), so that in state q the NFA might read substring u and switch
into either state p or state p′.
6. NFAs are often deﬁned so that λ-transitions are the only possible instances of non-
determinism. In this seemingly more restrictive kind of NFA, the transition function is
a map from K × (Σ ∪{λ}) to K.

Section 17.1
COMPUTATIONAL MODELS
1277
7. The class of languages accepted by ﬁnite automata is closed under all of the following
operations:
• union;
• concatenation;
• Kleene star (see §17.3.2);
• complementation;
• intersection.
8. Kleene’s theorem:
A language is regular (§17.3.4) if and only if it is the language
accepted by some ﬁnite automaton. (S. Kleene, 1956)
9. Some lexical scanning processes of compilers use ﬁnite automata.
10. The relation “yields” for ﬁnite automata is the reﬂexive, transitive closure of the
relation ⊢M.
11. Both Moore machines and Mealy machines have the computational capability of an
unrestricted ﬁnite transducer.
12. More comprehensive coverage of ﬁnite-state machines is provided by many text-
books, including [Si14] and [LePa97].
Examples:
1. The DFA speciﬁed by the following transition table and state diagram decides whether
a binary string has an even number of 1s.
Formally, M = (K, s, F, Σ, δ) with K =
{Even, Odd}, s = Even, F = {Even}, and Σ = {0, 1}.
0
Even
Odd
0
> Even Even  Odd
Odd 
Odd 
Even
1
0
1 
1 
2. In one early form of the language BASIC, an identiﬁer could be a letter or a letter
followed by a digit. The following state diagram speciﬁes a DFA that accepts this:
letter
digit
t
u
s
3. A “proper ﬁxed-point numeral” is a nonempty string of decimal digits, followed by
a decimal point, and then another nonempty string of digits. For instance, the number
zero would be represented as “0.0”. The following DFA decides whether the input string
is such a numeral.
Digit
•
Digit
Digit
Digit
4. An “integer” in some programming languages is a nonempty string of decimal digits,
possibly preceded by a sign + or −. The following NFA recognizes such an integer.

1278
Chapter 17
THEORETICAL COMPUTER SCIENCE
digit
digit
u
v
s
+
–
λ
5. The following ﬁnite-state transducer has {0, 1, $} and {0, 1} for its input and output
alphabets, respectively, where “$” serves as an end-of-string marker. It reads a binary
numeral, starting at the units digit, and prints a binary numeral whose value is double
the input numeral.
6. The following ﬁnite-state machine models a vending machine for a 20-cent toy. The
possible inputs are nickels and dimes, and a push of a button releases the toy if enough
change has been deposited. Each state indicates the amount that has been deposited.
This machine may be regarded as a transducer that produces symbol T (toy) if it receives
input B while in state 20.
0
5
10
15
20
5¢
5¢
5¢
5¢,10¢
10¢
10¢
10¢
B
B
B
B > T
B
7. NFAs can be used to model various solitaire games and puzzles, such as making a
complete knight’s tour of a chessboard.
17.1.2
PUSHDOWN AUTOMATA
Deﬁnitions:
A pushdown automaton (PDA) is essentially a (possibly nondeterministic) ﬁnite
automaton equipped with a stack. It is given by a 6-tuple (K, s, F, Σ, Γ, ∆) such that
• K is a ﬁnite set of states;
• s ∈K (starting state);
• F ⊆K (accepting states);
• Σ is a ﬁnite alphabet of input symbols;
• Γ is a ﬁnite alphabet of stack symbols;
• ∆: K × Σ∗× Γ∗→P(K × Γ∗) (transition function).
A transition of a PDA is an element (q, γ) in some ∆(p, u, β). The idea is that from
state p, the PDA may read the substring u and the stack substring β, and transfer into
state q while popping β and pushing γ, thereby replacing β by γ.
Note: A PDA is frequently deﬁned so that the only strings that can be read or written
or pushed or popped are single characters and the empty string. This has no eﬀect on
the computational generality.

Section 17.1
COMPUTATIONAL MODELS
1279
The computer model for a PDA consists of a logic box, programmed by ∆, equipped
with a read-only head that examines an input tape while moving in only one direction,
and also equipped with a stack. When it reads substring u from input while in state p
with substring β at the top of the stack, the automaton selects a corresponding entry
from ∆and makes that transition.
x
a
stack
b
a
a
b
F
reading head
q5
q0
q1
q2
q3
y
y
y
y
x
x
x
q4
A conﬁguration for a PDA is a triple (p, u, β) such that p ∈K, u ∈Σ∗, and β ∈Γ∗.
The PDA conﬁguration (p, ux, βα) yields the conﬁguration (q, x, γα) in one step if
there is a transition ((p, u, β), (q, γ)). Notation: (p, ux, βα) ⊢M (q, x, γα).
The PDA conﬁguration C yields the conﬁguration C′, denoted C ⊢∗
M C′, if there is
a sequence of conﬁgurations C = C0, C1, . . . , Cn = C such that Ci−1 ⊢M Ci, for i =
1, . . . , n.
A string w ∈Σ∗is accepted by a PDA M if there is an accepting state q such that
(s, w) ⊢∗
M (q, λ, λ). That is, it accepts string w if, starting in state s at the ﬁrst symbol,
its transition sequence can ultimately lead to an accepting state and an empty stack after
it has read the last symbol.
The language accepted by a PDA M is the set of all strings accepted by M. It is
denoted L(M).
A state diagram for a PDA is a digraph with vertex set K such that for each pair (q, γ)
in ∆(p, u, β) there is an arc from vertex p to vertex q, labeled u, β →γ. (Sometimes
a single arc is labeled with multiple symbols, instead of drawing multiple arcs between
the same pair of states.) The starting state is designated by an entering arrow, and the
accepting states by a double circle.
Facts:
1. The PDA model was invented by A. G. Oettinger in 1961.
2. A language L is context-free (see §17.3.3) if and only if there is a pushdown au-
tomaton M such that L is the language accepted by M. (M. Sch¨utzenberger 1963, and
independently by N. Chomsky and by J. Evey)
3. A PDA can test whether a string is a palindrome, or whether all the left and right
parentheses are matched, but a DFA cannot.
4. The class of languages accepted by deterministic PDAs is smaller than the class
accepted by nondeterministic PDAs.
5. More comprehensive coverage of pushdown automata is provided by many textbooks,
including [Si14] and [LePa97].

1280
Chapter 17
THEORETICAL COMPUTER SCIENCE
Examples:
1. The following PDA decides whether a sequence of left and right parentheses is well-
nested, in the sense that every left parenthesis is uniquely matched to a right parenthesis,
and vice versa. It is necessary and suﬃcient that, in counting left and right parentheses
while reading from left to right, the number of right parentheses never exceeds the number
of left parentheses and that the total counts are the same.
( , λ      (  
) , (        λ
s
2. The following PDA decides whether a string from the alphabet {0, 1, m} has the
form bmbr where b is a bitstring and br its reverse. The middle m signals when to switch
from pushing symbols onto the stack to popping them oﬀ.
0 , λ      0
1, λ      1
0, 0      λ
1,1      λ
m, λ      λ
s
t
3. The following nondeterministic PDA decides whether a binary string is of the form bbr,
that is, an even-length palindromic bitstring. In eﬀect, it considers every character in-
terspace as the possible middle.
0 , λ      0
1, λ      1
0, 0      λ
1,1      λ
λ, λ      λ
s
t
17.1.3
TURING MACHINES
Deﬁnitions:
A Turing machine (TM) models a computer as a 5-tuple (K, s, h, Σ, δ) such that
• K is a ﬁnite set of states;
• s ∈K (starting state);
• h /∈K (halting state);
• Σ is a ﬁnite set of symbols, including the blank #; (alphabet);
• δ: K × Σ →(K × Σ × {L, R}) ∪{h} (transition function).
The computer model for a Turing machine consists of a logic box, programmed by δ,
equipped with a read-write head that examines an input tape with a left end but no right
end. To start a computation, the input string is written at the left end of the tape, and
the rest of the tape ﬁlled with blanks. The head starts at the leftmost symbol. Whenever
the Turing machine reads symbol b on the tape while in state q, its internal logic produces
the triple δ(q, b) = (p, c, D), and the machine switches into state p, replaces b by c, and
moves one space in direction D, that is, either to the left (L) or to the right (R).

Section 17.1
COMPUTATIONAL MODELS
1281
x
halt
q0
q1
2-way read-write head
q2
q3
y
y
y
y
x
x
x
#
#
# ...
q4
A transition table for a Turing machine is a table whose rows are labeled with the
states and whose columns are labeled with the symbols, such that the entry in row q and
column b is δ(q, b). Thus, a typical table entry is a triple indicating to which state to
switch from q, the symbol to replace b, and whether to move one square to the right or
to the left. However, another possibility is that a table entry is the halt state h.
A conﬁguration for a Turing machine is a quadruple (q, u, b, v), such that q ∈K ∪{h},
u, v ∈Σ∗, and b ∈Σ, commonly written as (q, ubv). This means that the Turing machine
is in state q, that the present value of the tape is ubv, that the head is presently at the
indicated b, and that the rest of the tape to the right is ﬁlled with blanks.
The Turing machine conﬁguration (p, ubv) yields the conﬁguration (q, xcy) in one
step if the transition δ(p, b) changes conﬁguration (p, ubv) to conﬁguration (q, xcy). This
is denoted by (p, ubv) ⊢M (q, xcy).
The Turing machine accepts string w if it enters the halting state h after starting with
w on the tape.
An inﬁnite loop for a Turing machine is an inﬁnite sequence of conﬁgurations C0, C1,
C2, . . . such that Ci−1 ⊢M Ci, for i = 1, 2, . . ..
A Turing machine hangs if it moves left oﬀthe left end of the tape.
The output of a Turing machine is the string from the left end of the tape up to the
last non-blank character if the computation halts, and undeﬁned otherwise.
The partial function φM induced by a Turing machine M maps each input v for
which the computation halts to the output φM(v), and is otherwise undeﬁned.
A function f : Σ∗→Σ∗is Turing-computable if there is a Turing machine M that halts
for all inputs, and computes the function f, that is, f(v) = φM(v) for all v ∈Σ∗.
A Turing machine M = (K, s, h, Σ, δ) simulates another Turing machine M ′ = (K′, s′,
h, Σ′, δ′) if there exists a Turing-computable function β : Σ′∗→Σ∗such that φM(β(w)) =
φM′(w) for all w ∈domain(φM′) and φM(β(w)) is undeﬁned for all w /∈domain(φM′).
A universal Turing machine is a Turing machine U = (KU, sU, hU, ΣU, δU) that can
simulate every Turing machine, in the following sense. There is a rule αU for encod-
ing any Turing machine M and a rule βU for encoding any input w to M, such that
φU(αU(M)#βU(w)) is deﬁned and equals φM(w) whenever φM(w) is deﬁned, and is
undeﬁned otherwise.
A language L ⊆Σ∗is Turing-acceptable if there exists a Turing machine M that
accepts it. That is, L = {w ∈Σ∗| M accepts w}.
The characteristic function χL : Σ∗→{yes, no} of a language L ⊆Σ∗is given by the
rule χL(w) = yes if w ∈L, and no otherwise.

1282
Chapter 17
THEORETICAL COMPUTER SCIENCE
A language is Turing-decidable if its characteristic function is Turing-computable.
A (subset-membership) decision problem is undecidable if it does not correspond to a
Turing-decidable language.
An n-state busy beaver machine is an n-state Turing machine on the alphabet Σ =
{#, 1} that accepts a two-way inﬁnite input tape ﬁlled with #s and halts after placing a
maximum number of 1s on the tape. (The name busy beaver is from an analogy between
the machine piling up 1s and a beaver piling up logs.)
The busy beaver function BB(n) has as its value the number of 1s on the output tape
of an n-state busy beaver machine.
A nondeterministic Turing machine is deﬁned like a Turing machine, except that
instead of a transition function that assigns a unique change of symbol and direction of
motion for the head, there is a transition function that permits more than one possibility.
A linear bounded automaton (or LBA) is representable as a (possibly nondetermin-
istic) Turing machine that is fed only the ﬁnite stretch of tape containing the input word,
rather than an inﬁnite tape.
Facts:
1. A Turing machine is commonly regarded as a program to compute the partial func-
tion φM.
2. Every Turing-decidable language is Turing-acceptable.
3. If a language L ⊆Σ∗is Turing-decidable, then its complement L is also Turing-
decidable.
4. Every Turing-acceptable language L ⊆Σ∗whose complement L is also Turing-
acceptable is a Turing-decidable language.
5. The following problems about Turing machines are undecidable:
(a) Given a TM M and an input string w, does M halt on input w?
(b) Given a TM M, does M halt on the empty tape?
(c) Given a TM M, does there exist an input w for which M halts?
(d) Given a TM M, does M halt on every input string?
(e) Given two TMs M1 and M2, do they accept the same input?
(f) Given two numbers n and k, is BB(n) > k?
6. In view of part (d) of the preceding fact, there is no way to tell whether an arbitrary
computer program in a general language always halts, much less whether it calculates
what it is supposed to calculate.
7. It is possible to construct a universal Turing machine. A universal Turing machine
with six states and four symbols was constructed in 1982 by Y. Rogozhin.
8. The busy beaver problem was invented by Tibor Rad´o in 1962.
9. Turing machines have been extended in several ways, including: inﬁniteness in two
directions, more work tapes, and two-dimensional tapes.
10. Some of the extensions of a Turing machine can perform computations more quickly
and are easier to program.
11. Any function that can be computed by a Turing machine with (a) a two-way inﬁnite
tape, (b) k tapes, and/or (c) two-dimensional tapes, can also be computed by some
standard Turing machine.

Section 17.1
COMPUTATIONAL MODELS
1283
12. Any language that can be decided by a nondeterministic Turing machine can also
be decided by some standard Turing machine.
13. The following table gives known values and lower bounds for the busy beaver func-
tion:
n
1
2
3
4
5
6
BB(n)
1
4
6
13
≥4098
> 1018267
14. A linear bounded automaton is less powerful than a Turing machine. However, it
is more powerful than a pushdown automaton.
15. Alan M. Turing (1912–1954) was a British mathematician whose cryptanalytic work
during World War II led to the decryption of ciphertext from the German cipher machine
called the Enigma. This work was dramatized in the 2014 ﬁlm “The Imitation Game”.
16. Turing proposed that a machine be regarded as “thinking” if its responses to written
questions cannot be distinguished from those of a person. This criterion is called Turing’s
test.
17. More comprehensive coverage of Turing machines is provided by many textbooks,
including [Si14] and [LePa97].
Examples:
1. This is a 1-state Turing machine with alphabet Σ = {0, 1, #} that changes every
character preceding the ﬁrst blank into a blank. It accepts any string over its alphabet.
0
1
#
→a
#aR
#aR
h
2. This 3-state Turing machine with alphabet Σ = {0, 1, #} doesn’t change its input
tape at all. It halts whenever it encounters the third ‘1’. Thus, it accepts any tape with
at least three 1s but accepts no other strings.
0
1
#
→a
0aR
1bR
#aR
b
0bR
1cR
#bR
c
0cR
h
#cR
3. This 3-state Turing machine with alphabet Σ = {1, #} adds two positive integers,
each represented as a string of 1s.
For instance, the tape 111#11### · · · becomes
11111### · · ·.
1
#
→a
1aR
1bR
b
1bR
#cL
c
#cR
h
4. This 2-state Turing machine shows that BB(2) is at least 4.
#
1
→a
1bL
1bR
b
1aR
h

1284
Chapter 17
THEORETICAL COMPUTER SCIENCE
5. This 3-state Turing machine shows that BB(3) is at least 6.
#
1
→a
1bR
1cL
b
1cR
h
c
1aL
#bL
17.1.4
PARALLEL COMPUTATIONAL MODELS
Deﬁnitions:
A parallel computation model permits more than one instruction to be executed
simultaneously, instead of requiring that they be executed sequentially.
An interconnection network models parallel computation as a digraph in which each
vertex represents a processor. In each phase a processor communicates with its neighbors
and makes a computation.
An n-dimensional cellular automaton is an interconnection network in which there
is a processor at each integer lattice point of n-dimensional Euclidean space, and each
processor communicates with its immediate neighbors.
A random access machine (RAM) has several arithmetic registers and an inﬁnite
number of memory registers (often modeled as an inﬁnite array), each of which can be
accessed immediately via its address (the index in the array).
A parallel random access machine (PRAM) models parallel computation as a set
of global memory registers {Mj | j = 1, 2, . . .} and a set of processors {Pj | j = 1, 2, . . .}.
Each processor Pj also has sole access to an inﬁnite sequence of local registers.
In a PRAM, a processor performs read and write instructions involving global memory,
and other instructions involving only its local memory. All processors of a PRAM perform
the same program in synchrony, so that all processors that are not idle are performing
their task under the same instruction of the program.
In a PRAM, the concurrent construct par[a ≤j ≤b]Pj : Sj means that each processor
Pj for a ≤j ≤b is performing the operation Sj. The operation could be a calculation,
a read from some global register, or a write to some global register.
In a PRAM, a computation starts when all the processors execute the ﬁrst instruction.
It stops when all processors halt. The contents of the global memory are regarded as the
output.
In a PRAM, a memory conﬂict occurs when more than one processor attempts con-
currently to write to or read from the same global memory register.
• In exclusive read exclusive write (EREW), neither concurrent reads from
nor concurrent writes to the same location are allowed.
• In concurrent read exclusive write (CREW), concurrent reads are allowed,
but not concurrent writes to the same location.
• In concurrent read concurrent write (CRCW), both concurrent reads from
and concurrent writes to the same location are allowed.
In the common PRAM (CRCWcom PRAM) model, concurrent writes to the same
location are permitted provided all processors are trying to write the same data.

Section 17.1
COMPUTATIONAL MODELS
1285
Facts:
1. Some commercially available parallel computers have an array of elements in which
a single broadcast instruction applying to every element is executed simultaneously for
all the elements.
2. Random access machines are considered theoretical models of real sequential com-
puters.
3. PRAM programs are often described using high-level programming language con-
structs for array processing that are similar to sequential array processing, except that
the PRAM array locations are processed in parallel.
4. Whereas O(n) is regarded as fast for sequential processing, a parallel algorithm tends
to be regarded as fast if it runs in O(log n) time, where n is the size of the input.
Examples:
1. A parallel computer for sorting up to n items can be modeled as a row of processors
P1, P1, . . . , Pn each joined to its immediate predecessor Pj−1 and its immediate succes-
sor Pj+1. On the ﬁrst phase and on all subsequent odd-numbered phases, each processor
pair (P2j+1, P2j+2) compares items and swaps, if necessary, so that the smaller item
ends up in the lower-indexed processor. On the second phase and on all subsequent
even-numbered phases, each processor pair (P2j, P2j+1) compares items and swaps, if
necessary, so that the smaller item ends up in the lower-indexed processor.
After n
phases, the items are sorted in ascending order.
P1
P2
Pn-1
Pn
2. EREW PRAM: ﬁnding the maximum:
Given n numbers, with n = 2r, store the
numbers in global registers Mn, . . . , M2n−1. Then execute the following:
for i = r −1 downto 0
par[2i ≤j ≤2i+1] Pj : Mj := max{M2j, M2j+1}
The maximum ends up in global register M1.
3. CRCWcom PRAM: ﬁnding the maximum:
Given n numbers, store the numbers in
global registers M1, . . . , Mn. Use n2 processors Pi,j, 1 ≤i, j ≤n as follows:
par[1 ≤i, j ≤n] Pi,j : Mi+n := 0
par[1 ≤i, j ≤n] Pi,j : if Mi < Mj then Mi+n := 1
{Mi+n = 0 if and only if Mi = max{M1, . . . , Mn}}
par[1 ≤i, j ≤n] Pi,j : if Mn+i = 0 then M0 := Mi
This program is much faster than the EREW program, because all pairs are compared
simultaneously in a single parallel step.
4. Game of Life:
The “Game of Life”, invented by mathematician John H. Conway,
is played on an inﬁnite checkerboard. The neighbors of a square are the eight squares
that touch it, including at its corners. In the initial conﬁguration c0 of the game, some
squares are regarded as alive and all others dead. Each conﬁguration ck gives birth to a
new conﬁguration ck+1, according to the following rules:
• a live cell remains alive if it has either exactly two or exactly three live neighbors;
• a dead cell comes alive if and only if it has exactly three live neighbors.

1286
Chapter 17
THEORETICAL COMPUTER SCIENCE
The following sequence of conﬁgurations illustrates these rules.
The “Game of Life” can be regarded as a cellular automaton in which the squares are
the processors and each processor is joined to its eight neighbors.
5. A conﬁguration in the “Game of Life” has periodicity n if every nth conﬁguration is
the same, and n is the smallest such number. Here are three periodic conﬁgurations.
period 1
period 2
period 15
17.2
COMPUTABILITY
The theory of computability is concerned with distinguishing what can be computed from
what cannot. This is not a question of skill at performing calculations. The remarkable
truth is that the impossibility of computing certain functions can be proved from the
deﬁnition of what it means to compute a function.
17.2.1
RECURSIVE FUNCTIONS AND CHURCH’S THESIS
The implicit domain for the theory of computability is the set N of natural numbers.
The encoding of problems concerned with arbitrary objects into terms of natural numbers
permits general application of this theory.
Deﬁnitions:
The n-place constant zero function is the function ζn(x1, . . . , xn) = 0.
The successor function is the function σ(n) = n + 1.
The ith n-place projection function is the function πn
i (x1, . . . , xn) = xi.
The (multivariate) composition of the n-place function f(x1, . . . , xn) and the n
m-place functions g1(x1, . . . , xm), . . . , gn(x1, . . . , xm) is the m-place function
h(x1, . . . , xm) = f(g1(x1, . . . , xm), . . . , gn(x1, . . . , xm)).

Section 17.2
COMPUTABILITY
1287
(Multivariate) primitive recursion uses a previously deﬁned (n + 2)-place function
f(x1, . . . , xn+2) and a previously deﬁned n-place function g(x1, . . . , xn) to deﬁne the
following new (n+1)-place function:
h(x1, . . . , xn+1) =
(
g(x1, . . . , xn)
if xn+1 = 0;
f(x1, . . . , xn+1, h(x1, . . . , xn, xn+1 −1))
otherwise.
Unbounded minimalization uses an (n+1)-place function f(x1, . . . , xn+1) to deﬁne
the following new n-place function, which is denoted µm[f(x1, . . . , xn, m) = 0]:
g(x1, . . . , xn) =
(
the least y such that f(x1, . . . , xn, y) = 0
if it exists;
0
otherwise.
An (n+1)-place function f(x1, . . . , xn+1) is regular if for every n-tuple (x1, . . . , xn) there
is a y ∈N such that f(x1, . . . , xn, y) = 0.
The class P of primitive recursive functions is the smallest class of functions that
contains
• the constant zero functions ζn(x1, . . . , xn) = 0, for all n ∈N;
• the successor function σ(n) = n + 1;
• the projection functions πn
i (x1, . . . , xn) = xi, for all n ∈N and 1 ≤i ≤n;
and is closed under multivariate composition and multivariate primitive recursion.
The class RF of recursive functions is the smallest class of functions that contains
• the constant zero functions ζn(x1, . . . , xn) = 0, for all n ∈N;
• the successor function σ(n) = n + 1;
• the projection functions πn
i (x1, . . . , xn) = xi, for all n ∈N and 1 ≤i ≤n;
and is closed under multivariate composition, multivariate primitive recursion, and the
application of unbounded minimalization to regular functions.
A recursive function is a function in RF.
Church’s thesis, or the Church-Turing thesis, is the premise that recursive functions
and Turing machines are capable of representing every function that is computable or
partially computable.
A partial function on N is a function whose values are possibly undeﬁned for certain
natural numbers.
A partial function on N is called total if it is deﬁned on every natural number.
The class PR of partial recursive functions is the smallest class of partial functions
that contains the constant zero functions ζ, the successor function σ, and the projec-
tion functions πn
i , and is closed under multivariate composition, multivariate primitive
recursion, and the arbitrary application of unbounded minimalization.
A partial recursive function is a function in PR.
A partial recursive function f is represented by the Turing machine M if machine
M calculates the value f(n) for every number n on which f is deﬁned and fails to halt
for every number on which f is undeﬁned.

1288
Chapter 17
THEORETICAL COMPUTER SCIENCE
The Ackermann function A: N × N →N is deﬁned by
A(0, j) = j + 1;
A(i + 1, 0) = A(i, 1);
A(i + 1, j + 1) = A(i, A(i + 1, j)).
Facts:
1. The standard integer functions of arithmetic, including addition, subtraction, multi-
plication, division, and exponentiation, are all primitive recursive functions.
2. A function is a partial recursive function if and only if it can be represented by a
Turing machine.
3. There are several other models of computation that are equivalent to partial recur-
sive functions and to Turing machines, including labeled Markov algorithms and Post
production systems.
4. Church’s thesis identiﬁes formal concepts (recursive functions and Turing machines)
with the intuitive concept of what is computable; so it is not something that is subject
to proof.
5. Church’s thesis is often invoked in the proof of theorems about computable functions
to avoid dealing with low-level details of the model of computation.
6. The Ackermann function is recursive but not primitive recursive.
7. The Ackermann function grows faster than any primitive recursive function, in the
following sense. For every primitive recursive function f(n), there is an integer n0 such
that f(n) < A(n, n) for all n > n0.
Examples:
1. Addition is primitive recursive. Deﬁne
a(x, 0) = π1
1(x);
a(x, y + 1) = σ(π3
3(x, y + 1, a(x, y))).
Then a(x, y) = x + y.
2. Multiplication is primitive recursive. Deﬁne
m(x, 0) = 0;
m(x, y + 1) = a(m(x, y), π2
2(x, y)), where a(x, y) is addition.
Then m(x, y) = x · y.
3. Predecessor is primitive recursive. Deﬁne
p(0) = 0;
p(x + 1) = π1
1(x).
Then p(x) is the predecessor function (where the predecessor of 0 is 0).
4. Nonnegative subtraction, denoted x −. y, is deﬁned as x −y if x ≥y and 0 otherwise.
Deﬁne
s(x, 0) = π1
1(x);
s(x, y + 1) = p(s(x, y)), where p(x) is predecessor.
Then s(x, y) = x −. y.
5. The function p(n) = the nth prime number is a primitive recursive function.

Section 17.2
COMPUTABILITY
1289
17.2.2
RECURSIVE SETS AND DECIDABLE PROBLEMS
Deﬁnitions:
The characteristic function of a set A is the function
f(x) =
(
1
if x ∈A;
0
if x /∈A.
A set A is recursive if its characteristic function is recursive.
A problem is (computationally) decidable (or solvable) if it can be represented as a
membership problem that can be decided by a recursive function.
A set A ⊆N is recursively enumerable (r.e.) if A = ∅or A is the image of a recursive
function.
A G¨odel numbering of a set S is a one-to-one recursive function g : S →N whose
image in N is a recursive set.
Facts:
1. If a set A and its complement A are both recursively enumerable, then A is recursive.
2. If a recursively enumerable set A is the image of a nondecreasing function, then A is
recursive.
3. A set is recursively enumerable if and only if it is the image of a partial recursive
function.
4. A set is recursively enumerable if and only if it is the domain of a partial recursive
function.
5. The set of Turing machines has a G¨odel numbering.
Examples:
1. Every ﬁnite set of numbers is recursive.
2. The prime numbers are a recursive set.
3. The problem of deciding which Turing machines halt on all inputs is undecidable.
The set of G¨odel numbers for these Turing machines is neither recursive nor recursively
enumerable.
4. For any ﬁxed c ∈N, the problem of deciding which Turing machines halt when the
number c is supplied as input is undecidable. The set of G¨odel numbers for these Turing
machines is recursively enumerable, but not recursive.
5. The problem of deciding which Turing machines halt when their own G¨odel number
is supplied as input is undecidable. The set of G¨odel numbers for these Turing machines
is recursively enumerable, but not recursive.
6. Hilbert’s tenth problem:
Hilbert’s tenth problem (posed in 1900) was the problem
of devising an algorithm to determine, given a polynomial p(x1, . . . , xn) with integer
coeﬃcients, whether there exists an integer root. Y. Matiyasevich proved in 1970 that
no such algorithm exists. That is, the set of polynomials with integer coeﬃcients that
have an integer solution is not recursive. Hilbert’s tenth problem is called a “natural
example” of an undecidable problem, since the concepts used to deﬁne it are not from
within computability theory (i.e., unlike problems concerned with the behavior of Turing
machines). [Ma93]

1290
Chapter 17
THEORETICAL COMPUTER SCIENCE
17.3
LANGUAGES AND GRAMMARS
Strings of symbols are a general way to represent information, both in written text and
in a computer.
A language is a set of strings that are used within some domain of
discourse, and a grammar is a system for generating a language. A grammar is what
enables a compiler to determine whether a body of code is syntactically correct in a given
computer language. Formal language theory is concerned with languages, grammars, and
rudiments of combinatorics on strings. Applications of formal language theory range from
natural and programming languages, developmental biology, and computer graphics, to
semiotics, artiﬁcial intelligence, and artiﬁcial life.
17.3.1
ALPHABETS
Deﬁnitions:
An alphabet is a ﬁnite nonempty set.
A symbol is an element of an alphabet.
A string in an alphabet is a ﬁnite sequence of symbols over that alphabet.
A word in an alphabet is a ﬁnite or countably inﬁnite sequence of symbols over that
alphabet.
The length of a string w is the number of symbols in w, denoted |w|.
The empty string λ (or sometimes ǫ) is the string of length zero, that is, the string
with no symbols.
The frequency |w|a of a symbol a in a string w is the number of occurrences of a in
string w.
A substring of a string w is a sequence of consecutive symbols that occurs in w.
A subword of a word w is a sequence of consecutive symbols that occurs in w.
A preﬁx of a string w is a substring that starts at the leftmost symbol.
A suﬃx of a string w is a substring that ends at the rightmost symbol.
The reverse or mirror image xR of the string x = a1a2 . . . an is the string an . . . a2a1.
A palindrome is a string that is identical to its reverse.
A pseudopalindrome in an alphabet (such as English) that includes punctuation sym-
bols (such as comma, hyphen, or blank) is a word that becomes a palindrome when all
of its punctuation symbols are deleted.
The concatenation xy of two strings x = a1a2 . . . am and y = b1b2 . . . bn is the string
a1a2 . . . amb1b2 . . . bn obtained by appending string y to the right of string x.
The nth power of a string w, denoted wn, is the concatenation of n copies of w.
The shuﬄe x⊔⊥y of two strings x = x1 . . . xn and y = y1 . . . yn is the string x1y1 . . . xnyn.

Section 17.3
LANGUAGES AND GRAMMARS
1291
Facts:
1. A symbol of an alphabet is usually conceptualized as something that can be repre-
sented by a single byte or by a written character.
2. A ﬁnite word is a string.
3. The sum of the frequencies |w|a taken over all the symbols a in the alphabet equals
the length of the string w.
4. The length |xy| of a concatenation equals the sum |x|+|y| of the lengths of the strings
x and y from which it is formed.
5. The length of the nth power of a string w is n · |w|.
6. When a pseudopalindrome occurs in a natural language, it is commonly called a
palindrome.
Examples:
1. The English alphabet includes lower and upper case English letters, the blank symbol,
the digits 0, 1, . . . , 9, and various punctuation symbols.
2. ASCII (American Standard Code for Information Interchange) is an alphabet of
size 128 for many common computer languages. See Table 1.
3. ABLE WAS I ERE I SAW ELBA is a palindrome.
4. The names EVE, HANNAH, and OTTO are palindromes.
5. MADAM I’M ADAM and SIX AT-NOON TAXIS are pseudopalindromes.
6. The third power of the string 011 is 011011011.
7. The concatenation of BOOK and KEEPER is BOOKKEEPER.
8. The shuﬄe of FLOOD and RIVER is FRLIOVOEDR.
17.3.2
LANGUAGES
Deﬁnitions:
The free monoid V ∗generated by the alphabet V is the structure whose domain is
the set of all strings composable from symbols over V , with the operation of string
concatenation.
A (formal) language on the alphabet V is a subset of the free monoid V ∗.
The λ-free semigroup V + on an alphabet V is the set V ∗−{λ}, with the concatenation
operation.
A λ-free language on the alphabet V is a subset of the λ-free semigroup V +.
The length set of a language L is the set length(L) = {|x| | x ∈L}.
The concatenation L1L2 of two languages is the set {xy | x ∈L1, y ∈L2}.
The ith power of a language L is the language Li deﬁned recursively by the rule
L0 = {λ} and Li+1 = LiL, i ≥0.
The Kleene closure (or Kleene star) L∗of a language L is the union S
i≥0 Li of all
its powers.
The positive closure (or Kleene plus) L+ of a language L is the union S
i≥1 Li of all
its powers excluding the zeroth power.

1292
Chapter 17
THEORETICAL COMPUTER SCIENCE
Table 1: ASCII codes and binary.
000 0000
NUL
010 0000
SP
100 0000
@
110 0000
‘
000 0001
SOH
010 0001
!
100 0001
A
110 0001
a
000 0010
STX
010 0010
”
100 0010
B
110 0010
b
000 0011
ETX
010 0011
#
100 0011
C
110 0011
c
000 0100
EOT
010 0100
$
100 0100
D
110 0100
d
000 0101
ENQ
010 0101
%
100 0101
E
110 0101
e
000 0110
ACK
010 0110
&
100 0110
F
110 0110
f
000 0111
BEL
010 0111
’
100 0111
G
110 0111
g
000 1000
BS
010 1000
(
100 1000
H
110 1000
h
000 1001
HT
010 1001
)
100 1001
I
110 1001
i
000 1010
LF
010 1010
*
100 1010
J
110 1010
j
000 1011
VT
010 1011
+
100 1011
K
110 1011
k
000 1100
FF
010 1100
,
100 1100
L
110 1100
l
000 1101
CR
010 1101
-
100 1101
M
110 1101
m
000 1110
SO
010 1110
.
100 1110
N
110 1110
n
000 1111
SI
010 1111
/
100 1111
O
110 1111
o
001 0000
DLE
011 0000
0
101 0000
P
111 0000
p
001 0001
DC1
011 0001
1
101 0001
Q
111 0001
q
001 0010
DC2
011 0010
2
101 0010
R
111 0010
r
001 0011
DC3
011 0011
3
101 0011
S
111 0011
s
001 0100
DC4
011 0100
4
101 0100
T
111 0100
t
001 0101
NAK
011 0101
5
101 0101
U
111 0101
u
001 0110
SYN
011 0110
6
101 0110
V
111 0110
v
001 0111
ETB
011 0111
7
101 0111
W
111 0111
w
001 1000
CAN
011 1000
8
101 1000
X
111 1000
x
001 1001
EM
011 1001
9
101 1001
Y
111 1001
y
001 1010
SUB
011 1010
:
101 1010
Z
111 1010
z
001 1011
ESC
011 1011
;
101 1011
[
111 1011
}
001 1100
FS
011 1100
<
101 1100
\
111 1100
|
001 1101
GS
011 1101
=
101 1101
]
111 1101
}
001 1110
RS
011 1110
>
101 1110
ˆ
111 1110
˜
001 1111
US
011 1111
?
101 1111
−
111 1111
DEL
Control codes:
ACK: acknowledge,
BEL: bell,
BS: backspace,
CAN: cancel,
CR:
carriage return,
DC1–4: device controls, DEL: delete,
DLE: data link escape,
EM:
end of medium, ENQ: enquiry, EOT: end of transmission, ESC: escape, ETB: end of
transmission block, ETX: end of text, FF: form feed, FS: ﬁle separator, GS: group
separator, HT: horizontal tab, LF: line feed, NAK: negative acknowledgment, NUL:
null, RS: record separator, SI: shift in, SO: shift out, SOH: start of heading, SP: space,
STX: start of text, SUB: substitute, SYN: synchronous/idle, US: united separator, VT:
vertical tab.

Section 17.3
LANGUAGES AND GRAMMARS
1293
The union of two languages L1 and L2 is L1 ∪L2, using the usual set operation.
The intersection of two languages L1 and L2 is L1 ∩L2, using the usual set operation.
The complement of a language L over an alphabet V is the language L, where comple-
mentation is taken with V ∗as the universe.
A language is regular if it is any of the languages ∅, {λ}, or {b}, where b is a symbol of
its alphabet, or if it can be obtained by applying the operations of union, concatenation,
and Kleene star ﬁnitely many times to one of those languages.
The shuﬄe L1⊔⊥L2 of two languages L1, L2 is the language {w ∈V ∗| w = x⊔⊥y, for
some x ∈L1, y ∈L2}.
The mirror image mi(L) of the language L is the language {xR | x ∈L}. It is also
called the reverse of the language L.
The left quotient of the language L1 with respect to the language L2 on the same
alphabet V , is the language L2\L1 containing every string of V ∗that can be obtained
from a string in L1 by erasing a preﬁx from L2. That is, L2\L1 = {w ∈V ∗| there is x ∈
L2 such that xw ∈L1}.
The left derivative of language L with respect to the string x over the same alphabet V
is the language ∂x(L) = {x}\L.
The right quotient is the notion symmetric to left quotient.
The right derivative is the notion symmetric to left derivative.
A substitution for the alphabet V in the alphabet U is a mapping s: V −→2U∗. This
means that each symbol b ∈V may be replaced by any of the strings in the set s(b).
A ﬁnite substitution is a substitution such that the replacement set s(a) for each
symbol a ∈V is ﬁnite.
The extension of a substitution s: V −→2U∗from its domain alphabet V to the
set V ∗of strings over V is given by the rules s(λ) = {λ} and s(ax) = s(a)s(x), for
a ∈V, x ∈V ∗.
A morphism from the alphabet V to the alphabet U is a substitution s: V −→2U∗
such that the replacement set s(a) for every symbol a ∈V is a singleton set.
A λ-free substitution is a substitution such that λ is never substituted for a symbol.
That is, λ /∈s(a), for every symbol a ∈V .
A λ-free morphism is a morphism such that s(a) ̸= {λ}, for every symbol a ∈V .
The extension of a substitution s: V −→2U∗to the language L ⊆V ∗is the language
s(L) = S
x∈L s(x) that contains every string in U ∗obtainable from a string in L by
making replacements permissible under substitution s.
The inverse of a morphism h: V ∗−→U ∗is the mapping h−1 : U ∗−→2V ∗deﬁned
by h−1(x) = {y ∈V ∗| h(y) = x}, x ∈U ∗.
A family of languages is nontrivial if it contains at least one language diﬀerent from ∅
and {λ}.

1294
Chapter 17
THEORETICAL COMPUTER SCIENCE
Facts:
1. The set of all binary strings with at least as many 1s as 0s is a language.
2. The set of all binary strings in which no two occurrences of 1 are consecutive is a
language.
3. Some strings of a natural language such as English are categorized as nouns, verbs,
and adjectives. Other more complicated strings are categorized as sentences.
4. Some strings of common computer languages are categorized as identiﬁers and arith-
metic expressions. Other more complicated strings are categorized as statements, with
subcategories such as assignment statements, if-statements, and while-statements.
Examples:
1. Natural languages and computer languages are formal languages.
2. The Kleene closure of the language {00, 01, 10, 11} is the language of all strings of
even length.
3. The left derivative {bee}\English includes the following strings: f, n, p, r, s, t, tle,
ts, keeper, swax, feater, ping.
4. The substitution 0 7→{00, 01}, 1 7→{10, 11} over the free monoid {0, 1}∗is the
language of all strings of even length.
5. Given the alphabet {a, b}, deﬁne the morphism φ: {a, b} −→{a, b}∗by the replace-
ments φ(a) = ab and φ(b) = ba, and deﬁne the string wn by the recursion w0 = a and
wn+1 = φ(wn). Then
w1 = ab,
w2 = abba,
w3 = abbabaab,
w4 = abbabaabbaababba, . . . .
6. In Example 5 each word wn is a preﬁx of the next word wn+1. The Thue ω-word is
the inﬁnite word limn→∞wn.
7. Given the alphabet {a, b}, deﬁne the morphism ρ: {a, b} −→{a, b}∗by the replace-
ments ρ(a) = ab and ρ(b) = a, and deﬁne the string wn by the recursion w0 = a and
wn+1 = φ(wn). Then
w1 = ab,
w2 = aba,
w3 = abaab,
w4 = abaababa, . . . .
8. In Example 7 each word wn is a preﬁx of the next word wn+1. The Fibonacci ω-word
is the inﬁnite word limn→∞wn.
9. A language is regular if and only if it is the language of strings accepted by some
ﬁnite-state recognizer.
17.3.3
GRAMMARS AND THE CHOMSKY HIERARCHY
Deﬁnitions:
A phrase-structure grammar (or unrestricted grammar or type 0 grammar) is
a quadruple G = (N, T, S, P) such that
• N is a ﬁnite alphabet of symbols called nonterminals;
• T is a ﬁnite nonempty alphabet, disjoint from N, of symbols called terminals;
• S is a nonterminal called the start symbol;

Section 17.3
LANGUAGES AND GRAMMARS
1295
• P is a ﬁnite set of production rules of the form α →β, where α is a string in
N ∪T that contains at least one nonterminal and β is a string in N ∪T .
The antecedent of a production α →β is α. The consequent of a production α →β
is β.
The string y is directly derivable from the string x with respect to the grammar G if
there is a production rule u →v ∈P and if there are strings w1, w2 ∈(N ∪T )∗such
that x = w1uw2 and y = w1vw2.
The direct derivability relation x =⇒G y (or x =⇒y, when the grammar G is
understood) means that y is directly derivable from string x.
A derivation of the string y from the string x is a sequence of direct derivations x =⇒z1,
z1 =⇒z2, . . . , zn =⇒y. This is sometimes called parsing.
The string y is derivable from the string x with respect to the grammar G if there is a
derivation of y from x. Notation: x =⇒∗y.
The Chomsky normal form for a production rule is A →BC, where B and C are
nonterminals or the form A →a where a is a terminal.
The language generated by the grammar G is the language L(G) = {x ∈T ∗|
S =⇒∗x}.
Grammars G1 and G2 are equivalent if L(G1) = L(G2).
A leftmost derivation x =⇒left y is a derivation x =⇒y in which at each step the
leftmost nonterminal is replaced.
The leftmost language generated by the grammar G is the language Lleft(G) of
strings of terminals with leftmost derivations from the start symbol S.
A grammar (N, T, S, P) is length-increasing (or monotonic) if |u| ≤|v| for all pro-
ductions u →v. (However, the production S →λ is allowed, provided that S does not
appear in any consequent.)
A grammar (N, T, S, P) is context-sensitive (or of type 1) if for each production
u →v, the antecedent and consequent have the form u = u1Au2 and v = u1xu2, for
u1, u2 ∈(N ∪T )∗, A ∈N, x ∈(N ∪T )+. (The production S →λ is allowed, provided
that S does not appear in any consequent.)
A grammar (N, T, S, P) is context-free (or of type 2) if the antecedent of each pro-
duction u →v is a nonterminal.
An L-system is a production-based model for growth and life development.
A grammar (N, T, S, P) is linear if each production u →v has its antecedent u ∈N and
its consequent v ∈T ∗∪T ∗NT ∗.
A grammar (N, T, S, P) is right-linear if each production u →v has u ∈N and v ∈
T ∗∪T ∗N.
A grammar (N, T, S, P) is left-linear if each production u →v has u ∈N and v ∈
T ∗∪NT ∗.
A grammar (N, T, S, P) is regular (or of type 3) if each production u →v has u ∈N
and v ∈T ∪T N ∪{λ}.

1296
Chapter 17
THEORETICAL COMPUTER SCIENCE
Given a class of grammars, there are some basic decision problems about arbitrary gram-
mars G1, G2 in the class:
equivalence: are the grammars G1 and G2 equivalent?
inclusion: is the language L(G1) contained in the language L(G2)?
membership: given an arbitrary string x, is x an element of L(G1)?
emptiness: is the language L(G1) empty?
ﬁniteness: is the language L(G1) ﬁnite?
regularity: is L(G1) a regular language? (see §17.3.2)
The recursive languages are the languages with a decidable membership question.
The various classes of languages are denoted as follows:
RE (type 0): the class of all unrestricted languages;
CS (type 1): the class of all context-sensitive languages;
CF (type 2): the class of all context-free languages;
LIN: the class of all linear languages;
REG (type 3): the class of all regular languages.
Facts:
1. Chomsky hierarchy: The following strict inclusions hold.
REG ⊂LIN ⊂CF ⊂CS ⊂RE.
2. The language of an unrestricted grammar is recursively enumerable (RE).
3. CS (context sensitive) ⊂{recursive languages} ⊂RE (unrestricted).
4. The class of languages generated by monotonic grammars is identical to the class of
languages generated by context-sensitive grammars.
5. L-systems were introduced in 1968 by Aristid Lindenmayer (1922–1990), a Dutch
biologist, to model the development of some plant systems. (See [Li16].)
6. The classes of languages generated by right-linear or by left-linear grammars coincide.
This class is identical to the family of languages generated by regular grammars, as well
as to the class of regular languages (see §17.3.2).
7. Lleft(G) ∈CF (context-free) for each type-0 grammar G.
8. If G is a context-free grammar, then Lleft(G) = L(G).
9. Let G be a context-free grammar. Then there is an equivalent grammar G′, with every
rule in Chomsky normal form. Moreover, there is constructive method for transforming
grammar G into the grammar G′.
10. The following table gives closure properties for Chomsky hierarchy classes.

Section 17.3
LANGUAGES AND GRAMMARS
1297
RE
CS
CF
LIN
REG
union
yes
yes
yes
yes
yes
intersection
yes
yes
no
no
yes
complement
no
yes
no
no
yes
concatenation
yes
yes
yes
no
yes
Kleene star
yes
yes
yes
no
yes
intersection with
regular languages
yes
yes
yes
yes
yes
substitution
yes
no
yes
no
yes
λ-free substitution
yes
yes
yes
no
yes
morphisms
yes
no
yes
yes
yes
λ-free morphisms
yes
yes
yes
yes
yes
inverse morphisms
yes
yes
yes
yes
yes
left/right quotient
yes
no
no
no
yes
left/right quotients with
regular languages
yes
no
yes
yes
yes
left/right derivative
yes
yes
yes
yes
yes
shuﬄe
yes
yes
no
no
yes
mirror image
yes
yes
yes
yes
yes
11. Rice’s theorem: Let P be a nontrivial property of recursively enumerable languages
(i.e., at least one grammar has property P and at least one grammar does not have
property P). Then property P is undecidable.
12. A language is context-free if and only if it is the language accepted by some (possibly
nondeterministic) pushdown automaton.
13. The following table summarizes the decidability properties of the grammar classes
in the Chomsky hierarchy. In this table U stands for undecidable, D for decidable, and
T for trivial.
RE
CS
CF
LIN
REG
equivalence
U
U
U
U
D
inclusion
U
U
U
U
D
membership
U
D
D
D
D
emptiness
U
U
D
D
D
ﬁniteness
U
U
D
D
D
regularity
U
U
U
U
T
Examples:
1. In the grammar G = (N, T, S, P), where N = {S}, T = {0, 1}, and P = {S →
0S1, S →λ}, a derivation of the string 0011 is S =⇒0S1 =⇒00S11 =⇒0011.
2. The following are examples of languages generated by the grammar G = (N, T, S, P)
with N = {S, x, y, z}, T = {0, 1, 2}:
production set P
language L(G)
class
S →0x, x →1y, y →0x, x →1, y →λ
{01, 0101, 010101, . . .}
regular
S →λ, S →0x, S →01, x →S1
{0n1n | n ≥0}
linear
S →λ, S →0Sx2, 2x →x2, 0x →01, 1x →11
{0n1n2n | n ≥0}
unrestricted

1298
Chapter 17
THEORETICAL COMPUTER SCIENCE
17.3.4
REGULAR AND CONTEXT-FREE LANGUAGES
Deﬁnitions:
Given an alphabet V , a regular expression over V is a string w over the alphabet
V ∪

ǫ, ), (, +, ∗
	
that has one of the following forms:
• w ∈V or w = ǫ;
• w = (αβ), where α and β are regular expressions;
• w = (α + β), where α and β are regular expressions;
• w = α∗, where α is a regular expression.
The set of all regular expressions over alphabet V is denoted rexV .
The function L maps rexV to the set of all languages over the alphabet V , using the
following rules:
• L(ǫ) = ∅, and L(a) = {a} for all a ∈V ;
• L((αβ)) = L(α)L(β), L((α + β)) = L(α) ∪L(β), and L(α∗) = (L(α))∗.
A context-free grammar G is ambiguous if there is a string x ∈L(G) having two
diﬀerent leftmost derivations in G.
A context-free language L is inherently ambiguous if every context-free grammar of L
is ambiguous; otherwise, language L is called unambiguous.
Facts:
1. Kleene’s theorem: A language L is regular if and only if there is a regular expression e
such that L = L(e).
2. Every context-free language over a one-letter alphabet is regular.
3. Every regular language L can be represented in the form L = h4(h−1
3 (h2(h−1
1 (a∗b)))),
where h1, h2, h3, h4 are morphisms.
4. Each regular language is unambiguous.
5. There are inherently ambiguous linear languages.
6. The ambiguity problem for context-free grammars is undecidable.
7. The length set of a context-free language is a ﬁnite union of arithmetical progressions.
8. Every language L can be represented in the form L = h(L1 ∩L2), as well as in the
form L = L3\L4, where h is a morphism and L1, L2, L3, L4 are linear languages.
9. Pumping lemma for regular languages: If L is a regular language over the alphabet V ,
then there are numbers p and q such that every string z ∈L with length |z| > p can be
written in the form z = uvw, with u, v, w ∈V ∗, where |uv| ≤q, v ̸= λ, so that uviw ∈L
for all i ≥0.
10. Pumping lemma for linear languages: If L is a linear language on the alphabet V ,
then there are numbers p and q such that every string z ∈L with length |z| > p can be
written in the form z = uvwxy, with u, v, w, x, y ∈V ∗, where |uvxy| ≤q and vx ̸= λ, so
that uviwxiy ∈L for all i ≥0.
11. Bar-Hillel (uvwxy, pumping) lemma for context-free languages:
If L is a context-
free language over the alphabet V , then there are numbers p and q such that every string
z ∈L with length |z| > p can be written in the form z = uvwxy, with u, v, w, x, y ∈V ∗,
where |vwx| ≤q and vx ̸= λ, so that uviwxiy ∈L for all i ≥0.

Section 17.3
LANGUAGES AND GRAMMARS
1299
12. Ogden’s pumping lemma (pumping with marked positions):
If L is a context-free
language on the alphabet V , then there is a number p such that for every string z ∈L and
for every set of at least p marked occurrences of symbols in z, we can write z = uvwxy,
where (a) either each of u, v, w or each of w, x, y contains at least one marked symbol;
(b) vwx contains at most p marked symbols; and (c) uviwxiy ∈L for all i ≥0.
13. Let G be a context-free grammar G. Then there is a grammar G′ = (N, T, S, P),
with every rule of the form A →aα, for A ∈N, a ∈T, α ∈(N ∪T )∗, such that
L(G′) = L(G)−{λ}. Moreover, there is a constructive method for transforming grammar
G into the grammar G′, which is said to be in the Greibach normal form.
14. Let G be a context-free grammar G and (k, l, m) a triple of nonnegative integers.
Then an equivalent grammar G′ = (N, T, S, P) can be eﬀectively constructed whose every
rule is in one of the following two forms:
• A →xByCz, with A, B, C ∈N, x, y, z ∈T ∗, and |x| = k, |y| = l, |z| = m;
• A →x, with A ∈N, x ∈T ∗, |x| ∈length(L(G)).
Such a grammar G′ is said to be in super normal form.
Examples:
1. The following are some regular expressions over {0, 1} and the languages they repre-
sent.
1∗
all strings with no 0s
1∗01∗
all strings with exactly one 0
1∗(0 + ǫ)1∗
all strings with one or no 0s
(0 + 1)(0 + 1)
all strings of length 2
(0 + 1)(0 + 1 + ǫ)
all strings of length 1 or 2.
2. Backus-Naur form (BNF) (or Backus normal form) for specifying computer language
syntax uses context-free production rules. Nonterminals are enclosed in brackets; the
symbol ::= is used in place of →; and all the consequents of the same antecedent are
written in the same statement with the alternative consequents separated by vertical
bars. For instance, in some programming languages, the following might be the BNF for
the lexical token called an identiﬁer.
⟨identiﬁer⟩::= ⟨letter⟩|⟨letter⟩⟨alphameric string⟩
⟨letter⟩::= a|b|c|d|e|f|g|h|i|j|k|l|m|n|o|p|q|r|s|t|u|v|w|x|y|z
⟨alphameric string⟩::= ⟨alphameric⟩|⟨alphameric string⟩⟨alphameric⟩
⟨alphameric⟩::= ⟨letter⟩|⟨digit⟩
⟨digit⟩::= 0|1|2|3|4|5|6|7|8|9
17.3.5
COMBINATORICS ON WORDS
Note: In this subsection, a word is taken to be ﬁnite.
Deﬁnitions:
A (word) variable over an alphabet V is a symbol (such as x or y) not in V whose
values range over V ∗.
A pattern in a word is a string of word variables.

1300
Chapter 17
THEORETICAL COMPUTER SCIENCE
A pattern is present in a word w ∈V ∗if there exists an assignment of strings from V +
to the variables in that pattern such that the word formed thereby is a substring of w.
A square is a word of the pattern “xx”.
A square-free word is a word with no subwords of the pattern “xx”.
A cube is a word of the pattern “xxx”.
An abelian square is a word of the form xxp, where xp is some permutation of the
word x.
A word equation over an alphabet V is an expression of the form α = β such that α
and β are words containing letters of an alphabet V and some variables over V .
A word inequality is the negation of a word equation, which is commonly written in
the form α ̸= β.
A solution to a system S of (ﬁnitely many) word equations and word inequalities is a list
of words whose substitutions for their respective variables converts every word equation
and word inequality in the system into a true proposition.
A code is a nonempty language C ⊆V + such that whenever a word w in V can be
written as a concatenation of words in C, the write-up is always unique. That is, if
w = x1 . . . xm = y1 . . . yn, where m, n ≥1, and xi, yj ∈C, then m = n and xi = yi for
i = 1, . . . , m. This property is called unique decodability.
The code indicator of a word w ∈V ∗is the number ci(w) = |V |−|w|.
The code indicator of a language is the sum of the code indicators of all words in
the language.
Facts:
1. Certain patterns are unavoidable in suﬃciently long words.
2. Squares are avoidable in alphabets with three or more letters; that is, there are
arbitrarily long square-free words.
3. Cubes are avoidable over two-letter alphabets.
4. Although squares are avoidable in three-letter alphabets, abelian squares are un-
avoidable. Every word of length ≥8 over V = {a, b, c} contains a subword of the form
xxp, x ∈V +, where xp is a permutation of x.
5. Abelian squares are avoidable in alphabets with four or more letters.
6. It is decidable (by Makanin’s algorithm) whether or not a system of word equations
and inequalities has a solution.
7. It is decidable whether or not a given ﬁnite language is a code.
8. Every code C satisﬁes the inequality ci(C) ≤1.
9. If a language C = {w1, . . . , wn} over V is not a code then, by the so-called defect
theorem, the algebraic structure of C∗can be simulated by an alphabet with at most
n−1 letters: the smallest free submonoid of V ∗containing C is generated by at most
n−1 words.
10. The following three conditions are equivalent for any two words u and v:
• {u, v} is not a code;
• u and v are powers of the same word;
• uv = vu.

Section 17.4
ALGORITHMIC COMPLEXITY
1301
(This is a corollary to Fact 9.)
11. For every word w ∈V +, there are a unique shortest word ρ(w) and an integer n ≥1
such that w = (ρ(w))n. (The word ρ(w) is called the primitive root of w.)
12. Lyndon’s theorem: If uv = vw with u, v, w ∈V ∗, then there exist words x, y ∈V ∗
and a number n ≥0 such that u = xy, w = yx and v = (xy)nx = x(yx)n.
13. If uv = vu with u, v ∈V +, then ρ(u) = ρ(v) and, consequently, u and v are powers
of the same word. (This is a corollary to Lyndon’s theorem.)
14. Assume that words um and vn have a common preﬁx or suﬃx of length |u|+|v|−d,
where u, v ∈V +, m, n ≥1 and d = gcd(|u|, |v|). Then ρ(u) = ρ(v) and |ρ(u)| ≤d. Thus,
if d = 1 then u and v are powers of the same letter.
15. If um = vn, where m, n ≥1, then u and v are powers of the same word. (This is a
corollary to Fact 14.)
16. If umvn = wp, where m, n, p ≥2, then ρ(u) = ρ(v) = ρ(w).
Examples:
1. In the alphabet V = {a, b}, the only square-free three-letter words are aba and bab.
The two possible extensions of aba by one letter are abaa, which contains the square aa,
and abab, which is a square. Similarly, both extensions of bab by one letter contain a
square. Thus, squares are unavoidable in words of length ≥4 over two-letter alphabets.
2. All solutions for the system xaba = abax, xx ̸= x, x ̸= aba, over the alphabet V =
{a, b} are (by the corollary to Lyndon’s theorem) of the form x = (aba)n, n ≥2.
17.4
ALGORITHMIC COMPLEXITY
The “complexity of an algorithm” usually means a measure of the computational eﬀort or
cost of execution, relative to the “size” of the problem. Other factors that may aﬀect this
kind of complexity are the characteristics of the particular input and the values returned
by random number generators. The most common complexity measure is running time,
but other measures, such as space utilized and number of comparisons, are sometimes
used. Another view of complexity focuses on the complicatedness of the algorithm, rather
than on the eﬀort needed to execute it.
17.4.1
OVERVIEW OF COMPLEXITY
To simplify discussion, it is assumed that every function and algorithm under consider-
ation here has one argument. (Everything is easily generalized to multivariate functions
by regarding the list of arguments as an n-tuple.)
Deﬁnitions:
A function f : N →N is asymptotic to a function g : N →N if limn→∞f(n)/g(n) = 1.
Notation: f(n) ∼g(n). (See §1.3.3.)
The input size of the argument of an algorithm is the number of bits required to specify
a value of that argument (or sometimes the argument’s numeric value).

1302
Chapter 17
THEORETICAL COMPUTER SCIENCE
A (cost-based) complexity measure for an algorithm is any of several diﬀerent asymp-
totic measures of cost or diﬃculty in running that algorithm, relative to the input size.
It is given in big-O notation (or sometimes in Θ-notation). (See §1.3.3.)
A time-complexity measure of an algorithm is a big-O expression for the number
of operations or the running time needed for a complete execution of that algorithm,
represented as a function of the size of the input.
A space-complexity measure of an algorithm is a big-O expression for the amount of
computational space needed in the execution of that algorithm, represented as a function
of the size of the input.
An algorithm runs in polynomial time if its time-complexity is dominated by a poly-
nomial.
An algorithm runs in polynomial space if its space-complexity is dominated by a
polynomial.
A Kolmogorov-Chaitin complexity measure of an algorithm is a measure based on
the number of instructions of the algorithm, which is taken as an estimate of the logical
complicatedness.
The time-complexity of a computable function is the minimum time-complexity taken
over all algorithms that compute the function.
The parallel time-complexity of a computable function is the minimum time-com-
plexity taken over all parallel algorithms that compute the function.
The space-complexity of a computable function is the minimum space-complexity
taken over all algorithms that compute the function.
A decision function is a function on a countably inﬁnite domain that decides whether
an object is in some speciﬁed subset of that domain.
A computable decision function is in class P (polynomial) if its time-complexity is
polynomial.
A computable decision function is in class NP (nondeterministic polynomial) if its
parallel time-complexity is polynomial.
A function g reduces a decision function h to a decision function f if h = f ◦g.
A computable decision function f is NP-hard if every decision function in class NP can
be reduced to f by a polynomial-time function.
A computable decision function is NP-complete if it is NP-hard and is in class NP.
A tractable problem is a set-membership problem with a decision function in class P.
Facts:
1. The previous deﬁnitions can be rephrased in terms of problems and algorithms.
• A problem is in class P (or tractable) if it can be solved by an algorithm that
runs in polynomial time;
• A problem is in class NP if, given a tentative solution (obtained by any means),
it is possible to check that the solution is correct in polynomial time;
• A problem is NP-complete if it is in class NP and is NP-hard.

Section 17.4
ALGORITHMIC COMPLEXITY
1303
2. When considering whether a given problem belongs to P or NP, and whether it
might be NP-complete, it is helpful to rewrite the problem, or an associated problem,
as a decision problem (which has a yes/no answer) because decision problems have been
easier to characterize and classify than general problems. For example, see the description
of the traveling salesman problem in Example 3 in this section.
3. Time-complexity of sorting algorithms is typically measured by the number of com-
parisons needed.
4. The words good, eﬃcient, and feasible are commonly used interchangeably to mean
polynomial-time.
5. Additive and multiplicative constants that are ignored in big-O analysis of an algo-
rithm can sometimes be too large for practical application.
6. That a problem belongs to P does not necessarily imply that it can be solved in a
practical amount of time, since the polynomial bound of its complexity can be of high
degree. Fortunately, however, for most problems in P arising in practical applications,
the polynomial bound is of relatively small degree.
7. Belonging to class NP means that a solution can be checked in polynomial time, but
not necessarily found in polynomial time.
8. When a problem is in class NP, it may be possible to solve the problem for cases
arising in practical applications in a reasonable amount of time, even though there are
other cases for which this is not true. Also, such problems can often be attacked using
approximation algorithms that do not produce the exact solution, but instead produce
a solution guaranteed to be close in some precise sense to the actual solution.
9. Every problem in class P is in class NP.
10. It often requires only a small change to transform a problem in class P to one in
class NP. For example, the ﬁrst four problems in Example 2 (Euler graph, edge cover,
linear Diophantine equation, 2-satisﬁability) are in class P, but the similar ﬁrst four
problems in Example 3 (Hamilton graph, vertex cover, quadratic Diophantine equation,
3-satisﬁability), each of which results from seemingly small changes in the respective
problem from class P, are in class NP.
11. To show a problem is NP-complete, the problem can be transformed (in a speciﬁc
way) to a problem already known to be NP-complete. This is often much easier than
showing directly that the problem is NP-complete. See [GaJo79] for details.
12. If there is an NP-hard problem that belongs to P, then P = NP.
13. Not all NP problems are known to be NP-complete. (See Example 4 for such a
problem.)
14. Deciding whether P = NP is the outstanding problem in the theory of computational
complexity.
It is the common belief that P ̸= NP, based on an extensive search for
polynomial-time solutions to various NP problems.
15. The ﬁrst problem to be shown to be NP-complete was the satisﬁability problem
(Example 3). That the satisﬁability problem is NP-complete is called Cook’s theorem,
after Steven A. Cook, who discovered it in 1971. [Co71]
16. In 1972 Richard Karp proved that the traveling salesman problem (TSP) (and many
others) were NP-complete. [Ka72]
17. Hundreds of thousands of problems (in many areas, including mathematics, com-
puter science, operations research, physics, biology) are known to be NP-complete.
18. Extensive information on NP-completeness (methods of proof, examples, etc.) can
be found in [AtBl09a], [GaJo79], [GoD´ıTu14], and [vL90].

1304
Chapter 17
THEORETICAL COMPUTER SCIENCE
19. A more formal approach to complexity, given in terms of Turing machines, appears
in §17.5.
Examples:
1. The following table gives some diﬀerent input size variables for diﬀerent problem
types.
problem type
typical input size parameters
database sorting
number of records
graph algorithms
number of vertices and/or number of edges
arithmetic computation
numbers of digits in the numerals
convex hull construction
number of points
2. The following problems are in class P.
• Euler graph: given a graph, determine whether the graph has an Euler circuit;
• edge cover: given a graph G and positive integer n, determine whether there is a
subset E of edges of G with |E| ≤n and every vertex of G an endpoint of an
edge in E;
• linear Diophantine equation: given positive integers a, b, c, determine whether
ax + by = c has a solution in positive integers x and y;
• 2-satisﬁability: given a Boolean expression in conjunctive normal form in which
each sum contains only two variables, determine whether the expression is “sat-
isﬁable” (i.e., there is an assignment of 0 and 1 to the variables such that the
expression has value 1);
• cycle rank: given a graph G and positive integer n, determine whether there is a
subset E of edges of G with |E| ≤n such that each cycle in G contains an edge
in E;
• linear programming: maximize cx subject to Ax ≤b where A is a given q × n
matrix, c is a given row vector of length n, and b is a given column vector of
length q (see §16.1.1).
3. The following problems are NP-complete.
• Hamilton graph: given a graph, determine whether the graph has a Hamilton
circuit;
• vertex cover: given a graph G and positive integer n, determine whether there
is a subset V of vertices of G with |V | ≤n with every edge of G having an
endpoint in V ;
• quadratic Diophantine equation: given positive integers a, b, c, determine whether
the equation ax2 + by = c has a solution in positive integers x and y;
• 3-satisﬁability: given a Boolean expression in conjunctive normal form in which
each sum contains only three variables, determine whether the expression is
“satisﬁable” (i.e., there is an assignment of 0 and 1 to the variables such that
the expression has value 1);
• satisﬁability: given a Boolean expression in conjunctive normal form, determine
whether the expression is “satisﬁable” (i.e., there is an assignment of 0 and 1
to the variables such that the expression has value 1) (see Fact 15);
• traveling salesman problem: given a weighted graph and positive number k, de-
termine whether there is a Hamilton circuit of weight at most k (see §10.7.1);

Section 17.4
ALGORITHMIC COMPLEXITY
1305
• independent vertex set: given a graph G and a positive integer n, determine
whether G contains an independent vertex set of size at least n;
• knapsack problem: given a set S, values ai and bi for each i ∈S, and numbers a
and b, determine whether there is a subset T ⊆S such that P
i∈T ai ≤a and
P
i∈T bi ≥b (see §16.3.1);
• bin packing problem: given k bins (each of capacity c) and a collection of weights,
determine whether the weights can be placed in the bins so that no bin has its
capacity exceeded (see §16.3.2);
• 3-coloring: given a graph G, determine whether its vertices can be colored with
3 colors;
• clique problem: given a graph G and positive integer n, determine whether G has
a clique of size at least n;
• dominating set: given a graph G and positive integer n, determine whether G has
a dominating set of size at most n;
4. The following problem is an NP problem, but not known to be NP-complete nor
known to be in P.
• graph isomorphism: given two graphs, determine whether they are isomorphic.
17.4.2
WORST-CASE AND AVERAGE-CASE ANALYSIS
Deﬁnitions:
A worst-case complexity measure of an algorithm is based on the maximum com-
putational cost for any input of that size. It is usually expressed in big-O asymptotic
notation (or sometimes Θ-notation) as a formula based on the input size variables.
An average-case complexity measure of an algorithm is based on the expected com-
putational cost over a random distribution of its inputs of a given size.
Facts:
1. Algorithmic analysis of deterministic algorithms often assumes a uniform random
distribution of the possible inputs, when the actual distribution is unknown.
2. For sorting algorithms, an average-case analysis may assume that all input permuta-
tions of the keys to be sorted are equally likely. In practice, however, some permutations
may be far more likely than others, e.g., already sorted, almost sorted, or reverse sorted.
3. The input size measures for average-case analysis are usually the same as for worst-
case analysis.
Examples:
1. The following table gives the worst-case running times of some sorting algorithms
[CoEtal09], where the input size parameter n is the number of records.
sorting method
worst-case complexity
insertion sort
Θ(n2)
selection sort
Θ(n2)
bubble sort
Θ(n2)
heapsort
Θ(n log n)
quicksort
Θ(n2)
mergesort
Θ(n log n)

1306
Chapter 17
THEORETICAL COMPUTER SCIENCE
2. The following table gives the worst-case running times of some graph algorithms,
based on input size parameters |V | and |E|, which are the numbers of vertices and edges.
graph algorithm
worst-case complexity
Kruskal’s MST algorithm
Θ(|E| log |V |)
Dijkstra’s shortest-path algorithm
O(|V |2)
with linked-list priority queue
Dijkstra’s shortest-path algorithm
O(|E| log |V |) [CoEtal09]
with heap-based priority queue
Dijkstra’s shortest-path algorithm
O(|E| + |V | log |V |) [CoEtal09]
with Fibonacci-heap priority queue
Edmonds-Karp max-ﬂow algorithm
O(|V | · |E|2)
3. The following table gives the worst-case running times of some plane convex hull
algorithms (see §13.5.1), based on the number n of points supplied as input.
convex hull algorithm
worst-case complexity
Graham Scan
Θ(n log n)
Jarvis March (“gift-wrapping”)
Θ(nh), h = # corners (convex hull)
QuickHull
O(n2)
MergeHull
O(n log n)
4. The following table gives the average-case running times of some sorting algorithms,
where the input size parameter n is the number of records.
sorting method
average-case complexity
insertion sort
O(n2)
selection sort
O(n2)
bubble sort
O(n2)
heapsort
O(n log n)
quicksort
O(n log n)
mergesort
O(n log n)
5. Randomized quicksort (Algorithm 1) [CoEtal09]: A subarray from index p to index r
of an array A is sorted, using an external subroutine random(p, r) that generates a
number in the set {p, . . . , r} in O(1) time. Another external subroutine partition(A, p, r)
rearranges the subarray A[p . . r] and returns an index q, p ≤q < r, such that for
i = p, . . . , q, A[i] ≤A[q] and such that for i = q + 1, . . . , r, A[i] > A[q]; this subroutine
runs in Θ(r −p) worst-case time.
Algorithm 1:
Randomized quicksort.
procedure randomized-quicksort(A, p, r)
if p < r then
i := random(p, r)
exchange A[p] and A[i]
q := partition(A, p, r)
randomized-quicksort(A, p, q)
randomized-quicksort(A, q + 1, r)

Section 17.4
ALGORITHMIC COMPLEXITY
1307
To sort n keys, randomized quicksort takes Θ(n2) time in the worst case (when unlucky
enough to have partition sizes always unbalanced), but only Θ(n log n) time in the average
case (partition sizes are usually at least a constant fraction of the total).
6. Convex hull: For some distributions of n points in the plane, the expected value E[h]
of the number of vertices on the convex hull is known. This bound implies that the
average-case running time of Jarvis March is an additional factor of n greater:
average-case
distribution
E[h]
running time
uniform in convex polygon
O(log n)
O(n log n)
uniform in circle
O(n
1
3 )
O(n
4
3 )
normal in plane
O(√log n)
O(n√log n)
17.4.3
LOWER BOUNDS
Lower bounds on running times of algorithms are typically given as functions of input
size using Ω-notation (see §1.3.3).
Deﬁnitions:
An existential lower bound for an algorithm is a lower bound for its running time
that holds for at least one input.
An existential lower bound for a problem is a lower bound for every algorithm that
could solve that problem.
A comparison sort is a sorting method that rearranges records based only on compar-
isons between keys.
The Euclidean minimum spanning tree (or Euclidean MST) problem has as
input vertices a set of n points in the plane and as output a spanning tree of minimum
total edge length.
A reduction of a problem A to another problem B is the following sequence of steps:
• the input to problem A is transformed into an input to problem B;
• problem B is solved on the transformed input;
• the output of problem B is transformed back into a solution to problem A for the
original input.
An f(n) time reduction of problem A to problem B is a reduction such that the time
for the three steps together is f(n).
Facts:
1. For a given model of computation, if problem A has a lower bound of T (n) and it
reduces in f(n) time to problem B, then problem B has a lower bound of T (n) −f(n).
2. Every comparison sort on n records requires Ω(n log n) comparisons in the worst case.
This follows since there are n! possible solutions, and each comparison can reduce this
by at most a factor of 2. (Note that log2(n!) is Ω(n log n).)
3. Computing the Euclidean minimum spanning tree on n points takes Ω(n log n) time
in the worst case.

1308
Chapter 17
THEORETICAL COMPUTER SCIENCE
4. Unlike the Euclidean MST problem, most graph problems have no known nontrivial
lower bound. Some graph algorithms, however, have lower bounds on their implementa-
tion.
5. Running Dijkstra’s algorithm (see §10.3.2) on a directed graph with n vertices takes
Ω(n log n) time in the worst case.
6. Finding the vertices for the convex hull of n points in the plane, in any order, takes
Ω(n log n) time in the worst case.
7. Constructing the Voronoi diagram (see §13.5.3) on n points in the plane takes in the
worst case Ω(n log n) time.
Examples:
1. An O(n)-time reduction of sorting to a gift-wrap of a convex hull:
Given a set
of n positive numbers {x1, . . . , xn}, ﬁrst produce in Θ(n) time their respective squares
{x2
1, . . . , x2
n}. Since each point (xj, x2
j) lies on the parabola given by y = x2, the Jarvis
march on the convex hull of the points (xj, x2
j) is a list of points, ordered by abscissa.
Sequentially read oﬀthe ﬁrst coordinate of every point of the convex hull in Θ(n) time,
thereby producing the sorted list of numbers. This implies that ﬁnding the gift-wrapped
convex hull of n points requires at least Ω(n log n) −Θ(n) = Ω(n log n) time.
2. An O(n)-time reduction of sorting numbers to Euclidean MST: To sort n numbers
{x1, . . . , xn}, create n points {(xi, 0) | 1 ≤i ≤n} in the Euclidean plane. The Euclidean
MST of this set contains an edge between points (xi, 0) and (xj, 0) if and only if the
numbers xi and xj are consecutive in the sorted list of numbers. The Euclidean MST
is easily converted back to a sorted list of numbers in O(n) time.
This implies that
Euclidean MST requires at least Ω(n log n) −Θ(n) = Ω(n log n) time.
17.5
COMPLEXITY CLASSES
From a formal viewpoint, complexity theory is concerned with classifying the diﬃculty
of testing for membership in various languages. This means deciding whether any given
string is in the language. The general application of complexity theory is achieved by
encoding decision problems on natural topics such as graph coloring and ﬁnding integer
solutions to equations as set membership problems. A more comprehensive coverage of
complexity theory is provided by many textbooks, including [DuKo00] and [HeOg02].
17.5.1
ORACLES AND THE POLYNOMIAL HIERARCHY
Throughout this section, whenever the alphabet is unspeciﬁed, it may be assumed to be
the binary set {0, 1}. Also, throughout this section a Turing machine (see §17.1.3) is
assumed to have among its states a unique acceptance state qA and a unique rejection
state qR. All other states continue the computation.
Deﬁnitions:
A language over an alphabet is a set of strings on that alphabet (see §17.3.2).
A nondeterministic Turing machine is a 5-tuple (K, s, h, Σ, ∆) otherwise like a deter-
ministic Turing machine, except that the transition function ∆maps each state-symbol

Section 17.5
COMPLEXITY CLASSES
1309
pair (q, b) to a set of state-symbol-direction triples.
An oracle for a language L is a special computational state to which a machine
presents a string w, which switches to special state Y (“yes”) if w ∈L and to special
state N (“no”) if w /∈L.
An oracle Turing machine is a 6-tuple (K, s, h, Σ, δ or ∆, L), equipped with an oracle
for language L and with a special second tape on which it can write a string over the
alphabet of language L (which might be diﬀerent from Σ). Aside from oracle steps, it is
a Turing machine.
A Turing machine M accepts string w if there exists a computational path from the
starting conﬁguration with input w to the acceptance state qA.
A Turing machine M rejects string w if it does not accept w.
(Either M halts in
rejection state qR or does not halt.)
The language accepted by a Turing machine M is the set of all the strings it
accepts. It is denoted L(M).
The Turing machine M decides the language L(M) if it always halts, even for input
strings not in L(M).
A (possibly nondeterministic) Turing machine M is said to be of time complexity T (n)
if for each n, each word x of length n, and (if M is nondeterministic) each sequence of
nondeterministic choices made by M on input x, M halts within T (|x|) steps.
A (possibly nondeterministic) Turing machine M is said to be of space complexity
S(n) if for each n, each word x of length n, and (if M is nondeterministic) each sequence
of nondeterministic choices made by M on input x, M scans at most S(|x|) cells of any
storage tape.
A Turing machine M has polynomial time complexity if there exists a polynomial
p(n) such that M is of time complexity T (n).
A Turing machine M has polynomial space complexity if there exists a polynomial
p(n) such that M is of space complexity T (n).
The complexity class P contains every language that can be decided by a deterministic
TM with polynomial time complexity.
The complexity class PSPACE contains every language that can be decided by a deter-
ministic TM with polynomial space complexity.
The complexity class NP contains every language that can be decided by a nondeter-
ministic TM with polynomial time complexity.
For any language L, the complexity class PL contains every language that is decided in
polynomial time by a deterministic TM with oracle L.
For any language L, the complexity class NPL contains every language that is decided
in polynomial time by a nondeterministic TM with oracle L.
For any class C of languages, the complexity class PC contains every language that is
decidable in polynomial time by a deterministic TM with oracle L ∈C. Equivalently,
notated in terms of reductions (see §17.5.2), PC = {L | (∃B ∈C)[L ≤p
T B]}.
For any class C of languages, the complexity class NPC contains every language that is
decidable in polynomial time by a nondeterministic TM with oracle L ∈C.

1310
Chapter 17
THEORETICAL COMPUTER SCIENCE
The complexity classes Σp
n are deﬁned inductively by
Σp
k =
(
P
if k = 0
NPΣp
k−1
if k ≥1.
The polynomial hierarchy PH is the collection comprising every language A for which
there exists an n such that A ∈Σp
n.
For n ≥0, the polynomial hierarchy is said to collapse to its nth level if PH = Σp
n.
The polynomial hierarchy is said to collapse if for some n ≥0 it collapses to its nth
level.
For n ≥0, the complexity class Πp
n contains every language A such that A ∈Σp
n.
The complexity class coNP is Πp
1.
The complexity class ∆p
0 is P. For n ∈Z+, the complexity class ∆p
n is PΣp
n−1.
The complexity class P/poly contains each set A such that there exist a set B ∈P, a
function h, and a polynomial f(·) such that (∀n)[|h(n)| ≤f(n)] and (∀x) [x ∈A ⇐⇒
⟨x, h(|x|)⟩∈B].
A language B is sparse if there exists a polynomial p(n) such that for every n ∈N,
there are at most p(n) elements of length n in B.
Facts:
1. The following equalities hold: Σp
0 = Πp
0 = ∆p
0 = ∆p
1 = P.
2. For n ≥0, the following relationships hold:
∆p
n ⊆Σp
n ∩Πp
n
⊆Σp
n⊆
⊆Πp
n⊆Σp
n ∪Πp
n ⊆∆p
n+1.
3. PH ⊆PSPACE.
4. If PH = PSPACE, then the polynomial hierarchy collapses.
5. Downward separation: For each n ≥0 it holds that: if Σp
n = Σp
n+1, then PH = Σp
n.
In particular, P = NP if and only if P = PH.
6. Downward separation: For each n ≥1 it holds that: if Σp
n = Πp
n, then PH = Σp
n.
7. NP = Σp
1.
8. The complexity class Σp
n is closed under union and intersection, for all n ≥0.
9. PNP∩coNP = NP ∩coNP. More generally, PΣp
n∩Πp
n = Σp
n ∩Πp
n and P∆p
n = ∆p
n, for all
n ≥0.
10. Upward separation: Nondeterministic exponential time
 S
c>0 NTIME[2cn]

is equal
to deterministic exponential time
 S
c>0 DTIME[2cn]

if and only if NP −P contains no
sparse sets.
11. Succinct certiﬁcates:
For every language in NP there is a proof scheme in which
each member (and only members) has a polynomial-size “proof” of membership that
can be checked in deterministic polynomial time.
Such a short membership proof is
sometimes called a succinct certiﬁcate.
12. Sets in P/poly are said to “have small circuits.” Many circuit classes less inclusive
than P/poly play an important role in complexity theory; see [Vo99].
13. P/poly = P{S | S is sparse}.

Section 17.5
COMPLEXITY CLASSES
1311
Examples:
1. Logical proposition problems:
The problem of deciding whether a particular as-
signment of TRUE-FALSE values to the variables satisﬁes a logical proposition is in P.
Deciding whether a proposition has an assignment that satisﬁes it is in NP. Deciding
whether all assignments satisfy it (i.e., whether the proposition is a tautology) is in coNP.
2. Graph isomorphism problems:
Deciding whether a given vertex bijection between
two graphs realizes a graph isomorphism is in P.
Deciding whether two graphs are
isomorphic is in NP.
3. Graph coloring problems:
Deciding whether an assignment of colors from a set of
three colors to the vertices of a graph is a proper coloring is in P. Deciding whether a
graph has a proper 3-coloring is in NP.
4. Unique maximum clique problem:
Deﬁne UMC to be the set of graphs G with a
clique U ⊆VG such that every other clique is strictly smaller than U. Then UMC is in
the class ∆p
2 = PNP.
5. To prove by succinct certiﬁcate that a given graph has some clique of size at least k,
one can provide a list of k distinct vertices that are mutually adjacent. (The mutual
adjacency condition for the k vertices can be veriﬁed in polynomial time.)
6. P-selectivity: A set L is said to be semi-feasible (or, equivalently, P-selective) if there
is a polynomial-time computable function f such that for all x and y, (a) f(x, y) = x or
f(x, y) = y, and (b) {x, y}∩L ̸= ∅=⇒f(x, y) ∈L. All semi-feasible sets are in P/poly.
(This can be seen by a divide and conquer argument.)
7. The primality problem: The language PRIMES, which consists of the bitstrings that
represent prime numbers when interpreted as binary numerals, is in P.
17.5.2
REDUCIBILITY AND NP-COMPLETENESS
Deﬁnitions:
A language A over alphabet Σ is polynomial-time reducible (or m-p-reducible) to
a language B, denoted A ≤p
m B, if there exists a polynomial-time computable function f
such that, for each x ∈Σ∗, it holds that x ∈A if and only if f(x) ∈B.
A language A is NP-hard if every language in NP is polynomial-time reducible to A.
A language A is NP-complete if A is NP-hard and A ∈NP.
For a class of languages C, a language A is C-hard if every language in C is polynomial-
time reducible to A.
A language A is C-complete if A is C-hard and A ∈C.
A language A is Turing-p-reducible to the language B, denoted A ≤p
T B, if there is a
deterministic oracle TM M such that M B decides A in polynomial time.
A language A is C-Turing-p-hard if every language in C is Turing-p-reducible to A.
A language A is C-Turing-p-complete if A is C-Turing-p-hard and A ∈C.
Facts:
1. For most NP-complete problems, showing membership in NP is easy.
2. For integer linear programming, however, it is easy to show NP-hardness, but showing
membership in NP is nontrivial.

1312
Chapter 17
THEORETICAL COMPUTER SCIENCE
3. Polynomial-time reducibility is also called Karp reducibility after Richard Karp.
4. Turing-p-reducibility is also called Cook reducibility after Stephen Cook.
5. The complement of any NP-complete problem is coNP-complete.
6. If A is polynomial-time reducible to B, then A is Turing-p-reducible to B.
7. If A ≤p
m B and B ≤p
m C, then A ≤p
m C.
8. If A ≤p
T B and B ≤p
T C, then A ≤p
T C.
9. Downward closure: For n ≥0, if A ∈Σp
n and B ≤p
m A then B ∈Σp
n. In particular,
if any NP-complete set is in P, then P = NP.
10. Karp-Lipton theorem: If there is a sparse NP-≤p
T-hard set, then PH = Σp
2.
11. If there is a sparse NP-≤p
T-complete set, then PH = ∆p
2.
12. Mahaney’s theorem:
If there is a sparse NP-hard (or NP-complete) set, then P =
NP.
13. Ladner’s theorem:
If P ̸= NP, then there exists a set in NP −P that is not
NP-complete.
14. A large catalog of NP-complete problems appears in [GaJo79]. A few of the most
commonly cited appear in §17.4.1. A similar catalog for higher levels of the polynomial
hierarchy appears in [ScUm02].
Examples:
1. For examples of NP-complete problems, see §17.4.1, Example 3.
2. Quantiﬁed Boolean formulas: Let QBF be the class of true statements of the form
(∃x1) (∀x2) (∃x3) (∀x4) . . . (Qzxz) [F(x1, x2, . . . , xz)],
where F is a quantiﬁer-free formula over the Boolean variables x1, . . . , xz and where Qz
is ∃if z is odd and ∀if z is even. QBF is PSPACE-complete.
3. Tautologies problem: The classic coNP-complete language is the set TAUTOLOGY
of all logical propositions that are satisﬁed by every assignment of logical values to the
variables.
4. Graph isomorphism problem:
It is not known whether the set GI of isomorphic
graph pairs is in coNP or whether GI is NP-complete, though it is known that GI is
NP-complete only if the polynomial hierarchy collapses.
17.5.3
PROBABILISTIC TURING MACHINES
Deﬁnitions:
A probabilistic Turing machine is a nondeterministic Turing machine M with exactly
two choices at each step. Each such choice occurs with probability 1
2, and is independent
of all previous choices.
The acceptance probability pM(w) that a probabilistic Turing machine M accepts
input word w is the sum of the probabilities over all accepting computation paths on
input w.
A probabilistic Turing machine M accepts language L with one-sided error if
pM(w) > 1
2 if w ∈L, and pM(w) = 0 if w /∈L.

Section 17.5
COMPLEXITY CLASSES
1313
A probabilistic Turing machine M accepts language L with two-sided error if
pM(w) > 1
2 if w ∈L, and pM(w) ≤1
2 if w /∈L.
A probabilistic Turing machine M accepts language L with bounded two-sided
error if there exists some ǫ > 0 such that for all w, pM(w) >
1
2 + ǫ if w ∈L and
pM(w) < 1
2 −ǫ if w /∈L.
The complexity class RP (the random polynomial-time languages) is the class of lan-
guages that are decided by Turing machines with one-sided error in polynomial time.
The complexity class coRP contains the language A if A ∈RP.
The complexity class ZPP (the zero-error probabilistic polynomial-time languages) is
the intersection RP ∩coRP.
The complexity class PP (the probabilistic polynomial-time languages) is the class of
languages that are decided by Turing machines with two-sided error in polynomial time.
The complexity class BPP (the bounded-error probabilistic polynomial-time languages)
is decided by Turing machines with bounded two-sided error in polynomial time.
The complexity class #P is the class of all functions f such that there exists a nonde-
terministic polynomial-time Turing machine M such that, for each x, f(x) = #accM(x),
where #accM(x) denotes the number of accepting computation paths of machine M on
input x.
The major types of polynomial-time reductions deﬁned for functions include Turing re-
ductions, metric reductions, many-one reductions, and parsimonious reductions. The
latter three of these are deﬁned as follows. A function e : Σ∗→N polynomial-time
metric reduces to a function h : Σ∗→N if there exist two polynomial-time computable
functions ϕ and ψ such that (∀x ∈Σ∗)[e(x) = ψ(x, h(ϕ(x)))]. A function e : Σ∗→N
polynomial-time many-one reduces to a function h : Σ∗→N if there exist two
polynomial-time computable functions ϕ and ψ such that (∀x ∈Σ∗)[e(x) = ψ(h(ϕ(x)))].
A function e : Σ∗→N parsimoniously reduces to a function h : Σ∗→N if there is a
polynomial-time computable function ϕ such that (∀x ∈Σ∗)[e(x) = h(ϕ(x))].
Let ≤α be a reduction that is deﬁned for functions. A function f is said to be #P-
complete with respect to ≤α reductions if f ∈#P and for every g ∈#P it holds
that g ≤α f.
Facts:
1. ZPP is exactly the class of languages accepted by error-free probabilistic Turing
machines running in expected polynomial time.
2. ZPP = RP ∩coRP ⊆RP ⊆
⊆coRP⊆RP ∪coRP ⊆BPP ⊆PP ⊆PSPACE.
3. RP ⊆NP ⊆PP.
4. ZPPZPP = PZPP = ZPP; BPPBPP = PBPP = BPP.
5. BPP ⊆P/poly.
6. BPP ⊆NPBPP ⊆ZPPNP ⊆Σp
2 ∩Πp
2.
7. If there is a sparse NP-≤p
T-hard set, then PH = ZPPNP. (This extends the Karp-
Lipton theorem of §17.5.2.)
8. PH ⊆PPPH ⊆PPP = P#P.
9. PP is closed under all Boolean operations.
10. If NP ⊆BPP then BPP = PH and RP = NP.

1314
Chapter 17
THEORETICAL COMPUTER SCIENCE
11. It remains an open question whether BPP, RP, coRP, or ZPP have complete lan-
guages.
Examples:
1. SAT ∈PP: Consider a probabilistic polynomial-time Turing machine M that, given
a proposition F, immediately ﬂips its coin. If the result is “heads”, then proposition F
is accepted and machine M halts. If “tails”, then the machine, by a series of coin ﬂips,
randomly assigns each variable to be either true or false, and (on that computation
path) accepts F if the resulting assignment satisﬁes the proposition. Thus F is accepted
with probability exactly 1
2 if F is unsatisﬁable, and is accepted with probability at least
1
2 +
1
2k+1 if F is satisﬁable, where k is the number of logical variables in F.
Thus
SAT ∈PP. This implies that NP ⊆PP, since the language SAT is NP-complete.
2. Equality of polynomial products: Given two lists of rational-coeﬃcient polynomials,
where each polynomial in the lists has been speciﬁed by a list of (coeﬃcient, degree)
pairs, the problem of deciding whether the product of the polynomials in the ﬁrst list
yields the same polynomial as the product of the polynomials in the second list is in the
class coRP. This is because if the products are equal, then they will evaluate to the same
value on any argument, yet if the products are unequal then it can be argued that the
products will diﬀer with suﬃcient probability on a randomly chosen argument.
3. MAJORITY-SAT is PP-complete:
The language MAJORITY-SAT is the set of
(quantiﬁer-free) Boolean formulas F such that F is satisﬁed by more than half of the
possible variable assignments.
4. #SAT: #SAT, the function that maps from Boolean formulas F to the number of
solutions of F, is #P-complete with respect to parsimonious reductions (and thus with
respect to many-one, metric, and Turing reductions).
5. #SAT Is as Hard to Enumeratively Approximate (aka List Approximate) as It Is to
Compute Exactly:
If there exists a polynomial-time function f mapping from Boolean
formulas to lists of integers such that for each Boolean formula F it holds that the number
of solutions of F (i.e., #SAT (F)) is one of the integers in the list f(F), then #SAT can
be computed in polynomial time.
6. The Power Index: Power indices are very important in cooperative game theory and
political science. Given a list of states and for each a positive natural number indicating
the number of votes that state has, the raw Shapley-Shubik power index of a state s is
the number of permutations of the list in which the sum of the weights of all the states
coming before s in the permutation is less than or equal to half the total weight but the
sum of the weights of all the states up to and including s is strictly greater than half the
total weight. Informally, the index is a measure of how often s is the player who puts
a coalition over the ﬁnish line if players join the coalition in random order. The raw
Shapley-Shubik power index is #P-complete with respect to many-one reductions (and
thus with respect to metric and Turing reductions), but is not #P-complete with respect
to parsimonious reductions.

Section 17.6
RANDOMIZED ALGORITHMS
1315
17.6
RANDOMIZED ALGORITHMS
General randomization principles for algorithms have many speciﬁc applications.
In
particular, random algorithms from number theory have applications in cryptography
and ﬁngerprinting, Also, randomized algorithms for partitioning, for searching and sort-
ing, and for graph problems such as mincut and matching, including some heuristics
for NP-complete problems, have applications in testing and applications for parallel or
distributed environments.
17.6.1
OVERVIEW AND GENERAL PARADIGMS
Most randomized algorithms follow a few general paradigms. For many further topics
not covered here, see the excellent survey papers [Ka91,We83] and also the textbook
[MoRa95].
Deﬁnition:
A randomized algorithm is an algorithm that makes random choices during its ex-
ecution. Such random choices are guided by the output of a random (or, in practice,
pseudo-random) number generator.
Facts:
1. Many problems have no known deterministic algorithms to match the eﬃciency of
randomized algorithms. Even for problems for which eﬃcient deterministic algorithms
are known, randomized algorithms are often remarkably easier to understand and imple-
ment.
2. Worst-case instances of a randomized algorithm occur when the algorithm performs
badly for the overwhelming majority of its probabilistic choices.
3. Abundance of witnesses paradigm:
Deciding whether a given input has a certain
property sometimes reduces to ﬁnding a combinatorial object “witnessing” the property.
When the space of all potential witnesses is too large to be searched exhaustively, it
sometimes suﬃces to inspect a small random sample, selected so that one of the elements
of the sample will be a suitable witness with very high probability.
4. Random sampling: Sometimes a small random sample is indicative of the population
as a whole.
5. Intuitively, the power of randomization is analogous to the standard game-theoretic
fact that probabilistic game strategies are substantially more eﬀective than deterministic
ones. That is, an algorithm can be regarded as a player, and the problem to be solved
can be regarded as an adversary trying to present the player with input instances on
which the algorithm exhibits worst-case performance.
6. If an algorithm is deterministic, then the game-theoretic adversary knows in advance
the entire strategy of the player. Thus, the worst-case instances are well deﬁned and
can be presented as input to the algorithm. If an algorithm is probabilistic, then the
game-theoretic adversary does not know in advance the output of the random number
generator.
In particular, worst-case instances under deterministic strategies may be
smoothed out by randomization.

1316
Chapter 17
THEORETICAL COMPUTER SCIENCE
Examples:
1. Cryptography:
Some public key cryptography is based on the sharp dichotomy
between the eﬃciency of deciding whether a number is prime or composite, and the
apparent hardness of actually factoring composite numbers.
2. Fingerprinting:
A large data object is represented by a much smaller “ﬁngerprint”
such that, with very high probability, distinct objects map to distinct ﬁngerprints. A
similar strategy is used for “hashing” (see §18.4) where large objects are mapped to much
smaller keys with very low probabilities of collisions.
3. Testing identities: It is often possible to check if an algebraic expression is identically
equal to zero by substituting random values for the variables and checking whether the
expression evaluates to zero.
4. Symmetry breaking: It is often necessary for a set of processes to come collectively
to an arbitrary but consistent decision among a set of indistinguishable possibilities.
There is a method to break such symmetries using randomization: this yields an eﬃcient
parallel perfect matching algorithm, as well as protocols for distributed environments,
computation in the presence of errors, and Byzantine agreements.
5. Load balancing: For problems involving choice among resources, such as processors
or communication links, randomization can be useful in spreading out the load.
6. The probabilistic method: The probabilistic method is to demonstrate that a combi-
natorial object of interest occurs with nonzero probability in a suitably deﬁned probabil-
ity space. Sometimes the probabilistic method yields eﬃcient algorithmic constructions
rather than mere existential arguments. (See §7.11.)
17.6.2
LAS VEGAS AND MONTE CARLO ALGORITHMS
Randomized algorithms are classiﬁed into two types: Monte Carlo algorithms and Las
Vegas algorithms.
Deﬁnitions:
A Monte Carlo algorithm has bounded running time and produces correct output
with probability bounded away from zero.
The success ampliﬁcation method for a Monte Carlo algorithm is to perform k
independent runs of the algorithm.
A Las Vegas algorithm always produces correct output. However, its running time is
a random variable, whose expectation and variance are quantiﬁed in the analysis of the
algorithm.
The success ampliﬁcation method for a Las Vegas algorithm is to perform k/2
independent Las Vegas runs of 2E[T ] steps each, where E[T ] is the expected Las Vegas
running time.
The Monte Carlo to Las Vegas transformation, starting from a Monte Carlo algo-
rithm, is the Las Vegas algorithm of repeatedly running the Monte Carlo algorithm until
a success occurs.
The Las Vegas to Monte Carlo transformation, starting from a Las Vegas algo-
rithm, is the Monte Carlo algorithm obtained by running the Las Vegas scheme for kE[T ]
steps and halting.

Section 17.6
RANDOMIZED ALGORITHMS
1317
Facts:
1. If the probability of success of a single run is p, then the probability under the success
ampliﬁcation method that k independent runs fail is (1 −p)k. Thus, the probability of
success becomes 1 −(1 −p)k.
2. If p is the probability of success of a Monte Carlo algorithm, then the expected
number of Las Vegas trials before a success occurs is 1/p.
3. Markov’s inequality:
The probability that a positive random variable exceeds k
times its expectation is at most 1
k.
4. Markov’s inequality yields a general method to bound variances of Las Vegas algo-
rithms: If T is the running time of a Las Vegas algorithm, then Pr[ T > kE[T ] ] < 1/k.
5. The probability that a transformed Las Vegas to Monte Carlo algorithm is successful
is at least 1 −1/k.
6. If the expected running time of a Las Vegas algorithm is E(T ), then the running
time of the ampliﬁed algorithm is kE[T ]. However, the probability of success becomes
1 −( 1
2)
k
2 .
Examples:
1. A database problem:
In a large database whose keys are stored in no particular
order, ﬁnd a key that is not contained in that database, within time O(N), where N
is the size of the database. This would match the natural lower bound of Ω(N), the
time required just to read the entire database. The deterministic strategy of sorting and
checking for the ﬁrst missing key would take time O(N log N). Assume N = 225 and
that the keys are 32 digits long.
• a Monte Carlo randomized strategy:
Pick a random 32-digit key and then scan
the database! There are 232 potential keys and only N keys in the database.
Thus, the probability that a randomly chosen key is not in the database is
at least 1 −2−7, which is greater than 99%. The running time is dominated
by a single scan of the database to check whether the randomly chosen key is
suitable. Thus, it completes in O(N) time.
• success ampliﬁcation:
The probability that among k independently chosen ran-
dom keys none is found suitable is less than 0.01k = 10−2k; this quantity be-
comes negligible, even for very small values of k.
The running time of this
ampliﬁed algorithm is O(kN).
• from Monte Carlo to Las Vegas:
Repeatedly pick random keys until a suitable
key is found. The expected number of trials before a suitable key is found is
1
p < 100
99 . Thus, the expected running time is O( 100N
99 ) = O(N).
2. Finding the median: Among n keys ﬁnd the mth in increasing order. The determin-
istic strategy of sorting would take time O(n log n). Algorithm 1 does this within time
O(n) as follows.
• a Las Vegas randomized strategy:
Pick a random key s from the database and
consider the sets X and Y of keys in the database that are smaller and larger, re-
spectively, than s. If |X| ≥m, then the problem reduces to ﬁnding the mth key
in X. If n −|Y | ≤m, then the problem reduces to ﬁnding the (m−(n−|Y |))th
key in Y . Finally, if |X| < m < n −|Y |, then s is the mth key.

1318
Chapter 17
THEORETICAL COMPUTER SCIENCE
Algorithm 1:
In a set A of n distinct keys, ﬁnd the mth smallest.
input: a set A with n = |A|, and an integer m with 1 ≤m ≤n
FIND (A, m)
if A = {s} then return s
else
pick s uniformly at random from A
compute X = {a ∈A | a < s} and Y = {a ∈A | a > s}
if |X| ≥m then call FIND(X, m)
if n −|Y | ≤m then call FIND(Y, m −(n −|Y |))
if |X| < m < n −|Y | then return s
• expected running time: The randomly chosen key s splits the database into pieces
X and Y which are, on average, of size n
2 , and in most cases substantially smaller
than n. Thus, the problem of looking for a key in a set of size n reduces to a
problem of looking for a key in a set of size “approximately” n
2 and a running
time of the type T (n) ≈T ( n
2 )+O(n) = O(n) can intuitively be expected. More
precisely, let T (n, m) denote the running time to ﬁnd the mth key. Since any
of the keys could equally likely be picked as the splitter s, one can formulate a
recurrence for the expectation E[T (n, m)], which solves to E[T (n, m)] = O(n),
for all m.
• variance: Markov’s inequality bounds the variance of the running time by
Pr[ T (n, m) > kE[T (n, m)] ] < 1
k.
3. Primality testing: Algorithm 2 produces correct output with probability at least
1 −( 1
2)k. Thus, after log n trials of selecting a random integer less than n and testing,
the likelihood is very high for reasonably large n, that a prime number will be obtained.
This follows from the prime number theorem and Markov’s inequality.
Algorithm 2:
Test primality of n (with k witnesses).
input: positive integers n and k with n odd
pick a1, . . . , ak, each independently and uniformly at random from {2, 3, . . ., n−1}
compute gcd(n, ai) for all 1 ≤i ≤k
{gcd(n, ai) can be computed eﬃciently using the Euclidean algorithm}
if there exists an ai with gcd(n, ai) ̸= 1 then output “composite” and halt
else: compute zi = a(n−1)/2
i
(mod n) for all ai with 1 ≤i ≤k
{zi can be computed eﬃciently by repeated squaring}
if for some i, zi ̸≡±1 (mod n) then output “composite”
else if for some i, zi ≡−1 (mod n) then output “probably prime”
else output “probably composite”

REFERENCES
1319
REFERENCES
Printed Resources:
[AhEtal06] A. V. Aho, M. S. Lam, R. Sethi, and J. D. Ullman, Compilers: Principles,
Techniques, and Tools, 2nd ed., Addison Wesley, 2006.
[AhMaOr93] R. K. Ahuja, T. L. Magnanti, and J. B. Orlin, Network Flows: Theory,
Algorithms, and Applications, Prentice Hall, 1993.
[AtBl09a] M. J. Atallah and M. Blanton, Algorithms and Theory of Computation Hand-
book, Volume 1: General Concepts and Techniques, 2nd ed., Chapman & Hall/CRC,
2009.
[AtBl09b] M. J. Atallah and M. Blanton, Algorithms and Theory of Computation Hand-
book, Volume 2: Special Topics and Techniques, 2nd ed., Chapman & Hall/CRC,
2009.
[Co71] S. A. Cook, “The complexity of theorem-proving procedures”, Proceedings of the
Third Annual ACM Symposium on the Theory of Computing, 1971, 151–158.
[CoEtal09] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. S. Stein, Introduction to
Algorithms, 3rd ed., MIT Press, 2009.
[DuKo00] D-Z. Du and K-I Ko, Theory of Computational Complexity, Wiley, 2000.
[Ed65] J. Edmonds, “Paths, trees and ﬂowers”, Canadian Journal of Mathematics 17
(1965), 449–467.
[FlBe94] R. W. Floyd and R. Beigel, The Language of Machines: An Introduction to
Computability and Formal Languages, Computer Science Press, 1994.
[Fr77] R. Frievalds, “Probabilistic machines can use less running time”, B. Gilchrist (ed.),
Information Processing 77, Proceedings of IFIP 77, North-Holland, 1977, 839–842.
[GaJo79] M. R. Garey and D. S. Johnson, Computers and Intractability, A Guide to the
Theory of NP-Completeness, W. H. Freeman, 1979.
[GoD´ıTu14] T. Gonzalez, J. D´ıaz-Herrera, and A. Tucker, eds., Computing Handbook:
Computer Science and Software Engineering, 3rd ed., CRC Press, 2014.
[GrHoRu95] R. Greenlaw, H. J. Hoover, and W. L. Ruzzo, Limits to Parallel Computa-
tion: P-Completeness Theory, Oxford University Press, 1995.
[HeOg02] L. A. Hemaspaandra and M. Ogihara, The Complexity Theory Companion,
Springer-Verlag, 2002.
[Jo81] D. S. Johnson, “The NP-completeness column: an ongoing guide (1st edition)”,
Journal of Algorithms 2 (1981), 393–405.
[Jo90] D. S. Johnson, “A catalog of complexity classes”, Handbook of Theoretical Com-
puter Science, J. van Leeuwen (ed.), MIT Press/Elsevier, 1990, 67–161.
[Ka93] D. R. Karger, “Global mincuts in RNC, and other ramiﬁcations of a simple min-
cut algorithm”, Proceedings of the 4th Annual ACM-SIAM Symposium on Discrete
Algorithms, 1993, 21–30.
[Ka72] R. M. Karp, “Reducibility among combinatorial problems”, Complexity of Com-
puter Computations, R. E. Miller and J. W. Thatcher (eds.), Plenum Press, 1972,
85–103.

1320
Chapter 17
THEORETICAL COMPUTER SCIENCE
[Ka91] R. M. Karp, “An introduction to randomized algorithms”, Discrete Applied Math-
ematics 34 (1991), 165–201.
[Kn97] D. E. Knuth, Seminumerical Algorithms, Volume 2 of The Art of Computer
Programming, 3rd ed., Addison-Wesley, 1997.
[LePa97] H. R. Lewis and C. H. Papadimitriou, Elements of the Theory of Computation,
2nd ed., Prentice-Hall, 1997.
[Li16] P. Linz, An Introduction to Formal Languages and Automata, 6th ed., Jones and
Bartlett, 2016.
[Lo83] M. Lothaire, Combinatorics on Words, Addison-Wesley, 1983.
[Lo79] L. Lov´asz, “On determinants, matchings, and random algorithms”, L. Budach
(ed.), Fundamentals of Computing Theory, Akademia-Verlag, 1979.
[Ma93] Y. Matiyasevich, Hilbert’s Tenth Problem, MIT Press, 1993.
[MoRa95] R. Motwani and P. Raghavan, Randomized Algorithms, Cambridge University
Press, 1995.
[MuVaVa87] K. Mulmeley, U. V. Vazirani, and V. V. Vazirani, “Matching is as easy as
matrix inversion”, Combinatorica 7 (1987), 105–113.
[Pa94] C. Papadimitriou, Computational Complexity, Addison-Wesley, 1994.
[PaYa91] C. H. Papadimitriou and M. Yannakakis, “Optimization, approximation, and
complexity classes”, Journal of Computer and Systems Sciences 43 (1991), 425–440.
[PrSh85] F. P. Preparata and M. I. Shamos, Computational Geometry: An Introduction,
Springer-Verlag, 1985.
[Ra80] M. O. Rabin, “Probabilistic algorithms for testing primality”, Journal of Number
Theory 12 (1980), 128–138.
[R´e12] G. E. R´ev´esz, Introduction to Formal Languages, Dover, 2012.
[RiShAd78] R. L. Rivest, A. Shamir, and L. Adleman, “A method for obtaining digital
signatures and public key cryptosystems”, Communications of the ACM 21 (1978),
120–126.
[Sa85] A. Salomaa, Computation and Automata, Cambridge University Press, 1985.
[ScUm02] M. Schaefer and C. Umans, “Completeness in the polynomial-time hierarchy:
Part I: A compendium”, SIGACT News 33 (2002), 32–49.
[SeWa11] R. Sedgewick and K. Wayne, Algorithms, 4th ed., Addison-Wesley, 2011.
[Si14] M. Sipser, Introduction to the Theory of Computation, 3rd ed., Cengage, 2014.
[So87] R. I. Soare, Recursively Enumerable Sets and Degrees, Perspectives in Mathemat-
ical Logic, Springer-Verlag, 1987.
[SoSt77] R. Solovay and V. Strassen, “A fast Monte Carlo test for primality”, SIAM
Journal on Computing 6 (1977), 84–85.
[Ta83] R. E. Tarjan, Data Structures and Network Algorithms, SIAM, 1983.
[vL90] J. van Leeuwen, ed., Handbook of Theoretical Computer Science, Vol. A: Algo-
rithms and Complexity, Elsevier, 1990.
[Va82] L. G. Valiant, “A scheme for fast parallel communication”, SIAM Journal on
Computing 11 (1982), 350–361.

REFERENCES
1321
[Vo99] H. Vollmer, Introduction to Circuit Complexity: A Uniform Approach, Springer-
Verlag, 1999.
[We83] D. J. A. Welsh, “Randomized algorithms”, Discrete Applied Mathematics 5
(1983), 133-145.
Web Resources:
cafaq.com/lifefaq (Frequently Asked Questions About Cellular Automata.)
www3.cs.stonybrook.edu/~algorith/ (The Stony Brook Algorithm Repository.)
www.drb.insel.de/~heiner/BB/
(Busy Beaver Turing Machines.)
wwwhomes.uni-bielefeld.de/achim/gol.html (Achim’s Game of Life page.)


18
INFORMATION STRUCTURES
18.1 Abstract Datatypes
Charles H. Goldberg
18.1.1 Abstract Speciﬁcation of Records and Databases
18.1.2 Stacks and Queues
18.1.3 Two-Way Sequential Lists
18.1.4 Dictionaries and Random Access Lists
18.1.5 Priority Queues
18.2 Concrete Data Structures
Jonathan L. Gross
18.2.1 Modeling Computer Storage and Retrieval
18.2.2 Arrays and Linked Lists
18.2.3 Binary Search Trees
18.2.4 Priority Trees and Heaps
18.2.5 Network Incidence Structures
18.3 Sorting and Searching
Jianer Chen
18.3.1 Generic Concepts for Sorting and Searching
18.3.2 Sorting by Expanding a Sorted Subsequence
18.3.3 Sorting by Exchanging Out-of-Order Pairs
18.3.4 Sorting by Divide-and-Conquer
18.3.5 Sorting by Distribution
18.3.6 Searching
18.4 Hashing
Viera Krnanova Proulx
18.4.1 Hash Functions
18.4.2 Collision Resolution
18.5 Dynamic Graph Algorithms
Joan Feigenbaum and
18.5.1 Dynamically Maintainable Properties
Sampath Kannan
18.5.2 Techniques
18.5.3 Applications
18.5.4 Recent Results and Open Questions

1324
Chapter 18
INFORMATION STRUCTURES
INTRODUCTION
Information structures are groupings of related information into records and organization
of the records into databases. The mathematical structure of a record is speciﬁed as
an abstract datatype and represented concretely as a linkage of segments of computer
memory. General chapter references are [AhHoUl83], [Kn97], and [Kn98].
GLOSSARY
abstract datatype (ADT): a mathematically speciﬁed datatype equipped with oper-
ations that can be performed on its data objects.
adaptive bubblesort: a bubblesort that stops the ﬁrst time a scan produces no trans-
positions.
ADT-constructor: any of the three operations string of, set of, or tuple of used to
build more complex ADTs from simpler ADTs.
alphabetic datatype: an elementary datatype whose domain is a ﬁnite set of symbols,
and whose only primary operation is a total ordering query.
ambivalent data structure: a structure that keeps track of several alternatives at
many of its vertices, even though a global examination of the structure would deter-
mine which of these alternatives is optimal.
array data structure: an indexed sequence of cells ⟨aj | j = d, . . . , u⟩of ﬁxed size,
with consecutive indices.
AVL tree: a binary search tree with the property that the two subtrees of each node
diﬀer by at most one in height.
binary search: a recursive search method that proceeds by comparing the target key
to the key in the middle of the list, in order to determine which half of the list could
contain the target item, if it is present.
binary search tree: a binary tree in which the key at each node is larger than all the
keys in its left subtree, but smaller than all the keys in its right subtree.
binary search tree structure: a binary-tree structure in which for every cell, all cells
accessible through the left child have lower keys, and all cells accessible through the
right child have higher keys.
binary tree structure: a tree structure such that each cell has two pointers.
bubblesort: a sort that repeatedly scans an array from the highest index to the low-
est, on each iteration swapping every out-of-order pair of consecutive items that is
encountered.
cell (in a concrete data structure): a storage unit within the data structure that may
contain data and pointers to other cells.
certiﬁcate (for a property of a graph G): another graph that has the speciﬁed prop-
erty if and only if the graph G has the property.
chaining method (for hash tables): a hashing method that resolves collisions by plac-
ing all the records whose keys map to the same location in the main array into a
linked list (chain), which is rooted at that location, but stored in the secondary array.

GLOSSARY
1325
circular linked list: a set of cells, each with two pointers, one designated as its forward
pointer and the other as its backward pointer, plus a header with one or more pointers
to current cells, such that these conditions hold: (1) the sequence of cells formed by
following the forward pointers, starting from any cell, traverses the entire set and
returns to the starting cell; and (2) the sequence of cells formed by following the
backward pointers, starting from any cell, traverses the entire set and returns to the
starting cell.
closed hash table: a hash table in which collisions are resolved without the use of
secondary storage space, that is, by probing in the main array to ﬁnd available
locations.
cluster (in a spanning tree): a set of vertices whose induced subgraph is connected.
clustering property (of a probe function): the undesirable possibility that parts of
the probe sequences generated for two diﬀerent keys are identical.
collision instance (of a hash function): a pair of diﬀerent keys for which the value of
the hash function is the same.
collision resolution (of a hashing process): a procedure within the hashing process
used to deﬁne a sequence of alternative locations for storage of a record whose key
collides with the key of an existing record in the table.
comparison sort: a sorting method in which the ﬁnal sorted order is based solely on
comparisons between elements in the input sequence.
concrete data structure: a mathematical model for storing the current value of a
structured variable in computer memory.
database: a set of records, stored in a computer.
datatype: a set of objects, called the domain, and a set of mappings, called primary
operations, from the domain to itself or to the domain of some other datatype.
deheaping: removing the highest priority entry from a heap and patching the result so
that the heap properties are restored.
dictionary: an abstract datatype whose domain is a set of keyed pairs, in which arbi-
trary pairs may be accessed directly.
domain (of a datatype): the set of objects within that datatype.
dyadic graph property: a property deﬁned with respect to pairs of vertices.
dynamic structure (for a database): an information structure for the database whose
conﬁguration may be changed, for instance, by the insertion or deletion of elements.
dynamic update operation: (on a graph) an operation that changes the graph and
keeps track of whether the graph has some designated property.
edge-incidence table (for a graph): a dictionary whose keys are the vertices of a graph
or digraph. The data component for each key vertex is a list of all the edges that
are incident on that vertex. Each self-loop occurs twice in the list.
elementary datatype: an alphabetic datatype or a numeric datatype, usually in-
tended for direct representation in the hardware of a computer.
endpoint table (for a graph): a dictionary whose keys are the edges. The data com-
ponent for each key edge is the set of endpoints for that edge. If an edge is directed,
then its endpoints are marked as head and tail.
enheaping: placing a new entry into its correctly prioritized position in a heap.

1326
Chapter 18
INFORMATION STRUCTURES
entry (in a database): a 2-tuple, whose ﬁrst component is a key, and whose second
component is some data; also called a record.
external sorting method: a method that uses external storage, such as hard disk or
tape, outside the main memory during the sorting process.
far end (of a one-way linked list): the cell that contains a null pointer.
Fibonacci heap: a modiﬁcation of a heap, using the Fibonacci sequence, that permits
more eﬃcient implementation of a priority queue than a heap based on a left-complete
binary tree.
FIFO property (of a database): the property that the item retrieved is always the
item inserted the longest ago. FIFO means “ﬁrst-in-ﬁrst-out”.
ﬂat notation (in a postcondition of a primary operation speciﬁcation): the value X♭of
the variable X before the speciﬁed operation is executed.
fullness (of a closed hash table): the ratio of the number of records presently in the
table to the size of the table.
generic datatype: a speciﬁcation in an ADT-template that means that there are no
restrictions whatsoever on that datatype.
hash function (for storing records in a table): a function that maps each key to a lo-
cation in the table.
hash table: an array of locations for records (entries) in which each record is identiﬁed
by a unique key, and in which a hash function is used to perform the table-access op-
erations (of insertion, deletion, and search), possibly involving the use of a secondary
array to resolve competition for locations.
hashing: storage-retrieval in a large table in which the table location is computed from
the key of each data entry.
header (of a concrete data structure): a special memory unit (but not a cell) that con-
tains current information about the entire conﬁguration and pointers to some critical
cells in the structure.
heap: a concrete data structure that represents a priority tree as an array.
heapsort: sorting a set of entries by ﬁrst enheaping all items and then successively
deheaping them.
incidence matrix (for a graph): a 0-1 matrix that speciﬁes the incidence relation,
where rows are indexed by the vertices and columns by the edges. The entry in
the row corresponding to vertex v and edge e is 1, if v is an endpoint of e, and 0
otherwise.
in-place realization (of a sorting method): a method that uses, beyond the space need-
ed for one copy of each data entry, only a constant amount of additional space,
regardless of the size of the list to be sorted.
insertion sort: a sort that transforms an unsorted list into a sorted list by iteratively
transferring the next item from the remaining items in the unsorted input list and
inserting it into correct position in the sorted output list.
internal sorting method: any method that keeps all the entries in the primary mem-
ory of the computer during the process of rearrangement.
key (in a database entry): a value from an ordered set, used to store and retrieve data.
key domain: the ordered set from which values of keys are drawn.

GLOSSARY
1327
key entry (of a record in a table): a value from an ordered set (e.g., integer identiﬁca-
tion codes or alphabetic strings) used to store records in the table.
key randomization: a “preliminary” procedure within the hashing process for map-
ping non-numeric keys (or keys with poor distribution) into (more uniformly) random
distributed integers in the domain of the hash function.
keyed pair: a 2-tuple whose ﬁrst component, called a key, is used to locate the data
in the second component.
left child (of a cell in a binary tree structure): the cell to which the ﬁrst pointer points.
left-complete binary tree: either a binary tree that is complete, or a balanced binary
tree (§9.1.2) such that at depth one less than the maximum, the following hold: (1)
all nodes with two children are to the left of all nodes with one or no children; (2)
all nodes with no children are to the right of all nodes with one or two children; and
(3) there is at most one node with only one child, which must be a left child.
LIFO property (of a database): the property that the item retrieved is always the
item most recently inserted. LIFO means “last-in-ﬁrst-out”.
linear search: the technique of scanning the entries of a list in sequence, until either
some stopping condition occurs or the entire list has been scanned.
mergesort: a sort that partitions an unsorted list into lists of length one and then
iteratively merges them until a single sorted list is obtained.
near end (of a one-way linked list): the cell that is pointed to by the header and by no
other cell.
nearly complete (property of a binary tree): the possible property that the binary
tree is complete at every level except possibly at the bottom level. At the bottom
level, all the missing leaves are to the right of all the present leaves.
null pointer: a pointer that points to an artiﬁcial location, which serves as a signal to
an algorithm to react somewhat diﬀerently than to a pointer to an actual location.
numeric datatype: an elementary datatype whose domain is a set of numbers and
whose primary operations are a total ordering query and the arithmetic operators +
(addition), × (multiplication), and −(change of sign).
one-way linked list: a set of cells, each with one pointer, such that: (1) exactly one of
these cells is pointed to by the header but by no cell; (2) exactly one cell contains a
null pointer; and (3) the sequence of cells formed by following the pointers, starting
from the header, traverses the entire set, ending with the cell containing the null
pointer.
open hash table: a hash table that uses a secondary array to resolve collisions.
ordered datatype: a datatype with an order relation such that any two elements can
be compared.
pivot (in a quicksort): an entry at which the sequence is split.
plane graph: a planar graph, together with a particular imbedding in the plane.
pointer (to a cell): a representation of that cell’s location in computer memory.
postcondition (of a primary operation): a list of conditions that must hold after the
operation is executed, if the precondition is satisﬁed when the operation commences.
precondition (of a primary operation): a list of conditions that must hold immediately
before the operation is executed, for the operation to execute as speciﬁed.

1328
Chapter 18
INFORMATION STRUCTURES
primary key: the key component of highest precedence, when the key has more than
one component.
primary operation (for a datatype): a basic operation that retrieves information from
an object in the domain or modiﬁes the object.
priority queue: an abstract datatype whose domain is a set of records, in which only
the entry with the largest key is immediately accessible.
priority tree: a nearly complete binary tree whose nodes are assigned data entries from
an ordered set of “priorities”, such that there is no node whose priority supersedes
the priority of its parent node.
probe function: a function used iteratively to calculate an alternative location in a
closed hash table when the initial location calculated from the key or the previous
probe location is already occupied.
probe sequence (for a hash table location): the sequence of locations calculated by
the probe function in its eﬀort to ﬁnd an unoccupied place in the table.
query (to a datatype): a primary or secondary operation that changes nothing and
returns a logical value, i.e., true or false.
queue: an abstract datatype that organizes the records into a sequence, such that
records are inserted at one end (called the back of the queue) and extractions are
made from the other end (called the front).
quicksort: sorting by recursively partitioning a list around an entry (the pivot) so that
all smaller items precede the pivot and all larger items follow it.
radix sort: a sort using iterative partitioning into queues and recombining by concate-
nation, in which the partitioning is based on a digit in a numeral.
random access list: an abstract datatype whose domain is a set of records such that
the values of the key ﬁeld range within an interval of integers a ≤k ≤b; this permits
implementations that execute primary operations faster than a general table.
rank (of an element of a ﬁnite ordered set): the number of elements that it exceeds or
equals.
rank-counting sort: sorting by calculating the rank for each element, and then assign-
ing each element to its correct position according to its rank.
record: a 2-tuple, whose ﬁrst component is a key, and whose second component is some
data; also called an entry.
record in a table: a table entry containing a key and some data.
right child (of a cell in a binary-tree structure): the cell to which the second pointer
points.
root cell (of a tree structure): the cell to which the header points.
scanning a database (or a portion of a database): examining every record in that data-
base (or portion).
searching (a database): seeking either a target entry with a speciﬁc key or a target
entry whose key has some speciﬁed property.
secondary key: the key component of next highest precedence, when the key has more
than one component.
secondary operation (for a datatype): an operation constructed from primary oper-
ations and from previously deﬁned secondary operations.

GLOSSARY
1329
selection sort: a sort that transforms an unsorted list into a sorted list by iteratively
ﬁnding the item with smallest key from the remaining items in the unsorted input
list and appending it to the end of the sorted output list.
sequence of : an ADT-constructor that converts a datatype with domain D into a new
datatype, whose domain is the set of all ﬁnite sequences of objects from domain D,
and whose primary operations are some sequence operations.
set of : an ADT-constructor that converts a datatype with domain D into a new data-
type, whose domain is the set of all subsets of objects from domain D, and whose
primary operations are set operations.
shakersort: a bubblesort variation that alternates between bubbling upward and sink-
ing downward on alternate scans.
Shellsort: a sorting method that involves partitioning a list into sublists and insertion
sorting each of the sublists.
sinking sort: a “reverse bubblesort” that scans an array repeatedly from the lowest
index to the highest, each time swapping every out-of-order pair of consecutive items
that is encountered.
size (of a cell in a data structure): the number of bytes of computer memory that the
cell occupies.
size (of a hash table): the number of locations in the main array in which the records
are stored. (If chaining is used to resolve collision, the total number of records stored
may exceed the size of the main array.)
sorting algorithm: a method for arranging the entries of a database into a sequence
that conforms to the order of their keys.
sparse certiﬁcate: a strong certiﬁcate (for a property of a graph G) in which the
number of edges is O(|VG|).
sparse sequence: a sequence in which nearly all the entries are zeros.
stable certiﬁcate: a certiﬁcate produced by a stable function.
stable (certiﬁcate) function: a function A that maps graphs to strong certiﬁcates
such that: (1) A(G ∪H) = A(A(G) ∪H); and (2) A(G −e) diﬀers from A(G) by
O(1) edges, where e is an edge in G.
stack: an abstract datatype that organizes the records into a sequence, in which inser-
tion and extraction are made at the same end (called the top of the stack).
static structure (for a database): an information structure for the database whose
conﬁguration does not change during an algorithmic process.
strong certiﬁcate (for a property of a graph G): a certiﬁcate graph G′ for G with the
same vertex set as G such that, for every graph H, the graph G ∪H has property P
if and only if G′ ∪H has property P.
table: a set of keyed pairs, in which arbitrary pairs may be accessed directly; used as
the domain of a dictionary.
target (of a database search): an entry whose key has been designated as the objective
of the search.
2-3 tree: a tree in which each non-leaf node has 2 or 3 children, and in which every
path from the root to a leaf is of the same length.

1330
Chapter 18
INFORMATION STRUCTURES
tree structure: a concrete data structure such that the header points to a single cell,
and such that from that cell to each other cell, there is a single chain of pointers.
k-tuple of : an ADT-constructor that converts a list of k datatypes into a new data-
type, whose domain is the Cartesian product of the domains of the datatypes in that
list, and whose primary operations are the projection functions from a k-tuple to
each of its coordinates.
two-way incidence structure (for a graph): a pair consisting of an edge-incidence
table and an endpoint table.
two-way linked list: a set of cells, each with two pointers, one designated as its for-
ward pointer and the other as its backward pointer, plus a header with a forward
pointer and a backward pointer, such that these conditions hold: (1) considering
only the forward pointers, it is a one-way linked list; and (2) following the sequence
of backward pointers yields the reverse of the sequence obtained by following the
forward pointers.
two-way sequential list: an ADT-template whose domain is strings, in which an entry
is reached by applying the access operations forward and backward. Insertions are
made before or after the current location.
union-ﬁnd datatype: an abstract datatype whose records are mutually disjoint sets,
in which there is a primary operation to locate the set containing a speciﬁed target
element and a primary operation of merging two sets.
18.1
ABSTRACT DATATYPES
Organizing numbers and symbols into various kinds of records is a principal activity of
information engineering. The organizational structure of a record is called a datatype.
Abstractly, a datatype is characterized by a formal description of its domain and of the
intrinsic operations by which information is entered, modiﬁed, and retrieved. Providing
the speciﬁcation at this abstract level ensures that the datatype is independent of the
underlying types of information elements stored within the structure, and independent
also of the hardware and software used to implement this organization. See [AhHoUl83]
and [Kn97].
18.1.1
ABSTRACT SPECIFICATION OF RECORDS AND DATABASES
Information engineering uses discrete mathematics as a source of models for various
kinds of records and databases. The language of abstract mathematics is used to specify
a complex structure in terms of its elements. Constructors and templates are used to
create new kinds of data from old kinds.
Deﬁnitions:
A datatype consists of a set of objects, called the domain, and a set of mappings, called
primary operations, from the domain to itself or to the domain of some other datatype.
The domain of a datatype is its set of objects.

Section 18.1
ABSTRACT DATATYPES
1331
A primary operation for a datatype is a basic operation that retrieves information
from an object in the domain or modiﬁes the object.
A secondary operation on the domain of a datatype is an operation constructed from
primary operations and previously deﬁned secondary operations.
A query is a primary or secondary operation on a datatype domain that preserves the
values of all its arguments and returns a logical value, i.e., true or false.
An alphabetic datatype is a datatype whose domain is a ﬁnite set of symbols. Its only
primary operation is a total ordering query.
A numeric datatype is a datatype whose domain is a set of numbers and whose pri-
mary operations are a total ordering query and the arithmetic operators + (addition),
× (multiplication), and −(change of sign).
An elementary datatype is an alphabetic datatype or a numeric datatype, usually
intended for direct representation in the hardware of a computer.
An abstract datatype (ADT) is a mathematically speciﬁed datatype equipped with
operations that can be performed on its data objects.
An ADT-constructor is a template that converts a datatype into a new ADT.
The constructor sequence of transforms a datatype X-type with domain D into a new
datatype “sequence of X-type” whose domain is the set SeqD of all ﬁnite sequences of
elements of D. The primary operations of the resulting datatype are
• header(s), which yields a singleton sequence whose only element is the ﬁrst object
in the sequence s (or the empty sequence, if the sequence s is empty);
• trailer(s), which deletes the ﬁrst entry of sequence s (or yields the empty se-
quence, if the sequence s is empty);
• concat(s, t), which concatenates the two sequences;
• ﬁrst(s), which gives the value of the ﬁrst entry of a nonempty sequence s;
• append(s, d), which appends to sequence s ∈SeqD an entry d ∈D;
• nullseq( ), whose value is the null sequence λ.
The constructor set of converts a datatype X-type with domain D into a new datatype
“set of X-type” whose domain is the set of all subsets of D. The primary operations are
• inclusion(S, T ), a query whose value is true if S ⊆T ;
• union(S, T ), whose value is S ∪T ;
• intersection(S, T ), whose value is S ∩T ;
• diﬀerence(S, T ), whose value is S −T ;
• choose(S), whose value is an arbitrary element of a nonempty subset S;
• singleton(d), which transforms an element of D into the singleton set whose only
entry is d;
• emptyset( ), whose value is the empty set ∅;
• universe( ), whose value is the underlying domain D.
The constructor k-tuple of converts a list of k datatypes
X1-type, X2-type, . . . , Xk-type

1332
Chapter 18
INFORMATION STRUCTURES
into a new datatype “k-tuple (X1, . . . , Xk)” whose domain is the Cartesian product
D1 × D2 × · · · × Dk of the domains of the respective datatypes in that list. The primary
operations are the projection functions
• coordj(s), which gives the value of the jth coordinate of the k-tuple s;
• entuple (d1, . . . , dk), whose value is the k-tuple whose jth coordinate is the
element dj of domain Dj.
An elementary ADT-constructor is any of the three operations sequence of, set of,
or tuple of used to build more complex ADTs from simpler ADTs.
The Iverson truth function assigns to a proposition p the integer value (p) such that
(p) =
(
1
if p is true;
0
otherwise.
A datatype speciﬁcation uses a combination of elementary datatypes and ADT-
constructors to specify the domain and the primary operations. It may also use the
following mathematical notation:
• ∅denotes the empty set;
• λ denotes the empty sequence;
• · denotes the operation of appending one element to a sequence;
• ◦denotes the sequence concatenation operation.
Moreover, every primary operation is either a query or a procedure.
Specifying a datatype as generic in an ADT-template means that any datatype can be
used in that part of the template as a block in the construction of the new datatype.
Specifying a datatype as ordered in an ADT-template means that any ordered datatype
can be used in that part of the template as a block in the construction of the new datatype.
The precondition of a primary operation is a list of conditions that must hold imme-
diately before the operation is executed, for it to execute as described.
The postcondition of a primary operation is a speciﬁcation of conditions that must
hold after the operation is executed, if the precondition is satisﬁed when the operation
commences.
The ﬂat notation X♭in a postcondition of a primary operation speciﬁcation means the
value of the variable X before that operation is executed. Unadorned X (without the ♭)
means the value of X after the operation.
Facts:
1. The domain of an ADT is speciﬁed as a mathematical model, without saying how its
elements are to be represented.
2. Sometimes the domain of a datatype is speciﬁed by roster. Other times it is speciﬁed
with the use of set-theoretic operations.
3. A primary operation of an ADT is speciﬁed functionally. That is, its value on every
element of the domain is declared, but the choice of an algorithm to be used in its
implementation is omitted.
4. A primary operation may be implemented so that it has direct access to the data
representing the value of the computational variable to which it is applied.

Section 18.1
ABSTRACT DATATYPES
1333
5. A primary operation can modify the information within a variable in its datatype or
retrieve information from a variable.
6. A secondary function is implemented through calls to the primary operations from
which it is ultimately composed.
7. There is no set of standard conventions for writing ADTs.
8. Software designers frequently specify a particular concrete information structure (see
§18.2), instead of writing an ADT.
9. The advantage of writing an ADT, rather than a concrete datatype, is that it leaves
the implementer room to ﬁnd a new (and possibly improved) way to meet the require-
ments of the task.
10. In a datatype speciﬁcation, a functional subprogram is represented by a procedure
that produces a non-boolean value, and a variable to receive that value is speciﬁed as its
last parameter.
Examples:
1. Elementary numeric datatypes include the integers and the reals.
2. Elementary alphabetic datatypes include the ASCII set and the decimal digits.
3. Complex number is a datatype that represents complex numbers and their addition
and multiplication.
ADT complex number:
Domain
2-tuple (re: real, im: real)
Primary Operations
sum (w: complex number, z: complex number)
Comment: add two complex numbers.
{pre: none}
{post: sum(w, z) = entuple
 re(w) + re(z), im(w) + im(z)

}
prod (w: complex number, z: complex number)
Comment: multiply two complex numbers.
{pre: none}
{post: prod(w, z) = entuple
 re(w) · re(z) −im(w) · im(z), re(w) · im(z) + im(w) · re(z)

}
4. Baseten digit is a datatype that might be used in the construction of base-ten nu-
merals representing arbitrarily large integers and their addition.
ADT baseten digit:
Domain
{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}: integers
Primary Operations
add digits (x: baseten digit, y: baseten digit)
{pre: none}
{post: add digits(x, y) = x + y mod 10}
addcarry: (x: baseten digit, y: baseten digit)
{pre: none}
{post: addcarry(x, y) = (x + y ≥10)}

1334
Chapter 18
INFORMATION STRUCTURES
5. The datatype alphastring represents sequences of lowercase English letters.
ADT alphastring:
Domain
sequence of {a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z}
Primary Operations
none except from the constructor sequence of
6. The union-ﬁnd constructor transforms a datatype on domain set S into a datatype
whose objects are of three kinds: elements of S, subsets of S, and partitions of S. There
is a primary operation to merge two cells of a partition, and a primary operation to
locate the cell of a partition that contains a speciﬁed target element.
18.1.2
STACKS AND QUEUES
Access to entries in the interior of a list is unnecessary much of the time. Restricting
access to the ﬁrst and last entries is a precaution to prevent mistakes.
Deﬁnitions:
A stack is an ADT whose domain is a sequence, one end of which is called the top, and
the other the bottom. One primary operation, called pushing, appends a new entry to
the top; the other, called popping, removes the entry at the top and returns its value.
No entry may be examined, added to the stack, or deleted from the stack except by an
iterated composition of these operations.
The top of a stack is the end of that stack that can be accessed directly.
Pushing an entry onto a stack means appending it to the top of the stack.
Popping an entry from a stack means deleting it from the top of the stack and
possibly examining the data it contains.
The LIFO property of a database is that the item retrieved is always the item most
recently inserted. LIFO means “last-in-ﬁrst-out”.
A queue is an ADT whose domain is a sequence, one end of which is called the front,
and the other the back. One primary operation, called enqueueing, appends a new entry
to the back; the other, called dequeueing, removes the entry at the front and returns its
value. No entry may be examined, added to the queue, or deleted from the queue except
by an iterated composition of these operations.
The back of a queue is the end to which entries may be appended.
The front of a queue is the end from which entries may be deleted and possibly
examined.
Enqueueing an entry into a queue means appending it to the back of the queue.
Dequeueing an entry from a queue means deleting it from the front of the queue,
and possibly examining the data it contains.
The FIFO property of a database is that the item retrieved is always the item inserted
the longest ago. FIFO means “ﬁrst-in-ﬁrst-out”.

Section 18.1
ABSTRACT DATATYPES
1335
Facts:
1. Abstract speciﬁcation of stacks and queues mentions only the behavior of those
datatypes and totally avoids all details of implementation. This permits a skillful imple-
menter to innovate with eﬃcient concrete structures (see §18.2) that meet the behavioral
speciﬁcation.
2. Abstract speciﬁcation of stacks and queues is consistent with the principles of object-
oriented programming, in which details of implementation are hidden inside the data
objects, so that the rest of the program perceives only the speciﬁed functional behavior.
3. All stacks have the LIFO property. For any stack S and for any element b, after
executing the sequence of instructions
push(S, b), pop(S, x)
the resulting value of the stack S is whatever it was before the operations.
4. A stack is most commonly implemented as a linked list (see §18.2.2).
5. After changing the value of the variable y to that of the top entry on the stack S,
the sequence of instructions
pop(S, x), y := x, push(x)
restores S to its previous state.
6. All queues have the FIFO property. Given an empty queue Q and two elements b1
and b2, the sequence of operations
enqueue(Q, b1), enqueue(Q, b2), dequeue(Q, x1), dequeue(Q, x2),
yields x1 = b1, x2 = b2, and Q = λ.
7. A queue is most commonly implemented as a linked list (see §18.2.2).
Examples:
1. The following pseudocode speciﬁes the ADT stack of D, where D is an arbitrary
datatype.
Domain
sequence of D: generic
Primary Operations
create stack (S: stack)
Comment: Initialize variable S as an empty stack.
{pre: none}
{post: S = λ}
push (S: stack, x: element of D)
Comment: Put value of x at top of stack S
{pre: none}
{post: S = x · S♭}
pop (S: stack, x: element of D)
Comment: Remove top item of stack S; return it as value of variable x.
{pre: S ̸= λ}
{post: x · S = S♭}
query empty stack (S: stack)
Comment: Decide whether stack S is empty.
{pre: none}
{post: query empty stack = (S = λ)}

1336
Chapter 18
INFORMATION STRUCTURES
2. The following pseudocode speciﬁes the ADT queue of D, where D is an arbitrary
datatype.
Domain
sequence of D: generic
Primary Operations
create queue (Q: queue)
Comment: Initialize Q as an empty queue.
{pre: none}
{post: Q = λ }
enqueue (Q: queue, x: element of D)
Comment: Put x at the back of queue Q.
{pre: none}
{post: Q = Q♭· x}
dequeue (Q: queue, x: element of D)
Comment: Delete front of Q; return as x.
{pre: Q ̸= λ}
{post: Q♭= x · Q}
query empty queue ( Q: queue)
Comment: Decide whether queue Q is empty.
{pre: none}
{post: query empty queue = (Q = λ)}
3. The following ﬁgure illustrates the diﬀerence between stacking (last-in-ﬁrst-out) and
queueing (ﬁrst-in-ﬁrst out).
empty  stack
push  A
A
B
B
C
A
A
B
A
A
A
D
push  B
push  C
pop
C
push  D
pop
B
empty  queue
enqueue  A
A
A
B
A
B
C
B
C
C
D
C
enqueue  B
enqueue C
dequeue
dequeue
A
enqueue  D
B
18.1.3
TWO-WAY SEQUENTIAL LISTS
A two-way sequential list conceptualizes a linear list as having a current location, so that
entries may be inserted or deleted only at the current location.
Deﬁnitions:
A two-way sequential list is a list with a designated location at which access is per-
mitted.

Section 18.1
ABSTRACT DATATYPES
1337
The current location of a two-way sequential list is the location at which access is
permitted.
The forepart of a two-way sequential list is the part preceding the current location,
which is empty when the current location is at the start of the list.
The aftpart of a two-way sequential list is the part following the current location, which
is empty when the current location is at the ﬁnish of the list.
Facts:
1. A two-way sequential list does not maintain place-in-list numbers for the entries. The
result of such an additional requirement would force the insert operation to renumber
the part of the list following a newly inserted entry. This would slow the performance.
2. A two-way sequential list is easily implemented as a pair of stacks.
Example:
1. The following pseudocode speciﬁes the ADT seq list of D.
Domain
2-tuple (fore: sequence of D, aft: sequence of D)
type D: generic
Primary Operations
create list (L: seq list)
Comment: Initialize an empty list L.
{pre: none}
{post: fore(L) = λ ∧aft(L) = λ}
reset to start (L: seq list)
Comment: Reset to start of list.
{pre: none}
{post: fore(L) = λ ∧aft(L) = fore(L♭) ◦aft(L♭)}
advance (L: seq list)
Comment: Advance current position by one element.
{pre: aft(L) ̸= λ}
{post: (∃x : D)[fore(L) = fore(L♭) · x ∧aft(L♭) = x · aft(L)]}
query atstart (L: seq list)
{pre: none}
{post: query atstart = (fore(L) = λ)}
query atﬁnish (L: seq list)
{pre: none}
{post: query atfinish = (aft(L) = λ)}
insert (L: seq list, x: element of D)
{pre: none}
{post: aft(L) = x · aft(L♭) ∧fore(L) = fore(L♭)}
remove (L: seq list, x: element of D)
{pre: aft(L) ̸= λ}
{post: aft(L♭) = x · aft(L) ∧fore(L) = fore(L♭)}
swap right (L: seq list, M: seq list)
{pre: none}
{post: fore(L) = fore(L♭) ∧fore(M) = fore(M ♭)
∧aft(L) = aft(M ♭) ∧aft(M) = aft(L♭)}

1338
Chapter 18
INFORMATION STRUCTURES
18.1.4
DICTIONARIES AND RANDOM ACCESS LISTS
Deﬁnitions:
A keyed pair is a 2-tuple whose ﬁrst entry, which is called a key, is from an ordered
datatype and is used to access data in the second entry.
A table is a set of keyed pairs such that no two keys are identical.
A random access list is a table whose keys are consecutive integers.
A dictionary is another name for a table.
Facts:
1. A static table (whose size n does not change) can be implemented as an array.
2. A dynamic table (which permits inserts and deletes) is often implemented as a binary
search tree (see §18.2.3).
3. Specifying a datatype as a dictionary means that its primary retrieval operation can
execute in Θ(n) time.
4. Specifying a datatype as a random access list means that its primary retrieval oper-
ation can execute in Θ(1) time.
Examples:
1. The following pseudocode speciﬁes the ADT table.
Domain
set of table entry
type table entry: 2-tuple (key: ordered, data: generic)
Primary Operations
create table (T : table)
{pre: none}
{post: T = λ}
insert entry (T : table, e: table entry)
{pre: (∀e′ ∈T )[ key(e′) ̸= key(e)]}
{post: T = T ♭∪{e}}
remove entry (T : table, e: table entry)
{pre: e ∈T }
{post: T = T ♭−{e}}
ﬁnd entry (T : table, k: key, found: boolean, e: table entry)
{pre: none}
{post:
 (∃e′ ∈T )[e′.key = k]) ∧(found = true) ∧(e = e′)

∨
 ¬(∃e′ ∈T )[e′.key = k] ∧found = false

}
2. The following pseudocode speciﬁes the ADT Random access list.
Domain
set of table entry
type table entry: 2-tuple (key: subrange of integers, data: generic)
Primary Operations
Exactly the same as for the ADT table.

Section 18.2
CONCRETE DATA STRUCTURES
1339
18.1.5
PRIORITY QUEUES
A priority queue is an “unfair queue”, in which entries are not dequeued on a ﬁrst-
enqueued basis. Instead, each entry has a priority, and is dequeued on a highest priority
basis.
Deﬁnition:
A priority queue is a set of keyed pairs, such that the key of the entry returned by a
dequeue operation is not exceeded by the key of any other entry currently in the queue.
Facts:
1. A priority queue is usually implemented as a heap (see §18.2.4).
2. Two diﬀerent entries in a priority queue may have the same key.
3. The operating system for a multi-user programming environment places computa-
tional tasks into a priority queue.
Example:
1. The following pseudocode speciﬁes the ADT P queue.
Domain
set of Pq entry
type Pq entry: 2-tuple (key: ordered, data: generic)
Primary Operations
create Pq (PQ: P queue)
{ pre: none }
{ post: PQ = λ }
enPqueue(PQ: P queue, e: Pq entry)
{ pre: none }
{ post: PQ = PQ♭∪{e} }
dePqueue (PQ: P queue, e: Pq entry)
{ pre: PQ ̸= ∅}
{ post: (e ∈PQ♭) ∧(∀e′ ∈PQ)[key(e) ≤key(e′)] ∧PQ = PQ♭−{e} }
query empty Pqueue(PQ: P queue)
{ pre: none }
{ post: query empty Pqueue = (PQ = λ) }
18.2
CONCRETE DATA STRUCTURES
Concrete data structures conﬁgure computer memory into containers of related informa-
tion. They are used to implement abstract datatypes. Contiguous stretches of memory
are regarded as arrays, and noncontiguous portions are linked with pointers.

1340
Chapter 18
INFORMATION STRUCTURES
18.2.1
MODELING COMPUTER STORAGE AND RETRIEVAL
There are a few generic concepts common to nearly all concrete data structures.
Deﬁnitions:
A concrete data structure is a mathematical model for storing the current value of a
structured variable in computer memory.
A cell in a concrete data structure S is a unit within the data structure that may contain
data and pointers to other cells.
The header of a concrete data structure is a special unit that contains current informa-
tion about the entire conﬁguration and pointers to some critical cells in the structure. It
is not a cell.
An insert operation insert(S : structure, c: cell, loc: location) inserts a new cell c into
structure S at location loc.
A delete operation delete(S : structure, loc: location) deletes from a structure S the
cell at location loc.
A target predicate for a concrete data structure is a predicate that applies to the cells.
A ﬁnd operation ﬁnd(S : structure, t: target, loc: location) searches a structure S for
a cell that satisﬁes target predicate t. It returns false if there is no such cell. In addition
to returning the boolean value true if there is such a cell, it also assigns to its location
parameter loc the location of such a cell.
A next operation next(S : structure, loc: location) returns the boolean value true if
the structure S is nonempty, in which case it also assigns to its location parameter loc
the location of whatever cell it regards as next; it returns false if S is empty.
The size of a cell is the number of bytes of computer memory it occupies.
A pointer to a cell is a representation of its location in computer memory.
A null pointer is a pointer that points to an artiﬁcial location. Detecting a null pointer
is a signal to an algorithm to react somewhat diﬀerently than to a pointer to an actual
location.
Facts:
1. There may be several alternative suitable concrete data structures that can be used
to implement a given abstract datatype.
2. If the records of a database are all of the same ﬁxed size, then the records themselves
may be in the cells of a concrete data structure.
3. If the size of records is variable, then the cells of the concrete data structure often
contain pointers to the actual data, rather than the data itself.
This permits faster
execution of operations.
4. The most common form of target predicate for a concrete data structure is an asser-
tion that a key component of the cell matches some designated value.

Section 18.2
CONCRETE DATA STRUCTURES
1341
18.2.2
ARRAYS AND LINKED LISTS
Deﬁnitions:
An array is an indexed sequence of identically structured cells ⟨aj | j = d, . . . , u⟩, with
consecutive indices.
An array is zero-based if its lowest index is zero.
A one-way linked list is a set of cells, each with one pointer, such that
• exactly one of these cells is pointed to by the header but by no cell;
• exactly one cell contains a null pointer;
• the sequence of cells formed by following the pointers, starting from the header,
traverses the entire set, ending with the cell containing the null pointer.
The far end of a one-way linked list is the cell that contains a null pointer.
The near end of a one-way linked list is the cell that is pointed to by the header
and by no other cell.
A two-way linked list is a set of cells, each with two pointers, one designated as its
forward pointer and the other as its backward pointer, plus a header with a forward
pointer and a backward pointer, such that
• considering only the forward pointers, it is a one-way linked list;
• following the sequence of backward pointers yields the reverse of the sequence
obtained by following the forward pointers.
A sparse sequence is a sequence in which nearly all the entries are zeros.
A circular linked list is a set of cells, each with two pointers, one designated as its
forward pointer and the other as its backward pointer, plus a header with one or more
pointers to current cells, such that
• the sequence of cells formed by following the forward pointers, starting from any
cell, traverses the entire set and returns to the starting cell;
• the sequence of cells formed by following the backward pointers, starting from
any cell, traverses the entire set and returns to the starting cell.
Facts:
1. A random access list (see §18.1.4) can be implemented as an array so that a find
operation executes in O(1) time.
2. A stack (see §18.1.2) can be implemented as a one-way linked list with its top at the
near end, so that push and pop both execute in O(1) time.
3. A queue (see §18.1.2) can be implemented as a two-way linked list with its back at
the near end of the forward list and its front at the far end, so that enqueue and dequeue
both execute in O(1) time.
4. A two-way sequential list (see §18.1.3) can be implemented as a two-way linked list,
or as a pair of one-way linked lists.
Examples:
1. The following ﬁgure illustrates an array with cells ad, . . . , au.

1342
Chapter 18
INFORMATION STRUCTURES
root A
ad
ad+1
au-1
au
2. The following ﬁgure illustrates a one-way linked list, with cell ad at the near end and
cell au at the far end.
root A
ad
ad+1
au-1
au
3. The following ﬁgure illustrates a two-way linked list.
root A
ad
ad+1
au-1
au
4. Representing a sparse ﬁnite sequence by a linked list can save space. The cell given
to each nonzero entry includes its position in the sequence and points to the cell with
the next nonzero entry.
5. A queue whose maximum length is bounded can be represented by a circular linked
list with two header pointers, one to the back and one to the front of the queue. The
number of cells equals the maximum queue length. This eliminates the need for “garbage
collection”.
18.2.3
BINARY SEARCH TREES
Deﬁnitions:
A tree structure on n cells is a concrete data structure such that the header points to
a single cell, and such that from that cell to each other cell, there is a single chain of
pointers.
The root cell of a tree structure is the cell to which the header points.
A binary tree structure is a tree structure such that each cell has two pointers.
The left child of a cell in a binary tree structure is the cell to which the ﬁrst pointer
points.
The right child of a cell in a binary tree structure is the cell to which the second pointer
points.
A binary search tree structure is a binary tree structure in which for every cell, all
cells accessible through the left child have lower keys, and all cells accessible through the
right child have higher keys.
Facts:
1. The ADT table is commonly implemented as a binary search tree structure.

Section 18.2
CONCRETE DATA STRUCTURES
1343
2. The average running time for the ADT table operations of insertion, deletion, and
ﬁnd is O(log n). The time may be worse if relatively few cells have two children.
3. Using a 2-3 tree structure instead of a binary search tree structure for the ADT table
operations reduces the worst-case running time from O(n) to O(log n).
4. Algorithm 1 can be used in the binary search tree operations of ﬁnding, inserting,
and deleting.
Algorithm 1:
BSTsearch(T, t).
input: a binary-search tree T and a target key t
output: if t ∈T , the address of the node with key t, else the address where t
could be inserted
if root(T ) = NULL then return address of root
else if t < key(root) then BSTsearch(leftsubtree(T ), t)
else if t = key(root) then return address of root
else BSTsearch(rightsubtree(T ), t)
5. To ﬁnd a target key t in the binary search tree T , ﬁrst apply BSTsearch(T, t). If the
address of a null pointer is returned, then there is no node with key t. Otherwise, the
address returned is a node with key t.
6. To insert a node with target key t into the binary search tree T , ﬁrst apply the
algorithm BSTsearch(T, t). Then install the node at the location returned.
7. To delete node t from the binary search tree T , ﬁrst apply BSTsearch(T, t). Then
replace node t either by the node with the largest key in the left subtree or by the node
with the smallest key in the right subtree.
Examples:
1. The following ﬁgure illustrates a binary search tree.
10
25
36
45
40
27
29
19
15
12
7
3
2. Inserting 32 into the BST of Example 1 yields the following BST.
10
25
36
45
40
27
32
29
19
15
12
7
3
3. Deleting node 10 from the BST of Example 1 would yield one of the following two
BSTs.

1344
Chapter 18
INFORMATION STRUCTURES
7
25
36
45
40
27
29
19
15
12
3
12
25
36
45
40
27
29
19
15
7
3
18.2.4
PRIORITY TREES AND HEAPS
Deﬁnitions:
A binary tree is left-complete if it is complete or if it is a balanced binary tree (§9.1.2)
such that at depth one less than the maximum, the following conditions hold:
• all nodes with two children are to the left of all nodes with one or no children;
• all nodes with no children are to the right of all nodes with one or two children;
• there is at most one node with only one child, which must be a left-child.
A priority tree is a left-complete binary tree, with the following additional structure:
• each node has an attribute called a key;
• the values of the keys are drawn from a partially ordered set;
• no node has a higher priority key than its parent.
A heap is a representation of a priority tree as a zero-based array, such that each node
is represented at the location in the array whose index equals its location in the breadth-
ﬁrst-search order of the tree. Thus
• index(root) = 0;
• index(leftchild(v)) = 2 × index(v) + 1;
• index(rightchild(v)) = 2 × index(v) + 2;
• index(parent(v)) =
 index(v)−1
2

.
Enheaping an entry into a heap means placing it into a correctly prioritized position.
Trickle-up means enheaping by Algorithm 2.
Algorithm 2:
PriorityTreeEnqueue (T, x).
input: a priority tree T and a new entry x
output: tree T with the new node x inserted so that it remains a priority tree
install entry x into the ﬁrst vacant spot in the left-complete tree T
while x ̸= root(T ) and priority(x) > priority(parent(x))
swap x with parent(x)
Deheaping an entry from a heap means taking the root as the deheaped entry and
patching its left subtree and its right subtree back into a single tree.

Section 18.2
CONCRETE DATA STRUCTURES
1345
Trickle-down means deheaping by Algorithm 3.
Algorithm 3:
PriorityTreeDequeue (T ).
input: a priority tree T
output: tree T −root(T ) with priority-tree shape restored
replace root(T ) by rightmost entry y at bottom level of T
while y is not a leaf and [priority(y) ≤priority(leftchild(y)) or
priority(y) ≤priority(rightchild(y))]
if priority(leftchild(y)) > priority(rightchild(y))
then swap y with leftchild(y)
else swap y with rightchild(y)
A Fibonacci heap is a modiﬁcation of a heap, using the Fibonacci sequence, that
permits more eﬃcient implementation of a priority queue than a heap based on a left-
complete binary tree.
Facts:
1. For a tree with n nodes, the worst-case execution time of the priority tree enqueueing
algorithm (Algorithm 2) is in the class Θ(log n).
2. For a tree with n nodes, the worst-case execution time of the priority tree dequeueing
algorithm (Algorithm 3) is in the class Θ(log n).
Examples:
1. The following ﬁgure shows a left-complete binary tree.
2. The following ﬁgure shows a priority tree of height 3.
42
28
14
4
32
36
10
16
47
6
3. The following ﬁgure illustrates a priority tree insertion. It shows how 45 is inserted
into the priority tree of Example 2 in the correct location to maintain the left-compete
binary tree shape and then rises until the priority property is restored.

1346
Chapter 18
INFORMATION STRUCTURES
42
28
14
4
32
45
36
10
16
47
6
42
28
14
4
32
36
45
10
16
47
6
45
28
14
4
32
36
42
10
16
47
6
4. The following ﬁgure illustrates a priority tree deletion. It shows how the left-complete
binary tree shape and priority property are restored after the root is removed from the
priority tree of Example 2.
42
28
14
4
36
10
16
32
6
32
28
14
4
36
10
16
42
6
36
28
14
4
32
10
16
42
6
5. The following heap corresponds to the priority tree of Example 2. Observe that the
keys occur in the array according to the breadth-ﬁrst-search order of their vertices.
42
28
14
4
32
36
10
16
47
6
index
0
1
2
3
4
5
6
7
8
9
key
47
42
16
28
36
6
10
4
14
32
18.2.5
NETWORK INCIDENCE STRUCTURES
Deﬁnitions:
An incidence matrix for a graph is a 0-1 matrix that speciﬁes the incidence relation.
The rows are indexed by the vertices and the columns by the edges. The entry in the
row corresponding to vertex v and edge e is 1 if v is an endpoint of e, and 0 otherwise.
An endpoint table for a graph (§8.1.1) is a dictionary whose keys are the edges. The
data component for each key edge is the set of endpoints for that edge. If an edge is
directed, then its endpoints are marked as head and tail.
An edge-incidence table is a dictionary whose keys are the vertices of a graph or
digraph.
The data component for each key vertex is a list of all the edges that are
incident on that vertex. Each self-loop occurs twice in the list.
A two-way incidence structure for a graph is a pair consisting of an edge-incidence
table and an endpoint table.

Section 18.3
SORTING AND SEARCHING
1347
Facts:
1. The time required to insert a new vertex v into a two-way incidence structure for a
graph with n vertices and m edges is in Θ(log n). By way of contrast, the time for an
incidence matrix is in Θ(n · m).
2. The time required to delete a vertex v from a two-way incidence structure for a graph
with n vertices and m edges is in Θ(log n + deg(v)). By way of contrast, the time for an
incidence matrix is in Θ(n · m).
3. The time required to insert a new edge e into a two-way incidence structure for a
graph with n vertices and m edges is in Θ(log m). By way of contrast, the time for an
incidence matrix is in Θ(m · n).
4. An edge-incidence table can represent an imbedding of a graph on a surface as a
rotation system (§8.8.3).
Example:
1. The following graph corresponds to the network incidence structure given below.
a
u
w
y
x
d
e
b
v
c
g
f
h
EDGE-INCIDENCE TABLE
u.
a
b
d
v.
b
c
e
f
w.
a
c
g
x.
d
e
h
y.
f
g
h
ENDPOINT TABLE
a.
u
w
b.
u
v
c.
v
w
d.
u
x
e.
v
x
f.
v
y
g.
w
y
h.
x
y
18.3
SORTING AND SEARCHING
Since commercial data processing involves frequent sorting and searching of large quanti-
ties of data, eﬃcient sorting and searching algorithms are of great practical importance.
Sorting and searching strategies are also of fundamental theoretical importance, since
sorting and searching steps occur in many algorithms. The following table compares the
performance of some of the most common methods for sorting n items.

1348
Chapter 18
INFORMATION STRUCTURES
sorting method
average time factors
comments
expanding a sorted subsequence
selection sort
≈n2
2 comparisons
≈n exchanges
insertion sort
≈n2
4 comparisons
linear if input ﬁle is
≈n2
8 exchanges
“almost sorted”
Shellsort
< n3/2 comparisons
for “good” increments
1, 4, 13, 40, 131, . . .
exchanging out-of-order pairs
bubblesort
≈n2
2 comparisons
one pass if input ﬁle
≈n2
4 exchanges
is already sorted
sinking sort
≈n2
2 comparisons
one pass if input ﬁle
≈n2
4 exchanges
is already sorted
shakersort
≈n2
2 comparisons
≈n2
4 exchanges
heapsort
< 2n log2 n comparisons
always Θ(n log n)
divide-and-conquer
mergesort
≈n log2 n comparisons
always Θ(n log n)
quicksort
≈2n log2 n comparisons
worst-case n2
2
sorting by distribution
rank-counting
Θ(n)
radix sort on k-digit key
≈n log2 n comparisons
18.3.1
GENERIC CONCEPTS FOR SORTING AND SEARCHING
Deﬁnitions:
A database is a set of entries, stored in a computer as an information structure.
An entry in a database is a 2-tuple whose ﬁrst component is a key and whose second
component is some data.
A key in a database entry is a value from an ordered set, used to store and retrieve data.
The key domain is the ordered set from which keys are drawn.
The primary key is the component of highest precedence, when the key for database
records has more than one component.
The secondary key is the component of next highest precedence after the primary key.
A record is another name for a database entry.
Sorting is the process of arranging a collection of database entries into a sequence that
conforms to the order of their keys.
Searching a database means using a systematic procedure to ﬁnd an entry with a key
designated as the objective of the search.

Section 18.3
SORTING AND SEARCHING
1349
Scanning a database (or a portion of a database) means examining every record in
that database (or portion).
The target of a database search is an entry whose key has been designated as the
objective of the search.
A comparison sort is any sorting method that uses only comparisons of keys.
An internal sorting method keeps all the entries in the primary memory of the com-
puter during the process of rearrangement.
An external sorting method uses external storage outside the main memory during
the sorting process.
An in-place realization of a sorting method uses, beyond the space needed for one
copy of each data entry, only a constant amount of additional space, regardless of the
size of the list to be sorted.
A dynamic structure for a database is an information structure whose conﬁguration
may change during an algorithmic process, for instance, by the insertion or deletion of
elements.
A static structure for a database is a data structure whose conﬁguration does not
change during an algorithmic process.
Facts:
1. Several diﬀerent general strategies for sorting are given in the following subsections.
Each leads to more than one method for sorting.
2. Some elementary sorting methods take O(n2) time. Most practical comparison sort-
ing methods require O(n log n) time.
3. The worst-case running time of any comparison sort is at least Ω(n log n).
Examples:
1. The following are all internal comparison sorts: selection sort (§18.3.2), insertion
sort (§18.3.2), Shellsort (§18.3.2), bubblesort (§18.3.3), heapsort (§18.3.3), and quicksort
(§18.3.4).
2. Mergesort (§18.3.4) is a comparison sort that may be either internal or external.
3. Database model for a telephone directory:
Each entry has as key the name of a
person and as data that person’s telephone number. The target of a search is the entry
for a person whose number one wishes to call. Names of persons form an ordered key
domain under a modiﬁed lexicographic (“alphabetic”) ordering, in which it is understood
that a family name (a “last name” in European-based cultures) has higher precedence
than a given name.
4. Database model for a reverse telephone directory:
In a reverse telephone directory
entry, the key is a telephone number and the data is the name of the person with that
number. This permits the telephone company to retrieve the name of the person who
has a particular phone number, for instance, if someone inquires why some particular
telephone number occurs on a long-distance phone bill.
5. Database model for credit card information:
In a credit card database, the key to
each entry is a credit card number, and the data includes the name of the cardholder,
the maximum credit limit, and the present balance.

1350
Chapter 18
INFORMATION STRUCTURES
18.3.2
SORTING BY EXPANDING A SORTED SUBSEQUENCE
One general strategy for sorting is to iteratively expand a sorted subsequence, most often
implemented as an array or a linked list, until the expanded subsequence ultimately
contains all the entries of the database.
Deﬁnitions:
A selection sort iteratively transforms an unsorted input sequence into a sorted output
sequence. At each iteration, it selects the item with smallest key from the remaining
items in the unsorted input sequence and appends that item at the end of the sorted
output sequence.
An insertion sort iteratively transforms an unsorted input sequence into a sorted output
sequence. At each iteration, it takes the ﬁrst remaining item from the unsorted input
subsequence and inserts it into its proper position in the sorted output sequence.
A Shellsort of an unsorted sequence a1, . . . , an is based on a list of increments of de-
creasing size: h1 > h2 > · · · > ht = 1. On the kth iteration, the sequence is partitioned
into hk subsequences, such that for j = 1, . . . , hk, the jth subsequence is

aj+rhk | 0 ≤r ≤n −j
hk

and each of these j subsequences is sorted by an insertion sort.
Facts:
1. Selection sorts and insertion sorts both have time-complexity O(n2) in the worst case.
2. The time-complexity of a selection sort is independent of the order of the input
sequence, since ﬁnding the smallest remaining item requires scanning all the remaining
items.
3. The running time of an insertion sort can be signiﬁcantly reduced for “almost sorted”
sequences, with time O(n) as the limiting case.
4. Optimizing the running time of a Shellsort involves some very diﬃcult mathematical
problems, many of which have not yet been solved. In particular, it is not known which
choice of increments yields the best result.
5. It is known that Shellsort increments should not be multiples of each other, if the
objective is to achieve fast execution.
6. Evidence supporting the eﬃciency of the Shellsort increment list . . . , 40, 13, 4, 1 ac-
cording to the rule hi−1 = 3hi + 1 is given by Knuth [Kn98].
The increment list
. . . , 15, 7, 3, 1 satisfying the rule hi−1 = 2hi + 1 is also recommended.
7. Shellsort is a reﬁnement of a straight insertion sort. The motivation for its design in
1959 by D. L. Shell is based on the observation that an insertion sort works very fast for
“almost sorted” sequences.
8. Shellsort is guaranteed to produce a sorted list, because on the last pass, it applies
an insertion sort to the whole sequence.
9. An in-place realization of the strategy of expanding a sorted subsequence conceptually
partitions the array into a sorted subsequence at the front of the array A[1..n] and an
unsorted subsequence of remaining items at the back. Initially, the sorted subsequence is
the empty sequence and the unsorted subsequence is the whole list. At each step of the
iteration, the sorted front part expands by one item and the unsorted back part contracts
by one item.

Section 18.3
SORTING AND SEARCHING
1351
Algorithms:
1. Algorithm 1 is an in-place realization of a selection sort.
Algorithm 1:
SelectionSort of array A[1..n].
for i := 1 to n −1
minindex := i; minkey := A[i]
for j := i + 1 to n
if A[j] < minkey then
minindex := j; minkey := A[j]
swap A[i] with A[minindex]
2. Algorithm 2 is an in-place realization of an insertion sort.
Algorithm 2:
InsertionSort of array A[1..n].
for i := 2 to n
nextkey := A[i]; j := i −1
while j > 0 and A[j] > nextkey
A[j + 1] := A[j]; j := j −1
A[j + 1] := nextkey
Examples:
In the following examples of single-list implementations of SelectionSort and Insertion-
Sort, the symbol “
| ” separates the sorted subsequence at the front from the remaining
unsorted subsequence at the back. The arrows “←” and “→” indicate how far the index j
moves during an iteration.
1. On the sequence 15, 8, 10, 6, 13, 17, SelectionSort would progress as follows:
minkey
minkey
minkey
15
8
10 b6
13
17
6 | b8
10
15
13
17
6
8 | c
10
15
13
17
i
j −−−−−−−−−−→
i
j −−−−−−−−→
i
j −−−−−→
minkey
minkey
6
8
10 | 15
c
13
17
6
8
10
13 | c
15
17
6
8
10
13
15
17 |
i
j −→
i
j
2. On the sequence 15, 8, 10, 6, 13, 17, InsertionSort would progress as follows:
shift
shift
shift shift shift
15⃗| 8
10
6
13
17
8
15⃗| 10
6
13
17
8⃗10⃗15⃗| 6
13
17
←j
i
←j
i
←−−−−−j
i
shift
6
8
10
15⃗| 13
17
6
8
10
13
15 | 17
6
8
10
13
15
17 |
←j
i
j
i

1352
Chapter 18
INFORMATION STRUCTURES
3. If n = 13 and h3 = 4, then on the third iteration, ShellSort would insertion sort the
following subsequences:
a1, a5, a9, a13
a2, a6, a10
a3, a7, a11
a4, a8, a12
18.3.3
SORTING BY EXCHANGING OUT-OF-ORDER PAIRS
A standard measure of the totality of disorder of a sequence of n items is the number
of pairs (ai, aj) such that i < j but ai > aj. Thus, the disorder ranges from 0 (i.e.,
totally ordered) to
 n
2

(i.e., in reverse order). The strategy of exchange sorts is to swap
out-of-order pairs until all pairs are in order.
Deﬁnitions:
A bubblesort scans an array A[1..n] repeatedly from the highest index to lower indices,
each time swapping every out-of-order pair of consecutive items that is encountered.
A sinking sort scans an array A[1..n] repeatedly from the lowest index to higher indices,
each time swapping every out-of-order pair of consecutive items that is encountered.
A shakersort scans an array A[1..n] repeatedly, and alternates between bubbling upward
and sinking downward on alternate scans.
A bubblesort, sinking sort, or shakersort is adaptive if it stops the ﬁrst time a scan
produces no transpositions.
Heapsort sorts a sequence of entries by iteratively enheaping them all into a heap
(§18.2.4) and then iteratively deheaping them all. The order in which they deheap is
sorted.
Facts:
1. The name “bubblesort” suggests imagery in which lighter items (i.e., earlier in the
prescribed order of the key domain) bubble to the top of the list.
2. The name “sinking sort” suggests that heavier items sink to the bottom.
3. The name “shakersort” suggests a salt shaker that is turned upside down.
4. Since each swap during an exchange sort reduces the total disorder, it follows that
each scan brings the list closer to perfect order. By transitivity of the order relation, it
follows that if every consecutive pair in a sequence is in the correct order, then the entire
sequence is in order.
5. After the ﬁrst pass of a bubblesort from bottom to top, the smallest element is certain
to be in its correct ﬁnal position at the beginning of the list. After the second pass, the
second largest element must be in its correct position, and so on.
6. Bubblesort has worst-case time complexity O(n2).
7. For “almost sorted” sequences, an adaptive bubblesort can run much faster than
O(n2) time.
8. The priority property implies that the root of a priority tree is assigned the data
entry with ﬁrst precedence.

Section 18.3
SORTING AND SEARCHING
1353
9. Whereas a sequence of length n has
 n
2

pairs that might be out of order, a binary
tree of n elements has at most n log n pairs that could be out of order, if one compares
only those pairs such that one node is an ancestor of the other.
10. Heapsort improves upon the idea of bubblesort because it bubbles only along tree
paths between a bottom node and the root, instead of along the much longer path in a
linear sequence from a last item to the ﬁrst.
11. Heapsort runs in O(n log n) time.
12. Heapsort was invented by J. W. J. Williams in 1964.
Algorithms:
1. Algorithm 3 is an adaptive version of bubblesort.
Algorithm 3:
BubbleSort of array A[1..n].
ﬁrst := 1; last := n; exchange := true
while exchange
ﬁrst := ﬁrst + 1; exchange := false
for i := last downto ﬁrst
if A[i] < A[i −1] then
swap A[i] and A[i −1]; exchange := true
2. Algorithm 4 is a heapsort algorithm.
Algorithm 4:
HeapSort of array A[0..n] into array B[0..n].
procedure heapify(i)
if (A[i] is not a leaf) and (a child of A[i] is larger than A[i]) then
let A[k] be the larger child of A[i]
swap A[i] and A[k]
heapify(k)
procedure buildheap
for i := ⌊n
2 ⌋downto 0
heapify(i)
main program heapsort
buildheap for i = 0 to n
deheap root of A and transfer its value to B[n −i]
Examples:
1. When cancelled checks are returned to the payer by a bank, they may be in nearly
sorted order, since the payees are likely to deposit checks quite soon after they arrive.
Thus, they arrive for collection in an order rather close to the order in which they are
written. A shakersort might work quite quickly on such a distribution.
2. Starting with the unsorted list L = 15, 8, 10, 6, 17, 13, bubblesort would produce the

1354
Chapter 18
INFORMATION STRUCTURES
following sequence of lists.
initial list: 15
8
10
6
17
13
after one pass: 6
15
8
10
13
17
after two passes: 6
8
15
10
13
17
after three passes: 6
8
10
15
13
17
after four passes: 6
8
10
13
15
17
3. Starting with the unsorted list L = 15, 8, 10, 6, 17, 13, sinking sort would produce the
following sequence of lists.
initial list: 15
8
10
6
17
13
after one pass: 8
10
6
15
13
17
after two passes: 8
6
10
13
15
17
after three passes: 6
8
10
13
15
17
4. Starting with the unsorted list L = 15, 8, 10, 6, 17, 13, shakersort would produce the
following sequence of lists:
initial list: 15
8
10
6
17
13
after one pass: 6
15
8
10
13
17
after two passes: 6
8
10
13
15
17
18.3.4
SORTING BY DIVIDE-AND-CONQUER
The strategy of a divide-and-conquer sort is to partition the given sequence into smaller
subsequences, to sort the subsequences recursively, and ﬁnally to merge the sorted sub-
sequences into a single sorted sequence.
Deﬁnitions:
A top-down mergesort splits the input sequence into two equal (or nearly equal) sized
subsequences, recursively mergesorts the two subsequences, and ﬁnally merges the two
sorted subsequences into a single sorted sequence.
A bottom-up mergesort initially regards each entry in its input sequence as a list of
length one. It merges two consecutive pairs at a time into lists of length two. Then it
merges the lists of length two into lists of length four. Ultimately, all the initial items
are merged into a single list.
A quicksort selects an element x (called the pivot) in the input list and splits the input
list into two subsequences S1 and S2 such that every element in S1 is no larger than x
and every element in S2 is no smaller than x. Next it recursively sorts S1 and S2. Then
it concatenates the two sorted subsequences into a single sorted sequence.
The pivot in a quicksort iteration is the element x at which the sequence is split.

Section 18.3
SORTING AND SEARCHING
1355
Facts:
1. A top-down mergesort is usually implemented as an internal sort.
2. A bottom-up mergesort is a common form of external sort.
3. An outstanding merit of quicksort is that it can be performed quickly within a single
array.
4. Quicksort was ﬁrst described by C. A. R. Hoare in 1962.
5. The running time of a mergesort is O(n log n).
6. In the worst case, a quicksort takes time Ω(n2).
7. Choosing the quicksort pivot at random tends to avoid worst-case behavior.
8. The average running time for a quicksort is O(n log n).
9. External sorting is used to process very large ﬁles, much too large to ﬁt into the
primary memory of any computer.
10. The emphasis in devising good external sorting algorithms is on decreasing the
number of times the data are accessed because the time required to transfer data back
and forth between the the primary memory and the tape usually far outweighs the time
required to perform comparisons on data in the primary memory.
11. Formal algorithms and more detailed discussions of external sorting can be found
in [Kn98].
12. In an external sort, the number of times each element is read from or written
to the external memory is log( n
m) + 1, where m is the available internal memory size.
Improvements on the construction of runs as well as on the merging process are possible
(see [Kn98]).
Algorithms:
1. Algorithm 5 merges two sorted sequences into a single sorted sequence.
Algorithm 5:
Merge two sequences.
procedure merge(A[1..m], B[1..h], C[ ])
{Merge two sorted sequences A and B into a single sorted sequence C}
iA := 1; iB := 1; iC := 1
while iA ≤m and iB ≤h
if A[iA] ≤B[iB] then
C[iC] := A[iA]; iA := iA + 1
else
C[iC] := B[iB]; iB := iB + 1
iC := iC + 1
if iA > m then
move the remaining elements in B to C
else
move the remaining elements in A to C
2. Algorithm 6 mergesorts a sequence internally.

1356
Chapter 18
INFORMATION STRUCTURES
Algorithm 6:
MergeSort S.
procedure mergesort(S)
if length(S) ≤1 then return else
split S into two (equal or nearly equal)-sized subsequences S1 and S2
mergesort S1
mergesort S2
merge(S1, S2)
3. In a typical external mergesort such as Algorithm 7, there are two input tapes and
two output tapes. The entries are initially arranged onto the two input tapes, with half
the entries on each tape, and regarded as sublists of length one. A sublist from the ﬁrst
input tape is merged with a sublist from the second input tape and written as a sublist
of doubled length onto the ﬁrst output tape. Then the next sublist from the ﬁrst input
tape is merged with the next sublist from the second input tape and written as a sublist
of doubled length onto the second output tape. The alternating process is iterated until
the sublists from the input tapes have all been merged into sublists of doubled length
onto the two output tapes. Then the two output tapes become input tapes to another
iteration of the merging process. This continues until all the original entries are in a
single list.
Algorithm 7:
External MergeSort sequence S of length n.
for i := 1 to ⌈log n⌉
for j := 1 to ⌈log n
4i ⌉
merge next sublist from input A with next sublist from input B,
writing merged sublist onto output tape C
merge next sublist from input A with next sublist from input B,
writing merged sublist onto output tape D
reset output tape C as input tape A and vice versa
reset output tape D as input tape B and vice versa
4. The generic quicksort algorithm QuickSort (Algorithm 8) does not specify how to
select a pivot.
Algorithm 8:
QuickSort.
procedure split(x, S)
for each element y in S do
if x ≥y then
put y in S1
else
put y in S2
main program
if length(S) ≤1 then return else
choose an arbitrary element x in sequence S

Section 18.3
SORTING AND SEARCHING
1357
split(x, S) into S1 and S2
recursively sort S1 and S2
concatenate the two sorted subsequences
Example:
1. The following illustrates MergeSort on the sequence S = 21, 6, 8, 11, 10, 17, 15, 13.
21   6   8   11   10   17   15   13
6   8   10   11   13   15   17   21
10   17   15   13
10   13   15   17
21   6   8   11
6   8   11   21
21   6
6   21
8   11
10   17
13   15
21
8   11
10   17
15   13
6
8
11
10
17
15
13
(split)
(split)
(split)
(merge)
(merge)
(merge)
18.3.5
SORTING BY DISTRIBUTION
Prior knowledge of the distribution of the elements of the input sequence sometimes
permits sorting algorithms to break the lower bound of Ω(n log n) for running time of
comparison sorts.
Deﬁnitions:
The rank of an element of a ﬁnite ordered set is the number of elements that it exceeds
or equals.
A rank-counting sort calculates the “rank” for each element, and then assigns the
elements directly to their correct position according to their rank.
In a base-ten radix sort, the keys are base-ten integer numerals with at most k digits.
Each entry is appended to one of ten queues Q0, . . . , Q9, according to the value of its
least signiﬁcant digit, after which the list Q0 ◦· · · ◦Q9 is formed by concatenation. The
concatenated list is then similarly separated into ten queues, according to the values of
the next least signiﬁcant digit. This process is iterated up to the most signiﬁcant digit.
A radix sort is a sort like the base-ten radix sort, using an arbitrary radix, not necessarily
ten.
Facts:
1. A rank-counting sort gives favorable results when the input keys are n diﬀerent
positive integers, all bounded by cn for some constant c.

1358
Chapter 18
INFORMATION STRUCTURES
2. The running time of a rank-counting sort is O(n). The RankCountingSort (Algorithm
9) can be modiﬁed so that it sorts in linear time even when the input elements are not
all distinct [CoEtal09].
3. It can be proved that a radix sort correctly sorts the input (see [Kn98]).
4. The running time of RadixSort is bounded by O(kn), where k is the maximum
number of digits in a key. When k is a constant independent of n, RadixSort sorts in
linear time. Note, however, that if the input consists of n distinct numbers and the base
of the numbers is ﬁxed, then k is of order Ω(log n).
Algorithms:
1. In the rank-counting sort (Algorithm 9), the array A contains n input elements, the
array B is the output array, and the array C is an auxiliary array of size cn used for
counting. Step 3 causes count C[A[j] ] to be the rank of entry A[j].
Algorithm 9:
RankCountingSort of array A[1..] into array B[1..n].
{pre : max(A[i]) ≤cn}
for i := 1 to cn
C[i] := 0
for j := 1 to n
C[A[j]] := 1
for i := 2 to cn
C[i] := C[i] + C[i −1]
for j := 1 to n
B[C[A[j]]] := A[j]
2. The base-ten radix sort (Algorithm 10) starts with an input list A whose keys have
at most k digits.
Algorithm 10:
RadixSort of array A[1..n].
for d := 1 to k
for i := 0 to 9
make Qi an empty queue
for j := 1 to n
let h be the jth digit of A[j]
append A[j] to queue Qh
A := Q0 ◦· · · ◦Q9
{Concatenation}
18.3.6
SEARCHING
Deﬁnitions:
Searching a database means seeking either a target entry with a speciﬁc key or a
target entry whose key has some speciﬁed property.

Section 18.3
SORTING AND SEARCHING
1359
Linear search is the technique of scanning the entries of a list in sequence, either until
some stopping condition occurs or the entire list has been scanned.
Binary search is a recursive technique for seeking a speciﬁc target entry in a list. The
target key is compared to the key in the middle of the list, in order to determine which
half of the list could contain the target item, if it is present.
Hashing is storage-retrieval in a large table in which the table location is computed
from the key of each data entry (see §18.4).
A binary search tree is a binary tree in which each node has an attribute called its
key, and the keys are elements of an ordered datatype (e.g., the integers or alphabetic
strings). Moreover, at each node v the key is larger than all the keys in its left subtree,
but smaller than all the keys in its right subtree.
A 2-3 tree is a tree in which each non-leaf node has 2 or 3 children, and in which every
path from the root to a leaf is of the same length.
An AVL tree is a binary search tree with the property that the two subtrees of each
node diﬀer by at most 1 in height.
Facts:
1. Some common database search objectives are for a speciﬁed target entry, for the
maximum entry, for the minimum entry, or for the kth smallest entry.
2. The performance of a dynamic database structure that permits insertions and dele-
tions is measured by the time needed for insertions and deletions, as well as the time
needed for searching.
3. A binary search runs in average time O(log n) to search for a speciﬁed element x in
a sorted list of n elements.
4. In the worst case of searching by comparison-based algorithms for a speciﬁed target
element in a sorted list of length n, Ω(log n) comparisons are necessary.
5. A randomly constructed n-node binary search tree has expected height of at most
2 log n.
6. An AVL tree of n nodes has depth O(log n).
7. Insertion and deletion on an AVL tree, with patching if needed so that the result is
an AVL tree, can be performed in O(log n) worst-case running time [Kn98].
8. AVL trees are named for their inventors, G. M. Adelson-Velskii and Y. M. Landis.
9. A 2-3 tree for a set S of entries can be constructed by assigning the entries to the
leaves of the tree in order of increasing key from left to right. Each non-leaf node v
is labeled with two elements L[v] and M[v], which are the largest keys in the subtrees
rooted at its left child and middle child, respectively.
10. The operations of searching, ﬁnding a maximum or minimum, inserting a new entry,
and deleting an entry all execute within O(log n) time.
Algorithms:
1. To search for a speciﬁed target key x in a sorted list A[1..n], a call to the recursive
algorithm BinarySearch (A, 1, n, x) in Algorithm 11 can be used.
Its technique is to
compare the target to the middle entry of the list and to decide thereby in which half
the target might occur; then that half remains as the active portion of the list for the
next iteration of the search step, while the other half becomes inactive.

1360
Chapter 18
INFORMATION STRUCTURES
Algorithm 11:
BinarySearch (A, L, U, x).
{Look for x in A[L..U]; report its position if found, else report 0}
if L = U then
if x = A[L] then return L else return 0
else
M :=
 L+U
2

if x > A[M] then
return BinarySearch (A, M + 1, U, x)
else return BinarySearch (A, L, M, x)
2. Find the maximum [minimum] in an unsorted list: Scan the list from start to ﬁnish
and keep track of the largest [smallest] seen so far.
3. Finding the maximum in a binary search tree:
Start at the root and follow the
right-child pointers until some node has no right child. That node must contain the
maximum.
4. Finding the minimum in a binary search tree: Start at the root and follow the left-
child pointers until some node has no left child. That node must contain the minimum.
5. Searching for a target entry x in a 2-3 tree:
Start at the root, and use the keys at
non-leaf nodes to locate the correct leaf, as described by Algorithm 12.
Algorithm 12:
23TSearch(x, r).
{Case 1: r is a leaf}
if r is labeled with x then return “yes” else return “no”
{Case 2: r is not a leaf}
if x ≤L[r] then return 23TSearch(x, leftchild(r))
else if x ≤M[r] then return 23TSearch(x, midchild(r))
else if x has a right child then return 23TSearch(x, rightchild(r))
else return “no”
6. Finding the maximum in a 2-3 tree:
Starting from the root, follow the right-child
pointers to the rightmost leaf, which contains the maximum entry.
7. Finding the minimum in a 2-3 tree:
Starting from the root, follow the left-child
pointers to the leftmost leaf, which contains the minimum entry.
8. To insert a new entry x into a 2-3 tree:
First locate the non-leaf node v whose
child x “should” be. If v is a 2-child node v, then simply install x as a third child of v.
If v already has three children, then let v keep as children the two smallest of the set
comprising its three children and x. A new non-leaf node u becomes the parent of the
largest member of this set. Now recursively insert node u as a new child to the parent
of node v. If the process eventually makes the root of the tree a 4-child node, then the
last step is to create a new root with two new children, each of which has two of the four
children of the former root. Note that the labels of some non-leaf nodes may be updated
in this process.
9. To delete an entry x from a 2-3 tree:
Essentially, reverse the manner by which an
element is inserted. First ﬁnd the leaf v containing x. If the parent p of v has three

Section 18.3
SORTING AND SEARCHING
1361
children, then the leaf v is simply deleted. If p has only two children v and v′, then
select an adjacent sibling p′ of p. If p′ has only two children, then make v′ a child of p′,
and recursively delete the node p from the tree. If p′ has three children, then make an
appropriate child of p′ into a new child of p and delete the node v (note that now both p
and p′ have two children). Again the process may progress recursively to the root of the
tree, such that it is necessary to delete one of the only two children of the root. In this
case, delete the root and make the remaining child of the root into a new root of the
tree. Labels of some non-leaf nodes may need to be updated in this process.
10. Searching in a random list:
Finding the kth smallest element, for an arbitrary k,
in a random list of size n can also be done in linear time. The algorithm, Algorithm 13,
is based on the method of “Prune and Search”. That is, the process ﬁrst prunes away
in linear time a constant factor of the elements in the input, then recursively searches
the rest. A careful analysis shows that each of the two sets S1 and S2 contains at most
7n
10 elements. Therefore, if T (n) is the running time of Algorithm 13 for n elements, it
follows that T (n) ≤T ( n
5 ) + T ( 7n
10 ) + O(n). This relation gives T (n) = O(n).
Algorithm 13:
Finding-the-kth-Smallest.
divide the n input elements into ⌈n
5 ⌉groups of ﬁve elements
ﬁnd the median for each of the ⌈n
5 ⌉groups
recursively ﬁnd the median m∗of these group medians
partition the input into two sets S1 and S2 such that each element in S1 is no
larger than m∗and each element in S2 is no smaller than m∗
if S1 has ≥k elements then
recursively ﬁnd the kth smallest element in S1
else
recursively ﬁnd the (k −|S1|)th smallest element in S2
Example:
1. We apply Algorithm 11 to search for the target 64 in the following 16-element list:
5, 8, 9, 13, 16, 22, 25, 36, 47, 49, 64, 81, 100, 121, 144, 169.
First split it into these two 8-element sublists
5, 8, 9, 13, 16, 22, 25, 36
47, 49, 64, 81, 100, 121, 144, 169
and then compare 64 to the largest item in the ﬁrst list. Since 64 > 36, it follows that
64, if present in the original list, would have to be in the second sublist. Next split the
active sublist further into these two 4-element sublists
47, 49, 64, 81
100, 121, 144, 169
and then compare 64 to the largest item in the ﬁrst new sublist. Since 64 ≤81, it follows
that 64, if present, would have to be in the second sublist. Therefore, resplit the active
sublist further into these two 2-element sublists
47, 49
64, 81
and then compare the target 64 to the largest item in the ﬁrst new sublist. Since 64 > 49,
it follows that 64, if present, would have to be in the second sublist. Therefore, resplit
the active sublist further into these two 1-element sublists
64
81

1362
Chapter 18
INFORMATION STRUCTURES
and then compare 64 to the largest item in the ﬁrst new sublist. Since 64 ≤64, it follows
that 64, if present, would have to be in the ﬁrst sublist. Since that sublist has only one
element, namely 64, the target 64 is compared to that one element. Since they are a
match, the target has been located as the 11th item in the original list.
18.4
HASHING
Hashing, also known as “address calculation” and as “scatter storage”, is a mathematical
approach to organizing records within a table. The objective is to reduce the amount
of time needed to ﬁnd a record with a given key. Hashing is best suited for “dynamic”
tables, that is, for databases whose use involves interspersed lookups and insertions.
Dynamic dictionaries (such as spelling checkers) and compiler-generated symbol tables
are examples of applications where hashing may be useful.
18.4.1
HASH FUNCTIONS
Hashing is an approach to placing records into a table and retrieving them when needed,
in which the location for a record is calculated by an algorithm called a hash function.
Deﬁnitions:
A record is a pair of the form (k:key, d:data), in which the second component is data
and the ﬁrst component is a key used to store it in a table and to retrieve it subsequently
from that table.
A key domain is an ordered set, usually the integers, whose members serve as keys for
the records of a table. No two diﬀerent records have the same key.
A hash table is an array, in which the location for storing and retrieving a record are
calculated from the key of that record.
A hash function h is a function that maps a key domain to an interval of integers
[0..m−1]. The intent is that a record with key k is to be stored in or retrieved from the
location h(k) in the table.
A collision occurs when a hash function h assigns the same table location to two diﬀerent
keys k1 ̸= k2, i.e., when h(k1) = h(k2).
Collision resolution is the process of ﬁnding an alternative location for a new record
whose key collides with that of a record already in the table.
The fullness of a hash table T is the ratio α(T ) = n
m of the number n of records in the
table to the capacity m of the table.
Facts:
1. Hashing is often used when the set of keys (of the records in the database) is not a
consecutive sequence of integers or easily convertible to a consecutive sequence of integers.
2. Keys that are non-numeric (or do not have a good random distribution) can be trans-
formed into integers by using or modifying their binary representation. The resulting
integers are called virtual keys.

Section 18.4
HASHING
1363
3. It is desirable for a hash function to have the simplicity property, i.e., that it takes
only a few simple operations to compute the hash function value.
4. It is desirable for a hash function to have the uniformity property, i.e., that each
possible location in the range 0, . . . , m −1 of the hash function h: K →[0..m−1] is
generated with equal likelihood, that is, with probability
1
m.
Examples:
1. The division method h(k) = k mod m is a simple hash function that can be used
when the keys of the records are integers that extend far beyond a feasible table size.
The table size m must be chosen carefully to avoid high instance of collision, without
wasting too much storage space. Selecting m to be a prime not close to a power of 2 is
typically a good choice.
2. The multiplication method is another simple hashing rule. First the key (in base 2)
is multiplied by a constant value A such that 0 < A < 1, and then p = log m bits
are selected for the hash function value from somewhere in the fractional part of the
resulting base-2 numeral, often required to be away from its low end. (This is similar to
some methods for generating random numbers.)
3. As a simpliﬁed example of a multiplicative hash function, consider table size m = 16,
address size p = log 16 = 4 bits, and key size w = 6 bits. With fractional constant
A = 0.1010112, use the ﬁrst four bits of the fractional part. For instance, given key
k = 0110112, ﬁrst calculate R = A · k = 010010.0010012. Then take k = 00102, the
low-end four bits of the fractional part. Knuth [Kn98] suggests A = 0.6180339887... as a
good choice of a multiplier.
18.4.2
COLLISION RESOLUTION
Deﬁnitions:
Collision resolution is the process of computing an alternative location for a colliding
record. The two basic methods are chaining and rehashing.
Chaining is a collision resolution method that involves auxiliary storage space for records
outside the conﬁnes of the main array. Each slot in the main table can be used as the
root of a linked list that contains all the records assigned to that location by the hash
function. Each additional colliding record is inserted at the front of the linked list. When
searching for a record, the list is traversed until the record is found, or until the end of
the list is reached.
The size of a chained hash table is the number of linked list headers (i.e., the size of
the main array). Thus, a chained hash table said to be of size m may be used to store a
database with more than m records.
Rehashing is a collision resolution method in which there is no auxiliary storage outside
the main table, so that a colliding record must be stored elsewhere in the main table, that
is, at a location other than that assigned by the hash function to its key k. A collision
resolution function ﬁnds the substitute location.
A collision resolution function under rehashing generates a probe sequence ⟨h0(k) =
h(k), h1(k), h2(k), . . . , hm−1(k)⟩. The new record is inserted into the ﬁrst unoccupied
probe location. When searching for a record, the successive probes are tried until the
record is found or the probe ﬁnds an unoccupied location (i.e., unsuccessful search).

1364
Chapter 18
INFORMATION STRUCTURES
A probe sequence for key k is a sequence ⟨h0(k), h1(k), h2(k), . . . , hm−1(k)⟩of possible
storage locations in the table T that runs without repetition through the entire set
⟨0, 1, 2, . . ., m −1⟩of locations in the table, as possible places to store the record with
key k.
Clustering is a hashing phenomenon in which after two keys collide, their probe se-
quences continue to collide.
Linear probing means trying to resolve a collision at location h(k) with a probe se-
quence of the form hi(k) = (h0(k) + ci) mod m.
Quadratic probing means trying to resolve a collision at location h(k) by using a probe
sequence of the form hi(k) = (h0(k) + c1i + c2i2) mod m.
A secondary hash function is a hash function used to generate a probe sequence, once
a collision has occurred.
Double hashing means using a primary hash function h and a secondary hash func-
tion h′ to resolve collisions, so that hi(k) = (h(k) + ih′(k)) mod m.
Facts:
1. In designing a hash function, an objective is to keep the length of the probe sequences
short, so that records are stored and retrieved quickly.
2. Under chain hashing, inserting a record always requires O(1) time.
3. Under chain hashing, the time to ﬁnd a record is proportional to the length of the
chain from its key, and the average length of a chain in the table equals the fullness α of
the table.
4. Under chain hashing, if the number of records in the table is proportional to the table
capacity, then a ﬁnd operation needs O(1) time, on average.
5. Under chain hashing, a delete operation consists of a ﬁnd with time proportional to
table fullness, followed by removal from a linked list, which requires only O(1) time.
6. Analysis of rehashing performance is based on the following assumptions:
• uniform distribution of keys: each possible key in the key domain is equally likely
to occur as the key of some record;
• uniform distribution of initial hash locations (see §18.4.1);
• uniform distribution of probe sequences: each possible probe sequence
⟨h0(k), h1(k), h2(k), . . . , hm−1(k)⟩,
regarded as a permutation on the set of all table locations, is equally likely.
7. Under rehashing, the expected time to perform an insert operation is the same as
the expected time for unsuccessful search, and is at most
1
1−α.
8. Under rehashing, the expected time E(α) to perform a successful ﬁnd operation is at
most 1
α ln
1
1−α + 1
α. For instance, E(0.5) = 3.386 and E(0.9) = 3.670. That means that
if a table is 90% full, a record will be found, on average, with 3.67 probes.
9. Under rehashing, location of the deleted record needs to be marked as deleted so
that subsequent probe sequences do not terminate prematurely. Moreover, the running
time of a delete operation is the same as for a successful ﬁnd operation. (It also causes
the measure of fullness for searches to be diﬀerent from that used for insertions, because
a new record can be inserted into the location marked as deleted). However, in most
applications that use hashing, records are never deleted.

Section 18.5
DYNAMIC GRAPH ALGORITHMS
1365
Examples:
1. The following example of linear probing uses prime modulus m = 1013 and prime
multiplier c = 367. The keys are taken to be social security numbers.
key k
h0(k)
h1(k)
113548956
773
146834522
172
207639313
651
359487245
896
378545523
592
435112760
896
250
670149788
651
721666437
172
539
762456748
12
2. Linear probing suﬀers from clustering. That is, if hi(k1) = hj(k2), then hi+p(k1) =
hj+p(k2) for all p = 1, 2, . . . . All probe sequences follow the same (linear) pattern, from
which it follows that long chains of ﬁlled locations will cause a large number of probes
needed to insert a new record (and to locate it later).
3. Quadratic probing suﬀers from clustering. That is, if h0(k1) = h0(k2), then hi(k1) =
hi(k2) for i = 1, 2, . . . .
4. Pairing the primary hash function h(k) = k mod p with the secondary hash function
h′(k) = k div p, where p is a prime, yields the double hash function hi(k) = (h0(k) +
ih′(k)) mod p.
18.5
DYNAMIC GRAPH ALGORITHMS
Dynamic graph algorithms are algorithms that maintain information with regard to prop-
erties of a (possibly edge-weighted) graph while the graph is changing. These algorithms
are useful in a number of application areas, including communication networks, VLSI
design, distributed computing, and graphics, where the underlying graphs are subject
to dynamic changes. Eﬃcient dynamic graph algorithms are also used as subroutines in
algorithms that build and modify graphs as part of larger tasks, e.g., the algorithm for
constructing Voronoi diagrams by building planar subdivisions.
Notation:
Throughout this section, n and m denote the number of vertices and the
number of edges, respectively, of the graph G that is being maintained and queried
dynamically.
18.5.1
DYNAMICALLY MAINTAINABLE PROPERTIES
Deﬁnitions:
A (dynamic) update operation is an operation on a graph that keeps track of whether
the graph has a designated property.
A query is a request for information about the designated property.

1366
Chapter 18
INFORMATION STRUCTURES
Facts:
1. The primitive update operations for most dynamic graph algorithms are edge inser-
tions and deletions and, in the case of edge-weighted graphs, changes in edge weights.
2. For most dynamic graph algorithms, insertion or deletion of an isolated vertex can
be accomplished by an easy modiﬁcation of a nondynamic algorithm.
3. The insertion or deletion of vertices together with their incident edges is usually
harder and has to be done by iterating the associated edge update operation.
4. There is a trade-oﬀbetween the time required for update operations and the time
required to respond to queries about the property being maintained. Thus, running times
of the update operations depend strongly on the property being maintained.
5. Nontrivial dynamic algorithms corresponding to several graph properties are known
(see Examples).
Examples:
1. Connectivity: The permitted query is whether two vertices x and y are in the same
component. Permitted updates are edge insertions, edge deletions, and isolated vertex
insertions. Frederickson [Fr85] provides an algorithm for maintaining minimum spanning
forests that can easily be adapted to this problem. Improvements in running times have
been achieved by [EpEtal92b] and [EpGaIt93].
2. Bipartiteness:
Update operations are the same as for connectivity (Example 1). A
query simply asks whether a graph is bipartite. An algorithm is presented in [EpEtal92b],
with an improvement in [EpGaIt93].
3. Minimum spanning forests: The query is whether an edge is in a minimum spanning
forest. The graph is weighted, and the update operations are increments and decrements
of weights. (Edge insertion is accomplished by lowering the edge weight from ∞and edge
deletion by incrementing the edge weight to ∞.) [Fr85] contains the early result, with
improvements by [EpGaIt93]. The plane and planar graph cases have been considered
by [EpEtal92b] and [EpEtal93].
4. Biconnectivity and 2-edge connectivity: Update operations are the same as for con-
nectivity (Example 1). Queries ask whether two given vertices lie in the same biconnected
(resp., 2-edge connected) component. Eﬃcient algorithms for maintaining biconnectivity
are found in [EpEtal92b], [EpGaIt93], [Ra93], and [HeRaSu94]. Eﬃcient algorithms for
maintaining 2-edge connectivity are found in [EpEtal92b], [Fr91], and [Fr85]. Any al-
gorithm for dynamically maintaining biconnectivity translates to an algorithm with the
same time bounds for 2-edge connectivity [GaIt91].
5. Planarity:
Update operations include edge insertions and deletions. Queries ask
whether the graph is currently planar. Variants include queries that would test whether
the addition of a particular edge would destroy the current imbedding. Algorithms are
described in [EpEtal93] and [Ra93].
18.5.2
TECHNIQUES
Deﬁnitions:
A partially dynamic algorithm is usually an algorithm that handles only edge inser-
tions and, for edge-weighted graphs, decrements in edge weights. Less commonly, this
term can refer to an algorithm that handles only edge deletions or weight increments.

Section 18.5
DYNAMIC GRAPH ALGORITHMS
1367
A cluster in a spanning tree T for a graph G is a set of vertices such that the subgraph
of T induced on these vertices is connected.
An ambivalent data structure is a structure that, at many of its vertices, keeps track
of several alternatives, despite the fact that a global examination of the structure would
determine which of these alternatives is optimal.
A certiﬁcate for property P and graph G is a graph G′ such that G has property P if
and only if G′ has property P.
A strong certiﬁcate for property P and graph G is a graph G′, on the same vertex
set as G, such that, for every graph H, G ∪H has property P if and only if G′ ∪H has
property P.
A sparse certiﬁcate is a strong certiﬁcate in which the number of edges is O(n).
A function A that maps graphs to strong certiﬁcates is stable if it satisﬁes
• A(G ∪H) = A(A(G) ∪H);
• A(G −e) diﬀers from A(G) by O(1) edges, where e is an edge in G.
A stable certiﬁcate is one produced by a stable mapping.
A plane graph is a planar graph, together with a particular imbedding in the plane.
A compressed certiﬁcate for a property P of G, where G = (V, E) is a subgraph of a
larger graph F and X ⊂V separates G from F −G, is a small certiﬁcate G′ = (V ′, E′)
with X ⊂V ′ such that, for any graph H that is attached to G only at the vertices of X,
H ∪G has property P if and only if H ∪G′ does, and |V ′| = O(|X|).
A graph property P is dyadic if it is deﬁned with respect to a pair of vertices (x, y).
A graph C is a certiﬁcate of a dyadic property P for X in G if and only if, for any H
with V (H) ∩V (G) ⊂X and every x and y in V (H), P is true for (x, y) in the graph
G ∪H if and only if it is true for (x, y) in the graph C ∪H.
Facts:
1. Using the union-ﬁnd data structure [Ta75], it is possible to maintain connectivity
information in O(α(m, n)) amortized time per update or query. Here α(m, n) denotes
the inverse Ackermann function, and it grows extremely slowly.
2. For other graph properties, such as 2-edge connectivity and biconnectivity, a data
structure called the link/condense tree [WeTa92] maintains information in O(α(m, n))
amortized time per update or query.
3. The link/condense tree supports the operation of condensing an entire path in the
tree into a single vertex. This is important in the applications considered, because the
insertion of an edge may cause several biconnected components or 2-edge connected
components to be combined into one.
4. Link/condense trees are based on dynamic trees. [SlTa83]
5. The dynamic tree data structure maintains a set of rooted trees. It supports the
operations of linking the root of one tree as the child of a vertex in another tree, cutting
a tree at a speciﬁed edge, and everting a tree to make a speciﬁed vertex the root in
worst-case O(log n) time per operation. It also supports other operations based on keys
stored at vertices, such as ﬁnding the minimum key on the path from a given vertex to
the root in O(log n) time.

1368
Chapter 18
INFORMATION STRUCTURES
6. To maintain minimum spanning trees in an edge-weighted, connected graph subject
to changes in edge weights, Frederickson [Fr85] uses clustering and topology trees in
Algorithm 1.
Algorithm 1:
Maintaining a minimum spanning tree.
Preprocessing:
ﬁnd a minimum spanning tree T of the initial graph G
maintain a dynamic tree of T , using [SlTa83]
let z := n2/3
group the vertices of T into clusters whose sizes are between z and 3z −2
{There will be Θ(n1/3) such clusters}
for each pair of clusters i, j maintain the set of edges Eij as a min-heap
Updates:
decreases in tree edge weights do not change anything
increases in non-tree edge weights are handled by a suitable update of the
appropriate min-heap
decreases in non-tree edge weights are handled by using the dynamic tree
appropriately
if tree edge e increases in weight, remove it, thus partitioning the clusters into
two sets; then ﬁnd an edge of minimum cost between clusters on opposite sides
of the partition
7. Ambivalence may permit faster updates, possibly at the cost of slower queries.
8. Frederickson [Fr91] presents an ambivalent data structure for spanning forests that
builds on the ideas of multilevel partitions and (2-dimensional) topology trees developed
in [Fr85].
9. Let P be a property for which sparse certiﬁcates can be found in time f(n, m).
Suppose that there is a data structure that can be built in time g(n, m) and permits
static testing of property P in time q(n, m). Then there is a fully dynamic data struc-
ture for testing whether a graph has property P; update time for this structure is
f(n, O(n))O(log( m
n )) + g(n, O(n)), and query time is q(n, O(n)). This “basic sparsiﬁ-
cation technique” is used to dynamize static algorithms. To use it, one need only be able
to compute sparse certiﬁcates eﬃciently.
10. The sparsiﬁcation method of [EpEtal92b] partitions the input graph into sparse
subgraphs (with O(n) edges) and summarizes the relevant information about each sub-
graph in an even sparser “certiﬁcate”. Certiﬁcates are merged in pairs, producing larger
subgraphs that are themselves sparsiﬁed using the certiﬁcate technique. The result is
a balanced binary tree in which each vertex is a sparse certiﬁcate. Each insertion or
deletion of an edge in the input graph causes changes in log( m
n ) tree vertices. Because
these changes occur in graphs with O(n) edges, instead of the m edges in the input graph,
time bounds for updates are reduced in most natural problems.
11. Let P be a property for which stable sparse certiﬁcates can be maintained in time
f(n, m) per update. Suppose that there is a fully dynamic algorithm for P with update
time g(n, m) and query time q(n, m). Then this algorithm can be sped up; speciﬁcally, P
can be maintained fully dynamically in time f(n, O(n))O(log( m
n )) + g(n, O(n)) per up-
date, with query time q(n, O(n)). Because this “stable sparsiﬁcation technique” is used

Section 18.5
DYNAMIC GRAPH ALGORITHMS
1369
to speed up existing dynamic algorithms, it often yields better results than the basic
sparsiﬁcation technique described in Fact 10. However, to use it, one must be able to
maintain stable sparse certiﬁcates eﬃciently; this is a more stringent requirement than
what is needed to apply basic sparsiﬁcation.
12. Eppstein, Galil, and Italiano [EpGaIt93] improve the sparsiﬁcation technique to
remove the log( m
n ) factor in these bounds. They achieve this improvement by partitioning
the edge set of the original graph more carefully.
13. Dynamic algorithms restricted to plane graphs have been considered by several
authors. Eppstein et al. [EpEtal93] introduce a variant of sparsiﬁcation that permits the
design of eﬃcient dynamic algorithms for planar graphs in an imbedding-independent
way, as long as the updates to the graph preserve planarity. Because these graphs are
already sparse, Eppstein et al. design a separator-based sparsiﬁcation technique.
14. The fact that separator sizes are sublinear (O(√n) for planar graphs) allows the pos-
sibility of maintaining sublinear certiﬁcates. Eppstein et al. [EpEtal93] use a separator-
based decomposition tree as the sparsiﬁcation tree and show how to compute it in linear
time and maintain it dynamically. They use it to show the following: For a property P
for which compressed certiﬁcates can be built in time T (n), a data structure for testing
P built in time P(n), and queries answered in time Q(n), a fully dynamic algorithm for
maintaining P under planarity-preserving insertions and deletions takes amortized time
P(O(n1/2)) + T (O(n1/2)) per update and Q(O(n1/2)) per query.
15. A dyadic property P, for which compressed certiﬁcates can be built in time T (n),
a data structure for testing P built in time P(n), and queries answered in time Q(n),
can be maintained with updates taking T (O(n1/2)) amortized time and queries taking
P(O(n1/2)) + Q(O(n1/2)) + T (O(n1/2)) worst-case time.
16. In dealing with plane (as opposed to planar) graphs and allowing only updates that
can be performed in a planarity-preserving manner on the existing imbedding, simpler
techniques that rely on planar duality can be used [EpEtal92b].
17. When maintaining minimum spanning trees under updates that change only edge
weights, the most diﬃcult operation to handle is an increase in the weight of an MST
edge. However, in the dual graph this can be viewed as a decrease in the weight of a non-
MST edge. This idea and the handling of edge insertions and deletions are addressed by
the data structures of [GuSt85] and the edge-ordered tree data structure of [EpEtal92b].
These data structures help maintain the subdivision and its dual in the face of general
updates and also help perform required access operations eﬃciently. Edge-ordered trees
are an adaptation of the dynamic trees of [SlTa83].
18. Knowledge of the imbedding allows one to use topology trees in more eﬃcient ways.
Speciﬁcally, Rauch [Ra94] partitions the non-tree edges into equivalence classes called
bundles. In the cyclical ordering of edges emanating from a cluster, bundles are carefully
chosen, consecutive subsets of edges.
18.5.3
APPLICATIONS
Examples:
1. Bipartiteness [EpEtal92a], [EpGaIt93]:
A graph that is not bipartite contains an
odd cycle. The graph formed by adding the shortest edge inducing an odd cycle (if any)
to the minimum spanning forest is a stable certiﬁcate of (non-)bipartiteness. Using the
clustering techniques of [Fr85] and the improved sparsiﬁcation techniques of [EpGaIt93],

1370
Chapter 18
INFORMATION STRUCTURES
this certiﬁcate can be maintained in O(n1/2) time per update. The query time in this
example is O(1); one bit is used to indicate whether the operation is maintaining a
certiﬁcate of bipartiteness or of non-bipartiteness.
2. Minimum spanning forests [EpEtal92a], [EpGaIt93], [Fr85]: In this example, the goal
is not to maintain a data structure that supports eﬃcient testing of a property, but rather
to maintain the minimum spanning forest itself as edges are added to and deleted from
the input graph. It is shown in [EpEtal92a] how to deﬁne a canonical minimum spanning
forest that serves as the analogue of a stable sparse certiﬁcate. Frederickson [Fr85] uses
the topological approach to obtain a fully dynamic algorithm that maintains minimum
spanning forests in time O(m1/2) per update. Applying the improved stable sparsiﬁcation
technique with f(n, m) = g(n, m) = O(m1/2) yields a fully dynamic minimum spanning
forest algorithm with update time O(n1/2). For plane graphs, [EpEtal92b] show that
both updates and queries can be performed in O(log n) time per operation; for planar
graphs, [EpEtal93] show that O(log2 n) time per deletion and O(log n) time per insertion
are suﬃcient.
3. Connectivity [EpEtal92a], [EpGaIt93], [Fr85]: Simple enhancements to the minimum
spanning forest algorithms in [Fr85] yield fully dynamic algorithms for the connectivity
problem in which the update times are the same as they are for minimum spanning
forests, and the query times are O(1). Thus, as in Example 2, applying improved stable
sparsiﬁcation with f(n, m) = g(n, m) = O(m1/2) yields a fully dynamic connectivity
algorithm with update time O(n1/2) and query time O(1). Similarly, the planar and plane
graph algorithms for minimum spanning trees can be generalized to work for minimum
spanning forests and adapted to maintain connected components.
4. Biconnectivity [EpEtal92a], [EpGaIt93], [HeRaSu94], [Ra93]:
Cheriyan, Kao, and
Thurimella [ChKaTh93] show that C2 = C1 ∪B2 is a sparse certiﬁcate for biconnec-
tivity, where C1 is a breadth-ﬁrst spanning forest of the input graph G, and B2 is a
breadth-ﬁrst spanning forest of G −C1. Eppstein et al. [EpEtal92a] show that C2 is
in fact a strong certiﬁcate of biconnectivity. These strong certiﬁcates can be found in
time O(m), using classical breadth-ﬁrst search algorithms. Applying improved sparsiﬁ-
cation with f(n, m) = g(n, m) = O(m) yields a fully dynamic algorithm for maintaining
the biconnected components of a graph that has update time O(n).
5. The approach to biconnectivity in [Ra94] is to partition the graph G into clusters
and decompose a query that asks whether vertices u and v lie in the same biconnected
component into a query in the cluster of u, a query in the cluster of v, and a query
between clusters. The 2-dimensional topology tree is adapted in a novel way, and the
ambivalent data structures previously deﬁned for connectivity and 2-edge connectivity
are extended to test biconnectivity between clusters. To test biconnectivity within a
cluster C, the entire subgraph induced by C and a compressed certiﬁcate of G −C are
maintained.
Using all these ingredients, [Ra94] obtains amortized O(m1/2) time per
update and O(1) worst-case time per query.
6. Using clever data structures based on topology trees, bundles, and the idea of recipes,
ﬁrst introduced in this context in [HeRaSu94], the problem of fully dynamic biconnec-
tivity for plane graphs can be solved in O(log2 n) time per update and O(log n) time per
query.
7. 2-edge connectivity [EpEtal92a], [Fr85], [Fr91]:
Thurimella [Th89] and Nagamochi
and Ibaraki [NaIb92] show that the following structure U2 is a certiﬁcate for 2-edge
connectivity: U2 = U1∪F2, where U1 is a spanning forest of G, and F2 is a spanning forest
of G −U1. Eppstein et al. [EpEtal92a] show that U2 is in fact a stable, sparse certiﬁcate.
Frederickson’s minimum spanning forest algorithm [Fr85] can be adapted to maintain

Section 18.5
DYNAMIC GRAPH ALGORITHMS
1371
U2 in time f(n, m) = O(m1/2).
Frederickson’s ambivalent data structure technique
[Fr91] can be used to test 2-edge connectivity with update time g(n, m) = O(m1/2) and
query time q(n, m) = O(log n). Here a “query” is a pair of vertices, and the answer is
“yes” if they are in the same 2-edge connected component and “no” otherwise. Applying
improved stable sparsiﬁcation yields a fully dynamic algorithm with update time O(n1/2)
and query time O(log n).
8. Planarity [EpEtal93], [Ra93]:
Eppstein et al. [EpEtal93] use the separator-based
sparsiﬁcation technique to obtain a fully dynamic planarity-testing algorithm for general
graphs that answers queries of the form “is the graph currently planar?” and “would
the insertion of this edge preserve planarity?”. Their algorithm requires amortized run-
ning time O(n1/2) per update or query. Italiano, La Poutr´e, and Rauch [ItLaRa93] use
topology trees, bundles, and recipes to obtain a fully dynamic algorithm on plane graphs
that tests whether the insertion of a particular edge would destroy the given imbedding.
Their algorithm requires time O(log2 n) for updates and queries.
18.5.4
RECENT RESULTS AND OPEN QUESTIONS
Examples:
1. Alberts and Henzinger [AlHe95] investigate dynamic algorithms on random graphs
with n vertices and m0 edges on which a sequence of k arbitrary update operations is
performed. They obtain expected update times of O(k log n + Pk
i=1
n
√mi ) for minimum
spanning forest, connectivity, and bipartiteness and O(k log n + √log n Pk
i=1
n
√mi ) for
2-edge connectivity. The data structures required for these algorithms use linear space,
and the preprocessing times match those of the best algorithms for ﬁnding a minimum
spanning forest.
2. Fredman and Henzinger [FrHe98] investigate lower bounds in the cell probe model
of computation and obtain good results for k-edge connectivity, k-vertex connectivity,
and planarity-testing of imbedded planar graphs. Both average-case analysis and lower
bounds are important topics for future research on dynamic graph algorithms.
3. Klein et al. [KlEtal94] give a fully dynamic algorithm for the all-pairs shortest path
problem on planar graphs. If the sum of the absolute values of the edge-lengths is D,
then the time per operation is O(n9/7 log D) (worst case for queries, edge deletion, and
length changes, and amortized for edge insertion); the space requirement is O(n). Several
types of partially dynamic algorithms for shortest paths appear in [AuEtal91], [EvGa85],
[FrMaNa94], and [Ro85].
4. King [Ki99] developed a fully dynamic algorithm for maintaining all pairs shortest
paths in directed graphs where the weights of the edges are positive integers less than a
bound b; this algorithm runs in O(n2.5√b log n) amortized time.
5. Henzinger and King [HeKi99] obtained fully dynamic, randomized algorithms for
connectivity, 2-edge connectivity, bipartiteness, cycle equivalence, and constant-weight
minimum spanning trees that have polylogarithmic expected time per operation.

1372
Chapter 18
INFORMATION STRUCTURES
REFERENCES
Printed Resources:
[AhHoUl74] A. V. Aho, J. E. Hopcroft, and J. D. Ullman, The Design and Analysis of
Computer Algorithms, Addison-Wesley, 1974.
[AhHoUl83] A. V. Aho, J. E. Hopcroft, and J. D. Ullman, Data Structures and Algo-
rithms, Addison-Wesley, 1983.
[AlHe95] D. Alberts and M. Rauch Henzinger, “Average case analysis of dynamic graph
algorithms”, Proceedings of the 6th Symposium on Discrete Algorithms, ACM/SIAM,
1995, 312–321.
[AuEtal91] G. Ausiello, G. F. Italiano, A. Marchetti-Spaccamela, and U. Nanni, “In-
cremental algorithms for minimal length paths”, Journal of Algorithms 12 (1991),
615–638.
[ChKaTh93] J. Cheriyan, M-Y. Kao, and R. Thurimella, “Scan-ﬁrst search and sparse
certiﬁcates: an improved parallel algorithm for k-vertex connectivity”, SIAM Journal
on Computing 22 (1993), 157–174.
[CoEtal09] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction to
Algorithms, 3rd ed., MIT Press, 2009. (A comprehensive study of algorithms and
their analysis.)
[EpGaIt93] D. Eppstein, Z. Galil, and G. Italiano, Improved Sparsiﬁcation, Technical
Report 93-20, Department of Information and Computer Science, University of Cal-
ifornia, Irvine, 1993.
[EpEtal92a] D. Eppstein, Z. Galil, G. Italiano, and A. Nissenzweig, “Sparsiﬁcation –
A technique for speeding up dynamic graph algorithms”, Proceedings of the 33rd
Symposium on Foundations of Computer Science, IEEE Computer Society, 1992,
60–69.
[EpEtal93] D. Eppstein, Z. Galil, G. Italiano, and T. Spencer, “Separator based sparsi-
ﬁcation for dynamic planar graph algorithms”, Proceedings of the 25th Symposium
on Theory of Computing, ACM, 1993, 208–217.
[EpEtal92b] D. Eppstein, G. Italiano, R. Tamassia, R. Tarjan, J. Westbrook, and M.
Yung, “Maintenance of a minimum spanning forest in a dynamic plane graph”, Jour-
nal of Algorithms 13 (1992), 33–54.
[EvGa85] S. Even and H. Gazit, “Updating distances in dynamic graphs”, Methods of
Operations Research 49 (1985), 371–387.
[Fr85] G. N. Frederickson, “Data structures for on-line updating of minimum spanning
trees, with applications”, SIAM Journal on Computing 14 (1985), 781–798.
[Fr91] G. N. Frederickson, “Ambivalent data structures for dynamic 2-edge-connectivity
and k smallest spanning trees”, Proceedings of the 32nd Symposium on Foundations
of Computer Science, IEEE Computer Society, 1991, 632–641.
[FrHe98] M. Fredman and M. R. Henzinger, “Lower bounds for fully dynamic connectivity
problems in graphs”, Algorithmica 22 (1998), 351–362.
[FrMaNa94] D. Frigioni, A. Marchetti-Spaccamela, and U. Nanni, “Incremental algo-
rithms for the single-source shortest path problem”, in Foundations of Software Tech-
nology and Theoretical Computer Science, Lecture Notes in Computer Science 880,
Springer, 1994, 113–124.

REFERENCES
1373
[GaIt91] Z. Galil and G. Italiano, “Reducing edge connectivity to vertex connectivity”,
SIGACT News 22 (1991), 57–61.
[GoBa91] G. H. Gonnett and R. Baeze-Yates, Handbook of Algorithms and Data Struc-
tures in Pascal and C, 2nd ed., Addison Wesley, 1991. (An extensive collection of
algorithms with analysis and implementation; includes mathematical formulas used
in analysis.)
[GuSt85] L. J. Guibas and J. Stolﬁ, “Primitives for the manipulation of general subdivi-
sions and the computation of Voronoi diagrams”, ACM Transactions on Graphics 4
(1985), 74–123.
[HeKi99] M. R. Henzinger and V. King, “Randomized fully dynamic algorithms with
polylogarithmic time per operation”, Journal of the ACM 46 (1999), 502–516.
[HeRaSu94] J. Hershberger, M. Rauch, and S. Suri, “Fully dynamic 2-edge-connectivity
in planar graphs”, Theoretical Computer Science 130 (1994), 139–161.
[ItLaRa93] G. Italiano, H. La Poutr´e, and M. Rauch, “Fully dynamic planarity testing in
embedded graphs”, Algorithms – ESA ’93, Lecture Notes in Computer Science 726,
Springer, 1993, 212–223.
[KlEtal94] P. Klein, S. Rao, M. Rauch, and S. Subramanian, “Faster shortest-path algo-
rithms for planar graphs”, Proceedings of the 26th Symposium on Theory of Com-
puting, ACM, 1994, 27–37.
[Ki99] V. King, “Fully dynamic algorithms for maintaining all-pairs shortest paths and
transitive closure in digraphs,” Proceedings of the 40th Symposium on Foundations
of Computer Science, IEEE Computer Society, 1999, 81–89.
[Kn97] D. E. Knuth, The Art of Computer Programming. Vol. 1: Fundamental Algo-
rithms, 3rd ed., Addison-Wesley, 1997. (A seminal reference on the subject of algo-
rithms.)
[Kn98] D. E. Knuth, The Art of Computer Programming. Vol. 3: Sorting and Searching,
2nd ed., Addison-Wesley, 1998. (A seminal reference on the subject of algorithms.)
[NaIb92] H. Nagamochi and T. Ibaraki, “Linear time algorithms for ﬁnding a sparse
k-connected spanning subgraph of a k-connected graph”, Algorithmica 7 (1992),
583–596.
[Ra93] M. Rauch, “Fully dynamic graph algorithms and their data structures”, PhD
thesis, Computer Science Department, Princeton University, 1993.
[Ra94] M. Rauch, “Improved data structures for fully dynamic biconnectivity”, Proceed-
ings of the 26th Symposium on Theory of Computing, ACM, 1994, 686–695.
[Ro85] H. Rohnert, “A dynamization of the all-pairs least-cost path problem”, Proceed-
ings of the 2nd Symposium on Theoretical Aspects of Computer Science, Lecture
Notes in Computer Science 182, Springer, 1985, 279–286.
[SlTa83] D. Sleator and R. Tarjan, “A data structure for dynamic trees”, Journal of
Computer and System Sciences 26 (1983), 362–391.
[Ta75] R. Tarjan, “Eﬃciency of a good but not linear set union algorithm”, Journal of
the ACM 22 (1975), 215–225.
[Th89] R. Thurimella, “Techniques for the design of parallel graph algorithms”, PhD
thesis, Computer Science Department, University of Texas, Austin, 1989.

1374
Chapter 18
INFORMATION STRUCTURES
[WeTa92] J. Westbrook and R. Tarjan, “Maintaining bridge-connected and biconnected
components on-line”, Algorithmica 7 (1992), 433–464.
Web Resources:
http://www.cs.stonybrook.edu/~algorith/major section/1.1.shtml
(The Stony
Brook Algorithm Repository: Data Structures, developed by Steven Skiena.)
https://visualgo.net (Animations of many data structures and algorithms.)
https://www.cs.auckland.ac.nz/~jmor159/PLDS210/ds ToC.html (Data Structures
and Algorithms by John Morris.)
https://www.cs.auckland.ac.nz/~jmor159/PLDS210/niemann/s man.htm
(Sorting
and Searching Algorithms: A Cookbook by Thomas Niemann.)
https://www.cs.usfca.edu/~galles/visualization/Algorithms.html (Interactive
animations of data structures and algorithms.)
https://www.hackerearth.com/practice/notes/heaps-and-priority-queues/ (Has
notes on heaps and priority queues.)
https://www.toptal.com/developers/sorting-algorithms
(Animations of various
sorting algorithms.)

19
DATA MINING
19.1 Data Mining Fundamentals
Richard Scherl
19.1.1 Basic Concepts
19.1.2 Measures of Proximity
19.2 Frequent Itemset Mining and Association Rules
Richard Scherl
19.3 Classiﬁcation Methods
Richard Scherl
19.4 Clustering
Daniel Aloise and
19.4.1 Basic Concepts
Pierre Hansen
19.4.2 Types of Clustering
19.4.3 Hierarchical Clustering
19.4.4 Partitioning
19.4.5 Spectral Clustering
19.5 Outlier Detection
Richard Scherl

1376
Chapter 19
DATA MINING
INTRODUCTION
The amount of data generated daily has exploded over the past few decades. This data
is found in digital ﬁles, databases, and on the Internet. Much of the data is publicly
available, but much is privately held by individuals and a wide variety of diﬀerent types
of organizations. With the continuing collection of so much data, attention has grown
in using this data to discover new information. The subject of data mining has rapidly
evolved to meet this need; it is the process of uncovering relationships between elements in
large datasets. Data mining techniques take the input data and output a representation
that expresses information contained in the data or knowledge about the data. Data
mining plays an important role in many diverse disciplines, including retail business,
climate science, bioinformatics, telecommunications, and computer security.
Although data mining is usually considered to be part of computer science, it employs a
wide range of mathematical concepts and techniques. This chapter can be used to learn
about many of the important tasks in data mining, the key mathematical ideas used,
and some of the important algorithms used in data mining.
Data mining has an excellent chance of being useful wherever a large amount of informa-
tion has been collected. For example, data mining is used in banking, marketing, retail
sales, manufacturing, customer relationship management, fraud detection, intrusion de-
tection, healthcare, bioinformatics, criminology, meteorology, and most other disciplines
where large information stores are amassed.
Note. Diﬀerent and sometimes conﬂicting terminology is used in the data mining litera-
ture. Consequently, when consulting data mining literature it is important to determine
the terminology being used. Here, we present the most commonly used terms and men-
tion some synonymous terminology.
GLOSSARY
agglomerative hierarchical clustering: a technique that starts from single-entity
clusters and successively merges clusters until all objects belong to the same cluster.
anomaly: a data point that is signiﬁcantly diﬀerent from other data points and that is
considered abnormal in some way.
anomaly detection: a method for ﬁnding data points that are anomalies (or outliers).
association rule: a rule stating that if some features in a speciﬁed set have particular
values in an instance, then another feature has a high probability to have a speciﬁed
value.
basket: a collection of data items (synonymous with itemset or transaction).
classiﬁcation: the process of constructing a classiﬁer from data (also known as super-
vised learning) and using this classiﬁer to predict the class of new data points.
classiﬁer: a function or model that takes an element of a test set and predicts the class
of that element.
cluster: a set of instances that are similar in some way to each other, but distinct from
points outside the cluster in this same manner.
clustering: the process of partitioning a collection of data points into sets, where the
points in each set are similar to one another in some way and separated from other

GLOSSARY
1377
data points in the same way. (In machine learning, it is also known as unsupervised
learning.)
count: the number of transactions in which an item or set of items occurs.
data mining: the process of analyzing data using mathematical techniques and algo-
rithms to learn new information.
decision tree: a representation as a tree, where each non-leaf node denotes a test on
an attribute, each branch represents an outcome of the test, and each leaf node has
a class label.
dimension: a function on data points. See also feature.
distance measure: a numerical indication of how far apart two instances (points) are.
divisive hierarchical clustering: a technique that starts from a cluster containing all
objects and successively reﬁnes clusters until all objects belong to diﬀerent clusters.
entropy: a measure of the amount of disorder in a set of examples.
feature: a function on data points. See also dimension.
frequent itemset mining: the process of ﬁnding those itemsets that occur in a pre-
deﬁned fraction of the transactions in a speciﬁed set of transactions.
homogeneity (of a cluster): a measure of similarity of the objects in a cluster.
instance: a data point.
item: a data element.
itemset: a collection of data items.
knowledge discovery in databases: the complete process of obtaining useful knowl-
edge from collections of data, of which data mining is a part.
linear model for classiﬁcation: a model in which a line or hyperplane is drawn to
divide the dataset into positive and negative (in the binary case) examples. New
points are classiﬁed depending on which side of the line/hyperplane they fall.
metric space: a set of points together with a real-valued distance function d(x, y) called
a metric that maps pairs of points x, y to the real numbers, where distances cannot
be negative, the distance between two points can only be 0 if the points are the same,
the distance is symmetric, and the distance satisﬁes the triangle inequality.
naive Bayes classiﬁer: a classiﬁer, predicated on the assumption that features are in-
dependent of one another, which predicts that an item belongs to the class producing
the largest probability that this item belongs to it.
nearest neighbor classiﬁer: a classiﬁer in which training instances are stored and a
test example is put into the class of its closest point among the training instances.
outlier: a data point signiﬁcantly diﬀerent from the other data points and considered
to be abnormal in some way.
outlier detection (or anomaly detection): a method for ﬁnding data points that are
outliers (or anomalies).
separation of a cluster: a measure of how diﬀerent objects in a cluster are from those
in diﬀerent clusters.
similarity measure: a numerical indication of the closeness of two instances (points).

1378
Chapter 19
DATA MINING
spectral clustering: a clustering technique that uses the eigenvalues and eigenvectors
of certain graph-based matrices.
support (for a set of items): the fraction of the transactions that contain this set of
items as a subset.
test set: a set of points with class labels, where each point is described by a vector of
feature values.
training set: a set of points with class labels, used by a classiﬁcation method to learn
the classiﬁer.
transaction: a collection of data items (synonymous with itemset and basket).
19.1
DATA MINING FUNDAMENTALS
Diverse techniques can be used in data mining to derive useful information from large
sets of data. Nevertheless, a large percentage of data mining involves solving a few im-
portant types of problems. These problems are: ﬁnding relationships between variables
in databases (solved by constructing association rules, covered in §19.2), assigning cate-
gories to data items (solved by classifying items, covered in §19.3), partitioning datasets
into meaningful categories (solved by creating clusters of items, covered in §19.4), and
ﬁnding data points that do not conform to an expected pattern (solved using outlier
or anomaly detection, covered in §19.5). Comprehensive coverage of data mining, dis-
cussing the ﬁve areas listed here as well as many other problems, can be found in [Ag15],
[HaKaPe12], [LeRaUl14], and [MaRo10].
19.1.1
BASIC CONCEPTS
Deﬁnitions:
An association rule is a rule stating that if some features have particular values in an
instance, then another feature has a high probability to have a speciﬁed value.
Big data is the name used to describe datasets that are so large or so complex that
conventional software tools for working with them are inadequate.
A classiﬁer is a function that can be used to distinguish a particular class of data from
other data.
Classiﬁcation is the process of constructing a classiﬁer from the data. (In machine
learning, classiﬁcation is also known as supervised learning.)
Clustering is the process of partitioning the data points into groups of data points,
where the points in each group are similar to one another in some way and separated
from other data points in the same way.
(In machine learning, it is also known as
unsupervised learning.)
A cluster is a group of instances that are similar in some way to each other, but distinct
from points outside the cluster in the same way.
Data mining is the discipline that employs mathematical techniques and algorithms to
build models and analyze large collections of data in order to obtain useful information.

Section 19.1
DATA MINING FUNDAMENTALS
1379
A dimension or feature is a function on data points.
An instance is a data point.
The process of obtaining useful knowledge from collections of data is called Knowledge
Discovery in Databases (KDD).
An outlier (or anomaly) is a data point that so signiﬁcantly diﬀers from the other data
points that it is considered to be abnormal in some way.
Outlier detection or anomaly detection is a method for determining that a data
point is an outlier. It can be seen as complementary to clustering.
A relational database consists of a collection of tables, each consisting of a set of
attributes (or ﬁelds) and containing a set of records, where each record is an n-tuple for
some positive integer n.
Facts:
1. Data mining techniques are used within the larger process known as Knowledge
Discovery in Databases (KDD) (see [FaPiSm96] and [MaRo10]), which involves other
aspects of computing.
2. The goal of KDD is to obtain useful knowledge from stored data. The steps of KDD
(as described in [FaPiSm96]) are
• Modeling the application domain, the relevant prior knowledge, and the goals.
• Creating a target dataset by focusing on speciﬁc data samples and features.
• Carrying out data cleaning and preprocessing by removing noise and outliers,
collecting the necessary information to account for noise, and deciding how to
handle missing data ﬁelds.
• Performing data reduction and projection by ﬁnding useful features to represent
the data (depending on the goals of the process) and using dimensionality re-
duction methods to reduce the number of features.
• Choosing the particular data mining method, such as classiﬁcation, clustering,
constructing association methods, and so on.
• Choosing the particular data mining algorithm(s) and deciding on particular pa-
rameters.
• Performing the data mining task and deriving the representations that are output
by the chosen method.
• Interpreting the mined patterns.
• Assimilating the discovered knowledge.
3. Data mining generally refers to the modeling and analysis steps of KDD, and not
the other steps. In particular, data mining focuses on building mathematical models,
selecting the appropriate data mining tasks, applying the appropriate algorithms, and
generating and interpreting the output of these algorithms.
4. Special methods are often needed to handle big data, that is, large datasets that
cannot be handled in the usual way on computers because of their massive size (ranging
from terabytes to petabytes). Also, the number of dimensions or features may be so large
that it is diﬃcult to derive knowledge about the data. For discussions of these issues see
[LeRaUl14].

1380
Chapter 19
DATA MINING
5. Data mining can be used to analyze large collections of a wide range of data types.
In this chapter the focus is on mining data that is stored in ﬂat ﬁles (containing numbers
or text) or in relational databases.
6. Techniques for mining data not stored in ﬂat ﬁles or rational data bases, such as data
from multimedia databases, spatial databases, time-series databases, and the Web, are
not addressed in this chapter. (See [MaRo10] for information about data mining of these
and other types of specialized data collections.)
7. Following [Ag15], we can abstractly view the data mining problem by considering the
data being in the form of an n × d data matrix D, where n is the number of records and
d is the number of columns (or dimensions).
• Relationships between columns. Finding frequent relationships between columns
is the association rule mining problem. If a particular column is identiﬁed as
being more important (i.e., a class), then the problem of data classiﬁcation
is that of determining the relationships between the other columns and this
column.
• Relationships between rows. Finding those subsets of rows in which the values of
the columns are similar is clustering. Finding those rows in which the values of
the columns are very diﬀerent from the values of the columns in other rows is
outlier analysis, and the row with the diﬀerent values is an anomaly.
8. The Iris dataset, created by the statistician Sir Ronald Fisher in 1936, is commonly
used to illustrate methods of data mining and machine learning. This dataset contains
150 instances, encompassing three classes of ﬂowers: Iris setosa, Iris versicolor, and Iris
virginica. The data includes four numerical attributes (sepal length, sepal width, petal
length, and petal width), measured in centimeters.
9. Singular Value Decomposition is used in a wide variety of data mining tasks, including
classiﬁcation and clustering, two of the key tasks of data mining. It is also useful in
developing recommender systems. (See §6.7.4 for this and other applications.)
10. There are many diverse data mining software packages available, including both
public domain and commercial packages. Public domain packages include
• Weka, written in Java and developed at the University of Waikato in New
Zealand (http://www.cs.waikato.ac.nz/ml/weka/)
• R, a free software platform for statistical computing that can be extended with
many diﬀerent packages (https://www.r-project.org/)
• Rapid Miner, an environment for machine learning and data mining processes
(https://rapidminer.com/)
• ELKI, open-source data mining software in Java that specializes in classiﬁca-
tion and outlier detection (https://elki-project.github.io/)
Commercial software packages for data mining include
• SAS Enterprise Modeler (http://support.sas.com/software/products/miner/)
• IBM SPSS Modeler (https://www.ibm.com/us-en/marketplace/spss-modeler)
• MATLAB toolboxes with data mining capabilities (https://www.mathworks.com/
products.html)
Consult the later sections of this chapter for more information about these and other
more specialized software tools for data mining tasks.

Section 19.1
DATA MINING FUNDAMENTALS
1381
Examples:
1. Financial accesses, such as using a credit or debit card, create data that can be mined
to learn interesting information about spending and buying behavior.
2. Telephone and cell phone usage data records can be mined to obtain many types of
useful information, such as ﬁnding groups of people who communicate with one another.
3. Many on-line retailers mine customer purchase records to recommend other products
for customers to buy.
For example, two customers can be considered similar if the
purchase sets of the two customers have a large similarity measure (§19.1.2). Likewise, we
can consider two products to be similar if the two sets of customers who have purchased
these items have a large similarity measure.
4. The Netﬂix Challenge was a competition to develop an algorithm to predict a person’s
ratings for ﬁlms based upon his/her ratings for other ﬁlms. Other similar problems are
product recommendations made by on-line retailers such as Amazon.com, as well as the
recommendation of news articles to a reader based on articles they have read previously.
5. Classiﬁcation may be used to develop a method of determining which loan applica-
tions are acceptable and which are too risky.
6. Association rules may be used by a retail establishment to determine which products
to suggest to a customer who has just purchased a product. If the customer has just
bought a PC and a digital camera, perhaps he/she might buy a memory card [HaKaPe12].
7. Outlier analysis can be used by a credit card company to determine if a transaction
is likely to be due to fraud. For example, the purchase may be in a city or a type of store
not usual for that particular customer.
8. Records of which items are commonly bought together can help a large retailer with
product placement and advertising.
9. The database of adverse drug reactions has been mined to ﬁnd new signals of adverse
reactions to drugs.
10. When users access Web services, access logs are created at servers and various cluster
proﬁles are maintained at commercial Web sites. These logs can be mined to understand
patterns of user accesses, or unusual patterns indicating criminal behavior.
11. Records of meteorological data have been mined to learn predictors of conditions
like temperature and rainfall.
19.1.2
Measures of Proximity
In data mining, models not only include collections of data, but also speciﬁc ways to
determine how similar diﬀerent data points are or how far apart they are. We describe
here some of the most important of these measures of proximity.
They are used in
building models and in classiﬁcation, clustering, and outlier detection.
Deﬁnitions:
A distance measure or (dissimilarity measure) is a numerical indication of how far
apart two instances (points) are. High values indicate that the instances (points) are
diﬀerent, whereas low values indicate that they are similar.
A similarity measure is a numerical indication of the closeness of two instances (points).
High values indicate that the two instances (points) are similar, whereas low values in-
dicate that they are diﬀerent.

1382
Chapter 19
DATA MINING
A metric space consists of a set of points and a real-valued function d(x, y), called a
metric, mapping pairs of points to the real numbers and satisfying for all points x, y, z
• d(x, y) ≥0 (distances cannot be negative);
• d(x, y) = 0 if and only if x = y (distance can only be 0 if the two points are the
same);
• d(x, y) = d(y, x) (symmetry);
• d(x, y) ≤d(x, z) + d(z, y) (triangle inequality).
Euclidean n-space is the set of n-tuples of real numbers.
The Euclidean distance between x = (x1, x2, . . . , xn) and y = (y1, y2, . . . , yn) is given
by d2(x, y) = (Pn
i=1(xi −yi)2)
1/2.
The Manhattan distance between x = (x1, x2, . . . , xn) and y = (y1, y2, . . . , yn) is given
by d1(x, y) = Pn
i=1 |xi −yi|.
The Chebyshev distance (or supremum distance) between x = (x1, x2, . . . , xn) and
y = (y1, y2, . . . , yn) is given by d∞(x, y) = maxn
i=1 |xi −yi|.
More generally, whenever r is a real number with r ≥1, the distance measure dr(x, y) =
(Pn
i=1 |xi −yi|r)1/r is a metric, known as the Minkowski distance.
The Hamming distance between two vectors is the number of components in which
the vectors diﬀer.
The cosine similarity of the n-tuples x = (x1, x2, . . . , xn) and y = (y1, y2, . . . , yn) is
x · y
∥x ∥∥y ∥.
The cosine distance of the n-tuples x = (x1, x2, . . . , xn) and y = (y1, y2, . . . , yn) is the
arccosine of their cosine similarity.
The Jaccard similarity of the sets A and B equals |A ∩B|/|A ∪B|.
The Jaccard distance between two sets A and B is deﬁned to be d(A, B) = 1 −
|A ∩B|/|A ∪B|.
The edit distance between the strings x and y is the smallest number of insertions
and/or deletions of single characters that can be used to convert string x into string y.
The term vector of a document containing n diﬀerent words w1, w2, . . . , wn is the n-
tuple (f1, f2, . . . , fn), where fi is the number of occurrences of word wi in the document.
Facts:
1. In data mining, a wide variety of diﬀerent distance measures and metrics are used.
These measure the distance between real numbers, sets, strings, and other objects.
2. A metric is a distance measure, but not all distance measures are metrics.
3. Among the proximity measures commonly used in data mining are Euclidean dis-
tance, cosine similarity and distance, Manhattan distance, and supremum distance.
4. The Euclidean distance and Manhattan distance are special cases of the Minkowski
metric with r = 2 and r = 1, respectively.
5. Minkowski distance with parameter r is also called the ℓr distance (based on the ℓr
norm). Euclidean distance, Chebyshev distance, and Manhattan distance correspond to
ℓ2, ℓ∞, and ℓ1, respectively.

Section 19.1
DATA MINING FUNDAMENTALS
1383
6. From real analysis it follows that limr→∞dr(x, y) = maxn
i=1 |xi −yi|, which is the
Chebyshev distance between x and y. This implies that Chebyshev distance is a metric,
but it is easier to prove this directly from the deﬁnition.
7. Chebyshev distance is also known as chessboard distance, because in chess the mini-
mum number of moves needed by a king to go from one square on a chessboard to another
is the Chebyshev distance between the centers of the squares.
8. The cosine similarity of two vectors x and y is the cosine of the angle θ between them.
Here, the vectors have real numbers as entries. In data mining, vectors may have some
or all entries that only take only a ﬁnite number of values. For instance, some entries
may only take on the values 0 and 1.
9. The cosine distance is used to measure the angle between vectors.
10. The Jaccard similarity of two sets is the ratio of the size of their intersection to the
size of their union.
11. Jaccard distance is a metric on every space of sets [LeRaUl14].
12. On-line retailers can use customer purchase records to recommend other products
for customers to buy. For example, two customers can be considered similar if their
purchase sets have large Jaccard similarity. Likewise, two products can be considered
similar if the two sets of customers who have purchased these items have large Jaccard
similarity.
13. The similarity of two documents can be measured by ﬁnding the cosine similarity
of their term vectors.
14. The Jaccard similarity and Jaccard distance between two documents depends on
the particular sets we associate with these documents. The most common choices are
the set of all words a document contains or the set of all strings of length k it contains
(called k-shingles or k-grams), where k is a positive integer selected large enough so that
the probability that any given shingle appearing in the document is low.
15. The edit distance d(x, y), where x and y are two strings, equals l(x) + l(y) −
l(LCS(x, y)), where l(s) denotes the length of the string s and LCS(x, y) is the longest
common subsequence of x and y.
16. Hamming distance is a metric on every space of vectors.
17. The Manhattan distance gets its name because it measures the distance required
to walk between two addresses in the part of Manhattan (in New York City) where all
streets run either north-south or east-west. It is also called taxicab distance.
Examples:
1. Let S = {2, 4, 5, 6, 8} and T = {1, 3, 4, 5, 7}. Because S ∩T is {4, 5} and S ∪T is {1,
2, 3, 4, 5, 6, 7, 8}, the Jaccard similarity of S and T is |S ∩T |/|S ∪T | = 2/8 = 0.25.
The Jaccard distance is 1 −S ∩T |/|S ∪T | = 1 −0.25 = 0.75.
2. Jaccard distance can be used to measure the distance between documents. There
are a variety of ways to associate sets with documents. For instance, documents can be
viewed as sets of words. They can also be viewed as collections of k-shingles or k-grams,
where k speciﬁes the length of the character sequence.
3. Here we ﬁnd the similarity of and the distance between the vectors x = (2, 4, −1) and
y = (4, 2, 1) using a variety of measures and metrics.
• The Hamming distance between x and y is 3, because x and y diﬀer in all three
components.

1384
Chapter 19
DATA MINING
• The Manhattan distance between x and y is |2−4|+|4−2|+|−1−1| = 2+2+2 = 6
and the Chebyshev distance between them is 2.
• The cosine similarity of x and y is
8 + 8 −1
p
22 + 42 + (−1)2 ·
√
42 + 22 + 12 =
17
√
21 ·
√
21 = 17
21 ≈0.8095.
• The cosine distance between x and y is arccos( 17
21) ≈35.95o.
4. The edit distance between the strings x = dcba and y = fcab is 4 because one can
convert x to y by ﬁrst deleting the d, then inserting an f before the c, then deleting the
b, and ﬁnally inserting a b after the a. There is no way to change x into y using only
three insertions and/or deletions.
19.2
FREQUENT ITEMSET MINING AND ASSOCIATION RULES
The mining of association rules is one of the classic areas of data mining. It is based
on one of the more fundamental methods for characterizing data, the ﬁnding of frequent
itemsets. There are many areas of application besides the original retail application.
Deﬁnitions:
In this area of data mining, an element of the dataset is called an item.
A collection of items is known as an itemset (or basket or transaction). The items
are objects of some sort from a universe U. The term k-itemset is used to denote an
itemset that contains exactly k items.
A transaction is an n-tuple of items for some positive integer n.
The data to be analyzed consists of a set T of transactions (or itemsets or market-
baskets).
The number of transactions in which an item I or set of items occurs is called the count
of I, deﬁned as
count(I) = |{ti | I ⊆ti, ti ∈T }|,
where T = {t1, t2, . . . , tn} is the set of transactions. [Sometimes the word “support” is
used for this quantity, conﬂicting with the more common usage of this word given in the
next deﬁnition.]
The support for an item or set of items I is the fraction of the transactions that contain
I as a subset. That is,
support(I) = count(I)
|T |
.
Note that the support of I is the probability of I occurring in a transaction.
If an item or set of items I occurs in more than s percent of the transactions, where s is
called the support threshold, I is said to be frequent. [For those authors who treat
“support” as an absolute number, s is an absolute number as well.]
Suppose S is a set of items. An association rule is an implication of the form I →J,
where I and J are disjoint subsets of S.

Section 19.2
FREQUENT ITEMSETS AND ASSOCIATION RULES
1385
Given a set of transactions T = {t1, t2, . . . , tn}, frequent itemset mining is the process
of ﬁnding those itemsets that occur in a predeﬁned fraction m of the transactions in T .
The predeﬁned fraction m is called the minimum support (or minimum support
threshold).
The conﬁdence of an association rule I →J is the support for I ∪J divided by the
support for I. That is,
conﬁdence(I →J) = support(I ∪J)
support(I)
.
The support of an association rule I →J is the support for I ∪J. That is,
support(I →J) = support(I ∪J).
The interest [LeRaUl14] of an association rule I →J is the diﬀerence between the
conﬁdence of the rule and the support of J. That is,
interest(I →J) = conﬁdence(I →J) −support(J).
The lift of an association rule I →J is the conﬁdence of the rule divided by the support
of J. That is,
lift(I →J) = conﬁdence(I →J)
support(J)
.
Facts:
1. The original application of mining the frequency of itemsets to ﬁnd association rules
analyzed grocery or department store data. This is the origin of the market-basket model
of data, which is based on baskets of items. Typically the number of baskets is very large
and they are stored in a ﬁle containing a sequence of baskets. The number of items in a
basket is much smaller than the total number of possible items.
2. The folklore is that one of of the earliest discoveries made from market-basket data
analysis is that people who buy diapers tend to be likely to buy beer. The discovered
rule is then diapers →beer.
3. The support of every subset J of an itemset I is at least that of the support of itemset
I. This property is called the support monotonicity property.
4. If a set I of items is frequent, then so is every subset of I. This property is called
monotonicity or the downward closure property.
5. Rules with high conﬁdence are useful, but conﬁdence does not require a connection
between itemsets I and J. The conﬁdence of the rule expresses the conditional probability
Pr(J | I).
6. Interest expresses the type of connection between I and J. If the percentage of the
transactions with I and also J is the same as the percentage with J alone, then there
is no connection and the interest value will be 0. A positive number indicates a positive
connection and a negative one indicates that the occurrence of I makes it less likely that
J occurs.
7. If the lift of the association rule I →J is greater than 1, then I and J are positively
correlated. If the lift is 1, then I and J are independent and the rule is not interesting.
If the lift is below 1, then there is a negative correlation.

1386
Chapter 19
DATA MINING
8. The best-known algorithm for counting frequent subsets is the apriori algorithm in-
troduced by C. Agrawal and R. Srikant in 1994.
Given a set of transactions and a
minimum support threshold minsup, this algorithm ﬁnds the itemsets that are subsets
containing at least minsup transactions in the database using a bottom-up approach.
Frequent subsets are extended, an item at a time (candidate generation), and groups of
candidates are tested to determine those transactions with successively larger numbers
of elements. The algorithm terminates when no additional extensions exist. Details of
this algorithm can be found in [Ag15].
9. If there is a large number of items, the space needed to count the itemsets can
become prohibitive.
For 2-itemsets in a universe of 1000 objects, the number of 2-
itemsets is
 1000
2

= 499,500.
If we want to consider 3-itemsets, then an additional
 1000
3

= 166,167,000 possibilities must be included.
There are several techniques to
reduce the amount of space needed to store the counts of itemsets [LeRaUl14].
10. Generally, the size of the ﬁle containing the transactions is very large and does not
ﬁt in main memory. An important issue is reducing the number of times it is necessary
to read the ﬁle containing the transactions.
11. Another well-known method for obtaining frequent itemsets is called frequent pat-
tern growth or FP-growth. The ﬁrst scan of the database counts the frequent pairs as
in the apriori algorithm. The second pass builds the FP-tree. Further mining is done on
the FP-tree and no further scans over the transaction database are needed. Discussion
of this method as well as others can be found in [Ag15], [HaKaPe12], and [TaStKu06].
12. A transaction can be a document or a web page. The items are the words that
occur in the document. Assuming that we ignore frequently occurring words (known as
stop words), then frequent itemsets would represent words that often occur together in
documents.
13. Transactions can be sentences and the items can represent the documents in which
the sentence occurs. Frequent itemsets are then documents that share common sentences.
14. Many measures have been proposed to evaluate whether rules are interesting or
useful. Discussion of these various options can be found in [Ag15], [HaKaPe12], and
[TaStKu06]. Generally an association rule is considered to be interesting if it satisﬁes
both a minimum support threshold and a minimum conﬁdence threshold, where these
threshold values are set by the user.
15. Following are some free software tools for ﬁnding association rules and mining fre-
quent itemsets:
• arules, an R extension package (https://cran.r-project.org/web/packages/
arules/index.html)
• ARtool, a collection of algorithms and tools for the mining of association rules
in binary databases (http://www.cs.umb.edu/~laur/ARtool/index.html)
• FIMI (Frequent Item Mining Implementations) site (http://fimi.ua.ac.be/)
• opusminer, an R package providing an interface to OPUS Miner for ﬁnding key
associations in transaction data (https://cran.r-project.org/web/packages/
opusminer/index.html)
Examples:
1. Suppose the association rule iPhone 8 →iPad Pro for transactions made at an Apple
Store has support 3% and conﬁdence 15%. Then in 3% of all transactions at the store
an iPhone 8 and an iPad Pro were purchased (perhaps with other items), and 15% of
the customers who bought an iPhone 8 at the Apple Store also bought an iPad Pro at
this store.

Section 19.3
CLASSIFICATION METHODS
1387
2. Consider the following supermarket transactions from one hour at the Healthy Eating
Emporium:
Transaction ID
Items
1
{Keﬁr, Tortillas}
2
{Cone Filters, Keﬁr, Kombucha, Tortillas, Omega-3}
3
{Almonds, Apples, Cone Filters, Keﬁr, Soy Butter}
4
{Tortillas, Coconut Milk, Cone Filters, Keﬁr, Kombucha, Tortillas}
5
{Cone Filters, Tortillas, Keﬁr, Kombucha, Omega-3}
The count for {Tortillas} is 4, and therefore the support is 4/5. The count for {Keﬁr,
Kombucha} is 3, and therefore the support is 3/5. The support for {Kombucha, Keﬁr} →
{Omega-3} is 2/5 and the conﬁdence is 2/5
3/5 = 2/3. Since the support for Omega-3 is
2/5, the rule is somewhat interesting as the interest is 2/3 −2/5 = 4/15.
3. Frequent itemset analysis can be used for applications that do not involve market-
basket analysis. Some examples [LeRaUl14] are the following:
• The items consist of biomarkers (e.g., genes or blood proteins) and diseases. Each
basket represents data about a patient, e.g., genome, blood-chemistry analysis,
and medical history with regard to diseases. A frequent itemset that contains
a disease and one or more biomarkers provides a possible test for the disease.
• The items are documents and the baskets are sentences. An item is in a basket if
the sentence represented by the basket occurs in that document. Pairs of items
that occur in several baskets provide an indication that those documents share
several sentences and can be evidence of plagiarism.
• Documents are represented by baskets and the items are the words that occur
in the document. If the most common words (stop words) are ignored, then
frequent pairs may indicate a joint concept or related concepts.
19.3
CLASSIFICATION METHODS
Classiﬁcation is the problem of identifying the class or category of a data item given a
dataset of examples that are already identiﬁed as belonging to particular classes. A new
example can be given to the classiﬁer and the classiﬁer will predict the category of the
new example.
Deﬁnitions:
Items that are to be classiﬁed are m-tuples, where the diﬀerent dimensions are called
features. Diﬀerent features can belong to diﬀerent sets, such as the set of real numbers,
the set of positive integers, or some ﬁnite set. These items are also called instances or
data points.
The goal of classiﬁcation is to create a model that will specify (or predict) the class
label that identiﬁes the class each new data item belongs to.

1388
Chapter 19
DATA MINING
The training set given to a classiﬁcation method is a set of items used by the classiﬁca-
tion method to learn (or create) the classiﬁer. Each point or example in the training set
is an m-tuple (x1, x2, . . . , xm) of feature values, where each xi is a value or measurement
for the corresponding feature Fi, except for one value xi, which is the class of that item.
The test set is a set of items, each described by a vector of feature values, given to a
classiﬁer that predicts the class of each of these items.
The classiﬁer is a model that takes an item Xj of the test set and predicts the class of
that point.
The instance space is the m-dimensional space containing the items to be classiﬁed.
A nearest neighbor classiﬁer is a classiﬁer that assigns the class of the nearest item
to this item using a speciﬁed metric.
A k-nearest neighbor classiﬁer ﬁnds the classes of the k nearest neighbors to an
item (using a speciﬁed metric) and assigns to the item the class a majority of these
items belong to. Here the parameter k is chosen so that the classiﬁer selects the most
appropriate classes to the items.
A perceptron is a threshold function applied to the n-tuple (x1, x2, . . . , xn), where each
xi is a real number. A perceptron with weights w1, w2, . . . , wn and threshold value Θ
outputs 1 if Pn
i=1 wixi ≥Θ and gives −1 otherwise.
A dataset is linearly separable given a perceptron if there is a hyperplane that separates
all the points for which a perceptron has value 1 from those where it has value −1.
A linear model for classiﬁcation draws a line or hyperplane dividing the dataset into
positive and negative examples (in the binary case). New points are classiﬁed depending
on which side of the line/hyperplane they fall.
An decision tree represents a concept using a tree structure, where each non-leaf node
denotes a test on an attribute, each branch represents an outcome of the test, and each
leaf node has a class label.
A naive Bayes classiﬁer provides the probability that an item is a member of a par-
ticular class, assuming that the features occur independently of one another.
Suppose that D is a partition of the classiﬁed training items, where there are v diﬀerent
classes C1, C2, . . . , Cv. Let Ci,D be the set of data items of Ci that belong to D. Then
the entropy or expected information required to classify an item in D is
Entropy(D) = −
v
X
i=1
|Ci,D|/|D| log2{|Ci,D|/|D|}.
Suppose that D is a partition of the classiﬁed training items and D is partitioned using an
attribute A with m distinct values a1, a2, . . . , am into subsets D1, D2, . . . Dm, where Di
contains the items in D for which the value of A is ai. Then the entropy or expected
information of D with partition A is
EntropyA(D) =
m
X
i=1
|Di|Entropy(Di)/|D|.
The information gain obtained by a partition using an attribute A of a set D of
classiﬁed training items is Gain(A) = Entropy(D) −EntropyA(D).

Section 19.3
CLASSIFICATION METHODS
1389
Facts:
1. Classiﬁcation is also called supervised learning (in the parlance of machine learning)
because the training set includes the classes of the items in the training set and a new item
is classiﬁed using a classiﬁer built from this training set. This contrasts with clustering
(discussed in §19.4), where the classes of the items in the training set are not known in
advance.
2. Many diﬀerent classiﬁcation methods are used, including the nearest neighbor clas-
siﬁer, the k-nearest neighbor classiﬁer, perceptrons, linear classiﬁers, support vector ma-
chines, decision tree learning classiﬁers, naive Bayesian classiﬁers, and neural network
classiﬁers. However, many other classiﬁcation methods have been invented and new ones
are being developed with some regularity.
3. A perceptron can be used as a classiﬁer only if the data is linearly separable.
4. Perceptrons can be used for binary classiﬁcation, that is, when each feature takes on
one of two values (such as TRUE and FALSE, 1 and −1, and so on). They can be used
even when there is an extremely large number of features, such as for the classiﬁcation
of documents where there is a feature for each word in a dictionary that takes the value
1 if the word appears in the document and the value −1 if it does not.
5. In nearest neighbor and k-nearest neighbor classiﬁers, when all the values of items
in all dimensions are numerical, Euclidean distance is usually used. However, for certain
other applications alternative metrics might be preferable. When all values are discrete,
the Hamming distance can be used.
6. The choice of k for a k-nearest neighbor classiﬁer depends on the particular type of
data. However, the general idea is to make k small enough so that only relatively nearby
items are among the closest k items to new examples, but large enough to minimize
the eﬀects of data items that may have resulted from measurement error or noise. Some
experts suggest that k should equal the square root of the number of items in the training
set.
7. The nearest neighbor classiﬁer produces a Voronoi tessellation (§13.5.3) of the in-
stance space. The cell of a new example determines its class.
8. When a nearest neighbor or a k-nearest neighbor classiﬁer is used, it is important to
scale or normalize the units in each dimension. Otherwise, dimensions where the values
fall in wider ranges will dominate and dimensions where values are closer together will
have little eﬀect on the results of the classiﬁer.
9. Consider the case of deciding whether or not a point x = (x1, x2, . . . , xn) is a member
of some category C. In a linear classiﬁer, prediction is based on a weighted sum, where
each feature has a weight wi, giving
f(x) =
n
X
i=1
wixi.
If f(x) is positive then the point x is a positive example of C, otherwise its class is not
C. Typically the vectors x are normalized so that ∥x ∥= 1. The vector w can be seen
as deﬁning a hyperplane separating the positive and negative examples of C.
10. A support vector machine (SVM) uses a nonlinear mapping (if necessary) to trans-
form the training set into a higher dimension where the data is linearly separable. Here we
can search for an optimal separating hyperplane, which operates as a decision boundary
that separates the points of the two classes. The optimality of the separating hyperplane
is in the sense that it is a maximal margin hyperplane, one that separates the points
with the largest margin so that none of the data points lie within this margin of the

1390
Chapter 19
DATA MINING
hyperplane. The margin of a hyperplane is the distance from this hyperplane to the data
points closest to it. We describe part of the process used by SVM in Fact 11. Full details
can be found in [Ag15] and [HaKaPe12].
11. The goal of the SVM process is to ﬁnd a hyperplane w · x + b = 0 that not only
separates the data points, but also has the maximum margin γ. The following ﬁgure
illustrates the diﬀerence that choice of the decision line or hyperplane can make on the
margin. The core of the SVM optimization process ensures that this margin γ is as large
as possible.
+
+
+
+
+
+
+
+
12. A decision tree can be built in various ways using a greedy algorithm that uses a top-
down recursive divide-and-conquer approach. The training set is recursively partitioned
into successively smaller subsets during the construction of the tree. At the end of the
process, the tree that has been constructed contains both decision nodes and leaf nodes.
The decision nodes represent steps where sets are partitioned into smaller subsets, while
leaf nodes represent classiﬁcations of items.
13. The ID3 algorithm, developed by J. R. Quinlan, uses the attribute with the highest
information gain to drive a greedy algorithm used to build a decision tree. At a splitting
step of the algorithm, the attribute with the highest information gain is selected as the
splitting node.
14. To illustrate the process of creating a decision tree, we consider a simple scenario.
The recursive decision tree learning algorithm (Algorithm 1) begins at the root and
successively works downwards.
Algorithm 1:
Decision Tree Learning.
Decision-Tree-Learning(Examples, Attributes)
if Examples is empty, return the leaf node labeled with the most frequent
categorization in the example set
create a new node for the tree
if all examples are positive, return the node as a leaf node labeled with yes
if all examples are negative, return the node as a leaf node labeled with no
if Attributes is empty, return the leaf node labeled with the most frequent
categorization in the example set
else select the attribute A from Attributes that best classiﬁes the examples
and make A the label for node
for each value vi of A
add a new branch below node with the branch label of vi

Section 19.3
CLASSIFICATION METHODS
1391
set Examplesvi to the portions of Examples that have vi for attribute A
add to this branch the subtree
Decision-Tree-Learning(Examplesvi, Attributes−A)
15. Algorithm 1 at each call creates a node (the outermost call creating the root). Its
arguments are a set of examples and the attributes by which the examples are categorized
(the outermost call being given the complete set of examples and attributes). As the
recursion proceeds, the sets Examples and Attributes become smaller.
At each call, a determination is made with regard to how to label the node created in
that call. If all of the examples are of the same category, then the node is a leaf and is
labeled with either Yes or No. If the set of examples is empty or if the attributes are
empty, the algorithm with this training set has no basis upon which it can categorize
instances corresponding to this path in the tree. The node therefore must be a leaf and
by default it is given the most frequent label.
Otherwise, at each step the algorithm needs to determine which is the best attribute to
use as the label for a node. Once the attribute for a node is chosen, there will then be as
many branches below that node as there are values for the chosen attribute. The idea is
to prefer smaller trees. Therefore at each step the best attribute is the one that classiﬁes
the most examples. This can be given a mathematically precise deﬁnition by using the
notions of entropy and information gain.
16. Locality sensitive hashing can be used to ﬁnd nearest neighbors in near constant
time [LeRaUl14].
17. Classiﬁcation can also be done using neural networks, which are loosely based on a
biological model. In a neural network there are layers of nodes and weights between the
nodes. A perceptron is the simplest type of neural network. More expressive learning
methods use networks with hidden layers of nodes and weights between the input and
output layers. Backpropagation is used to train the network [LeRaUl14], [HaKaPe12].
18. Many software packages, including those mentioned in §19.1.1, support a wide va-
riety of classiﬁcation algorithms. Some packages that can be used without charge are
listed next:
• ELKI is open-source data mining software written in Java with an emphasis on
unsupervised methods in cluster analysis (https://elki-project.github.io/)
• SGI MLC++ provides open-source code written in C++ at Stanford Univer-
sity for machine learning (classiﬁcation), including implementations of the ID3
algorithm for decision trees and algorithms for nearest neighbor algorithms
(http://www.sgi.com/tech/mlc/)
•
The Scikit-learn open-source toolkit contains many classiﬁcation methods in-
cluding those discussed here. They are all implemented in Python. Scikit-learn
is included with both the Anaconda Continuum and the Enthought Canopy
distributions of Python packages (http://scikit-learn.org/stable/)
Examples:
1. Classiﬁcation can be used to determine whether an e-mail is spam, using two classes
for e-mails: spam and nonspam. Features that can be used are whether there are punc-
tuation errors in e-mail headers, whether certain suspect keywords are present, whether
nonsense strings are found, and so on.

1392
Chapter 19
DATA MINING
2. Classiﬁcation can help determine whether a patient has a particular disease using as
features the symptoms and medical data of this patient.
3. Categorizing documents by primary topic using keyword counts as features can be
done via classiﬁcation.
4. People can be categorized into the type of vacation they may be interested in based
on personal data including age, income, age of children, whether they have visited certain
places, and so on.
5. Optical character recognition can classify each written character into the letter it
represents by looking at various features.
6. Classifying videos can be done to determine whether a new example depicts a par-
ticular activity.
7. We can use two dimensions of data from the Iris database, sepalwidth and petalwidth,
to classify new instances using the nearest neighbor classiﬁer. The training set we use
includes three instances, with the following known values for (sepalwidth, petalwidth,
class).
Sample 1:
(3.4, 0.4, Iris setosa)
Sample 2:
(2.7, 1.3, Iris versicolor)
Sample 3:
(3.0, 2.1, Iris virginica)
We classify a new instance (3.8, 2.0) by computing the Euclidean distance between this
new instance and each of the three classiﬁed instances (using just the ﬁrst two dimen-
sions), obtaining 1.65 for Sample 1, 1.30 for Sample 2, and 0.81 for Sample 3. Because
the third instance is the closest, the new sample is classiﬁed as Iris virginica.
8. The days when a particular person will play an outdoor sport (such as golf) based
on the weather can be classiﬁed using a decision tree (as done in [Qu93],[Mi97], and
[WiFrHa11]). A possible decision tree for this problem is displayed next. To explore the
decision tree, one starts at the root and follows the values for each attribute of the day
to be classiﬁed until arriving at a leaf. The classiﬁcation is then the label of this leaf.
For example, if the Outlook for today is Sunny and the Humidity is High, then golf will
not be played. But if the Outlook for today is Cloudy then golf will be played.
9. To illustrate the method of constructing decision trees, consider the training examples
shown in the following table. Because there are nine positive examples and ﬁve negative
examples, it follows that Entropy(S) = −(9/14)·log2(9/14)−(5/14)·log2(5/14) = 0.940.
Now, consider the options for choosing the ﬁrst attribute for the decision tree of whether
golf will be played based on the given data. The information gain values for the attributes
are computed as follows:

Section 19.3
CLASSIFICATION METHODS
1393
Gain(S, Outlook) = 0.246
Gain(S, Humidity) = 0.151
Gain(S, Wind) = 0.048
Gain(S, Temperature) = 0.246
Day
Outlook
Temperature
Humidity
Wind
Play
Ex1
Sunny
Hot
High
Weak
No
Ex2
Sunny
Hot
High
Strong
No
Ex3
Cloudy
Hot
High
Weak
Yes
Ex4
Rain
Mild
High
Weak
Yes
Ex5
Rain
Cool
Normal
Weak
Yes
Ex6
Rain
Cool
Normal
Strong
No
Ex7
Cloudy
Cool
Normal
Strong
Yes
Ex8
Sunny
Mild
High
Weak
No
Ex9
Sunny
Cool
Normal
Weak
Yes
Ex10
Rain
Mild
Normal
Weak
Yes
Ex11
Sunny
Mild
Normal
Strong
Yes
Ex12
Cloudy
Mild
High
Strong
Yes
Ex13
Cloudy
Hot
Normal
Weak
Yes
Ex14
Rain
Mild
High
Strong
No
We see that Outlook gives us the highest information gain. Splitting on Outlook gives us
three branches. The instances where the value equals Cloudy are all decided as positive.
Of those with the value Sunny, 3 out of 5 are negative; of those with the value Rain, 2 out
of 5 are negative. If we were to choose Wind, then there would be two branches. Those
instances under the Weak branch give 6 out of 8 positive while those under the Strong
branch give 3 out of 6 positive. The value of information gain does roughly correspond
with our intuitions of how well the choice of attribute serves to categorize the instances.
10. Consider the problem of using the naive Bayes classiﬁer to classify a new instance
of the weather data above [Mi97], [WiFrHa11]. For this instance X, Outlook = Sunny,
Temperature = Cool, Humidity = High, and Wind = Strong. We can estimate the needed
probabilities as follows:
Pr(Play = Yes) = 9/14 = 0.64
Pr(Play = No) = 5/14 = 0.36
Pr(Wind = Strong | Play = Yes) = 3/9 = 0.33
Pr(Wind = Strong | Play = No) = 3/5 = 0.60
Pr(Outlook = Sunny | Play = Yes) = 2/9 = 0.22
Pr(Outlook = Sunny | Play = No) = 3/5 = 0.60
Pr(Play = Yes | X) = 0.64 × 0.33 × 0.22 × 0.33 × 0.33 = 0.0051
Pr(Play = No | X) = 0.36 × 0.60 × 0.60 × 0.20 × 0.60 = 0.016
So the naive Bayes classiﬁer predicts that the speciﬁed day should be placed in the No
category.

1394
Chapter 19
DATA MINING
19.4
CLUSTERING
Clustering is a powerful tool for automated analysis of data. It addresses the following
general problem: given a set of entities, ﬁnd subsets (or clusters) that are homogeneous
and/or well separated. Homogeneity means that entities in the same cluster should be
similar, while separation means that entities in diﬀerent clusters should diﬀer from one
another. Clustering is ubiquitous, with applications in the natural sciences, psychology,
medicine, engineering, economics, marketing, and other ﬁelds.
19.4.1
BASIC CONCEPTS
Deﬁnitions:
A sample is a set O = {o1, o2, . . . , on} of n entities/objects among which clusters are to
be found.
An n×s data matrix X is obtained by measuring or observing s features of the entities
of O.
An n × n matrix of dissimilarities D = (dij) between entities of O is computed from
the matrix X. Typically, these quantities satisfy dij = dji ≥0 for i, j = 1, 2, . . ., n and
dii = 0 for i = 1, 2, . . ., n. These values do not need to satisfy the triangle inequalities,
i.e., to be distances.
A criterion expresses homogeneity and/or separation of the clusters in the clustering
to be found.
Homogeneity of a cluster Cℓis commonly measured by
• the diameter d(Cℓ) of Cℓ, or maximum dissimilarity between entities of Cℓ:
d(Cℓ) =
max
i,j:oi,oj∈Cℓdij;
• the radius r(Cℓ) of Cℓ, or minimum among all entities oi of Cℓof the maximum
dissimilarity between oi and another entity of Cℓ:
r(Cℓ) = min
i:oi∈Cℓmax
j:oj∈Cℓdij;
• the star st(Cℓ) of Cℓ, or minimum among all entities oi of Cℓof the sum of
dissimilarities between oi and the other entities of Cℓ:
st(Cℓ) = min
i:oi∈Cℓ
X
j:oj∈Cℓ
dij;
• the clique cl(Cℓ) of Cℓ, or sum of dissimilarities between entities of Cℓ:
cl(Cℓ) =
X
i<j:oi,oj∈Cℓ
dij.
Separation of the cluster Cℓcan be expressed by

Section 19.4
CLUSTERING
1395
• the split s(Cℓ) of Cℓ, or minimum dissimilarity between an entity of Cℓand one
outside Cℓ:
s(Cℓ) =
min
i:oi∈Cℓ,j:oj /∈Cℓdij;
• the cut c(Cℓ) of Cℓ, or sum of dissimilarities between entities of Cℓand entities
outside Cℓ:
c(Cℓ) =
X
i:oi∈Cℓ
X
j:oj /∈Cℓ
dij.
Also used are the normalized star, clique, or cut obtained by dividing st(Cℓ) by |Cℓ|−1,
cl(Cℓ) by |Cℓ|(|Cℓ| −1), and c(Cℓ) by |Cℓ|(n −|Cℓ|), respectively.
Facts:
1. Criteria for assessing the overall eﬀectiveness of a given clustering can be based on
the six measures deﬁned earlier, which can be combined by summing or by calculating
the threshold (maximum or minimum) of the component cluster measures. The overall
objective is to maximize the combined separation measure or to minimize the combined
homogeneity measure.
2. The clustering problem is old. It can be traced back to Aristotle and was already
much studied by 18th century naturalists such as Buﬀon, Cuvier, and Linn´e.
3. The set of entities O may be associated with a mixture of distributions, the number
and parameters of which are to be found [Mi96, Chapter 2].
4. Dissimilarities can be computed from sources other than the data matrix X, for
instance when comparing biological sequences or partitions.
5. Speciﬁc distance and dissimilarity for entities represented by binary variables can be
derived in an ad hoc manner [An73], [GrRa69].
6. Nominal (or categorical) variables with more than two states as well as numerical
data can be transformed into binary variables [BoEtal97].
7. An axiomatic study of dissimilarities can be found in [Ba89].
8. Instead of computing dissimilarities, clustering can be performed directly on the
matrix X (e.g., Algorithm 3 of §19.4.4).
9. The computational complexity of a clustering problem depends on the criterion used.
For instance, split maximization is polynomially solvable [DeHa80] while diameter mini-
mization is NP-hard [Br78].
10. Cluster analysis is not the only way to study dissimilarities or distances between
entities in the ﬁeld of data analysis. Another common technique is principal component
analysis [Jo02].
11. A popular online source for benchmark datasets commonly used in cluster analysis
is the UCI Machine Learning Repository located at
• https://protect-us.mimecast.com/s/oXA1BRUZEdqZIQ?domain=archive.ics.
uci.edu
Examples:
1. Consider four entities o1, o2, o3, o4 comprising a cluster Cℓ, for which the associated
matrix of dissimilarities is given by
D =


0
3
4
5
3
0
6
8
4
6
0
1
5
8
1
0

.

1396
Chapter 19
DATA MINING
Then d(Cℓ) = 8 since d24 = 8, r(Cℓ) = 5 since d14 = 5, st(Cℓ) = 11, and cl(Cℓ) = 27.
2. The next ﬁgure illustrates three clusters, consisting of 2, 3, and 4 entities, respectively.
The top portion of the ﬁgure uses the diameter of the clusters to obtain an overall
measure: the left-hand side shows the maximum (threshold) cluster diameter, while the
right-hand side shows that the sum of the three cluster diameters is considered. The
bottom portion of the ﬁgure uses the split of the clusters to obtain an overall measure:
the left-hand side shows the minimum (threshold) cluster split, while the right-hand side
shows that the sum of the three splits is considered.
split
sum of splits
sum of diameters
diameter
3. The Iris dataset [Fi36] is one of the most popular datasets used in cluster analysis. It
consists of n = 150 samples obtained from each of three species of Iris ﬂowers (Iris setosa,
Iris virginica, and Iris versicolor). Each sample has s = 4 measured characteristics (in
mm): sepal length, sepal width, petal length, and petal width. Clustering is used in this
context to determine species of plants or animals from a given set of observations.
4. To ﬁnd out which of 15 well-known politicians are perceived as similar, a represen-
tative sample of people is given all 105 possible pairs of these politicians, and asked to
rank the pairs in terms of their similarity. By suitable aggregation of the rankings, a
symmetric distance matrix can be obtained whose values can be used to ﬁnd groups of
similar politicians, with as much diﬀerence as possible between the groups. This could,
for example, be relevant in forming election campaign teams [Sp85].
5. Recommendation algorithms are best known for their use on e-commerce web sites,
where they exploit input about a customer’s interests to generate a list of recommended
items.
Many applications use only the items that customers purchase and explicitly
rate to represent their interests, but they can also use other attributes, including items
viewed, demographic data, subject interests, and favorite artists. Clustering techniques
work by identifying groups of consumers who appear to have similar preferences. Once
the clusters are created, predictions for an individual can be made by averaging the
opinions of the other consumers in that cluster. Thus, the store radically changes based
on customer interests, showing programming titles to a software engineer and baby toys
to a new parent [LiSmYo03], [ScKoRi01].
19.4.2
TYPES OF CLUSTERING
Deﬁnitions:
Cluster analysis methods are based on generating certain collections of sets:

Section 19.4
CLUSTERING
1397
(i) Subset C of O;
(ii) Partition Pk = {C1, C2, . . . , Ck} of O into k clusters such that
(ii a) Cj ̸= ∅, for j = 1, 2, . . . , k;
(ii b) Ci ∩Cj = ∅, for i, j = 1, 2, . . ., k and i ̸= j;
(ii c)
kS
j=1
Cj = O;
(iii) Packing Pak = {C1, C2, . . . , Ck} of O with k clusters: same as (ii) but without
(ii c);
(iv) Covering Cok = {C1, C2, . . . , Ck} of O with k clusters: same as (ii) but with-
out (ii b);
(v) Hierarchy H = {P1, P2, . . . , Pq} of q ≤n partitions of O. The set of partitions
P1, P2, . . . , Pq of O is such that Ci ∈Pc, Cj ∈Pd and c > d imply Ci ⊂Cj or
Ci ∩Cj = ∅for all i, j, with i ̸= j, and c, d = 1, 2, . . ., q.
Facts:
1. By far the most frequently used types of clustering are the partition and the complete
hierarchy of partitions, i.e., the one containing n partitions.
2. Not all clustering criteria are independent. For instance, partitioning with the min-
imum sum of cliques criterion is equivalent to partitioning with the maximum sum of
cuts criterion, since
k
X
ℓ=1
cl(Cℓ) +
Pk
ℓ=1 c(Cℓ)
2
= ∆,
with ∆constant for all partitions Pk.
3. Relaxations of hierarchies are also studied.
They include hierarchies of packings
[MaBe83], weak hierarchies [BaDr89], and pyramids [BeDi85].
4. In constrained clustering, additional requirements are imposed on the clusters. The
most frequent ones are bounds on their cardinality, bounds on their weight, assuming
entities with weights (or connectedness), and assuming an adjacency matrix between
entities is given.
5. In fuzzy clustering, each entity has a degree of membership in one or several clusters.
These values represent the likelihood that the entity is assigned to that cluster.
Examples:
1. For the graph shown in part (a) of the following ﬁgure, we wish to cluster the ﬁve en-
tities into two clusters C1 and C2 to minimize the overall radius, i.e., max{r(C1), r(C2)}.
The associated matrix of dissimilarities is given by
D =


0
10
1
∞
∞
10
0
1
∞
∞
1
1
0
1
1
∞
∞
1
0
10
∞
∞
1
10
0


.
In an optimal partition, one of the clusters must have radius equal to 10 due to the
presence of entities o1 and o2, or entities o4 and o5, in the same cluster. In a covering,
entity o3 is allowed to belong to two clusters at the same time, and so it is possible to
construct two clusters with radius equal to 1, as shown in part (b) of the ﬁgure.

1398
Chapter 19
DATA MINING
o1
1
10
1
1
1
10
o2
o3
o4
o5
o1
1
10
1
1
1
10
o2
o3
o4
o5
(a
(
)
b)
C1
C2
2. A phylogenetic tree (§20.2) is a classical example of a hierarchy of partitions. Fusions
of clusters
are represented by the moment when two species (or two sets of species)
have separated from each other in the course of history. The next ﬁgure illustrates the
evolution of dogs—the Family Canidae.
black bear
domestic dog
gray wolf
coyote
cape hunting dog
black-backed jackal
bush dog
maned wolf
hoary fox
crab-eating fox
gray fox
bat-eared fox
raccoon dog
cape fox
red fox
fennec fox
kit fox
arctic fox
19.4.3
HIERARCHICAL CLUSTERING
This subsection discusses algorithms that can be used to carry out hierarchical clustering.
Both agglomerative methods as well as divisive methods are discussed.
Agglomerative Algorithms:
Agglomerative hierarchical clustering algorithms are among the oldest and still most
frequently used methods for cluster analysis. They start from an initial partition into n
single-entity clusters, and successively merge clusters according to some speciﬁed local
criterion until all entities belong to the same cluster.
Deﬁnitions:
A local criterion is a criterion that uses only the information provided by the current
partition.
Let Gc = (V, E) denote a complete graph, having a vertex vi associated with each entitiy

Section 19.4
CLUSTERING
1399
oi, for i = 1, 2, . . ., n, and with edge (vi, vj) weighted by the dissimilarity dij, for i < j.
Let MST denote a minimum spanning tree of Gc; see §10.1.1.
The single-linkage algorithm merges at each step two clusters achieving the smallest
inter-cluster dissimilarity.
The complete-linkage algorithm merges at each step two clusters for which the resulting
cluster has smallest diameter.
Facts:
1. Algorithm 1 uses a local criterion to successively merge partitions until all entities
belong to a single cluster.
Algorithm 1:
Agglomerative hierarchical algorithm.
input: set O = {o1, o2, . . . , on} of n entities
output: a hierarchy H = {P1, P2, . . . , Pn} of partitions of O
Ci := {oi}, i = 1, 2, . . . , n
Pn := {C1, C2, . . . , Cn}
k := 1
while n −k > 1 do
select Ci, Cj ∈Pn−k+1 following a local criterion
Cn+k := Ci ∪Cj
Pn−k := (Pn−k+1 ∪{Cn+k}) \{Ci, Cj}
k := k + 1
2. The values of the split for all subsets of entities of O, and hence for all partitions of
O, belong to the set of dissimilarity values associated with the set of edges of any MST
of Gc [Ro67].
3. The single-linkage algorithm provides maximum split partitions at all levels of the
hierarchy [DeHa80]. This is a corollary of Fact 2.
4. For other criteria, the partitions obtained after several steps of an agglomerative
algorithm are not necessary optimal. For instance, the complete-linkage algorithm does
not guarantee that after two or more steps the resulting partition is optimal regarding
the minimum diameter.
5. Computer code, in Fortran and Javascript, that implements the single-linkage and
complete-linkage algorithms can be found at the sites
• https://protect-us.mimecast.com/s/ANrmBzUvd34vSd?domain=pitt.edu
• https://protect-us.mimecast.com/s/3RmXBoh8AQ08uM?domain=
code.google.com
6. A parametric formula gives new dissimilarity values between cluster Ck and Ci, Cj
when these latter two are merged [LaWi67]:
dk,i∪j = αidik + αjdjk + δ|dik −djk|
The parameters for single-linkage and complete-linkage algorithms are given next.
method
αi
αj
δ
single-linkage
1
2
1
2
−1
2
complete-linkage
1
2
1
2
1
2

1400
Chapter 19
DATA MINING
Clusters to be merged at each iteration are those corresponding to the smallest updated
dissimilarity. Using a heap yields an O(n2 log n) implementation of Algorithm 1.
7. Four other hierarchical clustering methods are based on an extension of the previous
formula; further extensions are due to [Ja91].
8. Better algorithmic complexity can be derived in a few cases. For example, ﬁnding the
MST of Gc, ranking its edges by nondecreasing values, and merging entities at endpoints
of successive edges yields a θ(n2) implementation of the single-linkage algorithm. At each
iteration, clusters correspond to connected components of a graph with the same vertex
set as Gc and having as edges those of the MST considered.
9. The reducibility property
d(Ci, Cj) ≤min{d(Ci, Ck), d(Cj, Ck)}
implies
min{d(Ci, Ck), d(Cj, Ck)} ≤d((Ci ∪Cj), Ck)
∀i, j, k.
This means that by merging two clusters Ci and Cj, less dissimilar between themselves
than with another cluster Ck, cannot make the resultant cluster Ci ∪Cj less dissimilar
to Ck than Ci and Cj alone.
The dissimilarities D = (dij) induce a nearest neighbor relation, with one or more pairs of
reciprocal near neighbors. When the reducibility property holds, each pair of reciprocal
near neighbors will be merged before merging with other clusters.
10. Updating chains of nearest neighbors yields θ(n2) agglomerative hierarchical clus-
tering algorithms [Be82], [Mu83].
11. The results of hierarchical clustering can be represented graphically by a dendrogram
[Co71] or an espalier [HaJaSi96]. Vertical lines correspond to entities or clusters, and
horizontal lines joining endpoints of vertical lines correspond to mergings of clusters.
The height of the horizontal lines corresponds to the value of the updated dissimilarity
between the clusters merged, which can be a measure of separation or homogeneity of
the clusters obtained. In an espalier the length of the horizontal lines is used to represent
a second measure of homogeneity or separation of the clusters.
Examples:
1. The following ﬁgure illustrates a hierarchical clustering of eight entities. The left
portion shows a dendrogram and the right portion shows an espalier.
Entities
c10
c11
C 15
c12
0
1
2
3
4
5
6
7
8
9
10
11
12
Diameter
O1
O2
O3
O4
O5
O6
O7
O8
C 13
C 14
C 12
C 11
C 9
C 10
O1
O2
O3
O4
O5
O6
O7
O8
0
1
2
3
4
5
6
7
8
9
10
11
12
Diameter
C 12
C 14
C 11
C10
C 9
C 13
C 15
Entities
Splits

Section 19.4
CLUSTERING
1401
2. We apply the single-linkage algorithm to the following matrix of dissimilarities asso-
ciated with ﬁve entities:
C1 = {o1}
C2 = {o2}
C3 = {o3}
C4 = {o4}
C5 = {o5}
C1 = {o1}
0
1∗
4
7
5
C2 = {o2}
1∗
0
7
6
2
C3 = {o3}
4
7
0
3
8
C4 = {o4}
7
6
3
0
5
C5 = {o5}
5
2
8
5
0
The smallest split is between clusters C1 and C2, which are therefore merged into a
single cluster C6 = C1 ∪C2. By using the updating formula presented in Fact 6, new
dissimilarity values are obtained.
C6
C3
C4
C5
C6
0
4
6
2∗
C3
4
0
3
8
C4
6
3
0
5
C5
2∗
8
5
0
Since the current smallest split is found now between clusters C5 and C6, they are merged
into cluster C7 = C5 ∪C6, and then the matrix of dissimilarities is updated likewise.
C7
C3
C4
C7
0
4
5
C3
4
0
3∗
C4
5
3∗
0
Clusters C3 and C4 are next merged together to create cluster C8 = C3 ∪C4. After that,
we reach the last level of the hierarchy of partitions by merging clusters C7 and C8 to
form a single cluster C9.
C7
C8
C7
0
4∗
C8
4∗
0
The single-linkage algorithm obtains maximum split partitions at the ﬁve levels of the
hierarchy. As next shown, this is not a coincidence.
3. Consider the complete graph Gc = (V, E) with vertices vi associated with the entities
oi of Example 2, and with edges (vi, vj) weighted by the dissimilarities dij presented in its
ﬁrst table. The MST of Gc contains the following edges, listed by nondecreasing weight:
(v1, v2) (weight 1), (v2, v5) (weight 2), (v3, v4) (weight 3), and (v1, v3) (weight 4). Now
consider a graph G′ with the same vertex set as Gc and where clusters correspond to its
diﬀerent connected components. This is shown in part (a) of the following ﬁgure.
The single-linkage algorithm, based on ﬁnding the MST of Gc (Fact 8), considers the
MST edges in nondecreasing order, and at each iteration connects components of G′
using these edges. In this example, it starts by considering edge (v1, v2) and connecting
these vertices in order to create the connected component C6 = {v1, v2}; see part (b).
The following steps of the clustering are shown in parts (c)–(e), Namely, the next edge
(v2, v5) connects v5 to C6, creating C7 = {v1, v2, v5}. A new connected component C8

1402
Chapter 19
DATA MINING
is formed by connecting vertices v3 and v4.
Finally, the two connected components
are merged into a single connected component of G′ by considering edge (v1, v3). The
sequence of clusters obtained corresponds to that found in the previous example (Fact 2).
C1
v1
v2
v3
v4
v5
C2
C3
C4
C5
v1
v2
v3
v4
v5
C3
C4
C7
v1
v2
v3
v4
v5
C3
C4
C5
C6
v1
v2
v3
v4
v5
C9
v1
v2
v3
v4
v5
C8
C7
(a)
(b)
(c)
(d
(
)
e)
4. Consider the following set of ﬁve entities as well as its matrix of dissimilarities:
o1
o2
o3
o4
o5
o1
0
4
9
7
11
o2
4
0
1
6
3
o3
9
1
0
8
6
o4
7
6
8
0
4
o5
11
3
6
4
0
The complete-linkage algorithm applied to this set of entities produces the dendrogram
presented next, which represents the successive agglomerations performed by the algo-
rithm with the purpose of minimizing the diameter of each of the partitions obtained in
the hierarchy.
The complete-linkage algorithm is not assured of ﬁnding minimum diameter partitions
at all levels of the hierarchy (Fact 4). For instance, a better bipartition could have been
obtained by clustering entities o1 and o4 in one cluster and entities o2, o3, and o5 in
another one, yielding a bipartition of diameter 7.

Section 19.4
CLUSTERING
1403
0
2
4
6
8
10
12
diameter
entities
o1
o2
o3
o4
o5
Divisive Algorithms:
Divisive hierarchical clustering algorithms are less frequently used than agglomerative
ones.
They proceed from an initial cluster containing all entities and then perform
successive bipartitions of one cluster at a time until all entities belong to diﬀerent clusters.
Deﬁnitions:
Let MST′ denote a maximum spanning tree of Gc; see §10.1.1.
A coloring of a graph is an assignment of colors to its vertices such that no two adjacent
vertices have the same color; see §8.6.1. A bicoloring of a graph is a coloring that uses
only two colors.
An odd cycle in a graph is one containing an odd number of vertices.
Facts:
1. Algorithm 2 uses two local criteria to successively reﬁne partitions until each entity
belongs to a single cluster.
Algorithm 2:
Divisive hierarchical algorithm.
input: set O = {o1, o2, . . . , on} of n entities
output: a hierarchy H = {P1, P2, . . . , Pn} of partitions of O
P1 := {C1} = {{o1, o2, . . . , on}}
k := 1
while k < n do
select Ci ∈Pk following a ﬁrst local criterion
partition Ci into C2k and C2k+1 following a second local criterion
Pk+1 := (Pk ∪{C2k} ∪{C2k+1}) \{Ci}
k := k + 1
2. The role of the ﬁrst local criterion in Algorithm 2 is not crucial, as it only determines
the order in which clusters will be bipartitioned. The second criterion is more diﬃcult
to apply since optimal bipartitioning of the chosen cluster according to a given criterion
is a problem which may be NP-hard.

1404
Chapter 19
DATA MINING
3. The unique bicoloring of MST′ deﬁnes a minimum diameter bipartition of O [MoSu91].
The diameter of this bipartition is equal to the largest dissimilarity of an edge outside
MST′ closing an odd cycle in MST′.
4. Fact 3 yields an O(n2 log n) divisive hierarchical algorithm for the minimum diameter
criterion by building simultaneously maximum spanning trees at all levels of the hierarchy
[GuHaJa91].
Example:
5. Consider the matrix of dissimilarities between ﬁve entities from Example 4 in §19.4.3:
o1
o2
o3
o4
o5
o1
0
4
9
7
11
o2
4
0
1
6
3
o3
9
1
0
8
6
o4
7
6
8
0
4
o5
11
3
6
4
0
The MST′ of the associated complete graph Gc consists of the edges (v1, v5) (weight 11),
(v1, v3) (weight 9), (v3, v4) (weight 8), and (v2, v4) (weight 6). The largest dissimilarity
of an edge outside MST′ closing an odd cycle in MST′ is provided by edge (v1, v4) having
weight 7; see part (a) of the following ﬁgure. This is the value of the diameter of the
bipartition obtained from bicoloring the vertices in MST′ (Fact 3), i.e., the partition
formed by clusters C2 = {o1, o4} and C3 = {o2, o3, o5}.
v5
v1
v3
v4
v2
11
8
7
6
9
(a
(
)
b)
v5
v3
v2
1
3
6
For the next step, since the diameter of the partition results from cluster C2 = {o1, o4},
this cluster is chosen to be divided by the ﬁrst local criterion, yielding a new partition
C4 = {o1}, C5 = {o4}, and C3 = {o2, o3, o5} of diameter equal to 6 due to the dissimilar-
ity between entities o3 and o5 in C3. Thus, cluster C3 is chosen to be divided by the ﬁrst
local criterion. A new MST′ for the partial complete graph composed of vertices v2, v3, v5
has edges (v3, v5) (weight 6) and (v2, v5) (weight 3); see part (b) of the ﬁgure. Then a
bipartition of C3 is performed in order to build clusters C6 = {o2, o3} and C7 = {o5},
yielding a partition of diameter equal to 1 due to the dissimilarity between entities o2
and o3 in cluster C6. Finally, the last bipartition over C6 is trivial, and the last level of
the hierarchy is attained.
19.4.4
PARTITIONING
Deﬁnitions:
Let h(·) be a homogeneity (or heterogeneity) function on the subsets of O.

Section 19.4
CLUSTERING
1405
An optimal substructure indicates that optimal solutions of subproblems can be used
to ﬁnd optimal solutions of the overall problem.
A problem has overlapping subproblems if the problem can be broken down into
subproblems which are reused several times.
Dynamic programming is a method of solving problems exhibiting the properties of
overlapping subproblems and optimal substructure in a much faster way than complete
enumeration of solutions.
Recursion is a method of deﬁning functions in which the function being deﬁned is
applied within its own deﬁnition.
Branch-and-bound (§16.1.8) consists of an implicit enumeration of all solutions using
a tree, where large subsets of solutions are pruned before being explored by using upper
and lower bounds on the quantity being optimized.
Valid inequalities are inequalities in a mathematical programming formulation of a
problem that are necessarily satisﬁed by its variables.
Facts:
1. Dynamic programming provides an exact algorithm for partitioning by exploiting the
recursion
f ∗
k(O′) =



opt
Ok⊂O′(f ∗
k−1(O′ −Ok) + h(Ok)) for k > 1
h(O′), for k = 1
where f ∗
k(O′) denotes the optimal value for partitioning the set O′ ⊆O into k clusters,
and additivity is assumed. This formula is easily modiﬁed if the value of the objective
function is equal to the maximum or the minimum value for each of the clusters. Applying
this equation takes time exponential in n, so only small sets of entities may be considered.
Constraints can sometimes accelerate the computations, e.g., if all clusters must be small.
2. The single-linkage algorithm provides optimal partitions for the split criterion at all
levels of the hierarchy.
So it is also a θ(n2) algorithm for maximizing the split of a
partition of O into k clusters.
3. Let C = {C1, C2, . . . , C2n−1} denote the set of clusters obtained when applying the
single-linkage algorithm to O. Then for all k there exists a partition P ∗
k that maximizes
the sum of splits and consists solely of clusters of C [HaJaFr89].
4. Let t be the smallest dissimilarity value such that the partial graph Gt = (V, Et) of
G with Et = {(vi, vj) | dij ≥t} is k-colorable. Then the color classes in any optimal k-
coloring of Gt deﬁne a minimum diameter partition of O into k clusters [Ch75], [HaDe78].
5. Partitions obtained with the single-linkage algorithm may suﬀer from the chaining ef-
fect: dissimilar entities at the ends of a long chain of pairwise similar entities are assigned
to the same cluster. Partitions obtained by the coloring algorithm for minimum diam-
eter may suﬀer from the dissection eﬀect: similar entities may be assigned to diﬀerent
clusters.
6. Bicriterion cluster analysis algorithms seek compromise solutions in order to avoid
chaining and dissection eﬀects [DeHa80].
7. Branch-and-bound algorithms have been applied, with some success, to several par-
titioning problems in cluster analysis [Di85], [KlAr91], [KoNaFu75].
Their eﬃciency
depends on sharpness of the bounds used, availability of a good heuristic solution, and
eﬃcient branching, i.e., rules that improve bounds for all subproblems obtained in a fairly
balanced way.

1406
Chapter 19
DATA MINING
8. Brusco and Stahl [BrSt05] proposed a repetitive branch-and-bound algorithm (RBBA)
that solves subproblems in sequence by repeating a branch-and-bound procedure. In
RBBA, the resolution of a given subproblem by branch-and-bound depends on the op-
timal solutions of smaller subproblems already solved by the algorithm, as a look-ahead
component in the bound. For instance, using the minimum diameter criterion, RBBA
can solve exactly instances with n ≤250 entities and k ≤20 clusters.
9. Good results can also be obtained when bounds result from the solution of a math-
ematical program combined with heuristic methods. For minimum sum-of-stars parti-
tioning, primal-dual variable neighborhood search [HaEtal07] solved exactly for the ﬁrst
time instances with n = 15,000.
10. Several valid inequalities are given in [GrWa90] for the sum-of-cliques criterion.
These results are used in a cutting plane algorithm [GrWa89] to solve instances with
n ≤158. It appears that triangle inequalities (i.e., valid inequalities which state that if
the pairs of entities oi, oj and oj, ok belong to the same cluster then oi, ok also belong to
the same cluster) suﬃce in almost all cases.
11. Partitioning problems in cluster analysis can be formulated by considering all pos-
sible clusters in the following mathematical program:
minimize:
P
t∈T
h(Ct)yt
subject to:
P
t∈T
aityt = 1,
i = 1, . . . , n
P
t∈T
yt = k
yt ∈{0, 1},
t ∈T
where T = {1, . . . , 2n −1} and ait equals 1 if entity oi belongs to cluster Ct and is 0
otherwise. Variable yt equals 1 if cluster Ct is in the optimal partition and is 0 otherwise.
This is a large linear partitioning problem with a size constraint, for which the number
of variables is exponential in the number of entities n.
12. The standard way to tackle the mathematical program in Fact 11 is to solve the
linear relaxation by means of column generation [DeDeSo05], and then apply branch-
and-bound. The entering column is obtained by solving the following auxiliary problem,
whose unknowns are the coeﬃcients ai of the column:
minimize:
h(Ct) −Pn
i=1 aiλi + σ
subject to:
ai ∈{0, 1},
i = 1, . . . , n.
Here λi for i = 1, . . . , n and σ are the dual variables at the current iteration. Diﬃculty
varies depending on the form of h(Ct) as a function of the ai.
Once the entering column is found, the linear program solver proceeds. However, con-
vergence may be slow, particularly if there are few clusters in the partition and hence
massive degeneracy of the optimal solution. In fact, even when the optimal solution is
found, many more iterations may be needed to prove its optimality. Once the linear
relaxation of the problem is solved, one must check for integrality of the solution. If the
solution is not integer, branch-and-bound is needed.
Examples:
1. Consider three entities with the associated matrix of dissimilarities given by

Section 19.4
CLUSTERING
1407
o1
o2
o3
o1
0
4
9
o2
4
0
1
o3
9
1
0
They are easily clustered by simple inspection regardless of the criterion chosen. The
goal here is to show how dynamic programming works in solving a partitioning problem
(Fact 1). Suppose we want to minimize the diameter of a bipartition over these entities.
The algorithm starts with f ∗
1 (o1) = 0, f ∗
1 (o2) = 0, f ∗
1 (o3) = 0 since h(oi) = 0 for
i = 1, 2, 3. It is useless to calculate f ∗
2 (oi) for i = 1, 2, 3 since k is larger than n. Then
the algorithm proceeds by calculating
f ∗
1 (O12 = {o1, o2}) = h(O12) = 4, f ∗
1 (O13 = {o1, o3}) = h(O13) = 9,
f ∗
1 (O23 = {o2, o3}) = h(O23) = 1, f ∗
1 (O123 = {o1, o2, o3}) = h(O123) = 9.
With all the f ∗
1 (·) now calculated we obtain
f ∗
2 (O123)
=
min{f ∗
1 (O123) + h(∅), f ∗
1 (O12) + h(o3), f ∗
1 (O13) + h(o2), f ∗
1 (O23) + h(o1),
f ∗
1 (o1) + h(O23), f ∗
1 (o2) + h(O13), f ∗
1 (o3) + h(O12)}
=
f ∗
1 (O23) + h(o1) = 1,
which results in C1 = {o1} and C2 = {o2, o3}.
2. In Example 2 of §19.4.3, the single-linkage algorithm for n = 5 entities obtained
(5 × 4)/2 −1 = 9 diﬀerent clusters: C1 = {o1}, C2 = {o2}, C3 = {o3}, C4 = {o4}, C5 =
{o5}, C6 = {o1, o2}, C7 = {o1, o2, o5}, C8 = {o3, o4}, and C9 = {o1, o2, o3, o4, o5}. If a
maximum sum-of-splits partition with four clusters is demanded, from
 5
2

= 10 diﬀerent
possibilities, we can restrict ourselves to only two due to Fact 3:
P 1
4 = {C6, C3, C4, C5}, P 2
4 = {C1, C2, C8, C5},
which results in P ∗
4 = P 1
4 with sum of splits equal to 10.
Here, the maximum split
partition of size 4 obtained by the single-linkage algorithm coincides with the one that
maximizes the sum of splits, though this is not always true.
3. The minimum diameter partition with k = 3 for the ﬁve entities with matrix of dissim-
ilarities presented in Example 4 of §19.4.3 is determined by the smallest dissimilarity value
t such that the partial graph Gt = (V, Et) of G with Et = {(vi, vj) | dij ≥t} is 3-colorable
(Fact 4). For the given example, the smallest t for which Gt is 3-colorable is equal to 5,
since the partial graph Gt shown next is no longer 3-colorable if edges (v1, v2) and (v4, v5)
of weight 4 are added to it. The diameter of the partition P ∗
3 = {{o1, o2}, {o3}, {o4, o5}}
is equal to 4.
v1
v5
v4
v3
v2
11
8
7
6
9
6

1408
Chapter 19
DATA MINING
⊲Partitioning in Euclidean Space
Here we suppose that the entities oi are points xi of an s-dimensional Euclidean space.
Deﬁnitions:
The homogeneity of cluster Cj is measured by reference to a center of Cj, which is not
in general a point belonging to the cluster.
A popular criterion for partitioning points in Euclidean space is the minimum sum-of-
squares criterion ss(Cj) given by
ss(Cj) =
X
i:oi∈Cj
(∥xi −x∥2)2,
where ∥· ∥2 denotes Euclidean distance and x is given by the centroid of the points xi
in cluster Cj, i.e.,
x =
1
|Cj|
X
i:oi∈Cj
xi.
Other criteria can also be used:
• the variance v(Cj) of Cj, deﬁned as ss(Cj) divided by |Cj|;
• the continuous radius cr(Cj) of Cj, deﬁned by
cr(Cj) = min
x∈Rs max
i:oi∈Cj ∥xi −x∥2;
• the continuous star cst(Cj) of Cj, deﬁned by
cst(Cj) = min
x∈Rs
X
i:oi∈Cj
∥xi −x∥2.
Facts:
1. The minimum sum-of-squares criterion addresses both homogeneity and separation
[Sp80].
It is well known as the criterion tackled by the classical k-means clustering
procedure [Ma67], described by Algorithm 3. From an initial partition, the k-means
algorithm proceeds by reassigning the entities to their closest centroids and updating the
cluster centroids until stability is reached. It produces not only the cluster centroids xj
but also the cluster membership function m.
Algorithm 3:
k-means.
input: set X = {x1, x2, . . . , xn} of n points in Rs, number of clusters k
output: cluster centroids X = {x1, . . . , xk} ∈Rs, m : X →{1, . . ., k}
choose initial values of X
for i := 1 to n
m(xi) := argmin j∈{1,...,k}(∥xi −xj∥2)2
repeat
for j := 1 to k
calculate the centroid xj
for i := 1 to n
m(xi) := argmin j∈{1,...,k}(∥xi −xj∥2)2
until m does not change

Section 19.4
CLUSTERING
1409
2. The centroids used by criteria ss(Cj) and v(Cj) may be usefully considered as rep-
resentative of clusters in some applications.
3. The sum of squares of cluster Cj is equal to the sum of all squared distances between
pairs of entities of this cluster divided by its cardinality:
ss(Cj) =
1
|Cj|
X
i:oi∈Cj
X
ℓ̸=i:oℓ∈Cj
(∥xi −xℓ∥2)2.
4. The entities of clusters C1 and C2 in a bipartition may be separated by a hyperplane
of Rs. Moreover, one may choose this hyperplane from among those going through 1 to
s entities [Ha67]. This implies that the number of bipartitions is polynomially bounded
for ﬁxed dimensions. Using this fact, one can devise an exact polynomial O(ns+1 log n)
algorithm for minimum sum-of-squares hierarchical divisive clustering of points in s-
dimensional space [HaJaMl98].
5. Minimum sum-of-squares clustering is NP-hard for general dimensions even when
k = 2 [AlEtal09]. For general values of k, the problem is NP-hard even in the plane
[MaNiVa09]. If both k and the dimension s are ﬁxed, the problem can be solved in
O(nsk+1) time [InKaIm94], which may be very time consuming even for instances in the
plane. Exact methods with exponential computational complexity include dynamic pro-
gramming [vOMe04], branch-and-bound [Br06], column generation [dMEtal00], concave
minimization [XiPe05], and semideﬁnite programming [AlHa09], [PeXi05].
6. Implementations of the k-means algorithm can be found at the sites
• https://protect-us.mimecast.com/s/dqYaBRipE0KpFJ?domain=bonsai.
hgc.jp
• https://protect-us.mimecast.com/s/DzxkBmIOQ58OUg?domain=cs.umd.edu
• https://protect-us.mimecast.com/s/8JQwBdfQvNrQhK?domain=alglib.net
7. Because reassignments are performed only if proﬁtable and the number of partitions
is ﬁnite, the k-means algorithm always converges to a local minimum.
Better local
minima are provided with the j-means heuristic [HaMl01a] combined with variable-depth
neighborhood search [HaMl01b]; see §16.8.2. A detailed comparative study of heuristics
for minimum sum-of-squares clustering with many references is provided in [BrSt07].
8. Problems in one-dimensional Euclidean space are best solved by dynamic program-
ming. This method works well when optimal clusters possess the string property, i.e.,
consist of consecutive points on the line. Assume o1, o2, . . . , on are indexed in order of
nondecreasing values of x1, x2, . . . , xn. Let h(·) be a homogeneity function on the subsets
of O and let F k
m be the optimal value of clustering o1, o2, . . . , om into k clusters. The
dynamic programming recursion may be written
F k
m =
min
i∈{k,k+1,...,m}{F k−1
i−1 + h(Ci,m)}
where Ci,m = {oi, oi+1, . . . , om}. Using updating to compute h(·) for all potential clusters
yields O(n2) algorithms for various criteria [Ra71], [Sp80].
9. The objective in the classical Fermat-Weber problem is to locate a facility in Rs in
order to minimize the sum of its weighted Euclidean distances to the locations of a given
set of users. It is a central problem in continuous location theory [We93]; see §16.2.5.
Applying the continuous star criterion is equivalent to the Fermat-Weber problem with
equally weighted Euclidean distances.
10. For A ∈Rs×s and b ∈Rs,
∥x1 −x2∥= ∥Ax1 + b −Ax2 −b∥

1410
Chapter 19
DATA MINING
if and only if A is orthogonal, i.e., criteria involving Euclidean distances are invariant
with respect to orthogonal transformations and translations.
An interesting question is whether the optimal solution would change if scaling of mea-
surements is performed over the data. In general, a scale transformation is deﬁned as
a nonsingular matrix H ∈Rs×s with H = diag(h1, . . . , hs) (hℓ̸= 0 for ℓ= 1, . . . , s).
From the previous equation, we observe that Euclidean distance criteria are invariant
with respect to scale transformations if hℓ= ±1 for ℓ= 1, . . . , s, i.e., the composition of
optimal partitions is unaﬀected by scale transformations only if they amount to no more
than a change of sign. This is very restrictive for practical applications.
Examples:
1. Color image quantization is a data compression technique that reduces the total set
of colors in a digital image to a representative subset. In the RGB color model, a true
color image is represented by a matrix of pixel colors, each consisting of 24 bits: one byte
for red, one byte for green, and one byte for blue components. However, it is often the
case that only k ≪224 ≈16.8 million possible colors are stored.
The problem can be modeled by considering an image I as a function I : Ω→R3,
Ω⊂Z × Z, with each dimension of I(Ω) = {I(x) | x ∈Ω} representing one of the red,
green, and blue components of a pixel color, with values ranging between 0 and 255.
Thus, color quantization of I for the RGB color model consists of solving a partitioning
problem in the 3-dimensional Euclidean space I(Ω) with the number of clusters equal
to k. After clusters C1, C2, . . . , Ck are obtained, the color I(x) of each pixel x ∈Ωis
replaced by the centroid of its cluster.
2. We provide an example of the k-means algorithm applied in R2 to partitioning nine
entities into three clusters. The ﬁrst ﬁgure illustrates the initialization of the k-means
algorithm, deﬁned by ﬁrst determining the initial positions for the centroids of each
cluster (e.g., by random selection).

Section 19.4
CLUSTERING
1411
The next ﬁgure presents three iterations of the algorithm, performed until stabilization
is reached. The left-hand side of each panel shows the assignment step of the algorithm
where each point is assigned to its closest centroid, while the right-hand side indicates
the updated positions of the cluster centroids.
3. Consider two data matrices X and X′, each corresponding to the coordinates of six
points in the plane:

1412
Chapter 19
DATA MINING
X =









0
48
6
48
0
24
6
24
0
0
6
0









,
X′ =









0
12
24
12
0
6
24
6
0
0
24
0









.
It is easy to verify that X′ = XH where H =
 
4
0
0
1/4
!
.
Not surprisingly, since H is not orthogonal, the optimal partitions diﬀer for the data ma-
trices X and X′ (Fact 10). For instance, if the minimum sum-of-squares criterion is used
with k = 2, the optimal bipartition for points in X is given by {{x1, x2, x3, x4}, {x5, x6}},
while for points in X′ it is given by {{x′
1, x′
3, x′
5}, {x′
2, x′
4, x′
6}}.
19.4.5
SPECTRAL CLUSTERING
There are several other clustering methods found in the literature (e.g., model-based clus-
tering [FrRa02], additive clustering [Mi87], and subspace clustering [PaHaLi04]). Among
these methods, spectral clustering algorithms have become very popular in recent years.
Their main appeal relies on the fact that they are based on standard linear algebra
and are simple to implement.
Spectral clustering techniques use the eigenvalues and
eigenvectors of certain graph-based matrices in order to cluster data.
Deﬁnitions:
The unnormalized graph Laplacian is deﬁned as L = S −D, where D = (dij) and
S = diag(s1, s2, . . . , sn) with diagonal elements deﬁned by si = Pn
j=1 dij. It is customary
to let D represent a matrix of similarities, i.e., if dij is very large (small), then objects
oi and oj are very similar (diﬀerent).
The normalized graph Laplacian is given by Lnorm = S−1L = I −S−1D.
Facts:
1. A thorough coverage of Laplacian matrices and spectral clustering is found in [Ch97].
2. A diﬀerent normalized graph Laplacian matrix Lb = S−1/2LS−1/2 can be used by
spectral clustering algorithms [NgJoWe02].
3. The term normalization means that the largest eigenvalue is less than or equal to 2,
with equality only when the graph is bipartite.
4. Since Lnorm and Lb are positive semideﬁnite all their eigenvalues range between 0
and 2.
5. According to [vL07], Lb does not present any computational advantages over Lnorm.
In fact, spectral clustering with the eigenvectors of Lb might be problematic if they
contain particularly small entries.
6. The matrix L satisﬁes the following properties [Mo91]:
• f T Lf = 1
2
Pn
i,j=1 dij(fi −fj)2 holds for every vector f ∈Rn;
• L is symmetric and positive semideﬁnite;

Section 19.4
CLUSTERING
1413
• The smallest eigenvalue of L is 0, and a corresponding eigenvector is the constant
one vector (1, 1, . . . , 1)T ;
• L has n nonnegative, real-valued eigenvalues 0 = λ1 ≤λ2 ≤· · · ≤λn.
7. The unnormalized spectral clustering algorithm (Algorithm 4) uses the unnormalized
Laplacian matrix L to change the representation of the data matrix X to points yi ∈Rk
for i = 1, . . . , n.
Then the k-means procedure is executed on these points in order
to obtain the output clustering for the original data. Thus, if the k-means procedure
classiﬁes points yi and yj into the same cluster, then the output clustering likewise assigns
entities oi and oj to the same cluster.
Algorithm 4:
Unnormalized spectral clustering.
input: similarity matrix D ∈Rn×n, number of clusters k
output: clusters C1, . . . , Ck
construct a graph G = (V, E) from D
compute the unnormalized Laplacian L
compute the ﬁrst k eigenvectors f 1, . . . , fk of L associated with its k smallest
eigenvalues
let V ∈Rn×k be the matrix containing the vectors f 1, . . . , fk as columns
for i := 1 to n
let yi ∈Rk be the vector corresponding to the ith row of V
cluster the points (yi)i=1,...,n ∈Rk into k clusters using the k-means algorithm
8. The normalized Laplacian Lnorm satisﬁes the following properties [Ch97]:
• λ is an eigenvalue of Lnorm with eigenvector f if and only if λ and f solve the
generalized eigenproblem Lf = λSf;
• 0 is an eigenvalue of Lnorm, and a corresponding eigenvector is the constant one
vector (1, 1, . . . , 1)T ;
• Lnorm is positive semideﬁnite and has n real-valued eigenvalues 0 = λ1 ≤λ2 ≤
· · · ≤λn.
9. The normalized spectral clustering algorithm (Algorithm 5) is similar to the unnor-
malized version presented in Fact 7, except that it uses the eigenvectors of the normalized
Laplacian matrix Lnorm instead of those of L.
Algorithm 5:
Normalized spectral clustering.
input: similarity matrix D ∈Rn×n, number of clusters k
output: clusters C1, . . . , Ck
construct a graph G = (V, E) from D
compute the unnormalized Laplacian L
compute the ﬁrst k eigenvectors f 1, . . . , fk, associated with the k smallest
eigenvalues of Lnorm that solve the generalized eigenproblem Lf = λSf
let V ∈Rn×k be the matrix containing the vectors f 1, . . . , fk as columns
for i := 1 to n
let yi ∈Rk be the vector corresponding to the ith row of V
cluster the points (yi)i=1,...,n ∈Rk into k clusters using the k-means algorithm

1414
Chapter 19
DATA MINING
10. The similarity graphs G used by Algorithm 4 and Algorithm 5 may not be com-
plete. In fact, there are several ways of constructing a graph from a similarity matrix
D such that similarity relationships are well represented, e.g., fully connected graphs,
ε-neighborhood graphs, or k-nearest neighbor graphs [vL07].
11. Let G = (V, E) be an undirected graph with nonnegative weights on edges and with
|V | = n. Then the multiplicity of 0 as an eigenvalue of L and Lnorm is equal to the
number of connected components of G [Mo97]. Moreover, the eigenspace of 0 is spanned
by the indicator vectors associated with those components, where the indicator vector
σA = (t1, . . . , tn)T ∈Rn of a connected component with vertices A ⊆V is deﬁned such
that ti = 1 if vi ∈A and ti = 0 otherwise.
12. Unnormalized spectral clustering provides a relaxation to minimizing the ratio cut
criterion [HaKa92], which is expressed as
Rcut(Pk = {C1, C2, . . . , Ck}) =
k
X
j=1
c(Cj)
|Cj| .
13. Normalized spectral clustering provides a relaxation to minimizing the normalized
cut criterion [ShMa00], which is deﬁned as
Ncut(Pk = {C1, C2, . . . , Ck}) =
k
X
j=1
c(Cj)
P
i:oi∈Cj si
.
14. An argument in favor of normalized spectral clustering is that Ncut expresses both
homogeneity and separation. In fact,
X
i,j:oi,oj∈Cℓ
dij =
X
i,j:oi∈Cℓ
dij −
X
i,j:oi∈Cℓ,oj∈O\Cℓ
dij =
X
i:oi∈Cℓ
si −c(Cℓ).
Hence, homogeneity is maximized for a cluster Cℓif c(Cℓ) is small and if P
i:oi∈Cℓsi is
large. Both of these conditions are optimized for the Ncut criterion. However, Rcut
aims to maximize |Cℓ| of each cluster ℓ= 1, . . . , k, though the cardinality of a cluster is
not necessarily related to homogeneity as it also depends on the similarities between the
entities it contains.
15. Separation is maximized for both the Ncut and Rcut criteria since there is a term
c(Cℓ) for each cluster Cℓ, ℓ= 1, . . . , k, which is minimized in the numerator of both
formulas (recall that D is now a similarity matrix).
16. MATLAB code for spectral clustering is available at the sites
• https://protect-us.mimecast.com/s/QQegB6F2AeL2C2?domain=
cis.upenn.edu
• https://protect-us.mimecast.com/s/xN7GBqUevdgeU0?domain=
mathworks.com/44879-spectral-clustering
17. Besides its parallel with graph partitioning, spectral clustering is also related to
random walks on the similarity graph [MeSh01] and perturbation theory [vL07].
Example:
1. One hundred points are generated in the plane and plotted in following ﬁgure. The
similarity matrix is constructed such that dij for two points xi and xj is given by the
inverse of their Euclidean distance.
Each group of 25 points is generated by adding
normal(0,1) perturbations to the points (1, 1), (1, 10), (10, 1), (10, 10). The diﬀerent types

Section 19.4
CLUSTERING
1415
of marker represent the natural clusters for the dataset. A complete graph G, weighted
by the entries of matrix D, is used as the similarity graph. The ﬁrst four eigenvectors of
Lnorm are displayed in the next ﬁgures. The coordinates of an eigenvector f ∈R100 are
plotted consistent with the original 100 data points.
-2
0
2
4
6
8
10
12
-2
0
2
4
6
8
10
12
♦
♦
♦
♦♦
♦
♦
♦
♦
♦
♦
♦
♦
♦
♦
♦
♦
♦
♦
♦
♦
♦
♦
♦♦
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
++
+
+
+
+
□
□
□
□□□
□
□
□
□
□
□
□
□
□□□
□
□
□
□
□
□
□
□
×
×××
××
×
×
×
×
×
×
×
×
×
×
×
×
× ×
×
×
×
×
×
The ﬁrst eigenvalue of Lnorm is 0. Since G is fully connected its multiplicity is equal to
1, and the ﬁrst eigenvector is the constant one vector multiplied by a scalar (note that
G is a weighted graph).
0.099
0.0995
0.1
0.1005
0.101
0
10
20
30
40
50
60
70
80
90
100
1st eigenvector
♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦♦+++++++++++++++++++++++++□□□□□□□□□□□□□□□□□□□□□□□□□×××××××××××××××××××××××××
The following eigenvectors carry all the information about the clusters. If clusters in the
upper parts of the graphics are distinguished from clusters in the lower parts, we notice
that the second eigenvector separates clusters ♦and □from clusters + and ×. Similarly,
the third eigenvector separates clusters ♦and + from clusters □and ×, while the fourth
eigenvector separates clusters ♦and × from □and +. If the k-means procedure is then
applied to the rows of these eigenvectors, it identiﬁes the correct four clusters.

1416
Chapter 19
DATA MINING
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
0
10
20
30
40
50
60
70
80
90
100
2nd eigenvector
♦♦♦♦♦
♦
♦♦
♦
♦♦♦♦♦♦♦♦♦
♦♦♦♦♦♦♦
++
++
+
++
+
+++
+
+
+
+
+
+
++
+
+++
++
□□□
□□□□
□
□□
□
□
□□
□□□□
□
□
□
□
□□
□
××××××
××××
×
×
×
×
×××××××××××
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
0
10
20
30
40
50
60
70
80
90
100
3rd eigenvector
♦
♦
♦
♦♦
♦
♦
♦
♦♦♦♦♦
♦
♦♦♦
♦♦♦♦♦♦
♦♦
+++++++++
+
++
+
+
+
+
+++++++++
□□□□□□□□□□□□□
□□
□□□□
□
□
□□□□
×
××××××
×
×
×
×××
××
×
××××
××
×
××
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
0
10
20
30
40
50
60
70
80
90
100
4th eigenvector
♦
♦
♦♦♦
♦♦
♦
♦
♦♦♦♦♦♦♦♦
♦
♦♦♦♦♦♦♦
++++
+++
+
+
+
+
++
+
+
+
+++++++
++□□□□□□
□
□□□□□□
□□
□□□
□
□
□□□□□
××××××
×
×
××
×
×
×
×
××
××××
××
×××

Section 19.5
OUTLIER DETECTION
1417
19.5
OUTLIER DETECTION
Outlier detection, also known as anomaly detection, involves ﬁnding data points that do
not ﬁt in with other data points in some important way. Outlier detection has many
important applications in diverse areas, including ﬁnance, medicine, network security,
and image processing. Outlier detection is complementary to that of clustering in that
it determines individual data points that are so diﬀerent from the bulk of the data that
they can be classiﬁed as anomalies and not included in a cluster.
Deﬁnitions:
An outlier (or anomaly) is a data point that diﬀers so signiﬁcantly from other data
points that it is unlikely to have been generated by the same mechanism.
Collective outliers collectively deviate from a dataset, even though individual data
points do not.
Statistical methods (or model-based methods) for outlier detection assume that
normal data objects are generated by a statistical model. (However, outliers are data
points that do not follow the model.)
Among statistical methods, parametric methods for outlier detection assume normal
data are objects that are generated by a parametric distribution with parameter θ. The
probability density function f(y, θ) gives the probability than an object y is generated
by the distribution.
If the probability is small, then it is likely that it is an outlier
[HaKaPe12].
Among statistical methods, nonparametric methods for outlier detection do not as-
sume a prior statistical model, but instead try to determine the model from the data.
Proximity-based methods classify as outliers those objects that are signiﬁcantly far-
ther away than are other objects to points nearby, using some measure of distance or
similarity.
The distance-based outlier score of an object is its distance to its kth nearest neigh-
bor, where the integer k is speciﬁed by the user.
Among proximity-based methods, distance-based outlier detection methods look
at the neighborhood of a point. A point is an outlier if its neighborhood does not have
a suﬃcient number of other points.
Among proximity-based methods, density-based outlier detection methods classify
a point as an outlier if its density is much lower than that of its neighbors.
Clustering-based methods classify as outliers objects that either do not belong to a
cluster or belong to small or sparse clusters.
Classiﬁcation-based approaches are given a training set of examples of outliers and
non-outliers. A classiﬁcation method is then used to learn to predict whether a point is
an outlier.
Mahalanobis distance is the distance between a point and the centroid of a cluster,
normalized by the standard deviation in each dimension.

1418
Chapter 19
DATA MINING
Facts:
1. Surveys of methods for outlier detection can be found in [Ag15] and [HoAu04].
2. Classiﬁcation methods can be used to ﬁnd outliers. One way this might be done
successfully is to use an outlier class containing items that deviate signiﬁcantly from the
other data points.
3. Clustering methods can be adapted to ﬁnd outliers by ﬁnding clusters and then
looking for data points that are far from these. Normal points may belong to large dense
clusters, while outliers do not. However, this approach does not work when normal data
points do not share strong patterns, while outliers do.
4. If the assumption is made that univariate data has a normal distribution, then the
maximum likelihood method [HaKaPe12] can be used to estimate the mean µ and the
standard deviation σ by maximizing the log-likelihood function:
ln L(µ, σ2) =
n
X
i=1
ln f(xi | (µ, σ2)) =
−n
2 ln(2π) −n
2 ln(σ2) −
1
2σ2
n
X
i=1
(xi −µ)2.
Here n is the number of examples and f(x | (µ, σ2)) is the normal probability density
function. Derivatives with respect to µ and σ2 are taken and one obtains the following
maximum likelihood estimates:
ˆµ = ¯x = 1
n
n
X
i=1
xi,
ˆσ2 = 1
n
n
X
i=1
(xi −¯x).
Any object more than three standard deviations from the mean of the estimated distri-
bution is considered to be an outlier.
5. Another method is to compute the interquartile range (IQR). The ﬁrst quartile Q1 is
the point dividing the lower 25% of the data from the upper 75%, and the third quartile
Q3 is the point dividing the lower 75% of the data from the upper 25%. The IQR is
deﬁned as Q3 −Q1. Any point that is more than 1.5 × IQR below the lower quartile Q1
or 1.5 × IQR above the upper quartile Q3 is considered to be an outlier [HaKaPe12].
6. Grubb’s test is a straightforward statistical method for ﬁnding univariate outliers
[HaKaPe12]. A z-score is computed for each point x in the dataset using
z = | x −¯x |
s
,
where ¯x is the sample mean and s is the sample standard deviation of the given data. A
point x is considered an outlier if
z ≥N −1
√
N
v
u
u
t
t2
α/(2N),N−2
N −2 + t2
α/(2N),N−2
,
where t2
α/(2N),N−2 is the value taken by a t-distribution at a signiﬁcance level of α/(2N),
with N being the number of points in the dataset.
7. Multivariate outlier detection can be done using the Mahalanobis distance measure
[HaKaPe12]. For a multivariate dataset, let ¯o be the mean vector. For a point o in the
dataset, the Mahalanobis distance from o to ¯o is given by
MDist(o, ¯o) = (o −¯o)T S−1(o −¯o),

Section 19.5
OUTLIER DETECTION
1419
where S is the covariance matrix. The following steps are needed for outlier detection:
• Calculate the mean vector from the multivariate dataset.
• For each point o, calculate MDist(o, ¯o), the Mahalanobis distance from o to ¯o.
• Detect outliers using Grubb’s test on the set {MDist(o, ¯o) | o ∈D}.
• If MDist(o, ¯o) represents an outlier then o is an outlier.
8. Multivariate outlier detection can also be done using the χ2 statistic [HaKaPe12].
9. A nonparametric method can be used to construct a histogram from the input data.
The bins can be of equal width or equal depth. If a point falls within a bin, then it is
considered normal. If it falls outside the bins, it is considered an outlier [HaKaPe12].
10. A proximity-based approach can be based on the concept of distance-based outliers.
Given a set D of data items, the user speciﬁes a distance threshold r to deﬁne a neigh-
borhood of a point. For each point o, the number of points within the r-neighborhood is
determined. If most of the points in the dataset are far away from o, then the point o is
an outlier. Speciﬁcally, given a distance threshold r ≥0, a fraction threshold 0 < π ≤1,
and a distance measure dist(x, y), a point o is a distance-based outlier if
∥{o′ | dist(o, o′) ≤r} ∥
|D|
≤π.
An algorithm for ﬁnding distance-based outliers using this notion is given in [HaKaPe12].
11. Most data mining software packages support outlier detection.
Some worthy of
special mention are the following:
• Scikit-learn, bundled with many Python data science distributions, contains
outlier detection methods (http://scikit-learn.org/stable/modules/outlier
detection.html)
• Weka, a Java-based data mining kit, has tools to detect and to remove outliers
and extreme values (http://www.cs.waikato.ac.nz/ml/weka/)
• ELKI is a data mining software written in Java with an emphasis on unsuper-
vised methods in outlier analysis (https://elki-project.github.io/)
12. The Outlier Detection DataSets (ODDS) collection includes a large collection of
datasets from diﬀerent domains (http://odds.cs.stonybrook.edu). For instance, it
contains the YelpCHI database of reviews of hotels and restaurants in the Chicago area.
Yelp has a ﬁltering algorithm that identiﬁes reviews as fake or suspicious and puts them
in a ﬁltered list. Of the 67,395 reviews in this database, 13.23% have been put in the
ﬁltered list.
Examples:
1. Outlier detection can be used to identify credit card fraud.
For this application,
outliers represent unusual purchases or patterns of purchases in credit card activity.
2. Network intrusions can be detected using outlier detection by determining whether
TCP connection data is dissimilar from normal traﬃc.
3. Insider stock trading has been detected using techniques from outlier analysis.
4. Outlier detection has many applications in image processing, including mammogra-
phy and video surveillance.

1420
Chapter 19
DATA MINING
5. (Adapted from [HaKaPe12]) Suppose that a city’s average July temperatures over
ten years are 24.0◦, 28.9◦, 28.9◦, 29.0◦, 29.1◦, 29.1◦, 29.2◦, 29.2◦, 29.3◦, 29.4◦. Assume that
the average temperatures follow a normal distribution determined by the mean µ and
the standard deviation σ. Then the maximum likelihood estimates for the mean and
standard deviation are
ˆµ = 24.0 + 28.9 + 28.9 + 29.0 + 29.1 + 29.1 + 29.2 + 29.2 + 29.3 + 29.4
10
= 28.61
and
ˆσ2 = [(24.0−28.61)2+(28.9−28.61)2+(28.9−28.61)2+(29.0−28.61)2+(29.1−28.61)2+
(29.1−28.61)2+(29.2−28.61)2+(29.2−28.61)2+(29.3−28.61)2+(29.4−28.61)2]/10≈2.29,
giving ˆσ =
√
2.29 = 1.51. Then, following Fact 4, the value 24.0◦is identiﬁed as an
outlier because it is 4.61◦away from the sample mean and 4.61
1.51 = 3.04 > 3.
REFERENCES
Printed Resources:
[Ag14] C. Aggarwal, ed., Data Classiﬁcation, Algorithms and Applications, Chapman
and Hall, 2014.
[Ag15] C. Aggarwal, Data Mining: The Textbook, Springer, 2015.
[Ag17] C. Aggarwal, Outlier Analysis, 2nd ed., Springer, 2017.
[AgRe13] C. Aggarwal and C. Reddy, eds., Data Clustering: Algorithms and Applications,
Chapman and Hall, 2013
[AlEtal09] D. Aloise, A. Deshpande, P. Hansen, and P. Popat, “NP-hardness of Euclidean
sum-of-squares clustering”, Machine Learning 75 (2009), 245–248.
[AlHa09] D. Aloise and P. Hansen, “A branch-and-cut SDP-based algorithm for minimum
sum-of-squares clustering”, Pesquisa Operacional 29 (2009), 503–516.
[An73] M. R. Anderberg, Cluster Analysis for Applications, Academic Press, 1973.
[BaDr89] H. J. Bandelt and A. W. M. Dress, “Weak hierarchies associated with similarity
measures: an additive clustering technique”, Bulletin of Mathematical Biology 51
(1989), 133–166.
[Ba89] F. B. Baulieu, “A classiﬁcation of presence/absence based dissimilarity coeﬃ-
cients”, Journal of Classiﬁcation 6 (1989), 233–246.
[Be82] J. P. Benzecri, “Construction d’une classiﬁcation ascendante hi´erarchique par la
recherche en chaˆıne des voisins r´eciproques”, Les Cahiers de l’Analyse des Donn´ees
VII(2) (1982), 209–218.
[BeDi85] P. Bertrand and E. Diday, “A visual representation of the compatibility be-
tween an order and a dissimilarity index: the pyramids”, Computational Statistics
Quarterly 2 (1985), 31–44.
[BoEtal97] E. Boros, P. L. Hammer, T. Ibaraki, and A. Kogan. “Logical analysis of
numerical data”, Mathematical Programming 79 (1997), 163–190.
[Br78] P. Br¨ucker, “On the complexity of clustering problems”, Lecture Notes in Eco-
nomic and Mathematical Systems 157 (1978), 45–54.

REFERENCES
1421
[Br06] M. J. Brusco, “A repetitive branch-and-bound procedure for minimum within-
cluster sums of squares partitioning”, Psychometrika 71 (2006), 347–363.
[BrSt05] M. J. Brusco and S. Stahl, Branch-and-Bound Applications in Combinatorial
Data Analysis, Springer-Verlag, 2005.
[BrSt07] M. J. Brusco and D. Steinley, “A comparison of heuristic procedures for mini-
mum within-cluster sums of squares partitioning”, Psychometrika 72 (2007), 583–600.
[Ch75] N. Christoﬁdes, Graph Theory: An Algorithmic Approach, Academic Press, 1975.
[Ch97] F. Chung, Spectral Graph Theory, American Mathematical Society, 1997.
[Co71] R. M. Cormack, “A review of classiﬁcation (with discussion)”, Journal of the
Royal Statistical Society A 134 (1971), 321–367.
[DeHa80] M. Delattre and P. Hansen, “Bicriterion cluster analysis”, IEEE Transactions
on Pattern Analysis and Machine Intelligence PAMI-2(4) (1980), 277–291.
[DeDeSo05] G. Desaulniers, J. Desrosiers, and M. M. Solomon, eds., Column Generation,
Springer, 2005.
[Di85] G. Diehr, “Evaluation of a branch-and-bound algorithm for clustering”, SIAM
Journal on Scientiﬁc and Statistical Computing 6 (1985), 268–284.
[dMEtal00] O. du Merle, P. Hansen, B. Jaumard, and N. Mladenovi´c, “An interior point
algorithm for minimum sum-of-squares clustering”, SIAM Journal on Scientiﬁc Com-
puting 21 (2000), 1485–1505.
[FaPiSm96] U. Fayyad, G. Piatesky-Shapiro, and P. Smyt, “From data mining to knowl-
edge discovery in databases”, AI Magazine 1 (1996), 37–54.
[Fi36] R. A. Fisher, “The use of multiple measurements in taxonomic problems”, Annals
of Eugenics VII (1936), 179–188.
[FrRa02] C. Fraley and A. E. Raftery, “Model-based clustering, discriminant analysis,
and density estimation”, Journal of the American Statistical Association 97 (2002),
611–631.
[GrRa69] P. E. Green and V. R. Rao, “A note on proximity measures and cluster analy-
sis”, Journal of Marketing Research 6 (1969), 359–364.
[GrWa89] M. Gr¨otschel and Y. Wakabayashi, “A cutting plane algorithm for a clustering
problem”, Mathematical Programming 45 (1989), 59–96.
[GrWa90] M. Gr¨otschel and Y. Wakabayashi, “Facets of the clique partitioning polytope”,
Mathematical Programming 47 (1990), 367–387.
[GuHaJa91] A. Gu´enoche, P. Hansen, and B. Jaumard, “Eﬃcient algorithms for divisive
hierarchical clustering with the diameter criterion”, Journal of Classiﬁcation 8 (1991),
5–30.
[HaKa92] L. Hagen and A. Kahng, “New spectral methods for ratio cut partitioning and
clustering”, IEEE Transactions on Computer-Aided Design of Integrated Circuits
and Systems 11 (1992), 1074–1085.
[HaKaPe12] J. Han, M. Kamber, and J. Pei, Data Mining: Concepts and Techniques, 3rd
ed., Morgan Kaufmann, 2012.
[HaEtal07] P. Hansen, J. Brimberg, D. Urosevi´c, and N. Mladenovi´c, “Primal-dual vari-
able neighborhood search for the simple plant location problem”, INFORMS Journal
on Computing 19 (2007), 552–564.

1422
Chapter 19
DATA MINING
[HaDe78] P. Hansen and M. Delattre, “Complete-link cluster analysis by graph coloring”,
Journal of the American Statistical Association 73 (1978), 397–403.
[HaJaFr89] P. Hansen, B. Jaumard, and O. Frank, “Maximum sum-of-splits clustering”,
Journal of Classiﬁcation 6 (1989), 177–193.
[HaJaMl98] P. Hansen, B. Jaumard, and N. Mladenovi´c, “Minimum sum of squares clus-
tering in a low dimensional space”, Journal of Classiﬁcation 15 (1998), 37–55.
[HaJaSi96] P. Hansen, B. Jaumard, and B. Simeone, “Espaliers, a generalization of den-
drograms”, Journal of Classiﬁcation 13 (1996), 107–127.
[HaMl01a] P. Hansen and N. Mladenovi´c, “J-means: a new local search heuristic for
minimum sum of squares clustering”, Pattern Recognition 34 (2001), 405–413.
[HaMl01b] P. Hansen and N. Mladenovi´c, “Variable neighborhood search: principles and
applications”, European Journal of Operational Research 130 (2001), 449–467.
[Ha67] E. F. Harding, “The number of partitions of a set of n points in k dimensions
induced by hyperplanes”, Proceedings of the Edinburgh Mathematical Society 15
(1967), 285–289.
[Ha80] D. M. Hawkins. Identiﬁcation of Outliers, Chapman and Hall, 1980.
[HoAu04] V. Hodge and J. Austin, “A survey of outlier detection methodologies,” Arti-
ﬁcial Intelligence Review 22 (2004), 85–126.
[InKaIm94] M. Inaba, N. Katoh and H. Imai, “Applications of weighted Voronoi diagrams
and randomization to variance-based k-clustering”, Proceedings of the 10th ACM
Symposium on Computational Geometry, 1994, 332–339.
[Ja91] M. Jambu, Exploratory and Multivariate Data Analysis, Academic Press, 1991.
[Jo02] I. T. Jolliﬀe, Principal Component Analysis, 2nd ed., Springer, 2002.
[KlAr91] G. Klein and J. E. Aronson, “Optimal clustering: a model and method”, Naval
Research Logistics 38 (1991), 447–461.
[KoNaFu75] W. L. G. Koontz, P. M. Narendra, and F. Fukunaga, “A branch and bound
clustering algorithm”, IEEE Transactions on Computers C-24 (1975), 908–915.
[LaWi67] G. N. Lance and W. T. Williams, “A general theory of classiﬁcatory sorting
strategies. 1. Hierarchical systems”, The Computer Journal 9 (1967), 373–380.
[LeRaUl14] J. Leskovec, A. Rajaraman, and J. D. Ullman, Mining of Massive Datasets,
2nd ed., Cambridge University Press, 2014.
[LiSmYo03] G. Linden, B. Smith, and J. York, “Amazon.com recommendations: item-to-
item collaborative ﬁltering”, IEEE Internet Computing 7 (2003), 76–80.
[Ma67] J. B. MacQueen, “Some methods for classiﬁcation and analysis of multivariate
observations”, Proceedings of 5th Berkeley Symposium on Mathematical Statistics
and Probability, 1967, 281–297.
[MaNiVa09] M. Mahajan, P. Nimbhorkar, and K. Varadarajan, “The planar k-means
problem is NP-hard”, Lecture Notes in Computer Science 5431 (2009), 274–285.
[MaRo10] O. Maimon and L. Rokach, Data Mining and Knowledge Discovery Handbook,
2nd ed., Springer, 2010.
[MaBe83] D. W. Matula and L. L. Beck, “Smallest-last ordering and clustering and
graph-coloring algorithms”, Journal of the ACM 30 (1983), 417–427.

REFERENCES
1423
[MeSh01] M. Meila and J. Shi, “Learning segmentation by random walks” in Advances
in Neural Information Processing Systems 12, S. A. Solla, T. K. Leen, and K.-R.
M¨uller (eds.), MIT Press, 2001, 873–879.
[Mi87] B. Mirkin, “Additive clustering and qualitative factor analysis methods for simi-
larity matrices”, Journal of Classiﬁcation 4 (1987), 7–31.
[Mi96] B. Mirkin, Mathematical Classiﬁcation and Clustering, Kluwer, 1996.
[Mi97] T. M. Mitchell, Machine Learning, WCB/McGraw-Hill, 1997.
[Mo91] B. Mohar, “The Laplacian spectrum of graphs” in Graph Theory, Combinatorics,
and Applications, G. Chartrand, O. R. Oellermann, and A. J. Schwenk (eds.), Wiley,
1991, 871–898.
[Mo97] B. Mohar, “Some applications of Laplace eigenvalues of graphs” in Graph Symme-
try: Algebraic Methods and Applications, G. Hahn and G. Sabidussi (eds.), Kluwer,
1997, 225–275.
[MoSu91] C. Monma and S. Suri, “Partitioning points and graphs to minimize the max-
imum or the sum of diameters” in Graph Theory, Combinatorics, and Applications,
G. Chartrand, O. R. Oellermann, and A. J. Schwenk (eds.), Wiley, 1991, 899–912.
[Mu83] F. Murtagh, “A survey of recent advances in hierarchical clustering algorithms”,
The Computer Journal 26 (1983), 329–340.
[NgJoWe02] A. Ng, M. Jordan, and Y. Weiss, “On spectral clustering: analysis and an
algorithm” in Advances in Neural Information Processing Systems, T. Dietterich, S.
Becker, and Z. Guahramani (eds.), MIT Press, 2002, 849–856.
[PaHaLi04] L. Parsons, E. Haque, and H. Liu, “Subspace clustering for high dimensional
data: a review”, SIGKDD Explorations Newsletter 6 (2004), 90–105.
[PeXi05] J. Peng and Y. Xia, “A new theoretical framework for k-means-type clustering”,
Studies in Fuzziness and Soft Computing 180 (2005), 79–96.
[Qu93] J. R. Quinlin, C4.5: Programs for Machine Learning, Morgan Kaufmann, 1983.
[Ra71] M. R. Rao, “Cluster analysis and mathematical programming”, Journal of the
American Statistical Association 66 (1971), 622–626.
[Ro67] P. Rosenstiehl, “L’arbre minimum d’un graphe” in Th´eorie des Graphes, P. Rosen-
stiehl (ed.), Dunod, 1967, 357–368.
RuNo10] S. Russell and P. Norvig, Artiﬁcial Intelligence: A Modern Approach, 3rd ed.,
Prentice Hall, 2010.
[ScKoRi01] J. B. Schafer, J. A. Konstan, and J. Riedl, “E-commerce recommendation
applications”, Data Mining and Knowledge Discovery 5 (2001), 115–153.
[ShMa00] J. Shi and J. Malik, “Normalized cuts and image segmentation”, IEEE Trans-
actions on Pattern Recognition and Machine Intelligence 22 (2000), 888–905.
[SiUp12] K. Singh and S. Upadhaya, “Outlier detection: applications and techniques,”
International Journal of Computer Science Issues 9 (2012), 307–323.
[Sp80] H. Sp¨ath, Cluster Analysis Algorithms for Data Reduction and Classiﬁcation of
Objects, Wiley, 1980.
[Sp85] H. Sp¨ath, Cluster Dissection and Analysis: Theory, FORTRAN Programs, Exam-
ples, Ellis Horwood, 1985.
[TaStKu06] P. Tan, M. Steinbach, and V. Kumar, Introduction to Data Mining, Addison
Wesley, 2006.

1424
Chapter 19
DATA MINING
[vOMe04] B. J. van Os and J. J. Meulman, “Improving dynamic programming strategies
for partitioning”, Journal of Classiﬁcation 21 (2004), 207–230.
[vL07] U. von Luxburg, “A tutorial on spectral clustering”, Statistics and Computing 17
(2007), 395–416.
[We93] G. Wesolowsky, “The Weber problem: history and perspective”, Location Science
1 (1993), 5–23.
[WiFrHa11] I. H. Witten, E. Frank, and M. A. Hall, Data Mining: Practical Machine
Learning Tools and Techniques, 3rd ed., Morgan Kaufmann, 2011.
[WuEtal07] X. Wu et al., “Top 10 algorithms in data mining”, Knowledge and Information
Systems 14 (2008), 1–37.
[XiPe05] Y. Xia and J. Peng, “A cutting algorithm for the minimum sum-of-squared
error clustering”, Proceedings of the SIAM International Data Mining Conference,
2005, 150–160.
Web Resources:
http://archive.ics.uci.edu/ml/ (UCI Machine Learning Repository of benchmark
datasets for clustering algorithms.)
http://id3alg.altervista.org (Implementation of ID3 algorithm in C.)
http://www.alglib.net/dataanalysis/clustering.php (Implementations of the k-
means algorithm.)
http://www.cis.upenn.edu/~jshi/software/ (MATLAB code for normalized spec-
tral clustering.)
http://www.cs.umd.edu/~mount/Projects/KMeans (C++ code for the k-means algo-
rithm.)
http://www.cs.waikato.ac.nz/ml/weka/ (Machine learning tools for data mining.)
http://www.mathworks.com/matlabcentral/fileexchange/44879-spectral-
clustering (MATLAB code for spectral clustering.)
http://www.mmds.org/ (Online site for the textbook Mining of Massive Datasets by J.
Leskovec, A. Rajaraman, and J. D. Ullman; includes the latest version of the text,
lecture slides, and links to courses.)
https://code.google.com/p/figue/ (Javascript for a variety of clustering algorithms.)

20
DISCRETE BIOMATHEMATICS
20.1 Sequence Alignment
Stephen F. Altschul ∗and
20.1.1 Global and Local Pairwise Alignments
Mihai Pop
20.1.2 Pairwise Alignment Scores
20.1.3 Path Graphs and Optimal Global Pairwise Alignment
20.1.4 Optimal Local Pairwise Alignment
20.1.5 Substitution Matrices
20.1.6 Afﬁne Gap Scores
20.1.7 Sequence Alignment Heuristics
20.1.8 Exact String Matching
20.1.9 Indexing Methods for String Matching
20.2 Phylogenetics
Joseph Rusinko
20.2.1 Phylogenetic Trees
20.2.2 Tree Comparisons
20.2.3 Tree Agreement
20.2.4 Tree Reconstruction
20.2.5 Tree Distributions
20.2.6 Subtrees and Supertrees
20.2.7 Applications
20.3 Discrete-Time Dynamical Systems
Elena Dimitrova
20.3.1 Basic Concepts
20.3.2 Boolean Networks
20.3.3 Cellular Automata
20.3.4 Genetic Algorithms
20.4 Genome Assembly
Andy Jenkins and
20.4.1 Basic Concepts
Matthew Macauley
20.4.2 Overlap Graph Approaches
20.4.3 de Bruijn Graph Methods
20.5 RNA Folding
Qijun He
20.5.1 Basic Concepts
Matthew Macauley and
20.5.2 Combinatorial Models
Svetlana Poznanovi´c
20.5.3 Minimal Free Energy Folding Algorithms
20.5.4 Language-Theoretic Methods
20.6 Combinatorial Neural Codes
Carina Curto and
20.6.1 Basic Concepts
Vladimir Itskov
20.6.2 The Code of a Cover
20.6.3 The Neural Ring and Ideal
∗This author was supported by the Intramural Research Program of the National Institutes of Health,
National Library of Medicine.

1426
Chapter 20
DISCRETE BIOMATHEMATICS
20.6.4 Convex Codes
20.6.5 Feedforward and Hyperplane Codes
20.7 Food Webs and Graphs
Margaret Cozzens
20.7.1 Modeling Predator-Prey Relationships with Food Webs
20.7.2 Trophic Level and Trophic Status
20.7.3 Weighted Food Webs
20.7.4 Competition Graphs
20.7.5 Interval Graphs and Boxicity
20.7.6 Projection Graphs
20.7.7 Open Questions
INTRODUCTION
Modern biology makes use of a variety of discrete and combinatorial methods in order to
better understand biological phenomena at both the microscopic and macroscopic levels.
This chapter explains how such methods are useful in understanding the structure and
properties of the genome (both DNA and RNA structures), gene regulatory networks,
the ﬁrings of neurons, the classiﬁcation and evolution of species, as well as competition
between species. A variety of modern algorithms for analyzing biological data are also
presented in this chapter.
GLOSSARY
alignment (of two sequences): a matching in which individual letters/nulls from each
sequence are placed into correspondence.
alignment score: the sum of scores for aligned pairs of letters/nulls in an alignment.
arc diagram: a representation of an RNA structure in which the bases are placed along
a line and arcs (drawn in the upper-half plane) join base pairs.
basal species (in a food web): species located at the bottom of the food web.
Boolean network: a polynomial dynamical system over F2.
boxicity (of a graph G): the smallest dimension p such that two vertices are connected
by an edge in G if and only if their corresponding p-dimensional boxes overlap.
Burrows-Wheeler Transform BWT (of a string): a reversible transformation of the
string used to perform eﬃcient query searches.
cellular automaton: a lattice of cells, which at each step take on a value based on the
values present in its neighboring cells.
code (of a cover): the combinatorial neural code whose codewords correspond to regions
deﬁned by intersections of open sets in the cover.
combinatorial neural code: a collection of binary codewords in which each binary
digit is interpreted as the state (on/oﬀ) of a neuron.
common enemy graph: a graph whose vertices represent the species in a food web
and whose edges (a, b) signify that species a and species b have a common predator.
competition graph: a graph whose vertices represent the species in a food web and
whose edges (a, b) signify that species a and species b have a common prey.

GLOSSARY
1427
de Bruijn graph: a directed graph whose vertices consist of all length-k sequences
over an alphabet and in which the edge (u, v) indicates that the suﬃx of u equals
the preﬁx of v.
dependency graph (for a ﬁnite dynamical system): a directed graph whose vertices
correspond to variables and whose directed edges indicate functional dependencies
between variables.
discrete-time dynamical system: a system based on variables x1, x2, . . . , xn and state
transition functions for each variable xi.
DNA segment: a sequence of bases on a DNA strand, represented as a string over the
alphabet {A, C, G, T}.
DNA target sequence: a long unknown DNA sequence that is to be decoded.
Euler path (in a graph): a path that traverses every edge of the graph exactly once.
expanded sequence: a sequence into which null characters may be inserted.
food web: a directed graph whose vertices correspond to species and whose arcs corre-
spond to direct predator/prey relationships.
genetic algorithm: a heuristic search algorithm that mimics natural biological evolu-
tion, whereby a population of candidate solutions evolves over time via mutation and
recombination.
genome assembler: a computer program that carries out the gene assembly process
from given input data.
global pairwise alignment (of sequences): an alignment in which all letters and nulls
of the two (expanded) sequences must be aligned.
Hamilton path (in a graph): a path that visits every vertex of the graph exactly once.
hybridization (of a DNA strand): a technique to produce a list of all subsequences of
the DNA strand having some ﬁxed length k.
hyperplane code: a combinatorial code obtained from the output of a one-layer feed-
forward neural network.
KMP algorithm: an eﬃcient algorithm for ﬁnding an exact match between a query
string and a database.
Knudsen-Hein grammar: a probabilistic context-free grammar used to represent RNA
secondary structures.
local pairwise alignment (of sequences): an alignment of arbitrary-length segments
of the two (expanded) sequences.
maximum agreement subtree: a subtree induced by the largest set of common taxa.
nearest neighbor interchange: an exchange of two subtrees appearing on opposite
sides of an internal tree edge.
nerve (of a cover): the simplicial complex deﬁned by the nonempty intersections of the
given open sets.
neural ring: a quotient ring associated with a combinatorial code.
nucleotide: an organic molecule (having a distinguished nitrogenous base) that serves
as a building block for DNA and RNA.
overlap graph: a graph whose vertices represent known DNA strands and whose edges
represent overlaps between these DNA strands.

1428
Chapter 20
DISCRETE BIOMATHEMATICS
overlap-layout-consensus method: a technique for reassembling a DNA strand from
known DNA fragments.
path graph: a representation of possible alignments of two sequences using a rectan-
gular array of nodes joined by directed horizontal, vertical, and diagonal edges.
phylogenetic tree: a tree whose leaves are labeled with taxa (taxonomical units).
polytomy: a vertex in a phylogenetic tree with degree exceeding three.
predator projection graph: a weighted graph based on the competition graph.
prey projection graph: a weighted graph based on the common enemy graph.
RNA sequence: a sequence over the four letter alphabet {A, C, G, U}.
Robinson-Foulds distance: a measure of proximity between two trees, given by the
number of edge-induced partitions occurring in one tree but not the other.
secondary structure: an RNA structure whose arc diagram has no crossings.
seed: a relatively short matching subsequence that serves as an anchor point for larger
matches between two sequences.
shortest common superstring: a shortest string that contains each of a given set of
strings as a consecutive sequence.
shotgun sequencing: a method in which many copies of a DNA target sequence are
broken up into pieces, which are then individually sequenced and recombined.
state-space graph (for an n-dimensional dynamical system): a directed graph whose
vertices represent n-tuple states and whose edges represent transitions between such
states.
substitution matrix: a prescription of the score produced when two speciﬁed letters
are aligned.
suﬃx array (for a string): a lexicographically sorted array of all suﬃxes of the string.
suﬃx tree (for a string): a compact representation of all suﬃxes of the string.
trophic level (of a species x): a positioning measure based on the length of paths to x
from basal species that x consumes, either directly or indirectly, in a food web.
Yule-Harding distribution: a model for generating all rooted binary trees based on
certain rules for speciation.
20.1
SEQUENCE ALIGNMENT
Alignments are a powerful way to compare related DNA or protein sequences. They
can be used to capture various facts about the sequences aligned, such as common evo-
lutionary descent or common structural function. We take the general view that the
alignment of letters from two or multiple sequences represents the hypothesis that they
are descended from a common ancestral sequence.
DNA molecules are composed of chains of nucleotides, and protein molecules are com-
posed of chains of amino acids. The speciﬁc order of nucleotides or amino acids within
these chains are respectively called DNA and protein sequences. Perhaps chief among the

Section 20.1
SEQUENCE ALIGNMENT
1429
various biological functions of DNA sequences is to encode protein sequences, because
proteins are involved in most of the biological functions of living cells.
DNA sequences, and the protein sequences they encode, evolve by mutation followed by
natural selection. There are a variety of mechanisms for DNA mutation, but the most
common result is the substitution of a single nucleotide for another, or the deletion or
insertion of one or several adjacent nucleotides. At the protein level, the most common
resulting mutations are the substitution of one amino acid for another, or the insertion or
deletion of one or multiple adjacent amino acids. There is no simple biological mechanism
for exchanging the order of two letters in a DNA or protein sequence, so an alignment
representing the common descent of two DNA or protein sequences is co-linear, with no
“crossovers” between corresponding letters.
20.1.1
GLOBAL AND LOCAL PAIRWISE ALIGNMENTS
Deﬁnitions:
An alphabet is a ﬁnite set of letters and a sequence S is a ﬁnite string of letters, each
chosen from the alphabet.
A null character, generally represented by the symbol “–”, is a character not in the
alphabet that signiﬁes an absent letter.
Given a sequence S, an expanded sequence S′ is the sequence S with an arbitrary
number of null characters placed at its start, its end, or between any two of its characters.
A global pairwise alignment of sequences S and T is a one-to-one co-linear correspon-
dence of expanded sequences S′ and T ′, such that no nulls from S′ and T ′ correspond.
A local pairwise alignment of sequences S and T is a one-to-one co-linear correspon-
dence of equal-length segments of the expanded sequences S′ and T ′, such that no nulls
from the segments correspond.
Facts:
1. DNA molecules are composed of two strands, each a string of nucleotides, represented
by the four letters A, C, G, T.
2. The strands of a DNA molecule are directional and run in opposite directions. Each
nucleotide in one strand of a DNA molecule is paired, through chemical interaction, with
a complementary nucleotide in the other strand: A always pairs with T, and C with G.
Because of this complementarity, the nucleotide sequence of one strand determines the
nucleotide sequence of the other, and thus a DNA molecule is generally represented by
the nucleotide sequence of one of its strands, with that of the other implied.
3. Proteins are composed of one, or multiple, chemically interacting amino acid chains,
but here we conﬁne our attention to single chains. An amino acid chain is a directional
string of amino acids. There are twenty commonly occurring amino acids abbreviated
by the letters A, C, D, E, F, G, H, I, K, L, M, N, P, Q, R, S, T, V, W, Y.
4. A particular amino acid is encoded by three adjacent DNA nucleotides, called a
codon; for example, the codon ATG represents the amino acid methionine, or M. Some
amino acids are encoded by only one codon, whereas others by as many as six. The table
describing the correspondence between all 43 = 64 possible codons and the 20 amino
acids they represent is called the genetic code; except for minor variations, it is universal
to all life on earth. The genetic code contains three stop codons that represent no amino
acid.

1430
Chapter 20
DISCRETE BIOMATHEMATICS
Examples:
1. One possible global alignment of the protein sequences VHLTPEEKSAVTALWG and
VAFTEKQEALVSSSLEAF is
VHLT--PEEKSAV-TALWG-
VAFTEKQEA--LVSSSLEAF
2. One possible local alignment of the DNA sequences GTTACTTTGGACCCTCAA
and AAATTGATCTTTTAAC is
TTTGGACC
T-T-GATC
20.1.2
PAIRWISE ALIGNMENT SCORES
In order to select among the many possible global or local alignments of two sequences,
it is useful to assign an objective function, or score, to each possible alignment. We then
seek an alignment with an optimal score, using the convention that higher scores are
better. The simplest way to score an alignment is to specify scores for aligning particular
letters to one another, or for aligning letters to nulls, and then to deﬁne the score of an
alignment as the sum of the scores of all its aligned letters and nulls.
Deﬁnitions:
A column of a pairwise alignment is the one-to-one correspondence of a single letter (or
null) in one sequence with a single letter (or null) in the other.
A substitution is a column that aligns two letters. A substitution score is a score
deﬁned for a substitution involving a particular pair of letters.
An indel is a column that aligns a letter with a null. An indel score is the score deﬁned
for aligning a letter with a null.
A gap of length k is composed of k adjacent indels, each of which contains a letter
from one expanded sequence and a null from the other.
The alignment score is the sum of the substitution and the indel scores of an alignment’s
columns.
An optimal alignment is an alignment with maximum score.
Facts:
1. An alignment of two identical letters is still termed a substitution.
2. The term indel represents an abbreviation for “insertion or deletion”.
3. Indel scores are usually chosen to be independent of the particular letter aligned with
a null, but this restriction is not necessary.
4. The deﬁnition of an alignment score will be generalized later (see §20.1.6).
Example:
1. Consider the following alignment of two DNA sequences:
TGA-CG
-GTACC

Section 20.1
SEQUENCE ALIGNMENT
1431
Suppose the substitution scores for all columns aligning identical letters are +5 and for
all columns aligning mismatching letters are −1. Also, suppose all indel scores are −2.
Then the alignment score for this DNA alignment is −2 + 5 −1 −2 + 5 −1 = 4.
20.1.3
PATH GRAPHS AND OPTIMAL GLOBAL PAIRWISE ALIGNMENT
The most fruitful algorithms for pairwise sequence alignment are based on the concept
of a path graph.
Deﬁnitions:
Suppose we have two sequences of lengths m and n, respectively.
The path graph
consists of a rectangular (m + 1) × (n + 1) array of nodes, with directed horizontal,
vertical, and diagonal edges between adjacent nodes. The m letters of the ﬁrst sequence
are placed between successive rows of this array and the n letters of the second sequence
are placed between successive columns of this array. Diagonal edges of the path graph
correspond to substitutions, while horizontal and vertical edges correspond to indels.
Scores for the directed edges derive from the corresponding substitution and indel scores.
Facts:
1. The rectangular array underlying the path graph is composed of m × n cells. Each
cell is associated with a letter of the ﬁrst sequence and a letter of the second sequence.
The diagonal edge (ց) in this cell has weight corresponding to the substitution score for
these two letters. The lower horizontal edge (→) in a cell corresponds to a null in the
ﬁrst expanded sequence, and the rightmost vertical edge (↓) in a cell corresponds to a
null in the second expanded sequence; in each case, the null follows the letter that the
diagonal edge signiﬁes aligning.
2. There is a one-to-one correspondence between alignments and directed paths begin-
ning at the upper left node (0, 0) of the path graph and ending at the lower right node
(m, n). The score of an alignment equals the sum of the edge scores along its correspond-
ing path.
3. The problem of ﬁnding an optimal global alignment is then transformed into ﬁnding
an optimal path through the path graph.
4. Although the number of possible paths through the path graph grows exponentially
with m and n, optimal alignments can be found without examining all paths.
5. Algorithm 1 uses a dynamic programming approach to ﬁnd optimal paths in an
eﬃcient way. In brief, there are at most three nodes at locations (i −1, j), (i, j −1), (i −
1, j −1) with edges leading into a particular node at location (i, j). Therefore, if one
knows the score of an optimal path from the origin to each of these three nodes, one can
easily calculate the score of an optimal path through each of these nodes into (i, j). The
highest score among these paths is then the score of an optimal path to (i, j). Optimal
alignment scores for each node in a path graph may thus be calculated by simply ﬁlling
in scores for the nodes sequentially, beginning at the origin, and proceeding from left to
right along each row in turn [NeWu70], [Sa72], [Se74].
Algorithm 1:
Global pairwise alignment algorithm.
input: two sequences of lengths m and n, respectively
output: the optimal global alignment score as well as optimal alignments

1432
Chapter 20
DISCRETE BIOMATHEMATICS
create the corresponding path graph G with (m + 1) × (n + 1) nodes, indexed from
0 to m and from 0 to n
label the upper left node of G with 0
label the rest of the ﬁrst row and the ﬁrst column of G, based on the indel scores
for each remaining node in location (i, j) of the array compute
Vscore := label(i −1, j) + score of edge from (i −1, j) to (i, j)
Hscore := label(i, j −1) + score of edge from (i, j −1) to (i, j)
Dscore := label(i −1, j −1) + score of edge from (i −1, j −1) to (i, j)
label(i, j) := max{Vscore, Hscore, Dscore}; mark any edge achieving this
maximum value
label(m, n) represents the optimal global alignment score and the marked edges
can be used to retrieve alignments achieving this optimal score
6. In Algorithm 1, the values Vscore, Hscore, and Dscore represent the scores of paths
using (respectively) the vertical, horizontal, and diagonal edges entering the node located
at (i, j). By tracing back along the marked edges, starting from the lower right corner
of G, one can obtain optimal alignments for the given pair of sequences.
7. To align sequences of lengths m and n, the time and space complexity of Algorithm 1
are each O(mn). The space complexity can be reduced to O(min{m, n}) [MyMi88].
8. There are various methods for speeding up the algorithm by avoiding the considera-
tion of nodes through which an optimal alignment cannot possibly pass [Fi84], [Sp89].
9. Optimal alignments are unaltered by multiplying all substitution and indel scores
by a positive constant. Optimal alignments are unaltered by adding a constant a to all
substitution scores, and a/2 to all indel scores.
Examples:
1. We wish to align the sequences S1 = ACGC and S2 = GACTAC. The corresponding
path graph G has nodes arranged in an (m + 1) × (n + 1) = 5 × 7 rectangular array.
The four letters of the ﬁrst sequence are placed between successive rows of G and the six
letters of the second sequence are placed between successive columns of G. The shaded
cell corresponds to aligning the ﬁrst letter C of the ﬁrst sequence and the second letter
A of the second sequence.
The highlighted path (from the upper left node to the lower right node in G) corresponds
to the alignment
A-CG--C
GAC-TAC

Section 20.1
SEQUENCE ALIGNMENT
1433
2. Suppose that substitution scores are +1 for matches and 0 for mismatches, and that
indel scores are −1. Then the alignment in Example 1 has score 0−1+1−1−1−1+1 = −2.
This is not, however, an optimal global alignment of S1 and S2.
3. Optimal alignments, based on the substitution and indel scores speciﬁed in Example
2, can be found using Algorithm 1. After labeling the upper left node with 0, we label
nodes in the ﬁrst row and ﬁrst column using −1, −2, −3, . . . since the indel scores are −1.
Each remaining node is labeled with the maximum sum of node label plus edge score,
taken over the three nodes adjacent to and leading into the given node.
For example, to label the node at location (2, 1) we consider its three incident edges:
(1, 1) →(2, 1): label of (1, 1) + edge score (indel) = 0 −1 = −1
(2, 0) →(2, 1): label of (2, 0) + edge score (indel) = −2 −1 = −3
(1, 0) →(2, 1): label of (1, 0) + edge score (mismatch C, G) = −1 + 0 = −1
Node (2, 1) thus receives as its label the maximum sum −1. The following ﬁgure shows
all node labels obtained by following Algorithm 1. Since the label of the lower right node
(4, 6) is 1, this represents the optimal global alignment score.
4. In order to reconstruct the alignment or alignments corresponding to this optimal
score, the edge or edges corresponding to the best path into each node have been marked.
In Example 3, the edges (1, 1) and (1, 0) both give the maximum label −1 and so are
marked. All edges marked during the execution of Algorithm 1 are shown in the following
path graph.
5. We can then trace back along marked edges from the terminal node (5, 7) to the
origin node (1, 1), giving the highlighted edges shown in the following ﬁgure. From these
two paths, we obtain the two optimal alignments of S1 and S2, each with (maximum)
score = +1:
-ACG-C
-AC-GC
GACTAC
GACTAC

1434
Chapter 20
DISCRETE BIOMATHEMATICS
20.1.4
OPTIMAL LOCAL PAIRWISE ALIGNMENT
Many protein or DNA sequences are related only across subsequences, but not over their
entire lengths. To recognize these relationships, it is useful to introduce the concept of
a local alignment (§20.1.1) and to describe algorithms capable of ﬁnding optimal local
alignments.
Facts:
1. Algorithm 1 from the previous section can be modiﬁed [SmWa81] to ﬁnd optimal
local alignment(s) of two sequences by making a few minor modiﬁcations.
• Allow a path to start, with score 0, at any node within the path graph, not just at
the origin (top left) node. Thus, if all paths into a given a node have negative
score, none of them is chosen as optimal.
• Record which node within the path graph receives the highest score, and start
tracing back from there, rather than from the terminal node.
• Terminate the traceback when a node with score 0 is reached.
2. To align sequences of lengths m and n, the time and space complexity for this
local alignment algorithm are each O(mn). The space complexity can be reduced to
O(min{m, n}) [MyMi88].
3. Optimal local alignments are unaltered by multiplying all substitution and indel
scores by a positive constant.
4. In addition to optimal local alignment(s), one may wish to ﬁnd other local alignments
that align regions of the sequences in question that are not implicated in the optimal
local alignment. One may deﬁne “locally optimal” local alignments as those whose path
within the path graph has a score greater than or equal to the score for the path of any
local alignment that shares a common edge or node [Se84]. All such locally optimal local
alignments can also be found in O(mn) time [AlEr86b].
Examples:
1. We wish to align the sequences S1 = ACGC and S2 = GATTGA. We construct a
path graph with (m+1)×(n+1) = 5×7 nodes. Suppose that substitution scores are +4
for matches and −1 for mismatches, and that indel scores are −2. The following ﬁgure
shows the resulting labels for locally aligning these sequences, with only marked edges
shown.

Section 20.1
SEQUENCE ALIGNMENT
1435
2. The largest score obtained is 5 so this is the optimal local alignment score. Tracing
back from this location (4, 6) yields the highlighted edges shown next.
As a result, we ﬁnd two optimal local alignments, each with score 5:
AC-G
A-CG
ATTG
ATTG
20.1.5
SUBSTITUTION MATRICES
Although the algorithms described in §20.1.3 and §20.1.4 are unaﬀected by the choice of
substitution scores, the quality of the alignments they return depends strongly on which
scores are used. In particular, this subsection discusses the use of certain substitution
matrices to specify scoring values.
Deﬁnitions:
A substitution matrix S = (sij) prescribes the score value when letters i and j are
aligned.
A log-odds matrix S is deﬁned by sij = log qij
pipj . Here qij is the target frequency with
which the letters i and j are estimated to appear in columns from accurate alignments of
related sequences; pi and pj are the background probabilities with which the letters
i and j would appear by chance.
Facts:
1. Many diﬀerent substitution matrices have been proposed, but theory and practice
suggest that the best substitution matrices are log-odds matrices [Al91], [DaScOr78],
[KaAl90].
2. The target frequencies should depend upon the degree of evolutionary divergence
separating the sequences compared, and various methods of estimating these frequencies
for proteins have been described, giving rise to the commonly used PAM [ScDa78] and
BLOSUM [HeHe92] series of substitution matrices.

1436
Chapter 20
DISCRETE BIOMATHEMATICS
20.1.6
AFFINE GAP SCORES
A single mutational event can insert or delete multiple nucleotides into or from a DNA
sequence, and this can result in multiple contiguous amino acids being inserted into or
deleted from the encoded protein sequence. Thus, an alignment in which the several
insertions or deletions are adjacent to one another makes more biological sense than one
in which these insertions and deletions are separated. The scoring systems considered
earlier, however, do not reﬂect this biological fact. One remedy is to modify the alignment
scoring system by deﬁning appropriate scores for gaps of various lengths in place of scores
for individual indels.
Deﬁnition:
Aﬃne gap scores are given by g(k) = −(a + bk), where k is the gap length and a, b are
(generally nonnegative) constants.
Facts:
1. It is evident that when a = 0, the aﬃne gap score g(k) is equivalent to indel scores of
−b. However, when a > 0, the score for k adjacent indels is better than that for k indels
separated by substitution columns.
2. For either global or local alignments, the basic algorithms described in §20.1.3 and
in §20.1.4 must be modiﬁed to accommodate aﬃne gap costs. Fortunately, it is possible
to do this while retaining O(mn) time complexity by remembering not just the score of
the best path into each node, but also the best score for a path that arrives through a
horizontal, vertical, or diagonal edge [AlEr86a], [Go82]. For example, the incremental
score for a path leading out of a node along a horizontal edge is −b for a path that arrived
along a horizontal edge, but is −(a + b) for a path that arrived along any other type of
edge.
3. It is possible to construct O(mn) algorithms based on g(k) that are more involved
than aﬃne gap scores [MiMy88], but these algorithms are fairly complex and in practice
almost never used.
4. The elements of sequence alignment considered here been generalized in a great
variety of ways. These include rapid heuristic database search algorithms [AlEtal90],
[PeLi88]; protein proﬁle construction and comparison [AlEtal97], [GrMcEi87]; and mul-
tiple sequence alignment, for which there is a vast and growing literature.
Example:
1. The practical diﬀerence in using aﬃne gap costs in place of simple indel costs can
be illustrated in the optimal local alignments resulting from the comparison of two pro-
tein sequences. Both alignments shown next employ BLOSUM-62 substitution scores
[HeHe92], but the ﬁrst alignment uses indel scores of −6, while the second uses gap
scores of g(k) = −(11 + k). Identical letters are echoed on the central lines of these
alignments, and the sequence positions of the letters at the start and end of each line are
provided. The second alignment contains many fewer gaps, but also a smaller number of
identical aligned letters.

Section 20.1
SEQUENCE ALIGNMENT
1437
Alignment 1:
49 CERTLKYFLGIAGGKWVVSYFWVTQSIKERKMLNEHDFEVRGDVVNGRNHQGPKRARESQDRK-IFRGLEICCYG
122
C RT KYFL
A G
VS
WV
S
N
G
R
Q R
F
L
865 C-RTRKYFLCLASGIPCVSHVWVHDSCHANQLQNYRNY-L---LPAGYSLE-EQRILDWQPRENPFQNLKVLLVS
933
123 PFTNMPTDQLEWM-VQLC-GASVVKE-LSS-FT--LGTGVHPIVVVQPDAWTEDNGFHAIGQMCEAPVVTREWVL
191
L W
GA
VK
SS
GV
VV
P
PVV
EWV
934 D-QQQNFLEL-WSEILMTGGAASVKQhHSSAHNKDIALGVFDVVVTDPSC-PA-SVLKC-AEALQLPVVSQEWVI 1003
Alignment 2:
51 RTLKYFLGIAGGKWVVSYFWVTQSIKERKMLNEHDFEVRGDVVNGRNHQGPKRARESQDRK-IFRGLEICCYG
122
RT KYFL
A G
VS
WV
S
N
R
Q R
F
L
866 RTRKYFLCLASGIPCVSHVWVHDSCHANQLQNYRNY-----LLPAGYSLEEQRILDWQPRENPFQNLKVLLVS
933
123 PFTNMPTDQLEWMVQLCGASVVKELSSFT----LGTGVHPIVVVQPDAWTEDNGFHAIGQMCEAPVVTREWVL
191
GA
VK
S
GV
VV
P
PVV
EWV
934 DQQQNFLELWSEILMTGGAASVKQHHSSAHNKDIALGVFDVVVTDPSC---PASVLKCAEALQLPVVSQEWVI
1003
20.1.7
SEQUENCE ALIGNMENT HEURISTICS
Sequence alignment, as described so far, is computationally intensive, with the algorithms
requiring an execution time quadratic in the size of the sequences being aligned. In the
general case this run time is unavoidable. However, faster execution times can be obtained
when the sequences being aligned have high levels of similarity. Here we discuss banded
methods, exclusion methods, and the use of seeds to initiate searches.
Deﬁnitions:
Suppose that G is the path graph for the alignment of two sequences. The diagonal of
G consists of all nodes at locations (0, 0), (1, 1), . . ., (m, n). A band of width d consists
of all nodes within (Manhattan) distance d of the diagonal of G.
A k-mer is a subsequence of length k found within a given sequence (see §20.4.1).
A seed is a relatively short matching subsequence that serves as an anchor point for
larger matches between sequences. A spaced seed is a k-mer within which a number t
of ﬁxed positions (denoted by ∗) are allowed to match any letter; the spaced seed then
has width k and weight k −t.
Facts:
1. Suppose that two given sequences diﬀer by at most d edits (at most d letters are
changed, inserted, or deleted between the two sequences).
Then an optimal path in
the path graph G cannot pass through nodes that are further than distance d from the
diagonal of G as each “move” away from the diagonal implies an additional diﬀerence
being incorporated in the alignment.
2. By Fact 1 it is suﬃcient to compute the alignment scores for just a subset of the
nodes in G, speciﬁcally for nodes in the band of width d.
Using this banded align-
ment algorithm, an optimal alignment can then be determined in O(min{m, n}d) time
[ChPeMi92].
3. The value d does not need to be known a priori. Rather, an algorithm that adapts
dynamically to the actual similarity between the sequences can be devised by performing

1438
Chapter 20
DISCRETE BIOMATHEMATICS
a binary search for the band that encompasses the optimal path through the path graph.
Speciﬁcally, one starts with an initial guess for d, e.g., by setting d = 1. The banded
alignment algorithm is then executed to ﬁnd an alignment that occurs within the band
of width d. If the resulting alignment contains fewer than d edits, then this is also the
optimal alignment. If the alignment contains more than d edits, then d is doubled, and
the process is repeated until an optimal alignment is found.
4. The dynamic algorithm presented in Fact 3 also takes O(min{m, n}d) time, though
it can be up to four times slower than an algorithm that starts with an initial correct
guess for the value of d.
5. A special case of alignment occurs when one sequence is much longer than the other
one, e.g., when aligning one short sequence against an entire genome, or when aligning a
sequence against a database. In such situations, a run time proportional to the product
of the lengths of the two sequences can be prohibitive. Heuristics have been developed
that can focus on the regions of the alignment table that contain high-quality alignments.
6. Suppose we are searching for an alignment of a (query) sequence of length m within
a longer sequence (database) of length n, where m ≪n. Speciﬁcally, we are looking for
alignments with up to d diﬀerences (indels or substitutions). Then any inexact alignment
with d or fewer diﬀerences must contain an exact match between the query sequence and
the database that is of length at least ⌊m/(d + 1)⌋. Indeed, the worst-case scenario has
the d diﬀerences equally separated along the sequence, breaking it up into d+1 segments.
7. Exclusion methods: These approaches, based on Fact 6, use exact matching to speed
up inexact alignment [WuMa92]. First we identify all exact matches of length ⌊m/(d+1)⌋
between the two strings, corresponding to diagonal segments within the path graph. Then
we perform the banded alignment procedure to ﬁnd the highest scoring alignment within
the neighborhood of the exact match. This procedure is guaranteed to ﬁnd an optimal
alignment with fewer than d edits, if one exists, and is faster than the full alignment
algorithm as only a small fraction of the entire path graph needs to be explored. Regions
of the path graph that cannot contain good alignments are excluded from consideration.
8. A key factor determining the speed-up we can expect is the length of the exact match
being sought: k = ⌊m/(d+1)⌋. For small values of k (i.e., when the error being tolerated
is high with respect to the size of the query sequence), the expected number of k-mers
that yield a match between the sequence and the database can be very high, making this
approach ineﬃcient.
9. The size of k speciﬁed in Fact 8 is unnecessarily conservative as it derives from a worst-
case scenario where errors are equally distributed throughout the sequence. A random
distribution of errors within the sequence is unlikely to be uniform and so contains exact
segments that can be much longer than ⌊m/(d + 1)⌋.
10. The value of k could be determined from theoretical principles, e.g., by setting k to
be the expected length of the longest edit-free segment within an alignment of length m
that contains d randomly distributed edits. In practice, the exact value of d is usually
not known, and k is set empirically to a value that minimizes the number of alignments
that need to be performed without missing too many alignments.
11. To increase sensitivity without sacriﬁcing speed, Ma et al. [MaTrLi02] suggested the
use of spaced seeds. The intuition behind the use of spaced seeds is that the “don’t care”
symbols allow the seed to match even if a diﬀerence occurs in the middle of the seed.
12. Spaced seeds have a higher sensitivity than the exact k-mer seeds of the same weight
without a corresponding increase in the number of matches that need to be explored. In
practice, algorithms relying on spaced seeds use a combination of multiple seeds in order

Section 20.1
SEQUENCE ALIGNMENT
1439
to maximize sensitivity. Finding the optimal combination of seeds is NP-hard [MaLi07];
however simple heuristic algorithms are highly eﬀective in practice [IlIl07].
13. Spaced seeds can be generalized by extending the original observation underlying
all exclusion methods—that any alignment of high enough ﬁdelity must contain a long
enough exact match between the query and the database. It can be shown [GhPo09],
[KaNa07] that any alignment of high enough ﬁdelity must contain a long enough inexact
alignment of higher ﬁdelity than the original one. Speciﬁcally, if strings L1 and L2 of
length ℓmatch with an edit distance k, there exist substrings E1 in L1 and E2 in L2
of length e such that the two substrings match with an edit distance no greater than
k/(ℓ−e) [GhPo09].
14. Using Fact 13, one could search for inexact seeds that match between a query
and a database using a fast inexact alignment algorithm that can tolerate a limited
number of edits, and then use the resulting anchors to search for the full alignment. This
generalization provides a higher sensitivity than approaches based on spaced seeds.
Examples:
1. The following ﬁgure illustrates a path graph with a band of width d = 1 indicated.
An optimal alignment with at most d edits must occur within a band of width d on either
side of the diagonal. Paths traversing the grayed nodes involve more than d edits.
2. In the worst-case scenario, the d edits are equally spaced along the sequence, implying
that exact matches between the two strings are at least ⌊m/(d + 1)⌋characters long:
3. The spaced seed 111∗111 denotes the set of k-mers of width 7 and weight 6. This
seed allows the middle nucleotide to diﬀer between the query sequence and the database
sequence. In the following two strings, a spaced seed of width 7 is shared by two strings.
Only the characters aligned to 1s in the spaced seed need to match between the two
strings.
TATT
GTCT
CGTCTGA
111*111
CGTATGA
TCAT
TGCT

1440
Chapter 20
DISCRETE BIOMATHEMATICS
20.1.8
EXACT STRING MATCHING
Underlying the exclusion methods described in §20.1.7 is the assumption that ﬁnding
exact matches between two strings is computationally more eﬃcient than ﬁnding inexact
alignments. Exact string matching is of independent interest as well in many applications
(e.g., web search engines largely rely on exact matching of query strings). As such, the
problem has been extensively researched and many variants and algorithms have been
described in the literature. Here we highlight several key results.
Deﬁnitions:
Given a string S, a suﬃx tree is a compact representation of all suﬃxes in S. It is a
tree that contains a leaf for each suﬃx in S, and where each edge is labeled with a string
of characters so that the path from the root to each leaf spells the corresponding suﬃx.
The string spelled by the path from the root of a suﬃx tree to an internal node is termed
the string label of the node.
The suﬃx tree also contains a collection of suﬃx links, special edges connecting the
node in the tree with string label cα (where c is a single character and α is a string) to
the node with string label α.
The suﬃx array is a lexicographically sorted array of all suﬃxes of the string, together
with information necessary for speeding up the search process.
Facts:
1. A naive search for exact matches between a query string of length m and a database
of length n requires O(mn) run time, by checking whether all m letters in the query
match, starting at each of the n −m + 1 positions within the database.
2. KMP algorithm: Knuth, Morris, and Pratt [KnMoPr77] reduced the run time for
exact matching to O(m + n) by simple preprocessing of the query string. Speciﬁcally,
at each position i in the query, the KMP algorithm deﬁnes spi to be the length of the
longest nontrivial preﬁx of the query that matches a suﬃx of the string that ends at
position i.
3. This information can be computed in O(m) time and can be used to speed up align-
ment as follows. The query is compared to the database, one letter at a time, until a
mismatch is detected at position i + 1 in the query. If spi is deﬁned, the query is shifted
to the right so that the preﬁx of the query aligns to the corresponding suﬃx of the string
ending at position i prior to the shift. The comparison to the database continues from
position spi + 1, as the properties of the spi values guarantee that all prior positions
match the database sequence (these were already matched prior to the shift). If spi = 0,
the query is shifted such that its ﬁrst letter is aligned to the character mismatched prior
to the shift, and the comparison between the query and the database starts from the ﬁrst
letter of the pattern.
4. Two observations establish an O(n) run time for the search procedure, giving an
O(m + n) run time once the cost of preprocessing is taken into account.
• Once a character in the database is successfully matched to the query it is never
again compared to the query, thereby bounding the total number of exact
matches performed during the execution of the algorithm to n, the size of the
database.
• Every mismatch triggers a shift of the query by at least one position; thus the
number of mismatches is also bounded by n, yielding an overall O(n) run time.

Section 20.1
SEQUENCE ALIGNMENT
1441
5. The KMP algorithm can be used to ﬁnd full-length matches of a query against a
database, as well as matches that involve just a preﬁx of the query (e.g., ﬁnding the
longest preﬁx match in case a full-length match cannot be found).
To address more
complex exact matching problems, Weiner [We73] and McCreight [Mc76] independently
developed suﬃx trees.
6. The strings represented by the path from the root to any node in the suﬃx tree
are unique. In other words, the shared preﬁxes of strings represented in the tree are
represented by the same path from the root to some internal node in the tree.
7. To ensure that the suﬃxes of S reach the leaves of the tree (i.e., no suﬃx is a preﬁx of
some other suﬃx in the tree), a special character $ not found in the alphabet is appended
to the end of each string stored in the suﬃx tree.
8. Suﬃx links are useful in the eﬃcient construction of suﬃx trees as well as for solving
certain exact matching problems, such as identifying the longest substring that matches
between the query and the database.
9. When appropriately structured, suﬃx trees require O(m) space to store all the suﬃxes
of a given string of length m. To achieve such space eﬃciency, each edge is labeled not
with its string label, but with the coordinates within the string where the string label
occurs (see Example 2). Each edge in the tree thus requires constant storage space, and
the total size of the tree is determined by the number of leaves, equal to the number of
suﬃxes m.
10. If a query is represented within a suﬃx tree, it can be aligned against a database
as follows. Starting with the root of the tree, the matching process follows the edges of
the tree that match the database sequence until either reaching a leaf (in which case the
corresponding suﬃx represents a partial match between the query and the database) or
a mismatch occurs. The algorithm then follows the suﬃx link starting from the node
immediately above the location of the mismatch and the matching process continues from
the same position in the database, starting from the new location in the tree. Following
the suﬃx link implicitly discards the ﬁrst character in the database and is equivalent to
the shift procedure described in the KMP algorithm.
11. The search procedure requires O(m + n) time, matching the time complexity of the
KMP algorithm.
12. Despite its asymptotically linear space complexity, the suﬃx tree data structure
requires a substantial amount of memory, estimated at over 20 bytes per letter. Manber
and Myers [MaMy93] described an alternative approach for storing all the suﬃxes of a
string in a compact way that uses substantially less space at the cost of a small additional
increase in run time.
13. In a suﬃx array, the suﬃxes themselves are stored as simply an index within the
original string, ensuring that the space usage is proportional to the size of the string
even though the total length of all suﬃxes combined is proportional to the length of the
string.
14. In addition to the suﬃxes, the data structure for a suﬃx array comprises an array
that stores the longest common preﬁx (LCP) between adjacent suﬃxes. This information
is necessary to eﬃciently search within the suﬃx array using binary search with just an
additive overhead of O(log m). A trivial implementation of binary search within an array
of m strings of length O(m) requires O(m log m) time. Since the LCP values are only
used for the binary search process, it is suﬃcient to record just the longest common
preﬁxes of the pairs of strings that will be compared during the binary search, or O(m)
values. Suﬃx arrays thus occupy O(m) space, and a sequence of length n can be searched
against a suﬃx array in O(n + log m) time.

1442
Chapter 20
DISCRETE BIOMATHEMATICS
15. Constructing a suﬃx array involves sorting the set of suﬃxes of a string, a process
that can be performed in O(m log m) time using an algorithm that takes advantage of
the relationship between the suﬃxes of a same string [MaMy93].
Examples:
1. The following diagram illustrates the spi values used by the KMP algorithm and a
shift after a mismatch occurs at position i in the query. The shaded regions correspond
to identical sequences. After the shift, matching continues from the position marked X.
2. The following ﬁgure shows a suﬃx tree for the string BABBAB. The dashed line
is a suﬃx link connecting the node labeled BAB to the node labeled AB. Edge labels
are provided for clarity, but an eﬃcient implementation would compress them to two
integers. For example, the edge BAB$ can be represented as (4, 7), the range within the
original string that spells this string.
20.1.9
INDEXING METHODS FOR STRING MATCHING
When searching for multiple query sequences against the same database, search eﬃciency
can be increased by preprocessing or indexing the database. For the following, assume we
are searching for exact matches between multiple sequences of length m and a database
of length n.
Facts:
1. Storing the database in a suﬃx tree (§20.1.8) enables exact matching queries to be
executed in time independent of the database size. For each query sequence, the matching
process traverses the suﬃx tree from the root, following the path that matches the query
until either a match is found, or a mismatch indicates the query does not match within
the database, yielding a run time of O(m) per query.
2. As discussed in §20.1.8, the main drawback of this strategy is the large memory
necessary to store the suﬃx tree.
3. Inverted index: Finding short exact matches of length k between one or more query
sequences and a database, a component of the exclusion methods described in §20.1.7,

Section 20.1
SEQUENCE ALIGNMENT
1443
can be performed with a simple (inverted) index linking all strings of length k within the
database to their location in the database. Lookups within the index can be performed
eﬃciently if the entire index is stored in memory; however, the memory space necessary
to store all k-mers within a database can be very large. Furthermore, k-mers that occur
frequently within the database impact both memory usage and the time needed to search
the index, and are usually excluded from the index in practical implementations.
4. Burrows-Wheeler transform (BWT): A reversible permutation of the indexed string
can be used to construct a memory-eﬃcient indexing structure called the FM index
[FeMa00]. The Burrows-Wheeler transform [BuWh94] of a string S can be constructed
using Algorithm 2. It is assumed here that the added character $ is lexicographically
smallest.
Algorithm 2:
Burrows-Wheeler transform.
input: string S
output: Burrows-Wheeler transform of S
augment string S by appending to its end a character $ not found in the alphabet
construct a table T comprising all circular rotations of S as rows
sort the rows of T into lexicographic order
return the last column in T as the Burrows-Wheeler transform of S
5. The BWT and the suﬃx array of S are conceptually linked. The BWT represents
the characters that precede the lexicographically ordered suﬃxes of S, and the BWT can
be trivially constructed from the suﬃx array of S. See Example 2.
6. The BWT can be reversed, obtaining the original string, with the help of a simple
observation: the order in which multiple instances of the same character occur in the ﬁrst
column of the sorted BWT table is the same as the order in which these characters occur
in the last column of the table. Speciﬁcally, the ﬁrst A in the last column corresponds to
the same character in the original string as the ﬁrst A in the ﬁrst column, the third C in
the last column corresponds to the same character in the original string as the third C
in the ﬁrst column, etc. This last-to-ﬁrst (LF) mapping guides the reversal of the BWT;
see Algorithm 3.
Algorithm 3:
BWT reversal.
input: BWT table T for S
output: original string S
start with the ﬁrst row of the BWT table and output the character c in the last
column; this is the last character in the original string
repeat until the entire string is reconstructed
use the LF property to identify the corresponding instance of c in the ﬁrst column
prepend the character at the end of the respective row to the string reconstructed
so far
7. In Algorithm 3 only the ﬁrst and the last columns of the BWT matrix are necessary
to reverse the transformation. The last column is the BWT itself, while the ﬁrst column
is simply the lexicographically sorted list of characters in the original string.

1444
Chapter 20
DISCRETE BIOMATHEMATICS
8. To eﬃciently perform the LF mapping it is necessary to know the index of each
character within the last column. That is, for a character c within the ith row of the
matrix we need to know the number of characters equal to c that occur within the rows
above. Storing this information for each row of the matrix allows the LF lookup to occur
in constant time; however, it requires a large amount of memory O(n log n) for a string
of length n. A trade-oﬀbetween run time and memory usage can be obtained by storing
the index information at every bth row, yielding a memory usage of O((n log n)/b). Each
LF mapping, however, requires O(b) time as the index values need to be computed on
the ﬂy within the block between the selected rows. See Example 5.
9. The same procedure used for reversing the BWT can be used to match a query Q
against a database stored in the BWT. This process is similar to a binary search in that
the transitions between the ﬁrst and last columns of the BWT matrix repeatedly shrink
the range within which the query string may be found. See Algorithm 4.
Algorithm 4:
BWT query match.
input: query Q, BWT matrix T
output: matching string in the database
identify the block of rows in T starting with the last character in Q
repeat until the query is fully matched
within the last column of these rows, ﬁnd those rows containing the rightmost
unmatched character in Q; if no rows end in this character, no match has been
found
within the ﬁrst columns of the BWT matrix, identify the rows starting with the
characters found above
Examples:
1. We illustrate the Burrows-Wheeler transform of the string BABBAB. The character
$ is added to this string, giving the ﬁrst row of the table shown on the left; the rows
of this table represent successive circular rotations. After lexicographically sorting the
rows, we obtain the BWT matrix on the right, whose last column is the Burrows-Wheeler
transform of the original string.
2. In the following ﬁgure, the suﬃx array of the string in Example 1 is shown highlighted
in bold within the Burrows-Wheeler matrix.

Section 20.1
SEQUENCE ALIGNMENT
1445
3. We illustrate the LF mapping for the BWT matrix shown in Example 1. The ﬁrst B
in the last column is the fourth B occurring in the original string BABBAB; the ﬁrst B
in the ﬁrst column also corresponds to the fourth B in the original string. The second B
in the last column is the third B occurring in the original string BABBAB; the second
B in the ﬁrst column also corresponds to the third B in the original string.
4. Here we show how to reverse the BWT in Example 1 to generate the original string
BABBAB, starting from the rightmost character. The arrows indicate the LF mapping.
The string being reconstructed is shown above the table with the latest character added
shown in bold. Note that only the ﬁrst and last columns of the table are needed for this
operation.
5. A character index is needed for performing the LF mapping. The full index is shown
on the left. On the right is a sparse index that stores the information every b rows.
6. We illustrate the search process using a BWT/FM-index. The search range is iter-
atively reﬁned as more characters in the query string ABB are being matched, starting
with the rightmost character. The matched characters are shown in bold.

1446
Chapter 20
DISCRETE BIOMATHEMATICS
20.2
PHYLOGENETICS
Phylogenetics is the study of the evolutionary relationships among a collection of organ-
isms. These relationships are displayed in the form of a tree whose leaves are labeled
by the organisms. Internal vertices of the tree represent common ancestors. Biologists
apply statistical and combinatorial techniques to construct trees from data such as DNA
sequences. Phylogenetic trees are used in epidemiology, systematics, ecology, and linguis-
tics. We review the basic combinatorial principles of the discipline. Most of the results
can be found in [SeSt03] and [DrEtal11]. We refer the reader to [Fe04] for a biological
perspective on phylogenetics which is grounded in sound mathematics. Discrete math-
ematicians may enjoy the recent textbook [HuRuSc10] on phylogenetic networks, while
[PaSt05] provides a viewpoint on the subject from the perspective of algebraic geometry.
20.2.1
Phylogenetic Trees
Deﬁnitions:
The leaves of a tree are the vertices incident with a single edge.
The leaves of a phylogenetic tree are labeled with a set of operational taxonomical
units or taxa. A phylogenetic tree may be rooted or unrooted.
An internal edge is not incident with any of the taxa.
A vertex of a phylogenetic tree with degree greater than three is called a polytomy. A
binary tree has no polytomies.
Two leaves form a cherry if they are the only leaves adjacent to a common vertex.
Phylogenetic reconstruction is the process of transforming data about the relation-
ships among a particular set of taxa into a phylogenetic tree which represents the under-
lying relationships.
Facts:
1. Evolution is thought of as a binary process. Therefore most phylogenetic reconstruc-
tions aim to return binary trees.
2. Operational taxonomical units typically refer to a set of organisms or a set of genes.
However, they may also refer to non-living objects such as languages.
3. The input to a phylogenetic reconstruction algorithm is typically an aligned collection
of nucleotide sequences, amino acid sequences, or discrete morphological data.
4. An unrooted binary tree with n ≥3 taxa has n −3 internal edges. A rooted binary
tree has a single additional internal edge.
5. For n taxa the number of rooted binary phylogenetic trees is
(2n −3)!! = 1 · 3 · 5 · 7 . . . (2n −3).
This is also the number of unrooted binary trees with n + 1 taxa. As there are more
57-taxa trees than there are atoms in the visible universe, phylogenetic reconstruction
algorithms are often restricted from considering all possible trees as models.

Section 20.2
PHYLOGENETICS
1447
Example:
1. The following ﬁgure displays a phylogenetic tree with 11 internal edges and 15 taxa
{a, b, . . ., o}. This tree has four cherries (ab, ef, gh and kl) and a single polytomy adjacent
to taxa m, n and o.
f
e
d
a
b
c
o
n
m
g
h
i
l
k
j
20.2.2
Tree Comparisons
When testing phylogenetic reconstruction algorithms, one generates data assuming a
ﬁxed tree and model of evolution. Then one reconstructs trees using this simulated data.
To test the performance of the reconstruction algorithm, the reconstructed trees are then
compared to the ﬁxed tree using a tree metric.
Deﬁnitions:
Every edge in a phylogenetic tree induces a split or partition of the set of taxa into two
disjoint subsets.
The Robinson-Foulds distance between two trees is the number of splits which occur
in one tree but not the other.
Each internal edge divides a binary tree into four subtrees. A nearest neighbor inter-
change exchanges two of the subtrees on opposite sides of the internal edge.
The nearest neighbor interchange distance between two binary trees is the minimum
number of nearest neighbor interchanges required to transform one tree into the other.
Facts:
1. Both the Robinson-Foulds distance and the nearest neighbor interchange distance
deﬁne metrics on the set of n-taxa trees.
2. The Robinson-Foulds distance can be computed in linear time [Da85], while comput-
ing the nearest neighbor interchange distance is an NP-complete problem [LiTrZh96].
3. Asymptotically, the proportion of trees that share s splits with a tree T follows a
Poisson distribution with mean λ =
c
2n, where n is the number of taxa and c is the
number of cherries of T [BrSt09].
4. There exists a Hamilton walk connecting all binary n-taxa trees through a sequence
of nearest neighbor interchanges [GoFoStJ13].
5. For an n-taxa tree, the nearest neighbor distance between any two trees is at most
n log n + O(n) [LiTrZh96].
6. Maximum likelihood reconstruction assigns a likelihood score to each tree. Biologists
select the tree which maximizes the likelihood of observing the data given the particular

1448
Chapter 20
DISCRETE BIOMATHEMATICS
tree. In practice it is not feasible to compute a likelihood score for all possible trees. To
address this issue a guide tree is computed using a fast algorithm. Likelihood scores for
trees near the guide tree under the nearest neighbor interchange distance are compared
to estimate the maximum likelihood tree [TaEtal11].
Examples:
1. Three trees diﬀering by a single nearest neighbor interchange are shown in the fol-
lowing ﬁgure. Here S1, S2, S3, S4 represent subtrees connected by an internal edge e.
S1
S3
e
S2
S4
S2
S3
e
S1
S4
S1
S2
e
S3
S4
2. The tree shown on the left of the next ﬁgure has splits S = {ab|cdef, abc|def, abcd|ef}
The tree pictured on the right has splits S′ = {ab|cdef, abd|cef, abdf|ce}.
a
b
d
f
e
c
a
b
f
c
e
d
Since there are four splits which appear in one tree but not the other (all but abcd|ef)
the Robinson-Foulds distance between these trees is four.
3. The nearest neighbor interchange distance between the trees above is two as is demon-
strated by the sequence of nearest neighbor interchanges shown in the following ﬁgure.
a
b
d
f
e
c
a
b
c
f
e
d
a
b
f
c
e
d
The nearest neighbor interchange distance cannot be smaller than two, since the trees
diﬀer by a Robinson-Foulds distance of four, and each interchange can only change a
single split, so the diﬀerence in the number of splits can change by a maximum of two.
20.2.3
Tree Agreement
There are a variety of algorithms for combining trees on a shared set of taxa. These
methods may seek to reconcile the tree diﬀerences or highlight their similarities.
Deﬁnitions:
Let T be a phylogenetic tree on a set of taxa X. For every subset S ⊆X, the induced
subtree T |S is the tree constructed by taking the unique minimal connected subgraph
of T containing the leaves in S, and then removing all non-root vertices of degree two.

Section 20.2
PHYLOGENETICS
1449
Given trees T1, T2, . . . , Tk, a maximum agreement subtree is the induced subtree on
the largest set S ⊆X such that T1|S = T2|S = · · · = Tk|S. The maximum agreement
subtree need not be unique.
Given trees T1, T2, . . . , Tk on a set of taxa X, the majority rules consensus tree is the
tree T containing exactly the splits which occur in the majority of the trees T1, T2, . . . , Tk.
Facts:
1. For any collection of trees on a set of taxa, both the majority rules consensus tree
and the maximum agreement subtree exist. The majority rules consensus tree contains
all of the taxa, while the maximum agreement subtree typically contains a proper subset
of the taxa.
2. The majority rules consensus tree can be computed in linear time [AmClStJ03], while
the maximum agreement subtree can be computed in O(n log n) time [CoEtal00].
3. Two random binary phylogenetic trees on n taxa share a maximum agreement subtree
on Ω(log n) leaves [BrMcKSt03].
Examples:
1. The induced subtree on the set {a, b, c, d} for the 15-taxa ﬁgure shown next is the
4-taxa tree on the right.
f
e
d
a
b
c
o
n
m
g
h
i
l
k
j
c
a
b
d
2. In the next ﬁgure, a maximum agreement subtree of the two 6-taxa trees pictured on
the left is the 4-taxa tree pictured on the right.
d
b
e
f
a
c
a
b
e
f
d
c
e
f
b
c
3. The majority rules consensus tree will contain all of the taxa. The two trees from
the preceding ﬁgure share only the split (abcd|ef) so their majority rules consensus tree
is forced to contain a polytomy, as pictured on the following page.
20.2.4
Tree Reconstruction
Distance methods of phylogenetic reconstruction algorithms were among the ﬁrst applied
to DNA sequence data and remain in use due to their speed and simplicity.

1450
Chapter 20
DISCRETE BIOMATHEMATICS
f
e
a
b
c
d
Deﬁnitions:
One may assign a positive branch length to each edge of a phylogenetic tree.
Given a collection of n taxa, a dissimilarity matrix is a nonnegative, symmetric n × n
matrix where entry ai,j indicates the diﬀerence between taxa i and taxa j. We assume
ai,i = 0 for all 1 ≤i ≤n.
A dissimilarity matrix is a metric if ai,k ≤ai,j + aj,k for all i, j, k.
A dissimilarity matrix is a tree metric if there exists a phylogenetic tree T such that
for each pair i,j the matrix entry ai,j is the sum of the branch lengths along the unique
path from leaf i to leaf j.
A dissimilarity matrix is ultra-metric if it is a tree metric for a rooted tree T in which
every leaf is equidistant from the root.
Facts:
1. A dissimilarity matrix is frequently computed using a modiﬁed Hamming distance
on a pair of DNA sequences. The number of evolutionary changes is under-counted by
the Hamming distance. Various distance functions such as the Jukes-Cantor distance
[JuCa69] have been developed to account for these missing changes.
2. Every metric on three or fewer taxa is a tree metric.
3. When there are at least four taxa, a dissimilarity matrix is a tree metric if and only
if for all i, j, k, l we have ai,j +ak,l ≤max{ai,k +aj,l, ai,l +aj,k}. This condition is known
as the four point condition.
4. The set of tree metrics can be described by a tropical variety [SpSt04], while the set
of ultra-metric trees can be described as a polyhedral fan [ArKl06].
5. With over 40,000 citations, neighbor joining [SaNe87], described in Algorithm 1, is
the most popular distance-based algorithm.
Algorithm 1:
Neighbor joining.
input: dissimilarity matrix M = (mi,j)
output: phylogenetic tree T
1. construct the matrix S = (si,j) with
si,j = (n −2)mi,j −Pn
k=1(mi,k + mj,k).
2. ﬁnd a pair of taxa i and j which minimize si,j; construct a vertex A on the
tree which joins i and j.
3. calculate the distance from the taxa i and j to the new vertex A using the
formula di,A = 1
2mi,j +
1
2(n−2)
Pn
k=1(mi,k −mj,k).
4. calculate the distance from all other taxa x to the new vertex A using the
formula dx,A = 1
2(mx,i + mx,j −mi,j).
5. repeat Step 1 using the dissimilarity matrix constructed by replacing the taxa
i and j with the single newly constructed vertex A.

Section 20.2
PHYLOGENETICS
1451
Examples:
1. Given the dissimilarity matrix M =


0
3
8
9
3
0
9
10
8
9
0
9
9
10
9
0

for the set X = {a, b, c, d},
we note that ma,b + mc,d = 12, ma,c + mb,d = 18, and ma,d + mb,c = 18. The four point
condition is satisﬁed, as each individual sum is less than or equal to the maximum of
the other two. The four point condition can be reformulated to state that the maximum
value of the three sums is obtained by more than one of the three pairs of taxa (here
ma,c + mb,d and ma,d + mb,c). This viewpoint leads to the interpretation of the space of
trees as a Tropical Grassmanian (see [PaSt05] for details).
2. Since M satisﬁes the four point condition, we can use the neighbor joining algorithm
to compute a tree whose pairwise distances match the dissimilarity matrix. First we
compute the associated matrix S =


0
−36
−30
−30
−36
0
−30
−30
−30
−30
0
−36
−30
−30
−36
0

.
We choose sa,b as the minimum entry and construct a new vertex A connecting the ﬁrst
two taxa. Using Steps 3 and 4 from Algorithm 1 we compute da,A = 1, db,A = 2, dc,A =
7, dd,A = 8. This gives the following tree.
a
b
c
d
1
2
A
We repeat the process on the new matrix where the ﬁrst row and column now represent
the newly constructed vertex A. First we compute M =


0
7
8
7
0
9
8
9
0

. Then calculating
the S matrix yields S =


0
−24
−24
−24
0
−24
−24
−24
0

. Since there is a tie between all scores it
doesn’t matter which pair we join. For convenience we select mA,c to construct a new
vertex B which connects A and the taxa c. Then we compute dA,B = 3, dc,B = 4 and
dd,B = 5. Neighbor joining returns the phylogenetic tree shown in the following ﬁgure.
Notice that the pairwise distances among the taxa on this tree exactly match those in
the dissimilarity matrix.
a
b
c
d
3
1A
B
2
4
5

1452
Chapter 20
DISCRETE BIOMATHEMATICS
20.2.5
Tree Distributions
We would like to compare trees reconstructed from biological data to random trees. To
make this notion precise requires an underlying distribution on the space of trees.
Deﬁnitions:
The uniform distribution on the space of rooted or unrooted trees assumes that each
tree occurs with equal likelihood.
A ranked phylogenetic tree is a rooted binary tree where each internal vertex is assigned
a number from 1, 2, . . . , n −2 such that if vi is contained in the path from the root to a
vertex vj, then the number assigned to vi must be less than the number assigned to vj.
The Yule-Harding distribution on rooted binary phylogenetic trees is constructed by
taking the uniform distribution on ranked phylogenetic trees and ignoring the rank.
Facts:
1. There are n!(n −1)!
2n−1
ranked binary trees on a set of n taxa.
2. The probability of selecting a rooted binary tree T under the uniform model is
1
(2n −3)!!. Under the Yule-Harding model this probability is
2n−1
n!
Y
v∈T
(Des(v) −1)
, where
Des(v) denotes the number of leaves which are descendants of an internal vertex v.
3. In general, trees generated under Yule-Harding are more balanced about the root
than trees generated under the uniform distribution.
4. Trees generated under a broad class of evolutionary assumptions follow the Yule-
Harding distribution [LaSt13].
Examples:
1. The probability of observing the tree in the following ﬁgure is
1
15 under the uniform
distribution as it is one of the ﬁfteen rooted 4-taxa trees.
a
b
c
d
2. The root of the tree above has four leaf descendants, and each of the other two
internal vertices have two descendants. This allows us to compute the probability of this
tree under the Yule-Harding model as
2n−1
n!
Y
v∈T
(Des(v) −1)
=
8
24(4 −1)(2 −1)(2 −1) = 1
9.
3. The probability of a tree under the Yule-Harding model is completely determined
by the tree topology. The three 4-taxa trees with the balanced topology all occur with
probability 1
9. The twelve 4-taxa trees which have unbalanced topology around the root
occur with probability
1
18. Under the uniform distribution 1
5 of the rooted 4-taxa trees
sampled are balanced, while under the Yule-Harding model 1
3 are balanced.

Section 20.2
PHYLOGENETICS
1453
20.2.6
Subtrees and Supertrees
Biologists seek to reconstruct trees of ever increasing size with the ultimate goal of build-
ing a tree containing every living organism. To reconstruct large trees, it is important to
determine which collection of subtrees best reﬂects the structure of the tree itself. These
subtrees can then be used as inputs into a supertree algorithm designed to reconstruct
large trees from a collection of smaller input trees.
Deﬁnitions:
A quartet tree is a binary tree with four leaves. An unrooted quartet tree T is denoted
ab|cd if a and b form a cherry of T .
The support of a tree, denoted supp(T ), is the collection of taxa at the leaves of T .
A tree T1 displays a tree T2 if T1|supp(T2) = T2.
A collection of trees T1, T2, . . . , Tn is called deﬁnitive if there exists a unique tree which
displays all of the trees in the collection.
A collection of subsets of taxa S1, S2, . . . , Sk ⊆X is called decisive if for any tree T
with taxa X the subtrees T |S1, T |S2, . . . , T |Sk are deﬁnitive.
A collection of subsets S1, S2, . . . , Sk ⊆X satisﬁes the four-way partition property
if for all partitions of X into four nonempty sets X1, X2, X3 and X4 there exists a subset
Si which has a nonempty intersection with each of X1, X2, X3, X4.
Facts:
1. The collection of all 4-element subsets of a set of taxa is decisive.
2. For any unrooted binary n-taxa tree, there exists a collection of n −3 quartet trees
which are deﬁnitive.
3. A collection of subsets of taxa S1, S2, . . . , Sk ⊆X is decisive if and only if it satisﬁes
the four-way partition property.
Examples:
1. A quartet tree q = ab|cd is shown in the following ﬁgure.
The support of q is
{a, b, c, d}.
a
b
c
d
2. The quartet tree q = ab|cd is displayed by the following 5-taxa tree since the induced
subtree on the support of q is isomorphic to q.
a
b
d
e
c

1454
Chapter 20
DISCRETE BIOMATHEMATICS
3. For the taxon set X = {a, b, c, d, e} the quartet trees ab|cd and ac|de are deﬁnitive as
they are only displayed by the tree in the next ﬁgure.
a
b
d
e
c
4. The quartets ab|cd and ab|ce are not deﬁnitive as they are displayed by both trees in
the following ﬁgure.
a
b
d
e
c
a
b
c
e
d
5. The previous example also shows that the collection T = {{a, b, c, d}, {a, b, c, e}} is
not decisive. However, the collection of subsets
S = {{a, b, c, d}, {a, b, c, e}, {a, b, d, e}, {a, c, d, e}}
satisﬁes the four-way partition property and is thus decisive.
20.2.7
Applications
Facts:
1. Most phylogenetic reconstruction algorithms return an unrooted tree. For that rea-
son, biologists include a distantly related organism or outgroup in their sample to help
place the root of the tree.
2. Phylogenetic trees provide a tool for understanding biodiversity. The simplest mea-
sure of the biodiversity of a set of taxa is the sum of the branch lengths in the subtree
they induce. This metric is used in conservation biology when making decisions about
which organisms to protect.
3. It is possible that trees reconstructed from diﬀerent genes from the same collection
of species do not agree. A coalescent theory describes how diﬀerent gene trees can result
from a single species tree. In fact the species tree may not even match the most prevalent
gene tree [DeRo06].
Examples:
1. The following ﬁgure shows how a tiger could be used as an outgroup when recon-
structing the evolutionary history of primates.

Section 20.3
DISCRETE-TIME DYNAMICAL SYSTEMS
1455
Human
Chimp
Gorilla
Tiger
Human
Chimp
Gorilla
Tiger
2. In the following ﬁgure, the taxa {a, f, g, l, n} have the maximum biodiversity of any
5-taxa set for this tree.
f
e
d
a
b
c
o
n
m
g
h
i
l
k
j
3. In the next ﬁgure two gene trees evolve under the coalescent model within a species
tree which is represented as a band-like tree enclosed in the thick lines. In this ﬁgure
a, b and c represent distinct species. The dotted gene tree exhibits discordance with the
species tree topology. The solid gene tree matches the species tree topology.
a
b
c
20.3
DISCRETE-TIME DYNAMICAL SYSTEMS
Discrete-time dynamical systems are used in mathematical biology to model and simulate
interaction-based networks, such as gene regulatory networks. While the behavior of a

1456
Chapter 20
DISCRETE BIOMATHEMATICS
biological system may be seen as continuous, in that it moves continuously from one state
to another, the technology to record observations of the system, such as microarray chips,
is not continuous and the available experimental data consist of collections of discrete
instances of continuous processes.
20.3.1
BASIC CONCEPTS
Deﬁnitions:
A discrete-time dynamical system on n vertices is a triple (N, X, F) with the fol-
lowing properties:
1. N is a set of n variables x1, x2, . . . , xn;
2. X = X1 × · · · × Xn is the Cartesian product of state sets, in which each Xi is the
set of values, called states, that variable i can take; and
3. F = (f1, . . . , fn) : X →X is a function of the variables in N, called the global
function, and each fi : X →Xi is a transition function, associated with vertex xi.
The ﬁrst iterate of point x = (x1, . . . , xn) ∈X is F(x), the second iterate is F ◦F(x),
and so on.
A sequence of iterates x 7→F(x) 7→F ◦F(x) 7→· · · 7→F ◦· · ·◦F(x) is called a trajectory.
The set of all iterates is called the orbit of x ∈X. A trajectory that starts and ends at
the same point is called a (limit) cycle. A point x ∈X is a ﬁxed point (cycle of length
one) when F(x) = x.
A dynamical system is a ﬁnite dynamical system if each state set Xi is a ﬁnite set.
A ﬁnite dynamical system is a polynomial dynamical system if X = kn where k is a
ﬁnite ﬁeld.
Let R be a commutative ring. A polynomial ring R[x1, . . . , xn] in n variables is the
set of all linear combinations of monomials of the form xα = xα1
1 . . . xαn
n
over R, where
α is the n-tuple exponent α = (α1, . . . , αn) ∈Zn
≥0.
The support of f ∈R[x1, . . . , xn], denoted supp(f), is a subset {xi1, . . . , xim} of {x1, . . . ,
xn} such that m is the smallest integer with f ∈R[xi1, . . . , xim].
Let F be an n-dimensional ﬁnite dynamical system. The dependency graph of F,
denoted D(F), is a directed graph (V, E) where V = {x1, . . . , xn} and E = {(xi, xj) |
xi ∈supp(fj)}.
The state-space graph of an n-dimensional polynomial dynamical system F : kn →
kn, denoted S(F), is a directed graph (V, E) where V = kn and E = {(a, b) | a, b ∈
V and F(a) = b}.
Facts:
1. For simplicity, it is often assumed that all vertices have the same state set, that is,
Xi = X for all i = 1, . . . , n and X = Xn.
2. Let k be a ﬁnite ﬁeld. Then every function f : kn →k is a polynomial of degree at
most n.
3. The structure of a ﬁnite ﬁeld can be imposed on X by requiring that : X : is a power
of a prime number.

Section 20.3
DISCRETE-TIME DYNAMICAL SYSTEMS
1457
4. Each transition function fi : Xn →X is local in the sense it only depends on those
variables that correspond to vertices to which i is connected by an edge in D(F).
5. Any polynomial dynamical system gives rise to a directed graph on n vertices through
the construction of a dependency graph; the converse is also true: any directed graph on
n vertices can be the dependency graph for a polynomial dynamical system (not unique).
6. The dynamics of a polynomial dynamical system F : kn →kn are uniquely repre-
sented by its state-space graph S(F) which has kn vertices.
7. For visualization of state-space and dependency graphs of polynomial dynamical sys-
tems, the web-based software package ADAM is available at
• http://adam.plantsimlab.org
8. Boolean networks (see §20.3.2) can be viewed as polynomial dynamical systems over
k = F2, the ﬁeld of two elements.
Examples:
1. Let D1 = (N, X, F) be a discrete-time dynamical system with N = {x1, x2, x3},
X = Q3, and F = {f1, f2, f3} where
f1 = f1(x1, x2, x3)
=
2x1 + x3
f2 = f2(x1, x2, x3)
=
5.1x3
x2 + 1
f3 = f3(x1, x2, x3)
=
3x2.
We notice that supp(f1) = {x1, x3}, supp(f2) = {x2, x3}, and supp(f3) = {x2}. We
ﬁnd the ﬁrst iterate of point (0, 0, 1) by F(0, 0, 1) = (f1(0, 0, 1), f2(0, 0, 1), f3(0, 0, 1)) =
(1, 5.1, 1). In this example the system iterations are determined through a synchronous
update of the vertices; when the vertices operate on diﬀerent time scales, a sequential
updating scheme is needed. See [MoRe07] for an introduction to sequential dynamical
systems.
2. Let D2 = (N, X, F) be a ﬁnite (polynomial) dynamical system such that N =
{x1, x2}, X = F2
3, and F : F2
3 →F2
3 with transition functions
f1 = f1(x1, x2)
=
2x2
f2 = f2(x1, x2)
=
x1 + x2
1.
The dependency graph of D2 is shown in part (a) of the following ﬁgure. The state-space
graph in part (b) consists of 32 = 9 vertices arranged into three connected components.
The system has two ﬁxed points, (00) and (12), and a cycle of length 2.
21
11
12
20
02
22
00
10
01
x1
x2
(a)
(b)
3. Let D3 = (N, X, F) be a polynomial dynamical system with N = {x1, x2}, X = F2
3,
and F = {f1, f2} where
f1
=
x2
1x2
f2
=
x2.

1458
Chapter 20
DISCRETE BIOMATHEMATICS
D3 is an example of a monomial dynamical system: each transition polynomial consists
of a single monomial, i.e., fi = xα for i = 1, . . . , n. While little can be said about the
length and shape of the cycles of a general ﬁnite dynamical system without explicitly
calculating its state-space graph, for the case of monomial dynamical systems more is
known [CoJaLa06].
4. Let D3 be as in the previous example. It is also a representative of another im-
portant class of dynamical systems—ﬁxed point systems. Such systems have only ﬁxed
points as cycles. As seen in the following ﬁgure, the system D3 has the ﬁve ﬁxed points
(00), (01), (02), (11), (22) and no other, longer cycles. Fixed point systems are impor-
tant in biochemical modeling since biochemical networks often exhibit such steady-state
dynamics [Lu12].
21
11
12
20
02
22
00
10
01
5. Laubenbacher and Stigler [LaSt04] address the reverse engineering problem of gene
regulatory networks in the context of polynomial dynamical systems. They propose an
algorithm that takes a discrete time series dataset D = {s1, . . . , sm | si ∈kn, 1 ≤i ≤
m}, where k is a ﬁnite ﬁeld, and generates a minimal polynomial dynamical system
F = (f1, . . . , fn) such that F(si) = si+1 for each i < m.
By minimal it is meant
that each fj is reduced with respect to the ideal I({s1, . . . , sm}) = ∩iI(si) of polynomials
vanishing on all input data points, i.e., all polynomial dynamical systems that ﬁt the data
diﬀer by elements of I({s1, . . . , sm}). Their algorithm is based on the Buchberger-M¨oller
algorithm for Gr¨obner basis computation. It has been applied to the reverse engineering
of a gene regulatory network responsible for the segment polarity development in D.
melanogaster.
6. Sim˜ao et al. [SiEtal05] develop a qualitative model of tryptophan biosynthesis in E.
coli. While the other vertices in the network model can be represented by binary vari-
ables, a ternary variable (taking the values 0, 1, 2) is needed for the vertex representing
tryptophan.
20.3.2
BOOLEAN NETWORKS
Boolean networks were ﬁrst used in the life sciences in the 1960s, when Stuart Kauﬀ-
man introduced them to model gene regulatory networks as switching networks [Ka69].
They have the advantage of being computationally simple and intuitive, and they have
been successfully used to model regulatory networks, preserving the qualitative, generic
properties of global network dynamics.
Deﬁnitions:
A Boolean function is a function of the form f : Bn →B where B = {0, 1}.
A Boolean network G(V, F) is a set V = {x1, . . . , xn} of vertices representing network
nodes, together with a global function F = (f1, . . . , fn), where fi : Bn →B is the
transition function associated with vertex xi.
The maximum degree (sometimes called maximum connectivity) ∆(G) of a Boolean
network G(V, F) refers to the maximum number of vertices in V that are adjacent to a

Section 20.3
DISCRETE-TIME DYNAMICAL SYSTEMS
1459
single vertex.
A probabilistic Boolean network [ShEtal02] is a Boolean network in which to every
vertex xi, there corresponds a set Fi = {f (i)
j }j=1,...,l(i) where each f (i)
j
(called a predic-
tor) is a possible transition function determining the value of xi and l(i) is the number
of possible transition functions for xi. If l(i) = 1 for all i = 1, . . . , n, then Qn
i=1 l(i) = 1
and the probabilistic Boolean network reduces to a standard Boolean network.
Let f = (f (1), . . . , f (n)) be a random vector taking values in F1×· · ·×Fn. If f (1), . . . , f (n)
are independent, i.e., if the selection of the Boolean functions comprising a speciﬁc net-
work is independent, the probabilistic Boolean network is said to be independent (see
Fact 10).
A random Boolean network [Ka93] is another variation of a Boolean network in which
the state of each vertex is determined by the connections coming from other (or the same)
vertices. The connections are wired randomly, but remain ﬁxed during the dynamics of
the network. The way in which the vertices aﬀect each other is not only determined
by their connections, but also by logic functions, which are generated randomly using
lookup tables for each vertex, taking the states of the connecting vertices as inputs and
producing the state of the vertex as output. These also remain ﬁxed during the dynamics
of the network.
Facts:
1. A Boolean network on n vertices can be seen as a polynomial dynamical system over
F2, the ﬁeld of two elements, i.e., F = (f1, . . . , fn) : Fn
2 →Fn
2 with fi : Fn
2 →F2. Its
state-space graph S(F) consists of 2n vertices and its dependency graph is D(F) = (V, E)
where E = {(xi, xj) | xi ∈supp(fj)}.
2. There are 2n2n diﬀerent Boolean networks on n vertices.
3. The dynamics over a Boolean network ﬂow according to the updating functions and
scheme.
Since the state space is ﬁnite (size 2n, where n is the number of vertices),
eventually a state will be repeated and it is said to have reached an attractor. If the
attractor consists of one state, it is called a steady state or ﬁxed point, whereas if it
consists of two or more states, it is called a cycle. The set of states that ﬂow towards an
attractor is called the attractor basin.
4. Boolean functions are traditionally represented in terms of logical operations (AND
(∧), OR (∨), NOT (¬)). They can also be expressed as polynomials over F2. Namely,
any two logical variables a and b can be considered as values in F2 so a ∧b can be
expressed as ab, a ∨b as a + b + ab, and ¬a as a + 1.
5. When Boolean networks are used to model gene regulatory networks (see Examples
1 and 2), typically xi = 1 represents the fact that gene i is expressed and xi = 0
means it is not expressed. The list of Boolean functions F = (f1, . . . , fn) represents
the rules of regulatory interactions between genes. All network genes are assumed to
update synchronously in accordance with the functions assigned to them and this process
is then repeated. The artiﬁcial synchrony simpliﬁes computation while preserving the
qualitative, generic properties of global network dynamics [Hu99].
6. The maximum connectivity ∆in a Boolean network that models a gene regulatory
network is interpreted as the maximum number of genes that regulate some single gene.
7. A low connectivity ∆is more realistic for a real gene regulatory network [ThEtal98].
8. A Boolean network model is often a suitable representation of genetic networks be-
cause genetic manipulation often involves either overexpression or deletion of a gene
[Hu99].

1460
Chapter 20
DISCRETE BIOMATHEMATICS
9. A Boolean network model is often able to retain suﬃcient biological information to
realistically model genetic regulatory networks. For example, a Boolean network model
is able to provide a clear distinction between diﬀerent classes of sarcomas and diﬀerent
subclasses of gliomas [ShZh02].
10. A realization of a probabilistic Boolean network at a given instant of time is deter-
mined by a vector of Boolean functions. If there are N possible realizations, then there
are N vector functions f1, . . . , fN of the form fk = (f (1)
k1 , . . . , f (n)
kn ), for k = 1, . . . , N,
1 ≤ki ≤l(i) and where f (i)
ki ∈Fi, i = 1, . . . , n.
11. Let f = (f (1), . . . , f (n)) be a random vector taking values in F1 × · · · × Fn; the f (i)
are not necessarily independent. Then the probability that a predictor f (i)
j
is used to
predict gene i is c(i)
j
= Pr{f (i) = f (i)
j } = P
k:f (i)
ki =f (i)
j
Pr{f = fk} and since the c(i)
j
are
probabilities, they must satisfy Pl(i)
j=1 c(i)
j
= 1.
12. Kauﬀman was the ﬁrst to propose random Boolean networks, supporting the hy-
pothesis that living organisms could be constructed from random elements, without the
need of precisely programmed elements [Ka93]. Random Boolean networks are general-
izations of Boolean cellular automata (see §20.3.3).
13. The updating of the vertices in classic random Boolean networks is synchronous:
the states of vertices at time t + 1 depend on the states of vertices at time t. There can
be drastic diﬀerences if the updating scheme is changed [HaBo97].
14. There are three phases that can be distinguished in a random Boolean network:
ordered, chaotic, and critical. For deﬁnitions and methods for identifying the phases, see
[Ka00].
Examples:
1. Let G(V, F) be a Boolean network with V = {x1, x2, x3} and F = (f1, f2, f3) where
f1
=
x1 ∧x2 ∧x3
f2
=
(x1 ∨x2) ∧x3
f3
=
¬x1 ∧x2.
As seen in the following ﬁgure, the state space contains two types of attractors: two ﬁxed
points, (000) and (011), with basins of attraction of sizes 2 and 1, respectively, and one
cycle of length 2, with basin of attraction of size 5.
100
000
011
111
001
010
101
110
2. A simple model of the lactose metabolism in E. coli [StVe11], constructed under
the assumptions that molecular synthesis and degradation require one time unit and
extracellular lactose is always available, can be represented as
HM = ¬R ∧C
HP = M
HB = M
HC = ¬Ge
HR = ¬A ∧¬Al
HA = L ∧B
HAl = A ∧L ∧Ll
HL = ¬Ge ∧P ∧Le
HLl = ¬Ge ∧(L ∨Le)

Section 20.3
DISCRETE-TIME DYNAMICAL SYSTEMS
1461
where
M = lac mRNA
P, B = lac permease and β-galactosidase, respectively
C = catabolite activator protein CAP
R = repressor protein LacI
L, A = lactose and allolactose (inducer), respectively
Ll, Al = (at least) low concentration of lactose and allolactose, respectively.
Here Le and Ge represent extracellular lactose and glucose, respectively, and are consid-
ered as parameters in the model.
3. Albert and Othmer [AlOt03] constructed a Boolean model of the network of segment
polarity genes in the fruitﬂy D. melanogaster. The model was built from inferences given
gene and protein expression data. The following ﬁgure depicts the graph of connections
in the Boolean model.
The shape of the nodes indicates whether the corresponding
substances are mRNAs (ellipses), proteins (rectangles), or protein complexes (hexagons).
The edges of the network signify either biochemical reactions or regulatory interactions.
Terminating arrows generally indicate activation, while terminating segments indicate
inhibition.
SMO
EN
FZ
WG
PTC
PH
en
CIR
CIA
CI
HH
SLP
wg
ptc
smo
ci
hh
4. Elementary cellular automata, discussed in §20.3.3, can also be viewed as Boolean
networks whose dependency graph is circular and in which all vertices have the same
local Boolean function.
20.3.3
CELLULAR AUTOMATA
Cellular automata are simple mathematical idealizations of natural systems. They were
introduced by John von Neumann and Stanislaw Ulam as simple models in which to
study biological processes such as self-reproduction [vN66, Ch. 5]. A cellular automaton
is a discrete model that consists of a ﬁnite or inﬁnite regular grid of cells, each in one of a
ﬁnite number of states. Among other applications in systems biology, cellular automata
are used to model reaction-diﬀusion systems [We97] and biological cells [MaDe06].

1462
Chapter 20
DISCRETE BIOMATHEMATICS
Largely inspired by cellular automata, scientists of various backgrounds, such as Arthur
Burks (philosopher) [Bu71] and Christopher Langton (computer scientist) [La89], started
seeking to mimic life mathematically and generate known features of life from basic
principles under the premise that life can be manifested outside of biochemistry.
Deﬁnitions:
A cellular automaton is a lattice of identical sites (cells), each taking on a ﬁnite set
of values. The lattice can be in any ﬁnite number of dimensions. The state of a cell at
time t (time is discrete) is a function of the states of a ﬁnite number of cells (called its
neighborhood) at time t −1. These neighbors are a selection of cells relative to the
speciﬁed cell, and do not change. Every cell has the same rule for updating, based on
the values in this neighborhood. Each time the rules are applied to the whole lattice, a
new generation is produced.
A one-dimensional cellular automaton of length n can be deﬁned as a network on
n vertices, where its dependency graph is a circle.
An elementary cellular automaton is a one-dimensional cellular automaton whose
cells can only assume two values, e.g., 0 or 1.
Facts:
1. In the 1970s, John Conway’s “Game of Life” [Ga70] (also see §17.1.4), a two-state,
two-dimensional cellular automaton, became widely known. It is one of the simplest
examples demonstrating that elaborate patterns and behaviors can emerge from very
simple rules. A Java implementation can be found at
• http://www.ibiblio.org/lifepatterns/
2. An elementary cellular automaton is a ring consisting of a ﬁnite number of squares
(cells). Each square has two neighbors: a square to its left and a square to its right. Each
square can assume one of two colors. The color of each square in the next step depends
on its current color and the colors of its two neighbors. All squares use the same rule to
update their colors. Squares can be initialized to diﬀerent colors.
3. In 1982 Stephen Wolfram published the ﬁrst of a series of papers [Wo82] system-
atically investigating elementary cellular automata. The unexpected complexity of the
behavior of these simple rules led Wolfram to suspect that complexity in nature may be
due to similar mechanisms.
Examples:
1. The following ﬁgure presents two generations of an elementary cellular automaton of
length 8, with initial state given in part (a). Update rule: at time t each cell assumes
whichever color had the majority among itself and its neighborhood at time t −1. An
end cell’s neighbors are its adjacent cell and the cell at the other end (the array “wraps
around”). Part (b) shows the automaton’s state after one update.
(a
(
)
b)
2. The next ﬁgure shows a pattern obtained with a simple one-dimensional cellular
automaton. Starting from a “seed” containing a single nonzero cell, subsequent lines are
obtained by successive applications of the following update rule: a(t+1)
i
= a(t)
i−1 + a(t)
i+1
(mod 2), where a(t)
i
is the value of the cell at position i on time step t. The pattern
obtained is Pascal’s triangle of binomial coeﬃcients, reduced modulo 2 [Wo82].

Section 20.3
DISCRETE-TIME DYNAMICAL SYSTEMS
1463
3. Some organisms, such as Conus textile, grow complicated forms by repeated applica-
tion of simple local rules, closely resembling cellular automata [DeDo05].
4. Some researchers adopt a broader notion of artiﬁcial life and include synthetic bio-
chemistry and robotics [Ke09], [ItSa14]. Other interesting examples of attempts at creat-
ing artiﬁcial life include the Digital Organism Simulation Environment (DOSE) [CaLi14],
an executable DNA driven digital organism simulator, and OpenWorm [PaEtal12], an
international open science project to simulate the roundworm Caenorhabditis elegans at
the cellular level.
20.3.4
GENETIC ALGORITHMS
A genetic algorithm (GA) is a heuristic search algorithm that mimics the metaphor of
natural biological evolution. GAs were originally developed as a means of studying the
natural evolution process, and have been adapted for use in optimization since the 1960s
[Go89], [Ho75], [Ko91]. They operate on a population of potential solutions, applying
the principle of survival of the ﬁttest to produce better and better approximations to
a solution. GAs often outperform classical methods of optimization when applied to
problems in ﬁelds as diverse as biology, genetics, engineering, art, economics, marketing,
operations research, robotics, social sciences, physics, politics, and chemistry. Currently
three are three main avenues of research in simulated evolution: genetic algorithms,
evolution strategies, and evolutionary programming, each method emphasizing a diﬀerent
facet of natural evolution. A good introductory survey can be found in [Fo94].
Deﬁnitions:
Since GAs are inspired by natural evolution, terminology from genetics is often used.
The deﬁnitions below are valid only within the context of GAs and may even sometimes
disagree with their counterpart concepts from evolutionary biology [EiSm03].
A chromosome is a set of parameters that deﬁne a proposed solution to the given opti-
mization problem. The chromosome is often represented as a vector, with elements being
described as genes and varying values at speciﬁc positions called alleles. Chromosomes
are points in the genotype space where the search takes place.
The objects forming possible solutions within the original problem context are referred
to as phenotypes and their encoding, the individuals within the GA, are called geno-
types.
A population is a collection of genotypes (individuals), representing a possible solu-
tion.
Crossover, also called recombination, is an operator applied to two or more selected
individuals (parents) and results in one or more individuals (children). Mutation is
applied to one individual and results in one new individual. Recombination and mutation,

1464
Chapter 20
DISCRETE BIOMATHEMATICS
collectively called variation operators, produce a set of new individuals that compete
(based on their ﬁtness) with the old ones for a place in the next generation.
A ﬁtness function is a particular type of objective function that quantiﬁes the optimal-
ity of a chromosome so that a particular chromosome may be ranked against all the other
chromosomes. The chosen chromosomes are allowed to “breed” and mix their datasets,
thus producing a new generation that will hopefully be even closer to the optimal.
Fitness landscapes are used to visualize the relationship between genotypes and repro-
ductive success. Genotype ﬁtness corresponds to the height of the landscape. The set
of all possible genotypes, their degree of similarity, and their ﬁtness values constitute the
ﬁtness landscape. In the following ﬁgure, points A and C are local optima, while point B
is the global optimum. The trajectory of the ﬁlled circle traces a population that moves
from a very low ﬁtness value to the top of a peak.
A
B
C
Facts:
1. GAs consistently perform well in approximating solutions to all types of problems
because they do not make any assumption about the underlying ﬁtness landscape.
2. While there are variations in diﬀerent GAs, the common idea behind all of them
is that given a population of individuals, the environmental pressure causes natural
selection that results in a rise in the ﬁtness of the population. The environmental pressure
is mimicked through the application of a ﬁtness function that is to be maximized. The
process starts by randomly creating a set of candidate solutions and applying the ﬁtness
function to identify the best candidates for creating the next generation by means of
recombination and mutation. The process is repeated until an individual of suﬃciently
high ﬁtness is found.
3. There is no guarantee that the best solution found by a GA is actually the optimal
solution. In the context of GAs, “optimal solution” means the best solution we are able to
ﬁnd. The advantage of using a GA is that there is inherently some level of stochasticity;
that is, there is always the possibility of mutating to a better solution, as long as the
mutation rate is nonzero [LaHiOr15].
4. Uniform crossover is a typical method of crossover in which there is an equally likely
chance that the child will take each particular gene from either parent (see Example 1).
One-point crossover is another popular option (see Example 7).
5. During selection, ﬁtter individuals have a higher chance to be selected but even
weaker ones have a chance to survive and become parents. Combining recombination
and mutation with selection generally leads to improved ﬁtness.
6. Binary representation is commonly used for candidate solutions, encoding them as
binary strings (see Example 5). Other representations, such as ﬂoating-point, can be
found in the GA literature as well.
Examples:
1. In the “Rabbits and Grass” example discussed in [LaHiOr15], two randomly chosen
solutions are selected, p1 = (0101010101) and p2 = (1110110010). A child solution is

Section 20.3
DISCRETE-TIME DYNAMICAL SYSTEMS
1465
created using uniform crossover by going through each gene, i.e., each entry in the parent
solutions, and randomly selecting one of the values. An example of a child solution might
be pnew = (0111110111). The child solution is then subjected to mutation, e.g., the
second and the last entry of pnew are “mutated” through bit-ﬂipping to obtain pmut
new =
(0011110110). The algorithm continues for a predetermined number of steps, or until a
certain condition is met. For example, one might run the algorithm for 50 generations.
Another method is to repeat the process until no better solution has been found for some
speciﬁed number of consecutive generations. When the algorithm terminates, the best
current solution is selected as the candidate for an optimal solution [LaHiOr15].
2. GAs are population-based stochastic algorithms that repeatedly test individuals in a
population against a ﬁtness criterion. Algorithm 1 outlines the general principles of GAs
[EiSm03].
Algorithm 1:
General scheme of a GA.
input: population with random candidate solutions
output: suitable solution (candidate with suﬃcient quality)
EVALUATE each candidate in the population
while TERMINATION CONDITION not satisﬁed do
SELECT parents
CROSSOVER pairs of parents
MUTATE resulting oﬀspring
EVALUATE new candidates
SELECT individuals for next generation
return an individual (that meets TERMINATION CONDITION)
3. If the problem has a known optimal solution value, then reaching the optimal ﬁt-
ness level within a given precision can be used as a TERMINATION CONDITION in
Algorithm 1. However, because of the stochastic nature of the GA, there is no guarantee
of convergence. To ensure algorithm termination, other criteria are often applied, such
as restricting the CPU time allowed, ﬁxing the number of ﬁtness evaluations permitted,
terminating when the population diversity drops below a certain level, and setting a
minimum ﬁtness improvement for a number of generations or ﬁtness evaluations.
4. There are many GA variations. Algorithm 1 can be modiﬁed, for example, so that
the initial chromosomes are selected in a certain way, e.g., at random or so that they
are very diﬀerent from one another (i.e., solutions that come from diﬀerent regions of
the solution space). The crossover process can also be modiﬁed so that one parent is
favored over another, or one can forego the mutation step altogether. The likelihood of
mutation is another area where user input is important: a high level of mutation will
result in more variation of child chromosomes, and thus will not incorporate the relative
ﬁtness of the parent chromosomes as much. On the other hand, if the mutation step is
not included then there is a greater risk of the solutions converging to some solution that
is only locally minimal [LaHiOr15].
5. Suppose we want to ﬁnd an integer value x that maximizes F(x) = −x2. Then the
set of integers Z will be the set of phenotypes. Thus the integer 9 is a phenotype and,
if binary coding is used, its genotype representation is 1001. The ﬁtness function of this
genotype could be deﬁned as (the negative of) the square of its phenotype, 92 = 81.
6. Suppose the goal is to ﬁnd a vector of 100 bits, x ∈{0, 1}100, such that the sum
of all of the bits in the vector is maximized. The ﬁtness function could be written as

1466
Chapter 20
DISCRETE BIOMATHEMATICS
F(x) = P100
i=1 xi, where x = (x1, x2, . . . , x100). A candidate solution is evaluated using
F and its ﬁtness is identiﬁed in the range between 0 and 100. Let an initial population
of 100 parents be selected at random and subjected to selection using F(x) with the
probabilities of recombination and bit mutation being 0.8 and 0.01, respectively. The
following ﬁgure shows the rate of improvement of the best vector in the population, and
the average of all parents, at each generation. The process rapidly converges to the vector
of all 1s, with optimum ﬁtness value 100 [Fo94].
50
60
70
80
90
100
0
20
40
60
80
100
120
fitness score
generations
best
average
7. The 0-1 knapsack problem (§16.3.1), an optimization problem that arises in many
industrial problems, can be brieﬂy described as follows. Given a set of n items, each
of which has some value vi attached to it and some cost ci, how do we select a subset
of those items with cost at most Cmax that maximizes the overall value. A candidate
solution for this problem can be represented as a binary string of length n, where a 1 in
a given position indicates that an item is included and a 0 indicates that it is omitted.
The corresponding genotype space G is the set of all such strings, whose size 2n increases
exponentially with the number of items considered.
While it might seem natural to deﬁne the phenotype space P to be identical to G, a one-
to-one correspondence between the genotype and phenotype spaces leads to problems
[EiSm03]. Instead, let the ﬁtness of a given solution p, represented by a binary genotype
g, be determined by summing the values of the included items: Qp = Pn
i=1 vigi. In order
to break the one-to-one correspondence between G and P, when creating a solution
proceed as follows. Read from left to right along the binary string, and keep a running
tally of the cost of included items. When a value 1 is encountered, ﬁrst check to see if
including the item would violate the capacity constraint Qp = Pn
i=1 vigi ≤Cmax; i.e.,
rather than interpreting a value 1 as meaning to include this item, interpret it as meaning
to include this item if it does not violate the cost constraint.
For this problem, [EiSm03] chose as a recombination operator the one-point crossover,
which aligns two parents and picks a random point along their length. The two oﬀspring
are created by exchanging the tails of the parents at that point. This operator is applied
with 0.7 probability. That is, for each pair of parents select a random value uniformly
between 0 and 1; if it is below 0.7 then create two oﬀspring by crossover, otherwise make
copies of the parents. Bit-ﬂipping is used as a mutation operator: in each position invert
the value with a small probability pm ∈[0, 1), for example pm = 1/n. A termination
condition might be, for example, when no improvement in the ﬁtness of the best member
of the population has been observed for twenty ﬁve generations. Such a termination
criterion is appropriate since the maximum value that can be achieved is not known.

Section 20.4
GENOME ASSEMBLY
1467
20.4
GENOME ASSEMBLY
Every living organism has a genome that contains its complete hereditary information,
encoded by long DNA sequences.
Abstractly, the genome can be viewed as a set of
strings over the alphabet {A, T, C, G}. The task of decoding, or sequencing a genome,
is of considerable interest. However, it is a computationally daunting task because these
sequences can be hundreds of billions of base pairs in length, and even the most advanced
technology can only read sequences of a few thousand base pairs. As a result, sequenc-
ing a genome is done by breaking up numerous copies into small pieces which can be
individually read. These fragments can then be reassembled into the original sequence
by analyzing the overlaps, or by recording the length-k subsequences that they contain.
This section provides an overview of approaches and algorithms developed to solve this
massive biological jigsaw puzzle.
20.4.1
BASIC CONCEPTS
Deﬁnitions:
A strand of DNA consists of a sequence of four types of nucleotides distinguished by a
nitrogenous base: adenine (A), thymine (T), cytosine (C), or guanine (G).
A DNA segment is a consecutive subsequence of bases on a DNA strand, which one
can represent as a string or sequence over the alphabet {A, T, C, G}.
A DNA target sequence is a long unknown DNA sequence that one wishes to decode,
but is typically much too long to do so all at once, even using the state-of-the-art tools.
Long DNA strands need to be broken up into smaller strands called clones, which are
short enough to be individually sequenced using modern technology.
DNA clones that are sequenced are called reads, or fragments.
A k-mer of a string is a (consecutive) length-k substring. The preﬁx (respectively,
suﬃx) of a k-mer is its initial (respectively, terminal) (k−1)-mer.
The technique of sequencing by hybridization (SBH) uses a DNA microarray, or
biochip, to determine all k-mers in the reads. This information is then used to reconstruct
the target sequence.
Shotgun sequencing is a method where many copies of a long DNA strand are broken
up randomly into clones, which are sequenced into reads. The original target sequence
is reconstructed by analyzing the overlaps of the reads, or by the set of k-mers in the
reads.
A genome assembler is a computer program that performs the algorithmic gene as-
sembly process from given input data.
Facts:
1. Each base can chemically bond with a particular complement base, forming a base
pair (bp). Speciﬁcally, A bonds with T, and C bonds with G.
2. The bases in a DNA strand are strung along a sugar-phosphate backbone. The carbon
atoms in the sugar (ribose) are numbered 1′ through 5′. The 3′ carbon in each sugar

1468
Chapter 20
DISCRETE BIOMATHEMATICS
molecule is connected through a phosphate group to a 5′ carbon on the next one, giving
each DNA strand a directionality.
3. A DNA molecule consists of two DNA strands that bond to each other and twist into
a double-helix structure. These two strands bond in an anti-parallel fashion: namely, if
the sequence of a DNA strand is s = (s1, . . . , sn), then the complementary sequence is
t = (t1, . . . , tn), where ti = sn−i+1, i = 1, . . . , n, with the bar denoting the complement
base.
4. DNA sequencing was invented in the late 1970s, using an in vitro chain termination
method known as Sanger sequencing [SaNiCo77]. This led to the “ﬁrst generation” of
sequencers, which were the most widely used methods for about 25 years and which
produced reads of 300–900 bp in length.
5. First-generation sequencers have been supplanted (but not replaced) by the develop-
ment of high-throughput parallelized next-generation (Next-Gen) sequencing technolo-
gies.
6. Sequencing by hybridization using k-mers was developed in 1988 [DrEtal89]. The
range 8 ≤k ≤25 was common.
7. DNA hybridization for genome assembly proved to be very limited due to both its
accuracy and low values of k, but microarray technology that it utilized has since been
widely used to measure gene expression.
8. The bacterium Carsonella ruddii has the smallest known genome—about 160,000
bp [NaEtal06]. The largest known genome belongs to the ameboid Polychaos dubium,
containing approximately 670 billion bp. In contrast, the human genome has about 3
billion bp. The Animal Genome Size Database [Gr16] contains genomic data for over
5,600 animal species and is available online at
• http://www.genomesize.com
9. The ﬁrst successful genome sequence was completed in 1995 by a team at The Insti-
tute for Genomic Research (TIGR) and Johns Hopkins University. They sequenced the
entire 1.83 million base pair genome of the bacterium Haemophilus inﬂuenzae [FlEtal95].
10. A breakthrough whole-genome sequence assembly project was completed in 2000
by Eugene Myers et al., using the Celera assembler [MyEtal00]. They sequenced the
135 million base pair genome of the fruit ﬂy Drosophila melanogaster, which was over 25
times larger than any other genome assembled.
11. As of 2016, the single molecule real-time (SMRT) sequencer [RoCaSc13] from Pa-
ciﬁc Biosciences can generate reads with an average length of 10,000–15,000 bp and a
maximum of about 60,000 bp.
12. Other popular Next-Gen sequencers produce shorter but more accurate reads, such
as those developed by the companies Illumina [BeEtal08] (up to 300 bp/read) and 454
Life Sciences [MaEtal05] (up to 600 bp/read).
Examples:
1. The following diagram shows the bases appearing in a short DNA strand CGACTTGC,
oriented in the 5′ to 3′ direction. The complementary DNA strand is GCAAGTCG, also
oriented in the 5′ to 3′ direction.

Section 20.4
GENOME ASSEMBLY
1469
2. The following diagram illustrates a microarray that can be used to identify k-mers
for k = 3. In particular, the DNA fragment ACTAGC contains the 3-mers ACT, CTA,
TAG, and AGC, which are detected by the microarray shown.
20.4.2
OVERLAP GRAPH APPROACHES
The fragment assembly problem takes in a set of reads (strings) and asks for a larger
string s that “best explains” the reads. Originally, this was approached as a modiﬁed
shortest common superstring (SCS) problem with added constraints. The SCS problem
can be solved by constructing a graph whose vertices are the reads and whose edges rep-
resent overlaps. Due to repeated substrings, it is well known that the fragment assembly
problem is more complicated than SCS. Despite this, many assemblers still construct
some type of “overlap graph” from the reads and then seek a Hamilton path. Such as-
semblers often break this into three distinct phases: overlap, layout, and consensus. This
approach was introduced in 1984 [PeS¨oUk84]. It was the ﬁrst widespread method for
sequence assembly and was widely used for over 20 years, in popular assemblers such as
PHRAP, PCAP [HuWa03], TIGR, and Celera [MyEtal00].
Deﬁnitions:
Given a collection of strings (reads) S = {s1, . . . , sn}, a solution to the shortest com-
mon superstring (SCS) problem is a string s of minimum length that contains each
si as a (consecutive) substring.
The overlap of si with sj is the length of the longest suﬃx of si that is a preﬁx of sj.
Given S and a threshold ℓ> 0, the overlap graph is a weighted directed graph con-
structed as follows:
• the vertices are the strings si ∈S;
• there is a directed edge si →sj if the overlap of si with sj is at least ℓ;
• the cost (or weight) of an edge is negative of the overlap between si and sj.
A Hamilton path in a graph is a path that visits every vertex exactly once.

1470
Chapter 20
DISCRETE BIOMATHEMATICS
The three phases in the overlap-layout-consensus approach are
• overlap: ﬁnd signiﬁcant overlaps between reads, taking into account diﬃculties
such as unknown orientations and possible errors;
• layout: construct an appropriate graph and ﬁnd a Hamilton path corresponding
to the optimal layout of overlapping reads;
• consensus: ﬁx errors and resolve discrepancies in the layout.
If the orientation of overlapping reads is known, then this information can be tracked by
using annotated directed edges called dovetail edges, as shown next:
(a) Regular dovetail (b) Preﬁx dovetail
(c) Suﬃx dovetail
(d) Containment I
(e) Containment II
A dovetail path is a path in the overlap graph with the added requirement that it
respects the orientation of the tails; i.e., two consecutive edges common to a vertex
correspond to diﬀerent ends of that fragment.
Facts:
1. An SCS corresponds to a minimum cost Hamilton path in the overlap graph.
2. Finding a minimum cost Hamilton path is an NP-hard problem, even if all edge
weights are identical.
3. Some genomes are circular and others are linear. If the genome is circular, then an
SCS corresponds to a minimum cost Hamilton tour (or cycle) instead of a minimum cost
Hamilton path.
4. Non-optimal superstrings for the SCS problem can be found with a simple greedy
heuristic [TaUk88]: at each step, merge two strings with maximum overlap. This is known
to be a c-approximation algorithm for small c, with c = 2 conjectured. This heuristic is
used by popular genome assemblers such as TIGR (see [SuEtal95], [PoKo04]), PHRAP,
and CAP.
5. Due to the nature of DNA sequencing, the fragment assembly problem is harder than
the standard SCS, due to the following diﬃculties:
• the orientation of the strings may not be known;
• there could be sequencing errors;
• there are usually long repeated substrings in the actual genome;
• there could be diﬀerences between inherited copies of a genome (for example, in
humans, one copy is from the mother and the other is from the father).
6. Due to high computational costs, look-up tables are often used in the overlap phase.
Dynamic programming is also used to ﬁnd optimal alignments.
7. The biggest challenge with the fragment assembly problem is dealing with the abun-
dance of (frequently long) repeated subsequences. This is usually part of the layout phase.
As a result, the real genome is usually not the actual solution to the SCS problem, but
much longer.
8. One way to handle repeats using an SCS approach is to assume that the sampling
coverage is uniform, so that maximum likelihood methods can give evidence for repeated
substrings.
9. Majority voting, Bayesian models, and dynamic programming have all been used
within the consensus phase to determine an optimal multiple alignment.

Section 20.4
GENOME ASSEMBLY
1471
Examples:
1. Given reads S = {AAT, CGTA, TAA, TCGT}, the overlap graph with threshold
ℓ= 1 is shown next.
TCGT
CGTA
TAA
AAT
-3
-2
-1
-1
-2
-1
-1
A minimum cost Hamilton path in this graph is given by TCGT →CGTA →TAA →
AAT with cost −7. This sequence yields the shortest common superstring TCGTAAT,
of length 7.
2. Suppose that the greedy heuristic is applied to the reads in Example 1. The strings
TCGT and CGTA have maximum overlap (3) and are merged, giving the string TCGTA.
Next, strings TAA and AAT have maximum overlap (2) and so are merged, giving the
string TAAT. Finally, the strings TCGTA and TAAT are merged, producing the string
TCGTAAT with length 7, which is optimal (see Example 1).
3. To see that the greedy heuristic need not produce a minimum length superstring, con-
sider reads S = {ATATATATC, GATATATAT, TATATATA}. The strings GATATATAT
and ATATATATC have maximum overlap (8); these two strings are merged, giving the
new string GATATATATC. Since TATATATA and GATATATATC have no overlap, they
can be concatenated in either order, say giving the string TATATATAGATATATATC of
length 18.
However, the SCS has length 12, obtained by overlapping GATATATAT,
TATATATA, and ATATATATC, producing the optimal superstring GATATATATATC.
This example can be generalized to show that the ratio between the heuristic length and
the optimal length approaches 2 in these circumstances.
4. Consider a short circular genome CACTACGTAA, and suppose that shotgun se-
quencing produces the reads {ACAC, ACGTA, CACTAC, CGTAACA, CTACG}. The
overlap graph is shown in the following ﬁgure, using an overlap threshold of ℓ= 2 for
the edges. The target sequence is represented in the overlap graph by a Hamilton tour,
denoted by the thick edges.
CACTAC
CTACG
ACAC
ACGTA
CGTAACA
-2
-2
-4
-3
-2
-2
-3
-3
-2
-4
CACTAC
CTACG
ACGTA
CGTAACA
ACAC

1472
Chapter 20
DISCRETE BIOMATHEMATICS
5. Simpliﬁcations of the overlap graph, such as the following [My95], are frequently
implemented in the layout phase:
• require minimum overlap for edges;
• contained read removal: if s contains s′, then remove s′ and all incident edges;
• topologically smooth vertices: replace edges a →b and b →c with a single edge
a →c if b has no other incident edges.
AAGG
AGGT
GGTT
top. smooth
AAGGTT
• transitive edge removal: given edges a →b, b →c, and a →c, remove a →c.
AAGG
AGGT
GGTT
edge removal
AAGG
GGTT
6. The consensus phase uses a multiple alignment to correct errors, as shown in the
following example with four overlapping reads.
Alignment:
CCCCCGACGGATTAAAATAAGC
TTAATGACGGATTAATA
CCCCCCCCCCATTAAAATAAGCAGT
CCCATGTCGGACTAAAATAA
Consensus:
TTAATGACGGATTAAAATAAGCAGT
20.4.3
DE BRUIJN GRAPH METHODS
The standard de Bruijn graph on m symbols (for genome assembly, m = 4) is a di-
rected graph whose vertices consist of all length-k sequences and in which the edge (u, v)
indicates that the suﬃx of u equals the preﬁx of v (see §3.1.7). A number of genome
assemblers record all of the k-mers present in the reads, and then use a “modiﬁed de
Bruijn graph” to reconstruct the target sequence. There are two main approaches: (i)
represent the vertices as k-mers and edges as (preﬁx, suﬃx) pairs of (k −1)-mers, or
(ii) represent each k-mer as an edge, whose vertices are its (preﬁx, suﬃx) (k−1)-mers.
In the ﬁrst approach, the solution is encoded as a Hamilton path (visiting every vertex
exactly once), and in the second approach, as an Euler path (visiting every edge exactly
once). There is no consensus name for these graphs in the literature, so we will call them
Hamilton and Euler de Bruijn graphs, respectively.
Deﬁnitions:
Given a set of reads, construct the Hamilton de Bruijn graph as follows:
• the vertices are the k-mers that appear in some read;
• there is a directed edge u →v if the suﬃx of u is the preﬁx of v.
An Euler path in a graph is a path that traverses every edge exactly once.
Given a set of reads, construct the Euler de Bruijn graph as follows:

Section 20.4
GENOME ASSEMBLY
1473
• the vertices are the (k−1)-mers that appear in some read;
• there is a directed edge u →v if some k-mer contains u as a preﬁx and v as a
suﬃx.
Facts:
1. The early uses of sequencing by hybridization (SBH) used an overlap-layout-consensus
approach. An Euler de Bruijn graph approach was proposed in 1989 [Pe89]. Another
early SBH approach is described in [DrEtal93], with k = 8.
2. A Hamilton path in the Hamilton de Bruijn graph represents a feasible solution to
the fragment assembly problem.
3. An Euler path in the Euler de Bruijn graph represents a feasible solution to the
fragment assembly problem [IdWa95], [PeTaWa01].
4. If the genome is circular, then solutions to the fragment assembly problem correspond
to Hamilton (or Euler) cycles instead of paths.
5. A directed graph has an Euler cycle (or tour) if and only if the in-degree of each
vertex equals its out-degree. This property holds for both the standard and the Euler de
Bruijn graphs.
6. Algorithm 1 (Hierholzer’s algorithm) describes a simple approach for ﬁnding an Euler
cycle in a directed graph satisfying the property given in Fact 5. It can be implemented
to run in linear time. Also see §8.4.3.
Algorithm 1:
Euler cycle algorithm.
pick a vertex v, and travel along unused edges until returning to v
while the cycle is not Eulerian
pick a vertex w on the cycle that has unused outgoing edges
travel along unused edges until returning back at w
merge cycles
7. Algorithm 1 can be easily adapted to ﬁnd an Euler path in a directed graph that
possesses such a path (but not an Euler cycle).
8. Further modiﬁcations or generalizations of de Bruijn graphs, such as weighted edges
(counting how many times the k-mers occur), paired de Bruijn graphs [MeEtal11], and
pathset graphs [PhEtal13], attempt to improve the process of resolving repeats.
9. Other modiﬁed de Bruijn graphs are used in diﬀerent problems in bioinformatics,
such as RNA assembly [GrEtal11], protein sequencing [BaEtal08], and synteny block
reconstructions [PhPe10].
10. Unlike the overlap graphs, these modiﬁed de Bruijn graphs are not read coherent,
meaning that some paths through the graph are not consistent with the reads [My05].
11. de Bruijn graph assemblers perform well when the read length is short. For longer
reads (e.g., over 100 bp), overlap graph assemblers perform well [ScDeSa10].
12. When constructing a modiﬁed de Bruijn graph, the value of k should be small
enough so that nearly all of the k-mers in the genome will be found in the reads. The
range 30 ≤k ≤60 is common, and these k-mers are found directly from the reads (rather
than from hybridization).
13. Overlap graph approaches were used in most ﬁrst-generation sequencing projects
which assembled the the ﬁrst microbial genome in 1995 [FlEtal95] and the human genome
in 2001. However, most Next-Gen assemblers favor de Bruijn graph methods.

1474
Chapter 20
DISCRETE BIOMATHEMATICS
Examples:
1. Recall Example 4 from §20.4.2, in which shotgun sequencing produced the reads
{ACAC, ACGTA, CACTAC, CGTAACA, CTACG} from the (unknown) circular genome
CACTACGTAA. The Hamilton de Bruijn graph for k = 3 is shown in the following ﬁgure.
CAC
ACT
CTA
TAC
ACG
CGT
GTA
TAA
AAC
ACA
AC
CT
TA
AC
CG
GT
TA
AA
AC
CA
AC
TA
AC
AC
TA
AC
AC
AC
The target sequence CACTACGTAA is represented in this directed graph by the Hamil-
ton cycle that traverses the vertices in a clockwise order. Note that this directed graph
contains other Hamilton cycles such as CAC →ACG →CGT →GTA →TAC →ACT
→CTA →TAA →AAC →ACA →CAC, which encodes the alternative target sequence
CACGTACTAA. Further sequencing data would then be needed to identify the correct
target sequence.
In this example, the coverage of the reads is enough to recover all 3-mers in the original
genome. However, removing any read destroys this property, which would change the de
Bruijn graph. This highlights the importance of having a suﬃciently high coverage so
that all (or almost all) of the k-mers appear in at least one of the reads.
2. The next ﬁgure displays the Euler de Bruijn graph for the 3-mers present in the reads
given in Example 1. The target sequence is represented in this graph by the Euler cycle
CA →AC →CT →TA →AC →CG →GT →TA →AA →AC →CA.
CA
AC
CT
TA
AA
CG
GT
CAC
ACT
CTA
TAA
ACG
CGT
GTA
TAC
AAC
ACA
Note that this graph supports several Euler cycles representing diﬀerent target sequences.
For example, the Euler cycle CA →AC →CG →GT →TA →AA →AC →CT →TA
→AC →CA represents the diﬀerent circular target sequence CACGTAACTA. Another
diﬃculty arises from repeats: this same Euler de Bruijn graph would also be generated
from the circular genome CACACTACGTAA in which CAC and ACA are repeated.

Section 20.5
RNA FOLDING
1475
20.5
RNA FOLDING
Deoxyribonucleic acid (DNA) is a double-stranded macromolecule built from four types
of nucleotides linked together along a sugar-phosphate backbone that twists into the
familiar double-helix shape. Its cousin ribonucleic acid (RNA) is chemically similar but
with several important diﬀerences: it is single-stranded, and the nucleobase uracil (U)
takes the place of thymine (T). Since RNA has only one strand, it can fold and bond to
itself. The speciﬁc structure into which RNA folds often plays an important role in its
function. A central problem in computational biology is to predict how an RNA strand
will fold given only its raw sequence of nucleotides.
There are two main approaches
to the folding prediction problem that both employ dynamic programming: the ﬁrst
attempts to minimize the structure’s free energy, and the second generates structures
using a context-free grammar and then selects the most likely structure.
20.5.1
BASIC CONCEPTS
Deﬁnitions:
A strand of RNA consists of a sequence of four types of nucleotides distinguished by
a nitrogenous base (adenine (A), cytosine (C), guanine (G), uracil (U)) and connected
along a sugar-phosphate backbone.
The carbon atoms in the sugar (ribose) are numbered 1′ through 5′ and the 3′ carbon
in each sugar molecule is connected through a phosphate group to a 5′ carbon of the
next sugar molecule. This gives directionality to the RNA chain: a “front” end called
the 5′-end and a “back” end called the 3′-end.
Adenine and guanine are purines, and cytosine and uracil are pyrimidines.
The RNA strand folds onto itself by a formation of hydrogen bonds between the bases
from diﬀerent nucleotides. Two bases that share a chemical bond are called a base pair.
Facts:
1. The large majority of observed base pairs are either one of the two Watson-Crick
pairs, AU (two hydrogen bonds) and CG (three hydrogen bonds), or the weaker wobble
pair, GU (two hydrogen bonds). Other base pairs (e.g., purine-purine and pyrimidine-
pyrimidine pairs) are thermodynamically unstable and thus rare.
2. The length of an RNA strand can vary from a few tens to a few thousand nucleotides.
3. The structure of functional RNA has been evolutionary preserved. Therefore, when
given a set of homologous RNA sequences (the ones with shared ancestry), a common
structure can be found using covariation analysis. Even though highly accurate, this is
a manual process that cannot be applied to a single sequence and an important problem
in computational biology is the design of an equally accurate prediction method of low
computational complexity.
4. There exist viruses whose genetic material consists of RNA instead of DNA (e.g.,
SARS, polio, ebola, measles, and HIV).
Example:
1. The following ﬁgure illustrates the structure of RNA. The four chemical bases (A,
U, G, C) are attached to sugar groups (ribose R) on the sugar-phosphate (P) backbone.

1476
Chapter 20
DISCRETE BIOMATHEMATICS
The 5′ end is at the upper left and the 3′ end is at the lower right.
20.5.2
COMBINATORIAL MODELS
An RNA structure can be represented by a sequence over a four-letter alphabet with
additional information encoding which bases form pairs. These are typically represented
by graphs called arc diagrams.
Deﬁnitions:
An RNA sequence is a sequence b = b1b2 . . . bn, where bi ∈{A, C, G, U}.
A partial matching of V = {1, . . ., n} is a collection {(i1, j1), . . . , (ik, jk)} of disjoint
size-2 subsets of V called arcs.
An RNA structure is an RNA sequence b = b1b2 . . . bn along with a partial matching
of V = {1, . . ., n} which describes the base pairs. Elements in V are called nodes or
vertices.
A canonical way to view an RNA structure is by its arc diagram: draw either 1, . . . , n or
the sequence of bases horizontally, and then draw each arc (i, j) in the partial matching
in the upper-half plane.
The length of the arc (i, j) is j −i.
Two arcs (i1, j1) and (i2, j2) with i1 < i2 are
crossing if i1 < i2 < j1 < j2. Otherwise, they are noncrossing. These correspond to
geometric crossings and noncrossings of arcs in the arc diagram representation of RNA.
If the RNA structure has no crossings then it is called a secondary structure. Other-
wise, it is called a pseudoknot structure [Re10].
A pseudoknot structure is k-noncrossing if it has no k arcs that are mutually crossing.
A stack or helix of size σ is a maximal sequence of nested arcs
(i, j), (i + 1, j −1), . . . , (i + (σ −1), j −(σ −1)) .
A node v is accessible from a base pair (i, j) if i < v < j and there is no base pair (i′, j′)
such that i < i′ < v < j′ < j. A base pair (v, w) is accessible from (i, j) if both v and
w are accessible from it [SaEtal83].

Section 20.5
RNA FOLDING
1477
The null loop, or 0-loop, of an RNA structure is the set of nodes not accessible from
any base pair.
The k-loop Li,j closed by (i, j) is the subset of V formed from the k −1 base pairs and
the isolated bases that are accessible from (i, j). The size of a loop is the number of
isolated bases in it.
1-loops are also called hairpin loops and k-loops for k ≥3 are termed multibranch
loops. There are three types of 2-loops. If (i′, j′) is the (unique) base pair accessible
from (i, j), then the 2-loop closed by (i, j) is a
• stacked pair if i′ −i = j −j′ = 1 (i.e., it has size 0),
• bulge loop if exactly one of i′ −i and j −j′ is > 1,
• internal loop if both i′ −i and j −j′ are > 1.
Facts:
1. Base pairs of length 1 are biophysically infeasible. Base pairs of length 2 are thermo-
dynamically unstable and thus very rare. Most combinatorial models require a minimum
arc length λ ≥2, usually λ = 3 or λ = 4.
2. Let T λ(n) denote the number of length-n RNA secondary structures with minimum
arc length at least λ. There is no known closed formula for T λ(n), but it satisﬁes the
following recurrence [Wa78]:
T λ(n) = T λ(n −1) + Pn−(λ+1)
k=0
T λ(n −2 −k)T λ(k).
3. The numbers T λ(n) grow exponentially in n [StWa79]. For example,
T 2(n) ∼
q
15+7
√
5
8π
n−3/2 
3+
√
5
2
n
.
4. There is a bijection between secondary structures (noncrossing partial matchings) of
length n and Motzkin paths of length n: namely, lattice paths in the plane from (0, 0)
to (n, 0) consisting of three types of steps: ր , ց , and −→, where each has width 1.
5. The point-bracket notation of a size-n RNA secondary structure is a length-n string
over the 3-element alphabet

(, •, )
	
, where the three characters designate left arc
endpoint, isolated vertex, and right arc endpoint, respectively. Such a string corresponds
to a secondary structure if and only if any initial segment contains at least as many open
parentheses as closed ones.
6. RNA secondary structures can also be represented as plane trees in diﬀerent ways,
capturing various degrees of detail of the structure.
7. There is a bijection between k-noncrossing pseudoknot structures and the following
combinatorial structures (not deﬁned here, see [ChEtal07]):
• integer lattice walks in Zk−1 that start and end at (k −1, . . . , 2, 1) in the funda-
mental Weyl chamber of type Bk−1;
• vacillating standard Young tableaux of length n and height less than k.
8. Every RNA secondary structure has a well-deﬁned loop decomposition [SaEtal83] of
the vertex set into loops:
b = L0
S  S
(i,j) Li,j

,
where L0 is the null loop and the Li,j are distinct.

1478
Chapter 20
DISCRETE BIOMATHEMATICS
9. RNA pseudoknots were not discovered until 1982, when they were observed in the
RNA of the turnip yellow mosaic virus [RiEtal82].
10. A database of hundreds of pseudoknots can be found at [TaEtal09]. Most observed
pseudoknots are 3-noncrossing [TaEtal09].
11. The 3-noncrossing pseudoknots are precisely those that can be drawn as noncrossing
diagrams, if base pairs are additionally allowed to be drawn as arcs in the lower half-plane.
12. An RNA pseudoknot can be naturally associated a topological surface via a ribbon
graph or fatgraph [BoEtal08], [PeEtal10]. This allows pseudoknots to be classiﬁed by
their genus [AnEtal13].
Examples:
1. Two folds of the RNA strand b = AAAGUUCCUUUUUUGGAAAAAAA are shown
in the following diagram. The RNA structure on the left is a secondary structure because
its arc diagram is noncrossing.
The RNA structure on the right is a 3-noncrossing
pseudoknot.
A A A G U
U
C
C
U
U
U
U
U
U
G
G A A A A A A A
5′-end
3′-end
A A A G U U
C
C
U
U
U
U
U
U
G
G A A A A A A A
5′-end
3′-end
2. The arc diagrams corresponding to the RNA structures in Example 1 are shown next.
AAAGUUCCUUUUUUGGAAAAAAA
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15 16 17 18 19 20 21 22 23
AAAGUUCCUUUUUUGGAAAAAAA
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15 16 17 18 19 20 21 22 23
3. The Motzkin path and point-bracket notation of the secondary structure in Example 1
are displayed next.
0
1
2
3
4
5
6
7
8
9
10 11 12 13 14 15 16 17 18 19 20 21 22 23
0
1
2
3
4
• ( ( ( • • • ) ) ) ( ( ( • • • ) ) ) • • • •
4. The following ﬁgure shows an RNA structure with diﬀerent types of loops together
with several of its tree representations, capturing diﬀerent levels of structural detail.

Section 20.5
RNA FOLDING
1479
A
A-U
C-G
C-G
G-U
G
G
G
G
G-C
G-C
A-U
R
A-U
G-U
G-C
G-C
C
C
A-U
G-C
A
U
C-G
C
C
A
A
G
A-U
G-C
A-U
C-G
R 
M 
I 
B 
A
C U ACU C C G
A
U U
A C
C
A
A
U
G
G
G
U
A
C
G
U
G
G
U
G
U
G
C
G
G
C
C
G
U
G
G
C
A
C
C
G
A
U
G
G
C
U
A
GG
A
5
3
Hairpin loop
Internal loop
Bulge
Helix
Multibranch loop
B 
H 
H 
R 
M 
H 
H 
U-G
U-G
U-A
20.5.3
MINIMAL FREE ENERGY FOLDING ALGORITHMS
In thermodynamics, the free energy of a chemical structure is the amount of energy
needed to maintain its structural integrity. The lower the free energy the more stable
the structure is, whereas a positive free energy means that it would require energy to
maintain it. In most thermodynamics models for RNA folding, energy parameters or
“scores” are assigned to substructures (base pairs, loops, etc.), and the free energy is
determined by summing over all of its substructures.
Computing the minimum free
energy structure of an RNA sequence is a discrete optimization problem.
Deﬁnitions:
The dynamic programming approach to ﬁnding the minimal free energy of a secondary
structure of b = b1b2 . . . bn has two steps:
• recursive solution of the problem on substrings to ﬁnd the overall minimal free
energy;
• traceback, which recovers a structure that achieves this minimum.
Let e(i, j) be the free energy contribution if i and j were to form a base pair, and let
E(i, j) be the minimal energy of the subsequence bi,j = bibi+1 . . . bj.
Facts:
1. A simple energy-based RNA folding model that maximizes the number of base pairs
was proposed by Nussinov et al. [NuEtal78].

1480
Chapter 20
DISCRETE BIOMATHEMATICS
2. A weighted variant of the Nussinov model speciﬁes the free energies e(i, j) using
e(i, j) =









−3
{bi, bj} = {C, G},
i ≤j −4
−2
{bi, bj} = {A, U},
i ≤j −4
−1
{bi, bj} = {G, U},
i ≤j −4
0
otherwise.
3. In the model speciﬁed by Fact 2, the energy scores of −3 and −2 represent the number
of hydrogen bonds in the respective base pairs. The energy score −1 represents the two
hydrogen bonds appearing in the (less stable) wobble pairs.
4. The E(i, j) can be computed recursively via dynamic programming using
E(i, j) = min



E(i, j −1)
min
i≤k≤j−4 E(i, k −1) + E(k + 1, j −1) + e(k, j).
5. The recursion in Fact 4 can be justiﬁed as follows. If i > j −4 then E(i, j) = 0.
Otherwise, there are two possibilities for the structure that minimizes the energy: either
j is unpaired, in which case there is a secondary structure on bi,j−1, or j is paired with
some k, where i ≤k ≤j −4, in which case the noncrossing condition implies that the
secondary structure splits into substructures on bi,k−1 and bk+1,j−1. The optimal energy
score E(i, j) is the minimum value resulting from these two cases.
6. The value E(1, n) is the minimal free energy of b = b1,n.
7. The main assumption in the energy-based structure prediction methods is that the
RNA sequence folds to a structure that minimizes the free energy.
8. Nussinov’s model is overly simplistic. A more complex model for the free energy of
secondary structures is proposed in [TiEtal73]. In this model the free energy of a sec-
ondary structure is the sum of independent energies for each loop in the structure. The
model has evolved substantially over the years and since it assumes that the thermody-
namic stability of a base pair is dependent on the identity of the adjacent base pairs, it’s
known as the nearest-neighbor thermodynamic model (NNTM).
9. The free energy of an RNA structure depends primarily on three factors: (i) the
helices, or stacks, which are generally stabilizing and contribute negative free energy; (ii)
other loops, which are generally destabilizing and contribute positive free energy; and
(iii) the surrounding temperature.
10. State-of-the-art models have thousands of experimentally and computationally de-
termined parameters [TuMa10] and require computational resources to run, such as the
publicly available UNAFold web server [MaZu08]. Another popular RNA folding program
is the Vienna RNA Package [HoEtal94]. These programs, which exclude pseudoknots,
can be accessed at
• http://unafold.rna.albany.edu
• http://www.tbi.univie.ac.at/RNA
11. The theromodynamics of multi-branch loops and pseudoknots is still poorly under-
stood [DiTuMa01], [LyPe00].
12. The accuracy of the NNTM predictions varies widely [RoHe14].
13. Recent work in loop energy estimation ranks substructures more accurately but in
ways that are not handled well by dynamic programming [AaNa10], [ZhEtal08].

Section 20.5
RNA FOLDING
1481
14. Predicting RNA secondary structures containing pseudoknots of arbitrary types
is NP complete for a large class of reasonable free-energy functions [LyPe00]. Energy-
based, polynomial time algorithms for prediction of RNA secondary structures have been
developed in [Ak00], [RiEd99], [UeEtal99] but each of them allows a restricted class of
pseudoknots.
Examples:
1. Consider the sequence b = GGGACCUUCC. The e(i, j) can be calculated using
Fact 2. Namely, e(1, 5) = −3, e(1, 6) = −3, e(2, 6) = −3, e(1, 7) = −1, e(2, 7) = −1, . . .,
e(1, 10) = −3, e(2, 10) = −3, e(3, 10) = −3, e(4, 10) = 0, e(5, 10) = 0, e(6, 10) = 0.
2. Using the sequence in Example 1, we can recursively calculate the E(i, j) values. For
example, E(1, 5) = min{E(1, 4), E(1, 0) + E(2, 4) + e(1, 5)} = min{0, −3} = −3. In a
similar way, we determine E(1, 6) = −3 and E(2, 6) = −3. Consequently, E(1, 7) =
min{E(1, 6), E(1, 0) + E(2, 6) + e(1, 7), E(1, 1) + E(3, 6) + e(2, 7), E(1, 2) + E(4, 6) +
e(3, 7)} = min{−3, −4, −1, −1} = −4.
3. The values E(i, j) can be conveniently stored in a table. The following table shows
the values of E(i, j) for the sequence b = GGGACCUUCC.
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
G
G
G
A
C
C
U
U
C
C
G
G
G
A
C
C
U
U
C
C
−3
−3
−1
−2
0
0
−3
−3
−2
−2
0
−4
−3
−5
−2
−4
−5
−5
−6
−8
−8
G G G A C C U U C C
E(b) = −8
C C U U
C
C
A
G
G
G
The minimal free energy is E(1, 10), which is the upper-right entry in the table. It is
calculated as
E(1, 10)
=
min{E(1, 9), E(1, 0) + E(2, 9) + e(1, 10), E(1, 1) + E(3, 9) + e(2, 10),
E(1, 2) + E(4, 9) + e(3, 10), E(1, 3) + E(5, 9) + e(4, 10),
E(1, 4) + E(6, 9) + e(5, 10), E(1, 5) + E(7, 9) + e(6, 10)}
=
min{−6, −8, −8, −5, 0, 0, −3} = −8.
The actual RNA structure can be recovered by the dynamic programming traceback
step: start by circling the E(1, 10) entry, then circle a value that could have preceded
it in the recursive step, and repeat until a value of 0 is reached. The base pairs can
be recovered from the “jumps” in the circled values. One RNA structure achieving this
minimum energy is shown in the preceding ﬁgure.

1482
Chapter 20
DISCRETE BIOMATHEMATICS
20.5.4
LANGUAGE-THEORETIC METHODS
Formal language theory arose in the ﬁeld of linguistics, in an attempt to understand
the syntactic structure and evolution of natural languages. Since the 1950s, it has been
applied to genetics, because DNA and RNA sequences can be viewed as languages over
a four-letter alphabet.
Deﬁnitions:
A language is a set of (ﬁnite) strings over an alphabet Σ of terminal symbols.
A grammar is a collection of production rules for strings in a given language that dictates
how temporary nonterminal symbols can be rewritten into strings of terminal symbols,
along with a distinguished start symbol S.
A derivation of a string α is a sequence of production rules transforming S into α.
A context-free grammar (CFG) is a grammar in which every production rule is of
the form V →α, where V is a nonterminal symbol and α is a string of terminal and/or
nonterminal symbols.
A stochastic context-free grammar (SCFG) assigns probabilities to the production
rules, so that for each nonterminal symbol V , the probabilities Pr(V →αi) taken over
all αi sum to 1.
The Knudsen-Hein grammar [KnHe99] is an SCFG for RNA folding deﬁned as follows,
where pi + qi = 1 for each i = 1, 2, 3.
production
probability
production
probability
S →LS
p1
S →L
q1
L →dFd′
p2
L →s
q2
F →dFd′
p3
F →LS
q3
Each string in the resulting language corresponds to a secondary structure with minimal
arc length 3. The terminals s, d, d′ represent an isolated nucleotide, a left base pair end,
and a right base pair end, respectively. The nonterminals L and F create loops and
helices, respectively.
The probability of a derivation is the product of the probabilities of each individual
rule used and the probability of a structure is the sum of all left-to-right derivations
that yield that structure.
Facts:
1. The most restrictive class of grammars in the Chomsky hierarchy is the class of regular
grammars. These can be used to generate RNA sequences but not RNA structures.
2. The location of terminal symbols in regular languages is uncorrelated. Long distance
pairwise correlations between nucleotides in a sequence can be modeled by CFGs but not
by regular grammars. Therefore, CFGs are generally used for RNA structure prediction
algorithms.
3. Probabilistic modeling approaches for RNA secondary structure prediction using
SCFGs were ﬁrst introduced in the 1990s [SaEtal94].
4. The unknown probability parameters in an SCFG can be estimated using “training
algorithms” such as the inside-outside algorithm [Ba79], [LaYo90].

Section 20.6
COMBINATORIAL NEURAL CODES
1483
5. The Cocke-Younger-Kasami algorithm ﬁnds the probability of the most likely sec-
ondary structure for a given RNA sequence.
A traceback procedure can be used to
recover a structure that achieves this maximum probability [DuEtal98].
6. The Knudsen-Hein grammar is used in the secondary structure prediction software
Pfold [KnHe03]. This method predicts a structure for a set of aligned sequences. Pre-
dictions can be done on a web server at
• http://www.daimi.au.dk/~compbio/pfold
7. The Knudsen-Hein grammar was compared to eight other grammars in [DoEd04],
and was shown to perform at least as well as more complicated grammars.
8. It has been shown that the number of diﬀerent motifs (base pairs, helices, hairpin
loops, internal loops, etc.) is normally distributed among structures generated by the
Knudsen-Hein grammar [HePo14].
9. A topological approach to fold RNA pseudoknots with an SCFG has been proposed
[ReEtal11]; citations within this work refer to other language-theoretic approaches that
include pseudoknots.
Example:
1. The unique left-to-right derivation of the structure S for b = GGACUGC is
S
q1
=⇒L
p2
=⇒dFd′
q3
=⇒dLSd′
p2
=⇒ddFd′Sd′
q3
=⇒ddLSd′Sd′
q2
=⇒ddsSd′Sd′
q1
=⇒ddsLd′Sd′
q2
=⇒ddssd′Sd′
q1
=⇒ddssd′Ld′
q2
=⇒ddssd′sd′
and therefore Pr(S) = p2
2q3
1q3
2q2
3.
From the string α = ddssd′sd′ we infer the following RNA structure
G G A C U G C
20.6
COMBINATORIAL NEURAL CODES
Neural codes are the brain’s way of representing, transmitting, and storing information
about the world. Combinatorial neural codes are based on binary patterns of neural
activity, as opposed to the precise timing or rate of neural activity. The structure of a
combinatorial code may reﬂect important aspects of the represented stimuli or network
architecture. Combinatorial codes can be analyzed using an algebraic object called the
neural ring.
20.6.1
BASIC CONCEPTS
From simultaneous recordings of neurons in the brain we can infer which subsets of
neurons tend to ﬁre together. This information is captured by a combinatorial code.

1484
Chapter 20
DISCRETE BIOMATHEMATICS
Deﬁnitions:
The set of neurons is denoted by [n] = {1, . . . , n}.
An action potential, or spike, is an electrical event in a single neuron. This is the
fundamental unit of neural activity. We say that a neuron “ﬁres” action potentials, or
spikes.
A spike train is a sequence of spike times for a single neuron. This captures the electrical
activity of the neuron over time.
A codeword is a string of 0s and 1s, with a 1 for each active neuron and a 0 denoting
silence; equivalently, it is a subset σ ⊆[n] of (active) neurons ﬁring together.
For
example, if n = 6 the subset σ = {145} ⊆[6] is also denoted 100110.
A combinatorial neural code is a collection of codewords C ⊆2[n]. In other words, it
is a binary code of length n, where each binary digit is interpreted as the “on” or “oﬀ”
state of a neuron.
A maximal codeword is a codeword that is maximal in the code under inclusion. If
σ ∈C is maximal, then there is no τ ∈C such that τ ⊋σ.
An abstract simplicial complex ∆⊆2[n] is a collection of subsets of [n] that is closed
under inclusion (see §16.6.1). That is, if σ ∈∆and τ ⊂σ, then τ ∈∆. A facet of ∆is
an element of ∆that is maximal under inclusion.
The simplicial complex of a code, ∆(C), is the smallest abstract simplicial complex
on [n] that contains all elements of C:
∆(C) = {σ ⊆[n] | σ ⊆τ for some τ ∈C}.
Facts:
1. Spikes (action potentials) are all-or-none electrical events. It thus suﬃces to keep
track only of the spike times, as in a spike train.
2. Most combinatorial neural codes appear ill-suited for error correction [CuEtal13a].
3. Simplicial complexes are heavily-studied objects in topology and algebraic combina-
torics.
4. Each facet of ∆(C) corresponds to a maximal codeword of C.
5. The simplicial complex ∆(C) is useful for analyzing a code, but discards important
information.
All codes with the same maximal codewords have the same simplicial
complex.
6. Manin [Ma15] provides an historical overview contrasting neural codes with error-
correcting codes and cryptography.
Example:
1. Combinatorial codes can be obtained from neural data by temporally binning the
spikes into patterns of 0s and 1s. The following ﬁgure depicts a set of binned spike trains
and the resulting codewords. The set of unique codewords is the code C. The simplicial
complex ∆(C) has facets corresponding to the two maximal codewords, 1110 and 1101.

Section 20.6
COMBINATORIAL NEURAL CODES
1485
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
spike trains
codewords
neuron #
neuron #
time
1
2
3
4
1110
0001
0101
1101
1100
0010
code
simplicial complex
20.6.2
THE CODE OF A COVER
An important type of combinatorial neural code is one deﬁned by an arrangement of
open sets in Euclidean space. The open sets correspond to receptive ﬁelds.
Deﬁnitions:
A stimulus space X is a parametric space of stimuli. The stimuli could be sensory,
such as visual, auditory, or olfactory signals, or higher-level, such as an animal’s position
in space. Typically, a stimulus space is modeled as a subset of Euclidean space, X ⊆Rd.
A receptive ﬁeld is a subset Ui ⊆X of the stimulus space corresponding to a single
neuron i. The stimuli in Ui induce neuron i to ﬁre.
A subset V ⊆Rn is convex if, given any pair of points x, y ∈V , the point z = tx+(1−t)y
is contained in V for any t ∈[0, 1].
Convex receptive ﬁelds are convex subsets Ui ⊆X.
A collection of open sets U = {U1, . . . , Un} is an open cover of their union Sn
i=1 Ui.
U is a good cover if every nonempty intersection T
i∈σ Ui is contractible (that is, if it
can be continuously shrunk to a point).
The nerve of an open cover U is the simplicial complex
N(U) = {σ ⊆[n] |
\
i∈σ
Ui ̸= ∅}.
Given an open cover U, the code of the cover is the combinatorial neural code
C(U) = {σ ⊆[n] |
\
i∈σ
Ui \
[
j∈[n]\σ
Uj ̸= ∅}.
Facts:
1. Neurons in many brain areas, such as sensory cortices and the hippocampus, have
activity patterns that can be characterized by receptive ﬁelds.
2. Receptive ﬁelds are computed experimentally by correlating neural responses to in-
dependently measured external stimuli.

1486
Chapter 20
DISCRETE BIOMATHEMATICS
3. Intersections of convex sets are always convex, and all convex sets are contractible.
Thus, any open cover consisting of convex sets is a good cover.
4. Each codeword in C(U) corresponds to a region that is deﬁned by the intersections
of the open sets in U [CuEtal13b].
5. If U is an open cover, then C(U) ⊆N(U) and ∆(C(U)) = N(U). The nerve of the
cover can thus be recovered from the code by completing it to a simplicial complex, but
the code contains additional information about U that is not captured by the nerve alone.
6. Nerve lemma: If U is a good cover, then the covered space Y = Sn
i=1 Ui is homotopy-
equivalent to N(U). In particular, Y and N(U) have exactly the same homology groups.
7. Helly’s theorem:
Consider k convex subsets U1, . . . , Uk ⊆Rd, for d < k. If the
intersection of every d + 1 of these sets is nonempty, then the full intersection Tk
i=1 Ui is
also nonempty.
8. In addition to Helly’s theorem and the Nerve lemma, there is a great deal known
about N(U) for collections of convex sets in Rd. In particular, the f-vectors of such
simplicial complexes have been completely characterized by G. Kalai [Ka84], [Ka86].
9. The Nerve lemma has been exploited in the context of two-dimensional place ﬁeld
codes to show that topological features of an animal’s environment could be inferred
from neural codes representing position in the hippocampus [CuIt08].
Example:
1. The following ﬁgure, adapted from [CuEtal15], depicts an open cover consisting of
four convex sets (A) as well as the corresponding code (B). The nerve of the cover (C)
is identical to the simplicial complex ∆(C).
U1
U3
U4
U2
1
2
4
3
1000
1100
1010
1110
0010
0110
0001
0111
0011
0000
0000
1000
1100
1110
1010
0110
0111
0010
0011
0001
A
B
C
U1,U2,U3,U4
20.6.3
THE NEURAL RING AND IDEAL
The structure of a combinatorial code can be analyzed using the neural ring and ideal.
These are algebraic objects that keep track of the combinatorics of the code, much as the
Stanley-Reisner ring and ideal encode a simplicial complex [MiSt05]. For more details,
see [CuEtal13b].
Deﬁnitions:
F2 is the ﬁeld with two elements {0, 1}. We can regard a codeword on n neurons as an
element of Fn
2 and a combinatorial neural code as a subset C ⊆Fn
2 .
F2[x1, . . . , xn] is a polynomial ring with coeﬃcients in F2.

Section 20.6
COMBINATORIAL NEURAL CODES
1487
The ideal IC is the set of polynomials that vanish on all codewords in C:
IC = I(C) = {f ∈F2[x1, . . . , xn] | f(c) = 0 for all c ∈C}.
The neural ring RC is the quotient ring
RC = F2[x1, . . . , xn]/IC.
A pseudo-monomial is a polynomial f ∈F2[x1, . . . , xn] that can be written as
f =
Y
i∈σ
xi
Y
j∈τ
(1 −xj),
where σ, τ ⊆[n] satisfy σ ∩τ = ∅.
For any binary string v ∈Fn
2 , the indicator function
χv =
Y
{i|vi=1}
xi
Y
{j|vj=0}
(1 −xj)
is a pseudo-monomial with the property that χv(v) = 1 and χv(c) = 0 for any c ̸= v.
The neural ideal JC is generated by the indicator functions of all non-codewords:
JC = ⟨χν | ν ∈Fn
2 \ C⟩.
A pseudo-monomial f ∈JC is called minimal if there does not exist another pseudo-
monomial g ∈JC with deg(g) < deg(f) such that f = hg for some h ∈F2[x1, . . . , xn].
The canonical form of JC is the set of all minimal pseudo-monomials:
CF(JC) = {f ∈JC | f is a minimal pseudo-monomial}.
Facts:
1. A polynomial f ∈F2[x1, . . . , xn] can be evaluated on a binary string of length n
(such as a codeword) by simply replacing each indeterminate xi with the 0/1 value of
the ith position in the string. For example, if f = x1x3(1 −x2) ∈F2[x1, . . . , x4], then
f(1011) = 1 and f(1100) = 0.
2. Irrespective of C, the ideal IC always contains the relations B = ⟨x2
1−x1, . . . , x2
n−xn⟩,
due to the binary nature of codewords.
3. The ideals IC and JC carry all the combinatorial information about the code C. They
are closely related: IC = JC + B.
4. Fundamental lemma: Let C ⊆{0, 1}n be a neural code, and let U = {U1, . . . , Un} be
any collection of open sets (not necessarily convex) such that C = C(U). Then, for any
pair of subsets σ, τ ⊆[n],
Y
i∈σ
xi
Y
j∈τ
(1 −xj) ∈IC ⇔
\
i∈σ
Ui ⊆
[
j∈τ
Uj.
5. The canonical form is a special basis, similar to a Grobner basis but tailored to
a diﬀerent purpose. From the canonical form one can read oﬀminimal relationships
between receptive ﬁelds.
6. The canonical form CF(JC) can be computed algorithmically, starting from the code
C. In [CuEtal13b, Section 4.5], one such algorithm is described that uses the primary
decomposition of pseudo-monomial ideals.
This algorithm has since been improved
[CuYo15], and software for computing CF(JC) is publicly available at
• https://github.com/nebneuron/neural-ideal

1488
Chapter 20
DISCRETE BIOMATHEMATICS
Examples:
1. The code C(U) shown in panel (B) of §20.6.2, Example 1 has ten codewords and six
non-codewords: 0100, 1001, 0101, 1101, 1011, and 1111. The neural ideal is
JC
=
⟨x2(1 −x1)(1 −x3)(1 −x4), x1x4(1 −x2)(1 −x3), x2x4(1 −x1)(1 −x3),
x1x2x4(1 −x3), x1x3x4(1 −x2), x1x2x3x4⟩.
The canonical form is
CF(JC) = {x1x4, x2(1 −x1)(1 −x3), x2x4(1 −x3)}.
Using the fundamental lemma, we can read oﬀthe following receptive ﬁeld relationships:
U1 ∩U4 = ∅, U2 ⊆U1 ∪U3, and U2 ∩U4 ⊆U3. This is consistent with the original
arrangement of open sets shown in panel (A) of §20.6.2, Example 1.
2. The code C = {111, 011, 001, 000} on three neurons has the canonical form CF(JC) =
{x1(1 −x2), x1(1 −x3), x2(1 −x3)}. This indicates that U1 ⊆U2, U1 ⊆U3, and U2 ⊆U3.
20.6.4
CONVEX CODES
Deﬁnitions:
Let C be a combinatorial neural code on n neurons.
If there exists an open cover U = {U1, . . . , Un} such that C = C(U) and each Ui is a
convex open subset of Rd, then C is a convex code.
The minimum embedding dimension d(C) of a convex code is the minimum dimen-
sion such that C admits a convex representation.
A code C = C(U) has a local obstruction if there exists a nonempty intersection
Uσ = T
i∈σ Ui such that Uσ ⊆S
i∈τ Ui, but the nerve of the cover {Ui ∩Uσ}i∈τ is not
contractible.
Facts:
1. Convex codes have been observed in several brain areas. Orientation-selective neu-
rons in the visual cortex [BeBaSo95] have convex receptive ﬁelds that reﬂect a neuron’s
preference for a particular angle. Hippocampal place cells [McNEtal06], [OKDo71] are
neurons that have spatial receptive ﬁelds, called place ﬁelds, that are typically convex.
2. If C has a local obstruction, then C is not a convex code.
3. All codes on n ≤2 neurons are convex.
4. For n ≤3 there are six non-convex codes (up to permutation-equivalence, including
the all-zeros word): namely, {000, 010, 001, 110, 101}, {000, 010, 110, 101}, {000, 110, 101},
{000, 100, 010, 110, 101, 011}, {000, 100, 110, 101, 011}, {000, 110, 101, 011}.
Examples:
1. The code C = C(U) shown in panel (B) of §20.6.2, Example 1 is convex by construc-
tion. Panel (A) shows a two-dimensional convex realization. The minimum embedding
dimension for this code is d(C) = 2.
2. Consider the code ˆC = C \ {0110}, where C is the code in Example 1.
Code ˆC
diﬀers from C by only one codeword and has the same simplicial complex ∆( ˆC) = ∆(C).
However, ˆC is not a convex code. It has a local obstruction because U2 ∩U3 ⊆U1 ∪U4,
yet the nerve of the cover of Uσ = U2 ∩U3 by U1 ∩Uσ and U4 ∩Uσ is disconnected, and
thus not contractible.

Section 20.6
COMBINATORIAL NEURAL CODES
1489
3. The codes C1 = {111, 011, 001} and C2 = {111, 101, 011, 110, 100, 010} are both con-
vex and have the same simplicial complex, but possess diﬀerent embedding dimensions:
d(C1) = 1, while d(C2) = 2.
Open Questions:
1. How do we determine, in general, whether or not a code is convex?
2. Are there other obstructions to convexity beyond local obstructions?
3. If a code is convex, what is the minimum embedding dimension?
20.6.5
FEEDFORWARD AND HYPERPLANE CODES
Hyperplane codes are an important class of combinatorial codes. These are codes that
arise as an output of a one-layer feedforward neural network, and they are sometimes
referred to as feedforward codes.
Deﬁnitions:
A hyperplane code is a convex code, where the underlying open convex cover U =
{Ui}n
i=1 can be obtained as Ui = X ∩H+
i , where X ⊆Rm is an open convex set and the
H+
i = {y ∈Rm |
m
X
a=1
wiaya −θi > 0}
(3)
are open half-spaces.
A one-layer feedforward neural network is a network with input and output layers
connected as shown in the following ﬁgure.
The network inputs nonnegative numbers ya ≥0 and outputs nonnegative numbers
xi ≥0 according to the rule
xi(y) = φ
 m
X
a=1
wiaya −θi
!
,
i ∈[n].
(4)
Here θi ∈R are the neuronal thresholds, wia ∈R are the eﬀective strengths of the
feedforward connections, and the transfer function φ: R →R>0 satisﬁes the condition
φ(t) = 0 if t ≤0 and φ(t) > 0 if t > 0.
A feedforward code is a hyperplane code, where the underlying convex set X can be
chosen to be the positive orthant Rm
+. This class of codes arises as the output of a one-
layer feedforward neural network (4), where positivity of each row of (4) corresponds to
the halfspace H+
i in (3). Speciﬁcally the code of the network (4) is
C(w, θ) =

σ ⊆[n] | ∃y ∈Rm
+ such that xi(y) > 0 ⇔i ∈σ
	
.

1490
Chapter 20
DISCRETE BIOMATHEMATICS
Facts:
1. Hyperplane codes (and thus feedforward codes) are convex.
2. Not every convex code is a hyperplane code. Perhaps the smallest example is the code
C = {∅, 2, 3, 4, 12, 13, 14, 123, 124}, which can be easily seen to possess a 2-dimensional
convex realization. However, it can be proved to be not realizable as a hyperplane code.
3. Theorem [GiIt14]:
For every simplicial complex K with n vertices, there exists a
feedforward network (w, θ) described by (4) so that K is the simplicial complex of the
appropriate feedforward code K = ∆(C (w, θ)).
Example:
1. The following ﬁgure, adapted from [GiIt14], displays a feedforward code for a network
with two neurons in the input layer, corresponding to the axes y1 and y2, and three
neurons in the output layer, corresponding to the (oriented) hyperplanes H1, H2, and
H3. For each output neuron xi, the inputs y = (y1, y2) that yield xi(y) > 0 lie in the
positive halfspace H+
i . The resulting code C consists of combinations of output neurons
that can be simultaneously activated by at least one choice of nonnegative inputs.
1
2
3
C = {∅, {1}, {2}, {3}, {1, 2}}
= {000, 100, 010, 001, 110}
20.7
FOOD WEBS AND GRAPHS
The study of food webs has occurred over the last ﬁfty years, primarily undertaken by
ecologists working in natural habitats. Mathematicians subsequently became interested
in the graph-theoretical properties of food webs and their corresponding competition
graphs. This section introduces food webs, together with associated graphs and param-
eters that play an important role in linking mathematics and ecology.
20.7.1
MODELING PREDATOR-PREY RELATIONSHIPS WITH FOOD WEBS
Food webs, through both direct and indirect interactions, describe the ﬂow of energy
through an ecosystem, moving from one organism to another. Understanding food webs
can help to predict how important any given species is, and how ecosystems change with
the addition of a new species or removal of an existing species.

Section 20.7
FOOD WEBS AND GRAPHS
1491
Deﬁnitions:
A food web is represented by a directed graph (digraph) D = (V, A) with vertex set V
and arc set A. Each vertex represents a species in the ecosystem and the arc (x, y) is
directed from a prey species x to a predator y of that prey.
A basal species is one that does not depend for food on any other organism in the
ecosystem. That is, these are species located at the bottom of the food web.
Facts:
1. In the early 1960s, when food webs were ﬁrst used to model predator-prey relation-
ships, arcs were directed from predator to prey. The current usage, which tracks the ﬂow
of energy from prey to predator, reverses this earlier convention.
2. The interactions of species as they attempt to acquire food determine much of the
structure of a community.
Food webs represent these feeding relationships within a
community.
3. Basal species correspond to vertices with no incoming arcs: vertices with indegree 0.
Species at the top of the food web correspond to vertices with outdegree 0.
4. The digraph of a food web contains no directed cycles (since a species does not prey
upon itself, either directly or indirectly).
5. Various online tools can be used to construct food webs from ecological data, such as
• http://bioquest.org/esteem
Examples:
1. The following ﬁgure depicts a food web in which sharks eat sea otters, sea otters eat
sea urchins and large crabs, large crabs eat small ﬁshes, and sea urchins and small ﬁshes
eat kelp. Equivalently, sea urchins and large crabs are eaten by sea otters (both are prey
for sea otters) and sea otters are prey for sharks. Kelp is the only basal species, whereas
sharks are at the top of this food web.
Sea Urchins
K elp
Sharks
Sea Otters
Large Crabs
Small Fishes
2. A larger food web is deﬁned by the following predator-prey relationships.

1492
Chapter 20
DISCRETE BIOMATHEMATICS
species
species they feed on
sharks
sea otters
sea otters
sea stars, sea urchins, large ﬁsh & octopus,
large crabs, abalone
sea stars
abalone, small herbivorous ﬁshes, sea urchins,
organic debris
sea urchins
kelp, sessile invertebrates, organic debris
abalone
organic debris
large crabs
sea stars, small predatory ﬁshes, organic debris,
small herbivorous ﬁshes, kelp
small predatory ﬁshes
sessile invertebrates, planktonic invertebrates
small herbivorous ﬁshes
kelp
large ﬁsh & octopus
large crabs, small predatory ﬁshes
sessile invertebrates
microscopic planktonic algae, planktonic invertebrates
planktonic invertebrates
microscopic planktonic algae
From these relationships, the following food web can be constructed. The basal species
are seen to be kelp, organic debris, and microscopic planktonic algae. Sharks are the
only species at the top of this food web.
sharks
sea otters
sea starsl
arge crabs
large fish & octopus
small herbivorous fishes
abalone
small predatory fishes
sessile invertebrates
organic debris
planktonic invertebrates
kelp
microscopic planktonic algae
sea urchins
20.7.2
TROPHIC LEVEL AND TROPHIC STATUS
Trophic levels in food webs provide a way of organizing species in a community food web
into feeding groups. Scientists have used various measures to classify species in a food
web into these various feeding groups, typically based on the positioning of species in the
food web.
Deﬁnitions:
The length of a path in a directed graph (digraph) D is the number of arcs in the path.
A shortest path between vertices x and y in D has the smallest length of any path
from x to y; this shortest path length is denoted d(x, y).

Section 20.7
FOOD WEBS AND GRAPHS
1493
A longest path between vertices x and y in D has the largest length of any path from
x to y; this longest path length is denoted D(x, y).
Notation: We write u ≺v if species v consumes species u, either directly or indirectly.
The trophic level (option 1) T L1(x) of species x is deﬁned to be 0 if x is a basal species.
Otherwise, it is the smallest length of a path to x from any basal species that x consumes,
either directly or indirectly: T L1(x) = min{d(y, x) : y ≺x, and y is basal}.
The trophic level (option 2) T L2(x) of species x is deﬁned to be 0 if x is a basal species.
Otherwise, it is the largest length of a path to x from any basal species that x consumes,
either directly or indirectly: T L2(x) = max{D(y, x) : y ≺x, and y is basal}.
The trophic status T S(x) of species x is deﬁned to be 0 if x is a basal species. Otherwise,
it is given by T S(x) = P{D(y, x) : y ≺x}.
Species v is called dominant if there exists an arc (u, v), whose removal from the food
web allows u to have uncontrolled growth, and thus become a new “dominant” species.
A species is trophic status dominant if its trophic status is greater than the number
of non-basal species in the food web.
Facts:
1. Since the digraph D corresponding to a food web is acyclic, the longest path length
D(x, y) between any two vertices is well deﬁned and ﬁnite.
2. The trophic level T L1(x) can be calculated recursively as follows:
T L1(x) = 1 + min{T L1(y) : (y, x) ∈A}, where T L1(x) = 0 if x is a basal species.
3. The trophic level T L2(x) can be calculated recursively as follows:
T L2(x) = 1 + max{T L2(y) : (y, x) ∈A}, where T L2(x) = 0 if x is a basal species.
4. T L2(x) ≥T L1(x) holds for any species x in a food web.
5. Neither of the trophic level deﬁnitions T L1 or T L2 is entirely satisfactory in deter-
mining the hierarchical structure of a food web. For instance, neither deﬁnition reﬂects
the number of species that are direct or indirect prey of a species.
6. A reasonable property is that if species x is a predator of species y, then the trophic
level of species x is greater than the trophic level of species y. This property is satisﬁed
by T L2 and T S, but not by T L1.
7. High trophic status of a species means that there are many paths of various lengths
that reach that species in the food web, so the loss of any one path is inconsequential.
8. If vertex u has only one outgoing arc (u, v), then v is dominant. Such a vertex u
therefore has outdegree 1.
9. The concept of trophic status dominance incorporates both the number of species
that are direct or indirect prey and the extent of energy transfer, based on the trophic
status of the species. This deﬁnition resembles the deﬁnition of status for people in a
community or social network. In a social network, the status of person A is determined
by the number of people who can reach A and the (worst-case) lengths of paths to A.

1494
Chapter 20
DISCRETE BIOMATHEMATICS
Examples:
1. The trophic level (option 1) values for Example 1 of §20.7.1 are computed as fol-
lows: T L1(kelp) = 0; T L1(sea urchins) = T L1(small ﬁshes) = 1; T L1(large crabs) =
T L1(sea otters) = 2; T L1(sharks) = 3. Notice that from Fact 2, we have T L1(sea otters)
= 1 + min{T L1(sea urchins), T L1(large crabs)} = 1 + min{1, 2} = 2.
2. The trophic level (option 1) values for Example 2 of §20.7.1 are as follows: T L1(kelp) =
T L1(organic debris) = T L1(microscopic planktonic algae) = 0; T L1(abalone) = T L1(sea
stars) = T L1(sea urchins) = T L1(large crabs) = T L1(small herbivorous ﬁshes) = T L1(
sessile invertebrates) = T L1(planktonic invertebrates) = 1; T L1(sea otters) = T L1(small
predatory ﬁshes) = T L1(large ﬁsh & octopus) = 2; T L1(sharks) = 3.
Fact 2 veri-
ﬁes that T L1(sea otters) = 1 + min{T L1(sea urchins), T L1(sea stars), T L1(large crabs),
T L1(large ﬁsh & octopus)} = 1 + min{1, 1, 1, 2} = 2.
3. The trophic level (option 2) values for Example 1 of §20.7.1 are computed as fol-
lows: T L2(kelp) = 0; T L2(sea urchins) = T L2(small ﬁshes) = 1; T L2(large crabs) = 2;
T L2(sea otters) = 3; T L2(sharks) = 4. Notice that from Fact 3, we have T L2(sea otters)
= 1 + max{T L2(sea urchins), T L1(large crabs)} = 1 + max{1, 2} = 3.
4. The trophic level (option 2) values for Example 2 of §20.7.1 are computed as follows:
T L2(kelp) = T L2(organic debris) = T L2(microscopic planktonic algae) = 0; T L2(small
herbivorous ﬁshes) = T L2(abalone) = T L2(planktonic invertebrates) = 1; T L2(sessile
invertebrates) = 2; T L2(sea urchins) = T L2(small predatory ﬁshes) = 3; T L2(sea stars)
= 4; T L2(large crabs) = 5; T L2(large ﬁsh & octopus) = 6; T L2(sea otters) = 7; T L2(
sharks) = 8. Notice that T L2(x) ≥T L1(x) holds for each species x, as guaranteed by
Fact 4.
5. The trophic status of sea otters in Example 1 of §20.7.1 can be calculated by ﬁrst
determining longest path lengths to sea otters from all species that sea otters feed upon
(directly or indirectly): namely, D(sea urchins, sea otters) = D(large crabs, sea otters) =
1, D(small ﬁshes, sea otters) = 2, D(kelp, sea otters) = 3. This gives T S(sea otters =
1 + 1 + 2 + 3 = 7.
Similar calculations produce T S(kelp) = 0, T S(sea urchins) =
T S(small ﬁshes) = 1, T S(large crabs) = 3, and T S(sharks) = 12.
6. Trophic status for the species in Example 2 of §20.7.1 are found as T S(kelp) = T S(
organic debris)= T S(microscopic planktonic algae) = 0; T S(planktonic invertebrates) =
T S(abalone) = T S(small herbivorous ﬁshes)= 1; T S(sessile invertebrates) = 3; T S(small
predatory ﬁshes) = 6; T S(sea urchins) = 8; T S(sea stars) = 16; T S(large crabs) = 26;
T S(large ﬁsh & octopus) = 37; T S(sea otters) = 49; T S(sharks) = 62.
7. If the arc (sea urchins, sea otters) is removed from the food web in Example 1 of
§20.7.1, then sea urchins will have uncontrolled growth and become a new “dominant”
species in the food web. Therefore sea otters are considered a dominant species. Using
Fact 8, since (small ﬁshes, large crabs) is the only arc leaving small ﬁshes, large crabs
are a dominant species. Similarly, (sea otters, sharks) is the only arc leaving sea otters,
so sharks are dominant.
8. In the food web for Example 2 of §20.7.1, both sharks and sea otters are dominant.
9. In the food web for Example 2 of §20.7.1, sharks, sea otters, sea stars, large crabs, and
large ﬁsh & octopus are trophic status dominant since each has trophic status exceeding
14 −3 = 11. By contrast, only sharks and sea otters are dominant using the arc removal
deﬁnition (see Example 8).

Section 20.7
FOOD WEBS AND GRAPHS
1495
20.7.3
WEIGHTED FOOD WEBS
Not all ecological relationships have the same strength. Species may consume much more
of one prey species than another. To model this, we can assign weights to the arcs of a
food web to indicate food preferences.
Deﬁnitions:
The weight of arc (x, y) in a food web, denoted wxy, is the proportional food contribution
of species x to species y in the food web.
The ﬂow-based trophic level T LF(x) of species x is deﬁned to be 1 if x is a basal
species. Otherwise, it is based on the weighted sum of ﬂow-based trophic levels of the
species y that it directly consumes: T LF(x) = 1 + P{wyxT LF(y) | (y, x) ∈A}.
Fact:
1. The sum of the weights of the incoming arcs to a species is 1, since the set of incoming
arcs represents the full diet of the species.
Examples:
1. The following ﬁgure depicts a weighted food web. For example, the weight 0.6 on the
arc from rodents to snakes indicates that snakes eat rodents more frequently than other
lizards in the ratio of 6 to 4. Speciﬁcally, 60% of a snake’s diet comes from rodents, while
40% of its diet comes from other lizards.
Snakes
Rodents
Hawks
Foxes
Lizards
Other Lizard
I
s
nsects
1.0
0.7
0.4
0.6
1.0
0.3
Here T LF(snakes) = 1 + 0.6(1) + 0.4(1) = 2 and T LF(lizards) = 1 + 1(1) = 2, so that
T LF(hawks) = 1 + 0.7(2) + 0.3(2) = 3.
2. If prairie dogs are removed from the following weighted food web, then black-footed
ferrets have no food source and so will die oﬀ. If jackrabbits & small rodents are removed,
and species can survive on 50% of their normal diet, then coyotes will survive, but golden
eagles will have to increase their consumption of antelopes, prairie dogs, and black-footed
ferrets, a challenge given their limited numbers.
Flow-based trophic levels for species in this weighted food web are given by T LF(grasses
& sedges) = 1, T LF(prairie dogs) = T LF(jackrabbits & small rodents) = T LF(prong-
horn antelopes)= 2, T LF(black-footed ferrets) = 3, T LF(golden eagles) = T LF(coyotes)
= 3.05.

1496
Chapter 20
DISCRETE BIOMATHEMATICS
Coyotes
1.0
0.25
0.2
0.6
0.05
0.5
Prairie Dogs
Pronghorn 
Antelopes
Grasses & Sedges
Jackrabbits & 
Small Rodents
Black-footed 
Ferrets
Golden Eagles
1.0
1.0
0.05
0.05
0.3
1.0
20.7.4
COMPETITION GRAPHS
There has been considerable attention paid lately to creating graphical models for bet-
ter understanding predator-prey relationships, especially to inform conservation policy
makers. This subsection introduces several undirected graphs and parameters that are
useful in understanding the competition of species.
Deﬁnitions:
Suppose a food web is represented by the directed acyclic graph D with n vertices and
m arcs.
The directed connectance (density) of the digraph D is deﬁned to be C = m
n2 .
The competition graph associated with D is an undirected graph G whose vertices are
the species in D. There is an edge in G between species a and species b if and only if a
and b have a common prey: i.e., there is some vertex x such that there exist arcs (x, a)
and (x, b) in D.
The competition number k(G) for a graph G is the fewest number of isolated vertices
that need to be added to G so that G is the competition graph for some directed acyclic
graph.
The common enemy graph associated with D is an undirected graph G whose vertices
are the species in D. There is an edge in G between species a and species b if and only if
a and b have a common predator: i.e., there is some vertex x such that there exist arcs
(a, x) and (b, x) in D.
A clique of the graph G is a subgraph of G in which every pair of distinct vertices is
connected by an edge in the subgraph. A maximal clique is a clique that cannot be
extended by including additional vertices.
Facts:
1. If all possible arcs exist in the digraph D, there would be n(n −1) arcs, so the
maximum connectance is 1 −1
n < 1; the minimum connectance is 0 if there are no arcs.
Since species at the top of the food web have no outgoing arcs and the food web has no
cycles, the maximum is much less than 1 −1
n.

Section 20.7
FOOD WEBS AND GRAPHS
1497
2. It was long believed that the higher the connectance the more stable the food web.
In fact, using real or simulated models and population dynamics on the food web and
competition graph, the steady state (stability) is achieved only for small n and C, par-
ticularly when the product nC = m
n < 2. These results are robust under the change of
initial conditions and ecological parameters [DuWiMa02].
3. Competition graphs are also known as niche overlap graphs and predator graphs.
4. Common enemy graphs are also known as prey graphs.
5. If D is a directed acyclic graph, then there must exist an isolated vertex in its corre-
sponding competition graph G. One such vertex would be a vertex having no incoming
arcs in D. (Every directed acyclic graph contains at least one vertex of indegree 0.)
6. Any graph G can be the competition graph for some directed acyclic graph D by
adding a suﬃcient number of isolated vertices to G.
7. If G has exactly one hole (induced cycle of length at least 4), then k(G) ≤2 [ChKi05].
8. k(G) ≤number of holes + 1 [MaScSc14].
9. Determining the competition number is an NP-hard problem [Op82].
Examples:
1. In the food web of §20.7.1, Example 1 sea urchins and small ﬁshes both have kelp as
a common prey, so (sea urchins, small ﬁshes) is an edge of the competition graph. This
simple competition graph, shown next, has one edge and four independent vertices.
Sea Urchins
Small Fishes
Sea Otters
Sharks
Large Crabs
Kelp
2. The following ﬁgure shows a food web D for the polar bear. Here we use the abbrevi-
ations PB (Polar Bear), AB (Arctic Birds), P (Phytoplankton), RS (Ringed Seal), ACd
(Arctic Cod), HZ (Herbivorous Zooplankton), KW (Killer Whale), HS (Harbour Seal),
ACh (Arctic Char), HpS (Harp Seal), CZ (Carnivorous Zooplankton), and C (Capelin).
Note that Phytoplankton (P) is the only basal species of this food web.
PB
AB
RS
HZ
P
KW
HS
HpS
CZ
C
ACd
ACh
The corresponding competition graph G is shown next. The competition graph G con-
tains the isolated vertices P, C, and HZ. Since there are arcs in D from ACd to PB, AB,
RS, and HpS, those four vertices form a clique, in fact a maximal clique, in G.

1498
Chapter 20
DISCRETE BIOMATHEMATICS
KW
HpS
HS
RS
AB
PB
CZ
ACd
ACh
HZ
C
P
3. The following graph G contains a single hole, the induced 4-cycle abcd. Therefore
by Fact 7 it is possible to add at most two isolated vertices to G so that the result is a
competition graph for some digraph D.
a
b
c
d
e
f
The digraph D shown next generates a competition graph that consists of G plus a single
isolated vertex x. Thus k(G) = 1.
a
d
c
b
e
f
x
20.7.5
INTERVAL GRAPHS AND BOXICITY
This subsection discusses interval graphs and related concepts that are useful in assessing
the dimension of community habitats.
Deﬁnitions:
A graph is an interval graph if we can ﬁnd a set of intervals on the real line so that
each vertex is assigned an interval and two vertices are joined by an edge if and only if
their corresponding intervals overlap.
The boxicity box(G) of graph G is the smallest positive integer p having the property
that we can assign a box in Euclidean p-space to each vertex of G so that two vertices
are connected by an edge of G if and only if their corresponding boxes overlap.
The clique matrix for a graph G is the binary matrix M = (mij), whose rows correspond
to the maximal cliques of G and whose columns correspond to the vertices of G; mij is
1 if and only if vertex j is in maximal clique i, and is 0 otherwise.
A binary matrix has the consecutive ones property for columns if its rows can be
permuted so that the 1s in each column occur consecutively.
The sink food web for a subset of species X in a food web is the induced subgraph
deﬁned by all species that are prey of species in X.
The source food web for a subset of species W in a food web is the induced subgraph
deﬁned by all species that are predators of species in W.

Section 20.7
FOOD WEBS AND GRAPHS
1499
Facts:
1. If G is the competition graph corresponding to a real community food web and G
is an interval graph, then the species in the food web have one-dimensional habitats or
niches. That is, each species can be mapped to the real line with overlapping intervals
if they have common prey, and this single dimension applies to each species in the web.
This single dimension might be determined by temperature, moisture, pH, or a number
of other factors.
2. The maximal cliques of an interval graph can be ordered in a sequence such that
for any vertex v, the maximal cliques containing v occur consecutively in the sequence
[FuGr65]. This is one way of determining if a graph is an interval graph—does such an
ordering of maximal cliques exist?
3. A graph is an interval graph if and only if its clique matrix M has the consecutive
ones property for columns [FuGr65].
4. A graph is an interval graph if and only if it does not contain one of the following
“forbidden” structures as a subgraph: the cycle C4, the complete bipartite graph K3,3,
or the graph H having a pendant edge attached to each vertex of the complete graph K3
[GiHo64].
5. A graph is an interval graph if and only if it is a chordal graph (contains no holes)
and its complement is a comparability graph (has a transitive orientation) [LeBo62].
6. Interval graphs have boxicity 1.
7. If the boxicity of the competition graph is 2, then the species in the associated food
web have 2-dimensional habitats or niches.
8. Every graph can be represented as the intersection graph of boxes in some dimension,
so boxicity is well deﬁned in general. However, it is hard to compute [Co81].
9. There are fast algorithms to test if a graph is an interval graph. However, there are
no fast ways known for computing the boxicity of a general graph [Co81], [CoRo83].
10. The parameter box(G) is bounded above by the minimum size of a maximal match-
ing (§10.2.1) in the complement of G. When G is dense and its complement is sparse,
the boxicity tends to be small [Ro69].
11. In the 1960s, Joel Cohen found that food webs arising from “single habitat ecosys-
tems” (homogeneous ecosystems) generally have competition graphs that are interval
graphs. This remarkable empirical observation of Cohen, that real-world competition
graphs are usually interval graphs, has led to a great deal of research on the structure
of competition graphs and on the relation between the structure of digraphs and their
corresponding competition graphs. It has also led to a great deal of research in ecology
to determine just why this might be the case [Co78].
12. Using randomly generated food webs (digraphs), Cohen et al. showed that the prob-
ability that a competition graph is an interval graph goes to 0 as the number of species
increases. In other words, it should be highly unlikely that competition graphs corre-
sponding to food webs are interval graphs [CoKoMu79].
13. Cohen showed that a food web has a competition graph that is an interval graph if
and only if each sink food web contained in it is an interval graph. However, the same is
not true for source food webs [Co78].
14. A food web can have a competition graph that is an interval graph while some source
food web contained in it has a competition graph that is not an interval graph. The
following site provides an example of a food web whose competition graph is an interval
graph, together with a subset W of vertices whose source food web has a competition
graph of boxicity 2.

1500
Chapter 20
DISCRETE BIOMATHEMATICS
• http://dimacs.rutgers.edu/IMB/TalksEtc/Foodwebs-and-Biodiversity-
7-29-13.pdf
Examples:
1. The competition graph G shown next is an interval graph, since we can associate
intervals on the real line with its vertices so that the intervals corresponding to two
vertices overlap if and only if there is an edge between the two vertices in G. One such
representation is shown. Notice that the maximal cliques of G are cd, bde, ef. In this
ordering, the maximal cliques containing each vertex occur consecutively (Fact 2).
b
c
d
e
f
g
b
c
d
e
f
g
Graph G
Using the above ordering of cliques and the alphabetical ordering of the vertices produces
the following clique matrix, which has the consecutive ones property for columns (Fact 3):
M =



0
1
1
0
0
0
1
0
1
1
0
0
0
0
0
1
1
0


.
2. The competition graph G shown next is not an interval graph. An attempt to add an
interval for vertex a to the interval representation given in Example 1 is not successful.
There is no place for such an interval that intersects only the interval for b and no others.
Since G contains the forbidden subgraph H from Fact 4, we are assured that G is not an
interval graph. In fact, G has boxicity 2.
b
c
d
e
f
g
b
c
d
e
f
g
Graph G
a
a
?
3. Fact 4 guarantees that the cycle C4 is not an interval graph. A representation using
2-dimensional boxes is shown below, so box(C4) = 2.
a
c
b
d
C4
a
b
d
c

Section 20.7
FOOD WEBS AND GRAPHS
1501
20.7.6
PROJECTION GRAPHS
Projection graphs are weighted graphs based on competition graphs and common enemy
graphs.
Deﬁnitions:
Suppose D is a food web with n vertices. Let T denote the set of top species (no outgoing
arcs), B the set of basal species (no incoming arcs), and I the remaining (intermediate)
species. Let xij be a 0-1 variable, with xij = 1 if arc (i, j) is in D and xij = 0 otherwise.
The predator projection graph (PP) associated with D has a vertex for each species
in D and contains the edge (i, j) if i and j have a common prey. The weight A(i, j) of
edge (i, j) reﬂects the number of common prey:
A(i, j) =
1
n(|B| + |I|)
X
{xkixkj | k ∈B ∪I}.
The prey projection graph (EP) associated with D has a vertex for each species in
D and contains the edge (i, j) if i and j have a common predator. The weight B(i, j) of
edge (i, j) reﬂects the number of common predators:
B(i, j) =
1
n(|T | + |I|)
X
{xikxjk | k ∈T ∪I}.
Facts:
1. The PP graph is a weighted competition graph minus the basal species, since they
have no prey. We can use the competition graph to construct the PP graph, by removing
the basal species and then labelling edge (i, j) with weight A(i, j).
2. The EP graph is a weighted common enemy graph minus the top species, since they
have no predators. We can use the common enemy graph to construct the EP graph, by
removing the top species and then labelling edge (i, j) with weight B(i, j).
3. The matrix of edge weights in the PP graph is given by A = αXT X, where X = (xij)
and α = (n(n −|T |))−1.
4. The matrix of edge weights in the EP graph is given by B = βXXT, where X = (xij)
and β = (n(n −|B|))−1.
Examples:
1. For the polar bear food web (§20.7.4, Example 2), Phytoplankton (P) is the only
basal species so P does not appear in the corresponding PP graph, shown next.
KW
HpS
HS
RS
AB
PB
CZ
ACd
ACh
HZ
C
The matrix of edge weights in the PP graph, normalized by the divisor 96 = 12 × 8, is
given by

1502
Chapter 20
DISCRETE BIOMATHEMATICS
PB
AB
RS
ACd
KW
HS
HpS
HZ
CZ
ACh
C
PB
0
.0104
.0104
0
.0208
0
.0104
0
0
0
0
AB
.0104
0
.0104
0
0
0
.0104
0
0
0
0
RS
.0104
.0104
0
0
0
0
.0104
0
0
0
0
ACd
0
0
0
0
0
0
0
0
.0104
.0104
0
KW
.0208
0
0
0
0
0
0
0
0
0
0
HS
0
0
0
0
0
0
.0104
0
0
0
0
HpS
.0104
.0104
.0104
0
0
.0104
0
0
0
0
0
HZ
0
0
0
0
0
0
0
0
0
0
0
CZ
0
0
0
.0104
0
0
0
0
0
0
0
ACh
0
0
0
.0104
0
0
0
0
0
0
0
C
0
0
0
0
0
0
0
0
0
0
0
2. For the polar bear food web (§20.7.4, Example 2), the top species are PB, AB, KW,
and HpS; they do not appear in the corresponding EP graph, shown next.
ACd
RS
HS
CZ
HZ
ACh
P
C
The matrix of edge weights in the EP graph, normalized by the divisor 132 = 12 × 11, is
given by
RS
ACd
HS
P
HZ
CZ
ACh
C
RS
0
.00758
.01515
0
0
0
0
0
ACd
.00758
0
.00758
0
0
0
0
.00758
HS
.01515
.00758
0
0
0
0
0
0
P
0
0
0
0
0
0
0
0
HZ
0
0
0
0
0
.00758
0
0
CZ
0
0
0
0
.00758
0
0
0
ACh
0
0
0
0
0
0
0
0
C
0
.00758
0
0
0
0
0
0
20.7.7
OPEN QUESTIONS
There remain a number of open questions that suggest further study of graphs and that
may inform conservationists and policy makers. Several are mentioned here.
1. Can we characterize the directed graphs whose corresponding competition graphs are
interval graphs? This is a fundamental open question in applied graph theory. Indeed
there is no forbidden list of digraphs (ﬁnite or inﬁnite) such that when these digraphs
are excluded, one automatically has a competition graph that is an interval graph.

REFERENCES
1503
2. What are the ecological characteristics of food webs that seem to lead to their com-
petition graphs being interval graphs? Most directed graphs do not have interval graph
competition graphs, yet statistically most actual food webs do have interval competition
graphs. (This is an important unsolved problem described by Cohen, with no answers
to date.)
3. What is the relationship between the boxicity of the competition graph of a food web
and the boxicities of the competition graphs of its source food webs (note for every subset
W there is a source food web)? These can indeed be diﬀerent; see §20.7.5, Fact 14.
4. It has been shown recently that it is possible to determine if a graph has boxicity
2, yet it is diﬃcult to determine if a graph has boxicity k for k > 2 (this is an NP-
complete problem). There are no nice characterizations known for graphs with boxicity
2, in contrast to the case for interval graphs. Find a forbidden subgraph characterization
for graphs of boxicity 2.
5. Is there any relationship between the indegrees and outdegrees of vertices in a food
web, or the maximum indegree or outdegree over all vertices, and the competition number
of the graph? If you limit the indegree or outdegree (or both) of all vertices in a digraph,
is it more likely to have a competition graph that is an interval graph?
6. Is there any relationship between the connectance of a food web and the corresponding
competition graph?
7. Are there mathematical results obtainable from using the PP and EP graphs that
are not achievable from simply considering the competition graph, or its counterpart the
common enemy graph? Do these have ecological consequences?
REFERENCES
Printed Resources:
[AaNa10] D. P. Aalberts and N. Nandagopal, “A two-length-scale polymer theory for
RNA loop free energies and helix stacking”, RNA 16 (2010), 1350–1355.
[Ak00] T. Akutsu, “Dynamic programming algorithms for RNA secondary structure pre-
diction with pseudoknots”, Discrete Applied Mathematics 104 (2000), 45–62.
[AlOt03] R. Albert and H. Othmer, “The topology of the regulatory interactions predicts
the expression pattern of the segment polarity genes in Drosophila melanogaster”,
Journal of Theoretical Biology 223 (2003), 1–18.
[Al91] S. F. Altschul, “Amino acid substitution matrices from an information theoretic
perspective”, Journal of Molecular Biology 219 (1991), 555–565.
[AlEr86a] S. F. Altschul and B. W. Erickson, “Optimal sequence alignment using aﬃne
gap costs”, Bulletin of Mathematical Biology 48 (1986), 603–616.
[AlEr86b] S. F. Altschul and B. W. Erickson, “Locally optimal subalignments using
nonlinear similarity functions”, Bulletin of Mathematical Biology 48 (1986), 633–
660.
[AlEtal90] S. F. Altschul, W. Gish, W. Miller, E. W. Myers, and D. J. Lipman, “Basic
local alignment search tool”, Journal of Molecular Biology 215 (1990), 403–410.
[AlEtal97] S. F. Altschul, T. L. Madden, A. A. Sch¨aﬀer, J. Zhang, Z. Zhang, W. Miller,
and D. J. Lipman, “Gapped BLAST and PSI-BLAST: a new generation of protein
database search programs”, Nucleic Acids Research 25 (1997), 3389–3402.

1504
Chapter 20
DISCRETE BIOMATHEMATICS
[AmClStJ03] N. Amenta, F. Clarke, and K. St. John, “A linear-time majority tree algo-
rithm”, Algorithms in Bioinformatics 2812 (2003), 216–227.
[AnEtal13] J. E. Andersen, R. C. Penner, C. M. Reidys, and M. S. Waterman, “Topolog-
ical classiﬁcation and enumeration of RNA structures by genus”, Journal of Mathe-
matical Biology 67 (2013), 1261–1278.
[ArKl06] F. Ardila and C. Klivans, “The Bergman complex of a matroid and phylogenetic
trees”, Journal of Combinatorial Theory B 96 (2006), 38–49.
[Ba79] J. K. Baker, “Trainable grammars for speech recognition”, Proceeding of the
Spring Conference of the Acoustical Society of America, Boston, 1979, 547–550.
[BaEtal08] N. Bandeira, V. Pham, P. Pevzner, D. Arnott, and J. R. Lill, “Automated de
novo protein sequencing of monoclonal antibodies”, Nature Biotechnology 26 (2008),
1336–1338.
[BeEtal08] D. R. Bentley et al., “Accurate whole human genome sequencing using re-
versible terminator chemistry”, Nature 456 (2008), 53–59.
[BeBaSo95] R. Ben-Yishai, R. L. Bar-Or, and H. Sompolinsky, “Theory of orientation
tuning in visual cortex”, Proceedings of the National Academy of Sciences USA 92
(1995), 3844–3848.
[BoEtal08] M. Bon, G. Vernizzi, H. Orland, and A. Zee, “Topological classiﬁcation of
RNA structures”, Journal of Molecular Biology 379 (2008), 900–911.
[BrMcKSt03] D. Bryant, A. McKenzie, and M. Steel, “The size of a maximum agree-
ment subtree for random binary trees”, DIMACS Series in Discrete Mathematics
and Theoretical Computer Science 61 (2003), 55–66.
[BrSt09] D. Bryant and M. Steel, “Computing the distribution of a tree metric”, IEEE/
ACM Transactions on Computational Biology and Bioinformatics 6 (2009), 420–426.
[Bu71] A. W. Burks, Essays on Cellular Automata, University of Illinois Press, 1971.
[BuWh94] M. Burrows and D. Wheeler, “A block sorting lossless data compression algo-
rithm”, Technical Report 124, Digital Equipment Corporation, 1994.
[CaLi14] C. F. G. Castillo and M. H. T. Ling, “Digital Organism Simulation Environment
(DOSE): a library for ecologically-based in silico experimental evolution”, ACSIJ
Advances in Computer Science: An International Journal 3 (2014), 44–50.
[ChFrSi10] L. S. Chandran, M. C. Francis, and N. Sivadasan, “Geometric representation
of graphs in low dimension using axis parallel boxes”, Algorithmica 56 (2010), 129–
140.
[ChEtal07] W. Y. C. Chen, E. Y. P. Deng, R. R. X. Du, R. P. Stanley, and C. H. Yan,
“Crossing and nesting of matchings and partitions”, Transactions of the American
Mathematical Society 359 (2007), 1555–1575.
[ChPeMi92] K-M. Chao, W. R. Pearson, and W. Miller, “Aligning two sequences within
a speciﬁed diagonal band”, Computer Applications in the Biosciences 8 (1992), 481–
487.
[ChKi05] H. H. Cho and S. R. Kim, “A class of acyclic digraphs with interval competition
graphs”, Discrete Applied Mathematics 148 (2005), 171–180.
[Co78] J. E. Cohen, Food Webs and Niche Space, Princeton University Press, 1978.
[CoKoMu79] J. E. Cohen, J. Koml´os, and M. Mueller, “The probability of an interval
graph, and why it matters”, Proceeding of Symposia in Pure Mathematics 34 (1979),
97–115.

REFERENCES
1505
[CoEtal00] R. Cole, M. Farach-Colton, R. Hariharan, T. Przytycka, and M. Thorup, “An
O(n log n) algorithm for the maximum agreement subtree problem for binary trees”,
SIAM Journal on Computing 30 (2000), 1385–1404.
[CoJaLa06] O. Colon-Reyes, A. Jarrah, R. Laubenbacher, and B. Sturmfels, “Monomial
dynamical systems over ﬁnite ﬁelds”, Complex Systems 4 (2006), 333–342.
[CoPeTe11] P. E. C. Compeau, P. A. Pevzner, and G. Tesler, “How to apply de Bruijn
graphs to genome assembly”, Nature Biotechnology 29 (2011) 987–991.
[Co81] M. B. Cozzens, “Higher and multi-dimensional analogues of interval graphs”,
Ph.D. thesis, Department of Mathematics, Rutgers University, 1981.
[Co11] M. B. Cozzens, “Food webs, competition graphs, and habitat formation”, Math-
ematical Modeling of Natural Phenomena 6 (2011), 22–38.
[Co15] M. B. Cozzens, “Food webs and graphs” in Algebraic and Discrete Mathematical
Methods for Modern Biology, R. Robeva (ed.), Elsevier/Academic Press, 2015, 29–
49.
[CoRo83] M. B. Cozzens and F. S. Roberts,“Computing the boxicity of a graph by cov-
ering its complement by cointerval graphs”, Discrete Applied Mathematics 6 (1983),
217–228.
[CuEtal15] C. Curto, E. Gross, J. Jeﬀries, K. Morrison, M. Omar, Z. Rosen, A. Shiu, and
N. Youngs, “What makes a neural code convex?”, preprint, Nov. 2015; available at
http://arxiv.org/abs/1508.00150.
[CuIt08] C. Curto and V. Itskov, “Cell groups reveal structure of stimulus space”, PLoS
Computational Biology 4 (2008), e1000205.
[CuEtal13a] C. Curto, V. Itskov, K. Morrison, Z. Roth, and J. L. Walker, “Combinatorial
neural codes from a mathematical coding theory perspective”, Neural Computation
25 (2013), 1891–1925.
[CuEtal13b] C. Curto, V. Itskov, A. Veliz-Cuba, and N. Youngs, “The neural ring: an
algebraic tool for analyzing the intrinsic structure of neural codes, Bulletin of Math-
ematical Biology 75 (2013), 1571–1611.
[CuYo15] C. Curto and N. Youngs, “Neural ring homomorphisms and maps between neu-
ral codes”, preprint, Nov. 2015; available at http://arxiv.org/abs/1511.00255.
[Da85] W. Day, “Optimal algorithms for comparing trees with labeled leaves”, Journal
of Classiﬁcation 2 (1985), 7–28.
[DaScOr78] M. O. Dayhoﬀ, R. M. Schwartz, and B. C. Orcutt, “A model of evolutionary
change in proteins”, in Atlas of Protein Sequence and Structure, Volume 5, Supple-
ment 3, M. O. Dayhoﬀ(ed.), National Biomedical Research Foundation, Washington,
DC, 1978, pp. 345–352.
[DeRo06] J. Degnan and N. Rosenberg, “Discordance of species trees with their most
likely gene trees”, PLoS Genetics 2 (2006), e68.
[DeDo05] A. Deutsch and S. Dormann, Cellular Automaton Modelling of Biological Pat-
tern Formation, Birkhauser, 2005.
[DoEd04] R. D. Dowell and S. R. Eddy, “Evaluation of several lightweight stochastic
context-free grammars for RNA secondary structure prediction”, BMC Bioinformat-
ics 5:71 (2004).
[DiTuMa01] J. M. Diamond, D. H. Turner, and D. H. Mathews, “Thermodynamics of
three-way multibranch loops in RNA”, Biochemistry 40 (2001), 6971–6981.

1506
Chapter 20
DISCRETE BIOMATHEMATICS
[DrEtal11] A. Dress, K. Huber, J. Koolen, V. Moulton, and A. Spillner, Basic Phyloge-
netic Combinatorics, Cambridge University Press, 2011.
[DrEtal89] R. Drmanac, I. Labat, I. Brukner, and R. Crkvenjakov, “Sequencing of mega-
base plus DNA by hybridization: theory of the method”, Genomics 4 (1989), 114–128.
[DrEtal93] R. Drmanac et al., “DNA sequence determination by hybridization: a strategy
for eﬃcient large-scale sequencing”, Science 260 (1993), 1649–1652.
[DuWiMa02] J. A. Dunne, R. J. Williams, and N. D. Martinez, “Network structure
and biodiversity loss in food webs: robustness increases with connectance”, Ecology
Letters 5 (2002), 558–567.
[DuEtal98] R. Durbin, S. R. Eddy, A. Krogh, and G. Mitchison, Biological Sequence
Analysis: Probabilistic Models of Proteins and Nucleic Acids, Cambridge University
Press, 1998.
[EiSm03] A. E. Eiben and J. E. Smith, Introduction to Evolutionary Computing, Springer,
Natural Computing Series, 2003.
[Fe04] J. Felsenstein, Inferring Phylogenies, Sinauer Associates, 2004.
[FeMa00] P. Ferragina and G. Manzini, “Opportunistic data structures with applica-
tions”, Proceedings of the 41st Annual Symposium on Foundations of Computer
Science, 2000, 390–398.
[Fi84] J. W. Fickett, “Fast optimal alignment”, Nucleic Acids Research 12 (1984), 175–
180.
[FlEtal95] R. D. Fleischmann et al., “Whole-genome random sequencing and assembly
of Haemophilus Inﬂuenzae Rd”, Science 269 (1995), 496–512.
[Fo94] D. B. Fogel, “An introduction to simulated evolutionary optimization”, IEEE
Transactions on Neural Networks: Special Issue on Evolutionary Computation 5
(1994), 3–14.
[FuGr65] D. R. Fulkerson and O. A. Gross, “Incidence matrices and interval graphs”,
Paciﬁc Journal of Mathematics 15 (1965), 835–855.
[Ga74] H. Gabai, “Bounds for the boxicity of a graph”, York College, City University of
New York, 1974.
[Ga70] M. Gardner, “The fantastic combinations of John Conway’s new solitaire game
‘life’ ”, Scientiﬁc American 223 (1970), 120–123.
[GhPo09] M. Ghodsi and M. Pop, “Inexact local alignment search over suﬃx arrays”, Pro-
ceedings of the IEEE International Conference on Bioinformatics and Biomedicine
(BIBM ’09), 2009, 83–87.
[GiHo64] P. C. Gilmore and A. J. Hoﬀman, “A characterization of comparability graphs
and of interval graphs”, Canadian Journal of Mathematics 16 (1964), 539–548.
[GiIt14] C. Giusti and V. Itskov, “A no-go theorem for one-layer feedforward networks”,
Neural Computation 26 (2014), 2527–2540.
[Go89] D. E. Goldberg, Genetic Algorithms in Search, Optimization, and Machine Learn-
ing, Addison-Wesley, 1989.
[GoFoStJ13] K. Gordan, E. Ford, and K. St. John, “Hamiltonian walks of phylogenetic
treespaces”, IEEE/ACM Transactions on Computational Biology and Bioinformatics
10 (2013), 1076–1079.

REFERENCES
1507
[Go82] O. Gotoh, “An improved algorithm for matching biological sequences”, Journal
of Molecular Biology 162 (1982), 705–708.
[GrEtal11] M. G. Grabherr et al., “Full-length transcriptome assembly from RNA-Seq
data without a reference genome”, Nature Biotechnology 29 (2011), 644–652.
[Gr16] T. R. Gregory, “Animal genome size database”, http://www.genomesize.com,
2016.
[GrMcEi87] M. Gribskov, A. D. McLachlin, and D. Eisenberg, “Proﬁle analysis: detection
of distantly related proteins”, Proceedings of the National Academy of Sciences USA
84 (1987), 4355–4358.
[HaBo97] I. Harvey and T. Bossomaier, “Time out of joint: attractors in asynchronous
random Boolean networks”, in Proceedings of the Fourth European Conference on
Artiﬁcial Life (ECAL97), P. Husbands and I. Harvey (eds.), MIT Press, 1997, 67–75.
[HeMaDa15] Q. He, N. Macauley, and R. Davies, “RNA secondary structures: combinato-
rial models and folding algorithms” in Algebraic and Discrete Mathematical Methods
for Modern Biology, R. Robeva (ed.), Elsevier/Academic Press, 2015, 321–345.
[HePo14] C. Heitsch and S. Poznanovi´c, “Asymptotic distribution of motifs in a stochastic
context-free grammar model of RNA folding”, Journal of Mathematical Biology 69
(2014), 1743–1772.
[HeHe92] S. Henikoﬀand J. G. Henikoﬀ, “Amino acid substitution matrices from protein
blocks”, Proceedings of the National Academy of Sciences USA 89 (1992), 10915–
10919.
[HoEtal94] I. L. Hofacker, W. Fontana, P. F. Stadler, L. S. Bonhoeﬀer, M. Tacker, and P.
Schuster, “Fast folding and comparison of RNA secondary structures”, Monatshefte
f¨ur Chemie 125 (1994), 167–188.
[Ho75] J. H. Holland, Adaptation in Natural and Artiﬁcial Systems, University of Michi-
gan Press, 1975.
[Hu99] S. Huang, “Gene expression proﬁling, genetic networks, and cellular state: an inte-
grating concept for tumorigenesis and drug discovery”, Journal of Molecular Medicine
77 (1999), 469–480.
[HuWa03] X. Huang and J. Wang, “PCAP: a whole-genome assembly program”, Genome
Research 13 (2003), 2164–2170.
[HuRuSc10] D. Huson, R. Rupp, and C. Scornavacca, Phylogenetic Networks: Concepts,
Algorithms and Applications, Cambridge University Press, 2010.
[IdWa95] R. M. Idury and W. S. Waterman, “A new algorithm for DNA sequence as-
sembly”, Journal of Computational Biology 2 (1995), 291–306.
[IlIl07] L. Ilie and S. Ilie, “Multiple spaced seeds for homology search”, Bioinformatics
23 (2007), 2969–2977.
[ItSa14] K. Ito and N. Sakuraba, “Importance of real-world properties in chasing task:
simulation and analysis of dragonﬂy’s behavior”, Artiﬁcial Life and Robotics 19
(2014), 370–374.
[JuCa69] T. Jukes and C. Cantor, “Evolution of protein molecules”, Mammalian Protein
Metabolism 3 (1969), 21–132.
[Ka84] G. Kalai, “Characterization of f-vectors of families of convex sets in Rd. I. Ne-
cessity of Eckhoﬀ’s conditions”, Israel Journal of Mathematics 48 (1984), 175–195.

1508
Chapter 20
DISCRETE BIOMATHEMATICS
[Ka86] G. Kalai, Characterization of f-vectors of families of convex sets in Rd. II. Suf-
ﬁciency of Eckhoﬀ’s conditions”, Journal of Combinatorial Theory A 41 (1986),
167–188.
[K¨aNa07] J. K¨arkk¨ainen and J. C. Na, “Faster ﬁlters for approximate string match-
ing”, Proceedings of the 9th Workshop on Algorithm Engineering and Experiments
(ALENEX’ 07), 2007, 84–90.
[KaAl90] S. Karlin and S. F. Altschul, “Methods for assessing the statistical signiﬁcance
of molecular sequence features by using general scoring schemes”, Proceedings of the
National Academy of Sciences USA 87 (1990), 2264–2268.
[Ka69] S. A. Kauﬀman, “Metabolic stability and epigenesis in randomly constructed
genetic nets”, Journal of Theoretical Biology 22 (1969), 437–467.
[Ka93] S. A. Kauﬀman, The Origins of Order, Oxford University Press, 1993.
[Ka00] S. A. Kauﬀman, Investigations, Oxford University Press, 2000.
[Ke09] E. F. Keller, Making Sense of Life: Explaining Biological Development with Mod-
els, Metaphors, and Machines, Harvard University Press, 2009.
[KnHe99] B. Knudsen and J. Hein, “RNA secondary structure prediction using stochastic
context-free grammars and evolutionary history”, Bioinformatics 15 (1999), 446–454.
[KnHe03] B. Knudsen and J. Hein, “Pfold: RNA secondary structure prediction using
stochastic context-free grammars”, Nucleic Acids Research 31 (2003), 3423–3428.
[KnMoPr77] D. Knuth, J. H. Morris, and V. Pratt, “Fast pattern matching in strings”,
SIAM Journal on Computing 6 (1977), 323–350.
[Ko91] J. R. Koza, “Evolution and co-evolution of computer programs to control indepen-
dently-acting agents”, in Proceedings of the First International Conference on Sim-
ulation of Adaptive Behavior, MIT Press, 1991, 366–375.
[LaSt13] A. Lambert and T. Stadler, “Birth–death models and coalescent point processes:
the shape and probability of reconstructed phylogenies”, Theoretical Population Bi-
ology 90 (2013), 113–128.
[La89] C. G. Langton, ed., Artiﬁcial Life, Volume 6 of Santa Fe Institute Studies in the
Sciences of Complexity, Addison-Wesley, 1989.
[LaYo90] K. Lari and S. J. Young, “The estimation of stochastic context-free grammars
using the inside-outside algorithm”, Computer Speech and Language 4 (1990), 35–56.
[LaHiOr15] R. Laubenbacher, F. Hinkelmann, and M. Oremland, “Agent-based models
and optimal control in biology: a discrete approach” in Algebraic and Discrete Math-
ematical Methods for Modern Biology, R. Robeva (ed.), Elsevier/Academic Press,
2015, 143–178.
[LaSt04] R. Laubenbacher and B. Stigler, “A computational algebra approach to the
reverse engineering of gene regulatory networks”, Journal of Theoretical Biology 229
(2004), 523–537.
[LeBo62] C. B. Lekkerkerker and J. C. Boland, “Representation of a ﬁnite graph by a set
of intervals on the real line”, Fundamenta Mathematicae 51 (1962), 45–64.
[LiTrZh96] M. Li, J. Tromp, and L. Zhang, “On the nearest neighbor interchange distance
between evolutionary trees”, Journal of Theoretical Biology 182 (1996), 463–467.
[Lu12] A. C. J. Luo, Regularity and Complexity in Dynamical Systems, Springer, 2012.

REFERENCES
1509
[LyPe00] R. B. Lyngs¨o and C. N. S. Pedersen, “RNA pseudoknot prediction in energy-
based models”, Journal of Computational Biology 7 (2000), 409–427.
[MaLi07] B. Ma and M. Li, “On the complexity of the spaced seeds”, Journal of Computer
and System Sciences 73 (2007), 1024–1034.
[MaTrLi02] B. Ma, J. Tromp, and M. Li, “PatternHunter: faster and more sensitive
homology search”, Bioinformatics 18 (2002), 440–445.
[Ma55] R. H. MacArthur, “Fluctuations of animal populations and a measure of commu-
nity stability”, Ecology 36 (1955), 533–536.
[MaScSc14] B. MacKay, P. Schweitzer, and P. Schweitzer, “Competition numbers, quasi-
line graphs, and holes”, SIAM Journal on Discrete Mathematics 28 (2014), 77–91.
[MaDe06] D. G. Malleta and L. G. De Pillis, “A cellular automata model of tumor-
immune system interactions”, Journal of Theoretical Biology 239 (2006), 334–350.
[MaMy93] U. Manber and G. Myers, “Suﬃx arrays: a new method for on-line string
searches”, SIAM Journal on Computing 22 (1993), 935–948.
[Ma15] Y. I. Manin, “Neural codes and homotopy types: mathematical models of place
ﬁeld recognition”, Moscow Mathematical Journal 15 (2015), 741–748.
[MaEtal05] M. Margulies et al., “Genome sequencing in microfabricated high-density
picolitre reactors”, Nature 437 (2005), 376–380.
[MaZu08] N. R. Markham and M. Zuker, “Unafold: software for nucleic acid folding and
hybridization”, in Bioinformatics, Volume II. Structure, Function and Applications,
J. M. Keith (ed.), Springer, 2008, 3–31.
[Ma09] R. May, “Food web assembly and collapse: mathematical models and implications
for conservation”, Transactions of the Royal Society B: Biological Sciences 364 (2009),
1643–1646.
[Mc76] E. M. McCreight, “A space-economical suﬃx tree construction algorithm”, Jour-
nal of the ACM 23 (1976), 262–272.
[McNEtal06] B. L. McNaughton, F. P. Battaglia, O. Jensen, E. I. Moser, and M. B. Moser,
“Path integration and the neural basis of the ‘cognitive map’ ”, Nature Reviews
Neuroscience 7 (2006), 663–678.
[MeEtal11] P. Medvedev, S. Pham, M. Chaisson, G. Tesler, and P. Pevzner, “Paired de
Bruijn graphs: a novel approach for incorporating mate pair information into genome
assemblers”, Journal of Computational Biology 18 (2011), 1625–1634.
[Me09] J. Memmott, “Food webs: a ladder for picking strawberries or a practical tool for
practical problems?”, Transactions of the Royal Society B: Biological Sciences 364
(2009), 1693–1699.
[MiMy88] W. Miller and E. W. Myers, “Sequence comparison with concave weighting
functions”, Bulletin of Mathematical Biology 50 (1988), 97–120.
[MiSt05] E. Miller and B. Sturmfels, Combinatorial Commutative Algebra, Springer,
2005.
[MoRe07] H. S. Mortveit and C. M. Reidys, An Introduction to Sequential Dynamical
Systems, Springer-Verlag, 2007.
[My95] E. W. Myers, “Toward simplifying and accurately formulating fragment assem-
bly”, Journal of Computational Biology 2 (1995), 275–290.

1510
Chapter 20
DISCRETE BIOMATHEMATICS
[My05] E. W. Myers, “The fragment assembly string graph”, Bioinformatics 21 Suppl. 2
(2005), 79–85.
[MyEtal00] E. W. Myers et al., “A whole-genome assembly of Drosophila”, Science 287
(2000), 2196–2204.
[MyMi88] E. W. Myers and W. Miller, “Optimal alignments in linear space”, CABIOS
4 (1988), 11–17.
[NaEtal06] A. Nakabachi, A. Yamashita, H. Toh, H. Ishikawa, H. E. Dunbar, N. A.
Moran, and M. Hattori, “The 160-kilobase genome of the bacterial endosymbiont
Carsonella”, Science 314 (2006), 267.
[NeWu70] S. B. Needleman and C. D. Wunsch, “A general method applicable to the search
for similarities in the amino acid sequences of two proteins”, Journal of Molecular
Biology 48 (1970), 443–453.
[NuEtal78] R. Nussinov, G. Pieczenik, J. R. Griggs, and D. J. Kleitman, “Algorithms for
loop matchings”, SIAM Journal on Applied Mathematics 35 (1978), 68–82.
[OKDo71] J. O’Keefe and J. Dostrovsky, “The hippocampus as a spatial map. Prelimi-
nary evidence from unit activity in the freely-moving rat”, Brain Research 34 (1971),
171–175.
[Op82] R. Opsut, “On the competition number of a graph”, SIAM Journal on Algebraic
and Discrete Methods 3 (1982), 470–478.
[PaSt05] L. Pachter and B. Sturmfels, Algebraic Statistics for Computational Biology,
Cambridge University Press, 2005.
[PaEtal11] G. M. Palamara, V. Zlati´c, A. Scala, and G. Caldarelli, “Population dynamics
on complex food webs”, Advances in Complex Systems 16 (2011), 635–647.
[PaEtal12] A. Palyanov, S. Khayrulin, S. D. Larson, and A. Dibert, “Towards a virtual C.
elegans: a framework for simulation and visualization of the neuromuscular system
in a 3D physical environment”, In Silico Biology 11 (2012), 137–147.
[PeLi88] W. R. Pearson and D. J. Lipman, “Improved tools for biological sequence com-
parison”, Proceedings of the National Academy of Sciences USA 85 (1988), 2444–
2448.
[PeEtal83] H. Peltola, H. S¨oderlund, J. Tarhio, and E. Ukkonen, “Algorithms for some
string matching problems arising in molecular genetics”, Proceedings of the 9th IFIP
World Computer Congress, Elsevier, 1983, 59–64.
[PeS¨oUk84] H. Peltola, H. S¨oderlund, and E. Ukkonen, “SEQUAID: a DNA sequence as-
sembly program based on a mathematical model”, Nucleic Acids Research 12 (1984),
307–321.
[PeEtal10] R. C. Penner, M. Knudsen, C. Wiuf, and J. E. Andersen, “Fatgraph models of
proteins”, Communications on Pure and Applied Mathematics 63 (2010), 1249–1297.
[Pe89] P. Pevzner, “L-tuple DNA sequencing: computer analysis”, Journal of Biomolec-
ular Structure and Dynamics 7 (1989), 63–73.
[PeTaWa01] P. A. Pevzner, H. Tang, and M. S. Waterman, “An Eulerian path approach
to DNA fragment assembly”, Proceedings of the National Academy of Sciences USA
14 (2001), 9748–9753.
[PhEtal13] S. Pham, D. Antipov, A. Sirotkin, G. Tesler, P. Pevzner, and M. Alekseyev,
“Pathset graphs: a novel approach for comprehensive utilization of paired reads in
genome assembly”, Journal of Computational Biology 20 (2013), 359–371.

REFERENCES
1511
[PhPe10] S. K. Pham and P. A. Pevzner, “DRIMM-Synteny: decomposing genomes into
evolutionary conserved segments”, Bioinformatics 26 (2010), 2509–2516.
[PoKo04] M. Pop and D. Kosack, “Using the TIGR assembler in shotgun sequencing
projects”, Methods in Molecular Biology 255 (2004), 279–294.
[Re10] C. M. Reidys, Combinatorial Computational Biology of RNA: Pseudoknots and
Neutral Networks, Springer, 2010.
[ReEtal11] C. M. Reidys, F. W. D. Huang, J. E. Andersen, R. C. Penner, P. F. Stadler,
and M. E. Nebel, “Topology and prediction of RNA pseudoknots”, Bioinformatics
27 (2011), 1076–1085.
[RiEtal82] K. Rietveld, R. Van Poelgeest, C. W. Pleij, J. H. Van Boom, and L. Bosch,
“The tRNA-like structure at the 3′ terminus of turnip yellow mosaic virus RNA.
Diﬀerences and similarities with canonical tRNA”, Nucleic Acids Research 10 (1982),
1929–1946.
[RiEd99] E. Rivas and S. R. Eddy, “A dynamic programming algorithm for RNA struc-
ture prediction including pseudoknots”, Journal of Molecular Biology 285 (1999),
2053–2068.
[Ro69] F. S. Roberts, “On the boxicity and cubicity of a graph” in Recent Progress in
Combinatorics, W. T. Tutte (ed.), Academic Press, 1969, 301–310.
[Ro78] F. S. Roberts, “Food webs, competition graphs, and the boxicity of ecological
phase space” in Theory and Applications of Graphs, Y. Alavi and D. Lick (eds.),
Springer-Verlag, 1978, 477–490.
[RoCaSc13] R. J. Roberts, M. O. Carneiro, and M. C. Schatz, “The advantages of SMRT
sequencing”, Genome Biology 14 (2013), 405.
[RoHe14] E. Rogers and C. E. Heitsch, Proﬁling small RNA reveals multimodal substruc-
tural signals in a Boltzmann ensemble”, Nucleic Acids Research 42 (2014), e171.
[SaNe87] N. Saitou and M. Nei, “The neighbor-joining method: a new method for recon-
structing phylogenetic trees”, Molecular Biology and Evolution 4 (1987), 406–425.
[SaEtal94] Y. Sakakibara, M. Brown, R. Hughey, I. S. Mian, K. Sj¨olander, R. C. Un-
derwood, and D. Haussler, “Stochastic context-free grammars for tRNA modeling”,
Nucleic Acids Research 22 (1994), 5112–5120.
[SaNiCo77] F. Sanger, S. Nicklen, and A. R. Coulson, “DNA sequencing with chain-
terminating inhibitors”, Proceedings of the National Academy of Sciences USA 74
(1977), 5463–5467.
[Sa72] D. Sankoﬀ, “Matching sequences under deletion-insertion constraints”, Proceed-
ings of the National Academy of Sciences USA 69 (1972), 4–6.
[SaEtal83] D. Sankoﬀ, J. B. Kruskal, S. Mainville, and R. J. Cedergren, “Fast algorithms
to determine RNA secondary structures containing multiple loops”, in Time Warps,
String Edits, and Macromolecules: The Theory and Practice of Sequence Compari-
son, D. Sankoﬀand J. B. Kruskal (eds.), Addison-Wesley, 1983, 93–120.
[ScDeSa10] M. C. Schatz, A. L. Delcher, and S. L. Salzberg, “Assembly of large genomes
using second-generation sequencing.” Genome Research 20 (2010), 1165–1173.
[ScDa78] R. M. Schwartz and M. O. Dayhoﬀ, “Matrices for detecting distant relation-
ships”, in Atlas of Protein Sequence and Structure, Volume 5, Supplement 3, M.
O. Dayhoﬀ(ed.), National Biomedical Research Foundation, Washington, DC, 1978,
pp. 353–358.

1512
Chapter 20
DISCRETE BIOMATHEMATICS
[Se74] P. H. Sellers, “On the theory and computation of evolutionary distances”, SIAM
Journal on Applied Mathematics 26 (1974), 787–793.
[Se84] P. H. Sellers, “Pattern recognition in genetic sequences by mismatch density”,
Bulletin of Mathematical Biology 46 (1984), 501–514.
[SeSt03] C. Semple and M. Steel, Phylogenetics, Oxford University Press, 2003.
[ShEtal02] I. Shmulevich, E. R. Dougherty, S. Kim, and W. Zhang, “Probabilistic Boolean
networks: a rule-based uncertainty model for gene regulatory networks”, Bioinformat-
ics 18 (2002), 261–274.
[ShZh02] I. Shmulevich and W. Zhang, “Binary analysis and optimization-based normal-
ization of gene expression data”, Bioinformatics 18 (2002), 555–565.
[SiEtal05] E. Sim˜ao, E. Remy, D. Thieﬀry, and C. Chaouiya, “Qualitative modelling of
regulated metabolic pathways: application to the tryptophan biosynthesis in E. coli”,
Bioinformatics 21 (2005) Suppl. 2, 190–196.
[SiPo15] J. T. Simpson and M. Pop, “The theory and practice of genome sequence as-
sembly”, Annual Review of Genomics and Human Genetics 16 (2015), 153–172.
[SmWa81] T. F. Smith and M. S. Waterman, “Identiﬁcation of common molecular sub-
sequences”, Journal of Molecular Biology 147 (1981), 195–197.
[SpSt04] D. Speyer and B. Sturmfels, “The tropical Grassmanian”, Advances in Geometry
4 (2004), 389–411.
[Sp89] J. L. Spouge, “Speeding up dynamic programming algorithms for ﬁnding optimal
lattice paths”, SIAM Journal on Applied Mathematics 49 (1989), 1552–1566.
[StWa79] P. R. Stein and M. S. Waterman, “On some new sequences generalizing the
Catalan and Motzkin numbers”, Discrete Mathematics 26 (1979), 261–272.
[StVe11] B. Stigler and A. Veliz-Cuba, “Boolean models can explain bistability in the lac
operon”, Journal of Computational Biology 18 (2011), 783–794.
[SuEtal95] G. Sutton, O. White, M. Adams, and A. Kerlavage, “TIGR assembler: a
new tool for assembling large shotgun sequencing projects”, Genome Science and
Technology 1 (1995), 9–19.
[TaEtal11] K. Tamura, D. Peterson, N. Peterson, G. Stecher, M. Nei, and S. Kumar,
“MEGA5: molecular evolutionary genetics analysis using maximum likelihood, evo-
lutionary distance, and maximum parsimony methods”, Molecular Biology and Evo-
lution 28 (2011), 2731–2739.
[TaUk88] J. Tarhio and E. Ukkonen, “A greedy approximation algorithm for constructing
shortest common superstrings”, Theoretical Computer Science 51 (1988), 131–145.
[TaEtal09] M. Taufer, A. Licon, R. Araiza, D. Mireles, F. H. D. Van Batenburg, A. P.
Gultyaev, and M-Y. Leung, “Pseudobase++: an extension of pseudobase for easy
searching, formatting and visualization of pseudoknots”, Nucleic Acids Research 37
(2009), D127–D135.
[ThEtal98] D. Thieﬀry, A. M. Huerta, E. Perez-Rueda, and J. Collado Vides, “From
speciﬁc gene regulation to genomic networks: a global analysis of transcriptional
regulation in Escherichia coli”, BioEssays 20 (1998), 433–440.
[TiEtal73] I. Tinoco, P. N. Borer, B. Dengler, M. D. Levine, O. C. Uhlenbeck, D. M.
Crothers, and J. Gralla, “Improved estimation of secondary structure in ribonucleic
acids”, Nature 246 (1973), 40–41.

REFERENCES
1513
[TrSa12] T. J. Treangen and S. L. Salzberg, “Repetitive DNA and next-generation
sequencing: computational challenges and solutions”, Nature Reviews Genetics 13
(2012), 36–46.
[Tr79] W. T. Trotter, Jr., “A forbidden subgraph characterization of Roberts’ inequality
for boxicity”, Discrete Mathematics 28 (1979), 303–314.
[TuMa10] D. H. Turner and D. H. Mathews, “NNDB: the nearest neighbor parameter
database for predicting stability of nucleic acid secondary structure”, Nucleic Acids
Research 38 (2010), D280–D282.
[UeEtal99] Y. Uemura, A. Hasegawa, S. Kobayashi, and T. Yokomori, “Tree adjoining
grammars for RNA structure prediction”, Theoretical Computer Science 210 (1999),
277–303.
[vN66] J. von Neumann, The Theory of Self-Reproducing Automata, A. Burks (ed.),
Univ. of Illinois Press, 1966.
[Wa78] M. S. Waterman, “Secondary structure of single-stranded nucleic acid”, Advances
in Mathematics Supplementary Studies 1 (1978), 167–212.
[We97] J. Weimar, “Cellular automata for reaction-diﬀusion systems”, Parallel Comput-
ing 23 (1997), 1699–1715.
[We73] P. Weiner, “Linear pattern matching algorithms”, 14th Annual IEEE Symposium
on Switching and Automata Theory, 1973, 1–11.
[Wo82] S. Wolfram, “Cellular automata as simple self-organizing systems”, Caltech report
CALT-68-938, 1982.
[WuMa92] S. Wu and U. Manber, “Fast text searching: allowing errors”, Communications
of the ACM 35 (1992), 83–91.
[ZhEtal08] J. Zhang, M. Lin, R. Chen, W. Wang, and J. Liang, “Discrete state model
and accurate estimation of loop entropy of RNA secondary structures”, Journal of
Chemical Physics 128 (2008), 125107.
Web Resources:
http://adam.plantsimlab.org/ (Web-based software ADAM to analyze the dynamics
of discrete biological systems.)
http://bioquest.org/esteem (Contains Excel worksheets and Java scripts for analyz-
ing food webs.)
http://dimacs.rutgers.edu/IMB/TalksEtc/Foodwebs-and-Biodiversity-7-29-13.
pdf (Powerpoint slides for the presentation “Graphs, Food Webs and Biodiversity”.)
http://evolution.genetics.washington.edu/phylip/software.html (Website that
is maintained by J. Felsenstein; it lists available software for phylogenetic reconstruc-
tion. This page contains links to almost 400 phylogenetics software packages.)
http://mbcf149.dfci.harvard.edu/cmsmbr/biotools/biotools16.html (List of se-
quence alignment servers and databases.)
http://rosalind.info/problems/locations/ (Online platform for learning bioinfor-
matics algorithms through coding. Includes an extensive collection of exercises and
problems related to sequence alignment.)
http://seqanswers.com/wiki/Software (Detailed listing of software for bioinformat-
ics applications.)

1514
Chapter 20
DISCRETE BIOMATHEMATICS
http://treebase.org/treebase-web/ (The Treebase website contains an open access
database of recently published phylogenetic trees.)
http://unafold.rna.albany.edu
(UNAfold web server for nucleic acid folding and
hybridization.)
http://www.arn.org/docs/newman/rn artificiallife.htm (Artiﬁcial Life and Cel-
lular Automata by Robert C. Newman.)
http://www.daimi.au.dk/~compbio/pfold (Pfold RNA folding server that uses the
Knudsen-Hein grammar.)
http://www.ebi.ac.uk/Tools/emboss/align/ (Local and global tools for pairwise se-
quence alignment.)
http://www.genomesize.com (Comprehensive catalog of animal genome size data.)
http://www.ibiblio.org/lifepatterns/ (Java applet for Conway’s “Game of Life”.)
http://www-igm.univ-mlv.fr/~lecroq/string/
(Exact string matching algorithms
in C.)
http://www.langmead-lab.org/teaching-materials/ (Videos and lecture slides for
string matching algorithms from Ben Langmead, including Python code.)
http://www.phrap.com (PHRAP package for sequence assembly.)
http://www.tbi.univie.ac.at/RNA (RNA folding package to calculate minimal free
energy for secondary structures.)
https://blast.ncbi.nlm.nih.gov/Blast.cgi/ (Web server for running BLAST.)
https://github.com/nebneuron/neural-ideal (Software for computing with the neu-
ral ideal.)
https://www.cbcb.umd.edu/software (Open-source software for genome assembly.)

BIOGRAPHIES
Victor J. Katz
Niels Henrik Abel (1802–1829), born in Norway, was self-taught and studied the works
of many mathematicians. When he was nineteen years old, he proved that there is
no closed formula for solving the general ﬁfth degree equation. He also worked in the
areas of inﬁnite series and elliptic functions and integrals. The term abelian group
was coined in Abel’s honor in 1870 by Camille Jordan.
Leonard Adleman (born 1945) is credited with coining the term “computer virus”. He
was one of the inventors (with Adi Shamir and Ronald Rivest) of the RSA encryption
algorithm, currently used in secure data transmission. He received his Ph.D. from
the University of California, Berkeley in 1976. Adleman is also the creator of the ﬁeld
of DNA computing, beginning with a 1994 paper in which he used DNA to solve an
instance of the Hamiltonian Graph problem. He is currently a Professor of Computer
Science and Molecular Biology at the University of Southern California
Kenneth Appel (1932–2013) is best known for his solution of the four-color problem
(along with Wolfgang Haken). He received his Ph.D. from the University of Michigan
in 1959 and spent most of his career at the University of Illinois. The proof of the
four-color theorem broke new ground, since it rests on hundreds of hours of computer
calculation. Although the proof was initially disdained by many in the mathematical
community, it actually provided a start to a change in the attitudes of mathematicians
to computers, and has led to much further use of computers, even in theoretical
mathematics.
Aristotle (384–322 B.C.E.) was the most famous student at Plato’s academy in Athens.
After Plato’s death in 347 B.C.E., he was invited to the court of Philip II of Macedon
to educate Philip’s son Alexander, who soon thereafter began his successful conquest
of the Mediterranean world. Aristotle himself returned to Athens, where he founded
his own school, the Lyceum, and spent the remainder of his life writing and lecturing.
He wrote on numerous subjects, but is perhaps best known for his works on logic,
including the Prior Analytics and the Posterior Analytics. In these works, Aristotle
developed the notion of logical argument, based on several explicit principles.
In
particular, he built his arguments out of syllogisms and concluded that demonstrations
using his procedures were the only certain way of attaining scientiﬁc knowledge.
Emil Artin (1898–1962) was born in Vienna and in 1921 received a Ph.D. from the
University of Leipzig. He held a professorship at the University of Hamburg until 1937,
when he came to the United States. In the U.S. he taught at the University of Notre
Dame, Indiana University, and Princeton.
In 1958 he returned to the University
of Hamburg. Artin’s mathematical contributions were in number theory, algebraic
topology, linear algebra, and especially in many areas of abstract algebra.
Charles Babbage (1792–1871) was an English mathematician best known for his in-
vention of two of the earliest computing machines, the Diﬀerence Engine, designed
to calculate polynomial functions, and the Analytical Engine, a general purpose cal-
culating machine. The Diﬀerence Engine was designed to use the idea that the nth
order diﬀerences in nth degree polynomials were always constant and then to work

1516
BIOGRAPHIES
backwards from those diﬀerences to the original polynomial values. Although Bab-
bage received a grant from the British government to help in building the Engine, he
never was able to complete one because of various diﬃculties in developing machine
parts of suﬃcient accuracy. In addition, Babbage became interested in his more ad-
vanced Analytical Engine. This latter device was to consist of a store, in which the
numerical variables were kept, and a mill, in which the operations were performed.
The entire machine was to be controlled by instructions on punched cards. Unfor-
tunately, although Babbage made numerous engineering drawings of sections of the
Analytical Engine and gave a series of seminars in 1840 on its workings, he was never
able to build a working model.
Paul Gustav Heinrich Bachmann (1837–1920) studied mathematics at the University
of Berlin and at G¨ottingen. In 1862 he received a doctorate in group theory and held
positions at the universities at Breslau and M¨unster. He wrote several volumes on
number theory, introducing the big-O notation in his 1892 book.
John Backus (1924–2007) received bachelor’s and master’s degrees in mathematics
from Columbia University. He led the group at IBM that developed FORTRAN in
1954. Later in the 1950s, he was an inﬂuential member of the international team that
developed ALGOL, using the Backus-Naur form for the syntax of the language. He
received the National Medal of Science in 1974 and the Turing Award in 1977 “for
profound, inﬂuential, and lasting contributions to the design of practical high-level
programming systems.”
Abu-l-’Abbas Ahmad ibn Muhammad ibn al-Banna al-Marrakushi (1256–1321)
was an Islamic mathematician who lived in Marrakech in what is now Morocco. Ibn al-
Banna developed the ﬁrst known proof of the basic combinatorial formulas, beginning
by showing that the number of permutations of a set of n elements was n! and then
developing in a careful manner the multiplicative formula to compute the values for
the number of combinations of k objects in a set of n.
Using these two results,
he also showed how to calculate the number of permutations of k objects from a
set of n. The formulas themselves had been known in the Islamic world for many
years, in connection with speciﬁc problems like calculating the number of words of
a given length which could be formed from the letters of the Arabic alphabet. Ibn
al-Banna’s main contribution, then, was to abstract the general idea of permutations
and combinations out of the various speciﬁc problem situations considered earlier.
Thomas Bayes (1702–1761) an English Nonconformist, wrote an Introduction to the
Doctrine of Fluxions in 1736 as a response to Berkeley’s Analyst with its severe
criticism of the foundations of the calculus. He is best known, however, for attempting
to answer the basic question of statistical inference in his An Essay Towards Solving
a Problem in the Doctrine of Chances, published three years after his death. That
basic question is to determine the probability of an event, given empirical evidence
that it has occurred a certain number of times in a certain number of trials. To do
this, Bayes gave a straightforward deﬁnition of probability and then proved that for
two events E and F, the probability of E given that F has happened is the quotient of
the probability of both E and F happening divided by the probability of F alone. By
using areas to model probability, he was then able to show that, if x is the probability
of an event happening in a single trial, if the event has happened p times in n trials,
and if 0 < r < s < 1, then the probability that x is between r and s is given by
the quotient of two integrals. Although in principle these integrals can be calculated,
there has been a great debate since Bayes’ time about the circumstances under which
his formula gives an appropriate answer.
James Bernoulli (Jakob I) (1654–1705) was one of eight mathematicians in three

BIOGRAPHIES
1517
generations of his family. He was born in Basel, Switzerland, studied theology in
addition to mathematics and astronomy, and entered the ministry. In 1682 be began
to lecture at the University of Basel in natural philosophy and mechanics. He became
professor at the University of Basel in 1687, and remained there until his death. His
research included the areas of the calculus of variations, probability, and analytic
geometry.
His most well-known work is Ars Conjectandi, in which he described
results in combinatorics and probability, including applications to gambling and the
law of large numbers; this work also contained a reprint of the ﬁrst formal treatise in
probability, written in 1657 by Christiaan Huygens.
Bhaskara (1114–1185), the most famous of medieval Indian mathematicians, gave a
complete algorithmic solution to the Pell equation Dx2 ± 1 = y2. That equation had
been studied by several earlier Indian mathematicians as well. Bhaskara served much
of his adult life as the head of the astronomical observatory at Ujjain, some 300 miles
northeast of Bombay, and became widely respected for his skills in astronomy and the
mechanical arts, as well as mathematics. Bhaskara’s mathematical contributions are
chieﬂy found in two chapters, the Lilavati and the Bijaganita, of a major astronomical
work, the Siddh¯antasiromani. These include techniques of solving systems of linear
equations with more unknowns than equations as well as the basic combinatorial
formulas, although without any proofs.
Bel´a Bollob´as (born 1943) was on the Hungarian team in the ﬁrst three International
Mathematical Olympiads, beginning in 1959, and won two gold medals, thereby com-
ing to the attention of Paul Erd˝os, who encouraged him in his studies. Although he
received his Ph.D. from the University of Budapest in 1967, he soon was able to leave
Hungary and vowed not to return while it was still under communist control. He
received a fellowship to Trinity College, Cambridge, where he remained until 1996,
then accepted a professorship at the University of Memphis. He has made major
contributions to ﬁelds of extremal graph theory and the theory of random graphs
and has published several research monographs in his ﬁeld as well as introductory
textbooks in combinatorics and graph theory.
George Boole (1815–1864) was an English mathematician most famous for his work
in logic.
Born the son of a cobbler, he had to struggle to educate himself while
supporting his family. But he was so successful in his self-education that he was able
to set up his own school before he was 20 and was asked to give lectures on the work
of Isaac Newton. In 1849 he applied for and was appointed to the professorship in
mathematics at Queen’s College, Cork, despite having no university degree. In 1847,
Boole published a small book, The Mathematical Analysis of Logic, and seven years
later expanded it into An Investigation of the Laws of Thought. In these books, Boole
introduced what is now called Boolean algebra as part of his aim to “investigate the
fundamental laws of those operations of the mind by which reasoning is performed;
to give expression to them in the symbolical language of a Calculus, and upon this
foundation to establish the science of Logic and construct its method.” In addition
to his work on logic, Boole wrote texts on diﬀerential equations and on diﬀerence
equations that were used in Great Britain until the end of the nineteenth century.
William Burnside (1852–1927), was born in London, graduated from Cambridge in
1875, and remained there as lecturer until 1885. He then went to the Royal Naval
College at Greenwich, where he stayed until he retired. Although he published much
in applied mathematics, probability, and elliptic functions, he is best known for his
extensive work in group theory (including the classic book Theory of Groups). His
conjecture that groups of odd order are solvable was proved by Walter Feit and John
Thompson and published in 1963.

1518
BIOGRAPHIES
Georg Ferdinand Ludwig Philip Cantor (1845–1918) was born in Russia to Danish
parents, received a Ph.D. in number theory in 1867 at the University of Berlin, and in
1869 took a position at Halle University, where he remained until his retirement. He
is regarded as a founder of set theory. He was interested in theology and the nature of
the inﬁnite. His work on the convergence of Fourier series led to his study of certain
types of inﬁnite sets of real numbers, and ultimately to an investigation of transﬁnite
numbers.
Augustin-Louis Cauchy (1789–1857), the most proliﬁc mathematician of the nine-
teenth century, is most famous for his textbooks in analysis written in the 1820s for
use at the ´Ecole Polytechnique, textbooks which became the model for calculus texts
for the next hundred years. Although born in the year the French Revolution began,
Cauchy was a staunch conservative. When the July Revolution of 1830 led to the
overthrow of the last Bourbon king, Cauchy refused to take the oath of allegiance to
the new king and went into a self-imposed exile in Italy and then in Prague. He did
not return to his teaching posts until the Revolution of 1848 led to the removal of
the requirement of an oath of allegiance. Among the many mathematical subjects
to which he contributed besides calculus were the theory of matrices, in which he
demonstrated that every symmetric matrix can be diagonalized by use of an orthog-
onal substitution, and the theory of permutations, in which he was the earliest to
consider these from a functional point of view. In fact, he used a single letter, say S,
to denote a permutation and S−1 to denote its inverse and then noted that the pow-
ers S, S2, S3, . . . of a given permutation on a ﬁnite set must ultimately result in the
identity. He also introduced the current notation (a1a2 . . . an) to denote the cyclic
permutation on the letters a1, a2, . . . , an.
Arthur Cayley (1821–1895), although graduating from Trinity College, Cambridge as
Senior Wrangler, became a lawyer because there were no suitable mathematics posi-
tions available at that time in England. He produced nearly 300 mathematical papers
during his fourteen years as a lawyer, and in 1863 was named Sadlerian professor of
mathematics at Cambridge. Among his numerous mathematical achievements are the
earliest abstract deﬁnition of a group in 1854, out of which he was able to calculate all
possible groups of order up to eight, and the basic rules for operating with matrices,
including a statement (without proof) of the Cayley-Hamilton theorem that every
matrix satisﬁes its characteristic equation. Cayley also developed the mathematical
theory of trees in an article in 1857. In particular, he dealt with the notion of a rooted
tree, a tree with a designated vertex called a root, and developed a recursive formula
for determining the number of diﬀerent rooted trees in terms of its branches (edges).
In 1874, Cayley applied his results on trees to the study of chemical isomers.
Pafnuty Lvovich Chebyshev (1821–1894) was a Russian mathematician who received
his master’s degree in 1846 from Moscow University. From 1860 until 1882 he was a
professor at the University of St. Petersburg. His mathematical research in number
theory dealt with congruences and the distribution of primes; he also studied the
approximation of functions by polynomials.
Avram Noam Chomsky (born 1928) received a Ph.D. in linguistics at the University of
Pennsylvania. For many years he was a professor of foreign languages and linguistics
at M.I.T.
He has made many signiﬁcant contributions to the study of linguistics
and the study of grammars, but is probably better known for his left-wing political
activism.
Chrysippus (280–206 B.C.E.) was a Stoic philosopher who developed some of the basic
principles of the propositional logic, which ultimately replaced Aristotle’s logic of
syllogisms. He was born in Cilicia, in what is now Turkey, but spent most of his life

BIOGRAPHIES
1519
in Athens, and is said to have authored more than 700 treatises. Among his other
achievements, Chrysippus analyzed the rules of inference in the propositional calculus,
including the rules of modus ponens, modus tollens, the hypothetical syllogism, and
the alternative syllogism.
Fan Chung (born 1949) was born in Taiwan and received her Ph.D. from the University
of Pennsylvania in 1974.
She became an expert in Ramsey theory and has also
made numerous contributions to graph theory, in particular to the applications of
graph theory in Internet computing, communication networks, and software reliability.
She spent many years at Bell Labs, where she collaborated with Ron Graham and
eventually married him. She is currently a Distinguished Professor of Mathematics
at the University of California, San Diego.
Alonzo Church (1903–1995) studied under Hilbert at G¨ottingen, was on the faculty at
Princeton from 1927 until 1967, and then held a faculty position at UCLA. He is a
founding member of the Association for Symbolic Logic. He made many contributions
in various areas of logic and the theory of algorithms, and stated the Church-Turing
thesis (if a problem can be solved with an eﬀective algorithm, then the problem can
be solved by a Turing machine).
Cliﬀord Cocks (born 1950) is a British mathematician and cryptographer who invented
the public key algorithm in 1973 now known as the RSA algorithm (after its inde-
pendent invention four years later by Ronald Rivest, Adi Shamir, and Len Adleman).
Cocks had invented the algorithm, using prime factorization, while he was working
for the Communications-Electronics Security Group in Britain, an arm of the United
Kingdom Government Communications Headquarters (GCHQ). But although GCHQ
did not use the algorithm, they treated it as classiﬁed information and did not release
the research until 1997. In 2001, Cocks developed one of the ﬁrst secure identity
based encryption schemes.
George Dantzig (1914–2005) is an American mathematician who formulated the gen-
eral linear programming problem of maximizing a linear objective function subject
to several linear constraints and developed the simplex method of solution in 1947.
His study of linear programming grew out of his World War II service as a mem-
ber of Air Force Project SCOOP (Scientiﬁc Computation of Optimum Programs),
a project chieﬂy concerned with resource allocation problems. After the war, linear
programming was applied to numerous problems, especially military and economic
ones, but it was not until such problems could be solved on a computer that the real
impact of their solution could be felt. The ﬁrst successful solution of a major linear
programming problem on a computer took place in 1952 at the National Bureau of
Standards. After he left the Air Force, Dantzig worked for the RAND Corporation
and then served as a professor of operations research at Stanford University.
Richard Dedekind (1831–1916) was born in Brunswick, in northern Germany, and
received a doctorate in mathematics at G¨ottingen under Gauss. He held positions at
G¨ottingen and in Zurich before returning to the Polytechnikum in Brunswick. Al-
though at various times he could have received an appointment to a major German
university, he chose to remain in his home town where he felt he had suﬃcient free-
dom to pursue his mathematical research. Among his many contributions was his
invention of the concept of ideals to resolve the problem of the lack of unique factor-
ization in rings of algebraic integers. Even though the rings of integers themselves did
not possess unique factorization, Dedekind showed that every ideal is either prime or
uniquely expressible as the product of prime ideals. Dedekind published this theory
as a supplement to the second edition (1871) of Dirichlet’s Vorlesungen ¨uber Zahlen-
theorie, of which he was the editor. In the supplement, he also gave one of the ﬁrst

1520
BIOGRAPHIES
deﬁnitions of a ﬁeld, conﬁning this concept to subsets of the complex numbers.
Abraham de Moivre (1667–1754) was born into a Protestant family in Vitry, France,
a town about 100 miles east of Paris, and studied in Protestant schools up to the age
of 14. Soon after the revocation of the Edict of Nantes in 1685 made life very diﬃcult
for Protestants in France, however, he was imprisoned for two years. He then left
France for England, never to return. Although he was elected to the Royal Society in
1697, in recognition of a paper on “A method of raising an inﬁnite Multinomial to any
given Power or extracting any given Root of the same”, he never achieved a university
position. He made his living by tutoring and by solving problems arising from games of
chance and annuities for gamblers and speculators. De Moivre’s major mathematical
work was The Doctrine of Chances (1718, 1736, 1756), in which he devised methods
for calculating probabilities by use of binomial coeﬃcients. In particular, he derived
the normal approximation to the binomial distribution and, in essence, invented the
notion of the standard deviation.
Augustus De Morgan (1806–1871) graduated from Trinity College, Cambridge in
1827. He was the ﬁrst mathematics professor at University College in London, where
he remained on the faculty for 30 years. He founded the London Mathematical Society.
He wrote over 1000 articles and textbooks in probability, calculus, algebra, set theory,
and logic (including DeMorgan’s laws, an abstraction of the duality principle for sets).
He gave a precise deﬁnition of limit, developed tests for convergence of inﬁnite series,
and gave a clear explanation of the Principle of Mathematical Induction.
Ren´e Descartes (1596–1650) left school at 16 and went to Paris, where he studied
mathematics for two years.
In 1616 he earned a law degree at the University of
Poitiers. In 1617 he enlisted in the army and traveled through Europe until 1629,
when he settled in Holland for the next 20 years. During this productive period of
his life he wrote on mathematics and philosophy, attempting to reduce the sciences
to mathematics. In 1637 his Discours was published; this book contained the devel-
opment of analytic geometry. In 1649 he has invited to tutor the Queen Christina of
Sweden in philosophy. There he soon died of pneumonia.
Leonard Eugene Dickson (1874–1954) was born in Iowa and in 1896 received the ﬁrst
Ph.D. in mathematics given by the University of Chicago, where he spent much of his
faculty career. His research interests included abstract algebra (including the study
of matrix groups and ﬁnite ﬁelds) and number theory.
Edsger W. Dijkstra (1930–2002) was a Dutch computer scientist who helped shape
the emerging ﬁeld of computer science from both an engineering and theoretical per-
spective. In particular, he turned computer programming into a scientiﬁc discipline.
He made contributions in many areas of the emerging domain of computer science.
For example, he originated the phrase “structured programming”, and his ideas in
this area helped lay the foundation for the new discipline of software engineering. He
is known for Dijkstra’s algorithm, which found the shortest path in a graph between
any two given nodes. He was also on the team that implemented the ﬁrst compiler for
the language ALGOL 60. After twenty years of work in the Netherlands, he joined
the Burroughs Corporation in 1973 as a research fellow, moving eleven years later to
the University of Texas at Austin, where he remained until his retirement.
Diophantus (c. 250) was an Alexandrian mathematician about whose life little is known
except what is reported in an epigram of the Greek Anthology (c. 500), from which
it can be calculated that he lived to the age of 84. His major work, however, the
Arithmetica, has been extremely inﬂuential. Despite its title, this is a book on alge-
bra, consisting mostly of an organized collection of problems translatable into what

BIOGRAPHIES
1521
are today called indeterminate equations, all to be solved in rational numbers. Dio-
phantus introduced the use of symbolism into algebra and outlined the basic rules for
operating with algebraic expressions, including those involving subtraction. It was in
a note appended to Problem II-8 of the 1621 Latin edition of the Arithmetica — to
divide a given square number into two squares — that Pierre de Fermat ﬁrst asserted
the impossibility of dividing an nth power (n > 2) into the sum of two nth powers.
This result, now known as Fermat’s Last Theorem, was ﬁnally proved in 1994 by
Andrew Wiles.
Charles Lutwidge Dodgson (1832–1898) is more familiarly known as Lewis Carroll,
the pseudonym he used in writing his famous children’s works Alice in Wonderland
and Through the Looking Glass. Dodgson graduated from Oxford University in 1854
and the next year was appointed a lecturer in mathematics at Christ Church College,
Oxford.
Although he was not successful as a lecturer, he did contribute to four
areas of mathematics: determinants, geometry, the mathematics of tournaments and
elections, and recreational logic. In geometry, he wrote a ﬁve-act comedy, “Euclid
and His Modern Rivals”, about a mathematics lecturer Minos in whose dreams Euclid
debates his Elements with various modernizers but always manages to demolish the
opposition. He is better known, however, for his two books on logic, Symbolic Logic
and The Game of Logic. In the ﬁrst, he developed a symbolical calculus for analyzing
logical arguments and wrote many humorous exercises designed to teach his methods,
while in the second, he demonstrated a game which featured various forms of the
syllogism.
Eratosthenes (276–194 B.C.E) was born in Cyrene (North Africa) and studied at
Plato’s Academy in Athens.
He was tutor of the son of King Ptolemy III Euer-
getes in Alexandria and became chief librarian at Alexandria. He is recognized as the
foremost scholar of his time and wrote in many areas, including number theory (his
sieve for obtaining primes) and geometry. He introduced the concepts of meridians
of longitude and parallels of latitude and used these to measure distances, including
an estimation of the circumference of the earth.
Paul Erd˝os (1913–1996) was born in Budapest. At 21 he received a Ph.D. in mathemat-
ics from E˝otv˝os University. After leaving Hungary in 1934, he traveled extensively
throughout the world, with very few possessions and no permanent home, working
with other mathematicians in combinatorics, graph theory, number theory, and many
other areas. He was author or coauthor of approximately 1500 papers with 500 coau-
thors.
Euclid (c. 300 B.C.E.) is responsible for the most famous mathematics text of all time,
the Elements. Not only does this work deal with the standard results of plane ge-
ometry, but it also contains three chapters on number theory, one long chapter on
irrational quantities, and three chapters on solid geometry, culminating with the
construction of the ﬁve regular solids. The axiom-deﬁnition-theorem-proof style of
Euclid’s work has become the standard for formal mathematical writing up to the
present day. But about Euclid’s life virtually nothing is known. It is, however, gener-
ally assumed that he was among the ﬁrst mathematicians at the Museum and Library
of Alexandria, which was founded around 300 B.C.E by Ptolemy I Soter, the Mace-
donian general of Alexander the Great who became ruler of Egypt after Alexander’s
death in 323 B.C.E.
Leonhard Euler (1707–1783) was born in Basel, Switzerland and became one of the
earliest members of the St. Petersburg Academy of Sciences. He was the most proliﬁc
mathematician of all time, making contributions to virtually every area of the subject.
His series of analysis texts established many of the notations and methods still in use

1522
BIOGRAPHIES
today. He created the calculus of variations and established the theory of surfaces
in diﬀerential geometry.
His study of the K¨onigsberg bridge problem led to the
formulation and solution of one of the ﬁrst problems in graph theory.
He made
numerous discoveries in number theory, including a detailed study of the properties
of residues of powers and the ﬁrst statement of the quadratic reciprocity theorem.
He developed an algebraic formula for determining the number of partitions of an
integer n into m distinct parts, each of which is in a given set A of distinct positive
integers. And in a paper of 1782, he even posed the problem of the existence of a
pair of orthogonal Latin squares: If there are 36 oﬃcers, one of each of six ranks from
each of six diﬀerent regiments, can they be arranged in a square in such a way that
each row and column contains exactly one oﬃcer of each rank and one from each
regiment?
Abraham ibn Ezra (1089–1164) was a Spanish poet, philosopher, astrologer, and bib-
lical commentator who was born in Tudela, but spent the latter part of his life as a
wandering scholar in Italy, France, England, and Palestine. It was in an astrological
text that ibn Ezra developed a method for calculating numbers of combinations, in
connection with determining the number of possible conjunctions of the seven “plan-
ets” (including the sun and the moon). He gave a detailed argument for the cases
n = 7, k = 2 to 7, of a rule which can easily be generalized to the modern formula
C(n, k) = Pn−1
i=k−1 C(i, k −1). Ibn Ezra also wrote a work on arithmetic in which he
introduced the Hebrew-speaking community to the decimal place-value system. He
used the ﬁrst nine letters of the Hebrew alphabet to represent the ﬁrst nine numbers,
used a circle to represent zero, and demonstrated various algorithms for calculation
in this system.
Kam¯al al-D¯ın al-F¯aris¯ı (died 1320) was a Persian mathematician most famous for his
work in optics. In fact, he wrote a detailed commentary on the great optical work of
Ibn al-Haytham. But al-F¯aris¯ı also made major contributions to number theory. He
produced a detailed study of the properties of amicable numbers (pairs of numbers
in which the sum of the proper divisors of each is equal to the other). As part of this
study, al-F¯aris¯ı developed and applied various combinatorial principles. He showed
that the classical ﬁgurate numbers (triangular, pyramidal, etc.) could be interpreted
as numbers of combinations and thus helped to found the theory of combinatorics on
a more abstract basis.
Pierre de Fermat (1607–1665) was a lawyer and magistrate for whom mathematics was
a pastime that led to contributions in many areas: calculus, number theory, analytic
geometry, and probability theory. He received a bachelor’s degree in civil law in 1631,
and from 1648 until 1665 was King’s Counsellor. He suﬀered an attack of the plague
in 1652, and from then on he began to devote time to the study of mathematics. He
helped give a mathematical basis to probability theory when, together with Blaise
Pascal, he solved M´er´e’s paradox: why is it less likely to roll a 6 at least once in
four tosses of one die than to roll a double 6 in 24 tosses of two dice?
He was
a discoverer of analytic geometry and used inﬁnitesimals to ﬁnd tangent lines and
determine maximum and minimum values of curves. In 1657 he published a series of
mathematical challenges, including the conjecture that xn + yn = zn has no solution
in positive integers if n is an integer greater than 2. He wrote in the margin of a book
that he had a proof, but the proof would not ﬁt in the margin. His conjecture was
ﬁnally proved by Andrew Wiles in 1994.
Fibonacci (Leonardo of Pisa) (c. 1175–c. 1250) was the son of a Mediterranean mer-
chant and government worker named Bonaccio (hence his name ﬁlius Bonaccio, “son
of Bonaccio”). Fibonacci, born in Pisa and educated in Bougie (on the north coast of

BIOGRAPHIES
1523
Africa where his father was administrator of Pisa’s trading post), traveled extensively
around the Mediterranean. He is regarded as the greatest mathematician of the Mid-
dle Ages. In 1202 he wrote the book Liber Abaci, an extensive treatment of topics
in arithmetic and algebra, and emphasized the beneﬁts of Arabic numerals (which
he knew about as a result of his travels). In this book he also discussed the rabbit
problem that led to the sequence that bears his name: 1, 1, 2, 3, 5, 8, 13, . . .. In 1225
he wrote Liber Quadratorum (Book of Squares) as a response to a challenge from
Master John of Palermo to “ﬁnd a square number from which when ﬁve is added or
subtracted, always arises a square number.”
Lester R. Ford, Jr. (born 1927) is a pioneer in the ﬁeld of network ﬂows. He received
his doctorate from the University of Illinois in 1953. Ford worked as a researcher for
the Council for Economic and Industry Research and the RAND Corporation. While
working at RAND, he collaborated with D. R. Fulkerson on seminal work on network
ﬂows. Their Ford-Fulkerson algorithm, ﬁrst published in a technical report in 1954,
ﬁnds the maximum ﬂow in a capacitated network. His work on graph algorithms
includes the widely used Bellman-Ford algorithm.
This algorithm, invented with
Richard Bellman, ﬁnds the shortest path in a weighted digraph in which edge weights
can be negative. His father, Lester R. Ford, Sr., was a well-known mathematician
who worked in number theory and was esteemed for his mathematical exposition.
Joseph Fourier (1768–1830), orphaned at the age of 9, was educated in the military
school of his home town of Auxerre, 90 miles southeast of Paris. Although he hoped to
become an army engineer, such a career was not available to him at the time because
he was not of noble birth. He therefore took up a teaching position. During the
Revolution, he was outspoken in defense of victims of the Terror of 1794. Although
he was arrested, he was released after the death of Robespierre and was appointed in
1795 to a position at the ´Ecole Polytechnique. After serving in various administrative
posts under Napoleon, he was elected to the Acad´emie des Sciences and from 1822
until his death served as its perpetual secretary. It was in connection with his work
on heat diﬀusion, detailed in his Analytic Theory of Heat of 1822, and, in particular,
with his solution of the heat equation ∂v
∂t = ∂2v
∂x2 + ∂2v
∂y2 , that he developed the concept of
a Fourier series. Fourier also analyzed the relationship between the series solution of
a partial diﬀerential equation and an appropriate integral representation and thereby
initiated the study of Fourier integrals and Fourier transforms.
Georg Frobenius (1849–1917) organized and analyzed the central ideas of the theory of
matrices in his 1878 memoir “On linear substitutions and bilinear forms”. Frobenius
there deﬁned the general notion of equivalent matrices. He also dealt with the special
cases of congruent and similar matrices. Frobenius showed that when two symmetric
matrices were similar, the transforming matrix could be taken to be orthogonal, one
whose inverse equaled its transpose. He then made a detailed study of orthogonal
matrices and showed that their eigenvalues were complex numbers of absolute value 1.
He also gave the ﬁrst complete proof of the Cayley-Hamilton theorem that a matrix
satisﬁes its characteristic equation. Frobenius, a full professor in Zurich and later
in Berlin, made his major mathematical contribution in the area of group theory.
He was instrumental in developing the concept of an abstract group, as well as in
investigating the theory of ﬁnite matrix groups and group characters.
Delbert Ray Fulkerson (1924–1976) received his Ph.D. at the University of Wisconsin,
Madison in 1951. His most important work was the development (with Lester Ford,
Jr.) of the Ford-Fulkerson algorithm, used to solve the maximum ﬂow problem in
capacitated networks. Such problems occur in many real-world situations such as
airline crew scheduling and railway traﬃc ﬂow. Fulkerson began his career at the

1524
BIOGRAPHIES
RAND Corporation and later served as Professor of Engineering at Cornell University.
In his honor, the Fulkerson Prize for outstanding papers in discrete mathematics is
awarded every three years jointly by the Mathematical Optimization Society and the
American Mathematical Society.
David Gale (1921–2008) was a game theorist who received his Ph.D. in 1949 from
Princeton University. He made major contributions to mathematical economics, in-
cluding solving the n-dimensional Ramsey Problem and his theory of optimal eco-
nomic growth. In fact, for the last half of his career, he held a joint appointment
in mathematics, operations research, and economics at the University of California,
Berkeley. He was always fascinated with various kinds of puzzles and math games
and wrote the Mathematical Entertainments column of The Mathematical Intelli-
gencer from 1991 to 1997.
´Evariste Galois (1811–1832) led a brief, tragic life which ended in a duel fought under
mysterious circumstances. He was born in Bourg-la-Reine, a town near Paris. He
developed his mathematical talents early and submitted a memoir on the solvability
of equations of prime degree to the French Academy in 1829. Unfortunately, the
referees were never able to understand this memoir nor his revised version submitted
in 1831. Meanwhile, Galois became involved in the revolutionary activities surround-
ing the July revolution of 1830 and was arrested for threatening the life of King
Louis-Phillipe and then for wearing the uniform of a National Guard division which
had been dissolved because of its perceived threat to the throne. His mathematics
was not fully understood until ﬁfteen years after his death when his manuscripts were
ﬁnally published by Liouville in the Journal des math´ematique. But Galois had in
fact shown the relationship between subgroups of the group of permutations of the
roots of a polynomial equation and the various extension ﬁelds generated by these
roots, the relationship at the basis of what is now known as Galois theory. Galois
also developed the notion of a ﬁnite ﬁeld in connection with solving the problem of
ﬁnding solutions to congruences F(x) ≡0 (mod p), where F(x) is a polynomial of
degree n and no residue modulo the prime p is itself a solution.
Carl Friedrich Gauss (1777–1855), often referred to as the greatest mathematician who
ever lived, was born in Brunswick, Germany. He received a Ph.D. from the University
of Helmstedt in 1799, proving the Fundamental Theorem of Algebra as part of his
dissertation. At age 24 Gauss published his important work on number theory, the
Disquisitiones Arithmeticae, a work containing not only an extensive discussion of
the theory of congruences, culminating in the quadratic reciprocity theorem, but also
a detailed treatment of cyclotomic equations in which he showed how to construct
regular n-gons by Euclidean techniques whenever n is prime and n−1 is a power of 2.
Gauss also made fundamental contributions to the diﬀerential geometry of surfaces
as well as to complex analysis, astronomy, geodesy, and statistics during his long
tenure as a professor at the University of G¨ottingen. It was in connection with using
the method of least squares to solve an astronomical problem that Gauss devised the
systematic procedure for solving a system of linear equations today known as Gaussian
elimination. (Unknown to Gauss, the method had appeared in Chinese mathematics
texts 1800 years earlier.) Gauss’ notebooks, discovered after his death, contained
investigations in numerous areas of mathematics in which he did not publish, including
the basics of non-Euclidean geometry.
Sophie Germain (1776–1831) was forced to study in private due to the turmoil of
the French Revolution and the opposition of her parents.
She nevertheless mas-
tered mathematics through calculus and wanted to continue her study in the ´Ecole
Polytechnique when it opened in 1794. But because women were not admitted as

BIOGRAPHIES
1525
students, she diligently collected and studied the lecture notes from various mathe-
matics classes and, a few years later, began a correspondence with Gauss (under the
pseudonym Monsieur LeBlanc, fearing that Gauss would not be willing to recognize
the work of a woman) on ideas in number theory. She was, in fact, responsible for
suggesting to the French general leading the army occupying Brunswick in 1807 that
he insure Gauss’ safety. Germain’s chief mathematical contribution was in connection
with Fermat’s Last Theorem. She showed that xn + yn = zn has no positive integer
solution where xyz is not divisible by n for any odd prime n less than 100. She
also made contributions in the theory of elasticity and won a prize from the French
Academy in 1815 for an essay in this ﬁeld.
Kurt G¨odel (1906–1978) was an Austrian mathematician who spent most of his life at
the Institute for Advanced Study in Princeton. He made several surprising contribu-
tions to set theory, demonstrating that Hilbert’s goal of showing that a reasonable
axiomatic system for set theory could be proven to be complete and consistent was in
fact impossible. In several seminal papers published in the 1930s, G¨odel proved that
it was impossible to prove internally the consistency of the axioms of any reasonable
system of set theory containing the axioms for the natural numbers. Furthermore,
he showed that any such system was inherently incomplete, that is, that there are
propositions expressible in the system for which neither they nor their negations are
provable. G¨odel’s investigations were stimulated by the problems surrounding the
axiom of choice, the axiom that for any set S of nonempty disjoint sets, there is a
subset T of the union of S that has exactly one element in common with each member
of S. Since that axiom led to many counterintuitive results, it was important to show
that the axiom could not lead to contradictions. But given his initial results, the best
G¨odel could do was to show that the axiom of choice was relatively consistent, that
its addition to the Zermelo-Fraenkel axiom set did not lead to any contradictions that
would not already have been implied without it.
Ronald Graham (born 1935) is a central ﬁgure in the development of discrete math-
ematics. He received his Ph.D. in 1962 from the University of California, Berkeley
and spent most of his professional life at Bell Labs, although he was appointed to
an endowed chair at the University of California, San Diego in 1999. His work has
been seminal in the origin of three new branches of mathematics: Ramsey theory,
computational geometry, and worst-case analysis of multiprocessing algorithms. He
was a long-time collaborator with Paul Erd˝os, frequently housing that wandering
mathematician during the last decades of his life. Graham served as president of the
American Mathematical Society in 1993-1994 and the Mathematical Association of
America in 2003-2004 and received the Steele Award for Lifetime Achievement from
the former in 2003. He is also a skilled juggler and has served as president of the
International Jugglers’ Association.
Wolfgang Haken (born 1928), along with Kenneth Appel, produced the ﬁrst proof of
the four-color problem with the help of a computer, announcing it to the world on
July 22, 1976. He then presented a paper describing the proof at the August, 1976
Summer Meeting of the American Mathematical Society in Toronto, to a very mixed
reaction. Haken received his education in Germany, but spent most of his academic
career at the University of Illinois. In addition to his work on the four-color problem,
he has contributed numerous ideas to the subject of algorithmic topology.
Marshall Hall (1910–1990) was a group theorist whose 1959 textbook on the subject
is still being used today. Hall received his Ph.D. from Yale in 1936 and later taught
at Ohio State, the California Institute of Technology, and Emory University. During
World War II, he spent time at Bletchley Park, working on breaking the German

1526
BIOGRAPHIES
Enigma codes. Among his important results in group theory was his solution of the
Burnside problem of exponent 6, in which he showed that a ﬁnitely generated group in
which the order of every element divides 6 must be ﬁnite. Later, he worked out (with
James Senior) detailed descriptions of every group of order 32 and 64, publishing the
results in an organized fashion in The Groups of Order 2n n ≤6. Hall is also famous
for some fundamental work in the theory of projective planes as well as other areas
of combinatorics.
William Rowan Hamilton (1805–1865), born in Dublin, was a child prodigy who be-
came the Astronomer Royal of Ireland in 1827 in recognition of original work in optics
accomplished during his undergraduate years at Trinity College, Dublin. In 1837, he
showed how to introduce complex numbers into algebra axiomatically by considering
a + ib as a pair (a, b) of real numbers with appropriate computational rules. After
many years of seeking an appropriate deﬁnition for multiplication rules for triples of
numbers which could be applied to vector analysis in 3-dimensional space, he discov-
ered that it was in fact necessary to consider quadruplets of numbers, which Hamilton
named quaternions. Although quaternions never had the inﬂuence Hamilton forecast
for them in physics, their noncommutative multiplication provided the ﬁrst signiﬁcant
example of a mathematical system which did not obey one of the standard arithmeti-
cal laws of operation and thus opened the way for more “freedom” in the creation of
mathematical systems. Among Hamilton’s other contributions was the development
of the Icosian game, a graph with 20 vertices on which pieces were to be placed in
accordance with various conditions, the overriding one being that a piece was always
placed at the second vertex of an edge on which the previous piece had been placed.
One of the problems Hamilton set for the game was, in essence, to discover a cyclic
path on his game board which passed through each vertex exactly once. Such a path
in a more general setting is today called a Hamilton circuit.
Richard W. Hamming (1915–1998) was born in Chicago and received a Ph.D. in
mathematics from the University of Illinois in 1942. He was the author of the ﬁrst
major paper on error-correcting and detecting codes (1950). His work on this problem
had been stimulated in 1947 when he was using an early Bell System relay computer
on weekends only.
During the weekends the machine was unattended and would
dump any work in which it discovered an error and proceed to the next problem.
Hamming realized that it would be worthwhile for the machine to be able not only
to detect an error but also to correct it, so that his jobs would in fact be completed.
In his paper, Hamming used a geometric model by considering an n-digit code word
to be a vertex in the unit cube in the n-dimensional vector space over the ﬁeld of two
elements. He was then able to show that the relationship between the word length
n and the number m of digits which carry the information was 2m ≤
2n
n+1. (The
remaining k = n −m digits are check digits which enable errors to be detected and
corrected.) In particular, Hamming presented a particular type of code, today known
as a Hamming code, with n = 7 and m = 4. In this code, the set of actual code
words of 4 digits was a 4-dimensional vector subspace of the 7-dimensional space of
all 7-digit binary strings.
Godfrey Harold Hardy (1877–1947) graduated from Trinity College, Cambridge in
1899.
From 1906 until 1919 he was lecturer at Trinity College, and, recognizing
the genius of Ramanujan, invited Ramanujan to Cambridge in 1914. Hardy held
the Sullivan chair of geometry at Oxford from 1919 until 1931, when he returned to
Cambridge, where he was Sadlerian professor of pure mathematics until 1942. He
developed the Hardy-Weinberg law which predicts patterns of inheritance. His main
areas of mathematical research were analysis and number theory, and he published

BIOGRAPHIES
1527
over 100 joint papers with Cambridge colleague John Littlewood. Hardy’s book A
Course in Pure Mathematics revolutionized mathematics teaching, and his book A
Mathematician’s Apology gives his view of what mathematics is and the value of its
study.
Ab¯u ’Al¯ı al-Hasan ibn al-Haytham (Alhazen) (965–1039) was one of the most
inﬂuential Islamic scientists. He was born in Basra (now in Iraq) but spent most of
his life in Egypt, after he was invited to work on a Nile control project. Although the
project, an early version of the Aswan dam project, never came to fruition, ibn al-
Haytham did produce in Egypt his most important scientiﬁc work, the Optics. This
work was translated into Latin in the early thirteenth century and was studied and
commented on in Europe for several centuries thereafter. Although there was much
mathematics in the Optics, ibn al-Haytham’s most interesting mathematical work
was the development of a recursive procedure for producing formulas for the sum of
any integral powers of the integers. Formulas for the sums of the integers, squares,
and cubes had long been known, but ibn al-Haytham gave a consistent method for
deriving these and used this to develop the formula for the sum of fourth powers.
Although his method was easily generalizable to the discovery of formulas for ﬁfth
and higher powers, he gave none, probably because he only needed the fourth power
rule in his computation of the volume of a paraboloid of revolution.
David Hilbert (1862–1943) was one of the last of the universal mathematicians, who
contributed greatly to many areas of mathematics. Hilbert spent the ﬁrst 33 years of
his life in and around K¨onigsberg, then capital of East Prussia, now in Russia. He
attended the university there and, after receiving his doctorate, joined the faculty
in 1885. He only rose to prominence, however, after he was called by Felix Klein
to G¨ottingen, where he soon became one of the major reasons for that university’s
taking over from Berlin as the preeminent university for mathematics in Germany,
and probably the world, through the ﬁrst third of the twentieth century. Hilbert
began his career with the study of algebraic forms, then turned to algebraic number
theory, the foundations of geometry, integral equations, theoretical physics, and ﬁnally
the foundations of mathematics. He is probably most famous for his lecture at the
International Congress of Mathematicians in Paris in 1900, where he presented a
list of 23 problems which he felt would be of central importance for mathematics
in the twentieth century. Hilbert ﬁrmly believed that it was problems that drove
mathematical progress and was always conﬁdent that, “wir mussen wissen, wir werden
wissen (we must know, we will know).” After the Nazi seizure of power, Hilbert was
forced to witness the demise of the G¨ottingen he knew and loved and died a lonely
man during the Second World War.
Hypatia (c. 370–415), the ﬁrst woman mathematician on record, lived in Alexandria.
She was given a very thorough education in mathematics and philosophy by her
father Theon and became a popular and respected teacher.
She was responsible
for detailed commentaries on several important Greek works, including Ptolemy’s
Almagest, Apollonius’ Conics, and Diophantus’ Arithmetica. Unfortunately, Hypatia
was caught up in the pagan-Christian turmoil of her times and was murdered by an
enraged mob.
Leonid Kantorovich (1912–1986) was a Soviet economist responsible for the develop-
ment of linear optimization techniques in relation to planning in the Soviet economy.
The starting point of this development was a set of problems posed by the Leningrad
timber trust at the beginning of 1938 to the Mathematics Faculty at the University
of Leningrad. Kantorovich explored these problems in his 1939 book Mathematical
Methods in the Organization and Planning of Production. He believed that one way

1528
BIOGRAPHIES
to increase productivity in a factory or an entire industrial organization was to im-
prove the distribution of the work among individual machines, the orders to various
suppliers, the diﬀerent kinds of raw materials, the diﬀerent types of fuels, and so
on. He was the ﬁrst to recognize that these problems could all be put into the same
mathematical language and that the resulting mathematical problems could be solved
numerically, but for various reasons his work was not pursued by Soviet economists
or mathematicians.
Ab¯u Bakr al-Karaj¯ı (died 1019) was an Islamic mathematician who worked in Bagh-
dad. In the ﬁrst decade of the eleventh century he composed a major work on algebra
entitled al-Fakhr¯ı (The Marvelous), in which he developed many algebraic techniques,
including the laws of exponents and the algebra of polynomials, with the aim of sys-
tematizing methods for solving equations. He was also one of the early originators
of a form of mathematical induction, which was best expressed in his proof of the
formula for the sum of integral cubes.
Muhammad ibn Muhammad al-Full¯ani al-Kishn¯aw¯ı (died 1741) was a native of
northern Nigeria and one of the few African black scholars known to have made
contributions to “pure” mathematics before the modern era.
Muhammad’s most
important work, available in an incomplete manuscript in the library of the School
of Oriental and African Studies in London, deals with the theory of magic squares.
He gave a clear treatment of the “standard” construction of magic squares and also
studied several other constructions — using knight’s moves, borders added to a magic
square of lower order, and the formation of a square from a square number of smaller
magic squares.
Stephen Cole Kleene (1909–1994) studied under Alonzo Church and received his
Ph.D. from Princeton in 1934. His research has included the study of recursive func-
tions, computability, decidability, and automata theory. In 1956 he proved Kleene’s
Theorem, in which he characterized the sets that can be recognized by ﬁnite-state
automata.
Felix Klein (1849–1925) received his doctorate at the University of Bonn in 1868. In
1872 he was appointed to a position at the University of Erlangen, and in his opening
address laid out the Erlanger Programm for the study of geometry based on the
structure of groups. He described diﬀerent geometries in terms of the properties of a
set that are invariant under a group of transformations on the set and gave a program
of study using this deﬁnition.
From 1875 until 1880 he taught at the Technische
Hochschule in Munich, and from 1880 until 1886 in Leipzig. In 1886 Klein became
head of the mathematics department at G¨ottingen and during his tenure raised the
prestige of the institution greatly.
Donald E. Knuth (born 1938) received a Ph.D. in 1963 from the California Institute
of Technology and held faculty positions at the California Institute of Technology
(1963–1968) and Stanford (1968–1992). He made contributions in many areas, in-
cluding the study of compilers and computational complexity. He is the designer of
the mathematical typesetting system TEX, and is the author of The Art of Computer
Programming, the ﬁrst comprehensive manual in the subject, published in many in-
stallments beginning in 1968. He received the Turing Award in 1974 and the National
Medal of Technology in 1979.
Kazimierz Kuratowski (1896–1980) was the son of a famous Warsaw lawyer who
became an active member of the Warsaw School of Mathematics after World War I.
He taught both at Lw´ow Polytechnical University and at Warsaw University until the
outbreak of World War II. During that war, because of the persecution of educated
Poles, he went into hiding under an assumed name and taught at the clandestine

BIOGRAPHIES
1529
Warsaw University. After the war, he helped to revive Polish mathematics, serving
as director of the Polish National Mathematics Institute. His major mathematical
contributions were in topology. In particular, he formulated a version of a maximal
principle equivalent to the axiom of choice. This principle is today known as Zorn’s
lemma. Kuratowski also contributed to the theory of graphs by proving in 1930 that
any non-planar graph must contain a copy of one of two particularly simple non-planar
graphs.
Joseph-Louis Lagrange (1736–1813) was born in Turin into a family of French descent.
He was attracted to mathematics in school and at the age of 19 became a mathematics
professor at the Royal Artillery School in Turin. At about the same time, having
read a paper of Euler’s on the calculus of variations, he wrote to Euler explaining
a better method he had recently discovered. Euler praised Lagrange and arranged
to present his paper to the Berlin Academy, to which he was later appointed when
Euler returned to Russia. Although most famous for his Analytical Mechanics, a
work which demonstrated how problems in mechanics can generally be reduced to
solutions of ordinary or partial diﬀerential equations, and for his Theory of Analytic
Functions, which attempted to reduce the ideas of calculus to those of algebraic
analysis, he also made contributions in other areas. For example, he undertook a
detailed review of solutions to quadratic, cubic, and quartic polynomials to see how
these methods might generalize to higher degree polynomials. He was led to consider
permutations on the roots of the equations and functions on the roots left unchanged
by such permutations. As part of this work, he discovered a version of Lagrange’s
Theorem to the eﬀect that the order of any subgroup of a group divides the order
of the group. Although he did not complete his program and produce a method of
solving higher degree polynomial equations, his methods were applied by others early
in the nineteenth century to show that such solutions were impossible.
Gabriel Lam´e (1795–1870) was educated at the ´Ecole Polytechnique and the ´Ecole des
Mines before going to Russia to direct the School of Highways and Transportation
in St. Petersburg. After his return to France in 1832, he taught at the ´Ecole Poly-
technique while also working as an engineering consultant. Lam´e contributed original
work to number theory, applied mathematics, and thermodynamics. His best-known
work is his proof of the case n = 5 of Fermat’s Last Theorem in 1839. Eight years
later, he announced that he had found a general proof of the theorem, which be-
gan with the factorization of the expression xn + yn over the complex numbers as
(x + y)(x + αy)(x + α2y) . . . (x + αn−1y), where α is a primitive root of xn −1 = 0.
He planned to show that the factors in this expression are all relatively prime and
therefore that if xn + yn = zn, then each of the factors would itself be an nth power.
He would then use the technique of inﬁnite descent to ﬁnd a solution in smaller num-
bers. Unfortunately Lam´e’s idea required that the ring of integers in the cyclotomic
ﬁeld of the nth roots of unity be a unique factorization domain. And, as Kummer
had already proved three years earlier, unique factorization in fact fails in many such
domains.
Edmund Landau (1877–1938) received a doctorate under Frobenius and taught at the
University of Berlin and at G¨ottingen. His research areas were analysis and analytic
number theory, including the distribution of primes. He used the big-O notation (also
called a Landau symbol) in his work to estimate the growth of various functions.
Pierre-Simon de Laplace (1749–1827) entered the University of Caen in 1766 to
begin preparation for a career in the church. He soon discovered his mathematical
talents, however, and in 1768 left for Paris to continue his studies. He later taught
mathematics at the ´Ecole Militaire to aspiring cadets. Legend has it that he examined,

1530
BIOGRAPHIES
and passed, Napoleon there in 1785. He was later honored by both Napoleon and King
Louis XVIII. Laplace is best known for his contributions to celestial mechanics, but
he was also one of the founders of probability theory and made many contributions to
mathematical statistics. In fact, he was one of the ﬁrst to apply his theoretical results
in statistics to a genuine problem in statistical inference, when he showed from the
surplus of male to female births in Paris over a 25-year period that it was “morally
certain” that the probability of a male birth was in fact greater than 1
2.
Gottfried Wilhelm Leibniz (1646–1716), born in Leipzig, developed his version of
the calculus some ten years after Isaac Newton, but published it much earlier. He
based his calculus on the inverse relationship of sums and diﬀerences, generalized to
inﬁnitesimal quantities called diﬀerentials. Leibniz hoped that his most original con-
tribution to philosophy would be the development of an alphabet of human thought,
a way of representing all fundamental concepts symbolically and a method of combin-
ing these symbols to represent more complex thoughts. Although he never completed
this project, his interest in ﬁnding appropriate symbols ultimately led him to the d
and
R
symbols for the calculus that are used today. Leibniz spent much of his life in
the diplomatic service of the Elector of Mainz and later was a Counsellor to the Duke
of Hanover. But he always found time to pursue his mathematical ideas and to carry
on a lively correspondence on the subject with colleagues all over Europe.
Levi ben Gerson (1288–1344) was a rabbi as well as an astronomer, philosopher, biblical
commentator, and mathematician. He lived in Orange, in southern France, but little
is known of his life.
His most famous mathematical work is the Maasei Hoshev
(The Art of the Calculator) (1321), which contains detailed proofs of the standard
combinatorial formulas, some of which use the principle of mathematical induction.
About a dozen copies of this medieval manuscript are extant, but it is not known
whether the work had any direct inﬂuence elsewhere in Europe. Also, at the request
of a French music theorist, Levi produced an elegant proof of the theorem that a
power of two and a power of three cannot be consecutive numbers, except for the
obvious cases of 2 and 3, 3 and 4, and 8 and 9.
L´aszl´o Lov´asz (born 1948) was so inspired as a high school student by Paul Erd˝os
that he competed in the International Mathematical Olympiad in 1964, 1965, and
1966, winning a gold medal each time. He published his ﬁrst paper, on graph theory,
when he was only seventeen, and has since gone on to a remarkable career in which
he has split time between Hungary and the United States. He has numerous results
in discrete mathematics to his credit, results that have applications in other areas
of mathematics as well as in computer science. He has developed new methods for
solving combinatorial problems, including those relying on geometric polyhedral and
topological techniques, thus showing that combinatorics deserves to be considered as
one of the major branches of mathematics. Besides authoring several important re-
search monographs, he has also written an elementary textbook introducing students
to the basic ideas of discrete mathematics. His work has won him numerous prizes,
and he served as president of the International Mathematical Union from 2007 to
2010.
Augusta Ada Byron King Lovelace (1815–1852) was the child of the famous poet
George Gordon, the sixth Lord Byron, who left England ﬁve weeks after his daugh-
ter’s birth and never saw her again. She was raised by her mother, Anna Isabella
Millbanke, a student of mathematics herself, so she received considerably more math-
ematics education than was usual for girls of her time. She was tutored privately by
well-known mathematicians, including William Frend and Augustus DeMorgan. Her
husband, the Earl of Lovelace, was made a Fellow of the Royal Society in 1840, and

BIOGRAPHIES
1531
through this connection, Ada was able to gain access to the books and papers she
needed to continue her mathematical studies and, in particular, to understand the
workings of Babbage’s Analytical Engine. Her major mathematical work is a heavily
annotated translation of a paper by the Italian mathematician L. F. Menabrea dealing
with the Engine, in which she gave explicit descriptions of how it would solve spe-
ciﬁc problems and described, for the ﬁrst time in print, what would today be called
a computer program, in this case a program for computing the Bernoulli numbers.
Interestingly, only her initials, A.A.L., were used in the published version of the pa-
per. It was evidently not considered proper in mid-nineteenth century England for a
woman of her class to publish a mathematical work.
Jan  Lukasiewicz (1878–1956) studied at the University of Lw´ow and taught at the
University of Lw´ow, the University of Warsaw, and the Royal Irish Academy.
A
logician, he worked in the area of many-valued logic, writing papers on three-valued
and m-valued logics, He is best known for the parenthesis-free notation he developed
for propositions, called Polish notation.
Percy Alexander MacMahon (1854–1929) was born into a British army family and
joined the army himself in 1871, reaching the rank of major in 1889. Much of his army
service was spent as an instructor at the Royal Military Academy. His early math-
ematical work dealt with invariants, following on the work of Cayley and Sylvester,
but a study of symmetric functions eventually led to his interest in partitions and to
his extension of the idea of a partition to higher dimensions. MacMahon’s two vol-
ume treatise Combinatorial Analysis (1915–16) is a classic in the ﬁeld. It identiﬁed
and clariﬁed the basic results of combinatorics and showed the way toward numerous
applications.
Mah¯av¯ıra (ninth century) was an Indian mathematician of the medieval period whose
major work, the Ganitas¯arasa¯ngraha, was a compilation of problems solvable by var-
ious algebraic techniques. For example, the work included a version of the hundred
fowls problem: “Doves are sold at the rate of 5 for 3 coins, cranes at the rate of 7
for 5, swans at the rate of 9 for 7, and peacocks at the rate of 3 for 9. A certain
man was told to bring at these rates 100 birds for 100 coins for the amusement of the
king’s son and was sent to do so. What amount does he give for each?” Mah¯av¯ıra
also presented, without proof and in words, the rule for calculating the number of
combinations of r objects out of a set of n. His algorithm can be easily translated into
the standard formula. Mah¯av¯ıra then applied the rule to two problems, one about
combinations of tastes and another about combinations of jewels on a necklace.
Andrei Markov (1856–1922) was a Russian mathematician who ﬁrst deﬁned what are
now called Markov chains in a paper of 1906 dealing with the Law of Large Numbers
and subsequently proved many of the standard results about them. His interest in
these chains stemmed from the needs of probability theory. Markov never dealt with
their application to the sciences, only considering examples from literary texts, where
the two possible states in the chain were vowels and consonants. Markov taught at
St. Petersburg University from 1880 to 1905 and contributed to such ﬁelds as number
theory, continued fractions, and approximation theory. He was an active participant
in the liberal movement in pre-World War I Russia and often criticized publicly the
actions of state authorities. In 1913, when as a member of the Academy of Sciences he
was asked to participate in the pompous ceremonies celebrating the 300th anniversary
of the Romanov dynasty, he instead organized a celebration of the 200th anniversary
of Jacob Bernoulli’s publication of the Law of Large Numbers.
Marin Mersenne (1588–1648) was educated in Jesuit schools and in 1611 joined the
Order of Minims. From 1619 he lived in the Minim Convent de l’Annonciade near the

1532
BIOGRAPHIES
Place Royale in Paris and there held regular meetings of a group of mathematicians
and scientists to discuss the latest ideas. Mersenne also served as the unoﬃcial “sec-
retary” of the republic of scientiﬁc letters in Europe. As such, he received material
from various sources, copied it, and distributed it widely, thus serving as a “walking
scientiﬁc journal”. His own contributions were primarily in the area of music theory
as detailed in his two great works on the subject, the Harmonie universelle and the
Harmonicorum libri, both of which appeared in 1636. As part of his study of music,
he developed the basic combinatorial formulas by considering the possible tunes one
could create out of a given number of notes. Mersenne was also greatly interested
in the relationship of theology to science. He was quite concerned when he learned
that Galileo could not publish one of his works because of the Inquisition and, in fact,
oﬀered his assistance in this matter.
Hermann Minkowski (1864–1909) was a German mathematician who received his doc-
torate at the University of K¨onigsberg. He became a lifelong friend of David Hilbert
and, on Hilbert’s suggestion, was called to G¨ottingen in 1902. In 1883, he shared the
prize of the Paris Academy of Sciences for his essay on the topic of the representations
of an integer as a sum of squares. In his essay, he reconstructed the entire theory
of quadratic forms in n variables with integral coeﬃcients. In further work on num-
ber theory, he brought to bear geometric ideas beginning with the realization that a
symmetric convex body in n-space deﬁnes a notion of distance and hence a geometry
in that space. The connection with number theory depends on the representation of
forms by lattice points in space.
Ah. mad ibn Mun’im al-’Abdari (died 1228) was born in Andalusia, but then lived and
taught in Marrakesh, probably at the Almohade court during the reign of Muh.ammad
al-N¯as.ir. He is best known for his work On the Science of Calculation, which contains
a long section titled “On denumerating the Words Such That a Person Cannot Express
Himself without One of Them.” That is, he attempted to count all possible words that
can be formed out of the letters, including vowels, of the Arabic alphabet. He used
various counting techniques, including what is now called the Pascal triangle, to show
how this problem can be attacked. Although the question as posed cannot be answered
in full, ibn Mun’im did give methods for ﬁnding partial answers. For example, he
showed that the number of possible words of nine letters of which two are not repeated,
two are repeated twice, and one is repeated three times is 5,968,924,232,544,000. In
another section of his work, he discussed ﬁgured numbers, showing how they can be
generated recursively and giving a rule for calculating the sum of consecutive ﬁgured
numbers of a given type.
John F. Nash, Jr. (1928–2015) shared the 1994 Nobel Prize in Economics (together
with Reinhard Selten and John Harsanyi) for fundamental contributions to game the-
ory and its applications to economics. Nash earned his Ph.D. at Princeton University
with a dissertation deﬁning and explaining what became known as the Nash equilib-
rium in non-cooperative games. A group of players are in the Nash equilibrium if each
one has chosen a strategy and no one can beneﬁt by changing strategies, provided that
all the other players keep their strategies unchanged. Nash proved that such an equi-
librium exists under many game-theoretic conditions. As detailed in Sylvia Nasar’s
biography of Nash, “A Beautiful Mind”, he suﬀered from paranoid schizophrenia for
much of his adult life. He still managed to make major contributions, not only to
game theory but to diﬀerential geometry and partial diﬀerential equations. In fact, in
2015 he shared the Abel Prize with Louis Nirenberg for his work on nonlinear partial
diﬀerential equations. He was killed in an automobile accident on his way home from
Norway, where he had received the award.

BIOGRAPHIES
1533
Peter Naur (1928–2016), born in Denmark, was originally an astronomer, using com-
puters to calculate planetary motion, but in 1959 he became a full-time computer
scientist. He was a developer of the programming language ALGOL and worked on
compilers for ALGOL and COBOL. In 1969 he took a computer science faculty po-
sition at the University of Copenhagen, from which he retired in 1998. He won the
Turing award in 2005 “for fundamental contributions to programming language de-
sign and the deﬁnition of ALGOL 60, to compiler design, and to the art and practice
of computer programming.”
Amalie Emmy Noether (1882–1935) received her doctorate from the University of
Erlangen in 1908 and a few years later moved to G¨ottingen to assist Hilbert in the
study of general relativity. During her eighteen years there, she was extremely in-
ﬂuential in stimulating a new style of thinking in algebra by always emphasizing its
structural rather than computational aspects. She was forced to leave Germany in
the 1930s by the Nazis, but in 1934, she became a professor at Bryn Mawr College
and a member of the Institute for Advanced Study. She is most famous for her work
on Noetherian rings, and her inﬂuence is still evident in today’s textbooks in abstract
algebra.
Blaise Pascal (1623–1662) showed his mathematical precocity with his Essay on Conics
of 1640, in which he stated his theorem that the opposite sides of a hexagon inscribed
in a conic section always intersect in three collinear points. Pascal is better known,
however, for his detailed study of what is now called Pascal’s triangle of binomial
coeﬃcients. In that study Pascal gave an explicit description of mathematical induc-
tion and used that method, although not quite in the modern sense, to prove various
properties of the numbers in the triangle, including a method of determining the ap-
propriate division of stakes in a game interrupted before its conclusion. Pascal had
earlier discussed this matter, along with various other ideas in the theory of prob-
ability, in correspondence with Fermat in the 1650s. These letters, in fact, can be
considered the beginning of the mathematization of probability.
Giuseppe Peano (1858–1932) studied at the University of Turin and then spent the
remainder of his life there as a professor of mathematics. He was originally known as
an inspiring teacher, but as his studies turned to symbolic logic and the foundations of
mathematics and he attempted to introduce some of these notions in his elementary
classes, his teaching reputation changed for the worse. Peano is best known for his
axioms for the natural numbers, ﬁrst proposed in the Arithmetices principia, nova
methodo exposita of 1889. One of these axioms describes the principle of mathemati-
cal induction. Peano was also among the ﬁrst to present an axiomatic description of a
(ﬁnite-dimensional) vector space. In his Calcolo geometrico of 1888, Peano described
what he called a linear system, a set of quantities provided with the operations of
addition and scalar multiplication which satisfy the standard properties. He was then
able to give a coherent deﬁnition of the dimension of a linear system as the maximum
number of linearly independent quantities in the system.
Charles Sanders Peirce (1839–1914) was born in Massachusetts, the son of a Harvard
mathematics professor. He received a master’s degree from Harvard in 1862 and an
advanced degree in chemistry from the Lawrence Scientiﬁc School in 1863. He made
contributions to many areas of the foundations and philosophy of mathematics. He
was a proliﬁc writer, leaving over 100,000 pages of unpublished manuscript at his
death.
George P´olya (1887–1985) was a Hungarian mathematician who received his doctorate
at Budapest in 1912. From 1914 to 1940 he taught in Zurich, then emigrated to
the United States where he spent most of the rest of his professional life at Stanford

1534
BIOGRAPHIES
University. P´olya developed some inﬂuential enumeration ideas in several papers in
the 1930s, in particular dealing with the counting of certain conﬁgurations that are
not equivalent under the action of a particular permutation group. For example, there
are 16 ways in which one can color the vertices of a square using two colors, but only
six are non-equivalent under the various symmetries of the square. In 1937, P´olya
published a major article in the ﬁeld, “Combinatorial Enumeration of Groups, Graphs
and Chemical Compounds”, in which he discussed many mathematical aspects of the
theory of enumeration and applied it to various problems. P´olya’s work on problem
solving and heuristics, summarized in his two volume work Mathematics and Plausible
Reasoning, insured his fame as a mathematics educator; his ideas are at the forefront
of recent reforms in mathematics education at all levels.
Qin Jiushao (1202–1261), born in Sichuan, published a general procedure for solving
systems of linear congruences — the Chinese remainder theorem — in his Shushu
jiuzhang (Mathematical Treatise in Nine Sections) in 1247, a procedure which makes
essential use of the Euclidean algorithm. He also gave a complete description of a
method for numerically solving polynomial equations of any degree. Qin’s method
had been developed in China over a period of more than a thousand years; it is
similar to a method used in the Islamic world and is closely related to what is now
called the Horner method of solution, published by William Horner in 1819. Qin
studied mathematics at the Board of Astronomy, the Chinese agency responsible
for calendrical computations. He later served the government in several oﬃces, but
because he was “extravagant and boastful”, he was several times relieved of his duties
because of corruption. These ﬁrings notwithstanding, Qin became a wealthy man and
developed an impressive reputation in love aﬀairs.
Willard Van Orman Quine (1908–2000) was a logician and philosopher who received
his Ph.D. from Harvard in 1932 and remained with the university in various positions
for the remainder of his life. He was a chief proponent of the view that philosophy
is not conceptual analysis but is an abstract branch of the empirical sciences. While
he wrote many expository works in logic, his most innovative work was in set theory,
where at diﬀerent times in his life he proposed three variants of axiomatic set theory.
He also made many contributions to symbolic logic, where some of his results found
applications in computer science.
Richard Rado (1906–1989) left Germany for Great Britain in 1933, once the racial laws
were promulgated by the Nazis. He had already received a Ph.D. from the University
of Berlin, but then received a second from Cambridge University two years later.
Rado’s most important contributions were in combinatorics, and he wrote many joint
papers in the ﬁeld with Paul Erd˝os. In particular, the two of them developed the
partition calculus, which generalizes the following example: If six people meet by
chance, then either at least three all know each other or there are at least three with
no two knowing each other. Rado spent his academic career at three universities in
England: the University of Sheﬃeld, King’s College, London, and the University of
Reading, from which he retired.
Srinivasa Ramanujan (1887–1920) was born near Madras into the family of a book-
keeper. He studied mathematics on his own and soon began producing results in
combinatorial analysis, some already known and others previously unknown. At the
urging of friends, he sent some of his results to G. H. Hardy in England, who quickly
recognized Ramanujan’s genius and invited him to England to develop his untrained
mathematical talent. During the war years from 1914 to 1917, Hardy and Ramanu-
jan collaborated on a number of papers, including several dealing with the theory
of partitions. Unfortunately, Ramanujan fell ill during his years in the unfamiliar

BIOGRAPHIES
1535
climate of England and died at age 32 soon after returning to India. Ramanujan left
behind several notebooks containing statements of thousands of results, enough work
to keep many mathematicians occupied for years in understanding and proving them.
The 2015 ﬁlm “The Man Who Knew Inﬁnity” depicts his time at Trinity College,
Cambridge, and his mathematical collaborations with Hardy.
Frank Ramsey (1903–1930), son of the president of Magdalene College, Cambridge,
was educated at Winchester and Trinity Colleges. He was then elected a fellow of
King’s College, where he spent the remainder of his life. Ramsey made important
contributions to mathematical logic. What is now called Ramsey theory began with
his clever combinatorial arguments to prove a generalization of the pigeonhole prin-
ciple, published in the paper “On a Problem of Formal Logic”. The problem of that
paper was the Entscheidungsproblem (the decision problem), the problem of search-
ing for a general method of determining the consistency of a logical formula. Ramsey
also made contributions to the mathematical theory of economics and introduced the
subjective interpretation to probability. In that interpretation, Ramsey argues that
diﬀerent people when presented with the same evidence, will have diﬀerent degrees
of belief. And the way to measure a person’s belief is to propose a bet and see what
are the lowest odds the person will accept. Ramsey’s death at the age of 26 deprived
the mathematical community of a brilliant young scholar.
Ronald Rivest (born 1947) is a cryptographer and professor at MIT. He received his
Ph.D. in computer science from Stanford University in 1974 and three years later,
along with Adi Shamir and Len Adleman, invented the RSA public key algorithm, now
commonly used for secure data transmission. The algorithm is based on the diﬃculty
of factoring the product of two large prime numbers. (The algorithm had been earlier
invented by Cliﬀord Cocks, but had not been published because it was classiﬁed.)
According to legend, the algorithm was invented after the three had consumed much
wine during a Passover seder at the home of a student. Rivest has invented other
symmetric key encryption algorithms as well.
Alvin Roth (born 1951) won the Nobel Prize in Economics in 2012 (together with
Lloyd Shapley) for his work in market design. One of his major achievements was
demonstrating, in 1984, that the National Resident Matching Program, which matches
new medical doctors with hospital residency programs, was stable and strategy-proof.
His work was based on the theoretical foundations introduced by David Gale and
Lloyd Shapley in 1962. He later worked with the New York public school system
to redesign its matching programs for students entering high school, as well as with
the Boston public school system for its matching of primary school students. More
recently, he applied the ideas of stable matchings to the problem of paired kidney
exchanges.
Roth received his Ph.D. from Stanford University and is currently a
faculty member at the same institution.
Bertrand Arthur William Russell (1872–1970) was born in Wales and studied at
Trinity College, Cambridge. A philosopher/mathematician, he is one of the founders
of modern logic and wrote over 40 books in diﬀerent areas.
In his most famous
work, Principia Mathematica, published in 1910–13 with Alfred North Whitehead,
he attempted to deduce the entire body of mathematics from a single set of primitive
axioms. A paciﬁst, he fought for progressive causes, including women’s suﬀrage in
Great Britain and nuclear disarmament. In 1950 he won a Nobel Prize for literature.
al-Samaw’al ibn Yahy¯a ibn Yah¯uda al-Maghrib¯ı (1125–1180) was born in Bagh-
dad to well-educated Jewish parents. Besides giving him a religious education, they
encouraged him to study medicine and mathematics. He wrote his major mathemat-
ical work, Al-B¯ahir (The Shining), an algebra text that dealt extensively with the

1536
BIOGRAPHIES
algebra of polynomials, at the age of 19. In it, al-Samaw’al worked out the laws of
exponents, both positive and negative, and showed how to divide polynomials even
when the division was not exact. He also used a form of mathematical induction to
prove the binomial theorem, that (a + b)n = Pn
k=0 C(n, k)an−kbk, where the C(n, k)
are the entries in the Pascal triangle, for n ≤12. In fact, he showed why each entry
in the triangle can be formed by adding two numbers in the previous row. When al-
Samaw’al was about 40, he decided to convert to Islam. To justify his conversion to
the world, he wrote an autobiography in 1167 stating his arguments against Judaism,
a work which became famous as a source of Islamic polemics against the Jews.
Issai Schur (1875–1941) was born in the Russian Empire but spent most of his academic
life in Germany. He received his doctorate from the University of Berlin in 1901 and
was a professor there from 1919 until he was forced to retire by the Nazis in 1935.
In fact, he led a well-known school of mathematicians at Berlin working in group
representations as well as some related ones. He is best known for his work in that
ﬁeld, but he also contributed signiﬁcantly to combinatorics and number theory. In
particular, when he realized in the 1920s that group representations were important
for theoretical physics, he returned to their study with renewed energy and eventually
gave a complete description of the rational representations of the general linear group.
Unfortunately, after he was dismissed from his professorship, he declined some invi-
tations to go abroad. Eventually, all avenues of work were closed to him in Germany.
He was forced to relinquish membership in the Prussian Academy of Science. Finally,
in 1939, he left Germany for Palestine where he died two years later.
Adi Shamir (born 1952) is an Israeli cryptographer who received his Ph.D. in com-
puter science from the Weizmann Institute in 1977. He is the inventor (along with
Ronald Rivest and Len Adleman) of the RSA public key algorithm, now used in se-
cure data transmission. He has made numerous other contributions to cryptography
and computer science, including the independent discovery of diﬀerential cryptanaly-
sis (previously known to the U.S. National Security Agency). He is currently on the
faculty both of the Weizmann Institute and the ´Ecole Normale Sup´erieure in Paris.
Claude Elwood Shannon (1916–2001) applied Boolean algebra to switching circuits
in his master’s thesis at M.I.T in 1938. Shannon realized that a circuit can be rep-
resented by a set of equations and that the calculus necessary for manipulating these
equations is precisely the Boolean algebra of logic. Simplifying these equations for
a circuit would yield a simpler, equivalent circuit. Switches in Shannon’s calculus
were either open (represented by 1) or closed (represented by 0); placing switches in
parallel was represented by the Boolean operation “+”, while placing them in parallel
was represented by “ · ”. Using the basic rules of Boolean algebra, Shannon was, for
example, able to construct a circuit which would add two numbers given in binary
representation. He received his Ph.D. in mathematics from M.I.T. in 1940 and spent
much of his professional life at Bell Laboratories, where he worked on methods of
transmitting data eﬃciently and made many fundamental contributions to informa-
tion theory.
Lloyd Shapley (1923–2016) received the Nobel Prize in Economics (together with Alvin
Roth) in 2012 after a long career in game theory and mathematical economics. He
served in the U.S. Army Air Corps in World War II, after which he ﬁnished his
education with a Ph.D. from Princeton in 1953. He was a professor at the University of
California, Los Angeles from 1981, where he had a joint appointment in mathematics
and economics. His Nobel Prize was awarded ”for the theory of stable allocations and
the practice of market design.” In particular, he used Cooperative Game Theory to
study methods to pair diﬀerent players in a game in a stable matching, one in which

BIOGRAPHIES
1537
no two players would prefer one another over their current counterparts.
Emanuel Sperner (1905–1980) was a German mathematician who received his doctor-
ate in 1928 from the University of Hamburg. He is famous for several results in set
theory discovered when he was a young man. Sperner’s Theorem, published in 1928,
describes the largest possible collection of subsets of a given set with the property
that no subset in the collection contains any other subset in the collection. The result
known as “Sperner’s Lemma,” proved about the same time, deals with the labelings
of vertices in a special type of graph. Surprisingly, the lemma is crucial to an ele-
gant proof of the Brouwer Fixed Point Theorem and has numerous other applications
in topology. Sperner is also famous for his 1951 textbook Introduction to Modern
Algebra and Matrix Theory (with Otto Schreier), in which fundamental concepts in
algebra and in aﬃne and projective geometry are treated simultaneously. Sperner
became secretary of the German Mathematical Society in 1935, then resigned that
position to become editor of the Jahresbericht of the Society until its closure at the
end of 1943. He was a member of the board of the Society which, in 1938, wrote to
all remaining Jewish members asking them to resign.
James Stirling (1692–1770) studied at Glasgow University and at Balliol College, Ox-
ford and spent much of his life as a successful administrator of a mining company in
Scotland. His mathematical work included an exposition of Newton’s theory of cubic
curves and a 1730 book entitled Methodus Diﬀerentialis which dealt with summation
and interpolation formulas. In dealing with the convergence of series, Stirling found
it useful to convert factorials into powers. By considering tables of factorials, he was
able to derive the formula for log n!, which leads to what is now known as Stirling’s
approximation: n! ≈( n
e )n√
2πn. Stirling also developed the Stirling numbers of the
ﬁrst and second kinds, sequences of numbers important in enumeration.
Sun Zi (4th century) is the author of Sunzi suanjing (Master Sun’s Mathematical Man-
ual), a manual on arithmetical operations which eventually became part of the re-
quired course of study for Chinese civil servants. The most famous problem in the
work is one of the ﬁrst examples of what is today called the Chinese remainder prob-
lem: “We have things of which we do not know the number; if we count them by
threes, the remainder is 2; if we count them by ﬁves, the remainder is 3; if we count
them by sevens, the remainder is 2. How many things are there?” Sun Zi gives the
answer, 23, along with some explanation of how the problem should be solved. But
since this is the only problem of its type in the book, it is not known whether Sun Zi
had developed a general method of solving simultaneous linear congruences.
James Joseph Sylvester (1814–1897), who was born into a Jewish family in London
and studied for several years at Cambridge, was not permitted to take his degree
there for religious reasons. Therefore, he received his degree from Trinity College,
Dublin and soon thereafter accepted a professorship at the University of Virginia.
His horror of slavery, however, and an altercation with a student who did not show
him the respect he felt he deserved led to his resignation after only a brief tenure.
After his return to England, he spent 10 years as an attorney and 15 years as professor
of mathematics at the Royal Military Academy at Woolwich. Sylvester returned to
the United States in 1871 to accept the chair of mathematics at the newly opened
Johns Hopkins University in Baltimore, where he founded the American Journal of
Mathematics and helped initiate a tradition of graduate education in mathematics in
the United States. Sylvester’s primary mathematical contributions are in the ﬁelds
of invariant theory and the theory of partitions.
Terence Tao (born 1975) is the youngest participant to date in the International Math-
ematical Olympiad, where he ﬁrst competed on the Australian team at the age of

1538
BIOGRAPHIES
ten and where he won a gold medal at thirteen. He received his Ph.D. from Prince-
ton University at the age of 20 and then joined the faculty at UCLA, where he was
promoted to full professor at the age of 24. In 2006, he won the Fields Medal “for
his contributions to partial diﬀerential equations, combinatorics, harmonic analysis
and additive number theory.” Among his many accomplishments are his proof (with
Ben Green) that it is always possible to ﬁnd a progression of prime numbers of equal
spacing and any length, and his new method of attack, in 2014, on the Navier-Stokes
Millennium Problem.
Th¯abit ibn Qurra (836–901) was born in the south of what is now Turkey. His math-
ematical abilities were discovered by scholars from Baghdad, and so he was invited
to the capital of the caliphate where he eventually became court astronomer. He
translated many Greek mathematical works into Arabic and also produced an alge-
bra text in which justiﬁcations of the algorithms for solving quadratic equations were
based on theorems of Euclid’s Book II. His major mathematical work, however, was
the statement and proof of a result producing amicable numbers, a result that was
transmitted to Europe via several Hebrew mathematical texts.
John Wilder Tukey (1915–2000) received a Ph.D. in topology from Princeton in 1939.
After World War II he returned to Princeton as professor of statistics, where he
founded the Department of Statistics in 1966. His work in statistics included the
areas of spectra of time series and analysis of variance.
He invented (with J. W.
Cooley) the fast Fourier transform. He was awarded the National Medal of Science
and served on the President’s Science Advisory Committee. He also coined the word
“bit” for a binary digit.
Alan Turing (1912–1954) studied mathematics at King’s College, Cambridge and in
1936 invented the concept of a Turing machine to answer the questions of what a
computation is and whether a given computation can in fact be carried out. This
notion today lies at the basis of the modern all-purpose computer, a machine which
can be programmed to do any desired computation. At the outbreak of World War II,
Turing was called to serve at the Government Code and Cypher School in Bletchley
Park in Buckinghamshire. It was there, during the next few years, that he led the
successful eﬀort to crack the German “Enigma” code, an eﬀort which turned out to
be central to the defeat of Nazi Germany. After the war, Turing continued his interest
in automatic computing machines and so joined the National Physical Laboratory to
work on the design of a computer, continuing this work after 1948 at the University of
Manchester. Turing’s promising career came to a grinding halt, however, when he was
arrested in 1952 for homosexual acts. The penalty for this “crime” was submission
to psychoanalysis and hormone treatments to “cure” the disease. Unfortunately, the
cure proved worse than the disease, and, in a ﬁt of depression, Turing committed
suicide in June, 1954.
Alexandre-Th´eophile Vandermonde (1735–1796) was directed by his physician fa-
ther to a career in music. However, he later developed a brief but intense interest
in mathematics and wrote four important papers published in 1771 and 1772. These
papers include fundamental contributions to the theory of the roots of equations, the
theory of determinants, and the knight’s tour problem. In the ﬁrst paper, he showed
that any symmetric function of the roots of a polynomial equation can be expressed
in terms of the coeﬃcients of the equation. His paper on determinants was the ﬁrst
logical, connected exposition of the subject, so he can be thought of as the founder of
the theory. Toward the end of his life, he joined the cause of the French Revolution
and held several diﬀerent positions in government.
Fran¸cois Vi`ete (1540–1603), a lawyer and advisor to two kings of France, was one

BIOGRAPHIES
1539
of the earliest cryptanalysts and successfully decoded intercepted messages for his
patrons. In fact, he was so successful in this endeavor that he was denounced by
some who thought that the decipherment could only have been made by sorcery.
Although a mathematician only by avocation, he made important contributions to
the development of algebra. In particular, he introduced letters to stand for numerical
constants, thus enabling him to break away from the style of verbal algorithms of his
predecessors and treat general examples by formulas rather than by giving rules for
speciﬁc problems.
Edward Waring (1734–1798) graduated from Magdalen College, Cambridge in 1757
with highest honors and shortly thereafter was named a Fellow of the University. In
1760, despite opposition because of his youth, he was named Lucasian Professor of
Mathematics at Cambridge, a position he held until his death. To help solidify his
position, then, he published the ﬁrst chapter of his major work, Miscellanea analyt-
ica, which in later editions was renamed Meditationes algebraicae. Waring is best
remembered for his conjecture that every integer is the sum of at most four squares,
at most nine cubes, at most 19 fourth powers, and, in general, at most r kth powers,
where r depends on k. The general theorem that there is a ﬁnite r for each k was
proved by Hilbert in 1909. Although the result for squares was proved by Lagrange,
the speciﬁc results for cubes and fourth powers were not proved until the twentieth
century.
Andrew Viterbi (born 1935) is an electrical engineer who invented the Viterbi algo-
rithm. This is a dynamic programming algorithm for ﬁnding the most likely sequence
of hidden states that results in a sequence of observed events. The algorithm is used
in cell phones, speech recognition, DNA analysis and other applications. Viterbi im-
migrated to the United States with his parents in 1939 because of the Italian racial
laws restricting the civil rights of Jews. He received his Ph.D. from the University of
Southern California. Besides serving as a professor of electrical engineering at both
the University of California, Los Angeles and the University of California, San Diego,
he was the co-founder of Qualcomm Inc., a semiconductor and telecommunications
equipment company.
Hassler Whitney (1907–1989) received bachelor’s degrees in both physics and music
from Yale; in 1932 he received a doctorate in mathematics from Harvard. After a
brief stay in Princeton, he returned to Harvard, where he taught until 1952, when he
moved to the Institute for Advanced Study. Whitney produced more than a dozen
papers on graph theory in the 1930s, after his interest was aroused by the four color
problem. In particular, he deﬁned the notion of the dual graph of a map. It was
then possible to apply many of the results of the theory of graphs to gain insight into
the four color problem. During the last twenty years of his life, Whitney devoted his
energy to improving mathematical education, particularly at the elementary school
level. He emphasized that young children should be encouraged to solve problems
using their intuition, rather than only be taught techniques and results which have
no connection to their experience.
Andrew Wiles (born 1953) is best known for his proof of Fermat’s Last Theorem. Born
in England, he ﬁrst learned of the theorem when he was ten and vowed to be the ﬁrst
person to prove it. After receiving his Ph.D. from Cambridge, he spent time at the
Institute for Advanced Study in Princeton and then joined the Princeton mathematics
faculty, where he remained until he took a research professorship at Oxford in 2011.
After he announced his proof in 1993 at a series of lectures at Cambridge University,
the review process found a ﬂaw in the proof. Fortunately, with the help of Richard
Taylor, Wiles corrected the ﬂaw the following year. The 1995 issue of the Annals

1540
BIOGRAPHIES
of Mathematics contained the complete proof. The many new ideas in Wiles’s work
has led to many other new results in number theory. Wiles himself received the Abel
prize in 2016 in recognition of his achievement.
REFERENCES
Printed Resources:
Dictionary of Scientiﬁc Biography, Macmillan, 1998.
D. M. Burton, The History of Mathematics, An Introduction, 3rd ed., McGraw-Hill, 1996.
H. Eves, An Introduction to the History of Mathematics, 6th ed., Saunders, 1990.
H. Eves, Great Moments in Mathematics (Before 1650), Dolciani Mathematical Exposi-
tions, No. 5, Mathematical Association of America, 1983.
H. Eves, Great Moments in Mathematics (After 1650), Dolciani Mathematical Exposi-
tions, No. 7, Mathematical Association of America, 1983.
V. J. Katz, History of Mathematics, an Introduction, 3rd ed., Addison-Wesley, 2008.
V. J. Katz, M. Folkerts, B. Hughes, R. Wagner, and J. L. Berggren, eds., Sourcebook in
the Mathematics of Medieval Europe and North Africa, Princeton University Press,
2016.
V. J. Katz, A. Imhausen, E. Robson, J. W. Dauben, K. Plofker, and J. L. Berggren, eds.,
The Mathematics of Egypt, Mesopotamia, China, India, and Islam: A Sourcebook,
Princeton University Press, 2007.
Web Resources:
http://turnbull.mcs.st-and.ac.uk/~history/
(The MacTutor History of Mathe-
matics archive.)

INDEX
0-1 IP problem, 1169
1-4 tree, 715
1-center algorithm, 1179
1-factor, 580
1-factorization, 576, 623
1-median algorithm, 1179
1-skeleton, 580, 593
1-tough, 620
2-3 tree, 1329, 1359
2-3 tree search algorithm, 1360
3D printing, 1004
4-point lemma, 860
A5/1 cipher, 1095
[a, b]-factor, 623
Abel’s transformation, 199
Abel, Niels Henrik, 1515
abelian group, 324, 331
abelian square, 1266, 1300
absorbing boundary, 476, 502, 507
absorption property, 324
abstract datatype, 1324, 1331
abundant integer, 267
acceptance probability, 1266, 1312
accepted string, 1272, 1276, 1290
access, 382
accessibility, 1476
accessible state, 522
Ackermann function, 37, 1266, 1288
Ackermann, Wilhelm, 37
action, 2, 78
action potential, 1484
active adversary, 1070, 1078
active constraint, 1144, 1149
active security, 1116
active vertex, 762
activity net, 1195
activity-on-arc representation, 1195
activity-on-node representation, 1195
acyclic central plane arrangement, 939
acyclic digraph, 570
acyclic graph, 570, 609, 696
acyclic vector conﬁguration, 936
adaptive bubblesort, 1324
adaptive chosen-ciphertext attack security,
1100
adaptive sort, 1352
additive group, 331
additive manufacturing, 1004
additively homomorphic, 1118
adjacency matrix, 382, 448, 570, 571, 597,
602, 670, 722, 826
adjacency set, 722, 749, 826
adjoint, 382
Adleman, Leonard, 260, 1100, 1515
admissible arc, 762
admittance matrix, 571
ADT-constructor, 1324, 1331
advanced encryption standard (AES), 1070,
1084, 1088
adversary, 1078
AES cipher, 1088
AES cipher algorithm, 1088
aﬃne chirotope, 947
aﬃne cipher, 1070, 1080
aﬃne combination, 934
aﬃne conﬁguration, 936
aﬃne dependence, 935
aﬃne gap score, 1436
aﬃne hyperplane, 935
aﬃne independence, 934
aﬃne plane, 389, 880, 887, 897, 898, 900
aﬃne polar-duality, 940
aﬃne space, 880, 896, 935
aftpart, 1337
AG code, 1024, 1064
agglomerative hierarchical algorithm, 1399
agglomerative hierarchical clustering, 1376,
1398
Al-Kindi, 1080
Alberti, Leon Battista, 1080
aleph-null, 2
Alexandroﬀinequality, 455
Alford, W. R., 259
algebraic attack on stream ciphers, 1094
algebraic element, 354
algebraic extension, 324, 358
algebraic immunity, 1094
algebraic integer, 218, 302, 324, 358
algebraic multiplicity, 382, 435
algebraic number, 218, 223, 302, 324, 358
algebraic speciﬁcation, 571, 673
algebraic structure, 324, 329
1541

1542
INDEX
algorithm
1-center, 1179
1-median, 1179
2-3 tree search, 1360
agglomerative hierarchical, 1399
all-pairs shortest path, 754
all-terminal reliability, 516
augmenting path, 762
back substitution, 421
backward (HMM), 531
Baum-Welch, 535
bin packing, 1190
binary search, 1360
binary search tree, 1343
binomial coeﬃcients, 497
binomial probabilities, 500, 501
bipartite matching, 737
branch-and-bound, 1171
breadth-ﬁrst search, 707
bubblesort, 1353
BWT, 1443
BWT query match, 1443
BWT reversal, 1443
constructing parity check matrix, 1037
continued fraction, 297
convex hull, 993
cutting plane, 1171
cycle-canceling, 772
de Bruijn sequence, 160
decision tree learning, 1390
decoding BCH codes, 1049
decoding Hamming codes, 1041
decoding Reed-Muller codes, 1042
decoding using burst error correction,
1046
deheaping, 1345
depth-ﬁrst search, 706
Dijkstra, 751
distinguished triangle, 1218
divisive hierarchical, 1403
dual simplex, 1163
edge coloring, 635
enheaping, 1344
Euclidean, 233
Euler cycle, 1473
Euler tour, 618, 619
extended Euclidean, 234
ﬁxed point of simplex, 1221
Fleury, 619
Floyd-Warshall, 755
forward (HMM), 531
forward substitution, 421
Gale-Shapley, 744
Gaussian elimination, 423
general matching, 741
global pairwise alignment, 1431
Golub-Kahan bidiagonalization, 461
Gosper, 211
Graham Scan, 975
Gram-Schmidt orthogonalization, 396
greedy matroid, 921
Greene-Nijenhuis-Wilf, 135
heapsort, 1353
Huﬀman tree, 700
hyperplane intersection, 995
insertion sort, 1351
inverse transformation, 545
isomorphism testing, 628
Jacobi method, 441
Jarvis’ March, 975
Karmarkar, 1160
Karnaugh map, 375
Kirkpatrick-Siedel, 976
KMP, 1440
knapsack, 1186, 1187
Kruskal, 728
kth smallest element, 1361
label-correcting, 750
least common ancestor, 830
LU decomposition, 425
Lucas-Lehmer, 256
mergesort, 1356
Miller-Rabin, 260
minimum spanning tree update, 1368
mth smallest key, 1318
neighbor joining, 1450
neighborhood search, 1233
network simplex, 774
orthogonal range reporting, 988
parent-ﬁnder, 704
permutations, 108
planarity testing, 650
point location, 985
Pollard, 263
power method, 440
preﬂow-push, 763
Prim, 729
primality test, 1318
QR method, 440
quadratic sieve, 262
QuickHull, 975
quicksort, 1356

INDEX
1543
Quine-McCluskey, 377
radix sort, 1358
random graph generation, 683
randomized quicksort, 1306
range tree, 987
ray shooting, 991
Robinson-Schensted, 136
selection sort, 1351
set covering, 1192
sieve of Eratosthenes, 251
simplex, 1156, 1157
Sister Celine, 210
Sollin, 729
spectral clustering, 1413
Strassen, 413
strong orientation, 607, 608
strong probable prime test, 260
successive shortest path, 773
syndrome decoding, 1039
tabu search, 1242, 1246
topological sort, 609, 874
trial division, 261
triangulation hierarchy, 984
two-sided Jacobi, 462
two-terminal reliability, 516
variable-depth search, 1235
visibility map, 991
Viterbi, 533, 1060
Voronoi diagram, 979
Warshall, 45
weighted bipartite matching, 738
algorithm analysis, 1266
algorithm complexity, 1267
alignment, 1426
alignment score, 1426, 1430
all-pairs shortest path algorithm, 754
all-terminal network, 512
all-terminal reliability, 476, 515
all-terminal reliability algorithm, 516
allele, 1463
alpha-shapes, 1001
alphabet, 1029, 1031, 1266, 1275, 1278,
1280, 1290, 1429
alphabetic datatype, 1324, 1331
alphastring, 1334
alternating function, 946
alternating group, 324, 333, 345
alternating k-cycle, 872
alternating least squares, 465
alternating path, 722, 734
ambiguous context-free grammar, 1266,
1298
ambivalent data structure, 1324, 1367
amicable integers, 267
amino acid chain, 1429
analog channel, 1024, 1027
ancestor, 694, 698
AND, 2
angle, 395
anomaly, 1376, 1379
anomaly detection, 1376, 1377, 1379
anonymity, 1130
answer to a goal, 71
antecedent, 2, 14, 78, 1266, 1295
anti-aliasing, 926, 1003
antichain, 2, 48, 571, 687, 844, 855
antidiﬀerence, 140, 195, 199
antihole, 571, 669
antimagic graph, 571, 645
antisymmetric function, 946
antisymmetry, 2, 42
antithetic variates, 476, 548
aperiodic prototile, 926, 958
aperiodic state, 476, 522
aperiodic tiling, 999
Appel, Kenneth, 632, 638, 639, 1515
apriori algorithm, 1386
arc capacity, 759
arc diagram, 1426, 1476
arc length matrix, 754
arc list, 722, 826
arc sine laws, 504
area of a graph drawing, 652
argument form, 2, 51
Aristotle, 1395, 1515
arithmetic function, 218, 266
additive, 266
completely additive, 266
completely multiplicative, 266
multiplicative, 266
arrangement, 926, 980
arrangement graph, 926, 940
array, 1341
array data structure, 1324
arrival process, 476, 536
Arrow’s impossibility theorem, 871
arrows notation, 681
art gallery theorems, 982
Artin, Emil, 1515
ascent, 140, 152
ASCII, 1292

1544
INDEX
aspect ratio, 926, 972
assertion, 2, 63
assignment, 722, 803
assignment instruction, 64
assignment problem, 447, 455
associated Stirling number
of the ﬁrst kind, 161
of the second kind, 161
association rule, 1376, 1378, 1384
associative property, 324, 329, 331
asymmetry, 2, 42
asymptotic, 2
behavior, 39
equality, 39, 140
atom, 2, 70, 324, 368, 369, 844, 860
atomic
formula, 2, 20, 70
lattice, 844
proposition, 2, 13
attachment, 571, 649
attack
adaptive chosen-ciphertext, 1070
biclique, 1089
chosen-ciphertext, 1071, 1078
chosen-message, 1112
chosen-plaintext, 1071, 1078
ciphertext-only, 1071, 1078
cryptosystem, 1078
known-message, 1112
known-plaintext, 1073, 1078
plaintext-only, 1078
Pollard’s rho, 1104
public key only, 1112
quantum, 1074
attribute, 571, 585
attribute-based encryption, 1070, 1110
augmented matrix, 382, 420
augmenting path, 722, 734, 762
augmenting path algorithm, 762
augmenting path theorem, 735
authentication, 1071, 1077
autokey cipher, 1093
automated reasoning, 2, 79
automated theorem proving, 79
automorphism, 324, 334, 571, 591, 625, 880,
888
automorphism group, 571, 591, 625, 673,
888
autonomous set, 875
auxiliary graph, 844, 875
average code length, 1029
average degree, 807
average distance, 808
average genus, 627
average-case algorithm analysis, 494
average-case complexity, 476, 1266, 1305
AVL tree, 1324, 1359
axiom, 2, 23
of assignment, 65
of choice, 2, 31
of if-then, 66
of if-then-else, 66
of NOP, 64
of procedure, 69
of sequence, 65
of while, 68
axis-parallel polyhedron, 989
Azuma’s inequality, 560
Babbage, Charles, 1081, 1515
Bachet’s equation, 218, 292
table, 294
Bachet, Claude Gaspar, 292
Bachmann, Paul Gustav Heinrich, 1516
back edge, 705
back substitution, 382, 420
back, of a queue, 1334
backbone network, 722
background probability, 1435
backtrack, 694, 702
backtracking, 706
Backus, John, 1516
Backus-Naur form (BNF), 1266, 1299
backward algorithm (HMM), 530, 531
backward arc, 759
backward variable, 530
balanced alphabet, 342
balanced incomplete block design (BIBD),
880, 885
balanced matrix, 1144, 1192
balanced tree, 694, 699
ballot theorem, 504
band, 1437
Bang’s aﬃne plank conjecture, 957
Bang’s cylinder covering problem, 957
Bar-Hillel lemma, 1298
Barab´asi-Albert model, 722, 820
bargaining set, 1206
basal species, 1426, 1491
base, 218, 227, 1467, 1475
base pair, 1475
base-ten radix sort, 1357

INDEX
1545
basic class, 382
basic feasible solution, 1144, 1154
basic initial problem, 502
basic solution, 1144, 1154
basic variable, 1154
basis, 382, 392, 571, 605, 893, 926, 934, 946
basis axioms, 880, 916
basis matrix, 1154
basis step, 2, 59
basket, 1376, 1384
Baum, Leonard, 536
Baum-Welch algorithm, 535
Bayes’ formula, 484
Bayes, Thomas, 484, 1516
BCH code, 1024, 1049
beer, 1385
Bell number, 121
Bell, Eric Temple, 121
benchmark datasets for clustering, 1395
Berge graph, 571, 669
Bernoulli numbers, 140, 151
Bernoulli polynomials, 140, 151
Bernoulli random variable, 476, 492
Bernoulli, Jakob, 151, 492
Bernoulli, James (Jakob I), 1516
Bernstein, Felix, 551
best ﬁt decreasing method, 1191
best ﬁt method, 1191
Betti number, 613, 615
Bhaskara, 1517
biclique attack, 1089
bicoloring, 1403
biconditional, 13
biconnectivity, 1366, 1370
bicriterion cluster analysis, 1405
bid language, 1144
bidiagonal matrix, 460
Bienaym´e, Ir´en´ee-Jules, 494
Bienaym´e-Chebyshev inequality, 494
big circle, 939
big data, 1378, 1379
big encryption system (BES), 1089
big omega, 39, 140
big theta, 140
big-circle arrangement, 926, 939
big-oh, 2, 39, 140
bihomogeneous tree, 694, 704
bijection, 3, 33
bimatrix game, 1210
bin packing problem, 780, 1144, 1190
binary, 33
binary burst, 1051
binary code, 391, 394
binary coded decimal, 218, 228
binary digit (bit), 227, 497
binary Golay code, 1055
binary logarithm, 34
binary matroid, 880, 914
binary operation, 324
binary relation, 3
binary repetition code, 1034
binary representation, 218, 497
binary search, 1324, 1359
binary search algorithm, 1360
binary search tree, 694, 700, 1324, 1342,
1359
binary search tree operations, 1343
binary symmetric channel, 1024, 1030
binary tree, 694, 699, 717, 1446
Binet form, 146
Binet, Jacques, 146
binomial coeﬃcient, 86, 101
generalized, 101
extended, 101
Gaussian, 86
binomial convolution, 140, 177
binomial random variable, 476, 492, 810
binomial theorem, 107, 175, 178, 193
biomarkers, 1387
biorder representation, 844, 868
bipartite graph, 571, 592
bipartite matching algorithm, 737
bipartite poset, 844, 855
bipartiteness, 1369
biplane, 880, 897
Birch and Swinnerton-Dyer conjecture, 318
Birkhoﬀ-von Neumann theorem, 452
bit, 1027
bitcoin, 1071, 1130
bitcoin mining, 1071, 1131
Blakley’s secret sharing scheme, 1120
blind signatures based voting, 1132
blinding, 1131
block, 571, 615, 880, 885
block cipher, 1071, 1083
block cipher rounds, 1084
block code, 1031
block design
table, 887
blocking pair, 722, 743
blocking set, 686
blog network, 815

1546
INDEX
Blom non-interactive key distribution, 1121
blossom, 722, 741
BLS signature, 1114
Bluetooth encryption, 1094
body, of a clause, 3, 70
Bollob´as, Bel´a, 1517
bond matroid, 917
Bonferroni’s inequality, 482
Boole’s inequality, 481
Boole, George, 369, 481, 1517
Boolean algebra, 324, 369, 844, 853
Boolean expression, 371
Boolean function, 324, 371, 1458
Boolean network, 1426, 1458
Borda consensus function, 844, 871
Borda count, 844, 871
Bose-Einstein model, 476, 487
bottom-up mergesort, 1354
bound variable, 20
boundary, 476, 926
boundary of region, 571, 649, 675
boundary problem, 502
boundary walk, 702
bounded lattice, 324, 367, 860
bounded poset, 844
bounded tree, 694, 704
bouquet, 571, 592, 595
boxicity, 1426, 1498
BPP, 1313
bracket representation, 865
branch, 95
branch length, 1450
branch number, 1089
branch-and-bound, 1171, 1405
branch-and-bound algorithm, 1171
branch-and-bound tree, 1186
branching process, 476, 526
breadth-ﬁrst search, 694, 706
breadth-ﬁrst search tree, 706
bridge, 571, 607
Brooks’ theorem, 632
Brouwer’s ﬁxed-point theorem, 1220, 1221
bubblesort, 1324, 1352
bubblesort algorithm, 1353
bulge loop, 1477
Burmester-Desmedt interactive key distri-
bution, 1123
Burnside’s conjecture, 341
Burnside’s lemma, 128
Burnside, William, 128, 1517
Burrows-Wheeler Transform (BWT), 1426
burst error, 1024, 1032
busy beaver function, 1266, 1282, 1283
buyer-submodularity, 1227
BWT algorithm, 1443
BWT query match algorithm, 1443
BWT reversal algorithm, 1443
bypass, 844, 875
cactus, 571
Caesar cipher, 1071
Caesar, Julius, 1080
call instruction, 69
cancellation property, 324, 332, 352
candidate generation, 1386
cannonball problem, 314
canonical block form, 1192
canonical correlation analysis, 466
canonical form, 1487
Cantor, Georg, 29, 62, 1517
capacitated concentrator location problem,
723, 784
capacitated location problem, 1144, 1180
capacitated minimum spanning tree, 723,
779, 1239
capacitated network, 723
capacitated p-median problem, 1180
capacity, 723
capacity assignment problem, 723, 786
capacity, of a channel, 1024
Carath´eodory’s theorem, 937, 962
cardinal number, 3, 29
cardinality, 3, 23
Carmichael number, 218, 259
Carmichael, Robert D., 259
Carroll, Lewis, 1521
Cartesian product, 3, 25, 48, 559
of graphs, 572, 590, 677
of posets, 844, 855
Cassini’s identity, 147
Cassini, Jean Dominique, 147
Catalan numbers, 140, 149, 718
generating function, 177, 206
recurrence relation, 177, 181
Catalan’s equation, 219, 292
Catalan, Eug`ene Charles, 149, 292
catastrophic convolutional code, 1059
caterpillar, 572, 694, 697
Cauchy’s interlacing theorem, 443
Cauchy’s theorem, 339
Cauchy, Augustin-Louis, 339, 1518
Cauchy-Binet formula, 419

INDEX
1547
Cayley graph, 572, 673
Cayley’s formula, 417, 710
Cayley’s theorem, 335, 345
Cayley, Arthur, 335, 1518
Cayley-Hamilton theorem, 433
CBC-MAC, 1115
ceiling, 3, 35
cell, 927, 1324, 1340, 1462
cellular automaton, 1266, 1284, 1426, 1462
cellular imbedding, 572, 658
center, 338, 572, 613, 694, 697, 1408
centering transformation, 1159
centerpoint, 927, 936
central hyperplane, 935
central hyperplane arrangement, 927, 940
central plane arrangement, 927, 939
centralizer, 338
centroid, 1408
certiﬁcate, 1324, 1367
certiﬁcation authority, 1071
c-game, 1145
chain, 3, 48, 572, 687, 844, 854, 927, 977
chain partition, 864
chain rule, 484
chain-product, 844, 855
chaining, 1363
chaining method, 1324
change of basis matrix, 404
channel, 1030
channel capacity, 1030
channel encoder, 1027
Chapman-Kolmogorov equations, 521
character recognition, 1002
characteristic, 324, 357
characteristic equation, 140, 182, 382, 433
characteristic function, 3, 35, 495, 1144,
1213, 1266, 1281, 1289
characteristic polynomial, 382, 433
of a graph, 572, 670
characteristic root, 182
characteristic-function game, 1144, 1213
Chebyshev distance, 1382
Chebyshev’s inequality, 556
Chebyshev, Pafnuty Lvovich, 494, 1518
check polynomial, 1044
check symbol, 1024, 1036
Chernoﬀbounds, 559
cherry, 1446
child, 694, 698, 1463
Chinese postman problem, 619, 747
Chinese remainder theorem, 219, 239
chirotope, 927, 946, 949
chirotope axioms, 947
choice number, 632
Cholesky decomposition, 382, 426
Chomsky hierarchy, 1266, 1296
Chomsky normal form, 1266, 1295
Chomsky, Noam, 1518
chord, 694, 705, 726
chosen-ciphertext, 1078
chosen-ciphertext attack, 1071
chosen-message attack, 1112
chosen-plaintext, 1078
chosen-plaintext attack, 1071
Christoﬁdes’ heuristic, 796
chromatic index, 572, 635, 688
chromatic number, 572, 631, 638, 688, 968
chromatic polynomial, 632
chromatically critical graph, 572, 631
chromosome, 1463
Chrysippus, 1518
Chung, Fan, 1519
Church’s thesis, 1267, 1287
Church, Alonzo, 1519
cipher, 1071, 1076
A5/1, 1095
AES, 1088
aﬃne, 1070, 1080
autokey, 1093
block, 1071, 1083
Caesar, 1071, 1080
digram, 1072, 1080
E0, 1094
Feistel, 1073, 1084
Hill, 1073, 1080
one-time pad, 1091
permutation, 1080
Playfair, 1080
polyalphabetic substitution, 1074, 1080
polygram substitution, 1074, 1080
product, 1074, 1084
RC4, 1094
self-synchronizing stream, 1075, 1092
shift, 1075, 1079
SNOW, 1094
stream, 1075, 1091
substitution, 1075, 1079, 1084
synchronous stream, 1075, 1092
transposition, 1075, 1081
Vernam, 1075
Vigen`ere, 1075, 1080
cipher block chaining mode, 1071, 1090

1548
INDEX
cipher feedback mode, 1071, 1090
ciphertext, 1071, 1076
ciphertext space, 1071, 1076
ciphertext-only attack, 1071, 1078
circuit, 927, 947, 1126
circuit axioms, 880, 916, 947
circulant matrix, 382, 443
circular linked list, 1325, 1341
circular sequence, 948
Clarke-Wright savings heuristic, 796, 801
class, 382
class group relations method, 264
class label, 1387
class library, 927, 1011
class number, 305
classiﬁcation, 1376, 1378, 1418
classiﬁcation theorem
ﬁnite simple groups, 341
classiﬁcation-based outlier detection, 1417
classiﬁer, 1376, 1378, 1388
clause, 3, 70
Clausen’s identity, 209
clique, 572, 636, 1394, 1496
clique matrix, 1498
clique number, 572, 637, 688
clique partition number, 572, 688
clock phase, 1229
clone, 1467
closed class, 476
closed form, 140, 143, 197
closed formula, 3, 37, 70
closed half-space, 927, 935, 970
closed hash table, 1325
closed set, 880, 914
closed walk, 572, 604
closed wﬀ, 20
closure, 3, 881, 893, 914
of a relation, 43
closure axioms, 881, 917
closure operation, 881, 893
closure operator, 862
closure property, 325
cloud computing, 1071, 1133
cluster, 927, 1367, 1376, 1378
cluster homogeneity, 1377
cluster of stars, 948, 949
axioms, 949
permutation, 949
cluster separation, 1377
cluster variance, 1408
clustering, 732, 1364, 1377, 1378, 1418
clustering coeﬃcient, 723, 808
clustering property, 1325
clustering-based outlier detection, 1417
clutter, 855
CNF, 16, 371
coalition, 1144, 1213, 1227
coalitional welfare function, 1227
cobasis, 881, 917
cocircuit, 881, 917, 927, 948
cocircuit axioms, 948
Cocks, Cliﬀord, 1100, 1519
cocycle matroid, 917
code, 1024, 1029
AG, 1064
BCH, 1049
binary Golay, 1055
binary repetition, 1034
block, 1031, 1035
catastrophic convolutional, 1059
convolutional, 1024, 1057
cross-interleaved Reed-Solomon, 1052
cyclic, 1025, 1044
dual, 1025
extended, 1025
extended Nordstrom-Robinson, 1056
extended Preparata, 1056
extended, of a linear code, 1040
from graph incidence matrix, 1052
generalized Reed-Solomon, 1051
generator matrix, 1025
Golay, 1025
Goppa, 1064
Hamming, 1025, 1040
linear, 1025, 1035
low-density parity check, 1025
maximum distance separable, 1053
narrow-sense, 1049
[n, k, d], 1035
[n, k]-linear, 1035
(n, M), 1031
(n, M, d), 1031
nonlinear, 1026
Nordstrom-Robinson, 1026, 1056
optimal, 1053
perfect, 1026, 1055
preﬁx, 1029
Preparata, 1026, 1056
primitive, 1049
punctured, 1026, 1040
quantum, 1062
recursive systematic convolutional, 1060

INDEX
1549
Reed-Muller, 1026, 1041
Reed-Solomon, 1026, 1051
self-dual, 1026, 1036
self-orthogonal, 1026, 1036
shortened, 1026, 1040
stabilizer, 1026, 1063
systematic, 1026, 1036
t-cyclic burst error correcting, 1032
t-erasure correcting, 1032
ternary Golay, 1055
turbo, 1026, 1062
uniquely decodable, 1026, 1029
code correction, 1031
code detection, 1031
code indicator, 1267, 1300
code, for an alphabet, 1267, 1300
code, of a cover, 1426, 1485
code, of a design, 1052
Codetalker, 1082
codeword, 1024, 1029, 1031, 1484
coding theory, 1024, 1027
codomain, 3, 32
coeﬃcient matrix, 420
cographic matroid, 881, 917
Cohen, Joel E., 1499
Cohen-Lenstra heuristics, 304, 306
coherent system, 477, 509
coin tossing, 503
coindependent set, 881, 917
collaborative ﬁltering, 465
collective outliers, 1417
collision, 1096, 1362
collision instance, 1325
collision resolution, 1325, 1362, 1363
collision resolution function, 1363
collusion-resistant security, 1111
coloop, 881, 917
color image quantization, 1410
coloring, 127, 631, 968, 1403
coloring pattern, 86
column generation, 1406
column rank, 392
column space, 392
combination, 86
k-, 86, 101
combination coeﬃcient, 86, 101
combination-with-replacement, 86, 105
combinatorial auction, 1144, 1223
combinatorial geometry, 881, 914
combinatorial neural code, 1426, 1484
combinatorial optimization problem, 1232
common enemy graph, 1426, 1496
common logarithm, 34
common PRAM, 1267, 1284
common random numbers, 477, 548, 549
common ratio, 182
communicating class, 477, 522
communicating states, 522
communication system, 1027
commutative property, 325, 329, 331
commutative ring, 325, 348
commutator, 342
comparability, 851
comparability digraph, 844, 851
comparability graph, 572, 607, 844, 851
comparability invariant, 845, 875
comparable elements, 3, 47, 845
comparison sort, 1267, 1307, 1325, 1349
competition graph, 1426, 1496
competition number, 1496
competitive equilibrium, 1227
complement, 25, 368, 481
of a graph, 572, 591
of a language, 1267, 1293
of a relation, 3, 42
of a set, 3
complement operator, 3, 74
complementary slackness, 1162
complementary slackness optimality condi-
tions, 770, 773
complemented lattice, 325, 367, 860
complete, 3, 23
complete binary tree, 694, 699
complete bipartite graph, 573, 592, 595, 743
complete graph, 573, 592, 595
complete hypergraph, 573, 685
complete information, 1145, 1205
complete language, 1267, 1311
complete m-ary tree, 699
complete matching, 723, 737
complete maximum likelihood decoding, 1024,
1034
complete multipartite graph, 573, 593
complete pivoting, 382, 429
complete r-uniform hypergraph, 573, 685
complete set
of invariants, 573
of operations, 597
complete system of residues, 219, 235
complete-linkage algorithm, 1399
completely positive matrix, 451
completion, 456

1550
INDEX
complex number, 3, 23
complexity measure, 1302
component, 573, 613
composite, 219, 225, 240
composite heuristic, 723, 794
composite key, 3, 50
composition, 3, 33, 86, 117, 148, 1286
of relations, 43
vector, 117
composition series, 341
compound formula, 20
compound proposition, 4
compressed certiﬁcate, 1367
computational convexity, 927, 1000
computational Diﬃe-Hellman assumption,
1117
computational Diﬃe-Hellman problem, 316,
1103
computational models
table of, 1274
computational security, 1071
computationally secure, 1078, 1121
computer graphics, 403, 1002
computer model, 1275, 1279, 1280
computer vision, 1004
computer-assisted proof, 4, 79
concatenation, 1267, 1290, 1291
concentration bound, 560
conclusion, 51
of a proof, 4
of an argument, 4
concrete data structure, 1325, 1340
condensation, 605
condition, 4, 77
condition number, 382, 427
conditional assignment construct, 66
conditional connective, 13
conditional entropy, 1028
conditional independence property, 528
conditional probability, 477, 484
conditional statement, 4
Condorcet’s paradox, 871
conference key computation, 1121
conference key distribution, 1071, 1121
conﬁdence, 1385
conﬁdentiality, 1071, 1077
conﬁguration model, 723, 817
conﬁguration rank, 936
conﬁguration, pebbling, 642
congruence class, 219, 236
congruent integers, 219, 235
congruent number problem, 314
conjugacy problem, 342
conjugate, 117, 338, 342, 881, 905
conjugate roots, 302
conjugate sequence, 382
conjugate vector, 446
conjunction, 4, 13
conjunctive normal expression, 16
conjunctive normal form, 4, 16, 325, 371
connected facility location problem, 723, 789
connected matroid, 881, 919
connected relation, 43
connected triple, 808
connection cost, 784
connectivity, 573, 1366, 1370
connectivity relation, 43
connectivity requirement, 788
coNP, 1267, 1310
consecutive chain, 845, 864
consecutive ones property, 1498
consensus, 1470
consensus function, 845, 871
consensus ranking, 845, 871
consequent, 4, 14, 78, 1267, 1295
consistent, 4, 23
constrained clustering, 1397
construct, 4
construction heuristic, 723, 801, 805
constructor, 73
contact graph, 953
contact number problem, 953
context-free grammar, 1240, 1267, 1295, 1482
context-sensitive grammar, 1267, 1295
contingency, 16
continued fraction, 219, 296
algorithm, 297
method, 264
simple, 296
table, 297
continuous density hidden Markov model,
528
continuous distributions
generating functions, 496
continuous radius, 1408
continuous random variable(s), 490
table of, 491
continuous star, 1408
continuum hypothesis, 4
contraction, 927, 949, 1220
contradiction, 4, 16
contrapositive, 4, 17

INDEX
1551
control center, 779
convergent, 219, 300
converse, 4, 17
of a digraph, 573, 605
of a relation, 43
convex, 927
convex body, 927, 952, 961
convex code, 1488
convex combination, 935
convex decomposition, 927, 972
convex game, 1213
convex hull, 927, 935, 961, 975, 1145, 1170,
1217, 1307
convex hull algorithm, 993
convex independence, 935
convex mapping, 1220
convex polygon, 927, 935, 961
convex polyhedron, 927, 970
convex polytope, 928, 935
convex position, 928, 935, 961
convex receptive ﬁeld, 1485
convex set, 928, 935, 961, 1145, 1149, 1217,
1485
convolution, 140, 173
convolutional code, 1024, 1057
Cook’s theorem, 1303
coordinate vector, 392
copositive matrix, 456
coprime, 219, 231
core, 1213, 1227
corner coloring, 127
corollary, 4, 51
corona, 573, 641
coRP, 1313
correlation, 493
correlation inequalities, 561
coset, 325, 351, 1024, 1038
coset leader, 1025, 1038
cosine distance, 1382
cosine similarity, 1382
cospectral graphs, 671
count, 1377, 1384
countable, 28
countable set, 4
countably inﬁnite, 28
counter mode, 1071, 1090
counterexample, 4, 58
counting methods, 92
counting problems, summary, 88
Courant-Fischer identity, 419
covariance, 493
covariance matrix, 464
covariance stationary process, 547
cover, 48, 780, 1186
cover diagram, 48
cover graph, 845, 852
cover relation, 845
covering, 955, 1397
covering number, 956
CPM model, 1145
Cramer’s rule, 423
Cramer, Gabriel, 423
Cramer-Shoup cryptosystem, 1071, 1106
CRCW model, 1267, 1284
credit card database, 1349
credit card fraud, 1419
CREW model, 1267, 1284
crew scheduling, 1194
critical pair, 845, 872
critical path, 1145
Critical Path Method (CPM), 1195
critical process, 526
criticality index, 1145, 1201
cross edge, 705
Cross-Interleaved Reed-Solomon code, 1052
cross-interleaving of codewords, 1048
cross-modal factor analysis, 466
cross-sectional volume, 956
crosscap, 573, 654
crosscap number, 573, 654, 659
crossing arcs, 1476
crossing number, 573, 651
crossover, 1463
cryptanalysis, 1071, 1076
diﬀerential, 1086
linear, 1073, 1086
related-key diﬀerential, 1089
cryptocurrency, 1072, 1129
cryptographic counter based voting, 1132
cryptographic primitive, 1072, 1076
cryptographic protocol, 1072
cryptography, 1072, 1076
elliptic curve, 1104
quantum, 1074
cryptology, 1072, 1076
cryptosystem, 1072, 1076
AES, 1084
attack, 1078
Cramer-Shoup, 1071, 1106
DES, 1084
ElGamal, 1072, 1103
elliptic curve, 1072

1552
INDEX
Goldwasser-Micali, 1073, 1108
NTRU, 1074, 1105
Paillier, 1074, 1107
public key, 1100
requirements, 1077
RSA, 1074, 1100
CSL, 550
cube graph, 573, 593
cube, of a word, 1268, 1300
cubic reciprocity, 305
cumulative distribution function, 490
Cunningham number, 254
Cunningham, Alan, 254
cut, 723, 726, 759, 1395
cut capacity, 760
cut space, 1052
cut-edge, 573, 607, 615
cut-vertex, 573, 615
cutset, 477, 510, 615
cutting plane, 1145, 1170, 1171
cutting plane algorithm, 1171, 1406
cycle, 325, 343, 595, 1456
directed, 573
Hamilton, 576, 620
in a graph, 613
cycle decomposition, 124
cycle index, 86, 125, 662, 667
cycle matroid, 881, 914
cycle rank, 573, 613
cycle structure, 86, 125
cycle-canceling algorithm, 772
cyclic code, 1025, 1044
cyclic exchange, 1237
cyclic exchange neighborhood, 1237
cyclic group, 325, 339
cyclic partition, 121
cyclic polytope, 974
cyclic subgroup, 325, 336, 339
cyclotomic extension, 362
cyclotomic ﬁeld, 219, 309
cyclotomic polynomial, 254
DAG, 573
Dantzig, George, 1150, 1519
Danzer-Gr¨unbaum conjecture, 554
data compression, 466, 1025, 1027
data encryption standard (DES), 1072,
1084, 1085
data integrity, 1072, 1077
data matrix, 1394
data mining, 465, 1377, 1379
data origin authentication, 1072, 1077
data stream, 1061
database, 1325, 1348
database entry, 1348
datatype, 1325, 1330
datatype speciﬁcation, 1332
Davenport-Schinzel sequence, 928, 1000
de Bruijn diagram, 159
de Bruijn digraph, 619
de Bruijn graph, 1426
de Bruijn sequence, 140, 159
algorithm for generating, 160
de Bruijn, Nicolaas G., 159
de Moivre’s theorem, 182
de Moivre, Abraham, 1520
De Morgan, Augustus, 370
decentralized search, 814
decidable, 1289
decimal digit, 227
decision function, 1302
decision node, 1390
decision tree, 694, 699, 1377, 1388, 1390
decision tree learning, 1390
decisional Diﬃe-Hellman assumption, 1106
bilinear, 1112
decisive collection, 1453
decoding algorithm
BCH codes, 1049
burst error correction, 1046
Hamming codes, 1041
Reed-Muller codes, 1042
decoding problem, 477, 533
decryption, 1072, 1076
decryption algorithm, 1076
Dedekind zeta function, 308
Dedekind’s problem, 866
Dedekind, Richard, 306, 1519
DEEP CRACK, 1086
deﬁcient integer, 267
deﬁcit vertex, 772
deﬁning predicate, 22
deﬁnite answer to a goal, 71
deﬁnite clause, 4, 70
deﬁnitive collection, 1453
degenerate LP, 1149
degree, 328, 344, 354, 574, 585, 662, 685
of a ﬁeld extension, 325
of a permutation group, 325
of a relation, 50
degree distribution, 723, 810
degree matrix, 709

INDEX
1553
degree pair, 586
degree sequence, 574, 585, 586
degrees of freedom, 963
deheaping, 1325, 1344
deheaping algorithm, 1345
Dehn-Sommerville equations, 974
Delaunay triangulation, 979
delay element, 1058
delete operation, 1340
deletion, 949
∆p
n, 1310
demand vertex, 769, 789
De Morgan, Augustus, 1520
dendogram, 1400
density, 928, 952, 956
density function, 477, 490
density-based outlier detection, 1417
denumerable set, 4, 28
dependency digraph, 557
dependency graph, 1427, 1456
dependent edge, 845, 875
dependent events, 477
dependent set, 383, 913
depth, 694, 698
depth-ﬁrst search, 694, 706
depth-ﬁrst search tree, 706
dequeue, 1334
derangement(s), 86, 99, 112, 203, 483
exponential generating function, 204
derivation, 1268, 1295, 1482
derivation probability, 1482
DES-X, 1087
Descartes, Ren´e, 1520
descendant, 694, 698
descent, 152
designed distance, 1049, 1064
destination vertex, 786
determinant, 383, 415
deterministic activity net, 1145, 1195
deterministic ﬁnite automaton (DFA), 1268,
1274
devil’s pair, 627
diagonal matrix, 383, 405
diagonalizable matrix, 383, 435
diagonalization proof, 4, 62
diameter, 574, 613, 694, 704, 807, 966, 1394
diapers, 1385
Dickson, Leonard Eugene, 1520
dictionary, 1325, 1338
diﬀerence, 4
of relations, 43
of sets, 5
of vectors, 388
diﬀerence equation, 195
diﬀerence operator, 140, 191
diﬀerence sequence, 141, 194
diﬀerence set, 885, 903
table, 904
diﬀerence table, 141, 194
diﬀerential analysis, 1072
diﬀerential cryptanalysis, 1086
Diﬃe, Whitﬁeld, 1100, 1112
Diﬃe-Hellman key exchange, 1116
digit, 227
digital cash, 1072, 1129
digital envelope, 1102
digital ﬁngerprint, 1097
digital signature, 1072, 1077, 1102, 1112
digital signature algorithm (DSA), 1072, 1113
digital signature standard (DSS), 1104
digram cipher, 1072, 1080
digraph
acyclic, 570
basis, 571, 605
comparability, 844, 851
de Bruijn, 619
dependency, 557
Ferrers, 845, 868
strict, 586
table, 666, 668
transitive, 582, 602
weakly connected, 583, 605
digraphical sequence, 586
dihedral group, 325, 333, 345
Dijkstra’s algorithm, 751
Dijkstra, Edsger W., 1520
dilutor, 73
Dilworth’s theorem, 865, 877
dimension, 383, 392, 1377, 1379
dimension reduction, 464
diophantine approximation, 219
diophantine equation, 219
Diophantus, 287, 1520
dipole, 574, 592, 595
Dirac’s theorem, 620
direct derivability relation, 1295
direct evaluation, 530
direct product, 331, 574, 677, 855
direct proof, 5, 54
direct sum, 331, 383, 390
directed Chinese postman problem, 776
directed connectance, 1496

1554
INDEX
directed cycle, 574, 605
directed graph, 383, 450
directed network, 723, 748
directed ordering, 47
directed out-tree, 723, 749
directed path, 574, 605
directed trail, 605
directed two-terminal ﬂow network, 737
directed walk, 574, 604
directional dual, 605
Dirichlet cell, 979
Dirichlet drawer principle, 95
Dirichlet inverse, 273
Dirichlet product, 273
Dirichlet’s theorem, 219, 252, 301
Dirichlet’s unit theorem, 306
Dirichlet, P. G. L., 95
discharging, 639
disconnecting set, 574, 615
discordant permutation, 141, 162
discrepancy, 559
discrete distributions
generating functions, 496
discrete exponential function, 363
discrete hidden Markov model, 528
discrete logarithm, 219, 279, 363
problem, 316, 317
discrete random variable(s), 490
table of, 491
discrete-event simulation, 477, 543
discrete-time dynamical system, 1427, 1456
discrete-time Markov chain, 477, 518
discriminant, 305
disjoint, 5, 24
mutually, 25
pairwise, 25
disjoint cycles, 344
disjoint events, 481
disjoint products, 514
disjoint union, 590, 855
disjunction, 5, 13
disjunctive normal expression, 16
disjunctive normal form, 5, 16, 325, 371
disproof, 5, 58
dissimilar hypergeometric terms, 141
dissimilarity matrix, 1394, 1450
dissimilarity measure, 1381
distance, 383, 395, 574, 605, 613, 695, 1174
distance label, 723, 749, 762
distance measure, 1377, 1381
distance threshold, 1419
distance-based outlier, 1419
distance-based outlier detection, 1417
distance-based outlier score, 1417
distance-ranked drawing, 652
distance-ranked partition, 652
distinct sums, 556
distinguished simplex, 1218
distinguished triangle, 1218
distribution, 477, 490
distributive lattice, 325, 367, 845, 862
divide-and-conquer algorithm, 141, 189
dividend, 225
divides, 219, 225
exactly, 219
divisibility lattice, 5
divisibility poset, 853
division algorithm, 355
division method, 1363
division ring, 325, 352
divisive hierarchical algorithm, 1403
divisive hierarchical clustering, 1377, 1403
divisor, 219, 225
divisor lattice, 845, 860
Dixon’s identity, 209
DNA segment, 1427, 1467
DNA target sequence, 1427, 1467
DNF, 16, 371
dodecahedral graph, 574, 593, 620
Dodgson, Charles Lutwidge, 1521
dogs, 1398
domain, 5, 19, 32, 33, 50, 329, 1325, 1330
dominant eigenvalue, 383, 440
dominant species, 1493
dominant-strategy incentive compatible, 1227
dominated strategy, 1210
dominating set, 574, 640
domination, 515
in L¨owner order, 443
of functions, 39, 189
domination number, 574, 640
Dorogovtsev-Mendes model, 723, 812
dot product, 383
double factorial, 161
double hashing, 1364
double spending, 1130
doubly hypergeometric function, 141, 209
doubly stochastic matrix, 383, 451
doubly stochastic pattern, 451
dovetail edge, 1470
dovetail path, 1470
downset, 561, 574, 687, 845, 858

INDEX
1555
downward closure, 1312
downward closure property, 1385
downward separation, 1310
d-regular graph, 807
dual, 325, 367, 369, 845, 855
dual code, 1025, 1036
dual graph, 656
dual hypergraph, 575, 685
dual ideal, 858
dual imbedding, 574, 656
dual incidence structure, 896
dual LP, 1145, 1162
dual matroid, 881, 917
dual polytope, 928, 971
dual simplex algorithm, 1163
dual system, 509
dual variable, 1162
duality construction, 656
duality principle, 325, 367, 370
duality transformation, 928
dummy activity, 1195
dyadic graph property, 1325, 1367
dynamic programming, 1405, 1479
dynamic structure, 1325, 1349
dynamic tree, 1367
dynamic update operation, 1325, 1365
E0 cipher, 1094
eccentricity, 575, 613, 695, 697
edge chromatic number, 575, 635
edge coloring, 127, 575, 635
edge coloring algorithm, 635
edge connectivity, 575, 615, 1052
edge contraction, 573, 590
edge cut, 575, 1052
edge independence number, 575, 637
edge subdivision, 582, 591
edge-chromatically critical graph, 635
edge-complement, 575
edge-deleted subgraph, 575, 629
edge-exchange heuristic, 797
edge-incidence table, 1325, 1346
edge-reconstructible graph, 575, 630
edge-reconstructible invariant, 575, 630
edge-to-edge tiling, 958
edit distance, 1382, 1437
Edmonds’ theorem, 741
Ehrenfest diﬀusion model, 520
eigenfaces, 465
eigenspace, 383, 435
eigenvalue, 383, 433
computation, 440
dominant, 383, 440
of a graph, 575, 627, 671, 675
eigenvector, 383, 433
of a graph, 575, 670, 671
Eisenstein’s irreducibility criterion, 355
Eisenstein, Gotthold, 306, 355
ejection chain, 1234
elastic boundary, 477, 502
electricity auctions, 1224
electronic code book mode, 1072, 1090
Electronic Frontier Foundation, 1086
electronic voting, 1072, 1131
blind signature scheme, 1132
cryptographic counter scheme, 1132
mix-net scheme, 1133
element, 5, 22
elementary ADT-constructor, 1332
elementary cellular automaton, 1462
elementary contraction, 632
elementary datatype, 1325, 1331
elementary projection function, 5
ElGamal cryptosystem, 1072, 1103
ElGamal digital signature scheme, 1113
ElGamal, Taher, 1103
ellipsoid method, 1160
elliptic curve, 219, 311
addition law, 311
endomorphism, 314
group law, 311
L-series, 318
supersingular, 316
torsion point, 311
elliptic curve cryptosystem, 1072, 1104
elliptic curve digital signature algorithm
(ECDSA), 1104, 1114
elliptic curve discrete logarithm problem,
1104
elliptic curve method, 219, 262
Ellis, James H., 1100
embedding, 575, 647, 656
empire chromatic number, 638
emptiness problem, 1268, 1296
empty graph, 575, 584
empty set, 5, 22
empty string, 1268, 1290
encrypted key exchange, 1117
encryption, 1072, 1076
algorithm, 1076
attribute-based, 1070, 1110
Bluetooth, 1094

1556
INDEX
homomorphic, 1073, 1118, 1133
identity-based, 1073, 1109
probabilistic, 1074, 1077
rule, 1091
Sahari-Waters attribute-based, 1111
end vertex, 585, 697
endpoint table, 594, 1325, 1346
endpoints, 575, 584
enheaping, 1344
enheaping algorithm, 1344
Enigma machine, 1072, 1082, 1283
enqueue, 1334
entity authentication, 1073, 1077
entropy, 1025, 1028, 1029, 1377, 1388
epimorphism, 5, 32
equality
of sets, 5
equilibrium, 1145, 1206, 1210, 1227
equinumerous, 29
equitable facility location problem, 1184
equitable partition, 627
equivalence class, 5, 46, 342
equivalence problem, 1268, 1296
equivalence relation, 5, 46
equivalent automata, 1268, 1276
equivalent codes, 1025, 1032
equivalent colorings, 127
equivalent grammars, 1268, 1295
equivalent propositions, 5
equivocation, 1028
erasure, 1025, 1031
Eratosthenes, 114, 251, 1521
Erd˝os number, 824
Erd˝os, Paul, 251
Erd˝os-Szekeres problem, 937
Erd˝os-Szekeres theorem, 962
Erd˝os, Paul, 1521
Erd˝os-Silverman-Stein theorem, 560
EREW model, 1268, 1284
Erlang random variable, 537
Erlang, A. K., 536
error probability, 1035
error vector, 1031
error-correcting code, 999
error-correction coding, 1025, 1027
error-detection coding, 1025
espalier, 1400
ESTREAM competition, 1095
Euclid, 233, 250, 1521
Euclidean 1-center problem, 1182
Euclidean algorithm, 233
Euclidean distance, 1182, 1382
Euclidean domain, 325, 352
Euclidean ﬁeld, 304
Euclidean hyperplane arrangement, 928,
940
Euclidean line arrangement, 928
Euclidean minimum spanning tree problem,
1307
Euclidean norm, 325, 352, 1159
Euclidean pseudoconﬁguration, 928, 949
Euclidean space, 388
Euclidean TSP, 794
Euler characteristic, 575, 637, 656
Euler circuit, 617
Euler cycle algorithm, 1473
Euler de Bruijn graph, 1472
Euler numbers, 157
Euler path, 1427, 1472
Euler phi-function, 132, 219, 266
table, 268
Euler polynomials, 156
Euler tour, 575, 617
Euler tour algorithm, 618, 619
Euler trail, 575, 617
Euler’s criterion, 284
Euler’s equation, 649
Euler’s oﬃcer problem, 911
Euler’s relation, 974
Euler’s theorem, 219, 237
Euler, Leonhard, 250, 617, 1521
Eulerian graph, 575, 617
Eulerian numbers, 141, 152
evaluation map, 1064
evaluation problem, 477, 530
even contractile graph, 670
even permutation, 325, 344
even vertex, 723, 740
event, 477, 480
evolution of dogs, 1398
exactly divides, 225
excedance, 141, 152
excess ﬂow, 761
excess vertex, 772
excluded minor, 920
exclusive or, 13
existential lower bound, 1268, 1307
existential quantiﬁer, 5, 19
existentially quantiﬁed predicate, 5
existentially unforgeable scheme, 1115
expanded blossom, 741
expanded sequence, 1427, 1429

INDEX
1557
expander family, 575, 675
expected delivery time, 815
expected information, 1388
expected value, 477, 493, 551
experiment, 477, 480
explicit authentication, 1117
exponent, 383
exponential function, 5, 34
exponential generating function, 86, 141,
143, 177
exponential random variable, 537
exponentiation
of cardinals, 29
exposure problem, 1229
extended code, 1025, 1040
extended Euclidean algorithm, 234
extended matrix, 934
extended Nordstrom-Robinson code, 1056
extended Preparata code, 1056
extended Riemann hypothesis, 253
extension, 33
extension ﬁeld, 325, 356
extension, of a substitution, 1293
extensive form, 1205
exterior region, 576, 649
external sorting method, 1326, 1349
extinction probability, 477, 526
extremal graph, 576, 679
extreme point, 449
f-factor, 623
F-factor, 623
face, 649, 928, 970, 1218
face description, 935
face lattice, 941
face vector, 928, 974
face-to-face tiling, 928, 958
Facebook network, 816, 825
facet, 1484
facility, 1145
facility location, 1145, 1174
facility vertex, 789
fact set, 5, 77
factor, 220, 225, 623, 677
factor group, 328, 338
factor theorem, 355
factorial, 5, 99
factoring
hardness of, 1101
using quantum computers, 265
factoring assumption for RSA, 1102
factoring method, 514
factorization algorithm, 220
failed state, 508
falling power, 86, 99, 141, 192
Fano matroid, 916
Fano plane, 881, 888
far end, 1326, 1341
Farey series, 220, 231
F¯aris¯ı, Kam¯al al-D¯ın al-, 1522
Farkas’ lemma, 1150, 1152
Farkas-Minkowski-Weyl theorem, 449
farthest neighbor, 966
Fasenmyer, Mary Celine, 210
feasible direction, 1145, 1149
feasible ﬂow, 759, 769
feasible region, 1145, 1148, 1170
feasible solution, 1145, 1148
feature, 1377, 1379, 1387
feedforward code, 1489
Feistel cipher, 1073, 1084
Feit, Walter, 341
Feit-Thompson theorem, 341
Fermat equation, 220, 291
Fermat number(s), 220, 254
table of, 256
Fermat prime, 220, 254
Fermat’s last theorem, 220, 310, 311
Fermat’s little theorem, 220, 237
Fermat, Pierre de, 291, 1522
Fermat-Weber problem, 1409
Fermi-Dirac model, 477, 487
Ferrers diagram, 86, 117
Ferrers digraph, 845, 868
Ferrers relation, 868
Fiat-Naor scheme, 1122
Fibonacci (Leonardo of Pisa), 1522
Fibonacci heap, 1326, 1345
Fibonacci numbers, 141, 145, 220
generalized, 145
recurrence relation, 183
ﬁeld, 325, 352, 356
cyclotomic, 219, 309
Euclidean, 304
extension, 325, 356
ﬁnite, 325, 363
ﬁxed, 326, 359
Galois, 326, 363
number, 221, 303
quadratic, 222, 304
root, 328, 359
skew, 328

1558
INDEX
splitting, 328, 359
ﬁeld automorphism, 357
ﬁeld extension, 358
FIFO property, 1326, 1334
ﬁgurate number, 141, 162
ﬁll, 383, 430
ﬁlter, 687, 845, 858
ﬁnal class, 383, 451
ﬁnd operation, 1340
ﬁngerprinting, 1316
ﬁnite, 5
ﬁnite automaton, 1268, 1275
ﬁnite dynamical system, 1456
ﬁnite extension, 359
ﬁnite ﬁeld, 325, 363
ﬁnite linear space, 893
ﬁnite sequence, 37
ﬁnite set, 23
ﬁnite substitution, 1293
ﬁnite transducer, 1268, 1274, 1276
ﬁnite-state machine, 1268, 1276
ﬁnite-state recognizer, 1275
ﬁnitely generated group, 325, 331
ﬁnitely presentable, 342
ﬁniteness problem, 1268, 1296
ﬁrst ﬁt decreasing method, 1190
ﬁrst ﬁt method, 1190
ﬁrst passage time, 477, 502, 525
ﬁrst-order linear recurrence relation
with constant coeﬃcients, 141
ﬁrst-order logic, 5, 19
ﬁrst-order Reed-Muller code, 1041
Fisher’s inequality, 687
ﬁtness, 1464
ﬁtness function, 1464
ﬁtness landscape, 1464
ﬁxed ﬁeld, 326, 359
ﬁxed point, 128, 1145, 1220, 1456
ﬁxed point (simplex) algorithm, 1221
FKG inequality, 561
ﬂat, 881, 914
ﬂat notation, 1326, 1332
ﬂat-polygon drawing, 656
ﬂat-polygon representation, 656
Fleury’s algorithm, 619
ﬂip-ﬂop, 1058
ﬂoat, 1145
ﬂoating-point, 498
ﬂoating-point arithmetic, 477
ﬂoating-point operation, 499
ﬂoor, 5, 35
ﬂop, 383, 422, 477, 499
ﬂow, 724
ﬂow value, 726, 759
ﬂow-based trophic level, 1495
Floyd, Robert W., 755
Floyd-Warshall algorithm, 606, 755
Folkman-Lawrence theorem, 951
food web, 1427, 1491
forbidden subposet description, 845, 858
force vector, 388
Ford, Lester R., Jr., 616, 760, 1523
Ford-Fulkerson theorem, 447
forepart, 1337
forest, 576, 593, 695, 696
forged signature, 1077
formula, 5, 70
forward algorithm (HMM), 530, 531
forward arc, 759
forward auction, 1145, 1223
forward edge, 705
forward star, 724, 826
forward substitution, 383, 420
forward variable, 530
four color theorem, 79, 576, 632, 638
four function theorem, 561
four point condition, 1450
four-way partition property, 1453
Fourier, Joseph, 1523
FP-growth, 1386
FP-tree, 1386
fractional chromatic number, 639
fragment, 1467
free distance, 1060
free edge, 724, 734
free energy, 1479
free group, 342
free language, 1291
free monoid, 326, 330, 1268, 1291
free morphism, 1293
free semigroup, 330, 342
free variable, 20
free vertex, 724, 734
frequency analysis, 1073, 1080
frequency, of a symbol, 1268, 1290
frequency-based memory, 1145, 1245
frequent itemset, 1384
frequent itemset mining, 1377, 1385
frequent pattern growth, 1386
Friedman, William F., 1081
Frobenius map, 316
Frobenius norm, 458

INDEX
1559
Frobenius normal form, 451
Frobenius, Georg, 128, 1523
Frobenius-K¨onig theorem, 446
front, of a queue, 1334
Frucht’s theorem, 674
frugal coloring, 557, 558
Fulkerson, Delbert Ray, 616, 760, 1523
full binary tree, 717
full conjunctive normal form, 5
full disjunctive normal form, 5
full m-ary tree, 695, 699
fullness, 1326, 1362
fully homomorphic, 1118
fully indecomposable matrix, 383, 446
fully parenthesized proposition, 6, 14
function, 6, 32
bijection, 3, 33
epimorphism, 5, 32
exponential, 5, 34
graph, 32
identity, 35
injection, 7, 32
inverse, 33
multivariate, 33
one-to-one, 8, 32
onto, 8, 32
partial, 9, 33
root modulo m, 222
surjection, 11, 32
total, 33
value, 32
function complexity, 1267
functionally complete, 16, 326, 371
functionally complete set, 6
fundamental cycle, 695, 706
fundamental edge cut, 695, 706
fundamental homomorphism theorem, 338
for rings, 352
fundamental polygon representation, 656
fundamental system of cycles, 695, 706
fundamental system of edge cuts, 695, 706
fundamental theorem
of algebra, 355
of arithmetic, 220, 244
of discrete calculus, 199
of ﬁnite abelian groups, 335
of Galois theory, 360
fuzzy clustering, 1397
fuzzy complement, 73, 74
fuzzy intersection, 73
fuzzy logic, 6, 74
fuzzy set, 6, 73
fuzzy system, 74
fuzzy union, 73, 74
Gale, David, 1524
Gale-Ryser theorem, 446
Gale-Shapley algorithm, 744
Gallai-Milgram theorem, 877
Galois extension, 326, 359
Galois ﬁeld, 326, 363
Galois group, 326, 359
Galois, ´Evariste, 361, 1524
gambler’s fallacy, 485
gambler’s ruin problem, 477, 502–505
Game of Life, 1268, 1285, 1286, 1462
gamma function, 203
GAN model, 1145, 1201
gap length, 1430
garbled circuit, 1126
GASP, 550
Gauss’ identity, 208
Gauss’ lemma, 285
Gauss, Carl Friedrich, 251, 285, 1524
Gauss-Bonnet theorem, 971
Gaussian binomial coeﬃcient, 86, 101
Gaussian coeﬃcient, 861
Gaussian elimination, 383, 422
Gaussian integer, 220, 303, 350
GCHQ, 1100, 1116
gene, 1463
general graph, 576, 585
general linear group, 333
general matching algorithm, 741
general ruin problem, 506
generalized Weber problem, 1182
generalized circuit, 607
generalized continuum hypothesis, 6
generalized octahedral graph, 593
generalized precedence relation, 1195
generalized Reed-Solomon code, 1051
generalized Riemann hypothesis, 309
generating function, 86, 141, 173
exponential, 86, 141, 143, 177
ordinary, 86, 143, 173
generating set, 576, 673, 893
generation, 1462
generator, 325, 336, 339
of a shift register, 1058
generator matrix, 1036
of a code, 1025
of a stabilizer code, 1063

1560
INDEX
generator polynomial, 1025, 1044
generic datatype, 1326, 1332
genetic algorithm, 1427, 1463
genetics, 485
genome assembler, 1427, 1467
genotype, 1463
genotype space, 1463
Gentry, C., 1118
genus, 928, 971
of a graph, 576, 658
of a surface, 576, 654
geographic information system, 929, 1009
geometric constraint solving, 1010
geometric lattice, 845, 862
geometric multiplicity, 383, 435
geometric progression, 182
geometric random variable, 477
geometric series, 141, 207
Gerˇsgorin discs, 383, 438
Germain, Sophie, 1524
GERT model, 1145
(g, f)-factor, 623
Gilbert-Varshamov bound, 1054
Gilmore-Lawler lower bound, 804
girth, 552, 576, 649
global function, 1456
global pairwise alignment, 1427, 1429
global pairwise alignment algorithm, 1431
Glover, Fred, 1241
GMW multiparty computation, 1128
goal, 6, 71
goal programming, 1145, 1168
G¨odel numbering, 1268, 1289
G¨odel, Kurt, 29, 1525
Golay code, 1025
Goldbach’s conjecture, 253
Goldbach, Christian, 253
Goldwasser-Micali cryptosystem, 1073, 1108
Golomb’s self-generating sequence, 161
good cover, 1485
Goppa code, 1064
Gorenstein, Daniel, 341
Gosper’s algorithm, 211
Gosper, R. W., Jr., 211
GPSS, 550
graceful graph, 576, 645
graded poset, 846, 857
Graham Scan algorithm, 975
Graham’s conjecture, 644, 678
Graham, Ronald, 1525
Gram’s formula, 971
Gram, J¨orgen, 396
Gram-Schmidt orthogonalization, 396
grammar, 1268, 1482
Granville, Andrew, 259
graph
acyclic, 570, 609, 696
antimagic, 571, 645
arrangement, 926, 940
auxiliary, 844, 875
Berge, 571, 669
bipartite, 571, 592
cactus, 571
caterpillar, 572
Cayley, 572, 673
chromatically critical, 572, 631
common enemy, 1426, 1496
comparability, 572, 607, 844, 851
competition, 1426, 1496
complete, 573, 592, 595
complete bipartite, 743
complete multipartite, 573, 593
cover, 845
cube, 573, 593
cycle, 573, 592, 595
de Bruijn, 1426
dependency, 1427, 1456
dodecahedral, 574, 593, 620
empty, 575, 584
Euler de Bruijn, 1472
Eulerian, 575, 617
even contractile, 670
generalized octahedral, 593
graceful, 576, 645
Hamilton de Bruijn, 1472
Hamiltonian, 576, 620
Hamming, 679
harmonious, 576, 645
hypercube, 577, 593, 596
icosahedral, 577, 593
improvement, 1237
incomparability, 846, 852
intersection, 578, 593, 685
interval, 578, 593, 1498
Kneser, 643
Kuratowski, 578, 593, 596, 647
line, 579, 591, 635
magic, 579, 645
median, 679
nonplanar, 579, 647
null, 579, 584
octahedral, 580, 593

INDEX
1561
of a function, 6
of a matrix, 449
overlap, 1469
path, 1428, 1431
perfect, 580, 623, 669
permutation, 848, 875
Petersen, 580, 593, 596
planar, 580, 647
planar straight-line, 977
plane, 1327, 1367
Platonic, 580, 596
predator projection, 1428, 1501
prey projection, 1428, 1501
Ramanujan, 581, 675
random, 581, 682
regular, 581, 585, 610, 685
Schreier, 581, 673
self-complementary, 581, 591
state-space, 1428, 1456
strongly regular, 582, 671
supermagic, 582, 645
table, 661, 663
tetrahedral, 582, 593
theta, 620
tough, 582, 620
transfer, 1237
trivial, 583, 584
Tur´an, 583, 679
unit distance, 965
vertex-transitive, 673
visibility, 932, 982
voltage, 673
weighted, 583, 585
wheel, 583, 595
graph bicoloring, 1403
graph coloring, 1403
graph connectivity, 767
graph drawing, 1008
graph incidence matrix
code from, 1052
graph intersection, 590
graph invariant, 625
graph isomorphism, 625, 1305
graph mapping, 590
graph model, 576, 585, 599
table, 599
graph reconstruction conjecture, 630
graph representation, 581, 594
graph speciﬁcation, 581, 594
graph sum, 576, 590
graph union, 590
graph, of a digraph, 583, 602
graphic matroid, 882, 914
graphical sequence, 576, 585
graphical user interface, 929, 1003
Grassmann-Pl¨ucker relations, 929, 946
Gray code, 141, 158, 576, 620, 622
greatest common divisor, 220, 231
greatest element, 48, 367
greatest integer, 35
greatest lower bound, 6, 48, 846, 859
greedy matroid algorithm, 921
Greene-Kleitman theorem, 865
Greibach normal form, 1299
grid drawing, 652
ground formula, 6, 70
group, 326, 331
abelian, 324, 331
additive, 331
automorphism, 571, 591, 625, 673
dihedral, 325
ﬁnitely generated, 325, 331
free, 342
Galois, 326, 359
multiplicative, 331
of the rectangle, 345
permutation, 327, 344
quaternion, 333
quotient, 328, 338
simple, 328, 341
solvable, 341
sporadic, 341
symmetric, 329, 344
group divisible design (GDD), 882, 895
group key, 1121
group presentation, 342
group-type, 882
growth factor, 384, 429
Grubb’s test, 1418
Hadamard design, 882, 901, 902
Hadamard matrix, 882, 901, 902
Hadamard’s conjecture, 901
Hadamard’s inequality, 419
Hadamard, Jacques, 251
Hadwiger number, 953
Hadwiger’s conjecture, 632
Hadwiger-Levi covering conjecture, 957
hairpin loop, 1477
Haken, Wolfgang, 632, 638, 639, 1525
half-plane intersection, 939
half-space, 929

1562
INDEX
Halin’s conjecture, 630
Hall’s condition, 687
Hall’s theorem, 27, 623, 737
Hall, Marshall, 1525
Hall, Philip, 27, 623
halting function, 6, 37
halting problem, 1268
ham-sandwich cut, 929, 936
Hamilton cycle, 576, 620, 793
Hamilton de Bruijn graph, 1472
Hamilton path, 576, 620, 1427, 1469
Hamilton walk, 1447
Hamilton, William Rowan, 620, 1526
Hamiltonian graph, 576, 620
Hamming bound, 1053
Hamming code, 1025, 1040
Hamming distance, 428, 1025, 1031, 1382
of a code, 1025
Hamming graph, 679
Hamming weight, 1025, 1036
Hamming, Richard W., 1526
handle, 654
handshaking lemma, 587
Harary, Frank, 660
hard decision decoder, 1057
Hardy, G. H., 1526
Hardy-Littlewood-P´olya theorem, 447
harmonic numbers, 141, 158
harmonic sum, 191
harmonious graph, 576, 645
hash function, 1073, 1096, 1326, 1362
collision-resistant, 1097
MD2, 1099
MD4, 1099
MD5, 1099
MD6, 1099
one-way, 1097
SHA-0, 1099
SHA-1, 1099
SHA-2, 1099
SHA-3, 1099
universal, 1115
hash table, 1326, 1362
hash-values, 1096
hashing, 1326, 1359
Hasse diagram, 6, 48, 846, 852
Hasse’s theorem, 316
Havel’s theorem, 587
head, 576, 586, 601, 602
head, of a clause, 6, 70
header, 1340
heap, 1326, 1344
heapsort, 1326, 1352
heapsort algorithm, 1353
Hedetniemi’s conjecture, 678
hedge, 74
height, 695, 698, 846, 855
helix, 1476
Hellman, Martin, 1100, 1112
Helly’s theorem, 937, 962, 1486
hereditary family, 858
Hermitian adjoint, 384, 405
Hermitian matrix, 384
Hessenberg form, 441
heuristic algorithm, 724
hexadecimal expansion, 228
hexadecimal representation, 220
hidden Markov model (HMM), 478, 527
hidden surface removal, 983
hierarchy of partitions, 1397
higher-order logic, 20
Hilbert’s tenth problem, 1269, 1289
Hilbert, David, 29, 1527
Hill cipher, 1073, 1080
hitting time, 502
HMAC, 1115
Hoare, Charles A. R., 1355
Hoﬀman polynomial, 577, 671
Hoﬀman-Wielandt theorem, 444
Hofstadter’s G-sequence, 161
homeomeric tilings, 958
homeomorphic graphs, 577, 591
homogeneity, 1394
homogeneous recurrence relation, 141
homogeneous system, 420
homogeneous tree, 695, 704
homomorphic
additively, 1118
fully, 1118
multiplicatively, 1118
homomorphism, 326, 334
homothet, 956, 968
homothety, 956
hook, 86, 134
hooklength, 87, 134
Householder matrix, 460
Huﬀman code, 699
Huﬀman tree, 699
Hurwitz’s theorem, 301
hybrid method, 723
hybridization, 1427
Hypatia, 1527

INDEX
1563
hypercube, 555
hypercube graph, 577, 593, 596
hypergeometric random variable, 478, 492
hypergeometric series, 141, 207
hypergraph, 577, 684
complete, 573, 685
interval, 687
n-uniform, 551
partial, 580, 684
hypergraph clique, 687
hyperline sequence, 931, 948
hyperplane, 882, 896, 914, 929, 970
hyperplane arrangement, 929
hyperplane code, 1427, 1489
hyperplane intersection algorithm, 995
Hypothesis H, 253
ibn al-Banna al-Marrakushi, Abu-l-’Abbas
Ahmad ibn Muhammad, 1516
Ibn al-Durayhim, 1080
ibn al-Haytham, Ab¯u ’Al¯ı al-Hasan
(Alhazen), 1527
ibn Ezra, Abraham, 1522
ibn Mun’im al-’Abdari, Ah.mad, 1532
icosahedral graph, 577, 593
ID3 algorithm, 1390
ideal, 326, 349, 687, 846, 858, 1487
maximal, 350
prime, 350
principal, 350
ideal class, 305
ideal class group, 305
ideal security, 1097
idempotent, 33
idempotent Latin square, 882, 906
idempotent matrix, 384, 433
identity, 326
identity function, 6, 35
identity laws, 17, 26
identity matrix, 384
identity permutation, 344
identity property, 331
identity-based encryption, 1073, 1109
if-then instruction, 66
if-then-else instruction, 66
ill-conditioned matrix, 427
ill-conditioned system, 384
image, 32
image compression, 1003
image set, 6
imbedded Latin square, 882, 906
imbedding, 577, 647, 656
imperfect information, 1205
implication, 6
improper coloring, 631
improper subgroup, 326, 336
improvement graph, 1237
improvement heuristic, 724, 801, 805
improving direction, 1145, 1149
improving move, 1232, 1241
imputation, 1146, 1213
in-degree, 577, 586, 602
in-place realization, 1326, 1349
incidence function, 601
incidence matrix, 448, 577, 594, 602, 685,
882, 885, 1192, 1326, 1346
incidence rule, 577, 584
incidence structure, 882, 896
incident-edge table, 577, 594
inclusion problem, 1269, 1296
inclusion-exclusion method, 514
inclusion-exclusion principle, 110, 481
incomparability graph, 846, 852
incomparable elements, 6, 47, 846, 851
incomplete information, 1205
incomplete maximum likelihood decoding,
1025, 1034
incongruent integers, 235
incremental method, 976, 981
indeﬁnite clause, 70
indeﬁnite sum, 141, 199
indel, 1430
indel score, 1430
independence number, 577, 641, 688
independent dominating set, 641
independent domination number, 641
independent events, 478, 483, 484
independent random variables, 478, 490
independent set, 384, 577, 685, 882, 913
independent set of axioms, 6, 23
independent transversal, 557, 558
index, 220, 279, 326, 338, 885
index function, 363
index of coincidence, 1081
index of cyclicity, 384, 451
index of summation, 196
indicator function, 1487
indices
table of, 279
induced pair permutation, 346
induced pair-action group, 346
induced partition, 6, 46

1564
INDEX
induced poset, 852
induced subgraph, 577, 589, 590, 669
induced subtree, 1448
induction, 6
strong form, 59
weak form, 59
induction hypothesis, 6, 59
induction step, 6, 59
inductive proof, 6
inﬁnite loop, 1281
inﬁnite sequence, 37
inﬁnite set, 7, 23
inﬁnite tree, 695, 704
inﬁx notation, 14, 703
inﬂow, 761
information gain, 1388
information rate, 1031
information set, 1205
information symbols, 1025, 1036
information theory, 1025, 1027
inherently ambiguous context-free language,
1269, 1298
initial conditions, 180
initial distribution, 519
initial state distribution, 527
initial vector, 1090
injection, 7, 32
inner corner, 135
inner product, 384, 394
inorder traversal, 695, 703
input model, 544
insert operation, 1340
insertion sort, 1326, 1350
insertion sort algorithm, 1351
insider trading, 1419
instability, 743
installation cost, 786
instance, 1379, 1387
instance space, 1388
instantiation, 7, 19, 52, 78
integer partition, 846, 860
integer programming (IP) problem, 1146,
1169, 1224
integers, 7, 22, 223
integral domain, 326, 352
integrality theorem, 449
interactive key distribution, 1121
interconnection network, 1269, 1284
interest, 1385
interesting association rule, 1386
interior point method, 1146
interleaving of codewords, 1048
internal edge, 1446
internal loop, 1477
internal sorting method, 1326, 1349
internal state, 1088
internal vertex, 695, 698, 754
Internet Key Exchange (IKE), 1117
intersecting family, 846, 961
intersection, 7, 24, 480
of graphs, 590
of languages, 1293
of partial orderings, 846, 872
of relations, 43
intersection graph, 578, 593, 685
intersection relation, 7
interval, 7, 48, 846, 852
interval graph, 578, 593, 1498
interval hypergraph, 687
interval of summation, 196
interval order, 846, 868
interval representation, 846, 868
intransitivity, 42
intrusion detection, 1419
invariant, 578
inverse, 17, 326
modulo m, 220, 236
of a matrix, 384, 407
of a relation, 43
of a word, 342
inverse function, 7, 33
inverse image, 7, 32
inverse property, 331
inverse relation, 7
inverse transformation method, 545
inverted index, 1442
invertible function, 7
invertible matrix, 384
involution, 33, 134, 326, 344, 1085
IPSec, 1088
Iris dataset, 1380, 1392, 1396
irrational number, 7, 220, 223
irrationality measure, 301
table, 301
irreducibility, 355
irreducible chain, 478
irreducible element, 326, 860
irreducible Markov chain, 522
irreducible matrix, 384, 443
irreducible polynomial(s), 326, 354
table of, 366
irreducible tournament, 578, 610, 665

INDEX
1565
irreducible tree, 712
irredundance number, 641
irredundant set, 641
irreﬂexive property, 7, 42
irrelevant edge, 478, 515
isogonal tiling, 929, 958
isohedral tiling, 958
isomorphic conﬁgurations, 929, 936
isomorphic designs, 888
isomorphic trees, 698
isomorphism, 326, 384, 695, 699
of Boolean algebras, 369
of designs, 882, 888
of digraphs, 625
of ﬁelds, 356
of graphs, 578, 590, 625
of groups, 344
of lattices, 366, 846, 860
of matroids, 882, 914
of posets, 846, 852
of rings, 350
of semigroups, 330
of vector spaces, 392
isomorphism problem, 343
isomorphism test, 627
isomorphism testing algorithm, 628
isomorphism type, 578, 625
isoperimetric constant, 578, 675
isoperimetric quotient, 953
isotoxal tiling, 929, 958
item, 1377, 1384
itemset, 1377, 1384
iterate, 1456
iterated function, 33
iterated graph sum, 590
iterated logarithm, 35
iterated sum, 197
Iverson truth function, 1332
Jaccard distance, 1382
Jaccard similarity, 1382
Jackson-Rogers model, 724, 820
Jacobi logarithm, 363
Jacobi method, 441
Jacobi symbol(s), 220, 284
reciprocity law for, 285
Jacobi’s identity, 420
Jacobi, Carl Gustav Jacob, 284
Jarvis’ March algorithm, 975
Jeﬀerson disk, 1081
Jeﬀerson, Thomas, 1081
j-invariant, 311
Johnson bound, 1054
join, 50, 578, 590, 846, 859
join-irreducible element, 846, 860
joint distribution, 490
joint entropy, 1028
Jordan canonical form, 436
Jordan-Dedekind chain condition, 846, 857
Jupiter, 1037
k-chain, 844
k-choosable, 632
k-codimensional cylinder, 956
k-colorable, 631
k-connected, 578, 615
K-cutset, 478, 510
k-edge colorable, 635
k-edge-connected, 578, 615
k-exchange, 1234
k-exchange neighborhood, 1234
k-factor, 623
k-GDD, 882
k-level, 929, 939, 941
k-loop, 1477
k-means algorithm, 1408
k-mer, 1437, 1467
k-nearest neighbor classiﬁer, 1388
k-norm, 847, 864
k-opt exchange, 794
k-optimal tour, 794
k-out-of-n system, 511, 515
K-partition, 1236
K-terminal network, 512
k-tough, 623
K-tree, 478, 510
k-tuple chromatic number, 639
k-tuple of, 1330, 1331
Kakutani’s ﬁxed-point theorem, 1221
Kantorovich inequality, 444
Kantorovich, Leonid, 1150, 1527
Karaj¯ı, Ab¯u Bakr al-, 1528
Karmarkar’s algorithm, 1160
Karmarkar, Narendra, 1159
Karnaugh map, 374
Karnaugh map method, 375
Karnaugh, Maurice, 375
Karp-Lipton theorem, 1312, 1313
Kasiski test, 1081
Kasiski, Friedrich W., 1081
Kazhdan constant, 675
Kendall’s notation, 537

1566
INDEX
Kerckhoﬀ’s assumption, 1073, 1078
kernel, 327, 334, 350, 384, 398
Kevin Bacon network, 824
key, 1073, 1076, 1326, 1348
session, 1116
shared, 1116
key agreement, 1073
key distribution
Blom non-interactive, 1121
Burmester-Desmedt, 1123
conference, 1121
interactive, 1121
non-interactive, 1121
polynomial interpolation, 1122
tripartite, 1122
key domain, 1326, 1348, 1362
key entry, 1327
key equivocation function, 1078
key exchange, 1073, 1116
Diﬃe-Hellman, 1116
encrypted, 1117
internet, 1117
password-based, 1117
key randomization, 1327
key schedule, 1073, 1083, 1088
keyed pair, 1327, 1338
keyspace, 1073, 1076
keystream, 1091
keystream generator, 1073, 1091
Khachian, Leonid G., 1160
Khintchine’s constant, 300
king, 578, 610
Kirchhoﬀ, Gustav R., 597
Kirkman triple system, 882, 891
Kirkman’s schoolgirl problem, 882, 891
Kirkpatrick-Siedel algorithm, 976
kissing number, 953
Kleene closure, 1269, 1291
Kleene plus, 1291
Kleene star, 1269, 1291
Kleene’s theorem, 1277, 1298
Kleene, Stephen Cole, 1528
Klein bottle, 654
Klein four-group, 327, 345
Klein, Felix, 345, 1528
Kleinberg small-world model, 724, 814
Kleitman’s lemma, 687
Klemm-Egu´ıluz model, 724, 820
KMP algorithm, 1427, 1440
knapsack problem, 1146, 1186, 1466
core heuristic, 1187
greedy heuristic, 1186
Kneser graph, 643
Kneser’s conjecture, 689
knowledge discovery in databases, 1377, 1379
known-message attack, 1112
known-plaintext attack, 1073, 1078
Knudsen-Hein grammar, 1427, 1482
Knuth, Donald E., 1528
Koblitz, Neal, 1104
Kolmogorov’s inequality, 494
Kolmogorov, Andrey Nikolayevich, 494
Kolmogorov-Chaitin complexity, 1302
K¨onig’s lemma, 705
K¨onig’s theorem, 446, 623, 737
K¨onig, D´enes, 623
K¨onigsberg bridge problem, 617
Kraft’s inequality, 1029
Kronecker delta function, 87, 132
Kronecker product, 882, 901
Kronecker symbol, 220, 284
Kronecker, Leopold, 284
Kronecker-Weber theorem, 310
Kruskal’s algorithm, 728
kth smallest element algorithm, 1361
Kummer’s identity, 208
Kummer, Ernst, 306
Kuratowski graph, 578, 593, 596, 647
Kuratowski’s theorem, 647
Kuratowski, Kazimierz, 647, 1528
label-correcting algorithm, 750
labeled arrangement, 944
labeled graph, 578, 660, 665
table, 661
labeled tree, 695, 711
lactose metabolism model, 1460
Ladner’s theorem, 1312
Lagrange’s four-square theorem, 295
Lagrange’s theorem, 239, 337
Lagrange, Joseph-Louis, 147, 295, 337, 1529
Lah coeﬃcients, 141, 162
λ-free semigroup, 1291
λ-free substitution, 1293
λ-transition, 1272
λ, of a graph, 578
Lam´e, Gabriel, 234, 1529
Landau’s theorem, 447
Landau, Edmund, 1529
language, 1269, 1276, 1279, 1291, 1295, 1308,
1309, 1482
Laplace expansion, 415, 454

INDEX
1567
Laplace, Pierre-Simon de, 1529
Laplacian, 578, 671
Laplacian matrix, 384, 448
largest element, 367
Las Vegas algorithm, 1269, 1316
latent semantic indexing, 465
Latin rectangle, 883, 906
Latin square, 883, 905
table, 907
lattice, 846, 860
atomic, 844
bounded, 860
complemented, 860
distributive, 845, 862
divisor, 845, 860
geometric, 845, 862
lower semimodular, 847, 862
modular, 847, 862
partition, 848, 860
semimodular, 848
subset, 849, 860
subspace, 849, 860
upper semimodular, 850, 862
Young, 850, 860
lattice tiling, 929, 958
lattice(s), 7, 48, 327, 366
as partially ordered set, 367
bounded, 324, 367
complemented, 325, 367
diagram, 367
distributive, 325, 367
divisibility, 5
law of total probability, 484
layout, 1470
leaf, 585, 695, 698
leaf node, 1390
learning, 1388
learning problem, 478, 535
least common ancestor, 829
algorithm, 830
least common multiple, 220, 231
least element, 48, 367
least nonnegative residue, 220, 235
least positive residue, 235
least upper bound, 7, 48, 847, 859
left child, 695, 699, 1327, 1342
left coset, 325, 338
left derivative, 1293
left divisor of zero, 327, 352
left null space, 390
left quotient, 1293
left singular vector, 458
left subtree, 695, 699
left-complete, 1344
left-complete binary tree, 1327
left-linear grammar, 1295
left-right tree, 695, 717
leftmost derivation, 1269, 1295
leftmost language, 1269, 1295
Legendre symbol, 220, 284
Legendre, Adrien-Marie, 284
Leibniz’s theorem, 191
Leibniz, Gottfried Wilhelm, 1530
lemma, 7, 51
length, 384
length set, 1269, 1291
length, of a string, 1290
length-increasing grammar, 1269, 1295
Lenstra, H. W., 261, 262, 313
Lenstra-Lenstra-Lov´asz algorithm, 1106
Lenz’ construction, 967
Leonardo of Pisa, 145, 1522
Let’s Make a Deal, 486
level, 695
level ordering, 699, 703
Levi ben Gerson, 1530
L´evy’s law, 300
lexicographic order, 87, 108
lexicographic ordering, 847, 855
lexicographic product, 579, 677
li function, 250
LIFO property, 1327, 1334
lift, 1385
line arrangement, 929, 939, 948
line graph, 579, 591, 635
linear assignment problem, 724, 803
linear block code, 1035
linear bounded automaton (LBA), 1282
linear code, 1025, 1035
linear combination, 384, 392, 934
linear congruence, 238
linear congruential method, 220, 237
linear cryptanalysis, 1073, 1086
linear dependence, 392
linear diophantine equation, 287
linear equations, 420
linear extension, 847, 872
linear extension ordering, 579, 609
linear feedback shift register, 1073, 1092
linear grammar, 1269, 1295
linear independence, 392
linear model, 1377, 1388

1568
INDEX
linear operator, 384, 398
linear order, 847
linear ordering problem, 1242
linear probing, 1364
linear programming (LP) problem, 1146,
1148
linear recurrence relation
with constant coeﬃcients, 142
linear search, 1327, 1359
linear separability, 1388
linear subspace, 934
linear sum, of posets, 847, 855
linear system, 384
linear transformation, 384, 398
linearly dependent, 182
linearly dependent set, 934
linearly independent, 182
linearly independent set, 934
linearly ordered set, 7, 47, 847, 851
link capacity, 724
link/condense tree, 1367
linked adjacency list, 724, 826
Liouville’s function, 272
Liouville’s theorem, 301
list assignment, 632
list decoding, 1025
literal, 7, 327, 371
little omega, 39, 142
Little’s law, 539
little-oh, 7, 39, 142
Littlewood, J. E., 251
Littlewood-Oﬀord problem, 865
L-matrix, 457
local access network, 724
local central limit theorem, 506
local clustering coeﬃcient, 808
local criterion, 1398
local obstruction, 1488
local pairwise alignment, 1427, 1429
local search, 814
local search algorithm, 1232
locally ﬁnite poset, 847, 852
locally ﬁnite tree, 695, 704
locally optimal solution, 1232
location problem, 1146
log-odds matrix, 1435
log-supermodular, 561
logarithm, 7, 34
logic gate, 373
logic program, 7, 71
logical connective, 13
logical dual, 17
logical identity, 16
logical implication, 7, 63
logically equivalent, 7, 16
logically implies, 16
long-run arrival rate, 539
longest common preﬁx, 1441
longest common subsequence, 1383
longest path, 1493
loop, 579, 584, 883, 914
loop invariant, 7, 68
loop-body, 67
lottery odds, 488
Lov´asz local lemma, 557
Lov´asz, L´aszl´o, 1530
Lovelace, Ada, 1530
low-density parity check code, 1025
low-density parity check matrix, 1037
lower bandwidth, 430
lower bound, 8, 48, 324, 367, 847, 859
lower density, 929, 952, 956
lower envelope, 929, 939
lower extension, 872
lower limit, 196
lower semimodular lattice, 847, 862
lower triangular matrix, 384
LP relaxation, 1146, 1170, 1186, 1192
L-system, 1295
LU decomposition, 384, 425
Lucas numbers, 142, 145
recurrence relation, 183
Lucas, Fran¸cois, 145
Lucas-Lehmer test, 256
 Lukasiewicz, Jan, 1531
Lutz-Nagell theorem, 318
LYM inequality, 867
LYM order, 867
Lyndon’s theorem, 1301
m-ary tree, 694, 699
machine conﬁguration, 1275, 1279, 1281
machine learning, 1389
machine unit, 478, 499
MacMahon, Percy Alexander, 132, 1531
Macmillan’s inequality, 1029
MacWilliams’ identity, 1043
magic graph, 579, 645
Mah¯av¯ıra, 1531
Mahalanobis distance, 1417, 1418
Mahaney’s theorem, 1312
majority rule, 847, 871

INDEX
1569
majority rules consensus tree, 1449
majorization, 443, 446
Makanin’s algorithm, 1300
malicious security, 1126
mammography, 1419
Manhattan distance, 1382
manifold, 971
manifold polyhedron, 929, 971
Manin result, 1054
mantissa, 498
map chromatic number, 579
map coloring, 579, 638
margin, 1389
Mariner 9, 1037
market-basket, 1384
market-basket model, 1385
Markov chain
discrete-time, 518
irreducible, 522
time-homogeneous, 519
Markov property, 527
Markov’s inequality, 552, 1317
Markov, Andrei, 519, 1531
Markowitz pivoting, 384, 430
Markowitz, Harry, 430
Mars, 1037
Mary, Queen of Scots, 1080
matched edge, 724, 734
matched vertex, 724, 734
matching, 579, 623, 687, 724, 734
partial, 1476
stable, 725, 744, 747
matching number, 579, 637, 688
mate, 724, 734, 743
mathematical induction, 8
mathematical programming, 929, 998
matrix
(0, 1), 387, 446
adjacency, 382, 448, 570, 571, 597, 602,
670, 722, 826
admittance, 571
arc length, 754
augmented, 382, 420
balanced, 1192
bidiagonal, 460
circulant, 382, 443
clique, 1498
completely positive, 451
copositive, 456
covariance, 464
diagonal, 383, 405
diagonalizable, 383, 435
directed graph of, 383
dissimilarity, 1394, 1450
doubly stochastic, 383, 451
fully indecomposable, 383, 446
generator, 1036
Hadamard, 882, 901, 902
Hermitian, 384
idempotent, 384, 433
identity, 384
ill-conditioned, 427
incidence, 448, 577, 594, 602, 685, 882,
885, 1192, 1326, 1346
inverse, 384, 407
invertible, 384
irreducible, 384, 443
Laplacian, 384, 448
log-odds, 1435
low-density parity check, 1037
lower triangular, 384
maximal distance separable, 1089
nilpotent, 385, 433
nonnegative, 385, 443
nonsingular, 385, 407
normal, 385, 435
observation probability, 528
of a linear transformation, 384
orthogonal, 385, 407
parity check, 1026, 1037
partial, 456
partly decomposable, 385, 446
payoﬀ, 1210
permutation, 385, 425
positive, 385
positive deﬁnite, 385, 407, 443
positive semideﬁnite, 385, 443, 456
primitive, 385, 451
rank-deﬁcient, 463
reducible, 386, 443
row stochastic, 386, 435
sign-nonsingular, 457
similarity, 404
singular, 386
skew-Hermitian, 386
skew-symmetric, 386
sparse, 386, 405, 430
square, 386, 405
stochastic, 519
strictly diagonally dominant, 386, 438
strictly totally positive, 386, 443
substitution, 1428, 1435

1570
INDEX
symmetric, 387
totally unimodular, 449, 1148, 1192
tournament, 387
transition probability, 520, 527
triangular, 387
tridiagonal, 387
unitary, 387, 407
Vandermonde, 416
well-conditioned, 427
matrix completion, 382
matrix game, 1210
matrix multiplication algorithms, 411
matrix norm, 427
matrix p-norm, 458
matrix, of linear transformation, 401
matrix-tree theorem, 449, 597, 672, 710
matroid, 883, 913
binary, 880, 914
bond, 917
cocycle, 917
cographic, 881, 917
connected, 881, 919
cycle, 881, 914
dual, 881, 917
graphic, 882, 914
oriented, 930
paving, 883, 914
planar, 883, 914
regular, 884, 915
simple, 884, 914
table, 914
ternary, 884, 915
transversal, 884, 914
uniform, 885, 914
vector, 885, 914
matroid basis, 880, 913
matroid circuit, 880, 914
Matyas-Meyer-Oseas construction, 1098
max-ﬂow min-cut theorem, 760
max-min payoﬀ, 1206
max-min strategy, 1206
maximal clique, 1496
maximal codeword, 1484
maximal distance separable matrix, 1089
maximal element, 8, 48, 847, 855
maximal ideal, 327, 350
maximal margin hyperplane, 1389
maximum r-path partition, 643
maximum agreement subtree, 1427, 1449
maximum chain, 855
maximum connectivity, 1458
maximum crosscap number, 659
maximum cut problem, 1235
maximum degree, 1458
maximum distance separable code, 1053
maximum element, 8, 48
maximum ﬂow, 724, 759
maximum genus, 658
maximum likelihood method, 1418
maximum likelihood reconstruction, 1447
maximum size matching, 724
maximum spanning tree, 724, 726, 1403
maximum weight matching, 724
maxterm, 327, 371
Maxwell-Boltzmann model, 478, 487
Mazur’s theorem, 318
McCluskey, Edward, 375
McKay, Brendan D., 628
MD2, 1099
MD4, 1099
MD5, 1099
MD6, 1099
Mealy machine, 1269, 1276
mean, 493
median ﬁnding, 1318
median graph, 679
medical imaging, 1004
meet, 847, 860
meet-in-the-middle (MITM) attack, 1087
meet-irreducible element, 847, 860
member, 22
membership function, 8, 73
membership problem, 1269, 1296
membership table, 8, 26
memory conﬂict, 1284
memoryless channel, 1030
memoryless source, 1025, 1029
m´enage problem, 456
Mendel, Gregor, 485
Menger’s theorem, 616
Menger, Karl, 616
merge sort, 190, 1327
MergeHull algorithm, 976
mergesort algorithm, 1356
Merkle-Damg˚ard construction, 1098
Mersenne number, 254
Mersenne prime(s), 221, 254
table of, 257
Mersenne, Marin, 255, 1531
mesh generation, 1005
message, 1026, 1029

INDEX
1571
message authentication code (MAC), 1073,
1115
message digest, 1097
message space, 1073, 1076
message-digest algorithm family, 1099
metaheuristic, 724, 794, 798, 806
metric, 1382, 1450
metric space, 1146, 1174, 1377, 1382
migration, 507
Milgram, Stanley, 823
Miller, Victor, 1104
Miller-Rabin test, 260
Minc-B´regman inequality, 454
mincut, 478, 510
minimal Boolean expression, 374
minimal cover, 780
minimal element, 8, 48, 847
minimal polynomial, 327, 359, 385, 435
minimum cost ﬂow, 724, 770
minimum crosscap number, 659
minimum cut, 725, 760
minimum degree algorithm, 385, 430
minimum element, 8, 48
minimum embedding dimension, 1488
minimum error probability decoding, 1026,
1034
minimum genus, 658
minimum realizer encoding, 847, 872
minimum spanning forest, 1366, 1370
minimum spanning tree, 725, 726, 1399
minimum spanning tree update algorithm,
1368
minimum sum-of-squares, 1408
minimum sum-of-squares clustering, 1409
minimum support, 1385
minimum support threshold, 1385
minimum universal exponent, 274
Minkowski bound, 306
Minkowski distance, 1382
Minkowski, Hermann, 1532
minor, 385, 415, 579, 590, 919, 929, 949
minpath, 478, 510
minsum matrix multiplication, 754
minterm, 327, 371
mirror image, 1269, 1290, 1293
mix-net voting scheme, 1133
mixed integer programming problem, 1169
mixed strategy, 1146, 1210
M¨obius band, 579, 654
M¨obius function, 87, 132, 221, 272
table, 268
M¨obius, Augustus F., 131
mod, 35
model, 599
model-based method, 1417
modem, 1027
modiﬁed ﬁrst ﬁt decreasing method, 1190
modular intersection theorem, 687
modular lattice, 847, 862
module, 847, 875
modulus, 221, 385
molecular modeling, 1010
moment, 493
moment generating function, 495
monadic, 33
monadic operation, 327
monic polynomial, 303, 354
monogenic ring, 308
monohedral tiling, 930, 958
monoid, 327, 330
monomorphism, 8, 32
monotone function, 508
monotone increasing, 189
monotone vector, 446
monotonic grammar, 1295
Monte Carlo algorithm, 1269, 1316
Monte Carlo simulation, 478, 543
Moore machine, 1270, 1276
Moore-Penrose pseudo-inverse, 463
Mordell-Weil theorem, 318
morphism, 1270, 1293
morphism inverse, 1269, 1293
Motzkin hyperplane, 963
Motzkin path, 1477
move, 1146, 1232, 1241
move value, 1241
mth smallest key, 1318
Muhammad ibn Muhammad, 1528
multi-arc, 579
multi-edge, 579, 585
multi-valued logic, 8, 74
multibranch loop, 1477
multicombination coeﬃcient, 101
multigraph, 579, 585
multinomial coeﬃcient, 87, 101
multinomial random variable, 492
multiple edges, 585
multiple sum, 197
multiple, of an integer, 221, 225
multiplication method, 1363
multiplicative function, 221
multiplicative group, 331

1572
INDEX
multiplicative linear congruential generator,
478, 544
multiplicatively homomorphic, 1118
multiplicity, 23
multiplier, 544
multiplier conjecture, 904
multiplier theorem, 904
multiset, 8, 23
multivariate function, 33
mutation, 1463
mutually disjoint, 8
mutually orthogonal Latin squares, 881, 883,
909
mutually relatively prime, 221, 231
naive Bayes classiﬁer, 1377, 1388
naive set theory, 8, 23
Nakamoto, Satoshi, 1130
NAND, 8
narrow-sense code, 1049
Nash equilibrium, 1206, 1210
Nash, John F., Jr., 1211, 1532
National Institute of Standards and
Technology, 1085
natural logarithm, 34
natural number, 8, 22, 223
Naur, Peter, 1532
NAUTY, 628
near end, 1327, 1341
nearest insertion heuristic, 795
nearest neighbor, 728, 965
nearest neighbor classiﬁer, 1377, 1388
nearest neighbor heuristic, 795
nearest neighbor interchange, 1427, 1447
nearest neighbor interchange distance, 1447
nearly complete, 1327
necklace, 162
negation, 8, 13
negative cycle, 725, 749, 770
negative cycle detection, 751
negative cycle optimality conditions, 770
neighbor, 579, 585, 1236, 1241, 1462
neighbor joining algorithm, 1450
neighborhood, 1146, 1232, 1241, 1462
neighborhood search algorithm, 1146, 1232,
1233
nerve, 1427, 1485
nerve lemma, 1486
nested arcs, 1476
Netﬂix Challenge, 1381
network
Boolean, 1426, 1458
capacitated, 759
directed two-terminal, 737
neural, 1391, 1489
probabilistic Boolean, 1459
random Boolean, 1459
network design problem, 725, 786
network simplex algorithm, 774
neural ideal, 1487
neural network, 1391, 1489
neural ring, 1427, 1487
Newman-Watts model, 725, 812
Newton’s theorem, 193
next operation, 1340
nilpotent matrix, 385, 433
[n, k]-MDS code, 1053
Noether, Emmy, 1533
non-adaptive chosen-ciphertext attack
security, 1073, 1100
non-interactive key distribution, 1121
non-manifold polyhedron, 930, 971
non-repudiation, 1074, 1077
nonbasic variable, 1154
nonbasis matrix, 1154
nonce, 1074, 1129
nonconvex polyhedron, 930, 970
noncooperative game, 1146
noncrossing arcs, 1476
nondeterministic ﬁnite automaton (NFA),
1270, 1274, 1275
nondeterministic Turing machine, 1270,
1282, 1308
nonhomogeneous recurrence relation, 142
nonhomogeneous system, 420
nonimproving move, 1241
nonlinear code, 1026
nonnegative matrix, 385, 443
nonorientable surface, 579, 637, 654
nonparametric method, 1417
nonperiodic tiling, 930, 958
nonplanar graph, 579, 647
nonsingular matrix, 385, 407
nonterminal, 1270, 1294
nonterminal symbol, 1482
nontree edge, 726
nonvertical line, 939
NOP, 8, 64
NOR, 8
Nordhaus-Gaddum inequalities, 632
Nordstrom-Robinson code, 1026, 1056
norm, of a vector, 395

INDEX
1573
normal extension, 327, 359
normal form game, 1205
normal matrix, 385, 435
normal subgroup, 327, 338
normal tiling, 930, 953, 958
normalized characteristic function, 1146
normalized drawing, 579, 597
normalized graph Laplacian, 1412
normalized matching property, 847, 864
normalized spectral clustering, 1413
normalizer, 338
NOT, 8
not AND, 13
not OR, 13
NP, 1270, 1309
NP-complete, 1270, 1302, 1311
NP-hard, 1270, 1302, 1311
NPL, 1309
NTRU cryptosystem, 1074, 1105
nucleolus, 1146
nucleotide, 1427, 1467, 1475
null character, 1429
null graph, 579, 584
null loop, 1477
null pointer, 1327, 1340
null set, 8, 22
null space, 385, 390
nullity, 385, 392, 398
number ﬁeld, 221, 302
number ﬁeld sieve, 221, 263, 309
number of divisors
function, 267
table, 268
numbers
algebraic, 223, 302
complex, 3, 23
irrational, 223
natural, 223
rational, 223
transcendental, 223
numeric datatype, 1327, 1331
numerical rank, 463
numerically stable algorithm, 385, 422
numerically unstable algorithm, 385, 422
objective function, 1146, 1149
oblivious transfer, 1074, 1124
obnoxious facility location problem, 1183
observation probability matrix, 528
occupancy problem, 502
octahedral graph, 580, 593
octal expansion, 227
octal representation, 221
octic group, 327, 345
odd cycle, 1403
odd permutation, 327, 344
odd vertex, 725, 740
odd-cycle property, 623
OEIS, 162, 660, 712
Ogden’s pumping lemma, 1299
omega notation, 8
on-line partitioning algorithm, 865
one’s complement, 228
one’s complement expansion, 221
one-dimensional cellular automaton, 1462
one-dimensional simple random walk, 503
one-out-of-many oblivious transfer, 1125
one-out-of-two oblivious transfer, 1124
one-point crossover, 1464
one-point removal theorem, 873
one-time pad, 1074, 1091
one-to-one correspondence, 33
one-to-one function, 8, 32
one-way hash function, 1097
one-way linked list, 1327, 1341
onto function(s), 8, 32
generating function, 179
recurrence relation, 181
open cover, 1485
open half-space, 935
open hash table, 1327
open neighborhood, 807
operating state, 508
operation
binary, 329
monadic, 329
n-ary, 33, 329
unary, 33, 329
optical character recognition, 1392
optimal alignment, 1430
optimal code, 1053
optimal diﬀusion structure, 1089
optimal mixed strategy, 1210
optimal project compression problem, 1195
optimal solution, 1146
optimal substructure, 1405
OR, 8
OR bid language, 1224
oracle, 930, 1000, 1270, 1309
oracle Turing machine, 1270, 1309
orbit, 125, 327, 344, 1456
order, 327, 331, 344

1574
INDEX
of an element, 339
of an integer, 221, 274
order dimension, 872
order ideal, 858
order module, 848, 875
order relation, 29, 367, 848, 851
order statistic, 547
order type, 930, 936, 961
order-preserving mapping, 848, 852
ordered basis, 934
ordered datatype, 1327, 1332
ordered pair-action group, 346
ordered pair-permutation, 346
ordered selection, 87, 99
with replacement, 104
ordered simplex, 934
ordered tree, 695, 698, 717
ordinal number, 28
ordinary crossing, 963
ordinary generating function, 86, 87, 143
ordinary hyperplane, 963
ordinary line, 963
orientable surface, 580, 637, 654
orientation, 580, 607
oriented arrangement, 944
oriented hyperplane, 935
oriented line, 939
oriented matroid, 930, 947–949
origin vertex, 786
orthogonal array, 883, 912
orthogonal complement, 395, 1036
orthogonal drawing, 652
orthogonal Latin squares, 883, 909
orthogonal matrix, 385, 407
orthogonal range reporting, 988
orthogonal set, 395
orthogonal vectors, 385, 395
orthonormal set, 395
orthonormal vectors, 385
oscillation, 1247
out-degree, 580, 586, 602
outer corner, 135
outer product, 467
outﬂow, 761
outlier, 1377, 1379, 1417
outlier detection, 1377, 1379
Outlier Detection DataSets (ODDS), 1419
output feedback mode, 1074, 1090
output function, 1276, 1281
output stochastic process, 547
ovals of Cassini, 438
overall parity check symbol, 1040
overdetermined system, 463
overﬂow, 478, 499
overlap, 1469, 1470
overlap graph, 1427, 1469
overlap-layout-consensus method, 1427
overlapping subproblems, 1405
overpartition, 87, 117
P, 1270, 1309
#P, 1313
p-center, 1144, 1175
p-median, 1146, 1175
p-median theorem, 1176
P/poly, 1310
packing, 930, 952, 954, 1146, 1190, 1397
Paillier cryptosystem, 1074, 1107
paired kidney donations, 736
pairing
bilinear, 316
Tate-Lichtenbaum, 317
Weil, 317
pairwise balanced design (PBD), 883, 893
pairwise disjoint, 8
pairwise disjoint events, 481
pairwise relatively prime, 221, 231
palindrome, 221, 1270, 1290, 1291
paradox, 8
parallel algorithm, 930, 992
parallel class, 883
parallel computation model, 1270, 1284
parallel edges, 585, 586
parallel elements, 883, 914
parallel machine, 992
parallel random access machine, 930, 992
parallel random access machine (PRAM),
1271, 1284
parallel system, 478, 511
parallel time-complexity, 1302
parametric method, 1417
parametric search, 930, 995
parent, 696, 698, 1463
parentheses, well-formed sequences, 149
Pareto-optimal solution, 1206
parity check bit, 1026, 1032
parity check matrix, 1026, 1036, 1037
construction of, 1037
parity stream, 1061
parsimonious reduction, 1313
parsing a string, 1270, 1295
partial function, 9, 33

INDEX
1575
partial function, of a TM, 1270, 1281, 1287
partial hypergraph, 580, 684
partial Latin square, 883, 906
partial matching, 1476
partial matrix, 456
partial order, 9, 47
partial ordering, 851
partial pivoting, 385, 429
partial quotient, 221, 296
partial recursive function, 1270, 1287
partial sum, 197
partial transversal, 883, 905
partially dynamic algorithm, 1366
partially ordered set (poset), 9, 47, 87, 848,
851
partite sets, 592, 687
partition, 1397
partition lattice, 848, 860
partition(s), 9, 25, 87, 117, 176
cyclic, 121
number of, 204
vector, 117
partition-cut, 696, 706
partitioning problem, 1236
partly decomposable matrix, 385, 446
Pascal’s recursion, 107
Pascal’s triangle, 87, 101
Pascal, Blaise, 1533
passive adversary, 1074, 1078
passive security, 1116
password-based key exchange, 1117
PASTA, 539
path, 503, 580, 592, 595, 613, 697
directed, 580
Euler, 1427, 1472
Hamilton, 576, 620, 1427, 1469
Motzkin, 1477
simple, 479
path exchange, 1237
path exchange neighborhood, 1237
path factor, 623
path graph, 1428, 1431
path optimality conditions, 727
path relinking, 1146, 1249
pattern, 1270, 1299
pattern inventory, 87, 129
Pauli group, 1063
paving matroid, 883, 914
payoﬀfunction, 1146, 1205
payoﬀmatrix, 1210
PBD-closure, 883, 893
Peano deﬁnition, 9, 28
Peano, Giuseppe, 28, 1533
Pearson, Karl, 507
pebbling number, 580, 643
pebbling step, 580, 643
Peirce, Charles Sanders, 1533
Pell’s equation, 221, 292
table, 293
Pell, John, 292
Pepin’s criterion, 255
perceptron, 1388, 1389
perfect code, 1026, 1055
perfect graph, 580, 623, 669
perfect information, 1147, 1205
perfect integer, 267
perfect matching, 580, 725, 734
perfect number, 221, 254
perfect secrecy, 1074, 1075, 1078
period, 228, 296
period of a state, 522
periodic base b expansion, 221, 228
periodic state, 478
periodic tiling, 930, 958
permanent, 385, 454
permutation, 35, 87, 93, 99, 124, 327, 343
cyclic, 124
discordant, 141
even, 325, 344
k-, 87, 99
m-, 93
odd, 327, 344
sign of, 328, 344
permutation cipher, 1080
permutation coeﬃcient, 87, 99
permutation graph, 848, 875
permutation group, 87, 124, 327, 344
permutation matrix, 385, 425
permutation-with-replacement, 87, 105
permutation/diﬀusion, 1084
Perron root, 385
Perron’s theorem, 1222
Perron-Frobenius theorem, 444
PERT model, 1147, 1201
Petersen graph, 580, 593, 596
Petersen’s theorem, 623
Petersen, Julius, 623
phenotype, 1463
phrase-structure grammar, 1294
phylogenetic reconstruction, 1446
phylogenetic tree, 1398, 1428, 1446
pigeonhole principle, 95

1576
INDEX
generalized, 96
set-theoretic form, 96
Πp
n, 1310
pivot, 385, 422, 1147, 1154, 1327, 1354
PL, 1309
plagiarism, 1387
plaintext, 1074, 1076
plaintext awareness, 1107
plaintext space, 1076
planar graph, 580, 647
planar matroid, 883, 914
planar poset, 848, 852
planar straight-line graph, 930, 977
planarity, 1366, 1371
planarity testing algorithm, 650
plane, 884, 896
plane graph, 1327, 1367
plane tree, 699
planetary motion, 508
Platonic graph, 580, 596
Platonic solid, 580, 593
Playfair cipher, 1080
plurality consensus function, 848, 871
point conﬁguration, 930, 935, 936
point location algorithms, 985
point location problem, 930, 983
point-bracket notation, 1477
pointer, 1327, 1340
Poisson process, 537
Poisson random variable, 478, 810
poker hands, 104
probabilities of, 483
polar decomposition, 459
polar-duality, 930, 940
Polish notation, 9, 14, 703
Pollard’s algorithm, 263
Pollard’s rho attack, 1104
P´olya’s theorem, 506
P´olya’s urn scheme, 487
P´olya, George, 125, 487, 506, 660, 1533
polyalphabetic substitution cipher, 1074,
1080
polychromatic number, 968
polygram substitution cipher, 1074, 1080
polyhedron, 449, 580, 930, 1147, 1170
polyline drawing, 652
polynomial, 328
over a ring, 353
reducible, 328
separable, 328
polynomial dynamical system, 1456
polynomial hierarchy, 1271, 1310
polynomial hierarchy collapse, 1267, 1310
polynomial interpolation non-interactive
key distribution, 1122
polynomial ring, 328, 349, 354, 1456
polynomial space, 1271, 1302, 1309
polynomial time, 1271, 1302, 1309
polynomial-time reduction, 1311, 1313
polyomino, 142, 162
polytomy, 1428, 1446
polytope, 449, 930, 961, 970, 999, 1000
Pomerance, Carl, 259264
pop, 1334
population, 1463
poset, 9, 47, 87, 848, 851
bipartite, 844, 855
bounded, 844
divisibility, 853
graded, 846, 857
induced, 852
locally ﬁnite, 847, 852
planar, 848, 852
ranked, 848, 857
self-dual, 848, 855
poset dimension, 845
poset extension, 845, 872
poset width, 850, 857
positive closure, 1271, 1291
positive deﬁnite matrix, 385, 407, 443
positive matrix, 385
positive recurrent state, 522
positive semideﬁnite matrix, 385, 443, 456
postcondition, 9, 63, 1327, 1332
postﬁx notation, 9, 14, 703
postorder traversal, 696, 703
postulate, 23
potential function, 1159
Poussin, Charles de la Vall´ee, 251
power
of a language, 1271, 1290, 1291
of a matrix, 385
of a relation, 9, 43
power index, 1314
power method, 440
power set, 9, 25
power sum, 142, 161, 200
power-law degree distribution, 725, 817
powerful integer, 221, 225
PP, 1313
PRAM memory conﬂict, 1271
pre-image, 32

INDEX
1577
pre-period, 228, 296
precedence relation, 1195
precondition, 9, 63, 1327, 1332
predator projection graph, 1428, 1501
predecessor, 725
predecessor function, 749, 829
predicate, 9, 19
n-ary, 8, 70
predicate calculus, 9, 19
predicate logic, 19
predictor, 1459
preﬁx, 1290, 1467
preﬁx code, 699, 1029
preﬁx notation, 9, 14, 703
preﬂow, 725, 761
preﬂow-push algorithm, 763
preimage-resistance, 1096
premise, 9, 51
prenex normal form, 9, 20
preorder, 9, 47
preorder traversal, 696, 703
Preparata code, 1026, 1056
Pretty Good Privacy (PGP), 1102
prewhitening roundkey, 1088
prey projection graph, 1428, 1501
Price’s model, 820
Prim’s algorithm, 729
primal LP, 1162
primality proof, 259
primality test, 221, 1318
probabilistic, 222
primary key, 9, 50, 1328, 1348
primary operation, 1328, 1331
prime counting function, 250
table, 252
prime factorization, 221
prime ideal, 328, 350
prime number theorem, 221, 251
prime(s), 221, 225, 240
Fermat, 254
irregular, 291, 309
Mersenne, 255
regular, 291, 309
table of, 241
twin, 223, 250, 253
prime-power factorization, 221, 244
table, 244
primitive code, 1049
primitive element, 363
primitive matrix, 385, 451
primitive recursion, 1271, 1287
primitive recursive function, 1271, 1287
primitive root(s), 221
modulo m, 274
table of, 275
principal component analysis, 465
principal ideal, 350
principal ideal domain, 352
principal minor, 385, 415
principal submatrix, 386, 405
principal subtree, 699, 829
principle of mathematical induction, 59
priority queue, 700, 1328, 1339
priority tree, 1328, 1344
prism, 670
private key, 1100
probabilistic activity net, 1147, 1201
probabilistic Boolean network, 1459
probabilistic encryption, 1074, 1077
probabilistic method, 1316
probabilistic Turing machine, 1271, 1312
probability, 478
probability generating function, 495
probability measure, 481
probability space, 872
probe function, 1328
probe sequence, 1328, 1364
probl`eme des m´enages, 87
probl`eme des rencontres, 87
problem reduction, 1272, 1307
procedure, 69
body, 69
name, 69
product
of cardinals, 29
of matrices, 386
of ordinals, 29
product cipher, 1074, 1084
product ring, 349
product-of-sums expansion, 371
production rule, 9, 78, 1271, 1295
production system, 9, 78
proﬁle, 848, 871
program construct, 4, 9
program fragment, 9
program semantics, 9
project planning, 1195
projection function, 10, 35, 50, 1271, 1286
projection matrix, 397
projection vector, 396, 1159
projective arrangement, 940
projective line, 940

1578
INDEX
projective line arrangement, 931, 940
projective plane, 654, 881, 884, 888, 897–
899, 940
projective space, 884, 896
proof, 10, 54
by contradiction, 10, 54
direct, 5
done by hand, 10, 79
indirect, 6, 54
proof assistant, 79
proof-of-work, 1131
proper ancestor, 698
proper descendant, 698
proper divisor, 225
proper hypergeometric term, 210
proper labeling, 1218
proper subgroup, 328, 336
proper subset, 10, 23
proper vertex coloring, 631
Property B, 551, 553
proposition, 10, 13, 51
compound, 4, 13
simple, 13
propositional calculus, 10, 13
propositional logic, 13
propositional variable, 13
prototile, 931, 958
provable security, 1074
provably secure, 1078
proximity measure, 1247, 1381
proximity-based method, 1417
pseudo polar-duality, 932, 950
pseudo-monomial, 1487
pseudo-random number generator, 222
pseudo-random numbers, 478, 544
pseudocircle, 963
pseudoﬂow, 725, 770
pseudograph, 580, 585
pseudoknot structure, 1476
pseudoline, 943, 963
pseudoline arrangement, 931, 943
pseudopalindrome, 1290
pseudoprime, 222, 259
PSPACE, 1271, 1309
public key, 1100
public key certiﬁcate, 1074
public key cryptosystem, 1100
public key only attack, 1112
public key system, 1074
pumping lemma, 1271, 1298
punctured code, 1026, 1040
pure strategy, 1147, 1205
purine, 1475
push, 1334
pushdown automaton (PDA), 1271, 1274,
1278
pyrimidine, 1475
Pythagorean triple(s), 222, 289
primitive, 289
table of, 289
q-ary symmetric channel, 1030
Qin Jiushao, 1534
QR factorization, 437
QR method, 440
quadratic assignment problem, 725, 803
quadratic ﬁeld, 222, 303
imaginary, 303
real, 303
quadratic integer, 303
quadratic irrational, 222
quadratic nonresidue, 222, 284
quadratic probing, 1364
quadratic reciprocity, 222, 285
quadratic residue, 222, 284
quadratic residuosity problem, 1108
quadratic sieve, 222, 262
qualitative class, 456
quantum attack, 1074
quantum code, 1062
quantum computing, 1102
quantum cryptography, 1074
quantum state, 1062
quantum system, 1062
quartet tree, 1453
quasi-transitive orientation, 848, 875
quasicrystals, 999
quaternion group, 333
qubit, 1026, 1062
qudit, 1063
query, 1328, 1331, 1365
queue, 536, 1328, 1334
queue capacity, 478, 536
queue discipline, 479
queue-length process, 479, 538
queueing system, 479, 536
QuickHull algorithm, 975
quicksort, 1306, 1328, 1354
quicksort algorithm, 1356
Quine, Willard Van Orman, 375, 1534
Quine-McCluskey method, 377
quotient, 225

INDEX
1579
quotient group, 328, 338
quotient ring, 328, 351
Rabin, Michael, 260
radius, 580, 613, 1394
radius of convergence, 142, 203
radix, 227
radix sort, 1328, 1357
radix sort algorithm, 1358
Rado, Richard, 1534
Radon partition, 931, 947
Radon’s theorem, 937
Ramachandran’s conjecture, 630
Ramanujan graph, 581, 675
Ramanujan, Srinivasa, 1534
Ramsey number, 142, 154, 479, 551, 552,
554, 555, 581, 681
Ramsey point set, 968
Ramsey’s theorem, 154
Ramsey, Frank, 154, 1535
random access list, 1328, 1338
random access machine (RAM), 1271, 1284
random Boolean network, 1459
random graph, 581, 682, 725
generation algorithm, 683
models, 724, 810, 817
random number generator, 544
random numbers, 479, 544
random variable(s), 479, 490
Bernoulli, 476, 492
binomial, 476, 492
continuous, 490
discrete, 490
Erlang, 537
exponential, 537
geometric, 477
hypergeometric, 478, 492
independent, 478, 490
multinomial, 492
Poisson, 478
uniform, 480
Weibull, 546
random vector, 490
random walk, 183, 479, 502, 519
one-dimensional, 503
recurrent, 502
simple, 502
symmetric, 502, 503
transient, 502
randomized algorithm, 931, 993, 1271,
1315
randomized divide-and-conquer, 994
randomized quicksort, 1306
range, 10, 32, 386, 398
range counting problem, 931, 985
range emptiness problem, 931, 985
range problem, 502
range reporting algorithms, 986
range reporting problem, 931, 985
range searching algorithms, 986
range searching problems, 985
range tree algorithm, 987
rank, 386, 392, 398, 418, 848, 857, 884, 914,
1328, 1357
rank axioms, 884, 917
rank parameters, 848, 857
rank-based friendship, 815
rank-counting sort, 1328, 1357
rank-deﬁcient matrix, 463
rank-nullity theorem, 393
ranked poset, 848, 857
ranked tree, 1452
ranking, 848, 868
rational number, 10, 22, 222, 223
ray, 931, 989
ray shooting algorithms, 989, 991
ray shooting problem, 931, 989
ray tracing, 1003
RC4 cipher, 1094
reachable vertex, 581, 605, 613
read, 1467
real number, 10, 23
real random access machine, 931
realizable arrangement, 944
realizer, 848, 872
reasonable set, 1213
receiver, 586
recency-based memory, 1147, 1241
receptive ﬁeld, 1485
recombination, 1463
recommendation algorithms, 1396
recommender system, 465, 1380
reconstructible invariant, 581, 630
record, 1328, 1348, 1362
rectilinear distance, 1182
recurrence relation, 142, 143, 180
constant coeﬃcients, 180
ﬁrst-order, 141
homogeneous, 141, 180
kth-order, 180
linear, 142, 180
nonhomogeneous, 142, 180, 185

1580
INDEX
nonlinear, 180
second-order, 142
recurrent random walk, 502
recurrent state, 479, 522
recurrent walk, 479, 504
recursion, 1405
recursive deﬁnition, 37
of a function, 10
of a set, 10
recursive description, 22
recursive function, 1287
recursive set/language, 1271, 1289, 1296
recursive solution, 1479
recursive systematic convolutional code,
1060
recursively enumerable, 1271, 1289
Redﬁeld, John Howard, 130
reduced cost, 725, 770, 1147, 1154
reduced cost optimality conditions, 770
reduced Latin square, 884, 905
reduced system of residues, 222, 236
reduced tree, 696, 712
reduced walk, 696
reduced word, 342
reducibility, 1271, 1302
reducible Markov chain, 522
reducible matrix, 386, 443
reducible polynomial, 326, 328, 354
redundant constraint, 1147, 1149
Reed-Muller code, 1026, 1041
ﬁrst-order, 1041
kth-order, 1041
Reed-Solomon code, 1026, 1051
reﬁnement, 10, 25
of a partition, 848, 860
reﬂecting boundary, 479, 502, 504
reﬂection principle, 503
reﬂex edge, 931, 971
reﬂexive property, 10, 42
regenerator location problem, 725, 791
region, 581, 649
regular covering, 848, 865
regular expression, 1272, 1298
regular grammar, 1272, 1295, 1482
regular graph, 581, 585, 610, 685
regular language, 1272, 1293
regular lattice, 725, 812
regular matroid, 884, 915
regular polygon, 931, 958
regular polytope, 931, 971
regular tiling, 931, 958
regular tree, 696
regularity problem, 1272, 1296
rehashing, 1363
related-key diﬀerential cryptanalysis, 1089
relation, 10, 42
binary, 3, 42
converse, 4
equivalence, 5
n-ary, 8, 50
relational database, 1379
relative error, 479, 499
relatively prime, 222, 231
relator, 342
reliability, 479, 508, 509
remainder, 222, 225
rencontre number, 112
reorientation, 935, 936
repetitive branch-and-bound, 1406
replication, 548
replication number, 884, 885
repudiated signature, 1077, 1112
residence count, 1245
residual capacity, 725, 762, 770
residual network, 725, 762, 770
resolution, 884, 890
resolvable design, 890
table, 890
restriction, 33
of a function, 10
retract, 679
return instruction, 69
revealed preference, 1230
reverse auction, 1223
reverse Polish notation, 10, 14, 703
reverse, of a string, 1272, 1290, 1293
review ﬁltering, 1419
Rice’s theorem, 1297
Riemann hypothesis, 252
Riemann, Bernhard, 252
right child, 696, 699, 1328, 1342
right coset, 325, 338
right derivative, 1293
right divisor of zero, 328, 352
right null space, 390
right quotient, 1293
right singular vector, 458
right subtree, 696, 699
right-linear grammar, 1295
Rijndael, 1087
ring, 328, 347
commutative, 325, 348

INDEX
1581
division, 325
polynomial, 328, 349, 354
product, 349
quotient, 328, 351
with unity, 328, 348
ring automorphism, 350
ring endomorphism, 350
ring homomorphism, 350
rising power, 142, 192
Rivest, Ronald, 1100, 1535
RNA sequence, 1428, 1476
RNA structure, 1476
Robbins’ theorem, 607
Robbins, Herbert, 81
Robinson-Foulds distance, 1428, 1447
robotics, 1000
root, 95, 698
root cell, 1328, 1342
root ﬁeld, 328, 359
root vertex, 779
rooted pebbling number, 581, 643
rooted tree, 696, 698, 712
roster, 22
rotation system, 581, 658
Roth’s theorem, 301
Roth, Alvin, 745, 1535
rotor machine, 1082
round function, 1074, 1083, 1085
round-robin tournament, 665
roundoﬀerror, 386, 422, 479, 498
row rank, 392
row space, 392
row stochastic matrix, 386, 435
Roy, Bernard, 45
RSA assumption, 1102
RSA cryptosystem, 1074, 1100
RSA factoring challenge, 1102
RSA signature scheme, 1113
rule
of inference, 10, 52
of product, 93
of quotient, 93
of sum, 93
Rumely, Robert, 260
running time, 1272
Russell, Bertrand, 24, 1535
Ryser’s theorem, 447
S-box, 1084
Saalsch¨utz’s identity, 209
Sahari-Waters attribute-based encryption,
1111
Samaw’al ibn Yahy¯a ibn Yah¯uda al-Maghrib¯ı
al-, 1535
sample size, 479, 480
sample space, 479, 480
satisﬁability problem, 1303
satisﬁable proposition, 10, 16
saturated arc, 759
savings, 725
savings heuristic, 780
scalar, 386, 388
scalar multiple, 386
scalar multiplication, 400
scalar product, 386, 400, 407
scanning a database, 1328, 1349
Schauder’s ﬁxed-point theorem, 1221
Schmidt, Erhardt, 396
Schreier graph, 581, 673
Schr¨oder-Bernstein theorem, 30
Schur number, 154
Schur’s majorization theorem, 444
Schur’s theorem, 156
Schur, Issai, 154, 1536
scope, 10, 20
score vector, 446, 610
Scott-Suppes theorem, 869
scytale, 1075, 1081
SDR, 25
searching, 1328, 1348, 1358
second-order linear recurrence relation
with constant coeﬃcients, 142
secondary hash function, 1364
secondary key, 1328, 1348
secondary operation, 1328, 1331
secondary structure, 1428, 1476
secret sharing, 1075, 1119
secure hash algorithm (SHA), 1075, 1099
secure multiparty computation, 1075, 1126
security
active, 1116
collusion-resistant, 1111
computational, 1071, 1078
malicious, 1126
non-adaptive chosen-ciphertext attack,
1073
passive, 1116
provable, 1074, 1078
semi-honest, 1126
unconditional, 1074, 1075, 1078, 1079
seed, 544, 1428, 1437

1582
INDEX
Selberg, Atle, 251
selection
ordered, 99
unordered, 101
selection sort, 1329, 1350
selection sort algorithm, 1351
self-complementary graph, 581, 591
self-contradiction, 16
self-dual code, 1026, 1036
self-dual poset, 848, 855
self-generating sequence, 160
self-orthogonal code, 1026, 1036
self-synchronizing stream cipher, 1075, 1092
Selfridge, J. L., 260
semantic axiom, 2, 10, 63
semi-honest security, 1126
semi-Markov process, 1202
semi-space, 931, 936
semigroup, 328, 330
semimodular lattice, 848
semiorder, 848, 868
semiorder representation, 868
semiregular polyhedron, 931, 958
semiregular tiling, 931, 958
semistandard Young tableau, 134
sensitivity analysis, 1164
sentence, 10, 20
separable extension, 328, 359
separable polynomial, 328, 359
separated set, 966
separation, 1170, 1394
sequence, 10, 142, 143, 1429
sequence of, 1329, 1331
sequencing by hybridization, 1467
sequential sampling, 508
series system, 479, 511
series-parallel system, 515
service discipline, 536
service-time distribution, 479, 536
session key, 1116
set, 11, 22
set cover, 1147
set covering algorithm
greedy heuristic, 1192
implicit enumeration, 1193
randomized greedy heuristic, 1193
set covering problem, 1147, 1192
set diﬀerence, 25
set equation, 25
set expression, 25
set identity, 25
set mapping, 1220
set of, 1329, 1331
set paradox, 23
set partition, 1147
set partitioning problem, 1147, 1192
set-up cost, 784
SHA-0, 1099
SHA-1, 1099
SHA-2, 1099
SHA-3, 1099
shadow, 849
shakersort, 1329, 1352
Shamir’s secret sharing scheme, 1119
Shamir, Adi, 1100, 1536
Shannon capacity, 679
Shannon’s noiseless coding theorem, 1029
Shannon’s noisy channel coding theorem,
1035
Shannon, Claude Elwood, 1084, 1536
shape, of a Young tableau, 134
Shapley value, 1147, 1213
Shapley, Lloyd, 745, 1536
shared key, 1116
Shell, Donald L., 1350
Shellsort, 1329, 1350
Sherman-Morrison identity, 419
shift cipher, 1075, 1079
shift left operation, 411
shift operator, 142, 191
shift register, 1058
shift up operation, 411
shingle, 1383
Shor’s algorithm, 265
Shor, Peter, 265
shortened code, 1026, 1040
shortest common superstring, 1428, 1469
shortest path, 725, 749, 1492
shortest path optimality conditions, 749
shortest path tree, 749
shortest vector problem, 1106
shotgun sequencing, 1428, 1467
shrunken blossom, 741
shuﬄe, 1290, 1293
siblings, 696, 698
Sierpi´nski, Wac law, 253
sieve of Eratosthenes, 222, 251
sieve principle, 481
Σp
n, 1310
sigma expression, 196
sign pattern, 386, 456
sign, of a permutation, 328, 344

INDEX
1583
sign-nonsingular matrix, 457
sign-solvable system, 457
signed double covering, 946
signed permutation, 948
signed set, 946
signing key, 1112
SIMAN, 550
similar hypergeometric terms, 142
similar matrices, 386, 404
similarity measure, 1378, 1381
simple design, 884, 892
simple group, 328, 341
simple matroid, 884, 914
simple path, 479, 510, 791
simple random walk, 502
simple substitution cipher, 1079
simple Vigen`ere cipher, 1080
simplex, 932, 934, 972, 1147, 1210, 1217
simplex algorithm, 1147, 1156, 1157
simplicial complex, 932, 972, 1218, 1484
simplicial polytope, 932, 974
simplicial subdivision, 1147, 1218
SIMSCRIPT, 550
SIMULA, 550
simulation, 479
simultaneous ascending auction, 1229
simultaneous multi-round auction, 1229
single-elimination competition, 610
single-linkage algorithm, 1399
single-station queueing system, 479, 537
singleton, 11, 22
Singleton bound, 1053
singular matrix, 386
singular value decomposition, 386, 437, 458
computation, 460
Golub-Kahan bidiagonalization, 461
higher-order, 467
two-sided Jacobi, 462
singular values, 386, 435, 458
sink, 581, 586, 609, 759
sink food web, 1498
sinking sort, 1329, 1352
Sister Celine’s algorithm, 210
six degrees of separation, 823
skeleton, 1218
skew ﬁeld, 328
skew-Hermitian matrix, 386
skew-symmetric matrix, 386
Skipjack, 1085
slack variable, 1147, 1154
SLAM, 550
Sloane, Neal J. A., 162
slope order, 950
smallest element, 367
smooth number, 222, 261
S-norm operator, 73
SNOW cipher, 1094
social choice function, 871
soft decision decoder, 1057
soft dodecahedral conjecture, 955
software for
association rules, 1386
classiﬁcation, 1391
clustering, 1399, 1409
coding theory, 1032
combinatorial codes, 1487
computational geometry, 1006, 1008,
1010, 1011
data mining, 1380, 1386, 1391, 1419
detecting outliers, 1419
ecological modeling, 1491
integer programming, 1172
knapsack problems, 1188
linear algebra, 442
linear programming, 1158, 1161, 1163
network optimization, 730, 742, 752,
764, 774
primality testing, 261
project planning, 1197
pseudo-random number generation, 544
RNA folding, 1480
simulation, 551
spectral clustering, 1414
Sollin’s algorithm, 729
solvable by radicals, 362
solvable conﬁguration, 643
solvable group, 341
sorting, 1285, 1348
sorting algorithm, 1329
sorting methods
table of, 1348
source, 581, 586, 609, 759, 1029
source encoder, 1027
source food web, 1498
space complexity, 1272, 1302, 1309
spaced seed, 1437
spam, 1391
span, 386, 392
spanning set, 386, 392, 884, 914
spanning subgraph, 581, 589
spanning tree(s), 696, 705, 772
number of, 709

1584
INDEX
sparse certiﬁcate, 1329, 1367
sparse language, 1272, 1310
sparse matrix, 386, 405, 430
sparse sequence, 1329, 1341
sparsiﬁcation method, 1368
speciﬁcation, 11
spectral clustering, 1378, 1412
spectral gap, 675
spectral radius, 386, 438
spectral theorem, for normal matrices, 436
spectrum, 582, 671
spectrum allocation, 1224
Sperner family, 855
Sperner property, 849, 864
Sperner’s bound, 512
Sperner’s lemma, 687, 1218
Sperner’s theorem, 858
Sperner, Emanuel, 1536
sphere packing, 954
sphere-packing bound, 1053
spike, 1484
spike train, 1484
split, 1395, 1447
splitting ﬁeld, 328, 359
sporadic group, 341
square matrix, 386, 405
square root
modulo m, 222, 286
square, of a word, 1272, 1300
square-free integer, 222, 225
square-free word, 1272, 1300
SSYT, 134
s-t cutset, 479, 510
stability condition, 479
stabilizer code, 1026, 1063
stable certiﬁcate, 1329, 1367
stable function, 1329, 1367
stable matching, 725, 744, 747
stable partners, 725, 744
stable process, 526
stack, 1329, 1334, 1476
stacked pair, 1477
standard deviation, 493
standard form, 1036
standard k-chain, 849
standardized form, 142, 196
star, 948, 1394
star polygon, 982
start symbol, 1272, 1294, 1482
starting state, 1274, 1276, 1278, 1280
state, 1456
state diagram, 1272, 1275, 1279
state-space enumeration, 513
state-space graph, 1428, 1456
statement, 20
statement form, 11, 51
static structure, 1329, 1349
s-t cut, 725, 760, 779
steady-state distribution, 479, 539
steganography, 1075–1077
Steiner quadruple system, 892
Steiner system, 892
Steiner tree, 789
Steiner triple system, 884–886, 890, 892
Steiner vertex, 789
Steinitz exchange axiom, 849, 863
stereographic projection, 652
stimulus space, 1485
Stirling cycle number, 87, 121
Stirling number
of the ﬁrst kind, 88, 121
of the second kind, 88, 121
Stirling subset number, 88, 121, 861
Stirling’s approximation, 142, 203
Stirling, James, 1537
stochastic context-free grammar, 1482
stochastic matrix, 519
stochastic process, 479, 501
stock market prices, 508
stop word, 1386
straight-line drawing, 652
straight-line dual, 932, 979
straightedge and compass constructibility,
362
straightforward bidding, 1230
Strassen’s algorithm, 413
Strassen, Volker, 413
strategic behavior, 1147
strategic form game, 1205
strategic oscillation, 1147, 1247
strategy combination, 1147
Straus’ conjecture, 558
stream cipher, 1075, 1091
stretchable arrangement, 944
strict convexity, 932, 961
strict digraph, 586
strict Sperner property, 849, 864
strictly diagonally dominant matrix, 386,
438
strictly totally positive matrix, 386, 443
string, 11, 37
string matching, 1440

INDEX
1585
strong certiﬁcate, 1329, 1367
strong collision-resistance, 1097
strong component, 582, 605
strong duality theorem, 1162
strong orientation, 582, 607
strong orientation algorithm, 607, 608
strong perfect graph theorem, 670
strong probable prime test, 260
strong product, 582, 677
strong pseudoprime, 259
strong Sperner property, 849, 864
strong tournament, 582
strongly correct code, 11, 68
strongly regular graph, 582, 671
structure function, 480, 508
structure matrix, 446
structure probability, 1482
subdesign, 884, 890
subﬁeld, 328, 356
subgame, 1205
subgame perfect equilibrium, 1206
subgroup, 328, 336
generated by elements, 328
sublattice, 366, 849, 860
submatrix, 386, 405
submodular function, 849, 862
submonoid, 330
subposet, 849, 852
subring, 328, 349
subsemigroup, 330
subset, 11, 23
subset lattice, 849, 860
subspace, 386, 389
subspace lattice, 849, 860
subsquare, 884, 906
substitution, 11, 70, 1272, 1293, 1430
substitution box (S-box), 1088
substitution cipher, 1075, 1084
substitution matrix, 1428, 1435
substitution permutation network, 1089
substitution score, 1430
substitution/confusion, 1084
substring, 1290
subtraction, 348
subtree, 696, 779
subword, 1290
success ampliﬁcation method, 1316
successive shortest path algorithm, 773
successor function, 1286
succinct certiﬁcate, 1310
suﬃx, 1290, 1467
suﬃx array, 1428, 1440
suﬃx link, 1440
suﬃx tree, 1428, 1440
sum
of cardinals, 29
of matrices, 387
of ordinals, 28
of subspaces, 387, 390
sum of divisors
function, 267
table, 268
sum-free set, 551, 553
sum-of-products expansion, 371
summation variable, 196
summatory function, 266
Sun Zi, 1537
super normal form, 1299
supermagic graph, 582, 645
supervised learning, 1378, 1389
supplementary bidding phase, 1229
supply vertex, 769
support, 1378, 1384, 1385, 1453, 1456
support monotonicity property, 1385
support threshold, 1384
support vector machine, 1389
supremum distance, 1382
surjection, 11, 32
surveillance problems, 983
survivable network, 726
Sylow p-subgroup, 329, 340
Sylow subgroup, 340
Sylow’s theorem, 340
Sylow, Peter Ludvig Mejdell, 340
Sylvester’s law of inertia, 444
Sylvester, James Joseph, 1537
Sylvester-Gallai theorem, 963
symmetric chain, 687, 849, 864
symmetric chain order, 849, 864
symmetric design, 884, 897
symmetric diﬀerence, 25
of relations, 11, 43
of sets, 11
symmetric group, 329, 333, 344, 662
symmetric key system, 1075, 1076
symmetric matrix, 387
symmetric random walk, 502, 503
symmetric tiling, 932
symmetry, 11, 42, 88, 1382
of a ﬁgure, 125
symmetry breaking, 1316
symmetry group, 958

1586
INDEX
symmetry motion, 125
symptoms, 1391
synchronous stream cipher, 1075, 1092
syndrome, 1026, 1038
syndrome decoding, 1039
syndrome decoding algorithm, 1039
syndrome polynomial, 1044
system of distinct representatives (SDR),
11, 25, 455, 686, 767
systematic code, 1026, 1036
SYT, 134
t-(v, k, λ) design, 881, 892, 1052
t-design, 881, 892
t-ﬂat, 881, 896
table, 1329, 1338
of values, 37
tableau, 1147, 1153
tabu search, 1148, 1241
tabu search algorithm, 1242, 1246
tabu tenure, 1148
tabular form, 124
tail, 582, 586, 601, 602
tangent numbers, 142, 157
Tao, Terence, 1537
target, 1329, 1349
target frequency, 1435
target predicate, 1340
target value, 1168
tautology, 11, 16
taxa, 1446
telephone directory, 1349
ten most wanted numbers, 222, 265
tensor, 467
maximum rank, 467
rank, 467
rank-one, 467
typical rank, 467
term, 11, 70
term rank, 387, 446
term vector, 1382
terminal, 1272, 1294
terminal symbol, 1482
terminating expansion, 222
ternary Golay code, 1055
ternary matroid, 884, 915
test set, 1378, 1388
tetrahedral graph, 582, 593
Th¯abit ibn Qurra, 1538
The Imitation Game, 1283
theorem, 11, 51
theta, 39
theta graph, 620
theta notation, 11
thickness, 582, 651
Thompson, John, 341
three-dimensional visibility, 982
Thue’s lemma, 239
Thue’s ω-word, 1294
tiling, 932, 958
time complexity, 1272, 1302, 1309
time-complexity function, 189
time-homogeneous Markov chain, 519
time-persistent statistics, 547
T-norm operator, 73
top, of a stack, 1334
top-down mergesort, 1354
topological equivalence, 651
topological ordering, 849, 872
topological sort, 582, 609, 849, 872
topological sort algorithm, 609, 874
torus, 654
total chromatic number, 639
total coloring, 639
total coloring conjecture, 640
total dominating set, 582, 640
total domination number, 582, 641
total function, 33, 1272, 1287
total order, 849
totally ordered set, 11, 47, 851
totally unimodular matrix, 449, 1148, 1192
totient function, 222, 266
tough graph, 582, 620
tour, 793
tour construction, 794
tour improvement, 794
tour partitioning heuristic, 781
tournament, 446, 582, 610, 665
table, 666, 669
transitive, 610
tournament matrix, 387
Tower of Hanoi, 97, 187, 188
trace, 387, 405
trace-driven model, 544
traceback, 1479
tractable problem, 1272, 1302
traﬃc intensity, 480
trail, 582, 612
directed, 582
training set, 1378, 1388
trajectory, 480, 502, 503, 1456
transaction, 1378, 1384

INDEX
1587
transcendental element, 324, 329, 354, 358
transcendental extension, 358
transcendental number, 222, 223
transfer, 1236
transfer function, 1489
transfer graph, 1237
transient random walk, 502
transient state, 480, 522
transient walk, 480
transition count, 1245
transition diagram, 519
transition function, 1275, 1276, 1278, 1280,
1456
transition probability, 480, 520, 527
transition probability matrix, 480, 519, 520,
527
transition table, 1273, 1275, 1281
transitive closure, 11
transitive digraph, 582, 602
transitive orientation, 583, 607, 849, 875
transitive reduction, 11, 43
transitive tournament, 610
transitivity, 11, 42
transmitter, 583, 586, 610
transpose, 387
transposition, 329, 343
transposition cipher, 1075, 1081
transversal, 557, 558, 583, 686, 884, 905,
961
transversal design, 884, 896
transversal matroid, 884, 914
transversal number, 583, 688
trapping state, 1273, 1276
traveling salesman problem (TSP), 726, 793,
1303
tree, 583, 593, 696
1-4, 715
balanced, 694, 699
bihomogeneous, 694, 704
binary, 694, 699, 717, 1446
bounded, 704
breadth-ﬁrst search, 706
complete binary, 694, 699
complete m-ary, 699
decision, 694, 699, 1390
depth-ﬁrst search, 706
full binary, 717
full m-ary, 695, 699
homogeneous, 695, 704
Huﬀman, 699
inﬁnite, 695, 704
irreducible, 712
labeled, 695, 711
left-right, 695, 717
locally ﬁnite, 695, 704
m-ary, 694, 699
majority rules consensus, 1449
ordered, 695, 698, 717
phylogenetic, 1398, 1428
plane, 699
quartet, 1453
ranked, 1452
reduced, 696, 712
rooted, 696, 698, 712
spanning, 696, 705
suﬃx, 1428, 1440
tree diagram, 88, 95
tree edge, 696, 705, 726
tree metric, 1450
tree structure, 1330, 1342
tree traversal, 696
trial division, 222, 261
triangle degree, 807
triangle inequality, 793, 1382
triangular chord, 607, 849, 875
triangulation, 932, 972, 977
monotone polygon, 977
planar straight-line graph, 978
simple polygon, 978
triangulation hierarchy algorithm, 984
trickle-down, 1345
trickle-up, 1344
tridiagonal matrix, 387
tripartite key distribution, 1122
Triple-DES, 1085, 1087
trivial graph, 583, 584
trivial relator, 342
trophic level, 1428, 1493
trophic status, 1493
trophic status dominant species, 1493
truth table, 11, 13
truth value, 13, 74
Tukey, John Wilder, 1538
Tur´an graph, 583, 679
Tur´an number, 679
Tur´an’s theorem, 680
turbo code, 1026, 1062
Turing machine (TM), 1273, 1274, 1280
Turing’s test, 1273, 1283
Turing, Alan, 1082, 1283, 1538
Turing-acceptable language, 1273, 1281
Turing-computable function, 1273, 1281

1588
INDEX
Turing-decidable language, 1273, 1282
Turing-p-complete, 1311
Turing-p-hard, 1311
Turing-p-reducible, 1273, 1311
Tutte’s 1-factor theorem, 623
Tutte, W. T., 623
twin primes, 223, 250, 253
two’s complement, 228, 497
two’s complement expansion, 223
two-phase heuristic, 726, 801
two-terminal network, 480, 511
two-terminal reliability, 480, 515
two-terminal reliability algorithm, 516
two-valued logic, 11, 74
two-way incidence structure, 1330, 1346
two-way linked list, 1330, 1341
two-way sequential list, 1330, 1336
type 0 grammar, 1273, 1294
type 1 grammar, 1273, 1295
type 2 grammar, 1273, 1295
type 3 grammar, 1273, 1295
ultimately periodic expansion, 296
ultimately periodic sequence, 223
ultra-metric, 1450
unambiguous context-free language, 1273,
1298
unary operation, 33, 329
unbounded minimalization, 1273, 1287
uncertainty, 1028
uncomputable function, 1273
unconditional security, 1074, 1075, 1078
unconditionally secure, 1121
uncountable, 28
undecidable problem, 1273, 1282
underdetermined system, 463
underﬂow, 480, 499
undirected network, 726
unicity distance, 1078
uniform chirotope, 932, 947
uniform crossover, 1464
uniform distribution, 1452
uniform matroid, 885, 914
uniform random variable, 480
uniformly directed cutset, 1202
unilaterally connected, 583, 605
unimodular, 687
union, 11, 25, 480
of languages, 1293
of relations, 43
union-ﬁnd, 1367
union-ﬁnd datatype, 1330, 1334
unique decodability, 1300
unique existential quantiﬁer, 20
unique factorization property, 304
unique factorization theorem, 244, 355
uniquely decodable code, 1026, 1029
unit, 223, 302, 329, 348
unit distance graph, 965
unit simplex, 1159
unit triangular matrix, 387
unit vector, 395
unitary matrix, 387, 407
unity, 329, 348
universal address system, 703
universal domain, 11
universal hash function, 1115
universal quantiﬁer, 12, 20
universal Turing machine, 1273, 1281, 1282
universally quantiﬁed predicate, 12
universe of discourse, 12, 19
unnormalized graph Laplacian, 1412
unnormalized spectral clustering, 1413
unordered selection, 88
with replacement, 88, 105
unreasonable set, 1213
unreliability, 508, 509
unrestricted grammar, 1294
unsolvable problem, 1273
unstable process, 526
unsupervised learning, 1378
upper bandwidth, 430
upper bound, 12, 48, 324, 367, 849, 859
upper density, 932, 952, 956
upper domination number, 640
upper envelope, 932, 939
upper extension, 872
upper irredundance number, 641
upper limit, 196
upper semimodular lattice, 850, 862
upper total domination number, 641
upper triangular matrix, 387
upset, 561, 583, 687, 850, 858
Uranus, 1037, 1051
valence, 583, 585
valid argument form, 12, 52
valid inequalities, 1405, 1406
van der Waerden, B. L., 455
van der Waerden-Egorychev-Falikman
inequality, 455
Vandermonde convolution, 107, 176

INDEX
1589
Vandermonde matrix, 416
Vandermonde, Alexandre-Th´eophile, 1538
Vandiver’s conjecture, 310
variable-depth neighborhood search
algorithm, 1148, 1234, 1235
variance, 480, 493, 556
variance reduction techniques, 480, 547
variation operator, 1464
VCG discount, 1227
vector, 387, 388
vector composition, 117
vector conﬁguration, 932, 935
vector matroid, 885, 914
vector norm, 427
vector partition, 117
vector space, 387, 388
vehicle routing, 726, 801, 1194
Venn diagram, 12, 25
Venn, John, 25
veriﬁcation, 12
veriﬁcation key, 1112
Vernam cipher, 1075
vertex cardinality, 829
vertex chromatic number, 631
vertex coloring, 583
vertex connectivity, 583, 615
vertex cover, 734
vertex cut, 583
vertex degree, 734, 807
vertex deletion, 574, 590
vertex imbalance, 772
vertex independence number, 637
vertex invariant, 583, 625
vertex potential, 726, 770
vertex rank, 815
vertex-deleted subgraph, 583, 629
vertex-transitive graph, 673
very large-scale neighborhood, 1148, 1232
very large-scale neighborhood search algo-
rithm, 1148, 1232
Vickrey-Clarke-Groves (VCG) mechanism,
1148, 1227
Viergruppe, 345
Vi´ete, Fran¸cois, 1538
Vigen`ere cipher, 1075
virtual currency, 1129
visibility, 977
visibility graph, 932, 982
visibility map algorithm, 991
visibility problem, 932, 982
Viterbi algorithm, 533, 1060
Viterbi, Andrew, 533, 1539
Vizing’s conjecture, 642, 678
Vizing’s theorem, 635
Vizing, Vadim G., 635
(v, k, λ) design, 885
void design, 885
voltage graph, 673
Voronoi cell, 933, 953, 979
Voronoi diagram, 933, 942, 979
Voronoi diagram construction, 979
Voronoi tesselation, 1389
Voyager 2, 1037, 1051
VSLI layout, 1007
waiting time, 539
waiting-time process, 480
walk, 583, 612, 696
directed, 583
Waring’s problem, 223, 295
table, 296
Waring, Edward, 295, 1539
Warshall algorithm, 45
Warshall, Stephen, 45, 755
Watts-Strogatz clustering coeﬃcient, 808
Watts-Strogatz model, 726, 812
weak collision-resistance, 1097
weak duality theorem, 734, 760, 1162
weak excedance, 152
weak order, 850, 868
weak perfect graph theorem, 670
weakly connected digraph, 583, 605
weakly correct code, 12, 67
Wedderburn’s theorem, 353
Wedderburn, J. H. M., 353
Weibull random variable, 546
Weierstrass ℘-function, 319
Weierstrass equation, 311
Weierstrass, Karl, 311
weight distribution, 1043
weight enumerator, 1043
weight, of an operator, 1063
weighted bipartite matching algorithm, 738
weighted graph, 583, 585
Welch, Lloyd, 536
well-conditioned matrix, 427
well-conditioned system, 387
well-deﬁned, 32
well-formed formula, 12, 20
well-ordered set, 12, 48
well-ordering principle, 12, 59
Weyl’s theorem, 444

1590
INDEX
wﬀ, 20
wheel graph, 583, 595
while-condition, 67
while-loop instruction, 67
Whitehead, Alfred North, 24
Whitney number, 850, 857
Whitney’s 2-isomorphism theorem, 915
Whitney, Hassler, 1539
WiﬁProtection Access (WPA), 1088
Wiles, Andrew, 291, 1539
Wilf, Herbert, 81, 212
Williams, John W. J., 1353
Williamson, Malcolm J., 1116
Wilson’s theorem, 237
winner determination problem, 1148
witness, 259
Woodbury’s identity, 419
word, 329, 330, 342, 1029
word equation, 1273, 1300
word inequality, 1273, 1300
word problem, 342
word variable, 1273, 1299
Worpitzky’s identity, 153
Worpitzky, Julius, 153
worst-case complexity, 1273, 1305
wraparound, 497
WZ certiﬁcate, 212
WZ mate, 212
WZ pair, 212
WZ proof, 212
x-monotone curve, 939, 943
XOR, 12
XOR bid language, 1224
Yao’s millionaire problem, 1126
Yao’s two-party computation, 1127
Young diagram, 117
Young lattice, 850, 860
Young tableau, 88, 134, 1477
Young, Alfred, 133
Yule-Harding distribution, 1428, 1452
Zarankiewicz’s conjecture, 651
Zech logarithm, 363
Zeilberger, Doron, 81, 212
Zeller’s congruence, 238
Zermelo-Fraenkel axioms, 12, 31
zero, of a polynomial, 329, 354
zero-based array, 1341
zero-order logic, 12, 13
zero-sum game, 1210
zig-zag product, 676
zone, 933, 939
zone theorem, 941
zonotope, 933, 1000
ZPP, 1313

