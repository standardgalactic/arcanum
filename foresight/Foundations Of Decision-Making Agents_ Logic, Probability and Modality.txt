
Foundations of  
Decision-Making Agents 
Logic, Probability and Modality

This page intentionally left blank
This page intentionally left blank

World Scientific
Foundations of  
Decision-Making Agents 
Logic, Probability and Modality
Subrata Das
Imperial College Press

British Library Cataloguing-in-Publication Data
A catalogue record for this book is available from the British Library.
Cover image by Sebastien Das
For photocopying of material in this volume, please pay a copying fee through the Copyright Clearance Center,
Inc., 222 Rosewood Drive, Danvers, MA 01923, USA. In this case permission to photocopy is not required from
the publisher.
ISBN-13 978-981-277-983-0
ISBN-10 981-277-983-3
Editor:  Tjan Kwang Wei
All rights reserved. This book, or parts thereof, may not be reproduced in any form or by any means, electronic or
mechanical, including photocopying, recording or any information storage and retrieval system now known or to
be invented, without written permission from the Publisher.
Copyright © 2008 by World Scientific Publishing Co. Pte. Ltd.
Published by
World Scientific Publishing Co. Pte. Ltd.
5 Toh Tuck Link, Singapore 596224
USA office:  27 Warren Street, Suite 401-402, Hackensack, NJ 07601
UK office:  57 Shelton Street, Covent Garden, London WC2H 9HE
Printed in Singapore.
FOUNDATIONS OF DECISION-MAKING AGENTS
Logic, Probability and Modality

v 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
To Janique, Sébastien, and Kabita 
 

 
 
vi 
This page intentionally left blank
This page intentionally left blank

vii 
Preface 
 
 
Artificial Intelligence (AI) systems simulating human behavior are often called 
intelligent agents. By definition, these intelligent agents exhibit some form of 
human-like intelligence. For example, intelligent agents can learn from the 
environment, make plans and decisions, react to the environment with 
appropriate actions, express emotions, or revise beliefs. Intelligent agents 
typically represent human cognitive states using underlying beliefs and 
knowledge modeled in a knowledge representation language. We use the term 
epistemic state to refer to an actual or possible cognitive state that drives human 
behavior at a given point in time; the accurate determination (or estimation) of 
these epistemic states is crucial to an agent’s ability to correctly simulate human 
behavior. 
This book provides three fundamental and generic approaches (logical, 
probabilistic, and modal) to representing and reasoning with agent epistemic 
states, specifically in the context of decision making. In addition, the book 
introduces a formal integration of these three approaches into a single unified 
approach we call P3 (Propositional, Probabilistic, and Possible World), that 
combines the advantages of the other approaches. Each of these approaches can 
be applied to create the foundation for intelligent, decision-making software 
agents. Each approach is symbolic in nature (unlike the sub-symbolic neural 
network approach, for example), yielding agent “thought processes” that are 
sufficiently transparent to provide users with understandable explanations of 
agent “reasoning.” The classical logic and probability theory thrust of our 
approach naturally excludes from our consideration other potential symbolic 
approaches to decision-making such as fuzzy logics. The “symbolic 
argumentation” approach (P3) to decision making combines logic and 
probability, and therefore offers several advantages over the traditional approach 
to decision-making based on simple rule-based expert systems or expected utility 
theory. 
The generic logical, probabilistic, and modal reasoning techniques that we 
discuss in the book, such as logical deduction and evidence propagation, are 
applied to various decision-making problems that an agent will encounter in 
various circumstances. However, the book does not focus on particular 
applications of these reasoning techniques to such problems as planning, 

viii 
Decision-Making Agents 
 
 
learning, or belief revision. Neither does the book focus on communication and 
collaboration within agent societies. Such applications require extensive 
treatments in themselves and so are beyond the scope of the book; our purpose 
here is to provide the formal foundation necessary to support decision-making.  
This book is divided into four parts by the four reasoning approaches we 
develop. The first part focuses on logic-based modeling of an epistemic state as a 
set of propositions expressed by sentences from classical propositional and first-
order logics. The second part takes a probabilistic approach to modeling 
epistemic states. In this approach, states are represented by a probability measure 
defined over some space of events, which are represented as random variables. 
The third part focuses on extending logic-based modeling with modalities to 
yield various systems of modal logics, including epistemic logics. These logics 
are based on mental constructs that represent information and are suitable for 
reasoning with possible human cognitive states. Finally, the fourth part describes 
the combination of these three rather disparate approaches into the single 
integrated P3 approach within an argumentation framework especially 
appropriate for simulating human decision making. 
The purpose of this book is not to carry out an elaborate philosophical 
discussion on epistemic state modeling, but rather to provide readers with various 
practical ways of modeling and reasoning with agent epistemic states used in 
decision making. Therefore, the resolution-based logic programming paradigm is 
introduced under logic-based modeling; Bayesian belief networks are introduced 
as a way of modeling and reasoning with epistemic states where beliefs are 
represented by probability measures defined over some state space; and modal 
resolution schemes are introduced under modal logic-based modeling and 
reasoning with actual and possible worlds. 
Frameworks specific to decision making, such as the logic-based production 
rule or belief-network-based influence diagrams that incorporate the notions of 
action and utility, are also detailed in the book. The use of logic-based reasoning 
(especially resolution theorem proving) and probabilistic reasoning (especially 
Bayesian belief networks) is widespread when implementing agents capable of 
making decisions, generating plans, revising beliefs, learning, and so on. But the 
book goes a step further by bridging the gap between these two approaches, 
augmenting them with informational mental constructs, such as beliefs and 
knowledge, and then producing a coherent approach that culminates in the 
penultimate chapter of the book in the form of a symbolic argumentation 
approach for weighing the pros and cons of decision-making options. 

 
Decision-Making Agents 
ix 
 
In the book I have tried to balance discussions on theory and practice. As for 
theory, I have stated results and developed proofs wherever necessary to provide 
sufficient theoretical foundations for the concepts and procedures introduced. For 
example, if I have introduced a specific resolution-based theorem-proving 
procedure, then I also provide the associated soundness and completeness of the 
procedure with respect to the stated semantics. On the practical side, I have 
provided detailed algorithms that can be encoded in computer programs in a 
straightforward manner to incorporate aspects of human behavior into intelligent 
decision-making agents. For example, an implementation of the evidence 
propagation algorithms for belief networks and influence diagrams can be 
incorporated into agents that make decisions within a probabilistic framework. 
The decision-making problem is present in every industry; finance, medicine, 
and defense are a few of the big ones. Computer professionals and researchers in 
the decision science community within commerce and industry will find the book 
most useful for building intelligent and practical decision-aiding agents or, more 
generally, for embedding intelligence in various systems based on sound logical 
and probabilistic reasoning; the material is especially suited for building safety-
critical decision aiding applications based on well-founded theories. This book 
may also be used as an AI textbook on logic and probability for undergraduate 
and graduate courses, and as a reference book for researchers in universities. 
The chapters in the book and their dependencies are shown in the figure 
below. Readers are advised not to skip Chapter 1, which includes an informal 
overview of the rest of the book. A basic understanding of this chapter will 
prevent the reader from becoming lost later on. The beginner should not be 
dismayed by the use of unfamiliar terminology in this first chapter, as these terms 
will be explained in later chapters. 
Readers who have been exposed to the concepts of basic mathematics and 
probability may omit Chapter 2, which provides a background on mathematical 
preliminaries. Chapters 3 and 4, on classical logics and logic programming, 
should be read in sequence. Chapter 6, on Bayesian belief networks, can be read 
independently of Chapters 3 and 4, but Chapter 8, on modal logics, requires an 
understanding of classical logic as presented in Chapter 3. Chapters 3, 4, 6, and 8 
provide the semantic foundations of our three problem modeling approaches, 
namely, propositional, probabilistic, and possible world. Because production 
rules are a special form of rules in logic programming with embedded degrees of 
uncertainty I recommend to reading at least the logic programming section in 
Chapter 4 and the background material on probability theory before reading 

x 
Decision-Making Agents 
 
 
Chapter 5. Chapter 7, on influence diagrams, requires understanding the basics of 
Bayesian belief networks as it extends this understanding with the concepts of 
action and utility. The final chapter (Chapter 9) on symbolic argumentation 
requires an overall understanding of the contents of Chapters 3-8. Ultimately, 
three different decision-making frameworks are provided, in Chapters 5, 7, and 9. 
 
Mathematical Preliminaries – Chapter 2
Classical Logics & 
Logic Programming
Chapter 3 & 4
Propositional
Production Rules
Chapter 5
Influence Diagrams
Chapter 7
Symbolic Argumentation
Chapter 9
Modal & Epistemic 
Logics
Chapter 8
Possible World
Bayesian Belief 
Networks
Chapter 6
Probabilistic
P3 Model
Problem 
Modeling 
Semantics
Decision 
Making 
Frameworks
Modeling Agent Epistemic State: An Informal Overview – Chapter 1
 
Figure 1-1: Chapters and their dependencies 
There is another suggested way to follow this book for those readers who, 
with some exposure to mathematical logic and the theory of probability, are 
looking for some well-founded computational decision support technologies to 
build deployable systems. They can skip detailed theoretical materials as 
presented in some chapters (e.g. 3, 4, and 8), and focus more on the 
computational aspects (e.g. chapter 5, 6, 7, and 9).  
My sincere thanks go to my wife Janique, my son Sébastien, and my 
daughter Kabita, for their love, patience and inspiration throughout the 
preparation of this book (My children’s perception of the book are very different 
from each other. Sébastien’s view is very philosophical, which he manifested in 
his drawing for the title page of a sage from the Museum of Fine Arts in Boston. 
However, Kabita is more pragmatic, expressed herself through a dog’s decision-
making dilemma as depicted below.) Special thanks go to Karen DeSimone for 
her careful proofreading of the manuscript that enormously enhanced readability 
and enriched the content of the book. Thanks are due to my colleagues, Dave 

 
Decision-Making Agents 
xi 
 
Lawless and Josh Introne, with whom I have had many technical discussions on 
various aspects of this book. Thanks also to all of my colleagues at Charles River 
Analytics, especially Alex Feinman, Paul Gonsalves, Partha Kanjilal, Rich 
Reposa, and Greg Zacharias, and Avi Pfeffer at Harvard University, for their part 
in creating a stimulating intellectual environment which inspired me to write this 
book. Thanks to the World Scientific/Imperial College Press, especially Tjan 
Kwang Wei, for their help in producing the book from the beginning. Finally, I 
thank my parents, brothers, sisters and other family members back in one of 
several thousands of small villages in India for patiently accepting my absence 
and showing their encouragement and support through their many phone calls. 
This book is based on the work of many researchers, and I express my 
gratitude to those who have directly or indirectly contributed so much to the 
fields of deductive logic, probability theory, and artificial intelligence. I have 
made my best effort to make this book informative, readable, and free from 
mistakes, and I would welcome any criticism or suggestion for improvement. 
 
Subrata Das 
Cambridge, MA 
March 2007 
 
 
 
 
 
 
 
                                                          Dog’s dilemma

This page intentionally left blank
This page intentionally left blank

xiii 
Table of Contents 
Preface .......................................................................................................... vii 
Table of Contents......................................................................................... xiii 
Chapter 1..........................................................................................................1 
Modeling Agent Epistemic States: An Informal Overview 
1.1 
Models of Agent Epistemic States...........................................1 
1.2 
Propositional Epistemic Model................................................3 
1.3 
Probabilistic Epistemic Model.................................................8 
1.4 
Possible World Epistemic Model...........................................12 
1.5 
Comparisons of Models .........................................................16 
1.6 
P3 Model for Decision-Making Agents .................................17 
Chapter 2........................................................................................................23 
Mathematical Preliminaries 
2.1 
Usage of Symbols ..................................................................23 
2.2 
Sets, Relations, and Functions ...............................................24 
2.3 
Graphs and Trees ...................................................................29 
2.4 
Probability..............................................................................34 
2.5 
Algorithmic Complexity ........................................................40 
2.6 
Further Readings....................................................................44 
Chapter 3........................................................................................................45 
Classical Logics for the Propositional Epistemic Model 
3.1 
Propositional Logic................................................................46 
3.1.1 
Axiomatic Theory for Propositional Logic ...........54 
3.1.2 
Soundness and Completeness Theorem ................55 
3.2 
First-Order Logic ...................................................................57 

xiv 
Decision-Making Agents 
 
 
3.2.1 
Axiomatic Theory for First-order Logic................62 
3.2.2 
Soundness and Completeness Theorem ................63 
3.2.3 
Applications...........................................................70 
3.3 
Theorem Proving Procedure ..................................................71 
3.3.1 
Clausal Form .........................................................71 
3.3.2 
Herbrand's theorem................................................74 
3.3.3 
Implementation of Herbrand's theorem.................78 
3.4 
Resolution Theorem Proving .................................................80 
3.4.1 
Resolution principle and unification......................80 
3.5 
Refutation Procedure..............................................................91 
3.6 
Complexity Analysis..............................................................95 
3.7 
Further Readings....................................................................96 
Chapter 4........................................................................................................97 
Logic Programming 
4.1 
The Concept...........................................................................97 
4.2 
Program Clauses and Goals ...................................................99 
4.3 
Program Semantics ..............................................................106 
4.4 
Definite Programs ................................................................108 
4.5 
Normal Programs.................................................................114 
4.6 
Prolog...................................................................................121 
4.6.1 
Prolog Syntax ......................................................121 
4.6.2 
Theoretical Background ......................................123 
4.6.3 
Backtracking........................................................126 
4.6.4 
The Cut................................................................127 
4.6.5 
Special Constructs and Connectives....................128 
4.6.6 
Negation ..............................................................129 
4.6.7 
Equality ...............................................................130 
4.6.8 
List.......................................................................131 
4.6.9 
Arithmetic............................................................132 

 
Decision-Making Agents 
xv 
 
4.6.10 
Input/Output ........................................................133 
4.6.11 
Clause Management ............................................135 
4.6.12 
Set Evaluation......................................................136 
4.6.13 
Meta Programming..............................................139 
4.7 
Prolog Systems.....................................................................141 
4.8 
Complexity Analysis............................................................141 
4.9 
Further Readings..................................................................142 
Chapter 5......................................................................................................143 
Logical Rules for Making Decisions 
5.1 
Evolution of Rules ...............................................................144 
5.2 
Bayesian Probability Theory for Handling Uncertainty ......146 
5.3 
Dempster-Shafer Theory for Handling Uncertainty.............150 
5.4 
Measuring Consensus ..........................................................157 
5.5 
Combining Sources of Varying Confidence ........................162 
5.6 
Advantages and Disadvantages of Rule-Based Systems......163 
5.7 
Background and Further Readings.......................................164 
Chapter 6......................................................................................................165 
Bayesian Belief Networks 
6.1 
Bayesian Belief Networks....................................................166 
6.2 
Conditional Independence in Belief Networks ....................171 
6.3 
Evidence, Belief, and Likelihood.........................................179 
6.4 
Prior Probabilities in Networks without Evidence...............182 
6.5 
Belief Revision.....................................................................184 
6.6 
Evidence Propagation in Polytrees.......................................190 
6.6.1 
Upward Propagation in a Linear Fragment .........191 
6.6.2 
Downward Propagation in a Linear Fragment.....194 
6.6.3 
Upward Propagation in a Tree Fragment ............198 
6.6.4 
Downward Propagation in a Tree Fragment........200 

xvi 
Decision-Making Agents 
 
 
6.6.5 
Upward Propagation in a Polytree Fragment ......201 
6.6.6 
Downward Propagation in a Polytree Fragment..204 
6.6.7 
Propagation Algorithm ........................................208 
6.7 
Evidence Propagation in Directed Acyclic Graphs..............211 
6.7.1 
Graphical Transformation ...................................214 
6.7.2 
Join Tree Initialization.........................................222 
6.7.3 
Propagation in Join Tree and Marginalization ....224 
6.7.4 
Handling Evidence ..............................................227 
6.8 
Complexity of Inference Algorithms ...................................229 
6.9 
Acquisition of Probabilities .................................................230 
6.10 
Advantages and Disadvantages of Belief Networks........234 
6.11 
Belief Network Tools ......................................................235 
6.12 
Further Readings..............................................................235 
Chapter 7......................................................................................................237 
Influence Diagrams for Making Decisions 
7.1 
Expected Utility Theory and Decision Trees.......................237 
7.2 
Influence Diagrams..............................................................240 
7.3 
Inferencing in Influence Diagrams.......................................242 
7.4 
Compilation of Influence Diagrams.....................................248 
7.5 
Inferencing in Strong Junction Tress ...................................252 
7.6 
Further Readings..................................................................254 
Chapter 8......................................................................................................255 
Modal Logics for the Possible World Epistemic Model 
8.1 
Historical Development of Modal Logics............................256 
8.2 
Systems of Modal Logic ......................................................262 
8.3 
Deductions in Modal Systems..............................................265 
8.3.1 
Principle of Duality .............................................265 
8.3.2 
Theorems of K .....................................................266 

 
Decision-Making Agents 
xvii 
 
8.3.3 
Theorems of D.....................................................268 
8.3.4 
Theorems of T......................................................269 
8.3.5 
Theorems of S4 ....................................................269 
8.3.6 
Theorems of B .....................................................270 
8.3.7 
Theorems of S5 ....................................................271 
8.3.8 
Theorems of S5’ ...................................................272 
8.4 
Modality...............................................................................272 
8.5 
Decidability and Matrix Method..........................................273 
8.6 
Relationships among Modal Systems ..................................277 
8.7 
Possible World Semantics....................................................279 
8.8 
Soundness and Completeness Results..................................286 
8.9 
Complexity and Decidability of Modal Systems .................291 
8.10 
Modal First-Order Logics................................................294 
8.11 
Resolution in Modal First-Order Logics..........................300 
8.11.1 
Transformation Algorithm...................................302 
8.11.2 
Unification...........................................................304 
8.12 
Modal Epistemic Logics..................................................307 
8.13 
Logic of Agents Beliefs (LAB) ........................................309 
8.13.1 
Syntax of LAB .....................................................310 
8.13.2 
Axioms of LAB....................................................312 
8.13.3 
Possible World Semantics of LAB.......................313 
8.13.4 
Soundness and Completeness of LAB .................316 
8.13.5 
Rational Extension of LAB..................................319 
8.13.6 
Goals in LAB .......................................................320 
8.13.7 
Dempster-Shafer Interpretation of LAB...............320 
8.14 
Further Readings..............................................................323 
Chapter 9......................................................................................................325 
Symbolic Argumentation for Decision-Making 
9.1 
Toulmin’s Model of Argumentation....................................327 

xviii 
Decision-Making Agents 
 
 
9.2 
Domino Decision-Making Model for P3..............................328 
9.3 
Knowledge Representation Syntax of P3.............................330 
9.4 
Formalization of P3 via LAB................................................334 
9.5 
Aggregation via Dempster-Shafer Theory...........................335 
9.6 
Aggregation via Bayesian Belief Networks.........................339 
9.7 
Further Readings..................................................................345 
References....................................................................................................347 
Index ............................................................................................................355 
 

1 
Chapter 1 
 
Modeling Agent Epistemic 
States: An Informal Overview 
 
 
 
 
Recent advances in intelligent agent research (AGENTS, 1997-2001; AAMAS, 
2002-2006) have culminated in various agent-based applications that 
autonomously perform a range of tasks on behalf of human operators. Just to 
name a few, the kinds of tasks these applications perform include information 
filtering and retrieval, situation assessment and decision making, and interface 
personalization. Each of these tasks requires some form of human-like 
intelligence that must be simulated and embedded within the implemented agent-
based application. The concept of epistemic states is often used to represent an 
actual or a possible cognitive state that drives the human-like behavior of an 
agent. Established traditional artificial intelligence (AI) research in the areas of 
knowledge representation and inferencing has been transitioned to represent and 
intelligently reason about the various mental constructs of an agent, including 
beliefs, desires, goals, intentions, and knowledge (Cohen and Levesque, 1990; 
Rao and Georgeff, 1991), simulating its human-like cognitive states.  
1.1 
Models of Agent Epistemic States 
The most commonly used models of agent epistemic states are the propositional, 
probabilistic, and possible world models (Gärdenfors, 1988): 
1. An epistemic state in a propositional model is represented as a set of 
propositions that the agent accepts in the epistemic state. These 
propositions are expressed by sentences in an object language. 
2. An epistemic state in a probabilistic model is represented by a 
probability measure defined over the states of some collection of random 
variables. This probability measure provides the agent’s degree of belief 
about each state of each random variable in the epistemic state. 

2 
Decision-Making Agents 
 
 
3. An epistemic state in a possible world model is represented by a set of 
possible worlds that includes the agent’s actual state or world, along with 
any worlds compatible with the agent’s knowledge and beliefs. Each 
world consists of those propositions that the agent accepts in its 
epistemological worldview. 
Irrespective of the model chosen to represent an agent’s epistemic state, the state 
model must always be built via a substantial knowledge engineering effort; this 
effort may occasionally be aided by machine learning techniques (Mitchell, 
1997) to automatically extract knowledge from data. Part of the knowledge 
engineering effort is the knowledge acquisition process that a scientist or 
engineer goes through when extracting knowledge from a domain or subject 
matter expert (such as a medical doctor), to be used by the agent for problem 
solving in a particular domain (for example, diagnosing diseases). The 
knowledge gained from this process can then be represented and stored in a 
computer, using the syntax of logic, frames, or graphical representations, such as 
semantic networks and probabilistic belief networks. The focus in this text is on 
logical syntaxes and graphical belief networks (and their amalgamations) that 
encode an agent’s epistemic state that represents knowledge about an uncertain 
environment. The agent then uses this encoded knowledge for making decisions 
under uncertainty, that is, to choose between two or more different options. For 
example, the agent may need to choose to believe in one possibility among many 
(for example, a patient has cancer or an ulcer), and thus an agent is revising its 
own belief, or may need to adopt a plan (or take some action), so that the agent is 
choosing among many possible options (e.g. surgery, chemotherapy, medication) 
for action. 
A note of caution: When we refer to an agent making a decision, we do not 
really mean that agents always make decisions autonomously. In some situations, 
when not enough evidence is available to choose one option over the others, or in 
cases when some dilemma occurs, agents may very well present the human 
decision maker with the viable options along with their accumulated supports, 
backed by explanations. In these situations, agents autonomously provide 
“decision aids” to human decision makers. Regardless of their role as a decision 
maker or aid provider, labeling them with the term “agent” is always justified at 
least due to their autonomous role of routinely searching through huge volumes 
of data (e.g. Internet web pages) for relevant evidence (a task for which human 
performance is usually poor) to be used for the decision making process.  

 
Modeling Agent Epistemic States 
3 
 
In the process of introducing representation and reasoning within various 
epistemic models, we will often rely on the example of a game (for example, a 
sporting event) which is scheduled to occur sometime on the current day. In our 
example, the decision that an agent will usually be trying to make is to determine 
the status of the game, (that is, whether the game is “on,” “cancelled,” or 
“delayed”) while preparing to go to town based on accumulated evidence. 
1.2 
Propositional Epistemic Model 
The language in the propositional epistemic model of an agent is usually 
governed by classical propositional logic; it is uniquely identified by its 
particular syntax, set of axioms, and inference rules. In general, logic is a 
systematic study of valid arguments. Each argument consists of certain 
propositions, called premises, from which another proposition, called the 
conclusion,  follows. Consider the following argument by an agent to determine 
the status of a game: 
If the field is wet or there is no transportation, 
then the game is cancelled. (Premise) 
The field is wet. (Premise) 
Therefore, the game is cancelled. (Conclusion) 
In the above argument, the first two statements are premises. The first is a 
conditional statement, and the second is assertional.  The third statement is the 
conclusion, or argument. (The term “therefore” is a sign of argument.) The above 
argument is valid. (Technically, a valid argument is one in which the conclusion 
must be true whenever the premises are true.) 
Valid arguments are studied independently of the premises from which 
arguments are drawn. This is achieved by expressing valid arguments in their 
logical or symbolized form.  The valid argument above is symbolized at an 
abstract level as: 
,
P
Q
R P
R
∨
→
B
 
where the symbols P, Q, and R stand for the proposition “the field is wet,” “no 
transportation,” and “the game is cancelled,” respectively. So the first premise of 
the argument, which is a conditional premise, is symbolized as P
Q
R
∨
→
, 
where ‘ →’ reads “implies”. The second premise is symbolized by just the 
proposition P. The symbol B  is the consequence relationship and R is a 
consequence of the premises. R is arrived at by the use of another argument 

4 
Decision-Making Agents 
 
 
P
P
Q
∨
B
, and applying an inference rule of the logic, called modus ponens, on 
P
Q
R
∨
→
 and P
Q
∨
. The inference rule states that given X
Y
→
 and X is true 
then it follows that Y is true, where X and Y are arbitrary sentences of the logic. 
Although the above argument is valid,  it is not known if the outfield is wet 
in reality; it has just been assumed for the purpose of argument. The validity of 
an argument is neither concerned with the actual subject matter nor with the truth 
or falsehood of the premises and conclusion in reality. Mathematical logic does 
not study the truth or falsehood of the particular statements in the premises and 
conclusion, but rather focuses on the process of reasoning, i.e. whether or not the 
assumed truth of the premises implies the truth of the conclusion via permissible 
symbol manipulation within the context. This phenomenon, to some extent, 
stimulates interesting philosophical argument on whether an agent, whose 
epistemic state is modeled using mathematical logic, is really capable of human-
like thinking or is just “blindly” manipulating symbols (Searle, 1984). 
Often arguments are advanced without stating all the premises, as is evident 
from the following argument constructed out of the elements of the previous 
argument: 
The field is wet. (Premise) 
Therefore, the game is cancelled. (Conclusion) 
Due to the lack of an appropriate premise, this example is not a valid argument in 
the context of classical propositional logic unless the agent intelligently assumes 
that “If the field is wet then the game is cancelled” is by default a premise. 
Consider the following example, which has an incomplete premise: 
If the field is wet, then the game is cancelled. (Premise) 
The field is wet, or the field sensing equipment is inaccurate. (Premise) 
Therefore, the game is cancelled. (Conclusion) 
The premise of the above argument does not definitely imply that the game is 
cancelled, since the field may be dry and the field sensing equipment incorrectly 
signaled a wet field. Hence it is an invalid argument. 
There are various kinds of arguments that cannot be stated by propositional 
logic without an appropriate combinatorial enumeration over the underlying 
objects. Consider the following example of a valid argument that requires the 
syntax of the so-called first-order logic: 

 
Modeling Agent Epistemic States 
5 
 
Every wet field is unsuitable for playing a game. 
The field at Eden Garden is wet. 
Therefore, Eden Garden is unsuitable for playing a game. 
Clearly, the assertional second premise does not occur in the conditional first 
premise, so that propositional logic fails to produce the desired conclusion. One 
solution to this shortcoming is to produce a specific instantiation of the first 
premise for the specific Eden Garden field (a sporting venue) and then apply 
Modus Ponens. But first the conditional premise of the above argument is 
symbolized as a general sentence: 
(
( ,
)
( ))
x Field x Wet
Unsuitable x
∀
→
 
where x is a variable or placeholder for terms representing things such as field 
names, Wet is a constant symbol representing one possible type of field 
condition, Field is a binary relation or predicate symbol that relates a field to its 
condition, and Unsuitable is a unary relation representing a property of a field. 
The symbols ∀ and → are read as “for every” (or “for all”) and “implies” 
respectively. The expression 
( ,
)
Field x Wet , which appears on the left , is treated 
as the antecedent of the premise. The expression following the symbol is the 
consequent.  Note that there are other ways that the argument can be symbolized 
depending on the context and inference need. For example, one can define a 
unary predicate or property Wet Field of a field replacing the binary predicate 
Field. (This of course blocks any sort of inference that takes into account the 
dryness property of the field.) 
The second premise of the argument, which is an assertional premise, is 
symbolized based on the predicate symbol Field as follows 
(
,
)
Field EdenGarden Wet  
A first-order symbolization of the argument now looks like this: 
(
( ,
)
( )),
(
,
)
(
)
x Field x Wet
Unsuitable x
Field EdenGarden Wet
Unsuitable EdenGarden
∀
→
A 
This symbolization is within the framework of first-order logic. An axiomatic 
deduction or inferencing of the conclusion of the argument appears as follows: 

6 
Decision-Making Agents 
 
 
Step 1: 
(
( ,
)
( ))
x Field x Wet
Unsuitable x
∀
→
  
Given Premise 
Step 2: 
(
,
)
Field EdenGarden Wet  
 
 
Given Premise 
Step 3: 
(
,
)
(
)
Field EdenGarden Wet
Unsuitable EdenGarden
→
 
 
 
 
 
 
 
 
Axiom on Step 1 
Step 4: 
(
)
Unsuitable EdenGarden   
Modus Pones on Steps 2 & 3 
The premises in steps 1 and 2 are considered proper axioms.  The above 
deduction is a proof of the conclusion 
(
)
Unsuitable EdenGarden  and therefore is 
a theorem that follows from the first-order system with the two proper axioms. 
Step 3 is derived from step 1 by particularization of the first premise. If the first 
premise is true for all fields, then it is also true for the field Eden Garden. Step 4 
is arrived at by the application of Modus Ponens. These steps demonstrate the 
basic axiomatic theorem-proving approach. 
In contrast to the axiomatic theorem proving approach, resolution theorem 
proving is based on the refutation principle. This principle states that if a set of 
premises is inconsistent then there must be a refutation. Based on this principle, 
the negation of what is to be proved via a query is assumed as a premise and a 
refutation is established if the query is to be derived (this very general technique 
is formally known as reduction ad absurdum, or proof by contradiction) . A 
query could be as general as “Is there any unsuitable playing field?”, or could be 
more specific as in the case of our example, i.e. “Is Eden Garden unsuitable?” 
The negation of the conclusion in our example arguments context is “It is not the 
case that Eden Garden is unsuitable” is considered as a premise. The symbolized 
form of this premise is: 
(
)
Unsuitable EdenGarden
¬
 
where ¬ is read as “It is not the case.” As this assumption is inconsistent with 
the other two premises, a refutation will result. To do this, first the symbolized 
premises are converted to their equivalent clausal form.  In this example the 
assertional premises are already in clausal form, while the conditional premise 
can be converted to clausal form to give: 
(
( ,
)
( ))
x
Field x Wet
Unsuitable x
∀
¬
∨
 
where ∨ is read as  “or.” By convention, a sentence in clausal form is normally 
written by removing 
x
∀-type signs from its front and these are assumed by 
default. In clausal form, the first premise of the argument can now be read as: 
Every field is either not wet or is unsuitable for a game 

 
Modeling Agent Epistemic States 
7 
 
which has the same meaning (semantically) as the original form of the first 
premise. Using this equivalent clausal form, the steps of the refutation of the 
argument are as follows: 
Step 1: 
( ,
)
( )
Field x Wet
Unsuitable x
¬
∨
 
Premise 
Step 2: 
(
,
)
Field EdenGarden Wet  
 
Premise 
Step 3: 
(
)
Unsuitable EdenGarden
¬
 
Negation of query –  
 
assumed as premise 
Step 4: 
(
)
Unsuitable EdenGarden   Resolution of clause in Steps 1 and 2 
Step 5:  (empty clause) 
 
Resolution of clause in Steps 3 and 4 
The complement of the atomic premise 
(
,
)
Field EdenGarden Wet  in step 2 and 
the subexpression 
( ,
)
Field x Wet
¬
 of the premise in step 1 can be made equal by 
a most general unification { /
}
x EdenGarden , that is, by substituting Eden 
Garden in place of x. The clause 
(
)
Unsuitable EdenGarden  is then obtained by 
applying the resolution principle (Robinson, 1965)  to steps 1 and 2. This means 
applying most general unifiers to both expressions and then merging and 
canceling the complementary expressions. To complete above derivation, the 
resolution principle can be applied again to steps 3 and 4. After canceling the 
complementary atomic expressions, the resultant expression is empty and 
therefore a refutation of the argument has been arrived at (  denotes a 
refutation). This shows that the assumption in step 3 is wrong and therefore its 
complement is true, that is, Eden Garden is unsuitable for a game. 
If-then types of sentences are clauses of a special type that many believe are 
natural representations of human knowledge. So-called production rules in expert 
systems are based on the if-then syntax. Many subject matter experts are 
comfortable expressing their knowledge in this form. In the logic programming 
community, a set of these rules constitute a program that is guaranteed to be 
consistent, and most logic programming systems can directly handle such 
programs without needing any meta-interpreters. Therefore, the use of these rules 
in building an agent’s knowledge is both practical and consistent with established 
theories. 
In building an agent’s propositional epistemic state, logical omniscience is 
assumed which means that the agent knows all tautologies and that its knowledge 
is closed under Modus Ponens. However, omniscience is not realizable in 
practice since real agents are resource-bounded. Attempts to define knowledge in 
the presence of bounds include restricting what an agent knows to a set of 

8 
Decision-Making Agents 
 
 
formulae which is not closed under inference or under all instances of a given 
axiom. 
1.3 
Probabilistic Epistemic Model 
The knowledge that constitutes an epistemic state of an intelligent software agent 
will often be imprecise, vague, and uncertain. One major drawback of the 
propositional approach to modeling epistemic state is that it does not even 
provide the granularity needed to represent the uncertainty in sentences that we 
use in everyday life. The truth value of a logical sentence in the propositional 
approach is either true or false, and there is nothing in between. Therefore no 
special treatment can be given to a sentence like “If the field is wet then the game 
is 90% likely to be cancelled.” The probabilistic model provides this granularity 
with probabilities representing uncertainty in sentences. 
A probabilistic model of epistemic states is represented by a probability 
measure defined over some space of events represented as random variables. This 
probability measure provides the agent’s degree of belief in each possible world 
state. A random variable is a real-valued function defined over a sample space 
(that is, the domain of the variable) and its value is determined by the outcome of 
an experiment, known as an event. A discrete random variable is a random 
variable whose domain is finite or denumerable. Probabilistic modeling of an 
agent’s epistemic state involves first identifying the set of random variables in 
the domain and then developing a joint probability distribution of the identified 
variables to determine the likelihood of the epistemic state that the agent is in. To 
illustrate this, consider an example domain to determine the status of a game 
depending on the field condition and the transport. The random variables in this 
context are Game, Field, and Transport; they are defined over the sample spaces 
or states {on, cancelled, delayed}, {dry, wet}, and {present, absent} respectively. 
The probability distribution of a random variable is a function whose domain 
contains the values that the random variable can assume, and whose range is a set 
of values associated with the probabilities of the elements of the domain. The 
joint probability distribution of the three discrete random variables Game, Field, 
and Transport is a function whose domain is the set of triplets (x, y, z), where x, 
y, and z are possible values for Game, Field, and Transport, respectively, and 
whose range is the set of probability values corresponding to the ordered pairs in 
its domain. Therefore, the following probability measure 
(
,
,
)
0.8
p Game
cancelled Field
wet Transport
present
=
=
=
=
 

 
Modeling Agent Epistemic States 
9 
 
indicates that 0.8 is the probability that the game is cancelled and the field is wet 
and transport is present. The probabilities in a joint probability distribution have 
to be computed from previous data based on the frequency probability 
approaches. Given a distribution, you can then compute conditional probabilities 
of interest by applying multiplication and marginalization rules. In the frequency 
approach, probability is derived from observed or imagined frequency 
distributions. For example, the conditional probability that the game is on given 
that the field is wet and transport is absent is computed as follows: 
{
,
,
}
(
|
,
)
(
,
,
)
(
,
,
)
x
on cancelled
delayed
p Game
on Field
wet Transport
present
p Game
on Field
wet Transport
present
p Game
x Field
wet Transport
present
∈
=
=
=
=
=
=
=
=
=
=
∑
 
In the Bayesian approach, the probability on a particular statement regarding 
random variable states is not based on any precise computation, but describes an 
individual’s personal judgment (degree of belief) about how likely a particular 
event is to occur based on experience. The Bayesian approach is more general 
and expected to provide better results in practice than frequency probabilities 
alone because it incorporates subjective probability. The Bayesian approach can 
therefore obtain different probabilities for any particular statement by 
incorporating prior information from experts. The Bayesian approach is 
especially useful in situations where no historic data exists to compute prior 
probabilities. Bayes’ rule allows one to manipulate conditional probabilities as 
follows: 
(
|
) ( )
(
|
)
( )
p B A p A
p A B
p B
=
 
This rule estimates the probability of A in light of the observation B. Even if B 
did not happen, one can estimate the probability 
(
|
)
p A B  that takes into account 
the prior probability 
( )
p A . 
Computing the joint probability distribution for a domain with even a small 
set of variables can be a daunting task. Our example domain with the variables 
Game, Field, and Transport, and state space sizes 3, 2, and 2 respectively, 
requires 3 2
2 1
× ×
− probabilities for the joint distribution. A domain with just 10 
binary variables will require 
10
2
1
− probabilities. This problem can be mitigated 
by building Bayesian belief networks based on qualitative information in the 
domain, such as the following: 

10 
Decision-Making Agents 
 
 
The field condition and the transport situation together determine the status of 
the game. 
This is simply qualitative information about some dependency between the 
variables Field, Transport, and Game. We can add even more qualitative 
information relating the game status and radio commentary as follows: 
There is radio commentary when the game is on. 
Note that we have no explicit information stated on any dependency between the 
newly introduced variable Commentary and the two variables Field and 
Transport. 
A Bayesian belief network is a graphical, probabilistic knowledge 
representation of a collection of random variables (e.g. Game, Field, Transport, 
and Commentary) describing some domain. Nodes of a belief network denote 
random variables; links are added between the nodes to denote causal 
relationships between the variables. The topology encodes the qualitative 
knowledge about the domain. (These descriptions are usually in the form of 
“causal relationships”  among variables.) Conditional probability tables (CPTs) 
encode the quantitative details (strengths) of the causal relationships, such as the 
following: 
If the field is dry and transport is present there is a 90% chance that the game is 
on. 
The CPT of a variable without any parents consists of just its a priori (or prior) 
probabilities. The belief network of Figure 1-1 encodes the relationships over the 
domain consisting of the variables, Game, Field, Transport, and Commentary; its 
topology captures the commonsense about the variable dependencies discussed 
above. Each variable has a mutually exclusive and exhaustive set of possible 
states. For example, dry and wet are the states of the variable Field. 
As shown in the figure, the CPT specifies the probability of each possible 
value of the child variable conditioned on each possible combination of parent 
variable values. The probability that the game is on given the field is dry and the 
transport is present is 0.90, whereas the probability that the game is cancelled 
given the same two conditions is 0.09. Similarly, the probability of radio 
commentary given the game is on is 0.95. The prior probability that the field is 
dry is 0.8. Each CPT column must sum up to 1.0 as the states of a variable are 
mutually exclusive and exhaustive. Therefore, the probability that the game is 
delayed given that the field is dry and transport is present is 1.0
0.9
0.09
−
−
, or 
0.01.  

 
Modeling Agent Epistemic States 
11 
 
Transport
Game
Field
on
cancelled
delayed
present
absent
dry
wet
(
|
,
)
0.90
...
...
...
0.09
...
...
...
0.01
...
...
...
p Game Field Transport
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
0.8
(
)
0.2
p Field
⎡
⎤
= ⎢
⎥
⎣
⎦
0.9
(
)
0.1
p Transport = ⎡
⎤
⎢
⎥
⎣
⎦
Commentary
yes
no
(
|
)
0.95
...
...
0.05
...
...
p Commentary Game
⎡
⎤
⎢
⎥
⎣
⎦
 
Figure 1-1: Example Bayesian Belief Network 
As mentioned above, the structure of a belief network encodes other 
information as well. Specifically, the lack of links between certain variables 
represents a lack of direct causal influence, that is, they indicate conditional 
independence relations. This belief network encodes many independence 
relations, for example: 
{
,
}|
Commentary
Field Transport
Game
⊥
 
which states that Commentary is independent of Field and Transport given 
Game. In other words, once the value of Game is known, the values of Field and 
Transport add no further information about Commentary. Therefore, 
independence between two nodes is represented by the absence or blocking of 
links between the two nodes. Whether any dependence between a pair of nodes 
exists or not is determined by a property called d-separation. Conditional 
independence among variables in a belief network allows factorization of a joint 
probability distribution, thus eliminating the need for acquiring all the 
probabilities in a distribution. For example, the joint distribution of the variables 
in the belief network in Figure 1-1 is factorized as follows: 
(
,
,
,
)
(
) (
) (
|
,
) (
|
)
p Field Transport Game Commentary
p Field p Transport p Game Field Transport p Commentary Game
=
 
Only 13 (=1 1
8
3
+ +
+
) probabilities are required to compute the joint probability 
distribution in this case, as opposed to 23 (= 2
2 3 2 1
× × ×
−) when no causal 
relationship information is given. 

12 
Decision-Making Agents 
 
 
When new evidence is posted to a variable (i.e. its state is determined) in a 
belief network, that variable updates its own state probabilities that constitute its 
belief vector and then sends out messages indicating updated predictive and 
diagnostic support vectors to its children and parent nodes respectively. The 
messages are used by the other nodes, which update their own belief vectors, and 
also propagate their own updated support vectors. The probability distribution of 
a variable in a belief network at any stage is the posterior probability of that 
variable given all the evidence posted so far. In the case of polytrees, the 
separation of evidence yields a propagation algorithm (Pearl, 1988) in which 
update messages need only be passed in one direction between any two nodes 
after the posting of evidence. See (Jensen, 1996) for a propagation algorithm for 
the more general case of directed acyclic graphs (DAGs). 
It is fairly straightforward to model a decision-making problem in terms of a 
belief network, where a set of mutually exclusive decision options can be 
encoded as the states of a random variable representing the decision node within 
the network. Other attributes from the problem domain are also encoded as 
random variables connected to the decision node as per their causal relationships. 
Moreover, belief networks are suitable for synthesizing knowledge at higher 
levels of abstraction by performing aggregations on low-level evidence. Our 
experience with various subject matter experts suggests that their thought process 
during information aggregation is very similar to what the graphical 
representation of belief networks depicts via their dependencies among random 
variables. 
A belief network is built on the concept of random variables, but a decision 
making process often involves reasoning explicitly with actions and utilities. 
Influence diagrams are belief networks augmented with decision variables and a 
utility function to solve decision problems. There are three types of nodes in an 
influence diagram: chance nodes  (i.e. belief network nodes), decision nodes, and 
value or utility nodes. Using an influence diagram, agents will therefore be able 
to directly reason about, for example, what loss might be incurred if a decision 
for proceeding with the game under rainy conditions is made. 
1.4 
Possible World Epistemic Model 
If an agent receives information from a dynamic and uncertain environment, then 
the agent’s understanding about the environment will be based on its own 
“beliefs,” and potential realizations of each of the decision making options will 
naturally yield a possible world that the agent can transition to. Reasoning within 

 
Modeling Agent Epistemic States 
13 
 
an uncertain environment therefore requires an appropriate underlying 
knowledge representation language incorporating the concepts of belief and 
possibility in its semantics, along with a sound inference mechanism. Logical 
formalisms are especially appealing from the knowledge representation point of 
view. However, fragments of standard first-order logic in the form of Horn 
clauses used in most of these formalisms are often inadequate. Researchers have 
therefore moved to higher-order and non-classical logics. The notion of 
possibility in classical modal logics and the notion of belief in their epistemic 
interpretations together provide a useful means for representing an agent’s 
uncertain knowledge. The possibility concept naturally blends with the concept 
of decision options triggering agents to consider different possible worlds 
corresponding to those options. The language in the possible world epistemic 
model is governed by epistemic interpretations of the language of modal logics. 
The original purpose behind the revival of modern modal logic by Lewis in 
his book A Survey of Symbolic Logic (Lewis, 1918) was to address issues related 
to material implication of Principia Mathematica (Whitehead and Russell, 
1925−1927), by developing the concept of strict implication.  The language of 
modal propositional logics extends the language of classical propositional logic 
with two modal operators   (necessary) and ◊ (possibility). For example, when 
the necessary operator is applied to the sentence “If the field is wet then the game 
is cancelled” (symbolized as 
(
)
P
Q
→

, where P stand for “the field is wet” and 
Q stands for “the game is cancelled”), the resultant sentence is read as “It is 
necessary that if the field is wet then the game is cancelled.” The interpretation of 
(
)
P
Q
→

 in terms of possible worlds is that in every world that the agent 
considers possible, if the field is wet then the game is cancelled. Similarly, the 
interpretation of 
(
)
◊P
Q
→
 is that in at least one of worlds that the agent 
considers possible, if the field is wet then the game is cancelled. 
Modal epistemic logics (Hintikka, 1962) are instances of modal logics 
constructed by interpreting necessity and possibility in a suitable manner for 
handling mentalistic constructs for knowledge and belief. If an agent’s epistemic 
state contains 
(
)
bel
P
Q
〈
〉
→
, where the bel
〈
〉 represents the modal operator for 
belief analogous to the operator  , then in every world that the agent considers 
possible (or accessible from the current world), the agent believes that if the field 
is wet, then the game is cancelled. It may so happen that in one of the worlds that 
the agent considers possible the game is played despite the field being wet. In 
this case, the agent does not believe in P
Q
→
. 

14 
Decision-Making Agents 
 
 
Various modal systems are built by adding axioms to the base modal system 
K.. For example, the system T is obtained from K by adding the axiom 
F
F
→

, 
which states that whatever is necessary is also true in the current world. Various 
other modal systems, including the well-known S4, B, and S5 systems  are also 
obtained by adding appropriate axioms to K. The possible world interpretation of 
the modal operators as explained above is exactly what Kripke (1963) proposed 
as semantics for various modal systems. The semantics of each modal system 
imposes some restrictions on the accessibility among possible worlds. For 
example, the accessibility relation is reflexive in the case of system T. Axiomatic 
deductions in modal propositional logics are carried out in a manner similar to 
standard propositional logic, i.e. using a set of inference rules and axioms. 
The syntax of modal first-order logics can be obtained simply from the 
classical first-order logic by the introduction of modalities into the language. 
Then we can produce first-order modal logics analogous to various propositional 
modal systems such as K, D, T, S4, B, and S5. In principle, the semantics of these 
first-order modal systems could simply be obtained by extending the possible 
world semantics for modal propositional systems. However, lifting up modal 
propositional systems to their counterparts in modal first-order logics raises some 
interesting issues regarding the existence of objects in various possible worlds 
and interactions between the modal operators and quantifiers. For example, in a 
possible world there may be more or fewer objects than in the current world. 
Adding the Barcan Formula (BF) (Barcan, 1946)  can usually axiomatize first-
order modal logics that are adequate for applications with a fixed or non-growing 
domain: 
BF: 
[ ]
[ ]
x
F x
xF x
∀
→∀


 
where 
[ ]
F x  means F has zero or more free occurrences of the variable x. The 
above axiom means that if everything that exists necessarily possesses a certain 
property F then it is necessarily the case that everything possesses F. But in the 
cumulative domain case there might be some additional objects in accessible 
worlds that may not possess the property F.  
In general, there is no equivalent to the standard, first-order logic, clausal 
representation of an arbitrary modal formula where each component in the clause 
is one of the atomic modal formulae P, 
P
¬
, ◊P , and 
P

. But a modal formula 
can be transformed into an equivalent formula in first-order like world-path 
syntax, which can then be transformed into a set of clauses in the same way as in 
the first-order logic. For example, the formula 
( )
◊xP x
∀
 is transformed into 

 
Modeling Agent Epistemic States 
15 
 
([0 ], )
xP
a x
∀
, meaning that from the initial world 0 there is a world a accessible 
from 0 such that for all x 
( )
P x  holds, where P is interpreted in the world a. On 
the other hand, 
( )
xP x
∀

 is transformed into 
([0 ], )
u xP
u x
∀∀
, meaning that 
from the initial world 0 and for every world w that is accessible from a for all x, 
( )
P x  holds, where P is interpreted in the world u. Therefore, the symbolization 
of “It is necessary that every wet field is unsuitable for playing game”: 
(
( ,
)
( ))
x Field x Wet
Unsuitable x
∀
→

 
can be transformed into the following clause in world-path syntax: 
(
([0 ], ,
)
([0 ], ))
u x
Field
u x Wet
Unsuitable
u x
∀∀
¬
∨
 
Resolution theorem proving in this setting (Ohlbach, 1988) is carried out on a set 
of clauses of the above form in a way similar to standard first-order resolution 
theorem proving except for the unification of terms like [0 ]
u , which is carried 
out by taking into account the type of accessibility relation among possible 
worlds. 
The notion of possibility in classical modal logics is useful for representing 
and reasoning about the uncertain knowledge of an agent. However, this coarse 
grain representation of the knowledge of possibilities about assertions through 
this logic is quite inadequate for practical applications. That is to say, if two 
assertions are possible in the current world, they are indistinguishable in the 
modal formalism even if an agent knows that one of them is true in twice as 
many possible worlds as compared to the other one. Therefore, any epistemic 
interpretation of the modal operators for necessity and possibility would fail to 
incorporate the notion of an agent’s “degree of belief” in something into its 
epistemic states. 
Therefore, we developed an extended formalism of modal epistemic logic to 
allow an agent to represent its degrees of support about an assertion. The degrees 
are drawn from qualitative and quantitative dictionaries that are accumulated 
from the agent’s a priori knowledge about the application domain. We extend the 
syntax of traditional modal epistemic logic to include an indexed modal operator 
d
sup
〈
〉 to represent an agent’s degrees of support about an assertion. In this way, 
the proposed Logic of Agents Beliefs (LAB)  can model an argument that merely 
supports an assertion (to some extent d), but does not necessarily warrant an 
agent committing to believe in that assertion. The extended modal logic is given 
a modified form of possible world semantics by introducing the concept of an 
accessibility hyperelation. 

16 
Decision-Making Agents 
 
 
1.5 
Comparisons of Models 
Table 1-1 presents a comparison of various features of the three epistemic models 
presented in the last three sections. Detailed pros and cons of individual decision 
making frameworks under these models (e.g. logical rules and belief networks) 
and their suitability for different application types are discussed in their 
corresponding chapters. 
 
Underlying 
formalism 
Qualitative 
information 
Uncertainty 
handling 
Inferencing 
mechanism 
Efficiency 
enhancement 
Proposi-
tional 
Classical 
logics 
Logical 
sentences 
None 
Theorem 
proving 
Horn clauses, 
resolution 
Probabi-
listic 
Probability 
theory 
Causal 
networks 
Fine-
grained 
Evidence 
propagation 
Conditional 
independence 
Possible 
worlds 
Modal 
logics 
Modal 
sentences 
Coarse-
grained 
Theorem 
proving 
Modal 
resolution 
Table 1-1: Comparison of features of epistemic models 
The underlying formalism for the propositional epistemic model is the 
classical propositional and first-order logics, whereas various systems of modal 
logics formalize the possible world epistemic model. The probabilistic epistemic 
model, as its name suggests, is formalized by the theory of probability, especially 
Bayesian probability. 
Qualitative domain information in the propositional epistemic model is 
expressed by user-defined relations among domain objects and by logical 
connectives among various assertions, producing sentences. Qualitative domain 
information in the probabilistic epistemic model is in the form of causal 
relationships among various domain concepts represented as random variables, 
producing belief networks. Qualitative domain information in the possible world 
epistemic model is also expressed by user-defined relations and logical 
connectives. Additionally, modalities are allowed to apply to sentences to 
produce qualitative domain information. 
The truth-value of an assertion in the classical logics is Boolean (either true 
or false), and therefore the propositional epistemic model is unable to handle 
uncertainties in assertions. Modal logics provide a very coarse-grained 
uncertainty representation through their modal operator for possibility. An 
agent’s belief in an assertion does not imply that the assertion is true in the world 

 
Modeling Agent Epistemic States 
17 
 
the agent is in. The belief rather describes the agent’s uncertain assumption of the 
true nature of the assertion. Uncertainty handling in the probabilistic epistemic 
model is fine-grained in the sense that an agent can choose any number from the 
dense real-valued dictionary [0,1] of probability as its degree of belief in a 
random variable state. 
The underlying inferencing mechanism for deriving implicit assumptions in 
the propositional and possible world epistemic models is based on axiomatic 
theorem proving. The inferencing mechanism for deriving prior and posterior 
probability distributions in the probabilistic epistemic model is based on the 
application of Bayes’ rule, or takes the form of evidence propagation when 
models are available as causal belief networks.  
To enhance inferencing efficiency in the propositional epistemic model, the 
resolution theorem proving technique is employed. The possible world epistemic 
model employs the same concept in the context of modal logics. To further 
enhance inferencing efficiency, sentences in the propositional epistemic model 
are often expressed in the Horn clause syntax, which is less general than full 
first-order language but expressive enough to deal with most common 
applications. Expert system rules are just positive Horn clauses. Various 
conditional independences among random variables are assumed in the 
probabilistic epistemic model.  
1.6 
P3 Model for Decision-Making Agents 
So far we have described three models of epistemic states, namely propositional, 
possible world, and probabilistic, and compared their features. Propositional and 
possible world models are ideal for representing agent knowledge at a higher 
level natural language-like syntax and for thinking semantics in terms of possible 
worlds, and the probabilistic model provides the required granularity with 
probabilities representing uncertainty in sentences that we use in everyday life, 
but not one of these models by itself can sufficiently capture an agent’s epistemic 
states. Consequently, there is a need for suitably combining these three models 
into one. The focus of the final part of this book is to develop such an integrated 
model of agent epistemic states, called P3 (Propositional, Probabilistic, and 
Possible World). The P3 model is based on a logical language with embedded 
modalities and probabilities and is particularly suitable for making decisions in 
the context of choosing between possible courses of action or between possible 
assertions for belief revision. An intelligent agent continuously performs this 
kind of cognitive task when making decisions. 

18 
Decision-Making Agents 
 
 
The human decision-making process can be regarded as a complex 
information processing activity. According to (Rasmussen, 1983), the process is 
divided into three broad categories that correspond to activities at three different 
levels of complexity. At the lowest level is skill-based sensorimotor behavior,  
representing the most automated, largely unconscious level of skill-based 
performance such as deciding to brake upon suddenly seeing a car ahead. At the 
next level is rule-based behavior  exemplified by simple procedural skills for 
well-practiced, simple tasks, such as inferring the condition of a game-playing 
field based on the current rainy weather. Knowledge-based behavior represents 
the most complex cognitive processing. It is used to solve difficult and 
sometimes unfamiliar problems, and for making decisions that require dealing 
with various factors and uncertain data. Examples of this type of processing 
include determining the status of a game given the observation of transport 
disruption. 
The proposed P3 model, grounded in the logic LAB introduced earlier, 
supports embedding the human decision-making process within an agent at the 
knowledge base level by providing suggestions on alternative courses of action, 
and helping to determine the most suitable one. Human decision-makers often 
weigh the available alternatives and select the most promising one based on the 
associated pros and cons. The P3 model will represent these pros and cons as 
logical sentences with embedded probabilities as follows: 
0.7
bel Heavy Rain
sup
Cancelled
〈
〉
→〈
〉
 
The above sentence can be interpreted as follows: if the agent believes that it 
rained heavily then it asserts that there is a 70% chance (equivalently, generates 
an amount of support 0.7) that the game will be cancelled. An agent may obtain 
evidence from different sources as support for the cancellation, as well as support 
against the cancellation, as follows: 
0.6
bel Club FinancialCrisis
sup
Cancelled
〈
〉
→〈
〉¬
 
The above sentence states that if the club is in financial crisis then there is a 60% 
chance that the cancellation will be avoided. These types of P3 sentences that 
provide support both for and against various decision options, constitute 
arguments used by the agent to solve a decision problem. Such an 
argumentation-based decision-making framework has been developed in (Das et 
al. 1997; Fox and Das, 2000). 
The set of evidence for and against a certain decision option F must be 
aggregated to come up with the overall support for F. This aggregation process 

 
Modeling Agent Epistemic States 
19 
 
can be implemented using Dempster’s combination rule within the framework of 
the Dempster-Shafer theory of belief functions (Shafer, 1976; Yager et al., 1994), 
but this requires supports to be interpreted as a mass distribution, as opposed to a 
probability distribution, along with an appropriate evidence independence 
assumption. If supports are drawn from the dictionary of probability, any 
aggregation process must be consistent with the theory of probability, which is a 
special case of Dempster-Shafer theory. The belief network technology can 
perform such an aggregation process on arguments “for” options, but not on 
arguments “against” an option. Its propagation mechanism is consistent with the 
theory of probability. To illustrate this, let us explain the use of belief network 
technology for aggregation by considering the following two sentences 
supporting the option Cancelled: 
0.7
0.9
bel Heavy Rain
sup
Cancelled
bel TerroristThreat
sup
Cancelled
〈
〉
→〈
〉
〈
〉
→〈
〉
 
The agent will now go through an aggregation process to decide whether or not 
to believe in game cancellation. One approach is based on the assumption that the 
heavy rain and terrorist threat conditions are a set of independent conditions that 
are likely to cause the cancellation of game and that this likelihood does not 
diminish or increase when several of these conditions prevail simultaneously. 
This is the effect of the noisy-or technique in belief networks, which is usually 
applied to generate conditional probabilities in large tables for nodes with 
multiple parents. In many applications this approach makes sense, as experts will 
often enumerate a list of factors causally influencing a particular event along with 
their associated uncertainty. The belief network in Figure 1-2 is constructed out 
of the two sentences mentioned above for aggregating all evidence for the 
cancellation of the game. 
Terrorist Threat
Cancelled
Heavy Rain
CPT computed using:
(
|
)
(
|
)
p Cancelled Heavy Rain
p Cancelled Terrorist Threat
  
 
  
 
 
Figure 1-2: Aggregation with belief networks 

20 
Decision-Making Agents 
 
 
As shown in the figure, there are two parents of the node Cancelled, 
corresponding to the two sentences. The sentences provide the following 
probabilities: 
(
|
)
0.7
(
|
)
0.9
where
,
,
p C HR
p C TT
C
Cancelled HR
Heavy Rain TT
TerroristThreat
=
=
≡
≡
≡
  
By applying the noisy-or technique, the following probabilities for the table are 
computed: 
(
|
,
)
0.7
(
|
,
)
(
|
)
(
|
)
(
|
)
(
|
)
0.97
(
|
,
)
0.9
(
|
,
)
0
p C HR
TT
p C HR TT
p C HR
p C TT
p C HR
p C TT
p C
HR TT
p C
HR
TT
¬
=
=
+
−
×
=
¬
=
¬
¬
=
 
As evidence is posted into the network, the posterior probability (or the agent’s 
degree of belief in the cancellation node) changes. An agent may decide to accept 
the cancellation of the game as its belief and bel Cancelled
〈
〉
will be added to its 
database.  
To summarize the P3 model, we use the syntax of modal propositional logics 
for representing arguments, and include probabilities to represent their strengths. 
The use of modal logics allows agents to consider decision options in terms of 
the intuitive possible world concept, each of which is the result of committing to 
one decision option. When aggregating a set of arguments to choose a decision 
option, we can either apply the Dempster-Shafer theory of belief functions with 
an appropriate evidence independence assumption, or apply the belief network 
evidence propagation technique on a restricted set of arguments. 
Before we delve into the details of various agent epistemic models, it is 
worth pointing out the role of belief revision in the context of our proposed agent 
decision-making paradigm. As an agent perceives an environment, it revises its 
own belief about the environment, which amounts to updating some of its own 
knowledge representing its own epistemic state. The monotonic nature of logic-
based modeling of epistemic states does not automatically allow such a belief 
revision process. The agent can potentially continue accumulating knowledge as 
long as the knowledge base is not inconsistent (i.e. deriving a formula F and its 
negation). A special inference rule or some kind of meta-level reasoning is 
required for belief revision in case an inconsistency arises between the 
observation and what is already in the epistemic state. 

 
Modeling Agent Epistemic States 
21 
 
The belief network-based probabilistic epistemic model is more flexible in 
accommodating observations from uncertain environments because a consistent 
probability assignment can always be obtained to yield a consistent epistemic 
state unless it is a hard inconsistency (for example, the agent already believes in 
F with degree of support 1.0, but subsequently observed its negation with degree 
of support 1.0) or unless a soft probabilistic consistency concept is incorporated 
as in (Das and Lawless, 2005). Various propagation algorithms for computing 
posterior probabilities of random variables based on their priors and observations 
do these assignments. Our approach to decision making here deals with a 
“snapshot” of the agent epistemic state rather than its evolution over time. 
Moreover, the argumentation based P3 model does not require an epistemic state 
to be consistent because a hard inconsistency will at worst yield a dilemma 
amongst decision options. 

This page intentionally left blank
This page intentionally left blank

23 
Chapter 2 
 
Mathematical Preliminaries 
 
 
 
 
This chapter provides the background mathematical notations and concepts 
needed to understand the rest of the book. The topics discussed in this chapter 
include sets, relations, and functions (Section 2.2), graphs and trees (Section 2.3), 
basic probability theory (Section 2.4), and a concise introduction to the theory of 
algorithmic complexity (Section 2.5). (We will use this theory to analyze 
expected run time performance of logical inference schemes and evidence 
propagation algorithms.) But first, we explain our conventions for symbol usage.  
2.1 
Usage of Symbols 
In general, the following conventions are used for symbols representing terms, 
random variables, clauses, sets, and so on: 
Item 
Convention 
Example 
 Variables in a logical 
language 
 Variables 
representing random 
variable states 
 Italicized, lower case 
letters 
 
1
1
1
, , ,...,
,
,
,...
x y z
x y z
 
 Probabilistic random 
variables 
 Graph nodes 
 Italicized, upper case 
letters 
 Italicized string starting 
with an upper case letter 
 
, , ,
,
, ,
,...,
X Y Z N U V W
 
1
1
1
1
1
1
1
,
,
,
,
,
,
,...
X Y Z N U V W
 
 Rain, Game 
 Predicate symbols 
 Italicized upper case 
letters 
 Italicized string starting 
with an upper case letter 
 
1
1
1
,
, ,...,
,
,
,...
P Q R
P Q R
 
 Even, Employee 
 Constant symbols in 
a logical language 
 Random variable 
states 
 Italicized, lower case 
letters 
 Italicized string starting 
with a lower case letter 
 
1
1
1
, , ,...,
,
,
,...
a b c
a b c
 
 medium, yes, john, 
cancelled 

24 
Decision-Making Agents 
 
 
 Function symbols 
 Italicized, lower case 
letters 
 Italicized string starting 
with a lower case letter 
 
1
1
1
, , ,...,
,
,
,...
f g h
f g h
 
 succ  
 Atoms 
 Literals 
 Italicized, upper case 
letters 
 
, , ,
,...,
A B L M
 
    
1
1
1
1
,
,
,
,...
A B L M
 
 Formulae 
 Goals 
 Clauses 
 Italicized, upper case 
letters 
 
1
1
1
,
,
,
,
,
,...
F G H F G H
 
 Sets 
 Bold, Times-Roman 
typeface, upper case 
letters 
 
1
1
1
, , ,...,
,
,
,...
A B S
A B S
 
 Special logical 
systems 
 Special logical sets 
 Monotype Corsiva typeface 
string starting with an 
upper case letter 
 Pos, PL, S5, LAB 
 Prolog program code 
 Courier typeface 
 append([], L, L) 
New terminology is italicized on its first use. Propositions, theorems, figures, and 
tables are numbered by chapter and section. 
2.2 
Sets, Relations, and Functions 
Given a set S, a∈S  means “a is an element of S,” “a belongs to S,” or “a is a 
member of S.”  Similarly, a∉S  means “a is not an element of S.” A shorthand 
notation for a set S is 
{ |
( )}
x P x
=
S
 
which is read as “S is the set of all elements x such that the property P holds.” 
When it is possible to list all the elements, or at least to list a sufficient number to 
clearly define the elements of set S, then S is written as { , , ,...}
a b c
. The empty set 
has no elements and is denoted as Φ  or{}.  
A set A is a subset of a set S (or A is included in S, A is contained in S, or S 
contains A), denoted as 
⊆
A
S , if every element of A is an element of S. That is, 
a∈A  implies a∈S . So, for any set A, we always have 
⊆
A
A and Φ ⊆A . If 
two sets A and B are the same (or equal), then that is denoted as 
=
A
B ; set 
equality is equivalent to having both 
⊆
A
B and 
⊆
B
A . If 
⊆
A
S  and 
≠
A
S , 
then A is a proper subset of S and is denoted as 
⊂
A
S . In other words, if 
⊂
A
S  
then there exists at least one a∈S  such that a∉A . 

 
Mathematical Preliminaries 
25 
 
The union (or join) of two sets A and B is written as 
∪
A
B , and is the set 
{
or
}
x x
x
∈
∈
|
A
B . That is, it is the set of all elements that are members of either 
A or B (or both). The union of an arbitrary collection of sets is the set of all 
elements that are members of at least one of the sets in the collection. The 
intersection of two sets A and B, denoted 
∩
A
B , is the set {
and
}
x x
x
∈
∈
|
A
B , 
that is, the set of all elements that are members of both A and B. The intersection 
of an arbitrary collection of sets is the set of all elements that are members of 
every set in the collection. The difference of two sets A and B, denoted 
−
A
B , is 
the set {
and
}
x x
x
∈
∉
|
A
B , that is, the set of all elements that are members of A 
but not members of B. 
Example 
{1,4,7,9} , {{ , }, , }
a b c d , {0,1,2,3,...}, {
,
,
}
on cancelled delayed  are all examples 
of sets. Let N be the set {1,2,3,...} of all natural numbers. Then {
 is odd}
x
x
∈N |
 
is the set {1,3,5,...} . The set {4,7}  is a proper subset of {1,4,7,9} . Suppose A = 
{ , , }
a b c  and B = { , , }
a c d . Then 
∪
A
B  = { , , , }
a b c d , 
∩
A
B  = { , }
a c , 
−
A
B  = 
{ }
b .  
 
Suppose I is an arbitrary set and for each i∈I  we have a corresponding set 
i
A . Then the set of sets {
|
}
i i∈
A
I  is denoted as {
}
i
i∈I
A
, and I is called an index 
set. The join of all sets in the collection {
}
i
i∈I
A
 is denoted as 
i
i∈∪
I A  and the 
intersection is denoted as 
i
i∈∩
I A . 
Given a set S, the power set of S, denoted as 
( )
S
P
 or 2S , is the set 
{
|
}
⊆
A A
S , that is, the set of all possible subsets of S. If a set S contains n 
elements then 
( )
S
P
 contains 2n  elements. 
Example 
Suppose S = { , , }
a b c . Then 
( )
S
P
 = { ,{ },{ },{ },{ , },{ , },{ , },{ , , }}
a
b
c
a b
a c
b c
a b c
∅
.   
 

26 
Decision-Making Agents 
 
 
Given a collection of sets 
1
2
,
,...,
n
S S
S , the cartesian product of these n sets, 
denoted by 
1
2
...
n
×
×
×
S
S
S  or more concisely as 
1
n
i
i=∏S , is the set of all possible 
ordered n-tuples 
1
2
,
,...,
n
a a
a
〈
〉 such that 
i
i
a ∈S , for 
1,2,...,
i
n
=
. When each 
iS  
is equal to S  then the product is written as 
n
S . 
Example 
Suppose 
1
1
2
{ ,
}
a a
=
S
, 
2
{ }
b
=
S
, 
3
1
2
3
{ ,
,
}
c c c
=
S
. Then 
1
2
3
×
×
S
S
S  = {
1
1
, ,
a b c
〈
〉, 
1
2
, ,
a b c
〈
〉, 
1
3
, ,
a b c
〈
〉, 
2
1
, ,
a b c
〈
〉, 
2
2
, ,
a b c
〈
〉, 
2
3
, ,
a b c
〈
〉.   
 
A relation is formally described by a statement involving elements from a 
collection of sets. An n-ary relation R on the sets 
1
2
,
,...,
n
S S
S  is a subset of 
1
2
...
n
×
×
×
S
S
S . An n-ary relation R on a set S is a subset of 
n
S . If R is a binary 
relation on a set S, then the notation x y
R  implies that 
,x y
〈
〉∈R , where 
,x y∈S . A binary relation R on a set S is 
• 
serial if for all x∈S  there is a y∈S such that x y
R  
• 
reflexive if x x
R , for all x∈S  
• 
symmetric if x y
R  implies y x
R , for all ,x y∈S  
• 
transitive if x y
R  and y z
R  implies x z
R , for all , ,
x y z ∈S  
• 
euclidean if x y
R  and x z
R  implies y z
R , for all , ,
x y z ∈S  
• 
antisymmetric if x y
R  and y x
R  implies x
y
=
, for all ,x y∈S  
• 
equivalence if it is reflexive, symmetric, and transitive 
Example 
As relations on the real numbers, x
y
<
is serial and transitive; x
y
≤
is serial, 
reflexive, transitive, and antisymmetric; and x
y
=
is a simple equivalence 
relation.  
 

 
Mathematical Preliminaries 
27 
 
Given an equivalence relation R on a set S , the equivalence class of a∈S  is 
the set {
|
}
x
a x
∈S
R
 and is denoted by 
[ ,
]
Eq a
S
R . The following theorem 
provides an association between an equivalence relation and equivalence classes. 
Theorem 2-1: Let S  be a set and R  be an equivalence relation on S . Then R  
induces a decomposition of S  as a union of mutually disjoint subsets of S . 
(Informally, we say that R  partitions the set S .) Conversely, given a 
decomposition on S  as a union of mutually disjoint subsets of S , an 
equivalence relation can be defined on S  such that each of these subsets are 
the distinct equivalence classes. 
Proof: Since x x
R  holds, 
[ ,
]
x
Eq x
∈
S
R . Suppose 
[ ,
]
Eq a
S
R  and 
[ ,
]
Eq b
S
R  are 
two equivalence classes and let 
[ ,
]
[ ,
]
x
Eq a
Eq b
∈
∩
S
S
R
R . Then since 
[ ,
]
x
Eq a
∈
S
R , a x
R . Since 
[ ,
]
x
Eq b
∈
S
R , b x
R , i.e., x b
R , since R is symmetric. 
Now, a x
R  and x b
R , and therefore by transitivity of R, a b
R .  
Suppose 
[ ,
]
x
Eq b
∈
S
R  and thus b x
R . a b
R  is already proved. Therefore, by 
transitivity of R, a x
R  and hence 
[ ,
]
x
Eq a
∈
S
R . Thus, 
[ ,
]
[ ,
]
Eq a
Eq b
⊆
S
S
R
R . 
Similarly, we can prove that 
[ ,
]
[ ,
]
Eq b
Eq a
⊆
S
S
R
R . Thus, 
[ ,
]
[ ,
]
Eq b
Eq a
=
S
S
R
R  
and hence equivalence classes are either identical or mutually disjoint. 
Now, every s∈S is in an equivalence class, namely 
[ ,
]
Eq s
S
R , we have our 
decomposition of S into a union of mutually disjoint subsets, i.e. the equivalence 
classes under R.  
To prove the converse, suppose 
α
α∈
= ∪
I
S
S , where I is an index set and 
α
β
∩
= ∅
S
S
, when α
β
≠
 and 
,
α β ∈I . Define an equivalence relation R on S 
as x y
R if and only if x and y belong to the same 
α
S . It is then straightforward to 
verify that R is an equivalence relation on S.  
Example 
Let S be the set of all integers. Define a relation 
5
Mod  on S as 
5
xMod y  if and 
only if x
y
−
 is divisible by 5, for ,x y∈S . (We say x is congruent to y modulo 
5.) Then: 
• 
0
x
x
−
=
and 0 is divisible by 5. Therefore, 
5
xMod x , so
5
Mod  is 
reflexive. 

28 
Decision-Making Agents 
 
 
• 
If x
y
−
 is divisible by 5 (
5
xMod y ) then y
x
−
 is also divisible by 5 
(
5
yMod x ). Therefore, 
5
Mod  is symmetric. 
• 
If x
y
−
 is divisible by 5 (
5
xMod y ) and y
z
−
 is divisible by 5 
(
5
yMod z ), then (
)
(
)
x
y
y
z
x
z
−
+
−
=
−
 is also divisible by 5 (
5
xMod z ). 
Therefore, 
5
Mod  is transitive. 
Hence, 
5
Mod  is an equivalence relation on S. (In fact, such an equivalence 
relation 
i
Mod  exists on S, for every positive integer i.) The equivalence class of 
3 is 
5
[3,
]
Eq
Mod
S
 = {..., 7, 2,3,8,13,...}
−
−
.  The mutually disjoint equivalence 
classes corresponding to 
5
Mod  are {..., 10, 5,0,5,10,...}
−
−
, {..., 9
4,1,6,11,...}
−−
, 
{..., 8, 3,2,7,12,...}
−
−
, {..., 7, 2,3,8,13,...}
−
−
, and { 6, 1,4,9,14,...}
−
−
.  
 
Functions provide a way of describing associations between elements of 
different sets.  
Let A and B be two non-empty sets. Then a function f (or mapping) from A 
to B, denoted as 
:
f
→
A
B , is a subset C of 
×
A
B  such that for every x∈A  
there exists a unique y∈B  for which 
,x y
〈
〉∈
×
A
B . 
Clearly, any function 
:
f
→
A
B  is a particular kind of relation on A and B. 
The element x is said to have been “mapped into y” (or “y is the image of x” or “y 
is the value of f at x”) and y is denoted as 
( )
f x ; we usually write 
( )
y
f x
=
. 
Also, 
(
f A)  denotes the set {
:for some 
,
,
}
y
x
x y
∈
∈
〈
〉∈
B
A
C  and is called the 
image of f. The set A is called the domain of f and B is called the range of f.  
A mapping 
:
f
→
A
B  is said to be an identity mapping if 
( )
f x
x
=
, for all 
x∈A (so that implicitly 
⊆
A
B ). The mapping is said to be onto (or surjective) if 
given y∈B , there exists an element x∈A  such that 
( )
f x
y
=
. The mapping is 
said to be one-to-one (or injective) if for all 
,x y∈A , x
y
≠
implies that 
( )
( )
f x
f y
≠
. The mapping is a one-to-one correspondence between the two sets 
A and B (or bijective) if it is a one-to-one and onto mapping from A to B. 
Two mappings 
:
f
→
A
B  and 
:
g
→
A
B  are equal if for all x∈A , 
( )
( )
f x
g x
=
 (we usually write f
g
≡
). The composition of the two mappings 
:
f
→
A
B  and 
:
g
→
B
C , denoted as g
f
D
(or just gf in some cases), is a 
mapping 
:
g
f
→
A
C
D
 defined by (
)( )
( ( ))
g
f
x
g f x
=
D
, for all x∈A . 

 
Mathematical Preliminaries 
29 
 
Now, ((
)
)( )
f
g
h x
D
D
 = (
)( ( ))
f
g h x
D
 = 
( ( ( )))
f g h x
 = 
((
)( ))
f
g h x
D
 = 
(
(
))( )
f
g h
x
D
D
, that is, (
)
(
)
f
g
h
f
g h
≡
D
D
D
D
, where f, g and h are mappings 
with appropriate domains and ranges. Thus, the mapping composition operation 
follows the associative property, making the positions of parentheses irrelevant 
when mappings are composed. 
Example 
Suppose I, E, O, and N are, respectively, the sets of all integers, even integers, 
odd integers and natural numbers (i.e. positive integers). Let 
0
N  be 
{0}
∪
N
. 
Then the mapping 
:
f
→
E
O  defined by 
( )
1
f x
x
=
+  is a one-to-one 
correspondence between E and O. The mapping 
0
:
f
→
I
N  defined by 
( ) |
|
f x
x
=
 is onto but not one-to-one. The mapping 
0
:
f
→
I
N  defined by 
2
( )
f x
x
=
 is neither one-to-one nor onto. 
 
2.3 
Graphs and Trees 
The fundamental modeling tools known as graphs and trees are introduced in this 
section. Graphs and trees are most often presented graphically (hence their 
names) as in the examples below; we give their formal non-graphical definitions 
here, as this background is useful for issues such as theoretical discussions and 
algorithm development. 
A simple graph G is a pair 
,
〈
〉
V E , where V is a non-empty set of elements 
called vertices (or nodes), and E is a set of unordered pairs of distinct elements of 
V called edges.  Edges are denoted as 
i
j
N N , where 
,
i
j
N N ∈V . The definition 
of a directed graph  (or digraph) is given in the same way as a graph except that 
the set E is a set of ordered pairs (
i
j
j
i
N N
N N
≠
) of elements of V called directed 
edges. A simple graph is said to have been obtained from a directed graph by 
removing the direction of each of its edges.  
A path  (of length m) in a simple graph 
,
〈
〉
V E  is a finite sequence of edges 
of the form 
0
1
1
2
1
,
,...,
m
m
N N N N
N
N
−
 
where 
1
i
i
N N + ∈E  for each term in the sequence, and where the 
i
N ∈V are 
distinct vertices (except possibly 
0
m
N
N
=
).  Informally, a path consists of a 

30 
Decision-Making Agents 
 
 
sequence of “hops” along edges of the graph to distinct vertices. The sequence 
above can be written in abbreviated form as  
0
1
2
1
...
m
m
N
N
N
N
N
−
−
−
−
−
−
 
Therefore, if 
0
1
...
m
N
N
N
−
−
−
 is a path in a simple graph then 
1
0
...
m
m
N
N
N
−
−
−
−
 is also a path in the graph. Similarly, a path (of length m) in 
a directed graph 
,
〈
〉
V E  is a finite sequence of directed edges of the form 
0
1
2
1
...
m
m
N
N
N
N
N
−
→
→
→
→
→
 
where each ordered pair 
1
i
i
N N + ∈E  in the sequence is understood to be a 
directed edge of the digraph.  
Two nodes are connected if there is path between them. A cycle (of length 
1
m + ) in a simple graph 
,
〈
〉
V E  is a finite sequence of edges of the form 
0
1
2
0
...
m
N
N
N
N
N
−
−
−
−
−
, where 
2
m ≥
. A cycle (of length 
1
m + ) in a 
directed graph 
,
〈
〉
V E  is a finite sequence of edges of the form 
0
1
2
0
...
m
N
N
N
N
N
→
→
→
→
→
. Thus, if 
0
m
N
N
=
 in a path then we return to 
our starting point, and the path is called a cycle (or directed cycle in the case of a 
digraph). 
Example 
Figure 2-1a represents a simple graph 
,
〈
〉
V E , where V = 
1
2
3
4
5
{
,
,
,
,
}
N N
N
N
N
 
and E = 
1
2
1
3
2
4
3
4
3
5
{
,
,
,
,
}
N N
N N
N N
N N
N N
. Since each edge is an unordered pair 
of elements, 
i
j
j
i
N N
N N
=
, for all i, j. An example path of length 3 in this simple 
graph is 
1
2
4
3
N
N
N
N
−
−
−
. An example path of length 4 in this simple graph is 
1
2
4
3
1
N
N
N
N
N
−
−
−
−
. 
Figure 2-1b represents a directed version of the graph, where E = 
2
1
4
2
3
4
1
3
3
5
{
,
,
,
,
}
N N N N
N N
N N
N N
. An example path of length 3 in this directed 
graph is 
1
3
4
2
N
N
N
N
→
→
→
. An example path of length 4 in this directed 
graph is 
1
3
4
2
1
N
N
N
N
N
→
→
→
→
. 

 
Mathematical Preliminaries 
31 
 
N1
N2
N3
N4
N5
N1
N2
N3
N4
N5
 
(a) 
(b) 
Figure 2-1: Simple and directed graphs 
A simple graph is said to be acyclic if it has no cycles. A directed graph is 
said to be acyclic (or a directed acyclic graph or simply a DAG) if it has no 
cycles. Neither of the simple and directed graphs in Figure 2-1 is acyclic. 
Examples of simple and directed acyclic graphs are shown in Figure 2-2.  
N1
N2
N3
N4
N5
N1
N2
N3
N4
N5
 
(a) 
(b) 
Figure 2-2: Simple and directed acyclic graphs 
 
A simple graph is a polytree if and only if any two vertices of the graph are 
connected by exactly one path. A directed graph is a polytree if and only if its 

32 
Decision-Making Agents 
 
 
underlying simple graph is a polytree. Example polytrees are shown in Figure 
2-3. 
Suppose 
0
1
2
1
...
m
m
N
N
N
N
N
−
→
→
→
→
→
 is a path of a directed graph. 
The vertices occurring in this path are described in genealogical terms as follows: 
• 
1
i
N +  is a child of 
i
N , for 0
i
m
≤<
 
• 
1
i
N − is a parent of 
i
N , for 0
i
m
< ≤
 
• 
i
N  is an ancestor of 
j
N , for 0
i
j
m
≤<
≤
 
• 
j
N  is a descendant of 
i
N , for 0
i
j
m
≤<
≤
 
A leaf of a directed polytree is a node without any children in any path of the tree 
containing the node. A root of a directed tree is a node without any parent in any 
path of the tree containing the node. 
N1
N2
N3
N4
N5
N1
N2
N3
N4
N5
 
(a) 
(b) 
Figure 2-3: Simple and directed polytrees 
Example 
Figure 2-3a is a simple polytree and Figure 2-3b is a directed polytree. Consider 
the directed polytree in the figure: 
3
N  is the only child of 
1
N ; 
4
N  and 
5
N  are 
the children of 
3
N ; 
2
N  and 
3
N  are the parents of 
4
N ; and 
3
N  is a parent of 
4
N  
and 
5
N . 
3
N , 
4
N , 
5
N  are the descendants of 
1
N ; 
1
N  and 
3
N  are the ancestors of 
5
N ; 
1
N  and 
2
N  are the root nodes; and 
4
N  and 
5
N  are the leaf nodes. 

 
Mathematical Preliminaries 
33 
 
 
A directed graph is a tree if it is a polytree with only one root node. The level 
of a vertex in such a tree is the number of edges in the path between the vertex 
and the root. The depth of such tree is the maximum level of the vertices in the 
tree. The level of the root in a tree is 0. Example trees are shown in Figure 2-4. 
The root and leaf nodes of a simple polytree are not well-defined as its edges 
are undirected. For example, the node 
4
N  in the simple polytree in Figure 2-3a 
can be taken as a root node as well as a leaf node. To resolve this kind of 
ambiguity, we can designate a set of such vertices as root nodes and convert a 
simple polytree into a rooted tree. For example, the nodes 
1
N  and 
2
N  in Figure 
2-3a can be designated as roots. Unless otherwise stated, each reference in this 
book to the term “tree” will be implicitly regarded as ”rooted tree.” 
Example 
Figure 2-4a is a simple tree and Figure 2-4b is a directed tree. Vertex 
1
N  is the 
root node of the directed tree, whereas the vertex 
1
N has been designated as the 
root of the simple tree. The level of the vertices 
1
N , 
2
N , 
3
N , 
4
N , and 
5
N  are 0, 
1, 1, 2, and 2 respectively. Therefore the depth of each of these two trees is 2. 
N1
N2
N3
N4
N5
N1
N2
N3
N4
N5
 
(a) 
(b) 
Figure 2-4: Simple and directed trees 
 

34 
Decision-Making Agents 
 
 
2.4 
Probability 
Probabilities are defined in terms of likely outcomes of random experiments. A 
repetitive process, observation, or operation that determines the results of any 
one of a number of possible outcomes is called a random experiment.  An event 
is an outcome of a random experiment. The set of all possible outcomes of an 
experiment is called the sample space or event space.  
Example 
Random experiments and outcomes include: tossing a coin hundred times to 
determine the number of heads, rolling a pair of dice a couple of hundred times to 
determine the number of times the sum of the upturned faces is 7, observing the 
weather throughout the month of March to determine the number of sunny 
mornings, and sensing day temperatures over a month to determine the number 
of hot days. Therefore, tossing a head, rolling a six and a three, and a sunny 
morning are example events. The sets {
,
}
head tail , {(1,1),(1,2),...,(6,6)}, 
{
,
,
}
sunny rain snow , and { :
[0
,100
]}
o
o
t t
C
C
∈
 are, respectively, examples of 
sample spaces for these experiments. 
 
A probability provides a quantitative description of the likely occurrence of a 
particular event. The probability of an event x, denoted as 
( )
p x , is 
conventionally expressed on a scale from 0 to 1, inclusive. 
Example 
In the single die experiment, the probability of rolling a six is 1/6. There are 36 
possible combinations of numbers when two dice are rolled. The sample points 
for the two events x and y consisting of sums of 7 and 10 are respectively 
{(1, 6), (2, 5), (3, 4), (4, 3), (5, 2), (6, 1)}
x =
 
and 
{(4, 6), (5, 5), (6, 4)}
y =
. 
Hence, we have 
( )
6/36
p x =
, 
( )
3/36
p y =
. 
 
As defined above, an event consists of a single outcome in the sample space. 
Let us generalize this definition by calling it an elementary (or simple event or 
atomic event), and by defining a compound event as an event that consists of 
multiple simple events. In general, an event is either a simple event or a 

 
Mathematical Preliminaries 
35 
 
compound event. Set theory can be used to represent various relationships among 
events. In general, if x and y are two events (which may be either simple or 
compound) in the sample space S then: 
• 
x
y
∪
 means either x or y occurs (or both occur) 
• 
x
y
∩
 or xy means both x and y occur 
• 
x
y
⊆
 means if x occurs then so does y 
• 
x  means event x does not occur (or equivalently, the complement of x 
occurs) 
• 
Φ  represents an impossible event  
• 
S is an event that is certain to occur  
Two events x and y are said to be mutually exclusive if x
y
∩
= Φ .  (The 
occurrence of both x and y is impossible, and therefore the two events are 
mutually exclusive.) On the other hand, two events x and y are said to be 
independent if 
(
)
( )
( )
p x
y
p x
p y
∩
=
×
. As a result, when dealing with 
independent events x and y in an event space, the sets x and y must have a point 
(event) in common if both x and y have nonzero probabilities. Mutually 
exclusive, non-impossible events x and y cannot be independent as x
y
∩
= Φ , so 
that 
(
)
0
p x
y
∩
=
, but 
( )
( )
0
p x
p y
×
≠
. 
Example 
Suppose in the two-dice experiment we want to find the probability that the first 
die shows even and the second die shows odd. We consider the event x as the set 
of all sample points with the first element even and event y as the set of all 
sample points with the second element odd. Therefore, x is {(2,1),(2,2),...,(6,6)} 
and y is {(1,1),(2,1),...,(6,5)}. Each of these two events has 18 points and the two 
sets have nine points in common. Hence, 
( )
18/36
p x =
, 
( )
18/36
p y =
, and 
(
)
9/36
p x
y
∩
=
. Therefore, 
(
)
( )
( )
p x
y
p x
p y
∩
=
×
 holds. So by definition, x 
and y are independent. 
 
There are three approaches that provide guidelines on how to assign 
probability values: 
• 
The classical approach 

36 
Decision-Making Agents 
 
 
• 
The relative frequency approach 
• 
The axiomatic approach 
In the classical approach, the probability of an event x in a finite sample 
space S is defined as follows: 
( )
( )
( )
n x
p x
n S
=
 
where (
)
n X  is the cardinality of the (finite) set X. Since x
S
⊆
, 0
( )
1
p x
≤
≤ and 
( )
1
p S = . 
In the relative frequency approach, the probability of an event x is defined as 
the ratio of the number (say, n) of outcomes or occurrences of x to the total 
number (say, N) of trials in a random experiment. The choice of N depends on 
the particular experiment, but if an experiment is repeated at least N times 
without changing the experimental conditions, then the relative frequency of any 
particular event will (in theory) eventually settle down to some value. The 
probability of the event can then be defined as the limiting value of the relative 
frequency:  
( )
lim
n
n
p x
N
→∞
=
 
where n is the number of occurrences of x and N is total number of trials. For 
example, if a die is rolled many times then the relative frequency of the event 
“six” will settle down to a value of approximately 1/6.    
In the axiomatic approach, the concept of probability is axiomatized as 
follows: 
• 
( )
0
p x ≥
, where x is an arbitrary event 
• 
( )
1
p S = , where S is a certain event (i.e. the whole event space) 
• 
(
)
( )
( )
p x
y
p x
p y
∪
=
+
, where x and y are mutually exclusive events. 
Note that while the axiomatic approach merely provides guidance on how to 
assign values to probabilities, the classical and relative frequency approaches 
specify what values to assign. 
A subjective probability describes an individual’s personal judgment about 
how likely a particular event is to occur. It is not based on any precise 
computation, but is an assessment by a subject matter expert based on his or her 
experience (that is, it’s a “guesstimate”).  

 
Mathematical Preliminaries 
37 
 
Now we turn to formally defining random variables and probability 
distributions, the concepts central to the development of probabilistic models for 
decision-making. A random variable is a function defined over an event space 
(that is, the domain of a random variable consists of random events from the 
sample space) and its value is determined by the outcome of an event. A discrete 
random variable  is a random variable whose range is finite or denumerable. The 
elements in the range (i.e. possible values) of a random variable are called its 
states.  
Example 
Consider the process of rolling a pair of dice, whose sample space is 
{(1,1),(1,2),...,(6,6)}. Consider the random variable Dice defined over this 
sample space, where its values are determined by the sum of the upturned faces, 
that is, 
( , )
Dice i j
i
j
= +
, for each sample point ( , )
i j . For example, 
(2,3)
Dice
 is 
equal to 5. Therefore, Dice is discrete, with a range of {2,3,4,...,12}. Consider 
another random variable Weather defined over the sample space of the morning 
weather conditions in a particular month, where the current weather determines 
its value on a particular morning. The possible values of the discrete random 
variable 
Weather 
might 
be 
{
,
,
}
sunny rain snow . 
The 
domain 
{ :
[0
,100
]}
o
o
t t
C
C
∈
of the random variable Temperature is continuous, and the 
range could be kept the same as the domain. If the range is considered as, for 
example, {
,
,
,
,
}
hot warm normal cold freezing  then it becomes a discrete random 
variable. 
 
The probability distribution of a random variable is a function whose domain 
is the range of the random variable, and whose range is a set of values associated 
with the probabilities of the elements of the domain. The probability distribution 
of a discrete random variable is called a discrete probability distribution. The 
probability distribution of a continuous random variable is called a continuous 
probability distribution. In this book, we are mainly concerned about discrete 
probability distributions. 
The probability distribution of a discrete random variable is represented by 
its probability mass function   (or probability density function or pdf). A 
probability density function f of a random variable X with states 
1
{ ,...,
}
n
x
x
 is 
defined as follows: 
( )
i
f x
 is the probability that X will assume the value 
ix . 

38 
Decision-Making Agents 
 
 
Examples 
Consider the random variable Dice defined as the sum of the upturned faces of 
two 
dice, 
and 
therefore 
having 
the 
range 
{2,3,4,...,12}. 
Now, 
{( , )
{(1,1),(1,2),...,(6,6)}:
5}
i j
i
j
∈
+
=
 is equal to {(1,4),(2,3),(3,2),(4,1)}. 
Therefore, 
(
5)
4/36.
p Dice =
=
 
Similarly, 
we 
have 
the 
following: 
(
2)
1/36
p Dice =
=
, 
(
3)
2/36
p Dice =
=
, 
(
4)
3/36
p Dice =
=
, 
(
5)
4/36
p Dice =
=
, 
(
6)
5/36
p Dice =
=
, 
(
7)
6/36
p Dice =
=
, 
(
8)
5/36
p Dice =
=
, 
(
9)
4/36
p Dice =
=
, 
(
10)
3/36
p Dice =
=
, 
(
11)
2/36
p Dice =
=
, 
(
12)
1/36
p Dice =
=
. Also, the sum of all the probabilities 
is equal to 1, that is, 
{2,3,...,12}
(
)
1
x
p Dice
x
∈
=
=
∑
. 
Consider the random variable Weather with range {
,
,
}
sunny rain snow . 
Define 
(
)
0.55
p Weather
sunny
=
=
, 
(
)
0.15
p Weather
rain
=
=
, 
and 
(
)
0.30
p Weather
snow
=
=
. Figure 2-5 represents the graphs of the two 
probability density functions associated with the random variables Dice and 
Weather. Such a graphical depiction of a probability mass function is called a 
probability histogram.  
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
p(n)
2
3
4
5
6
7
8
9
10 11 12
n
0.00
0.10
0.20
0.30
0.40
0.50
0.60
p(w)
Sunny
Rainy
Snowy
w
 
Figure 2-5: Probability density functions for the random variables Dice and 
Weather  
 
The joint probability distribution of two discrete random variables X and Y, 
denoted as 
(
, )
p X Y  or 
(
)
p XY , is a function whose domain is the set of ordered 
pairs ( , )
x y  of events, where x and y are possible values for X and Y, respectively, 
and whose range is the set of probability values corresponding to the ordered 

 
Mathematical Preliminaries 
39 
 
pairs in its domain. Such a probability is denoted by 
(
,
)
p X
x Y
y
=
=
 (or simply 
( , )
p x y  when X and Y are clear from the context) and is defined as 
( , )
(
,
)
(
&
)
p x y
p X
x Y
y
p X
x
Y
y
=
=
=
=
=
=
 
The definition of the joint probability distribution can be extended to three or 
more random variables. In general, the joint probability distribution of the set of 
discrete random variables 
1,...,
n
X
X , denoted as 
1
(
,...,
)
n
p X
X
 or 
1
(
...
)
n
p X
X
, is 
given by 
1
1
1
1
1
( ,...,
)
(
,...,
)
(
&...&
)
n
n
n
n
n
p x
x
p X
x
X
x
p X
x
X
x
=
=
=
=
=
=
 
The notion of conditional probability distribution arises when you want to 
know the probability of an event, given the occurrence of another event. For 
example, the probability of snowy weather later today given that the current 
temperature is freezing. Formally, the conditional probability distribution of the 
two random variables X and Y, denoted as 
(
|
)
p X Y , is a function whose domain 
is the set of ordered pairs ( , )
x y , where x and y are possible values for X and Y, 
respectively, and is a function whose range is the set of probability values 
corresponding to the ordered pairs. The conditional probability distribution is 
defined as follows:  
(
)
(
|
)
( )
p XY
p X Y
p Y
=
, if 
( )
0
p Y >
 
Following are some important results for conditional probabilities that follow 
from this definition: 
Multiplication Rule 
0
1
0
1
0
2
0
1
0
1
1
(
,
,...,
)
(
) (
|
) (
|
)... (
|
...
)
n
n
n
p X
X
X
p X
p X
X
p X
X X
p X
X X
X −
=
, 
if 
0
1
(
,
,...,
)
0
n
p X
X
X
>
 
Total Probability Rule 
1
(
)
(
|
) ( )
n
i
i
i
p X
p X Y p Y
=
=∑
, given
( )
0
i
p Y >
, for every i, 
and given
1
( )
1
n
i
i
p Y
=
=
∑
 
Special Case: 
(
)
(
|
) ( )
(
|
) ( )
p X
p X Y p Y
p X Y p Y
=
+
,  if 0
( )
1
p Y
<
<  

40 
Decision-Making Agents 
 
 
Marginalization Rule 
1
(
)
(
,
)
n
i
i
p X
p X Y
=
=∑
,  given 
( )
0
i
p Y >
, for every i, and given
1
( )
1
n
i
i
p Y
=
=
∑
 
Special Case: 
(
)
(
, )
(
, )
p X
p X Y
p X Y
=
+
,  if 0
( )
1
p Y
<
<  
Bayes’ Rule  
1
(
|
) (
)
(
|
)
(
|
) ( )
j
j
j
n
i
i
i
p X Y
p Y
p Y
X
p X Y p Y
=
=
∑
, if 
(
)
0
p X >
 and
( )
0
i
p Y >
, for every i, and 
1
( )
1
n
i
i
p Y
=
=
∑
 
Special Case: 
(
|
) (
)
(
|
)
( )
p Y X p X
p X Y
p Y
=
, if 
(
)
0
p X >
and 
( )
0
p Y >
 
2.5 
Algorithmic Complexity 
An algorithm is a program that is guaranteed to give a correct answer to a 
problem within a certain time. We say an algorithm runs in polynomial time 
when there is a polynomial 
2
0
1
2
( )
...
k
k
p x
a
a x
a x
a x
=
+
+
+
+
 
such that the time taken to run the algorithm is less than or equal to 
( )
p x , where 
x is the “input length”  (essentially, the amount of data needed to describe an 
instance of the problem), and 
0
1
2
,
,
,...,
k
a a a
a  are non-negative integers. Formally, 
if an algorithm runs in polynomial time, then we say that the algorithm 
complexity (or simply, the algorithm) is “of order 
kx ” or 
(
)
k
O x
, where k is the 
highest power in 
( )
p x  above. This theory of computational complexity started 
with Cook’s paper (Cook, 1971), and its close relationship to combinatorial 
optimization can be found in (Karp, 1972). 
A problem for which an algorithm of order 
kx  exists is said to be in the 
polynomial class or P class. Unfortunately, time-complexity functions are not 
always bounded this way; for example, some algorithms belong to the 
exponential class or EXP class. These algorithms have “exploding” time 
functions which contain exponential factors, like 2n  or !
n  (where n is again the 
input length of the problem). They grow extremely quickly, much faster than any 

 
Mathematical Preliminaries 
41 
 
polynomial function. A problem with time complexity bounded by a polynomial 
function is considered tractable; otherwise, it is intractable.  
A decision problem (or recognition problem) is one that takes the form of a 
question with a “yes” or “no” answer. Consider, for example, the Traveling 
Salesman Problem (TSP),  Problem which decides if there is some tour or circuit 
in a complete weighted graph which visits every node exactly once, with total 
path weight less than some given value. This differs from the corresponding 
optimization problem of finding the shortest tour of all the cities, which requires 
exponential running time. We say that a decision problem belongs to the NP 
(nondeterministic polynomial) complexity class if every “yes” instance has a 
certificate whose validity can be checked in polynomial time. For example, if the 
TSP decision problem has the answer “yes” then one certificate is a list of the 
orders in which the nodes should be visited. It takes only time 
( )
O x  to add up 
the length of this tour and check it is less than the given value. By reversing the 
roles played by “yes” and “no” we obtain a complexity class known as Co-NP. In 
particular, for every decision problem in NP there is an associated decision 
problem in Co-NP obtained by framing the NP question in the negative, for 
example, the question “Do all traveling salesman tours have length greater than 
certain value?” For Co-NP problems, every “no” instance has a certificate whose 
validity can be checked in polynomial time. 
The complexity class NP-complete is the set of decision problems that are 
the hardest problems in NP in the sense that they are the ones most likely not to 
be in P. Formally, A problem is NP-complete if  
• 
It is in NP 
• 
Every NP problem can be reduced to it in polynomial time 
An optimization problem for which the related decision problem is NP-complete 
is termed NP-hard. Such a problem is at least as hard or harder than any problem 
in NP. Thus an NP-hard problem is any problem such that every NP problem can 
be converted to it (reduced to it) in polynomial time. Therefore, if we are given 
an algorithm that solves an NP-hard problem, then the algorithm can also be used 
to solve any problem in NP with no more than 
(
)
k
O x
 extra time. The class of 
NP-complete problems is the intersection of the classes of NP-hard and NP 
problems. The running time of a P problem on a deterministic Turing machine 
has a polynomial-length input, and NP problems are the polynomial-time 
problems on nondeterministic Turing machines. 

42 
Decision-Making Agents 
 
 
P
NP
EXP
Graph Isomorphism
Sorting
Binary Search
Minimal Spanning Tree
Shortest Path
Matrix Multiplication
PSPACE
QBF
Chess
NP-Complete
NP-Hard
Propositional Satisfiability
Traveling Salesman
Hamiltonian Circuit
Halting Problem
Optimization Problems
co-NP
Co SAT
 
Figure 2-6: Relationships among complexity classes 
The set of all P problems is contained in the set of all NP problems, which, in 
turn, is contained in the set of all EXP problems. Some examples of P problems 
are sorting, binary search, minimal spanning tree, shortest path, and matrix 
multiplication. The most well-known NP-complete problem is the propositional 
satisfiability (SAT) problem which determines whether a given propositional 
formula is satisfiable (this will be discussed in detail in the next chapter on 
classical logic). SAT has 2N  possible solutions if there are N propositional 
variables, and so has exponential time complexity (assuming we must check 
them all to determine if there is a solution). But a possible solution is easy to 
check in polynomial time, and therefore it is an NP-complete problem. Another 
NP-complete problem is the Hamiltonian Circuit Problem which finds a circuit in 
a graph that passes through each vertex exactly once. The most famous 
outstanding question in complexity theory is whether P = NP, that is, whether the 
NP problems actually do have polynomial-time solutions that haven’t yet been 

 
Mathematical Preliminaries 
43 
 
discovered. Although there is strong suspicion that this is not the case, no one has 
been able to prove it. 
As mentioned previously, decision problems are associated with optimization 
problems. For example, for the TSP the associated optimization question is 
“What is the length of the shortest tour?” If an optimization problem asks for a 
certain type of structure with the minimum “cost” among such structures, we can 
associate with that problem a decision problem that includes a numerical bound 
B as an additional parameter and that asks whether there exists a structure of the 
required type having a cost no more than B. The problem of finding the best 
clique tree (which will be defined in the context of junction tree algorithms in the 
chapter on belief networks) is such an optimization problem which is NP-hard. 
The graph isomorphism problem (Are two graphs isomorphic?) is suspected 
to be neither in P nor NP-complete, though it is obviously in NP. There exists no 
known P algorithm for graph isomorphism testing, although the problem has also 
not been shown to be NP-complete. The subgraph isomorphism problem (Is a 
graph isomorphic to a subgraph of another graph?) is NP-complete. The halting 
problem (Given an algorithm and an input, will the algorithm ever stop?) is a 
decision problem, but is not NP-complete. It is an NP-hard problem. 
The following types of algorithms are used in practice to deal with 
intractable problems and problems not admitting reasonably efficient algorithms: 
• 
Approximation algorithms that settle for less than optimum solutions 
• 
Heuristic algorithms that are probably efficient for most cases of the 
problem 
• 
Randomized algorithms that are probably correct in typical problem 
instances 
For undecidable problems, there is no algorithm that always solves them, no 
matter how much time or space is allowed. The halting problem is an 
undecidable problem. In general, it cannot be solved in finite time. First-order 
logic (to be discussed in the next chapter) is also undecidable in the sense that, in 
general, there is no effective procedure to determine whether a formula is a 
theorem or not. 
The space complexity of an algorithm is concerned with the amount of 
memory used but not time. The difference between space complexity and time 
complexity is that space can be reused. Space complexity is not affected by 
determinism or nondeterminism, as deterministic machines can simulate 
nondeterministic machines using a small amount of space (Savitch’s theorem). A 

44 
Decision-Making Agents 
 
 
problem is said to be in the class PSPACE if it can be solved in space polynomial 
in the size of its input. Clearly, a Turing machine that uses polynomial time also 
uses at most polynomial space. However, a Turing machine that uses polynomial 
space may use an exceedingly large amount of time before halting. Therefore, NP 
is a subset of PSPACE. A problem is said to be PSPACE-complete if it is in 
PSPACE and if every other PSPACE problem is polynomial-time reducible to it. 
It is widely believed that PSPACE-complete problems are strictly harder than the 
NP-complete problems. Proving the validity of Quantified Boolean Formulae 
(QBF) and perfect chess playing are examples of PSPACE-complete problems. 
2.6 
Further Readings 
There are plenty of well-written text books in each of the areas covered in this 
chapter. Here I provide only a few popular ones. Stoll’s book (1963) is a very 
well-written and easy to understand book on set theory (and logic). The graph 
theory book by Deo (1974) is one of the first comprehensive texts on graph 
theory with applications to computer science and engineering. Wilson’s book 
(1996) is another good introductory book on graph theory. Two good books on 
probability are (Feller, 1968) and (Chung, 2000). Finally, (Papadimitriou, 1993) 
is a good text book on complexity theory. 

45 
Chapter 3 
 
Classical Logics for the 
Propositional Epistemic Model 
 
 
 
This chapter presents the classical logics to help provide a foundation for 
building decision-making agents based on the propositional epistemic model. We 
start with classical propositional logic and then move directly to full first-order 
logic. The model theoretic semantics of these logics are developed, and the 
soundness and completeness theorems are established. We introduce resolution 
theorem proving as a way to efficiently carry out deductions within the classical 
logics. A subsequent chapter then introduces special subsets of the classical 
logics that constitute the basis of logic programming, a declarative (as opposed to 
procedural) programming paradigm. Later, we present the logic programming 
language Prolog, which can be effectively used to build the complex reasoning 
processes involved when implementing the logical and modal epistemic models, 
via “meta-interpreters.” 
The presentation style in this chapter is rather informal, and is directed 
towards providing an agent with a practical and sound approach to reasoning 
with the propositional epistemic model. In many places supporting propositions 
and theorems have been stated without proofs based on rather subjective 
selection criteria to enhance reading flow and make room for more relevant 
material. In some cases, proofs follow immediately from results preceding them. 
For the rest, readers can consult standard textbooks on logic. Plenty of examples 
have been given throughout the chapter. Finally, please note that the relevance of 
all this foundational material to building decision-making agents was touched on 
in the introduction and will become clearer in subsequent chapters. Overall, the 
soundness and completeness results of this chapter validate resolution theorem 
proving, which is the fundamental technique for reasoning with logic-based agent 
epistemic models. 

46 
Decision-Making Agents 
 
 
3.1 
Propositional Logic 
A proposition is a declarative sentence that is either true or false, but not both.  
Examples of propositions are: 
Field is wet 
It is raining at the field 
Propositions will be symbolized as, for example, 
,
, ,...
P Q R
, and are called atoms 
or atomic formulae. Compound propositions are formed by modifying with the 
word “not” or by connecting sentences via the words/phrases “and,” “or,” “if ... 
then,” and “if and only if.” These five words/phrases are called logical 
connectives and are usually symbolized as shown in Table 3-1. 
Connective 
Symbol 
not 
¬ 
and 
∧ 
or 
∨ 
if … then 
→ 
if and only if 
↔ 
Table 3-1: Standard logical connectives 
Example 
Examples of compound propositions are as follows: 
If sprinkler is on then the field is wet 
Sprinkler is not on 
It is raining or sprinkler is on 
The above composite propositions are symbolized respectively in the 
propositional language as: 
P
Q
P
R
P
→
¬
∨
 
where: 

 
Classical Logics 
47 
 
P stands for “Sprinkler is on” 
Q stands for “Field is wet” 
R stands for “It is raining” 
 
Each of these compound propositions is called a well-formed formula (wff) or 
formula  as part of the propositional language, and the symbols occurring in the 
formulae are part of the propositional alphabet. Formal definitions of these terms 
are given below. 
A propositional alphabet consists of the following: 
• 
Two parentheses (  and ) 
• 
A set of propositional variables 
,
, ,...
P Q R
 as atoms 
• 
A set of logical connectives 
, , ,
,and
¬ ∧∨→
↔ 
Well-formed formulae or formulae in propositional logic are defined as follows: 
• 
An atomic formula is a formula, for example, 
,
, ,...
P Q R
, as mentioned 
previously. 
• 
If F is a formula, then (
)
F
¬
 is a formula. 
• 
If F and G are formulae, then (
)
F
G
∧
, (
)
F
G
∨
, (
)
F
G
→
, (
)
F
G
↔
 
are formulae. 
Given a propositional alphabet, the propositional language comprises the set of 
all formulae constructed from the symbols of the alphabet. An expression is a 
formula only if it can be shown to be a formula by the above three conditions. A 
formula of the form (
)
F
¬
 is called the negation of the formula F. Formulae of 
the forms (
)
F
G
∧
 and (
)
F
G
∨
 are called the conjunction and disjunction, 
respectively, of the formulae F and G. A formula of the form (
)
F
G
→
 is called 
a conditional formula; F is called the antecedent and G is called the consequent. 
A formula of the form (
)
F
G
↔
 is called a biconditional formula. We will see 
later that disjunctions are one way that uncertain knowledge can be represented 
within the propositional epistemic model of an agent. 
The following conventions are used to avoid using parentheses in a formula. 
The connective ¬ is applied to the smallest formula following it, then ∧ is to 
connect the smallest formulae surrounding it, and so on for the rest of the 
connectives ∨, → and ↔ in that order. 

48 
Decision-Making Agents 
 
 
Example 
If parentheses are restored in the formula 
P
Q
R
Q
R
P
¬
∧
→
↔
∨
∧
 
then the resulting formula would be 
((((
)
)
)
(
(
)))
P
Q
R
Q
R
P
¬
∧
→
↔
∨
∧
 
   
We are especially interested in assessing the “truth” of a formula as a 
function of the “truth” of its atoms. To do this, we first assign a truth value to 
each atom; that is, we assign “true” (denoted as F ) or “false” (denoted as ⊥) to 
the symbolized atomic propositions that occur in a formula. Then we compute 
the truth value of the formula using special rules to handle the logical 
connectives. So for every assignment of truth values F  or ⊥ to the symbolized 
atomic propositions that occur in a formula, there corresponds a truth value for 
the formula. This can be determined using the truth table of the formula. 
Example 
Table 3-2 is the combined truth table for the fundamental formulae 
P
¬
, P
Q
∧
, 
P
Q
∨
, P
Q
→
, and P
Q
↔
; it shows how to compute truth values for a formula 
containing these logical connectives. Using this basic truth table, the truth table 
for any formula can then be constructed; for example, the truth table for 
(
)
(
(
))
P
Q
P
Q
R
¬
↔
→
∧
∨
 is displayed in Table 3-3. 
P
Q
P
P
Q
P
Q
P
Q
P
Q
¬
∧
∨
→
↔
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
F
F
F
F
F
F
F
F
F
F
F
F
F
F
F
 
Table 3-2: Truth table in propositional logic 

 
Classical Logics 
49 
 
(
)
(
)
(
(
))
P
Q
R
P
P
Q
Q
R
P
Q
R
P
Q
P
Q
R
¬
¬
↔
∨
∧
∨
¬
↔
→
∧
∨
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
F
F
F
F
F
F
F
F
F
F
F
F
F
F
F
F
F
F
F
F
F
F
F
F
F
F
F
F
F
F
F
F
F
F
 
Table 3-3: Truth table for (
)
(
(
))
P
Q
P
Q
R
¬
↔
→
∧
∨
 
 
Given a formula F, suppose 
1
2
,
,...,
n
P P
P  are all atomic formulae occurring in 
F. Then an interpretation of F is an assignment of truth values to 
1
2
,
,...,
n
P P
P , 
where no 
iP  is assigned both F  and ⊥. Hence every row in a truth table for a 
formula F is an interpretation of F. 
A formula F is a tautology or is valid (denoted as 
F
B
)  if its value is F  
under all possible interpretations of F. 
Example 
The formula 
(
)
P
P
Q
→
∨
 is a tautology according to Table 3-4. 
 
(
)
P
Q
P
Q
P
P
Q
∨
→
∨
⊥
⊥
⊥
⊥
⊥
F
F
F
F
F
F
F
F
F
F
F
 
Table 3-4: A tautology 
 

50 
Decision-Making Agents 
 
 
A formula F is false (or inconsistent or a contradiction) if and only if its 
value is ⊥ under all possible interpretations. 
Example 
The formula P
Q
P
Q
∨
↔¬
∧¬
 is false by Table 3-5. 
P
Q
P
Q
P
Q
P
Q
P
Q
P
Q
¬
¬
∨
¬
∧¬
∨
↔¬
∧¬
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
F
F
F
F
F
F
F
F
F
F
F
F
 
Table 3-5: A contradiction 
 
If a formula F is true under an interpretation I, then I satisfies F or F is 
satisfied by I and in such cases, I is a model of F.  
Two formulae F and G are said to be equivalent (or F is equivalent to G), 
denoted as F
G
≡
, if and only if the truth values of F and G are the same under 
every interpretation. In other words if F
G
↔
 is a tautology then F and G are 
equivalent. 
Examples 
(
)
(
)
(
)
F
G
F
G
F
G
F
G
F
G
F
G
G
F
∨
≡¬
→
∧
≡¬
→¬
↔
≡
→
∧
→
 
 
In view of the above equivalences, it can be seen that a formula F can be 
transformed to an equivalent formula G ( F
G
≡
) containing only the connectives 
¬ and →. In addition, the following two equivalences show the associative 
properties of the two connectives ∧ and ∨: 
(
)
(
)
(
)
(
)
F
G
H
F
G
H
F
G
H
F
G
H
∧
∧
≡
∧
∧
∨
∨
≡
∨
∨
 

 
Classical Logics 
51 
 
Hence the positions of parentheses or their order are immaterial between 
subformulae 
1
2
,
,...,
n
F F
F  in a formula 
1
2
...
n
F
F
F
∨
∨
∨
 or 
1
2
...
n
F
F
F
∧
∧
∧
. 
A formula of the form 
1
2
...
n
F
F
F
∨
∨
∨
 is called a disjunction of 
1
2
,
,...,
n
F F
F  
and a formula of the form 
1
2
...
n
F
F
F
∧
∧
∧
 is called a conjunction of 
1
2
,
,...,
n
F F
F . 
A literal is either an atom or the negation of an atom. The complement of a 
literal L, denoted as 
cL , is defined as follows: if L is an atom then 
cL  is the 
negation of the atom, but if L is the negation of an atom then 
cL  is the atom 
itself. A formula F is in conjunctive normal form (or CNF) if F has the form 
1
2
...
n
F
F
F
∧
∧
∧
 and each 
iF  is a disjunction of literals.  A formula F is in 
disjunctive normal form (or DNF) if F has the form 
1
2
...
n
F
F
F
∨
∨
∨
 and each 
iF  
is a conjunction of literals. 
Example 
The 
formula 
(
)
(
)
(
)
P
Q
Q
R
P
R
S
∨
∧
∨¬
∧
∨
∨
 
is 
in 
cnf, 
and 
(
)
(
)
P
Q
R
Q
S
∧
∧
∨
∧¬
 is in dnf. 
 
The following laws can easily be verified (by constructing their truth tables): 
(
)
(
)
(
)
(Distributive laws)
(
)
(
)
(
) 
F
G
H
F
G
F
H
F
G
H
F
G
F
H
∨
∧
≡
∨
∧
∨
⎫
⎬
∧
∨
≡
∧
∨
∧
⎭
 
(
)
                     
 (
 
'  laws) 
(
)
 
F
G
F
G
De Morgan s
F
G
F
G
¬
∨
≡¬
∧¬
⎫
⎬
¬
∧
≡¬
∨¬
⎭
 
Using the above laws and the negation elimination 
(
)
F
F
¬ ¬
≡
, any formula can 
be transformed to its equivalent disjunctive or conjunctive normal form and the 
procedure is informally described in the following. Its counterpart for first-order 
logic will be described more formally later. 
Step 1:  Eliminate → and ↔ using the equivalences F
G
F
G
→
≡¬
∨
 and 
(
)
(
)
F
G
F
G
G
F
↔
≡¬
∨
∧¬
∨
. 
Step 2: Repeatedly use De Morgan’s laws and the equivalence 
(
)
F
G
F
G
¬
∨
≡¬
∧¬
, 
(
)
F
G
F
G
¬
∧
≡¬
∨¬
, and 
(
)
F
F
¬ ¬
≡
 to bring 
the negation sign immediately before atoms. 

52 
Decision-Making Agents 
 
 
Step 3:  Repeatedly use the two distributive laws to obtain the required 
normal form. 
The following example illustrates the above algorithm. 
Example 
(
)
(
(
))
P
Q
P
Q
R
¬
↔
→
∧
∨
  
 
 
Given formula 
(
)
(
)
(
(
))
P
Q
Q
P
P
Q
R
¬¬
∨
∧¬
∨¬
→
∧
∨
   
By Step 1 
((
)
(
))
(
(
))
P
Q
Q
P
P
Q
R
¬ ¬¬
∨
∧¬
∨¬
∨
∧
∨
  
By Step 1 
(
)
(
)
(
(
))
P
Q
Q
P
P
Q
R
¬
∨
∨¬ ¬
∨¬
∨
∧
∨
  
 
By Step 2 
(
)
(
)
(
(
))
P
Q
Q
P
P
Q
R
¬
∧¬
∨
∧
∨
∧
∨
  
 
By Step 2 
(
)
(
)
(
)
(
)
P
Q
Q
P
P
Q
P
R
¬
∧¬
∨
∧
∨
∧
∨
∧
   
By Step 3 
This formula is in disjunctive normal form. 
 
A formula G is said to be a logical consequence of a set of formulae 
1
2
,
,...,
n
F F
F , denoted as 
1
2
,
,...,
n
F F
F
G
B
, if and only if for any interpretation I in 
which 
1
2
...
n
F
F
F
∧
∧
∧
 is true G is also true. 
Example 
The formula P
R
∨
 is a logical consequence of P
Q
∨
 and Q
R
→
. 
 
This completes our brief introduction to propositional logic. Now we give an 
example to demonstrate how propositional logic establishes the validity of an 
argument. 
Example 
Consider the following valid argument: 
If it is raining at the field then the field is wet. 
The field is not wet. 
Therefore, it is not raining at the field. 

 
Classical Logics 
53 
 
A symbolization of the two premises of the above argument in propositional 
logic is P
Q
→
 and 
Q
¬
, where P stands for “It is raining at the field” and Q 
stands for “The field is wet.” To prove the validity of the argument, it is 
necessary to show that 
P
¬
 (the symbolization of the conclusion of the 
argument) is a logical consequence of P
Q
→
 and 
Q
¬
. Table 3-6 establishes 
,
P
Q
Q
P
→
¬
¬
B
. 
P
Q
P
Q
Q
P
→
¬
¬
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
⊥
F
F
F
F
F
F
F
F
F
F
F
 
Table 3-6: Truth table to establish 
,
P
Q
Q
P
→
¬
¬
B
 
 
An alternative approach to truth tables for dealing with more complex 
problems concerning logical connectives is that of formal axiomatic theories. 
Such a theory T consists of the following: 
• 
A countable set of symbols (also called an alphabet) . An expression is a 
finite sequence of such symbols. 
• 
A set of formulae (also called the language over the alphabet), which is a 
subset of the set of all expressions of T, and an effective procedure to 
determine whether an expression is a valid formula or not. 
• 
A set of axioms which is a subset of the set of all formulae of T, and an 
effective procedure to determine whether a formula is an axiom or not. 
• 
A finite set of rules of inference, each of which is a relation among 
formulae. Given an (
1)
n +
-ary relation R as a rule of inference, and 
given an arbitrary set of formulae 
1
2
,
,...,
,
n
F F
F F , there is an effective 
procedure to determine whether 
1
2
,
,...,
n
F F
F  are in relation R to F (that 
is, whether 
1
2
(
,
,...,
,
)
R
n
F F
F F ∈
). If so, the formula F is called a direct 
consequence of 
1
2
,
,...,
n
F F
F  by the application of the rule of inference R. 
A proof (or axiomatic deduction) in the axiomatic theory T is a finite 
sequence 
1
2
,
,...,
n
F F
F  of formulae such that each 
iF  is either an axiom or a direct 
consequence of some of 
1
1
,...,
i
F
F − by the application of a rule of inference. A 

54 
Decision-Making Agents 
 
 
proof 
1
2
,
,...,
n
F F
F  is called the proof of 
n
F . A formula F is a theorem in the 
axiomatic theory T, denoted as 
F
AT
 (or simply 
F
A
 when the theory T is clear 
from the context) if there is a proof of F in T. 
Suppose S is a set of formulae in the axiomatic theory T.  A formula F is said 
to be a theorem of S in T, denoted as 
F
S AT
 (or again simply 
F
S A
 when the 
theory T is clear from the context), if and only if there is a sequence 
1
2
,
,...,
n
F F
F  
of formulae such that 
n
F
F
=
 and each 
iF  is either in S or a direct consequence 
of some of 
1
2
1
,
,...,
i
F F
F − by the application of a rule of inference. Such a 
sequence is called a proof of F from S. The members of S are called hypotheses 
or premises.  If S is 
1
2
{
,
,...,
}
n
F F
F
 then 
1
2
{
,
,...,
}
n
F F
F
F
A
 would simply be 
written as 
1
2
,
,...,
n
F F
F
F
A
. If S is empty then F is a theorem, that is 
F
A
. 
An axiomatic theory T is decidable if there exists an effective procedure to 
determine whether an arbitrary formula is a theorem or not. If there is no such 
procedure, then T is called undecidable. 
3.1.1 
Axiomatic Theory for Propositional Logic 
A formal axiomatic theory PL for the propositional logic is defined as follows: 
• 
The symbols of PL are a propositional alphabet: 
− 
Parentheses  (, )  
− 
Logical connectives  
,
¬ →   (elimination of the other connectives 
from the alphabet is deliberate as they can be defined in terms of 
these two, and hence are considered as abbreviations rather than part 
of the alphabet). 
− 
Propositional variables 
1
1
1
,
, ,
,
,
,...
P Q R P Q R
 
• 
Formulae of PL (the propositional language over the propositional 
alphabet) are inductively defined as follows: 
− 
All propositional variables are formulae. 
− 
If F is a formula then (
)
F
¬
 is a formula. 
− 
If F and G are formulae then (
)
F
G
→
 is a formula. 
− 
An expression is a formula if and only if it can be shown to be a 
formula on the basis of the above conditions. 

 
Classical Logics 
55 
 
For a particular theory, only those symbols that occur in the theory are used to 
construct formulae. The normal convention is adopted concerning the omission 
of parentheses in a formula. 
• 
If F, G, H are any formulae of PL, then the following are axioms of PL: 
− 
A1:  
(
)
F
G
F
→
→
 
− 
A2:  (
(
))
((
)
(
))
F
G
H
F
G
F
H
→
→
→
→
→
→
 
− 
A3:  (
)
(
)
G
F
F
G
¬
→¬
→
→
 
• 
The rule of inference of PL is as follows: 
− 
Modus Ponens (MP) : G is a direct consequence of F and F
G
→
 
The study of axiomatic theory for a propositional language is completed by 
introducing the other logic connectives ∧, ∨ and ↔ through the following 
syntactic equivalents: 
(
)
F
G
F
G
∧
≡¬
→¬
 
F
G
F
G
∨
≡¬
→
  
(
)
(
)
F
G
F
G
G
F
↔
≡
→
∧
→
 
3.1.2 
Soundness and Completeness Theorem 
The soundness and completeness theorem for propositional logic provides 
equivalence between the two approaches, namely the truth-table approach and 
the formal axiomatic approach. These two approaches are also called the model-
theoretic approach and the proof-theoretic approach. This section states the 
theorem along with other important results in propositional logic. Their proofs 
have not been provided here because more generalized versions exist in first-
order logic and they will be established in a subsequent section. 
Proposition 3-1: If F and G are formulae in propositional logic, then 
F
B
 and 
F
G
→
B
 implies 
G
B
. 
Theorem 3-1: (Deduction Theorem) If Γ  is a set of formulae and F and G are 
formulae in propositional logic, then ,F
G
Γ
A
 implies 
F
G
Γ
→
A
. 

56 
Decision-Making Agents 
 
 
Theorem 3-2: (Soundness and Completeness Theorem) A formula F in 
propositional logic is a theorem if and only if it is a tautology, that is, 
F
A
 if 
and only if 
F
B
. 
Theorem 3-3: (Strong Soundness and Completeness Theorem) If 
1
2
,
,...,
n
F F
F  
and F are formulae in propositional logic then 
1
2
,
,...,
n
F F
F
F
A
 if and only if 
1
2
,
,...,
n
F F
F
F
B
.  
Theorem 3-4: (Consistency Theorem) The theory PL is consistent. In other 
words, for any formula F, 
F
A
 and 
F
¬
A
 cannot hold simulataneously. 
Proposition 3-2: 
F
F
↔¬¬
A
, for any formula F. 
Proposition 3-3: 
1
2
1
2
F
F
G
G
↔
↔
A
, where 
2
G  is obtained from 
1
G  by 
simultaneously replacing each occurrence of 
1F  in 
1
G  by
2
F . 
Example 
In contrast to the justification of the valid argument 
,
P
Q
Q
P
→
¬
¬
A
 by the 
truth-table method, the following is an axiomatic deduction of the conclusion of 
the argument: 
Step 1: P
Q
→
 
 
 
 
 
Hypothesis 
Step 2: 
Q
¬
  
 
 
 
 
Hypothesis 
Step 3: (
)
(
)
P
Q
Q
P
¬¬
→¬¬
→¬
→¬
 
 
Axiom A3 
Step 4: (
)
(
)
P
Q
Q
P
→
→¬
→¬
 
 
 
 
From Step 3, Proposition 3-2, and Proposition 3-3 
Step 5:  
Q
P
¬
→¬
  
 
Modus Ponens on Steps 1 and 4 
Step 6:  
P
¬
 
 
 
Modus Ponens on Steps 2 and 5 
 

 
Classical Logics 
57 
 
3.2 
First-Order Logic 
There are various kinds of arguments that cannot be conveniently stated in the 
language of propositional logic. Consider the following argument as an example: 
It is raining at the field Eden Garden 
Rain makes every field wet 
Therefore, the field Eden Garden is wet 
The above argument is a valid argument. However, if the three premises are 
symbolized as P, Q and R respectively, it is not possible to prove R from P and Q 
within the framework of propositional logic, unless the second premise “Rain 
makes every field wet” is instantiated for the specific field Eden Garden. The 
correctness of the above argument relies upon the meaning of the expression 
“every,” which has not been considered in the propositional logic. However, 
first-order logic handles this kind of argument, and also extends the propositional 
logic by incorporating more logical notations, such as terms, predicates and 
quantifiers. The set of symbols (the first-order alphabet)  in the case of first-
order logic is defined as follows: 
• 
Delimiter: 
, (comma) 
• 
Parentheses: 
(, ) 
• 
Primitive connectives: ¬ (negation), → (implication) 
• 
Universal quantifier: 
∀(for all)  
• 
Individual variables: 
1
1
1
, , ,
,
,
,...
x y z x y z
 
• 
Individual constants: 
1
1
1
, , ,
,
,
,...
a b c a b c
 
• 
For each natural number n, n-ary predicate symbols:
1
1
1
,
, ,
,
,
,...
P Q R P Q R
 
1. For each natural number n, n-ary function symbols:
1
1
1
, , ,
,
,
,...
f g h f g h
 
Terms are expressions, which are defined recursively as follows: 
• 
A variable or an individual constant is a term. 
• 
If f is an n-ary function symbol and 
1
2
, ,..., n
t t
t  are terms then 
1
2
( , ,..., )
n
f t t
t
 is a term. 
• 
An expression is a term if it can be shown to be so only on the basis of 
the above two conditions. 
A predicate is a function which evaluates to either true or false, or a statement 
about a relation that may be true or false. If P is an n-ary predicate symbol and 

58 
Decision-Making Agents 
 
 
1
2
, ,..., n
t t
t  are terms, then 
1
2
( , ,..., )
n
P t t
t
 is an atomic formula   (or atom or 
positive literal).  A negative literal is a formula of the form 
A
¬ , where A is an 
atom. A literal is either positive or negative. Based on these primitive notions, 
the well-formed formulae (wffs) or formulae of first-order logic are recursively 
defined as follows: 
• 
Every atomic formula is a formula. 
• 
If F is a formula then 
F
¬
 is a formula. 
• 
If F is a formula and x is a variable then 
( )
x F
∀
 is a formula. 
• 
If F and G are formulae then F
G
→
 is a formula. 
• 
An expression is a formula only if it can be generated by the above four 
conditions. 
For convenience and improved readability of formulae, the other logical 
connectives, ∧, ∨, and ↔, are also introduced and defined in terms of ¬ and 
→ just as in the case of propositional logic. Additionally, an existential 
quantifier, denoted as ∃, is introduced and defined as follows: 
( )
(
(
))
x F
x
F
∃
≡¬ ∀¬
 
In the formulae 
( )
x F
∃
 and 
( )
x G
∀
, F and G are called the scope of the 
quantifiers x
∃ and 
x
∀ respectively. As in the case of propositional calculus, the 
same convention is made about the omission of parentheses in a formula. A 
formula in propositional logic can be considered as a formula in first-order logic 
(where the atoms are 0-ary predicates, and there are no variables, functions, or 
quantifiers). Hence all the results established so far in connection with 
propositional logic are also applicable to the set of all quantifier and variable-free 
formulae in first-order logic. Each ground atomic formula (no occurrence of 
variables) occurring in this set is considered as a propositional symbol. 
Given a first-order alphabet, the first-order language L comprises the set of 
all formulae constructed from the symbols of the alphabet. Using the first-order 
language, a symbolization of the first two premises of the argument presented in 
the beginning of this subsection is as follows: 
(
)
(
( )
( ,
))
(
,
)
Rain EdenGarden
x Rain x
Field x Wet
Field EdenGarden Wet
∀
→
 
where Rain and Field are unary and binary predicate symbols respectively, and 
Eden Garden and Wet are constants. 

 
Classical Logics 
59 
 
An occurrence of a variable x in a formula F is bound (or x is bound in F) if x 
lies within the scope of a quantifier 
x
∀ in F. If the occurrence of x in F is not 
bound, its occurrence is free in F (or x is free in F). A variable occurrence in a 
formula may be both free and bound. This case may be avoided by 
simultaneously renaming the variables in the quantifier and its associated bound 
occurrences by new variables. A formula without any free variable is called a 
closed formula  (or a sentence or statement). If 
1,...,
n
x
x  are all free variables of 
F, then formula
1...
n
x
x F
∀
∀
 is called the closure of F and is abbreviated as 
F
∀
. 
Example 
Suppose 
( , )
F
xP x y
= ∀
 and 
( ( , )
( ))
G
x P x y
yQ y
= ∀
→∀
. The variable x is 
bound in F and G. The variable y is free in F and both free (the first occurrence) 
and bound (the second occurrence) in G. 
Use of the notation 
1
2
[ ,
,...,
]
n
F x x
x
 emphasizes that 
1
2
,
,...,
n
x x
x  are some or 
all of the free variables of F. The notation 
1
1
2
2
[
/ ,
/
,...,
/
]
n
n
F x
t x
t
x
t
 denotes 
substitution of the terms 
1
2
, ,..., n
t t
t  for all free occurrences of 
1
2
,
,...,
n
x x
x  
respectively in F. 
A term t is said to be free for a variable x in a formula F if no free 
occurrence of x lies within the scope of any quantifier 
y
∀, where y is a variable 
occurring in t (which could become problematic if we perform the substitution 
[ / ]
F x y , as x would become y and so become bound). 
Example 
Consider the formula 
( ( , ))
( )
x P x y
Q z
∀
→
 and the term 
( , )
f a x . The term is free 
for z in the formula but not free for y in the same formula. 
 
An interpretation I of a first-order language L consists of the following: 
• 
A non-empty set D, called the domain of interpretation of L. 
• 
An assignment to each n-ary predicate symbol of an n-ary relation in D. 
• 
An assignment to each n-ary function symbol of an n-ary function with 
domain 
n
D  and co-domain D. 
• 
An assignment to each individual constant of a fixed element of D. 

60 
Decision-Making Agents 
 
 
In a given interpretation I of L, the logical connectives and quantifiers are 
given their usual meanings, and the variables are thought of as ranging over D. If 
t is a closed term (or function or predicate symbol), then 
( )t
I
 denotes the 
corresponding assignment by I. If t is a closed term, and has the 
form
1
2
( , ,..., )
n
f t t
t
 (where the ti are all terms), then the corresponding assignment 
by I is 
1
2
( )( ( ), ( ),..., ( ))
n
f
t
t
t
I
I
I
I
 (if ti is a variable, then ( )
i
i
t
t
=
I
and ranges over 
D). 
Suppose I is an interpretation with domain D and t ∈D. Then 
(
/ )
ix
t
I
 is an 
interpretation that is exactly the same as I except that the i-th variable 
ix  always 
takes the value t rather than ranging over the whole domain. 
Let I be an interpretation with domain D. Let Σ  be the set of all sequences 
of elements of D. For a given sequence 
1
2
,
,...,
n
s s
s
= 〈
〉∈Σ
S
 and for a term t 
consider the following term assignment of t with respect to I and S, denoted as 
*( )t
S
: 
• 
If t is a variable 
jx  then its assignment is 
js . (We may assume some 
fixed indexing of the variables of L.) 
• 
If t is a constant then its assignment is according to I. 
• 
If 
1
2
, ,..., n
r r
r  are the term assignments of 1
2
, ,..., n
t t
t  respectively and 
'f  
is the assignment of the n-ary function symbol f, then 
1
2
'( ,
,...,
)
n
f
r r
r ∈D 
is the term assignment of 
1
2
( , ,..., )
n
f t t
t
. 
The definition of satisfaction of a formula with respect to a sequence and an 
interpretation can be inductively defined as follows: 
• 
If F is an atomic wff 
1
2
( , ,..., )
n
P t t
t
, then the sequence 
1
2
,
,...,
n
s s
s
= 〈
〉
S
 
satisfies F if and only if 
1
2
'( ,
,...,
)
n
P r r
r
. That is, the n-tuple 
1
2
,
,..., n
r r
r
〈
〉 
is in the relation 
'
P , where 
'
P  is the corresponding n-place relation of 
the interpretation of P. 
• 
S satisfies 
F
¬
 if and only if S does not satisfy F. 
• 
S satisfies F
G
∧
 if and only if S satisfies F and S satisfies G. 
• 
S satisfies F
G
∨
 if and only if S satisfies F or S satisfies G. 
• 
S satisfies F
G
→
 if and only if either S does not satisfy F or S satisfies 
G. 

 
Classical Logics 
61 
 
• 
S satisfies F
G
↔
 if and only if S satisfies both F and G or S satisfies 
neither F nor G. 
• 
S satisfies 
( )
ix F
∃
 if and only if there is a sequence 
1S  that differs from S 
in at most the i-th component such that 
1S  satisfies F. 
• 
S satisfies 
( )
ix F
∀
 if and only if every sequence that differs from S in at 
most the i-th component satisfies F. 
A wff F is true for the interpretation (alternatively, F can be given the truth 
value F ) I (written 
F
BI
 or 
( )
F =F
I
) if and only if every sequence in Σ  
satisfies F; F is said to be false for the interpretation I if and only if no sequence 
of Σ  satisfies F. 
If a formula is not closed, then some sequences may satisfy the formula, 
while the rest of the sequences may not. Fortunately, the truth value of a closed 
formula does not depend on any particular sequence S of interpretation; in this 
case the satisfaction of the formula depends only on the interpretation I. The rest 
of this text mainly deals with such closed formulae. 
Let I be an interpretation of a first-order language L. Then I is said to be a 
model for a closed wff F if F is true with respect to I. The interpretation is said 
to be a model for a set Γ  of closed wffs of L if and only if every wff in Γ  is true 
with respect to I. Let Γ  be a set of closed wffs of a first-order language L. Then 
Γ  is satisfiable if and only if L has at least one interpretation that is a model for 
Γ . Γ  is valid if and only if every interpretation of Γ  is a model for Γ . Γ  is 
unsatisfiable if and only if no interpretation of L is a model for Γ . 
Let F be a closed wff of a first-order language L. A closed wff G is said to be 
implied by F (or, equivalently, F implies G) if and only if for every interpretation 
I of L, I is a model for F implies I is a model for G. Two closed wffs F and G 
are said to be equivalent if and only if they imply each other. Let Γ  be a set of 
closed wffs of L. A closed wff F is said to be a logical consequence of Γ  
(written 
F
Γ B
) if and only if for every interpretation I of L, I is a model for Γ  
implies that I is a model for F. Therefore, 
F
Γ B
 means the formulae in Γ  
collectively imply F. 

62 
Decision-Making Agents 
 
 
Example 
Consider the following two formulae in a first-order language L: 
( )
( ( )
( ( )))
P a
x P x
Q f x
∀
→
 
Consider an interpretation of L as follows (concentrating only on the symbols 
occurring in the above two clauses): 
• 
The domain of interpretation is the set of all natural numbers. 
• 
Assign a to 0. 
• 
Assign f to the successor function 
'f , i.e. 
'( )
1
f
n
n
=
+ . 
• 
Suppose P and Q are assigned to 
'
P  and 
'
Q  respectively under the 
interpretation. A natural number x is in relation 
'
P  if and only if x is 
even. A natural number x is in relation 
'
Q  if and only if x is odd. 
The two formulae are obviously true under the given interpretation. Note that if 
the function f were instead to be interpreted as function 
'f  where 
'( )
2
f
x
x
=
+
, 
then the second formula would have been false. 
 
3.2.1 
Axiomatic Theory for First-order Logic 
A formal axiomatic theory T for first-order logic, known as a first-order theory, 
is defined as follows: 
• 
The set of symbols of a first-order theory is a first-order alphabet. 
• 
The language of a first-order theory is a first-order language over the 
alphabet. For a particular first-order theory T, only those symbols that 
occur in T are used to construct the language of T. 
• 
The axioms of T are divided into the following two classes: 
− 
Logical axioms:  If F, G, H are formulae, then the following are 
logical axioms of the theory: 
A1: 
(
)
F
G
F
→
→
 
A2: (
(
))
((
)
(
))
F
G
H
F
G
F
H
→
→
→
→
→
→
 
A3: (
)
(
)
G
F
F
G
¬
→¬
→
→
 

 
Classical Logics 
63 
 
A4: (
)
xF
G
∀
→
, if G is a wff obtained from F by substituting all 
free occurrences of x by a term t, where t is free for x in F. 
A5: 
(
)
(
)
x F
G
F
xG
∀
→
→
→∀
, if F is a wff that contains no free 
occurrences of x. 
− 
Proper axioms:  Proper axioms vary from theory to theory. A first-
order theory in which there are no proper axioms is called a first-
order predicate logic.  
• 
The rules of inference of T are the following: 
− 
Modus ponens (MP): G follows from F and F
G
→
. 
− 
Generalization: 
( )
x F
∀
 follows from F. 
Since the alphabet of a first-order language is considered denumerable and 
formulae of the language are strings of primitive symbols, the set of all formulae 
of the language can be proved to be denumerable. 
3.2.2 
Soundness and Completeness Theorem 
Similar to the case of propositional logic, this section provides the equivalence 
between the model-theoretic and proof-theoretic approaches of first-order logic. 
To prove this equivalence, some definitions and results must be established. 
A set of formulae Γ  is inconsistent if and only if 
F
Γ A
and 
F
Γ
¬
A
, for 
some formula F. Γ  is consistent if it is not inconsistent. 
Proposition 3-4: If Γ  is inconsistent, then 
F
F
Γ
∧¬
A
, for some formula F. 
 
A set of statements M is maximally consistent if M is consistent, and for any 
statement F, if 
{ }
F
∪
M
 is consistent then F ∈M . 
Proposition 3-5: If M is a maximally consistent set then the following two 
properties hold in M: 
• 
F ∈M  if and only if 
F
M A
. 
• 
Exactly one of F and 
F
¬
 is a member of M. 

64 
Decision-Making Agents 
 
 
Proof:  
• 
Suppose F ∈M . Then obviously 
F
M A
. Conversely, assume 
F
M A
. 
This means there exists a finite subset 
1
M  of M such that 
1
F
M A
. Then 
1
{ }
F
∪
M
 is consistent. If not, there exists a finite subset 
2
M  of M and a 
formula G such that 
2,F
G
G
∧¬
M
A
. Since 
1
F
M A
, therefore 
2
1
,
G
G
∧¬
M
M A
, which contradicts the consistency of M. Thus 
1
{ }
F
∪
M
 is consistent. Since M is a maximally consistent set, F ∈M . 
• 
Both F and 
F
¬
 cannot be members of M; otherwise the consistency of 
M would be contradicted. If neither F nor 
F
¬
 is a member of M, then 
by maximality of M each of 
{ }
F
∪
M
 and 
{
}
F
∪¬
M
 is inconsistent. 
Since M is consistent and 
{ }
F
∪
M
 is inconsistent, 
F
¬
M A
. Similarly, 
F
M A
 and hence 
F
F
∧¬
M A
, which again contradicts the consistency 
of M.  
Proposition 3-6: If 
F
B
 then 
F
∀
B
, where 
F
∀
 is the closure of F. 
Proposition 3-7: Suppose 
[ ]
F x  is a formula in which x is free for y. Then the 
following hold: 
• 
[ ]
[ / ]
xF x
F x y
∀
→
B
 
• 
[ / ]
[ ]
F x y
xF x
→∃
B
 
Proposition 3-8: Suppose F is a formula without any free occurrences of the 
variable x and 
[ ]
G x  is any formula. Then the following hold: 
• 
[ ]
F
xG x
→∀
B
 if 
[ ]
F
G x
→
B
 
• 
[ ]
xG x
F
∃
→
B
 if 
[ ]
G x
F
→
B
 
Proposition 3-9: If F and G are formulae in first-order logic, then 
F
B
 and 
F
G
→
B
 imply 
G
B
. 
Theorem 3-5: (Soundness Theorem) If F is a formula in first-order logic then 
F
A
 implies 
F
B
. 

 
Classical Logics 
65 
 
Proof: Using the soundness theorem for propositional calculus (Theorem 3-2) 
and Proposition 3-7(1), it can be proved that the theorem is valid for each 
instance of each axiom schema. Again, using Proposition 3-8(1) and Proposition 
3-9, it can be proved that any theorem F is valid which has been obtained by the 
application of a rule of inference on the previous two theorems. Hence 
F
A
 
implies 
F
B
.   
 
Theorem 3-6: (Weak Deduction Theorem) If Γ is a set of formulae and F and G 
are formulae of first-order logic such that F is closed, then 
,F
G
Γ
A
 implies 
F
G
Γ
→
A
. 
Proposition 3-10: If S  is a set of formulae and F and G are two formulae then 
[ ]
F
G x
→
S A
 implies 
[ ]
F
xG x
→∃
S A
. 
Proposition 3-11: If S is a set of formulae and F a formula such that 
[ / ]
F x a
S A
, where a does not occur in any member of 
{[ ]}
F
x
∪
S
, then 
[ / ]
F x y
S A
, for some variable y which does not occur in a proof P of 
[ / ]
F x a . 
Proof: Since the constant a does not occur in any member of 
{[ ]}
F
x
∪
S
, 
another proof of 
[ / ]
F x y  can be constructed from P by replacing each occurrence 
of a in P by y.  
Proposition 3-12: If S is a set of formulae and 
[ ]
F x  a formula such that 
[ ]
F x
S A
, then 
[ / ]
F x y
S A
, where y does not occur in F. 
 
In practice, when we deal with consistent agent knowledge bases, the 
following theorem ensures that such a knowledge base is satisfiable. This means 
the agent has at least one interpretation which is a model, that is, every sentence 
in the knowledge base is true in the interpretation. 

66 
Decision-Making Agents 
 
 
Theorem 3-7: Let 
c
S  be a consistent set of statements of a first-order predicate 
logic P. Then 
c
S  is satisfiable in a countable domain D. Moreover, the 
domain D is a one-to-one correspondent to the cardinality of the set of 
primitive symbols of P. 
Proof: Suppose the domain of individual constants of P is extended by adding 
1
2
,
,...
a a
. The resulting domain is also denumerable. Let P’ be the extended logic 
and 
1
2
[ ],
[ ],...
xF x
xF x
∃
∃
 are all the statements of P’ of the form 
[ ]
xF x
∃
. Suppose 
0
c
=
S
S . Let 
1ia  be the first constant in 
1
2
,
,...
a a
 that does not occur in 
1[ ]
xF x
∃
, 
where 1i
j
=
, for some 
1,2,...
j =
. Consider the following: 
1
1
0
1
1
{
[ ]
[ /
]}
i
xF x
F x a
=
∪∃
→
S
S
 
Let 
1
jia
+  be the first constant in 
1
2
,
,...
a a
 that does not occur in 
1
1[ /
],...,
[ /
]
j
i
j
i
F x a
F x a
. From the set 
j
S , for 
1,2,...
j =
, the set 
1
j+
S
 is defined as 
follows: 
1
1
1
1
{
[ ]
[ /
]}
j
j
j
j
j
i
xF
x
F
x a
+
+
+
+
=
∪∃
→
S
S
 
Now, if 
1
S  is inconsistent, by Proposition 3-4, for some formula H: 
1
1
1
,
[ ]
[ /
]
c
i
xF x
F x a
H
H
∃
→
∧¬
S
A
 
i.e, 
1
1
1
(
[ ]
[ /
])
c
i
xF x
F x a
H
H
∃
→
→
∧¬
S A
 
 
[Theorem 3-6] 
i.e, 
1
1
(
[ ]
[ / ])
c
xF x
F x y
H
H
∃
→
→
∧¬
S A
, for some y 
[Proposition 3-11] 
i.e., 
1
1
(
[ ]
[ / ])
c
xF x
yF x y
H
H
∃
→∃
→
∧¬
S A
, for some y [Proposition 3-8(2)] 
i.e., 
1
1
(
[ ]
[ ])
c
xF x
xF x
H
H
∃
→∃
→
∧¬
S A
 
 
[Proposition 3-12] 
But, 
1
1
[ ]
[ ]
c
xF x
xF x
∃
→∃
S A
,  
[Since F
F
→
 is valid] 
Hence, 
c
H
H
∧¬
S A
  
[By applying Modus Ponens] 
Hence 
c
S  is inconsistent, which violates the initial assumption that 
c
S  is 
consistent. By applying the induction hypothesis, it can easily be proved that 
each 
j
S  is consistent for 
1,2,...
j =
 . 
Let 
j
j∈
= ∪
N
S
S , where N is the set of all natural numbers. Construct a set M as 
follows: 

 
Classical Logics 
67 
 
0 =
M
S  
1
{ }
j
j
A
+ =
∪
M
M
, if 
{ }
j
A
∪
M
 is consistent. 
j
j∈
= ∪
N
M
M  
Thus 
c ⊆
⊆
S
S
M . We claim M is a maximally consistent set of P’: Let F be any 
formula such that 
{ }
F
∪
M
 is consistent. Suppose that F is the (
1)
n +
-th 
formula in the chosen enumeration of the formulae of P’. Since 
{ }
F
∪
M
 is 
consistent, 
{ }
n
F
∪
M
 is consistent. Hence by definition of 
1
n+
M
, F is a member 
of 
1
n+
M
 and therefore a member of M, which shows that M is maximally 
consistent. Since M is a maximally consistent set, the two properties in 
Proposition 3-5 hold. Suppose a formula of the form 
[ ]
xF x
∃
 is in M. From the 
construction of 
⊆
S
M , 
[ ]
[ /
]
j
xF x
F x a
∃
→
 is in 
j
S , for some constant 
ja . Now 
(A) 
[ /
]
j
F x a
∈M , for some j. 
Otherwise, if 
[ /
]
j
F x a
∉M , for all j, then 
[ ]
[ /
]
j
xF x
F x a
∃
→
∉M , for all j 
(since 
[ ]
xF x
∃
 is in M, and M is consistent). Hence 
[ ]
[ /
]
j
xF x
F x a
∃
→
∉S , for 
all j, which violates the fact that 
0 ⊆
S
S .  
Next, we construct a model for M. Define an interpretation I of the language of 
P’ as follows: 
• 
The domain D of I is the set of all individual constants of P’. 
• 
If P is an n-place predicate symbol in the language of P’, then the 
corresponding n-ary relation R is defined as 
1
2
, ,..., n
t t
t
R
〈
〉∈
 if and only 
if 
1
2
( , ,..., )
n
P t t
t
M A
, for all closed terms 1
2
, ,..., n
t t
t . 
• 
If f is an n-place function symbol in the language of P’, then the 
corresponding 
n-place 
function 
'f  
is 
defined 
as 
1
2
1
2
'( , ,..., )
( , ,..., )
n
n
f t t
t
f t t
t
=
, for all closed terms 1
2
, ,..., n
t t
t . 
To prove that the interpretation I is a model of M, it suffices to show that 
(B) 
F is true in I if and only if 
F
M A
 
for each statement F of P’, that is, 
F
B
 if and only if 
F
M A
. Suppose this is 
established and G is any formula such that 
G
A
. Then, by generalization, 
G
∀
M A
 and thus 
G
∀
B
 (by (B)). Therefore, 
G
B
 (since by Proposition 3-6, for 
any formula F, 
F
∀
B
 if and only if 
F
B
). Hence I is a model of M. 

68 
Decision-Making Agents 
 
 
The proof of (B) is carried out by induction on the number n of connectives 
and quantifiers of F. Suppose (B) holds for any closed formulae with fewer than 
k connectives and quantifiers. Consider the following cases: 
• 
Suppose F is a closed atomic formula of the form 
1
2
( , ,..., )
n
P t t
t
. Then 
(B) follows directly from the definition of P. 
• 
Suppose F is of the form 
G
¬
. Then the number of connectives in G is 
one less than that of F. Now assume that F is true in I. Then, by 
induction hypothesis, it is not the case that 
G
M A
. Hence by Proposition 
3-5(1), G is not a member of M. Again, by Proposition 3-5(2), 
G
¬
 is a 
member of M. Hence by Proposition 3-5(1), 
G
¬
M A
, that is, 
F
M A
. 
To prove the converse, assume that 
F
M A
, that is, 
G
¬
M A
. Since M is 
consistent, it is not the case that 
G
M A
. By the induction hypothesis, G 
is not true and hence F is true in I. 
• 
Suppose F is G
H
→
. Since F is closed, so are G and H. Let 
F
M A
 and 
assume F is false for I. Then G is true and H is false for I. Each of G 
and H contains fewer connectives than in F. Hence by induction 
hypothesis, 
G
M A
 and it is not the case that 
H
M A
. Consider the 
following proof: 
Step 1: 
G
M A
 
 
 
 
Given 
Step 2: 
H
¬
M A
 
 
 
 
As the 2nd case above 
Step 3: 
(
(
))
G
H
G
H
→¬
→¬
→
M A
 
Tautology 
Step 4: 
(
)
H
G
H
¬
→¬
→
M A
 
 
MP on Steps 1 and 3 
Step 5: 
(
)
G
H
¬
→
M A
 
 
 
MP on Steps 2 and 4 
Step 6: 
F
¬
M A
 
 
 
 
Since F is G
H
→
 
Since M is consistent, it is not the case that 
F
M A
. This contradicts the 
initial assumption and hence F is true for I. On the other hand, assume F 
is true for I. If it is not the case that 
F
M A
, then by the properties of 
Proposition 3-5, 
F
¬
M A
, that is, 
G
H
∧¬
M A
, which means 
G
M A
 
and 
H
¬
M A
. By induction, G is true for I. Since M is consistent, it is 
not the case that 
H
M A
. Therefore H is false for I and F is false for I. 
This again contradicts the initial assumption. Thus F is true for I. 
• 
Suppose F is 
G
∀
 and G is a closed formula. By induction hypothesis, 
G
B
 if and only if 
G
M A
, that is, 
G
xG
↔∀
M A
, since x is not free in 

 
Classical Logics 
69 
 
G. Hence 
G
M A
 and 
xG
∀
M A
. Furthermore, 
G
M B
 if and only if 
xG
∀
M B
, since x is not free in G. Hence 
xG
∀
M A
 if and only if 
xG
∀
B
, that is, 
F
M A
 if and only if 
F
B
. 
• 
Suppose F is 
[ ]
xG x
∀
 and 
[ ]
G x  is not closed. Since F is closed, x is the 
only free variable in G. Suppose 
[ ]
xG x
∀
B
, and assume it is not the case 
that 
[ ]
G x
∀
M A
. Applying Proposition 3-5, 
[ ]
G x
¬∀
M A
, that is, 
[ ]
x G x
∃¬
M A  . From (A) there exists an individual constant a of P’ such 
that 
[ / ]
G x a
¬
M A
. By induction, 
[ / ]
G x a
¬
B
. Also, 
F
B
, that is, 
[ ]
xG x
∀
B
. Therefore, 
[ ]
[ / ]
xG x
G x a
∀
→
B
, that is, 
[ / ]
G x a
B
, which 
contradicts the consistency of the interpretation I. Therefore, 
[ ]
G x
∀
M A
. Again, let 
[ ]
xG x
∀
M A
 and if possible assume it is not the 
case that 
[ ]
xG x
∀
B
. Hence some sequence S does not satisfy 
[ ]
G x . Let t 
be the i-th component of S that has been substituted for x in showing the 
unsatisfiability of 
[ ]
G x  wrt S. Then the sequence S does not satisfy 
[ / ]
G x t  either, since the closed term t is mapped to itself under S. 
Furthermore, 
[ ]
xG x
∀
M A
, that is, 
[ / ]
G x t
∀
M A
. Hence by our 
induction hypothesis, 
[ / ]
G x t
B
. Therefore a contradiction arises and 
[ ]
xG x
∀
B
, that is, 
F
B
. 
Thus the interpretation I is a model for M and hence a model for 
c
S . The 
domain of this interpretation is a one-to-one correspondent with the set of all 
primitive symbols of P. Hence the proof.  
 
The following theorem is an immediate consequence of Theorem 3-7. 
Theorem 3-8: Every consistent theory has a denumerable model. 
Theorem 3-9: (Gödel's Completeness Theorem) For each formula F in first-order 
logic, if 
F
B
 then 
F
A
. 
Proof: Suppose that 
F
B
. Then by Proposition 3-6, 
F
∀
B
 and hence 
F
¬∀
 is 
not satisfiable. By Theorem 3-7, {
}
F
¬∀
 is inconsistent. Therefore, by 
Proposition 3-4, 
F
G
G
¬∀
∧¬
A
, for some formula G. Then, by the weak 
deduction theorem (Theorem 3-6), 
F
G
G
¬∀
→
∧¬
A
, and thus 
F
∀
A
. Suppose 

70 
Decision-Making Agents 
 
 
1
2
,
,...,
n
x x
x  are free variables of F and thus 
1
2...
n
x
x
x F
∀∀
∀
A
. Since a variable is 
free for itself, axiom A4 gives 
1
2
2
...
...
n
n
x
x
x F
x
x F
∀∀
∀
→∀
∀
A
 
By applying MP, 
2...
n
x
x F
∀
∀
A
. Continuing this way gives 
F
A
. 
 
The next two fundamental soundness and completeness theorems follow 
immediately from the soundness theorem established earlier, and from the above 
completeness theorem. 
Theorem 3-10: (Soundness and Completeness) For each formula F in first-order 
logic, 
F
B
 if and only if 
F
A
. 
Theorem 3-11: (Strong Soundness and Completeness) Suppose Γ  is any set of 
formulae and F is a formula in first-order logic. Then 
F
Γ B
 if and only if 
F
Γ A
. In other words, F is a logical consequence of Γ  if and only if F is a 
theorem of Γ . 
3.2.3 
Applications 
This section will establish some important theorems by using the soundness and 
completeness theorem of first-order logic established above. These theorems, 
especially the compactness theorem, help to establish Herbrand’s theorem, which 
is an important theoretical foundation for the resolution theorem proving. 
A theory 
1
K  is said to be an extension of a theory K if every theorem of K is 
a theorem of 
1
K . A theory K is complete if for any closed formula F of K, either 
F
A
 or 
F
¬
A
. 
Proposition 3-13: Suppose K is a consistent theory. Then K is complete if and 
only if K is maximally consistent. 
Proposition 3-14: (Lindenbaum's Lemma) If K is a consistent theory then K has 
a consistent, complete extension. 

 
Classical Logics 
71 
 
Theorem 3-12: (Compactness Theorem) A set of formulae S  in first-order logic 
is satisfiable if and only if every finite subset of S  is satisfiable. 
Proof: The forward part of the theorem follows easily. To prove the converse, 
suppose every finite subset of S is satisfiable. Then every finite subset of S is 
consistent. If not, there is a finite subset 
1
S  of S that is inconsistent. Then, for 
some formula F, 
1
F
S A
 and 
1
F
¬
S A
. By the soundness theorem, 
1
F
S B
 and 
1
F
¬
S B
. Hence 
1
S  cannot be satisfiable, which is a contradiction. Since every 
finite subset of S is consistent, S itself is consistent. For, if S is inconsistent then 
some finite subset of S is inconsistent which is a contradiction. Since S is 
consistent, by Theorem 3-7, S is satisfiable.  
Theorem 3-13: (Skolem-Löwenheim Theorem) Any theory K that has a model 
has a denumerable model. 
Proof: If K has a model, then K is consistent. For if K is inconsistent, then 
F
F
∧¬
A
 for some formula F. By the soundness theorem, 
F
B
 and 
F
¬
B
. If a 
formula is both true and false then K cannot have a model, which is a 
contradiction. By Theorem 3-8, K has a denumerable model.  
 
3.3 
Theorem Proving Procedure 
A procedure for determining whether or not a formula is a theorem of a particular 
theory K is called a theorem proving procedure or proof procedure for the theory 
K. Theorem proving procedures deal with formulae in standard forms, for 
example, prenex normal form, Skolem conjunctive normal form, and clausal 
form. This section provides tools for obtaining these forms from given formulae. 
3.3.1 
Clausal Form 
A formula is said to be in prenex normal form if it is of the form 
1 1
2
2...
n
n
Q x Q x
Q x B  
where each 
i
Q  is either ∀ or ∃, and the formula B is quantifier free. The 
formula B is called the matrix.   A prenex normal form formula is said to be in 
Skolem conjunctive normal form if it has the form 
1
2...
n
x
x
x B
∀∀
∀
 

72 
Decision-Making Agents 
 
 
where the matrix B is in conjunctive normal form, that is, B is a conjunction of a 
disjunction of literals (as defined in Section 3.1). Such a Skolem conjunctive 
normal form formula is said to be a clause if it has the form 
1
2
1
2
...
(
...
)
n
m
x
x
x L
L
L
∀∀
∀
∨
∨
∨
 
where each 
iL  is a literal and 
1
2
,
,...,
n
x x
x  are the free variables of the disjunction 
1
2
...
m
L
L
L
∨
∨
∨
. A formula is said to be in clausal form if it is a clause. 
For the sake of convenience, a clause is rewritten as the disjunction 
1
2
...
m
L
L
L
∨
∨
∨
 of literals without its quantifiers or as the set 
1
2
{
,
,...,
}
m
L L
L
 of 
literals. Thus when a disjunction 
1
2
...
m
L
L
L
∨
∨
∨
 or a set 
1
2
{
,
,...,
}
m
L L
L
 is given 
as a clause C, where each 
iL  is a literal, then C is regarded as being of the form 
1
2
1
2
...
(
...
)
n
m
x
x
x L
L
L
∀∀
∀
∨
∨
∨
, where 
1
2
,
,...,
n
x x
x  are all the free variables 
occurring in all the 
iL s. 
Every formula F can be transformed to some formula G in Skolem 
conjunctive normal form such that F
G
≡
 by applying the appropriate 
transformations of the following steps. 
Step 1: (Elimination of → and ↔) Apply the following two conversion 
rules to any subformula within the given formula: 
F
G
→
 to 
F
G
¬
∨
 
F
G
↔
 to (
)
(
)
F
G
G
F
¬
∨
∧¬
∨
 
Step 2: (Moving ¬ inwards) Apply the following conversion rules 
repeatedly until all negations are immediately to the left of an atomic 
formula: 
F
¬¬
 to F 
(
)
F
G
¬
∧
 to 
F
G
¬
∨¬
 
(
)
F
G
¬
∨
 to 
F
G
¬
∧¬
 
xF
¬∀
 to x F
∃¬
 
xF
¬∃
 to 
x F
∀¬
 
Step 3: (Moving quantifiers inwards) Apply the following conversion rules to 
any subformula within the formula until no rule is applicable: 
(
)
x F
G
∀
∧
 to 
xF
xG
∀
∧∀
 
(
)
x F
G
∃
∨
 to xF
xG
∃
∨∃
 

 
Classical Logics 
73 
 
(
)
x F
G
∀
∨
 to 
xF
G
∀
∨
, provided x is not free in G 
(
)
x F
G
∃
∧
 to xF
G
∃
∧
, provided x is not free in G 
(
)
x F
G
∀
∨
 to F
xG
∨∀
, provided x is not free in F 
(
)
x F
G
∃
∧
 to F
xG
∧∃
, provided x is not free in F 
xF
∀
 to F, provided x is not free in F 
xF
∃
 to F, provided x is not free in F 
(
)
x y F
G
∀∀
∨
 to 
(
)
y x F
G
∀∀
∨
 
(
)
x y F
G
∃∃
∧
 to 
(
)
y x F
G
∃∃
∧
 
The last two transformations in this step should be applied in a 
restricted manner to avoid infinite computation. These two should be 
applied to a subformula of a given formula provided y is free in both 
F and G and x is not free in either F or G. 
Step 4: (Variable renaming) Repeat this step until no two quantifiers share 
the same variable. 
When two quantifiers share the same variable, simultaneously 
rename one of the variables in the quantifier and its associated bound 
occurrences by a new variable. 
 Step 5: (Skolemization) Repeat this step until the formula is free from 
existential quantifiers. 
Suppose the formula contains an existential quantifier 
x
∃. The 
variable x will be within the scope of n universally quantified 
variables 
1
2
,
,...,
n
x x
x  (for some 
0
n ≥
). At each occurrence of x 
(other than as the quantifier name), replace x by the term 
1
2
( ,
,...,
)
n
f x x
x
, where f is an n-ary function symbol that does not 
occur in the formula. (If 
0
n =
 then use a constant symbol instead 
of
1
2
( ,
,...,
)
n
f x x
x
.) This process of removing existential quantifiers 
from a formula is called Skolemization and each newly entered 
1
2
( ,
,...,
)
n
f x x
x
 is called a Skolem function instance. 
Step 6: (Rewrite in prenex normal form) Remove all universal quantifiers 
from the formula and place them at the front of the remaining 
quantifier free formula (which is the matrix). The resulting formula 
is now in prenex normal form. 

74 
Decision-Making Agents 
 
 
Step 7: (Rewrite in Skolem conjunctive normal form) Apply the following 
transformations repeatedly to the matrix of the formula until the 
matrix is transformed to conjunctive normal form: 
(
)
F
G
H
∨
∧
 to (
)
(
)
F
G
F
H
∨
∧
∨
 
(
)
F
G
H
∧
∨
 to (
)
(
)
F
H
G
H
∨
∧
∨
 
Example 
This example shows the conversion of a formula to its equivalent Skolem 
conjunctive normal form. The applicable step from the above transformation 
procedure is given in the right-hand column. 
(
( )
( )
( , ))
( ( )
( , ))
x
R x
P a
z Q z a
x P x
yQ y x
∀
¬
→
∧¬∃¬
∧∀
→∃
 
 
Given 
(
( )
( )
( , ))
(
( )
( , ))
x
R x
P a
z Q z a
x
P x
yQ y x
∀
¬¬
∨
∧¬∃¬
∧∀
¬
∨∃
 
 
Step 1 
( ( )
( )
( , ))
(
( )
( , ))
x R x
P a
z
Q z a
x
P x
yQ y x
∀
∨
∧∀¬¬
∧∀
¬
∨∃
 
 
Step 2 
( ( )
( )
( , ))
(
( )
( , ))
x R x
P a
zQ z a
x
P x
yQ y x
∀
∨
∧∀
∧∀
¬
∨∃
  
 
Step 2 
(
( )
( )
( , ))
(
( )
( , ))
xR x
P a
zQ z a
x
P x
yQ y x
∀
∨
∧∀
∧∀¬
∨∃
  
 
Step 3 
1
1
1
(
( )
( )
( , ))
(
(
)
( ,
))
xR x
P a
zQ z a
x
P x
yQ y x
∀
∨
∧∀
∧∀
¬
∨∃
  
 
Step 4 
1
1
1
1
(
( )
( )
( , ))
(
(
)
( (
),
))
xR x
P a
zQ z a
x
P x
Q f x
x
∀
∨
∧∀
∧∀
¬
∨
 
 
Step 5 
1
1
1
1
(( ( )
( )
( , ))
(
(
)
( (
),
)))
x z x
R x
P a
Q z a
P x
Q f x
x
∀∀∀
∨
∧
∧¬
∨
 
 
Step 6 
1
1
1
1
(( ( )
( ))
( ( )
( , ))
(
(
)
( (
),
)))
x z x
R x
P a
R x
Q z a
P x
Q f x
x
∀∀∀
∨
∧
∨
∧¬
∨
 
Step 7 
 
3.3.2 
Herbrand's theorem 
By the definition of satisfiability, a formula F is unsatisfiable if and only if it is 
false under all interpretations over all domains. Therefore it is an enormous and 
almost impossible task to consider all interpretations over all domains to verify 
the unsatisfiability of F. Hence it is desirable to have one specified domain such 
that F is unsatisfiable if and only if F is false under all interpretations over this 
special domain. Fortunately, this kind of special domain exists: it is the Herbrand 
universe of F, for a given formula F. The power of the Herbrand universe will 
now be demonstrated by using it to establish some results. 

 
Classical Logics 
75 
 
Given a formula F in Skolem conjunctive normal form, the Herbrand 
universe of F, denoted as 
( )
F
HU
 is inductively defined as follows: 
• 
Any constant symbol occurring in F is a member of 
( )
F
HU
. If F does 
not contain any constant symbol, then 
( )
F
HU
 contains the symbol a. 
• 
If f is an n-ary function symbol occurring in F and 
1
2
, ,..., n
t t
t  are in 
( )
F
HU
 then 
1
2
( , ,..., )
n
f t t
t
 is a member of 
( )
F
HU
. 
The Herbrand base of a formula F, denoted by 
( )
F
HB
, is the following set: 
{
1
2
( , ,..., )
n
P t t
t
| P  is an n-ary predicate symbol occurring in F 
and 1
2
, ,...,
( )
n
t t
t
F
∈HU
}. 
A ground instance of a formula F in Skolem conjunctive normal form is a 
formula obtained from the matrix of F by replacing its variables by constants. 
Example 
Suppose F = 
( ( )
(
( )
( ( ))))
x Q x
P x
P f x
∀
∧¬
∨
. Then 
( )
F
HU
 = { ,
( ),
( ( )),...}
a f a
f f a
 and 
( )
F
HB
 = { ( ),
( ( )),
( ( ( ))),..., ( ), ( ( )), ( ( ( ))),...}
Q a Q f a
Q f f a
P a P f a
P f f a
 
The formula 
( ( ))
(
( ( ))
( ( ( ))))
Q f a
P f a
P f f a
∧¬
∨
 is a ground instance of F. 
Suppose F = 
( ( , )
(
( , )
( )))
x P a b
P x y
Q x
∀
∧¬
∨
. Then 
( )
F
HU
 = { , }
a b  and 
( )
F
HB
 = { ( , ), ( , ), ( , ), ( , ),
( ),
( )}
P a a P a b P b a P b b Q a Q b
. 
 
Given a formula F in prenex normal form, an interpretation HI over 
( )
F
HU
 
is a Herbrand interpretation if the following conditions are satisfied: 
• 
HI assigns every constant in 
( )
F
HU
to itself. 
• 
Let f be an n-ary function symbol and 1
2
, ,..., n
t t
t  be elements of 
( )
F
HU
. 
Then HI assigns f to an n-place function that maps the n-tuple 
1
2
, ,..., n
t t
t
〈
〉 to 
1
2
( , ,..., )
n
f t t
t
. 

76 
Decision-Making Agents 
 
 
Proposition 3-15: Suppose F = 
1
2
1
2
...
[ ,
,...,
]
p
p
x
x
x B x x
x
∀∀
∀
 is a formula in 
prenex normal form, where B is the matrix of F. Then F is unsatisfiable if 
and only if F is false under all Herbrand interpretations. 
Proof: If a formula is unsatisfiable then F is false under all interpretations and 
hence false under all Herbrand interpretations. 
Conversely, suppose F is satisfiable. Then there is an interpretation I over a 
domain D such that ( )
F =F
I
. Construct a Herbrand interpretation HI as follows: 
For any n-ary predicate symbol P occurring in F and any 
1
2
, ,...,
( )
n
t t
t
F
∈HU
, 
1
2
, ,..., n
t t
t
P
〈
〉∈
 
if 
and 
only 
if 
1
2
( ), ( ),..., ( )
( )
n
t
t
t
P
〈
〉∈
I
I
I
I
. 
For any atomic formula A, 
( )
( )
A
A
=
HI
I
. Hence if 
0
p =
 in F, that is, if F is a 
quantifier free formula, 
( )
( )
F
F
=
=F
HI
I
. If 
0
p >
 in F, then without any loss 
of generality one can assume that F has the form 
[ ]
xB x
∀
, where B is the matrix 
of F. 
Now ( )
(
[ ])
F
xB x
=
∀
=F
I
I
. 
Therefore ( [ / ( )])
B x
t
=F
I
I
, where t is an arbitrary member of D. 
Now 
( / )
( / ( ))
( [ ])
( [ ])
x t
x
t
B x
B x
=
HI
I
I
. 
Since [ ]
B x  is quantifier free, 
( [ / ])
( [ / ])
B x t
B x t
=
HI
I
. 
Therefore 
( [ / ])
( [ / ( )])
B x t
B x
t
=
=F
HI
I
I
. 
Thus if a formula F is satisfiable in some interpretation, then F is also satisfiable 
under the Herbrand interpretation. Hence if F is unsatisfiable under all Herbrand 
interpretations then F is unsatisfiable.  
 
Suppose 
1
2
1
2
...
[ ,
,...,
]
n
n
x
x
x B x x
x
∀∀
∀
 is a formula in prenex normal form. An 
instance of matrix B means a formula of the form 
1
1
2
2
[
/ ,
/
,...,
/
]
n
n
B x
t x
t
x
t
 where 
1
2
, ,..., n
t t
t are elements of the Herbrand universe of the given formula. 
Theorem 3-14: (Herbrand's Theorem) Suppose F = 
1
2
1
2
...
[ ,
,...,
]
p
p
x
x
x B x x
x
∀∀
∀
 
is a formula in prenex normal form. Then F is unsatisfiable if and only if 
there are finitely many instances of the matrix 
1
2
[
,
,...,
]
p
B x x
x
 which are 
unsatisfiable. 

 
Classical Logics 
77 
 
Proof: Without any loss of generality, the formula F can be assumed to be of the 
form 
[ ]
xB x
∀
, where B is quantifier free. Suppose 
1
2
[ / ], [ /
],..., [ /
]
n
B x t
B x t
B x t
 
are 
finitely 
many 
instances 
of 
[ ]
B x . 
Then
1
2
[ ]
[ / ]
[ /
]
...
[ /
]
n
xB x
B x t
B x t
B x t
∀
∧
∧
∧
A
. Thus the satisfiability of 
[ ]
xB x
∀
 
implies the satisfiability of the instances 
1
2
[ / ], [ /
],..., [ /
]
n
B x t
B x t
B x t
. Hence if 
the instances are unsatisfiable, then the formula 
[ ]
xB x
∀
 itself is unsatisfiable. 
Conversely, suppose 
[ ]
xB x
∀
 is unsatisfiable and, if possible, let any finitely 
many instances of 
[ ]
B x  be satisfiable. Then, by the compactness theorem, 
{ [ / ]|
( )}
B x t
t
F
∈HU
 is satisfiable. Accordingly, 
(
[ ])
xB x
∀
=F
HI
, contradicting 
the assumption that 
[ ]
xB x
∀
 is unsatisfiable. Hence there exist finitely many 
instances of [ ]
B x  which are unsatisfiable. 
 
Suppose F is an arbitrary formula. With the aid of the transformation 
procedure described earlier in this subsection, one can assume that the formula F 
has the Skolem conjunctive normal form 
1
2
1
2
...
(
...
)
n
m
x
x
x B
B
B
∀∀
∀
∧
∧
∧
, where 
each 
iB  is a disjunction of literals. If 
1
2
,
,...,
n
x x
x  are all the free variables of the 
conjunction 
then, 
for 
the 
sake 
of 
convenience, 
the 
formula 
1
2
1
2
...
(
...
)
n
m
x
x
x B
B
B
∀∀
∀
∧
∧
∧
 is rewritten as the set 
1
2
{
,
,...,
}
m
B B
B
. 
Thus when a set S = 
1
2
{
,
,...,
}
m
B B
B
 is given, where each 
iB  is a disjunction 
of literals, then S is regarded as the form 
1
2
1
2
...
(
...
)
n
m
x
x
x B
B
B
∀∀
∀
∧
∧
∧
 in 
which 
1
2
,
,...,
n
x x
x  are all the free variables occurring in all the 
iB s. The set S can 
also be regarded as the conjunction of clauses 
1
2
,
,...,
m
B
B
B
∀
∀
∀
. 
Example 
Continuing with the previous example, the formula 
1
1
1
1
(( ( )
( ))
( ( )
( , ))
(
(
)
( (
),
)))
x z x
R x
P a
R x
Q z a
P x
Q f x
x
∀∀∀
∨
∧
∨
∧¬
∨
 
can equivalently be written as a set 
1
1
1
1
{
( ( )
( )),
( ( )
( , )),
(
(
)
( (
),
))}
x R x
P a
x z R x
Q z a
x
P x
Q f x
x
∀
∨
∀∀
∨
∀
¬
∨
 
of clauses or as a set 
1
1
1
{ ( )
( ), ( )
( , ),
(
)
( (
),
)}
R x
P a R x
Q z a
P x
Q f x
x
∨
∨
¬
∨
 

78 
Decision-Making Agents 
 
 
of clauses when the free variables are assumed to be universally quantified at the 
front of each clause. 
 
An alternative version of Herbrand's theorem (Theorem 3-14) based on the 
representation of a formula as a set of clauses is stated below. The proof follows 
immediately from the original theorem. 
Proposition 3-16: A set S  of clauses is unsatisfiable if and only if there are 
finitely many ground instances of clauses of S  which are unsatisfiable. 
 
3.3.3 
Implementation of Herbrand's theorem 
Suppose S is a finite, unsatisfiable set of clauses. Then according to Herbrand's 
theorem, the unsatisfiability of S can be proved in the following manner. First, 
enumerate a sequence 
1
2
,
,...
S S
 of finite sets of ground instances from S such that 
1
2
...
⊆
⊆
S
S
and then test the satisfiability of each 
iS . One way of enumerating 
the sequence 
1
2
,
,...
S S
 is that each member of 
iS  is of length at most i. (If 
iS is 
the finite set of all ground instances of length 
i
≤, then we eventually have 
⊆
k
S
S for some k; 
k
S  will be unsatisfiable.) Since each member of 
iS  is ground, 
its satisfiability can be checked by a standard method (for example by truth table) 
of propositional logic. 
Gilmore's approach (Gilmore, 1960) was to transform each 
iS  as it is 
generated to its equivalent disjunctive normal form D and then to remove from D 
any conjunction in D containing a complementary pair of literals. If D 
(disjunction equivalent to 
iS , for some i) becomes empty at some stage due to 
this transformation, then 
iS  is unsatisfiable and therefore S is unsatisfiable. 
Gilmore's idea was combinatorially explosive because, for example, if an 
iS  
contains 10 three-literal clauses then there will be 
10
3  conjunctions to be tested. 
A more efficient approach to test the unsatisfiability for a set of ground 
clauses was devised by Davis and Putnam (1960). This method consists of a 
number of rules which are stated below. 

 
Classical Logics 
79 
 
• 
Tautology Rule:  Suppose 
1
S  is obtained from S by deleting all clauses 
from S that are tautologies. Then 
1
S is unsatisfiable if and only if S is 
unsatisfiable. 
• 
One-literal Rule: Suppose there is a unit clause L in S and 
1
S is obtained 
from S by deleting all clauses from S that contains an occurrence of L, 
and then replacing each clause C from the rest of S by a clause obtained 
from C by removing the occurrence (if any) of the complement of L. 
Then 
1
S is unsatisfiable if and only if S is unsatisfiable. 
• 
Pure-literal Rule:  If a literal L occurs in some clause in S but its 
complement 
'
L  does not occur in any clause, then 
1
S  is obtained from S 
by deleting all clauses containing an occurrence of L. Then S is 
unsatisfiable if and only if 
1
S  is unsatisfiable. The literal L in this case is 
called a pure literal. 
• 
Splitting 
Rule: 
 
Suppose 
S 
= 
1
1
1
{
,...,
,
,...,
,
,...,
}
m
n
p
C
A
C
A D
A
D
A E
E
∨
∨
∨¬
∨¬
, where A is an atom 
and each 
i
C , 
j
D , and 
k
E  does not contain any occurrence of A or 
A
¬ . 
Set 
1
S  = 
1
1
{
,...,
,
,...,
}
m
p
C
C
E
E
 and 
2
S  = 
1
1
{
,...,
,
,...,
}
n
p
D
D E
E
. Then S is 
unsatisfiable if and only if each 
1
S  and 
2
S  is unsatisfiable. 
• 
Subsumption Rule:  Suppose 
1
S  is obtained from S by deleting every 
clause D from S for which there is another clause C in S such that every 
literal that occurs in C also occurs in D. Then S is unsatisfiable if and 
only if 
1
S  is unsatisfiable. 
The above set of rules is applied repeatedly until no more rules can be 
applied on the resulting sets. If each of the resulting sets contains the empty 
clause, then the given set of clauses will be unsatisfiable. 
Example 
The following steps provide an example: 
Step 1: S = {
( )
( )
( )
P a
P a
Q b
∨¬
∨
, 
( )
Q a , 
( )
( )
Q a
P c
∨
, 
( )
( )
R a
P b
∨
, 
( )
( )
( )
Q a
S a
R b
¬
∨
∨
, 
( )
( )
S b
P b
¬
∨¬
, ( )
S b } 
Step 2: {
( )
Q a , 
( )
( )
Q a
P c
∨
, 
( )
( )
R a
P b
∨
, 
( )
( )
( )
Q a
S a
R b
¬
∨
∨
, 
( )
( )
S b
P b
¬
∨¬
, ( )
S b }, by the Tautology rule 

80 
Decision-Making Agents 
 
 
Step 3: {
( )
( )
R a
P b
∨
, ( )
( )
S a
R b
∨
, 
( )
( )
S b
P b
¬
∨¬
, ( )
S b }, by the One-
literal rule 
Step 4: { ( )
( )
S a
R b
∨
, 
( )
( )
S b
P b
¬
∨¬
, ( )
S b }, by the Pure literal rule 
Step 5: {
( )
( )
S b
P b
¬
∨¬
, ( )
S b }, by the Pure literal rule 
Step 6: 
1
{
( ), ( )}
S b S b
= ¬
S
, 
2
{
( )}
P b
= ¬
S
, by the Splitting rule, 
with
( )
A
S b
=
 
Step 7: 
1
{
( ), ( )}
S b S b
= ¬
S
, 
2
{}
=
S
, by the One-literal rule 
The set 
1
S  is unsatisfiable but 
2
S  is not. Therefore, S  is satisfiable. 
 
3.4 
Resolution Theorem Proving 
Theorem proving in a system of first-order logic using the resolution principle as 
the sole inference rule is called resolution theorem proving. This style of theorem 
proving avoids the major combinatorial obstacles to efficiency found in earlier 
theorem proving methods (Gilmore, 1960; Davis and Putnam, 1960), which used 
procedures based on Herbrand’s fundamental theorem concerning first-order 
logic. A logic programming system adopts a version of resolution theorem 
proving as its inference subsystem. Hence it is important to have a clear idea 
about this kind of theorem proving to understand the internal execution of a logic 
programming system. This section carries out a detailed discussion of the 
resolution principle, resolution theorem proving and some important relevant 
topics including unification and refinements of the resolution principle. Most of 
the results in this chapter (except the last section) are due to Robinson (1965). 
3.4.1 
Resolution principle and unification 
The resolution principle is an inference rule of first-order logic which states that 
from any two clauses C and D, one can infer a resolvent of C and D. The 
principle idea behind the concept of resolvent, and hence behind the resolution 
principle, is that of unification. Unification is a process of determining whether 
two expressions can be made identical by some appropriate substitution for their 
variables. Some definitions and results must be established before formally 
introducing the concept of unification. 
Terms and literals are the only well-formed expressions  (or simply 
expressions). A substitution θ  is a finite set of pairs of variables and terms, 

 
Classical Logics 
81 
 
denoted by 
1
1
2
2
{
/ ,
/
,...,
/ }
n
n
x
t x
t
x
t
, where 
ix s are distinct variables and each it  is 
a term different from 
ix . The term 
it  is called a binding for the variable 
ix . A 
substitution θ  is called a ground substitution if each 
it  is a ground term. The 
substitution given by the empty set is called the empty substitution (or identity 
substitution)  and is denoted by {} or ε . In a substitution 
1
1
2
2
{
/ ,
/
,...,
/ }
n
n
x
t x
t
x
t
, 
variables 
1
2
,
,...,
n
x x
x  are called the variables of the substitution and 1
2
, ,..., n
t t
t  are 
called the terms of the substitution. 
Let θ  = 
1
1
2
2
{
/ ,
/
,...,
/ }
n
n
x
t x
t
x
t
 be a substitution and E be an expression. The 
application of θ  to E, denoted by Eθ , is the expression obtained by 
simultaneously replacing each occurrence of the variable 
ix  in E by the term 
it . 
In this case Eθ  is called the instance of E by θ . If Eθ  is ground then Eθ  is 
called a ground instance of E, and E is referred to as a generalization of Eθ . 
Let 
1
1
2
2
{
/ ,
/
,...,
/
}
m
m
x
t x
t
x
t
θ =
 and 
1
1
2
2
{
/
,
/
,...,
/
}
n
n
y
s y
s
y
s
φ =
 be two 
substitutions. Then the composition θφ  of θ  and φ  is the substitution obtained 
from the set 
1
1
2
2
1
1
2
2
{
/
,
/
,...,
/
,
/
,
/
,...,
/
}
m
m
n
n
x
t
x
t
x
t
y
s y
s
y
s
φ
φ
φ
 
by deleting any binding 
/
i
i
x t φ  for which 
i
i
x
t φ
=
 and deleting any binding 
/
i
i
y
s  
for which 
1
2
{ ,
,...,
}
i
m
y
x x
x
∈
. 
Example 
Let 
( ,
( ), , ( ))
E
p x f x y g a
=
 
and 
{ / ,
/ ( )}
x b y h x
θ =
. 
Then 
( ,
( ), ( ), ( ))
E
p b f b h x g a
θ =
. Let 
{ / ,
/ ( )}
x b y h z
θ =
 and 
{ / }
z c
φ =
. Then 
{ / ,
/ ( ), / }
x b y h c z c
θφ =
. 
 
The following set of results describes different properties of substitutions and 
their compositions. 
Proposition 3-17: For any substitution θ , θε
εθ
θ
=
=
, where ε  is the empty 
substitution. 

82 
Decision-Making Agents 
 
 
Proposition 3-18: Let E be any string and 
,
α β  be two arbitrary substitutions. 
Then (
)
(
)
E
E
α β
αβ
=
. 
Proof: Let 
1
1
2
2
{
/
,
/
,...,
/
}
n
n
x
e x
e
x
e
α =
 and 
1
1
2
2
{
/
,
/
,...,
/
}
m
m
y
s y
s
y
s
β =
. The 
string E can be assumed to be of the form 
1
2
0
1
1
...
p
i
i
p
i
p
E
E x E x
E
x E
−
=
, where none 
of the substring 
j
E  of E contains occurrences of variables 
1
2
,
,...,
n
x x
x , some of 
j
E  
are 
possibly 
null, 
and 
1
ji
n
≤
≤
 
for 
1,2,...,
j
p
=
. 
Therefore, 
1
2
0
1
1
...
p
i
i
p
i
p
E
E e E e
E
e E
α
−
=
 and 
1
2
0
1
1
(
)
...
p
i
i
p
i
p
E
T t Tt
T
t T
α β
−
=
, where each 
j
j
i
i
t
e β
=
 
and 
j
j
T
E γ
=
, and γ  is a substitution whose variables are not among 
1
2
,
,...,
n
x x
x . But each element in the composite substitution αβ  is of the form 
/
i
k
x
t  whenever kt  is different from 
ix . Hence 
1
2
0
1
1
(
)
...
p
i
i
p
i
p
E
T t T t
T
t T
αβ
−
=
. 
Proposition 3-19: Let 
,
α β  be two arbitrary substitutions. If E
E
α
β
=
 for all 
strings E, then α
β
=
. 
Proof: Let 
1
2
,
,...,
n
x x
x  be all variables of the two substitutions α  and β . Since 
E
E
α
β
=
 for string E and each 
ix  is also a string, 
i
i
x
x
α
β
=
, 
1,2,...,
i
n
=
. Hence 
the elements of α  and β  are the same.  
Proposition 3-20: The composite operation on substitution is associative, that is, 
for any substitution 
, ,
α β γ , (
)
(
)
αβ γ
α βγ
=
. Hence in writing a composition 
of substitutions, parentheses can be omitted.  
Proof: Let E be a string. Then by Proposition 3-18, we have the following: 
((
) )
( (
))
((
) )
(
)(
)
( (
))
E
E
E
E
E
αβ γ
αβ γ
α β γ
α
βγ
α βγ
=
=
=
=
. 
Hence by Proposition 3-19, (
)
(
)
αβ γ
α βγ
=
.  
 
A unifier of two expressions E and 
'
E  is a substitution θ  such that Eθ  is 
syntactically identical to 
'
E θ . If the two atoms do not have a unifier then they 
are not unifiable.  A unifier θ is called a most general unifier (mgu) for the two 
expressions E and 
'
E  if for each unifier α  of E and 
'
E  there exists a 
substitution β  such that α
θβ
=
. 

 
Classical Logics 
83 
 
Example 
An mgu of two expressions 
( ,
( , ))
p x f a y
 and 
( , )
p b z  is { / , /
( , )}
x b z f a y
. A 
unifier of these two expressions is { / ,
/ , /
( , )}
x b y c z f a c
. 
An mgu is unique up to variable renaming. Because of this property the mgu 
of two expressions is used quite often. The concept of disagreement set for a set 
of expressions is pertinent here for presenting an algorithm to find an mgu for a 
set of expressions (if it exists). 
Let S be any set 
1
2
{
,
,...
}
n
E E
E
 of well-formed expressions. Then the 
disagreement set of S, denoted as 
( )
S
D
, is obtained by locating the first symbol 
at which not all 
iE  have exactly the same symbol, and then extracting from each 
iE  the subexpression 
it  that begins with the symbol. The set 
1
2
{ , ,... }
n
t t
t
 is the 
disagreement set of S. 
Example 
Let S be { ( , , , ), ( , ,
( , ), ), ( , , ( ( )), )}
P a b x y P a b f x y z P a b g h x
y
. The string of the first 
six symbols in each of the expressions in S is “
( , ,
P a b ”. The first symbol 
position at which not all expressions in S are exactly the same is the seventh 
position. The extracted subexpressions from each of the expressions in S starting 
from the seventh position are ,
( , )
x f x y , and 
( ( ))
g h x
. Hence the disagreement 
set 
( )
S
D
 = { ,
( , ), ( ( ))}
x f x y g h x
. 
 
Algorithm 3-1: (Unification Algorithm)  
Input: S, a set of well-formed expressions.  
Step 1: Set 
0
i =
, 
0
θ
ε
=
. 
Step 2: If 
iθ
S
 is singleton then set 
i
θ
θ
=
S
 and return θS  as an mgu for S. 
Step 3: If elements 
ix  and 
ie  do not exist in the disagreement set 
(
)
iθ
S
D
 
such that 
ix  is a variable and 
ix  does not occur in 
ie  then stop; S is 
not unifiable. This check of whether 
ix  occurs in 
ie  or not is called 
the occur check.  
Step 4: Set 
1
{
/ }
i
i
i
i
x
e
θ
θ
+ =
. 
Step 5: Set 
1
i
i
= +  and go to Step 2. 

84 
Decision-Making Agents 
 
 
This unification algorithm always terminates for any finite non-empty set of 
well-formed expressions; otherwise, an infinite sequence 
0
1
,
,...
θ
θ
S
S
 would be 
generated. Each such 
iθ
S
 is a finite non-empty set of well-formed expressions 
and 
1
iθ +
S
 contains one less variable than 
iθ
S
. So an infinite sequence is not 
possible, because S contains only finitely many distinct variables.  
Theorem 3-15 below proves that if S is unifiable, then the algorithm always 
finds an mgu for S. 
Examples 
We find an mgu for 
{ ( , ( ),
( ( ))), ( , ,
( ))}
P x g y
f g b
P a z f z
=
S
 by applying the 
above unification algorithm. 
• 
0
i =
, 
0
θ
ε
=
 
 
 
 
 
 
By Step 1 
• 
0
θ =
S
S  and 
0
θ
S
 is not a singleton 
 
 
By Step 2 
• 
0
(
)
{ , }
x a
θ
=
S
D
, 
0x
x
=
, 
0e
a
=
  
 
 
By Step 3 
• 
1
{ / }
x a
θ =
 
 
 
 
 
 
By Step 4 
• 
1
i =  
 
 
 
 
 
 
By Step 5 
• 
1
{ ( , ( ),
( ( ))), ( , ,
( ))}
P a g y
f g b
P a z f z
θ =
S
 
 
By Step 2 
• 
1
(
)
{ ( ), }
g y z
θ
=
S
D
, 
1x
z
=
, 
1
( )
e
g y
=
 
 
 
By Step 3 
• 
2
1{ / ( )}
{ / }{ / ( )}
{ / , / ( )}
z g y
x a
z g y
x a z g y
θ
θ
=
=
=
  
By Step 4 
• 
2
i =
 
 
 
 
 
 
 
By Step 5 
• 
2
{ ( , ( ),
( ( ))), ( , ( ),
( ( )))}
P a g y
f g b
P a g y
f g y
θ =
S
  
By Step 2 
• 
2
(
)
{ , }
y b
θ
=
S
D
, 
2x
y
=
, 
2e
b
=
  
 
 
By Step 3 
• 
3
2{ / }
{ / , / ( )}{ / }
{ / , / ( ),
/ }
y b
x a z g y
y b
x a z g b y b
θ
θ
=
=
=
, By Step 4 
• 
3
i =
 
 
 
 
 
 
 
   By Step 5 
• 
3
{ ( , ( ),
( ( )))}
P a g b
f g b
θ =
S
, 
3
θ
S
 is singleton and 
3
θ
θ
=
S
, By Step 2 
The 
algorithm 
terminates 
here. 
As 
another 
example, 
the 
set 
{ ( , ), ( ( ), ( ))}
P x x P f a g a
=
S
does not have an mgu. 

 
Classical Logics 
85 
 
Theorem 3-15: Let S  be any finite non-empty set of well-formed expressions. If 
S  is unifiable then the above unification algorithm always terminates at Step 
2, and θS  is an mgu of S . 
Proof: Suppose S is unifiable. To prove that θS  is an mgu of S, then for each 
0
i ≥
 (until the algorithm terminates) and for any unifier θ  of S, 
i
i
θ
θ β
=
 must 
hold at Step 2, for some substitution 
iβ . This is proved by induction on i. 
Base Step: 
0
i =
, 
0
θ
ε
=
 and hence 
iβ  can be taken as θ . 
Induction Step:  Assume 
i
i
θ
θ β
=
 holds for 0
i
n
≤≤
. The unification 
algorithm stops at Step 2 if 
n
θ
S
 is a singleton then, since 
n
n
θ
θ β
=
, 
n
θ  is an mgu 
of S. The unification algorithm will find 
(
)
n
θ
S
D
 at Step 3 if 
n
θ
S
 is not a 
singleton then. Since 
n
n
θ
θ β
=
 and θ  is a unifier of S, 
n
β  unifies 
n
θ
S
. Therefore 
n
β  also unifies the disagreement set 
(
)
n
θ
S
D
. Hence 
nx  and 
ne  defined in Step 3 
of the unification algorithm satisfy 
n
n
n
n
x
e
β
β
=
. Since 
(
)
n
θ
S
D
 is a disagreement 
set, at least one well-formed expression in 
(
)
n
θ
S
D
 begins with a variable. Since 
a variable is also a well-formed expression, at least one well-formed expression 
in 
(
)
n
θ
S
D
 is a variable. Take this variable as 
nx  and suppose 
ne  is any other 
well-formed expression from 
(
)
n
θ
S
D
. 
n
n
n
n
x
e
β
β
=
, since 
n
β  unifies 
(
)
n
θ
S
D
 
and 
nx , 
ne  are members of 
(
)
n
θ
S
D
. Now if 
nx  occurs in 
ne , 
n
n
x β  occurs in 
n
n
e β . This is impossible because 
nx  and 
ne  are distinct well-formed expressions 
and 
n
n
n
n
x
e
β
β
=
. Therefore 
nx  does not occur in 
ne . Hence the algorithm does 
not stop at Step 3. Step 4 will set 
1
{
/
}
n
n
n
n
x
e
θ
θ
+ =
. Step 5 will set 
1
i
n
=
+  and 
the control of the algorithm will be back at Step 2. 
1
1
1
1
1
1
1
1
1
Set 
{
/
}
{
/
}
{
/
}
Since 
{
/
({
/
}
)}
Since 
{
/
}
{
/
}
Since 
 does not occur in 
{
/
}
Definition of composi
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
n
x
x
x
x
x
e
x
e
x
e
x
x
x
x
x
e
x
e
x
e
β
β
β
β
β
β
β
β
β
β
β
β
β
β
β
β
β
β
β
+
+
+
+
+
+
+
+
+
=
−
=
∪
=
∪
=
=
∪
∪
=
∪
=
∪
=
∪
te substitution
 
Thus 
1
1
1
{
/
}
n
n
n
n
n
n
n
n
x
e
θ
θ β
θ
β
θ
β
+
+
+
=
=
=
. 
Hence 
i
i
θ
θ β
=
, for all 
0,1,...,
1
i
n
=
+ . 

86 
Decision-Making Agents 
 
 
By the induction principle, for all 
0
i ≥
, there is a substitution 
iβ  such that 
i
i
θ
θ β
=
 until the algorithm terminates in Step 2, for some i
m
=
. Furthermore, 
m
θ
θ
=
S
 is an mgu for S. 
 
Suppose a subset of the set of all literals occurring in a clause C has the same 
sign (that is, all are negated, or all are not negated) and has an mgu of θ . Then 
Cθ  is called a factor of C. If Cθ  is a unit clause, then it is called a unit factor of 
C. 
Example 
Suppose 
( , )
( ( ), )
( , )
C
P x a
P f y z
Q y z
=
∨
∨
. Then the literals 
( , )
P x a  and 
( ( ), )
P f y z  have the same sign and unify with an mgu 
{ /
( ), / )}
x f y z a
θ =
. Thus 
( ( ), )
( , )
C
p f y a
Q y a
θ =
∨
 
is 
a 
factor 
of 
C. 
If 
( , )
( ( ), )
( ( ), )
C
P x a
P f y z
P f b a
=
∨
∨
, then 
( ( ), )
P f b a  is a unit factor of C. 
 
Suppose C and D are two clauses with no variables in common. Suppose L 
and M are literals occurring in C and D respectively, where the literals L and M 
are complementary to each other and their atoms unify with an mgu of θ . Then 
the clause (
{
})
(
{
})
C
L
D
M
θ
θ
θ
θ
−
∪
−
, where X
Y
−
 means removal of Y from 
the disjunction X, is called a binary resolvent of C and D. The literals L and M 
are called the literals resolved upon and C and D are the parent clauses of the 
resolution operation. 
A resolvent of clauses C and D is a binary resolvent of 
1
C  and 
1
D , where 
1
C  
is either C or a factor of C, and 
1
D  is either D or a factor of D. Hence a binary 
resolvent of two clauses C and D is also a resolvent of C and D. The symbol   
denotes an empty resolvent, which is always unsatisfiable. 
Example 
Suppose 
( , )
( ( ))
( )
C
P x z
Q f a
Q z
=
∨
∨
 and 
( ( ))
( )
D
Q f y
R y
= ¬
∨¬
. Consider L 
and M as 
( ( ))
Q f a
 and 
( ( ))
Q f y
¬
 respectively. Then 
{ / }
y a
θ =
 and 
(
{
})
(
{
})
C
L
D
M
θ
θ
θ
θ
−
∪
−
 
= 
( , )
( )
( )
P x z
Q z
R a
∨
∨¬
. 
The 
resolvent 

 
Classical Logics 
87 
 
( ,
( ))
( )
P x f a
R a
∨¬
 of C and D is a binary resolvent between the factor 
( ,
( ))
( ( ))
P x f a
Q f a
∨
 of C and 
( ( ))
( )
Q f a
R a
¬
∨¬
 of D. 
 
This section first establishes a resolution theorem for ground clauses. It is 
then generalized to establish a resolution theorem for first-order clauses. 
The resolution of a set S of clauses, denoted by 
( )
S
Res
, is the set of all 
clauses consisting of the members of S, together with all the resolvents of all 
pairs of members of S. The n-th resolution of a set S of clauses, denoted by 
( )
n S
Res
, is recursively defined as follows: 
• 
0( ) =
S
S
Res
 
• 
1( )
(
( ))
n
n
+
=
S
S
Res
Res Res
, 
0,1,...
n =
 
It is clear from the above definition that 
0
1
2
( )
( )
( )
...
=
⊆
⊆
⊆
S
S
S
S
Res
Res
Res
. 
Proposition 3-21: If S  is a finite set of ground clauses then not all inclusions in 
the above chain are proper. 
Proof: Straightforward because resolution does not introduce any new literals in 
the case of ground clauses.  
Theorem 3-16: (Ground Resolution Theorem) If S  is any finite set of ground 
clauses then S  is unsatisfiable if and only if 
( )
n S
Res
 contains   (empty 
clause), for some 
0
n ≥
. 
Proof: 
Suppose 
S 
is 
unsatisfiable 
and 
consider 
the 
chain 
0
1
2
( )
( )
( )
...
=
⊆
⊆
⊆
S
S
S
S
Res
Res
Res
. By Proposition 3-21, this chain terminates. 
Let 
( )
n
=
T
S
Res
 be the terminating set. The set T is closed under resolution and 
⊆
S
T . Suppose T does not contain the empty clause  . Suppose 
1,...,
m
A
A  are 
all the atoms occurring in T. Let M be a set constructed as follows: 

88 
Decision-Making Agents 
 
 
0
1
1
1
1
{
},
if there does not exist a clause 
...
 in 
{
,...,
}
, where 
 is the complement of 
{
},
otherwise for 
1,2,..., .
j
j
j
p
c
c
c
p
j
i
i
j
j
A
M
M
M
M
M
M
A
j
m
−
−
= Φ
=
∪
∨
∨
⊆
=
∪¬
=
M
M
M
T
M
M
 
m
=
M
M  
It can now be shown that M satisfies T. If not, there is a clause C in T such 
that the complement of each of the literals occurring in C belongs to 
j
M , for the 
least j, 0
j
m
≤
≤
. Hence 
j
M  is 
1
{
}
j
j
A
−∪¬
M
. Thus C contains 
jA  (by the 
leastness of j). Since 
j
M  is 
1
{
}
j
jA
−∪¬
M
, there exists a clause D in T such that 
the complement of each of the literals occurring in D belongs to 
1
{
}
j
jA
−∪
M
. 
Thus D contains 
j
A
¬
 (by the leastness of j). If the resolvent 
(
{
})
(
{
})
j
j
C
A
D
A
−
∪
−¬
 is non-empty, then the complement of each of the 
literals occurring in this resolvent is in the set 
1
j−
M
. By the definition of 
construction of T, (
{
})
(
{
})
j
j
C
A
D
A
−
∪
−¬
 is a member of T and cannot be 
empty. This contradicts the leastness of j. Hence M satisfies T. Since T is 
satisfiable and 
⊆
S
T , S is also satisfiable, therefore, it is a contradiction. Thus 
the original assumption, that T does not contain empty clause, is false. Hence T 
contains  . 
To prove the converse of the theorem, suppose 
( )
n S
Res
 contains   and 
hence 
( )
n S
Res
 is unsatisfiable. Therefore 
( )
n S
Res
 does not have any model. If C 
and D are two ground clauses, then any model of {C, D} is also a model of 
{ ,
,(
{ })
(
{
})}
c
C D C
L
D
L
−
∪
−
. Therefore any model of S is also a model of 
( )
n S
Res
. Since 
( )
n S
Res
 does not have any model, S does not have any model. 
Therefore S is unsatisfiable. 
 
Proposition 3-22: (Lifting Lemma) If 
'
1
C  and 
'
2
C  are instances of 
1
C  and 
2
C  
respectively and 
'
C  is a resolvent of 
'
1
C  and 
'
2
C , then there is a resolvent C 
of 
1
C  and 
2
C  such that 
'
C  is an instance of C. 

 
Classical Logics 
89 
 
Proof: It can be assumed that there is no common variable between 
1
C  and 
2
C . 
If 
there 
is, 
then 
the 
variables 
are 
renamed 
accordingly. 
Suppose 
'
'
'
'
1
1
2
2
'
(
{
})
(
{
})
p
p
p
p
C
C
L
C
L
θ
θ
θ
θ
=
−
∪
−
, where 
p
θ  is an mgu of the set of atoms 
occurring in 
'
1L  and 
'
2
L  since 
'
i
C  is an instance of 
i
C , 
'
i
i
C
C α
=
, 
1,2
i =
, for 
some substitution α . Let 
1,...,
in
i
i
L
L  be the literals of 
i
C  corresponding to 
'
iL , for 
1,2
i =
, that is, 
1
'
...
in
i
i
i
L
L
L
α
α
=
=
=
. 
When 
1
in > , suppose 
iβ  is an mgu for 
1
{ ,...,
}
in
i
i
L
L
 and let 
1
...
in
i
i
i
i
i
L
L
L
β
β
=
=
=
, 
1,2
i =
.  Then 
iL  is a literal in the factor 
i
i
C β  of 
i
C . When 
1
in = , let 
iβ
ε
=
 and 
'
i
i
L
L
=
. Then 
'
iL  is an instance of 
iL . Since the atoms 
occurring in 
'
1L  and 
'
2
L  are unifiable and 
'
iL  is an instance of 
iL , the atoms 
occurring in 
1L  and 
2
L  are unifiable. Let θ  be an mgu in this case. 
1
2
1
1
1
2
2
2
1
1
1
1
2
1
1
1
2
2
1
2
2
2
1
2
Let 
((
)
{
})
((
)
{
})
(
(
)
{ ...
}(
))
(
(
)
{
...
}(
))
n
n
C
C
L
C
L
C
L
L
C
L
L
β θ
θ
β θ
θ
β β θ
β β θ
β β θ
β β θ
=
−
∪
−
=
−
−
−
 
since 
iβ  does not act on 
k
jL , i
j
≠
, ,
1,2
i j =
, 1
i
k
n
≤
≤
.  
1
2
'
'
'
'
1
1
2
2
1
1
1
1
1
2
2
2
Now 
'
(
{
})
(
{
})
(
(
)
{ ...
}(
))
(
(
)
{
...
}(
))
p
p
p
p
n
n
p
p
p
p
C
C
L
C
L
C
L
L
C
L
L
θ
θ
θ
θ
αθ
αθ
αθ
αθ
=
−
∪
−
=
−
−
−
 
Since 
1
2
β β θ  is more general than 
p
αθ , therefore 
'
C  is an instance of C. Hence 
the lemma. 
 
If S is any set of clauses and P is any set of terms, then the saturation of S 
over P, denoted as 
( )
P S
Ground
, is the set of all ground clauses obtained from 
members of S by replacing variables with members of P. Occurrences of the 
same variable in any one clause are replaced by occurrences of the same term. 
When P is 
( )
S
HU
, then 
( )
P S
Ground
is simply 
( )
S
Ground
. 
Proposition 3-23: Let S  be any set of clauses and P  be any subset of 
( )
S
HU
. 
Then 
(
( ))
(
( ))
⊆
P
P
S
S
Res Ground
Ground
Res
. 
Proof: Suppose 
'
(
( ))
C ∈
P S
Res Ground
. Then there are two clauses 
1
C  and 
2
C  in 
S and their ground instances 
1
1
C α  and 
2
2
C α  such that 
'
C  is a resolvent of 
1
1
C α  
and 
2
2
C α . Then, by the lifting lemma (Proposition 3-22), 
'
C  is an instance of C, 

90 
Decision-Making Agents 
 
 
where C is a resolvent of 
1
C  and 
2
C . Since C is a member of 
( )
S
Res
 and 
'
C  is a 
ground 
instance 
of 
C, 
'
(
( ))
C ∈
P
S
Ground
Res
. 
Hence 
(
( ))
(
( ))
⊆
P
P
S
S
Res Ground
Ground
Res
. 
Example 
{ ( )
( ),
( )}
{ }
( )
{ , }
( )
{ ( )
( ),
( ), ( )}
(
( ))
{ ( )
( ),
( ), ( )}
( )
{ ( )
( ),
( )}
(
( ))
{ ( )
( ),
( )}
(
( ))
(
( ))
(
P a
Q x Q b
a
a b
P a
Q x Q b P a
P a
Q a Q b P a
P a
Q a Q b
P a
Q a Q b
=
∨¬
=
=
=
∨¬
=
∨¬
=
∨¬
=
∨¬
⊆
P
P
P
P
P
S
P
S
S
S
S
S
S
S
HU
Res
Ground
Res
Ground
Res Ground
Res Ground
Ground
Res
Res Gr
( ))
(
( ))
≠
P
P
S
S
ound
Ground
Res
 
Hence the converse of Proposition 3-23 is not necessarily true.  
Proposition 3-24: If S  is any set of clauses and P  is any subset of 
( )
S
HU
, then 
(
( ))
(
( ))
i
i
⊆
P
P
S
S
Res Ground
Ground
Res
, for all 
0
i ≥
. 
Proof: This proposition is established by induction on i: 
Base Step: 
0
i =
. Then 
0
0
(
( ))
( )
(
( ))
=
=
P
P
P
S
S
S
Res Ground
Ground
Ground
Res
. 
Induction Step:  Suppose the result is true for i
n
=
. Then: 
1
1
(
( ))
(
(
( )))
(
(
( ))),
for 
(
(
( ))),
Lemma @lemma rtp-res@
=
(
( )),
By definition
n
n
n
n
n
i
n
+
+
=
⊆
=
⊆
P
P
P
P
P
S
S
S
S
S
Res
Ground
Res Res Ground
Res Ground
Res
Ground
Res Res
Ground
Res
 
Hence the proposition. 
 
The basic version of Herbrand’s theorem (Theorem 3-14) can be restated as 
follows. 

 
Classical Logics 
91 
 
Theorem 3-17: If S is any finite set of clauses, then S  is unsatisfiable if and only 
if for some finite subset P  of 
( )
S
HU
, 
( )
P S
Ground
 is unsatisfiable. 
Theorem 3-18: If S  is any finite set of clauses, then S  is unsatisfiable if and 
only if for some finite subset 
P  of 
( )
S
HU
 and some 
0
n ≥
, 
(
( ))
n
P S
Res
Ground
 contains  . 
Proof: Since S and P are finite, 
( )
P S
Ground
 is also finite. Hence the theorem 
follows from Theorem 3-16 and Theorem 3-17.  
Theorem 3-19: If S  is any finite set of clauses, then S  is unsatisfiable if and 
only if for some finite subset 
P  of 
( )
S
HU
and some 
0
n ≥
, 
(
( ))
n
P
S
Ground
Res
 contains  . 
Proof: Follows from Theorem 3-18 and Proposition 3-24.  
Theorem 3-20:  (Resolution Theorem) If S  is any finite set of clauses then S  is 
unsatisfiable if and only if 
( )
n S
Res
 contains  , for some 
0
n ≥
. 
Proof: The set 
(
( ))
n
P
S
Ground
Res
 will contain   if and only if 
( )
n S
Res
 contains 
  (since the replacement of variables by terms cannot produce   from a non-
empty clause in 
( )
n S
Res
). Hence the theorem follows from Theorem 3-19 above.  
 
3.5 
Refutation Procedure 
This section provides a procedure based on the above resolution theorem 
(Theorem 3-20) to derive the empty clause from an unsatisfiable set of clauses. 
Later in this section some techniques are provided to improve the efficiency of 
the procedure. 
Let S be a set of clauses (called input clauses). A derivation  (or deduction) 
in S is a sequence of clauses 
1
2
,
,...
C C
such that each 
i
C  is either in S or is a 
resolvent of 
j
C  and 
k
C , where 1
,1
,
i
n
j k
i
≤≤
≤
≤. In the latter case, 
i
C  is a 
derived clause. A derivation is either finite or infinite according to the length of 
its sequence. 

92 
Decision-Making Agents 
 
 
A refutation of S is a finite derivation 
1,...,
n
C
C  in S such that 
n
C =. 
The following theorem, which follows immediately from the resolution 
theorem, is the completeness theorem of a system of logic whose sole inference 
rule is the resolution principle stated in Section 3.4. 
Theorem 3-21: A finite set S  of clauses is unsatisfiable if and only if there is a 
refutation of S . 
One of the ways to find a refutation from a set of unsatisfiable clauses is to 
compute the sequence 
1
2
,
( ),
( ),...
S
S
S
Res
Res
 until 
( )
n S
Res
 contains  . However, 
this procedure would be very inefficient. The following example demonstrates 
this inefficiency. 
Example 
S = { 
Clause 1: 
( )
P a
¬
 
 
Given 
Clause 2: 
( )
( )
P x
Q x
∨¬
 
Given 
Clause 3: 
( )
( ( ))
P x
R f x
∨¬
 
Given 
Clause 4: 
( )
( ( ))
Q a
R f a
∨
 
Given 
Clause 5: 
( )
R b   
 
Given 
Clause 6: 
( )
( )
R c
R c
∨¬
  
Given 
} 
Clause 7: 
( )
{
=
∪
S
S
Res
 
Clause 8: 
( )
Q a
¬
 
 
1 and 2 
Clause 9: 
( ( ))
R f a
¬
 
 
1 and 3 
Clause 9: 
( )
( ( ))
P a
R f a
∨
 
2 and 4 
Clause 10: 
( )
( )
P a
Q a
∨
  
3 and 4 
} 

 
Classical Logics 
93 
 
2( )
( )
{
=
∪
S
S
Res
Res
 
Clause 11: 
( ( ))
R f a
 
 
1 and 9 
Clause 12: 
( )
Q a  
 
1 and 10 
Clause 13: 
( )
P a  
 
2 and 10 
Clause 14: 
( )
P a  
 
3 and 8 
Clause 15: 
( )
P a  
 
3 and 9 
Clause 16: 
( ( ))
R f a
 
 
4 and 7 
Clause 17: 
( )
Q a  
 
4 and 8 
} 
3
2
( )
( )
{
=
∪
S
S
Res
Res
 
Clause 18:    
 
1 and 13 
Clause 19: … 
} 
 
Many irrelevant, redundant clauses have been generated in the above 
example. Only the clauses 10 and 13 need to be generated to show that the set of 
clauses is unsatisfiable. The other clauses are redundant. Clause 6 is a tautology, 
which is true in any interpretation. Therefore if a tautology is deleted from an 
unsatisfiable set of clauses, the remaining set of clauses must still be 
unsatisfiable. Clause 
( )
P a  is redundantly generated three times. Satisfiability of 
the set S will not depend on clause 5 as there is no clause in 
( )
n S
Res
 with an 
occurrence of a literal 
A
¬  such that A unifies 
( )
R b . Hence in this example, the 
given set S of clauses is satisfiable if and only if the set of clauses in S other than 
clause 5 is satisfiable. 
To avoid the inefficiencies that would be caused for the above reasons, the 
refutation procedure incorporates a number of search principles.  
• 
Purity Principle:  Let S be any finite set of clauses. A literal L occurring 
in a clause C in S is said to be pure if there is no clause in D for which a 
resolvent (
)
(
)
C
L
D
M
θ
θ
θ
θ
−
∪
−
 exists. The purity principle is then 
stated as follows: From a finite set S of clauses any clause C containing a 

94 
Decision-Making Agents 
 
 
pure literal can be deleted from S. Then S is satisfiable if and only if the 
resulting set 
{ }
C
−
S
 is satisfiable. 
• 
Tautology Principle:  From a finite set S of clauses, any clause C which 
is a tautology can be deleted. Then S is satisfiable if and only if the 
remaining set of clauses 
{ }
C
−
S
 is satisfiable. 
• 
Subsumption Principle:  A clause C subsumes a clause D (or D is 
subsumed by C), where C
D
≠
, if there exists a substitution θ  such that 
all the literals that occur in Cθ  also occur in D. The subsumption 
principle is then stated as follows:  From a finite set S of clauses, any 
clause D which is subsumed by a clause in 
{ }
D
−
S
 can be deleted. Then 
S is satisfiable if and only if the resulting set 
{ }
D
−
S
 is satisfiable. 
• 
Replacement Principle:  The replacement principle can be stated as 
follows: Suppose C and D are two clauses and R is a resolvent of C and 
D which subsumes one of C and D. Then in adding R by the resolution 
principle, one of C and D which R subsumes can be simultaneously 
deleted. 
The above search principles can be used to delete some of the redundant and 
irrelevant clauses generated during the refutation procedure. In spite of this, there 
are still many irrelevant clauses which cannot be deleted that are generated 
during the refutation procedure. Refinement of resolution is necessary to achieve 
an efficient theorem proving procedure. Many refinements of the resolution 
principles (Chang, 1970) have been proposed. 
For example, input resolution requires that one of the parent clauses of each 
resolution operation must be an input clause, that is, not a derived clause. Unit 
resolution requires that at least one of the parent clauses or its factor in each 
resolution operation be a unit clause. Both unit and input resolutions are 
incomplete in general. Linear resolution is an extension of input resolution in 
which at least one of the parent clauses to each resolution operation must be 
either an input clause or an ancestor clause of the parent. Linear resolution is 
complete. Linear resolution with selection function (or SL resolution) (Kowalski 
and Kuehner, 1971) is a restricted form of linear resolution. The main restriction 
is effected by a selection function which chooses from each clause one single 
literal to be resolved upon in that clause. In the next chapter, we will make use of 
linear resolution for inferencing in logic programs. 

 
Classical Logics 
95 
 
3.6 
Complexity Analysis 
Recall that a propositional formula F is satisfiable if there is at least one 
interpretation I which evaluates the formula F as F (true). If I satisfies F then I is 
a model of F. This model checking problem of a formula under an interpretation 
can be done in polynomial time. On the other hand, the satisfiability problem 
(that is, to check if there is an interpretation which is a model) is NP-complete. 
The corresponding Co-NP problem is testing if F is a logical consequence of a 
set Γ  of formulae (i.e.
F
Γ B
). The best algorithms for solving NP-complete or 
Co-NP-complete problems require exponential time in the worst case. 
Before studying the complexity of satisfiability and logical consequence 
within the domain of first-order logic, we study first the complexity within an 
intermediate syntax, called Quantified Boolean Formulae (QBF), which are a 
generalization of propositional formulae. QBF characterize the complexity class 
PSPACE, and the complexity results of some systems of propositional and first-
order logics can be established by comparing them to QBF. 
A QBF is of the form 
1
1
1
...
(
,...,
)
n
n
n
Q X
Q X F X
X
 
where F is a propositional formula involving the propositional boolean variables 
1,...,
n
X
X  and each 
i
Q  is an existential (∃) or universal quantifier (∀). The 
expression 
i
X F
∃
 is read as “there exists a truth assignment to 
i
X  such that F is 
true,” and the expression 
i
X F
∀
 is read as “for every truth assignment to 
i
X , F 
is true.” Following is an example of a true QBF: 
1
2
1
2
1
2
((
)
(
))
X
X
X
X
X
X
∀
∃
∨
∧¬
∨¬
 
On the other hand, 
1
2
1
2
1
2
((
)
(
))
X
X
X
X
X
X
∃
∀
∨
∧¬
∨¬
 is false. The evaluation 
problem for a QBF is to decide whether it is true or not. The propositional 
satisfiability problem for a formula F coincides with the evaluation problem for 
1...
n
X
X F
∃
∃
, where 
1,...,
n
X
X  are the propositional variables occurring in F, 
which is NP-complete. The logical consequence problem for a formula F from a 
set of formulae Γ  coincides with the evaluation problem 
1...
(
)
n
X
X
F
∀
∀
Γ →
, 
where 
1,...,
n
X
X  are the propositional variables occurring in Γ  and F, which is 
Co-NP-complete. In general, the evaluation problem for a QBF is PSPACE-
complete. This is because an algorithm can be devised easily that cycles through 
all possible assignments to the free variables, and substitutions for the quantified 
variables. In each cycle one needs to store polynomially many values. As the 

96 
Decision-Making Agents 
 
 
values in each cycle are independent of the previous cycle, we can overwrite 
them to stay in PSPACE. 
Model checking for first-order logic is PSPACE-complete (Stockmeyer 
1974, Vardi 1982).   Given a finite first-order structure and a first-order formula, 
just implement an algorithm that cycles through all possible assignments to the 
free variables, and cycles through possible substitutions for the quantified 
variables. In each cycle we only need to store polynomially many values. As the 
values in each cycle are independent of the previous one, we can “overwrite” 
them, and stay in PSPACE. 
The proof is a reduction from the Quantified Boolean Formula (QBF) 
satisfiability problem. Essentially, evaluating a QBF is shown to be equivalent to 
model-checking a structure with just two distinguishable elements representing 
the Boolean values F  (true) or ⊥ (false). Thus, the complexity of model 
checking in first-order logic comes purely from large and complicated input 
formulas. 
3.7 
Further Readings 
There are dozens of good text books on classical logics, of which three excellent 
ones are (Copi, 1979), (Mendelson, 1987), and (Stoll, 1963). Robinson’s paper 
(1965) is highly recommended as a foundational reading. Chang and Lee (1973) 
provide a thorough coverage of automated theorem proving. For a more recent 
account on automated theorem proving, see Gallier (1987 & 2003). 

97 
Chapter 4 
 
Logic Programming 
 
 
 
 
This chapter presents the theory and practice of logic programming, a declarative 
programming paradigm, as opposed to procedural programming such as Java and 
C++. A logic program is just a collection of logical formulae represented in the 
form of definite if-then rules. A logic program can be regarded as a propositional 
epistemic model of an agent. Reasoning with such epistemic models, which are 
represented in a restricted first-order language syntax, not only guarantees 
decidability and avoids the high complexity involved in theorem proving, but 
also provides a natural way of expressing an agent’s day-to-day decision-making 
knowledge as arguments. 
Program clauses that constitute a logic program are classified according to 
their syntactic structures. Several categories of logic programs are presented here 
along with their models and proof-theoretic semantics. Then we describe 
specialized resolution theorem proving schemes for these categories, namely 
SLD resolution for implementing backward chaining. Finally, we present the 
logic programming language Prolog, which can be effectively used to implement 
the logical and modal epistemic models via  “meta-interpreters.” 
4.1 
The Concept 
Kowalski (1979b) represented the analysis of an algorithm by the equation 
Algorithm
Logic
Control
=
+
 
that is, given an algorithm for solving a particular problem, the logic component 
specifies the knowledge which can be used in solving the problem and the 
control component determines the way this knowledge can be used to arrive at a 
solution to the problem. To clarify the concepts of logic and control components, 
consider the following definition of factorial, which is specified using standard 
mathematical symbols: 

98 
Decision-Making Agents 
 
 
( )
1
when
0
(
1)
when
0
Fact n
n
n
Fact n
n
=
=
=
×
−
>
 
This symbolized definition of factorial constitutes the logic component of the 
following procedural algorithm for computing a factorial: 
Input: 
n 
Step 1: 
1
Fact =  
Step 2: 
If 
0
n =
 then return Fact 
Step 3: 
Fact
Fact
n
=
×
; set 
1
n
n
=
−; go to Step 2 
The control component of the above algorithm is embedded within it in the form 
of branching operators such as “return” and “go to”. Unlike procedural 
programming, in a logic programming system, a programmer specifies only the 
logic component of an algorithm, called the logic program, using the notion of 
mathematical logic. For example, the definition of factorial as the logic 
component in a logic programming system is expressed by a well-formed 
formula in the first-order logic as 
1
2
1
2
1
1
2
1
2
2
1
2
2
(0,1)
(
( ,
)
1
(
,
))
Fact
x
x
y
y
Fact x y
x
x
y
x
y
Fact x
y
∧∀∀∀∀
∧
=
+ ∧
=
×
→
 
or, equivalently, as a set of two definite clauses as 
(0,1)
Fact
 
2
2
1
1
2
1
2
2
1
(
,
)
(
,
)
1
Fact x
y
Fact x y
x
x
y
x
y
←
∧
=
+ ∧
=
×
 
where the interpretation of 
( , )
Fact x y  is that number y is the factorial of number 
x and the other symbols are given their usual meaning ( ← is the same as 
implication). 
The control component in a logic programming system can either be 
expressed by the programmer through a separate control-specifying language or 
can be determined entirely by the logic programming system itself. The 
efficiency of an algorithm can be increased by tuning its control component 
without changing its logic component. Although a logic program lacks sufficient 
procedural information for the system to do anything other than make an 
exhaustive search to find the correct execution path, separating the logic 
component from the control component has several advantages, including 
advantages pertaining to the correctness of the algorithm, program improvement, 
and modification. In a logic programming system, the logic component of an 
algorithm is specified in mathematical logic. More generally, in a declarative 

 
Logic Programming 
99 
 
programming environment, the programmer specifies the logic component of an 
algorithm in a standard symbolized manner. 
4.2 
Program Clauses and Goals 
A non-empty clause can be rewritten as 
1
1
...
...
,
1
p
q
M
M
N
N
p
q
∨
∨
∨¬
∨
∨¬
+
≥ 
where 
i
M s and 
j
N s are positive literals, and variables are implicitly, universally 
quantified over the whole disjunction. Every closed wff can be transformed into 
this clausal form. The above clause can be written in the form of a program 
clause as 
1
1
1
...
...
...
,
1,
0,
0
k
q
k
p
M
M
N
N
M
M
k
p
q
+
∨
∨
←
∧
∧
∧¬
∧¬
≥
≥
≥
 
or, in the form of a goal as 
1
1
...
...
,
1
q
p
N
N
M
M
p
q
←
∧
∧
∧¬
∨
∨¬
+
≥ 
Thus, a program clause (or simply clause) is a formula of the form 
1
1
...
...
,
1,
0
m
n
A
A
L
L
m
n
∨
∨
←
∧
∧
≥
≥
 
where 
1
...
m
A
A
∨
∨
 is the head (or conclusion or consequent) of the program 
clause and 
1
...
n
L
L
∧
∧
 is the body (or condition or antecedent) . Each 
iA  is an 
atom and each 
jL  is either an atom (a positive condition) or a negated atom (a 
negative condition).  Any variables in 
1
1
,...,
,
,...,
m
n
A
A
L
L  are assumed to be 
universally quantified over the whole formula. Each 
iA  (respectively, 
jL ) is said 
to have occurred in the head (respectively, body) of the clause. The various 
forms of the clause 
1
1
...
...
m
n
A
A
L
L
∨
∨
←
∧
∧
 with values of m and n of interest 
are the following: 
1. 
1,
0
m
n
=
=
. That is, the clause has the form 
A ← (or simply A) 
in which the body is empty and the head is a single atom. This clause is 
called a unit clause or unit assertion.  
2. 
1,
0
m
n
=
≥
, and each 
iL  is an atom. That is, the clause has the form 
1
...
n
A
B
B
←
∧
∧
 
in which A and 
iB s are atoms. This clause is called a definite clause.  

100 
Decision-Making Agents 
 
 
3. 
1,
0
m
n
=
≥
. That is, the clause has the form 
1
...
n
A
L
L
←
∧
∧
 
in which A is an atom and 
iL s are literals. This is a normal clause (or 
simply clause).  
4. 
0,
1
m
n
=
≥, and each 
iL  is an atom. That is, the clause has the form 
1
...
n
B
B
←
∧
∧
 
in which 
iB s are atoms. This clause is called a definite goal.  
5. 
0,
1
m
n
=
≥. That is, the clause has the form 
1
...
n
L
L
←
∧
∧
 
in which 
iL s are literals. This clause is a normal goal (or simply goal).  
6. 
1
1
...
...
,
1,
0
m
n
A
A
L
L
m
n
∨
∨
←
∧
∧
>
≥
. This clause is called an indefinite 
clause.  
Thus a unit clause is a special form of a definite clause; a definite clause is a 
special form of a normal clause; and a definite goal is a special form of a normal 
goal. The following example contains various types of clauses involved with the 
problem of representing numbers. 
Example 
Clause  
 
 
 
 
 
 
Type 
C1.  
(
,
)
Weather EdenGarden Wet  
 
 
 
Unit 
C2.  
( , )
( , )
( , )
Field x y
Weather x z
Causes z y
←
∧
  
          Definite 
C3.  
( )
( , )
( ,
)
Game x
Weather x y
Disruption y Transport
←
∧¬
 
Normal 
C4.  
( )
( ,
)
( , )
Game x
Disruption y Transport
Weather x y
∨
←
 
Indefinite 
Clause C1 represents that the weather condition is wet in the Eden Garden 
area. Clause C2 represents that if the weather condition is z in the area of x and z 
causes the condition y of a field, then the condition of the field x is y. Clause C3 
represents that if the weather condition in the area of the field x is y and the 
condition y does not disrupt transport, the game holds at a field x. Clause C4 
represents that if the weather condition in the area of the field x is y then game 
holds at x or the condition y disrupts transport. Although clauses C3 and C4 are 
logically equivalent, the semantics of a logic program, which includes clause C3, 

 
Logic Programming 
101 
 
may be different if the program replaces C3 by C4. In such cases, the use of the 
negation symbol ¬ becomes a default rule which may be different from the 
classical negation studied so far. But it is always clear from the context which 
particular approach has been taken. 
 
The rest of the chapter does not deal with indefinite clauses with consequent 
1
...
m
A
A
∨
∨
 with 
1
m > . The main argument against such considerations is not 
only to avoid reasoning complexity but also the coarse level of uncertainty that 
an indefinite clause can represent via its disjunction sign will be subsumed by a 
more general modeling technique (to be presented in the penultimate chapter) 
that attaches probabilities to clauses as their degrees of uncertainty. 
A subgoal of a goal G is a literal occurring in G. A definite program is a 
finite set of definite clauses. A normal program (or simply program) is a finite 
set of normal clauses. A definite program is a special form of a normal program 
consists only definite clauses. In writing a set of clauses to represent a program, 
the same variables may be used in two different clauses. If there are any common 
variables between two different clauses then they are renamed before they are 
used. 
Example 
Program: 
C1.  
( )
( , )
( ,
)
Game x
Weather x y
Disruption y Transport
←
∧¬
 
C2.  
( , )
( , )
( , )
Field x y
Weather x z
Causes z y
←
∧
 
C3.  
( )
( )
Commentary x
Game x
←
 
C4.  
(
,
)
Weather EdenGarden Rainy  
C5.  
(
, )
Disruption Snow x  
C6.  
(
,
)
Disruption Rain Transport  
The clause C1 makes the above program normal. In this program, the non-ground 
atom 
(
, )
Disruption Snow x  represents that the snowy weather disrupts everything. 
In the context of the program, you can have a definite goal 
(
)
Commentary EdenGarden
←
 

102 
Decision-Making Agents 
 
 
to find whether the radio commentary is being played from Eden Garden or not. 
Similarly a normal goal of the form 
( ,
),
( )
Field x Wet
Game x
←
¬
 
will find those wet fields where games are not being played. 
 
If 
1,...,
p
x
x  are all the free variables in a normal goal of the form 
1
...
n
L
L
←
∧
∧
, then the goal in the context of a program P is interpreted as a 
request for a constructive proof for the formula 
1
1
...
(
...
)
p
n
x
x
L
L
∃
∃
∧
∧
 
This means that one should find a substitution θ  for the free variables in the goal 
such that 
1
(
...
)
n
L
L θ
∧
∧
 is true according to the semantics of the program. 
Programs have so far been classified by looking at the structure of the 
individual clauses. Further subclassifications of programs can be achieved by 
looking at their constituent set of clauses as a whole. This classification can be 
facilitated by the use of dependency graphs.  
Consider the class of normal programs. The dependency graph of a normal 
program P has a node for each predicate symbol occurring in P and a directed 
edge from the node for predicate Q to the node for predicate P whenever 
predicate Q is in the body of some clause and P is in the head of the same clause. 
An edge from node Q to node P is positive iff there is a clause C in P in which P 
is in the head of C, and Q is the predicate symbol of a positive literal in the body 
of C. The edge is negative if Q is the predicate symbol of a negative literal in the 
body of C. The length of a cycle in a dependency graph is the number of edges 
occurring in the cycle. 
Example 
Consider the following program: 
( )
( )
( )
( )
( )
( )
( , )
( )
( )
( , )
P x
Q x
R x
R x
S x
S x
T x y
P y
Q a
T a b
←
∧¬
←
←
∧
 

 
Logic Programming 
103 
 
The dependency graph for the above program is shown in Figure 4-1. The set of 
clauses in this example contains nodes P, Q, R, S and T. The edges from Q to P, 
from S to R, from T to S and from P to S are positive while the edge from R to P 
is negative. 
P
Q
R
S
T
+
–
+
+
+
 
Figure 4-1: Example dependency graph 
 
If the dependency graph for a normal program does not contain any cycles 
then the program is hierarchical.  
Having cycles in the dependency graph of a program P implies that some 
predicates of P are directly or indirectly defined by themselves. This, of course, 
complicates the structure of a program and goal evaluations in the context of that 
program. This class of programs is formally classified through the following 
definitions. 
Let P be a normal program and Γ be the dependency graph for P. A predicate 
P occurring in P is recursive if it occurs in a cycle of Γ. Two predicates P and Q 
are mutually recursive if they both occur in a cycle of Γ. If the dependency graph 
for a program contains a cycle, then the program is recursive.  
Example 
Consider a program P that contains the following clause: 
( , )
( , )
( , )
Field x y
Adjacent x z
Field z y
←
∧
 

104 
Decision-Making Agents 
 
 
The clause states that the condition of a field x is y provided there exists a field z 
which is adjacent to x and the condition of z is y. The clause therefore helps to 
determine the conditions of a field in terms of its adjacent fields. The program P 
is recursive and the predicate Field in the program is recursive. 
 
Programs allow negated atoms in their bodies. When this happens, one 
should be careful about the meaning of the program. Consider, for example, a 
program P that contains the following two clauses: 
( )
( )
( )
( )
Field Open x
Field Closed x
Field Closed x
Field Open x
←¬
←¬
. 
The dependency graph of the above program contains a cycle with negative 
edges, that is, a recursion via negation (so does the program presented earlier 
whose dependency graph is shown in Figure 4-1). The completion of the program 
P, which is a popular way of defining the semantics of a program, becomes 
inconsistent. This kind of situation can be prevented by not allowing a recursion 
via negation. 
A normal program is stratified  (Chandra and Harel, 1985; Apt et al., 1988) if 
each cycle of the dependency graph is formed by positive edges only (the 
remainder of the graph may contain negative edges). 
Clearly every hierarchical program can be regarded as a specific stratified 
program and all definite programs are stratified. An alternative definition of 
stratified programs can be given by introducing the notions of stratification 
among the definitions of predicate symbols of programs. 
Suppose P is a program. The definition of a predicate symbol P is the subset 
of P consisting of all clauses with P occurring in their heads. Then P is regarded 
as stratified if there is a partition 
1
...
n
=
∪
∪
P
P
P  
such that the following two conditions hold for 
1,...,
i
n
=
: 
• 
If an atom A occurs positively in the body (that is, A occurs in the body) 
of a clause in 
iP  then the definition of its predicate symbol is contained 
within 
j
P  with j
i
≤. 

 
Logic Programming 
105 
 
• 
If an atom A occurs negatively in the body (that is, 
A
¬  occurs in the 
body) of a clause in 
iP  then the definition of its predicate symbol is 
contained within 
j
P  with j
i
< . 
The program P is said to be stratified by 
1
...
n
∪
∪
P
P  and each 
iP  is called a 
stratum of P. The level of a predicate symbol is the index of the strata within 
which it is defined. 
A third alternative equivalent definition of stratified programs in terms of its 
level mapping, which is a mapping from its set of predicates to the set of non-
negative integers defined as follows: 
• 
The level of a literal is the level of the predicate symbol of the literal. 
• 
The level of a clause is the maximum level of any predicate symbol in 
the clause. 
A program is stratified if it has a level mapping such that, for every clause C, the 
level of the predicate symbol of every positive condition in C is less than or equal 
to the level of the predicate symbol in the head of C and the level of the predicate 
of every negated condition in C is less than the level of the predicate symbol in 
the head of C. 
Example 
Consider the following program: 
1:
( )
( )
( )
2:
( )
( )
3:
( )
( , )
( )
4:
( )
( )
5:
( )
6:
( )
7:
( , )
C
P x
Q x
R x
C
R x
S x
C
S x
T x y
U y
C
S x
U x
C
Q a
C
R a
C
T a b
←
∧¬
←
←
∧¬
←
 
The above program is stratified by 
1
2
3
∪
∪
P
P
P , where 
3
1
{
}
C
=
P
, 
2
2
3
4
6
{
,
,
,
}
C C C C
=
P
 and 
1
5
7
{
,
}
C C
=
P
. An alternative way the program can be 
stratified 
is 
by 
1
2
3
4
∪
∪
∪
P
P
P
P , 
where 
4
1
{
}
C
=
P
, 
3
2
6
{
,
}
C C
=
P
, 
3
3
4
7
{
,
,
}
C C C
=
P
 and 
1
5
{
}
C
=
P
. 
 

106 
Decision-Making Agents 
 
 
4.3 
Program Semantics 
The semantics of a program (van Emden and Kowalski, 1976) deals with its 
meaning. Three methods for defining program semantics are introduced in this 
section: declarative, procedural, and fixpoint. 
• 
The declarative semantics of a program deals with the interpretation of 
the program’s language, including logical implications, and truth. A 
program’s declarative semantics can be defined by selecting one or more 
of its models. These models determine which substitution of a given goal 
is correct in the context of the program. 
• 
The procedural semantics (or operational semantics) of a program deals 
with well-formed formulae and their syntax, axioms, rules of inference, 
and proofs within the program’s language. This semantics defines the 
input/output relations computed by a program in terms of the individual 
operations evoked by the program inside the machine. Procedural 
semantics refers to a computational method, called proof procedure, for 
obtaining the meaning of a program. 
• 
The fixpoint semantics of a program defines the meaning of a program to 
be the input/output relation which is the minimal fixpoint of 
transformation associated with the program P. The fixpoint operator 
builds the intended model of the program in a step-by-step process. 
Van Emden and Kowalski first investigated the semantics of Horn logic as a 
programming language (van Emden and Kowalski, 1976) and compared the 
declarative semantics of a program with the classical model-theoretic semantics, 
compared operational semantics with classical proof-theoretic semantics, and 
compared fixpoint semantics with the fixpoint operator corresponding to a 
program. 
All logical consequences of a program can be considered as positive 
information. Declarative, procedural, and fixpoint semantics are sometimes 
concerned only with the positive information about a program that can be derived 
from that program. The negative information is not explicitly stored in a program 
and it is assumed by default. This additional implicit negative information cannot 
be proved from the program by normal inference rules such as modus ponens. 
Rules for inferring negative information are rules that infer this additional 
implicit negative information. Hence a rule for inferring negative information 
defines the semantics for negative informations in the program and is given in 

 
Logic Programming 
107 
 
either a declarative or a procedural manner. This rule will be studied alongside 
the other three kinds of semantics mentioned above. 
Example 
Consider the following program P: 
( )
( )
(
)
(
,
)
Commentary x
Game x
Game EdenGarden
Field Fenway Park Wet
←
 
Several different semantics for the above program can be defined as follows: 
• 
Declarative semantics: Facts that are in a minimal model of P are taken 
as true. 
• 
Procedural semantics: Facts that can be derived from P are taken as true. 
• 
Semantics for negative information: Facts that are in 
( )
P
HB
 and not in 
at least one minimal model of P are taken as false. 
Then the declarative and procedural semantics coincide and are represented by 
the set 
{
(
),
(
),
(
,
)}
Game EdenGarden Commentary EdenGarden
Field Fenway Park Wet
 
According to these two semantics, the above set is the set of all true facts in the 
context of P. The semantics for negative information generates the set 
( )
{
(
),
(
),
(
,
)}
Game EdenGarden Commentary EdenGarden
Field Fenway Park Wet
−
P
HB
 
and therefore, 
(
)
Game Fenway Park  is taken as false in the context of P 
according to the semantics for negative information. 
 
Let R be a rule for inferring negative information for a particular class (such 
as definite or stratified or normal) of programs and P be a program from that 
class. Then 
( )
P
R
 will denote the set of negative facts that can be inferred from P 
by the application of R. The following properties (Ross and Topor, 1988) are 
expected to be satisfied in a typical program P of this class: 
• 
Concise: Use of R should enable a useful reduction in the amount of 
negative information that would otherwise have to be stored as clauses of 

108 
Decision-Making Agents 
 
 
P. In other words, the set of facts 
( )
P
R
 is relatively large compared to 
P. This reflects the property that in a typical program the number of facts 
that are true is much less than the whole Herbrand base associated with 
the database. In the context of the above example program P, the number 
of true facts is 3, compared to 15 which is the size of 
( )
P
HB
. 
• 
Efficient: It should be relatively easy to determine whether R can infer an 
item of negative information from P. In other words, the decision 
procedure of 
( )
P
R
 should be relatively efficient. 
• 
Consistent: If the program P is consistent, then 
( )
∪
P
P
R
 should be 
consistent. 
Detailed studies of declarative and procedural semantics on individual 
classes of logic programs are carried out in the next two subsections. 
4.4 
Definite Programs 
This section deals with the semantics of definite programs. Declarative semantics 
is studied by employing classical model-theoretic semantics and procedural 
semantics using the SLD-resolution scheme. Unless otherwise stated, in this 
section the terms “program” and “goal” will always mean “definite program” and 
“definite goal,” respectively. Recall that a definite program is defined as a finite 
set of definite clauses of the form
1
...
m
A
A
A
←
∧
∧
 and a definite goal is defined 
as 
1
...
n
B
B
←
∧
∧
, where 
0,
1
m
n
≥
≥, and A, 
iA  and 
j
B  are atoms. 
The only declarative semantics of a definite program studied here is the one 
given in van Emden and Kowalski (1976), which is formally defined as follows. 
Let P be a definite program. Then the declarative semantics of P, denoted as 
( )
P
M
, is defined as 
( )
{ |
( ) and 
}
A A
A
=
∈
P
P
P B
M
HB
 
which means that 
( )
P
M
 is the set of all ground atoms of the Herbrand base of P 
that are logical consequences of P. The following theorem can be considered as 
an alternative definition of the semantics in terms of Herbrand models of P. 
Theorem 4-1: Let P  be a program and 
( )
P
HM
 be the set of all Herbrand 
models of P . Then 
( )
{ |
( )}
A A
=
∈∩
P
P
M
HM
. 

 
Logic Programming 
109 
 
Proof:  
( )
A∈
P
M
 
Iff 
A
P B
 
iff 
{
}
A
∪¬
P
 has no model 
iff 
{
}
A
∪¬
P
 has no Herbrand model 
iff 
A
¬  is false in all Herbrand models of P 
iff A is true in all Herbrand models of P 
iff 
( )
A∈∩
P
HM
.  
 
The following theorem establishes the model intersection property of a set of 
Horn clauses. 
Theorem 4-2: Suppose P  is a consistent set of Horn clauses and 
( )
P
hm
 is a 
non-empty set of Herbrand models of P . Then 
( )
∩
P
hm
 is also a model of 
P . 
Proof: If possible, do not let 
( )
∩
P
hm
 be a Herbrand model of P. Then there is a 
clause C in P and a ground instance θ  of C such that Cθ  is false in 
( )
∩
P
hm
. If 
C has the form 
1
...
n
A
A
A
←
∧
∧
, then 
( )
Aθ ∉∩
P
hm
 and 
1 ,...,
( )
n
A
A
θ
θ ∈∩
P
hm
. 
Therefore for some 
( )
∈
M
P
hm
, Aθ ∉M  and 
1 ,...,
n
A
A
θ
θ ∈M . Hence C is false 
in M, which contradicts the assumption that M is a model of P. The other case, 
when C has the form of a negative Horn clause is similar. 
 
In view of Theorem 4-2, it can be said that 
( )
∩
P
HM
 is also a Herbrand 
model and hence the minimal model of P. Thus the declarative semantics of a 
definite program is equal to its minimal model. 
Example 
Consider the following program P: 

110 
Decision-Making Agents 
 
 
( , )
( , )
( , )
( , )
( , )
( , )
( , )
P x y
Q x y
P x y
Q x z
P z y
Q a b
Q b c
←
←
∧
 
Then 
( )
{ ( , ),
( , ), ( , ), ( , ), ( , )}
Q a b Q b c P a b P b c P a c
=
P
M
. Any other model of P, for 
example { ( , ),
( , ), ( , ), ( , ), ( , ), ( , ), ( , )}
Q a b Q b c P a b P b c P a c P b a P a a
, contains P. 
 
The procedural semantics of a definite program is studied in this section 
using the SLD resolution (SL resolution for Definite clauses) schemes.   First, the 
resolution scheme and its properties are considered. 
Let P be a definite program and G be a goal. An SLD derivation of 
{ }
G
∪
P
 
consists of a sequence 
0
1
,
,....
G
G G
=
 of goals, a sequence 
1
2
,
,...
C C
 of variants of 
clauses in P (called the input clauses of the derivation) and a sequence 
1
2
,
,...
θ θ
 
of substitutions. Each non-empty goal 
i
G  contains one atom, which is the 
selected atom of 
i
G . The clause 
1
i
G +  is said to be derived from 
i
G  and 
i
C  with 
substitutions 
iθ  and is carried out as follows. Suppose 
i
G  is 
1
...
...
1
k
n
B
B
B
n
←
∧
∧
∧
∧
≥ 
and 
k
B  is the selected atom. Let 
1
...
i
m
C
A
A
A
=
←
∧
∧
 
be any clause in P such that A and 
k
B  are unifiable with the most general unifier 
θ . Then 
1
i
G +  is 
1
1
1
1
(
...
...
...
)
k
m
k
n
B
B
A
A
B
B θ
−
+
←
∧
∧
∧
∧
∧
∧
∧
∧
 
and 
1
iθ +  is θ . An SLD refutation is a derivation ending at an empty clause. 
In the definitions of SLD derivation and SLD refutation, the consideration of 
most general unifiers instead of just one unifier reduces the search space for the 
refutation procedure. A search space for the SLD refutation procedure is an SLD 
tree as defined below. 
Let P be a definite program and G be a goal or an empty clause. An SLD tree 
for 
{ }
G
∪
P
 has G as root and each node is either a goal or an empty clause. 
Suppose 
1
...
...
1
k
n
B
B
B
n
←
∧
∧
∧
∧
≥ 

 
Logic Programming 
111 
 
is a non-empty node with selected atom 
k
B . Then this node has a descendant for 
every clause 
1
...
m
A
A
A
←
∧
∧
 
 such that A and 
k
B  are unifiable with the most general unifier θ . The 
descendant is  
1
1
1
1
(
...
...
...
)
k
m
k
n
B
B
A
A
B
B θ
−
+
←
∧
∧
∧
∧
∧
∧
∧
∧
 
Each path in an SLD tree is an SLD derivation and a path ending at an empty 
clause is an SLD refutation. The search space is the totality of derivations 
constructed to find an SLD-refutation. 
An SLD derivation may be finite or infinite. An SLD refutation is a 
successful SLD derivation. A failed SLD-derivation is one that ends in a non-
empty goal with the property that the selected atom in this goal does not unify 
with the head of any program clause. Branches corresponding to successful 
derivation in an SLD-tree are called success branches, branches corresponding to 
infinite derivations are called infinite branches and branches corresponding to 
failed derivations are called failed branches. All branches of a finitely failed SLD 
tree are failed branches.  
In general, a given 
{ }
G
∪
P
 has different SLD trees depending on which 
atoms are selected atoms. 
Example 
Consider the following recursive program presented earlier in this subsection and 
the goal 
( , )
P a y
←
:  
( , )
( , )
( , )
( , )
( , )
( , )
( , )
P x y
Q x y
P x y
Q x z
P z y
Q a b
Q b c
←
←
∧
 
The tree in Figure 4-2 (respectively, Figure 4-3) is based on a literal selection 
strategy which selects the leftmost (respectively, rightmost) literal from a goal. 
The tree in Figure 4-3 has an infinite branch whereas all the branches of the tree 
in Figure 4-2 are finite. 

112 
Decision-Making Agents 
 
 
←P(a, y)
←Q(a, y)
←Q(a, z) ∧P(z, y)
←Q(b, y)
←P(b, y)
←P(c, y)

←Q(c, y)
SUCCESS
FAILURE
C1
C2
C3
C3
C4
←Q(b, z1) ∧P(z1, y)
←Q(c, z2) ∧P(z2, y)
C1
C2
{y/b}
{z/b}
{z1/c}

SUCCESS
C4
{y/c}
C1
C2
FAILURE
 
Figure 4-2:  An SLD-tree with leftmost literal selection strategy. 
←P(a, y)
←Q(a, y)
←Q(a, z) ∧P(z, y)
←Q(a, b)
←Q(a, z) ∧Q(z, y)
←Q(a, z) ∧Q(z, b)

←Q(a, z) ∧Q(z, a) 
SUCCESS
FAILURE
C1
C2
C3
C1
C4
←Q(a, z) ∧Q(z, z1) ∧Q(z1, y)
←Q(a, a)
C3
C1
{y/b}
{z/c}
{z1/a, y/b}

SUCCESS
C3
{}
C3
C3
FAILURE
←Q(a, z) ∧Q(z, z1) ∧P(z1, y)
C2
←Q(a, a)
FAILURE
C4
INFINITE
{z/a, y/b}
{z/b, y/c}
{z1/b, y/c}
 
Figure 4-3:  An SLD-tree with rightmost literal selection strategy. 

 
Logic Programming 
113 
 
 
The equivalence between declarative and procedural semantics for definite 
programs is basically the soundness and completeness of SLD-resolution with 
respect to their minimal models. 
Theorem 4-3: (Soundness of SLD Resolution) Let P  be a program and G a goal 
such that 
{ }
G
∪
P
 has an SLD refutation with substitution θ . Then 
{ }
G
∪
P
 
is inconsistent. 
 
The above soundness theorem also implies that if the goal G is of the form 
Q
←
, then Qθ  is a logical consequence of the program P.  
Theorem 4-4: (Completeness of SLD Resolution) Let P  be a program and G be 
a goal such that 
{ }
G
∪
P
 is inconsistent. Then every SLD tree with G as root 
contains a success branch. 
 
The above completeness theorem also implies that if the goal G is of the 
form 
Q
←
, then for every substitution θ  of 
Q
←
, if Qθ  is a logical 
consequence of P, then there exists an SLD refutation of 
{
}
Q
∪←
P
 with answer 
substitution 
p
θ  such that θ  is more general than 
p
θ . 
The completeness theorem establishes the fact that if 
{ }
G
∪
P
 is 
inconsistent, then a refutation can be found if a proper algorithm (for example, 
breadth-first) is employed to search the tree. The above results also establishes 
that the set 
{ |
( ) and there exists an SLD refutation of 
{
}}
A A
A
∈
∪←
P
P
HB
 
representing the procedural or operational semantics of P is equal to the 
declarative semantics { |
( ) and 
}
A A
A
∈
P
P B
HB
 of P. 
The above-mentioned declarative and procedural semantics for programs are 
only concerned with the positive information that can be derived from the 
program. Closed World Assumption (CWA) and Negation As Failure (NAF) 
rules are provided below for studying semantics for negative information in 
definite programs. Each of these is consistent for the class of definite programs 

114 
Decision-Making Agents 
 
 
and can be used in conjunction with the other two semantics introduced above to 
obtain the complete meaning of a definite program. 
Given a program P and 
( )
A∈
P
HB
, one of the ways to say that 
A
¬  may be 
inferred from P is if A is not a logical consequence of P. This declarative 
definition for inferring negative information is what is called the closed world 
assumption and hence is considered a rule for inferring negative information 
from a program. Thus 
( )
{ |
( ) and  is not a logical consequence of }
CWA
A A
A
=
∈
P
P
P
HB
 
if A is not a logical consequence of P, then each branch of an SLD tree for 
{
}
A
∪←
P
  is either finitely failed or infinite. This operational aspect of the 
CWA provides a new rule for inferring negative information from a program and 
is called the negation as failure rule. The rule can be formally stated as follows. 
Given a program P and 
( )
A∈
P
HB
, if 
{
}
A
∪←
P
 has a finitely failed SLD tree, 
then A is not a logical consequence of P and hence 
A
¬  can be inferred from P. 
Thus 
( )
{ |
( ) and 
{
} has a finitely failed SLD tree}
NAF
A A
A
=
∈
∪←
P
P
P
HB
 
Naturally CWA is more powerful from the point of view that A may not be a 
logical consequence of P but every SLD tree for 
{
}
A
∪←
P
 has an infinite 
branch. Consider, for example, the program 
{ ( )
( )}
P a
P a
=
←
P
. Clearly 
( )
P a  is 
not a logical consequence of P and hence CWA infers 
( )
P a
¬
 from P. But the 
only branch of the tree for 
{
( )}
P a
∪←
P
 is infinite. If a system implementing 
the SLD resolution procedure is not capable of detecting such infinite branches, 
then a task for finding whether A is a logical consequence or not would be 
incomplete in that system. As far as the properties of rules for negative 
information are concerned, both CWA and NAF for definite programs are 
consistent. 
4.5 
Normal Programs 
The material presented in this section relates to normal programs. The declarative 
semantics of normal programs is studied through well-known program 
completion. SLDNF-resolution is also introduced in this section and a relation is 
established with its declarative counterpart, that is, program completion through 
the soundness and completeness result. Recall that a normal program has been 
defined as a finite set of normal clauses of the form 
1
...
0
m
A
L
L
m
←
∧
∧
≥
 

 
Logic Programming 
115 
 
and that a normal goal has been defined as a clause of the form 
1
...
1
n
M
M n
←
∧
∧
≥ 
where A is an atom and 
iL  and 
j
M  are literals. Unless otherwise mentioned, any 
reference to programs and goals will always mean normal programs and normal 
goals. 
In the previous section, the declarative semantics of a definite program P was 
defined by the set { |
( ) and 
}
A A
A
∈
P
P B
HB
, and that this semantics coincides 
with the minimal model of P. In contrast, a normal program may not have a 
unique minimal model. Consider the program 
{
}
Game
Rain
=
←¬
P
. The 
Herbrand base of P is {
,
}
Game Rain . The two minimal models of P are {
}
Game  
and {
}
Rain , and their intersection is the empty set, which is not a model of P. 
While it is said that the program P has two minimal models {
}
Game  and 
{
}
Rain , the three different program clause representations of Game
Rain
←¬
 in 
P, namely Game
Rain
←¬
 itself, Game
Rain
∨
 and Rain
Game
←¬
, are not 
distinguished from each other. However, the intent of writing Game
Rain
←¬
 is 
to say that Game is true when Rain is not, and probably the negation of Rain will 
be inferred by default if Rain cannot be inferred. Neither the reverse 
Game
Rain
←¬
 of 
Game
Rain
←¬
 is intended nor the disjunction 
Game
Rain
∨
. By distinguishing these representations from each other, the 
completed program semantics for a normal program is given below. 
According to the program completion semantics, the clauses of a program P 
provide the if-parts of the definition of the predicates. The only-if parts in a 
program are implicitly defined and are obtained by completing each predicate 
symbol. 
Example 
Consider a normal program containing only the following two clauses defining 
the predicate Weather: 
(
,
)
( , )
( , )
( , )
Weather Fenway Park Wet
Weather x y
Adjacent x z
Weather z y
←
∧
 
These two clauses jointly state that the weather in the Fenway Park area is known 
to be wet, and weather in the area of a field x is y if there exists a field z which is 
adjacent to x and the weather in the area of z is y. Symbolically, 

116 
Decision-Making Agents 
 
 
( , )
(
,
)
( , )
( , )
Weather x y
Weather Fenway Park Wet
Adjacent x z
Weather z y
←
∨
∧
 
However, the definition leaves open the possibility that the weather in the area of 
a field may be known for some other reason. What has been implicitly meant by 
the program is that the weather in the area of a field x is y only if x is Fenway 
Park and y is Wet or there exists a field z which is adjacent to x and the weather 
in the area of z is y. Symbolically, 
( , )
(
,
)
( , )
( , )
Weather x y
Weather Fenway Park Wet
Adjacent x z
Weather z y
→
∨
∧
 
The above two clauses jointly give the completed definition of the predicate 
Weather. The formal definition of predicate and program completions are 
provided below. 
 
Suppose that P is a program and 
1
1
( ,..., )
...
n
m
P t
t
L
L
←
∧
∧
 
is a clause in P. If the predicate symbol = is interpreted as the equality (or 
identity) relation, and 
1,...,
n
x
x  are variables not appearing elsewhere in the 
clause, then the above clause is equivalent to the clause 
1
1
1
1
( ,...,
)
...
...
n
n
n
m
P x
x
x
t
x
t
L
L
←
=
∧
∧
=
∧
∧
∧
 
If 
1,...,
p
y
y  are the variables of the original clause then this can be transformed to 
the following general form of the clause:  
1
1
1
1
1
( ,...,
)
...
(
...
...
)
n
p
n
n
m
P x
x
y
y
x
t
x
t
L
L
←∃
∃
=
∧
∧
=
∧
∧
∧
 
Suppose there are exactly k clauses, 
0
k ≥
, in the program P defining P (a clause 
C defines P if C has P in its head). Let 
1
1
1
( ,...,
)
...
( ,...,
)
n
n
k
P x
x
E
P x
x
E
←
←
 
be the k clauses in general form. Then the completed definition of P is the 
formula 
1
1
1
...
( (
,...,
)
...
)
n
n
k
x
x
P x
x
E
E
∀
∀
↔
∨
∨
 

 
Logic Programming 
117 
 
Some predicate symbols in the program may not appear in the head of any 
program clause. For each such predicate Q, the completed definition of Q is the 
formula 
1
1
...
( ,...,
)
n
n
x
x
Q x
x
∀
∀¬
 
Additional axioms for the equality symbol = are needed as part of the completed 
definition of a program. The equality theory EQ for a completed program 
contains these axioms, and they are listed below: 
• 
c
d
≠
, for all pairs c, d of distinct constants (the symbol ≠ stands for not 
equal, that is, c
d
≠
 stands for 
(
)
c
d
¬
=
). 
• 
1
1
1
1
...
...
( ( ,...,
)
(
,...,
))
n
m
n
m
x
x
y
y
f x
x
g y
y
∀
∀∀
∀
≠
, for all pairs f, g of distinct 
function symbols. 
• 
1
1
...
( ( ,...,
)
)
n
n
x
x
f x
x
c
∀
∀
≠
, for each constant c and function symbol f. 
• 
( [ ]
)
x t x
x
∀
≠
, for each term [ ]
t x  containing x and different from x. 
• 
1
1
1
1
...
...
((
)
...
(
)
n
n
n
n
x
x
y
y
x
y
x
y
∀
∀∀
∀
≠
∧
∧
≠
→
1
1
(
,...,
)
(
,...,
))
n
n
f x
x
f y
y
≠
, for each function symbol f. 
• 
1
1
1
1
...
...
((
)
...
(
)
n
n
n
n
x
x
y
y
x
y
x
y
∀
∀∀
∀
=
∧
∧
=
→
1
1
( ,...,
)
(
,...,
))
n
n
f x
x
f y
y
=
, 
for each function symbol f. 
• 
(
)
x x
x
∀
=
 
• 
1
1
1
1
...
...
((
)
...
(
)
n
n
n
n
x
x
y
y
x
y
x
y
∀
∀∀
∀
=
∧
∧
=
→
1
1
( ( ,...,
)
(
,...,
)))
n
n
P x
x
P y
y
→
, for each predicate symbol P (including =). 
The fact that = is an equivalence relation is implied by the above axioms. The 
completion of P (Clark, 1978; Lloyd, 1987), denoted by 
( )
P
Comp
, is the 
collection of completed definitions of predicate symbols in P together with the 
equality theory. 
Example 
Let the predicate symbol P be defined by the following clauses: 
( , )
( , )
( )
( , )
P a x
Q x y
R y
P b c
←
∧¬
 
Then the completed definition of P is. 

118 
Decision-Making Agents 
 
 
1
2
1
2
1
2
1
2
( ( ,
)
(
((
)
(
)
( , )
( ))
((
)
(
))))
x
x P x x
x y x
a
x
x
Q x y
R y
x
b
x
c
∀∀
↔
∃∃
=
∧
=
∧
∧¬
∨
=
∧
=
 
and the completed definitions of Q and R are 
1
2
1
2
( ,
)
x
x
Q x x
∀∀¬
 and 
1
1
(
)
x
R x
∀¬
 
respectively. 
 
Consider the program P that has only one clause P
P
←¬
. Apart from the 
equality axioms, 
( )
P
Comp
 is {
}
P
P
↔¬
, which is inconsistent. Therefore, the 
completion of a program may not be consistent. But, 
( )
P
Comp
 is consistent when 
P is stratified. The program {
}
P
P
↔¬
 is of course not stratified. 
SLDNF resolution is thought of as a procedural counterpart of the declarative 
semantics given in terms of completed programs. This resolution scheme is, to a 
large extent, the basis of present-day logic programming. The resolution scheme 
is essentially SLD resolution augmented by the negation as failure (NAF) 
inference rule. 
Let P be a program and G be a goal. An SLDNF derivation of 
{ }
G
∪
P
 
consists of a sequence 
0
1
,
,....
G
G G
=
 of goals, a sequence 
1
2
,
,...
C C
 of variants of 
clauses of P or negative literals, and a sequence 
1
2
,
,...
θ θ
 of substitutions 
satisfying the following: 
• 
Consider the case when 
i
G  is 
1
...
m
L
L
←
∧
∧
, 
1
m ≥, and the selected 
literal 
kL  is positive. Suppose 
1
...
n
A
M
M
←
∧
∧
 is the input clause 
1
i
C +  
such that 
kL  and A are unifiable with the mgu 
1
iθ + . Then the derived goal 
1
i
G +  is 
1
1
1
1
1
(
...
...
...
)
k
n
k
m
i
L
L
M
M
L
L θ
−
+
+
←
∧
∧
∧
∧
∧
∧
∧
∧
 
• 
Consider the case when 
i
G  is 
1
...
m
L
L
←
∧
∧
, 
1
m ≥, and the selected 
literal 
kL  is a ground negative literal of the form 
A
¬ . If there is a 
finitely failed SLDNF tree for 
{
}
A
∪←
P
, then 
1
iθ +  is the identity 
substitution, 
1
i
C +  is 
A
¬ , and 
1
i
G +  is  
1
1
1
...
...
k
k
n
L
L
L
L
−
+
←
∧
∧
∧
∧
∧
 
• 
If the sequence 
0
1
,
,....
G G
 is finite, then either 
− the last goal is empty, or 

 
Logic Programming 
119 
 
− the last goal is 
1
...
m
L
L
←
∧
∧
, 
1
m ≥, the selected literal 
kL  is 
positive, and there is no program clause whose head unifies with 
kL , or 
− the last goal is 
1
...
m
L
L
←
∧
∧
, 
1
m ≥, the selected literal 
kL  is a 
ground negative literal 
A
¬ , and there is an SLDNF refutation of 
{
}
A
∪←
P
. 
An SLDNF derivation is finite if it consists of a finite sequence of goals; 
otherwise it is infinite. An SLDNF derivation is successful if it is finite and the 
last goal is the empty goal. A successful SLDNF derivation is an SLDNF 
refutation.  An SLDNF-derivation is failed if it is finite and the last goal is not 
the empty goal. 
Let P be a normal program and G be a goal. Then an SLDNF tree for 
{ }
G
∪
P
 is defined as follows: 
• 
Each node of the tree is a goal. 
• 
The root node is G. 
• 
Let 
1
...
m
L
L
←
∧
∧
, 
1
m ≥, be a node in the tree and suppose that the 
selected literal 
kL  is positive. Then this node has a descendant for each 
input clause 
1
...
n
A
M
M
←
∧
∧
 such that 
kL  and A are unifiable. The 
descendant is 
1
1
1
1
(
...
...
...
)
k
n
k
m
L
L
M
M
L
L θ
−
+
←
∧
∧
∧
∧
∧
∧
∧
∧
 
where θ  is the most general unifier of 
kL  and A. 
• 
Let 
1
...
m
L
L
←
∧
∧
, 
1
m ≥, be a node in the tree and suppose that the 
selected literal 
kL  is a ground negative literal of the form 
A
¬ . If for 
every branch of an SLDNF tree for 
{
}
A
∪←
P
 the terminating node is a 
non-empty goal, then the single descendant of the node is  
1
1
1
...
...
k
k
n
L
L
L
L
−
+
←
∧
∧
∧
∧
∧
 
If there exists a branch of an SLDNF tree for 
{
}
A
∪←
P
 for which the 
terminating node is an empty goal, then the node has no descendant. 
• 
A node which is the empty goal has no descendants. 
In an SLDNF tree, a branch which terminates at an empty goal is a success 
branch, a branch which does not terminate is an infinite branch and a branch 
which terminates at a non-empty goal is a failed branch.  An SLDNF tree for 

120 
Decision-Making Agents 
 
 
which every branch is a failed branch is indeed a finitely failed SLDNF tree. Each 
branch of an SLDNF-tree corresponds to an SLDNF-derivation. 
According to the definition of SLDNF resolution, a computation cannot 
proceed (or flounders) further from a goal with only non-ground negative literals 
left. The allowedness restriction on programs and goals are introduced in order to 
prevent floundering of any computation. Formally, a clause 
1
...
n
A
M
M
←
∧
∧
 in 
a program P is allowed or range-restricted (Clark, 1978; Lloyd, 1987; Gallaire et 
al., 1984) if every variable that occurs in the clause occurs in a positive literal of 
the body 
1
...
n
M
M
∧
∧
. The whole program P is allowed if each of its clauses is 
allowed. A goal 
1
...
m
L
L
←
∧
∧
 is allowed if every variable that occurs in the 
goal occurs in a positive literal of the goal. If a program is range-restricted, then 
by definition, each of its assertions (that is, a clause with empty body) is ground. 
Theorem 4-5: (Soundness of SLDNF Resolution) Let P  be a program and 
1
...
n
G
L
L
=←
∧
∧
 be a goal. If 
{ }
G
∪
P
 has an SLDNF refutation with 
substitutions 
1,...,
m
θ
θ , then 
1
1
((
...
) ...
)
n
m
L
L θ
θ
∀
∧
∧
 is a logical consequence 
of 
( )
P
Comp
. 
 
Although SLDNF resolution is sound with respect to the completion, the 
resolution is not necessarily complete even when the completion is consistent. 
For example, if P is {
,
,
}
P
Q P
Q Q
Q
←
←¬
←
, then P is stratified and hence 
( )
P
Comp
 is consistent. The atom P is a logical consequence of 
( )
P
Comp
, but 
{
}
P
∪←
P
 does not have an SLDNF refutation. This example also proves that 
the SLDNF-resolution is not even complete for stratified programs. Hence one 
should look for a class of programs which is less general than the stratified class 
of programs. 
In his paper, Clark (1978) proved that the SLDNF-resolution is complete for 
an allowed hierarchical class of programs and goals. A further completeness 
result was also established for structured programs introduced by Barbuti and 
Martelli (1986). The completeness result has been pushed up to the class of 
allowed strict stratified programs (Cavedon and Lloyd, 1989; Baratelli and File, 
1988).  

 
Logic Programming 
121 
 
4.6 
Prolog 
This chapter presents the logic programming language Prolog. The main topics 
discussed in this chapter include the syntax of Prolog programs and goals, the 
style of their execution, semantics, and usefulness of extralogical features and 
meta-programming. 
Prolog is a logic programming language and a Prolog program is a normal 
logic program, which is a finite set of normal clauses, and a Prolog goal is a 
normal goal. Some additional extralogical features from outside the framework 
of first-order logic have been incorporated within normal Prolog programs and 
goals. These extra features allow (among other things) the achievement of the 
following: 
• 
Performance enhancement by reducing the search space, that is, by 
pruning the redundant branches of search trees (for example, using the 
cut symbol) 
• 
Increasing readability (for example, using if-then/or constructs) 
• 
Input/output operations (for example, using read/write predicates) 
• 
Clause management (for example, using assert/retract predicates) 
• 
Set-type evaluation in addition to the resolution-based, tuple-at-a-time 
approach (for example, using setof/bagof predicates) 
• 
Meta-programming (for example, using clause, call predicates) 
To incorporate these features, the syntax of Prolog programs (and Prolog goals) 
is extended using some special symbols, special predicates, and special 
constructs. 
4.6.1 
Prolog Syntax 
A standard Prolog type of syntax is now introduced for dealing with the normal 
Prolog programs and goals in the rest of the book. Extralogical symbols, 
predicates, and constructs are introduced in the relevant sections. 
• 
In general, constants, functions and predicate symbols of a Prolog 
program begin with a lower-case letter (or the dollar symbol, $) or may 
comprise any string of characters enclosed in single quotes. The rest of a 
line starts with % is treated as comments. 
• 
Variables begin with at least one upper-case letter (or the underline 
symbol _). The symbol _ on its own is a special type of variable called 

122 
Decision-Making Agents 
 
 
the “don't know” variable. Discrete occurrences of this particular variable 
in a program clause or in a goal are considered different from each other. 
• 
The symbols for ∧ (and sign), ∨ (or sign), ← (implication) and ¬ 
(negation) are ,, ;, :-, and not respectively. 
• 
Arithmetic function symbols (for example, +, -, *, /) and some 
comparison predicates (such as <, >) may be written in their infix forms. 
For example, the addition of two numbers represented by variables X and 
Y is usually expressed as X+Y rather than +(X,Y) (although this is also 
perfectly correct syntax). 
• 
Arithmetic assignment is achieved by the predicate is and the arithmetic 
equality symbol is represented by the symbol =:=. 
• 
The unification of two terms is performed using the symbol = whereas to 
check whether or not two terms are syntactically equal (that is, the 
equality symbol) the symbol == is used. 
In describing a goal, subgoal, or a piece of code, the occurrence of an 
expression of the form 〈type〉 or 〈type-n〉 can be replaced by an arbitrary 
expression which is of type type. So, for example, a subgoal 
square(〈integer-1〉,〈integer-2〉) means the arguments of the 
predicate square are arbitrary integers. With these conventions, a Prolog clause 
has the following form: 
〈atom〉:- 〈literal-1〉, 〈literal-2〉, ..., 〈literal-m〉. 
where 
0
m ≥
. Every clause in a Prolog program or a goal is terminated with the 
dot symbol (.). 
Example 
The normal clause 
( )
( ,
)
( )
Rain x
Condition x Wet
Sprinkler x
←
∧¬
 
encodes the statement that if a field’s condition is wet and the sprinkler system of 
the field is not on then it is raining at the field. In the syntax of first-order logic, 
this clause is expressed in Prolog syntax as 
rain(X):- condition(X, wet), not sprinkler(X). 
 

 
Logic Programming 
123 
 
A Prolog program’s goal is expressed by the symbol ?- followed by a 
conjunction of literals and then terminated with (.). Hence a normal Prolog goal 
has the form 
?- 〈literal-1〉, 〈literal-2〉, ..., 〈literal-n〉. 
Example 
A Prolog goal to find all fields where it is raining and not closed: is 
?- rain(X), not closed(X). 
Commands to a Prolog interpreter are in the form of goals. 
4.6.2 
Theoretical Background 
The inference mechanism adopted in a standard Prolog system (considered as a 
Prolog interpreter throughout the text) is the same as SLDNF-resolution with the 
following restrictions: 
• 
The computation rule in standard Prolog systems always selects the 
leftmost literal in a goal (it follows a leftmost literal selection strategy). 
• 
Standard Prolog uses the order of clauses in a program as the fixed order 
in which clauses are to be tried, that is, the search tree is searched depth-
first. 
• 
Prolog omits the occur check condition for unifying two expressions. 
Therefore, the search trees in the context of the execution of normal Prolog 
programs and goals are similar in structure to SLDNF-trees. Given a Prolog 
program and a goal, the search tree is unique because of the fixed computation 
rule and the fixed clause order in which they are fetched for the purpose of 
resolution. For your convenience, the definition of search tree is provided below 
in the terminology of Prolog. 
Suppose P is a normal Prolog program and G is a Prolog goal. Then the 
search tree T for 
{ }
G
∪
P
   (or the search tree for G with respect to P) is defined 
as follows: 
• 
The root of T is G. 
• 
A leaf node of T is either a goal (a failure node) or an empty clause (a 
success node). 
• 
Suppose the node N is 

124 
Decision-Making Agents 
 
 
?- 〈literal-1〉, 〈literal-2〉, ..., 〈literal-n〉 
and the leftmost literal 〈literal-1〉 is positive. Suppose 
〈atom-1〉:-〈literal-conjunction-1〉 
… 
〈atom-p〉:-〈literal-conjunction-p〉 
are the only clauses of P such that 〈literal-1〉 and each of 〈atom-i〉 
unifies with an mgu 〈substitution-i〉. The order shown for these 
clauses is the order in which they will be tried for resolution with the 
goal of node N. Then N has p descendants and they are (from left to 
right) 
?- (〈literal-conjunction-1〉, 〈literal-2〉, ..., 
           〈literal-n〉)〈substitution-1〉 
… 
?- (〈literal-conjunction_p〉, 〈literal-2〉, ..., 
           〈literal-n〉)〈substitution-p〉. 
If for some i, 〈literal-conjunction-i〉 is true, (that is, the 
clause 〈atom-i〉:-〈literal-conjunction-i〉 is a fact), then the 
corresponding descendant is 
?- (〈literal-2〉, ..., 〈literal-n〉)〈substitution-i〉. 
This becomes the empty clause , when 
0
n =
. 
• 
Suppose the node N is 
?- 〈literal-1〉, 〈literal-2〉, ..., 〈literal-n〉 
and the leftmost literal 〈literal-1〉 is a negative literal of the form 
not 〈atom〉 (The search tree construction does not stop even if the 
leftmost literal is non-ground and the answer in this case may be 
incorrect.) Then a recursive process is established to apply the negation 
as failure (NAF) rule, that is, to find the search tree for the goal ?- 
〈atom〉 with respect to P. If all the branches of this tree result in failure, 
then the only descendant of N is 
?- 〈literal-2〉, ..., 〈literal-n〉 
without any substitution of its variables; otherwise, if the system is able 
to find a success node of the search tree without going into an infinite 
loop, then N is considered a failure node. 

 
Logic Programming 
125 
 
In the above definition, a success branch corresponds to a successful Prolog 
derivation which causes some instantiations to the variables of G and is an 
answer to the goal G. The following example constructs a Prolog search tree. 
Example 
Consider the following program P and goal G: 
Program: 
R1.  p(X,Y):- q(X,Y),not r(Y). 
R2.  p(X,Y):- s(X),r(Y). 
F1.  q(a,b). 
F2.  q(a,c). 
F3.  r(c). 
F4.  s(a). 
F5.  t(a). 
Goal: 
?- p(X,Y),t(X). 
The Prolog search tree for 
{ }
G
∪
P
 is given in Figure 4-4. The leftmost branch is 
a successful derivation and is given below (selected literals are underlined). 
Goals  
 
 
 
Input Clauses 
 
Substitutions 
?- p(X,Y),t(X) 
 
 
R1 
 
 
{X/X, Y/Y} 
?- q(X,Y),not r(Y),t(X) 
F1 
 
 
{X/a, Y/b} 
?- not r(b),t(a)  
 
not r(b) 
 
{} 
?- t(a) 
 
 
 
t(a)  
 
{} 
  

126 
Decision-Making Agents 
 
 
?- p(X,Y),t(X)
?- q(X,Y),not r(Y),t(X)
?- s(X),r(Y),t(X)
?- not r(b),t(a)
?- not r(c),t(a)
?- r(Y),t(a)
?- t(a)
?- t(a)


SUCCESS
SUCCESS
FAILURE
R1
R2
F1
F2
F4
F3
F5
F5
NAF
{X/a, Y/b}
{X/a, Y/c}
{X/a}
{Y/c}
 
Figure 4-4: A Prolog search tree. 
 
4.6.3 
Backtracking 
When a complete search tree for a goal with respect to a program is generated 
without going into a loop, all success branches of the tree generate all answers to 
the goal G. In the context of the example presented in the previous sub-section, 
there are two answers to the goal ?-p(X,Y),t(X) and they correspond to the 
two success branches of Figure 4-4. These two answers are {X/a, Y/b} and {X/a, 
Y/c}. Since the search tree is generated depth-first, the leftmost branch is 
generated first and therefore the answer {X/a, Y/b} results. 
To try to find the next alternative answer, the user can initiate backtracking, 
which is a request for an alternative solution. Backtracking after detecting a 
failed branch is automatic. Hence, after the user initiates backtracking, all 
subsequent backtrackings are automatic until the interpreter finds an alternative 
answer to the goal. In the case of the above example, when backtracking is 
initiated after the first answer has been generated, the system must try to find an 

 
Logic Programming 
127 
 
alternative solution for the last subgoal t(a) to find an alternative answer for 
the original goal. Since there is no more t(a) in the program, the last but one 
subgoal is tried. This is the negative subgoal not r(b) which cannot generate 
any new answers. 
Backtracking further, the subgoal q(X,Y) is resolved against the second 
fact q(a,c) under the predicate q and another descendant ?- not 
r(c),t(a) is created from the node ?-q(X,Y),not r(Y),t(X). The 
selected subgoal not r(c) from this node does not succeed and hence this 
node is considered a failure node. The interpreter backtracks automatically to the 
root node and generates the rightmost success branch eventually. 
4.6.4 
The Cut 
Prolog provides an extralogical system predicate called the cut (!) to reduce the 
search space. This reduction is achieved by dynamically pruning redundant 
branches of the search tree. A normal clause with one cut looks like 
〈atom〉:- 〈literal-1〉, ..., 〈literal-i〉, !, 
                    〈literal-i+1〉, ..., 〈literal-n〉. 
On finding a solution, the cut prunes all alternative solutions to the conjunction 
〈literal-1〉, ..., 〈literal-i〉, but does not affect the conjunction 
〈literal-i+1〉, ..., 〈literal-n〉. To illustrate this, consider the 
following example. 
Example 
Refer again to the program P which generated the search tree in Figure 4-4 for 
the goal ?- p(X,Y), t(X). The search tree for 
{ }
G
∪
P
, where G is the goal 
?- p(X,Y), !, t(X), is same as the one in Figure 4-4. Due to the presence 
of the cut symbol in the goal the alternative solution p(X,Y) (with the help of 
R2) is pruned. Again refer to the same program P. If the rule R1 is replaced by 
p(X,Y):- q(X,Y), !, not r(b), then the tree contains only the 
leftmost branch of Figure 4-4. 
 
Cuts are divided into two different categories. Green cuts prune only those 
branches of the search tree that do not lead to any new solutions. Hence removal 
of those cuts from a program does not change the declarative meaning of the 
program.  

128 
Decision-Making Agents 
 
 
Example 
Consider the following program which computes the absolute value of an 
integer X and illustrates the use of a green cut in a program: 
absolute(X, X):- X >= 0, !. 
absolute(X, Y):- X < 0, Y is -X. 
Without using the cut in the first clause a redundant check would be performed 
when calculating the absolute value of a non-negative integer using a goal such 
as ?- absolute(3, Z). By using a cut this redundant check is eliminated. 
 
All other cuts are red cuts and they do affect the declarative meaning of a 
program. Red cuts should be used in a program with special care. 
Example 
Consider the following example illustrating the use of a red cut: 
absolute(X, X):- X >= 0,  !. 
absolute(X, Y):- Y is -X. 
distance(X, Y, Z):- Z1 is X-Y, absolute(Z1, Z). 
The cut in the above program is absolutely essential. In its absence a goal of the 
form ?-distance(2, -3, Z) would instantiate Z to 5 first and then to –5 
upon backtracking which is obviously incorrect. 
 
4.6.5 
Special Constructs and Connectives 
To increase the readability and, to some extent, the efficiency of a Prolog 
program, a few extra connectives and constructs have been introduced. When the 
or-connective ; is used in program clauses (and also in goals) as 
〈atom〉:- 〈lit-conj-1〉, (lit_conj-2; 
                lit-conj_3), lit-conj_4. 
this single clause is interpreted as a combination of two clauses 
〈atom〉:- 〈lit-conj-1〉, lit-conj-2, lit-conj-4 
〈atom〉:- 〈lit-conj-1〉, lit-conj-3, lit-conj-4 

 
Logic Programming 
129 
 
where lit-conj is an abbreviation of literal-conjunction. In the body 
of a Prolog program clause an occurrence of an expression of the form 
〈lit-conj-1〉 -> 〈lit-conj-2〉; 〈lit-conj-3〉 
using the special conditional if-then construct (->) can be thought of as an 
alternative form of an atom (not a first-order atom) 
〈predicate-symbol〉(〈lit-conj-1〉, 
                        〈lit-conj-2〉, 〈lit-conj-3〉) 
where the definition of 〈predicate-symbol〉 is 
〈predicate-symbol〉(〈lit-conj1〉, 〈lit-conj-2〉, _):- 
〈lit-conj-1〉, !, 〈lit-conj-2〉. 
〈predicate-symbol〉(_, _, 〈lit-conj-3〉):- 
〈lit-conj-3〉. 
The above use of the cut is red. 
Example 
An example of the use of the construct -> is to define the absolute value of a 
number X as in the program 
absolute(X, Y):- 
X >= 0 -> Y = X; Y is -X. 
where Y gives the absolute value of X. 
 
4.6.6 
Negation 
Following SLDNF-resolution, Prolog provides a form of negation based on the 
negation as failure principle. The negation of 〈atom〉 is written in Prolog as not 
〈atom〉 and can be implemented using the cut as follows: 
not X :- X, !, fail. 
not X. 
The first of the above clauses prunes the other branches of the SLDNF-tree for 
the goal “?- X” when a success branch is found and marks the original goal as 
failed. This way of inferring negation may fail to generate all answers as is 
evident from the following example. 

130 
Decision-Making Agents 
 
 
Example 
Program: 
p(a). 
q(b). 
Goals: 
?- p(X), not q(X). 
?- not q(X), p(X). 
The above pair of goals is the same except for the order of the literals. With 
leftmost literal selection strategy, the first goal succeeds and returns X = a as 
an answer. However, the second goal has a finitely failed search tree with respect 
to the program and therefore does not generate any answers. 
 
As was pointed out in the description of SLDNF-resolution, negative literals 
should be selected only when they are ground. If at some stage of a computation 
the goal contains only non-ground negative literals, then the computation 
flounders. When programs and goals are considered to be range-restricted, a 
literal selection strategy can always be found such that computations never 
flounder. Under Prolog's leftmost literal selection strategy, even if programs and 
goals are range-restricted, a computation may flounder. This can be demonstrated 
by considering the above program and its second goal. However, by rearranging 
the literals in the goals as well as in the bodies of the program clauses, it is 
always possible to make the negative literals ground before their selection. One 
of the ways to achieve this is by placing all negative literals in a goal or in the 
body of a clause after all the positive literals. By applying this strategy to the 
above program and the second goal, it is reduced to the first one. 
It is again worth mentioning here that if a non-ground negative literal “not 
A” is chosen from a goal and if the goal “?- A” has a finitely failed search tree 
with respect to the program, then the computation can be continued without fear 
of losing any answers. 
4.6.7 
Equality 
The symbol = in Prolog is treated as an unifying relation. That is, a subgoal of 
the form 
〈expression-1〉 = 〈expression-2〉 

 
Logic Programming 
131 
 
succeeds when 〈expression-1〉 and 〈expression-2〉 unify. Such a success 
will cause instantiation to the variables of 〈expression-1〉 and 
〈expression-2〉 giving an mgu of 〈expression-1〉 and 〈expression-
2〉. For example, the goal 
?- f(X,b) = f(a,Y). 
will succeed with variables X and Y instantiated to a and b respectively, that is, 
the goal returns {X/a, Y/b} as an mgu. 
When the two arguments of the predicate = are equal, the mgu in this case 
can be taken as an identity substitution. The predicate \= is just the negation of 
the predicate = and is defined using the cut and fail combination as follows: 
〈expression-1〉 \= 〈expression-2〉:- 
         〈expression-1〉 = 〈expression-2〉, !, fail. 
_ \= _. 
Since Prolog omits the occur check condition when unifying two 
expressions, goals of the form 
?- X = f(X). 
do not fail although the normal unification procedure would, of course, fail to 
unify the two expressions X and f(X). Prolog, however, would instantiate X to 
the infinite string f(f(f(f(f(.... 
The predicate == is used to check whether or not two terms are syntactically 
equal. The subgoal 
?- f(X,b) == f(a,Y). 
fails as its constituent terms f(X,b) and f(a,Y) are not syntactically equal 
(though the two terms are unifiable). The predicate \== is the negation of == 
and can be defined using the cut and fail combination in the same way as \= was 
defined by =. 
4.6.8 
List 
A list structure is a sequence of any finite number of term structures. In Prolog it 
is represented as 
 [〈expression-1〉, ..., 〈expression-n〉] 
where each 〈expression-1〉, ..., 〈expression-n〉 is a member of the list. 
The expression 〈expression-1〉 is called the head of the list and the list 
[〈expression-2〉, ..., 〈expression-n〉] is called the tail of the list. 

132 
Decision-Making Agents 
 
 
The empty list, denoted as [], does not have any members. A list in Prolog is 
abbreviated as [〈expression〉|〈list〉] in which 〈expression〉 is the head 
and 〈list〉 is the tail. Thus the goal 
?- [H|T] = [a, b, c, d]. 
would succeed with instantiation {H/a, T/[b, c, d]}. 
A list structure can be thought of as a functional value 
(
, )
list H T  which is 
when the one-to-one function symbol list is applied on its two arguments H and 
T. The functional value 
(
, )
list H T  is also a list in which H is the head and T is 
the tail. 
Example 
As an application of list structure, consider the following Prolog program 
which checks whether a given structure is a member of a list or not: 
member(X, [Y|_]):-X == Y. 
member(X, [_|T]):-member(X, T). 
The subgoal member(X, L) succeeds when X is a member of L. Another 
useful operation on lists is to join two lists together to form another. The 
following Prolog program serves this purpose: 
append([], L, L). 
append([X|L1], L2, [X|L3]):-append(L1, L2, L3). 
The subgoal append(L1, L2, L3) succeeds when the list L3 is obtained by 
appending L2 to L1. 
4.6.9 
Arithmetic 
The arithmetic function symbols (*, /, +, -, mod, div) are interpreted function 
symbols in Prolog. The assignment of the value of an arithmetic expression to a 
variable is achieved by the binary predicate symbol is. A subgoal of the form 
〈variable〉 is 〈arithmetic-expression〉 
causes an evaluation of 〈arithmetic-expression〉, giving a value. Then 
〈variable〉 is instantiated to this value. For example, the subgoal 
?- X is 2+3*5. 

 
Logic Programming 
133 
 
succeeds and instantiates X to 17. Equality testing between the values of two 
arithmetic expressions is carried out in Prolog by means of the predicate =:=. 
Hence a subgoal of the form 
〈arithmetic-expression-1〉 =:= 
                 〈arithmetic-expression-2〉 
succeeds when the value of 〈arithmetic-expression-1〉 is equal to that 
of 〈arithmetic-expression-2〉. Thus the goal 
?- 2+3*5 =:= 57/3-2. 
succeeds. Other arithmetic binary predicates are < (strictly less than), > (strictly 
greater than), <= (less than or equal to), >= (greater than or equal to). The 
arguments of these predicates and also the predicate =:= should be ground. 
4.6.10 Input/Output 
A Prolog program can read data from several input files (called input streams) 
and can output data to several output files (called output streams). However only 
one input stream and one output stream can be active at any time during the 
execution of the program. Reading input from a user's terminal and outputing 
data to a user's terminal are considered as input and output to a special file named 
user. At the beginning of the execution of a program these two streams are open. 
The subgoal 
see(〈file-name〉) 
succeeds if the file 〈file-name〉 exists. The subgoal will cause the current 
input stream to switch to 〈file-name〉. The existence of a file can be tested by 
the predicate exists. The subgoal 
exists(〈file-name〉) 
succeeds if the file 〈file-name〉 exists in the current directory. In dealing with 
output streams, the subgoal 
tell(〈file-name〉) 
always succeeds and causes the current output stream to switch to 〈file-
name〉. 
Reading a term from the current input stream is accomplished by the unary 
predicate read and writing a term to the current output stream is done by the 
predicate write. Hence the subgoal 
read(〈variable〉) 

134 
Decision-Making Agents 
 
 
succeeds with an instantiation of the variable 〈variable〉 to a term from a 
current input file. When the end of the current input file is reached, 〈variable〉 
is instantiated to the constant end_of_file. Each term of the input file must 
be followed by . and a carriage return. In a similar way, the subgoal 
write(〈term〉) 
succeeds and causes the term 〈term〉 to be written on the current output stream. 
The subgoal 
tab(〈integer〉) 
succeeds and writes 〈integer〉 number of spaces on the current output stream. 
Also the nullary predicate nl causes the start of a new line on the current output 
stream. Current input and output streams can be closed by the nullary predicates 
seen and told respectively. 
Example 
Consider the following program to display the contents of a file on the terminal 
(assuming the current output stream is user) using the predicates read and 
write: 
display_file(FileName):- 
exists(FileName), 
see(FileName), 
repeat, 
read(Term), 
(Term = end_of_file -> !, seen ; 
write( Term ), write('.'), nl, fail). 
A goal of the form 
?- display_file(〈file-name〉). 
displays the contents of the file 〈file-name〉 on the terminal but the variable 
names occurring in any terms therein are replaced by suitable internal variable 
names. 
 
As opposed to reading and writing terms, single characters from the input or 
output streams can also be read or written with the help of the unary predicates 
get0 and put. For example, a single character can be read from an input stream 
by using a subgoal of the form 

 
Logic Programming 
135 
 
get0(〈variable〉) 
and 〈variable〉 will be instantiated to the ASCII character code for the 
character read. A subgoal of the form 
put(〈variable〉) 
displays the character whose ASCII character code is 〈variable〉. 
Example 
The above program to display the contents of a file can be rewritten using the 
two predicates get0 and put as follows: 
display_file(FileName):- 
exists(FileName), 
see(FileName), 
repeat, 
get0(Char), 
(Char = 26 -> % ASCII code for control-Z 
!, seen ; 
put(Char), fail). 
In this case the variable names will be displayed as they were in the input file. 
 
4.6.11 Clause Management 
To read clauses from a file the predicate consult is used. Hence a command 
?- consult(〈file-name〉). 
reads the clauses from the file 〈file-name〉 into the Prolog interpreter. An 
alternative syntax for this is 
?- [〈file-name〉]. 
A number of different files can be consulted as 
?- [〈file-name-1〉, ..., 〈file-name-n〉]. 
This is useful when different parts of a program are held in a number of different 
files. A command of the form 
?- reconsult(〈file-name〉). 

136 
Decision-Making Agents 
 
 
results in the clauses in the file being added to the existing clause set. At the 
same time, any clauses with the same predicate symbol and arity in their heads as 
those in 〈file-name〉 are deleted from the existing clause set. An alternative 
syntax for reconsulting the file is 
?- [-〈file-name〉]. 
A sequence of files can be consulted and reconsulted. As an example, consider 
the command 
?- [〈file-name-1〉, -〈file-name-2〉, 〈file-name-3〉]. 
which causes 〈file-name-1〉 to be consulted, then 〈file-name-2〉 to be 
reconsulted on the resultant set of clauses and then 〈file-name-3〉 to be 
consulted on the resultant set of clauses. Consulting or reconsulting the special 
file user causes the system to read in clauses from the terminal until the predicate 
symbol end_of_file is entered.  
An individual clause can be added to an existing set of clauses by the 
command 
?- assert(〈clause〉). 
There are two different versions of the assert predicate, namely asserta 
and assertz. The former causes the clause to be placed towards the beginning 
of the existing set of clauses before any other clauses with same head predicate 
and the latter will place it at the end after any clauses with the same head 
predicate. Deleting a clause from an existing set of clauses is achieved by 
?- retract(〈clause〉). 
Considering clause as a term, Prolog attempts to unify it with an existing 
clause. The first clause for which this unification succeed is deleted from the 
existing clause set. The command 
?- retractall(〈clause〉). 
deletes all the clauses whose heads unify with clause. The command 
?- abolish(〈predicate-symbol〉, 〈integer〉). 
causes the deletion of all clauses whose head predicate symbol is 〈predicate-
symbol〉 with arity 〈integer〉. 
4.6.12 Set Evaluation 
The evaluation strategy for producing answers in pure Prolog is basically tuple-
at-a-time, that is, branches of the search tree are explored one after another using 

 
Logic Programming 
137 
 
a depth-first strategy with backtracking. Every success branch produces an 
answer. When Prolog tries to find another success branch through backtracking, 
all information about the previous branch of the computation is lost. Therefore, in 
Prolog, there is no connection between the computation of answers coming from 
two different branches of the search tree. 
The predicate setof enables you to accumulate different answers to a query 
and thus provides a set-at-a-time evaluation strategy. The syntax of this predicate 
is outside the framework of first-order logic and can be given as 
setof(〈term〉, 〈conjunction〉, 〈list〉) 
The above subgoal is true when 〈list〉 is the sorted set of instances of 〈term〉 
for which the goal 〈conjunction〉 succeeds. A slightly different version of the 
setof predicate is bagof whose syntax is similar to the above, that is 
bagof(〈term〉, 〈conjunction〉, 〈list〉) 
The semantics of the above subgoal is given as true when 〈list〉 is the multiset 
(that is, a well-defined collection of elements, not necessarily distinct) of all 
instances of 〈term〉 for which ?- 〈conjunction〉 succeeds. 
Example 
Consider a program containing the following clauses: 
event_type(fenway_park, baseball). 
event_type(fenway_park, concert). 
event_type(fenway_park, baseball). 
event_type(eden_garden, cricket). 
event_type(eden_garden, soccer). 
event_type(eden_garden, cricket). 
event_type(mcc_lords, cricket). 
A goal to find all the types of event held at Fenway Park can be given as 
?- setof(X, event_type(fenway_park, X), L). 
which causes an instantiation of L as 
L = [baseball,concert] 
On the other hand, the goal 
?- bagof(X, event_type(fenway_park, X), L). 
causes an instantiation of L to 

138 
Decision-Making Agents 
 
 
L = [baseball,concert,baseball] 
The predicate setof can have multiple solutions. The goal 
?- setof(X, event_type(Y, X), L). 
causes an instantiation of Y and L as 
L = [baseball,concert] 
Y = fenway_park 
respectively. Upon backtracking, the next instantiation of Y and L would be 
L = [cricket,soccer] 
Y = eden_garden 
and so on. The same goal can be interpreted to find all Xs such that father(Y, 
X) is true for some Y and is expressed as 
?- setof(X, Y^event_type(Y, X), L). 
where Y^ can be interpreted as an existential quantifier. The instantiation to L 
would be 
L = [baseball,concert,cricket,soccer] 
The predicate bagof has similar interpretations in the above two cases but does 
not discard any duplicate elements from a list. 
 
The system predicate setof can be implemented by using some of the 
predicates already introduced. 
Example 
Consider one such alternative implementation to find all solutions without 
backtracking: 
set_of(Term, Goal, Instance):- 
assert(term_goal(Term, Goal)), 
set_of([], UInstance), 
sort(UInstance, Instance), 
retract(term_goal(_,_)). 
set_of(L, UInstance):- 
term_goal(Term, Goal), 
Goal, 
not member(Term, L), !, 

 
Logic Programming 
139 
 
set_of([Term|L], UInstance). 
set_of(UInstance, UInstance). 
member(H, [H|_]):-!. 
member(H, [_|T]):-member(H, T). 
 
A major drawback in the above implementation is that each time a new 
solution is generated, the search tree corresponding to the goal Goal is traversed 
from the beginning and not from the point where the last solution is found. A 
further alternative implementation can be achieved by using the assert and 
retract predicates. This implementation would be particularly difficult in the 
presence of rules. 
4.6.13 Meta Programming 
A meta-program analyzes, transforms, and simulates other programs by treating 
them as data. Meta-programming is inherent in the nature of logic programming 
and Prolog is no exception. Some meta-programming examples are as follows: a 
translator (or a compiler) for a particular language is a meta-program which takes 
programs written in that language as input, the output for the translator (or 
compiler) is another program in another language, and an editor can also be 
viewed as a meta-program which treats other programs as data. 
An interpreter for a language, called a meta-interpreter, is also a meta-
program. If the language of a meta-interpreter M, that is, the language in which 
M is written, coincides with the language for which M is written, then the meta-
interpreter M is called a meta-circular interpreter. Since program and data are 
uniformly expressed as clauses in logic programming environments, writing a 
meta-circular interpreter is an inherent feature. 
Example 
Consider an example of a meta-circular interpreter written in Prolog which 
simulates Prolog-like execution by selecting the rightmost atom from a goal: 
solve(true). 
solve((B, A)):- solve(B), solve(A). 
solve(not A):- not solve(A). 
solve(A):- clause(A, B), solve(B). 

140 
Decision-Making Agents 
 
 
Another example is the meta-program which transforms a set of definite rules 
(with no occurrence of constant symbols) to another program by saturating each 
non-recursive predicate occurring in the body of a rule. For example, the 
following Prolog version of some definite program: 
a(X, Y):- p(X, Y). 
a(X, Y):- p(X, Z), a(Z, Y). 
p(X, Y):- f(X, Y). 
p(X, Y):- m(X, Y). 
m(X, Y):- f(Z, Y), h(Z, X). 
can be transformed to the following Prolog program: 
a(X, Y):- f(X, Y). 
a(X, Y):- f(Z, Y), h(Z, X). 
a(X, Y):- f(X, Z), a(Z, Y). 
a(X, Y):- f(Z1, Z), h(Z1, X), a(Z, Y). 
p(X, Y):- f(X, Y). 
p(X, Y):- f(Z, Y), h(Z, X). 
m(X, Y):- f(Z, Y), h(Z, X). 
To write this meta-program, suppose the program clauses are stored under a 
unary predicate rule in the form of rule(〈rule〉). Then, in the following 
meta-program, the subgoal saturate(〈rule-1〉, 〈rule-2〉) succeeds 
only when 〈rule-2〉 is obtained from 〈rule-1〉 by the saturation process 
described above: 
base(P):- not rule((P:-_)). 
recursive(P):- rule((P:-B)), recursive(P, B), !. 
recursive(H, H). 
recursive(H, (B,Bs)):- 
recursive(H, B); recursive(H, Bs). 
recursive(H, H1):- 
rule((H1:-B)), recursive( H, B ). 
expand((B,Bs), (ExpB,ExpBs)):-!, 
expand(B, ExpB), expand(Bs, ExpBs). 
expand(H, H):- (recursive( H ); base(H)), !. 
expand(H, ExpB):- rule((H:-B)), expand(B, ExpB). 

 
Logic Programming 
141 
 
4.7 
Prolog Systems 
SICStus Prolog (http://www.sics.se/isl/sicstus.html) is an ISO standard compliant 
(http://www.logic-programming.org/prolog_std.html) 
Prolog 
development 
system 
with 
constraint 
solver. 
Logic 
Programming 
Associates 
(http://www.lpa.co.uk/) offers Prolog compilers on Windos, DOS, and Macintosh 
platforms. GNU Prolog (http://pauillac.inria.fr/~diaz/gnu-prolog/) is a free and 
ISO standard compliant Prolog compiler with constraint solving over finite 
domains. Amzi! (http://www.amzi.com/) offers Prolog + Logic servers on 
various platforms, including Windows, Linux, and Solaris. 
The Prolog system E5 (Environment for 5th Generation Application), being 
developed by the author, has three modes of operations: Prolog, Lisp, and Expert 
System. It is an interpreter developed fully in Visual C++ 6.0 and therefore runs 
only under the Microsoft Windows environment. The system provides an 
interactive, multi-document, and graphical user interface. The system is a full 
implementation of ANSI Prolog and supports various features, including libraries 
for graphical user interface development, routines for integrating ODBC 
compliant data sources as facts, and a debugging facility. The interpreter is also 
COM compliant so that it can be called by other applications. E5 is an ideal 
platform for rapid prototyping of complex reasoning processes. Please contact 
the author for more details or to obtain an evaluation version. 
4.8 
Complexity Analysis 
Complexities of various classes of logic programming are given in terms of 
polynomial hierarchy. The classes 
p
k
Σ , 
p
k
Π , and 
p
k
Δ  of the polynomial hierarchy 
are recursively defined as follows: 
0
0
0
p
p
p
P
Σ = Π = Δ =
 
1
1
1
1
,
,
p
p
k
k
p
p
p
p
k
k
k
k
NP
co
P
Σ
Σ
+
+
+
+
Σ
=
Π
=
−Σ
Δ
=
, for all 
0
k ≥
 
Therefore, 
1
1
1
NP,
co-NP,
P
p
p
p
Σ =
Π =
Δ =
. The class 
2
p
Σ  contains all problems 
solvable in non-deterministic polynomial time, provided it is possible to freely 
use a subroutine for a problem in NP. Testing the validity of a QBF of the form 
1
1
...
...
n
m
X
X
Y
Y F
∃
∃
∀
∀
 
is 
2
p
Σ -complete. On the other hand, testing the validity of a QBF of the form 
1
1
...
...
n
m
X
X
Y
Y F
∀
∀
∃
∃
 

142 
Decision-Making Agents 
 
 
is 
2
p
Π -complete.  
We study here breifly the complexity of the following query problem in logic 
programming: given a program P and a goal or query of the form 
1
...
n
L
L
←
∧
∧
, 
determine from the completion of the program P whether there is a proof for the 
formula 
1
1
...
(
...
)
p
n
x
x
L
L
∃
∃
∧
∧
 
where 
1,...,
p
x
x  are all the free variables in 
1
...
n
L
L
←
∧
∧
. Note that there is no 
difference between data complexity and program complexity, as defined in 
(Vardi, 1982), since the program can be considered as part of the query; by 
complexity here we mean the expression or the combined complexity. If P is 
non-recursive without function symbol then the complexity is PSPACE-
complete. For stratified non-recursive programs without function symbol, the 
complexity is still PSPACE-complete, and NEXP-complete (if P = NP then 
NEXP = EXP) in the case of recursive programs. In the case of arbitrary function 
symbols, the complexity for programs with n levels of stratification is 
p
n
Σ -
complete. See (Dantsin et al., 2001) for a detailed discussion on the complexity 
of various classes of logic programs. 
4.9 
Further Readings 
The author’s own book (Das, 1992) presents a detailed overview of logic 
programming and its related field deductive databases. Kowalski’s book (1979a) 
is must for foundational reading. A theoretical account of logic programming can 
be found in (Lloyd, 1987). (Russell and Norvig, 2002) is an excellent book that 
discusses, among other areas, logic, logic programming, and Prolog from the 
artificial intelligence perspective. There are a number of good books on Prolog, 
including (Bratko, 2000), (Clocksin and Mellish, 2003), (O’Keefe, 1990), and 
(Sterling and Shapiro, 1994). 

143 
Chapter 5 
 
Logical Rules for Making 
Decisions 
 
 
 
 
This chapter provides a foundation for building propositional epistemic models 
for decision-making agents by describing the use of a special subset of classical 
logics that take the form of conditional if-then type rules. The if-then type syntax 
provides domain experts a natural way of expressing their knowledge, be it as 
arguments for or against carrying out actions or simple conditional statements for 
deriving implicit knowledge from observations. Unlike the definite rules handled 
in the last chapter, the syntax of rules considered in this chapter allows negative 
consequents for representing arguments against certain options. Moreover, to 
break the coarse grain granularity of Boolean type truth-value, we embed 
numbers from the dense set [0,1]  to represent degrees of uncertainty with two 
semantics in mind: 1) classical probability; and 2) mass values, as in Dempster-
Shafer theory. To determine ranking among a set of decision options by 
aggregating incoming evidence, we apply the Bayesian formula to compute 
posterior probabilities in the case of probability semantics, and apply the 
Dempster combination rule to compute overall degrees of belief in the case of 
belief semantics. 
The embellishment of rules with degrees of uncertainty really combines the 
pure propositional epistemic model with the probabilistic epistemic model. The 
adoption of the Dempster-Shafer theory is a slight departure from the probability 
thrust of this book, but it fits naturally within the realm of possible world 
semantics to be adopted later, and also generalizes the theory of probability.  
When multiple, disparate, and uncertain knowledge sources, be they subject 
matter experts or fielded sensors, are involved in making decisions, some kind of 
quantitative criterion is useful for measuring the quality of the consensus 
generated by pooling evidence from these knowledge sources. Formally, 

144 
Decision-Making Agents 
 
 
consensus in such a disparate, group-decision-making context refers to agreement 
on some decision by all members (experts and sensors) of the group, rather than a 
majority, and the consensus process is what a group goes through to reach this 
agreement. (The consensus process provides a way to focus on areas where 
experts disagree to help initiate conflict-resolving discussions.) 
In this chapter, we present a criterion for measuring consensus among 
disparate sources when the underlying mechanism for handling uncertainty is the 
Dempster-Shafer theory (applicable to the Bayesian theory as a special case). The 
two approaches for combining knowledge sources described in this chapter 
implicitly assume that all sources are equally credible. This is simply not the case 
when there are experts of various degrees of expertise and sensors with various 
degrees of reliability. This chapter also presents a method for taking into account 
such confidence factors during the handling of uncertainty. 
We begin the presentation of this chapter with an introduction of how the if-
then type rule syntax evolved from the syntax of classical logics. 
5.1 
Evolution of Rules 
In Chapter 3, on the classical logics, we presented a procedure for converting an 
arbitrary first-order sentence to its equivalent conjunctive normal form, where 
each conjunct is a clause or in clausal form. Such a clause is then interpreted as 
various types of program clauses or if-then type of rules. Figure 5-1 shows the 
overall evolution along with examples. 
Expert system rules and Prolog rules are just the definite version of general 
if-then type rules with attached uncertainty values. Rules normally used in rule-
based expert systems to represent expert knowledge are restricted to 
propositional syntax to avoid reasoning complexity and to enhance efficiency. 
Each such rule has an associated degree of uncertainty represented by elements 
from dictionaries (for example, certainty factor values in MYCIN expert system 
are drawn from the dictionary [ 1, 1]
−+
 (Heckerman and Shortliffe, 1991)). 
Arguments are more general than definite rules since the syntax of an argument 
against an option requires negative consequent. 

 
Logical Rules 
145 
 
Logical Formulae
Propositional, First, and Higher Order
First-Order Formulae
(
( )
( )
( , ))
( ( )
( )
( , )
( , ))
x
R x
P a
z Q z a
x z P x
R z
yQ y x
S z a
∀
¬
→
∧¬∃¬
∧
∀∀
∧¬
→∃
∨¬
Conjunctive Normal Forms
1
2
1
2
1
2
1
2
(( ( )
( ))
( ( )
( , ))
(
(
)
(
)
( (
,
),
)
(
, )))
x z x
x
R x
P a
R x
Q z a
P x
R x
Q f x x
x
S x a
∀∀∀∀
∨
∧
∨
∧
¬
∨
∨
∨¬
If-Then Rules
( )
( )
( ( , ), )
( , )))
P x
R y
Q f x y
x
S y a
¬
∨
∨
∨¬
Expert System Rules
Prolog Rules
Arguments
: 0.7
: 0.8
P
S
R
P
S
R
∧
→
∧
→
−
Clausal Forms
( )
( , )
( )
( , )
P x
S y a
R y
Q x y
∧
∧¬
→
1
2
( )
( , )
( , )
( ) :
( )
( , )
( , )
( ) :
P x
S y a
Q x y
R y
d
P x
S y a
Q x y
R y
d
∧
∧¬
→
∧
∧¬
→¬
( )
( , )
( )
( ( , ), ))
P x
S y a
R y
Q f x y
y
∧
→
∨
 
Figure 5-1: Evolution of rules with examples 
The if-then type rules in the decision-making context we will be dealing with 
in this chapter have the following general form:  
IF  
 
Events 
THEN   
Hypothesis (D) 
The above rule is interpreted as “If Events occur, then Hypothesis follows with a 
certain degree of uncertainty D.” In the absence of D, the rule becomes a definite 
rule, and reasoning with such rules can be performed using any of the resolution 
principles without the need to handle uncertainty.  If D is a probability value, 
then the rule is interpreted as “If Events occur, then the probability that 
Hypothesis will follow is D.” Events is a conjunction of propositional symbols, 
each representing an event, and Hypothesis can be a property symbol as in  
IF Rainy Weather AND Players Injured THEN Game On (–0.8) 
or an action symbol as in 
IF Rainy Weather AND Players Injured THEN Cancel Game (0.8) 
But the logical reasoning presented here does not distinguish between a property 
symbol and an action symbol. Degrees of uncertainty can be drawn from a 
quantitative dictionary (for example, [0,1]  or [ 1, 1]
−+
) or a qualitative dictionary 

146 
Decision-Making Agents 
 
 
(for example, {High, Medium, Low}). Our objective here is to interpret rules 
upon receiving evidence (for example, evidence on the state of weather) and then 
compute aggregated evidence on individual decision options (for example, Game 
On and Cancel Game) by handling uncertainty appropriately. In the next two 
sections, we present two approaches to handling uncertainty: Bayesian 
probability theory and the Dempster-Shafer theory of belief function. 
5.2 
Bayesian Probability Theory for Handling Uncertainty 
 In this approach, degrees of uncertainty are represented as a probability value 
from [0,1] . This approach describes the decision options from an application as a 
set of possible outcomes, termed “hypotheses.” Bayesian inference requires an 
initial (prior) probability for each hypothesis in the problem space. The inference 
scheme then updates probabilities using evidence. Each piece of evidence may 
update the probability of a set of hypotheses calculated via Bayesian rule, which 
is defined in the following:   
(
|
) ( )
(
|
)
( )
p B A p A
p A B
p B
=
 
where A and B are events that are not necessarily mutually exclusive, 
(
|
)
p A B  is 
the conditional probability of event A occurring given that event B has occurred, 
(
|
)
p B A  is the conditional probability of event B occurring given that event A 
has occurred, 
( )
p A  is the probability of event A occurring, and 
( )
p B  is the 
probability of event B occurring. In general, if there are m mutually exclusive and 
exhaustive hypotheses 
1,...,
m
H
H  (that is, 
1
(
)
1
m
i
i
p H
=
=
∑
 and n possible events 
1,...,
n
E
E  that can occur, then the probability of a hypothesis given some 
evidence is computed as follows: 
1
(
|
) (
)
(
|
)
(
|
) (
)
j
i
i
i
j
m
j
k
k
k
p E
H
p H
p H
E
p E
H
p H
=
=
∑
 
The Bayesian inference mechanism is illustrated in the following example. 
Example 
Consider a knowledge base consisting of the following three rules describing the 
chances of canceling a game based on a variety of evidence:. 

 
Logical Rules 
147 
 
IF
THEN
(0.7)
IF
THEN
(0.8)
IF
THEN
(0.9)
Heavy Rain
Cancelled
Players Injured
Cancelled
TerroristThreat
Cancelled
 
The two mutually exclusive and exhaustive hypotheses are: 
1
2
H
Cancelled
H
Cancelled
=
=¬
 
Three independent events on which evidence can be gathered are: 
1
2
3
E
Heavy Rain
E
Players Injured
E
TerroristThreat
=
=
=
 
Below, we illustrate how posterior probabilities of the hypotheses are updated as 
evidence on rain and player injury are gathered. 
Initial State 
Conditional probabilities 
(
|
)
i
j
p H
E
 for the hypotheses are inferred as follows 
from 
the 
three 
rules 
of 
the 
knowledge 
base 
and 
the 
relation 
1
2
(
|
)
(
|
)
1
j
j
p H
E
p H
E
+
= : 
1
1
2
1
1
2
2
2
1
3
2
3
(
|
)
(
|
)
0.7
(
|
)
(
|
)
0.3
(
|
)
(
|
)
0.8
(
|
)
(
|
)
0.2
(
|
)
(
|
)
0.9
(
|
)
p H
E
p Cancelled Heavy Rain
p H
E
p
Cancelled Heavy Rain
p H
E
p Cancelled Players Injured
p H
E
p
Cancelled Players Injured
p H
E
p Cancelled TerroistThreat
p H
E
=
=
=
¬
=
=
=
=
¬
=
=
=
(
|
)
0.1
p
Cancelled TerroistThreat
=
¬
=
 
Given prior probabilities 
(
)
j
p E
 and 
(
)
i
p H
 are listed below: 
1
1
2
2
3
(
)
(
)
0.20
(
)
(
)
0.2
(
)
(
)
0.10
(
)
(
)
0.8
(
)
(
)
0.01
p E
p Heavy Rain
p H
p Cancelled
p E
p Players Injured
p H
p
Cancelled
p E
p TerroistThreat
=
=
=
=
=
=
=
¬
=
=
=
 
The following conditional probabilities 
(
|
)
j
i
p E
H
 for evidence are computed via 
the application of Bayesian rule: 

148 
Decision-Making Agents 
 
 
1
1
1
2
2
1
2
2
3
1
(
|
)
(
|
)
0.7
(
|
)
(
|
)
0.075
(
|
)
(
|
)
0.4
(
|
)
(
|
)
0.025
(
|
)
(
|
)
0.045
(
p E
H
p Heavy Rain Cancelled
p E
H
p Heavy Rain
Cancelled
p E
H
p Players Injured Cancelled
p E
H
p Players Injured
Cancelled
p E
H
p TerroistThreat Cancelled
p
=
=
=
¬
=
=
=
=
¬
=
=
=
3
2
|
)
(
|
)
0.001
E
H
p TerroistThreat
Cancelled
=
¬
=
 
Evidence of Heavy Rain 
Assume that we first receive confirming evidence e on 
1
E  (that is, heavy rain). 
Then the probability 
(
|
)
p Cancelled Heavy Rain  directly provides the posterior 
probability 0.7 for the game being cancelled, which is a significant increase from 
the prior probability 
(
)
0.2
p Cancelled =
. If you are not completely sure about 
heavy rain, then soft evidence e can be encoded as the following likelihood: 
( |
)
0.95
( |
0.05
p e Heavy Rain
p e
Heavy Rain
⎡
⎤
⎡
⎤
=
⎢
⎥
⎢
⎥
¬
⎣
⎦
⎣
⎦
 
The posterior probability of 
1
E  upon receiving e is computed as shown below (α 
is the normalizing constant): 
(
| )
( |
)
(
)
0.95
0.2
0.83
0.05
0.8
0.17
p Heavy Rain e
p e Heavy Rain
p HeavyRain
α
α
=
×
×
⎡
⎤
⎡
⎤
⎡
⎤
=
×
×
=
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
⎣
⎦
⎣
⎦
 
You can then compute the posterior probability of the hypotheses as follows: 
(
| )
(
|
)
(
| )
(
|
)
(
| )
p Cancelled e
p Cancelled Heavy Rain
p HeavyRain e
p Cancelled
Heavy Rain
p
HeavyRain e
=
×
+
¬
×
¬
 
But we have 

 
Logical Rules 
149 
 
(
|
)
(
|
)
(
)
(
)
(1
(
|
))
(
)
(1
(
))
(1
0.7)
0.2
0.07
1
0.2
p Cancelled
Heavy Rain
p
Heavy Rain Cancelled
p Cancelled
p
Heavy Rain
p Heavy Rain Cancelled
p Cancelled
p Heavy Rain
¬
¬
×
=
¬
−
×
=
−
−
×
=
=
−
 
Therefore, 
(
| )
0.7
0.83
0.07
0.17
0.59
(
| )
0.41
p Cancelled e
p
Cancelled e
=
×
+
×
=
¬
=
 
Note that the probability of the game being cancelled has increased significantly 
from the earlier prior value 0.2, but not as much as to 0.7 when evidence on 
1
E  
was certain. 
Evidence of Player Injury 
In addition to the confirming evidence on 
1
E , suppose now we observe 
confirming evidence on 
2
E  (that is, player injury). The posterior probabilities are 
computed using the following formulae: 
(
)
(
)
(
)
(
)
(
)
1
2
1
2
2
1
2
1
,          
1, 2
i
i
i
j
j
j
p E E H
p H
p H E E
i
p E E H
p H
=
×
=
=
×
∑
 
Since 
1
E  and 
2
E  are independent, 
(
)
(
)
(
)
1
2
1
2
i
i
i
p E E H
p E H
p E H
=
×
. 
Therefore, 
(
)
,
0.7
0.4
0.2
0.97
0.7
0.4
0.2
0.075 0.025 0.8
p Cancelled Heavy Rain Players Injured
×
×
=
=
×
×
+
×
×
 
(
)
,
0.075 0.025 0.8
0.03
0.7
0.4
0.2
0.075 0.025 0.8
p
Cancelled Heavy Rain Players Injured
¬
×
×
=
=
×
×
+
×
×
 
Note that the posterior probability of the game being cancelled has increased 
further due to evidence of both heavy rain and player injury. This process of 
probability revision continues as evidence arrives.  

150 
Decision-Making Agents 
 
 
 
When the requisite initial assumptions (for example, prior probabilities, event 
independence) are fairly accurate, the Bayesian approach typically provides 
optimum results and is difficult to beat.  However, there is always some question 
as to how accurate our a priori assumptions are for any given situation we are 
modeling. Under such circumstances, where a priori assumptions are inaccurate, 
Bayesian methods may perform poorly. The Dempster-Shafer theory was 
specifically developed to mitigate these weaknesses. 
5.3 
Dempster-Shafer Theory for Handling Uncertainty 
The theory of belief functions (Shafer, 1976), also known as Dempster-Shafer 
theory, is a generalization of the Bayesian theory of subjective probability 
(mainly by virtue of its explicit definition of the concept of ignorance) to 
combine accumulative evidence or to change prior opinions in the light of new 
evidence. Whereas the Bayesian theory requires probabilities for each question of 
interest, belief functions allow us to base degrees of belief for one question (for 
example, whether the game is on) on probabilities for a related question. Arthur 
P. Dempster set out the basic ideas of the theory (Dempster, 1966) and then 
Glenn Shafer developed the theory further (Shafer, 1976). Briefly, the theory 
may be summarized as follows. 
Suppose expert X (for example, a weatherman, traffic cop, or grounds man) 
says that the game is not on due to heavy rain. The decision maker’s subjective 
probabilities for expert X being reliable or unreliable are 0.7 and 0.3. Now, 
expert X’s statement must be true if reliable, but not necessarily false if 
unreliable. The expert’s testimony therefore justifies 0.7 “degrees of belief” that 
the game is not on, but only a zero (not 0.3) degree of belief that the game is on. 
The numbers 0.7 and 0 together constitute a belief function.  
Suppose subjective probabilities were based on the decision maker’s 
knowledge of the frequency with which experts like X are reliable witnesses. 
70% of statements made would be true by reliable witnesses, n% would be true 
by unreliable ones, and (30
)%
n
−
 would be false statements by unreliable 
witnesses. 0.7 and 0 are the lower bounds of true probabilities (70
)/100
n
+
 and 
(30
)/100
n
−
 respectively. Thus, a single belief function is always a consistent 
system of probability bounds, but may represent contradictory opinions from 
various experts. For example, consider the belief function 0.7 and 0 from expert 
X’s opinion of the game not being on, and 0.8 and 0 from expert Y’s opinion of 

 
Logical Rules 
151 
 
the game being on. The lower bound of the true probability for the game not 
being on in the first case is 0.7, but the upper bound is 0.2 in the second case, 
yielding contradiction. 
Let Ω  be a finite set of mutually exclusive and exhaustive propositions, 
called 
the 
frame-of-discernment, 
about 
some 
problem 
domain 
(
{
,
}
Cancelled
Cancelled
Ω =
¬
 in our example decision making problem) and 
( )
Π Ω  is be the power set of Ω . A basic probability assignment (BPA) or mass 
function is the mapping 
:
( )
[0,1]
m Π Ω →
 
which is used to quantify the belief committed to a particular subset A of the 
frame of discernment given certain evidence. The probability number 
( )
m A , the 
mass of A, says how much belief there is that some member of A is in fact the 
case, where 
(
)
0
m Φ =
 and 
( )
1
A
m A
⊆Ω
=
∑
 
The value 0 indicates no belief and the value 1 indicates total belief, and any 
values between these two limits indicate partial beliefs. If the probability number 
p for only a partial set A of hypotheses is known then the residual complementary 
probability number 1
p
−
 is assigned to the frame-of-discernment, thus allowing 
the representation of ignorance. A basic probability assignment m is Bayesian if 
( )
0
m A =
 for every non-singleton set A. For any set A ⊆Ω  for which 
( )
0
m A ≠
, 
A is called a focal element.  
The measure of total belief committed to A ⊆Ω  can be obtained by 
computing the belief function Bel for A ⊆Ω  which simply adds the mass of all 
the subsets of A: 
( )
( )
B
A
Bel A
m B
⊆
= ∑
 
A single belief function represents the lower limit of the true probability and the 
following plausibility function provides the upper limit of the probability: 
( )
( )
1
(
)
c
B
A
Pl A
m B
Bel A
∩≠Φ
=
= −
∑
 
Mass can be recovered from belief function as follows: 
|
|
( )
( 1)
( )
A B
A
B
m B
Bel A
−
⊆
=
−
∑
 

152 
Decision-Making Agents 
 
 
So there is a one-to-one correspondence between the two functions m and Bel. 
Two independent evidences expressed as two basic probability assignments 
1
m  
and 
2
m can be combined into a single joined basic assignment 
1,2
m
 by 
Dempster’s rule of combination: 
1
2
1,2
1
2
( )
( )
( )
1
( )
( )
0
B
C
A
B
C
m B m C
A
m
A
m B m C
A
∩
=
∩
=Φ
⎧
⎪
≠Φ
⎪
=
−
⎨
⎪
⎪
= Φ
⎩
∑
∑
 
Example 
In order to illustrate the Dempster-Shafer theory in the context of our example, 
we consider only the following three different expert rules for canceling the game 
along with the degrees of reliability on experts from whom the rules have been 
acquired: 
IF Heavy Rain THEN Game Cancelled 
Expert: Grounds Man; Reliability: 0.7 
 
IF Players Injured THEN Game Cancelled 
Expert: Club Physiotherapist; Reliability: 0.8 
 
IF Attack Threat THEN Game Cancelled 
Expert: Homeland Security; Reliability: 0.9 
Note that Dempster-Shafer theory requires that evidences to be combined are 
independent. In the above set of rules the potential usable evidences (rainy 
condition, player injury, and terrorist attack threat) are essentially so. 
In case of rain evidence, the values 0.7 and 0 together constitute a belief 
function. The focal element is {
}
Cancelled  and the mass distribution is 
1({
})
0.7
m
Cancelled
=
. We know nothing about the remaining probability so it is 
allocated to the whole frame of discernment as 
1( )
0.3
m Ω =
, where 
{
,
}
Cancelled
Cancelled
Ω =
¬
. Evidence of player injury provides the focal 
element {
}
Cancelled  other than Ω , with 
2({
})
0.8
m
Cancelled
=
. The remaining 

 
Logical Rules 
153 
 
probability, as before, is allocated to the whole frame of discernment as 
2( )
0.2
m Ω =
. Dempster’s rule can then be used to combine the masses as 
follows: 
Can
Cancelled
−
 
2({
})
0.8
m
Can
=
 
2( )
0.2
m Ω =
 
1({
})
0.7
m
Can
=
 
1,2({
})
0.56
m
Can
=
 
1,2({
})
0.14
m
Can
=
 
1( )
0.3
m Ω =
 
1,2({
})
0.24
m
Can
=
 
1,2( )
0.06
m
Ω =
 
Now, 
({
})
0.56
0.24
0.14
0.94
Bel
Cancelled
=
+
+
=
. Therefore, the combined 
belief and plausibility are computed in the following table: 
Focal Element (A) 
( )
Bel A  
( )
Pl A  
{
}
Cancelled  
0.94 
1.0 
Ω  
1.0 
1.0 
 
Example 
Here is a more interesting example with a third decision option for the game to 
be 
delayed. 
Therefore, 
the 
frame 
of 
discernment 
is 
{
,
,
}
On Cancelled Delayed
Ω =
. We consider the following set of expert rules 
along with the degrees of reliability: 
IF Heavy Rain THEN NOT On 
Expert: Grounds Man; Reliability: 0.7 
 
IF Players Injured THEN Cancelled  
Expert: Club Physiotherapist; Reliability: 0.8 
 
IF Financial Crisis THEN NOT Cancelled 
Expert: Club Administrator; Reliability: 0.6 
Here also the potential usable evidences (rainy condition, player injury, and club 
financial situation) are essentially independent. 

154 
Decision-Making Agents 
 
 
In case of rain evidence, the game not being on (that is, cancelled or delayed) 
and the values 0.7 and 0 together constitute a belief function. The focal element 
other than Ω  is {
,
}
Cancelled Delayed  and the mass is distributed to it as 
1({
,
})
0.7
m
Cancelled Delayed
=
. We know nothing about the remaining 
probability so it is allocated to the whole frame of discernment as 
1( )
0.3
m Ω =
. 
There is also evidence that the current financial situation of the club is bad, 
resulting in 0.6 subjective probability that the game will not be cancelled in this 
situation. The new evidence suggests the focal element {
,
}
On Delayed  other than 
Ω  with 
2({
,
})
0.6
m
On Delayed
=
. The remaining probability, as before, is 
allocated to the whole frame of discernment as 
2( )
0.4
m Ω =
. Considering that 
the rainy condition and the current financial situation are independent of each 
other, Dempster’s rule can then be used to combine the masses as follows: 
Can
Cancelled
Del
Delayed
−
−
 
2({
,
})
0.6
m
On Del
=
 
2( )
0.4
m Ω =
 
1({
,
})
0.7
m
Can Del
=
 
1,2({
})
0.42
m
Del
=
 
1,2({
,
})
0.28
m
Can Del
=
 
1( )
0.3
m Ω =
 
1,2({
,
})
0.18
m
On Del
=
 
1,2( )
0.12
m
Ω =
 
Therefore the combined belief and plausibility are computed in the following 
table: 
Focal Element (A) 
( )
Bel A  
( )
Pl A  
{
}
Delayed  
0.42 
1.0 
{
,
}
On Delayed  
0.60 
1.0 
{
,
}
Cancelled Delayed  
0.70 
1.0 
Ω  
1.0 
1.0 
 The basic probability assignments 
1
m  and 
2
m  are different but consistent, and 
therefore 
the 
degrees 
of 
belief 
in 
both 
{
,
}
Cancelled Delayed  
and 
{
,
}
On Delayed being true (that is, the game is delayed) is the product of 
1({
,
})
m
Cancelled Delayed
 and 
2({
,
})
m
On Delayed
, or 0.42. 

 
Logical Rules 
155 
 
Finally, the player injury situation suggests the focal element {
}
Cancelled  
and Ω  with 
3({
})
0.8
m
Cancelled
=
 and 
3( )
0.2
m Ω =
. The Dempster rule of 
combination applies as before, but with one modification. When the evidence is 
inconsistent, their products of masses are assigned to a single measure of 
inconsistency, say k. 
Can
Cancelled
Del
Delayed
−
−
 
3({
})
0.8
m
Can
=
 
3( )
0.2
m Ω =
 
1,2({
})
0.42
m
Del
=
 
0.336
k =
 
({
})
0.084
m
Del
=
 
1,2({
,
})
0.18
m
On Del
=
 
0.144
k =
 
({
,
})
0.036
m On Del
=
 
1,2({
,
})
0.28
m
Can Del
=
 
({
})
0.224
m Can
=
 
({
,
})
0.056
m Can Del
=
 
1,2( )
0.12
m
Ω =
 
({
})
0.096
m Can
=
 
( )
0.024
m Ω =
 
The total mass of evidence assigned to inconsistency k is 0.336 + 0.144 = 0.48. 
The normalizing factor is 1
0.52
k
−
=
. The resulting masses of evidence are as 
follows: 
({
})
(0.224
0.096)/0.52
0.62
({
})
0.084/0.52
0.16
({
,
})
0.036/0.52
0.07
({
,
})
0.056/0.52
0.11
( )
0.024/0.52
0.04
m Cancelled
m
Delayed
m On Delayed
m Cancelled Delayed
m
=
+
=
=
=
=
=
=
=
Ω =
=
 
Therefore, the combined belief and plausibility are computed in the following 
table: 
Focal Element (A) 
( )
Bel A  
( )
Pl A  
{
}
Cancelled  
0.62 
0.77 
{
}
Delayed  
0.16 
0.38 
{
,
}
On Delayed  
0.23 
0.38 
{
,
}
Cancelled Delayed  
0.89 
1.0 
Ω  
1.0 
1.0 

156 
Decision-Making Agents 
 
 
Hence, the most likely hypothesis is the cancellatation of the game. 
 
Let us consider two examples to illustrate two special cases for evidence 
aggregation. 
Example 
Hypothetically, consider the case when the set of focal elements of the basic 
probability distribution 
2
m is exactly the same as 
1
m . The evidence combination 
table is shown below: 
Can
Cancelled
Del
Delayed
−
−
 
2({
,
})
0.6
m
Can Del
=
 
2( )
0.4
m Ω =
 
1({
,
})
0.7
m
Can Del
=
 
1,2({
,
})
0.42
m
Can Del
=
 
1,2({
,
})
0.28
m
Can Del
=
 
1( )
0.3
m Ω =
 
1,2({
,
})
0.18
m
Can Del
=
 
1,2( )
0.12
m
Ω =
 
 
Now,  
({
,
})
0.42
0.18
0.28
0.88
0.6
0.7
0.6
0.7
Bel
Cancelled Delayed
=
+
+
=
=
+
−
×
 
In general, when two mass distributions 
1
m  and 
2
m  agree on focal elements, then 
the combined degree of belief on a common focal element is 
1
2
1
2
p
p
p
p
+
−
×
, 
where 
1p  and 
2p  are mass assignments on the focal element by the two 
distributions. 
As opposed to agreeing on focal elements, if 
2
m  is contradictory to 
1
m  then 
an example evidence combination is shown below:  
Can
Cancelled
Del
Delayed
−
−
 
2({
})
0.6
m
On
=
 
2( )
0.4
m Ω =
 
1({
,
})
0.7
m
Can Del
=
 
0.42
k =
 
1,2({
,
})
0.28
m
Can Del
=
 

 
Logical Rules 
157 
 
1( )
0.3
m Ω =
 
1,2(
)
0.18
m
On =
 
1,2( )
0.12
m
Ω =
 
In this case,  
({
,
})
0.28/(1
0.42)
0.7(1
0.6)/(1
0.42)
Bel
Cancelled Delayed
=
−
=
−
−
 
 
In general, when two mass distributions 
1
m  and 
2
m  are contradictory, then 
the combined degree of belief on the focal element for 
1
m  is 
1
2
1
2
(1
)/(1
)
p
p
p
p
−
−
×
 and the combined degree of belief on the focal element 
for 
2
m  is 
2
1
1
2
(1
)/(1
)
p
p
p
p
−
−
×
, where 
1p  and 
2p  are mass assignments on the 
focal element by the two distributions. 
5.4 
Measuring Consensus 
The need to measure consensus in a decision-making context with the Dempster-
Shafer theory as the underlying theoretical foundation is first explained in this 
section with the help of a concrete example. 
Example 
Suppose some source of evidence heavily suggests against the game being 
cancelled (that is, the game is either on or delayed) and the values 0.9 and 0 
together constitute a belief function. The focal element is {
,
}
On Delayed  and the 
mass distribution is 
1({
,
})
0.9
m
On Delayed
=
. We know nothing about the 
remaining probability 0.1 so it is allocated to the whole frame of discernment 
{
,
,
}
On Cancelled Delayed
Ω =
. Another source of evidence heavily suggests 
against the game being on. The mass distribution in this case is 
2({
,
})
0.9
m
Cancelled Delayed
=
. The remaining probability, as before, is 
allocated to the whole frame of discernment. Dempster’s rule can then be used to 
combine the masses as follows: 
Can
Cancelled
Del
Delayed
−
−
 
2({
,
})
0.9
m
Can Del
=
 
2( )
0.1
m Ω =
 
1({
,
})
0.9
m
On Del
=
 
1,2({
})
0.81
m
Del
=
 
1,2({
,
})
0.09
m
On Del
=
 

158 
Decision-Making Agents 
 
 
1( )
0.1
m Ω =
 
1,2({
,
})
0.09
m
Can Del
=
 
1,2( )
0.01
m
Ω =
 
Therefore, the combined belief and plausibility are computed in the following 
table: 
Focal Element (A) 
( )
Bel A  
( )
Pl A  
{
}
Delayed  
0.81 
1.0 
{
,
}
On Delayed  
0.09 
0.91 
{
,
}
Cancelled Delayed  
0.09 
0.91 
Ω  
1.0 
1.0 
 
The above result is counterintuitive, given that none of the two sources of 
evidence explicitly supported the game being delayed. Moreover, the table above 
does not reflect the underlying high degree of disagreement. The consensus 
metric provided below will be able to highlight this case. The “entropy”-based 
algorithm presented here generates a very low (2.3 out of 3) degree of consensus 
(zero or minimum entropy is the highest consensus). The consensus-measuring 
criterion is based on the generalization of the concept of entropy from point 
function to set function (Stephanou and Lu, 1988). This generalization is 
composed of three measurements: belief entropy, core entropy, and partial 
ignorance.  
Suppose Ω  is a frame of discernment, m is a basic probability assignment, 
and 
1
{
,...,
}
n
A
A
=
F
 is the set of focal elements. 
The belief entropy of the pair 
,
m
〈
〉
F  is defined as follows (log is base 2):  
1
( , )
(
)log
(
)
n
b
i
i
i
E m
m A
m A
=
= −∑
F
 
The belief entropy is a measure of the degree of confusion in the decision-
maker’s knowledge about the exact fraction of belief that should be committed to 
each focal element in F. Thus, the belief entropy is naturally equal to zero if the 
entire belief is committed to a single focal element, that is, 
(
)
1
k
m A
= , for some 
k. The maximum belief entropy occurs when belief is divided in equal fractions 
among the focal elements. 
The core entropy of the pair 
,
m
〈
〉
F  is defined as follows:  

 
Logical Rules 
159 
 
1
( , )
log
n
c
i
i
i
E m
r
r
=
= −∑
F
 
where  
1
||
|| 1
(||
|| 1)
i
i
n
i
j
A
r
A
=
−
=
−
∑
 
and ||
||
iA
 is the cardinality of 
iA  ( 0log0  is considered 0). The core entropy is a 
measure of the degree of confusion in the decision-maker’s knowledge of which 
focal elements the true value might be in. If belief is committed to a single focal 
element 
kA , then 
kr  is one, and therefore the core entropy is equal to zero. The 
core entropy is maximum when belief is divided among a number of focal 
elements with the same cardinality. 
The partial ignorance of the pair 
,
m
〈
〉
F  is defined as follows:  
1
( , )
(
) (
)
n
i
i
i
i
I m
m A s A
=
= ∑
F
 
where 
||
|| 1
(
)
||
|| 1
i
i
i
A
s A
−
=
Ω − and ||
||
iA
 is the cardinality of 
iA , and the assumption is 
that the frame of discernment has more than one element. Partial ignorance is a 
measure of the inability to confine the true value within a small-sized focal 
element. Consequently, partial ignorance is large when large belief is committed 
to large focal elements. It is zero when the entire belief is committed to a 
singleton. 
The generalized entropy of the pair 
,
m
〈
〉
F  is then defined as follows:  
( , )
( , )
( , )
( , )
b
c
E m
E m
E m
I m
=
+
+
F
F
F
F  
Now suppose that we have two basic probability assignments 
1
m  and 
2
m , 
corresponding to two different expert knowledge sources, defining focal elements 
{ ,
}
A Ω  and { ,
}
B Ω  respectively, where A ⊂Ω  and B ⊂Ω . Let us consider four 
different cases as shown in Figure 5-2 (Stephanou and Lu, 1988). 

160 
Decision-Making Agents 
 
 
A
B
Case 1
Case 2
Case 3
Case 4
Ω
Ω
Ω
Ω
A
B
A
B
A
A
B
 
Figure 5-2: Combining expert knowledge sources 
It can be shown that the generalized entropy of the knowledge sources via 
Dempster’s rule of combination for each of the first three cases is smaller than 
the sum of the individual entropies of the knowledge sources. In other words, the 
following relation holds for the first three cases: 
12
1
2
(
, )
(
, )
(
, )
E m
E m
E m
≤
+
F
F
F  
The above implies that the pooling of two concordant bodies of evidence reduces 
entropy. The disjointedness between A and B in the last case indicates that there 
are two bodies of evidence supporting two conflicting propositions, and hence 
the combined generalized entropy does not necessarily get reduced. 
Example 
To illustrate reduction of entropy, consider the frame of discernment 
{
,
,
}
On Cancelled Delayed
Ω =
 the following two focal elements, and their 
corresponding basic probability assignments: 
1
2
1
1
2
1
{
,
}
{
,
}
{ ,
}
{ ,
}
( )
0.7,
( )
0.3
( )
0.6,
( )
0.4
A
Cancelled Delayed
B
On Delayed
A
B
m A
m
m B
m
=
=
=
Ω
=
Ω
=
Ω =
=
Ω =
F
F
 
The entropies and ignorance are computed as follows: 

 
Logical Rules 
161 
 
Belief Entropy: 
1
1
1
1
1
1
2
2
2
2
2
2
(
,
)
( )log
( )
( )log
( )
0.7log0.7
0.3log0.3
0.88
(
,
)
( )log
( )
( )log
( )
0.6log0.6
0.4log0.4
0.97
b
b
E m
m A
m A
m
m
E m
m
A
m
A
m
m
= −
−
Ω
Ω
= −
−
=
= −
−
Ω
Ω
= −
−
=
F
F
 
Core Entropy:  
1
1
1
1
1
1
2
2
2
2
2
2
(
,
)
( )log ( )
( )log ( )
1
1
2
2
log
log
0.92
3
3
3
3
(
,
)
( )log
( )
( )log
( )
1
1
2
2
log
log
0.92
3
3
3
3
c
c
E m
r A
r A
r
r
E m
r A
r A
r
r
= −
−
Ω
Ω
= −
−
=
= −
−
Ω
Ω
= −
−
=
F
F
 
Partial Ignorance:  
1
1
1
1
1
2
2
2
2
1
2
2
(
,
)
( ) ( )
( )
( )
1
2
0.7
0.3
0.65
2
2
(
,
)
( ) ( )
( )
( )
1
2
0.6
0.4
0.7
2
2
I m
m A s A
m
s
I m
m B s B
m
s
=
+
Ω
Ω
=
⋅
+
⋅
=
=
+
Ω
Ω
=
⋅
+
⋅
=
F
F
 
Generalized Entropy:  
1
1
2
2
(
,
)
0.88
0.92
0.65
2.45
(
,
)
0.97
0.92
0.70
2.59
E m
E m
=
+
+
=
=
+
+
=
F
F
 
Now, consider Case 3 in Figure 5-1. The Dempster combination rule yields a 
new set of focal elements and basic probability assignments as follows: 
1,2
1,2
1,2
1,2
{
}
{ , ,
,
}
( )
0.18,
( )
0.28,
(
)
0.42,
( )
0.12
A
B
Delayed
A B A
B
m
A
m
B
m
A
B
m
∩
=
=
∩
Ω
=
=
∩
=
Ω =
F
 
The entropies and ignorance for the combined set of focal elements are computed 
as follows: 

162 
Decision-Making Agents 
 
 
Belief Entropy: 
1,2
(
, )
0.18log0.18
0.28log0.28
0.42log0.42
0.12log0.12
1.85
b
E m
= −
−
−
−
=
F
 
Core Entropy: 
1,2
1
1
1
1
2
2
(
, )
log
log
log
1.5
4
4
4
4
4
4
c
E m
= −
−
−
=
F
 
Partial Ignorance: 
1,2
1
1
0
2
(
, )
0.18
0.28
0.42
0.12
0.35
2
2
2
2
I m
=
⋅
+
⋅
+
⋅
+
⋅
=
F
 
Generalized Entropy: 
1,2
(
, )
1.85
1.50
0.35
3.70
E m
=
+
+
=
F
 
Thus we observe that the relation 
12
1
2
(
, )
(
, )
(
, )
E m
E m
E m
≤
+
F
F
F  holds. 
 
5.5 
Combining Sources of Varying Confidence 
Here we present a credibility transformation function (Yager, 2004) for 
combining sources with various confidences. This approach discounts the 
evidence with a credibility factor α  and distributes remaining evidence 1 α
−
 
equally among the n elements of the frame of discernment. The transformed 
credibility function takes the following form: 
1
( )
( )
m
A
m A
n
α
α
α
−
=
⋅
+
 
In the Bayesian formalism where each focal element A is a singleton set, we 
distribute the remaining evidence 1 α
−
 as per their prior probabilities. In other 
words, we modify the credibility function as the following: 
0
1
( )
( )
( )
p
A
p A
p
A
n
α
α
α
−
=
⋅
+
⋅
 
When prior probabilities are uniformly distributed among elements of the frame 
of discernment, this case becomes a special case of the credibility function for the 
Dempster-Shafer case. Moreover, if the prior probability of A is zero, then it 
remains zero after the transformation via the credibility function.  

 
Logical Rules 
163 
 
Example 
1
{
,
,
}
{
,
}
( )
0.7
On Cancelled Delayed
A
On Cancelled
m A
Ω =
=
=
 
If the reliability of the source of 
1
m  is 0.8, then  
0.8
1
0.8
( )
0.7 0.8
0.66
2
m
A
−
=
⋅
+
=
 
0.8
1
0.8
( )
0.3 0.8
0.34
2
m
−
Ω =
⋅
+
=
 
 
5.6 
Advantages and Disadvantages of Rule-Based Systems 
Subject matter experts usually express their predictive and actionable domain 
knowledge in terms of expressions such as “this action should be taken under that 
condition,” “this situation means that,” and so on. Such knowledge, with clearly 
separated antecedent and consequent parts, can be easily transformed to rules. 
Moreover, each rule is a self-documented, independent piece of knowledge that 
can be maintained and acquired in a local context without analyzing its global 
impact. 
Rule-based systems provide both data-driven reasoning (that is, forward 
chaining) and goal-driven reasoning (that is, backward chaining as in SLD 
resolution presented in the last chapter). A derivation via each such reasoning 
method is transparent in the sense that derivation steps can be laid out in 
sequence to provide a basis for explaining an answer to a query. Rules can be 
attached with supports drawn from various types of qualitative and quantitative 
dictionaries, other than just probabilities, provided appropriate algebras exist for 
manipulating dictionary terms (for example, mass values in Dempster-Shafer, 
certainty factors). Rules can also be learned, given sufficient training instances, 
using standard machine learning techniques, such as decision trees for learning 
propositional rules and inductive logic programming for learning first-order rules 
(Mitchell, 1997).  
Since each rule is self-documented, a special reasoning procedure is required 
to implement abductive or diagnostic reasoning (this kind of reasoning is an 
integral part of evidence propagation algorithms for belief networks). Moreover, 
a knowledge base consisting of many isolated rules does not provide a global 

164 
Decision-Making Agents 
 
 
view of the domain that it is modeling in terms of links and relationships among 
variables. 
5.7 
Background and Further Readings 
Maturity and high-level of interests in the area of rule-based expert system have 
culminated dozens of text books. (Jackson, 2002) provides a good background of 
the subject. Shafer’s own book (1976) and the edited volume by Yager et al. 
(1994) are good sources on Dempster-Shafer theory of belief function. The paper 
by Stephanou and Lu (1988) is the source of the entropy-based consensus 
measurement presented here. See (Smets, 1991) for various other formalisms for 
handling uncertainty, including fuzzy logic (Zadeh, 1965) and possibility theory 
(Zadeh, 1978; Dubois and Prade, 1988), and their relations to the theory of belief 
function. 

165 
Chapter 6 
 
Bayesian Belief Networks 
 
 
 
 
This chapter presents the Bayesian belief network (or simply belief network; also 
known as causal network or probabilistic network) technology for developing the 
probabilistic foundation of epistemic models for decision-making agents. A 
straightforward Bayesian approach to probabilistic epistemic modeling via a set 
of random variables is too inefficient, because problems with a large number of 
variables require maintaining large joint distributions to compute posterior 
probabilities based on evidence. The belief network technology avoids this via 
certain independence assumptions based on which application network models 
are constructed.    
In this chapter, we start by defining belief networks and illustrating the 
concept of conditional independence within belief networks. Then we presents 
the evidence, belief, and likelihood concepts. Next, we present two propagation 
algorithms to compute variables’ posterior probabilities based on evidence: one 
for the class of networks without cycles, and another for a class of networks with 
restrictive cycles, and analyze the complexities of these two algorithms. 
The task of constructing a belief network can be divided into two subtasks: 1) 
specification of the causal structure among variables of the network; and 2) 
specification of prior and conditional probabilities. Usually, automated learning 
of the structure from the data is much harder than learning probabilities, although 
techniques exist for automated learning in each of these areas. However, the 
structure is much easier to elicit from experts than probabilities, especially when 
nodes have several parents (in which case their conditional probabilities become 
large and complex). We provide some guidance for acquiring network 
probabilities, including the noisy-or technique. Finally, we conclude the chapter 
by summarizing the overall advantages and disadvantages of belief network 
technology and tools. 

166 
Decision-Making Agents 
 
 
6.1 
Bayesian Belief Networks 
A Bayesian belief network is a network (that is, a graph consisting of nodes 
and links) with the following interpretation:  
• 
Each node in the network represents a random variable that can take on 
multiple discrete values; these values are mutually exclusive events 
constituting the sample space over which the random variable is defined. 
The terms node and variable are synonymous in the context of a belief 
network and are often used interchangeably.  
• 
Each link in the network represents a relation or conditional dependence 
between the two variables it connects, and an associated conditional 
probability table (CPT) quantifies the relation associated with the link. 
• 
There is a notion of causality between two linked variables, so links with 
explicit direction (represented by arrows) are drawn from “cause” nodes 
to “effect” nodes. 
• 
The network is based on some marginal and conditional independence 
assumptions among nodes. These assumptions are described in the next 
section. 
• 
The state of a node is called a belief, and reflects the posterior probability 
distribution of the values associated with that node, given all the a priori 
evidence. 
The probabilities in a CPT are typically acquired from subject matter experts 
in the domain, but can also be learned automatically given a large enough 
number of training instances. The causality restriction can sometimes be relaxed 
by allowing links between a pair of nodes that are simply correlated and the 
direction of the arrow between the two nodes is decided based on the ease of 
acquisition of the probabilities in the CPT. A typical causal relationship between 
two variables inevitably brings the temporal dimension into the modeling 
problem. We avoid building any formal dynamic or temporal belief networks 
within which the state of a variable is allowed to change over time by modeling a 
simple snapshot of the problem domain at a particular time. 
Example 
An example belief network is shown in Figure 6-1. It illustrates causal influences 
on whether or not a game is going to be played, given the current weather and 
other related sources of evidence. Note that there are many ways to approach 

 
Bayesian Belief Networks 
167 
 
modeling a decision-making problem with belief networks. The selection of 
random variables and their granularities and interdependences are largely 
subjective, but should be driven by the problem solving requirements. (Please 
note that we are not emphasizing any specific modeling methodology; the 
specific belief network in Figure 6-1 is constructed to illustrate the technology 
via a single unified example.) 
Weather
Field
Game
Ticket Sale
Concession
Sprinkler
high
low
none
high
low
none
on
cancelled
muddy
normal
burst pipe
normal
open
disruptive
closed
sunny
rain
snow
Economy
good
bad
Roads
 
Figure 6-1: An example belief network 
Each node in the belief network in Figure 6-1 and its associated mutually 
exclusive states (possible values of the random variable representing the node) 
are described below (from top to bottom): 
• 
Weather: Overall weather condition during the game – {sunny, rain, 
snow} 
• 
Sprinkler: Current state of the water sprinkler – {normal, burst pipe} 
• 
Roads: Condition of city roads before the game – {open, disruptive, 
closed} 

168 
Decision-Making Agents 
 
 
• 
Field: Overall condition of the field for the game – {muddy, normal} 
• 
Game: Whether the game is on or cancelled – {on, cancelled} 
• 
Economy: Current economic condition of the country – {good, bad} 
• 
Ticket Sale: Level of ticket sale at the gate – {high, low, none} 
• 
Concession: Level of concession (for example, snacks and drinks) inside 
the stadium – {high, low, none} 
For example, all states in the random variable Weather are mutually exclusive, 
that is, the overall weather condition cannot be both sunny and rain at the same 
time. But a probability density function f of the variable Weather with the domain 
{sunny, rain, snow} can be defined and interpreted as “
( )
f x is the probability 
that Weather will assume the value x.” One such function can be defined as 
follows: 
(
)
0.55,
(
)
0.15,
(
)
0.3
f sunny
f rain
f snow
=
=
=
 
The causal influences by numbers (as shown in Figure 6-2) along the directions 
of the arrows are defined as follows: 
• 
Weather condition determines the condition of the roads. For example, 
snowy weather is likely to cause roads to be closed. 
• 
Weather condition and the sprinkler state together determine the 
suitability of the field condition. For example, a rainy day is likely to 
cause the field to be muddy and therefore unsuitable for a game. A burst 
pipe is likely to have the same effect. 
• 
Road and field conditions together determine the status of a game, that is, 
whether it is going to be played or not. For example, the disruptive 
condition of the roads is likely to cause the game to be cancelled as is an 
unsuitable field condition. 
• 
Condition of the roads, game status, and the country’s economy together 
determine the ticket sale volume. For example, when the game is on, the 
ticket sale is likely to be low due to disruptive road conditions, since 
these conditions might also cause low attendance (not modeled 
explicitly). The volume of ticket sale is likely to be even lower if the 
economy is bad. 
• 
Ticket sale determines the concession volume, since the presence of 
fewer customers (low ticket sales) is likely to cause a lower volume of 
concession sales. 

 
Bayesian Belief Networks 
169 
 
Weather
Field
Game
Ticket Sale
Concession
Sprinkler
Economy
Roads
high
low
none
high
low
none
sunny
rain
snow
burst pipe
normal
muddy
normal
open
disruptive
closed
on
cancelled
good
bad
(1)
(2)
(3)
(4)
(5)
(
|
)
p Roads Weather
(
)
p Weather
(
|
,
)
p Game Roads Field
(
)
p Sprinkler
(
|
,
)
p Field Weather Sprinkler
(
|
,
,
)
p Ticket Sale Roads Game Economy
(
|
)
p Concession Ticket Sale
(
)
p Economy
 
Figure 6-2: Belief network with prior and conditional probabilities 
The three prior probabilities (corresponding to the three nodes without 
parents) and five conditional probabilities of the belief network in Figure 6-2 are 
provided in the following tables. 
sunny 
0.55 
rain 
0.15 
Weather 
snow 
0.3 
Table 6-1: 
(
)
p Weather  
burst pipe 
0.01 
Sprinkler 
normal 
0.99 
Table 6-2: 
(
)
p Sprinkler  

170 
Decision-Making Agents 
 
 
good
0.6
Economy 
bad 
0.4
Table 6-3: 
(
)
p Economy  
Weather 
sunny
rain
snow
open 
0.9 
0.7 
0.8 
disruptive
0.09 
0.2 
0.15 
Roads 
closed 
0.01 
0.1 
0.05 
Table 6-4: 
(
|
)
p Roads Weather  
Weather 
sunny 
rain 
snow 
Sprinkler 
burst 
pipe 
normal 
burst 
pipe 
normal 
burst 
pipe 
normal 
muddy 
0.6 
0.95 
0.1 
0.3 
0.2 
0.4 
Field 
normal 
0.4 
0.05 
0.9 
0.7 
0.8 
0.6 
Table 6-5: 
(
|
,
)
p Field Weather Sprinkler  
Roads 
open 
disruptive 
closed 
Field 
muddy 
normal 
muddy 
normal 
muddy 
normal 
on 
0.99 
0.1 
0.8 
0.1 
0.01 
0 
Game 
cancelled 
0.01 
0.9 
0.2 
0.9 
0.09 
1 
Table 6-6: 
(
|
,
)
p Game Roads Field  

 
Bayesian Belief Networks 
171 
 
Roads 
open 
disruptive 
closed 
Game 
on 
cancell
ed 
on 
cancell
ed 
on 
cancelle
d 
Economy 
go
od 
ba
d 
go
od
ba
d 
go
od
ba
d 
go
od 
ba
d 
go
od 
ba
d 
go
od 
ba
d 
high 
.9 
.8 
0 
0 
.2 
.1 
0 
0 
.09 
0 
0 
0 
low 
.1 
.2 
0 
0 
.8 
.9 
0 
0 
.9 
.9 
0 
0 
Ticket 
Sale 
none 
0 
0 
1 
1 
0 
0 
1 
1 
.01 
.1 
1 
1 
Table 6-7: 
(
|
,
,
)
p Ticket Sale Roads Game Economy  
Ticket Sale 
high
low
none
high 
1 
0 
0 
low 
0 
1 
0 
Concession 
none
0 
0 
1 
Table 6-8: 
(
|
)
p Concession Ticket Sale  
 
This belief network and its associated CPTs shown above are used 
throughout the rest of this chapter to illustrate algorithms and other related 
concepts. Note that the CPT in Table 6-8 is in the form of an identity matrix, and 
guarantees the perfect causal relationship between the states of Concession and 
Ticket Sale variables. 
6.2 
Conditional Independence in Belief Networks 
Two random variables Y and Z are said to be (marginally) independent, denoted 
as Y
Z
⊥
, if  
( , )
p Y Z  = 
( ) ( )
p Y p Z  
for any combination of values for the variables Y and Z. The variable Y is 
conditionally independent of Z given another variable X, denoted as 
|
Y
Z X
⊥
, if 

172 
Decision-Making Agents 
 
 
(
)
,
|
p Y Z X  = (
) (
)
|
|
p Y X p Z X . 
Therefore, 
(
)
|
,
p Y Z X  = 
(
)
(
)
,
|
|
p Y Z X
p Z X
 = 
(
) (
)
(
)
|
|
|
p Y X p Z X
p Z X
 = (
)
|
p Y X  
Similarly, 
(
)
| ,
p Z Y X  = 
(
)
|
p Z X . Note that marginal independence (no 
conditioning) does not imply conditional independence; nor does conditional 
independence imply marginal independence. 
Figure 6-3 represents conditional independence in a chain fragment of a 
belief network where a node X is between two other nodes Y and Z. We factorize 
the joint probability distribution of the variables X, Y, and Z as follows: 
(
)
, ,
p X Y Z  = (
) (
)
|
,
,
p Z X Y p X Y  = (
) (
) ( )
|
|
p Z X p X Y p Y  
X
Z
Y
 
Figure 6-3: Conditional independence in chain fragment: Z is conditionally 
independent of Y given X 
Example 
Figure 6-4 shows an example instantiation of conditional independence in a chain 
network fragment as shown in Figure 6-3. The variables X, Y, and Z represent 
respectively the sprinkler state, field condition, and the status of the game, 
respectively. If we observe, with complete certainty, that the field condition is 
unsuitable, then the probability of the game being cancelled is determined. 

 
Bayesian Belief Networks 
173 
 
Therefore, confirmation that the sprinkler is normal or that it has a burst pipe will 
not change the probability of the game status, and vice versa. 
Field
Game
Sprinkler
 
Figure 6-4: Example conditional independence in a chain network fragment: 
Sprinkler and Game are conditionally independent given Field  
 
Figure 6-5 represents conditional independence in a tree network fragment of 
a belief network where the node X is the parent of two other nodes Y and Z.  In 
this case, we factorize the joint probability distribution of the variables X, Y, and 
Z as follows: 
(
)
, ,
p X Y Z  = (
) (
)
|
,
,
p Z X Y p X Y  = (
) (
) (
)
|
|
p Z X p Y X p X  
X
Z
Y
 
Figure 6-5: Conditional independence in tree network fragment: Z is 
conditionally independent of Y given X 

174 
Decision-Making Agents 
 
 
Example 
Figure 6-6 shows an example instantiation of conditional independence in a tree 
network fragment. The variables X, Y and Z represent the weather, roads, and 
field conditions, respectively. If we observe rainy weather then the probabilities 
of the road condition being disruptive and the field condition unsuitable are 
determined, and the confirmation of the road condition being disruptive will not 
change the probability the field being unsuitable, and vice versa. 
Weather
Field
Roads
 
Figure 6-6: Example conditional independence in a tree network fragment: Roads 
and Field are conditionally independent given Weather 
 
Figure 6-7 shows conditional dependence in a polytree network fragment 
between the nodes Y and Z, given that we know about X; the two variables are 
marginally independent if we know nothing about X. For a polytree fragment as 
shown in Figure 6-7, the probability distribution of the variables Y, Z, and X can 
be factorized as follows: 
(
, , )
p X Y Z  = 
(
| , ) ( , )
p X Y Z p Y Z  = 
(
| , ) ( ) ( )
p X Y Z p Y p Z  
Y
X
Z
 
Figure 6-7: Conditional dependence in a polytree fragment: Y is conditionally 
dependent on Z given X 

 
Bayesian Belief Networks 
175 
 
Example 
Figure 6-8 shows an example instantiation of the conditional dependence in a 
polytree network fragment. Both the weather condition and the sprinkler status 
can affect the field condition. Before any observation is made on the field 
condition, the probability of the weather condition to be in a particular state is 
independent of the probability of the sprinkler being normal or having a burst 
pipe. However, once a particular field condition, say the unsuitable condition, is 
observed, the weather condition may influence the sprinkler. For example, 
observation of rainy weather (thus explaining why the field is unsuitable) may 
decrease the probability of a burst pipe in the sprinkler system. This phenomenon 
is termed as explaining away. In other words, observation of the status of one 
parent “explains away” the other, given a value of the child node.  
Weather
Field
Sprinkler
 
Figure 6-8: Example conditional independence in a polytree network fragment: 
Weather is conditionally dependent on Sprinkler given Field 
 
In view of the joint distribution formulae for these three types of network 
fragments, the joint probability distribution in a directed acyclic graph (DAG)  
(“directed” means the links have an explicit direction represented by arrows, and 
“acyclic” means that the arrows may not form a directional cycle in the network) 
type of network can be factored into conditional probabilities, where each factor 
involves only a node and its parents. This is stated through the following result:  
Proposition 6-1: Consider a network consisting of variables 
1
2
,
,...,
n
X
X
X . The 
joint probability distribution 
1
2,
(
,
...,
)
n
p X
X
X
 is the product of all 
conditional probabilities specified in the network: 

176 
Decision-Making Agents 
 
 
1
2,
(
,
...,
)
n
p X
X
X
 = 
1
(
|
(
))
n
i
i
i
p X
pa X
=∏
 
where 
(
)
i
pa X
 denotes the parent variables of 
i
X . 
 
Example 
Consider the network shown in Figure 6-1. For any combination of values w, s, r, 
f, g, e, t, c of the variables Weather, Sprinkler, Roads, Field, Game, Economy, 
Ticket Sale, and Concession, respectively, the joint probability is 
( , , , , , , , )
p w s r f g e t c  =  
( ) ( ) ( |
) (
|
, ) (
| ,
) ( | , , ) ( | )
p w p s p r w p f w s p g r f p t r g e p c t  
 
Influence in a belief network is only allowed to flow along the links given in 
the network. Therefore, independence between two nodes is represented by the 
absence or blocking of links between the two nodes. Whether a link between a 
pair of nodes exists or not is determined by a property called d-separation. 
Before we formally define d-separation, we need to introduce three kinds of 
connections between a node X and two of its neighbors Y and Z. The three 
possibilities are shown in the three figures Figure 6-3, Figure 6-5, and Figure 6-7. 
Their example instantiations are shown in Figure 6-4, Figure 6-6, and Figure 6-8 
respectively. 
In Figure 6-3, there are links from Y to X and from X to Z. In other words, Y 
has an influence on X, which in turn influences Z. The connection or path 
between Y and Z is called linear. In this case, causal or deductive evidence from 
Y will change the belief of X, which then changes the belief of Z. Similarly, 
diagnostic or abductive evidence from Z will change the belief of X, which then 
changes the belief of Y. But if the state of X is known, then the connection 
between Y and Z are blocked and cannot influence each other any more. Thus, Y 
and Z become independent given X. We say that Y and Z are d-separated given X.  
In Figure 6-5, there are links from X to Y as well as from X to Z. In other 
words, X has influence on both Y and Z, and Y and Z will influence each other via 
X. The connection or path between Y and Z is called diverging and the node X is 
said to have diverging arrows. As in the linear case, if the state of X is known 

 
Bayesian Belief Networks 
177 
 
then Y and Z cannot influence each other any more, and we say that Y and Z are 
d-separated given X. 
The third case, shown in Figure 6-7, is opposite to the previous case. In this 
case, there are links from Y to X as well as from Z to X. In other words, both Y 
and Z have influence on X. The connection or path in this case is called 
converging and the node X is said to have converging arrows. In this case, if 
nothing is known about X then Y and Z are independent, and therefore cannot 
influence each other. But if the state of X is known then Y and Z can influence 
each other. In other words, Y and Z are already d-separated, but not when X is 
given.  
In general, two nodes Y and Z in a DAG type of network are d-separated if 
for all paths between Y and Z, there is an intermediate node X such that either: 
• 
The path between Y and Z is serial or diverging at node X and the state of 
X is known, or 
• 
The path between Y and Z is converging at node X and neither X nor any 
of its descendants have received evidence  
Two nodes Y and Z in a DAG are d-connected if they are not d-separated. The 
following proposition establishes a connection between conditional independence 
and d-separateness.  
Proposition 6-2: If any two nodes Y and Z in a DAG are d-separated with 
evidence e entered, then Y and Z are conditionally independent given e  (i.e. 
|
Y
Z e
⊥
 or 
( |
, )
p Y Z e  = 
( | )
p Y e ). 
 
Example 
Consider the network shown in Figure 6-1. Let Y = Roads and Z = Sprinkler. The 
two paths between the two nodes Y and Z are: 
Roads
Weather
Field
Sprinkler
Roads
Game
Field
Sprinkler
←
→
←
→
←
←
 
The first path contains a diverging node (Weather) and the second path contains a 
converging node (Game). If the state of the variable Weather is known and the 
variable Game and its descendents Ticket Sale, and Concession have not received 

178 
Decision-Making Agents 
 
 
evidence, then the nodes Roads and Sprinkler are d-separated. Alternatively, if 
the variable Field and its descendents Game, Ticket Sale and Concession have 
not received evidence, then the nodes Roads and Sprinkler are d-separated. 
 
The above definition of d-separation between two nodes takes into account 
the evidence entered into the network. Here we present a more generalized 
definition of d-separation that identifies a set of nodes, instead of a single node, 
that could potentially separate two nodes in a network. Moreover, the definition 
provided here is between two sets of nodes rather than between two nodes. 
For any three disjoint subsets 
X
S , 
Y
S , and 
Z
S  of nodes, 
X
S  is said to d-
separate 
Y
S  and 
Z
S  if every path between a node in 
Y
S  to a node in 
Z
S  there is 
a node X satisfying one of the following two conditions: 
• 
X has converging arrows and none of X or its descendants are in 
X
S , or 
• 
X does not have a converging arrows and X is in 
X
S  
Example 
Consider the network shown in Figure 6-1. Let 
Y
S  = {
}
Roads  and 
Z
S  = 
{
}
Sprinkler . The set of all paths from a node in 
Y
S  to a node in 
Z
S  is: 
Roads
Weather
Field
Sprinkler
Roads
Game
Field
Sprinkler
←
→
←
→
←
←
 
Suppose 
X
S  = Φ . The first path contains node Field with converging arrows and 
none of its three descendants Game, Ticket Sale, and Concession is in 
X
S . The 
second path contains the node Game with converging arrows and none of its two 
descendants Ticket Sale and Concession is in 
X
S . Therefore, 
Y
S  and 
Z
S  are d-
separated by the empty set. But, if we consider 
X
S  = {
}
Ticket Sale , the first path 
contains the node Field with converging arrows and its descendant Ticket Sale is 
in 
X
S . Also, the second path contains the node Game with converging arrows 
and its descendants Ticket Sale is in 
X
S . Although, the first path contains the 
node Weather without converging arrows, the node does not belong to 
X
S . 
Therefore, {
}
Ticket Sale does not d-separate
Y
S  and 
Z
S . Note that Φ  does not d-

 
Bayesian Belief Networks 
179 
 
separate {
}
Roads  and {
}
Field , but {
}
Weather  does, and so does 
{
}
,
Weather Ticket Sale . 
 
The generalized set-theoretic definition of d-separation above yields the 
following proposition. 
Proposition 6-3: For any three disjoint subsets 
X
S , 
Y
S , and 
Z
S  of variables in a 
DAG, 
X
S  d-separates 
Y
S  and 
Z
S  if and only if 
Y
S  and 
Z
S  are conditionally 
independent given 
X
S  (that is, 
|
Y
Z
X
S
S
S
⊥
). 
 
Once we have built belief networks, either with the help of domain experts or 
via automated learning from past observations, we need to reason with them, that 
is, examine how the variables in a network change their beliefs when 
observations are propagated into the network as evidence.  
6.3 
Evidence, Belief, and Likelihood 
Evidence on a variable is a statement of certainties of its states based on certain 
observation. Since the states of a belief network variable are mutually exclusive, 
such a statement of certainty of a state variable is usually made with a percentage 
that represents the chance of being in that state. If the statement constituting 
evidence for a variable gives the exact state of the variable (that is, 100%), then it 
is hard evidence   (which is also called instantiation); otherwise, the evidence is 
called soft.  As an example, consider the variable Weather whose states are 
sunny, rain, and snow. If the evidence e is based on someone’s direct observation 
of the weather and states that the weather is sunny, then it is hard evidence and is 
denoted by Weather = sunny. In general, if 
1
{
,...,
}
n
E
X
X
=
 is the set of all 
variables whose values are known as 
1
1,...,
n
n
X
a
X
a
=
=
then 
1
1
{
,...,
}
n
n
e
X
a
X
a
=
=
=
 
where each 
ia  is hard evidence of the state of 
i
X . For example, if E = {Weather, 
Roads, Game} and the evidence states that the weather is sunny, the roads are 
open, and the game is on, then 
{
,
,
}
e
Weather
sunny Roads
open Game
on
=
=
=
=
 

180 
Decision-Making Agents 
 
 
On the other hand, consider the situation when the source of evidence on the 
variable Weather is based on the observation of output from a sensor (the sensor 
is not explicitly modeled as a belief network variable). The statement constituting 
evidence states that there is a 80% chance that the weather is sunny, 15% chance 
that the weather is rainy, and 5% chance that the weather is snowy. The evidence 
in this case is inexact and therefore soft. Evidence on a variable X yields a 
likelihood vector, denoted as 
(
)
X
λ
, expressed in terms of probability measures. 
For example, the above soft evidence on the variable Weather yields the 
likelihood vector 
0.80
(
)
0.15
0.05
Weather
λ
⎡
⎤
⎢
⎥
= ⎢
⎥
⎢
⎥
⎣
⎦
 
The hard evidence 
{
}
e
Weather
sunny
=
=
 yields the likelihood vector 
1
0
0
⎡⎤
⎢⎥
⎢⎥
⎢⎥
⎣⎦
. 
Usually, total evidence accumulated on the states of a variable is expected to 
be more or less than 100%. An example of such type of evidence obtained by 
observing output from a sensor (for example, the sensor displays green for sunny 
weather, blue for rainy, and red for snowy; ambiguities occur because a faulty 
sensor or a noisy environment can cause display of different lights for a given 
weather state) states that there is a 70% chance that the weather is sunny, a 50% 
chance that the weather is rainy, and a 15% chance that the weather is snowy. 
The likelihood vector for this evidence is the following: 
0.70
(
)
0.50
0.15
Weather
λ
⎡
⎤
⎢
⎥
= ⎢
⎥
⎢
⎥
⎣
⎦
 
The above evidence states that if all 100 weather circumstances similar to the 
current one are sunny, the sensor prediction is likely to be correct (that is, 
displays green) 70 times; if all are rainy, it is likely to be correct (blue) 50 times; 
and if all are snowy, it is likely to be correct (red) 15 times.  
How do we then relate evidence to probability? Observe that 70:50:15 is the 
ratio of the number of times the sensor is likely to produce 
Weather
e
 if all 100 
weather circumstances are sunny, to the number of times it is likely to produce 

 
Bayesian Belief Networks 
181 
 
Weather
e
 if all 100 are rainy, to the number of times it is likely to produce 
Weather
e
 if 
all 100 are snowy. This relation yields the following likelihood ratio: 
(
|
):
(
|
):
(
|
)
70:50:15
Weather
Weather
Weather
p e
Weather
sunny
p e
Weather
rainy
p e
Weather
snowy
=
=
=
=
 
This ratio gives the following likelihood:    
(
)
Weather
sunny
λ
=
 = 
(
|
)
0.70
Weather
p e
Weather
sunny
=
=
 
(
)
Weather
rainy
λ
=
 = 
(
|
)
0.50
Weather
p e
Weather
rain
=
=
  
(
)
Weather
snowy
λ
=
 = 
(
|
)
0.15
Weather
p e
Weather
snow
=
=
 
The likelihood vector 
(
)
Weather
λ
 is 
(
|
)
Weather
p e
Weather , and we therefore have 
the following: 
(
)
Weather
λ
 = 
(
|
)
Weather
p e
Weather  = 
0.70
0.50
0.15
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
 
If the sensor always displays green when the weather is sunny, then the above 
likelihood vector changes to 
1.0
(
)
0.5
0.15
Weather
λ
⎡
⎤
⎢
⎥
= ⎢
⎥
⎢
⎥
⎣
⎦
 
But, conversely, the green display does not necessarily mean the weather is 
sunny though it certainly indicates a high probability of sunny weather. 
The CPTs of a belief network remain unchanged upon the arrival of 
evidence. When evidence is posted to the designated node to compute posterior 
probabilities of the nodes in the networks, the node state certainties, or 
probability distribution, change. After receiving evidence e, the posterior 
probability of node X is 
(
| )
p X e . The belief of the node X of a belief network, 
denoted as 
(
)
Bel X , is the overall belief of the node X contributed by all 
evidence so far received. Therefore, if e is the evidence received so far then 
(
)
Bel X  = 
(
| )
p X e . 
Consider the network fragment as shown in Figure 6-9. Suppose 
X
e+  and 
X
e− 
are the total evidence connected to X through its parents and children, 
respectively. In other words, 
X
e+  and 
X
e− are the evidence contained in the upper 

182 
Decision-Making Agents 
 
 
and lower sub-networks with respect to the node X. We then define the following 
two π  and λ  vectors:  
(
)
X
π
 = 
(
|
)
X
p X e+  
(
)
X
λ
 = 
(
|
)
X
p e
X
−
 
The vectors 
(
)
X
π
 and 
(
)
X
λ
 represent the distributions of the total supports 
among the states of X through its parents and children respectively. If the 
network that contains the fragment is a tree, then the vectors 
(
)
X
π
 and 
(
)
X
λ
represent the distributions of the total causal and diagnostic supports 
among the states of X by all its ancestors and descendants, respectively. 
(
)
(
|
)
X
X
p X e
π
+
=
X
+
X
e
(
)
(
|
)
X
X
p e
X
λ
−
=
)
(
)
(
)
(
X
X
X
Bel
λ
π
α
=
−
X
e
 
Figure 6-9: Network fragment containing node X 
6.4 
Prior Probabilities in Networks without Evidence 
Evidence propagation and belief updating starts with “fresh” networks without 
any observed evidence. Then the π  and λ  vectors of the variables, and hence 
the belief vectors, are updated incrementally as evidence is accumulated. In this 
section, we detail how these initial vectors of the variables in a fresh network are 
computed. 
If no evidence has yet been propagated in a network, then, for every variable 
X in the network, 
(
)
X
π
 is 
(
)
p X , since 
X
e+  is empty. Therefore, 
(
)
X
π
 is simply 
the prior probability of the variable X.  Since 
X
e− is the empty set, 
(
)
X
λ
 is 
(
|
)
p
X
Φ
. Since Φ  is a constant, each 
(
| )
p
x
Φ
 is equal to 1
n , where n is the 
number of states of X. For the purpose of simplicity, we will write an initialλ  
vector simply as a non-normalized n-vector (1,1,...,1) . 

 
Bayesian Belief Networks 
183 
 
Recall that the relationships among the variables in a network are quantified 
via CPTs of the form 
1
2
(
|
,
,...,
)
n
p X U U
U
 for a variable X with parents 
1
2
,
,...,
n
U U
U . Therefore, if X has no parent (that is, is a root node) then its CPT 
is just 
(
)
p X , which is its prior probability. With this in mind, we present a 
simple recursive routine to compute the beliefs and the π  and λ  vectors in a 
fresh network. 
First, mark all the root nodes. Then recursively compute 
(
)
p X  of a node X, 
each of whose parents is already marked, and mark the node X itself. If X has 
parents 
1
2
,
,...,
n
U U
U  then 
1
1
1
1
,...,
1
1
2
1
,...,
(
)
(
|
,...,
) (
,...,
)
(
|
,...,
) (
|
,...,
)... (
|
) (
)
n
n
n
n
U
U
n
n
n
n
n
U
U
p X
p X U
U
p U
U
p X U
U
p U
U
U
p U
U
p U
−
=
=
∑
∑
 
Since 
1
2
,
,...,
n
U U
U  are marginally independent 
1
1
,...,
1
(
)
(
|
,...,
)
(
)
n
n
n
i
U
U
i
p X
p X U
U
p U
=
= ∑
∏
 
Thus, 
(
)
p X can be computed using its CPT and the prior probabilities of its 
parents. 
Example 
Consider the network in Figure 6-10, along with the prior probabilities 
(
)
p Roads  and 
(
)
p Field  of the root nodes Roads and Field, respectively, and the 
two CPTs 
(
|
,
)
p Game Roads Field  and 
(
|
)
p Ticket Sale Game  of the two other 
nodes of the network. The network also shows the initial π  and λ  vectors, and 
hence belief vectors, of each of the two root nodes.  
The prior probability of the node Game and then of the node Ticket Sale are 
computed as follows: 
,
0.67
(
)
(
|
,
) (
) (
)
0.33
Roads Field
p Game
p Game Roads Field p Roads p Field
⎡
⎤
=
= ⎢
⎥
⎣
⎦
∑
 
0.50
(
)
(
|
) (
)
0.17
0.33
Game
p Ticket Sale
p Ticket Sale Game p Game
⎡
⎤
⎢
⎥
=
= ⎢
⎥
⎢
⎥
⎣
⎦
∑
 

184 
Decision-Making Agents 
 
 
Field
Game
Ticket Sale
Roads
0.85
1
0.85
0.10
1
0.10
0.05
1
0.05
Bel
π
λ
⎡
⎤
⎡⎤
⎡
⎤
⎢
⎥
⎢⎥
⎢
⎥
⎢
⎥
⎢⎥
⎢
⎥
⎢
⎥
⎢⎥
⎢
⎥
⎣
⎦
⎣⎦
⎣
⎦
0.7
1
0.7
0.3
1
0.3
Bel
π
λ
⎡
⎤
⎡⎤
⎡
⎤
⎢
⎥
⎢⎥
⎢
⎥
⎣
⎦
⎣⎦
⎣
⎦
(
|
,
)
0.99
0.1
0.8
0.1
0.01
0
0.01
0.9
0.2
0.9
0.99
1
p Game Roads Field =
⎡
⎤
⎢
⎥
⎣
⎦
(
|
)
0.75
0.0
0.25
0.0
0.0
1.0
p Ticket Sale Game =
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
0.85
(
)
0.10
0.05
p Roads
⎡
⎤
⎢
⎥
= ⎢
⎥
⎢
⎥
⎣
⎦
0.7
(
)
0.3
p Field
⎡
⎤
= ⎢
⎥
⎣
⎦
0.67
(
)
0.33
p Game
⎡
⎤
= ⎢
⎥
⎣
⎦
0.67
1
0.67
0.33
1
0.33
Bel
π
λ
⎡
⎤
⎡⎤
⎡
⎤
⎢
⎥
⎢⎥
⎢
⎥
⎣
⎦
⎣⎦
⎣
⎦
0.50
1
0.50
0.17
1
0.17
0.33
1
0.33
Bel
π
λ
⎡
⎤
⎡⎤
⎡
⎤
⎢
⎥
⎢⎥
⎢
⎥
⎢
⎥
⎢⎥
⎢
⎥
⎢
⎥
⎢⎥
⎢
⎥
⎣
⎦
⎣⎦
⎣
⎦
0.50
(
)
0.17
0.33
p Ticket Sale
⎡
⎤
⎢
⎥
= ⎢
⎥
⎢
⎥
⎣
⎦
 
Figure 6-10: Initial probabilities, beliefs, and π  and λ  vectors 
 
6.5 
Belief Revision 
In this section, we describe how a node revises its own belief upon receiving 
evidence on itself. Suppose a node X receives evidence 
X
e  and the probability 
vector 
(
)
p X  is its current state certainties. Then its posterior probability is 
defined as: 
(
) (
|
)
(
|
)
(
) (
)
(
)
X
X
X
p X p e
X
p X e
p X
X
p e
α
λ
=
=
  
where the normalizing constant α  is computed by summing over mutually 
exclusive and exhaustive states of the variable X (that is, 
(
)
1
X
x
p X
=
=
∑
): 
1
1
1
(
)
(
,
)
(
|
) (
)
X
X
X
X
X
p e
p X e
p e
X p X
α =
=
=
∑
∑
 

 
Bayesian Belief Networks 
185 
 
Therefore, the belief of the node X after receiving evidence 
X
e  becomes the 
normalized product of its prior probability vector 
(
)
p X  with the likelihood 
vector (
)
X
λ
. 
Example 
Consider the node Weather whose prior probability 
(
)
p Weather  and posted 
evidence 
Weather
e
 is shown in Figure 6-11. A particular evidence 
X
e  on a variable 
X in a network will be hypothetically considered as a binary child node of the 
node X, where the CPT 
(
|
)
(
)
X
p e
X
X
λ
=
. 
Weather
0.55
(
)
0.15
0.30
p Weather
⎡
⎤
⎢
⎥
= ⎢
⎥
⎢
⎥
⎣
⎦
Weather
e
0.80
(
)
0.15
0.05
Weather
e
Weather
λ
⎡
⎤
⎢
⎥
= ⎢
⎥
⎢
⎥
⎣
⎦
sunny
rain
snow
 
Figure 6-11: Posting evidence on a node 
Posterior probability of the state sunny, for example, is computed as follows: 
0.55 0.80
0.92
(
|
)
0.15 0.15
0.05
0.30
0.05
0.03
1
2.09
0.55 0.80
0.15 0.15
0.30
0.05
Weather
p Weather e
where
α
α
α
α
×
×
⎡
⎤
⎡
⎤
⎢
⎥
⎢
⎥
=
×
×
=
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
×
×
⎣
⎦
⎣
⎦
=
=
×
+
×
+
×
 
 
  
Now consider belief revision in the case illustrated in Figure 6-9. The 
posterior probability of the node X upon receiving all the evidence is computed 
as follows: 

186 
Decision-Making Agents 
 
 
(
)
(
|
,
)
(
,
,
)
(
,
)
(
|
,
) (
|
) (
)
(
,
)
(
|
) (
|
)
(
) (
)
X
X
X
X
X
X
X
X
X
X
X
X
X
X
Bel X
p X e
e
p e
e
X
p e
e
p e
e
X p X e
p e
p e
e
p X e
p e
X
X
X
α
α π
λ
+
−
+
−
+
−
−
+
+
+
+
−
+
−
=
=
=
=
=
 
where 
(
)
1
(
,
)
(
|
)
1
(
|
,
) (
)
1
(
|
,
) (
|
)
1
(
) (
)
X
X
X
X
X
X
X
X
x
X
X
X
X
x
X
x
p e
p e
e
p e
e
p e
e
X p X
p e
e
X p X e
X
X
α
π
λ
+
−
+
−
+
−
+
=
−
+
+
=
=
=
=
=
=
=
∑
∑
∑
 
Note that 
(
|
,
)
X
X
p e
e
X
−
+
= 
(
|
)
X
p e
X
−
because X separates 
X
e+  and 
X
e−. The node 
belief is therefore the normalized product of its λ  and π  vectors, which can be 
factorized as follows. 
Consider a concrete instantiation of the above case as shown in Figure 6-12. 
The node X has parents U, V, and W, through which it received evidence 
Ue+ , 
Ve+ , 
and 
W
e+  respectively. Node X has children Y, Z, and Q, through which it received 
evidence 
Ye−, 
Ze−, and 
Q
e− respectively. Thus, 
{
,
,
}
{
,
,
}
X
U
V
W
X
Y
Z
Q
e
e
e
e
e
e
e
e
+
+
+
+
−
−
−
−
=
=
 

 
Bayesian Belief Networks 
187 
 
X
Y
)
(X
Y
λ
−
Ye
Z
(
)
X W
π
−
Ze
V
W
U
+
U
e
+
Ve
+
W
e
Q
Q
e−
(
)
Z X
λ
(
)
Q X
λ
( )
X V
π
(
)
X U
π
 
Figure 6-12: Node X has multiple parents and children 
, ,
, ,
, ,
(
)
(
|
)
(
|
,
,
)
(
|
, ,
,
,
,
) ( , ,
|
,
,
)
(
|
, ,
) (
|
,
,
,
,
)
(
|
,
,
,
) (
|
,
,
)
(
|
, ,
) (
|
) (
|
) (
|
)
(
|
, ,
X
U
V
W
U
V
W
U
V
W
U V W
U
V
W
U V W
U
V
W
U
V
W
U
V
W
U V W
X
p X e
p X e
e
e
p X U V W e
e
e
p U V W e
e
e
p X U V W p U V W e
e
e
p V W e
e
e
p W e
e
e
p X U V W p U e
p V e
p W e
p X U V
π
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
=
=
=
=
=
=
∑
∑
∑
, ,
)
( )
( )
(
)
X
X
X
U V W
W
U
V
W
π
π
π
∑
  
The above derivation uses the conditional assumption since U, V, and W separate 
X from 
Ue+ , 
Ve+ , and 
W
e+ , respectively. The derivation also uses independence 

188 
Decision-Making Agents 
 
 
relationships, such as U is independent of V, W, 
Ve+ , and 
W
e+ , V is independent of 
U, W, 
Ue+ , and 
W
e+ , and W is independent of U, V, 
Ue+ , and 
Ve+ . Similarly, 
(
)
(
|
)
(
,
,
|
)
(
|
,
,
) (
|
,
) (
|
)
(
|
) (
|
) (
|
)
(
)
(
)
(
)
X
Y
Z
Q
Y
Z
Q
Z
Q
Q
Y
Z
Q
Y
Z
Q
X
p e
X
p e
e
e
X
p e
e
e
X p e
e
X p e
X
p e
X p e
X p e
X
X
X
X
λ
λ
λ
λ
−
−
−
−
−
−
−
−
−
−
−
−
−
=
=
=
=
=
  
This derivation uses the conditional independence assumptions that given X, 
Ye− 
is independent of 
Ze− and 
Q
e−, and 
Ze− is independent of 
Q
e−. 
Now consider the case shown in Figure 6-13, when a given node X receives 
evidence 
X
e . In addition, suppose 
X
e+  and 
X
e− are the total evidence connected to 
X through its parents and children, respectively.  Then the revised 
(
)
X
λ
 can be 
computed by using the conditional independence assumption derived from the 
fact that X separates 
X
e  and 
X
e−: 
X
+
X
e
)
|
,
(
)
(
X
e
e
p
X
X
X
−
=
λ
−
X
e
X
e
)
(X
X
eλ
 
Figure 6-13: Node X receives evidence 
X
e  
(
)
(
|
)
(
|
,
) (
|
)
(
|
) (
|
)
(
)
(
)
X
new
X
X
X
X
X
X
X
e
X
p e e
X
p e
e
X p e
X
p e
X p e
X
X
X
λ
λ
λ
−
−
−
=
=
=
=
  
Thus, a node revises its λ  vector by multiplying its λ  vector with the likelihood 
vector for the evidence. Note that 
(
)
X
π
 remains unchanged as 
X
e+  is unchanged. 

 
Bayesian Belief Networks 
189 
 
The revised belief of X is computed as follows, using the necessary independence 
assumption derived from the fact that X separates 
X
e+  from 
X
e−, and 
X
e :  
(
)
(
|
,
,
)
(
,
,
,
)
(
,
,
)
(
,
|
,
) (
|
) (
)
(
,
,
)
1
(
|
) (
,
|
)
where 
 is 
(
,
|
)
(
)
(
)
new
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
new
Bel
X
p X e
e
e
p e
e
e
X
p e
e
e
p e
e
e
X p X e
p e
p e
e
e
p X e
p e
e
X
p e
e
e
X
X
α
α
α π
λ
+
−
+
−
+
−
−
+
+
+
+
−
+
−
−
+
=
=
=
⎡
⎤
=
⎢
⎥
⎣
⎦
=
  
Therefore, the revised belief is simply the product of the revised 
(
)
X
λ
 with 
unchanged 
(
)
X
π
. 
Example 
Consider the network fragment at the top half of Figure 6-14 along with the π , 
λ , and belief vectors.  
Ticket Sale
0.54
0.6
0.80
0.14
0.3
0.11
0.32
0.1
0.09
Bel
π
λ
⎡
⎤
⎡
⎤
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
⎣
⎦
⎣
⎦
(
)
0.85
0.10
0.05
X
e
X
λ
⎡
⎤
⎢
⎥
= ⎢
⎥
⎢
⎥
⎣
⎦
X
e
Ticket Sale
0.54
0.51
0.98
0.14
0.03
0.01
0.32
0.005
0.01
Bel
π
λ
⎡
⎤
⎡
⎤
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
⎣
⎦
⎣
⎦
X
Ticket Sale
=
 
Figure 6-14: Example belief revision 

190 
Decision-Making Agents 
 
 
As shown in the figure, evidence 
X
e  has been posted into the node Ticket 
Sale. Then the revised λ  and belief vectors are computed below (also shown at 
the bottom half of the figure). 
(
)
new X
λ
 = (
)
(
)
X
e
X
X
λ
λ
 = 
0.6
0.85
0.3
0.10
0.1
0.05
⎡
⎤⎡
⎤
⎢
⎥⎢
⎥
⎢
⎥⎢
⎥
⎢
⎥⎢
⎥
⎣
⎦⎣
⎦
 = 
0.51
0.03
0.005
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
 
(
)
X
π
 = 
0.54
0.14
0.32
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
 
(
)
Bel X  = 
(
)
0.54
0.51
(
)
0.14
0.03
0.32
0.005
new
X
X
απ
λ
α
⎡
⎤⎡
⎤
⎢
⎥⎢
⎥
=
⎢
⎥⎢
⎥
⎢
⎥⎢
⎥
⎣
⎦⎣
⎦
 = 
0.98
0.015
0.005
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
 
 
6.6 
Evidence Propagation in Polytrees 
In the previous section, we detailed how a node updates its own belief upon 
receiving evidence on itself. This section discusses how a node X in a polytree 
updates its own beliefs when evidence is observed on one of its neighboring 
nodes and how it propagates the effects of that evidence to the neighboring nodes 
to help update their beliefs. We consider six different cases corresponding to 
possible polytree fragments around the node X that contain its immediate 
neighbors: 
• 
Upward propagation in a linear fragment: X has only one parent U, and 
one child Y, and the child Y receives evidence 
• 
Downward propagation in a linear fragment: X has only one parent U, 
and one child Y, and the parent U receives evidence 
• 
Upward propagation in a tree fragment: X has only one parent U, and 
three children Y, Z, and Q, and one of the children, say Y, receives 
evidence 
• 
Downward propagation in a tree fragment: X has only one parent U, and 
three children Y, Z, and Q, and the parent U receives evidence 

 
Bayesian Belief Networks 
191 
 
• 
Upward propagation in a polytree fragment: X has parents U, V, and W, 
and three children Y, Z, and Q, and one of the children, say Y, receives 
evidence 
• 
Downward propagation in a polytree fragment: X has parents U, V, and 
W, and three children Y, Z, and Q, and one of the parents, say U, receives 
evidence 
6.6.1 
Upward Propagation in a Linear Fragment 
This case is illustrated in Figure 6-15, where the node X has only one parent U, 
and one child Y, and the child Y receives evidence 
Ye . The node Y updates its 
belief and sends the message 
(
)
Y X
λ
 to X. The node X updates its belief upon 
receiving the message from Y and, in turn, sends the message 
( )
X U
λ
 to U to help 
update its belief. All the π  vectors remain unchanged, as there is no new causal 
evidence. Next, we compute the values of 
(
)
Y X
λ
, 
( )
X U
λ
, and their relations to 
the new beliefs of X and U. 
X
Y
U
)
(U
X
λ
)
(X
Y
λ
Ye
)
(Y
Y
eλ
−
Ye
 
Figure 6-15: Upward propagation of evidence 
Ye  in a linear fragment 
Let 
Ye− be the total evidence connected to Y, but not through its parent X. If Y 
now receives evidence 
Ye , then 

192 
Decision-Making Agents 
 
 
( )
(
,
|
)
new
Y
Y
Y
p e e
Y
λ
−
=
 
Since Y separates X from 
Ye and 
Ye−, the revised (
)
X
λ
 can now be computed as 
follows: 
(
)
(
|
)
(
| ,
) (
|
)
(
|
) (
|
)
( ) (
|
)
new
Y
Y
Y
Y
Y
Y
Y
Y
new
Y
X
p e e
X
p e e
Y X p Y X
p e e
Y p Y X
Y p Y X
λ
λ
−
−
−
=
=
=
=
∑
∑
∑
  
Therefore, the revised 
(
)
X
λ
 can be computed at the node Y by taking the 
product of the revised ( )
Y
λ
 and the CPT 
(
|
)
p Y X . The revised value 
(
)
new X
λ
 is 
then sent to the node X from Y as the message 
(
)
(
)
Y
new
X
X
λ
λ
=
. Note that 
(
)
X
π
 
remains unchanged as 
X
e+  is unchanged. Since X separates 
X
e+  from 
Ye  and 
Ye−, 
the node X revises its belief as follows: 
(
)
(
|
,
,
)
(
|
) (
,
|
)
(
)
(
)
new
Y
Y
X
X
Y
Y
new
Bel
X
p X e e
e
p X e
p e e
X
X
X
α
α π
λ
−
+
+
−
=
=
=
  
Therefore, X revises its belief by multiplying the revised 
(
)
X
λ
, sent as a 
message by Y, with its unchanged 
(
)
X
π
. The revised 
( )
U
λ
 can now be 
computed as follows: 
( )
(
) (
|
)
new
new
X
U
X p X U
λ
λ
=∑
 
X sends 
( )
new U
λ
 as a message 
( )
X U
λ
 to U. 
Example 
Consider the linear fragment shown in Figure 6-16 along with the π , λ , and 
belief vectors.  

 
Bayesian Belief Networks 
193 
 
Roads
Game
Weather
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
=
05
.0
1.0
01
.0
15
.0
2.0
09
.0
80
.0
7.0
90
.0
)
|
(
Weather
Roads
p
⎥⎦
⎤
⎢⎣
⎡
=
99
.0
55
.0
45
.0
01
.0
45
.0
55
.0
)
|
(
Roads
Game
p
0.55
1
0.55
0.15
1
0.15
0.30
1
0.30
Bel
π
λ
⎡
⎤
⎡⎤
⎡
⎤
⎢
⎥
⎢⎥
⎢
⎥
⎢
⎥
⎢⎥
⎢
⎥
⎢
⎥
⎢⎥
⎢
⎥
⎣
⎦
⎣⎦
⎣
⎦
0.55
(
)
0.15
0.30
p Weather
⎡
⎤
⎢
⎥
= ⎢
⎥
⎢
⎥
⎣
⎦
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
04
.0
12
.0
84
.0
1
1
1
04
.0
12
.0
84
.0
Bel
λ
π
0.52
1
0.52
0.48
1
0.48
Bel
π
λ
⎡
⎤
⎡⎤
⎡
⎤
⎢
⎥
⎢⎥
⎢
⎥
⎣
⎦
⎣⎦
⎣
⎦
 
Figure 6-16: Example linear fragment 
The upward propagation of evidence due to the posting of evidence 
Game
e
 at 
the node Game is shown in Figure 6-17. 
The two λ-values 
(
)
Game Roads
λ
 and 
(
)
Roads Weather
λ
 in the figure are 
computed as follows: 
(
)
(
)
(
) (
|
)
0.9
0.55
0.1 0.45
0.54
0.9
0.45
0.1 0.55
0.46
0.9
0.01
0.1 0.99
0.11
Game
new
new
Game
Roads
Roads
Game p Game Roads
λ
λ
λ
=
=
×
+
×
⎡
⎤
⎡
⎤
⎢
⎥
⎢
⎥
=
×
+
×
=
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
×
+
×
⎣
⎦
⎣
⎦
∑
 
(
)
(
)
(
) (
|
)
0.54
0.90
0.46
0.09
0.11 0.01
0.53
0.54
0.7
0.46
0.2
0.11 0.1
0.48
0.54
0.80
0.46
0.50
0.11 0.05
0.51
Rodas
new
new
Roads
Weather
Weather
Roads p Roads Weather
λ
λ
λ
=
=
×
+
×
+
×
⎡
⎤
⎡
⎤
⎢
⎥
⎢
⎥
=
×
+
×
+
×
=
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
×
+
×
+
×
⎣
⎦
⎣
⎦
∑
 

194 
Decision-Making Agents 
 
 
Roads
Game
Weather
0.9
(
)
0.1
Game
e
Game
λ
⎡
⎤
= ⎢
⎥
⎣
⎦
0.84
0.48
0.88
0.12
0.42
0.11
0.04
0.10
0.01
Bel
π
λ
⎡
⎤
⎡
⎤
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
⎣
⎦
⎣
⎦
0.55
0.35
0.56
0.15
0.32
0.14
0.30
0.33
0.30
Bel
π
λ
⎡
⎤
⎡
⎤
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
⎣
⎦
⎣
⎦
0.52
0.9
0.91
0.48
0.1
0.09
Bel
π
λ
⎡
⎤
⎡
⎤
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
⎣
⎦
⎣
⎦
0.35
(
)
0.32
0.33
Roads Weather
λ
⎡
⎤
⎢
⎥
= ⎢
⎥
⎢
⎥
⎣
⎦
0.48
(
)
0.42
0.10
Game Roads
λ
⎡
⎤
⎢
⎥
= ⎢
⎥
⎢
⎥
⎣
⎦
Game
e
 
Figure 6-17: Example upward propagation in a linear fragment 
 
6.6.2 
Downward Propagation in a Linear Fragment 
This case is illustrated in Figure 6-18, where the node X has only one parent U, 
and one child Y, and the parent node U receives evidence 
Ue . The node U 
updates its belief and sends the message 
( )
X U
π
 to X. The node X updates its 
belief upon receiving the message from U and, in turn, sends the message 
(
)
Y X
π
 
to X to help update its belief. Next, we compute the values of 
( )
X U
π
, 
(
)
Y X
π
, 
and their relations to the new beliefs of X and Y respectively. 

 
Bayesian Belief Networks 
195 
 
X
Y
U
)
(U
X
π
)
(X
Y
π
U
e
)
(U
U
eλ
+
U
e
 
Figure 6-18: Downward propagation of evidence 
Ue  in a linear fragment 
Let 
Ue+  be the total evidence connected to U but not through its child X. If U 
now receives evidence 
Ue , then 
( )
new U
λ
 = 
( ) ( )
U
e U
U
λ
λ
. Note that 
( )
U
π
 
remains unchanged since there is no change in evidence connected through the 
parents of U. The revised
(
)
X
π
 can now be computed as follows: 
(
)
(
|
)
(
|
,
,
) (
|
,
)
(
|
) (
|
)
 separates 
 from 
 and 
( ) (
|
)
new
U
U
U
U
U
U
U
U
U
U
U
U
X
U
X
p X e e
p X U e
e
p U e
e
p X U p U e e
U
X
e
e
U p X U
π
π
+
+
+
+
+
=
=
⎡
⎤
=
⎣
⎦
=
∑
∑
∑
  
where 
( )
(
|
)
X
U
U
U
p U e e
π
+
=
 is simplified as follows: 

196 
Decision-Making Agents 
 
 
( )
(
|
)
(
,
,
)
(
,
)
(
|
,
) (
|
) (
)
(
,
)
(
|
) (
|
)
since 
 separates 
 and 
1
( )
( )
is 
(
|
)
U
X
U
U
U
U
U
U
U
U
U
U
U
U
U
U
U
U
e
U
U
U
p U e e
p e
e U
p e
e
p e
e U p U e
p e
p e
e
p U e
p e
U
U
e
e
U
U
p e
e
π
α
α π
λ
α
+
+
+
+
+
+
+
+
+
+
=
=
=
⎡
⎤
=
⎣
⎦
⎡
⎤
=
⎢
⎥
⎣
⎦
 
The node U can compute 
( )
X U
π
 by multiplying its likelihood vector for the 
evidence with its π  vector. Therefore, the revised 
(
)
X
π
, 
(
)
new X
π
, can be 
computed at the node X by taking the product of 
( )
X U
π
and the CPT 
(
|
)
p X U . 
The revised value 
(
)
new X
π
 is then sent to the node Y from X as the 
message
(
)
Y X
π
. Note that 
(
)
X
λ
 remains unchanged since 
X
e− is unchanged. 
The node X revises its belief as follows: 
,
(
)
(
|
,
,
)
(
|
) (
|
) since 
 separates 
from
and 
(
) (
)
( ) (
|
) (
)
new
U
U
U
U
U
U
U
U
U
new
X
Bel
X
p X e
e
e
p e e
X p X e
X
e
e
e
X
X
U p X U
X
α
απ
λ
α π
λ
+
−
+
−
−
+
=
⎡
⎤
=
⎣
⎦
=
=
  
Therefore, X revises its belief by multiplying message
( )
X U
π
 sent by U, 
with its unchanged λ  vector 
(
)
X
λ
 and the CPT 
(
|
)
p X U . Similarly, X sends a 
message 
(
)
Y X
π
 to Y to help revising its belief.  
Example 
Consider the linear fragment shown in Figure 6-19 along with the π , λ , and 
belief vectors.  

 
Bayesian Belief Networks 
197 
 
Roads
Game
Weather
⎥
⎥
⎥
⎦
⎤
⎢
⎢
⎢
⎣
⎡
=
05
.0
1.0
01
.0
15
.0
2.0
09
.0
80
.0
7.0
90
.0
)
|
(
Weather
Roads
p
⎥⎦
⎤
⎢⎣
⎡
=
99
.0
55
.0
45
.0
01
.0
45
.0
55
.0
)
|
(
Roads
Game
p
0.55
(
)
0.15
0.30
p Weather
⎡
⎤
⎢
⎥
= ⎢
⎥
⎢
⎥
⎣
⎦
Weather
e
0.84
0.48
0.88
0.12
0.42
0.11
0.04
0.10
0.01
Bel
π
λ
⎡
⎤
⎡
⎤
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
⎣
⎦
⎣
⎦
0.55
0.35
0.56
0.15
0.32
0.14
0.30
0.33
0.30
Bel
π
λ
⎡
⎤
⎡
⎤
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
⎣
⎦
⎣
⎦
0.52
0.9
0.91
0.48
0.1
0.09
Bel
π
λ
⎡
⎤
⎡
⎤
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
⎣
⎦
⎣
⎦
0.80
(
)
0.15
0.05
Weather
e
Weather
λ
⎡
⎤
⎢
⎥
= ⎢
⎥
⎢
⎥
⎣
⎦
 
Figure 6-19: Example linear fragment 
The downward propagation of evidence due to the posting of evidence 
Weather
e
 at the node Weather is shown in Figure 6-20. 
Roads
Game
Weather
0.92
(
)
0.05
0.03
Roads Weather
π
⎡
⎤
⎢
⎥
= ⎢
⎥
⎢
⎥
⎣
⎦
0.89
(
)
0.10
0.01
Games Roads
π
⎡
⎤
⎢
⎥
= ⎢
⎥
⎢
⎥
⎣
⎦
0.89
0.48
0.91
0.10
0.42
0.09
0.01
0.10
0.0
Bel
π
λ
⎡
⎤
⎡
⎤
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
⎣
⎦
⎣
⎦
0.55
0.81
0.93
0.15
0.14
0.04
0.30
0.05
0.03
Bel
π
λ
⎡
⎤
⎡
⎤
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
⎣
⎦
⎣
⎦
0.53
0.9
0.91
0.47
0.1
0.09
Bel
π
λ
⎡
⎤
⎡
⎤
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
⎣
⎦
⎣
⎦
Weather
e
0.80
(
)
0.15
0.05
Weather
e
Weather
λ
⎡
⎤
⎢
⎥
= ⎢
⎥
⎢
⎥
⎣
⎦
 
Figure 6-20: Example downward propagation in a linear fragment 

198 
Decision-Making Agents 
 
 
 
6.6.3 
Upward Propagation in a Tree Fragment 
This case is illustrated in Figure 6-21, where the node X has only one parent U, 
and three children Y, Z, and Q, and the child Y receives evidence 
Ye . The node Y 
updates its belief and sends the message 
(
)
Y X
λ
 to X. The node X updates its 
belief upon receiving the message from Y and, in turn, sends the diagnostic 
message 
( )
X U
λ
 to U and the causal messages 
(
)
Z X
π
 and
(
)
Q X
π
to Z and Q, 
respectively, to help update their beliefs. The messages 
(
)
Y X
λ
 and 
( )
X U
λ
 are 
computed as above in the case of upward propagation in a linear fragment. Next, 
we compute 
(
)
Z X
π
, 
(
)
Q X
π
, and their relations to the new beliefs of Z and Q. 
X
U
)
(U
X
λ
Ye
)
(Y
Y
eλ
)
(X
Z
π
+
U
e
Y
)
(X
Y
λ
−
Ye
Z
−
Ze
Q
Q
e−
(
)
Q X
π
 
Figure 6-21: Upward propagation of evidence 
Ye  in a tree fragment 

 
Bayesian Belief Networks 
199 
 
Let 
Ye−, 
Ze−, and 
Q
e− be the total diagnostic evidence associated with Y, Z, and 
Q respectively that is not obtained through their parent X, and 
Ue+  be the total 
causal evidence associated with U  that is not obtained through its child X. 
( )
(
|
,
,
,
)
(
|
,
,
,
,
) (
|
,
,
,
)
(
|
,
,
,
) (
|
,
,
,
)
given ,  is independent of 
(
|
)
(
) given ,  is independent of 
,
,
new
Y
Y
Q
U
Y
Y
Q
U
Y
Y
Q
U
X
Y
Y
Q
Y
Y
Q
U
X
U
Z
Y
Y
Q
X
Z
p Z e e
e
e
p Z X e e
e
e
p X e e
e
e
p Z X e e
e
p X e e
e
e
X Z
e
p Z X
X
X Z
e e
e
π
π
−
−
+
−
−
+
−
−
+
−
−
−
−
+
+
−
−
=
=
=
⎡
⎤
⎣
⎦
⎡
=
⎣
∑
∑
∑
⎤⎦
 
where 
 
(
,
,
,
,
)
(
)
(
|
,
,
,
)
(
,
,
,
)
(
,
|
,
,
) (
|
,
) (
|
) (
)
(
,
,
,
)
1
(
,
|
,
,
) (
|
,
) (
|
)
(
,
,
|
)
(
Y
Y
Q
U
Z
Y
Y
Q
U
Y
Y
Q
U
Y
Y
Q
U
Q
U
U
U
Y
Y
Q
U
Y
Y
Q
U
Q
U
U
Y
Y
Q
U
Y
p e e
e
e
X
X
p X e e
e
e
p e e
e
e
p e e
e
e
X p e
e
X p X e
p e
p e e
e
e
p e e
e
e
X p e
e
X p X e
p e e
e
e
p e
π
α
α
α
−
−
+
−
−
+
−
−
+
−
−
+
−
+
+
+
−
−
+
−
−
+
−
+
+
−
−
+
=
=
=
⎡
⎤
=
=
⎢
⎥
⎢
⎥
⎣
⎦
=
,
|
) (
|
,
) (
|
)
 separates 
,
 from 
,
(
,
|
) (
|
) (
|
)
 separates 
 from 
(
)
(
) (
)
(
)
(
)
(
) (
)
(
)
(
)
(
)
Y
Q
U
U
Y
Y
Q
U
Y
Y
Q
U
Q
U
Y
Q
Y
Z
Q
Z
new
Z
e
X p e
e
X p X e
X
e e
e
e
p e e
X p e
X p X e
X
e
e
X
X
X
X
X
X
X
X
Bel
X
X
α
α λ
λ
π
λ
λ
λ
π
α
λ
α
λ
−
−
+
+
−
−
+
−
−
+
−
+
⎡
⎤
⎣
⎦
⎡
⎤
=
⎣
⎦
=
=
=
  
Therefore, the revised 
( )
Z
π
 can be computed at the node Z by taking the product 
of the message
(
)
(
)
new
Z
Bel
X
X
λ
, sent by its parent X, and the CPT 
(
|
)
p Z X . Similarly, 

200 
Decision-Making Agents 
 
 
X sends 
(
)
(
)
new
Q
Bel
X
X
λ
 to Q to update its belief. Note that 
( )
Z
λ
 remains unchanged 
since 
Ze− is unchanged. The node Z revises its belief as follows: 
,
( )
(
|
,
,
,
,
)
(
|
,
,
) (
|
) since  separates 
 from 
,
,
,
( ) ( )
(
) (
|
) ( )
new
Y
Y
Q
U
Z
Y
Y
Q
U
Z
Z
Y
Y
Q
U
new
Z
Bel
Z
p Z e e
e
e
e
p Z e e
e
e
p e
Z
Z
e
e e
e
e
Z
Z
X p Z X
Z
α
α π
λ
α π
λ
−
−
+
−
−
−
+
−
−
−
−
+
=
⎡
⎤
=
⎣
⎦
=
=
Therefore, Z revises its belief by multiplying the message
(
)
Z X
π
, sent by its 
parent X, with its unchanged λ  vector ( )
Z
λ
 and its CPT 
(
|
)
p Z X . 
6.6.4 
Downward Propagation in a Tree Fragment 
This case, which is illustrated in Figure 6-22, is similar to the case of downward 
propagation in a linear fragment presented earlier. 
X
U
)
(U
X
π
U
e
)
(U
U
eλ
+
U
e
)
(X
Y
π
)
(X
Z
π
Y
−
Ye
Z
−
Ze
Q
Q
e−
(
)
Q X
π
 
Figure 6-22: Downward propagation of evidence 
Ue  in a tree fragment 

 
Bayesian Belief Networks 
201 
 
6.6.5 
Upward Propagation in a Polytree Fragment 
This case is illustrated in Figure 6-23, where the node X has three parents U, V, 
and W, and three children Y, Z, and Q, and the child Y receives evidence 
Ye . The 
node Y updates its belief and sends the message 
(
)
Y X
λ
 to X. The node X updates 
its belief upon receiving the message from Y and, in turn, sends the causal 
messages 
(
)
Z X
π
 and
(
)
Q X
π
 to Z and Q respectively to help update their beliefs. 
The messages 
(
)
Z X
π
 and 
(
)
Q X
π
 are 
(
)
( )
new
Bel
X
Z
λ
 and 
(
)
( )
new
Bel
X
Q
λ
computed as 
above in the case of upward propagation in a tree fragment. In the following, we 
show how U, V, and W update their beliefs upon receiving the messages 
( )
X U
λ
, 
( )
X V
λ
, and
(
)
X W
λ
 respectively from their common child X. 
X
)
(V
X
λ
)
(U
X
λ
)
(W
X
λ
V
W
U
+
U
e
+
Ve
+
W
e
Ye
)
(Y
Yeλ
)
(X
Z
π
Y
)
(X
Y
λ
−
Ye
Z
−
Ze
Q
Q
e−
(
)
Q X
π
 
Figure 6-23: Upward propagation of evidence 
Ye  in a polytree fragment 

202 
Decision-Making Agents 
 
 
The revised ( )
U
λ
 can now be computed as follows: 
( )
(
,
,
,
,
,
|
)
(
,
,
,
,
,
|
, ,
,
) ( ,
,
|
)
(
,
,
,
|
,
,
, ,
,
)
(
,
|
, ,
,
) ( ,
,
|
)
(
,
,
,
|
) (
|
,
, ,
,
)
(
|
,
new
Y
Y
Z
Q
V
W
Y
Y
Z
Q
V
W
X
V
W
Y
Y
Z
Q
V
W
X
V
W
V
W
Y
Y
Z
Q
V
W
W
U
p e e
e
e
e
e
U
p e e
e
e
e
e
U V W X p V W X U
p e e
e
e
e
e
U V W X
p e
e
U V W X p V W X U
p e e
e
e
X p e
e
U V W X
p e
U V
λ
−
−
−
+
+
−
−
−
+
+
−
−
−
+
+
+
+
−
−
−
+
+
+
=
=
=
=
∑∑∑
∑∑∑
,
,
) ( ,
,
|
)
since 
 seprates 
,
,
,
 from 
,
,
, ,
(
) (
|
, ,
) (
|
,
) ( ,
,
|
)
since 
 seprates 
 from 
, , and 
 separates 
 from 
,
(
)
X
V
W
Y
Y
Z
Q
V
W
new
V
W
W
X
V
W
V
W
new
W X p V W X U
X
e e
e
e
e
e
U V W
X p e
e
V W p e
V W p V W X U
V
e
U X
W
e
U X
X
p
λ
λ
−
−
−
+
+
+
+
+
+
+
⎡
⎤
⎣
⎦
=
⎡
⎤
⎢
⎥
⎢
⎥
⎣
⎦
=
∑∑∑
∑∑∑
(
,
|
,
) (
|
,
,
) ( ,
|
)
V
W
X
V
W
e
e
V W p X V W U p V W U
+
+
∑
∑∑
  
Since U, V, and W are marginally independent, 
( )
new U
λ
 can further be simplified 
as follows: 
( ,
|
,
)
( )
(
)
(
|
,
,
) ( ,
)
( ,
)
(
)
(
|
,
,
) (
|
,
) (
|
,
,
)
(
)
(
|
) (
|
) (
|
,
,
)
(
)
( )
(
) (
|
,
,
)
V
W
new
new
X
V
W
new
V
W
V
W
X
V
W
new
V
W
X
V
W
new
X
X
X
V
W
p V W e
e
U
X
p X V W U p V W
p V W
X
p V W e
e
p W e
e
p X V W U
X
p V e
p W e
p X V W U
X
V
W p X V W U
λ
α
λ
α
λ
α
λ
α
λ
π
π
+
+
+
+
+
+
+
+
=
=
=
=
∑
∑∑
∑
∑∑
∑
∑∑
∑
∑∑
 
Therefore, the message
( )
X U
λ
, which will be sent to U from X as the new λ  
vector for U, is the above expression computed at X. Note that 
(
|
,
,
)
p X V W U  is 
the CPT of X stored at the node X. The revised belief of U is obtained by 
multiplying its unchanged π  vector with the above λ  vector. The messages that 
are sent to V and W are the following: 

 
Bayesian Belief Networks 
203 
 
( )
(
)
( )
(
) (
|
,
, )
X
new
X
X
X
U
W
V
X
U
W p X U W V
λ
α
λ
π
π
= ∑
∑∑
 
(
)
(
)
( )
( ) (
|
, ,
)
X
new
X
X
X
U
V
W
X
U
V p X U V W
λ
α
λ
π
π
= ∑
∑∑
 
Example 
Consider the network fragment shown in Figure 6-24 along with the π , λ , and 
belief vectors, where the two CPTs for the nodes Ticket Sale and Concession are 
same as the two CPTs in Table 6-7 and Table 6-8, respectively, in our main 
belief network example. 
Game
Ticket Sale
Concession
Economy
Roads
0.84
1
0.84
0.12
1
0.12
0.04
1
0.04
Bel
π
λ
⎡
⎤
⎡⎤
⎡
⎤
⎢
⎥
⎢⎥
⎢
⎥
⎢
⎥
⎢⎥
⎢
⎥
⎢
⎥
⎢⎥
⎢
⎥
⎣
⎦
⎣⎦
⎣
⎦
high
low
none
high
low
none
on
cancelled
good
bad
open
disruptive
closed
(
|
,
,
)
p Ticket Sale Roads Game Economy
(
|
)
p Concession Ticket Sale
0.68
1
0.68
0.32
1
0.32
Bel
π
λ
⎡
⎤
⎡⎤
⎡
⎤
⎢
⎥
⎢⎥
⎢
⎥
⎣
⎦
⎣⎦
⎣
⎦
0.60
1
0.60
0.40
1
0.40
Bel
π
λ
⎡
⎤
⎡⎤
⎡
⎤
⎢
⎥
⎢⎥
⎢
⎥
⎣
⎦
⎣⎦
⎣
⎦
0.51
1
0.51
0.17
1
0.17
0.32
1
0.32
Bel
π
λ
⎡
⎤
⎡⎤
⎡
⎤
⎢
⎥
⎢⎥
⎢
⎥
⎢
⎥
⎢⎥
⎢
⎥
⎢
⎥
⎢⎥
⎢
⎥
⎣
⎦
⎣⎦
⎣
⎦
0.51
1
0.51
0.17
1
0.17
0.32
1
0.32
Bel
π
λ
⎡
⎤
⎡⎤
⎡
⎤
⎢
⎥
⎢⎥
⎢
⎥
⎢
⎥
⎢⎥
⎢
⎥
⎢
⎥
⎢⎥
⎢
⎥
⎣
⎦
⎣⎦
⎣
⎦
 
Figure 6-24: Example polytree fragment 
The upward propagation of evidence due to the posting of evidence 
Concession
e
 
at the node Concession is shown in Figure 6-25. 

204 
Decision-Making Agents 
 
 
Game
Ticket Sale
Concession
Economy
Roads
0.84
0.53
0.95
0.12
0.15
0.04
0.04
0.09
0.01
Bel
π
λ
⎡
⎤
⎡
⎤
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
⎣
⎦
⎣
⎦
0.68
0.69
1
0.32
0.0
0
Bel
π
λ
⎡
⎤
⎡
⎤
⎡⎤
⎢
⎥
⎢
⎥
⎢⎥
⎣
⎦
⎣
⎦
⎣⎦
0.60
0.49
0.63
0.40
0.44
0.37
Bel
π
λ
⎡
⎤
⎡
⎤
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
⎣
⎦
⎣
⎦
0.51
0.9
0.96
0.17
0.1
0.04
0.32
0.0
0.00
Bel
π
λ
⎡
⎤
⎡
⎤
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
⎣
⎦
⎣
⎦
(
)
0.9
0.1
0.0
Concession
e
Concession
λ
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
Concession
e
(
)
0.53
0.15
0.09
Ticket Sale Roads
λ
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
(
)
0.69
0.0
Ticket Sale Game
λ
⎡
⎤
⎢
⎥
⎣
⎦
(
)
0.9
0.1
0.0
Concession Ticket Sale
λ
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
(
)
0.49
0.44
Ticket Sale Economy
λ
⎡
⎤
⎢
⎥
⎣
⎦
0.51
0.9
0.96
0.17
0.1
0.04
0.32
0.0
0.00
Bel
π
λ
⎡
⎤
⎡
⎤
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
⎣
⎦
⎣
⎦
 
Figure 6-25: Example upward propagation in a polytree fragment 
 
6.6.6 
Downward Propagation in a Polytree Fragment 
This case is illustrated in Figure 6-26, where the node X has three parents U, V, 
and W, and three children Y, Z, and Q, and the parent U receives evidence 
Ue . 
Node U updates its belief and sends the causal message 
( )
X U
π
 to X. Node X 
updates its belief upon receiving the message from Y and, in turn, sends the 
messages 
(
)
Y X
π
, 
(
)
Z X
π
 and
(
)
Q X
π
 to Y, Z and Q, respectively, to help update 
their beliefs. The messages 
(
)
Y X
π
, 
(
)
Z X
π
, and
(
)
Q X
π
 are 
(
)
( )
new
Bel
X
Y
λ
, 
(
)
( )
new
Bel
X
Z
λ
, and 
(
)
( )
new
Bel
X
Q
λ
, respectively, computed as above in the case of 
upward propagation in a tree fragment. Next, we show how V and W update their 

 
Bayesian Belief Networks 
205 
 
beliefs upon receiving respectively the messages 
( )
X V
λ
 and
(
)
X W
λ
 from their 
common child X. 
X
V
)
(V
X
λ
W
U
)
(W
X
λ
+
U
e
)
(U
X
π
U
e
)
(U
U
e
λ
+
Ve
+
W
e
)
(X
Y
π
)
(X
Z
π
Y
−
Ye
Z
−
Ze
Q
Q
e−
(
)
Q X
π
 
Figure 6-26: Downward propagation of evidence 
Ue  in a polytree fragment 
( )
(
,
,
,
,
,
|
)
(
,
,
,
,
,
|
,
,
,
) ( ,
,
|
)
(
,
,
|
,
,
, ,
,
,
)
(
,
,
|
,
,
,
) ( ,
,
|
)
(
,
,
|
) (
|
,
, ,
,
,
)
(
,
new
U
U
W
Y
Z
Q
U
U
W
Y
Z
Q
X
U
W
Y
Z
Q
U
U
W
X
U
W
U
U
W
Y
Z
Q
U
U
W
U
V
p e
e
e
e
e
e
V
p e
e
e
e
e
e
V U W X p U W X V
p e
e
e
e
e
e
V U W X
p e
e
e
V U W X p U W X V
p e
e
e
X p e
e
e
V U W X
p e
λ
+
+
−
−
−
+
+
−
−
−
−
−
−
+
+
+
+
−
−
−
+
+
+
=
=
=
=
∑∑∑
∑∑∑
|
,
,
,
) ( ,
,
|
)
since 
 seprates 
,
,
 from 
,
,
,
, ,
X
U
W
W
Y
Z
Q
U
U
W
e
V U W X p U W X V
X
e
e
e
e
e
e
U V W
+
−
−
−
+
+
⎡
⎤
⎣
⎦
∑∑∑
 

206 
Decision-Making Agents 
 
 
(
) (
|
) (
,
|
,
, ) ( ,
,
|
)
since 
 seprates 
 from 
,
,
, ,
,and
since 
,
 seprate 
,
 from ,
( ) (
|
)
(
)
(
|
) ( ,
|
) (
|
,
, )
since 
,
areindep
U
U
U
W
X
U
W
U
U
W
U
W
e
U
X
U
W
W
U
W
X p e
U p e
e
U W V p U W X V
U
e
e
e
X V W
U W
e
e
V X
U p e
U
X
p e
W p U W V p X U W V
e
e
λ
λ
λ
+
+
+
+
+
+
+
+
+
+
=
⎡
⎤
⎢
⎥
⎢
⎥
⎣
⎦
=
∑∑∑
∑
∑∑
[
]
endentof each other given 
or 
(
)
( )
( )
(
) (
|
,
, )
since 
 and 
 are marginally independent
U
e
X
X
X
U
W
U
W
X
U
U
W p X U W V
U
W
α
λ
λ
π
π
⎡
⎤
⎣
⎦
= ∑
∑∑
 
Note that if X did not receive any diagnostic evidence from its descendants Y, Z, 
and Q, then the λ  vector for X would still be (1,1,...,1) . In this case, the above 
message sent to V from X due to the evidence on U would still be (1,1,...,1) , 
making no impact on the belief of V. This is consistent with the network marginal 
independence property, which says that U, V, and W are independent of each 
other if neither X nor any of its descendants received evidence. This is illustrated 
below. 
Suppose 
1,...,
n
x
x  (n states) are all possible instantiations of X. Then, from the 
CPT of X, U, V, W, we have for any u, v, w: 
(
| , , )
i
i
p x u v w
∑
 = 1 
If the variable X did not receive any evidence from its descendants then 
( )
ix
λ
 = 
1, for every i. If the variable V has m states and 
1,...,
m
v
v  are all possible 
instantiations of V, then from the derivation of 
( )
new V
λ
above, 
(
)
( )
( )
( )
(
) (
|
,
,
)
( )
( )
(
) (
|
,
,
)
( )
( )
(
)
(
|
,
,
)
( )
( )
(
)
U
U
U
U
X
j
i
e
X
X
i
j
i
U
W
e
X
X
i
j
i
U
W
e
X
X
i
j
U
W
i
e
X
X
U
W
v
x
U
U
W p x U W v
U
U
W p x U W v
U
U
W
p x U W v
U
U
W
λ
α
λ
λ
π
π
α
λ
π
π
α
λ
π
π
α
λ
π
π
=
=
=
=
∑
∑∑
∑∑∑
∑∑
∑
∑∑
  

 
Bayesian Belief Networks 
207 
 
Therefore, each 
(
)
X
jv
λ
 has the same value, making the vector 
( )
X V
λ
 a unit 
vector that does not change the belief of V.  
Example 
Consider the network fragment shown in Figure 6-27 along with the π , λ , and 
belief vectors, where the two CPTs for the nodes Ticket Sale and Concession are 
same as the two CPTs in Table 6-7 and Table 6-8, respectively, in our main 
belief network example. 
Game
Ticket Sale
Concession
Economy
Roads
high
low
none
high
low
none
on
cancelled
good
bad
open
disruptive
closed
(
|
,
,
)
p Ticket Sale Roads Game Economy
(
|
)
p Concession Ticket Sale
0.51
0.9
0.96
0.17
0.1
0.04
0.32
0.0
0.00
Bel
π
λ
⎡
⎤
⎡
⎤
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
⎣
⎦
⎣
⎦
0.51
0.9
0.96
0.17
0.1
0.04
0.32
0.0
0.00
Bel
π
λ
⎡
⎤
⎡
⎤
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
⎣
⎦
⎣
⎦
0.60
0.49
0.63
0.40
0.44
0.37
Bel
π
λ
⎡
⎤
⎡
⎤
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
⎣
⎦
⎣
⎦
0.68
0.69
1
0.32
0.0
0
Bel
π
λ
⎡
⎤
⎡
⎤
⎡⎤
⎢
⎥
⎢
⎥
⎢⎥
⎣
⎦
⎣
⎦
⎣⎦
0.84
0.53
0.95
0.12
0.15
0.04
0.04
0.09
0.01
Bel
π
λ
⎡
⎤
⎡
⎤
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
⎣
⎦
⎣
⎦
 
Figure 6-27: Example polytree fragment 
The downward propagation of evidence due to the posting of evidence 
Roads
e
 at 
the node Roads is shown in Figure 6-28. 

208 
Decision-Making Agents 
 
 
Game
Ticket Sale
Concession
Economy
Roads
0.68
0.77
1
0.32
0.0
0
Bel
π
λ
⎡
⎤
⎡
⎤
⎡⎤
⎢
⎥
⎢
⎥
⎢⎥
⎣
⎦
⎣
⎦
⎣⎦
0.57
0.9
0.98
0.11
0.1
0.02
0.32
0.0
0.00
Bel
π
λ
⎡
⎤
⎡
⎤
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
⎣
⎦
⎣
⎦
0.57
0.9
0.98
0.11
0.1
0.02
0.32
0.0
0.00
Bel
π
λ
⎡
⎤
⎡
⎤
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
⎣
⎦
⎣
⎦
(
)
0.8
0.2
0.0
Roads
e
Roads
λ
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
Roads
e
(
)
0.97
0.03
0.0
Ticket Sale Roads
π
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
(
)
0.77
0.0
Ticket Sale Game
λ
⎡
⎤
⎢
⎥
⎣
⎦
(
)
0.57
0.11
0.32
Concession Ticket Sale
π
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
(
)
0.54
0.49
Ticket Sale Economy
λ
⎡
⎤
⎢
⎥
⎣
⎦
0.60
0.54
0.625
0.40
0.49
0.385
Bel
π
λ
⎡
⎤
⎡
⎤
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
⎣
⎦
⎣
⎦
0.84
0.43
0.99
0.12
0.03
0.01
0.04
0.00
0.00
Bel
π
λ
⎡
⎤
⎡
⎤
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
⎣
⎦
⎣
⎦
 
Figure 6-28: Example downward propagation in a polytree fragment  
 
6.6.7 
Propagation Algorithm 
Now that we have illustrated different belief propagation with a series of 
examples, we generalize these steps to a concrete algorithm for belief 
propagation in polytrees. 
Input 
• 
Belief network N  (causal structure with associated CPTs) 
• 
Evidence 
X
e  on the variable X of N 
Output 
• 
Revised belief vector for each node, i.e., 
(
|
)
X
p Y e
, for each node Y 
Node Structure 

 
Bayesian Belief Networks 
209 
 
• 
Each node X in N with p number of states stores the following 
information locally in a suitable data structure (as shown in Figure 6-29): 
− p-ary π  vector
(
)
X
π
 
− p-aryλ  vector (
)
X
λ
 
− p-ary belief vector
(
)
Bel X  
− p-ary evidence vector
(
)
e X
λ
 
− p
n
×
 CPT
1
(
|
,...,
)
n
p X U
U
, if X has n parents 
1
2
,
,...,
n
U U
U  
− q-ary parent π  vector
(
)
X
i
U
π
, for each parent 
i
U  with q number of 
states 
− p-ary childλ  vector
(
)
iY X
λ
, for each child 
iY  
X
1
(
)
X U
π
1
U
2
U
n
U
1Y
2Y
m
Y
2
(
)
X U
π
(
)
X
n
U
π
1(
)
Y X
λ
2 (
)
Y X
λ
(
)
m
Y
X
λ
(
)
e X
λ
X
e
(
)
(
)
(
)
X
X
Bel X
λ
π
1
(
|
,...,
)
n
p X U
U
 
Figure 6-29: Node structure 
Initial Probability Computation 
• 
For each node X in N with p number of states, perform the following: 

210 
Decision-Making Agents 
 
 
− If X is a root node, then set each of its π  and belief vectors to its 
prior probability distribution 
− Set the evidence vector 
(
)
e X
λ
, λ  vector 
(
)
X
λ
, and each child λ  
vector 
(
)
iY X
λ
 to (1,...,1)  
• 
Mark each root node and then recursively compute the π  and belief 
vectors of the rest of the nodes through the following steps 
• 
If the parents of a node X are already marked, then perform the 
following: 
− Set each parent π  vector 
(
)
X
i
U
π
 to the π  vector of the parent 
i
U  
− Set its π  vector 
(
)
X
π
 to 
1
2
1
2
,
,...,
1
(
|
,
,...,
)
(
)
n
n
n
X
i
u u
u
i
p X u u
u
u
π
=
∑
∏
, if X 
has parents 
1
2
,
,...,
n
U U
U  
− Set its belief vector 
(
)
Bel X equal to its π  vector 
Evidence Propagation 
• 
Set the evidence vector 
(
)
e X
λ
 of X to the likelihood vector 
(
|
)
X
p e
X  
• 
Revise the beliefs of X: 
− Compute the λ  vector 
(
)
X
λ
 of X as the product of all its child λ  
vectors and the evidence vector 
− Compute the π  vector
(
)
X
π
 of X as the product of all its parent π  
vectors 
− Compute the belief vector of X as the product of its π  and λ  vectors 
• 
Generate messages from X on the update of its evidence vector: 
− Send a message 
1
1
1
1
1
1
,...,
,
...,
( )
( |
,...,
,
,...,
,
)
(
)
i
i
n
i
i
n
i
X
k
x
u
u
u
u
k i
x
p x u
u
u
u U
u
α
λ
π
−
+
−
+
≠
∑
∑
∏
  
to each parent 
i
U  of X 
− Send a message 
(
)
(
)
iY
Bel X
X
α λ
 to each child 
iY  of X 
• 
Generate messages from X on the update of the 
j
Y
λ  vector: 

 
Bayesian Belief Networks 
211 
 
− Send a message 
1
1
1
1
1
1
,...,
,
...,
( )
( |
,...,
,
,...,
,
)
(
)
i
i
n
i
i
n
i
X
k
x
u
u
u
u
k i
x
p x u
u
u
u U
u
α
λ
π
−
+
−
+
≠
∑
∑
∏
  
to each parent 
i
U  of X 
− Send a message 
(
)
(
)
iY
Bel X
X
α λ
 to each child 
iY  of X other than the child 
jY  
• 
Generate messages from X on the update of the 
(
)
X
j
U
π
 vector: 
− Send a message 
1
1
1
1
1
1
,...,
,
...,
( )
( |
,...,
,
,...,
,
)
(
)
i
i
n
i
i
n
i
X
k
x
u
u
u
u
k i
x
p x u
u
u
u U
u
α
λ
π
−
+
−
+
≠
∑
∑
∏
  
to each parent 
i
U  of X other than 
j
U  
− Send a message 
(
)
(
)
iY
Bel X
X
α λ
 to each child 
iY  of X 
If a network already received evidence on some of its variables, then only the 
evidence propagation step is followed, bypassing the initial probability 
computation step. 
6.7 
Evidence Propagation in Directed Acyclic Graphs 
The evidence propagation algorithm presented in the last section cannot usually 
handle DAGs (such as the one in Figure 6-1) because the evidence propagation 
on a variable in a DAG that is not a polytree may never terminate due to the 
recursive computation in the algorithm. This is explained in the context of the 
DAG shown in Figure 6-30. 
Suppose no evidence has been posted to the variable Weather, making the 
two variables Roads and Field marginally independent. Assume further that the 
variable Game received some evidence, making Roads and Field conditionally 
dependent on each other. Now, if evidence is observed on Roads, then Game will 
receive causal evidence and will send diagnostic evidence to the Field. The node 
Roads then receives evidence from Field via Weather. This cyclic process of 
evidence propagation continues unless a stopping criterion is put in place based 
on repeated evidence and changes in node belief. A similar cyclic process 
continues in the other direction when Roads send diagnostic evidence to Weather 
upon receiving evidence on itself. 

212 
Decision-Making Agents 
 
 
Weather
Field
Game
Roads
Sprinkler
 
Figure 6-30: Example DAG which is not a polytree 
A solution to this problem is to deal with the joint probability distribution of 
the variables in the network. Although belief network technology reduces the 
joint probability distribution of a network to merely the products of a few CPTs, 
we have ruled out this approach due to the large number of entries that need to be 
computed for a belief revision. Even for the small network in Figure 6-30, there 
are 72 entries in its joint probability distribution table that need to be computed 
using the following formula each time a node receives evidence. (Only the initial 
letter has been used from each variable name)  
For example, Roads receives evidence 
Roads
e
: 
(
, , ,
,
|
)
Roads
p W S R F G e
 = 
(
) ( ) (
|
) (
|
, ) (
|
,
) (
|
)
Roads
p W p S p R W p F W S p G R F p e
R   
To make use of the simple and elegant message passing propagation 
algorithm while avoiding large joint probability distributions, one approach is to 
transform a DAG into a polytree by identifying and clustering loops into single 
nodes. Thus a node in a transformed cluster network is a set of nodes (also called 
a clique) instead of a single node, and joint probability distributions are 
computed in smaller chunks locally at the network nodes. For example, the 
network in Figure 6-30 is transformed into the network (undirected) in Figure 
6-31 with two cliques. 
Y: Weather, Roads,
Field, Game
X: Weather,
Sprinkler, Field
 
Figure 6-31: Transformed Network 

 
Bayesian Belief Networks 
213 
 
Evidence on a variable in the original network is posted to the nodes in the 
transformed network containing the variable. The joint probability distributions 
are computed at the nodes, where evidence is posted and messages are passed to 
neighboring nodes. For example, if an evidence on the variable Weather is 
observed in the cluster network in Figure 6-31, then the joint probability 
distribution 
(
, ,
,
)
p W R F G  is computed at the node Y using some of the CPTs. 
Field and Weather are the only variable in the intersection of X and Y. Thus Y 
sends the change in the joint probability distribution of Field and Weather as a 
message to the variable X, which then computes the joint probability distribution 
of its three variables and derives the new belief of the variable Sprinkler via 
marginalization. 
But in a complex network, a cycle could be very large and therefore local 
computation of joint probability distribution is still impractical. A solution is to 
make smaller size clusters. Since the joint probability distribution of a network is 
derived from the CPTs and the CPT of a variable involves only its immediate 
neighbors, clusters around variables can be formed using only their neighboring 
nodes. One such cluster network constructed from the network in Figure 6-30 is 
shown in Figure 6-32. 
X: Field,
Weather, Sprinkler 
Z: Field,
Roads, Game
Y: Field,
Roads, Weather
 
Figure 6-32: Cluster network 
As before, if evidence on a variable, such as Roads, is observed, then it is 
posted to one of two nodes Y and Z (say,  Y) containing the variable Roads. Each 
of these nodes then computes the joint distribution locally to find the beliefs of 
the individual variables. The node Y then passes the revised joint probability 
distributions of the variables Field and Weather to X (respectively, Field and 
Roads to Z) to help compute the revised belief for the node Sprinkler 
(respectively, Game) given the evidence on Roads.  
In the following subsections, we present the junction tree algorithm for 
dealing with DAGs in general. This algorithm systematically constructs a cluster 
network from a belief network called a junction tree (or clique tree or join tree). 
The stages of the algorithm are shown on the left side of Figure 6-33. 

214 
Decision-Making Agents 
 
 
Belief Network
Graphical
Transformation
Join Tree Structure
Inconsistent Join Tree
Consistent Join Tree
Initialization
Propagation
Marginalization
p(X)
Moralization
Clique Identification
Triangulation
Join Tree Formation
 
Figure 6-33: Steps for constructing join trees 
The first stage, which is the graphical transformation stage, is expanded on 
the right side of the figure. This stage consists of four steps: moralization, 
triangulation, clique identification, and join tree formation. These steps construct 
a join tree from a belief network. The subsequent stages of the algorithm then 
compute prior beliefs of the variables in the network via an initialization of the 
join tree structure, followed by propagation and marginalization. The final 
subsection discusses how to handle evidence by computing the posterior beliefs 
of the variables in the network.  
6.7.1 
Graphical Transformation 
The moral graph of a DAG is obtained by adding a link between any pair of 
variables with a common child, and dropping the directions of the original links 
in the DAG. 

 
Bayesian Belief Networks 
215 
 
Example 
The moral graph of the network in Figure 6-1 is shown in Figure 6-34. The 
dotted lines in the network in Figure 6-34 are the links added to the original 
network. For example, the nodes Roads and Economy have a common child 
Ticket Sale, and therefore are linked with a dotted line. 
Weather
Field
Game
Ticket Sale
Concession
Sprinkler
Economy
Roads
 
Figure 6-34: The moral graph of the network in Figure 6-1 (added links are 
indicated by dotted lines) 
 
An undirected graph is triangulated if any cycle of length greater than 3 has 
a chord, that is, an edge joining two non-consecutive nodes along the cycle. 
Example 

216 
Decision-Making Agents 
 
 
The triangulated graph of the graph in Figure 6-34 is shown in Figure 6-35. The 
only link added by triangulation is the link between the nodes Roads and Field. 
These two nodes are two non-consecutive nodes along the cycle 
Weather
Roads
Game
Field
−
−
−
 of length 4.  
Weather
Field
Game
Ticket Sale
Concession
Sprinkler
Economy
Roads
 
Figure 6-35: A triangulated graph of the network in Figure 6-34 (new link is 
indicated by dotted lines) 
 
Note that there are, in general, more than one triangulation of a given graph. 
In the case of the graph in Figure 6-34, we could have added a link between the 
nodes Weather and Game, instead of between the nodes Roads and Field along 
the cycle Weather
Roads
Game
Field
−
−
−
, yielding a different triangulation.  
The nodes of a join tree for a graph are the cliques in the graph (maximal sets 
of variables that are all pairwise linked).  

 
Bayesian Belief Networks 
217 
 
Example 
The five cliques in the graph in Figure 6-35 are listed below: 
Weather
Field
Game
Ticket Sale
Concession
Sprinkler
Economy
Roads
 
Figure 6-36: An example clique (surrounded by dotted lines) in the network in 
Figure 6-35 
C1: {Weather, Roads, Field} 
C2: {Weather, Sprinkler, Field} 
C3: {Roads, Field, Game} 
C4: {Roads, Game, Economy, Ticket Sale} 
C5: {Ticket Sale, Concession} 
The clique C4 is shown in Figure 6-36, surrounded by dotted lines. Note that 
the set {Weather, Sprinkler, Roads, Field} is not a clique because its nodes are 
not all pairwise connected since Roads and Sprinkler are not connected. Though 
the nodes in the set {Roads, Game, Economy} are pairwise connected, it is not a 

218 
Decision-Making Agents 
 
 
clique because the set is contained in another clique {Roads, Game, Economy, 
Ticket Sale}, and it is therefore not maximal. 
 
The triangulation and clique identification techniques described above are 
combined into a formal two-step recursive algorithm for systematically 
identifying cliques from an arbitrary undirected moral graph: 
• 
Select a node X from the network N and make the cluster consisting of 
the node and its neighbor complete by adding the necessary edges. 
Choose the node that causes the least number of edges to be added. 
Break ties by choosing the node that induces the cluster with smallest 
weight, where the weight of a node is the number of states, and the 
weight of a cluster is the product of the weights of its constituent nodes. 
• 
Remove X along with its edges (forming a clique) and repeat the 
previous step if there are still nodes left. Otherwise, if there are no nodes 
left, N is now triangulated. Note that a clique is formed in this stage only 
if it is not a subset of a subsequently induced clique. 
Example 
The graph in Figure 6-37 shows the triangulated graph obtained from the moral 
graph in Figure 6-34. The table in the figure shows the variable selection 
ordering in the first step of the algorithm. In each selection, the weight of the 
variable is computed and the induced cluster or clique is identified if it is bold 
and shaded. 
The candidate variables to be selected first are Sprinkler, Concession, Ticket 
Sale, and Economy since, for each of these four variables, the cluster consisting 
of the variable and its neighbors is already complete without the requirements for 
adding any edge. For example, the cluster consisting of the variable Sprinkler and 
its two neighbors (Weather and Field) is complete, but the cluster consisting of 
the variable Weather and its two neighbors (Roads and Field) is not complete 
because an edge between Roads and Field needs to be added. The variable 
Concession is chosen among the four candidate variables because its weight 9 is 
the least among the four weights 12, 9, 108, and 36 for the four candidate 
variables respectively. The variable Concession is then removed along with the 
edge from it leading to Ticket Sale. The process then continues with the 
remainder of the network and the variable Sprinkler is chosen next. When the 

 
Bayesian Belief Networks 
219 
 
variable Economy is chosen, the corresponding induced cluster is not identified 
as a clique because it is a subset of a clique that is already identified. 
Weather
(3)
Sprinkler
(2)
Roads
(3)
Field
(2)
Game
(2)
Economy
(2)
Ticket Sale
(3)
Concession
(3)
No. of states
-
{Weather, Sprinkler, Field}
12
Sprinkler
-
{Weather, Roads}
9
Weather
-
{Weather, Roads, Field}
18
Field
(Roads, Field)
{Roads, Field, Game}
12
Game
-
{Roads, Game, Economy}
12
Economy
{Roads, Game, Economy, 
Ticket Sale}
36
Ticket Sale
-
{Roads}
3
Roads
-
{Ticket Sale, Concession}
9
Concession
Added 
Edges
Induced Cluster
Weight
Selected 
Variable
-
{Weather, Sprinkler, Field}
12
Sprinkler
-
{Weather, Roads}
9
Weather
-
{Weather, Roads, Field}
18
Field
(Roads, Field)
{Roads, Field, Game}
12
Game
-
{Roads, Game, Economy}
12
Economy
{Roads, Game, Economy, 
Ticket Sale}
36
Ticket Sale
-
{Roads}
3
Roads
-
{Ticket Sale, Concession}
9
Concession
Added 
Edges
Induced Cluster
Weight
Selected 
Variable
 
Figure 6-37: Triangulation and clique identification 
The network is now left with the four variables Weather, Roads, Field, and 
Game, and the selection of any of these will require an edge to be added. The two 
candidate variables are Field and Game as each of these has weight 12, and each 
of the remaining two variables Weather and Roads has weight 18. The selection 
of the variable Field needs the edge from Weather to Game to be added, and the 
selection of the variable Game needs the edge from Roads to Field to be added. 
Once we have identified cliques, we rank the nodes by assigning numbers to 
systematically construct a join tree out of cliques. Nodes are numbered from 1 to 
n in increasing order by assigning the next number to the node with the largest 
set of previously numbered neighbors. For example, an ordering of the nodes of 
the graph in Figure 6-1 is given below: 

220 
Decision-Making Agents 
 
 
1 – Weather 
2 – Sprinkler 
3 – Field 
4 – Roads 
5 – Game 
6 – Economy 
7 – Ticket Sale 
8 – Concession 
The choice between the two nodes Weather and Sprinkler as a starting node is 
arbitrary to break the tie. The node Field comes before the node Roads because 
Field has two neighbors, Weather and Sprinkler, which have already been 
numbered. On the other hand, only one neighbor Weather of the node Roads has 
been numbered. The rest of the sequence is numbered in a similar manner. 
To form a join tree based on the above ranking scheme of graph nodes, first 
order the cliques of the graph by rank of the highest vertex of each clique. For 
example, the set of five cliques C1, C2, C3, C4, and C5 in Figure 6-35 is as 
follows: 
C5: {Ticket Sale, Concession}  
C4: {Roads, Game, Economy, Ticket Sale} 
C3: {Roads, Field, Game} 
C1: {Weather, Roads, Field} 
C2: {Weather, Sprinkler, Field} 
For example, the clique C5 comes before the clique C4 because C5 contains the 
node Ticket Sale whose rank is higher than the rank of any node in C4. 
The join tree from an ordered set of cliques C1, C2, …, Cn are formed by 
connecting each Ci to a predecessor Cj sharing the highest number of vertices 
with Ci. For example, a join tree for the above ordered set of cliques C5, C4, C3, 
C1, C2 is shown in Figure 6-38. Each edge in the tree is labeled with the 
intersection of the adjacent cliques, which is called a separator set or a sepset.  

 
Bayesian Belief Networks 
221 
 
Roads
Game
Ticket Sale
Economy
Roads
Field
Game
Ticket Sale
Concession
Roads
Field
Weather
Field
Roads
Game
Ticket Sale
Weather
Sprinkler
Field
Weather
Roads
Field
C1
S13
C3
S34
S12
C2
C4
S45
C5
Cluster
Separator Set (sepset)
Legend
 
Figure 6-38: Join tree for the graph in Figure 6-35 
The following algorithm helps systematically construct a join tree as shown 
in Figure 6-38. The algorithm is optimal with respect to the mass and cost criteria 
adopted during the selection of sepsets. 
• 
Create a forest of n distinct trees, where each tree consists of only one 
node made out of the set of n cliques produced by the triangulation and 
clique identification procedure above. Also, create a set S of 
(
1)
n n −
 
distinct sepsets obtained by intersecting all possible pairs of distinct 
elements in the set of cliques. 
• 
Repeat the following steps 
1
n − times: 
− Select a sepset SXY (that is,
∩
X
Y ) from S that has the largest mass, 
where mass of a sepset SXY is the number of variables it contains. If 
two or more sepsets of equal mass can be chosen then choose the 
candidate sepset with the smallest cost, where the cost of a sepset 
SXY is the sum of the product of the number of states of the variables 
in X and product of the number of states of the variables in Y. 
− Insert the selected sepset SXY between the cliques X and Y only if X 
and Y are on different trees in the forest. 

222 
Decision-Making Agents 
 
 
In the construction of the join tree in Figure 6-38, first the forest is formed 
containing all five cliques C1, C2, C3, C4, and C5. Each of the sepsets S12, S13, and 
S34 has a mass 2 and weight 6, are therefore inserted first into the join tree. Note 
that the sepset {Field} was not inserted between C2 and C3 before S12 or S13 as 
{Field} has lower mass than each of S12 or S13. 
6.7.2 
Join Tree Initialization 
A join tree maintains a joint probability distribution at each node, cluster, or 
sepset) in terms of a belief potential, which is a function that maps each 
instantiation of the set of variables in the node into a real number. The belief 
potential of a set X of variables will be denoted as ϕX , and 
( )
x
ϕX
 is the number 
that the potential maps x onto. The probability distribution of a set X of variables 
is just the special case of a potential whose elements add up to 1. In other words, 
( )
( )
1
x
x
x
p x
ϕ
∈
∈
=
=
∑
∑
X
X
X
 
The marginalization and multiplication operations on potentials are defined in a 
manner similar to the same operations on probability distributions. 
Belief potentials encode the joint distribution p(X) of the belief network 
according to the following: 
C
S
( )
i
j
i
j
p
φ
φ
= ∏
∏
X
 
where 
Ci
ϕ
 and 
S j
ϕ
 are the cluster and sepset potentials respectively. We have the 
following joint distribution for the join tree in Figure 6-38: 
1
2
3
4
5
12
13
34
45
C
C
C
C
C
S
S
S
S
(
, , ,
,
, , ,
)
WRF
WSF
RFG
RGTE
TC
WF
RF
RG
T
p W S R F G E T C
φ φ φ φ φ
φ φ φ φ
φ
φ
φ
φ
φ
φ
φ φ φ
=
=
 
It is imperative that a cluster potential agrees on the variables in common with its 
neighboring sepsets up to marginalization. This imperative is formalized by the 
concept of local consistency. A join tree is locally consistent if for each cluster C 
and neighboring sepset S, the following holds: 
C
S
C\S
φ
φ
=
∑
 

 
Bayesian Belief Networks 
223 
 
To start initialization, for each cluster C and sepset S, set the following: 
C
S
1,
1
φ
φ
←
← 
Then assign each variable X to a cluster C that contains X and its parents pa(X). 
Then set the following: 
C
C (
|
(
))
p X
pa X
φ
φ
←
 
Example 
To illustrate the initialization process, consider the join tree in Figure 6-38. The 
allocation of prior and conditional probability tables are shown in Figure 6-39. 
The variable Field has been assigned to cluster C2 as it contains the variable and 
its parents Weather and Sprinkler. The probability table p(Weather) could have 
been assigned to any of C1 and C2, but C1 is chosen arbitrarily. 
Roads
Game
Ticket Sale
Economy
Roads
Field
Game
Ticket Sale
Concession
Roads
Field
Weather
Field
Roads
Game
Ticket Sale
Weather
Sprinkler
Field
Weather
Roads
Field
C1
S13
C3
S34
S12
C2
C4
S45
C5
(
|
,
)
p Field Weather Sprinkler
(
)
p Sprinkler
(
)
p Weather
(
|
)
p Concession Ticket Sale
(
|
,
,
)
p Ticket Sale Roads Game Economy
(
)
p Economy
(
|
,
)
p Game Roads Field
(
|
)
p Roads Weather
 
Figure 6-39: Allocation of prior and conditional probability tables for join tree 
initialization  
As an example, Figure 6-40 shows the computation of potential for the clique C1 
by multiplying p(Weather) and 
(
|
)
p Roads Weather . 

224 
Decision-Making Agents 
 
 
Weather
Field
Weather
Roads
Field
(
|
)
p Roads Weather
C1
S12
(
)
p Weather
0.30
p(Injury = snow)
0.15
p(Injury = rain)
0.55
p(Injury = sunny)
0.30
p(Injury = snow)
0.15
p(Injury = rain)
0.55
p(Injury = sunny)
0.05
0.15
0.80
snow
0.10
0.01
p(Roads = closed)
0.20
0.09
p(Roads = disruptive)
0.70
0.90
p(Roads = open)
rain
sunny
Weather ⇒
0.05
0.15
0.80
snow
0.10
0.01
p(Roads = closed)
0.20
0.09
p(Roads = disruptive)
0.70
0.90
p(Roads = open)
rain
sunny
Weather ⇒
0.05 × 0.30 × 1 = 0.0150
normal
closed
snow
0.05 × 0.30 × 1 = 0.0150
muddy
closed
snow
0.15 × 0.30 × 1 = 0.0450
normal
disruptive
snow
0.15 × 0.30 × 1 = 0.0450
muddy
disruptive
snow
0.80 × 0.30 × 1 = 0.2400
normal
open
snow
0.80 × 0.30 × 1 = 0.2400
muddy
open
snow
0.10 × 0.15 × 1 = 0.0150
normal
closed
rain
0.10 × 0.15 × 1 = 0.0150
muddy
closed
rain
0.20 × 0.15 × 1 = 0.0300
normal
disruptive
rain
0.20 × 0.15 × 1 = 0.0300
muddy
disruptive
rain
0.70 × 0.15 × 1 = 0.1050
normal
open
rain
0.70 × 0.15 × 1 = 0.1050
muddy
open
rain
0.01 × 0.55 × 1 = 0.0055
normal
closed
sunny
0.01 × 0.55 × 1 = 0.0055
muddy
closed
sunny
0.09 × 0.55 × 1 = 0.0495
normal
disruptive
sunny
0.09 × 0.55 × 1 = 0.0495
muddy
disruptive
sunny
0.90 × 0.55 × 1 = 0.4950
normal
open
sunny
0.90 × 0.55 × 1 = 0.4950
muddy
open
sunny
Initial Values
Field
Roads
Weather
0.05 × 0.30 × 1 = 0.0150
normal
closed
snow
0.05 × 0.30 × 1 = 0.0150
muddy
closed
snow
0.15 × 0.30 × 1 = 0.0450
normal
disruptive
snow
0.15 × 0.30 × 1 = 0.0450
muddy
disruptive
snow
0.80 × 0.30 × 1 = 0.2400
normal
open
snow
0.80 × 0.30 × 1 = 0.2400
muddy
open
snow
0.10 × 0.15 × 1 = 0.0150
normal
closed
rain
0.10 × 0.15 × 1 = 0.0150
muddy
closed
rain
0.20 × 0.15 × 1 = 0.0300
normal
disruptive
rain
0.20 × 0.15 × 1 = 0.0300
muddy
disruptive
rain
0.70 × 0.15 × 1 = 0.1050
normal
open
rain
0.70 × 0.15 × 1 = 0.1050
muddy
open
rain
0.01 × 0.55 × 1 = 0.0055
normal
closed
sunny
0.01 × 0.55 × 1 = 0.0055
muddy
closed
sunny
0.09 × 0.55 × 1 = 0.0495
normal
disruptive
sunny
0.09 × 0.55 × 1 = 0.0495
muddy
disruptive
sunny
0.90 × 0.55 × 1 = 0.4950
normal
open
sunny
0.90 × 0.55 × 1 = 0.4950
muddy
open
sunny
Initial Values
Field
Roads
Weather
1
φC
1
normal
closed
1
muddy
closed
1
normal
disruptive
1
muddy
disruptive
1
normal
open
1
muddy
open
Initial Values
Field
Weather
1
normal
closed
1
muddy
closed
1
normal
disruptive
1
muddy
disruptive
1
normal
open
1
muddy
open
Initial Values
Field
Weather
12
φS
 
Figure 6-40: Computation of potential 
6.7.3 
Propagation in Join Tree and Marginalization 
The join tree thus formed is not locally consistent as, for example, 
1
12
1
12
C
S
C \S
φ
φ
≠
∑
. 
An inconsistent join tree can be made consistent using a global propagation. The 
message passing mechanism is at the heart of global propagation. 

 
Bayesian Belief Networks 
225 
 
Consider two adjacent clusters C1 and C2 with sepset S. A message pass from 
C1 to C2 consists of the following two steps: 
• 
Projection: 
1
1
S
S
S
C
C \S
old
φ
φ
φ
φ
←
←∑
 
• 
Absorption: 
2
2
S
C
C
old
S
φ
φ
φ
φ
←
 
It can be easily verified that any number of messages passing as shown above 
encodes the joint distribution p(X) of the belief network. Global propagation is a 
systematic collection of message passing via the following two recursive 
procedures: Collect Evidence and Distribute Evidence. 
• 
Choose an arbitrary cluster C. 
• 
Unmark all clusters and call the following three steps of Collect 
Evidence(C): 
− Mark C. 
− Recursively call Collect Evidence on each unmarked neighboring 
cluster of C. 
− Pass a message from C to the cluster which invoked Collect 
Evidence(C) . 
• 
Unmark all clusters and call the following three steps of Distribute 
Evidence(C): 
− Mark C. 
− Pass a message from C to each of its unmarked neighboring cluster. 
− Recursively call Distribute Evidence on each unmarked neighboring 
cluster of C. 
Example 
Figure 6-41 shows the message flow order when the cluster C3 is chosen as the 
starting cluster in the above algorithm. First, Collect Evidence is called on cluster 
C3, which causes two calls of Collect Evidence on each of C1 and C4. The first 
call of these two calls on C1 triggers a call of Collect Evidence on C2. The node 
then passes message 1 to C1. The process continues, yielding a total of eight 
messages as shown in Figure 6-41.  

226 
Decision-Making Agents 
 
 
Roads
Game
Ticket Sale
Economy
Roads
Field
Game
Ticket Sale
Concession
Roads
Field
Weather
Field
Roads
Game
Ticket Sale
Weather
Sprinkler
Field
Weather
Roads
Field
C1
S13
C3
S34
S12
C2
C4
S45
C5
Starting
Cluster
5
6
1
3
8
7
4
Collect Evidence Messages
Distribute Evidence Messages
Legend
2
 
Figure 6-41: Message flow in global propagation 
Once the join tree has been made consistent, prior probabilities of variables 
can be obtained using marginalization. First, identify a cluster C that contains the 
variable X of interest. Then compute p(X) by marginalizing 
C
φ  as 
C
C\{
}
(
)
X
p X
φ
= ∑
. An example of this computation for the variable Field from 
cluster C2 is shown in Figure 6-42. 

 
Bayesian Belief Networks 
227 
 
normal
burst pipe
normal
burst pipe
normal
burst pipe
normal
burst pipe
normal
burst pipe
normal
burst pipe
Sprinkler
0.85
normal
snow
0.15
muddy
snow
0.99
normal
snow
0.01
muddy
snow
0.4
normal
rain
0.6
muddy
rain
0.9
normal
rain
0.1
muddy
rain
0.01
normal
sunny
0.99
muddy
sunny
0.2
normal
sunny
0.8
muddy
sunny
Potential 
Field
Weather
normal
burst pipe
normal
burst pipe
normal
burst pipe
normal
burst pipe
normal
burst pipe
normal
burst pipe
Sprinkler
0.85
normal
snow
0.15
muddy
snow
0.99
normal
snow
0.01
muddy
snow
0.4
normal
rain
0.6
muddy
rain
0.9
normal
rain
0.1
muddy
rain
0.01
normal
sunny
0.99
muddy
sunny
0.2
normal
sunny
0.8
muddy
sunny
Potential 
Field
Weather
Clique 
{
,
,
}
2
Weather Sprinkler Field
=
C
0.56 = 3.35 / (2.65 + 3.35)
p(Field = normal)
0.44 = 2.65 / (2.65 + 3.35)
p(Field = muddy)
0.56 = 3.35 / (2.65 + 3.35)
p(Field = normal)
0.44 = 2.65 / (2.65 + 3.35)
p(Field = muddy)
Sum (Shaded Rows): 2.65
Sum (Clear Rows): 3.35
2
C
φ
 
Figure 6-42: Marginalization from potential and normalization 
 
6.7.4 
Handling Evidence 
Figure 6-43 shows the overall flow for using evidence in join trees to compute 
the variables’ posterior probabilities. Compare this figure with Figure 6-33, 
which shows the flow for computing only prior probabilities. When new 
evidence on a variable is entered into the tree, it becomes inconsistent and 
requires a global propagation to make it consistent. The posterior probabilities 
can be computed via marginalization and normalization from the global 
propagation. If evidence on a variable is updated, then the tree requires 
initialization. Next, we present initialization, normalization, and marginalization 
procedures for handling evidence. 

228 
Decision-Making Agents 
 
 
Belief Network
Graphical
Transformation
Join Tree Structure
Inconsistent Join Tree
Consistent Join Tree
Initialization
Marginalization
p(X)
(
| )
p X e
Initialization &
Evidence Entry
Marginalization
Normalization
New Evidence
Evidence
Update
Global
Propagation
 
Figure 6-43: Steps for handling evidence in join trees 
As before, to start initialization, for each cluster C and sepset S, set the 
following: 
C
S
1,
1
φ
φ
←
← 
Then assign each variable X to a cluster C that contains X and its parents pa(X), 
and then set the following: 
C
C (
|
(
))
1
X
p X
pa X
φ
φ
λ
←
←
 
where 
X
λ  is the likelihood vector for the variable X. Now, perform the following 
steps for each piece of evidence on a variable X: 
• 
Encode the evidence on the variable as a likelihood 
new
X
λ
. 
• 
Identify a cluster C that contains X (e.g. one containing the variable and 
its parents). 
• 
Update as follows: 

 
Bayesian Belief Networks 
229 
 
new
X
C
C
X
new
X
X
λ
φ
φ
λ
λ
λ
←
←
 
Now perform a global propagation using the Collect Evidence and Distribute 
Evidence procedures. Note that if the belief potential of one cluster C is 
modified, then it is sufficient to unmark all clusters and call only Distribute 
Evidence(C). 
The potential 
C
ϕ  for each cluster C is now 
( , )
p
e
C
, where e denotes 
evidence incorporated into the tree. Now marginalize C into the variable as 
C
C\{
}
(
, )
X
p X e
φ
= ∑
 
Compute posterior 
(
| )
p X e  as follows: 
(
, )
(
, )
(
| )
( )
(
, )
X
p X e
p X e
p X e
p e
p X e
=
= ∑
 
To update evidence, for each variable X on which evidence has been obtained, 
update its likelihood vector. Then initialize the join tree by incorporating the 
observations. Finally, perform global propagation, marginalization, etc. 
6.8 
Complexity of Inference Algorithms 
Probabilistic inference using belief networks is computationally intractable, that 
is, NP-hard (Cooper 1990). Informally, this means if there exists an algorithm 
that solves our problems in polynomial time, then the polynomial-time algorithm 
would exist for practically all discrete problems, such as the propositional 
satisfiability problem. The fact that inferencing in belief networks is not tractable 
does not mean it can never be applied; it simply means that there are cases when 
its inferencing time will take too long for this algorithm to be practical. 
The computational complexity of Pearl’s message passing algorithm for 
acyclic networks can be shown to be 
(
2 )
d
O n
d
×
×
, where n is the number of 
vertices in the network and d is the network’s maximal in-degree. The 
computational complexity of Lauritzen and Spiegelhalter's junction tree 
algorithm equals 
(
2 )
c
O n×
, where n is the number of vertices in the network and 
c is the number of vertices in the largest clique in the clique tree that is 
constructed from the network. Note that the algorithm’s complexity is 
exponential in the size of the largest clique. 

230 
Decision-Making Agents 
 
 
If the clique sizes in the junction tree algorithm are bounded by a constant, 
then the algorithm takes linear time. Since the computational complexity of the 
junction tree algorithm relates exponentially to clique size, the best clique tree to 
use in practical applications is a tree inducing smallest state space. The problem 
of finding such a clique tree is known to be NP-hard (Wen, 1990). Various 
efficient heuristic algorithms are available for finding a clique tree for an acyclic 
network. However, these algorithms do not exhibit any optimality properties. 
Therefore, it seems unlikely that an exact algorithm can be developed to 
perform probabilistic inference efficiently over all classes of belief networks. 
This result suggests that research should be directed away from the search for a 
general, efficient probabilistic inference algorithm, and towards the design of 
efficient special-case (for example, tree structure or inherently modular network), 
average-case, and approximation algorithms. 
6.9 
Acquisition of Probabilities 
The acquisition of probabilities for belief network structures involves eliciting 
conditional probabilities from subject matter experts along causal directions. 
These probabilities are “causal” conditional probabilities of the form 
(
|
,
)
p Report Rain Sensor , indicating the chance of obtaining a sensor report from 
the appropriate sensor when it is raining. This chance is related to the sensor 
functionality in the presence of rain, which may be best estimated by the sensor 
designers. Similarly, a causal probability of the form 
(
|
,
)
p Game Field Roads  
indicates the chance of a game given the field and road conditions. The game 
referees, together with the people managing the game, can best estimate this 
probability. On the other hand, the “diagnostic” conditional probabilities in the 
belief context are probabilities of the form 
(
|
)
p Field Game , indicating of the 
chance that various field conditions will affect the status of the game. An 
experienced groundskeeper or a regular game observer in the field may best 
estimate this probability. Both causal and diagnostic probabilities can be used to 
compute joint probability distributions. 
The major issue related to eliciting probabilities from subject matter experts 
is how to phrase questions to experts so as to accurately and efficiently determine 
relevant prior and conditional probabilities (Druzdel and van der Gaag, 1995).  

 
Bayesian Belief Networks 
231 
 
Example 
Consider, for example, a revised version of the fragment of the network in Figure 
6-1 shown in Figure 6-44, which says that sunny weather (SW), good economy 
(GE), and holiday season (HS) together cause tickets to be sold out (TS). Each 
variable X (SW, GE, HS, and TS) in the network is binary with two states X and 
X
¬
. If we are eliciting causal probabilities, then experts will be asked questions 
for determining the priors 
(
)
p SW , 
(
)
p GE , and 
(
)
p HS , and the conditional 
(
|
,
,
)
p TS SW GE HS . 
Sunny
Weather
Tickets
Sold Out
Holiday
Session
Good
Economy
 
Figure 6-44: BN illustrating probability acquisition 
If prior probability 
(
)
p TS  is available, then diagnostic conditional 
probabilities are elicited by employing the arc reversal approach (Shachter and 
Heckerman, 1987). But even if the diagnostic conditional probabilities 
(
|
)
p SW TS , 
(
|
)
p GE TS , and 
(
|
)
p HS TS  are known, it is not possible to 
uniquely determine the causal probability 
(
|
,
,
)
p TS SW GE HS . 
Our experience suggests that subject matter experts, scientists, and 
knowledge engineers often comfortable drawing arrows in the causal direction 
once the term “causality” is explained. The directionality has great impact on the 
resultant ease of knowledge elicitation. For example, if all three arrows in the 
network in Figure 6-44 are reversed, then the resultant network is equally 
effective in determining whether the tickets are going to be sold out or not, but 
now the a priori probability 
(
)
p TS  is required, as are the conditional 
probabilities 
(
|
)
p SW TS , 
(
|
)
p GE TS , and 
(
|
)
p HS TS .  
The number of conditional probabilities required for a node to complete its 
conditional probability table, like 
(
|
,
,
)
p TS SW GE HS  for TS, grows 
exponentially with the number of parents. But we can employ the noisy-or 
technique to avoid building large probability tables, provided certain conditions 
are met (Pearl, 1988). Informally, the noisy-or technique states that any members 

232 
Decision-Making Agents 
 
 
of a set of independent conditions are likely to cause a certain event and that this 
chance is not reduced when several of these conditions occur simultaneously. In 
the case of the node TS, chances for the tickets to be sold out can only increase 
when several of the three conditions, sunny weather, good economy, and holiday 
season, occur simultaneously. Given this knowledge, we can generate the CPT 
(
|
,
,
)
p TS SW GE HS  of 8 independent entries from only three values. Formally, 
the noisy-or is a belief network node representing an event (e.g. Tickets Sold Out) 
at which the following assumptions are made:  
• 
The accountability condition requires that we explicitly list as parent 
nodes all the conditions likely to influence the event. In the example, this 
assumption requires that we explicitly list all the conditions likely to 
cause the tickets to be sold out. In other words, the condition states that 
an event is presumed false if all conditions listed as causes of the event 
are false. 
• 
The exception independence condition requires that whatever inhibits 
each parent node from implying the event is independent of whatever 
inhibits other parent nodes from implying the event. For example, 
assume that the only factor inhibiting the tickets to be sold out when the 
weather is sunny is high temperature, and the only factor inhibiting the 
tickets to be sold out when the economy is good is very high ticket price. 
The exception independence condition holds since the two inhibitory 
factors can be assumed independent of each other. On the other hand, 
going to the beach inhibits the tickets to be sold out when the weather is 
sunny, and the same factor inhibits the tickets to be sold when it is 
holiday season, thus violating the exception independence condition.  
Example 
An incomplete CPT, like the one in Table 6-9, can be completed by deriving the 
missing probabilities through the noisy-or technique. Suppose, only the following 
entries in the CPT are known: 
(
|
,
,
)
0.7
(
|
,
,
)
0.6
(
|
,
,
)
0.9
p TS SW
GE
HS
p TS
SW GE
HS
p TS
SW
GE HS
¬
¬
=
¬
¬
=
¬
¬
=
 

 
Bayesian Belief Networks 
233 
 
Sunny Weather (SW)  
SW 
¬SW 
Good Economy (GE) 
GE 
¬GE 
GE 
¬GE 
Holiday Session (HS) 
HS
¬HS
HS
¬HS
HS
¬HS
HS ¬HS 
TS 
? 
? 
? 
0.7 
? 
0.6 
0.9 
? 
Tickets Sold Out 
(TS) 
¬TS 
? 
? 
? 
0.3 
? 
0.4 
0.1 
? 
Table 6-9: An incomplete CPT to be completed by the noisy-or technique 
Therefore, we have the following: 
(
|
,
,
)
0.3
(
|
,
,
)
0.4
(
|
,
,
)
0.1
p
TS SW
GE
HS
p
TS
SW GE
HS
p
TS
SW
GE HS
¬
¬
¬
=
¬
¬
¬
=
¬
¬
¬
=
 
Now, 
(
|
,
,
)
1
(
|
,
,
)
p TS SW GE
HS
p
TS SW GE
HS
¬
= −
¬
¬
. 
The accountability condition states that TS is false if all conditions listed as 
causes of TS are false. Therefore, 
(
|
,
,
)
(
|
,
)
p
TS SW GE
HS
p
TS SW GE
¬
¬
=
¬
 
Thus, 
(
|
,
,
)
1
(
|
,
)
p TS SW GE
HS
p
TS SW GE
¬
= −
¬
 
Now, the exception independence condition states that the inhibitory conditions 
SW and GE for TS are independent of each other. Therefore, 
(
|
,
)
(
|
) (
|
)
p
TS SW GE
p
TS SW p
TS GE
¬
=
¬
¬
 
Thus, 
(
|
,
,
)
p TS SW GE
HS
¬
 
1
(
|
) (
|
)
p
TS SW p
TS GE
= −
¬
¬
 
1
(1
(
|
))(1
(
|
))
p TS SW
p TS GE
= −
−
−
 
1
(1
0.7)(1
0.6)
0.88
= −
−
−
=
 
The rest of the entries in the CPT can be computed in a similar manner. 
 

234 
Decision-Making Agents 
 
 
6.10 Advantages and Disadvantages of Belief Networks 
Like any other computational formalism, belief network technology offers certain 
advantages and disadvantages.  Advantages of belief networks include: 
• 
Sound theoretical foundation: The computation of beliefs using 
probability estimates is guaranteed to be consistent with probability 
theory. This advantage stems from the Bayesian update procedure’s strict 
derivation from the axioms of probability. 
• 
Graphical 
models: 
Belief 
networks 
graphically 
depict 
the 
interdependencies that exist between related pieces of domain 
knowledge, enhancing understanding of the domain. The structure of a 
belief network captures the cause-effect relationships that exist amongst 
the variables of the domain. The ease of causal interpretation in belief 
network models typically makes them easier to construct than other 
models, minimizing the knowledge engineering costs and making them 
easier to modify. 
• 
Predictive and diagnostic reasoning: Belief networks combine both 
deductive/predictive 
and 
abductive/diagnostic 
reasoning. 
Interdependencies among variables in a network are accurately captured 
and speculative if-then type computation can be performed. 
• 
Computational tractability: Belief networks are computationally tractable 
for most practical applications. This efficiency stems principally from the 
exploitation of conditional independence relationships over the domain. 
We have presented an efficient single-pass evidence propagation 
algorithm for networks without loops. 
• 
Evidence handling: Evidence can be posted to any node in a belief 
network. This means that subjective evidence can be posted at an 
intermediate node representing an abstract concept. 
A major disadvantage of belief network technology is the high level of effort 
required to build network models. Although it is relatively easy to build a belief 
network structure with the help of subject matter experts, the model will require a 
significant amount of probability data as the number of nodes and links in the 
structure increase. The size of a CPT corresponding to a node with multiple 
parents can potentially be huge. For example, the number of independent entries 
in the CPT of a binary node (a node with two states) with 8 binary parent 
variables is 128. 

 
Bayesian Belief Networks 
235 
 
Belief networks are also poor at handling continuous variables. Current 
software handles continuous variables in a very restrictive manner (for example, 
they must be Gaussian and can only be children). Lener et al. (2001) developed 
an inference algorithm for static hybrid belief networks, which are Conditional 
Linear Gaussian models, where the conditional distribution of the continuous 
variables assigned to the discrete variables is a multivariate Gaussian. Cob and 
Shenoy (2004) developed an inference algorithm in hybrid belief networks using 
Mixtures of Truncated Potentials. But these techniques are yet to be incorporated 
in commercial software.  
6.11 Belief Network Tools 
Various free and commercial software tools and packages are currently available 
for manipulating Bayesian belief networks incorporating some of the above 
functionalities. Of these, HUGIN (www.hugin.com) is the most widely 
commercial package that contains a flexible, user friendly and comprehensive 
graphical user interface. The package allows modeling decision-making 
problems via influence diagrams and handles continuous variables with some 
restrictions. 
Other 
popular 
tools 
and 
packages 
include 
Netica 
(http://www.norsys.com/), BayesiaLab (http://www.bayesia.com/), and BNetTM 
(http://www.cra.com/). 
6.12 Further Readings 
The book by Pearl (1988) is still the most comprehensive account on belief 
networks, and more generally on using probabilistic reasoning to handle 
uncertainty. Various cases of the evidence propagation algorithms in polytrees 
presented here closely follow Pearl’s book. Though Pearl himself developed an 
exact inference algorithm for DAGs, called loop cutset conditioning (Pearl, 
1986), the junction tree algorithm of Lauritzen and Spiegelhalter (1988), as 
refined by Jensen et al (1990) in HUGIN, is more general and the most popular 
inference algorithm for general belief networks. A good comprehensive 
procedural account of the algorithm can be found in (Huang and Darwiche, 
1996). Jensen’s books (1996, 2002) are also useful guides in this field. 
 

This page intentionally left blank
This page intentionally left blank

237 
Chapter 7 
 
Influence Diagrams for 
Making Decisions 
 
 
 
 
This chapter describes how to use Bayesian belief networks to make decisions, 
and therefore provides a foundation for building probabilistic epistemic models 
for decision making agents. Belief networks do not explicitly incorporate the 
concepts of action and utility, which are ubiquitous in the decision-making 
context. By incorporating the concepts of action and utility, belief networks are 
converted to influence diagrams, subsuming the functionality of the normative 
theory of decision making under expected utility theory and decision trees. For 
inferencing in influence diagrams, we extend the junction tree algorithm for 
belief networks presented in the last chapter. The extended algorithm presented 
in this chapter compiles an influence diagram into a strong junction tree in which 
the computation of maximum expected utility can be done by local message 
passing in the tree. 
7.1 
Expected Utility Theory and Decision Trees 
A decision is a choice between several alternative courses of risky or uncertain 
action. Expected Utility Theory (EUT) states that the decision-maker chooses 
between alternative courses of action by comparing their expected utility values, 
which is the weighted sum obtained by adding the utility values of possible 
outcomes multiplied by their respective probabilities. Formally, if 
1
{ ,...,
}
n
A
a
a
=
 
is the set of all possible actions as decision alternatives and 
1
{
,...,
}
n
W
w
w
=
 is the 
corresponding set of possible outcomes or world states, then the expected utility 
(EU) for action 
ka  is the following: 
(
)
(
) (
|
)
k
i
i
k
i
EU a
U w p w a
=∑
 

238 
Decision-Making Agents 
 
 
where 
(
)
i
U w  is the utility of the outcome 
iw  and 
(
|
)
i
k
p w a
 is the probability 
that the outcome is 
iw  if the action 
ka  is taken. The Maximum Expected Utility 
(MEU) operation is used to choose the best alternative course of action: 
( )
max
(
)
k
k
MEU A
EU a
=
 
In cases where an outcome 
iw  does not depend on the decision-maker’s action 
but on other context-dependent factors which the decision maker has no control 
over, then the expected utility is simply defined without actions, that is, 
(
) (
)
i
i
i
EU
U w p w
=∑
 
The computation of MEU is not relevant in this case.  
A decision tree poses an ordered list of systematic questions that leads the 
decision-maker through a series of decisions to a logical endpoint that results in a 
solution. These likely outcomes of possible solutions to the problem are projected 
as utilities. The systematic exploration of likely outcomes is organized in a tree in 
which each branch node represents a choice between a number of alternatives, 
and each leaf node represents a solution. A branch node is either a chance node 
or an action node. MEUs are computed at action nodes and EUs are computed at 
chance nodes. 
Example 
Figure 7-1 shows an example decision tree in the context of our ball-game 
example in which ovals, rectangles, and diamonds represent chance nodes, 
decision nodes, and terminal nodes respectively. The root node is a chance node 
representing the status of rain, and branches are its uncertain outcomes. It is 70% 
likely that rain will be present and 30% likely that rain will be absent. These two 
possible outcomes are represented as two branches coming out of the chance 
node for rain. Given either of these two alternatives of rain status, the next step is 
to decide whether to proceed with the game or abandon it. This decision requires 
exploring four possible combinations of options between the rain status and the 
game action. For each of these combinations, the likely ticket sale volumes need 
to be considered to compute profits that are projected as utilities. 
For example, if the action taken is to proceed with the game, and rain is 
present, then there is a 10% chance that the ticket sale volume will be high and 
90% chance that the sale volume will be low. Each of these two possibilities 
yields different profit as utility and are attached to the terminal node. So the top 

 
Influence Diagrams 
239 
 
branch of the tree in the figure encodes a possible outcome, which states that if 
the rain is present, action to proceed with the game is taken, and the ticket sale 
volume is high, then the utility is 900. Eight possible outcomes are explored in 
this manner in the context of the example. 
Rain
Profit
Game
Ticket Sale
Game
Ticket Sale
Ticket Sale
Ticket Sale
Profit
Profit
Profit
present (0.7)
absent (0.3)
proceed
abandon
proceed
abandon
Profit
Profit
Profit
Profit
high (0.1)
low (0.9)
-300
-300
-300
-300
900
-150
900
-150
high (0.1)
low (0.9)
low (0.2)
low (0.2)
high (0.8)
high (0.8)
EU = -45
(900×0.1-150×0.9)
EU = -300
EU = 690
EU = -300
MEU = -45
MEU = 690
EU = 175.5
(-45×0.7+690×0.3)
Decision Node
Chance Node
Terminal Node
Decision Node
Chance Node
Terminal Node
 
Figure 7-1: An example decision tree 
EUs and MEUs at branch nodes are computed bottom up along the tree (right 
to left in the figure). For example, the EU at the Ticket Sale node of the topmost 
branch 
is 
computed 
using 
the 
formula 
(
) (
)
i
i
i
U w p w
∑
, 
yielding 
900
0.1
( 150)
0.9
×
+ −
×
, that is, –45. To compute the MEU at the node Game in 
the upper branch of the tree, EUs of the two actions Proceed and Abandon, 
EU(Proceed) and EU(Abandon),  need to be computed using the formula 
(
) (
|
)
i
i
k
i
U w p w a
∑
. Now, the EUs of the two Ticket Sale nodes of the upper 
branch are –45 and –300. Therefore, the MEU of the node Game is the maximum 
of –45 and –300, that is, –45. Finally, the EU of the node Rain is computed using 
the formula 
(
) (
)
i
i
i
U w p w
∑
, yielding 175.5. 

240 
Decision-Making Agents 
 
 
 
The procedure for computing utilities using decision trees, as explained 
above, is a simple statistical procedure. Decision trees also provide easy-to-
understand graphic representations. But laying out all the possible options is not 
feasible when there are several factors, each with multiple outcomes. Moreover, 
the procedure provides no scope for incorporating subjective knowledge. 
Influence diagrams address some of these issues. 
7.2 
Influence Diagrams 
Influence diagrams are belief networks augmented with decision variables and 
utility functions, and used to solve decision problems. There are three types of 
nodes in an influence diagram: 
• 
Chance nodes  (that is, belief network nodes), represented by ovals 
• 
Decision nodes, represented by rectangles 
• 
Value or utility nodes, represented by diamonds 
As opposed to chance nodes representing probabilistic variables, decision nodes 
represent actions that are under the full control of decision-makers and hence no 
CPT is attached to a decision node. 
Example 
Figure 7-2 shows an example influence diagram from our game example. There 
are two chance nodes (Rain and Ticket Sale), one decision node (Game), and one 
value node (Profit). The arrow from the node Rain to the node Ticket Sale 
represents the causal influence of the status of rain on the volume of ticket sales. 
The CPT quantifies this causality as in usual belief networks. As we mentioned 
before, there is no CPT attached to a decision node, but the arrow from the 
chance node Rain to the decision node Game represents the fact that the 
knowledge of the status of rain should be known before making the decision 
(whether to Proceed or Abandon) about the game. The value node Profit has two 
parents, representing the causal influence of the ticket sale volume and the game 
status on the profit amount. The table represents a utility function whose 
definition quantifies the utilities for all possible combinations of the parent 
values. For example, if the decision-maker proceeded with the game and the 
ticket sale is low then the profit is negative 150.  
 

 
Influence Diagrams 
241 
 
Game
proceed | abandon
Profit
Rain
present | absent
Ticket Sale
high | low
Decision Node
Chance Node
Value Node
Decision Node
Chance Node
Value Node
CPT for Ticket Sale
0.2
0.9
p(Ticket Sale=low)
0.8
0.1
p(Ticket Sale=high)
absent
present
Rain ⇒
0.2
0.9
p(Ticket Sale=low)
0.8
0.1
p(Ticket Sale=high)
absent
present
Rain ⇒
Utility for Profit
Ticket Sale ⇒
-300
low
abandon
proceed
high
low
high
Game ⇒
Utility
-300
-150
900
Ticket Sale ⇒
-300
low
abandon
proceed
high
low
high
Game ⇒
Utility
-300
-150
900
 
Figure 7-2: An example influence diagram 
 
Inferences in influence diagrams involve computing the EU for each of the 
action alternatives. This kind of computation is sensitive to the temporal order in 
which the decisions are made. Therefore, influence diagrams require a directed 
path connecting all the decision nodes sequentially. EUs are evaluated on the 
condition that an optimal choice has been made in all previous steps. 
Suppose, 
1,...,
m
A
A  is the ordered set of actions to be decided (
m
A  is the last 
action), where each 
iA  is a set of mutually exclusive action options; 
0,...,
m
V
V   is 
a partition of the chance variables 
1,...,
n
X
X  such that 
1
iV − is the set of variables 
instantiated before deciding on the action 
iA . Therefore we have the following 
ordering: 
0
1
1
...
m
m
V
A
V
A
V
≺
≺
≺
≺
≺
 
Then MEU for a chosen set of alternatives for 
1,...,
m
A
A  is given below: 

242 
Decision-Making Agents 
 
 
1
0
1
1
1
0
1
1
(
,...,
)
max
...
max
(
,
,...,
|
,...,
)
m
m
m
m
m
m
A
A
V
V
V
V
U
MEU A
A
U
p V V
V
A
A
−
=
×
∑
∑∑
∑∑
 
The MEU for action 
kA  is given as 
1
0
1
1
(
)
max
...
max
(
,...,
|
,...,
,
,...,
)
k
m
k
m
m
k
k
m
k
m
A
A
V
V
V
U
MEU A
U
p V
V
V
V
A
A
−
−
=
×
∑∑
∑∑
 
Thus the computation of MEU in an influence diagram involves a series of 
alternating sum-marginalization and max-marginalization to eliminate the 
variables. 
7.3 
Inferencing in Influence Diagrams 
In this section, we illustrate inferencing in influence diagrams considering the 
following two special cases of interactions between decisions and other types of 
variables: 
• 
Non-intervening interactions: Actions which have no impact on variables 
(or probability distribution) in the network 
• 
Intervening interactions: Actions which have an impact on variables in 
the network that then affect the beliefs of other variables, or affect the 
utilities of value nodes  
Next, we present a junction-tree-based algorithm for inferencing in influence 
diagrams containing arbitrary sequences of interactions. 
Example 
Figure 7-3 shows an influence diagram with one node of each type: chance, 
decision, and value. The decision node represents a non-intervening action 
Game, which has no impact on 
(
)
p Rain , that is, 
(
|
)
(
)
p Rain Game
p Rain
=
. 
Given the likelihood e as shown in the figure, the posterior probability for Rain 
and expected utilities for each of the actions are computed as follows: 
(
| )
0.9
0.2
0.692
(
| )
(
| )
0.1
0.8
0.308
p Rain
Present e
p Rain e
p Rain
Absent e
α
=
⎡
⎤
⎡
⎤⎡
⎤
⎡
⎤
=
=
=
⎢
⎥
⎢
⎥⎢
⎥
⎢
⎥
=
⎣
⎦
⎣
⎦⎣
⎦
⎣
⎦
 
EUs for each of the actions based on the posterior probabilities are computed as 
follows: 

 
Influence Diagrams 
243 
 
Game
proceed | abandon
Profit
Rain
present | absent
Utility for Profit
Rain ⇒
-1500
absent
abandon
proceed
present
absent
present
Game ⇒
Utility
-1500
4500
-750
Rain ⇒
-1500
absent
abandon
proceed
present
absent
present
Game ⇒
Utility
-1500
4500
-750
0.8
p(Rain=absent)
0.2
p(Rain=present)
0.8
p(Rain=absent)
0.2
p(Rain=present)
CPT for Rain
0.9
0.1
e
⎡
⎤
= ⎢
⎥
⎣
⎦
 
Figure 7-3: Influence diagram with non-intervening action 
(
)
(
)
(
)
(
)
(
)
( 750)
0.692
4500
0.308
865
(
)
( 1500)
0.692
( 1500)
0.308
1500
EU Game
Proceed |e
U Game
Proceed,Rain= Present
p Rain= Present |e
U Game
Proceed,Rain= Absent
p Rain= Absent |e
EU Game
Abandon|e
=
=
=
×
+
=
×
= −
×
+
×
≈
=
= −
×
+ −
×
= −
 
The MEU operation to choose the best alternative course of action is computed 
below: 
(
| )
{
(
| ),
(
| )}
{865, 1500}
865
MEU Game e
max EU Game
Proceed e EU Game
Abandon e
max
=
=
=
=
−
=
 
This MEU computation can be executed in a decision tree equivalent to the 
influence diagram in Figure 7-3. One such tree is shown in Figure 7-4, where the 
decision and chance variables are systematically laid out (parent variables before 
their children) to explore all possible combinations of states. Therefore, there will 
be as many leaf nodes in the tree as the number of states in the joint distribution 
of the set of all decision and chance variables. Each branch yields some utility in 
terms of positive or negative profit obtained from the utility table in Figure 7-3. 
The computation of EU in the tree is performed in the usual manner starting at 
the leaf nodes and moving backwards. 

244 
Decision-Making Agents 
 
 
Rain
Profit
Game
Game
Profit
present (0.2)
absent (0. 8)
proceed
abandon
proceed
abandon
Profit
Profit
4500
-1500
-750
-1500
-750
4500
EU: 3450
(-750×0.2+4500×0.8)
As many leaf nodes 
as the number of 
states in the joint 
distribution of the set 
of all decision and 
chance variables
 
Figure 7-4: Decision tree equivalent to the influence diagram in Figure 7-3 
When evidence on the variable Rain is obtained, its posterior probability is 
computed by component-wise multiplication followed by normalization. The EU 
is then computed based on the posterior probability of Rain as shown in Figure 
7-5. 
EU: 865
(-750×0.69+4500×0.31)
0.9
0.1
e
⎡
⎤
= ⎢
⎥
⎣
⎦
Rain
present (0.69)
absent (0. 31)
 
Figure 7-5: Decision tree of Figure 7-4 with evidence entered 
Example 
Figure 7-6 shows an influence diagram with a set of non-intervening actions. The 
EU is computed by summing up the MEU of each of the constituent actions. In 
other words, 

 
Influence Diagrams 
245 
 
(
,
| )
(
| )
(
| )
MEU Clean Field SaleTicket e
MEU Clean Field e
MEU SaleTicket e
=
+
 
MEU for the variable Sale Ticket can be computed as follows:  
(
)
( 750)
0.692
4500
0.308
865
(
)
( 1500)
0.692
( 1500)
0.308
1500
(
)
865
EU SaleTicket
Proceed |e
EU SaleTicket
Abandon|e
MEU SaleTicket |e
=
= −
×
+
×
≈
=
= −
×
+ −
×
= −
≈
 
Similarly, 
MEU 
of 
Clean 
Field 
can 
be 
computed, 
giving 
(
| )
200
MEU CleanField e = −
. 
Therefore, 
(
,
| )
665
MEU Clean Field SaleTicket e ≈
 
Clean Field
proceed | abandon
Cost
Rain
present | absent
Profit
Sale Ticket 
proceed | abandon
0.8
p(Rain=absent)
0.2
p(Rain=present)
0.8
p(Rain=absent)
0.2
p(Rain=present)
CPT for Rain
Utility for Cost
Rain ⇒
0
absent
abandon
proceed
present
absent
present
Clean Field ⇒
Utility
-1500
-200
-200
Rain ⇒
0
absent
abandon
proceed
present
absent
present
Clean Field ⇒
Utility
-1500
-200
-200
Utility for Profit
Rain ⇒
-1500
absent
abandon
proceed
present
absent
present
Sale Ticket ⇒
Utility
-1500
4500
-750
Rain ⇒
-1500
absent
abandon
proceed
present
absent
present
Sale Ticket ⇒
Utility
-1500
4500
-750
Game
proceed | abandon
0.9
0.1
e
⎡
⎤
= ⎢
⎥
⎣
⎦
 
Figure 7-6: Influence diagram with a set of non-intervening actions 
A decision tree equivalent to the influence diagram of Figure 7-6 is shown in 
Figure 7-7. The utility along a branch of the tree is computed by summing up the 
cost and profit corresponding to the actions specified along the branch. For 
example, the topmost branch states that if rain is present and the option Proceed 
is chosen for each of the three action variables Game, Clean Field, and Sale 
Ticket, then the utility is –950. This utility is obtained by adding the cost of 
proceeding with the Clean Field action in the presence of rain (-200) to the profit 
for proceeding with Sale Ticket action in the presence of rain (-750). 

246 
Decision-Making Agents 
 
 
Rain
Utility
Game
Game
present (0. 2)
absent (0.8)
Utility
proceed
abandon
-950
-950
4500
EU: 3410
(-950×0.2+4500×0.8)
Sale Ticket
-1700
Utility
Utility
proceed
abandon
-2250
Sale Ticket
-3000
Utility
Utility
proceed
abandon
-950
Sale Ticket
-1700
Utility
Utility
proceed
abandon
-2250
Sale Ticket
-3000
Utility
Utility
proceed
abandon
4300
Sale Ticket
-1700
Utility
Utility
proceed
abandon
4500
Sale Ticket
-1500
Utility
Utility
proceed
abandon
4300
Sale Ticket
-1700
Utility
Utility
proceed
abandon
4500
Sale Ticket
-1500
Clean Field
Clean Field
Clean Field
Clean Field
proceed
proceed
proceed
proceed
abandon
abandon
abandon
abandon
abandon
abandon
proceed
proceed
Utility
=
Cost + Profit
-950
-2250
-950
-2250
4300
4500
4300
4500
-950
-950
4500
4500
 
Figure 7-7: Decision tree equivalence to the influence diagram in Figure 7-6 
Example 
Figure 7-8 shows an influence diagram with one decision node representing the 
intervening action Game, that directly influences the variable Ticket Sale. The 
posterior probability and EU for the variable Ticket Sale when the option 
Proceed is chosen is computed as follows: 
0.42
(
| ,
)
0.51
0.07
(
)
4500
0.42
( 500)
0.51
( 1500)
0.07
1504
p Ticket Sale e Game
Proceed
EU Game
Proceed |e
⎡
⎤
⎢
⎥
=
= ⎢
⎥
⎢
⎥
⎣
⎦
=
=
×
+ −
×
+ −
×
≈
 
Similarly, EU for the option Abandon is computed similarly, yielding the 
following: 
(
)
1500
EU Game
Abandon|e
=
= −
 
Therefore, 
(
)
1500
MEU Game|e = −
 

 
Influence Diagrams 
247 
 
Game
proceed | abandon
Profit
Rain
present | absent
Utility for Profit
Rain ⇒
-1500
absent
abandon
proceed
present
absent
present
Game ⇒
Utility
-1500
4500
-750
Rain ⇒
-1500
absent
abandon
proceed
present
absent
present
Game ⇒
Utility
-1500
4500
-750
0.8
p(Rain=absent)
0.2
p(Rain=present)
0.8
p(Rain=absent)
0.2
p(Rain=present)
CPT for Rain
0.9
0.1
e
⎡
⎤
= ⎢
⎥
⎣
⎦
 
Figure 7-8: Influence diagram with intervening action 
A decision tree equivalent to the influence diagram in Figure 7-8 is shown in 
Figure 7-9. 
Rain
Profit
Game
Ticket Sale
present (0.2)
absent (0.8)
proceed
abandon
Profit
high (0.2)
low (0.7)
4500
-500
400
(4500×0.2-
500×0.7-
1500×0.1)
-1500
400
Value: 3272
(400×0.2+3990×0.8)
Profit
none (0.1)
-1500
Profit
Ticket Sale
Profit
high (0)
low (0)
4500
-500
Profit
none (1)
-1500
Profit
Game
Ticket Sale
proceed
abandon
Profit
high (0.90)
low (0.09)
4500
-500
3990
(4500×0.90-
500×0.09-
1500×0.01)
-1500
3990
Profit
none (0.01)
-1500
Profit
Ticket Sale
Profit
high (0)
low (0)
4500
-500
Profit
none (1)
-1500  
Figure 7-9: Decision tree equivalent to the influence diagram in Figure 7-8 

248 
Decision-Making Agents 
 
 
7.4 
Compilation of Influence Diagrams 
Compiling an influence diagram involves transforming it to a strong junction tree  
(or rooted junction tree), that maintains a special ordering relative to the root 
clique. Just to recall, a junction tree is a tree of cliques if for each pair 
1
C  and 
2
C  of cliques, 
1
2
∩
C
C  is contained in every clique on the path connecting 
1
C  
and 
2
C . A separator of the two adjacent cliques 
1
C  and 
2
C  is the intersection 
1
2
∩
C
C .  
The compilation steps for influence diagrams are shown on the right side of 
Figure 7-10. Steps to compile belief networks into junction trees are shown on 
the left. Details of each step are explained below. 
Partitioning
Moralization
Deletion (Arc)
Triangulation
Deletion (Node)
Strong Join Tree Formation
for Belief Networks
for Influence Diagrams
Moralization
Clique Identification
Triangulation
Join Tree Formation
Triangulation
Procedure
Cliques Numbering
 
Figure 7-10: Steps in the compilation process of influence diagrams 
• 
Partitioning:  Partition the chance variables 
1,...,
n
X
X  into 
0,...,
p
V
V  such 
that 
1
iV − is the set of variables instantiated before deciding on the action 
iA , yielding the ordering 
0
1
1
...
m
m
V
A
V
A
V
≺
≺
≺
≺
≺
. 

 
Influence Diagrams 
249 
 
• 
Deletion (Arc): Delete edges pointing into decision nodes. 
• 
Moralization: Marry parents with common children including parents of 
utility nodes and drop the directions of the original links. 
• 
Deletion (Node): Delete utility nodes along with their edges. 
• 
Triangulation: Triangulate in such as a way that it facilitates the 
computation of MEU. Do this by adopting a special elimination order 
based on the ordering found during partitioning. Start eliminating 
variables using the triangulation procedure in the following order: 
eliminate variables from the set 
m
V , then the decision variable 
m
A , then 
variables from the set 
1
m
V −, then the decision variable 
1
m
A −, and so on. 
The elimination sequence constitutes a numbering where the first 
eliminated variable is assigned the highest number. 
• 
Clique Numbering: A clique is numbered after its variable with the 
highest number k (otherwise 1) such that the rest of the variables in the 
clique have a common neighbor outside the clique with a number lower 
than k. 
• 
Strong Join Tree Formation: A strong root of a junction tree is a 
distinguished clique R such that for each pair 
1
C  and 
2
C  of adjacent 
cliques with 
1
C  closer to R than 
2
C , there is an ordering in 
2
C  that 
respects ≺and with the vertices of the separator 
1
2
∩
C
C  precede the 
vertices of 
2
1
−
C
C . A junction tree is strong if it has a strong root. The 
ordering within a junction tree ensures that the computation of the MEU 
can be done by local message passing in the tree. The following steps are 
followed to form a strong junction tree from the sequence 
1,...,
n
C
C  of 
cliques arranged in increasing order according to their indices: 
− Start with 
1
C  as the root clique with number 1. 
− Connect the cliques as follows in the increasing order of their 
numbers: First, compute the following for each clique 
(
1)
k
C
k >
: 
1
1
k
k
k
i
i
S
C
C
−
=
=
∩∪
 
Connect the clique 
k
C  with another clique which contains 
kS . 
− Variables that were eliminated first are placed farthest from the root 
in the strong junction tree. 

250 
Decision-Making Agents 
 
 
The following example illustrates these steps to construct a strong junction 
tree. 
Example 
Figure 7-11 is an example influence diagram modified from our original belief 
network for the game example. The chance variable Field is the only parent of 
the decision variable Game, indicating that information about field condition is 
required before the action for canceling the game or going ahead with the game. 
If the game is on, there will be a cost for organizing the game, but profits can be 
made from both ticket sales and concession.  
Weather
sunny | rain | snow
Sprinkler
burst pipe | normal
Roads
open | disruptive | closed
Field
muddy | normal
Economy
good | bad
Ticket Sale
high | low | none
Concession
high | low | none
Cost
Game
on | cancel
Profit
Profit
Utility for Cost
-2000
cancel
on
Ticket Sale ⇒
Utility
0
-2000
cancel
on
Ticket Sale ⇒
Utility
0
Utility for Profit
500
low
none
high
Ticket Sale ⇒
Utility
0
5000
500
low
none
high
Ticket Sale ⇒
Utility
0
5000
Utility for Profit
100
low
none
high
Concession ⇒
Utility
0
1000
100
low
none
high
Concession ⇒
Utility
0
1000
 
Figure 7-11: Example influence diagram 
Consider the following ordering of the chance and decision variables of the 
belief network: 
{
,
,
,
}
{
}
{
,
,
}
Sprinkler Weather Field Roads
Economy Ticket Sale Concession
≺
≺
Game
 

 
Influence Diagrams 
251 
 
The sequence in which the variables are eliminated during the triangulation step 
is shown in Figure 7-12, along with the assigned numbers. 
4
Roads
1
Sprinkler
3
Field
5
Game
6
Economy
7
Ticket Sale
8
Concession
2
Weather
Number
Variable
4
Roads
1
Sprinkler
3
Field
5
Game
6
Economy
7
Ticket Sale
8
Concession
2
Weather
Number
Variable
 
Figure 7-12: Numbering of variables during triangulation 
The five cliques identified during the triangulation step and their numbers are 
given below: 
C1: {Weather, Sprinkler, Field} → 3 
C2: {Weather, Roads, Field} → 4 
C3: {Roads, Field, Game} → 5 
C4: {Roads, Game, Economy, Ticket Sale} → 7 
C5: {Ticket Sale, Concession} → 8 
To construct a strong junction tree, we start with C1 as the root. Figure 7-13 
shows the strong junction tree based on these cliques. The variables that were 
eliminated first (such as Concession and Ticket Sale) are placed farthest from the 
root (clique number 1) in the strong junction tree. 

252 
Decision-Making Agents 
 
 
Roads
Game
Ticket Sale
Economy
Roads
Field
Game
Ticket Sale
Concession
Roads
Field
Weather
Field
Roads
Game
Ticket Sale
Weather
Roads
Field
4
5
1
7
8
ROOT
Clique 
Number
Weather
Sprinkler
Field
C5
C4
C3
C2
C1
 
Figure 7-13: Strong junction tree 
 
7.5 
Inferencing in Strong Junction Tress 
Inferencing in a strong junction tree employs a special collect operation from the 
leaves to the root of the tree. This operation is similar to the one for computing 
EU at the root node of a decision tree as shown in Figure 7-1. The difference is 
that a node here takes the form of a clique rather than a single variable. In 
addition to associating a probability potential with each clique of the tree, a 
utility potential is also associated with each clique. The utility potential for a 
clique is the sum of the utility functions assigned to it. The utility potential for a 
clique is a null function if no utility functions are assigned to it. 
Example 
Continuing with our earlier example, Figure 7-14 shows an assignment of 
probability distributions and utility functions. The two profit utility functions are 

 
Influence Diagrams 
253 
 
assigned to the clique C5 since the parents of both these two utility nodes belong 
to C5. Similarly, the cost utility function could have been assigned to any of the 
two cliques C3 and C4 containing the variable Game, and C3 was chosen 
arbitrarily. 
Roads
Game
Ticket Sale
Economy
Roads
Field
Game
Ticket Sale
Concession
Roads
Field
Weather
Field
Roads
Game
Ticket Sale
Weather
Roads
Field
ROOT
Special collect 
operation from the 
leaf to the root
Weather
Sprinkler
Field
(
|
,
)
p Field Weather Sprinkler
(
)
p Sprinkler
(
)
p Weather
(
|
)
p Concession Ticket Sale
(
|
,
,
)
p Ticket Sale Roads Game Economy
(
)
p Economy
(
|
)
p Roads Weather
(
)
U Cost
(
)
U Profit
(
)
U Profit
U
∑
U
∑∑
C5
C4
C3
C2
C1
 
Figure 7-14: Assignment of probability distributions and utility functions to 
cliques 
The computation of the utility potential for the node C5 is given below in 
Figure 7-15 based on the two utility functions for the two profit nodes. 
Consider the following ordering of the chance and decision variables as 
before: 
{
,
,
,
}
{
}
{
,
,
}
Sprinkler Weather Field Roads
Economy Ticket Sale Concession
≺
≺
Game
 
Then the MEU for Game based on this ordering is: 
{ , , }
(
)
max
( , ,
| ,
,
, ,
)
G
E T C
U
MEU Game
U
p E T C S W F R G
=
×
∑∑
 

254 
Decision-Making Agents 
 
 
0
none
none
100
low
none
1000
high
none
500
none
low
600
low
low
1500
high
low
5000
none
high
5100
low
high
6000
high
high
Utility
Concession
Ticket Sale
0
none
none
100
low
none
1000
high
none
500
none
low
600
low
low
1500
high
low
5000
none
high
5100
low
high
6000
high
high
Utility
Concession
Ticket Sale
Utility for Profit
500
low
none
high
Ticket Sale ⇒
Utility
0
5000
500
low
none
high
Ticket Sale ⇒
Utility
0
5000
Utility for Profit
100
low
none
high
Concession ⇒
Utility
0
1000
100
low
none
high
Concession ⇒
Utility
0
1000
 
Figure 7-15: Utility potential 
But the special collect operation on the strong junction tree in Figure 7-14 allows 
the MEU to be computed at node 5 (clique C3) as follows: 
{ , }
{ , }
(
)
max
(
)
( ,
,
)
( ,
, ,
)
( )
( ,
)
G
R F
E T
C
U
MEU Game
U Cost
p R F G
p R G T E
U P
p T C
=
×
×
∑
∑
∑∑
 
where U(P) is the sum utility as shown in Figure 7-15. In this example, node 5 
could also have been considered as the root node. 
 
7.6 
Further Readings 
More details on the strong junction tree algorithm presented in this chapter 
can be found in (Jensen et al., 1994) and (Shenoy, 1992). From the historical 
perspective, Howard and Matheson (1981) first introduced influence diagrams, 
and their ability to model decision problems with uncertainty by transforming 
them into decision trees, for a single decision-maker. Shachter (1986) described a 
reduction-based approach for evaluating influence diagrams. 

255 
Chapter 8 
 
Modal Logics for the Possible 
World Epistemic Model  
 
 
 
 
This chapter presents modal and epistemic logics for the possible world model 
that provide a foundation for decision-making agents. Unlike our earlier chapter 
on the logical (classical) and probabilistic foundations of epistemic models, in 
this chapter we start with a brief presentation of the history of modal logics since 
the field is not as widely known. We then introduce some relevant systems of 
modal logics, including K, D, T, S4, B, and S5, and their interrelationships. We 
develop possible world semantics of modal logics using accessibility relations 
and establish the associated soundness and completeness results. We then define 
a modal resolution scheme to efficiently reason within various systems of modal 
logics. We present the epistemic instantiation of modal logics as a way of 
reasoning with various mentalistic constructs of agents such as “belief” and 
“knowledge.” 
Modal logics are often criticized for their coarse-grained representation of 
knowledge of assertion possibilities. That is to say, if two assertions in an 
application are possible in the current world, their further properties are 
indistinguishable in the modal formalism even if an agent knows that one of them 
is true in twice as many of its possible worlds as compared to the other one. 
Epistemic logic, that is, the logic of knowledge and belief, cannot avoid this 
shortcoming because it inherits the syntax and semantics of modal logics. In the 
penultimate section of this chapter, we develop an extended formalism of modal 
epistemic logic that allows an agent to represent its degrees of support about an 
assertion. The degrees are drawn from qualitative and quantitative dictionaries 
that are accumulated from an agent's a priori knowledge about the application 
domain. Using the accessibility hyperelation concept, possible-world semantics 
of the extended logic and its rational extension, is developed, and the soundness 
and completeness results are established. 

256 
Decision-Making Agents 
 
 
8.1 
Historical Development of Modal Logics 
Propositions in classical logics are categorized as either true or false. In modal 
logics, true propositions can be divided into necessary and contingently true 
propositions in a given context. No context allows straightforward inconsistency 
(that is, A and not A are true simultaneously). Within the context where 
mathematical and physical theories are accepted, propositions like “there are 
infinitely many prime numbers” and “the sum of two sides of a triangle is greater 
than the remaining third side” are true by the necessity of mathematical and 
physical theories. On the other hand, propositions like “Tajmahal is in India” and 
“Paris is the capital of France” are contingently true, that is, true only by their 
existence or specific conditions. Similarly, propositions like “Pyramids are in 
France” are contingently false propositions, and  “there is only a finite number of 
prime numbers” or “Maria loves me and I am not loved” is false by the necessity 
of mathematical or logical properties and therefore termed as impossible. Figure 
8-1 presents this categorization of propositions in modal logic.  
Proposition
True
False
Necessary
Contingently True
Contingently False
Impossible
Possible
 
Figure 8-1: Categorization of propositions 
Traditionally, the modality is restricted to the two basic modal concepts, 
possibility and necessity, and the ones that are definable in terms of these two. A 
logic based on these concepts is called a classical modal logic, and a classical 
modal statement ”is a statement containing the words “necessary”, “possible”, or 
words definable in terms of these two words. However, the scope of modal logic 
has been widened by extensive work throughout the last century to include: 1) 
deontic modals such as obligations, permission, may, must, ought; 2) temporal 

 
Modal Logics 
257 
 
modals such as since, after, next; and 3) epistemic modals such as knowledge and 
belief. Modal logic is a study of valid arguments involving modal concepts. 
The origin of modal logics goes back to the work of Aristotle. In his work of 
Organon, Prior Analytics of De Interpretation, he developed a theory of modal 
statements. He called statements of the form “It is necessary …” and “It is 
possible …” apodeictic and problematic, respectively, and he called ordinary 
propositions assertoric statements. Aristotle developed a theory of a modal logic, 
called modal syllogism  (a syllogism is an inference with two premises, major 
and minor, and one conclusion) by generalizing his theory of pure assertoric or 
categorical syllogism. An example modal syllogism from the 72 different types 
that Aristotle studied is as follows: 
It is necessary that all M is P 
All S is M 
It is necessary that all S is P 
Unlike categorical syllogism, Aristotle’s modal syllogism contains a number of 
controversial ideas which were discussed by several authors (see Lukasiewicz’s 
book (1951)). Aristotle also showed that the terms “necessary” and “possible” 
can be defined in terms of each other using negation. 
Aristotle’s modal syllogism remained almost neglected until Hugh MacColl 
published a series of papers in Mind during the late 19th century and early 20th 
century under the title Symbolic Reasoning. MacColl’s work on symbolic 
reasoning is based on implication (:), which is different from the classical 
implication, and is defined in terms of classical implication using the necessity 
operator as follows: 
:
(
)
A B
A
B ε
=
→
 
But unlike Aristotle’s modal syllogism, MacColl’s study did not produce a 
complete axiomatic system. 
The root of modern modal logics can be found in Lewis’s book Survey of 
Symbolic Logic (Lewis, 1918; later also included in (Lewis and Langford, 1930)). 
Following MacColl, Lewis adopted a version of implication called strict 
implication, different from the “material implication” of Whitehead and Russel’s 
Principia Mathematica  (Whiteheadn and Russell, 1925-27). In Lewis’s strict 
implication, “P implies Q” is synonymous with “P is deducible from Q.” Lewis 
pointed out that this synonymy does not hold in the case of material implication. 
Using material implication leads to paradoxes like “a false proposition implies 

258 
Decision-Making Agents 
 
 
every proposition” and “a true proposition is implied by any proposition”. Lewis 
also pointed out that if “P is consistent with Q” means “P does not imply the 
falsity of Q” and “Q is independent of P” means “P does not imply Q” then, in 
terms of material implication, no two propositions can be at once consistent and 
independent. 
Let us consider the following example from Lewis’ Survey of Symbolic 
Logic. Consider P as “roses are red” and Q as “sugar is sweet”. P is consistent 
with Q, and P is independent of Q. Now, it is not the case that “roses are red” is 
true and “sugar is sweet” is false, since “sugar is sweet” is true. Thus, 
(
)
P
Q
¬
∧¬
, that is, P
Q
→
. But P does not strictly imply Q as “sugar is sweet” 
cannot be derived from “roses are red”. 
Lewis introduced self-consistency, or the possibility of a proposition P as 
◊P , which is read as “P is self-consistent” or “P is possible”.  Then he defined 
strict implication in terms of negative possibility as follows (Lewis used 
conjunction P
Q
∧
 as simply PQ and the symbol ‘¬’ for negation): 
(
)
◊
P
Q
P
Q
∝
≡¬
∧¬
 
Thus, “P strictly implies Q” means “It is false that it is possible that P should be 
true and Q false.” Consider an example where P is “he doesn’t do his homework” 
and Q is “he will fail in the examination”. Therefore, the negation of P
Q
∝
 is 
(
)
◊P
Q
∧¬
, which means that it is possible he doesn’t do his homework and he 
will not fail in the examination. The possibility of “he doesn’t do his homework 
and he will not fail in the examination” suggests that P
Q
∝
 does not hold in this 
case as opposed to P
Q
→
. Lewis also distinguished between intensional and 
extensional disjunctions. Lewis argued that the meanings of either-or in these 
disjunctions are different from each other. 
Unlike MacColl, Lewis developed several complete axiomatic systems of 
modal logic. The axiomatic system set out by him first in 1918, known as the 
Survey, takes logical impossibility as a primitive symbol. Later in his book, the 
impossibility concept was eliminated and the primitive idea of possibility, using 
the symbol ◊, was introduced. Impossibility was then defined as 
◊
¬
. This 
definition gave Lewis’s system of strict implication the following set of axioms 
(with changes suggested by E. L. Post), where the strict implication ∝ is defined 
as 
(
)
◊
P
Q
P
Q
∝
≡¬
∧¬
: 

 
Modal Logics 
259 
 
A1:
A5:
(
)
A2:
A6:((
)
(
))
(
)
A3:
A7:
A4:
(
)
(
)
A8:(
)
(
)
P
Q
Q
P
P
P
Q
P
P
P
Q
Q
R
P
R
P
P
P
P
P
P
Q
R
Q
P
R
P
Q
Q
P
∧
∝
∧
∝¬ ¬
∧
∝
∝
∧
∝
∝
∝
∝
∧
¬
∝¬
∧
∧
∝
∧
∧
∝
∝¬
∝¬
◊
◊
◊
 
Lewis called the axiom A8 as the axiom of syllogism. The system included 
multiple inference rules, including substitution, detachment, and adjunction, as 
follows: 
• 
Substitution: The result of replacing at one or more places an expression 
in a theorem by its equivalent expression is itself a theorem. The result of 
replacing any variable in a theorem by any wff (well-formed formula) is 
itself a theorem. 
• 
Detachment: If P is a theorem and P
Q
∝
 is a theorem, then so is Q. This 
is, of course, the modal logic equivalent of classical modus ponens 
• 
Adjunction: If P and Q are theorems, then so is P
Q
∧
. In classical logic 
adjunction is redundant because P
Q
∧
 can be derived from P and Q. 
McKinsey (1934) showed that A5 can be deduced from the rest of the axioms 
and inference rules. In his book, Lewis proposed a new set of axioms for various 
modal systems as follows: 
B1-B6:
A1-A6
B7:(
(
))
B8: (
)
B9:(
) (
)
(
)
Sameas
P
P
Q
Q
P
Q
Q
P Q
P
Q
P
Q
∧
∝
∝
∧
∝
∃∃
¬
∝
∧¬
∝¬
◊
◊
 
The first seven axioms (B1-B7) are deductively equivalent to the axioms A1-A7, 
and constitute Lewis’ most primitive system S1 of modal logics. The system S2 is 
obtained by adding to S1 the axiom B8, which Lewis called the consistency 
postulate. The axioms B1-B8 can be derived from his Survey system axioms A1-
A8. The axiom B9 states that there are at least two propositions, say P and Q, 
such that P implies nothing about the truth or falsity of Q. Lewis introduced this 
axiom to prevent interpretation of the postulates B1-B8 of strict implication as a 
system for material implication by interpreting ◊P  as equivalent to P and the 
symbol ‘∝’ is syntactically equivalent to ‘→’. Lewis argued that the formula 
(
)
(
)
P
Q
P
Q
→
∨
→¬
 is a theorem of the system of material implication, but 
(
)
(
)
P
Q
P
Q
∝
∨
∝¬
 contradicts B9. 

260 
Decision-Making Agents 
 
 
One aspect of the study of modal systems is to determine properties of their 
modalities and modal functions, for example, whether ◊P  and 
◊◊P
¬ ¬
 are 
equivalent. A modal function is recursively defined as follows: 
• 
Each propositional variable is a modal function of one variable. 
• 
If F and G are modal functions, then each of 
F
¬
, ◊F , F
G
∧
 is a 
modal function of the number of variables it contains. 
• 
Any function equivalent by definition to a modal function is a modal 
function. 
A modality is a modal function of one variable in whose construction no 
operator other that ◊ and ¬ are used. For example, 
◊◊P
¬ ¬
 is a modality, 
whereas ◊
◊
P
P
∧
¬ is a modal function but not a modality. 
With a view that some modalities in Lewis’s system have no intuitive 
interpretation, Becker (1930) proposed the following set of axioms, any one or 
more of which can be added to the Survey system (A1-8) to reduce the number of 
modalities:  
C10:
C11:
C12:
◊
◊
◊
◊
◊◊
◊◊
P
P
P
P
P
P
¬ ¬
∝¬ ¬¬ ¬
∝¬ ¬
∝¬ ¬
 
Becker calls C12 as Brouwerian Axiom. C11 is deductively equivalent to C10 
and C12. Becker also introduces an inference rule (we will refer to it as Becker’s 
rule) as “if P
Q
∝
 is established as a theorem then so is◊
◊
P
Q
∝
.” 
The addition of C11 to the Survey system allows us to reduce all complex 
modalities to six, namely, P , 
P
¬
, ◊P , 
◊P
¬
, 
◊P
¬ ¬
, and ◊P
¬
. Parry (1939) 
proved that there are only 42 non-reducible complex modalities in Lewis’s 
Survey system. McKinsey (1940) demonstrated that S2, and hence S1, has an 
infinite number of complex modalities. In particular, he proved that all modalities 
of the form 
...
◊◊◊or ◊n  are not reducible, which makes Lewis’s systems open.  
Lewis in his book defined a necessarily true proposition P as “it is not 
logically inconceivable that P should be false” (pp.161), that is, 
◊P
¬ ¬
 
symbolically. Axioms C10-C12 can therefore be rewritten as follows with   as 
the symbol of necessary:  
10:
11:
12:
◊
◊
◊
C
P
P
C
P
P
C
P
P
∝
∝
∝




 

 
Modal Logics 
261 
 
Lewis’s system S4 is determined by B1-B7 (which is equivalent to A1-A7) and 
C10. Axiom A8 can be derived from S4, thus making S4 stronger than the Survey 
system (S3). Lewis’s system S5 is determined by B1-B7 plus C11. The system S5 
is stronger than S4. 
In the early 1930s, Gödel reviewed Becker’s work in connection with 
intuitionistic logic, and formalized assertions of provability by a connective B, 
reading Bα  as “α  is provable.” Gödel defined an axiomatic system consisting 
of  
( (
)
)
B
B
B
B
B
BB
α
α
α
α
β
β
α
α
→
→
→
→
→
 
and the inference rule “from α  infer Bα .” Gödel showed that the system is 
equivalent to Lewis’s S4 when Bα  is translated as 
α

. Modal logics are 
currently presented in the above axiomatic style of Gödel. 
The Decision problem of a logical system is to find whether an arbitrary 
given sentence of the system is provable. McKinsey (1941) was the first to offer 
a solution to the decision problem of S2 and S4 by means of algebraic 
interpretation via finite matrices. He first defined a matrix M as a quintuple 
,
, , ,
〈
−∗×〉
K D
, where K is a set and D is a non-empty proper subset of K, called 
the designated elements, − and ∗ are unary functions defined over K, and ×  is a 
binary function defined over K. The symbols −, ∗, and ×  correspond to the 
negation, possibility, and conjunction of a modal system, respectively. A normal 
matrix is one that satisfies conditions corresponding to Lewis’s rules of 
inference, adjunction, and replacement. McKinsey proved that a normal S2 
matrix is a Boolean algebra with certain characteristic properties, and provided a 
set of necessary and sufficient conditions for a matrix to be a normal S2 matrix. 
He developed similar results for S4. McKinsey transformed the decision problem 
of a modal system to the decision problem in the equivalent Boolean algebra. 
McKinsey’s proposal is interesting from theoretical point of view, but highly 
impractical as it requires a huge number of matrices be considered to decide on a 
formula.  
During the late 1950s and early 1960s, Kripke published several papers on 
model theoretic semantics of modal logics. Kripke defined (1963) a normal 
model structure for the systems S4, S5, T, and B in terms of possible worlds and 
accessibility relation. Semantic tableaux methods were used to prove 
completeness results. 

262 
Decision-Making Agents 
 
 
During the latter half of the last century, the traditional concept of modality 
(that is, “necessary” and “possible”) was interpreted in a variety of ways, 
including deontic notions of obligation and permissibility, epistemic notions of 
knowledge and belief, and tense notions of now, since, until, etc. Our specific 
interest here is the epistemic interpretation that includes reasoning with an 
agent’s knowledge. The formal study of epistemic logic was initiated by von 
Wright (1951), and his insights were extended by Hintikka (1962). 
8.2 
Systems of Modal Logic 
Many scientists, logicians, and philosophers have proposed hundreds of systems 
of modal logics since the revival of the area by Lewis in the beginning of the last 
century. The aim here is not to cover each and every system, but introduce a few 
important ones that are useful and practical, and can potentially form a basis for 
representing and reasoning with agents’ knowledge. From this perspective, we 
first lay down some needed requirements from candidate modal systems, 
including Lewis’s distinction between strict and material implications and their 
relations to the modal notions of possibility and necessity. We then introduce 
some important modal systems that satisfy those requirements, including some of 
Lewis’ with different axiomatic bases.  
• 
Any modal system should contain “properly” the propositional logic or 
the system of material implication as termed by Lewis. This means that 
the axioms and inference rules of a candidate modal system should 
contain all the axioms and inference rules of propositional logic. In 
addition, the possibility operator ◊ should not be definable in terms of a 
combination of the logical connectives. 
• 
The definition of the necessity of a proposition P, denoted as 
P

, is that 
it is not possible that P should be false. Symbolically, 
◊
P
P
≡¬ ¬

. 
• 
P strictly implies Q means that it is not possible that P should be true and 
Q false. Symbolically,
(
)
◊
P
Q
P
Q
∝
≡¬
∧¬
, that is, 
(
)
P
Q
P
Q
∝
≡
→

 
since 
◊
P
P
≡¬ ¬

 and 
(
)
P
Q
P
Q
→
≡¬
∧¬
. 
• 
Whatever follows logically from a necessarily true proposition is 
necessarily true. Symbolically, 
(
)
P
P
Q
Q
∧
∝
→


. 
• 
Valid propositions are not merely true but necessarily true. Symbolically, 
if 
P
A
 then 
P
A
. 
The second and third properties come from Lewis’s own definitions of necessity 
and strict implication. The last two somewhat correspond to the inference rule in 

 
Modal Logics 
263 
 
Lewis’s systems. The set of requirements listed above yield the most basic 
system K of modal logic. Next, we introduce this system formally and then bring 
additional modal properties to yield various other modal systems. 
A formal axiomatic theory K for the modal logic is defined as follows:  
• 
The symbols of K are a modal propositional alphabet: 
− 
Parentheses ( and ) 
− 
Logical connectives ¬ and → 
− 
Modality ◊ 
− 
Propositional variables P, Q, R, … 
• 
Formulae of K are inductively defined as follows: 
− 
All propositional variables are formulae. 
− 
If F is a formula then so are 
F
¬
and ◊F . 
− 
If F and G are formulae the so is F
G
→
. 
− 
An expression is a formula if and only if it can be shown to be a 
formula on the basis of the above three conditions. 
• 
If F, G, and H are any formula of K , then the following are axioms of K: 
− 
Propositional axioms: 
PC1: 
(
)
F
G
F
→
→
 
PC2: 
(
)
((
)
(
))
F
G
H
F
G
F
H
→
→
→
→
→
→
 
PC3: (
)
(
)
G
F
F
G
¬
→¬
→
→
 
− 
Modal axiom K: 
K: 
(
)
(
)
F
G
F
G
→
→
→



 
• 
The following are the rules of inference of K (A  means deducibility in 
K): 
− 
Modus Ponens (MP): If 
F
A
 and 
F
G
→
A
 then 
G
A
 
− 
Rule of Necessitation (RN):  If 
F
A
 then 
F
A
 
• 
The following are the definitions in K: 

264 
Decision-Making Agents 
 
 
(
)
(
)
(
)
◊
F
G
F
G
F
G
F
G
F
G
F
G
G
F
F
F
∧
≡¬
→¬
∨
≡¬
→
↔
≡
→
∧
→
≡¬ ¬

 
It is clear that K includes the classical propositional calculus PC. 
An intuitively sound principle that can be considered as part of this very 
basic system of modal logic is that whatever is necessarily true is true. This is 
called the axiom of necessity or T axiom. Symbolically 
T :
F
F
→

 
But, for the purpose of reasoning with, for example, an agent’s permissible 
and obligatory actions via deontic logic, the symbol   will be interpreted “it is 
obligatory that.” The axiom T in this context is intuitively unsatisfactory because 
whatever is obligatory is not necessarily true. The following weaker deontic 
axiom D is considered instead:  
D:
◊
F
F
→

 
For the purpose of reasoning with an agent’s knowledge and belief via 
epistemic logic, the symbols   and ◊ will be taken as knowledge and belief 
operators respectively, and interpreted as “agent knows that” and “agent believes 
that,” respectively. In this context, an intuitively sound assumption is that an 
agent knows what it knows and knows its beliefs. These two principles yield the 
following two potential axioms 4 and E (C10 and C11, respectively, presented 
above) to be considered for modal systems-based epistemic logics:  
4:
E :◊
◊
F
F
F
F
→
→



 
Finally, an intuitively sound principle is that if something is true then it is 
necessarily possible, which can be axiomatized as the following Brouwerian 
axiom B (axiom C12 presented in the historical development section):  
B:
◊
F
F
→
 
We now introduce a few renowned modal systems with K as their common 
foundation. Later we will study their properties and interrelationships. The 
system D is obtained from K by adding the axiom D into K.  The system T is 
obtained from K by adding T into K. The systems that are obtained from T by 
adding separately the two axioms 4 and E are equivalent to Lewis’s S4 and S5, 

 
Modal Logics 
265 
 
respectively. The system B is obtained from T by adding the axiom B. The 
system S5’ is obtained from T  by adding axioms 4 and E.  
8.3 
Deductions in Modal Systems 
This section presents some deduction principles and techniques, and then derives 
some well-known results within the modal systems K, D, T, B, S4, and S5 to be 
used later for establishing the soundness and completeness results. The bracketed 
information alongside a derivation step lists any axioms, steps, or inference rules 
used to derive the step (PL stands for Propositional Logic). 
8.3.1 
Principle of Duality 
Proof by duality eliminates tedious repetitions. The dual of a formula F, written 
as 
*
F , is obtained from F by repeatedly applying the following set of rules: 
DR1: 
*
P
P
=
, where P is a propositional variable 
DR2: 
*
*
(
)
F
F
¬
=
 
DR3: 
*
*
*
(
)
F
G
F
G
∧
=
∨
 
DR4: 
*
*
*
(
)
F
G
F
G
∨
=
∧
 
DR5: 
*
*
*
(
)
(
)
F
G
G
F
→
= ¬
→
 
DR6: 
*
*
*
(
)
(
)
F
G
F
G
↔
= ¬
↔
 
DR7: 
*
*
(
)
(
)
◊
F
F
=

 
DR8: 
*
*
(
)
(
)
F
F
=
◊

 
Note that if a formula only contains ¬, ∧, ∨, ◊, and   symbols, then its dual 
can be obtained by replacing each propositional variable by its negation and 
interchanging all occurrences of ∧ with ∨, and ◊ with  . 
Example 
Steps for computing the dual of 
(
)
(
)
P
Q
P
Q
→
→
→



 are given below: 
Step 1: 
*
( (
)
(
))
P
Q
P
Q
→
→
→



 
 
[Dual] 
Step 2: 
*
*
((
)
( (
)) )
P
Q
P
Q
¬
→
→
→



 
 
[DR 5] 
Step 3: 
*
*
*
( ((
)
(
) )
(
) )
◊
Q
P
P
Q
¬ ¬
→
→
→


 
[DR 5 & DR 7] 

266 
Decision-Making Agents 
 
 
Step 4: 
*
*
*
*
( ( (
)
(
))
(
))
◊
◊
◊
Q
P
Q
P
¬ ¬
→
→
¬
→
 
[DR 5 & DR 7] 
Step 5: 
( (
)
(
))
◊
◊
◊
Q
P
Q
P
¬ ¬
→
→
¬
→
 
 
[DR 1] 
Step 6: 
(
(
)
(
))
◊
◊
◊
Q
P
Q
P
¬ ¬ ¬
→
→
→
 
 
[PL] 
 
Result 
If 
F
G
→
A
 then 
*
*
G
F
→
A
 
If 
F
G
↔
A
 then 
*
*
F
G
↔
A
 
 
8.3.2 
Theorems of K 
 
TK 1: If 
F
G
→
A
 then 
F
G
→
A

 
Step 1: 
F
G
→
A
 
Step 2: 
(
)
F
G
→
A
  
 
 
RN] 
Step 3: 
(
)
(
)
F
G
F
G
→
→
→
A


  
[Axiom K] 
Step 4: 
F
G
→
A

 
 
 
[Step 2, Step 3, & MP] 
 
TK 2: If 
F
G
→
A
 then 
◊
◊
F
G
→
A
 
Step 1: 
F
G
→
A
 
Step 2: 
G
F
¬
→¬
A
  
 
 
[PL] 
Step 3: 
(
)
G
F
¬
→¬
A
  
 
 
[RN] 
Step 4: 
G
F
¬
→¬
A

 
 
 
[TK 1] 
Step 5: 
F
G
¬ ¬
→¬ ¬
A


  
 
[PL] 
Step 6: 
◊
◊
F
G
→
A
  
 
 
[Definition] 
 

 
Modal Logics 
267 
 
TK 3: 
(
)
(
)
F
G
F
G
∧
↔
∧



 
Step 1: F
G
F
∧
→
  
 
 
 
[PL] 
Step 2: 
(
)
F
G
F
∧
→

  
 
 
 
[RN] 
Step 3: 
(
)
F
G
F
∧
→


  
 
 
 
[TK 1] 
Step 4: 
(
)
F
G
G
∧
→


  
 
 
 
[Similar to Step 3] 
Step 5: 
(
)
F
G
F
G
∧
→
∧



  
 
 
[PL, Step 3, & Step 4] 
Step 6: 
(
)
F
G
F
G
→
→
∧
   
 
 
[PL] 
Step 7: 
(
(
))
F
G
F
G
→
→
∧

  
 
 
[Step 6 & RN] 
Step 8: 
(
)
F
G
F
G
→
→
∧


  
 
 
[Step 7 & TK 1] 
Step 9: 
(
)
(
(
))
G
F
G
G
F
G
→
∧
→
→
∧



 
[Axiom K & RS] 
Step 10: 
(
(
))
F
G
F
G
→
→
∧



   
 
[Step 8, Step 9, & PL] 
Step 11: 
(
)
F
G
F
G
∧
→
∧



  
 
 
[Step 10 & PL] 
Step 12: 
(
)
(
)
F
G
F
G
∧
↔
∧



   
 
[Step 5 & Step 11] 
 
TK 4: If 
F
G
H
∧
→
A
 then 
F
G
H
∧
→
A


 
Step 1: 
F
G
H
∧
→
A
 
Step 2: 
(
)
F
G
H
∧
→
A

  
[TK 1 & RS] 
Step 3: 
F
G
H
∧
→
A


  
[TK 3 & RS] 
 
TK 5: 
(
)
F
G
F
G
∨
→
∨
A 


K
 
Step 1: 
F
F
G
→
∨
AK
  
 
 
[PL] 
Step 2: 
(
)
F
F
G
→
∨
A 

K
   
 
[TK 1] 
Step 3: 
(
)
G
F
G
→
∨
A 

K
   
 
[Similar to Step 2] 
Step 4: 
(
)
F
G
F
G
∨
→
∨
A 


K
  
 
[Step 2, Step 3, & PL] 
 

268 
Decision-Making Agents 
 
 
TK 6: (
)
(
)
◊
◊
◊
F
G
F
G
∨
↔
∨
 
TK6 follows from TK 3 by applying the principle of duality. 
 
TK 7: (
)
(
)
◊
◊
◊
F
G
F
G
∧
→
∧
 
Step 1: F
G
F
∧
→
  
 
 
[PC] 
Step 2: (
)
◊
◊
F
G
F
∧
→
  
 
 
[TK 2] 
Step 3: (
)
◊
◊
F
G
G
∧
→
  
 
 
[Similar to Step 2] 
Step 4: (
)
◊
◊
◊
F
G
F
G
∧
→
∧
  
 
[Step2, Step 3 & PL] 
 
8.3.3 
Theorems of D 
 
TD 1: 
◊
◊
F
F
∨
¬
AD
 
Step 1: 
◊
F
F
→
A 
D
  
 
 
[Axiom D] 
Step 2: 
◊
◊
F
F
¬ ¬
→
AD
  
 
 
[Definition of  ] 
Step 3: 
◊
◊
F
F
∨
¬
AD
 
 
 
[PL] 
 
TD 2: 
◊
A
F
D
 or 
¬ ⊥
A

D
 
Step 1: 
(
)
(
)
◊
◊
◊
¬
∨
↔
¬
∨
A
F
F
F
F
D
  
[TK 6] 
Step 2: 
(
)
◊
◊
↔
→
A
F
F
F
D
  
 
[PL & Defn. of  ] 
Step 3: 
◊
→
A F
F
D
  
 
 
[Axiom D] 
Step 4: 
◊
A
F
D
  
 
 
 
[Step  2, Step 3, & PL] 
 

 
Modal Logics 
269 
 
8.3.4 
Theorems of T 
 
TT 1: 
◊
F
F
→
AT
 
Step 1: 
F
F
¬
→¬
A 
T
  
 
[Axiom T] 
Step 2: 
F
F
¬¬
→¬ ¬
A

T
   
[PL] 
Step 3: 
F
F
↔¬¬
AT
  
 
[PL] 
Step 4: 
◊
F
F
→
AT
  
 
[Step 2, Step 3, PL, and Defn. of ◊] 
 
TT 2: 
◊
F
F
→
A 
T
 
Follows immediately from Axiom T and TT 1. 
 
8.3.5 
Theorems of S4 
 
TS4 1: 
F
F
↔
A 

S4
 
Step 1: 
F
F
→
A 
S4
  
 
[Axiom T] 
Step 2: 
F
F
→
A 

S4
  
 
[TK 1] 
Step 3: 
F
F
→
A 

S4
  
 
[Axiom 4] 
Step 4: 
F
F
↔
A 

S4
 
 
[Step 2, Step 3, & PL] 
TS4 2: 
◊
◊◊
F
F
↔
AS4
 
TS4 2 follows by applying the duality principle on TS4 1. 
 
TS4 3: 
◊
F
F
→
A 
 
S4
 
Step 1: 
◊
F
F
→
A 

S4
   
 
[Axiom D] 

270 
Decision-Making Agents 
 
 
Step 2: 
◊
F
F
→
A 
 
S4
  
 
[TK 1] 
Step 3: 
F
F
↔
A 

S4
   
 
[From TS4 1 & PL] 
Step 4: 
◊
F
F
→
A 
 
S4
  
 
 
[Step 2, Step 3 & PL] 
TS4 4: 
◊◊
◊
F
F
→
A

S4
 
TS4 4 follows by applying the duality principle on TS4 3. 
 
TS4 5: 
◊◊
◊
F
F
↔
A  

S4
 
Step 1: 
◊◊
◊
F
F
→
A

S4
   
 
[TS4 4] 
Step 2: 
◊◊
◊
F
F
→
A  

S4
  
 
[TK 1] 
Step 3: 
◊
◊◊
F
F
→
A 
 
S4
  
 
[TS4 3 & RS] 
Step 4: 
◊◊
◊
F
F
↔
A  

S4
  
 
[Step 2, Step 3 & PL] 
 
TS4 6: 
◊◊
◊
F
F
↔
A
 

S4
 
TS4 6 follows by applying the duality principle on TS4 5. 
 
8.3.6 
Theorems of B 
 
TB 1: 
◊F
F
→
A

B
 
Step 1: 
◊
F
F
¬
→
¬
A

B
  
 
[Axiom B & RS] 
Step 2: 
◊F
F
¬
¬
→
A

B
  
 
[PL] 
Step 3: 
◊F
F
→
A

B
  
 
[Definition of   and ◊] 
 

 
Modal Logics 
271 
 
TB 2: If 
◊F
G
→
AB
 then 
F
G
→
A

B
 
Step 1: 
◊F
G
→
AB
 
Step 2: 
◊F
G
→
A 

B
  
 
[TK 1] 
Step 3: 
◊
F
F
→
A

B
  
 
[Axiom B] 
Step 4: 
F
G
→
A

B
  
 
[Step 2, Step 3, & PL] 
 
8.3.7 
Theorems of S5 
 
TS5 1: 
◊F
F
→
A


S5
 
Step 1: 
◊
◊
F
F
¬
→
¬
A

S5
   
[Axiom E and RS] 
Step 2: 
◊
◊
F
F
¬
¬
→¬ ¬
A

S5
  
[PL] 
Step 3: 
◊F
F
→
A


S5
  
 
[Definition of   and ◊] 
 
TS5 2: 
◊
F
F
→
A

S5
 
Step 1: 
◊
◊
F
F
→
A

S5
 
 
[Axiom E] 
Step 2: 
◊
F
F
→
AS5
  
 
[TT 1] 
Step 3: 
◊
F
F
→
A

S5
  
 
[Step 1, Step 2, & PL] 
 
TS5 3: 
F
F
→
A 

S5
 
Step 1: 
◊F
F
→
A  

S5
   
[TS5 1 & TK 1] 
Step 2: 
◊
F
F
→
A 
 
S5
 
 
[TS5 2 & RS] 
Step 3: 
F
F
→
A 

S5
  
 
[Step 1, Step 2, & PL] 
 

272 
Decision-Making Agents 
 
 
8.3.8 
Theorems of S5’ 
 
TS5' 1: 
◊
◊
F
F
→
A

S5'
 
Step 1: 
◊◊
◊
F
F
→
AS5'
  
 
[TS4 2] 
Step 2: 
◊◊
◊
F
F
→
A 

S5'
   
[TK 1] 
Step 3: 
◊
◊◊
F
F
→
A

S5'
   
[B & RS] 
Step 4: 
◊
◊
F
F
→
A

S5'
  
 
[Step 2, Step 3, & PL] 
 
8.4 
Modality 
A modality is a finite sequence of symbols ◊,  , and ¬. The null sequence 
represents the null modality, denoted as ϕ . Examples of modalities are ◊,  , 
◊
¬

 and, 
◊
¬ ¬
¬
 
. If a modality contains an even or zero number of 
occurrences of  ¬ then it is called affirmative; otherwise, the modality is 
negative. Due to the definition of   as 
◊
¬ ¬, each affirmative modality can be 
reduced to a modality without the negation symbol. Also, each negative modality 
can be reduced to a modality of the form M¬, where M does not contain any 
negation symbol. We define the following four kinds of modalities:  
• 
Type A: Affirmative A-modalities of the form M beginning with   
• 
Type B: Affirmative B-modalities of the form M beginning with ◊ 
• 
Type C: Negative A-modalities of the form M¬ beginning with   
• 
Type D: Negative B-modalities of the form M¬ beginning with ◊ 
The degree of a modality is the number of times the symbol ◊ or   occurs. For 
example, the degree of  
◊
¬ ¬
¬
 
 is 3 and that of P or 
P
¬
 is 0. A proper 
modality is a modality of degree higher than zero. 
Example 
Results TS4 1 and TS4 2 assert that in every modality in S4 two of ◊ or   in 
immediate succession can be replaced respectively by one ◊ or  . Based on 
these results, there are only 3 Type A modalities in S4:   (degree 1), 
◊

 

 
Modal Logics 
273 
 
(degree 2), 
◊
   (degree 3). An affirmative A-modality of the form 
◊◊
 
 
reduces to 
◊

 by TS4 5. Similarly, there are only three Type B modalities in S4, 
namely ◊, ◊ , and ◊◊

, that are duals of three Type A modalities. The set of 
12 irreducible proper modalities of S4 are listed below: 
Type A Modality of S4:  , 
◊

, 
◊
   
Type B Modality of S4: ◊, ◊ , ◊◊

 
Type C Modality of S4: 
¬

, 
◊¬

, 
◊¬
 
 
Type D Modality of S4: ◊¬, ◊¬

, ◊◊¬

 
The following implications hold between modalities in S4: 
◊
◊
◊◊
◊
◊
◊
◊
◊◊
◊
◊
◊
◊
P
P
P
P
P
P
P
P
P
P
P
P
P
P
P
P
P
P
⎧
⎫
→
→
→
→
⎨
⎬
⎩
⎭
¬
⎧
⎫
¬
→
¬
→
→
¬
→
¬
⎨
⎬
¬
⎩
⎭
→
→
¬
→¬
→
¬


 




 




 
The 14 modalities of S4, 12 irreducible ones plus ϕ  and ¬ cannot be further 
reduced.  
 
8.5 
Decidability and Matrix Method 
A matrix method can be used to decide whether a given formula in a modal 
system is valid or not. This method is a generalization of the truth-table method 
in propositional calculus. The method defines a matrix M as a quintuple 
,
, , ,
〈
−∗⊃〉
K D
, where K is a set of possible values and D is a non-empty proper 
subset of K, called the designated values, − and ∗ are unary functions defined 
over K, and ⊃ is a binary function defined over K. Intuitively, the symbols −, 
∗, and ⊃ correspond to negation, possibility, and implication of a modal system, 
respectively. The triplet 
, ,
〈
−⊃〉
K
 is a Boolean algebra defined as follows: 
BL1: K contains at least two elements, 
 
 
 
with two distinguished elements 0 and 1 
BL2: If ,a b∈K  then 
,
,
,
a a
b a
b a
b
−
⊃
×
+ ∈K , 
 
 
 
where 
(
)
a
b
a
b
×
= −
⊃−
, a
b
a
b
+
= −⊃
 

274 
Decision-Making Agents 
 
 
BL3: 
(
)
a
b
c
a
b
a
c
×
+
=
×
+
× , for all , ,
a b c∈K  
BL4: 
0
a
a
+
=
 and 
1
a
a
× =
,  for all a∈K  
BL5: 
1
a
a
+ −=  and 
0
a
a
× −=
,  for all a∈K  
Intuitively, the symbols ×  and +  correspond to conjunction and disjunction of a 
logical system, respectively. The relation between 
, ,
〈
−⊃〉
K
 and the 
propositional logic can be described as follows. 
We first define a valuation or assignment V of a propositional formula F as 
some member V(F)  of K. V assigns each variable P in F some member of V(P) 
of K and then evaluates V(F)  satisfying the following properties: 
V1: If 
( )
V F
a
=
then 
(
)
V
F
a
¬
= − 
V2: If 
( )
V F
a
=
 and 
( )
V G
b
=
 then 
(
)
V F
G
a
b
→
=
⊃
 
The algebra 
, ,
〈
−⊃〉
K
 validates a formula F if and only if for every valuation V 
to the variables in F, 
( )
1
V F = . It can be proven that a propositional formula is 
valid if and only if every Boolean algebra validates it. We shall present a similar 
result for modal systems by developing extended Boolean algebras. 
We add to Boolean algebra 
, ,
〈
−⊃〉
K
 the unary operator ∗ corresponding to 
the modal notion of possibility, and then define the K algebra or matrix 
,
, , ,
〈
−∗⊃〉
K D
 with the following additional properties: 
BL6: If a∈K  then 
,#
a
a
∗
∈K , where #a
a
= −∗− 
BL7: If a∈D  then #a∈D  
BL8: If ,a b∈D  then if #(
)
a
b
⊃
∈D  then #
#
a
b
⊃
∈D  
Intuitively, the symbol # corresponds to the modal notion of necessity. The 
property BL7 above is due to the rule of necessitation of the modal system K and 
the property BL8 corresponding to the axiom K of K. In the case of propositional 
logic, K is {0, 1} and D is {1}. 
As for the valuation of the formula in a given modal system, we first choose 
the sets K and D, and define operator matrices for each primitive operator. The 
valuation V is extended for modal formulae by including the following: 
V3: If 
( )
V F
a
=
then 
(
)
◊
V
F
a
= ∗ 
These operator matrices are then used to compute the value of any formulae of 
the system. If the value of a formula is not a member of the designated set, then it 
is considered an invalid formula. The set of matrices for the operators have the 
following properties: 

 
Modal Logics 
275 
 
• 
Every axiom of the system takes one of the designated values for every 
assignment 
of 
values 
to 
the 
variables. 
For 
example, 
if 
(
)
(
)
F
G
F
G
→
→
→



 is an axiom of the modal system under 
consideration, then for every assignment of values from K to F and G, 
the value of the axiom should be a member from D. 
• 
Every inference rule of the system can be verified for every assignment 
of values to the variables. For example, if  “If 
F
A
 and 
F
G
→
A
 then 
G
A
” is an inference rule of the underlying modal system, then for every 
assignment of values from K to F and G for which the values of F and 
F
G
→
 are members of the designated set D, the value of G should also 
be a member of D. 
The above two conditions together ensure that every formula derived from the 
axioms and inference rules will also take one of the designated values for any 
assignment of values to the variables. 
Validity of a formula can be guaranteed by applying McKinsey’s theorem, 
which states that a formula F containing n sub-sentences is valid in the modal 
system K if and only if it is verified by every K algebra containing at most 
2
2
n elements. By a sub-sentence G of a modal formula F we mean a well-formed 
part of F written in its primitive notations ¬, → and ◊. For example, the 
formula 
(
)
◊
◊
P
Q
P
→¬
→
 has the following seven well-formed parts: 
,
,
,
,
,
◊
◊
◊
◊
◊
P Q
P
Q
Q
Q
P
¬
¬
→
, and 
(
)
◊
◊
P
Q
P
→¬
→
. To verify the validity 
of this formula, we need to consider every K algebra containing not more than
7
2
2  
elements, which is simply not practical. But the procedure is useful to check non-
validity of formulae in the context of a modal system by constructing simple, 
small matrices as counter examples. 
Example 
Consider a matrix M = 
,
, , ,
〈
−∗⊃〉
K D
, where 
{0,1,2,3}
=
K
 and 
{1}
=
D
. The 
operators are defined via tables shown in Figure 8-2. It can be easily verified that 
M satisfies BL1-BL8. Therefore, M is a K algebra. 

276 
Decision-Making Agents 
 
 
,
, , ,
{0,1,2,3}
{1}
= 〈
−∗⊃〉
=
=
K D
K
D
M
0
1
2
3
0
0
1
2
3
1
1
1
1
1
2
2
1
2
1
3
3
1
1
3
a
b
+
0
1
2
3
0
1
1
1
1
1
0
1
2
3
2
3
1
1
3
3
2
1
2
1
a
b
⊃
#
0
1
0
3
1
0
2
1
2
3
3
3
3
2
2
2
a
a
a
a
−
∗
0
1
2
3
0
0
0
0
0
1
0
1
2
3
2
0
2
2
0
3
0
3
0
3
a
b
×
 
Figure 8-2: Matrices defining K algebra 
Applying McKinsey’s result as stated above, it can be verified easily that the 
formula 
◊
P
P
→

 is not valid in K. Since 
◊
P
P
→

 has 4 well-formed parts, 
if it is valid in K, then the formula should be verified by every K algebra 
containing at most 
4
2
2 elements. In particular, the formula should be verified by 
the above example algebra with only 4 elements. But considering 1 as an 
assignment for P, the valuation of the formula is #1
1
⊃∗, that is, 2, which is not 
a member of the designated set {1}. 
 
We shall rely on this algebraic approach to establish relationships among 
various modal systems. But first we introduce a few more algebras corresponding 
to various modal systems. 
BL9:   If a∈K  then if #a∈D  then a
∗∈D  
BL10: If a∈K  then if #a∈D  then a∈D  
BL11: If a∈K  then if #a∈D  then ##a∈D  
BL12: If a∈K  then if a
∗∈D  then # a
∗∈D  
BL13: If a∈K  then if a∈D  then # a
∗∈D  
A D algebra is an algebra satisfying BL1-BL9, and a T algebra is the one 
which also satisfies BL10. An S4 algebra is an algebra satisfying BL1-BL11, an 

 
Modal Logics 
277 
 
S5 algebra is an algebra satisfying BL1-BL10 and BL12, and finally, a B algebra 
is an algebra satisfying BL1-BL10 and BL13. 
8.6 
Relationships among Modal Systems 
The system D is obtained from K by adding the axiom D into K.  The system 
T is obtained by adding T into K. The systems that are obtained from T  by adding 
separately three axioms 4, E, and B are S4, S5, and B, respectively. The system 
S5’ is obtained from T by adding axioms 4 and E. The results TS5 2 and TS5 3 
prove that S5’ is a subsystem of S5 and the result TS5' 1 proves that S5 is a 
subsystem of S5’. Therefore, the two systems S5 and S5’ are equivalent. We have 
the following relationship: 
⎧
⎫
⊆
⊆
⊆
⊆
⎨
⎬
⎩
⎭
S4
K
D
T
S5
B
 
where ⊆ represents the subsystem relationship, that is, 
⊆
X
Y  means every 
theorem of X is also a theorem of Y. Note that a logical system is defined as the 
set of all theorems that are deducible using its axioms and inference rules. Next, 
we prove by constructing examples that the subsystem relationships among the 
modal systems K, D, T, S4, B, and S5 as presented above are  “proper” or ⊂ in 
the sense that there is a theorem of D which is not a theorem of K, and there is a 
theorem of T which is not a theorem of D, and so on. To prove 
⊆
K
D , we will 
construct, for example, a K algebra and show that the algebra does not satisfy 
axiom D. Then McKinsey’s result proves that D is not a theorem of K. 
Consider a matrix M = 
,
, , ,
〈
−∗⊃〉
K D
, where 
{0,1,2,3}
=
K
 and 
{1}
=
D
. A 
set of * operators are defined via tables shown in Figure 8-3, of which (a) is the K 
algebra shown in Figure 8-2.  
• 
⊂
K
D : Consider the * operator as defined in Figure 8-3(a). 
,
, , ,
〈
−∗⊃〉
K D
 is a K algebra. Assigning P to 1, the valuation of the 
axiom 
◊
P
P
→

 of D is 2, which is not a member of the designated set 
{1}. Therefore, 
◊
P
P
→

 is not a theorem of K. 
• 
⊂
D
T : Consider the * operator as defined in Figure 8-3(b). For every 
assignment of P, the valuation of 
◊
P
P
→

always takes the value 1. 
Therefore, 
,
, , ,
〈
−∗⊃〉
K D
 is a D algebra. But, assigning P to 2, the 
valuation of the axiom 
P
P
→

 of T  is 2, which is not a member of the 
designated set {1}. Therefore, 
P
P
→

 is not a theorem of D. 

278 
Decision-Making Agents 
 
 
#
0
0
0
1
1
1
2
1
3
3
2
0
a
a
a
∗
#
0
0
3
1
2
1
2
3
3
3
2
2
a
a
a
∗
⊂
K
D
⊂
D
T
#
0
3
0
1
1
2
2
1
0
3
1
0
a
a
a
∗
#
0
0
0
1
1
1
2
2
0
3
1
3
a
a
a
∗
,
⊂
T
S4 B
≠
S4
B
0
1
1
0
2
3
3
2
a
a
−
0
1
2
3
0
1
1
1
1
1
0
1
2
3
2
3
1
1
3
3
2
1
2
1
a
b
⊃
#
0
0
0
1
1
1
2
2
0
3
1
3
a
a
a
∗
⊂
S4
S5
#
0
0
0
1
1
1
2
2
0
3
1
3
a
a
a
∗
⊂
B
S5
(a) K Algebra
(b) D Algebra
(c) T Algebra
(d) S4 Algebra
(e) S4 Algebra
(f) B Algebra
 
Figure 8-3: Example algebras for establishing relationships among modal 
systems 
• 
,
⊂
T
S4  B : Consider the * operator as defined in Figure 8-3(c). For every 
assignment of P, the valuation of 
P
P
→

always has the value 1. 
Therefore, 
,
, , ,
〈
−∗⊃〉
K D
 is a T algebra. But, assigning P to 1, the 
valuation of the axiom 
P
P
→


 of S4 is 3, which is not a member of 
the designated set {1}. Therefore, 
P
P
→


 is not a theorem of T. 
Similarly, assigning P to 1, the valuation of the axiom 
◊
P
P
→
 of B is 
2, which is not a member of the designated set {1}. Therefore, 
◊
P
P
→
 is not a theorem of T. 
• 
≠
S4
B : Left as an exercise 
• 
⊂
S4
S5 : Left as an exercise 
• 
⊂
B
S5 : Left as an exercise 
Figure 8-4 shows the relationship among the modal systems K, D, T, S4, B, 
and S5 introduced so far.  

 
Modal Logics 
279 
 
K
D
T
S4
B
S5
{PC, RN, K}
K ∪{D}
D ∪{T}
T ∪{B}
T ∪{4}
S4 ∪{B}
T ∪{E}
B ∪{4}
D
T
4
B
E
Κ :
(
)
(
)
D :
T :
4:
E :
B:
F
G
F
G
F
F
F
F
F
F
F
F
F
F
→
→
→
→
→
→
→
→
◊
◊
◊
◊









B
4
 
Figure 8-4: Relationships among modal systems 
In Figure 8-4, an arrow → from X  to Y  with label A means that the system 
Y  can be obtained from the system X  by adding axiom A, and that every 
theorem of X  is also a theorem of X  but not vice-versa. See (Chellas, 1980) for 
more relationships that hold among the subsets of the six axioms K, D, T, 4, E, 
and B. For example, KTE = KTB4 = KDB4 = KDBE, where KTE means the modal 
system containing axioms K, T, and E, and so on. 
8.7 
Possible World Semantics 
For the purpose of reasoning with agent intelligence, an agent’s knowledge is 
represented by a finite set of modal sentences from an underlying 
epistemological theory. These sentences constitute the proper axioms of the 
theory. The model theoretic semantics of such modal sentences are given in 
terms of possible worlds. 
By a real world we do not mean the planet earth on which we live, but all 
those items existing in the universe, events that have occurred, and propositions 
and relations that characterize the items and events. An item is either a physical 
object such as a Pyramid, tree, Empire State building, moon, or tiger; an abstract 
object such as the set of prime numbers or classical logic; an individual such as 
Lincoln, Gandhi, Evita, or Brando. Example events are the assassination of John 

280 
Decision-Making Agents 
 
 
Lennon or the Gulf War. Properties such as being supersonic or being tall 
characterize items (for example, Concord is supersonic and the Eiffel Tower is 
tall) Relations, such as being a father to, being a colder place, or being located, 
characterize items (for example, London is colder than Cairo). 
Now we begin defining the term “possible world” by including the real or 
current world as one of the possible worlds based on the fact that if something 
actually exists then it is possible that it exists. Every possible world, which is not 
the real world, will possess one or more of the following properties: 
• 
It has more objects, events, and individuals than the real one, such as a 
cancer cure, a bridge between the earth and the moon, a third world war, 
a ten feet tall person, a Mars expedition, or Superman. 
• 
It has less objects, events, and individuals than the real one, such as no 
Pyramids, no Gulf war, and no Shakespeare. 
• 
It differs with respect to properties, for example, the Sydney opera house 
is red, and I am a multi-millionaire. 
• 
It differs with respect to relations, for example, the Great Wall of China 
is located in the Middle East. 
The set of possible worlds is defined depending on the circumstances or 
applications at hand. For example, when logical reasoning occurs about what can 
be done with the current technology, the set of all possible worlds is the set of all 
technologically possible worlds. In this case, the worlds other than the 
technologically possible worlds will be considered impossible. Similarly, when 
logical reasoning occurs in the domain of physics, the set of all possible worlds is 
the set of all physically possible worlds.  The set of all worlds in each of which 
only valid mathematical theories that exist today are used is the set of all 
mathematically possible worlds. Therefore, the set of all mathematically possible 
worlds contains the set of all physically possible worlds. A world where it is 
assumed that the 3-satisfiability problem is P-complete is mathematically 
possible. A world in which straightforward inconsistency (P and not P) can be 
detected is considered inconsistent and impossible. Therefore, irrespective of the 
circumstance or application, the set of all impossible worlds contains the set of 
all inconsistent worlds. The relations between various types of possible worlds as 
discussed here are shown in Figure 8-5.  

 
Modal Logics 
281 
 
Mathematically
Impossible World
Mathematically
Possible Worlds
Physically
Possible Worlds
Technologically
Possible Worlds
Inconsistent 
Worlds
Real World
Technologically
Impossible World
Physically
Impossible World
 
Figure 8-5: Categorization of worlds 
A normal model M is a structure of the following form:  
, ,
R B
= 〈
〉
W
M
 
where W is a non-empty set of elements, to be called worlds; R is any binary 
relation on W, to be called the accessibility relation for W; and B is a valuation 
function that determines the truth value of a proposition in a world as follows: 
:
{
,
}
B
×
→
⊥
P
W
F
 
Thus if 
( , )
B P w  is F or ⊥ then we say that the proposition P is true or false, 
respectively, at the world w in a model M, and this is written as follows: 
w P
BM
 
Equivalently, B can be defined as a function to assign a set of propositions to 
each world w that are true in w: 
:
( )
B
→
W
P
P
 

282 
Decision-Making Agents 
 
 
A third equivalent alternative, which we have adopted here, defines B as a 
function which determines for each proposition P a set B(P) of worlds in each of 
which P is true: 
:
(
)
B
→
P
W
P
 
For an arbitrary formula F, the satisfiability of a closed formula F with 
respect to a model 
, ,
R B
= 〈
〉
W
M
 and a world w∈W , written as 
w F
BM
, is 
recursively defined as follows: 
w P
BM
 iff 
( )
w
B P
∈
  
w
F
¬
BM
 iff 
w F
HM
 
w F
G
→
BM
 iff 
w F
BM
 implies 
w G
BM
 
w
F
B 
M
 iff for every 
'
w ∈W  such that 
'
wRw , 
'
w F
BM
 
Proposition 8-1: 
◊
w
F
BM
 if and only if there exists 
'
w  such that wRw'  and 
w' F
BM
. 
Proof: 
◊
w
F
BM
 
iff 
w
F
¬ ¬
B

M
, by the definition of ◊ 
iff 
w
F
¬
H 
M
, by the definition of  satisfiability 
iff there is a w' ∈W  such that wRw'  and 
w
F
¬
HM
, 
by the definition of  satisfiability 
iff there is a w' ∈W  such that wRw'  and 
w
F
¬¬
BM
, 
by the definition of  satisfiability 
iff there is a w' ∈W  such that wRw'  and 
w F
BM
.  
 
A formula F is satisfiable iff 
w F
BM
, for some model 
, ,
R B
= 〈
〉
W
M
 and 
w∈W . F is valid in M , written as
F
BM
, iff 
w F
BM
, for every w∈W . F is valid 
in a class of models Σ , written as 
F
Σ
B
, iff 
F
BM
, for every 
∈Σ
M
. F is valid, 
written as 
F
B
, iff F is valid in every class of models. 
We now prepare the development of the soundness and completeness results. 
The notion of connected models is pertinent in this context. Connected models 

 
Modal Logics 
283 
 
are defined in terms of ancestral or transitive closure relationships among 
possible worlds. Suppose 
1
ρ  and 
2
ρ  are two binary relations on a set X. The 
relation 
1
ρ  is called the ancestral of the relation 
2
ρ  if the following condition is 
satisfied: 
1
x
y
ρ
 
iff 
for 
all 
' ⊆
X
X  
such 
that 
if 
'
x∈X  
and 
2
{ :
'( '
'& '
)}
z
z z
z
z
ρ
∃
∈
⊆
X
X  then 
'
y∈X . Alternatively, the transitive closure or 
ancestral ρ∗ of a binary relation ρ  on a set X is recursively defined as follows: 
• 
x
y
ρ ∗
 if x y
ρ
 
• 
x
y
ρ ∗
 if z
∃ such that x z
ρ  and z
y
ρ ∗ 
If x
y
ρ ∗
 then we call y a descendant of x. A model 
, ,
R B
= 〈
〉
W
M
 is connected if 
and only if there exists w∈W  such that W = { }
{ ':
'}
w
w
wRw
∪
. Then a model is 
connected if we can find a world w such that W is the collection of w together 
with all of its descendants. 
Consider model 
, ,
R B
= 〈
〉
W
M
 and suppose w∈W . Then the connected 
model 
,
,
w
w
w
w
R
B
= 〈
〉
W
M
 of M generated by M is defined as follows:  
w
W  = 
*
{ }
{ ':
'}
w
w
wR w
∪
 
w
R  = {
,
' :
' &
'
}
w
w w
wRw
w
〈
〉
∈W
 
( )
w
B
P  = 
( )
w
B P ∩W  
Proposition 8-2: 
,
,
w
w
w
w
R
B
= 〈
〉
W
M
 is connected. 
Proof: Follows from the definition. 
 
We shall now show that only those worlds that are descendants to w are 
relevant in determining the truth value of a formula F wrt to a world w. 
Theorem 8-1: Suppose 
, ,
R B
= 〈
〉
W
M
 is a model and w∈W . Then 
w F
BM
 iff 
w
w
F
BM
. 
Proof: We prove the theorem by induction on the number of connectives of F.  

284 
Decision-Making Agents 
 
 
Case I: F
P
=
, that is, F is a proposition without any connective. 
w P
BM
 iff 
( )
w
B P
∈
, and 
w
w
P
BM
 iff 
( )
w
w
B
P
∈
 iff 
( )
w
w
B P
∈
∩W . By definition, 
w
w∈W  
and thus 
( )
w
B P
∈
 iff 
( )
w
w
B P
∈
∩W . Therefore, 
w P
BM
 iff 
w
w
P
BM
. 
Case II: F
G
= ¬
. 
w F
BM
 iff 
w G
HM
, and 
w
w
F
BM
 iff 
w
w
G
HM
. By induction 
hypothesis, 
w G
BM
 iff 
w
w
G
BM
, that is, 
w G
HM
 iff 
w
w
G
HM
. Therefore, 
w F
BM
 iff 
w
w
F
BM
. 
Case III: F
G
H
=
→
. 
w F
BM
 iff 
w G
H
→
BM
 iff 
w G
BM
 implies 
w H
BM
, and 
w
w
F
BM
 iff 
w
w
G
BM
 implies 
w
w
H
BM
. By induction hypothesis, 
w G
BM
 iff 
w
w
G
BM
 
and 
w H
BM
 iff 
w
w
H
BM
. Therefore, 
w F
BM
 iff 
w
w
F
BM
. 
Case IV: F
G
=
. 
w F
BM
 iff 
w
G
B 
M
 iff for every w' ∈W  such that wRw' , 
'
w G
BM
, and 
w
w
F
BM
 iff 
w
w
G
B

M
 iff for every w' ∈W  such that 
w
wR w' , 
'
w
w
G
BM
. 
We prove 
w F
BM
 iff 
w
w
F
BM
 by establishing the equivalence between “for every 
'
w ∈W  such that 
'
wRw , 
'
w G
BM
” and “for every 
'
w ∈W  such that 
'
w
wR w , 
'
w
w
G
BM
”. 
 Suppose, for every 
'
w ∈W  such that 
'
wRw , 
'
w G
BM
, and 
'
w
wR w , for some 
'
w ∈W . Since, 
'
w
wR w , by definition, 
'
wRw  and 
'
w
w ∈W . Therefore, 
'
w G
BM
. 
By induction hypothesis, 
'
w
w
G
BM
. 
Conversely, suppose, for every 
'
w ∈W  such that 
'
w
wR w , 
'
w
w
G
BM
, and 
'
wRw , for some 
'
w ∈W . Since w∈W  and 
'
wRw , by definition, 
'
w
w ∈W , that 
is, 
'
w
wR w . Therefore, 
'
w
w
G
BM
, and by induction hypothesis, 
'
w G
BM
. 
Thus, 
w
G
B 
M
 iff 
w
w
G
B

M
, that is, 
w F
BM
 iff 
w
w
F
BM
.  
 
Corollary 8-1: 
F
B
 iff 
F
Σ
B
, for the class Σ  of connected models. 
Proof: The forward half is straightforward. To prove the converse, suppose, 
F
H
. Then there exists a model 
, ,
R B
= 〈
〉
W
M
 and w∈W  such that 
w F
HM
. 
The above theorem states that 
w F
BM
 iff 
w
w
F
BM
.  Therefore, 
w F
HM
 iff 
w
w
F
HM
, 
which yields a contradiction as 
w
M  is connected. Therefore, we have 
F
B
 iff 
F
Σ
B
.  

 
Modal Logics 
285 
 
Theorem 8-2: If 
F
B
 then 
F
B
 
Proof: Consider a model 
, ,
R B
= 〈
〉
W
M
 and suppose 
F
BM
. Therefore, 
w F
BM
, 
for every w∈W . To prove 
w
F
B 
M
, suppose 
'
wRw , for some 
'
w . By the 
definition of 
F
B
, we have 
'
w F
BM
. Hence, 
w
F
B 
M
, and therefore, 
F
B
, since w 
and M  are chosen arbitrarily.  
Theorem 8-3: 
(
)
(
)
F
G
F
G
→
→
→
B


 
Proof: Consider a model 
, ,
R B
= 〈
〉
W
M
, and suppose 
(
)
w
F
G
→
B 
M
 and 
w
F
B 
M
, for some w∈W . To prove 
w
G
B 
M
, consider world 
'
w
W
∈
such that 
'
wRw . Then, 
'
w F
G
→
BM
 and 
'
w F
BM
, yielding 
'
w G
BM
. Therefore, 
w
G
B 
M
. 
(
)
(
)
w
F
G
F
G
→
→
→
B 


M
, and thus, 
(
)
(
)
F
G
F
G
→
→
→
B


 follows 
from the deduction theorem in PL.  
Theorem 8-4: Consider a model 
, ,
R B
= 〈
〉
W
M
 and a world w∈W . Then the 
following properties hold: 
If R is serial, 
◊
w
F
F
→
B 
M
 
If R is reflexive, 
w
F
F
→
B 
M
 
If R is reflexive and symmetric, 
◊
w F
F
→
B

M
 
If R is reflexive and transitive, 
w
F
F
→
B 

M
 
If R is reflexive and euclidean, 
◊
◊
w
F
F
→
B

M
 
Proof: 
◊
w
F
F
→
B 
M
: Suppose 
w
F
B 
M
. Then for every world 
'
w ∈W  such that 
'
wRw , 
'
w F
BM
. Since R is serial, there exists a world 
'
w ∈W  such that 
'
wRw . 
Therefore, there is a world 
'
w ∈W  such that 
'
wRw , 
'
w F
BM
, that is, 
◊
w
F
BM
. 
w
F
F
→
B 
M
: Suppose 
w
F
B 
M
. Then for every world 
'
w ∈W  such that 
'
wRw , 
'
w F
BM
. Since R is reflexive, wRw, and thus 
w F
BM
. 
◊
w F
F
→
B

M
: Suppose 
w F
BM
, and, if possible, assume 
◊
w
F
H 
M
. Since 
◊
w
F
H 
M
, there is a world 
'
w ∈W  such that 
'
wRw  and 
' ◊
w
F
HM
. Since 
' ◊
w
F
HM
, 
there is no world 
"
w ∈W  such that 
'
"
w Rw  and 
"
w F
BM
. But R is symmetric, and 

286 
Decision-Making Agents 
 
 
'
wRw  means 
'
w Rw. Moreover, 
w F
BM
. Thus, there is a world 
"
w
w
=
 such that 
'
"
w Rw  and 
"
w F
BM
, yielding a contradiction. Therefore, 
◊
w
F
B 
M
. 
w
F
F
→
B 

M
: Suppose 
w
F
B 
M
, and, if possible, assume 
w
F
H 
M
. Since, 
w
F
H 
M
, there is a world 
'
w ∈W  such that 
'
wRw  and 
'
w
F
H 
M
. Since 
'
w
F
H 
M
, 
there is a world 
"
w ∈W  such that 
'
"
w Rw  and 
"
w F
HM
. But R is transitive, and 
'
wRw  and 
'
"
w Rw  mean 
"
wRw . Therefore, 
w
F
B 
M
 means 
"
w F
BM
, yielding a 
contradiction. Therefore, 
w
F
B 
M
. 
◊
◊
w
F
F
→
B

M
: Suppose 
◊
w
F
BM
, and, if possible, assume 
◊
w
F
H 
M
. Since 
◊
w
F
BM
, there is a world 
'
w ∈W such that 
'
wRw  and 
'
w F
BM
. Since 
◊
w
F
H 
M
, 
there is a world 
"
w ∈W  such that 
"
wRw  and 
" ◊
w
F
HM
. Since, 
" ◊
w
F
HM
, there is 
no world in relation to 
"
w  where F is true. But R is euclidean, and 
"
wRw  and 
'
wRw  mean 
"
'
w Rw . Therefore, there is a world 
'
w in W such that 
"
'
w Rw  and 
'
w F
BM
, yielding a contradiction. Therefore, 
◊
w
F
B 
M
. 
 
8.8 
Soundness and Completeness Results 
Theorem 8-5 and Theorem 8-6 below provide the soundness results for the 
systems K, D, T, B, S4, and S5. 
Theorem 8-5: If 
F
AK
 then 
F
B
 
Proof: The MP rule preserves the truth, that is, if 
F
B
 and 
F
G
→
B
 then 
G
B
. 
Taking this and Theorem 8-2 and Theorem 8-3 into consideration, the rest of the 
proof can be established easily by applying induction on the number of 
connectives in a formula.  
Theorem 8-6: Consider model 
, ,
R B
= 〈
〉
W
M
. Then the following properties 
hold: 
If 
F
AD
 then 
F
BM
, if R is serial 
If 
F
AT
 then 
F
BM
, if R is reflexive 
If 
F
AB
 then 
F
BM
, if R is reflexive and symmetric 

 
Modal Logics 
287 
 
If 
F
AS4
 then 
F
BM
, if R is reflexive and transitive 
If 
F
AS5
 then 
F
BM
, if R is reflexive and euclidean 
Proof: Follows from Theorem 8-4.  
 
To prove the completeness for S, where S is one of K, D, T, B, S4, and S5, we 
consider the following canonical model
,
,
R
B
= 〈
〉
W
S
S
S
S
M
 of S: 
• 
WS  is the set of all maximal consistent extensions of S 
• 
'
wR w
S
 iff {
:
}
'
F
F
w
w
∈
⊆

 
• 
( )
{ :
}
B
P
w P
w
=
∈
S
 
Proposition 8-3: 
'
wR w
S
 iff {
:
'}
◊F F
w
w
∈
⊆
 
Proof: Suppose, 
'
wR w
S
 and 
'
F
w
∈
, and, if possible, let ◊F
w
∉
. Since w is 
maximal consistent, and therefore complete, 
◊F
w
¬
∈
, that is, 
F
w
¬
∈

. Then, 
by the definition of 
'
wR w
S
, 
'
F
w
¬
∈
. Therefore, both F and 
F
¬
 are in 
'
w , 
making 
'
w  inconsistent, which is a contradiction. Therefore, ◊F
w
∈
. 
Conversely, 
suppose 
{
:
'}
◊F F
w
w
∈
⊆
, 
and, 
if 
possible, 
let 
{
:
}
'
F
F
w
w
∈

⊊
. This means that there is F for which 
F
w
∈

 but 
'
F
w
∉
. 
Since 
'
F
w
∉
and 
'
w  is maximal consistent, 
'
F
w
¬
∈
. Thus, ◊F
w
¬
∈
, that is, 
F
w
¬
∈

. This contradicts with 
F
w
∈

 as w is consistent. Therefore, 
{
:
}
'
F
F
w
w
∈
⊆

, that is, 
'
wR w
S
. 
Theorem 8-7: In a canonical model
,
,
R
B
= 〈
〉
W
S
S
S
S
M
 of a system S, 
F
AS
 iff 
F
B
S
M
. 
Proof: 
F
AS
 iff, for all w∈WS , F
w
∈
. Thus, to prove the theorem we only 
need to prove that 
w
F
B
S
M
 iff F
w
∈
, for every w∈WS . We apply the induction 
principle on the number of connectives in F. 
 
Suppose, 
w
F
B
S
M
. 

288 
Decision-Making Agents 
 
 
Case I: F is P. Then 
w
P
B
S
M
 implies that
( )
w
B
P
∈
S
, that is, P
w
∈
. 
Case II: F is 
G
¬
. Then 
w
G
¬
B
S
M
 implies that 
w
G
H
S
M
, that is, G
w
∉
, by 
induction hypothesis. Since w is maximal consistent, 
G
w
¬
∈
. 
Case III: F is G
H
→
. Then  
w
G
H
→
B
S
M
 implies that if 
w
G
B
S
M
 then 
w
H
B
S
M
. Therefore, by induction hypothesis, if G
w
∈
 then H
w
∈
. Since w is 
maximal consistent, G
H
w
→
∈
. 
Case IV: F is 
G

. Then 
w
G
B

S
M
 implies that for all 
'
w ∈WS such that 
'
wR w
S
, 
'
w
G
B
S
M
. By induction hypothesis, 
w
F
B
S
M
 implies that for all 
'
w ∈WS such that 
'
wR w
S
, 
'
G
w
∈
. Now, 
'
wR w
S
iff {
:
}
'
H
H
w
w
∈
⊆

. Thus, G 
belongs to every maximal consistent extension of {
:
}
H
H
w
∈

 within the 
system S. Therefore, {
:
}
H
H
w
G
∈

AS
, by applying the maximal consistency 
property. 
Thus, 
there 
exist 
1,...,
n
H
H  
in 
{
:
}
H
H
w
∈

 
such 
that 
1,...,
n
H
H
w
∈


 and 
1
{
,...,
}
n
H
H
G
AS
. Therefore, 
1
...
n
H
H
G
∧
∧
→
AS
, that is, 
1
...
n
H
H
G
∧
∧
→
A 


S
, by a generalization of TK 4. Since 
1,...,
n
H
H
w
∈


 
and w is maximal consistent, 
G
w
∈

. 
 
Conversely, suppose, F
w
∈
. 
Case I: F is P. Then P
w
∈
 implies that
( )
w
B
P
∈
S
, that is, 
w
P
B
S
M
 
Case II: F is 
G
¬
. Then 
G
w
¬
∈
 implies that G
w
∉
, by the maximal 
consistency of w. By induction hypothesis, 
w
G
H
S
M
, that is, 
w
G
¬
B
S
M
, that is, 
w
F
B
S
M
. 
Case III: F is G
H
→
. Then G
H
w
→
∈
 implies that if G
w
∈
 then H
w
∈
, 
by the maximal consistency of w. By induction hypothesis, if 
w
G
B
S
M
 then 
w
H
B
S
M
, that is, 
w
G
H
→
B
S
M
, that is, 
w
F
B
S
M
. 
Case IV: F is 
G

. Then  
G
w
∈

. Suppose, 
'
wR w
S
, for some
'
w ∈WS . By 
definition of canonical model, {
:
}
'
H
H
w
w
∈
⊆

. Therefore, 
'
G
w
∈
, that is, 
'
w
G
B
S
M
. Thus, 
w
G
B

S
M
, that is, 
w
F
B
S
M
. 

 
Modal Logics 
289 
 
Theorem 8-8: In a canonical model
,
,
R
B
= 〈
〉
W
S
S
S
S
M
 of a system S, where S 
ranges over K, D, T, B, S4, or S5, the following holds: 
RD  is serial 
RT  is reflexive 
RB  is reflexive and symmetric 
RS4  is reflexive and transitive 
RS5  is reflexive and euclidean 
Proof: 
,
,
R
B
= 〈
〉
W
D
D
D
D
M
: We have to prove the existence of 
'
w ∈WD  such that 
'
wR w
D
. Now, 
'
wR w
D
 iff {
:
}
'
F
F
w
w
∈
⊆

. Consider the set 
{
:
}
F
F
w
Γ =
∈

. 
If Γ  is not D-consistent, there exists 
1,...,
n
F
F ∈Γ  such that each 
1,...,
n
F
F
w
∈


 
and 
F
¬
AD
, where 
1
...
n
F
F
F
=
∧
∧
. By RN, 
F
¬
A 
D
, that is, 
◊F
¬
AD
. Since w 
is a maximal consistent extension of D, 
◊F
w
¬
∈
. By a generalization of TK 1, 
1
...
n
F
F
F
=
∧
∧



. Since each 
iF
w
∈

 and w is maximally consistent, 
F
w
∈

. Since w is a maximal consistent set containing D, 
◊
F
F
w
→
∈

. 
Therefore, ◊F
w
∈
, which violates the consistency of w. Therefore, Γ  is 
consistent and there will be a maximal consistent extension 
'
w  of Γ  such that 
'
w
Γ ⊆
, that is, {
:
}
'
F
F
w
w
∈
⊆

, that is, 
'
wR w
D
. 
,
,
R
B
= 〈
〉
W
T
T
T
T
M
: We have to prove that for every w∈WT , wR w
T
, that is, 
{
:
}
F
F
w
w
∈
⊆

. Since T (and therefore w as well) contains 
F
F
→

, F
w
∈
 
for each 
F

 in w. Thus, {
:
}
F
F
w
w
∈
⊆

 and therefore, wR w
T
. 
,
,
R
B
= 〈
〉
W
B
B
B
B
M
: We have to prove that for every 
,
'
w w ∈WB , if 
'
wR w
B
 
then 
'
w R w
B
. Suppose, 
'
wR w
B
, that is, {
:
}
'
F
F
w
w
∈
⊆

. To prove 
'
w R w
B
, we 
have to show that {
:
'}
F
F
w
w
∈
⊆

. Consider the set 
{
:
'}
F
F
w
Γ =
∈

, and, if 
possible, let 
'
F
w
∈

 and F ∈Γ  but F
w
∉
. Since w is maximal consistent, 
F
w
¬
∈
. Also, w contains B means ◊F
F
w
→
∈

 by TB 1, and thus 
◊F
w
¬
∈

, that is, 
F
w
¬
∈
 
. Since {
:
}
'
F
F
w
w
∈
⊆

, 
'
F
w
¬
∈

, which 
yields a contradiction. Therefore, {
:
'}
F
F
w
w
∈
⊆

, that is, 
'
w R w
B
. 
,
,
R
B
= 〈
〉
W
S4
S4
S4
S4
M
: Since S4 contains T, RS4  is reflexive. We only have to 
prove that RS4  is transitive, that is, for every 
,
',
"
w w w ∈WS4 , if 
'
wR w
S4
 and 
'
"
w R w
S4
 then 
"
wR w
S4
. Since 
'
wR w
S4
 and 
'
"
w R w
S4
, {
:
}
'
F
F
w
w
∈
⊆

 and 

290 
Decision-Making Agents 
 
 
{
:
'}
"
F
F
w
w
∈
⊆

. Consider 
{
:
}
F
F
w
Γ =
∈

. To prove, 
"
wR w
S4
, that is, 
"
w
Γ ⊆
, suppose F ∈Γ . By definition of Γ , 
F
w
∈

. Also, w contains S4 
means 
F
F
w
→
∈


, and thus 
F
w
∈

, since w is maximal consistent. 
Since 
{
:
}
'
F
F
w
w
∈
⊆

, 
'
F
w
∈

. Since 
{
:
'}
"
F
F
w
w
∈
⊆

, 
"
F
w
∈
. 
Therefore, 
"
w
Γ ⊆
, that is, {
:
}
"
F
F
w
w
∈
⊆

, that is, 
"
wR w
S4
. 
,
,
R
B
= 〈
〉
W
S5
S5
S5
S5
M
: Since S5 contains T, RS5  is reflexive. We only have to 
prove that RS5  is euclidean, that is, for every 
,
',
"
w w w ∈WB , if 
'
wR w
S5
 and 
"
wR w
S5
 then 
'
"
w R w
S5
. Since 
'
wR w
S5
 and 
"
wR w
S5
, {
:
}
'
F
F
w
w
∈
⊆

 and 
{
:
}
"
F
F
w
w
∈
⊆

. Consider 
{
:
'}
F
F
w
Γ =
∈

. To prove, 
'
"
w R w
S5
, that is, 
"
w
Γ ⊆
, suppose F ∈Γ . By definition of Γ , 
'
F
w
∈

, that is, 
'
F
w
¬ ¬
∈

, that 
is, 
'
F
w
¬
∉

, since 
'
w  is maximal consistent containing Axiom D. If 
"
F
w
∉
 
then 
{
:
}
F
F
F
w
∉
∈

, since{
:
}
"
F
F
w
w
∈
⊆

. Therefore, 
F
w
¬
∈

, since w 
is maximal consistent. By TS5 1 and maximal consistency of w containing S5, 
F
F
w
→
∈
◊

. Thus, 
◊F
w
¬
∈

, that is, 
◊F
w
¬
∈

, that is, 
'
◊F
w
¬
∈
, 
since 
{
:
}
'
F
F
w
w
∈
⊆

. Therefore, 
'
F
w
¬
∈

, which contradicts the 
consistency of 
'
w . Therefore, 
"
F
w
∈
, that is, {
:
'}
"
F
F
w
w
∈
⊆

, that is, 
'
"
w R w
S5
.  
Theorem 8-9: The following properties hold in modal systems: 
• 
F
AD
 iff 
F
BM
, for every model 
, ,
R B
= 〈
〉
W
M
 in which if R is serial 
• 
F
AT
 iff 
F
BM
, for every model 
, ,
R B
= 〈
〉
W
M
 in which if R is 
reflexive 
• 
F
AB
 iff 
F
BM
, for every model 
, ,
R B
= 〈
〉
W
M
 in which if R is 
reflexive and symmetric 
• 
F
AS4
 iff 
F
BM
, for every model 
, ,
R B
= 〈
〉
W
M
 in which if R is 
reflexive and transitive 
• 
F
AS5
 iff 
F
BM
, for every model 
, ,
R B
= 〈
〉
W
M
 in which if R is 
reflexive and euclidean 
Proof: Theorem 8-6 provides the “if” part of each of the above five properties. 
To prove the “only if” part of the first of the above five properties, suppose 
F
BM
, for every model 
, ,
R B
= 〈
〉
W
M
 in which if R is serial. In particular, 

 
Modal Logics 
291 
 
F
B
D
M
, for each canonical model 
,
,
R
B
= 〈
〉
W
D
D
D
D
M
, where RD  is serial.  From 
Theorem 8-7, 
F
AD
. The rest of the proof follows in a similar manner.  
 
Generalized versions of Theorem 8-4 and Theorem 8-8 have been proved by 
Lemmon (1977), who considered a general axiom scheme of the following form: 
◊
◊
m
n
p
q
F
F
→


 
where m, n, p, and q are non-negative integers. Note that the each of the axioms 
D, T, B, 4, and E can be obtained from the general representation by varying the 
values of m, n, p, and q. For example, m = 1, n = 0, p =1, and q = 1 give axiom E, 
which is ◊
◊
F
F
→
. In a similar manner, the following general relation scheme 
was considered: 
For all 
1
2
,
,
w w w , if 
1
m
wR w  and 
2
p
wR w  then there exists 
'
w  such 
that 
1
'
n
w R w  and 
2
'
q
w R w , where 
0
i
j
i
j
w R w
w
w
≡
=
 
Various instantiations of R can be obtained by choosing different values for m, n, 
p, and q. For example, choosing m = 0, n = 1, p = 0, and q = 0 yields the serial 
relation R as “For all 
1
w  there exists
'
w  such that 
1
'
w Rw .” A result analogous to 
Theorem 8-6 is that 
F
BM
 for the relation of M satisfying this general relation. 
8.9 
Complexity and Decidability of Modal Systems 
We say that a modal system is decidable if there is a decision procedure that 
determines whether or not a formula is a theorem or not. In this section, we shall 
prove the decidability results only for the systems K, D, T, S4, B, and S5 that we 
have been focusing on in this chapter. 
To prove whether an arbitrary formula F is a theorem of a modal system S or 
not, our approach is to construct an abstract finite model M based on F and the 
canonical model 
S
M . We can then show that if F is not a theorem of S, then the 
negation of F is satisfiable in M. Since the size of M is finite, a decision 
procedure exists to determine the satisfiability of F with respect to M. Our 
procedure is not practical, but it establishes the decidability results. Practical 
procedures based on the resolution principle for determining unsatisfiability will 
be developed later in this chapter. 
Ladner (1977) proved that the complexity of the satisfiability problem in the 
modal logics K, D, T, B, and S4 is PSPACE-complete, and in S5 is NP-complete. 

292 
Decision-Making Agents 
 
 
Halpern (1995b) also studied the effect of bounding modal depth on the 
complexity of modal logics. 
Suppose we have a formula F in a modal system S whose canonical model is 
,
,
R
B
= 〈
〉
W
S
S
S
S
M
. Define an equivalence relation ρ  on WS  as follows: 
'
w w
ρ
 iff for every subformula G of F, G
w
∈
 iff 
'
G
w
∈
 
It can be easily verified that ρ  is an equivalence relation on WS . Moreover, if n 
is the number of subformulae of F then there are 2n  possible combinations of the 
subformulae that both w and 
'
w  can contain.  Therefore, the relation ρ  
partitions WS  into at most 2n  equivalence classes (from a result presented earlier 
in the mathematical prelimary section). Each class containing a world w is 
denoted by [
]
w ρ
/
. We construct a new model 
,
,
R
B
= 〈
〉
W
S
S
S
S
M
 based on 
,
,
R
B
= 〈
〉
W
S
S
S
S
M
 as follows: 
WS  = {[
]
w ρ
/
: w∈WS } 
[
]
[ '
]
w
R
w
ρ
ρ
S
/
/
 iff for every subformula 
G

 of F, if 
G
w
∈

 then 
'
G
w
∈
 
BS  =  {[
]
w ρ
/
: G is subformula of F and G
w
∈
} 
The above definition is independent of the choice of representatives w and 
'
w  
from classes [
]
w ρ
/
 and [ '
]
w
ρ
/
, respectively. To prove this, let [
]
[ '
]
w
R
w
ρ
ρ
S
/
/
 
and 
1
w w
ρ
 and 
2
'
w
w
ρ
. We need to show that 
1
2
[
]
[
]
w
R
w
ρ
ρ
S
/
/
. Since 
[
]
[ '
]
w
R
w
ρ
ρ
S
/
/
, for every subformula 
G

 of F, if 
G
w
∈

 then 
'
G
w
∈
. 
Suppose, 
1
G
w
∈

 for a subformula 
G

 of F. Since 
1
w w
ρ
, 
1
G
w
∈

 implies 
G
w
∈

 and thus 
'
G
w
∈
. Since 
2
'
w
w
ρ
, 
'
G
w
∈
 implies that 
2
G
w
∈
. Thus, for 
every subformula 
G

 of F, if 
1
G
w
∈

 then 
2
G
w
∈
. Therefore, 
1
2
[
]
[
]
w
R
w
ρ
ρ
S
/
/
. 
Proposition 8-4: If 
'
wR w
S
 then [
]
[ '
]
w
R
w
ρ
ρ
S
/
/
 
Proof: If 
'
wR w
S
 then {
:
}
'
G
G
w
w
∈
⊆

. If 
G

 is a subformula of F and 
G
w
∈

 then 
'
G
w
∈
. Therefore, by definition of RS , [
]
[ '
]
w
R
w
ρ
ρ
S
/
/
. 

 
Modal Logics 
293 
 
Theorem 8-10: Suppose S is the modal system K. Then, for any formula F in S 
and for every subformula G of F, 
[
]
w
G
ρ
B
S
M
/
 iff 
w
G
B
S
M
. 
Proof: We prove by applying the induction principle on the length of the 
subformula G. 
Case I: G is P. Then 
[
]
w
P
ρ
B
S
M
/
 iff [
]
( )
w
B
P
ρ ∈
S
/
. Since P is a subformula of 
itself, [
]
( )
w
B
P
ρ ∈
S
/
 iff P
w
∈
. By definition, P
w
∈
 iff 
w
P
B
S
M
. Therefore, 
[
]
w
P
ρ
B
S
M
/
 iff 
w
P
B
S
M
. 
Case II: G is 
H
¬
. Since H is a subformula of F of length less than the length 
of H, by induction hypothesis, 
[
]
w
H
ρ
B
S
M
/
 iff 
w
H
B
S
M
, that is, 
[
]
w
H
ρ
H
S
M
/
 iff 
w
H
H
S
M
, 
that is, 
[
]
w
H
ρ ¬
B
S
M
/
 iff 
w
H
¬
B
S
M
, that is, 
[
]
w
G
ρ
B
S
M
/
 iff 
w
G
B
S
M
. 
Case III: G is 
1
2
H
H
→
. By induction hypothesis, 
[
]
1
w
H
ρ
B
S
M
/
 iff 
1
w
H
B
S
M
 and 
[
]
2
w
H
ρ
B
S
M
/
 iff 
2
w
H
B
S
M
. Therefore, 
[
]
1
2
w
H
H
ρ
→
B
S
M
/
 iff 
1
2
w
H
H
→
B
S
M
, that is, 
[
]
w
G
ρ
B
S
M
/
 iff 
w
G
B
S
M
. 
Case IV: G is 
H

. Suppose 
[
]
w
H
ρ
B

S
M
/
. Then, for all [ '
]
w
ρ
/
 such that 
[
]
[ '
]
w
R
w
ρ
ρ
S
/
/
, 
[
'
]
w
H
ρ
B
S
M
/
. To show that 
w
H
B

S
M
, suppose 
'
wR w
S
. Then 
[
]
[ '
]
w
R
w
ρ
ρ
S
/
/
, by Proposition 8-4, and thus 
[
'
]
w
H
ρ
B
S
M
/
. By induction 
hypothesis, 
[
'
]
w
H
ρ
B
S
M
/
 iff 
'
w
H
B
S
M
. Therefore, 
w
H
B

S
M
. 
Conversely, suppose 
w
H
B

S
M
. Then, for all 
'
w  such that 
'
wR w
S
, 
'
w
H
B
S
M
. To 
show that 
[
]
w
H
ρ
B

S
M
/
, suppose [
]
[ '
]
w
R
w
ρ
ρ
S
/
/
. Since 
w
H
B

S
M
 and w is maximal 
consistent, 
H
w
∈

. Since [
]
[ '
]
w
R
w
ρ
ρ
S
/
/
 and 
H
w
∈

, by the definition of 
RS , 
'
H
w
∈
, that is, 
'
w
H
B
S
M
, since 
'
w  is maximal consistent. By induction 
hypothesis, 
[
'
]
w
H
ρ
B
S
M
/
. Therefore, 
[
]
w
H
ρ
B

S
M
/
.  
Theorem 8-11: Systems K, D, T, S4, B, and S5 are decidable. 
Proof: Suppose F represents a formula, with n subformulae, of the modal system 
S under consideration, 
,
,
R
B
= 〈
〉
W
S
S
S
S
M
 is the canonical model of S, and w is a 
world in WS . 

294 
Decision-Making Agents 
 
 
K is decidable: Since F is a subformula of itself, from Theorem 8-7 and 
Theorem 8-10, 
F
AK
 iff 
F
B
K
M
 iff 
F
B
K
M
. Since 
K
M  is finite (at most 2n ), we 
can determine whether 
F
B
K
M
 or not. Therefore, K is decidable. 
D is decidable: Since RD  is serial, 
'
wR w
D
, for some 
'
w . Since 
'
wR w
D
, 
{
:
}
'
F
F
w
w
∈
⊆

. In particular, for any subformula 
G

 of F, if 
G
w
∈

 then 
'
G
w
∈
. Thus, there exists a world 
'
w  such that [
]
[ '
]
w
R
w
ρ
ρ
D
/
/
. Thus, RD  is 
serial. From Theorem 8-9, 
F
AD
 iff 
F
B
D
M
. Since 
D
M  is finite, the truth value of 
a formula with respect to 
D
M  can be determined in a finite time. Therefore, the 
decidability problem in D can be equivalently transformed to a satisfiability 
problem which can be solved in a finite time. 
T is decidable: Since RT  is reflexive, wR w
T
, for every w. Since wR w
T
, 
{
:
}
F
F
w
w
∈
⊆

. In particular, for any subformula 
G

 of F, if 
G
w
∈

 then 
G
w
∈
. Thus, for every world w, [
]
[
]
w
R
w
ρ
ρ
T
/
/
. Thus, RT  is serial.  The 
decidability of T follows as per the argument above. 
The rest of the cases are left as exercises.  
 
8.10 Modal First-Order Logics 
This section briefly introduces modal first-order logics. The syntax of modal 
first-order logics can be obtained from the classical first-order logic by the 
introduction of modalities into the language. Then we can produce modal first-
order systems analogous to the modal propositional systems K, D, T, S4, B, and 
S5 presented in the previous section. In principle, semantics of these first-order 
modal systems could be obtained by extending the possible world semantics for 
modal propositional systems. However, complications arise when due 
consideration is given to issues related to the underlying domain structure and 
term rigidity. Before addressing these issues, we briefly capture the syntax, 
axioms, and semantics of first-order modal systems. 
A formal axiomatic theory for the first-order version of K, denoted as 
FOL+K, can be obtained from the classical first-order logic (presented in chapter 
3) as follows: 
• 
The symbols of K are the first-order alphabet and the symbol ‘ ’ 
(necessary). 

 
Modal Logics 
295 
 
• 
Terms are expressions which are defined inductively as follows: 
− 
A variable or an individual constant is a term. 
− 
If f is an n-ary function symbol and 
1
2
, ,..., n
t t
t  are terms, then 
1
2
( , ,..., )
n
f t t
t
 is a term. 
− 
An expression is a term if it can be shown to be so only on the basis 
of the above two conditions. 
• 
Well-formed formulae (wffs) or formulae of modal first-order logics are 
recursively defined as follows: 
− 
If P is an n-ary predicate symbol and 
1
2
, ,..., n
t t
t  are terms, then 
1
2
( , ,..., )
n
P t t
t
 is an atomic formula. 
− 
If F is a formula, then 
F
¬
 and 
F

 are formulae. 
− 
If F is a formula and x is a variable, then 
( )
x F
∀
 is a formula. 
− 
If F and G are formulae, then F
G
→
 is a formula. 
− 
An expression is a formula only if it can be generated by the above 
four conditions. 
• 
If F, G, H are formulae, then the following are axioms of the theory: 
− 
Logical axioms from first-order logic 
− 
Modal Axioms:  
K: 
(
)
(
)
F
G
F
G
→
→
→



 
• 
The following are the rules of inference of K: 
− 
Modus Ponens (MP): If 
F
A
 and 
F
G
→
A
 then 
G
A
 
− 
Generalization: If 
F
A
 then 
( )
x F
∀
A
 
− 
Rule of Necessitation (RN): If 
F
A
 then 
F
A
 
• 
The definitions in K are the usual definitions of the connectives , ,
∧∨↔, 
and the following equivalencies of the existential and the modal operator 
for possibility: 
( )
(
)
◊
x F
x
F
F
F
∃
≡¬∀
¬
≡¬ ¬

 
The first-order versions of the systems D, T, S4, B, and S5 can be obtained 
from FOL+K as follows: 
FOL+D = FOL+K+ D:
◊
F
F
→

 

296 
Decision-Making Agents 
 
 
FOL+T  = FOL+K+ T :
F
F
→

 
FOL+S4  = FOL+K+ T :
F
F
→

+ 4:
F
F
→


 
FOL+B  = FOL+K+ T :
F
F
→

+ B:
◊
F
F
→
 
FOL+S5  = FOL+K+ T :
F
F
→

+ E :◊
◊
F
F
→
 
A normal model M is a structure of the following form:  
,
, , , ,
R B δ φ
= 〈
〉
W D
M
 
where W is a non-empty set of elements, to be called worlds; D is a non-empty 
set, called the domain of interpretation; R is any binary relation on W, to be 
called the accessibility relation for W; and B is a valuation function determines 
the truth value of a ground atom in a world as shown below: 
:
{
,
}
B
×
→
⊥
P
W
F
 
where P is the set of all ground atoms defined as follows: 
P = 
1
1
{ (
,...,
) |
 is an n-ary predicate symbol & (
,...,
)
}
n
n
n
p a
a
p
a
a
∈D
 
Thus if 
1
( (
,...,
), )
n
B p a
a
w  is F or ⊥, then we say that the atom 
1
(
,...,
)
n
p a
a
 is 
true or false, respectively, at the world w in a model M, and this is written as 
follows: 
1
(
,...,
)
w
n
p a
a
BM
 
Equivalently, B can be defined as a function to assign a set of ground atoms to 
each world w that is true in w: 
:
( )
B
→
W
P
P
 
δ  is a function that assigns to each world w a non-empty subset of D, called the 
domain of W. Thus, 
( )
w
δ
⊆D  
Finally, ϕ  is a function that represents the interpretation of constants and 
function symbols. It maps each pair of world w and n-ary function symbol f to a 
function from 
n →
D
D . Thus, 
( ,
)
n
w f
φ
∈
→
D
D  
Note that a constant a is a 0-ary function symbol and therefore ( , )
w a
φ
 is a fixed 
member of D. The assignment of a term t with respect to the model is recursively 
defined as follows: 
• 
If t is the individual constant a, then its assignment is ( , )
w a
φ
. 

 
Modal Logics 
297 
 
• 
If t is of the form 
1
( ,..., )
n
f t
t
, then its assignment 
( , )
w t
φ
 is 
1
( ,
)( ( , ),..., ( , ))
n
w f
w t
w t
φ
φ
φ
. 
For an arbitrary formula F, the satisfiability of a formula F with respect to a 
model 
,
, , , ,
R B δ φ
= 〈
〉
W D
M
 and a world w∈W , written as 
w F
BM
, is 
recursively defined as follows: 
1
( ,...,
)
w
n
P t
t
BM
 iff 
1
( ( , ),..., ( , ))
( )
n
P
w t
w t
B w
φ
φ
∈
  
w
F
¬
BM
 iff 
w F
HM
 
w F
G
→
BM
 iff 
w F
BM
 implies 
w G
BM
 
w
xF
∀
BM
 iff for all 
( )
d
w
δ
∈
 
[ / ]
w F x d
BM
 
w
xF
∃
BM
 iff there exists 
( )
d
w
δ
∈
 such that 
[ / ]
w F x d
BM
 
w
F
B 
M
 iff for every 
'
w ∈W  such that 
'
wRw , 
'
w F
BM
 
◊
w
F
BM
 iff for there is a 
'
w ∈W  such that 
'
wRw  and 
'
w F
BM
 
A formula F is satisfiable iff 
w F
BM
, for some model 
,
, , , ,
R B δ φ
= 〈
〉
W D
M
 and 
w∈W . F is valid in M , written as
F
BM
, iff 
w F
BM
, for every w∈W . F is valid 
in a class of models Σ , written as 
F
Σ
B
, iff 
F
BM
, for every 
∈Σ
M
. F is valid, 
written as 
F
BM
, iff F is valid in every class of models. 
There are at least two aspects of the model that should be discussed regarding 
the two functions δ  and φ  that were added to the normal model for 
propositional logic to handle object domains and term designation. For any two 
worlds w and 
'
w , if 
'
wRw , we have the following three possibilities for the 
relationship between ( )
w
δ
 and ( ')
w
δ
: 
• 
( )
( ')
w
w
δ
δ
=
: This is the fixed domain case where every world has the 
fixed set of objects. 
• 
( )
( ')
w
w
δ
δ
⊆
: This is the cumulative domain case where the objects in 
the current world continue to exist in every accessible world, but there 
may be new objects in the accessible worlds. In biology, for example, 
new species are being discovered all the time. 
• 
( )
( ')
w
w
δ
δ
⊇
: This is the case where the objects in the current world are 
not guaranteed to exist in accessible worlds. 
• 
No restrictions on the relation between ( )
w
δ
 and ( ')
w
δ
 

298 
Decision-Making Agents 
 
 
From the practical application standpoint, both the fixed domain and cumulative 
domain cases are important, though the fixed-domain interpretation has the 
advantages of simplicity and familiarity. As for the function, for any two worlds 
w and 
'
w , and for every function symbol f, we have the following two cases: 
• 
( ,
)
( ',
)
w f
w f
φ
φ
=
: This is the rigid designation case where the 
denotation of terms is the same in every world. 
• 
No restrictions on the relation between ( ,
)
w f
φ
 and ( ',
)
w f
φ
 
In the case of rigid designation, 
( )
p c

 follows from the axiom (
)
xF
G
∀
→
 and 
the two formulae 
( )
p c  and 
( ( )
( ))
x p x
p x
∀
→
. But this is not the case when the 
denotation of c is allowed to vary from world to world. 
The soundness and completeness results for each of the first-order versions 
of the systems D, T, S4, B, and S5 can be proven with respect to the 
model
,
, , , ,
R B δ φ
= 〈
〉
W D
M
, where the usual restriction on R applies, the domain 
is cumulative, and the term designation is rigid. Adding the Barcan Formula (BF) 
(Barcan 1946) can usually axiomatize first-order modal logics that are adequate 
for applications with a fixed domain:  
BF: 
[ ]
[ ]
x
F x
xF x
∀
→∀


 
where 
[ ]
F x  means F has zero or more free occurrences of the variable x. The 
above axiom means that if everything that exists necessarily possesses a certain 
property F then it is necessarily the case that everything possesses F. But in the 
cumulative domain case there might be some additional objects in accessible 
worlds that may not possess the property F. On the other hand, the converse 
[ ]
[ ]
xF x
x
F x
∀
→∀


 of the Barcan formula, however, can be validated in first-
order modal logics with cumulative domain and rigid designation: 
 
TFML 1: 
[ ]
[ ]
xF x
x
F x
∀
→∀
A 

K
 
Step 1: 
[ ]
[ / ]
xF x
F x y
∀
→
AK
 
[Axiom (
)
xF
G
∀
→
, where y is free for x in F] 
Step 2: 
(
[ ]
[ / ])
xF x
F x y
∀
→
A 
K
 
 
[Rule of Necessitation] 
Step 3: 
[ ]
[ / ])
xF x
F x y
∀
→
A 

K
 
[Axiom 
(
)
(
)
F
G
F
G
→
→
→



 and MP] 

 
Modal Logics 
299 
 
Step 4: 
(
[ ]
[ / ]))
y
xF x
F x y
∀
∀
→
A


K
 
[Generalization Rule] 
Step 5: 
[ ]
[ / ])
xF x
y
F x y
∀
→∀
A 

K
 
[Axiom 
(
)
(
)
x F
G
F
xG
∀
→
→
→∀
] 
Step 6: 
[ ]
[ ])
xF x
x
F x
∀
→∀
A 

K
  
[Variable renaming] 
 
The Barcan formula BF is not provable in any of the systems FOL+D, 
FOL+T, and FOL+S4 (Lemmon, 1960), but provable in each of the systems 
FOL+B and FOL+S5. 
 
TFML 2: 
[ ]
[ ]
x
F x
xF x
∀
→∀
A


S5
 
Step 1: 
[ ]
[ / ]
x
F x
F x y
∀
→
A


S5
 
 
[Axiom (
)
xF
G
∀
→
, where y is free for x in F] 
Step 2: 
[ ]
[ / ]
◊
◊
x
F x
F x y
∀
→
A


S5
 
[TK 2: If 
F
G
→
A
 then 
◊
◊
F
G
→
A
] 
Step 3: 
[ ]
[ / ]
◊x
F x
F x y
∀
→
A

S5
 
 
[TB 1: 
◊F
F
→
A

B
] 
Step 4: 
(
[ ]
[ / ])
◊
y
x
F x
F x y
∀
∀
→
A

S5
 
[Generalization Rule] 
Step 5: 
[ ]
[ / ]
◊x
F x
yF x y
∀
→∀
A

S5
 
[Axiom 
(
)
(
)
x F
G
F
xG
∀
→
→
→∀
] 
Step 6: 
[ ]
[ ]
◊x
F x
xF x
∀
→∀
A

S5
 
 
[Variable renaming] 
Step 7: 
[ ]
[ ]
◊x
F x
xF x
∀
→∀
A 


S5
 
[Axiom 
(
)
(
)
F
G
F
G
→
→
→



 and MP] 
Step 8: 
[ ]
[ ]
x
F x
xF x
∀
→∀
A


S5
 
 
[Axiom B:
◊
F
F
→
] 
 
Every model in the context of the systems FOL+B, and FOL+S5 is fixed as 
they derive the Bracan formula BF. The soundness and completeness results for 
each of the systems, namely FOL+D, FOL+T, and FOL+S4, with BF can be 

300 
Decision-Making Agents 
 
 
established with respect to the model
,
, , , ,
R B δ φ
= 〈
〉
W D
M
, where the usual 
restriction on R applies and the domain is fixed. 
8.11 Resolution in Modal First-Order Logics 
Each formula in first-order logic can be transformed to an equivalent set of 
clauses in the form 
1
...
n
L
L
∨
∨
, where each 
iL  is an atomic formula of the form 
P or 
P
¬
. The resolution principle that we presented in the mathematical logic 
chapter is applied on a transformed set of clauses. If a set of clauses is 
unsatisfiable, then the resolution process derives contradiction. To determine 
whether a formula F is a theorem of a set of clauses S or not, we first transformed 
F
¬
into a set of clauses and then combined that set with S. If no contradiction 
can be derived from the combined set using the resolution procedure, then F is a 
theorem of S. 
The main reason why an equivalent procedure cannot be developed in a 
modal system context is that, in general, an arbitrary modal formula cannot be 
transformed to an equivalent clause where each component is one of the atomic 
modal formulae P, 
P
¬
, ◊P , or 
P

. For example, there is no equivalent clausal 
representation of modal formulae 
(
)
P
Q
∨

 or 
(
)
◊P
Q
∧
 in the systems that we 
have considered. But, using the equivalence 
◊
P
P
≡¬ ¬

 and other equivalences 
between the logical connectives, an arbitrary modal formula can be transformed 
to an equivalent modal clause representation where only negation, disjunction, 
and conjunction symbols appear, and a negation appears only as a prefix of a 
proposition. An example of this transformation of the formula ◊
◊
P
P
→
is as 
follows: 
P
P
P
P
P
P
P
P
→
≡¬
∨
≡¬ ¬ ¬
∨
≡
¬
∨
,
,
◊
◊
◊◊
◊





 
The modal resolution procedure presented here first transforms modal 
formulae into these kinds of modal clauses. The procedure then eliminates 
modalities by introducing appropriate world symbols and quantifiers, thus 
transforming a modal clause to a formula in classical logic (not necessarily first-
order), which can then be transformed to a set of clauses for applying an 
enhanced resolution procedure. We first explain the transformation procedure 
below in the context of a few examples: 

 
Modal Logics 
301 
 
• 
( )
◊xP x
∀
 to 
([0 ], )
xP
a x
∀
: From the initial world 0 there is a world a 
accessible from 0 such that for all x 
( )
P x  holds, where P is interpreted in 
the world a. 
• 
( )
◊
x P x
∀
 to 
([0 ( )], )
xP
a x
x
∀
: From the initial world 0 there is a world a 
accessible from 0 but depending on x such that for all x 
( )
P x  holds, 
where P is interpreted in the world a. 
• 
( )
xP x
∀

 to 
([0 ], )
w xP
w x
∀∀
: From the initial world 0 and for every 
world w that is accessible from 0, for all x 
( )
P x  holds, where P is 
interpreted in the world w. 
• 
( )
x
P x
∀
 to 
([0 ], )
x wP
w x
∀∀
: From the initial world 0 and for every 
world w that is accessible from 0 and therefore independent of x, for all x 
( )
P x  holds, where P is interpreted in the world w. 
• 
( )
◊
xP x
∀

 to 
([0
], )
w xP
aw x
∀∀
: From the initial world 0 there is a 
world a accessible from 0, and for all worlds w which are accessible 
from a, for all x 
( )
P x  holds, where P is interpreted in the world w. 
• 
( )
◊xP x
∀

 to 
([0
], )
w xP
wa x
∀∀
: From the initial world 0 and for every 
world w accessible from 0 there is a world a accessible from w such that 
for every x 
( )
P x  holds, where P is interpreted in the world a. 
• 
( , )
◊x yP x y
∀∃

 to 
([0
], ,
([0
], ))
w xP
wa x f
wa x
∀∀
: Same as above 
except f is a skolem function that denotes a y that must exist in the world 
a in the form dependent on x. 
Our objective is to transform a modal formula into an equivalent formula in 
world-path syntax, which can then be further transformed into a set of clauses in 
the same way as in first-order logic. As in the above examples, a skolem function 
needs to have a world-path as its first argument because every term unification 
has to be carried out in a world that is indicated by the world-path in the context. 
But the standard skolemization for first-order logic cannot introduce such world-
paths. Skolemization from the process of transforming a world-path formula into 
a set of clauses can be avoided by considering modal formulae that are in 
negation normal form without the implication and equivalence sign, and with all 
negation signs moved in front of atoms.  For example, 
(
( )
( ))
x
P x
Q x
¬∀
∨

 is 
not in negation normal form, but its equivalent formula 
(
( )
( ))
◊
x
P x
Q x
∃
¬
∧¬
 is 
in negation normal form. Any modal formula can be brought into this form using 

302 
Decision-Making Agents 
 
 
the appropriate tranformation rules of first-order logic and the two additional 
equivalences 
◊
F
F
¬
≡
¬

 and 
◊F
F
¬
≡¬

. 
8.11.1 Transformation Algorithm 
Notation 
• 
WP: world-path of the form 
1
[
...
]
m
w
w
, where each 
iw  is a world symbol 
• 
VL: variable list of the form 
1
( ...
)
n
x
x
, where each 
ix  is a world symbol 
• 
|
|
VL : length of the list VL, that is, n if VL is 
1
( ...
)
n
x
x
 
• 
WP
w
+
: world-path 
1
[
...
]
m
w
w w  obtained by appending the word w to 
1
[
...
]
m
WP
w
w
=
 
• 
VL
x
+
: variable list 
1
( ...
)
n
x
x x  obtained by appending the variable x to 
1
( ...
)
n
VL
x
x
=
 
• 
(
)
f VL : the term 
1
(
,...,
)
n
f x
x
, where 
1
( ...
)
n
VL
x
x
=
 
Input 
• 
Modal formula F in negation normal form 
Output 
• 
The formula in the world-path syntax obtained by applying repeatedly 
the following set of transformation rules on the top-level call 
( ,[0],())
F
T
, where 0 denotes the initial world and () is the empty 
variable list 
Transformation Rules 
Rule 1: 
(
,
,
):
( ,
,
)
P WP VL
P WP VL
¬
= ¬
T
T
 
Rule 2: 
(
,
,
):
( ,
,
)
( ,
,
)
F
G WP VL
F WP VL
G WP VL
∧
=
∧
T
T
T
 
Rule 3: 
(
,
,
):
( ,
,
)
( ,
,
)
F
G WP VL
F WP VL
G WP VL
∨
=
∨
T
T
T
 
Rule 4: 
(
,
,
):
( ,
,
)
xF WP VL
x
F WP VL
x
∀
= ∀
+
T
T
 
Rule 5: 
(
,
,
):
( ,
,
)
F WP VL
w
F WP
w VL
= ∀
+

T
T
, where w is an added new 
variable which does not occur in F 

 
Modal Logics 
303 
 
 Rule 6: 
(
,
,
):
( { /
(
)},
,
)
xF WP VL
F x f VL
WP VL
∃
=
T
T
, where f is an added 
new |
|
VL -place function symbol and 
{ /
(
)}
F x f VL
 means substitute x 
by 
(
)
f VL  
Rule 7: 
(
,
,
):
( ,
(
),
)
◊F WP VL
F WP
a VL VL
=
+
T
T
, where a is an added new 
|
|
VL -place function symbol  
Rule 8: 
1
1
( ( ,..., ),
,
):
(
,
( ,
,
),...,
( ,
,
))
n
n
P t
t
WP VL
P WP
t WP VL
t WP VL
=
T
T
T
, where 
P is an n-place predicate symbol 
Rule 
9: 
1
1
( ( ,..., ),
,
):
(
,
( ,
,
),...,
( ,
,
))
n
n
f t
t
WP VL
f WP
t WP VL
t WP VL
=
T
T
T
, 
where f is an n-place function symbol 
Rule 10: 
( ,
,
):
x WP VL
x
=
T
 
Since a formula in world-path syntax contains no existential quantifier and 
no modal operator, it can be transformed to an equivalent set of clauses in the 
same way as in first-order logic but without the need for skolemiztion. 
Example 
(
( ( )
( )),[0],())
◊
x
P x
Q x
∀
∨

T
 
 
 
 
Given 
( ( ( )
( )),[0],( ))
◊
x
P x
Q x
x
∀
∨

T
 
 
 
 
Rule 4 
( ( )
( ),[0 ],( ))
◊
x
P x
Q x
w
x
∀
∨
T
 
 
 
 
Rule 5 
( ( ( ),[0 ],( ))
(
( ),[0 ],( )))
◊
x
P x
w
x
Q x
w
x
∀
∨
T
T
  
 
Rule 3 
( ( ( ),[0 ],( ))
( ( ),[0
( )],( )))
x
P x
w
x
Q x
wa x
x
∀
∨
T
T
 
 
Rule 7 
( ([0 ], )
([0
( )], ))
x P
w x
Q
wa x
x
∀
∨
 
 
 
 
Rules 8, 10 
Example 
(
(
( , )
( , , )),[0],())
x y
P x y
zQ x y z
∀∀
¬
∧∃
◊


T
 
 
Given 
(
(
( , )
( , , )),[0 ],())
◊
w
x y
P x y
zQ x y z
w
∀
∀∀
¬
∧∃

T
 
 
Rule 5 
 
( (
( , )
( , , )),[0 ],(
))
◊
w x y
P x y
zQ x y z
w
xy
∀∀∀
¬
∧∃

T
  
Rule 4 
(
( , )
( , , ),[0
( , )],(
))
w x y
P x y
zQ x y z
wa x y
xy
∀∀∀
¬
∧∃

T
 
Rule 7 
( (
( , ),[0
( , )],(
))
(
( , , ),[0
( , )],(
)))
w x y
P x y
wa x y
xy
zQ x y z
wa x y
xy
∀∀∀
¬
∧
∃
T
T

  
 
Rule 2 

304 
Decision-Making Agents 
 
 
(
(
( , ),[0
( , ) ],(
))
( ( , ,
( , )),[0
( , )],(
)))
w x y
u
P x y
wa x y u
xy
Q x y f x y
wa x y
xy
∀∀∀
∀
¬
∧
T
T
 
 
Rules 5, 6 
(
( ( , ),[0
( , ) ],(
))
( ( , ,
( , )),[0
( , )],(
)))
w x y
u
P x y
wa x y u
xy
Q x y f x y
wa x y
xy
∀∀∀
∀¬
∧
T
T
 
 
Rule 1 
(
([0
( , ) ], , )
([0
( , )], , ,
([0
( , )], , )))
w x y
u P
wa x y u x y
Q
wa x y
x y f
wa x y
x y
∀∀∀
∀¬
∧
 
 
Rules 8-10 
 
8.11.2 Unification 
The very first argument of each predicate or function symbol in a transformed 
modal formula is a world-path. The unification of these world-paths is the only 
difference from the unification of first-order logic terms. The world-path 
unification algorithm is dependent on the properties of the accessibility relation 
by which the worlds are connected. In the following list, we informally describe 
the algorithm in each of the cases of the accessibility relation. 
• 
Serial: In this case, two world-paths are unifiable when they are of equal 
length and the world-paths can be treated like ordinary terms. Therefore, 
world-paths [0
]
ua  and [0
]
bv  are unifiable with a unifier { / , / }
u b v a , 
whereas the world-paths [0
]
ua  and [0
]
bvw  are not. There is at most one 
mgu in this case. 
• 
Non-Serial: When the accessibility relation is not serial, 
(
)
P
P
∧¬

 is 
not necessarily inconsistent because no worlds may be accessible from 
the current world. But the transformation algorithm on 
(
)
P
P
∧¬

 
produces 
the 
formula 
( ([0 ])
([0 ]))
w P
w
P
w
∀
∧¬
, 
which 
yields 
inconsistency at the world w by a simple world-path matching. 
Therefore, we must assume that the accessibility relation is at least serial. 
• 
Reflexive: The meaning of the formula 
([
], )
P aw x  is that 
( )
P x  is true in 
every world w accessible from the current world a. Now, if the 
accessibility relation is reflexive, then a itself can substitute for w and in 
this case 
( )
P x  is true in a. Thus, for the purpose of unifying [
]
aw  and 
another world-path, say[ ]
a , the world w can simply be removed from the 
world-path [
]
aw  (or be substituted with the empty path []) and the 
formula written as 
([ ], )
P a x . Therefore, if the accessibility relation is 
reflexive, then the unification algorithm must consider all possibilities to 

 
Modal Logics 
305 
 
remove a world variable w by the substitution { /[]}
w
 as well as all 
possibilities to unify two world-paths pair-wise as is the case when the 
accessibility relation is serial. For example, the two world-paths [0
]
ua  
and [0
]
bvw  are unifiable with the two unifiers { / , / ,
/[]}
u b v a w
 and 
{ / , /[],
/ }
u b v
w a . 
• 
Symmetric: The meaning of the formula 
([
], )
P abw x  is that 
( )
P x  is 
true in every world w accessible from the world b which is accessible 
from the current world a. Now, if the accessibility relation is symmetric, 
then a is accessible from b, and thus a can substitute for w and in this 
case 
( )
P x  is true in a. Thus, for the purpose of unifying [
]
abw  and 
another world-path, say[ ]
a , bw can simply be removed from the world-
path [
]
abw  (or we can substitute 
1
b− for w) and the formula can be 
written as 
([ ], )
P a x . Therefore, if the accessibility relation is symmetric, 
then the unification algorithm must consider all possibilities to remove a 
world variable w and its predecessor u from the world-path by the 
substitution 
1
{ /
}
w u−
 and must consider all possibilities to unify two 
world-paths pair-wise as is the case when the accessibility relation is 
serial. For example, the two world-paths [0 ]
u  and [0
]
bvw  are unifiable 
with the unifier 
1
{ / , /
}
u b v w−
. 
• 
Transitive: The meaning of the formula 
([
], )
P abc x  is that 
( )
P x  is true 
in the world c accessible from the world b which is accessible from the 
current world a. Now, if the accessibility relation is transitive then c is 
accessible from a. Thus, for the purpose of unifying [
]
abc  and another 
world-path, say[
]
aw , bc can simply be collapsed to c (or substitute [bc] 
for w) to be unifiable with w. For example, the two world-paths [0
]
wab  
and [0
]
cdub  are unifiable with the unifier { /[
], / }
w cd u a , but the 
substitution 
1
1
2
2
{ /[
], /[
],
/ }
w cdu
u u u
u
a , that is, 
1
1
{ /[
], /[
]}
w cdu
u u a
 is also 
a unifier by splitting u via the introduction of 
1u . Therefore, if the 
accessibility relation is transitive, then the unification algorithm must 
consider both the collapsing of world-paths and the splitting of world 
variables. 
• 
Euclidean: The meaning of the formula 
([
], )
P abw x  is that 
( )
P x  is true 
in every world w accessible from the world b which is accessible from 
the current world a. The meaning of the formula 
([
], )
P ac x  is that 
( )
P x  
is true in the world c accessible from the world a. Therefore, the 

306 
Decision-Making Agents 
 
 
unification of the two world-paths [
]
abw  and [
]
ac  is possible provided 
the accessibility relation is Euclidean, guaranteeing c is accessible from 
b. Therefore, if the unification {[
]/ }
bw
c  is right, then c must be 
accessible from b. The unification algorithm must keep track of this kind 
of information. 
• 
Reflexive and Symmetric: The ideas of removing world variables for 
reflexivity and for symmetry can be combined. Therefore, if the 
accessibility relation is reflexive and symmetric then the unification 
algorithm must consider all possibilities to remove a world variable w by 
the substitution { /[]}
w
, to remove a world variable w and its predecessor 
u in the world-path by the substitution 
1
{ /
}
w u−
, and to unify two world-
paths pair-wise. For example, the two world-paths [0 ]
u  and [0
]
bcvw  are 
unifiable with the unifier 
1
{ / , /
,
/[]}
u b v c
w
−
. 
• 
Reflexive and Transitive: The ideas of removing world variables for 
reflexivity and collapsing world-paths and splitting world variables for 
transitivity can be combined. Therefore, if the accessibility relation is 
reflexive and transitive, then the unification algorithm must consider all 
possibilities to remove a world variable w by the substitution { /[]}
w
, 
collapse a world-path, split a world variable, and unify two world-paths 
pair-wise. For example, the two world-paths [0
]
ude  and [0
]
bcvw  are 
unifiable 
with 
the 
two 
unifiers 
1
{ / , /
,
/[
]}
u b v c
w de
−
 
and 
{ /[
], / ,
/ }
u bc v d w e . 
• 
Equivalence: The ideas of removing world variables for reflexivity and 
for symmetry, and collapsing world-paths and splitting world variables 
for transitivity can be combined. Moreover, when the accessibility 
relation is an equivalence relation, the modal system is S5, which has six 
irreducible modalities ϕ , ¬, ◊,  , ◊¬, and 
¬

. Therefore, world-
paths are expected to be shorter in length. 
The following example illustrates the transformation and unification 
algorithms. Specifically, we prove via refutation that in the modal system S4, in 
which the accessibility relation is reflexive and transitive, the formula 
◊
P
P
→

 
 is a theorem. 

 
Modal Logics 
307 
 
Example 
◊
P
P
→

 
 
 
 
Given 
(
)
◊
P
P
¬
→

 
  
 
Negation 
◊◊
P
P
∧
¬


 
 
 
Negation normal form 
([0 ])
([0
])
uP
u
v P
avb
∀
∧∀¬
  
World-path syntax 
([0 ])
([0
])
uP
u
v P
avb
∀
∧∀¬
  
Conjunctive normal form 
1:
([0 ])
2:
([0
])
C
P
u
C
P
avb
⎧
⎨
¬
⎩
 
 
Clausal form 
1&
2:
{ /[], /[
]}
C
C
empty
unifier
v
u
ab
=
 
 
Resolution refutation 
 
8.12 Modal Epistemic Logics 
Epistemic logic is concerned with the ideas of an agent’s or decision-maker’s 
“knowledge” and “belief” within a context. Modal epistemic logic is a logic for 
knowledge and belief and is viewed as an instance of modal logic by interpreting 
necessity and possibility in an epistemic manner. Therefore, the soundness and 
completeness results and the resolution principle that we have presented before 
can be adopted for modal epistemic logics. 
Traditionally, logic of knowledge is based on the modal system S5. The logic, 
which will be denoted as 
K
L , can be obtained from S5 by interpreting the notion 
of necessity as knowledge. Thus know F
〈
〉
 is written instead of 
F

 to represent 
“the agent knows that F.” The axioms and inference rules of 
K
L  are analogous to 
those in system S5.  
Axioms of 
K
L : 
LK 1: All propositional axioms 
LK 2:  
(
)
(
)
know F
G
know F
know G
〈
〉
→
→〈
〉
→〈
〉
 
LK 3:  know F
F
〈
〉
→
 
LK 4:  know F
know know F
〈
〉
→〈
〉〈
〉
 
LK 5:  
know F
know
know F
¬〈
〉
→〈
〉¬〈
〉
 

308 
Decision-Making Agents 
 
 
Inference Rules of 
K
L : 
Modus Ponens (MP): If 
F
A
 and 
F
G
→
A
 then 
G
A
 
Rule of Necessitation (RN): If 
F
A
 then 
know F
〈
〉
A
 
Axiom LK 3 (axiom T in S5) states that the agent’s known facts are true. Axioms 
LK 4 (axiom 4 in S5) and LK 5 (a version of axiom E in S5) are positive and 
negative introspection, respectively. Axiom LK 4 states that an agent knows that 
it knows something whereas axiom LK 5 states that an agent knows that it does 
not know something. Epistemic formulae in 
K
L  are interpreted on Kripke’s 
possible world model structures in a manner exactly like the formulae in system 
S5. 
The idea of an agent’s belief differs from knowledge only in that an agent 
may believe in something that is not true (as opposed to Axiom LK 3) but will 
not believe in anything that is demonstrably false. Thus bel F
〈
〉
 is written 
specifically to represent “the agent believes that F” and the relation between the 
modal operators know
〈
〉 and bel
〈
〉is the following: 
know F
F
bel F
〈
〉
≡
∧〈
〉
 
Therefore know F
bel F
〈
〉
→〈
〉
 holds. This gives rise to the logic of belief, 
denoted as 
B
L , based on the modal system KD4E. The axioms and inference 
rules of 
B
L  are analogous to those in system KD4E. 
Axioms of 
B
L : 
LB 1: All propositional axioms 
LB 2: 
(
)
(
)
bel
F
G
bel F
bel G
〈
〉
→
→〈
〉
→〈
〉
 
LB 3: 
bel
¬〈
〉⊥ 
LB 4: bel F
bel bel F
〈
〉
→〈
〉〈
〉
 
LB 5: 
bel F
bel
bel F
¬〈
〉
→〈
〉¬〈
〉
 
Inference Rules of 
B
L : 
Modus Ponens (MP): If 
F
A
 and 
F
G
→
A
 then 
G
A
 
Rule of Necessitation (RN): If 
F
A
 then 
bel F
〈
〉
A
 
Axiom LB 2 states that an agent believes all the logical consequences of its 
beliefs, that is, an agent's beliefs are closed under logical deduction. For example, 
belief in Cancelled generates belief in Cancelled
Delayed
∨
 due to the 
knowledge 
Cancelled
Cancelled
Delayed
→
∨
. Axiom LB 3, which is 

 
Modal Logics 
309 
 
equivalent to axiom D in KD4E, states that an agent does not believe in 
inconsistency. The derivation of the symbol ⊥ from the database implies 
inconsistency. The two facts that an agent believes that it believes in something 
and an agent believes that it does not believe in something are expressed by 
Axioms LB 4 and LB 5, respectively.  
Proposition 8-5: The following are theorems of 
B
L : 
(
)
(
)
bel
F
G
bel F
bel G
bel F
bel G
bel
F
G
bel F
bel
F
〈
〉
∧
↔〈
〉
∧〈
〉
〈
〉
∨〈
〉
→〈
〉
∨
〈
〉
→¬〈
〉¬
 
 
We can now construct a modal epistemic logic of knowledge and belief, 
denoted 
KB
L
, that has two modal operators 
know
〈
〉 and 
bel
〈
〉 with the 
relationship know F
F
bel F
〈
〉
≡
∧〈
〉
, and whose axioms are the combined set of 
axioms of 
K
L  and 
B
L . Axiom LK 4 can be excluded from this combined set as 
TS5 3 suggests that it is derivable. Although know F
bel F
〈
〉
→〈
〉
 is derivable 
from the relationship know F
F
bel F
〈
〉
≡
∧〈
〉
, it can be added as an axiom for the 
sake of convenience. Consequently, one can discard the rule of necessitation of 
B
L  as it is superfluous in presence of the rule of necessitation of 
K
L . 
8.13 Logic of Agents Beliefs (LAB) 
As mentioned in the introduction of this chapter, modal logics are not suitable to 
represent an agent’s degrees of support about an assertion. In this section, we 
extend the syntax of traditional modal epistemic logic to include an indexed 
modal operator 
d
sup
〈
〉 to represent these degrees. The proposed Logic of Agents 
Beliefs (LAB) can model support to an assertion (to some extent d), but does not 
necessarily warrant an agent committing to believe in that assertion. The 
extended modal logic is given a uniform possible world semantics by introducing 
the concept of an accessibility hyperelation. Within our proposed framework, 
levels of support may be taken from one of a variety of different qualitative, 
semi-qualitative, and quantitative “dictionaries” which may be supplied with 
aggregation operations for combining levels of support. 
There are two approaches to handling degrees of supports about assertions: 
considering degrees of supports as terms of the underlying logic (Fagin et al., 

310 
Decision-Making Agents 
 
 
1990), and handling degrees of supports as modalities (Fattorosi-Barnaba and 
Amati, 1987; Halpern and Rabin, 1987; Alechina, 1994; Chatalic and 
Froidevaux, 1992). The former approach requires axioms to handle the dictionary 
representing degrees of supports, and an associated algebra (for example, axioms 
of arithmetic and probability if the probabilistic dictionary is considered). On the 
other hand, the modality-based approach, which LAB has adopted, requires 
modal axioms for the properties of supports, and the degrees of these supports are 
manipulated via a meta-level reasoning. The main advantage of this approach is 
that it keeps the logic generic, capable of multiple interpretations for various 
qualitative and quantitative dictionaries. The modal operator for support in LAB 
is interpreted via hyperelation, and is a generalization of the accessibility relation 
concept of possible world semantics. The extended hyperelation-based possible 
world semantics coincides with the probabilistic Kripke model (Voorbraak; 
1996) when the probabilistic dictionary is considered to introduce the support 
modal operator. The uniqueness of LAB is that its approach is not grounded to 
any fixed dictionary, and it has the ability to deal with inconsistent supports. 
Both of these features are highly desirable when reasoning with uncertain agent 
knowledge from multiple sources. 
8.13.1 Syntax of LAB 
Suppose P is the set of all propositions that includes the special symbol F  (true) 
and D is an arbitrary dictionary of symbols that will be used to label supports in 
propositions and assertions. In general, a dictionary will be a semi-lattice with a 
partial order relation ≤ and top element Δ. For the dictionary of probability, the 
lattice is a chain and Δ is 1. LAB is essentially a propositional logic extended 
with certain modal operators. The modal operator bel
〈
〉 of LAB corresponds to 
“belief.” In addition, for each dictionary symbol d ∈ D, we have the modal 
“support” operator 
d
sup
〈
〉 in LAB. The formulae (or assertions) of LAB extend 
the domain of propositional formulae to the domain of formulae as follows: 
• 
propositions are formulae. 
• 
bel F
〈
〉
 is a formula, where F is a formula. 
• 
d
sup
F
〈
〉
 is a formula, where F is a formula and d is in the dictionary D. 
• 
F
¬
and F
G
∧
 are formulae, where F and G are formulae. 
We take ⊥ (false) to be an abbreviation of ¬ F . Other logical connectives and 
the existential quantifier are defined using ¬ and ∧ in the usual manner.  

 
Modal Logics 
311 
 
Example 
Consider a situation when the agent (or decision-maker) observes heavy rain 
while preparing to go to town for the game. The newly discovered weather status 
then becomes the agent’s knowledge. The agent then infers from its common 
sense knowledge that there are three mutually exclusive and exhaustive 
possibilities or “candidate” states of the game, On, Cancelled, and Delayed, and 
so constructs arguments for and against these alternatives. These arguments use 
other beliefs, based on observations, such as transport disruption due to rain, the 
time of the game, and terrorist attack threats. Here is how a part of this problem 
is modeled as various types of sentences in the syntax of LAB: 
Arguments: 
(
)
0.6
0.9
0.7
bel Transport Disruption
sup
Delayed
bel Last Game
sup
On
Cancelled
bel Terrorist Threat
sup
Delayed
〈
〉
→〈
〉
〈
〉
→〈
〉
∨
〈
〉
→〈
〉
 
Rules: 
(
)
bel
Heavy Rain
Transport_Disruption
〈
〉
→
 
Integrity Constraints:  
( (
)
(
)
(
))
(
)
know
On
Delayed
On
Cancelled
Delayed
Cancelled
know On
Delayed
Cancelled
〈
〉¬
∧
∧¬
∧
∧¬
∧
〈
〉
∨
∨
 
Beliefs: 
know Heavy Rain
bel LastGame
〈
〉
〈
〉
 
In this model, informally, 1) arguments are for and against decision options 
at hand; 2) rules help to deduce implicit knowledge; 3) integrity constraints (Das, 
1992) are properties that the knowledge base must satisfy; and 4) beliefs reflect 
the agent’s understanding of the current situation. In the above representation, 
arguments encode the agent’s own supports for various candidates, the only rule 
infers the believability of transport disruption from heavy rain, the integrity 
constraints encode three mutually exclusive and exhaustive candidates, and 
beliefs are the current beliefs and knowledge of the agent. Note that the presence 
of the knowledge Heavy Rain  means it is currently raining. On the other hand, 

312 
Decision-Making Agents 
 
 
LastGame
bel
〈
〉
 means the agent believes that the game is the last game of the 
day.  
 
8.13.2 Axioms of LAB 
The axioms of LAB include all the axioms and inference rules of the logic of 
belief 
B
L  relating the modal operator bel
〈
〉. We now present the axioms and 
inference rules relating to the modal operator 
d
sup
〈
〉. The following inference 
rule states that the support operator is closed under implication. In other words, if 
F has support d and F
G
→
 is valid in LAB then G too has support d. 
LAB IR 1: 
,
d
d
if
F
G then
sup
F
sup
G for everyd
D
→
〈
〉
→〈
〉
∈
A
A
 
An assertion that is believed by an agent always has the highest support (at least 
from the agent itself as a subjective judgment), and the following axiom reflects 
this behavior: 
LAB 1: bel F
sup
F
Δ
〈
〉
→〈
〉
 
Proposition 8-6: if
F then
sup
F
Δ
〈
〉
A
A
 
 
Support operators can be combined to obtain a single support operator by 
using the following axiom: 
LAB 2: 
1
2
1
2 (
)
d
d
d
d
sup
F
sup
G
sup
F
G
⊗
〈
〉
∧〈
〉
→〈
〉
∧
 
where 
:
⊗
×
→
D
D
D  is the function for combining supports for two assertions. 
The axiom states that if 
1d  and 
2
d  are supports for F and G respectively then 
1
2
(
,
)
d d
⊗
 (or 
1
2
d
d
⊗
 in infix notation) is a derived support for F
G
∧
. Note that 
d
d
⊗Δ =
, for every d in D. The function takes into account the degrees of 
dependency between the two supports it is combining via their common source of 
evidence. In the Dempster-Shafer interpretation of LAB to be presented later in 
this subsection, the function becomes Dempster’s combination rule for 
combining two masses on focal element. 

 
Modal Logics 
313 
 
Proposition 8-7: The following are theorems of LAB: 
1
2
1
2
(
)
(
)
(
)
(
)
d
d
d
d
d
d
d
sup
sup
F
sup
F
G
sup
F
G
sup
F
G
sup
F
sup
G
Δ
⊗
〈
〉
〈
〉
∧〈
〉
→
→〈
〉
∧
〈
〉
→
→〈
〉
→〈
〉
F
 
Example 
Here is a simple example derivation of support for the game to be delayed: 
know Heavy Rain
〈
〉
A
 
 
 
 
Given 
bel Heavy Rain
〈
〉
A
  
 
Axiom know F
bel F
〈
〉
→〈
〉
 
(
)
bel
Heavy Rain
Transport Disruption
〈
〉
→
A
 
Given 
bel Transport Disruption
〈
〉
A
 
Axiom LB 2 and MP 
0.6
sup
Delayed
〈
〉
A
  
 
Given first argument and MP 
 
8.13.3 Possible World Semantics of LAB 
This section presents a coherent possible world semantics that clarifies the 
concept of support within the concepts of knowledge and belief. A model of LAB 
is a 4-tuple 
, , ,
s
V R R
W
 
in which W is a set of possible worlds. A world consists of a set of qualified 
assertions outlining what is true in the world. V is a valuation that associates each 
world with a subset of the set of propositions. In other words, 
:
( )
V
→Π
W
P  
where P is the set of propositions and 
( )
Π P  is the power set of P. The image of 
the world w under the mapping V, written as 
( )
V w , is the set of all propositions 
that are true in the world w. This means that p holds in w for each p in 
( )
V w . 
The relation R is an accessibility relation that relates a world w to a set of 
worlds considered possible (satisfying all the integrity constraints) by the 
decision-maker from w. In a decision-making context, if there are n mutually 
exclusive and exhaustive candidates for a decision that are active in a world w, 
then there are n possible worlds. 

314 
Decision-Making Agents 
 
 
We introduce semantics for our support operator by introducing the concept 
of a hyperelation that relates sets of worlds to the current world as opposed to 
relating only one set of worlds to the current world by the accessibility relation 
for the belief operator. The hyperelation 
s
R  is a subset of the set 
(
)
×
×Π
W
D
W  
Semantically, if 
, ,
s
w d
'
R
〈
〉∈
W
, then there is an amount of support d for moving 
to one of the possible worlds in W'  from the world w. If 
'
W  is empty, then the 
support is for an inconsistent world.  
An assertion is a belief of a decision-maker at a world w if and only if it is 
true in all possible worlds that are accessible from the world w. Note that the 
members of 
s
R  have been considered to be of the form 
, ,
w d
′
〈
〉
W  rather than 
, ,
w d w′
〈
〉. The main reason is that the derivability of 
d
sup
F
〈
〉
 means F is true 
only in a subset of all possible worlds accessible from w.  If F is true in all 
possible worlds accessible from w then we would have had bel F
〈
〉
, which 
implies the highest form of support for F that is greater than or equal to d. 
Due to the axioms related to the modal operator bel
〈
〉, the standard set of 
model properties that will be possessed by the accessibility relation R is the 
following: 
LAB MP 1: R is serial, transitive, and Euclidean. 
The hyperelation 
s
R  satisfies the following properties due to the axioms related 
to the modal operator 
d
sup
〈
〉: 
LAB MP 2: For every 
1
2
,
,
w w w  in W and 
1
2
,
d d  in D, the relation 
s
R  satisfies 
the following: 
• 
, ,
s
w
R
〈
Δ
〉∈
W
. 
• 
If 
1
1
,
,
w d
〈
〉
W  
and 
2
2
,
,
w d
〈
〉
W
 
are 
in 
s
R , 
then 
1
2
1
2
,
,
s
w d
d
R
〈
⊗
∩
〉∈
W
W
. 
Given the current world w, a decision-maker either stays in the current world if 
nothing happens (non-occurrence of events) or makes a transition to one of the 
possible worlds accessible from w by 
s
R . In either case, the new current world 
always belongs to W, and thus 
, ,
s
w
R
〈
Δ
〉∈
W
. The other property simply 
composes supports for worlds using the function ⊗ defined earlier, but checks 
dependencies between 
1
W  and 
2
W . 

 
Modal Logics 
315 
 
The semantics of supports and beliefs are as follows. Given a model 
, , ,
s
V R R
= W
M
, the truth values of formulae with respect to a world w are 
determined by the rules given below: 
• 
,w
〈
〉
B
F
M
 
• 
,w p
〈
〉
B M
 if and only if 
( )
p
V w
∈
 
• 
,w
d
sup
F
〈
〉〈
〉
B M
 if and only if there exists 
, ,
w d
'
〈
〉
W
 in 
s
R  such that 
,
'
w F
〈
〉
B M
, for every w'
'
∈W  
• 
,w
bel F
〈
〉〈
〉
B M
 if and only if for every w' ∈W  such that wRw' , 
,
'
w F
〈
〉
B M
 
• 
,w
F
〈
〉¬
B M
 if and only if 
,w F
〈
〉
B M
 
• 
,w F
G
〈
〉
∧
B M
 if and only if 
,w F
〈
〉
B M
 and 
,w G
〈
〉
B M
 
A formula F is said to be true in model M (written as 
F
BM
 or simply 
F
B
 
when M is clear from the context) if and only if 
,w F
〈
〉
B M
, for every w in W. A 
formula F is said to be valid if F is true in every model.  
Example 
Continuing with our earlier example, the set P of propositions is the following set 
of 7 elements: 
{On, Delayed, Cancelled, Heavy Rain, Transport Disruption, Last Game, 
Terrorist Threat} 
Therefore, the set of all possible worlds can potentially contain 
7
2  elements, 
each corresponding to an element of the power set of P. But the set of integrity 
constraints, along with the beliefs and knowledge, will make some of worlds 
impossible. For example, a world containing the elements On and Cancelled or a 
world that does not contain Heavy Rain. Consequently, the set W contains 48 
elements, instead of 128, two of which are shown below as examples: 
{
,
,
}
{
,
,
,
}
Heavy Rain LastGame Delayed
Heavy Rain Transport Disruption LastGame Cancelled   
Given the current state of world (say, 
0
w ) as shown in the example, the relation 
R will not relate the first of these examples as a believable world as it does not 
contain a proposition that is currently believed (for example, Transport 
Disruption). The following set has to be a part of every possible world from 
0
w : 

316 
Decision-Making Agents 
 
 
{
,
,
}
b
w
Heavy Rain LastGame Transport Disruption
=
 
The relation R relates the following set of worlds to the current world: 
1
2
3
{
}
{
}
{
}
b
b
b
w
w
On
w
w
Delayed
w
w
Cancelled
=
∪
=
∪
=
∪
 
The relations R and 
s
R  in the model definition from 
0
w  are defined as follows 
(see Figure 8-6): 
0
0
1
0
2
0
3
0
0
2
0
1
3
(
)
{
,
,
,
,
,
}
(
)
{
,0.6,{
} ,
,0.9,{
,
} }
s
R w
w w
w w
w w
R w
w
w
w
w w
= 〈
〉〈
〉〈
〉
= 〈
〉〈
〉 
1
w
0
w
2
w
3
w
1
w
0
w
2
w
3
w
R
s
R
0.6
0.9
 
Figure 8-6: Illustration of accessibility relation and hyperelation 
 
8.13.4 Soundness and Completeness of LAB 
First of all, the following two propositions prove that the validity in a class of 
models is preserved by the use of the rules of inference and the axioms of LAB. 
Proposition 8-8: For all formulae F and G and model M: 
• 
If 
F
B
 then 
bel F
〈
〉
B
 
• 
If 
F
G
→
B
 then 
d
d
sup
F
sup
G
〈
〉
→〈
〉
B
 

 
Modal Logics 
317 
 
• 
If 
F
B
 then 
sup
F
Δ
〈
〉
B
 
Proof: Let 
, , ,
s
V R R
= W
M
 be a model and w∈W . 
• 
Suppose 
F
B
. Then for every w' ∈W , 
,w' F
〈
〉
B M
. In particular, for every 
w' ∈W  such that wRw' , 
,w' F
〈
〉
B M
. Therefore, 
,w
bel F
〈
〉〈
〉
B M
. 
• 
Suppose 
,w
d
sup
F
〈
〉〈
〉
B M
. Then there is 
, ,
s
w d
R
′
〈
〉∈
W
 such that 
,w' F
〈
〉
B M
, for every w'
'
∈W . Since 
F
G
→
B
, for every w'
'
∈W , 
,w' F
G
〈
〉
→
B M
. Therefore, for every w'
'
∈W , 
,w' G
〈
〉
B M
. Therefore, 
,w
d
sup
G
〈
〉〈
〉
B M
. 
• 
Suppose 
F
BM
.  From the property of 
s
R , 
, ,
s
w
W
R
〈
Δ
〉∈
. Since 
F
BM
, 
for every w' ∈W , 
,w' F
〈
〉
B M
. Thus, by definition, 
sup
F
Δ
〈
〉
BM
. 
Proposition 8-9: For all formulae F and G, axioms of LB and LAB 1 and LAB 2 
are valid in LAB. 
Proof: Proofs of validity of the axioms related to the modal operator bel
〈
〉 can 
be found above in the section on modal logic. 
To prove bel F
sup
F
Δ
〈
〉
→〈
〉
, suppose 
,w
bel F
〈
〉〈
〉
B M
. Therefore, for every 
w' ∈W  such that wRw' , 
,w' F
〈
〉
B M
. But 
, ,
s
w
R
〈
Δ
〉∈
W
 and 
,w' F
〈
〉
B M
, for every 
w' ∈W . Therefore, by definition, 
,w
sup
F
〈
〉
Δ
〈
〉
B M
. 
To prove 
1
2
1
2 (
)
d
d
d
d
sup
F
sup
G
sup
F
G
⊗
〈
〉
∧〈
〉
→〈
〉
∧
, suppose 
1
,w
d
sup
F
〈
〉〈
〉
B M
 
and 
2
,w
d
sup
G
〈
〉〈
〉
B M
. 
Therefore, 
there 
exists 
1
(
)
∈Π
W
W  
such 
that 
1
1
,
,
s
w d
R
〈
〉∈
W
 and for every 
1
w' ∈W , 
,w' F
〈
〉
B M
. Similarly, there exists 
2
(
)
∈Π
W
W  such that 
2
2
,
,
s
w d
R
〈
〉∈
W
 and for every 
2
w' ∈W , 
,w' G
〈
〉
B M
. Thus, 
for 
every 
1
2
w' ∈
∩
W
W , 
,w' F
G
〈
〉
∧
B M
. 
By 
LAB 
MP 
2, 
1
2
1
2
,
,
s
w d
d
R
〈
⊗
∩
〉∈
W
W
. Thus, there exists 
1
2
(
)
∩
∈Π
W
W
W  such that 
1
2
1
2
,
,
s
w d
d
R
〈
⊗
∩
〉∈
W
W
 
and 
for 
every 
1
2
w' ∈
∩
W
W , 
,
'
M w F
G
〈
〉
∧
B
. 
Therefore, by definition, 
1
2
,
(
)
w
d
d
sup
F
G
〈
〉
⊗
〈
〉
∧
B M
. 
 
Proposition 8-8 and Proposition 8-9 establish the basis of the soundness 
result. Next, we outline the completeness proof along the line of (Chellas, 1980). 
For the completeness result, the following class of models is relevant. 

318 
Decision-Making Agents 
 
 
A model 
, , ,
s
V R R
= W
M
 of LAB is called a canonical model, written as 
Mc, if and only if: 
• 
W = {w: w is a maximal consistent set in logic LAB}. 
• 
For every w, bel F
w
〈
〉
∈
 if and only if for every w' ∈W such that 
wRw' , F
w'
∈
. 
• 
For every w and d, 
d
sup
F
w
〈
〉
∈
 if and only if there exists 
, ,
s
w d
'
R
〈
〉∈
W
 such that F
w'
∈
, for every w' ∈W . 
• 
For each proposition P, 
,w P
〈
〉
B M
 if and only if P ∈ w. 
Proposition 8-10: Let M be a canonical model of LAB. Then for every w in W, 
,w F
〈
〉
B M
 if and only if F ∈ w. 
 
Therefore, the worlds in a canonical model M for LAB will always verify just 
those sentences they contain. In other words, the sentences that are true in such a 
model are precisely the theorems of LAB. In other words, 
F
A
 if and only if 
F
BM
. 
Existence of a canonical model for LAB is shown by the existence of a proper 
canonical model defined as follows. 
A model 
, , ,
s
V R R
= W
M
 of LAB is called a proper canonical model, if 
and only if:  
• 
W = {w: w is a maximally consistent set in LAB}. 
• 
For every w and w' , 
b
wR w'  if and only if {
:
}
F
bel F
w
w'
〈
〉
∈
⊆
. 
• 
For every w, d and 
'
W , 
, ,
s
w d
'
R
〈
〉∈
W
 if and only if 
{
:
}
d
sup
F F
'
w
〈
〉
∈∩
⊆
W
. 
• 
For each proposition P, 
P
BM
 if and only if P
w
∈
. 
By definition, a proper canonical model exists and a proper canonical model 
is a canonical model. 
Proposition 8-11: If M is a proper canonical model of LAB then the model 
satisfies LAB MP 1 and LAB MP 2. 

 
Modal Logics 
319 
 
Suppose Γ is the class of all models satisfying LAB MP 1 and LAB MP 2. 
Then the following soundness and completeness theorem establishes the fact that 
LAB is determined by Γ. 
Theorem 8-12: For every formula F in LAB, 
F
A
 if and only if 
F
B
. 
 
8.13.5 Rational Extension of LAB 
A rational system, denoted as LAB(R), extends LAB by some axioms reflecting 
the behavior of a rational decision-making agent. First of all, a rational agent 
always believes in anything that has support with the top element of the 
dictionary. Thus, the following axiom should also be considered for LAB(R): 
LAB 3: sup
F
bel F
Δ
〈
〉
→〈
〉
 
This axiom, of course, derives that an assertion and its negation are not 
simultaneously derivable with the top element as support. That is, we have an 
integrity constraint of the following form: 
Proposition 8-12: sup
F
sup
F
Δ
Δ
〈
〉
∧〈
〉¬
→⊥ 
 
It is difficult to maintain consistency of a database in the presence of the 
above axiom, particularly when the database is constructed from different 
sources; mutual inconsistency and mistakes sometimes need to be tolerated. In 
these circumstances, it might be left to the decision-maker to arbitrate over which 
assertions to believe and which not to believe. 
Consider the following restriction on the hyperelation Rs, which says that the 
believable worlds are a subset of the worlds with the highest support: 
LAB MP 3: For every w in W, if 
, ,
s
w
'
R
〈
Δ
〉∈
W
 then 
( )
'
R w
=
W
, where 
( )
R w  is {
:
}
w'
wRw'
∈W
. 
If an agent is rational, then the concepts of believability and support with the 
top element coincide. In other words, 
LAB IR 1: bel F
sup
F
Δ
〈
〉
↔〈
〉
 

320 
Decision-Making Agents 
 
 
The soundness and completeness theorems for the rational extension can 
easily be established when the corresponding model property is taken into 
account. 
8.13.6 Goals in LAB 
This subsection extends LAB with axioms dealing with the goals of an agent. We 
adopt the following two standard axioms for goals (Cohen and Levesque, 1990; 
Wainer, 1994): 
LAB G 1: 
goal
¬〈
〉⊥ 
The above axiom states that something that is impossible to achieve cannot be a 
goal of a decision-maker. 
LAB G 2: 
(
)
goal F
goal
F
G
goal G
〈
〉
∧〈
〉
→
→〈
〉
 
The above axiom states that all the logical consequences of an agent’s goals are 
also goals. For technical reasons, all worlds compatible with an agent's goals 
must be included in those compatible with the agent's beliefs (Cohen and 
Levesque, 1990). This is summarized in the following axiom: 
LAB G 3: bel F
goal F
〈
〉
→〈
〉
 
Consequently, many redundant goals will be generated due to the presence of the 
above axiom. A goal will be considered achieved (resp. active) in a state if it is 
believed (resp. not believed) in the state. Thus, a goal F generated through the 
above axiom will be considered achieved and therefore not pursued actively. But 
the presence of a goal of this kind is a gentle reminder to the agent for 
establishing the truth of F, not just believing in F.  
The possible world semantic of LAB and the soundness and completeness 
results can be extended incorporating the above modal concepts of goals. See 
(Fox and Das, 2000) for details. 
8.13.7 Dempster-Shafer Interpretation of LAB 
We introduced the Dempster-Shafer theory of belief function earlier in the 
chapter on logical rules. In the Dempster-Shafer interpretation of LAB, we 
consider the dictionary D as the probabilistic dictionary [0,1]  and the modal 
operator 
d
sup
〈
〉 is used to represent the basic probability assignment (BPA) m. 
But an agent may obtain evidence, often inconsistent, from various sources, each 
of which will represent a BPA. 

 
Modal Logics 
321 
 
Continuing with our rainy game example in which Ω  ={On, Delayed, 
Cancelled}, handling each incoming piece of evidence as a belief provides a 
BPA. Suppose 
1
m , 
2
m , and 
3
m  are BPAs for evidence of transport disruption, 
last game of the day, and terrorist threats, respectively. These BPAs have to be 
combined in the order they arrive via a meta-level reasoning outside the scope of 
the logic. So, Dempster’s combination rule for 
1
m  and 
2
m  yields 
12
m , which is 
then combined with 
3
m , giving m.  LAB 2 formalizes this “static” combination 
process at the logic level to yield a combined BPA without updating the 
knowledge base. As an example of  this combination, 
1({
})
m
Delayed
 and 
2({
,
})
m
On Cancelled
 yield a certain amount of belief committed to 
inconsistency, producing 
12({ })
0.54
m
⊥
=
. Moreover, multiple evidences on a 
single assertion are obtained that need to be combined. For example, 
1({
})
m
Delayed
 and 
1({
})
m
Delayed
. The belief function that is obtained from 
the resulting BPA m is then modeled via another modal operator 
d
bel
〈
〉. 
We add a new modal operator 
d
bel
〈
〉 into LAB for every d ∈D  to represent 
the notion of the belief function Bel. Our argument for not having this operator in 
the core LAB is based on the assumption that an agent only finds evidence that 
“supports” (captured via modal operator 
d
sup
〈
〉) a focal element locally without 
thinking about aggregated evidence for the focal element. The agent’s thought 
process for aggregation is a meta-level reasoning, results of which are “degrees 
of belief” (captured via 
d
bel
〈
〉). A model of LAB is extended as a quintuple 
, , ,
,
s
b
W V R R R
, where 
, , ,
s
W V R R  are defined as before. The hyperelation 
b
R  is 
a special case of 
s
R , and the following is the rule for determining truth values of 
formulae involving the modal operator 
d
bel
〈
〉: 
,w
d
bel
F
〈
〉〈
〉
B M
 iff there exists only one 
, ,
w d
'
〈
〉
W
 in 
s
R  such that 
'
W  
uniquely characterizes F 
We also define the modal operator for plausibility in terms of the modal operator 
d
bel
〈
〉 as follows: 
1
d
d
pl
F
bel
F
−
〈
〉
≡〈
〉¬
 
In a pure Bayesian system, the concept of belief amounts to that of 
probability, and, therefore, 
1
d
d
bel
F
bel
F
−
〈
〉
≡〈
〉¬
. Thus, the notions of 
believability and plausibility coincide. Since believability implies plausibility, the 
following property is expected to hold in LAB. 

322 
Decision-Making Agents 
 
 
Proposition 8-13: 
'
d
d
bel
F
pl
F
〈
〉
→〈
〉
, for some d'
d
≥
 
Proof: Suppose 
,w
d
bel
F
〈
〉〈
〉
B M
. Then there exists 
, ,
w d
'
〈
〉
W
 in 
s
R  such that 
,w' F
〈
〉
B M
 for every w' ∈W .  This means for every w'
'
∈
−
W
W , 
,
'
w
F
〈
〉¬
B M
. 
But 
'
−
W
W  uniquely characterizes 
F
¬
. Therefore, for some 
d' , 
,w
d'
bel
F
〈
〉〈
〉¬
B M
, that is, 
,
1 (1
)
w
d'
bel
F
〈
〉
−
−
〈
〉¬
B M
, that is, 
,
1
w
d'
pl
F
〈
〉
−
〈
〉
B M
. But the 
sum of the degrees of beliefs for F  and 
F
¬
 cannot exceed 1, which yields 
1
d
d'
+
≤, that is, 
1
d
d'
≤−
. Therefore, 
,
1
w
d
d'
bel
pl
F
〈
〉
−
〈
〉→〈
〉
B M
.  
 
As per Dempster’s combination rule, the operator ⊗ in LAB 2 yields the 
total support for F as 
1
2
1
2
d
d
d
d
+
−
×
 when F and G are equivalent; otherwise, 
the total support for F
G
∧
, if F
G
∧
 is consistent, is just 
1
2
d
d
×
. In case of 
inconsistency, Dempster’s combination rule excludes the amount of belief 
committed to inconsistency during the normalization process for computing 
degrees of beliefs. Therefore, along the line of axiom LB 3 for bel
〈
〉, we 
consider the following axiom for 
d
bel
〈
〉 which states that there can be no degrees 
of belief for an inconsistency: 
LAB 4: 
,
d
bel
for every d
¬〈
〉⊥
∈D  
Now, if a focal element is believed, then the highest element of the dictionary is 
its total degree of belief. This property yields the following axiom for 
d
bel
〈
〉: 
LAB 5: bel F
bel
F
Δ
〈
〉
→〈
〉
 
Though the degree of belief for F is computed by summing up all the relevant 
supports for F, LAB does not provide any axiom to formalize this summation so 
that it can remain sufficiently generic. Rather, we view the computation of 
degrees of belief as a meta-level reasoning on the underlying logic. The 
following are the set of properties that are possessed by the accessibility relation 
s
R  in the new model: 
LAB MP 4: For every 
1
2
,
,
w w w  in W and 
1
2
,
d d  in D, the relation 
s
R  
satisfies the following conditions: 
• 
, ,
s
w
R
〈
Δ
〉∈
W
. 
• 
If 
, ,
w d
〈
〉
W  is in 
s
R  then 
≠Φ
W
. 
• 
If 
1
1
,
,
w d
〈
〉
W  and 
2
2
,
,
w d
〈
〉
W
 are in 
s
R  such that 
1
2
⊆
W
W , then 
1
2
d
d
≤
. 

 
Modal Logics 
323 
 
The first two conditions are due to LAB 4 and LAB 5 respectively. To 
illustrate the third condition in the context of our rainy game example, suppose 
1
1
{
}
w
=
W
 and 
2
1
3
{
,
}
w w
=
W
. Then 
1
2
⊆
W
W , and 
1
W  and 
2
W  uniquely 
characterize 
b
w
On
∧
 and 
(
)
b
w
On
Cancelled
∧
∨
, respectively (here 
b
w  stands 
for the conjunction of its constituent elements).  If 
1
1
2
2
,
,
,
,
,
s
w d
w d
R
〈
〉〈
〉∈
W
W
 
then 
1
2
,
,
(
)
(
(
))
w
d
b
w
d
b
bel
w
On
bel
w
On
Cancelled
〈
〉
〈
〉
〈
〉
∧
〈
〉
∧
∨
B
B
M
M
 
Since 
(
)
b
b
w
On
w
On
Cancelled
∧
→
∧
∨
, the degrees of belief 
1d  in 
b
w
On
∧
 is 
expected to be not more than the degrees of belief 
2
d  in 
(
)
b
w
On
Cancelled
∧
∨
.  
Unfortunately, it is not possible to axiomatize the third condition in LAB MP 4 
without introducing the dictionary symbols and associated algebra into the logic. 
The soundness and completeness results can be easily extended with respect to 
the models satisfying the rest of the conditions. 
8.14 Further Readings 
The two most popular text books on modal logics are (Chellas, 1980) and 
(Hughes and Cresswell, 1986). A good historical review of modal logics can be 
found in (Goldblatt, 2001). (Blackburn et al., 2006) is a great source book for 
modal logics in general. Lewis and Langofrd’s book (1932) provides a detailed 
foundation of modern day modal logics. Another concise book on various modal 
systems and their interrelationships is by Lemmon (1977).  The soundness and 
completeness results presented in this chapter are largely based on his book. 
Kripke’s paper (1963) is the seminal work on possible world semantics. For an 
elaborate discussion on modal first order logics, see (Hughes and Cresswell, 
1986) and (Fitting and Mendelsohn, 1998). The modal resolution schemes 
presented here are based on Ohlbach’s work (1988). See the authors’ work in 
(Fox and Das, 2000) for a detailed account of a logic along the line of LAB. Two 
books that are worth mentioning from the point of view of reasoning about 
agents’ knowledge and belief are (Fagin et al., 1995) and (Meyer et al., 2001). 

324 
This page intentionally left blank
This page intentionally left blank

325 
Chapter 9 
 
Symbolic Argumentation for 
Decision-Making 
 
 
 
 
Chapters 3, 6, and 8 provided foundations in classical logics, probabilistic 
networks, and modal logics, respectively. All of these foundations were built 
upon to create reasoning for decision-making by intelligent agents. Modal logics 
are extensions to classical logics, and probabilistic or belief networks are an 
efficient computational paradigm for the probabilistic foundation. As shown in 
chapters 5 and 7, sound and practical decision-making systems can be built via 
logic-based rules that incorporates probabilities or mass values as strengths, and 
via influence diagrams based on belief networks. The question then arises on 
how to combine the strengths of each approach, such as the logic-based 
declarative representation of agent knowledge, intuitive probability semantics 
used in everyday language, automatic aggregation of support for decision options 
via belief networks and Dempster’s rule of combination, and the possible world 
concept that parallels decision-making options. This chapter presents one 
approach that combines aspects of classical logics, modal logics, and belief 
networks into an argumentation framework, yielding the P3 (Propositional, 
Probability, and Possibility) model for agent-based decision-making. The 
underlying theoretical foundation of P3 is the logic LAB presented in the last 
section. 
Within an argumentation framework for decision-making, such as the one 
developed in (Fox and Das, 2000) and adopted here, the agent combines 
arguments for and against the decision options at hand and then commits to the 
one with the highest aggregated evidence. In this chapter, we use the syntax of a 
modal propositional logic to represent arguments, and probabilities to represent 
the strength of each argument. Axiomatic inferencing schemes within this type of 
logics are insufficient for aggregating arguments, as the typical aggregation 
process is a meta-level reasoning involving sets of arguments. We propose two 

326 
Decision-Making Agents 
 
 
schemes for aggregating arguments: one that uses the Dempster-Shafer theory of 
belief functions, and the other that uses Bayesian belief networks. The mass 
distribution concept in the theory of belief functions naturally lends itself to the 
consideration of possible worlds. Aggregation via belief networks is performed 
on networks that are “automatically” constructed out of available arguments 
without requiring any additional knowledge. 
Essentially, this aggregation process provides the agent with an overall 
judgment on the suitability of each decision option. Whether the agent accepts 
this judgment from a mechanical, meta-level reasoning process depends on the 
nature of the agent. We shall describe a rational agent as one who accepts the 
judgment and acts and revises its own belief accordingly. 
An argumentation-based decision-making framework like the one described 
here is functionally similar to the classical rule-based framework presented in 
Chapters 4 and 5, with the following exceptions:   
• 
It deals with more expressive knowledge in the form of arguments “for” 
and “against” (as opposed to simple rules which deal only with 
arguments “for”), and can use a variety of dictionaries.  
• 
It incorporates an inference mechanism which is capable of aggregating 
arguments for and against decision options, and is therefore more general 
than simple forward chaining. 
As for an overall comparison with belief network based decision-making 
framework presented in Chapter 6, it is easier to acquire a set of arguments from 
domain experts than construct a belief network. The latter involves a much more 
methodical approach to knowledge elicitation, and is usually much more time 
consuming to establish variables and CPTs. In this respect, a major advantage of 
an argumentation-based framework is that an overall support can be generated to 
help make a decision even with very few arguments, making the framework 
highly flexible. But a typical propagation algorithm for a belief network fails to 
work even if a single entry within a CPT of the network is missing. 
The rest of the chapter is organized as follows. We first provide readers with 
a short background of the argumentation concept. Then we present the domino 
decision-making model underlying P3, along with the model’s knowledge 
representation language for expressing agents’ beliefs and knowledge for making 
decisions. We then describe the belief function and belief network-based 
approaches to aggregation of arguments. 

 
Symbolic Argumentation 
327 
 
9.1 
Toulmin’s Model of Argumentation 
In his book, Toulmin (1958) discussed how difficult it is to cast everyday, 
practical arguments into classical deductive form. He claimed that arguments 
needed to be analyzed using a richer format than the simple if-then form of 
classical logic. He characterizes practical argumentation using the scheme shown 
in Figure 9-1. 
Data
Qualifier, Claim
Warrant
Backing
Rebuttal
The game is 
cancelled
Supports
It is raining heavily
Since
Heavy rain causes the field to be 
flooded, a condition unsuitable 
for the game
Because,
According to the game organizer,
70% of times the games have been 
cancelled due to flooded field
Unless
Special draining system has been 
installed at the field
 
Figure 9-1: Toulmin’s model of argumentation 
As shown in Figure 9-1, Toulmin’s model decomposes an argument into a 
number of constituent elements: 
• 
Claim: The point an agent (or decision-maker) is trying to make 
• 
Data: The facts about a situation relied on to clarify a claim 
• 
Warrant: Statements indicating general ways of arguing 
• 
Backing: Generalizations providing explicit support for an argument 
• 
Qualifier: Phrases showing the confidence an argument confers on a 
claim 
• 
Rebuttal: Acknowledges exceptions or limitations to the argument 

328 
Decision-Making Agents 
 
 
To illustrate, consider an argument (as annotated in Figure 9-1) claiming that 
the game, which was to be held today, has been cancelled. The fact or belief (that 
is, data) on which this claim is made is that there is heavy rain. General 
principles or rules, such as “heavy rain causes the field to be flooded, a condition 
unsuitable for the game,” warrant the argument, based on statistical research 
published by the game organizer, which is the backing. Since the argument is not 
conclusive, we insert the qualifier “supports” in front of the claim, and note the 
possibility that the conclusion may be rebutted on other grounds, such as 
installation of a new drainage system at the game venue, or hearing radio 
commentary (not shown in the picture) that clarifies the game status. 
Our approach is to transform Toulmin’s work to a more formal setting. We 
deal with the concepts of warrant and rebuttal, but as very simple propositional 
arguments for and against. We do not deal with first-order sentences that are 
more suitable for representing backings in Toulmin’s model; rather, we introduce 
the use of a single qualifier called support. 
9.2 
Domino Decision-Making Model for P3  
This section develops a generic architecture for P3’s argumentation based 
decision-making process as presented above via Toulmin’s model. Continuing 
with our rainy game example, Figure 9-2 shows the underlying process which 
starts when the decision maker observes heavy rain while preparing to go to town 
for the game. 
Determine
Game
Status
Decide
Activity
Heavy Rain
Withdraw
Money
Travel to
Town
shop
On
Cancelled
Delayed
Shopping
Movie
 
Figure 9-2: Decision-making flow 

 
Symbolic Argumentation 
329 
 
The newly discovered weather status then becomes the decision maker’s 
belief. Given that the decision maker “believes” that it is raining heavily, it raises 
a “goal” of finding the status of the game. It then infers from its common sense 
knowledge that there are three possible or “candidate” states of the game, On, 
Cancelled, and Delayed, and constructs arguments for and against these 
alternatives. These arguments use other beliefs of the agent, based on 
observations such as transport availability and radio commentary. In this case, 
the balance of “argument” is in favor of the game being cancelled, and this 
conclusion is added into the decision maker’s database of beliefs. 
Given this new belief regarding the cancelled status of the game, a new goal 
is raised, that is,  to “plan” for alternative activities. As in determining the status 
of the game, the agent first enumerates the options. In this case there are two 
options for alternative activities, shopping and going to a movie, and the 
decision-maker once again constructs arguments for the alternatives, taking into 
account the crowded holiday season, transport, and costs, and recommends going 
shopping as the most preferred alternate activity on the basis of the arguments. 
The adoption of the shopping “plan” leads to an appropriate scheduling of 
“actions” involved in shopping, such as withdrawing money, traveling to town, 
and going to stores. The effects of these actions are recorded in the decision-
maker’s database, which may lead to further goals, and so on. 
Figure 9-3, the Domino model, captures graphically the decision-making 
framework, where the chain of arrows in the figure represents the previous 
example decision-making and planning process. Within our proposed framework, 
a decision schema has several component parts: an evoking situation, a goal, one 
or more candidates and corresponding arguments for and against, one or more 
commitment rules, and alternative plans and actions. Please note that temporal 
planning is out of the scope of this book. See (Fox and Das, 2000) for 
information on how to deal with a scheduled plan that is committed.  
A situation describes, as a Boolean expression on the database of beliefs, the 
situation or event that initiates decision-making. For example, a belief that an 
abnormality (for example, heavy rain) is present may lead to a choice between 
alternative possible causes or effects of the abnormality. 

330 
Decision-Making Agents 
 
 
Plan
Arguments
Action
Situation
Candidates
Goals
 
Figure 9-3: Domino process view of the example 
A goal is raised as soon as the evoking situation occurs. In particular, the 
belief that an abnormality is present may raise the goal of determining its cause 
or effects. For example, if it is raining heavily, one of the possible effects is the 
cancellation of the game, and therefore the goal is to determine game status. Or, 
if there is no radio commentary, we would again want to determine the status of 
the game, as its cancellation causes no radio commentary. Typically, a goal is 
represented by a state that the decision-maker tries to bring about. 
Candidates are a set of alternative decision options, such as On, Cancelled, 
and Delayed. In principle, the set of candidates may be defined extensionally (as 
a set of propositions) or intentionally (by rules), but we only consider the former 
case here. 
Arguments are modal-propositional rules that define the arguments 
appropriate for choosing between candidates for the decision. Argument schemas 
are typically concerned with evidence when the decision involves competing 
hypotheses (beliefs), and with preferences and values when the decision is 
concerned with actions or plans. 
Commitment rules  (not illustrated in Figure 9-3) define the conditions under 
which the decision may be recommended, or taken autonomously, by the 
decision-maker. It may include logical and/or numerical conditions on the 
argument and belief databases. 
9.3 
Knowledge Representation Syntax of P3 
The concept of the Domino model-based decision scheme and its components is 
captured in a high-level declarative syntax. Figure 9-4 gives the decision 
construct (actual syntax of input to the parser) representing the decision circle 
Determine Game Status in Figure 9-2 or the left box of the Domino model in 

 
Symbolic Argumentation 
331 
 
Figure 9-3. All decisions have an evoking situation, which, if the decision-maker 
believes it to be true, raises the corresponding goal. The three possible paths from 
the decision circle go to the following three alternative pathways: On, Cancelled, 
and Delayed. These candidates are represented explicitly in the decision 
construct. The arguments and commitments within a decision construct are also 
represented explicitly. 
decision:: game_status
situation
heavy_rain
goal
determine_game_status
candidates
on;
cancelled;
delayed
arguments
heavy_rain => support(not on, 0.7);
terrorist_attack_threat => support(not on, 1.0);
players_injury => support(cancelled, 0.8);
players_union_strike => support(cancelled, 1.0);
club_financial_crisis => support(not cancelled, 0.6);
transport_disruption => support(delayed, 0.7);
commits
netsupport(X, U) & netsupport(Y, V) &
netsupport(Z, W) & U > V & U > W => add(X).
 
Figure 9-4: Example decision construct 
The decimal number in an argument represents the probabilistic measure of 
support given by the argument to the decision candidate. The basic idea is that an 
argument is a reason to believe something, or a reason to act in some way, and an 
argument schema is a rule for applying such reasons during decision-making. 
The more arguments there are for a candidate belief or action, then the more a 
decision-maker is justified in committing to it. The aggregation function can be a 
simple “weighing of pros and cons” (such as the netsupport function in the 
example of Figure 9-4), but it represents a family of more or less sophisticated 
functions by which we may assess the merit of alternative candidates based on 
the arguments about them.  
In general, an argument schema is like an ordinary inference rule with 
support(〈candidate〉, 〈sign〉) 

332 
Decision-Making Agents 
 
 
as its consequent, where 〈sign〉 is drawn from a set called a dictionary. 〈sign〉 
represents, loosely, the confidence that the inference confers on the candidate. 
The dictionary may be strictly quantitative  (for example, the numbers in the 
[0,1] interval) or qualitative, such as the symbols {+, -} or {high, medium, low}. 
Here we are dealing with probabilistic arguments, so 〈sign〉 is drawn from the 
probability dictionary [0,1]. An example argument from the decision construct in 
Figure 9-4 is 
transport_disruption ⇒ support(delayed, 0.7) 
where 〈candidate〉 is delayed. Informally, the argument states that if there 
is transport disruption then there is a 70% chance that the game will be delayed. 
The rest of the arguments of the decision construct provide support for and 
against the decision options based on the evidence of radio commentary, lack of 
radio commentary, terrorist attack threat, player injury, a players’ union strike, 
and the hosting club’s financial condition. A knowledge base for the decision-
maker consists of a set of definitions of this and various other decision tasks. For 
the Dempster-Shafer theory to be applicable, one acceptable interpretation of 0.7 
is that it is the confidence on the source from which the information about the 
transport disruption is obtained. Such a source could be a human observer or a 
sensor placed on a highway. More on this interpretation is discussed in the 
following section.  
A decision-maker considers the decision game_status in Figure 9-4 for 
activation when the belief heavy_rain is added to the database.  When the 
decision-maker detects this, it checks whether any candidate has already been 
committed. 
If 
not, 
the 
decision 
will 
be 
activated 
and 
the 
goal 
determine_game_status is raised; otherwise no action is taken. While the 
goal is raised, further information about the situation (such as the weather) can be 
examined to determine whether the premises of any argument schemas are 
instantiated. 
A commitment rule is like an ordinary rule with one of 
add(〈property〉) 
schedule(〈plan〉) 
as its consequent. The former adds a new belief to the knowledge base, and 
the latter causes a plan to be scheduled (see Figure 9-3). The scheduled plan is 
hierarchically broken down into actions via plan constructs (Please note that 
temporal planning is out of the scope of this book.). The generated actions from a 

 
Symbolic Argumentation 
333 
 
plan are executed in the environment, thus changing the state of the environment 
and the decision-maker’s beliefs. 
When a decision is in progress, as additional arguments become valid, the 
decision’s commitment rules are evaluated to determine whether it is justified to 
select a candidate. A commitment rule will often make use of an aggregation 
function, such as netsupport, but this is not mandatory. The function 
evaluates collections of arguments for and against any candidate to yield an 
overall assessment of confidence and establish an ordering over the set of 
candidates; this ordering may be based on qualitative criteria or on quantitative 
assessment of the strength of the arguments. This function has the form: 
netsupport(〈candidate〉, 〈support〉) 
Later in this section, we implement the netsupport function based on two 
approaches: Dempste’s rule of combination, and an evidence propagation 
algorithm in belief networks. 
decision:: alternative_activity
situation
cancelled
goal
decide_alternative_ activity
candidates
shopping;
movie
arguments
holiday_season =>
support(no shopping, 0.8);
…
commits
… => schedule(shopping).
 
Figure 9-5: Example decision construct 
A decision-maker considers the decision alternative_activity in 
Figure 9-5 for activation when the belief about the game cancellation is added to 
the database. Figure 9-6 illustrates mapping these decision and plan constructs 
into the Domino model. 

334 
Decision-Making Agents 
 
 
heavy_rain
determine_game_status
• on
• cancelled
• delayed
heavy_rain => support(not on, 0.7);
...
cancelled
decide_alternate_activity
•shopping
•movie
holiday_season =>
support(not shopping,’+’)
...
shopping
• withdraw_money
• travel_to_town
• shop
Plan
Arguments
Action
Situation
Candidates
Goals
 
Figure 9-6: Decision and plan constructs mapped into Domino model 
9.4 
Formalization of P3 via LAB 
In this section, we provide some example sentences in LAB that are translations 
of the decision construct shown in Figure 9-4. The situation and goal portion in 
the decision game_status in Figure 9-4 is translated to the following modal 
rule: 
bel Transport Disruption
goal Determine Game Status
〈
〉
→〈
〉
 
The above LAB sentence states that if Transport Disruption is believed, then a 
goal is Determine Game Status. A goal is considered achieved as soon as it 
becomes true. In the context of the decision game_status, this is reflected in 
the following formulae: 
(
)
(
)
(
)
bel On
Cancelled
Delayed
bel Determine Game Status
bel Cancelled
On
Delayed
bel Determine Game Status
bel
Postponed
On
Delayed
bel Determine Game Status
〈
〉
∧¬
∧¬
→〈
〉
〈
〉
∧¬
∧¬
→〈
〉
〈
〉
∧¬
∧¬
→〈
〉
 
The first of these three sentences states that if it is believed that the game is on, 
and neither cancelled nor delayed, then Determine Game Status is believed. In 
other words, the earlier goal Determine Game Status is considered achieved upon 
believing that the game is on. The LAB representations for the arguments in the 
diagnosis decision are: 

 
Symbolic Argumentation 
335 
 
0.7
0.8
0.6
(
)
(
)
bel Heavy Rain
sup
On
bel Terrorist AttackThreat
On
bel Players Injury
sup
Cancelled
bel
PlayersUnionStrike
Cancelled
bel ClubFinancialCrisis
sup
Cancelled
bel Transport Disruption
s
〈
〉
→〈
〉¬
〈
〉
→¬
〈
〉
→〈
〉¬
〈
〉
→
〈
〉
→〈
〉¬
〈
〉
→〈
0.7
up
Delayed
〉
 
This 
straightforward 
translation 
mechanism 
from 
the 
knowledge 
representation syntax of P3 to the syntax of the LAB logic of belief provides a 
strong theoretical foundation for P3. Moreover, necessary derivations in P3 for 
answering queries can be carried out via inferencing in LAB. This is the path we 
took to develop a prototype implementation of the Domino model. 
9.5 
Aggregation via Dempster-Shafer Theory 
In order to illustrate the Dempster-Shafer theory in the context of our example, 
we consider only the following subset of three arguments from the set of 
arguments presented in the last section: 
0.7
0.6
0.8
bel Heavy Rain
sup
On
bel ClubFinancialCrisis
sup
Cancelled
bel Players Injury
sup
Cancelled
〈
〉
→〈
〉¬
〈
〉
→〈
〉¬
〈
〉
→〈
〉¬
 
Note that Dempster-Shafer theory requires that evidences to be combined are 
independent. In this set of arguments, the potential usable evidences (rainy 
condition, financial situation, and player injury) are independent. We excluded 
the argument relating transport disruption because it is causally related to the 
heavy rain condition. We have also excluded the definitive arguments (with 
support 1.0) from our consideration to make our evidence aggregation process 
illustration more interesting. The frame-of-discernment Ω  in this example is 
{On, Delayed, Cancelled}. 
The first evidence corresponds to the decision-maker’s observation of heavy 
rain while preparing to go to town for the game. The decision maker has 0.7 
subjective probability for the game not being on (that is, cancelled or delayed) 
given this rainy weather. Therefore, evidence of heavy rain alone justifies a 0.7 
degree of belief that the game is not on, but only a zero degree of belief (not 0.3) 
that the game is on. This zero belief does not mean that the decision-maker is 
sure that the game is not on (as a zero probability would), but states that evidence 
of heavy rain gives the decision-maker no reason to believe that the game is on. 

336 
Decision-Making Agents 
 
 
The values 0.7 and 0 together constitute a belief function; they can be thought of 
as lower bounds on probabilities 
(
|
)
p Cancelled or Delayed Heavy Rain and 
( (
) |
)
p
Cancelled or Delayed
Heavy Rain
¬
, respectively. 
The evidence suggest the focal elements are {
,
}
Cancelled Delayed  and Ω , 
and 
1({
,
})
0.7
m
Cancelled Delayed
=
. We know nothing about the remaining 
probability so it is allocated to the whole frame of discernment as 
1( )
0.3
m Ω =
. 
The decision-maker also believes that the current financial situation of the club is 
bad, resulting in 0.6 subjective probability that the game will not be cancelled in 
this situation. The new evidence provides the focal elements {
,
}
On Delayed  and 
Ω , with 
2({
,
})
0.6
m
On Delayed
=
. The remaining probability, as before, is 
allocated to the whole frame of discernment as 
2( )
0.4
m Ω =
. Because the current 
financial situation and transport disruption are independent of each other, 
Dempster’s rule can be used to combine the masses as in Table 9-1. 
Table 9-1 
Can
Cancelled
Del
Delayed
−
−
 
2({
,
})
0.6
m
On Del
=
 
2( )
0.4
m Ω =
 
1({
,
})
0.7
m
Can Del
=
 
1,2({
})
0.42
m
Del
=
 
1,2({
,
})
0.28
m
Can Del
=
 
1( )
0.3
m Ω =
 
1,2({
,
})
0.18
m
On Del
=
 
1,2( )
0.12
m
Ω =
 
The basic probability assignments 
1
m  and 
2
m  are different but consistent, 
and therefore the degrees of belief in both {
,
}
Cancelled Delayed  and 
{
,
}
On Delayed being true (that is, the game is delayed) is the product of 
1({
,
})
m
Cancelled Delayed
 and 
2({
,
})
m
On Delayed
, that is, 0.42. The revised 
focal elements and their beliefs and plausibilities are shown in Table 9-2. 
Table 9-2 
Focal Element (A) 
( )
Bel A
( )
Pl A
{
}
Delayed  
0.42 
1.0 
{
,
}
On Delayed  
0.60 
1.0 
{
,
}
Cancelled Delayed  
0.70 
1.0 
Ω  
1.0 
1.0 
Finally, the player injury situation suggests the focal elements are 
{
}
Cancelled  and Ω , so that 
3({
})
0.8
m
Cancelled
=
, 
3( )
0.2
m Ω =
. Dempster’s 

 
Symbolic Argumentation 
337 
 
rule of combination applies as shown in Table 9-3, but with one modification. 
When the evidence is inconsistent, their products of masses are assigned to a 
single measure of inconsistency, say k.  
Table 9-3 
Can
Cancelled
Del
Delayed
−
−
 
3({
})
0.8
m
Can
=
 
3( )
0.2
m Ω =
 
1,2({
})
0.42
m
Del
=
 
0.336
k =
 
({
})
0.084
m
Del
=
 
1,2({
,
})
0.18
m
On Del
=
 
0.144
k =
 
({
,
})
0.036
m On Del
=
 
1,2({
,
})
0.28
m
Can Del
=
 
({
})
0.224
m Can
=
 
({
,
})
0.056
m Can Del
=
 
1,2( )
0.12
m
Ω =
 
({
})
0.096
m Can
=
 
( )
0.024
m Ω =
 
The total mass of evidence assigned to inconsistency k is 0.336 + 0.144 = 
0.48. The normalizing factor is 1
0.52
k
−
=
. The resulting masses of evidence are 
as follows: 
({
})
(0.224
0.096)/0.52
0.62
({
})
0.084/0.52
0.16
({
,
})
0.036/0.52
0.07
({
,
})
0.056/0.52
0.11
( )
0.024/0.52
0.04
m Cancelled
m
Delayed
m On Delayed
m Cancelled Delayed
m
=
+
=
=
=
=
=
=
=
Ω =
=
 
The revised focal elements and their beliefs and plausibilities are shown in Table 
9-4. 
Table 9-4 
Focal Element (A) 
( )
Bel A
( )
Pl A
{
}
Cancelled  
0.62 
0.77 
{
}
Delayed  
0.16 
0.38 
{
,
}
On Delayed  
0.23 
0.38 
{
,
}
Cancelled Delayed  
0.89 
1.0 
Ω  
1.0 
1.0 

338 
Decision-Making Agents 
 
 
We consider two examples to illustrate two special cases for evidence 
aggregation. Hypothetically, consider the case when the set of focal elements of 
the basic probability distribution 
2
m is exactly the same as 
1
m . The evidence 
combination table is shown in Table 9-5.  
Table 9-5 
Can
Cancelled
Del
Delayed
−
−
 
2({
,
})
0.6
m
Can Del
=
 
2( )
0.4
m Ω =
 
1({
,
})
0.7
m
Can Del
=
 
1,2({
,
})
0.42
m
Can Del
=
 
1,2({
,
})
0.28
m
Can Del
=
 
1( )
0.3
m Ω =
 
1,2({
,
})
0.18
m
Can Del
=
 
1,2( )
0.12
m
Ω =
 
Now,  
({
,
})
0.42
0.18
0.28
0.88
0.6
0.7
0.6
0.7
Bel
Cancelled Delayed
=
+
+
=
=
+
−
×
. 
In general, when two mass distributions 
1
m  and 
2
m  agree on focal elements, the 
combined degree of belief on a common focal element is 
1
2
1
2
p
p
p
p
+
−
×
, 
where 
1p  and 
2p  are evidences on the focal element by the two distributions. 
This formula coincides with the noisy-or technique in Bayesian belief networks 
for combining probabilities of variables that have certain properties. 
As opposed to agreeing on focal elements, if 
2
m  is contradictory to 
1
m  then 
an example evidence combination is shown in Table 9-6. 
Table 9-6 
Can
Cancelled
Del
Delayed
−
−
 
2({
})
0.6
m
On
=
 
2( )
0.4
m Ω =
 
1({
,
})
0.7
m
Can Del
=
 
0.42
k =
 
1,2({
,
})
0.28
m
Can Del
=
 
1( )
0.3
m Ω =
 
1,2(
)
0.18
m
On =
 
1,2( )
0.12
m
Ω =
 
Now,  
({
,
})
0.28/(1
0.42)
0.7(1
0.6)/(1
0.42)
Bel
Cancelled Delayed
=
−
=
−
−
. 
In general, when two mass distributions 
1
m  and 
2
m  are contradictory, then the 
combined degree of belief on the focal element for 
1
m  is 
1
2
1
2
(1
)/(1
)
p
p
p
p
−
−
×
 
and the combined degree of belief on the focal element for 
2
m  is 

 
Symbolic Argumentation 
339 
 
2
1
1
2
(1
)/(1
)
p
p
p
p
−
−
×
, where 
1p  and 
2p  are evidences on the focal element by 
the two distributions. 
9.6 
Aggregation via Bayesian Belief Networks 
In order to illustrate the aggregation process in the context of our example, we 
consider the following five arguments to ensure that the potential evidences that 
generate support for individual decision options are independent: 
0.8
0.6
0.75
0.9
0.7
bel Players Injury
sup
Cancelled
bel Heavy Rain
sup
Cancelled
bel Heavy Rain
sup
Delayed
bel Transport Disruption
sup
Delayed
bel Terrorist AttackThreat
sup
Delayed
〈
〉
→〈
〉
〈
〉
→〈
〉
〈
〉
→〈
〉
〈
〉
→〈
〉
〈
〉
→〈
〉
 
The three decision options are On, Cancelled, and Delayed. Due to differences in 
the type of knowledge represented and in the formalism used to represent 
uncertainty, much of the knowledge needed to build an equivalent belief network 
for aggregation could not be extracted from the above set of arguments. In our 
approach, we will be able to automatically extract the network structure fully 
without requiring additional knowledge under the assumption that the conditions 
(such as Player Injury or Heavy Rain) based on which arguments for the decision 
options are generated are independent. 
We first construct fragments of networks using the arguments relevant to the 
decision-making task at hand. Note that, given a network fragment with a 
variable, and its parents and CPT, the fragment can be equivalently viewed as a 
set of arguments. For example, consider the network fragment in Figure 9-7, 
which states that player injury and rain together can determine the status of the 
game. 
Each column of the CPT yields an argument for and an argument against a 
state of the variable Game. For example, if there is player injury and it rains, then 
there is an argument for a game with support 0.05. 
0.05
(
)
bel
Injury
Rain
sup
Game
〈
〉
∧
→〈
〉
 

340 
Decision-Making Agents 
 
 
Injury
Rain
Game
yes
no
yes
no
yes
no
0.05
0.20
0.70
0.1
0.95
0.80
0.30
0.0
⎡
⎤
⎢
⎥
⎣
⎦
 
Figure 9-7: Example belief network fragment  
Since the arguments are probabilistic, there will be another argument 
(corresponding to the argument above) which states that if there is player injury 
and it rains, then there is an argument against the game with support 1 − 0.05, 
that is, 0.95, yielding the following: 
0.95
(
)
bel
Injury
Rain
sup
Game
〈
〉
∧
→〈
〉¬
 
The rest of the entries of the CPT can be translated to arguments in a similar 
manner. 
Continuing with our illustration of the network construction process from the 
considered set of five arguments, each set of arguments for a decision option is 
translated to a network fragment containing a random variable corresponding to 
the option as the child node with as many parent nodes as the number of 
arguments. The parent nodes are formed using the antecedents in the arguments. 
Therefore, the set of five arguments in our example for the two decision options 
Cancelled and Delayed are translated to the two fragments shown in Figure 9-8. 
Players Injury
Heavy Rain
Cancelled
Arguments
Heavy Rain
Transport 
Disruption
Terrorist 
Attack 
Threat
Delayed
 
Figure 9-8: Belief network fragments created by converting arguments 
Each of the nodes is binary with states yes and no. Since a particular decision 
option may occur in consequents of many arguments, their corresponding nodes 

 
Symbolic Argumentation 
341 
 
(for example, Heavy Rain) may be shared among the network fragments. The 
common nodes are collapsed under such circumstances. Figure 9-9 shows the 
transformed belief network along with the CPTs. 
Players Injury
Cancelled
Noisy Or
Heavy Rain
Transport 
Disruption
Terrorist 
Attack 
Threat
Delayed
0.92
0.8
0.6
0.5
0.08
0.2
0.4
0.5
⎡
⎤
⎢
⎥
⎣
⎦
0.9925
0.975
0.925
0.75
0.97
0.9
0.7
0
0.0075
0.025
0.075
0.25
0.03
0.1
0.3
1
⎡
⎤
⎢
⎥
⎣
⎦
0.05
0.95
⎡
⎤
⎢
⎥
⎣
⎦
0.1
0.9
⎡
⎤
⎢
⎥
⎣
⎦
0.15
0.85
⎡
⎤
⎢
⎥
⎣
⎦
0.01
0.99
⎡
⎤
⎢
⎥
⎣
⎦
 
Figure 9-9: Belief network fragments along with CPTs 
The entries for the CPTs are computed by applying the noisy-or technique (see 
the chapter on belief networks). For example, the following entry in the CPT 
comes directly from the argument: 
(
 |  
,
 
,
)
 0.75
0.9
0.75 0.9= 0.975
p Delayed
yes
Heavy Rain
yes Transport Disruption
yes
Terrorist AttackThreat
no
=
=
=
=
=
+
−
×
 
where the arguments for the decision option Delayed provide the following 
probabilities:  
(
|  
)
0.75
(
|
 
)
0.9
p Delayed
yes
Heavy Rain
yes
p Delayed
yes Transport Disruption
yes
=
=
=
=
=
=
 
The prior probabilities for the parent nodes are chosen arbitrarily. These prior 
probabilities can be assumed by default to be equally distributed. 
Now that we have network fragments for arguments for and against 
individual decision options, we need to combine these arguments to rank the 
decision options. Since evidence for and against each decision option was 
combined individually, they may not necessarily sum to one, which is necessary 
as we assume that decision options are mutually exclusive. Therefore, we need to 
perform a normalization step. For this, we create a random variable with the 
states corresponding to the decision options for the task at hand. In the context of 

342 
Decision-Making Agents 
 
 
our example, we create a random variable called Game with three states: on, 
cancelled, and delayed. The variable has three parents corresponding to the three 
decision options. The decision options are ranked based on the aggregation of 
arguments for and against the decision options; the values of the CPT are 
determined accordingly. For example, if we have aggregated evidence for each of 
the three decision options On, Cancelled, and Delayed, then the probability 
distribution of the variable Game is evenly distributed as follows: 
(
 
 
|
, 
, 
) 
 0.33 
(
 
 
|
, 
, 
) 
 0.33
(
 
 
|
, 
, 
) 
 0.33
p Game
on On Cancelled Delayed
p Game
cancelled On Cancelled Delayed
p Game
delayed On Cancelled Delayed
=
=
=
=
=
=
 
Note that we have the same probability distribution as when we aggregated 
evidence against each of the three decision options (that is, not On, not 
Cancelled, and not Delayed). On the other hand, for example, if we have 
aggregated evidence for each of the two decision options On and Cancelled, and 
aggregated evidence against the decision option Delayed, then the probability 
distribution on the states of the variable Game is as follows: 
(
 
 
|
, 
, 
 
) 
 0.5 
(
 
 
|
, 
, 
 
)
0.5
(
 
 
|
, 
, 
 
)
0.0
p Game
on On Cancelled not Delayed
p Game
cancelled On Cancelled not Delayed
p Game
delayed On Cancelled not Delayed
=
=
=
=
=
=
 
Figure 9-10 illustrates the CPT for Game. 
On
Cancelled
Delayed
Game
on
cancelled
delayed
0.33
0.5
0.5
1
0
0
0
0.33
0.33
0.5
0
0
0.5
1
0
0.33
0.33
0
0.5
0
0.5
0
1
0.33
⎡
⎤
⎢
⎥
⎢
⎥
⎢
⎥
⎣
⎦
 
Figure 9-10: Belief network fragment for aggregating arguments for and against 
decision options 
Consider the case when two variables (not corresponding to decision options) 
are causally connected by two rules in the knowledge base as follows: 

 
Symbolic Argumentation 
343 
 
0.95
0.9
bel PowerOutage
sup
Transport Disruption
bel
PowerOutage
sup
Transport Disruption
〈
〉
→〈
〉
〈
〉¬
→〈
〉¬
 
These two yield the following two conditional probabilities: 
(
 
 |  
) 
 0.95
(
 
 
 |  
 
) 
 0.90
P Transport Disruption
PowerOutage
P not Transport Disruption
not PowerOutage
=
=
 
Thus, the rules together can be transformed to a network fragment along with a 
complete CPT as shown on the left hand side in Figure 9-11. 
Power Outage
Transport 
Disruption
On
Radio 
Commentary
0.95
0.1
0.05
0.9
⎡
⎤
⎢
⎥
⎣
⎦
1
0
0
1
⎡
⎤
⎢
⎥
⎣
⎦
 
Figure 9-11: Belief network fragments corresponding to domain knowledge 
Consider the case when a decision option causally influences another 
variable as follows:  
(
)
bel On
RadioCommentary
〈
〉
→
 
The rule is an absolute one in the sense that there will always be radio 
commentary if the game is on. This rule is translated to the belief network 
fragment shown on the right hand side in Figure 9-11. In this fragment, evidence 
of radio commentary will increase the belief in that the game is on. The belief 
network formalism produces this additional diagnostic information though it is 
not explicitly specified in the above argument from which the fragment is 
constructed. We can assume some form of completed definition (see predicate 
completion in the logic programming chapter) of the variable RadioCommentary 
to correspond to this diagnostic reasoning. 
Figure 9-12 shows the combined network for aggregating the arguments. 
Such a network has three blocks: the Argument Block, the Aggregation Block, 
and the Rule Block. The Argument Block (shown in Figure 9-9) is constructed 
out of the network fragments obtained by translating the arguments in the 
decision construct. The Aggregation Block (shown in Figure 9-10) implements 

344 
Decision-Making Agents 
 
 
the normalization step. The rest of the network, the Rule Block, is based on the 
rules from the underlying knowledge base. 
Power Outage
Players Injury
Heavy Rain
Transport 
Disruption
Terrorist 
Attack 
Threat
Cancelled
On
Delayed
Game
Radio
Commentary
Rule
Rule
Noisy Or
Normalization
Argument
 
Figure 9-12: Combined belief network for argument aggregation 
In the absence of any evidence, no arguments are generated and the a priori 
probabilities of the decision options are as follows: 
(
 
 
) 
 0.81
(
 
 
) 
 0.07
(
 
 
) 
 0.12
P Game
on
P Game
cancelled
P Game
delayed
=
=
=
=
=
=
 
No evidence in the network has been posted at this stage, not even for any prior 
beliefs on the variables. Now, given that there is transport disruption and heavy 
rain, the network ranks the decision options based on the following posterior 
probabilities: 
(
 
 
 |  
 
, 
)
 0.36
(
 
 
 |  
 
, 
) 
 0.22
(
 
 
 |  
 
, 
)  
 0.42
P Game
on
Transport Disruption Heavy Rain
P Game
cancelled
Transport Disruption Heavy Rain
P Game
delayed
Transport Disruption Heavy Rain
=
=
=
=
=
=
 
The chances for game to be on have gone down dramatically. If we now receive 
information the rain has almost subsided (evidence (0.1, 0.9) for the variable 
Heavy Rain) then the network ranks the decision options as follows: 

 
Symbolic Argumentation 
345 
 
(
|
,
)
0.49
(
|
,
)
0.02
(
|
,
)
0.49
P Game
on Disruption Heavy Rain
P Game
cancelled Disruption Heavy Rain
P Game
delayed Disruption Heavy Rain
=
=
=
=
=
=
 
The dilemma occurs between the two decision options On and Delayed. Based on 
the above probability distribution, the decision-maker may decide that the game 
will not be cancelled.  
Though the propagation of evidence in the above aggregation process is 
always forward, the network in Figure 9-12 can also be used for speculative 
computations via diagnostic reasoning. For example, one can set the state of the 
variable Game as cancelled and then examine the likely state of the other 
variables if the game was to be cancelled. 
9.7 
Further Readings 
More details of the Domino model, the knowledge representation language, and 
the underlying theoretical foundation via LAB can be found in (Fox and Das, 
2000) and also in (Das, 2006). See (Das, 2007) for an extension of the Domino 
model.

This page intentionally left blank
This page intentionally left blank

347 
References 
 
AGENTS (1997-2001). Proceedings of the International Conferences on Autonomous Agents (1st – 
Marina del Rey, California; 2nd – Minneapolis, Minnesota; 3rd – Seattle, Washington; 4th – 
Barcelona, Spain; 5th – Montreal, Canada), ACM Press. 
AAMAS (2002-2006). Proceedings of the International Joint Conferences on Autonomous Agents 
and Multi-Agent Systems (1st – Bologna, Italy; 2nd – Melbourne, Australia; 3rd – New York, 
NY; 4th –  The Netherlands; 5th – Hakodate, Japan), ACM Press. 
Alechina, N. (1994). “Logics with probabilistic operators.” ILLC Research Report CT-94-11, 
University of Amsterdam. 
Apt, K. R. and Van Emden, M. H. (1982). “Contributions to the theory of logic programming.” 
Journal of the Association for Computing Machinery, 29, pp. 841−862. 
Bacchus, F. (1990). Representing and Reasoning with Probabilistic Knowledge. MIT Press. 
Barbuti, R. and Martelli, M. (1986). “Completeness of the SLDNF-resolution for a class of logic 
programs.” In Proceedings of the 3rd International Conference on Logic Programming 
(London, July), pp. 600−614. 
Becker, 
O. 
(1930). 
“Zur 
Logik 
der 
Modalitaten.” 
Jahrbuch 
fur 
Philosophie 
und 
Phanomenologische Forschung, Vol. 11, pp. 497-548. 
Blackburn, P., De Rijke, M., and Venema, Y. (2001). Modal Logic. Cambridge Tracts in 
Theoretical Computer Science, No 53. Cambridge, England: Cambridge University Press. 
Blackburn, P., Wolter, F., and van Benthem, J. (eds.) (2006). Handbook of Modal Logic, Elsevier 
Science & Technology Books. 
Bradley, R. and Swartz N. (1979). Possible Worlds: An Introduction to Logic and Its Philosophy, 
Hackett, Indianapolis, IN. 
Bratko, I. (2000). Prolog Programming for Artificial Intelligence, 3rd Edition. Wokingham, 
England: Addison-Wesley. 
Cavedon, L. and Lloyd, J. W. (1989). “A Completeness theorem for SLDNF-resolution.” Journal 
of Logic Programming, 7, pp. 177−191. 
Chang, C. L. and Lee, R. C. T. (1973). Symbolic Logic and Mechanical Theorem Proving. New 
York: Academic Press. 
Chatalic, P. and Froidevaux, C. (1992). “Lattice-based graded logic: a multimodal approach.” 
Proceedings of the Conference on Uncertainty in Artificial Intelligence, pp. 33-40. 
Chellas, B. F. (1980). Modal Logic: An Introduction. Cambridge. Cambridge, England: Cambridge 
University Press. 
Chung, K. L. (2000). A Course in Probability Theory, Academic Press. 

348 
Decision-Making Agents 
 
 
Clark, K. L. (1978). “Negation as failure.” In Logic and Databases, H. Gallaire and J. Minker 
(eds.), New York: Plenum Press, pp. 293−322. 
Clocksin, W. F. and Mellish, C. S. (2003). Programming in Prolog, 5th edition. Berlin, Germany: 
Springer-Verlag. 
Cohen, P. R. and Levesque, H. J. (1990). “Intention is choice with commitment.” Artificial 
Intelligence, 42, 13−61. 
Cook, S. A. (1971). “The complexity of theorem proving procedures.” Proceedings of the Third 
Annual ACM Symposium on the Theory of Computing, ACM, New York, 151−158 
Cooper, G. (1990), “The computational complexity of probabilistic inference using Bayesian belief 
networks.” Artificial Intelligence, 42. 
Copi, I. M. (1979). Symbolic Logic. New York: Macmillan. 
Dantsin, E., Eiter, T., Gottlob, G., and Voronkov, A. (2001). “Complexity and Expressive Power of 
Logic Programming.” ACM Computing Surveys, 33(3), pp. 374–425. 
Das, S. K. (1992). Deductive Databases and Logic Programming. Wokingham, England: Addison-
Wesley. 
Das, S., Fox, J., Elsdon, D., and Hammond, P. (1997). “A flexible architecture for autonomous 
agents”, Journal of Experimental and Theoretical Artificial Intelligence, 9(4): 407−440 
Das, S. and Grecu, D. (2000). “COGENT: Cognitive agent to amplify human perception and 
cognition.” Proceedings of the 4th International Conference On Autonomous Agents, 
Barcelona, June. 
Das, S., Shuster, K., and Wu, C. (2002). “ACQUIRE: Agent-based Complex QUery and 
Information Retrieval Engine,” Proceedings of the 1st International Joint Conference on 
Autonomous Agents and Multi-Agent Systems, Bologna, Italy. 
Das, S. (2005). “Symbolic Argumentation for Decision Making under Uncertainty,” Proceedings of 
the 8th International Conference on Information Fusion, Philadelphia, PA.  
Das, S. (2007). “Envelope of Human Cognition for Battlefield Information Processing Agents,” 
Proceedings of the 10th International Conference on Information Fusion, Quebec, Canada.  
Davis, M. and Putnam, H. (1960). “A computing procedure for quantification theory.” Journal of 
the Association for Computing Machinery, 7, 201−215. 
Deo, N. (1974). Graph Theory with Applications to Engineering and Computer Science, Prentice-
Hall. 
Dubois, D. and Prade, H. (1988). Possibility Theory. Plenum Press, New York. 
Fagin, R. 1988. “Belief, Awareness, and Limited Reasoning.” Artificial Intelligence, 34(1):39–76. 
Fagin, R., Halpern, J. Y., and Megiddo, N. (1990). “A logic for reasoning about probabilities.” 
Information and Computation, 87, Nos. 1 & 2, pp. 78−128. 
Fagin, R., Halpern, J. Y., Moses Y. and Vardi, M. Y. (1995). Reasoning about Knowledge. 
Cambridge: MIT Press. 

 
References 
349 
 
Fattorosi-Barnaba, M. and Amati, G. (1987). “Modal operators with probabilistic interpretations I.” 
Studia Logica, 46, pp. 383−393. 
Feller, W. (1968). An Introduction to Probability Theory and Its Applications, Vol. 1, John Wiley 
& Sons. 
Fitting, M. and Mendelsohn, R. L. (1998). First-Order Modal Logic, Kluwer Academic Publishers. 
Fox, J. and Das, S. K. (2000). Safe and Sound: Artificial Intelligence in Hazardous Applications. 
AAAI-MIT Press, June. 
Fox, J., Krause, P, and Ambler, S. (1992). “Arguments, contradictions, and practical reasoning.” 
Proceedings of the 10th European Conference on Artificial Intelligence, Vienna, August, pp. 
623–626. 
Gallaire, H., Minker, J. and Nicolas, J. M. (1984). “Logic and databases: a deductive approach,” 
ACM Computing Surveys, 16(2), 153−185. 
Gallier, J. H. (2003). Logic for Computer Science, New York: John Wiley & Sons. Revised On-
Line Version (2003): http://www.cis.upenn.edu/~jean/gbooks/logic.html. 
Garson, J. W. (1984). Quantification in modal logic. In Handbook of Philosophical Logic, D. 
Gabbay and F. Guenthner (eds.), 2, 249−307, Reidel Publication Co. 
Ginsberg, M. L. (ed.) (1987). Readings in Nonmonotonic Reasoning, Los Altos, CA: Morgan 
Kaufmann. 
Goldblatt, R. (2001). “Mathematical modal logic: a view of its evolution.” In A History of 
Mathematical Logic, van Dalen et al. (Eds). 
Halpern, J. Y. and Moses, Y. O. (1985). “A guide to the modal logics of knowledge and belief.” 
Proc. of the 9th International Joint Conference on Artificial Intelligence, 480−490. 
Halpern, J. and Rabin, M. (1987). “A logic to reason about likelihood.” Artificial Intelligence, 32, 
379−405. 
Halpern, J. Y. (1995a). “Reasoning about knowledge: a survey.” Handbook of Logic in Artificial 
Intelligence and Logic Programming, 4, D. Gabbay, et al., eds., Oxford University Press, 
1−34. 
Halpern, J. Y. (1995b). “The effect of bounding the number of primitive propositions and the depth 
of nesting on the complexity of modal logic.” Artificial Intelligence, 75. 
Heckerman, D. E. and Shortliffe. E. H. (1991). “From Certainty Factors to Belief Networks.” 
Artificial Intelligence in Medicine, 4, pp. 35−52. 
Hintikka, J. (1962). Knowledge and Belief: An Introduction to the Logic of the Two Notions. N.Y.: 
Cornell University Press. 
Howard, R. A. and Matheson, J. E. (1984). “Influence diagrams.” In Readings on the Principles 
and Applications of Decision Analysis, 721–762. Strategic Decisions Group. 
Huang, C. and Darwiche, A. (1996). “Inference in belief networks: A procedural guide.” 
International Journal of Approximate Reasoning volume,15 (3), 225-263. 

350 
Decision-Making Agents 
 
 
Hughes, G. E., and M. J. Cresswell. (1996). A New Introduction to Modal Logic. London: 
Routledge. 
Jackson, P. (1998). Introduction to Expert Systems. Addison-Wesley. 
Jensen, F. V., Lauritzen, S. L., and Olesen. K. G. (1990). ““Bayesian updating in causal 
probabilistic networks by Local Computations.” Computational Statistics, Q. 4, 269-282. 
Jensen, F., Jensen, F. V. and Dittmer, S. L. (1994). “From influence diagrams to junction trees.” 
Proceedings of the 10th Conference on Uncertainty in Artificial Intelligence (UAI). 
Jensen, F.V. (1996). An Introduction to Bayesian Networks. Springer-Verlag. 
Jensen, F. V. (2002). Bayesian Networks and Decision Graphs, Springer. 
Karp, R. (1972) “Reducibility among Combinatorial Problems.” “Complexity of Computer 
Computations”, ed. R. Miller and J. Thatcher (eds), 85−103, in Plenum Press, New York. 
Knight, K. (1989). “Unification: A multidisciplinary survey.” ACM Computing Surveys, 21(1), pp. 
93−124. 
Kowalski, R. A. (1979a). Logic for Problem Solving, New York: North-Holland. 
Kowalski, R. A. (1979b). “Algorithm = Logic + Control.” Communications of the ACM, 22(7), 
424−435. 
Kowalski, R. A. and Kuehner, D. (1971). “Linear resolution with selection function.” Artificial 
Intelligence, 2, 227−260.  
Kripke, S. A. (1963). Semantical Analysis of Modal Logic I: Normal Modal Propositional Calculi. 
Zeitschrift fur Mathematische Logik und Grundladen der Mathematik (ZMLGM), 9, 67–96. 
Ladner, R. (1977). “The computational complexity of provability in systems of modal propositional 
logic.” SIAM Journal of Computing, 6, 467–480. 
Lauritzen, S. L. and D. J. Spiegelhalter (1988). “Local computations with probabilities on graphical 
structures and their applications to expert systems.” Journal of the Royal Statistical Society, B 
50(2), pp.154−227. 
Lemmon, E. J. (1960). Quantified S4 and the Barcan formula. Journal of Symbolic Logic, 24, 
391−392. 
Lemmon, E. J. (1977). An Introduction to Modal Logic. American Philosophical Quarterly, 
Monograph No. 11. 
Lewis, C. I. (1918). A Survey of Symbolic Logic. University of California Press. 
Lewis, C. I. and Langford, C. H. (1932). Symbolic Logic. Dover Publications, Inc., New York. 
Lloyd, J. W. (1987). Foundations of Logic Programming, 2nd Edition, Berlin, Germany: Springer 
Verlag. 
Lukasiewicz, J. (1951). Aristotle's Syllogistic, Oxford University Press. 
McCarthy, J. (1980). “Circumscription: A form of non-monotonic reasoning.” Artificial 
Intelligence, 13, 27−39. 

 
References 
351 
 
McKinsey, J. C. C. (1934). “A reduction in the number of postulates for C. I. Lewis’ system of 
strict implication.” Bulletin of American Mathematical Society, 40, pp. 425-427. 
McKinsey, J. C. C. (1940). “Proof that there are infinitely many modalities in Lewis's system S2.” 
Journal of Symbolic Logic, 5(3), pp. 110-112. 
McKinsey, J. C. C. (1941). “A solution of the decision problem for the Lewis systems S2 and S4, 
with an application to topology.” Journal of Symbolic Logic, 6, pp. 117-134. 
Mendelson, E. (1987). Introduction to Mathematical Logic. California, USA: Wadsworth & 
Brooks/Cole Advanced Books and Software. 
Mitchell, T. (1997). Machine Learning. McGraw-Hill, 1997. 
Meyer, J.-J.Ch and Hoek, W. van der (1995). Epistemic Logic for AI and Computer Science. 
Cambridge Tracts in Theoretical Computer Science 41. Cambridge: Cambridge University 
Press. 
Meyer, J.-J.Ch and Hoek, W. van der (1995). Epistemic Logic for AI and Computer Science. 
Cambridge Tracts in Theoretical Computer Science 41. Cambridge: Cambridge University 
Press. 
O’Keefe, R. A. (1990). Craft of Prolog. Cambridge, MA: MIT Press. 
Ohlbach, H. J. (1988). “A resolution calculus for modal logics.” Proceedings of the 9th International 
Conference on Automated Deduction, Argonne, USA.  
Papadimitriou, C. H. (1993). Computational Complexity. Addison Wesley, Reading, MA, USA. 
Parry, W. T. (1939). “Modalities in the survey system of strict implication.” Journal of Symbolic 
Logic, 4(4), pp. 137-154. 
Pearl, J. (1986). “A constraint-propagation approach to probabilistic reasoning.” Proceedings of the 
Conference on Uncertainty in Artificial Intelligence, pp. 357-369, Amsterdam: North-Holland. 
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. 
San Mateo, CA, Morgan Kaufmann. 
Quine, W. V. O. (1980). Elementary Logic. Cambridge, MA: Harvard University Press. 
Rao, A. S. and Georgeff, M. P. (1991). “Modeling rational agents within a BDI-architecture.” 
Proceedings of Knowledge Representation and Reasoning, pp. 473-484. Morgan Kaufmann 
Publishers: San Mateo, CA. 
Rao, A. S. and Georgeff, M. P. (1991). “Modelling rational agents within a BDI-architecture.” 
Proceedings of the Knowledge Representation and Reasoning, 473−484. 
Rasmussen, J. (1983). “Skills, Rules and Knowledge: Signals, Signs and Symbolism, and Other 
Distinctions in Human Performance Models.” IEEE Transactions on Systems, Man, and 
Cybernetics, 12, 257−266. 
Reiter, R. (1978b). “On closed world databases.” Logic and Databases, H. Gallaire and J. Minker 
(eds.), New York: Plenum Press, 55−76. 
Reike, R. D. and Sillars, M. O. (2001). Argumentation and Critical Decision Making. Longman, 
New York. 

352 
Decision-Making Agents 
 
 
Robinson, J. A. (1965). “A machine-oriented logic based on the resolution principle.” Journal of 
the Association for Computing Machinery, 12, 23−41. 
Russell, S. J. and Norvig, P. (2002). Artificial Intelligence: A Modern Approach. Prentice Hall. 
Shachter, R. D. (1986). “Evaluating influence diagrams.” Operations Research, 34, 871−882. 
Shafer, G. (1976). A Mathematical Theory of Evidence. Princeton, NJ, Princeton University Press. 
Shenoy, 
P. 
P. 
(1992). 
“Valuation-based 
systems 
for 
Bayesian 
decision 
analysis.” 
 Operations Research, 40(3), 463-484. 
Smets, P. (1991). “Varieties of ignorance and the need for well-founded theories,” Information 
Sciences, 57-58: 135-144. 
Sterling, L. and Shapiro, E. (1994). The Art of Prolog. Cambridge, MA: MIT Press. 
Stockmeyer, L. J. and Meyer, A.R. (1973). “Word problems requiring exponential time.” 
Proceedings of the 5th  ACM Symposium on Theory of Computing, 1−9. 
Stephanou, H. E. and Lu, S.-Y. (1988). “Measuring consensus effectiveness by a generalized 
entropy criterion.” IEEE Transactions on Pattern Analysis and Machine Intelligence, 10(4), 
pp. 544-554. 
Stoll, R. R. (1963). Set Theory and Logic, New York: W. H. Freeman and Company. Reprint 
edition (1979) by Dover Publications. 
Toulmin, S. 1956. The Uses of Argument. Cambridge, U.K.: Cambridge University Press. 
Ullman, J. D. (1984). Principles of Database Systems, 2nd edition. Maryland, USA: Computer 
Science Press. 
Van Emden, M. H. and Kowalski, R. A. (1976). “The semantics of predicate logic as a 
programming language.” Journal of the Association for Computing Machinery, 23(4), pp. 
733−742. 
Vardi, M. Y. (1982). “The complexity of relational query languages.” Proceedings of the 14th 
Annual ACM Symposium on Theory of Computing, San Francisco, California, pp. 137–146. 
Von Wright, G., H. (1951). An Essay on Modal Logic. Amsterdam: North-Holland Publishing 
Company. 
Voorbraak, F. (1996). Epistemic Logic and Uncertainty. Workshop on Quantitative and Symbolic 
Approaches to Uncertainty, European Summer School on Logic, Language, and Information, 
August 12−23, Prague. 
Wainer, J. (1994). “Yet Another Semantics of Goals and Goal Priorities.” Proceedings of the 
Eleventh European Conference on Artificial Intelligence, Amsterdam, The Netherlands, pp. 
269-273. 
Wen W. X. (1990). “Optimal decomposition of belief networks.” Proceedings of the 6th Conference 
on Uncertainty in Artificial Intelligence, 245−256. Morgan Kaufmann. 
Whitehead, A. N. and Russell, B. (1925−1927). Principia Mathematica, 1, 2 & 3, 2nd Edition. 
Cambridge, England: Cambridge University Press. 
Wilson, R. (1996). Introduction to Graph Theory, Addison-Wesley. 

 
References 
353 
 
Yager, R. R., Kacprzyk, J., and Fedrizzi, M. (Editors) (1994). Advances in the Dempster-Shafer 
Theory of Evidence, John Wiley & Sons. 
Yager, R. R. (2004). “On the determination of strength of belief for decision support under 
uncertainty – Part II: fusing strengths of belief.” Fuzzy Sets and Systems, 142, pp. 129–142. 
Zadeh, L. A. (1965). “Fuzzy Sets.” Information and Control, 8: 338-353. 
Zadeh, L. A. (1978). “Fuzzy Sets as the Basis for a Theory of Possibility.” Fuzzy Sets and Systems, 
1:3-28. 

This page intentionally left blank
This page intentionally left blank

 
Index 
355 
 
Index 
 
 
a priori probability, 10 
Accessibility relation, 281, 296 
Accountability condition, 232 
Action, 12, 329 
Action node, 238 
Acyclic graph, 31 
Agent. See Intelligent agent 
Aggregation, 335, 339 
Algorithm 
definition, 40 
evidence 
propagation. 
See 
Evidence 
propagation algorithm 
exponential class, 40 
junction tree. See junction tree algorithm 
order of, 40 
polynomial class, 40 
unification. See Unification algorithm 
Algorithmic complexity, 40 
(
)
k
O x
, 41 
Co-NP, 41 
EXP, 40 
NEXP, 142 
NP, 41 
NP-hard, 41 
polynomial, 40 
PSPACE, 44 
space, 43 
Allowed program, 120 
Alphabet, 53 
first-order. See First-order alphabet 
propositional. See Propositional alphabet 
Ancestor node, 32 
Antecedent, 5, 47 
Antecedent of a clause, 99 
Argument, 3, 329 
conclusion, 3 
premise, 3 
Argumentation, 325 
Toulmin’s model. See Toulmin’s model 
of argumentation 
Assertional statement, 3 
Asymmetric relation, 26 
Atom, 46, 58 
Atomic event, 34 
Atomic formula, 46, 58 
Axiom, 53 
4, 264 
B, 264 
D, 264 
E, 264 
logical. See Logical axiom 
proper. See Proper axiom 
T, 264 
Axiom of necessaity. See Axiom T 
Axiom of syllogism, 259 
Axiomatic approach to probability, 36 
Axiomatic deduction, 5, 53 
Axiomatic theorem proving, 6 
B, 14, 255, 265, 277 
Backtracking, 126 
Barcan formula, 14, 298 
Basic Probability Assignment, 151 

356 
Decision-Making Agents 
 
 
Bayes’ rule, 40 
Bayesian belief network. See Belief network 
Bayesian formula, 9, 40, 146 
Bayesian probability, 9, 146 
Bayesian rule. See Bayesian formula 
Behavior 
knowledge-based. See Knowledge-based 
behavior 
rule-based. See Rule-based behavior 
skill-based. See Skill-based behavior 
Belief, 13, 166, 181, 308, 314 
Belief entropy, 158 
Belief function, 151 
Belief network, 9, 165, 166, 339 
Belief network tools, 235 
Belief potential. See Potential 
Belief revision, 2, 184 
Belief vector, 12 
Biconditional formula, 47 
Binary relation, 26 
Binary resolvent, 86 
Binding. See Variable binding 
Body of a clause, 99 
BPA. See Basic Probability Assignment 
Brouwerian axiom, 260, See Axiom B 
Candidate. See Candidate hypothesis 
Candidate hypothesis, 329 
Canonical model, 318 
Causal network. See Belief network 
Causality, 10 
Chance node, 12, 238, 240 
Child node, 32 
Classical approach to probability, 35 
Classical modal logics, 256 
Clausal form, 6 
Clausal formula, 72 
Clause, 72, 99 
definite. See Definite clause 
indefinite. See Indefinite clause 
input. See Input clause 
normal. See Normal clause 
program. See Program clause 
unit. See Unit clause 
Clique, 216 
Closed formula, 59 
Closed World Assumption, 113 
Closure of a formula, 59 
Clsuter tree. See Junction tree 
CNF. See Conjunctive Normal Form 
Commitment rule, 330 
Compactness theorem, 71 
Complete theory, 70 
Completion of a program, 117 
Complexity analysis, 40, 95, 141, 291 
Compound event, 34 
Conclusion of a clause, 99 
Conclusion of an argument, 3, 53 
Condition of a clause, 99 
Conditional dependence, 166 
Conditional formula, 47 
Conditional independence, 11, 171 
Conditional probability, 9, 146 
Conditional probability distribution, 39 
Conditional Probability Table, 10, 166 
Conditional statement, 3 
Conjunction of formulae, 47, 51 
Conjunctive Normal Form, 51, 71 
Connected model, 283 
Connected nodes, 30 
Connective, 54 
Consensus, 157 
Consequence of a clause, 99 

 
Index 
357 
 
Consequent, 5, 47 
Consistency, 63 
Consistency postulate, 259 
Consistency theorem, 56 
Constant. See Individual constant 
Continuous probability distribution, 37 
Contradiction, 50 
Control component, 97 
Core entropy, 158 
CPT. See Conditional Probability Table 
Cumulative domain, 297 
Cut predicate, 127 
CWA. See Closed World Assumption 
Cycle, 30 
Cycle length, 102 
D, 255, 265, 277 
DAG. See Directed Acyclic Graph 
d-connection, 177 
De Morgan’s law, 51 
Decidability, 273, 291 
axiomatic theory, 54 
Decidable theory, 54 
Decision, 237 
Decision aid, 2 
Decision node, 12, 240 
Decision problem, 41, 261 
Decision tree, 238 
Declarative programming, 97, 99 
Declarative semantics, 106 
Deduction, 91 
Deduction theorem, 55 
Definite clause, 99 
Definite goal, 100 
Definite program, 101, 108 
Definition of a predicate, 104 
Degree of belief, 15, 143, 150 
Degree of consensus, 158 
Degree of support, 15, 255 
Degree of uncertainty, 143 
Dempster combination rule, 19 
Dempster-Shafer theory, 19, 143, 150, 320, 
335 
Deontic axiom. See Axiom D 
Dependency graph, 102 
Depth of a tree, 33 
Derivation, 91 
SLD. See SLD derivation 
SLDNF. See SLDNF derivation 
Descendant  node, 32 
Dictionary of symbol, 310 
Difference of two sets, 25 
Direct consequence, 53 
Directed Acyclic Graph, 31, 175, 211 
Directed graph, 29 
Discrete probability distribution, 37 
Discrete random variable, 8, 37 
Disjunction of formulae, 47, 51 
Disjunctive Normal Form, 51 
Distributive law, 51 
DNF. See Disjunctive Normal Form 
Domain of a function, 28 
Domain of interpretation, 59, 296 
Domino model, 328 
Downward evidence propagation, 190, 194, 
200, 204 
d-separation, 11, 176 
Edge, 29 
Elementary event, 34 
Empty set, 24 
Entropy, 158 
belief. See Belief entropy 
core. See core entropy 

358 
Decision-Making Agents 
 
 
generalized. See generalized entropy 
Epistemic logic, 13, 307 
Epistemic model 
possible world. See Possible world 
epistemic model 
probabilistic. See Probabilistic epistemic 
model 
propositional. See Propositional epistemic 
model 
Epistemic state, 1 
EQ. See equality theory 
Equal mapping, 28 
Equality theory, 117 
Equivalence class, 27 
Equivalence relation, 26 
Equivalent formulae, 50, 61 
EU. See Expected Utility 
Euclidean relation, 26 
EUT. See Expected Utility Theory 
Event, 8, 34, 145 
Event space, 34 
Evidence, 179 
hard. See Hard evidence 
soft. See Soft evidence 
Evidence combination, 162 
Evidence handling, 227 
Evidence propagation, 17, 190, 211 
downward. See Downward evidence 
propagation 
upward. 
See 
Upward 
evidence 
propagation 
Evidence propagation algorithm, 208 
Exception independence condition, 232 
Existential quantifier, 58 
Expected Utility, 237 
Expected Utility Theory, 237 
Explaining away, 175 
Extension of a theory, 70 
Failed branch, 119 
False formula, 48 
First-order 
alphabet, 57 
language, 58 
logic, 4, 57 
predicate logic, 63 
theory, 62 
Fixed domain, 297 
Fixpoint semantics, 106 
Floundering computation, 120 
Focal element, 151 
Formal axiomatic theory, 53 
Formula, 47, 53, 58 
atomic. See Atomic formula 
biconditional. See Biconditional formula 
closed. See Closed formula 
conditional. See Conditional formula 
false. See False formula 
inconsistent. See Inconsistent formula 
quantified 
Boolean. 
See 
Quantified 
Boolean Formula 
tautology. See Tautology 
true. See True formula 
valid. See Valid formula 
well-formed. See Well-formed formula 
Frame of discernment, 151 
Free for a variable, 59 
Function, 28 
Game example, 3 
General form of a clause, 116 
Generalization, 63 
Generalized entropy, 159 
Global propagation, 224 
Goal, 99, 100, 320, 329 

 
Index 
359 
 
definite. See Definite goal 
normal. See Normal goal 
Prolog. See Prolog goal 
Gödel’s Completeness Theorem, 69 
Graph, 29 
dependency. See Dependency graph 
directed. See Directed graph 
moral. See Moral graph 
simple. See Simple graph 
triangulated. See Triangulated graph 
Ground instance, 75 
Ground resolution theorem, 87 
Hard evidence, 179 
Head of a clause, 99 
Herbrand 
base, 75 
interpretation, 75 
theorem, 74, 76 
universe, 75 
Hierarchical program, 103 
Hyperelation, 15, 309, 314 
Hypothesis, 54, 145 
Identity mapping, 28 
if-then rule, 143, 145 
Implication, 5 
Strict. See Strict implication 
Inconsistency, 63 
Inconsistent formula, 50 
Indefinite clause, 100 
Independence 
conditional. 
See 
Conditional 
independence 
marginal. See Marginal independence 
Independent events, 35 
Index set, 25 
Individual constant, 57 
Inference rule. See Rule of inference 
Inferencing, 5 
Inferencing in influence diagrams, 242 
Infinite branch, 119 
Influence diagram, 12, 240 
Input clause, 91, 110 
Input length, 40 
Input resolution, 94 
Instance, 81 
ground, 81 
Interpretation, 59 
Interpretation of a formula, 49 
Intersection of two sets, 25 
Intractable problem, 41 
Join tree. See Junction tree 
Joint probability distribution, 8, 38 
Junction tree, 213 
strong. See Strong junction tree 
Junction tree algorithm, 213 
K, 14, 255, 263, 265, 277 
KD4E, 308 
Knowledge, 307 
Knowledge-based behavior, 18 
LAB. See Logic of Agents Beliefs 
Lambda vector, 182 
Language 
first-order. See First-order language 
propositional. See Propositional language 
Leaf node, 32 
Level mapping, 105 
Level of a vertex, 33 
Lifting lemma, 88 
Likelihood vector, 180 
Lindenbaum’s lemma, 70 
Linear fragment, 190 
Linear resolution, 94 

360 
Decision-Making Agents 
 
 
Linear tree fragment, 176 
Literal, 51, 58 
negative. See Negative literal 
positive. See Positive literal 
Logic 
first-order. See First-order logic 
first-order 
predicate. 
See 
First-order 
predicate logic 
modal. See Modal logics 
propositional. See Propositional logic 
Logic component, 97 
Logic of Agents Belief, 15, 309 
Logic programming, 7, 97 
Logical axiom, 62 
Logical connective, 46, 54 
Logical consequence, 52, 61 
Logical form, 3 
Logical omniscience, 7 
Machine learning, 2 
Mapping, 28 
equal. See Equal mapping 
identity. See Identity mapping 
one-to-one. See One-to-one mapping 
onto. See Onto mapping 
Mapping composition, 28 
Marginal independence, 166, 171 
Marginalization rule, 40 
Mass function. See Basic Probability 
Assignment 
Material implication, 13 
Matrix, 71, 76, 261 
Matrix method, 273 
Maximally consistent, 63 
Maximum Expected Utility, 238 
Member of a set, 24 
Meta interpreter, 139 
Meta programming, 139 
MEU. See Maximum Expected Utility 
mgu. See Most General Unifier 
Modal clause, 300 
Modal epistemic logic, 13 
Modal first-order logic, 294 
Modal function, 260 
Modal logics, 13, 255 
B. See B 
classical. See Classical modal logics 
epistemic. See Epistemic logic 
first-order. See Modal first-order logic 
K. See K 
KD4E. See KD4E 
LAB. See Logic of Agents Belief 
S4. See S4 
S5. See S5 
T. See T 
Modal operator 
belief, 308 
knowledge, 307 
necessary, 13, 260, 262 
possible, 13, 258, 262 
support, 15, 310 
Modal resolution, 300 
Modal syllogism, 257 
Modal unification, 304 
Modality, 260, 272 
affirmative, 272 
degree of, 272 
negative, 272 
null, 272 
proper, 272 
type A, B, C, D, 272 
Model, 61, 313 
canonical. See Canonical model 

 
Index 
361 
 
connected. See Connected model 
normal. See Normal model 
P3. See P3 
possible world. See Possible world model 
probabilistic epistemic. See Probabilistic 
epistemic model 
propositional epistemic. See Propositional 
epistemic model 
Model-theoretic approach, 55 
Modus Ponens, 4, 55, 63, 263, 308 
Moral graph, 214 
Moralization, 249 
Most General Unifier, 7, 82 
MP, 63, See Modus Ponens 
Multiplication rule, 39 
Mutually exclusive events, 35 
Mutually recursive predicates, 103 
NAF. See Negation As Failure 
Negation, 47 
Negation As Failure, 113 
Negative condition, 99 
Negative edge, 102 
Negative literal, 58 
Node, 29, 166 
chance. See Chance node 
decision. See Decision node 
utility. See Utility node 
value. See Utility node 
Noisy-or technique, 19, 231 
Nondeterministic 
Polynomial. 
See 
NP-
completeness 
Normal clause, 100 
Normal goal, 100 
Normal model, 281, 296 
Normal program, 101, 114 
NP-completeness, 41 
Occur check, 83 
Occurrence of a variable 
bound, 59 
free, 59 
One-literal rule, 79 
One-to-one correspondence, 28 
One-to-one mapping, 28 
Onto mapping, 28 
Operational semantics, 106 
p(X). See Probability 
P3, 17, 328, 330, 334 
Parent node, 32 
Partial ignorance, 159 
Path, 29 
pdf. See Probability Density Function 
Pi vector, 182 
Plan, 329 
Polytree, 31 
Polytree fragment, 191 
Positive condition, 99 
Positive edge, 102 
Positive literal, 58 
Possible world, 261, 280 
Possible world epistemic model, 12 
Possible world model, 2 
Possible world semantics, 14, 279, 313 
Posterior probability, 12 
Posting of evidence, 181 
Potential, 222 
Power set, 25 
Predicate, 57 
recursive. See Recursive predicate 
Premise, 54 
Premise of an argument, 3 
Prenex Normal Form, 71 
Principia Mathematica, 13, 257 
Principle of duality, 265 

362 
Decision-Making Agents 
 
 
Prior 
probability, 
182, 
See 
a 
priori 
probability  
Probabilistic epistemic model, 8 
Probabilistic model, 1 
Probabilistic network. See Belief network 
Probability, 34 
a priori. See a priori probability  
acquisition, 230 
Bayesian. See Bayesian probability 
conditional. See Conditional probability 
posterior. See Posterior probability 
prior. See a priori probability  
subjective. See Subjective probability 
Probability Density Function, 37 
Probability distribution, 8, 37 
conditional. See Conditional probability 
distribution 
continuous. See Continuous probability 
distribution  
discrete. 
See 
Discrete 
probability 
distribution  
joint. See Joint probability distribution 
Probability histogram, 38 
Probability mass function. See Probability 
Density Function 
Procedural programming, 97 
Procedural semantics, 106 
Program, 101 
allowed. See Allowed program 
definite. See Definite program 
hierarchical. See Hierarchical program 
normal. See Normal program, See Normal 
program 
Prolog. See Prolog program 
range-restricted. See Range restricted 
program 
recursive. See Recursive program 
stratified. See Stratified program 
Program clause, 99 
Program 
complexity. 
See 
complexity 
analysis 
Program semantics, 106 
Programming 
declarative. See Declarative programming 
logic. See Logic programming 
procedural. See Procedural programming 
Prolog, 121 
!, 127 
*, /, +, -, mod, div, 132 
;, 128 
[], 131 
abolish, 136 
arithmetic, 132 
assert, 136 
backtracking, 126 
bagof, 137 
consult, 135 
cut, 127 
end_of_file, 136 
equality, 130 
exists, 133 
get0, 135 
input/output, 133 
list, 131 
meta interpreter, 139 
meta programming, 139 
negation, 129 
not, 129 
put, 135 
read, 134 
reconsult, 136 
repeat, 134 
retract, 136 
retractall, 136 

 
Index 
363 
 
search tree, 123 
see, 133 
setof, 137 
tab, 134 
tell, 133 
write, 134 
Prolog goal, 121 
Prolog program, 121 
Proof, 6, 53 
Proof by contradiction, 6 
Proof procedure, 71, 106 
Proof-theoretic approach, 55 
Propagation algorithm, 12 
Proper axiom, 6, 63 
Proper canonical model, 318 
Proper subset, 24 
Proposition, 46, 256 
contingently false, 256 
contingently true, 256 
false, 256 
impossible, 256 
necessary, 256 
possible, 256 
true, 256 
Propositional alphabet, 47 
Propositional epistemic model, 3 
Propositional language, 46, 47, 54 
Propositional logic, 4, 46 
Propositional model, 1 
Propositional satisfiability problem, 42 
Propositional variable, 54 
Provability, 261 
Pure-literal rule, 79 
Purity principle, 93 
QBF. See Quantified Boolean Formula 
Quantified Boolean Formula, 95 
Quantifier 
existential. See Existential quantifier 
universal. See Universal quantifier 
Random experiment, 34 
Random variable, 8, 37 
discrete. See Discrete random variable 
Range of a function, 28 
Range-restricted program, 120 
Rational agent, 326 
Rational decision-making agent, 319 
Rational system, 319 
Recursive predicate, 103 
Recursive program, 103 
Reflexive relation, 26 
Refutation, 6, 92 
SLD. See SLD refulation 
SLDNF. See SLDNF refutation 
Refutation procedure, 91 
Relation, 26 
accessibility. See Accessibility relation, 
See Accessibility relation 
asymmetric. See Asymmetric relation 
binary. See Binary relation 
equivalence. See Equivalence relation 
euclidean. See Euclidean relation 
reflexive. See Reflexive relation 
serial. See Serial relation 
symmetric. See Symmetric relation 
transitive. See Transitive relation 
Relative frequency approach, 36 
Replacement principle, 94 
Resolution, 87 
input. See Input resolution 
linear. See Linear reaolution  
SL. See SL resolution 
SLD. See SLD resolution 

364 
Decision-Making Agents 
 
 
SLDNF. See SLDNF resolution 
unit. See Unit resolution 
Resolution principle, 7, 80 
Resolution refinement, 94 
Resolution rule 
one-literal. See One-literal rule 
pure-literal. See Pure-literal rule 
splitting. See Splitting rule 
subsumption. See Subsumption rule 
tautology. See Tautology rule 
Resolution theorem, 91 
Resolution theorem proving, 6, 80 
modal, 15 
Resolvent, 80, 86 
binary. See Binary resolvent 
RN. See Rule of Necessitation 
Root node, 32 
Rooted tree, 33 
Rule. See if-then rule  
Rule of inference, 4, 53, 55 
Rule of Necessitation, 263, 308 
Rule-based behavior, 18 
Rules for inferring negative information, 
106 
S4, 14, 255, 265, 277 
S5, 14, 255, 265, 277 
Sample space. See Event space 
SAT. 
See 
Propositional 
satisfiability 
problem 
Satisfaction of a formula, 60 
Satisfiability, 50, 60, 61, 282 
Scope of a quantifier, 58 
Search principle, 93 
purity. See Purity principle 
replacement. See Replacement principle 
subsumption. See Subsumption principle 
tautology. See Tautology principle 
Search space, 110 
Search tree, 123 
Selected atom, 110 
Semantics 
declarative. See Declarative semantics 
fixpoint. See Fixpoint semantics 
operational. See Operational semantics 
possible world. See Possible world 
semantics 
procedural. See Procedural semantics 
Sentence, 59 
Separator, 248 
Separator set, 220 
Sepset. See Separator set 
Serial relation, 26 
Set 
empty. See Empty set  
index. See Index set 
power. See Poser set 
Set operation 
difference. See Difference of two sets  
intersection. See Intersection of two sets  
union. See Union of two sets  
Simple event, 34 
Simple graph, 29 
Situation, 329 
Skill-based behavior, 18 
Skolem Conjunctive Normal Form, 71 
Skolemization, 73 
Skolem-Löwenheim Theorem, 71 
SL Resolution, 94 
SLD 
derivation, 110 
refutation, 110 
resolution, 110 

 
Index 
365 
 
tree, 110 
SLDNF 
derivation, 118 
refutation, 119 
resolution, 118 
SLDNF tree, 119 
Soft evidence, 179 
Soundness and completeness theorem, 55, 
63, 70, 286, 316 
strong, 56 
Soundness theorem, 64 
Splitting rule, 79 
State of a random variable, 37 
Statement, 59 
assertional. See Assertional statement 
conditional. See Conditional statement 
Stratified program, 104 
Stratum of a program, 105 
Strict implication, 13 
Strong junction tree, 248, 249 
Strong root, 249 
Subgoal, 101 
Subjective probability, 36 
Subset, 24 
Substitution, 80 
composition, 81 
empty, 81 
ground, 81 
identity, 81 
Subsumption principle, 94 
Subsumption rule, 79 
Success branch, 119 
Survey of Symbolic Logic, 13, 257 
Survey system, 258 
Symbol, 53 
Symbolic 
argumentation. 
See 
Argumentation 
Symbolic reasoning, 257 
Symbolization, 3 
Symmetric relation, 26 
T, 14, 255, 265, 277 
Tautology, 49 
Tautology principle, 94 
Tautology rule, 79 
Term, 57 
Term assignment, 60 
Theorem, 6, 54 
compactness, 71 
completeness of SLD resolution, 113 
consistency, 56 
deduction, 55 
Gödel’s completeness, 69 
ground resolution, 87 
Herbrand, 74, 76 
resolution, 91 
Skolem-Löwenheim, 71 
soundness, 64 
soundness and completeness, 55, 63, 70 
soundness of SLD resolution, 113 
soundness of SLDNF resolution, 120 
weak deduction, 65 
Theorem proving 
Axiomatic. 
See 
Axiomatic 
theorem 
proving 
Resolution. 
See 
Resolution 
theorem 
proving 
Theorem proving procedure, 71 
Theory 
Bayesian 
probability. 
See 
Bayesian 
probability 
Dempster-Shafer. See Dempster-Shafer 
theory 

366 
Decision-Making Agents 
 
 
first-order. See First-order theory 
Theory of belief function. See Dempster-
Shafer theory 
Total probability rule, 39 
Toulmin’s model of argumentation, 327 
Tractable problem, 41 
Transitive relation, 26 
Traveling Salesman Problem, 41 
Tree, 29, 33 
cluster. See Junction tree 
decision. See Decision tree 
join. See Junction tree 
junction. See Junction tree 
SLD. See SLD tree 
SLDNF. See SLDNF tree 
Tree fragment, 190 
Triangulated graph, 215 
Triangulation, 249 
True formula, 48 
Truth table, 48 
Truth value, 48, 61 
TSP. See Traveling Salesman Problem  
Uncertainty handling, 143 
Undecidability, 43 
Undecidability of axiomatic theory, 54 
Undecidable theory, 54 
Unifiability, 82 
Unification, 80 
Unification algorithm, 83 
Unifier, 82 
Union of two sets, 25 
Unit assertion, 99 
Unit clause, 99 
Unit resolution, 94 
Universal quantifier, 5, 57 
Unsatisfiability, 61 
Upward evidence propagation, 190, 191, 
198, 201 
Utility, 12 
Utility node, 12, 240 
Valid argument, 4 
Valid formula, 49 
Validity, 61 
Valuation, 281 
Value node. See Utility node 
Variable, 57, 166 
propositional, 54 
Variable binding, 81 
Vector 
belief. See Belief vector 
lambda. See Lambda vector 
likelihood. See Likelihood vector 
pi. See Pi vector 
Vertex, 29 
Weak deduction theorem, 65 
Well-formed expression, 80 
Well-formed formula, 47, 58 
wff. See Well-formed formula 
 

