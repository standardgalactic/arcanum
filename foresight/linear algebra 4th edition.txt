
Linear
Algebra

This page intentionally left blank 

Fourth Edition
Stephen H. Friedberg
Arnold J. Insel
Lawrence E. Spence
Illinois State University
PEARSON EDUCATION, Upper Saddle River, New Jersey 07458

Library of Congress Cataloging-in-Publication Data
Friedberg, Stephen H.
Linear algebra / Stephen H. Friedberg, Arnold J. Insel, Lawrence E. Spence.--4th ed.
p.
cm.
Includes indexes.
ISBN 0-13-008451-4
1. Algebra, Linear.
I. Insel, Arnold J.
II. Spence, Lawrence E.
III. Title.
QA184.2.F75
2003
2002032677
512‚Äô.5 --dc21
Acquisitions Editor: George Lobell
Editor in Chief: Sally Yagan
Production Editor: Lynn Savino Wendel
Vice President/Director of Production and Manufacturing: David W. Riccardi
Senior Managing Editor: Linda Mihatov Behrens
Assistant Managing Editor: Bayani DeLeon
Executive Managing Editor: Kathleen Schiaparelli
Manufacturing Buyer: Michael Bell
Manufacturing Manager: Trudy Pisciotti
Editorial Assistant: Jennifer Brady
Marketing Manager: Halee Dinsey
Marketing Assistant: Rachel Beckman
Art Director: Jayne Conte
Cover Designer: Bruce Kenselaar
Cover Photo Credits: Anni Albers, Wandbehang We 791 (Orange), 1926/64. Dreifachgewebe:
Baumwolle und Kunstseide, schwarz, wei√ü, Orange 175 √ó 118 cm. Foto: Gunter
Lepkowski, Berlin. Bauhaus-Archiv, Berlin, Inv. Nr. 1575. Lit.: Das
Bauhaus webt, Berlin 1998, Nr. 38.
c‚Éù2003, 1997, 1989, 1979 by Pearson Education, Inc.
Pearson Education, Inc.
Upper Saddle River, New Jersey 07458
All rights reserved. No part of this book may be
reproduced, in any form or by any means,
without permission in writing from the publisher.
Printed in the United States of America
10
9
8
7
6
5
4
3
2
1
ISBN 0-13-008451-4
Pearson Education, Ltd., London
Pearson Education Australia Pty. Limited, Sydney
Pearson Education Singapore, Pte., Ltd
Pearson Education North Asia Ltd, Hong Kong
Pearson Education Canada, Ltd., Toronto
Pearson Educacion de Mexico, S.A. de C.V.
Pearson Education -- Japan, Tokyo
Pearson Education Malaysia, Pte. Ltd

To our families:
Ruth Ann, Rachel, Jessica, and Jeremy
Barbara, Thomas, and Sara
Linda, Stephen, and Alison

This page intentionally left blank 

Contents
Preface
ix
1 Vector Spaces
1
1.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.3
Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
1.4
Linear Combinations and Systems of Linear Equations . . . .
24
1.5
Linear Dependence and Linear Independence . . . . . . . . .
35
1.6
Bases and Dimension
. . . . . . . . . . . . . . . . . . . . . .
42
1.7‚àó
Maximal Linearly Independent Subsets
. . . . . . . . . . . .
58
Index of DeÔ¨Ånitions
. . . . . . . . . . . . . . . . . . . . . . .
62
2 Linear Transformations and Matrices
64
2.1
Linear Transformations, Null Spaces, and Ranges . . . . . . .
64
2.2
The Matrix Representation of a Linear Transformation
. . .
79
2.3
Composition of Linear Transformations
and Matrix Multiplication . . . . . . . . . . . . . . . . . . . .
86
2.4
Invertibility and Isomorphisms . . . . . . . . . . . . . . . . .
99
2.5
The Change of Coordinate Matrix . . . . . . . . . . . . . . .
110
2.6‚àó
Dual Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . .
119
2.7‚àó
Homogeneous Linear DiÔ¨Äerential Equations
with Constant CoeÔ¨Écients
. . . . . . . . . . . . . . . . . . .
127
Index of DeÔ¨Ånitions
. . . . . . . . . . . . . . . . . . . . . . .
145
3 Elementary Matrix Operations and Systems of Linear
Equations
147
3.1
Elementary Matrix Operations and Elementary Matrices
. .
147
*Sections denoted by an asterisk are optional.
v

vi
Table of Contents
3.2
The Rank of a Matrix and Matrix Inverses
. . . . . . . . . .
152
3.3
Systems of Linear Equations‚ÄîTheoretical Aspects . . . . . .
168
3.4
Systems of Linear Equations‚ÄîComputational Aspects . . . .
182
Index of DeÔ¨Ånitions
. . . . . . . . . . . . . . . . . . . . . . .
198
4 Determinants
199
4.1
Determinants of Order 2
. . . . . . . . . . . . . . . . . . . .
199
4.2
Determinants of Order n
. . . . . . . . . . . . . . . . . . . .
209
4.3
Properties of Determinants . . . . . . . . . . . . . . . . . . .
222
4.4
Summary‚ÄîImportant Facts about Determinants . . . . . . .
232
4.5‚àó
A Characterization of the Determinant
. . . . . . . . . . . .
238
Index of DeÔ¨Ånitions
. . . . . . . . . . . . . . . . . . . . . . .
244
5 Diagonalization
245
5.1
Eigenvalues and Eigenvectors . . . . . . . . . . . . . . . . . .
245
5.2
Diagonalizability . . . . . . . . . . . . . . . . . . . . . . . . .
261
5.3‚àó
Matrix Limits and Markov Chains . . . . . . . . . . . . . . .
283
5.4
Invariant Subspaces and the Cayley‚ÄìHamilton Theorem . . .
313
Index of DeÔ¨Ånitions
. . . . . . . . . . . . . . . . . . . . . . .
328
6 Inner Product Spaces
329
6.1
Inner Products and Norms
. . . . . . . . . . . . . . . . . . .
329
6.2
The Gram‚ÄìSchmidt Orthogonalization Process
and Orthogonal Complements
. . . . . . . . . . . . . . . . .
341
6.3
The Adjoint of a Linear Operator
. . . . . . . . . . . . . . .
357
6.4
Normal and Self-Adjoint Operators
. . . . . . . . . . . . . .
369
6.5
Unitary and Orthogonal Operators and Their Matrices
. . .
379
6.6
Orthogonal Projections and the Spectral Theorem . . . . . .
398
6.7‚àó
The Singular Value Decomposition and the Pseudoinverse . .
405
6.8‚àó
Bilinear and Quadratic Forms
. . . . . . . . . . . . . . . . .
422
6.9‚àó
Einstein‚Äôs Special Theory of Relativity . . . . . . . . . . . . .
451
6.10‚àóConditioning and the Rayleigh Quotient . . . . . . . . . . . .
464
6.11‚àóThe Geometry of Orthogonal Operators . . . . . . . . . . . .
472
Index of DeÔ¨Ånitions
. . . . . . . . . . . . . . . . . . . . . . .
480

Table of Contents
vii
7 Canonical Forms
482
7.1
The Jordan Canonical Form I . . . . . . . . . . . . . . . . . .
482
7.2
The Jordan Canonical Form II . . . . . . . . . . . . . . . . .
497
7.3
The Minimal Polynomial
. . . . . . . . . . . . . . . . . . . .
516
7.4‚àó
The Rational Canonical Form . . . . . . . . . . . . . . . . . .
524
Index of DeÔ¨Ånitions
. . . . . . . . . . . . . . . . . . . . . . .
548
Appendices
549
A
Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
549
B
Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
551
C
Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
552
D
Complex Numbers . . . . . . . . . . . . . . . . . . . . . . . .
555
E
Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . .
561
Answers to Selected Exercises
571

This page intentionally left blank 

Preface
The language and concepts of matrix theory and, more generally, of linear
algebra have come into widespread usage in the social and natural sciences,
computer science, and statistics. In addition, linear algebra continues to be
of great importance in modern treatments of geometry and analysis.
The primary purpose of this fourth edition of Linear Algebra is to present
a careful treatment of the principal topics of linear algebra and to illustrate
the power of the subject through a variety of applications. Our major thrust
emphasizes the symbiotic relationship between linear transformations and
matrices. However, where appropriate, theorems are stated in the more gen-
eral inÔ¨Ånite-dimensional case. For example, this theory is applied to Ô¨Ånding
solutions to a homogeneous linear diÔ¨Äerential equation and the best approx-
imation by a trigonometric polynomial to a continuous function.
Although the only formal prerequisite for this book is a one-year course
in calculus, it requires the mathematical sophistication of typical junior and
senior mathematics majors. This book is especially suited for a second course
in linear algebra that emphasizes abstract vector spaces, although it can be
used in a Ô¨Årst course with a strong theoretical emphasis.
The book is organized to permit a number of diÔ¨Äerent courses (ranging
from three to eight semester hours in length) to be taught from it.
The
core material (vector spaces, linear transformations and matrices, systems of
linear equations, determinants, diagonalization, and inner product spaces) is
found in Chapters 1 through 5 and Sections 6.1 through 6.5. Chapters 6 and
7, on inner product spaces and canonical forms, are completely independent
and may be studied in either order. In addition, throughout the book are
applications to such areas as diÔ¨Äerential equations, economics, geometry, and
physics. These applications are not central to the mathematical development,
however, and may be excluded at the discretion of the instructor.
We have attempted to make it possible for many of the important topics
of linear algebra to be covered in a one-semester course. This goal has led
us to develop the major topics with fewer preliminaries than in a traditional
approach. (Our treatment of the Jordan canonical form, for instance, does
not require any theory of polynomials.) The resulting economy permits us to
cover the core material of the book (omitting many of the optional sections
and a detailed discussion of determinants) in a one-semester four-hour course
for students who have had some prior exposure to linear algebra.
Chapter 1 of the book presents the basic theory of vector spaces: sub-
spaces, linear combinations, linear dependence and independence, bases, and
dimension. The chapter concludes with an optional section in which we prove
ix

x
Preface
that every inÔ¨Ånite-dimensional vector space has a basis.
Linear transformations and their relationship to matrices are the subject
of Chapter 2. We discuss the null space and range of a linear transformation,
matrix representations of a linear transformation, isomorphisms, and change
of coordinates.
Optional sections on dual spaces and homogeneous linear
diÔ¨Äerential equations end the chapter.
The application of vector space theory and linear transformations to sys-
tems of linear equations is found in Chapter 3. We have chosen to defer this
important subject so that it can be presented as a consequence of the pre-
ceding material. This approach allows the familiar topic of linear systems to
illuminate the abstract theory and permits us to avoid messy matrix computa-
tions in the presentation of Chapters 1 and 2. There are occasional examples
in these chapters, however, where we solve systems of linear equations. (Of
course, these examples are not a part of the theoretical development.) The
necessary background is contained in Section 1.4.
Determinants, the subject of Chapter 4, are of much less importance than
they once were. In a short course (less than one year), we prefer to treat
determinants lightly so that more time may be devoted to the material in
Chapters 5 through 7. Consequently we have presented two alternatives in
Chapter 4‚Äîa complete development of the theory (Sections 4.1 through 4.3)
and a summary of important facts that are needed for the remaining chapters
(Section 4.4). Optional Section 4.5 presents an axiomatic development of the
determinant.
Chapter 5 discusses eigenvalues, eigenvectors, and diagonalization. One of
the most important applications of this material occurs in computing matrix
limits. We have therefore included an optional section on matrix limits and
Markov chains in this chapter even though the most general statement of some
of the results requires a knowledge of the Jordan canonical form. Section 5.4
contains material on invariant subspaces and the Cayley‚ÄìHamilton theorem.
Inner product spaces are the subject of Chapter 6.
The basic mathe-
matical theory (inner products; the Gram‚ÄìSchmidt process; orthogonal com-
plements; the adjoint of an operator; normal, self-adjoint, orthogonal and
unitary operators; orthogonal projections; and the spectral theorem) is con-
tained in Sections 6.1 through 6.6. Sections 6.7 through 6.11 contain diverse
applications of the rich inner product space structure.
Canonical forms are treated in Chapter 7. Sections 7.1 and 7.2 develop
the Jordan canonical form, Section 7.3 presents the minimal polynomial, and
Section 7.4 discusses the rational canonical form.
There are Ô¨Åve appendices. The Ô¨Årst four, which discuss sets, functions,
Ô¨Åelds, and complex numbers, respectively, are intended to review basic ideas
used throughout the book.
Appendix E on polynomials is used primarily
in Chapters 5 and 7, especially in Section 7.4. We prefer to cite particular
results from the appendices as needed rather than to discuss the appendices

Preface
xi
independently.
The following diagram illustrates the dependencies among the various
chapters.
Chapter 1
?
Chapter 2
?
Chapter 3
?
Sections 4.1‚Äì4.3
or Section 4.4
?
Sections 5.1 and 5.2
- Chapter 6
?
Section 5.4
?
Chapter 7
One Ô¨Ånal word is required about our notation. Sections and subsections
labeled with an asterisk (‚àó) are optional and may be omitted as the instructor
sees Ô¨Åt. An exercise accompanied by the dagger symbol (‚Ä†) is not optional,
however‚Äîwe use this symbol to identify an exercise that is cited in some later
section that is not optional.
DIFFERENCES BETWEEN THE THIRD AND FOURTH EDITIONS
The principal content change of this fourth edition is the inclusion of a
new section (Section 6.7) discussing the singular value decomposition and
the pseudoinverse of a matrix or a linear transformation between Ô¨Ånite-
dimensional inner product spaces. Our approach is to treat this material as
a generalization of our characterization of normal and self-adjoint operators.
The organization of the text is essentially the same as in the third edition.
Nevertheless, this edition contains many signiÔ¨Åcant local changes that im-

xii
Preface
prove the book. Section 5.1 (Eigenvalues and Eigenvectors) has been stream-
lined, and some material previously in Section 5.1 has been moved to Sec-
tion 2.5 (The Change of Coordinate Matrix). Further improvements include
revised proofs of some theorems, additional examples, new exercises, and
literally hundreds of minor editorial changes.
We are especially indebted to Jane M. Day (San Jose State University)
for her extensive and detailed comments on the fourth edition manuscript.
Additional comments were provided by the following reviewers of the fourth
edition manuscript: Thomas BanchoÔ¨Ä(Brown University), Christopher Heil
(Georgia Institute of Technology), and Thomas Shemanske (Dartmouth Col-
lege).
To Ô¨Ånd the latest information about this book, consult our web site on
the World Wide Web. We encourage comments, which can be sent to us by
e-mail or ordinary post. Our web site and e-mail addresses are listed below.
web site:
http://www.math.ilstu.edu/linalg
e-mail:
linalg@math.ilstu.edu
Stephen H. Friedberg
Arnold J. Insel
Lawrence E. Spence

1
Vector Spaces
1.1
Introduction
1.2
Vector Spaces
1.3
Subspaces
1.4
Linear Combinations and Systems of Linear Equations
1.5
Linear Dependence and Linear Independence
1.6
Bases and Dimension
1.7*
Maximal Linearly Independent Subsets
1.1
INTRODUCTION
Many familiar physical notions, such as forces, velocities,1 and accelerations,
involve both a magnitude (the amount of the force, velocity, or acceleration)
and a direction. Any such entity involving both magnitude and direction is
called a ‚Äúvector.‚Äù A vector is represented by an arrow whose length denotes
the magnitude of the vector and whose direction represents the direction of
the vector. In most physical situations involving vectors, only the magnitude
and direction of the vector are signiÔ¨Åcant; consequently, we regard vectors
with the same magnitude and direction as being equal irrespective of their
positions. In this section the geometry of vectors is discussed. This geometry
is derived from physical experiments that test the manner in which two vectors
interact.
Familiar situations suggest that when two like physical quantities act si-
multaneously at a point, the magnitude of their eÔ¨Äect need not equal the sum
of the magnitudes of the original quantities. For example, a swimmer swim-
ming upstream at the rate of 2 miles per hour against a current of 1 mile per
hour does not progress at the rate of 3 miles per hour. For in this instance
the motions of the swimmer and current oppose each other, and the rate of
progress of the swimmer is only 1 mile per hour upstream. If, however, the
1The word velocity is being used here in its scientiÔ¨Åc sense‚Äîas an entity having
both magnitude and direction. The magnitude of a velocity (without regard for the
direction of motion) is called its speed.
1

2
Chap. 1
Vector Spaces
swimmer is moving downstream (with the current), then his or her rate of
progress is 3 miles per hour downstream.
Experiments show that if two like quantities act together, their eÔ¨Äect is
predictable. In this case, the vectors used to represent these quantities can be
combined to form a resultant vector that represents the combined eÔ¨Äects of
the original quantities. This resultant vector is called the sum of the original
vectors, and the rule for their combination is called the parallelogram law.
(See Figure 1.1.)

:






3
x
y
x + y
P
Q
Figure 1.1
Parallelogram Law for Vector Addition.
The sum of two vectors
x and y that act at the same point P is the vector beginning at P that is
represented by the diagonal of parallelogram having x and y as adjacent sides.
Since opposite sides of a parallelogram are parallel and of equal length, the
endpoint Q of the arrow representing x + y can also be obtained by allowing
x to act at P and then allowing y to act at the endpoint of x. Similarly, the
endpoint of the vector x + y can be obtained by Ô¨Årst permitting y to act at
P and then allowing x to act at the endpoint of y. Thus two vectors x and
y that both act at the point P may be added ‚Äútail-to-head‚Äù; that is, either
x or y may be applied at P and a vector having the same magnitude and
direction as the other may be applied to the endpoint of the Ô¨Årst. If this is
done, the endpoint of the second vector is the endpoint of x + y.
The addition of vectors can be described algebraically with the use of
analytic geometry. In the plane containing x and y, introduce a coordinate
system with P at the origin. Let (a1, a2) denote the endpoint of x and (b1, b2)
denote the endpoint of y. Then as Figure 1.2(a) shows, the endpoint Q of x+y
is (a1 + b1, a2 + b2). Henceforth, when a reference is made to the coordinates
of the endpoint of a vector, the vector should be assumed to emanate from
the origin. Moreover, since a vector beginning at the origin is completely
determined by its endpoint, we sometimes refer to the point x rather than
the endpoint of the vector x if x is a vector emanating from the origin.
Besides the operation of vector addition, there is another natural operation
that can be performed on vectors‚Äîthe length of a vector may be magniÔ¨Åed

Sec. 1.1
Introduction
3

:






3
x
y
x + y
P
Q
(a1 + b1, a2 + b2)
(a1 + b1, b2)
(b1, b2)
(a1, a2)
(a)

















x
tx
(ta1, ta2)
(a1, a2)
(b)
a1
ta1
Figure 1.2
or contracted. This operation, called scalar multiplication, consists of mul-
tiplying the vector by a real number. If the vector x is represented by an
arrow, then for any real number t, the vector tx is represented by an arrow in
the same direction if t ‚â•0 and in the opposite direction if t < 0. The length
of the arrow tx is |t| times the length of the arrow x. Two nonzero vectors
x and y are called parallel if y = tx for some nonzero real number t. (Thus
nonzero vectors having the same or opposite directions are parallel.)
To describe scalar multiplication algebraically, again introduce a coordi-
nate system into a plane containing the vector x so that x emanates from the
origin. If the endpoint of x has coordinates (a1, a2), then the coordinates of
the endpoint of tx are easily seen to be (ta1, ta2). (See Figure 1.2(b).)
The algebraic descriptions of vector addition and scalar multiplication for
vectors in a plane yield the following properties:
1. For all vectors x and y, x + y = y + x.
2. For all vectors x, y, and z, (x + y) + z = x + (y + z).
3. There exists a vector denoted 0 such that x + 0 = x for each vector x.
4. For each vector x, there is a vector y such that x + y = 0.
5. For each vector x, 1x = x.
6. For each pair of real numbers a and b and each vector x, (ab)x = a(bx).
7. For each real number a and each pair of vectors x and y, a(x + y) =
ax + ay.
8. For each pair of real numbers a and b and each vector x, (a + b)x =
ax + bx.
Arguments similar to the preceding ones show that these eight properties,
as well as the geometric interpretations of vector addition and scalar multipli-
cation, are true also for vectors acting in space rather than in a plane. These
results can be used to write equations of lines and planes in space.

4
Chap. 1
Vector Spaces
Consider Ô¨Årst the equation of a line in space that passes through two
distinct points A and B. Let O denote the origin of a coordinate system in
space, and let u and v denote the vectors that begin at O and end at A and
B, respectively. If w denotes the vector beginning at A and ending at B, then
‚Äútail-to-head‚Äù addition shows that u+w = v, and hence w = v‚àíu, where ‚àíu
denotes the vector (‚àí1)u. (See Figure 1.3, in which the quadrilateral OABC
is a parallelogram.) Since a scalar multiple of w is parallel to w but possibly
of a diÔ¨Äerent length than w, any point on the line joining A and B may be
obtained as the endpoint of a vector that begins at A and has the form tw
for some real number t. Conversely, the endpoint of every vector of the form
tw that begins at A lies on the line joining A and B. Thus an equation of the
line through A and B is x = u + tw = u + t(v ‚àíu), where t is a real number
and x denotes an arbitrary point on the line. Notice also that the endpoint
C of the vector v ‚àíu in Figure 1.3 has coordinates equal to the diÔ¨Äerence of
the coordinates of B and A.







:
j
j
O
A
B
C
u
v
v ‚àíu
w
Figure 1.3
Example 1
Let A and B be points having coordinates (‚àí2, 0, 1) and (4, 5, 3), respectively.
The endpoint C of the vector emanating from the origin and having the same
direction as the vector beginning at A and terminating at B has coordinates
(4, 5, 3) ‚àí(‚àí2, 0, 1) = (6, 5, 2). Hence the equation of the line through A and
B is
x = (‚àí2, 0, 1) + t(6, 5, 2).
‚ô¶
Now let A, B, and C denote any three noncollinear points in space. These
points determine a unique plane, and its equation can be found by use of our
previous observations about vectors. Let u and v denote vectors beginning at
A and ending at B and C, respectively. Observe that any point in the plane
containing A, B, and C is the endpoint S of a vector x beginning at A and
having the form su + tv for some real numbers s and t. The endpoint of su is
the point of intersection of the line through A and B with the line through S

Sec. 1.1
Introduction
5














3
-
-
A
B
C
S
u
su
x
tv
v
Figure 1.4
parallel to the line through A and C. (See Figure 1.4.) A similar procedure
locates the endpoint of tv. Moreover, for any real numbers s and t, the vector
su + tv lies in the plane containing A, B, and C. It follows that an equation
of the plane containing A, B, and C is
x = A + su + tv,
where s and t are arbitrary real numbers and x denotes an arbitrary point in
the plane.
Example 2
Let A, B, and C be the points having coordinates (1, 0, 2), (‚àí3, ‚àí2, 4), and
(1, 8, ‚àí5), respectively. The endpoint of the vector emanating from the origin
and having the same length and direction as the vector beginning at A and
terminating at B is
(‚àí3, ‚àí2, 4) ‚àí(1, 0, 2) = (‚àí4, ‚àí2, 2).
Similarly, the endpoint of a vector emanating from the origin and having the
same length and direction as the vector beginning at A and terminating at C
is (1, 8, ‚àí5)‚àí(1, 0, 2) = (0, 8, ‚àí7). Hence the equation of the plane containing
the three given points is
x = (1, 0, 2) + s(‚àí4, ‚àí2, 2) + t(0, 8, ‚àí7).
‚ô¶
Any mathematical structure possessing the eight properties on page 3 is
called a vector space. In the next section we formally deÔ¨Åne a vector space
and consider many examples of vector spaces other than the ones mentioned
above.
EXERCISES
1.
Determine whether the vectors emanating from the origin and termi-
nating at the following pairs of points are parallel.

6
Chap. 1
Vector Spaces
(a)
(3, 1, 2) and (6, 4, 2)
(b)
(‚àí3, 1, 7) and (9, ‚àí3, ‚àí21)
(c)
(5, ‚àí6, 7) and (‚àí5, 6, ‚àí7)
(d)
(2, 0, ‚àí5) and (5, 0, ‚àí2)
2.
Find the equations of the lines through the following pairs of points in
space.
(a)
(3, ‚àí2, 4) and (‚àí5, 7, 1)
(b)
(2, 4, 0) and (‚àí3, ‚àí6, 0)
(c)
(3, 7, 2) and (3, 7, ‚àí8)
(d)
(‚àí2, ‚àí1, 5) and (3, 9, 7)
3.
Find the equations of the planes containing the following points in space.
(a)
(2, ‚àí5, ‚àí1), (0, 4, 6), and (‚àí3, 7, 1)
(b)
(3, ‚àí6, 7), (‚àí2, 0, ‚àí4), and (5, ‚àí9, ‚àí2)
(c)
(‚àí8, 2, 0), (1, 3, 0), and (6, ‚àí5, 0)
(d)
(1, 1, 1), (5, 5, 5), and (‚àí6, 4, 2)
4.
What are the coordinates of the vector 0 in the Euclidean plane that
satisÔ¨Åes property 3 on page 3? Justify your answer.
5.
Prove that if the vector x emanates from the origin of the Euclidean
plane and terminates at the point with coordinates (a1, a2), then the
vector tx that emanates from the origin terminates at the point with
coordinates (ta1, ta2).
6.
Show that the midpoint of the line segment joining the points (a, b) and
(c, d) is ((a + c)/2, (b + d)/2).
7.
Prove that the diagonals of a parallelogram bisect each other.
1.2
VECTOR SPACES
In Section 1.1, we saw that with the natural deÔ¨Ånitions of vector addition and
scalar multiplication, the vectors in a plane satisfy the eight properties listed
on page 3. Many other familiar algebraic systems also permit deÔ¨Ånitions of
addition and scalar multiplication that satisfy the same eight properties. In
this section, we introduce some of these systems, but Ô¨Årst we formally deÔ¨Åne
this type of algebraic structure.
DeÔ¨Ånitions.
A vector space (or linear space) V over a Ô¨Åeld 2 F
consists of a set on which two operations (called addition and scalar mul-
tiplication, respectively) are deÔ¨Åned so that for each pair of elements x, y,
2Fields are discussed in Appendix C.

Sec. 1.2
Vector Spaces
7
in V there is a unique element x + y in V, and for each element a in F and
each element x in V there is a unique element ax in V, such that the following
conditions hold.
(VS 1) For all x, y in V, x + y = y + x (commutativity of addition).
(VS 2) For all x, y, z in V, (x + y) + z = x + (y + z) (associativity of
addition).
(VS 3) There exists an element in V denoted by 0 such that x + 0 = x for
each x in V.
(VS 4) For each element x in V there exists an element y in V such that
x + y = 0.
(VS 5) For each element x in V, 1x = x.
(VS 6) For each pair of elements a, b in F and each element x in V,
(ab)x = a(bx).
(VS 7) For each element a in F and each pair of elements x, y in V,
a(x + y) = ax + ay.
(VS 8) For each pair of elements a, b in F and each element x in V,
(a + b)x = ax + bx.
The elements x + y and ax are called the sum of x and y and the product
of a and x, respectively.
The elements of the Ô¨Åeld F are called scalars and the elements of the
vector space V are called vectors. The reader should not confuse this use of
the word ‚Äúvector‚Äù with the physical entity discussed in Section 1.1: the word
‚Äúvector‚Äù is now being used to describe any element of a vector space.
A vector space is frequently discussed in the text without explicitly men-
tioning its Ô¨Åeld of scalars. The reader is cautioned to remember, however,
that every vector space is regarded as a vector space over a given Ô¨Åeld, which
is denoted by F. Occasionally we restrict our attention to the Ô¨Åelds of real
and complex numbers, which are denoted R and C, respectively.
Observe that (VS 2) permits us to unambiguously deÔ¨Åne the addition of
any Ô¨Ånite number of vectors (without the use of parentheses).
In the remainder of this section we introduce several important examples
of vector spaces that are studied throughout this text. Observe that in de-
scribing a vector space, it is necessary to specify not only the vectors but also
the operations of addition and scalar multiplication.
An object of the form (a1, a2, . . . , an), where the entries a1, a2, . . . , an are
elements of a Ô¨Åeld F, is called an n-tuple with entries from F. The elements

8
Chap. 1
Vector Spaces
a1, a2, . . . , an are called the entries or components of the n-tuple. Two
n-tuples (a1, a2, . . . , an) and (b1, b2, . . . , bn) with entries from a Ô¨Åeld F are
called equal if ai = bi for i = 1, 2, . . . , n.
Example 1
The set of all n-tuples with entries from a Ô¨Åeld F is denoted by Fn. This set is a
vector space over F with the operations of coordinatewise addition and scalar
multiplication; that is, if u = (a1, a2, . . . , an) ‚ààFn, v = (b1, b2 . . . , bn) ‚ààFn,
and c ‚ààF, then
u + v = (a1 + b1, a2 + b2, . . . , an + bn)
and
cu = (ca1, ca2, . . . , can).
Thus R3 is a vector space over R. In this vector space,
(3, ‚àí2, 0) + (‚àí1, 1, 4) = (2, ‚àí1, 4)
and
‚àí5(1, ‚àí2, 0) = (‚àí5, 10, 0).
Similarly, C2 is a vector space over C. In this vector space,
(1 + i, 2) + (2 ‚àí3i, 4i) = (3 ‚àí2i, 2 + 4i)
and
i(1 + i, 2) = (‚àí1 + i, 2i).
Vectors in Fn may be written as column vectors
‚éõ
‚éú
‚éú
‚éú
‚éù
a1
a2
...
an
‚éû
‚éü
‚éü
‚éü
‚é†
rather than as row vectors (a1, a2, . . . , an). Since a 1-tuple whose only entry
is from F can be regarded as an element of F, we usually write F rather than
F1 for the vector space of 1-tuples with entry from F.
‚ô¶
An m√ón matrix with entries from a Ô¨Åeld F is a rectangular array of the
form
‚éõ
‚éú
‚éú
‚éú
‚éù
a11
a12
¬∑ ¬∑ ¬∑
a1n
a21
a22
¬∑ ¬∑ ¬∑
a2n
...
...
...
am1
am2
¬∑ ¬∑ ¬∑
amn
‚éû
‚éü
‚éü
‚éü
‚é†,
where each entry aij (1 ‚â§i ‚â§m, 1 ‚â§j ‚â§n) is an element of F.
We
call the entries aij with i = j the diagonal entries of the matrix.
The
entries ai1, ai2, . . . , ain compose the ith row of the matrix, and the entries
a1j, a2j, . . . , amj compose the j th column of the matrix. The rows of the
preceding matrix are regarded as vectors in Fn, and the columns are regarded
as vectors in Fm. The m √ó n matrix in which each entry equals zero is called
the zero matrix and is denoted by O.

Sec. 1.2
Vector Spaces
9
In this book, we denote matrices by capital italic letters (e.g., A, B, and
C), and we denote the entry of a matrix A that lies in row i and column j by
Aij. In addition, if the number of rows and columns of a matrix are equal,
the matrix is called square.
Two m √ó n matrices A and B are called equal if all their corresponding
entries are equal, that is, if Aij = Bij for 1 ‚â§i ‚â§m and 1 ‚â§j ‚â§n.
Example 2
The set of all m√ón matrices with entries from a Ô¨Åeld F is a vector space, which
we denote by Mm√ón(F), with the following operations of matrix addition
and scalar multiplication: For A, B ‚ààMm√ón(F) and c ‚ààF,
(A + B)ij = Aij + Bij
and
(cA)ij = cAij
for 1 ‚â§i ‚â§m and 1 ‚â§j ‚â§n. For instance,

2
0
‚àí1
1
‚àí3
4
	
+

‚àí5
‚àí2
6
3
4
‚àí1
	
=

‚àí3
‚àí2
5
4
1
3
	
and
‚àí3

1
0
‚àí2
‚àí3
2
3
	
=

‚àí3
0
6
9
‚àí6
‚àí9
	
in M2√ó3(R).
‚ô¶
Example 3
Let S be any nonempty set and F be any Ô¨Åeld, and let F(S, F) denote the
set of all functions from S to F. Two functions f and g in F(S, F) are called
equal if f(s) = g(s) for each s ‚ààS. The set F(S, F) is a vector space with
the operations of addition and scalar multiplication deÔ¨Åned for f, g ‚ààF(S, F)
and c ‚ààF by
(f + g)(s) = f(s) + g(s)
and
(cf)(s) = c[f(s)]
for each s ‚ààS. Note that these are the familiar operations of addition and
scalar multiplication for functions used in algebra and calculus.
‚ô¶
A polynomial with coeÔ¨Écients from a Ô¨Åeld F is an expression of the form
f(x) = anxn + an‚àí1xn‚àí1 + ¬∑ ¬∑ ¬∑ + a1x + a0,
where n is a nonnegative integer and each ak, called the coeÔ¨Écient of xk, is
in F. If f(x) = 0, that is, if an = an‚àí1 = ¬∑ ¬∑ ¬∑ = a0 = 0, then f(x) is called
the zero polynomial and, for convenience, its degree is deÔ¨Åned to be ‚àí1;

10
Chap. 1
Vector Spaces
otherwise, the degree of a polynomial is deÔ¨Åned to be the largest exponent
of x that appears in the representation
f(x) = anxn + an‚àí1xn‚àí1 + ¬∑ ¬∑ ¬∑ + a1x + a0
with a nonzero coeÔ¨Écient. Note that the polynomials of degree zero may be
written in the form f(x) = c for some nonzero scalar c. Two polynomials,
f(x) = anxn + an‚àí1xn‚àí1 + ¬∑ ¬∑ ¬∑ + a1x + a0
and
g(x) = bmxm + bm‚àí1xm‚àí1 + ¬∑ ¬∑ ¬∑ + b1x + b0,
are called equal if m = n and ai = bi for i = 0, 1, . . . , n.
When F is a Ô¨Åeld containing inÔ¨Ånitely many scalars, we usually regard
a polynomial with coeÔ¨Écients from F as a function from F into F.
(See
page 569.) In this case, the value of the function
f(x) = anxn + an‚àí1xn‚àí1 + ¬∑ ¬∑ ¬∑ + a1x + a0
at c ‚ààF is the scalar
f(c) = ancn + an‚àí1cn‚àí1 + ¬∑ ¬∑ ¬∑ + a1c + a0.
Here either of the notations f or f(x) is used for the polynomial function
f(x) = anxn + an‚àí1xn‚àí1 + ¬∑ ¬∑ ¬∑ + a1x + a0.
Example 4
Let
f(x) = anxn + an‚àí1xn‚àí1 + ¬∑ ¬∑ ¬∑ + a1x + a0
and
g(x) = bmxm + bm‚àí1xm‚àí1 + ¬∑ ¬∑ ¬∑ + b1x + b0
be polynomials with coeÔ¨Écients from a Ô¨Åeld F. Suppose that m ‚â§n, and
deÔ¨Åne bm+1 = bm+2 = ¬∑ ¬∑ ¬∑ = bn = 0. Then g(x) can be written as
g(x) = bnxn + bn‚àí1xn‚àí1 + ¬∑ ¬∑ ¬∑ + b1x + b0.
DeÔ¨Åne
f(x) + g(x) = (an + bn)xn+(an‚àí1+ bn‚àí1)xn‚àí1+¬∑ ¬∑ ¬∑+(a1 + b1)x+(a0 + b0)
and for any c ‚ààF, deÔ¨Åne
cf(x) = canxn + can‚àí1xn‚àí1 + ¬∑ ¬∑ ¬∑ + ca1x + ca0.
With these operations of addition and scalar multiplication, the set of all
polynomials with coeÔ¨Écients from F is a vector space, which we denote by
P(F).
‚ô¶

Sec. 1.2
Vector Spaces
11
We will see in Exercise 23 of Section 2.4 that the vector space deÔ¨Åned in
the next example is essentially the same as P(F).
Example 5
Let F be any Ô¨Åeld. A sequence in F is a function œÉ from the positive integers
into F. In this book, the sequence œÉ such that œÉ(n) = an for n = 1, 2, . . . is
denoted {an}. Let V consist of all sequences {an} in F that have only a Ô¨Ånite
number of nonzero terms an. If {an} and {bn} are in V and t ‚ààF, deÔ¨Åne
{an} + {bn} = {an + bn}
and
t{an} = {tan}.
With these operations V is a vector space.
‚ô¶
Our next two examples contain sets on which addition and scalar multi-
plication are deÔ¨Åned, but which are not vector spaces.
Example 6
Let S = {(a1, a2): a1, a2 ‚ààR}. For (a1, a2), (b1, b2) ‚ààS and c ‚ààR, deÔ¨Åne
(a1, a2) + (b1, b2) = (a1 + b1, a2 ‚àíb2)
and
c(a1, a2) = (ca1, ca2).
Since (VS 1), (VS 2), and (VS 8) fail to hold, S is not a vector space with
these operations.
‚ô¶
Example 7
Let S be as in Example 6. For (a1, a2), (b1, b2) ‚ààS and c ‚ààR, deÔ¨Åne
(a1, a2) + (b1, b2) = (a1 + b1, 0)
and
c(a1, a2) = (ca1, 0).
Then S is not a vector space with these operations because (VS 3) (hence
(VS 4)) and (VS 5) fail.
‚ô¶
We conclude this section with a few of the elementary consequences of the
deÔ¨Ånition of a vector space.
Theorem 1.1 (Cancellation Law for Vector Addition).
If x, y,
and z are vectors in a vector space V such that x + z = y + z, then x = y.
Proof. There exists a vector v in V such that z + v = 0 (VS 4). Thus
x = x + 0 = x + (z + v) = (x + z) + v
= (y + z) + v = y + (z + v) = y + 0 = y
by (VS 2) and (VS 3).
Corollary 1. The vector 0 described in (VS 3) is unique.

12
Chap. 1
Vector Spaces
Proof. Exercise.
Corollary 2. The vector y described in (VS 4) is unique.
Proof. Exercise.
The vector 0 in (VS 3) is called the zero vector of V, and the vector y in
(VS 4) (that is, the unique vector such that x+y = 0) is called the additive
inverse of x and is denoted by ‚àíx.
The next result contains some of the elementary properties of scalar mul-
tiplication.
Theorem 1.2. In any vector space V, the following statements are true:
(a) 0x = 0 for each x ‚ààV.
(b) (‚àía)x = ‚àí(ax) = a(‚àíx) for each a ‚ààF and each x ‚ààV.
(c) a0 = 0 for each a ‚ààF.
Proof. (a) By (VS 8), (VS 3), and (VS 1), it follows that
0x + 0x = (0 + 0)x = 0x = 0x + 0 = 0 + 0x.
Hence 0x = 0 by Theorem 1.1.
(b) The vector ‚àí(ax) is the unique element of V such that ax+[‚àí(ax)] =
0. Thus if ax+(‚àía)x = 0, Corollary 2 to Theorem 1.1 implies that (‚àía)x =
‚àí(ax). But by (VS 8),
ax + (‚àía)x = [a + (‚àía)]x = 0x = 0
by (a).
Consequently (‚àía)x = ‚àí(ax).
In particular, (‚àí1)x = ‚àíx.
So,
by (VS 6),
a(‚àíx) = a[(‚àí1)x] = [a(‚àí1)]x = (‚àía)x.
The proof of (c) is similar to the proof of (a).
EXERCISES
1.
Label the following statements as true or false.
(a)
Every vector space contains a zero vector.
(b)
A vector space may have more than one zero vector.
(c)
In any vector space, ax = bx implies that a = b.
(d)
In any vector space, ax = ay implies that x = y.
(e)
A vector in Fn may be regarded as a matrix in Mn√ó1(F).
(f)
An m √ó n matrix has m columns and n rows.
(g)
In P(F), only polynomials of the same degree may be added.
(h)
If f and g are polynomials of degree n, then f + g is a polynomial
of degree n.
(i)
If f is a polynomial of degree n and c is a nonzero scalar, then cf
is a polynomial of degree n.

Sec. 1.2
Vector Spaces
13
(j)
A nonzero scalar of F may be considered to be a polynomial in
P(F) having degree zero.
(k)
Two functions in F(S, F) are equal if and only if they have the
same value at each element of S.
2.
Write the zero vector of M3√ó4(F).
3.
If
M =

1
2
3
4
5
6
	
,
what are M13, M21, and M22?
4.
Perform the indicated operations.
(a)

2
5
‚àí3
1
0
7
	
+

4
‚àí2
5
‚àí5
3
2
	
(b)
‚éõ
‚éù
‚àí6
4
3
‚àí2
1
8
‚éû
‚é†+
‚éõ
‚éù
7
‚àí5
0
‚àí3
2
0
‚éû
‚é†
(c)
4

2
5
‚àí3
1
0
7
	
(d)
‚àí5
‚éõ
‚éù
‚àí6
4
3
‚àí2
1
8
‚éû
‚é†
(e)
(2x4 ‚àí7x3 + 4x + 3) + (8x3 + 2x2 ‚àí6x + 7)
(f)
(‚àí3x3 + 7x2 + 8x ‚àí6) + (2x3 ‚àí8x + 10)
(g)
5(2x7 ‚àí6x4 + 8x2 ‚àí3x)
(h)
3(x5 ‚àí2x3 + 4x + 2)
Exercises 5 and 6 show why the deÔ¨Ånitions of matrix addition and scalar
multiplication (as deÔ¨Åned in Example 2) are the appropriate ones.
5.
Richard Gard (‚ÄúEÔ¨Äects of Beaver on Trout in Sagehen Creek, Cali-
fornia,‚Äù J. Wildlife Management, 25, 221-242) reports the following
number of trout having crossed beaver dams in Sagehen Creek.
Upstream Crossings
Fall
Spring
Summer
Brook trout
8
3
1
Rainbow trout
3
0
0
Brown trout
3
0
0

14
Chap. 1
Vector Spaces
Downstream Crossings
Fall
Spring
Summer
Brook trout
9
1
4
Rainbow trout
3
0
0
Brown trout
1
1
0
Record the upstream and downstream crossings in two 3 √ó 3 matrices,
and verify that the sum of these matrices gives the total number of
crossings (both upstream and downstream) categorized by trout species
and season.
6.
At the end of May, a furniture store had the following inventory.
Early
Mediter-
American
Spanish
ranean
Danish
Living room suites
4
2
1
3
Bedroom suites
5
1
1
4
Dining room suites
3
1
2
6
Record these data as a 3 √ó 4 matrix M. To prepare for its June sale,
the store decided to double its inventory on each of the items listed in
the preceding table. Assuming that none of the present stock is sold
until the additional furniture arrives, verify that the inventory on hand
after the order is Ô¨Ålled is described by the matrix 2M. If the inventory
at the end of June is described by the matrix
A =
‚éõ
‚éù
5
3
1
2
6
2
1
5
1
0
3
3
‚éû
‚é†,
interpret 2M ‚àíA. How many suites were sold during the June sale?
7.
Let S = {0, 1} and F = R. In F(S, R), show that f = g and f + g = h,
where f(t) = 2t + 1, g(t) = 1 + 4t ‚àí2t2, and h(t) = 5t + 1.
8.
In any vector space V, show that (a + b)(x + y) = ax + ay + bx + by for
any x, y ‚ààV and any a, b ‚ààF.
9.
Prove Corollaries 1 and 2 of Theorem 1.1 and Theorem 1.2(c).
10.
Let V denote the set of all diÔ¨Äerentiable real-valued functions deÔ¨Åned
on the real line. Prove that V is a vector space with the operations of
addition and scalar multiplication deÔ¨Åned in Example 3.

Sec. 1.2
Vector Spaces
15
11.
Let V = {0} consist of a single vector 0 and deÔ¨Åne 0 + 0 = 0 and
c0 = 0 for each scalar c in F. Prove that V is a vector space over F.
(V is called the zero vector space.)
12.
A real-valued function f deÔ¨Åned on the real line is called an even func-
tion if f(‚àít) = f(t) for each real number t. Prove that the set of even
functions deÔ¨Åned on the real line with the operations of addition and
scalar multiplication deÔ¨Åned in Example 3 is a vector space.
13.
Let V denote the set of ordered pairs of real numbers. If (a1, a2) and
(b1, b2) are elements of V and c ‚ààR, deÔ¨Åne
(a1, a2) + (b1, b2) = (a1 + b1, a2b2)
and
c(a1, a2) = (ca1, a2).
Is V a vector space over R with these operations? Justify your answer.
14.
Let V = {(a1, a2, . . . , an): ai ‚ààC for i = 1, 2, . . . n}; so V is a vector
space over C by Example 1. Is V a vector space over the Ô¨Åeld of real
numbers with the operations of coordinatewise addition and multipli-
cation?
15.
Let V = {(a1, a2, . . . , an): ai ‚ààR for i = 1, 2, . . . n}; so V is a vec-
tor space over R by Example 1. Is V a vector space over the Ô¨Åeld of
complex numbers with the operations of coordinatewise addition and
multiplication?
16.
Let V denote the set of all m √ó n matrices with real entries; so V
is a vector space over R by Example 2. Let F be the Ô¨Åeld of rational
numbers. Is V a vector space over F with the usual deÔ¨Ånitions of matrix
addition and scalar multiplication?
17.
Let V = {(a1, a2): a1, a2 ‚ààF}, where F is a Ô¨Åeld. DeÔ¨Åne addition of
elements of V coordinatewise, and for c ‚ààF and (a1, a2) ‚ààV, deÔ¨Åne
c(a1, a2) = (a1, 0).
Is V a vector space over F with these operations? Justify your answer.
18.
Let V = {(a1, a2): a1, a2 ‚ààR}. For (a1, a2), (b1, b2) ‚ààV and c ‚ààR,
deÔ¨Åne
(a1, a2) + (b1, b2) = (a1 + 2b1, a2 + 3b2)
and
c(a1, a2) = (ca1, ca2).
Is V a vector space over R with these operations? Justify your answer.

16
Chap. 1
Vector Spaces
19.
Let V = {(a1, a2): a1, a2 ‚ààR}. DeÔ¨Åne addition of elements of V coor-
dinatewise, and for (a1, a2) in V and c ‚ààR, deÔ¨Åne
c(a1, a2) =
‚éß
‚é™
‚é®
‚é™
‚é©
(0, 0)
if c = 0

ca1, a2
c

if c Ã∏= 0.
Is V a vector space over R with these operations? Justify your answer.
20.
Let V be the set of sequences {an} of real numbers. (See Example 5 for
the deÔ¨Ånition of a sequence.) For {an}, {bn} ‚ààV and any real number
t, deÔ¨Åne
{an} + {bn} = {an + bn}
and
t{an} = {tan}.
Prove that, with these operations, V is a vector space over R.
21.
Let V and W be vector spaces over a Ô¨Åeld F. Let
Z = {(v, w): v ‚ààV and w ‚ààW}.
Prove that Z is a vector space over F with the operations
(v1, w1) + (v2, w2) = (v1 + v2, w1 + w2)
and
c(v1, w1) = (cv1, cw1).
22.
How many matrices are there in the vector space Mm√ón(Z2)?
(See
Appendix C.)
1.3
SUBSPACES
In the study of any algebraic structure, it is of interest to examine subsets that
possess the same structure as the set under consideration. The appropriate
notion of substructure for vector spaces is introduced in this section.
DeÔ¨Ånition.
A subset W of a vector space V over a Ô¨Åeld F is called a
subspace of V if W is a vector space over F with the operations of addition
and scalar multiplication deÔ¨Åned on V.
In any vector space V, note that V and {0} are subspaces. The latter is
called the zero subspace of V.
Fortunately it is not necessary to verify all of the vector space properties
to prove that a subset is a subspace. Because properties (VS 1), (VS 2),
(VS 5), (VS 6), (VS 7), and (VS 8) hold for all vectors in the vector space,
these properties automatically hold for the vectors in any subset. Thus a
subset W of a vector space V is a subspace of V if and only if the following
four properties hold.

Sec. 1.3
Subspaces
17
1. x+y ‚ààW whenever x ‚ààW and y ‚ààW. (W is closed under addition.)
2. cx ‚ààW whenever c ‚ààF and x ‚ààW.
(W is closed under scalar
multiplication.)
3. W has a zero vector.
4. Each vector in W has an additive inverse in W.
The next theorem shows that the zero vector of W must be the same as
the zero vector of V and that property 4 is redundant.
Theorem 1.3. Let V be a vector space and W a subset of V. Then W
is a subspace of V if and only if the following three conditions hold for the
operations deÔ¨Åned in V.
(a) 0 ‚ààW.
(b) x + y ‚ààW whenever x ‚ààW and y ‚ààW.
(c) cx ‚ààW whenever c ‚ààF and x ‚ààW.
Proof. If W is a subspace of V, then W is a vector space with the operations
of addition and scalar multiplication deÔ¨Åned on V. Hence conditions (b) and
(c) hold, and there exists a vector 0 ‚Ä≤ ‚ààW such that x + 0 ‚Ä≤ = x for each
x ‚ààW. But also x + 0 = x, and thus 0 ‚Ä≤ = 0 by Theorem 1.1 (p. 11). So
condition (a) holds.
Conversely, if conditions (a), (b), and (c) hold, the discussion preceding
this theorem shows that W is a subspace of V if the additive inverse of each
vector in W lies in W. But if x ‚ààW, then (‚àí1)x ‚ààW by condition (c), and
‚àíx = (‚àí1)x by Theorem 1.2 (p. 12). Hence W is a subspace of V.
The preceding theorem provides a simple method for determining whether
or not a given subset of a vector space is a subspace. Normally, it is this result
that is used to prove that a subset is, in fact, a subspace.
The transpose At of an m √ó n matrix A is the n √ó m matrix obtained
from A by interchanging the rows with the columns; that is, (At)ij = Aji.
For example,

1
‚àí2
3
0
5
‚àí1
	t
=
‚éõ
‚éù
1
0
‚àí2
5
3
‚àí1
‚éû
‚é†
and

1
2
2
3
	t
=

1
2
2
3
	
.
A symmetric matrix is a matrix A such that At = A. For example, the
2 √ó 2 matrix displayed above is a symmetric matrix. Clearly, a symmetric
matrix must be square. The set W of all symmetric matrices in Mn√ón(F) is
a subspace of Mn√ón(F) since the conditions of Theorem 1.3 hold:
1. The zero matrix is equal to its transpose and hence belongs to W.
It is easily proved that for any matrices A and B and any scalars a and b,
(aA + bB)t = aAt + bBt. (See Exercise 3.) Using this fact, we show that the
set of symmetric matrices is closed under addition and scalar multiplication.

18
Chap. 1
Vector Spaces
2. If A ‚ààW and B ‚ààW, then At = A and Bt = B. Thus (A + B)t =
At + Bt = A + B, so that A + B ‚ààW.
3. If A ‚ààW, then At = A. So for any a ‚ààF, we have (aA)t = aAt = aA.
Thus aA ‚ààW.
The examples that follow provide further illustrations of the concept of a
subspace. The Ô¨Årst three are particularly important.
Example 1
Let n be a nonnegative integer, and let Pn(F) consist of all polynomials in
P(F) having degree less than or equal to n. Since the zero polynomial has
degree ‚àí1, it is in Pn(F). Moreover, the sum of two polynomials with degrees
less than or equal to n is another polynomial of degree less than or equal to n,
and the product of a scalar and a polynomial of degree less than or equal to
n is a polynomial of degree less than or equal to n. So Pn(F) is closed under
addition and scalar multiplication. It therefore follows from Theorem 1.3 that
Pn(F) is a subspace of P(F).
‚ô¶
Example 2
Let C(R) denote the set of all continuous real-valued functions deÔ¨Åned on R.
Clearly C(R) is a subset of the vector space F(R, R) deÔ¨Åned in Example 3
of Section 1.2. We claim that C(R) is a subspace of F(R, R). First note
that the zero of F(R, R) is the constant function deÔ¨Åned by f(t) = 0 for all
t ‚ààR. Since constant functions are continuous, we have f ‚ààC(R). Moreover,
the sum of two continuous functions is continuous, and the product of a real
number and a continuous function is continuous. So C(R) is closed under
addition and scalar multiplication and hence is a subspace of F(R, R) by
Theorem 1.3.
‚ô¶
Example 3
An n √ó n matrix M is called a diagonal matrix if Mij = 0 whenever i Ã∏= j,
that is, if all its nondiagonal entries are zero. Clearly the zero matrix is a
diagonal matrix because all of its entries are 0. Moreover, if A and B are
diagonal n √ó n matrices, then whenever i Ã∏= j,
(A + B)ij = Aij + Bij = 0 + 0 = 0
and
(cA)ij = cAij = c 0 = 0
for any scalar c. Hence A + B and cA are diagonal matrices for any scalar
c. Therefore the set of diagonal matrices is a subspace of Mn√ón(F) by Theo-
rem 1.3.
‚ô¶
Example 4
The trace of an n √ó n matrix M, denoted tr(M), is the sum of the diagonal
entries of M; that is,
tr(M) = M11 + M22 + ¬∑ ¬∑ ¬∑ + Mnn.

Sec. 1.3
Subspaces
19
It follows from Exercise 6 that the set of n √ó n matrices having trace equal
to zero is a subspace of Mn√ón(F).
‚ô¶
Example 5
The set of matrices in Mm√ón(R) having nonnegative entries is not a subspace
of Mm√ón(R) because it is not closed under scalar multiplication (by negative
scalars).
‚ô¶
The next theorem shows how to form a new subspace from other sub-
spaces.
Theorem 1.4. Any intersection of subspaces of a vector space V is a
subspace of V.
Proof. Let C be a collection of subspaces of V, and let W denote the
intersection of the subspaces in C. Since every subspace contains the zero
vector, 0 ‚ààW. Let a ‚ààF and x, y ‚ààW. Then x and y are contained in each
subspace in C. Because each subspace in C is closed under addition and scalar
multiplication, it follows that x + y and ax are contained in each subspace in
C. Hence x + y and ax are also contained in W, so that W is a subspace of V
by Theorem 1.3.
Having shown that the intersection of subspaces of a vector space V is a
subspace of V, it is natural to consider whether or not the union of subspaces
of V is a subspace of V. It is easily seen that the union of subspaces must
contain the zero vector and be closed under scalar multiplication, but in
general the union of subspaces of V need not be closed under addition. In fact,
it can be readily shown that the union of two subspaces of V is a subspace of V
if and only if one of the subspaces contains the other. (See Exercise 19.) There
is, however, a natural way to combine two subspaces W1 and W2 to obtain
a subspace that contains both W1 and W2. As we already have suggested,
the key to Ô¨Ånding such a subspace is to assure that it must be closed under
addition. This idea is explored in Exercise 23.
EXERCISES
1.
Label the following statements as true or false.
(a)
If V is a vector space and W is a subset of V that is a vector space,
then W is a subspace of V.
(b)
The empty set is a subspace of every vector space.
(c)
If V is a vector space other than the zero vector space, then V
contains a subspace W such that W Ã∏= V.
(d)
The intersection of any two subsets of V is a subspace of V.

20
Chap. 1
Vector Spaces
(e)
An n √ó n diagonal matrix can never have more than n nonzero
entries.
(f)
The trace of a square matrix is the product of its diagonal entries.
(g)
Let W be the xy-plane in R3; that is, W = {(a1, a2, 0): a1, a2 ‚ààR}.
Then W = R2.
2.
Determine the transpose of each of the matrices that follow. In addition,
if the matrix is square, compute its trace.
(a)

‚àí4
2
5
‚àí1
	
(b)

0
8
‚àí6
3
4
7
	
(c)
‚éõ
‚éù
‚àí3
9
0
‚àí2
6
1
‚éû
‚é†
(d)
‚éõ
‚éù
10
0
‚àí8
2
‚àí4
3
‚àí5
7
6
‚éû
‚é†
(e)

1
‚àí1
3
5

(f)

‚àí2
5
1
4
7
0
1
‚àí6
	
(g)
‚éõ
‚éù
5
6
7
‚éû
‚é†
(h)
‚éõ
‚éù
‚àí4
0
6
0
1
‚àí3
6
‚àí3
5
‚éû
‚é†
3.
Prove that (aA + bB)t = aAt + bBt for any A, B ‚ààMm√ón(F) and any
a, b ‚ààF.
4.
Prove that (At)t = A for each A ‚ààMm√ón(F).
5.
Prove that A + At is symmetric for any square matrix A.
6.
Prove that tr(aA + bB) = a tr(A) + b tr(B) for any A, B ‚ààMn√ón(F).
7.
Prove that diagonal matrices are symmetric matrices.
8.
Determine whether the following sets are subspaces of R3 under the
operations of addition and scalar multiplication deÔ¨Åned on R3. Justify
your answers.
(a)
W1 = {(a1, a2, a3) ‚ààR3 : a1 = 3a2 and a3 = ‚àía2}
(b)
W2 = {(a1, a2, a3) ‚ààR3 : a1 = a3 + 2}
(c)
W3 = {(a1, a2, a3) ‚ààR3 : 2a1 ‚àí7a2 + a3 = 0}
(d)
W4 = {(a1, a2, a3) ‚ààR3 : a1 ‚àí4a2 ‚àía3 = 0}
(e)
W5 = {(a1, a2, a3) ‚ààR3 : a1 + 2a2 ‚àí3a3 = 1}
(f)
W6 = {(a1, a2, a3) ‚ààR3 : 5a2
1 ‚àí3a2
2 + 6a2
3 = 0}
9.
Let W1, W3, and W4 be as in Exercise 8. Describe W1 ‚à©W3, W1 ‚à©W4,
and W3 ‚à©W4, and observe that each is a subspace of R3.

Sec. 1.3
Subspaces
21
10.
Prove that W1 = {(a1, a2, . . . , an) ‚ààFn : a1 + a2 + ¬∑ ¬∑ ¬∑ + an = 0} is a
subspace of Fn, but W2 = {(a1, a2, . . . , an) ‚ààFn : a1 +a2 +¬∑ ¬∑ ¬∑+an = 1}
is not.
11.
Is the set W = {f(x) ‚ààP(F): f(x) = 0 or f(x) has degree n} a subspace
of P(F) if n ‚â•1? Justify your answer.
12.
An m√ón matrix A is called upper triangular if all entries lying below
the diagonal entries are zero, that is, if Aij = 0 whenever i > j. Prove
that the upper triangular matrices form a subspace of Mm√ón(F).
13.
Let S be a nonempty set and F a Ô¨Åeld. Prove that for any s0 ‚ààS,
{f ‚ààF(S, F): f(s0) = 0}, is a subspace of F(S, F).
14.
Let S be a nonempty set and F a Ô¨Åeld. Let C(S, F) denote the set of
all functions f ‚ààF(S, F) such that f(s) = 0 for all but a Ô¨Ånite number
of elements of S. Prove that C(S, F) is a subspace of F(S, F).
15.
Is the set of all diÔ¨Äerentiable real-valued functions deÔ¨Åned on R a sub-
space of C(R)? Justify your answer.
16.
Let Cn(R) denote the set of all real-valued functions deÔ¨Åned on the
real line that have a continuous nth derivative. Prove that Cn(R) is a
subspace of F(R, R).
17.
Prove that a subset W of a vector space V is a subspace of V if and
only if W Ã∏= ‚àÖ, and, whenever a ‚ààF and x, y ‚ààW, then ax ‚ààW and
x + y ‚ààW.
18.
Prove that a subset W of a vector space V is a subspace of V if and only
if 0 ‚ààW and ax + y ‚ààW whenever a ‚ààF and x, y ‚ààW.
19.
Let W1 and W2 be subspaces of a vector space V. Prove that W1 ‚à™W2
is a subspace of V if and only if W1 ‚äÜW2 or W2 ‚äÜW1.
20.‚Ä† Prove that if W is a subspace of a vector space V and w1, w2, . . . , wn are
in W, then a1w1 +a2w2 +¬∑ ¬∑ ¬∑+anwn ‚ààW for any scalars a1, a2, . . . , an.
21.
Show that the set of convergent sequences {an} (i.e., those for which
limn‚Üí‚àûan exists) is a subspace of the vector space V in Exercise 20 of
Section 1.2.
22.
Let F1 and F2 be Ô¨Åelds. A function g ‚ààF(F1, F2) is called an even
function if g(‚àít) = g(t) for each t ‚ààF1 and is called an odd function
if g(‚àít) = ‚àíg(t) for each t ‚ààF1. Prove that the set of all even functions
in F(F1, F2) and the set of all odd functions in F(F1, F2) are subspaces
of F(F1, F2).
‚Ä†A dagger means that this exercise is essential for a later section.

22
Chap. 1
Vector Spaces
The following deÔ¨Ånitions are used in Exercises 23‚Äì30.
DeÔ¨Ånition. If S1 and S2 are nonempty subsets of a vector space V, then
the sum of S1 and S2, denoted S1 +S2, is the set {x+y: x ‚ààS1 and y ‚ààS2}.
DeÔ¨Ånition. A vector space V is called the direct sum of W1 and W2 if
W1 and W2 are subspaces of V such that W1 ‚à©W2 = {0} and W1 + W2 = V.
We denote that V is the direct sum of W1 and W2 by writing V = W1 ‚äïW2.
23.
Let W1 and W2 be subspaces of a vector space V.
(a)
Prove that W1 + W2 is a subspace of V that contains both W1 and
W2.
(b)
Prove that any subspace of V that contains both W1 and W2 must
also contain W1 + W2.
24.
Show that Fn is the direct sum of the subspaces
W1 = {(a1, a2, . . . , an) ‚ààFn : an = 0}
and
W2 = {(a1, a2, . . . , an) ‚ààFn : a1 = a2 = ¬∑ ¬∑ ¬∑ = an‚àí1 = 0}.
25.
Let W1 denote the set of all polynomials f(x) in P(F) such that in the
representation
f(x) = anxn + an‚àí1xn‚àí1 + ¬∑ ¬∑ ¬∑ + a1x + a0,
we have ai = 0 whenever i is even. Likewise let W2 denote the set of
all polynomials g(x) in P(F) such that in the representation
g(x) = bmxm + bm‚àí1xm‚àí1 + ¬∑ ¬∑ ¬∑ + b1x + b0,
we have bi = 0 whenever i is odd. Prove that P(F) = W1 ‚äïW2.
26.
In Mm√ón(F) deÔ¨Åne W1 = {A ‚ààMm√ón(F): Aij = 0 whenever i > j}
and W2 = {A ‚ààMm√ón(F): Aij = 0 whenever i ‚â§j}.
(W1 is the
set of all upper triangular matrices deÔ¨Åned in Exercise 12.) Show that
Mm√ón(F) = W1 ‚äïW2.
27.
Let V denote the vector space consisting of all upper triangular n √ó n
matrices (as deÔ¨Åned in Exercise 12), and let W1 denote the subspace of
V consisting of all diagonal matrices. Show that V = W1 ‚äïW2, where
W2 = {A ‚ààV: Aij = 0 whenever i ‚â•j}.

Sec. 1.3
Subspaces
23
28.
A matrix M is called skew-symmetric if M t = ‚àíM. Clearly, a skew-
symmetric matrix is square. Let F be a Ô¨Åeld. Prove that the set W1
of all skew-symmetric n √ó n matrices with entries from F is a subspace
of Mn√ón(F). Now assume that F is not of characteristic 2 (see Ap-
pendix C), and let W2 be the subspace of Mn√ón(F) consisting of all
symmetric n √ó n matrices. Prove that Mn√ón(F) = W1 ‚äïW2.
29.
Let F be a Ô¨Åeld that is not of characteristic 2. DeÔ¨Åne
W1 = {A ‚ààMn√ón(F): Aij = 0 whenever i ‚â§j}
and W2 to be the set of all symmetric n √ó n matrices with entries
from F.
Both W1 and W2 are subspaces of Mn√ón(F).
Prove that
Mn√ón(F) = W1 ‚äïW2. Compare this exercise with Exercise 28.
30.
Let W1 and W2 be subspaces of a vector space V. Prove that V is the
direct sum of W1 and W2 if and only if each vector in V can be uniquely
written as x1 + x2, where x1 ‚ààW1 and x2 ‚ààW2.
31.
Let W be a subspace of a vector space V over a Ô¨Åeld F. For any v ‚ààV
the set {v}+W = {v +w: w ‚ààW} is called the coset of W containing
v. It is customary to denote this coset by v + W rather than {v} + W.
(a)
Prove that v + W is a subspace of V if and only if v ‚ààW.
(b)
Prove that v1 + W = v2 + W if and only if v1 ‚àív2 ‚ààW.
Addition and scalar multiplication by scalars of F can be deÔ¨Åned in the
collection S = {v + W: v ‚ààV} of all cosets of W as follows:
(v1 + W) + (v2 + W) = (v1 + v2) + W
for all v1, v2 ‚ààV and
a(v + W) = av + W
for all v ‚ààV and a ‚ààF.
(c)
Prove that the preceding operations are well deÔ¨Åned; that is, show
that if v1 + W = v‚Ä≤
1 + W and v2 + W = v‚Ä≤
2 + W, then
(v1 + W) + (v2 + W) = (v‚Ä≤
1 + W) + (v‚Ä≤
2 + W)
and
a(v1 + W) = a(v‚Ä≤
1 + W)
for all a ‚ààF.
(d)
Prove that the set S is a vector space with the operations deÔ¨Åned in
(c). This vector space is called the quotient space of V modulo
W and is denoted by V/W.

24
Chap. 1
Vector Spaces
1.4
LINEAR COMBINATIONS AND SYSTEMS OF LINEAR
EQUATIONS
In Section 1.1, it was shown that the equation of the plane through three
noncollinear points A, B, and C in space is x = A + su + tv, where u and
v denote the vectors beginning at A and ending at B and C, respectively,
and s and t denote arbitrary real numbers. An important special case occurs
when A is the origin. In this case, the equation of the plane simpliÔ¨Åes to
x = su + tv, and the set of all points in this plane is a subspace of R3. (This
is proved as Theorem 1.5.) Expressions of the form su + tv, where s and t
are scalars and u and v are vectors, play a central role in the theory of vector
spaces. The appropriate generalization of such expressions is presented in the
following deÔ¨Ånitions.
DeÔ¨Ånitions. Let V be a vector space and S a nonempty subset of V. A
vector v ‚ààV is called a linear combination of vectors of S if there exist
a Ô¨Ånite number of vectors u1, u2, . . . , un in S and scalars a1, a2, . . . , an in F
such that v = a1u1 + a2u2 + ¬∑ ¬∑ ¬∑ + anun. In this case we also say that v is
a linear combination of u1, u2, . . . , un and call a1, a2, . . . , an the coeÔ¨Écients
of the linear combination.
Observe that in any vector space V, 0v = 0 for each v ‚ààV. Thus the zero
vector is a linear combination of any nonempty subset of V.
Example 1
TABLE 1.1 Vitamin Content of 100 Grams of Certain Foods
A
B1
B2
Niacin
C
(units)
(mg)
(mg)
(mg)
(mg)
Apple butter
0
0.01
0.02
0.2
2
Raw, unpared apples (freshly harvested)
90
0.03
0.02
0.1
4
Chocolate-coated candy with coconut
0
0.02
0.07
0.2
0
center
Clams (meat only)
100
0.10
0.18
1.3
10
Cupcake from mix (dry form)
0
0.05
0.06
0.3
0
Cooked farina (unenriched)
(0)a
0.01
0.01
0.1
(0)
Jams and preserves
10
0.01
0.03
0.2
2
Coconut custard pie (baked from mix)
0
0.02
0.02
0.4
0
Raw brown rice
(0)
0.34
0.05
4.7
(0)
Soy sauce
0
0.02
0.25
0.4
0
Cooked spaghetti (unenriched)
0
0.01
0.01
0.3
0
Raw wild rice
(0)
0.45
0.63
6.2
(0)
Source: Bernice K. Watt and Annabel L. Merrill, Composition of Foods (Agriculture Hand-
book Number 8), Consumer and Food Economics Research Division, U.S. Department of
Agriculture, Washington, D.C., 1963.
aZeros in parentheses indicate that the amount of a vitamin present is either none or too
small to measure.

Sec. 1.4
Linear Combinations and Systems of Linear Equations
25
Table 1.1 shows the vitamin content of 100 grams of 12 foods with respect to
vitamins A, B1 (thiamine), B2 (riboÔ¨Çavin), niacin, and C (ascorbic acid).
The vitamin content of 100 grams of each food can be recorded as a column
vector in R5‚Äîfor example, the vitamin vector for apple butter is
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
0.00
0.01
0.02
0.20
2.00
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
.
Considering the vitamin vectors for cupcake, coconut custard pie, raw brown
rice, soy sauce, and wild rice, we see that
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
0.00
0.05
0.06
0.30
0.00
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
+
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
0.00
0.02
0.02
0.40
0.00
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
+
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
0.00
0.34
0.05
4.70
0.00
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
+ 2
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
0.00
0.02
0.25
0.40
0.00
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
=
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
0.00
0.45
0.63
6.20
0.00
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
.
Thus the vitamin vector for wild rice is a linear combination of the vitamin
vectors for cupcake, coconut custard pie, raw brown rice, and soy sauce. So
100 grams of cupcake, 100 grams of coconut custard pie, 100 grams of raw
brown rice, and 200 grams of soy sauce provide exactly the same amounts of
the Ô¨Åve vitamins as 100 grams of raw wild rice. Similarly, since
2
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
0.00
0.01
0.02
0.20
2.00
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
+
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
90.00
0.03
0.02
0.10
4.00
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
+
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
0.00
0.02
0.07
0.20
0.00
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
+
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
0.00
0.01
0.01
0.10
0.00
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
+
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
10.00
0.01
0.03
0.20
2.00
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
+
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
0.00
0.01
0.01
0.30
0.00
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
=
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
100.00
0.10
0.18
1.30
10.00
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
,
200 grams of apple butter, 100 grams of apples, 100 grams of chocolate candy,
100 grams of farina, 100 grams of jam, and 100 grams of spaghetti provide
exactly the same amounts of the Ô¨Åve vitamins as 100 grams of clams.
‚ô¶
Throughout Chapters 1 and 2 we encounter many diÔ¨Äerent situations in
which it is necessary to determine whether or not a vector can be expressed
as a linear combination of other vectors, and if so, how. This question often
reduces to the problem of solving a system of linear equations. In Chapter 3,
we discuss a general method for using matrices to solve any system of linear
equations. For now, we illustrate how to solve a system of linear equations by
showing how to determine if the vector (2, 6, 8) can be expressed as a linear
combination of
u1 = (1, 2, 1),
u2 = (‚àí2, ‚àí4, ‚àí2),
u3 = (0, 2, 3),

26
Chap. 1
Vector Spaces
u4 = (2, 0, ‚àí3),
and
u5 = (‚àí3, 8, 16).
Thus we must determine if there are scalars a1, a2, a3, a4, and a5 such that
(2, 6, 8) = a1u1 + a2u2 + a3u3 + a4u4 + a5u5
= a1(1, 2, 1) + a2(‚àí2, ‚àí4, ‚àí2) + a3(0, 2, 3)
+ a4(2, 0, ‚àí3) + a5(‚àí3, 8, 16)
= (a1 ‚àí2a2 + 2a4 ‚àí3a5, 2a1 ‚àí4a2 + 2a3 + 8a5,
a1 ‚àí2a2 + 3a3 ‚àí3a4 + 16a5).
Hence (2, 6, 8) can be expressed as a linear combination of u1, u2, u3, u4, and
u5 if and only if there is a 5-tuple of scalars (a1, a2, a3, a4, a5) satisfying the
system of linear equations
a1 ‚àí2a2
+ 2a4 ‚àí3a5 = 2
2a1 ‚àí4a2 + 2a3
+ 8a5 = 6
a1 ‚àí2a2 + 3a3 ‚àí3a4 + 16a5 = 8,
(1)
which is obtained by equating the corresponding coordinates in the preceding
equation.
To solve system (1), we replace it by another system with the same solu-
tions, but which is easier to solve. The procedure to be used expresses some
of the unknowns in terms of others by eliminating certain unknowns from
all the equations except one. To begin, we eliminate a1 from every equation
except the Ô¨Årst by adding ‚àí2 times the Ô¨Årst equation to the second and ‚àí1
times the Ô¨Årst equation to the third. The result is the following new system:
a1 ‚àí2a2
+ 2a4 ‚àí3a5 = 2
2a3 ‚àí4a4 + 14a5 = 2
3a3 ‚àí5a4 + 19a5 = 6.
(2)
In this case, it happened that while eliminating a1 from every equation
except the Ô¨Årst, we also eliminated a2 from every equation except the Ô¨Årst.
This need not happen in general. We now want to make the coeÔ¨Écient of a3 in
the second equation equal to 1, and then eliminate a3 from the third equation.
To do this, we Ô¨Årst multiply the second equation by 1
2, which produces
a1 ‚àí2a2
+ 2a4 ‚àí3a5 = 2
a3 ‚àí2a4 + 7a5 = 1
3a3 ‚àí5a4 + 19a5 = 6.
Next we add ‚àí3 times the second equation to the third, obtaining
a1 ‚àí2a2
+ 2a4 ‚àí3a5 = 2
a3 ‚àí2a4 + 7a5 = 1
a4 ‚àí2a5 = 3.
(3)

Sec. 1.4
Linear Combinations and Systems of Linear Equations
27
We continue by eliminating a4 from every equation of (3) except the third.
This yields
a1 ‚àí2a2
+ a5 = ‚àí4
a3
+ 3a5 =
7
a4 ‚àí2a5 =
3.
(4)
System (4) is a system of the desired form: It is easy to solve for the Ô¨Årst
unknown present in each of the equations (a1, a3, and a4) in terms of the
other unknowns (a2 and a5). Rewriting system (4) in this form, we Ô¨Ånd that
a1 = 2a2 ‚àía5 ‚àí4
a3 =
‚àí3a5 + 7
a4 =
2a5 + 3.
Thus for any choice of scalars a2 and a5, a vector of the form
(a1, a2, a3, a4, a5) = (2a2 ‚àía5 ‚àí4, a2, ‚àí3a5 + 7, 2a5 + 3, a5)
is a solution to system (1). In particular, the vector (‚àí4, 0, 7, 3, 0) obtained
by setting a2 = 0 and a5 = 0 is a solution to (1). Therefore
(2, 6, 8) = ‚àí4u1 + 0u2 + 7u3 + 3u4 + 0u5,
so that (2, 6, 8) is a linear combination of u1, u2, u3, u4, and u5.
The procedure just illustrated uses three types of operations to simplify
the original system:
1. interchanging the order of any two equations in the system;
2. multiplying any equation in the system by a nonzero constant;
3. adding a constant multiple of any equation to another equation in the
system.
In Section 3.4, we prove that these operations do not change the set of
solutions to the original system. Note that we employed these operations to
obtain a system of equations that had the following properties:
1. The Ô¨Årst nonzero coeÔ¨Écient in each equation is one.
2. If an unknown is the Ô¨Årst unknown with a nonzero coeÔ¨Écient in some
equation, then that unknown occurs with a zero coeÔ¨Écient in each of
the other equations.
3. The Ô¨Årst unknown with a nonzero coeÔ¨Écient in any equation has a
larger subscript than the Ô¨Årst unknown with a nonzero coeÔ¨Écient in
any preceding equation.

28
Chap. 1
Vector Spaces
To help clarify the meaning of these properties, note that none of the
following systems meets these requirements.
x1 + 3x2
+ x4 =
7
2x3 ‚àí5x4 = ‚àí1
(5)
x1 ‚àí2x2 + 3x3
+ x5 = ‚àí5
x3
‚àí2x5 =
9
x4 + 3x5 =
6
(6)
x1
‚àí2x3
+ x5 = 1
x4 ‚àí6x5 = 0
x2 + 5x3
‚àí3x5 = 2.
(7)
SpeciÔ¨Åcally, system (5) does not satisfy property 1 because the Ô¨Årst nonzero
coeÔ¨Écient in the second equation is 2; system (6) does not satisfy property 2
because x3, the Ô¨Årst unknown with a nonzero coeÔ¨Écient in the second equa-
tion, occurs with a nonzero coeÔ¨Écient in the Ô¨Årst equation; and system (7)
does not satisfy property 3 because x2, the Ô¨Årst unknown with a nonzero
coeÔ¨Écient in the third equation, does not have a larger subscript than x4, the
Ô¨Årst unknown with a nonzero coeÔ¨Écient in the second equation.
Once a system with properties 1, 2, and 3 has been obtained, it is easy
to solve for some of the unknowns in terms of the others (as in the preceding
example). If, however, in the course of using operations 1, 2, and 3 a system
containing an equation of the form 0 = c, where c is nonzero, is obtained,
then the original system has no solutions. (See Example 2.)
We return to the study of systems of linear equations in Chapter 3. We
discuss there the theoretical basis for this method of solving systems of linear
equations and further simplify the procedure by use of matrices.
Example 2
We claim that
2x3 ‚àí2x2 + 12x ‚àí6
is a linear combination of
x3 ‚àí2x2 ‚àí5x ‚àí3
and
3x3 ‚àí5x2 ‚àí4x ‚àí9
in P3(R), but that
3x3 ‚àí2x2 + 7x + 8
is not. In the Ô¨Årst case we wish to Ô¨Ånd scalars a and b such that
2x3 ‚àí2x2 + 12x ‚àí6 = a(x3 ‚àí2x2 ‚àí5x ‚àí3) + b(3x3 ‚àí5x2 ‚àí4x ‚àí9)

Sec. 1.4
Linear Combinations and Systems of Linear Equations
29
= (a + 3b)x3 + (‚àí2a ‚àí5b)x2 + (‚àí5a ‚àí4b)x + (‚àí3a ‚àí9b).
Thus we are led to the following system of linear equations:
a + 3b =
2
‚àí2a ‚àí5b = ‚àí2
‚àí5a ‚àí4b = 12
‚àí3a ‚àí9b = ‚àí6.
Adding appropriate multiples of the Ô¨Årst equation to the others in order to
eliminate a, we Ô¨Ånd that
a + 3b = 2
b = 2
11b = 22
0b = 0.
Now adding the appropriate multiples of the second equation to the others
yields
a = ‚àí4
b =
2
0 =
0
0 =
0.
Hence
2x3 ‚àí2x2 + 12x ‚àí6 = ‚àí4(x3 ‚àí2x2 ‚àí5x ‚àí3) + 2(3x3 ‚àí5x2 ‚àí4x ‚àí9).
In the second case, we wish to show that there are no scalars a and b for
which
3x3 ‚àí2x2 + 7x + 8 = a(x3 ‚àí2x2 ‚àí5x ‚àí3) + b(3x3 ‚àí5x2 ‚àí4x ‚àí9).
Using the preceding technique, we obtain a system of linear equations
a + 3b =
3
‚àí2a ‚àí5b = ‚àí2
‚àí5a ‚àí4b =
7
‚àí3a ‚àí9b =
8.
(8)
Eliminating a as before yields
a + 3b = 3
b = 4
11b = 22
0 = 17.
But the presence of the inconsistent equation 0 = 17 indicates that (8)
has no solutions. Hence 3x3 ‚àí2x2 + 7x + 8 is not a linear combination of
x3 ‚àí2x2 ‚àí5x ‚àí3 and 3x3 ‚àí5x2 ‚àí4x ‚àí9.
‚ô¶

30
Chap. 1
Vector Spaces
Throughout this book, we form the set of all linear combinations of some
set of vectors. We now name such a set of linear combinations.
DeÔ¨Ånition. Let S be a nonempty subset of a vector space V. The span
of S, denoted span(S), is the set consisting of all linear combinations of the
vectors in S. For convenience, we deÔ¨Åne span(‚àÖ) = {0}.
In R3, for instance, the span of the set {(1, 0, 0), (0, 1, 0)} consists of all
vectors in R3 that have the form a(1, 0, 0) + b(0, 1, 0) = (a, b, 0) for some
scalars a and b. Thus the span of {(1, 0, 0), (0, 1, 0)} contains all the points in
the xy-plane. In this case, the span of the set is a subspace of R3. This fact
is true in general.
Theorem 1.5. The span of any subset S of a vector space V is a subspace
of V. Moreover, any subspace of V that contains S must also contain the
span of S.
Proof. This result is immediate if S = ‚àÖbecause span(‚àÖ) = {0}, which
is a subspace that is contained in any subspace of V.
If S Ã∏= ‚àÖ, then S contains a vector z. So 0z = 0 is in span(S). Let
x, y ‚ààspan(S). Then there exist vectors u1, u2, . . . , um, v1, v2, . . . , vn in S
and scalars a1, a2, . . . , am, b1, b2, . . . , bn such that
x = a1u1 + a2u2 + ¬∑ ¬∑ ¬∑ + amum
and
y = b1v1 + b2v2 + ¬∑ ¬∑ ¬∑ + bnvn.
Then
x + y = a1u1 + a2u2 + ¬∑ ¬∑ ¬∑ + amum + b1v1 + b2v2 + ¬∑ ¬∑ ¬∑ + bnvn
and, for any scalar c,
cx = (ca1)u1 + (ca2)u2 + ¬∑ ¬∑ ¬∑ + (cam)um
are clearly linear combinations of the vectors in S; so x + y and cx are in
span(S). Thus span(S) is a subspace of V.
Now let W denote any subspace of V that contains S. If w ‚ààspan(S), then
w has the form w = c1w1+c2w2+¬∑ ¬∑ ¬∑+ckwk for some vectors w1, w2, . . . , wk in
S and some scalars c1, c2, . . . , ck. Since S ‚äÜW, we have w1, w2, . . . , wk ‚ààW.
Therefore w = c1w1 + c2w2 + ¬∑ ¬∑ ¬∑ + ckwk is in W by Exercise 20 of Section
1.3. Because w, an arbitrary vector in span(S), belongs to W, it follows that
span(S) ‚äÜW.
DeÔ¨Ånition.
A subset S of a vector space V generates (or spans) V
if span(S) = V. In this case, we also say that the vectors of S generate (or
span) V.

Sec. 1.4
Linear Combinations and Systems of Linear Equations
31
Example 3
The vectors (1, 1, 0), (1, 0, 1), and (0, 1, 1) generate R3 since an arbitrary vector
(a1, a2, a3) in R3 is a linear combination of the three given vectors; in fact,
the scalars r, s, and t for which
r(1, 1, 0) + s(1, 0, 1) + t(0, 1, 1) = (a1, a2, a3)
are
r = 1
2(a1 + a2 ‚àía3), s = 1
2(a1 ‚àía2 + a3),
and t = 1
2(‚àía1 + a2 + a3). ‚ô¶
Example 4
The polynomials x2 + 3x ‚àí2, 2x2 + 5x ‚àí3, and ‚àíx2 ‚àí4x + 4 generate P2(R)
since each of the three given polynomials belongs to P2(R) and each polyno-
mial ax2 + bx + c in P2(R) is a linear combination of these three, namely,
(‚àí8a + 5b + 3c)(x2 + 3x ‚àí2) + (4a ‚àí2b ‚àíc)(2x2 + 5x ‚àí3)
+(‚àía + b + c)(‚àíx2 ‚àí4x + 4) = ax2 + bx + c.
‚ô¶
Example 5
The matrices

1
1
1
0
	
,

1
1
0
1
	
,

1
0
1
1
	
,
and

0
1
1
1
	
generate M2√ó2(R) since an arbitrary matrix A in M2√ó2(R) can be expressed
as a linear combination of the four given matrices as follows:

a11
a12
a21
a22
	
=
(1
3a11 + 1
3a12 + 1
3a21 ‚àí2
3a22)

1
1
1
0
	
+
(1
3a11 + 1
3a12 ‚àí2
3a21 + 1
3a22)

1
1
0
1
	
+
(1
3a11 ‚àí2
3a12 + 1
3a21 + 1
3a22)

1
0
1
1
	
+
(‚àí2
3a11 + 1
3a12 + 1
3a21 + 1
3a22)

0
1
1
1
	
.
On the other hand, the matrices

1
0
0
1
	
,

1
1
0
1
	
,
and

1
0
1
1
	

32
Chap. 1
Vector Spaces
do not generate M2√ó2(R) because each of these matrices has equal diagonal
entries. So any linear combination of these matrices has equal diagonal en-
tries. Hence not every 2 √ó 2 matrix is a linear combination of these three
matrices.
‚ô¶
At the beginning of this section we noted that the equation of a plane
through three noncollinear points in space, one of which is the origin, is of
the form x = su + tv, where u, v ‚ààR3 and s and t are scalars. Thus x ‚ààR3 is
a linear combination of u, v ‚ààR3 if and only if x lies in the plane containing
u and v. (See Figure 1.5.)














3
-
-
u
su
x
tv
v
Figure 1.5
Usually there are many diÔ¨Äerent subsets that generate a subspace W. (See
Exercise 13.) It is natural to seek a subset of W that generates W and is as
small as possible. In the next section we explore the circumstances under
which a vector can be removed from a generating set to obtain a smaller
generating set.
EXERCISES
1.
Label the following statements as true or false.
(a)
The zero vector is a linear combination of any nonempty set of
vectors.
(b)
The span of ‚àÖis ‚àÖ.
(c)
If S is a subset of a vector space V, then span(S) equals the inter-
section of all subspaces of V that contain S.
(d)
In solving a system of linear equations, it is permissible to multiply
an equation by any constant.
(e)
In solving a system of linear equations, it is permissible to add any
multiple of one equation to another.
(f)
Every system of linear equations has a solution.

Sec. 1.4
Linear Combinations and Systems of Linear Equations
33
2.
Solve the following systems of linear equations by the method intro-
duced in this section.
(a)
2x1 ‚àí2x2 ‚àí3x3
= ‚àí2
3x1 ‚àí3x2 ‚àí2x3 + 5x4 =
7
x1 ‚àíx2 ‚àí2x3 ‚àíx4 = ‚àí3
(b)
3x1 ‚àí7x2 + 4x3 = 10
x1 ‚àí2x2 + x3 = 3
2x1 ‚àíx2 ‚àí2x3 = 6
(c)
x1 + 2x2 ‚àíx3 + x4 = 5
x1 + 4x2 ‚àí3x3 ‚àí3x4 = 6
2x1 + 3x2 ‚àíx3 + 4x4 = 8
(d)
x1 + 2x2 + 2x3
=
2
x1
+ 8x3 + 5x4 = ‚àí6
x1 + x2 + 5x3 + 5x4 =
3
(e)
x1 + 2x2 ‚àí4x3 ‚àí
x4 + x5 =
7
‚àíx1
+ 10x3 ‚àí3x4 ‚àí4x5 = ‚àí16
2x1 + 5x2 ‚àí5x3 ‚àí4x4 ‚àíx5 =
2
4x1 + 11x2 ‚àí7x3 ‚àí10x4 ‚àí2x5 =
7
(f)
x1 + 2x2 + 6x3 = ‚àí1
2x1 + x2 +
x3 =
8
3x1 + x2 ‚àí
x3 = 15
x1 + 3x2 + 10x3 = ‚àí5
3.
For each of the following lists of vectors in R3, determine whether the
Ô¨Årst vector can be expressed as a linear combination of the other two.
(a)
(‚àí2, 0, 3), (1, 3, 0), (2, 4, ‚àí1)
(b)
(1, 2, ‚àí3), (‚àí3, 2, 1), (2, ‚àí1, ‚àí1)
(c)
(3, 4, 1), (1, ‚àí2, 1), (‚àí2, ‚àí1, 1)
(d)
(2, ‚àí1, 0), (1, 2, ‚àí3), (1, ‚àí3, 2)
(e)
(5, 1, ‚àí5), (1, ‚àí2, ‚àí3), (‚àí2, 3, ‚àí4)
(f)
(‚àí2, 2, 2), (1, 2, ‚àí1), (‚àí3, ‚àí3, 3)
4.
For each list of polynomials in P3(R), determine whether the Ô¨Årst poly-
nomial can be expressed as a linear combination of the other two.
(a)
x3 ‚àí3x + 5, x3 + 2x2 ‚àíx + 1, x3 + 3x2 ‚àí1
(b)
4x3 + 2x2 ‚àí6, x3 ‚àí2x2 + 4x + 1, 3x3 ‚àí6x2 + x + 4
(c)
‚àí2x3 ‚àí11x2 + 3x + 2, x3 ‚àí2x2 + 3x ‚àí1, 2x3 + x2 + 3x ‚àí2
(d)
x3 + x2 + 2x + 13, 2x3 ‚àí3x2 + 4x + 1, x3 ‚àíx2 + 2x + 3
(e)
x3 ‚àí8x2 + 4x, x3 ‚àí2x2 + 3x ‚àí1, x3 ‚àí2x + 3
(f)
6x3 ‚àí3x2 + x + 2, x3 ‚àíx2 + 2x + 3, 2x3 ‚àí3x + 1

34
Chap. 1
Vector Spaces
5.
In each part, determine whether the given vector is in the span of S.
(a)
(2, ‚àí1, 1),
S = {(1, 0, 2), (‚àí1, 1, 1)}
(b)
(‚àí1, 2, 1),
S = {(1, 0, 2), (‚àí1, 1, 1)}
(c)
(‚àí1, 1, 1, 2),
S = {(1, 0, 1, ‚àí1), (0, 1, 1, 1)}
(d)
(2, ‚àí1, 1, ‚àí3),
S = {(1, 0, 1, ‚àí1), (0, 1, 1, 1)}
(e)
‚àíx3 + 2x2 + 3x + 3,
S = {x3 + x2 + x + 1, x2 + x + 1, x + 1}
(f)
2x3 ‚àíx2 + x + 3,
S = {x3 + x2 + x + 1, x2 + x + 1, x + 1}
(g)

1
2
‚àí3
4
	
,
S =

1
0
‚àí1
0
	
,

0
1
0
1
	
,

1
1
0
0
	
(h)

1
0
0
1
	
,
S =

1
0
‚àí1
0
	
,

0
1
0
1
	
,

1
1
0
0
	
6.
Show that the vectors (1, 1, 0), (1, 0, 1), and (0, 1, 1) generate F3.
7.
In Fn, let ej denote the vector whose jth coordinate is 1 and whose
other coordinates are 0. Prove that {e1, e2, . . . , en} generates Fn.
8.
Show that Pn(F) is generated by {1, x, . . . , xn}.
9.
Show that the matrices

1
0
0
0
	
,

0
1
0
0
	
,

0
0
1
0
	
,
and

0
0
0
1
	
generate M2√ó2(F).
10.
Show that if
M1 =

1
0
0
0
	
,
M2 =

0
0
0
1
	
,
and
M3 =

0
1
1
0
	
,
then the span of {M1, M2, M3} is the set of all symmetric 2√ó2 matrices.
11.‚Ä† Prove that span({x}) = {ax: a ‚ààF} for any vector x in a vector space.
Interpret this result geometrically in R3.
12.
Show that a subset W of a vector space V is a subspace of V if and only
if span(W) = W.
13.‚Ä† Show that if S1 and S2 are subsets of a vector space V such that S1 ‚äÜS2,
then span(S1) ‚äÜspan(S2). In particular, if S1 ‚äÜS2 and span(S1) = V,
deduce that span(S2) = V.
14.
Show that if S1 and S2 are arbitrary subsets of a vector space V, then
span(S1‚à™S2) = span(S1)+span(S2). (The sum of two subsets is deÔ¨Åned
in the exercises of Section 1.3.)

Sec. 1.5
Linear Dependence and Linear Independence
35
15.
Let S1 and S2 be subsets of a vector space V. Prove that span(S1‚à©S2) ‚äÜ
span(S1) ‚à©span(S2).
Give an example in which span(S1 ‚à©S2) and
span(S1) ‚à©span(S2) are equal and one in which they are unequal.
16.
Let V be a vector space and S a subset of V with the property that
whenever v1, v2, . . . , vn ‚ààS and a1v1 + a2v2 + ¬∑ ¬∑ ¬∑ + anvn = 0, then
a1 = a2 = ¬∑ ¬∑ ¬∑ = an = 0. Prove that every vector in the span of S can
be uniquely written as a linear combination of vectors of S.
17.
Let W be a subspace of a vector space V. Under what conditions are
there only a Ô¨Ånite number of distinct subsets S of W such that S gen-
erates W?
1.5
LINEAR DEPENDENCE AND LINEAR INDEPENDENCE
Suppose that V is a vector space over an inÔ¨Ånite Ô¨Åeld and that W is a subspace
of V. Unless W is the zero subspace, W is an inÔ¨Ånite set. It is desirable to
Ô¨Ånd a ‚Äúsmall‚Äù Ô¨Ånite subset S that generates W because we can then describe
each vector in W as a linear combination of the Ô¨Ånite number of vectors in
S. Indeed, the smaller that S is, the fewer computations that are required
to represent vectors in W.
Consider, for example, the subspace W of R3
generated by S = {u1, u2, u3, u4}, where u1 = (2, ‚àí1, 4), u2 = (1, ‚àí1, 3),
u3 = (1, 1, ‚àí1), and u4 = (1, ‚àí2, ‚àí1). Let us attempt to Ô¨Ånd a proper subset
of S that also generates W.
The search for this subset is related to the
question of whether or not some vector in S is a linear combination of the
other vectors in S. Now u4 is a linear combination of the other vectors in S
if and only if there are scalars a1, a2, and a3 such that
u4 = a1u1 + a2u2 + a3u3,
that is, if and only if there are scalars a1, a2, and a3 satisfying
(1, ‚àí2, ‚àí1) = (2a1 + a2 + a3, ‚àía1 ‚àía2 + a3, 4a1 + 3a2 ‚àía3).
Thus u4 is a linear combination of u1, u2, and u3 if and only if the system of
linear equations
2a1 + a2 + a3 =
1
‚àía1 ‚àía2 + a3 = ‚àí2
4a1 + 3a2 ‚àía3 = ‚àí1
has a solution. The reader should verify that no such solution exists. This
does not, however, answer our question of whether some vector in S is a linear
combination of the other vectors in S. It can be shown, in fact, that u3 is a
linear combination of u1, u2, and u4, namely, u3 = 2u1 ‚àí3u2 + 0u4.

36
Chap. 1
Vector Spaces
In the preceding example, checking that some vector in S is a linear
combination of the other vectors in S could require that we solve several
diÔ¨Äerent systems of linear equations before we determine which, if any, of
u1, u2, u3, and u4 is a linear combination of the others.
By formulating
our question diÔ¨Äerently, we can save ourselves some work. Note that since
u3 = 2u1 ‚àí3u2 + 0u4, we have
‚àí2u1 + 3u2 + u3 ‚àí0u4 = 0.
That is, because some vector in S is a linear combination of the others, the
zero vector can be expressed as a linear combination of the vectors in S using
coeÔ¨Écients that are not all zero. The converse of this statement is also true:
If the zero vector can be written as a linear combination of the vectors in S
in which not all the coeÔ¨Écients are zero, then some vector in S is a linear
combination of the others. For instance, in the example above, the equation
‚àí2u1 + 3u2 + u3 ‚àí0u4 = 0 can be solved for any vector having a nonzero
coeÔ¨Écient; so u1, u2, or u3 (but not u4) can be written as a linear combination
of the other three vectors. Thus, rather than asking whether some vector in
S is a linear combination of the other vectors in S, it is more eÔ¨Écient to
ask whether the zero vector can be expressed as a linear combination of the
vectors in S with coeÔ¨Écients that are not all zero. This observation leads us
to the following deÔ¨Ånition.
DeÔ¨Ånition. A subset S of a vector space V is called linearly dependent
if there exist a Ô¨Ånite number of distinct vectors u1, u2, . . . , un in S and scalars
a1, a2, . . . , an, not all zero, such that
a1u1 + a2u2 + ¬∑ ¬∑ ¬∑ + anun = 0.
In this case we also say that the vectors of S are linearly dependent.
For any vectors u1, u2, . . . , un, we have a1u1 + a2u2 + ¬∑ ¬∑ ¬∑ + anun = 0
if a1 = a2 = ¬∑ ¬∑ ¬∑ = an = 0. We call this the trivial representation of 0 as
a linear combination of u1, u2, . . . , un. Thus, for a set to be linearly depen-
dent, there must exist a nontrivial representation of 0 as a linear combination
of vectors in the set. Consequently, any subset of a vector space that con-
tains the zero vector is linearly dependent, because 0 = 1¬∑0 is a nontrivial
representation of 0 as a linear combination of vectors in the set.
Example 1
Consider the set
S = {(1, 3, ‚àí4, 2), (2, 2, ‚àí4, 0), (1, ‚àí3, 2, ‚àí4), (‚àí1, 0, 1, 0)}
in R4. We show that S is linearly dependent and then express one of the
vectors in S as a linear combination of the other vectors in S. To show that

Sec. 1.5
Linear Dependence and Linear Independence
37
S is linearly dependent, we must Ô¨Ånd scalars a1, a2, a3, and a4, not all zero,
such that
a1(1, 3, ‚àí4, 2) + a2(2, 2, ‚àí4, 0) + a3(1, ‚àí3, 2, ‚àí4) + a4(‚àí1, 0, 1, 0) = 0.
Finding such scalars amounts to Ô¨Ånding a nonzero solution to the system of
linear equations
a1 + 2a2 + a3 ‚àía4 = 0
3a1 + 2a2 ‚àí3a3
= 0
‚àí4a1 ‚àí4a2 + 2a3 + a4 = 0
2a1
‚àí4a3
= 0.
One such solution is a1 = 4, a2 = ‚àí3, a3 = 2, and a4 = 0. Thus S is a
linearly dependent subset of R4, and
4(1, 3, ‚àí4, 2) ‚àí3(2, 2, ‚àí4, 0) + 2(1, ‚àí3, 2, ‚àí4) + 0(‚àí1, 0, 1, 0) = 0.
‚ô¶
Example 2
In M2√ó3(R), the set

1
‚àí3
2
‚àí4
0
5
	
,

‚àí3
7
4
6
‚àí2
‚àí7
	
,

‚àí2
3
11
‚àí1
‚àí3
2
	
is linearly dependent because
5

1
‚àí3
2
‚àí4
0
5
	
+3

‚àí3
7
4
6
‚àí2
‚àí7
	
‚àí2

‚àí2
3
11
‚àí1
‚àí3
2
	
=

0
0
0
0
0
0
	
.‚ô¶
DeÔ¨Ånition. A subset S of a vector space that is not linearly dependent
is called linearly independent.
As before, we also say that the vectors of
S are linearly independent.
The following facts about linearly independent sets are true in any vector
space.
1. The empty set is linearly independent, for linearly dependent sets must
be nonempty.
2. A set consisting of a single nonzero vector is linearly independent. For
if {u} is linearly dependent, then au = 0 for some nonzero scalar a.
Thus
u = a‚àí1(au) = a‚àí10 = 0.
3. A set is linearly independent if and only if the only representations of
0 as linear combinations of its vectors are trivial representations.

38
Chap. 1
Vector Spaces
The condition in item 3 provides a useful method for determining whether
a Ô¨Ånite set is linearly independent. This technique is illustrated in the exam-
ples that follow.
Example 3
To prove that the set
S = {(1, 0, 0, ‚àí1), (0, 1, 0, ‚àí1), (0, 0, 1, ‚àí1), (0, 0, 0, 1)}
is linearly independent, we must show that the only linear combination of
vectors in S that equals the zero vector is the one in which all the coeÔ¨Écients
are zero. Suppose that a1, a2, a3, and a4 are scalars such that
a1(1, 0, 0, ‚àí1) + a2(0, 1, 0, ‚àí1) + a3(0, 0, 1, ‚àí1) + a4(0, 0, 0, 1) = (0, 0, 0, 0).
Equating the corresponding coordinates of the vectors on the left and the right
sides of this equation, we obtain the following system of linear equations.
a1
= 0
a2
= 0
a3
= 0
‚àía1 ‚àía2 ‚àía3 + a4 = 0
Clearly the only solution to this system is a1 = a2 = a3 = a4 = 0, and so S
is linearly independent.
‚ô¶
Example 4
For k = 0, 1, . . . , n let pk(x) = xk + xk+1 + ¬∑ ¬∑ ¬∑ + xn. The set
{p0(x), p1(x), . . . , pn(x)}
is linearly independent in Pn(F). For if
a0p0(x) + a1p1(x) + ¬∑ ¬∑ ¬∑ + anpn(x) = 0
for some scalars a0, a1, . . . , an, then
a0 + (a0 + a1)x + (a0 + a1 + a2)x2 + ¬∑ ¬∑ ¬∑ + (a0 + a1 + ¬∑ ¬∑ ¬∑ + an)xn = 0.
By equating the coeÔ¨Écients of xk on both sides of this equation for k =
1, 2, . . . , n, we obtain
a0
= 0
a0 + a1
= 0
a0 + a1 + a2
= 0
...
a0 + a1 + a2 + ¬∑ ¬∑ ¬∑ + an = 0.
Clearly the only solution to this system of linear equations is a0 = a1 = ¬∑ ¬∑ ¬∑ =
an = 0.
‚ô¶

Sec. 1.5
Linear Dependence and Linear Independence
39
The following important results are immediate consequences of the deÔ¨Å-
nitions of linear dependence and linear independence.
Theorem 1.6. Let V be a vector space, and let S1 ‚äÜS2 ‚äÜV. If S1 is
linearly dependent, then S2 is linearly dependent.
Proof. Exercise.
Corollary. Let V be a vector space, and let S1 ‚äÜS2 ‚äÜV. If S2 is linearly
independent, then S1 is linearly independent.
Proof. Exercise.
Earlier in this section, we remarked that the issue of whether S is the
smallest generating set for its span is related to the question of whether
some vector in S is a linear combination of the other vectors in S. Thus
the issue of whether S is the smallest generating set for its span is related
to the question of whether S is linearly dependent.
To see why, consider
the subset S = {u1, u2, u3, u4} of R3, where u1 = (2, ‚àí1, 4), u2 = (1, ‚àí1, 3),
u3 = (1, 1, ‚àí1), and u4 = (1, ‚àí2, ‚àí1). We have previously noted that S is
linearly dependent; in fact,
‚àí2u1 + 3u2 + u3 ‚àí0u4 = 0.
This equation implies that u3 (or alternatively, u1 or u2) is a linear combina-
tion of the other vectors in S. For example, u3 = 2u1 ‚àí3u2 + 0u4. Therefore
every linear combination a1u1 + a2u2 + a3u3 + a4u4 of vectors in S can be
written as a linear combination of u1, u2, and u4:
a1u1 + a2u2 + a3u3 + a4u4 = a1u1 + a2u2 + a3(2u1 ‚àí3u2 + 0u4) + a4u4
= (a1 + 2a3)u1 + (a2 ‚àí3a3)u2 + a4u4.
Thus the subset S‚Ä≤ = {u1, u2, u4} of S has the same span as S!
More generally, suppose that S is any linearly dependent set containing
two or more vectors. Then some vector v ‚ààS can be written as a linear
combination of the other vectors in S, and the subset obtained by removing
v from S has the same span as S. It follows that if no proper subset of S
generates the span of S, then S must be linearly independent. Another way
to view the preceding statement is given in Theorem 1.7.
Theorem 1.7. Let S be a linearly independent subset of a vector space
V, and let v be a vector in V that is not in S.
Then S ‚à™{v} is linearly
dependent if and only if v ‚ààspan(S).

40
Chap. 1
Vector Spaces
Proof. If S‚à™{v} is linearly dependent, then there are vectors u1, u2, . . . , un
in S ‚à™{v} such that a1u1 + a2u2 + ¬∑ ¬∑ ¬∑ + anun = 0 for some nonzero scalars
a1, a2, . . . , an.
Because S is linearly independent, one of the ui‚Äôs, say u1,
equals v. Thus a1v + a2u2 + ¬∑ ¬∑ ¬∑ + anun = 0, and so
v = a‚àí1
1 (‚àía2u2 ‚àí¬∑ ¬∑ ¬∑ ‚àíanun) = ‚àí(a‚àí1
1 a2)u2 ‚àí¬∑ ¬∑ ¬∑ ‚àí(a‚àí1
1 an)un.
Since v is a linear combination of u2, . . . , un, which are in S, we have v ‚àà
span(S).
Conversely, let v ‚ààspan(S). Then there exist vectors v1, v2, . . . , vm in S
and scalars b1, b2, . . . , bm such that v = b1v1 + b2v2 + ¬∑ ¬∑ ¬∑ + bmvm. Hence
0 = b1v1 + b2v2 + ¬∑ ¬∑ ¬∑ + bmvm + (‚àí1)v.
Since v Ã∏= vi for i = 1, 2, . . . , m, the coeÔ¨Écient of v in this linear combination
is nonzero, and so the set {v1, v2, . . . , vm, v} is linearly dependent. Therefore
S ‚à™{v} is linearly dependent by Theorem 1.6.
Linearly independent generating sets are investigated in detail in Sec-
tion 1.6.
EXERCISES
1.
Label the following statements as true or false.
(a)
If S is a linearly dependent set, then each vector in S is a linear
combination of other vectors in S.
(b)
Any set containing the zero vector is linearly dependent.
(c)
The empty set is linearly dependent.
(d)
Subsets of linearly dependent sets are linearly dependent.
(e)
Subsets of linearly independent sets are linearly independent.
(f)
If a1x1 + a2x2 + ¬∑ ¬∑ ¬∑ + anxn = 0 and x1, x2, . . . , xn are linearly
independent, then all the scalars ai are zero.
2.3 Determine whether the following sets are linearly dependent or linearly
independent.
(a)

1
‚àí3
‚àí2
4
	
,

‚àí2
6
4
‚àí8
	
in M2√ó2(R)
(b)

1
‚àí2
‚àí1
4
	
,

‚àí1
1
2
‚àí4
	
in M2√ó2(R)
(c)
{x3 + 2x2, ‚àíx2 + 3x + 1, x3 ‚àíx2 + 2x ‚àí1} in P3(R)
3The computations in Exercise 2(g), (h), (i), and (j) are tedious unless technology is
used.

Sec. 1.5
Linear Dependence and Linear Independence
41
(d)
{x3 ‚àíx, 2x2 + 4, ‚àí2x3 + 3x2 + 2x + 6} in P3(R)
(e)
{(1, ‚àí1, 2), (1, ‚àí2, 1), (1, 1, 4)} in R3
(f)
{(1, ‚àí1, 2), (2, 0, 1), (‚àí1, 2, ‚àí1)} in R3
(g)

1
0
‚àí2
1
	
,

0
‚àí1
1
1
	
,

‚àí1
2
1
0
	
,

2
1
‚àí4
4
	
in M2√ó2(R)
(h)

1
0
‚àí2
1
	
,

0
‚àí1
1
1
	
,

‚àí1
2
1
0
	
,

2
1
2
‚àí2
	
in M2√ó2(R)
(i)
{x4 ‚àíx3 + 5x2 ‚àí8x + 6, ‚àíx4 + x3 ‚àí5x2 + 5x ‚àí3,
x4 +3x2 ‚àí3x+5, 2x4 +3x3 +4x2 ‚àíx+1, x3 ‚àíx+2} in P4(R)
(j)
{x4 ‚àíx3 + 5x2 ‚àí8x + 6, ‚àíx4 + x3 ‚àí5x2 + 5x ‚àí3,
x4 + 3x2 ‚àí3x + 5, 2x4 + x3 + 4x2 + 8x} in P4(R)
3.
In M2√ó3(F), prove that the set
‚éß
‚é®
‚é©
‚éõ
‚éù
1
1
0
0
0
0
‚éû
‚é†,
‚éõ
‚éù
0
0
1
1
0
0
‚éû
‚é†,
‚éõ
‚éù
0
0
0
0
1
1
‚éû
‚é†,
‚éõ
‚éù
1
0
1
0
1
0
‚éû
‚é†,
‚éõ
‚éù
0
1
0
1
0
1
‚éû
‚é†
‚é´
‚é¨
‚é≠
is linearly dependent.
4.
In Fn, let ej denote the vector whose jth coordinate is 1 and whose other
coordinates are 0. Prove that {e1, e2, ¬∑ ¬∑ ¬∑ , en} is linearly independent.
5.
Show that the set {1, x, x2, . . . , xn} is linearly independent in Pn(F).
6.
In Mm√ón(F), let Eij denote the matrix whose only nonzero entry is 1 in
the ith row and jth column. Prove that {Eij : 1 ‚â§i ‚â§m, 1 ‚â§j ‚â§n}
is linearly independent.
7.
Recall from Example 3 in Section 1.3 that the set of diagonal matrices in
M2√ó2(F) is a subspace. Find a linearly independent set that generates
this subspace.
8.
Let S = {(1, 1, 0), (1, 0, 1), (0, 1, 1)} be a subset of the vector space F3.
(a)
Prove that if F = R, then S is linearly independent.
(b)
Prove that if F has characteristic 2, then S is linearly dependent.
9.‚Ä† Let u and v be distinct vectors in a vector space V. Show that {u, v} is
linearly dependent if and only if u or v is a multiple of the other.
10.
Give an example of three linearly dependent vectors in R3 such that
none of the three is a multiple of another.

42
Chap. 1
Vector Spaces
11.
Let S = {u1, u2, . . . , un} be a linearly independent subset of a vector
space V over the Ô¨Åeld Z2. How many vectors are there in span(S)?
Justify your answer.
12.
Prove Theorem 1.6 and its corollary.
13.
Let V be a vector space over a Ô¨Åeld of characteristic not equal to two.
(a)
Let u and v be distinct vectors in V. Prove that {u, v} is linearly
independent if and only if {u + v, u ‚àív} is linearly independent.
(b)
Let u, v, and w be distinct vectors in V. Prove that {u, v, w} is
linearly independent if and only if {u + v, u + w, v + w} is linearly
independent.
14.
Prove that a set S is linearly dependent if and only if S = {0} or
there exist distinct vectors v, u1, u2, . . . , un in S such that v is a linear
combination of u1, u2, . . . , un.
15.
Let S = {u1, u2, . . . , un} be a Ô¨Ånite set of vectors.
Prove that S is
linearly dependent if and only if u1 = 0 or uk+1 ‚ààspan({u1, u2, . . . , uk})
for some k (1 ‚â§k < n).
16.
Prove that a set S of vectors is linearly independent if and only if each
Ô¨Ånite subset of S is linearly independent.
17.
Let M be a square upper triangular matrix (as deÔ¨Åned in Exercise 12
of Section 1.3) with nonzero diagonal entries. Prove that the columns
of M are linearly independent.
18.
Let S be a set of nonzero polynomials in P(F) such that no two have
the same degree. Prove that S is linearly independent.
19.
Prove that if {A1, A2, . . . , Ak} is a linearly independent subset of
Mn√ón(F), then {At
1, At
2, . . . , At
k} is also linearly independent.
20.
Let f, g, ‚ààF(R, R) be the functions deÔ¨Åned by f(t) = ert and g(t) = est,
where r Ã∏= s. Prove that f and g are linearly independent in F(R, R).
1.6
BASES AND DIMENSION
We saw in Section 1.5 that if S is a generating set for a subspace W and
no proper subset of S is a generating set for W, then S must be linearly
independent. A linearly independent generating set for W possesses a very
useful property‚Äîevery vector in W can be expressed in one and only one way
as a linear combination of the vectors in the set. (This property is proved
below in Theorem 1.8.) It is this property that makes linearly independent
generating sets the building blocks of vector spaces.

Sec. 1.6
Bases and Dimension
43
DeÔ¨Ånition. A basis Œ≤ for a vector space V is a linearly independent
subset of V that generates V.
If Œ≤ is a basis for V, we also say that the
vectors of Œ≤ form a basis for V.
Example 1
Recalling that span(‚àÖ) = {0} and ‚àÖis linearly independent, we see that ‚àÖ
is a basis for the zero vector space.
‚ô¶
Example 2
In Fn, let e1 = (1, 0, 0, . . . , 0), e2 = (0, 1, 0, . . . , 0), . . . , en = (0, 0, . . . , 0, 1);
{e1, e2, . . . , en} is readily seen to be a basis for Fn and is called the standard
basis for Fn.
‚ô¶
Example 3
In Mm√ón(F), let Eij denote the matrix whose only nonzero entry is a 1 in
the ith row and jth column. Then {Eij : 1 ‚â§i ‚â§m, 1 ‚â§j ‚â§n} is a basis for
Mm√ón(F).
‚ô¶
Example 4
In Pn(F) the set {1, x, x2, . . . , xn} is a basis. We call this basis the standard
basis for Pn(F).
‚ô¶
Example 5
In P(F) the set {1, x, x2, . . .} is a basis.
‚ô¶
Observe that Example 5 shows that a basis need not be Ô¨Ånite. In fact,
later in this section it is shown that no basis for P(F) can be Ô¨Ånite. Hence
not every vector space has a Ô¨Ånite basis.
The next theorem, which is used frequently in Chapter 2, establishes the
most signiÔ¨Åcant property of a basis.
Theorem 1.8. Let V be a vector space and Œ≤ = {u1, u2, . . . , un} be a
subset of V. Then Œ≤ is a basis for V if and only if each v ‚ààV can be uniquely
expressed as a linear combination of vectors of Œ≤, that is, can be expressed in
the form
v = a1u1 + a2u2 + ¬∑ ¬∑ ¬∑ + anun
for unique scalars a1, a2, . . . , an.
Proof. Let Œ≤ be a basis for V.
If v ‚ààV, then v ‚ààspan(Œ≤) because
span(Œ≤) = V. Thus v is a linear combination of the vectors of Œ≤. Suppose
that
v = a1u1 + a2u2 + ¬∑ ¬∑ ¬∑ + anun
and
v = b1u1 + b2u2 + ¬∑ ¬∑ ¬∑ + bnun

44
Chap. 1
Vector Spaces
are two such representations of v. Subtracting the second equation from the
Ô¨Årst gives
0 = (a1 ‚àíb1)u1 + (a2 ‚àíb2)u2 + ¬∑ ¬∑ ¬∑ + (an ‚àíbn)un.
Since Œ≤ is linearly independent, it follows that a1 ‚àíb1 = a2 ‚àíb2 = ¬∑ ¬∑ ¬∑ =
an ‚àíbn = 0.
Hence a1 = b1, a2 = b2, ¬∑ ¬∑ ¬∑ , an = bn, and so v is uniquely
expressible as a linear combination of the vectors of Œ≤.
The proof of the converse is an exercise.
Theorem 1.8 shows that if the vectors u1, u2, . . . , un form a basis for a
vector space V, then every vector in V can be uniquely expressed in the form
v = a1u1 + a2u2 + ¬∑ ¬∑ ¬∑ + anun
for appropriately chosen scalars a1, a2, . . . , an. Thus v determines a unique
n-tuple of scalars (a1, a2, . . . , an) and, conversely, each n-tuple of scalars de-
termines a unique vector v ‚ààV by using the entries of the n-tuple as the
coeÔ¨Écients of a linear combination of u1, u2, . . . , un. This fact suggests that
V is like the vector space Fn, where n is the number of vectors in the basis
for V. We see in Section 2.4 that this is indeed the case.
In this book, we are primarily interested in vector spaces having Ô¨Ånite
bases. Theorem 1.9 identiÔ¨Åes a large class of vector spaces of this type.
Theorem 1.9. If a vector space V is generated by a Ô¨Ånite set S, then
some subset of S is a basis for V. Hence V has a Ô¨Ånite basis.
Proof. If S = ‚àÖor S = {0}, then V = {0} and ‚àÖis a subset of S that is a
basis for V. Otherwise S contains a nonzero vector u1. By item 2 on page 37,
{u1} is a linearly independent set. Continue, if possible, choosing vectors
u2, . . . , uk in S such that {u1, u2, . . . , uk} is linearly independent. Since S is
a Ô¨Ånite set, we must eventually reach a stage at which Œ≤ = {u1, u2, . . . , uk} is
a linearly independent subset of S, but adjoining to Œ≤ any vector in S not in Œ≤
produces a linearly dependent set. We claim that Œ≤ is a basis for V. Because
Œ≤ is linearly independent by construction, it suÔ¨Éces to show that Œ≤ spans V.
By Theorem 1.5 (p. 30) we need to show that S ‚äÜspan(Œ≤). Let v ‚ààS. If
v ‚ààŒ≤, then clearly v ‚ààspan(Œ≤). Otherwise, if v /‚ààŒ≤, then the preceding
construction shows that Œ≤ ‚à™{v} is linearly dependent. So v ‚ààspan(Œ≤) by
Theorem 1.7 (p. 39). Thus S ‚äÜspan(Œ≤).
Because of the method by which the basis Œ≤ was obtained in the proof
of Theorem 1.9, this theorem is often remembered as saying that a Ô¨Ånite
spanning set for V can be reduced to a basis for V. This method is illustrated
in the next example.

Sec. 1.6
Bases and Dimension
45
Example 6
Let
S = {(2, ‚àí3, 5), (8, ‚àí12, 20), (1, 0, ‚àí2), (0, 2, ‚àí1), (7, 2, 0)}.
It can be shown that S generates R3.
We can select a basis for R3 that
is a subset of S by the technique used in proving Theorem 1.9. To start,
select any nonzero vector in S, say (2, ‚àí3, 5), to be a vector in the basis.
Since 4(2, ‚àí3, 5) = (8, ‚àí12, 20), the set {(2, 3, ‚àí5), (8, ‚àí12, 20)} is linearly
dependent by Exercise 9 of Section 1.5. Hence we do not include (8, ‚àí12, 20)
in our basis. On the other hand, (1, 0, ‚àí2) is not a multiple of (2, ‚àí3, 5) and
vice versa, so that the set {(2, ‚àí3, 5), (1, 0, ‚àí2)} is linearly independent. Thus
we include (1, 0, ‚àí2) as part of our basis.
Now we consider the set {(2, ‚àí3, 5), (1, 0, ‚àí2), (0, 2, ‚àí1)} obtained by ad-
joining another vector in S to the two vectors that we have already included
in our basis. As before, we include (0, 2, ‚àí1) in our basis or exclude it from
the basis according to whether {(2, ‚àí3, 5), (1, 0, ‚àí2), (0, 2, ‚àí1)} is linearly in-
dependent or linearly dependent. An easy calculation shows that this set is
linearly independent, and so we include (0, 2, ‚àí1) in our basis. In a similar
fashion the Ô¨Ånal vector in S is included or excluded from our basis according
to whether the set
{(2, ‚àí3, 5), (1, 0, ‚àí2), (0, 2, ‚àí1), (7, 2, 0)}
is linearly independent or linearly dependent. Because
2(2, ‚àí3, 5) + 3(1, 0, ‚àí2) + 4(0, 2, ‚àí1) ‚àí(7, 2, 0) = (0, 0, 0),
we exclude (7, 2, 0) from our basis. We conclude that
{(2, ‚àí3, 5), (1, 0, ‚àí2), (0, 2, ‚àí1)}
is a subset of S that is a basis for R3.
‚ô¶
The corollaries of the following theorem are perhaps the most signiÔ¨Åcant
results in Chapter 1.
Theorem 1.10 (Replacement Theorem).
Let V be a vector space
that is generated by a set G containing exactly n vectors, and let L be a
linearly independent subset of V containing exactly m vectors. Then m ‚â§n
and there exists a subset H of G containing exactly n ‚àím vectors such that
L ‚à™H generates V.
Proof. The proof is by mathematical induction on m. The induction begins
with m = 0; for in this case L = ‚àÖ, and so taking H = G gives the desired
result.

46
Chap. 1
Vector Spaces
Now suppose that the theorem is true for some integer m ‚â•0. We prove
that the theorem is true for m + 1. Let L = {v1, v2, . . . , vm+1} be a linearly
independent subset of V consisting of m + 1 vectors.
By the corollary to
Theorem 1.6 (p. 39), {v1, v2, . . . , vm} is linearly independent, and so we may
apply the induction hypothesis to conclude that m ‚â§n and that there is a
subset {u1, u2, . . . , un‚àím} of G such that {v1, v2, . . . , vm}‚à™{u1, u2, . . . , un‚àím}
generates V. Thus there exist scalars a1, a2, . . . , am, b1, b2, . . . , bn‚àím such that
a1v1 + a2v2 + ¬∑ ¬∑ ¬∑ + amvm + b1u1 + b2u2 + ¬∑ ¬∑ ¬∑ + bn‚àímun‚àím = vm+1.
(9)
Note that n ‚àím > 0, lest vm+1 be a linear combination of v1, v2, . . . , vm,
which by Theorem 1.7 (p. 39) contradicts the assumption that L is linearly
independent. Hence n > m; that is, n ‚â•m + 1. Moreover, some bi, say b1, is
nonzero, for otherwise we obtain the same contradiction. Solving (9) for u1
gives
u1 = (‚àíb‚àí1
1 a1)v1 + (‚àíb‚àí1
1 a2)v2 + ¬∑ ¬∑ ¬∑ + (‚àíb‚àí1
1 am)vm + (b‚àí1
1 )vm+1
+ (‚àíb‚àí1
1 b2)u2 + ¬∑ ¬∑ ¬∑ + (‚àíb‚àí1
1 bn‚àím)un‚àím.
Let H = {u2, . . . , un‚àím}. Then u1 ‚ààspan(L‚à™H), and because v1, v2, . . . , vm,
u2, . . . , un‚àím are clearly in span(L ‚à™H), it follows that
{v1, v2, . . . , vm, u1, u2, . . . , un‚àím} ‚äÜspan(L ‚à™H).
Because {v1, v2, . . . , vm, u1, u2, . . . , un‚àím} generates V, Theorem 1.5 (p. 30)
implies that span(L ‚à™H) = V.
Since H is a subset of G that contains
(n ‚àím) ‚àí1 = n ‚àí(m + 1) vectors, the theorem is true for m + 1. This
completes the induction.
Corollary 1. Let V be a vector space having a Ô¨Ånite basis. Then every
basis for V contains the same number of vectors.
Proof. Suppose that Œ≤ is a Ô¨Ånite basis for V that contains exactly n vectors,
and let Œ≥ be any other basis for V. If Œ≥ contains more than n vectors, then
we can select a subset S of Œ≥ containing exactly n + 1 vectors. Since S is
linearly independent and Œ≤ generates V, the replacement theorem implies that
n+1 ‚â§n, a contradiction. Therefore Œ≥ is Ô¨Ånite, and the number m of vectors
in Œ≥ satisÔ¨Åes m ‚â§n. Reversing the roles of Œ≤ and Œ≥ and arguing as above, we
obtain n ‚â§m. Hence m = n.
If a vector space has a Ô¨Ånite basis, Corollary 1 asserts that the number
of vectors in any basis for V is an intrinsic property of V. This fact makes
possible the following important deÔ¨Ånitions.
DeÔ¨Ånitions.
A vector space is called Ô¨Ånite-dimensional if it has a
basis consisting of a Ô¨Ånite number of vectors. The unique number of vectors

Sec. 1.6
Bases and Dimension
47
in each basis for V is called the dimension of V and is denoted by dim(V).
A vector space that is not Ô¨Ånite-dimensional is called inÔ¨Ånite-dimensional.
The following results are consequences of Examples 1 through 4.
Example 7
The vector space {0} has dimension zero.
‚ô¶
Example 8
The vector space Fn has dimension n.
‚ô¶
Example 9
The vector space Mm√ón(F) has dimension mn.
‚ô¶
Example 10
The vector space Pn(F) has dimension n + 1.
‚ô¶
The following examples show that the dimension of a vector space depends
on its Ô¨Åeld of scalars.
Example 11
Over the Ô¨Åeld of complex numbers, the vector space of complex numbers has
dimension 1. (A basis is {1}.)
‚ô¶
Example 12
Over the Ô¨Åeld of real numbers, the vector space of complex numbers has
dimension 2. (A basis is {1, i}.)
‚ô¶
In the terminology of dimension, the Ô¨Årst conclusion in the replacement
theorem states that if V is a Ô¨Ånite-dimensional vector space, then no linearly
independent subset of V can contain more than dim(V) vectors. From this
fact it follows that the vector space P(F) is inÔ¨Ånite-dimensional because it
has an inÔ¨Ånite linearly independent set, namely {1, x, x2, . . .}. This set is,
in fact, a basis for P(F). Yet nothing that we have proved in this section
guarantees an inÔ¨Ånite-dimensional vector space must have a basis. In Section
1.7 it is shown, however, that every vector space has a basis.
Just as no linearly independent subset of a Ô¨Ånite-dimensional vector space
V can contain more than dim(V) vectors, a corresponding statement can be
made about the size of a generating set.
Corollary 2. Let V be a vector space with dimension n.
(a) Any Ô¨Ånite generating set for V contains at least n vectors, and a gener-
ating set for V that contains exactly n vectors is a basis for V.

48
Chap. 1
Vector Spaces
(b) Any linearly independent subset of V that contains exactly n vectors is
a basis for V.
(c) Every linearly independent subset of V can be extended to a basis for
V.
Proof. Let Œ≤ be a basis for V.
(a) Let G be a Ô¨Ånite generating set for V. By Theorem 1.9 some subset H
of G is a basis for V. Corollary 1 implies that H contains exactly n vectors.
Since a subset of G contains n vectors, G must contain at least n vectors.
Moreover, if G contains exactly n vectors, then we must have H = G, so that
G is a basis for V.
(b) Let L be a linearly independent subset of V containing exactly n
vectors. It follows from the replacement theorem that there is a subset H of
Œ≤ containing n ‚àín = 0 vectors such that L ‚à™H generates V. Thus H = ‚àÖ,
and L generates V. Since L is also linearly independent, L is a basis for V.
(c) If L is a linearly independent subset of V containing m vectors, then
the replacement theorem asserts that there is a subset H of Œ≤ containing
exactly n ‚àím vectors such that L ‚à™H generates V. Now L ‚à™H contains at
most n vectors; therefore (a) implies that L ‚à™H contains exactly n vectors
and that L ‚à™H is a basis for V.
Example 13
It follows from Example 4 of Section 1.4 and (a) of Corollary 2 that
{x2 + 3x ‚àí2, 2x2 + 5x ‚àí3, ‚àíx2 ‚àí4x + 4}
is a basis for P2(R).
‚ô¶
Example 14
It follows from Example 5 of Section 1.4 and (a) of Corollary 2 that

1
1
1
0
	
,

1
1
0
1
	
,

1
0
1
1
	
,

0
1
1
1
	
is a basis for M2√ó2(R).
‚ô¶
Example 15
It follows from Example 3 of Section 1.5 and (b) of Corollary 2 that
{(1, 0, 0, ‚àí1), (0, 1, 0, ‚àí1), (0, 0, 1, ‚àí1), (0, 0, 0, 1)}
is a basis for R4.
‚ô¶

Sec. 1.6
Bases and Dimension
49
Example 16
For k = 0, 1, . . . , n, let pk(x) = xk+xk+1+¬∑ ¬∑ ¬∑+xn. It follows from Example 4
of Section 1.5 and (b) of Corollary 2 that
{p0(x), p1(x), . . . , pn(x)}
is a basis for Pn(F).
‚ô¶
A procedure for reducing a generating set to a basis was illustrated in
Example 6. In Section 3.4, when we have learned more about solving systems
of linear equations, we will discover a simpler method for reducing a gener-
ating set to a basis. This procedure also can be used to extend a linearly
independent set to a basis, as (c) of Corollary 2 asserts is possible.
An Overview of Dimension and Its Consequences
Theorem 1.9 as well as the replacement theorem and its corollaries contain
a wealth of information about the relationships among linearly independent
sets, bases, and generating sets. For this reason, we summarize here the main
results of this section in order to put them into better perspective.
A basis for a vector space V is a linearly independent subset of V that
generates V. If V has a Ô¨Ånite basis, then every basis for V contains the same
number of vectors. This number is called the dimension of V, and V is said
to be Ô¨Ånite-dimensional. Thus if the dimension of V is n, every basis for V
contains exactly n vectors. Moreover, every linearly independent subset of
V contains no more than n vectors and can be extended to a basis for V
by including appropriately chosen vectors. Also, each generating set for V
contains at least n vectors and can be reduced to a basis for V by excluding
appropriately chosen vectors. The Venn diagram in Figure 1.6 depicts these
relationships.
................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................
................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................
Linearly
independent
sets
Generating
sets
Bases
Figure 1.6

50
Chap. 1
Vector Spaces
The Dimension of Subspaces
Our next result relates the dimension of a subspace to the dimension of
the vector space that contains it.
Theorem 1.11. Let W be a subspace of a Ô¨Ånite-dimensional vector space
V.
Then W is Ô¨Ånite-dimensional and dim(W) ‚â§dim(V).
Moreover, if
dim(W) = dim(V), then V = W.
Proof. Let dim(V) = n. If W = {0}, then W is Ô¨Ånite-dimensional and
dim(W) = 0 ‚â§n. Otherwise, W contains a nonzero vector x1; so {x1} is a
linearly independent set. Continue choosing vectors, x1, x2, . . . , xk in W such
that {x1, x2, . . . , xk} is linearly independent. Since no linearly independent
subset of V can contain more than n vectors, this process must stop at a
stage where k ‚â§n and {x1, x2, . . . , xk} is linearly independent but adjoining
any other vector from W produces a linearly dependent set. Theorem 1.7
(p. 39) implies that {x1, x2, . . . , xk} generates W, and hence it is a basis for
W. Therefore dim(W) = k ‚â§n.
If dim(W) = n, then a basis for W is a linearly independent subset of V
containing n vectors. But Corollary 2 of the replacement theorem implies
that this basis for W is also a basis for V; so W = V.
Example 17
Let
W = {(a1, a2, a3, a4, a5) ‚ààF5 : a1 + a3 + a5 = 0, a2 = a4}.
It is easily shown that W is a subspace of F5 having
{(‚àí1, 0, 1, 0, 0), (‚àí1, 0, 0, 0, 1), (0, 1, 0, 1, 0)}
as a basis. Thus dim(W) = 3.
‚ô¶
Example 18
The set of diagonal n√ón matrices is a subspace W of Mn√ón(F) (see Example 3
of Section 1.3). A basis for W is
{E11, E22, . . . , Enn},
where Eij is the matrix in which the only nonzero entry is a 1 in the ith row
and jth column. Thus dim(W) = n.
‚ô¶
Example 19
We saw in Section 1.3 that the set of symmetric n √ó n matrices is a subspace
W of Mn√ón(F). A basis for W is
{Aij : 1 ‚â§i ‚â§j ‚â§n},

Sec. 1.6
Bases and Dimension
51
where Aij is the n √ó n matrix having 1 in the ith row and jth column, 1 in
the jth row and ith column, and 0 elsewhere. It follows that
dim(W) = n + (n ‚àí1) + ¬∑ ¬∑ ¬∑ + 1 = 1
2n(n + 1).
‚ô¶
Corollary. If W is a subspace of a Ô¨Ånite-dimensional vector space V, then
any basis for W can be extended to a basis for V.
Proof. Let S be a basis for W. Because S is a linearly independent subset of
V, Corollary 2 of the replacement theorem guarantees that S can be extended
to a basis for V.
Example 20
The set of all polynomials of the form
a18x18 + a16x16 + ¬∑ ¬∑ ¬∑ + a2x2 + a0,
where a18, a16, . . . , a2, a0 ‚ààF, is a subspace W of P18(F). A basis for W is
{1, x2, . . . , x16, x18}, which is a subset of the standard basis for P18(F).
‚ô¶
We can apply Theorem 1.11 to determine the subspaces of R2 and R3.
Since R2 has dimension 2, subspaces of R2 can be of dimensions 0, 1, or 2
only. The only subspaces of dimension 0 or 2 are {0} and R2, respectively.
Any subspace of R2 having dimension 1 consists of all scalar multiples of some
nonzero vector in R2 (Exercise 11 of Section 1.4).
If a point of R2 is identiÔ¨Åed in the natural way with a point in the Euclidean
plane, then it is possible to describe the subspaces of R2 geometrically: A
subspace of R2 having dimension 0 consists of the origin of the Euclidean
plane, a subspace of R2 with dimension 1 consists of a line through the origin,
and a subspace of R2 having dimension 2 is the entire Euclidean plane.
Similarly, the subspaces of R3 must have dimensions 0, 1, 2, or 3. Inter-
preting these possibilities geometrically, we see that a subspace of dimension
zero must be the origin of Euclidean 3-space, a subspace of dimension 1 is
a line through the origin, a subspace of dimension 2 is a plane through the
origin, and a subspace of dimension 3 is Euclidean 3-space itself.
The Lagrange Interpolation Formula
Corollary 2 of the replacement theorem can be applied to obtain a useful
formula.
Let c0, c1, . . . , cn be distinct scalars in an inÔ¨Ånite Ô¨Åeld F.
The
polynomials f0(x), f1(x), . . . , fn(x) deÔ¨Åned by
fi(x) = (x ‚àíc0) ¬∑ ¬∑ ¬∑ (x ‚àíci‚àí1)(x ‚àíci+1) ¬∑ ¬∑ ¬∑ (x ‚àícn)
(ci ‚àíc0) ¬∑ ¬∑ ¬∑ (ci ‚àíci‚àí1)(ci ‚àíci+1) ¬∑ ¬∑ ¬∑ (ci ‚àícn) =
n

k=0
kÃ∏=i
x ‚àíck
ci ‚àíck

52
Chap. 1
Vector Spaces
are called the Lagrange polynomials (associated with c0, c1, . . . , cn). Note
that each fi(x) is a polynomial of degree n and hence is in Pn(F). By re-
garding fi(x) as a polynomial function fi : F ‚ÜíF, we see that
fi(cj) =

0
if i Ã∏= j
1
if i = j.
(10)
This property of Lagrange polynomials can be used to show that Œ≤ =
{f0, f1, . . . , fn} is a linearly independent subset of Pn(F). Suppose that
n

i=0
aifi = 0
for some scalars a0, a1, . . . , an,
where 0 denotes the zero function. Then
n

i=0
aifi(cj) = 0
for j = 0, 1, . . . , n.
But also
n

i=0
aifi(cj) = aj
by (10). Hence aj = 0 for j = 0, 1, . . . , n; so Œ≤ is linearly independent. Since
the dimension of Pn(F) is n+1, it follows from Corollary 2 of the replacement
theorem that Œ≤ is a basis for Pn(F).
Because Œ≤ is a basis for Pn(F), every polynomial function g in Pn(F) is a
linear combination of polynomial functions of Œ≤, say,
g =
n

i=0
bifi.
It follows that
g(cj) =
n

i=0
bifi(cj) = bj;
so
g =
n

i=0
g(ci)fi
is the unique representation of g as a linear combination of elements of Œ≤.
This representation is called the Lagrange interpolation formula. Notice

Sec. 1.6
Bases and Dimension
53
that the preceding argument shows that if b0, b1, . . . , bn are any n + 1 scalars
in F (not necessarily distinct), then the polynomial function
g =
n

i=0
bifi
is the unique polynomial in Pn(F) such that g(cj) = bj. Thus we have found
the unique polynomial of degree not exceeding n that has speciÔ¨Åed values
bj at given points cj in its domain (j = 0, 1, . . . , n).
For example, let us
construct the real polynomial g of degree at most 2 whose graph contains the
points (1, 8), (2, 5), and (3, ‚àí4). (Thus, in the notation above, c0 = 1, c1 = 2,
c2 = 3, b0 = 8, b1 = 5, and b2 = ‚àí4.) The Lagrange polynomials associated
with c0, c1, and c2 are
f0(x) = (x ‚àí2)(x ‚àí3)
(1 ‚àí2)(1 ‚àí3) = 1
2(x2 ‚àí5x + 6),
f1(x) = (x ‚àí1)(x ‚àí3)
(2 ‚àí1)(2 ‚àí3) = ‚àí1(x2 ‚àí4x + 3),
and
f2(x) = (x ‚àí1)(x ‚àí2)
(3 ‚àí1)(3 ‚àí2) = 1
2(x2 ‚àí3x + 2).
Hence the desired polynomial is
g(x) =
2

i=0
bifi(x) = 8f0(x) + 5f1(x) ‚àí4f2(x)
= 4(x2 ‚àí5x + 6) ‚àí5(x2 ‚àí4x + 3) ‚àí2(x2 ‚àí3x + 2)
= ‚àí3x2 + 6x + 5.
An important consequence of the Lagrange interpolation formula is the fol-
lowing result: If f ‚ààPn(F) and f(ci) = 0 for n+1 distinct scalars c0, c1, . . . , cn
in F, then f is the zero function.
EXERCISES
1.
Label the following statements as true or false.
(a)
The zero vector space has no basis.
(b)
Every vector space that is generated by a Ô¨Ånite set has a basis.
(c)
Every vector space has a Ô¨Ånite basis.
(d)
A vector space cannot have more than one basis.

54
Chap. 1
Vector Spaces
(e)
If a vector space has a Ô¨Ånite basis, then the number of vectors in
every basis is the same.
(f)
The dimension of Pn(F) is n.
(g)
The dimension of Mm√ón(F) is m + n.
(h)
Suppose that V is a Ô¨Ånite-dimensional vector space, that S1 is a
linearly independent subset of V, and that S2 is a subset of V that
generates V. Then S1 cannot contain more vectors than S2.
(i)
If S generates the vector space V, then every vector in V can be
written as a linear combination of vectors in S in only one way.
(j)
Every subspace of a Ô¨Ånite-dimensional space is Ô¨Ånite-dimensional.
(k)
If V is a vector space having dimension n, then V has exactly one
subspace with dimension 0 and exactly one subspace with dimen-
sion n.
(l)
If V is a vector space having dimension n, and if S is a subset of
V with n vectors, then S is linearly independent if and only if S
spans V.
2.
Determine which of the following sets are bases for R3.
(a)
{(1, 0, ‚àí1), (2, 5, 1), (0, ‚àí4, 3)}
(b)
{(2, ‚àí4, 1), (0, 3, ‚àí1), (6, 0, ‚àí1)}
(c)
{(1, 2, ‚àí1), (1, 0, 2), (2, 1, 1)}
(d)
{(‚àí1, 3, 1), (2, ‚àí4, ‚àí3), (‚àí3, 8, 2)}
(e)
{(1, ‚àí3, ‚àí2), (‚àí3, 1, 3), (‚àí2, ‚àí10, ‚àí2)}
3.
Determine which of the following sets are bases for P2(R).
(a)
{‚àí1 ‚àíx + 2x2, 2 + x ‚àí2x2, 1 ‚àí2x + 4x2}
(b)
{1 + 2x + x2, 3 + x2, x + x2}
(c)
{1 ‚àí2x ‚àí2x2, ‚àí2 + 3x ‚àíx2, 1 ‚àíx + 6x2}
(d)
{‚àí1 + 2x + 4x2, 3 ‚àí4x ‚àí10x2, ‚àí2 ‚àí5x ‚àí6x2}
(e)
{1 + 2x ‚àíx2, 4 ‚àí2x + x2, ‚àí1 + 18x ‚àí9x2}
4.
Do the polynomials x3‚àí2x2+1, 4x2‚àíx+3, and 3x‚àí2 generate P3(R)?
Justify your answer.
5.
Is {(1, 4, ‚àí6), (1, 5, 8), (2, 1, 1), (0, 1, 0)} a linearly independent subset of
R3? Justify your answer.
6.
Give three diÔ¨Äerent bases for F2 and for M2√ó2(F).
7.
The vectors u1 = (2, ‚àí3, 1), u2 = (1, 4, ‚àí2), u3 = (‚àí8, 12, ‚àí4), u4 =
(1, 37, ‚àí17), and u5 = (‚àí3, ‚àí5, 8) generate R3. Find a subset of the set
{u1, u2, u3, u4, u5} that is a basis for R3.

Sec. 1.6
Bases and Dimension
55
8.
Let W denote the subspace of R5 consisting of all the vectors having
coordinates that sum to zero. The vectors
u1 = (2, ‚àí3, 4, ‚àí5, 2),
u2 = (‚àí6, 9, ‚àí12, 15, ‚àí6),
u3 = (3, ‚àí2, 7, ‚àí9, 1),
u4 = (2, ‚àí8, 2, ‚àí2, 6),
u5 = (‚àí1, 1, 2, 1, ‚àí3),
u6 = (0, ‚àí3, ‚àí18, 9, 12),
u7 = (1, 0, ‚àí2, 3, ‚àí2),
u8 = (2, ‚àí1, 1, ‚àí9, 7)
generate W. Find a subset of the set {u1, u2, . . . , u8} that is a basis for
W.
9.
The vectors u1 = (1, 1, 1, 1), u2 = (0, 1, 1, 1), u3 = (0, 0, 1, 1), and
u4 = (0, 0, 0, 1) form a basis for F4. Find the unique representation
of an arbitrary vector (a1, a2, a3, a4) in F4 as a linear combination of
u1, u2, u3, and u4.
10.
In each part, use the Lagrange interpolation formula to construct the
polynomial of smallest degree whose graph contains the following points.
(a)
(‚àí2, ‚àí6), (‚àí1, 5), (1, 3)
(b)
(‚àí4, 24), (1, 9), (3, 3)
(c)
(‚àí2, 3), (‚àí1, ‚àí6), (1, 0), (3, ‚àí2)
(d)
(‚àí3, ‚àí30), (‚àí2, 7), (0, 15), (1, 10)
11.
Let u and v be distinct vectors of a vector space V. Show that if {u, v}
is a basis for V and a and b are nonzero scalars, then both {u + v, au}
and {au, bv} are also bases for V.
12.
Let u, v, and w be distinct vectors of a vector space V. Show that if
{u, v, w} is a basis for V, then {u+v +w, v +w, w} is also a basis for V.
13.
The set of solutions to the system of linear equations
x1 ‚àí2x2 + x3 = 0
2x1 ‚àí3x2 + x3 = 0
is a subspace of R3. Find a basis for this subspace.
14.
Find bases for the following subspaces of F5:
W1 = {(a1, a2, a3, a4, a5) ‚ààF5 : a1 ‚àía3 ‚àía4 = 0}
and
W2 = {(a1, a2, a3, a4, a5) ‚ààF5 : a2 = a3 = a4 and a1 + a5 = 0}.
What are the dimensions of W1 and W2?

56
Chap. 1
Vector Spaces
15.
The set of all n√ón matrices having trace equal to zero is a subspace W
of Mn√ón(F) (see Example 4 of Section 1.3). Find a basis for W. What
is the dimension of W?
16.
The set of all upper triangular n √ó n matrices is a subspace W of
Mn√ón(F) (see Exercise 12 of Section 1.3). Find a basis for W. What is
the dimension of W?
17.
The set of all skew-symmetric n √ó n matrices is a subspace W of
Mn√ón(F) (see Exercise 28 of Section 1.3). Find a basis for W. What is
the dimension of W?
18.
Find a basis for the vector space in Example 5 of Section 1.2. Justify
your answer.
19.
Complete the proof of Theorem 1.8.
20.‚Ä† Let V be a vector space having dimension n, and let S be a subset of V
that generates V.
(a)
Prove that there is a subset of S that is a basis for V. (Be careful
not to assume that S is Ô¨Ånite.)
(b)
Prove that S contains at least n vectors.
21.
Prove that a vector space is inÔ¨Ånite-dimensional if and only if it contains
an inÔ¨Ånite linearly independent subset.
22.
Let W1 and W2 be subspaces of a Ô¨Ånite-dimensional vector space V.
Determine necessary and suÔ¨Écient conditions on W1 and W2 so that
dim(W1 ‚à©W2) = dim(W1).
23.
Let v1, v2, . . . , vk, v be vectors in a vector space V, and deÔ¨Åne W1 =
span({v1, v2, . . . , vk}), and W2 = span({v1, v2, . . . , vk, v}).
(a)
Find necessary and suÔ¨Écient conditions on v such that dim(W1) =
dim(W2).
(b)
State and prove a relationship involving dim(W1) and dim(W2) in
the case that dim(W1) Ã∏= dim(W2).
24.
Let f(x) be a polynomial of degree n in Pn(R). Prove that for any
g(x) ‚ààPn(R) there exist scalars c0, c1, . . . , cn such that
g(x) = c0f(x) + c1f ‚Ä≤(x) + c2f ‚Ä≤‚Ä≤(x) + ¬∑ ¬∑ ¬∑ + cnf (n)(x),
where f (n)(x) denotes the nth derivative of f(x).
25.
Let V, W, and Z be as in Exercise 21 of Section 1.2. If V and W are
vector spaces over F of dimensions m and n, determine the dimension
of Z.

Sec. 1.6
Bases and Dimension
57
26.
For a Ô¨Åxed a ‚ààR, determine the dimension of the subspace of Pn(R)
deÔ¨Åned by {f ‚ààPn(R): f(a) = 0}.
27.
Let W1 and W2 be the subspaces of P(F) deÔ¨Åned in Exercise 25 in
Section 1.3. Determine the dimensions of the subspaces W1 ‚à©Pn(F)
and W2 ‚à©Pn(F).
28.
Let V be a Ô¨Ånite-dimensional vector space over C with dimension n.
Prove that if V is now regarded as a vector space over R, then dim V =
2n. (See Examples 11 and 12.)
Exercises 29‚Äì34 require knowledge of the sum and direct sum of subspaces,
as deÔ¨Åned in the exercises of Section 1.3.
29. (a)
Prove that if W1 and W2 are Ô¨Ånite-dimensional subspaces of a
vector space V, then the subspace W1 + W2 is Ô¨Ånite-dimensional,
and dim(W1 + W2) = dim(W1) + dim(W2) ‚àídim(W1 ‚à©W2). Hint:
Start with a basis {u1, u2, . . . , uk} for W1 ‚à©W2 and extend this
set to a basis {u1, u2, . . . , uk, v1, v2, . . . vm} for W1 and to a basis
{u1, u2, . . . , uk, w1, w2, . . . wp} for W2.
(b)
Let W1 and W2 be Ô¨Ånite-dimensional subspaces of a vector space
V, and let V = W1 + W2. Deduce that V is the direct sum of W1
and W2 if and only if dim(V) = dim(W1) + dim(W2).
30.
Let
V = M2√ó2(F),
W1 =

a
b
c
a
	
‚ààV: a, b, c ‚ààF

,
and
W2 =

0
a
‚àía
b
	
‚ààV: a, b ‚ààF

.
Prove that W1 and W2 are subspaces of V, and Ô¨Ånd the dimensions of
W1, W2, W1 + W2, and W1 ‚à©W2.
31.
Let W1 and W2 be subspaces of a vector space V having dimensions m
and n, respectively, where m ‚â•n.
(a)
Prove that dim(W1 ‚à©W2) ‚â§n.
(b)
Prove that dim(W1 + W2) ‚â§m + n.
32. (a)
Find an example of subspaces W1 and W2 of R3 with dimensions
m and n, where m > n > 0, such that dim(W1 ‚à©W2) = n.
(b)
Find an example of subspaces W1 and W2 of R3 with dimensions
m and n, where m > n > 0, such that dim(W1 + W2) = m + n.

58
Chap. 1
Vector Spaces
(c)
Find an example of subspaces W1 and W2 of R3 with dimensions
m and n, where m ‚â•n, such that both dim(W1 ‚à©W2) < n and
dim(W1 + W2) < m + n.
33. (a)
Let W1 and W2 be subspaces of a vector space V such that V =
W1‚äïW2. If Œ≤1 and Œ≤2 are bases for W1 and W2, respectively, show
that Œ≤1 ‚à©Œ≤2 = ‚àÖand Œ≤1 ‚à™Œ≤2 is a basis for V.
(b)
Conversely, let Œ≤1 and Œ≤2 be disjoint bases for subspaces W1 and
W2, respectively, of a vector space V. Prove that if Œ≤1 ‚à™Œ≤2 is a
basis for V, then V = W1 ‚äïW2.
34. (a)
Prove that if W1 is any subspace of a Ô¨Ånite-dimensional vector
space V, then there exists a subspace W2 of V such that V =
W1 ‚äïW2.
(b)
Let V = R2 and W1 = {(a1, 0): a1 ‚ààR}. Give examples of two
diÔ¨Äerent subspaces W2 and W‚Ä≤
2 such that V = W1 ‚äïW2 and V =
W1 ‚äïW‚Ä≤
2.
The following exercise requires familiarity with Exercise 31 of Section 1.3.
35.
Let W be a subspace of a Ô¨Ånite-dimensional vector space V, and consider
the basis {u1, u2, . . . , uk} for W. Let {u1, u2, . . . , uk, uk+1, . . . , un} be
an extension of this basis to a basis for V.
(a)
Prove that {uk+1 + W, uk+2 + W, . . . , un + W} is a basis for V/W.
(b)
Derive a formula relating dim(V), dim(W), and dim(V/W).
1.7‚àó
MAXIMAL LINEARLY INDEPENDENT SUBSETS
In this section, several signiÔ¨Åcant results from Section 1.6 are extended to
inÔ¨Ånite-dimensional vector spaces. Our principal goal here is to prove that
every vector space has a basis.
This result is important in the study of
inÔ¨Ånite-dimensional vector spaces because it is often diÔ¨Écult to construct an
explicit basis for such a space. Consider, for example, the vector space of
real numbers over the Ô¨Åeld of rational numbers. There is no obvious way to
construct a basis for this space, and yet it follows from the results of this
section that such a basis does exist.
The diÔ¨Éculty that arises in extending the theorems of the preceding sec-
tion to inÔ¨Ånite-dimensional vector spaces is that the principle of mathematical
induction, which played a crucial role in many of the proofs of Section 1.6,
is no longer adequate.
Instead, a more general result called the maximal
principle is needed. Before stating this principle, we need to introduce some
terminology.
DeÔ¨Ånition. Let F be a family of sets.
A member M of F is called
maximal (with respect to set inclusion) if M is contained in no member of
F other than M itself.

Sec. 1.7
Maximal Linearly Independent Subsets
59
Example 1
Let F be the family of all subsets of a nonempty set S. (This family F is
called the power set of S.) The set S is easily seen to be a maximal element
of F.
‚ô¶
Example 2
Let S and T be disjoint nonempty sets, and let F be the union of their power
sets. Then S and T are both maximal elements of F.
‚ô¶
Example 3
Let F be the family of all Ô¨Ånite subsets of an inÔ¨Ånite set S. Then F has no
maximal element. For if M is any member of F and s is any element of S
that is not in M, then M ‚à™{s} is a member of F that contains M as a proper
subset.
‚ô¶
DeÔ¨Ånition.
A collection of sets C is called a chain (or nest or tower)
if for each pair of sets A and B in C, either A ‚äÜB or B ‚äÜA.
Example 4
For each positive integer n let An = {1, 2, . . . , n}. Then the collection of
sets C = {An : n = 1, 2, 3, . . .} is a chain. In fact, Am ‚äÜAn if and only if
m ‚â§n.
‚ô¶
With this terminology we can now state the maximal principle.
Maximal Principle.4 Let F be a family of sets. If, for each chain C ‚äÜF,
there exists a member of F that contains each member of C, then F contains
a maximal member.
Because the maximal principle guarantees the existence of maximal el-
ements in a family of sets satisfying the hypothesis above, it is useful to
reformulate the deÔ¨Ånition of a basis in terms of a maximal property. In The-
orem 1.12, we show that this is possible; in fact, the concept deÔ¨Åned next is
equivalent to a basis.
DeÔ¨Ånition. Let S be a subset of a vector space V. A maximal linearly
independent subset of S is a subset B of S satisfying both of the following
conditions.
(a) B is linearly independent.
(b) The only linearly independent subset of S that contains B is B itself.
4The Maximal Principle is logically equivalent to the Axiom of Choice, which
is an assumption in most axiomatic developments of set theory. For a treatment
of set theory using the Maximal Principle, see John L. Kelley, General Topology,
Graduate Texts in Mathematics Series, Vol. 27, Springer-Verlag, 1991.

60
Chap. 1
Vector Spaces
Example 5
Example 2 of Section 1.4 shows that
{x3 ‚àí2x2 ‚àí5x ‚àí3, 3x3 ‚àí5x2 ‚àí4x ‚àí9}
is a maximal linearly independent subset of
S = {2x3 ‚àí2x2 + 12x ‚àí6, x3 ‚àí2x2 ‚àí5x ‚àí3, 3x3 ‚àí5x2 ‚àí4x ‚àí9}
in P2(R). In this case, however, any subset of S consisting of two polynomials
is easily shown to be a maximal linearly independent subset of S.
Thus
maximal linearly independent subsets of a set need not be unique.
‚ô¶
A basis Œ≤ for a vector space V is a maximal linearly independent subset
of V, because
1. Œ≤ is linearly independent by deÔ¨Ånition.
2. If v ‚ààV and v /‚ààŒ≤, then Œ≤ ‚à™{v} is linearly dependent by Theorem 1.7
(p. 39) because span(Œ≤) = V.
Our next result shows that the converse of this statement is also true.
Theorem 1.12. Let V be a vector space and S a subset that generates
V. If Œ≤ is a maximal linearly independent subset of S, then Œ≤ is a basis for V.
Proof. Let Œ≤ be a maximal linearly independent subset of S. Because Œ≤
is linearly independent, it suÔ¨Éces to prove that Œ≤ generates V. We claim
that S ‚äÜspan(Œ≤), for otherwise there exists a v ‚ààS such that v /‚ààspan(Œ≤).
Since Theorem 1.7 (p. 39) implies that Œ≤ ‚à™{v} is linearly independent, we
have contradicted the maximality of Œ≤. Therefore S ‚äÜspan(Œ≤). Because
span(S) = V, it follows from Theorem 1.5 (p. 30) that span(Œ≤) = V.
Thus a subset of a vector space is a basis if and only if it is a maximal
linearly independent subset of the vector space. Therefore we can accomplish
our goal of proving that every vector space has a basis by showing that every
vector space contains a maximal linearly independent subset.
This result
follows immediately from the next theorem.
Theorem 1.13. Let S be a linearly independent subset of a vector space
V. There exists a maximal linearly independent subset of V that contains S.
Proof. Let F denote the family of all linearly independent subsets of V
that contain S. In order to show that F contains a maximal element, we must
show that if C is a chain in F, then there exists a member U of F that contains
each member of C. We claim that U, the union of the members of C, is the
desired set. Clearly U contains each member of C, and so it suÔ¨Éces to prove

Sec. 1.7
Maximal Linearly Independent Subsets
61
that U ‚ààF (i.e., that U is a linearly independent subset of V that contains S).
Because each member of C is a subset of V containing S, we have S ‚äÜU ‚äÜV.
Thus we need only prove that U is linearly independent. Let u1, u2, . . . , un
be in U and a1, a2, . . . , an be scalars such that a1u1 + a2u2 + ¬∑ ¬∑ ¬∑ + anun = 0.
Because ui ‚ààU for i = 1, 2, . . . , n, there exists a set Ai in C such that ui ‚ààAi.
But since C is a chain, one of these sets, say Ak, contains all the others. Thus
ui ‚ààAk for i = 1, 2, . . . , n. However, Ak is a linearly independent set; so
a1u1 + a2u2 + ¬∑ ¬∑ ¬∑ + anun = 0 implies that a1 = a2 = ¬∑ ¬∑ ¬∑ = an = 0. It follows
that U is linearly independent.
The maximal principle implies that F has a maximal element. This el-
ement is easily seen to be a maximal linearly independent subset of V that
contains S.
Corollary. Every vector space has a basis.
It can be shown, analogously to Corollary 1 of the replacement theorem
(p. 46), that every basis for an inÔ¨Ånite-dimensional vector space has the same
cardinality. (Sets have the same cardinality if there is a one-to-one and onto
mapping between them.) (See, for example, N. Jacobson, Lectures in Ab-
stract Algebra, vol.
2, Linear Algebra, D. Van Nostrand Company, New
York, 1953, p. 240.)
Exercises 4-7 extend other results from Section 1.6 to inÔ¨Ånite-dimensional
vector spaces.
EXERCISES
1.
Label the following statements as true or false.
(a)
Every family of sets contains a maximal element.
(b)
Every chain contains a maximal element.
(c)
If a family of sets has a maximal element, then that maximal
element is unique.
(d)
If a chain of sets has a maximal element, then that maximal ele-
ment is unique.
(e)
A basis for a vector space is a maximal linearly independent subset
of that vector space.
(f)
A maximal linearly independent subset of a vector space is a basis
for that vector space.
2.
Show that the set of convergent sequences is an inÔ¨Ånite-dimensional
subspace of the vector space of all sequences of real numbers.
(See
Exercise 21 in Section 1.3.)
3.
Let V be the set of real numbers regarded as a vector space over the
Ô¨Åeld of rational numbers. Prove that V is inÔ¨Ånite-dimensional. Hint:

62
Chap. 1
Vector Spaces
Use the fact that œÄ is transcendental, that is, œÄ is not a zero of any
polynomial with rational coeÔ¨Écients.
4.
Let W be a subspace of a (not necessarily Ô¨Ånite-dimensional) vector
space V. Prove that any basis for W is a subset of a basis for V.
5.
Prove the following inÔ¨Ånite-dimensional version of Theorem 1.8 (p. 43):
Let Œ≤ be a subset of an inÔ¨Ånite-dimensional vector space V. Then Œ≤ is a
basis for V if and only if for each nonzero vector v in V, there exist unique
vectors u1, u2, . . . , un in Œ≤ and unique nonzero scalars c1, c2, . . . , cn such
that v = c1u1 + c2u2 + ¬∑ ¬∑ ¬∑ + cnun.
6.
Prove the following generalization of Theorem 1.9 (p. 44): Let S1 and
S2 be subsets of a vector space V such that S1 ‚äÜS2. If S1 is linearly
independent and S2 generates V, then there exists a basis Œ≤ for V such
that S1 ‚äÜŒ≤ ‚äÜS2. Hint: Apply the maximal principle to the family of
all linearly independent subsets of S2 that contain S1, and proceed as
in the proof of Theorem 1.13.
7.
Prove the following generalization of the replacement theorem. Let Œ≤
be a basis for a vector space V, and let S be a linearly independent
subset of V. There exists a subset S1 of Œ≤ such that S ‚à™S1 is a basis
for V.
INDEX OF DEFINITIONS FOR CHAPTER 1
Additive inverse
12
Basis
43
Cancellation law
11
Column vector
8
Chain
59
Degree of a polynomial
9
Diagonal entries of a matrix
8
Diagonal matrix
18
Dimension
47
Finite-dimensional space
46
Generates
30
InÔ¨Ånite-dimensional space
47
Lagrange interpolation formula
52
Lagrange polynomials
52
Linear combination
24
Linearly dependent
36
Linearly independent
37
Matrix
8
Maximal element of a family
of sets
58
Maximal linearly independent
subset
59
n-tuple
7
Polynomial
9
Row vector
8
Scalar
7
Scalar multiplication
6
Sequence
11
Span of a subset
30
Spans
30
Square matrix
9
Standard basis for Fn
43
Standard basis for Pn(F)
43
Subspace
16
Subspace generated by the elements
of a set
30
Symmetric matrix
17
Trace
18
Transpose
17
Trivial representation of 0
36

Chap. 1
Index of DeÔ¨Ånitions
63
Vector
7
Vector addition
6
Vector space
6
Zero matrix
8
Zero polynomial
9
Zero subspace
16
Zero vector
12
Zero vector space
15

2
Linear Transformations
and Matrices
2.1
Linear Transformations, Null spaces, and Ranges
2.2
The Matrix Representation of a Linear Transformation
2.3
Composition of Linear Transformations and Matrix Multiplication
2.4
Invertibility and Isomorphisms
2.5
The Change of Coordinate Matrix
2.6*
Dual Spaces
2.7*
Homogeneous Linear DiÔ¨Äerential Equations with Constant CoeÔ¨Écients
In Chapter 1, we developed the theory of abstract vector spaces in consid-
erable detail. It is now natural to consider those functions deÔ¨Åned on vector
spaces that in some sense ‚Äúpreserve‚Äù the structure. These special functions
are called linear transformations, and they abound in both pure and applied
mathematics. In calculus, the operations of diÔ¨Äerentiation and integration
provide us with two of the most important examples of linear transforma-
tions (see Examples 6 and 7 of Section 2.1). These two examples allow us
to reformulate many of the problems in diÔ¨Äerential and integral equations in
terms of linear transformations on particular vector spaces (see Sections 2.7
and 5.2).
In geometry, rotations, reÔ¨Çections, and projections (see Examples 2, 3,
and 4 of Section 2.1) provide us with another class of linear transformations.
Later we use these transformations to study rigid motions in Rn (Section
6.10).
In the remaining chapters, we see further examples of linear transforma-
tions occurring in both the physical and the social sciences. Throughout this
chapter, we assume that all vector spaces are over a common Ô¨Åeld F.
2.1
LINEAR TRANSFORMATIONS, NULL SPACES, AND RANGES
In this section, we consider a number of examples of linear transformations.
Many of these transformations are studied in more detail in later sections.
Recall that a function T with domain V and codomain W is denoted by
64

Sec. 2.1
Linear Transformations, Null Spaces, and Ranges
65
T: V ‚ÜíW. (See Appendix B.)
DeÔ¨Ånition. Let V and W be vector spaces (over F). We call a function
T: V ‚ÜíW a linear transformation from V to W if, for all x, y ‚ààV and
c ‚ààF, we have
(a) T(x + y) = T(x) + T(y) and
(b) T(cx) = cT(x).
If the underlying Ô¨Åeld F is the Ô¨Åeld of rational numbers, then (a) implies
(b) (see Exercise 37), but, in general (a) and (b) are logically independent.
See Exercises 38 and 39.
We often simply call T linear. The reader should verify the following
properties of a function T: V ‚ÜíW. (See Exercise 7.)
1. If T is linear, then T(0) = 0.
2. T is linear if and only if T(cx + y) = cT(x) + T(y) for all x, y ‚ààV and
c ‚ààF.
3. If T is linear, then T(x ‚àíy) = T(x) ‚àíT(y) for all x, y ‚ààV.
4. T is linear if and only if, for x1, x2, . . . , xn ‚ààV and a1, a2, . . . , an ‚ààF,
we have
T
 n

i=1
aixi

=
n

i=1
aiT(xi).
We generally use property 2 to prove that a given transformation is linear.
Example 1
DeÔ¨Åne
T: R2 ‚ÜíR2 by T(a1, a2) = (2a1 + a2, a1).
To show that T is linear, let c ‚ààR and x, y ‚ààR2, where x = (b1, b2) and
y = (d1, d2). Since
cx + y = (cb1 + d1, cb2 + d2),
we have
T(cx + y) = (2(cb1 + d1) + cb2 + d2, cb1 + d1).
Also
cT(x) + T(y) = c(2b1 + b2, b1) + (2d1 + d2, d1)
= (2cb1 + cb2 + 2d1 + d2, cb1 + d1)
= (2(cb1 + d1) + cb2 + d2, cb1 + d1).
So T is linear.
‚ô¶

66
Chap. 2
Linear Transformations and Matrices

1





(a1, a2)
TŒ∏(a1, a2)
Œ∏
Œ±
.................................................
.....................
k
M
(a) Rotation




@
@
@@
R
(a1, a2)
T(a1, a2) =
(a1, ‚àía2)
I

?
6
.........................................................................................................................................
(b) ReÔ¨Çection

3
-
(a1, a2)
T(a1, a2) =
(a1, 0)
(c) Projection
Figure 2.1
As we will see in Chapter 6, the applications of linear algebra to geometry
are wide and varied. The main reason for this is that most of the important
geometrical transformations are linear. Three particular transformations that
we now consider are rotation, reÔ¨Çection, and projection. We leave the proofs
of linearity to the reader.
Example 2
For any angle Œ∏, deÔ¨Åne TŒ∏ : R2 ‚ÜíR2 by the rule: TŒ∏(a1, a2) is the vector
obtained by rotating (a1, a2) counterclockwise by Œ∏ if (a1, a2) Ã∏= (0, 0), and
TŒ∏(0, 0) = (0, 0). Then TŒ∏ : R2 ‚ÜíR2 is a linear transformation that is called
the rotation by Œ∏.
We determine an explicit formula for TŒ∏. Fix a nonzero vector (a1, a2) ‚àà
R2.
Let Œ± be the angle that (a1, a2) makes with the positive x-axis (see
Figure 2.1(a)), and let r =

a2
1 + a2
2. Then a1 = r cos Œ± and a2 = r sin Œ±.
Also, TŒ∏(a1, a2) has length r and makes an angle Œ± + Œ∏ with the positive
x-axis. It follows that
TŒ∏(a1, a2) = (r cos(Œ± + Œ∏), r sin(Œ± + Œ∏))
= (r cos Œ± cos Œ∏ ‚àír sin Œ± sin Œ∏, r cos Œ± sin Œ∏ + r sin Œ± cos Œ∏)
= (a1 cos Œ∏ ‚àía2 sin Œ∏, a1 sin Œ∏ + a2 cos Œ∏).
Finally, observe that this same formula is valid for (a1, a2) = (0, 0).
It is now easy to show, as in Example 1, that TŒ∏ is linear.
‚ô¶
Example 3
DeÔ¨Åne T: R2 ‚ÜíR2 by T(a1, a2) = (a1, ‚àía2).
T is called the reÔ¨Çection
about the x -axis. (See Figure 2.1(b).)
‚ô¶
Example 4
DeÔ¨Åne T: R2 ‚ÜíR2 by T(a1, a2) = (a1, 0). T is called the projection on the
x-axis. (See Figure 2.1(c).)
‚ô¶

Sec. 2.1
Linear Transformations, Null Spaces, and Ranges
67
We now look at some additional examples of linear transformations.
Example 5
DeÔ¨Åne T: Mm√ón(F) ‚ÜíMn√óm(F) by T(A) = At, where At is the transpose
of A, deÔ¨Åned in Section 1.3. Then T is a linear transformation by Exercise 3
of Section 1.3.
‚ô¶
Example 6
DeÔ¨Åne T: Pn(R) ‚ÜíPn‚àí1(R) by T(f(x)) = f ‚Ä≤(x), where f ‚Ä≤(x) denotes the
derivative of f(x). To show that T is linear, let g(x), h(x) ‚ààPn(R) and a ‚ààR.
Now
T(ag(x) + h(x)) = (ag(x) + h(x))‚Ä≤ = ag‚Ä≤(x) + h‚Ä≤(x) = aT(g(x)) + T(h(x)).
So by property 2 above, T is linear.
‚ô¶
Example 7
Let V = C(R), the vector space of continuous real-valued functions on R. Let
a, b ‚ààR, a < b. DeÔ¨Åne T: V ‚ÜíR by
T(f) =
 b
a
f(t) dt
for all f ‚ààV. Then T is a linear transformation because the deÔ¨Ånite integral
of a linear combination of functions is the same as the linear combination of
the deÔ¨Ånite integrals of the functions.
‚ô¶
Two very important examples of linear transformations that appear fre-
quently in the remainder of the book, and therefore deserve their own nota-
tion, are the identity and zero transformations.
For vector spaces V and W (over F), we deÔ¨Åne the identity transfor-
mation IV : V ‚ÜíV by IV(x) = x for all x ‚ààV and the zero transformation
T0 : V ‚ÜíW by T0(x) = 0 for all x ‚ààV.
It is clear that both of these
transformations are linear. We often write I instead of IV.
We now turn our attention to two very important sets associated with
linear transformations: the range and null space. The determination of these
sets allows us to examine more closely the intrinsic properties of a linear
transformation.
DeÔ¨Ånitions. Let V and W be vector spaces, and let T: V ‚ÜíW be linear.
We deÔ¨Åne the null space (or kernel) N(T) of T to be the set of all vectors
x in V such that T(x) = 0; that is, N(T) = {x ‚ààV: T(x) = 0}.
We deÔ¨Åne the range (or image) R(T) of T to be the subset of W con-
sisting of all images (under T) of vectors in V; that is, R(T) = {T(x): x ‚ààV}.

68
Chap. 2
Linear Transformations and Matrices
Example 8
Let V and W be vector spaces, and let I: V ‚ÜíV and T0 : V ‚ÜíW be the
identity and zero transformations, respectively. Then N(I) = {0}, R(I) = V,
N(T0) = V, and R(T0) = {0}.
‚ô¶
Example 9
Let T: R3 ‚ÜíR2 be the linear transformation deÔ¨Åned by
T(a1, a2, a3) = (a1 ‚àía2, 2a3).
It is left as an exercise to verify that
N(T) = {(a, a, 0): a ‚ààR}
and
R(T) = R2.
‚ô¶
In Examples 8 and 9, we see that the range and null space of each of the
linear transformations is a subspace. The next result shows that this is true
in general.
Theorem 2.1. Let V and W be vector spaces and T: V ‚ÜíW be linear.
Then N(T) and R(T) are subspaces of V and W, respectively.
Proof. To clarify the notation, we use the symbols 0 V and 0 W to denote
the zero vectors of V and W, respectively.
Since T(0 V) = 0 W, we have that 0 V ‚ààN(T). Let x, y ‚ààN(T) and c ‚ààF.
Then T(x+y) = T(x)+T(y) = 0 W +0 W = 0 W, and T(cx) = cT(x) = c0 W =
0 W. Hence x + y ‚ààN(T) and cx ‚ààN(T), so that N(T) is a subspace of V.
Because T(0 V) = 0 W, we have that 0 W ‚ààR(T). Now let x, y ‚ààR(T) and
c ‚ààF. Then there exist v and w in V such that T(v) = x and T(w) = y. So
T(v +w) = T(v)+T(w) = x+y, and T(cv) = cT(v) = cx. Thus x+y ‚ààR(T)
and cx ‚ààR(T), so R(T) is a subspace of W.
The next theorem provides a method for Ô¨Ånding a spanning set for the
range of a linear transformation.
With this accomplished, a basis for the
range is easy to discover using the technique of Example 6 of Section 1.6.
Theorem 2.2. Let V and W be vector spaces, and let T: V ‚ÜíW be
linear. If Œ≤ = {v1, v2, . . . , vn} is a basis for V, then
R(T) = span(T(Œ≤)) = span({T(v1), T(v2), . . . , T(vn)}).
Proof. Clearly T(vi) ‚ààR(T) for each i.
Because R(T) is a subspace,
R(T) contains span({T(v1), T(v2), . . . , T(vn)}) = span(T(Œ≤)) by Theorem 1.5
(p. 30).

Sec. 2.1
Linear Transformations, Null Spaces, and Ranges
69
Now suppose that w ‚ààR(T). Then w = T(v) for some v ‚ààV. Because Œ≤
is a basis for V, we have
v =
n

i=1
aivi
for some a1, a2, . . . , an ‚ààF.
Since T is linear, it follows that
w = T(v) =
n

i=1
aiT(vi) ‚ààspan(T(Œ≤)).
So R(T) is contained in span(T(Œ≤)).
It should be noted that Theorem 2.2 is true if Œ≤ is inÔ¨Ånite, that is, R(T) =
span({T(v): v ‚ààŒ≤}). (See Exercise 33.)
The next example illustrates the usefulness of Theorem 2.2.
Example 10
DeÔ¨Åne the linear transformation T: P2(R) ‚ÜíM2√ó2(R) by
T(f(x)) =

f(1) ‚àíf(2)
0
0
f(0)
	
.
Since Œ≤ = {1, x, x2} is a basis for P2(R), we have
R(T) = span(T(Œ≤)) = span({T(1), T(x), T(x2)})
= span

0
0
0
1
	
,

‚àí1
0
0
0
	
,

‚àí3
0
0
0
		
= span

0
0
0
1
	
,

‚àí1
0
0
0
		
.
Thus we have found a basis for R(T), and so dim(R(T)) = 2.
‚ô¶
As in Chapter 1, we measure the ‚Äúsize‚Äù of a subspace by its dimension.
The null space and range are so important that we attach special names to
their respective dimensions.
DeÔ¨Ånitions.
Let V and W be vector spaces, and let T: V ‚ÜíW be
linear. If N(T) and R(T) are Ô¨Ånite-dimensional, then we deÔ¨Åne the nullity
of T, denoted nullity(T), and the rank of T, denoted rank(T), to be the
dimensions of N(T) and R(T), respectively.
ReÔ¨Çecting on the action of a linear transformation, we see intuitively that
the larger the nullity, the smaller the rank. In other words, the more vectors
that are carried into 0, the smaller the range. The same heuristic reasoning
tells us that the larger the rank, the smaller the nullity. This balance between
rank and nullity is made precise in the next theorem, appropriately called the
dimension theorem.

70
Chap. 2
Linear Transformations and Matrices
Theorem 2.3 (Dimension Theorem). Let V and W be vector spaces,
and let T: V ‚ÜíW be linear. If V is Ô¨Ånite-dimensional, then
nullity(T) + rank(T) = dim(V).
Proof. Suppose that dim(V) = n, dim(N(T)) = k, and {v1, v2, . . . , vk} is
a basis for N(T). By the corollary to Theorem 1.11 (p. 51), we may extend
{v1, v2, . . . , vk} to a basis Œ≤ = {v1, v2, . . . , vn} for V. We claim that S =
{T(vk+1), T(vk+2), . . . , T(vn)} is a basis for R(T).
First we prove that S generates R(T). Using Theorem 2.2 and the fact
that T(vi) = 0 for 1 ‚â§i ‚â§k, we have
R(T) = span({T(v1), T(v2), . . . , T(vn)}
= span({T(vk+1), T(vk+2), . . . , T(vn)} = span(S).
Now we prove that S is linearly independent. Suppose that
n

i=k+1
biT(vi) = 0
for bk+1, bk+2, . . . , bn ‚ààF.
Using the fact that T is linear, we have
T

n

i=k+1
bivi

= 0.
So
n

i=k+1
bivi ‚ààN(T).
Hence there exist c1, c2, . . . , ck ‚ààF such that
n

i=k+1
bivi =
k

i=1
civi
or
k

i=1
(‚àíci)vi +
n

i=k+1
bivi = 0.
Since Œ≤ is a basis for V, we have bi = 0 for all i. Hence S is linearly indepen-
dent. Notice that this argument also shows that T(vk+1), T(vk+2), . . . , T(vn)
are distinct; therefore rank(T) = n ‚àík.
If we apply the dimension theorem to the linear transformation T in Ex-
ample 9, we have that nullity(T) + 2 = 3, so nullity(T) = 1.
The reader should review the concepts of ‚Äúone-to-one‚Äù and ‚Äúonto‚Äù pre-
sented in Appendix B. Interestingly, for a linear transformation, both of these
concepts are intimately connected to the rank and nullity of the transforma-
tion. This is demonstrated in the next two theorems.

Sec. 2.1
Linear Transformations, Null Spaces, and Ranges
71
Theorem 2.4. Let V and W be vector spaces, and let T: V ‚ÜíW be
linear. Then T is one-to-one if and only if N(T) = {0}.
Proof. Suppose that T is one-to-one and x ‚ààN(T). Then T(x) = 0 =
T(0). Since T is one-to-one, we have x = 0. Hence N(T) = {0}.
Now assume that N(T) = {0}, and suppose that T(x) = T(y).
Then
0 = T(x) ‚àíT(y) = T(x ‚àíy) by property 3 on page 65. Therefore x ‚àíy ‚àà
N(T) = {0}. So x ‚àíy = 0, or x = y. This means that T is one-to-one.
The reader should observe that Theorem 2.4 allows us to conclude that
the transformation deÔ¨Åned in Example 9 is not one-to-one.
Surprisingly, the conditions of one-to-one and onto are equivalent in an
important special case.
Theorem 2.5. Let V and W be vector spaces of equal (Ô¨Ånite) dimension,
and let T: V ‚ÜíW be linear. Then the following are equivalent.
(a) T is one-to-one.
(b) T is onto.
(c) rank(T) = dim(V).
Proof. From the dimension theorem, we have
nullity(T) + rank(T) = dim(V).
Now, with the use of Theorem 2.4, we have that T is one-to-one if and only if
N(T) = {0}, if and only if nullity(T) = 0, if and only if rank(T) = dim(V), if
and only if rank(T) = dim(W), and if and only if dim(R(T)) = dim(W). By
Theorem 1.11 (p. 50), this equality is equivalent to R(T) = W, the deÔ¨Ånition
of T being onto.
We note that if V is not Ô¨Ånite-dimensional and T: V ‚ÜíV is linear, then
it does not follow that one-to-one and onto are equivalent. (See Exercises 15,
16, and 21.)
The linearity of T in Theorems 2.4 and 2.5 is essential, for it is easy to
construct examples of functions from R into R that are not one-to-one, but
are onto, and vice versa.
The next two examples make use of the preceding theorems in determining
whether a given linear transformation is one-to-one or onto.
Example 11
Let T: P2(R) ‚ÜíP3(R) be the linear transformation deÔ¨Åned by
T(f(x)) = 2f ‚Ä≤(x) +
 x
0
3f(t) dt.

72
Chap. 2
Linear Transformations and Matrices
Now
R(T) = span({T(1), T(x), T(x2)}) = span({3x, 2 + 3
2x2, 4x + x3}).
Since {3x, 2 + 3
2x2, 4x + x3} is linearly independent, rank(T) = 3.
Since
dim(P3(R)) = 4, T is not onto. From the dimension theorem, nullity(T) +
3 = 3. So nullity(T) = 0, and therefore, N(T) = {0}. We conclude from
Theorem 2.4 that T is one-to-one.
‚ô¶
Example 12
Let T: F2 ‚ÜíF2 be the linear transformation deÔ¨Åned by
T(a1, a2) = (a1 + a2, a1).
It is easy to see that N(T) = {0}; so T is one-to-one. Hence Theorem 2.5
tells us that T must be onto.
‚ô¶
In Exercise 14, it is stated that if T is linear and one-to-one, then a
subset S is linearly independent if and only if T(S) is linearly independent.
Example 13 illustrates the use of this result.
Example 13
Let T: P2(R) ‚ÜíR3 be the linear transformation deÔ¨Åned by
T(a0 + a1x + a2x2) = (a0, a1, a2).
Clearly T is linear and one-to-one. Let S = {2 ‚àíx + 3x2, x + x2, 1 ‚àí2x2}.
Then S is linearly independent in P2(R) because
T(S) = {(2, ‚àí1, 3), (0, 1, 1), (1, 0, ‚àí2)}
is linearly independent in R3.
‚ô¶
In Example 13, we transferred a property from the vector space of polyno-
mials to a property in the vector space of 3-tuples. This technique is exploited
more fully later.
One of the most important properties of a linear transformation is that it is
completely determined by its action on a basis. This result, which follows from
the next theorem and corollary, is used frequently throughout the book.
Theorem 2.6. Let V and W be vector spaces over F, and suppose that
{v1, v2, . . . , vn} is a basis for V. For w1, w2, . . . , wn in W, there exists exactly
one linear transformation T: V ‚ÜíW such that T(vi) = wi for i = 1, 2, . . . , n.

Sec. 2.1
Linear Transformations, Null Spaces, and Ranges
73
Proof. Let x ‚ààV. Then
x =
n

i=1
aivi,
where a1a2, . . . , an are unique scalars. DeÔ¨Åne
T: V ‚ÜíW
by
T(x) =
n

i=1
aiwi.
(a) T is linear: Suppose that u, v ‚ààV and d ‚ààF. Then we may write
u =
n

i=1
bivi
and
v =
n

i=1
civi
for some scalars b1, b2, . . . , bn, c1, c2, . . . , cn. Thus
du + v =
n

i=1
(dbi + ci)vi.
So
T(du + v) =
n

i=1
(dbi + ci)wi = d
n

i=1
biwi +
n

i=1
ciwi = dT(u) + T(v).
(b) Clearly
T(vi) = wi
for i = 1, 2, . . . , n.
(c) T is unique: Suppose that U: V ‚ÜíW is linear and U(vi) = wi for
i = 1, 2, . . . , n. Then for x ‚ààV with
x =
n

i=1
aivi,
we have
U(x) =
n

i=1
aiU(vi) =
n

i=1
aiwi = T(x).
Hence U = T.
Corollary. Let V and W be vector spaces, and suppose that V has a
Ô¨Ånite basis {v1, v2, . . . , vn}. If U, T: V ‚ÜíW are linear and U(vi) = T(vi) for
i = 1, 2, . . . , n, then U = T.

74
Chap. 2
Linear Transformations and Matrices
Example 14
Let T: R2 ‚ÜíR2 be the linear transformation deÔ¨Åned by
T(a1, a2) = (2a2 ‚àía1, 3a1),
and suppose that U: R2 ‚ÜíR2 is linear. If we know that U(1, 2) = (3, 3) and
U(1, 1) = (1, 3), then U = T. This follows from the corollary and from the
fact that {(1, 2), (1, 1)} is a basis for R2.
‚ô¶
EXERCISES
1.
Label the following statements as true or false. In each part, V and W
are Ô¨Ånite-dimensional vector spaces (over F), and T is a function from
V to W.
(a)
If T is linear, then T preserves sums and scalar products.
(b)
If T(x + y) = T(x) + T(y), then T is linear.
(c)
T is one-to-one if and only if the only vector x such that T(x) = 0
is x = 0.
(d)
If T is linear, then T(0 V) = 0 W.
(e)
If T is linear, then nullity(T) + rank(T) = dim(W).
(f)
If T is linear, then T carries linearly independent subsets of V onto
linearly independent subsets of W.
(g)
If T, U: V ‚ÜíW are both linear and agree on a basis for V, then
T = U.
(h)
Given x1, x2 ‚ààV and y1, y2 ‚ààW, there exists a linear transforma-
tion T: V ‚ÜíW such that T(x1) = y1 and T(x2) = y2.
For Exercises 2 through 6, prove that T is a linear transformation, and Ô¨Ånd
bases for both N(T) and R(T). Then compute the nullity and rank of T, and
verify the dimension theorem. Finally, use the appropriate theorems in this
section to determine whether T is one-to-one or onto.
2.
T: R3 ‚ÜíR2 deÔ¨Åned by T(a1, a2, a3) = (a1 ‚àía2, 2a3).
3.
T: R2 ‚ÜíR3 deÔ¨Åned by T(a1, a2) = (a1 + a2, 0, 2a1 ‚àía2).
4.
T: M2√ó3(F) ‚ÜíM2√ó2(F) deÔ¨Åned by
T

a11
a12
a13
a21
a22
a23
	
=

2a11 ‚àía12
a13 + 2a12
0
0
	
.
5.
T: P2(R) ‚ÜíP3(R) deÔ¨Åned by T(f(x)) = xf(x) + f ‚Ä≤(x).

Sec. 2.1
Linear Transformations, Null Spaces, and Ranges
75
6.
T: Mn√ón(F) ‚ÜíF deÔ¨Åned by T(A) = tr(A). Recall (Example 4, Sec-
tion 1.3) that
tr(A) =
n

i=1
Aii.
7.
Prove properties 1, 2, 3, and 4 on page 65.
8.
Prove that the transformations in Examples 2 and 3 are linear.
9.
In this exercise, T: R2 ‚ÜíR2 is a function. For each of the following
parts, state why T is not linear.
(a)
T(a1, a2) = (1, a2)
(b)
T(a1, a2) = (a1, a2
1)
(c)
T(a1, a2) = (sin a1, 0)
(d)
T(a1, a2) = (|a1|, a2)
(e)
T(a1, a2) = (a1 + 1, a2)
10.
Suppose that T: R2 ‚ÜíR2 is linear, T(1, 0) = (1, 4), and T(1, 1) = (2, 5).
What is T(2, 3)? Is T one-to-one?
11.
Prove that there exists a linear transformation T: R2 ‚ÜíR3 such that
T(1, 1) = (1, 0, 2) and T(2, 3) = (1, ‚àí1, 4). What is T(8, 11)?
12.
Is there a linear transformation T: R3 ‚ÜíR2 such that T(1, 0, 3) = (1, 1)
and T(‚àí2, 0, ‚àí6) = (2, 1)?
13.
Let V and W be vector spaces, let T: V ‚ÜíW be linear, and let
{w1, w2, . . . , wk} be a linearly independent subset of R(T). Prove that
if S = {v1, v2, . . . , vk} is chosen so that T(vi) = wi for i = 1, 2, . . . , k,
then S is linearly independent.
14.
Let V and W be vector spaces and T: V ‚ÜíW be linear.
(a)
Prove that T is one-to-one if and only if T carries linearly inde-
pendent subsets of V onto linearly independent subsets of W.
(b)
Suppose that T is one-to-one and that S is a subset of V. Prove
that S is linearly independent if and only if T(S) is linearly inde-
pendent.
(c)
Suppose Œ≤ = {v1, v2, . . . , vn} is a basis for V and T is one-to-one
and onto. Prove that T(Œ≤) = {T(v1), T(v2), . . . , T(vn)} is a basis
for W.
15.
Recall the deÔ¨Ånition of P(R) on page 10. DeÔ¨Åne
T: P(R) ‚ÜíP(R)
by
T(f(x)) =
 x
0
f(t) dt.
Prove that T linear and one-to-one, but not onto.

76
Chap. 2
Linear Transformations and Matrices
16.
Let T: P(R) ‚ÜíP(R) be deÔ¨Åned by T(f(x)) = f ‚Ä≤(x). Recall that T is
linear. Prove that T is onto, but not one-to-one.
17.
Let V and W be Ô¨Ånite-dimensional vector spaces and T: V ‚ÜíW be
linear.
(a)
Prove that if dim(V) < dim(W), then T cannot be onto.
(b)
Prove that if dim(V) > dim(W), then T cannot be one-to-one.
18.
Give an example of a linear transformation T: R2 ‚ÜíR2 such that
N(T) = R(T).
19.
Give an example of distinct linear transformations T and U such that
N(T) = N(U) and R(T) = R(U).
20.
Let V and W be vector spaces with subspaces V1 and W1, respectively.
If T: V ‚ÜíW is linear, prove that T(V1) is a subspace of W and that
{x ‚ààV: T(x) ‚ààW1} is a subspace of V.
21.
Let V be the vector space of sequences described in Example 5 of Sec-
tion 1.2. DeÔ¨Åne the functions T, U: V ‚ÜíV by
T(a1, a2, . . .) = (a2, a3, . . .)
and
U(a1, a2, . . .) = (0, a1, a2, . . .).
T and U are called the left shift and right shift operators on V,
respectively.
(a)
Prove that T and U are linear.
(b)
Prove that T is onto, but not one-to-one.
(c)
Prove that U is one-to-one, but not onto.
22.
Let T: R3 ‚ÜíR be linear. Show that there exist scalars a, b, and c such
that T(x, y, z) = ax + by + cz for all (x, y, z) ‚ààR3. Can you generalize
this result for T: Fn ‚ÜíF? State and prove an analogous result for
T: Fn ‚ÜíFm.
23.
Let T: R3 ‚ÜíR be linear. Describe geometrically the possibilities for
the null space of T. Hint: Use Exercise 22.
The following deÔ¨Ånition is used in Exercises 24‚Äì27 and in Exercise 30.
DeÔ¨Ånition.
Let V be a vector space and W1 and W2 be subspaces of
V such that V = W1 ‚äïW2. (Recall the deÔ¨Ånition of direct sum given in the
exercises of Section 1.3.) A function T: V ‚ÜíV is called the projection on
W1 along W2 if, for x = x1 + x2 with x1 ‚ààW1 and x2 ‚ààW2, we have
T(x) = x1.
24.
Let T: R2 ‚ÜíR2. Include Ô¨Ågures for each of the following parts.

Sec. 2.1
Linear Transformations, Null Spaces, and Ranges
77
(a)
Find a formula for T(a, b), where T represents the projection on
the y-axis along the x-axis.
(b)
Find a formula for T(a, b), where T represents the projection on
the y-axis along the line L = {(s, s): s ‚ààR}.
25.
Let T: R3 ‚ÜíR3.
(a)
If T(a, b, c) = (a, b, 0), show that T is the projection on the xy-
plane along the z-axis.
(b)
Find a formula for T(a, b, c), where T represents the projection on
the z-axis along the xy-plane.
(c)
If T(a, b, c) = (a ‚àíc, b, 0), show that T is the projection on the
xy-plane along the line L = {(a, 0, a): a ‚ààR}.
26.
Using the notation in the deÔ¨Ånition above, assume that T: V ‚ÜíV is
the projection on W1 along W2.
(a)
Prove that T is linear and W1 = {x ‚ààV: T(x) = x}.
(b)
Prove that W1 = R(T) and W2 = N(T).
(c)
Describe T if W1 = V.
(d)
Describe T if W1 is the zero subspace.
27.
Suppose that W is a subspace of a Ô¨Ånite-dimensional vector space V.
(a)
Prove that there exists a subspace W‚Ä≤ and a function T: V ‚ÜíV
such that T is a projection on W along W‚Ä≤.
(b)
Give an example of a subspace W of a vector space V such that
there are two projections on W along two (distinct) subspaces.
The following deÔ¨Ånitions are used in Exercises 28‚Äì32.
DeÔ¨Ånitions.
Let V be a vector space, and let T: V ‚ÜíV be linear. A
subspace W of V is said to be T-invariant if T(x) ‚ààW for every x ‚ààW, that
is, T(W) ‚äÜW. If W is T-invariant, we deÔ¨Åne the restriction of T on W to
be the function TW : W ‚ÜíW deÔ¨Åned by TW(x) = T(x) for all x ‚ààW.
Exercises 28‚Äì32 assume that W is a subspace of a vector space V and that
T: V ‚ÜíV is linear. Warning: Do not assume that W is T-invariant or that
T is a projection unless explicitly stated.
28.
Prove that the subspaces {0}, V, R(T), and N(T) are all T-invariant.
29.
If W is T-invariant, prove that TW is linear.
30.
Suppose that T is the projection on W along some subspace W‚Ä≤. Prove
that W is T-invariant and that TW = IW.
31.
Suppose that V = R(T)‚äïW and W is T-invariant. (Recall the deÔ¨Ånition
of direct sum given in the exercises of Section 1.3.)

78
Chap. 2
Linear Transformations and Matrices
(a)
Prove that W ‚äÜN(T).
(b)
Show that if V is Ô¨Ånite-dimensional, then W = N(T).
(c)
Show by example that the conclusion of (b) is not necessarily true
if V is not Ô¨Ånite-dimensional.
32.
Suppose that W is T-invariant. Prove that N(TW) = N(T) ‚à©W and
R(TW) = T(W).
33.
Prove Theorem 2.2 for the case that Œ≤ is inÔ¨Ånite, that is, R(T) =
span({T(v): v ‚ààŒ≤}).
34.
Prove the following generalization of Theorem 2.6: Let V and W be
vector spaces over a common Ô¨Åeld, and let Œ≤ be a basis for V. Then for
any function f : Œ≤ ‚ÜíW there exists exactly one linear transformation
T: V ‚ÜíW such that T(x) = f(x) for all x ‚ààŒ≤.
Exercises 35 and 36 assume the deÔ¨Ånition of direct sum given in the exercises
of Section 1.3.
35.
Let V be a Ô¨Ånite-dimensional vector space and T: V ‚ÜíV be linear.
(a)
Suppose that V = R(T) + N(T). Prove that V = R(T) ‚äïN(T).
(b)
Suppose that R(T) ‚à©N(T) = {0}. Prove that V = R(T) ‚äïN(T).
Be careful to say in each part where Ô¨Ånite-dimensionality is used.
36.
Let V and T be as deÔ¨Åned in Exercise 21.
(a)
Prove that V = R(T)+N(T), but V is not a direct sum of these two
spaces. Thus the result of Exercise 35(a) above cannot be proved
without assuming that V is Ô¨Ånite-dimensional.
(b)
Find a linear operator T1 on V such that R(T1)‚à©N(T1) = {0} but
V is not a direct sum of R(T1) and N(T1). Conclude that V being
Ô¨Ånite-dimensional is also essential in Exercise 35(b).
37.
A function T: V ‚ÜíW between vector spaces V and W is called additive
if T(x + y) = T(x) + T(y) for all x, y ‚ààV. Prove that if V and W
are vector spaces over the Ô¨Åeld of rational numbers, then any additive
function from V into W is a linear transformation.
38.
Let T: C ‚ÜíC be the function deÔ¨Åned by T(z) = z. Prove that T is
additive (as deÔ¨Åned in Exercise 37) but not linear.
39.
Prove that there is an additive function T: R ‚ÜíR (as deÔ¨Åned in Ex-
ercise 37) that is not linear. Hint: Let V be the set of real numbers
regarded as a vector space over the Ô¨Åeld of rational numbers. By the
corollary to Theorem 1.13 (p. 60), V has a basis Œ≤. Let x and y be two
distinct vectors in Œ≤, and deÔ¨Åne f : Œ≤ ‚ÜíV by f(x) = y, f(y) = x, and
f(z) = z otherwise. By Exercise 34, there exists a linear transformation

Sec. 2.2
The Matrix Representation of a Linear Transformation
79
T: V ‚ÜíV such that T(u) = f(u) for all u ‚ààŒ≤. Then T is additive, but
for c = y/x, T(cx) Ã∏= cT(x).
The following exercise requires familiarity with the deÔ¨Ånition of quotient space
given in Exercise 31 of Section 1.3.
40.
Let V be a vector space and W be a subspace of V. DeÔ¨Åne the mapping
Œ∑: V ‚ÜíV/W by Œ∑(v) = v + W for v ‚ààV.
(a)
Prove that Œ∑ is a linear transformation from V onto V/W and that
N(Œ∑) = W.
(b)
Suppose that V is Ô¨Ånite-dimensional.
Use (a) and the dimen-
sion theorem to derive a formula relating dim(V), dim(W), and
dim(V/W).
(c)
Read the proof of the dimension theorem. Compare the method of
solving (b) with the method of deriving the same result as outlined
in Exercise 35 of Section 1.6.
2.2
THE MATRIX REPRESENTATION OF A LINEAR
TRANSFORMATION
Until now, we have studied linear transformations by examining their ranges
and null spaces. In this section, we embark on one of the most useful ap-
proaches to the analysis of a linear transformation on a Ô¨Ånite-dimensional
vector space: the representation of a linear transformation by a matrix. In
fact, we develop a one-to-one correspondence between matrices and linear
transformations that allows us to utilize properties of one to study properties
of the other.
We Ô¨Årst need the concept of an ordered basis for a vector space.
DeÔ¨Ånition.
Let V be a Ô¨Ånite-dimensional vector space. An ordered
basis for V is a basis for V endowed with a speciÔ¨Åc order; that is, an ordered
basis for V is a Ô¨Ånite sequence of linearly independent vectors in V that
generates V.
Example 1
In F3, Œ≤ = {e1, e2, e3} can be considered an ordered basis.
Also Œ≥ =
{e2, e1, e3} is an ordered basis, but Œ≤ Ã∏= Œ≥ as ordered bases.
‚ô¶
For the vector space Fn, we call {e1, e2, . . . , en} the standard ordered
basis for Fn. Similarly, for the vector space Pn(F), we call {1, x, . . . , xn} the
standard ordered basis for Pn(F).
Now that we have the concept of ordered basis, we can identify abstract
vectors in an n-dimensional vector space with n-tuples. This identiÔ¨Åcation is
provided through the use of coordinate vectors, as introduced next.

80
Chap. 2
Linear Transformations and Matrices
DeÔ¨Ånition.
Let Œ≤ = {u1, u2, . . . , un} be an ordered basis for a Ô¨Ånite-
dimensional vector space V. For x ‚ààV, let a1, a2, . . . , an be the unique scalars
such that
x =
n

i=1
aiui.
We deÔ¨Åne the coordinate vector of x relative to Œ≤, denoted [x]Œ≤, by
[x]Œ≤ =
‚éõ
‚éú
‚éú
‚éú
‚éù
a1
a2
...
an
‚éû
‚éü
‚éü
‚éü
‚é†.
Notice that [ui]Œ≤ = ei in the preceding deÔ¨Ånition. It is left as an exercise
to show that the correspondence x ‚Üí[x]Œ≤ provides us with a linear transfor-
mation from V to Fn. We study this transformation in Section 2.4 in more
detail.
Example 2
Let V = P2(R), and let Œ≤ = {1, x, x2} be the standard ordered basis for V. If
f(x) = 4 + 6x ‚àí7x2, then
[f]Œ≤ =
‚éõ
‚éù
4
6
‚àí7
‚éû
‚é†.
‚ô¶
Let us now proceed with the promised matrix representation of a linear
transformation. Suppose that V and W are Ô¨Ånite-dimensional vector spaces
with ordered bases Œ≤ = {v1, v2, . . . , vn} and Œ≥ = {w1, w2, . . . , wm}, respec-
tively. Let T: V ‚ÜíW be linear. Then for each j, 1 ‚â§j ‚â§n, there exist
unique scalars aij ‚ààF, 1 ‚â§i ‚â§m, such that
T(vj) =
m

i=1
aijwi
for 1 ‚â§j ‚â§n.
DeÔ¨Ånition. Using the notation above, we call the m√ón matrix A deÔ¨Åned
by Aij = aij the matrix representation of T in the ordered bases Œ≤
and Œ≥ and write A = [T]Œ≥
Œ≤. If V = W and Œ≤ = Œ≥, then we write A = [T]Œ≤.
Notice that the jth column of A is simply [T(vj)]Œ≥. Also observe that if
U: V ‚ÜíW is a linear transformation such that [U]Œ≥
Œ≤ = [T]Œ≥
Œ≤, then U = T by
the corollary to Theorem 2.6 (p. 73).
We illustrate the computation of [T]Œ≥
Œ≤ in the next several examples.

Sec. 2.2
The Matrix Representation of a Linear Transformation
81
Example 3
Let T: R2 ‚ÜíR3 be the linear transformation deÔ¨Åned by
T(a1, a2) = (a1 + 3a2, 0, 2a1 ‚àí4a2).
Let Œ≤ and Œ≥ be the standard ordered bases for R2 and R3, respectively. Now
T(1, 0) = (1, 0, 2) = 1e1 + 0e2 + 2e3
and
T(0, 1) = (3, 0, ‚àí4) = 3e1 + 0e2 ‚àí4e3.
Hence
[T]Œ≥
Œ≤ =
‚éõ
‚éù
1
3
0
0
2
‚àí4
‚éû
‚é†.
If we let Œ≥‚Ä≤ = {e3, e2, e1}, then
[T]Œ≥‚Ä≤
Œ≤ =
‚éõ
‚éù
2
‚àí4
0
0
1
3
‚éû
‚é†.
‚ô¶
Example 4
Let T: P3(R) ‚ÜíP2(R) be the linear transformation deÔ¨Åned by T(f(x)) =
f ‚Ä≤(x).
Let Œ≤ and Œ≥ be the standard ordered bases for P3(R) and P2(R),
respectively. Then
T(1) = 0¬∑1 + 0¬∑x + 0¬∑x2
T(x) = 1¬∑1 + 0¬∑x + 0¬∑x2
T(x2) = 0¬∑1 + 2¬∑x + 0¬∑x2
T(x3) = 0¬∑1 + 0¬∑x + 3¬∑x2.
So
[T]Œ≥
Œ≤ =
‚éõ
‚éù
0
1
0
0
0
0
2
0
0
0
0
3
‚éû
‚é†.
Note that when T(xj) is written as a linear combination of the vectors of Œ≥,
its coeÔ¨Écients give the entries of the jth column of [T]Œ≥
Œ≤.
‚ô¶

82
Chap. 2
Linear Transformations and Matrices
Now that we have deÔ¨Åned a procedure for associating matrices with linear
transformations, we show in Theorem 2.8 that this association ‚Äúpreserves‚Äù
addition and scalar multiplication. To make this more explicit, we need some
preliminary discussion about the addition and scalar multiplication of linear
transformations.
DeÔ¨Ånition. Let T, U: V ‚ÜíW be arbitrary functions, where V and W
are vector spaces over F, and let a ‚ààF.
We deÔ¨Åne T + U: V ‚ÜíW by
(T + U)(x) = T(x) + U(x) for all x ‚ààV, and aT: V ‚ÜíW by (aT)(x) = aT(x)
for all x ‚ààV.
Of course, these are just the usual deÔ¨Ånitions of addition and scalar mul-
tiplication of functions. We are fortunate, however, to have the result that
both sums and scalar multiples of linear transformations are also linear.
Theorem 2.7. Let V and W be vector spaces over a Ô¨Åeld F, and let
T, U: V ‚ÜíW be linear.
(a) For all a ‚ààF, aT + U is linear.
(b) Using the operations of addition and scalar multiplication in the pre-
ceding deÔ¨Ånition, the collection of all linear transformations from V to
W is a vector space over F.
Proof. (a) Let x, y ‚ààV and c ‚ààF. Then
(aT + U)(cx + y) = aT(cx + y) + U(cx + y)
= a[T(cx + y)] + cU(x) + U(y)
= a[cT(x) + T(y)] + cU(x) + U(y)
= acT(x) + cU(x) + aT(y) + U(y)
= c(aT + U)(x) + (aT + U)(y).
So aT + U is linear.
(b) Noting that T0, the zero transformation, plays the role of the zero
vector, it is easy to verify that the axioms of a vector space are satisÔ¨Åed,
and hence that the collection of all linear transformations from V into W is a
vector space over F.
DeÔ¨Ånitions.
Let V and W be vector spaces over F. We denote the
vector space of all linear transformations from V into W by L(V, W). In the
case that V = W, we write L(V) instead of L(V, W).
In Section 2.4, we see a complete identiÔ¨Åcation of L(V, W) with the vector
space Mm√ón(F), where n and m are the dimensions of V and W, respectively.
This identiÔ¨Åcation is easily established by the use of the next theorem.
Theorem 2.8. Let V and W be Ô¨Ånite-dimensional vector spaces with
ordered bases Œ≤ and Œ≥, respectively, and let T, U: V ‚ÜíW be linear transfor-
mations. Then

Sec. 2.2
The Matrix Representation of a Linear Transformation
83
(a) [T + U]Œ≥
Œ≤ = [T]Œ≥
Œ≤ + [U]Œ≥
Œ≤ and
(b) [aT]Œ≥
Œ≤ = a[T]Œ≥
Œ≤ for all scalars a.
Proof. Let Œ≤ = {v1, v2, . . . , vn} and Œ≥ = {w1, w2, . . . , wm}. There exist
unique scalars aij and bij (1 ‚â§i ‚â§m, 1 ‚â§j ‚â§n) such that
T(vj) =
m

i=1
aijwi
and
U(vj) =
m

i=1
bijwi
for 1 ‚â§j ‚â§n.
Hence
(T + U)(vj) =
m

i=1
(aij + bij)wi.
Thus
([T + U]Œ≥
Œ≤)ij = aij + bij = ([T]Œ≥
Œ≤ + [U]Œ≥
Œ≤)ij.
So (a) is proved, and the proof of (b) is similar.
Example 5
Let T: R2 ‚ÜíR3 and U: R2 ‚ÜíR3 be the linear transformations respectively
deÔ¨Åned by
T(a1, a2) = (a1 + 3a2, 0, 2a1 ‚àí4a2) and U(a1, a2) = (a1 ‚àía2, 2a1, 3a1 + 2a2).
Let Œ≤ and Œ≥ be the standard ordered bases of R2 and R3, respectively. Then
[T]Œ≥
Œ≤ =
‚éõ
‚éù
1
3
0
0
2
‚àí4
‚éû
‚é†,
(as computed in Example 3), and
[U]Œ≥
Œ≤ =
‚éõ
‚éù
1
‚àí1
2
0
3
2
‚éû
‚é†.
If we compute T + U using the preceding deÔ¨Ånitions, we obtain
(T + U)(a1, a2) = (2a1 + 2a2, 2a1, 5a1 ‚àí2a2).
So
[T + U]Œ≥
Œ≤ =
‚éõ
‚éù
2
2
2
0
5
‚àí2
‚éû
‚é†,
which is simply [T]Œ≥
Œ≤ + [U]Œ≥
Œ≤, illustrating Theorem 2.8.
‚ô¶

84
Chap. 2
Linear Transformations and Matrices
EXERCISES
1.
Label the following statements as true or false. Assume that V and
W are Ô¨Ånite-dimensional vector spaces with ordered bases Œ≤ and Œ≥,
respectively, and T, U: V ‚ÜíW are linear transformations.
(a)
For any scalar a, aT + U is a linear transformation from V to W.
(b)
[T]Œ≥
Œ≤ = [U]Œ≥
Œ≤ implies that T = U.
(c)
If m = dim(V) and n = dim(W), then [T]Œ≥
Œ≤ is an m √ó n matrix.
(d)
[T + U]Œ≥
Œ≤ = [T]Œ≥
Œ≤ + [U]Œ≥
Œ≤.
(e)
L(V, W) is a vector space.
(f)
L(V, W) = L(W, V).
2.
Let Œ≤ and Œ≥ be the standard ordered bases for Rn and Rm, respectively.
For each linear transformation T: Rn ‚ÜíRm, compute [T]Œ≥
Œ≤.
(a)
T: R2 ‚ÜíR3 deÔ¨Åned by T(a1, a2) = (2a1 ‚àía2, 3a1 + 4a2, a1).
(b)
T: R3 ‚ÜíR2 deÔ¨Åned by T(a1, a2, a3) = (2a1 + 3a2 ‚àía3, a1 + a3).
(c)
T: R3 ‚ÜíR deÔ¨Åned by T(a1, a2, a3) = 2a1 + a2 ‚àí3a3.
(d)
T: R3 ‚ÜíR3 deÔ¨Åned by
T(a1, a2, a3) = (2a2 + a3, ‚àía1 + 4a2 + 5a3, a1 + a3).
(e)
T: Rn ‚ÜíRn deÔ¨Åned by T(a1, a2, . . . , an) = (a1, a1, . . . , a1).
(f)
T: Rn ‚ÜíRn deÔ¨Åned by T(a1, a2, . . . , an) = (an, an‚àí1, . . . , a1).
(g)
T: Rn ‚ÜíR deÔ¨Åned by T(a1, a2, . . . , an) = a1 + an.
3.
Let T: R2 ‚ÜíR3 be deÔ¨Åned by T(a1, a2) = (a1 ‚àía2, a1, 2a1 + a2). Let Œ≤
be the standard ordered basis for R2 and Œ≥ = {(1, 1, 0), (0, 1, 1), (2, 2, 3)}.
Compute [T]Œ≥
Œ≤. If Œ± = {(1, 2), (2, 3)}, compute [T]Œ≥
Œ±.
4.
DeÔ¨Åne
T: M2√ó2(R) ‚ÜíP2(R)
by
T

a
b
c
d
	
= (a + b) + (2d)x + bx2.
Let
Œ≤ =

1
0
0
0
	
,

0
1
0
0
	
,

0
0
1
0
	
,

0
0
0
1
	
and
Œ≥ = {1, x, x2}.
Compute [T]Œ≥
Œ≤.
5.
Let
Œ± =

1
0
0
0
	
,

0
1
0
0
	
,

0
0
1
0
	
,

0
0
0
1
	
,
Œ≤ = {1, x, x2},
and
Œ≥ = {1}.

Sec. 2.2
The Matrix Representation of a Linear Transformation
85
(a)
DeÔ¨Åne T: M2√ó2(F) ‚ÜíM2√ó2(F) by T(A) = At. Compute [T]Œ±.
(b)
DeÔ¨Åne
T: P2(R) ‚ÜíM2√ó2(R)
by
T(f(x)) =

f ‚Ä≤(0)
2f(1)
0
f ‚Ä≤‚Ä≤(3)
	
,
where ‚Ä≤ denotes diÔ¨Äerentiation. Compute [T]Œ±
Œ≤.
(c)
DeÔ¨Åne T: M2√ó2(F) ‚ÜíF by T(A) = tr(A). Compute [T]Œ≥
Œ±.
(d)
DeÔ¨Åne T: P2(R) ‚ÜíR by T(f(x)) = f(2). Compute [T]Œ≥
Œ≤.
(e)
If
A =

1
‚àí2
0
4
	
,
compute [A]Œ±.
(f)
If f(x) = 3 ‚àí6x + x2, compute [f(x)]Œ≤.
(g)
For a ‚ààF, compute [a]Œ≥.
6.
Complete the proof of part (b) of Theorem 2.7.
7.
Prove part (b) of Theorem 2.8.
8.‚Ä† Let V be an n-dimensional vector space with an ordered basis Œ≤. DeÔ¨Åne
T: V ‚ÜíFn by T(x) = [x]Œ≤. Prove that T is linear.
9.
Let V be the vector space of complex numbers over the Ô¨Åeld R. DeÔ¨Åne
T: V ‚ÜíV by T(z) = z, where z is the complex conjugate of z. Prove
that T is linear, and compute [T]Œ≤, where Œ≤ = {1, i}. (Recall by Exer-
cise 38 of Section 2.1 that T is not linear if V is regarded as a vector
space over the Ô¨Åeld C.)
10.
Let V be a vector space with the ordered basis Œ≤ = {v1, v2, . . . , vn}.
DeÔ¨Åne v0 = 0. By Theorem 2.6 (p. 72), there exists a linear trans-
formation T: V ‚ÜíV such that T(vj) = vj + vj‚àí1 for j = 1, 2, . . . , n.
Compute [T]Œ≤.
11.
Let V be an n-dimensional vector space, and let T: V ‚ÜíV be a linear
transformation. Suppose that W is a T-invariant subspace of V (see the
exercises of Section 2.1) having dimension k. Show that there is a basis
Œ≤ for V such that [T]Œ≤ has the form

A
B
O
C
	
,
where A is a k √ó k matrix and O is the (n ‚àík) √ó k zero matrix.

86
Chap. 2
Linear Transformations and Matrices
12.
Let V be a Ô¨Ånite-dimensional vector space and T be the projection on
W along W‚Ä≤, where W and W‚Ä≤ are subspaces of V. (See the deÔ¨Ånition
in the exercises of Section 2.1 on page 76.) Find an ordered basis Œ≤ for
V such that [T]Œ≤ is a diagonal matrix.
13.
Let V and W be vector spaces, and let T and U be nonzero linear
transformations from V into W.
If R(T) ‚à©R(U) = {0}, prove that
{T, U} is a linearly independent subset of L(V, W).
14.
Let V = P(R), and for j ‚â•1 deÔ¨Åne Tj(f(x)) = f (j)(x), where f (j)(x)
is the jth derivative of f(x). Prove that the set {T1, T2, . . . , Tn} is a
linearly independent subset of L(V) for any positive integer n.
15.
Let V and W be vector spaces, and let S be a subset of V.
DeÔ¨Åne
S0 = {T ‚ààL(V, W): T(x) = 0 for all x ‚ààS}.
Prove the following
statements.
(a)
S0 is a subspace of L(V, W).
(b)
If S1and S2 are subsets of V and S1 ‚äÜS2, then S0
2 ‚äÜS0
1.
(c)
If V1 and V2 are subspaces of V, then (V1 + V2)0 = V0
1 ‚à©V0
2.
16.
Let V and W be vector spaces such that dim(V) = dim(W), and let
T: V ‚ÜíW be linear. Show that there exist ordered bases Œ≤ and Œ≥ for
V and W, respectively, such that [T]Œ≥
Œ≤ is a diagonal matrix.
2.3
COMPOSITION OF LINEAR TRANSFORMATIONS
AND MATRIX MULTIPLICATION
In Section 2.2, we learned how to associate a matrix with a linear transforma-
tion in such a way that both sums and scalar multiples of matrices are associ-
ated with the corresponding sums and scalar multiples of the transformations.
The question now arises as to how the matrix representation of a composite
of linear transformations is related to the matrix representation of each of the
associated linear transformations. The attempt to answer this question leads
to a deÔ¨Ånition of matrix multiplication. We use the more convenient notation
of UT rather than U ‚ó¶T for the composite of linear transformations U and T.
(See Appendix B.)
Our Ô¨Årst result shows that the composite of linear transformations is lin-
ear.
Theorem 2.9. Let V, W, and Z be vector spaces over the same Ô¨Åeld F,
and let T: V ‚ÜíW and U: W ‚ÜíZ be linear. Then UT: V ‚ÜíZ is linear.
Proof. Let x, y ‚ààV and a ‚ààF. Then
UT(ax + y) = U(T(ax + y)) = U(aT(x) + T(y))
= aU(T(x)) + U(T(y)) = a(UT)(x) + UT(y).

Sec. 2.3
Composition of Linear Transformations and Matrix Multiplication
87
The following theorem lists some of the properties of the composition of
linear transformations.
Theorem 2.10. Let V be a vector space. Let T, U1, U2 ‚ààL(V). Then
(a) T(U1 + U2) = TU1 + TU2 and (U1 + U2)T = U1T + U2T
(b) T(U1U2) = (TU1)U2
(c) TI = IT = T
(d) a(U1U2) = (aU1)U2 = U1(aU2) for all scalars a.
Proof. Exercise.
A more general result holds for linear transformations that have domains
unequal to their codomains. (See Exercise 8.)
Let T: V ‚ÜíW and U: W ‚ÜíZ be linear transformations, and let A = [U]Œ≥
Œ≤
and B = [T]Œ≤
Œ±, where Œ± = {v1, v2, . . . , vn}, Œ≤ = {w1, w2, . . . , wm}, and Œ≥ =
{z1, z2, . . . , zp} are ordered bases for V, W, and Z, respectively. We would
like to deÔ¨Åne the product AB of two matrices so that AB = [UT]Œ≥
Œ±. Consider
the matrix [UT]Œ≥
Œ±. For 1 ‚â§j ‚â§n, we have
(UT)(vj) = U(T(vj)) = U
 m

k=1
Bkjwk

=
m

k=1
BkjU(wk)
=
m

k=1
Bkj
 p

i=1
Aikzi

=
p

i=1
 m

k=1
AikBkj

zi
=
p

i=1
Cijzi,
where
Cij =
m

k=1
AikBkj.
This computation motivates the following deÔ¨Ånition of matrix multiplication.
DeÔ¨Ånition.
Let A be an m √ó n matrix and B be an n √ó p matrix. We
deÔ¨Åne the product of A and B, denoted AB, to be the m √ó p matrix such
that
(AB)ij =
n

k=1
AikBkj
for 1 ‚â§i ‚â§m,
1 ‚â§j ‚â§p.
Note that (AB)ij is the sum of products of corresponding entries from the
ith row of A and the jth column of B. Some interesting applications of this
deÔ¨Ånition are presented at the end of this section.

88
Chap. 2
Linear Transformations and Matrices
The reader should observe that in order for the product AB to be deÔ¨Åned,
there are restrictions regarding the relative sizes of A and B. The following
mnemonic device is helpful: ‚Äú(m √ó n)¬∑(n √ó p) = (m √ó p)‚Äù; that is, in order
for the product AB to be deÔ¨Åned, the two ‚Äúinner‚Äù dimensions must be equal,
and the two ‚Äúouter‚Äù dimensions yield the size of the product.
Example 1
We have

1
2
1
0
4
‚àí1
	 ‚éõ
‚éù
4
2
5
‚éû
‚é†=

1¬∑4 + 2¬∑2 + 1¬∑5
0¬∑4 + 4¬∑2 + (‚àí1)¬∑5
	
=

13
3
	
.
Notice again the symbolic relationship (2 √ó 3)¬∑(3 √ó 1) = 2 √ó 1.
‚ô¶
As in the case with composition of functions, we have that matrix multi-
plication is not commutative. Consider the following two products:

1
1
0
0
	 
0
1
1
0
	
=

1
1
0
0
	
and

0
1
1
0
	 
1
1
0
0
	
=

0
0
1
1
	
.
Hence we see that even if both of the matrix products AB and BA are deÔ¨Åned,
it need not be true that AB = BA.
Recalling the deÔ¨Ånition of the transpose of a matrix from Section 1.3, we
show that if A is an m√ón matrix and B is an n√óp matrix, then (AB)t = BtAt.
Since
(AB)t
ij = (AB)ji =
n

k=1
AjkBki
and
(BtAt)ij =
n

k=1
(Bt)ik(At)kj =
n

k=1
BkiAjk,
we are Ô¨Ånished. Therefore the transpose of a product is the product of the
transposes in the opposite order.
The next theorem is an immediate consequence of our deÔ¨Ånition of matrix
multiplication.
Theorem 2.11. Let V, W, and Z be Ô¨Ånite-dimensional vector spaces with
ordered bases Œ±, Œ≤, and Œ≥, respectively. Let T: V ‚ÜíW and U: W ‚ÜíZ be
linear transformations. Then
[UT]Œ≥
Œ± = [U]Œ≥
Œ≤[T]Œ≤
Œ±.

Sec. 2.3
Composition of Linear Transformations and Matrix Multiplication
89
Corollary. Let V be a Ô¨Ånite-dimensional vector space with an ordered
basis Œ≤. Let T, U ‚ààL(V). Then [UT]Œ≤ = [U]Œ≤[T]Œ≤.
We illustrate Theorem 2.11 in the next example.
Example 2
Let U: P3(R) ‚ÜíP2(R) and T: P2(R) ‚ÜíP3(R) be the linear transformations
respectively deÔ¨Åned by
U(f(x)) = f ‚Ä≤(x)
and
T(f(x)) =
 x
0
f(t) dt.
Let Œ± and Œ≤ be the standard ordered bases of P3(R) and P2(R), respectively.
From calculus, it follows that UT = I, the identity transformation on P2(R).
To illustrate Theorem 2.11, observe that
[UT]Œ≤ = [U]Œ≤
Œ±[T]Œ±
Œ≤ =
‚éõ
‚éù
0
1
0
0
0
0
2
0
0
0
0
3
‚éû
‚é†
‚éõ
‚éú
‚éú
‚éú
‚éù
0
0
0
1
0
0
0
1
2
0
0
0
1
3
‚éû
‚éü
‚éü
‚éü
‚é†=
‚éõ
‚éù
1
0
0
0
1
0
0
0
1
‚éû
‚é†= [I]Œ≤. ‚ô¶
The preceding 3 √ó 3 diagonal matrix is called an identity matrix and is
deÔ¨Åned next, along with a very useful notation, the Kronecker delta.
DeÔ¨Ånitions. We deÔ¨Åne the Kronecker delta Œ¥ij by Œ¥ij = 1 if i = j and
Œ¥ij = 0 if i Ã∏= j. The n √ó n identity matrix In is deÔ¨Åned by (In)ij = Œ¥ij.
Thus, for example,
I1 = (1),
I2 =

1
0
0
1
	
,
and
I3 =
‚éõ
‚éù
1
0
0
0
1
0
0
0
1
‚éû
‚é†.
The next theorem provides analogs of (a), (c), and (d) of Theorem 2.10.
Theorem 2.10(b) has its analog in Theorem 2.16. Observe also that part (c) of
the next theorem illustrates that the identity matrix acts as a multiplicative
identity in Mn√ón(F).
When the context is clear, we sometimes omit the
subscript n from In.
Theorem 2.12. Let A be an m √ó n matrix, B and C be n √ó p matrices,
and D and E be q √ó m matrices. Then
(a) A(B + C) = AB + AC and (D + E)A = DA + EA.
(b) a(AB) = (aA)B = A(aB) for any scalar a.
(c) ImA = A = AIn.
(d) If V is an n-dimensional vector space with an ordered basis Œ≤, then
[IV]Œ≤ = In.

90
Chap. 2
Linear Transformations and Matrices
Proof. We prove the Ô¨Årst half of (a) and (c) and leave the remaining proofs
as an exercise. (See Exercise 5.)
(a) We have
[A(B + C)]ij =
n

k=1
Aik(B + C)kj =
n

k=1
Aik(Bkj + Ckj)
=
n

k=1
(AikBkj + AikCkj) =
n

k=1
AikBkj +
n

k=1
AikCkj
= (AB)ij + (AC)ij = [AB + AC]ij.
So A(B + C) = AB + AC.
(c) We have
(ImA)ij =
m

k=1
(Im)ikAkj =
m

k=1
Œ¥ikAkj = Aij.
Corollary. Let A be an m √ó n matrix, B1, B2, . . . , Bk be n √ó p matrices,
C1, C2, . . . , Ck be q √ó m matrices, and a1, a2, . . . , ak be scalars. Then
A
 k

i=1
aiBi

=
k

i=1
aiABi
and
 k

i=1
aiCi

A =
k

i=1
aiCiA.
Proof. Exercise.
For an n √ó n matrix A, we deÔ¨Åne A1 = A, A2 = AA, A3 = A2A, and, in
general, Ak = Ak‚àí1A for k = 2, 3, . . . . We deÔ¨Åne A0 = In.
With this notation, we see that if
A =

0
0
1
0
	
,
then A2 = O (the zero matrix) even though A Ã∏= O. Thus the cancellation
property for multiplication in Ô¨Åelds is not valid for matrices. To see why,
assume that the cancellation law is valid. Then, from A¬∑A = A2 = O = A¬∑O,
we would conclude that A = O, which is false.
Theorem 2.13. Let A be an m √ó n matrix and B be an n √ó p matrix.
For each j (1 ‚â§j ‚â§p) let uj and vj denote the jth columns of AB and B,
respectively. Then

Sec. 2.3
Composition of Linear Transformations and Matrix Multiplication
91
(a) uj = Avj
(b) vj = Bej, where ej is the jth standard vector of Fp.
Proof. (a) We have
uj =
‚éõ
‚éú
‚éú
‚éú
‚éù
(AB)1j
(AB)2j
...
(AB)mj
‚éû
‚éü
‚éü
‚éü
‚é†=
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
n

k=1
A1kBkj
n

k=1
A2kBkj
...
n

k=1
AmkBkj
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
= A
‚éõ
‚éú
‚éú
‚éú
‚éù
B1j
B2j
...
Bnj
‚éû
‚éü
‚éü
‚éü
‚é†= Avj.
Hence (a) is proved. The proof of (b) is left as an exercise. (See Exercise 6.)
It follows (see Exercise 14) from Theorem 2.13 that column j of AB is
a linear combination of the columns of A with the coeÔ¨Écients in the linear
combination being the entries of column j of B. An analogous result holds
for rows; that is, row i of AB is a linear combination of the rows of B with
the coeÔ¨Écients in the linear combination being the entries of row i of A.
The next result justiÔ¨Åes much of our past work. It utilizes both the matrix
representation of a linear transformation and matrix multiplication in order
to evaluate the transformation at any given vector.
Theorem 2.14. Let V and W be Ô¨Ånite-dimensional vector spaces having
ordered bases Œ≤ and Œ≥, respectively, and let T: V ‚ÜíW be linear. Then, for
each u ‚ààV, we have
[T(u)]Œ≥ = [T]Œ≥
Œ≤[u]Œ≤.
Proof. Fix u ‚ààV, and deÔ¨Åne the linear transformations f : F ‚ÜíV by
f(a) = au and g: F ‚ÜíW by g(a) = aT(u) for all a ‚ààF. Let Œ± = {1} be
the standard ordered basis for F. Notice that g = Tf. Identifying column
vectors as matrices and using Theorem 2.11, we obtain
[T(u)]Œ≥ = [g(1)]Œ≥ = [g]Œ≥
Œ± = [Tf]Œ≥
Œ± = [T]Œ≥
Œ≤[f]Œ≤
Œ± = [T]Œ≥
Œ≤[f(1)]Œ≤ = [T]Œ≥
Œ≤[u]Œ≤.
Example 3
Let T: P3(R) ‚ÜíP2(R) be the linear transformation deÔ¨Åned by T(f(x)) =
f ‚Ä≤(x), and let Œ≤ and Œ≥ be the standard ordered bases for P3(R) and P2(R),
respectively. If A = [T]Œ≥
Œ≤, then, from Example 4 of Section 2.2, we have
A =
‚éõ
‚éù
0
1
0
0
0
0
2
0
0
0
0
3
‚éû
‚é†.

92
Chap. 2
Linear Transformations and Matrices
We illustrate Theorem 2.14 by verifying that [T(p(x))]Œ≥ = [T]Œ≥
Œ≤[p(x)]Œ≤, where
p(x) ‚ààP3(R) is the polynomial p(x) = 2‚àí4x+x2 +3x3. Let q(x) = T(p(x));
then q(x) = p‚Ä≤(x) = ‚àí4 + 2x + 9x2. Hence
[T(p(x))]Œ≥ = [q(x)]Œ≥ =
‚éõ
‚éù
‚àí4
2
9
‚éû
‚é†,
but also
[T]Œ≥
Œ≤[p(x)]Œ≤ = A[p(x)]Œ≤ =
‚éõ
‚éù
0
1
0
0
0
0
2
0
0
0
0
3
‚éû
‚é†
‚éõ
‚éú
‚éú
‚éù
2
‚àí4
1
3
‚éû
‚éü
‚éü
‚é†=
‚éõ
‚éù
‚àí4
2
9
‚éû
‚é†.
‚ô¶
We complete this section with the introduction of the left-multiplication
transformation LA, where A is an m√ón matrix. This transformation is proba-
bly the most important tool for transferring properties about transformations
to analogous properties about matrices and vice versa. For example, we use
it to prove that matrix multiplication is associative.
DeÔ¨Ånition.
Let A be an m √ó n matrix with entries from a Ô¨Åeld F.
We denote by LA the mapping LA : Fn ‚ÜíFm deÔ¨Åned by LA(x) = Ax (the
matrix product of A and x) for each column vector x ‚ààFn. We call LA a
left-multiplication transformation.
Example 4
Let
A =

1
2
1
0
1
2
	
.
Then A ‚ààM2√ó3(R) and LA : R3 ‚ÜíR2. If
x =
‚éõ
‚éù
1
3
‚àí1
‚éû
‚é†,
then
LA(x) = Ax =

1
2
1
0
1
2
	 ‚éõ
‚éù
1
3
‚àí1
‚éû
‚é†=

6
1
	
.
‚ô¶
We see in the next theorem that not only is LA linear, but, in fact, it has
a great many other useful properties. These properties are all quite natural
and so are easy to remember.

Sec. 2.3
Composition of Linear Transformations and Matrix Multiplication
93
Theorem 2.15. Let A be an m √ó n matrix with entries from F. Then
the left-multiplication transformation LA : Fn ‚ÜíFm is linear. Furthermore,
if B is any other m √ó n matrix (with entries from F) and Œ≤ and Œ≥ are the
standard ordered bases for Fn and Fm, respectively, then we have the following
properties.
(a) [LA]Œ≥
Œ≤ = A.
(b) LA = LB if and only if A = B.
(c) LA+B = LA + LB and LaA = aLA for all a ‚ààF.
(d) If T: Fn ‚ÜíFm is linear, then there exists a unique m√ón matrix C such
that T = LC. In fact, C = [T]Œ≥
Œ≤.
(e) If E is an n √ó p matrix, then LAE = LALE.
(f) If m = n, then LIn = IFn.
Proof. The fact that LA is linear follows immediately from Theorem 2.12.
(a) The jth column of [LA]Œ≥
Œ≤ is equal to LA(ej). However LA(ej) = Aej,
which is also the jth column of A by Theorem 2.13(b). So [LA]Œ≥
Œ≤ = A.
(b) If LA = LB, then we may use (a) to write A = [LA]Œ≥
Œ≤ = [LB]Œ≥
Œ≤ = B.
Hence A = B. The proof of the converse is trivial.
(c) The proof is left as an exercise. (See Exercise 7.)
(d) Let C = [T]Œ≥
Œ≤.
By Theorem 2.14, we have [T(x)]Œ≥ = [T]Œ≥
Œ≤[x]Œ≤, or
T(x) = Cx = LC(x) for all x ‚ààF n. So T = LC. The uniqueness of C follows
from (b).
(e) For any j (1 ‚â§j ‚â§p), we may apply Theorem 2.13 several times to
note that (AE)ej is the jth column of AE and that the jth column of AE is
also equal to A(Eej). So (AE)ej = A(Eej). Thus
LAE(ej) = (AE)ej = A(Eej) = LA(Eej) = LA(LE(ej)).
Hence LAE = LALE by the corollary to Theorem 2.6 (p. 73).
(f) The proof is left as an exercise. (See Exercise 7.)
We now use left-multiplication transformations to establish the associa-
tivity of matrix multiplication.
Theorem 2.16. Let A, B, and C be matrices such that A(BC) is de-
Ô¨Åned. Then (AB)C is also deÔ¨Åned and A(BC) = (AB)C; that is, matrix
multiplication is associative.
Proof. It is left to the reader to show that (AB)C is deÔ¨Åned. Using (e)
of Theorem 2.15 and the associativity of functional composition (see Ap-
pendix B), we have
LA(BC) = LALBC = LA(LBLC) = (LALB)LC = LABLC = L(AB)C.
So from (b) of Theorem 2.15, it follows that A(BC) = (AB)C.

94
Chap. 2
Linear Transformations and Matrices
Needless to say, this theorem could be proved directly from the deÔ¨Ånition
of matrix multiplication (see Exercise 18). The proof above, however, provides
a prototype of many of the arguments that utilize the relationships between
linear transformations and matrices.
Applications
A large and varied collection of interesting applications arises in connec-
tion with special matrices called incidence matrices. An incidence matrix
is a square matrix in which all the entries are either zero or one and, for
convenience, all the diagonal entries are zero. If we have a relationship on a
set of n objects that we denote by 1, 2, . . . , n, then we deÔ¨Åne the associated
incidence matrix A by Aij = 1 if i is related to j, and Aij = 0 otherwise.
To make things concrete, suppose that we have four people, each of whom
owns a communication device. If the relationship on this group is ‚Äúcan trans-
mit to,‚Äù then Aij = 1 if i can send a message to j, and Aij = 0 otherwise.
Suppose that
A =
‚éõ
‚éú
‚éú
‚éù
0
1
0
0
1
0
0
1
0
1
0
1
1
1
0
0
‚éû
‚éü
‚éü
‚é†.
Then since A34 = 1 and A14 = 0, we see that person 3 can send to 4 but 1
cannot send to 4.
We obtain an interesting interpretation of the entries of A2. Consider, for
instance,
(A2)31 = A31A11 + A32A21 + A33A31 + A34A41.
Note that any term A3kAk1 equals 1 if and only if both A3k and Ak1 equal 1,
that is, if and only if 3 can send to k and k can send to 1. Thus (A2)31 gives
the number of ways in which 3 can send to 1 in two stages (or in one relay).
Since
A2 =
‚éõ
‚éú
‚éú
‚éù
1
0
0
1
1
2
0
0
2
1
0
1
1
1
0
1
‚éû
‚éü
‚éü
‚é†,
we see that there are two ways 3 can send to 1 in two stages. In general,
(A + A2 + ¬∑ ¬∑ ¬∑ + Am)ij is the number of ways in which i can send to j in at
most m stages.
A maximal collection of three or more people with the property that any
two can send to each other is called a clique. The problem of determining
cliques is diÔ¨Écult, but there is a simple method for determining if someone

Sec. 2.3
Composition of Linear Transformations and Matrix Multiplication
95
belongs to a clique. If we deÔ¨Åne a new matrix B by Bij = 1 if i and j can send
to each other, and Bij = 0 otherwise, then it can be shown (see Exercise 19)
that person i belongs to a clique if and only if (B3)ii > 0. For example,
suppose that the incidence matrix associated with some relationship is
A =
‚éõ
‚éú
‚éú
‚éù
0
1
0
1
1
0
1
0
1
1
0
1
1
1
1
0
‚éû
‚éü
‚éü
‚é†.
To determine which people belong to cliques, we form the matrix B, described
earlier, and compute B3. In this case,
B =
‚éõ
‚éú
‚éú
‚éù
0
1
0
1
1
0
1
0
0
1
0
1
1
0
1
0
‚éû
‚éü
‚éü
‚é†
and
B3 =
‚éõ
‚éú
‚éú
‚éù
0
4
0
4
4
0
4
0
0
4
0
4
4
0
4
0
‚éû
‚éü
‚éü
‚é†.
Since all the diagonal entries of B3 are zero, we conclude that there are no
cliques in this relationship.
Our Ô¨Ånal example of the use of incidence matrices is concerned with the
concept of dominance. A relation among a group of people is called a dom-
inance relation if the associated incidence matrix A has the property that
for all distinct pairs i and j, Aij = 1 if and only if Aji = 0, that is, given
any two people, exactly one of them dominates (or, using the terminology of
our Ô¨Årst example, can send a message to) the other. Since A is an incidence
matrix, Aii = 0 for all i. For such a relation, it can be shown (see Exercise 21)
that the matrix A + A2 has a row [column] in which each entry is positive
except for the diagonal entry. In other words, there is at least one person
who dominates [is dominated by] all others in one or two stages. In fact, it
can be shown that any person who dominates [is dominated by] the greatest
number of people in the Ô¨Årst stage has this property. Consider, for example,
the matrix
A =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
0
1
0
1
0
0
0
1
0
0
1
0
0
1
0
0
1
0
0
1
1
1
1
0
0
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
.
The reader should verify that this matrix corresponds to a dominance relation.
Now
A + A2 =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
0
2
1
1
1
1
0
1
1
0
1
2
0
2
1
1
2
2
0
1
2
2
2
2
0
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
.

96
Chap. 2
Linear Transformations and Matrices
Thus persons 1, 3, 4, and 5 dominate (can send messages to) all the others
in at most two stages, while persons 1, 2, 3, and 4 are dominated by (can
receive messages from) all the others in at most two stages.
EXERCISES
1.
Label the following statements as true or false.
In each part, V, W,
and Z denote vector spaces with ordered (Ô¨Ånite) bases Œ±, Œ≤, and Œ≥,
respectively; T: V ‚ÜíW and U: W ‚ÜíZ denote linear transformations;
and A and B denote matrices.
(a)
[UT]Œ≥
Œ± = [T]Œ≤
Œ±[U]Œ≥
Œ≤.
(b)
[T(v)]Œ≤ = [T]Œ≤
Œ±[v]Œ± for all v ‚ààV.
(c)
[U(w)]Œ≤ = [U]Œ≤
Œ±[w]Œ≤ for all w ‚ààW.
(d)
[IV]Œ± = I.
(e)
[T2]Œ≤
Œ± = ([T]Œ≤
Œ±)2.
(f)
A2 = I implies that A = I or A = ‚àíI.
(g)
T = LA for some matrix A.
(h)
A2 = O implies that A = O, where O denotes the zero matrix.
(i)
LA+B = LA + LB.
(j)
If A is square and Aij = Œ¥ij for all i and j, then A = I.
2. (a)
Let
A =

1
3
2
‚àí1
	
,
B =

1
0
‚àí3
4
1
2
	
,
C =

1
1
4
‚àí1
‚àí2
0
	
,
and
D =
‚éõ
‚éù
2
‚àí2
3
‚éû
‚é†.
Compute A(2B + 3C), (AB)D, and A(BD).
(b)
Let
A =
‚éõ
‚éù
2
5
‚àí3
1
4
2
‚éû
‚é†,
B =
‚éõ
‚éù
3
‚àí2
0
1
‚àí1
4
5
5
3
‚éû
‚é†,
and
C =

4
0
3

.
Compute At, AtB, BCt, CB, and CA.
3.
Let g(x) = 3 + x. Let T: P2(R) ‚ÜíP2(R) and U: P2(R) ‚ÜíR3 be the
linear transformations respectively deÔ¨Åned by
T(f(x)) = f ‚Ä≤(x)g(x) + 2f(x) and U(a + bx + cx2) = (a + b, c, a ‚àíb).
Let Œ≤ and Œ≥ be the standard ordered bases of P2(R) and R3, respectively.

Sec. 2.3
Composition of Linear Transformations and Matrix Multiplication
97
(a)
Compute [U]Œ≥
Œ≤, [T]Œ≤, and [UT]Œ≥
Œ≤ directly. Then use Theorem 2.11
to verify your result.
(b)
Let h(x) = 3 ‚àí2x + x2. Compute [h(x)]Œ≤ and [U(h(x))]Œ≥. Then
use [U]Œ≥
Œ≤ from (a) and Theorem 2.14 to verify your result.
4.
For each of the following parts, let T be the linear transformation deÔ¨Åned
in the corresponding part of Exercise 5 of Section 2.2. Use Theorem 2.14
to compute the following vectors:
(a)
[T(A)]Œ±, where A =

1
4
‚àí1
6
	
.
(b)
[T(f(x))]Œ±, where f(x) = 4 ‚àí6x + 3x2.
(c)
[T(A)]Œ≥, where A =

1
3
2
4
	
.
(d)
[T(f(x))]Œ≥, where f(x) = 6 ‚àíx + 2x2.
5.
Complete the proof of Theorem 2.12 and its corollary.
6.
Prove (b) of Theorem 2.13.
7.
Prove (c) and (f) of Theorem 2.15.
8.
Prove Theorem 2.10. Now state and prove a more general result involv-
ing linear transformations with domains unequal to their codomains.
9.
Find linear transformations U, T: F2 ‚ÜíF2 such that UT = T0 (the zero
transformation) but TU Ã∏= T0. Use your answer to Ô¨Ånd matrices A and
B such that AB = O but BA Ã∏= O.
10.
Let A be an n √ó n matrix. Prove that A is a diagonal matrix if and
only if Aij = Œ¥ijAij for all i and j.
11.
Let V be a vector space, and let T: V ‚ÜíV be linear. Prove that T2 = T0
if and only if R(T) ‚äÜN(T).
12.
Let V, W, and Z be vector spaces, and let T: V ‚ÜíW and U: W ‚ÜíZ
be linear.
(a)
Prove that if UT is one-to-one, then T is one-to-one. Must U also
be one-to-one?
(b)
Prove that if UT is onto, then U is onto. Must T also be onto?
(c)
Prove that if U and T are one-to-one and onto, then UT is also.
13.
Let A and B be n √ó n matrices. Recall that the trace of A is deÔ¨Åned
by
tr(A) =
n

i=1
Aii.
Prove that tr(AB) = tr(BA) and tr(A) = tr(At).

98
Chap. 2
Linear Transformations and Matrices
14.
Assume the notation in Theorem 2.13.
(a)
Suppose that z is a (column) vector in Fp. Use Theorem 2.13(b)
to prove that Bz is a linear combination of the columns of B. In
particular, if z = (a1, a2, . . . , ap)t, then show that
Bz =
p

j=1
ajvj.
(b)
Extend (a) to prove that column j of AB is a linear combination
of the columns of A with the coeÔ¨Écients in the linear combination
being the entries of column j of B.
(c)
For any row vector w ‚ààFm, prove that wA is a linear combination
of the rows of A with the coeÔ¨Écients in the linear combination
being the coordinates of w. Hint: Use properties of the transpose
operation applied to (a).
(d)
Prove the analogous result to (b) about rows: Row i of AB is a
linear combination of the rows of B with the coeÔ¨Écients in the
linear combination being the entries of row i of A.
15.‚Ä† Let M and A be matrices for which the product matrix MA is deÔ¨Åned.
If the jth column of A is a linear combination of a set of columns
of A, prove that the jth column of MA is a linear combination of the
corresponding columns of MA with the same corresponding coeÔ¨Écients.
16.
Let V be a Ô¨Ånite-dimensional vector space, and let T: V ‚ÜíV be linear.
(a)
If rank(T) = rank(T2), prove that R(T) ‚à©N(T) = {0}. Deduce
that V = R(T) ‚äïN(T) (see the exercises of Section 1.3).
(b)
Prove that V = R(Tk) ‚äïN(Tk) for some positive integer k.
17.
Let V be a vector space. Determine all linear transformations T: V ‚ÜíV
such that T = T2. Hint: Note that x = T(x) + (x ‚àíT(x)) for every
x in V, and show that V = {y: T(y) = y} ‚äïN(T) (see the exercises of
Section 1.3).
18.
Using only the deÔ¨Ånition of matrix multiplication, prove that multipli-
cation of matrices is associative.
19.
For an incidence matrix A with related matrix B deÔ¨Åned by Bij = 1 if
i is related to j and j is related to i, and Bij = 0 otherwise, prove that
i belongs to a clique if and only if (B3)ii > 0.
20.
Use Exercise 19 to determine the cliques in the relations corresponding
to the following incidence matrices.

Sec. 2.4
Invertibility and Isomorphisms
99
(a)
‚éõ
‚éú
‚éú
‚éù
0
1
0
1
1
0
0
0
0
1
0
1
1
0
1
0
‚éû
‚éü
‚éü
‚é†
(b)
‚éõ
‚éú
‚éú
‚éù
0
0
1
1
1
0
0
1
1
0
0
1
1
0
1
0
‚éû
‚éü
‚éü
‚é†
21.
Let A be an incidence matrix that is associated with a dominance rela-
tion. Prove that the matrix A + A2 has a row [column] in which each
entry is positive except for the diagonal entry.
22.
Prove that the matrix
A =
‚éõ
‚éù
0
1
0
0
0
1
1
0
0
‚éû
‚é†
corresponds to a dominance relation.
Use Exercise 21 to determine
which persons dominate [are dominated by] each of the others within
two stages.
23.
Let A be an n √ó n incidence matrix that corresponds to a dominance
relation. Determine the number of nonzero entries of A.
2.4
INVERTIBILITY AND ISOMORPHISMS
The concept of invertibility is introduced quite early in the study of functions.
Fortunately, many of the intrinsic properties of functions are shared by their
inverses. For example, in calculus we learn that the properties of being con-
tinuous or diÔ¨Äerentiable are generally retained by the inverse functions. We
see in this section (Theorem 2.17) that the inverse of a linear transformation
is also linear. This result greatly aids us in the study of inverses of matrices.
As one might expect from Section 2.3, the inverse of the left-multiplication
transformation LA (when it exists) can be used to determine properties of the
inverse of the matrix A.
In the remainder of this section, we apply many of the results about in-
vertibility to the concept of isomorphism. We will see that Ô¨Ånite-dimensional
vector spaces (over F) of equal dimension may be identiÔ¨Åed. These ideas will
be made precise shortly.
The facts about inverse functions presented in Appendix B are, of course,
true for linear transformations. Nevertheless, we repeat some of the deÔ¨Åni-
tions for use in this section.
DeÔ¨Ånition. Let V and W be vector spaces, and let T: V ‚ÜíW be linear.
A function U: W ‚ÜíV is said to be an inverse of T if TU = IW and UT = IV.
If T has an inverse, then T is said to be invertible. As noted in Appendix B,
if T is invertible, then the inverse of T is unique and is denoted by T‚àí1.

100
Chap. 2
Linear Transformations and Matrices
The following facts hold for invertible functions T and U.
1. (TU)‚àí1 = U‚àí1T‚àí1.
2. (T‚àí1)‚àí1 = T; in particular, T‚àí1 is invertible.
We often use the fact that a function is invertible if and only if it is both
one-to-one and onto. We can therefore restate Theorem 2.5 as follows.
3. Let T: V ‚ÜíW be a linear transformation, where V and W are Ô¨Ånite-
dimensional spaces of equal dimension. Then T is invertible if and only
if rank(T) = dim(V).
Example 1
Let T: P1(R) ‚ÜíR2 be the linear transformation deÔ¨Åned by T(a + bx) =
(a, a + b). The reader can verify directly that T‚àí1 : R2 ‚ÜíP1(R) is deÔ¨Åned by
T‚àí1(c, d) = c + (d ‚àíc)x. Observe that T‚àí1 is also linear. As Theorem 2.17
demonstrates, this is true in general.
‚ô¶
Theorem 2.17. Let V and W be vector spaces, and let T: V ‚ÜíW be
linear and invertible. Then T‚àí1 : W ‚ÜíV is linear.
Proof. Let y1, y2 ‚ààW and c ‚ààF. Since T is onto and one-to-one, there
exist unique vectors x1 and x2 such that T(x1) = y1 and T(x2) = y2. Thus
x1 = T‚àí1(y1) and x2 = T‚àí1(y2); so
T‚àí1(cy1 + y2) = T‚àí1[cT(x1) + T(x2)] = T‚àí1[T(cx1 + x2)]
= cx1 + x2 = cT‚àí1(y1) + T‚àí1(y2).
It now follows immediately from Theorem 2.5 (p. 71) that if T is a linear
transformation between vector spaces of equal (Ô¨Ånite) dimension, then the
conditions of being invertible, one-to-one, and onto are all equivalent.
We are now ready to deÔ¨Åne the inverse of a matrix. The reader should
note the analogy with the inverse of a linear transformation.
DeÔ¨Ånition.
Let A be an n √ó n matrix. Then A is invertible if there
exists an n √ó n matrix B such that AB = BA = I.
If A is invertible, then the matrix B such that AB = BA = I is unique. (If
C were another such matrix, then C = CI = C(AB) = (CA)B = IB = B.)
The matrix B is called the inverse of A and is denoted by A‚àí1.
Example 2
The reader should verify that the inverse of

5
7
2
3
	
is

3
‚àí7
‚àí2
5
	
.
‚ô¶

Sec. 2.4
Invertibility and Isomorphisms
101
In Section 3.2, we learn a technique for computing the inverse of a matrix.
At this point, we develop a number of results that relate the inverses of
matrices to the inverses of linear transformations.
Lemma. Let T be an invertible linear transformation from V to W. Then
V is Ô¨Ånite-dimensional if and only if W is Ô¨Ånite-dimensional. In this case,
dim(V) = dim(W).
Proof. Suppose that V is Ô¨Ånite-dimensional. Let Œ≤ = {x1, x2, . . . , xn} be a
basis for V. By Theorem 2.2 (p. 68), T(Œ≤) spans R(T) = W; hence W is Ô¨Ånite-
dimensional by Theorem 1.9 (p. 44). Conversely, if W is Ô¨Ånite-dimensional,
then so is V by a similar argument, using T‚àí1.
Now suppose that V and W are Ô¨Ånite-dimensional. Because T is one-to-one
and onto, we have
nullity(T) = 0
and
rank(T) = dim(R(T)) = dim(W).
So by the dimension theorem (p. 70), it follows that dim(V) = dim(W).
Theorem 2.18. Let V and W be Ô¨Ånite-dimensional vector spaces with
ordered bases Œ≤ and Œ≥, respectively. Let T: V ‚ÜíW be linear. Then T is
invertible if and only if [T]Œ≥
Œ≤ is invertible. Furthermore, [T‚àí1]Œ≤
Œ≥ = ([T]Œ≥
Œ≤)‚àí1.
Proof. Suppose that T is invertible. By the lemma, we have dim(V) =
dim(W). Let n = dim(V). So [T]Œ≥
Œ≤ is an n √ó n matrix. Now T‚àí1 : W ‚ÜíV
satisÔ¨Åes TT‚àí1 = IW and T‚àí1T = IV. Thus
In = [IV]Œ≤ = [T‚àí1T]Œ≤ = [T‚àí1]Œ≤
Œ≥[T]Œ≥
Œ≤.
Similarly, [T]Œ≥
Œ≤[T‚àí1]Œ≤
Œ≥ = In. So [T]Œ≥
Œ≤ is invertible and

[T]Œ≥
Œ≤
‚àí1
= [T‚àí1]Œ≤
Œ≥.
Now suppose that A = [T]Œ≥
Œ≤ is invertible. Then there exists an n √ó n
matrix B such that AB = BA = In. By Theorem 2.6 (p. 72), there exists
U ‚ààL(W, V) such that
U(wj) =
n

i=1
Bijvi
for j = 1, 2, . . . , n,
where Œ≥ = {w1, w2, . . . , wn} and Œ≤ = {v1, v2, . . . , vn}. It follows that [U]Œ≤
Œ≥ =
B. To show that U = T‚àí1, observe that
[UT]Œ≤ = [U]Œ≤
Œ≥[T]Œ≥
Œ≤ = BA = In = [IV]Œ≤
by Theorem 2.11 (p. 88). So UT = IV, and similarly, TU = IW.

102
Chap. 2
Linear Transformations and Matrices
Example 3
Let Œ≤ and Œ≥ be the standard ordered bases of P1(R) and R2, respectively. For
T as in Example 1, we have
[T]Œ≥
Œ≤ =

1
0
1
1
	
and
[T‚àí1]Œ≤
Œ≥ =

1
0
‚àí1
1
	
.
It can be veriÔ¨Åed by matrix multiplication that each matrix is the inverse of
the other.
‚ô¶
Corollary 1. Let V be a Ô¨Ånite-dimensional vector space with an ordered
basis Œ≤, and let T: V ‚ÜíV be linear. Then T is invertible if and only if [T]Œ≤
is invertible. Furthermore, [T‚àí1]Œ≤ = ([T]Œ≤)‚àí1.
Proof. Exercise.
Corollary 2. Let A be an n √ó n matrix. Then A is invertible if and only
if LA is invertible. Furthermore, (LA)‚àí1 = LA‚àí1.
Proof. Exercise.
The notion of invertibility may be used to formalize what may already
have been observed by the reader, that is, that certain vector spaces strongly
resemble one another except for the form of their vectors. For example, in
the case of M2√ó2(F) and F4, if we associate to each matrix

a
b
c
d
	
the 4-tuple (a, b, c, d), we see that sums and scalar products associate in a
similar manner; that is, in terms of the vector space structure, these two
vector spaces may be considered identical or isomorphic.
DeÔ¨Ånitions.
Let V and W be vector spaces. We say that V is isomor-
phic to W if there exists a linear transformation T: V ‚ÜíW that is invertible.
Such a linear transformation is called an isomorphism from V onto W.
We leave as an exercise (see Exercise 13) the proof that ‚Äúis isomorphic
to‚Äù is an equivalence relation. (See Appendix A.) So we need only say that
V and W are isomorphic.
Example 4
DeÔ¨Åne T: F2 ‚ÜíP1(F) by T(a1, a2) = a1 + a2x. It is easily checked that T is
an isomorphism; so F2 is isomorphic to P1(F).
‚ô¶

Sec. 2.4
Invertibility and Isomorphisms
103
Example 5
DeÔ¨Åne
T: P3(R) ‚ÜíM2√ó2(R)
by T(f) =

f(1)
f(2)
f(3)
f(4)
	
.
It is easily veriÔ¨Åed that T is linear. By use of the Lagrange interpolation
formula in Section 1.6, it can be shown (compare with Exercise 22) that
T(f) = O only when f is the zero polynomial. Thus T is one-to-one (see
Exercise 11). Moreover, because dim(P3(R)) = dim(M2√ó2(R)), it follows that
T is invertible by Theorem 2.5 (p. 71). We conclude that P3(R) is isomorphic
to M2√ó2(R).
‚ô¶
In each of Examples 4 and 5, the reader may have observed that isomor-
phic vector spaces have equal dimensions. As the next theorem shows, this
is no coincidence.
Theorem 2.19. Let V and W be Ô¨Ånite-dimensional vector spaces (over
the same Ô¨Åeld). Then V is isomorphic to W if and only if dim(V) = dim(W).
Proof. Suppose that V is isomorphic to W and that T: V ‚ÜíW is an
isomorphism from V to W. By the lemma preceding Theorem 2.18, we have
that dim(V) = dim(W).
Now suppose that dim(V) = dim(W), and let Œ≤ = {v1, v2, . . . , vn} and
Œ≥ = {w1, w2, . . . , wn} be bases for V and W, respectively. By Theorem 2.6
(p. 72), there exists T: V ‚ÜíW such that T is linear and T(vi) = wi for
i = 1, 2, . . . , n. Using Theorem 2.2 (p. 68), we have
R(T) = span(T(Œ≤)) = span(Œ≥) = W.
So T is onto. From Theorem 2.5 (p. 71), we have that T is also one-to-one.
Hence T is an isomorphism.
By the lemma to Theorem 2.18, if V and W are isomorphic, then either
both of V and W are Ô¨Ånite-dimensional or both are inÔ¨Ånite-dimensional.
Corollary. Let V be a vector space over F. Then V is isomorphic to Fn
if and only if dim(V) = n.
Up to this point, we have associated linear transformations with their
matrix representations. We are now in a position to prove that, as a vector
space, the collection of all linear transformations between two given vector
spaces may be identiÔ¨Åed with the appropriate vector space of m√ón matrices.
Theorem 2.20. Let V and W be Ô¨Ånite-dimensional vector spaces over F
of dimensions n and m, respectively, and let Œ≤ and Œ≥ be ordered bases for V
and W, respectively. Then the function Œ¶: L(V, W) ‚ÜíMm√ón(F), deÔ¨Åned by
Œ¶(T) = [T]Œ≥
Œ≤ for T ‚ààL(V, W), is an isomorphism.

104
Chap. 2
Linear Transformations and Matrices
Proof. By Theorem 2.8 (p. 82), Œ¶ is linear. Hence we must show that Œ¶
is one-to-one and onto. This is accomplished if we show that for every m √ó n
matrix A, there exists a unique linear transformation T: V ‚ÜíW such that
Œ¶(T) = A. Let Œ≤ = {v1, v2, . . . , vn}, Œ≥ = {w1, w2, . . . , wm}, and let A be a
given m √ó n matrix. By Theorem 2.6 (p. 72), there exists a unique linear
transformation T: V ‚ÜíW such that
T(vj) =
m

i=1
Aijwi
for 1 ‚â§j ‚â§n.
But this means that [T]Œ≥
Œ≤ = A, or Œ¶(T) = A. Thus Œ¶ is an isomorphism.
Corollary. Let V and W be Ô¨Ånite-dimensional vector spaces of dimensions
n and m, respectively. Then L(V, W) is Ô¨Ånite-dimensional of dimension mn.
Proof. The proof follows from Theorems 2.20 and 2.19 and the fact that
dim(Mm√ón(F)) = mn.
We conclude this section with a result that allows us to see more clearly
the relationship between linear transformations deÔ¨Åned on abstract Ô¨Ånite-
dimensional vector spaces and linear transformations from Fn to Fm.
We begin by naming the transformation x ‚Üí[x]Œ≤ introduced in Sec-
tion 2.2.
DeÔ¨Ånition. Let Œ≤ be an ordered basis for an n-dimensional vector space
V over the Ô¨Åeld F. The standard representation of V with respect to
Œ≤ is the function œÜŒ≤ : V ‚ÜíFn deÔ¨Åned by œÜŒ≤(x) = [x]Œ≤ for each x ‚ààV.
Example 6
Let Œ≤ = {(1, 0), (0, 1)} and Œ≥ = {(1, 2), (3, 4)}. It is easily observed that Œ≤
and Œ≥ are ordered bases for R2. For x = (1, ‚àí2), we have
œÜŒ≤(x) = [x]Œ≤ =

1
‚àí2
	
and
œÜŒ≥(x) = [x]Œ≥ =

‚àí5
2
	
.
‚ô¶
We observed earlier that œÜŒ≤ is a linear transformation. The next theorem
tells us much more.
Theorem 2.21. For any Ô¨Ånite-dimensional vector space V with ordered
basis Œ≤, œÜŒ≤ is an isomorphism.
Proof. Exercise.
This theorem provides us with an alternate proof that an n-dimensional
vector space is isomorphic to Fn (see the corollary to Theorem 2.19).

Sec. 2.4
Invertibility and Isomorphisms
105
Fn
Fm
V
W
-
-
?
?
T
LA
œÜŒ≤
œÜŒ≥
........................................................................................................................................................... .....................................................................................................................................................................
............................................................................................................................
......................
........................................
..................
..................
..................
..................
..................
..................
.................
-
?
(1)
(2)
Figure 2.2
Let V and W be vector spaces of dimension n and m, respectively, and let
T: V ‚ÜíW be a linear transformation. DeÔ¨Åne A = [T]Œ≥
Œ≤, where Œ≤ and Œ≥ are
arbitrary ordered bases of V and W, respectively. We are now able to use œÜŒ≤
and œÜŒ≥ to study the relationship between the linear transformations T and
LA : Fn ‚ÜíFm.
Let us Ô¨Årst consider Figure 2.2. Notice that there are two composites of
linear transformations that map V into Fm:
1. Map V into Fn with œÜŒ≤ and follow this transformation with LA; this
yields the composite LAœÜŒ≤.
2. Map V into W with T and follow it by œÜŒ≥ to obtain the composite œÜŒ≥T.
These two composites are depicted by the dashed arrows in the diagram.
By a simple reformulation of Theorem 2.14 (p. 91), we may conclude that
LAœÜŒ≤ = œÜŒ≥T;
that is, the diagram ‚Äúcommutes.‚Äù Heuristically, this relationship indicates
that after V and W are identiÔ¨Åed with Fn and Fm via œÜŒ≤ and œÜŒ≥, respectively,
we may ‚Äúidentify‚Äù T with LA. This diagram allows us to transfer operations
on abstract vector spaces to ones on Fn and Fm.
Example 7
Recall the linear transformation T: P3(R) ‚ÜíP2(R) deÔ¨Åned in Example 4 of
Section 2.2 (T(f(x)) = f ‚Ä≤(x)). Let Œ≤ and Œ≥ be the standard ordered bases for
P3(R) and P2(R), respectively, and let œÜŒ≤ : P3(R) ‚ÜíR4 and œÜŒ≥ : P2(R) ‚ÜíR3
be the corresponding standard representations of P3(R) and P2(R). If A =
[T]Œ≥
Œ≤, then
A =
‚éõ
‚éù
0
1
0
0
0
0
2
0
0
0
0
3
‚éû
‚é†.

106
Chap. 2
Linear Transformations and Matrices
Consider the polynomial p(x) = 2+x‚àí3x2+5x3. We show that LAœÜŒ≤(p(x)) =
œÜŒ≥T(p(x)). Now
LAœÜŒ≤(p(x)) =
‚éõ
‚éù
0
1
0
0
0
0
2
0
0
0
0
3
‚éû
‚é†
‚éõ
‚éú
‚éú
‚éù
2
1
‚àí3
5
‚éû
‚éü
‚éü
‚é†=
‚éõ
‚éù
1
‚àí6
15
‚éû
‚é†.
But since T(p(x)) = p‚Ä≤(x) = 1 ‚àí6x + 15x2, we have
œÜŒ≥T(p(x)) =
‚éõ
‚éù
1
‚àí6
15
‚éû
‚é†.
So LAœÜŒ≤(p(x)) = œÜŒ≥T(p(x)).
‚ô¶
Try repeating Example 7 with diÔ¨Äerent polynomials p(x).
EXERCISES
1.
Label the following statements as true or false. In each part, V and
W are vector spaces with ordered (Ô¨Ånite) bases Œ± and Œ≤, respectively,
T: V ‚ÜíW is linear, and A and B are matrices.
(a)

[T]Œ≤
Œ±
‚àí1 = [T‚àí1]Œ≤
Œ±.
(b)
T is invertible if and only if T is one-to-one and onto.
(c)
T = LA, where A = [T]Œ≤
Œ±.
(d)
M2√ó3(F) is isomorphic to F5.
(e)
Pn(F) is isomorphic to Pm(F) if and only if n = m.
(f)
AB = I implies that A and B are invertible.
(g)
If A is invertible, then (A‚àí1)‚àí1 = A.
(h)
A is invertible if and only if LA is invertible.
(i)
A must be square in order to possess an inverse.
2.
For each of the following linear transformations T, determine whether
T is invertible and justify your answer.
(a)
T: R2 ‚ÜíR3 deÔ¨Åned by T(a1, a2) = (a1 ‚àí2a2, a2, 3a1 + 4a2).
(b)
T: R2 ‚ÜíR3 deÔ¨Åned by T(a1, a2) = (3a1 ‚àía2, a2, 4a1).
(c)
T: R3 ‚ÜíR3 deÔ¨Åned by T(a1, a2, a3) = (3a1 ‚àí2a3, a2, 3a1 + 4a2).
(d)
T: P3(R) ‚ÜíP2(R) deÔ¨Åned by T(p(x)) = p‚Ä≤(x).
(e)
T: M2√ó2(R) ‚ÜíP2(R) deÔ¨Åned by T

a
b
c
d
	
= a + 2bx + (c + d)x2.
(f)
T: M2√ó2(R) ‚ÜíM2√ó2(R) deÔ¨Åned by T

a
b
c
d
	
=

a + b
a
c
c + d
	
.

Sec. 2.4
Invertibility and Isomorphisms
107
3.
Which of the following pairs of vector spaces are isomorphic? Justify
your answers.
(a)
F3 and P3(F).
(b)
F4 and P3(F).
(c)
M2√ó2(R) and P3(R).
(d)
V = {A ‚ààM2√ó2(R): tr(A) = 0} and R4.
4.‚Ä† Let A and B be n √ó n invertible matrices. Prove that AB is invertible
and (AB)‚àí1 = B‚àí1A‚àí1.
5.‚Ä† Let A be invertible. Prove that At is invertible and (At)‚àí1 = (A‚àí1)t.
6.
Prove that if A is invertible and AB = O, then B = O.
7.
Let A be an n √ó n matrix.
(a)
Suppose that A2 = O. Prove that A is not invertible.
(b)
Suppose that AB = O for some nonzero n √ó n matrix B. Could A
be invertible? Explain.
8.
Prove Corollaries 1 and 2 of Theorem 2.18.
9.
Let A and B be n√ón matrices such that AB is invertible. Prove that A
and B are invertible. Give an example to show that arbitrary matrices
A and B need not be invertible if AB is invertible.
10.‚Ä† Let A and B be n √ó n matrices such that AB = In.
(a)
Use Exercise 9 to conclude that A and B are invertible.
(b)
Prove A = B‚àí1 (and hence B = A‚àí1). (We are, in eÔ¨Äect, saying
that for square matrices, a ‚Äúone-sided‚Äù inverse is a ‚Äútwo-sided‚Äù
inverse.)
(c)
State and prove analogous results for linear transformations de-
Ô¨Åned on Ô¨Ånite-dimensional vector spaces.
11.
Verify that the transformation in Example 5 is one-to-one.
12.
Prove Theorem 2.21.
13.
Let ‚àºmean ‚Äúis isomorphic to.‚Äù Prove that ‚àºis an equivalence relation
on the class of vector spaces over F.
14.
Let
V =

a
a + b
0
c
	
: a, b, c ‚ààF

.
Construct an isomorphism from V to F3.

108
Chap. 2
Linear Transformations and Matrices
15.
Let V and W be Ô¨Ånite-dimensional vector spaces, and let T: V ‚ÜíW be
a linear transformation. Suppose that Œ≤ is a basis for V. Prove that T
is an isomorphism if and only if T(Œ≤) is a basis for W.
16.
Let B be an n √ó n invertible matrix. DeÔ¨Åne Œ¶: Mn√ón(F) ‚ÜíMn√ón(F)
by Œ¶(A) = B‚àí1AB. Prove that Œ¶ is an isomorphism.
17.‚Ä† Let V and W be Ô¨Ånite-dimensional vector spaces and T: V ‚ÜíW be an
isomorphism. Let V0 be a subspace of V.
(a)
Prove that T(V0) is a subspace of W.
(b)
Prove that dim(V0) = dim(T(V0)).
18.
Repeat Example 7 with the polynomial p(x) = 1 + x + 2x2 + x3.
19.
In Example 5 of Section 2.1, the mapping T: M2√ó2(R) ‚ÜíM2√ó2(R) de-
Ô¨Åned by T(M) = M t for each M ‚ààM2√ó2(R) is a linear transformation.
Let Œ≤ = {E11, E12, E21, E22}, which is a basis for M2√ó2(R), as noted in
Example 3 of Section 1.6.
(a)
Compute [T]Œ≤.
(b)
Verify that LAœÜŒ≤(M) = œÜŒ≤T(M) for A = [T]Œ≤ and
M =

1
2
3
4
	
.
20.‚Ä† Let T: V ‚ÜíW be a linear transformation from an n-dimensional vector
space V to an m-dimensional vector space W. Let Œ≤ and Œ≥ be ordered
bases for V and W, respectively. Prove that rank(T) = rank(LA) and
that nullity(T) = nullity(LA), where A = [T]Œ≥
Œ≤. Hint: Apply Exercise 17
to Figure 2.2.
21.
Let V and W be Ô¨Ånite-dimensional vector spaces with ordered bases
Œ≤ = {v1, v2, . . . , vn} and Œ≥ = {w1, w2, . . . , wm}, respectively. By The-
orem 2.6 (p. 72), there exist linear transformations Tij : V ‚ÜíW such
that
Tij(vk) =

wi
if k = j
0
if k Ã∏= j.
First prove that {Tij : 1 ‚â§i ‚â§m, 1 ‚â§j ‚â§n} is a basis for L(V, W).
Then let M ij be the m√ón matrix with 1 in the ith row and jth column
and 0 elsewhere, and prove that [Tij]Œ≥
Œ≤ = M ij. Again by Theorem 2.6,
there exists a linear transformation Œ¶: L(V, W) ‚ÜíMm√ón(F) such that
Œ¶(Tij) = M ij. Prove that Œ¶ is an isomorphism.

Sec. 2.4
Invertibility and Isomorphisms
109
22.
Let c0, c1, . . . , cn be distinct scalars from an inÔ¨Ånite Ô¨Åeld F.
DeÔ¨Åne
T: Pn(F) ‚ÜíFn+1 by T(f) = (f(c0), f(c1), . . . , f(cn)). Prove that T is
an isomorphism. Hint: Use the Lagrange polynomials associated with
c0, c1, . . . , cn.
23.
Let V denote the vector space deÔ¨Åned in Example 5 of Section 1.2, and
let W = P(F). DeÔ¨Åne
T: V ‚ÜíW
by
T(œÉ) =
n

i=0
œÉ(i)xi,
where n is the largest integer such that œÉ(n) Ã∏= 0. Prove that T is an
isomorphism.
The following exercise requires familiarity with the concept of quotient space
deÔ¨Åned in Exercise 31 of Section 1.3 and with Exercise 40 of Section 2.1.
24.
Let T: V ‚ÜíZ be a linear transformation of a vector space V onto a
vector space Z. DeÔ¨Åne the mapping
T: V/N(T) ‚ÜíZ
by
T(v + N(T)) = T(v)
for any coset v + N(T) in V/N(T).
(a)
Prove that T is well-deÔ¨Åned; that is, prove that if v + N(T) =
v‚Ä≤ + N(T), then T(v) = T(v‚Ä≤).
(b)
Prove that T is linear.
(c)
Prove that T is an isomorphism.
(d)
Prove that the diagram shown in Figure 2.3 commutes; that is,
prove that T = TŒ∑.
V/N(T)
V
Z
T
Œ∑
T
-
U

Figure 2.3
25.
Let V be a nonzero vector space over a Ô¨Åeld F, and suppose that S is
a basis for V. (By the corollary to Theorem 1.13 (p. 60) in Section 1.7,
every vector space has a basis). Let C(S, F) denote the vector space of
all functions f ‚ààF(S, F) such that f(s) = 0 for all but a Ô¨Ånite number

110
Chap. 2
Linear Transformations and Matrices
of vectors in S. (See Exercise 14 of Section 1.3.) Let Œ®: C(S, F) ‚ÜíV
be the function deÔ¨Åned by
Œ®(f) =

s‚ààS,f(s)Ã∏=0
f(s)s.
Prove that Œ® is an isomorphism. Thus every nonzero vector space can
be viewed as a space of functions.
2.5
THE CHANGE OF COORDINATE MATRIX
In many areas of mathematics, a change of variable is used to simplify the
appearance of an expression. For example, in calculus an antiderivative of
2xex2 can be found by making the change of variable u = x2. The resulting
expression is of such a simple form that an antiderivative is easily recognized:

2xex2 dx =

eu du = eu + c = ex2 + c.
Similarly, in geometry the change of variable
x = 2
‚àö
5x‚Ä≤ ‚àí1
‚àö
5y‚Ä≤
y = 1
‚àö
5x‚Ä≤ + 2
‚àö
5y‚Ä≤
can be used to transform the equation 2x2 ‚àí4xy + 5y2 = 1 into the simpler
equation (x‚Ä≤)2+6(y‚Ä≤)2 = 1, in which form it is easily seen to be the equation of
an ellipse. (See Figure 2.4.) We see how this change of variable is determined
in Section 6.5. Geometrically, the change of variable

x
y
	
‚Üí

x‚Ä≤
y‚Ä≤
	
is a change in the way that the position of a point P in the plane is described.
This is done by introducing a new frame of reference, an x‚Ä≤y‚Ä≤-coordinate
system with coordinate axes rotated from the original xy-coordinate axes. In
this case, the new coordinate axes are chosen to lie in the direction of the
axes of the ellipse. The unit vectors along the x‚Ä≤-axis and the y‚Ä≤-axis form an
ordered basis
Œ≤‚Ä≤ =
 1
‚àö
5

2
1
	
,
1
‚àö
5

‚àí1
2
	
for R2, and the change of variable is actually a change from [P]Œ≤ =

x
y
	
, the
coordinate vector of P relative to the standard ordered basis Œ≤ = {e1, e2}, to

Sec. 2.5
The Change of Coordinate Matrix
111
[P]Œ≤‚Ä≤ =

x‚Ä≤
y‚Ä≤
	
, the coordinate vector of P relative to the new rotated basis Œ≤‚Ä≤.
-
6
K
*
x
x‚Ä≤
y
y‚Ä≤
Figure 2.4
A natural question arises: How can a coordinate vector relative to one ba-
sis be changed into a coordinate vector relative to the other? Notice that the
system of equations relating the new and old coordinates can be represented
by the matrix equation

x
y
	
=
1
‚àö
5

2
‚àí1
1
2
	 
x‚Ä≤
y‚Ä≤
	
.
Notice also that the matrix
Q =
1
‚àö
5

2
‚àí1
1
2
	
equals [I]Œ≤
Œ≤‚Ä≤, where I denotes the identity transformation on R2. Thus [v]Œ≤ =
Q[v]Œ≤‚Ä≤ for all v ‚ààR2. A similar result is true in general.
Theorem 2.22. Let Œ≤ and Œ≤‚Ä≤ be two ordered bases for a Ô¨Ånite-dimensional
vector space V, and let Q = [IV]Œ≤
Œ≤‚Ä≤. Then
(a) Q is invertible.
(b) For any v ‚ààV, [v]Œ≤ = Q[v]Œ≤‚Ä≤.
Proof. (a) Since IV is invertible, Q is invertible by Theorem 2.18 (p. 101).
(b) For any v ‚ààV,
[v]Œ≤ = [IV(v)]Œ≤ = [IV]Œ≤
Œ≤‚Ä≤[v]Œ≤‚Ä≤ = Q[v]Œ≤‚Ä≤
by Theorem 2.14 (p. 91).

112
Chap. 2
Linear Transformations and Matrices
The matrix Q = [IV]Œ≤
Œ≤‚Ä≤ deÔ¨Åned in Theorem 2.22 is called a change of coor-
dinate matrix. Because of part (b) of the theorem, we say that Q changes
Œ≤‚Ä≤-coordinates into Œ≤-coordinates. Observe that if Œ≤ = {x1, x2, . . . , xn}
and Œ≤‚Ä≤ = {x‚Ä≤
1, x‚Ä≤
2, . . . , x‚Ä≤
n}, then
x‚Ä≤
j =
n

i=1
Qijxi
for j = 1, 2, . . . , n; that is, the jth column of Q is [x‚Ä≤
j]Œ≤.
Notice that if Q changes Œ≤‚Ä≤-coordinates into Œ≤-coordinates, then Q‚àí1
changes Œ≤-coordinates into Œ≤‚Ä≤-coordinates. (See Exercise 11.)
Example 1
In R2, let Œ≤ = {(1, 1), (1, ‚àí1)} and Œ≤‚Ä≤ = {(2, 4), (3, 1)}. Since
(2, 4) = 3(1, 1) ‚àí1(1, ‚àí1)
and
(3, 1) = 2(1, 1) + 1(1, ‚àí1),
the matrix that changes Œ≤‚Ä≤-coordinates into Œ≤-coordinates is
Q =

3
2
‚àí1
1
	
.
Thus, for instance,
[(2, 4)]Œ≤ = Q[(2, 4)]Œ≤‚Ä≤ = Q

1
0
	
=

3
‚àí1
	
.
‚ô¶
For the remainder of this section, we consider only linear transformations
that map a vector space V into itself. Such a linear transformation is called a
linear operator on V. Suppose now that T is a linear operator on a Ô¨Ånite-
dimensional vector space V and that Œ≤ and Œ≤‚Ä≤ are ordered bases for V. Then
V can be represented by the matrices [T]Œ≤ and [T]Œ≤‚Ä≤. What is the relationship
between these matrices? The next theorem provides a simple answer using a
change of coordinate matrix.
Theorem 2.23. Let T be a linear operator on a Ô¨Ånite-dimensional vector
space V, and let Œ≤ and Œ≤‚Ä≤ be ordered bases for V. Suppose that Q is the
change of coordinate matrix that changes Œ≤‚Ä≤-coordinates into Œ≤-coordinates.
Then
[T]Œ≤‚Ä≤ = Q‚àí1[T]Œ≤Q.
Proof. Let I be the identity transformation on V. Then T = IT = TI;
hence, by Theorem 2.11 (p. 88),
Q[T]Œ≤‚Ä≤ = [I]Œ≤
Œ≤‚Ä≤[T]Œ≤‚Ä≤
Œ≤‚Ä≤ = [IT]Œ≤
Œ≤‚Ä≤ = [TI]Œ≤
Œ≤‚Ä≤ = [T]Œ≤
Œ≤[I]Œ≤
Œ≤‚Ä≤ = [T]Œ≤Q.
Therefore [T]Œ≤‚Ä≤ = Q‚àí1[T]Œ≤Q.

Sec. 2.5
The Change of Coordinate Matrix
113
Example 2
Let T be the linear operator on R2 deÔ¨Åned by
T

a
b
	
=

3a ‚àíb
a + 3b
	
,
and let Œ≤ and Œ≤‚Ä≤ be the ordered bases in Example 1. The reader should verify
that
[T]Œ≤ =

3
1
‚àí1
3
	
.
In Example 1, we saw that the change of coordinate matrix that changes
Œ≤‚Ä≤-coordinates into Œ≤-coordinates is
Q =

3
2
‚àí1
1
	
,
and it is easily veriÔ¨Åed that
Q‚àí1 = 1
5

1
‚àí2
1
3
	
.
Hence, by Theorem 2.23,
[T]Œ≤‚Ä≤ = Q‚àí1[T]Œ≤Q =

4
1
‚àí2
2
	
.
To show that this is the correct matrix, we can verify that the image
under T of each vector of Œ≤‚Ä≤ is the linear combination of the vectors of Œ≤‚Ä≤
with the entries of the corresponding column as its coeÔ¨Écients. For example,
the image of the second vector in Œ≤‚Ä≤ is
T

3
1
	
=

8
6
	
= 1

2
4
	
+ 2

3
1
	
.
Notice that the coeÔ¨Écients of the linear combination are the entries of the
second column of [T]Œ≤‚Ä≤.
‚ô¶
It is often useful to apply Theorem 2.23 to compute [T]Œ≤, as the next
example shows.
Example 3
Recall the reÔ¨Çection about the x-axis in Example 3 of Section 2.1. The rule
(x, y) ‚Üí(x, ‚àíy) is easy to obtain. We now derive the less obvious rule for
the reÔ¨Çection T about the line y = 2x. (See Figure 2.5.) We wish to Ô¨Ånd an
expression for T(a, b) for any (a, b) in R2. Since T is linear, it is completely

114
Chap. 2
Linear Transformations and Matrices









HHHHHHHHHH


H
H
H
Y
-
6
r
r
(1, 2)
(‚àí2, 1)
y = 2x
(a, b)
T(a, b)
x
y
Figure 2.5
determined by its values on a basis for R2.
Clearly, T(1, 2) = (1, 2) and
T(‚àí2, 1) = ‚àí(‚àí2, 1) = (2, ‚àí1). Therefore if we let
Œ≤‚Ä≤ =

1
2
	
,

‚àí2
1
	
,
then Œ≤‚Ä≤ is an ordered basis for R2 and
[T]Œ≤‚Ä≤ =

1
0
0
‚àí1
	
.
Let Œ≤ be the standard ordered basis for R2, and let Q be the matrix that
changes Œ≤‚Ä≤-coordinates into Œ≤-coordinates. Then
Q =

1
‚àí2
2
1
	
and Q‚àí1[T]Œ≤Q = [T]Œ≤‚Ä≤. We can solve this equation for [T]Œ≤ to obtain that
[T]Œ≤ = Q[T]Œ≤‚Ä≤Q‚àí1. Because
Q‚àí1 = 1
5

1
2
‚àí2
1
	
,
the reader can verify that
[T]Œ≤ = 1
5

‚àí3
4
4
3
	
.
Since Œ≤ is the standard ordered basis, it follows that T is left-multiplication
by [T]Œ≤. Thus for any (a, b) in R2, we have
T

a
b
	
= 1
5

‚àí3
4
4
3
	 
a
b
	
= 1
5

‚àí3a + 4b
4a + 3b
	
.
‚ô¶

Sec. 2.5
The Change of Coordinate Matrix
115
A useful special case of Theorem 2.23 is contained in the next corollary,
whose proof is left as an exercise.
Corollary. Let A ‚ààMn√ón(F), and let Œ≥ be an ordered basis for Fn. Then
[LA]Œ≥ = Q‚àí1AQ, where Q is the n √ó n matrix whose jth column is the jth
vector of Œ≥.
Example 4
Let
A =
‚éõ
‚éù
2
1
0
1
1
3
0
‚àí1
0
‚éû
‚é†,
and let
Œ≥ =
‚éß
‚é®
‚é©
‚éõ
‚éù
‚àí1
0
0
‚éû
‚é†,
‚éõ
‚éù
2
1
0
‚éû
‚é†,
‚éõ
‚éù
1
1
1
‚éû
‚é†
‚é´
‚é¨
‚é≠,
which is an ordered basis for R3. Let Q be the 3√ó3 matrix whose jth column
is the jth vector of Œ≥. Then
Q =
‚éõ
‚éù
‚àí1
2
1
0
1
1
0
0
1
‚éû
‚é†
and
Q‚àí1 =
‚éõ
‚éù
‚àí1
2
‚àí1
0
1
‚àí1
0
0
1
‚éû
‚é†.
So by the preceding corollary,
[LA]Œ≥ = Q‚àí1AQ =
‚éõ
‚éù
0
2
8
‚àí1
4
6
0
‚àí1
‚àí1
‚éû
‚é†.
‚ô¶
The relationship between the matrices [T]Œ≤‚Ä≤ and [T]Œ≤ in Theorem 2.23 will
be the subject of further study in Chapters 5, 6, and 7. At this time, however,
we introduce the name for this relationship.
DeÔ¨Ånition.
Let A and B be matrices in Mn√ón(F). We say that B is
similar to A if there exists an invertible matrix Q such that B = Q‚àí1AQ.
Observe that the relation of similarity is an equivalence relation (see Ex-
ercise 9). So we need only say that A and B are similar.
Notice also that in this terminology Theorem 2.23 can be stated as follows:
If T is a linear operator on a Ô¨Ånite-dimensional vector space V, and if Œ≤ and
Œ≤‚Ä≤ are any ordered bases for V, then [T]Œ≤‚Ä≤ is similar to [T]Œ≤.
Theorem 2.23 can be generalized to allow T: V ‚ÜíW, where V is distinct
from W. In this case, we can change bases in V as well as in W (see Exercise 8).

116
Chap. 2
Linear Transformations and Matrices
EXERCISES
1.
Label the following statements as true or false.
(a)
Suppose that Œ≤ = {x1, x2, . . . , xn} and Œ≤‚Ä≤ = {x‚Ä≤
1, x‚Ä≤
2, . . . , x‚Ä≤
n} are
ordered bases for a vector space and Q is the change of coordinate
matrix that changes Œ≤‚Ä≤-coordinates into Œ≤-coordinates. Then the
jth column of Q is [xj]Œ≤‚Ä≤.
(b)
Every change of coordinate matrix is invertible.
(c)
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V,
let Œ≤ and Œ≤‚Ä≤ be ordered bases for V, and let Q be the change of
coordinate matrix that changes Œ≤‚Ä≤-coordinates into Œ≤-coordinates.
Then [T]Œ≤ = Q[T]Œ≤‚Ä≤Q‚àí1.
(d)
The matrices A, B ‚ààMn√ón(F) are called similar if B = QtAQ for
some Q ‚ààMn√ón(F).
(e)
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V.
Then for any ordered bases Œ≤ and Œ≥ for V, [T]Œ≤ is similar to [T]Œ≥.
2.
For each of the following pairs of ordered bases Œ≤ and Œ≤‚Ä≤ for R2, Ô¨Ånd
the change of coordinate matrix that changes Œ≤‚Ä≤-coordinates into Œ≤-
coordinates.
(a)
Œ≤ = {e1, e2} and Œ≤‚Ä≤ = {(a1, a2), (b1, b2)}
(b)
Œ≤ = {(‚àí1, 3), (2, ‚àí1)} and Œ≤‚Ä≤ = {(0, 10), (5, 0)}
(c)
Œ≤ = {(2, 5), (‚àí1, ‚àí3)} and Œ≤‚Ä≤ = {e1, e2}
(d)
Œ≤ = {(‚àí4, 3), (2, ‚àí1)} and Œ≤‚Ä≤ = {(2, 1), (‚àí4, 1)}
3.
For each of the following pairs of ordered bases Œ≤ and Œ≤‚Ä≤ for P2(R),
Ô¨Ånd the change of coordinate matrix that changes Œ≤‚Ä≤-coordinates into
Œ≤-coordinates.
(a)
Œ≤ = {x2, x, 1} and
Œ≤‚Ä≤ = {a2x2 + a1x + a0, b2x2 + b1x + b0, c2x2 + c1x + c0}
(b)
Œ≤ = {1, x, x2} and
Œ≤‚Ä≤ = {a2x2 + a1x + a0, b2x2 + b1x + b0, c2x2 + c1x + c0}
(c)
Œ≤ = {2x2 ‚àíx, 3x2 + 1, x2} and Œ≤‚Ä≤ = {1, x, x2}
(d)
Œ≤ = {x2 ‚àíx + 1, x + 1, x2 + 1} and
Œ≤‚Ä≤ = {x2 + x + 4, 4x2 ‚àí3x + 2, 2x2 + 3}
(e)
Œ≤ = {x2 ‚àíx, x2 + 1, x ‚àí1} and
Œ≤‚Ä≤ = {5x2 ‚àí2x ‚àí3, ‚àí2x2 + 5x + 5, 2x2 ‚àíx ‚àí3}
(f)
Œ≤ = {2x2 ‚àíx + 1, x2 + 3x ‚àí2, ‚àíx2 + 2x + 1} and
Œ≤‚Ä≤ = {9x ‚àí9, x2 + 21x ‚àí2, 3x2 + 5x + 2}
4.
Let T be the linear operator on R2 deÔ¨Åned by
T

a
b
	
=

2a + b
a ‚àí3b
	
,

Sec. 2.5
The Change of Coordinate Matrix
117
let Œ≤ be the standard ordered basis for R2, and let
Œ≤‚Ä≤ =

1
1
	
,

1
2
	
.
Use Theorem 2.23 and the fact that

1
1
1
2
	‚àí1
=

2
‚àí1
‚àí1
1
	
to Ô¨Ånd [T]Œ≤‚Ä≤.
5.
Let T be the linear operator on P1(R) deÔ¨Åned by T(p(x)) = p‚Ä≤(x),
the derivative of p(x). Let Œ≤ = {1, x} and Œ≤‚Ä≤ = {1 + x, 1 ‚àíx}. Use
Theorem 2.23 and the fact that

1
1
1
‚àí1
	‚àí1
=
‚éõ
‚éù
1
2
1
2
1
2
‚àí1
2
‚éû
‚é†
to Ô¨Ånd [T]Œ≤‚Ä≤.
6.
For each matrix A and ordered basis Œ≤, Ô¨Ånd [LA]Œ≤. Also, Ô¨Ånd an invert-
ible matrix Q such that [LA]Œ≤ = Q‚àí1AQ.
(a)
A =

1
3
1
1
	
and
Œ≤ =

1
1
	
,

1
2
	
(b)
A =

1
2
2
1
	
and
Œ≤ =

1
1
	
,

1
‚àí1
	
(c)
A =
‚éõ
‚éù
1
1
‚àí1
2
0
1
1
1
0
‚éû
‚é†
and
Œ≤ =
‚éß
‚é®
‚é©
‚éõ
‚éù
1
1
1
‚éû
‚é†,
‚éõ
‚éù
1
0
1
‚éû
‚é†,
‚éõ
‚éù
1
1
2
‚éû
‚é†
‚é´
‚é¨
‚é≠
(d)
A =
‚éõ
‚éù
13
1
4
1
13
4
4
4
10
‚éû
‚é†
and
Œ≤ =
‚éß
‚é®
‚é©
‚éõ
‚éù
1
1
‚àí2
‚éû
‚é†,
‚éõ
‚éù
1
‚àí1
0
‚éû
‚é†,
‚éõ
‚éù
1
1
1
‚éû
‚é†
‚é´
‚é¨
‚é≠
7.
In R2, let L be the line y = mx, where m Ã∏= 0. Find an expression for
T(x, y), where
(a)
T is the reÔ¨Çection of R2 about L.
(b)
T is the projection on L along the line perpendicular to L. (See
the deÔ¨Ånition of projection in the exercises of Section 2.1.)
8.
Prove the following generalization of Theorem 2.23. Let T: V ‚ÜíW be
a linear transformation from a Ô¨Ånite-dimensional vector space V to a
Ô¨Ånite-dimensional vector space W. Let Œ≤ and Œ≤‚Ä≤ be ordered bases for

118
Chap. 2
Linear Transformations and Matrices
V, and let Œ≥ and Œ≥‚Ä≤ be ordered bases for W. Then [T]Œ≥‚Ä≤
Œ≤‚Ä≤ = P ‚àí1[T]Œ≥
Œ≤Q,
where Q is the matrix that changes Œ≤‚Ä≤-coordinates into Œ≤-coordinates
and P is the matrix that changes Œ≥‚Ä≤-coordinates into Œ≥-coordinates.
9.
Prove that ‚Äúis similar to‚Äù is an equivalence relation on Mn√ón(F).
10.
Prove that if A and B are similar n √ó n matrices, then tr(A) = tr(B).
Hint: Use Exercise 13 of Section 2.3.
11.
Let V be a Ô¨Ånite-dimensional vector space with ordered bases Œ±, Œ≤,
and Œ≥.
(a)
Prove that if Q and R are the change of coordinate matrices that
change Œ±-coordinates into Œ≤-coordinates and Œ≤-coordinates into
Œ≥-coordinates, respectively, then RQ is the change of coordinate
matrix that changes Œ±-coordinates into Œ≥-coordinates.
(b)
Prove that if Q changes Œ±-coordinates into Œ≤-coordinates, then
Q‚àí1 changes Œ≤-coordinates into Œ±-coordinates.
12.
Prove the corollary to Theorem 2.23.
13.‚Ä† Let V be a Ô¨Ånite-dimensional vector space over a Ô¨Åeld F, and let Œ≤ =
{x1, x2, . . . , xn} be an ordered basis for V. Let Q be an n √ó n invertible
matrix with entries from F. DeÔ¨Åne
x‚Ä≤
j =
n

i=1
Qijxi
for 1 ‚â§j ‚â§n,
and set Œ≤‚Ä≤ = {x‚Ä≤
1, x‚Ä≤
2, . . . , x‚Ä≤
n}. Prove that Œ≤‚Ä≤ is a basis for V and hence
that Q is the change of coordinate matrix changing Œ≤‚Ä≤-coordinates into
Œ≤-coordinates.
14.
Prove the converse of Exercise 8: If A and B are each m √ó n matrices
with entries from a Ô¨Åeld F, and if there exist invertible m√óm and n√ón
matrices P and Q, respectively, such that B = P ‚àí1AQ, then there exist
an n-dimensional vector space V and an m-dimensional vector space W
(both over F), ordered bases Œ≤ and Œ≤‚Ä≤ for V and Œ≥ and Œ≥‚Ä≤ for W, and a
linear transformation T: V ‚ÜíW such that
A = [T]Œ≥
Œ≤
and
B = [T]Œ≥‚Ä≤
Œ≤‚Ä≤.
Hints: Let V = Fn, W = Fm, T = LA, and Œ≤ and Œ≥ be the standard
ordered bases for Fn and Fm, respectively. Now apply the results of
Exercise 13 to obtain ordered bases Œ≤‚Ä≤ and Œ≥‚Ä≤ from Œ≤ and Œ≥ via Q and
P, respectively.

Sec. 2.6
Dual Spaces
119
2.6‚àó
DUAL SPACES
In this section, we are concerned exclusively with linear transformations from
a vector space V into its Ô¨Åeld of scalars F, which is itself a vector space of di-
mension 1 over F. Such a linear transformation is called a linear functional
on V. We generally use the letters f, g, h, . . . to denote linear functionals. As
we see in Example 1, the deÔ¨Ånite integral provides us with one of the most
important examples of a linear functional in mathematics.
Example 1
Let V be the vector space of continuous real-valued functions on the interval
[0, 2œÄ]. Fix a function g ‚ààV. The function h: V ‚ÜíR deÔ¨Åned by
h(x) = 1
2œÄ
 2œÄ
0
x(t)g(t) dt
is a linear functional on V. In the cases that g(t) equals sin nt or cos nt, h(x)
is often called the nth Fourier coeÔ¨Écient of x.
‚ô¶
Example 2
Let V = Mn√ón(F), and deÔ¨Åne f : V ‚ÜíF by f(A) = tr(A), the trace of A. By
Exercise 6 of Section 1.3, we have that f is a linear functional.
‚ô¶
Example 3
Let V be a Ô¨Ånite-dimensional vector space, and let Œ≤ = {x1, x2, . . . , xn} be
an ordered basis for V. For each i = 1, 2, . . . , n, deÔ¨Åne fi(x) = ai, where
[x]Œ≤ =
‚éõ
‚éú
‚éú
‚éú
‚éù
a1
a2
...
an
‚éû
‚éü
‚éü
‚éü
‚é†
is the coordinate vector of x relative to Œ≤. Then fi is a linear functional on V
called the ith coordinate function with respect to the basis Œ≤. Note
that fi(xj) = Œ¥ij, where Œ¥ij is the Kronecker delta. These linear functionals
play an important role in the theory of dual spaces (see Theorem 2.24).
‚ô¶
DeÔ¨Ånition.
For a vector space V over F, we deÔ¨Åne the dual space of
V to be the vector space L(V, F), denoted by V‚àó.
Thus V‚àóis the vector space consisting of all linear functionals on V with
the operations of addition and scalar multiplication as deÔ¨Åned in Section 2.2.
Note that if V is Ô¨Ånite-dimensional, then by the corollary to Theorem 2.20
(p. 104)
dim(V‚àó) = dim(L(V, F)) = dim(V)¬∑ dim(F) = dim(V).

120
Chap. 2
Linear Transformations and Matrices
Hence by Theorem 2.19 (p. 103), V and V‚àóare isomorphic. We also deÔ¨Åne
the double dual V‚àó‚àóof V to be the dual of V‚àó. In Theorem 2.26, we show,
in fact, that there is a natural identiÔ¨Åcation of V and V‚àó‚àóin the case that V
is Ô¨Ånite-dimensional.
Theorem 2.24. Suppose that V is a Ô¨Ånite-dimensional vector space with
the ordered basis Œ≤ = {x1, x2, . . . , xn}. Let fi (1 ‚â§i ‚â§n) be the ith coordi-
nate function with respect to Œ≤ as just deÔ¨Åned, and let Œ≤‚àó= {f1, f2, . . . , fn}.
Then Œ≤‚àóis an ordered basis for V‚àó, and, for any f ‚ààV‚àó, we have
f =
n

i=1
f(xi)fi.
Proof. Let f ‚ààV‚àó. Since dim(V‚àó) = n, we need only show that
f =
n

i=1
f(xi)fi,
from which it follows that Œ≤‚àógenerates V‚àó, and hence is a basis by Corollary
2(a) to the replacement theorem (p. 47). Let
g =
n

i=1
f(xi)fi.
For 1 ‚â§j ‚â§n, we have
g(xj) =
 n

i=1
f(xi)fi

(xj) =
n

i=1
f(xi)fi(xj)
=
n

i=1
f(xi)Œ¥ij = f(xj).
Therefore f = g by the corollary to Theorem 2.6 (p. 72).
DeÔ¨Ånition.
Using the notation of Theorem 2.24, we call the ordered
basis Œ≤‚àó= {f1, f2, . . . , fn} of V‚àóthat satisÔ¨Åes fi(xj) = Œ¥ij (1 ‚â§i, j ‚â§n) the
dual basis of Œ≤.
Example 4
Let Œ≤ = {(2, 1), (3, 1)} be an ordered basis for R2. Suppose that the dual
basis of Œ≤ is given by Œ≤‚àó= {f1, f2}. To explicitly determine a formula for f1,
we need to consider the equations
1 = f1(2, 1) = f1(2e1 + e2) = 2f1(e1) + f1(e2)
0 = f1(3, 1) = f1(3e1 + e2) = 3f1(e1) + f1(e2).
Solving these equations, we obtain f1(e1) = ‚àí1 and f1(e2) = 3; that is,
f1(x, y) = ‚àíx + 3y. Similarly, it can be shown that f2(x, y) = x ‚àí2y.
‚ô¶

Sec. 2.6
Dual Spaces
121
We now assume that V and W are Ô¨Ånite-dimensional vector spaces over F
with ordered bases Œ≤ and Œ≥, respectively. In Section 2.4, we proved that there
is a one-to-one correspondence between linear transformations T: V ‚ÜíW and
m √ó n matrices (over F) via the correspondence T ‚Üî[T]Œ≥
Œ≤. For a matrix of
the form A = [T]Œ≥
Œ≤, the question arises as to whether or not there exists a
linear transformation U associated with T in some natural way such that U
may be represented in some basis as At. Of course, if m Ã∏= n, it would be
impossible for U to be a linear transformation from V into W. We now answer
this question by applying what we have already learned about dual spaces.
Theorem 2.25. Let V and W be Ô¨Ånite-dimensional vector spaces over
F with ordered bases Œ≤ and Œ≥, respectively. For any linear transformation
T: V ‚ÜíW, the mapping Tt : W‚àó‚ÜíV‚àódeÔ¨Åned by Tt(g) = gT for all g ‚ààW‚àó
is a linear transformation with the property that [Tt]Œ≤‚àó
Œ≥‚àó= ([T]Œ≥
Œ≤)t.
Proof. For g ‚ààW‚àó, it is clear that Tt(g) = gT is a linear functional on V
and hence is in V‚àó. Thus Tt maps W‚àóinto V‚àó. We leave the proof that Tt is
linear to the reader.
To complete the proof, let Œ≤ = {x1, x2, . . . , xn} and Œ≥ = {y1, y2, . . . , ym}
with dual bases Œ≤‚àó= {f1, f2, . . . , fn} and Œ≥‚àó= {g1, g2, . . . , gm}, respectively.
For convenience, let A = [T]Œ≥
Œ≤.
To Ô¨Ånd the jth column of [Tt]Œ≤‚àó
Œ≥‚àó, we be-
gin by expressing Tt(gj) as a linear combination of the vectors of Œ≤‚àó. By
Theorem 2.24, we have
Tt(gj) = gjT =
n

s=1
(gjT)(xs)fs.
So the row i, column j entry of [Tt]Œ≤‚àó
Œ≥‚àóis
(gjT)(xi) = gj(T(xi)) = gj
 m

k=1
Akiyk

=
m

k=1
Akigj(yk) =
m

k=1
AkiŒ¥jk = Aji.
Hence [Tt]Œ≤‚àó
Œ≥‚àó= At.
The linear transformation Tt deÔ¨Åned in Theorem 2.25 is called the trans-
pose of T. It is clear that Tt is the unique linear transformation U such that
[U]Œ≤‚àó
Œ≥‚àó= ([T]Œ≥
Œ≤)t.
We illustrate Theorem 2.25 with the next example.

122
Chap. 2
Linear Transformations and Matrices
Example 5
DeÔ¨Åne T: P1(R) ‚ÜíR2 by T(p(x)) = (p(0), p(2)). Let Œ≤ and Œ≥ be the standard
ordered bases for P1(R) and R2, respectively. Clearly,
[T]Œ≥
Œ≤ =

1
0
1
2
	
.
We compute [Tt]Œ≤‚àó
Œ≥‚àódirectly from the deÔ¨Ånition. Let Œ≤‚àó= {f1, f2} and Œ≥‚àó=
{g1, g2}. Suppose that [Tt]Œ≤‚àó
Œ≥‚àó=

a
b
c
d
	
. Then Tt(g1) = af1 + cf2. So
Tt(g1)(1) = (af1 + cf2)(1) = af1(1) + cf2(1) = a(1) + c(0) = a.
But also
(Tt(g1))(1) = g1(T(1)) = g1(1, 1) = 1.
So a = 1. Using similar computations, we obtain that c = 0, b = 1, and
d = 2. Hence a direct computation yields
[Tt]Œ≤‚àó
Œ≥‚àó=

1
1
0
2
	
=

[T]Œ≥
Œ≤
t
,
as predicted by Theorem 2.25.
‚ô¶
We now concern ourselves with demonstrating that any Ô¨Ånite-dimensional
vector space V can be identiÔ¨Åed in a natural way with its double dual V‚àó‚àó.
There is, in fact, an isomorphism between V and V‚àó‚àóthat does not depend
on any choice of bases for the two vector spaces.
For a vector x ‚ààV, we deÔ¨Åne x: V‚àó‚ÜíF by x(f) = f(x) for every f ‚ààV‚àó.
It is easy to verify that x is a linear functional on V‚àó, so x ‚ààV‚àó‚àó.
The
correspondence x ‚Üîx allows us to deÔ¨Åne the desired isomorphism between
V and V‚àó‚àó.
Lemma. Let V be a Ô¨Ånite-dimensional vector space, and let x ‚ààV. If
x(f) = 0 for all f ‚ààV‚àó, then x = 0.
Proof. Let x Ã∏= 0. We show that there exists f ‚ààV‚àósuch that x(f) Ã∏= 0.
Choose an ordered basis Œ≤ = {x1, x2, . . . , xn} for V such that x1 = x. Let
{f1, f2, . . . , fn} be the dual basis of Œ≤. Then f1(x1) = 1 Ã∏= 0. Let f = f1.
Theorem 2.26. Let V be a Ô¨Ånite-dimensional vector space, and deÔ¨Åne
œà: V ‚ÜíV‚àó‚àóby œà(x) = x. Then œà is an isomorphism.

Sec. 2.6
Dual Spaces
123
Proof. (a) œà is linear: Let x, y ‚ààV and c ‚ààF. For f ‚ààV‚àó, we have
œà(cx + y)(f) = f(cx + y) = cf(x) + f(y) = cx(f) + y(f)
= (cx + y)(f).
Therefore
œà(cx + y) = cx + y = cœà(x) + œà(y).
(b) œà is one-to-one: Suppose that œà(x) is the zero functional on V‚àófor
some x ‚ààV. Then x(f) = 0 for every f ‚ààV‚àó. By the previous lemma, we
conclude that x = 0.
(c) œà is an isomorphism: This follows from (b) and the fact that dim(V) =
dim(V‚àó‚àó).
Corollary. Let V be a Ô¨Ånite-dimensional vector space with dual space V‚àó.
Then every ordered basis for V‚àóis the dual basis for some basis for V.
Proof. Let {f1, f2, . . . , fn} be an ordered basis for V‚àó. We may combine
Theorems 2.24 and 2.26 to conclude that for this basis for V‚àóthere exists a
dual basis {x1, x2, . . . , xn} in V‚àó‚àó, that is, Œ¥ij = xi(fj) = fj(xi) for all i and
j. Thus {f1, f2, . . . , fn} is the dual basis of {x1, x2, . . . , xn}.
Although many of the ideas of this section, (e.g., the existence of a dual
space), can be extended to the case where V is not Ô¨Ånite-dimensional, only a
Ô¨Ånite-dimensional vector space is isomorphic to its double dual via the map
x ‚Üíx. In fact, for inÔ¨Ånite-dimensional vector spaces, no two of V, V‚àó, and
V‚àó‚àóare isomorphic.
EXERCISES
1.
Label the following statements as true or false. Assume that all vector
spaces are Ô¨Ånite-dimensional.
(a)
Every linear transformation is a linear functional.
(b)
A linear functional deÔ¨Åned on a Ô¨Åeld may be represented as a 1√ó1
matrix.
(c)
Every vector space is isomorphic to its dual space.
(d)
Every vector space is the dual of some other vector space.
(e)
If T is an isomorphism from V onto V‚àóand Œ≤ is a Ô¨Ånite ordered
basis for V, then T(Œ≤) = Œ≤‚àó.
(f)
If T is a linear transformation from V to W, then the domain of
(Tt)t is V‚àó‚àó.
(g)
If V is isomorphic to W, then V‚àóis isomorphic to W‚àó.

124
Chap. 2
Linear Transformations and Matrices
(h)
The derivative of a function may be considered as a linear func-
tional on the vector space of diÔ¨Äerentiable functions.
2.
For the following functions f on a vector space V, determine which are
linear functionals.
(a)
V = P(R); f(p(x)) = 2p‚Ä≤(0)+p‚Ä≤‚Ä≤(1), where ‚Ä≤ denotes diÔ¨Äerentiation
(b)
V = R2; f(x, y) = (2x, 4y)
(c)
V = M2√ó2(F); f(A) = tr(A)
(d)
V = R3; f(x, y, z) = x2 + y2 + z2
(e)
V = P(R); f(p(x)) =
 1
0 p(t) dt
(f)
V = M2√ó2(F); f(A) = A11
3.
For each of the following vector spaces V and bases Œ≤, Ô¨Ånd explicit
formulas for vectors of the dual basis Œ≤‚àófor V‚àó, as in Example 4.
(a)
V = R3; Œ≤ = {(1, 0, 1), (1, 2, 1), (0, 0, 1)}
(b)
V = P2(R); Œ≤ = {1, x, x2}
4.
Let V = R3, and deÔ¨Åne f1, f2, f3 ‚ààV‚àóas follows:
f1(x, y, z) = x ‚àí2y,
f2(x, y, z) = x + y + z,
f3(x, y, z) = y ‚àí3z.
Prove that {f1, f2, f3} is a basis for V‚àó, and then Ô¨Ånd a basis for V for
which it is the dual basis.
5.
Let V = P1(R), and, for p(x) ‚ààV, deÔ¨Åne f1, f2 ‚ààV‚àóby
f1(p(x)) =
 1
0
p(t) dt
and
f2(p(x)) =
 2
0
p(t) dt.
Prove that {f1, f2} is a basis for V‚àó, and Ô¨Ånd a basis for V for which it
is the dual basis.
6.
DeÔ¨Åne f ‚àà(R2)‚àóby f(x, y) = 2x + y and T: R2 ‚ÜíR2 by T(x, y) =
(3x + 2y, x).
(a)
Compute Tt(f).
(b)
Compute [Tt]Œ≤‚àó, where Œ≤ is the standard ordered basis for R2 and
Œ≤‚àó= {f1, f2} is the dual basis, by Ô¨Ånding scalars a, b, c, and d such
that Tt(f1) = af1 + cf2 and Tt(f2) = bf1 + df2.
(c)
Compute [T]Œ≤ and ([T]Œ≤)t, and compare your results with (b).
7.
Let V = P1(R) and W = R2 with respective standard ordered bases Œ≤
and Œ≥. DeÔ¨Åne T: V ‚ÜíW by
T(p(x)) = (p(0) ‚àí2p(1), p(0) + p‚Ä≤(0)),
where p‚Ä≤(x) is the derivative of p(x).

Sec. 2.6
Dual Spaces
125
(a)
For f ‚ààW‚àódeÔ¨Åned by f(a, b) = a ‚àí2b, compute Tt(f).
(b)
Compute [Tt]Œ≤‚àó
Œ≥‚àówithout appealing to Theorem 2.25.
(c)
Compute [T]Œ≥
Œ≤ and its transpose, and compare your results with
(b).
8.
Show that every plane through the origin in R3 may be identiÔ¨Åed with
the null space of a vector in (R3)‚àó. State an analogous result for R2.
9.
Prove that a function T: Fn ‚ÜíFm is linear if and only if there exist
f1, f2, . . . , fm ‚àà(Fn)‚àósuch that T(x) = (f1(x), f2(x), . . . , fm(x)) for all
x ‚ààFn. Hint: If T is linear, deÔ¨Åne fi(x) = (giT)(x) for x ‚ààFn; that is,
fi = Tt(gi) for 1 ‚â§i ‚â§m, where {g1, g2, . . . , gm} is the dual basis of
the standard ordered basis for Fm.
10.
Let V = Pn(F), and let c0, c1, . . . , cn be distinct scalars in F.
(a)
For 0 ‚â§i ‚â§n, deÔ¨Åne fi ‚ààV‚àóby fi(p(x)) = p(ci). Prove that
{f0, f1, . . . , fn} is a basis for V‚àó. Hint: Apply any linear combi-
nation of this set that equals the zero transformation to p(x) =
(x ‚àíc1)(x ‚àíc2) ¬∑ ¬∑ ¬∑ (x ‚àícn), and deduce that the Ô¨Årst coeÔ¨Écient is
zero.
(b)
Use the corollary to Theorem 2.26 and (a) to show that there exist
unique polynomials p0(x), p1(x), . . . , pn(x) such that pi(cj) = Œ¥ij
for 0 ‚â§i ‚â§n. These polynomials are the Lagrange polynomials
deÔ¨Åned in Section 1.6.
(c)
For any scalars a0, a1, . . . , an (not necessarily distinct), deduce that
there exists a unique polynomial q(x) of degree at most n such that
q(ci) = ai for 0 ‚â§i ‚â§n. In fact,
q(x) =
n

i=0
aipi(x).
(d)
Deduce the Lagrange interpolation formula:
p(x) =
n

i=0
p(ci)pi(x)
for any p(x) ‚ààV.
(e)
Prove that
 b
a
p(t) dt =
n

i=0
p(ci)di,
where
di =
 b
a
pi(t) dt.

126
Chap. 2
Linear Transformations and Matrices
Suppose now that
ci = a + i(b ‚àía)
n
for i = 0, 1, . . . , n.
For n = 1, the preceding result yields the trapezoidal rule for
evaluating the deÔ¨Ånite integral of a polynomial. For n = 2, this
result yields Simpson‚Äôs rule for evaluating the deÔ¨Ånite integral of
a polynomial.
11.
Let V and W be Ô¨Ånite-dimensional vector spaces over F, and let œà1 and
œà2 be the isomorphisms between V and V‚àó‚àóand W and W‚àó‚àó, respec-
tively, as deÔ¨Åned in Theorem 2.26. Let T: V ‚ÜíW be linear, and deÔ¨Åne
Ttt = (Tt)t. Prove that the diagram depicted in Figure 2.6 commutes
(i.e., prove that œà2T = Tttœà1).
V
T
‚àí‚àí‚àí‚àí‚Üí
W
œà1
‚èê‚èê!
‚èê‚èê!œà2
V‚àó‚àó
Ttt
‚àí‚àí‚àí‚àí‚ÜíW‚àó‚àó
Figure 2.6
12.
Let V be a Ô¨Ånite-dimensional vector space with the ordered basis Œ≤.
Prove that œà(Œ≤) = Œ≤‚àó‚àó, where œà is deÔ¨Åned in Theorem 2.26.
In Exercises 13 through 17, V denotes a Ô¨Ånite-dimensional vector space over
F. For every subset S of V, deÔ¨Åne the annihilator S0 of S as
S0 = {f ‚ààV‚àó: f(x) = 0 for all x ‚ààS}.
13. (a)
Prove that S0 is a subspace of V‚àó.
(b)
If W is a subspace of V and x Ã∏‚ààW, prove that there exists f ‚ààW0
such that f(x) Ã∏= 0.
(c)
Prove that (S0)0 = span(œà(S)), where œà is deÔ¨Åned as in Theo-
rem 2.26.
(d)
For subspaces W1 and W2, prove that W1 = W2 if and only if
W0
1 = W0
2.
(e)
For subspaces W1 and W2, show that (W1 + W2)0 = W0
1 ‚à©W0
2.
14.
Prove that if W is a subspace of V, then dim(W) + dim(W0) = dim(V).
Hint: Extend an ordered basis {x1, x2, . . . , xk} of W to an ordered ba-
sis Œ≤ = {x1, x2, . . . , xn} of V.
Let Œ≤‚àó= {f1, f2, . . . , fn}.
Prove that
{fk+1, fk+2, . . . , fn} is a basis for W0.

Sec. 2.7
Homogeneous Linear DiÔ¨Äerential Equations with Constant CoeÔ¨Écients 127
15.
Suppose that W is a Ô¨Ånite-dimensional vector space and that T: V ‚ÜíW
is linear. Prove that N(Tt) = (R(T))0.
16.
Use Exercises 14 and 15 to deduce that rank(LAt) = rank(LA) for any
A ‚ààMm√ón(F).
17.
Let T be a linear operator on V, and let W be a subspace of V. Prove
that W is T-invariant (as deÔ¨Åned in the exercises of Section 2.1) if and
only if W0 is Tt-invariant.
18.
Let V be a nonzero vector space over a Ô¨Åeld F, and let S be a basis
for V. (By the corollary to Theorem 1.13 (p. 60) in Section 1.7, every
vector space has a basis.) Let Œ¶: V‚àó‚ÜíL(S, F) be the mapping deÔ¨Åned
by Œ¶(f) = fS, the restriction of f to S. Prove that Œ¶ is an isomorphism.
Hint: Apply Exercise 34 of Section 2.1.
19.
Let V be a nonzero vector space, and let W be a proper subspace of V
(i.e., W Ã∏= V). Prove that there exists a nonzero linear functional f ‚ààV‚àó
such that f(x) = 0 for all x ‚ààW. Hint: For the inÔ¨Ånite-dimensional
case, use Exercise 34 of Section 2.1 as well as results about extending
linearly independent sets to bases in Section 1.7.
20.
Let V and W be nonzero vector spaces over the same Ô¨Åeld, and let
T: V ‚ÜíW be a linear transformation.
(a)
Prove that T is onto if and only if Tt is one-to-one.
(b)
Prove that Tt is onto if and only if T is one-to-one.
Hint: Parts of the proof require the result of Exercise 19 for the inÔ¨Ånite-
dimensional case.
2.7‚àó
HOMOGENEOUS LINEAR DIFFERENTIAL EQUATIONS
WITH CONSTANT COEFFICIENTS
As an introduction to this section, consider the following physical problem. A
weight of mass m is attached to a vertically suspended spring that is allowed to
stretch until the forces acting on the weight are in equilibrium. Suppose that
the weight is now motionless and impose an xy-coordinate system with the
weight at the origin and the spring lying on the positive y-axis (see Figure 2.7).
Suppose that at a certain time, say t = 0, the weight is lowered a distance
s along the y-axis and released. The spring then begins to oscillate.
We describe the motion of the spring. At any time t ‚â•0, let F(t) denote
the force acting on the weight and y(t) denote the position of the weight along
the y-axis. For example, y(0) = ‚àís. The second derivative of y with respect

128
Chap. 2
Linear Transformations and Matrices
.............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................
-
6
x
y
Figure 2.7
to time, y‚Ä≤‚Ä≤(t), is the acceleration of the weight at time t; hence, by Newton‚Äôs
second law of motion,
F(t) = my‚Ä≤‚Ä≤(t).
(1)
It is reasonable to assume that the force acting on the weight is due totally
to the tension of the spring, and that this force satisÔ¨Åes Hooke‚Äôs law: The force
acting on the weight is proportional to its displacement from the equilibrium
position, but acts in the opposite direction. If k > 0 is the proportionality
constant, then Hooke‚Äôs law states that
F(t) = ‚àíky(t).
(2)
Combining (1) and (2), we obtain my‚Ä≤‚Ä≤ = ‚àíky or
y‚Ä≤‚Ä≤ + k
my = 0.
(3)
The expression (3) is an example of a diÔ¨Äerential equation. A diÔ¨Äerential
equation in an unknown function y = y(t) is an equation involving y, t, and
derivatives of y. If the diÔ¨Äerential equation is of the form
any(n) + an‚àí1y(n‚àí1) + ¬∑ ¬∑ ¬∑ + a1y(1) + a0y = f,
(4)
where a0, a1, . . . , an and f are functions of t and y(k) denotes the kth deriva-
tive of y, then the equation is said to be linear. The functions ai are called
the coeÔ¨Écients of the diÔ¨Äerential equation (4).
Thus (3) is an example
of a linear diÔ¨Äerential equation in which the coeÔ¨Écients are constants and
the function f is identically zero. When f is identically zero, (4) is called
homogeneous.
In this section, we apply the linear algebra we have studied to solve ho-
mogeneous linear diÔ¨Äerential equations with constant coeÔ¨Écients. If an Ã∏= 0,

Sec. 2.7
Homogeneous Linear DiÔ¨Äerential Equations with Constant CoeÔ¨Écients 129
we say that diÔ¨Äerential equation (4) is of order n. In this case, we divide
both sides by an to obtain a new, but equivalent, equation
y(n) + bn‚àí1y(n‚àí1) + ¬∑ ¬∑ ¬∑ + b1y(1) + b0y = 0,
where bi = ai/an for i = 0, 1, . . . , n ‚àí1. Because of this observation, we
always assume that the coeÔ¨Écient an in (4) is 1.
A solution to (4) is a function that when substituted for y reduces (4)
to an identity.
Example 1
The function y(t) = sin

k/m t is a solution to (3) since
y‚Ä≤‚Ä≤(t) + k
my(t) = ‚àík
m sin
"
k
m t + k
m sin
"
k
m t = 0
for all t. Notice, however, that substituting y(t) = t into (3) yields
y‚Ä≤‚Ä≤(t) + k
my(t) = k
mt,
which is not identically zero. Thus y(t) = t is not a solution to (3).
‚ô¶
In our study of diÔ¨Äerential equations, it is useful to regard solutions as
complex-valued functions of a real variable even though the solutions that
are meaningful to us in a physical sense are real-valued. The convenience
of this viewpoint will become clear later. Thus we are concerned with the
vector space F(R, C) (as deÔ¨Åned in Example 3 of Section 1.2). In order to
consider complex-valued functions of a real variable as solutions to diÔ¨Äerential
equations, we must deÔ¨Åne what it means to diÔ¨Äerentiate such functions. Given
a complex-valued function x ‚ààF(R, C) of a real variable t, there exist unique
real-valued functions x1 and x2 of t, such that
x(t) = x1(t) + ix2(t)
for
t ‚ààR,
where i is the imaginary number such that i2 = ‚àí1. We call x1 the real part
and x2 the imaginary part of x.
DeÔ¨Ånitions. Given a function x ‚ààF(R, C) with real part x1 and imag-
inary part x2, we say that x is diÔ¨Äerentiable if x1 and x2 are diÔ¨Äerentiable.
If x is diÔ¨Äerentiable, we deÔ¨Åne the derivative x‚Ä≤ of x by
x‚Ä≤ = x‚Ä≤
1 + ix‚Ä≤
2.
We illustrate some computations with complex-valued functions in the
following example.

130
Chap. 2
Linear Transformations and Matrices
Example 2
Suppose that x(t) = cos 2t + i sin 2t. Then
x‚Ä≤(t) = ‚àí2 sin 2t + 2i cos 2t.
We next Ô¨Ånd the real and imaginary parts of x2. Since
x2(t) = (cos 2t + i sin 2t)2 = (cos2 2t ‚àísin2 2t) + i(2 sin 2t cos 2t)
= cos 4t + i sin 4t,
the real part of x2(t) is cos 4t, and the imaginary part is sin 4t.
‚ô¶
The next theorem indicates that we may limit our investigations to a
vector space considerably smaller than F(R, C). Its proof, which is illustrated
in Example 3, involves a simple induction argument, which we omit.
Theorem 2.27. Any solution to a homogeneous linear diÔ¨Äerential equa-
tion with constant coeÔ¨Écients has derivatives of all orders; that is, if x is a
solution to such an equation, then x(k) exists for every positive integer k.
Example 3
To illustrate Theorem 2.27, consider the equation
y(2) + 4y = 0.
Clearly, to qualify as a solution, a function y must have two derivatives. If y
is a solution, however, then
y(2) = ‚àí4y.
Thus since y(2) is a constant multiple of a function y that has two derivatives,
y(2) must have two derivatives. Hence y(4) exists; in fact,
y(4) = ‚àí4y(2).
Since y(4) is a constant multiple of a function that we have shown has at
least two derivatives, it also has at least two derivatives; hence y(6) exists.
Continuing in this manner, we can show that any solution has derivatives of
all orders.
‚ô¶
DeÔ¨Ånition. We use C‚àûto denote the set of all functions in F(R, C) that
have derivatives of all orders.
It is a simple exercise to show that C‚àûis a subspace of F(R, C) and hence
a vector space over C. In view of Theorem 2.27, it is this vector space that

Sec. 2.7
Homogeneous Linear DiÔ¨Äerential Equations with Constant CoeÔ¨Écients 131
is of interest to us. For x ‚ààC‚àû, the derivative x‚Ä≤ of x also lies in C‚àû. We
can use the derivative operation to deÔ¨Åne a mapping D: C‚àû‚ÜíC‚àûby
D(x) = x‚Ä≤
for x ‚ààC‚àû.
It is easy to show that D is a linear operator. More generally, consider any
polynomial over C of the form
p(t) = antn + an‚àí1tn‚àí1 + ¬∑ ¬∑ ¬∑ + a1t + a0.
If we deÔ¨Åne
p(D) = anDn + an‚àí1Dn‚àí1 + ¬∑ ¬∑ ¬∑ + a1D + a0I,
then p(D) is a linear operator on C‚àû. (See Appendix E.)
DeÔ¨Ånitions. For any polynomial p(t) over C of positive degree, p(D) is
called a diÔ¨Äerential operator. The order of the diÔ¨Äerential operator p(D)
is the degree of the polynomial p(t).
DiÔ¨Äerential operators are useful since they provide us with a means of
reformulating a diÔ¨Äerential equation in the context of linear algebra. Any
homogeneous linear diÔ¨Äerential equation with constant coeÔ¨Écients,
y(n) + an‚àí1y(n‚àí1) + ¬∑ ¬∑ ¬∑ + a1y(1) + a0y = 0,
can be rewritten using diÔ¨Äerential operators as
(Dn + an‚àí1Dn‚àí1 + ¬∑ ¬∑ ¬∑ + a1D + a0I)(y) = 0.
DeÔ¨Ånition.
Given the diÔ¨Äerential equation above, the complex polyno-
mial
p(t) = tn + an‚àí1tn‚àí1 + ¬∑ ¬∑ ¬∑ + a1t + a0
is called the auxiliary polynomial associated with the equation.
For example, (3) has the auxiliary polynomial
p(t) = t2 + k
m.
Any homogeneous linear diÔ¨Äerential equation with constant coeÔ¨Écients
can be rewritten as
p(D)(y) = 0,
where p(t) is the auxiliary polynomial associated with the equation. Clearly,
this equation implies the following theorem.

132
Chap. 2
Linear Transformations and Matrices
Theorem 2.28. The set of all solutions to a homogeneous linear diÔ¨Äeren-
tial equation with constant coeÔ¨Écients coincides with the null space of p(D),
where p(t) is the auxiliary polynomial associated with the equation.
Proof. Exercise.
Corollary. The set of all solutions to a homogeneous linear diÔ¨Äerential
equation with constant coeÔ¨Écients is a subspace of C‚àû.
In view of the preceding corollary, we call the set of solutions to a homo-
geneous linear diÔ¨Äerential equation with constant coeÔ¨Écients the solution
space of the equation. A practical way of describing such a space is in terms
of a basis.
We now examine a certain class of functions that is of use in
Ô¨Ånding bases for these solution spaces.
For a real number s, we are familiar with the real number es, where e is
the unique number whose natural logarithm is 1 (i.e., ln e = 1). We know,
for instance, certain properties of exponentiation, namely,
es+t = eset
and
e‚àít = 1
et
for any real numbers s and t. We now extend the deÔ¨Ånition of powers of e to
include complex numbers in such a way that these properties are preserved.
DeÔ¨Ånition.
Let c = a + ib be a complex number with real part a and
imaginary part b. DeÔ¨Åne
ec = ea(cos b + i sin b).
The special case
eib = cos b + i sin b
is called Euler‚Äôs formula.
For example, for c = 2 + i(œÄ/3),
ec = e2 
cos œÄ
3 + i sin œÄ
3

= e2

1
2 + i
‚àö
3
2

.
Clearly, if c is real (b = 0), then we obtain the usual result: ec = ea. Using
the approach of Example 2, we can show by the use of trigonometric identities
that
ec+d = eced
and
e‚àíc = 1
ec
for any complex numbers c and d.

Sec. 2.7
Homogeneous Linear DiÔ¨Äerential Equations with Constant CoeÔ¨Écients 133
DeÔ¨Ånition.
A function f : R ‚ÜíC deÔ¨Åned by f(t) = ect for a Ô¨Åxed
complex number c is called an exponential function.
The derivative of an exponential function, as described in the next theo-
rem, is consistent with the real version. The proof involves a straightforward
computation, which we leave as an exercise.
Theorem 2.29. For any exponential function f(t) = ect, f ‚Ä≤(t) = cect.
Proof. Exercise.
We can use exponential functions to describe all solutions to a homoge-
neous linear diÔ¨Äerential equation of order 1. Recall that the order of such an
equation is the degree of its auxiliary polynomial. Thus an equation of order
1 is of the form
y‚Ä≤ + a0y = 0.
(5)
Theorem 2.30. The solution space for (5) is of dimension 1 and has
{e‚àía0t} as a basis.
Proof. Clearly (5) has e‚àía0t as a solution. Suppose that x(t) is any solution
to (5). Then
x‚Ä≤(t) = ‚àía0x(t)
for all t ‚ààR.
DeÔ¨Åne
z(t) = ea0tx(t).
DiÔ¨Äerentiating z yields
z‚Ä≤(t) = (ea0t)‚Ä≤x(t) + ea0tx‚Ä≤(t) = a0ea0tx(t) ‚àía0ea0tx(t) = 0.
(Notice that the familiar product rule for diÔ¨Äerentiation holds for complex-
valued functions of a real variable. A justiÔ¨Åcation of this involves a lengthy,
although direct, computation.)
Since z‚Ä≤ is identically zero, z is a constant function. (Again, this fact, well
known for real-valued functions, is also true for complex-valued functions.
The proof, which relies on the real case, involves looking separately at the
real and imaginary parts of z.) Thus there exists a complex number k such
that
z(t) = ea0tx(t) = k
for all t ‚ààR.
So
x(t) = ke‚àía0t.
We conclude that any solution to (5) is a linear combination of e‚àía0t.

134
Chap. 2
Linear Transformations and Matrices
Another way of stating Theorem 2.30 is as follows.
Corollary. For any complex number c, the null space of the diÔ¨Äerential
operator D ‚àícI has {ect} as a basis.
We next concern ourselves with diÔ¨Äerential equations of order greater
than one. Given an nth order homogeneous linear diÔ¨Äerential equation with
constant coeÔ¨Écients,
y(n) + an‚àí1y(n‚àí1) + ¬∑ ¬∑ ¬∑ + a1y(1) + a0y = 0,
its auxiliary polynomial
p(t) = tn + an‚àí1tn‚àí1 + ¬∑ ¬∑ ¬∑ + a1t + a0
factors into a product of polynomials of degree 1, that is,
p(t) = (t ‚àíc1)(t ‚àíc2) ¬∑ ¬∑ ¬∑ (t ‚àícn),
where c1, c2, . . . , cn are (not necessarily distinct) complex numbers.
(This
follows from the fundamental theorem of algebra in Appendix D.) Thus
p(D) = (D ‚àíc1I)(D ‚àíc2I) ¬∑ ¬∑ ¬∑ (D ‚àícnI).
The operators D ‚àíciI commute, and so, by Exercise 9, we have that
N(D ‚àíciI) ‚äÜN(p(D))
for all i.
Since N(p(D)) coincides with the solution space of the given diÔ¨Äerential equa-
tion, we can deduce the following result from the preceding corollary.
Theorem 2.31. Let p(t) be the auxiliary polynomial for a homogeneous
linear diÔ¨Äerential equation with constant coeÔ¨Écients. For any complex num-
ber c, if c is a zero of p(t), then ect is a solution to the diÔ¨Äerential equation.
Example 4
Given the diÔ¨Äerential equation
y‚Ä≤‚Ä≤ ‚àí3y‚Ä≤ + 2y = 0,
its auxiliary polynomial is
p(t) = t2 ‚àí3t + 2 = (t ‚àí1)(t ‚àí2).
Hence, by Theorem 2.31, et and e2t are solutions to the diÔ¨Äerential equa-
tion because c = 1 and c = 2 are zeros of p(t). Since the solution space
of the diÔ¨Äerential equation is a subspace of C‚àû, span({et, e2t}) lies in the
solution space. It is a simple matter to show that {et, e2t} is linearly inde-
pendent. Thus if we can show that the solution space is two-dimensional, we
can conclude that {et, e2t} is a basis for the solution space. This result is a
consequence of the next theorem.
‚ô¶

Sec. 2.7
Homogeneous Linear DiÔ¨Äerential Equations with Constant CoeÔ¨Écients 135
Theorem 2.32. For any diÔ¨Äerential operator p(D) of order n, the null
space of p(D) is an n-dimensional subspace of C‚àû.
As a preliminary to the proof of Theorem 2.32, we establish two lemmas.
Lemma 1.
The diÔ¨Äerential operator D ‚àícI: C‚àû‚ÜíC‚àûis onto for any
complex number c.
Proof. Let v ‚ààC‚àû. We wish to Ô¨Ånd a u ‚ààC‚àûsuch that (D ‚àícI)u = v.
Let w(t) = v(t)e‚àíct for t ‚ààR. Clearly, w ‚ààC‚àûbecause both v and e‚àíct lie in
C‚àû. Let w1 and w2 be the real and imaginary parts of w. Then w1 and w2 are
continuous because they are diÔ¨Äerentiable. Hence they have antiderivatives,
say, W1 and W2, respectively. Let W: R ‚ÜíC be deÔ¨Åned by
W(t) = W1(t) + iW2(t)
for t ‚ààR.
Then W ‚ààC‚àû, and the real and imaginary parts of W are W1 and W2,
respectively. Furthermore, W ‚Ä≤ = w. Finally, let u: R ‚ÜíC be deÔ¨Åned by
u(t) = W(t)ect for t ‚ààR. Clearly u ‚ààC‚àû, and since
(D ‚àícI)u(t) = u‚Ä≤(t) ‚àícu(t)
= W ‚Ä≤(t)ect + W(t)cect ‚àícW(t)ect
= w(t)ect
= v(t)e‚àíctect
= v(t),
we have (D ‚àícI)u = v.
Lemma 2.
Let V be a vector space, and suppose that T and U are
linear operators on V such that U is onto and the null spaces of T and U are
Ô¨Ånite-dimensional. Then the null space of TU is Ô¨Ånite-dimensional, and
dim(N(TU)) = dim(N(T)) + dim(N(U)).
Proof. Let p = dim(N(T)), q = dim(N(U)), and {u1, u2, . . . , up} and
{v1, v2, . . . , vq} be bases for N(T) and N(U), respectively. Since U is onto,
we can choose for each i (1 ‚â§i ‚â§p) a vector wi ‚ààV such that U(wi) = ui.
Note that the wi‚Äôs are distinct. Furthermore, for any i and j, wi Ã∏= vj, for
otherwise ui = U(wi) = U(vj) = 0‚Äîa contradiction. Hence the set
Œ≤ = {w1, w2, . . . , wp, v1, v2, . . . , vq}
contains p+q distinct vectors. To complete the proof of the lemma, it suÔ¨Éces
to show that Œ≤ is a basis for N(TU).

136
Chap. 2
Linear Transformations and Matrices
We Ô¨Årst show that Œ≤ generates N(TU).
Since for any wi and vj in Œ≤,
TU(wi) = T(ui) = 0 and TU(vj) = T(0) = 0, it follows that Œ≤ ‚äÜN(TU).
Now suppose that v ‚ààN(TU). Then 0 = TU(v) = T(U(v)). Thus U(v) ‚àà
N(T). So there exist scalars a1, a2, . . . , ap such that
U(v) = a1u1 + a2u2 + ¬∑ ¬∑ ¬∑ + apup
= a1U(w1) + a2U(w2) + ¬∑ ¬∑ ¬∑ + apU(wp)
= U(a1w1 + a2w2 + ¬∑ ¬∑ ¬∑ + apwp).
Hence
U(v ‚àí(a1w1 + a2w2 + ¬∑ ¬∑ ¬∑ + apwp)) = 0.
Consequently, v ‚àí(a1w1 + a2w2 + ¬∑ ¬∑ ¬∑ + apwp) lies in N(U). It follows that
there exist scalars b1, b2, . . . , bq such that
v ‚àí(a1w1 + a2w2 + ¬∑ ¬∑ ¬∑ + apwp) = b1v1 + b2v2 + ¬∑ ¬∑ ¬∑ + bqvq
or
v = a1w1 + a2w2 + ¬∑ ¬∑ ¬∑ + apwp + b1v1 + b2v2 + ¬∑ ¬∑ ¬∑ + bqvq.
Therefore Œ≤ spans N(TU).
To prove that Œ≤ is linearly independent, let a1, a2, . . . , ap, b1, b2, . . . , bq be
any scalars such that
a1w1 + a2w2 + ¬∑ ¬∑ ¬∑ + apwp + b1v1 + b2v2 + ¬∑ ¬∑ ¬∑ + bqvq = 0.
(6)
Applying U to both sides of (6), we obtain
a1u1 + a2u2 + ¬∑ ¬∑ ¬∑ + apup = 0.
Since {u1, u2, . . . , up} is linearly independent, the ai‚Äôs are all zero. Thus (6)
reduces to
b1v1 + b2v2 + ¬∑ ¬∑ ¬∑ + bqvq = 0.
Again, the linear independence of {v1, v2, . . . , vq} implies that the bi‚Äôs are
all zero. We conclude that Œ≤ is a basis for N(TU). Hence N(TU) is Ô¨Ånite-
dimensional, and dim(N(TU)) = p + q = dim(N(T)) + dim(N(U)).
Proof of Theorem 2.32. The proof is by mathematical induction on the
order of the diÔ¨Äerential operator p(D). The Ô¨Årst-order case coincides with
Theorem 2.30. For some integer n > 1, suppose that Theorem 2.32 holds
for any diÔ¨Äerential operator of order less than n, and consider a diÔ¨Äerential

Sec. 2.7
Homogeneous Linear DiÔ¨Äerential Equations with Constant CoeÔ¨Écients 137
operator p(D) of order n. The polynomial p(t) can be factored into a product
of two polynomials as follows:
p(t) = q(t)(t ‚àíc),
where q(t) is a polynomial of degree n ‚àí1 and c is a complex number. Thus
the given diÔ¨Äerential operator may be rewritten as
p(D) = q(D)(D ‚àícI).
Now, by Lemma 1, D ‚àícI is onto, and by the corollary to Theorem 2.30,
dim(N(D ‚àícI)) = 1. Also, by the induction hypothesis, dim(N(q(D)) = n ‚àí1.
Thus, by Lemma 2, we conclude that
dim(N(p(D))) = dim(N(q(D))) + dim(N(D ‚àícI))
= (n ‚àí1) + 1 = n.
Corollary. The solution space of any nth-order homogeneous linear dif-
ferential equation with constant coeÔ¨Écients is an n-dimensional subspace of
C‚àû.
The corollary to Theorem 2.32 reduces the problem of Ô¨Ånding all solutions
to an nth-order homogeneous linear diÔ¨Äerential equation with constant coeÔ¨É-
cients to Ô¨Ånding a set of n linearly independent solutions to the equation. By
the results of Chapter 1, any such set must be a basis for the solution space.
The next theorem enables us to Ô¨Ånd a basis quickly for many such equations.
Hints for its proof are provided in the exercises.
Theorem 2.33. Given n distinct complex numbers c1, c2, . . . , cn, the set
of exponential functions {ec1t, ec2t, . . . , ecnt} is linearly independent.
Proof. Exercise. (See Exercise 10.)
Corollary. For any nth-order homogeneous linear diÔ¨Äerential equation
with constant coeÔ¨Écients, if the auxiliary polynomial has n distinct zeros
c1, c2, . . . , cn, then {ec1t, ec2t, . . . , ecnt} is a basis for the solution space of the
diÔ¨Äerential equation.
Proof. Exercise. (See Exercise 10.)
Example 5
We Ô¨Ånd all solutions to the diÔ¨Äerential equation
y‚Ä≤‚Ä≤ + 5y‚Ä≤ + 4y = 0.

138
Chap. 2
Linear Transformations and Matrices
Since the auxiliary polynomial factors as (t + 4)(t + 1), it has two distinct
zeros, ‚àí1 and ‚àí4. Thus {e‚àít, e‚àí4t} is a basis for the solution space. So any
solution to the given equation is of the form
y(t) = b1e‚àít + b2e‚àí4t
for unique scalars b1 and b2.
‚ô¶
Example 6
We Ô¨Ånd all solutions to the diÔ¨Äerential equation
y‚Ä≤‚Ä≤ + 9y = 0.
The auxiliary polynomial t2 + 9 factors as (t ‚àí3i)(t + 3i) and hence has
distinct zeros c1 = 3i and c2 = ‚àí3i. Thus {e3it, e‚àí3it} is a basis for the
solution space. Since
cos 3t = 1
2(e3it + e‚àí3it)
and
sin 3t = 1
2i(e3it ‚àíe‚àí3it),
it follows from Exercise 7 that {cos 3t, sin 3t} is also a basis for this solution
space. This basis has an advantage over the original one because it consists of
the familiar sine and cosine functions and makes no reference to the imaginary
number i.
Using this latter basis, we see that any solution to the given
equation is of the form
y(t) = b1 cos 3t + b2 sin 3t
for unique scalars b1and b2.
‚ô¶
Next consider the diÔ¨Äerential equation
y‚Ä≤‚Ä≤ + 2y‚Ä≤ + y = 0,
for which the auxiliary polynomial is (t + 1)2. By Theorem 2.31, e‚àít is a
solution to this equation.
By the corollary to Theorem 2.32, its solution
space is two-dimensional. In order to obtain a basis for the solution space,
we need a solution that is linearly independent of e‚àít. The reader can verify
that te‚àít is a such a solution. The following lemma extends this result.
Lemma. For a given complex number c and positive integer n, suppose
that (t ‚àíc)n is the auxiliary polynomial of a homogeneous linear diÔ¨Äerential
equation with constant coeÔ¨Écients. Then the set
Œ≤ = {ect, tect, . . . , tn‚àí1ect}
is a basis for the solution space of the equation.

Sec. 2.7
Homogeneous Linear DiÔ¨Äerential Equations with Constant CoeÔ¨Écients 139
Proof. Since the solution space is n-dimensional, we need only show that
Œ≤ is linearly independent and lies in the solution space. First, observe that
for any positive integer k,
(D ‚àícI)(tkect) = ktk‚àí1ect + ctkect ‚àíctkect
= ktk‚àí1ect.
Hence for k < n,
(D ‚àícI)n(tkect) = 0.
It follows that Œ≤ is a subset of the solution space.
We next show that Œ≤ is linearly independent. Consider any linear combi-
nation of vectors in Œ≤ such that
b0ect + b1tect + ¬∑ ¬∑ ¬∑ + bn‚àí1tn‚àí1ect = 0
(7)
for some scalars b0, b1, . . . , bn‚àí1. Dividing by ect in (7), we obtain
b0 + b1t + ¬∑ ¬∑ ¬∑ + bn‚àí1tn‚àí1 = 0.
(8)
Thus the left side of (8) must be the zero polynomial function. We conclude
that the coeÔ¨Écients b0, b1, . . . , bn‚àí1 are all zero. So Œ≤ is linearly independent
and hence is a basis for the solution space.
Example 7
We Ô¨Ånd all solutions to the diÔ¨Äerential equation
y(4) ‚àí4y(3) + 6y(2) ‚àí4y(1) + y = 0.
Since the auxiliary polynomial is
t4 ‚àí4t3 + 6t2 ‚àí4t + 1 = (t ‚àí1)4,
we can immediately conclude by the preceding lemma that {et, tet, t2et, t3et}
is a basis for the solution space. So any solution y to the given diÔ¨Äerential
equation is of the form
y(t) = b1et + b2tet + b3t2et + b4t3et
for unique scalars b1, b2, b3, and b4.
‚ô¶
The most general situation is stated in the following theorem.
Theorem 2.34. Given a homogeneous linear diÔ¨Äerential equation with
constant coeÔ¨Écients and auxiliary polynomial
(t ‚àíc1)n1(t ‚àíc2)n2 ¬∑ ¬∑ ¬∑ (t ‚àíck)nk,
where n1, n2, . . . , nk are positive integers and c1, c2, . . . , ck are distinct com-
plex numbers, the following set is a basis for the solution space of the equation:
{ec1t, tec1t, . . . , tn1‚àí1ec1t, . . . , eckt, teckt, . . . , tnk‚àí1eckt}.

140
Chap. 2
Linear Transformations and Matrices
Proof. Exercise.
Example 8
The diÔ¨Äerential equation
y(3) ‚àí4y(2) + 5y(1) ‚àí2y = 0
has the auxiliary polynomial
t3 ‚àí4t2 + 5t ‚àí2 = (t ‚àí1)2(t ‚àí2).
By Theorem 2.34, {et, tet, e2t} is a basis for the solution space of the diÔ¨Äer-
ential equation. Thus any solution y has the form
y(t) = b1et + b2tet + b3e2t
for unique scalars b1, b2, and b3.
‚ô¶
EXERCISES
1.
Label the following statements as true or false.
(a)
The set of solutions to an nth-order homogeneous linear diÔ¨Äerential
equation with constant coeÔ¨Écients is an n-dimensional subspace of
C‚àû.
(b)
The solution space of a homogeneous linear diÔ¨Äerential equation
with constant coeÔ¨Écients is the null space of a diÔ¨Äerential operator.
(c)
The auxiliary polynomial of a homogeneous linear diÔ¨Äerential
equation with constant coeÔ¨Écients is a solution to the diÔ¨Äerential
equation.
(d)
Any solution to a homogeneous linear diÔ¨Äerential equation with
constant coeÔ¨Écients is of the form aect or atkect, where a and c
are complex numbers and k is a positive integer.
(e)
Any linear combination of solutions to a given homogeneous linear
diÔ¨Äerential equation with constant coeÔ¨Écients is also a solution to
the given equation.
(f)
For any homogeneous linear diÔ¨Äerential equation with constant
coeÔ¨Écients having auxiliary polynomial p(t), if c1, c2, . . . , ck are
the distinct zeros of p(t), then {ec1t, ec2t, . . . , eckt} is a basis for
the solution space of the given diÔ¨Äerential equation.
(g)
Given any polynomial p(t) ‚ààP(C), there exists a homogeneous lin-
ear diÔ¨Äerential equation with constant coeÔ¨Écients whose auxiliary
polynomial is p(t).

Sec. 2.7
Homogeneous Linear DiÔ¨Äerential Equations with Constant CoeÔ¨Écients 141
2.
For each of the following parts, determine whether the statement is true
or false. Justify your claim with either a proof or a counterexample,
whichever is appropriate.
(a)
Any Ô¨Ånite-dimensional subspace of C‚àûis the solution space of a
homogeneous linear diÔ¨Äerential equation with constant coeÔ¨Écients.
(b)
There exists a homogeneous linear diÔ¨Äerential equation with con-
stant coeÔ¨Écients whose solution space has the basis {t, t2}.
(c)
For any homogeneous linear diÔ¨Äerential equation with constant
coeÔ¨Écients, if x is a solution to the equation, so is its derivative
x‚Ä≤.
Given two polynomials p(t) and q(t) in P(C), if x ‚ààN(p(D)) and y ‚àà
N(q(D)), then
(d)
x + y ‚ààN(p(D)q(D)).
(e)
xy ‚ààN(p(D)q(D)).
3.
Find a basis for the solution space of each of the following diÔ¨Äerential
equations.
(a)
y‚Ä≤‚Ä≤ + 2y‚Ä≤ + y = 0
(b)
y‚Ä≤‚Ä≤‚Ä≤ = y‚Ä≤
(c)
y(4) ‚àí2y(2) + y = 0
(d)
y‚Ä≤‚Ä≤ + 2y‚Ä≤ + y = 0
(e)
y(3) ‚àíy(2) + 3y(1) + 5y = 0
4.
Find a basis for each of the following subspaces of C‚àû.
(a)
N(D2 ‚àíD ‚àíI)
(b)
N(D3 ‚àí3D2 + 3D ‚àíI)
(c)
N(D3 + 6D2 + 8D)
5.
Show that C‚àûis a subspace of F(R, C).
6. (a)
Show that D: C‚àû‚ÜíC‚àûis a linear operator.
(b)
Show that any diÔ¨Äerential operator is a linear operator on C‚àû.
7.
Prove that if {x, y} is a basis for a vector space over C, then so is
1
2(x + y), 1
2i(x ‚àíy)

.
8.
Consider a second-order homogeneous linear diÔ¨Äerential equation with
constant coeÔ¨Écients in which the auxiliary polynomial has distinct con-
jugate complex roots a + ib and a ‚àíib, where a, b ‚ààR. Show that
{eat cos bt, eat sin bt} is a basis for the solution space.

142
Chap. 2
Linear Transformations and Matrices
9.
Suppose that {U1, U2, . . . , Un} is a collection of pairwise commutative
linear operators on a vector space V (i.e., operators such that UiUj =
UjUi for all i, j). Prove that, for any i (1 ‚â§i ‚â§n),
N(Ui) ‚äÜN(U1U2 ¬∑ ¬∑ ¬∑ Un).
10.
Prove Theorem 2.33 and its corollary. Hint: Suppose that
b1ec1t + b2ec2t + ¬∑ ¬∑ ¬∑ + bnecnt = 0
(where the ci‚Äôs are distinct).
To show the bi‚Äôs are zero, apply mathematical induction on n as follows.
Verify the theorem for n = 1. Assuming that the theorem is true for
n ‚àí1 functions, apply the operator D ‚àícnI to both sides of the given
equation to establish the theorem for n distinct exponential functions.
11.
Prove Theorem 2.34. Hint: First verify that the alleged basis lies in
the solution space. Then verify that this set is linearly independent by
mathematical induction on k as follows. The case k = 1 is the lemma
to Theorem 2.34. Assuming that the theorem holds for k ‚àí1 distinct
ci‚Äôs, apply the operator (D ‚àíckI)nk to any linear combination of the
alleged basis that equals 0.
12.
Let V be the solution space of an nth-order homogeneous linear diÔ¨Äer-
ential equation with constant coeÔ¨Écients having auxiliary polynomial
p(t). Prove that if p(t) = g(t)h(t), where g(t) and h(t) are polynomials
of positive degree, then
N(h(D)) = R(g(DV)) = g(D)(V),
where DV : V ‚ÜíV is deÔ¨Åned by DV(x) = x‚Ä≤ for x ‚ààV. Hint: First prove
g(D)(V) ‚äÜN(h(D)). Then prove that the two spaces have the same
Ô¨Ånite dimension.
13.
A diÔ¨Äerential equation
y(n) + an‚àí1y(n‚àí1) + ¬∑ ¬∑ ¬∑ + a1y(1) + a0y = x
is called a nonhomogeneous linear diÔ¨Äerential equation with constant
coeÔ¨Écients if the ai‚Äôs are constant and x is a function that is not iden-
tically zero.
(a)
Prove that for any x ‚ààC‚àûthere exists y ‚ààC‚àûsuch that y is
a solution to the diÔ¨Äerential equation.
Hint: Use Lemma 1 to
Theorem 2.32 to show that for any polynomial p(t), the linear
operator p(D): C‚àû‚ÜíC‚àûis onto.

Sec. 2.7
Homogeneous Linear DiÔ¨Äerential Equations with Constant CoeÔ¨Écients 143
(b)
Let V be the solution space for the homogeneous linear equation
y(n) + an‚àí1y(n‚àí1) + ¬∑ ¬∑ ¬∑ + a1y(1) + a0y = 0.
Prove that if z is any solution to the associated nonhomogeneous
linear diÔ¨Äerential equation, then the set of all solutions to the
nonhomogeneous linear diÔ¨Äerential equation is
{z + y: y ‚ààV}.
14.
Given any nth-order homogeneous linear diÔ¨Äerential equation with con-
stant coeÔ¨Écients, prove that, for any solution x and any t0 ‚ààR, if
x(t0) = x‚Ä≤(t0) = ¬∑ ¬∑ ¬∑ = x(n‚àí1)(t0) = 0, then x = 0 (the zero function).
Hint: Use mathematical induction on n as follows. First prove the con-
clusion for the case n = 1. Next suppose that it is true for equations of
order n ‚àí1, and consider an nth-order diÔ¨Äerential equation with aux-
iliary polynomial p(t). Factor p(t) = q(t)(t ‚àíc), and let z = q((D))x.
Show that z(t0) = 0 and z‚Ä≤ ‚àícz = 0 to conclude that z = 0. Now apply
the induction hypothesis.
15.
Let V be the solution space of an nth-order homogeneous linear dif-
ferential equation with constant coeÔ¨Écients. Fix t0 ‚ààR, and deÔ¨Åne a
mapping Œ¶: V ‚ÜíCn by
Œ¶(x) =
‚éõ
‚éú
‚éú
‚éú
‚éù
x(t0)
x‚Ä≤(t0)
...
x(n‚àí1)(t0)
‚éû
‚éü
‚éü
‚éü
‚é†
for each x in V.
(a)
Prove that Œ¶ is linear and its null space is the zero subspace of V.
Deduce that Œ¶ is an isomorphism. Hint: Use Exercise 14.
(b)
Prove the following: For any nth-order homogeneous linear dif-
ferential equation with constant coeÔ¨Écients, any t0 ‚ààR, and any
complex numbers c0, c1, . . . , cn‚àí1 (not necessarily distinct), there
exists exactly one solution, x, to the given diÔ¨Äerential equation
such that x(t0) = c0 and x(k)(t0) = ck for k = 1, 2, . . . n ‚àí1.
16.
Pendular Motion. It is well known that the motion of a pendulum is
approximated by the diÔ¨Äerential equation
Œ∏‚Ä≤‚Ä≤ + g
l Œ∏ = 0,
where Œ∏(t) is the angle in radians that the pendulum makes with a
vertical line at time t (see Figure 2.8), interpreted so that Œ∏ is positive
if the pendulum is to the right and negative if the pendulum is to the

144
Chap. 2
Linear Transformations and Matrices
..................................................................................................................................
....................................................
S
S
S
Sq
.......................................
Œ∏(t)
l
>
Figure 2.8
left of the vertical line as viewed by the reader. Here l is the length
of the pendulum and g is the magnitude of acceleration due to gravity.
The variable t and constants l and g must be in compatible units (e.g.,
t in seconds, l in meters, and g in meters per second per second).
(a)
Express an arbitrary solution to this equation as a linear combi-
nation of two real-valued solutions.
(b)
Find the unique solution to the equation that satisÔ¨Åes the condi-
tions
Œ∏(0) = Œ∏0 > 0
and
Œ∏‚Ä≤(0) = 0.
(The signiÔ¨Åcance of these conditions is that at time t = 0 the
pendulum is released from a position displaced from the vertical
by Œ∏0.)
(c)
Prove that it takes 2œÄ

l/g units of time for the pendulum to make
one circuit back and forth. (This time is called the period of the
pendulum.)
17.
Periodic Motion of a Spring without Damping. Find the general solu-
tion to (3), which describes the periodic motion of a spring, ignoring
frictional forces.
18.
Periodic Motion of a Spring with Damping. The ideal periodic motion
described by solutions to (3) is due to the ignoring of frictional forces.
In reality, however, there is a frictional force acting on the motion that
is proportional to the speed of motion, but that acts in the opposite
direction. The modiÔ¨Åcation of (3) to account for the frictional force,
called the damping force, is given by
my‚Ä≤‚Ä≤ + ry‚Ä≤ + ky = 0,
where r > 0 is the proportionality constant.
(a)
Find the general solution to this equation.

Chap. 2
Index of DeÔ¨Ånitions
145
(b)
Find the unique solution in (a) that satisÔ¨Åes the initial conditions
y(0) = 0 and y‚Ä≤(0) = v0, the initial velocity.
(c)
For y(t) as in (b), show that the amplitude of the oscillation de-
creases to zero; that is, prove that lim
t‚Üí‚àûy(t) = 0.
19.
In our study of diÔ¨Äerential equations, we have regarded solutions as
complex-valued functions even though functions that are useful in de-
scribing physical motion are real-valued. Justify this approach.
20.
The following parts, which do not involve linear algebra, are included
for the sake of completeness.
(a)
Prove Theorem 2.27. Hint: Use mathematical induction on the
number of derivatives possessed by a solution.
(b)
For any c, d ‚ààC, prove that
ec+d = cced
and
e‚àíc = 1
ec .
(c)
Prove Theorem 2.28.
(d)
Prove Theorem 2.29.
(e)
Prove the product rule for diÔ¨Äerentiating complex-valued func-
tions of a real variable: For any diÔ¨Äerentiable functions x and
y in F(R, C), the product xy is diÔ¨Äerentiable and
(xy)‚Ä≤ = x‚Ä≤y + xy‚Ä≤.
Hint: Apply the rules of diÔ¨Äerentiation to the real and imaginary
parts of xy.
(f)
Prove that if x ‚ààF(R, C) and x‚Ä≤ = 0, then x is a constant func-
tion.
INDEX OF DEFINITIONS FOR CHAPTER 2
Auxiliary polynomial
131
Change of coordinate matrix
112
Clique
94
CoeÔ¨Écients of a diÔ¨Äerential equation
128
Coordinate function
119
Coordinate vector relative to a basis
80
DiÔ¨Äerential equation
128
DiÔ¨Äerential operator
131
Dimension theorem
69
Dominance relation
95
Double dual
120
Dual basis
120
Dual space
119
Euler‚Äôs formula
132
Exponential function
133
Fourier coeÔ¨Écient
119
Homogeneous linear diÔ¨Äerential
equation
128
Identity matrix
89
Identity transformation
67

146
Chap. 2
Linear Transformations and Matrices
Incidence matrix
94
Inverse of a linear transformation
99
Inverse of a matrix
100
Invertible linear transformation
99
Invertible matrix
100
Isomorphic vector spaces
102
Isomorphism
102
Kronecker delta
89
Left-multiplication transformation
92
Linear functional
119
Linear operator
112
Linear transformation
65
Matrix representing a linear trans-
formation
80
Nonhomogeneous diÔ¨Äerential equa-
tion
142
Nullity of a linear transformation
69
Null space
67
Ordered basis
79
Order of a diÔ¨Äerential equation
129
Order of a diÔ¨Äerential operator
131
Product of matrices
87
Projection on a subspace
76
Projection on the x-axis
66
Range
67
Rank of a linear transformation
69
ReÔ¨Çection about the x-axis
66
Rotation
66
Similar matrices
115
Solution to a diÔ¨Äerential equation
129
Solution space of a homogeneous dif-
ferential equation
132
Standard ordered basis for Fn
79
Standard ordered basis for Pn(F)
79
Standard representation of a vector
space with respect to a basis
104
Transpose of a linear transformation
121
Zero transformation
67

3
Elementary Matrix
Operations and Systems
of Linear Equations
3.1
Elementary Matrix Operations and Elementary Matrices
3.2
The Rank of a Matrix and Matrix Inverses
3.3
Systems of Linear Equations‚ÄîTheoretical Aspects
3.4
Systems of Linear Equations‚ÄîComputational Aspects
This chapter is devoted to two related objectives:
1. the study of certain ‚Äúrank-preserving‚Äù operations on matrices;
2. the application of these operations and the theory of linear transforma-
tions to the solution of systems of linear equations.
As a consequence of objective 1, we obtain a simple method for com-
puting the rank of a linear transformation between Ô¨Ånite-dimensional vector
spaces by applying these rank-preserving matrix operations to a matrix that
represents that transformation.
Solving a system of linear equations is probably the most important ap-
plication of linear algebra. The familiar method of elimination for solving
systems of linear equations, which was discussed in Section 1.4, involves the
elimination of variables so that a simpler system can be obtained. The tech-
nique by which the variables are eliminated utilizes three types of operations:
1. interchanging any two equations in the system;
2. multiplying any equation in the system by a nonzero constant;
3. adding a multiple of one equation to another.
In Section 3.3, we express a system of linear equations as a single matrix
equation. In this representation of the system, the three operations above
are the ‚Äúelementary row operations‚Äù for matrices. These operations provide
a convenient computational method for determining all solutions to a system
of linear equations.
147

148
Chap. 3
Elementary Matrix Operations and Systems of Linear Equations
3.1
ELEMENTARY MATRIX OPERATIONS AND ELEMENTARY
MATRICES
In this section, we deÔ¨Åne the elementary operations that are used throughout
the chapter. In subsequent sections, we use these operations to obtain simple
computational methods for determining the rank of a linear transformation
and the solution of a system of linear equations. There are two types of el-
ementary matrix operations‚Äîrow operations and column operations. As we
will see, the row operations are more useful. They arise from the three opera-
tions that can be used to eliminate variables in a system of linear equations.
DeÔ¨Ånitions.
Let A be an m √ó n matrix.
Any one of the following
three operations on the rows [columns] of A is called an elementary row
[column] operation:
(1) interchanging any two rows [columns] of A;
(2) multiplying any row [column] of A by a nonzero scalar;
(3) adding any scalar multiple of a row [column] of A to another row [col-
umn].
Any of these three operations is called an elementary operation. Elemen-
tary operations are of type 1, type 2, or type 3 depending on whether they
are obtained by (1), (2), or (3).
Example 1
Let
A =
‚éõ
‚éù
1
2
3
4
2
1
‚àí1
3
4
0
1
2
‚éû
‚é†.
Interchanging the second row of A with the Ô¨Årst row is an example of an
elementary row operation of type 1. The resulting matrix is
B =
‚éõ
‚éù
2
1
‚àí1
3
1
2
3
4
4
0
1
2
‚éû
‚é†.
Multiplying the second column of A by 3 is an example of an elementary
column operation of type 2. The resulting matrix is
C =
‚éõ
‚éù
1
6
3
4
2
3
‚àí1
3
4
0
1
2
‚éû
‚é†.

Sec. 3.1
Elementary Matrix Operations and Elementary Matrices
149
Adding 4 times the third row of A to the Ô¨Årst row is an example of an
elementary row operation of type 3. In this case, the resulting matrix is
M =
‚éõ
‚éù
17
2
7
12
2
1
‚àí1
3
4
0
1
2
‚éû
‚é†.
‚ô¶
Notice that if a matrix Q can be obtained from a matrix P by means of an
elementary row operation, then P can be obtained from Q by an elementary
row operation of the same type. (See Exercise 8.) So, in Example 1, A can
be obtained from M by adding ‚àí4 times the third row of M to the Ô¨Årst row
of M.
DeÔ¨Ånition.
An n √ó n elementary matrix is a matrix obtained by
performing an elementary operation on In. The elementary matrix is said
to be of type 1, 2, or 3 according to whether the elementary operation
performed on In is a type 1, 2, or 3 operation, respectively.
For example, interchanging the Ô¨Årst two rows of I3 produces the elemen-
tary matrix
E =
‚éõ
‚éù
0
1
0
1
0
0
0
0
1
‚éû
‚é†.
Note that E can also be obtained by interchanging the Ô¨Årst two columns of
I3. In fact, any elementary matrix can be obtained in at least two ways‚Äî
either by performing an elementary row operation on In or by performing an
elementary column operation on In. (See Exercise 4.) Similarly,
‚éõ
‚éù
1
0
‚àí2
0
1
0
0
0
1
‚éû
‚é†
is an elementary matrix since it can be obtained from I3 by an elementary
column operation of type 3 (adding ‚àí2 times the Ô¨Årst column of I3 to the
third column) or by an elementary row operation of type 3 (adding ‚àí2 times
the third row to the Ô¨Årst row).
Our Ô¨Årst theorem shows that performing an elementary row operation on
a matrix is equivalent to multiplying the matrix by an elementary matrix.
Theorem 3.1. Let A ‚ààMm√ón(F), and suppose that B is obtained from
A by performing an elementary row [column] operation. Then there exists an
m √ó m [n √ó n] elementary matrix E such that B = EA [B = AE]. In fact,
E is obtained from Im [In] by performing the same elementary row [column]
operation as that which was performed on A to obtain B. Conversely, if E is

150
Chap. 3
Elementary Matrix Operations and Systems of Linear Equations
an elementary m √ó m [n √ó n] matrix, then EA [AE] is the matrix obtained
from A by performing the same elementary row [column] operation as that
which produces E from Im [In].
The proof, which we omit, requires verifying Theorem 3.1 for each type
of elementary row operation. The proof for column operations can then be
obtained by using the matrix transpose to transform a column operation into
a row operation. The details are left as an exercise. (See Exercise 7.)
The next example illustrates the use of the theorem.
Example 2
Consider the matrices A and B in Example 1. In this case, B is obtained from
A by interchanging the Ô¨Årst two rows of A. Performing this same operation
on I3, we obtain the elementary matrix
E =
‚éõ
‚éù
0
1
0
1
0
0
0
0
1
‚éû
‚é†.
Note that EA = B.
In the second part of Example 1, C is obtained from A by multiplying the
second column of A by 3. Performing this same operation on I4, we obtain
the elementary matrix
E =
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
0
3
0
0
0
0
1
0
0
0
0
1
‚éû
‚éü
‚éü
‚é†.
Observe that AE = C.
‚ô¶
It is a useful fact that the inverse of an elementary matrix is also an
elementary matrix.
Theorem 3.2. Elementary matrices are invertible, and the inverse of an
elementary matrix is an elementary matrix of the same type.
Proof. Let E be an elementary n √ó n matrix. Then E can be obtained by
an elementary row operation on In. By reversing the steps used to transform
In into E, we can transform E back into In.
The result is that In can
be obtained from E by an elementary row operation of the same type. By
Theorem 3.1, there is an elementary matrix E such that EE = In. Therefore,
by Exercise 10 of Section 2.4, E is invertible and E‚àí1 = E.

Sec. 3.1
Elementary Matrix Operations and Elementary Matrices
151
EXERCISES
1.
Label the following statements as true or false.
(a)
An elementary matrix is always square.
(b)
The only entries of an elementary matrix are zeros and ones.
(c)
The n √ó n identity matrix is an elementary matrix.
(d)
The product of two n √ó n elementary matrices is an elementary
matrix.
(e)
The inverse of an elementary matrix is an elementary matrix.
(f)
The sum of two n√ón elementary matrices is an elementary matrix.
(g)
The transpose of an elementary matrix is an elementary matrix.
(h)
If B is a matrix that can be obtained by performing an elementary
row operation on a matrix A, then B can also be obtained by
performing an elementary column operation on A.
(i)
If B is a matrix that can be obtained by performing an elemen-
tary row operation on a matrix A, then A can be obtained by
performing an elementary row operation on B.
2.
Let
A =
‚éõ
‚éù
1
2
3
1
0
1
1
‚àí1
1
‚éû
‚é†, B =
‚éõ
‚éù
1
0
3
1
‚àí2
1
1
‚àí3
1
‚éû
‚é†, and C =
‚éõ
‚éù
1
0
3
0
‚àí2
‚àí2
1
‚àí3
1
‚éû
‚é†.
Find an elementary operation that transforms A into B and an elemen-
tary operation that transforms B into C. By means of several additional
operations, transform C into I3.
3.
Use the proof of Theorem 3.2 to obtain the inverse of each of the fol-
lowing elementary matrices.
(a)
‚éõ
‚éù
0
0
1
0
1
0
1
0
0
‚éû
‚é†
(b)
‚éõ
‚éù
1
0
0
0
3
0
0
0
1
‚éû
‚é†
(c)
‚éõ
‚éù
1
0
0
0
1
0
‚àí2
0
1
‚éû
‚é†
4.
Prove the assertion made on page 149: Any elementary n√ón matrix can
be obtained in at least two ways‚Äîeither by performing an elementary
row operation on In or by performing an elementary column operation
on In.
5.
Prove that E is an elementary matrix if and only if Et is.
6.
Let A be an m √ó n matrix. Prove that if B can be obtained from A by
an elementary row [column] operation, then Bt can be obtained from
At by the corresponding elementary column [row] operation.
7.
Prove Theorem 3.1.

152
Chap. 3
Elementary Matrix Operations and Systems of Linear Equations
8.
Prove that if a matrix Q can be obtained from a matrix P by an elemen-
tary row operation, then P can be obtained from Q by an elementary
matrix of the same type.
Hint: Treat each type of elementary row
operation separately.
9.
Prove that any elementary row [column] operation of type 1 can be
obtained by a succession of three elementary row [column] operations
of type 3 followed by one elementary row [column] operation of type 2.
10.
Prove that any elementary row [column] operation of type 2 can be
obtained by dividing some row [column] by a nonzero scalar.
11.
Prove that any elementary row [column] operation of type 3 can be
obtained by subtracting a multiple of some row [column] from another
row [column].
12.
Let A be an m √ó n matrix.
Prove that there exists a sequence of
elementary row operations of types 1 and 3 that transforms A into an
upper triangular matrix.
3.2
THE RANK OF A MATRIX AND MATRIX INVERSES
In this section, we deÔ¨Åne the rank of a matrix.
We then use elementary
operations to compute the rank of a matrix and a linear transformation. The
section concludes with a procedure for computing the inverse of an invertible
matrix.
DeÔ¨Ånition. If A ‚ààMm√ón(F), we deÔ¨Åne the rank of A, denoted rank(A),
to be the rank of the linear transformation LA : Fn ‚ÜíFm.
Many results about the rank of a matrix follow immediately from the
corresponding facts about a linear transformation. An important result of
this type, which follows from Fact 3 (p. 100) and Corollary 2 to Theorem 2.18
(p. 102), is that an n √ó n matrix is invertible if and only if its rank is n.
Every matrix A is the matrix representation of the linear transformation
LA with respect to the appropriate standard ordered bases. Thus the rank
of the linear transformation LA is the same as the rank of one of its matrix
representations, namely, A. The next theorem extends this fact to any ma-
trix representation of any linear transformation deÔ¨Åned on Ô¨Ånite-dimensional
vector spaces.
Theorem 3.3. Let T: V ‚ÜíW be a linear transformation between Ô¨Ånite-
dimensional vector spaces, and let Œ≤ and Œ≥ be ordered bases for V and W,
respectively. Then rank(T) = rank([T]Œ≥
Œ≤).
Proof. This is a restatement of Exercise 20 of Section 2.4.

Sec. 3.2
The Rank of a Matrix and Matrix Inverses
153
Now that the problem of Ô¨Ånding the rank of a linear transformation has
been reduced to the problem of Ô¨Ånding the rank of a matrix, we need a result
that allows us to perform rank-preserving operations on matrices. The next
theorem and its corollary tell us how to do this.
Theorem 3.4. Let A be an m √ó n matrix. If P and Q are invertible
m √ó m and n √ó n matrices, respectively, then
(a) rank(AQ) = rank(A),
(b) rank(PA) = rank(A),
and therefore,
(c) rank(PAQ) = rank(A).
Proof. First observe that
R(LAQ) = R(LALQ) = LALQ(Fn) = LA(LQ(Fn)) = LA(Fn) = R(LA)
since LQ is onto. Therefore
rank(AQ) = dim(R(LAQ)) = dim(R(LA)) = rank(A).
This establishes (a). To establish (b), apply Exercise 17 of Section 2.4 to
T = LP . We omit the details. Finally, applying (a) and (b), we have
rank(PAQ) = rank(PA) = rank(A).
Corollary. Elementary row and column operations on a matrix are rank-
preserving.
Proof. If B is obtained from a matrix A by an elementary row operation,
then there exists an elementary matrix E such that B = EA. By Theorem 3.2
(p. 150), E is invertible, and hence rank(B) = rank(A) by Theorem 3.4. The
proof that elementary column operations are rank-preserving is left as an
exercise.
Now that we have a class of matrix operations that preserve rank, we
need a way of examining a transformed matrix to ascertain its rank. The
next theorem is the Ô¨Årst of several in this direction.
Theorem 3.5. The rank of any matrix equals the maximum number of its
linearly independent columns; that is, the rank of a matrix is the dimension
of the subspace generated by its columns.
Proof. For any A ‚ààMm√ón(F),
rank(A) = rank(LA) = dim(R(LA)).

154
Chap. 3
Elementary Matrix Operations and Systems of Linear Equations
Let Œ≤ be the standard ordered basis for Fn. Then Œ≤ spans Fn and hence, by
Theorem 2.2 (p. 68),
R(LA) = span(LA(Œ≤)) = span ({LA(e1), LA(e2), . . . , LA(en)}) .
But, for any j, we have seen in Theorem 2.13(b) (p. 90) that LA(ej) = Aej =
aj, where aj the jth column of A. Hence
R(LA) = span ({a1, a2, . . . , an}) .
Thus
rank(A) = dim(R(LA)) = dim(span ({a1, a2, . . . , an})).
Example 1
Let
A =
‚éõ
‚éù
1
0
1
0
1
1
1
0
1
‚éû
‚é†.
Observe that the Ô¨Årst and second columns of A are linearly independent and
that the third column is a linear combination of the Ô¨Årst two. Thus
rank(A) = dim
‚éõ
‚éùspan
‚éõ
‚éù
‚éß
‚é®
‚é©
‚éõ
‚éù
1
0
1
‚éû
‚é†,
‚éõ
‚éù
0
1
0
‚éû
‚é†,
‚éõ
‚éù
1
1
1
‚éû
‚é†
‚é´
‚é¨
‚é≠
‚éû
‚é†
‚éû
‚é†= 2.
‚ô¶
To compute the rank of a matrix A, it is frequently useful to postpone the
use of Theorem 3.5 until A has been suitably modiÔ¨Åed by means of appro-
priate elementary row and column operations so that the number of linearly
independent columns is obvious. The corollary to Theorem 3.4 guarantees
that the rank of the modiÔ¨Åed matrix is the same as the rank of A.
One
such modiÔ¨Åcation of A can be obtained by using elementary row and col-
umn operations to introduce zero entries. The next example illustrates this
procedure.
Example 2
Let
A =
‚éõ
‚éù
1
2
1
1
0
3
1
1
2
‚éû
‚é†.
If we subtract the Ô¨Årst row of A from rows 2 and 3 (type 3 elementary row
operations), the result is
‚éõ
‚éù
1
2
1
0
‚àí2
2
0
‚àí1
1
‚éû
‚é†.

Sec. 3.2
The Rank of a Matrix and Matrix Inverses
155
If we now subtract twice the Ô¨Årst column from the second and subtract the
Ô¨Årst column from the third (type 3 elementary column operations), we obtain
‚éõ
‚éù
1
0
0
0
‚àí2
2
0
‚àí1
1
‚éû
‚é†.
It is now obvious that the maximum number of linearly independent columns
of this matrix is 2. Hence the rank of A is 2.
‚ô¶
The next theorem uses this process to transform a matrix into a particu-
larly simple form. The power of this theorem can be seen in its corollaries.
Theorem 3.6. Let A be an m √ó n matrix of rank r. Then r ‚â§m, r ‚â§n,
and, by means of a Ô¨Ånite number of elementary row and column operations,
A can be transformed into the matrix
D =

Ir
O1
O2
O3
	
,
where O1, O2, and O3 are zero matrices. Thus Dii = 1 for i ‚â§r and Dij = 0
otherwise.
Theorem 3.6 and its corollaries are quite important. Its proof, though
easy to understand, is tedious to read. As an aid in following the proof, we
Ô¨Årst consider an example.
Example 3
Consider the matrix
A =
‚éõ
‚éú
‚éú
‚éù
0
2
4
2
2
4
4
4
8
0
8
2
0
10
2
6
3
2
9
1
‚éû
‚éü
‚éü
‚é†.
By means of a succession of elementary row and column operations, we can
transform A into a matrix D as in Theorem 3.6. We list many of the inter-
mediate matrices, but on several occasions a matrix is transformed from the
preceding one by means of several elementary operations. The number above
each arrow indicates how many elementary operations are involved. Try to
identify the nature of each elementary operation (row or column and type)
in the following matrix transformations.
‚éõ
‚éú
‚éú
‚éù
0
2
4
2
2
4
4
4
8
0
8
2
0
10
2
6
3
2
9
1
‚éû
‚éü
‚éü
‚é†
1
‚àí‚Üí
‚éõ
‚éú
‚éú
‚éù
4
4
4
8
0
0
2
4
2
2
8
2
0
10
2
6
3
2
9
1
‚éû
‚éü
‚éü
‚é†
1
‚àí‚Üí
‚éõ
‚éú
‚éú
‚éù
1
1
1
2
0
0
2
4
2
2
8
2
0
10
2
6
3
2
9
1
‚éû
‚éü
‚éü
‚é†
2
‚àí‚Üí

156
Chap. 3
Elementary Matrix Operations and Systems of Linear Equations
‚éõ
‚éú
‚éú
‚éù
1
1
1
2
0
0
2
4
2
2
0
‚àí6
‚àí8
‚àí6
2
0
‚àí3
‚àí4
‚àí3
1
‚éû
‚éü
‚éü
‚é†
3
‚àí‚Üí
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
0
0
2
4
2
2
0
‚àí6
‚àí8
‚àí6
2
0
‚àí3
‚àí4
‚àí3
1
‚éû
‚éü
‚éü
‚é†
1
‚àí‚Üí
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
0
0
1
2
1
1
0
‚àí6
‚àí8
‚àí6
2
0
‚àí3
‚àí4
‚àí3
1
‚éû
‚éü
‚éü
‚é†
2
‚àí‚Üí
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
0
0
1
2
1
1
0
0
4
0
8
0
0
2
0
4
‚éû
‚éü
‚éü
‚é†
3
‚àí‚Üí
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
0
0
1
0
0
0
0
0
4
0
8
0
0
2
0
4
‚éû
‚éü
‚éü
‚é†
1
‚àí‚Üí
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
0
0
1
0
0
0
0
0
1
0
2
0
0
2
0
4
‚éû
‚éü
‚éü
‚é†
1
‚àí‚Üí
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
0
0
1
0
0
0
0
0
1
0
2
0
0
0
0
0
‚éû
‚éü
‚éü
‚é†
1
‚àí‚Üí
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
0
0
‚éû
‚éü
‚éü
‚é†= D
By the corollary to Theorem 3.4, rank(A) = rank(D).
Clearly, however,
rank(D) = 3; so rank(A) = 3.
‚ô¶
Note that the Ô¨Årst two elementary operations in Example 3 result in a
1 in the 1,1 position, and the next several operations (type 3) result in 0‚Äôs
everywhere in the Ô¨Årst row and Ô¨Årst column except for the 1,1 position. Sub-
sequent elementary operations do not change the Ô¨Årst row and Ô¨Årst column.
With this example in mind, we proceed with the proof of Theorem 3.6.
Proof of Theorem 3.6. If A is the zero matrix, r = 0 by Exercise 3. In
this case, the conclusion follows with D = A.
Now suppose that A Ã∏= O and r = rank(A); then r > 0. The proof is by
mathematical induction on m, the number of rows of A.
Suppose that m = 1. By means of at most one type 1 column operation
and at most one type 2 column operation, A can be transformed into a matrix
with a 1 in the 1,1 position.
By means of at most n ‚àí1 type 3 column
operations, this matrix can in turn be transformed into the matrix

1
0
¬∑ ¬∑ ¬∑
0

.
Note that there is one linearly independent column in D. So rank(D) =
rank(A) = 1 by the corollary to Theorem 3.4 and by Theorem 3.5. Thus the
theorem is established for m = 1.
Next assume that the theorem holds for any matrix with at most m ‚àí1
rows (for some m > 1). We must prove that the theorem holds for any matrix
with m rows.
Suppose that A is any m √ó n matrix.
If n = 1, Theorem 3.6 can be
established in a manner analogous to that for m = 1 (see Exercise 10).
We now suppose that n > 1. Since A Ã∏= O, Aij Ã∏= 0 for some i, j. By
means of at most one elementary row and at most one elementary column

Sec. 3.2
The Rank of a Matrix and Matrix Inverses
157
operation (each of type 1), we can move the nonzero entry to the 1,1 position
(just as was done in Example 3). By means of at most one additional type 2
operation, we can assure a 1 in the 1,1 position. (Look at the second operation
in Example 3.) By means of at most m‚àí1 type 3 row operations and at most
n ‚àí1 type 3 column operations, we can eliminate all nonzero entries in the
Ô¨Årst row and the Ô¨Årst column with the exception of the 1 in the 1,1 position.
(In Example 3, we used two row and three column operations to do this.)
Thus, with a Ô¨Ånite number of elementary operations, A can be transformed
into a matrix
B =
‚éõ
‚éú
‚éú
‚éú
‚éù
1
0
¬∑ ¬∑ ¬∑
0
0
...
0
B‚Ä≤
‚éû
‚éü
‚éü
‚éü
‚é†,
where B‚Ä≤ is an (m ‚àí1) √ó (n ‚àí1) matrix. In Example 3, for instance,
B‚Ä≤ =
‚éõ
‚éù
2
4
2
2
‚àí6
‚àí8
‚àí6
2
‚àí3
‚àí4
‚àí3
1
‚éû
‚é†.
By Exercise 11, B‚Ä≤ has rank one less than B. Since rank(A) = rank(B) =
r, rank(B‚Ä≤) = r ‚àí1. Therefore r ‚àí1 ‚â§m ‚àí1 and r ‚àí1 ‚â§n ‚àí1 by the
induction hypothesis. Hence r ‚â§m and r ‚â§n.
Also by the induction hypothesis, B‚Ä≤ can be transformed by a Ô¨Ånite num-
ber of elementary row and column operations into the (m‚àí1)√ó(n‚àí1) matrix
D‚Ä≤ such that
D‚Ä≤ =

Ir‚àí1
O4
O5
O6
	
,
where O4, O5, and O6 are zero matrices. That is, D‚Ä≤ consists of all zeros
except for its Ô¨Årst r ‚àí1 diagonal entries, which are ones. Let
D =
‚éõ
‚éú
‚éú
‚éú
‚éù
1
0
¬∑ ¬∑ ¬∑
0
0
...
0
D‚Ä≤
‚éû
‚éü
‚éü
‚éü
‚é†.
We see that the theorem now follows once we show that D can be obtained
from B by means of a Ô¨Ånite number of elementary row and column operations.
However this follows by repeated applications of Exercise 12.
Thus, since A can be transformed into B and B can be transformed into
D, each by a Ô¨Ånite number of elementary operations, A can be transformed
into D by a Ô¨Ånite number of elementary operations.

158
Chap. 3
Elementary Matrix Operations and Systems of Linear Equations
Finally, since D‚Ä≤ contains ones as its Ô¨Årst r‚àí1 diagonal entries, D contains
ones as its Ô¨Årst r diagonal entries and zeros elsewhere. This establishes the
theorem.
Corollary 1.
Let A be an m √ó n matrix of rank r. Then there exist
invertible matrices B and C of sizes m √ó m and n √ó n, respectively, such that
D = BAC, where
D =

Ir
O1
O2
O3
	
is the m √ó n matrix in which O1, O2, and O3 are zero matrices.
Proof. By Theorem 3.6, A can be transformed by means of a Ô¨Ånite number
of elementary row and column operations into the matrix D. We can appeal
to Theorem 3.1 (p. 149) each time we perform an elementary operation. Thus
there exist elementary m √ó m matrices E1, E2, . . . , Ep and elementary n √ó n
matrices G1, G2, . . . , Gq such that
D = EpEp‚àí1 ¬∑ ¬∑ ¬∑ E2E1AG1G2 ¬∑ ¬∑ ¬∑ Gq.
By Theorem 3.2 (p. 150), each Ej and Gj is invertible. Let B = EpEp‚àí1 ¬∑ ¬∑ ¬∑ E1
and C = G1G2 ¬∑ ¬∑ ¬∑ Gq. Then B and C are invertible by Exercise 4 of Sec-
tion 2.4, and D = BAC.
Corollary 2. Let A be an m √ó n matrix. Then
(a) rank(At) = rank(A).
(b) The rank of any matrix equals the maximum number of its linearly
independent rows; that is, the rank of a matrix is the dimension of the
subspace generated by its rows.
(c) The rows and columns of any matrix generate subspaces of the same
dimension, numerically equal to the rank of the matrix.
Proof. (a) By Corollary 1, there exist invertible matrices B and C such
that D = BAC, where D satisÔ¨Åes the stated conditions of the corollary.
Taking transposes, we have
Dt = (BAC)t = CtAtBt.
Since B and C are invertible, so are Bt and Ct by Exercise 5 of Section 2.4.
Hence by Theorem 3.4,
rank(At) = rank(CtAtBt) = rank(Dt).
Suppose that r = rank(A). Then Dt is an n √ó m matrix with the form of the
matrix D in Corollary 1, and hence rank(Dt) = r by Theorem 3.5. Thus
rank(At) = rank(Dt) = r = rank(A).
This establishes (a).
The proofs of (b) and (c) are left as exercises. (See Exercise 13.)

Sec. 3.2
The Rank of a Matrix and Matrix Inverses
159
Corollary 3. Every invertible matrix is a product of elementary matrices.
Proof. If A is an invertible n √ó n matrix, then rank(A) = n. Hence the
matrix D in Corollary 1 equals In, and there exist invertible matrices B and
C such that In = BAC.
As in the proof of Corollary 1, note that B = EpEp‚àí1 ¬∑ ¬∑ ¬∑ E1 and C =
G1G2 ¬∑ ¬∑ ¬∑ Gq, where the Ei‚Äôs and Gi‚Äôs are elementary matrices. Thus A =
B‚àí1InC‚àí1 = B‚àí1C‚àí1, so that
A = E‚àí1
1 E‚àí1
2
¬∑ ¬∑ ¬∑ E‚àí1
p G‚àí1
q G‚àí1
q‚àí1 ¬∑ ¬∑ ¬∑ G‚àí1
1 .
The inverses of elementary matrices are elementary matrices, however, and
hence A is the product of elementary matrices.
We now use Corollary 2 to relate the rank of a matrix product to the rank
of each factor. Notice how the proof exploits the relationship between the
rank of a matrix and the rank of a linear transformation.
Theorem 3.7. Let T: V ‚ÜíW and U: W ‚ÜíZ be linear transformations
on Ô¨Ånite-dimensional vector spaces V, W, and Z, and let A and B be matrices
such that the product AB is deÔ¨Åned. Then
(a) rank(UT) ‚â§rank(U).
(b) rank(UT) ‚â§rank(T).
(c) rank(AB) ‚â§rank(A).
(d) rank(AB) ‚â§rank(B).
Proof. We prove these items in the order: (a), (c), (d), and (b).
(a) Clearly, R(T) ‚äÜW. Hence
R(UT) = UT(V) = U(T(V)) = U(R(T)) ‚äÜU(W) = R(U).
Thus
rank(UT) = dim(R(UT)) ‚â§dim(R(U)) = rank(U).
(c) By (a),
rank(AB) = rank(LAB) = rank(LALB) ‚â§rank(LA) = rank(A).
(d) By (c) and Corollary 2 to Theorem 3.6,
rank(AB) = rank((AB)t) = rank(BtAt) ‚â§rank(Bt) = rank(B).
(b) Let Œ±, Œ≤, and Œ≥ be ordered bases for V, W, and Z, respectively, and
let A‚Ä≤ = [U]Œ≥
Œ≤ and B‚Ä≤ = [T]Œ≤
Œ±. Then A‚Ä≤B‚Ä≤ = [UT]Œ≥
Œ± by Theorem 2.11 (p. 88).
Hence, by Theorem 3.3 and (d),
rank(UT) = rank(A‚Ä≤B‚Ä≤) ‚â§rank(B‚Ä≤) = rank(T).

160
Chap. 3
Elementary Matrix Operations and Systems of Linear Equations
It is important to be able to compute the rank of any matrix. We can
use the corollary to Theorem 3.4, Theorems 3.5 and 3.6, and Corollary 2 to
Theorem 3.6 to accomplish this goal.
The object is to perform elementary row and column operations on a
matrix to ‚Äúsimplify‚Äù it (so that the transformed matrix has many zero entries)
to the point where a simple observation enables us to determine how many
linearly independent rows or columns the matrix has, and thus to determine
its rank.
Example 4
(a) Let
A =

1
2
1
1
1
1
‚àí1
1
	
.
Note that the Ô¨Årst and second rows of A are linearly independent since one
is not a multiple of the other. Thus rank(A) = 2.
(b) Let
A =
‚éõ
‚éù
1
3
1
1
1
0
1
1
0
3
0
0
‚éû
‚é†.
In this case, there are several ways to proceed. Suppose that we begin with
an elementary row operation to obtain a zero in the 2,1 position. Subtracting
the Ô¨Årst row from the second row, we obtain
‚éõ
‚éù
1
3
1
1
0
‚àí3
0
0
0
3
0
0
‚éû
‚é†.
Now note that the third row is a multiple of the second row, and the Ô¨Årst and
second rows are linearly independent. Thus rank(A) = 2.
As an alternative method, note that the Ô¨Årst, third, and fourth columns
of A are identical and that the Ô¨Årst and second columns of A are linearly
independent. Hence rank(A) = 2.
(c) Let
A =
‚éõ
‚éù
1
2
3
1
2
1
1
1
1
‚àí1
1
0
‚éû
‚é†.
Using elementary row operations, we can transform A as follows:
A ‚àí‚Üí
‚éõ
‚éù
1
2
3
1
0
‚àí3
‚àí5
‚àí1
0
‚àí3
‚àí2
‚àí1
‚éû
‚é†‚àí‚Üí
‚éõ
‚éù
1
2
3
1
0
‚àí3
‚àí5
‚àí1
0
0
3
0
‚éû
‚é†.

Sec. 3.2
The Rank of a Matrix and Matrix Inverses
161
It is clear that the last matrix has three linearly independent rows and hence
has rank 3.
‚ô¶
In summary, perform row and column operations until the matrix is sim-
pliÔ¨Åed enough so that the maximum number of linearly independent rows or
columns is obvious.
The Inverse of a Matrix
We have remarked that an n√ón matrix is invertible if and only if its rank
is n. Since we know how to compute the rank of any matrix, we can always
test a matrix to determine whether it is invertible. We now provide a simple
technique for computing the inverse of a matrix that utilizes elementary row
operations.
DeÔ¨Ånition.
Let A and B be m √ó n and m √ó p matrices, respectively.
By the augmented matrix (A|B), we mean the m √ó (n + p) matrix (A B),
that is, the matrix whose Ô¨Årst n columns are the columns of A, and whose
last p columns are the columns of B.
Let A be an invertible n √ó n matrix, and consider the n √ó 2n augmented
matrix C = (A|In). By Exercise 15, we have
A‚àí1C = (A‚àí1A|A‚àí1In) = (In|A‚àí1).
(1)
By Corollary 3 to Theorem 3.6, A‚àí1 is the product of elementary matrices,
say A‚àí1 = EpEp‚àí1 ¬∑ ¬∑ ¬∑ E1. Thus (1) becomes
EpEp‚àí1 ¬∑ ¬∑ ¬∑ E1(A|In) = A‚àí1C = (In|A‚àí1).
Because multiplying a matrix on the left by an elementary matrix transforms
the matrix by an elementary row operation (Theorem 3.1 p. 149), we have
the following result: If A is an invertible n √ó n matrix, then it is possible to
transform the matrix (A|In) into the matrix (In|A‚àí1) by means of a Ô¨Ånite
number of elementary row operations.
Conversely, suppose that A is invertible and that, for some n √ó n matrix
B, the matrix (A|In) can be transformed into the matrix (In|B) by a Ô¨Ånite
number of elementary row operations. Let E1, E2, . . . , Ep be the elementary
matrices associated with these elementary row operations as in Theorem 3.1;
then
EpEp‚àí1 ¬∑ ¬∑ ¬∑ E1(A|In) = (In|B).
(2)
Letting M = EpEp‚àí1 ¬∑ ¬∑ ¬∑ E1, we have from (2) that
(MA|M) = M(A|In) = (In|B).

162
Chap. 3
Elementary Matrix Operations and Systems of Linear Equations
Hence MA = In and M = B. It follows that M = A‚àí1. So B = A‚àí1. Thus
we have the following result: If A is an invertible n√ón matrix, and the matrix
(A|In) is transformed into a matrix of the form (In|B) by means of a Ô¨Ånite
number of elementary row operations, then B = A‚àí1.
If, on the other hand, A is an n √ó n matrix that is not invertible, then
rank(A) < n. Hence any attempt to transform (A|In) into a matrix of the
form (In|B) by means of elementary row operations must fail because oth-
erwise A can be transformed into In using the same row operations. This
is impossible, however, because elementary row operations preserve rank. In
fact, A can be transformed into a matrix with a row containing only zero
entries, yielding the following result: If A is an n √ó n matrix that is not
invertible, then any attempt to transform (A|In) into a matrix of the form
(In|B) produces a row whose Ô¨Årst n entries are zeros.
The next two examples demonstrate these comments.
Example 5
We determine whether the matrix
A =
‚éõ
‚éù
0
2
4
2
4
2
3
3
1
‚éû
‚é†
is invertible, and if it is, we compute its inverse.
We attempt to use elementary row operations to transform
(A|I) =
‚éõ
‚éù
0
2
4
1
0
0
2
4
2
0
1
0
3
3
1
0
0
1
‚éû
‚é†
into a matrix of the form (I|B). One method for accomplishing this transfor-
mation is to change each column of A successively, beginning with the Ô¨Årst
column, into the corresponding column of I. Since we need a nonzero entry
in the 1,1 position, we begin by interchanging rows 1 and 2. The result is
‚éõ
‚éù
2
4
2
0
1
0
0
2
4
1
0
0
3
3
1
0
0
1
‚éû
‚é†.
In order to place a 1 in the 1,1 position, we must multiply the Ô¨Årst row by 1
2;
this operation yields
‚éõ
‚éú
‚éù
1
2
1
0
1
2
0
0
2
4
1
0
0
3
3
1
0
0
1
‚éû
‚éü
‚é†.

Sec. 3.2
The Rank of a Matrix and Matrix Inverses
163
We now complete work in the Ô¨Årst column by adding ‚àí3 times row 1 to row
3 to obtain
‚éõ
‚éú
‚éù
1
2
1
0
1
2
0
0
2
4
1
0
0
0
‚àí3
‚àí2
0
‚àí3
2
1
‚éû
‚éü
‚é†.
In order to change the second column of the preceding matrix into the
second column of I, we multiply row 2 by 1
2 to obtain a 1 in the 2,2 position.
This operation produces
‚éõ
‚éú
‚éù
1
2
1
0
1
2
0
0
1
2
1
2
0
0
0
‚àí3
‚àí2
0
‚àí3
2
1
‚éû
‚éü
‚é†.
We now complete our work on the second column by adding ‚àí2 times row 2
to row 1 and 3 times row 2 to row 3. The result is
‚éõ
‚éú
‚éú
‚éù
1
0
‚àí3
‚àí1
1
2
0
0
1
2
1
2
0
0
0
0
4
3
2
‚àí3
2
1
‚éû
‚éü
‚éü
‚é†.
Only the third column remains to be changed. In order to place a 1 in the
3,3 position, we multiply row 3 by 1
4; this operation yields
‚éõ
‚éú
‚éú
‚éù
1
0
‚àí3
‚àí1
1
2
0
0
1
2
1
2
0
0
0
0
1
3
8
‚àí3
8
1
4
‚éû
‚éü
‚éü
‚é†.
Adding appropriate multiples of row 3 to rows 1 and 2 completes the process
and gives
‚éõ
‚éú
‚éú
‚éù
1
0
0
1
8
‚àí5
8
3
4
0
1
0
‚àí1
4
3
4
‚àí1
2
0
0
1
3
8
‚àí3
8
1
4
‚éû
‚éü
‚éü
‚é†.
Thus A is invertible, and
A‚àí1 =
‚éõ
‚éú
‚éú
‚éù
1
8
‚àí5
8
3
4
‚àí1
4
3
4
‚àí1
2
3
8
‚àí3
8
1
4
‚éû
‚éü
‚éü
‚é†.
‚ô¶

164
Chap. 3
Elementary Matrix Operations and Systems of Linear Equations
Example 6
We determine whether the matrix
A =
‚éõ
‚éù
1
2
1
2
1
‚àí1
1
5
4
‚éû
‚é†
is invertible, and if it is, we compute its inverse. Using a strategy similar to
the one used in Example 5, we attempt to use elementary row operations to
transform
(A|I) =
‚éõ
‚éù
1
2
1
1
0
0
2
1
‚àí1
0
1
0
1
5
4
0
0
1
‚éû
‚é†
into a matrix of the form (I|B). We Ô¨Årst add ‚àí2 times row 1 to row 2 and
‚àí1 times row 1 to row 3. We then add row 2 to row 3. The result,
‚éõ
‚éù
1
2
1
1
0
0
2
1
‚àí1
0
1
0
1
5
4
0
0
1
‚éû
‚é†‚àí‚Üí
‚éõ
‚éù
1
2
1
1
0
0
0
‚àí3
‚àí3
‚àí2
1
0
0
3
3
‚àí1
0
1
‚éû
‚é†
‚àí‚Üí
‚éõ
‚éù
1
2
1
1
0
0
0
‚àí3
‚àí3
‚àí2
1
0
0
0
0
‚àí3
1
1
‚éû
‚é†,
is a matrix with a row whose Ô¨Årst 3 entries are zeros. Therefore A is not
invertible.
‚ô¶
Being able to test for invertibility and compute the inverse of a matrix
allows us, with the help of Theorem 2.18 (p. 101) and its corollaries, to test
for invertibility and compute the inverse of a linear transformation. The next
example demonstrates this technique.
Example 7
Let T: P2(R) ‚ÜíP2(R) be deÔ¨Åned by T(f(x)) = f(x) + f ‚Ä≤(x) + f ‚Ä≤‚Ä≤(x), where
f ‚Ä≤(x) and f ‚Ä≤‚Ä≤(x) denote the Ô¨Årst and second derivatives of f(x).
We use
Corollary 1 of Theorem 2.18 (p. 102) to test T for invertibility and compute
the inverse if T is invertible. Taking Œ≤ to be the standard ordered basis of
P2(R), we have
[T]Œ≤ =
‚éõ
‚éù
1
1
2
0
1
2
0
0
1
‚éû
‚é†.

Sec. 3.2
The Rank of a Matrix and Matrix Inverses
165
Using the method of Examples 5 and 6, we can show that [T]Œ≤ is invertible
with inverse
([T]Œ≤)‚àí1 =
‚éõ
‚éù
1
‚àí1
0
0
1
‚àí2
0
0
1
‚éû
‚é†.
Thus T is invertible, and ([T]Œ≤)‚àí1 = [T‚àí1]Œ≤. Hence by Theorem 2.14 (p. 91),
we have
[T‚àí1(a0 + a1x + a2x2)]Œ≤ =
‚éõ
‚éù
1
‚àí1
0
0
1
‚àí2
0
0
1
‚éû
‚é†
‚éõ
‚éù
a0
a1
a2
‚éû
‚é†
=
‚éõ
‚éù
a0 ‚àía1
a1 ‚àí2a2
a2
‚éû
‚é†.
Therefore
T‚àí1(a0 + a1x + a2x2) = (a0 ‚àía1) + (a1 ‚àí2a2)x + a2x2.
‚ô¶
EXERCISES
1.
Label the following statements as true or false.
(a)
The rank of a matrix is equal to the number of its nonzero columns.
(b)
The product of two matrices always has rank equal to the lesser of
the ranks of the two matrices.
(c)
The m √ó n zero matrix is the only m √ó n matrix having rank 0.
(d)
Elementary row operations preserve rank.
(e)
Elementary column operations do not necessarily preserve rank.
(f)
The rank of a matrix is equal to the maximum number of linearly
independent rows in the matrix.
(g)
The inverse of a matrix can be computed exclusively by means of
elementary row operations.
(h)
The rank of an n √ó n matrix is at most n.
(i)
An n √ó n matrix having rank n is invertible.
2.
Find the rank of the following matrices.
(a)
‚éõ
‚éù
1
1
0
0
1
1
1
1
0
‚éû
‚é†
(b)
‚éõ
‚éù
1
1
0
2
1
1
1
1
1
‚éû
‚é†
(c)

1
0
2
1
1
4
	

166
Chap. 3
Elementary Matrix Operations and Systems of Linear Equations
(d)

1
2
1
2
4
2
	
(e)
‚éõ
‚éú
‚éú
‚éù
1
2
3
1
1
1
4
0
1
2
0
2
‚àí3
0
1
1
0
0
0
0
‚éû
‚éü
‚éü
‚é†
(f)
‚éõ
‚éú
‚éú
‚éù
1
2
0
1
1
2
4
1
3
0
3
6
2
5
1
‚àí4
‚àí8
1
‚àí3
1
‚éû
‚éü
‚éü
‚é†
(g)
‚éõ
‚éú
‚éú
‚éù
1
1
0
1
2
2
0
2
1
1
0
1
1
1
0
1
‚éû
‚éü
‚éü
‚é†
3.
Prove that for any m √ó n matrix A, rank(A) = 0 if and only if A is the
zero matrix.
4.
Use elementary row and column operations to transform each of the
following matrices into a matrix D satisfying the conditions of Theo-
rem 3.6, and then determine the rank of each matrix.
(a)
‚éõ
‚éù
1
1
1
2
2
0
‚àí1
2
1
1
1
2
‚éû
‚é†
(b)
‚éõ
‚éù
2
1
‚àí1
2
2
1
‚éû
‚é†
5.
For each of the following matrices, compute the rank and the inverse if
it exists.
(a)

1
2
1
1
	
(b)

1
2
2
4
	
(c)
‚éõ
‚éù
1
2
1
1
3
4
2
3
‚àí1
‚éû
‚é†
(d)
‚éõ
‚éù
0
‚àí2
4
1
1
‚àí1
2
4
‚àí5
‚éû
‚é†
(e)
‚éõ
‚éù
1
2
1
‚àí1
1
2
1
0
1
‚éû
‚é†
(f)
‚éõ
‚éù
1
2
1
1
0
1
1
1
1
‚éû
‚é†
(g)
‚éõ
‚éú
‚éú
‚éù
1
2
1
0
2
5
5
1
‚àí2
‚àí3
0
3
3
4
‚àí2
‚àí3
‚éû
‚éü
‚éü
‚é†
(h)
‚éõ
‚éú
‚éú
‚éù
1
0
1
1
1
1
‚àí1
2
2
0
1
0
0
‚àí1
1
‚àí3
‚éû
‚éü
‚éü
‚é†
6.
For each of the following linear transformations T, determine whether
T is invertible, and compute T‚àí1 if it exists.
(a)
T: P2(R) ‚ÜíP2(R) deÔ¨Åned by T(f(x)) = f ‚Ä≤‚Ä≤(x) + 2f ‚Ä≤(x) ‚àíf(x).
(b)
T: P2(R) ‚ÜíP2(R) deÔ¨Åned by T(f(x)) = (x + 1)f ‚Ä≤(x).
(c)
T: R3 ‚ÜíR3 deÔ¨Åned by
T(a1, a2, a3) = (a1 + 2a2 + a3, ‚àía1 + a2 + 2a3, a1 + a3).

Sec. 3.2
The Rank of a Matrix and Matrix Inverses
167
(d)
T: R3 ‚ÜíP2(R) deÔ¨Åned by
T(a1, a2, a3) = (a1 + a2 + a3) + (a1 ‚àía2 + a3)x + a1x2.
(e)
T: P2(R) ‚ÜíR3 deÔ¨Åned by T(f(x)) = (f(‚àí1), f(0), f(1)).
(f)
T: M2√ó2(R) ‚ÜíR4 deÔ¨Åned by
T(A) = (tr(A), tr(At), tr(EA), tr(AE)),
where
E =

0
1
1
0
	
.
7.
Express the invertible matrix
‚éõ
‚éù
1
2
1
1
0
1
1
1
2
‚éû
‚é†
as a product of elementary matrices.
8.
Let A be an m √ó n matrix. Prove that if c is any nonzero scalar, then
rank(cA) = rank(A).
9.
Complete the proof of the corollary to Theorem 3.4 by showing that
elementary column operations preserve rank.
10.
Prove Theorem 3.6 for the case that A is an m √ó 1 matrix.
11.
Let
B =
‚éõ
‚éú
‚éú
‚éú
‚éù
1
0
¬∑ ¬∑ ¬∑
0
0
...
0
B‚Ä≤
‚éû
‚éü
‚éü
‚éü
‚é†,
where B‚Ä≤ is an m √ó n submatrix of B. Prove that if rank(B) = r, then
rank(B‚Ä≤) = r ‚àí1.
12.
Let B‚Ä≤ and D‚Ä≤ be m√ón matrices, and let B and D be (m+1)√ó(n+1)
matrices respectively deÔ¨Åned by
B =
‚éõ
‚éú
‚éú
‚éú
‚éù
1
0
¬∑ ¬∑ ¬∑
0
0
...
0
B‚Ä≤
‚éû
‚éü
‚éü
‚éü
‚é†
and
D =
‚éõ
‚éú
‚éú
‚éú
‚éù
1
0
¬∑ ¬∑ ¬∑
0
0
...
0
D‚Ä≤
‚éû
‚éü
‚éü
‚éü
‚é†.
Prove that if B‚Ä≤ can be transformed into D‚Ä≤ by an elementary row
[column] operation, then B can be transformed into D by an elementary
row [column] operation.

168
Chap. 3
Elementary Matrix Operations and Systems of Linear Equations
13.
Prove (b) and (c) of Corollary 2 to Theorem 3.6.
14.
Let T, U: V ‚ÜíW be linear transformations.
(a)
Prove that R(T+U) ‚äÜR(T)+R(U). (See the deÔ¨Ånition of the sum
of subsets of a vector space on page 22.)
(b)
Prove that if W is Ô¨Ånite-dimensional, then rank(T+U) ‚â§rank(T)+
rank(U).
(c)
Deduce from (b) that rank(A + B) ‚â§rank(A) + rank(B) for any
m √ó n matrices A and B.
15.
Suppose that A and B are matrices having n rows.
Prove that
M(A|B) = (MA|MB) for any m √ó n matrix M.
16.
Supply the details to the proof of (b) of Theorem 3.4.
17.
Prove that if B is a 3 √ó 1 matrix and C is a 1 √ó 3 matrix, then the 3√ó 3
matrix BC has rank at most 1. Conversely, show that if A is any 3 √ó 3
matrix having rank 1, then there exist a 3 √ó 1 matrix B and a 1 √ó 3
matrix C such that A = BC.
18.
Let A be an m √ó n matrix and B be an n √ó p matrix. Prove that AB
can be written as a sum of n matrices of rank one.
19.
Let A be an m √ó n matrix with rank m and B be an n √ó p matrix with
rank n. Determine the rank of AB. Justify your answer.
20.
Let
A =
‚éõ
‚éú
‚éú
‚éù
1
0
‚àí1
2
1
‚àí1
1
3
‚àí1
0
‚àí2
1
4
‚àí1
3
3
‚àí1
‚àí5
1
‚àí6
‚éû
‚éü
‚éü
‚é†.
(a)
Find a 5 √ó 5 matrix M with rank 2 such that AM = O, where O
is the 4 √ó 5 zero matrix.
(b)
Suppose that B is a 5 √ó 5 matrix such that AB = O. Prove that
rank(B) ‚â§2.
21.
Let A be an m √ó n matrix with rank m. Prove that there exists an
n √ó m matrix B such that AB = Im.
22.
Let B be an n √ó m matrix with rank m. Prove that there exists an
m √ó n matrix A such that AB = Im.
3.3
SYSTEMS OF LINEAR EQUATIONS‚ÄîTHEORETICAL ASPECTS
This section and the next are devoted to the study of systems of linear equa-
tions, which arise naturally in both the physical and social sciences. In this
section, we apply results from Chapter 2 to describe the solution sets of

Sec. 3.3
Systems of Linear Equations‚ÄîTheoretical Aspects
169
systems of linear equations as subsets of a vector space. In Section 3.4, el-
ementary row operations are used to provide a computational method for
Ô¨Ånding all solutions to such systems.
The system of equations
(S)
a11x1 + a12x2 + ¬∑ ¬∑ ¬∑ + a1nxn = b1
a21x1 + a22x2 + ¬∑ ¬∑ ¬∑ + a2nxn = b2
...
am1x1 + am2x2 + ¬∑ ¬∑ ¬∑ + amnxn = bm,
where aij and bi (1 ‚â§i ‚â§m and 1 ‚â§j ‚â§n) are scalars in a Ô¨Åeld F and
x1, x2, . . . , xn are n variables taking values in F, is called a system of m
linear equations in n unknowns over the Ô¨Åeld F.
The m √ó n matrix
A =
‚éõ
‚éú
‚éú
‚éú
‚éù
a11
a12
¬∑ ¬∑ ¬∑
a1n
a21
a22
¬∑ ¬∑ ¬∑
a2n
...
...
...
am1
am2
¬∑ ¬∑ ¬∑
amn
‚éû
‚éü
‚éü
‚éü
‚é†
is called the coeÔ¨Écient matrix of the system (S).
If we let
x =
‚éõ
‚éú
‚éú
‚éú
‚éù
x1
x2
...
xn
‚éû
‚éü
‚éü
‚éü
‚é†
and
b =
‚éõ
‚éú
‚éú
‚éú
‚éù
b1
b2
...
bm
‚éû
‚éü
‚éü
‚éü
‚é†,
then the system (S) may be rewritten as a single matrix equation
Ax = b.
To exploit the results that we have developed, we often consider a system of
linear equations as a single matrix equation.
A solution to the system (S) is an n-tuple
s =
‚éõ
‚éú
‚éú
‚éú
‚éù
s1
s2
...
sn
‚éû
‚éü
‚éü
‚éü
‚é†‚ààFn
such that As = b. The set of all solutions to the system (S) is called the
solution set of the system. System (S) is called consistent if its solution
set is nonempty; otherwise it is called inconsistent.

170
Chap. 3
Elementary Matrix Operations and Systems of Linear Equations
Example 1
(a) Consider the system
x1 + x2 = 3
x1 ‚àíx2 = 1.
By use of familiar techniques, we can solve the preceding system and conclude
that there is only one solution: x1 = 2, x2 = 1; that is,
s =

2
1
	
.
In matrix form, the system can be written

1
1
1
‚àí1
	 
x1
x2
	
=

3
1
	
;
so
A =

1
1
1
‚àí1
	
and
B =

3
1
	
.
(b) Consider
2x1 + 3x2 + x3 = 1
x1 ‚àíx2 + 2x3 = 6;
that is,

2
3
1
1
‚àí1
2
	 ‚éõ
‚éù
x1
x2
x3
‚éû
‚é†=

1
6
	
.
This system has many solutions, such as
s =
‚éõ
‚éù
‚àí6
2
7
‚éû
‚é†
and
s =
‚éõ
‚éù
8
‚àí4
‚àí3
‚éû
‚é†.
(c) Consider
x1 + x2 = 0
x1 + x2 = 1;
that is,

1
1
1
1
	 
x1
x2
	
=
0
1
	
.
It is evident that this system has no solutions. Thus we see that a system of
linear equations can have one, many, or no solutions.
‚ô¶

Sec. 3.3
Systems of Linear Equations‚ÄîTheoretical Aspects
171
We must be able to recognize when a system has a solution and then be
able to describe all its solutions. This section and the next are devoted to
this end.
We begin our study of systems of linear equations by examining the class
of homogeneous systems of linear equations. Our Ô¨Årst result (Theorem 3.8)
shows that the set of solutions to a homogeneous system of m linear equations
in n unknowns forms a subspace of Fn. We can then apply the theory of vector
spaces to this set of solutions. For example, a basis for the solution space can
be found, and any solution can be expressed as a linear combination of the
vectors in the basis.
DeÔ¨Ånitions.
A system Ax = b of m linear equations in n unknowns
is said to be homogeneous if b = 0. Otherwise the system is said to be
nonhomogeneous.
Any homogeneous system has at least one solution, namely, the zero vec-
tor. The next result gives further information about the set of solutions to a
homogeneous system.
Theorem 3.8. Let Ax = 0 be a homogeneous system of m linear equa-
tions in n unknowns over a Ô¨Åeld F. Let K denote the set of all solutions
to Ax = 0. Then K = N(LA); hence K is a subspace of Fn of dimension
n ‚àírank(LA) = n ‚àírank(A).
Proof. Clearly, K = {s ‚ààFn : As = 0} = N(LA). The second part now
follows from the dimension theorem (p. 70).
Corollary. If m < n, the system Ax = 0 has a nonzero solution.
Proof. Suppose that m < n. Then rank(A) = rank(LA) ‚â§m. Hence
dim(K) = n ‚àírank(LA) ‚â•n ‚àím > 0,
where K = N(LA). Since dim(K) > 0, K Ã∏= {0}. Thus there exists a nonzero
vector s ‚ààK; so s is a nonzero solution to Ax = 0.
Example 2
(a) Consider the system
x1 + 2x2 + x3 = 0
x1 ‚àíx2 ‚àíx3 = 0.
Let
A =

1
2
1
1
‚àí1
‚àí1
	

172
Chap. 3
Elementary Matrix Operations and Systems of Linear Equations
be the coeÔ¨Écient matrix of this system. It is clear that rank(A) = 2. If K is
the solution set of this system, then dim(K) = 3 ‚àí2 = 1. Thus any nonzero
solution constitutes a basis for K. For example, since
‚éõ
‚éù
1
‚àí2
3
‚éû
‚é†
is a solution to the given system,
‚éß
‚é®
‚é©
‚éõ
‚éù
1
‚àí2
3
‚éû
‚é†
‚é´
‚é¨
‚é≠
is a basis for K. Thus any vector in K is of the form
t
‚éõ
‚éù
1
‚àí2
3
‚éû
‚é†=
‚éõ
‚éù
t
‚àí2t
3t
‚éû
‚é†,
where t ‚ààR.
(b) Consider the system x1 ‚àí2x2 + x3 = 0 of one equation in three
unknowns. If A =

1
‚àí2
1

is the coeÔ¨Écient matrix, then rank(A) = 1.
Hence if K is the solution set, then dim(K) = 3 ‚àí1 = 2. Note that
‚éõ
‚éù
2
1
0
‚éû
‚é†
and
‚éõ
‚éù
‚àí1
0
1
‚éû
‚é†
are linearly independent vectors in K. Thus they constitute a basis for K, so
that
K =
‚éß
‚é®
‚é©t1
‚éõ
‚éù
2
1
0
‚éû
‚é†+ t2
‚éõ
‚éù
‚àí1
0
1
‚éû
‚é†: t1, t2 ‚ààR
‚é´
‚é¨
‚é≠.
‚ô¶
In Section 3.4, explicit computational methods for Ô¨Ånding a basis for the
solution set of a homogeneous system are discussed.
We now turn to the study of nonhomogeneous systems. Our next result
shows that the solution set of a nonhomogeneous system Ax = b can be
described in terms of the solution set of the homogeneous system Ax = 0. We
refer to the equation Ax = 0 as the homogeneous system corresponding
to Ax = b.
Theorem 3.9. Let K be the solution set of a system of linear equations
Ax = b, and let KH be the solution set of the corresponding homogeneous
system Ax = 0. Then for any solution s to Ax = b
K = {s} + KH = {s + k: k ‚ààKH}.

Sec. 3.3
Systems of Linear Equations‚ÄîTheoretical Aspects
173
Proof. Let s be any solution to Ax = b. We must show that K = {s}+KH.
If w ‚ààK, then Aw = b. Hence
A(w ‚àís) = Aw ‚àíAs = b ‚àíb = 0.
So w ‚àís ‚ààKH. Thus there exists k ‚ààKH such that w ‚àís = k. It follows that
w = s + k ‚àà{s} + KH, and therefore
K ‚äÜ{s} + KH.
Conversely, suppose that w ‚àà{s} + KH; then w = s + k for some k ‚ààKH.
But then Aw = A(s + k) = As + Ak = b + 0 = b; so w ‚ààK. Therefore
{s} + KH ‚äÜK, and thus K = {s} + KH.
Example 3
(a) Consider the system
x1 + 2x2 + x3 =
7
x1 ‚àíx2 ‚àíx3 = ‚àí4.
The corresponding homogeneous system is the system in Example 2(a). It is
easily veriÔ¨Åed that
s =
‚éõ
‚éù
1
1
4
‚éû
‚é†
is a solution to the preceding nonhomogeneous system. So the solution set of
the system is
K =
‚éß
‚é®
‚é©
‚éõ
‚éù
1
1
4
‚éû
‚é†+ t
‚éõ
‚éù
1
‚àí2
3
‚éû
‚é†: t ‚ààR
‚é´
‚é¨
‚é≠
by Theorem 3.9.
(b) Consider the system x1 ‚àí2x2 + x3 = 4. The corresponding homoge-
neous system is the system in Example 2(b). Since
s =
‚éõ
‚éù
4
0
0
‚éû
‚é†
is a solution to the given system, the solution set K can be written as
K =
‚éß
‚é®
‚é©
‚éõ
‚éù
4
0
0
‚éû
‚é†+ t1
‚éõ
‚éù
2
1
0
‚éû
‚é†+ t2
‚éõ
‚éù
‚àí1
0
1
‚éû
‚é†: t1, t2 ‚ààR
‚é´
‚é¨
‚é≠.
‚ô¶

174
Chap. 3
Elementary Matrix Operations and Systems of Linear Equations
The following theorem provides us with a means of computing solutions
to certain systems of linear equations.
Theorem 3.10. Let Ax = b be a system of n linear equations in n
unknowns. If A is invertible, then the system has exactly one solution, namely,
A‚àí1b. Conversely, if the system has exactly one solution, then A is invertible.
Proof. Suppose that A is invertible. Substituting A‚àí1b into the system, we
have A(A‚àí1b) = (AA‚àí1)b = b. Thus A‚àí1b is a solution. If s is an arbitrary
solution, then As = b. Multiplying both sides by A‚àí1 gives s = A‚àí1b. Thus
the system has one and only one solution, namely, A‚àí1b.
Conversely, suppose that the system has exactly one solution s. Let KH
denote the solution set for the corresponding homogeneous system Ax = 0.
By Theorem 3.9, {s} = {s} + KH. But this is so only if KH = {0}. Thus
N(LA) = {0}, and hence A is invertible.
Example 4
Consider the following system of three linear equations in three unknowns:
2x2 + 4x3 = 2
2x1 + 4x2 + 2x3 = 3
3x1 + 3x2 + x3 = 1.
In Example 5 of Section 3.2, we computed the inverse of the coeÔ¨Écient matrix
A of this system. Thus the system has exactly one solution, namely,
‚éõ
‚éù
x1
x2
x3
‚éû
‚é†= A‚àí1b =
‚éõ
‚éú
‚éú
‚éù
1
8
‚àí5
8
3
4
‚àí1
4
3
4
‚àí1
2
3
8
‚àí3
8
1
4
‚éû
‚éü
‚éü
‚é†
‚éõ
‚éù
2
3
1
‚éû
‚é†=
‚éõ
‚éú
‚éú
‚éù
‚àí7
8
5
4
‚àí1
8
‚éû
‚éü
‚éü
‚é†.
‚ô¶
We use this technique for solving systems of linear equations having in-
vertible coeÔ¨Écient matrices in the application that concludes this section.
In Example 1(c), we saw a system of linear equations that has no solutions.
We now establish a criterion for determining when a system has solutions.
This criterion involves the rank of the coeÔ¨Écient matrix of the system Ax = b
and the rank of the matrix (A|b). The matrix (A|b) is called the augmented
matrix of the system Ax = b.
Theorem 3.11. Let Ax = b be a system of linear equations. Then the
system is consistent if and only if rank(A) = rank(A|b).
Proof. To say that Ax = b has a solution is equivalent to saying that
b ‚ààR(LA). (See Exercise 9.) In the proof of Theorem 3.5 (p. 153), we saw
that
R(LA) = span({a1, a2, . . . , an}),

Sec. 3.3
Systems of Linear Equations‚ÄîTheoretical Aspects
175
the span of the columns of A.
Thus Ax = b has a solution if and only
if b ‚ààspan({a1, a2, . . . , an}).
But b ‚ààspan({a1, a2, . . . , an}) if and only
if span({a1, a2, . . . , an}) = span({a1, a2, . . . , an, b}). This last statement is
equivalent to
dim(span({a1, a2, . . . , an})) = dim(span({a1, a2, . . . , an, b})).
So by Theorem 3.5, the preceding equation reduces to
rank(A) = rank(A|b).
Example 5
Recall the system of equations
x1 + x2 = 0
x1 + x2 = 1
in Example 1(c).
Since
A =

1
1
1
1
	
and
(A|b) =

1
1
0
1
1
1
	
,
rank(A) = 1 and rank(A|b) = 2. Because the two ranks are unequal, the
system has no solutions.
‚ô¶
Example 6
We can use Theorem 3.11 to determine whether (3, 3, 2) is in the range of the
linear transformation T: R3 ‚ÜíR3 deÔ¨Åned by
T(a1, a2, a3) = (a1 + a2 + a3, a1 ‚àía2 + a3, a1 + a3).
Now (3, 3, 2) ‚ààR(T) if and only if there exists a vector s = (x1, x2, x3)
in R3 such that T(s) = (3, 3, 2). Such a vector s must be a solution to the
system
x1 + x2 + x3 = 3
x1 ‚àíx2 + x3 = 3
x1
+ x3 = 2.
Since the ranks of the coeÔ¨Écient matrix and the augmented matrix of this
system are 2 and 3, respectively, it follows that this system has no solutions.
Hence (3, 3, 2) /‚ààR(T).
‚ô¶

176
Chap. 3
Elementary Matrix Operations and Systems of Linear Equations
An Application
In 1973, Wassily Leontief won the Nobel prize in economics for his work
in developing a mathematical model that can be used to describe various
economic phenomena. We close this section by applying some of the ideas we
have studied to illustrate two special cases of his work.
We begin by considering a simple society composed of three people
(industries)‚Äîa farmer who grows all the food, a tailor who makes all the
clothing, and a carpenter who builds all the housing. We assume that each
person sells to and buys from a central pool and that everything produced is
consumed. Since no commodities either enter or leave the system, this case
is referred to as the closed model.
Each of these three individuals consumes all three of the commodities pro-
duced in the society. Suppose that the proportion of each of the commodities
consumed by each person is given in the following table. Notice that each of
the columns of the table must sum to 1.
Food
Clothing
Housing
Farmer
0.40
0.20
0.20
Tailor
0.10
0.70
0.20
Carpenter
0.50
0.10
0.60
Let p1, p2, and p3 denote the incomes of the farmer, tailor, and carpenter,
respectively. To ensure that this society survives, we require that the con-
sumption of each individual equals his or her income. Note that the farmer
consumes 20% of the clothing. Because the total cost of all clothing is p2,
the tailor‚Äôs income, the amount spent by the farmer on clothing is 0.20p2.
Moreover, the amount spent by the farmer on food, clothing, and housing
must equal the farmer‚Äôs income, and so we obtain the equation
0.40p1 + 0.20p2 + 0.20p3 = p1.
Similar equations describing the expenditures of the tailor and carpenter pro-
duce the following system of linear equations:
0.40p1 + 0.20p2 + 0.20p3 = p1
0.10p1 + 0.70p2 + 0.20p3 = p2
0.50p1 + 0.10p2 + 0.60p3 = p3.
This system can be written as Ap = p, where
p =
‚éõ
‚éù
p1
p2
p3
‚éû
‚é†

Sec. 3.3
Systems of Linear Equations‚ÄîTheoretical Aspects
177
and A is the coeÔ¨Écient matrix of the system. In this context, A is called
the input‚Äìoutput (or consumption) matrix, and Ap = p is called the
equilibrium condition.
For vectors b = (b1, b2, . . . , bn) and c = (c1, c2, . . . , cn) in Rn, we use the
notation b ‚â•c [b > c] to mean bi ‚â•ci [bi > ci] for all i. The vector b is called
nonnegative [positive] if b ‚â•0 [b > 0].
At Ô¨Årst, it may seem reasonable to replace the equilibrium condition by
the inequality Ap ‚â§p, that is, the requirement that consumption not exceed
production. But, in fact, Ap ‚â§p implies that Ap = p in the closed model.
For otherwise, there exists a k for which
pk >

j
Akjpj.
Hence, since the columns of A sum to 1,

i
pi >

i

j
Aijpj =

j

i
Aij

pj =

j
pj,
which is a contradiction.
One solution to the homogeneous system (I‚àíA)x = 0, which is equivalent
to the equilibrium condition, is
p =
‚éõ
‚éù
0.25
0.35
0.40
‚éû
‚é†.
We may interpret this to mean that the society survives if the farmer, tailor,
and carpenter have incomes in the proportions 25 : 35 : 40 (or 5 : 7 : 8).
Notice that we are not simply interested in any nonzero solution to the
system, but in one that is nonnegative. Thus we must consider the question
of whether the system (I ‚àíA)x = 0 has a nonnegative solution, where A is a
matrix with nonnegative entries whose columns sum to 1. A useful theorem
in this direction (whose proof may be found in ‚ÄúApplications of Matrices to
Economic Models and Social Science Relationships,‚Äù by Ben Noble, Proceed-
ings of the Summer Conference for College Teachers on Applied Mathematics,
1971, CUPM, Berkeley, California) is stated below.
Theorem 3.12. Let A be an n √ó n input‚Äìoutput matrix having the form
A =

B
C
D
E
	
,
where D is a 1√ó(n‚àí1) positive vector and C is an (n‚àí1)√ó1 positive vector.
Then (I ‚àíA)x = 0 has a one-dimensional solution set that is generated by a
nonnegative vector.

178
Chap. 3
Elementary Matrix Operations and Systems of Linear Equations
Observe that any input‚Äìoutput matrix with all positive entries satisÔ¨Åes
the hypothesis of this theorem. The following matrix does also:
‚éõ
‚éù
0.75
0.50
0.65
0
0.25
0.35
0.25
0.25
0
‚éû
‚é†.
In the open model, we assume that there is an outside demand for each
of the commodities produced. Returning to our simple society, let x1, x2,
and x3 be the monetary values of food, clothing, and housing produced with
respective outside demands d1, d2, and d3. Let A be the 3 √ó 3 matrix such
that Aij represents the amount (in a Ô¨Åxed monetary unit such as the dollar)
of commodity i required to produce one monetary unit of commodity j. Then
the value of the surplus of food in the society is
x1 ‚àí(A11x1 + A12x2 + A13x3),
that is, the value of food produced minus the value of food consumed while
producing the three commodities. The assumption that everything produced
is consumed gives us a similar equilibrium condition for the open model,
namely, that the surplus of each of the three commodities must equal the
corresponding outside demands. Hence
xi ‚àí
3

j=1
Aijxj = di
for i = 1, 2, 3.
In general, we must Ô¨Ånd a nonnegative solution to (I ‚àíA)x = d, where
A is a matrix with nonnegative entries such that the sum of the entries of
each column of A does not exceed one, and d ‚â•0. It is easy to see that if
(I ‚àíA)‚àí1 exists and is nonnegative, then the desired solution is (I ‚àíA)‚àí1d.
Recall that for a real number a, the series 1 + a + a2 + ¬∑ ¬∑ ¬∑ converges to
(1 ‚àía)‚àí1 if |a| < 1. Similarly, it can be shown (using the concept of conver-
gence of matrices developed in Section 5.3) that the series I + A + A2 + ¬∑ ¬∑ ¬∑
converges to (I ‚àíA)‚àí1 if {An} converges to the zero matrix. In this case,
(I ‚àíA)‚àí1 is nonnegative since the matrices I, A, A2, . . . are nonnegative.
To illustrate the open model, suppose that 30 cents worth of food, 10
cents worth of clothing, and 30 cents worth of housing are required for the
production of $1 worth of food. Similarly, suppose that 20 cents worth of
food, 40 cents worth of clothing, and 20 cents worth of housing are required
for the production of $1 of clothing. Finally, suppose that 30 cents worth of
food, 10 cents worth of clothing, and 30 cents worth of housing are required
for the production of $1 worth of housing. Then the input‚Äìoutput matrix is
A =
‚éõ
‚éù
0.30
0.20
0.30
0.10
0.40
0.10
0.30
0.20
0.30
‚éû
‚é†;

Sec. 3.3
Systems of Linear Equations‚ÄîTheoretical Aspects
179
so
I ‚àíA =
‚éõ
‚éù
0.70
‚àí0.20
‚àí0.30
‚àí0.10
0.60
‚àí0.10
‚àí0.30
‚àí0.20
0.70
‚éû
‚é†
and
(I ‚àíA)‚àí1 =
‚éõ
‚éù
2.0
1.0
1.0
0.5
2.0
0.5
1.0
1.0
2.0
‚éû
‚é†.
Since (I ‚àíA)‚àí1 is nonnegative, we can Ô¨Ånd a (unique) nonnegative solution to
(I ‚àíA)x = d for any demand d. For example, suppose that there are outside
demands for $30 billion in food, $20 billion in clothing, and $10 billion in
housing. If we set
d =
‚éõ
‚éù
30
20
10
‚éû
‚é†,
then
x = (I ‚àíA)‚àí1d =
‚éõ
‚éù
90
60
70
‚éû
‚é†.
So a gross production of $90 billion of food, $60 billion of clothing, and $70
billion of housing is necessary to meet the required demands.
EXERCISES
1.
Label the following statements as true or false.
(a)
Any system of linear equations has at least one solution.
(b)
Any system of linear equations has at most one solution.
(c)
Any homogeneous system of linear equations has at least one so-
lution.
(d)
Any system of n linear equations in n unknowns has at most one
solution.
(e)
Any system of n linear equations in n unknowns has at least one
solution.
(f)
If the homogeneous system corresponding to a given system of lin-
ear equations has a solution, then the given system has a solution.
(g)
If the coeÔ¨Écient matrix of a homogeneous system of n linear equa-
tions in n unknowns is invertible, then the system has no nonzero
solutions.
(h)
The solution set of any system of m linear equations in n unknowns
is a subspace of Fn.
2.
For each of the following homogeneous systems of linear equations, Ô¨Ånd
the dimension of and a basis for the solution set.

180
Chap. 3
Elementary Matrix Operations and Systems of Linear Equations
(a)
x1 + 3x2 = 0
2x1 + 6x2 = 0
(b)
x1 + x2 ‚àíx3 = 0
4x1 + x2 ‚àí2x3 = 0
(c)
x1 + 2x2 ‚àíx3 = 0
2x1 + x2 + x3 = 0
(d)
2x1 + x2 ‚àíx3 = 0
x1 ‚àíx2 + x3 = 0
x1 + 2x2 ‚àí2x3 = 0
(e) x1 + 2x2 ‚àí3x3 + x4 = 0
(f) x1 + 2x2 = 0
x1 ‚àíx2 = 0
(g) x1 + 2x2 + x3 + x4 = 0
x2 ‚àíx3 + x4 = 0
3.
Using the results of Exercise 2, Ô¨Ånd all solutions to the following sys-
tems.
(a)
x1 + 3x2 = 5
2x1 + 6x2 = 10
(b)
x1 + x2 ‚àíx3 = 1
4x1 + x2 ‚àí2x3 = 3
(c)
x1 + 2x2 ‚àíx3 = 3
2x1 + x2 + x3 = 6
(d)
2x1 + x2 ‚àíx3 = 5
x1 ‚àíx2 + x3 = 1
x1 + 2x2 ‚àí2x3 = 4
(e) x1 + 2x2 ‚àí3x3 + x4 = 1
(f) x1 + 2x2 =
5
x1 ‚àíx2 = ‚àí1
(g) x1 + 2x2 + x3 + x4 = 1
x2 ‚àíx3 + x4 = 1
4.
For each system of linear equations with the invertible coeÔ¨Écient matrix
A,
(1) Compute A‚àí1.
(2) Use A‚àí1 to solve the system.
(a)
x1 + 3x2 = 4
2x1 + 5x2 = 3
(b)
x1 + 2x2 ‚àíx3 = 5
x1 + x2 + x3 = 1
2x1 ‚àí2x2 + x3 = 4
5.
Give an example of a system of n linear equations in n unknowns with
inÔ¨Ånitely many solutions.
6.
Let T: R3 ‚ÜíR2 be deÔ¨Åned by T(a, b, c) = (a + b, 2a ‚àíc). Determine
T‚àí1(1, 11).
7.
Determine which of the following systems of linear equations has a so-
lution.

Sec. 3.3
Systems of Linear Equations‚ÄîTheoretical Aspects
181
(a)
x1 + x2 ‚àíx3 + 2x4 = 2
x1 + x2 + 2x3
= 1
2x1 + 2x2 + x3 + 2x4 = 4
(b)
x1 + x2 ‚àíx3 = 1
2x1 + x2 + 3x3 = 2
(c)
x1 + 2x2 + 3x3 = 1
x1 + x2 ‚àíx3 = 0
x1 + 2x2 + x3 = 3
(d)
x1 + x2 + 3x3 ‚àíx4 = 0
x1 + x2 + x3 + x4 = 1
x1 ‚àí2x2 + x3 ‚àíx4 = 1
4x1 + x2 + 8x3 ‚àíx4 = 0
(e)
x1 + 2x2 ‚àíx3 = 1
2x1 + x2 + 2x3 = 3
x1 ‚àí4x2 + 7x3 = 4
8.
Let T: R3 ‚ÜíR3 be deÔ¨Åned by T(a, b, c) = (a + b, b ‚àí2c, a + 2c). For
each vector v in R3, determine whether v ‚ààR(T).
(a)
v = (1, 3, ‚àí2)
(b)
v = (2, 1, 1)
9.
Prove that the system of linear equations Ax = b has a solution if and
only if b ‚ààR(LA).
10.
Prove or give a counterexample to the following statement: If the co-
eÔ¨Écient matrix of a system of m linear equations in n unknowns has
rank m, then the system has a solution.
11.
In the closed model of Leontief with food, clothing, and housing as the
basic industries, suppose that the input‚Äìoutput matrix is
A =
‚éõ
‚éú
‚éú
‚éù
7
16
1
2
3
16
5
16
1
6
5
16
1
4
1
3
1
2
‚éû
‚éü
‚éü
‚é†.
At what ratio must the farmer, tailor, and carpenter produce in order
for equilibrium to be attained?
12.
A certain economy consists of two sectors: goods and services. Suppose
that 60% of all goods and 30% of all services are used in the production
of goods. What proportion of the total economic output is used in the
production of goods?
13.
In the notation of the open model of Leontief, suppose that
A =
‚éõ
‚éù
1
2
1
5
1
3
1
5
‚éû
‚é†
and
d =

2
5
	
are the input‚Äìoutput matrix and the demand vector, respectively. How
much of each commodity must be produced to satisfy this demand?

182
Chap. 3
Elementary Matrix Operations and Systems of Linear Equations
14.
A certain economy consisting of the two sectors of goods and services
supports a defense system that consumes $90 billion worth of goods and
$20 billion worth of services from the economy but does not contribute
to economic production.
Suppose that 50 cents worth of goods and
20 cents worth of services are required to produce $1 worth of goods
and that 30 cents worth of of goods and 60 cents worth of services are
required to produce $1 worth of services. What must the total output
of the economic system be to support this defense system?
3.4
SYSTEMS OF LINEAR EQUATIONS‚Äî
COMPUTATIONAL ASPECTS
In Section 3.3, we obtained a necessary and suÔ¨Écient condition for a system
of linear equations to have solutions (Theorem 3.11 p. 174) and learned how
to express the solutions to a nonhomogeneous system in terms of solutions
to the corresponding homogeneous system (Theorem 3.9 p. 172). The latter
result enables us to determine all the solutions to a given system if we can
Ô¨Ånd one solution to the given system and a basis for the solution set of the
corresponding homogeneous system. In this section, we use elementary row
operations to accomplish these two objectives simultaneously. The essence of
this technique is to transform a given system of linear equations into a system
having the same solutions, but which is easier to solve (as in Section 1.4).
DeÔ¨Ånition.
Two systems of linear equations are called equivalent if
they have the same solution set.
The following theorem and corollary give a useful method for obtaining
equivalent systems.
Theorem 3.13. Let Ax = b be a system of m linear equations in n
unknowns, and let C be an invertible m √ó m matrix.
Then the system
(CA)x = Cb is equivalent to Ax = b.
Proof. Let K be the solution set for Ax = b and K‚Ä≤ the solution set for
(CA)x = Cb. If w ‚ààK, then Aw = b. So (CA)w = Cb, and hence w ‚ààK‚Ä≤.
Thus K ‚äÜK‚Ä≤.
Conversely, if w ‚ààK‚Ä≤, then (CA)w = Cb. Hence
Aw = C‚àí1(CAw) = C‚àí1(Cb) = b;
so w ‚ààK. Thus K‚Ä≤ ‚äÜK, and therefore, K = K‚Ä≤.
Corollary. Let Ax = b be a system of m linear equations in n unknowns.
If (A‚Ä≤|b‚Ä≤) is obtained from (A|b) by a Ô¨Ånite number of elementary row opera-
tions, then the system A‚Ä≤x = b‚Ä≤ is equivalent to the original system.

Sec. 3.4
Systems of Linear Equations‚ÄîComputational Aspects
183
Proof. Suppose that (A‚Ä≤|b‚Ä≤) is obtained from (A|b) by elementary row
operations. These may be executed by multiplying (A|b) by elementary m√óm
matrices E1, E2, . . . , Ep. Let C = Ep ¬∑ ¬∑ ¬∑ E2E1; then
(A‚Ä≤|b‚Ä≤) = C(A|b) = (CA|Cb).
Since each Ei is invertible, so is C. Now A‚Ä≤ = CA and b‚Ä≤ = Cb. Thus by
Theorem 3.13, the system A‚Ä≤x = b‚Ä≤ is equivalent to the system Ax = b.
We now describe a method for solving any system of linear equations.
Consider, for example, the system of linear equations
3x1 + 2x2 + 3x3 ‚àí2x4 = 1
x1 + x2 + x3
= 3
x1 + 2x2 + x3 ‚àíx4 = 2.
First, we form the augmented matrix
‚éõ
‚éù
3
2
3
‚àí2
1
1
1
1
0
3
1
2
1
‚àí1
2
‚éû
‚é†.
By using elementary row operations, we transform the augmented matrix
into an upper triangular matrix in which the Ô¨Årst nonzero entry of each row
is 1, and it occurs in a column to the right of the Ô¨Årst nonzero entry of each
preceding row. (Recall that matrix A is upper triangular if Aij = 0 whenever
i > j.)
1. In the leftmost nonzero column, create a 1 in the Ô¨Årst row.
In our
example, we can accomplish this step by interchanging the Ô¨Årst and
third rows. The resulting matrix is
‚éõ
‚éù
1
2
1
‚àí1
2
1
1
1
0
3
3
2
3
‚àí2
1
‚éû
‚é†.
2. By means of type 3 row operations, use the Ô¨Årst row to obtain zeros in
the remaining positions of the leftmost nonzero column. In our example,
we must add ‚àí1 times the Ô¨Årst row to the second row and then add ‚àí3
times the Ô¨Årst row to the third row to obtain
‚éõ
‚éù
1
2
1
‚àí1
2
0
‚àí1
0
1
1
0
‚àí4
0
1
‚àí5
‚éû
‚é†.
3. Create a 1 in the next row in the leftmost possible column, without using
previous row(s).
In our example, the second column is the leftmost

184
Chap. 3
Elementary Matrix Operations and Systems of Linear Equations
possible column, and we can obtain a 1 in the second row, second column
by multiplying the second row by ‚àí1. This operation produces
‚éõ
‚éù
1
2
1
‚àí1
2
0
1
0
‚àí1
‚àí1
0
‚àí4
0
1
‚àí5
‚éû
‚é†.
4. Now use type 3 elementary row operations to obtain zeros below the 1
created in the preceding step. In our example, we must add four times
the second row to the third row. The resulting matrix is
‚éõ
‚éù
1
2
1
‚àí1
2
0
1
0
‚àí1
‚àí1
0
0
0
‚àí3
‚àí9
‚éû
‚é†.
5. Repeat steps 3 and 4 on each succeeding row until no nonzero rows
remain. (This creates zeros above the Ô¨Årst nonzero entry in each row.)
In our example, this can be accomplished by multiplying the third row
by ‚àí1
3. This operation produces
‚éõ
‚éù
1
2
1
‚àí1
2
0
1
0
‚àí1
‚àí1
0
0
0
1
3
‚éû
‚é†.
We have now obtained the desired matrix. To complete the simpliÔ¨Åcation
of the augmented matrix, we must make the Ô¨Årst nonzero entry in each row
the only nonzero entry in its column. (This corresponds to eliminating certain
unknowns from all but one of the equations.)
6. Work upward, beginning with the last nonzero row, and add multiples of
each row to the rows above. (This creates zeros above the Ô¨Årst nonzero
entry in each row.) In our example, the third row is the last nonzero
row, and the Ô¨Årst nonzero entry of this row lies in column 4. Hence we
add the third row to the Ô¨Årst and second rows to obtain zeros in row 1,
column 4 and row 2, column 4. The resulting matrix is
‚éõ
‚éù
1
2
1
0
5
0
1
0
0
2
0
0
0
1
3
‚éû
‚é†.
7. Repeat the process described in step 6 for each preceding row until it is
performed with the second row, at which time the reduction process is
complete. In our example, we must add ‚àí2 times the second row to the
Ô¨Årst row in order to make the Ô¨Årst row, second column entry become
zero. This operation produces
‚éõ
‚éù
1
0
1
0
1
0
1
0
0
2
0
0
0
1
3
‚éû
‚é†.

Sec. 3.4
Systems of Linear Equations‚ÄîComputational Aspects
185
We have now obtained the desired reduction of the augmented matrix.
This matrix corresponds to the system of linear equations
x1 +
x3
= 1
x2
= 2
x4 = 3.
Recall that, by the corollary to Theorem 3.13, this system is equivalent to
the original system. But this system is easily solved. Obviously x2 = 2 and
x4 = 3. Moreover, x1 and x3 can have any values provided their sum is 1.
Letting x3 = t, we then have x1 = 1 ‚àít. Thus an arbitrary solution to the
original system has the form
‚éõ
‚éú
‚éú
‚éù
1 ‚àít
2
t
3
‚éû
‚éü
‚éü
‚é†=
‚éõ
‚éú
‚éú
‚éù
1
2
0
3
‚éû
‚éü
‚éü
‚é†+ t
‚éõ
‚éú
‚éú
‚éù
‚àí1
0
1
0
‚éû
‚éü
‚éü
‚é†.
Observe that
‚éß
‚é™
‚é™
‚é®
‚é™
‚é™
‚é©
‚éõ
‚éú
‚éú
‚éù
‚àí1
0
1
0
‚éû
‚éü
‚éü
‚é†
‚é´
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é≠
is a basis for the homogeneous system of equations corresponding to the given
system.
In the preceding example we performed elementary row operations on the
augmented matrix of the system until we obtained the augmented matrix of a
system having properties 1, 2, and 3 on page 27. Such a matrix has a special
name.
DeÔ¨Ånition.
A matrix is said to be in reduced row echelon form if
the following three conditions are satisÔ¨Åed.
(a) Any row containing a nonzero entry precedes any row in which all the
entries are zero (if any).
(b) The Ô¨Årst nonzero entry in each row is the only nonzero entry in its
column.
(c) The Ô¨Årst nonzero entry in each row is 1 and it occurs in a column to
the right of the Ô¨Årst nonzero entry in the preceding row.
Example 1
(a) The matrix on page 184 is in reduced row echelon form. Note that the
Ô¨Årst nonzero entry of each row is 1 and that the column containing each such
entry has all zeros otherwise. Also note that each time we move downward to

186
Chap. 3
Elementary Matrix Operations and Systems of Linear Equations
a new row, we must move to the right one or more columns to Ô¨Ånd the Ô¨Årst
nonzero entry of the new row.
(b) The matrix
‚éõ
‚éù
1
1
0
0
1
0
1
0
1
‚éû
‚é†,
is not in reduced row echelon form, because the Ô¨Årst column, which contains
the Ô¨Årst nonzero entry in row 1, contains another nonzero entry. Similarly,
the matrix
‚éõ
‚éù
0
1
0
2
1
0
0
1
0
0
1
1
‚éû
‚é†,
is not in reduced row echelon form, because the Ô¨Årst nonzero entry of the
second row is not to the right of the Ô¨Årst nonzero entry of the Ô¨Årst row.
Finally, the matrix

2
0
0
0
1
0
	
,
is not in reduced row echelon form, because the Ô¨Årst nonzero entry of the Ô¨Årst
row is not 1.
‚ô¶
It can be shown (see the corollary to Theorem 3.16) that the reduced
row echelon form of a matrix is unique; that is, if diÔ¨Äerent sequences of
elementary row operations are used to transform a matrix into matrices Q
and Q‚Ä≤ in reduced row echelon form, then Q = Q‚Ä≤. Thus, although there are
many diÔ¨Äerent sequences of elementary row operations that can be used to
transform a given matrix into reduced row echelon form, they all produce the
same result.
The procedure described on pages 183‚Äì185 for reducing an augmented
matrix to reduced row echelon form is called Gaussian elimination.
It
consists of two separate parts.
1. In the forward pass (steps 1-5), the augmented matrix is transformed
into an upper triangular matrix in which the Ô¨Årst nonzero entry of each
row is 1, and it occurs in a column to the right of the Ô¨Årst nonzero entry
of each preceding row.
2. In the backward pass or back-substitution (steps 6-7), the upper trian-
gular matrix is transformed into reduced row echelon form by making
the Ô¨Årst nonzero entry of each row the only nonzero entry of its column.

Sec. 3.4
Systems of Linear Equations‚ÄîComputational Aspects
187
Of all the methods for transforming a matrix into its reduced row ech-
elon form, Gaussian elimination requires the fewest arithmetic operations.
(For large matrices, it requires approximately 50% fewer operations than the
Gauss-Jordan method, in which the matrix is transformed into reduced row
echelon form by using the Ô¨Årst nonzero entry in each row to make zero all
other entries in its column.) Because of this eÔ¨Éciency, Gaussian elimination
is the preferred method when solving systems of linear equations on a com-
puter. In this context, the Gaussian elimination procedure is usually modiÔ¨Åed
in order to minimize roundoÔ¨Äerrors. Since discussion of these techniques is
inappropriate here, readers who are interested in such matters are referred to
books on numerical analysis.
When a matrix is in reduced row echelon form, the corresponding sys-
tem of linear equations is easy to solve. We present below a procedure for
solving any system of linear equations for which the augmented matrix is in
reduced row echelon form. First, however, we note that every matrix can be
transformed into reduced row echelon form by Gaussian elimination. In the
forward pass, we satisfy conditions (a) and (c) in the deÔ¨Ånition of reduced
row echelon form and thereby make zero all entries below the Ô¨Årst nonzero
entry in each row. Then in the backward pass, we make zero all entries above
the Ô¨Årst nonzero entry in each row, thereby satisfying condition (b) in the
deÔ¨Ånition of reduced row echelon form.
Theorem 3.14. Gaussian elimination transforms any matrix into its re-
duced row echelon form.
We now describe a method for solving a system in which the augmented
matrix is in reduced row echelon form. To illustrate this procedure, we con-
sider the system
2x1 + 3x2 + x3 + 4x4 ‚àí9x5 = 17
x1 + x2 + x3 + x4 ‚àí3x5 = 6
x1 + x2 + x3 + 2x4 ‚àí5x5 = 8
2x1 + 2x2 + 2x3 + 3x4 ‚àí8x5 = 14,
for which the augmented matrix is
‚éõ
‚éú
‚éú
‚éù
2
3
1
4
‚àí9
17
1
1
1
1
‚àí3
6
1
1
1
2
‚àí5
8
2
2
2
3
‚àí8
14
‚éû
‚éü
‚éü
‚é†.
Applying Gaussian elimination to the augmented matrix of the system pro-
duces the following sequence of matrices.
‚éõ
‚éú
‚éú
‚éù
2
3
1
4
‚àí9
17
1
1
1
1
‚àí3
6
1
1
1
2
‚àí5
8
2
2
2
3
‚àí8
14
‚éû
‚éü
‚éü
‚é†‚àí‚Üí
‚éõ
‚éú
‚éú
‚éù
1
1
1
1
‚àí3
6
2
3
1
4
‚àí9
17
1
1
1
2
‚àí5
8
2
2
2
3
‚àí8
14
‚éû
‚éü
‚éü
‚é†‚àí‚Üí

188
Chap. 3
Elementary Matrix Operations and Systems of Linear Equations
‚éõ
‚éú
‚éú
‚éù
1
1
1
1
‚àí3
6
0
1
‚àí1
2
‚àí3
5
0
0
0
1
‚àí2
2
0
0
0
1
‚àí2
2
‚éû
‚éü
‚éü
‚é†‚àí‚Üí
‚éõ
‚éú
‚éú
‚éù
1
1
1
1
‚àí3
6
0
1
‚àí1
2
‚àí3
5
0
0
0
1
‚àí2
2
0
0
0
0
0
0
‚éû
‚éü
‚éü
‚é†‚àí‚Üí
‚éõ
‚éú
‚éú
‚éù
1
1
1
0
‚àí1
4
0
1
‚àí1
0
1
1
0
0
0
1
‚àí2
2
0
0
0
0
0
0
‚éû
‚éü
‚éü
‚é†‚àí‚Üí
‚éõ
‚éú
‚éú
‚éù
1
0
2
0
‚àí2
3
0
1
‚àí1
0
1
1
0
0
0
1
‚àí2
2
0
0
0
0
0
0
‚éû
‚éü
‚éü
‚é†.
The system of linear equations corresponding to this last matrix is
x1
+ 2x3
‚àí2x5 = 3
x2 ‚àíx3
+ x5 = 1
x4 ‚àí2x5 = 2.
Notice that we have ignored the last row since it consists entirely of zeros.
To solve a system for which the augmented matrix is in reduced row
echelon form, divide the variables into two sets.
The Ô¨Årst set consists of
those variables that appear as leftmost variables in one of the equations of
the system (in this case the set is {x1, x2, x4}). The second set consists of
all the remaining variables (in this case, {x3, x5}). To each variable in the
second set, assign a parametric value t1, t2, . . . (x3 = t1, x5 = t2), and then
solve for the variables of the Ô¨Årst set in terms of those in the second set:
x1 = ‚àí2x3 + 2x5 + 3 = ‚àí2t1 + 2t2 + 3
x2 =
x3 ‚àíx5 + 1 =
t1 ‚àít2 + 1
x4 =
2x5 + 2 =
2t2 + 2.
Thus an arbitrary solution is of the form
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
x1
x2
x3
x4
x5
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
=
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
‚àí2t1 + 2t2 + 3
t1 ‚àít2 + 1
t1
2t2 + 2
t2
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
=
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
3
1
0
2
0
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
+ t1
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
‚àí2
1
1
0
0
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
+ t2
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
2
‚àí1
0
2
1
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
,
where t1, t2 ‚ààR. Notice that
‚éß
‚é™
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é™
‚é©
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
‚àí2
1
1
0
0
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
,
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
2
‚àí1
0
2
1
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
‚é´
‚é™
‚é™
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é™
‚é™
‚é≠

Sec. 3.4
Systems of Linear Equations‚ÄîComputational Aspects
189
is a basis for the solution set of the corresponding homogeneous system of
equations and
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
3
1
0
2
0
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
is a particular solution to the original system.
Therefore, in simplifying the augmented matrix of the system to reduced
row echelon form, we are in eÔ¨Äect simultaneously Ô¨Ånding a particular solu-
tion to the original system and a basis for the solution set of the associated
homogeneous system. Moreover, this procedure detects when a system is in-
consistent, for by Exercise 3, solutions exist if and only if, in the reduction of
the augmented matrix to reduced row echelon form, we do not obtain a row
in which the only nonzero entry lies in the last column.
Thus to use this procedure for solving a system Ax = b of m linear equa-
tions in n unknowns, we need only begin to transform the augmented matrix
(A|b) into its reduced row echelon form (A‚Ä≤|b‚Ä≤) by means of Gaussian elimi-
nation. If a row is obtained in which the only nonzero entry lies in the last
column, then the original system is inconsistent. Otherwise, discard any zero
rows from (A‚Ä≤|b‚Ä≤), and write the corresponding system of equations. Solve
this system as described above to obtain an arbitrary solution of the form
s = s0 + t1u1 + t2u2 + ¬∑ ¬∑ ¬∑ + tn‚àírun‚àír,
where r is the number of nonzero rows in A‚Ä≤ (r ‚â§m). The preceding equation
is called a general solution of the system Ax = b. It expresses an arbitrary
solution s of Ax = b in terms of n ‚àír parameters. The following theorem
states that s cannot be expressed in fewer than n ‚àír parameters.
Theorem 3.15. Let Ax = b be a system of r nonzero equations in n
unknowns. Suppose that rank(A) = rank(A|b) and that (A|b) is in reduced
row echelon form. Then
(a) rank(A) = r.
(b) If the general solution obtained by the procedure above is of the form
s = s0 + t1u1 + t2u2 + ¬∑ ¬∑ ¬∑ + tn‚àírun‚àír,
then {u1, u2, . . . , un‚àír} is a basis for the solution set of the correspond-
ing homogeneous system, and s0 is a solution to the original system.
Proof. Since (A|b) is in reduced row echelon form, (A|b) must have r
nonzero rows. Clearly these rows are linearly independent by the deÔ¨Ånition
of the reduced row echelon form, and so rank(A|b) = r. Thus rank(A) = r.

190
Chap. 3
Elementary Matrix Operations and Systems of Linear Equations
Let K be the solution set for Ax = b, and let KH be the solution set for
Ax = 0. Setting t1 = t2 = ¬∑ ¬∑ ¬∑ = tn‚àír = 0, we see that s = s0 ‚ààK. But by
Theorem 3.9 (p. 172), K = {s0} + KH. Hence
KH = {‚àís0} + K = span({u1, u2, . . . , un‚àír}).
Because rank(A) = r, we have dim(KH) = n ‚àír. Thus since dim(KH) = n ‚àír
and KH is generated by a set {u1, u2, . . . , un‚àír} containing at most n ‚àír
vectors, we conclude that this set is a basis for KH.
An Interpretation of the Reduced Row Echelon Form
Let A be an m √ó n matrix with columns a1, a2, . . . , an, and let B be the
reduced row echelon form of A. Denote the columns of B by b1, b2, . . . , bn. If
the rank of A is r, then the rank of B is also r by the corollary to Theorem 3.4
(p. 153). Because B is in reduced row echelon form, no nonzero row of B can
be a linear combination of the other rows of B. Hence B must have exactly
r nonzero rows, and if r ‚â•1, the vectors e1, e2, . . . , er must occur among the
columns of B. For i = 1, 2, . . . , r, let ji denote a column number of B such
that bji = ei. We claim that aj1, aj2, . . . , ajr, the columns of A corresponding
to these columns of B, are linearly independent. For suppose that there are
scalars c1, c2, . . . , cr such that
c1aj1 + c2aj2 + ¬∑ ¬∑ ¬∑ + crajr = 0.
Because B can be obtained from A by a sequence of elementary row oper-
ations, there exists (as in the proof of the corollary to Theorem 3.13) an
invertible m √ó m matrix M such that MA = B. Multiplying the preceding
equation by M yields
c1Maj1 + c2Maj2 + ¬∑ ¬∑ ¬∑ + crMajr = 0.
Since Maji = bji = ei, it follows that
c1e1 + c2e2 + ¬∑ ¬∑ ¬∑ + crer = 0.
Hence c1 = c2 = ¬∑ ¬∑ ¬∑ = cr = 0, proving that the vectors aj1, aj2, . . . , ajr are
linearly independent.
Because B has only r nonzero rows, every column of B has the form
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
d1
d2
...
dr
0
...
0
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†

Sec. 3.4
Systems of Linear Equations‚ÄîComputational Aspects
191
for scalars d1, d2, . . . , dr. The corresponding column of A must be
M ‚àí1(d1e1 + d2e2 + ¬∑ ¬∑ ¬∑ + drer) = d1M ‚àí1e1 + d2M ‚àí1e2 + ¬∑ ¬∑ ¬∑ + drM ‚àí1er
= d1M ‚àí1bj1 + d2M ‚àí1bj2 + ¬∑ ¬∑ ¬∑ + drM ‚àí1bjr
= d1aj1 + d2aj2 + ¬∑ ¬∑ ¬∑ + drajr.
The next theorem summarizes these results.
Theorem 3.16. Let A be an m √ó n matrix of rank r, where r > 0, and
let B be the reduced row echelon form of A. Then
(a) The number of nonzero rows in B is r.
(b) For each i = 1, 2, . . . , r, there is a column bji of B such that bji = ei.
(c) The columns of A numbered j1, j2, . . . , jr are linearly independent.
(d) For each k = 1, 2, . . . n, if column k of B is d1e1 +d2e2 +¬∑ ¬∑ ¬∑+drer, then
column k of A is d1aj1 + d2aj2 + ¬∑ ¬∑ ¬∑ + drajr.
Corollary. The reduced row echelon form of a matrix is unique.
Proof. Exercise. (See Exercise15.)
Example 2
Let
A =
‚éõ
‚éú
‚éú
‚éù
2
4
6
2
4
1
2
3
1
1
2
4
8
0
0
3
6
7
5
9
‚éû
‚éü
‚éü
‚é†.
The reduced row echelon form of A is
B =
‚éõ
‚éú
‚éú
‚éù
1
2
0
4
0
0
0
1
‚àí1
0
0
0
0
0
1
0
0
0
0
0
‚éû
‚éü
‚éü
‚é†.
Since B has three nonzero rows, the rank of A is 3. The Ô¨Årst, third, and Ô¨Åfth
columns of B are e1, e2, and e3; so Theorem 3.16(c) asserts that the Ô¨Årst,
third, and Ô¨Åfth columns of A are linearly independent.
Let the columns of A be denoted a1, a2, a3, a4, and a5. Because the second
column of B is 2e1, it follows from Theorem 3.16(d) that a2 = 2a1, as is easily
checked. Moreover, since the fourth column of B is 4e1 + (‚àí1)e2, the same
result shows that
a4 = 4a1 + (‚àí1)a3.
‚ô¶

192
Chap. 3
Elementary Matrix Operations and Systems of Linear Equations
In Example 6 of Section 1.6, we extracted a basis for R3 from the gener-
ating set
S = {(2, ‚àí3, 5), (8, ‚àí12, 20), (1, 0, ‚àí2), (0, 2, ‚àí1), (7, 2, 0)}.
The procedure described there can be streamlined by using Theorem 3.16.
We begin by noting that if S were linearly independent, then S would be a
basis for R3. In this case, it is clear that S is linearly dependent because
S contains more than dim(R3) = 3 vectors. Nevertheless, it is instructive
to consider the calculation that is needed to determine whether S is linearly
dependent or linearly independent.
Recall that S is linearly dependent if
there are scalars c1, c2, c3, c4, and c5, not all zero, such that
c1(2, ‚àí3, 5)+c2(8, ‚àí12, 20)+c3(1, 0, ‚àí2)+c4(0, 2, ‚àí1)+c5(7, 2, 0) = (0, 0, 0).
Thus S is linearly dependent if and only if the system of linear equations
2c1 + 8c2 + c3
+ 7c5 = 0
‚àí3c1 ‚àí12c2
+ 2c4 + 2c5 = 0
5c1 + 20c2 ‚àí2c3 ‚àíc4
= 0
has a nonzero solution. The augmented matrix of this system of equations is
A =
‚éõ
‚éù
2
8
1
0
7
0
‚àí3
‚àí12
0
2
2
0
5
20
‚àí2
‚àí1
0
0
‚éû
‚é†,
and its reduced row echelon form is
B =
‚éõ
‚éù
1
4
0
0
2
0
0
0
1
0
3
0
0
0
0
1
4
0
‚éû
‚é†.
Using the technique described earlier in this section, we can Ô¨Ånd nonzero
solutions of the preceding system, conÔ¨Årming that S is linearly dependent.
However, Theorem 3.16(c) gives us additional information. Since the Ô¨Årst,
third, and fourth columns of B are e1, e2, and e3, we conclude that the Ô¨Årst,
third, and fourth columns of A are linearly independent. But the columns
of A other than the last column (which is the zero vector) are vectors in S.
Hence
Œ≤ = {(2, ‚àí3, 5), (1, 0, ‚àí2), (0, 2, ‚àí1)}
is a linearly independent subset of S. If follows from (b) of Corollary 2 to the
replacement theorem (p. 47) that Œ≤ is a basis for R3.
Because every Ô¨Ånite-dimensional vector space over F is isomorphic to Fn
for some n, a similar approach can be used to reduce any Ô¨Ånite generating
set to a basis. This technique is illustrated in the next example.

Sec. 3.4
Systems of Linear Equations‚ÄîComputational Aspects
193
Example 3
The set
S ={2+x+2x2+3x3, 4+2x+4x2+6x3, 6+3x+8x2+7x3, 2+x+5x3, 4+x+9x3}
generates a subspace V of P3(R). To Ô¨Ånd a subset of S that is a basis for V,
we consider the subset
S‚Ä≤ = {(2, 1, 2, 3), (4, 2, 4, 6), (6, 3, 8, 7), (2, 1, 0, 5), (4, 1, 0, 9)}
consisting of the images of the polynomials in S under the standard repre-
sentation of P3(R) with respect to the standard ordered basis. Note that the
4 √ó 5 matrix in which the columns are the vectors in S‚Ä≤ is the matrix A in
Example 2. From the reduced row echelon form of A, which is the matrix B
in Example 2, we see that the Ô¨Årst, third, and Ô¨Åfth columns of A are linearly
independent and the second and fourth columns of A are linear combinations
of the Ô¨Årst, third, and Ô¨Åfth columns. Hence
{(2, 1, 2, 3), (6, 3, 8, 7), (4, 1, 0, 9)}
is a basis for the subspace of R4 that is generated by S‚Ä≤. It follows that
{2 + x + 2x2 + 3x3, 6 + 3x + 8x2 + 7x3, 4 + x + 9x3}
is a basis for the subspace V of P3(R).
‚ô¶
We conclude this section by describing a method for extending a linearly
independent subset S of a Ô¨Ånite-dimensional vector space V to a basis for V.
Recall that this is always possible by (c) of Corollary 2 to the replacement
theorem (p. 47). Our approach is based on the replacement theorem and
assumes that we can Ô¨Ånd an explicit basis Œ≤ for V. Let S‚Ä≤ be the ordered set
consisting of the vectors in S followed by those in Œ≤. Since Œ≤ ‚äÜS‚Ä≤, the set
S‚Ä≤ generates V. We can then apply the technique described above to reduce
this generating set to a basis for V containing S.
Example 4
Let
V = {(x1, x2, x3, x4, x5) ‚ààR5 : x1 + 7x2 + 5x3 ‚àí4x4 + 2x5 = 0}.
It is easily veriÔ¨Åed that V is a subspace of R5 and that
S = {(‚àí2, 0, 0, ‚àí1, ‚àí1), (1, 1, ‚àí2, ‚àí1, ‚àí1), (‚àí5, 1, 0, 1, 1)}
is a linearly independent subset of V.

194
Chap. 3
Elementary Matrix Operations and Systems of Linear Equations
To extend S to a basis for V, we Ô¨Årst obtain a basis Œ≤ for V. To do so,
we solve the system of linear equations that deÔ¨Ånes V. Since in this case V is
deÔ¨Åned by a single equation, we need only write the equation as
x1 = ‚àí7x2 ‚àí5x3 + 4x4 ‚àí2x5
and assign parametric values to x2, x3, x4, and x5. If x2 = t1, x3 = t2,
x4 = t3, and x5 = t4, then the vectors in V have the form
(x1,x2, x3, x4, x5) = (‚àí7t1 ‚àí5t2 + 4t3 ‚àí2t4, t1, t2, t3, t4)
= t1(‚àí7, 1, 0, 0, 0) + t2(‚àí5, 0, 1, 0, 0) + t3(4, 0, 0, 1, 0) + t4(‚àí2, 0, 0, 0, 1).
Hence
Œ≤ = {(‚àí7, 1, 0, 0, 0), (‚àí5, 0, 1, 0, 0), (4, 0, 0, 1, 0), (‚àí2, 0, 0, 0, 1)}
is a basis for V by Theorem 3.15.
The matrix whose columns consist of the vectors in S followed by those
in Œ≤ is
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
‚àí2
1
‚àí5
‚àí7
‚àí5
4
‚àí2
0
1
1
1
0
0
0
0
‚àí2
0
0
1
0
0
‚àí1
‚àí1
1
0
0
1
0
‚àí1
‚àí1
1
0
0
0
1
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
,
and its reduced row echelon form is
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
1
0
0
1
1
0
‚àí1
0
1
0
0
‚àí.5
0
0
0
0
1
1
.5
0
0
0
0
0
0
0
1
‚àí1
0
0
0
0
0
0
0
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
.
Thus
{(‚àí2, 0, 0, ‚àí1, ‚àí1), (1, 1, ‚àí2, ‚àí1, ‚àí1), (‚àí5, 1, 0, 1, 1), (4, 0, 0, 1, 0)}
is a basis for V containing S.
‚ô¶
EXERCISES
1.
Label the following statements as true or false.
(a)
If (A‚Ä≤|b‚Ä≤) is obtained from (A|b) by a Ô¨Ånite sequence of elementary
column operations, then the systems Ax = b and A‚Ä≤x = b‚Ä≤ are
equivalent.

Sec. 3.4
Systems of Linear Equations‚ÄîComputational Aspects
195
(b)
If (A‚Ä≤|b‚Ä≤) is obtained from (A|b) by a Ô¨Ånite sequence of elemen-
tary row operations, then the systems Ax = b and A‚Ä≤x = b‚Ä≤ are
equivalent.
(c)
If A is an n √ó n matrix with rank n, then the reduced row echelon
form of A is In.
(d)
Any matrix can be put in reduced row echelon form by means of
a Ô¨Ånite sequence of elementary row operations.
(e)
If (A|b) is in reduced row echelon form, then the system Ax = b is
consistent.
(f)
Let Ax = b be a system of m linear equations in n unknowns for
which the augmented matrix is in reduced row echelon form. If
this system is consistent, then the dimension of the solution set of
Ax = 0 is n ‚àír, where r equals the number of nonzero rows in A.
(g)
If a matrix A is transformed by elementary row operations into a
matrix A‚Ä≤ in reduced row echelon form, then the number of nonzero
rows in A‚Ä≤ equals the rank of A.
2.
Use Gaussian elimination to solve the following systems of linear equa-
tions.
(a)
x1 + 2x2 ‚àíx3 = ‚àí1
2x1 + 2x2 + x3 =
1
3x1 + 5x2 ‚àí2x3 = ‚àí1
(b)
x1 ‚àí2x2 ‚àíx3 = 1
2x1 ‚àí3x2 + x3 = 6
3x1 ‚àí5x2
= 7
x1
+ 5x3 = 9
(c)
x1 + 2x2
+ 2x4 = 6
3x1 + 5x2 ‚àíx3 + 6x4 = 17
2x1 + 4x2 + x3 + 2x4 = 12
2x1
‚àí7x3 + 11x4 = 7
(d)
x1 ‚àíx2 ‚àí2x3 + 3x4 = ‚àí7
2x1 ‚àíx2 + 6x3 + 6x4 = ‚àí2
‚àí2x1 + x2 ‚àí4x3 ‚àí3x4 =
0
3x1 ‚àí2x2 + 9x3 + 10x4 = ‚àí5
(e)
x1 ‚àí4x2 ‚àíx3 + x4 =
3
2x1 ‚àí8x2 + x3 ‚àí4x4 =
9
‚àíx1 + 4x2 ‚àí2x3 + 5x4 = ‚àí6
(f)
x1 + 2x2 ‚àíx3 + 3x4 = 2
2x1 + 4x2 ‚àíx3 + 6x4 = 5
x2
+ 2x4 = 3
(g)
2x1 ‚àí2x2 ‚àíx3 + 6x4 ‚àí2x5 = 1
x1 ‚àíx2 + x3 + 2x4 ‚àíx5 = 2
4x1 ‚àí4x2 + 5x3 + 7x4 ‚àíx5 = 6
(h)
3x1 ‚àíx2 + x3 ‚àíx4 + 2x5 = 5
x1 ‚àíx2 ‚àíx3 ‚àí2x4 ‚àíx5 = 2
5x1 ‚àí2x2 + x3 ‚àí3x4 + 3x5 = 10
2x1 ‚àíx2
‚àí2x4 + x5 = 5

196
Chap. 3
Elementary Matrix Operations and Systems of Linear Equations
(i)
3x1 ‚àíx2 + 2x3 + 4x4 + x5 =
2
x1 ‚àíx2 + 2x3 + 3x4 + x5 = ‚àí1
2x1 ‚àí3x2 + 6x3 + 9x4 + 4x5 = ‚àí5
7x1 ‚àí2x2 + 4x3 + 8x4 + x5 =
6
(j)
2x1
+ 3x3
‚àí4x5 =
5
3x1 ‚àí4x2 + 8x3 + 3x4
=
8
x1 ‚àíx2 + 2x3 + x4 ‚àíx5 =
2
‚àí2x1 + 5x2 ‚àí9x3 ‚àí3x4 ‚àí5x5 = ‚àí8
3.
Suppose that the augmented matrix of a system Ax = b is transformed
into a matrix (A‚Ä≤|b‚Ä≤) in reduced row echelon form by a Ô¨Ånite sequence
of elementary row operations.
(a)
Prove that rank(A‚Ä≤) Ã∏= rank(A‚Ä≤|b‚Ä≤) if and only if (A‚Ä≤|b‚Ä≤) contains a
row in which the only nonzero entry lies in the last column.
(b)
Deduce that Ax = b is consistent if and only if (A‚Ä≤|b‚Ä≤) contains no
row in which the only nonzero entry lies in the last column.
4.
For each of the systems that follow, apply Exercise 3 to determine
whether the system is consistent. If the system is consistent, Ô¨Ånd all
solutions. Finally, Ô¨Ånd a basis for the solution set of the corresponding
homogeneous system.
(a)
x1 + 2x2 ‚àíx3 + x4 = 2
2x1 + x2 + x3 ‚àíx4 = 3
x1 + 2x2 ‚àí3x3 + 2x4 = 2
(b)
x1 + x2 ‚àí3x3 + x4 = ‚àí2
x1 + x2 + x3 ‚àíx4 =
2
x1 + x2 ‚àíx3
=
0
(c)
x1 + x2 ‚àí3x3 + x4 = 1
x1 + x2 + x3 ‚àíx4 = 2
x1 + x2 ‚àíx3
= 0
5.
Let the reduced row echelon form of A be
‚éõ
‚éù
1
0
2
0
‚àí2
0
1
‚àí5
0
‚àí3
0
0
0
1
6
‚éû
‚é†.
Determine A if the Ô¨Årst, second, and fourth columns of A are
‚éõ
‚éù
1
‚àí1
3
‚éû
‚é†,
‚éõ
‚éù
0
‚àí1
1
‚éû
‚é†,
and
‚éõ
‚éù
1
‚àí2
0
‚éû
‚é†,
respectively.
6.
Let the reduced row echelon form of A be
‚éõ
‚éú
‚éú
‚éù
1
‚àí3
0
4
0
5
0
0
1
3
0
2
0
0
0
0
1
‚àí1
0
0
0
0
0
0
‚éû
‚éü
‚éü
‚é†.

Sec. 3.4
Systems of Linear Equations‚ÄîComputational Aspects
197
Determine A if the Ô¨Årst, third, and sixth columns of A are
‚éõ
‚éú
‚éú
‚éù
1
‚àí2
‚àí1
3
‚éû
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éù
‚àí1
1
2
‚àí4
‚éû
‚éü
‚éü
‚é†,
and
‚éõ
‚éú
‚éú
‚éù
3
‚àí9
2
5
‚éû
‚éü
‚éü
‚é†,
respectively.
7.
It can be shown that the vectors u1 = (2, ‚àí3, 1), u2 = (1, 4, ‚àí2), u3 =
(‚àí8, 12, ‚àí4), u4 = (1, 37, ‚àí17), and u5 = (‚àí3, ‚àí5, 8) generate R3. Find
a subset of {u1, u2, u3, u4, u5} that is a basis for R3.
8.
Let W denote the subspace of R5 consisting of all vectors having coor-
dinates that sum to zero. The vectors
u1 = (2, ‚àí3, 4, ‚àí5, 2),
u2 = (‚àí6, 9, ‚àí12, 15, ‚àí6),
u3 = (3, ‚àí2, 7, ‚àí9, 1),
u4 = (2, ‚àí8, 2, ‚àí2, 6),
u5 = (‚àí1, 1, 2, 1, ‚àí3),
u6 = (0, ‚àí3, ‚àí18, 9, 12),
u7 = (1, 0, ‚àí2, 3, ‚àí2),
and
u8 = (2, ‚àí1, 1, ‚àí9, 7)
generate W. Find a subset of {u1, u2, . . . , u8} that is a basis for W.
9.
Let W be the subspace of M2√ó2(R) consisting of the symmetric 2 √ó 2
matrices. The set
S =

0
‚àí1
‚àí1
1
	
,

1
2
2
3
	
,

2
1
1
9
	
,

1
‚àí2
‚àí2
4
	
,

‚àí1
2
2
‚àí1
	
generates W. Find a subset of S that is a basis for W.
10.
Let
V = {(x1, x2, x3, x4, x5) ‚ààR5 : x1 ‚àí2x2 + 3x3 ‚àíx4 + 2x5 = 0}.
(a)
Show that S = {(0, 1, 1, 1, 0)} is a linearly independent subset of
V.
(b)
Extend S to a basis for V.
11.
Let V be as in Exercise 10.
(a)
Show that S = {(1, 2, 1, 0, 0)} is a linearly independent subset of
V.
(b)
Extend S to a basis for V.
12.
Let V denote the set of all solutions to the system of linear equations
x1 ‚àíx2
+ 2x4 ‚àí3x5 + x6 = 0
2x1 ‚àíx2 ‚àíx3 + 3x4 ‚àí4x5 + 4x6 = 0.

198
Chap. 3
Elementary Matrix Operations and Systems of Linear Equations
(a)
Show that S = {(0, ‚àí1, 0, 1, 1, 0), (1, 0, 1, 1, 1, 0)} is a linearly inde-
pendent subset of V.
(b)
Extend S to a basis for V.
13.
Let V be as in Exercise 12.
(a)
Show that S = {(1, 0, 1, 1, 1, 0), (0, 2, 1, 1, 0, 0)} is a linearly inde-
pendent subset of V.
(b)
Extend S to a basis for V.
14.
If (A|b) is in reduced row echelon form, prove that A is also in reduced
row echelon form.
15.
Prove the corollary to Theorem 3.16: The reduced row echelon form of
a matrix is unique.
INDEX OF DEFINITIONS FOR CHAPTER 3
Augmented matrix
161
Augmented matrix of a system of lin-
ear equations
174
Backward pass
186
Closed model of a simple economy
176
CoeÔ¨Écient matrix of a system of lin-
ear equations
169
Consistent system of linear equations
169
Elementary column operation
148
Elementary matrix
149
Elementary operation
148
Elementary row operation
148
Equilibrium condition for a simple
economy
177
Equivalent systems of linear equa-
tions
182
Forward pass
186
Gaussian elimination
186
General solution of a system of linear
equations
189
Homogeneous
system
correspond-
ing to a nonhomogeneous system
172
Homogeneous system of linear equa-
tions
171
Inconsistent system of linear equa-
tions
169
Input‚Äìoutput matrix
177
Nonhomogeneous system of linear
equations
171
Nonnegative vector
177
Open model of a simple economy
178
Positive matrix
177
Rank of a matrix
152
Reduced row echelon form of a ma-
trix
185
Solution to a system of linear equa-
tions
169
Solution set of a system of equations
169
System of linear equations
169
Type 1, 2, and 3 elementary opera-
tions
148

4
Determinants
4.1
Determinants of Order 2
4.2
Determinants of Order n
4.3
Properties of Determinants
4.4
Summary ‚Äî Important Facts about Determinants
4.5*
A Characterization of the Determinant
The determinant, which has played a prominent role in the theory of lin-
ear algebra, is a special scalar-valued function deÔ¨Åned on the set of square
matrices. Although it still has a place in the study of linear algebra and its
applications, its role is less central than in former times. Yet no linear algebra
book would be complete without a systematic treatment of the determinant,
and we present one here. However, the main use of determinants in this book
is to compute and establish the properties of eigenvalues, which we discuss in
Chapter 5.
Although the determinant is not a linear transformation on Mn√ón(F)
for n > 1, it does possess a kind of linearity (called n-linearity) as well
as other properties that are examined in this chapter.
In Section 4.1, we
consider the determinant on the set of 2√ó2 matrices and derive its important
properties and develop an eÔ¨Écient computational procedure. To illustrate the
important role that determinants play in geometry, we also include optional
material that explores the applications of the determinant to the study of
area and orientation. In Sections 4.2 and 4.3, we extend the deÔ¨Ånition of the
determinant to all square matrices and derive its important properties and
develop an eÔ¨Écient computational procedure. For the reader who prefers to
treat determinants lightly, Section 4.4 contains the essential properties that
are needed in later chapters. Finally, Section 4.5, which is optional, oÔ¨Äers
an axiomatic approach to determinants by showing how to characterize the
determinant in terms of three key properties.
4.1
DETERMINANTS OF ORDER 2
In this section, we deÔ¨Åne the determinant of a 2 √ó 2 matrix and investigate
its geometric signiÔ¨Åcance in terms of area and orientation.
199

200
Chap. 4
Determinants
DeÔ¨Ånition. If
A =

a
b
c
d
	
is a 2√ó2 matrix with entries from a Ô¨Åeld F, then we deÔ¨Åne the determinant
of A, denoted det(A) or |A|, to be the scalar ad ‚àíbc.
Example 1
For the matrices
A =

1
2
3
4
	
and
B =

3
2
6
4
	
in M2√ó2(R), we have
det(A) = 1¬∑4 ‚àí2¬∑3 = ‚àí2 and det(B) = 3¬∑4 ‚àí2¬∑6 = 0.
‚ô¶
For the matrices A and B in Example 1, we have
A + B =

4
4
9
8
	
,
and so
det(A + B) = 4¬∑8 ‚àí4¬∑9 = ‚àí4.
Since det(A + B) Ã∏= det(A) + det(B), the function det: M2√ó2(R) ‚ÜíR is
not a linear transformation. Nevertheless, the determinant does possess an
important linearity property, which is explained in the following theorem.
Theorem 4.1. The function det: M2√ó2(F) ‚ÜíF is a linear function of
each row of a 2 √ó 2 matrix when the other row is held Ô¨Åxed. That is, if u, v,
and w are in F2 and k is a scalar, then
det

u + kv
w
	
= det

u
w
	
+ k det

v
w
	
and
det

w
u + kv
	
= det

w
u
	
+ k det

w
v
	
.
Proof. Let u = (a1, a2), v = (b1, b2), and w = (c1, c2) be in F2 and k be a
scalar. Then
det

u
w
	
+ k det

v
w
	
= det

a1
a2
c1
c2
	
+ k det

b1
b2
c1
c2
	

Sec. 4.1
Determinants of Order 2
201
= (a1c2 ‚àía2c1) + k(b1c2 ‚àíb2c1)
= (a1 + kb1)c2 ‚àí(a2 + kb2)c1
= det

a1 + kb1
a2 + kb2
c1
c2
	
= det

u + kv
w
	
.
A similar calculation shows that
det

w
u
	
+ k det

w
v
	
= det

w
u + kv
	
.
For the 2 √ó 2 matrices A and B in Example 1, it is easily checked that A
is invertible but B is not. Note that det(A) Ã∏= 0 but det(B) = 0. We now
show that this property is true in general.
Theorem 4.2. Let A ‚ààM2√ó2(F). Then the determinant of A is nonzero
if and only if A is invertible. Moreover, if A is invertible, then
A‚àí1 =
1
det(A)

A22
‚àíA12
‚àíA21
A11
	
.
Proof. If det(A) Ã∏= 0, then we can deÔ¨Åne a matrix
M =
1
det(A)

A22
‚àíA12
‚àíA21
A11
	
.
A straightforward calculation shows that AM = MA = I, and so A is invert-
ible and M = A‚àí1.
Conversely, suppose that A is invertible. A remark on page 152 shows
that the rank of
A =

A11
A12
A21
A22
	
must be 2. Hence A11 Ã∏= 0 or A21 Ã∏= 0. If A11 Ã∏= 0, add ‚àíA21/A11 times row 1
of A to row 2 to obtain the matrix
‚éõ
‚éù
A11
A12
0
A22 ‚àíA12A21
A11
‚éû
‚é†.
Because elementary row operations are rank-preserving by the corollary to
Theorem 3.4 (p. 153), it follows that
A22 ‚àíA12A21
A11
Ã∏= 0.

202
Chap. 4
Determinants
Therefore det(A) = A11A22 ‚àíA12A21 Ã∏= 0. On the other hand, if A21 Ã∏= 0,
we see that det(A) Ã∏= 0 by adding ‚àíA11/A21 times row 2 of A to row 1 and
applying a similar argument. Thus, in either case, det(A) Ã∏= 0.
In Sections 4.2 and 4.3, we extend the deÔ¨Ånition of the determinant to
n √ó n matrices and show that Theorem 4.2 remains true in this more general
context. In the remainder of this section, which can be omitted if desired,
we explore the geometric signiÔ¨Åcance of the determinant of a 2 √ó 2 matrix.
In particular, we show the importance of the sign of the determinant in the
study of orientation.
The Area of a Parallelogram
By the angle between two vectors in R2, we mean the angle with measure
Œ∏ (0 ‚â§Œ∏ < œÄ) that is formed by the vectors having the same magnitude and
direction as the given vectors but emanating from the origin. (See Figure 4.1.)




@
@
@
@
I




@
@
@
@
I
-
6
.................................................................
Œ∏
y
x
Figure 4.1: Angle between two vectors in R2
If Œ≤ = {u, v} is an ordered basis for R2, we deÔ¨Åne the orientation of Œ≤
to be the real number
O

u
v
	
=
det

u
v
	
####det

u
v
	####
.
(The denominator of this fraction is nonzero by Theorem 4.2.) Clearly
O

u
v
	
= ¬±1.
Notice that
O

e1
e2
	
= 1
and
O

e1
‚àíe2
	
= ‚àí1.
Recall that a coordinate system {u, v} is called right-handed if u can
be rotated in a counterclockwise direction through an angle Œ∏ (0 < Œ∏ < œÄ)

Sec. 4.1
Determinants of Order 2
203
to coincide with v. Otherwise {u, v} is called a left-handed system. (See
Figure 4.2.) In general (see Exercise 12),


XXX
z-
6
y
x
u
v
A right-handed coordinate system

*
@
@
@
I
-
6
y
x
u
v
A left-handed coordinate system
Figure 4.2
O

u
v
	
= 1
if and only if the ordered basis {u, v} forms a right-handed coordinate system.
For convenience, we also deÔ¨Åne
O

u
v
	
= 1
if {u, v} is linearly dependent.
Any ordered set {u, v} in R2 determines a parallelogram in the following
manner. Regarding u and v as arrows emanating from the origin of R2, we
call the parallelogram having u and v as adjacent sides the parallelogram
determined by u and v. (See Figure 4.3.) Observe that if the set {u, v}
v
u
x
y
v
u
x
y
Figure 4.3: Parallelograms determined by u and v
is linearly dependent (i.e., if u and v are parallel), then the ‚Äúparallelogram‚Äù
determined by u and v is actually a line segment, which we consider to be a
degenerate parallelogram having area zero.

204
Chap. 4
Determinants
There is an interesting relationship between
A

u
v
	
,
the area of the parallelogram determined by u and v, and
det

u
v
	
,
which we now investigate. Observe Ô¨Årst, however, that since
det

u
v
	
may be negative, we cannot expect that
A

u
v
	
= det

u
v
	
.
But we can prove that
A

u
v
	
= O

u
v
	
¬∑ det

u
v
	
,
from which it follows that
A

u
v
	
=
####det

u
v
	#### .
Our argument that
A

u
v
	
= O

u
v
	
¬∑ det

u
v
	
employs a technique that, although somewhat indirect, can be generalized to
Rn. First, since
O

u
v
	
= ¬±1,
we may multiply both sides of the desired equation by
O

u
v
	
to obtain the equivalent form
O

u
v
	
¬∑A

u
v
	
= det

u
v
	
.

Sec. 4.1
Determinants of Order 2
205
We establish this equation by verifying that the three conditions of Exercise 11
are satisÔ¨Åed by the function
Œ¥

u
v
	
= O

u
v
	
¬∑A

u
v
	
.
(a) We begin by showing that for any real number c
Œ¥

u
cv
	
= c¬∑Œ¥

u
v
	
.
Observe that this equation is valid if c = 0 because
Œ¥

u
cv
	
= O

u
0
	
¬∑A

u
0
	
= 1¬∑0 = 0.
So assume that c Ã∏= 0. Regarding cv as the base of the parallelogram deter-
mined by u and cv, we see that
A

u
cv
	
= base √ó altitude = |c|(length of v)(altitude) = |c|¬∑A

u
v
	
,
since the altitude h of the parallelogram determined by u and cv is the same
as that in the parallelogram determined by u and v. (See Figure 4.4.) Hence






-
-












v
cv
h
u
Figure 4.4
Œ¥

u
cv
	
= O

u
cv
	
¬∑A

u
cv
	
=
$ c
|c| ¬∑O

u
v
	% $
|c|¬∑A

u
v
	%
= c¬∑O

u
v
	
¬∑A

u
v
	
= c¬∑Œ¥

u
v
	
.
A similar argument shows that
Œ¥

cu
v
	
= c¬∑Œ¥

u
v
	
.

206
Chap. 4
Determinants
We next prove that
Œ¥

u
au + bw
	
= b¬∑Œ¥

u
w
	
for any u, w ‚ààR2 and any real numbers a and b. Because the parallelograms
determined by u and w and by u and u + w have a common base u and the
same altitude (see Figure 4.5), it follows that






*
-







u
u + w
w
Figure 4.5
A

u
w
	
= A

u
u + w
	
.
If a = 0, then
Œ¥

u
au + bw
	
= Œ¥

u
bw
	
= b¬∑Œ¥

u
w
	
by the Ô¨Årst paragraph of (a). Otherwise, if a Ã∏= 0, then
Œ¥

u
au + bw

= a¬∑Œ¥
‚éõ
‚éù
u
u + b
aw
‚éû
‚é†= a¬∑Œ¥
‚éõ
‚éùu
b
aw
‚éû
‚é†= b¬∑Œ¥

u
w

.
So the desired conclusion is obtained in either case.
We are now able to show that
Œ¥

u
v1 + v2
	
= Œ¥

u
v1
	
+ Œ¥

u
v2
	
for all u, v1, v2 ‚ààR2. Since the result is immediate if u = 0, we assume that
u Ã∏= 0. Choose any vector w ‚ààR2 such that {u, w} is linearly independent.
Then for any vectors v1, v2 ‚ààR2 there exist scalars ai and bi such that
vi = aiu + biw (i = 1, 2). Thus
Œ¥

u
v1 + v2
	
= Œ¥

u
(a1 + a2)u + (b1 + b2)w
	
= (b1 + b2)Œ¥

u
w
	

Sec. 4.1
Determinants of Order 2
207
= Œ¥

u
a1u + b1w
	
+ Œ¥

u
a2u + b2w
	
= Œ¥

u
v1
	
+ Œ¥

u
v2
	
.
A similar argument shows that
Œ¥

u1 + u2
v
	
= Œ¥

u1
v
	
+ Œ¥

u2
v
	
for all u1, u2, v ‚ààR2.
(b) Since
A

u
u
	
= 0,
it follows that
Œ¥

u
u
	
= O

u
u
	
¬∑A

u
u
	
= 0
for any u ‚ààR2.
(c) Because the parallelogram determined by e1 and e2 is the unit square,
Œ¥

e1
e2
	
= O

e1
e2
	
¬∑A

e1
e2
	
= 1 ¬∑ 1 = 1.
Therefore Œ¥ satisÔ¨Åes the three conditions of Exercise 11, and hence Œ¥ = det.
So the area of the parallelogram determined by u and v equals
O

u
v
	
¬∑ det

u
v
	
.
Thus we see, for example, that the area of the parallelogram determined
by u = (‚àí1, 5) and v = (4, ‚àí2) is
####det

u
v
	#### =
####det

‚àí1
5
4
‚àí2
	#### = 18.
EXERCISES
1.
Label the following statements as true or false.
(a)
The function det: M2√ó2(F) ‚ÜíF is a linear transformation.
(b)
The determinant of a 2 √ó 2 matrix is a linear function of each row
of the matrix when the other row is held Ô¨Åxed.
(c)
If A ‚ààM2√ó2(F) and det(A) = 0, then A is invertible.
(d)
If u and v are vectors in R2 emanating from the origin, then the
area of the parallelogram having u and v as adjacent sides is
det

u
v
	
.

208
Chap. 4
Determinants
(e)
A coordinate system is right-handed if and only if its orientation
equals 1.
2.
Compute the determinants of the following matrices in M2√ó2(R).
(a)

6
‚àí3
2
4
	
(b)

‚àí5
2
6
1
	
(c)

8
0
3
‚àí1
	
3.
Compute the determinants of the following matrices in M2√ó2(C).
(a)

‚àí1 + i
1 ‚àí4i
3 + 2i
2 ‚àí3i
	
(b)

5 ‚àí2i
6 + 4i
‚àí3 + i
7i
	
(c)

2i
3
4
6i
	
4.
For each of the following pairs of vectors u and v in R2, compute the
area of the parallelogram determined by u and v.
(a)
u = (3, ‚àí2) and v = (2, 5)
(b)
u = (1, 3) and v = (‚àí3, 1)
(c)
u = (4, ‚àí1) and v = (‚àí6, ‚àí2)
(d)
u = (3, 4) and v = (2, ‚àí6)
5.
Prove that if B is the matrix obtained by interchanging the rows of a
2 √ó 2 matrix A, then det(B) = ‚àídet(A).
6.
Prove that if the two columns of A ‚ààM2√ó2(F) are identical, then
det(A) = 0.
7.
Prove that det(At) = det(A) for any A ‚ààM2√ó2(F).
8.
Prove that if A ‚ààM2√ó2(F) is upper triangular, then det(A) equals the
product of the diagonal entries of A.
9.
Prove that det(AB) = det(A)¬∑ det(B) for any A, B ‚ààM2√ó2(F).
10.
The classical adjoint of a 2 √ó 2 matrix A ‚ààM2√ó2(F) is the matrix
C =

A22
‚àíA12
‚àíA21
A11
	
.
Prove that
(a)
CA = AC = [det(A)]I.
(b)
det(C) = det(A).
(c)
The classical adjoint of At is Ct.
(d)
If A is invertible, then A‚àí1 = [det(A)]‚àí1C.
11.
Let Œ¥: M2√ó2(F) ‚ÜíF be a function with the following three properties.
(i) Œ¥ is a linear function of each row of the matrix when the other row
is held Ô¨Åxed.
(ii)
If the two rows of A ‚ààM2√ó2(F) are identical, then Œ¥(A) = 0.

Sec. 4.2
Determinants of Order n
209
(iii)
If I is the 2 √ó 2 identity matrix, then Œ¥(I) = 1.
Prove that Œ¥(A) = det(A) for all A ‚ààM2√ó2(F). (This result is general-
ized in Section 4.5.)
12.
Let {u, v} be an ordered basis for R2. Prove that
O

u
v
	
= 1
if and only if {u, v} forms a right-handed coordinate system.
Hint:
Recall the deÔ¨Ånition of a rotation given in Example 2 of Section 2.1.
4.2
DETERMINANTS OF ORDER n
In this section, we extend the deÔ¨Ånition of the determinant to n √ó n matrices
for n ‚â•3.
For this deÔ¨Ånition, it is convenient to introduce the following
notation: Given A ‚ààMn√ón(F), for n ‚â•2, denote the (n‚àí1)√ó(n‚àí1) matrix
obtained from A by deleting row i and column j by ÀúAij. Thus for
A =
‚éõ
‚éù
1
2
3
4
5
6
7
8
9
‚éû
‚é†‚ààM3√ó3(R),
we have
ÀúA11 =

5
6
8
9
	
,
ÀúA13 =

4
5
7
8
	
,
and
ÀúA32 =

1
3
4
6
	
,
and for
B =
‚éõ
‚éú
‚éú
‚éù
1
‚àí1
2
‚àí1
‚àí3
4
1
‚àí1
2
‚àí5
‚àí3
8
‚àí2
6
‚àí4
1
‚éû
‚éü
‚éü
‚é†‚ààM4√ó4(R),
we have
ÀúB23 =
‚éõ
‚éù
1
‚àí1
‚àí1
2
‚àí5
8
‚àí2
6
1
‚éû
‚é†
and
ÀúB42 =
‚éõ
‚éù
1
2
‚àí1
‚àí3
1
‚àí1
2
‚àí3
8
‚éû
‚é†.
DeÔ¨Ånitions.
Let A ‚ààMn√ón(F). If n = 1, so that A = (A11), we deÔ¨Åne
det(A) = A11. For n ‚â•2, we deÔ¨Åne det(A) recursively as
det(A) =
n

j=1
(‚àí1)1+jA1j ¬∑ det( ÀúA1j).

210
Chap. 4
Determinants
The scalar det(A) is called the determinant of A and is also denoted by |A|.
The scalar
(‚àí1)i+j det( ÀúAij)
is called the cofactor of the entry of A in row i, column j.
Letting
cij = (‚àí1)i+j det( ÀúAij)
denote the cofactor of the row i, column j entry of A, we can express the
formula for the determinant of A as
det(A) = A11c11 + A12c12 + ¬∑ ¬∑ ¬∑ + A1nc1n.
Thus the determinant of A equals the sum of the products of each entry in row
1 of A multiplied by its cofactor. This formula is called cofactor expansion
along the Ô¨Årst row of A. Note that, for 2 √ó 2 matrices, this deÔ¨Ånition of
the determinant of A agrees with the one given in Section 4.1 because
det(A) = A11(‚àí1)1+1 det( ÀúA11) + A12(‚àí1)1+2 det( ÀúA12) = A11A22 ‚àíA12A21.
Example 1
Let
A =
‚éõ
‚éù
1
3
‚àí3
‚àí3
‚àí5
2
‚àí4
4
‚àí6
‚éû
‚é†‚ààM3√ó3(R).
Using cofactor expansion along the Ô¨Årst row of A, we obtain
det(A) = (‚àí1)1+1A11 ¬∑ det( ÀúA11) + (‚àí1)1+2A12 ¬∑ det( ÀúA12)
+ (‚àí1)1+3A13 ¬∑ det( ÀúA13)
= (‚àí1)2(1)¬∑ det

‚àí5
2
4
‚àí6
	
+ (‚àí1)3(3)¬∑

‚àí3
2
‚àí4
‚àí6
	
+ (‚àí1)4(‚àí3)¬∑ det

‚àí3
‚àí5
‚àí4
4
	
= 1 [‚àí5(‚àí6) ‚àí2(4)] ‚àí3 [‚àí3(‚àí6) ‚àí2(‚àí4)] ‚àí3 [‚àí3(4) ‚àí(‚àí5)(‚àí4)]
= 1(22) ‚àí3(26) ‚àí3(‚àí32)
= 40.
‚ô¶

Sec. 4.2
Determinants of Order n
211
Example 2
Let
B =
‚éõ
‚éù
0
1
3
‚àí2
‚àí3
‚àí5
4
‚àí4
4
‚éû
‚é†‚ààM3√ó3(R).
Using cofactor expansion along the Ô¨Årst row of B, we obtain
det(B) = (‚àí1)1+1B11 ¬∑ det( ÀúB11) + (‚àí1)1+2B12 ¬∑ det( ÀúB12)
+ (‚àí1)1+3B13 ¬∑ det( ÀúB13)
= (‚àí1)2(0)¬∑ det

‚àí3
‚àí5
‚àí4
4
	
+ (‚àí1)3(1)¬∑ det

‚àí2
‚àí5
4
4
	
+ (‚àí1)4(3)¬∑ det

‚àí2
‚àí3
4
‚àí4
	
= 0 ‚àí1 [‚àí2(4) ‚àí(‚àí5)(4)] + 3 [‚àí2(‚àí4) ‚àí(‚àí3)(4)]
= 0 ‚àí1(12) + 3(20)
= 48.
‚ô¶
Example 3
Let
C =
‚éõ
‚éú
‚éú
‚éù
2
0
0
1
0
1
3
‚àí3
‚àí2
‚àí3
‚àí5
2
4
‚àí4
4
‚àí6
‚éû
‚éü
‚éü
‚é†‚ààM4√ó4(R).
Using cofactor expansion along the Ô¨Årst row of C and the results of Examples 1
and 2, we obtain
det(C) = (‚àí1)2(2)¬∑ det( ÀúC11) + (‚àí1)3(0)¬∑ det( ÀúC12)
+ (‚àí1)4(0)¬∑ det( ÀúC13) + (‚àí1)5(1)¬∑ det( ÀúC14)
= (‚àí1)2(2)¬∑ det
‚éõ
‚éù
1
3
‚àí3
‚àí3
‚àí5
2
‚àí4
4
‚àí6
‚éû
‚é†+ 0 + 0
+ (‚àí1)5(1)¬∑ det
‚éõ
‚éù
0
1
3
‚àí2
‚àí3
‚àí5
4
‚àí4
4
‚éû
‚é†
= 2(40) + 0 + 0 ‚àí1(48)
= 32.
‚ô¶

212
Chap. 4
Determinants
Example 4
The determinant of the n√ón identity matrix is 1. We prove this assertion by
mathematical induction on n. The result is clearly true for the 1 √ó 1 identity
matrix. Assume that the determinant of the (n ‚àí1) √ó (n ‚àí1) identity matrix
is 1 for some n ‚â•2, and let I denote the n√ón identity matrix. Using cofactor
expansion along the Ô¨Årst row of I, we obtain
det(I) = (‚àí1)2(1)¬∑ det(ÀúI11) + (‚àí1)3(0)¬∑ det(ÀúI12) + ¬∑ ¬∑ ¬∑
+ (‚àí1)1+n(0)¬∑ det(ÀúI1n)
= 1(1) + 0 + ¬∑ ¬∑ ¬∑ + 0
= 1
because ÀúI11 is the (n ‚àí1) √ó (n ‚àí1) identity matrix. This shows that the
determinant of the n √ó n identity matrix is 1, and so the determinant of any
identity matrix is 1 by the principle of mathematical induction.
‚ô¶
As is illustrated in Example 3, the calculation of a determinant using
the recursive deÔ¨Ånition is extremely tedious, even for matrices as small as
4√ó4. Later in this section, we present a more eÔ¨Écient method for evaluating
determinants, but we must Ô¨Årst learn more about them.
Recall from Theorem 4.1 (p. 200) that, although the determinant of a 2√ó2
matrix is not a linear transformation, it is a linear function of each row when
the other row is held Ô¨Åxed. We now show that a similar property is true for
determinants of any size.
Theorem 4.3. The determinant of an n √ó n matrix is a linear function
of each row when the remaining rows are held Ô¨Åxed. That is, for 1 ‚â§r ‚â§n,
we have
det
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
a1
...
ar‚àí1
u + kv
ar+1
...
an
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
= det
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
a1
...
ar‚àí1
u
ar+1
...
an
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
+ k det
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
a1
...
ar‚àí1
v
ar+1
...
an
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
whenever k is a scalar and u, v, and each ai are row vectors in Fn.
Proof. The proof is by mathematical induction on n. The result is imme-
diate if n = 1. Assume that for some integer n ‚â•2 the determinant of any
(n ‚àí1) √ó (n ‚àí1) matrix is a linear function of each row when the remaining

Sec. 4.2
Determinants of Order n
213
rows are held Ô¨Åxed. Let A be an n√ón matrix with rows a1, a2, . . . , an, respec-
tively, and suppose that for some r (1 ‚â§r ‚â§n), we have ar = u+kv for some
u, v ‚ààFn and some scalar k. Let u = (b1, b2, . . . , bn) and v = (c1, c2, . . . , cn),
and let B and C be the matrices obtained from A by replacing row r of A by
u and v, respectively. We must prove that det(A) = det(B) + k det(C). We
leave the proof of this fact to the reader for the case r = 1. For r > 1 and
1 ‚â§j ‚â§n, the rows of ÀúA1j, ÀúB1j, and ÀúC1j are the same except for row r ‚àí1.
Moreover, row r ‚àí1 of ÀúA1j is
(b1 + kc1, . . . , bj‚àí1 + kcj‚àí1, bj+1 + kcj+1, . . . , bn + kcn),
which is the sum of row r ‚àí1 of ÀúB1j and k times row r ‚àí1 of ÀúC1j. Since ÀúB1j
and ÀúC1j are (n ‚àí1) √ó (n ‚àí1) matrices, we have
det( ÀúA1j) = det( ÀúB1j) + k det( ÀúC1j)
by the induction hypothesis. Thus since A1j = B1j = C1j, we have
det(A) =
n

j=1
(‚àí1)1+jA1j ¬∑ det( ÀúA1j)
=
n

j=1
(‚àí1)1+jA1j ¬∑
&
det( ÀúB1j) + k det( ÀúC1j)
'
=
n

j=1
(‚àí1)1+jA1j ¬∑ det( ÀúB1j) + k
n

j=1
(‚àí1)1+jA1j ¬∑ det( ÀúC1j)
= det(B) + k det(C).
This shows that the theorem is true for n √ó n matrices, and so the theorem
is true for all square matrices by mathematical induction.
Corollary. If A ‚ààMn√ón(F) has a row consisting entirely of zeros, then
det(A) = 0.
Proof. See Exercise 24.
The deÔ¨Ånition of a determinant requires that the determinant of a matrix
be evaluated by cofactor expansion along the Ô¨Årst row. Our next theorem
shows that the determinant of a square matrix can be evaluated by cofactor
expansion along any row. Its proof requires the following technical result.
Lemma. Let B ‚ààMn√ón(F), where n ‚â•2. If row i of B equals ek for
some k (1 ‚â§k ‚â§n), then det(B) = (‚àí1)i+k det( ÀúBik).

214
Chap. 4
Determinants
Proof. The proof is by mathematical induction on n. The lemma is easily
proved for n = 2. Assume that for some integer n ‚â•3, the lemma is true for
(n ‚àí1) √ó (n ‚àí1) matrices, and let B be an n √ó n matrix in which row i of B
equals ek for some k (1 ‚â§k ‚â§n). The result follows immediately from the
deÔ¨Ånition of the determinant if i = 1. Suppose therefore that 1 < i ‚â§n. For
each j Ã∏= k (1 ‚â§j ‚â§n), let Cij denote the (n ‚àí2) √ó (n ‚àí2) matrix obtained
from B by deleting rows 1 and i and columns j and k. For each j, row i ‚àí1
of ÀúB1j is the following vector in Fn‚àí1:
‚éß
‚é™
‚é®
‚é™
‚é©
ek‚àí1
if j < k
0
if j = k
ek
if j > k.
Hence by the induction hypothesis and the corollary to Theorem 4.3, we have
det( ÀúB1j) =
‚éß
‚é™
‚é®
‚é™
‚é©
(‚àí1)(i‚àí1)+(k‚àí1) det(Cij)
if j < k
0
if j = k
(‚àí1)(i‚àí1)+k det(Cij)
if j > k.
Therefore
det(B) =
n

j=1
(‚àí1)1+jB1j ¬∑ det( ÀúB1j)
=

j<k
(‚àí1)1+jB1j ¬∑ det( ÀúB1j) +

j>k
(‚àí1)1+jB1j ¬∑ det( ÀúB1j)
=

j<k
(‚àí1)1+jB1j ¬∑
&
(‚àí1)(i‚àí1)+(k‚àí1) det(Cij)
'
+

j>k
(‚àí1)1+jB1j ¬∑
&
(‚àí1)(i‚àí1)+k det(Cij)
'
= (‚àí1)i+k
‚é°
‚é£
j<k
(‚àí1)1+jB1j ¬∑ det(Cij)
+

j>k
(‚àí1)1+(j‚àí1)B1j ¬∑ det(Cij)
‚é§
‚é¶.
Because the expression inside the preceding bracket is the cofactor expan-
sion of ÀúBik along the Ô¨Årst row, it follows that
det(B) = (‚àí1)i+k det( ÀúBik).
This shows that the lemma is true for n √ó n matrices, and so the lemma is
true for all square matrices by mathematical induction.

Sec. 4.2
Determinants of Order n
215
We are now able to prove that cofactor expansion along any row can be
used to evaluate the determinant of a square matrix.
Theorem 4.4. The determinant of a square matrix can be evaluated by
cofactor expansion along any row. That is, if A ‚ààMn√ón(F), then for any
integer i (1 ‚â§i ‚â§n),
det(A) =
n

j=1
(‚àí1)i+jAij ¬∑ det( ÀúAij).
Proof. Cofactor expansion along the Ô¨Årst row of A gives the determinant
of A by deÔ¨Ånition. So the result is true if i = 1. Fix i > 1. Row i of A can
be written as ,n
j=1 Aijej. For 1 ‚â§j ‚â§n, let Bj denote the matrix obtained
from A by replacing row i of A by ej. Then by Theorem 4.3 and the lemma,
we have
det(A) =
n

j=1
Aij det(Bj) =
n

j=1
(‚àí1)i+jAij ¬∑ det( ÀúAij).
Corollary. If A ‚ààMn√ón(F) has two identical rows, then det(A) = 0.
Proof. The proof is by mathematical induction on n. We leave the proof
of the result to the reader in the case that n = 2. Assume that for some
integer n ‚â•3, it is true for (n ‚àí1) √ó (n ‚àí1) matrices, and let rows r and
s of A ‚ààMn√ón(F) be identical for r Ã∏= s. Because n ‚â•3, we can choose an
integer i (1 ‚â§i ‚â§n) other than r and s. Now
det(A) =
n

j=1
(‚àí1)i+jAij ¬∑ det( ÀúAij)
by Theorem 4.4. Since each ÀúAij is an (n ‚àí1) √ó (n ‚àí1) matrix with two
identical rows, the induction hypothesis implies that each det( ÀúAij) = 0, and
hence det(A) = 0. This completes the proof for n √ó n matrices, and so the
lemma is true for all square matrices by mathematical induction.
It is possible to evaluate determinants more eÔ¨Éciently by combining co-
factor expansion with the use of elementary row operations. Before such a
process can be developed, we need to learn what happens to the determinant
of a matrix if we perform an elementary row operation on that matrix. The-
orem 4.3 provides this information for elementary row operations of type 2
(those in which a row is multiplied by a nonzero scalar). Next we turn our
attention to elementary row operations of type 1 (those in which two rows
are interchanged).

216
Chap. 4
Determinants
Theorem 4.5. If A ‚ààMn√ón(F) and B is a matrix obtained from A by
interchanging any two rows of A, then det(B) = ‚àídet(A).
Proof. Let the rows of A ‚ààMn√ón(F) be a1, a2, . . . , an, and let B be the
matrix obtained from A by interchanging rows r and s, where r < s. Thus
A =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
a1
...
ar
...
as
...
an
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
and
B =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
a1
...
as
...
ar
...
an
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
.
Consider the matrix obtained from A by replacing rows r and s by ar + as.
By the corollary to Theorem 4.4 and Theorem 4.3, we have
0 = det
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
a1
...
ar + as
...
ar + as
...
an
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
= det
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
a1
...
ar
...
ar + as
...
an
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
+ det
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
a1
...
as
...
ar + as
...
an
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
= det
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
a1
...
ar
...
ar
...
an
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
+ det
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
a1
...
ar
...
as
...
an
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
+ det
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
a1
...
as
...
ar
...
an
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
+ det
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
a1
...
as
...
as
...
an
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
= 0 + det(A) + det(B) + 0.
Therefore det(B) = ‚àídet(A).
We now complete our investigation of how an elementary row operation
aÔ¨Äects the determinant of a matrix by showing that elementary row operations
of type 3 do not change the determinant of a matrix.
Theorem 4.6. Let A ‚ààMn√ón(F), and let B be a matrix obtained by
adding a multiple of one row of A to another row of A. Then det(B) = det(A).

Sec. 4.2
Determinants of Order n
217
Proof. Suppose that B is the n √ó n matrix obtained from A by adding k
times row r to row s, where r Ã∏= s. Let the rows of A be a1, a2, . . . , an, and
the rows of B be b1, b2, . . . , bn. Then bi = ai for i Ã∏= s and bs = as + kar.
Let C be the matrix obtained from A by replacing row s with ar. Applying
Theorem 4.3 to row s of B, we obtain
det(B) = det(A) + k det(C) = det(A)
because det(C) = 0 by the corollary to Theorem 4.4.
In Theorem 4.2 (p. 201), we proved that a 2 √ó 2 matrix is invertible if
and only if its determinant is nonzero. As a consequence of Theorem 4.6, we
can prove half of the promised generalization of this result in the following
corollary. The converse is proved in the corollary to Theorem 4.7.
Corollary. If A ‚ààMn√ón(F) has rank less than n, then det(A) = 0.
Proof. If the rank of A is less than n, then the rows a1, a2, . . . , an of A are
linearly dependent. By Exercise 14 of Section 1.5, some row of A, say, row r,
is a linear combination of the other rows. So there exist scalars ci such that
ar = c1a1 + ¬∑ ¬∑ ¬∑ + cr‚àí1ar‚àí1 + cr+1ar+1 + ¬∑ ¬∑ ¬∑ + cnan.
Let B be the matrix obtained from A by adding ‚àíci times row i to row r for
each i Ã∏= r. Then row r of B consists entirely of zeros, and so det(B) = 0.
But by Theorem 4.6, det(B) = det(A). Hence det(A) = 0.
The following rules summarize the eÔ¨Äect of an elementary row operation
on the determinant of a matrix A ‚ààMn√ón(F).
(a) If B is a matrix obtained by interchanging any two rows of A, then
det(B) = ‚àídet(A).
(b) If B is a matrix obtained by multiplying a row of A by a nonzero scalar
k, then det(B) = k det(A).
(c) If B is a matrix obtained by adding a multiple of one row of A to another
row of A, then det(B) = det(A).
These facts can be used to simplify the evaluation of a determinant. Con-
sider, for instance, the matrix in Example 1:
A =
‚éõ
‚éù
1
3
‚àí3
‚àí3
‚àí5
2
‚àí4
4
‚àí6
‚éû
‚é†.
Adding 3 times row 1 of A to row 2 and 4 times row 1 to row 3, we obtain
M =
‚éõ
‚éù
1
4
‚àí3
0
4
‚àí7
0
16
‚àí18
‚éû
‚é†.

218
Chap. 4
Determinants
Since M was obtained by performing two type 3 elementary row operations
on A, we have det(A) = det(M). The cofactor expansion of M along the Ô¨Årst
row gives
det(M) = (‚àí1)1+1(1)¬∑ det( Àú
M11) + (‚àí1)1+2(4)¬∑ det( Àú
M12)
+ (‚àí1)1+3(‚àí3)¬∑ det( Àú
M13).
Both Àú
M12 and Àú
M13 have a column consisting entirely of zeros, and so
det( Àú
M12) = det( Àú
M13) = 0 by the corollary to Theorem 4.6. Hence
det(M) = (‚àí1)1+1(1)¬∑ det( Àú
M11)
= (‚àí1)1+1(1)¬∑ det

4
‚àí7
16
‚àí18
	
= 1[4(‚àí18) ‚àí(‚àí7)(16)] = 40.
Thus with the use of two elementary row operations of type 3, we have reduced
the computation of det(A) to the evaluation of one determinant of a 2 √ó 2
matrix.
But we can do even better. If we add ‚àí4 times row 2 of M to row 3
(another elementary row operation of type 3), we obtain
P =
‚éõ
‚éù
1
4
‚àí3
0
4
‚àí7
0
0
10
‚éû
‚é†.
Evaluating det(P) by cofactor expansion along the Ô¨Årst row, we have
det(P) = (‚àí1)1+1(1)¬∑ det( ÀúP11)
= (‚àí1)1+1(1)¬∑ det

4
‚àí7
0
10
	
= 1¬∑4¬∑10 = 40,
as described earlier.
Since det(A) = det(M) = det(P), it follows that
det(A) = 40.
The preceding calculation of det(P) illustrates an important general fact.
The determinant of an upper triangular matrix is the product of its diagonal
entries. (See Exercise 23.) By using elementary row operations of types 1
and 3 only, we can transform any square matrix into an upper triangular
matrix, and so we can easily evaluate the determinant of any square matrix.
The next two examples illustrate this technique.
Example 5
To evaluate the determinant of the matrix
B =
‚éõ
‚éù
0
1
3
‚àí2
‚àí3
‚àí5
4
‚àí4
4
‚éû
‚é†

Sec. 4.2
Determinants of Order n
219
in Example 2, we must begin with a row interchange. Interchanging rows 1
and 2 of B produces
C =
‚éõ
‚éù
‚àí2
‚àí3
‚àí5
0
1
3
4
‚àí4
4
‚éû
‚é†.
By means of a sequence of elementary row operations of type 3, we can
transform C into an upper triangular matrix:
‚éõ
‚éù
‚àí2
‚àí3
‚àí5
0
1
3
4
‚àí4
4
‚éû
‚é†‚àí‚Üí
‚éõ
‚éù
‚àí2
‚àí3
‚àí5
0
1
3
0
‚àí10
‚àí6
‚éû
‚é†‚àí‚Üí
‚éõ
‚éù
‚àí2
‚àí3
‚àí5
0
1
3
0
0
24
‚éû
‚é†.
Thus det(C) = ‚àí2¬∑1¬∑24 = ‚àí48. Since C was obtained from B by an inter-
change of rows, it follows that
det(B) = ‚àídet(C) = 48.
‚ô¶
Example 6
The technique in Example 5 can be used to evaluate the determinant of the
matrix
C =
‚éõ
‚éú
‚éú
‚éù
2
0
0
1
0
1
3
‚àí3
‚àí2
‚àí3
‚àí5
2
4
‚àí4
4
‚àí6
‚éû
‚éü
‚éü
‚é†
in Example 3.
This matrix can be transformed into an upper triangular
matrix by means of the following sequence of elementary row operations of
type 3:
‚éõ
‚éú
‚éú
‚éù
2
0
0
1
0
1
3
‚àí3
‚àí2
‚àí3
‚àí5
2
4
‚àí4
4
‚àí6
‚éû
‚éü
‚éü
‚é†‚àí‚Üí
‚éõ
‚éú
‚éú
‚éù
2
0
0
1
0
1
3
‚àí3
0
‚àí3
‚àí5
3
0
‚àí4
4
‚àí8
‚éû
‚éü
‚éü
‚é†‚àí‚Üí
‚éõ
‚éú
‚éú
‚éù
2
0
0
1
0
1
3
‚àí3
0
0
4
‚àí6
0
0
16
‚àí20
‚éû
‚éü
‚éü
‚é†
‚àí‚Üí
‚éõ
‚éú
‚éú
‚éù
2
0
0
1
0
1
3
‚àí3
0
0
4
‚àí6
0
0
0
4
‚éû
‚éü
‚éü
‚é†.
Thus det(C) = 2¬∑1¬∑4¬∑4 = 32.
‚ô¶
Using elementary row operations to evaluate the determinant of a matrix,
as illustrated in Example 6, is far more eÔ¨Écient than using cofactor expansion.
Consider Ô¨Årst the evaluation of a 2 √ó 2 matrix. Since
det

a
b
c
d
	
= ad ‚àíbc,

220
Chap. 4
Determinants
the evaluation of the determinant of a 2 √ó 2 matrix requires 2 multiplications
(and 1 subtraction). For n ‚â•3, evaluating the determinant of an n√ón matrix
by cofactor expansion along any row expresses the determinant as a sum of n
products involving determinants of (n‚àí1)√ó(n‚àí1) matrices. Thus in all, the
evaluation of the determinant of an n√ón matrix by cofactor expansion along
any row requires over n! multiplications, whereas evaluating the determinant
of an n √ó n matrix by elementary row operations as in Examples 5 and 6
can be shown to require only (n3 + 2n ‚àí3)/3 multiplications. To evaluate
the determinant of a 20 √ó 20 matrix, which is not large by present standards,
cofactor expansion along a row requires over 20! ‚âà2.4 √ó 1018 multiplica-
tions. Thus it would take a computer performing one billion multiplications
per second over 77 years to evaluate the determinant of a 20 √ó 20 matrix by
this method. By contrast, the method using elementary row operations re-
quires only 2679 multiplications for this calculation and would take the same
computer less than three-millionths of a second! It is easy to see why most
computer programs for evaluating the determinant of an arbitrary matrix do
not use cofactor expansion.
In this section, we have deÔ¨Åned the determinant of a square matrix in
terms of cofactor expansion along the Ô¨Årst row. We then showed that the
determinant of a square matrix can be evaluated using cofactor expansion
along any row.
In addition, we showed that the determinant possesses a
number of special properties, including properties that enable us to calculate
det(B) from det(A) whenever B is a matrix obtained from A by means of an
elementary row operation. These properties enable us to evaluate determi-
nants much more eÔ¨Éciently. In the next section, we continue this approach
to discover additional properties of determinants.
EXERCISES
1.
Label the following statements as true or false.
(a)
The function det: Mn√ón(F) ‚ÜíF is a linear transformation.
(b)
The determinant of a square matrix can be evaluated by cofactor
expansion along any row.
(c)
If two rows of a square matrix A are identical, then det(A) = 0.
(d)
If B is a matrix obtained from a square matrix A by interchanging
any two rows, then det(B) = ‚àídet(A).
(e)
If B is a matrix obtained from a square matrix A by multiplying
a row of A by a scalar, then det(B) = det(A).
(f)
If B is a matrix obtained from a square matrix A by adding k
times row i to row j, then det(B) = k det(A).
(g)
If A ‚ààMn√ón(F) has rank n, then det(A) = 0.
(h)
The determinant of an upper triangular matrix equals the product
of its diagonal entries.

Sec. 4.2
Determinants of Order n
221
2.
Find the value of k that satisÔ¨Åes the following equation:
det
‚éõ
‚éù
3a1
3a2
3a3
3b1
3b2
3b3
3c1
3c2
3c3
‚éû
‚é†= k det
‚éõ
‚éù
a1
a2
a3
b1
b2
b3
c1
c2
c3
‚éû
‚é†.
3.
Find the value of k that satisÔ¨Åes the following equation:
det
‚éõ
‚éù
2a1
2a2
2a3
3b1 + 5c1
3b2 + 5c2
3b3 + 5c3
7c1
7c2
7c3
‚éû
‚é†= k det
‚éõ
‚éù
a1
a2
a3
b1
b2
b3
c1
c2
c3
‚éû
‚é†.
4.
Find the value of k that satisÔ¨Åes the following equation:
det
‚éõ
‚éù
b1 + c1
b2 + c2
b3 + c3
a1 + c1
a2 + c2
a3 + c3
a1 + b1
a2 + b2
a3 + b3
‚éû
‚é†= k det
‚éõ
‚éù
a1
a2
a3
b1
b2
b3
c1
c2
c3
‚éû
‚é†.
In Exercises 5‚Äì12, evaluate the determinant of the given matrix by cofactor
expansion along the indicated row.
5.
‚éõ
‚éù
0
1
2
‚àí1
0
‚àí3
2
3
0
‚éû
‚é†
along the Ô¨Årst row
6.
‚éõ
‚éù
1
0
2
0
1
5
‚àí1
3
0
‚éû
‚é†
along the Ô¨Årst row
7.
‚éõ
‚éù
0
1
2
‚àí1
0
‚àí3
2
3
0
‚éû
‚é†
along the second row
8.
‚éõ
‚éù
1
0
2
0
1
5
‚àí1
3
0
‚éû
‚é†
along the third row
9.
‚éõ
‚éù
0
1 + i
2
‚àí2i
0
1 ‚àíi
3
4i
0
‚éû
‚é†
along the third row
10.
‚éõ
‚éù
i
2 + i
0
‚àí1
3
2i
0
‚àí1
1 ‚àíi
‚éû
‚é†
along the second row
11.
‚éõ
‚éú
‚éú
‚éù
0
2
1
3
1
0
‚àí2
2
3
‚àí1
0
1
‚àí1
1
2
0
‚éû
‚éü
‚éü
‚é†
along the fourth row
12.
‚éõ
‚éú
‚éú
‚éù
1
‚àí1
2
‚àí1
‚àí3
4
1
‚àí1
2
‚àí5
‚àí3
8
‚àí2
6
‚àí4
1
‚éû
‚éü
‚éü
‚é†
along the fourth row
In Exercises 13‚Äì22, evaluate the determinant of the given matrix by any le-
gitimate method.

222
Chap. 4
Determinants
13.
‚éõ
‚éù
0
0
1
0
2
3
4
5
6
‚éû
‚é†
14.
‚éõ
‚éù
2
3
4
5
6
0
7
0
0
‚éû
‚é†
15.
‚éõ
‚éù
1
2
3
4
5
6
7
8
9
‚éû
‚é†
16.
‚éõ
‚éù
‚àí1
3
2
4
‚àí8
1
2
2
5
‚éû
‚é†
17.
‚éõ
‚éù
0
1
1
1
2
‚àí5
6
‚àí4
3
‚éû
‚é†
18.
‚éõ
‚éù
1
‚àí2
3
‚àí1
2
‚àí5
3
‚àí1
2
‚éû
‚é†
19.
‚éõ
‚éù
i
2
‚àí1
3
1 + i
2
‚àí2i
1
4 ‚àíi
‚éû
‚é†
20.
‚éõ
‚éù
‚àí1
2 + i
3
1 ‚àíi
i
1
3i
2
‚àí1 + i
‚éû
‚é†
21.
‚éõ
‚éú
‚éú
‚éù
1
0
‚àí2
3
‚àí3
1
1
2
0
4
‚àí1
1
2
3
0
1
‚éû
‚éü
‚éü
‚é†
22.
‚éõ
‚éú
‚éú
‚éù
1
‚àí2
3
‚àí12
‚àí5
12
‚àí14
19
‚àí9
22
‚àí20
31
‚àí4
9
‚àí14
15
‚éû
‚éü
‚éü
‚é†
23.
Prove that the determinant of an upper triangular matrix is the product
of its diagonal entries.
24.
Prove the corollary to Theorem 4.3.
25.
Prove that det(kA) = kn det(A) for any A ‚ààMn√ón(F).
26.
Let A ‚ààMn√ón(F). Under what conditions is det(‚àíA) = det(A)?
27.
Prove that if A ‚ààMn√ón(F) has two identical columns, then det(A) = 0.
28.
Compute det(Ei) if Ei is an elementary matrix of type i.
29.‚Ä† Prove that if E is an elementary matrix, then det(Et) = det(E).
30.
Let the rows of A ‚ààMn√ón(F) be a1, a2, . . . , an, and let B be the matrix
in which the rows are an, an‚àí1, . . . , a1. Calculate det(B) in terms of
det(A).
4.3
PROPERTIES OF DETERMINANTS
In Theorem 3.1, we saw that performing an elementary row operation on
a matrix can be accomplished by multiplying the matrix by an elementary
matrix. This result is very useful in studying the eÔ¨Äects on the determinant of
applying a sequence of elementary row operations. Because the determinant

Sec. 4.3
Properties of Determinants
223
of the n√ón identity matrix is 1 (see Example 4 in Section 4.2), we can interpret
the statements on page 217 as the following facts about the determinants of
elementary matrices.
(a) If E is an elementary matrix obtained by interchanging any two rows
of I, then det(E) = ‚àí1.
(b) If E is an elementary matrix obtained by multiplying some row of I by
the nonzero scalar k, then det(E) = k.
(c) If E is an elementary matrix obtained by adding a multiple of some row
of I to another row, then det(E) = 1.
We now apply these facts about determinants of elementary matrices to
prove that the determinant is a multiplicative function.
Theorem 4.7. For any A, B ‚ààMn√ón(F), det(AB) = det(A)¬∑ det(B).
Proof. We begin by establishing the result when A is an elementary matrix.
If A is an elementary matrix obtained by interchanging two rows of I, then
det(A) = ‚àí1. But by Theorem 3.1 (p. 149), AB is a matrix obtained by
interchanging two rows of B. Hence by Theorem 4.5 (p. 216), det(AB) =
‚àídet(B) = det(A)¬∑ det(B). Similar arguments establish the result when A
is an elementary matrix of type 2 or type 3. (See Exercise 18.)
If A is an n √ó n matrix with rank less than n, then det(A) = 0 by the
corollary to Theorem 4.6 (p. 216). Since rank(AB) ‚â§rank(A) < n by Theo-
rem 3.7 (p. 159), we have det(AB) = 0. Thus det(AB) = det(A)¬∑ det(B) in
this case.
On the other hand, if A has rank n, then A is invertible and hence is
the product of elementary matrices (Corollary 3 to Theorem 3.6 p. 159), say,
A = Em ¬∑ ¬∑ ¬∑ E2E1. The Ô¨Årst paragraph of this proof shows that
det(AB) = det(Em ¬∑ ¬∑ ¬∑ E2E1B)
= det(Em)¬∑ det(Em‚àí1 ¬∑ ¬∑ ¬∑ E2E1B)
...
= det(Em)¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ det(E2)¬∑ det(E1)¬∑ det(B)
= det(Em ¬∑ ¬∑ ¬∑ E2E1)¬∑ det(B)
= det(A)¬∑ det(B).
Corollary. A matrix A ‚ààMn√ón(F) is invertible if and only if det(A) Ã∏= 0.
Furthermore, if A is invertible, then det(A‚àí1) =
1
det(A).
Proof. If A ‚ààMn√ón(F) is not invertible, then the rank of A is less than n.
So det(A) = 0 by the corollary to Theorem 4.6 (p, 217). On the other hand,
if A ‚ààMn√ón(F) is invertible, then
det(A)¬∑ det(A‚àí1) = det(AA‚àí1) = det(I) = 1

224
Chap. 4
Determinants
by Theorem 4.7. Hence det(A) Ã∏= 0 and det(A‚àí1) =
1
det(A).
In our discussion of determinants until now, we have used only the rows
of a matrix. For example, the recursive deÔ¨Ånition of a determinant involved
cofactor expansion along a row, and the more eÔ¨Écient method developed in
Section 4.2 used elementary row operations. Our next result shows that the
determinants of A and At are always equal.
Since the rows of A are the
columns of At, this fact enables us to translate any statement about determi-
nants that involves the rows of a matrix into a corresponding statement that
involves its columns.
Theorem 4.8. For any A ‚ààMn√ón(F), det(At) = det(A).
Proof. If A is not invertible, then rank(A) < n. But rank(At) = rank(A)
by Corollary 2 to Theorem 3.6 (p. 158), and so At is not invertible. Thus
det(At) = 0 = det(A) in this case.
On the other hand, if A is invertible, then A is a product of elementary
matrices, say A = Em ¬∑ ¬∑ ¬∑ E2E1.
Since det(Ei) = det(Et
i) for every i by
Exercise 29 of Section 4.2, by Theorem 4.7 we have
det(At) = det(Et
1Et
2 ¬∑ ¬∑ ¬∑ Et
m)
= det(Et
1)¬∑ det(Et
2)¬∑ ¬∑ ¬∑ ¬∑ ¬∑ det(Et
m)
= det(E1)¬∑ det(E2)¬∑ ¬∑ ¬∑ ¬∑ ¬∑ det(Em)
= det(Em)¬∑ ¬∑ ¬∑ ¬∑ ¬∑ det(E2)¬∑ det(E1)
= det(Em ¬∑ ¬∑ ¬∑ E2E1)
= det(A).
Thus, in either case, det(At) = det(A).
Among the many consequences of Theorem 4.8 are that determinants can
be evaluated by cofactor expansion along a column, and that elementary col-
umn operations can be used as well as elementary row operations in evaluating
a determinant. (The eÔ¨Äect on the determinant of performing an elementary
column operation is the same as the eÔ¨Äect of performing the corresponding
elementary row operation.) We conclude our discussion of determinant prop-
erties with a well-known result that relates determinants to the solutions of
certain types of systems of linear equations.
Theorem 4.9 (Cramer‚Äôs Rule).
Let Ax = b be the matrix form of
a system of n linear equations in n unknowns, where x = (x1, x2, . . . , xn)t.
If det(A) Ã∏= 0, then this system has a unique solution, and for each k (k =
1, 2, . . . , n),
xk = det(Mk)
det(A) ,

Sec. 4.3
Properties of Determinants
225
where Mk is the n √ó n matrix obtained from A by replacing column k of A
by b.
Proof. If det(A) Ã∏= 0, then the system Ax = b has a unique solution by
the corollary to Theorem 4.7 and Theorem 3.10 (p. 174). For each integer k
(1 ‚â§k ‚â§n), let ak denote the kth column of A and Xk denote the matrix
obtained from the n √ó n identity matrix by replacing column k by x. Then
by Theorem 2.13 (p. 90), AXk is the n √ó n matrix whose ith column is
Aei = ai if i Ã∏= k
and
Ax = b if i = k.
Thus AXk = Mk. Evaluating Xk by cofactor expansion along row k produces
det(Xk) = xk ¬∑ det(In‚àí1) = xk.
Hence by Theorem 4.7,
det(Mk) = det(AXk) = det(A)¬∑ det(Xk) = det(A)¬∑xk.
Therefore
xk = [det(A)]‚àí1 ¬∑ det(Mk).
Example 1
We illustrate Theorem 4.9 by using Cramer‚Äôs rule to solve the following system
of linear equations:
x1 + 2x2 + 3x3 = 2
x1
+ x3 = 3
x2 + x2 ‚àíx3 = 1.
The matrix form of this system of linear equations is Ax = b, where
A =
‚éõ
‚éù
1
2
3
1
0
1
1
1
‚àí1
‚éû
‚é†
and
b =
‚éõ
‚éù
2
3
1
‚éû
‚é†.
Because det(A) = 6 Ã∏= 0, Cramer‚Äôs rule applies. Using the notation of Theo-
rem 4.9, we have
x1 = det(M1)
det(A) =
det
‚éõ
‚éù
2
2
3
3
0
1
1
1
‚àí1
‚éû
‚é†
det(A)
= 15
6 = 5
2,
x2 = det(M2)
det(A) =
det
‚éõ
‚éù
1
2
3
1
3
1
1
1
‚àí1
‚éû
‚é†
det(A)
= ‚àí6
6 = ‚àí1,

226
Chap. 4
Determinants
and
x3 = det(M3)
det(A) =
det
‚éõ
‚éù
1
2
2
1
0
3
1
1
1
‚éû
‚é†
det(A)
= 3
6 = 1
2.
Thus the unique solution to the given system of linear equations is
(x1, x2, x3) =
5
2, ‚àí1, 1
2
	
.
‚ô¶
In applications involving systems of linear equations, we sometimes need
to know that there is a solution in which the unknowns are integers. In this
situation, Cramer‚Äôs rule can be useful because it implies that a system of linear
equations with integral coeÔ¨Écients has an integral solution if the determinant
of its coeÔ¨Écient matrix is ¬±1. On the other hand, Cramer‚Äôs rule is not useful
for computation because it requires evaluating n + 1 determinants of n √ó n
matrices to solve a system of n linear equations in n unknowns. The amount
of computation to do this is far greater than that required to solve the system
by the method of Gaussian elimination, which was discussed in Section 3.4.
Thus Cramer‚Äôs rule is primarily of theoretical and aesthetic interest, rather
than of computational value.
As in Section 4.1, it is possible to interpret the determinant of a matrix
A ‚ààMn√ón(R) geometrically. If the rows of A are a1, a2, . . . , an, respectively,
then | det(A)| is the n-dimensional volume (the generalization of area in
R2 and volume in R3) of the parallelepiped having the vectors a1, a2, . . . , an
as adjacent sides.
(For a proof of a more generalized result, see Jerrold
E. Marsden and Michael J. HoÔ¨Äman, Elementary Classical Analysis, W.H.
Freeman and Company, New York, 1993, p. 524.)
Example 2
The volume of the parallelepiped having the vectors a1 = (1, ‚àí2, 1), a2 =
(1, 0, ‚àí1), and a3 = (1, 1, 1) as adjacent sides is
######
det
‚éõ
‚éù
1
‚àí2
1
1
0
‚àí1
1
1
1
‚éû
‚é†
######
= 6.
Note that the object in question is a rectangular parallelepiped (see Fig-
ure 4.6) with sides of lengths
‚àö
6,
‚àö
2, and
‚àö
3. Hence by the familiar formula
for volume, its volume should be
‚àö
6¬∑
‚àö
2¬∑
‚àö
3 = 6, as the determinant calcu-
lation shows.
‚ô¶
In our earlier discussion of the geometric signiÔ¨Åcance of the determinant
formed from the vectors in an ordered basis for R2, we also saw that this

Sec. 4.3
Properties of Determinants
227
x
y
z
(1, 1, 1)
(1, 0, ‚àí1)
(1, ‚àí2, 1)
Figure 4.6: Parallelepiped determined by three vectors in R3.
determinant is positive if and only if the basis induces a right-handed coor-
dinate system. A similar statement is true in Rn. SpeciÔ¨Åcally, if Œ≥ is any
ordered basis for Rn and Œ≤ is the standard ordered basis for Rn, then Œ≥ in-
duces a right-handed coordinate system if and only if det(Q) > 0, where Q is
the change of coordinate matrix changing Œ≥-coordinates into Œ≤-coordinates.
Thus, for instance,
Œ≥ =
‚éß
‚é®
‚é©
‚éõ
‚éù
1
1
0
‚éû
‚é†,
‚éõ
‚éù
1
‚àí1
0
‚éû
‚é†,
‚éõ
‚éù
0
0
1
‚éû
‚é†
‚é´
‚é¨
‚é≠
induces a left-handed coordinate system in R3 because
det
‚éõ
‚éù
1
1
0
1
‚àí1
0
0
0
1
‚éû
‚é†= ‚àí2 < 0,
whereas
Œ≥‚Ä≤ =
‚éß
‚é®
‚é©
‚éõ
‚éù
1
2
0
‚éû
‚é†,
‚éõ
‚éù
‚àí2
1
0
‚éû
‚é†,
‚éõ
‚éù
0
0
1
‚éû
‚é†
‚é´
‚é¨
‚é≠
induces a right-handed coordinate system in R3 because
det
‚éõ
‚éù
1
‚àí2
0
2
1
0
0
0
1
‚éû
‚é†= 5 > 0.

228
Chap. 4
Determinants
More generally, if Œ≤ and Œ≥ are two ordered bases for Rn, then the coordinate
systems induced by Œ≤ and Œ≥ have the same orientation (either both are
right-handed or both are left-handed) if and only if det(Q) > 0, where Q is
the change of coordinate matrix changing Œ≥-coordinates into Œ≤-coordinates.
EXERCISES
1.
Label the following statements as true or false.
(a)
If E is an elementary matrix, then det(E) = ¬±1.
(b)
For any A, B ‚ààMn√ón(F), det(AB) = det(A)¬∑ det(B).
(c)
A matrix M ‚ààMn√ón(F) is invertible if and only if det(M) = 0.
(d)
A matrix M ‚ààMn√ón(F) has rank n if and only if det(M) Ã∏= 0.
(e)
For any A ‚ààMn√ón(F), det(At) = ‚àídet(A).
(f)
The determinant of a square matrix can be evaluated by cofactor
expansion along any column.
(g)
Every system of n linear equations in n unknowns can be solved
by Cramer‚Äôs rule.
(h)
Let Ax = b be the matrix form of a system of n linear equations
in n unknowns, where x = (x1, x2, . . . , xn)t. If det(A) Ã∏= 0 and if
Mk is the n √ó n matrix obtained from A by replacing row k of A
by bt, then the unique solution of Ax = b is
xk = det(Mk)
det(A)
for k = 1, 2, . . . , n.
In Exercises 2‚Äì7, use Cramer‚Äôs rule to solve the given system of linear equa-
tions.
2.
a11x1 + a12x2 = b1
a21x1 + a22x2 = b2
where a11a22 ‚àía12a21 Ã∏= 0
3.
2x1 + x2 ‚àí3x3 = 5
x1 ‚àí2x2 + x3 = 10
3x1 + 4x2 ‚àí2x3 = 0
4.
2x1 + x2 ‚àí3x3 =
1
x1 ‚àí2x2 + x3 =
0
3x1 + 4x2 ‚àí2x3 = ‚àí5
5.
x1 ‚àíx2 + 4x3 = ‚àí4
‚àí8x1 + 3x2 + x3 =
8
2x1 ‚àíx2 + x3 =
0
6.
x1 ‚àíx2 + 4x3 = ‚àí2
‚àí8x1 + 3x2 + x3 =
0
2x1 ‚àíx2 + x3 =
6
7.
3x1 + x2 + x3 =
4
‚àí2x1 ‚àíx2
= 12
x1 + 2x2 + x3 = ‚àí8
8.
Use Theorem 4.8 to prove a result analogous to Theorem 4.3 (p. 212),
but for columns.
9.
Prove that an upper triangular n √ó n matrix is invertible if and only if
all its diagonal entries are nonzero.

Sec. 4.3
Properties of Determinants
229
10.
A matrix M ‚ààMn√ón(C) is called nilpotent if, for some positive integer
k, M k = O, where O is the n √ó n zero matrix. Prove that if M is
nilpotent, then det(M) = 0.
11.
A matrix M ‚ààMn√ón(C) is called skew-symmetric if M t = ‚àíM.
Prove that if M is skew-symmetric and n is odd, then M is not invert-
ible. What happens if n is even?
12.
A matrix Q ‚ààMn√ón(R) is called orthogonal if QQt = I. Prove that
if Q is orthogonal, then det(Q) = ¬±1.
13.
For M ‚ààMn√ón(C), let M be the matrix such that (M)ij = Mij for all
i, j, where Mij is the complex conjugate of Mij.
(a)
Prove that det(M) = det(M).
(b)
A matrix Q ‚ààMn√ón(C) is called unitary if QQ‚àó= I, where
Q‚àó= Qt. Prove that if Q is a unitary matrix, then | det(Q)| = 1.
14.
Let Œ≤ = {u1, u2, . . . , un} be a subset of Fn containing n distinct vectors,
and let B be the matrix in Mn√ón(F) having uj as column j. Prove that
Œ≤ is a basis for Fn if and only if det(B) Ã∏= 0.
15.‚Ä† Prove that if A, B ‚ààMn√ón(F) are similar, then det(A) = det(B).
16.
Use determinants to prove that if A, B ‚ààMn√ón(F) are such that AB =
I, then A is invertible (and hence B = A‚àí1).
17.
Let A, B ‚ààMn√ón(F) be such that AB = ‚àíBA. Prove that if n is odd
and F is not a Ô¨Åeld of characteristic two, then A or B is not invertible.
18.
Complete the proof of Theorem 4.7 by showing that if A is an elementary
matrix of type 2 or type 3, then det(AB) = det(A)¬∑ det(B).
19.
A matrix A ‚ààMn√ón(F) is called lower triangular if Aij = 0 for
1 ‚â§i < j ‚â§n. Suppose that A is a lower triangular matrix. Describe
det(A) in terms of the entries of A.
20.
Suppose that M ‚ààMn√ón(F) can be written in the form
M =

A
B
O
I
	
,
where A is a square matrix. Prove that det(M) = det(A).
21.‚Ä† Prove that if M ‚ààMn√ón(F) can be written in the form
M =

A
B
O
C
	
,
where A and C are square matrices, then det(M) = det(A)¬∑ det(C).

230
Chap. 4
Determinants
22.
Let T: Pn(F) ‚ÜíFn+1 be the linear transformation deÔ¨Åned in Exer-
cise 22 of Section 2.4 by T(f) = (f(c0), f(c1), . . . , f(cn)), where
c0, c1, . . . , cn are distinct scalars in an inÔ¨Ånite Ô¨Åeld F. Let Œ≤ be the
standard ordered basis for Pn(F) and Œ≥ be the standard ordered basis
for Fn+1.
(a)
Show that M = [T]Œ≥
Œ≤ has the form
‚éõ
‚éú
‚éú
‚éú
‚éù
1
c0
c2
0
¬∑ ¬∑ ¬∑
cn
0
1
c1
c2
1
¬∑ ¬∑ ¬∑
cn
1
...
...
...
...
1
cn
c2
n
¬∑ ¬∑ ¬∑
cn
n
‚éû
‚éü
‚éü
‚éü
‚é†.
A matrix with this form is called a Vandermonde matrix.
(b)
Use Exercise 22 of Section 2.4 to prove that det(M) Ã∏= 0.
(c)
Prove that
det(M) =

0‚â§i<j‚â§n
(cj ‚àíci),
the product of all terms of the form cj ‚àíci for 0 ‚â§i < j ‚â§n.
23.
Let A ‚ààMn√ón(F) be nonzero. For any m (1 ‚â§m ‚â§n), an m √ó m
submatrix is obtained by deleting any n ‚àím rows and any n ‚àím
columns of A.
(a)
Let k (1 ‚â§k ‚â§n) denote the largest integer such that some k √ó k
submatrix has a nonzero determinant. Prove that rank(A) = k.
(b)
Conversely, suppose that rank(A) = k. Prove that there exists a
k √ó k submatrix with a nonzero determinant.
24.
Let A ‚ààMn√ón(F) have the form
A =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
0
0
0
¬∑ ¬∑ ¬∑
0
a0
‚àí1
0
0
¬∑ ¬∑ ¬∑
0
a1
0
‚àí1
0
¬∑ ¬∑ ¬∑
0
a2
...
...
...
...
...
0
0
0
¬∑ ¬∑ ¬∑
‚àí1
an‚àí1
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
.
Compute det(A + tI), where I is the n √ó n identity matrix.
25.
Let cjk denote the cofactor of the row j, column k entry of the matrix
A ‚ààMn√ón(F).
(a)
Prove that if B is the matrix obtained from A by replacing column
k by ej, then det(B) = cjk.

Sec. 4.3
Properties of Determinants
231
(b)
Show that for 1 ‚â§j ‚â§n, we have
A
‚éõ
‚éú
‚éú
‚éú
‚éù
cj1
cj2
...
cjn
‚éû
‚éü
‚éü
‚éü
‚é†= det(A)¬∑ej.
Hint: Apply Cramer‚Äôs rule to Ax = ej.
(c)
Deduce that if C is the n √ó n matrix such that Cij = cji, then
AC = [det(A)]I.
(d)
Show that if det(A) Ã∏= 0, then A‚àí1 = [det(A)]‚àí1C.
The following deÔ¨Ånition is used in Exercises 26‚Äì27.
DeÔ¨Ånition. The classical adjoint of a square matrix A is the transpose
of the matrix whose ij-entry is the ij-cofactor of A.
26.
Find the classical adjoint of each of the following matrices.
(a)

A11
A12
A21
A22
	
(b)
‚éõ
‚éù
4
0
0
0
4
0
0
0
4
‚éû
‚é†
(c)
‚éõ
‚éù
‚àí4
0
0
0
2
0
0
0
5
‚éû
‚é†
(d)
‚éõ
‚éù
3
6
7
0
4
8
0
0
5
‚éû
‚é†
(e)
‚éõ
‚éù
1 ‚àíi
0
0
4
3i
0
2i
1 + 4i
‚àí1
‚éû
‚é†
(f)
‚éõ
‚éù
7
1
4
6
‚àí3
0
‚àí3
5
‚àí2
‚éû
‚é†
(g)
‚éõ
‚éù
‚àí1
2
5
8
0
‚àí3
4
6
1
‚éû
‚é†
(h)
‚éõ
‚éù
3
2 + i
0
‚àí1 + i
0
i
0
1
3 ‚àí2i
‚éû
‚é†
27.
Let C be the classical adjoint of A ‚ààMn√ón(F). Prove the following
statements.
(a)
det(C) = [det(A)]n‚àí1.
(b)
Ct is the classical adjoint of At.
(c)
If A is an invertible upper triangular matrix, then C and A‚àí1 are
both upper triangular matrices.
28.
Let y1, y2, . . . , yn be linearly independent functions in C‚àû. For each
y ‚ààC‚àû, deÔ¨Åne T(y) ‚ààC‚àûby
[T(y)](t) = det
‚éõ
‚éú
‚éú
‚éú
‚éù
y(t)
y1(t)
y2(t)
¬∑ ¬∑ ¬∑
yn(t)
y‚Ä≤(t)
y‚Ä≤
1(t)
y‚Ä≤
2(t)
¬∑ ¬∑ ¬∑
y‚Ä≤
n(t)
...
...
...
...
y(n)(t)
y(n)
1
(t)
y(n)
2
(t)
¬∑ ¬∑ ¬∑
y(n)
n (t)
‚éû
‚éü
‚éü
‚éü
‚é†.

232
Chap. 4
Determinants
The preceding determinant is called the Wronskian of y, y1, . . . , yn.
(a)
Prove that T: C‚àû‚ÜíC‚àûis a linear transformation.
(b)
Prove that N(T) = span({y1, y2, . . . , yn}).
4.4
SUMMARY‚ÄîIMPORTANT FACTS ABOUT DETERMINANTS
In this section, we summarize the important properties of the determinant
needed for the remainder of the text. The results contained in this section
have been derived in Sections 4.2 and 4.3; consequently, the facts presented
here are stated without proofs.
The determinant of an n √ó n matrix A having entries from a Ô¨Åeld F is a
scalar in F, denoted by det(A) or |A|, and can be computed in the following
manner:
1. If A is 1 √ó 1, then det(A) = A11, the single entry of A.
2. If A is 2 √ó 2, then det(A) = A11A22 ‚àíA12A21. For example,
det

‚àí1
2
5
3
	
= (‚àí1)(3) ‚àí(2)(5) = ‚àí13.
3. If A is n √ó n for n > 2, then
det(A) =
n

j=1
(‚àí1)i+jAij ¬∑ det( ÀúAij)
(if the determinant is evaluated by the entries of row i of A) or
det(A) =
n

i=1
(‚àí1)i+jAij ¬∑ det( ÀúAij)
(if the determinant is evaluated by the entries of column j of A), where
ÀúAij is the (n‚àí1)√ó(n‚àí1) matrix obtained by deleting row i and column
j from A.
In the formulas above, the scalar (‚àí1)i+j det( ÀúAij) is called the cofactor
of the row i column j entry of A. In this language, the determinant of A is
evaluated as the sum of terms obtained by multiplying each entry of some
row or column of A by the cofactor of that entry. Thus det(A) is expressed
in terms of n determinants of (n ‚àí1) √ó (n ‚àí1) matrices. These determinants
are then evaluated in terms of determinants of (n‚àí2)√ó(n‚àí2) matrices, and
so forth, until 2 √ó 2 matrices are obtained. The determinants of the 2 √ó 2
matrices are then evaluated as in item 2.

Sec. 4.4
Summary‚ÄîImportant Facts about Determinants
233
Let us consider two examples of this technique in evaluating the determi-
nant of the 4 √ó 4 matrix
A =
‚éõ
‚éú
‚éú
‚éù
2
1
1
5
1
1
‚àí4
‚àí1
2
0
‚àí3
1
3
6
1
2
‚éû
‚éü
‚éü
‚é†.
To evaluate the determinant of A by expanding along the fourth row, we
must know the cofactors of each entry of that row. The cofactor of A41 = 3
is (‚àí1)4+1 det(B), where
B =
‚éõ
‚éù
1
1
5
1
‚àí4
‚àí1
0
‚àí3
1
‚éû
‚é†.
Let us evaluate this determinant by expanding along the Ô¨Årst column. We
have
det(B) = (‚àí1)1+1(1) det

‚àí4 ‚àí1
‚àí3
1
	
+ (‚àí1)2+1(1) det

1 5
‚àí3 1
	
+ (‚àí1)3+1(0) det

1
5
‚àí4 ‚àí1
	
= 1(1)[(‚àí4)(1) ‚àí(‚àí1)(‚àí3)] + (‚àí1)(1)[(1)(1) ‚àí(5)(‚àí3)] + 0
= ‚àí7 ‚àí16 + 0 = ‚àí23.
Thus the cofactor of A41 is (‚àí1)5(‚àí23) = 23. Similarly, the cofactors of A42,
A43, and A44 are 8, 11, and ‚àí13, respectively.
We can now evaluate the
determinant of A by multiplying each entry of the fourth row by its cofactor;
this gives
det(A) = 3(23) + 6(8) + 1(11) + 2(‚àí13) = 102.
For the sake of comparison, let us also compute the determinant of A
by expansion along the second column. The reader should verify that the
cofactors of A12, A22, and A42 are ‚àí14, 40, and 8, respectively. Thus
det(A) = (‚àí1)1+2(1) det
‚éõ
‚éù
1
‚àí4
‚àí1
2
‚àí3
1
3
1
2
‚éû
‚é†+ (‚àí1)2+2(1) det
‚éõ
‚éù
2
1
5
2
‚àí3
1
3
1
2
‚éû
‚é†
+ (‚àí1)3+2(0) det
‚éõ
‚éù
2
1
5
1
‚àí4
‚àí1
3
1
2
‚éû
‚é†+ (‚àí1)4+2(6) det
‚éõ
‚éù
2
1
5
1
‚àí4
‚àí1
2
‚àí3
1
‚éû
‚é†
= 14 + 40 + 0 + 48 = 102.

234
Chap. 4
Determinants
Of course, the fact that the value 102 is obtained again is no surprise since the
value of the determinant of A is independent of the choice of row or column
used in the expansion.
Observe that the computation of det(A) is easier when expanded along
the second column than when expanded along the fourth row. The diÔ¨Äerence
is the presence of a zero in the second column, which makes it unnecessary
to evaluate one of the cofactors (the cofactor of A32). For this reason, it is
beneÔ¨Åcial to evaluate the determinant of a matrix by expanding along a row or
column of the matrix that contains the largest number of zero entries. In fact,
it is often helpful to introduce zeros into the matrix by means of elementary
row operations before computing the determinant. This technique utilizes
the Ô¨Årst three properties of the determinant.
Properties of the Determinant
1. If B is a matrix obtained by interchanging any two rows or interchanging
any two columns of an n √ó n matrix A, then det(B) = ‚àídet(A).
2. If B is a matrix obtained by multiplying each entry of some row or
column of an n √ó n matrix A by a scalar k, then det(B) = k¬∑ det(A).
3. If B is a matrix obtained from an n √ó n matrix A by adding a multiple
of row i to row j or a multiple of column i to column j for i Ã∏= j, then
det(B) = det(A).
As an example of the use of these three properties in evaluating deter-
minants, let us compute the determinant of the 4 √ó 4 matrix A considered
previously. Our procedure is to introduce zeros into the second column of
A by employing property 3, and then to expand along that column. (The
elementary row operations used here consist of adding multiples of row 1 to
rows 2 and 4.) This procedure yields
det(A) = det
‚éõ
‚éú
‚éú
‚éù
2
1
1
5
1
1
‚àí4
‚àí1
2
0
‚àí3
1
3
6
1
2
‚éû
‚éü
‚éü
‚é†= det
‚éõ
‚éú
‚éú
‚éù
2
1
1
5
‚àí1
0
‚àí5
‚àí6
2
0
‚àí3
1
‚àí9
0
‚àí5
‚àí28
‚éû
‚éü
‚éü
‚é†
= 1(‚àí1)1+2 det
‚éõ
‚éù
‚àí1
‚àí5
‚àí6
2
‚àí3
1
‚àí9
‚àí5
‚àí28
‚éû
‚é†.
The resulting determinant of a 3 √ó 3 matrix can be evaluated in the same
manner: Use type 3 elementary row operations to introduce two zeros into
the Ô¨Årst column, and then expand along that column. This results in the
value ‚àí102. Therefore
det(A) = 1(‚àí1)1+2(‚àí102) = 102.

Sec. 4.4
Summary‚ÄîImportant Facts about Determinants
235
The reader should compare this calculation of det(A) with the preceding
ones to see how much less work is required when properties 1, 2, and 3 are
employed.
In the chapters that follow, we often have to evaluate the determinant of
matrices having special forms. The next two properties of the determinant
are useful in this regard:
4. The determinant of an upper triangular matrix is the product of its
diagonal entries. In particular, det(I) = 1.
5. If two rows (or columns) of a matrix are identical, then the determinant
of the matrix is zero.
As an illustration of property 4, notice that
det
‚éõ
‚éù
‚àí3
1
2
0
4
5
0
0
‚àí6
‚éû
‚é†= (‚àí3)(4)(‚àí6) = 72.
Property 4 provides an eÔ¨Écient method for evaluating the determinant of a
matrix:
(a) Use Gaussian elimination and properties 1, 2, and 3 above to reduce the
matrix to an upper triangular matrix.
(b) Compute the product of the diagonal entries.
For instance,
det
‚éõ
‚éú
‚éú
‚éù
1
‚àí1
2
1
2
‚àí1
‚àí1
4
‚àí4
5
‚àí10
‚àí6
3
‚àí2
10
‚àí1
‚éû
‚éü
‚éü
‚é†= det
‚éõ
‚éú
‚éú
‚éù
1
‚àí1
2
1
0
1
‚àí5
2
0
1
‚àí2
‚àí2
0
1
4
‚àí4
‚éû
‚éü
‚éü
‚é†
= det
‚éõ
‚éú
‚éú
‚éù
1
‚àí1
2
1
0
1
‚àí5
2
0
0
3
‚àí4
0
0
9
‚àí6
‚éû
‚éü
‚éü
‚é†= det
‚éõ
‚éú
‚éú
‚éù
1
‚àí1
2
1
0
1
‚àí5
2
0
0
3
‚àí4
0
0
0
6
‚éû
‚éü
‚éü
‚é†
= 1¬∑1¬∑3¬∑6 = 18.
The next three properties of the determinant are used frequently in later
chapters. Indeed, perhaps the most signiÔ¨Åcant property of the determinant
is that it provides a simple characterization of invertible matrices. (See prop-
erty 7.)
6. For any n √ó n matrices A and B, det(AB) = det(A)¬∑ det(B).

236
Chap. 4
Determinants
7. An n √ó n matrix A is invertible if and only if det(A) Ã∏= 0. Furthermore,
if A is invertible, then det(A‚àí1) =
1
det(A).
8. For any n √ó n matrix A, the determinants of A and At are equal.
For example, property 7 guarantees that the matrix A on page 233 is
invertible because det(A) = 102.
The Ô¨Ånal property, stated as Exercise 15 of Section 4.3, is used in Chap-
ter 5. It is a simple consequence of properties 6 and 7.
9. If A and B are similar matrices, then det(A) = det(B).
EXERCISES
1.
Label the following statements as true or false.
(a)
The determinant of a square matrix may be computed by expand-
ing the matrix along any row or column.
(b)
In evaluating the determinant of a matrix, it is wise to expand
along a row or column containing the largest number of zero en-
tries.
(c)
If two rows or columns of A are identical, then det(A) = 0.
(d)
If B is a matrix obtained by interchanging two rows or two columns
of A, then det(B) = det(A).
(e)
If B is a matrix obtained by multiplying each entry of some row
or column of A by a scalar, then det(B) = det(A).
(f)
If B is a matrix obtained from A by adding a multiple of some row
to a diÔ¨Äerent row, then det(B) = det(A).
(g)
The determinant of an upper triangular n√ón matrix is the product
of its diagonal entries.
(h)
For every A ‚ààMn√ón(F), det(At) = ‚àídet(A).
(i)
If A, B ‚ààMn√ón(F), then det(AB) = det(A)¬∑ det(B).
(j)
If Q is an invertible matrix, then det(Q‚àí1) = [det(Q)]‚àí1.
(k)
A matrix Q is invertible if and only if det(Q) Ã∏= 0.
2.
Evaluate the determinant of the following 2 √ó 2 matrices.
(a)

4
‚àí5
2
3
	
(b)

‚àí1
7
3
8
	
(c)

2 + i
‚àí1 + 3i
1 ‚àí2i
3 ‚àíi
	
(d)

3
4i
‚àí6i
2i
	
3.
Evaluate the determinant of the following matrices in the manner indi-
cated.

Sec. 4.4
Summary‚ÄîImportant Facts about Determinants
237
(a)
‚éõ
‚éù
0
1
2
‚àí1
0
‚àí3
2
3
0
‚éû
‚é†
along the Ô¨Årst row
(b)
‚éõ
‚éù
1
0
2
0
1
5
‚àí1
3
0
‚éû
‚é†
along the Ô¨Årst column
(c)
‚éõ
‚éù
0
1
2
‚àí1
0
‚àí3
2
3
0
‚éû
‚é†
along the second column
(d)
‚éõ
‚éù
1
0
2
0
1
5
‚àí1
3
0
‚éû
‚é†
along the third row
(e)
‚éõ
‚éù
0
1 + i
2
‚àí2i
0
1 ‚àíi
3
4i
0
‚éû
‚é†
along the third row
(f)
‚éõ
‚éù
i
2 + i
0
‚àí1
3
2i
0
‚àí1
1 ‚àíi
‚éû
‚é†
along the third column
(g)
‚éõ
‚éú
‚éú
‚éù
0
2
1
3
1
0
‚àí2
2
3
‚àí1
0
1
‚àí1
1
2
0
‚éû
‚éü
‚éü
‚é†
along the fourth column
(h)
‚éõ
‚éú
‚éú
‚éù
1
‚àí1
2
‚àí1
‚àí3
4
1
‚àí1
2
‚àí5
‚àí3
8
‚àí2
6
‚àí4
1
‚éû
‚éü
‚éü
‚é†
along the fourth row
4.
Evaluate the determinant of the following matrices by any legitimate
method.
(a)
‚éõ
‚éù
1
2
3
4
5
6
7
8
9
‚éû
‚é†
(b)
‚éõ
‚éù
‚àí1
3
2
4
‚àí8
1
2
2
5
‚éû
‚é†
(c)
‚éõ
‚éù
0
1
1
1
2
‚àí5
6
‚àí4
3
‚éû
‚é†
(d)
‚éõ
‚éù
1
‚àí2
3
‚àí1
2
‚àí5
3
‚àí1
2
‚éû
‚é†
(e)
‚éõ
‚éù
i
2
‚àí1
3
1 + i
2
‚àí2i
1
4 ‚àíi
‚éû
‚é†
(f)
‚éõ
‚éù
‚àí1
2 + i
3
1 ‚àíi
i
1
3i
2
‚àí1 + i
‚éû
‚é†
(g)
‚éõ
‚éú
‚éú
‚éù
1
0
‚àí2
3
‚àí3
1
1
2
0
4
‚àí1
1
2
3
0
1
‚éû
‚éü
‚éü
‚é†
(h)
‚éõ
‚éú
‚éú
‚éù
1
‚àí2
3
‚àí12
‚àí5
12
‚àí14
19
‚àí9
22
‚àí20
31
‚àí4
9
‚àí14
15
‚éû
‚éü
‚éü
‚é†
5.
Suppose that M ‚ààMn√ón(F) can be written in the form
M =

A
B
O
I
	
,
where A is a square matrix. Prove that det(M) = det(A).

238
Chap. 4
Determinants
6.‚Ä† Prove that if M ‚ààMn√ón(F) can be written in the form
M =

A
B
O
C
	
,
where A and C are square matrices, then det(M) = det(A)¬∑ det(C).
4.5‚àó
A CHARACTERIZATION OF THE DETERMINANT
In Sections 4.2 and 4.3, we showed that the determinant possesses a number of
properties. In this section, we show that three of these properties completely
characterize the determinant; that is, the only function Œ¥: Mn√ón(F) ‚ÜíF
having these three properties is the determinant. This characterization of
the determinant is the one used in Section 4.1 to establish the relationship
between det

u
v
	
and the area of the parallelogram determined by u and
v. The Ô¨Årst of these properties that characterize the determinant is the one
described in Theorem 4.3 (p. 212).
DeÔ¨Ånition. A function Œ¥: Mn√ón(F) ‚ÜíF is called an n-linear function
if it is a linear function of each row of an n √ó n matrix when the remaining
n ‚àí1 rows are held Ô¨Åxed, that is, Œ¥ is n-linear if, for every r = 1, 2, . . . , n, we
have
Œ¥
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
a1
...
ar‚àí1
u + kv
ar+1
...
an
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
= Œ¥
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
a1
...
ar‚àí1
u
ar+1
...
an
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
+ kŒ¥
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
a1
...
ar‚àí1
v
ar+1
...
an
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
whenever k is a scalar and u, v, and each ai are vectors in Fn.
Example 1
The function Œ¥: Mn√ón(F) ‚ÜíF deÔ¨Åned by Œ¥(A) = 0 for each A ‚ààMn√ón(F)
is an n-linear function.
‚ô¶
Example 2
For 1 ‚â§j ‚â§n, deÔ¨Åne Œ¥j : Mn√ón(F) ‚ÜíF by Œ¥j(A) = A1jA2j ¬∑ ¬∑ ¬∑ Anj for each
A ‚ààMn√ón(F); that is, Œ¥j(A) equals the product of the entries of column j of

Sec. 4.5
A Characterization of the Determinant
239
A. Let A ‚ààMn√ón(F), ai = (Ai1, Ai2, . . . , Ain), and v = (b1, b2, . . . , bn) ‚ààFn.
Then each Œ¥j is an n-linear function because, for any scalar k, we have
Œ¥
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
a1
...
ar‚àí1
ar + kv
ar+1
...
an
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
= A1j ¬∑ ¬∑ ¬∑ A(r‚àí1)j(Arj + kbj)A(r+1)j ¬∑ ¬∑ ¬∑ Anj
= A1j ¬∑ ¬∑ ¬∑ A(r‚àí1)jArjA(r+1)j ¬∑ ¬∑ ¬∑ Anj
+ A1j ¬∑ ¬∑ ¬∑ A(r‚àí1)j(kbj)A(r+1)j ¬∑ ¬∑ ¬∑ Anj
= A1j ¬∑ ¬∑ ¬∑ A(r‚àí1)jArjA(r+1)j ¬∑ ¬∑ ¬∑ Anj
+ k(A1j ¬∑ ¬∑ ¬∑ A(r‚àí1)jbjA(r+1)j ¬∑ ¬∑ ¬∑ Anj)
= Œ¥
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
a1
...
ar‚àí1
ar
ar+1
...
an
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
+ kŒ¥
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
a1
...
ar‚àí1
v
ar+1
...
an
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
.
‚ô¶
Example 3
The function Œ¥: Mn√ón(F) ‚ÜíF deÔ¨Åned for each A ‚ààMn√ón(F) by Œ¥(A) =
A11A22 ¬∑ ¬∑ ¬∑ Ann (i.e., Œ¥(A) equals the product of the diagonal entries of A) is
an n-linear function.
‚ô¶
Example 4
The function Œ¥: Mn√ón(R) ‚ÜíR deÔ¨Åned for each A ‚ààMn√ón(R) by Œ¥(A) =
tr(A) is not an n-linear function for n ‚â•2. For if I is the n √ó n identity
matrix and A is the matrix obtained by multiplying the Ô¨Årst row of I by 2,
then Œ¥(A) = n + 1 Ã∏= 2n = 2¬∑Œ¥(I).
‚ô¶
Theorem 4.3 (p. 212) asserts that the determinant is an n-linear function.
For our purposes this is the most important example of an n-linear function.
Now we introduce the second of the properties used in the characterization
of the determinant.
DeÔ¨Ånition. An n-linear function Œ¥: Mn√ón(F) ‚ÜíF is called alternating
if, for each A ‚ààMn√ón(F), we have Œ¥(A) = 0 whenever two adjacent rows of
A are identical.

240
Chap. 4
Determinants
Theorem 4.10. Let Œ¥: Mn√ón(F) ‚ÜíF be an alternating n-linear function.
(a) If A ‚ààMn√ón(F) and B is a matrix obtained from A by interchanging
any two rows of A, then Œ¥(B) = ‚àíŒ¥(A).
(b) If A ‚ààMn√ón(F) has two identical rows, then Œ¥(A) = 0.
Proof. (a) Let A ‚ààMn√ón(F), and let B be the matrix obtained from A
by interchanging rows r and s, where r < s. We Ô¨Årst establish the result in
the case that s = r + 1. Because Œ¥: Mn√ón(F) ‚ÜíF is an n-linear function
that is alternating, we have
0 = Œ¥
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
a1
...
ar + ar+1
ar + ar+1
...
an
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
= Œ¥
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
a1
...
ar
ar + ar+1
...
an
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
+ Œ¥
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
a1
...
ar+1
ar + ar+1
...
an
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
= Œ¥
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
a1
...
ar
ar
...
an
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
+ Œ¥
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
a1
...
ar
ar+1
...
an
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
+ Œ¥
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
a1
...
ar+1
ar
...
an
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
+ Œ¥
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
a1
...
ar+1
ar+1
...
an
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
= 0 + Œ¥(A) + Œ¥(B) + 0.
Thus Œ¥(B) = ‚àíŒ¥(A).
Next suppose that s > r + 1, and let the rows of A be a1, a2, . . . , an.
Beginning with ar and ar+1, successively interchange ar with the row that
follows it until the rows are in the sequence
a1, a2, . . . , ar‚àí1, ar+1, . . . , as, ar, as+1, . . . , an.
In all, s‚àír interchanges of adjacent rows are needed to produce this sequence.
Then successively interchange as with the row that precedes it until the rows
are in the order
a1, a2, . . . , ar‚àí1, as, ar+1, . . . , as‚àí1, ar, as+1, . . . , an.
This process requires an additional s ‚àír ‚àí1 interchanges of adjacent rows
and produces the matrix B. It follows from the preceding paragraph that
Œ¥(B) = (‚àí1)(s‚àír)+(s‚àír‚àí1)Œ¥(A) = ‚àíŒ¥(A).
(b) Suppose that rows r and s of A ‚ààMn√ón(F) are identical, where r < s.
If s = r + 1, then Œ¥(A) = 0 because Œ¥ is alternating and two adjacent rows

Sec. 4.5
A Characterization of the Determinant
241
of A are identical. If s > r + 1, let B be the matrix obtained from A by
interchanging rows r + 1 and s. Then Œ¥(B) = 0 because two adjacent rows of
B are identical. But Œ¥(B) = ‚àíŒ¥(A) by (a). Hence Œ¥(A) = 0.
Corollary 1. Let Œ¥: Mn√ón(F) ‚ÜíF be an alternating n-linear function.
If B is a matrix obtained from A ‚ààMn√ón(F) by adding a multiple of some
row of A to another row, then Œ¥(B) = Œ¥(A).
Proof. Let B be obtained from A ‚ààMn√ón(F) by adding k times row i of
A to row j, where j Ã∏= i, and let C be obtained from A by replacing row j of
A by row i of A. Then the rows of A, B, and C are identical except for row
j. Moreover, row j of B is the sum of row j of A and k times row j of C.
Since Œ¥ is an n-linear function and C has two identical rows, it follows that
Œ¥(B) = Œ¥(A) + kŒ¥(C) = Œ¥(A) + k¬∑0 = Œ¥(A).
The next result now follows as in the proof of the corollary to Theorem 4.6
(p. 216). (See Exercise 11.)
Corollary 2. Let Œ¥: Mn√ón(F) ‚ÜíF be an alternating n-linear function.
If M ‚ààMn√ón(F) has rank less than n, then Œ¥(M) = 0.
Proof. Exercise.
Corollary 3. Let Œ¥: Mn√ón(F) ‚ÜíF be an alternating n-linear function,
and let E1, E2, and E3 in Mn√ón(F) be elementary matrices of types 1, 2,
and 3, respectively. Suppose that E2 is obtained by multiplying some row
of I by the nonzero scalar k. Then Œ¥(E1) = ‚àíŒ¥(I), Œ¥(E2) = k¬∑Œ¥(I), and
Œ¥(E3) = Œ¥(I).
Proof. Exercise.
We wish to show that under certain circumstances, the only alternating
n-linear function Œ¥: Mn√ón(F) ‚ÜíF is the determinant, that is, Œ¥(A) = det(A)
for all A ‚ààMn√ón(F). In view of Corollary 3 to Theorem 4.10 and the facts
on page 223 about the determinant of an elementary matrix, this can happen
only if Œ¥(I) = 1. Hence the third condition that is used in the characterization
of the determinant is that the determinant of the n √ó n identity matrix is 1.
Before we can establish the desired characterization of the determinant, we
must Ô¨Årst show that an alternating n-linear function Œ¥ such that Œ¥(I) = 1 is
a multiplicative function. The proof of this result is identical to the proof of
Theorem 4.7 (p. 223), and so it is omitted. (See Exercise 12.)
Theorem 4.11. Let Œ¥: Mn√ón(F) ‚ÜíF be an alternating n-linear function
such that Œ¥(I) = 1. For any A, B ‚ààMn√ón(F), we have Œ¥(AB) = Œ¥(A)¬∑Œ¥(B).

242
Chap. 4
Determinants
Proof. Exercise.
Theorem 4.12. If Œ¥: Mn√ón(F) ‚ÜíF is an alternating n-linear function
such that Œ¥(I) = 1, then Œ¥(A) = det(A) for every A ‚ààMn√ón(F).
Proof. Let Œ¥: Mn√ón(F) ‚ÜíF be an alternating n-linear function such that
Œ¥(I) = 1, and let A ‚ààMn√ón(F). If A has rank less than n, then by Corollary 2
to Theorem 4.10, Œ¥(A) = 0. Since the corollary to Theorem 4.6 (p. 217) gives
det(A) = 0, we have Œ¥(A) = det(A) in this case. If, on the other hand, A has
rank n, then A is invertible and hence is the product of elementary matrices
(Corollary 3 to Theorem 3.6 p. 159), say A = Em ¬∑ ¬∑ ¬∑ E2E1. Since Œ¥(I) = 1,
it follows from Corollary 3 to Theorem 4.10 and the facts on page 223 that
Œ¥(E) = det(E) for every elementary matrix E.
Hence by Theorems 4.11
and 4.7 (p. 223), we have
Œ¥(A) = Œ¥(Em ¬∑ ¬∑ ¬∑ E2E1)
= Œ¥(Em)¬∑ ¬∑ ¬∑ ¬∑ ¬∑Œ¥(E2)¬∑Œ¥(E1)
= det(Em)¬∑ ¬∑ ¬∑ ¬∑ ¬∑ det(E2)¬∑ det(E1)
= det(Em ¬∑ ¬∑ ¬∑ E2E1)
= det(A).
Theorem 4.12 provides the desired characterization of the determinant: It
is the unique function Œ¥: Mn√ón(F) ‚ÜíF that is n-linear, is alternating, and
has the property that Œ¥(I) = 1.
EXERCISES
1.
Label the following statements as true or false.
(a)
Any n-linear function Œ¥: Mn√ón(F) ‚ÜíF is a linear transformation.
(b)
Any n-linear function Œ¥: Mn√ón(F) ‚ÜíF is a linear function of each
row of an n √ó n matrix when the other n ‚àí1 rows are held Ô¨Åxed.
(c)
If Œ¥: Mn√ón(F) ‚ÜíF is an alternating n-linear function and the
matrix A ‚ààMn√ón(F) has two identical rows, then Œ¥(A) = 0.
(d)
If Œ¥: Mn√ón(F) ‚ÜíF is an alternating n-linear function and B is
obtained from A ‚ààMn√ón(F) by interchanging two rows of A, then
Œ¥(B) = Œ¥(A).
(e)
There is a unique alternating n-linear function Œ¥: Mn√ón(F) ‚ÜíF.
(f)
The function Œ¥: Mn√ón(F) ‚ÜíF deÔ¨Åned by Œ¥(A) = 0 for every
A ‚ààMn√ón(F) is an alternating n-linear function.
2.
Determine all the 1-linear functions Œ¥: M1√ó1(F) ‚ÜíF.
Determine which of the functions Œ¥: M3√ó3(F) ‚ÜíF in Exercises 3‚Äì10 are
3-linear functions. Justify each answer.

Sec. 4.5
A Characterization of the Determinant
243
3.
Œ¥(A) = k, where k is any nonzero scalar
4.
Œ¥(A) = A22
5.
Œ¥(A) = A11A23A32
6.
Œ¥(A) = A11 + A23 + A32
7.
Œ¥(A) = A11A21A32
8.
Œ¥(A) = A11A31A32
9.
Œ¥(A) = A2
11A2
22A2
33
10.
Œ¥(A) = A11A22A33 ‚àíA11A21A32
11.
Prove Corollaries 2 and 3 of Theorem 4.10.
12.
Prove Theorem 4.11.
13.
Prove that det: M2√ó2(F) ‚ÜíF is a 2-linear function of the columns of
a matrix.
14.
Let a, b, c, d ‚ààF. Prove that the function Œ¥: M2√ó2(F) ‚ÜíF deÔ¨Åned by
Œ¥(A) = A11A22a + A11A21b + A12A22c + A12A21d is a 2-linear function.
15.
Prove that Œ¥: M2√ó2(F) ‚ÜíF is a 2-linear function if and only if it has
the form
Œ¥(A) = A11A22a + A11A21b + A12A22c + A12A21d
for some scalars a, b, c, d ‚ààF.
16.
Prove that if Œ¥: Mn√ón(F) ‚ÜíF is an alternating n-linear function, then
there exists a scalar k such that Œ¥(A) = k det(A) for all A ‚ààMn√ón(F).
17.
Prove that a linear combination of two n-linear functions is an n-linear
function, where the sum and scalar product of n-linear functions are as
deÔ¨Åned in Example 3 of Section 1.2 (p. 9).
18.
Prove that the set of all n-linear functions over a Ô¨Åeld F is a vector
space over F under the operations of function addition and scalar mul-
tiplication as deÔ¨Åned in Example 3 of Section 1.2 (p. 9).
19.
Let Œ¥: Mn√ón(F) ‚ÜíF be an n-linear function and F a Ô¨Åeld that does
not have characteristic two. Prove that if Œ¥(B) = ‚àíŒ¥(A) whenever B is
obtained from A ‚ààMn√ón(F) by interchanging any two rows of A, then
Œ¥(M) = 0 whenever M ‚ààMn√ón(F) has two identical rows.
20.
Give an example to show that the implication in Exercise 19 need not
hold if F has characteristic two.

244
Chap. 4
Determinants
INDEX OF DEFINITIONS FOR CHAPTER 4
Alternating n-linear function
239
Angle between two vectors
202
Cofactor
210
Cofactor expansion along the Ô¨Årst
row
210
Cramer‚Äôs rule
224
Determinant of a 2 √ó 2 matrix
200
Determinant of a matrix
210
Left-handed
coordinate
system
203
n-linear function
238
Orientation
of
an
ordered
basis
202
Parallelepiped, volume of
226
Parallelogram
determined
by
two
vectors
203
Right-handed
coordinate
system
202

5
Diagonalization
5.1
Eigenvalues and Eigenvectors
5.2
Diagonalizability
5.3*
Matrix Limits and Markov Chains
5.4
Invariant Subspaces and the Cayley-Hamilton Theorem
This chapter is concerned with the so-called diagonalization problem. For
a given linear operator T on a Ô¨Ånite-dimensional vector space V, we seek
answers to the following questions.
1. Does there exist an ordered basis Œ≤ for V such that [T]Œ≤ is a diagonal
matrix?
2. If such a basis exists, how can it be found?
Since computations involving diagonal matrices are simple, an aÔ¨Érmative
answer to question 1 leads us to a clearer understanding of how the operator T
acts on V, and an answer to question 2 enables us to obtain easy solutions to
many practical problems that can be formulated in a linear algebra context.
We consider some of these problems and their solutions in this chapter; see,
for example, Section 5.3.
A solution to the diagonalization problem leads naturally to the concepts
of eigenvalue and eigenvector.
Aside from the important role that these
concepts play in the diagonalization problem, they also prove to be useful
tools in the study of many nondiagonalizable operators, as we will see in
Chapter 7.
5.1
EIGENVALUES AND EIGENVECTORS
In Example 3 of Section 2.5, we were able to obtain a formula for the
reÔ¨Çection of R2 about the line y = 2x. The key to our success was to Ô¨Ånd a
basis Œ≤‚Ä≤ for which [T]Œ≤‚Ä≤ is a diagonal matrix. We now introduce the name for
an operator or matrix that has such a basis.
DeÔ¨Ånitions. A linear operator T on a Ô¨Ånite-dimensional vector space V
is called diagonalizable if there is an ordered basis Œ≤ for V such that [T]Œ≤
245

246
Chap. 5
Diagonalization
is a diagonal matrix. A square matrix A is called diagonalizable if LA is
diagonalizable.
We want to determine when a linear operator T on a Ô¨Ånite-dimensional
vector space V is diagonalizable and, if so, how to obtain an ordered basis
Œ≤ = {v1, v2, . . . , vn} for V such that [T]Œ≤ is a diagonal matrix. Note that, if
D = [T]Œ≤ is a diagonal matrix, then for each vector vj ‚ààŒ≤, we have
T(vj) =
n

i=1
Dijvi = Djjvj = Œªjvj,
where Œªj = Djj.
Conversely, if Œ≤ = {v1, v2, . . . , vn} is an ordered basis for V such that
T(vj) = Œªjvj for some scalars Œª1, Œª2, . . . , Œªn, then clearly
[T]Œ≤ =
‚éõ
‚éú
‚éú
‚éú
‚éù
Œª1
0
¬∑ ¬∑ ¬∑
0
0
Œª2
¬∑ ¬∑ ¬∑
0
...
...
...
0
0
¬∑ ¬∑ ¬∑
Œªn
‚éû
‚éü
‚éü
‚éü
‚é†.
In the preceding paragraph, each vector v in the basis Œ≤ satisÔ¨Åes the
condition that T(v) = Œªv for some scalar Œª. Moreover, because v lies in a
basis, v is nonzero. These computations motivate the following deÔ¨Ånitions.
DeÔ¨Ånitions. Let T be a linear operator on a vector space V. A nonzero
vector v ‚ààV is called an eigenvector of T if there exists a scalar Œª such
that T(v) = Œªv. The scalar Œª is called the eigenvalue corresponding to the
eigenvector v.
Let A be in Mn√ón(F). A nonzero vector v ‚ààFn is called an eigenvector
of A if v is an eigenvector of LA; that is, if Av = Œªv for some scalar Œª. The
scalar Œª is called the eigenvalue of A corresponding to the eigenvector v.
The words characteristic vector and proper vector are also used in place of
eigenvector. The corresponding terms for eigenvalue are characteristic value
and proper value.
Note that a vector is an eigenvector of a matrix A if and only if it is an
eigenvector of LA. Likewise, a scalar Œª is an eigenvalue of A if and only if it is
an eigenvalue of LA. Using the terminology of eigenvectors and eigenvalues,
we can summarize the preceding discussion as follows.
Theorem 5.1. A linear operator T on a Ô¨Ånite-dimensional vector space V
is diagonalizable if and only if there exists an ordered basis Œ≤ for V consisting
of eigenvectors of T. Furthermore, if T is diagonalizable, Œ≤ = {v1, v2, . . . , vn}
is an ordered basis of eigenvectors of T, and D = [T]Œ≤, then D is a diagonal
matrix and Djj is the eigenvalue corresponding to vj for 1 ‚â§j ‚â§n.

Sec. 5.1
Eigenvalues and Eigenvectors
247
To diagonalize a matrix or a linear operator is to Ô¨Ånd a basis of eigenvec-
tors and the corresponding eigenvalues.
Before continuing our study of the diagonalization problem, we consider
three examples of eigenvalues and eigenvectors.
Example 1
Let
A =

1
3
4
2
	
,
v1 =

1
‚àí1
	
,
and
v2 =

3
4
	
.
Since
LA(v1) =

1
3
4
2
	 
1
‚àí1
	
=

‚àí2
2
	
= ‚àí2

1
‚àí1
	
= ‚àí2v1,
v1 is an eigenvector of LA, and hence of A. Here Œª1 = ‚àí2 is the eigenvalue
corresponding to v1. Furthermore,
LA(v2) =

1
3
4
2
	 
3
4
	
=

15
20
	
= 5

3
4
	
= 5v2,
and so v2 is an eigenvector of LA, and hence of A, with the corresponding
eigenvalue Œª2 = 5. Note that Œ≤ = {v1, v2} is an ordered basis for R2 consisting
of eigenvectors of both A and LA, and therefore A and LA are diagonalizable.
Moreover, by Theorem 5.1,
[LA]Œ≤ =

‚àí2
0
0
5
	
.
‚ô¶
Example 2
Let T be the linear operator on R2 that rotates each vector in the plane
through an angle of œÄ/2. It is clear geometrically that for any nonzero vector
v, the vectors v and T(v) are not collinear; hence T(v) is not a multiple of
v. Therefore T has no eigenvectors and, consequently, no eigenvalues. Thus
there exist operators (and matrices) with no eigenvalues or eigenvectors. Of
course, such operators and matrices are not diagonalizable.
‚ô¶
Example 3
Let C‚àû(R) denote the set of all functions f : R ‚ÜíR having derivatives of all
orders. (Thus C‚àû(R) includes the polynomial functions, the sine and cosine
functions, the exponential functions, etc.) Clearly, C‚àû(R) is a subspace of
the vector space F(R, R) of all functions from R to R as deÔ¨Åned in Section
1.2.
Let T: C‚àû(R) ‚ÜíC‚àû(R) be the function deÔ¨Åned by T(f) = f ‚Ä≤, the
derivative of f. It is easily veriÔ¨Åed that T is a linear operator on C‚àû(R). We
determine the eigenvalues and eigenvectors of T.

248
Chap. 5
Diagonalization
Suppose that f is an eigenvector of T with corresponding eigenvalue Œª.
Then f ‚Ä≤ = T(f) = Œªf. This is a Ô¨Årst-order diÔ¨Äerential equation whose solu-
tions are of the form f(t) = ceŒªt for some constant c. Consequently, every
real number Œª is an eigenvalue of T, and Œª corresponds to eigenvectors of the
form ceŒªt for c Ã∏= 0. Note that for Œª = 0, the eigenvectors are the nonzero
constant functions.
‚ô¶
In order to obtain a basis of eigenvectors for a matrix (or a linear opera-
tor), we need to be able to determine its eigenvalues and eigenvectors. The
following theorem gives us a method for computing eigenvalues.
Theorem 5.2. Let A ‚ààMn√ón(F). Then a scalar Œª is an eigenvalue of A
if and only if det(A ‚àíŒªIn) = 0.
Proof. A scalar Œª is an eigenvalue of A if and only if there exists a nonzero
vector v ‚ààFn such that Av = Œªv, that is, (A ‚àíŒªIn)(v) = 0. By Theorem 2.5
(p. 71), this is true if and only if A ‚àíŒªIn is not invertible. However, this
result is equivalent to the statement that det(A ‚àíŒªIn) = 0.
DeÔ¨Ånition.
Let A ‚ààMn√ón(F). The polynomial f(t) = det(A ‚àítIn) is
called the characteristic polynomial 1 of A.
Theorem 5.2 states that the eigenvalues of a matrix are the zeros of its
characteristic polynomial. When determining the eigenvalues of a matrix or
a linear operator, we normally compute its characteristic polynomial, as in
the next example.
Example 4
To Ô¨Ånd the eigenvalues of
A =

1
1
4
1
	
‚ààM2√ó2(R),
we compute its characteristic polynomial:
det(A ‚àítI2) = det

1 ‚àít
1
4
1 ‚àít
	
= t2 ‚àí2t ‚àí3 = (t ‚àí3)(t + 1).
It follows from Theorem 5.2 that the only eigenvalues of A are 3 and ‚àí1.
‚ô¶
1The observant reader may have noticed that the entries of the matrix A ‚àítIn
are not scalars in the Ô¨Åeld F. They are, however, scalars in another Ô¨Åeld F(t), the
Ô¨Åeld of quotients of polynomials in t with coeÔ¨Écients from F. Consequently, any
results proved about determinants in Chapter 4 remain valid in this context.

Sec. 5.1
Eigenvalues and Eigenvectors
249
It is easily shown that similar matrices have the same characteristic poly-
nomial (see Exercise 12). This fact enables us to deÔ¨Åne the characteristic
polynomial of a linear operator as follows.
DeÔ¨Ånition. Let T be a linear operator on an n-dimensional vector space
V with ordered basis Œ≤. We deÔ¨Åne the characteristic polynomial f(t) of
T to be the characteristic polynomial of A = [T]Œ≤. That is,
f(t) = det(A ‚àítIn).
The remark preceding this deÔ¨Ånition shows that the deÔ¨Ånition is indepen-
dent of the choice of ordered basis Œ≤. Thus if T is a linear operator on a
Ô¨Ånite-dimensional vector space V and Œ≤ is an ordered basis for V, then Œª is
an eigenvalue of T if and only if Œª is an eigenvalue of [T]Œ≤. We often denote
the characteristic polynomial of an operator T by det(T ‚àítI).
Example 5
Let T be the linear operator on P2(R) deÔ¨Åned by T(f(x)) = f(x)+(x+1)f ‚Ä≤(x),
let Œ≤ be the standard ordered basis for P2(R), and let A = [T]Œ≤. Then
A =
‚éõ
‚éù
1
1
0
0
2
2
0
0
3
‚éû
‚é†.
The characteristic polynomial of T is
det(A ‚àítI3) = det
‚éõ
‚éù
1 ‚àít
1
0
0
2 ‚àít
2
0
0
3 ‚àít
‚éû
‚é†
= (1 ‚àít)(2 ‚àít)(3 ‚àít)
= ‚àí(t ‚àí1)(t ‚àí2)(t ‚àí3).
Hence Œª is an eigenvalue of T (or A) if and only if Œª = 1, 2, or 3.
‚ô¶
Examples 4 and 5 suggest that the characteristic polynomial of an n √ó n
matrix A is a polynomial of degree n. The next theorem tells us even more.
It can be proved by a straightforward induction argument.
Theorem 5.3. Let A ‚ààMn√ón(F).
(a) The characteristic polynomial of A is a polynomial of degree n with
leading coeÔ¨Écient (‚àí1)n.
(b) A has at most n distinct eigenvalues.
Proof. Exercise.

250
Chap. 5
Diagonalization
Theorem 5.2 enables us to determine all the eigenvalues of a matrix or
a linear operator on a Ô¨Ånite-dimensional vector space provided that we can
compute the zeros of the characteristic polynomial. Our next result gives
us a procedure for determining the eigenvectors corresponding to a given
eigenvalue.
Theorem 5.4. Let T be a linear operator on a vector space V, and let Œª
be an eigenvalue of T. A vector v ‚ààV is an eigenvector of T corresponding
to Œª if and only if v Ã∏= 0 and v ‚ààN(T ‚àíŒªI).
Proof. Exercise.
Example 6
To Ô¨Ånd all the eigenvectors of the matrix
A =

1
1
4
1
	
in Example 4, recall that A has two eigenvalues, Œª1 = 3 and Œª2 = ‚àí1. We
begin by Ô¨Ånding all the eigenvectors corresponding to Œª1 = 3. Let
B1 = A ‚àíŒª1I =

1
1
4
1
	
‚àí

3
0
0
3
	
=

‚àí2
1
4
‚àí2
	
.
Then
x =

x1
x2
	
‚ààR2
is an eigenvector corresponding to Œª1 = 3 if and only if x Ã∏= 0 and x ‚ààN(LB1);
that is, x Ã∏= 0 and

‚àí2
1
4
‚àí2
	 
x1
x2
	
=

‚àí2x1 + x2
4x1 ‚àí2x2
	
=

0
0
	
.
Clearly the set of all solutions to this equation is

t

1
2
	
: t ‚ààR

.
Hence x is an eigenvector corresponding to Œª1 = 3 if and only if
x = t

1
2
	
for some t Ã∏= 0.
Now suppose that x is an eigenvector of A corresponding to Œª2 = ‚àí1. Let
B2 = A ‚àíŒª2I =

1
1
4
1
	
‚àí

‚àí1
0
0
‚àí1
	
=

2
1
4
2
	
.

Sec. 5.1
Eigenvalues and Eigenvectors
251
Then
x =

x1
x2
	
‚ààN(LB2)
if and only if x is a solution to the system
2x1 + x2 = 0
4x1 + 2x2 = 0.
Hence
N(LB2) =

t

1
‚àí2
	
: t ‚ààR

.
Thus x is an eigenvector corresponding to Œª2 = ‚àí1 if and only if
x = t

1
‚àí2
	
for some t Ã∏= 0.
Observe that

1
2
	
,

1
‚àí2
	
is a basis for R2 consisting of eigenvectors of A. Thus LA, and hence A, is
diagonalizable.
‚ô¶
Suppose that Œ≤ is a basis for Fn consisting of eigenvectors of A.
The
corollary to Theorem 2.23 assures us that if Q is the n √ó n matrix whose
columns are the vectors in Œ≤, then Q‚àí1AQ is a diagonal matrix. In Example 6,
for instance, if
Q =

1
1
2
‚àí2
	
,
then
Q‚àí1AQ =

3
0
0
‚àí1
	
.
Of course, the diagonal entries of this matrix are the eigenvalues of A that
correspond to the respective columns of Q.
To Ô¨Ånd the eigenvectors of a linear operator T on an n-dimensional vector
space, select an ordered basis Œ≤ for V and let A = [T]Œ≤. Figure 5.1 is the
special case of Figure 2.2 in Section 2.4 in which V = W and Œ≤ = Œ≥. Recall
that for v ‚ààV, œÜŒ≤(v) = [v]Œ≤, the coordinate vector of v relative to Œ≤. We
show that v ‚ààV is an eigenvector of T corresponding to Œª if and only if œÜŒ≤(v)

252
Chap. 5
Diagonalization
V
T
‚àí‚àí‚àí‚àí‚ÜíV
œÜŒ≤
‚èê‚èê!
‚èê‚èê!œÜŒ≤
Fn
LA
‚àí‚àí‚àí‚àí‚ÜíFn
Figure 5.1
is an eigenvector of A corresponding to Œª. Suppose that v is an eigenvector
of T corresponding to Œª. Then T(v) = Œªv. Hence
AœÜŒ≤(v) = LAœÜŒ≤(v) = œÜŒ≤T(v) = œÜŒ≤(Œªv) = ŒªœÜŒ≤(v).
Now œÜŒ≤(v) Ã∏= 0, since œÜŒ≤ is an isomorphism; hence œÜŒ≤(v) is an eigenvector
of A.
This argument is reversible, and so we can establish that if œÜŒ≤(v)
is an eigenvector of A corresponding to Œª, then v is an eigenvector of T
corresponding to Œª. (See Exercise 13.)
An equivalent formulation of the result discussed in the preceding para-
graph is that for an eigenvalue Œª of A (and hence of T), a vector y ‚ààFn is an
eigenvector of A corresponding to Œª if and only if œÜ‚àí1
Œ≤ (y) is an eigenvector of
T corresponding to Œª.
Thus we have reduced the problem of Ô¨Ånding the eigenvectors of a linear
operator on a Ô¨Ånite-dimensional vector space to the problem of Ô¨Ånding the
eigenvectors of a matrix. The next example illustrates this procedure.
Example 7
Let T be the linear operator on P2(R) deÔ¨Åned in Example 5, and let Œ≤ be the
standard ordered basis for P2(R). Recall that T has eigenvalues 1, 2, and 3
and that
A = [T]Œ≤ =
‚éõ
‚éù
1
1
0
0
2
2
0
0
3
‚éû
‚é†.
We consider each eigenvalue separately.
Let Œª1 = 1, and deÔ¨Åne
B1 = A ‚àíŒª1I =
‚éõ
‚éù
0
1
0
0
1
2
0
0
2
‚éû
‚é†.
Then
x =
‚éõ
‚éù
x1
x2
x3
‚éû
‚é†‚ààR3

Sec. 5.1
Eigenvalues and Eigenvectors
253
is an eigenvector corresponding to Œª1 = 1 if and only if x Ã∏= 0 and x ‚ààN(LB1);
that is, x is a nonzero solution to the system
x2
= 0
x2 + 2x3 = 0
2x3 = 0.
Notice that this system has three unknowns, x1, x2, and x3, but one of these,
x1, does not actually appear in the system. Since the values of x1 do not
aÔ¨Äect the system, we assign x1 a parametric value, say x1 = a, and solve the
system for x2 and x3. Clearly, x2 = x3 = 0, and so the eigenvectors of A
corresponding to Œª1 = 1 are of the form
a
‚éõ
‚éù
1
0
0
‚éû
‚é†= ae1
for a Ã∏= 0. Consequently, the eigenvectors of T corresponding to Œª1 = 1 are
of the form
œÜ‚àí1
Œ≤ (ae1) = aœÜ‚àí1
Œ≤ (e1) = a¬∑1 = a
for any a Ã∏= 0. Hence the nonzero constant polynomials are the eigenvectors
of T corresponding to Œª1 = 1.
Next let Œª2 = 2, and deÔ¨Åne
B2 = A ‚àíŒª2I =
‚éõ
‚éù
‚àí1
1
0
0
0
2
0
0
1
‚éû
‚é†.
It is easily veriÔ¨Åed that
N(LB2) =
‚éß
‚é®
‚é©a
‚éõ
‚éù
1
1
0
‚éû
‚é†: a ‚ààR
‚é´
‚é¨
‚é≠,
and hence the eigenvectors of T corresponding to Œª2 = 2 are of the form
œÜ‚àí1
Œ≤
‚éõ
‚éùa
‚éõ
‚éù
1
1
0
‚éû
‚é†
‚éû
‚é†= aœÜ‚àí1
Œ≤ (e1 + e2) = a(1 + x)
for a Ã∏= 0.
Finally, consider Œª3 = 3 and
B3 = A ‚àíŒª3I =
‚éõ
‚éù
‚àí2
1
0
0
‚àí1
2
0
0
0
‚éû
‚é†.

254
Chap. 5
Diagonalization
Since
N(LB3) =
‚éß
‚é®
‚é©a
‚éõ
‚éù
1
2
1
‚éû
‚é†: a ‚ààR
‚é´
‚é¨
‚é≠,
the eigenvectors of T corresponding to Œª3 = 3 are of the form
œÜ‚àí1
Œ≤
‚éõ
‚éùa
‚éõ
‚éù
1
2
1
‚éû
‚é†
‚éû
‚é†= aœÜ‚àí1
Œ≤ (e1 + 2e2 + e3) = a(1 + 2x + x2)
for a Ã∏= 0.
For each eigenvalue, select the corresponding eigenvector with a = 1 in the
preceding descriptions to obtain Œ≥ = {1, 1+x, 1+2x+x2}, which is an ordered
basis for P2(R) consisting of eigenvectors of T. Thus T is diagonalizable, and
[T]Œ≥ =
‚éõ
‚éù
1
0
0
0
2
0
0
0
3
‚éû
‚é†.
‚ô¶
We close this section with a geometric description of how a linear operator
T acts on an eigenvector in the context of a vector space V over R. Let v be
an eigenvector of T and Œª be the corresponding eigenvalue. We can think of
W = span({v}), the one-dimensional subspace of V spanned by v, as a line
in V that passes through 0 and v. For any w ‚ààW, w = cv for some scalar c,
and hence
T(w) = T(cv) = cT(v) = cŒªv = Œªw;
so T acts on the vectors in W by multiplying each such vector by Œª. There
are several possible ways for T to act on the vectors in W, depending on the
value of Œª. We consider several cases. (See Figure 5.2.)
Case 1. If Œª > 1, then T moves vectors in W farther from 0 by a factor
of Œª.
Case 2. If Œª = 1, then T acts as the identity operator on W.
Case 3. If 0 < Œª < 1, then T moves vectors in W closer to 0 by a factor
of Œª.
Case 4. If Œª = 0, then T acts as the zero transformation on W.
Case 5. If Œª < 0, then T reverses the orientation of W; that is, T moves
vectors in W from one side of 0 to the other.

Sec. 5.1
Eigenvalues and Eigenvectors
255
((((((((((((((((((((
r0
ry
r
T(y)
Case 1: Œª > 1
((((((((((((((((((((
r0
r
y = T(y)
Case 2: Œª = 1
((((((((((((((((((((
r0
r
T(y)
ry
Case 3: 0 < Œª < 1
((((((((((((((((((((
r
0 = T(y)
ry
Case 4: Œª = 0
((((((((((((((((((((
r0
ry
r
T(y)
Case 5: Œª < 0
Figure 5.2: The action of T on W = span({x}) when x is an eigenvector of T.
To illustrate these ideas, we consider the linear operators in Examples 3,
4, and 2 of Section 2.1.
For the operator T on R2 deÔ¨Åned by T(a1, a2) = (a1, ‚àía2), the reÔ¨Çection
about the x-axis, e1 and e2 are eigenvectors of T with corresponding eigen-
values 1 and ‚àí1, respectively. Since e1 and e2 span the x-axis and the y-axis,
respectively, T acts as the identity on the x-axis and reverses the orientation
of the y-axis.
For the operator T on R2 deÔ¨Åned by T(a1, a2) = (a1, 0), the projection on
the x-axis, e1 and e2 are eigenvectors of T with corresponding eigenvalues 1
and 0, respectively. Thus, T acts as the identity on the x-axis and as the zero
operator on the y-axis.
Finally, we generalize Example 2 of this section by considering the oper-
ator that rotates the plane through the angle Œ∏, which is deÔ¨Åned by
TŒ∏(a1, a2) = (a1 cos Œ∏ ‚àía2 sin Œ∏, a1 sin Œ∏ + a2 cos Œ∏).
Suppose that 0 < Œ∏ < œÄ. Then for any nonzero vector v, the vectors v and
TŒ∏(v) are not collinear, and hence TŒ∏ maps no one-dimensional subspace of
R2 into itself. But this implies that TŒ∏ has no eigenvectors and therefore
no eigenvalues. To conÔ¨Årm this conclusion, we note that the characteristic
polynomial of TŒ∏ is
det(TŒ∏ ‚àítI) = det

cos Œ∏ ‚àít
‚àísin Œ∏
sin Œ∏
cos Œ∏ ‚àít
	
= t2 ‚àí(2 cos Œ∏)t + 1,

256
Chap. 5
Diagonalization
which has no real zeros because, for 0 < Œ∏ < œÄ, the discriminant 4 cos2 Œ∏ ‚àí4
is negative.
EXERCISES
1.
Label the following statements as true or false.
(a)
Every linear operator on an n-dimensional vector space has n dis-
tinct eigenvalues.
(b)
If a real matrix has one eigenvector, then it has an inÔ¨Ånite number
of eigenvectors.
(c)
There exists a square matrix with no eigenvectors.
(d)
Eigenvalues must be nonzero scalars.
(e)
Any two eigenvectors are linearly independent.
(f)
The sum of two eigenvalues of a linear operator T is also an eigen-
value of T.
(g)
Linear operators on inÔ¨Ånite-dimensional vector spaces never have
eigenvalues.
(h)
An n √ó n matrix A with entries from a Ô¨Åeld F is similar to a
diagonal matrix if and only if there is a basis for Fn consisting of
eigenvectors of A.
(i)
Similar matrices always have the same eigenvalues.
(j)
Similar matrices always have the same eigenvectors.
(k)
The sum of two eigenvectors of an operator T is always an eigen-
vector of T.
2.
For each of the following linear operators T on a vector space V and
ordered bases Œ≤, compute [T]Œ≤, and determine whether Œ≤ is a basis
consisting of eigenvectors of T.
(a)
V = R2, T

a
b
	
=

10a ‚àí6b
17a ‚àí10b
	
, and Œ≤ =

1
2
	
,

2
3
	
(b)
V = P1(R), T(a + bx) = (6a ‚àí6b) + (12a ‚àí11b)x, and
Œ≤ = {3 + 4x, 2 + 3x}
(c)
V = R3, T
‚éõ
‚éù
a
b
c
‚éû
‚é†=
‚éõ
‚éù
3a + 2b ‚àí2c
‚àí4a ‚àí3b + 2c
‚àíc
‚éû
‚é†, and
Œ≤ =
‚éß
‚é®
‚é©
‚éõ
‚éù
0
1
1
‚éû
‚é†,
‚éõ
‚éù
1
‚àí1
0
‚éû
‚é†,
‚éõ
‚éù
1
0
2
‚éû
‚é†
‚é´
‚é¨
‚é≠
(d)
V = P2(R), T(a + bx + cx2) =
(‚àí4a + 2b ‚àí2c) ‚àí(7a + 3b + 7c)x + (7a + b + 5c)x2,
and Œ≤ = {x ‚àíx2, ‚àí1 + x2, ‚àí1 ‚àíx + x2}

Sec. 5.1
Eigenvalues and Eigenvectors
257
(e)
V = P3(R), T(a + bx + cx2 + dx3) =
‚àíd + (‚àíc + d)x + (a + b ‚àí2c)x2 + (‚àíb + c ‚àí2d)x3,
and Œ≤ = {1 ‚àíx + x3, 1 + x2, 1, x + x2}
(f)
V = M2√ó2(R), T

a
b
c
d
	
=

‚àí7a ‚àí4b + 4c ‚àí4d
b
‚àí8a ‚àí4b + 5c ‚àí4d
d
	
, and
Œ≤ =

1
0
1
0
	
,

‚àí1
2
0
0
	
,

1
0
2
0
	
,

‚àí1
0
0
2
	
3.
For each of the following matrices A ‚ààMn√ón(F),
(i) Determine all the eigenvalues of A.
(ii) For each eigenvalue Œª of A, Ô¨Ånd the set of eigenvectors correspond-
ing to Œª.
(iii) If possible, Ô¨Ånd a basis for Fn consisting of eigenvectors of A.
(iv) If successful in Ô¨Ånding such a basis, determine an invertible matrix
Q and a diagonal matrix D such that Q‚àí1AQ = D.
(a)
A =

1
2
3
2
	
for F = R
(b)
A =
‚éõ
‚éù
0
‚àí2
‚àí3
‚àí1
1
‚àí1
2
2
5
‚éû
‚é†
for F = R
(c)
A =

i
1
2
‚àíi
	
for F = C
(d)
A =
‚éõ
‚éù
2
0
‚àí1
4
1
‚àí4
2
0
‚àí1
‚éû
‚é†
for F = R
4.
For each linear operator T on V, Ô¨Ånd the eigenvalues of T and an ordered
basis Œ≤ for V such that [T]Œ≤ is a diagonal matrix.
(a)
V = R2 and T(a, b) = (‚àí2a + 3b, ‚àí10a + 9b)
(b)
V = R3 and T(a, b, c) = (7a ‚àí4b + 10c, 4a ‚àí3b + 8c, ‚àí2a + b ‚àí2c)
(c)
V = R3 and T(a, b, c) = (‚àí4a+3b‚àí6c, 6a‚àí7b+12c, 6a‚àí6b+11c)
(d)
V = P1(R) and T(ax + b) = (‚àí6a + 2b)x + (‚àí6a + b)
(e)
V = P2(R) and T(f(x)) = xf ‚Ä≤(x) + f(2)x + f(3)
(f)
V = P3(R) and T(f(x)) = f(x) + f(2)x
(g)
V = P3(R) and T(f(x)) = xf ‚Ä≤(x) + f ‚Ä≤‚Ä≤(x) ‚àíf(2)
(h)
V = M2√ó2(R) and T

a
b
c
d
	
=

d
b
c
a
	

258
Chap. 5
Diagonalization
(i)
V = M2√ó2(R) and T

a
b
c
d
	
=

c
d
a
b
	
(j)
V = M2√ó2(R) and T(A) = At + 2 ¬∑ tr(A) ¬∑ I2
5.
Prove Theorem 5.4.
6.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V, and
let Œ≤ be an ordered basis for V. Prove that Œª is an eigenvalue of T if
and only if Œª is an eigenvalue of [T]Œ≤.
7.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V. We
deÔ¨Åne the determinant of T, denoted det(T), as follows: Choose any
ordered basis Œ≤ for V, and deÔ¨Åne det(T) = det([T]Œ≤).
(a)
Prove that the preceding deÔ¨Ånition is independent of the choice
of an ordered basis for V. That is, prove that if Œ≤ and Œ≥ are two
ordered bases for V, then det([T]Œ≤) = det([T]Œ≥).
(b)
Prove that T is invertible if and only if det(T) Ã∏= 0.
(c)
Prove that if T is invertible, then det(T‚àí1) = [det(T)]‚àí1.
(d)
Prove that if U is also a linear operator on V, then det(TU) =
det(T)¬∑ det(U).
(e)
Prove that det(T ‚àíŒªIV) = det([T]Œ≤ ‚àíŒªI) for any scalar Œª and any
ordered basis Œ≤ for V.
8. (a)
Prove that a linear operator T on a Ô¨Ånite-dimensional vector space
is invertible if and only if zero is not an eigenvalue of T.
(b)
Let T be an invertible linear operator. Prove that a scalar Œª is an
eigenvalue of T if and only if Œª‚àí1 is an eigenvalue of T‚àí1.
(c)
State and prove results analogous to (a) and (b) for matrices.
9.
Prove that the eigenvalues of an upper triangular matrix M are the
diagonal entries of M.
10.
Let V be a Ô¨Ånite-dimensional vector space, and let Œª be any scalar.
(a)
For any ordered basis Œ≤ for V, prove that [ŒªIV]Œ≤ = ŒªI.
(b)
Compute the characteristic polynomial of ŒªIV.
(c)
Show that ŒªIV is diagonalizable and has only one eigenvalue.
11.
A scalar matrix is a square matrix of the form ŒªI for some scalar Œª;
that is, a scalar matrix is a diagonal matrix in which all the diagonal
entries are equal.
(a)
Prove that if a square matrix A is similar to a scalar matrix ŒªI,
then A = ŒªI.
(b)
Show that a diagonalizable matrix having only one eigenvalue is a
scalar matrix.

Sec. 5.1
Eigenvalues and Eigenvectors
259
(c)
Prove that

1
1
0
1
	
is not diagonalizable.
12. (a)
Prove that similar matrices have the same characteristic polyno-
mial.
(b)
Show that the deÔ¨Ånition of the characteristic polynomial of a linear
operator on a Ô¨Ånite-dimensional vector space V is independent of
the choice of basis for V.
13.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V over a
Ô¨Åeld F, let Œ≤ be an ordered basis for V, and let A = [T]Œ≤. In reference
to Figure 5.1, prove the following.
(a)
If v ‚ààV and œÜŒ≤(v) is an eigenvector of A corresponding to the
eigenvalue Œª, then v is an eigenvector of T corresponding to Œª.
(b)
If Œª is an eigenvalue of A (and hence of T), then a vector y ‚ààFn
is an eigenvector of A corresponding to Œª if and only if œÜ‚àí1
Œ≤ (y) is
an eigenvector of T corresponding to Œª.
14.‚Ä† For any square matrix A, prove that A and At have the same charac-
teristic polynomial (and hence the same eigenvalues).
15.‚Ä† (a)
Let T be a linear operator on a vector space V, and let x be an
eigenvector of T corresponding to the eigenvalue Œª. For any posi-
tive integer m, prove that x is an eigenvector of Tm corresponding
to the eigenvalue Œªm.
(b)
State and prove the analogous result for matrices.
16. (a)
Prove that similar matrices have the same trace. Hint: Use Exer-
cise 13 of Section 2.3.
(b)
How would you deÔ¨Åne the trace of a linear operator on a Ô¨Ånite-
dimensional vector space?
Justify that your deÔ¨Ånition is well-
deÔ¨Åned.
17.
Let T be the linear operator on Mn√ón(R) deÔ¨Åned by T(A) = At.
(a)
Show that ¬±1 are the only eigenvalues of T.
(b)
Describe the eigenvectors corresponding to each eigenvalue of T.
(c)
Find an ordered basis Œ≤ for M2√ó2(R) such that [T]Œ≤ is a diagonal
matrix.
(d)
Find an ordered basis Œ≤ for Mn√ón(R) such that [T]Œ≤ is a diagonal
matrix for n > 2.
18.
Let A, B ‚ààMn√ón(C).
(a)
Prove that if B is invertible, then there exists a scalar c ‚ààC such
that A + cB is not invertible. Hint: Examine det(A + cB).

260
Chap. 5
Diagonalization
(b)
Find nonzero 2√ó2 matrices A and B such that both A and A+cB
are invertible for all c ‚ààC.
19.‚Ä† Let A and B be similar n √ó n matrices. Prove that there exists an n-
dimensional vector space V, a linear operator T on V, and ordered bases
Œ≤ and Œ≥ for V such that A = [T]Œ≤ and B = [T]Œ≥. Hint: Use Exercise 14
of Section 2.5.
20.
Let A be an n √ó n matrix with characteristic polynomial
f(t) = (‚àí1)ntn + an‚àí1tn‚àí1 + ¬∑ ¬∑ ¬∑ + a1t + a0.
Prove that f(0) = a0 = det(A). Deduce that A is invertible if and only
if a0 Ã∏= 0.
21.
Let A and f(t) be as in Exercise 20.
(a)
Prove that f(t) = (A11 ‚àít)(A22 ‚àít) ¬∑ ¬∑ ¬∑ (Ann ‚àít)+q(t), where q(t)
is a polynomial of degree at most n‚àí2. Hint: Apply mathematical
induction to n.
(b)
Show that tr(A) = (‚àí1)n‚àí1an‚àí1.
22.‚Ä† (a)
Let T be a linear operator on a vector space V over the Ô¨Åeld F,
and let g(t) be a polynomial with coeÔ¨Écients from F. Prove that
if x is an eigenvector of T with corresponding eigenvalue Œª, then
g(T)(x) = g(Œª)x. That is, x is an eigenvector of g(T) with corre-
sponding eigenvalue g(Œª).
(b)
State and prove a comparable result for matrices.
(c)
Verify (b) for the matrix A in Exercise 3(a) with polynomial g(t) =
2t2 ‚àít + 1, eigenvector x =

2
3
	
, and corresponding eigenvalue
Œª = 4.
23.
Use Exercise 22 to prove that if f(t) is the characteristic polynomial
of a diagonalizable linear operator T, then f(T) = T0, the zero opera-
tor. (In Section 5.4 we prove that this result does not depend on the
diagonalizability of T.)
24.
Use Exercise 21(a) to prove Theorem 5.3.
25.
Prove Corollaries 1 and 2 of Theorem 5.3.
26.
Determine the number of distinct characteristic polynomials of matrices
in M2√ó2(Z2).

Sec. 5.2
Diagonalizability
261
5.2
DIAGONALIZABILITY
In Section 5.1, we presented the diagonalization problem and observed that
not all linear operators or matrices are diagonalizable. Although we are able
to diagonalize operators and matrices and even obtain a necessary and suf-
Ô¨Åcient condition for diagonalizability (Theorem 5.1 p. 246), we have not yet
solved the diagonalization problem. What is still needed is a simple test to
determine whether an operator or a matrix can be diagonalized, as well as a
method for actually Ô¨Ånding a basis of eigenvectors. In this section, we develop
such a test and method.
In Example 6 of Section 5.1, we obtained a basis of eigenvectors by choos-
ing one eigenvector corresponding to each eigenvalue.
In general, such a
procedure does not yield a basis, but the following theorem shows that any
set constructed in this manner is linearly independent.
Theorem 5.5. Let T be a linear operator on a vector space V, and let
Œª1, Œª2, . . . , Œªk be distinct eigenvalues of T. If v1, v2, . . . , vk are eigenvectors of
T such that Œªi corresponds to vi (1 ‚â§i ‚â§k), then {v1, v2, . . . , vk} is linearly
independent.
Proof. The proof is by mathematical induction on k. Suppose that k = 1.
Then v1 Ã∏= 0 since v1 is an eigenvector, and hence {v1} is linearly independent.
Now assume that the theorem holds for k ‚àí1 distinct eigenvalues, where
k ‚àí1 ‚â•1, and that we have k eigenvectors v1, v2, . . . , vk corresponding to the
distinct eigenvalues Œª1, Œª2, . . . , Œªk. We wish to show that {v1, v2, . . . , vk} is
linearly independent. Suppose that a1, a2, . . . , ak are scalars such that
a1v1 + a2v2 + ¬∑ ¬∑ ¬∑ + akvk = 0.
(1)
Applying T ‚àíŒªkI to both sides of (1), we obtain
a1(Œª1 ‚àíŒªk)v1 + a2(Œª2 ‚àíŒªk)v2 + ¬∑ ¬∑ ¬∑ + ak‚àí1(Œªk‚àí1 ‚àíŒªk)vk‚àí1 = 0.
By the induction hypothesis {v1, v2, . . . , vk‚àí1} is linearly independent, and
hence
a1(Œª1 ‚àíŒªk) = a2(Œª2 ‚àíŒªk) = ¬∑ ¬∑ ¬∑ = ak‚àí1(Œªk‚àí1 ‚àíŒªk) = 0.
Since Œª1, Œª2, . . . , Œªk are distinct, it follows that Œªi ‚àíŒªk Ã∏= 0 for 1 ‚â§i ‚â§k ‚àí1.
So a1 = a2 = ¬∑ ¬∑ ¬∑ = ak‚àí1 = 0, and (1) therefore reduces to akvk = 0. But
vk Ã∏= 0 and therefore ak = 0. Consequently a1 = a2 = ¬∑ ¬∑ ¬∑ = ak = 0, and it
follows that {v1, v2, . . . , vk} is linearly independent.
Corollary. Let T be a linear operator on an n-dimensional vector space
V. If T has n distinct eigenvalues, then T is diagonalizable.

262
Chap. 5
Diagonalization
Proof. Suppose that T has n distinct eigenvalues Œª1, . . . , Œªn. For each i
choose an eigenvector vi corresponding to Œªi. By Theorem 5.5, {v1, . . . , vn}
is linearly independent, and since dim(V) = n, this set is a basis for V. Thus,
by Theorem 5.1 (p. 246), T is diagonalizable.
Example 1
Let
A =

1
1
1
1
	
‚ààM2√ó2(R).
The characteristic polynomial of A (and hence of LA) is
det(A ‚àítI) = det

1 ‚àít
1
1
1 ‚àít
	
= t(t ‚àí2),
and thus the eigenvalues of LA are 0 and 2. Since LA is a linear operator on the
two-dimensional vector space R2, we conclude from the preceding corollary
that LA (and hence A) is diagonalizable.
‚ô¶
The converse of Theorem 5.5 is false. That is, it is not true that if T is
diagonalizable, then it has n distinct eigenvalues. For example, the identity
operator is diagonalizable even though it has only one eigenvalue, namely,
Œª = 1.
We have seen that diagonalizability requires the existence of eigenvalues.
Actually, diagonalizability imposes a stronger condition on the characteristic
polynomial.
DeÔ¨Ånition. A polynomial f(t) in P(F) splits over F if there are scalars
c, a1, . . . , an (not necessarily distinct) in F such that
f(t) = c(t ‚àía1)(t ‚àía2) ¬∑ ¬∑ ¬∑ (t ‚àían).
For example, t2 ‚àí1 = (t + 1)(t ‚àí1) splits over R, but (t2 + 1)(t ‚àí2) does not
split over R because t2 +1 cannot be factored into a product of linear factors.
However, (t2 + 1)(t ‚àí2) does split over C because it factors into the product
(t+i)(t‚àíi)(t‚àí2). If f(t) is the characteristic polynomial of a linear operator
or a matrix over a Ô¨Åeld F, then the statement that f(t) splits is understood
to mean that it splits over F.
Theorem 5.6. The characteristic polynomial of any diagonalizable linear
operator splits.
Proof. Let T be a diagonalizable linear operator on the n-dimensional
vector space V, and let Œ≤ be an ordered basis for V such that [T]Œ≤ = D is a

Sec. 5.2
Diagonalizability
263
diagonal matrix. Suppose that
D =
‚éõ
‚éú
‚éú
‚éú
‚éù
Œª1
0
¬∑ ¬∑ ¬∑
0
0
Œª2
¬∑ ¬∑ ¬∑
0
...
...
...
0
0
¬∑ ¬∑ ¬∑
Œªn
‚éû
‚éü
‚éü
‚éü
‚é†,
and let f(t) be the characteristic polynomial of T. Then
f(t) = det(D ‚àítI) = det
‚éõ
‚éú
‚éú
‚éú
‚éù
Œª1 ‚àít
0
¬∑ ¬∑ ¬∑
0
0
Œª2 ‚àít
¬∑ ¬∑ ¬∑
0
...
...
...
0
0
¬∑ ¬∑ ¬∑
Œªn ‚àít
‚éû
‚éü
‚éü
‚éü
‚é†
= (Œª1 ‚àít)(Œª2 ‚àít) ¬∑ ¬∑ ¬∑ (Œªn ‚àít) = (‚àí1)n(t ‚àíŒª1)(t ‚àíŒª2) ¬∑ ¬∑ ¬∑ (t ‚àíŒªn).
From this theorem, it is clear that if T is a diagonalizable linear operator
on an n-dimensional vector space that fails to have distinct eigenvalues, then
the characteristic polynomial of T must have repeated zeros.
The converse of Theorem 5.6 is false; that is, the characteristic polynomial
of T may split, but T need not be diagonalizable. (See Example 3, which
follows.) The following concept helps us determine when an operator whose
characteristic polynomial splits is diagonalizable.
DeÔ¨Ånition.
Let Œª be an eigenvalue of a linear operator or matrix with
characteristic polynomial f(t). The (algebraic) multiplicity of Œª is the
largest positive integer k for which (t ‚àíŒª)k is a factor of f(t).
Example 2
Let
A =
‚éõ
‚éù
3
1
0
0
3
4
0
0
4
‚éû
‚é†,
which has characteristic polynomial f(t) = ‚àí(t ‚àí3)2(t ‚àí4). Hence Œª = 3 is
an eigenvalue of A with multiplicity 2, and Œª = 4 is an eigenvalue of A with
multiplicity 1.
‚ô¶
If T is a diagonalizable linear operator on a Ô¨Ånite-dimensional vector space
V, then there is an ordered basis Œ≤ for V consisting of eigenvectors of T. We
know from Theorem 5.1 (p. 246) that [T]Œ≤ is a diagonal matrix in which the
diagonal entries are the eigenvalues of T. Since the characteristic polynomial
of T is det([T]Œ≤ ‚àítI), it is easily seen that each eigenvalue of T must occur
as a diagonal entry of [T]Œ≤ exactly as many times as its multiplicity. Hence

264
Chap. 5
Diagonalization
Œ≤ contains as many (linearly independent) eigenvectors corresponding to an
eigenvalue as the multiplicity of that eigenvalue. So the number of linearly
independent eigenvectors corresponding to a given eigenvalue is of interest in
determining whether an operator can be diagonalized. Recalling from Theo-
rem 5.4 (p. 250) that the eigenvectors of T corresponding to the eigenvalue
Œª are the nonzero vectors in the null space of T ‚àíŒªI, we are led naturally to
the study of this set.
DeÔ¨Ånition.
Let T be a linear operator on a vector space V, and let
Œª be an eigenvalue of T. DeÔ¨Åne EŒª = {x ‚ààV: T(x) = Œªx} = N(T ‚àíŒªIV).
The set EŒª is called the eigenspace of T corresponding to the eigenvalue
Œª. Analogously, we deÔ¨Åne the eigenspace of a square matrix A to be the
eigenspace of LA.
Clearly, EŒª is a subspace of V consisting of the zero vector and the eigen-
vectors of T corresponding to the eigenvalue Œª. The maximum number of
linearly independent eigenvectors of T corresponding to the eigenvalue Œª is
therefore the dimension of EŒª. Our next result relates this dimension to the
multiplicity of Œª.
Theorem 5.7. Let T be a linear operator on a Ô¨Ånite-dimensional vec-
tor space V, and let Œª be an eigenvalue of T having multiplicity m. Then
1 ‚â§dim(EŒª) ‚â§m.
Proof. Choose an ordered basis {v1, v2, . . . , vp} for EŒª, extend it to an or-
dered basis Œ≤ = {v1, v2, . . . , vp, vp+1, . . . , vn} for V, and let A = [T]Œ≤. Observe
that vi (1 ‚â§i ‚â§p) is an eigenvector of T corresponding to Œª, and therefore
A =

ŒªIp
B
O
C
	
.
By Exercise 21 of Section 4.3, the characteristic polynomial of T is
f(t) = det(A ‚àítIn) = det

(Œª ‚àít)Ip
B
O
C ‚àítIn‚àíp
	
= det((Œª ‚àít)Ip) det(C ‚àítIn‚àíp)
= (Œª ‚àít)pg(t),
where g(t) is a polynomial. Thus (Œª ‚àít)p is a factor of f(t), and hence the
multiplicity of Œª is at least p. But dim(EŒª) = p, and so dim(EŒª) ‚â§m.
Example 3
Let T be the linear operator on P2(R) deÔ¨Åned by T(f(x)) = f ‚Ä≤(x).
The
matrix representation of T with respect to the standard ordered basis Œ≤ for

Sec. 5.2
Diagonalizability
265
P2(R) is
[T]Œ≤ =
‚éõ
‚éù
0
1
0
0
0
2
0
0
0
‚éû
‚é†.
Consequently, the characteristic polynomial of T is
det([T]Œ≤ ‚àítI) = det
‚éõ
‚éù
‚àít
1
0
0
‚àít
2
0
0
‚àít
‚éû
‚é†= ‚àít3.
Thus T has only one eigenvalue (Œª = 0) with multiplicity 3. Solving T(f(x)) =
f ‚Ä≤(x) = 0 shows that EŒª = N(T ‚àíŒªI) = N(T) is the subspace of P2(R) con-
sisting of the constant polynomials. So {1} is a basis for EŒª, and therefore
dim(EŒª) = 1. Consequently, there is no basis for P2(R) consisting of eigen-
vectors of T, and therefore T is not diagonalizable.
‚ô¶
Example 4
Let T be the linear operator on R3 deÔ¨Åned by
T
‚éõ
‚éù
a1
a2
a3
‚éû
‚é†=
‚éõ
‚éù
4a1
+ a3
2a1 + 3a2 + 2a3
a1
+ 4a3
‚éû
‚é†.
We determine the eigenspace of T corresponding to each eigenvalue. Let Œ≤
be the standard ordered basis for R3. Then
[T]Œ≤ =
‚éõ
‚éù
4
0
1
2
3
2
1
0
4
‚éû
‚é†,
and hence the characteristic polynomial of T is
det([T]Œ≤ ‚àítI) = det
‚éõ
‚éù
4 ‚àít
0
1
2
3 ‚àít
2
1
0
4 ‚àít
‚éû
‚é†= ‚àí(t ‚àí5)(t ‚àí3)2.
So the eigenvalues of T are Œª1 = 5 and Œª2 = 3 with multiplicities 1 and 2,
respectively.
Since
EŒª1 = N(T ‚àíŒª1I) =
‚éß
‚é®
‚é©
‚éõ
‚éù
x1
x2
x3
‚éû
‚é†‚ààR3 :
‚éõ
‚éù
‚àí1
0
1
2
‚àí2
2
1
0
‚àí1
‚éû
‚é†
‚éõ
‚éù
x1
x2
x3
‚éû
‚é†=
‚éõ
‚éù
0
0
0
‚éû
‚é†
‚é´
‚é¨
‚é≠,

266
Chap. 5
Diagonalization
EŒª1 is the solution space of the system of linear equations
‚àíx1
+ x3 = 0
2x1 ‚àí2x2 + 2x3 = 0
x1
‚àíx3 = 0.
It is easily seen (using the techniques of Chapter 3) that
‚éß
‚é®
‚é©
‚éõ
‚éù
1
2
1
‚éû
‚é†
‚é´
‚é¨
‚é≠
is a basis for EŒª1. Hence dim(EŒª1) = 1.
Similarly, EŒª2 = N(T ‚àíŒª2I) is the solution space of the system
x1 + x3 = 0
2x1 + 2x3 = 0
x1 + x3 = 0.
Since the unknown x2 does not appear in this system, we assign it a para-
metric value, say, x2 = s, and solve the system for x1 and x3, introducing
another parameter t. The result is the general solution to the system
‚éõ
‚éù
x1
x2
x3
‚éû
‚é†= s
‚éõ
‚éù
0
1
0
‚éû
‚é†+ t
‚éõ
‚éù
‚àí1
0
1
‚éû
‚é†, for s, t ‚ààR.
It follows that
‚éß
‚é®
‚é©
‚éõ
‚éù
0
1
0
‚éû
‚é†,
‚éõ
‚éù
‚àí1
0
1
‚éû
‚é†
‚é´
‚é¨
‚é≠
is a basis for EŒª2, and dim(EŒª2) = 2.
In this case, the multiplicity of each eigenvalue Œªi is equal to the dimension
of the corresponding eigenspace EŒªi. Observe that the union of the two bases
just derived, namely,
‚éß
‚é®
‚é©
‚éõ
‚éù
1
2
1
‚éû
‚é†,
‚éõ
‚éù
0
1
0
‚éû
‚é†,
‚éõ
‚éù
‚àí1
0
1
‚éû
‚é†
‚é´
‚é¨
‚é≠,
is linearly independent and hence is a basis for R3 consisting of eigenvectors
of T. Consequently, T is diagonalizable.
‚ô¶

Sec. 5.2
Diagonalizability
267
Examples 3 and 4 suggest that an operator whose characteristic polyno-
mial splits is diagonalizable if and only if the dimension of each eigenspace
is equal to the multiplicity of the corresponding eigenvalue. This is indeed
true, as we now show. We begin with the following lemma, which is a slight
variation of Theorem 5.5.
Lemma. Let T be a linear operator, and let Œª1, Œª2, . . . , Œªk be distinct
eigenvalues of T. For each i = 1, 2, . . . , k, let vi ‚ààEŒªi, the eigenspace corre-
sponding to Œªi. If
v1 + v2 + ¬∑ ¬∑ ¬∑ + vk = 0,
then vi = 0 for all i.
Proof. Suppose otherwise. By renumbering if necessary, suppose that, for
1 ‚â§m ‚â§k, we have vi Ã∏= 0 for 1 ‚â§i ‚â§m, and vi = 0 for i > m. Then, for
each i ‚â§m, vi is an eigenvector of T corresponding to Œªi and
v1 + v2 + ¬∑ ¬∑ ¬∑ + vm = 0.
But this contradicts Theorem 5.5, which states that these vi‚Äôs are linearly
independent. We conclude, therefore, that vi = 0 for all i.
Theorem 5.8. Let T be a linear operator on a vector space V, and let
Œª1, Œª2, . . . , Œªk be distinct eigenvalues of T. For each i = 1, 2, . . . , k, let Si
be a Ô¨Ånite linearly independent subset of the eigenspace EŒªi.
Then S =
S1 ‚à™S2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Sk is a linearly independent subset of V.
Proof. Suppose that for each i
Si = {vi1, vi2, . . . , vini}.
Then S = {vij : 1 ‚â§j ‚â§ni, and 1 ‚â§i ‚â§k}. Consider any scalars {aij} such
that
k

i=1
ni

j=1
aijvij = 0.
For each i, let
wi =
ni

j=1
aijvij.
Then wi ‚ààEŒªi for each i, and w1 + ¬∑ ¬∑ ¬∑ + wk = 0. Therefore, by the lemma,
wi = 0 for all i. But each Si is linearly independent, and hence aij = 0 for
all j. We conclude that S is linearly independent.

268
Chap. 5
Diagonalization
Theorem 5.8 tells us how to construct a linearly independent subset of
eigenvectors, namely, by collecting bases for the individual eigenspaces. The
next theorem tells us when the resulting set is a basis for the entire space.
Theorem 5.9. Let T be a linear operator on a Ô¨Ånite-dimensional vector
space V such that the characteristic polynomial of T splits. Let Œª1, Œª2, . . . , Œªk
be the distinct eigenvalues of T. Then
(a) T is diagonalizable if and only if the multiplicity of Œªi is equal to
dim(EŒªi) for all i.
(b) If T is diagonalizable and Œ≤i is an ordered basis for EŒªi for each i, then
Œ≤ = Œ≤1‚à™Œ≤2‚à™¬∑ ¬∑ ¬∑‚à™Œ≤k is an ordered basis2 for V consisting of eigenvectors
of T.
Proof. For each i, let mi denote the multiplicity of Œªi, di = dim(EŒªi), and
n = dim(V).
First, suppose that T is diagonalizable. Let Œ≤ be a basis for V consisting
of eigenvectors of T. For each i, let Œ≤i = Œ≤ ‚à©EŒªi, the set of vectors in Œ≤ that
are eigenvectors corresponding to Œªi, and let ni denote the number of vectors
in Œ≤i. Then ni ‚â§di for each i because Œ≤i is a linearly independent subset of
a subspace of dimension di, and di ‚â§mi by Theorem 5.7. The ni‚Äôs sum to n
because Œ≤ contains n vectors. The mi‚Äôs also sum to n because the degree of
the characteristic polynomial of T is equal to the sum of the multiplicities of
the eigenvalues. Thus
n =
k

i=1
ni ‚â§
k

i=1
di ‚â§
k

i=1
mi = n.
It follows that
k

i=1
(mi ‚àídi) = 0.
Since (mi ‚àídi) ‚â•0 for all i, we conclude that mi = di for all i.
Conversely, suppose that mi = di for all i. We simultaneously show that
T is diagonalizable and prove (b). For each i, let Œ≤i be an ordered basis for
EŒªi, and let Œ≤ = Œ≤1 ‚à™Œ≤2 ‚à™¬∑ ¬∑ ¬∑‚à™Œ≤k. By Theorem 5.8, Œ≤ is linearly independent.
Furthermore, since di = mi for all i, Œ≤ contains
k

i=1
di =
k

i=1
mi = n
2We regard Œ≤1 ‚à™Œ≤2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≤k as an ordered basis in the natural way‚Äîthe vectors
in Œ≤1 are listed Ô¨Årst (in the same order as in Œ≤1), then the vectors in Œ≤2 (in the same
order as in Œ≤2), etc.

Sec. 5.2
Diagonalizability
269
vectors. Therefore Œ≤ is an ordered basis for V consisting of eigenvectors of V,
and we conclude that T is diagonalizable.
This theorem completes our study of the diagonalization problem. We
summarize our results.
Test for Diagonalization
Let T be a linear operator on an n-dimensional vector space V. Then T
is diagonalizable if and only if both of the following conditions hold.
1. The characteristic polynomial of T splits.
2. For each eigenvalue Œª of T, the multiplicity of Œª equals n‚àírank(T‚àíŒªI).
These same conditions can be used to test if a square matrix A is diagonal-
izable because diagonalizability of A is equivalent to diagonalizability of the
operator LA.
If T is a diagonalizable operator and Œ≤1, Œ≤2, . . . , Œ≤k are ordered bases for
the eigenspaces of T, then the union Œ≤ = Œ≤1 ‚à™Œ≤2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≤k is an ordered basis
for V consisting of eigenvectors of T, and hence [T]Œ≤ is a diagonal matrix.
When testing T for diagonalizability, it is usually easiest to choose a conve-
nient basis Œ± for V and work with B = [T]Œ±. If the characteristic polynomial
of B splits, then use condition 2 above to check if the multiplicity of each
repeated eigenvalue of B equals n ‚àírank(B ‚àíŒªI). (By Theorem 5.7, condi-
tion 2 is automatically satisÔ¨Åed for eigenvalues of multiplicity 1.) If so, then
B, and hence T, is diagonalizable.
If T is diagonalizable and a basis Œ≤ for V consisting of eigenvectors of T
is desired, then we Ô¨Årst Ô¨Ånd a basis for each eigenspace of B. The union of
these bases is a basis Œ≥ for Fn consisting of eigenvectors of B. Each vector
in Œ≥ is the coordinate vector relative to Œ± of an eigenvector of T. The set
consisting of these n eigenvectors of T is the desired basis Œ≤.
Furthermore, if A is an n √ó n diagonalizable matrix, we can use the corol-
lary to Theorem 2.23 (p. 115) to Ô¨Ånd an invertible n √ó n matrix Q and a
diagonal n √ó n matrix D such that Q‚àí1AQ = D. The matrix Q has as its
columns the vectors in a basis of eigenvectors of A, and D has as its jth
diagonal entry the eigenvalue of A corresponding to the jth column of Q.
We now consider some examples illustrating the preceding ideas.
Example 5
We test the matrix
A =
‚éõ
‚éù
3
1
0
0
3
0
0
0
4
‚éû
‚é†‚ààM3√ó3(R)
for diagonalizability.

270
Chap. 5
Diagonalization
The characteristic polynomial of A is det(A‚àítI) = ‚àí(t‚àí4)(t‚àí3)2, which
splits, and so condition 1 of the test for diagonalization is satisÔ¨Åed. Also A
has eigenvalues Œª1 = 4 and Œª2 = 3 with multiplicities 1 and 2, respectively.
Since Œª1 has multiplicity 1, condition 2 is satisÔ¨Åed for Œª1. Thus we need only
test condition 2 for Œª2. Because
A ‚àíŒª2I =
‚éõ
‚éù
0
1
0
0
0
0
0
0
1
‚éû
‚é†
has rank 2, we see that 3 ‚àírank(A ‚àíŒª2I) = 1, which is not the multiplicity
of Œª2. Thus condition 2 fails for Œª2, and A is therefore not diagonalizable.
‚ô¶
Example 6
Let T be the linear operator on P2(R) deÔ¨Åned by
T(f(x)) = f(1) + f ‚Ä≤(0)x + (f ‚Ä≤(0) + f ‚Ä≤‚Ä≤(0))x2.
We Ô¨Årst test T for diagonalizability. Let Œ± denote the standard ordered basis
for P2(R) and B = [T]Œ±. Then
B =
‚éõ
‚éù
1
1
1
0
1
0
0
1
2
‚éû
‚é†.
The characteristic polynomial of B, and hence of T, is ‚àí(t‚àí1)2(t‚àí2), which
splits. Hence condition 1 of the test for diagonalization is satisÔ¨Åed. Also B
has the eigenvalues Œª1 = 1 and Œª2 = 2 with multiplicities 2 and 1, respectively.
Condition 2 is satisÔ¨Åed for Œª2 because it has multiplicity 1. So we need only
verify condition 2 for Œª1 = 1. For this case,
3 ‚àírank(B ‚àíŒª1I) = 3 ‚àírank
‚éõ
‚éù
0
1
1
0
0
0
0
1
1
‚éû
‚é†= 3 ‚àí1 = 2,
which is equal to the multiplicity of Œª1. Therefore T is diagonalizable.
We now Ô¨Ånd an ordered basis Œ≥ for R3 of eigenvectors of B. We consider
each eigenvalue separately.
The eigenspace corresponding to Œª1 = 1 is
EŒª1 =
‚éß
‚é®
‚é©
‚éõ
‚éù
x1
x2
x3
‚éû
‚é†‚ààR3 :
‚éõ
‚éù
0
1
1
0
0
0
0
1
1
‚éû
‚é†
‚éõ
‚éù
x1
x2
x3
‚éû
‚é†= 0
‚é´
‚é¨
‚é≠,

Sec. 5.2
Diagonalizability
271
which is the solution space for the system
x2 + x3 = 0,
and has
Œ≥1 =
‚éß
‚é®
‚é©
‚éõ
‚éù
1
0
0
‚éû
‚é†,
‚éõ
‚éù
0
‚àí1
1
‚éû
‚é†
‚é´
‚é¨
‚é≠
as a basis.
The eigenspace corresponding to Œª2 = 2 is
EŒª2 =
‚éß
‚é®
‚é©
‚éõ
‚éù
x1
x2
x3
‚éû
‚é†‚ààR3 :
‚éõ
‚éù
‚àí1
1
1
0
‚àí1
0
0
1
0
‚éû
‚é†
‚éõ
‚éù
x1
x2
x3
‚éû
‚é†= 0
‚é´
‚é¨
‚é≠,
which is the solution space for the system
‚àíx1 + x2 + x3 = 0
x2
= 0,
and has
Œ≥2 =
‚éß
‚é®
‚é©
‚éõ
‚éù
1
0
1
‚éû
‚é†
‚é´
‚é¨
‚é≠
as a basis.
Let
Œ≥ = Œ≥1 ‚à™Œ≥2 =
‚éß
‚é®
‚é©
‚éõ
‚éù
1
0
0
‚éû
‚é†,
‚éõ
‚éù
0
‚àí1
1
‚éû
‚é†,
‚éõ
‚éù
1
0
1
‚éû
‚é†
‚é´
‚é¨
‚é≠.
Then Œ≥ is an ordered basis for R3 consisting of eigenvectors of B.
Finally, observe that the vectors in Œ≥ are the coordinate vectors relative
to Œ± of the vectors in the set
Œ≤ = {1, ‚àíx + x2, 1 + x2},
which is an ordered basis for P2(R) consisting of eigenvectors of T. Thus
[T]Œ≤ =
‚éõ
‚éù
1
0
0
0
1
0
0
0
2
‚éû
‚é†.
‚ô¶

272
Chap. 5
Diagonalization
Our next example is an application of diagonalization that is of interest
in Section 5.3.
Example 7
Let
A =

0
‚àí2
1
3
	
.
We show that A is diagonalizable and Ô¨Ånd a 2√ó2 matrix Q such that Q‚àí1AQ
is a diagonal matrix. We then show how to use this result to compute An for
any positive integer n.
First observe that the characteristic polynomial of A is (t ‚àí1)(t ‚àí2), and
hence A has two distinct eigenvalues, Œª1 = 1 and Œª2 = 2. By applying the
corollary to Theorem 5.5 to the operator LA, we see that A is diagonalizable.
Moreover,
Œ≥1 =

‚àí2
1
	
and
Œ≥2 =

‚àí1
1
	
are bases for the eigenspaces EŒª1 and EŒª2, respectively. Therefore
Œ≥ = Œ≥1 ‚à™Œ≥2 =

‚àí2
1
	
,

‚àí1
1
	
is an ordered basis for R2 consisting of eigenvectors of R2. Let
Q =

‚àí2
‚àí1
1
1
	
,
the matrix whose columns are the vectors in Œ≥. Then, by the corollary to
Theorem 2.23 (p. 115),
D = Q‚àí1AQ = [LA]Œ≤ =

1
0
0
2
	
.
To Ô¨Ånd An for any positive integer n, observe that A = QDQ‚àí1. Therefore
An = (QDQ‚àí1)n
= (QDQ‚àí1)(QDQ‚àí1) ¬∑ ¬∑ ¬∑ (QDQ‚àí1)
= QDnQ‚àí1
= Q

1n
0
0
2n
	
Q‚àí1
=

‚àí2
‚àí1
1
1
	 
1
0
0
2n
	 
‚àí1
‚àí1
1
2
	
=

2 ‚àí2n
2 ‚àí2n+1
‚àí1 + 2n
‚àí1 + 2n+1
	
.
‚ô¶

Sec. 5.2
Diagonalizability
273
We now consider an application that uses diagonalization to solve a system
of diÔ¨Äerential equations.
Systems of DiÔ¨Äerential Equations
Consider the system of diÔ¨Äerential equations
x‚Ä≤
1 = 3x1 + x2 + x3
x‚Ä≤
2 = 2x1 + 4x2 + 2x3
x‚Ä≤
3 = ‚àíx1 ‚àíx2 + x3,
where, for each i, xi = xi(t) is a diÔ¨Äerentiable real-valued function of the
real variable t. Clearly, this system has a solution, namely, the solution in
which each xi(t) is the zero function. We determine all of the solutions to
this system.
Let x: R ‚ÜíR3 be the function deÔ¨Åned by
x(t) =
‚éõ
‚éú
‚éù
x1(t)
x2(t)
x3(t)
‚éû
‚éü
‚é†.
The derivative of x, denoted x‚Ä≤, is deÔ¨Åned by
x‚Ä≤(t) =
‚éõ
‚éú
‚éù
x‚Ä≤
1(t)
x‚Ä≤
2(t)
x‚Ä≤
3(t)
‚éû
‚éü
‚é†.
Let
A =
‚éõ
‚éù
3
1
1
2
4
2
‚àí1
‚àí1
1
‚éû
‚é†
be the coeÔ¨Écient matrix of the given system, so that we can rewrite the
system as the matrix equation x‚Ä≤ = Ax.
It can be veriÔ¨Åed that for
Q =
‚éõ
‚éù
‚àí1
0
‚àí1
0
‚àí1
‚àí2
1
1
1
‚éû
‚é†
and
D =
‚éõ
‚éù
2
0
0
0
2
0
0
0
4
‚éû
‚é†,
we have Q‚àí1AQ = D.
Substitute A = QDQ‚àí1 into x‚Ä≤ = Ax to obtain
x‚Ä≤ = QDQ‚àí1x or, equivalently, Q‚àí1x‚Ä≤ = DQ‚àí1x. The function y: R ‚ÜíR3
deÔ¨Åned by y(t) = Q‚àí1x(t) can be shown to be diÔ¨Äerentiable, and y‚Ä≤ = Q‚àí1x‚Ä≤
(see Exercise 16). Hence the original system can be written as y‚Ä≤ = Dy.

274
Chap. 5
Diagonalization
Since D is a diagonal matrix, the system y‚Ä≤ = Dy is easy to solve. Setting
y(t) =
‚éõ
‚éú
‚éù
y1(t)
y2(t)
y3(t)
‚éû
‚éü
‚é†,
we can rewrite y‚Ä≤ = Dy as
‚éõ
‚éú
‚éù
y‚Ä≤
1(t)
y‚Ä≤
2(t)
y‚Ä≤
3(t)
‚éû
‚éü
‚é†=
‚éõ
‚éù
2
0
0
0
2
0
0
0
4
‚éû
‚é†
‚éõ
‚éú
‚éù
y1(t)
y2(t)
y3(t)
‚éû
‚éü
‚é†=
‚éõ
‚éú
‚éù
2y1(t)
2y2(t)
4y3(t)
‚éû
‚éü
‚é†.
The three equations
y‚Ä≤
1 = 2y1
y‚Ä≤
2 = 2y2
y‚Ä≤
3 = 4y3
are independent of each other, and thus can be solved individually.
It is
easily seen (as in Example 3 of Section 5.1) that the general solution to these
equations is y1(t) = c1e2t, y2(t) = c2e2t, and y3(t) = c3e4t, where c1, c2, and
c3 are arbitrary constants. Finally,
‚éõ
‚éú
‚éù
x1(t)
x2(t)
x3(t)
‚éû
‚éü
‚é†= x(t) = Qy(t) =
‚éõ
‚éù
‚àí1
0
‚àí1
0
‚àí1
‚àí2
1
1
1
‚éû
‚é†
‚éõ
‚éú
‚éù
c1e2t
c2e2t
c3e4t
‚éû
‚éü
‚é†
=
‚éõ
‚éù
‚àíc1e2t
‚àíc3e4t
‚àíc2e2t ‚àí2c3e4t
c1e2t + c2e2t + c3e4t
‚éû
‚é†
yields the general solution of the original system. Note that this solution can
be written as
x(t) = e2t
‚é°
‚é£c1
‚éõ
‚éù
‚àí1
0
1
‚éû
‚é†+ c2
‚éõ
‚éù
0
‚àí1
1
‚éû
‚é†
‚é§
‚é¶+ e4t
‚é°
‚é£c3
‚éõ
‚éù
‚àí1
‚àí2
1
‚éû
‚é†
‚é§
‚é¶.
The expressions in brackets are arbitrary vectors in EŒª1 and EŒª2, respectively,
where Œª1 = 2 and Œª2 = 4. Thus the general solution of the original system is
x(t) = e2tz1 + e4tz2, where z1 ‚ààEŒª1 and z2 ‚ààEŒª2. This result is generalized
in Exercise 15.
Direct Sums*
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V. There
is a way of decomposing V into simpler subspaces that oÔ¨Äers insight into the

Sec. 5.2
Diagonalizability
275
behavior of T. This approach is especially useful in Chapter 7, where we study
nondiagonalizable linear operators. In the case of diagonalizable operators,
the simpler subspaces are the eigenspaces of the operator.
DeÔ¨Ånition.
Let W1, W2, . . . , Wk be subspaces of a vector space V. We
deÔ¨Åne the sum of these subspaces to be the set
{v1 + v2 + ¬∑ ¬∑ ¬∑ + vk : vi ‚ààWi for 1 ‚â§i ‚â§k},
which we denote by W1 + W2 + ¬∑ ¬∑ ¬∑ + Wk or
k

i=1
Wi.
It is a simple exercise to show that the sum of subspaces of a vector space
is also a subspace.
Example 8
Let V = R3, let W1 denote the xy-plane, and let W2 denote the yz-plane.
Then R3 = W1 + W2 because, for any vector (a, b, c) ‚ààR3, we have
(a, b, c) = (a, 0, 0) + (0, b, c),
where (a, 0, 0) ‚ààW1 and (0, b, c) ‚ààW2.
‚ô¶
Notice that in Example 8 the representation of (a, b, c) as a sum of vectors
in W1 and W2 is not unique. For example, (a, b, c) = (a, b, 0) + (0, 0, c) is
another representation. Because we are often interested in sums for which
representations are unique, we introduce a condition that assures this out-
come.
The deÔ¨Ånition of direct sum that follows is a generalization of the
deÔ¨Ånition given in the exercises of Section 1.3.
DeÔ¨Ånition.
Let W1, W2, . . . , Wk be subspaces of a vector space V. We
call V the direct sum of the subspaces W1, W2, . . . , Wk and write V =
W1 ‚äïW2 ‚äï¬∑ ¬∑ ¬∑ ‚äïWk, if
V =
k

i=1
Wi
and
Wj ‚à©

iÃ∏=j
Wi = {0}
for each j (1 ‚â§j ‚â§k).
Example 9
Let V = R4, W1 = {(a, b, 0, 0): a, b, ‚ààR}, W2 = {(0, 0, c, 0): c ‚ààR}, and
W3 = {(0, 0, 0, d): d ‚ààR}. For any (a, b, c, d) ‚ààV,
(a, b, c, d) = (a, b, 0, 0) + (0, 0, c, 0) + (0, 0, 0, d) ‚ààW1 + W2 + W3.

276
Chap. 5
Diagonalization
Thus
V =
3

i=1
Wi.
To show that V is the direct sum of W1, W2, and W3, we must prove that
W1 ‚à©(W2 + W3) = W2 ‚à©(W1 + W3) = W3 ‚à©(W1 + W2) = {0}. But these
equalities are obvious, and so V = W1 ‚äïW2 ‚äïW3.
‚ô¶
Our next result contains several conditions that are equivalent to the
deÔ¨Ånition of a direct sum.
Theorem 5.10. Let W1, W2, . . . , Wk be subspaces of a Ô¨Ånite-dimensional
vector space V. The following conditions are equivalent.
(a) V = W1 ‚äïW2 ‚äï¬∑ ¬∑ ¬∑ ‚äïWk.
(b) V =
k

i=1
Wi and, for any vectors v1, v2, . . . , vk such that vi ‚ààWi
(1 ‚â§i ‚â§k), if v1 + v2 + ¬∑ ¬∑ ¬∑ + vk = 0, then vi = 0 for all i.
(c) Each vector v ‚ààV can be uniquely written as v = v1 + v2 + ¬∑ ¬∑ ¬∑ + vk,
where vi ‚ààWi.
(d) If Œ≥i is an ordered basis for Wi (1 ‚â§i ‚â§k), then Œ≥1 ‚à™Œ≥2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≥k is an
ordered basis for V.
(e) For each i = 1, 2, . . . , k, there exists an ordered basis Œ≥i for Wi such
that Œ≥1 ‚à™Œ≥2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≥k is an ordered basis for V.
Proof. Assume (a). We prove (b). Clearly
V =
k

i=1
Wi.
Now suppose that v1, v2, . . . , vk are vectors such that vi ‚ààWi for all i and
v1 + v2 + ¬∑ ¬∑ ¬∑ + vk = 0. Then for any j
‚àívj =

iÃ∏=j
vi ‚àà

iÃ∏=j
Wi.
But ‚àívj ‚ààWj and hence
‚àívj ‚ààWj ‚à©

iÃ∏=j
Wi = {0}.
So vj = 0, proving (b).
Now assume (b). We prove (c). Let v ‚ààV. By (b), there exist vectors
v1, v2, . . . , vk such that vi ‚ààWi and v = v1 + v2 + ¬∑ ¬∑ ¬∑ + vk. We must show

Sec. 5.2
Diagonalizability
277
that this representation is unique. Suppose also that v = w1 + w2 + ¬∑ ¬∑ ¬∑ + wk,
where wi ‚ààWi for all i. Then
(v1 ‚àíw1) + (v2 ‚àíw2) + ¬∑ ¬∑ ¬∑ + (vk ‚àíwk) = 0.
But vi ‚àíwi ‚ààWi for all i, and therefore vi ‚àíwi = 0 for all i by (b). Thus
vi = wi for all i, proving the uniqueness of the representation.
Now assume (c). We prove (d). For each i, let Œ≥i be an ordered basis for
Wi. Since
V =
k

i=1
Wi
by (c), it follows that Œ≥1 ‚à™Œ≥2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≥k generates V.
To show that this
set is linearly independent, consider vectors vij ‚ààŒ≥i (j = 1, 2, . . . , mi and
i = 1, 2, . . . , k) and scalars aij such that

i,j
aijvij = 0.
For each i, set
wi =
mi

j=1
aijvij.
Then for each i, wi ‚ààspan(Œ≥i) = Wi and
w1 + w2 + ¬∑ ¬∑ ¬∑ + wk =

i,j
aijvij = 0.
Since 0 ‚ààWi for each i and 0 + 0 + ¬∑ ¬∑ ¬∑ + 0 = w1 + w2 + ¬∑ ¬∑ ¬∑ + wk, (c) implies
that wi = 0 for all i. Thus
0 = wi =
mi

j=1
aijvij
for each i. But each Œ≥i is linearly independent, and hence aij = 0 for all i
and j. Consequently Œ≥1 ‚à™Œ≥2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≥k is linearly independent and therefore
is a basis for V.
Clearly (e) follows immediately from (d).
Finally, we assume (e) and prove (a). For each i, let Œ≥i be an ordered
basis for Wi such that Œ≥1 ‚à™Œ≥2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≥k is an ordered basis for V. Then
V = span(Œ≥1 ‚à™Œ≥2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≥k)

278
Chap. 5
Diagonalization
= span(Œ≥1) + span(Œ≥2) + ¬∑ ¬∑ ¬∑ + span(Œ≥k) =
k

i=1
Wi
by repeated applications of Exercise 14 of Section 1.4. Fix j (1 ‚â§j ‚â§k), and
suppose that, for some nonzero vector v ‚ààV,
v ‚ààWj ‚à©

iÃ∏=j
Wi.
Then
v ‚ààWj = span(Œ≥j)
and
v ‚àà

iÃ∏=j
Wi = span
‚éõ
‚éù-
iÃ∏=j
Œ≥i
‚éû
‚é†.
Hence v is a nontrivial linear combination of both Œ≥j and
‚éõ
‚éù-
iÃ∏=j
Œ≥i
‚éû
‚é†, so that
v can be expressed as a linear combination of Œ≥1 ‚à™Œ≥2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≥k in more than
one way. But these representations contradict Theorem 1.8 (p. 43), and so
we conclude that
Wj ‚à©

iÃ∏=j
Wi = {0},
proving (a).
With the aid of Theorem 5.10, we are able to characterize diagonalizability
in terms of direct sums.
Theorem 5.11. A linear operator T on a Ô¨Ånite-dimensional vector space
V is diagonalizable if and only if V is the direct sum of the eigenspaces of T.
Proof. Let Œª1, Œª2, . . . , Œªk be the distinct eigenvalues of T.
First suppose that T is diagonalizable, and for each i choose an ordered
basis Œ≥i for the eigenspace EŒªi. By Theorem 5.9, Œ≥1 ‚à™Œ≥2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≥k is a basis
for V, and hence V is a direct sum of the EŒªi‚Äôs by Theorem 5.10.
Conversely, suppose that V is a direct sum of the eigenspaces of T. For
each i, choose an ordered basis Œ≥i of EŒªi.
By Theorem 5.10, the union
Œ≥1 ‚à™Œ≥2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≥k is a basis for V. Since this basis consists of eigenvectors of
T, we conclude that T is diagonalizable.
Example 10
Let T be the linear operator on R4 deÔ¨Åned by
T(a, b, c, d) = (a, b, 2c, 3d).

Sec. 5.2
Diagonalizability
279
It is easily seen that T is diagonalizable with eigenvalues Œª1 = 1, Œª2 = 2,
and Œª3 = 3. Furthermore, the corresponding eigenspaces coincide with the
subspaces W1, W2, and W3 of Example 9. Thus Theorem 5.11 provides us
with another proof that R4 = W1 ‚äïW2 ‚äïW3.
‚ô¶
EXERCISES
1.
Label the following statements as true or false.
(a)
Any linear operator on an n-dimensional vector space that has
fewer than n distinct eigenvalues is not diagonalizable.
(b)
Two distinct eigenvectors corresponding to the same eigenvalue
are always linearly dependent.
(c)
If Œª is an eigenvalue of a linear operator T, then each vector in EŒª
is an eigenvector of T.
(d)
If Œª1 and Œª2 are distinct eigenvalues of a linear operator T, then
EŒª1 ‚à©EŒª2 = {0}.
(e)
Let A ‚ààMn√ón(F) and Œ≤ = {v1, v2, . . . , vn} be an ordered basis for
Fn consisting of eigenvectors of A. If Q is the n √ó n matrix whose
jth column is vj (1 ‚â§j ‚â§n), then Q‚àí1AQ is a diagonal matrix.
(f)
A linear operator T on a Ô¨Ånite-dimensional vector space is diago-
nalizable if and only if the multiplicity of each eigenvalue Œª equals
the dimension of EŒª.
(g)
Every diagonalizable linear operator on a nonzero vector space has
at least one eigenvalue.
The following two items relate to the optional subsection on direct sums.
(h)
If a vector space is the direct sum of subspaces W1, W2, . . . , Wk,
then Wi ‚à©Wj = {0} for i Ã∏= j.
(i)
If
V =
k

i=1
Wi
and
Wi ‚à©Wj = {0}
for i Ã∏= j,
then V = W1 ‚äïW2 ‚äï¬∑ ¬∑ ¬∑ ‚äïWk.
2.
For each of the following matrices A ‚ààMn√ón(R), test A for diagonal-
izability, and if A is diagonalizable, Ô¨Ånd an invertible matrix Q and a
diagonal matrix D such that Q‚àí1AQ = D.
(a)

1
2
0
1
	
(b)

1
3
3
1
	
(c)

1
4
3
2
	
(d)
‚éõ
‚éù
7
‚àí4
0
8
‚àí5
0
6
‚àí6
3
‚éû
‚é†
(e)
‚éõ
‚éù
0
0
1
1
0
‚àí1
0
1
1
‚éû
‚é†
(f)
‚éõ
‚éù
1
1
0
0
1
2
0
0
3
‚éû
‚é†

280
Chap. 5
Diagonalization
(g)
‚éõ
‚éù
3
1
1
2
4
2
‚àí1
‚àí1
1
‚éû
‚é†
3.
For each of the following linear operators T on a vector space V, test
T for diagonalizability, and if T is diagonalizable, Ô¨Ånd a basis Œ≤ for V
such that [T]Œ≤ is a diagonal matrix.
(a)
V = P3(R) and T is deÔ¨Åned by T(f(x)) = f ‚Ä≤(x) + f ‚Ä≤‚Ä≤(x), respec-
tively.
(b)
V = P2(R) and T is deÔ¨Åned by T(ax2 + bx + c) = cx2 + bx + a.
(c)
V = R3 and T is deÔ¨Åned by
T
‚éõ
‚éù
a1
a2
a3
‚éû
‚é†=
‚éõ
‚éù
a2
‚àía1
2a3
‚éû
‚é†.
(d)
V = P2(R) and T is deÔ¨Åned by T(f(x)) = f(0) + f(1)(x + x2).
(e)
V = C2 and T is deÔ¨Åned by T(z, w) = (z + iw, iz + w).
(f)
V = M2√ó2(R) and T is deÔ¨Åned by T(A) = At.
4.
Prove the matrix version of the corollary to Theorem 5.5:
If A ‚àà
Mn√ón(F) has n distinct eigenvalues, then A is diagonalizable.
5.
State and prove the matrix version of Theorem 5.6.
6. (a)
Justify the test for diagonalizability and the method for diagonal-
ization stated in this section.
(b)
Formulate the results in (a) for matrices.
7.
For
A =

1
4
2
3
	
‚ààM2√ó2(R),
Ô¨Ånd an expression for An, where n is an arbitrary positive integer.
8.
Suppose that A ‚ààMn√ón(F) has two distinct eigenvalues, Œª1 and Œª2,
and that dim(EŒª1) = n ‚àí1. Prove that A is diagonalizable.
9.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V, and
suppose there exists an ordered basis Œ≤ for V such that [T]Œ≤ is an upper
triangular matrix.
(a)
Prove that the characteristic polynomial for T splits.
(b)
State and prove an analogous result for matrices.
The converse of (a) is treated in Exercise 32 of Section 5.4.

Sec. 5.2
Diagonalizability
281
10.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V with
the distinct eigenvalues Œª1, Œª2, . . . , Œªk and corresponding multiplicities
m1, m2, . . . , mk. Suppose that Œ≤ is a basis for V such that [T]Œ≤ is an
upper triangular matrix. Prove that the diagonal entries of [T]Œ≤ are
Œª1, Œª2, . . . , Œªk and that each Œªi occurs mi times (1 ‚â§i ‚â§k).
11.
Let A be an n √ó n matrix that is similar to an upper triangular ma-
trix and has the distinct eigenvalues Œª1, Œª2, . . . , Œªk with corresponding
multiplicities m1, m2, . . . , mk. Prove the following statements.
(a)
tr(A) =
k

i=1
miŒªi
(b)
det(A) = (Œª1)m1(Œª2)m2 ¬∑ ¬∑ ¬∑ (Œªk)mk.
12.
Let T be an invertible linear operator on a Ô¨Ånite-dimensional vector
space V.
(a)
Recall that for any eigenvalue Œª of T, Œª‚àí1 is an eigenvalue of T‚àí1
(Exercise 8 of Section 5.1). Prove that the eigenspace of T corre-
sponding to Œª is the same as the eigenspace of T‚àí1 corresponding
to Œª‚àí1.
(b)
Prove that if T is diagonalizable, then T‚àí1 is diagonalizable.
13.
Let A ‚ààMn√ón(F). Recall from Exercise 14 of Section 5.1 that A and
At have the same characteristic polynomial and hence share the same
eigenvalues with the same multiplicities. For any eigenvalue Œª of A and
At, let EŒª and E‚Ä≤
Œª denote the corresponding eigenspaces for A and At,
respectively.
(a)
Show by way of example that for a given common eigenvalue, these
two eigenspaces need not be the same.
(b)
Prove that for any eigenvalue Œª, dim(EŒª) = dim(E‚Ä≤
Œª).
(c)
Prove that if A is diagonalizable, then At is also diagonalizable.
14.
Find the general solution to each system of diÔ¨Äerential equations.
(a)
x‚Ä≤ = x + y
y‚Ä≤ = 3x ‚àíy
(b) x‚Ä≤
1 =
8x1 + 10x2
x‚Ä≤
2 = ‚àí5x1 ‚àí7x2
(c)
x‚Ä≤
1 = x1
+ x3
x‚Ä≤
2 =
x2 + x3
x‚Ä≤
3 =
2x3
15.
Let
A =
‚éõ
‚éú
‚éú
‚éú
‚éù
a11
a12
¬∑ ¬∑ ¬∑
a1n
a21
a22
¬∑ ¬∑ ¬∑
a2n
...
...
...
an1
an2
¬∑ ¬∑ ¬∑
ann
‚éû
‚éü
‚éü
‚éü
‚é†

282
Chap. 5
Diagonalization
be the coeÔ¨Écient matrix of the system of diÔ¨Äerential equations
x‚Ä≤
1 = a11x1 + a12x2 + ¬∑ ¬∑ ¬∑ + a1nxn
x‚Ä≤
2 = a21x1 + a22x2 + ¬∑ ¬∑ ¬∑ + a2nxn
...
x‚Ä≤
n = an1x1 + an2x2 + ¬∑ ¬∑ ¬∑ + annxn.
Suppose that A is diagonalizable and that the distinct eigenvalues of A
are Œª1, Œª2, . . . , Œªk. Prove that a diÔ¨Äerentiable function x: R ‚ÜíRn is a
solution to the system if and only if x is of the form
x(t) = eŒª1tz1 + eŒª2tz2 + ¬∑ ¬∑ ¬∑ + eŒªktzk,
where zi ‚ààEŒªi for i = 1, 2, . . . , k. Use this result to prove that the set
of solutions to the system is an n-dimensional real vector space.
16.
Let C ‚ààMm√ón(R), and let Y be an n √ó p matrix of diÔ¨Äerentiable
functions. Prove (CY )‚Ä≤ = CY ‚Ä≤, where (Y ‚Ä≤)ij = Y ‚Ä≤
ij for all i, j.
Exercises 17 through 19 are concerned with simultaneous diagonalization.
DeÔ¨Ånitions. Two linear operators T and U on a Ô¨Ånite-dimensional vector
space V are called simultaneously diagonalizable if there exists an ordered
basis Œ≤ for V such that both [T]Œ≤ and [U]Œ≤ are diagonal matrices. Similarly,
A, B ‚ààMn√ón(F) are called simultaneously diagonalizable if there exists
an invertible matrix Q ‚ààMn√ón(F) such that both Q‚àí1AQ and Q‚àí1BQ are
diagonal matrices.
17. (a)
Prove that if T and U are simultaneously diagonalizable linear
operators on a Ô¨Ånite-dimensional vector space V, then the matrices
[T]Œ≤ and [U]Œ≤ are simultaneously diagonalizable for any ordered
basis Œ≤.
(b)
Prove that if A and B are simultaneously diagonalizable matrices,
then LA and LB are simultaneously diagonalizable linear operators.
18. (a)
Prove that if T and U are simultaneously diagonalizable operators,
then T and U commute (i.e., TU = UT).
(b)
Show that if A and B are simultaneously diagonalizable matrices,
then A and B commute.
The converses of (a) and (b) are established in Exercise 25 of Section 5.4.
19.
Let T be a diagonalizable linear operator on a Ô¨Ånite-dimensional vector
space, and let m be any positive integer. Prove that T and Tm are
simultaneously diagonalizable.
Exercises 20 through 23 are concerned with direct sums.

Sec. 5.3
Matrix Limits and Markov Chains
283
20.
Let W1, W2, . . . , Wk be subspaces of a Ô¨Ånite-dimensional vector space V
such that
k

i=1
Wi = V.
Prove that V is the direct sum of W1, W2, . . . , Wk if and only if
dim(V) =
k

i=1
dim(Wi).
21.
Let V be a Ô¨Ånite-dimensional vector space with a basis Œ≤, and let
Œ≤1, Œ≤2, . . . , Œ≤k be a partition of Œ≤ (i.e., Œ≤1, Œ≤2, . . . , Œ≤k are subsets of Œ≤
such that Œ≤ = Œ≤1 ‚à™Œ≤2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≤k and Œ≤i ‚à©Œ≤j = ‚àÖif i Ã∏= j). Prove that
V = span(Œ≤1) ‚äïspan(Œ≤2) ‚äï¬∑ ¬∑ ¬∑ ‚äïspan(Œ≤k).
22.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V, and
suppose that the distinct eigenvalues of T are Œª1, Œª2, . . . , Œªk. Prove that
span({x ‚ààV: x is an eigenvector of T}) = EŒª1 ‚äïEŒª2 ‚äï¬∑ ¬∑ ¬∑ ‚äïEŒªk.
23.
Let W1, W2, K1, K2, . . . , Kp, M1, M2, . . . , Mq be subspaces of a vector
space V such that W1 = K1‚äïK2‚äï¬∑ ¬∑ ¬∑‚äïKp and W2 = M1‚äïM2‚äï¬∑ ¬∑ ¬∑‚äïMq.
Prove that if W1 ‚à©W2 = {0}, then
W1 + W2 = W1 ‚äïW2 = K1 ‚äïK2 ‚äï¬∑ ¬∑ ¬∑ ‚äïKp ‚äïM1 ‚äïM2 ‚äï¬∑ ¬∑ ¬∑ ‚äïMq.
5.3‚àó
MATRIX LIMITS AND MARKOV CHAINS
In this section, we apply what we have learned thus far in Chapter 5 to study
the limit of a sequence of powers A, A2, . . . , An, . . ., where A is a square
matrix with complex entries. Such sequences and their limits have practical
applications in the natural and social sciences.
We assume familiarity with limits of sequences of real numbers.
The
limit of a sequence of complex numbers {zm : m = 1, 2, . . .} can be deÔ¨Åned
in terms of the limits of the sequences of the real and imaginary parts: If
zm = rm + ism, where rm and sm are real numbers, and i is the imaginary
number such that i2 = ‚àí1, then
lim
m‚Üí‚àûzm = lim
m‚Üí‚àûrm + i lim
m‚Üí‚àûsm,
provided that lim
m‚Üí‚àûrm and lim
m‚Üí‚àûsm exist.

284
Chap. 5
Diagonalization
DeÔ¨Ånition. Let L, A1, A2, . . . be n √ó p matrices having complex entries.
The sequence A1, A2, . . . is said to converge to the n √ó p matrix L, called
the limit of the sequence, if
lim
m‚Üí‚àû(Am)ij = Lij
for all 1 ‚â§i ‚â§n and 1 ‚â§j ‚â§p. To designate that L is the limit of the
sequence, we write
lim
m‚Üí‚àûAm = L.
Example 1
If
Am =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
1 ‚àí1
m

‚àí3
4
	m
3m2
m2+1 + i

2m+1
m‚àí1
	

i
2
	m
2

1 + 1
m
	m
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
,
then
lim
m‚Üí‚àûAm =

1
0
3 + 2i
0
2
e
	
,
where e is the base of the natural logarithm.
‚ô¶
A simple, but important, property of matrix limits is contained in the next
theorem. Note the analogy with the familiar property of limits of sequences
of real numbers that asserts that if lim
m‚Üí‚àûam exists, then
lim
m‚Üí‚àûcam = c

lim
m‚Üí‚àûam

.
Theorem 5.12. Let A1, A2, . . . be a sequence of n √ó p matrices with
complex entries that converges to the matrix L. Then for any P ‚ààMr√ón(C)
and Q ‚ààMp√ós(C),
lim
m‚Üí‚àûPAm = PL
and
lim
m‚Üí‚àûAmQ = LQ.
Proof. For any i (1 ‚â§i ‚â§r) and j (1 ‚â§j ‚â§p),
lim
m‚Üí‚àû(PAm)ij = lim
m‚Üí‚àû
n

k=1
Pik(Am)kj

Sec. 5.3
Matrix Limits and Markov Chains
285
=
n

k=1
Pik ¬∑ lim
m‚Üí‚àû(Am)kj =
n

k=1
PikLkj = (PL)ij.
Hence lim
m‚Üí‚àûPAm = PL. The proof that lim
m‚Üí‚àûAmQ = LQ is similar.
Corollary. Let A ‚ààMn√ón(C) be such that
lim
m‚Üí‚àûAm = L. Then for any
invertible matrix Q ‚ààMn√ón(C),
lim
m‚Üí‚àû(QAQ‚àí1)m = QLQ‚àí1.
Proof. Since
(QAQ‚àí1)m = (QAQ‚àí1)(QAQ‚àí1) ¬∑ ¬∑ ¬∑ (QAQ‚àí1) = QAmQ‚àí1,
we have
lim
m‚Üí‚àû(QAQ‚àí1)m = lim
m‚Üí‚àûQAmQ‚àí1 = Q

lim
m‚Üí‚àûAm
Q‚àí1 = QLQ‚àí1
by applying Theorem 5.12 twice.
In the discussion that follows, we frequently encounter the set
S = {Œª ‚ààC : |Œª| < 1 or Œª = 1}.
Geometrically, this set consists of the complex number 1 and the interior of
the unit disk (the disk of radius 1 centered at the origin).
This set is of
interest because if Œª is a complex number, then
lim
m‚Üí‚àûŒªn exists if and only
Œª ‚ààS. This fact, which is obviously true if Œª is real, can be shown to be true
for complex numbers also.
The following important result gives necessary and suÔ¨Écient conditions
for the existence of the type of limit under consideration.
Theorem 5.13. Let A be a square matrix with complex entries. Then
lim
m‚Üí‚àûAm exists if and only if both of the following conditions hold.
(a) Every eigenvalue of A is contained in S.
(b) If 1 is an eigenvalue of A, then the dimension of the eigenspace corre-
sponding to 1 equals the multiplicity of 1 as an eigenvalue of A.
One proof of this theorem, which relies on the theory of Jordan canonical
forms (Section 7.2), can be found in Exercise 19 of Section 7.2. A second
proof, which makes use of Schur‚Äôs theorem (Theorem 6.14 of Section 6.4),
can be found in the article by S. H. Friedberg and A. J. Insel, ‚ÄúConvergence
of matrix powers,‚Äù Int. J. Math. Educ. Sci. Technol., 1992, Vol. 23, no. 5,
pp. 765-769.

286
Chap. 5
Diagonalization
The necessity of condition (a) is easily justiÔ¨Åed. For suppose that Œª is an
eigenvalue of A such that Œª /‚ààS. Let v be an eigenvector of A corresponding
to Œª. Regarding v as an n √ó 1 matrix, we see that
lim
m‚Üí‚àû(Amv) =

lim
m‚Üí‚àûAm
v = Lv
by Theorem 5.12, where L =
lim
m‚Üí‚àûAm.
But
lim
m‚Üí‚àû(Amv) =
lim
m‚Üí‚àû(Œªmv)
diverges because
lim
m‚Üí‚àûŒªm does not exist.
Hence if
lim
m‚Üí‚àûAm exists, then
condition (a) of Theorem 5.13 must hold.
Although we are unable to prove the necessity of condition (b) here, we
consider an example for which this condition fails. Observe that the charac-
teristic polynomial for the matrix
B =

1
1
0
1
	
is (t ‚àí1)2, and hence B has eigenvalue Œª = 1 with multiplicity 2. It can
easily be veriÔ¨Åed that dim(EŒª) = 1, so that condition (b) of Theorem 5.13
is violated. A simple mathematical induction argument can be used to show
that
Bm =

1
m
0
1
	
,
and therefore that
lim
m‚Üí‚àûBm does not exist. We see in Chapter 7 that if A
is a matrix for which condition (b) fails, then A is similar to a matrix whose
upper left 2 √ó 2 submatrix is precisely this matrix B.
In most of the applications involving matrix limits, the matrix is diag-
onalizable, and so condition (b) of Theorem 5.13 is automatically satisÔ¨Åed.
In this case, Theorem 5.13 reduces to the following theorem, which can be
proved using our previous results.
Theorem 5.14. Let A ‚ààMn√ón(C) satisfy the following two conditions.
(i) Every eigenvalue of A is contained in S.
(ii) A is diagonalizable.
Then lim
m‚Üí‚àûAm exists.
Proof. Since A is diagonalizable, there exists an invertible matrix Q such
that Q‚àí1AQ = D is a diagonal matrix. Suppose that
D =
‚éõ
‚éú
‚éú
‚éú
‚éù
Œª1
0
¬∑ ¬∑ ¬∑
0
0
Œª2
¬∑ ¬∑ ¬∑
0
...
...
...
0
0
¬∑ ¬∑ ¬∑
Œªn
‚éû
‚éü
‚éü
‚éü
‚é†.

Sec. 5.3
Matrix Limits and Markov Chains
287
Because Œª1, Œª2, . . . , Œªn are the eigenvalues of A, condition (i) requires that for
each i, either Œªi = 1 or |Œªi| < 1. Thus
lim
m‚Üí‚àûŒªi
m =

1
if Œªi = 1
0
otherwise.
But since
Dm =
‚éõ
‚éú
‚éú
‚éú
‚éù
Œª1
m
0
¬∑ ¬∑ ¬∑
0
0
Œª2
m
¬∑ ¬∑ ¬∑
0
...
...
...
0
0
¬∑ ¬∑ ¬∑
Œªn
m
‚éû
‚éü
‚éü
‚éü
‚é†,
the sequence D, D2, . . . converges to a limit L. Hence
lim
m‚Üí‚àûAm = lim
m‚Üí‚àû(QDQ‚àí1)m = QLQ‚àí1
by the corollary to Theorem 5.12.
The technique for computing lim
m‚Üí‚àûAm used in the proof of Theorem 5.14
can be employed in actual computations, as we now illustrate. Let
A =
‚éõ
‚éú
‚éú
‚éù
7
4
‚àí9
4
‚àí15
4
3
4
7
4
3
4
3
4
‚àí9
4
‚àí11
4
‚éû
‚éü
‚éü
‚é†.
Using the methods in Sections 5.1 and 5.2, we obtain
Q =
‚éõ
‚éù
1
3
‚àí1
‚àí3
‚àí2
1
2
3
‚àí1
‚éû
‚é†
and
D =
‚éõ
‚éú
‚éù
1
0
0
0
‚àí1
2
0
0
0
1
4
‚éû
‚éü
‚é†
such that Q‚àí1AQ = D. Hence
lim
m‚Üí‚àûAm = lim
m‚Üí‚àû(QDQ‚àí1)m = lim
m‚Üí‚àûQDmQ‚àí1 = Q

lim
m‚Üí‚àûDm
Q‚àí1
=
‚éõ
‚éù
1
3
‚àí1
‚àí3
‚àí2
1
2
3
‚àí1
‚éû
‚é†
‚é°
‚é¢‚é£lim
m‚Üí‚àû
‚éõ
‚éú
‚éù
1
0
0
0
(‚àí1
2)m
0
0
0
( 1
4)m
‚éû
‚éü
‚é†
‚é§
‚é•‚é¶
‚éõ
‚éù
‚àí1
0
1
‚àí1
1
2
‚àí5
3
7
‚éû
‚é†
=
‚éõ
‚éù
1
3
‚àí1
‚àí3
‚àí2
1
2
3
‚àí1
‚éû
‚é†
‚éõ
‚éù
1
0
0
0
0
0
0
0
0
‚éû
‚é†
‚éõ
‚éù
‚àí1
0
1
‚àí1
1
2
‚àí5
3
7
‚éû
‚é†=
‚éõ
‚éù
‚àí1
0
1
3
0
‚àí3
‚àí2
0
2
‚éû
‚é†.

288
Chap. 5
Diagonalization
Next, we consider an application that uses the limit of powers of a ma-
trix.
Suppose that the population of a certain metropolitan area remains
constant but there is a continual movement of people between the city and
the suburbs. SpeciÔ¨Åcally, let the entries of the following matrix A represent
the probabilities that someone living in the city or in the suburbs on January
1 will be living in each region on January 1 of the next year.
Currently
Currently
living in
living in
the city
the suburbs
Living next year in the city
Living next year in the suburbs

0.90
0.02
0.10
0.98
	
= A
For instance, the probability that someone living in the city (on January 1)
will be living in the suburbs next year (on January 1) is 0.10. Notice that
since the entries of A are probabilities, they are nonnegative. Moreover, the
assumption of a constant population in the metropolitan area requires that
the sum of the entries of each column of A be 1.
Any square matrix having these two properties (nonnegative entries and
columns that sum to 1) is called a transition matrix or a stochastic ma-
trix. For an arbitrary n √ó n transition matrix M, the rows and columns
correspond to n states, and the entry Mij represents the probability of mov-
ing from state j to state i in one stage.
In our example, there are two states (residing in the city and residing in
the suburbs). So, for example, A21 is the probability of moving from the
city to the suburbs in one stage, that is, in one year. We now determine the
City 
*
HHHHHHHH
j
City
Suburbs
HHHHHHH
j

* Suburbs
0.90
0.10
0.10
0.98
Figure 5.3
probability that a city resident will be living in the suburbs after 2 years.
There are two diÔ¨Äerent ways in which such a move can be made: remaining
in the city for 1 year and then moving to the suburbs, or moving to the
suburbs during the Ô¨Årst year and remaining there the second year.
(See

Sec. 5.3
Matrix Limits and Markov Chains
289
Figure 5.3.) The probability that a city dweller remains in the city for the
Ô¨Årst year is 0.90, whereas the probability that the city dweller moves to the
suburbs during the Ô¨Årst year is 0.10. Hence the probability that a city dweller
stays in the city for the Ô¨Årst year and then moves to the suburbs during the
second year is the product (0.90)(0.10). Likewise, the probability that a city
dweller moves to the suburbs in the Ô¨Årst year and remains in the suburbs
during the second year is the product (0.10)(0.98). Thus the probability that
a city dweller will be living in the suburbs after 2 years is the sum of these
products, (0.90)(0.10) + (0.10)(0.98) = 0.188. Observe that this number is
obtained by the same calculation as that which produces (A2)21, and hence
(A2)21 represents the probability that a city dweller will be living in the
suburbs after 2 years. In general, for any transition matrix M, the entry
(M m)ij represents the probability of moving from state j to state i in m
stages.
Suppose additionally that 70% of the 2000 population of the metropolitan
area lived in the city and 30% lived in the suburbs. We record these data as
a column vector:
Proportion of city dwellers
Proportion of suburb residents

0.70
0.30
	
= P.
Notice that the rows of P correspond to the states of residing in the city and
residing in the suburbs, respectively, and that these states are listed in the
same order as the listing in the transition matrix A. Observe also that the
column vector P contains nonnegative entries that sum to 1; such a vector is
called a probability vector. In this terminology, each column of a transition
matrix is a probability vector. It is often convenient to regard the entries of a
transition matrix or a probability vector as proportions or percentages instead
of probabilities, as we have already done with the probability vector P.
In the vector AP, the Ô¨Årst coordinate is the sum (0.90)(0.70)+(0.02)(0.30).
The Ô¨Årst term of this sum, (0.90)(0.70), represents the proportion of the 2000
metropolitan population that remained in the city during the next year, and
the second term, (0.02)(0.30), represents the proportion of the 2000 metropoli-
tan population that moved into the city during the next year. Hence the Ô¨Årst
coordinate of AP represents the proportion of the metropolitan population
that was living in the city in 2001. Similarly, the second coordinate of
AP =

0.636
0.364
	
represents the proportion of the metropolitan population that was living in
the suburbs in 2001. This argument can be easily extended to show that the
coordinates of
A2P = A(AP) =

0.57968
0.42032
	

290
Chap. 5
Diagonalization
represent the proportions of the metropolitan population that were living
in each location in 2002. In general, the coordinates of AmP represent the
proportion of the metropolitan population that will be living in the city and
suburbs, respectively, after m stages (m years after 2000).
Will the city eventually be depleted if this trend continues? In view of
the preceding discussion, it is natural to deÔ¨Åne the eventual proportion of
the city dwellers and suburbanites to be the Ô¨Årst and second coordinates,
respectively, of
lim
m‚Üí‚àûAmP. We now compute this limit. It is easily shown
that A is diagonalizable, and so there is an invertible matrix Q and a diagonal
matrix D such that Q‚àí1AQ = D. In fact,
Q =
‚éõ
‚éù
1
6
‚àí1
6
5
6
1
6
‚éû
‚é†
and
D =

1
0
0
0.88
	
.
Therefore
L = lim
m‚Üí‚àûAm = lim
m‚Üí‚àûQDmQ‚àí1 = Q

1
0
0
0
	
Q‚àí1 =
‚éõ
‚éù
1
6
1
6
5
6
5
6
‚éû
‚é†.
Consequently
lim
m‚Üí‚àûAmP = LP =
‚éõ
‚éù
1
6
5
6
‚éû
‚é†.
Thus, eventually, 1
6 of the population will live in the city and 5
6 will live in the
suburbs each year. Note that the vector LP satisÔ¨Åes A(LP) = LP. Hence
LP is both a probability vector and an eigenvector of A corresponding to
the eigenvalue 1. Since the eigenspace of A corresponding to the eigenvalue
1 is one-dimensional, there is only one such vector, and LP is independent
of the initial choice of probability vector P. (See Exercise 15.) For example,
had the 2000 metropolitan population consisted entirely of city dwellers, the
limiting outcome would be the same.
In analyzing the city‚Äìsuburb problem, we gave probabilistic interpreta-
tions of A2 and AP, showing that A2 is a transition matrix and AP is a
probability vector. In fact, the product of any two transition matrices is a
transition matrix, and the product of any transition matrix and probability
vector is a probability vector. A proof of these facts is a simple corollary
of the next theorem, which characterizes transition matrices and probability
vectors.
Theorem 5.15. Let M be an n√ón matrix having real nonnegative entries,
let v be a column vector in Rn having nonnegative coordinates, and let u ‚ààRn
be the column vector in which each coordinate equals 1. Then

Sec. 5.3
Matrix Limits and Markov Chains
291
(a) M is a transition matrix if and only if M tu = u;
(b) v is a probability vector if and only if utv = (1).
Proof. Exercise.
Corollary.
(a) The product of two n √ó n transition matrices is an n √ó n transition
matrix. In particular, any power of a transition matrix is a transition
matrix.
(b) The product of a transition matrix and a probability vector is a prob-
ability vector.
Proof. Exercise.
The city‚Äìsuburb problem is an example of a process in which elements of
a set are each classiÔ¨Åed as being in one of several Ô¨Åxed states that can switch
over time. In general, such a process is called a stochastic process. The
switching to a particular state is described by a probability, and in general
this probability depends on such factors as the state in question, the time
in question, some or all of the previous states in which the object has been
(including the current state), and the states that other objects are in or have
been in.
For instance, the object could be an American voter, and the state of the
object could be his or her preference of political party; or the object could
be a molecule of H2O, and the states could be the three physical states in
which H2O can exist (solid, liquid, and gas). In these examples, all four of
the factors mentioned above inÔ¨Çuence the probability that an object is in a
particular state at a particular time.
If, however, the probability that an object in one state changes to a diÔ¨Äer-
ent state in a Ô¨Åxed interval of time depends only on the two states (and not on
the time, earlier states, or other factors), then the stochastic process is called
a Markov process. If, in addition, the number of possible states is Ô¨Ånite,
then the Markov process is called a Markov chain. We treated the city‚Äì
suburb example as a two-state Markov chain. Of course, a Markov process is
usually only an idealization of reality because the probabilities involved are
almost never constant over time.
With this in mind, we consider another Markov chain. A certain com-
munity college would like to obtain information about the likelihood that
students in various categories will graduate. The school classiÔ¨Åes a student
as a sophomore or a freshman depending on the number of credits that the
student has earned. Data from the school indicate that, from one fall semester
to the next, 40% of the sophomores will graduate, 30% will remain sopho-
mores, and 30% will quit permanently. For freshmen, the data show that
10% will graduate by next fall, 50% will become sophomores, 20% will re-
main freshmen, and 20% will quit permanently.
During the present year,

292
Chap. 5
Diagonalization
50% of the students at the school are sophomores and 50% are freshmen. As-
suming that the trend indicated by the data continues indeÔ¨Ånitely, the school
would like to know
1. the percentage of the present students who will graduate, the percentage
who will be sophomores, the percentage who will be freshmen, and the
percentage who will quit school permanently by next fall;
2. the same percentages as in item 1 for the fall semester two years hence;
and
3. the probability that one of its present students will eventually graduate.
The preceding paragraph describes a four-state Markov chain with the
following states:
1. having graduated
2. being a sophomore
3. being a freshman
4. having quit permanently.
The given data provide us with the transition matrix
A =
‚éõ
‚éú
‚éú
‚éù
1
0.4
0.1
0
0
0.3
0.5
0
0
0
0.2
0
0
0.3
0.2
1
‚éû
‚éü
‚éü
‚é†
of the Markov chain. (Notice that students who have graduated or have quit
permanently are assumed to remain indeÔ¨Ånitely in those respective states.
Thus a freshman who quits the school and returns during a later semester
is not regarded as having changed states‚Äîthe student is assumed to have
remained in the state of being a freshman during the time he or she was not
enrolled.) Moreover, we are told that the present distribution of students is
half in each of states 2 and 3 and none in states 1 and 4. The vector
P =
‚éõ
‚éú
‚éú
‚éù
0
0.5
0.5
0
‚éû
‚éü
‚éü
‚é†
that describes the initial probability of being in each state is called the initial
probability vector for the Markov chain.
To answer question 1, we must determine the probabilities that a present
student will be in each state by next fall. As we have seen, these probabilities
are the coordinates of the vector
AP =
‚éõ
‚éú
‚éú
‚éù
1
0.4
0.1
0
0
0.3
0.5
0
0
0
0.2
0
0
0.3
0.2
1
‚éû
‚éü
‚éü
‚é†
‚éõ
‚éú
‚éú
‚éù
0
0.5
0.5
0
‚éû
‚éü
‚éü
‚é†=
‚éõ
‚éú
‚éú
‚éù
0.25
0.40
0.10
0.25
‚éû
‚éü
‚éü
‚é†.

Sec. 5.3
Matrix Limits and Markov Chains
293
Hence by next fall, 25% of the present students will graduate, 40% will be
sophomores, 10% will be freshmen, and 25% will quit the school permanently.
Similarly,
A2P = A(AP) =
‚éõ
‚éú
‚éú
‚éù
1
0.4
0.1
0
0
0.3
0.5
0
0
0
0.2
0
0
0.3
0.2
1
‚éû
‚éü
‚éü
‚é†
‚éõ
‚éú
‚éú
‚éù
0.25
0.40
0.10
0.25
‚éû
‚éü
‚éü
‚é†=
‚éõ
‚éú
‚éú
‚éù
0.42
0.17
0.02
0.39
‚éû
‚éü
‚éü
‚é†
provides the information needed to answer question 2: within two years 42%
of the present students will graduate, 17% will be sophomores, 2% will be
freshmen, and 39% will quit school.
Finally, the answer to question 3 is provided by the vector LP, where
L = lim
m‚Üí‚àûAm. For the matrices
Q =
‚éõ
‚éú
‚éú
‚éù
1
4
19
0
0
‚àí7
‚àí40
0
0
0
8
0
0
3
13
1
‚éû
‚éü
‚éü
‚é†
and
D =
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
0
0.3
0
0
0
0
0.2
0
0
0
0
1
‚éû
‚éü
‚éü
‚é†,
we have Q‚àí1AQ = D. Thus
L = lim
m‚Üí‚àûAm = Q

lim
m‚Üí‚àûDm
Q‚àí1
=
‚éõ
‚éú
‚éú
‚éù
1
4
19
0
0
‚àí7
‚àí40
0
0
0
8
0
0
3
13
1
‚éû
‚éü
‚éü
‚é†
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
‚éû
‚éü
‚éü
‚é†
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
1
4
7
27
56
0
0
‚àí1
7
‚àí5
7
0
0
0
1
8
0
0
3
7
29
56
1
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
=
‚éõ
‚éú
‚éú
‚éú
‚éù
1
4
7
27
56
0
0
0
0
0
0
0
0
0
0
3
7
29
56
1
‚éû
‚éü
‚éü
‚éü
‚é†.
So
LP =
‚éõ
‚éú
‚éú
‚éú
‚éù
1
4
7
27
56
0
0
0
0
0
0
0
0
0
0
3
7
29
56
1
‚éû
‚éü
‚éü
‚éü
‚é†
‚éõ
‚éú
‚éú
‚éù
0
0.5
0.5
0
‚éû
‚éü
‚éü
‚é†=
‚éõ
‚éú
‚éú
‚éú
‚éù
59
112
0
0
53
112
‚éû
‚éü
‚éü
‚éü
‚é†,
and hence the probability that one of the present students will graduate is 59
112.

294
Chap. 5
Diagonalization
In the preceding two examples, we saw that
lim
m‚Üí‚àûAmP, where A is the
transition matrix and P is the initial probability vector of the Markov chain,
gives the eventual proportions in each state. In general, however, the limit of
powers of a transition matrix need not exist. For example, if
M =

0
1
1
0
	
,
then
lim
m‚Üí‚àûM m does not exist because odd powers of M equal M and even
powers of M equal I. The reason that the limit fails to exist is that con-
dition (a) of Theorem 5.13 does not hold for M (‚àí1 is an eigenvalue). In
fact, it can be shown (see Exercise 20 of Section 7.2) that the only transition
matrices A such that lim
m‚Üí‚àûAm does not exist are precisely those matrices for
which condition (a) of Theorem 5.13 fails to hold.
But even if the limit of powers of the transition matrix exists, the compu-
tation of the limit may be quite diÔ¨Écult. (The reader is encouraged to work
Exercise 6 to appreciate the truth of the last sentence.) Fortunately, there is
a large and important class of transition matrices for which this limit exists
and is easily computed‚Äîthis is the class of regular transition matrices.
DeÔ¨Ånition.
A transition matrix is called regular if some power of the
matrix contains only positive entries.
Example 2
The transition matrix

0.90
0.02
0.10
0.98
	
of the Markov chain used in the city‚Äìsuburb problem is clearly regular because
each entry is positive. On the other hand, the transition matrix
A =
‚éõ
‚éú
‚éú
‚éù
1
0.4
0.1
0
0
0.3
0.5
0
0
0
0.2
0
0
0.3
0.2
1
‚éû
‚éü
‚éü
‚é†
of the Markov chain describing community college enrollments is not regular
because the Ô¨Årst column of Am is
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
‚éû
‚éü
‚éü
‚é†
for any power m.

Sec. 5.3
Matrix Limits and Markov Chains
295
Observe that a regular transition matrix may contain zero entries. For
example,
M =
‚éõ
‚éù
0.9
0.5
0
0
0.5
0.4
0.1
0
0.6
‚éû
‚é†
is regular because every entry of M 2 is positive.
‚ô¶
The remainder of this section is devoted to proving that, for a regular
transition matrix A, the limit of the sequence of powers of A exists and
has identical columns. From this fact, it is easy to compute this limit. In
the course of proving this result, we obtain some interesting bounds for the
magnitudes of eigenvalues of any square matrix. These bounds are given in
terms of the sum of the absolute values of the rows and columns of the matrix.
The necessary terminology is introduced in the deÔ¨Ånitions that follow.
DeÔ¨Ånitions. Let A ‚ààMn√ón(C). For 1 ‚â§i, j ‚â§n, deÔ¨Åne œÅi(A) to be the
sum of the absolute values of the entries of row i of A, and deÔ¨Åne ŒΩj(A) to be
equal to the sum of the absolute values of the entries of column j of A. Thus
œÅi(A) =
n

j=1
|Aij|
for i = 1, 2, . . . n
and
ŒΩj(A) =
n

i=1
|Aij|
for j = 1, 2, . . . n.
The row sum of A, denoted œÅ(A), and the column sum of A, denoted ŒΩ(A),
are deÔ¨Åned as
œÅ(A) = max{œÅi(A): 1 ‚â§i ‚â§n}
and
ŒΩ(A) = max{ŒΩj(A): 1 ‚â§j ‚â§n}.
Example 3
For the matrix
A =
‚éõ
‚éù
1
‚àíi
3 ‚àí4i
‚àí2 + i
0
6
3
2
i
‚éû
‚é†,
œÅ1(A) = 7, œÅ2(A) = 6 +
‚àö
5, œÅ3(A) = 6, ŒΩ1(A) = 4 +
‚àö
5, ŒΩ2(A) = 3, and
ŒΩ3(A) = 12. Hence œÅ(A) = 6 +
‚àö
5 and ŒΩ(A) = 12.
‚ô¶

296
Chap. 5
Diagonalization
Our next results show that the smaller of œÅ(A) and ŒΩ(A) is an upper
bound for the absolute values of eigenvalues of A. In the preceding example,
for instance, A has no eigenvalue with absolute value greater than 6 +
‚àö
5.
To obtain a geometric view of the following theorem, we introduce some
terminology. For an n√ón matrix A, we deÔ¨Åne the ith Gerschgorin disk Ci to
be the disk in the complex plane with center Aii and radius ri = œÅi(A)‚àí|Aii|;
that is,
Ci = {z ‚ààC : |z ‚àíAii| < ri}.
For example, consider the matrix
A =

1 + 2i
1
2i
‚àí3
	
.
For this matrix, C1 is the disk with center 1 + 2i and radius 1, and C2 is the
disk with center ‚àí3 and radius 2. (See Figure 5.4.)
................................................................................................................................................................................................................................................................................................................................................................................................................
q
q
q
................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................
real axis
imaginary axis
‚àí3
1 + 2i
1
2
0
C1
C2
Figure 5.4
Gershgorin‚Äôs disk theorem, stated below, tells us that all the eigenvalues
of A are located within these two disks. In particular, we see that 0 is not an
eigenvalue, and hence by Exercise 8(c) of section 5.1, A is invertible.
Theorem 5.16 (Gerschgorin‚Äôs Disk Theorem).
Let A ‚ààMn√ón(C).
Then every eigenvalue of A is contained in a Gerschgorin disk.
Proof. Let Œª be an eigenvalue of A with the corresponding eigenvector
v =
‚éõ
‚éú
‚éú
‚éú
‚éù
v1
v2
...
vn
‚éû
‚éü
‚éü
‚éü
‚é†.

Sec. 5.3
Matrix Limits and Markov Chains
297
Then v satisÔ¨Åes the matrix equation Av = Œªv, which can be written
n

j=1
Aijvj = Œªvi
(i = 1, 2, . . . , n).
(2)
Suppose that vk is the coordinate of v having the largest absolute value; note
that vk Ã∏= 0 because v is an eigenvector of A.
We show that Œª lies in Ck, that is, |Œª ‚àíAkk| ‚â§rk. For i = k, it follows
from (2) that
|Œªvk ‚àíAkkvk| =
######
n

j=1
Akjvj ‚àíAkkvk
######
=
######

jÃ∏=k
Akjvj
######
‚â§

jÃ∏=k
|Akj||vj| ‚â§

jÃ∏=k
|Akj||vk|
= |vk|

jÃ∏=k
|Akj| = |vk|rk.
Thus
|vk||Œª ‚àíAkk| ‚â§|vk|rk;
so
|Œª ‚àíAkk| ‚â§rk
because |vk| > 0.
Corollary 1. Let Œª be any eigenvalue of A ‚ààMn√ón(C). Then |Œª| ‚â§œÅ(A).
Proof. By Gerschgorin‚Äôs disk theorem, |Œª ‚àíAkk| ‚â§rk for some k. Hence
|Œª| = |(Œª ‚àíAkk) + Akk| ‚â§|Œª ‚àíAkk| + |Akk|
‚â§rk + |Akk| = œÅk(A) ‚â§œÅ(A).
Corollary 2. Let Œª be any eigenvalue of A ‚ààMn√ón(C). Then
|Œª| ‚â§min{œÅ(A), ŒΩ(A)}.
Proof. Since |Œª| ‚â§œÅ(A) by Corollary 1, it suÔ¨Éces to show that |Œª| ‚â§ŒΩ(A).
By Exercise 14 of Section 5.1, Œª is an eigenvalue of At, and so |Œª| ‚â§œÅ(At)
by Corollary 2.
But the rows of At are the columns of A; consequently
œÅ(At) = ŒΩ(A). Therefore |Œª| ‚â§ŒΩ(A).
The next corollary is immediate from Corollary 2.

298
Chap. 5
Diagonalization
Corollary 3. If Œª is an eigenvalue of a transition matrix, then |Œª| ‚â§1.
The next result asserts that the upper bound in Corollary 3 is attained.
Theorem 5.17. Every transition matrix has 1 as an eigenvalue.
Proof. Let A be an n √ó n transition matrix, and let u ‚ààRn be the column
vector in which each coordinate is 1. Then Atu = u by Theorem 5.15, and
hence u is an eigenvector of At corresponding to the eigenvalue 1. But since
A and At have the same eigenvalues, it follows that 1 is also an eigenvalue of
A.
Suppose that A is a transition matrix for which some eigenvector corre-
sponding to the eigenvalue 1 has only nonnegative coordinates. Then some
multiple of this vector is a probability vector P as well as an eigenvector of
A corresponding to eigenvalue 1. It is interesting to observe that if P is the
initial probability vector of a Markov chain having A as its transition matrix,
then the Markov chain is completely static. For in this situation, AmP = P
for every positive integer m; hence the probability of being in each state never
changes. Consider, for instance, the city‚Äìsuburb problem with
P =
‚éõ
‚éù
1
6
5
6
‚éû
‚é†.
Theorem 5.18. Let A ‚ààMn√ón(C) be a matrix in which each entry is
positive, and let Œª be an eigenvalue of A such that |Œª| = œÅ(A). Then Œª = œÅ(A)
and {u} is a basis for EŒª, where u ‚ààCn is the column vector in which each
coordinate equals 1.
Proof. Let v be an eigenvector of A corresponding to Œª, with coordinates
v1, v2, . . . , vn. Suppose that vk is the coordinate of v having the largest ab-
solute value, and let b = |vk|. Then
|Œª|b = |Œª||vk| = |Œªvk| =
######
n

j=1
Akjvj
######
‚â§
n

j=1
|Akjvj|
=
n

j=1
|Akj||vj| ‚â§
n

j=1
|Akj|b = œÅk(A)b ‚â§œÅ(A)b.
(3)
Since |Œª| = œÅ(A), the three inequalities in (3) are actually equalities; that is,
(a)
######
n

j=1
Akjvj
######
=
n

j=1
|Akjvj|,

Sec. 5.3
Matrix Limits and Markov Chains
299
(b)
n

j=1
|Akj||vj| =
n

j=1
|Akj|b, and
(c) œÅk(A) = œÅ(A).
We see in Exercise 15(b) of Section 6.1 that (a) holds if and only if all
the terms Akjvj (j = 1, 2, . . . , n) are nonnegative multiples of some nonzero
complex number z. Without loss of generality, we assume that |z| = 1. Thus
there exist nonnegative real numbers c1, c2, . . . , cn such that
Akjvj = cjz.
(4)
By (b) and the assumption that Akj Ã∏= 0 for all k and j, we have
|vj| = b
for j = 1, 2, . . . , n.
(5)
Combining (4) and (5), we obtain
b = |vj| =
####
cj
Akj
z
#### = cj
Akj
for j = 1, 2, . . . , n,
and therefore by (4), we have vj = bz for all j. So
v =
‚éõ
‚éú
‚éú
‚éú
‚éù
v1
v2
...
vn
‚éû
‚éü
‚éü
‚éü
‚é†=
‚éõ
‚éú
‚éú
‚éú
‚éù
bz
bz
...
bz
‚éû
‚éü
‚éü
‚éü
‚é†= bzu,
and hence {u} is a basis for EŒª.
Finally, observe that all of the entries of Au are positive because the same
is true for the entries of both A and u. But Au = Œªu, and hence Œª > 0.
Therefore, Œª = |Œª| = œÅ(A).
Corollary 1.
Let A ‚ààMn√ón(C) be a matrix in which each entry is
positive, and let Œª be an eigenvalue of A such that |Œª| = ŒΩ(A). Then Œª = ŒΩ(A),
and the dimension of EŒª = 1.
Proof. Exercise.
Corollary 2.
Let A ‚ààMn√ón(C) be a transition matrix in which each
entry is positive, and let Œª be any eigenvalue of A other than 1. Then |Œª| < 1.
Moreover, the eigenspace corresponding to the eigenvalue 1 has dimension 1.
Proof. Exercise.
Our next result extends Corollary 2 to regular transition matrices and thus
shows that regular transition matrices satisfy condition (a) of Theorems 5.13
and 5.14.

300
Chap. 5
Diagonalization
Theorem 5.19. Let A be a regular transition matrix, and let Œª be an
eigenvalue of A. Then
(a) |Œª| ‚â§1.
(b) If |Œª| = 1, then Œª = 1, and dim(EŒª) = 1.
Proof. Statement (a) was proved as Corollary 3 to Theorem 5.16.
(b) Since A is regular, there exists a positive integer s such that As has
only positive entries.
Because A is a transition matrix and the entries of
As are positive, the entries of As+1 = As(A) are positive.
Suppose that
|Œª| = 1. Then Œªs and Œªs+1 are eigenvalues of As and As+1, respectively,
having absolute value 1. So by Corollary 2 to Theorem 5.18, Œªs = Œªs+1 = 1.
Thus Œª = 1. Let EŒª and E‚Ä≤
Œª denote the eigenspaces of A and As, respectively,
corresponding to Œª = 1. Then EŒª ‚äÜE‚Ä≤
Œª and, by Corollary 2 to Theorem 5.18,
dim(E‚Ä≤
Œª) = 1. Hence EŒª = E‚Ä≤
Œª, and dim(EŒª) = 1.
Corollary. Let A be a regular transition matrix that is diagonalizable.
Then lim
m‚Üí‚àûAm exists.
The preceding corollary, which follows immediately from Theorems 5.19
and 5.14, is not the best possible result. In fact, it can be shown that if A is
a regular transition matrix, then the multiplicity of 1 as an eigenvalue of A is
1. Thus, by Theorem 5.7 (p. 264), condition (b) of Theorem 5.13 is satisÔ¨Åed.
So if A is a regular transition matrix,
lim
m‚Üí‚àûAm exists regardless of whether
A is or is not diagonalizable. As with Theorem 5.13, however, the fact that
the multiplicity of 1 as an eigenvalue of A is 1 cannot be proved at this time.
Nevertheless, we state this result here (leaving the proof until Exercise 20 of
Section 7.2) and deduce further facts about
lim
m‚Üí‚àûAm when A is a regular
transition matrix.
Theorem 5.20. Let A be an n √ó n regular transition matrix. Then
(a) The multiplicity of 1 as an eigenvalue of A is 1.
(b)
lim
m‚Üí‚àûAm exists.
(c) L = lim
m‚Üí‚àûAm is a transition matrix.
(d) AL = LA = L.
(e) The columns of L are identical. In fact, each column of L is equal to
the unique probability vector v that is also an eigenvector of A corre-
sponding to the eigenvalue 1.
(f) For any probability vector w, lim
m‚Üí‚àû(Amw) = v.
Proof. (a) See Exercise 20 of Section 7.2.
(b) This follows from (a) and Theorems 5.19 and 5.13.
(c) By Theorem 5.15, we must show that utL = ut. Now Am is a transition
matrix by the corollary to Theorem 5.15, so
utL = ut lim
m‚Üí‚àûAm = lim
m‚Üí‚àûutAm = lim
m‚Üí‚àûut = ut,

Sec. 5.3
Matrix Limits and Markov Chains
301
and it follows that L is a transition matrix.
(d) By Theorem 5.12,
AL = A lim
m‚Üí‚àûAm = lim
m‚Üí‚àûAAm = lim
m‚Üí‚àûAm+1 = L.
Similarly, LA = L.
(e) Since AL = L by (d), each column of L is an eigenvector of A cor-
responding to the eigenvalue 1.
Moreover, by (c), each column of L is a
probability vector. Thus, by (a), each column of L is equal to the unique
probability vector v corresponding to the eigenvalue 1 of A.
(f) Let w be any probability vector, and set y = lim
m‚Üí‚àûAmw = Lw. Then
y is a probability vector by the corollary to Theorem 5.15, and also Ay =
ALw = Lw = y by (d). Hence y is also an eigenvector corresponding to the
eigenvalue 1 of A. So y = v by (e).
DeÔ¨Ånition.
The vector v in Theorem 5.20(e) is called the Ô¨Åxed prob-
ability vector or stationary vector of the regular transition matrix A.
Theorem 5.20 can be used to deduce information about the eventual dis-
tribution in each state of a Markov chain having a regular transition matrix.
Example 4
A survey in Persia showed that on a particular day 50% of the Persians
preferred a loaf of bread, 30% preferred a jug of wine, and 20% preferred
‚Äúthou beside me in the wilderness.‚Äù
A subsequent survey 1 month later
yielded the following data: Of those who preferred a loaf of bread on the Ô¨Årst
survey, 40% continued to prefer a loaf of bread, 10% now preferred a jug of
wine, and 50% preferred ‚Äúthou‚Äù; of those who preferred a jug of wine on the
Ô¨Årst survey, 20% now preferred a loaf of bread, 70% continued to prefer a jug
of wine, and 10% now preferred ‚Äúthou‚Äù; of those who preferred ‚Äúthou‚Äù on the
Ô¨Årst survey, 20% now preferred a loaf of bread, 20% now preferred a jug of
wine, and 60% continued to prefer ‚Äúthou.‚Äù
Assuming that this trend continues, the situation described in the preced-
ing paragraph is a three-state Markov chain in which the states are the three
possible preferences. We can predict the percentage of Persians in each state
for each month following the original survey. Letting the Ô¨Årst, second, and
third states be preferences for bread, wine, and ‚Äúthou‚Äù, respectively, we see
that the probability vector that gives the initial probability of being in each
state is
P =
‚éõ
‚éù
0.50
0.30
0.20
‚éû
‚é†,

302
Chap. 5
Diagonalization
and the transition matrix is
A =
‚éõ
‚éù
0.40
0.20
0.20
0.10
0.70
0.20
0.50
0.10
0.60
‚éû
‚é†.
The probabilities of being in each state m months after the original survey
are the coordinates of the vector AmP. The reader may check that
AP =
‚éõ
‚éù
0.30
0.30
0.40
‚éû
‚é†, A2P =
‚éõ
‚éù
0.26
0.32
0.42
‚éû
‚é†, A3P =
‚éõ
‚éù
0.252
0.334
0.414
‚éû
‚é†, and A4P =
‚éõ
‚éù
0.2504
0.3418
0.4078
‚éû
‚é†.
Note the apparent convergence of AmP.
Since A is regular, the long-range prediction concerning the Persians‚Äô pref-
erences can be found by computing the Ô¨Åxed probability vector for A. This
vector is the unique probability vector v such that (A ‚àíI)v = 0. Letting
v =
‚éõ
‚éù
v1
v2
v3
‚éû
‚é†,
we see that the matrix equation (A ‚àíI)v = 0 yields the following system of
linear equations:
‚àí0.60v1 + 0.20v2 + 0.20v3 = 0
0.10v1 ‚àí0.30v2 + 0.20v3 = 0
0.50v1 + 0.10v2 ‚àí0.40v3 = 0 .
It is easily shown that
‚éõ
‚éù
5
7
8
‚éû
‚é†
is a basis for the solution space of this system. Hence the unique Ô¨Åxed prob-
ability vector for A is
‚éõ
‚éú
‚éú
‚éù
5
5+7+8
7
5+7+8
8
5+7+8
‚éû
‚éü
‚éü
‚é†=
‚éõ
‚éù
0.25
0.35
0.40
‚éû
‚é†.
Thus, in the long run, 25% of the Persians prefer a loaf of bread, 35% prefer
a jug of wine, and 40% prefer ‚Äúthou beside me in the wilderness.‚Äù

Sec. 5.3
Matrix Limits and Markov Chains
303
Note that if
Q =
‚éõ
‚éù
5
0
‚àí3
7
‚àí1
‚àí1
8
1
4
‚éû
‚é†,
then
Q‚àí1AQ =
‚éõ
‚éù
1
0
0
0
0.5
0
0
0
0.2
‚éû
‚é†.
So
lim
m‚Üí‚àûAm = Q
‚é°
‚é£lim
m‚Üí‚àû
‚éõ
‚éù
1
0
0
0
0.5
0
0
0
0.2
‚éû
‚é†
m‚é§
‚é¶Q‚àí1 = Q
‚éõ
‚éù
1
0
0
0
0
0
0
0
0
‚éû
‚é†Q‚àí1
=
‚éõ
‚éù
0.25
0.25
0.25
0.35
0.35
0.35
0.40
0.40
0.40
‚éû
‚é†.
‚ô¶
Example 5
Farmers in Lamron plant one crop per year‚Äîeither corn, soybeans, or wheat.
Because they believe in the necessity of rotating their crops, these farmers do
not plant the same crop in successive years. In fact, of the total acreage on
which a particular crop is planted, exactly half is planted with each of the
other two crops during the succeeding year. This year, 300 acres of corn, 200
acres of soybeans, and 100 acres of wheat were planted.
The situation just described is another three-state Markov chain in which
the three states correspond to the planting of corn, soybeans, and wheat,
respectively. In this problem, however, the amount of land devoted to each
crop, rather than the percentage of the total acreage (600 acres), is given. By
converting these amounts into fractions of the total acreage, we see that the
transition matrix A and the initial probability vector P of the Markov chain
are
A =
‚éõ
‚éú
‚éú
‚éù
0
1
2
1
2
1
2
0
1
2
1
2
1
2
0
‚éû
‚éü
‚éü
‚é†
and
P =
‚éõ
‚éú
‚éú
‚éù
300
600
200
600
100
600
‚éû
‚éü
‚éü
‚é†=
‚éõ
‚éú
‚éú
‚éù
1
2
1
3
1
6
‚éû
‚éü
‚éü
‚é†.
The fraction of the total acreage devoted to each crop in m years is given by
the coordinates of AmP, and the eventual proportions of the total acreage
used for each crop are the coordinates of
lim
m‚Üí‚àûAmP.
Thus the eventual

304
Chap. 5
Diagonalization
amounts of land devoted to each crop are found by multiplying this limit by
the total acreage; that is, the eventual amounts of land used for each crop
are the coordinates of 600 ¬∑ lim
m‚Üí‚àûAmP.
Since A is a regular transition matrix, Theorem 5.20 shows that lim
m‚Üí‚àûAm
is a matrix L in which each column equals the unique Ô¨Åxed probability vector
for A. It is easily seen that the Ô¨Åxed probability vector for A is
‚éõ
‚éú
‚éú
‚éù
1
3
1
3
1
3
‚éû
‚éü
‚éü
‚é†.
Hence
L =
‚éõ
‚éú
‚éú
‚éù
1
3
1
3
1
3
1
3
1
3
1
3
1
3
1
3
1
3
‚éû
‚éü
‚éü
‚é†;
so
600 ¬∑ lim
m‚Üí‚àûAmP = 600LP =
‚éõ
‚éù
200
200
200
‚éû
‚é†.
Thus, in the long run, we expect 200 acres of each crop to be planted each
year. (For a direct computation of 600 ¬∑ lim
m‚Üí‚àûAmP, see Exercise 14.)
‚ô¶
In this section, we have concentrated primarily on the theory of regular
transition matrices. There is another interesting class of transition matrices
that can be represented in the form

I
B
O
C
	
,
where I is an identity matrix and O is a zero matrix. (Such transition ma-
trices are not regular since the lower left block remains O in any power of
the matrix.) The states corresponding to the identity submatrix are called
absorbing states because such a state is never left once it is entered. A
Markov chain is called an absorbing Markov chain if it is possible to go
from an arbitrary state into an absorbing state in a Ô¨Ånite number of stages.
Observe that the Markov chain that describes the enrollment pattern in a
community college is an absorbing Markov chain with states 1 and 4 as its ab-
sorbing states. Readers interested in learning more about absorbing Markov
chains are referred to Introduction to Finite Mathematics (third edition) by

Sec. 5.3
Matrix Limits and Markov Chains
305
J. Kemeny, J. Snell, and G. Thompson (Prentice-Hall, Inc., Englewood CliÔ¨Äs,
N. J., 1974) or Discrete Mathematical Models by Fred S. Roberts (Prentice-
Hall, Inc., Englewood CliÔ¨Äs, N. J., 1976).
An Application
In species that reproduce sexually, the characteristics of an oÔ¨Äspring with
respect to a particular genetic trait are determined by a pair of genes, one
inherited from each parent. The genes for a particular trait are of two types,
which are denoted by G and g. The gene G represents the dominant char-
acteristic, and g represents the recessive characteristic. OÔ¨Äspring with geno-
types GG or Gg exhibit the dominant characteristic, whereas oÔ¨Äspring with
genotype gg exhibit the recessive characteristic.
For example, in humans,
brown eyes are a dominant characteristic and blue eyes are the correspond-
ing recessive characteristic; thus the oÔ¨Äspring with genotypes GG or Gg are
brown-eyed, whereas those of type gg are blue-eyed.
Let us consider the probability of oÔ¨Äspring of each genotype for a male
parent of genotype Gg. (We assume that the population under consideration
is large, that mating is random with respect to genotype, and that the distri-
bution of each genotype within the population is independent of sex and life
expectancy.) Let
P =
‚éõ
‚éù
p
q
r
‚éû
‚é†
denote the proportion of the adult population with genotypes GG, Gg, and
gg, respectively, at the start of the experiment. This experiment describes a
three-state Markov chain with the following transition matrix:
Genotype of female parent
GG
Gg
gg
Genotype
GG
of
Gg
oÔ¨Äspring
gg
‚éõ
‚éú
‚éú
‚éù
1
2
1
4
0
1
2
1
2
1
2
0
1
4
1
2
‚éû
‚éü
‚éü
‚é†= B.
It is easily checked that B2 contains only positive entries; so B is regular.
Thus, by permitting only males of genotype Gg to reproduce, the proportion
of oÔ¨Äspring in the population having a certain genotype will stabilize at the
Ô¨Åxed probability vector for B, which is
‚éõ
‚éú
‚éú
‚éù
1
4
1
2
1
4
‚éû
‚éü
‚éü
‚é†.

306
Chap. 5
Diagonalization
Now suppose that similar experiments are to be performed with males of
genotypes GG and gg. As already mentioned, these experiments are three-
state Markov chains with transition matrices
A =
‚éõ
‚éú
‚éù
1
1
2
0
0
1
2
1
0
0
0
‚éû
‚éü
‚é†
and
C =
‚éõ
‚éú
‚éù
0
0
0
1
1
2
0
0
1
2
1
‚éû
‚éü
‚é†,
respectively. In order to consider the case where all male genotypes are per-
mitted to reproduce, we must form the transition matrix M = pA+qB +rC,
which is the linear combination of A, B, and C weighted by the proportion
of males of each genotype. Thus
M =
‚éõ
‚éú
‚éú
‚éù
p + 1
2q
1
2p + 1
4q
0
1
2q + r
1
2p + 1
2q + 1
2r
p + 1
2q
0
1
4q + 1
2r
1
2q + r
‚éû
‚éü
‚éü
‚é†.
To simplify the notation, let a = p + 1
2q and b = 1
2q + r. (The numbers a and
b represent the proportions of G and g genes, respectively, in the population.)
Then
M =
‚éõ
‚éú
‚éú
‚éù
a
1
2a
0
b
1
2
a
0
1
2b
b
‚éû
‚éü
‚éü
‚é†,
where a + b = p + q + r = 1.
Let p‚Ä≤, q‚Ä≤, and r‚Ä≤ denote the proportions of the Ô¨Årst-generation oÔ¨Äspring
having genotypes GG, Gg, and gg, respectively. Then
‚éõ
‚éù
p‚Ä≤
q‚Ä≤
r‚Ä≤
‚éû
‚é†= MP =
‚éõ
‚éú
‚éú
‚éù
ap + 1
2aq
bp + 1
2q + ar
1
2bq + br
‚éû
‚éü
‚éü
‚é†=
‚éõ
‚éù
a2
2ab
b2
‚éû
‚é†.
In order to consider the eÔ¨Äects of unrestricted matings among the Ô¨Årst-
generation oÔ¨Äspring, a new transition matrix 0
M must be determined based
upon the distribution of Ô¨Årst-generation genotypes. As before, we Ô¨Ånd that
0
M =
‚éõ
‚éú
‚éú
‚éù
p‚Ä≤ + 1
2q‚Ä≤
1
2p‚Ä≤ + 1
4q‚Ä≤
0
1
2q‚Ä≤ + r‚Ä≤
1
2p‚Ä≤ + 1
2q‚Ä≤ + 1
2r‚Ä≤
p‚Ä≤ + 1
2q‚Ä≤
0
1
4q‚Ä≤ + 1
2r‚Ä≤
1
2q‚Ä≤ + r‚Ä≤
‚éû
‚éü
‚éü
‚é†=
‚éõ
‚éú
‚éú
‚éù
a‚Ä≤
1
2a‚Ä≤
0
b‚Ä≤
1
2
a‚Ä≤
0
1
2b‚Ä≤
b‚Ä≤
‚éû
‚éü
‚éü
‚é†,

Sec. 5.3
Matrix Limits and Markov Chains
307
where a‚Ä≤ = p‚Ä≤ + 1
2q‚Ä≤ and b‚Ä≤ = 1
2q‚Ä≤ + r‚Ä≤. However
a‚Ä≤ = a2 + 1
2(2ab) = a(a + b) = a
and
b‚Ä≤ = 1
2(2ab) + b2 = b(a + b) = b.
Thus 0
M = M; so the distribution of second-generation oÔ¨Äspring among
the three genotypes is
0
M(MP) = M 2P =
‚éõ
‚éù
a3 + a2b
a2b + ab + ab2
ab2 + b3
‚éû
‚é†=
‚éõ
‚éù
a2(a + b)
ab(a + 1 + b)
b2(a + b)
‚éû
‚é†=
‚éõ
‚éù
a2
2ab
b2
‚éû
‚é†
= MP,
the same as the Ô¨Årst-generation oÔ¨Äspring. In other words, MP is the Ô¨Åxed
probability vector for M, and genetic equilibrium is achieved in the population
after only one generation. (This result is called the Hardy‚ÄìWeinberg law.)
Notice that in the important special case that a = b (or equivalently, that
p = r), the distribution at equilibrium is
MP =
‚éõ
‚éù
a2
2ab
b2
‚éû
‚é†=
‚éõ
‚éú
‚éú
‚éù
1
4
1
2
1
4
‚éû
‚éü
‚éü
‚é†.
EXERCISES
1.
Label the following statements as true or false.
(a)
If A ‚ààMn√ón(C) and lim
m‚Üí‚àûAm = L, then, for any invertible matrix
Q ‚ààMn√ón(C), we have lim
m‚Üí‚àûQAmQ‚àí1 = QLQ‚àí1.
(b)
If 2 is an eigenvalue of A ‚ààMn√ón(C), then
lim
m‚Üí‚àûAm does not
exist.
(c)
Any vector
‚éõ
‚éú
‚éú
‚éú
‚éù
x1
x2
...
xn
‚éû
‚éü
‚éü
‚éü
‚é†‚ààRn
such that x1 + x2 + ¬∑ ¬∑ ¬∑ + xn = 1 is a probability vector.
(d)
The sum of the entries of each row of a transition matrix equals 1.
(e)
The product of a transition matrix and a probability vector is a
probability vector.

308
Chap. 5
Diagonalization
(f)
Let z be any complex number such that |z| < 1. Then the matrix
‚éõ
‚éù
1
z
‚àí1
z
1
1
‚àí1
1
z
‚éû
‚é†
does not have 3 as an eigenvalue.
(g)
Every transition matrix has 1 as an eigenvalue.
(h)
No transition matrix can have ‚àí1 as an eigenvalue.
(i)
If A is a transition matrix, then lim
m‚Üí‚àûAm exists.
(j)
If A is a regular transition matrix, then
lim
m‚Üí‚àûAm exists and has
rank 1.
2.
Determine whether
lim
m‚Üí‚àûAm exists for each of the following matrices
A, and compute the limit if it exists.
(a)

0.1
0.7
0.7
0.1
	
(b)

‚àí1.4
0.8
‚àí2.4
1.8
	
(c)

0.4
0.7
0.6
0.3
	
(d)

‚àí1.8
4.8
‚àí0.8
2.2
	
(e)

‚àí2
‚àí1
4
3
	
(f)

2.0
‚àí0.5
3.0
‚àí0.5
	
(g)
‚éõ
‚éù
‚àí1.8
0
‚àí1.4
‚àí5.6
1
‚àí2.8
2.8
0
2.4
‚éû
‚é†
(h)
‚éõ
‚éù
3.4
‚àí0.2
0.8
3.9
1.8
1.3
‚àí16.5
‚àí2.0
‚àí4.5
‚éû
‚é†
(i)
‚éõ
‚éú
‚éù
‚àí1
2 ‚àí2i
4i
1
2 + 5i
1 + 2i
‚àí3i
‚àí1 ‚àí4i
‚àí1 ‚àí2i
4i
1 + 5i
‚éû
‚éü
‚é†
(j)
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
‚àí26 + i
3
‚àí28 ‚àí4i
3
28
‚àí7 + 2i
3
‚àí5 + i
3
7 ‚àí2i
‚àí13 + 6i
6
‚àí5 + 6i
6
35 ‚àí20i
6
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
3.
Prove that if A1, A2, . . . is a sequence of n √ó p matrices with complex
entries such that lim
m‚Üí‚àûAm = L, then lim
m‚Üí‚àû(Am)t = Lt.
4.
Prove that if A ‚ààMn√ón(C) is diagonalizable and L = lim
m‚Üí‚àûAm exists,
then either L = In or rank(L) < n.

Sec. 5.3
Matrix Limits and Markov Chains
309
5.
Find 2 √ó 2 matrices A and B having real entries such that
lim
m‚Üí‚àûAm,
lim
m‚Üí‚àûBm, and lim
m‚Üí‚àû(AB)m all exist, but
lim
m‚Üí‚àû(AB)m Ã∏= ( lim
m‚Üí‚àûAm)( lim
m‚Üí‚àûBm).
6.
A hospital trauma unit has determined that 30% of its patients are
ambulatory and 70% are bedridden at the time of arrival at the hospital.
A month after arrival, 60% of the ambulatory patients have recovered,
20% remain ambulatory, and 20% have become bedridden. After the
same amount of time, 10% of the bedridden patients have recovered,
20% have become ambulatory, 50% remain bedridden, and 20% have
died. Determine the percentages of patients who have recovered, are
ambulatory, are bedridden, and have died 1 month after arrival. Also
determine the eventual percentages of patients of each type.
7.
A player begins a game of chance by placing a marker in box 2, marked
Start. (See Figure 5.5.) A die is rolled, and the marker is moved one
square to the left if a 1 or a 2 is rolled and one square to the right if a
3, 4, 5, or 6 is rolled. This process continues until the marker lands in
square 1, in which case the player wins the game, or in square 4, in which
case the player loses the game. What is the probability of winning this
game? Hint: Instead of diagonalizing the appropriate transition matrix
Win
Start
Lose
1
2
3
4
Figure 5.5
A, it is easier to represent e2 as a linear combination of eigenvectors of
A and then apply An to the result.
8.
Which of the following transition matrices are regular?
(a)
‚éõ
‚éù
0.2
0.3
0.5
0.3
0.2
0.5
0.5
0.5
0
‚éû
‚é†
(b)
‚éõ
‚éù
0.5
0
1
0.5
0
0
0
1
0
‚éû
‚é†
(c)
‚éõ
‚éù
0.5
0
0
0.5
0
1
0
1
0
‚éû
‚é†
(d)
‚éõ
‚éù
0.5
0
1
0.5
1
0
0
0
0
‚éû
‚é†
(e)
‚éõ
‚éú
‚éú
‚éù
1
3
0
0
1
3
1
0
1
3
0
1
‚éû
‚éü
‚éü
‚é†
(f)
‚éõ
‚éù
1
0
0
0
0.7
0.2
0
0.3
0.8
‚éû
‚é†

310
Chap. 5
Diagonalization
(g)
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
0
1
2
0
0
1
2
0
0
0
1
4
1
4
1
0
1
4
1
4
0
1
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
(h)
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
1
4
1
4
0
0
1
4
1
4
0
0
1
4
1
4
1
0
1
4
1
4
0
1
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
9.
Compute lim
m‚Üí‚àûAm if it exists, for each matrix A in Exercise 8.
10.
Each of the matrices that follow is a regular transition matrix for a
three-state Markov chain. In all cases, the initial probability vector is
P =
‚éõ
‚éù
0.3
0.3
0.4
‚éû
‚é†.
For each transition matrix, compute the proportions of objects in each
state after two stages and the eventual proportions of objects in each
state by determining the Ô¨Åxed probability vector.
(a)
‚éõ
‚éù
0.6
0.1
0.1
0.1
0.9
0.2
0.3
0
0.7
‚éû
‚é†
(b)
‚éõ
‚éù
0.8
0.1
0.2
0.1
0.8
0.2
0.1
0.1
0.6
‚éû
‚é†
(c)
‚éõ
‚éù
0.9
0.1
0.1
0.1
0.6
0.1
0
0.3
0.8
‚éû
‚é†
(d)
‚éõ
‚éù
0.4
0.2
0.2
0.1
0.7
0.2
0.5
0.1
0.6
‚éû
‚é†
(e)
‚éõ
‚éù
0.5
0.3
0.2
0.2
0.5
0.3
0.3
0.2
0.5
‚éû
‚é†
(f)
‚éõ
‚éù
0.6
0
0.4
0.2
0.8
0.2
0.2
0.2
0.4
‚éû
‚é†
11.
In 1940, a county land-use survey showed that 10% of the county land
was urban, 50% was unused, and 40% was agricultural. Five years later,
a follow-up survey revealed that 70% of the urban land had remained
urban, 10% had become unused, and 20% had become agricultural.
Likewise, 20% of the unused land had become urban, 60% had remained
unused, and 20% had become agricultural.
Finally, the 1945 survey
showed that 20% of the agricultural land had become unused while
80% remained agricultural. Assuming that the trends indicated by the
1945 survey continue, compute the percentages of urban, unused, and
agricultural land in the county in 1950 and the corresponding eventual
percentages.
12.
A diaper liner is placed in each diaper worn by a baby.
If, after a
diaper change, the liner is soiled, then it is discarded and replaced by a
new liner. Otherwise, the liner is washed with the diapers and reused,
except that each liner is discarded and replaced after its third use (even
if it has never been soiled). The probability that the baby will soil any
diaper liner is one-third. If there are only new diaper liners at Ô¨Årst,
eventually what proportions of the diaper liners being used will be new,

Sec. 5.3
Matrix Limits and Markov Chains
311
once used, and twice used? Hint: Assume that a diaper liner ready for
use is in one of three states: new, once used, and twice used. After its
use, it then transforms into one of the three states described.
13.
In 1975, the automobile industry determined that 40% of American car
owners drove large cars, 20% drove intermediate-sized cars, and 40%
drove small cars. A second survey in 1985 showed that 70% of the large-
car owners in 1975 still owned large cars in 1985, but 30% had changed
to an intermediate-sized car. Of those who owned intermediate-sized
cars in 1975, 10% had switched to large cars, 70% continued to drive
intermediate-sized cars, and 20% had changed to small cars in 1985.
Finally, of the small-car owners in 1975, 10% owned intermediate-sized
cars and 90% owned small cars in 1985. Assuming that these trends
continue, determine the percentages of Americans who own cars of each
size in 1995 and the corresponding eventual percentages.
14.
Show that if A and P are as in Example 5, then
Am =
‚éõ
‚éù
rm
rm+1
rm+1
rm+1
rm
rm+1
rm+1
rm+1
rm
‚éû
‚é†,
where
rm = 1
3
$
1 + (‚àí1)m
2m‚àí1
%
.
Deduce that
600(AmP) = Am
‚éõ
‚éù
300
200
100
‚éû
‚é†=
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
200 + (‚àí1)m
2m
(100)
200
200 + (‚àí1)m+1
2m
(100)
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
.
15.
Prove that if a 1-dimensional subspace W of Rn contains a nonzero vec-
tor with all nonnegative entries, then W contains a unique probability
vector.
16.
Prove Theorem 5.15 and its corollary.
17.
Prove the two corollaries of Theorem 5.18.
18.
Prove the corollary of Theorem 5.19.
19.
Suppose that M and M ‚Ä≤ are n √ó n transition matrices.

312
Chap. 5
Diagonalization
(a)
Prove that if M is regular, N is any n √ó n transition matrix, and
c is a real number such that 0 < c ‚â§1, then cM + (1 ‚àíc)N is a
regular transition matrix.
(b)
Suppose that for all i, j, we have that M ‚Ä≤
ij > 0 whenever Mij > 0.
Prove that there exists a transition matrix N and a real number c
with 0 < c ‚â§1 such that M ‚Ä≤ = cM + (1 ‚àíc)N.
(c)
Deduce that if the nonzero entries of M and M ‚Ä≤ occur in the same
positions, then M is regular if and only if M ‚Ä≤ is regular.
The following deÔ¨Ånition is used in Exercises 20‚Äì24.
DeÔ¨Ånition. For A ‚ààMn√ón(C), deÔ¨Åne eA = lim
m‚Üí‚àûBm, where
Bm = I + A + A2
2! + ¬∑ ¬∑ ¬∑ + Am
m!
(see Exercise 22). Thus eA is the sum of the inÔ¨Ånite series
I + A + A2
2! + A3
3! + ¬∑ ¬∑ ¬∑ ,
and Bm is the mth partial sum of this series. (Note the analogy with the
power series
ea = 1 + a + a2
2! + a3
3! + ¬∑ ¬∑ ¬∑ ,
which is valid for all complex numbers a.)
20.
Compute eO and eI, where O and I denote the n √ó n zero and identity
matrices, respectively.
21.
Let P ‚àí1AP = D be a diagonal matrix. Prove that eA = PeDP ‚àí1.
22.
Let A ‚ààMn√ón(C) be diagonalizable. Use the result of Exercise 21 to
show that eA exists. (Exercise 21 of Section 7.2 shows that eA exists
for every A ‚ààMn√ón(C).)
23.
Find A, B ‚ààM2√ó2(R) such that eAeB Ã∏= eA+B.
24.
Prove that a diÔ¨Äerentiable function x: R ‚ÜíRn is a solution to the
system of diÔ¨Äerential equations deÔ¨Åned in Exercise 15 of Section 5.2 if
and only if x(t) = etAv for some v ‚ààRn, where A is deÔ¨Åned in that
exercise.

Sec. 5.4
Invariant Subspaces and the Cayley‚ÄìHamilton Theorem
313
5.4
INVARIANT SUBSPACES AND THE CAYLEY‚ÄìHAMILTON
THEOREM
In Section 5.1, we observed that if v is an eigenvector of a linear operator
T, then T maps the span of {v} into itself. Subspaces that are mapped into
themselves are of great importance in the study of linear operators (see, e.g.,
Exercises 28‚Äì32 of Section 2.1).
DeÔ¨Ånition. Let T be a linear operator on a vector space V. A subspace
W of V is called a T-invariant subspace of V if T(W) ‚äÜW, that is, if
T(v) ‚ààW for all v ‚ààW.
Example 1
Suppose that T is a linear operator on a vector space V. Then the following
subspaces of V are T-invariant:
1. {0}
2. V
3. R(T)
4. N(T)
5. EŒª, for any eigenvalue Œª of T.
The proofs that these subspaces are T-invariant are left as exercises. (See
Exercise 3.)
‚ô¶
Example 2
Let T be the linear operator on R3 deÔ¨Åned by
T(a, b, c) = (a + b, b + c, 0).
Then the xy-plane = {(x, y, 0): x, y ‚ààR} and the x-axis = {(x, 0, 0): x ‚ààR}
are T-invariant subspaces of R3.
‚ô¶
Let T be a linear operator on a vector space V, and let x be a nonzero
vector in V. The subspace
W = span({x, T(x), T2(x), . . .})
is called the T-cyclic subspace of V generated by x. It is a simple matter
to show that W is T-invariant. In fact, W is the ‚Äúsmallest‚Äù T-invariant sub-
space of V containing x. That is, any T-invariant subspace of V containing x
must also contain W (see Exercise 11). Cyclic subspaces have various uses.
We apply them in this section to establish the Cayley‚ÄìHamilton theorem. In
Exercise 31, we outline a method for using cyclic subspaces to compute the
characteristic polynomial of a linear operator without resorting to determi-
nants. Cyclic subspaces also play an important role in Chapter 7, where we
study matrix representations of nondiagonalizable linear operators.

314
Chap. 5
Diagonalization
Example 3
Let T be the linear operator on R3 deÔ¨Åned by
T(a, b, c) = (‚àíb + c, a + c, 3c).
We determine the T-cyclic subspace generated by e1 = (1, 0, 0). Since
T(e1) = T(1, 0, 0) = (0, 1, 0) = e2
and
T2(e1) = T(T(e1)) = T(e2) = (‚àí1, 0, 0) = ‚àíe1,
it follows that
span({e1, T(e1), T2(e1), . . .}) = span({e1, e2}) = {(s, t, 0): s, t ‚ààR}.
‚ô¶
Example 4
Let T be the linear operator on P(R) deÔ¨Åned by T(f(x)) = f ‚Ä≤(x). Then the
T-cyclic subspace generated by x2 is span({x2, 2x, 2}) = P2(R).
‚ô¶
The existence of a T-invariant subspace provides the opportunity to deÔ¨Åne
a new linear operator whose domain is this subspace. If T is a linear operator
on V and W is a T-invariant subspace of V, then the restriction TW of T to
W (see Appendix B) is a mapping from W to W, and it follows that TW is
a linear operator on W (see Exercise 7). As a linear operator, TW inherits
certain properties from its parent operator T. The following result illustrates
one way in which the two operators are linked.
Theorem 5.21. Let T be a linear operator on a Ô¨Ånite-dimensional vector
space V, and let W be a T-invariant subspace of V. Then the characteristic
polynomial of TW divides the characteristic polynomial of T.
Proof. Choose an ordered basis Œ≥ = {v1, v2, . . . , vk} for W, and extend it
to an ordered basis Œ≤ = {v1, v2, . . . , vk, vk+1, . . . , vn} for V. Let A = [T]Œ≤ and
B1 = [TW]Œ≥. Then, by Exercise 12, A can be written in the form
A =

B1
B2
O
B3
	
.
Let f(t) be the characteristic polynomial of T and g(t) the characteristic
polynomial of TW. Then
f(t) = det(A ‚àítIn) = det

B1 ‚àítIk
B2
O
B3 ‚àítIn‚àík
	
= g(t)¬∑ det(B3 ‚àítIn‚àík)
by Exercise 21 of Section 4.3. Thus g(t) divides f(t).

Sec. 5.4
Invariant Subspaces and the Cayley‚ÄìHamilton Theorem
315
Example 5
Let T be the linear operator on R4 deÔ¨Åned by
T(a, b, c, d) = (a + b + 2c ‚àíd, b + d, 2c ‚àíd, c + d),
and let W = {(t, s, 0, 0): t, s ‚ààR}. Observe that W is a T-invariant subspace
of R4 because, for any vector (a, b, 0, 0) ‚ààR4,
T(a, b, 0, 0) = (a + b, b, 0, 0) ‚ààW.
Let Œ≥ = {e1, e2}, which is an ordered basis for W. Extend Œ≥ to the standard
ordered basis Œ≤ for R4. Then
B1 = [TW]Œ≥ =

1
1
0
1
	
and
A = [T]Œ≤ =
‚éõ
‚éú
‚éú
‚éù
1
1
2
‚àí1
0
1
0
1
0
0
2
‚àí1
0
0
1
1
‚éû
‚éü
‚éü
‚é†
in the notation of Theorem 5.21. Let f(t) be the characteristic polynomial of
T and g(t) be the characteristic polynomial of TW. Then
f(t) = det(A ‚àítI4) = det
‚éõ
‚éú
‚éú
‚éù
1 ‚àít
1
2
‚àí1
0
1 ‚àít
0
1
0
0
2 ‚àít
‚àí1
0
0
1
1 ‚àít
‚éû
‚éü
‚éü
‚é†
= det

1 ‚àít
1
0
1 ‚àít
	
¬∑ det

2 ‚àít
‚àí1
1
1 ‚àít
	
= g(t)¬∑ det

2 ‚àít
‚àí1
1
1 ‚àít
	
.
‚ô¶
In view of Theorem 5.21, we may use the characteristic polynomial of TW
to gain information about the characteristic polynomial of T itself. In this re-
gard, cyclic subspaces are useful because the characteristic polynomial of the
restriction of a linear operator T to a cyclic subspace is readily computable.
Theorem 5.22. Let T be a linear operator on a Ô¨Ånite-dimensional vector
space V, and let W denote the T-cyclic subspace of V generated by a nonzero
vector v ‚ààV. Let k = dim(W). Then
(a) {v, T(v), T2(v), . . . , Tk‚àí1(v)} is a basis for W.
(b) If a0v +a1T(v)+¬∑ ¬∑ ¬∑+ak‚àí1Tk‚àí1(v)+Tk(v) = 0, then the characteristic
polynomial of TW is f(t) = (‚àí1)k(a0 + a1t + ¬∑ ¬∑ ¬∑ + ak‚àí1tk‚àí1 + tk).

316
Chap. 5
Diagonalization
Proof. (a) Since v Ã∏= 0, the set {v} is linearly independent. Let j be the
largest positive integer for which
Œ≤ = {v, T(v), . . . , Tj‚àí1(v)}
is linearly independent. Such a j must exist because V is Ô¨Ånite-dimensional.
Let Z = span(Œ≤).
Then Œ≤ is a basis for Z.
Furthermore, Tj(v) ‚ààZ by
Theorem 1.7 (p. 39). We use this information to show that Z is a T-invariant
subspace of V. Let w ‚ààZ. Since w is a linear combination of the vectors of
Œ≤, there exist scalars b0, b1, . . . , bj‚àí1 such that
w = b0v + b1T(v) + ¬∑ ¬∑ ¬∑ + bj‚àí1Tj‚àí1(v),
and hence
T(w) = b0T(v) + b1T2(v) + ¬∑ ¬∑ ¬∑ + bj‚àí1Tj(v).
Thus T(w) is a linear combination of vectors in Z, and hence belongs to Z.
So Z is T-invariant. Furthermore, v ‚ààZ. By Exercise 11, W is the smallest
T-invariant subspace of V that contains v, so that W ‚äÜZ. Clearly, Z ‚äÜW,
and so we conclude that Z = W. It follows that Œ≤ is a basis for W, and
therefore dim(W) = j. Thus j = k. This proves (a).
(b) Now view Œ≤ (from (a)) as an ordered basis for W. Let a0, a1, . . . , ak‚àí1
be the scalars such that
a0v + a1T(v) + ¬∑ ¬∑ ¬∑ + ak‚àí1Tk‚àí1(v) + Tk(v) = 0.
Observe that
[TW]Œ≤ =
‚éõ
‚éú
‚éú
‚éú
‚éù
0
0
¬∑ ¬∑ ¬∑
0
‚àía0
1
0
¬∑ ¬∑ ¬∑
0
‚àía1
...
...
...
...
0
0
¬∑ ¬∑ ¬∑
1
‚àíak‚àí1
‚éû
‚éü
‚éü
‚éü
‚é†,
which has the characteristic polynomial
f(t) = (‚àí1)k(a0 + a1t + ¬∑ ¬∑ ¬∑ + ak‚àí1tk‚àí1 + tk)
by Exercise 19. Thus f(t) is the characteristic polynomial of TW, proving (b).
Example 6
Let T be the linear operator of Example 3, and let W = span({e1, e2}), the
T-cyclic subspace generated by e1. We compute the characteristic polyno-
mial f(t) of TW in two ways: by means of Theorem 5.22 and by means of
determinants.

Sec. 5.4
Invariant Subspaces and the Cayley‚ÄìHamilton Theorem
317
(a) By means of Theorem 5.22. From Example 3, we have that {e1, e2} is
a cycle that generates W, and that T2(e1) = ‚àíe1. Hence
1e1 + 0T(e1) + T2(e1) = 0.
Therefore, by Theorem 5.22(b),
f(t) = (‚àí1)2(1 + 0t + t2) = t2 + 1.
(b) By means of determinants. Let Œ≤ = {e1, e2}, which is an ordered basis
for W. Since T(e1) = e2 and T(e2) = ‚àíe1, we have
[TW]Œ≤ =

0
‚àí1
1
0
	
and therefore,
f(t) = det

‚àít
‚àí1
1
‚àít
	
= t2 + 1.
‚ô¶
The Cayley‚ÄìHamilton Theorem
As an illustration of the importance of Theorem 5.22, we prove a well-
known result that is used in Chapter 7.
The reader should refer to Ap-
pendix E for the deÔ¨Ånition of f(T), where T is a linear operator and f(x) is
a polynomial.
Theorem 5.23 (Cayley‚ÄìHamilton).
Let T be a linear operator on a
Ô¨Ånite-dimensional vector space V, and let f(t) be the characteristic polyno-
mial of T. Then f(T) = T0, the zero transformation. That is, T ‚ÄúsatisÔ¨Åes‚Äù
its characteristic equation.
Proof. We show that f(T)(v) = 0 for all v ‚ààV. This is obvious if v = 0
because f(T) is linear; so suppose that v Ã∏= 0. Let W be the T-cyclic subspace
generated by v, and suppose that dim(W) = k. By Theorem 5.22(a), there
exist scalars a0, a1, . . . , ak‚àí1 such that
a0v + a1T(v) + ¬∑ ¬∑ ¬∑ + ak‚àí1Tk‚àí1(v) + Tk(v) = 0.
Hence Theorem 5.22(b) implies that
g(t) = (‚àí1)k(a0 + a1t + ¬∑ ¬∑ ¬∑ + ak‚àí1tk‚àí1 + tk)
is the characteristic polynomial of TW. Combining these two equations yields
g(T)(v) = (‚àí1)k(a0I + a1T + ¬∑ ¬∑ ¬∑ + ak‚àí1Tk‚àí1 + Tk)(v) = 0.
By Theorem 5.21, g(t) divides f(t); hence there exists a polynomial q(t) such
that f(t) = q(t)g(t). So
f(T)(v) = q(T)g(T)(v) = q(T)(g(T)(v)) = q(T)(0) = 0.

318
Chap. 5
Diagonalization
Example 7
Let T be the linear operator on R2 deÔ¨Åned by T(a, b) = (a+2b, ‚àí2a+b), and
let Œ≤ = {e1, e2}. Then
A =

1
2
‚àí2
1
	
,
where A = [T]Œ≤. The characteristic polynomial of T is, therefore,
f(t) = det(A ‚àítI) = det

1 ‚àít
2
‚àí2
1 ‚àít
	
= t2 ‚àí2t + 5.
It is easily veriÔ¨Åed that T0 = f(T) = T2 ‚àí2T + 5I. Similarly,
f(A) = A2 ‚àí2A + 5I =

‚àí3
4
‚àí4
‚àí3
	
+

‚àí2
‚àí4
4
‚àí2
	
+

5
0
0
5
	
=

0
0
0
0
	
.
‚ô¶
Example 7 suggests the following result.
Corollary (Cayley‚ÄìHamilton Theorem for Matrices).
Let A be
an n √ó n matrix, and let f(t) be the characteristic polynomial of A. Then
f(A) = O, the n √ó n zero matrix.
Proof. See Exercise 15.
Invariant Subspaces and Direct Sums*3
It is useful to decompose a Ô¨Ånite-dimensional vector space V into a direct
sum of as many T-invariant subspaces as possible because the behavior of T
on V can be inferred from its behavior on the direct summands. For example,
T is diagonalizable if and only if V can be decomposed into a direct sum
of one-dimensional T-invariant subspaces (see Exercise 36). In Chapter 7,
we consider alternate ways of decomposing V into direct sums of T-invariant
subspaces if T is not diagonalizable. We proceed to gather a few facts about
direct sums of T-invariant subspaces that are used in Section 7.4. The Ô¨Årst
of these facts is about characteristic polynomials.
Theorem 5.24. Let T be a linear operator on a Ô¨Ånite-dimensional vector
space V, and suppose that V = W1 ‚äïW2 ‚äï¬∑ ¬∑ ¬∑ ‚äïWk, where Wi is a T-
invariant subspace of V for each i (1 ‚â§i ‚â§k). Suppose that fi(t) is the
characteristic polynomial of TWi (1 ‚â§i ‚â§k). Then f1(t)¬∑f2(t)¬∑ ¬∑ ¬∑ ¬∑ ¬∑fk(t) is
the characteristic polynomial of T.
3This subsection uses optional material on direct sums from Section 5.2.

Sec. 5.4
Invariant Subspaces and the Cayley‚ÄìHamilton Theorem
319
Proof. The proof is by mathematical induction on k. In what follows, f(t)
denotes the characteristic polynomial of T. Suppose Ô¨Årst that k = 2. Let Œ≤1
be an ordered basis for W1, Œ≤2 an ordered basis for W2, and Œ≤ = Œ≤1 ‚à™Œ≤2.
Then Œ≤ is an ordered basis for V by Theorem 5.10(d) (p. 276). Let A = [T]Œ≤,
B1 = [TW1]Œ≤1, and B2 = [TW2]Œ≤2. By Exercise 34, it follows that
A =

B1
O
O‚Ä≤
B2
	
,
where O and O‚Ä≤ are zero matrices of the appropriate sizes. Then
f(t) = det(A ‚àítI) = det(B1 ‚àítI)¬∑ det(B2 ‚àítI) = f1(t)¬∑f2(t)
as in the proof of Theorem 5.21, proving the result for k = 2.
Now assume that the theorem is valid for k‚àí1 summands, where k‚àí1 ‚â•2,
and suppose that V is a direct sum of k subspaces, say,
V = W1 ‚äïW2 ‚äï¬∑ ¬∑ ¬∑ ‚äïWk.
Let W = W1 +W2 +¬∑ ¬∑ ¬∑+Wk‚àí1. It is easily veriÔ¨Åed that W is T-invariant and
that V = W ‚äïWk. So by the case for k = 2, f(t) = g(t)¬∑fk(t), where g(t) is
the characteristic polynomial of TW. Clearly W = W1 ‚äïW2 ‚äï¬∑ ¬∑ ¬∑‚äïWk‚àí1, and
therefore g(t) = f1(t)¬∑f2(t)¬∑ ¬∑ ¬∑ ¬∑ ¬∑fk‚àí1(t) by the induction hypothesis. We
conclude that f(t) = g(t)¬∑fk(t) = f1(t)¬∑f2(t)¬∑ ¬∑ ¬∑ ¬∑ ¬∑fk(t).
As an illustration of this result, suppose that T is a diagonalizable lin-
ear operator on a Ô¨Ånite-dimensional vector space V with distinct eigenvalues
Œª1, Œª2, . . . , Œªk. By Theorem 5.11 (p. 278), V is a direct sum of the eigenspaces
of T. Since each eigenspace is T-invariant, we may view this situation in the
context of Theorem 5.24. For each eigenvalue Œªi, the restriction of T to EŒªi
has characteristic polynomial (Œªi ‚àít)mi, where mi is the dimension of EŒªi.
By Theorem 5.24, the characteristic polynomial f(t) of T is the product
f(t) = (Œª1 ‚àít)m1(Œª2 ‚àít)m2 ¬∑ ¬∑ ¬∑ (Œªk ‚àít)mk.
It follows that the multiplicity of each eigenvalue is equal to the dimension
of the corresponding eigenspace, as expected.
Example 8
Let T be the linear operator on R4 deÔ¨Åned by
T(a, b, c, d) = (2a ‚àíb, a + b, c ‚àíd, c + d),
and let W1 = {(s, t, 0, 0): s, t ‚ààR} and W2 = {(0, 0, s, t): s, t ‚ààR}. Notice
that W1 and W2 are each T-invariant and that R4 = W1 ‚äïW2. Let Œ≤1 =
{e1, e2}, Œ≤2 = {e3, e4}, and Œ≤ = Œ≤1 ‚à™Œ≤2 = {e1, e2, e3, e4}. Then Œ≤1 is an

320
Chap. 5
Diagonalization
ordered basis for W1, Œ≤2 is an ordered basis for W2, and Œ≤ is an ordered basis
for R4. Let A = [T]Œ≤, B1 = [TW1]Œ≤1, and B2 = [TW2]Œ≤2. Then
B1 =

2
‚àí1
1
1
	
,
B2 =

1
‚àí1
1
1
	
,
and
A =

B1
O
O
B2
	
=
‚éõ
‚éú
‚éú
‚éù
2
‚àí1
0
0
1
1
0
0
0
0
1
‚àí1
0
0
1
1
‚éû
‚éü
‚éü
‚é†.
Let f(t), f1(t), and f2(t) denote the characteristic polynomials of T, TW1,
and TW2, respectively. Then
f(t) = det(A ‚àítI) = det(B1 ‚àítI)¬∑ det(B2 ‚àítI) = f1(t)¬∑f2(t).
‚ô¶
The matrix A in Example 8 can be obtained by joining the matrices B1
and B2 in the manner explained in the next deÔ¨Ånition.
DeÔ¨Ånition. Let B1 ‚ààMm√óm(F), and let B2 ‚ààMn√ón(F). We deÔ¨Åne the
direct sum of B1 and B2, denoted B1 ‚äïB2, as the (m+n)√ó(m+n) matrix
A such that
Aij =
‚éß
‚é™
‚é®
‚é™
‚é©
(B1)ij
for 1 ‚â§i, j ‚â§m
(B2)(i‚àím),(j‚àím)
for m + 1 ‚â§i, j ‚â§n + m
0
otherwise.
If B1, B2, . . . , Bk are square matrices with entries from F, then we deÔ¨Åne the
direct sum of B1, B2, . . . , Bk recursively by
B1 ‚äïB2 ‚äï¬∑ ¬∑ ¬∑ ‚äïBk = (B1 ‚äïB2 ‚äï¬∑ ¬∑ ¬∑ ‚äïBk‚àí1) ‚äïBk.
If A = B1 ‚äïB2 ‚äï¬∑ ¬∑ ¬∑ ‚äïBk, then we often write
A =
‚éõ
‚éú
‚éú
‚éú
‚éù
B1
O
¬∑ ¬∑ ¬∑
O
O
B2
¬∑ ¬∑ ¬∑
O
...
...
...
O
O
¬∑ ¬∑ ¬∑
Bk
‚éû
‚éü
‚éü
‚éü
‚é†.
Example 9
Let
B1 =

1
2
1
1
	
,
B2 = (3),
and
B3 =
‚éõ
‚éù
1
2
1
1
2
3
1
1
1
‚éû
‚é†.

Sec. 5.4
Invariant Subspaces and the Cayley‚ÄìHamilton Theorem
321
Then
B1 ‚äïB2 ‚äïB3 =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
1
2
0
0
0
0
1
1
0
0
0
0
0
0
3
0
0
0
0
0
0
1
2
1
0
0
0
1
2
3
0
0
0
1
1
1
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
.
‚ô¶
The Ô¨Ånal result of this section relates direct sums of matrices to direct
sums of invariant subspaces. It is an extension of Exercise 34 to the case
k ‚â•2.
Theorem 5.25. Let T be a linear operator on a Ô¨Ånite-dimensional vector
space V, and let W1, W2, . . . , Wk be T-invariant subspaces of V such that
V = W1 ‚äïW2 ‚äï¬∑ ¬∑ ¬∑ ‚äïWk. For each i, let Œ≤i be an ordered basis for Wi, and
let Œ≤ = Œ≤1 ‚à™Œ≤2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≤k. Let A = [T]Œ≤ and Bi = [TWi]Œ≤i for i = 1, 2, . . . , k.
Then A = B1 ‚äïB2 ‚äï¬∑ ¬∑ ¬∑ ‚äïBk.
Proof. See Exercise 35.
EXERCISES
1.
Label the following statements as true or false.
(a)
There exists a linear operator T with no T-invariant subspace.
(b)
If T is a linear operator on a Ô¨Ånite-dimensional vector space V and
W is a T-invariant subspace of V, then the characteristic polyno-
mial of TW divides the characteristic polynomial of T.
(c)
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V,
and let v and w be in V. If W is the T-cyclic subspace generated
by v, W‚Ä≤ is the T-cyclic subspace generated by w, and W = W‚Ä≤,
then v = w.
(d)
If T is a linear operator on a Ô¨Ånite-dimensional vector space V,
then for any v ‚ààV the T-cyclic subspace generated by v is the
same as the T-cyclic subspace generated by T(v).
(e)
Let T be a linear operator on an n-dimensional vector space. Then
there exists a polynomial g(t) of degree n such that g(T) = T0.
(f)
Any polynomial of degree n with leading coeÔ¨Écient (‚àí1)n is the
characteristic polynomial of some linear operator.
(g)
If T is a linear operator on a Ô¨Ånite-dimensional vector space V, and
if V is the direct sum of k T-invariant subspaces, then there is an
ordered basis Œ≤ for V such that [T]Œ≤ is a direct sum of k matrices.

322
Chap. 5
Diagonalization
2.
For each of the following linear operators T on the vector space V,
determine whether the given subspace W is a T-invariant subspace of
V.
(a)
V = P3(R), T(f(x)) = f ‚Ä≤(x), and W = P2(R)
(b)
V = P(R), T(f(x)) = xf(x), and W = P2(R)
(c)
V = R3, T(a, b, c) = (a + b + c, a + b + c, a + b + c), and
W = {(t, t, t): t ‚ààR}
(d)
V = C([0, 1]), T(f(t)) =
& 1
0 f(x) dx
'
t, and
W = {f ‚ààV: f(t) = at + b for some a and b}
(e)
V = M2√ó2(R), T(A) =

0
1
1
0
	
A, and W = {A ‚ààV: At = A}
3.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V. Prove
that the following subspaces are T-invariant.
(a)
{0} and V
(b)
N(T) and R(T)
(c)
EŒª, for any eigenvalue Œª of T
4.
Let T be a linear operator on a vector space V, and let W be a T-
invariant subspace of V. Prove that W is g(T)-invariant for any poly-
nomial g(t).
5.
Let T be a linear operator on a vector space V. Prove that the inter-
section of any collection of T-invariant subspaces of V is a T-invariant
subspace of V.
6.
For each linear operator T on the vector space V, Ô¨Ånd an ordered basis
for the T-cyclic subspace generated by the vector z.
(a)
V = R4, T(a, b, c, d) = (a + b, b ‚àíc, a + c, a + d), and z = e1.
(b)
V = P3(R), T(f(x)) = f ‚Ä≤‚Ä≤(x), and z = x3.
(c)
V = M2√ó2(R), T(A) = At, and z =

0
1
1
0
	
.
(d)
V = M2√ó2(R), T(A) =

1
1
2
2
	
A, and z =

0
1
1
0
	
.
7.
Prove that the restriction of a linear operator T to a T-invariant sub-
space is a linear operator on that subspace.
8.
Let T be a linear operator on a vector space with a T-invariant subspace
W. Prove that if v is an eigenvector of TW with corresponding eigenvalue
Œª, then the same is true for T.
9.
For each linear operator T and cyclic subspace W in Exercise 6, compute
the characteristic polynomial of TW in two ways, as in Example 6.

Sec. 5.4
Invariant Subspaces and the Cayley‚ÄìHamilton Theorem
323
10.
For each linear operator in Exercise 6, Ô¨Ånd the characteristic polynomial
f(t) of T, and verify that the characteristic polynomial of TW (computed
in Exercise 9) divides f(t).
11.
Let T be a linear operator on a vector space V, let v be a nonzero vector
in V, and let W be the T-cyclic subspace of V generated by v. Prove
that
(a)
W is T-invariant.
(b)
Any T-invariant subspace of V containing v also contains W.
12.
Prove that A =

B1
B2
O
B3
	
in the proof of Theorem 5.21.
13.
Let T be a linear operator on a vector space V, let v be a nonzero vector
in V, and let W be the T-cyclic subspace of V generated by v. For any
w ‚ààV, prove that w ‚ààW if and only if there exists a polynomial g(t)
such that w = g(T)(v).
14.
Prove that the polynomial g(t) of Exercise 13 can always be chosen so
that its degree is less than or equal to dim(W).
15.
Use the Cayley‚ÄìHamilton theorem (Theorem 5.23) to prove its corol-
lary for matrices. Warning: If f(t) = det(A ‚àítI) is the characteristic
polynomial of A, it is tempting to ‚Äúprove‚Äù that f(A) = O by saying
‚Äúf(A) = det(A ‚àíAI) = det(O) = 0.‚Äù But this argument is nonsense.
Why?
16.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V.
(a)
Prove that if the characteristic polynomial of T splits, then so
does the characteristic polynomial of the restriction of T to any
T-invariant subspace of V.
(b)
Deduce that if the characteristic polynomial of T splits, then any
nontrivial T-invariant subspace of V contains an eigenvector of T.
17.
Let A be an n √ó n matrix. Prove that
dim(span({In, A, A2, . . .})) ‚â§n.
18.
Let A be an n √ó n matrix with characteristic polynomial
f(t) = (‚àí1)ntn + an‚àí1tn‚àí1 + ¬∑ ¬∑ ¬∑ + a1t + a0.
(a)
Prove that A is invertible if and only if a0 Ã∏= 0.
(b)
Prove that if A is invertible, then
A‚àí1 = (‚àí1/a0)[(‚àí1)nAn‚àí1 + an‚àí1An‚àí2 + ¬∑ ¬∑ ¬∑ + a1In].

324
Chap. 5
Diagonalization
(c)
Use (b) to compute A‚àí1 for
A =
‚éõ
‚éù
1
2
1
0
2
3
0
0
‚àí1
‚éû
‚é†.
19.
Let A denote the k √ó k matrix
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
0
0
¬∑ ¬∑ ¬∑
0
‚àía0
1
0
¬∑ ¬∑ ¬∑
0
‚àía1
0
1
¬∑ ¬∑ ¬∑
0
‚àía2
...
...
...
...
0
0
¬∑ ¬∑ ¬∑
0
‚àíak‚àí2
0
0
¬∑ ¬∑ ¬∑
1
‚àíak‚àí1
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
,
where a0, a1, . . . , ak‚àí1 are arbitrary scalars. Prove that the character-
istic polynomial of A is
(‚àí1)k(a0 + a1t + ¬∑ ¬∑ ¬∑ + ak‚àí1tk‚àí1 + tk).
Hint: Use mathematical induction on k, expanding the determinant
along the Ô¨Årst row.
20.
Let T be a linear operator on a vector space V, and suppose that V is
a T-cyclic subspace of itself. Prove that if U is a linear operator on V,
then UT = TU if and only if U = g(T) for some polynomial g(t). Hint:
Suppose that V is generated by v. Choose g(t) according to Exercise 13
so that g(T)(v) = U(v).
21.
Let T be a linear operator on a two-dimensional vector space V. Prove
that either V is a T-cyclic subspace of itself or T = cI for some scalar c.
22.
Let T be a linear operator on a two-dimensional vector space V and
suppose that T Ã∏= cI for any scalar c.
Show that if U is any linear
operator on V such that UT = TU, then U = g(T) for some polynomial
g(t).
23.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V, and
let W be a T-invariant subspace of V. Suppose that v1, v2, . . . , vk are
eigenvectors of T corresponding to distinct eigenvalues. Prove that if
v1 +v2 +¬∑ ¬∑ ¬∑+vk is in W, then vi ‚ààW for all i. Hint: Use mathematical
induction on k.
24.
Prove that the restriction of a diagonalizable linear operator T to any
nontrivial T-invariant subspace is also diagonalizable. Hint: Use the
result of Exercise 23.

Sec. 5.4
Invariant Subspaces and the Cayley‚ÄìHamilton Theorem
325
25. (a)
Prove the converse to Exercise 18(a) of Section 5.2: If T and U
are diagonalizable linear operators on a Ô¨Ånite-dimensional vector
space V such that UT = TU, then T and U are simultaneously
diagonalizable. (See the deÔ¨Ånitions in the exercises of Section 5.2.)
Hint: For any eigenvalue Œª of T, show that EŒª is U-invariant, and
apply Exercise 24 to obtain a basis for EŒª of eigenvectors of U.
(b)
State and prove a matrix version of (a).
26.
Let T be a linear operator on an n-dimensional vector space V such that
T has n distinct eigenvalues. Prove that V is a T-cyclic subspace of itself.
Hint: Use Exercise 23 to Ô¨Ånd a vector v such that {v, T(v), . . . , Tn‚àí1(v)}
is linearly independent.
Exercises 27 through 32 require familiarity with quotient spaces as deÔ¨Åned
in Exercise 31 of Section 1.3. Before attempting these exercises, the reader
should Ô¨Årst review the other exercises treating quotient spaces: Exercise 35
of Section 1.6, Exercise 40 of Section 2.1, and Exercise 24 of Section 2.4.
For the purposes of Exercises 27 through 32, T is a Ô¨Åxed linear operator on
a Ô¨Ånite-dimensional vector space V, and W is a nonzero T-invariant subspace
of V. We require the following deÔ¨Ånition.
DeÔ¨Ånition.
Let T be a linear operator on a vector space V, and let W
be a T-invariant subspace of V. DeÔ¨Åne T: V/W ‚ÜíV/W by
T(v + W) = T(v) + W
for any v + W ‚ààV/W.
27. (a)
Prove that T is well deÔ¨Åned.
That is, show that T(v + W) =
T(v‚Ä≤ + W) whenever v + W = v‚Ä≤ + W.
(b)
Prove that T is a linear operator on V/W.
(c)
Let Œ∑: V ‚ÜíV/W be the linear transformation deÔ¨Åned in Exer-
cise 40 of Section 2.1 by Œ∑(v) = v + W. Show that the diagram of
Figure 5.6 commutes; that is, prove that Œ∑T = TŒ∑. (This exercise
does not require the assumption that V is Ô¨Ånite-dimensional.)
V
T
‚àí‚àí‚àí‚àí‚Üí
V
Œ∑
‚èê‚èê!
‚èê‚èê!Œ∑
V/W
T
‚àí‚àí‚àí‚àí‚ÜíV/W
Figure 5.6
28.
Let f(t), g(t), and h(t) be the characteristic polynomials of T, TW,
and T, respectively.
Prove that f(t) = g(t)h(t).
Hint:
Extend an
ordered basis Œ≥ = {v1, v2, . . . , vk} for W to an ordered basis Œ≤ =
{v1, v2, . . . , vk, vk+1, . . . , vn} for V.
Then show that the collection of

326
Chap. 5
Diagonalization
cosets Œ± = {vk+1 + W, vk+2 + W, . . . , vn + W} is an ordered basis for
V/W, and prove that
[T]Œ≤ =

B1
B2
O
B3
	
,
where B1 = [T]Œ≥ and B3 = [T]Œ±.
29.
Use the hint in Exercise 28 to prove that if T is diagonalizable, then so
is T.
30.
Prove that if both TW and T are diagonalizable and have no common
eigenvalues, then T is diagonalizable.
The results of Theorem 5.22 and Exercise 28 are useful in devising methods
for computing characteristic polynomials without the use of determinants.
This is illustrated in the next exercise.
31.
Let A =
‚éõ
‚éù
1
1
‚àí3
2
3
4
1
2
1
‚éû
‚é†, let T = LA, and let W be the cyclic subspace
of R3 generated by e1.
(a)
Use Theorem 5.22 to compute the characteristic polynomial of TW.
(b)
Show that {e2 + W} is a basis for R3/W, and use this fact to
compute the characteristic polynomial of T.
(c)
Use the results of (a) and (b) to Ô¨Ånd the characteristic polynomial
of A.
32.
Prove the converse to Exercise 9(a) of Section 5.2: If the characteristic
polynomial of T splits, then there is an ordered basis Œ≤ for V such
that [T]Œ≤ is an upper triangular matrix. Hints: Apply mathematical
induction to dim(V). First prove that T has an eigenvector v, let W =
span({v}), and apply the induction hypothesis to T: V/W ‚ÜíV/W.
Exercise 35(b) of Section 1.6 is helpful here.
Exercises 33 through 40 are concerned with direct sums.
33.
Let T be a linear operator on a vector space V, and let W1, W2, . . . , Wk
be T-invariant subspaces of V. Prove that W1 + W2 + ¬∑ ¬∑ ¬∑ + Wk is also
a T-invariant subspace of V.
34.
Give a direct proof of Theorem 5.25 for the case k = 2. (This result is
used in the proof of Theorem 5.24.)
35.
Prove Theorem 5.25. Hint: Begin with Exercise 34 and extend it using
mathematical induction on k, the number of subspaces.

Sec. 5.4
Invariant Subspaces and the Cayley‚ÄìHamilton Theorem
327
36.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V.
Prove that T is diagonalizable if and only if V is the direct sum of
one-dimensional T-invariant subspaces.
37.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V,
and let W1, W2, . . . , Wk be T-invariant subspaces of V such that V =
W1 ‚äïW2 ‚äï¬∑ ¬∑ ¬∑ ‚äïWk. Prove that
det(T) = det(TW1) det(TW2) ¬∑ ¬∑ ¬∑ det(TWk).
38.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V,
and let W1, W2, . . . , Wk be T-invariant subspaces of V such that V =
W1 ‚äïW2 ‚äï¬∑ ¬∑ ¬∑ ‚äïWk. Prove that T is diagonalizable if and only if TWi
is diagonalizable for all i.
39.
Let C be a collection of diagonalizable linear operators on a Ô¨Ånite-
dimensional vector space V.
Prove that there is an ordered basis Œ≤
such that [T]Œ≤ is a diagonal matrix for all T ‚ààC if and only if the
operators of C commute under composition. (This is an extension of
Exercise 25.) Hints for the case that the operators commute: The result
is trivial if each operator has only one eigenvalue. Otherwise, establish
the general result by mathematical induction on dim(V), using the fact
that V is the direct sum of the eigenspaces of some operator in C that
has more than one eigenvalue.
40.
Let B1, B2, . . . , Bk be square matrices with entries in the same Ô¨Åeld, and
let A = B1 ‚äïB2 ‚äï¬∑ ¬∑ ¬∑ ‚äïBk. Prove that the characteristic polynomial
of A is the product of the characteristic polynomials of the Bi‚Äôs.
41.
Let
A =
‚éõ
‚éú
‚éú
‚éú
‚éù
1
2
¬∑ ¬∑ ¬∑
n
n + 1
n + 2
¬∑ ¬∑ ¬∑
2n
...
...
...
n2 ‚àín + 1
n2 ‚àín + 2
¬∑ ¬∑ ¬∑
n2
‚éû
‚éü
‚éü
‚éü
‚é†.
Find the characteristic polynomial of A. Hint: First prove that A has
rank 2 and that span({(1, 1, . . . , 1), (1, 2, . . . , n)}) is LA-invariant.
42.
Let A ‚ààMn√ón(R) be the matrix deÔ¨Åned by Aij = 1 for all i and j.
Find the characteristic polynomial of A.

328
Chap. 5
Diagonalization
INDEX OF DEFINITIONS FOR CHAPTER 5
Absorbing Markov chain
304
Absorbing state
304
Characteristic polynomial of a linear
operator
249
Characteristic polynomial of a ma-
trix
248
Column sum of a matrix
295
Convergence of matrices
284
Cyclic subspace
313
Diagonalizable linear operator
245
Diagonalizable matrix
246
Direct sum of matrices
320
Direct sum of subspaces
275
Eigenspace of a linear operator
264
Eigenspace of a matrix
264
Eigenvalue of a linear operator
246
Eigenvalue of a matrix
246
Eigenvector
of
a
linear
operator
246
Eigenvector of a matrix
246
Fixed probability vector
301
Generator of a cyclic subspace
313
Gerschgorin disk
296
Initial
probability
vector
for
a
Markov chain
292
Invariant subspace
313
Limit of a sequence of matrices
284
Markov chain
291
Markov process
291
Multiplicity of an eigenvalue
263
Probability vector
289
Regular transition matrix
294
Row sum of a matrix
295
Splits
262
Stochastic process
288
Sum of subspaces
275
Transition matrix
288

6
Inner Product Spaces
6.1
Inner Products and Norms
6.2
The Gram-Schmidt Orthogonalization Process and Orthogonal
Complements
6.3
The Adjoint of a Linear Operator
6.4
Normal and Self-Adjoint Operators
6.5
Unitary and Orthogonal Operators and Their Matrices
6.6
Orthogonal Projections and the Spectral Theorem
6.7*
The Singular Value Decomposition and the Pseudoinverse
6.8*
Bilinear and Quadratic Forms
6.9*
Einstein‚Äôs Special Theory of Relativity
6.10* Conditioning and the Rayleigh Quotient
6.11* The Geometry of Orthogonal Operators
Most applications of mathematics are involved with the concept of mea-
surement and hence of the magnitude or relative size of various quantities. So
it is not surprising that the Ô¨Åelds of real and complex numbers, which have a
built-in notion of distance, should play a special role. Except for Section 6.8,
we assume that all vector spaces are over the Ô¨Åeld F, where F denotes either
R or C. (See Appendix D for properties of complex numbers.)
We introduce the idea of distance or length into vector spaces via a much
richer structure, the so-called inner product space structure.
This added
structure provides applications to geometry (Sections 6.5 and 6.11), physics
(Section 6.9), conditioning in systems of linear equations (Section 6.10), least
squares (Section 6.3), and quadratic forms (Section 6.8).
6.1
INNER PRODUCTS AND NORMS
Many geometric notions such as angle, length, and perpendicularity in R2
and R3 may be extended to more general real and complex vector spaces. All
of these ideas are related to the concept of inner product.
DeÔ¨Ånition.
Let V be a vector space over F. An inner product on V
is a function that assigns, to every ordered pair of vectors x and y in V, a
329

330
Chap. 6
Inner Product Spaces
scalar in F, denoted ‚ü®x, y‚ü©, such that for all x, y, and z in V and all c in F,
the following hold:
(a) ‚ü®x + z, y‚ü©= ‚ü®x, y‚ü©+ ‚ü®z, y‚ü©.
(b) ‚ü®cx, y‚ü©= c ‚ü®x, y‚ü©.
(c) ‚ü®x, y‚ü©= ‚ü®y, x‚ü©, where the bar denotes complex conjugation.
(d) ‚ü®x, x‚ü©> 0 if x Ã∏= 0.
Note that (c) reduces to ‚ü®x, y‚ü©= ‚ü®y, x‚ü©if F = R. Conditions (a) and (b)
simply require that the inner product be linear in the Ô¨Årst component.
It is easily shown that if a1, a2, . . . , an ‚ààF and y, v1, v2, . . . , vn ‚ààV, then
1 n

i=1
aivi, y
2
=
n

i=1
ai ‚ü®vi, y‚ü©.
Example 1
For x = (a1, a2, . . . , an) and y = (b1, b2, . . . , bn) in Fn, deÔ¨Åne
‚ü®x, y‚ü©=
n

i=1
aibi.
The veriÔ¨Åcation that ‚ü®¬∑, ¬∑‚ü©satisÔ¨Åes conditions (a) through (d) is easy. For
example, if z = (c1, c2, . . . , cn), we have for (a)
‚ü®x + z, y‚ü©=
n

i=1
(ai + ci)bi =
n

i=1
aibi +
n

i=1
cibi
= ‚ü®x, y‚ü©+ ‚ü®z, y‚ü©.
Thus, for x = (1 + i, 4) and y = (2 ‚àí3i, 4 + 5i) in C2,
‚ü®x, y‚ü©= (1 + i)(2 + 3i) + 4(4 ‚àí5i) = 15 ‚àí15i.
‚ô¶
The inner product in Example 1 is called the standard inner product
on Fn. When F = R the conjugations are not needed, and in early courses
this standard inner product is usually called the dot product and is denoted
by x qy instead of ‚ü®x, y‚ü©.
Example 2
If ‚ü®x, y‚ü©is any inner product on a vector space V and r > 0, we may deÔ¨Åne
another inner product by the rule ‚ü®x, y‚ü©‚Ä≤ = r ‚ü®x, y‚ü©. If r ‚â§0, then (d) would
not hold.
‚ô¶

Sec. 6.1
Inner Products and Norms
331
Example 3
Let V = C([0, 1]), the vector space of real-valued continuous functions on
[0, 1]. For f, g ‚ààV, deÔ¨Åne ‚ü®f, g‚ü©=
 1
0 f(t)g(t) dt. Since the preceding integral
is linear in f, (a) and (b) are immediate, and (c) is trivial. If f Ã∏= 0, then f 2
is bounded away from zero on some subinterval of [0, 1] (continuity is used
here), and hence ‚ü®f, f‚ü©=
 1
0 [f(t)]2 dt > 0.
‚ô¶
DeÔ¨Ånition.
Let A ‚ààMm√ón(F). We deÔ¨Åne the conjugate transpose
or adjoint of A to be the n √ó m matrix A‚àósuch that (A‚àó)ij = Aji for all i, j.
Example 4
Let
A =

i
1 + 2i
2
3 + 4i
	
.
Then
A‚àó=

‚àíi
2
1 ‚àí2i
3 ‚àí4i
	
.
‚ô¶
Notice that if x and y are viewed as column vectors in Fn, then ‚ü®x, y‚ü©=
y‚àóx.
The conjugate transpose of a matrix plays a very important role in the
remainder of this chapter. In the case that A has real entries, A‚àóis simply
the transpose of A.
Example 5
Let V = Mn√ón(F), and deÔ¨Åne ‚ü®A, B‚ü©= tr(B‚àóA) for A, B ‚ààV. (Recall that
the trace of a matrix A is deÔ¨Åned by tr(A) = ,n
i=1 Aii.) We verify that
(a) and (d) of the deÔ¨Ånition of inner product hold and leave (b) and (c) to
the reader. For this purpose, let A, B, C ‚ààV. Then (using Exercise 6 of
Section 1.3)
‚ü®A + B, C‚ü©= tr(C‚àó(A + B)) = tr(C‚àóA + C‚àóB)
= tr(C‚àóA) + tr(C‚àóB) = ‚ü®A, C‚ü©+ ‚ü®B, C‚ü©.
Also
‚ü®A, A‚ü©= tr(A‚àóA) =
n

i=1
(A‚àóA)ii =
n

i=1
n

k=1
(A‚àó)ikAki
=
n

i=1
n

k=1
AkiAki =
n

i=1
n

k=1
|Aki|2.
Now if A Ã∏= O, then Aki Ã∏= 0 for some k and i. So ‚ü®A, A‚ü©> 0.
‚ô¶

332
Chap. 6
Inner Product Spaces
The inner product on Mn√ón(F) in Example 5 is called the Frobenius
inner product.
A vector space V over F endowed with a speciÔ¨Åc inner product is called
an inner product space. If F = C, we call V a complex inner product
space, whereas if F = R, we call V a real inner product space.
It is clear that if V has an inner product ‚ü®x, y‚ü©and W is a subspace of
V, then W is also an inner product space when the same function ‚ü®x, y‚ü©is
restricted to the vectors x, y ‚ààW.
Thus Examples 1, 3, and 5 also provide examples of inner product spaces.
For the remainder of this chapter, Fn denotes the inner product space with
the standard inner product as deÔ¨Åned in Example 1.
Likewise, Mn√ón(F)
denotes the inner product space with the Frobenius inner product as deÔ¨Åned
in Example 5. The reader is cautioned that two distinct inner products on
a given vector space yield two distinct inner product spaces. For instance, it
can be shown that both
‚ü®f(x), g(x)‚ü©1 =
 1
0
f(t)g(t) dt
and
‚ü®f(x), g(x)‚ü©2 =
 1
‚àí1
f(t)g(t) dt
are inner products on the vector space P(R). Even though the underlying
vector space is the same, however, these two inner products yield two diÔ¨Äerent
inner product spaces. For example, the polynomials f(x) = x and g(x) = x2
are orthogonal in the second inner product space, but not in the Ô¨Årst.
A very important inner product space that resembles C([0, 1]) is the space
H of continuous complex-valued functions deÔ¨Åned on the interval [0, 2œÄ] with
the inner product
‚ü®f, g‚ü©= 1
2œÄ
 2œÄ
0
f(t)g(t) dt.
The reason for the constant 1/2œÄ will become evident later. This inner prod-
uct space, which arises often in the context of physical situations, is examined
more closely in later sections.
At this point, we mention a few facts about integration of complex-valued
functions. First, the imaginary number i can be treated as a constant under
the integration sign. Second, every complex-valued function f may be written
as f = f1 + if2, where f1 and f2 are real-valued functions. Thus we have

f =

f1 + i

f2
and

f =

f.
From these properties, as well as the assumption of continuity, it follows
that H is an inner product space (see Exercise 16(a)).
Some properties that follow easily from the deÔ¨Ånition of an inner product
are contained in the next theorem.

Sec. 6.1
Inner Products and Norms
333
Theorem 6.1. Let V be an inner product space. Then for x, y, z ‚ààV and
c ‚ààF, the following statements are true.
(a) ‚ü®x, y + z‚ü©= ‚ü®x, y‚ü©+ ‚ü®x, z‚ü©.
(b) ‚ü®x, cy‚ü©= c ‚ü®x, y‚ü©.
(c) ‚ü®x, 0‚ü©= ‚ü®0, x‚ü©= 0.
(d) ‚ü®x, x‚ü©= 0 if and only if x = 0.
(e) If ‚ü®x, y‚ü©= ‚ü®x, z‚ü©for all x ‚ààV, then y = z.
Proof. (a) We have
‚ü®x, y + z‚ü©= ‚ü®y + z, x‚ü©= ‚ü®y, x‚ü©+ ‚ü®z, x‚ü©
= ‚ü®y, x‚ü©+ ‚ü®z, x‚ü©= ‚ü®x, y‚ü©+ ‚ü®x, z‚ü©.
The proofs of (b), (c), (d), and (e) are left as exercises.
The reader should observe that (a) and (b) of Theorem 6.1 show that the
inner product is conjugate linear in the second component.
In order to generalize the notion of length in R3 to arbitrary inner product
spaces, we need only observe that the length of x = (a, b, c) ‚ààR3 is given by
‚àö
a2 + b2 + c2 =

‚ü®x, x‚ü©. This leads to the following deÔ¨Ånition.
DeÔ¨Ånition.
Let V be an inner product space. For x ‚ààV, we deÔ¨Åne the
norm or length of x by ‚à•x‚à•=

‚ü®x, x‚ü©.
Example 6
Let V = Fn. If x = (a1, a2 . . . , an), then
‚à•x‚à•= ‚à•(a1, a2 . . . , an)‚à•=
3 n

i=1
|ai|2
41/2
is the Euclidean deÔ¨Ånition of length. Note that if n = 1, we have ‚à•a‚à•= |a|.
‚ô¶
As we might expect, the well-known properties of Euclidean length in R3
hold in general, as shown next.
Theorem 6.2. Let V be an inner product space over F. Then for all
x, y ‚ààV and c ‚ààF, the following statements are true.
(a) ‚à•cx‚à•= |c|¬∑‚à•x‚à•.
(b) ‚à•x‚à•= 0 if and only if x = 0. In any case, ‚à•x‚à•‚â•0.
(c) (Cauchy‚ÄìSchwarz Inequality) | ‚ü®x, y‚ü©| ‚â§‚à•x‚à•¬∑‚à•y‚à•.
(d) (Triangle Inequality) ‚à•x + y‚à•‚â§‚à•x‚à•+ ‚à•y‚à•.

334
Chap. 6
Inner Product Spaces
Proof. We leave the proofs of (a) and (b) as exercises.
(c) If y = 0, then the result is immediate. So assume that y Ã∏= 0. For any
c ‚ààF, we have
0 ‚â§‚à•x ‚àícy‚à•2 = ‚ü®x ‚àícy, x ‚àícy‚ü©= ‚ü®x, x ‚àícy‚ü©‚àíc ‚ü®y, x ‚àícy‚ü©
= ‚ü®x, x‚ü©‚àíc ‚ü®x, y‚ü©‚àíc ‚ü®y, x‚ü©+ cc ‚ü®y, y‚ü©.
In particular, if we set
c = ‚ü®x, y‚ü©
‚ü®y, y‚ü©,
the inequality becomes
0 ‚â§‚ü®x, x‚ü©‚àí| ‚ü®x, y‚ü©|2
‚ü®y, y‚ü©
= ‚à•x‚à•2 ‚àí| ‚ü®x, y‚ü©|2
‚à•y‚à•2
,
from which (c) follows.
(d) We have
‚à•x + y‚à•2 = ‚ü®x + y, x + y‚ü©= ‚ü®x, x‚ü©+ ‚ü®y, x‚ü©+ ‚ü®x, y‚ü©+ ‚ü®y, y‚ü©
= ‚à•x‚à•2 + 2‚Ñú‚ü®x, y‚ü©+ ‚à•y‚à•2
‚â§‚à•x‚à•2 + 2| ‚ü®x, y‚ü©| + ‚à•y‚à•2
‚â§‚à•x‚à•2 + 2‚à•x‚à•¬∑‚à•y‚à•+ ‚à•y‚à•2
= (‚à•x‚à•+ ‚à•y‚à•)2,
where ‚Ñú‚ü®x, y‚ü©denotes the real part of the complex number ‚ü®x, y‚ü©. Note that
we used (c) to prove (d).
The case when equality results in (c) and (d) is considered in Exercise 15.
Example 7
For Fn, we may apply (c) and (d) of Theorem 6.2 to the standard inner
product to obtain the following well-known inequalities:
#####
n

i=1
aibi
##### ‚â§
3 n

i=1
|ai|2
41/2 3 n

i=1
|bi|2
41/2
and
3 n

i=1
|ai + bi|2
41/2
‚â§
3 n

i=1
|ai|2
41/2
+
3 n

i=1
|bi|2
41/2
.
‚ô¶

Sec. 6.1
Inner Products and Norms
335
The reader may recall from earlier courses that, for x and y in R3 or R2,
we have that ‚ü®x, y‚ü©= ‚à•x‚à•¬∑‚à•y‚à•cos Œ∏, where Œ∏ (0 ‚â§Œ∏ ‚â§œÄ) denotes the angle
between x and y. This equation implies (c) immediately since | cos Œ∏| ‚â§1.
Notice also that nonzero vectors x and y are perpendicular if and only if
cos Œ∏ = 0, that is, if and only if ‚ü®x, y‚ü©= 0.
We are now at the point where we can generalize the notion of perpendic-
ularity to arbitrary inner product spaces.
DeÔ¨Ånitions. Let V be an inner product space. Vectors x and y in V are
orthogonal (perpendicular) if ‚ü®x, y‚ü©= 0. A subset S of V is orthogonal
if any two distinct vectors in S are orthogonal. A vector x in V is a unit
vector if ‚à•x‚à•= 1. Finally, a subset S of V is orthonormal if S is orthogonal
and consists entirely of unit vectors.
Note that if S = {v1, v2, . . .}, then S is orthonormal if and only if ‚ü®vi, vj‚ü©=
Œ¥ij, where Œ¥ij denotes the Kronecker delta. Also, observe that multiplying
vectors by nonzero scalars does not aÔ¨Äect their orthogonality and that if x is
any nonzero vector, then (1/‚à•x‚à•)x is a unit vector. The process of multiplying
a nonzero vector by the reciprocal of its length is called normalizing.
Example 8
In F3, {(1, 1, 0), (1, ‚àí1, 1), (‚àí1, 1, 2)} is an orthogonal set of nonzero vectors,
but it is not orthonormal; however, if we normalize the vectors in the set, we
obtain the orthonormal set
 1
‚àö
2(1, 1, 0), 1
‚àö
3(1, ‚àí1, 1), 1
‚àö
6(‚àí1, 1, 2)

.
‚ô¶
Our next example is of an inÔ¨Ånite orthonormal set that is important in
analysis. This set is used in later examples in this chapter.
Example 9
Recall the inner product space H (deÔ¨Åned on page 332). We introduce an im-
portant orthonormal subset S of H. For what follows, i is the imaginary num-
ber such that i2 = ‚àí1. For any integer n, let fn(t) = eint, where 0 ‚â§t ‚â§2œÄ.
(Recall that eint = cos nt + i sin nt.) Now deÔ¨Åne S = {fn : n is an integer}.
Clearly S is a subset of H. Using the property that eit = e‚àíit for every real
number t, we have, for m Ã∏= n,
‚ü®fm, fn‚ü©= 1
2œÄ
 2œÄ
0
eimteint dt = 1
2œÄ
 2œÄ
0
ei(m‚àín)t dt
=
1
2œÄ(m ‚àín)ei(m‚àín)t
####
2œÄ
0
= 0.

336
Chap. 6
Inner Product Spaces
Also,
‚ü®fn, fn‚ü©= 1
2œÄ
 2œÄ
0
ei(n‚àín)t dt = 1
2œÄ
 2œÄ
0
1 dt = 1.
In other words, ‚ü®fm, fn‚ü©= Œ¥mn.
‚ô¶
EXERCISES
1.
Label the following statements as true or false.
(a)
An inner product is a scalar-valued function on the set of ordered
pairs of vectors.
(b)
An inner product space must be over the Ô¨Åeld of real or complex
numbers.
(c)
An inner product is linear in both components.
(d)
There is exactly one inner product on the vector space Rn.
(e)
The triangle inequality only holds in Ô¨Ånite-dimensional inner prod-
uct spaces.
(f)
Only square matrices have a conjugate-transpose.
(g)
If x, y, and z are vectors in an inner product space such that
‚ü®x, y‚ü©= ‚ü®x, z‚ü©, then y = z.
(h)
If ‚ü®x, y‚ü©= 0 for all x in an inner product space, then y = 0.
2.
Let x = (2, 1 + i, i) and y = (2 ‚àíi, 2, 1 + 2i) be vectors in C3. Compute
‚ü®x, y‚ü©, ‚à•x‚à•, ‚à•y‚à•, and ‚à•x + y‚à•. Then verify both the Cauchy‚ÄìSchwarz
inequality and the triangle inequality.
3.
In C([0, 1]), let f(t) = t and g(t) = et. Compute ‚ü®f, g‚ü©(as deÔ¨Åned in
Example 3), ‚à•f‚à•, ‚à•g‚à•, and ‚à•f + g‚à•.
Then verify both the Cauchy‚Äì
Schwarz inequality and the triangle inequality.
4. (a)
Complete the proof in Example 5 that ‚ü®¬∑, ¬∑‚ü©is an inner product
(the Frobenius inner product) on Mn√ón(F).
(b)
Use the Frobenius inner product to compute ‚à•A‚à•, ‚à•B‚à•, and ‚ü®A, B‚ü©
for
A =

1
2 + i
3
i
	
and
B =

1 + i
0
i
‚àíi
	
.
5.
In C2, show that ‚ü®x, y‚ü©= xAy‚àóis an inner product, where
A =

1
i
‚àíi
2
	
.
Compute ‚ü®x, y‚ü©for x = (1 ‚àíi, 2 + 3i) and y = (2 + i, 3 ‚àí2i).

Sec. 6.1
Inner Products and Norms
337
6.
Complete the proof of Theorem 6.1.
7.
Complete the proof of Theorem 6.2.
8.
Provide reasons why each of the following is not an inner product on
the given vector spaces.
(a)
‚ü®(a, b), (c, d)‚ü©= ac ‚àíbd on R2.
(b)
‚ü®A, B‚ü©= tr(A + B) on M2√ó2(R).
(c)
‚ü®f(x), g(x)‚ü©=
 1
0 f ‚Ä≤(t)g(t) dt on P(R), where ‚Ä≤ denotes diÔ¨Äerentia-
tion.
9.
Let Œ≤ be a basis for a Ô¨Ånite-dimensional inner product space.
(a)
Prove that if ‚ü®x, z‚ü©= 0 for all z ‚ààŒ≤, then x = 0.
(b)
Prove that if ‚ü®x, z‚ü©= ‚ü®y, z‚ü©for all z ‚ààŒ≤, then x = y.
10.‚Ä† Let V be an inner product space, and suppose that x and y are orthog-
onal vectors in V. Prove that ‚à•x + y‚à•2 = ‚à•x‚à•2 + ‚à•y‚à•2. Deduce the
Pythagorean theorem in R2.
11.
Prove the parallelogram law on an inner product space V; that is, show
that
‚à•x + y‚à•2 + ‚à•x ‚àíy‚à•2 = 2‚à•x‚à•2 + 2‚à•y‚à•2
for all x, y ‚ààV.
What does this equation state about parallelograms in R2?
12.‚Ä† Let {v1, v2, . . . , vk} be an orthogonal set in V, and let a1, a2, . . . , ak be
scalars. Prove that
55555
k

i=1
aivi
55555
2
=
k

i=1
|ai|2‚à•vi‚à•2.
13.
Suppose that ‚ü®¬∑, ¬∑‚ü©1 and ‚ü®¬∑, ¬∑‚ü©2 are two inner products on a vector space
V. Prove that ‚ü®¬∑, ¬∑‚ü©= ‚ü®¬∑, ¬∑‚ü©1 + ‚ü®¬∑, ¬∑‚ü©2 is another inner product on V.
14.
Let A and B be n √ó n matrices, and let c be a scalar.
Prove that
(A + cB)‚àó= A‚àó+ cB‚àó.
15. (a)
Prove that if V is an inner product space, then | ‚ü®x, y‚ü©| = ‚à•x‚à•¬∑‚à•y‚à•
if and only if one of the vectors x or y is a multiple of the other.
Hint: If the identity holds and y Ã∏= 0, let
a = ‚ü®x, y‚ü©
‚à•y‚à•2 ,

338
Chap. 6
Inner Product Spaces
and let z = x ‚àíay. Prove that y and z are orthogonal and
|a| = ‚à•x‚à•
‚à•y‚à•.
Then apply Exercise 10 to ‚à•x‚à•2 = ‚à•ay + z‚à•2 to obtain ‚à•z‚à•= 0.
(b)
Derive a similar result for the equality ‚à•x + y‚à•= ‚à•x‚à•+ ‚à•y‚à•, and
generalize it to the case of n vectors.
16. (a)
Show that the vector space H with ‚ü®¬∑, ¬∑‚ü©deÔ¨Åned on page 332 is an
inner product space.
(b)
Let V = C([0, 1]), and deÔ¨Åne
‚ü®f, g‚ü©=
 1/2
0
f(t)g(t) dt.
Is this an inner product on V?
17.
Let T be a linear operator on an inner product space V, and suppose
that ‚à•T(x)‚à•= ‚à•x‚à•for all x. Prove that T is one-to-one.
18.
Let V be a vector space over F, where F = R or F = C, and let W be
an inner product space over F with inner product ‚ü®¬∑, ¬∑‚ü©. If T: V ‚ÜíW
is linear, prove that ‚ü®x, y‚ü©‚Ä≤ = ‚ü®T(x), T(y)‚ü©deÔ¨Ånes an inner product on
V if and only if T is one-to-one.
19.
Let V be an inner product space. Prove that
(a)
‚à•x ¬± y‚à•2 = ‚à•x‚à•2 ¬± 2‚Ñú‚ü®x, y‚ü©+ ‚à•y‚à•2 for all x, y ‚ààV, where ‚Ñú‚ü®x, y‚ü©
denotes the real part of the complex number ‚ü®x, y‚ü©.
(b)
| ‚à•x‚à•‚àí‚à•y‚à•| ‚â§‚à•x ‚àíy‚à•for all x, y ‚ààV.
20.
Let V be an inner product space over F. Prove the polar identities: For
all x, y ‚ààV,
(a)
‚ü®x, y‚ü©= 1
4‚à•x + y‚à•2 ‚àí1
4‚à•x ‚àíy‚à•2
if F = R;
(b)
‚ü®x, y‚ü©= 1
4
,4
k=1 ik‚à•x + iky‚à•2
if F = C, where i2 = ‚àí1.
21.
Let A be an n √ó n matrix. DeÔ¨Åne
A1 = 1
2(A + A‚àó)
and
A2 = 1
2i(A ‚àíA‚àó).
(a)
Prove that A‚àó
1 = A1, A‚àó
2 = A2, and A = A1 + iA2. Would it be
reasonable to deÔ¨Åne A1 and A2 to be the real and imaginary parts,
respectively, of the matrix A?
(b)
Let A be an n √ó n matrix. Prove that the representation in (a) is
unique. That is, prove that if A = B1 + iB2, where B‚àó
1 = B1 and
B‚àó
2 = B2, then B1 = A1 and B2 = A2.

Sec. 6.1
Inner Products and Norms
339
22.
Let V be a real or complex vector space (possibly inÔ¨Ånite-dimensional),
and let Œ≤ be a basis for V. For x, y ‚ààV there exist v1, v2, . . . , vn ‚ààŒ≤
such that
x =
n

i=1
aivi
and
y =
n

i=1
bivi.
DeÔ¨Åne
‚ü®x, y‚ü©=
n

i=1
aibi.
(a)
Prove that ‚ü®¬∑, ¬∑‚ü©is an inner product on V and that Œ≤ is an or-
thonormal basis for V. Thus every real or complex vector space
may be regarded as an inner product space.
(b)
Prove that if V = Rn or V = Cn and Œ≤ is the standard ordered
basis, then the inner product deÔ¨Åned above is the standard inner
product.
23.
Let V = Fn, and let A ‚ààMn√ón(F).
(a)
Prove that ‚ü®x, Ay‚ü©= ‚ü®A‚àóx, y‚ü©for all x, y ‚ààV.
(b)
Suppose that for some B ‚ààMn√ón(F), we have ‚ü®x, Ay‚ü©= ‚ü®Bx, y‚ü©
for all x, y ‚ààV. Prove that B = A‚àó.
(c)
Let Œ± be the standard ordered basis for V. For any orthonormal
basis Œ≤ for V, let Q be the n √ó n matrix whose columns are the
vectors in Œ≤. Prove that Q‚àó= Q‚àí1.
(d)
DeÔ¨Åne linear operators T and U on V by T(x) = Ax and U(x) =
A‚àóx. Show that [U]Œ≤ = [T]‚àó
Œ≤ for any orthonormal basis Œ≤ for V.
The following deÔ¨Ånition is used in Exercises 24‚Äì27.
DeÔ¨Ånition.
Let V be a vector space over F, where F is either R or
C. Regardless of whether V is or is not an inner product space, we may still
deÔ¨Åne a norm ‚à•¬∑‚à•as a real-valued function on V satisfying the following three
conditions for all x, y ‚ààV and a ‚ààF:
(1) ‚à•x‚à•‚â•0, and ‚à•x‚à•= 0 if and only if x = 0.
(2) ‚à•ax‚à•= |a|¬∑‚à•x‚à•.
(3) ‚à•x + y‚à•‚â§‚à•x‚à•+ ‚à•y‚à•.
24.
Prove that the following are norms on the given vector spaces V.
(a)
V = Mm√ón(F);
‚à•A‚à•= max
i,j |Aij|
for all A ‚ààV
(b)
V = C([0, 1]);
‚à•f‚à•= max
t‚àà[0,1] |f(t)|
for all f ‚ààV

340
Chap. 6
Inner Product Spaces
(c)
V = C([0, 1]);
‚à•f‚à•=
 1
0
|f(t)| dt
for all f ‚ààV
(d)
V = R2;
‚à•(a, b)‚à•= max{|a|, |b|}
for all (a, b) ‚ààV
25.
Use Exercise 20 to show that there is no inner product ‚ü®¬∑, ¬∑‚ü©on R2
such that ‚à•x‚à•2 = ‚ü®x, x‚ü©for all x ‚ààR2 if the norm is deÔ¨Åned as in
Exercise 24(d).
26.
Let ‚à•¬∑‚à•be a norm on a vector space V, and deÔ¨Åne, for each ordered pair
of vectors, the scalar d(x, y) = ‚à•x ‚àíy‚à•, called the distance between x
and y. Prove the following results for all x, y, z ‚ààV.
(a)
d(x, y) ‚â•0.
(b)
d(x, y) = d(y, x).
(c)
d(x, y) ‚â§d(x, z) + d(z, y).
(d)
d(x, x) = 0.
(e)
d(x, y) Ã∏= 0 if x Ã∏= y.
27.
Let ‚à•¬∑‚à•be a norm on a real vector space V satisfying the parallelogram
law given in Exercise 11. DeÔ¨Åne
‚ü®x, y‚ü©= 1
4
6
‚à•x + y‚à•2 ‚àí‚à•x ‚àíy‚à•27
.
Prove that ‚ü®¬∑, ¬∑‚ü©deÔ¨Ånes an inner product on V such that ‚à•x‚à•2 = ‚ü®x, x‚ü©
for all x ‚ààV.
Hints:
(a)
Prove ‚ü®x, 2y‚ü©= 2 ‚ü®x, y‚ü©for all x, y ‚ààV.
(b)
Prove ‚ü®x + u, y‚ü©= ‚ü®x, y‚ü©+ ‚ü®u, y‚ü©for all x, u, y ‚ààV.
(c)
Prove ‚ü®nx, y‚ü©= n ‚ü®x, y‚ü©for every positive integer n and every
x, y ‚ààV.
(d)
Prove m
8 1
mx, y
9
= ‚ü®x, y‚ü©for every positive integer m and every
x, y ‚ààV.
(e)
Prove ‚ü®rx, y‚ü©= r ‚ü®x, y‚ü©for every rational number r and every
x, y ‚ààV.
(f)
Prove | ‚ü®x, y‚ü©| ‚â§‚à•x‚à•‚à•y‚à•for every x, y ‚ààV. Hint: Condition (3) in
the deÔ¨Ånition of norm can be helpful.
(g)
Prove that for every c ‚ààR, every rational number r, and every
x, y ‚ààV,
|c ‚ü®x, y‚ü©‚àí‚ü®cx, y‚ü©| = |(c‚àír) ‚ü®x, y‚ü©‚àí‚ü®(c‚àír)x, y‚ü©| ‚â§2|c‚àír|‚à•x‚à•‚à•y‚à•.
(h)
Use the fact that for any c ‚ààR, |c ‚àír| can be made arbitrarily
small, where r varies over the set of rational numbers, to establish
item (b) of the deÔ¨Ånition of inner product.

Sec. 6.2
Gram-Schmidt Orthogonalization Process
341
28.
Let V be a complex inner product space with an inner product ‚ü®¬∑, ¬∑‚ü©.
Let [¬∑, ¬∑] be the real-valued function such that [x, y] is the real part of
the complex number ‚ü®x, y‚ü©for all x, y ‚ààV. Prove that [¬∑, ¬∑] is an inner
product for V, where V is regarded as a vector space over R. Prove,
furthermore, that [x, ix] = 0 for all x ‚ààV.
29.
Let V be a vector space over C, and suppose that [¬∑, ¬∑] is a real inner
product on V, where V is regarded as a vector space over R, such that
[x, ix] = 0 for all x ‚ààV. Let ‚ü®¬∑, ¬∑‚ü©be the complex-valued function
deÔ¨Åned by
‚ü®x, y‚ü©= [x, y] + i[x, iy]
for x, y ‚ààV.
Prove that ‚ü®¬∑, ¬∑‚ü©is a complex inner product on V.
30.
Let ‚à•¬∑‚à•be a norm (as deÔ¨Åned in Exercise 24) on a complex vector
space V satisfying the parallelogram law given in Exercise 11. Prove
that there is an inner product ‚ü®¬∑, ¬∑‚ü©on V such that ‚à•x‚à•2 = ‚ü®x, x‚ü©for
all x ‚ààV.
Hint: Apply Exercise 27 to V regarded as a vector space over R. Then
apply Exercise 29.
6.2
THE GRAM‚ÄìSCHMIDT ORTHOGONALIZATION PROCESS
AND ORTHOGONAL COMPLEMENTS
In previous chapters, we have seen the special role of the standard ordered
bases for Cn and Rn. The special properties of these bases stem from the fact
that the basis vectors form an orthonormal set. Just as bases are the building
blocks of vector spaces, bases that are also orthonormal sets are the building
blocks of inner product spaces. We now name such bases.
DeÔ¨Ånition.
Let V be an inner product space.
A subset of V is an
orthonormal basis for V if it is an ordered basis that is orthonormal.
Example 1
The standard ordered basis for Fn is an orthonormal basis for Fn.
‚ô¶
Example 2
The set
 1
‚àö
5, 2
‚àö
5
	
,
 2
‚àö
5, ‚àí1
‚àö
5
	
is an orthonormal basis for R2.
‚ô¶

342
Chap. 6
Inner Product Spaces
The next theorem and its corollaries illustrate why orthonormal sets and,
in particular, orthonormal bases are so important.
Theorem 6.3. Let V be an inner product space and S = {v1, v2, . . . , vk}
be an orthogonal subset of V consisting of nonzero vectors. If y ‚ààspan(S),
then
y =
k

i=1
‚ü®y, vi‚ü©
‚à•vi‚à•2 vi.
Proof. Write y =
k

i=1
aivi, where a1, a2, . . . , ak ‚ààF. Then, for 1 ‚â§j ‚â§k,
we have
‚ü®y, vj‚ü©=
1 k

i=1
aivi, vj
2
=
k

i=1
ai ‚ü®vi, vj‚ü©= aj ‚ü®vj, vj‚ü©= aj‚à•vj‚à•2.
So aj = ‚ü®y, vj‚ü©
‚à•vj‚à•2 , and the result follows.
The next corollary follows immediately from Theorem 6.3.
Corollary 1.
If, in addition to the hypotheses of Theorem 6.3, S is
orthonormal and y ‚ààspan(S), then
y =
k

i=1
‚ü®y, vi‚ü©vi.
If V possesses a Ô¨Ånite orthonormal basis, then Corollary 1 allows us to
compute the coeÔ¨Écients in a linear combination very easily. (See Example 3.)
Corollary 2. Let V be an inner product space, and let S be an orthogonal
subset of V consisting of nonzero vectors. Then S is linearly independent.
Proof. Suppose that v1, v2, . . . , vk ‚ààS and
k

i=1
aivi = 0.
As in the proof of Theorem 6.3 with y = 0, we have aj = ‚ü®0, vj‚ü©/‚à•vj‚à•2 = 0
for all j. So S is linearly independent.

Sec. 6.2
Gram-Schmidt Orthogonalization Process
343
Example 3
By Corollary 2, the orthonormal set
 1
‚àö
2(1, 1, 0), 1
‚àö
3(1, ‚àí1, 1), 1
‚àö
6(‚àí1, 1, 2)

obtained in Example 8 of Section 6.1 is an orthonormal basis for R3. Let
x = (2, 1, 3).
The coeÔ¨Écients given by Corollary 1 to Theorem 6.3 that
express x as a linear combination of the basis vectors are
a1 =
1
‚àö
2(2 + 1) =
3
‚àö
2,
a2 =
1
‚àö
3(2 ‚àí1 + 3) =
4
‚àö
3,
and
a3 =
1
‚àö
6(‚àí2 + 1 + 6) =
5
‚àö
6.
As a check, we have
(2, 1, 3) = 3
2(1, 1, 0) + 4
3(1, ‚àí1, 1) + 5
6(‚àí1, 1, 2).
‚ô¶
Corollary 2 tells us that the vector space H in Section 6.1 contains an
inÔ¨Ånite linearly independent set, and hence H is not a Ô¨Ånite-dimensional vector
space.
Of course, we have not yet shown that every Ô¨Ånite-dimensional inner prod-
uct space possesses an orthonormal basis. The next theorem takes us most
of the way in obtaining this result. It tells us how to construct an orthogonal
set from a linearly independent set of vectors in such a way that both sets
generate the same subspace.
Before stating this theorem, let us consider a simple case. Suppose that
{w1, w2} is a linearly independent subset of an inner product space (and
hence a basis for some two-dimensional subspace).
We want to construct
an orthogonal set from {w1, w2} that spans the same subspace. Figure 6.1
suggests that the set {v1, v2}, where v1 = w1 and v2 = w2 ‚àícw1, has this
property if c is chosen so that v2 is orthogonal to W1.
To Ô¨Ånd c, we need only solve the following equation:
0 = ‚ü®v2, w1‚ü©= ‚ü®w2 ‚àícw1, w1‚ü©= ‚ü®w2, w1‚ü©‚àíc ‚ü®w1, w1‚ü©.
So
c = ‚ü®w2, w1‚ü©
‚à•w1‚à•2 .
Thus
v2 = w2 ‚àí‚ü®w2, w1‚ü©
‚à•w1‚à•2 w1.

344
Chap. 6
Inner Product Spaces










1
B
B
B
B
BBM
1
w2
v2
cw1
w1 = v1
Figure 6.1
The next theorem shows us that this process can be extended to any Ô¨Ånite
linearly independent subset.
Theorem 6.4. Let V be an inner product space and S = {w1, w2, . . . , wn}
be a linearly independent subset of V. DeÔ¨Åne S‚Ä≤ = {v1, v2, . . . , vn}, where
v1 = w1 and
vk = wk ‚àí
k‚àí1

j=1
‚ü®wk, vj‚ü©
‚à•vj‚à•2 vj
for 2 ‚â§k ‚â§n.
(1)
Then S‚Ä≤ is an orthogonal set of nonzero vectors such that span(S‚Ä≤) = span(S).
Proof. The proof is by mathematical induction on n, the number of vectors
in S.
For k = 1, 2, . . . , n, let Sk = {w1, w2, . . . , wk}.
If n = 1, then the
theorem is proved by taking S‚Ä≤
1 = S1; i.e., v1 = w1 Ã∏= 0. Assume then that the
set S‚Ä≤
k‚àí1 = {v1, v2, . . . , vk‚àí1} with the desired properties has been constructed
by the repeated use of (1). We show that the set S‚Ä≤
k = {v1, v2, . . . , vk‚àí1, vk}
also has the desired properties, where vk is obtained from S‚Ä≤
k‚àí1 by (1). If vk =
0, then (1) implies that wk ‚ààspan(S‚Ä≤
k‚àí1) = span(Sk‚àí1), which contradicts
the assumption that Sk is linearly independent. For 1 ‚â§i ‚â§k ‚àí1, it follows
from (1) that
‚ü®vk, vi‚ü©= ‚ü®wk, vi‚ü©‚àí
k‚àí1

j=1
‚ü®wk, vj‚ü©
‚à•vj‚à•2
‚ü®vj, vi‚ü©= ‚ü®wk, vi‚ü©‚àí‚ü®wk, vi‚ü©
‚à•vi‚à•2 ‚à•vi‚à•2 = 0,
since ‚ü®vj, vi‚ü©= 0 if i Ã∏= j by the induction assumption that S‚Ä≤
k‚àí1 is orthogonal.
Hence S‚Ä≤
k is an orthogonal set of nonzero vectors. Now, by (1), we have that
span(S‚Ä≤
k) ‚äÜspan(Sk).
But by Corollary 2 to Theorem 6.3, S‚Ä≤
k is linearly
independent; so dim(span(S‚Ä≤
k)) = dim(span(Sk)) = k. Therefore span(S‚Ä≤
k) =
span(Sk).
The construction of {v1, v2, . . . , vn} by the use of Theorem 6.4 is called
the Gram‚ÄìSchmidt process.

Sec. 6.2
Gram-Schmidt Orthogonalization Process
345
Example 4
In R4, let w1 = (1, 0, 1, 0), w2 = (1, 1, 1, 1), and w3 = (0, 1, 2, 1).
Then
{w1, w2, w3} is linearly independent. We use the Gram‚ÄìSchmidt process to
compute the orthogonal vectors v1, v2, and v3, and then we normalize these
vectors to obtain an orthonormal set.
Take v1 = w1 = (1, 0, 1, 0). Then
v2 = w2 ‚àí‚ü®w2, v1‚ü©
‚à•v1‚à•2 v1
= (1, 1, 1, 1) ‚àí2
2(1, 0, 1, 0)
= (0, 1, 0, 1).
Finally,
v3 = w3 ‚àí‚ü®w3, v1‚ü©
‚à•v1‚à•2 v1 ‚àí‚ü®w3, v2‚ü©
‚à•v2‚à•2 v2
= (0, 1, 2, 1) ‚àí2
2(1, 0, 1, 0) ‚àí2
2(0, 1, 0, 1)
= (‚àí1, 0, 1, 0).
These vectors can be normalized to obtain the orthonormal basis {u1, u2, u3},
where
u1 =
1
‚à•v1‚à•v1 =
1
‚àö
2(1, 0, 1, 0),
u2 =
1
‚à•v2‚à•v2 =
1
‚àö
2(0, 1, 0, 1),
and
u3 =
v3
‚à•v3‚à•=
1
‚àö
2(‚àí1, 0, 1, 0).
‚ô¶
Example 5
Let V = P(R) with the inner product ‚ü®f(x), g(x)‚ü©=
 1
‚àí1 f(t)g(t) dt, and
consider the subspace P2(R) with the standard ordered basis Œ≤. We use the
Gram‚ÄìSchmidt process to replace Œ≤ by an orthogonal basis {v1, v2, v3} for
P2(R), and then use this orthogonal basis to obtain an orthonormal basis for
P2(R).
Take v1 = 1. Then ‚à•v1‚à•2 =
 1
‚àí1
12 dt = 2, and ‚ü®x, v1‚ü©=
 1
‚àí1
t ¬∑ 1 dt = 0.
Thus
v2 = x ‚àí‚ü®v1, x‚ü©
‚à•v1‚à•2 = x ‚àí0
2 = x.

346
Chap. 6
Inner Product Spaces
Furthermore,
8
x2, v1
9
=
 1
‚àí1
t2 ¬∑ 1 dt = 2
3
and
8
x2, v2
9
=
 1
‚àí1
t2 ¬∑ t dt = 0.
Therefore
v3 = x2 ‚àí
8
x2, v1
9
‚à•v1‚à•2 v1 ‚àí
8
x2, v2
9
‚à•v2‚à•2 v2
= x2 ‚àí1
3 ¬∑ 1 ‚àí0 ¬∑ x
= x2 ‚àí1
3.
We conclude that {1, x, x2 ‚àí1
3} is an orthogonal basis for P2(R).
To obtain an orthonormal basis, we normalize v1, v2, and v3 to obtain
u1 =
1
: 1
‚àí1 12 dt
=
1
‚àö
2,
u2 =
x
: 1
‚àí1 t2 dt
=
"
3
2 x,
and similarly,
u3 =
v3
‚à•v3‚à•=
"
5
8 (3x2 ‚àí1).
Thus {u1, u2, u3} is the desired orthonormal basis for P2(R).
‚ô¶
If we continue applying the Gram‚ÄìSchmidt orthogonalization process to
the basis {1, x, x2, . . .} for P(R), we obtain an orthogonal basis whose elements
are called the Legendre polynomials. The orthogonal polynomials v1, v2, and
v3 in Example 5 are the Ô¨Årst three Legendre polynomials.
The following result gives us a simple method of representing a vector as
a linear combination of the vectors in an orthonormal basis.
Theorem 6.5. Let V be a nonzero Ô¨Ånite-dimensional inner product space.
Then V has an orthonormal basis Œ≤. Furthermore, if Œ≤ = {v1, v2, . . . , vn} and
x ‚ààV, then
x =
n

i=1
‚ü®x, vi‚ü©vi.

Sec. 6.2
Gram-Schmidt Orthogonalization Process
347
Proof. Let Œ≤0 be an ordered basis for V. Apply Theorem 6.4 to obtain
an orthogonal set Œ≤‚Ä≤ of nonzero vectors with span(Œ≤‚Ä≤) = span(Œ≤0) = V. By
normalizing each vector in Œ≤‚Ä≤, we obtain an orthonormal set Œ≤ that generates
V. By Corollary 2 to Theorem 6.3, Œ≤ is linearly independent; therefore Œ≤
is an orthonormal basis for V. The remainder of the theorem follows from
Corollary 1 to Theorem 6.3.
Example 6
We use Theorem 6.5 to represent the polynomial f(x) = 1 + 2x + 3x2 as
a linear combination of the vectors in the orthonormal basis {u1, u2, u3} for
P2(R) obtained in Example 5. Observe that
‚ü®f(x), u1‚ü©=
 1
‚àí1
1
‚àö
2(1 + 2t + 3t2) dt = 2
‚àö
2,
‚ü®f(x), u2‚ü©=
 1
‚àí1
"
3
2t(1 + 2t + 3t2) dt = 2
‚àö
6
3 ,
and
‚ü®f(x), u3‚ü©=
 1
‚àí1
"
5
8(3t2 ‚àí1)(1 + 2t + 3t2) dt = 2
‚àö
10
5
.
Therefore f(x) = 2
‚àö
2 u1 + 2
‚àö
6
3
u2 + 2
‚àö
10
5
u3.
‚ô¶
Theorem 6.5 gives us a simple method for computing the entries of the
matrix representation of a linear operator with respect to an orthonormal
basis.
Corollary. Let V be a Ô¨Ånite-dimensional inner product space with an
orthonormal basis Œ≤ = {v1, v2, . . . , vn}. Let T be a linear operator on V, and
let A = [T]Œ≤. Then for any i and j, Aij = ‚ü®T(vj), vi‚ü©.
Proof. From Theorem 6.5, we have
T(vj) =
n

i=1
‚ü®T(vj), vi‚ü©vi.
Hence Aij = ‚ü®T(vj), vi‚ü©.
The scalars ‚ü®x, vi‚ü©given in Theorem 6.5 have been studied extensively
for special inner product spaces.
Although the vectors v1, v2, . . . , vn were
chosen from an orthonormal basis, we introduce a terminology associated
with orthonormal sets Œ≤ in more general inner product spaces.

348
Chap. 6
Inner Product Spaces
DeÔ¨Ånition.
Let Œ≤ be an orthonormal subset (possibly inÔ¨Ånite) of an
inner product space V, and let x ‚ààV. We deÔ¨Åne the Fourier coeÔ¨Écients
of x relative to Œ≤ to be the scalars ‚ü®x, y‚ü©, where y ‚ààŒ≤.
In the Ô¨Årst half of the 19th century, the French mathematician Jean Bap-
tiste Fourier was associated with the study of the scalars
 2œÄ
0
f(t) sin nt dt
and
 2œÄ
0
f(t) cos nt dt,
or more generally,
cn = 1
2œÄ
 2œÄ
0
f(t)e‚àíint dt,
for a function f. In the context of Example 9 of Section 6.1, we see that
cn = ‚ü®f, fn‚ü©, where fn(t) = eint; that is, cn is the nth Fourier coeÔ¨Écient for a
continuous function f ‚ààV relative to S. These coeÔ¨Écients are the ‚Äúclassical‚Äù
Fourier coeÔ¨Écients of a function, and the literature concerning the behavior of
these coeÔ¨Écients is extensive. We learn more about these Fourier coeÔ¨Écients
in the remainder of this chapter.
Example 7
Let S = {eint : n is an integer}. In Example 9 of Section 6.1, S was shown to
be an orthonormal set in H. We compute the Fourier coeÔ¨Écients of f(t) = t
relative to S. Using integration by parts, we have, for n Ã∏= 0,
‚ü®f, fn‚ü©= 1
2œÄ
 2œÄ
0
teint dt = 1
2œÄ
 2œÄ
0
te‚àíint dt = ‚àí1
in ,
and, for n = 0,
‚ü®f, 1‚ü©= 1
2œÄ
 2œÄ
0
t(1) dt = œÄ.
As a result of these computations, and using Exercise 16 of this section, we
obtain an upper bound for the sum of a special inÔ¨Ånite series as follows:
‚à•f‚à•2 ‚â•
‚àí1

n=‚àík
| ‚ü®f, fn‚ü©|2 + | ‚ü®f, 1‚ü©|2 +
k

n=1
| ‚ü®f, fn‚ü©|2
=
‚àí1

n=‚àík
1
n2 + œÄ2 +
k

n=1
1
n2
= 2
k

n=1
1
n2 + œÄ2

Sec. 6.2
Gram-Schmidt Orthogonalization Process
349
for every k. Now, using the fact that ‚à•f‚à•2 = 4
3œÄ2, we obtain
4
3œÄ2 ‚â•2
k

n=1
1
n2 + œÄ2,
or
œÄ2
6 ‚â•
k

n=1
1
n2 .
Because this inequality holds for all k, we may let k ‚Üí‚àûto obtain
œÄ2
6 ‚â•
‚àû

n=1
1
n2 .
Additional results may be produced by replacing f by other functions.
‚ô¶
We are now ready to proceed with the concept of an orthogonal comple-
ment.
DeÔ¨Ånition. Let S be a nonempty subset of an inner product space V. We
deÔ¨Åne S‚ä•(read ‚ÄúS perp‚Äù) to be the set of all vectors in V that are orthogonal
to every vector in S; that is, S‚ä•= {x ‚ààV: ‚ü®x, y‚ü©= 0 for all y ‚ààS}. The set
S‚ä•is called the orthogonal complement of S.
It is easily seen that S‚ä•is a subspace of V for any subset S of V.
Example 8
The reader should verify that {0}‚ä•= V and V‚ä•= {0} for any inner product
space V.
‚ô¶
Example 9
If V = R3 and S = {e3}, then S‚ä•equals the xy-plane (see Exercise 5).
‚ô¶
Exercise 18 provides an interesting example of an orthogonal complement
in an inÔ¨Ånite-dimensional inner product space.
Consider the problem in R3 of Ô¨Ånding the distance from a point P to a
plane W. (See Figure 6.2.) Problems of this type arise in many settings. If
we let y be the vector determined by 0 and P, we may restate the problem
as follows: Determine the vector u in W that is ‚Äúclosest‚Äù to y. The desired
distance is clearly given by ‚à•y ‚àíu‚à•. Notice from the Ô¨Ågure that the vector
z = y ‚àíu is orthogonal to every vector in W, and so z ‚ààW‚ä•.
The next result presents a practical method of Ô¨Ånding u in the case that
W is a Ô¨Ånite-dimensional subspace of an inner product space.

350
Chap. 6
Inner Product Spaces
r










 P

*
6
u
y
z = y ‚àíu
W
0
Figure 6.2
Theorem 6.6. Let W be a Ô¨Ånite-dimensional subspace of an inner product
space V, and let y ‚ààV. Then there exist unique vectors u ‚ààW and z ‚ààW‚ä•
such that y = u + z. Furthermore, if {v1, v2, . . . , vk} is an orthonormal basis
for W, then
u =
k

i=1
‚ü®y, vi‚ü©vi.
Proof. Let {v1, v2, . . . , vk} be an orthonormal basis for W, let u be as
deÔ¨Åned in the preceding equation, and let z = y ‚àíu. Clearly u ‚ààW and
y = u + z.
To show that z ‚ààW‚ä•, it suÔ¨Éces to show, by Exercise 7, that z is orthog-
onal to each vj. For any j, we have
‚ü®z, vj‚ü©=
1
y ‚àí
k

i=1
‚ü®y, vi‚ü©vi

, vj
2
= ‚ü®y, vj‚ü©‚àí
k

i=1
‚ü®y, vi‚ü©‚ü®vi, vj‚ü©
= ‚ü®y, vj‚ü©‚àí‚ü®y, vj‚ü©= 0.
To show uniqueness of u and z, suppose that y = u + z = u‚Ä≤ + z‚Ä≤, where
u‚Ä≤ ‚ààW and z‚Ä≤ ‚ààW‚ä•. Then u ‚àíu‚Ä≤ = z‚Ä≤ ‚àíz ‚ààW ‚à©W‚ä•= {0}. Therefore,
u = u‚Ä≤ and z = z‚Ä≤.
Corollary. In the notation of Theorem 6.6, the vector u is the unique
vector in W that is ‚Äúclosest‚Äù to y; that is, for any x ‚ààW, ‚à•y ‚àíx‚à•‚â•‚à•y ‚àíu‚à•,
and this inequality is an equality if and only if x = u.
Proof. As in Theorem 6.6, we have that y = u + z, where z ‚ààW‚ä•. Let
x ‚ààW. Then u ‚àíx is orthogonal to z, so, by Exercise 10 of Section 6.1, we

Sec. 6.2
Gram-Schmidt Orthogonalization Process
351
have
‚à•y ‚àíx‚à•2 = ‚à•u + z ‚àíx‚à•2 = ‚à•(u ‚àíx) + z‚à•2 = ‚à•u ‚àíx‚à•2 + ‚à•z‚à•2
‚â•‚à•z‚à•2 = ‚à•y ‚àíu‚à•2.
Now suppose that ‚à•y ‚àíx‚à•= ‚à•y ‚àíu‚à•. Then the inequality above becomes an
equality, and therefore ‚à•u ‚àíx‚à•2 + ‚à•z‚à•2 = ‚à•z‚à•2. It follows that ‚à•u ‚àíx‚à•= 0,
and hence x = u. The proof of the converse is obvious.
The vector u in the corollary is called the orthogonal projection of y
on W. We will see the importance of orthogonal projections of vectors in the
application to least squares in Section 6.3.
Example 10
Let V = P3(R) with the inner product
‚ü®f(x), g(x)‚ü©=
 1
‚àí1
f(t)g(t) dt
for all f(x), g(x) ‚ààV.
We compute the orthogonal projection f1(x) of f(x) = x3 on P2(R).
By Example 5,
{u1, u2, u3} =

1
‚àö
2,
"
3
2 x,
"
5
8 (3x2 ‚àí1)
;
is an orthonormal basis for P2(R). For these vectors, we have
‚ü®f(x), u1‚ü©=
 1
‚àí1
t3 1
‚àö
2 dt = 0,
‚ü®f(x), u2‚ü©=
 1
‚àí1
t3
"
3
2 t dt =
‚àö
6
5 ,
and
‚ü®f(x), u3‚ü©=
 1
‚àí1
t3
"
5
8 (3t2 ‚àí1) dt = 0.
Hence
f1(x) = ‚ü®f(x), u1‚ü©u1 + ‚ü®f(x), u2‚ü©u2 + ‚ü®f(x), u3‚ü©u3 = 3
5x.
‚ô¶
It was shown (Corollary 2 to the replacement theorem, p. 47) that any lin-
early independent set in a Ô¨Ånite-dimensional vector space can be extended to
a basis. The next theorem provides an interesting analog for an orthonormal
subset of a Ô¨Ånite-dimensional inner product space.

352
Chap. 6
Inner Product Spaces
Theorem 6.7. Suppose that S = {v1, v2, . . . , vk} is an orthonormal set
in an n-dimensional inner product space V. Then
(a) S can be extended to an orthonormal basis {v1, v2, . . . , vk, vk+1, . . . , vn}
for V.
(b) If
W = span(S), then S1 = {vk+1, vk+2, . . . , vn} is an orthonormal
basis for W‚ä•(using the preceding notation).
(c) If W is any subspace of V, then dim(V) = dim(W) + dim(W‚ä•).
Proof. (a) By Corollary 2 to the replacement theorem (p. 47), S can be
extended to an ordered basis S‚Ä≤ = {v1, v2, . . . , vk, wk+1, . . . , wn} for V. Now
apply the Gram‚ÄìSchmidt process to S‚Ä≤. The Ô¨Årst k vectors resulting from
this process are the vectors in S by Exercise 8, and this new set spans V.
Normalizing the last n ‚àík vectors of this set produces an orthonormal set
that spans V. The result now follows.
(b) Because S1 is a subset of a basis, it is linearly independent. Since S1
is clearly a subset of W‚ä•, we need only show that it spans W‚ä•. Note that,
for any x ‚ààV, we have
x =
n

i=1
‚ü®x, vi‚ü©vi.
If x ‚ààW‚ä•, then ‚ü®x, vi‚ü©= 0 for 1 ‚â§i ‚â§k. Therefore,
x =
n

i=k+1
‚ü®x, vi‚ü©vi ‚ààspan(S1).
(c) Let W be a subspace of V. It is a Ô¨Ånite-dimensional inner product
space because V is, and so it has an orthonormal basis {v1, v2, . . . , vk}. By
(a) and (b), we have
dim(V) = n = k + (n ‚àík) = dim(W) + dim(W‚ä•).
Example 11
Let W = span({e1, e2}) in F3. Then x = (a, b, c) ‚ààW‚ä•if and only if 0 =
‚ü®x, e1‚ü©= a and 0 = ‚ü®x, e2‚ü©= b.
So x = (0, 0, c), and therefore W‚ä•=
span({e3}). One can deduce the same result by noting that e3 ‚ààW‚ä•and,
from (c), that dim(W‚ä•) = 3 ‚àí2 = 1.
‚ô¶
EXERCISES
1.
Label the following statements as true or false.
(a)
The Gram‚ÄìSchmidt orthogonalization process allows us to con-
struct an orthonormal set from an arbitrary set of vectors.

Sec. 6.2
Gram-Schmidt Orthogonalization Process
353
(b)
Every nonzero Ô¨Ånite-dimensional inner product space has an or-
thonormal basis.
(c)
The orthogonal complement of any set is a subspace.
(d)
If {v1, v2, . . . , vn} is a basis for an inner product space V, then for
any x ‚ààV the scalars ‚ü®x, vi‚ü©are the Fourier coeÔ¨Écients of x.
(e)
An orthonormal basis must be an ordered basis.
(f)
Every orthogonal set is linearly independent.
(g)
Every orthonormal set is linearly independent.
2.
In each part, apply the Gram‚ÄìSchmidt process to the given subset S of
the inner product space V to obtain an orthogonal basis for span(S).
Then normalize the vectors in this basis to obtain an orthonormal basis
Œ≤ for span(S), and compute the Fourier coeÔ¨Écients of the given vector
relative to Œ≤. Finally, use Theorem 6.5 to verify your result.
(a)
V = R3, S = {(1, 0, 1), (0, 1, 1), (1, 3, 3)}, and x = (1, 1, 2)
(b)
V = R3, S = {(1, 1, 1), (0, 1, 1), (0, 0, 1)}, and x = (1, 0, 1)
(c)
V = P2(R) with the inner product ‚ü®f(x), g(x)‚ü©=
 1
0 f(t)g(t) dt,
S = {1, x, x2}, and h(x) = 1 + x
(d)
V = span(S), where S = {(1, i, 0), (1 ‚àíi, 2, 4i)}, and
x = (3 + i, 4i, ‚àí4)
(e)
V = R4, S = {(2, ‚àí1, ‚àí2, 4), (‚àí2, 1, ‚àí5, 5), (‚àí1, 3, 7, 11)}, and x =
(‚àí11, 8, ‚àí4, 18)
(f)
V = R4, S = {(1, ‚àí2, ‚àí1, 3), (3, 6, 3, ‚àí1), (1, 4, 2, 8)},
and x = (‚àí1, 2, 1, 1)
(g)
V = M2√ó2(R), S =

3
5
‚àí1
1
	
,

‚àí1
9
5
‚àí1
	
,

7
‚àí17
2
‚àí6
	
, and
A =

‚àí1
27
‚àí4
8
	
(h)
V = M2√ó2(R), S =

2
2
2
1
	
,

11
4
2
5
	
,

4
‚àí12
3
‚àí16
	
, and A =

8
6
25
‚àí13
	
(i)
V = span(S) with the inner product ‚ü®f, g‚ü©=
 œÄ
0
f(t)g(t) dt,
S = {sin t, cos t, 1, t}, and h(t) = 2t + 1
(j)
V = C4, S = {(1, i, 2 ‚àíi, ‚àí1), (2 + 3i, 3i, 1 ‚àíi, 2i),
(‚àí1+7i, 6+10i, 11‚àí4i, 3+4i)}, and x = (‚àí2+7i, 6+9i, 9‚àí3i, 4+4i)
(k)
V = C4, S = {(‚àí4, 3 ‚àí2i, i, 1 ‚àí4i),
(‚àí1‚àí5i, 5‚àí4i, ‚àí3+5i, 7‚àí2i), (‚àí27‚àíi, ‚àí7‚àí6i, ‚àí15+25i, ‚àí7‚àí6i)},
and x = (‚àí13 ‚àí7i, ‚àí12 + 3i, ‚àí39 ‚àí11i, ‚àí26 + 5i)

354
Chap. 6
Inner Product Spaces
(l)
V = M2√ó2(C), S =

1 ‚àíi
‚àí2 ‚àí3i
2 + 2i
4 + i
	
,

8i
4
‚àí3 ‚àí3i
‚àí4 + 4i
	
,

‚àí25 ‚àí38i
‚àí2 ‚àí13i
12 ‚àí78i
‚àí7 + 24i
	
, and A =

‚àí2 + 8i
‚àí13 + i
10 ‚àí10i
9 ‚àí9i
	
(m)
V = M2√ó2(C), S =

‚àí1 + i
‚àíi
2 ‚àíi
1 + 3i
	
,

‚àí1 ‚àí7i
‚àí9 ‚àí8i
1 + 10i
‚àí6 ‚àí2i
	
,

‚àí11 ‚àí132i
‚àí34 ‚àí31i
7 ‚àí126i
‚àí71 ‚àí5i
	
, and A =

‚àí7 + 5i
3 + 18i
9 ‚àí6i
‚àí3 + 7i
	
3.
In R2, let
Œ≤ =
 1
‚àö
2, 1
‚àö
2
	
,
 1
‚àö
2, ‚àí1
‚àö
2
	
.
Find the Fourier coeÔ¨Écients of (3, 4) relative to Œ≤.
4.
Let S = {(1, 0, i), (1, 2, 1)} in C3. Compute S‚ä•.
5.
Let S0 = {x0}, where x0 is a nonzero vector in R3. Describe S‚ä•
0 ge-
ometrically. Now suppose that S = {x1, x2} is a linearly independent
subset of R3. Describe S‚ä•geometrically.
6.
Let V be an inner product space, and let W be a Ô¨Ånite-dimensional
subspace of V.
If x /‚ààW, prove that there exists y ‚ààV such that
y ‚ààW‚ä•, but ‚ü®x, y‚ü©Ã∏= 0. Hint: Use Theorem 6.6.
7.
Let Œ≤ be a basis for a subspace W of an inner product space V, and let
z ‚ààV. Prove that z ‚ààW‚ä•if and only if ‚ü®z, v‚ü©= 0 for every v ‚ààŒ≤.
8.
Prove that if {w1, w2, . . . , wn} is an orthogonal set of nonzero vectors,
then the vectors v1, v2, . . . , vn derived from the Gram‚ÄìSchmidt process
satisfy vi = wi for i = 1, 2, . . . , n. Hint: Use mathematical induction.
9.
Let W = span({(i, 0, 1)}) in C3. Find orthonormal bases for W and W‚ä•.
10.
Let W be a Ô¨Ånite-dimensional subspace of an inner product space V.
Prove that there exists a projection T on W along W‚ä•that satisÔ¨Åes
N(T) = W‚ä•.
In addition, prove that ‚à•T(x)‚à•‚â§‚à•x‚à•for all x ‚ààV.
Hint: Use Theorem 6.6 and Exercise 10 of Section 6.1. (Projections are
deÔ¨Åned in the exercises of Section 2.1.)
11.
Let A be an n √ó n matrix with complex entries. Prove that AA‚àó= I if
and only if the rows of A form an orthonormal basis for Cn.
12.
Prove that for any matrix A ‚ààMm√ón(F), (R(LA‚àó))‚ä•= N(LA).

Sec. 6.2
Gram-Schmidt Orthogonalization Process
355
13.
Let V be an inner product space, S and S0 be subsets of V, and W be
a Ô¨Ånite-dimensional subspace of V. Prove the following results.
(a)
S0 ‚äÜS implies that S‚ä•‚äÜS‚ä•
0 .
(b)
S ‚äÜ(S‚ä•)‚ä•; so span(S) ‚äÜ(S‚ä•)‚ä•.
(c)
W = (W‚ä•)‚ä•. Hint: Use Exercise 6.
(d)
V = W ‚äïW‚ä•. (See the exercises of Section 1.3.)
14.
Let W1 and W2 be subspaces of a Ô¨Ånite-dimensional inner product space.
Prove that (W1 +W2)‚ä•= W‚ä•
1 ‚à©W‚ä•
2 and (W1 ‚à©W2)‚ä•= W‚ä•
1 +W‚ä•
2 . (See
the deÔ¨Ånition of the sum of subsets of a vector space on page 22.) Hint
for the second equation: Apply Exercise 13(c) to the Ô¨Årst equation.
15.
Let V be a Ô¨Ånite-dimensional inner product space over F.
(a)
Parseval‚Äôs Identity. Let {v1, v2, . . . , vn} be an orthonormal basis
for V. For any x, y ‚ààV prove that
‚ü®x, y‚ü©=
n

i=1
‚ü®x, vi‚ü©‚ü®y, vi‚ü©.
(b)
Use (a) to prove that if Œ≤ is an orthonormal basis for V with inner
product ‚ü®¬∑, ¬∑‚ü©, then for any x, y ‚ààV
‚ü®œÜŒ≤(x), œÜŒ≤(y)‚ü©‚Ä≤ = ‚ü®[x]Œ≤, [y]Œ≤‚ü©‚Ä≤ = ‚ü®x, y‚ü©,
where ‚ü®¬∑, ¬∑‚ü©‚Ä≤ is the standard inner product on Fn.
16. (a)
Bessel‚Äôs Inequality. Let V be an inner product space, and let S =
{v1, v2, . . . , vn} be an orthonormal subset of V. Prove that for any
x ‚ààV we have
‚à•x‚à•2 ‚â•
n

i=1
| ‚ü®x, vi‚ü©|2.
Hint: Apply Theorem 6.6 to x ‚ààV and W = span(S). Then use
Exercise 10 of Section 6.1.
(b)
In the context of (a), prove that Bessel‚Äôs inequality is an equality
if and only if x ‚ààspan(S).
17.
Let T be a linear operator on an inner product space V. If ‚ü®T(x), y‚ü©= 0
for all x, y ‚ààV, prove that T = T0. In fact, prove this result if the
equality holds for all x and y in some basis for V.
18.
Let V = C([‚àí1, 1]). Suppose that We and Wo denote the subspaces of V
consisting of the even and odd functions, respectively. (See Exercise 22

356
Chap. 6
Inner Product Spaces
of Section 1.3.) Prove that W‚ä•
e = Wo, where the inner product on V is
deÔ¨Åned by
‚ü®f, g‚ü©=
 1
‚àí1
f(t)g(t) dt.
19.
In each of the following parts, Ô¨Ånd the orthogonal projection of the
given vector on the given subspace W of the inner product space V.
(a)
V = R2, u = (2, 6), and W = {(x, y): y = 4x}.
(b)
V = R3, u = (2, 1, 3), and W = {(x, y, z): x + 3y ‚àí2z = 0}.
(c)
V = P(R) with the inner product ‚ü®f(x), g(x)‚ü©=
 1
0 f(t)g(t) dt,
h(x) = 4 + 3x ‚àí2x2, and W = P1(R).
20.
In each part of Exercise 19, Ô¨Ånd the distance from the given vector to
the subspace W.
21.
Let V = C([‚àí1, 1]) with the inner product ‚ü®f, g‚ü©=
 1
‚àí1 f(t)g(t) dt, and
let W be the subspace P2(R), viewed as a space of functions.
Use
the orthonormal basis obtained in Example 5 to compute the ‚Äúbest‚Äù
(closest) second-degree polynomial approximation of the function h(t) =
et on the interval [‚àí1, 1].
22.
Let V = C([0, 1]) with the inner product ‚ü®f, g‚ü©=
 1
0 f(t)g(t) dt. Let W
be the subspace spanned by the linearly independent set {t,
‚àö
t}.
(a)
Find an orthonormal basis for W.
(b)
Let h(t) = t2. Use the orthonormal basis obtained in (a) to obtain
the ‚Äúbest‚Äù (closest) approximation of h in W.
23.
Let V be the vector space deÔ¨Åned in Example 5 of Section 1.2, the
space of all sequences œÉ in F (where F = R or F = C) such that
œÉ(n) Ã∏= 0 for only Ô¨Ånitely many positive integers n. For œÉ, Œº ‚ààV, we
deÔ¨Åne ‚ü®œÉ, Œº‚ü©=
‚àû

n=1
œÉ(n)Œº(n). Since all but a Ô¨Ånite number of terms of
the series are zero, the series converges.
(a)
Prove that ‚ü®¬∑, ¬∑‚ü©is an inner product on V, and hence V is an inner
product space.
(b)
For each positive integer n, let en be the sequence deÔ¨Åned by
en(k) = Œ¥n,k, where Œ¥n,k is the Kronecker delta.
Prove that
{e1, e2, . . .} is an orthonormal basis for V.
(c)
Let œÉn = e1 + en and W = span({œÉn : n ‚â•2}.
(i) Prove that e1 /‚ààW, so W Ã∏= V.
(ii) Prove that W‚ä•= {0}, and conclude that W Ã∏= (W‚ä•)‚ä•.

Sec. 6.3
The Adjoint of a Linear Operator
357
Thus the assumption in Exercise 13(c) that W is Ô¨Ånite-dimensional
is essential.
6.3
THE ADJOINT OF A LINEAR OPERATOR
In Section 6.1, we deÔ¨Åned the conjugate transpose A‚àóof a matrix A. For
a linear operator T on an inner product space V, we now deÔ¨Åne a related
linear operator on V called the adjoint of T, whose matrix representation
with respect to any orthonormal basis Œ≤ for V is [T]‚àó
Œ≤. The analogy between
conjugation of complex numbers and adjoints of linear operators will become
apparent. We Ô¨Årst need a preliminary result.
Let V be an inner product space, and let y ‚ààV. The function g: V ‚ÜíF
deÔ¨Åned by g(x) = ‚ü®x, y‚ü©is clearly linear. More interesting is the fact that if
V is Ô¨Ånite-dimensional, every linear transformation from V into F is of this
form.
Theorem 6.8. Let V be a Ô¨Ånite-dimensional inner product space over F,
and let g: V ‚ÜíF be a linear transformation. Then there exists a unique
vector y ‚ààV such that g(x) = ‚ü®x, y‚ü©for all x ‚ààV.
Proof. Let Œ≤ = {v1, v2, . . . , vn} be an orthonormal basis for V, and let
y =
n

i=1
g(vi)vi.
DeÔ¨Åne h: V ‚ÜíF by h(x) = ‚ü®x, y‚ü©, which is clearly linear. Furthermore, for
1 ‚â§j ‚â§n we have
h(vj) = ‚ü®vj, y‚ü©=
1
vj,
n

i=1
g(vi)vi
2
=
n

i=1
g(vi) ‚ü®vj, vi‚ü©
=
n

i=1
g(vi)Œ¥ji = g(vj).
Since g and h both agree on Œ≤, we have that g = h by the corollary to
Theorem 2.6 (p. 73).
To show that y is unique, suppose that g(x) = ‚ü®x, y‚Ä≤‚ü©for all x. Then
‚ü®x, y‚ü©= ‚ü®x, y‚Ä≤‚ü©for all x; so by Theorem 6.1(e) (p. 333), we have y = y‚Ä≤.
Example 1
DeÔ¨Åne g: R2 ‚ÜíR by g(a1, a2) = 2a1+a2; clearly g is a linear transformation.
Let Œ≤ = {e1, e2}, and let y = g(e1)e1 + g(e2)e2 = 2e1 + e2 = (2, 1), as in the
proof of Theorem 6.8. Then g(a1, a2) = ‚ü®(a1, a2), (2, 1)‚ü©= 2a1 + a2.
‚ô¶

358
Chap. 6
Inner Product Spaces
Theorem 6.9. Let V be a Ô¨Ånite-dimensional inner product space, and let
T be a linear operator on V. Then there exists a unique function T‚àó: V ‚ÜíV
such that ‚ü®T(x), y‚ü©= ‚ü®x, T‚àó(y)‚ü©for all x, y ‚ààV. Furthermore, T‚àóis linear.
Proof. Let y ‚ààV. DeÔ¨Åne g: V ‚ÜíF by g(x) = ‚ü®T(x), y‚ü©for all x ‚ààV. We
Ô¨Årst show that g is linear. Let x1, x2 ‚ààV and c ‚ààF. Then
g(cx1 + x2) = ‚ü®T(cx1 + x2), y‚ü©= ‚ü®cT(x1) + T(x2), y‚ü©
= c ‚ü®T(x1), y‚ü©+ ‚ü®T(x2), y‚ü©= cg(x1) + g(x2).
Hence g is linear.
We now apply Theorem 6.8 to obtain a unique vector y‚Ä≤ ‚ààV such that
g(x) = ‚ü®x, y‚Ä≤‚ü©; that is, ‚ü®T(x), y‚ü©= ‚ü®x, y‚Ä≤‚ü©for all x ‚ààV. DeÔ¨Åning T‚àó: V ‚ÜíV
by T‚àó(y) = y‚Ä≤, we have ‚ü®T(x), y‚ü©= ‚ü®x, T‚àó(y)‚ü©.
To show that T‚àóis linear, let y1, y2 ‚ààV and c ‚ààF. Then for any x ‚ààV,
we have
‚ü®x, T‚àó(cy1 + y2)‚ü©= ‚ü®T(x), cy1 + y2‚ü©
= c ‚ü®T(x), y1‚ü©+ ‚ü®T(x), y2‚ü©
= c ‚ü®x, T‚àó(y1)‚ü©+ ‚ü®x, T‚àó(y2)‚ü©
= ‚ü®x, cT‚àó(y1) + T‚àó(y2)‚ü©.
Since x is arbitrary, T‚àó(cy1 + y2) = cT‚àó(y1) + T‚àó(y2) by Theorem 6.1(e)
(p. 333).
Finally, we need to show that T‚àóis unique.
Suppose that U: V ‚ÜíV
is linear and that it satisÔ¨Åes ‚ü®T(x), y‚ü©= ‚ü®x, U(y)‚ü©for all x, y ‚ààV.
Then
‚ü®x, T‚àó(y)‚ü©= ‚ü®x, U(y)‚ü©for all x, y ‚ààV, so T‚àó= U.
The linear operator T‚àódescribed in Theorem 6.9 is called the adjoint of
the operator T. The symbol T‚àóis read ‚ÄúT star.‚Äù
Thus T‚àóis the unique operator on V satisfying ‚ü®T(x), y‚ü©= ‚ü®x, T‚àó(y)‚ü©for
all x, y ‚ààV. Note that we also have
‚ü®x, T(y)‚ü©= ‚ü®T(y), x‚ü©= ‚ü®y, T‚àó(x)‚ü©= ‚ü®T‚àó(x), y‚ü©;
so ‚ü®x, T(y)‚ü©= ‚ü®T‚àó(x), y‚ü©for all x, y ‚ààV.
We may view these equations
symbolically as adding a * to T when shifting its position inside the inner
product symbol.
For an inÔ¨Ånite-dimensional inner product space, the adjoint of a linear op-
erator T may be deÔ¨Åned to be the function T‚àósuch that ‚ü®T(x), y‚ü©= ‚ü®x, T‚àó(y)‚ü©
for all x, y ‚ààV, provided it exists. Although the uniqueness and linearity of
T‚àófollow as before, the existence of the adjoint is not guaranteed (see Exer-
cise 24). The reader should observe the necessity of the hypothesis of Ô¨Ånite-
dimensionality in the proof of Theorem 6.8. Many of the theorems we prove

Sec. 6.3
The Adjoint of a Linear Operator
359
about adjoints, nevertheless, do not depend on V being Ô¨Ånite-dimensional.
Thus, unless stated otherwise, for the remainder of this chapter we adopt the
convention that a reference to the adjoint of a linear operator on an inÔ¨Ånite-
dimensional inner product space assumes its existence.
Theorem 6.10 is a useful result for computing adjoints.
Theorem 6.10. Let V be a Ô¨Ånite-dimensional inner product space, and
let Œ≤ be an orthonormal basis for V. If T is a linear operator on V, then
[T‚àó]Œ≤ = [T]‚àó
Œ≤.
Proof. Let A = [T]Œ≤, B = [T‚àó]Œ≤, and Œ≤ = {v1, v2, . . . , vn}. Then from the
corollary to Theorem 6.5 (p. 346), we have
Bij = ‚ü®T‚àó(vj), vi‚ü©= ‚ü®vi, T‚àó(vj)‚ü©= ‚ü®T(vi), vj‚ü©= Aji = (A‚àó)ij.
Hence B = A‚àó.
Corollary. Let A be an n √ó n matrix. Then LA‚àó= (LA)‚àó.
Proof. If Œ≤ is the standard ordered basis for Fn, then, by Theorem 2.16
(p. 93), we have [LA]Œ≤ = A. Hence [(LA)‚àó]Œ≤ = [LA]‚àó
Œ≤ = A‚àó= [LA‚àó]Œ≤, and so
(LA)‚àó= LA‚àó.
As an illustration of Theorem 6.10, we compute the adjoint of a speciÔ¨Åc
linear operator.
Example 2
Let T be the linear operator on C2 deÔ¨Åned by T(a1, a2) = (2ia1+3a2, a1‚àía2).
If Œ≤ is the standard ordered basis for C2, then
[T]Œ≤ =

2i
3
1
‚àí1
	
.
So
[T‚àó]Œ≤ = [T]‚àó
Œ≤ =

‚àí2i
1
3
‚àí1
	
.
Hence
T‚àó(a1, a2) = (‚àí2ia1 + a2, 3a1 ‚àía2).
‚ô¶
The following theorem suggests an analogy between the conjugates of
complex numbers and the adjoints of linear operators.
Theorem 6.11. Let V be an inner product space, and let T and U be
linear operators on V. Then

360
Chap. 6
Inner Product Spaces
(a) (T + U)‚àó= T‚àó+ U‚àó;
(b) (cT)‚àó= c T‚àófor any c ‚ààF;
(c) (TU)‚àó= U‚àóT‚àó;
(d) T‚àó‚àó= T;
(e) I‚àó= I.
Proof. We prove (a) and (d); the rest are proved similarly. Let x, y ‚ààV.
(a) Because
‚ü®x, (T + U)‚àó(y)‚ü©= ‚ü®(T + U)(x), y‚ü©= ‚ü®T(x) + U(x), y‚ü©
= ‚ü®T(x), y‚ü©+ ‚ü®U(x), y‚ü©= ‚ü®x, T‚àó(y)‚ü©+ ‚ü®x, U‚àó(y)‚ü©
= ‚ü®x, T‚àó(y) + U‚àó(y)‚ü©= ‚ü®x, (T‚àó+ U‚àó)(y)‚ü©,
T‚àó+ U‚àóhas the property unique to (T + U)‚àó. Hence T‚àó+ U‚àó= (T + U)‚àó.
(d) Similarly, since
‚ü®x, T(y)‚ü©= ‚ü®T‚àó(x), y‚ü©= ‚ü®x, T‚àó‚àó(y)‚ü©,
(d) follows.
The same proof works in the inÔ¨Ånite-dimensional case, provided that the
existence of T‚àóand U‚àóis assumed.
Corollary. Let A and B be n √ó n matrices. Then
(a) (A + B)‚àó= A‚àó+ B‚àó;
(b) (cA)‚àó= cA‚àófor all c ‚ààF;
(c) (AB)‚àó= B‚àóA‚àó;
(d) A‚àó‚àó= A;
(e) I‚àó= I.
Proof. We prove only (c); the remaining parts can be proved similarly.
Since L(AB)‚àó= (LAB)‚àó= (LALB)‚àó= (LB)‚àó(LA)‚àó= LB‚àóLA‚àó= LB‚àóA‚àó, we
have (AB)‚àó= B‚àóA‚àó.
In the preceding proof, we relied on the corollary to Theorem 6.10. An
alternative proof, which holds even for nonsquare matrices, can be given by
appealing directly to the deÔ¨Ånition of the conjugate transpose of a matrix
(see Exercise 5).
Least Squares Approximation
Consider the following problem: An experimenter collects data by taking
measurements y1, y2, . . . , ym at times t1, t2, . . . , tm, respectively. For example,
he or she may be measuring unemployment at various times during some
period.
Suppose that the data (t1, y1), (t2, y2), . . . , (tm, ym) are plotted as
points in the plane.
(See Figure 6.3.)
From this plot, the experimenter

Sec. 6.3
The Adjoint of a Linear Operator
361
feels that there exists an essentially linear relationship between y and t, say
y = ct + d, and would like to Ô¨Ånd the constants c and d so that the line
y = ct + d represents the best possible Ô¨Åt to the data collected. One such
estimate of Ô¨Åt is to calculate the error E that represents the sum of the
squares of the vertical distances from the points to the line; that is,
E =
m

i=1
(yi ‚àícti ‚àíd)2.
6
-
y
t

W
r
(t1, y1)
r
r
r
r
(ti, yi)
r
(ti, cti + d)
r
y = ct + d
Figure 6.3
Thus the problem is reduced to Ô¨Ånding the constants c and d that minimize
E. (For this reason the line y = ct + d is called the least squares line.) If
we let
A =
‚éõ
‚éú
‚éú
‚éú
‚éù
t1
1
t2
1
...
...
tm
1
‚éû
‚éü
‚éü
‚éü
‚é†,
x =

c
d
	
,
and
y =
‚éõ
‚éú
‚éú
‚éú
‚éù
y1
y2
...
ym
‚éû
‚éü
‚éü
‚éü
‚é†,
then it follows that E = ‚à•y ‚àíAx‚à•2.
We develop a general method for Ô¨Ånding an explicit vector x0 ‚ààFn that
minimizes E; that is, given an m √ó n matrix A, we Ô¨Ånd x0 ‚ààFn such that
‚à•y ‚àíAx0‚à•‚â§‚à•y ‚àíAx‚à•for all vectors x ‚ààFn. This method not only allows us
to Ô¨Ånd the linear function that best Ô¨Åts the data, but also, for any positive
integer n, the best Ô¨Åt using a polynomial of degree at most n.

362
Chap. 6
Inner Product Spaces
First, we need some notation and two simple lemmas. For x, y ‚ààFn, let
‚ü®x, y‚ü©n denote the standard inner product of x and y in Fn. Recall that if x
and y are regarded as column vectors, then ‚ü®x, y‚ü©n = y‚àóx.
Lemma 1. Let A ‚ààMm√ón(F), x ‚ààFn, and y ‚ààFm. Then
‚ü®Ax, y‚ü©m = ‚ü®x, A‚àóy‚ü©n .
Proof. By a generalization of the corollary to Theorem 6.11 (see Exer-
cise 5(b)), we have
‚ü®Ax, y‚ü©m = y‚àó(Ax) = (y‚àóA)x = (A‚àóy)‚àóx = ‚ü®x, A‚àóy‚ü©n .
Lemma 2. Let A ‚ààMm√ón(F). Then rank(A‚àóA) = rank(A).
Proof. By the dimension theorem, we need only show that, for x ‚ààFn,
we have A‚àóAx = 0 if and only if Ax = 0. Clearly, Ax = 0 implies that
A‚àóAx = 0. So assume that A‚àóAx = 0. Then
0 = ‚ü®A‚àóAx, x‚ü©n = ‚ü®Ax, A‚àó‚àóx‚ü©m = ‚ü®Ax, Ax‚ü©m ,
so that Ax = 0.
Corollary. If A is an m √ó n matrix such that rank(A) = n, then A‚àóA is
invertible.
Now let A be an m √ó n matrix and y ‚ààFm. DeÔ¨Åne W = {Ax: x ‚ààFn};
that is, W = R(LA). By the corollary to Theorem 6.6 (p. 350), there exists a
unique vector in W that is closest to y. Call this vector Ax0, where x0 ‚ààFn.
Then ‚à•Ax0 ‚àíy‚à•‚â§‚à•Ax ‚àíy‚à•for all x ‚ààFn; so x0 has the property that
E = ‚à•Ax0 ‚àíy‚à•is minimal, as desired.
To develop a practical method for Ô¨Ånding such an x0, we note from The-
orem 6.6 and its corollary that Ax0 ‚àíy ‚ààW‚ä•; so ‚ü®Ax, Ax0 ‚àíy‚ü©m = 0 for
all x ‚ààFn. Thus, by Lemma 1, we have that ‚ü®x, A‚àó(Ax0 ‚àíy)‚ü©n = 0 for all
x ‚ààFn; that is, A‚àó(Ax0 ‚àíy) = 0. So we need only Ô¨Ånd a solution x0 to
A‚àóAx = A‚àóy. If, in addition, we assume that rank(A) = n, then by Lemma 2
we have x0 = (A‚àóA)‚àí1A‚àóy. We summarize this discussion in the following
theorem.
Theorem 6.12. Let A ‚ààMm√ón(F) and y ‚ààFm.
Then there exists
x0 ‚ààFn such that (A‚àóA)x0 = A‚àóy and ‚à•Ax0 ‚àíy‚à•‚â§‚à•Ax ‚àíy‚à•for all x ‚ààFn.
Furthermore, if rank(A) = n, then x0 = (A‚àóA)‚àí1A‚àóy.

Sec. 6.3
The Adjoint of a Linear Operator
363
To return to our experimenter, let us suppose that the data collected are
(1, 2), (2, 3), (3, 5), and (4, 7). Then
A =
‚éõ
‚éú
‚éú
‚éù
1
1
2
1
3
1
4
1
‚éû
‚éü
‚éü
‚é†
and
y =
‚éõ
‚éú
‚éú
‚éù
2
3
5
7
‚éû
‚éü
‚éü
‚é†;
hence
A‚àóA =

1
2
3
4
1
1
1
1
	
‚éõ
‚éú
‚éú
‚éù
1
1
2
1
3
1
4
1
‚éû
‚éü
‚éü
‚é†=

30
10
10
4
	
.
Thus
(A‚àóA)‚àí1 = 1
20

4
‚àí10
‚àí10
30
	
.
Therefore

c
d
	
= x0 = 1
20

4
‚àí10
‚àí10
30
	 
1
2
3
4
1
1
1
1
	
‚éõ
‚éú
‚éú
‚éù
2
3
5
7
‚éû
‚éü
‚éü
‚é†=

1.7
0
	
.
It follows that the line y = 1.7t is the least squares line. The error E may be
computed directly as ‚à•Ax0 ‚àíy‚à•2 = 0.3.
Suppose that the experimenter chose the times ti (1 ‚â§i ‚â§m) to satisfy
m

i=1
ti = 0.
Then the two columns of A would be orthogonal, so A‚àóA would be a diagonal
matrix (see Exercise 19). In this case, the computations are greatly simpliÔ¨Åed.
In practice, the m √ó 2 matrix A in our least squares application has rank
equal to two, and hence A‚àóA is invertible by the corollary to Lemma 2. For,
otherwise, the Ô¨Årst column of A is a multiple of the second column, which
consists only of ones. But this would occur only if the experimenter collects
all the data at exactly one time.
Finally, the method above may also be applied if, for some k, the ex-
perimenter wants to Ô¨Åt a polynomial of degree at most k to the data. For
instance, if a polynomial y = ct2 + dt + e of degree at most 2 is desired, the
appropriate model is
x =
‚éõ
‚éù
c
d
e
‚éû
‚é†,
y =
‚éõ
‚éú
‚éú
‚éú
‚éù
y1
y2
...
ym
‚éû
‚éü
‚éü
‚éü
‚é†,
and
A =
‚éõ
‚éú
‚éù
t2
1
t1
1
...
...
...
t2
m
tm
1
‚éû
‚éü
‚é†.

364
Chap. 6
Inner Product Spaces
Minimal Solutions to Systems of Linear Equations
Even when a system of linear equations Ax = b is consistent, there may
be no unique solution. In such cases, it may be desirable to Ô¨Ånd a solution of
minimal norm. A solution s to Ax = b is called a minimal solution if ‚à•s‚à•‚â§
‚à•u‚à•for all other solutions u. The next theorem assures that every consistent
system of linear equations has a unique minimal solution and provides a
method for computing it.
Theorem 6.13. Let A ‚ààMm√ón(F) and b ‚ààFm. Suppose that Ax = b is
consistent. Then the following statements are true.
(a) There exists exactly one minimal solution s of Ax = b, and s ‚ààR(LA‚àó).
(b) The vector s is the only solution to Ax = b that lies in R(LA‚àó); that is,
if u satisÔ¨Åes (AA‚àó)u = b, then s = A‚àóu.
Proof. (a) For simplicity of notation, we let W = R(LA‚àó) and W‚Ä≤ = N(LA).
Let x be any solution to Ax = b. By Theorem 6.6 (p. 350), x = s + y for
some s ‚ààW and y ‚ààW‚ä•. But W‚ä•= W‚Ä≤ by Exercise 12, and therefore
b = Ax = As + Ay = As. So s is a solution to Ax = b that lies in W. To
prove (a), we need only show that s is the unique minimal solution. Let v be
any solution to Ax = b. By Theorem 3.9 (p. 172), we have that v = s + u,
where u ‚ààW‚Ä≤. Since s ‚ààW, which equals W‚Ä≤‚ä•by Exercise 12, we have
‚à•v‚à•2 = ‚à•s + u‚à•2 = ‚à•s‚à•2 + ‚à•u‚à•2 ‚â•‚à•s‚à•2
by Exercise 10 of Section 6.1. Thus s is a minimal solution. We can also see
from the preceding calculation that if ‚à•v‚à•= ‚à•s‚à•, then u = 0; hence v = s.
Therefore s is the unique minimal solution to Ax = b, proving (a).
(b) Assume that v is also a solution to Ax = b that lies in W. Then
v ‚àís ‚ààW ‚à©W‚Ä≤ = W ‚à©W‚ä•= {0};
so v = s.
Finally, suppose that (AA‚àó)u = b, and let v = A‚àóu. Then v ‚ààW and
Av = b. Therefore s = v = A‚àóu by the discussion above.
Example 3
Consider the system
x + 2y + z =
4
x ‚àíy + 2z = ‚àí11
x + 5y
=
19.
Let
A =
‚éõ
‚éù
1
2
1
1
‚àí1
2
1
5
0
‚éû
‚é†
and
b =
‚éõ
‚éù
4
‚àí11
19
‚éû
‚é†.

Sec. 6.3
The Adjoint of a Linear Operator
365
To Ô¨Ånd the minimal solution to this system, we must Ô¨Årst Ô¨Ånd some solution
u to AA‚àóx = b. Now
AA‚àó=
‚éõ
‚éù
6
1
11
1
6
‚àí4
11
‚àí4
26
‚éû
‚é†;
so we consider the system
6x + y + 11z =
4
x + 6y ‚àí4z = ‚àí11
11x ‚àí4y + 26z =
19,
for which one solution is
u =
‚éõ
‚éù
1
‚àí2
0
‚éû
‚é†.
(Any solution will suÔ¨Éce.) Hence
s = A‚àóu =
‚éõ
‚éù
‚àí1
4
‚àí3
‚éû
‚é†
is the minimal solution to the given system.
‚ô¶
EXERCISES
1.
Label the following statements as true or false. Assume that the under-
lying inner product spaces are Ô¨Ånite-dimensional.
(a)
Every linear operator has an adjoint.
(b)
Every linear operator on V has the form x ‚Üí‚ü®x, y‚ü©for some y ‚ààV.
(c)
For every linear operator T on V and every ordered basis Œ≤ for V,
we have [T‚àó]Œ≤ = ([T]Œ≤)‚àó.
(d)
The adjoint of a linear operator is unique.
(e)
For any linear operators T and U and scalars a and b,
(aT + bU)‚àó= aT‚àó+ bU‚àó.
(f)
For any n √ó n matrix A, we have (LA)‚àó= LA‚àó.
(g)
For any linear operator T, we have (T‚àó)‚àó= T.
2.
For each of the following inner product spaces V (over F) and linear
transformations g: V ‚ÜíF, Ô¨Ånd a vector y such that g(x) = ‚ü®x, y‚ü©for
all x ‚ààV.

366
Chap. 6
Inner Product Spaces
(a)
V = R3, g(a1, a2, a3) = a1 ‚àí2a2 + 4a3
(b)
V = C2, g(z1, z2) = z1 ‚àí2z2
(c)
V = P2(R) with ‚ü®f, h‚ü©=
 1
0
f(t)h(t) dt, g(f) = f(0) + f ‚Ä≤(1)
3.
For each of the following inner product spaces V and linear operators T
on V, evaluate T‚àóat the given vector in V.
(a)
V = R2, T(a, b) = (2a + b, a ‚àí3b), x = (3, 5).
(b)
V = C2, T(z1, z2) = (2z1 + iz2, (1 ‚àíi)z1), x = (3 ‚àíi, 1 + 2i).
(c)
V = P1(R) with ‚ü®f, g‚ü©=
 1
‚àí1
f(t)g(t) dt, T(f) = f ‚Ä≤ + 3f,
f(t) = 4 ‚àí2t
4.
Complete the proof of Theorem 6.11.
5. (a)
Complete the proof of the corollary to Theorem 6.11 by using
Theorem 6.11, as in the proof of (c).
(b)
State a result for nonsquare matrices that is analogous to the corol-
lary to Theorem 6.11, and prove it using a matrix argument.
6.
Let T be a linear operator on an inner product space V. Let U1 = T+T‚àó
and U2 = TT‚àó. Prove that U1 = U‚àó
1 and U2 = U‚àó
2.
7.
Give an example of a linear operator T on an inner product space V
such that N(T) Ã∏= N(T‚àó).
8.
Let V be a Ô¨Ånite-dimensional inner product space, and let T be a linear
operator on V. Prove that if T is invertible, then T‚àóis invertible and
(T‚àó)‚àí1 = (T‚àí1)‚àó.
9.
Prove that if V = W ‚äïW‚ä•and T is the projection on W along W‚ä•,
then T = T‚àó. Hint: Recall that N(T) = W‚ä•. (For deÔ¨Ånitions, see the
exercises of Sections 1.3 and 2.1.)
10.
Let T be a linear operator on an inner product space V. Prove that
‚à•T(x)‚à•= ‚à•x‚à•for all x ‚ààV if and only if ‚ü®T(x), T(y)‚ü©= ‚ü®x, y‚ü©for all
x, y ‚ààV. Hint: Use Exercise 20 of Section 6.1.
11.
For a linear operator T on an inner product space V, prove that T‚àóT =
T0 implies T = T0. Is the same result true if we assume that TT‚àó= T0?
12.
Let V be an inner product space, and let T be a linear operator on V.
Prove the following results.
(a)
R(T‚àó)‚ä•= N(T).
(b)
If V is Ô¨Ånite-dimensional, then R(T‚àó) = N(T)‚ä•. Hint: Use Exer-
cise 13(c) of Section 6.2.

Sec. 6.3
The Adjoint of a Linear Operator
367
13.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V. Prove
the following results.
(a)
N(T‚àóT) = N(T). Deduce that rank(T‚àóT) = rank(T).
(b)
rank(T) = rank(T‚àó). Deduce from (a) that rank(TT‚àó) = rank(T).
(c)
For any n √ó n matrix A, rank(A‚àóA) = rank(AA‚àó) = rank(A).
14.
Let V be an inner product space, and let y, z ‚ààV. DeÔ¨Åne T: V ‚ÜíV by
T(x) = ‚ü®x, y‚ü©z for all x ‚ààV. First prove that T is linear. Then show
that T‚àóexists, and Ô¨Ånd an explicit expression for it.
The following deÔ¨Ånition is used in Exercises 15‚Äì17 and is an extension of the
deÔ¨Ånition of the adjoint of a linear operator.
DeÔ¨Ånition.
Let T: V ‚ÜíW be a linear transformation, where V and W
are Ô¨Ånite-dimensional inner product spaces with inner products ‚ü®¬∑, ¬∑‚ü©1 and
‚ü®¬∑, ¬∑‚ü©2, respectively.
A function T‚àó: W ‚ÜíV is called an adjoint of T if
‚ü®T(x), y‚ü©2 = ‚ü®x, T‚àó(y)‚ü©1 for all x ‚ààV and y ‚ààW.
15.
Let T: V ‚ÜíW be a linear transformation, where V and W are Ô¨Ånite-
dimensional inner product spaces with inner products ‚ü®¬∑, ¬∑‚ü©1 and ‚ü®¬∑, ¬∑‚ü©2,
respectively. Prove the following results.
(a)
There is a unique adjoint T‚àóof T, and T‚àóis linear.
(b)
If Œ≤ and Œ≥ are orthonormal bases for V and W, respectively, then
[T‚àó]Œ≤
Œ≥ = ([T]Œ≥
Œ≤)‚àó.
(c)
rank(T‚àó) = rank(T).
(d)
‚ü®T‚àó(x), y‚ü©1 = ‚ü®x, T(y)‚ü©2 for all x ‚ààW and y ‚ààV.
(e)
For all x ‚ààV, T‚àóT(x) = 0 if and only if T(x) = 0.
16.
State and prove a result that extends the Ô¨Årst four parts of Theorem 6.11
using the preceding deÔ¨Ånition.
17.
Let T: V ‚ÜíW be a linear transformation, where V and W are Ô¨Ånite-
dimensional inner product spaces. Prove that (R(T‚àó))‚ä•= N(T), using
the preceding deÔ¨Ånition.
18.‚Ä† Let A be an n √ó n matrix. Prove that det(A‚àó) = det(A).
19.
Suppose that A is an m√ón matrix in which no two columns are identical.
Prove that A‚àóA is a diagonal matrix if and only if every pair of columns
of A is orthogonal.
20.
For each of the sets of data that follows, use the least squares approx-
imation to Ô¨Ånd the best Ô¨Åts with both (i) a linear function and (ii) a
quadratic function. Compute the error E in both cases.
(a)
{(‚àí3, 9), (‚àí2, 6), (0, 2), (1, 1)}

368
Chap. 6
Inner Product Spaces
(b)
{(1, 2), (3, 4), (5, 7), (7, 9), (9, 12)}
(c)
{(‚àí2, 4), (‚àí1, 3), (0, 1), (1, ‚àí1), (2, ‚àí3)}
21.
In physics, Hooke‚Äôs law states that (within certain limits) there is a
linear relationship between the length x of a spring and the force y
applied to (or exerted by) the spring. That is, y = cx + d, where c is
called the spring constant. Use the following data to estimate the
spring constant (the length is given in inches and the force is given in
pounds).
Length
Force
x
y
3.5
1.0
4.0
2.2
4.5
2.8
5.0
4.3
22.
Find the minimal solution to each of the following systems of linear
equations.
(a) x + 2y ‚àíz = 12
(b)
x + 2y ‚àíz = 1
2x + 3y + z = 2
4x + 7y ‚àíz = 4
(c)
x + y ‚àíz = 0
2x ‚àíy + z = 3
x ‚àíy + z = 2
(d)
x + y + z ‚àíw = 1
2x ‚àíy
+ w = 1
23.
Consider the problem of Ô¨Ånding the least squares line y = ct + d corre-
sponding to the m observations (t1, y1), (t2, y2), . . . , (tm, ym).
(a)
Show that the equation (A‚àóA)x0 = A‚àóy of Theorem 6.12 takes the
form of the normal equations:
 m

i=1
t2
i

c +
 m

i=1
ti

d =
m

i=1
tiyi
and
 m

i=1
ti

c + md =
m

i=1
yi.
These equations may also be obtained from the error E by setting
the partial derivatives of E with respect to both c and d equal to
zero.

Sec. 6.4
Normal and Self-Adjoint Operators
369
(b)
Use the second normal equation of (a) to show that the least
squares line must pass through the center of mass, (t, y), where
t = 1
m
m

i=1
ti
and
y = 1
m
m

i=1
yi.
24.
Let V and {e1, e2, . . .} be deÔ¨Åned as in Exercise 23 of Section 6.2. DeÔ¨Åne
T: V ‚ÜíV by
T(œÉ)(k) =
‚àû

i=k
œÉ(i)
for every positive integer k.
Notice that the inÔ¨Ånite series in the deÔ¨Ånition of T converges because
œÉ(i) Ã∏= 0 for only Ô¨Ånitely many i.
(a)
Prove that T is a linear operator on V.
(b)
Prove that for any positive integer n, T(en) = ,n
i=1 ei.
(c)
Prove that T has no adjoint.
Hint:
By way of contradiction,
suppose that T‚àóexists.
Prove that for any positive integer n,
T‚àó(en)(k) Ã∏= 0 for inÔ¨Ånitely many k.
6.4
NORMAL AND SELF-ADJOINT OPERATORS
We have seen the importance of diagonalizable operators in Chapter 5. For
these operators, it is necessary and suÔ¨Écient for the vector space V to possess
a basis of eigenvectors. As V is an inner product space in this chapter, it
is reasonable to seek conditions that guarantee that V has an orthonormal
basis of eigenvectors. A very important result that helps achieve our goal is
Schur‚Äôs theorem (Theorem 6.14). The formulation that follows is in terms of
linear operators. The next section contains the more familiar matrix form.
We begin with a lemma.
Lemma. Let T be a linear operator on a Ô¨Ånite-dimensional inner product
space V. If T has an eigenvector, then so does T‚àó.
Proof. Suppose that v is an eigenvector of T with corresponding eigenvalue
Œª. Then for any x ‚ààV,
0 = ‚ü®0, x‚ü©= ‚ü®(T ‚àíŒªI)(v), x‚ü©= ‚ü®v, (T ‚àíŒªI)‚àó(x)‚ü©=
8
v, (T ‚àó‚àíŒªI)(x)
9
,
and hence v is orthogonal to the range of T ‚àó‚àíŒªI. So T ‚àó‚àíŒªI is not onto
and hence is not one-to-one. Thus T ‚àó‚àíŒªI has a nonzero null space, and any
nonzero vector in this null space is an eigenvector of T‚àówith corresponding
eigenvalue Œª.

370
Chap. 6
Inner Product Spaces
Recall (see the exercises of Section 2.1 and see Section 5.4) that a subspace
W of V is said to be T-invariant if T(W) is contained in W. If W is T-
invariant, we may deÔ¨Åne the restriction TW : W ‚ÜíW by TW(x) = T(x) for all
x ‚ààW. It is clear that TW is a linear operator on W. Recall from Section 5.2
that a polynomial is said to split if it factors into linear polynomials.
Theorem 6.14 (Schur).
Let T be a linear operator on a Ô¨Ånite-
dimensional inner product space V.
Suppose that the characteristic poly-
nomial of T splits. Then there exists an orthonormal basis Œ≤ for V such that
the matrix [T]Œ≤ is upper triangular.
Proof. The proof is by mathematical induction on the dimension n of V.
The result is immediate if n = 1. So suppose that the result is true for linear
operators on (n ‚àí1)-dimensional inner product spaces whose characteristic
polynomials split. By the lemma, we can assume that T‚àóhas a unit eigen-
vector z. Suppose that T‚àó(z) = Œªz and that W = span({z}). We show that
W‚ä•is T-invariant. If y ‚ààW‚ä•and x = cz ‚ààW, then
‚ü®T(y), x‚ü©= ‚ü®T(y), cz‚ü©= ‚ü®y, T‚àó(cz)‚ü©= ‚ü®y, cT‚àó(z)‚ü©= ‚ü®y, cŒªz‚ü©
= cŒª ‚ü®y, z‚ü©= cŒª(0) = 0.
So T(y) ‚ààW‚ä•. It is easy to show (see Theorem 5.21 p. 314, or as a con-
sequence of Exercise 6 of Section 4.4) that the characteristic polynomial of
TW‚ä•divides the characteristic polynomial of T and hence splits. By Theo-
rem 6.7(c) (p. 352), dim(W‚ä•) = n ‚àí1, so we may apply the induction hy-
pothesis to TW‚ä•and obtain an orthonormal basis Œ≥ of W‚ä•such that [TW‚ä•]Œ≥
is upper triangular. Clearly, Œ≤ = Œ≥ ‚à™{z} is an orthonormal basis for V such
that [T]Œ≤ is upper triangular.
We now return to our original goal of Ô¨Ånding an orthonormal basis of
eigenvectors of a linear operator T on a Ô¨Ånite-dimensional inner product space
V. Note that if such an orthonormal basis Œ≤ exists, then [T]Œ≤ is a diagonal
matrix, and hence [T‚àó]Œ≤ = [T]‚àó
Œ≤ is also a diagonal matrix. Because diagonal
matrices commute, we conclude that T and T‚àócommute. Thus if V possesses
an orthonormal basis of eigenvectors of T, then TT‚àó= T‚àóT .
DeÔ¨Ånitions.
Let V be an inner product space, and let T be a linear
operator on V. We say that T is normal if TT‚àó= T‚àóT. An n √ó n real or
complex matrix A is normal if AA‚àó= A‚àóA.
It follows immediately from Theorem 6.10 (p. 359) that T is normal if and
only if [T]Œ≤ is normal, where Œ≤ is an orthonormal basis.

Sec. 6.4
Normal and Self-Adjoint Operators
371
Example 1
Let T: R2 ‚ÜíR2 be rotation by Œ∏, where 0 < Œ∏ < œÄ. The matrix representation
of T in the standard ordered basis is given by
A =

cos Œ∏
‚àísin Œ∏
sin Œ∏
cos Œ∏
	
.
Note that AA‚àó= I = A‚àóA; so A, and hence T, is normal.
‚ô¶
Example 2
Suppose that A is a real skew-symmetric matrix; that is, At = ‚àíA. Then A
is normal because both AAt and AtA are equal to ‚àíA2.
‚ô¶
Clearly, the operator T in Example 1 does not even possess one eigenvec-
tor. So in the case of a real inner product space, we see that normality is not
suÔ¨Écient to guarantee an orthonormal basis of eigenvectors. All is not lost,
however. We show that normality suÔ¨Éces if V is a complex inner product
space.
Before we prove the promised result for normal operators, we need some
general properties of normal operators.
Theorem 6.15. Let V be an inner product space, and let T be a normal
operator on V. Then the following statements are true.
(a) ‚à•T(x)‚à•= ‚à•T‚àó(x)‚à•for all x ‚ààV.
(b) T ‚àícI is normal for every c ‚ààF.
(c) If x is an eigenvector of T, then x is also an eigenvector of T‚àó. In fact,
if T(x) = Œªx, then T‚àó(x) = Œªx.
(d) If Œª1 and Œª2 are distinct eigenvalues of T with corresponding eigenvec-
tors x1 and x2, then x1 and x2 are orthogonal.
Proof. (a) For any x ‚ààV, we have
‚à•T(x)‚à•2 = ‚ü®T(x), T(x)‚ü©= ‚ü®T‚àóT(x), x‚ü©= ‚ü®TT‚àó(x), x‚ü©
= ‚ü®T‚àó(x), T‚àó(x)‚ü©= ‚à•T‚àó(x)‚à•2.
The proof of (b) is left as an exercise.
(c) Suppose that T(x) = Œªx for some x ‚ààV. Let U = T ‚àíŒªI. Then
U(x) = 0, and U is normal by (b). Thus (a) implies that
0 = ‚à•U(x)‚à•= ‚à•U‚àó(x)‚à•= ‚à•(T‚àó‚àíŒªI)(x)‚à•= ‚à•T‚àó(x) ‚àíŒªx‚à•.
Hence T‚àó(x) = Œªx. So x is an eigenvector of T‚àó.
(d) Let Œª1 and Œª2 be distinct eigenvalues of T with corresponding eigen-
vectors x1 and x2. Then, using (c), we have
Œª1 ‚ü®x1, x2‚ü©= ‚ü®Œª1x1, x2‚ü©= ‚ü®T(x1), x2‚ü©= ‚ü®x1, T‚àó(x2)‚ü©

372
Chap. 6
Inner Product Spaces
=
8
x1, Œª2x2
9
= Œª2 ‚ü®x1, x2‚ü©.
Since Œª1 Ã∏= Œª2, we conclude that ‚ü®x1, x2‚ü©= 0.
Theorem 6.16. Let T be a linear operator on a Ô¨Ånite-dimensional com-
plex inner product space V. Then T is normal if and only if there exists an
orthonormal basis for V consisting of eigenvectors of T.
Proof. Suppose that T is normal. By the fundamental theorem of algebra
(Theorem D.4), the characteristic polynomial of T splits. So we may apply
Schur‚Äôs theorem to obtain an orthonormal basis Œ≤ = {v1, v2, . . . , vn} for V
such that [T]Œ≤ = A is upper triangular. We know that v1 is an eigenvector
of T because A is upper triangular. Assume that v1, v2, . . . , vk‚àí1 are eigen-
vectors of T. We claim that vk is also an eigenvector of T. It then follows
by mathematical induction on k that all of the vi‚Äôs are eigenvectors of T.
Consider any j < k, and let Œªj denote the eigenvalue of T corresponding to
vj. By Theorem 6.15, T ‚àó(vj) = Œªjvj. Since A is upper triangular,
T(vk) = A1kv1 + A2kv2 + ¬∑ ¬∑ ¬∑ + Ajkvj + ¬∑ ¬∑ ¬∑ + Akkvk.
Furthermore, by the corollary to Theorem 6.5 (p. 347),
Ajk = ‚ü®T(vk), vj‚ü©= ‚ü®vk, T‚àó(vj)‚ü©=
8
vk, Œªjvj
9
= Œªj ‚ü®vk, vj‚ü©= 0.
It follows that T(vk) = Akkvk, and hence vk is an eigenvector of T. So by
induction, all the vectors in Œ≤ are eigenvectors of T.
The converse was already proved on page 370.
Interestingly, as the next example shows, Theorem 6.16 does not extend
to inÔ¨Ånite-dimensional complex inner product spaces.
Example 3
Consider the inner product space H with the orthonormal set S from Exam-
ple 9 in Section 6.1. Let V = span(S), and let T and U be the linear operators
on V deÔ¨Åned by T(f) = f1f and U(f) = f‚àí1f. Then
T(fn) = fn+1
and
U(fn) = fn‚àí1
for all integers n. Thus
‚ü®T(fm), fn‚ü©= ‚ü®fm+1, fn‚ü©= Œ¥(m+1),n = Œ¥m,(n‚àí1) = ‚ü®fm, fn‚àí1‚ü©= ‚ü®fm, U(fn)‚ü©.
It follows that U = T‚àó. Furthermore, TT‚àó= I = T‚àóT; so T is normal.
We show that T has no eigenvectors. Suppose that f is an eigenvector of
T, say, T(f) = Œªf for some Œª. Since V equals the span of S, we may write
f =
m

i=n
aifi,
where am Ã∏= 0.

Sec. 6.4
Normal and Self-Adjoint Operators
373
Hence
m

i=n
aifi+1 = T(f) = Œªf =
m

i=n
Œªaifi.
Since am Ã∏= 0, we can write fm+1 as a linear combination of fn, fn+1, . . . , fm.
But this is a contradiction because S is linearly independent.
‚ô¶
Example 1 illustrates that normality is not suÔ¨Écient to guarantee the
existence of an orthonormal basis of eigenvectors for real inner product spaces.
For real inner product spaces, we must replace normality by the stronger
condition that T = T‚àóin order to guarantee such a basis.
DeÔ¨Ånitions.
Let T be a linear operator on an inner product space V.
We say that T is self-adjoint (Hermitian) if T = T‚àó. An n √ó n real or
complex matrix A is self-adjoint (Hermitian) if A = A‚àó.
It follows immediately that if Œ≤ is an orthonormal basis, then T is self-
adjoint if and only if [T]Œ≤ is self-adjoint. For real matrices, this condition
reduces to the requirement that A be symmetric.
Before we state our main result for self-adjoint operators, we need some
preliminary work.
By deÔ¨Ånition, a linear operator on a real inner product space has only
real eigenvalues. The lemma that follows shows that the same can be said
for self-adjoint operators on a complex inner product space. Similarly, the
characteristic polynomial of every linear operator on a complex inner product
space splits, and the same is true for self-adjoint operators on a real inner
product space.
Lemma. Let T be a self-adjoint operator on a Ô¨Ånite-dimensional inner
product space V. Then
(a) Every eigenvalue of T is real.
(b) Suppose that V is a real inner product space. Then the characteristic
polynomial of T splits.
Proof. (a) Suppose that T(x) = Œªx for x Ã∏= 0.
Because a self-adjoint
operator is also normal, we can apply Theorem 6.15(c) to obtain
Œªx = T(x) = T‚àó(x) = Œªx.
So Œª = Œª; that is, Œª is real.
(b) Let n = dim(V), Œ≤ be an orthonormal basis for V, and A = [T]Œ≤.
Then A is self-adjoint.
Let TA be the linear operator on Cn deÔ¨Åned by
TA(x) = Ax for all x ‚ààCn. Note that TA is self-adjoint because [TA]Œ≥ = A,
where Œ≥ is the standard ordered (orthonormal) basis for Cn.
So, by (a),
the eigenvalues of TA are real. By the fundamental theorem of algebra, the

374
Chap. 6
Inner Product Spaces
characteristic polynomial of TA splits into factors of the form t‚àíŒª. Since each
Œª is real, the characteristic polynomial splits over R. But TA has the same
characteristic polynomial as A, which has the same characteristic polynomial
as T. Therefore the characteristic polynomial of T splits.
We are now able to establish one of the major results of this chapter.
Theorem 6.17. Let T be a linear operator on a Ô¨Ånite-dimensional real
inner product space V. Then T is self-adjoint if and only if there exists an
orthonormal basis Œ≤ for V consisting of eigenvectors of T.
Proof. Suppose that T is self-adjoint. By the lemma, we may apply Schur‚Äôs
theorem to obtain an orthonormal basis Œ≤ for V such that the matrix A = [T]Œ≤
is upper triangular. But
A‚àó= [T]‚àó
Œ≤ = [T‚àó]Œ≤ = [T]Œ≤ = A.
So A and A‚àóare both upper triangular, and therefore A is a diagonal matrix.
Thus Œ≤ must consist of eigenvectors of T.
The converse is left as an exercise.
Theorem 6.17 is used extensively in many areas of mathematics and statis-
tics. We restate this theorem in matrix form in the next section.
Example 4
As we noted earlier, real symmetric matrices are self-adjoint, and self-adjoint
matrices are normal. The following matrix A is complex and symmetric:
A =

i
i
i
1
	
and
A‚àó=

‚àíi
‚àíi
‚àíi
1
	
.
But A is not normal, because (AA‚àó)12 = 1+i and (A‚àóA)12 = 1‚àíi. Therefore
complex symmetric matrices need not be normal.
‚ô¶
EXERCISES
1.
Label the following statements as true or false. Assume that the under-
lying inner product spaces are Ô¨Ånite-dimensional.
(a)
Every self-adjoint operator is normal.
(b)
Operators and their adjoints have the same eigenvectors.
(c)
If T is an operator on an inner product space V, then T is normal
if and only if [T]Œ≤ is normal, where Œ≤ is any ordered basis for V.
(d)
A real or complex matrix A is normal if and only if LA is normal.
(e)
The eigenvalues of a self-adjoint operator must all be real.

Sec. 6.4
Normal and Self-Adjoint Operators
375
(f)
The identity and zero operators are self-adjoint.
(g)
Every normal operator is diagonalizable.
(h)
Every self-adjoint operator is diagonalizable.
2.
For each linear operator T on an inner product space V, determine
whether T is normal, self-adjoint, or neither. If possible, produce an
orthonormal basis of eigenvectors of T for V and list the corresponding
eigenvalues.
(a)
V = R2 and T is deÔ¨Åned by T(a, b) = (2a ‚àí2b, ‚àí2a + 5b).
(b)
V = R3 and T is deÔ¨Åned by T(a, b, c) = (‚àía + b, 5b, 4a ‚àí2b + 5c).
(c)
V = C2 and T is deÔ¨Åned by T(a, b) = (2a + ib, a + 2b).
(d)
V = P2(R) and T is deÔ¨Åned by T(f) = f ‚Ä≤, where
‚ü®f, g‚ü©=
 1
0
f(t)g(t) dt.
(e)
V = M2√ó2(R) and T is deÔ¨Åned by T(A) = At.
(f)
V = M2√ó2(R) and T is deÔ¨Åned by T

a
b
c
d
	
=

c
d
a
b
	
.
3.
Give an example of a linear operator T on R2 and an ordered basis for
R2 that provides a counterexample to the statement in Exercise 1(c).
4.
Let T and U be self-adjoint operators on an inner product space V.
Prove that TU is self-adjoint if and only if TU = UT.
5.
Prove (b) of Theorem 6.15.
6.
Let V be a complex inner product space, and let T be a linear operator
on V. DeÔ¨Åne
T1 = 1
2(T + T‚àó)
and
T2 = 1
2i(T ‚àíT‚àó).
(a)
Prove that T1 and T2 are self-adjoint and that T = T1 + i T2.
(b)
Suppose also that T = U1 + iU2, where U1 and U2 are self-adjoint.
Prove that U1 = T1 and U2 = T2.
(c)
Prove that T is normal if and only if T1T2 = T2T1.
7.
Let T be a linear operator on an inner product space V, and let W be
a T-invariant subspace of V. Prove the following results.
(a)
If T is self-adjoint, then TW is self-adjoint.
(b)
W‚ä•is T‚àó-invariant.
(c)
If W is both T- and T‚àó-invariant, then (TW)‚àó= (T‚àó)W.
(d)
If W is both T- and T‚àó-invariant and T is normal, then TW is
normal.

376
Chap. 6
Inner Product Spaces
8.
Let T be a normal operator on a Ô¨Ånite-dimensional complex inner
product space V, and let W be a subspace of V. Prove that if W is
T-invariant, then W is also T‚àó-invariant. Hint: Use Exercise 24 of Sec-
tion 5.4.
9.
Let T be a normal operator on a Ô¨Ånite-dimensional inner product space
V. Prove that N(T) = N(T‚àó) and R(T) = R(T‚àó). Hint: Use Theo-
rem 6.15 and Exercise 12 of Section 6.3.
10.
Let T be a self-adjoint operator on a Ô¨Ånite-dimensional inner product
space V. Prove that for all x ‚ààV
‚à•T(x) ¬± ix‚à•2 = ‚à•T(x)‚à•2 + ‚à•x‚à•2.
Deduce that T ‚àíiI is invertible and that [(T ‚àíiI)‚àí1]‚àó= (T + iI)‚àí1.
11.
Assume that T is a linear operator on a complex (not necessarily Ô¨Ånite-
dimensional) inner product space V with an adjoint T‚àó.
Prove the
following results.
(a)
If T is self-adjoint, then ‚ü®T(x), x‚ü©is real for all x ‚ààV.
(b)
If T satisÔ¨Åes ‚ü®T(x), x‚ü©= 0 for all x ‚ààV, then T = T0.
Hint:
Replace x by x + y and then by x + iy, and expand the resulting
inner products.
(c)
If ‚ü®T(x), x‚ü©is real for all x ‚ààV, then T = T‚àó.
12.
Let T be a normal operator on a Ô¨Ånite-dimensional real inner product
space V whose characteristic polynomial splits. Prove that V has an
orthonormal basis of eigenvectors of T.
Hence prove that T is self-
adjoint.
13.
An n√ón real matrix A is said to be a Gramian matrix if there exists a
real (square) matrix B such that A = BtB. Prove that A is a Gramian
matrix if and only if A is symmetric and all of its eigenvalues are non-
negative. Hint: Apply Theorem 6.17 to T = LA to obtain an orthonor-
mal basis {v1, v2, . . . , vn} of eigenvectors with the associated eigenvalues
Œª1, Œª2, . . . , Œªn. DeÔ¨Åne the linear operator U by U(vi) = ‚àöŒªivi.
14.
Simultaneous Diagonalization.
Let V be a Ô¨Ånite-dimensional real inner
product space, and let U and T be self-adjoint linear operators on V
such that UT = TU. Prove that there exists an orthonormal basis for
V consisting of vectors that are eigenvectors of both U and T. (The
complex version of this result appears as Exercise 10 of Section 6.6.)
Hint: For any eigenspace W = EŒª of T, we have that W is both T- and
U-invariant. By Exercise 7, we have that W‚ä•is both T- and U-invariant.
Apply Theorem 6.17 and Theorem 6.6 (p. 350).

Sec. 6.4
Normal and Self-Adjoint Operators
377
15.
Let A and B be symmetric n √ó n matrices such that AB = BA. Use
Exercise 14 to prove that there exists an orthogonal matrix P such that
P tAP and P tBP are both diagonal matrices.
16.
Prove the Cayley‚ÄìHamilton theorem for a complex n√ón matrix A. That
is, if f(t) is the characteristic polynomial of A, prove that f(A) = O.
Hint: Use Schur‚Äôs theorem to show that A may be assumed to be upper
triangular, in which case
f(t) =
n

i=1
(Aii ‚àít).
Now if T = LA, we have (AjjI ‚àíT)(ej) ‚ààspan({e1, e2, . . . , ej‚àí1}) for
j ‚â•2, where {e1, e2, . . . , en} is the standard ordered basis for Cn. (The
general case is proved in Section 5.4.)
The following deÔ¨Ånitions are used in Exercises 17 through 23.
DeÔ¨Ånitions.
A linear operator T on a Ô¨Ånite-dimensional inner product
space is called positive deÔ¨Ånite [positive semideÔ¨Ånite] if T is self-adjoint
and ‚ü®T(x), x‚ü©> 0 [‚ü®T(x), x‚ü©‚â•0] for all x Ã∏= 0.
An n √ó n matrix A with entries from R or C is called positive deÔ¨Ånite
[positive semideÔ¨Ånite] if LA is positive deÔ¨Ånite [positive semideÔ¨Ånite].
17.
Let T and U be a self-adjoint linear operators on an n-dimensional inner
product space V, and let A = [T]Œ≤, where Œ≤ is an orthonormal basis for
V. Prove the following results.
(a)
T is positive deÔ¨Ånite [semideÔ¨Ånite] if and only if all of its eigenval-
ues are positive [nonnegative].
(b)
T is positive deÔ¨Ånite if and only if

i,j
Aijajai > 0 for all nonzero n-tuples (a1, a2, . . . , an).
(c)
T is positive semideÔ¨Ånite if and only if A = B‚àóB for some square
matrix B.
(d)
If T and U are positive semideÔ¨Ånite operators such that T2 = U2,
then T = U.
(e)
If T and U are positive deÔ¨Ånite operators such that TU = UT, then
TU is positive deÔ¨Ånite.
(f)
T is positive deÔ¨Ånite [semideÔ¨Ånite] if and only if A is positive def-
inite [semideÔ¨Ånite].
Because of (f), results analogous to items (a) through (d) hold for ma-
trices as well as operators.

378
Chap. 6
Inner Product Spaces
18.
Let T: V ‚ÜíW be a linear transformation, where V and W are Ô¨Ånite-
dimensional inner product spaces. Prove the following results.
(a)
T‚àóT and TT‚àóare positive semideÔ¨Ånite. (See Exercise 15 of Sec-
tion 6.3.)
(b)
rank(T‚àóT) = rank(TT‚àó) = rank(T).
19.
Let T and U be positive deÔ¨Ånite operators on an inner product space
V. Prove the following results.
(a)
T + U is positive deÔ¨Ånite.
(b)
If c > 0, then cT is positive deÔ¨Ånite.
(c)
T‚àí1 is positive deÔ¨Ånite.
20.
Let V be an inner product space with inner product ‚ü®¬∑, ¬∑‚ü©, and let T be
a positive deÔ¨Ånite linear operator on V. Prove that ‚ü®x, y‚ü©‚Ä≤ = ‚ü®T(x), y‚ü©
deÔ¨Ånes another inner product on V.
21.
Let V be a Ô¨Ånite-dimensional inner product space, and let T and U be
self-adjoint operators on V such that T is positive deÔ¨Ånite. Prove that
both TU and UT are diagonalizable linear operators that have only real
eigenvalues. Hint: Show that UT is self-adjoint with respect to the inner
product ‚ü®x, y‚ü©‚Ä≤ = ‚ü®T(x), y‚ü©. To show that TU is self-adjoint, repeat the
argument with T‚àí1 in place of T.
22.
This exercise provides a converse to Exercise 20.
Let V be a Ô¨Ånite-
dimensional inner product space with inner product ‚ü®¬∑, ¬∑‚ü©, and let ‚ü®¬∑, ¬∑‚ü©‚Ä≤
be any other inner product on V.
(a)
Prove that there exists a unique linear operator T on V such
that ‚ü®x, y‚ü©‚Ä≤ = ‚ü®T(x), y‚ü©for all x and y in V.
Hint:
Let Œ≤ =
{v1, v2, . . . , vn} be an orthonormal basis for V with respect to
‚ü®¬∑, ¬∑‚ü©, and deÔ¨Åne a matrix A by Aij = ‚ü®vj, vi‚ü©‚Ä≤ for all i and j.
Let T be the unique linear operator on V such that [T]Œ≤ = A.
(b)
Prove that the operator T of (a) is positive deÔ¨Ånite with respect
to both inner products.
23.
Let U be a diagonalizable linear operator on a Ô¨Ånite-dimensional inner
product space V such that all of the eigenvalues of U are real. Prove that
there exist positive deÔ¨Ånite linear operators T1 and T‚Ä≤
1 and self-adjoint
linear operators T2 and T‚Ä≤
2 such that U = T2T1 = T‚Ä≤
1T‚Ä≤
2. Hint: Let ‚ü®¬∑, ¬∑‚ü©
be the inner product associated with V, Œ≤ a basis of eigenvectors for U,
‚ü®¬∑, ¬∑‚ü©‚Ä≤ the inner product on V with respect to which Œ≤ is orthonormal
(see Exercise 22(a) of Section 6.1), and T1 the positive deÔ¨Ånite operator
according to Exercise 22. Show that U is self-adjoint with respect to
‚ü®¬∑, ¬∑‚ü©‚Ä≤ and U = T‚àí1
1 U‚àóT1 (the adjoint is with respect to ‚ü®¬∑, ¬∑‚ü©). Let
T2 = T1
‚àí1U‚àó.

Sec. 6.5
Unitary and Orthogonal Operators and Their Matrices
379
24.
This argument gives another proof of Schur‚Äôs theorem. Let T be a linear
operator on a Ô¨Ånite dimensional inner product space V.
(a)
Suppose that Œ≤ is an ordered basis for V such that [T]Œ≤ is an upper
triangular matrix. Let Œ≥ be the orthonormal basis for V obtained
by applying the Gram‚ÄìSchmidt orthogonalization process to Œ≤ and
then normalizing the resulting vectors. Prove that [T]Œ≥ is an upper
triangular matrix.
(b)
Use Exercise 32 of Section 5.4 and (a) to obtain an alternate proof
of Schur‚Äôs theorem.
6.5
UNITARY AND ORTHOGONAL OPERATORS
AND THEIR MATRICES
In this section, we continue our analogy between complex numbers and linear
operators. Recall that the adjoint of a linear operator acts similarly to the
conjugate of a complex number (see, for example, Theorem 6.11 p. 359). A
complex number z has length 1 if zz = 1. In this section, we study those
linear operators T on an inner product space V such that TT‚àó= T‚àóT = I. We
will see that these are precisely the linear operators that ‚Äúpreserve length‚Äù
in the sense that ‚à•T(x)‚à•= ‚à•x‚à•for all x ‚ààV. As another characterization,
we prove that, on a Ô¨Ånite-dimensional complex inner product space, these are
the normal operators whose eigenvalues all have absolute value 1.
In past chapters, we were interested in studying those functions that pre-
serve the structure of the underlying space. In particular, linear operators
preserve the operations of vector addition and scalar multiplication, and iso-
morphisms preserve all the vector space structure. It is now natural to con-
sider those linear operators T on an inner product space that preserve length.
We will see that this condition guarantees, in fact, that T preserves the inner
product.
DeÔ¨Ånitions.
Let T be a linear operator on a Ô¨Ånite-dimensional inner
product space V (over F). If ‚à•T(x)‚à•= ‚à•x‚à•for all x ‚ààV, we call T a unitary
operator if F = C and an orthogonal operator if F = R.
It should be noted that, in the inÔ¨Ånite-dimensional case, an operator sat-
isfying the preceding norm requirement is generally called an isometry. If,
in addition, the operator is onto (the condition guarantees one-to-one), then
the operator is called a unitary or orthogonal operator.
Clearly, any rotation or reÔ¨Çection in R2 preserves length and hence is
an orthogonal operator. We study these operators in much more detail in
Section 6.11.

380
Chap. 6
Inner Product Spaces
Example 1
Let h ‚ààH satisfy |h(x)| = 1 for all x. DeÔ¨Åne the linear operator T on H by
T(f) = hf. Then
‚à•T(f)‚à•2 = ‚à•hf‚à•2 = 1
2œÄ
 2œÄ
0
h(t)f(t)h(t)f(t) dt = ‚à•f‚à•2
since |h(t)|2 = 1 for all t. So T is a unitary operator.
‚ô¶
Theorem 6.18. Let T be a linear operator on a Ô¨Ånite-dimensional inner
product space V. Then the following statements are equivalent.
(a) TT‚àó= T‚àóT = I.
(b) ‚ü®T(x), T(y)‚ü©= ‚ü®x, y‚ü©for all x, y ‚ààV.
(c) If Œ≤ is an orthonormal basis for V, then T(Œ≤) is an orthonormal basis
for V.
(d) There exists an orthonormal basis Œ≤ for V such that T(Œ≤) is an orthonor-
mal basis for V.
(e) ‚à•T(x)‚à•= ‚à•x‚à•for all x ‚ààV.
Thus all the conditions above are equivalent to the deÔ¨Ånition of a uni-
tary or orthogonal operator. From (a), it follows that unitary or orthogonal
operators are normal.
Before proving the theorem, we Ô¨Årst prove a lemma. Compare this lemma
to Exercise 11(b) of Section 6.4.
Lemma. Let U be a self-adjoint operator on a Ô¨Ånite-dimensional inner
product space V. If ‚ü®x, U(x)‚ü©= 0 for all x ‚ààV, then U = T0.
Proof. By either Theorem 6.16 (p. 372) or 6.17 (p. 374), we may choose
an orthonormal basis Œ≤ for V consisting of eigenvectors of U. If x ‚ààŒ≤, then
U(x) = Œªx for some Œª. Thus
0 = ‚ü®x, U(x)‚ü©= ‚ü®x, Œªx‚ü©= Œª ‚ü®x, x‚ü©;
so Œª = 0. Hence U(x) = 0 for all x ‚ààŒ≤, and thus U = T0.
Proof of Theorem 6.18. We prove Ô¨Årst that (a) implies (b). Let x, y ‚ààV.
Then ‚ü®x, y‚ü©= ‚ü®T‚àóT(x), y‚ü©= ‚ü®T(x), T(y)‚ü©.
Second, we prove that (b) implies (c).
Let Œ≤ = {v1, v2, . . . , vn} be an
orthonormal basis for V; so T(Œ≤) = {T(v1), T(v2), . . . , T(vn)}. It follows that
‚ü®T(vi), T(vj)‚ü©= ‚ü®vi, vj‚ü©= Œ¥ij. Therefore T(Œ≤) is an orthonormal basis for V.
That (c) implies (d) is obvious.
Next we prove that (d) implies (e). Let x ‚ààV, and let Œ≤ = {v1, v2, . . . , vn}.
Now
x =
n

i=1
aivi

Sec. 6.5
Unitary and Orthogonal Operators and Their Matrices
381
for some scalars ai, and so
‚à•x‚à•2 =
1 n

i=1
aivi,
n

j=1
ajvj
2
=
n

i=1
n

j=1
aiaj ‚ü®vi, vj‚ü©
=
n

i=1
n

j=1
aiajŒ¥ij =
n

i=1
|ai|2
since Œ≤ is orthonormal.
Applying the same manipulations to
T(x) =
n

i=1
aiT(vi)
and using the fact that T(Œ≤) is also orthonormal, we obtain
‚à•T(x)‚à•2 =
n

i=1
|ai|2.
Hence ‚à•T(x)‚à•= ‚à•x‚à•.
Finally, we prove that (e) implies (a). For any x ‚ààV, we have
‚ü®x, x‚ü©= ‚à•x‚à•2 = ‚à•T(x)‚à•2 = ‚ü®T(x), T(x)‚ü©= ‚ü®x, T‚àóT(x)‚ü©.
So ‚ü®x, (I ‚àíT‚àóT)(x)‚ü©= 0 for all x ‚ààV. Let U = I ‚àíT‚àóT; then U is self-
adjoint, and ‚ü®x, U(x)‚ü©= 0 for all x ‚ààV. Hence, by the lemma, we have
T0 = U = I ‚àíT‚àóT, and therefore T‚àóT = I. Since V is Ô¨Ånite-dimensional, we
may use Exercise 10 of Section 2.4 to conclude that TT‚àó= I.
It follows immediately from the deÔ¨Ånition that every eigenvalue of a uni-
tary or orthogonal operator has absolute value 1. In fact, even more is true.
Corollary 1.
Let T be a linear operator on a Ô¨Ånite-dimensional real
inner product space V. Then V has an orthonormal basis of eigenvectors of
T with corresponding eigenvalues of absolute value 1 if and only if T is both
self-adjoint and orthogonal.
Proof. Suppose that V has an orthonormal basis {v1, v2, . . . , vn} such that
T(vi) = Œªivi and |Œªi| = 1 for all i. By Theorem 6.17 (p. 374), T is self-adjoint.
Thus (TT‚àó)(vi) = T(Œªivi) = ŒªiŒªivi = Œª2
i vi = vi for each i. So TT‚àó= I, and
again by Exercise 10 of Section 2.4, T is orthogonal by Theorem 6.18(a).
If T is self-adjoint, then, by Theorem 6.17, we have that V possesses an
orthonormal basis {v1, v2, . . . , vn} such that T(vi) = Œªivi for all i. If T is also
orthogonal, we have
|Œªi|¬∑‚à•vi‚à•= ‚à•Œªivi‚à•= ‚à•T(vi)‚à•= ‚à•vi‚à•;
so |Œªi| = 1 for every i.

382
Chap. 6
Inner Product Spaces
Corollary 2. Let T be a linear operator on a Ô¨Ånite-dimensional complex
inner product space V. Then V has an orthonormal basis of eigenvectors of T
with corresponding eigenvalues of absolute value 1 if and only if T is unitary.
Proof. The proof is similar to the proof of Corollary 1.
Example 2
Let T: R2 ‚ÜíR2 be a rotation by Œ∏, where 0 < Œ∏ < œÄ. It is clear geometrically
that T ‚Äúpreserves length‚Äù, that is, that ‚à•T(x)‚à•= ‚à•x‚à•for all x ‚ààR2. The
fact that rotations by a Ô¨Åxed angle preserve perpendicularity not only can be
seen geometrically but now follows from (b) of Theorem 6.18. Perhaps the
fact that such a transformation preserves the inner product is not so obvious;
however, we obtain this fact from (b) also. Finally, an inspection of the matrix
representation of T with respect to the standard ordered basis, which is

cos Œ∏
‚àísin Œ∏
sin Œ∏
cos Œ∏
	
,
reveals that T is not self-adjoint for the given restriction on Œ∏. As we men-
tioned earlier, this fact also follows from the geometric observation that T
has no eigenvectors and from Theorem 6.15 (p. 371). It is seen easily from
the preceding matrix that T‚àóis the rotation by ‚àíŒ∏.
‚ô¶
DeÔ¨Ånition. Let L be a one-dimensional subspace of R2. We may view L
as a line in the plane through the origin. A linear operator T on R2 is called
a reÔ¨Çection of R2 about L if T(x) = x for all x ‚ààL and T(x) = ‚àíx for all
x ‚ààL‚ä•.
As an example of a reÔ¨Çection, consider the operator deÔ¨Åned in Example 3 of
Section 2.5.
Example 3
Let T be a reÔ¨Çection of R2 about a line L through the origin. We show that
T is an orthogonal operator. Select vectors v1 ‚ààL and v2 ‚ààL‚ä•such that
‚à•v1‚à•= ‚à•v2‚à•= 1.
Then T(v1) = v1 and T(v2) = ‚àív2.
Thus v1 and v2
are eigenvectors of T with corresponding eigenvalues 1 and ‚àí1, respectively.
Furthermore, {v1, v2} is an orthonormal basis for R2. It follows that T is an
orthogonal operator by Corollary 1 to Theorem 6.18.
‚ô¶
We now examine the matrices that represent unitary and orthogonal trans-
formations.
DeÔ¨Ånitions.
A square matrix A is called an an orthogonal matrix if
AtA = AAt = I and unitary if A‚àóA = AA‚àó= I.

Sec. 6.5
Unitary and Orthogonal Operators and Their Matrices
383
Since for a real matrix A we have A‚àó= At, a real unitary matrix is also
orthogonal. In this case, we call A orthogonal rather than unitary.
Note that the condition AA‚àó= I is equivalent to the statement that the
rows of A form an orthonormal basis for Fn because
Œ¥ij = Iij = (AA‚àó)ij =
n

k=1
Aik(A‚àó)kj =
n

k=1
AikAjk,
and the last term represents the inner product of the ith and jth rows of A.
A similar remark can be made about the columns of A and the condition
A‚àóA = I.
It also follows from the deÔ¨Ånition above and from Theorem 6.10 (p. 359)
that a linear operator T on an inner product space V is unitary [orthogonal]
if and only if [T]Œ≤ is unitary [orthogonal] for some orthonormal basis Œ≤ for V.
Example 4
From Example 2, the matrix

cos Œ∏
‚àísin Œ∏
sin Œ∏
cos Œ∏
	
is clearly orthogonal. One can easily see that the rows of the matrix form
an orthonormal basis for R2. Similarly, the columns of the matrix form an
orthonormal basis for R2.
‚ô¶
Example 5
Let T be a reÔ¨Çection of R2 about a line L through the origin, let Œ≤ be the
standard ordered basis for R2, and let A = [T]Œ≤. Then T = LA. Since T is
an orthogonal operator and Œ≤ is an orthonormal basis, A is an orthogonal
matrix. We describe A.
Suppose that Œ± is the angle from the positive x-axis to L.
Let v1 =
(cos Œ±, sin Œ±) and v2 = (‚àísin Œ±, cos Œ±).
Then ‚à•v1‚à•= ‚à•v2‚à•= 1, v1 ‚ààL,
and v2 ‚ààL‚ä•. Hence Œ≥ = {v1, v2} is an orthonormal basis for R2. Because
T(v1) = v1 and T(v2) = ‚àív2, we have
[T]Œ≥ = [LA]Œ≥ =

1
0
0
‚àí1
	
.
Let
Q =
cos Œ±
‚àísin Œ±
sin Œ±
cos Œ±
	
.
By the corollary to Theorem 2.23 (p. 115),
A = Q[LA]Œ≥Q‚àí1

384
Chap. 6
Inner Product Spaces
=

cos Œ±
‚àísin Œ±
sin Œ±
cos Œ±
	 
1
0
0
‚àí1
	 
cos Œ±
sin Œ±
‚àísin Œ±
cos Œ±
	
=

cos2 Œ± ‚àísin2 Œ±
2 sin Œ± cos Œ±
2 sin Œ± cos Œ±
‚àí(cos2 Œ± ‚àísin2 Œ±)
	
=

cos 2Œ±
sin 2Œ±
sin 2Œ±
‚àícos 2Œ±
	
.
‚ô¶
We know that, for a complex normal [real symmetric] matrix A, there
exists an orthonormal basis Œ≤ for Fn consisting of eigenvectors of A. Hence A
is similar to a diagonal matrix D. By the corollary to Theorem 2.23 (p. 115),
the matrix Q whose columns are the vectors in Œ≤ is such that D = Q‚àí1AQ.
But since the columns of Q are an orthonormal basis for Fn, it follows that Q
is unitary [orthogonal]. In this case, we say that A is unitarily equivalent
[orthogonally equivalent] to D. It is easily seen (see Exercise 18) that this
relation is an equivalence relation on Mn√ón(C) [Mn√ón(R)]. More generally,
A and B are unitarily equivalent [orthogonally equivalent] if and only if there
exists a unitary [orthogonal] matrix P such that A = P ‚àóBP.
The preceding paragraph has proved half of each of the next two theo-
rems.
Theorem 6.19. Let A be a complex n √ó n matrix. Then A is normal if
and only if A is unitarily equivalent to a diagonal matrix.
Proof. By the preceding remarks, we need only prove that if A is unitarily
equivalent to a diagonal matrix, then A is normal.
Suppose that A = P ‚àóDP, where P is a unitary matrix and D is a diagonal
matrix. Then
AA‚àó= (P ‚àóDP)(P ‚àóDP)‚àó= (P ‚àóDP)(P ‚àóD‚àóP) = P ‚àóDID‚àóP = P ‚àóDD‚àóP.
Similarly, A‚àóA = P ‚àóD‚àóDP. Since D is a diagonal matrix, however, we have
DD‚àó= D‚àóD. Thus AA‚àó= A‚àóA.
Theorem 6.20. Let A be a real n √ó n matrix. Then A is symmetric if
and only if A is orthogonally equivalent to a real diagonal matrix.
Proof. The proof is similar to the proof of Theorem 6.19 and is left as an
exercise.
Example 6
Let
A =
‚éõ
‚éù
4
2
2
2
4
2
2
2
4
‚éû
‚é†.

Sec. 6.5
Unitary and Orthogonal Operators and Their Matrices
385
Since A is symmetric, Theorem 6.20 tells us that A is orthogonally equivalent
to a diagonal matrix. We Ô¨Ånd an orthogonal matrix P and a diagonal matrix
D such that P tAP = D.
To Ô¨Ånd P, we obtain an orthonormal basis of eigenvectors. It is easy to
show that the eigenvalues of A are 2 and 8. The set {(‚àí1, 1, 0), (‚àí1, 0, 1)}
is a basis for the eigenspace corresponding to 2.
Because this set is not
orthogonal, we apply the Gram‚ÄìSchmidt process to obtain the orthogonal
set {(‚àí1, 1, 0), ‚àí1
2(1, 1, ‚àí2)}. The set {(1, 1, 1)} is a basis for the eigenspace
corresponding to 8. Notice that (1, 1, 1) is orthogonal to the preceding two
vectors, as predicted by Theorem 6.15(d) (p. 371). Taking the union of these
two bases and normalizing the vectors, we obtain the following orthonormal
basis for R3 consisting of eigenvectors of A:
 1
‚àö
2(‚àí1, 1, 0), 1
‚àö
6(1, 1, ‚àí2), 1
‚àö
3(1, 1, 1)

.
Thus one possible choice for P is
P =
‚éõ
‚éú
‚éú
‚éù
‚àí1
‚àö
2
1
‚àö
6
1
‚àö
3
1
‚àö
2
1
‚àö
6
1
‚àö
3
0
‚àí2
‚àö
6
1
‚àö
3
‚éû
‚éü
‚éü
‚é†,
and
D =
‚éõ
‚éù
2
0
0
0
2
0
0
0
8
‚éû
‚é†.
‚ô¶
Because of Schur‚Äôs theorem (Theorem 6.14 p. 370), the next result is
immediate. As it is the matrix form of Schur‚Äôs theorem, we also refer to it as
Schur‚Äôs theorem.
Theorem 6.21 (Schur). Let A ‚ààMn√ón(F) be a matrix whose charac-
teristic polynomial splits over F.
(a) If F = C, then A is unitarily equivalent to a complex upper triangular
matrix.
(b) If F = R, then A is orthogonally equivalent to a real upper triangular
matrix.
Rigid Motions*
The purpose of this application is to characterize the so-called rigid mo-
tions of a Ô¨Ånite-dimensional real inner product space. One may think intu-
itively of such a motion as a transformation that does not aÔ¨Äect the shape of
a Ô¨Ågure under its action, hence the term rigid. The key requirement for such
a transformation is that it preserves distances.
DeÔ¨Ånition.
Let V be a real inner product space. A function f : V ‚ÜíV
is called a rigid motion if
‚à•f(x) ‚àíf(y)‚à•= ‚à•x ‚àíy‚à•

386
Chap. 6
Inner Product Spaces
for all x, y ‚ààV.
For example, any orthogonal operator on a Ô¨Ånite-dimensional real inner
product space is a rigid motion.
Another class of rigid motions are the translations. A function g: V ‚ÜíV,
where V is a real inner product space, is called a translation if there exists
a vector v0 ‚ààV such that g(x) = x + v0 for all x ‚ààV. We say that g is
the translation by v0. It is a simple exercise to show that translations, as
well as composites of rigid motions on a real inner product space, are also
rigid motions. (See Exercise 22.) Thus an orthogonal operator on a Ô¨Ånite-
dimensional real inner product space V followed by a translation on V is a
rigid motion on V. Remarkably, every rigid motion on V may be characterized
in this way.
Theorem 6.22. Let f : V ‚ÜíV be a rigid motion on a Ô¨Ånite-dimensional
real inner product space V. Then there exists a unique orthogonal operator
T on V and a unique translation g on V such that f = g ‚ó¶T.
Any orthogonal operator is a special case of this composite, in which
the translation is by 0. Any translation is also a special case, in which the
orthogonal operator is the identity operator.
Proof. Let T: V ‚ÜíV be deÔ¨Åned by
T(x) = f(x) ‚àíf(0)
for all x ‚ààV.
We show that T is an orthogonal operator, from which it
follows that f = g ‚ó¶T, where g is the translation by f(0). Observe that T is
the composite of f and the translation by ‚àíf(0); hence T is a rigid motion.
Furthermore, for any x ‚ààV
‚à•T(x)‚à•2 = ‚à•f(x) ‚àíf(0)‚à•2 = ‚à•x ‚àí0‚à•2 = ‚à•x‚à•2,
and consequently ‚à•T(x)‚à•= ‚à•x‚à•for any x ‚ààV. Thus for any x, y ‚ààV,
‚à•T(x) ‚àíT(y)‚à•2 = ‚à•T(x)‚à•2 ‚àí2 ‚ü®T(x), T(y)‚ü©+ ‚à•T(y)‚à•2
= ‚à•x‚à•2 ‚àí2 ‚ü®T(x), T(y)‚ü©+ ‚à•y‚à•2
and
‚à•x ‚àíy‚à•2 = ‚à•x‚à•2 ‚àí2 ‚ü®x, y‚ü©+ ‚à•y‚à•2.
But ‚à•T(x) ‚àíT(y)‚à•2 = ‚à•x ‚àíy‚à•2; so ‚ü®T(x), T(y)‚ü©= ‚ü®x, y‚ü©for all x, y ‚ààV.
We are now in a position to show that T is a linear transformation. Let
x, y ‚ààV, and let a ‚ààR. Then
‚à•T(x + ay) ‚àíT(x) ‚àíaT(y)‚à•2 = ‚à•[T(x + ay) ‚àíT(x)] ‚àíaT(y)‚à•2

Sec. 6.5
Unitary and Orthogonal Operators and Their Matrices
387
= ‚à•T(x + ay) ‚àíT(x)‚à•2 + a2‚à•T(y)‚à•2 ‚àí2a ‚ü®T(x + ay) ‚àíT(x), T(y)‚ü©
= ‚à•(x + ay) ‚àíx‚à•2 + a2‚à•y‚à•2 ‚àí2a[‚ü®T(x + ay), T(y)‚ü©‚àí‚ü®T(x), T(y)‚ü©]
= a2‚à•y‚à•2 + a2‚à•y‚à•2 ‚àí2a[‚ü®x + ay, y‚ü©‚àí‚ü®x, y‚ü©]
= 2a2‚à•y‚à•2 ‚àí2a[‚ü®x, y‚ü©+ a‚à•y‚à•2 ‚àí‚ü®x, y‚ü©]
= 0.
Thus T(x+ay) = T(x)+aT(y), and hence T is linear. Since T also preserves
inner products, T is an orthogonal operator.
To prove uniqueness, suppose that u0 and v0 are in V and T and U are
orthogonal operators on V such that
f(x) = T(x) + u0 = U(x) + v0
for all x ‚ààV. Substituting x = 0 in the preceding equation yields u0 = v0,
and hence the translation is unique.
This equation, therefore, reduces to
T(x) = U(x) for all x ‚ààV, and hence T = U.
Orthogonal Operators on R2
Because of Theorem 6.22, an understanding of rigid motions requires a
characterization of orthogonal operators. The next result characterizes or-
thogonal operators on R2. We postpone the case of orthogonal operators on
more general spaces to Section 6.11.
Theorem 6.23. Let T be an orthogonal operator on R2, and let A = [T]Œ≤,
where Œ≤ is the standard ordered basis for R2. Then exactly one of the following
conditions is satisÔ¨Åed:
(a) T is a rotation, and det(A) = 1.
(b) T is a reÔ¨Çection about a line through the origin, and det(A) = ‚àí1.
Proof. Because T is an orthogonal operator, T(Œ≤) = {T(e1), T(e2)} is an
orthonormal basis for R2 by Theorem 6.18(c). Since T(e1) is a unit vector,
there is a unique angle Œ∏, 0 ‚â§Œ∏ < 2œÄ, such that T(e1) = (cos Œ∏, sin Œ∏). Since
T(e2) is a unit vector and is orthogonal to T(e1), there are only two possible
choices for T(e2). Either
T(e2) = (‚àísin Œ∏, cos Œ∏)
or
T(e2) = (sin Œ∏, ‚àícos Œ∏).
First, suppose that T(e2) = (‚àísin Œ∏, cos Œ∏). Then A =

cos Œ∏
‚àísin Œ∏
sin Œ∏
cos Œ∏
	
.
It follows from Example 1 of Section 6.4 that T is a rotation by the angle Œ∏.
Also
det(A) = cos2 Œ∏ + sin2 Œ∏ = 1.

388
Chap. 6
Inner Product Spaces
Now suppose that T(e2) = (sin Œ∏, ‚àícos Œ∏). Then A =

cos Œ∏
sin Œ∏
sin Œ∏
‚àícos Œ∏
	
.
Comparing this matrix to the matrix A of Example 5, we see that T is the
reÔ¨Çection of R2 about a line L, so that Œ± = Œ∏/2 is the angle from the positive
x-axis to L. Furthermore,
det(A) = ‚àícos2 Œ∏ ‚àísin2 Œ∏ = ‚àí1.
Combining Theorems 6.22 and 6.23, we obtain the following characteriza-
tion of rigid motions on R2.
Corollary. Any rigid motion on R2 is either a rotation followed by a trans-
lation or a reÔ¨Çection about a line through the origin followed by a translation.
Example 7
Let
A =
‚éõ
‚éú
‚éú
‚éù
1
‚àö
5
2
‚àö
5
2
‚àö
5
‚àí1
‚àö
5
‚éû
‚éü
‚éü
‚é†.
We show that LA is the reÔ¨Çection of R2 about a line L through the origin, and
then describe L.
Clearly AA‚àó= A‚àóA = I, and therefore A is an orthogonal matrix. Hence
LA is an orthogonal operator. Furthermore,
det(A) = ‚àí1
5 ‚àí4
5 = ‚àí1,
and thus LA is a reÔ¨Çection of R2 about a line L through the origin by The-
orem 6.23. Since L is the one-dimensional eigenspace corresponding to the
eigenvalue 1 of LA, it suÔ¨Éces to Ô¨Ånd an eigenvector of LA corresponding to 1.
One such vector is v = (2,
‚àö
5 ‚àí1). Thus L is the span of {v}. Alternatively,
L is the line through the origin with slope (
‚àö
5 ‚àí1)/2, and hence is the line
with the equation
y =
‚àö
5 ‚àí1
2
x.
‚ô¶
Conic Sections
As an application of Theorem 6.20, we consider the quadratic equation
ax2 + 2bxy + cy2 + dx + ey + f = 0.
(2)

Sec. 6.5
Unitary and Orthogonal Operators and Their Matrices
389
For special choices of the coeÔ¨Écients in (2), we obtain the various conic
sections.
For example, if a = c = 1, b = d = e = 0, and f = ‚àí1, we
obtain the circle x2 + y2 = 1 with center at the origin.
The remaining
conic sections, namely, the ellipse, parabola, and hyperbola, are obtained
by other choices of the coeÔ¨Écients. If b = 0, then it is easy to graph the
equation by the method of completing the square because the xy-term is
absent. For example, the equation x2 +2x+y2 +4y +2 = 0 may be rewritten
as (x + 1)2 + (y + 2)2 = 3, which describes a circle with radius
‚àö
3 and center
at (‚àí1, ‚àí2) in the xy-coordinate system. If we consider the transformation
of coordinates (x, y) ‚Üí(x‚Ä≤, y‚Ä≤), where x‚Ä≤ = x + 1 and y‚Ä≤ = y + 2, then our
equation simpliÔ¨Åes to (x‚Ä≤)2 + (y‚Ä≤)2 = 3. This change of variable allows us to
eliminate the x- and y-terms.
We now concentrate solely on the elimination of the xy-term. To accom-
plish this, we consider the expression
ax2 + 2bxy + cy2,
(3)
which is called the associated quadratic form of (2). Quadratic forms are
studied in more generality in Section 6.8.
If we let
A =

a
b
b
c
	
and
X =

x
y
	
,
then (3) may be written as XtAX = ‚ü®AX, X‚ü©. For example, the quadratic
form 3x2 + 4xy + 6y2 may be written as
Xt

3
2
2
6
	
X.
The fact that A is symmetric is crucial in our discussion. For, by Theo-
rem 6.20, we may choose an orthogonal matrix P and a diagonal matrix D
with real diagonal entries Œª1 and Œª2 such that P tAP = D. Now deÔ¨Åne
X‚Ä≤ =

x‚Ä≤
y‚Ä≤
	
by X‚Ä≤ = P tX or, equivalently, by PX‚Ä≤ = PP tX = X. Then
XtAX = (PX‚Ä≤)tA(PX‚Ä≤) = X‚Ä≤t(P tAP)X‚Ä≤ = X‚Ä≤tDX‚Ä≤ = Œª1(x‚Ä≤)2 + Œª2(y‚Ä≤)2.
Thus the transformation (x, y) ‚Üí(x‚Ä≤, y‚Ä≤) allows us to eliminate the xy-term
in (3), and hence in (2).
Furthermore, since P is orthogonal, we have by Theorem 6.23 (with T =
LP ) that det(P) = ¬±1. If det(P) = ‚àí1, we may interchange the columns

390
Chap. 6
Inner Product Spaces
of P to obtain a matrix Q. Because the columns of P form an orthonormal
basis of eigenvectors of A, the same is true of the columns of Q. Therefore,
QtAQ =

Œª2
0
0
Œª1
	
.
Notice that det(Q) = ‚àídet(P) = 1. So, if det(P) = ‚àí1, we can take Q for
our new P; consequently, we may always choose P so that det(P) = 1. By
Lemma 4 to Theorem 6.22 (with T = LP ), it follows that matrix P represents
a rotation.
In summary, the xy-term in (2) may be eliminated by a rotation of the
x-axis and y-axis to new axes x‚Ä≤ and y‚Ä≤ given by X = PX‚Ä≤, where P is an
orthogonal matrix and det(P) = 1. Furthermore, the coeÔ¨Écients of (x‚Ä≤)2 and
(y‚Ä≤)2 are the eigenvalues of
A =

a
b
b
c
	
.
This result is a restatement of a result known as the principal axis theorem
for R2. The arguments above, of course, are easily extended to quadratic
equations in n variables. For example, in the case n = 3, by special choices
of the coeÔ¨Écients, we obtain the quadratic surfaces‚Äîthe elliptic cone, the
ellipsoid, the hyperbolic paraboloid, etc.
As an illustration of the preceding transformation, consider the quadratic
equation
2x2 ‚àí4xy + 5y2 ‚àí36 = 0,
for which the associated quadratic form is 2x2 ‚àí4xy + 5y2. In the notation
we have been using,
A =

2
‚àí2
‚àí2
5
	
,
so that the eigenvalues of A are 1 and 6 with associated eigenvectors

2
1
	
and

‚àí1
2
	
.
As expected (from Theorem 6.15(d) p. 371), these vectors are orthogonal.
The corresponding orthonormal basis of eigenvectors
Œ≤ =
‚éß
‚é™
‚é™
‚é®
‚é™
‚é™
‚é©
‚éõ
‚éú
‚éú
‚éù
2
‚àö
5
1
‚àö
5
‚éû
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éù
‚àí1
‚àö
5
2
‚àö
5
‚éû
‚éü
‚éü
‚é†
‚é´
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é≠

Sec. 6.5
Unitary and Orthogonal Operators and Their Matrices
391
determines new axes x‚Ä≤ and y‚Ä≤ as in Figure 6.4. Hence if
P =
‚éõ
‚éú
‚éú
‚éù
2
‚àö
5
‚àí1
‚àö
5
1
‚àö
5
2
‚àö
5
‚éû
‚éü
‚éü
‚é†=
1
‚àö
5

2
‚àí1
1
2
	
,
then
P tAP =

1
0
0
6
	
.
Under the transformation X = PX‚Ä≤ or
x = 2
‚àö
5x‚Ä≤ ‚àí1
‚àö
5y‚Ä≤
y = 1
‚àö
5x‚Ä≤ + 2
‚àö
5y‚Ä≤ ,
we have the new quadratic form (x‚Ä≤)2 + 6(y‚Ä≤)2. Thus the original equation
2x2‚àí4xy+5y2 = 36 may be written in the form (x‚Ä≤)2+6(y‚Ä≤)2 = 36 relative to
a new coordinate system with the x‚Ä≤- and y‚Ä≤-axes in the directions of the Ô¨Årst
and second vectors of Œ≤, respectively. It is clear that this equation represents
-
6
K
*
x
x‚Ä≤
y
y‚Ä≤
Figure 6.4
an ellipse. (See Figure 6.4.) Note that the preceding matrix P has the form

cos Œ∏
‚àísin Œ∏
sin Œ∏
cos Œ∏
	
,
where Œ∏ = cos‚àí1 2
‚àö
5 ‚âà26.6‚ó¶. So P is the matrix representation of a rotation
of R2 through the angle Œ∏. Thus the change of variable X = PX‚Ä≤ can be ac-
complished by this rotation of the x- and y-axes. There is another possibility

392
Chap. 6
Inner Product Spaces
for P, however. If the eigenvector of A corresponding to the eigenvalue 6 is
taken to be (1, ‚àí2) instead of (‚àí1, 2), and the eigenvalues are interchanged,
then we obtain the matrix
‚éõ
‚éú
‚éú
‚éù
1
‚àö
5
2
‚àö
5
‚àí2
‚àö
5
1
‚àö
5
‚éû
‚éü
‚éü
‚é†,
which is the matrix representation of a rotation through the angle Œ∏ =
sin‚àí1

‚àí2
‚àö
5
	
‚âà‚àí63.4‚ó¶. This possibility produces the same ellipse as the
one in Figure 6.4, but interchanges the names of the x‚Ä≤- and y‚Ä≤-axes.
EXERCISES
1.
Label the following statements as true or false. Assume that the under-
lying inner product spaces are Ô¨Ånite-dimensional.
(a)
Every unitary operator is normal.
(b)
Every orthogonal operator is diagonalizable.
(c)
A matrix is unitary if and only if it is invertible.
(d)
If two matrices are unitarily equivalent, then they are also similar.
(e)
The sum of unitary matrices is unitary.
(f)
The adjoint of a unitary operator is unitary.
(g)
If T is an orthogonal operator on V, then [T]Œ≤ is an orthogonal
matrix for any ordered basis Œ≤ for V.
(h)
If all the eigenvalues of a linear operator are 1, then the operator
must be unitary or orthogonal.
(i)
A linear operator may preserve the norm, but not the inner prod-
uct.
2.
For each of the following matrices A, Ô¨Ånd an orthogonal or unitary
matrix P and a diagonal matrix D such that P ‚àóAP = D.
(a)

1
2
2
1
	
(b)

0
‚àí1
1
0
	
(c)

2
3 ‚àí3i
3 + 3i
5
	
(d)
‚éõ
‚éù
0
2
2
2
0
2
2
2
0
‚éû
‚é†
(e)
‚éõ
‚éù
2
1
1
1
2
1
1
1
2
‚éû
‚é†
3.
Prove that the composite of unitary [orthogonal] operators is unitary
[orthogonal].

Sec. 6.5
Unitary and Orthogonal Operators and Their Matrices
393
4.
For z ‚ààC, deÔ¨Åne Tz : C ‚ÜíC by Tz(u) = zu. Characterize those z for
which Tz is normal, self-adjoint, or unitary.
5.
Which of the following pairs of matrices are unitarily equivalent?
(a)

1
0
0
1
	
and

0
1
1
0
	
(b)

0
1
1
0
	
and
‚éõ
‚éù
0
1
2
1
2
0
‚éû
‚é†
(c)
‚éõ
‚éù
0
1
0
‚àí1
0
0
0
0
1
‚éû
‚é†
and
‚éõ
‚éù
2
0
0
0
‚àí1
0
0
0
0
‚éû
‚é†
(d)
‚éõ
‚éù
0
1
0
‚àí1
0
0
0
0
1
‚éû
‚é†
and
‚éõ
‚éù
1
0
0
0
i
0
0
0
‚àíi
‚éû
‚é†
(e)
‚éõ
‚éù
1
1
0
0
2
2
0
0
3
‚éû
‚é†
and
‚éõ
‚éù
1
0
0
0
2
0
0
0
3
‚éû
‚é†
6.
Let V be the inner product space of complex-valued continuous func-
tions on [0, 1] with the inner product
‚ü®f, g‚ü©=
 1
0
f(t)g(t) dt.
Let h ‚ààV, and deÔ¨Åne T: V ‚ÜíV by T(f) = hf. Prove that T is a
unitary operator if and only if |h(t)| = 1 for 0 ‚â§t ‚â§1.
7.
Prove that if T is a unitary operator on a Ô¨Ånite-dimensional inner prod-
uct space V, then T has a unitary square root; that is, there exists a
unitary operator U such that T = U2.
8.
Let T be a self-adjoint linear operator on a Ô¨Ånite-dimensional inner
product space. Prove that (T+iI)(T‚àíiI)‚àí1 is unitary using Exercise 10
of Section 6.4.
9.
Let U be a linear operator on a Ô¨Ånite-dimensional inner product space
V. If ‚à•U(x)‚à•= ‚à•x‚à•for all x in some orthonormal basis for V, must U
be unitary? Justify your answer with a proof or a counterexample.
10.
Let A be an n √ó n real symmetric or complex normal matrix. Prove
that
tr(A) =
n

i=1
Œªi
and
tr(A‚àóA) =
n

i=1
|Œªi|2,
where the Œªi‚Äôs are the (not necessarily distinct) eigenvalues of A.

394
Chap. 6
Inner Product Spaces
11.
Find an orthogonal matrix whose Ô¨Årst row is ( 1
3, 2
3, 2
3).
12.
Let A be an n √ó n real symmetric or complex normal matrix. Prove
that
det(A) =
n

i=1
Œªi,
where the Œªi‚Äôs are the (not necessarily distinct) eigenvalues of A.
13.
Suppose that A and B are diagonalizable matrices. Prove or disprove
that A is similar to B if and only if A and B are unitarily equivalent.
14.
Prove that if A and B are unitarily equivalent matrices, then A is pos-
itive deÔ¨Ånite [semideÔ¨Ånite] if and only if B is positive deÔ¨Ånite [semidef-
inite]. (See the deÔ¨Ånitions in the exercises in Section 6.4.)
15.
Let U be a unitary operator on an inner product space V, and let W be
a Ô¨Ånite-dimensional U-invariant subspace of V. Prove that
(a)
U(W) = W;
(b)
W‚ä•is U-invariant.
Contrast (b) with Exercise 16.
16.
Find an example of a unitary operator U on an inner product space and
a U-invariant subspace W such that W‚ä•is not U-invariant.
17.
Prove that a matrix that is both unitary and upper triangular must be
a diagonal matrix.
18.
Show that ‚Äúis unitarily equivalent to‚Äù is an equivalence relation on
Mn√ón(C).
19.
Let W be a Ô¨Ånite-dimensional subspace of an inner product space V.
By Theorem 6.7 (p. 352) and the exercises of Section 1.3, V = W‚äïW‚ä•.
DeÔ¨Åne U: V ‚ÜíV by U(v1 + v2) = v1 ‚àív2, where v1 ‚ààW and v2 ‚ààW‚ä•.
Prove that U is a self-adjoint unitary operator.
20.
Let V be a Ô¨Ånite-dimensional inner product space. A linear operator U
on V is called a partial isometry if there exists a subspace W of V
such that ‚à•U(x)‚à•= ‚à•x‚à•for all x ‚ààW and U(x) = 0 for all x ‚ààW‚ä•.
Observe that W need not be U-invariant. Suppose that U is such an
operator and {v1, v2, . . . , vk} is an orthonormal basis for W. Prove the
following results.
(a)
‚ü®U(x), U(y)‚ü©= ‚ü®x, y‚ü©for all x, y ‚ààW. Hint: Use Exercise 20 of
Section 6.1.
(b)
{U(v1), U(v2), . . . , U(vk)} is an orthonormal basis for R(U).

Sec. 6.5
Unitary and Orthogonal Operators and Their Matrices
395
(c)
There exists an orthonormal basis Œ≥ for V such that the Ô¨Årst
k columns of [U]Œ≥ form an orthonormal set and the remaining
columns are zero.
(d)
Let {w1, w2, . . . , wj} be an orthonormal basis for R(U)‚ä•and Œ≤ =
{U(v1), U(v2), . . . , U(vk), w1, . . . , wj}. Then Œ≤ is an orthonormal
basis for V.
(e)
Let T be the linear operator on V that satisÔ¨Åes T(U(vi)) = vi
(1 ‚â§i ‚â§k) and T(wi) = 0 (1 ‚â§i ‚â§j). Then T is well deÔ¨Åned,
and T = U‚àó. Hint: Show that ‚ü®U(x), y‚ü©= ‚ü®x, T(y)‚ü©for all x, y ‚ààŒ≤.
There are four cases.
(f)
U‚àóis a partial isometry.
This exercise is continued in Exercise 9 of Section 6.6.
21.
Let A and B be n √ó n matrices that are unitarily equivalent.
(a)
Prove that tr(A‚àóA) = tr(B‚àóB).
(b)
Use (a) to prove that
n

i,j=1
|Aij|2 =
n

i,j=1
|Bij|2.
(c)
Use (b) to show that the matrices

1
2
2
i
	
and

i
4
1
1
	
are not unitarily equivalent.
22.
Let V be a real inner product space.
(a)
Prove that any translation on V is a rigid motion.
(b)
Prove that the composite of any two rigid motions on V is a rigid
motion on V.
23.
Prove the following variation of Theorem 6.22: If f : V ‚ÜíV is a rigid
motion on a Ô¨Ånite-dimensional real inner product space V, then there
exists a unique orthogonal operator T on V and a unique translation g
on V such that f = T ‚ó¶g.
24.
Let T and U be orthogonal operators on R2. Use Theorem 6.23 to prove
the following results.
(a)
If T and U are both reÔ¨Çections about lines through the origin, then
UT is a rotation.
(b)
If T is a rotation and U is a reÔ¨Çection about a line through the
origin, then both UT and TU are reÔ¨Çections about lines through
the origin.

396
Chap. 6
Inner Product Spaces
25.
Suppose that T and U are reÔ¨Çections of R2 about the respective lines
L and L‚Ä≤ through the origin and that œÜ and œà are the angles from
the positive x-axis to L and L‚Ä≤, respectively. By Exercise 24, UT is a
rotation. Find its angle of rotation.
26.
Suppose that T and U are orthogonal operators on R2 such that T is
the rotation by the angle œÜ and U is the reÔ¨Çection about the line L
through the origin. Let œà be the angle from the positive x-axis to L.
By Exercise 24, both UT and TU are reÔ¨Çections about lines L1 and L2,
respectively, through the origin.
(a)
Find the angle Œ∏ from the positive x-axis to L1.
(b)
Find the angle Œ∏ from the positive x-axis to L2.
27.
Find new coordinates x‚Ä≤, y‚Ä≤ so that the following quadratic forms can
be written as Œª1(x‚Ä≤)2 + Œª2(y‚Ä≤)2.
(a)
x2 + 4xy + y2
(b)
2x2 + 2xy + 2y2
(c)
x2 ‚àí12xy ‚àí4y2
(d)
3x2 + 2xy + 3y2
(e)
x2 ‚àí2xy + y2
28.
Consider the expression XtAX, where Xt = (x, y, z) and A is as deÔ¨Åned
in Exercise 2(e).
Find a change of coordinates x‚Ä≤, y‚Ä≤, z‚Ä≤ so that the
preceding expression is of the form Œª1(x‚Ä≤)2 + Œª2(y‚Ä≤)2 + Œª3(z‚Ä≤)2.
29.
QR-Factorization. Let w1, w2, . . . , wn be linearly independent vectors
in Fn, and let v1, v2, . . . , vn be the orthogonal vectors obtained from
w1, w2, . . . , wn by the Gram‚ÄìSchmidt process. Let u1, u2, . . . , un be the
orthonormal basis obtained by normalizing the vi‚Äôs.
(a)
Solving (1) in Section 6.2 for wk in terms of uk, show that
wk = ‚à•vk‚à•uk +
k‚àí1

j=1
‚ü®wk, uj‚ü©uj
(1 ‚â§k ‚â§n).
(b)
Let A and Q denote the n √ó n matrices in which the kth columns
are wk and uk, respectively. DeÔ¨Åne R ‚ààMn√ón(F) by
Rjk =
‚éß
‚é™
‚é®
‚é™
‚é©
‚à•vj‚à•
if j = k
‚ü®wk, uj‚ü©
if j < k
0
if j > k.
Prove A = QR.
(c)
Compute Q and R as in (b) for the 3√ó3 matrix whose columns are
the vectors w1, w2, w3, respectively, in Example 4 of Section 6.2.

Sec. 6.5
Unitary and Orthogonal Operators and Their Matrices
397
(d)
Since Q is unitary [orthogonal] and R is upper triangular in (b),
we have shown that every invertible matrix is the product of a uni-
tary [orthogonal] matrix and an upper triangular matrix. Suppose
that A ‚ààMn√ón(F) is invertible and A = Q1R1 = Q2R2, where
Q1, Q2 ‚ààMn√ón(F) are unitary and R1, R2 ‚ààMn√ón(F) are upper
triangular. Prove that D = R2R‚àí1
1
is a unitary diagonal matrix.
Hint: Use Exercise 17.
(e)
The QR factorization described in (b) provides an orthogonaliza-
tion method for solving a linear system Ax = b when A is in-
vertible. Decompose A to QR, by the Gram‚ÄìSchmidt process or
other means, where Q is unitary and R is upper triangular. Then
QRx = b, and hence Rx = Q‚àób. This last system can be easily
solved since R is upper triangular. 1
Use the orthogonalization method and (c) to solve the system
x1 + 2x2 + 2x3 =
1
x1
+ 2x3 = 11
x2 + x3 = ‚àí1.
30.
Suppose that Œ≤ and Œ≥ are ordered bases for an n-dimensional real [com-
plex] inner product space V. Prove that if Q is an orthogonal [unitary]
n √ó n matrix that changes Œ≥-coordinates into Œ≤-coordinates, then Œ≤ is
orthonormal if and only if Œ≥ is orthonormal.
The following deÔ¨Ånition is used in Exercises 31 and 32.
DeÔ¨Ånition.
Let V be a Ô¨Ånite-dimensional complex [real] inner product
space, and let u be a unit vector in V. DeÔ¨Åne the Householder operator
Hu : V ‚ÜíV by Hu(x) = x ‚àí2 ‚ü®x, u‚ü©u for all x ‚ààV.
31.
Let Hu be a Householder operator on a Ô¨Ånite-dimensional inner product
space V. Prove the following results.
(a)
Hu is linear.
(b)
Hu(x) = x if and only if x is orthogonal to u.
(c)
Hu(u) = ‚àíu.
(d)
H‚àó
u = Hu and H2
u = I, and hence Hu is a unitary [orthogonal]
operator on V.
(Note: If V is a real inner product space, then in the language of Sec-
tion 6.11, Hu is a reÔ¨Çection.)
1At one time, because of its great stability, this method for solving large sys-
tems of linear equations with a computer was being advocated as a better method
than Gaussian elimination even though it requires about three times as much work.
(Later, however, J. H. Wilkinson showed that if Gaussian elimination is done ‚Äúprop-
erly,‚Äù then it is nearly as stable as the orthogonalization method.)

398
Chap. 6
Inner Product Spaces
32.
Let V be a Ô¨Ånite-dimensional inner product space over F. Let x and y
be linearly independent vectors in V such that ‚à•x‚à•= ‚à•y‚à•.
(a)
If F = C, prove that there exists a unit vector u in V and a complex
number Œ∏ with |Œ∏| = 1 such that Hu(x) = Œ∏y. Hint: Choose Œ∏ so
that ‚ü®x, Œ∏y‚ü©is real, and set u =
1
‚à•x ‚àíŒ∏y‚à•(x ‚àíŒ∏y).
(b)
If F = R, prove that there exists a unit vector u in V such that
Hu(x) = y.
6.6
ORTHOGONAL PROJECTIONS
AND THE SPECTRAL THEOREM
In this section, we rely heavily on Theorems 6.16 (p. 372) and 6.17 (p. 374) to
develop an elegant representation of a normal (if F = C) or a self-adjoint (if
F = R) operator T on a Ô¨Ånite-dimensional inner product space. We prove that
T can be written in the form Œª1T1 + Œª2T2 + ¬∑ ¬∑ ¬∑ + ŒªkTk, where Œª1, Œª2, . . . , Œªk
are the distinct eigenvalues of T and T1, T2, . . . , Tk are orthogonal projections.
We must Ô¨Årst develop some results about these special projections.
We assume that the reader is familiar with the results about direct sums
developed at the end of Section 5.2. The special case where V is a direct sum
of two subspaces is considered in the exercises of Section 1.3.
Recall from the exercises of Section 2.1 that if V = W1 ‚äïW2, then a linear
operator T on V is the projection on W1 along W2 if, whenever x = x1+x2,
with x1 ‚ààW1 and x2 ‚ààW2, we have T(x) = x1. By Exercise 26 of Section 2.1,
we have
R(T) = W1 = {x ‚ààV: T(x) = x}
and
N(T) = W2.
So V = R(T) ‚äïN(T).
Thus there is no ambiguity if we refer to T as a
‚Äúprojection on W1‚Äù or simply as a ‚Äúprojection.‚Äù In fact, it can be shown
(see Exercise 17 of Section 2.3) that T is a projection if and only if T = T2.
Because V = W1 ‚äïW2 = W1 ‚äïW3 does not imply that W2 = W3, we see that
W1 does not uniquely determine T. For an orthogonal projection T, however,
T is uniquely determined by its range.
DeÔ¨Ånition.
Let V be an inner product space, and let T: V ‚ÜíV be a
projection. We say that T is an orthogonal projection if R(T)‚ä•= N(T)
and N(T)‚ä•= R(T).
Note that by Exercise 13(c) of Section 6.2, if V is Ô¨Ånite-dimensional, we
need only assume that one of the preceding conditions holds. For example, if
R(T)‚ä•= N(T), then R(T) = R(T)‚ä•‚ä•= N(T)‚ä•.
Now assume that W is a Ô¨Ånite-dimensional subspace of an inner product
space V. In the notation of Theorem 6.6 (p. 350), we can deÔ¨Åne a function

Sec. 6.6
Orthogonal Projections and the Spectral Theorem
399
T: V ‚ÜíV by T(y) = u. It is easy to show that T is an orthogonal projection
on W. We can say even more‚Äîthere exists exactly one orthogonal projection
on W. For if T and U are orthogonal projections on W, then R(T) = W =
R(U). Hence N(T) = R(T)‚ä•= R(U)‚ä•= N(U), and since every projection is
uniquely determined by its range and null space, we have T = U. We call T
the orthogonal projection of V on W.
To understand the geometric diÔ¨Äerence between an arbitrary projection
on W and the orthogonal projection on W, let V = R2 and W = span{(1, 1)}.
DeÔ¨Åne U and T as in Figure 6.5, where T(v) is the foot of a perpendicular
from v on the line y = x and U(a1, a2) = (a1, a1). Then T is the orthogo-
nal projection of V on W, and U is a diÔ¨Äerent projection on W. Note that
v ‚àíT(v) ‚ààW‚ä•, whereas v ‚àíU(v) /‚ààW‚ä•.







@
@@















U(v)
T(v)
W
v
Figure 6.5
From Figure 6.5, we see that T(v) is the ‚Äúbest approximation in W to v‚Äù;
that is, if w ‚ààW, then ‚à•w ‚àív‚à•‚â•‚à•T(v) ‚àív‚à•. In fact, this approximation
property characterizes T. These results follow immediately from the corollary
to Theorem 6.6 (p. 350).
As an application to Fourier analysis, recall the inner product space H and
the orthonormal set S in Example 9 of Section 6.1. DeÔ¨Åne a trigonometric
polynomial of degree n to be a function g ‚ààH of the form
g(t) =
n

j=‚àín
ajfj(t) =
n

j=‚àín
ajeijt,
where an or a‚àín is nonzero.
Let f ‚ààH. We show that the best approximation to f by a trigonometric
polynomial of degree less than or equal to n is the trigonometric polynomial

400
Chap. 6
Inner Product Spaces
whose coeÔ¨Écients are the Fourier coeÔ¨Écients of f relative to the orthonormal
set S. For this result, let W = span({fj : |j| ‚â§n}), and let T be the orthogo-
nal projection of H on W. The corollary to Theorem 6.6 (p. 350) tells us that
the best approximation to f by a function in W is
T(f) =
n

j=‚àín
‚ü®f, fj‚ü©fj.
An algebraic characterization of orthogonal projections follows in the next
theorem.
Theorem 6.24. Let V be an inner product space, and let T be a linear
operator on V. Then T is an orthogonal projection if and only if T has an
adjoint T‚àóand T2 = T = T‚àó.
Proof. Suppose that T is an orthogonal projection. Since T2 = T because
T is a projection, we need only show that T‚àóexists and T = T‚àó.
Now
V = R(T) ‚äïN(T) and R(T)‚ä•= N(T). Let x, y ‚ààV. Then we can write
x = x1 + x2 and y = y1 + y2, where x1, y1 ‚ààR(T) and x2, y2 ‚ààN(T). Hence
‚ü®x, T(y)‚ü©= ‚ü®x1 + x2, y1‚ü©= ‚ü®x1, y1‚ü©+ ‚ü®x2, y1‚ü©= ‚ü®x1, y1‚ü©
and
‚ü®T(x), y‚ü©= ‚ü®x1, y1 + y2‚ü©= ‚ü®x1, y1‚ü©+ ‚ü®x1, y2‚ü©= ‚ü®x1, y1‚ü©.
So ‚ü®x, T(y)‚ü©= ‚ü®T(x), y‚ü©for all x, y ‚ààV; thus T‚àóexists and T = T‚àó.
Now suppose that T2 = T = T‚àó. Then T is a projection by Exercise 17 of
Section 2.3, and hence we must show that R(T) = N(T)‚ä•and R(T)‚ä•= N(T).
Let x ‚ààR(T) and y ‚ààN(T). Then x = T(x) = T‚àó(x), and so
‚ü®x, y‚ü©= ‚ü®T‚àó(x), y‚ü©= ‚ü®x, T(y)‚ü©= ‚ü®x, 0‚ü©= 0.
Therefore x ‚ààN(T)‚ä•, from which it follows that R(T) ‚äÜN(T)‚ä•.
Let y ‚ààN(T)‚ä•. We must show that y ‚ààR(T), that is, T(y) = y. Now
‚à•y ‚àíT(y)‚à•2 = ‚ü®y ‚àíT(y), y ‚àíT(y)‚ü©
= ‚ü®y, y ‚àíT(y)‚ü©‚àí‚ü®T(y), y ‚àíT(y)‚ü©.
Since y ‚àíT(y) ‚ààN(T), the Ô¨Årst term must equal zero. But also
‚ü®T(y), y ‚àíT(y)‚ü©= ‚ü®y, T‚àó(y ‚àíT(y))‚ü©= ‚ü®y, T(y ‚àíT(y))‚ü©= ‚ü®y, 0‚ü©= 0.
Thus y ‚àíT(y) = 0; that is, y = T(y) ‚ààR(T). Hence R(T) = N(T)‚ä•.
Using the preceding results, we have R(T)‚ä•= N(T)‚ä•‚ä•‚äáN(T) by Exer-
cise 13(b) of Section 6.2. Now suppose that x ‚ààR(T)‚ä•. For any y ‚ààV, we
have ‚ü®T(x), y‚ü©= ‚ü®x, T‚àó(y)‚ü©= ‚ü®x, T(y)‚ü©= 0. So T(x) = 0, and thus x ‚ààN(T).
Hence R(T)‚ä•= N(T).

Sec. 6.6
Orthogonal Projections and the Spectral Theorem
401
Let V be a Ô¨Ånite-dimensional inner product space, W be a subspace of V,
and T be the orthogonal projection of V on W. We may choose an orthonormal
basis Œ≤ = {v1, v2, . . . , vn} for V such that {v1, v2, . . . , vk} is a basis for W.
Then [T]Œ≤ is a diagonal matrix with ones as the Ô¨Årst k diagonal entries and
zeros elsewhere. In fact, [T]Œ≤ has the form

Ik
O1
O2
O3
	
.
If U is any projection on W, we may choose a basis Œ≥ for V such that [U]Œ≥ has
the form above; however Œ≥ is not necessarily orthonormal.
We are now ready for the principal theorem of this section.
Theorem 6.25 (The Spectral Theorem). Suppose that T is a linear
operator on a Ô¨Ånite-dimensional inner product space V over F with the dis-
tinct eigenvalues Œª1, Œª2, . . . , Œªk. Assume that T is normal if F = C and that
T is self-adjoint if F = R. For each i (1 ‚â§i ‚â§k), let Wi be the eigenspace of
T corresponding to the eigenvalue Œªi, and let Ti be the orthogonal projection
of V on Wi. Then the following statements are true.
(a) V = W1 ‚äïW2 ‚äï¬∑ ¬∑ ¬∑ ‚äïWk.
(b) If W‚Ä≤
i denotes the direct sum of the subspaces Wj for j Ã∏= i, then
W‚ä•
i = W‚Ä≤
i.
(c) TiTj = Œ¥ijTi for 1 ‚â§i, j ‚â§k.
(d) I = T1 + T2 + ¬∑ ¬∑ ¬∑ + Tk.
(e) T = Œª1T1 + Œª2T2 + ¬∑ ¬∑ ¬∑ + ŒªkTk.
Proof. (a) By Theorems 6.16 (p. 372) and 6.17 (p. 374), T is diagonalizable;
so
V = W1 ‚äïW2 ‚äï¬∑ ¬∑ ¬∑ ‚äïWk
by Theorem 5.11 (p. 278).
(b) If x ‚ààWi and y ‚ààWj for some i Ã∏= j, then ‚ü®x, y‚ü©= 0 by The-
orem 6.15(d) (p. 371).
It follows easily from this result that W‚Ä≤
i ‚äÜW‚ä•
i .
From (a), we have
dim(W‚Ä≤
i) =

jÃ∏=i
dim(Wj) = dim(V) ‚àídim(Wi).
On the other hand, we have dim(W‚ä•
i ) = dim(V)‚àídim(Wi) by Theorem 6.7(c)
(p. 352). Hence W‚Ä≤
i = W‚ä•
i , proving (b).
(c) The proof of (c) is left as an exercise.
(d) Since Ti is the orthogonal projection of V on Wi, it follows from
(b) that N(Ti) = R(Ti)‚ä•= W‚ä•
i
= W‚Ä≤
i.
Hence, for x ‚ààV, we have x =
x1 + x2 + ¬∑ ¬∑ ¬∑ + xk, where Ti(x) = xi ‚ààWi, proving (d).

402
Chap. 6
Inner Product Spaces
(e) For x ‚ààV, write x = x1 + x2 + ¬∑ ¬∑ ¬∑ + xk, where xi ‚ààWi. Then
T(x) = T(x1) + T(x2) + ¬∑ ¬∑ ¬∑ + T(xk)
= Œª1x1 + Œª2x2 + ¬∑ ¬∑ ¬∑ + Œªkxk
= Œª1T1(x) + Œª2T2(x) + ¬∑ ¬∑ ¬∑ + ŒªkTk(x)
= (Œª1T1 + Œª2T2 + ¬∑ ¬∑ ¬∑ + ŒªkTk)(x).
The set {Œª1, Œª2, . . . , Œªk} of eigenvalues of T is called the spectrum of T,
the sum I = T1+T2+¬∑ ¬∑ ¬∑+Tk in (d) is called the resolution of the identity
operator induced by T, and the sum T = Œª1T1 + Œª2T2 + ¬∑ ¬∑ ¬∑ + ŒªkTk in (e)
is called the spectral decomposition of T. The spectral decomposition of
T is unique up to the order of its eigenvalues.
With the preceding notation, let Œ≤ be the union of orthonormal bases of
the Wi‚Äôs and let mi = dim(Wi). (Thus mi is the multiplicity of Œªi.) Then
[T]Œ≤ has the form
‚éõ
‚éú
‚éú
‚éú
‚éù
Œª1Im1
O
¬∑ ¬∑ ¬∑
O
O
Œª2Im2
¬∑ ¬∑ ¬∑
O
...
...
...
O
O
¬∑ ¬∑ ¬∑
ŒªkImk
‚éû
‚éü
‚éü
‚éü
‚é†;
that is, [T]Œ≤ is a diagonal matrix in which the diagonal entries are the eigen-
values Œªi of T, and each Œªi is repeated mi times. If Œª1T1 + Œª2T2 + ¬∑ ¬∑ ¬∑ + ŒªkTk
is the spectral decomposition of T, then it follows (from Exercise 7) that
g(T) = g(Œª1)T1 + g(Œª2)T2 + ¬∑ ¬∑ ¬∑ + g(Œªk)Tk for any polynomial g. This fact is
used below.
We now list several interesting corollaries of the spectral theorem; many
more results are found in the exercises. For what follows, we assume that T
is a linear operator on a Ô¨Ånite-dimensional inner product space V over F.
Corollary 1.
If F = C, then T is normal if and only if T‚àó= g(T) for
some polynomial g.
Proof. Suppose Ô¨Årst that T is normal. Let T = Œª1T1 + Œª2T2 + ¬∑ ¬∑ ¬∑ + ŒªkTk
be the spectral decomposition of T. Taking the adjoint of both sides of the
preceding equation, we have T‚àó= Œª1T1 + Œª2T2 + ¬∑ ¬∑ ¬∑ + ŒªkTk since each Ti is
self-adjoint. Using the Lagrange interpolation formula (see page 52), we may
choose a polynomial g such that g(Œªi) = Œªi for 1 ‚â§i ‚â§k. Then
g(T)=g(Œª1)T1 + g(Œª2)T2 + ¬∑ ¬∑ ¬∑ + g(Œªk)Tk =Œª1T1 + Œª2T2 + ¬∑ ¬∑ ¬∑ + ŒªkTk =T‚àó.
Conversely, if T‚àó= g(T) for some polynomial g, then T commutes with
T‚àósince T commutes with every polynomial in T. So T is normal.

Sec. 6.6
Orthogonal Projections and the Spectral Theorem
403
Corollary 2. If F = C, then T is unitary if and only if T is normal and
|Œª| = 1 for every eigenvalue Œª of T.
Proof. If T is unitary, then T is normal and every eigenvalue of T has
absolute value 1 by Corollary 2 to Theorem 6.18 (p. 382).
Let T = Œª1T1 + Œª2T2 + ¬∑ ¬∑ ¬∑ + ŒªkTk be the spectral decomposition of T. If
|Œª| = 1 for every eigenvalue Œª of T, then by (c) of the spectral theorem,
TT‚àó= (Œª1T1 + Œª2T2 + ¬∑ ¬∑ ¬∑ + ŒªkTk)(Œª1T1 + Œª2T2 + ¬∑ ¬∑ ¬∑ + ŒªkTk)
= |Œª1|2T1 + |Œª2|2T2 + ¬∑ ¬∑ ¬∑ + |Œªk|2Tk
= T1 + T2 + ¬∑ ¬∑ ¬∑ + Tk
= I.
Hence T is unitary.
Corollary 3.
If F = C and T is normal, then T is self-adjoint if and
only if every eigenvalue of T is real.
Proof. Let T = Œª1T1 + Œª2T2 + ¬∑ ¬∑ ¬∑ + ŒªkTk be the spectral decomposition
of T. Suppose that every eigenvalue of T is real. Then
T‚àó= Œª1T1 + Œª2T2 + ¬∑ ¬∑ ¬∑ + ŒªkTk = Œª1T1 + Œª2T2 + ¬∑ ¬∑ ¬∑ + ŒªkTk = T.
The converse has been proved in the lemma to Theorem 6.17 (p. 374).
Corollary 4. Let T be as in the spectral theorem with spectral decom-
position T = Œª1T1 + Œª2T2 + ¬∑ ¬∑ ¬∑ + ŒªkTk. Then each Tj is a polynomial in
T.
Proof. Choose a polynomial gj (1 ‚â§j ‚â§k) such that gj(Œªi) = Œ¥ij. Then
gj(T) = gj(Œª1)T1 + gj(Œª2)T2 + ¬∑ ¬∑ ¬∑ + gj(Œªk)Tk
= Œ¥1jT1 + Œ¥2jT2 + ¬∑ ¬∑ ¬∑ + Œ¥kjTk = Tj.
EXERCISES
1.
Label the following statements as true or false. Assume that the under-
lying inner product spaces are Ô¨Ånite-dimensional.
(a)
All projections are self-adjoint.
(b)
An orthogonal projection is uniquely determined by its range.
(c)
Every self-adjoint operator is a linear combination of orthogonal
projections.

404
Chap. 6
Inner Product Spaces
(d)
If T is a projection on W, then T(x) is the vector in W that is
closest to x.
(e)
Every orthogonal projection is a unitary operator.
2.
Let V = R2, W = span({(1, 2)}), and Œ≤ be the standard ordered basis
for V. Compute [T]Œ≤, where T is the orthogonal projection of V on W.
Do the same for V = R3 and W = span({(1, 0, 1)}).
3.
For each of the matrices A in Exercise 2 of Section 6.5:
(1) Verify that LA possesses a spectral decomposition.
(2) For each eigenvalue of LA, explicitly deÔ¨Åne the orthogonal projec-
tion on the corresponding eigenspace.
(3) Verify your results using the spectral theorem.
4.
Let W be a Ô¨Ånite-dimensional subspace of an inner product space V.
Show that if T is the orthogonal projection of V on W, then I ‚àíT is the
orthogonal projection of V on W‚ä•.
5.
Let T be a linear operator on a Ô¨Ånite-dimensional inner product space
V.
(a)
If T is an orthogonal projection, prove that ‚à•T(x)‚à•‚â§‚à•x‚à•for all
x ‚ààV. Give an example of a projection for which this inequality
does not hold.
What can be concluded about a projection for
which the inequality is actually an equality for all x ‚ààV?
(b)
Suppose that T is a projection such that ‚à•T(x)‚à•‚â§‚à•x‚à•for x ‚ààV.
Prove that T is an orthogonal projection.
6.
Let T be a normal operator on a Ô¨Ånite-dimensional inner product space.
Prove that if T is a projection, then T is also an orthogonal projection.
7.
Let T be a normal operator on a Ô¨Ånite-dimensional complex inner prod-
uct space V. Use the spectral decomposition Œª1T1 + Œª2T2 + ¬∑ ¬∑ ¬∑ + ŒªkTk
of T to prove the following results.
(a)
If g is a polynomial, then
g(T) =
k

i=1
g(Œªi)Ti.
(b)
If Tn = T0 for some n, then T = T0.
(c)
Let U be a linear operator on V. Then U commutes with T if and
only if U commutes with each Ti.
(d)
There exists a normal operator U on V such that U2 = T.
(e)
T is invertible if and only if Œªi Ã∏= 0 for 1 ‚â§i ‚â§k.
(f)
T is a projection if and only if every eigenvalue of T is 1 or 0.

Sec. 6.7
The Singular Value Decomposition and the Pseudoinverse
405
(g)
T = ‚àíT‚àóif and only if every Œªi is an imaginary number.
8.
Use Corollary 1 of the spectral theorem to show that if T is a normal
operator on a complex Ô¨Ånite-dimensional inner product space and U is
a linear operator that commutes with T, then U commutes with T‚àó.
9.
Referring to Exercise 20 of Section 6.5, prove the following facts about
a partial isometry U.
(a)
U‚àóU is an orthogonal projection on W.
(b)
UU‚àóU = U.
10.
Simultaneous diagonalization. Let U and T be normal operators on a
Ô¨Ånite-dimensional complex inner product space V such that TU = UT.
Prove that there exists an orthonormal basis for V consisting of vectors
that are eigenvectors of both T and U. Hint: Use the hint of Exercise 14
of Section 6.4 along with Exercise 8.
11.
Prove (c) of the spectral theorem.
6.7‚àó
THE SINGULAR VALUE DECOMPOSITION
AND THE PSEUDOINVERSE
In Section 6.4, we characterized normal operators on complex spaces and self-
adjoint operators on real spaces in terms of orthonormal bases of eigenvectors
and their corresponding eigenvalues (Theorems 6.16, p. 372, and 6.17, p. 374).
In this section, we establish a comparable theorem whose scope is the entire
class of linear transformations on both complex and real Ô¨Ånite-dimensional
inner product spaces‚Äîthe singular value theorem for linear transformations
(Theorem 6.26). There are similarities and diÔ¨Äerences among these theorems.
All rely on the use of orthonormal bases and numerical invariants. However,
because of its general scope, the singular value theorem is concerned with
two (usually distinct) inner product spaces and with two (usually distinct)
orthonormal bases. If the two spaces and the two bases are identical, then the
transformation would, in fact, be a normal or self-adjoint operator. Another
diÔ¨Äerence is that the numerical invariants in the singular value theorem, the
singular values, are nonnegative, in contrast to their counterparts, the eigen-
values, for which there is no such restriction. This property is necessary to
guarantee the uniqueness of singular values.
The singular value theorem encompasses both real and complex spaces.
For brevity, in this section we use the terms unitary operator and unitary
matrix to include orthogonal operators and orthogonal matrices in the context
of real spaces. Thus any operator T for which ‚ü®T(x), T(y)‚ü©= ‚ü®x, y‚ü©, or any
matrix A for which ‚ü®Ax, Ay‚ü©= ‚ü®x, y‚ü©, for all x and y is called unitary for the
purposes of this section.

406
Chap. 6
Inner Product Spaces
In Exercise 15 of Section 6.3, the deÔ¨Ånition of the adjoint of an operator
is extended to any linear transformation T: V ‚ÜíW, where V and W are
Ô¨Ånite-dimensional inner product spaces. By this exercise, the adjoint T‚àóof
T is a linear transformation from W to V and [T‚àó]Œ≤
Œ≥ = ([T]Œ≥
Œ≤)‚àó, where Œ≤ and
Œ≥ are orthonormal bases for V and W, respectively. Furthermore, the linear
operator T‚àóT on V is positive semideÔ¨Ånite and rank(T‚àóT) = rank(T) by
Exercise 18 of Section 6.4.
With these facts in mind, we begin with the principal result.
Theorem 6.26 (Singular Value Theorem for Linear Transforma-
tions). Let V and W be Ô¨Ånite-dimensional inner product spaces, and let
T: V ‚ÜíW be a linear transformation of rank r. Then there exist orthonormal
bases {v1, v2, . . . , vn} for V and {u1, u2, . . . , um} for W and positive scalars
œÉ1 ‚â•œÉ2 ‚â•¬∑ ¬∑ ¬∑ ‚â•œÉr such that
T(vi) =

œÉiui
if 1 ‚â§i ‚â§r
0
if i > r.
(4)
Conversely, suppose that the preceding conditions are satisÔ¨Åed.
Then for
1 ‚â§i ‚â§n, vi is an eigenvector of T‚àóT with corresponding eigenvalue œÉ2
i if
1 ‚â§i ‚â§r and 0 if i > r. Therefore the scalars œÉ1, œÉ2, . . . , œÉr are uniquely
determined by T.
Proof. We Ô¨Årst establish the existence of the bases and scalars. By Ex-
ercises 18 of Section 6.4 and 15(d) of Section 6.3, T‚àóT is a positive semidef-
inite linear operator of rank r on V; hence there is an orthonormal basis
{v1, v2, . . . , vn} for V consisting of eigenvectors of T‚àóT with corresponding
eigenvalues Œªi, where Œª1 ‚â•Œª2 ‚â•¬∑ ¬∑ ¬∑ ‚â•Œªr > 0, and Œªi = 0 for i > r. For
1 ‚â§i ‚â§r, deÔ¨Åne œÉi = ‚àöŒªi and ui = 1
œÉi
T(vi). We show that {u1, u2, . . . , ur}
is an orthonormal subset of W. Suppose 1 ‚â§i, j ‚â§r. Then
‚ü®ui, uj‚ü©=
1
1
œÉ iT(vi), 1
œÉ jT(vj)
2
=
1
œÉiœÉj
1
T‚àóT(vi), vj
2
=
1
œÉiœÉj
‚ü®Œªivi, vj‚ü©
=
œÉ2
i
œÉiœÉj
‚ü®vi, vj‚ü©
= Œ¥ij,

Sec. 6.7
The Singular Value Decomposition and the Pseudoinverse
407
and hence {u1, u2, . . . , ur} is orthonormal. By Theorem 6.7(a) (p. 352), this
set extends to an orthonormal basis {u1, u2, . . . , ur, . . . , um} for W. Clearly
T(vi) = œÉiui if 1 ‚â§i ‚â§r. If i > r, then T‚àóT(vi) = 0, and so T(vi) = 0 by
Exercise 15(d) of Section 6.3.
To establish uniqueness, suppose that {v1, v2, . . . , vn}, {u1, u2, . . . , um},
and œÉ1 ‚â•œÉ2 ‚â•¬∑ ¬∑ ¬∑ ‚â•œÉr > 0 satisfy the properties stated in the Ô¨Årst part of
the theorem. Then for 1 ‚â§i ‚â§m and 1 ‚â§j ‚â§n,
‚ü®T‚àó(ui), vj‚ü©= ‚ü®ui, T(vj)‚ü©
=

œÉi
if i = j ‚â§r
0
otherwise,
and hence for any 1 ‚â§i ‚â§m,
T‚àó(ui) =
n

j=1
‚ü®T‚àó(ui), vj‚ü©vj =

œÉivi
if i = j ‚â§r
0
otherwise.
(5)
So for i ‚â§r,
T‚àóT(vi) = T‚àó(œÉiui) = œÉiT‚àó(ui) = œÉ2
i ui
and T‚àóT(vi) = T‚àó(0) = 0 for i > r. Therefore each vi is an eigenvector of
T‚àóT with corresponding eigenvalue œÉ2
i if i ‚â§r and 0 if i > r.
DeÔ¨Ånition. The unique scalars œÉ1, œÉ2, . . . , œÉr in Theorem 6.26 are called
the singular values of T. If r is less than both m and n, then the term
singular value is extended to include œÉr+1 = ¬∑ ¬∑ ¬∑ = œÉk = 0, where k is the
minimum of m and n.
Although the singular values of a linear transformation T are uniquely de-
termined by T, the orthonormal bases given in the statement of Theorem 6.26
are not uniquely determined because there is more than one orthonormal basis
of eigenvectors of T‚àóT.
In view of (5), the singular values of a linear transformation T: V ‚ÜíW
and its adjoint T‚àóare identical. Furthermore, the orthonormal bases for V
and W given in Theorem 6.26 are simply reversed for T‚àó.
Example 1
Let P2(R) and P1(R) be the polynomial spaces with inner products deÔ¨Åned
by
‚ü®f(x), g(x)‚ü©=
 1
‚àí1
f(t)g(t) dt.

408
Chap. 6
Inner Product Spaces
Let T: P2(R) ‚ÜíP1(R) be the linear transformation deÔ¨Åned by T(f(x)) =
f ‚Ä≤(x). Find orthonormal bases Œ≤ = {v1, v2, v3} for P2(R) and Œ≥ = {u1, u2} for
P1(R) such that T(vi) = œÉiui for i = 1, 2 and T(v3) = 0, where œÉ1 ‚â•œÉ2 > 0
are the nonzero singular values of T.
To facilitate the computations, we translate this problem into the corre-
sponding problem for a matrix representation of T. Caution is advised here
because not any matrix representation will do. Since the adjoint is deÔ¨Åned
in terms of inner products, we must use a matrix representation constructed
from orthonormal bases for P2(R) and P1(R) to guarantee that the adjoint
of the matrix representation of T is the same as the matrix representation of
the adjoint of T. (See Exercise 15 of Section 6.3.) For this purpose, we use
the results of Exercise 21(a) of Section 6.2 to obtain orthonormal bases
Œ± =

1
‚àö
2,
"
3
2 x,
"
5
8 (3x2 ‚àí1)
;
and
Œ±‚Ä≤ =

1
‚àö
2,
"
3
2 x
;
for P2(R) and P1(R), respectively.
Let
A = [T]Œ±‚Ä≤
Œ± =

0
‚àö
3
0
0
0
‚àö
15
	
.
Then
A‚àóA =
‚éõ
‚éù
0
0
‚àö
3
0
0
‚àö
15
‚éû
‚é†

0
‚àö
3
0
0
0
‚àö
15
	
=
‚éõ
‚éù
0
0
0
0
3
0
0
0
15
‚éû
‚é†,
which has eigenvalues (listed in descending order of size) Œª1 = 15, Œª2 = 3,
and Œª3 = 0. These eigenvalues correspond, respectively, to the orthonormal
eigenvectors e3 = (0, 0, 1), e2 = (0, 1, 0), and e1 = (1, 0, 0) in R3. Translating
everything into the context of T, P2(R), and P1(R), let
v1 =
"
5
8 (3x2 ‚àí1),
v2 =
"
3
2 x,
and
v3 =
1
‚àö
2.
Then Œ≤ = {v1, v2, v3} is an orthonormal basis for P2(R) consisting of eigen-
vectors of T‚àóT with corresponding eigenvalues Œª1, Œª2, and Œª3.
Now set
œÉ1 = ‚àöŒª1 =
‚àö
15 and œÉ2 = ‚àöŒª2 =
‚àö
3, the nonzero singular values of T,
and take
u1 = 1
œÉ1
T(v1) =
"
3
2 x
and
u2 = 1
œÉ2
T(v2) =
1
‚àö
2,
to obtain the required basis Œ≥ = {u1, u2} for P1(R).
‚ô¶

Sec. 6.7
The Singular Value Decomposition and the Pseudoinverse
409
We can use singular values to describe how a Ô¨Ågure is distorted by a linear
transformation. This is illustrated in the next example.
Example 2
Let T be an invertible linear operator on R2 and S = {x ‚ààR2 : ‚à•x‚à•= 1}, the
unit circle in R2. We apply Theorem 6.26 to describe S‚Ä≤ = T(S).
Since T is invertible, it has rank equal to 2 and hence has singular values
œÉ1 ‚â•œÉ2 > 0. Let {v1, v2} and Œ≤ = {u1, u2} be orthonormal bases for R2 so
that T(v1) = œÉ1u1 and T(v2) = œÉ2u2, as in Theorem 6.26. Then Œ≤ determines
a coordinate system, which we shall call the x‚Ä≤y‚Ä≤-coordinate system for R2,
where the x‚Ä≤-axis contains u1 and the y‚Ä≤-axis contains u2. For any vector
u ‚ààR2, if u = x‚Ä≤
1u1 + x‚Ä≤
2u2, then [u]Œ≤ =
x‚Ä≤
1
x‚Ä≤
2
	
is the coordinate vector of u
relative to Œ≤. We characterize S‚Ä≤ in terms of an equation relating x‚Ä≤
1 and x‚Ä≤
2.
For any vector v = x1v1 + x2v2 ‚ààR2, the equation u = T(v) means that
u = T(x1v1 + x2v2) = x1T(v1) + x2T(v2) = x1œÉ1u1 + x2œÉ2u2.
Thus for u = x‚Ä≤
1u1 + x‚Ä≤
2u2, we have x‚Ä≤
1 = x1œÉ1 and x‚Ä≤
2 = x2œÉ2. Furthermore,
u ‚ààS‚Ä≤ if and only if v ‚ààS if and only if
(x‚Ä≤
1)2
œÉ2
1
+ (x‚Ä≤
2)2
œÉ2
2
= x2
1 + x2
2 = 1.
If œÉ1 = œÉ2, this is the equation of a circle of radius œÉ1, and if œÉ1 > œÉ2, this is
the equation of an ellipse with major axis and minor axis oriented along the
x‚Ä≤-axis and the y‚Ä≤-axis, respectively. (See Figure 6.6.)
‚ô¶
................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................


X
X
X
y
v1
v2
S
?
s
v = x1v1 + x2v2
-
T












@
@
@
@
@
@
@
@
@
@
@
@





@
R
@
I
œÉ1
œÉ2
........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................



@
@
I
x‚Ä≤
y‚Ä≤
u1
u2
S‚Ä≤
?
s
u = x‚Ä≤
1u1 + x‚Ä≤
2u2
Figure 6.6

410
Chap. 6
Inner Product Spaces
The singular value theorem for linear transformations is useful in its ma-
trix form because we can perform numerical computations on matrices. We
begin with the deÔ¨Ånition of the singular values of a matrix.
DeÔ¨Ånition. Let A be an m √ó n matrix. We deÔ¨Åne the singular values
of A to be the singular values of the linear transformation LA.
Theorem 6.27 (Singular Value Decomposition Theorem for Ma-
trices).
Let A be an m √ó n matrix of rank r with the positive singular
values œÉ1 ‚â•œÉ2 ‚â•¬∑ ¬∑ ¬∑ ‚â•œÉr, and let Œ£ be the m √ó n matrix deÔ¨Åned by
Œ£ij =

œÉi
if i = j ‚â§r
0
otherwise.
Then there exists an m √ó m unitary matrix U and an n √ó n unitary matrix
V such that
A = UŒ£V ‚àó.
Proof. Let T = LA : Fn ‚ÜíFm. By Theorem 6.26, there exist orthonormal
bases Œ≤ = {v1, v2, . . . , vn} for Fn and Œ≥ = {u1, u2, . . . , um} for Fm such that
T(vi) = œÉiui for 1 ‚â§i ‚â§r and T(vi) = 0 for i > r. Let U be the m √ó m
matrix whose jth column is uj for all j, and let V be the n √ó n matrix whose
jth column is vj for all j. Note that both U and V are unitary matrices.
By Theorem 2.13(a) (p. 90), the jth column of AV is Avj = œÉjuj. Observe
that the jth column of Œ£ is œÉjej, where ej is the jth standard vector of Fm.
So by Theorem 2.13(a) and (b), the jth column of UŒ£ is given by
U(œÉjej) = œÉjU(ej) = œÉjuj.
It follows that AV and UŒ£ are m √ó n matrices whose corresponding columns
are equal, and hence AV = UŒ£. Therefore A = AV V ‚àó= UŒ£V ‚àó.
DeÔ¨Ånition.
Let A be an m √ó n matrix of rank r with positive singular
values œÉ1 ‚â•œÉ2 ‚â•¬∑ ¬∑ ¬∑ ‚â•œÉr. A factorization A = UŒ£V ‚àówhere U and V are
unitary matrices and Œ£ is the m √ó n matrix deÔ¨Åned as in Theorem 6.27 is
called a singular value decomposition of A.
In the proof of Theorem 6.27, the columns of V are the vectors in Œ≤, and
the columns of U are the vectors in Œ≥. Furthermore, the nonzero singular
values of A are the same as those of LA; hence they are the square roots of
the nonzero eigenvalues of A‚àóA or of AA‚àó. (See Exercise 9.)

Sec. 6.7
The Singular Value Decomposition and the Pseudoinverse
411
Example 3
We Ô¨Ånd a singular value decomposition for A =

1
1
‚àí1
1
1
‚àí1
	
.
First observe that for
v1 =
1
‚àö
3
‚éõ
‚éù
1
1
‚àí1
‚éû
‚é†,
v2 =
1
‚àö
2
‚éõ
‚éù
1
‚àí1
0
‚éû
‚é†,
and
v3 =
1
‚àö
6
‚éõ
‚éù
1
1
2
‚éû
‚é†,
the set Œ≤ = {v1, v2, v3} is an orthonormal basis for R3 consisting of eigen-
vectors of A‚àóA with corresponding eigenvalues Œª1 = 6, and Œª2 = Œª3 = 0.
Consequently, œÉ1 =
‚àö
6 is the only nonzero singular value of A. Hence, as in
the proof of Theorem 6.27, we let V be the matrix whose columns are the
vectors in Œ≤. Then
Œ£ =
‚àö
6
0
0
0
0
0
	
and
V =
‚éõ
‚éú
‚éú
‚éù
1
‚àö
3
1
‚àö
2
1
‚àö
6
1
‚àö
3
‚àí1
‚àö
2
1
‚àö
6
‚àí1
‚àö
3
0
2
‚àö
6
‚éû
‚éü
‚éü
‚é†.
Also, as in Theorem 6.27, we take
u1 = 1
œÉi
LA(v1) = 1
œÉi
Av1 =
1
‚àö
2

1
1
	
.
Next choose u2 =
1
‚àö
2

1
‚àí1
	
, a unit vector orthogonal to u1, to obtain the
orthonormal basis Œ≥ = {u1, u2} for R2, and set
U =
 1
‚àö
2
1
‚àö
2
1
‚àö
2
‚àí1
‚àö
2

.
Then A = UŒ£V ‚àóis the desired singular value decomposition.
‚ô¶
The Polar Decomposition of a Square Matrix
A singular value decomposition of a matrix can be used to factor a square
matrix in a manner analogous to the factoring of a complex number as the
product of a complex number of length 1 and a nonnegative number. In the
case of matrices, the complex number of length 1 is replaced by a unitary
matrix, and the nonnegative number is replaced by a positive semideÔ¨Ånite
matrix.
Theorem 6.28 (Polar Decomposition).
For any square matrix A,
there exists a unitary matrix W and a positive semideÔ¨Ånite matrix P such
that
A = WP.

412
Chap. 6
Inner Product Spaces
Furthermore, if A is invertible, then the representation is unique.
Proof. By Theorem 6.27, there exist unitary matrices U and V and a
diagonal matrix Œ£ with nonnegative diagonal entries such that A = UŒ£V ‚àó.
So
A = UŒ£V ‚àó= UV ‚àóV Œ£V ‚àó= WP,
where W = UV ‚àóand P = V Œ£V ‚àó. Since W is the product of unitary matrices,
W is unitary, and since Œ£ is positive semideÔ¨Ånite and P is unitarily equivalent
to Œ£, P is positive semideÔ¨Ånite by Exercise 14 of Section 6.5.
Now suppose that A is invertible and factors as the products
A = WP = ZQ,
where W and Z are unitary and P and Q are positive semideÔ¨Ånite. Since A
is invertible, it follows that P and Q are positive deÔ¨Ånite and invertible, and
therefore Z‚àóW = QP ‚àí1. Thus QP ‚àí1 is unitary, and so
I = (QP ‚àí1)‚àó(QP ‚àí1) = P ‚àí1Q2P ‚àí1.
Hence P 2 = Q2. Since both P and Q are positive deÔ¨Ånite, it follows that
P = Q by Exercise 17 of Section 6.4. Therefore W = Z, and consequently
the factorization is unique.
The factorization of a square matrix A as WP where W is unitary and P
is positive semideÔ¨Ånite, is called a polar decomposition of A.
Example 4
To Ô¨Ånd the polar decomposition of A =

11
‚àí5
‚àí2
10
	
, we begin by Ô¨Ånding a sin-
gular value decomposition UŒ£V ‚àóof A. The object is to Ô¨Ånd an orthonormal
basis Œ≤ for R2 consisting of eigenvectors of A‚àóA. It can be shown that
v1 =
1
‚àö
2

1
‚àí1
	
and
v2 =
1
‚àö
2

1
1
	
are orthonormal eigenvectors of A‚àóA with corresponding eigenvalues Œª1 = 200
and Œª2 = 50. So Œ≤ = {v1, v2} is an appropriate basis. Thus œÉ1 =
‚àö
200 =
10
‚àö
2 and œÉ2 =
‚àö
50 = 5
‚àö
2 are the singular values of A. So we have
V =
 1
‚àö
2
1
‚àö
2
‚àí1
‚àö
2
1
‚àö
2

and
Œ£ =
10
‚àö
2
0
0
5
‚àö
2
	
.
Next, we Ô¨Ånd the columns u1 and u2 of U:
u1 = 1
œÉ1
Av1 = 1
5

4
‚àí3
	
and
u2 = 1
œÉ2
Av2 = 1
5

3
4
	
.

Sec. 6.7
The Singular Value Decomposition and the Pseudoinverse
413
Thus
U =

4
5
3
5
‚àí3
5
4
5

.
Therefore, in the notation of Theorem 6.28, we have
W = UV ‚àó=

4
5
3
5
‚àí3
5
4
5
  1
‚àö
2
‚àí1
‚àö
2
1
‚àö
2
1
‚àö
2

=
1
5
‚àö
2

7
‚àí1
1
7
	
,
and
P = V Œ£V ‚àó=
 1
‚àö
2
1
‚àö
2
‚àí1
‚àö
2
1
‚àö
2
 
10
‚àö
2
0
0
5
‚àö
2
	  1
‚àö
2
‚àí1
‚àö
2
1
‚àö
2
1
‚àö
2

=
5
‚àö
2

3
‚àí1
‚àí1
3
	
.
‚ô¶
The Pseudoinverse
Let V and W be Ô¨Ånite-dimensional inner product spaces over the same
Ô¨Åeld, and let T: V ‚ÜíW be a linear transformation. It is desirable to have a
linear transformation from W to V that captures some of the essence of an
inverse of T even if T is not invertible. A simple approach to this problem
is to focus on the ‚Äúpart‚Äù of T that is invertible, namely, the restriction of
T to N(T)‚ä•. Let L: N(T)‚ä•‚ÜíR(T) be the linear transformation deÔ¨Åned by
L(x) = T(x) for all x ‚ààN(T)‚ä•. Then L is invertible, and we can use the
inverse of L to construct a linear transformation from W to V that salvages
some of the beneÔ¨Åts of an inverse of T.
DeÔ¨Ånition.
Let V and W be Ô¨Ånite-dimensional inner product spaces
over the same Ô¨Åeld, and let T: V ‚ÜíW be a linear transformation.
Let
L: N(T)‚ä•‚ÜíR(T) be the linear transformation deÔ¨Åned by L(x) = T(x) for all
x ‚ààN(T)‚ä•. The pseudoinverse (or Moore-Penrose generalized inverse) of
T, denoted by T ‚Ä†, is deÔ¨Åned as the unique linear transformation from W to
V such that
T‚Ä†(y) =

L‚àí1(y)
for y ‚ààR(T)
0
for y ‚ààR(T)‚ä•.
The pseudoinverse of a linear transformation T on a Ô¨Ånite-dimensional
inner product space exists even if T is not invertible.
Furthermore, if T
is invertible, then T‚Ä† = T‚àí1 because N(T)‚ä•= V, and L (as just deÔ¨Åned)
coincides with T.
As an extreme example, consider the zero transformation T0 : V ‚ÜíW
between two Ô¨Ånite-dimensional inner product spaces V and W. Then R(T0) =
{0}, and therefore T‚Ä† is the zero transformation from W to V.

414
Chap. 6
Inner Product Spaces
We can use the singular value theorem to describe the pseudoinverse of a
linear transformation. Suppose that V and W are Ô¨Ånite-dimensional vector
spaces and T: V ‚ÜíW is a linear transformation or rank r. Let {v1, v2, . . . , vn}
and {u1, u2, . . . , um} be orthonormal bases for V and W, respectively, and let
œÉ1 ‚â•œÉ2 ‚â•¬∑ ¬∑ ¬∑ ‚â•œÉr be the nonzero singular values of T satisfying (4) in Theo-
rem 6.26. Then {v1, v2, . . . , vr} is a basis for N(T)‚ä•, {vr+1, vr+2, . . . , vn} is a
basis for N(T), {u1, u2, . . . , ur} is a basis for R(T), and {ur+1, ur+2, . . . , um} is
a basis for R(T)‚ä•. Let L be the restriction of T to N(T)‚ä•, as in the deÔ¨Ånition
of pseudoinverse. Then L‚àí1(ui) = 1
œÉi
vi for 1 ‚â§i ‚â§r. Therefore
T‚Ä†(ui) =
‚éß
‚é®
‚é©
1
œÉi
vi
if 1 ‚â§i ‚â§r
0
if r < i ‚â§m.
(6)
Example 5
Let T: P2(R) ‚ÜíP1(R) be the linear transformation deÔ¨Åned by T(f(x)) =
f ‚Ä≤(x), as in Example 1. Let Œ≤ = {v1, v2, v3} and Œ≥ = {u1, u2} be the or-
thonormal bases for P2(R) and P1(R) in Example 1. Then œÉ1 =
‚àö
15 and
œÉ2 =
‚àö
3 are the nonzero singular values of T. It follows that
T‚Ä†
"
3
2x

= T‚Ä†(u1) = 1
œÉ1
v1 =
1
‚àö
15
"
5
8(3x2 ‚àí1),
and hence
T‚Ä†(x) = 1
6(3x2 ‚àí1).
Similarly, T‚Ä†(1) = x. Thus, for any polynomial a + bx ‚ààP1(R),
T‚Ä†(a + bx) = aT‚Ä†(1) + bT‚Ä†(x) = ax + b
6(3x2 ‚àí1).
‚ô¶
The Pseudoinverse of a Matrix
Let A be an m √ó n matrix. Then there exists a unique n √ó m matrix B
such that (LA)‚Ä† : Fm ‚ÜíFn is equal to the left-multiplication transformation
LB. We call B the pseudoinverse of A and denote it by B = A‚Ä†. Thus
(LA)‚Ä† = LA‚Ä†.
Let A be an m √ó n matrix of rank r. The pseudoinverse of A can be
computed with the aid of a singular value decomposition A = UŒ£V ‚àó. Let
Œ≤ and Œ≥ be the ordered bases whose vectors are the columns of V and U,

Sec. 6.7
The Singular Value Decomposition and the Pseudoinverse
415
respectively, and let œÉ1 ‚â•œÉ2 ‚â•¬∑ ¬∑ ¬∑ ‚â•œÉr be the nonzero singular values of
A. Then Œ≤ and Œ≥ are orthonormal bases for Fn and Fm, respectively, and (4)
and (6) are satisÔ¨Åed for T = LA. Reversing the roles of Œ≤ and Œ≥ in the proof
of Theorem 6.27, we obtain the following result.
Theorem 6.29. Let A be an m√ón matrix of rank r with a singular value
decomposition A = UŒ£V ‚àóand nonzero singular values œÉ1 ‚â•œÉ2 ‚â•¬∑ ¬∑ ¬∑ ‚â•œÉr.
Let Œ£‚Ä† be the n √ó m matrix deÔ¨Åned by
Œ£‚Ä†
ij =
‚éß
‚é®
‚é©
1
œÉi
if i = j ‚â§r
0
otherwise.
Then A‚Ä† = V Œ£‚Ä†U ‚àó, and this is a singular value decomposition of A‚Ä†.
Notice that Œ£‚Ä† as deÔ¨Åned in Theorem 6.29 is actually the pseudoinverse
of Œ£.
Example 6
We Ô¨Ånd A‚Ä† for the matrix A =

1
1
‚àí1
1
1
‚àí1
	
.
Since A is the matrix of Example 3, we can use the singular value decom-
position obtained in that example:
A = UŒ£V ‚àó=
 1
‚àö
2
1
‚àö
2
1
‚àö
2
‚àí1
‚àö
2
 ‚àö
6
0
0
0
0
0
	
‚éõ
‚éú
‚éú
‚éù
1
‚àö
3
1
‚àö
2
1
‚àö
6
1
‚àö
3
‚àí1
‚àö
2
1
‚àö
6
‚àí1
‚àö
3
0
2
‚àö
6
‚éû
‚éü
‚éü
‚é†
‚àó
.
By Theorem 6.29, we have
A‚Ä† = V Œ£‚Ä†U ‚àó=
‚éõ
‚éú
‚éú
‚éù
1
‚àö
3
1
‚àö
2
1
‚àö
6
1
‚àö
3
‚àí1
‚àö
2
1
‚àö
6
‚àí1
‚àö
3
0
2
‚àö
6
‚éû
‚éü
‚éü
‚é†
‚éõ
‚éú
‚éù
1
‚àö
6
0
0
0
0
0
‚éû
‚éü
‚é†
 1
‚àö
2
1
‚àö
2
1
‚àö
2
‚àí1
‚àö
2

= 1
6
‚éõ
‚éù
1
1
1
1
‚àí1
‚àí1
‚éû
‚é†.
‚ô¶
Notice that the linear transformation T of Example 5 is LA, where A is
the matrix of Example 6, and that T‚Ä† = LA‚Ä†.
The Pseudoinverse and Systems of Linear Equations
Let A be an m √ó n matrix with entries in F. Then for any b ‚ààFm, the
matrix equation Ax = b is a system of linear equations, and so it either has no
solutions, a unique solution, or inÔ¨Ånitely many solutions. We know that the

416
Chap. 6
Inner Product Spaces
system has a unique solution for every b ‚ààFm if and only if A is invertible,
in which case the solution is given by A‚àí1b. Furthermore, if A is invertible,
then A‚àí1 = A‚Ä†, and so the solution can be written as x = A‚Ä†b. If, on the
other hand, A is not invertible or the system Ax = b is inconsistent, then A‚Ä†b
still exists. We therefore pose the following question: In general, how is the
vector A‚Ä†b related to the system of linear equations Ax = b?
In order to answer this question, we need the following lemma.
Lemma. Let V and W be Ô¨Ånite-dimensional inner product spaces, and let
T: V ‚ÜíW be linear. Then
(a) T‚Ä†T is the orthogonal projection of V on N(T)‚ä•.
(b) TT‚Ä† is the orthogonal projection of W on R(T).
Proof. As in the earlier discussion, we deÔ¨Åne L: N(T)‚ä•‚ÜíW by L(x) =
T(x) for all x ‚ààN(T)‚ä•. If x ‚ààN(T)‚ä•, then T‚Ä†T(x) = L‚àí1L(x) = x, and if
x ‚ààN(T), then T‚Ä†T(x) = T‚Ä†(0) = 0. Consequently T‚Ä†T is the orthogonal
projection of V on N(T)‚ä•. This proves (a).
The proof of (b) is similar and is left as an exercise.
Theorem 6.30. Consider the system of linear equations Ax = b, where
A is an m √ó n matrix and b ‚ààFm. If z = A‚Ä†b, then z has the following
properties.
(a) If Ax = b is consistent, then z is the unique solution to the system
having minimum norm. That is, z is a solution to the system, and if y
is any solution to the system, then ‚à•z‚à•‚â§‚à•y‚à•with equality if and only
if z = y.
(b) If Ax = b is inconsistent, then z is the unique best approximation to a
solution having minimum norm. That is, ‚à•Az ‚àíb‚à•‚â§‚à•Ay ‚àíb‚à•for any
y ‚ààFn, with equality if and only if Az = Ay. Furthermore, if Az = Ay,
then ‚à•z‚à•‚â§‚à•y‚à•with equality if and only if z = y.
Proof. For convenience, let T = LA.
(a) Suppose that Ax = b is consistent, and let z = A‚Ä†b. Observe that
b ‚ààR(T), and therefore Az = AA‚Ä†b = TT‚Ä†(b) = b by part (b) of the lemma.
Thus z is a solution to the system. Now suppose that y is any solution to the
system. Then
T‚Ä†T(y) = A‚Ä†Ay = A‚Ä†b = z,
and hence z is the orthogonal projection of y on N(T)‚ä•by part (a) of the
lemma. Therefore, by the corollary to Theorem 6.6 (p. 350), we have that
‚à•z‚à•‚â§‚à•y‚à•with equality if and only if z = y.
(b) Suppose that Ax = b is inconsistent. By the lemma, Az = AA‚Ä†b =
TT‚Ä†(b) = b is the orthogonal projection of b on R(T); therefore, by the corol-
lary to Theorem 6.6 (p. 350), Az is the vector in R(T) nearest b. That is, if

Sec. 6.7
The Singular Value Decomposition and the Pseudoinverse
417
Ay is any other vector in R(T), then ‚à•Az ‚àíb‚à•‚â§‚à•Ay ‚àíb‚à•with equality if
and only if Az = Ay.
Finally, suppose that y is any vector in Fn such that Az = Ay = c. Then
A‚Ä†c = A‚Ä†Az = A‚Ä†AA‚Ä†b = A‚Ä†b = z
by Exercise 23; hence we may apply part (a) of this theorem to the system
Ax = c to conclude that ‚à•z‚à•‚â§‚à•y‚à•with equality if and only if z = y.
Note that the vector z = A‚Ä†b in Theorem 6.30 is the vector x0 described
in Theorem 6.12 that arises in the least squares application on pages 360‚Äì364.
Example 7
Consider the linear systems
x1 + x2 ‚àíx3 = 1
x1 + x2 ‚àíx3 = 1
and
x1 + x2 ‚àíx3 = 1
x1 + x2 ‚àíx3 = 2.
The Ô¨Årst system has inÔ¨Ånitely many solutions. Let A =

1
1
‚àí1
1
1
‚àí1
	
, the
coeÔ¨Écient matrix of the system, and let b =

1
1
	
. By Example 6,
A‚Ä† = 1
6
‚éõ
‚éù
1
1
1
1
‚àí1
‚àí1
‚éû
‚é†,
and therefore
z = A‚Ä†b = 1
6
‚éõ
‚éù
1
1
1
1
‚àí1
‚àí1
‚éû
‚é†

1
1
	
= 1
3
‚éõ
‚éù
1
1
‚àí1
‚éû
‚é†
is the solution of minimal norm by Theorem 6.30(a).
The second system is obviously inconsistent. Let b =

1
2
	
. Thus, al-
though
z = A‚Ä†b = 1
6
‚éõ
‚éù
1
1
1
1
‚àí1
‚àí1
‚éû
‚é†

1
2
	
= 1
2
‚éõ
‚éù
1
1
‚àí1
‚éû
‚é†
is not a solution to the second system, it is the ‚Äúbest approximation‚Äù to a
solution having minimum norm, as described in Theorem 6.30(b).
‚ô¶

418
Chap. 6
Inner Product Spaces
EXERCISES
1.
Label the following statements as true or false.
(a)
The singular values of any linear operator on a Ô¨Ånite-dimensional
vector space are also eigenvalues of the operator.
(b)
The singular values of any matrix A are the eigenvalues of A‚àóA.
(c)
For any matrix A and any scalar c, if œÉ is a singular value of A,
then |c|œÉ is a singular value of cA.
(d)
The singular values of any linear operator are nonnegative.
(e)
If Œª is an eigenvalue of a self-adjoint matrix A, then Œª is a singular
value of A.
(f)
For any m√ón matrix A and any b ‚ààFn, the vector A‚Ä†b is a solution
to Ax = b.
(g)
The pseudoinverse of any linear operator exists even if the operator
is not invertible.
2.
Let T: V ‚ÜíW be a linear transformation of rank r, where V and W
are Ô¨Ånite-dimensional inner product spaces. In each of the following,
Ô¨Ånd orthonormal bases {v1, v2, . . . , vn} for V and {u1, u2, . . . , um} for
W, and the nonzero singular values œÉ1 ‚â•œÉ2 ‚â•¬∑ ¬∑ ¬∑ ‚â•œÉr of T such that
T(vi) = œÉiui for 1 ‚â§i ‚â§r.
(a)
T: R2 ‚ÜíR3 deÔ¨Åned by T(x1, x2) = (x1, x1 + x2, x1 ‚àíx2)
(b)
T: P2(R) ‚ÜíP1(R), where T(f(x)) = f ‚Ä≤‚Ä≤(x), and the inner prod-
ucts are deÔ¨Åned as in Example 1
(c)
Let V = W = span({1, sin x, cos x}) with the inner product deÔ¨Åned
by ‚ü®f, g‚ü©=
 2œÄ
0
f(t)g(t) dt, and T is deÔ¨Åned by T(f) = f ‚Ä≤ + 2f
(d)
T: C2 ‚ÜíC2 deÔ¨Åned by T(z1, z2) = ((1 ‚àíi)z2, (1 + i)z1 + z2)
3.
Find a singular value decomposition for each of the following matrices.
(a)
‚éõ
‚éù
1
1
1
1
‚àí1
‚àí1
‚éû
‚é†
(b)

1
0
1
1
0
‚àí1
	
(c)
‚éõ
‚éú
‚éú
‚éù
1
1
0
1
1
0
1
1
‚éû
‚éü
‚éü
‚é†
(d)
‚éõ
‚éù
1
1
1
1
‚àí1
0
1
0
‚àí1
‚éû
‚é†
(e)

1 + i
1
1 ‚àíi
‚àíi
	
(f)
‚éõ
‚éù
1
1
1
1
1
0
‚àí2
1
1
‚àí1
1
1
‚éû
‚é†
4.
Find a polar decomposition for each of the following matrices.
(a)

1
1
2
‚àí2
	
(b)
‚éõ
‚éù
20
4
0
0
0
1
4
20
0
‚éû
‚é†
5.
Find an explicit formula for each of the following expressions.

Sec. 6.7
The Singular Value Decomposition and the Pseudoinverse
419
(a)
T‚Ä†(x1, x2, x3), where T is the linear transformation of Exercise 2(a)
(b)
T‚Ä†(a + bx + cx2), where T is the linear transformation of Exer-
cise 2(b)
(c)
T‚Ä†(a + b sin x + c cos x), where T is the linear transformation of
Exercise 2(c)
(d)
T‚Ä†(z1, z2), where T is the linear transformation of Exercise 2(d)
6.
Use the results of Exercise 3 to Ô¨Ånd the pseudoinverse of each of the
following matrices.
(a)
‚éõ
‚éù
1
1
1
1
‚àí1
‚àí1
‚éû
‚é†
(b)

1
0
1
1
0
‚àí1
	
(c)
‚éõ
‚éú
‚éú
‚éù
1
1
0
1
1
0
1
1
‚éû
‚éü
‚éü
‚é†
(d)
‚éõ
‚éù
1
1
1
1
‚àí1
0
1
0
‚àí1
‚éû
‚é†
(e)

1 + i
1
1 ‚àíi
‚àíi
	
(f)
‚éõ
‚éù
1
1
1
1
1
0
‚àí2
1
1
‚àí1
1
1
‚éû
‚é†
7.
For each of the given linear transformations T: V ‚ÜíW,
(i) Describe the subspace Z1 of V such that T‚Ä†T is the orthogonal
projection of V on Z1.
(ii) Describe the subspace Z2 of W such that TT‚Ä† is the orthogonal
projection of W on Z2.
(a)
T is the linear transformation of Exercise 2(a)
(b)
T is the linear transformation of Exercise 2(b)
(c)
T is the linear transformation of Exercise 2(c)
(d)
T is the linear transformation of Exercise 2(d)
8.
For each of the given systems of linear equations,
(i) If the system is consistent, Ô¨Ånd the unique solution having mini-
mum norm.
(ii) If the system is inconsistent, Ô¨Ånd the ‚Äúbest approximation to a
solution‚Äù having minimum norm, as described in Theorem 6.30(b).
(Use your answers to parts (a) and (f) of Exercise 6.)
(a)
x1 +
x2 = 1
x1 +
x2 = 2
‚àíx1 + ‚àíx2 = 0
(b)
x1 + x2 + x3 + x4 =
2
x1
‚àí2x3 + x4 = ‚àí1
x1 ‚àíx2 + x3 + x4 =
2
9.
Let V and W be Ô¨Ånite-dimensional inner product spaces over F, and sup-
pose that {v1, v2, . . . , vn} and {u1, u2, . . . , um} are orthonormal bases
for V and W, respectively. Let T: V ‚ÜíW is a linear transformation of
rank r, and suppose that œÉ1 ‚â•œÉ2 ‚â•¬∑ ¬∑ ¬∑ ‚â•œÉr > 0 are such that
T(vi) =

œÉiui
if 1 ‚â§i ‚â§r
0
if r < i.

420
Chap. 6
Inner Product Spaces
(a)
Prove that {u1, u2, . . . , um} is a set of eigenvectors of TT‚àówith
corresponding eigenvalues Œª1, Œª2, . . . , Œªm, where
Œªi =

œÉ2
i
if 1 ‚â§i ‚â§r
0
if r < i.
(b)
Let A be an m√ón matrix with real or complex entries. Prove that
the nonzero singular values of A are the positive square roots of
the nonzero eigenvalues of AA‚àó, including repetitions.
(c)
Prove that TT‚àóand T‚àóT have the same nonzero eigenvalues, in-
cluding repetitions.
(d)
State and prove a result for matrices analogous to (c).
10.
Use Exercise 8 of Section 2.5 to obtain another proof of Theorem 6.27,
the singular value decomposition theorem for matrices.
11.
This exercise relates the singular values of a well-behaved linear operator
or matrix to its eigenvalues.
(a)
Let T be a normal linear operator on an n-dimensional inner prod-
uct space with eigenvalues Œª1, Œª2, . . . , Œªn. Prove that the singular
values of T are |Œª1|, |Œª2|, . . . , |Œªn|.
(b)
State and prove a result for matrices analogous to (a).
12.
Let A be a normal matrix with an orthonormal basis of eigenvectors
Œ≤ = {v1, v2, . . . , vn} and corresponding eigenvalues Œª1, Œª2, . . . , Œªn. Let
V be the n √ó n matrix whose columns are the vectors in Œ≤. Prove that
for each i there is a scalar Œ∏i of absolute value 1 such that if U is the
n √ó n matrix with Œ∏ivi as column i and Œ£ is the diagonal matrix such
that Œ£ii = |Œªi| for each i, then UŒ£V ‚àóis a singular value decomposition
of A.
13.
Prove that if A is a positive semideÔ¨Ånite matrix, then the singular values
of A are the same as the eigenvalues of A.
14.
Prove that if A is a positive deÔ¨Ånite matrix and A = UŒ£V ‚àóis a singular
value decomposition of A, then U = V .
15.
Let A be a square matrix with a polar decomposition A = WP.
(a)
Prove that A is normal if and only if WP 2 = P 2W.
(b)
Use (a) to prove that A is normal if and only if WP = PW.
16.
Let A be a square matrix. Prove an alternate form of the polar de-
composition for A: There exists a unitary matrix W and a positive
semideÔ¨Ånite matrix P such that A = PW.

Sec. 6.7
The Singular Value Decomposition and the Pseudoinverse
421
17.
Let T and U be linear operators on R2 deÔ¨Åned for all (x1, x2) ‚ààR2 by
T(x1, x2) = (x1, 0) and U(x1, x2) = (x1 + x2, 0).
(a)
Prove that (UT)‚Ä† Ã∏= T‚Ä†U‚Ä†.
(b)
Exhibit matrices A and B such that AB is deÔ¨Åned, but (AB)‚Ä† Ã∏=
B‚Ä†A‚Ä†.
18.
Let A be an m √ó n matrix. Prove the following results.
(a)
For any m √ó m unitary matrix G, (GA)‚Ä† = A‚Ä†G‚àó.
(b)
For any n √ó n unitary matrix H, (AH)‚Ä† = H‚àóA‚Ä†.
19.
Let A be a matrix with real or complex entries. Prove the following
results.
(a)
The nonzero singular values of A are the same as the nonzero
singular values of A‚àó, which are the same as the nonzero singular
values of At.
(b)
(A‚Ä†)‚àó= (A‚àó)‚Ä†.
(c)
(A‚Ä†)t = (At)‚Ä†.
20.
Let A be a square matrix such that A2 = O. Prove that (A‚Ä†)2 = O.
21.
Let V and W be Ô¨Ånite-dimensional inner product spaces, and let
T: V ‚ÜíW be linear. Prove the following results.
(a)
TT‚Ä†T = T.
(b)
T‚Ä†TT‚Ä† = T‚Ä†.
(c)
Both T‚Ä†T and TT‚Ä† are self-adjoint.
The preceding three statements are called the Penrose conditions,
and they characterize the pseudoinverse of a linear transformation as
shown in Exercise 22.
22.
Let V and W be Ô¨Ånite-dimensional inner product spaces. Let T: V ‚ÜíW
and U: W ‚ÜíV be linear transformations such that TUT = T, UTU = U,
and both UT and TU are self-adjoint. Prove that U = T‚Ä†.
23.
State and prove a result for matrices that is analogous to the result of
Exercise 21.
24.
State and prove a result for matrices that is analogous to the result of
Exercise 22.
25.
Let V and W be Ô¨Ånite-dimensional inner product spaces, and let
T: V ‚ÜíW be linear. Prove the following results.
(a)
If T is one-to-one, then T‚àóT is invertible and T‚Ä† = (T‚àóT)‚àí1T‚àó.
(b)
If T is onto, then TT‚àóis invertible and T‚Ä† = T‚àó(TT‚àó)‚àí1.

422
Chap. 6
Inner Product Spaces
26.
Let V and W be Ô¨Ånite-dimensional inner product spaces with orthonor-
mal bases Œ≤ and Œ≥, respectively, and let T: V ‚ÜíW be linear. Prove
that ([T]Œ≥
Œ≤)‚Ä† = [T‚Ä†]Œ≤
Œ≥.
27.
Let V and W be Ô¨Ånite-dimensional inner product spaces, and let
T: V ‚ÜíW be a linear transformation. Prove part (b) of the lemma
to Theorem 6.30: TT‚Ä† is the orthogonal projection of W on R(T).
6.8‚àó
BILINEAR AND QUADRATIC FORMS
There is a certain class of scalar-valued functions of two variables deÔ¨Åned on
a vector space that arises in the study of such diverse subjects as geometry
and multivariable calculus. This is the class of bilinear forms. We study the
basic properties of this class with a special emphasis on symmetric bilinear
forms, and we consider some of its applications to quadratic surfaces and
multivariable calculus.
Bilinear Forms
DeÔ¨Ånition.
Let V be a vector space over a Ô¨Åeld F. A function H from
the set V √ó V of ordered pairs of vectors to F is called a bilinear form on V
if H is linear in each variable when the other variable is held Ô¨Åxed; that is,
H is a bilinear form on V if
(a) H(ax1 + x2, y) = aH(x1, y) + H(x2, y)
for all x1, x2, y ‚ààV and a ‚ààF
(b) H(x, ay1 + y2) = aH(x, y1) + H(x, y2)
for all x, y1, y2 ‚ààV and a ‚ààF.
We denote the set of all bilinear forms on V by B(V). Observe that an
inner product on a vector space is a bilinear form if the underlying Ô¨Åeld is
real, but not if the underlying Ô¨Åeld is complex.
Example 1
DeÔ¨Åne a function H : R2 √ó R2 ‚ÜíR by
H

a1
a2
	
,

b1
b2
		
= 2a1b1 + 3a1b2 + 4a2b1 ‚àía2b2
for

a1
a2
	
,

b1
b2
	
‚ààR2.
We could verify directly that H is a bilinear form on R2. However, it is more
enlightening and less tedious to observe that if
A =

2
3
4
‚àí1
	
,
x =

a1
a2
	
,
and
y =

b1
b2
	
,
then
H(x, y) = xtAy.
The bilinearity of H now follows directly from the distributive property of
matrix multiplication over matrix addition.
‚ô¶

Sec. 6.8
Bilinear and Quadratic Forms
423
The preceding bilinear form is a special case of the next example.
Example 2
Let V = Fn, where the vectors are considered as column vectors. For any
A ‚ààMn√ón(F), deÔ¨Åne H : V √ó V ‚ÜíF by
H(x, y) = xtAy
for x, y ‚ààV.
Notice that since x and y are n√ó1 matrices and A is an n√ón matrix, H(x, y)
is a 1√ó1 matrix. We identify this matrix with its single entry. The bilinearity
of H follows as in Example 1. For example, for a ‚ààF and x1, x2, y ‚ààV, we
have
H(ax1 + x2, y) = (ax1 + x2)tAy = (axt
1 + xt
2)Ay
= axt
1Ay + xt
2Ay
= aH(x1, y) + H(x2, y).
‚ô¶
We list several properties possessed by all bilinear forms. Their proofs are
left to the reader (see Exercise 2).
For any bilinear form H on a vector space V over a Ô¨Åeld F, the following
properties hold.
1. If, for any x ‚ààV, the functions Lx, Rx : V ‚ÜíF are deÔ¨Åned by
Lx(y) = H(x, y)
and
Rx(y) = H(y, x)
for all y ‚ààV,
then Lx and Rx are linear.
2. H(0, x) = H(x, 0) = 0 for all x ‚ààV.
3. For all x, y, z, w ‚ààV,
H(x + y, z + w) = H(x, z) + H(x, w) + H(y, z) + H(y, w).
4. If J : V √ó V ‚ÜíF is deÔ¨Åned by J(x, y) = H(y, x), then J is a bilinear
form.
DeÔ¨Ånitions.
Let V be a vector space, let H1 and H2 be bilinear forms
on V, and let a be a scalar. We deÔ¨Åne the sum H1 + H2 and the scalar
product aH1 by the equations
(H1 + H2)(x, y) = H1(x, y) + H2(x, y)
and
(aH1)(x, y) = a(H1(x, y))
for all x, y ‚ààV.
The following theorem is an immediate consequence of the deÔ¨Ånitions.

424
Chap. 6
Inner Product Spaces
Theorem 6.31.
For any vector space V, the sum of two bilinear forms
and the product of a scalar and a bilinear form on V are again bilinear forms
on V. Furthermore, B(V) is a vector space with respect to these operations.
Proof. Exercise.
Let Œ≤ = {v1, v2, . . . , vn} be an ordered basis for an n-dimensional vector
space V, and let H ‚ààB(V). We can associate with H an n √ó n matrix A
whose entry in row i and column j is deÔ¨Åned by
Aij = H(vi, vj)
for i, j = 1, 2, . . . , n.
DeÔ¨Ånition.
The matrix A above is called the matrix representation
of H with respect to the ordered basis Œ≤ and is denoted by œàŒ≤(H).
We can therefore regard œàŒ≤ as a mapping from B(V) to Mn√ón(F), where
F is the Ô¨Åeld of scalars for V, that takes a bilinear form H into its matrix
representation œàŒ≤(H). We Ô¨Årst consider an example and then show that œàŒ≤
is an isomorphism.
Example 3
Consider the bilinear form H of Example 1, and let
Œ≤ =

1
1
	
,

1
‚àí1
	
and
B = œàŒ≤(H).
Then
B11 = H

1
1
	
,

1
1
		
= 2 + 3 + 4 ‚àí1 = 8,
B12 = H

1
1
	
,

1
‚àí1
		
= 2 ‚àí3 + 4 + 1 = 4,
B21 = H

1
‚àí1
	
,

1
1
		
= 2 + 3 ‚àí4 + 1 = 2,
and
B22 = H

1
‚àí1
	
,

1
‚àí1
		
= 2 ‚àí3 ‚àí4 ‚àí1 = ‚àí6.
So
œàŒ≤(H) =

8
4
2
‚àí6
	
.
If Œ≥ is the standard ordered basis for R2, the reader can verify that
œàŒ≥(H) =

2
3
4
‚àí1
	
.
‚ô¶

Sec. 6.8
Bilinear and Quadratic Forms
425
Theorem 6.32.
For any n-dimensional vector space V over F and any
ordered basis Œ≤ for V, œàŒ≤ : B(V) ‚ÜíMn√ón(F) is an isomorphism.
Proof. We leave the proof that œàŒ≤ is linear to the reader.
To show that œàŒ≤ is one-to-one, suppose that œàŒ≤(H) = O for some H ‚àà
B(V). Fix vi ‚ààŒ≤, and recall the mapping Lvi : V ‚ÜíF, which is linear by
property 1 on page 423. By hypothesis, Lvi(vj) = H(vi, vj) = 0 for all vj ‚ààŒ≤.
Hence Lvi is the zero transformation from V to F. So
H(vi, x) = Lvi(x) = 0
for all x ‚ààV and vi ‚ààŒ≤.
(7)
Next Ô¨Åx an arbitrary y ‚ààV, and recall the linear mapping Ry : V ‚ÜíF deÔ¨Åned
in property 1 on page 423. By (7), Ry(vi) = H(vi, y) = 0 for all vi ‚ààŒ≤, and
hence Ry is the zero transformation. So H(x, y) = Ry(x) = 0 for all x, y ‚ààV.
Thus H is the zero bilinear form, and therefore œàŒ≤ is one-to-one.
To show that œàŒ≤ is onto, consider any A ‚ààMn√ón(F). Recall the isomor-
phism œÜŒ≤ : V ‚ÜíFn deÔ¨Åned in Section 2.4. For x ‚ààV, we view œÜŒ≤(x) ‚ààFn as
a column vector. Let H : V √ó V ‚ÜíF be the mapping deÔ¨Åned by
H(x, y) = [œÜŒ≤(x)]tA[œÜŒ≤(y)]
for all x, y ‚ààV.
A slight embellishment of the method of Example 2 can be used to prove that
H ‚ààB(V). We show that œàŒ≤(H) = A. Let vi, vj ‚ààŒ≤. Then œÜŒ≤(vi) = ei and
œÜŒ≤(vj) = ej; hence, for any i and j,
H(vi, vj) = [œÜŒ≤(vi)]tA[œÜŒ≤(vj)] = et
iAej = Aij.
We conclude that œàŒ≤(H) = A and œàŒ≤ is onto.
Corollary 1.
For any n-dimensional vector space V, B(V) has dimen-
sion n2.
Proof. Exercise.
The following corollary is easily established by reviewing the proof of
Theorem 6.32.
Corollary 2.
Let V be an n-dimensional vector space over F with
ordered basis Œ≤. If H ‚ààB(V) and A ‚ààMn√ón(F), then œàŒ≤(H) = A if and
only if H(x, y) = [œÜŒ≤(x)]tA[œÜŒ≤(y)] for all x, y ‚ààV.
The following result is now an immediate consequence of Corollary 2.
Corollary 3. Let F be a Ô¨Åeld, n a positive integer, and Œ≤ be the standard
ordered basis for Fn. Then for any H ‚ààB(Fn), there exists a unique matrix
A ‚ààMn√ón(F), namely, A = œàŒ≤(H), such that
H(x, y) = xtAy
for all x, y ‚ààFn.

426
Chap. 6
Inner Product Spaces
Example 4
DeÔ¨Åne a function H : R2 √ó R2 ‚ÜíR by
H

a1
a2
	
,

b1
b2
		
= det

a1
b1
a2
b2
	
= a1b2 ‚àía2b1
for

a1
a2
	
,

b1
b2
	
‚ààR2.
It can be shown that H is a bilinear form. We Ô¨Ånd the matrix A in Corollary 3
such that H(x, y) = xtAy for all x, y ‚ààR2.
Since Aij = H(ei, ej) for all i and j, we have
A11 = det

1
1
0
0
	
= 0
A12 = det

1
0
0
1
	
= 1,
A21 = det

0
1
1
0
	
= ‚àí1
and
A22 = det

0
0
1
1
	
= 0.
Therefore A =

0
1
‚àí1
0
	
.
‚ô¶
There is an analogy between bilinear forms and linear operators on Ô¨Ånite-
dimensional vector spaces in that both are associated with unique square
matrices and the correspondences depend on the choice of an ordered basis for
the vector space. As in the case of linear operators, one can pose the following
question: How does the matrix corresponding to a Ô¨Åxed bilinear form change
when the ordered basis is changed?
As we have seen, the corresponding
question for matrix representations of linear operators leads to the deÔ¨Ånition
of the similarity relation on square matrices. In the case of bilinear forms,
the corresponding question leads to another relation on square matrices, the
congruence relation.
DeÔ¨Ånition.
Let A, B ‚ààMn√ón(F). Then B is said to be congruent to
A if there exists an invertible matrix Q ‚ààMn√ón(F) such that B = QtAQ.
Observe that the relation of congruence is an equivalence relation (see
Exercise 12).
The next theorem relates congruence to the matrix representation of a
bilinear form.
Theorem 6.33. Let V be a Ô¨Ånite-dimensional vector space with ordered
bases Œ≤ = {v1, v2, . . . , vn} and Œ≥ = {w1, w2, . . . , wn}, and let Q be the change
of coordinate matrix changing Œ≥-coordinates into Œ≤-coordinates. Then, for
any H ‚ààB(V), we have œàŒ≥(H) = QtœàŒ≤(H)Q. Therefore œàŒ≥(H) is congruent
to œàŒ≤(H).
Proof. There are essentially two proofs of this theorem. One involves a
direct computation, while the other follows immediately from a clever obser-
vation. We give the more direct proof here, leaving the other proof for the
exercises (see Exercise 13).

Sec. 6.8
Bilinear and Quadratic Forms
427
Suppose that A = œàŒ≤(H) and B = œàŒ≥(H). Then for 1 ‚â§i, j ‚â§n,
wi =
n

k=1
Qkivk
and
wj =
n

r=1
Qrjvr.
Thus
Bij = H(wi, wj) = H
 n

k=1
Qkivk, wj

=
n

k=1
QkiH(vk, wj)
=
n

k=1
QkiH

vk,
n

r=1
Qrjvr

=
n

k=1
Qki
n

r=1
QrjH(vk, vr)
=
n

k=1
Qki
n

r=1
QrjAkr
=
n

k=1
Qki
n

r=1
AkrQrj
=
n

k=1
Qki(AQ)kj
=
n

k=1
Qt
ik(AQ)kj = (QtAQ)ij.
Hence B = QtAQ.
The following result is the converse of Theorem 6.33.
Corollary. Let V be an n-dimensional vector space with ordered basis Œ≤,
and let H be a bilinear form on V. For any n √ó n matrix B, if B is congruent
to œàŒ≤(H), then there exists an ordered basis Œ≥ for V such that œàŒ≥(H) = B.
Furthermore, if B = QtœàŒ≤(H)Q for some invertible matrix Q, then Q changes
Œ≥-coordinates into Œ≤-coordinates.
Proof. Suppose that B = QtœàŒ≤(H)Q for some invertible matrix Q and
that Œ≤ = {v1, v2, . . . , vn}. Let Œ≥ = {w1, w2, . . . , wn}, where
wj =
n

i=1
Qijvi
for 1 ‚â§j ‚â§n.

428
Chap. 6
Inner Product Spaces
Since Q is invertible, Œ≥ is an ordered basis for V, and Q is the change of
coordinate matrix that changes Œ≥-coordinates into Œ≤-coordinates. Therefore,
by Theorem 6.32,
B = QtœàŒ≤(H)Q = œàŒ≥(H).
Symmetric Bilinear Forms
Like the diagonalization problem for linear operators, there is an analogous
diagonalization problem for bilinear forms, namely, the problem of determin-
ing those bilinear forms for which there are diagonal matrix representations.
As we will see, there is a close relationship between diagonalizable bilinear
forms and those that are called symmetric.
DeÔ¨Ånition.
A bilinear form H on a vector space V is symmetric if
H(x, y) = H(y, x) for all x, y ‚ààV.
As the name suggests, symmetric bilinear forms correspond to symmetric
matrices.
Theorem 6.34. Let H be a bilinear form on a Ô¨Ånite-dimensional vector
space V, and let Œ≤ be an ordered basis for V. Then H is symmetric if and
only if œàŒ≤(H) is symmetric.
Proof. Let Œ≤ = {v1, v2, . . . , vn} and B = œàŒ≤(H).
First assume that H is symmetric. Then for 1 ‚â§i, j ‚â§n,
Bij = H(vi, vj) = H(vj, vi) = Bji,
and it follows that B is symmetric.
Conversely, suppose that B is symmetric. Let J : V √ó V ‚ÜíF, where F is
the Ô¨Åeld of scalars for V, be the mapping deÔ¨Åned by J(x, y) = H(y, x) for all
x, y ‚ààV. By property 4 on page 423, J is a bilinear form. Let C = œàŒ≤(J).
Then, for 1 ‚â§i, j ‚â§n,
Cij = J(vi, vj) = H(vj, vi) = Bji = Bij.
Thus C = B. Since œàŒ≤ is one-to-one, we have J = H. Hence H(y, x) =
J(x, y) = H(x, y) for all x, y ‚ààV, and therefore H is symmetric.
DeÔ¨Ånition. A bilinear form H on a Ô¨Ånite-dimensional vector space V is
called diagonalizable if there is an ordered basis Œ≤ for V such that œàŒ≤(H)
is a diagonal matrix.
Corollary. Let H be a diagonalizable bilinear form on a Ô¨Ånite-dimensional
vector space V. Then H is symmetric.

Sec. 6.8
Bilinear and Quadratic Forms
429
Proof. Suppose that H is diagonalizable. Then there is an ordered basis Œ≤
for V such that œàŒ≤(H) = D is a diagonal matrix. Trivially, D is a symmetric
matrix, and hence, by Theorem 6.34, H is symmetric.
Unfortunately, the converse is not true, as is illustrated by the following
example.
Example 5
Let F = Z2, V = F2, and H : V √ó V ‚ÜíF be the bilinear form deÔ¨Åned by
H

a1
a2
	
,

b1
b2
		
= a1b2 + a2b1.
Clearly H is symmetric. In fact, if Œ≤ is the standard ordered basis for V, then
A = œàŒ≤(H) =

0
1
1
0
	
,
a symmetric matrix. We show that H is not diagonalizable.
By way of contradiction, suppose that H is diagonalizable. Then there is
an ordered basis Œ≥ for V such that B = œàŒ≥(H) is a diagonal matrix. So by
Theorem 6.33, there exists an invertible matrix Q such that B = QtAQ. Since
Q is invertible, it follows that rank(B) = rank(A) = 2, and consequently the
diagonal entries of B are nonzero. Since the only nonzero scalar of F is 1,
B =

1
0
0
1
	
.
Suppose that
Q =

a
b
c
d
	
.
Then

1
0
0
1
	
= B = QtAQ
=

a
c
b
d
	 
0
1
1
0
	 
a
b
c
d
	
=

ac + ac
bc + ad
bc + ad
bd + bd
	
.
But p + p = 0 for all p ‚ààF; hence ac + ac = 0. Thus, comparing the row
1, column 1 entries of the matrices in the equation above, we conclude that
1 = 0, a contradiction. Therefore H is not diagonalizable.
‚ô¶
The bilinear form of Example 5 is an anomaly. Its failure to be diagonal-
izable is due to the fact that the scalar Ô¨Åeld Z2 is of characteristic two. Recall

430
Chap. 6
Inner Product Spaces
from Appendix C that a Ô¨Åeld F is of characteristic two if 1 + 1 = 0 in F.
If F is not of characteristic two, then 1 + 1 = 2 has a multiplicative inverse,
which we denote by 1/2.
Before proving the converse of the corollary to Theorem 6.34 for scalar
Ô¨Åelds that are not of characteristic two, we establish the following lemma.
Lemma. Let H be a nonzero symmetric bilinear form on a vector space
V over a Ô¨Åeld F not of characteristic two. Then there is a vector x in V such
that H(x, x) Ã∏= 0.
Proof. Since H is nonzero, we can choose vectors u, v ‚ààV such that
H(u, v) Ã∏= 0.
If H(u, u) Ã∏= 0 or H(v, v) Ã∏= 0, there is nothing to prove.
Otherwise, set x = u + v. Then
H(x, x) = H(u, u) + H(u, v) + H(v, u) + H(v, v) = 2H(u, v) Ã∏= 0
because 2 Ã∏= 0 and H(u, v) Ã∏= 0.
Theorem 6.35.
Let V be a Ô¨Ånite-dimensional vector space over a Ô¨Åeld
F not of characteristic two.
Then every symmetric bilinear form on V is
diagonalizable.
Proof. We use mathematical induction on n = dim(V). If n = 1, then every
element of B(V) is diagonalizable. Now suppose that the theorem is valid
for vector spaces of dimension less than n for some Ô¨Åxed integer n > 1, and
suppose that dim(V) = n. If H is the zero bilinear form on V, then trivially H
is diagonalizable; so suppose that H is a nonzero symmetric bilinear form on
V. By the lemma, there exists a nonzero vector x in V such that H(x, x) Ã∏= 0.
Recall the function Lx : V ‚ÜíF deÔ¨Åned by Lx(y) = H(x, y) for all y ‚ààV. By
property 1 on page 423, Lx is linear. Furthermore, since Lx(x) = H(x, x) Ã∏= 0,
Lx is nonzero. Consequently, rank(Lx) = 1, and hence dim(N(Lx)) = n ‚àí1.
The restriction of H to N(Lx) is obviously a symmetric bilinear form on
a vector space of dimension n ‚àí1. Thus, by the induction hypothesis, there
exists an ordered basis {v1, v2, . . . , vn‚àí1} for N(Lx) such that H(vi, vj) = 0
for i Ã∏= j (1 ‚â§i, j ‚â§n ‚àí1).
Set vn = x.
Then vn /‚ààN(Lx), and so
Œ≤ = {v1, v2, . . . , vn} is an ordered basis for V.
In addition, H(vi, vn) =
H(vn, vi) = 0 for i = 1, 2, . . . , n ‚àí1. We conclude that œàŒ≤(H) is a diagonal
matrix, and therefore H is diagonalizable.
Corollary. Let F be a Ô¨Åeld that is not of characteristic two.
If A ‚àà
Mn√ón(F) is a symmetric matrix, then A is congruent to a diagonal matrix.
Proof. Exercise.

Sec. 6.8
Bilinear and Quadratic Forms
431
Diagonalization of Symmetric Matrices
Let A be a symmetric n √ó n matrix with entries from a Ô¨Åeld F not of
characteristic two.
By the corollary to Theorem 6.35, there are matrices
Q, D ‚ààMn√ón(F) such that Q is invertible, D is diagonal, and QtAQ = D. We
now give a method for computing Q and D. This method requires familiarity
with elementary matrices and their properties, which the reader may wish to
review in Section 3.1.
If E is an elementary n√ón matrix, then AE can be obtained by performing
an elementary column operation on A. By Exercise 21, EtA can be obtained
by performing the same operation on the rows of A rather than on its columns.
Thus EtAE can be obtained from A by performing an elementary operation
on the columns of A and then performing the same operation on the rows
of AE. (Note that the order of the operations can be reversed because of
the associative property of matrix multiplication.)
Suppose that Q is an
invertible matrix and D is a diagonal matrix such that QtAQ = D.
By
Corollary 3 to Theorem 3.6 (p. 159), Q is a product of elementary matrices,
say Q = E1E2 ¬∑ ¬∑ ¬∑ Ek. Thus
D = QtAQ = Et
kEt
k‚àí1 ¬∑ ¬∑ ¬∑ Et
1AE1E2 ¬∑ ¬∑ ¬∑ Ek.
From the preceding equation, we conclude that by means of several elemen-
tary column operations and the corresponding row operations, A can be trans-
formed into a diagonal matrix D.
Furthermore, if E1, E2, . . . , Ek are the
elementary matrices corresponding to these elementary column operations in-
dexed in the order performed, and if Q = E1E2 ¬∑ ¬∑ ¬∑ Ek, then QtAQ = D.
Example 6
Let A be the symmetric matrix in M3√ó3(R) deÔ¨Åned by
A =
‚éõ
‚éù
1
‚àí1
3
‚àí1
2
1
3
1
1
‚éû
‚é†.
We use the procedure just described to Ô¨Ånd an invertible matrix Q and a
diagonal matrix D such that QtAQ = D.
We begin by eliminating all of the nonzero entries in the Ô¨Årst row and
Ô¨Årst column except for the entry in column 1 and row 1. To this end, we
add the Ô¨Årst column of A to the second column to produce a zero in row 1
and column 2. The elementary matrix that corresponds to this elementary
column operation is
E1 =
‚éõ
‚éù
1
1
0
0
1
0
0
0
1
‚éû
‚é†.

432
Chap. 6
Inner Product Spaces
We perform the corresponding elementary operation on the rows of AE1 to
obtain
Et
1AE1 =
‚éõ
‚éù
1
0
3
0
1
4
3
4
1
‚éû
‚é†.
We now use the Ô¨Årst column of Et
1AE1 to eliminate the 3 in row 1 column 3,
and follow this operation with the corresponding row operation. The corre-
sponding elementary matrix E2 and the result of the elementary operations
Et
2Et
1AE1E2 are, respectively,
E2 =
‚éõ
‚éù
1
0
‚àí3
0
1
0
0
0
1
‚éû
‚é†
and
Et
2Et
1AE1E2 =
‚éõ
‚éù
1
0
0
0
1
4
0
4
‚àí8
‚éû
‚é†.
Finally, we subtract 4 times the second column of Et
2Et
1AE1E2 from the
third column and follow this with the corresponding row operation. The cor-
responding elementary matrix E3 and the result of the elementary operations
Et
3Et
2Et
1AE1E2E3 are, respectively,
E3 =
‚éõ
‚éù
1
0
0
0
1
‚àí4
0
0
1
‚éû
‚é†
and
Et
3Et
2Et
1AE1E2E3 =
‚éõ
‚éù
1
0
0
0
1
0
0
0
‚àí24
‚éû
‚é†.
Since we have obtained a diagonal matrix, the process is complete. So we let
Q = E1E2E3 =
‚éõ
‚éù
1
1
‚àí7
0
1
‚àí4
0
0
1
‚éû
‚é†
and
D =
‚éõ
‚éù
1
0
0
0
1
0
0
0
‚àí24
‚éû
‚é†
to obtain the desired diagonalization QtAQ = D.
‚ô¶
The reader should justify the following method for computing Q without
recording each elementary matrix separately. The method is inspired by the
algorithm for computing the inverse of a matrix developed in Section 3.2.
We use a sequence of elementary column operations and corresponding row
operations to change the n √ó 2n matrix (A|I) into the form (D|B), where D
is a diagonal matrix and B = Qt. It then follows that D = QtAQ.
Starting with the matrix A of the preceding example, this method pro-
duces the following sequence of matrices:
(A|I) =
‚éõ
‚éù
1
‚àí1
3
1
0
0
‚àí1
2
1
0
1
0
3
1
1
0
0
1
‚éû
‚é†‚àí‚Üí
‚éõ
‚éù
1
0
3
1
0
0
‚àí1
1
1
0
1
0
3
4
1
0
0
1
‚éû
‚é†

Sec. 6.8
Bilinear and Quadratic Forms
433
‚àí‚Üí
‚éõ
‚éù
1
0
3
1
0
0
0
1
4
1
1
0
3
4
1
0
0
1
‚éû
‚é†‚àí‚Üí
‚éõ
‚éù
1
0
0
1
0
0
0
1
4
1
1
0
3
4
‚àí8
0
0
1
‚éû
‚é†
‚àí‚Üí
‚éõ
‚éù
1
0
0
1
0
0
0
1
4
1
1
0
0
4
‚àí8
‚àí3
0
1
‚éû
‚é†‚àí‚Üí
‚éõ
‚éù
1
0
0
1
0
0
0
1
0
1
1
0
0
4
‚àí24
‚àí3
0
1
‚éû
‚é†
‚àí‚Üí
‚éõ
‚éù
1
0
0
1
0
0
0
1
0
1
1
0
0
0
‚àí24
‚àí7
‚àí4
1
‚éû
‚é†= (D|Qt).
Therefore
D =
‚éõ
‚éù
1
0
0
0
1
0
0
0
‚àí24
‚éû
‚é†,
Qt =
‚éõ
‚éù
1
0
0
1
1
0
‚àí7
‚àí4
1
‚éû
‚é†,
and
Q =
‚éõ
‚éù
1
1
‚àí7
0
1
‚àí4
0
0
1
‚éû
‚é†.
Quadratic Forms
Associated with symmetric bilinear forms are functions called quadratic
forms.
DeÔ¨Ånition.
Let V be a vector space over F. A function K : V ‚ÜíF is
called a quadratic form if there exists a symmetric bilinear form H ‚ààB(V)
such that
K(x) = H(x, x)
for all x ‚ààV.
(8)
If the Ô¨Åeld F is not of characteristic two, there is a one-to-one correspon-
dence between symmetric bilinear forms and quadratic forms given by (8).
In fact, if K is a quadratic form on a vector space V over a Ô¨Åeld F not of
characteristic two, and K(x) = H(x, x) for some symmetric bilinear form H
on V, then we can recover H from K because
H(x, y) = 1
2[K(x + y) ‚àíK(x) ‚àíK(y)]
(9)
(See Exercise 16.)
Example 7
The classic example of a quadratic form is the homogeneous second-degree
polynomial of several variables. Given the variables t1, t2, . . . , tn that take
values in a Ô¨Åeld F not of characteristic two and given (not necessarily distinct)
scalars aij (1 ‚â§i ‚â§j ‚â§n), deÔ¨Åne the polynomial
f(t1, t2, . . . , tn) =

i‚â§j
aijtitj.

434
Chap. 6
Inner Product Spaces
Any such polynomial is a quadratic form. In fact, if Œ≤ is the standard or-
dered basis for Fn, then the symmetric bilinear form H corresponding to the
quadratic form f has the matrix representation œàŒ≤(H) = A, where
Aij = Aji =

aii
if i = j
1
2aij
if i Ã∏= j.
To see this, apply (9) to obtain H(ei, ej) = Aij from the quadratic form K,
and verify that f is computable from H by (8) using f in place of K.
For example, given the polynomial
f(t1, t2, t3) = 2t2
1 ‚àít2
2 + 6t1t2 ‚àí4t2t3
with real coeÔ¨Écients, let
A =
‚éõ
‚éù
2
3
0
3
‚àí1
‚àí2
0
‚àí2
0
‚éû
‚é†.
Setting H(x, y) = xtAy for all x, y ‚ààR3, we see that
f(t1, t2, t3) = (t1, t2, t3)A
‚éõ
‚éù
t1
t2
t3
‚éû
‚é†
for
‚éõ
‚éù
t1
t2
t3
‚éû
‚é†‚ààR3.
‚ô¶
Quadratic Forms Over the Field R
Since symmetric matrices over R are orthogonally diagonalizable (see The-
orem 6.20 p. 384), the theory of symmetric bilinear forms and quadratic forms
on Ô¨Ånite-dimensional vector spaces over R is especially nice. The following
theorem and its corollary are useful.
Theorem 6.36. Let V be a Ô¨Ånite-dimensional real inner product space,
and let H be a symmetric bilinear form on V. Then there exists an orthonor-
mal basis Œ≤ for V such that œàŒ≤(H) is a diagonal matrix.
Proof. Choose any orthonormal basis Œ≥ = {v1, v2, . . . , vn} for V, and let
A = œàŒ≥(H).
Since A is symmetric, there exists an orthogonal matrix Q
and a diagonal matrix D such that D = QtAQ by Theorem 6.20. Let Œ≤ =
{w1, w2, . . . , wn} be deÔ¨Åned by
wj =
n

i=1
Qijvi
for 1 ‚â§j ‚â§n.
By Theorem 6.33, œàŒ≤(H) = D. Furthermore, since Q is orthogonal and Œ≥ is
orthonormal, Œ≤ is orthonormal by Exercise 30 of Section 6.5.

Sec. 6.8
Bilinear and Quadratic Forms
435
Corollary. Let K be a quadratic form on a Ô¨Ånite-dimensional real inner
product space V. There exists an orthonormal basis Œ≤ = {v1, v2, . . . , vn} for
V and scalars Œª1, Œª2, . . . , Œªn (not necessarily distinct) such that if x ‚ààV and
x =
n

i=1
sivi,
si ‚ààR,
then
K(x) =
n

i=1
Œªis2
i .
In fact, if H is the symmetric bilinear form determined by K, then Œ≤ can
be chosen to be any orthonormal basis for V such that œàŒ≤(H) is a diagonal
matrix.
Proof. Let H be the symmetric bilinear form for which K(x) = H(x, x)
for all x ‚ààV.
By Theorem 6.36, there exists an orthonormal basis Œ≤ =
{v1, v2, . . . , vn} for V such that œàŒ≤(H) is the diagonal matrix
D =
‚éõ
‚éú
‚éú
‚éú
‚éù
Œª1
0
¬∑ ¬∑ ¬∑
0
0
Œª2
¬∑ ¬∑ ¬∑
0
...
...
...
0
0
¬∑ ¬∑ ¬∑
Œªn
‚éû
‚éü
‚éü
‚éü
‚é†.
Let x ‚ààV, and suppose that x = ,n
i=1 sivi. Then
K(x)=H(x, x) = [œÜŒ≤(x)]tD[œÜŒ≤(x)]=(s1, s2, . . . , sn)D
‚éõ
‚éú
‚éú
‚éú
‚éù
s1
s2
...
sn
‚éû
‚éü
‚éü
‚éü
‚é†=
n

i=1
Œªis2
i .
Example 8
For the homogeneous real polynomial of degree 2 deÔ¨Åned by
f(t1, t2) = 5t2
1 + 2t2
2 + 4t1t2,
(10)
we Ô¨Ånd an orthonormal basis Œ≥ = {x1, x2} for R2 and scalars Œª1 and Œª2 such
that if

t1
t2
	
‚ààR2
and

t1
t2
	
= s1x1 + s2x2,
then f(t1, t2) = Œª1s2
1 + Œª2s2
2. We can think of s1 and s2 as the coordinates of
(t1, t2) relative to Œ≥. Thus the polynomial f(t1, t2), as an expression involving

436
Chap. 6
Inner Product Spaces
the coordinates of a point with respect to the standard ordered basis for R2,
is transformed into a new polynomial g(s1, s2) = Œª1s2
1 + Œª2s2
2 interpreted as
an expression involving the coordinates of a point relative to the new ordered
basis Œ≥.
Let H denote the symmetric bilinear form corresponding to the quadratic
form deÔ¨Åned by (10), let Œ≤ be the standard ordered basis for R2, and let
A = œàŒ≤(H). Then
A = œàŒ≤(H) =

5
2
2
2
	
.
Next, we Ô¨Ånd an orthogonal matrix Q such that QtAQ is a diagonal matrix.
For this purpose, observe that Œª1 = 6 and Œª2 = 1 are the eigenvalues of A
with corresponding orthonormal eigenvectors
v1 =
1
‚àö
5

2
1
	
and
v2 =
1
‚àö
5

1
‚àí2
	
.
Let Œ≥ = {v1, v2}. Then Œ≥ is an orthonormal basis for R2 consisting of eigen-
vectors of A. Hence, setting
Q =
1
‚àö
5

2
1
1
‚àí2
	
,
we see that Q is an orthogonal matrix and
QtAQ =

6
0
0
1
	
.
Clearly Q is also a change of coordinate matrix. Consequently,
œàŒ≥(H) = QtœàŒ≤(H)Q = QtAQ =

6
0
0
1
	
.
Thus by the corollary to Theorem 6.36,
K(x) = 6s2
1 + s2
2
for any x = s1v1 + s2v2 ‚ààR2. So g(s1, s2) = 6s2
1 + s2
2.
‚ô¶
The next example illustrates how the theory of quadratic forms can be
applied to the problem of describing quadratic surfaces in R3.
Example 9
Let S be the surface in R3 deÔ¨Åned by the equation
2t2
1 + 6t1t2 + 5t2
2 ‚àí2t2t3 + 2t2
3 + 3t1 ‚àí2t2 ‚àít3 + 14 = 0.
(11)

Sec. 6.8
Bilinear and Quadratic Forms
437
Then (11) describes the points of S in terms of their coordinates relative to Œ≤,
the standard ordered basis for R3. We Ô¨Ånd a new orthonormal basis Œ≥ for R3
so that the equation describing the coordinates of S relative to Œ≥ is simpler
than (11).
We begin with the observation that the terms of second degree on the left
side of (11) add to form a quadratic form K on R3:
K
‚éõ
‚éù
t1
t2
t3
‚éû
‚é†= 2t2
1 + 6t1t2 + 5t2
2 ‚àí2t2t3 + 2t2
3.
Next, we diagonalize K. Let H be the symmetric bilinear form corre-
sponding to K, and let A = œàŒ≤(H). Then
A =
‚éõ
‚éù
2
3
0
3
5
‚àí1
0
‚àí1
2
‚éû
‚é†.
The characteristic polynomial of A is (‚àí1)(t ‚àí2)(t ‚àí7)t; hence A has the
eigenvalues Œª1 = 2, Œª2 = 7, and Œª3 = 0. Corresponding unit eigenvectors are
v1 =
1
‚àö
10
‚éõ
‚éù
1
0
3
‚éû
‚é†,
v2 =
1
‚àö
35
‚éõ
‚éù
3
5
‚àí1
‚éû
‚é†,
and
v3 =
1
‚àö
14
‚éõ
‚éù
‚àí3
2
1
‚éû
‚é†.
Set Œ≥ = {v1, v2, v3} and
Q =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
1
‚àö
10
3
‚àö
35
‚àí3
‚àö
14
0
5
‚àö
35
2
‚àö
14
3
‚àö
10
‚àí1
‚àö
35
1
‚àö
14
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
.
As in Example 8, Q is a change of coordinate matrix changing Œ≥-coordinates
to Œ≤-coordinates, and
œàŒ≥(H) = QtœàŒ≤(H)Q = QtAQ =
‚éõ
‚éù
2
0
0
0
7
0
0
0
0
‚éû
‚é†.
By the corollary to Theorem 6.36, if x = s1v1 + s2v2 + s3v3, then
K(x) = 2s2
1 + 7s2
2.
(12)

438
Chap. 6
Inner Product Spaces
r
x‚Ä≤
y‚Ä≤
z‚Ä≤
v3
v1
v2
S









XXXXXXXXX
6



9
XX
X
z
Figure 6.7
We are now ready to transform (11) into an equation involving coordinates
relative to Œ≥. Let x = (t1, t2, t3) ‚ààR3, and suppose that x = s1v1+s2v2+s3v3.
Then, by Theorem 2.22 (p. 111),
x =
‚éõ
‚éù
t1
t2
t3
‚éû
‚é†= Q
‚éõ
‚éù
s1
s2
s3
‚éû
‚é†,
and therefore
t1 =
s1
‚àö
10 + 3s2
‚àö
35 ‚àí3s3
‚àö
14,
t2 =
5s2
‚àö
35 + 2s3
‚àö
14,
and
t3 = 3s1
‚àö
10 ‚àí
s2
‚àö
35 + s3
‚àö
14.

Sec. 6.8
Bilinear and Quadratic Forms
439
Thus
3t1 ‚àí2t2 ‚àít3 = ‚àí14s3
‚àö
14 = ‚àí
‚àö
14s3.
Combining (11), (12), and the preceding equation, we conclude that if x ‚ààR3
and x = s1v1 + s2v2 + s3v3, then x ‚ààS if and only if
2s2
1 + 7s2
2 ‚àí
‚àö
14s3 + 14 = 0
or
s3 =
‚àö
14
7 s2
1 +
‚àö
14
2 s2
2 +
‚àö
14.
Consequently, if we draw new axes x‚Ä≤, y‚Ä≤, and z‚Ä≤ in the directions of v1, v2,
and v3, respectively, the graph of the equation, rewritten as
z‚Ä≤ =
‚àö
14
7 (x‚Ä≤)2 +
‚àö
14
2 (y‚Ä≤)2 +
‚àö
14,
coincides with the surface S. We recognize S to be an elliptic paraboloid.
Figure 6.7 is a sketch of the surface S drawn so that the vectors v1, v2 and
v3 are oriented to lie in the principal directions. For practical purposes, the
scale of the z‚Ä≤ axis has been adjusted so that the Ô¨Ågure Ô¨Åts the page.
‚ô¶
The Second Derivative Test for Functions of Several Variables
We now consider an application of the theory of quadratic forms to mul-
tivariable calculus‚Äîthe derivation of the second derivative test for local ex-
trema of a function of several variables. We assume an acquaintance with the
calculus of functions of several variables to the extent of Taylor‚Äôs theorem.
The reader is undoubtedly familiar with the one-variable version of Taylor‚Äôs
theorem. For a statement and proof of the multivariable version, consult, for
example, An Introduction to Analysis 2d ed, by William R. Wade (Prentice
Hall, Upper Saddle River, N.J., 2000).
Let z = f(t1, t2, . . . , tn) be a Ô¨Åxed real-valued function of n real variables
for which all third-order partial derivatives exist and are continuous. The
function f is said to have a local maximum at a point p ‚ààRn if there exists
a Œ¥ > 0 such that f(p) ‚â•f(x) whenever ||x ‚àíp|| < Œ¥. Likewise, f has a local
minimum at p ‚ààRn if there exists a Œ¥ > 0 such that f(p) ‚â§f(x) whenever
||x‚àíp|| < Œ¥. If f has either a local minimum or a local maximum at p, we say
that f has a local extremum at p. A point p ‚ààRn is called a critical point
of f if ‚àÇf(p)/‚àÇti = 0 for i = 1, 2, . . . , n. It is a well-known fact that if f has
a local extremum at a point p ‚ààRn, then p is a critical point of f. For, if f
has a local extremum at p = (p1, p2, . . . , pn), then for any i = 1, 2, . . . , n the

440
Chap. 6
Inner Product Spaces
function œÜi deÔ¨Åned by œÜi(t) = f(p1, p2, . . . , pi‚àí1, t, pi+1, . . . , pn) has a local
extremum at t = pi. So, by an elementary single-variable argument,
‚àÇf(p)
‚àÇti
= dœÜi(pi)
dt
= 0.
Thus p is a critical point of f. But critical points are not necessarily local
extrema.
The second-order partial derivatives of f at a critical point p can often
be used to test for a local extremum at p. These partials determine a matrix
A(p) in which the row i, column j entry is
‚àÇ2f(p)
(‚àÇti)(‚àÇtj).
This matrix is called the Hessian matrix of f at p. Note that if the third-
order partial derivatives of f are continuous, then the mixed second-order
partials of f at p are independent of the order in which they are taken, and
hence A(p) is a symmetric matrix. In this case, all of the eigenvalues of A(p)
are real.
Theorem 6.37 (The Second Derivative Test).
Let f(t1, t2, . . . , tn)
be a real-valued function in n real variables for which all third-order partial
derivatives exist and are continuous. Let p = (p1, p2, . . . , pn) be a critical
point of f, and let A(p) be the Hessian of f at p.
(a) If all eigenvalues of A(p) are positive, then f has a local minimum at p.
(b) If all eigenvalues of A(p) are negative, then f has a local maximum at p.
(c) If A(p) has at least one positive and at least one negative eigenvalue,
then f has no local extremum at p (p is called a saddle-point of f).
(d) If rank(A(p)) < n and A(p) does not have both positive and negative
eigenvalues, then the second derivative test is inconclusive.
Proof. If p Ã∏= 0, we may deÔ¨Åne a function g: Rn ‚ÜíR by
g(t1, t2, . . . , tn) = f(t1 + p1, t2 + p2, . . . , pn + tn) ‚àíf(p).
The following facts are easily veriÔ¨Åed.
1. The function f has a local maximum [minimum] at p if and only if g
has a local maximum [minimum] at 0 = (0, 0, . . . , 0).
2. The partial derivatives of g at 0 are equal to the corresponding partial
derivatives of f at p.
3. 0 is a critical point of g.
4. Aij(p) =
‚àÇ2g(0)
(‚àÇti)(‚àÇtj)
for all i and j.

Sec. 6.8
Bilinear and Quadratic Forms
441
In view of these facts, we may assume without loss of generality that p = 0
and f(p) = 0.
Now we apply Taylor‚Äôs theorem to f to obtain the Ô¨Årst-order approxima-
tion of f around 0. We have
f(t1, t2, . . . , tn) = f(0)+
n

i=1
‚àÇf(0)
‚àÇti
ti+ 1
2
n

i,j=1
‚àÇ2f(0)
(‚àÇti)(‚àÇtj)titj+S(t1, t2, . . . , tn)
= 1
2
n

i,j=1
‚àÇ2f(0)
(‚àÇti)(‚àÇtj)titj + S(t1, t2, . . . , tn),
(13)
where S is a real-valued function on Rn such that
lim
x‚Üí0
S(x)
||x||2 =
lim
(t1,t2,...,tn)‚Üí0
S(t1, t2, . . . , tn)
t2
1 + t2
2 + ¬∑ ¬∑ ¬∑ + t2n
= 0.
(14)
Let K : Rn ‚ÜíR be the quadratic form deÔ¨Åned by
K
‚éõ
‚éú
‚éú
‚éú
‚éù
t1
t2
...
tn
‚éû
‚éü
‚éü
‚éü
‚é†= 1
2
n

i,j=1
‚àÇ2f(0)
(‚àÇti)(‚àÇtj)titj,
(15)
H be the symmetric bilinear form corresponding to K, and Œ≤ be the standard
ordered basis for Rn. It is easy to verify that œàŒ≤(H) = 1
2A(p). Since A(p)
is symmetric, Theorem 6.20 (p. 384) implies that there exists an orthogonal
matrix Q such that
QtA(p)Q =
‚éõ
‚éú
‚éú
‚éú
‚éù
Œª1
0
. . .
0
0
Œª2
. . .
0
...
...
...
0
0
. . .
Œªn
‚éû
‚éü
‚éü
‚éü
‚é†
is a diagonal matrix whose diagonal entries are the eigenvalues of A(p). Let
Œ≥ = {v1, v2, . . . , vn} be the orthogonal basis for Rn whose ith vector is the
ith column of Q. Then Q is the change of coordinate matrix changing Œ≥-
coordinates into Œ≤-coordinates, and by Theorem 6.33
œàŒ≥(H) = QtœàŒ≤(H)Q = 1
2QtA(p)Q =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
Œª1
2
0
. . .
0
0
Œª2
2
. . .
0
...
...
...
0
0
. . .
Œªn
2
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
.

442
Chap. 6
Inner Product Spaces
Suppose that A(p) is not the zero matrix. Then A(p) has nonzero eigen-
values. Choose œµ > 0 such that œµ < |Œªi|/2 for all Œªi Ã∏= 0. By (14), there
exists Œ¥ > 0 such that for any x ‚ààRn satisfying 0 < ||x|| < Œ¥, we have
|S(x)| < œµ||x||2. Consider any x ‚ààRn such that 0 < ||x|| < Œ¥. Then, by (13)
and (15),
|f(x) ‚àíK(x)| = |S(x)| < œµ||x||2,
and hence
K(x) ‚àíœµ||x||2 < f(x) < K(x) + œµ||x||2.
(16)
Suppose that x =
n

i=1
sivi. Then
||x||2 =
n

i=1
s2
i
and
K(x) = 1
2
n

i=1
Œªis2
i .
Combining these equations with (16), we obtain
n

i=1
1
2Œªi ‚àíœµ
	
s2
i < f(x) <
n

i=1
1
2Œªi + œµ
	
s2
i .
(17)
Now suppose that all eigenvalues of A(p) are positive. Then 1
2Œªi ‚àíœµ > 0
for all i, and hence, by the left inequality in (17),
f(0) = 0 ‚â§
n

i=1
1
2Œªi ‚àíœµ
	
s2
i < f(x).
Thus f(0) ‚â§f(x) for ||x|| < Œ¥, and so f has a local minimum at 0. By a
similar argument using the right inequality in (17), we have that if all of the
eigenvalues of A(p) are negative, then f has a local maximum at 0. This
establishes (a) and (b) of the theorem.
Next, suppose that A(p) has both a positive and a negative eigenvalue,
say, Œªi > 0 and Œªj < 0 for some i and j. Then 1
2Œªi ‚àíœµ > 0 and 1
2Œªj + œµ < 0.
Let s be any real number such that 0 < |s| < Œ¥. Substituting x = svi and
x = svj into the left inequality and the right inequality of (17), respectively,
we obtain
f(0) = 0 < ( 1
2Œªi ‚àíœµ)s2 < f(svi)
and
f(svj) < ( 1
2Œªj + œµ)s2 < 0 = f(0).
Thus f attains both positive and negative values arbitrarily close to 0; so f
has neither a local maximum nor a local minimum at 0. This establishes (c).

Sec. 6.8
Bilinear and Quadratic Forms
443
To show that the second-derivative test is inconclusive under the condi-
tions stated in (d), consider the functions
f(t1, t2) = t2
1 ‚àít4
2
and
g(t1, t2) = t2
1 + t4
2
at p = 0. In both cases, the function has a critical point at p, and
A(p) =

2
0
0
0
	
.
However, f does not have a local extremum at 0, whereas g has a local
minimum at 0.
Sylvester‚Äôs Law of Inertia
Any two matrix representations of a bilinear form have the same rank
because rank is preserved under congruence.
We can therefore deÔ¨Åne the
rank of a bilinear form to be the rank of any of its matrix representations.
If a matrix representation is a diagonal matrix, then the rank is equal to the
number of nonzero diagonal entries of the matrix.
We conÔ¨Åne our analysis to symmetric bilinear forms on Ô¨Ånite-dimensional
real vector spaces. Each such form has a diagonal matrix representation in
which the diagonal entries may be positive, negative, or zero. Although these
entries are not unique, we show that the number of entries that are positive
and the number that are negative are unique. That is, they are independent
of the choice of diagonal representation. This result is called Sylvester‚Äôs law
of inertia. We prove the law and apply it to describe the equivalence classes
of congruent symmetric real matrices.
Theorem 6.38 (Sylvester‚Äôs Law of Inertia).
Let H be a symmetric
bilinear form on a Ô¨Ånite-dimensional real vector space V. Then the number of
positive diagonal entries and the number of negative diagonal entries in any
diagonal matrix representation of H are each independent of the diagonal
representation.
Proof. Suppose that Œ≤ and Œ≥ are ordered bases for V that determine di-
agonal representations of H. Without loss of generality, we may assume that
Œ≤ and Œ≥ are ordered so that on each diagonal the entries are in the order
of positive, negative, and zero. It suÔ¨Éces to show that both representations
have the same number of positive entries because the number of negative en-
tries is equal to the diÔ¨Äerence between the rank and the number of positive
entries. Let p and q be the number of positive diagonal entries in the matrix
representations of H with respect to Œ≤ and Œ≥, respectively. We suppose that
p Ã∏= q and arrive at a contradiction. Without loss of generality, assume that
p < q. Let
Œ≤ = {v1, v2, . . . , vp, . . . , vr, . . . , vn} and Œ≥ = {w1, w2, . . . , wq, . . . , wr, . . . , wn},

444
Chap. 6
Inner Product Spaces
where r is the rank of H and n = dim(V). Let L: V ‚ÜíRp+r‚àíq be the mapping
deÔ¨Åned by
L(x) = (H(x, v1), H(x, v2), . . . , H(x, vp), H(x, wq+1), . . . , H(x, wr)).
It is easily veriÔ¨Åed that L is linear and rank(L) ‚â§p + r ‚àíq. Hence
nullity(L) ‚â•n ‚àí(p + r ‚àíq) > n ‚àír.
So there exists a nonzero vector v0 such that v0 /‚ààspan({vr+1, vr+2, . . . , vn}),
but v0 ‚ààN(L). Since v0 ‚ààN(L), it follows that H(v0, vi) = 0 for i ‚â§p and
H(v0, wi) = 0 for q < i ‚â§r. Suppose that
v0 =
n

j=1
ajvj =
n

j=1
bjwj.
For any i ‚â§p,
H(v0, vi) = H
‚éõ
‚éù
n

j=1
ajvj, vi
‚éû
‚é†=
n

j=1
ajH(vj, vi) = aiH(vi, vi).
But for i ‚â§p, we have H(vi, vi) > 0 and H(v0, vi) = 0, so that ai =
0.
Similarly, bi = 0 for q + 1 ‚â§i ‚â§r.
Since v0 is not in the span of
{vr+1, vr+2, . . . , vn}, it follows that ai Ã∏= 0 for some p < i ‚â§r. Thus
H(v0, v0)=H
‚éõ
‚éù
n

j=1
ajvj,
n

i=1
aivi
‚éû
‚é†=
n

j=1
a2
jH(vj, vj)=
r

j=p+1
a2
jH(vj, vj)<0.
Furthermore,
H(v0, v0)=H
‚éõ
‚éù
n

j=1
bjwj,
n

i=1
biwi
‚éû
‚é†=
n

j=1
b2
jH(wj, wj)=
r

j=p+1
b2
jH(wj, wj)‚â•0.
So H(v0, v0) < 0 and H(v0, v0) ‚â•0, which is a contradiction. We conclude
that p = q.
DeÔ¨Ånitions.
The number of positive diagonal entries in a diagonal
representation of a symmetric bilinear form on a real vector space is called
the index of the form. The diÔ¨Äerence between the number of positive and
the number of negative diagonal entries in a diagonal representation of a
symmetric bilinear form is called the signature of the form. The three terms
rank, index, and signature are called the invariants of the bilinear form
because they are invariant with respect to matrix representations.
These
same terms apply to the associated quadratic form. Notice that the values of
any two of these invariants determine the value of the third.

Sec. 6.8
Bilinear and Quadratic Forms
445
Example 10
The bilinear form corresponding to the quadratic form K of Example 9 has
a 3 √ó 3 diagonal matrix representation with diagonal entries of 2, 7, and 0.
Therefore the rank, index, and signature of K are each 2.
‚ô¶
Example 11
The matrix representation of the bilinear form corresponding to the quadratic
form K(x, y) = x2 ‚àíy2 on R2 with respect to the standard ordered basis is
the diagonal matrix with diagonal entries of 1 and ‚àí1. Therefore the rank of
K is 2, the index of K is 1, and the signature of K is 0.
‚ô¶
Since the congruence relation is intimately associated with bilinear forms,
we can apply Sylvester‚Äôs law of inertia to study this relation on the set of real
symmetric matrices. Let A be an n √ó n real symmetric matrix, and suppose
that D and E are each diagonal matrices congruent to A. By Corollary 3
to Theorem 6.32, A is the matrix representation of the bilinear form H on
Rn deÔ¨Åned by H(x, y) = xtAy with respect to the standard ordered basis for
Rn. Therefore Sylvester‚Äôs law of inertia tells us that D and E have the same
number of positive and negative diagonal entries. We can state this result as
the matrix version of Sylvester‚Äôs law.
Corollary 1 (Sylvester‚Äôs Law of Inertia for Matrices).
Let A be
a real symmetric matrix. Then the number of positive diagonal entries and
the number of negative diagonal entries in any diagonal matrix congruent to
A is independent of the choice of the diagonal matrix.
DeÔ¨Ånitions. Let A be a real symmetric matrix, and let D be a diagonal
matrix that is congruent to A. The number of positive diagonal entries of
D is called the index of A. The diÔ¨Äerence between the number of positive
diagonal entries and the number of negative diagonal entries of D is called
the signature of A. As before, the rank, index, and signature of a matrix
are called the invariants of the matrix, and the values of any two of these
invariants determine the value of the third.
Any two of these invariants can be used to determine an equivalence class
of congruent real symmetric matrices.
Corollary 2.
Two real symmetric n √ó n matrices are congruent if and
only if they have the same invariants.
Proof. If A and B are congruent n √ó n symmetric matrices, then they are
both congruent to the same diagonal matrix, and it follows that they have
the same invariants.
Conversely, suppose that A and B are n √ó n symmetric matrices with the
same invariants. Let D and E be diagonal matrices congruent to A and B,

446
Chap. 6
Inner Product Spaces
respectively, chosen so that the diagonal entries are in the order of positive,
negative, and zero. (Exercise 23 allows us to do this.) Since A and B have
the same invariants, so do D and E. Let p and r denote the index and the
rank, respectively, of both D and E. Let di denote the ith diagonal entry
of D, and let Q be the n √ó n diagonal matrix whose ith diagonal entry qi is
given by
qi =
‚éß
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é©
1
‚àödi
if 1 ‚â§i ‚â§p
1
‚àö‚àídi
if p < i ‚â§r
1
if r < i.
Then QtDQ = Jpr, where
Jpr =
‚éõ
‚éù
Ip
O
O
O
‚àíIr‚àíp
O
O
O
O
‚éû
‚é†.
It follows that A is congruent to Jpr. Similarly, B is congruent to Jpr, and
hence A is congruent to B.
The matrix Jpr acts as a canonical form for the theory of real symmet-
ric matrices. The next corollary, whose proof is contained in the proof of
Corollary 2, describes the role of Jpr.
Corollary 3. A real symmetric n √ó n matrix A has index p and rank r
if and only if A is congruent to Jpr (as just deÔ¨Åned).
Example 12
Let
A =
‚éõ
‚éù
1
1
‚àí3
‚àí1
2
1
3
1
1
‚éû
‚é†,
B =
‚éõ
‚éù
1
2
1
2
3
2
1
2
1
‚éû
‚é†,
and
C =
‚éõ
‚éù
1
0
1
0
1
2
1
2
1
‚éû
‚é†.
We apply Corollary 2 to determine which pairs of the matrices A, B, and C
are congruent.
The matrix A is the 3 √ó 3 matrix of Example 6, where it is shown that
A is congruent to a diagonal matrix with diagonal entries 1, 1, and ‚àí24.
Therefore, A has rank 3 and index 2. Using the methods of Example 6 (it is
not necessary to compute Q), it can be shown that B and C are congruent,
respectively, to the diagonal matrices
‚éõ
‚éù
1
0
0
0
‚àí1
0
0
0
‚àí1
‚éû
‚é†
and
‚éõ
‚éù
1
0
0
0
1
0
0
0
‚àí4
‚éû
‚é†.

Sec. 6.8
Bilinear and Quadratic Forms
447
It follows that both A and C have rank 3 and index 2, while B has rank 3 and
index 1. We conclude that A and C are congruent but that B is congruent
to neither A nor C.
‚ô¶
EXERCISES
1.
Label the following statements as true or false.
(a)
Every quadratic form is a bilinear form.
(b)
If two matrices are congruent, they have the same eigenvalues.
(c)
Symmetric bilinear forms have symmetric matrix representations.
(d)
Any symmetric matrix is congruent to a diagonal matrix.
(e)
The sum of two symmetric bilinear forms is a symmetric bilinear
form.
(f)
Two symmetric matrices with the same characteristic polynomial
are matrix representations of the same bilinear form.
(g)
There exists a bilinear form H such that H(x, y) Ã∏= 0 for all x and
y.
(h)
If V is a vector space of dimension n, then dim(B(V )) = 2n.
(i)
Let H be a bilinear form on a Ô¨Ånite-dimensional vector space V
with dim(V) > 1. For any x ‚ààV, there exists y ‚ààV such that
y Ã∏= 0, but H(x, y) = 0.
(j)
If H is any bilinear form on a Ô¨Ånite-dimensional real inner product
space V, then there exists an ordered basis Œ≤ for V such that œàŒ≤(H)
is a diagonal matrix.
2.
Prove properties 1, 2, 3, and 4 on page 423.
3. (a)
Prove that the sum of two bilinear forms is a bilinear form.
(b)
Prove that the product of a scalar and a bilinear form is a bilinear
form.
(c)
Prove Theorem 6.31.
4.
Determine which of the mappings that follow are bilinear forms. Justify
your answers.
(a)
Let V = C[0, 1] be the space of continuous real-valued functions on
the closed interval [0, 1]. For f, g ‚ààV, deÔ¨Åne
H(f, g) =
 1
0
f(t)g(t)dt.
(b)
Let V be a vector space over F, and let J ‚ààB(V) be nonzero.
DeÔ¨Åne H : V √ó V ‚ÜíF by
H(x, y) = [J(x, y)]2
for all x, y ‚ààV.

448
Chap. 6
Inner Product Spaces
(c)
DeÔ¨Åne H : R √ó R ‚ÜíR by H(t1, t2) = t1 + 2t2.
(d)
Consider the vectors of R2 as column vectors, and let H : R2 ‚ÜíR
be the function deÔ¨Åned by H(x, y) = det(x, y), the determinant of
the 2 √ó 2 matrix with columns x and y.
(e)
Let V be a real inner product space, and let H : V √ó V ‚ÜíR be the
function deÔ¨Åned by H(x, y) = ‚ü®x, y‚ü©for x, y ‚ààV.
(f)
Let V be a complex inner product space, and let H : V √ó V ‚ÜíC
be the function deÔ¨Åned by H(x, y) = ‚ü®x, y‚ü©for x, y ‚ààV.
5.
Verify that each of the given mappings is a bilinear form. Then compute
its matrix representation with respect to the given ordered basis Œ≤.
(a)
H : R3 √ó R3 ‚ÜíR, where
H
‚éõ
‚éù
‚éõ
‚éù
a1
a2
a3
‚éû
‚é†,
‚éõ
‚éù
b1
b2
b3
‚éû
‚é†
‚éû
‚é†= a1b1 ‚àí2a1b2 + a2b1 ‚àía3b3
and
Œ≤ =
‚éß
‚é®
‚é©
‚éõ
‚éù
1
0
1
‚éû
‚é†,
‚éõ
‚éù
1
0
‚àí1
‚éû
‚é†,
‚éõ
‚éù
0
1
0
‚éû
‚é†
‚é´
‚é¨
‚é≠.
(b)
Let V = M2√ó2(R) and
Œ≤ =

1
0
0
0
	
,

0
1
0
0
	
,

0
0
1
0
	
,

0
0
0
1
	
.
DeÔ¨Åne H : V √ó V ‚ÜíR by H(A, B) = tr(A)¬∑ tr(B).
(c)
Let Œ≤ = {cos t, sin t, cos 2t, sin 2t}.
Then Œ≤ is an ordered basis
for V = span(Œ≤), a four-dimensional subspace of the space of all
continuous functions on R. Let H : V √ó V ‚ÜíR be the function
deÔ¨Åned by H(f, g) = f ‚Ä≤(0) ¬∑ g‚Ä≤‚Ä≤(0).
6.
Let H : R2 ‚ÜíR be the function deÔ¨Åned by
H

a1
a2
	
,

b1
b2
		
= a1b2 + a2b1
for

a1
a2
	
,

b1
b2
	
‚ààR2.
(a)
Prove that H is a bilinear form.
(b)
Find the 2√ó2 matrix A such that H(x, y) = xtAy for all x, y ‚ààR2.
For a 2√ó2 matrix M with columns x and y, the bilinear form H(M) =
H(x, y) is called the permanent of M.
7.
Let V and W be vector spaces over the same Ô¨Åeld, and let T: V ‚ÜíW be
a linear transformation. For any H ‚ààB(W), deÔ¨Åne T(H): V √ó V ‚ÜíF
by T(H)(x, y) = H(T(x), T(y)) for all x, y ‚ààV. Prove the following
results.

Sec. 6.8
Bilinear and Quadratic Forms
449
(a)
If H ‚ààB(W), then T(H) ‚ààB(V).
(b)
T: B(W) ‚ÜíB(V) is a linear transformation.
(c)
If T is an isomorphism, then so is T.
8.
Assume the notation of Theorem 6.32.
(a)
Prove that for any ordered basis Œ≤, œàŒ≤ is linear.
(b)
Let Œ≤ be an ordered basis for an n-dimensional space V over F, and
let œÜŒ≤ : V ‚ÜíFn be the standard representation of V with respect
to Œ≤. For A ‚ààMn√ón(F), deÔ¨Åne H : V √ó V ‚ÜíF by H(x, y) =
[œÜŒ≤(x)]tA[œÜŒ≤(y)]. Prove that H ‚ààB(V). Can you establish this as
a corollary to Exercise 7?
(c)
Prove the converse of (b): Let H be a bilinear form on V.
If
A = œàŒ≤(H), then H(x, y) = [œÜŒ≤(x)]tA[œÜŒ≤(y)].
9. (a)
Prove Corollary 1 to Theorem 6.32.
(b)
For a Ô¨Ånite-dimensional vector space V, describe a method for
Ô¨Ånding an ordered basis for B(V).
10.
Prove Corollary 2 to Theorem 6.32.
11.
Prove Corollary 3 to Theorem 6.32.
12.
Prove that the relation of congruence is an equivalence relation.
13.
The following outline provides an alternative proof to Theorem 6.33.
(a)
Suppose that Œ≤ and Œ≥ are ordered bases for a Ô¨Ånite-dimensional
vector space V, and let Q be the change of coordinate matrix
changing Œ≥-coordinates to Œ≤-coordinates. Prove that œÜŒ≤ = LQœÜŒ≥,
where œÜŒ≤ and œÜŒ≥ are the standard representations of V with respect
to Œ≤ and Œ≥, respectively.
(b)
Apply Corollary 2 to Theorem 6.32 to (a) to obtain an alternative
proof of Theorem 6.33.
14.
Let V be a Ô¨Ånite-dimensional vector space and H ‚ààB(V). Prove that,
for any ordered bases Œ≤ and Œ≥ of V, rank(œàŒ≤(H)) = rank(œàŒ≥(H)).
15.
Prove the following results.
(a)
Any square diagonal matrix is symmetric.
(b)
Any matrix congruent to a diagonal matrix is symmetric.
(c)
the corollary to Theorem 6.35
16.
Let V be a vector space over a Ô¨Åeld F not of characteristic two, and let
H be a symmetric bilinear form on V. Prove that if K(x) = H(x, x) is
the quadratic form associated with H, then, for all x, y ‚ààV,
H(x, y) = 1
2[K(x + y) ‚àíK(x) ‚àíK(y)].

450
Chap. 6
Inner Product Spaces
17.
For each of the given quadratic forms K on a real inner product space
V, Ô¨Ånd a symmetric bilinear form H such that K(x) = H(x, x) for all
x ‚ààV. Then Ô¨Ånd an orthonormal basis Œ≤ for V such that œàŒ≤(H) is a
diagonal matrix.
(a)
K : R2 ‚ÜíR deÔ¨Åned by K

t1
t2
	
= ‚àí2t2
1 + 4t1t2 + t2
2
(b)
K : R2 ‚ÜíR deÔ¨Åned by K

t1
t2
	
= 7t2
1 ‚àí8t1t2 + t2
2
(c)
K : R3 ‚ÜíR deÔ¨Åned by K
‚éõ
‚éù
t1
t2
t3
‚éû
‚é†= 3t2
1 + 3t2
2 + 3t2
3 ‚àí2t1t3
18.
Let S be the set of all (t1, t2, t3) ‚ààR3 for which
3t2
1 + 3t2
2 + 3t2
3 ‚àí2t1t3 + 2
‚àö
2(t1 + t3) + 1 = 0.
Find an orthonormal basis Œ≤ for R3 for which the equation relating
the coordinates of points of S relative to Œ≤ is simpler.
Describe S
geometrically.
19.
Prove the following reÔ¨Ånement of Theorem 6.37(d).
(a)
If 0 < rank(A) < n and A has no negative eigenvalues, then f has
no local maximum at p.
(b)
If 0 < rank(A) < n and A has no positive eigenvalues, then f has
no local minimum at p.
20.
Prove the following variation of the second-derivative test for the case
n = 2: DeÔ¨Åne
D =
$‚àÇ2f(p)
‚àÇt2
1
% $‚àÇ2f(p)
‚àÇt2
2
%
‚àí
$‚àÇ2f(p)
‚àÇt1‚àÇt2
%2
.
(a)
If D > 0 and ‚àÇ2f(p)/‚àÇt2
1 > 0, then f has a local minimum at p.
(b)
If D > 0 and ‚àÇ2f(p)/‚àÇt2
1 < 0, then f has a local maximum at p.
(c)
If D < 0, then f has no local extremum at p.
(d)
If D = 0, then the test is inconclusive.
Hint: Observe that, as in Theorem 6.37, D = det(A) = Œª1Œª2, where Œª1
and Œª2 are the eigenvalues of A.
21.
Let A and E be in Mn√ón(F), with E an elementary matrix. In Sec-
tion 3.1, it was shown that AE can be obtained from A by means of
an elementary column operation. Prove that EtA can be obtained by
means of the same elementary operation performed on the rows rather
than on the columns of A. Hint: Note that EtA = (AtE)t.

Sec. 6.9
Einstein‚Äôs Special Theory of Relativity
451
22.
For each of the following matrices A with entries from R, Ô¨Ånd a diagonal
matrix D and an invertible matrix Q such that QtAQ = D.
(a)

1
3
3
2
	
(b)

0
1
1
0
	
(c)
‚éõ
‚éù
3
1
2
1
4
0
2
0
‚àí1
‚éû
‚é†
Hint for (b): Use an elementary operation other than interchanging
columns.
23.
Prove that if the diagonal entries of a diagonal matrix are permuted,
then the resulting diagonal matrix is congruent to the original one.
24.
Let T be a linear operator on a real inner product space V, and deÔ¨Åne
H : V √ó V ‚ÜíR by H(x, y) = ‚ü®x, T(y)‚ü©for all x, y ‚ààV.
(a)
Prove that H is a bilinear form.
(b)
Prove that H is symmetric if and only if T is self-adjoint.
(c)
What properties must T have for H to be an inner product on V?
(d)
Explain why H may fail to be a bilinear form if V is a complex
inner product space.
25.
Prove the converse to Exercise 24(a): Let V be a Ô¨Ånite-dimensional real
inner product space, and let H be a bilinear form on V. Then there
exists a unique linear operator T on V such that H(x, y) = ‚ü®x, T(y)‚ü©for
all x, y ‚ààV. Hint: Choose an orthonormal basis Œ≤ for V, let A = œàŒ≤(H),
and let T be the linear operator on V such that [T]Œ≤ = A.
Apply
Exercise 8(c) of this section and Exercise 15 of Section 6.2 (p. 355).
26.
Prove that the number of distinct equivalence classes of congruent n√ón
real symmetric matrices is
(n + 1)(n + 2)
2
.
6.9‚àó
EINSTEIN‚ÄôS SPECIAL THEORY OF RELATIVITY
As a consequence of physical experiments performed in the latter half of the
nineteenth century (most notably the Michelson‚ÄìMorley experiment of 1887),
physicists concluded that the results obtained in measuring the speed of light
are independent of the velocity of the instrument used to measure the speed of
light. For example, suppose that while on Earth, an experimenter measures
the speed of light emitted from the sun and Ô¨Ånds it to be 186,000 miles per
second. Now suppose that the experimenter places the measuring equipment
in a spaceship that leaves Earth traveling at 100,000 miles per second in a
direction away from the sun. A repetition of the same experiment from the
spaceship yields the same result: Light is traveling at 186,000 miles per second

452
Chap. 6
Inner Product Spaces
relative to the spaceship, rather than 86,000 miles per second as one might
expect!
This revelation led to a new way of relating coordinate systems used to
locate events in space‚Äìtime. The result was Albert Einstein‚Äôs special theory
of relativity. In this section, we develop via a linear algebra viewpoint the
essence of Einstein‚Äôs theory.
................................................................................................................................................................................................................................................................................................................................................................................................................................
................................................................................................................................................................................................................................................................................................................................................................................................................................
r
r
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9

>
ZZ
~

1
6
-
-
6

1
z
z‚Ä≤
y
x‚Ä≤
y‚Ä≤
S‚Ä≤
x
S
C
C‚Ä≤
Figure 6.8
The basic problem is to compare two diÔ¨Äerent inertial (nonaccelerating)
coordinate systems S and S‚Ä≤ in three-space (R3) that are in motion relative
to each other under the assumption that the speed of light is the same when
measured in either system. We assume that S‚Ä≤ moves at a constant velocity
in relation to S as measured from S. (See Figure 6.8.) To simplify matters,
let us suppose that the following conditions hold:
1. The corresponding axes of S and S‚Ä≤ (x and x‚Ä≤, y and y‚Ä≤, z and z‚Ä≤) are
parallel, and the origin of S‚Ä≤ moves in the positive direction of the x-axis
of S at a constant velocity v > 0 relative to S.
2. Two clocks C and C‚Ä≤ are placed in space‚Äîthe Ô¨Årst stationary relative
to the coordinate system S and the second stationary relative to the
coordinate system S‚Ä≤. These clocks are designed to give real numbers
in units of seconds as readings. The clocks are calibrated so that at the
instant the origins of S and S‚Ä≤ coincide, both clocks give the reading
zero.
3. The unit of length is the light second (the distance light travels in 1
second), and the unit of time is the second. Note that, with respect to
these units, the speed of light is 1 light second per second.
Given any event (something whose position and time of occurrence can be
described), we may assign a set of space‚Äìtime coordinates to it. For example,

Sec. 6.9
Einstein‚Äôs Special Theory of Relativity
453
if p is an event that occurs at position
‚éõ
‚éù
x
y
z
‚éû
‚é†
relative to S and at time t as read on clock C, we can assign to p the set of
coordinates
‚éõ
‚éú
‚éú
‚éù
x
y
z
t
‚éû
‚éü
‚éü
‚é†.
This ordered 4-tuple is called the space‚Äìtime coordinates of p relative to
S and C. Likewise, p has a set of space‚Äìtime coordinates
‚éõ
‚éú
‚éú
‚éù
x‚Ä≤
y‚Ä≤
z‚Ä≤
t‚Ä≤
‚éû
‚éü
‚éü
‚é†
relative to S‚Ä≤ and C‚Ä≤.
For a Ô¨Åxed velocity v, let Tv : R4 ‚ÜíR4 be the mapping deÔ¨Åned by
Tv
‚éõ
‚éú
‚éú
‚éù
x
y
z
t
‚éû
‚éü
‚éü
‚é†=
‚éõ
‚éú
‚éú
‚éù
x‚Ä≤
y‚Ä≤
z‚Ä≤
t‚Ä≤
‚éû
‚éü
‚éü
‚é†,
where
‚éõ
‚éú
‚éú
‚éù
x
y
z
t
‚éû
‚éü
‚éü
‚é†
and
‚éõ
‚éú
‚éú
‚éù
x‚Ä≤
y‚Ä≤
z‚Ä≤
t‚Ä≤
‚éû
‚éü
‚éü
‚é†
are the space‚Äìtime coordinates of the same event with respect to S and C
and with respect to S‚Ä≤ and C‚Ä≤, respectively.
Einstein made certain assumptions about Tv that led to his special theory
of relativity. We formulate an equivalent set of assumptions.
Axioms of the Special Theory of Relativity
(R 1) The speed of any light beam, when measured in either coordinate system
using a clock stationary relative to that coordinate system, is 1.

454
Chap. 6
Inner Product Spaces
(R 2) The mapping Tv : R4 ‚ÜíR4 is an isomorphism.
(R 3) If
Tv
‚éõ
‚éú
‚éú
‚éù
x
y
z
t
‚éû
‚éü
‚éü
‚é†=
‚éõ
‚éú
‚éú
‚éù
x‚Ä≤
y‚Ä≤
z‚Ä≤
t‚Ä≤
‚éû
‚éü
‚éü
‚é†,
then y‚Ä≤ = y and z‚Ä≤ = z.
(R 4) If
Tv
‚éõ
‚éú
‚éú
‚éù
x
y1
z1
t
‚éû
‚éü
‚éü
‚é†=
‚éõ
‚éú
‚éú
‚éù
x‚Ä≤
y‚Ä≤
z‚Ä≤
t‚Ä≤
‚éû
‚éü
‚éü
‚é†
and
Tv
‚éõ
‚éú
‚éú
‚éù
x
y2
z2
t
‚éû
‚éü
‚éü
‚é†=
‚éõ
‚éú
‚éú
‚éù
x‚Ä≤‚Ä≤
y‚Ä≤‚Ä≤
z‚Ä≤‚Ä≤
t‚Ä≤‚Ä≤
‚éû
‚éü
‚éü
‚é†,
then x‚Ä≤‚Ä≤ = x‚Ä≤ and t‚Ä≤‚Ä≤ = t‚Ä≤.
(R 5) The origin of S moves in the negative direction of the x‚Ä≤-axis of S‚Ä≤ at
the constant velocity ‚àív < 0 as measured from S‚Ä≤.
Axioms (R 3) and (R 4) tell us that for p ‚ààR4, the second and third coor-
dinates of Tv(p) are unchanged and the Ô¨Årst and fourth coordinates of Tv(p)
are independent of the second and third coordinates of p.
As we will see, these Ô¨Åve axioms completely characterize Tv. The operator
Tv is called the Lorentz transformation in direction x.
We intend to
compute Tv and use it to study the curious phenomenon of time contraction.
Theorem 6.39. On R4, the following statements are true.
(a) Tv(ei) = ei for i = 2, 3.
(b) span({e2, e3}) is Tv-invariant.
(c) span({e1, e4}) is Tv-invariant.
(d) Both span({e2, e3}) and span({e1, e4}) are T‚àó
v-invariant.
(e) T‚àó
v(ei) = ei for i = 2, 3.
Proof. (a) By axiom (R 2),
Tv
‚éõ
‚éú
‚éú
‚éù
0
0
0
0
‚éû
‚éü
‚éü
‚é†=
‚éõ
‚éú
‚éú
‚éù
0
0
0
0
‚éû
‚éü
‚éü
‚é†,
and hence, by axiom (R 4), the Ô¨Årst and fourth coordinates of
Tv
‚éõ
‚éú
‚éú
‚éù
0
a
b
0
‚éû
‚éü
‚éü
‚é†

Sec. 6.9
Einstein‚Äôs Special Theory of Relativity
455
are both zero for any a, b ‚ààR. Thus, by axiom (R 3),
Tv
‚éõ
‚éú
‚éú
‚éù
0
1
0
0
‚éû
‚éü
‚éü
‚é†=
‚éõ
‚éú
‚éú
‚éù
0
1
0
0
‚éû
‚éü
‚éü
‚é†
and
Tv
‚éõ
‚éú
‚éú
‚éù
0
0
1
0
‚éû
‚éü
‚éü
‚é†=
‚éõ
‚éú
‚éú
‚éù
0
0
1
0
‚éû
‚éü
‚éü
‚é†.
The proofs of (b), (c), and (d) are left as exercises.
(e) For any j Ã∏= 2, ‚ü®T‚àó
v(e2), ej‚ü©= ‚ü®e2, Tv(ej)‚ü©= 0 by (a) and (c); for j = 2,
‚ü®T‚àó
v(e2), ej‚ü©= ‚ü®e2, Tv(e2)‚ü©= ‚ü®e2, e2‚ü©= 1 by (a). We conclude that T‚àó
v(e2) is
a multiple of e2 (i.e., that T‚àó
v(e2) = ke2 for some k ‚ààR). Thus,
1 = ‚ü®e2, e2‚ü©= ‚ü®e2, Tv(e2)‚ü©= ‚ü®T‚àó
v(e2), e2‚ü©= ‚ü®ke2, e2‚ü©= k,
and hence T‚àó
v(e2) = e2. Similarly, T‚àó
v(e3) = e3.
Suppose that, at the instant the origins of S and S‚Ä≤ coincide, a light
Ô¨Çash is emitted from their common origin. The event of the light Ô¨Çash when
measured either relative to S and C or relative to S‚Ä≤ and C‚Ä≤ has space‚Äìtime
coordinates
‚éõ
‚éú
‚éú
‚éù
0
0
0
0
‚éû
‚éü
‚éü
‚é†.
Let P be the set of all events whose space‚Äìtime coordinates
‚éõ
‚éú
‚éú
‚éù
x
y
z
t
‚éû
‚éü
‚éü
‚é†
relative to S and C are such that the Ô¨Çash is observable from the point with
coordinates
‚éõ
‚éù
x
y
z
‚éû
‚é†
(as measured relative to S) at the time t (as measured on C). Let us charac-
terize P in terms of x, y, z, and t. Since the speed of light is 1, at any time
t ‚â•0 the light Ô¨Çash is observable from any point whose distance to the origin
of S (as measured on S) is t ¬∑ 1 = t. These are precisely the points that lie on
the sphere of radius t with center at the origin. The coordinates (relative to

456
Chap. 6
Inner Product Spaces
S) of such points satisfy the equation x2 + y2 + z2 ‚àít2 = 0. Hence an event
lies in P if and only if its space‚Äìtime coordinates
‚éõ
‚éú
‚éú
‚éù
x
y
z
t
‚éû
‚éü
‚éü
‚é†
(t ‚â•0)
relative to S and C satisfy the equation x2 + y2 + z2 ‚àít2 = 0. By virtue of
axiom (R 1), we can characterize P in terms of the space‚Äìtime coordinates
relative to S‚Ä≤ and C‚Ä≤ similarly: An event lies in P if and only if, relative to
S‚Ä≤ and C‚Ä≤, its space‚Äìtime coordinates
‚éõ
‚éú
‚éú
‚éù
x‚Ä≤
y‚Ä≤
z‚Ä≤
t‚Ä≤
‚éû
‚éü
‚éü
‚é†
(t ‚â•0)
satisfy the equation (x‚Ä≤)2 + (y‚Ä≤)2 + (z‚Ä≤)2 ‚àí(t‚Ä≤)2 = 0.
Let
A =
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
‚àí1
‚éû
‚éü
‚éü
‚é†.
Theorem 6.40. If ‚ü®LA(w), w‚ü©= 0 for some w ‚ààR4, then
‚ü®T‚àó
vLATv(w), w‚ü©= 0.
Proof. Let
w =
‚éõ
‚éú
‚éú
‚éù
x
y
z
t
‚éû
‚éü
‚éü
‚é†‚ààR4,
and suppose that ‚ü®LA(w), w‚ü©= 0.
Case 1.
t ‚â•0. Since ‚ü®LA(w), w‚ü©= x2 + y2 + z2 ‚àít2, the vector w gives
the coordinates of an event in P relative to S and C. Because
‚éõ
‚éú
‚éú
‚éù
x
y
z
t
‚éû
‚éü
‚éü
‚é†
and
‚éõ
‚éú
‚éú
‚éù
x‚Ä≤
y‚Ä≤
z‚Ä≤
t‚Ä≤
‚éû
‚éü
‚éü
‚é†

Sec. 6.9
Einstein‚Äôs Special Theory of Relativity
457
are the space‚Äìtime coordinates of the same event relative to S‚Ä≤ and C‚Ä≤, the
discussion preceding Theorem 6.40 yields
(x‚Ä≤)2 + (y‚Ä≤)2 + (z‚Ä≤)2 ‚àí(t‚Ä≤)2 = 0.
Thus ‚ü®T‚àó
vLATv(w), w‚ü©= ‚ü®LATv(w), Tv(w)‚ü©= (x‚Ä≤)2 + (y‚Ä≤)2 + (z‚Ä≤)2 ‚àí(t‚Ä≤)2 = 0,
and the conclusion follows.
Case 2.
t < 0. The proof follows by applying case 1 to ‚àíw.
We now proceed to deduce information about Tv. Let
w1 =
‚éõ
‚éú
‚éú
‚éù
1
0
0
1
‚éû
‚éü
‚éü
‚é†
and
w2 =
‚éõ
‚éú
‚éú
‚éù
1
0
0
‚àí1
‚éû
‚éü
‚éü
‚é†.
By Exercise 3, {w1, w2} is an orthogonal basis for span({e1, e4}), and
span({e1, e4}) is T‚àó
vLATv-invariant. The next result tells us even more.
Theorem 6.41. There exist nonzero scalars a and b such that
(a) T‚àó
vLATv(w1) = aw2.
(b) T‚àó
vLATv(w2) = bw1.
Proof. (a) Because ‚ü®LA(w1), w1‚ü©= 0, ‚ü®T‚àó
vLATv(w1), w1‚ü©= 0 by Theo-
rem 6.40.
Thus T‚àó
vLATv(w1) is orthogonal to w1.
Since span({e1, e4}) =
span({w1, w2}) is T‚àó
vLATv-invariant, T‚àó
vLATv(w1) must lie in this set. But
{w1, w2} is an orthogonal basis for this subspace, and so T‚àó
vLATv(w1) must
be a multiple of w2. Thus T‚àó
vLATv(w1) = aw2 for some scalar a. Since Tv
and A are invertible, so is T‚àó
vLATv. Thus a Ã∏= 0, proving (a).
The proof of (b) is similar to (a).
Corollary. Let Bv = [Tv]Œ≤, where Œ≤ is the standard ordered basis for R4.
Then
(a) B‚àó
vABv = A.
(b) T‚àó
vLATv = LA.
We leave the proof of the corollary as an exercise. For hints, see Exercise 4.
Now consider the situation 1 second after the origins of S and S‚Ä≤ have
coincided as measured by the clock C. Since the origin of S‚Ä≤ is moving along
the x-axis at a velocity v as measured in S, its space‚Äìtime coordinates relative
to S and C are
‚éõ
‚éú
‚éú
‚éù
v
0
0
1
‚éû
‚éü
‚éü
‚é†.

458
Chap. 6
Inner Product Spaces
Similarly, the space‚Äìtime coordinates for the origin of S‚Ä≤ relative to S‚Ä≤ and
C‚Ä≤ must be
‚éõ
‚éú
‚éú
‚éù
0
0
0
t‚Ä≤
‚éû
‚éü
‚éü
‚é†
for some t‚Ä≤ > 0. Thus we have
Tv
‚éõ
‚éú
‚éú
‚éù
v
0
0
1
‚éû
‚éü
‚éü
‚é†=
‚éõ
‚éú
‚éú
‚éù
0
0
0
t‚Ä≤
‚éû
‚éü
‚éü
‚é†
for some t‚Ä≤ > 0.
(18)
By the corollary to Theorem 6.41,
1
T‚àó
vLATv
‚éõ
‚éú
‚éú
‚éù
v
0
0
1
‚éû
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éù
v
0
0
1
‚éû
‚éü
‚éü
‚é†
2
=
1
LA
‚éõ
‚éú
‚éú
‚éù
v
0
0
1
‚éû
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éù
v
0
0
1
‚éû
‚éü
‚éü
‚é†
2
= v2 ‚àí1.
(19)
But also
1
T‚àó
vLATv
‚éõ
‚éú
‚éú
‚éù
v
0
0
1
‚éû
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éù
v
0
0
1
‚éû
‚éü
‚éü
‚é†
2
=
1
LATv
‚éõ
‚éú
‚éú
‚éù
v
0
0
1
‚éû
‚éü
‚éü
‚é†, Tv
‚éõ
‚éú
‚éú
‚éù
v
0
0
1
‚éû
‚éü
‚éü
‚é†
2
=
1
LA
‚éõ
‚éú
‚éú
‚éù
0
0
0
t‚Ä≤
‚éû
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éù
0
0
0
t‚Ä≤
‚éû
‚éü
‚éü
‚é†
2
= ‚àí(t‚Ä≤)2.
(20)
Combining (19) and (20), we conclude that v2 ‚àí1 = ‚àí(t‚Ä≤)2, or
t‚Ä≤ =

1 ‚àív2.
(21)
Thus, from (18) and (21), we obtain
Tv
‚éõ
‚éú
‚éú
‚éù
v
0
0
1
‚éû
‚éü
‚éü
‚é†=
‚éõ
‚éú
‚éú
‚éù
0
0
0
‚àö
1 ‚àív2
‚éû
‚éü
‚éü
‚é†.
(22)
Next recall that the origin of S moves in the negative direction of the
x‚Ä≤-axis of S‚Ä≤ at the constant velocity ‚àív < 0 as measured from S‚Ä≤. [This fact

Sec. 6.9
Einstein‚Äôs Special Theory of Relativity
459
is axiom (R 5).] Consequently, 1 second after the origins of S and S‚Ä≤ have
coincided as measured on clock C, there exists a time t‚Ä≤‚Ä≤ > 0 as measured on
clock C‚Ä≤ such that
Tv
‚éõ
‚éú
‚éú
‚éù
0
0
0
1
‚éû
‚éü
‚éü
‚é†=
‚éõ
‚éú
‚éú
‚éù
‚àívt‚Ä≤‚Ä≤
0
0
t‚Ä≤‚Ä≤
‚éû
‚éü
‚éü
‚é†.
(23)
From (23), it follows in a manner similar to the derivation of (22) that
t‚Ä≤‚Ä≤ =
1
‚àö
1 ‚àív2 ;
(24)
hence, from (23) and (24),
Tv
‚éõ
‚éú
‚éú
‚éù
0
0
0
1
‚éû
‚éü
‚éü
‚é†=
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
‚àív
‚àö
1 ‚àív2
0
0
1
‚àö
1 ‚àív2
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
.
(25)
The following result is now easily proved using (22), (25), and Theorem 6.39.
Theorem 6.42. Let Œ≤ be the standard ordered basis for R4. Then
[TV ]Œ≤ = Bv =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
1
‚àö
1 ‚àív2
0
0
‚àív
‚àö
1 ‚àív2
0
1
0
0
0
0
1
0
‚àív
‚àö
1 ‚àív2
0
0
1
‚àö
1 ‚àív2
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
.
Time Contraction
A most curious and paradoxical conclusion follows if we accept Einstein‚Äôs
theory. Suppose that an astronaut leaves our solar system in a space vehicle
traveling at a Ô¨Åxed velocity v as measured relative to our solar system. It
follows from Einstein‚Äôs theory that, at the end of time t as measured on Earth,
the time that passes on the space vehicle is only t
‚àö
1 ‚àív2. To establish this
result, consider the coordinate systems S and S‚Ä≤ and clocks C and C‚Ä≤ that
we have been studying.
Suppose that the origin of S‚Ä≤ coincides with the
space vehicle and the origin of S coincides with a point in the solar system

460
Chap. 6
Inner Product Spaces
(stationary relative to the sun) so that the origins of S and S‚Ä≤ coincide and
clocks C and C‚Ä≤ read zero at the moment the astronaut embarks on the trip.
As viewed from S, the space‚Äìtime coordinates of the vehicle at any time
t > 0 as measured by C are
‚éõ
‚éú
‚éú
‚éù
vt
0
0
t
‚éû
‚éü
‚éü
‚é†,
whereas, as viewed from S‚Ä≤, the space‚Äìtime coordinates of the vehicle at any
time t‚Ä≤ > 0 as measured by C‚Ä≤ are
‚éõ
‚éú
‚éú
‚éù
0
0
0
t‚Ä≤
‚éû
‚éü
‚éü
‚é†.
But if two sets of space‚Äìtime coordinates
‚éõ
‚éú
‚éú
‚éù
vt
0
0
t
‚éû
‚éü
‚éü
‚é†
and
‚éõ
‚éú
‚éú
‚éù
0
0
0
t‚Ä≤
‚éû
‚éü
‚éü
‚é†
are to describe the same event, it must follow that
Tv
‚éõ
‚éú
‚éú
‚éù
vt
0
0
t
‚éû
‚éü
‚éü
‚é†=
‚éõ
‚éú
‚éú
‚éù
0
0
0
t‚Ä≤
‚éû
‚éü
‚éü
‚é†.
Thus
[TV ]Œ≤ = Bv =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
1
‚àö
1 ‚àív2
0
0
‚àív
‚àö
1 ‚àív2
0
1
0
0
0
0
1
0
‚àív
‚àö
1 ‚àív2
0
0
1
‚àö
1 ‚àív2
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
‚éõ
‚éú
‚éú
‚éù
vt
0
0
t
‚éû
‚éü
‚éü
‚é†=
‚éõ
‚éú
‚éú
‚éù
0
0
0
t‚Ä≤
‚éû
‚éü
‚éü
‚é†.
From the preceding equation, we obtain
‚àív2t
‚àö
1 ‚àív2 +
t
‚àö
1 ‚àív2 = t‚Ä≤, or
t‚Ä≤ = t

1 ‚àív2.
(26)

Sec. 6.9
Einstein‚Äôs Special Theory of Relativity
461
This is the desired result.
A dramatic consequence of time contraction is that distances are con-
tracted along the line of motion (see Exercise 9).
Let us make one additional point.
Suppose that we consider units of
distance and time more commonly used than the light second and second,
such as the mile and hour, or the kilometer and second. Let c denote the
speed of light relative to our chosen units of distance. It is easily seen that if
an object travels at a velocity v relative to a set of units, then it is traveling
at a velocity v/c in units of light seconds per second. Thus, for an arbitrary
set of units of distance and time, (26) becomes
t‚Ä≤ = t
"
1 ‚àív2
c2 .
EXERCISES
1.
Prove (b), (c), and (d) of Theorem 6.39.
2.
Complete the proof of Theorem 6.40 for the case t < 0.
3.
For
w1 =
‚éõ
‚éú
‚éú
‚éù
1
0
0
1
‚éû
‚éü
‚éü
‚é†
and
w2 =
‚éõ
‚éú
‚éú
‚éù
1
0
0
‚àí1
‚éû
‚éü
‚éü
‚é†,
show that
(a)
{w1, w2} is an orthogonal basis for span({e1, e4});
(b)
span({e1, e4}) is T‚àó
vLATv-invariant.
4.
Prove the corollary to Theorem 6.41.
Hints:
(a)
Prove that
B‚àó
vABv =
‚éõ
‚éú
‚éú
‚éù
p
0
0
q
0
1
0
0
0
0
1
0
‚àíq
0
0
‚àíp
‚éû
‚éü
‚éü
‚é†,
where
p = a + b
2
and
q = a ‚àíb
2
.

462
Chap. 6
Inner Product Spaces
(b)
Show that q = 0 by using the fact that B‚àó
vABv is self-adjoint.
(c)
Apply Theorem 6.40 to
w =
‚éõ
‚éú
‚éú
‚éù
0
1
0
1
‚éû
‚éü
‚éü
‚é†
to show that p = 1.
5.
Derive (24), and prove that
Tv
‚éõ
‚éú
‚éú
‚éù
0
0
0
1
‚éû
‚éü
‚éü
‚é†=
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
‚àív
‚àö
1 ‚àív2
0
0
1
‚àö
1 ‚àív2
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
.
(25)
Hint: Use a technique similar to the derivation of (22).
6.
Consider three coordinate systems S, S‚Ä≤, and S‚Ä≤‚Ä≤ with the corresponding
axes (x,x‚Ä≤,x‚Ä≤‚Ä≤; y,y‚Ä≤,y‚Ä≤‚Ä≤; and z,z‚Ä≤,z‚Ä≤‚Ä≤) parallel and such that the x-, x‚Ä≤-,
and x‚Ä≤‚Ä≤-axes coincide. Suppose that S‚Ä≤ is moving past S at a velocity
v1 > 0 (as measured on S), S‚Ä≤‚Ä≤ is moving past S‚Ä≤ at a velocity v2 > 0
(as measured on S‚Ä≤), and S‚Ä≤‚Ä≤ is moving past S at a velocity v3 > 0 (as
measured on S), and that there are three clocks C, C‚Ä≤, and C‚Ä≤‚Ä≤ such
that C is stationary relative to S, C‚Ä≤ is stationary relative to S‚Ä≤, and
C‚Ä≤‚Ä≤ is stationary relative to S‚Ä≤‚Ä≤. Suppose that when measured on any
of the three clocks, all the origins of S, S‚Ä≤, and S‚Ä≤‚Ä≤ coincide at time 0.
Assuming that Tv3 = Tv2Tv1 (i.e., Bv3 = Bv2Bv1), prove that
v3 = v1 + v2
1 + v1v2
.
Note that substituting v2 = 1 in this equation yields v3 = 1. This tells
us that the speed of light as measured in S or S‚Ä≤ is the same. Why
would we be surprised if this were not the case?
7.
Compute (Bv)‚àí1. Show (Bv)‚àí1 = B(‚àív). Conclude that if S‚Ä≤ moves at
a negative velocity v relative to S, then [Tv]Œ≤ = Bv, where Bv is of the
form given in Theorem 6.42.
8.
Suppose that an astronaut left Earth in the year 2000 and traveled to
a star 99 light years away from Earth at 99% of the speed of light and
that upon reaching the star immediately turned around and returned
to Earth at the same speed.
Assuming Einstein‚Äôs special theory of

Sec. 6.9
Einstein‚Äôs Special Theory of Relativity
463
relativity, show that if the astronaut was 20 years old at the time of
departure, then he or she would return to Earth at age 48.2 in the year
2200. Explain the use of Exercise 7 in solving this problem.
9.
Recall the moving space vehicle considered in the study of time contrac-
tion. Suppose that the vehicle is moving toward a Ô¨Åxed star located on
the x-axis of S at a distance b units from the origin of S. If the space
vehicle moves toward the star at velocity v, Earthlings (who remain ‚Äúal-
most‚Äù stationary relative to S) compute the time it takes for the vehicle
to reach the star as t = b/v. Due to the phenomenon of time contraction,
the astronaut perceives a time span of t‚Ä≤ = t
‚àö
1 ‚àív2 = (b/v)
‚àö
1 ‚àív2.
A paradox appears in that the astronaut perceives a time span incon-
sistent with a distance of b and a velocity of v. The paradox is resolved
by observing that the distance from the solar system to the star as
measured by the astronaut is less than b.
Assuming that the coordinate systems S and S‚Ä≤ and clocks C and C‚Ä≤
are as in the discussion of time contraction, prove the following results.
(a)
At time t (as measured on C), the space‚Äìtime coordinates of star
relative to S and C are
‚éõ
‚éú
‚éú
‚éù
b
0
0
t
‚éû
‚éü
‚éü
‚é†.
(b)
At time t (as measured on C), the space‚Äìtime coordinates of the
star relative to S‚Ä≤ and C‚Ä≤ are
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
b ‚àívt
‚àö
1 ‚àív2
0
0
t ‚àíbv
‚àö
1 ‚àív2
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
.
(c)
For
x‚Ä≤ =
b ‚àítv
‚àö
1 ‚àív2
and
t‚Ä≤ =
t ‚àíbv
‚àö
1 ‚àív2 ,
we have x‚Ä≤ = b
‚àö
1 ‚àív2 ‚àít‚Ä≤v.
This result may be interpreted to mean that at time t‚Ä≤ as measured by
the astronaut, the distance from the astronaut to the star as measured
by the astronaut (see Figure 6.9) is
b

1 ‚àív2 ‚àít‚Ä≤v.

464
Chap. 6
Inner Product Spaces
................................................................................................................................................................................................................................................................................................................................................................................................................................
................................................................................................................................................................................................................................................................................................................................................................................................................................
r
r
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9

>
ZZ
~

1
6
-
-
6

1
z
z‚Ä≤
y
x‚Ä≤
y‚Ä≤
S‚Ä≤
x
S
C
C‚Ä≤
*(star)
(x‚Ä≤, 0, 0)
coordinates
relative to S‚Ä≤
(b, 0, 0)
coordinates
relative to S
Figure 6.9
(d)
Conclude from the preceding equation that
(1)
the speed of the space vehicle relative to the star, as measured
by the astronaut, is v;
(2)
the distance from Earth to the star, as measured by the astro-
naut, is b
‚àö
1 ‚àív2.
Thus distances along the line of motion of the space vehicle appear
to be contracted by a factor of
‚àö
1 ‚àív2.
6.10‚àó
CONDITIONING AND THE RAYLEIGH QUOTIENT
In Section 3.4, we studied speciÔ¨Åc techniques that allow us to solve systems of
linear equations in the form Ax = b, where A is an m √ó n matrix and b is an
m √ó 1 vector. Such systems often arise in applications to the real world. The
coeÔ¨Écients in the system are frequently obtained from experimental data,
and, in many cases, both m and n are so large that a computer must be used
in the calculation of the solution. Thus two types of errors must be considered.
First, experimental errors arise in the collection of data since no instruments
can provide completely accurate measurements. Second, computers introduce
roundoÔ¨Äerrors. One might intuitively feel that small relative changes in the
coeÔ¨Écients of the system cause small relative errors in the solution. A system
that has this property is called well-conditioned; otherwise, the system is
called ill-conditioned.
We now consider several examples of these types of errors, concentrating
primarily on changes in b rather than on changes in the entries of A.
In
addition, we assume that A is a square, complex (or real), invertible matrix
since this is the case most frequently encountered in applications.

Sec. 6.10
Conditioning and the Rayleigh Quotient
465
Example 1
Consider the system
x1 + x2 = 5
x1 ‚àíx2 = 1.
The solution to this system is

3
2
	
.
Now suppose that we change the system somewhat and consider the new
system
x1 + x2 = 5
x1 ‚àíx2 = 1.0001.
This modiÔ¨Åed system has the solution

3.00005
1.99995
	
.
We see that a change of 10‚àí4 in one coeÔ¨Écient has caused a change of less
than 10‚àí4 in each coordinate of the new solution. More generally, the system
x1 + x2 = 5
x1 ‚àíx2 = 1 + Œ¥
has the solution

3 + Œ¥/2
2 ‚àíŒ¥/2
	
.
Hence small changes in b introduce small changes in the solution. Of course,
we are really interested in relative changes since a change in the solution of,
say, 10, is considered large if the original solution is of the order 10‚àí2, but
small if the original solution is of the order 106.
We use the notation Œ¥b to denote the vector b‚Ä≤ ‚àíb, where b is the vector
in the original system and b‚Ä≤ is the vector in the modiÔ¨Åed system. Thus we
have
Œ¥b =

5
1 + h
	
‚àí

5
1
	
=

0
h
	
.
We now deÔ¨Åne the relative change in b to be the scalar ‚à•Œ¥b‚à•/‚à•b‚à•, where
‚à•¬∑ ‚à•denotes the standard norm on Cn (or Rn); that is, ‚à•b‚à•=

‚ü®b, b‚ü©. Most

466
Chap. 6
Inner Product Spaces
of what follows, however, is true for any norm. Similar deÔ¨Ånitions hold for
the relative change in x. In this example,
‚à•Œ¥b‚à•
‚à•b‚à•= |h|
‚àö
26
and
‚à•Œ¥x‚à•
‚à•x‚à•=
5555

3 + (h/2)
2 ‚àí(h/2)
	
‚àí

3
2
	5555
5555

3
2
	5555
= |h|
‚àö
26.
Thus the relative change in x equals, coincidentally, the relative change in b;
so the system is well-conditioned.
‚ô¶
Example 2
Consider the system
x1
+
x2
=
3
x1
+
1.00001x2
=
3.00001,
which has

2
1
	
as its solution. The solution to the related system
x1 +
x2 = 3
x1 + 1.00001x2 = 3.00001 + Œ¥
is

2 ‚àí(105)h
1 + (105)h
	
.
Hence,
‚à•Œ¥x‚à•
‚à•x‚à•= 105
2/5 |h| ‚â•104|h|,
while
‚à•Œ¥b‚à•
‚à•b‚à•‚âà|h|
3
‚àö
2.
Thus the relative change in x is at least 104 times the relative change in b!
This system is very ill-conditioned. Observe that the lines deÔ¨Åned by the two
equations are nearly coincident. So a small change in either line could greatly
alter the point of intersection, that is, the solution to the system.
‚ô¶

Sec. 6.10
Conditioning and the Rayleigh Quotient
467
To apply the full strength of the theory of self-adjoint matrices to the
study of conditioning, we need the notion of the norm of a matrix.
(See
Exercise 24 of Section 6.1 for further results about norms.)
DeÔ¨Ånition.
Let A be a complex (or real) n √ó n matrix. DeÔ¨Åne the
(Euclidean) norm of A by
‚à•A‚à•= max
xÃ∏=0
‚à•Ax‚à•
‚à•x‚à•,
where x ‚ààCn or x ‚ààRn.
Intuitively, ‚à•A‚à•represents the maximum magniÔ¨Åcation of a vector by the
matrix A. The question of whether or not this maximum exists, as well as
the problem of how to compute it, can be answered by the use of the so-called
Rayleigh quotient.
DeÔ¨Ånition.
Let B be an n √ó n self-adjoint matrix.
The Rayleigh
quotient for x Ã∏= 0 is deÔ¨Åned to be the scalar R(x) = ‚ü®Bx, x‚ü©/‚à•x‚à•2.
The following result characterizes the extreme values of the Rayleigh quo-
tient of a self-adjoint matrix.
Theorem 6.43. For a self-adjoint matrix B ‚ààMn√ón(F), we have that
max
xÃ∏=0 R(x) is the largest eigenvalue of B and min
xÃ∏=0 R(x) is the smallest eigenvalue
of B.
Proof. By Theorems 6.19 (p. 384) and 6.20 (p. 384), we may choose an
orthonormal basis {v1, v2, . . . , vn} of eigenvectors of B such that Bvi = Œªivi
(1 ‚â§i ‚â§n), where Œª1 ‚â•Œª2 ‚â•¬∑ ¬∑ ¬∑ ‚â•Œªn. (Recall that by the lemma to
Theorem 6.17, p. 373, the eigenvalues of B are real.) Now, for x ‚ààFn, there
exist scalars a1, a2, . . . , an such that
x =
n

i=1
aivi.
Hence
R(x)
=
‚ü®Bx, x‚ü©
‚à•x‚à•2
=
<,n
i=1 aiŒªivi, ,n
j=1 ajvj
=
‚à•x‚à•2
=
,n
i=1 Œªi|ai|2
‚à•x‚à•2
‚â§Œª1
,n
i=1 |ai|2
‚à•x‚à•2
= Œª1‚à•x‚à•2
‚à•x‚à•2
= Œª1.
It is easy to see that R(v1) = Œª1, so we have demonstrated the Ô¨Årst half of
the theorem. The second half is proved similarly.

468
Chap. 6
Inner Product Spaces
Corollary 1. For any square matrix A, ‚à•A‚à•is Ô¨Ånite and, in fact, equals
‚àö
Œª, where Œª is the largest eigenvalue of A‚àóA.
Proof. Let B be the self-adjoint matrix A‚àóA, and let Œª be the largest
eigenvalue of B. Since, for x Ã∏= 0,
0 ‚â§‚à•Ax‚à•2
‚à•x‚à•2
= ‚ü®Ax, Ax‚ü©
‚à•x‚à•2
= ‚ü®A‚àóAx, x‚ü©
‚à•x‚à•2
= ‚ü®Bx, x‚ü©
‚à•x‚à•2
= R(x),
it follows from Theorem 6.43 that ‚à•A‚à•2 = Œª.
Observe that the proof of Corollary 1 shows that all the eigenvalues of
A‚àóA are nonnegative. For our next result, we need the following lemma.
Lemma. For any square matrix A, Œª is an eigenvalue of A‚àóA if and only
if Œª is an eigenvalue of AA‚àó.
Proof. Let Œª be an eigenvalue of A‚àóA. If Œª = 0, then A‚àóA is not invertible.
Hence A and A‚àóare not invertible, so that Œª is also an eigenvalue of AA‚àó.
The proof of the converse is similar.
Suppose now that Œª Ã∏= 0. Then there exists x Ã∏= 0 such that A‚àóAx = Œªx.
Apply A to both sides to obtain (AA‚àó)(Ax) = Œª(Ax). Since Ax Ã∏= 0 (lest
Œªx = 0), we have that Œª is an eigenvalue of AA‚àó. The proof of the converse
is left as an exercise.
Corollary 2.
Let A be an invertible matrix. Then ‚à•A‚àí1‚à•= 1/
‚àö
Œª,
where Œª is the smallest eigenvalue of A‚àóA.
Proof. Recall that Œª is an eigenvalue of an invertible matrix if and only if
Œª‚àí1 is an eigenvalue of its inverse.
Now let Œª1 ‚â•Œª2 ‚â•¬∑ ¬∑ ¬∑ ‚â•Œªn be the eigenvalues of A‚àóA, which by the
lemma are the eigenvalues of AA‚àó. Then ‚à•A‚àí1‚à•2 equals the largest eigenvalue
of (A‚àí1)‚àóA‚àí1 = (AA‚àó)‚àí1, which equals 1/Œªn.
For many applications, it is only the largest and smallest eigenvalues that
are of interest. For example, in the case of vibration problems, the smallest
eigenvalue represents the lowest frequency at which vibrations can occur.
We see the role of both of these eigenvalues in our study of conditioning.
Example 3
Let
A =
‚éõ
‚éù
1
0
1
‚àí1
1
0
0
1
1
‚éû
‚é†.

Sec. 6.10
Conditioning and the Rayleigh Quotient
469
Then
B = A‚àóA =
‚éõ
‚éù
2
‚àí1
1
‚àí1
2
1
1
1
2
‚éû
‚é†.
The eigenvalues of B are 3, 3, and 0. Therefore, ‚à•A‚à•=
‚àö
3. For any
x =
‚éõ
‚éù
a
b
c
‚éû
‚é†Ã∏= 0,
we may compute R(x) for the matrix B as
3 ‚â•R(x) = ‚ü®Bx, x‚ü©
‚à•x‚à•2
= 2(a2 + b2 + c2 ‚àíab + ac + bc)
a2 + b2 + c2
.
‚ô¶
Now that we know ‚à•A‚à•exists for every square matrix A, we can make use
of the inequality ‚à•Ax‚à•‚â§‚à•A‚à•¬∑ ‚à•x‚à•, which holds for every x.
Assume in what follows that A is invertible, b Ã∏= 0, and Ax = b. For
a given Œ¥b, let Œ¥x be the vector that satisÔ¨Åes A(x + Œ¥x) = b + Œ¥b.
Then
A(Œ¥x) = Œ¥b, and so Œ¥x = A‚àí1(Œ¥b). Hence
‚à•b‚à•= ‚à•Ax‚à•‚â§‚à•A‚à•¬∑ ‚à•x‚à•
and
‚à•Œ¥x‚à•= ‚à•A‚àí1(Œ¥b)‚à•‚â§‚à•A‚àí1‚à•¬∑ ‚à•Œ¥b‚à•.
Thus
‚à•Œ¥x‚à•
‚à•x‚à•‚â§
‚à•x‚à•2
‚à•b‚à•/‚à•A‚à•‚â§‚à•A‚àí1‚à•¬∑ ‚à•Œ¥b‚à•¬∑ ‚à•A‚à•
‚à•b‚à•
= ‚à•A‚à•¬∑ ‚à•A‚àí1‚à•¬∑
‚à•Œ¥b‚à•
‚à•b‚à•
	
.
Similarly (see Exercise 9),
1
‚à•A‚à•¬∑ ‚à•A‚àí1‚à•
‚à•Œ¥b‚à•
‚à•b‚à•
	
‚â§‚à•Œ¥x‚à•
‚à•x‚à•.
The number ‚à•A‚à•¬∑ ‚à•A‚àí1‚à•is called the condition number of A and is
denoted cond(A). It should be noted that the deÔ¨Ånition of cond(A) depends
on how the norm of A is deÔ¨Åned. There are many reasonable ways of deÔ¨Åning
the norm of a matrix.
In fact, the only property needed to establish the
inequalities above is that ‚à•Ax‚à•‚â§‚à•A‚à•¬∑ ‚à•x‚à•for all x. We summarize these
results in the following theorem.
Theorem 6.44. For the system Ax = b, where A is invertible and b Ã∏= 0,
the following statements are true.
(a) For any norm ‚à•¬∑ ‚à•, we have
1
cond(A)
‚à•Œ¥b‚à•
‚à•b‚à•‚â§‚à•Œ¥x‚à•
‚à•x‚à•‚â§cond(A)‚à•Œ¥b‚à•
‚à•b‚à•.

470
Chap. 6
Inner Product Spaces
(b) If ‚à•¬∑ ‚à•is the Euclidean norm, then cond(A) =

Œª1/Œªn , where Œª1 and
Œªn are the largest and smallest eigenvalues, respectively, of A‚àóA.
Proof. Statement (a) follows from the previous inequalities, and (b) follows
from Corollaries 1 and 2 to Theorem 6.43.
It is clear from Theorem 6.44 that cond(A) ‚â•1. It is left as an exercise
to prove that cond(A) = 1 if and only if A is a scalar multiple of a unitary or
orthogonal matrix. Moreover, it can be shown with some work that equality
can be obtained in (a) by an appropriate choice of b and Œ¥b.
We can see immediately from (a) that if cond(A) is close to 1, then a
small relative error in b forces a small relative error in x. If cond(A) is large,
however, then the relative error in x may be small even though the relative
error in b is large, or the relative error in x may be large even though the
relative error in b is small! In short, cond(A) merely indicates the potential
for large relative errors.
We have so far considered only errors in the vector b. If there is an error
Œ¥A in the coeÔ¨Écient matrix of the system Ax = b, the situation is more
complicated. For example, A + Œ¥A may fail to be invertible. But under the
appropriate assumptions, it can be shown that a bound for the relative error
in x can be given in terms of cond(A). For example, Charles Cullen (Charles
G. Cullen, An Introduction to Numerical Linear Algebra, PWS Publishing
Co., Boston 1994, p. 60) shows that if A + Œ¥A is invertible, then
‚à•Œ¥x‚à•
‚à•x + Œ¥x‚à•‚â§cond(A)‚à•Œ¥A‚à•
‚à•A‚à•.
It should be mentioned that, in practice, one never computes cond(A)
from its deÔ¨Ånition, for it would be an unnecessary waste of time to compute
A‚àí1 merely to determine its norm. In fact, if a computer is used to Ô¨Ånd
A‚àí1, the computed inverse of A in all likelihood only approximates A‚àí1, and
the error in the computed inverse is aÔ¨Äected by the size of cond(A). So we
are caught in a vicious circle! There are, however, some situations in which
a usable approximation of cond(A) can be found. Thus, in most cases, the
estimate of the relative error in x is based on an estimate of cond(A).
EXERCISES
1.
Label the following statements as true or false.
(a)
If Ax = b is well-conditioned, then cond(A) is small.
(b)
If cond(A) is large, then Ax = b is ill-conditioned.
(c)
If cond(A) is small, then Ax = b is well-conditioned.
(d)
The norm of A equals the Rayleigh quotient.
(e)
The norm of A always equals the largest eigenvalue of A.

Sec. 6.10
Conditioning and the Rayleigh Quotient
471
2.
Compute the norms of the following matrices.
(a)

4
0
1
3
	
(b)

5
3
‚àí3
3
	
(c)
‚éõ
‚éú
‚éú
‚éù
1
‚àí2
‚àö
3
0
0
‚àí2
‚àö
3
1
0
2
‚àö
3
1
‚éû
‚éü
‚éü
‚é†
3.
Prove that if B is symmetric, then ‚à•B‚à•is the largest eigenvalue of B.
4.
Let A and A‚àí1 be as follows:
A =
‚éõ
‚éù
6
13
‚àí17
13
29
‚àí38
‚àí17
‚àí38
50
‚éû
‚é†
and
A‚àí1 =
‚éõ
‚éù
6
‚àí4
1
‚àí4
11
7
‚àí1
7
5
‚éû
‚é†.
The eigenvalues of A are approximately 84.74, 0.2007, and 0.0588.
(a)
Approximate ‚à•A‚à•, ‚à•A‚àí1‚à•, and cond(A). (Note Exercise 3.)
(b)
Suppose that we have vectors x and Àúx such that Ax = b and
‚à•b ‚àíAÀúx‚à•‚â§0.001.
Use (a) to determine upper bounds for
‚à•Àúx ‚àíA‚àí1b‚à•(the absolute error) and ‚à•Àúx ‚àíA‚àí1b‚à•/‚à•A‚àí1b‚à•(the rel-
ative error).
5.
Suppose that x is the actual solution of Ax = b and that a computer
arrives at an approximate solution Àúx. If cond(A) = 100, ‚à•b‚à•= 1, and
‚à•b ‚àíAÀúx‚à•= 0.1, obtain upper and lower bounds for ‚à•x ‚àíÀúx‚à•/‚à•x‚à•.
6.
Let
B =
‚éõ
‚éù
2
1
1
1
2
1
1
1
2
‚éû
‚é†.
Compute
R
‚éõ
‚éù
1
‚àí2
3
‚éû
‚é†,
‚à•B‚à•,
and
cond(B).
7.
Let B be a symmetric matrix. Prove that min
xÃ∏=0 R(x) equals the smallest
eigenvalue of B.
8.
Prove that if Œª is an eigenvalue of AA‚àó, then Œª is an eigenvalue of A‚àóA.
This completes the proof of the lemma to Corollary 2 to Theorem 6.43.
9.
Prove that if A is an invertible matrix and Ax = b, then
1
‚à•A‚à•¬∑ ‚à•A‚àí1‚à•
‚à•Œ¥b‚à•
‚à•b‚à•
	
‚â§‚à•Œ¥x‚à•
‚à•x‚à•.

472
Chap. 6
Inner Product Spaces
10.
Prove the left inequality of (a) in Theorem 6.44.
11.
Prove that cond(A) = 1 if and only if A is a scalar multiple of a unitary
or orthogonal matrix.
12. (a)
Let A and B be square matrices that are unitarily equivalent.
Prove that ‚à•A‚à•= ‚à•B‚à•.
(b)
Let T be a linear operator on a Ô¨Ånite-dimensional inner product
space V. DeÔ¨Åne
‚à•T‚à•= max
xÃ∏=0
‚à•T(x)‚à•
‚à•x‚à•
.
Prove that ‚à•T‚à•= ‚à•[T]Œ≤‚à•, where Œ≤ is any orthonormal basis for V.
(c)
Let V be an inÔ¨Ånite-dimensional inner product space with an or-
thonormal basis {v1, v2, . . .}. Let T be the linear operator on V
such that T(vk) = kvk. Prove that ‚à•T‚à•(deÔ¨Åned in (b)) does not
exist.
The next exercise assumes the deÔ¨Ånitions of singular value and pseudoinverse
and the results of Section 6.7.
13.
Let A be an n √ó n matrix of rank r with the nonzero singular values
œÉ1 ‚â•œÉ2 ‚â•¬∑ ¬∑ ¬∑ ‚â•œÉr. Prove each of the following results.
(a)
‚à•A‚à•= œÉ1.
(b)
‚à•A‚Ä†‚à•= 1
œÉr
.
(c)
If A is invertible (and hence r = n), then cond(A) = œÉ1
œÉn
.
6.11‚àó
THE GEOMETRY OF ORTHOGONAL OPERATORS
By Theorem 6.22 (p. 386), any rigid motion on a Ô¨Ånite-dimensional real inner
product space is the composite of an orthogonal operator and a translation.
Thus, to understand the geometry of rigid motions thoroughly, we must ana-
lyze the structure of orthogonal operators. Such is the aim of this section. We
show that any orthogonal operator on a Ô¨Ånite-dimensional real inner product
space is the composite of rotations and reÔ¨Çections.
This material assumes familiarity with the results about direct sums de-
veloped at the end of Section 5.2, and familiarity with the deÔ¨Ånition and
elementary properties of the determinant of a linear operator deÔ¨Åned in Ex-
ercise 7 of Section 5.1.
DeÔ¨Ånitions. Let T be a linear operator on a Ô¨Ånite-dimensional real inner
product space V. The operator T is called a rotation if T is the identity on

Sec. 6.11
The Geometry of Orthogonal Operators
473
V or if there exists a two-dimensional subspace W of V, an orthonormal basis
Œ≤ = {x1, x2} for W, and a real number Œ∏ such that
T(x1) = (cos Œ∏)x1 + (sin Œ∏)x2,
T(x2) = (‚àísin Œ∏)x1 + (cos Œ∏)x2,
and T(y) = y for all y ‚ààW‚ä•. In this context, T is called a rotation of W
about W‚ä•. The subspace W‚ä•is called the axis of rotation.
Rotations are deÔ¨Åned in Section 2.1 for the special case that V = R2.
DeÔ¨Ånitions.
Let T be a linear operator on a Ô¨Ånite-dimensional real
inner product space V. The operator T is called a reÔ¨Çection if there exists
a one-dimensional subspace W of V such that T(x) = ‚àíx for all x ‚ààW and
T(y) = y for all y ‚ààW‚ä•. In this context, T is called a reÔ¨Çection of V about
W‚ä•.
It should be noted that rotations and reÔ¨Çections (or composites of these)
are orthogonal operators (see Exercise 2). The principal aim of this section
is to establish that the converse is also true, that is, any orthogonal operator
on a Ô¨Ånite-dimensional real inner product space is the composite of rotations
and reÔ¨Çections.
Example 1
A Characterization of Orthogonal Operators on a One-Dimensional Real In-
ner Product Space
Let T be an orthogonal operator on a one-dimensional inner product space
V. Choose any nonzero vector x in V. Then V = span({x}), and so T(x) = Œªx
for some Œª ‚ààR. Since T is orthogonal and Œª is an eigenvalue of T, Œª = ¬±1.
If Œª = 1, then T is the identity on V, and hence T is a rotation. If Œª = ‚àí1,
then T(x) = ‚àíx for all x ‚ààV; so T is a reÔ¨Çection of V about V‚ä•= {0}. Thus
T is either a rotation or a reÔ¨Çection. Note that in the Ô¨Årst case, det(T) = 1,
and in the second case, det(T) = ‚àí1.
‚ô¶
Example 2
Some Typical ReÔ¨Çections
(a) DeÔ¨Åne T: R2 ‚ÜíR2 by T(a, b) = (‚àía, b), and let W = span({e1}).
Then T(x) = ‚àíx for all x ‚ààW, and T(y) = y for all y ‚ààW‚ä•. Thus T is a
reÔ¨Çection of R2 about W‚ä•= span({e2}), the y-axis.
(b) Let T: R3 ‚ÜíR3 be deÔ¨Åned by T(a, b, c) = (a, b, ‚àíc), and let W =
span({e3}). Then T(x) = ‚àíx for all x ‚ààW, and T(y) = y for all y ‚ààW‚ä•=
span({e1, e2}), the xy-plane. Hence T is a reÔ¨Çection of R3 about W‚ä•.
‚ô¶
Example 1 characterizes all orthogonal operators on a one-dimensional
real inner product space. The following theorem characterizes all orthogonal

474
Chap. 6
Inner Product Spaces
operators on a two-dimensional real inner product space V. The proof fol-
lows from Theorem 6.23 (p. 387) since all two-dimensional real inner product
spaces are structurally identical. For a rigorous justiÔ¨Åcation, apply Theo-
rem 2.21 (p. 104), where Œ≤ is an orthonormal basis for V. By Exercise 15 of
Section 6.2, the resulting isomorphism œÜŒ≤ : V ‚ÜíR2 preserves inner products.
(See Exercise 8.)
Theorem 6.45. Let T be an orthogonal operator on a two-dimensional
real inner product space V. Then T is either a rotation or a reÔ¨Çection. Fur-
thermore, T is a rotation if and only if det(T) = 1, and T is a reÔ¨Çection if
and only if det(T) = ‚àí1.
A complete description of the reÔ¨Çections of R2 is given in Section 6.5.
Corollary. Let V be a two-dimensional real inner product space. The
composite of a reÔ¨Çection and a rotation on V is a reÔ¨Çection on V.
Proof. If T1 is a reÔ¨Çection on V and T2 is a rotation on V, then by
Theorem 6.45, det(T1) = 1 and det(T2) = ‚àí1.
Let T = T2T1 be the
composite. Since T2 and T1 are orthogonal, so is T. Moreover, det(T) =
det(T2)¬∑ det(T1) = ‚àí1. Thus, by Theorem 6.45, T is a reÔ¨Çection. The proof
for T1T2 is similar.
We now study orthogonal operators on spaces of higher dimension.
Lemma. If T is a linear operator on a nonzero Ô¨Ånite-dimensional real
vector space V, then there exists a T-invariant subspace W of V such that
1 ‚â§dim(W) ‚â§2.
Proof. Fix an ordered basis Œ≤ = {y1, y2, . . . , yn} for V, and let A = [T]Œ≤.
Let œÜŒ≤ : V ‚ÜíRn be the linear transformation deÔ¨Åned by œÜŒ≤(yi) = ei for
i = 1, 2, . . . , n. Then œÜŒ≤ is an isomorphism, and, as we have seen in Sec-
tion 2.4, the diagram in Figure 6.10 commutes, that is, LAœÜŒ≤ = œÜŒ≤T. As a
consequence, it suÔ¨Éces to show that there exists an LA-invariant subspace Z
of Rn such that 1 ‚â§dim(Z) ‚â§2. If we then deÔ¨Åne W = œÜ‚àí1
Œ≤ (Z), it follows
that W satisÔ¨Åes the conclusions of the lemma (see Exercise 13).
V
T
‚àí‚àí‚àí‚àí‚ÜíV
‚èê‚èê!œÜŒ≤
‚èê‚èê!œÜŒ≤
Rn
LA
‚àí‚àí‚àí‚àí‚ÜíRn
Figure 6.10

Sec. 6.11
The Geometry of Orthogonal Operators
475
The matrix A can be considered as an n √ó n matrix over C and, as such,
can be used to deÔ¨Åne a linear operator U on Cn by U(v) = Av. Since U
is a linear operator on a Ô¨Ånite-dimensional vector space over C, it has an
eigenvalue Œª ‚ààC. Let x ‚ààCn be an eigenvector corresponding to Œª. We may
write Œª = Œª1 + iŒª2, where Œª1 and Œª2 are real, and
x =
‚éõ
‚éú
‚éú
‚éú
‚éù
a1 + ib1
a2 + ib2
...
an + ibn
‚éû
‚éü
‚éü
‚éü
‚é†,
where the ai‚Äôs and bi‚Äôs are real. Thus, setting
x1 =
‚éõ
‚éú
‚éú
‚éú
‚éù
a1
a2
...
an
‚éû
‚éü
‚éü
‚éü
‚é†
and
x2 =
‚éõ
‚éú
‚éú
‚éú
‚éù
b1
b2
...
bn
‚éû
‚éü
‚éü
‚éü
‚é†,
we have x = x1 + ix2, where x1 and x2 have real entries. Note that at least
one of x1 or x2 is nonzero since x Ã∏= 0. Hence
U(x) = Œªx = (Œª1 + iŒª2)(x1 + ix2) = (Œª1x1 ‚àíŒª2x2) + i(Œª1x2 + Œª2x1).
Similarly,
U(x) = A(x1 + ix2) = Ax1 + iAx2.
Comparing the real and imaginary parts of these two expressions for U(x),
we conclude that
Ax1 = Œª1x1 ‚àíŒª2x2
and
Ax2 = Œª1x2 + Œª2x1.
Finally, let Z = span({x1, x2}), the span being taken as a subspace of Rn.
Since x1 Ã∏= 0 or x2 Ã∏= 0, Z is a nonzero subspace. Thus 1 ‚â§dim(Z) ‚â§2, and
the preceding pair of equations shows that Z is LA-invariant.
Theorem 6.46. Let T be an orthogonal operator on a nonzero Ô¨Ånite-
dimensional real inner product space V.
Then there exists a collection of
pairwise orthogonal T-invariant subspaces {W1, W2, . . . , Wm} of V such that
(a) 1 ‚â§dim(Wi) ‚â§2
for i = 1, 2, . . . , m.
(b) V = W1 ‚äïW2 ‚äï¬∑ ¬∑ ¬∑ ‚äïWm.
Proof. The proof is by mathematical induction on dim(V). If dim(V) = 1,
the result is obvious. So assume that the result is true whenever dim(V) < n
for some Ô¨Åxed integer n > 1.

476
Chap. 6
Inner Product Spaces
Suppose dim(V) = n. By the lemma, there exists a T-invariant subspace
W1 of V such that 1 ‚â§dim(W) ‚â§2. If W1 = V, the result is established.
Otherwise, W‚ä•
1 Ã∏= {0}. By Exercise 14, W‚ä•
1 is T-invariant and the restriction
of T to W‚ä•
1 is orthogonal. Since dim(W‚ä•
1 ) < n, we may apply the induc-
tion hypothesis to TW‚ä•
1 and conclude that there exists a collection of pair-
wise orthogonal T-invariant subspaces {W1, W2, . . . , Wm} of W‚ä•
1 such that
1 ‚â§dim(Wi) ‚â§2 for i = 2, 3, . . . , m and W‚ä•
1
= W2 ‚äïW3 ‚äï¬∑ ¬∑ ¬∑ ‚äïWm.
Thus {W1, W2, . . . , Wm} is pairwise orthogonal, and by Exercise 13(d) of
Section 6.2,
V = W1 ‚äïW‚ä•
1 = W1 ‚äïW2 ‚äï¬∑ ¬∑ ¬∑ ‚äïWm.
Applying Example 1 and Theorem 6.45 in the context of Theorem 6.46,
we conclude that the restriction of T to Wi is either a rotation or a reÔ¨Çection
for each i = 2, 3, . . . , m. Thus, in some sense, T is composed of rotations and
reÔ¨Çections. Unfortunately, very little can be said about the uniqueness of the
decomposition of V in Theorem 6.46. For example, the Wi‚Äôs, the number m
of Wi‚Äôs, and the number of Wi‚Äôs for which TWi is a reÔ¨Çection are not unique.
Although the number of Wi‚Äôs for which TWi is a reÔ¨Çection is not unique,
whether this number is even or odd is an intrinsic property of T. Moreover,
we can always decompose V so that TWi is a reÔ¨Çection for at most one Wi.
These facts are established in the following result.
Theorem 6.47. Let T, V, W1, . . . , Wm be as in Theorem 6.46.
(a) The number of Wi‚Äôs for which TWi is a reÔ¨Çection is even or odd according
to whether det(T) = 1 or det(T) = ‚àí1.
(b) It is always possible to decompose V as in Theorem 6.46 so that the
number of Wi‚Äôs for which TWi is a reÔ¨Çection is zero or one according to
whether det(T) = 1 or det(T) = ‚àí1. Furthermore, if TWi is a reÔ¨Çection,
then dim(Wi) = 1.
Proof. (a) Let r denote the number of Wi‚Äôs in the decomposition for which
TWi is a reÔ¨Çection. Then, by Exercise 15,
det(T) = det(TW1)¬∑ det(TW2)¬∑ ¬∑ ¬∑ ¬∑ ¬∑ det(TWm) = (‚àí1)r,
proving (a).
(b) Let E = {x ‚ààV: T(x) = ‚àíx}; then E is a T-invariant subspace
of V.
If W = E‚ä•, then W is T-invariant.
So by applying Theorem 6.46
to TW, we obtain a collection of pairwise orthogonal T-invariant subspaces
{W1, W2, . . . , Wk} of W such that W = W1 ‚äïW2 ‚äï¬∑ ¬∑ ¬∑ ‚äïWk and for 1 ‚â§
i ‚â§k, the dimension of each Wi is either 1 or 2. Observe that, for each
i = 1, 2, . . . , k, TWi is a rotation. For otherwise, if TWi is a reÔ¨Çection, there
exists a nonzero x ‚ààWi for which T(x) = ‚àíx. But then, x ‚ààWi ‚à©E ‚äÜ
E‚ä•‚à©E = {0}, a contradiction. If E = {0}, the result follows. Otherwise,

Sec. 6.11
The Geometry of Orthogonal Operators
477
choose an orthonormal basis Œ≤ for E containing p vectors (p > 0).
It is
possible to decompose Œ≤ into a pairwise disjoint union Œ≤ = Œ≤1 ‚à™Œ≤2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≤r
such that each Œ≤i contains exactly two vectors for i < r, and Œ≤r contains
two vectors if p is even and one vector if p is odd. For each i = 1, 2, . . . , r,
let Wk+i = span(Œ≤i). Then, clearly, {W1, W2, . . . , Wk, . . . , Wk+r} is pairwise
orthogonal, and
V = W1 ‚äïW2 ‚äï¬∑ ¬∑ ¬∑ ‚äïWk ‚äï¬∑ ¬∑ ¬∑ ‚äïWk+r.
(27)
Moreover, if any Œ≤i contains two vectors, then
det(TWk+i) = det([TWk+i]Œ≤i) = det

‚àí1
0
0
‚àí1
	
= 1.
So TWk+i is a rotation, and hence TWj is a rotation for j < k + r. If Œ≤r
consists of one vector, then dim(Wk+r) = 1 and
det(TWk+r) = det([TWk+r]Œ≤r) = det(‚àí1) = ‚àí1.
Thus TWk+r is a reÔ¨Çection by Theorem 6.46, and we conclude that the de-
composition in (27) satisÔ¨Åes the condition of (b).
As a consequence of the preceding theorem, an orthogonal operator can
be factored as a product of rotations and reÔ¨Çections.
Corollary. Let T be an orthogonal operator on a Ô¨Ånite-dimensional real
inner product space V. Then there exists a collection {T1, T2, . . . , Tm} of
orthogonal operators on V such that the following statements are true.
(a) For each i, Ti is either a reÔ¨Çection or a rotation.
(b) For at most one i, Ti is a reÔ¨Çection.
(c) TiTj = TjTi for all i and j.
(d) T = T1T2 ¬∑ ¬∑ ¬∑ Tm.
(e) det(T) =

1
if Ti is a rotation for each i
‚àí1
otherwise.
Proof. As in the proof of Theorem 6.47(b), we can write
V = W1 ‚äïW2 ‚äï¬∑ ¬∑ ¬∑ ‚äïWm,
where TWi is a rotation for i < m. For each i = 1, 2, . . . , m, deÔ¨Åne Ti : V ‚ÜíV
by
Ti(x1 + x2 + ¬∑ ¬∑ ¬∑ + xm) = x1 + x2 + ¬∑ ¬∑ ¬∑ + xi‚àí1 + T(xi) + xi+1 + ¬∑ ¬∑ ¬∑ + xm,
where xj ‚ààWj for all j. It is easily shown that each Ti is an orthogonal
operator on V. In fact, Ti is a rotation or a reÔ¨Çection according to whether
TWi is a rotation or a reÔ¨Çection. This establishes (a) and (b). The proofs
of (c), (d), and (e) are left as exercises. (See Exercise 16.)

478
Chap. 6
Inner Product Spaces
Example 3
Orthogonal Operators on a Three-Dimensional Real Inner Product Space
Let T be an orthogonal operator on a three-dimensional real inner product
space V. We show that T can be decomposed into the composite of a rotation
and at most one reÔ¨Çection. Let
V = W1 ‚äïW2 ‚äï¬∑ ¬∑ ¬∑ ‚äïWm
be a decomposition as in Theorem 6.47(b). Clearly, m = 2 or m = 3.
If m = 2, then V = W1 ‚äïW2. Without loss of generality, suppose that
dim(W1) = 1 and dim(W2) = 2. Thus TW1 is a reÔ¨Çection or the identity on
W1, and TW2 is a rotation. DeÔ¨Åning T1 and T2 as in the proof of the corollary
to Theorem 6.47, we have that T = T1T2 is the composite of a rotation and
at most one reÔ¨Çection. (Note that if TW1 is not a reÔ¨Çection, then T1 is the
identity on V and T = T2.)
If m = 3, then V = W1 ‚äïW2 ‚äïW3 and dim(Wi) = 1 for all i. For each
i, let Ti be as in the proof of the corollary to Theorem 6.47. If TWi is not a
reÔ¨Çection, then Ti is the identity on Wi. Otherwise, Ti is a reÔ¨Çection. Since
TWi is a reÔ¨Çection for at most one i, we conclude that T is either a single
reÔ¨Çection or the identity (a rotation).
‚ô¶
EXERCISES
1.
Label the following statements as true or false. Assume that the under-
lying vector spaces are Ô¨Ånite-dimensional real inner product spaces.
(a)
Any orthogonal operator is either a rotation or a reÔ¨Çection.
(b)
The composite of any two rotations on a two-dimensional space is
a rotation.
(c)
The composite of any two rotations on a three-dimensional space
is a rotation.
(d)
The composite of any two rotations on a four-dimensional space is
a rotation.
(e)
The identity operator is a rotation.
(f)
The composite of two reÔ¨Çections is a reÔ¨Çection.
(g)
Any orthogonal operator is a composite of rotations.
(h)
For any orthogonal operator T, if det(T) = ‚àí1, then T is a reÔ¨Çec-
tion.
(i)
ReÔ¨Çections always have eigenvalues.
(j)
Rotations always have eigenvalues.
2.
Prove that rotations, reÔ¨Çections, and composites of rotations and re-
Ô¨Çections are orthogonal operators.

Sec. 6.11
The Geometry of Orthogonal Operators
479
3.
Let
A =
‚éõ
‚éú
‚éú
‚éù
1
2
‚àö
3
2
‚àö
3
2
‚àí1
2
‚éû
‚éü
‚éü
‚é†
and
B =

1
0
0
‚àí1
	
.
(a)
Prove that LA is a reÔ¨Çection.
(b)
Find the axis in R2 about which LA reÔ¨Çects, that is, the subspace
of R2 on which LA acts as the identity.
(c)
Prove that LAB and LBA are rotations.
4.
For any real number œÜ, let
A =

cos œÜ
sin œÜ
sin œÜ
‚àícos œÜ
	
.
(a)
Prove that LA is a reÔ¨Çection.
(b)
Find the axis in R2 about which LA reÔ¨Çects.
5.
For any real number œÜ, deÔ¨Åne TœÜ = LA, where
A =

cos œÜ
‚àísin œÜ
sin œÜ
cos œÜ
	
.
(a)
Prove that any rotation on R2 is of the form TœÜ for some œÜ.
(b)
Prove that TœÜTœà = T(œÜ+œà) for any œÜ, œà ‚ààR.
(c)
Deduce that any two rotations on R2 commute.
6.
Prove that the composite of any two rotations on R3 is a rotation on
R3.
7.
Given real numbers œÜ and œà, deÔ¨Åne matrices
A =
‚éõ
‚éù
1
0
0
0
cos œÜ
‚àísin œÜ
0
sin œÜ
cos œÜ
‚éû
‚é†
and
B =
‚éõ
‚éù
cos œà
‚àísin œà
0
sin œà
cos œà
0
0
0
1
‚éû
‚é†.
(a)
Prove that LA and LB are rotations.
(b)
Prove that LAB is a rotation.
(c)
Find the axis of rotation for LAB.
8.
Prove Theorem 6.45 using the hints preceding the statement of the
theorem.
9.
Prove that no orthogonal operator can be both a rotation and a reÔ¨Çec-
tion.

480
Chap. 6
Inner Product Spaces
10.
Prove that if V is a two- or three-dimensional real inner product space,
then the composite of two reÔ¨Çections on V is a rotation of V.
11.
Give an example of an orthogonal operator that is neither a reÔ¨Çection
nor a rotation.
12.
Let V be a Ô¨Ånite-dimensional real inner product space. DeÔ¨Åne T: V ‚ÜíV
by T(x) = ‚àíx. Prove that T is a product of rotations if and only if
dim(V) is even.
13.
Complete the proof of the lemma to Theorem 6.46 by showing that
W = œÜ‚àí1
Œ≤ (Z) satisÔ¨Åes the required conditions.
14.
Let T be an orthogonal [unitary] operator on a Ô¨Ånite-dimensional real
[complex] inner product space V. If W is a T-invariant subspace of V,
prove the following results.
(a)
TW is an orthogonal [unitary] operator on W.
(b)
W‚ä•is a T-invariant subspace of V. Hint: Use the fact that TW
is one-to-one and onto to conclude that, for any y ‚ààW, T‚àó(y) =
T‚àí1(y) ‚ààW.
(c)
TW‚ä•is an orthogonal [unitary] operator on W.
15.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V, where
V is a direct sum of T-invariant subspaces, say, V = W1‚äïW2‚äï¬∑ ¬∑ ¬∑‚äïWk.
Prove that det(T) = det(TW1)¬∑ det(TW2)¬∑ ¬∑ ¬∑ ¬∑ ¬∑ det(TWk).
16.
Complete the proof of the corollary to Theorem 6.47.
17.
Let T be a linear operator on an n-dimensional real inner product space
V. Suppose that T is not the identity. Prove the following results.
(a)
If n is odd, then T can be expressed as the composite of at most
one reÔ¨Çection and at most 1
2(n ‚àí1) rotations.
(b)
If n is even, then T can be expressed as the composite of at most
1
2n rotations or as the composite of one reÔ¨Çection and at most
1
2(n ‚àí2) rotations.
18.
Let V be a real inner product space of dimension 2. For any x, y ‚ààV
such that x Ã∏= y and ‚à•x‚à•= ‚à•y‚à•= 1, show that there exists a unique
rotation T on V such that T(x) = y.
INDEX OF DEFINITIONS FOR CHAPTER 6
Adjoint of a linear operator
358
Adjoint of a matrix
331
Axis of rotation
473
Bilinear form
422
Complex inner product space
332
Condition number
469

Chap. 6
Index of DeÔ¨Ånitions
481
Congruent matrices
426
Conjugate transpose (adjoint) of a
matrix
331
Critical point
439
Diagonalizable bilinear form
428
Fourier coeÔ¨Écients of a vector rela-
tive to an orthonormal set
348
Frobenius inner product
332
Gram-Schmidt orthogonalization
process
344
Hessian matrix
440
Index of a bilinear form
444
Index of a matrix
445
Inner product
329
Inner product space
332
Invariants of a bilinear form
444
Invariants of a matrix
445
Least squares line
361
Legendre polynomials
346
Local extremum
439
Local maximum
439
Local minimum
439
Lorentz transformation
454
Matrix representation of a bilinear
form
424
Minimal solution of a system of equa-
tions
364
Norm of a matrix
467
Norm of a vector
333
Normal matrix
370
Normal operator
370
Normalizing a vector
335
Orthogonal complement of a subset
of an inner product space
349
Orthogonally equivalent
matrices
384
Orthogonal matrix
382
Orthogonal operator
379
Orthogonal projection
398
Orthogonal projection on a subspace
351
Orthogonal subset of an inner prod-
uct space
335
Orthogonal vectors
335
Orthonormal basis
341
Orthonormal set
335
Penrose conditions
421
Permanent of a 2 √ó 2 matrix
448
Polar decomposition of a matrix
412
Pseudoinverse of a linear transforma-
tion
413
Pseudoinverse of a matrix
414
Quadratic form
433
Rank of a bilinear form
443
Rayleigh quotient
467
Real inner product space
332
ReÔ¨Çection
473
Resolution of the identity operator
induced by a linear transformation
402
Rigid motion
385
Rotation
472
Self-adjoint matrix
373
Self-adjoint operator
373
Signature of a form
444
Signature of a matrix
445
Singular value decomposition of a
matrix
410
Singular value of a linear transforma-
tion
407
Singular value of a matrix
410
Space-time coordinates
453
Spectral decomposition of a linear
operator
402
Spectrum of a linear operator
402
Standard inner product
330
Symmetric bilinear form
428
Translation
386
Trigonometric polynomial
399
Unitarily equivalent matrices
384
Unitary matrix
382
Unitary operator
379
Unit vector
335

7
Canonical Forms
7.1
The Jordan Canonical Form I
7.2
The Jordan Canonical Form II
7.3
The Minimal Polynomial
7.4*
The Rational Canonical Form
As we learned in Chapter 5, the advantage of a diagonalizable linear oper-
ator lies in the simplicity of its description. Such an operator has a diagonal
matrix representation, or, equivalently, there is an ordered basis for the un-
derlying vector space consisting of eigenvectors of the operator. However, not
every linear operator is diagonalizable, even if its characteristic polynomial
splits. Example 3 of Section 5.2 describes such an operator.
It is the purpose of this chapter to consider alternative matrix repre-
sentations for nondiagonalizable operators. These representations are called
canonical forms. There are diÔ¨Äerent kinds of canonical forms, and their ad-
vantages and disadvantages depend on how they are applied. The choice of a
canonical form is determined by the appropriate choice of an ordered basis.
Naturally, the canonical forms of a linear operator are not diagonal matrices
if the linear operator is not diagonalizable.
In this chapter, we treat two common canonical forms. The Ô¨Årst of these,
the Jordan canonical form, requires that the characteristic polynomial of
the operator splits. This form is always available if the underlying Ô¨Åeld is
algebraically closed, that is, if every polynomial with coeÔ¨Écients from the Ô¨Åeld
splits. For example, the Ô¨Åeld of complex numbers is algebraically closed by
the fundamental theorem of algebra (see Appendix D). The Ô¨Årst two sections
deal with this form. The rational canonical form, treated in Section 7.4, does
not require such a factorization.
7.1
THE JORDAN CANONICAL FORM I
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V, and suppose
that the characteristic polynomial of T splits. Recall from Section 5.2 that
the diagonalizability of T depends on whether the union of ordered bases
for the distinct eigenspaces of T is an ordered basis for V.
So a lack of
diagonalizability means that at least one eigenspace of T is too ‚Äúsmall.‚Äù
482

Sec. 7.1
The Jordan Canonical Form I
483
In this section, we extend the deÔ¨Ånition of eigenspace to generalized
eigenspace. From these subspaces, we select ordered bases whose union is
an ordered basis Œ≤ for V such that
[T]Œ≤ =
‚éõ
‚éú
‚éú
‚éú
‚éù
A1
O
¬∑ ¬∑ ¬∑
O
O
A2
¬∑ ¬∑ ¬∑
O
...
...
...
O
O
¬∑ ¬∑ ¬∑
Ak
‚éû
‚éü
‚éü
‚éü
‚é†,
where each O is a zero matrix, and each Ai is a square matrix of the form
(Œª) or
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
Œª
1
0
¬∑ ¬∑ ¬∑
0
0
0
Œª
1
¬∑ ¬∑ ¬∑
0
0
...
...
...
...
...
0
0
0
¬∑ ¬∑ ¬∑
Œª
1
0
0
0
¬∑ ¬∑ ¬∑
0
Œª
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
for some eigenvalue Œª of T.
Such a matrix Ai is called a Jordan block
corresponding to Œª, and the matrix [T]Œ≤ is called a Jordan canonical form
of T. We also say that the ordered basis Œ≤ is a Jordan canonical basis
for T. Observe that each Jordan block Ai is ‚Äúalmost‚Äù a diagonal matrix‚Äîin
fact, [T]Œ≤ is a diagonal matrix if and only if each Ai is of the form (Œª).
Example 1
Suppose that T is a linear operator on C8, and Œ≤ = {v1, v2, . . . , v8} is an
ordered basis for C8 such that
J = [T]Œ≤ =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
2
1
0
0
0
0
0
0
0
2
1
0
0
0
0
0
0
0
2
0
0
0
0
0
0
0
0
2
0
0
0
0
0
0
0
0
3
1
0
0
0
0
0
0
0
3
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
is a Jordan canonical form of T. Notice that the characteristic polynomial
of T is det(J ‚àítI) = (t ‚àí2)4(t ‚àí3)2t2, and hence the multiplicity of each
eigenvalue is the number of times that the eigenvalue appears on the diagonal
of J. Also observe that v1, v4, v5, and v7 are the only vectors in Œ≤ that are
eigenvectors of T. These are the vectors corresponding to the columns of J
with no 1 above the diagonal entry.
‚ô¶

484
Chap. 7
Canonical Forms
In Sections 7.1 and 7.2, we prove that every linear operator whose charac-
teristic polynomial splits has a Jordan canonical form that is unique up to the
order of the Jordan blocks. Nevertheless, it is not the case that the Jordan
canonical form is completely determined by the characteristic polynomial of
the operator. For example, let T‚Ä≤ be the linear operator on C8 such that
[T‚Ä≤]Œ≤ = J‚Ä≤, where Œ≤ is the ordered basis in Example 1 and
J‚Ä≤ =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
2
0
0
0
0
0
0
0
0
2
0
0
0
0
0
0
0
0
2
0
0
0
0
0
0
0
0
2
0
0
0
0
0
0
0
0
3
0
0
0
0
0
0
0
0
3
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
.
Then the characteristic polynomial of T‚Ä≤ is also (t ‚àí2)4(t ‚àí3)2t2. But the
operator T‚Ä≤ has the Jordan canonical form J‚Ä≤, which is diÔ¨Äerent from J, the
Jordan canonical form of the linear operator T of Example 1.
Consider again the matrix J and the ordered basis Œ≤ of Example 1. Notice
that T(v2) = v1+2v2 and therefore, (T‚àí2I)(v2) = v1. Similarly, (T‚àí2I)(v3) =
v2. Since v1 and v4 are eigenvectors of T corresponding to Œª = 2, it follows
that (T ‚àí2I)3(vi) = 0 for i = 1, 2, 3, and 4. Similarly (T ‚àí3I)2(vi) = 0 for
i = 5, 6, and (T ‚àí0I)2(vi) = 0 for i = 7, 8.
Because of the structure of each Jordan block in a Jordan canonical form,
we can generalize these observations: If v lies in a Jordan canonical basis for
a linear operator T and is associated with a Jordan block with diagonal entry
Œª, then (T ‚àíŒªI)p(v) = 0 for suÔ¨Éciently large p. Eigenvectors satisfy this
condition for p = 1.
DeÔ¨Ånition. Let T be a linear operator on a vector space V, and let Œª be
a scalar. A nonzero vector x in V is called a generalized eigenvector of T
corresponding to Œª if (T ‚àíŒªI)p(x) = 0 for some positive integer p.
Notice that if x is a generalized eigenvector of T corresponding to Œª, and p
is the smallest positive integer for which (T‚àíŒªI)p(x) = 0, then (T‚àíŒªI)p‚àí1(x)
is an eigenvector of T corresponding to Œª. Therefore Œª is an eigenvalue of T.
In the context of Example 1, each vector in Œ≤ is a generalized eigenvector
of T. In fact, v1, v2, v3 and v4 correspond to the scalar 2, v5 and v6 correspond
to the scalar 3, and v7 and v8 correspond to the scalar 0.
Just as eigenvectors lie in eigenspaces, generalized eigenvectors lie in ‚Äúgen-
eralized eigenspaces.‚Äù
DeÔ¨Ånition. Let T be a linear operator on a vector space V, and let Œª be
an eigenvalue of T. The generalized eigenspace of T corresponding to

Sec. 7.1
The Jordan Canonical Form I
485
Œª, denoted KŒª, is the subset of V deÔ¨Åned by
KŒª = {x ‚ààV: (T ‚àíŒªI)p(x) = 0 for some positive integer p}.
Note that KŒª consists of the zero vector and all generalized eigenvectors
corresponding to Œª.
Recall that a subspace W of V is T-invariant for a linear operator T if
T(W) ‚äÜW. In the development that follows, we assume the results of Exer-
cises 3 and 4 of Section 5.4. In particular, for any polynomial g(x), if W is
T-invariant, then it is also g(T)-invariant. Furthermore, the range of a linear
operator T is T-invariant.
Theorem 7.1. Let T be a linear operator on a vector space V, and let Œª
be an eigenvalue of T. Then
(a) KŒª is a T-invariant subspace of V containing EŒª (the eigenspace of T
corresponding to Œª).
(b) For any scalar Œº Ã∏= Œª, the restriction of T ‚àíŒºI to KŒª is one-to-one.
Proof. (a) Clearly, 0 ‚ààKŒª. Suppose that x and y are in KŒª. Then there
exist positive integers p and q such that
(T ‚àíŒªI)p(x) = (T ‚àíŒªI)q(y) = 0.
Therefore
(T ‚àíŒªI)p+q(x + y) = (T ‚àíŒªI)p+q(x) + (T ‚àíŒªI)p+q(y)
= (T ‚àíŒªI)q(0) + (T ‚àíŒªI)p(0)
= 0,
and hence x+y ‚ààKŒª. The proof that KŒª is closed under scalar multiplication
is straightforward.
To show that KŒª is T-invariant, consider any x ‚ààKŒª. Choose a positive
integer p such that (T ‚àíŒªI)p(x) = 0. Then
(T ‚àíŒªI)pT(x) = T(T ‚àíŒªI)p(x) = T(0) = 0.
Therefore T(x) ‚ààKŒª.
Finally, it is a simple observation that EŒª is contained in KŒª.
(b) Let x ‚ààKŒª and (T ‚àíŒºI)(x) = 0. By way of contradiction, suppose
that x Ã∏= 0. Let p be the smallest integer for which (T ‚àíŒªI)p(x) = 0, and let
y = (T ‚àíŒªI)p‚àí1(x). Then
(T ‚àíŒªI)(y) = (T ‚àíŒªI)p(x) = 0,
and hence y ‚ààEŒª. Furthermore,
(T ‚àíŒºI)(y) = (T ‚àíŒºI)(T ‚àíŒªI)p‚àí1(x) = (T ‚àíŒªI)p‚àí1(T ‚àíŒºI)(x) = 0,
so that y ‚ààEŒº.
But EŒª ‚à©EŒº = {0}, and thus y = 0, contrary to the
hypothesis. So x = 0, and the restriction of T ‚àíŒºI to KŒª is one-to-one.

486
Chap. 7
Canonical Forms
Theorem 7.2. Let T be a linear operator on a Ô¨Ånite-dimensional vector
space V such that the characteristic polynomial of T splits. Suppose that Œª
is an eigenvalue of T with multiplicity m. Then
(a) dim(KŒª) ‚â§m.
(b) KŒª = N((T ‚àíŒªI)m).
Proof. (a) Let W = KŒª, and let h(t) be the characteristic polynomial of TW.
By Theorem 5.21 (p. 314), h(t) divides the characteristic polynomial of T, and
by Theorem 7.1(b), Œª is the only eigenvalue of TW. Hence h(t) = (‚àí1)d(t‚àíŒª)d,
where d = dim(W), and d ‚â§m.
(b) Clearly N((T ‚àíŒªI)m) ‚äÜKŒª. Now let W and h(t) be as in (a). Then
h(TW) is identically zero by the Cayley‚ÄìHamilton theorem (p. 317); therefore
(T‚àíŒªI)d(x) = 0 for all x ‚ààW. Since d ‚â§m, we have KŒª ‚äÜN((T‚àíŒªI)m).
Theorem 7.3. Let T be a linear operator on a Ô¨Ånite-dimensional vec-
tor space V such that the characteristic polynomial of T splits, and let
Œª1, Œª2, . . . , Œªk be the distinct eigenvalues of T. Then, for every x ‚ààV, there
exist vectors vi ‚ààKŒªi, 1 ‚â§i ‚â§k, such that
x = v1 + v2 + ¬∑ ¬∑ ¬∑ + vk.
Proof. The proof is by mathematical induction on the number k of dis-
tinct eigenvalues of T. First suppose that k = 1, and let m be the multiplic-
ity of Œª1. Then (Œª1 ‚àít)m is the characteristic polynomial of T, and hence
(Œª1I ‚àíT)m = T0 by the Cayley-Hamilton theorem (p.317). Thus V = KŒª1,
and the result follows.
Now suppose that for some integer k > 1, the result is established when-
ever T has fewer than k distinct eigenvalues, and suppose that T has k distinct
eigenvalues. Let m be the multiplicity of Œªk, and let f(t) be the characteristic
polynomial of T. Then f(t) = (t ‚àíŒªk)mg(t) for some polynomial g(t) not
divisible by (t ‚àíŒªk). Let W = R((T ‚àíŒªkI)m). Clearly W is T-invariant.
Observe that (T ‚àíŒªkI)m maps KŒªi onto itself for i < k. For suppose that
i < k. Since (T ‚àíŒªkI)m maps KŒªi into itself and Œªk Ã∏= Œªi, the restriction
of T ‚àíŒªkI to KŒªi is one-to-one (by Theorem 7.1(b)) and hence is onto. One
consequence of this is that for i < k, KŒªi is contained in W; hence Œªi is an
eigenvalue of TW for i < k.
Next, observe that Œªk is not an eigenvalue of TW. For suppose that T(v) =
Œªkv for some v ‚ààW. Then v = (T ‚àíŒªkI)m(y) for some y ‚ààV, and it follows
that
0 = (T ‚àíŒªkI)(v) = (T ‚àíŒªkI)m+1(y).
Therefore y ‚ààKŒªk. So by Theorem 7.2, v = (T ‚àíŒªkI)m(y) = 0.
Since every eigenvalue of TW is an eigenvalue of T, the distinct eigenvalues
of TW are Œª1, Œª2, . . . , Œªk‚àí1.

Sec. 7.1
The Jordan Canonical Form I
487
Now let x ‚ààV. Then (T ‚àíŒªkI)m(x) ‚ààW. Since TW has the k ‚àí1 distinct
eigenvalues Œª1, Œª2, . . . , Œªk‚àí1, the induction hypothesis applies. Hence there
are vectors wi ‚ààK‚Ä≤Œªi, 1 ‚â§i ‚â§k ‚àí1, such that
(T ‚àíŒªkI)m(x) = w1 + w2 + ¬∑ ¬∑ ¬∑ + wk‚àí1.
Since K‚Ä≤Œªi ‚äÜKŒªi for i < k and (T ‚àíŒªkI)m maps KŒªi onto itself for i < k,
there exist vectors vi ‚ààKŒªi such that (T ‚àíŒªkI)m(vi) = wi for i < k. Thus
we have
(T ‚àíŒªkI)m(x) = (T ‚àíŒªkI)m(v1) + (T ‚àíŒªkI)m(v2) + ¬∑ ¬∑ ¬∑ + (T ‚àíŒªkI)m(vk‚àí1),
and it follows that x ‚àí(v1 + v2 + ¬∑ ¬∑ ¬∑ + vk‚àí1) ‚ààKŒªk. Therefore there exists a
vector vk ‚ààKŒªk such that
x = v1 + v2 + ¬∑ ¬∑ ¬∑ + vk.
The next result extends Theorem 5.9(b) (p. 268) to all linear operators
whose characteristic polynomials split. In this case, the eigenspaces are re-
placed by generalized eigenspaces.
Theorem 7.4. Let T be a linear operator on a Ô¨Ånite-dimensional vec-
tor space V such that the characteristic polynomial of T splits, and let
Œª1, Œª2, . . . , Œªk be the distinct eigenvalues of T with corresponding multiplici-
ties m1, m2, . . . , mk. For 1 ‚â§i ‚â§k, let Œ≤i be an ordered basis for KŒªi. Then
the following statements are true.
(a) Œ≤i ‚à©Œ≤j = ‚àÖfor i Ã∏= j.
(b) Œ≤ = Œ≤1 ‚à™Œ≤2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≤k is an ordered basis for V.
(c) dim(KŒªi) = mi for all i.
Proof. (a) Suppose that x ‚ààŒ≤i ‚à©Œ≤j ‚äÜKŒªi ‚à©KŒªj, where i Ã∏= j.
By
Theorem 7.1(b), T‚àíŒªiI is one-to-one on KŒªj, and therefore (T‚àíŒªiI)p(x) Ã∏= 0
for any positive integer p. But this contradicts the fact that x ‚ààKŒªi, and the
result follows.
(b) Let x ‚ààV. By Theorem 7.3, for 1 ‚â§i ‚â§k, there exist vectors vi ‚ààKŒªi
such that x = v1 + v2 + ¬∑ ¬∑ ¬∑ + vk. Since each vi is a linear combination of
the vectors of Œ≤i, it follows that x is a linear combination of the vectors of Œ≤.
Therefore Œ≤ spans V. Let q be the number of vectors in Œ≤. Then dim V ‚â§q.
For each i, let di = dim(KŒªi). Then, by Theorem 7.2(a),
q =
k

i=1
di ‚â§
k

i=1
mi = dim(V).
Hence q = dim(V). Consequently Œ≤ is a basis for V by Corollary 2 to the
replacement theorem (p. 47).

488
Chap. 7
Canonical Forms
(c) Using the notation and result of (b), we see that
k

i=1
di =
k

i=1
mi. But
di ‚â§mi by Theorem 7.2(a), and therefore di = mi for all i.
Corollary. Let T be a linear operator on a Ô¨Ånite-dimensional vector space
V such that the characteristic polynomial of T splits. Then T is diagonalizable
if and only if EŒª = KŒª for every eigenvalue Œª of T.
Proof. Combining Theorems 7.4 and 5.9(a) (p. 268), we see that T is
diagonalizable if and only if dim(EŒª) = dim(KŒª) for each eigenvalue Œª of T.
But EŒª ‚äÜKŒª, and hence these subspaces have the same dimension if and only
if they are equal.
We now focus our attention on the problem of selecting suitable bases for
the generalized eigenspaces of a linear operator so that we may use Theo-
rem 7.4 to obtain a Jordan canonical basis for the operator. For this purpose,
we consider again the basis Œ≤ of Example 1. We have seen that the Ô¨Årst four
vectors of Œ≤ lie in the generalized eigenspace K2. Observe that the vectors in
Œ≤ that determine the Ô¨Årst Jordan block of J are of the form
{v1, v2, v3} = {(T ‚àí2I)2(v3), (T ‚àí2I)(v3), v3}.
Furthermore, observe that (T‚àí2I)3(v3) = 0. The relation between these vec-
tors is the key to Ô¨Ånding Jordan canonical bases. This leads to the following
deÔ¨Ånitions.
DeÔ¨Ånitions.
Let T be a linear operator on a vector space V, and let x
be a generalized eigenvector of T corresponding to the eigenvalue Œª. Suppose
that p is the smallest positive integer for which (T ‚àíŒªI)p(x) = 0. Then the
ordered set
{(T ‚àíŒªI)p‚àí1(x), (T ‚àíŒªI)p‚àí2(x), . . . , (T ‚àíŒªI)(x), x}
is called a cycle of generalized eigenvectors of T corresponding to Œª.
The vectors (T ‚àíŒªI)p‚àí1(x) and x are called the initial vector and the end
vector of the cycle, respectively. We say that the length of the cycle is p.
Notice that the initial vector of a cycle of generalized eigenvectors of a
linear operator T is the only eigenvector of T in the cycle. Also observe that
if x is an eigenvector of T corresponding to the eigenvalue Œª, then the set {x}
is a cycle of generalized eigenvectors of T corresponding to Œª of length 1.
In Example 1, the subsets Œ≤1 = {v1, v2, v3}, Œ≤2 = {v4}, Œ≤3 = {v5, v6},
and Œ≤4 = {v7, v8} are the cycles of generalized eigenvectors of T that occur
in Œ≤. Notice that Œ≤ is a disjoint union of these cycles. Furthermore, setting
Wi = span(Œ≤i) for 1 ‚â§i ‚â§4, we see that Œ≤i is a basis for Wi and [TWi]Œ≤i is
the ith Jordan block of the Jordan canonical form of T. This is precisely the
condition that is required for a Jordan canonical basis.

Sec. 7.1
The Jordan Canonical Form I
489
Theorem 7.5. Let T be a linear operator on a Ô¨Ånite-dimensional vector
space V whose characteristic polynomial splits, and suppose that Œ≤ is a basis
for V such that Œ≤ is a disjoint union of cycles of generalized eigenvectors of
T. Then the following statements are true.
(a) For each cycle Œ≥ of generalized eigenvectors contained in Œ≤, W = span(Œ≥)
is T-invariant, and [TW]Œ≥ is a Jordan block.
(b) Œ≤ is a Jordan canonical basis for V.
Proof. (a) Suppose that Œ≥ corresponds to Œª, Œ≥ has length p, and x is the
end vector of Œ≥. Then Œ≥ = {v1, v2, . . . , vp}, where
vi = (T ‚àíŒªI)p‚àíi(x) for i < p
and
vp = x.
So
(T ‚àíŒªI)(v1) = (T ‚àíŒªI)p(x) = 0,
and hence T(v1) = Œªv1. For i > 1,
(T ‚àíŒªI)(vi) = (T ‚àíŒªI)p‚àí(i‚àí1)(x) = vi‚àí1.
Therefore T maps W into itself, and, by the preceding equations, we see that
[TW]Œ≥ is a Jordan block.
For (b), simply repeat the arguments of (a) for each cycle in Œ≤ in order to
obtain [T]Œ≤. We leave the details as an exercise.
In view of this result, we must show that, under appropriate conditions,
there exist bases that are disjoint unions of cycles of generalized eigenvectors.
Since the characteristic polynomial of a Jordan canonical form splits, this is
a necessary condition. We will soon see that it is also suÔ¨Écient. The next
result moves us toward the desired existence theorem.
Theorem 7.6. Let T be a linear operator on a vector space V, and let
Œª be an eigenvalue of T. Suppose that Œ≥1, Œ≥2, . . . , Œ≥q are cycles of generalized
eigenvectors of T corresponding to Œª such that the initial vectors of the Œ≥i‚Äôs
are distinct and form a linearly independent set. Then the Œ≥i‚Äôs are disjoint,
and their union Œ≥ =
q-
i=1
Œ≥i is linearly independent.
Proof. Exercise 5 shows that the Œ≥i‚Äôs are disjoint.
The proof that Œ≥ is linearly independent is by mathematical induction on
the number of vectors in Œ≥. If this number is less than 2, then the result is
clear. So assume that, for some integer n > 1, the result is valid whenever Œ≥
has fewer than n vectors, and suppose that Œ≥ has exactly n vectors. Let W
be the subspace of V generated by Œ≥. Clearly W is (T ‚àíŒªI)-invariant, and
dim(W) ‚â§n. Let U denote the restriction of T ‚àíŒªI to W.

490
Chap. 7
Canonical Forms
For each i, let Œ≥‚Ä≤
i denote the cycle obtained from Œ≥i by deleting the end
vector. Note that if Œ≥i has length one, then Œ≥‚Ä≤
i = ‚àÖ. In the case that Œ≥‚Ä≤
i Ã∏= ‚àÖ,
each vector of Œ≥‚Ä≤
i is the image under U of a vector in Œ≥i, and conversely, every
nonzero image under U of a vector of Œ≥i is contained in Œ≥‚Ä≤
i. Let Œ≥‚Ä≤ =
-
i
Œ≥‚Ä≤
i.
Then by the last statement, Œ≥‚Ä≤ generates R(U). Furthermore, Œ≥‚Ä≤ consists of
n ‚àíq vectors, and the initial vectors of the Œ≥‚Ä≤
i‚Äôs are also initial vectors of
the Œ≥i‚Äôs. Thus we may apply the induction hypothesis to conclude that Œ≥‚Ä≤ is
linearly independent. Therefore Œ≥‚Ä≤ is a basis for R(U). Hence dim(R(U)) =
n ‚àíq. Since the q initial vectors of the Œ≥i‚Äôs form a linearly independent set
and lie in N(U), we have dim(N(U)) ‚â•q. From these inequalities and the
dimension theorem, we obtain
n ‚â•dim(W)
= dim(R(U)) + dim(N(U))
‚â•(n ‚àíq) + q
= n.
We conclude that dim(W) = n. Since Œ≥ generates W and consists of n vectors,
it must be a basis for W. Hence Œ≥ is linearly independent.
Corollary. Every cycle of generalized eigenvectors of a linear operator is
linearly independent.
Theorem 7.7. Let T be a linear operator on a Ô¨Ånite-dimensional vector
space V, and let Œª be an eigenvalue of T. Then KŒª has an ordered basis con-
sisting of a union of disjoint cycles of generalized eigenvectors corresponding
to Œª.
Proof. The proof is by mathematical induction on n = dim(KŒª).
The
result is clear for n = 1. So suppose that for some integer n > 1 the result is
valid whenever dim(KŒª) < n, and assume that dim(KŒª) = n. Let U denote the
restriction of T‚àíŒªI to KŒª. Then R(U) is a subspace of KŒª of lesser dimension,
and R(U) is the space of generalized eigenvectors corresponding to Œª for the
restriction of T to R(U). Therefore, by the induction hypothesis, there exist
disjoint cycles Œ≥1, Œ≥2, . . . , Œ≥q of generalized eigenvectors of this restriction, and
hence of T itself, corresponding to Œª for which Œ≥ =
q-
i=1
Œ≥i is a basis for R(U).
For 1 ‚â§i ‚â§q, the end vector of Œ≥i is the image under U of a vector vi ‚ààKŒª,
and so we can extend each Œ≥i to a larger cycle ÀúŒ≥i = Œ≥i ‚à™{vi} of generalized
eigenvectors of T corresponding to Œª. For 1 ‚â§i ‚â§q, let wi be the initial vector
of ÀúŒ≥i (and hence of Œ≥i). Since {w1, w2, . . . , wq} is a linearly independent sub-
set of EŒª, this set can be extended to a basis {w1, w2, . . . , wq, u1, u2, . . . , us}

Sec. 7.1
The Jordan Canonical Form I
491
for EŒª. Then ÀúŒ≥1, ÀúŒ≥2, . . . , ÀúŒ≥q, {u1}, {u2}, . . . , {us} are disjoint cycles of gener-
alized eigenvectors of T corresponding to Œª such that the initial vectors of
these cycles are linearly independent. Therefore their union ÀúŒ≥ is a linearly
independent subset of KŒª by Theorem 7.6.
We show that ÀúŒ≥ is a basis for KŒª.
Suppose that Œ≥ consists of r =
rank(U) vectors. Then ÀúŒ≥ consists of r + q + s vectors. Furthermore, since
{w1, w2, . . . , wq, u1, u2, . . . , us} is a basis for EŒª = N(U), it follows that
nullity(U) = q + s. Therefore
dim(KŒª) = rank(U) + nullity(U) = r + q + s.
So ÀúŒ≥ is a linearly independent subset of KŒª containing dim(KŒª) vectors. It
follows that ÀúŒ≥ is a basis for KŒª.
The following corollary is immediate.
Corollary 1.
Let T be a linear operator on a Ô¨Ånite-dimensional vec-
tor space V whose characteristic polynomial splits.
Then T has a Jordan
canonical form.
Proof. Let Œª1, Œª2, . . . , Œªk be the distinct eigenvalues of T. By Theorem 7.7,
for each i there is an ordered basis Œ≤i consisting of a disjoint union of cycles
of generalized eigenvectors corresponding to Œªi. Let Œ≤ = Œ≤1 ‚à™Œ≤2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≤k.
Then, by Theorem 7.4(b), Œ≤ is an ordered basis for V.
The Jordan canonical form also can be studied from the viewpoint of
matrices.
DeÔ¨Ånition. Let A ‚ààMn√ón(F) be such that the characteristic polynomial
of A (and hence of LA) splits. Then the Jordan canonical form of A is
deÔ¨Åned to be the Jordan canonical form of the linear operator LA on Fn.
The next result is an immediate consequence of this deÔ¨Ånition and Corol-
lary 1.
Corollary 2. Let A be an n √ó n matrix whose characteristic polynomial
splits. Then A has a Jordan canonical form J, and A is similar to J.
Proof. Exercise.
We can now compute the Jordan canonical forms of matrices and linear
operators in some simple cases, as is illustrated in the next two examples.
The tools necessary for computing the Jordan canonical forms in general are
developed in the next section.

492
Chap. 7
Canonical Forms
Example 2
Let
A =
‚éõ
‚éù
3
1
‚àí2
‚àí1
0
5
‚àí1
‚àí1
4
‚éû
‚é†‚ààM3√ó3(R).
To Ô¨Ånd the Jordan canonical form for A, we need to Ô¨Ånd a Jordan canonical
basis for T = LA.
The characteristic polynomial of A is
f(t) = det(A ‚àítI) = ‚àí(t ‚àí3)(t ‚àí2)2.
Hence Œª1 = 3 and Œª2 = 2 are the eigenvalues of A with multiplicities 1
and 2, respectively. By Theorem 7.4, dim(KŒª1) = 1, and dim(KŒª2) = 2. By
Theorem 7.2, KŒª1 = N(T‚àí3I), and KŒª2 = N((T‚àí2I)2). Since EŒª1 = N(T‚àí3I),
we have that EŒª1 = KŒª1.
Observe that (‚àí1, 2, 1) is an eigenvector of T
corresponding to Œª1 = 3; therefore
Œ≤1 =
‚éß
‚é®
‚é©
‚éõ
‚éù
‚àí1
2
1
‚éû
‚é†
‚é´
‚é¨
‚é≠
is a basis for KŒª1.
Since dim(KŒª2) = 2 and a generalized eigenspace has a basis consisting of
a union of cycles, this basis is either a union of two cycles of length 1 or a
single cycle of length 2. The former case is impossible because the vectors in
the basis would be eigenvectors‚Äîcontradicting the fact that dim(EŒª2) = 1.
Therefore the desired basis is a single cycle of length 2. A vector v is the end
vector of such a cycle if and only if (A ‚àí2I)v Ã∏= 0, but (A ‚àí2I)2v = 0. It
can easily be shown that
‚éß
‚é®
‚é©
‚éõ
‚éù
1
‚àí3
‚àí1
‚éû
‚é†,
‚éõ
‚éù
‚àí1
2
0
‚éû
‚é†
‚é´
‚é¨
‚é≠
is a basis for the solution space of the homogeneous system (A ‚àí2I)2x = 0.
Now choose a vector v in this set so that (A ‚àí2I)v Ã∏= 0. The vector v =
(‚àí1, 2, 0) is an acceptable candidate for v. Since (A ‚àí2I)v = (1, ‚àí3, ‚àí1), we
obtain the cycle of generalized eigenvectors
Œ≤2 = {(A ‚àí2I)v, v} =
‚éß
‚é®
‚é©
‚éõ
‚éù
1
‚àí3
‚àí1
‚éû
‚é†,
‚éõ
‚éù
‚àí1
2
0
‚éû
‚é†
‚é´
‚é¨
‚é≠

Sec. 7.1
The Jordan Canonical Form I
493
as a basis for KŒª2. Finally, we take the union of these two bases to obtain
Œ≤ = Œ≤1 ‚à™Œ≤2 =
‚éß
‚é®
‚é©
‚éõ
‚éù
‚àí1
2
1
‚éû
‚é†,
‚éõ
‚éù
1
‚àí3
‚àí1
‚éû
‚é†,
‚éõ
‚éù
‚àí1
2
0
‚éû
‚é†
‚é´
‚é¨
‚é≠,
which is a Jordan canonical basis for A. Therefore,
J = [T]Œ≤ =
‚éõ
‚éù
3
0
0
0
2
1
0
0
2
‚éû
‚é†
is a Jordan canonical form for A. Notice that A is similar to J. In fact,
J = Q‚àí1AQ, where Q is the matrix whose columns are the vectors in Œ≤.
‚ô¶
Example 3
Let T be the linear operator on P2(R) deÔ¨Åned by T(g(x)) = ‚àíg(x) ‚àíg‚Ä≤(x).
We Ô¨Ånd a Jordan canonical form of T and a Jordan canonical basis for T.
Let Œ≤ be the standard ordered basis for P2(R). Then
[T]Œ≤ =
‚éõ
‚éù
‚àí1
‚àí1
0
0
‚àí1
‚àí2
0
0
‚àí1
‚éû
‚é†,
which has the characteristic polynomial f(t) = ‚àí(t + 1)3. Thus Œª = ‚àí1 is
the only eigenvalue of T, and hence KŒª = P2(R) by Theorem 7.4. So Œ≤ is a
basis for KŒª. Now
dim(EŒª) = 3 ‚àírank(A + I) = 3 ‚àírank
‚éõ
‚éù
0
‚àí1
0
0
0
‚àí2
0
0
0
‚éû
‚é†= 3 ‚àí2 = 1.
Therefore a basis for KŒª cannot be a union of two or three cycles because
the initial vector of each cycle is an eigenvector, and there do not exist two
or more linearly independent eigenvectors. So the desired basis must consist
of a single cycle of length 3. If Œ≥ is such a cycle, then Œ≥ determines a single
Jordan block
[T]Œ≥ =
‚éõ
‚éù
‚àí1
1
0
0
‚àí1
1
0
0
‚àí1
‚éû
‚é†,
which is a Jordan canonical form of T.
The end vector h(x) of such a cycle must satisfy (T + I)2(h(x)) Ã∏= 0. In
any basis for KŒª, there must be a vector that satisÔ¨Åes this condition, or else

494
Chap. 7
Canonical Forms
no vector in KŒª satisÔ¨Åes this condition, contrary to our reasoning. Testing
the vectors in Œ≤, we see that h(x) = x2 is acceptable. Therefore
Œ≥ = {(T + I)2(x2), (T + I)(x2), x2} = {2, ‚àí2x, x2}
is a Jordan canonical basis for T.
‚ô¶
In the next section, we develop a computational approach for Ô¨Ånding a
Jordan canonical form and a Jordan canonical basis. In the process, we prove
that Jordan canonical forms are unique up to the order of the Jordan blocks.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V, and sup-
pose that the characteristic polynomial of T splits. By Theorem 5.11 (p. 278),
T is diagonalizable if and only if V is the direct sum of the eigenspaces of T.
If T is diagonalizable, then the eigenspaces and the generalized eigenspaces
coincide. The next result, which is optional, extends Theorem 5.11 to the
nondiagonalizable case.
Theorem 7.8. Let T be a linear operator on a Ô¨Ånite-dimensional vector
space V whose characteristic polynomial splits. Then V is the direct sum of
the generalized eigenspaces of T.
Proof. Exercise.
EXERCISES
1.
Label the following statements as true or false.
(a)
Eigenvectors of a linear operator T are also generalized eigenvec-
tors of T.
(b)
It is possible for a generalized eigenvector of a linear operator T
to correspond to a scalar that is not an eigenvalue of T.
(c)
Any linear operator on a Ô¨Ånite-dimensional vector space has a Jor-
dan canonical form.
(d)
A cycle of generalized eigenvectors is linearly independent.
(e)
There is exactly one cycle of generalized eigenvectors correspond-
ing to each eigenvalue of a linear operator on a Ô¨Ånite-dimensional
vector space.
(f)
Let T be a linear operator on a Ô¨Ånite-dimensional vector space
whose characteristic polynomial splits, and let Œª1, Œª2, . . . , Œªk be
the distinct eigenvalues of T. If, for each i, Œ≤i is a basis for KŒªi,
then Œ≤1 ‚à™Œ≤2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≤k is a Jordan canonical basis for T.
(g)
For any Jordan block J, the operator LJ has Jordan canonical
form J.
(h)
Let T be a linear operator on an n-dimensional vector space whose
characteristic polynomial splits. Then, for any eigenvalue Œª of T,
KŒª = N((T ‚àíŒªI)n).

Sec. 7.1
The Jordan Canonical Form I
495
2.
For each matrix A, Ô¨Ånd a basis for each generalized eigenspace of LA
consisting of a union of disjoint cycles of generalized eigenvectors. Then
Ô¨Ånd a Jordan canonical form J of A.
(a) A =

1
1
‚àí1
3
	
(b) A =

1
2
3
2
	
(c) A =
‚éõ
‚éù
11
‚àí4
‚àí5
21
‚àí8
‚àí11
3
‚àí1
0
‚éû
‚é†
(d) A =
‚éõ
‚éú
‚éú
‚éù
2
1
0
0
0
2
1
0
0
0
3
0
0
1
‚àí1
3
‚éû
‚éü
‚éü
‚é†
3.
For each linear operator T, Ô¨Ånd a basis for each generalized eigenspace
of T consisting of a union of disjoint cycles of generalized eigenvectors.
Then Ô¨Ånd a Jordan canonical form J of T.
(a)
T is the linear operator on P2(R) deÔ¨Åned by T(f(x)) = 2f(x) ‚àí
f ‚Ä≤(x)
(b)
V is the real vector space of functions spanned by the set of real
valued functions {1, t, t2, et, tet}, and T is the linear operator on V
deÔ¨Åned by T(f) = f ‚Ä≤.
(c)
T is the linear operator on M2√ó2(R) deÔ¨Åned by T(A) =

1
1
0
1
	
¬∑A
for all A ‚ààM2√ó2(R).
(d)
T(A) = 2A + At for all A ‚ààM2√ó2(R).
4.‚Ä† Let T be a linear operator on a vector space V, and let Œ≥ be a cycle
of generalized eigenvectors that corresponds to the eigenvalue Œª. Prove
that span(Œ≥) is a T-invariant subspace of V.
5.
Let Œ≥1, Œ≥2, . . . , Œ≥p be cycles of generalized eigenvectors of a linear op-
erator T corresponding to an eigenvalue Œª.
Prove that if the initial
eigenvectors are distinct, then the cycles are disjoint.
6.
Let T: V ‚ÜíW be a linear transformation. Prove the following results.
(a)
N(T) = N(‚àíT).
(b)
N(Tk) = N((‚àíT)k).
(c)
If V = W (so that T is a linear operator on V) and Œª is an eigen-
value of T, then for any positive integer k
N((T ‚àíŒªIV)k) = N((ŒªIV ‚àíT)k).
7.
Let U be a linear operator on a Ô¨Ånite-dimensional vector space V. Prove
the following results.
(a)
N(U) ‚äÜN(U2) ‚äÜ¬∑ ¬∑ ¬∑ ‚äÜN(Uk) ‚äÜN(Uk+1) ‚äÜ¬∑ ¬∑ ¬∑ .

496
Chap. 7
Canonical Forms
(b)
If rank(Um) = rank(Um+1) for some positive integer m, then
rank(Um) = rank(Uk) for any positive integer k ‚â•m.
(c)
If rank(Um) = rank(Um+1) for some positive integer m, then
N(Um) = N(Uk) for any positive integer k ‚â•m.
(d)
Let T be a linear operator on V, and let Œª be an eigenvalue of T.
Prove that if rank((T‚àíŒªI)m) = rank((T‚àíŒªI)m+1) for some integer
m, then KŒª = N((T ‚àíŒªI)m).
(e)
Second Test for Diagonalizability. Let T be a linear operator on
V whose characteristic polynomial splits, and let Œª1, Œª2, . . . , Œªk be
the distinct eigenvalues of T. Then T is diagonalizable if and only
if rank(T ‚àíŒªI) = rank((T ‚àíŒªI)2) for 1 ‚â§i ‚â§k.
(f)
Use (e) to obtain a simpler proof of Exercise 24 of Section 5.4: If
T is a diagonalizable linear operator on a Ô¨Ånite-dimensional vec-
tor space V and W is a T-invariant subspace of V, then TW is
diagonalizable.
8.
Use Theorem 7.4 to prove that the vectors v1, v2, . . . , vk in the statement
of Theorem 7.3 are unique.
9.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V whose
characteristic polynomial splits.
(a)
Prove Theorem 7.5(b).
(b)
Suppose that Œ≤ is a Jordan canonical basis for T, and let Œª be an
eigenvalue of T. Let Œ≤‚Ä≤ = Œ≤ ‚à©KŒª. Prove that Œ≤‚Ä≤ is a basis for KŒª.
10.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space whose
characteristic polynomial splits, and let Œª be an eigenvalue of T.
(a)
Suppose that Œ≥ is a basis for KŒª consisting of the union of q disjoint
cycles of generalized eigenvectors. Prove that q ‚â§dim(EŒª).
(b)
Let Œ≤ be a Jordan canonical basis for T, and suppose that J = [T]Œ≤
has q Jordan blocks with Œª in the diagonal positions. Prove that
q ‚â§dim(EŒª).
11.
Prove Corollary 2 to Theorem 7.7.
Exercises 12 and 13 are concerned with direct sums of matrices, deÔ¨Åned in
Section 5.4 on page 320.
12.
Prove Theorem 7.8.
13.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V such
that the characteristic polynomial of T splits, and let Œª1, Œª2, . . . , Œªk be
the distinct eigenvalues of T. For each i, let Ji be the Jordan canonical
form of the restriction of T to KŒªi. Prove that
J = J1 ‚äïJ2 ‚äï¬∑ ¬∑ ¬∑ ‚äïJk
is the Jordan canonical form of J.

Sec. 7.2
The Jordan Canonical Form II
497
7.2
THE JORDAN CANONICAL FORM II
For the purposes of this section, we Ô¨Åx a linear operator T on an n-dimensional
vector space V such that the characteristic polynomial of T splits.
Let
Œª1, Œª2, . . . , Œªk be the distinct eigenvalues of T.
By Theorem 7.7 (p. 490), each generalized eigenspace KŒªi contains an
ordered basis Œ≤i consisting of a union of disjoint cycles of generalized eigen-
vectors corresponding to Œªi. So by Theorems 7.4(b) (p. 487) and 7.5 (p. 489),
the union Œ≤ =
k-
i=1
Œ≤i is a Jordan canonical basis for T. For each i, let Ti
be the restriction of T to KŒªi, and let Ai = [Ti]Œ≤i. Then Ai is the Jordan
canonical form of Ti, and
J = [T]Œ≤ =
‚éõ
‚éú
‚éú
‚éú
‚éù
A1
O
¬∑ ¬∑ ¬∑
O
O
A2
¬∑ ¬∑ ¬∑
O
...
...
...
O
O
¬∑ ¬∑ ¬∑
Ak
‚éû
‚éü
‚éü
‚éü
‚é†
is the Jordan canonical form of T. In this matrix, each O is a zero matrix of
appropriate size.
In this section, we compute the matrices Ai and the bases Œ≤i, thereby
computing J and Œ≤ as well.
While developing a method for Ô¨Ånding J, it
becomes evident that in some sense the matrices Ai are unique.
To aid in formulating the uniqueness theorem for J, we adopt the following
convention: The basis Œ≤i for KŒªi will henceforth be ordered in such a way
that the cycles appear in order of decreasing length. That is, if Œ≤i is a disjoint
union of cycles Œ≥1, Œ≥2, . . . , Œ≥ni and if the length of the cycle Œ≥j is pj, we index
the cycles so that p1 ‚â•p2 ‚â•¬∑ ¬∑ ¬∑ ‚â•pni. This ordering of the cycles limits the
possible orderings of vectors in Œ≤i, which in turn determines the matrix Ai.
It is in this sense that Ai is unique. It then follows that the Jordan canonical
form for T is unique up to an ordering of the eigenvalues of T. As we will
see, there is no uniqueness theorem for the bases Œ≤i or for Œ≤. SpeciÔ¨Åcally, we
show that for each i, the number ni of cycles that form Œ≤i, and the length pj
(j = 1, 2, . . . , ni) of each cycle, is completely determined by T.
Example 1
To illustrate the discussion above, suppose that, for some i, the ordered basis
Œ≤i for KŒªi is the union of four cycles Œ≤i = Œ≥1 ‚à™Œ≥2 ‚à™Œ≥3 ‚à™Œ≥4 with respective

498
Chap. 7
Canonical Forms
lengths p1 = 3, p2 = 3, p3 = 2, and p4 = 1. Then
Ai =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
Œªi
1
0
0
0
0
0
0
0
0
Œªi
1
0
0
0
0
0
0
0
0
Œªi
0
0
0
0
0
0
0
0
0
Œªi
1
0
0
0
0
0
0
0
0
Œªi
1
0
0
0
0
0
0
0
0
Œªi
0
0
0
0
0
0
0
0
0
Œªi
1
0
0
0
0
0
0
0
0
Œªi
0
0
0
0
0
0
0
0
0
Œªi
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
.
‚ô¶
To help us visualize each of the matrices Ai and ordered bases Œ≤i, we
use an array of dots called a dot diagram of Ti, where Ti is the restriction
of T to KŒªi.
Suppose that Œ≤i is a disjoint union of cycles of generalized
eigenvectors Œ≥1, Œ≥2, . . . , Œ≥ni with lengths p1 ‚â•p2 ‚â•¬∑ ¬∑ ¬∑ ‚â•pni, respectively.
The dot diagram of Ti contains one dot for each vector in Œ≤i, and the dots
are conÔ¨Ågured according to the following rules.
1. The array consists of ni columns (one column for each cycle).
2. Counting from left to right, the jth column consists of the pj dots that
correspond to the vectors of Œ≥j starting with the initial vector at the
top and continuing down to the end vector.
Denote the end vectors of the cycles by v1, v2, . . . , vni. In the following
dot diagram of Ti, each dot is labeled with the name of the vector in Œ≤i to
which it corresponds.
‚Ä¢ (T ‚àíŒªiI)p1‚àí1(v1)
‚Ä¢ (T ‚àíŒªiI)p2‚àí1(v2)
¬∑ ¬∑ ¬∑
‚Ä¢ (T ‚àíŒªiI)pni‚àí1(vni)
‚Ä¢ (T ‚àíŒªiI)p1‚àí2(v1)
‚Ä¢ (T ‚àíŒªiI)p2‚àí2(v2)
¬∑ ¬∑ ¬∑
‚Ä¢ (T ‚àíŒªiI)pni‚àí2(vni)
...
...
...
‚Ä¢ (T ‚àíŒªiI)(vni)
‚Ä¢ vni
‚Ä¢ (T ‚àíŒªiI)(v2)
‚Ä¢ v2
‚Ä¢ (T ‚àíŒªiI)(v1)
‚Ä¢ v1
Notice that the dot diagram of Ti has ni columns (one for each cycle) and
p1 rows. Since p1 ‚â•p2 ‚â•¬∑ ¬∑ ¬∑ ‚â•pni, the columns of the dot diagram become
shorter (or at least not longer) as we move from left to right.
Now let rj denote the number of dots in the jth row of the dot diagram.
Observe that r1 ‚â•r2 ‚â•¬∑ ¬∑ ¬∑ ‚â•rp1. Furthermore, the diagram can be re-
constructed from the values of the ri‚Äôs. The proofs of these facts, which are
combinatorial in nature, are treated in Exercise 9.

Sec. 7.2
The Jordan Canonical Form II
499
In Example 1, with ni = 4, p1 = p2 = 3, p3 = 2, and p4 = 1, the dot
diagram of Ti is as follows:
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
Here r1 = 4, r2 = 3, and r3 = 2.
We now devise a method for computing the dot diagram of Ti using the
ranks of linear operators determined by T and Œªi. Hence the dot diagram
is completely determined by T, from which it follows that it is unique. On
the other hand, Œ≤i is not unique. For example, see Exercise 8. (It is for this
reason that we associate the dot diagram with Ti rather than with Œ≤i.)
To determine the dot diagram of Ti, we devise a method for computing
each rj, the number of dots in the jth row of the dot diagram, using only T
and Œªi. The next three results give us the required method. To facilitate our
arguments, we Ô¨Åx a basis Œ≤i for KŒªi so that Œ≤i is a disjoint union of ni cycles
of generalized eigenvectors with lengths p1 ‚â•p2 ‚â•¬∑ ¬∑ ¬∑ ‚â•pni.
Theorem 7.9.
For any positive integer r, the vectors in Œ≤i that are
associated with the dots in the Ô¨Årst r rows of the dot diagram of Ti constitute
a basis for N((T ‚àíŒªiI)r). Hence the number of dots in the Ô¨Årst r rows of the
dot diagram equals nullity((T ‚àíŒªiI)r).
Proof. Clearly, N((T ‚àíŒªiI)r) ‚äÜKŒªi, and KŒªi is invariant under (T ‚àíŒªiI)r.
Let U denote the restriction of (T ‚àíŒªiI)r to KŒªi. By the preceding remarks,
N((T ‚àíŒªiI)r) = N(U), and hence it suÔ¨Éces to establish the theorem for U.
Now deÔ¨Åne
S1 = {x ‚ààŒ≤i : U(x) = 0}
and
S2 = {x ‚ààŒ≤i : U(x) Ã∏= 0}.
Let a and b denote the number of vectors in S1 and S2, respectively, and let
mi = dim(KŒªi). Then a + b = mi. For any x ‚ààŒ≤i, x ‚ààS1 if and only if x is
one of the Ô¨Årst r vectors of a cycle, and this is true if and only if x corresponds
to a dot in the Ô¨Årst r rows of the dot diagram. Hence a is the number of dots
in the Ô¨Årst r rows of the dot diagram. For any x ‚ààS2, the eÔ¨Äect of applying
U to x is to move the dot corresponding to x exactly r places up its column to
another dot. It follows that U maps S2 in a one-to-one fashion into Œ≤i. Thus
{U(x): x ‚ààS2} is a basis for R(U) consisting of b vectors. Hence rank(U) = b,
and so nullity(U) = mi ‚àíb = a. But S1 is a linearly independent subset of
N(U) consisting of a vectors; therefore S1 is a basis for N(U).
In the case that r = 1, Theorem 7.9 yields the following corollary.
Corollary. The dimension of EŒªi is ni. Hence in a Jordan canonical form
of T, the number of Jordan blocks corresponding to Œªi equals the dimension
of EŒªi.

500
Chap. 7
Canonical Forms
Proof. Exercise.
We are now able to devise a method for describing the dot diagram in
terms of the ranks of operators.
Theorem 7.10. Let rj denote the number of dots in the jth row of the
dot diagram of Ti, the restriction of T to KŒªi. Then the following statements
are true.
(a) r1 = dim(V) ‚àírank(T ‚àíŒªiI).
(b) rj = rank((T ‚àíŒªiI)j‚àí1) ‚àírank((T ‚àíŒªiI)j)
if j > 1.
Proof. By Theorem 7.9, for 1 ‚â§j ‚â§p1, we have
r1 + r2 + ¬∑ ¬∑ ¬∑ + rj = nullity((T ‚àíŒªiI)j)
= dim(V) ‚àírank((T ‚àíŒªiI)j).
Hence
r1 = dim(V) ‚àírank(T ‚àíŒªiI),
and for j > 1,
rj = (r1 + r2 + ¬∑ ¬∑ ¬∑ + rj) ‚àí(r1 + r2 + ¬∑ ¬∑ ¬∑ + rj‚àí1)
= [dim(V) ‚àírank((T ‚àíŒªiI)j)] ‚àí[dim(V) ‚àírank((T ‚àíŒªiI)j‚àí1)]
= rank((T ‚àíŒªiI)j‚àí1) ‚àírank((T ‚àíŒªiI)j).
Theorem 7.10 shows that the dot diagram of Ti is completely determined
by T and Œªi. Hence we have proved the following result.
Corollary. For any eigenvalue Œªi of T, the dot diagram of Ti is unique.
Thus, subject to the convention that the cycles of generalized eigenvectors
for the bases of each generalized eigenspace are listed in order of decreasing
length, the Jordan canonical form of a linear operator or a matrix is unique
up to the ordering of the eigenvalues.
We apply these results to Ô¨Ånd the Jordan canonical forms of two matrices
and a linear operator.
Example 2
Let
A =
‚éõ
‚éú
‚éú
‚éù
2
‚àí1
0
1
0
3
‚àí1
0
0
1
1
0
0
‚àí1
0
3
‚éû
‚éü
‚éü
‚é†.

Sec. 7.2
The Jordan Canonical Form II
501
We Ô¨Ånd the Jordan canonical form of A and a Jordan canonical basis for the
linear operator T = LA. The characteristic polynomial of A is
det(A ‚àítI) = (t ‚àí2)3(t ‚àí3).
Thus A has two distinct eigenvalues, Œª1 = 2 and Œª2 = 3, with multiplicities 3
and 1, respectively. Let T1 and T2 be the restrictions of LA to the generalized
eigenspaces KŒª1 and KŒª2, respectively.
Suppose that Œ≤1 is a Jordan canonical basis for T1. Since Œª1 has multi-
plicity 3, it follows that dim(KŒª1) = 3 by Theorem 7.4(c) (p. 487); hence the
dot diagram of T1 has three dots. As we did earlier, let rj denote the number
of dots in the jth row of this dot diagram. Then, by Theorem 7.10,
r1 = 4 ‚àírank(A ‚àí2I) = 4 ‚àírank
‚éõ
‚éú
‚éú
‚éù
0
‚àí1
0
1
0
1
‚àí1
0
0
1
‚àí1
0
0
‚àí1
0
1
‚éû
‚éü
‚éü
‚é†= 4 ‚àí2 = 2,
and
r2 = rank(A ‚àí2I) ‚àírank((A ‚àí2I)2) = 2 ‚àí1 = 1.
(Actually, the computation of r2 is unnecessary in this case because r1 = 2 and
the dot diagram only contains three dots.) Hence the dot diagram associated
with Œ≤1 is
‚Ä¢
‚Ä¢
‚Ä¢
So
A1 = [T1]Œ≤1 =
‚éõ
‚éù
2
1
0
0
2
0
0
0
2
‚éû
‚é†.
Since Œª2 = 3 has multiplicity 1, it follows that dim(KŒª2) = 1, and conse-
quently any basis Œ≤2 for KŒª2 consists of a single eigenvector corresponding to
Œª2 = 3. Therefore
A2 = [T2]Œ≤2 = (3).
Setting Œ≤ = Œ≤1 ‚à™Œ≤2, we have
J = [LA]Œ≤ =
‚éõ
‚éú
‚éú
‚éù
2
1
0
0
0
2
0
0
0
0
2
0
0
0
0
3
‚éû
‚éü
‚éü
‚é†,

502
Chap. 7
Canonical Forms
and so J is the Jordan canonical form of A.
We now Ô¨Ånd a Jordan canonical basis for T = LA. We begin by determin-
ing a Jordan canonical basis Œ≤1 for T1. Since the dot diagram of T1 has two
columns, each corresponding to a cycle of generalized eigenvectors, there are
two such cycles. Let v1 and v2 denote the end vectors of the Ô¨Årst and second
cycles, respectively. We reprint below the dot diagram with the dots labeled
with the names of the vectors to which they correspond.
‚Ä¢ (T ‚àí2I)(v1)
‚Ä¢ v2
‚Ä¢ v1
From this diagram we see that v1 ‚ààN((T ‚àí2I)2) but v1 /‚ààN(T ‚àí2I). Now
A ‚àí2I =
‚éõ
‚éú
‚éú
‚éù
0
‚àí1
0
1
0
1
‚àí1
0
0
1
‚àí1
0
0
‚àí1
0
1
‚éû
‚éü
‚éü
‚é†
and
(A ‚àí2I)2 =
‚éõ
‚éú
‚éú
‚éù
0
‚àí2
1
1
0
0
0
0
0
0
0
0
0
‚àí2
1
1
‚éû
‚éü
‚éü
‚é†.
It is easily seen that
‚éß
‚é™
‚é™
‚é®
‚é™
‚é™
‚é©
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
‚éû
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éù
0
1
2
0
‚éû
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éù
0
1
0
2
‚éû
‚éü
‚éü
‚é†
‚é´
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é≠
is a basis for N((T ‚àí2I)2) = KŒª1. Of these three basis vectors, the last two
do not belong to N(T ‚àí2I), and hence we select one of these for v1. Suppose
that we choose
v1 =
‚éõ
‚éú
‚éú
‚éù
0
1
2
0
‚éû
‚éü
‚éü
‚é†.
Then
(T ‚àí2I)(v1) = (A ‚àí2I)(v1) =
‚éõ
‚éú
‚éú
‚éù
0
‚àí1
0
1
0
1
‚àí1
0
0
1
‚àí1
0
0
‚àí1
0
1
‚éû
‚éü
‚éü
‚é†
‚éõ
‚éú
‚éú
‚éù
0
1
2
0
‚éû
‚éü
‚éü
‚é†=
‚éõ
‚éú
‚éú
‚éù
‚àí1
‚àí1
‚àí1
‚àí1
‚éû
‚éü
‚éü
‚é†.
Now simply choose v2 to be a vector in EŒª1 that is linearly independent of
(T ‚àí2I)(v1); for example, select
v2 =
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
‚éû
‚éü
‚éü
‚é†.

Sec. 7.2
The Jordan Canonical Form II
503
Thus we have associated the Jordan canonical basis
Œ≤1 =
‚éß
‚é™
‚é™
‚é®
‚é™
‚é™
‚é©
‚éõ
‚éú
‚éú
‚éù
‚àí1
‚àí1
‚àí1
‚àí1
‚éû
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éù
0
1
2
0
‚éû
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
‚éû
‚éü
‚éü
‚é†
‚é´
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é≠
with the dot diagram in the following manner.
‚Ä¢
‚éõ
‚éú
‚éú
‚éù
‚àí1
‚àí1
‚àí1
‚àí1
‚éû
‚éü
‚éü
‚é†
‚Ä¢
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
‚éû
‚éü
‚éü
‚é†
‚Ä¢
‚éõ
‚éú
‚éú
‚éù
0
1
2
0
‚éû
‚éü
‚éü
‚é†
By Theorem 7.6 (p. 489), the linear independence of Œ≤1 is guaranteed since
v2 was chosen to be linearly independent of (T ‚àí2I)(v1).
Since Œª2 = 3 has multiplicity 1, dim(KŒª2) = dim(EŒª2) = 1. Hence any
eigenvector of LA corresponding to Œª2 = 3 constitutes an appropriate basis
Œ≤2. For example,
Œ≤2 =
‚éß
‚é™
‚é™
‚é®
‚é™
‚é™
‚é©
‚éõ
‚éú
‚éú
‚éù
1
0
0
1
‚éû
‚éü
‚éü
‚é†
‚é´
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é≠
.
Thus
Œ≤ = Œ≤1 ‚à™Œ≤2 =
‚éß
‚é™
‚é™
‚é®
‚é™
‚é™
‚é©
‚éõ
‚éú
‚éú
‚éù
‚àí1
‚àí1
‚àí1
‚àí1
‚éû
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éù
0
1
2
0
‚éû
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
‚éû
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éù
1
0
0
1
‚éû
‚éü
‚éü
‚é†
‚é´
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é≠
is a Jordan canonical basis for LA.
Notice that if
Q =
‚éõ
‚éú
‚éú
‚éù
‚àí1
0
1
1
‚àí1
1
0
0
‚àí1
2
0
0
‚àí1
0
0
1
‚éû
‚éü
‚éü
‚é†,
then J = Q‚àí1AQ.
‚ô¶

504
Chap. 7
Canonical Forms
Example 3
Let
A =
‚éõ
‚éú
‚éú
‚éù
2
‚àí4
2
2
‚àí2
0
1
3
‚àí2
‚àí2
3
3
‚àí2
‚àí6
3
7
‚éû
‚éü
‚éü
‚é†.
We Ô¨Ånd the Jordan canonical form J of A, a Jordan canonical basis for LA,
and a matrix Q such that J = Q‚àí1AQ.
The characteristic polynomial of A is det(A ‚àítI) = (t ‚àí2)2(t ‚àí4)2. Let
T = LA, Œª1 = 2, and Œª2 = 4, and let Ti be the restriction of LA to KŒªi for
i = 1, 2.
We begin by computing the dot diagram of T1. Let r1 denote the number
of dots in the Ô¨Årst row of this diagram. Then
r1 = 4 ‚àírank(A ‚àí2I) = 4 ‚àí2 = 2;
hence the dot diagram of T1 is as follows.
‚Ä¢
‚Ä¢
Therefore
A1 = [T1]Œ≤1 =

2
0
0
2
	
,
where Œ≤1 is any basis corresponding to the dots. In this case, Œ≤1 is an arbitrary
basis for EŒª1 = N(T ‚àí2I), for example,
Œ≤1 =
‚éß
‚é™
‚é™
‚é®
‚é™
‚é™
‚é©
‚éõ
‚éú
‚éú
‚éù
2
1
0
2
‚éû
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éù
0
1
2
0
‚éû
‚éü
‚éü
‚é†
‚é´
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é≠
.
Next we compute the dot diagram of T2. Since rank(A ‚àí4I) = 3, there
is only 4 ‚àí3 = 1 dot in the Ô¨Årst row of the diagram.
Since Œª2 = 4 has
multiplicity 2, we have dim(KŒª2) = 2, and hence this dot diagram has the
following form:
‚Ä¢
‚Ä¢
Thus
A2 = [T2]Œ≤2 =

4
1
0
4
	
,

Sec. 7.2
The Jordan Canonical Form II
505
where Œ≤2 is any basis for KŒª2 corresponding to the dots. In this case, Œ≤2
is a cycle of length 2. The end vector of this cycle is a vector v ‚ààKŒª2 =
N((T ‚àí4I)2) such that v /‚ààN(T ‚àí4I). One way of Ô¨Ånding such a vector was
used to select the vector v1 in Example 2.
In this example, we illustrate
another method. A simple calculation shows that a basis for the null space
of LA ‚àí4I is
‚éß
‚é™
‚é™
‚é®
‚é™
‚é™
‚é©
‚éõ
‚éú
‚éú
‚éù
0
1
1
1
‚éû
‚éü
‚éü
‚é†
‚é´
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é≠
.
Choose v to be any solution to the system of linear equations
(A ‚àí4I)x =
‚éõ
‚éú
‚éú
‚éù
0
1
1
1
‚éû
‚éü
‚éü
‚é†,
for example,
v =
‚éõ
‚éú
‚éú
‚éù
1
‚àí1
‚àí1
0
‚éû
‚éü
‚éü
‚é†.
Thus
Œ≤2 = {(LA ‚àí4I)(v), v} =
‚éß
‚é™
‚é™
‚é®
‚é™
‚é™
‚é©
‚éõ
‚éú
‚éú
‚éù
0
1
1
1
‚éû
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éù
1
‚àí1
‚àí1
0
‚éû
‚éü
‚éü
‚é†
‚é´
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é≠
.
Therefore
Œ≤ = Œ≤1 ‚à™Œ≤2 =
‚éß
‚é™
‚é™
‚é®
‚é™
‚é™
‚é©
‚éõ
‚éú
‚éú
‚éù
2
1
0
2
‚éû
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éù
0
1
2
0
‚éû
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éù
0
1
1
1
‚éû
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éù
1
‚àí1
‚àí1
0
‚éû
‚éü
‚éü
‚é†
‚é´
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é≠
is a Jordan canonical basis for LA. The corresponding Jordan canonical form
is given by
J = [LA]Œ≤ =

A1
O
O
A2
	
=
‚éõ
‚éú
‚éú
‚éù
2
0
0
0
0
2
0
0
0
0
4
1
0
0
0
4
‚éû
‚éü
‚éü
‚é†.

506
Chap. 7
Canonical Forms
Finally, we deÔ¨Åne Q to be the matrix whose columns are the vectors of Œ≤
listed in the same order, namely,
Q =
‚éõ
‚éú
‚éú
‚éù
2
0
0
1
1
1
1
‚àí1
0
2
1
‚àí1
2
0
1
0
‚éû
‚éü
‚éü
‚é†.
Then J = Q‚àí1AQ.
‚ô¶
Example 4
Let V be the vector space of polynomial functions in two real variables x
and y of degree at most 2.
Then V is a vector space over R and Œ± =
{1, x, y, x2, y2, xy} is an ordered basis for V. Let T be the linear operator
on V deÔ¨Åned by
T(f(x, y)) = ‚àÇ
‚àÇxf(x, y).
For example, if f(x, y) = x + 2x2 ‚àí3xy + y, then
T(f(x, y)) = ‚àÇ
‚àÇx(x + 2x2 ‚àí3xy + y) = 1 + 4x ‚àí3y.
We Ô¨Ånd the Jordan canonical form and a Jordan canonical basis for T.
Let A = [T]Œ±. Then
A =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
0
1
0
0
0
0
0
0
0
2
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
,
and hence the characteristic polynomial of T is
det(A ‚àítI) = det
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
‚àít
1
0
0
0
0
0
‚àít
0
2
0
0
0
0
‚àít
0
0
1
0
0
0
‚àít
0
0
0
0
0
0
‚àít
0
0
0
0
0
0
‚àít
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
= t6.
Thus Œª = 0 is the only eigenvalue of T, and KŒª = V. For each j, let rj denote
the number of dots in the jth row of the dot diagram of T. By Theorem 7.10,
r1 = 6 ‚àírank(A) = 6 ‚àí3 = 3,

Sec. 7.2
The Jordan Canonical Form II
507
and since
A2 =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
0
0
0
2
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
,
r2 = rank(A) ‚àírank(A2) = 3 ‚àí1 = 2.
Because there are a total of six dots in the dot diagram and r1 = 3 and
r2 = 2, it follows that r3 = 1. So the dot diagram of T is
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
We conclude that the Jordan canonical form of T is
J =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
.
We now Ô¨Ånd a Jordan canonical basis for T. Since the Ô¨Årst column of the
dot diagram of T consists of three dots, we must Ô¨Ånd a polynomial f1(x, y)
such that ‚àÇ2
‚àÇx2 f1(x, y) Ã∏= 0. Examining the basis Œ± = {1, x, y, x2, y2, xy} for
KŒª = V, we see that x2 is a suitable candidate. Setting f1(x, y) = x2, we see
that
(T ‚àíŒªI)(f1(x, y)) = T(f1(x, y)) = ‚àÇ
‚àÇx(x2) = 2x
and
(T ‚àíŒªI)2(f1(x, y)) = T2(f1(x, y)) = ‚àÇ2
‚àÇx2 (x2) = 2.
Likewise, since the second column of the dot diagram consists of two dots, we
must Ô¨Ånd a polynomial f2(x, y) such that
‚àÇ
‚àÇx(f2(x, y)) Ã∏= 0,
but
‚àÇ2
‚àÇx2 (f2(x, y)) = 0.

508
Chap. 7
Canonical Forms
Since our choice must be linearly independent of the polynomials already
chosen for the Ô¨Årst cycle, the only choice in Œ± that satisÔ¨Åes these constraints
is xy. So we set f2(x, y) = xy. Thus
(T ‚àíŒªI)(f2(x, y)) = T(f2(x, y)) = ‚àÇ
‚àÇx(xy) = y.
Finally, the third column of the dot diagram consists of a single polynomial
that lies in the null space of T. The only remaining polynomial in Œ± is y2,
and it is suitable here. So set f3(x, y) = y2. Therefore we have identiÔ¨Åed
polynomials with the dots in the dot diagram as follows.
‚Ä¢ 2
‚Ä¢ y
‚Ä¢ y2
‚Ä¢ 2x
‚Ä¢ xy
‚Ä¢ x2
Thus Œ≤ = {2, 2x, x2, y, xy, y2} is a Jordan canonical basis for T.
‚ô¶
In the three preceding examples, we relied on our ingenuity and the con-
text of the problem to Ô¨Ånd Jordan canonical bases. The reader can do the
same in the exercises. We are successful in these cases because the dimen-
sions of the generalized eigenspaces under consideration are small. We do
not attempt, however, to develop a general algorithm for computing Jordan
canonical bases, although one could be devised by following the steps in the
proof of the existence of such a basis (Theorem 7.7 p. 490).
The following result may be thought of as a corollary to Theorem 7.10.
Theorem 7.11. Let A and B be n √ó n matrices, each having Jordan
canonical forms computed according to the conventions of this section. Then
A and B are similar if and only if they have (up to an ordering of their
eigenvalues) the same Jordan canonical form.
Proof. If A and B have the same Jordan canonical form J, then A and B
are each similar to J and hence are similar to each other.
Conversely, suppose that A and B are similar. Then A and B have the
same eigenvalues. Let JA and JB denote the Jordan canonical forms of A and
B, respectively, with the same ordering of their eigenvalues. Then A is similar
to both JA and JB, and therefore, by the corollary to Theorem 2.23 (p. 115),
JA and JB are matrix representations of LA. Hence JA and JB are Jordan
canonical forms of LA. Thus JA = JB by the corollary to Theorem 7.10.
Example 5
We determine which of the matrices
A =
‚éõ
‚éù
‚àí3
3
‚àí2
‚àí7
6
‚àí3
1
‚àí1
2
‚éû
‚é†,
B =
‚éõ
‚éù
0
1
‚àí1
‚àí4
4
‚àí2
‚àí2
1
1
‚éû
‚é†,

Sec. 7.2
The Jordan Canonical Form II
509
C =
‚éõ
‚éù
0
‚àí1
‚àí1
‚àí3
‚àí1
‚àí2
7
5
6
‚éû
‚é†,
and
D =
‚éõ
‚éù
0
1
2
0
1
1
0
0
2
‚éû
‚é†
are similar. Observe that A, B, and C have the same characteristic poly-
nomial ‚àí(t ‚àí1)(t ‚àí2)2, whereas D has ‚àít(t ‚àí1)(t ‚àí2) as its characteristic
polynomial. Because similar matrices have the same characteristic polynomi-
als, D cannot be similar to A, B, or C. Let JA, JB, and JC be the Jordan
canonical forms of A, B, and C, respectively, using the ordering 1, 2 for their
common eigenvalues. Then (see Exercise 4)
JA =
‚éõ
‚éù
1
0
0
0
2
1
0
0
2
‚éû
‚é†,
JB =
‚éõ
‚éù
1
0
0
0
2
0
0
0
2
‚éû
‚é†,
and
JC =
‚éõ
‚éù
1
0
0
0
2
1
0
0
2
‚éû
‚é†.
Since JA = JC, A is similar to C. Since JB is diÔ¨Äerent from JA and JC, B is
similar to neither A nor C.
‚ô¶
The reader should observe that any diagonal matrix is a Jordan canonical
form. Thus a linear operator T on a Ô¨Ånite-dimensional vector space V is diag-
onalizable if and only if its Jordan canonical form is a diagonal matrix. Hence
T is diagonalizable if and only if the Jordan canonical basis for T consists of
eigenvectors of T. Similar statements can be made about matrices. Thus,
of the matrices A, B, and C in Example 5, A and C are not diagonalizable
because their Jordan canonical forms are not diagonal matrices.
EXERCISES
1.
Label the following statements as true or false. Assume that the char-
acteristic polynomial of the matrix or linear operator splits.
(a)
The Jordan canonical form of a diagonal matrix is the matrix itself.
(b)
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V
that has a Jordan canonical form J. If Œ≤ is any basis for V, then
the Jordan canonical form of [T]Œ≤ is J.
(c)
Linear operators having the same characteristic polynomial are
similar.
(d)
Matrices having the same Jordan canonical form are similar.
(e)
Every matrix is similar to its Jordan canonical form.
(f)
Every
linear
operator
with
the
characteristic
polynomial
(‚àí1)n(t ‚àíŒª)n has the same Jordan canonical form.
(g)
Every linear operator on a Ô¨Ånite-dimensional vector space has a
unique Jordan canonical basis.
(h)
The dot diagrams of a linear operator on a Ô¨Ånite-dimensional vec-
tor space are unique.

510
Chap. 7
Canonical Forms
2.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V such
that the characteristic polynomial of T splits. Suppose that Œª1 = 2,
Œª2 = 4, and Œª3 = ‚àí3 are the distinct eigenvalues of T and that the dot
diagrams for the restriction of T to KŒªi (i = 1, 2, 3) are as follows:
Œª1 = 2
Œª2 = 4
Œª3 = ‚àí3
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
Find the Jordan canonical form J of T.
3.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V with
Jordan canonical form‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
2
1
0
0
0
0
0
0
2
1
0
0
0
0
0
0
2
0
0
0
0
0
0
0
2
1
0
0
0
0
0
0
2
0
0
0
0
0
0
0
3
0
0
0
0
0
0
0
3
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
.
(a)
Find the characteristic polynomial of T.
(b)
Find the dot diagram corresponding to each eigenvalue of T.
(c)
For which eigenvalues Œªi, if any, does EŒªi = KŒªi?
(d)
For each eigenvalue Œªi, Ô¨Ånd the smallest positive integer pi for
which KŒªi = N((T ‚àíŒªiI)pi).
(e)
Compute the following numbers for each i, where Ui denotes the
restriction of T ‚àíŒªiI to KŒªi.
(i) rank(Ui)
(ii) rank(U2
i )
(iii) nullity(Ui)
(iv) nullity(U2
i )
4.
For each of the matrices A that follow, Ô¨Ånd a Jordan canonical form
J and an invertible matrix Q such that J = Q‚àí1AQ. Notice that the
matrices in (a), (b), and (c) are those used in Example 5.
(a) A =
‚éõ
‚éù
‚àí3
3
‚àí2
‚àí7
6
‚àí3
1
‚àí1
2
‚éû
‚é†
(b) A =
‚éõ
‚éù
0
1
‚àí1
‚àí4
4
‚àí2
‚àí2
1
1
‚éû
‚é†
(c) A =
‚éõ
‚éù
0
‚àí1
‚àí1
‚àí3
‚àí1
‚àí2
7
5
6
‚éû
‚é†
(d) A =
‚éõ
‚éú
‚éú
‚éù
0
‚àí3
1
2
‚àí2
1
‚àí1
2
‚àí2
1
‚àí1
2
‚àí2
‚àí3
1
4
‚éû
‚éü
‚éü
‚é†

Sec. 7.2
The Jordan Canonical Form II
511
5.
For each linear operator T, Ô¨Ånd a Jordan canonical form J of T and a
Jordan canonical basis Œ≤ for T.
(a)
V is the real vector space of functions spanned by the set of real-
valued functions {et, tet, t2et, e2t}, and T is the linear operator on
V deÔ¨Åned by T(f) = f ‚Ä≤.
(b)
T is the linear operator on P3(R) deÔ¨Åned by T(f(x)) = xf ‚Ä≤‚Ä≤(x).
(c)
T is the linear operator on P3(R) deÔ¨Åned by
T(f(x)) = f ‚Ä≤‚Ä≤(x) + 2f(x).
(d)
T is the linear operator on M2√ó2(R) deÔ¨Åned by
T(A) =

3
1
0
3
	
¬∑ A ‚àíAt.
(e)
T is the linear operator on M2√ó2(R) deÔ¨Åned by
T(A) =

3
1
0
3
	
¬∑ (A ‚àíAt).
(f)
V is the vector space of polynomial functions in two real variables
x and y of degree at most 2, as deÔ¨Åned in Example 4, and T is the
linear operator on V deÔ¨Åned by
T(f(x, y)) = ‚àÇ
‚àÇxf(x, y) + ‚àÇ
‚àÇy f(x, y).
6.
Let A be an n √ó n matrix whose characteristic polynomial splits. Prove
that A and At have the same Jordan canonical form, and conclude that
A and At are similar. Hint: For any eigenvalue Œª of A and At and any
positive integer r, show that rank((A ‚àíŒªI)r) = rank((At ‚àíŒªI)r).
7.
Let A be an n √ó n matrix whose characteristic polynomial splits, Œ≥ be
a cycle of generalized eigenvectors corresponding to an eigenvalue Œª,
and W be the subspace spanned by Œ≥. DeÔ¨Åne Œ≥‚Ä≤ to be the ordered set
obtained from Œ≥ by reversing the order of the vectors in Œ≥.
(a)
Prove that [TW]Œ≥‚Ä≤ = ([TW]Œ≥)t.
(b)
Let J be the Jordan canonical form of A. Use (a) to prove that J
and Jt are similar.
(c)
Use (b) to prove that A and At are similar.
8.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space, and
suppose that the characteristic polynomial of T splits. Let Œ≤ be a Jordan
canonical basis for T.
(a)
Prove that for any nonzero scalar c, {cx: x ‚ààŒ≤} is a Jordan canon-
ical basis for T.

512
Chap. 7
Canonical Forms
(b)
Suppose that Œ≥ is one of the cycles of generalized eigenvectors that
forms Œ≤, and suppose that Œ≥ corresponds to the eigenvalue Œª and
has length greater than 1. Let x be the end vector of Œ≥, and let y
be a nonzero vector in EŒª. Let Œ≥‚Ä≤ be the ordered set obtained from
Œ≥ by replacing x by x + y. Prove that Œ≥‚Ä≤ is a cycle of generalized
eigenvectors corresponding to Œª, and that if Œ≥‚Ä≤ replaces Œ≥ in the
union that deÔ¨Ånes Œ≤, then the new union is also a Jordan canonical
basis for T.
(c)
Apply (b) to obtain a Jordan canonical basis for LA, where A is the
matrix given in Example 2, that is diÔ¨Äerent from the basis given
in the example.
9.
Suppose that a dot diagram has k columns and m rows with pj dots in
column j and ri dots in row i. Prove the following results.
(a)
m = p1 and k = r1.
(b)
pj = max {i: ri ‚â•j} for 1 ‚â§j ‚â§k and ri = max {j : pj ‚â•i} for
1 ‚â§i ‚â§m. Hint: Use mathematical induction on m.
(c)
r1 ‚â•r2 ‚â•¬∑ ¬∑ ¬∑ ‚â•rm.
(d)
Deduce that the number of dots in each column of a dot diagram
is completely determined by the number of dots in the rows.
10.
Let T be a linear operator whose characteristic polynomial splits, and
let Œª be an eigenvalue of T.
(a)
Prove that dim(KŒª) is the sum of the lengths of all the blocks
corresponding to Œª in the Jordan canonical form of T.
(b)
Deduce that EŒª = KŒª if and only if all the Jordan blocks corre-
sponding to Œª are 1 √ó 1 matrices.
The following deÔ¨Ånitions are used in Exercises 11‚Äì19.
DeÔ¨Ånitions. A linear operator T on a vector space V is called nilpotent
if Tp = T0 for some positive integer p. An n√ón matrix A is called nilpotent
if Ap = O for some positive integer p.
11.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V, and
let Œ≤ be an ordered basis for V. Prove that T is nilpotent if and only if
[T]Œ≤ is nilpotent.
12.
Prove that any square upper triangular matrix with each diagonal entry
equal to zero is nilpotent.
13.
Let T be a nilpotent operator on an n-dimensional vector space V, and
suppose that p is the smallest positive integer for which Tp = T0. Prove
the following results.
(a)
N(Ti) ‚äÜN(Ti+1) for every positive integer i.

Sec. 7.2
The Jordan Canonical Form II
513
(b)
There is a sequence of ordered bases Œ≤1, Œ≤2, . . . , Œ≤p such that Œ≤i is
a basis for N(Ti) and Œ≤i+1 contains Œ≤i for 1 ‚â§i ‚â§p ‚àí1.
(c)
Let Œ≤ = Œ≤p be the ordered basis for N(Tp) = V in (b). Then [T]Œ≤
is an upper triangular matrix with each diagonal entry equal to
zero.
(d)
The characteristic polynomial of T is (‚àí1)ntn. Hence the charac-
teristic polynomial of T splits, and 0 is the only eigenvalue of T.
14.
Prove the converse of Exercise 13(d): If T is a linear operator on an n-
dimensional vector space V and (‚àí1)ntn is the characteristic polynomial
of T, then T is nilpotent.
15.
Give an example of a linear operator T on a Ô¨Ånite-dimensional vector
space such that T is not nilpotent, but zero is the only eigenvalue of T.
Characterize all such operators.
16.
Let T be a nilpotent linear operator on a Ô¨Ånite-dimensional vector space
V. Recall from Exercise 13 that Œª = 0 is the only eigenvalue of T, and
hence V = KŒª. Let Œ≤ be a Jordan canonical basis for T. Prove that for
any positive integer i, if we delete from Œ≤ the vectors corresponding to
the last i dots in each column of a dot diagram of Œ≤, the resulting set is
a basis for R(Ti). (If a column of the dot diagram contains fewer than i
dots, all the vectors associated with that column are removed from Œ≤.)
17.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V such
that the characteristic polynomial of T splits, and let Œª1, Œª2, . . . , Œªk be
the distinct eigenvalues of T. Let S: V ‚ÜíV be the mapping deÔ¨Åned by
S(x) = Œª1v1 + Œª2v2 + ¬∑ ¬∑ ¬∑ + Œªkvk,
where, for each i, vi is the unique vector in KŒªi such that x = v1 +
v2 +¬∑ ¬∑ ¬∑+vk. (This unique representation is guaranteed by Theorem 7.3
(p. 486) and Exercise 8 of Section 7.1.)
(a)
Prove that S is a diagonalizable linear operator on V.
(b)
Let U = T ‚àíS. Prove that U is nilpotent and commutes with S,
that is, SU = US.
18.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V, and
let J be the Jordan canonical form of T. Let D be the diagonal matrix
whose diagonal entries are the diagonal entries of J, and let M = J ‚àíD.
Prove the following results.
(a)
M is nilpotent.
(b)
MD = DM.

514
Chap. 7
Canonical Forms
(c)
If p is the smallest positive integer for which M p = O, then, for
any positive integer r < p,
Jr = Dr + rDr‚àí1M + r(r ‚àí1)
2!
Dr‚àí2M 2 + ¬∑ ¬∑ ¬∑ + rDM r‚àí1 + M r,
and, for any positive integer r ‚â•p,
Jr = Dr + rDr‚àí1M + r(r ‚àí1)
2!
Dr‚àí2M 2 + ¬∑ ¬∑ ¬∑
+
r!
(r ‚àíp + 1)!(p ‚àí1)!Dr‚àíp+1M p‚àí1.
19.
Let
J =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
Œª
1
0
¬∑ ¬∑ ¬∑
0
0
Œª
1
¬∑ ¬∑ ¬∑
0
0
0
Œª
¬∑ ¬∑ ¬∑
0
...
...
...
...
0
0
0
¬∑ ¬∑ ¬∑
1
0
0
0
¬∑ ¬∑ ¬∑
Œª
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
be the m √ó m Jordan block corresponding to Œª, and let N = J ‚àíŒªIm.
Prove the following results:
(a)
N m = O, and for 1 ‚â§r < m,
N r
ij =

1
if j = i + r
0
otherwise.
(b)
For any integer r ‚â•m,
Jr =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
Œªr rŒªr‚àí1 r(r ‚àí1)
2!
Œªr‚àí2 ¬∑ ¬∑ ¬∑ r(r ‚àí1) ¬∑ ¬∑ ¬∑ (r ‚àím + 2)
(m ‚àí1)!
Œªr‚àím+1
0
Œªr
rŒªr‚àí1 ¬∑ ¬∑ ¬∑ r(r ‚àí1) ¬∑ ¬∑ ¬∑ (r ‚àím + 3)
(m ‚àí2)!
Œªr‚àím+2
...
...
...
...
0
0
0
¬∑ ¬∑ ¬∑
Œªr
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
.
(c)
lim
r‚Üí‚àûJr exists if and only if one of the following holds:
(i) |Œª| < 1.
(ii) Œª = 1 and m = 1.

Sec. 7.2
The Jordan Canonical Form II
515
(Note that lim
r‚Üí‚àûŒªr exists under these conditions. See the discus-
sion preceding Theorem 5.13 on page 285.) Furthermore, lim
r‚Üí‚àûJr
is the zero matrix if condition (i) holds and is the 1 √ó 1 matrix (1)
if condition (ii) holds.
(d)
Prove Theorem 5.13 on page 285.
The following deÔ¨Ånition is used in Exercises 20 and 21.
DeÔ¨Ånition. For any A ‚ààMn√ón(C), deÔ¨Åne the norm of A by
‚à•A‚à•= max {|Aij|: 1 ‚â§i, j ‚â§n}.
20.
Let A, B ‚ààMn√ón(C). Prove the following results.
(a)
‚à•A‚à•‚â•0 and ‚à•A‚à•= 0 if and only if A = O.
(b)
‚à•cA‚à•= |c|¬∑‚à•A‚à•for any scalar c.
(c)
‚à•A + B‚à•‚â§‚à•A‚à•+ ‚à•B‚à•.
(d)
‚à•AB‚à•‚â§n‚à•A‚à•‚à•B‚à•.
21.
Let A ‚ààMn√ón(C) be a transition matrix. (See Section 5.3.) Since C is
an algebraically closed Ô¨Åeld, A has a Jordan canonical form J to which
A is similar. Let P be an invertible matrix such that P ‚àí1AP = J.
Prove the following results.
(a)
‚à•Am‚à•‚â§1 for every positive integer m.
(b)
There exists a positive number c such that ‚à•Jm‚à•‚â§c for every
positive integer m.
(c)
Each Jordan block of J corresponding to the eigenvalue Œª = 1 is a
1 √ó 1 matrix.
(d)
lim
m‚Üí‚àûAm exists if and only if 1 is the only eigenvalue of A with
absolute value 1.
(e)
Theorem 5.20(a) using (c) and Theorem 5.19.
The next exercise requires knowledge of absolutely convergent series as well
as the deÔ¨Ånition of eA for a matrix A. (See page 312.)
22.
Use Exercise 20(d) to prove that eA exists for every A ‚ààMn√ón(C).
23.
Let x‚Ä≤ = Ax be a system of n linear diÔ¨Äerential equations, where x is
an n-tuple of diÔ¨Äerentiable functions x1(t), x2(t), . . . , xn(t) of the real
variable t, and A is an n √ó n coeÔ¨Écient matrix as in Exercise 15 of
Section 5.2. In contrast to that exercise, however, do not assume that
A is diagonalizable, but assume that the characteristic polynomial of A
splits. Let Œª1, Œª2, . . . , Œªk be the distinct eigenvalues of A.

516
Chap. 7
Canonical Forms
(a)
Prove that if u is the end vector of a cycle of generalized eigenvec-
tors of LA of length p and u corresponds to the eigenvalue Œªi, then
for any polynomial f(t) of degree less than p, the function
eŒªit[f(t)(A ‚àíŒªiI)p‚àí1 + f ‚Ä≤(t)(A ‚àíŒªiI)p‚àí2 + ¬∑ ¬∑ ¬∑ + f (p‚àí1)(t)]u
is a solution to the system x‚Ä≤ = Ax.
(b)
Prove that the general solution to x‚Ä≤ = Ax is a sum of the functions
of the form given in (a), where the vectors u are the end vectors of
the distinct cycles that constitute a Ô¨Åxed Jordan canonical basis
for LA.
24.
Use Exercise 23 to Ô¨Ånd the general solution to each of the following sys-
tems of linear equations, where x, y, and z are real-valued diÔ¨Äerentiable
functions of the real variable t.
(a)
x‚Ä≤ = 2x + y
y‚Ä≤ =
2y ‚àíz
z‚Ä≤ =
3z
(b)
x‚Ä≤ = 2x + y
y‚Ä≤ =
2y + z
z‚Ä≤ =
2z
7.3
THE MINIMAL POLYNOMIAL
The Cayley-Hamilton theorem (Theorem 5.23 p. 317) tells us that for any
linear operator T on an n-dimensional vector space, there is a polynomial
f(t) of degree n such that f(T) = T0, namely, the characteristic polynomial
of T. Hence there is a polynomial of least degree with this property, and this
degree is at most n. If g(t) is such a polynomial, we can divide g(t) by its
leading coeÔ¨Écient to obtain another polynomial p(t) of the same degree with
leading coeÔ¨Écient 1, that is, p(t) is a monic polynomial. (See Appendix E.)
DeÔ¨Ånition.
Let T be a linear operator on a Ô¨Ånite-dimensional vector
space. A polynomial p(t) is called a minimal polynomial of T if p(t) is a
monic polynomial of least positive degree for which p(T) = T0.
The preceding discussion shows that every linear operator on a Ô¨Ånite-
dimensional vector space has a minimal polynomial. The next result shows
that it is unique.
Theorem 7.12. Let p(t) be a minimal polynomial of a linear operator T
on a Ô¨Ånite-dimensional vector space V.
(a) For any polynomial g(t), if g(T) = T0, then p(t) divides g(t). In partic-
ular, p(t) divides the characteristic polynomial of T.
(b) The minimal polynomial of T is unique.
Proof. (a) Let g(t) be a polynomial for which g(T) = T0. By the division
algorithm for polynomials (Theorem E.1 of Appendix E, p. 562), there exist
polynomials q(t) and r(t) such that
g(t) = q(t)p(t) + r(t),
(1)

Sec. 7.3
The Minimal Polynomial
517
where r(t) has degree less than the degree of p(t). Substituting T into (1)
and using that g(T) = p(T) = T0, we have r(T) = T0. Since r(t) has degree
less than p(t) and p(t) is the minimal polynomial of T, r(t) must be the zero
polynomial. Thus (1) simpliÔ¨Åes to g(t) = q(t)p(t), proving (a).
(b) Suppose that p1(t) and p2(t) are each minimal polynomials of T. Then
p1(t) divides p2(t) by (a). Since p1(t) and p2(t) have the same degree, we have
that p2(t) = cp1(t) for some nonzero scalar c. Because p1(t) and p2(t) are
monic, c = 1; hence p1(t) = p2(t).
The minimal polynomial of a linear operator has an obvious analog for a
matrix.
DeÔ¨Ånition. Let A ‚ààMn√ón(F). The minimal polynomial p(t) of A is
the monic polynomial of least positive degree for which p(A) = O.
The following results are now immediate.
Theorem 7.13. Let T be a linear operator on a Ô¨Ånite-dimensional vector
space V, and let Œ≤ be an ordered basis for V. Then the minimal polynomial
of T is the same as the minimal polynomial of [T]Œ≤.
Proof. Exercise.
Corollary. For any A ‚ààMn√ón(F), the minimal polynomial of A is the
same as the minimal polynomial of LA.
Proof. Exercise.
In view of the preceding theorem and corollary, Theorem 7.12 and all
subsequent theorems in this section that are stated for operators are also
valid for matrices.
For the remainder of this section, we study primarily minimal polynomials
of operators (and hence matrices) whose characteristic polynomials split. A
more general treatment of minimal polynomials is given in Section 7.4.
Theorem 7.14. Let T be a linear operator on a Ô¨Ånite-dimensional vector
space V, and let p(t) be the minimal polynomial of T.
A scalar Œª is an
eigenvalue of T if and only if p(Œª) = 0. Hence the characteristic polynomial
and the minimal polynomial of T have the same zeros.
Proof. Let f(t) be the characteristic polynomial of T. Since p(t) divides
f(t), there exists a polynomial q(t) such that f(t) = q(t)p(t). If Œª is a zero of
p(t), then
f(Œª) = q(Œª)p(Œª) = q(Œª)¬∑0 = 0.
So Œª is a zero of f(t); that is, Œª is an eigenvalue of T.

518
Chap. 7
Canonical Forms
Conversely, suppose that Œª is an eigenvalue of T, and let x ‚ààV be an
eigenvector corresponding to Œª. By Exercise 22 of Section 5.1, we have
0 = T0(x) = p(T)(x) = p(Œª)x.
Since x Ã∏= 0, it follows that p(Œª) = 0, and so Œª is a zero of p(t).
The following corollary is immediate.
Corollary. Let T be a linear operator on a Ô¨Ånite-dimensional vector space
V with minimal polynomial p(t) and characteristic polynomial f(t). Suppose
that f(t) factors as
f(t) = (Œª1 ‚àít)n1(Œª2 ‚àít)n2 ¬∑ ¬∑ ¬∑ (Œªk ‚àít)nk,
where Œª1, Œª2, . . . , Œªk are the distinct eigenvalues of T. Then there exist inte-
gers m1, m2, . . . , mk such that 1 ‚â§mi ‚â§ni for all i and
p(t) = (t ‚àíŒª1)m1(t ‚àíŒª2)m2 ¬∑ ¬∑ ¬∑ (t ‚àíŒªk)mk.
Example 1
We compute the minimal polynomial of the matrix
A =
‚éõ
‚éù
3
‚àí1
0
0
2
0
1
‚àí1
2
‚éû
‚é†.
Since A has the characteristic polynomial
f(t) = det
‚éõ
‚éù
3 ‚àít
‚àí1
0
0
2 ‚àít
0
1
‚àí1
2 ‚àít
‚éû
‚é†= ‚àí(t ‚àí2)2(t ‚àí3),
the minimal polynomial of A must be either (t ‚àí2)(t ‚àí3) or (t ‚àí2)2(t ‚àí3)
by the corollary to Theorem 7.14. Substituting A into p(t) = (t ‚àí2)(t ‚àí3),
we Ô¨Ånd that p(A) = O; hence p(t) is the minimal polynomial of A.
‚ô¶
Example 2
Let T be the linear operator on R2 deÔ¨Åned by
T(a, b) = (2a + 5b, 6a + b)
and Œ≤ be the standard ordered basis for R2. Then
[T]Œ≤ =

2
5
6
1
	
,
and hence the characteristic polynomial of T is
f(t) = det

2 ‚àít
5
6
1 ‚àít
	
= (t ‚àí7)(t + 4).
Thus the minimal polynomial of T is also (t ‚àí7)(t + 4).
‚ô¶

Sec. 7.3
The Minimal Polynomial
519
Example 3
Let D be the linear operator on P2(R) deÔ¨Åned by D(g(x)) = g‚Ä≤(x), the deriva-
tive of g(x). We compute the minimal polynomial of T. Let Œ≤ be the standard
ordered basis for P2(R). Then
[D]Œ≤ =
‚éõ
‚éù
0
1
0
0
0
2
0
0
0
‚éû
‚é†,
and it follows that the characteristic polynomial of D is ‚àít3.
So by the
corollary to Theorem 7.14, the minimal polynomial of D is t, t2, or t3. Since
D2(x2) = 2 Ã∏= 0, it follows that D2 Ã∏= T0; hence the minimal polynomial of D
must be t3.
‚ô¶
In Example 3, it is easily veriÔ¨Åed that P2(R) is a D-cyclic subspace (of
itself).
Here the minimal and characteristic polynomials are of the same
degree. This is no coincidence.
Theorem 7.15. Let T be a linear operator on an n-dimensional vector
space V such that V is a T-cyclic subspace of itself. Then the characteristic
polynomial f(t) and the minimal polynomial p(t) have the same degree, and
hence f(t) = (‚àí1)np(t).
Proof. Since V is a T-cyclic space, there exists an x ‚ààV such that
Œ≤ = {x, T(x), . . . , Tn‚àí1(x)}
is a basis for V (Theorem 5.22 p. 315). Let
g(t) = a0 + a1t + ¬∑ ¬∑ ¬∑ + aktk,
be a polynomial of degree k < n. Then ak Ã∏= 0 and
g(T)(x) = a0x + a1T(x) + ¬∑ ¬∑ ¬∑ + akTk(x),
and so g(T)(x) is a linear combination of the vectors of Œ≤ having at least one
nonzero coeÔ¨Écient, namely, ak. Since Œ≤ is linearly independent, it follows
that g(T)(x) Ã∏= 0; hence g(T) Ã∏= T0. Therefore the minimal polynomial of T
has degree n, which is also the degree of the characteristic polynomial of T.
Theorem 7.15 gives a condition under which the degree of the minimal
polynomial of an operator is as large as possible. We now investigate the
other extreme. By Theorem 7.14, the degree of the minimal polynomial of an
operator must be greater than or equal to the number of distinct eigenvalues
of the operator.
The next result shows that the operators for which the
degree of the minimal polynomial is as small as possible are precisely the
diagonalizable operators.

520
Chap. 7
Canonical Forms
Theorem 7.16. Let T be a linear operator on a Ô¨Ånite-dimensional vector
space V. Then T is diagonalizable if and only if the minimal polynomial of T
is of the form
p(t) = (t ‚àíŒª1)(t ‚àíŒª2) ¬∑ ¬∑ ¬∑ (t ‚àíŒªk),
where Œª1, Œª2, . . . , Œªk are the distinct eigenvalues of T.
Proof. Suppose that T is diagonalizable. Let Œª1, Œª2, . . . , Œªk be the distinct
eigenvalues of T, and deÔ¨Åne
p(t) = (t ‚àíŒª1)(t ‚àíŒª2) ¬∑ ¬∑ ¬∑ (t ‚àíŒªk).
By Theorem 7.14, p(t) divides the minimal polynomial of T.
Let Œ≤ =
{v1, v2, . . . , vn} be a basis for V consisting of eigenvectors of T, and con-
sider any vi ‚ààŒ≤. Then (T ‚àíŒªjI)(vi) = 0 for some eigenvalue Œªj. Since t ‚àíŒªj
divides p(t), there is a polynomial qj(t) such that p(t) = qj(t)(t ‚àíŒªj). Hence
p(T)(vi) = qj(T)(T ‚àíŒªjI)(vi) = 0.
It follows that p(T) = T0, since p(T) takes each vector in a basis for V into
0. Therefore p(t) is the minimal polynomial of T.
Conversely, suppose that there are distinct scalars Œª1, Œª2, . . . , Œªk such that
the minimal polynomial p(t) of T factors as
p(t) = (t ‚àíŒª1)(t ‚àíŒª2) ¬∑ ¬∑ ¬∑ (t ‚àíŒªk).
By Theorem 7.14, the Œªi‚Äôs are eigenvalues of T.
We apply mathematical
induction on n = dim(V).
Clearly T is diagonalizable for n = 1.
Now
assume that T is diagonalizable whenever dim(V) < n for some n > 1, and
let dim(V) = n and W = R(T ‚àíŒªkI). Obviously W Ã∏= V, because Œªk is an
eigenvalue of T. If W = {0}, then T = ŒªkI, which is clearly diagonalizable.
So suppose that 0 < dim(W) < n. Then W is T-invariant, and for any x ‚ààW,
(T ‚àíŒª1I)(T ‚àíŒª2I) ¬∑ ¬∑ ¬∑ (T ‚àíŒªk‚àí1I)(x) = 0.
It follows that the minimal polynomial of TW divides the polynomial
(t ‚àíŒª1)(t ‚àíŒª2) ¬∑ ¬∑ ¬∑ (t ‚àíŒªk‚àí1).
Hence by the induction hypothesis, TW is
diagonalizable. Furthermore, Œªk is not an eigenvalue of TW by Theorem 7.14.
Therefore W ‚à©N(T ‚àíŒªkI) = {0}. Now let Œ≤1 = {v1, v2, . . . , vm} be a ba-
sis for W consisting of eigenvectors of TW (and hence of T), and let Œ≤2 =
{w1, w2, . . . , wp} be a basis for N(T‚àíŒªkI), the eigenspace of T corresponding
to Œªk. Then Œ≤1 and Œ≤2 are disjoint by the previous comment. Moreover,
m + p = n by the dimension theorem applied to T ‚àíŒªkI. We show that
Œ≤ = Œ≤1 ‚à™Œ≤2 is linearly independent.
Consider scalars a1, a2, . . . , am and
b1, b2, . . . , bp such that
a1v1 + a2v2 + ¬∑ ¬∑ ¬∑ + amvm + b1w1 + b2w2 + ¬∑ ¬∑ ¬∑ + bpwp = 0.

Sec. 7.3
The Minimal Polynomial
521
Let
x =
m

i=1
aivi
and
y =
p

i=1
biwi.
Then x ‚ààW, y ‚ààN(T ‚àíŒªkI), and x + y = 0. It follows that x = ‚àíy ‚àà
W ‚à©N(T ‚àíŒªkI), and therefore x = 0. Since Œ≤1 is linearly independent, we
have that a1 = a2 = ¬∑ ¬∑ ¬∑ = am = 0. Similarly, b1 = b2 = ¬∑ ¬∑ ¬∑ = bp = 0,
and we conclude that Œ≤ is a linearly independent subset of V consisting of n
eigenvectors. It follows that Œ≤ is a basis for V consisting of eigenvectors of T,
and consequently T is diagonalizable.
In addition to diagonalizable operators, there are methods for determin-
ing the minimal polynomial of any linear operator on a Ô¨Ånite-dimensional
vector space. In the case that the characteristic polynomial of the operator
splits, the minimal polynomial can be described using the Jordan canonical
form of the operator. (See Exercise 13.) In the case that the characteristic
polynomial does not split, the minimal polynomial can be described using the
rational canonical form, which we study in the next section. (See Exercise 7
of Section 7.4.)
Example 4
We determine all matrices A ‚ààM2√ó2(R) for which A2 ‚àí3A + 2I = O. Let
g(t) = t2 ‚àí3t + 2 = (t ‚àí1)(t ‚àí2). Since g(A) = O, the minimal polynomial
p(t) of A divides g(t). Hence the only possible candidates for p(t) are t ‚àí1,
t ‚àí2, and (t ‚àí1)(t ‚àí2). If p(t) = t ‚àí1 or p(t) = t ‚àí2, then A = I or A = 2I,
respectively. If p(t) = (t‚àí1)(t‚àí2), then A is diagonalizable with eigenvalues
1 and 2, and hence A is similar to

1
0
0
2
	
.
‚ô¶
Example 5
Let A ‚ààMn√ón(R) satisfy A3 = A. We show that A is diagonalizable. Let
g(t) = t3 ‚àít = t(t + 1)(t ‚àí1). Then g(A) = O, and hence the minimal
polynomial p(t) of A divides g(t). Since g(t) has no repeated factors, neither
does p(t). Thus A is diagonalizable by Theorem 7.16.
‚ô¶
Example 6
In Example 3, we saw that the minimal polynomial of the diÔ¨Äerential operator
D on P2(R) is t3. Hence, by Theorem 7.16, D is not diagonalizable.
‚ô¶

522
Chap. 7
Canonical Forms
EXERCISES
1.
Label the following statements as true or false. Assume that all vector
spaces are Ô¨Ånite-dimensional.
(a)
Every linear operator T has a polynomial p(t) of largest degree for
which p(T) = T0.
(b)
Every linear operator has a unique minimal polynomial.
(c)
The characteristic polynomial of a linear operator divides the min-
imal polynomial of that operator.
(d)
The minimal and the characteristic polynomials of any diagonal-
izable operator are equal.
(e)
Let T be a linear operator on an n-dimensional vector space V, p(t)
be the minimal polynomial of T, and f(t) be the characteristic
polynomial of T.
Suppose that f(t) splits.
Then f(t) divides
[p(t)]n.
(f)
The minimal polynomial of a linear operator always has the same
degree as the characteristic polynomial of the operator.
(g)
A linear operator is diagonalizable if its minimal polynomial splits.
(h)
Let T be a linear operator on a vector space V such that V is a
T-cyclic subspace of itself. Then the degree of the minimal poly-
nomial of T equals dim(V).
(i)
Let T be a linear operator on a vector space V such that T has n
distinct eigenvalues, where n = dim(V). Then the degree of the
minimal polynomial of T equals n.
2.
Find the minimal polynomial of each of the following matrices.
(a)

2
1
1
2
	
(b)

1
1
0
1
	
(c)
‚éõ
‚éù
4
‚àí14
5
1
‚àí4
2
1
‚àí6
4
‚éû
‚é†
(d)
‚éõ
‚éù
3
0
1
2
2
2
‚àí1
0
1
‚éû
‚é†
3.
For each linear operator T on V, Ô¨Ånd the minimal polynomial of T.
(a)
V = R2 and T(a, b) = (a + b, a ‚àíb)
(b)
V = P2(R) and T(g(x)) = g‚Ä≤(x) + 2g(x)
(c)
V = P2(R) and T(f(x)) = ‚àíxf ‚Ä≤‚Ä≤(x) + f ‚Ä≤(x) + 2f(x)
(d)
V = Mn√ón(R) and T(A) = At. Hint: Note that T2 = I.
4.
Determine which of the matrices and operators in Exercises 2 and 3 are
diagonalizable.
5.
Describe all linear operators T on R2 such that T is diagonalizable and
T3 ‚àí2T2 + T = T0.

Sec. 7.3
The Minimal Polynomial
523
6.
Prove Theorem 7.13 and its corollary.
7.
Prove the corollary to Theorem 7.14.
8.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space, and let
p(t) be the minimal polynomial of T. Prove the following results.
(a)
T is invertible if and only if p(0) Ã∏= 0.
(b)
If T is invertible and p(t) = tn + an‚àí1tn‚àí1 + ¬∑ ¬∑ ¬∑ + a1t + a0, then
T‚àí1 = ‚àí1
a0

Tn‚àí1 + an‚àí1Tn‚àí2 + ¬∑ ¬∑ ¬∑ + a2T + a1I

.
9.
Let T be a diagonalizable linear operator on a Ô¨Ånite-dimensional vector
space V. Prove that V is a T-cyclic subspace if and only if each of the
eigenspaces of T is one-dimensional.
10.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V, and
suppose that W is a T-invariant subspace of V. Prove that the minimal
polynomial of TW divides the minimal polynomial of T.
11.
Let g(t) be the auxiliary polynomial associated with a homogeneous lin-
ear diÔ¨Äerential equation with constant coeÔ¨Écients (as deÔ¨Åned in Section
2.7), and let V denote the solution space of this diÔ¨Äerential equation.
Prove the following results.
(a)
V is a D-invariant subspace, where D is the diÔ¨Äerentiation operator
on C‚àû.
(b)
The minimal polynomial of DV (the restriction of D to V) is g(t).
(c)
If the degree of g(t) is n, then the characteristic polynomial of DV
is (‚àí1)ng(t).
Hint: Use Theorem 2.32 (p. 135) for (b) and (c).
12.
Let D be the diÔ¨Äerentiation operator on P(R), the space of polynomials
over R. Prove that there exists no polynomial g(t) for which g(D) = T0.
Hence D has no minimal polynomial.
13.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space, and
suppose that the characteristic polynomial of T splits. Let Œª1, Œª2, . . . , Œªk
be the distinct eigenvalues of T, and for each i let pi be the order of the
largest Jordan block corresponding to Œªi in a Jordan canonical form of
T. Prove that the minimal polynomial of T is
(t ‚àíŒª1)p1(t ‚àíŒª2)p2 ¬∑ ¬∑ ¬∑ (t ‚àíŒªk)pk.
The following exercise requires knowledge of direct sums (see Section 5.2).

524
Chap. 7
Canonical Forms
14.
Let T be linear operator on a Ô¨Ånite-dimensional vector space V, and
let W1 and W2 be T-invariant subspaces of V such that V = W1 ‚äïW2.
Suppose that p1(t) and p2(t) are the minimal polynomials of TW1 and
TW2, respectively.
Prove or disprove that p1(t)p2(t) is the minimal
polynomial of T.
Exercise 15 uses the following deÔ¨Ånition.
DeÔ¨Ånition.
Let T be a linear operator on a Ô¨Ånite-dimensional vector
space V, and let x be a nonzero vector in V. The polynomial p(t) is called
a T-annihilator of x if p(t) is a monic polynomial of least degree for which
p(T)(x) = 0.
15.‚Ä† Let T be a linear operator on a Ô¨Ånite-dimensional vector space V, and
let x be a nonzero vector in V. Prove the following results.
(a)
The vector x has a unique T-annihilator.
(b)
The T-annihilator of x divides any polynomial g(t) for which
g(T) = T0.
(c)
If p(t) is the T-annihilator of x and W is the T-cyclic subspace
generated by x, then p(t) is the minimal polynomial of TW, and
dim(W) equals the degree of p(t).
(d)
The degree of the T-annihilator of x is 1 if and only if x is an
eigenvector of T.
16.
T be a linear operator on a Ô¨Ånite-dimensional vector space V, and let
W1 be a T-invariant subspace of V. Let x ‚ààV such that x /‚ààW1. Prove
the following results.
(a)
There exists a unique monic polynomial g1(t) of least positive de-
gree such that g1(T)(x) ‚ààW1.
(b)
If h(t) is a polynomial for which h(T)(x) ‚ààW1, then g1(t) divides
h(t).
(c)
g1(t) divides the minimal and the characteristic polynomials of T.
(d)
Let W2 be a T-invariant subspace of V such that W2 ‚äÜW1, and
let g2(t) be the unique monic polynomial of least degree such that
g2(T)(x) ‚ààW2. Then g1(t) divides g2(t).
7.4‚àó
THE RATIONAL CANONICAL FORM
Until now we have used eigenvalues, eigenvectors, and generalized eigenvec-
tors in our analysis of linear operators with characteristic polynomials that
split. In general, characteristic polynomials need not split, and indeed, oper-
ators need not have eigenvalues! However, the unique factorization theorem
for polynomials (see Appendix E) guarantees that the characteristic polyno-
mial f(t) of any linear operator T on an n-dimensional vector space factors

Sec. 7.4
The Rational Canonical Form
525
uniquely as
f(t) = (‚àí1)n(œÜ1(t))n1(œÜ2(t))n2 ¬∑ ¬∑ ¬∑ (œÜk(t))nk,
where the œÜi(t)‚Äôs (1 ‚â§i ‚â§k) are distinct irreducible monic polynomials and
the ni‚Äôs are positive integers. In the case that f(t) splits, each irreducible
monic polynomial factor is of the form œÜi(t) = t‚àíŒªi, where Œªi is an eigenvalue
of T, and there is a one-to-one correspondence between eigenvalues of T and
the irreducible monic factors of the characteristic polynomial.
In general,
eigenvalues need not exist, but the irreducible monic factors always exist. In
this section, we establish structure theorems based on the irreducible monic
factors of the characteristic polynomial instead of eigenvalues.
In this context, the following deÔ¨Ånition is the appropriate replacement for
eigenspace and generalized eigenspace.
DeÔ¨Ånition.
Let T be a linear operator on a Ô¨Ånite-dimensional vector
space V with characteristic polynomial
f(t) = (‚àí1)n(œÜ1(t))n1(œÜ2(t))n2 ¬∑ ¬∑ ¬∑ (œÜk(t))nk,
where the œÜi(t)‚Äôs (1 ‚â§i ‚â§k) are distinct irreducible monic polynomials and
the ni‚Äôs are positive integers. For 1 ‚â§i ‚â§k, we deÔ¨Åne the subset KœÜi of V by
KœÜi = {x ‚ààV: (œÜi(T))p(x) = 0 for some positive integer p}.
We show that each KœÜi is a nonzero T-invariant subspace of V. Note that
if œÜi(t) = t ‚àíŒª is of degree one, then KœÜi is the generalized eigenspace of T
corresponding to the eigenvalue Œª.
Having obtained suitable generalizations of the related concepts of eigen-
value and eigenspace, our next task is to describe a canonical form of a linear
operator suitable to this context. The one that we study is called the rational
canonical form. Since a canonical form is a description of a matrix represen-
tation of a linear operator, it can be deÔ¨Åned by specifying the form of the
ordered bases allowed for these representations.
Here the bases of interest naturally arise from the generators of certain
cyclic subspaces. For this reason, the reader should recall the deÔ¨Ånition of
a T-cyclic subspace generated by a vector and Theorem 5.22 (p. 315). We
brieÔ¨Çy review this concept and introduce some new notation and terminology.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V, and let
x be a nonzero vector in V. We use the notation Cx for the T-cyclic subspace
generated by x. Recall (Theorem 5.22) that if dim(Cx) = k, then the set
{x, T(x), T2(x), . . . , Tk‚àí1(x)}
is an ordered basis for Cx. To distinguish this basis from all other ordered
bases for Cx, we call it the T-cyclic basis generated by x and denote it by

526
Chap. 7
Canonical Forms
Œ≤x. Let A be the matrix representation of the restriction of T to Cx relative
to the ordered basis Œ≤x. Recall from the proof of Theorem 5.22 that
A =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
0
0
¬∑ ¬∑ ¬∑
0
‚àía0
1
0
¬∑ ¬∑ ¬∑
0
‚àía1
0
1
¬∑ ¬∑ ¬∑
0
‚àía2
...
...
...
...
0
0
¬∑ ¬∑ ¬∑
1
‚àíak‚àí1
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
,
where
a0x + a1T(x) + ¬∑ ¬∑ ¬∑ + ak‚àí1Tk‚àí1(x) + Tk(x) = 0.
Furthermore, the characteristic polynomial of A is given by
det(A ‚àítI) = (‚àí1)k(a0 + a1t + ¬∑ ¬∑ ¬∑ + ak‚àí1tk‚àí1 + tk).
The matrix A is called the companion matrix of the monic polynomial
h(t) = a0 + a1t + ¬∑ ¬∑ ¬∑ + ak‚àí1tk‚àí1 + tk. Every monic polynomial has a com-
panion matrix, and the characteristic polynomial of the companion matrix of
a monic polynomial g(t) of degree k is equal to (‚àí1)kg(t). (See Exercise 19
of Section 5.4.) By Theorem 7.15 (p. 519), the monic polynomial h(t) is also
the minimal polynomial of A. Since A is the matrix representation of the
restriction of T to Cx, h(t) is also the minimal polynomial of this restriction.
By Exercise 15 of Section 7.3, h(t) is also the T-annihilator of x.
It is the object of this section to prove that for every linear operator T
on a Ô¨Ånite-dimensional vector space V, there exists an ordered basis Œ≤ for V
such that the matrix representation [T]Œ≤ is of the form
‚éõ
‚éú
‚éú
‚éú
‚éù
C1
O
¬∑ ¬∑ ¬∑
O
O
C2
¬∑ ¬∑ ¬∑
O
...
...
...
O
O
¬∑ ¬∑ ¬∑
Cr
‚éû
‚éü
‚éü
‚éü
‚é†,
where each Ci is the companion matrix of a polynomial (œÜ(t))m such that œÜ(t)
is a monic irreducible divisor of the characteristic polynomial of T and m is
a positive integer. A matrix representation of this kind is called a rational
canonical form of T. We call the accompanying basis a rational canonical
basis for T.
The next theorem is a simple consequence of the following lemma, which
relies on the concept of T-annihilator, introduced in the Exercises of Sec-
tion 7.3.
Lemma. Let T be a linear operator on a Ô¨Ånite-dimensional vector space
V, let x be a nonzero vector in V, and suppose that the T-annihilator of x
is of the form (œÜ(t))p for some irreducible monic polynomial œÜ(t). Then œÜ(t)
divides the minimal polynomial of T, and x ‚ààKœÜ.

Sec. 7.4
The Rational Canonical Form
527
Proof. By Exercise 15(b) of Section 7.3, (œÜ(t))p divides the minimal poly-
nomial of T. Therefore œÜ(t) divides the minimal polynomial of T. Further-
more, x ‚ààKœÜ by the deÔ¨Ånition of KœÜ.
Theorem 7.17. Let T be a linear operator on a Ô¨Ånite-dimensional vector
space V, and let Œ≤ be an ordered basis for V. Then Œ≤ is a rational canonical
basis for T if and only if Œ≤ is the disjoint union of T-cyclic bases Œ≤vi, where
each vi lies in KœÜ for some irreducible monic divisor œÜ(t) of the characteristic
polynomial of T.
Proof. Exercise.
Example 1
Suppose that T is a linear operator on R8 and
Œ≤ = {v1, v2, v3, v4, v5, v6, v7, v8}
is a rational canonical basis for T such that
C = [T]Œ≤ =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
0
‚àí3
0
0
0
0
0
0
1
1
0
0
0
0
0
0
0
0
0
0
0
‚àí1
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
‚àí2
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
‚àí1
0
0
0
0
0
0
1
0
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
is a rational canonical form of T. In this case, the submatrices C1, C2, and
C3 are the companion matrices of the polynomials œÜ1(t), (œÜ2(t))2, and œÜ2(t),
respectively, where
œÜ1(t) = t2 ‚àít + 3
and
œÜ2(t) = t2 + 1.
In the context of Theorem 7.17, Œ≤ is the disjoint union of the T-cyclic bases;
that is,
Œ≤ = Œ≤v1 ‚à™Œ≤v3 ‚à™Œ≤v7
= {v1, v2} ‚à™{v3, v4, v5, v6} ‚à™{v7, v8}.
By Exercise 40 of Section 5.4, the characteristic polynomial f(t) of T is the
product of the characteristic polynomials of the companion matrices:
f(t) = œÜ1(t)(œÜ2(t))2œÜ2(t) = œÜ1(t)(œÜ2(t))3.
‚ô¶

528
Chap. 7
Canonical Forms
The rational canonical form C of the operator T in Example 1 is con-
structed from matrices of the form Ci, each of which is the companion matrix
of some power of a monic irreducible divisor of the characteristic polynomial
of T. Furthermore, each such divisor is used in this way at least once.
In the course of showing that every linear operator T on a Ô¨Ånite dimen-
sional vector space has a rational canonical form C, we show that the com-
panion matrices Ci that constitute C are always constructed from powers of
the monic irreducible divisors of the characteristic polynomial of T. A key
role in our analysis is played by the subspaces KœÜ, where œÜ(t) is an irreducible
monic divisor of the minimal polynomial of T. Since the minimal polynomial
of an operator divides the characteristic polynomial of the operator, every ir-
reducible divisor of the former is also an irreducible divisor of the latter. We
eventually show that the converse is also true; that is, the minimal polynomial
and the characteristic polynomial have the same irreducible divisors.
We begin with a result that lists several properties of irreducible divisors
of the minimal polynomial. The reader is advised to review the deÔ¨Ånition of
T-annihilator and the accompanying Exercise 15 of Section 7.3.
Theorem 7.18. Let T be a linear operator on a Ô¨Ånite-dimensional vector
space V, and suppose that
p(t) = (œÜ1(t))m1(œÜ2(t))m2 ¬∑ ¬∑ ¬∑ (œÜk(t))mk
is the minimal polynomial of T, where the œÜi(t)‚Äôs (1 ‚â§i ‚â§k) are the distinct
irreducible monic factors of p(t) and the mi‚Äôs are positive integers. Then the
following statements are true.
(a) KœÜi is a nonzero T-invariant subspace of V for each i.
(b) If x is a nonzero vector in some KœÜi, then the T-annihilator of x is of
the form (œÜi(t))p for some integer p.
(c) KœÜi ‚à©KœÜj = {0} for i Ã∏= j. .
(d) KœÜi is invariant under œÜj(T) for i Ã∏= j, and the restriction of œÜj(T) to
KœÜi is one-to-one and onto.
(e) KœÜi = N((œÜi(T))mi) for each i.
Proof. If k = 1, then (a), (b), and (e) are obvious, while (c) and (d) are
vacuously true. Now suppose that k > 1.
(a) The proof that KœÜi is a T-invariant subspace of V is left as an exer-
cise. Let fi(t) be the polynomial obtained from p(t) by omitting the factor
(œÜi(t))mi. To prove that KœÜi is nonzero, Ô¨Årst observe that fi(t) is a proper di-
visor of p(t); therefore there exists a vector z ‚ààV such that x = fi(T)(z) Ã∏= 0.
Then x ‚ààKœÜi because
(œÜi(T))mi(x) = (œÜi(T))mifi(T)(z) = p(T)(z) = 0.
(b) Assume the hypothesis. Then (œÜi(T))q(x) = 0 for some positive in-
teger q. Hence the T-annihilator of x divides (œÜi(t))q by Exercise 15(b) of
Section 7.3, and the result follows.

Sec. 7.4
The Rational Canonical Form
529
(c) Assume i Ã∏= j. Let x ‚ààKœÜi ‚à©KœÜj, and suppose that x Ã∏= 0. By (b), the
T-annihilator of x is a power of both œÜi(t) and œÜj(t). But this is impossible
because œÜi(t) and œÜj(t) are relatively prime (see Appendix E). We conclude
that x = 0.
(d) Assume i Ã∏= j. Since KœÜi is T-invariant, it is also œÜj(T)-invariant.
Suppose that œÜj(T)(x) = 0 for some x ‚ààKœÜi. Then x ‚ààKœÜi ‚à©KœÜj = {0}
by (c). Therefore the restriction of œÜj(T) to KœÜi is one-to-one. Since V is
Ô¨Ånite-dimensional, this restriction is also onto.
(e) Suppose that 1 ‚â§i ‚â§k. Clearly, N((œÜi(T))mi) ‚äÜKœÜi. Let fi(t) be the
polynomial deÔ¨Åned in (a). Since fi(t) is a product of polynomials of the form
œÜj(t) for j Ã∏= i, we have by (d) that the restriction of fi(T) to KœÜi is onto.
Let x ‚ààKœÜi. Then there exists y ‚ààKœÜi such that fi(T)(y) = x. Therefore
((œÜi(T))mi)(x) = ((œÜi(T))mi)fi(T)(y) = p(T)(y) = 0,
and hence x ‚ààN((œÜi(T))mi). Thus KœÜi = N((œÜi(T))mi).
Since a rational canonical basis for an operator T is obtained from a union
of T-cyclic bases, we need to know when such a union is linearly independent.
The next major result, Theorem 7.19, reduces this problem to the study of
T-cyclic bases within KœÜ, where œÜ(t) is an irreducible monic divisor of the
minimal polynomial of T. We begin with the following lemma.
Lemma. Let T be a linear operator on a Ô¨Ånite-dimensional vector space
V, and suppose that
p(t) = (œÜ1(t))m1(œÜ2(t))m2 ¬∑ ¬∑ ¬∑ (œÜk(t))mk
is the minimal polynomial of T, where the œÜi‚Äôs (1 ‚â§i ‚â§k) are the dis-
tinct irreducible monic factors of p(t) and the mi‚Äôs are positive integers. For
1 ‚â§i ‚â§k, let vi ‚ààKœÜi be such that
v1 + v2 + ¬∑ ¬∑ ¬∑ + vk = 0.
(2)
Then vi = 0 for all i.
Proof. The result is trivial if k = 1, so suppose that k > 1. Consider
any i. Let fi(t) be the polynomial obtained from p(t) by omitting the factor
(œÜi(t))mi. As a consequence of Theorem 7.18, fi(T) is one-to-one on KœÜi, and
fi(T)(vj) = 0 for i Ã∏= j. Thus, applying fi(T) to (2), we obtain fi(T)(vi) = 0,
from which it follows that vi = 0.
Theorem 7.19. Let T be a linear operator on a Ô¨Ånite-dimensional vector
space V, and suppose that
p(t) = (œÜ1(t))m1(œÜ2(t))m2 ¬∑ ¬∑ ¬∑ (œÜk(t))mk

530
Chap. 7
Canonical Forms
is the minimal polynomial of T, where the œÜi‚Äôs (1 ‚â§i ‚â§k) are the dis-
tinct irreducible monic factors of p(t) and the mi‚Äôs are positive integers. For
1 ‚â§i ‚â§k, let Si be a linearly independent subset of KœÜi. Then
(a) Si ‚à©Sj = ‚àÖfor i Ã∏= j
(b) S1 ‚à™S2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Sk is linearly independent.
Proof. If k = 1, then (a) is vacuously true and (b) is obvious.
Now
suppose that k > 1. Then (a) follows immediately from Theorem 7.18(c).
Furthermore, the proof of (b) is identical to the proof of Theorem 5.8 (p. 267)
with the eigenspaces replaced by the subspaces KœÜi.
In view of Theorem 7.19, we can focus on bases of individual spaces of
the form KœÜ(t), where œÜ(t) is an irreducible monic divisor of the minimal
polynomial of T. The next several results give us ways to construct bases for
these spaces that are unions of T-cyclic bases. These results serve the dual
purposes of leading to the existence theorem for the rational canonical form
and of providing methods for constructing rational canonical bases.
For Theorems 7.20 and 7.21 and the latter‚Äôs corollary, we Ô¨Åx a linear
operator T on a Ô¨Ånite-dimensional vector space V and an irreducible monic
divisor œÜ(t) of the minimal polynomial of T.
Theorem 7.20. Let v1, v2, . . . , vk be distinct vectors in KœÜ such that
S1 = Œ≤v1 ‚à™Œ≤v2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≤vk
is linearly independent. For each i, choose wi ‚ààV such that œÜ(T)(wi) = vi.
Then
S2 = Œ≤w1 ‚à™Œ≤w2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≤wk
is also linearly independent.
Proof. Consider any linear combination of vectors in S2 that sums to zero,
say,
k

i=1
ni

j=0
aijTj(wi) = 0.
(3)
For each i, let fi(t) be the polynomial deÔ¨Åned by
fi(t) =
ni

j=0
aijtj.
Then (3) can be rewritten as
k

i=1
fi(T)(wi) = 0.
(4)

Sec. 7.4
The Rational Canonical Form
531
Apply œÜ(T) to both sides of (4) to obtain
k

i=1
œÜ(T)fi(T)(wi) =
k

i=1
fi(T)œÜ(T)(wi) =
k

i=1
fi(T)(vi) = 0.
This last sum can be rewritten as a linear combination of the vectors in S1
so that each fi(T)(vi) is a linear combination of the vectors in Œ≤vi. Since S1
is linearly independent, it follows that
fi(T)(vi) = 0
for all i.
Therefore the T-annihilator of vi divides fi(t) for all i. (See Exercise 15 of
Section 7.3.) By Theorem 7.18(b), œÜ(t) divides the T-annihilator of vi, and
hence œÜ(t) divides fi(t) for all i. Thus, for each i, there exists a polynomial
gi(t) such that fi(t) = gi(t)œÜ(t). So (4) becomes
k

i=1
gi(T)œÜ(T)(wi) =
k

i=1
gi(T)(vi) = 0.
Again, linear independence of S1 requires that
fi(T)(wi) = gi(T)(vi) = 0
for all i.
But fi(T)(wi) is the result of grouping the terms of the linear combination
in (3) that arise from the linearly independent set Œ≤wi. We conclude that for
each i, aij = 0 for all j. Therefore S2 is linearly independent.
We now show that KœÜ has a basis consisting of a union of T-cycles.
Lemma. Let W be a T-invariant subspace of KœÜ, and let Œ≤ be a basis for
W. Then the following statements are true.
(a) Suppose that x ‚ààN(œÜ(T)), but x /‚ààW. Then Œ≤ ‚à™Œ≤x is linearly inde-
pendent.
(b) For some w1, w2, . . . , ws in N(œÜ(T)), Œ≤ can be extended to the linearly
independent set
Œ≤‚Ä≤ = Œ≤ ‚à™Œ≤w1 ‚à™Œ≤w2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≤ws,
whose span contains N(œÜ(T)).
Proof. (a) Let Œ≤ = {v1, v2, . . . , vk}, and suppose that
k

i=1
aivi + z = 0
and
z =
d‚àí1

j=0
bjTj(x),

532
Chap. 7
Canonical Forms
where d is the degree of œÜ(t). Then z ‚ààCx ‚à©W, and hence Cz ‚äÜCx ‚à©W.
Suppose that z Ã∏= 0. Then z has œÜ(t) as its T-annihilator, and therefore
d = dim(Cz) ‚â§dim(Cx ‚à©W) ‚â§dim(Cx) = d.
It follows that Cx ‚à©W = Cx, and consequently x ‚ààW, contrary to hypothesis.
Therefore z = 0, from which it follows that bj = 0 for all j.
Since Œ≤ is
linearly independent, it follows that ai = 0 for all i. Thus Œ≤ ‚à™Œ≤x is linearly
independent.
(b) Suppose that W does not contain N(œÜ(T)). Choose a vector w1 ‚àà
N(œÜ(t)) that is not in W. By (a), Œ≤1 = Œ≤ ‚à™Œ≤w1 is linearly independent.
Let W1 = span(Œ≤1). If W1 does not contain N(œÜ(t)), choose a vector w2 in
N(œÜ(t)), but not in W1, so that Œ≤2 = Œ≤1 ‚à™Œ≤w2 = Œ≤ ‚à™Œ≤w1 ‚à™Œ≤w2 is linearly inde-
pendent. Continuing this process, we eventually obtain vectors w1, w2, . . . , ws
in N(œÜ(T)) such that the union
Œ≤‚Ä≤ = Œ≤ ‚à™Œ≤w1 ‚à™Œ≤w2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≤ws
is a linearly independent set whose span contains N(œÜ(T)).
Theorem 7.21. If the minimal polynomial of T is of the form p(t) =
(œÜ(t))m, then there exists a rational canonical basis for T.
Proof. The proof is by mathematical induction on m. Suppose that m = 1.
Apply (b) of the lemma to W = {0} to obtain a linearly independent subset
of V of the form Œ≤v1 ‚à™Œ≤v2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≤vk, whose span contains N(œÜ(T)). Since
V = N(œÜ(T)), this set is a rational canonical basis for V.
Now suppose that, for some integer m > 1, the result is valid whenever the
minimal polynomial of T is of the form (œÜ(T))k, where k < m, and assume
that the minimal polynomial of T is p(t) = (œÜ(t))m. Let r = rank(œÜ(T)).
Then R(œÜ(T)) is a T-invariant subspace of V, and the restriction of T to this
subspace has (œÜ(t))m‚àí1 as its minimal polynomial. Therefore we may apply
the induction hypothesis to obtain a rational canonical basis for the restriction
of T to R(T). Suppose that v1, v2, . . . , vk are the generating vectors of the
T-cyclic bases that constitute this rational canonical basis. For each i, choose
wi in V such that vi = œÜ(T)(wi). By Theorem 7.20, the union Œ≤ of the sets Œ≤wi
is linearly independent. Let W = span(Œ≤). Then W contains R(œÜ(T)). Apply
(b) of the lemma and adjoin additional T-cyclic bases Œ≤wk+1, Œ≤wk+2, . . . , Œ≤ws
to Œ≤, if necessary, where wi is in N(œÜ(T)) for i ‚â•k, to obtain a linearly
independent set
Œ≤‚Ä≤ = Œ≤w1 ‚à™Œ≤w2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≤wk ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≤ws
whose span W‚Ä≤ contains both W and N(œÜ(T)).

Sec. 7.4
The Rational Canonical Form
533
We show that W‚Ä≤ = V. Let U denote the restriction of œÜ(T) to W‚Ä≤, which
is œÜ(T)-invariant. By the way in which W‚Ä≤ was obtained from R(œÜ(T)), it
follows that R(U) = R(œÜ(T)) and N(U) = N(œÜ(T)). Therefore
dim(W‚Ä≤) = rank(U) + nullity(U)
= rank(œÜ(T)) + nullity(œÜ(T))
= dim(V).
Thus W‚Ä≤ = V, and Œ≤‚Ä≤ is a rational canonical basis for T.
Corollary. KœÜ has a basis consisting of the union of T-cyclic bases.
Proof. Apply Theorem 7.21 to the restriction of T to KœÜ.
We are now ready to study the general case.
Theorem 7.22. Every linear operator on a Ô¨Ånite-dimensional vector space
has a rational canonical basis and, hence, a rational canonical form.
Proof. Let T be a linear operator on a Ô¨Ånite-dimensional vector space V,
and let p(t) = (œÜ1(t))m1(œÜ2(t))m2 ¬∑ ¬∑ ¬∑ (œÜk(t))mk be the minimal polynomial
of T, where the œÜi(t)‚Äôs are the distinct irreducible monic factors of p(t) and
mi > 0 for all i. The proof is by mathematical induction on k. The case
k = 1 is proved in Theorem 7.21.
Suppose that the result is valid whenever the minimal polynomial contains
fewer than k distinct irreducible factors for some k > 1, and suppose that p(t)
contains k distinct factors. Let U be the restriction of T to the T-invariant
subspace W = R((œÜk(T)mk), and let q(t) be the minimal polynomial of U.
Then q(t) divides p(t) by Exercise 10 of Section 7.3. Furthermore, œÜk(t) does
not divide q(t). For otherwise, there would exist a nonzero vector x ‚ààW such
that œÜk(U)(x) = 0 and a vector y ‚ààV such that x = (œÜk(T))mk(y). It follows
that (œÜk(T))mk+1(y) = 0, and hence y ‚ààKœÜk and x = (œÜk(T))mk(y) =
0 by Theorem 7.18(e), a contradiction.
Thus q(t) contains fewer than k
distinct irreducible divisors. So by the induction hypothesis, U has a rational
canonical basis Œ≤1 consisting of a union of U-cyclic bases (and hence T-cyclic
bases) of vectors from some of the subspaces KœÜi, 1 ‚â§i ‚â§k ‚àí1. By the
corollary to Theorem 7.21, KœÜk has a basis Œ≤2 consisting of a union of T-
cyclic bases. By Theorem 7.19, Œ≤1 and Œ≤2 are disjoint, and Œ≤ = Œ≤1 ‚à™Œ≤2 is
linearly independent. Let s denote the number of vectors in Œ≤. Then
s = dim(R((œÜk(T))mk)) + dim(KœÜk)
= rank((œÜk(T))mk) + nullity((œÜk(T))mk)
= n.
We conclude that Œ≤ is a basis for V. Therefore Œ≤ is a rational canonical basis,
and T has a rational canonical form.

534
Chap. 7
Canonical Forms
In our study of the rational canonical form, we relied on the minimal
polynomial. We are now able to relate the rational canonical form to the
characteristic polynomial.
Theorem 7.23. Let T be a linear operator on an n-dimensional vector
space V with characteristic polynomial
f(t) = (‚àí1)n(œÜ1(t))n1(œÜ2(t))n2 ¬∑ ¬∑ ¬∑ (œÜk(t))nk,
where the œÜi(t)‚Äôs (1 ‚â§i ‚â§k) are distinct irreducible monic polynomials and
the ni‚Äôs are positive integers. Then the following statements are true.
(a) œÜ1(t), œÜ2(t), . . . , œÜk(t) are the irreducible monic factors of the minimal
polynomial.
(b) For each i, dim(KœÜi) = dini, where di is the degree of œÜi(t).
(c) If Œ≤ is a rational canonical basis for T, then Œ≤i = Œ≤ ‚à©KœÜi is a basis for
KœÜi for each i.
(d) If Œ≥i is a basis for KœÜi for each i, then Œ≥ = Œ≥1 ‚à™Œ≥2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≥k is a basis
for V. In particular, if each Œ≥i is a disjoint union of T-cyclic bases, then
Œ≥ is a rational canonical basis for T.
Proof. (a) By Theorem 7.22, T has a rational canonical form C.
By
Exercise 40 of Section 5.4, the characteristic polynomial of C, and hence of
T, is the product of the characteristic polynomials of the companion matrices
that compose C. Therefore each irreducible monic divisor œÜi(t) of f(t) divides
the characteristic polynomial of at least one of the companion matrices, and
hence for some integer p, (œÜi(t))p is the T-annihilator of a nonzero vector of
V. We conclude that (œÜi(t))p, and so œÜi(t), divides the minimal polynomial
of T. Conversely, if œÜ(t) is an irreducible monic polynomial that divides the
minimal polynomial of T, then œÜ(t) divides the characteristic polynomial of
T because the minimal polynomial divides the characteristic polynomial.
(b), (c), and (d) Let C = [T]Œ≤, which is a rational canonical form of T.
Consider any i, (1 ‚â§i ‚â§k). Since f(t) is the product of the characteristic
polynomials of the companion matrices that compose C, we may multiply
those characteristic polynomials that arise from the T-cyclic bases in Œ≤i to
obtain the factor (œÜi(t))ni of f(t). Since this polynomial has degree nidi, and
the union of these bases is a linearly independent subset Œ≤i of KœÜi, we have
nidi ‚â§dim(KœÜi).
Furthermore, n =
k

i=1
dini, because this sum is equal to the degree of f(t).
Now let s denote the number of vectors in Œ≥. By Theorem 7.19, Œ≥ is linearly
independent, and therefore
n =
k

i=1
dini ‚â§
k

i=1
dim(KœÜi) = s ‚â§n.

Sec. 7.4
The Rational Canonical Form
535
Hence n = s, and dini = dim(KœÜi) for all i. It follows that Œ≥ is a basis for V
and Œ≤i is a basis for KœÜi for each i.
Uniqueness of the Rational Canonical Form
Having shown that a rational canonical form exists, we are now in a po-
sition to ask about the extent to which it is unique. Certainly, the rational
canonical form of a linear operator T can be modiÔ¨Åed by permuting the T-
cyclic bases that constitute the corresponding rational canonical basis. This
has the eÔ¨Äect of permuting the companion matrices that make up the rational
canonical form. As in the case of the Jordan canonical form, we show that
except for these permutations, the rational canonical form is unique, although
the rational canonical bases are not.
To simplify this task, we adopt the convention of ordering every rational
canonical basis so that all the T-cyclic bases associated with the same irre-
ducible monic divisor of the characteristic polynomial are grouped together.
Furthermore, within each such grouping, we arrange the T-cyclic bases in
decreasing order of size. Our task is to show that, subject to this order, the
rational canonical form of a linear operator is unique up to the arrangement
of the irreducible monic divisors.
As in the case of the Jordan canonical form, we introduce arrays of dots
from which we can reconstruct the rational canonical form. For the Jordan
canonical form, we devised a dot diagram for each eigenvalue of the given
operator. In the case of the rational canonical form, we deÔ¨Åne a dot diagram
for each irreducible monic divisor of the characteristic polynomial of the given
operator. A proof that the resulting dot diagrams are completely determined
by the operator is also a proof that the rational canonical form is unique.
In what follows, T is a linear operator on a Ô¨Ånite-dimensional vector space
with rational canonical basis Œ≤; œÜ(t) is an irreducible monic divisor of the char-
acteristic polynomial of T; Œ≤v1, Œ≤v2, . . . , Œ≤vk are the T-cyclic bases of Œ≤ that
are contained in KœÜ; and d is the degree of œÜ(t). For each j, let (œÜ(t))pj be the
annihilator of vj. This polynomial has degree dpj; therefore, by Exercise 15
of Section 7.3, Œ≤vj contains dpj vectors. Furthermore, p1 ‚â•p2 ‚â•¬∑ ¬∑ ¬∑ ‚â•pk
since the T-cyclic bases are arranged in decreasing order of size. We deÔ¨Åne
the dot diagram of œÜ(t) to be the array consisting of k columns of dots with
pj dots in the jth column, arranged so that the jth column begins at the top
and terminates after pj dots. For example, if k = 3, p1 = 4, p2 = 2, and
p3 = 2, then the dot diagram is
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
Although each column of a dot diagram corresponds to a T-cyclic basis

536
Chap. 7
Canonical Forms
Œ≤vi in KœÜ, there are fewer dots in the column than there are vectors in the
basis.
Example 2
Recall the linear operator T of Example 1 with the rational canonical basis
Œ≤ and the rational canonical form C = [T]Œ≤. Since there are two irreducible
monic divisors of the characteristic polynomial of T, œÜ1(t) = t2 ‚àít + 3 and
œÜ2(t) = t2 + 1, there are two dot diagrams to consider.
Because œÜ1(t) is
the T-annihilator of v1 and Œ≤v1 is a basis for KœÜ1, the dot diagram for œÜ1(t)
consists of a single dot. The other two T cyclic bases, Œ≤v3 and Œ≤v7, lie in KœÜ2.
Since v3 has T-annihilator (œÜ2(t))2 and v7 has T-annihilator œÜ2(t), in the dot
diagram of œÜ2(t) we have p1 = 2 and p2 = 1. These diagrams are as follows:
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
Dot diagram for œÜ1(t)
Dot diagram for œÜ2(t)
‚ô¶
In practice, we obtain the rational canonical form of a linear operator
from the information provided by dot diagrams. This is illustrated in the
next example.
Example 3
Let T be a linear operator on a Ô¨Ånite-dimensional vector space over R, and
suppose that the irreducible monic divisors of the characteristic polynomial
of T are
œÜ1(t) = t ‚àí1,
œÜ2(t) = t2 + 2,
and
œÜ3(t) = t2 + t + 1.
Suppose, furthermore, that the dot diagrams associated with these divisors
are as follows:
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
Diagram for œÜ1(t)
Diagram for œÜ2(t)
Diagram for œÜ3(t)
Since the dot diagram for œÜ1(t) has two columns, it contributes two companion
matrices to the rational canonical form. The Ô¨Årst column has two dots, and
therefore corresponds to the 2 √ó 2 companion matrix of (œÜ1(t))2 = (t ‚àí1)2.
The second column, with only one dot, corresponds to the 1 √ó 1 companion
matrix of œÜ1(t) = t ‚àí1. These two companion matrices are given by
C1 =
0
‚àí1
1
2
	
and
C2 =
1
.
The dot diagram for œÜ2(t) = t2 +2 consists of two columns. each containing a
single dot; hence this diagram contributes two copies of the 2 √ó 2 companion

Sec. 7.4
The Rational Canonical Form
537
matrix for œÜ2(t), namely,
C3 = C4 =

0
‚àí2
1
0
	
.
The dot diagram for œÜ3(t) = t2 + t + 1 consists of a single column with a
single dot contributing the single 2 √ó 2 companion matrix
C5 =

0
‚àí1
1
‚àí1
	
.
Therefore the rational canonical form of T is the 9 √ó 9 matrix
C =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
C1
O
O
O
O
O
C2
O
O
O
O
O
C3
O
O
O
O
O
C4
O
O
O
O
O
C5
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
=
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
0
‚àí1
0
0
0
0
0
0
0
1
2
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
‚àí2
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
‚àí2
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
‚àí1
0
0
0
0
0
0
0
1
‚àí1
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
.
‚ô¶
We return to the general problem of Ô¨Ånding dot diagrams. As we did
before, we Ô¨Åx a linear operator T on a Ô¨Ånite-dimensional vector space and an
irreducible monic divisor œÜ(t) of the characteristic polynomial of T. Let U
denote the restriction of the linear operator œÜ(T) to KœÜ. By Theorem 7.18(d),
Uq = T0 for some positive integer q. Consequently, by Exercise 12 of Sec-
tion 7.2, the characteristic polynomial of U is (‚àí1)mtm, where m = dim(KœÜ).
Therefore KœÜ is the generalized eigenspace of U corresponding to Œª = 0, and
U has a Jordan canonical form. The dot diagram associated with the Jordan
canonical form of U gives us a key to understanding the dot diagram of T
that is associated with œÜ(t). We now relate the two diagrams.
Let Œ≤ be a rational canonical basis for T, and Œ≤v1, Œ≤v2, . . . , Œ≤vk be the T-
cyclic bases of Œ≤ that are contained in KœÜ. Consider one of these T-cyclic
bases Œ≤vj, and suppose again that the T-annihilator of vj is (œÜ(t))pj. Then
Œ≤vj consists of dpj vectors in Œ≤.
For 0 ‚â§i < d, let Œ≥i be the cycle of
generalized eigenvectors of U corresponding to Œª = 0 with end vector Ti(vj),

538
Chap. 7
Canonical Forms
where T0(vj) = bj. Then
Œ≥i = {(œÜ(T))pj‚àí1Ti(vj), (œÜ(T))pj‚àí2Ti(vj), . . . , (œÜ(T))Ti(vj), Ti(vj)}.
By Theorem 7.1 (p. 485), Œ≥i is a linearly independent subset of Cvi. Now let
Œ±j = Œ≥0 ‚à™Œ≥1 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≥d‚àí1.
Notice that Œ±j contains pjd vectors.
Lemma 1. Œ±j is an ordered basis for Cvj.
Proof. The key to this proof is Theorem 7.4 (p. 487). Since Œ±j is the union
of cycles of generalized eigenvectors of U corresponding to Œª = 0, it suÔ¨Éces
to show that the set of initial vectors of these cycles
{(œÜ(T))pj‚àí1(vj), (œÜ(T))pj‚àí1T(vj), . . . , (œÜ(T))pj‚àí1Td‚àí1(vj)}
is linearly independent. Consider any linear combination of these vectors
a0(œÜ(T))pj‚àí1(vj) + a1(œÜ(T))pj‚àí1T(vj) + ¬∑ ¬∑ ¬∑ + ad‚àí1(œÜ(T))pj‚àí1Td‚àí1(vj),
where not all of the coeÔ¨Écients are zero. Let g(t) be the polynomial deÔ¨Åned
by g(t) = a0 + a1t + ¬∑ ¬∑ ¬∑ + ad‚àí1td‚àí1. Then g(t) is a nonzero polynomial of
degree less than d, and hence (œÜ(t))pj‚àí1g(t) is a nonzero polynomial with
degree less than pjd.
Since (œÜ(t))pj is the T-annihilator of vj, it follows
that (œÜ(T))pj‚àí1g(T)(vj) Ã∏= 0. Therefore the set of initial vectors is linearly
independent. So by Theorem 7.4, Œ±j is linearly independent, and the Œ≥i‚Äôs are
disjoint. Consequently, Œ±j consists of pjd linearly independent vectors in Cvj,
which has dimension pjd. We conclude that Œ±j is a basis for Cvj.
Thus we may replace Œ≤vj by Œ±j as a basis for Cvj. We do this for each j
to obtain a subset Œ± = Œ±1 ‚à™Œ±2 ¬∑ ¬∑ ¬∑ ‚à™Œ±k of KœÜ.
Lemma 2. Œ± is a Jordan canonical basis for KœÜ.
Proof. Since Œ≤v1 ‚à™Œ≤v2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≤vk is a basis for KœÜ, and since span(Œ±i) =
span(Œ≤vi) = Cvi, Exercise 9 implies that Œ± is a basis for KœÜ. Because Œ± is
a union of cycles of generalized eigenvectors of U, we conclude that Œ± is a
Jordan canonical basis.
We are now in a position to relate the dot diagram of T corresponding to
œÜ(t) to the dot diagram of U, bearing in mind that in the Ô¨Årst case we are
considering a rational canonical form and in the second case we are consider-
ing a Jordan canonical form. For convenience, we designate the Ô¨Årst diagram
D1, and the second diagram D2. For each j, the presence of the T-cyclic
basis Œ≤xj results in a column of pj dots in D1. By Lemma 1, this basis is

Sec. 7.4
The Rational Canonical Form
539
replaced by the union Œ±j of d cycles of generalized eigenvectors of U, each of
length pj, which becomes part of the Jordan canonical basis for U. In eÔ¨Äect,
Œ±j determines d columns each containing pj dots in D2. So each column in
D1 determines d columns in D2 of the same length, and all columns in D2 are
obtained in this way. Alternatively, each row in D2 has d times as many dots
as the corresponding row in D1. Since Theorem 7.10 (p. 500) gives us the
number of dots in any row of D2, we may divide the appropriate expression
in this theorem by d to obtain the number of dots in the corresponding row
of D1. Thus we have the following result.
Theorem 7.24. Let T be a linear operator on a Ô¨Ånite-dimensional vector
space V, let œÜ(t) be an irreducible monic divisor of the characteristic poly-
nomial of T of degree d, and let ri denote the number of dots in the ith row
of the dot diagram for œÜ(t) with respect to a rational canonical basis for T.
Then
(a) r1 = 1
d[dim(V) ‚àírank(œÜ(T))]
(b) ri = 1
d[rank((œÜ(T))i‚àí1) ‚àírank((œÜ(T))i)]
for i > 1.
Thus the dot diagrams associated with a rational canonical form of an op-
erator are completely determined by the operator. Since the rational canoni-
cal form is completely determined by its dot diagrams, we have the following
uniqueness condition.
Corollary. Under the conventions described earlier, the rational canonical
form of a linear operator is unique up to the arrangement of the irreducible
monic divisors of the characteristic polynomial.
Since the rational canonical form of a linear operator is unique, the poly-
nomials corresponding to the companion matrices that determine this form
are also unique. These polynomials, which are powers of the irreducible monic
divisors, are called the elementary divisors of the linear operator. Since a
companion matrix may occur more than once in a rational canonical form,
the same is true for the elementary divisors. We call the number of such
occurrences the multiplicity of the elementary divisor.
Conversely, the elementary divisors and their multiplicities determine the
companion matrices and, therefore, the rational canonical form of a linear
operator.
Example 4
Let
Œ≤ = {ex cos 2x, ex sin 2x, xex cos 2x, xex sin 2x}

540
Chap. 7
Canonical Forms
be viewed as a subset of F(R, R), the space of all real-valued functions deÔ¨Åned
on R, and let V = span(Œ≤). Then V is a four-dimensional subspace of F(R, R),
and Œ≤ is an ordered basis for V. Let D be the linear operator on V deÔ¨Åned by
D(y) = y‚Ä≤, the derivative of y, and let A = [D]Œ≤. Then
A =
‚éõ
‚éú
‚éú
‚éù
1
2
1
0
‚àí2
1
0
1
0
0
1
2
0
0
‚àí2
1
‚éû
‚éü
‚éü
‚é†,
and the characteristic polynomial of D, and hence of A, is
f(t) = (t2 ‚àí2t + 5)2.
Thus œÜ(t) = t2‚àí2t+5 is the only irreducible monic divisor of f(t). Since œÜ(t)
has degree 2 and V is four-dimensional, the dot diagram for œÜ(t) contains only
two dots. Therefore the dot diagram is determined by r1, the number of dots
in the Ô¨Årst row. Because ranks are preserved under matrix representations,
we can use A in place of D in the formula given in Theorem 7.24. Now
œÜ(A) =
‚éõ
‚éú
‚éú
‚éù
0
0
0
4
0
0
‚àí4
0
0
0
0
0
0
0
0
0
‚éû
‚éü
‚éü
‚é†,
and so
r1 = 1
2[4 ‚àírank(œÜ(A))] = 1
2[4 ‚àí2] = 1.
It follows that the second dot lies in the second row, and the dot diagram is
as follows:
‚Ä¢
‚Ä¢
Hence V is a D-cyclic space generated by a single function with D-annihilator
(œÜ(t))2. Furthermore, its rational canonical form is given by the companion
matrix of (œÜ(t))2 = t4 ‚àí4t3 + 14t2 ‚àí20t + 25, which is
‚éõ
‚éú
‚éú
‚éù
0
0
0
‚àí25
1
0
0
20
0
1
0
‚àí14
0
0
1
4
‚éû
‚éü
‚éü
‚é†.
Thus (œÜ(t))2 is the only elementary divisor of D, and it has multiplicity 1. For
the cyclic generator, it suÔ¨Éces to Ô¨Ånd a function g in V for which œÜ(D)(g) Ã∏= 0.

Sec. 7.4
The Rational Canonical Form
541
Since œÜ(A)(e3) Ã∏= 0, it follows that œÜ(D)(xex cos 2x) Ã∏= 0; therefore g(x) =
xex cos 2x can be chosen as the cyclic generator. Hence
Œ≤g = {xex cos 2x, D(xex cos 2x), D2(xex cos 2x), D3(xex cos 2x)}
is a rational canonical basis for D. Notice that the function h deÔ¨Åned by
h(x) = xex sin 2x can be chosen in place of g. This shows that the rational
canonical basis is not unique.
‚ô¶
It is convenient to refer to the rational canonical form and elementary
divisors of a matrix, which are deÔ¨Åned in the obvious way.
DeÔ¨Ånitions.
Let A ‚ààMn√ón(F).
The rational canonical form of
A is deÔ¨Åned to be the rational canonical form of LA. Likewise, for A, the
elementary divisors and their multiplicities are the same as those of LA.
Let A be an n√ón matrix, let C be a rational canonical form of A, and let
Œ≤ be the appropriate rational canonical basis for LA. Then C = [LA]Œ≤, and
therefore A is similar to C. In fact, if Q is the matrix whose columns are the
vectors of Œ≤ in the same order, then Q‚àí1AQ = C.
Example 5
For the following real matrix A, we Ô¨Ånd the rational canonical form C of A
and a matrix Q such that Q‚àí1AQ = C.
A =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
0
2
0
‚àí6
2
1
‚àí2
0
0
2
1
0
1
‚àí3
2
1
‚àí2
1
‚àí1
2
1
‚àí4
3
‚àí3
4
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
The characteristic polynomial of A is f(t) = ‚àí(t2 + 2)2(t ‚àí2); therefore
œÜ1(t) = t2 + 2 and œÜ2(t) = t ‚àí2 are the distinct irreducible monic divisors of
f(t). By Theorem 7.23, dim(KœÜ1) = 4 and dim(KœÜ2) = 1. Since the degree
of œÜ1(t) is 2, the total number of dots in the dot diagram of œÜ1(t) is 4/2 = 2,
and the number of dots r1 in the Ô¨Årst row is given by
r1 = 1
2[dim(R5) ‚àírank(œÜ1(A))]
= 1
2[5 ‚àírank(A2 + 2I)]
= 1
2[5 ‚àí1] = 2.
Thus the dot diagram of œÜ1(t) is
‚Ä¢
‚Ä¢

542
Chap. 7
Canonical Forms
and each column contributes the companion matrix

0
‚àí2
1
0
	
for œÜ1(t) = t2 + 2 to the rational canonical form C. Consequently œÜ1(t) is an
elementary divisor with multiplicity 2. Since dim(KœÜ2) = 1, the dot diagram
of œÜ2(t) = t ‚àí2 consists of a single dot, which contributes the 1 √ó 1 matrix

2

. Hence œÜ2(t) is an elementary divisor with multiplicity 1. Therefore the
rational canonical form C is
C =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
0
‚àí2
0
0
0
1
0
0
0
0
0
0
0
‚àí2
0
0
0
1
0
0
0
0
0
0
2
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
.
We can infer from the dot diagram of œÜ1(t) that if Œ≤ is a rational canonical
basis for LA, then Œ≤ ‚à©KœÜ1 is the union of two cyclic bases Œ≤v1 and Œ≤v2, where
v1 and v2 each have annihilator œÜ1(t). It follows that both v1 and v2 lie in
N(œÜ1(LA)). It can be shown that
‚éß
‚é™
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é™
‚é©
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
1
0
0
0
0
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
,
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
0
1
0
0
0
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
,
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
0
0
2
1
0
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
,
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
0
0
‚àí1
0
1
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
‚é´
‚é™
‚é™
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é™
‚é™
‚é≠
is a basis for N(œÜ1(LA)). Setting v1 = e1, we see that
Av1 =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
0
1
1
1
1
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
.
Next choose v2 in KœÜ1 = N(œÜ(LA)), but not in the span of Œ≤v1 = {v1, Av1}.
For example, v2 = e2. Then it can be seen that
Av2 =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
2
‚àí2
0
‚àí2
‚àí4
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
,
and Œ≤v1 ‚à™Œ≤v2 is a basis for KœÜ1.

Sec. 7.4
The Rational Canonical Form
543
Since the dot diagram of œÜ2(t) = t‚àí2 consists of a single dot, any nonzero
vector in KœÜ2 is an eigenvector of A corresponding to the eigenvalue Œª = 2.
For example, choose
v3 =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
0
1
1
1
2
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
.
By Theorem 7.23, Œ≤ = {v1, Av1, v2, Av2, v3} is a rational canonical basis for
LA. So setting
Q =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
1
0
0
2
0
0
1
1
‚àí2
1
0
1
0
0
1
0
1
0
‚àí2
1
0
1
0
‚àí4
2
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
,
we have Q‚àí1AQ = C.
‚ô¶
Example 6
For the following matrix A, we Ô¨Ånd the rational canonical form C and a
matrix Q such that Q‚àí1AQ = C:
A =
‚éõ
‚éú
‚éú
‚éù
2
1
0
0
0
2
1
0
0
0
2
0
0
0
0
2
‚éû
‚éü
‚éü
‚é†.
Since the characteristic polynomial of A is f(t) = (t‚àí2)4, the only irreducible
monic divisor of f(t) is œÜ(t) = t ‚àí2, and so KœÜ = R4. In this case, œÜ(t) has
degree 1; hence in applying Theorem 7.24 to compute the dot diagram for
œÜ(t), we obtain
r1 = 4 ‚àírank(œÜ(A)) = 4 ‚àí2 = 2,
r2 = rank(œÜ(A)) ‚àírank((œÜ(A))2) = 2 ‚àí1 = 1,
and
r3 = rank((œÜ(A))2) ‚àírank((œÜ(A))3) = 1 ‚àí0 = 1,
where ri is the number of dots in the ith row of the dot diagram. Since there
are dim(R4) = 4 dots in the diagram, we may terminate these computations

544
Chap. 7
Canonical Forms
with r3. Thus the dot diagram for A is
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
Since (t ‚àí2)3 has the companion matrix
‚éõ
‚éù
0
0
8
1
0
‚àí12
0
1
6
‚éû
‚é†
and (t ‚àí2) has the companion matrix

2

, the rational canonical form of A
is given by
C =
‚éõ
‚éú
‚éú
‚éù
0
0
8
0
1
0
‚àí12
0
0
1
6
0
0
0
0
2
‚éû
‚éü
‚éü
‚é†.
Next we Ô¨Ånd a rational canonical basis for LA. The preceding dot diagram
indicates that there are two vectors v1 and v2 in R4 with annihilators (œÜ(t))3
and œÜ(t), respectively, and such that
Œ≤ = {Œ≤v1 ‚à™Œ≤v1} = {v1, Av1, A2v1, v2}
is a rational canonical basis for LA. Furthermore, v1 /‚ààN((LA ‚àí2I)2), and
v2 ‚ààN(LA ‚àí2I). It can easily be shown that
N(LA ‚àí2I) = span({e1, e4})
and
N((LA ‚àí2I)2) = span({e1, e2, e4}).
The standard vector e3 meets the criteria for v1; so we set v1 = e3. It follows
that
Av1 =
‚éõ
‚éú
‚éú
‚éù
0
1
2
0
‚éû
‚éü
‚éü
‚é†
and
A2v1 =
‚éõ
‚éú
‚éú
‚éù
1
4
4
0
‚éû
‚éü
‚éü
‚é†.
Next we choose a vector v2 ‚ààN(LA‚àí2I) that is not in the span of Œ≤v1. Clearly,
v2 = e4 satisÔ¨Åes this condition. Thus
‚éß
‚é™
‚é™
‚é®
‚é™
‚é™
‚é©
‚éõ
‚éú
‚éú
‚éù
0
0
1
0
‚éû
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éù
0
1
2
0
‚éû
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éù
1
4
4
0
‚éû
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éù
0
0
0
1
‚éû
‚éü
‚éü
‚é†
‚é´
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é≠

Sec. 7.4
The Rational Canonical Form
545
is a rational canonical basis for LA.
Finally, let Q be the matrix whose columns are the vectors of Œ≤ in the
same order:
Q =
‚éõ
‚éú
‚éú
‚éù
0
0
1
0
0
1
4
0
1
2
4
0
0
0
0
1
‚éû
‚éü
‚éü
‚é†.
Then C = Q‚àí1AQ.
‚ô¶
Direct Sums*
The next theorem is a simple consequence of Theorem 7.23.
Theorem 7.25 (Primary Decomposition Theorem).
Let T be a
linear operator on an n-dimensional vector space V with characteristic poly-
nomial
f(t) = (‚àí1)n(œÜ1(t))n1(œÜ2(t))n2 ¬∑ ¬∑ ¬∑ (œÜk(t))nk,
where the œÜi(t)‚Äôs (1 ‚â§i ‚â§k) are distinct irreducible monic polynomials and
the ni‚Äôs are positive integers. Then the following statements are true.
(a) V = KœÜ1 ‚äïKœÜ2 ‚äï¬∑ ¬∑ ¬∑ ‚äïKœÜk.
(b) If Ti (1 ‚â§i ‚â§k) is the restriction of T to KœÜi and Ci is the rational
canonical form of Ti, then C1 ‚äïC2 ‚äï¬∑ ¬∑ ¬∑ ‚äïCk is the rational canonical
form of T.
Proof. Exercise.
The next theorem is a simple consequence of Theorem 7.17.
Theorem 7.26. Let T be a linear operator on a Ô¨Ånite-dimensional vector
space V. Then V is a direct sum of T-cyclic subspaces Cvi, where each vi lies
in KœÜ for some irreducible monic divisor œÜ(t) of the characteristic polynomial
of T.
Proof. Exercise.
EXERCISES
1.
Label the following statements as true or false.
(a)
Every rational canonical basis for a linear operator T is the union
of T-cyclic bases.

546
Chap. 7
Canonical Forms
(b)
If a basis is the union of T-cyclic bases for a linear operator T,
then it is a rational canonical basis for T.
(c)
There exist square matrices having no rational canonical form.
(d)
A square matrix is similar to its rational canonical form.
(e)
For any linear operator T on a Ô¨Ånite-dimensional vector space, any
irreducible factor of the characteristic polynomial of T divides the
minimal polynomial of T.
(f)
Let œÜ(t) be an irreducible monic divisor of the characteristic poly-
nomial of a linear operator T. The dots in the diagram used to
compute the rational canonical form of the restriction of T to KœÜ
are in one-to-one correspondence with the vectors in a basis for
KœÜ.
(g)
If a matrix has a Jordan canonical form, then its Jordan canonical
form and rational canonical form are similar.
2.
For each of the following matrices A ‚ààMn√ón(F), Ô¨Ånd the rational
canonical form C of A and a matrix Q ‚ààMn√ón(F) such that Q‚àí1AQ =
C.
(a)
A =
‚éõ
‚éù
3
1
0
0
3
1
0
0
3
‚éû
‚é†
F = R
(b)
A =

0
‚àí1
1
‚àí1
	
F = R
(c)
A =

0
‚àí1
1
‚àí1
	
F = C
(d)
A =
‚éõ
‚éú
‚éú
‚éù
0
‚àí7
14
‚àí6
1
‚àí4
6
‚àí3
0
‚àí4
9
‚àí4
0
‚àí4
11
‚àí5
‚éû
‚éü
‚éü
‚é†
F = R
(e)
A =
‚éõ
‚éú
‚éú
‚éù
0
‚àí4
12
‚àí7
1
‚àí1
3
‚àí3
0
‚àí1
6
‚àí4
0
‚àí1
8
‚àí5
‚éû
‚éü
‚éü
‚é†
F = R
3.
For each of the following linear operators T, Ô¨Ånd the elementary divisors,
the rational canonical form C, and a rational canonical basis Œ≤.
(a)
T is the linear operator on P3(R) deÔ¨Åned by
T(f(x)) = f(0)x ‚àíf ‚Ä≤(1).
(b)
Let S = {sin x, cos x, x sin x, x cos x}, a subset of F(R, R), and let
V = span(S). DeÔ¨Åne T to be the linear operator on V such that
T(f) = f ‚Ä≤.
(c)
T is the linear operator on M2√ó2(R) deÔ¨Åned by

Sec. 7.4
The Rational Canonical Form
547
T(A) =

0
1
‚àí1
1
	
¬∑A.
(d)
Let S = {sin x sin y, sin x cos y, cos x sin y, cos x cos y}, a subset of
F(R √ó R, R), and let V = span(S).
DeÔ¨Åne T to be the linear
operator on V such that
T(f)(x, y) = ‚àÇf(x, y)
‚àÇx
+ ‚àÇf(x, y)
‚àÇy
.
4.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V with
minimal polynomial (œÜ(t))m for some positive integer m.
(a)
Prove that R(œÜ(T)) ‚äÜN((œÜ(T))m‚àí1).
(b)
Give an example to show that the subspaces in (a) need not be
equal.
(c)
Prove that the minimal polynomial of the restriction of T to
R(œÜ(T)) equals (œÜ(t))m‚àí1.
5.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space. Prove
that the rational canonical form of T is a diagonal matrix if and only if
T is diagonalizable.
6.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V with
characteristic polynomial f(t) = (‚àí1)nœÜ1(t)œÜ2(t), where œÜ1(t) and œÜ2(t)
are distinct irreducible monic polynomials and n = dim(V).
(a)
Prove that there exist v1, v2 ‚ààV such that v1 has T-annihilator
œÜ1(t), v2 has T-annihilator œÜ2(t), and Œ≤v1 ‚à™Œ≤v2 is a basis for V.
(b)
Prove that there is a vector v3 ‚ààV with T-annihilator œÜ1(t)œÜ2(t)
such that Œ≤v3 is a basis for V.
(c)
Describe the diÔ¨Äerence between the matrix representation of T
with respect to Œ≤v1 ‚à™Œ≤v2 and the matrix representation of T with
respect to Œ≤v3.
Thus, to assure the uniqueness of the rational canonical form, we re-
quire that the generators of the T-cyclic bases that constitute a rational
canonical basis have T-annihilators equal to powers of irreducible monic
factors of the characteristic polynomial of T.
7.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space with
minimal polynomial
f(t) = (œÜ1(t))m1(œÜ2(t))m2 ¬∑ ¬∑ ¬∑ (œÜk(t))mk,
where the œÜi(t)‚Äôs are distinct irreducible monic factors of f(t). Prove
that for each i, mi is the number of entries in the Ô¨Årst column of the
dot diagram for œÜi(t).

548
Chap. 7
Canonical Forms
8.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space V. Prove
that for any irreducible polynomial œÜ(t), if œÜ(T) is not one-to-one, then
œÜ(t) divides the characteristic polynomial of T. Hint: Apply Exercise 15
of Section 7.3.
9.
Let V be a vector space and Œ≤1, Œ≤2, . . . , Œ≤k be disjoint subsets of V whose
union is a basis for V.
Now suppose that Œ≥1, Œ≥2, . . . , Œ≥k are linearly
independent subsets of V such that span(Œ≥i) = span(Œ≤i) for all i. Prove
that Œ≥1 ‚à™Œ≥2 ‚à™¬∑ ¬∑ ¬∑ ‚à™Œ≥k is also a basis for V.
10.
Let T be a linear operator on a Ô¨Ånite-dimensional vector space, and
suppose that œÜ(t) is an irreducible monic factor of the characteristic
polynomial of T. Prove that if œÜ(t) is the T-annihilator of vectors x and
y, then x ‚ààCy if and only if Cx = Cy.
Exercises 11 and 12 are concerned with direct sums.
11.
Prove Theorem 7.25.
12.
Prove Theorem 7.26.
INDEX OF DEFINITIONS FOR CHAPTER 7
Companion matrix
526
Cycle
of
generalized
eigenvectors
488
Cyclic basis
525
Dot diagram for Jordan canonical
form
498
Dot diagram for rational canonical
form
535
Elementary divisor of a linear oper-
ator
539
Elementary divisor of a matrix
541
End vector of a cycle
488
Generalized eigenspace
484
Generalized eigenvector
484
Generator of a cyclic basis
525
Initial vector of a cycle
488
Jordan block
483
Jordan canonical basis
483
Jordan canonical form of a linear op-
erator
483
Jordan canonical form of a matrix
491
Length of a cycle
488
Minimal polynomial of a linear oper-
ator
516
Minimal
polynomial
of
a
matrix
517
Multiplicity of an elementary divisor
539
Rational canonical basis of a linear
operator
526
Rational canonical form for a linear
operator
526
Rational canonical form of a matrix
541

Appendices
APPENDIX A
SETS
A set is a collection of objects, called elements of the set. If x is an element
of the set A, then we write x ‚ààA; otherwise, we write x Ã∏‚ààA. For example,
if Z is the set of integers, then 3 ‚ààZ and 1
2 Ã∏‚ààZ.
One set that appears frequently is the set of real numbers, which we denote
by R throughout this text.
Two sets A and B are called equal, written A = B, if they contain exactly
the same elements. Sets may be described in one of two ways:
1. By listing the elements of the set between set braces {
}.
2. By describing the elements of the set in terms of some characteristic
property.
For example, the set consisting of the elements 1, 2, 3, and 4 can be
written as {1, 2, 3, 4} or as
{x: x is a positive integer less than 5}.
Note that the order in which the elements of a set are listed is immaterial;
hence
{1, 2, 3, 4} = {3, 1, 2, 4} = {1, 3, 1, 4, 2}.
Example 1
Let A denote the set of real numbers between 1 and 2.
Then A may be
written as
A = {x ‚ààR: 1 < x < 2}.
‚ô¶
A set B is called a subset of a set A, written B ‚äÜA or A ‚äáB, if every
element of B is an element of A. For example, {1, 2, 6} ‚äÜ{2, 8, 7, 6, 1}. If
B ‚äÜA, and B Ã∏= A, then B is called a proper subset of A. Observe that
A = B if and only if A ‚äÜB and B ‚äÜA, a fact that is often used to prove
that two sets are equal.
The empty set, denoted by ‚àÖ, is the set containing no elements. The
empty set is a subset of every set.
Sets may be combined to form other sets in two basic ways. The union
of two sets A and B, denoted A ‚à™B, is the set of elements that are in A, or
B, or both; that is,
A ‚à™B = {x: x ‚ààA or x ‚ààB}.
549

550
Appendices
The intersection of two sets A and B, denoted A ‚à©B, is the set of elements
that are in both A and B; that is,
A ‚à©B = {x: x ‚ààA and x ‚ààB}.
Two sets are called disjoint if their intersection equals the empty set.
Example 2
Let A = {1, 3, 5} and B = {1, 5, 7, 8}. Then
A ‚à™B = {1, 3, 5, 7, 8}
and
A ‚à©B = {1, 5}.
Likewise, if X = {1, 2, 8} and Y = {3, 4, 5}, then
X ‚à™Y = {1, 2, 3, 4, 5, 8}
and
X ‚à©Y = ‚àÖ.
Thus X and Y are disjoint sets.
‚ô¶
The union and intersection of more than two sets can be deÔ¨Åned analo-
gously. SpeciÔ¨Åcally, if A1, A2, . . . , An are sets, then the union and intersec-
tions of these sets are deÔ¨Åned, respectively, by
n
-
i=1
Ai = {x: x ‚ààAi for some i = 1, 2, . . . , n}
and
n
>
i=1
Ai = {x: x ‚ààAi for all i = 1, 2, . . . , n}.
Similarly, if Œõ is an index set and {AŒ± : Œ± ‚ààŒõ} is a collection of sets, the
union and intersection of these sets are deÔ¨Åned, respectively, by
-
Œ±‚ààŒõ
AŒ± = {x: x ‚ààAŒ± for some Œ± ‚ààŒõ}
and
>
Œ±‚ààŒõ
AŒ± = {x: x ‚ààAŒ± for all Œ± ‚ààŒõ}.
Example 3
Let Œõ = {Œ± ‚ààR: Œ± > 1}, and let
AŒ± =

x ‚ààR: ‚àí1
Œ± ‚â§x ‚â§1 + Œ±

for each Œ± ‚ààŒõ. Then
-
Œ±‚ààŒõ
AŒ± = {x ‚ààR: x > ‚àí1}
and
>
Œ±‚ààŒõ
AŒ± = {x ‚ààR: 0 ‚â§x ‚â§2}.
‚ô¶

Appendix B
Functions
551
By a relation on a set A, we mean a rule for determining whether or not,
for any elements x and y in A, x stands in a given relationship to y. More
precisely, a relation on A is a set S of ordered pairs of elements of A such
that (x, y) ‚ààS if and only if x stands in the given relationship to y. On the
set of real numbers, for instance, ‚Äúis equal to,‚Äù ‚Äúis less than,‚Äù and ‚Äúis greater
than or equal to‚Äù are familiar relations. If S is a relation on a set A, we often
write x ‚àºy in place of (x, y) ‚ààS.
A relation S on a set A is called an equivalence relation on A if the
following three conditions hold:
1. For each x ‚ààA, x ‚àºx
(reÔ¨Çexivity).
2. If x ‚àºy, then y ‚àºx
(symmetry).
3. If x ‚àºy and y ‚àºz, then x ‚àºz
(transitivity).
For example, if we deÔ¨Åne x ‚àºy to mean that x ‚àíy is divisible by a Ô¨Åxed
integer n, then ‚àºis an equivalence relation on the set of integers.
APPENDIX B
FUNCTIONS
If A and B are sets, then a function f from A to B, written f : A ‚ÜíB, is
a rule that associates to each element x in A a unique element denoted f(x)
in B. The element f(x) is called the image of x (under f), and x is called
a preimage of f(x) (under f). If f : A ‚ÜíB, then A is called the domain
of f, B is called the codomain of f, and the set {f(x): x ‚ààA} is called the
range of f. Note that the range of f is a subset of B. If S ‚äÜA, we denote
by f(S) the set {f(x): x ‚ààS} of all images of elements of S. Likewise, if
T ‚äÜB, we denote by f ‚àí1(T) the set {x ‚ààA: f(x) ‚ààT} of all preimages of
elements in T. Finally, two functions f : A ‚ÜíB and g: A ‚ÜíB are equal,
written f = g, if f(x) = g(x) for all x ‚ààA.
Example 1
Suppose that A = [‚àí10, 10]. Let f : A ‚ÜíR be the function that assigns
to each element x in A the element x2 + 1 in R; that is, f is deÔ¨Åned by
f(x) = x2+1. Then A is the domain of f, R is the codomain of f, and [1, 101]
is the range of f. Since f(2) = 5, the image of 2 is 5, and 2 is a preimage
of 5. Notice that ‚àí2 is another preimage of 5. Moreover, if S = [1, 2] and
T = [82, 101], then f(S) = [2, 5] and f ‚àí1(T) = [‚àí10, ‚àí9] ‚à™[9, 10].
‚ô¶
As Example 1 shows, the preimage of an element in the range need not be
unique. Functions such that each element of the range has a unique preimage
are called one-to-one; that is f : A ‚ÜíB is one-to-one if f(x) = f(y) implies
x = y or, equivalently, if x Ã∏= y implies f(x) Ã∏= f(y).
If f : A ‚ÜíB is a function with range B, that is, if f(A) = B, then f is
called onto. So f is onto if and only if the range of f equals the codomain
of f.

552
Appendices
Let f : A ‚ÜíB be a function and S ‚äÜA. Then a function fS : S ‚ÜíB,
called the restriction of f to S, can be formed by deÔ¨Åning fS(x) = f(x) for
each x ‚ààS.
The next example illustrates these concepts.
Example 2
Let f : [‚àí1, 1] ‚Üí[0, 1] be deÔ¨Åned by f(x) = x2. This function is onto, but
not one-to-one since f(‚àí1) = f(1) = 1. Note that if S = [0, 1], then fS is
both onto and one-to-one. Finally, if T = [ 1
2, 1], then fT is one-to-one, but
not onto.
‚ô¶
Let A, B, and C be sets and f : A ‚ÜíB and g: B ‚ÜíC be functions. By
following f with g, we obtain a function g ‚ó¶f : A ‚ÜíC called the composite
of g and f.
Thus (g ‚ó¶f)(x) = g(f(x)) for all x ‚ààA.
For example, let
A = B = C = R, f(x) = sin x, and g(x) = x2 + 3.
Then (g ‚ó¶f)(x) =
(g(f(x)) = sin2 x + 3, whereas (f ‚ó¶g)(x) = f(g(x)) = sin(x2 + 3). Hence,
g ‚ó¶f Ã∏= f ‚ó¶g.
Functional composition is associative, however; that is, if
h: C ‚ÜíD is another function, then h ‚ó¶(g ‚ó¶f) = (h ‚ó¶g) ‚ó¶f.
A function f : A ‚ÜíB is said to be invertible if there exists a function
g: B ‚ÜíA such that (f ‚ó¶g)(y) = y for all y ‚ààB and (g ‚ó¶f)(x) = x for all
x ‚ààA. If such a function g exists, then it is unique and is called the inverse
of f. We denote the inverse of f (when it exists) by f ‚àí1. It can be shown
that f is invertible if and only if f is both one-to-one and onto.
Example 3
The function f : R ‚ÜíR deÔ¨Åned by f(x) = 3x + 1 is one-to-one and onto;
hence f is invertible. The inverse of f is the function f ‚àí1 : R ‚ÜíR deÔ¨Åned
by f ‚àí1(x) = (x ‚àí1)/3.
‚ô¶
The following facts about invertible functions are easily proved.
1. If f : A ‚ÜíB is invertible, then f ‚àí1 is invertible, and (f ‚àí1)‚àí1 = f.
2. If f : A ‚ÜíB and g: B ‚ÜíC are invertible, then g ‚ó¶f is invertible, and
(g ‚ó¶f)‚àí1 = f ‚àí1 ‚ó¶g‚àí1.
APPENDIX C
FIELDS
The set of real numbers is an example of an algebraic structure called a
Ô¨Åeld.
Basically, a Ô¨Åeld is a set in which four operations (called addition,
multiplication, subtraction, and division) can be deÔ¨Åned so that, with the
exception of division by zero, the sum, product, diÔ¨Äerence, and quotient of
any two elements in the set is an element of the set. More precisely, a Ô¨Åeld is
deÔ¨Åned as follows.

Appendix C
Fields
553
DeÔ¨Ånitions. A Ô¨Åeld F is a set on which two operations + and ¬∑ (called
addition and multiplication, respectively) are deÔ¨Åned so that, for each pair
of elements x, y in F, there are unique elements x+y and x¬∑y in F for which
the following conditions hold for all elements a, b, c in F.
(F 1) a + b = b + a
and
a¬∑b = b¬∑a
(commutativity of addition and multiplication)
(F 2) (a + b) + c = a + (b + c)
and
(a¬∑b)¬∑c = a¬∑(b¬∑c)
(associativity of addition and multiplication)
(F 3) There exist distinct elements 0 and 1 in F such that
0 + a = a
and
1¬∑a = a
(existence of identity elements for addition and multiplication)
(F 4) For each element a in F and each nonzero element b in F, there exist
elements c and d in F such that
a + c = 0
and
b¬∑d = 1
(existence of inverses for addition and multiplication)
(F 5) a¬∑(b + c) = a¬∑b + a¬∑c
(distributivity of multiplication over addition)
The elements x + y and x¬∑y are called the sum and product, respectively,
of x and y. The elements 0 (read ‚Äúzero‚Äù) and 1 (read ‚Äúone‚Äù) mentioned in
(F 3) are called identity elements for addition and multiplication, respec-
tively, and the elements c and d referred to in (F 4) are called an additive
inverse for a and a multiplicative inverse for b, respectively.
Example 1
The set of real numbers R with the usual deÔ¨Ånitions of addition and multi-
plication is a Ô¨Åeld.
‚ô¶
Example 2
The set of rational numbers with the usual deÔ¨Ånitions of addition and multi-
plication is a Ô¨Åeld.
‚ô¶
Example 3
The set of all real numbers of the form a + b
‚àö
2, where a and b are rational
numbers, with addition and multiplication as in R is a Ô¨Åeld.
‚ô¶
Example 4
The Ô¨Åeld Z2 consists of two elements 0 and 1 with the operations of addition
and multiplication deÔ¨Åned by the equations
0 + 0 = 0,
0 + 1 = 1 + 0 = 1,
1 + 1 = 0,
0¬∑0 = 0,
0¬∑1 = 1¬∑0 = 0,
and
1¬∑1 = 1.
‚ô¶

554
Appendices
Example 5
Neither the set of positive integers nor the set of integers with the usual
deÔ¨Ånitions of addition and multiplication is a Ô¨Åeld, for in either case (F 4)
does not hold.
‚ô¶
The identity and inverse elements guaranteed by (F 3) and (F 4) are
unique; this is a consequence of the following theorem.
Theorem C.1 (Cancellation Laws). For arbitrary elements a, b, and
c in a Ô¨Åeld, the following statements are true.
(a) If a + b = c + b, then a = c.
(b) If a¬∑b = c¬∑b and b Ã∏= 0, then a = c.
Proof. (a) The proof of (a) is left as an exercise.
(b) If b Ã∏= 0, then (F 4) guarantees the existence of an element d in the
Ô¨Åeld such that b¬∑d = 1. Multiply both sides of the equality a¬∑b = c ¬∑ b by d
to obtain (a¬∑b)¬∑d = (c¬∑b)¬∑d. Consider the left side of this equality: By (F 2)
and (F 3), we have
(a¬∑b)¬∑d = a¬∑(b¬∑d) = a¬∑1 = a.
Similarly, the right side of the equality reduces to c. Thus a = c.
Corollary. The elements 0 and 1 mentioned in (F 3), and the elements c
and d mentioned in (F 4), are unique.
Proof. Suppose that 0‚Ä≤ ‚ààF satisÔ¨Åes 0‚Ä≤ + a = a for each a ‚ààF. Since
0 + a = a for each a ‚ààF, we have 0‚Ä≤ + a = 0 + a for each a ‚ààF. Thus 0‚Ä≤ = 0
by Theorem C.1.
The proofs of the remaining parts are similar.
Thus each element b in a Ô¨Åeld has a unique additive inverse and, if b Ã∏= 0,
a unique multiplicative inverse. (It is shown in the corollary to Theorem C.2
that 0 has no multiplicative inverse.) The additive inverse and the multi-
plicative inverse of b are denoted by ‚àíb and b‚àí1, respectively.
Note that
‚àí(‚àíb) = b and (b‚àí1)‚àí1 = b.
Subtraction and division can be deÔ¨Åned in terms of addition and multi-
plication by using the additive and multiplicative inverses. SpeciÔ¨Åcally, sub-
traction of b is deÔ¨Åned to be addition of ‚àíb and division by b Ã∏= 0 is deÔ¨Åned
to be multiplication by b‚àí1; that is,
a ‚àíb = a + (‚àíb)
and
a
b = a¬∑b‚àí1.
In particular, the symbol 1
b denotes b‚àí1. Division by zero is undeÔ¨Åned, but,
with this exception, the sum, product, diÔ¨Äerence, and quotient of any two
elements of a Ô¨Åeld are deÔ¨Åned.

Appendix C
Fields
555
Many of the familiar properties of multiplication of real numbers are true
in any Ô¨Åeld, as the next theorem shows.
Theorem C.2. Let a and b be arbitrary elements of a Ô¨Åeld. Then each
of the following statements are true.
(a) a¬∑0 = 0.
(b) (‚àía)¬∑b = a¬∑(‚àíb) = ‚àí(a¬∑b).
(c) (‚àía)¬∑(‚àíb) = a¬∑b.
Proof. (a) Since 0 + 0 = 0, (F 5) shows that
0 + a¬∑0 = a¬∑0 = a¬∑(0 + 0) = a¬∑0 + a¬∑0.
Thus 0 = a¬∑0 by Theorem C.1.
(b) By deÔ¨Ånition, ‚àí(a¬∑b) is the unique element of F with the property
a¬∑b + [‚àí(a¬∑b)] = 0. So in order to prove that (‚àía)¬∑b = ‚àí(a¬∑b), it suÔ¨Éces
to show that a¬∑b + (‚àía)¬∑b = 0.
But ‚àía is the element of F such that
a + (‚àía) = 0; so
a¬∑b + (‚àía)¬∑b = [a + (‚àía)]¬∑b = 0¬∑b = b¬∑0 = 0
by (F 5) and (a). Thus (‚àía)¬∑b = ‚àí(a¬∑b). The proof that a¬∑(‚àíb) = ‚àí(a¬∑b)
is similar.
(c) By applying (b) twice, we Ô¨Ånd that
(‚àía)¬∑(‚àíb) = ‚àí[a¬∑(‚àíb)] = ‚àí[‚àí(a¬∑b)] = a¬∑b.
Corollary. The additive identity of a Ô¨Åeld has no multiplicative inverse.
In an arbitrary Ô¨Åeld F, it may happen that a sum 1 + 1 + ¬∑ ¬∑ ¬∑ + 1 (p sum-
mands) equals 0 for some positive integer p. For example, in the Ô¨Åeld Z2
(deÔ¨Åned in Example 4), 1+1 = 0. In this case, the smallest positive integer p
for which a sum of p 1‚Äôs equals 0 is called the characteristic of F; if no such
positive integer exists, then F is said to have characteristic zero. Thus Z2
has characteristic two, and R has characteristic zero. Observe that if F is a
Ô¨Åeld of characteristic p Ã∏= 0, then x+x+¬∑ ¬∑ ¬∑ +x (p summands) equals 0 for all
x ‚ààF. In a Ô¨Åeld having nonzero characteristic (especially characteristic two),
many unnatural problems arise. For this reason, some of the results about
vector spaces stated in this book require that the Ô¨Åeld over which the vector
space is deÔ¨Åned be of characteristic zero (or, at least, of some characteristic
other than two).
Finally, note that in other sections of this book, the product of two ele-
ments a and b in a Ô¨Åeld is usually denoted ab rather than a¬∑b.

556
Appendices
APPENDIX D
COMPLEX NUMBERS
For the purposes of algebra, the Ô¨Åeld of real numbers is not suÔ¨Écient, for
there are polynomials of nonzero degree with real coeÔ¨Écients that have no
zeros in the Ô¨Åeld of real numbers (for example, x2 + 1). It is often desirable
to have a Ô¨Åeld in which any polynomial of nonzero degree with coeÔ¨Écients
from that Ô¨Åeld has a zero in that Ô¨Åeld. It is possible to ‚Äúenlarge‚Äù the Ô¨Åeld of
real numbers to obtain such a Ô¨Åeld.
DeÔ¨Ånitions. A complex number is an expression of the form z = a+bi,
where a and b are real numbers called the real part and the imaginary part
of z, respectively.
The sum and product of two complex numbers z = a+bi and w = c+di
(where a, b, c, and d are real numbers) are deÔ¨Åned, respectively, as follows:
z + w = (a + bi) + (c + di) = (a + c) + (b + d)i
and
zw = (a + bi)(c + di) = (ac ‚àíbd) + (bc + ad)i.
Example 1
The sum and product of z = 3 ‚àí5i and w = 9 + 7i are, respectively,
z + w = (3 ‚àí5i) + (9 + 7i) = (3 + 9) + [(‚àí5) + 7]i = 12 + 2i
and
zw = (3 ‚àí5i)(9 + 7i) = [3¬∑9 ‚àí(‚àí5)¬∑7] + [(‚àí5)¬∑9 + 3¬∑7]i = 62 ‚àí24i.
‚ô¶
Any real number c may be regarded as a complex number by identifying c
with the complex number c + 0i. Observe that this correspondence preserves
sums and products; that is,
(c + 0i) + (d + 0i) = (c + d) + 0i
and
(c + 0i)(d + 0i) = cd + 0i.
Any complex number of the form bi = 0 + bi, where b is a nonzero real
number, is called imaginary. The product of two imaginary numbers is real
since
(bi)(di) = (0 + bi)(0 + di) = (0 ‚àíbd) + (b¬∑0 + 0¬∑d)i = ‚àíbd.
In particular, for i = 0 + 1i, we have i ¬∑ i = ‚àí1.
The observation that i2 = i¬∑i = ‚àí1 provides an easy way to remember the
deÔ¨Ånition of multiplication of complex numbers: simply multiply two complex
numbers as you would any two algebraic expressions, and replace i2 by ‚àí1.
Example 2 illustrates this technique.

Appendix D
Complex Numbers
557
Example 2
The product of ‚àí5 + 2i and 1 ‚àí3i is
(‚àí5 + 2i)(1 ‚àí3i) = ‚àí5(1 ‚àí3i) + 2i(1 ‚àí3i)
= ‚àí5 + 15i + 2i ‚àí6i2
= ‚àí5 + 15i + 2i ‚àí6(‚àí1)
= 1 + 17i.
‚ô¶
The real number 0, regarded as a complex number, is an additive identity
element for the complex numbers since
(a + bi) + 0 = (a + bi) + (0 + 0i) = (a + 0) + (b + 0)i = a + bi.
Likewise the real number 1, regarded as a complex number, is a multiplicative
identity element for the set of complex numbers since
(a + bi)¬∑1 = (a + bi)(1 + 0i) = (a¬∑1 ‚àíb¬∑0) + (b¬∑1 + a¬∑0)i = a + bi.
Every complex number a + bi has an additive inverse, namely (‚àía) + (‚àíb)i.
But also each complex number except 0 has a multiplicative inverse. In fact,
(a + bi)‚àí1 =

a
a2 + b2
	
‚àí

b
a2 + b2
	
i.
In view of the preceding statements, the following result is not surprising.
Theorem D.1. The set of complex numbers with the operations of addi-
tion and multiplication previously deÔ¨Åned is a Ô¨Åeld.
Proof. Exercise.
DeÔ¨Ånition.
The (complex) conjugate of a complex number a + bi is
the complex number a ‚àíbi. We denote the conjugate of the complex number
z by z.
Example 3
The conjugates of ‚àí3 + 2i, 4 ‚àí7i, and 6 are, respectively,
‚àí3 + 2i = ‚àí3 ‚àí2i,
4 ‚àí7i = 4 + 7i,
and
6 = 6 + 0i = 6 ‚àí0i = 6.
‚ô¶
The next theorem contains some important properties of the conjugate of
a complex number.
Theorem D.2. Let z and w be complex numbers. Then the following
statements are true.

558
Appendices
(a) z = z.
(b) (z + w) = z + w.
(c) zw = z¬∑w.
(d)
 z
w

= z
w if w Ã∏= 0.
(e) z is a real number if and only if z = z.
Proof. We leave the proofs of (a), (d), and (e) to the reader.
(b) Let z = a + bi and w = c + di, where a, b, c, d ‚ààR. Then
(z + w) = (a + c) + (b + d)i = (a + c) ‚àí(b + d)i
= (a ‚àíbi) + (c ‚àídi) = z + w.
(c) For z and w, we have
zw = (a + bi)(c + di) = (ac ‚àíbd) + (ad + bc)i
= (ac ‚àíbd) ‚àí(ad + bc)i = (a ‚àíbi)(c ‚àídi) = z¬∑w.
For any complex number z = a + bi, zz is real and nonnegative, for
zz = (a + bi)(a ‚àíbi) = a2 + b2.
This fact can be used to deÔ¨Åne the absolute value of a complex number.
DeÔ¨Ånition.
Let z = a + bi, where a, b ‚ààR. The absolute value (or
modulus) of z is the real number
‚àö
a2 + b2. We denote the absolute value
of z by |z|.
Observe that zz = |z|2. The fact that the product of a complex number
and its conjugate is real provides an easy method for determining the quotient
of two complex numbers; for if c + di Ã∏= 0, then
a + bi
c + di = a + bi
c + di ¬∑ c ‚àídi
c ‚àídi = (ac + bd) + (bc ‚àíad)i
c2 + d2
= ac + bd
c2 + d2 + bc ‚àíad
c2 + d2 i.
Example 4
To illustrate this procedure, we compute the quotient (1 + 4i)/(3 ‚àí2i):
1 + 4i
3 ‚àí2i = 1 + 4i
3 ‚àí2i ¬∑ 3 + 2i
3 + 2i = ‚àí5 + 14i
9 + 4
= ‚àí5
13 + 14
13i.
‚ô¶
The absolute value of a complex number has the familiar properties of the
absolute value of a real number, as the following result shows.
Theorem D.3. Let z and w denote any two complex numbers. Then the
following statements are true.

Appendix D
Complex Numbers
559
(a) |zw| = |z|¬∑|w|.
(b)
### z
w
### = |z|
|w| if w Ã∏= 0.
(c) |z + w| ‚â§|z| + |w|.
(d) |z| ‚àí|w| ‚â§|z + w|.
Proof. (a) By Theorem D.2, we have
|zw|2 = (zw)(zw) = (zw)(z ¬∑ w) = (zz)(ww) = |z|2|w|2,
proving (a).
(b) For the proof of (b), apply (a) to the product
 z
w

w.
(c) For any complex number x = a + bi, where a, b ‚ààR, observe that
x + x = (a + bi) + (a ‚àíbi) = 2a ‚â§2

a2 + b2 = 2|x|.
Thus x + x is real and satisÔ¨Åes the inequality x + x ‚â§2|x|. Taking x = wz,
we have, by Theorem D.2 and (a),
wz + wz ‚â§2|wz| = 2|w||z| = 2|z||w|.
Using Theorem D.2 again gives
|z + w|2 = (z + w)(z + w) = (z + w)(z + w) = zz + wz + zw + ww
‚â§|z|2 + 2|z||w| + |w|2 = (|z| + |w|)2.
By taking square roots, we obtain (c).
(d) From (a) and (c), it follows that
|z| = |(z + w) ‚àíw| ‚â§|z + w| + | ‚àíw| = |z + w| + |w|.
So
|z| ‚àí|w| ‚â§|z + w|,
proving (d).
It is interesting as well as useful that complex numbers have both a ge-
ometric and an algebraic representation. Suppose that z = a + bi, where a
and b are real numbers. We may represent z as a vector in the complex plane
(see Figure D.1(a)). Notice that, as in R2, there are two axes, the real axis
and the imaginary axis. The real and imaginary parts of z are the Ô¨Årst and
second coordinates, and the absolute value of z gives the length of the vector
z. It is clear that addition of complex numbers may be represented as in R2
using the parallelogram law.

560
Appendices






imaginary axis
real axis
z = a + bi
b
a
0
(a)
................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................


3
@
@
@
@
I
.....................................
.................................................................................................
K
=
Œ∏
œÜ
eiŒ∏
z = |z|eiœÜ
1
‚àí1
0
(b)
Figure D.1
In Section 2.7 (p.132), we introduce Euler‚Äôs formula. The special case
eiŒ∏ = cos Œ∏ + i sin Œ∏ is of particular interest. Because of the geometry we have
introduced, we may represent the vector eiŒ∏ as in Figure D.1(b); that is, eiŒ∏
is the unit vector that makes an angle Œ∏ with the positive real axis. From
this Ô¨Ågure, we see that any nonzero complex number z may be depicted as
a multiple of a unit vector, namely, z = |z|eiœÜ, where œÜ is the angle that the
vector z makes with the positive real axis. Thus multiplication, as well as
addition, has a simple geometric interpretation: If z = |z|eiŒ∏ and w = |w|eiœâ
are two nonzero complex numbers, then from the properties established in
Section 2.7 and Theorem D.3, we have
zw = |z|eiŒ∏ ¬∑ |w|eiœâ = |zw|ei(Œ∏+œâ).
So zw is the vector whose length is the product of the lengths of z and w,
and makes the angle Œ∏ + œâ with the positive real axis.
Our motivation for enlarging the set of real numbers to the set of complex
numbers is to obtain a Ô¨Åeld such that every polynomial with nonzero degree
having coeÔ¨Écients in that Ô¨Åeld has a zero. Our next result guarantees that
the Ô¨Åeld of complex numbers has this property.
Theorem D.4 (The Fundamental Theorem of Algebra). Suppose
that p(z) = anzn + an‚àí1zn‚àí1 + ¬∑ ¬∑ ¬∑ + a1z + a0 is a polynomial in P(C) of
degree n ‚â•1. Then p(z) has a zero.
The following proof is based on one in the book Principles of Mathematical
Analysis 3d., by Walter Rudin (McGraw-Hill Higher Education, New York,
1976).
Proof. We want to Ô¨Ånd z0 in C such that p(z0) = 0. Let m be the greatest
lower bound of {|p(z)|: z ‚ààC}. For |z| = s > 0, we have
|p(z)| = |anzn + an‚àí1zn‚àí1 + ¬∑ ¬∑ ¬∑ + a0|

Appendix D
Complex Numbers
561
‚â•|an||zn| ‚àí|an‚àí1||z|n‚àí1 ‚àí¬∑ ¬∑ ¬∑ ‚àí|a0|
= |an|sn ‚àí|an‚àí1|sn‚àí1 ‚àí¬∑ ¬∑ ¬∑ ‚àí|a0|
= sn[|an| ‚àí|an‚àí1|s‚àí1 ‚àí¬∑ ¬∑ ¬∑ ‚àí|a0|s‚àín].
Because the last expression approaches inÔ¨Ånity as s approaches inÔ¨Ånity, we
may choose a closed disk D about the origin such that |p(z)| > m + 1 if z is
not in D. It follows that m is the greatest lower bound of {|p(z)|: z ‚ààD}.
Because D is closed and bounded and p(z) is continuous, there exists z0 in
D such that |p(z0)| = m.
We want to show that m = 0.
We argue by
contradiction.
Assume that m Ã∏= 0. Let q(z) = p(z + z0)
p(z0)
. Then q(z) is a polynomial of
degree n, q(0) = 1, and |q(z)| ‚â•1 for all z in C. So we may write
q(z) = 1 + bkzk + bk+1zk+1 + ¬∑ ¬∑ ¬∑ + bnzn,
where bk Ã∏= 0. Because ‚àí|bk|
bk
has modulus one, we may pick a real number Œ∏
such that eikŒ∏ = ‚àí|bk|
bk
, or eikŒ∏bk = ‚àí|bk|. For any r > 0, we have
q(reiŒ∏) = 1 + bkrkeikŒ∏ + bk+1rk+1ei(k+1)Œ∏ + ¬∑ ¬∑ ¬∑ + bnrneinŒ∏
= 1 ‚àí|bk|rk + bk+1rk+1ei(k+1)Œ∏ + ¬∑ ¬∑ ¬∑ + bnrneinŒ∏.
Choose r small enough so that 1 ‚àí|bk|rk > 0. Then
|q(reiŒ∏)| ‚â§1 ‚àí|bk|rk + |bk+1|rk+1 + ¬∑ ¬∑ ¬∑ + |bn|rn
= 1 ‚àírk[|bk| ‚àí|bk+1|r ‚àí¬∑ ¬∑ ¬∑ ‚àí|bn|rn‚àík].
Now choose r even smaller, if necessary, so that the expression within the
brackets is positive. We obtain that |q(reiŒ∏)| < 1. But this is a contradiction.
The following important corollary is a consequence of Theorem D.4 and
the division algorithm for polynomials (Theorem E.1).
Corollary. If p(z) = anzn + an‚àí1zn‚àí1 + ¬∑ ¬∑ ¬∑ + a1z + a0 is a polynomial
of degree n ‚â•1 with complex coeÔ¨Écients, then there exist complex numbers
c1, c2, ¬∑ ¬∑ ¬∑ , cn (not necessarily distinct) such that
p(z) = an(z ‚àíc1)(z ‚àíc2) ¬∑ ¬∑ ¬∑ (z ‚àícn).
Proof. Exercise.
A Ô¨Åeld is called algebraically closed if it has the property that every
polynomial of positive degree with coeÔ¨Écients from that Ô¨Åeld factors as a
product of polynomials of degree 1. Thus the preceding corollary asserts that
the Ô¨Åeld of complex numbers is algebraically closed.

562
Appendices
APPENDIX E
POLYNOMIALS
In this appendix, we discuss some useful properties of the polynomials with
coeÔ¨Écients from a Ô¨Åeld.
For the deÔ¨Ånition of a polynomial, refer to Sec-
tion 1.2. Throughout this appendix, we assume that all polynomials have
coeÔ¨Écients from a Ô¨Åxed Ô¨Åeld F.
DeÔ¨Ånition. A polynomial f(x) divides a polynomial g(x) if there exists
a polynomial q(x) such that g(x) = f(x)q(x).
Our Ô¨Årst result shows that the familiar long division process for polyno-
mials with real coeÔ¨Écients is valid for polynomials with coeÔ¨Écients from an
arbitrary Ô¨Åeld.
Theorem E.1 (The Division Algorithm for Polynomials).
Let
f(x) be a polynomial of degree n, and let g(x) be a polynomial of degree
m ‚â•0. Then there exist unique polynomials q(x) and r(x) such that
f(x) = q(x)g(x) + r(x),
(1)
where the degree of r(x) is less than m.
Proof. We begin by establishing the existence of q(x) and r(x) that sat-
isfy (1).
Case 1.
If n < m, take q(x) = 0 and r(x) = f(x) to satisfy (1).
Case 2.
When 0 ‚â§m ‚â§n, we apply mathematical induction on n.
First suppose that n = 0. Then m = 0, and it follows that f(x) and g(x)
are nonzero constants. Hence we may take q(x) = f(x)/g(x) and r(x) = 0 to
satisfy (1).
Now suppose that the result is valid for all polynomials with degree less
than n for some Ô¨Åxed n > 0, and assume that f(x) has degree n. Suppose
that
f(x) = anxn + an‚àí1xn‚àí1 + ¬∑ ¬∑ ¬∑ + a1x + a0
and
g(x) = bmxm + bm‚àí1xm‚àí1 + ¬∑ ¬∑ ¬∑ + b1x + b0,
and let h(x) be the polynomial deÔ¨Åned by
h(x) = f(x) ‚àíanb‚àí1
m xn‚àímg(x).
(2)
Then h(x) is a polynomial of degree less than n, and therefore we may ap-
ply the induction hypothesis or Case 1 (whichever is relevant) to obtain
polynomials q1(x) and r(x) such that r(x) has degree less than m and
h(x) = q1(x)g(x) + r(x).
(3)

Appendix E
Polynomials
563
Combining (2) and (3) and solving for f(x) gives us f(x) = q(x)g(x) + r(x)
with q(x) = anb‚àí1
m xn‚àím + q1(x), which establishes (a) and (b) for any n ‚â•0
by mathematical induction. This establishes the existence of q(x) and r(x).
We now show the uniqueness of q(x) and r(x). Suppose that q1(x), q2(x),
r1(x), and r2(x) exist such that r1(x) and r2(x) each has degree less than m
and
f(x) = q1(x)g(x) + r1(x) = q2(x)g(x) + r2(x).
Then
[q1(x) ‚àíq2(x)] g(x) = r2(x) ‚àír1(x).
(4)
The right side of (4) is a polynomial of degree less than m. Since g(x) has
degree m, it must follow that q1(x) ‚àíq2(x) is the zero polynomial. Hence
q1(x) = q2(x); thus r1(x) = r2(x) by (4).
In the context of Theorem E.1, we call q(x) and r(x) the quotient and
remainder, respectively, for the division of f(x) by g(x).
For example,
suppose that F is the Ô¨Åeld of complex numbers.
Then the quotient and
remainder for the division of
f(x) = (3 + i)x5 ‚àí(1 ‚àíi)x4 + 6x3 + (‚àí6 + 2i)x2 + (2 + i)x + 1
by
g(x) = (3 + i)x2 ‚àí2ix + 4
are, respectively,
q(x) = x3 + ix2 ‚àí2
and
r(x) = (2 ‚àí3i)x + 9.
Corollary 1. Let f(x) be a polynomial of positive degree, and let a ‚ààF.
Then f(a) = 0 if and only if x ‚àía divides f(x).
Proof. Suppose that x ‚àía divides f(x). Then there exists a polynomial
q(x) such that f(x) = (x ‚àía)q(x). Thus f(a) = (a ‚àía)q(a) = 0¬∑q(a) = 0.
Conversely, suppose that f(a) = 0. By the division algorithm, there exist
polynomials q(x) and r(x) such that r(x) has degree less than one and
f(x) = q(x)(x ‚àía) + r(x).
Substituting a for x in the equation above, we obtain r(a) = 0. Since r(x)
has degree less than 1, it must be the constant polynomial r(x) = 0. Thus
f(x) = q(x)(x ‚àía).

564
Appendices
For any polynomial f(x) with coeÔ¨Écients from a Ô¨Åeld F, an element a ‚ààF
is called a zero of f(x) if f(a) = 0. With this terminology, the preceding
corollary states that a is a zero of f(x) if and only if x ‚àía divides f(x).
Corollary 2. Any polynomial of degree n ‚â•1 has at most n distinct
zeros.
Proof. The proof is by mathematical induction on n. The result is obvious
if n = 1. Now suppose that the result is true for some positive integer n, and
let f(x) be a polynomial of degree n + 1. If f(x) has no zeros, then there is
nothing to prove. Otherwise, if a is a zero of f(x), then by Corollary 1 we
may write f(x) = (x‚àía)q(x) for some polynomial q(x). Note that q(x) must
be of degree n; therefore, by the induction hypothesis, q(x) can have at most
n distinct zeros. Since any zero of f(x) distinct from a is also a zero of q(x),
it follows that f(x) can have at most n + 1 distinct zeros.
Polynomials having no common divisors arise naturally in the study of
canonical forms. (See Chapter 7.)
DeÔ¨Ånition. Two nonzero polynomials are called relatively prime if no
polynomial of positive degree divides each of them.
For example, the polynomials with real coeÔ¨Écients f(x) = x2(x ‚àí1) and
h(x) = (x ‚àí1)(x ‚àí2) are not relatively prime because x ‚àí1 divides each of
them. On the other hand, consider f(x) and g(x) = (x ‚àí2)(x ‚àí3), which do
not appear to have common factors. Could other factorizations of f(x) and
g(x) reveal a hidden common factor? We will soon see (Theorem E.9) that
the preceding factors are the only ones. Thus f(x) and g(x) are relatively
prime because they have no common factors of positive degree.
Theorem E.2. If f1(x) and f2(x) are relatively prime polynomials, there
exist polynomials q1(x) and q2(x) such that
q1(x)f1(x) + q2(x)f2(x) = 1,
where 1 denotes the constant polynomial with value 1.
Proof. Without loss of generality, assume that the degree of f1(x) is greater
than or equal to the degree of f2(x). The proof is by mathematical induction
on the degree of f2(x). If f2(x) has degree 0, then f2(x) is a nonzero constant
c. In this case, we can take q1(x) = 0 and q2(x) = 1/c.
Now suppose that the theorem holds whenever the polynomial of lesser
degree has degree less than n for some positive integer n, and suppose that
f2(x) has degree n. By the division algorithm, there exist polynomials q(x)
and r(x) such that r(x) has degree less than n and
f1(x) = q(x)f2(x) + r(x).
(5)

Appendix E
Polynomials
565
Since f1(x) and f2(x) are relatively prime, r(x) is not the zero polynomial. We
claim that f2(x) and r(x) are relatively prime. Suppose otherwise; then there
exists a polynomial g(x) of positive degree that divides both f2(x) and r(x).
Hence, by (5), g(x) also divides f1(x), contradicting the fact that f1(x) and
f2(x) are relatively prime. Since r(x) has degree less than n, we may apply
the induction hypothesis to f2(x) and r(x). Thus there exist polynomials
g1(x) and g2(x) such that
g1(x)f2(x) + g2(x)r(x) = 1.
(6)
Combining (5) and (6), we have
1 = g1(x)f2(x) + g2(x) [f1(x) ‚àíq(x)f2(x)]
= g2(x)f1(x) + [g1(x) ‚àíg2(x)q(x)] f2(x).
Thus, setting q1(x) = g2(x) and q2(x) = g1(x) ‚àíg2(x)q(x), we obtain the
desired result.
Example 1
Let f1(x) = x3 ‚àíx2 + 1 and f2(x) = (x ‚àí1)2. As polynomials with real
coeÔ¨Écients, f1(x) and f2(x) are relatively prime. It is easily veriÔ¨Åed that the
polynomials q1(x) = ‚àíx + 2 and q2(x) = x2 ‚àíx ‚àí1 satisfy
q1(x)f1(x) + q2(x)f2(x) = 1,
and hence these polynomials satisfy the conclusion of Theorem E.2.
‚ô¶
Throughout Chapters 5, 6, and 7, we consider linear operators that are
polynomials in a particular operator T and matrices that are polynomials in a
particular matrix A. For these operators and matrices, the following notation
is convenient.
DeÔ¨Ånitions. Let
f(x) = a0 + a1(x) + ¬∑ ¬∑ ¬∑ + anxn
be a polynomial with coeÔ¨Écients from a Ô¨Åeld F. If T is a linear operator on
a vector space V over F, we deÔ¨Åne
f(T) = a0I + a1T + ¬∑ ¬∑ ¬∑ + anTn.
Similarly, if A is a n √ó n matrix with entries from F, we deÔ¨Åne
f(A) = a0I + a1A + ¬∑ ¬∑ ¬∑ + anAn.

566
Appendices
Example 2
Let T be the linear operator on R2 deÔ¨Åned by T(a, b) = (2a + b, a ‚àíb), and
let f(x) = x2 + 2x ‚àí3. It is easily checked that T2(a, b) = (5a + b, a + 2b); so
f(T)(a, b) = (T2 + 2T ‚àí3I)(a, b)
= (5a + b, a + 2b) + (4a + 2b, 2a ‚àí2b) ‚àí3(a, b)
= (6a + 3b, 3a ‚àí3b).
Similarly, if
A =

2
1
1
‚àí1
	
,
then
f(A) = A2+2A‚àí3I =

5
1
1
2
	
+2

2
1
1
‚àí1
	
‚àí3

1
0
0
1
	
=

6
3
3
‚àí3
	
.
‚ô¶
The next three results use this notation.
Theorem E.3. Let f(x) be a polynomial with coeÔ¨Écients from a Ô¨Åeld F,
and let T be a linear operator on a vector space V over F. Then the following
statements are true.
(a) f(T) is a linear operator on V.
(b) If Œ≤ is a Ô¨Ånite ordered basis for V and A = [T]Œ≤, then [f(T)]Œ≤ = f(A).
Proof. Exercise.
Theorem E.4. Let T be a linear operator on a vector space V over a
Ô¨Åeld F, and let A be a square matrix with entries from F. Then, for any
polynomials f1(x) and f2(x) with coeÔ¨Écients from F,
(a) f1(T)f2(T) = f2(T)f1(T)
(b) f1(A)f2(A) = f2(A)f1(A).
Proof. Exercise.
Theorem E.5. Let T be a linear operator on a vector space V over a
Ô¨Åeld F, and let A be an n √ó n matrix with entries from F. If f1(x) and
f2(x) are relatively prime polynomials with entries from F, then there exist
polynomials q1(x) and q2(x) with entries from F such that
(a) q1(T)f1(T) + q2(T)f2(T) = I
(b) q1(A)f1(A) + q2(A)f2(A) = I.
Proof. Exercise.

Appendix E
Polynomials
567
In Chapters 5 and 7, we are concerned with determining when a linear
operator T on a Ô¨Ånite-dimensional vector space can be diagonalized and with
Ô¨Ånding a simple (canonical) representation of T. Both of these problems are
aÔ¨Äected by the factorization of a certain polynomial determined by T (the
characteristic polynomial of T). In this setting, particular types of polynomi-
als play an important role.
DeÔ¨Ånitions. A polynomial f(x) with coeÔ¨Écients from a Ô¨Åeld F is called
monic if its leading coeÔ¨Écient is 1. If f(x) has positive degree and cannot be
expressed as a product of polynomials with coeÔ¨Écients from F each having
positive degree, then f(x) is called irreducible.
Observe that whether a polynomial is irreducible depends on the Ô¨Åeld F
from which its coeÔ¨Écients come. For example, f(x) = x2 + 1 is irreducible
over the Ô¨Åeld of real numbers, but it is not irreducible over the Ô¨Åeld of complex
numbers since x2 + 1 = (x + i)(x ‚àíi).
Clearly any polynomial of degree 1 is irreducible. Moreover, for polyno-
mials with coeÔ¨Écients from an algebraically closed Ô¨Åeld, the polynomials of
degree 1 are the only irreducible polynomials.
The following facts are easily established.
Theorem E.6. Let œÜ(x) and f(x) be polynomials. If œÜ(x) is irreducible
and œÜ(x) does not divide f(x), then œÜ(x) and f(x) are relatively prime.
Proof. Exercise.
Theorem E.7. Any two distinct irreducible monic polynomials are rela-
tively prime.
Proof. Exercise.
Theorem E.8. Let f(x), g(x), and œÜ(x) be polynomials. If œÜ(x) is ir-
reducible and divides the product f(x)g(x), then œÜ(x) divides f(x) or œÜ(x)
divides g(x).
Proof. Suppose that œÜ(x) does not divide f(x). Then œÜ(x) and f(x) are
relatively prime by Theorem E.6, and so there exist polynomials q1(x) and
q2(x) such that
1 = q1(x)œÜ(x) + q2(x)f(x).
Multiplying both sides of this equation by g(x) yields
g(x) = q1(x)œÜ(x)g(x) + q2(x)f(x)g(x).
(7)
Since œÜ(x) divides f(x)g(x), there is a polynomial h(x) such that f(x)g(x) =
œÜ(x)h(x). Thus (7) becomes
g(x) = q1(x)œÜ(x)g(x) + q2(x)œÜ(x)h(x) = œÜ(x) [q1(x)g(x) + q2(x)h(x)] .
So œÜ(x) divides g(x).

568
Appendices
Corollary. Let œÜ(x), œÜ1(x), œÜ2(x), . . . , œÜn(x) be irreducible monic polyno-
mials. If œÜ(x) divides the product œÜ1(x)œÜ2(x) ¬∑ ¬∑ ¬∑ œÜn(x), then œÜ(x) = œÜi(x)
for some i (i = 1, 2, . . . , n).
Proof. We prove the corollary by mathematical induction on n. For n = 1,
the result is an immediate consequence of Theorem E.7. Suppose then that for
some n > 1, the corollary is true for any n‚àí1 irreducible monic polynomials,
and let œÜ1(x), œÜ2(x), . . . , œÜn(x) be n irreducible polynomials. If œÜ(x) divides
œÜ1(x)œÜ2(x) ¬∑ ¬∑ ¬∑ œÜn(x) = [œÜ1(x)œÜ2(x) ¬∑ ¬∑ ¬∑ œÜn‚àí1(x)] œÜn(x),
then œÜ(x) divides the product œÜ1(x)œÜ2(x) ¬∑ ¬∑ ¬∑ œÜn‚àí1(x) or œÜ(x) divides œÜn(x) by
Theorem E.8. In the Ô¨Årst case, œÜ(x) = œÜi(x) for some i (i = 1, 2, . . . , n‚àí1) by
the induction hypothesis; in the second case, œÜ(x) = œÜn(x) by Theorem E.7.
We are now able to establish the unique factorization theorem, which is
used throughout Chapters 5 and 7. This result states that every polynomial
of positive degree is uniquely expressible as a constant times a product of
irreducible monic polynomials.
Theorem E.9 (Unique Factorization Theorem for Polynomials).
For any polynomial f(x) of positive degree, there exist a unique constant
c; unique distinct irreducible monic polynomials œÜ1(x), œÜ2(x), . . . , œÜk(x); and
unique positive integers n1, n2, . . . , nk such that
f(x) = c[œÜ1(x)]n1[œÜ2(x)]n2 ¬∑ ¬∑ ¬∑ [œÜk(x)]nk.
Proof. We begin by showing the existence of such a factorization using
mathematical induction on the degree of f(x). If f(x) is of degree 1, then
f(x) = ax + b for some constants a and b with a Ã∏= 0. Setting œÜ(x) = x + b/a,
we have f(x) = aœÜ(x). Since œÜ(x) is an irreducible monic polynomial, the
result is proved in this case. Now suppose that the conclusion is true for any
polynomial with positive degree less than some integer n > 1, and let f(x)
be a polynomial of degree n. Then
f(x) = anxn + ¬∑ ¬∑ ¬∑ + a1x + a0
for some constants ai with an Ã∏= 0. If f(x) is irreducible, then
f(x) = an

xn + an‚àí1
an
xn‚àí1 + ¬∑ ¬∑ ¬∑ + a1
an
+ a0
an
	
is a representation of f(x) as a product of an and an irreducible monic poly-
nomial. If f(x) is not irreducible, then f(x) = g(x)h(x) for some polynomials
g(x) and h(x), each of positive degree less than n. The induction hypothesis

Appendix E
Polynomials
569
guarantees that both g(x) and h(x) factor as products of a constant and pow-
ers of distinct irreducible monic polynomials. Consequently f(x) = g(x)h(x)
also factors in this way. Thus, in either case, f(x) can be factored as a product
of a constant and powers of distinct irreducible monic polynomials.
It remains to establish the uniqueness of such a factorization. Suppose
that
f(x) = c[œÜ1(x)]n1[œÜ2(x)]n2 ¬∑ ¬∑ ¬∑ [œÜk(x)]nk
= d[œà1(x)]m1[œà2(x)]m2 ¬∑ ¬∑ ¬∑ [œàr(x)]mr,
(8)
where c and d are constants, œÜi(x) and œàj(x) are irreducible monic polynomi-
als, and ni and mj are positive integers for i = 1, 2, . . . , k and j = 1, 2, . . . , r.
Clearly both c and d must be the leading coeÔ¨Écient of f(x); hence c = d.
Dividing by c, we Ô¨Ånd that (8) becomes
[œÜ1(x)]n1[œÜ2(x)]n2 ¬∑ ¬∑ ¬∑ [œÜk(x)]nk = [œà1(x)]m1[œà2(x)]m2 ¬∑ ¬∑ ¬∑ [œàr(x)]mr.
(9)
So œÜi(x) divides the right side of (9) for i = 1, 2, . . . , k. Consequently, by the
corollary to Theorem E.8, each œÜi(x) equals some œàj(x), and similarly, each
œàj(x) equals some œÜi(x). We conclude that r = k and that, by renumbering
if necessary, œÜi(x) = œài(x) for i = 1, 2, . . . , k. Suppose that ni Ã∏= mi for some
i. Without loss of generality, we may suppose that i = 1 and n1 > m1. Then
by canceling [œÜ1(x)]m1 from both sides of (9), we obtain
[œÜ1(x)]n1‚àím1[œÜ2(x)]n2 ¬∑ ¬∑ ¬∑ [œÜk(x)]nk = [œÜ2(x)]m2 ¬∑ ¬∑ ¬∑ [œÜk(x)]mk.
(10)
Since n1 ‚àím1 > 0, œÜ1(x) divides the left side of (10) and hence divides the
right side also. So œÜ1(x) = œÜi(x) for some i = 2, . . . , k by the corollary to
Theorem E.8. But this contradicts that œÜ1(x), œÜ2(x), . . . , œÜk(x) are distinct.
Hence the factorizations of f(x) in (8) are the same.
It is often useful to regard a polynomial f(x) = anxn +¬∑ ¬∑ ¬∑+a1x+a0 with
coeÔ¨Écients from a Ô¨Åeld F as a function f : F ‚ÜíF. In this case, the value of
f at c ‚ààF is f(c) = ancn + ¬∑ ¬∑ ¬∑ + a1c + a0. Unfortunately, for arbitrary Ô¨Åelds
there is not a one-to-one correspondence between polynomials and polynomial
functions. For example, if f(x) = x2 and g(x) = x are two polynomials over
the Ô¨Åeld Z2 (deÔ¨Åned in Example 4 of Appendix C), then f(x) and g(x) have
diÔ¨Äerent degrees and hence are not equal as polynomials. But f(a) = g(a) for
all a ‚ààZ2, so that f and g are equal polynomial functions. Our Ô¨Ånal result
shows that this anomaly cannot occur over an inÔ¨Ånite Ô¨Åeld.
Theorem E.10. Let f(x) and g(x) be polynomials with coeÔ¨Écients from
an inÔ¨Ånite Ô¨Åeld F. If f(a) = g(a) for all a ‚ààF, then f(x) and g(x) are equal.
Proof. Suppose that f(a) = g(a) for all a ‚ààF. DeÔ¨Åne h(x) = f(x) ‚àíg(x),
and suppose that h(x) is of degree n ‚â•1. It follows from Corollary 2 to

570
Appendices
Theorem E.1 that h(x) can have at most n zeroes. But
h(a) = f(a) ‚àíg(a) = 0
for every a ‚ààF, contradicting the assumption that h(x) has positive degree.
Thus h(x) is a constant polynomial, and since h(a) = 0 for each a ‚ààF, it
follows that h(x) is the zero polynomial. Hence f(x) = g(x).

Answers
to Selected Exercises
CHAPTER 1
SECTION 1.1
1. Only the pairs in (b) and (c) are parallel.
2. (a) x = (3, ‚àí2, 4) + t(‚àí8, 9, ‚àí3)
(c) x = (3, 7, 2) + t(0, 0, ‚àí10)
3. (a) x = (2, ‚àí5, ‚àí1) + s(‚àí2, 9, 7) + t(‚àí5, 12, 2)
(c) x = (‚àí8, 2, 0) + s(9, 1, 0) + t(14, ‚àí7, 0)
SECTION 1.2
1. (a) T
(b) F
(c) F
(d) F
(e) T
(f) F
(g) F
(h) F
(i) T
(j) T
(k) T
3. M13 = 3, M21 = 4, and M22 = 5
4. (a)

6
3
2
‚àí4
3
9
	
(c)
8
20
‚àí12
4
0
28
	
(e) 2x4 + x3 + 2x2 ‚àí2x + 10
(g) 10x7 ‚àí30x4 + 40x2 ‚àí15x
13. No, (VS 4) fails.
14. Yes
15. No
17. No, (VS 5) fails.
22. 2mn
SECTION 1.3
1. (a) F
(b) F
(c) T
(d) F
(e) T
(f) F
(g) F
2. (a)
‚àí4
5
2
‚àí1
	
; the trace is ‚àí5
(c)
‚àí3
0
6
9
‚àí2
1
	
(e)
‚éõ
‚éú
‚éú
‚éù
1
‚àí1
3
5
‚éû
‚éü
‚éü
‚é†
(g) 
5
6
7
8. (a) Yes
(c) Yes
(e) No
11. No, the set is not closed under addition.
15. Yes
571

572
Answers to Selected Exercises
SECTION 1.4
1. (a) T
(b) F
(c) T
(d) F
(e) T
(f) F
2. (a) {r(1, 1, 0, 0) + s(‚àí3, 0, ‚àí2, 1) + (5, 0, 4, 0): r, s ‚ààR}
(c) There are no solutions.
(e) {r(10, ‚àí3, 1, 0, 0) + s(‚àí3, 2, 0, 1, 0) + (‚àí4, 3, 0, 0, 5): r, s ‚ààR}
3. (a) Yes
(c) No
(e) No
4. (a) Yes
(c) Yes
(e) No
5. (a) Yes
(c) No
(e) Yes
(g) Yes
SECTION 1.5
1. (a) F
(b) T
(c) F
(d) F
(e) T
(f) T
2. (a) linearly dependent
(c) linearly independent
(e) linearly dependent
(g) linearly dependent
(i) linearly independent
7.
1
0
0
0
	
,
0
0
0
1
	
11. 2n
SECTION 1.6
1. (a) F
(b) T
(c) F
(d) F
(e) T
(f) F
(g) F
(h) T
(i) F
(j) T
(k) T
(l) T
2. (a) Yes
(c) Yes
(e) No
3. (a) No
(c) Yes
(e) No
4. No
5. No
7. {u1, u2, u5}
9. (a1, a2, a3, a4) = a1u1 + (a2 ‚àía1)u2 + (a3 ‚àía2)u3 + (a4 ‚àía3)u4
10. (a) ‚àí4x2 ‚àíx + 8
(c) ‚àíx3 + 2x2 + 4x ‚àí5
13. {(1, 1, 1)}
15. n2 ‚àí1
17.
1
2n(n ‚àí1)
26. n
30. dim(W1) = 3, dim(W2) = 2, dim(W1 + W2) = 4, and dim(W1 ‚à©W2) = 1
SECTION 1.7
1. (a) F
(b) F
(c) F
(d) T
(e) T
(f) T
CHAPTER 2
SECTION 2.1
1. (a) T
(b) F
(c) F
(d) T
(e) F
(f) F
(g) T
(h) F

Answers to Selected Exercises
573
2. The nullity is 1, and the rank is 2. T is not one-to-one but is onto.
4. The nullity is 4, and the rank is 2. T is neither one-to-one nor onto.
5. The nullity is 0, and the rank is 3. T is one-to-one but not onto.
10. T(2, 3) = (5, 11). T is one-to-one.
12. No.
SECTION 2.2
1. (a) T
(b) T
(c) F
(d) T
(e) T
(f) F
2. (a)
‚éõ
‚éù
2
‚àí1
3
4
1
0
‚éû
‚é†
(c) 
2
1
‚àí3
(d)
‚éõ
‚éù
0
2
1
‚àí1
4
5
1
0
1
‚éû
‚é†
(f)
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
0
0
¬∑ ¬∑ ¬∑
0
1
0
0
¬∑ ¬∑ ¬∑
1
0
...
...
...
...
0
1
¬∑ ¬∑ ¬∑
0
0
1
0
¬∑ ¬∑ ¬∑
0
0
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
(g) 
1
0
¬∑ ¬∑ ¬∑
0
1
3. [T]Œ≥
Œ≤ =
‚éõ
‚éú
‚éù
‚àí1
3
‚àí1
0
1
2
3
0
‚éû
‚éü
‚é†
and [T]Œ≥
Œ± =
‚éõ
‚éú
‚éù
‚àí7
3
‚àí11
3
2
3
2
3
4
3
‚éû
‚éü
‚é†
5. (a)
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
0
0
1
0
0
1
0
0
0
0
0
1
‚éû
‚éü
‚éü
‚é†
(b)
‚éõ
‚éú
‚éú
‚éù
0
1
0
2
2
2
0
0
0
0
0
2
‚éû
‚éü
‚éü
‚é†
(e)
‚éõ
‚éú
‚éú
‚éù
1
‚àí2
0
4
‚éû
‚éü
‚éü
‚é†
10.
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
1
1
0
¬∑ ¬∑ ¬∑
0
0
1
1
¬∑ ¬∑ ¬∑
0
0
0
1
¬∑ ¬∑ ¬∑
0
...
...
...
...
0
0
0
¬∑ ¬∑ ¬∑
1
0
0
0
¬∑ ¬∑ ¬∑
1
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
SECTION 2.3
1. (a) F
(b) T
(c) F
(d) T
(e) F
(f) F
(g) F
(h) F
(i) T
(j) T
2. (a) A(2B + 3C) =
20
‚àí9
18
5
10
8
	
and
A(BD) =

29
‚àí26
	
(b) AtB =
23
19
0
26
‚àí1
10
	
and
CB = 
27
7
9
3. (a) [T]Œ≤ =
‚éõ
‚éù
2
3
0
0
3
6
0
0
4
‚éû
‚é†, [U]Œ≥
Œ≤ =
‚éõ
‚éù
1
1
0
0
0
1
1
‚àí1
0
‚éû
‚é†, and [UT]Œ≥
Œ≤ =
‚éõ
‚éù
2
6
6
0
0
4
2
0
‚àí6
‚éû
‚é†
4. (a)
‚éõ
‚éú
‚éú
‚éù
1
‚àí1
4
6
‚éû
‚éü
‚éü
‚é†
(c) (5)

574
Answers to Selected Exercises
12. (a) No.
(b) No.
SECTION 2.4
1. (a) F
(b) T
(c) F
(d) F
(e) T
(f) F
(g) T
(h) T
(i) T
2. (a) No
(b) No
(c) Yes
(d) No
(e) No
(f) Yes
3. (a) No
(b) Yes
(c) Yes
(d) No
19. (b) [T]Œ≤ =
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
0
0
1
0
0
1
0
0
0
0
0
1
‚éû
‚éü
‚éü
‚é†
SECTION 2.5
1. (a) F
(b) T
(c) T
(d) F
(e) T
2. (a)
a1
b1
a2
b2
	
(c)
3
‚àí1
5
‚àí2
	
3. (a)
‚éõ
‚éù
a2
b2
c2
a1
b1
c1
a0
b0
c0
‚éû
‚é†
(c)
‚éõ
‚éù
0
‚àí1
0
1
0
0
‚àí3
2
1
‚éû
‚é†
(e)
‚éõ
‚éù
5
‚àí6
3
0
4
‚àí1
3
‚àí1
2
‚éû
‚é†
4. [T]Œ≤‚Ä≤ =

2
‚àí1
‚àí1
1
	 2
1
1
‚àí3
	 1
1
1
2
	
=

8
13
‚àí5
‚àí9
	
5. [T]Œ≤‚Ä≤ =
‚éõ
‚éù
1
2
1
2
1
2
‚àí1
2
‚éû
‚é†
0
1
0
0
	 1
1
1
‚àí1
	
=
‚éõ
‚éù
1
2
‚àí1
2
1
2
‚àí1
2
‚éû
‚é†
6. (a) Q =
1
1
1
2
	
, [LA]Œ≤ =

6
11
‚àí2
‚àí4
	
(c) Q =
‚éõ
‚éù
1
1
1
1
0
1
1
1
2
‚éû
‚é†, [LA]Œ≤ =
‚éõ
‚éù
2
2
2
‚àí2
‚àí3
‚àí4
1
1
2
‚éû
‚é†
7. (a) T(x, y) =
1
1 + m2 ((1 ‚àím2)x + 2my, 2mx + (m2 ‚àí1)y)
SECTION 2.6
1. (a) F
(b) T
(c) T
(d) T
(e) F
(f) T
(g) T
(h) F
2. The functions in (a), (c), (e), and (f) are linear functionals.
3. (a) f1(x, y, z) = x ‚àí1
2y, f2(x, y, z) = 1
2y, and f3(x, y, z) = ‚àíx + z
5. The basis for V is {p1(x), p2(x)}, where p1(x) = 2 ‚àí2x and p2(x) = ‚àí1
2 + x.
7. (a) Tt(f) = g, where g(a + bx) = ‚àí3a ‚àí4b
(b) [Tt]Œ≤‚àó
Œ≥‚àó=
‚àí1
1
‚àí2
1
	
(c) [T]Œ≥
Œ≤ =
‚àí1
‚àí2
1
1
	

Answers to Selected Exercises
575
SECTION 2.7
1. (a) T
(b) T
(c) F
(d) F
(e) T
(f) F
(g) T
2. (a) F
(b) F
(c) T
(d) T
(e) F
3. (a) {e‚àít, te‚àít}
(c) {e‚àít, te‚àít, et, tet}
(e) {e‚àít, et cos 2t, et sin 2t}
4. (a) {e(1+
‚àö
5)t/2, e(1‚àí
‚àö
5)t/2}
(c) {1, e‚àí4t, e‚àí2t}
CHAPTER 3
SECTION 3.1
1. (a) T
(b) F
(c) T
(d) F
(e) T
(f) F
(g) T
(h) F
(i) T
2. Adding ‚àí2 times column 1 to column 2 transforms A into B.
3. (a)
‚éõ
‚éù
0
0
1
0
1
0
1
0
0
‚éû
‚é†
(c)
‚éõ
‚éù
1
0
0
0
1
0
2
0
1
‚éû
‚é†
SECTION 3.2
1. (a) F
(b) F
(c) T
(d) T
(e) F
(f) T
(g) T
(h) T
(i) T
2. (a) 2
(c) 2
(e) 3
(g) 1
4. (a)
‚éõ
‚éù
1
0
0
0
0
1
0
0
0
0
0
0
‚éû
‚é†; the rank is 2.
5. (a) The rank is 2, and the inverse is
‚àí1
2
1
‚àí1
	
.
(c) The rank is 2, and so no inverse exists.
(e) The rank is 3, and the inverse is
‚éõ
‚éú
‚éù
1
6
‚àí1
3
1
2
1
2
0
‚àí1
2
‚àí1
6
1
3
1
2
‚éû
‚éü
‚é†.
(g) The rank if 4, and the inverse is
‚éõ
‚éú
‚éú
‚éù
‚àí51
15
7
12
31
‚àí9
‚àí4
‚àí7
‚àí10
3
1
2
‚àí3
1
1
1
‚éû
‚éü
‚éü
‚é†.
6. (a) T‚àí1(ax2 + bx + c) = ‚àíax2 ‚àí(4a + b)x ‚àí(10a + 2b + c)
(c) T‚àí1(a, b, c) =  1
6a ‚àí1
3b + 1
2c, 1
2a ‚àí1
2c, ‚àí1
6 + 1
3b + 1
2c
(e) T‚àí1(a, b, c) =  1
2a ‚àíb + 1
2c
x2 + 
‚àí1
2a + 1
2c
x + b
7.
‚éõ
‚éù
1
0
0
0
1
0
1
0
1
‚éû
‚é†
‚éõ
‚éù
1
0
0
1
1
0
0
0
1
‚éû
‚é†
‚éõ
‚éù
1
0
0
0
‚àí2
0
0
0
1
‚éû
‚é†
‚éõ
‚éù
1
2
0
0
1
0
0
0
1
‚éû
‚é†
‚éõ
‚éù
1
0
0
0
1
0
0
‚àí1
1
‚éû
‚é†
‚éõ
‚éù
1
0
1
0
1
0
0
0
1
‚éû
‚é†

576
Answers to Selected Exercises
20. (a)
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
1
3
0
0
0
‚àí2
1
0
0
0
1
0
0
0
0
0
‚àí2
0
0
0
0
1
0
0
0
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
SECTION 3.3
1. (a) F
(b) F
(c) T
(d) F
(e) F
(f) F
(g) T
(h) F
2. (a)
‚àí3
1
	
(c)
‚éß
‚é®
‚é©
‚éõ
‚éù
‚àí1
1
1
‚éû
‚é†
‚é´
‚é¨
‚é≠
(e)
‚éß
‚é™
‚é™
‚é®
‚é™
‚é™
‚é©
‚éõ
‚éú
‚éú
‚éù
‚àí2
1
0
0
‚éû
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éù
3
0
1
0
‚éû
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éù
‚àí1
0
0
1
‚éû
‚éü
‚éü
‚é†
‚é´
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é≠
(g)
‚éß
‚é™
‚é™
‚é®
‚é™
‚é™
‚é©
‚éõ
‚éú
‚éú
‚éù
‚àí3
1
1
0
‚éû
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éù
1
‚àí1
0
1
‚éû
‚éü
‚éü
‚é†
‚é´
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é≠
3. (a)
5
0
	
+ t
‚àí3
1
	
: t ‚ààR

(c)
‚éß
‚é®
‚é©
‚éõ
‚éù
2
1
1
‚éû
‚é†+ t
‚éõ
‚éù
‚àí1
1
1
‚éû
‚é†: t ‚ààR
‚é´
‚é¨
‚é≠
(e)
‚éß
‚é™
‚é™
‚é®
‚é™
‚é™
‚é©
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
‚éû
‚éü
‚éü
‚é†+ r
‚éõ
‚éú
‚éú
‚éù
‚àí2
1
0
0
‚éû
‚éü
‚éü
‚é†+ s
‚éõ
‚éú
‚éú
‚éù
3
0
1
0
‚éû
‚éü
‚éü
‚é†+ t
‚éõ
‚éú
‚éú
‚éù
‚àí1
0
0
1
‚éû
‚éü
‚éü
‚é†: r, s, t ‚ààR
‚é´
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é≠
(g)
‚éß
‚é™
‚é™
‚é®
‚é™
‚é™
‚é©
‚éõ
‚éú
‚éú
‚éù
0
0
0
1
‚éû
‚éü
‚éü
‚é†+ r
‚éõ
‚éú
‚éú
‚éù
‚àí3
1
1
0
‚éû
‚éü
‚éü
‚é†+ s
‚éõ
‚éú
‚éú
‚éù
1
‚àí1
0
1
‚éû
‚éü
‚éü
‚é†: r, s, ‚ààR
‚é´
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é≠
4. (b) (1) A‚àí1 =
‚éõ
‚éú
‚éù
1
3
0
1
3
1
9
1
3
‚àí2
9
‚àí4
9
2
3
‚àí1
9
‚éû
‚éü
‚é†
(2)
‚éõ
‚éù
x1
x2
x3
‚éû
‚é†=
‚éõ
‚éù
3
0
‚àí2
‚éû
‚é†
6. T‚àí1{(1, 11)} =
‚éß
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é©
‚éõ
‚éú
‚éú
‚éú
‚éù
11
2
‚àí9
2
0
‚éû
‚éü
‚éü
‚éü
‚é†+ t
‚éõ
‚éù
1
‚àí1
2
‚éû
‚é†: t ‚ààR
‚é´
‚é™
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é™
‚é≠
7. The systems in parts (b), (c), and (d) have solutions.
11. The farmer, tailor, and carpenter must have incomes in the proportions 4 : 3 : 4.
13. There must be 7.8 units of the Ô¨Årst commodity and 9.5 units of the second.
SECTION 3.4
1. (a) F
(b) T
(c) T
(d) T
(e) F
(f) T
(g) T

Answers to Selected Exercises
577
2. (a)
‚éõ
‚éù
4
‚àí3
‚àí1
‚éû
‚é†
(c)
‚éõ
‚éú
‚éú
‚éù
2
3
‚àí2
‚àí1
‚éû
‚éü
‚éü
‚é†
(e)
‚éß
‚é™
‚é™
‚é®
‚é™
‚é™
‚é©
‚éõ
‚éú
‚éú
‚éù
4
0
1
0
‚éû
‚éü
‚éü
‚é†+ r
‚éõ
‚éú
‚éú
‚éù
4
1
0
0
‚éû
‚éü
‚éü
‚é†+ s
‚éõ
‚éú
‚éú
‚éù
1
0
2
1
‚éû
‚éü
‚éü
‚é†: r, s ‚ààR
‚é´
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é≠
(g)
‚éß
‚é™
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é™
‚é©
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
‚àí23
0
7
9
0
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
+ r
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
1
1
0
0
0
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
+ s
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
‚àí23
0
6
9
1
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
: r, s ‚ààR
‚é´
‚é™
‚é™
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é™
‚é™
‚é≠
(i)
‚éß
‚é™
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é™
‚é©
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
2
0
0
‚àí1
0
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
+ r
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
0
2
1
0
0
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
+ s
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
1
‚àí4
0
‚àí2
1
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
: r, s ‚ààR
‚é´
‚é™
‚é™
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é™
‚é™
‚é≠
4. (a)
‚éß
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é©
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
4
3
1
3
0
0
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
+ t
‚éõ
‚éú
‚éú
‚éù
1
‚àí1
1
2
‚éû
‚éü
‚éü
‚é†: t ‚ààR
‚é´
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é≠
(c) There are no solutions.
5.
‚éõ
‚éù
1
0
2
1
4
‚àí1
‚àí1
3
‚àí2
‚àí7
3
1
1
0
‚àí9
‚éû
‚é†
7. {u1, u2, u5}
11. (b) {(1, 2, 1, 0, 0), (2, 1, 0, 0, 0), (1, 0, 0, 1, 0), (‚àí2, 0, 0, 0, 1)}
13. (b) {(1, 0, 1, 1, 1, 0), (0, 2, 1, 1, 0, 0), (1, 1, 1, 0, 0, 0), (‚àí3, ‚àí2, 0, 0, 0, 1)}
CHAPTER 4
SECTION 4.1
1. (a) F
(b) T
(c) F
(d) F
(e) T
2. (a) 30
(c) ‚àí8
3. (a) ‚àí10 + 15i
(c) ‚àí24
4. (a) 19
(c) 14
SECTION 4.2
1. (a) F
(b) T
(c) T
(d) T
(e) F
(f) F
(g) F
(h) T
3. 42
5. ‚àí12
7. ‚àí12
9. 22
11. ‚àí3
13. ‚àí8
15. 0
17. ‚àí49
19. ‚àí28 ‚àíi
21. 95

578
Answers to Selected Exercises
SECTION 4.3
1. (a) F
(b) T
(c) F
(d) T
(e) F
(f) T
(g) F
(h) F
3. (4, ‚àí3, 0)
5. (‚àí20, ‚àí48, ‚àí8)
7. (0, ‚àí12, 16)
24. tn + an‚àí1tn‚àí1 + ¬∑ ¬∑ ¬∑ + a1t + a0
26. (a)

A22
‚àíA12
‚àíA21
A11
	
(c)
‚éõ
‚éù
10
0
0
0
‚àí20
0
0
0
‚àí8
‚éû
‚é†
(e)
‚éõ
‚éù
‚àí3i
0
0
4
‚àí1 + i
0
10 + 16i
‚àí5 ‚àí3i
3 + 3i
‚éû
‚é†
(g)
‚éõ
‚éù
18
28
‚àí6
‚àí20
‚àí21
37
48
14
‚àí16
‚éû
‚é†
SECTION 4.4
1. (a) T
(b) T
(c) T
(d) F
(e) F
(f) T
(g) T
(h) F
(i) T
(j) T
(k) T
2. (a) 22
(c) 2 ‚àí4i
3. (a) ‚àí12
(c) ‚àí12
(e) 22
(g) ‚àí3
4. (a) 0
(c) ‚àí49
(e) ‚àí28 ‚àíi
(g) 95
SECTION 4.5
1. (a) F
(b) T
(c) T
(d) F
(e) F
(f) T
3. No
5. Yes
7. Yes
9. No
CHAPTER 5
SECTION 5.1
1. (a) F
(b) T
(c) T
(d) F
(e) F
(f) F
(g) F
(h) T
(i) T
(j) F
(k) F
2. (a) [T]Œ≤ =

0
2
‚àí1
0
	
, no
(c) [T]Œ≤ =
‚éõ
‚éù
‚àí1
0
0
0
1
0
0
0
‚àí1
‚éû
‚é†, yes
(e) [T]Œ≤ =
‚éõ
‚éú
‚éú
‚éù
‚àí1
1
0
0
0
‚àí1
1
0
0
0
‚àí1
0
0
0
0
‚àí1
‚éû
‚éü
‚éü
‚é†, no
3. (a) The eigenvalues are 4 and ‚àí1, a basis of eigenvectors is
2
3
	
,

1
‚àí1
	
,
Q =
2
1
3
‚àí1
	
,
and
D =
4
0
0
‚àí1
	
.
(c) The eigenvalues are 1 and ‚àí1, a basis of eigenvectors is

1
1 ‚àíi
	
,

1
‚àí1 ‚àíi
	
, Q =

1
1
1 ‚àíi
‚àí1 ‚àíi
	
, and D =
1
0
0
‚àí1
	
.

Answers to Selected Exercises
579
4. (a) Œª = 3, 4
Œ≤ = {(3, 5), (1, 2)}
(b) Œª = ‚àí1, 1, 2
Œ≤ = {(1, 2, 0), (1, ‚àí1, ‚àí1), (2, 0, ‚àí1)}
(f) Œª = 1, 3
Œ≤ = {‚àí2 + x, ‚àí4 + x2, ‚àí8 + x3, x}
(h) Œª = ‚àí1, 1, 1, 1
Œ≤ =
‚àí1
0
0
1
	
,
0
1
0
0
	
,
1
0
0
1
	
,
0
0
1
0
	
(i) Œª = 1, 1, ‚àí1, ‚àí1
Œ≤ =
1
0
1
0
	
,
0
1
0
1
	
,
‚àí1
0
1
0
	
,
0
‚àí1
0
1
	
(j) Œª = ‚àí1, 1, 5
Œ≤ =

0
1
‚àí1
0
	
,
1
0
0
‚àí1
	
,
0
1
1
0
	
,
1
0
0
1
	
26. 4
SECTION 5.2
1. (a) F
(b) F
(c) F
(d) T
(e) T
(f) F
(g) T
(h) T
(i) F
2. (a) Not diagonalizable
(c) Q =
1
4
1
‚àí3
	
(e) Not diagonalizable
(g) Q =
‚éõ
‚éù
1
1
1
2
‚àí1
0
‚àí1
0
‚àí1
‚éû
‚é†
3. (a) Not diagonalizable
(c) Not diagonalizable
(d) Œ≤ = {x ‚àíx2, 1 ‚àíx ‚àíx2, x + x2}
(e) Œ≤ = {(1, 1), (1, ‚àí1)}
7. An = 1
3
5n + 2(‚àí1)n
2(5n) ‚àí2(‚àí1)n
5n ‚àí(‚àí1)n
2(5)n + (‚àí1)n
	
14. (b) x(t) = c1e3t
‚àí2
1
	
+ c2e‚àí2t

1
‚àí1
	
(c) x(t) = et
‚é°
‚é£c1
‚éõ
‚éù
1
0
0
‚éû
‚é†+ c2
‚éõ
‚éù
0
1
0
‚éû
‚é†
‚é§
‚é¶+ c3e2t
‚éõ
‚éù
1
1
1
‚éû
‚é†
SECTION 5.3
1. (a) T
(b) T
(c) F
(d) F
(e) T
(f) T
(g) T
(h) F
(i) F
(j) T
2. (a)
0
0
0
0
	
(c)
‚éõ
‚éú
‚éù
7
13
7
13
6
13
6
13
‚éû
‚éü
‚é†
(e) No limit exists.
(g)
‚éõ
‚éù
‚àí1
0
‚àí1
‚àí4
1
‚àí2
2
0
2
‚éû
‚é†
(i) No limit exists.
6. One month after arrival, 25% of the patients have recovered, 20% are ambu-
latory, 41% are bedridden, and 14% have died. Eventually 59
90 recover and 31
90
die.

580
Answers to Selected Exercises
7.
3
7.
8. Only the matrices in (a) and (b) are regular transition matrices.
9. (a)
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
1
3
1
3
1
3
1
3
1
3
1
3
1
3
1
3
1
3
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
(c) No limit exists.
(e)
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
0
0
0
1
2
1
0
1
2
0
1
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
(g)
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
0
0
0
0
0
0
0
0
1
2
1
2
1
0
1
2
1
2
0
1
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
10. (a)
‚éõ
‚éù
0.225
0.441
0.334
‚éû
‚é†after two stages and
‚éõ
‚éù
0.20
0.60
0.20
‚éû
‚é†eventually
(c)
‚éõ
‚éù
0.372
0.225
0.403
‚éû
‚é†after two stages and
‚éõ
‚éù
0.50
0.20
0.30
‚éû
‚é†eventually
(e)
‚éõ
‚éù
0.329
0.334
0.337
‚éû
‚é†after two stages and
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
1
3
1
3
1
3
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
eventually
12.
9
19 new,
6
19 once-used, and
4
19 twice-used
13. In 1995, 24% will own large cars, 34% will own intermediate-sized cars, and
42% will own small cars; the corresponding eventual percentages are 10%, 30%,
and 60%.
20. eO = I and eI = eI.
SECTION 5.4
1. (a) F
(b) T
(c) F
(d) F
(e) T
(f) T
(g) T
2. The subspaces in (a), (c), and (d) are T-invariant.
6. (a)
‚éß
‚é™
‚é™
‚é®
‚é™
‚é™
‚é©
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
‚éû
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éù
1
0
1
1
‚éû
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éù
1
‚àí1
2
2
‚éû
‚éü
‚éü
‚é†
‚é´
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é≠
(c)
0
1
1
0
	
9. (a) ‚àít(t2 ‚àí3t + 3)
(c) 1 ‚àít
10. (a) t(t ‚àí1)(t2 ‚àí3t + 3)
(c) (t ‚àí1)3(t + 1)
18. (c) A‚àí1 = 1
2
‚éõ
‚éù
2
‚àí2
‚àí4
0
1
3
0
0
‚àí2
‚éû
‚é†

Answers to Selected Exercises
581
31. (a) t2 ‚àí6t + 6
(c) ‚àí(t + 1)(t2 ‚àí6t + 6)
CHAPTER 6
SECTION 6.1
1. (a) T
(b) T
(c) F
(d) F
(e) F
(f) F
(g) F
(h) T
2. ‚ü®x, y‚ü©= 8 + 5i, ‚à•x‚à•=
‚àö
7, ‚à•y‚à•=
‚àö
14, and ‚à•x + y‚à•=
‚àö
37.
3. ‚ü®f, g‚ü©= 1, ‚à•f‚à•=
‚àö
3
3 ,
‚à•g‚à•=
"
e2 ‚àí1
2
,
and ‚à•f + g‚à•=
"
11 + 3e2
6
.
16. (b) No
SECTION 6.2
1. (a) F
(b) T
(c) T
(d) F
(e) T
(f) F
(g) T
2. For each part the orthonormal basis and the Fourier coeÔ¨Écients are given.
(b)
? ‚àö
3
3 (1, 1, 1),
‚àö
6
6 (‚àí2, 1, 1),
‚àö
2
2 (0, ‚àí1, 1)
@
;
2
‚àö
3
3 , ‚àí
‚àö
6
6 ,
‚àö
2
2 .
(c) {1, 2
‚àö
3(x ‚àí1
2), 6
‚àö
5(x2 ‚àíx + 1
6)};
3
2,
‚àö
3
6 , 0.
(e)
?
1
5(2, ‚àí1, ‚àí2, 4),
1
‚àö
30(‚àí4, 2, ‚àí3, 1),
1
‚àö
155(‚àí3, 4, 9, 7)
@
;
10, 3
‚àö
30,
‚àö
155
(g)
1
6

3
5
‚àí1
1
	
,
1
6
‚àö
2
‚àí4
4
6
‚àí2
	
,
1
9
‚àö
2
9
‚àí3
6
‚àí6
	
;
24, 6
‚àö
2, ‚àí9
‚àö
2
(i)
?:
2
œÄ sin t,
:
2
œÄ cos t,
:
œÄ
œÄ2‚àí8(1 ‚àí4
œÄ sin t),
:
12œÄ
œÄ4‚àí96(t + 4
œÄ cos t ‚àíœÄ
2 )
@
;
:
2
œÄ (2œÄ + 2), ‚àí4
:
2
œÄ ,
:
œÄ2‚àí8
œÄ
(1 + œÄ),
:
œÄ4‚àí96
3œÄ
(k)
?
1
‚àö
47(‚àí4, 3 ‚àí2i, i, 1 ‚àí4i),
1
‚àö
60(3 ‚àíi, ‚àí5i, ‚àí2 + 4i, 2 + i),
1
‚àö
1160(‚àí17 ‚àíi, ‚àí9 + 8i, ‚àí18 + 6i, ‚àí9 + 8i)
@
;
‚àö
47(‚àí1 ‚àíi),
‚àö
60(‚àí1 + 2i),
‚àö
1160(1 + i)
(m)

1
‚àö
18
‚àí1 + i
‚àíi
2 ‚àíi
1 + 3i
	
,
1
‚àö
246
 ‚àí4i
‚àí11 ‚àí9i
1 + 5i
1 ‚àíi
	
,
1
‚àö
39063
‚àí5 ‚àí118i
‚àí7 ‚àí26i
‚àí145i
‚àí58
	
;
‚àö
18(2 + i),
‚àö
246(‚àí1 ‚àíi), 0
4. S‚ä•= span({(i, ‚àí1
2(1 + i), 1)})
5. S‚ä•
0 is the plane through the origin that is perpendicular to x0; S‚ä•is the line
through the origin that is perpendicular to the plane containing x1 and x2.
19. (a) 1
17
 26
104
	
(b) 1
14
‚éõ
‚éù
29
17
40
‚éû
‚é†
20. (b)
1
‚àö
14

582
Answers to Selected Exercises
SECTION 6.3
1. (a) T
(b) F
(c) F
(d) T
(e) F
(f) T
(g) T
2. (a) y = (1, ‚àí2, 4)
(c) y = 210x2 ‚àí204x + 33
3. (a) T‚àó(x) = (11, ‚àí12)
(c) T‚àó(f(t)) = 12 + 6t
14. T‚àó(x) = ‚ü®x, z‚ü©y
20.
(a) The linear function is y = ‚àí2t + 5/2 with E = 1, and the quadratic
function is y = t2/3 ‚àí4t/3 + 2 with E = 0.
(b) The linear function is y = 1.25t + 0.55 with E = 0.3, and the quadratic
function is t2/56 + 15t/14 + 239/280 with E = 0.22857 (approximation).
21. The spring constant is approximately 2.1.
22.
(a) x = 2
7, y = 3
7, z = 1
7
(d) x =
7
12, y =
1
12, z = 1
4, w = ‚àí1
12
SECTION 6.4
1. (a) T
(b) F
(c) F
(d) T
(e) T
(f) T
(g) F
(h) T
2.
(a) T is self-adjoint. An orthonormal basis of eigenvectors is
 1
‚àö
5
(1, ‚àí2),
1
‚àö
5
(2, 1)

, with corresponding eigenvalues 6 and 1.
(c) T is normal, but not self-adjoint. An orthonormal basis of eigenvectors
is1
2(1 + i,
‚àö
2), 1
2(1 + i, ‚àí
‚àö
2)

with corresponding eigenvalues
2 + 1 + i
‚àö
2
and 2 ‚àí1 + i
‚àö
2
.
(e) T is self-adjoint. An orthonormal basis of eigenvectors is
 1
‚àö
2
0
1
1
0
	
, 1
‚àö
2
1
0
0
1
	
, 1
‚àö
2
0
‚àí1
1
0
	
, 1
‚àö
2
‚àí1
0
0
1
	
with corresponding eigenvalues 1, 1, ‚àí1, ‚àí1.
SECTION 6.5
1. (a) T
(b) F
(c) F
(d) T
(e) F
(f) T
(g) F
(h) F
(i) F
2. (a) P =
1
‚àö
2
1
1
1
‚àí1
	
and
D =
3
0
0
‚àí1
	
(d) P =
‚éõ
‚éú
‚éú
‚éú
‚éù
1
‚àö
2
1
‚àö
6
1
‚àö
3
‚àí1
‚àö
2
1
‚àö
6
1
‚àö
3
0
‚àí2
‚àö
6
1
‚àö
3
‚éû
‚éü
‚éü
‚éü
‚é†
and
D =
‚éõ
‚éù
‚àí2
0
0
0
‚àí2
0
0
0
4
‚éû
‚é†
4. Tz is normal for all z ‚ààC, Tz is self-adjoint if and only if z ‚ààR, and Tz is
unitary if and only if |z| = 1.
5. Only the pair of matrices in (d) are unitarily equivalent.

Answers to Selected Exercises
583
25. 2(œà ‚àíœÜ)
26. (a) œà ‚àíœÜ
2
(b) œà + œÜ
2
27. (a) x =
1
‚àö
2
x‚Ä≤ + 1
‚àö
2
y‚Ä≤
and
y =
1
‚àö
2
x‚Ä≤ ‚àí1
‚àö
2
y‚Ä≤
The new quadratic form is 3(x‚Ä≤)2 ‚àí(y‚Ä≤)2.
(c) x =
3
‚àö
13
x‚Ä≤ +
2
‚àö
13
y‚Ä≤
and
y = ‚àí2
‚àö
13
x‚Ä≤ +
2
‚àö
13
y‚Ä≤
The new quadratic form is 5(x‚Ä≤)2 ‚àí8(y‚Ä≤)2.
29. (c) P =
‚éõ
‚éú
‚éú
‚éú
‚éù
1
‚àö
2
1
‚àö
3
‚àí6
‚àö
6
1
‚àö
2
‚àí1
‚àö
3
‚àö
6
6
0
1
‚àö
3
‚àö
6
3
‚éû
‚éü
‚éü
‚éü
‚é†
and
R =
‚éõ
‚éú
‚éú
‚éù
‚àö
2
‚àö
2
2
‚àö
2
0
‚àö
3
‚àö
3
3
0
0
‚àö
6
3
‚éû
‚éü
‚éü
‚é†
(e) x1 = 3, x2 = ‚àí5, x3 = 4
SECTION 6.6
1. (a) F
(b) T
(c) T
(d) F
(e) F
2. For W = span({(1, 2)}), [T]Œ≤ =

1
5
2
5
2
5
4
5

.
3. (2) (a) T1(a, b) = 1
2(a + b, a + b) and T2(a, b) = 1
2(a ‚àíb, ‚àía + b)
(d) T1(a, b, c) = 1
3(2a ‚àíb ‚àíc, ‚àía + 2b ‚àíc, ‚àía ‚àíb + 2c) and
T2(a, b, c) = 1
3(a + b + c, a + b + c, a + b + c)
SECTION 6.7
1. (a) F
(b) F
(c) T
(d) T
(e) F
(f) F
(g) T
2. (a) v1 =
1
0
	
, v2 =
0
1
	
,
u1 =
1
‚àö
3
‚éõ
‚éù
1
1
1
‚éû
‚é†, u2 =
1
‚àö
2
‚éõ
‚éù
0
1
‚àí1
‚éû
‚é†, u3 =
1
‚àö
6
‚éõ
‚éù
2
‚àí1
‚àí1
‚éû
‚é†
œÉ1 =
‚àö
3, œÉ2 =
‚àö
2
(c) v1 =
1
‚àöœÄ sin x, v2 =
1
‚àöœÄ cos x, v3 =
1
‚àö
2œÄ
u1 = cos x + 2 sin x
‚àö
5œÄ
, u2 = 2 cos x ‚àísin x
‚àö
5œÄ
, u3 =
1
‚àö
2œÄ
,
œÉ1 =
‚àö
5, œÉ2 =
‚àö
5, œÉ3 = 2
3. (a)
‚éõ
‚éú
‚éú
‚éù
1
‚àö
3
1
‚àö
2
1
‚àö
6
1
‚àö
3
‚àí1
‚àö
2
1
‚àö
6
‚àí1
‚àö
3
0
2
‚àö
6
‚éû
‚éü
‚éü
‚é†
‚éõ
‚éù
‚àö
6
0
0
0
0
0
‚éû
‚é†
 1
‚àö
2
1
‚àö
2
1
‚àö
2
‚àí1
‚àö
2
‚àó

584
Answers to Selected Exercises
(c)
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
2
‚àö
10
0
1
‚àö
2
1
‚àö
10
1
‚àö
10
‚àí1
‚àö
2
0
‚àí
2
‚àö
10
1
‚àö
10
1
‚àö
2
0
‚àí
2
‚àö
10
2
‚àö
10
0
‚àí1
‚àö
2
1
‚àö
10
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
‚éõ
‚éú
‚éú
‚éù
‚àö
5
0
0
1
0
0
0
0
‚éû
‚éü
‚éü
‚é†
 1
‚àö
2
1
‚àö
2
1
‚àö
2
‚àí1
‚àö
2
‚àó
(e)
 1+i
2
1+i
2
1‚àíi
2
‚àí1+i
2
 ‚àö
6
0
0
0
	 
2
‚àö
6
1‚àíi
‚àö
6
1+i
‚àö
6
‚àí2
‚àö
6
‚àó
4. (a) WP =
 1
‚àö
2
1
‚àö
2
1
‚àö
2
‚àí1
‚àö
2
  ‚àö
8+
‚àö
2
2
‚àí
‚àö
8+
‚àö
2
2
‚àí
‚àö
8+
‚àö
2
2
‚àö
8+
‚àö
2
2

5. (a) T ‚Ä†(x, y, z) =
x + y + z
3
, y ‚àíz
2

(c) T ‚Ä†(a + b sin x + c cos x) = T ‚àí1(a + b sin x + c cos x) =
a
2 + (2b + c) sin x + (‚àíb + 2c) cos x
5
6. (a) 1
6
1
1
‚àí1
1
1
‚àí1
	
(c) 1
5
1
‚àí2
3
1
1
3
‚àí2
1
	
(e) 1
6
1 ‚àíi
1 + i
1
i
	
7. (a) Z1 = N(T)‚ä•= R2
and
Z2 = R(T) = span{(1, 1, 1), (0, 1, ‚àí1)}
(c) Z1 = N(T)‚ä•= V
and
Z2 = R(T) = V
8. (a) No solution
1
2
1
1
	
SECTION 6.8
1. (a) F
(b) F
(c) T
(d) F
(e) T
(f) F
(g) F
(h) F
(i) T
(j) F
4. (a) Yes
(b) No
(c) No
(d) Yes
(e) Yes
(f) No
5. (a)
‚éõ
‚éù
0
2
‚àí2
2
0
‚àí2
1
1
0
‚éû
‚é†
(b)
‚éõ
‚éú
‚éú
‚éù
1
0
0
1
0
0
0
0
0
0
0
0
1
0
0
1
‚éû
‚éü
‚éü
‚é†
(c)
‚éõ
‚éú
‚éú
‚éù
0
0
0
0
‚àí1
0
‚àí4
0
0
0
0
0
‚àí2
0
‚àí8
0
‚éû
‚éü
‚éü
‚é†
17. (a) and (b)
‚éß
‚é®
‚é©
‚éõ
‚éù
2
‚àö
5
‚àí1
‚àö
5
‚éû
‚é†,
‚éõ
‚éù
1
‚àö
5
2
‚àö
5
‚éû
‚é†
‚é´
‚é¨
‚é≠
(c)
‚éß
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é©
‚éõ
‚éú
‚éú
‚éú
‚éù
1
‚àö
2
0
1
‚àö
2
‚éû
‚éü
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éù
0
1
0
‚éû
‚éü
‚éü
‚é†,
‚éõ
‚éú
‚éú
‚éú
‚éù
1
‚àö
2
0
‚àí1
‚àö
2
‚éû
‚éü
‚éü
‚éü
‚é†
‚é´
‚é™
‚é™
‚é™
‚é¨
‚é™
‚é™
‚é™
‚é≠
18. Same as Exercise 17(c)
22. (a) Q =
1
‚àí3
0
1
	
and
D =
1
0
0
‚àí7
	
(b) Q =

1
‚àí1
2
1
1
2

and
D =

2
0
0
‚àí1
2

(c) Q =
‚éõ
‚éù
0
0
1
0
1
‚àí0.25
1
0
2
‚éû
‚é†
and
D =
‚éõ
‚éù
‚àí1
0
0
0
4
0
0
0
6.75
‚éû
‚é†

Answers to Selected Exercises
585
SECTION 6.9
7. (Bv)‚àí1 =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
1
‚àö
1 ‚àív2
0
0
v
‚àö
1 ‚àív2
0
1
0
0
0
0
1
0
v
‚àö
1 ‚àív2
0
0
1
‚àö
1 ‚àív2
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
SECTION 6.10
1. (a) F
(b) T
(c) T
(d) F
(e) F
2. (a)
‚àö
18
(c) approximately 2.34
4. (a) ‚à•A‚à•‚âà84.74, ‚à•A‚àí1‚à•‚âà17.01, and cond(A) ‚âà1441
(b) ‚à•Àúx ‚àíA‚àí1b‚à•‚â§‚à•A‚àí1‚à•¬∑ ‚à•AÀúx ‚àíb‚à•‚âà0.17 and
‚à•Àúx ‚àíA‚àí1b‚à•
‚à•A‚àí1b‚à•
‚â§cond(A)‚à•b ‚àíAÀúx‚à•
‚à•b‚à•
‚âà14.41
‚à•b‚à•
5. 0.001 ‚â§‚à•x ‚àíÀúx‚à•
‚à•x‚à•
‚â§10
6. R
‚éõ
‚éù
1
‚àí2
3
‚éû
‚é†= 9
7, ‚à•B‚à•= 2, and cond(B) = 2.
SECTION 6.11
1. (a) F
(b) T
(c) T
(d) F
(e) T
(f) F
(g) F
(h) F
(i) T
(j) F
3. (b)

t
‚àö
3
1
	
: t ‚ààR

4. (b)

t
1
0
	
: t ‚ààR

if œÜ = 0 and

t
cos œÜ + 1
sin œÜ
	
: t ‚ààR

if œÜ Ã∏= 0
7.
(c) There are six possibilities:
(1) Any line through the origin if œÜ = œà = 0
(2)
‚éß
‚é®
‚é©t
‚éõ
‚éù
0
0
1
‚éû
‚é†: t ‚ààR
‚é´
‚é¨
‚é≠
if œÜ = 0 and œà = œÄ
(3)
‚éß
‚é®
‚é©t
‚éõ
‚éù
cos œà + 1
‚àísin œà
0
‚éû
‚é†: t ‚ààR
‚é´
‚é¨
‚é≠
if œÜ = œÄ and œà Ã∏= œÄ
(4)
‚éß
‚é®
‚é©t
‚éõ
‚éù
0
cos œÜ ‚àí1
sin œÜ
‚éû
‚é†: t ‚ààR
‚é´
‚é¨
‚é≠
if œà = œÄ and œÜ Ã∏= œÄ
(5)
‚éß
‚é®
‚é©t
‚éõ
‚éù
0
1
0
‚éû
‚é†: t ‚ààR
‚é´
‚é¨
‚é≠
if œÜ = œà = œÄ

586
Answers to Selected Exercises
(6)
‚éß
‚é®
‚é©t
‚éõ
‚éù
sin œÜ(cos œà + 1)
‚àísin œÜ sin œà
sin œà(cos œÜ + 1)
‚éû
‚é†: t ‚ààR
‚é´
‚é¨
‚é≠
otherwise
CHAPTER 7
SECTION 7.1
1. (a) T
(b) F
(c) F
(d) T
(e) F
(f) F
(g) T
(h) T
2. (a) For Œª = 2,
‚àí1
‚àí1
	
,
1
0
	
J =
2
1
0
2
	
(c) For Œª = ‚àí1,
‚éß
‚é®
‚é©
‚éõ
‚éù
1
3
0
‚éû
‚é†
‚é´
‚é¨
‚é≠
For Œª = 2,
‚éß
‚é®
‚é©
‚éõ
‚éù
1
1
1
‚éû
‚é†,
‚éõ
‚éù
1
2
0
‚éû
‚é†
‚é´
‚é¨
‚é≠
J =
‚éõ
‚éù
‚àí1
0
0
0
2
1
0
0
2
‚éû
‚é†
3. (a) For Œª = 2, {2, ‚àí2x, x2}
J =
‚éõ
‚éù
2
1
0
0
2
1
0
0
2
‚éû
‚é†
(c) For Œª = 1,
1
0
0
0
	
,
0
0
1
0
	
,
0
1
0
0
	
,
0
0
0
1
	
J =
‚éõ
‚éú
‚éú
‚éù
1
1
0
0
0
1
0
0
0
0
1
1
0
0
0
1
‚éû
‚éü
‚éü
‚é†
SECTION 7.2
1. (a) T
(b) T
(c) F
(d) T
(e) T
(f) F
(g) F
(h) T
2. J =
‚éõ
‚éù
A1
O
O
O
A2
O
O
O
A3
‚éû
‚é†
where A1 =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
2
1
0
0
0
0
0
2
1
0
0
0
0
0
2
0
0
0
0
0
0
2
1
0
0
0
0
0
2
0
0
0
0
0
0
2
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
,
A2 =
‚éõ
‚éú
‚éú
‚éù
4
1
0
0
0
4
1
0
0
0
4
0
0
0
0
4
‚éû
‚éü
‚éü
‚é†
and
A3 =
‚àí3
0
0
‚àí3
	
3. (a) ‚àí(t ‚àí2)5(t ‚àí3)2
(b)
Œª1 = 2
Œª2 = 3
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢
(c) Œª2 = 3
(d) p1 = 3 and p2 = 1
(e) (i) rank(U1) = 3 and rank(U2) = 0
(ii) rank(U2
1) = 1 and rank(U2
2) = 0
(iii) nullity(U1) = 2 and nullity(U2) = 2
(iv) nullity(U2
1) = 4 and nullity(U2
2) = 2

Answers to Selected Exercises
587
4. (a) J =
‚éõ
‚éù
1
0
0
0
2
1
0
0
2
‚éû
‚é†
and
Q =
‚éõ
‚éù
1
1
1
2
1
2
1
‚àí1
0
‚éû
‚é†
(d) J =
‚éõ
‚éú
‚éú
‚éù
0
1
0
0
0
0
0
0
0
0
2
0
0
0
0
2
‚éû
‚éü
‚éü
‚é†
and
Q =
‚éõ
‚éú
‚éú
‚éù
1
0
1
‚àí1
1
‚àí1
0
1
1
‚àí2
0
1
1
0
1
0
‚éû
‚éü
‚éü
‚é†
5. (a) J =
‚éõ
‚éú
‚éú
‚éù
1
1
0
0
0
1
1
0
0
0
1
0
0
0
0
2
‚éû
‚éü
‚éü
‚é†
and
Œ≤ = {2et, 2tet, t2et, e2t}
(c) J =
‚éõ
‚éú
‚éú
‚éù
2
1
0
0
0
2
0
0
0
0
2
1
0
0
0
2
‚éû
‚éü
‚éü
‚é†
and
Œ≤ = {6x, x3, 2, x2}
(d) J =
‚éõ
‚éú
‚éú
‚éù
2
1
0
0
0
2
1
0
0
0
2
0
0
0
0
4
‚éû
‚éü
‚éü
‚é†
and
Œ≤ =
1
0
0
0
	
,
0
1
1
0
	
,
0
‚àí1
0
2
	
,
1
‚àí2
2
0
	
24. (a)
‚éõ
‚éù
x
y
z
‚éû
‚é†= e2t
‚é°
‚é£(c1 + c2t)
‚éõ
‚éù
1
0
0
‚éû
‚é†+ c2
‚éõ
‚éù
0
1
0
‚éû
‚é†
‚é§
‚é¶+ c3e3t
‚éõ
‚éù
1
1
‚àí1
‚éû
‚é†
(b)
‚éõ
‚éù
x
y
z
‚éû
‚é†= e2t
‚é°
‚é£(c1 + c2t + c3t2)
‚éõ
‚éù
1
0
0
‚éû
‚é†+ (c2 + 2c3t)
‚éõ
‚éù
0
1
0
‚éû
‚é†+ 2c3
‚éõ
‚éù
0
0
1
‚éû
‚é†
‚é§
‚é¶
SECTION 7.3
1. (a) F
(b) T
(c) F
(d) F
(e) T
(f) F
(g) F
(h) T
(i) T
2. (a) (t ‚àí1)(t ‚àí3)
(c) (t ‚àí1)2(t ‚àí2)
(d) (t ‚àí2)2
3. (a) t2 ‚àí2
(c) (t ‚àí2)2
(d) (t ‚àí1)(t + 1)
4. For (2), (a);
for (3), (a) and (d)
5. The operators are T0, I, and all operators having both 0 and 1 as eigenvalues.
SECTION 7.4
1. (a) T
(b) F
(c) F
(d) T
(e) T
(f) F
(g) T
2. (a)
‚éõ
‚éù
0
0
27
1
0
‚àí27
0
1
9
‚éû
‚é†
(b)
0
‚àí1
1
‚àí1
	
(c)
 1
2(‚àí1 + i
‚àö
3)
0
0
1
2(‚àí1 ‚àíi
‚àö
3)
	
(e)
‚éõ
‚éú
‚éú
‚éù
0
‚àí2
0
0
1
0
0
0
0
0
0
‚àí3
0
0
1
0
‚éû
‚éü
‚éü
‚é†

3. (a) t2 + 1 and t2
C =
‚éõ
‚éú
‚éú
‚éù
0
‚àí1
0
0
1
0
0
0
0
0
0
0
0
0
0
0
‚éû
‚éü
‚éü
‚é†;
Œ≤ = {1, x, ‚àí2x + x2, ‚àí3x + x3}
(c) t2 ‚àít + 1
C =
‚éõ
‚éú
‚éú
‚éù
0
‚àí1
0
0
1
1
0
0
0
0
0
‚àí1
0
0
1
1
‚éû
‚éü
‚éü
‚é†
Œ≤ =
1
0
0
0
	
,

0
0
‚àí1
0
	
,
0
1
0
0
	
,
0
0
0
‚àí1
	
588

Index
589
Index
Absolute value of a complex num-
ber, 558
Absorbing Markov chain, 304
Absorbing state, 304
Addition
of matrices, 9
Addition of vectors, 6
Additive function, 78
Additive inverse
of an element of a Ô¨Åeld, 553
of a vector, 12
Adjoint
of a linear operator, 358‚Äì360
of a linear transformation, 367
of a matrix, 331, 359‚Äì360
uniqueness, 358
Algebraic multiplicity of an eigen-
value, see Multiplicity of an
eigenvalue
Algebraically closed Ô¨Åeld, 482, 561
Alternating n-linear function, 239
Angle between two vectors, 202,
335
Annihilator
of a subset, 126
of a vector, 524, 528
Approximation property of an or-
thogonal projection, 399
Area of a parallelogram, 204
Associated quadratic form, 389
Augmented matrix, 161, 174
Auxiliary polynomial, 131, 134, 137‚Äì
140
Axioms of the special theory of
relativity, 453
Axis of rotation, 473
Back substitution, 186
Backward pass, 186
Basis, 43‚Äì49, 60‚Äì61, 192‚Äì194
cyclic, 526
dual, 120
Jordan canonical, 483
ordered, 79
orthonormal, 341, 346‚Äì347, 372
rational canonical, 526
standard basis for Fn, 43
standard basis for Pn(F), 43
standard ordered basis for Fn,
79
standard ordered basis for Pn(F),
79
uniqueness of size, 46
Bessel‚Äôs inequality, 355
Bilinear form, 422‚Äì433
diagonalizable, 428
diagonalization, 428‚Äì435
index, 444
invariants, 444
matrix representation, 424‚Äì428
product with a scalar, 423
rank, 443
signature, 444
sum, 423
symmetric, 428‚Äì430, 433‚Äì435
vector space, 424
Cancellation law for vector addi-
tion, 11
Cancellation laws for a Ô¨Åeld, 554
Canonical form
Jordan, 483‚Äì516
rational, 526‚Äì548
for a symmetric matrix, 446
Cauchy‚ÄìSchwarz inequality, 333
Cayley‚ÄìHamilton theorem
for a linear operator, 317
for a matrix, 318, 377
Chain of sets, 59
Change of coordinate matrix, 112‚Äì
115
Characteristic of a Ô¨Åeld, 23, 41,
42, 430, 449, 555
Characteristic polynomial, 373

590
Index
of a linear operator, 249
of a matrix, 248
Characteristic value, see Eigenvalue
Characteristic vector, see Eigen-
vector
Classical adjoint
of an n √ó n matrix, 231
of a 2 √ó 2 matrix, 208
Clique, 94, 98
Closed model of a simple econ-
omy, 176‚Äì178
Closure
under addition, 17
under scalar multiplication, 17
Codomain, 551
CoeÔ¨Écient matrix of a system of
linear equations, 169
CoeÔ¨Écients
Fourier, 119
of a diÔ¨Äerential equation, 128
of a linear combination, 24, 43
of a polynomial, 9
Cofactor, 210, 232
Cofactor expansion, 210, 215, 232
Column of a matrix, 8
Column operation, 148
Column sum of matrices, 295
Column vector, 8
Companion matrix, 526
Complex number, 556‚Äì561
absolute value, 558
conjugate, 557
fundamental theorem of alge-
bra, 482, 560
imaginary part, 556
real part, 556
Composition
of functions, 552
of linear transformations, 86‚Äì
89
Condition number, 469
Conditioning of a system of linear
equations, 464
Congruent matrices, 426, 445, 451
Conic sections, 388‚Äì392
Conjugate linear property, 333
Conjugate of a complex number,
557
Conjugate transpose of a matrix,
331, 359‚Äì360
Consistent system of linear equa-
tions, 169
Consumption matrix, 177
Convergence of matrices, 284‚Äì288
Coordinate function, 119‚Äì120
Coordinate system
left-handed, 203
right-handed, 202
Coordinate vector, 80, 91, 110‚Äì
111
Corresponding homogeneous sys-
tem of linear equations, 172
Coset, 23, 109
Cramer‚Äôs rule, 224
Critical point, 439
Cullen, Charles G., 470
Cycle of generalized eigenvectors,
488‚Äì491
end vector, 488
initial vector, 488
length, 488
Cyclic basis, 526
Cyclic subspace, 313‚Äì317
Degree of a polynomial, 10
Determinant, 199‚Äì243
area of a parallelogram, 204
characterization of, 242
cofactor expansion, 210, 215, 232
Cramer‚Äôs rule, 224
of an identity matrix, 212
of an invertible matrix, 223
of a linear operator, 258, 474,
476‚Äì477
of a matrix transpose, 224
of an n √ó n matrix, 210, 232
n-dimensional volume, 226
properties of, 234‚Äì236
of a square matrix, 367, 394
of a 2 √ó 2 matrix, 200
uniqueness of, 242
of an upper triangular matrix,
218

Index
591
volume of a parallelepiped, 226
Wronskian, 232
Diagonal entries of a matrix, 8
Diagonal matrix, 18, 97
Diagonalizable bilinear form, 428
Diagonalizable linear operator, 245
Diagonalizable matrix, 246
Diagonalization
of a bilinear form, 428‚Äì435
problem, 245
simultaneous, 282, 325, 327, 376,
405
of a symmetric matrix, 431‚Äì433
test, 269, 496
Diagonalize, 247
DiÔ¨Äerentiable function, 129
DiÔ¨Äerential equation, 128
auxiliary polynomial, 131, 134,
137‚Äì140
coeÔ¨Écients, 128
homogeneous, 128, 137‚Äì140, 523
linear, 128
nonhomogeneous, 142
order, 129
solution, 129
solution space, 132, 137‚Äì140
system, 273, 516
DiÔ¨Äerential operator, 131
null space, 134‚Äì137
order, 131, 135
Dimension, 47‚Äì48, 50‚Äì51, 103, 119,
425
Dimension theorem, 70
Direct sum
of matrices, 320‚Äì321, 496, 545
of subspaces, 22, 58, 98, 275‚Äì
279, 318, 355, 366, 394, 398,
401, 475‚Äì478, 494, 545
Disjoint sets, 550
Distance, 340
Division algorithm for polynomi-
als, 562
Domain, 551
Dominance relation, 95‚Äì96, 99
Dot diagram
of a Jordan canonical form, 498‚Äì
500
of a rational canonical form, 535‚Äì
539
Double dual, 120, 123
Dual basis, 120
Dual space, 119‚Äì123
Economics, see Leontief, Wassily
Eigenspace
generalized, 485‚Äì491
of a linear operator or matrix,
264
Eigenvalue
of a generalized eigenvector, 484
of a linear operator or matrix,
246, 371‚Äì374, 467‚Äì470
multiplicity, 263
Eigenvector
generalized, 484‚Äì491
of a linear operator or matrix,
246, 371‚Äì374
Einstein, Albert, see Special the-
ory of relativity
Element, 549
Elementary column operation, 148,
153
Elementary divisor
of a linear operator, 539
of a matrix, 541
Elementary matrix, 149‚Äì150, 159
Elementary operation, 148
Elementary row operation, 148, 153,
217
Ellipse, see Conic sections
Empty set, 549
End vector of a cycle of general-
ized eigenvectors, 488
Entry of a matrix, 8
Equality
of functions, 9, 551
of matrices, 9
of n-tuples, 8
of polynomials, 10
of sets, 549
Equilibrium condition for a sim-
ple economy, 177
Equivalence relation, 107, 551
congruence, 449, 451

592
Index
unitary equivalence, 394, 472
Equivalent systems of linear equa-
tions, 182‚Äì183
Euclidean norm of a matrix, 467‚Äì
470
Euler‚Äôs formula, 132
Even function, 15, 21, 355
Exponential function, 133‚Äì140
Exponential of a matrix, 312, 515
Extremum, see Local extremum
Field, 553‚Äì555
algebraically closed, 482, 561
cancellation laws, 554
characteristic, 23, 41, 42, 430,
449, 555
of complex numbers, 556‚Äì561
product of elements, 553
of real numbers, 549
sum of elements, 553
Field of scalars, 6‚Äì7, 47
Finite-dimensional vector space, 46‚Äì
51
Fixed probability vector, 301
Forward pass, 186
Fourier, Jean Baptiste, 348
Fourier coeÔ¨Écients, 119, 348, 400
Frobenius inner product, 332
Function, 551‚Äì552
additive, 78
alternating n-linear, 239
codomain of, 551
composite, 552
coordinate function, 119‚Äì120
diÔ¨Äerentiable, 129
domain of, 551
equality of, 9, 551
even, 15, 21, 355
exponential, 133‚Äì140
image of, 551
imaginary part of, 129
inverse, 552
invertible, 552
linear, see Linear transforma-
tion
n-linear, 238‚Äì242
norm, 339
odd, 21, 355
one-to-one, 551
onto, 551
polynomial, 10, 51‚Äì53, 569
preimage of, 551
range of, 551
real part of, 129
restriction of, 552
sum of, 9
vector space, 9
Fundamental theorem of algebra,
482, 560
Gaussian elimination, 186‚Äì187
back substitution, 186
backward pass, 186
forward pass, 186
General solution of a system of
linear equations, 189
Generalized eigenspace, 485‚Äì491
Generalized eigenvector, 484‚Äì491
Generates, 30
Generator of a cyclic subspace, 313
Geometry, 385, 392, 436, 472‚Äì478
Gerschgorin‚Äôs disk theorem, 296
Gram‚ÄìSchmidt process, 344, 396
Gramian matrix, 376
Hardy‚ÄìWeinberg law, 307
Hermitian operator or matrix, see
Self-adjoint linear operator
or matrix
Hessian matrix, 440
Homogeneous linear diÔ¨Äerential equa-
tion, 128, 137‚Äì140, 523
Homogeneous polynomial of de-
gree two, 433
Homogeneous system of linear equa-
tions, 171
Hooke‚Äôs law, 128, 368
Householder operator, 397
Identity element
in C, 557
in a Ô¨Åeld, 553, 554
Identity matrix, 89, 93, 212
Identity transformation, 67
Ill-conditioned system, 464

Index
593
Image, see Range
Image of an element, 551
Imaginary number, 556
Imaginary part
of a complex number, 556
of a function, 129
Incidence matrix, 94‚Äì96, 98
Inconsistent system of linear equa-
tions, 169
Index
of a bilinear form, 444
of a matrix, 445
InÔ¨Ånite-dimensional vector space,
47
Initial probability vector, 292
Initial vector of a cycle of gener-
alized eigenvectors, 488
Inner product, 329‚Äì336
Frobenius, 332
on H, 335
standard, 330
Inner product space
complex, 332
H, 332, 343, 348‚Äì349, 380, 399
real, 332
Input‚Äìoutput matrix, 177
Intersection of sets, 550
Invariant subspace, 77‚Äì78, 313‚Äì
315
Invariants
of a bilinear form, 444
of a matrix, 445
Inverse
of a function, 552
of a linear transformation, 99‚Äì
102, 164‚Äì165
of a matrix, 100‚Äì102, 107, 161‚Äì
164
Invertible function, 552
Invertible linear transformation, 99‚Äì
102
Invertible matrix, 100‚Äì102, 111,
223, 469
Irreducible polynomial, 525, 567‚Äì
569
Isometry, 379
Isomorphic vector spaces, 102‚Äì105
Isomorphism, 102‚Äì105, 123, 425
Jordan block, 483
Jordan canonical basis, 483
Jordan canonical form
dot diagram, 498‚Äì500
of a linear operator, 483‚Äì516
of a matrix, 491
uniqueness, 500
Kernel, see Null space
Kronecker delta, 89, 335
Lagrange interpolation formula, 51‚Äì
53, 125, 402
Lagrange polynomials, 51, 109, 125
Least squares approximation, 360‚Äì
364
Least squares line, 361
Left shift operator, 76
Left-handed coordinate system, 203
Left-multiplication transformation,
92‚Äì94
Legendre polynomials, 346
Length of a cycle of generalized
eigenvectors, 488
Length of a vector, see Norm
Leontief
closed model, 176‚Äì178
open model, 178‚Äì179
Leontief, Wassily, 176
Light second, 452
Limit of a sequence of matrices,
284‚Äì288
Linear combination, 24‚Äì26, 28‚Äì30,
39
uniqueness of coeÔ¨Écients, 43
Linear dependence, 36‚Äì40
Linear diÔ¨Äerential equation, 128
Linear equations, see System of
linear equations
Linear functional, 119
Linear independence, 37‚Äì40, 59‚Äì
61, 342
Linear operator, (see also Linear
transformation), 112
adjoint, 358‚Äì360
characteristic polynomial, 249

594
Index
determinant, 258, 474, 476‚Äì477
diagonalizable, 245
diagonalize, 247
diÔ¨Äerential, 131
diÔ¨Äerentiation, 131, 134‚Äì137
eigenspace, 264, 401
eigenvalue, 246, 371‚Äì374
eigenvector, 246, 371‚Äì374
elementary divisor, 539
Householder operator, 397
invariant subspace, 77‚Äì78, 313‚Äì
315
isometry, 379
Jordan canonical form, 483‚Äì516
left shift, 76
Lorentz transformation, 454‚Äì461
minimal polynomial, 516‚Äì521
nilpotent, 512
normal, 370, 401‚Äì403
orthogonal, 379‚Äì385, 472‚Äì478
partial isometry, 394, 405
positive deÔ¨Ånite, 377‚Äì378
positive semideÔ¨Ånite, 377‚Äì378
projection, 398‚Äì403
projection on a subspace, 86,
117
projection on the x-axis, 66
quotient space, 325‚Äì326
rational canonical form, 526‚Äì548
reÔ¨Çection, 66, 113, 117, 387, 472‚Äì
478
right shift, 76
rotation, 66, 382, 387, 472‚Äì478
self-adjoint, 373, 401‚Äì403
simultaneous diagonalization, 282,
405
spectral decomposition, 402
spectrum, 402
unitary, 379‚Äì385, 403
Linear space, see Vector space
Linear transformation, (see also
Linear operator), 65
adjoint, 367
composition, 86‚Äì89
identity, 67
image, see Range
inverse, 99‚Äì102, 164‚Äì165
invertible, 99‚Äì102
isomorphism, 102‚Äì105, 123, 425
kernel, see Null space
left-multiplication, 92‚Äì94
linear functional, 119
matrix representation, 80, 88‚Äì
92, 347, 359
null space, 67‚Äì69, 134‚Äì137
nullity, 69‚Äì71
one-to-one, 71
onto, 71
product with a scalar, 82
pseudoinverse, 413
range, 67‚Äì69
rank, 69‚Äì71, 159
restriction, 77‚Äì78
singular value, 407
singular value theorem, 406
sum, 82
transpose, 121, 126, 127
vector space of, 82, 103
zero, 67
Local extremum, 439, 450
Local maximum, 439, 450
Local minimum, 439, 450
Lorentz transformation, 454‚Äì461
Lower triangular matrix, 229
Markov chain, 291, 304
Markov process, 291
Matrix, 8
addition, 9
adjoint, 331, 359‚Äì360
augmented, 161, 174
change of coordinate, 112‚Äì115
characteristic polynomial, 248
classical adjoint, 208, 231
coeÔ¨Écient, 169
cofactor, 210, 232
column of, 8
column sum, 295
companion, 526
condition number, 469
congruent, 426, 445, 451
conjugate transpose, 331, 359‚Äì
360
consumption, 177

Index
595
convergence, 284‚Äì288
determinant of, 200, 210, 232,
367, 394
diagonal, 18, 97
diagonal entries of, 8
diagonalizable, 246
diagonalize, 247
direct sum, 320‚Äì321, 496, 545
eigenspace, 264
eigenvalue, 246, 467‚Äì470
eigenvector, 246
elementary, 149‚Äì150, 159
elementary divisor, 541
elementary operations, 148
entry, 8
equality of, 9
Euclidean norm, 467‚Äì470
exponential of, 312, 515
Gramian, 376
Hessian, 440
identity, 89
incidence, 94‚Äì96, 98
index, 445
input‚Äìoutput, 177
invariants, 445
inverse, 100‚Äì102, 107, 161‚Äì164
invertible, 100‚Äì102, 111, 223,
469
Jordan block, 483
Jordan canonical form, 491
limit of, 284‚Äì288
lower triangular, 229
minimal polynomial, 517‚Äì521
multiplication with a scalar, 9
nilpotent, 229, 512
norm, 339, 467‚Äì470, 515
normal, 370
orthogonal, 229, 382‚Äì385
orthogonally equivalent, 384‚Äì385
permanent of a 2 √ó 2, 448
polar decomposition, 411‚Äì413
positive deÔ¨Ånite, 377
positive semideÔ¨Ånite, 377
product, 87‚Äì94
product with a scalar, 9
pseudoinverse, 414
rank, 152‚Äì159
rational canonical form, 541
reduced row echelon form, 185,
190‚Äì191
regular, 294
representation of a bilinear form,
424‚Äì428
representation of a linear trans-
formation, 80, 88‚Äì92, 347,
359
row of, 8
row sum, 295
scalar, 258
self-adjoint, 373, 467
signature, 445
similarity, 115, 118, 259, 508
simultaneous diagonalization, 282
singular value, 410
singular value decomposition, 410
skew-symmetric, 23, 229, 371
square, 9
stochastic, see Transition ma-
trix
submatrix, 230
sum, 9
symmetric, 17, 373, 384, 389,
446
trace, 18, 20, 97, 118, 259, 281,
331, 393
transition, 288‚Äì291, 515
transpose, 17, 20, 67, 88, 127,
224, 259
transpose of a matrix inverse,
107
transpose of a product, 88
unitary, 229, 382‚Äì385
unitary equivalence, 384‚Äì385, 394,
472
upper triangular, 21, 218, 258,
370, 385, 397
Vandermonde, 230
vector space, 9, 331, 425
zero, 8
Maximal element of a family of
sets, 58
Maximal linearly independent sub-
set, 59‚Äì61
Maximal principle, 59

596
Index
Member, see Element
Michelson‚ÄìMorley experiment, 451
Minimal polynomial
of a linear operator, 516‚Äì521
of a matrix, 517‚Äì521
uniqueness, 516
Minimal solution to a system of
linear equations, 364‚Äì365
Monic polynomial, 567‚Äì569
Multiplicative inverse of an ele-
ment of a Ô¨Åeld, 553
Multiplicity of an eigenvalue, 263
Multiplicity of an elementary di-
visor, 539, 541
n-dimensional volume, 226
n-linear function, 238‚Äì242
n-tuple, 7
equality, 8
scalar multiplication, 8
sum, 8
vector space, 8
Nilpotent linear operator, 512
Nilpotent matrix, 229, 512
Nonhomogeneous linear diÔ¨Äeren-
tial equation, 142
Nonhomogeneous system of linear
equations, 171
Nonnegative vector, 177
Norm
Euclidean, 467‚Äì470
of a function, 339
of a matrix, 339, 467‚Äì470, 515
of a vector, 333‚Äì336, 339
Normal equations, 368
Normal linear operator or matrix,
370, 401‚Äì403
Normalizing a vector, 335
Null space, 67‚Äì69, 134‚Äì137
Nullity, 69‚Äì71
Numerical methods
conditioning, 464
QR factorization, 396‚Äì397
Odd function, 21, 355
One-to-one function, 551
One-to-one linear transformation,
71
Onto function, 551
Onto linear transformation, 71
Open model of a simple economy,
178‚Äì179
Order
of a diÔ¨Äerential equation, 129
of a diÔ¨Äerential operator, 131,
135
Ordered basis, 79
Orientation of an ordered basis,
202
Orthogonal complement, 349, 352,
398‚Äì401
Orthogonal equivalence of matrices,
384‚Äì385
Orthogonal matrix, 229, 382‚Äì385
Orthogonal operator, 379‚Äì385, 472‚Äì
478
on R2, 387‚Äì388
Orthogonal projection, 398‚Äì403
Orthogonal projection of a vector,
351
Orthogonal subset, 335, 342
Orthogonal vectors, 335
Orthonormal basis, 341, 346‚Äì347,
372
Orthonormal subset, 335
Parallel vectors, 3
Parallelogram
area of, 204
law, 2, 337
Parseval‚Äôs identity, 355
Partial isometry, 394, 405
Pendular motion, 143
Penrose conditions, 421
Periodic motion of a spring, 127,
144
Permanent of a 2 √ó 2 matrix, 448
Perpendicular vectors, see Orthog-
onal vectors
Physics
Hooke‚Äôs law, 128, 368
pendular motion, 143
periodic motion of a spring, 144
special theory of relativity, 451‚Äì
461

Index
597
spring constant, 368
Polar decomposition of a matrix,
411‚Äì413
Polar identities, 338
Polynomial, 9
annihilator of a vector, 524, 528
auxiliary, 131, 134, 137‚Äì140
characteristic, 373
coeÔ¨Écients of, 9
degree of a, 10
division algorithm, 562
equality, 10
function, 10, 51‚Äì53, 569
fundamental theorem of alge-
bra, 482, 560
homogeneous of degree two, 433
irreducible, 525, 567‚Äì569
Lagrange, 51, 109, 125
Legendre, 346
minimal, 516‚Äì521
monic, 567‚Äì569
product with a scalar, 10
quotient, 563
relatively prime, 564
remainder, 563
splits, 262, 370, 373
sum, 10
trigonometric, 399
unique factorization theorem, 568
vector space, 10
zero, 9
zero of a, 62, 134, 560, 564
Positive deÔ¨Ånite matrix, 377
Positive deÔ¨Ånite operator, 377‚Äì378
Positive semideÔ¨Ånite matrix, 377
Positive semideÔ¨Ånite operator, 377‚Äì
378
Positive vector, 177
Power set, 59
Preimage of an element, 551
Primary decomposition theorem,
545
Principal axis theorem, 390
Probability, see Markov chain
Probability vector, 289
Ô¨Åxed, 301
initial, 292
Product
of a bilinear form and a scalar,
423
of complex numbers, 556
of elements of a Ô¨Åeld, 553
of a linear transformation and
scalar, 82
of matrices, 87‚Äì94
of a matrix and a scalar, 9
of a vector and a scalar, 7
Projection
on a subspace, 76, 86, 98, 117,
398‚Äì403
on the x-axis, 66
orthogonal, 398‚Äì403
Proper subset, 549
Proper value, see Eigenvalue
Proper vector, see Eigenvector
Pseudoinverse
of a linear transformation, 413
of a matrix, 414
Pythagorean theorem, 337
QR factorization, 396‚Äì397
Quadratic form, 389, 433‚Äì439
Quotient of polynomials, 563
Quotient space, 23, 58, 79, 109,
325‚Äì326
Range, 67‚Äì69, 551
Rank
of a bilinear form, 443
of a linear transformation, 69‚Äì
71, 159
of a matrix, 152‚Äì159
Rational canonical basis, 526
Rational canonical form
dot diagram, 535‚Äì539
elementary divisor, 539, 541
of a linear operator, 526‚Äì548
of a matrix, 541
uniqueness, 539
Rayleigh quotient, 467
Real part
of a complex number, 556
of a function, 129
Reduced row echelon form of a
matrix, 185, 190‚Äì191

598
Index
ReÔ¨Çection, 66, 117, 472‚Äì478
of R2, 113, 382‚Äì383, 387, 388
Regular transition matrix, 294
Relation on a set, 551
Relative change in a vector, 465
Relatively prime polynomials, 564
Remainder, 563
Replacement theorem, 45‚Äì46
Representation of a linear trans-
formation by a matrix, 80
Resolution of the identity opera-
tor, 402
Restriction
of a function, 552
of a linear operator on a sub-
space, 77‚Äì78
Right shift operator, 76
Right-handed coordinate system,
202
Rigid motion, 385‚Äì387
in the plane, 388
Rotation, 66, 382, 387, 472‚Äì478
Row of a matrix, 8
Row operation, 148
Row sum of matrices, 295
Row vector, 8
Rudin, Walter, 560
Saddle point, 440
Scalar, 7
Scalar matrix, 258
Scalar multiplication, 6
Schur‚Äôs theorem
for a linear operator, 370
for a matrix, 385
Second derivative test, 439‚Äì443,
450
Self-adjoint linear operator or ma-
trix, 373, 401‚Äì403, 467
Sequence, 11
Set, 549‚Äì551
chain, 59
disjoint, 550
element of a, 549
empty, 549
equality of, 549
equivalence relation, 107, 394,
449, 451
equivalence relation on a, 551
intersection, 550
linearly dependent, 36‚Äì40
linearly independent, 37‚Äì40
orthogonal, 335, 342
orthonormal, 335
power, 59
proper subset, 549
relation on a, 551
subset, 549
union, 549
Signature
of a bilinear form, 444
of a matrix, 445
Similar matrices, 115, 118, 259,
508
Simpson‚Äôs rule, 126
Simultaneous diagonalization, 282,
325, 327, 376, 405
Singular value
of a linear transformation, 407
of a matrix, 410
Singular value decomposition of a
matrix, 410
Singular value decomposition the-
orem for matrices, 410
Singular value theorem for linear
transformations, 406
Skew-symmetric matrix, 23, 229,
371
Solution
of a diÔ¨Äerential equation, 129
minimal, 364‚Äì365
to a system of linear equations,
169
Solution set of a system of linear
equations, 169, 182
Solution space of a homogeneous
diÔ¨Äerential equation, 132, 137‚Äì
140
Space‚Äìtime coordinates, 453
Span, 30, 34, 343
Special theory of relativity, 451‚Äì
461
axioms, 453
Lorentz transformation, 454‚Äì461
space‚Äìtime coordinates, 453

Index
599
time contraction, 459‚Äì461
Spectral decomposition, 402
Spectral theorem, 401
Spectrum, 402
Splits, 262, 370, 373
Spring, periodic motion of, 127,
144
Spring constant, 368
Square matrix, 9
Square root of a unitary operator,
393
Standard basis
for Fn, 43
for Pn(F), 43
Standard inner product on Fn, 330
Standard ordered basis
for Fn, 79
for Pn(F), 79
Standard representation of a vec-
tor space, 104‚Äì105
States
absorbing, 304
of a transition matrix, 288
Stationary vector, see Fixed prob-
ability vector
Statistics, see Least squares ap-
proximation
Stochastic matrix, see Transition
matrix
Stochastic process, 291
Submatrix, 230
Subset, 549
linearly dependent, 36‚Äì40
linearly independent, 59‚Äì61
maximal linearly independent,
59‚Äì61
orthogonal, 335, 342
orthogonal complement of a, 349,
352, 398‚Äì401
orthonormal, 335
span of a, 30, 34, 343
sum, 22
Subspace, 16‚Äì19, 50‚Äì51
cyclic, 313‚Äì317
dimension of a, 50‚Äì51
direct sum, 22, 58, 98, 275‚Äì279,
318, 355, 366, 394, 398, 401,
475‚Äì478, 494, 545
generated by a set, 30
invariant, 77‚Äì78
sum, 275
zero, 16
Sum
of bilinear forms, 423
of complex numbers, 556
of elements of a Ô¨Åeld, 553
of functions, 9
of linear transformations, 82
of matrices, 9
of n-tuples, 8
of polynomials, 10
of subsets, 22
of vectors, 7
Sum of subspaces, (see also Direct
sum, of subspaces), 275
Sylvester‚Äôs law of inertia
for a bilinear form, 443
for a matrix, 445
Symmetric bilinear form, 428‚Äì430,
433‚Äì435
Symmetric matrix, 17, 373, 384,
389, 446
System of diÔ¨Äerential equations,
273, 516
System of linear equations, 25‚Äì30,
169
augmented matrix, 174
coeÔ¨Écient matrix, 169
consistent, 169
corresponding homogeneous sys-
tem, 172
equivalent, 182‚Äì183
Gaussian elimination, 186‚Äì187
general solution, 189
homogeneous, 171
ill-conditioned, 464
inconsistent, 169
minimal solution, 364‚Äì365
nonhomogeneous, 171
solution to, 169
well-conditioned, 464
T-annihilator, 524, 528
T-cyclic basis, 526

600
Index
T-cyclic subspace, 313‚Äì317
T-invariant subspace, 77‚Äì78, 313‚Äì
315
Taylor‚Äôs theorem, 441
Test for diagonalizability, 496
Time contraction, 459‚Äì461
Trace of a matrix, 18, 20, 97, 118,
259, 281, 331, 393
Transition matrix, 288‚Äì291, 515
regular, 294
states, 288
Translation, 386
Transpose
of an invertible matrix, 107
of a linear transformation, 121,
126, 127
of a matrix, 17, 20, 67, 88, 127,
224, 259
Trapezoidal rule, 126
Triangle inequality, 333
Trigonometric polynomial, 399
Trivial representation of zero vec-
tor, 36‚Äì38
Union of sets, 549
Unique factorization theorem for
polynomials, 568
Uniqueness
of adjoint, 358
of coeÔ¨Écients of a linear com-
bination, 43
of Jordan canonical form, 500
of minimal polynomial, 516
of rational canonical form, 539
of size of a basis, 46
Unit vector, 335
Unitary equivalence of matrices,
384‚Äì385, 394, 472
Unitary matrix, 229, 382‚Äì385
Unitary operator, 379‚Äì385, 403
Upper triangular matrix, 21, 218,
258, 370, 385, 397
Vandermonde matrix, 230
Vector, 7
additive inverse of a, 12
annihilator of a, 524, 528
column, 8
coordinate, 80, 91, 110‚Äì111
Ô¨Åxed probability, 301
Fourier coeÔ¨Écients, 119, 348, 400
initial probability, 292
linear combination, 24
nonnegative, 177
norm, 333‚Äì336, 339
normalizing, 335
orthogonal, 335
orthogonal projection of a, 351
parallel, 3
perpendicular, see Orthogonal
vectors
positive, 177
probability, 289
product with a scalar, 8
Rayleigh quotient, 467
row, 8
sum, 7
unit, 335
zero, 12, 36‚Äì38
Vector space, 6
addition, 6
basis, 43‚Äì49, 192‚Äì194
of bilinear forms, 424
of continuous functions, 18, 67,
119, 331, 345, 356
of cosets, 23
dimension, 47‚Äì48, 103, 119, 425
dual, 119‚Äì123
Ô¨Ånite-dimensional, 46‚Äì51
of functions from a set into a
Ô¨Åeld, 9, 109, 127
inÔ¨Ånite-dimensional, 47
of inÔ¨Ånitely diÔ¨Äerentiable func-
tions, 130‚Äì137, 247, 523
isomorphism, 102‚Äì105, 123, 425
of linear transformations, 82, 103
of matrices, 9, 103, 331, 425
of n-tuples, 8
of polynomials, 10, 86, 109
quotient, 23, 58, 79, 109
scalar multiplication, 6
of sequences, 11, 109, 356, 369
subspace, 16‚Äì19, 50‚Äì51
zero, 15
zero vector of a, 12

Index
601
Volume of a parallelepiped, 226
Wade, William R., 439
Well-conditioned system, 464
Wilkinson, J. H., 397
Wronskian, 232
Z2, 16, 42, 429, 553
Zero matrix, 8
Zero of a polynomial, 62, 134, 560,
564
Zero polynomial, 9
Zero subspace, 16
Zero transformation, 67
Zero vector, 12, 36‚Äì38
trivial representation, 36‚Äì38
Zero vector space, 15

