Every Model Learned by Gradient Descent Is Approximately
a Kernel Machine
Pedro Domingos
pedrod@cs.washington.edu
Paul G. Allen School of Computer Science & Engineering
University of Washington
Seattle, WA 98195-2350, USA
Abstract
Deep learningâ€™s successes are often attributed to its ability to automatically discover new
representations of the data, rather than relying on handcrafted features like other learning
methods. We show, however, that deep networks learned by the standard gradient de-
scent algorithm are in fact mathematically approximately equivalent to kernel machines, a
learning method that simply memorizes the data and uses it directly for prediction via a
similarity function (the kernel). This greatly enhances the interpretability of deep network
weights, by elucidating that they are eï¬€ectively a superposition of the training examples.
The network architecture incorporates knowledge of the target function into the kernel.
This improved understanding should lead to better learning algorithms.
Keywords:
gradient descent, kernel machines, deep learning, representation learning,
neural tangent kernel
1. Introduction
Despite its many successes, deep learning remains poorly understood (Goodfellow et al.,
2016). In contrast, kernel machines are based on a well-developed mathematical theory,
but their empirical performance generally lags behind that of deep networks (SchÂ¨olkopf
and Smola, 2002). The standard algorithm for learning deep networks, and many other
models, is gradient descent (Rumelhart et al., 1986).
Here we show that every model
learned by this method, regardless of architecture, is approximately equivalent to a kernel
machine with a particular type of kernel. This kernel measures the similarity of the model
at two data points in the neighborhood of the path taken by the model parameters during
learning. Kernel machines store a subset of the training data points and match them to
the query using the kernel.
Deep network weights can thus be seen as a superposition
of the training data points in the kernelâ€™s feature space, enabling their eï¬ƒcient storage
and matching. This contrasts with the standard view of deep learning as a method for
discovering representations from data, with the attendant lack of interpretability (Bengio
et al., 2013). Our result also has signiï¬cant implications for boosting algorithms (Freund
and Schapire, 1997), probabilistic graphical models (Koller and Friedman, 2009), and convex
optimization (Boyd and Vandenberghe, 2004).
arXiv:2012.00152v1  [cs.LG]  30 Nov 2020

Domingos
2. Path Kernels
A kernel machine is a model of the form
y = g
 X
i
aiK(x, xi) + b
!
,
where x is the query data point, the sum is over training data points xi, g is an optional
nonlinearity, the aiâ€™s and b are learned parameters, and the kernel K measures the similarity
of its arguments (SchÂ¨olkopf and Smola, 2002). In supervised learning, ai is typically a linear
function of yâˆ—
i , the known output for xi. Kernels may be predeï¬ned or learned (Cortes
et al., 2009).
Kernel machines, also known as support vector machines, are one of the
most developed and widely used machine learning methods. In the last decade, however,
they have been eclipsed by deep networks, also known as neural networks and multilayer
perceptrons, which are composed of multiple layers of nonlinear functions. Kernel machines
can be viewed as neural networks with one hidden layer, with the kernel as the nonlinearity.
For example, a Gaussian kernel machine is a radial basis function network (Poggio and
Girosi, 1990). But a deep network would seem to be irreducible to a kernel machine, since
it can represent some functions exponentially more compactly than a shallow one (Delalleau
and Bengio, 2011; Cohen et al., 2016).
Whether a representable function is actually learned, however, depends on the learning
algorithm. Most deep networks, and indeed most machine learning models, are trained
using variants of gradient descent (Rumelhart et al., 1986). Given an initial parameter
vector w0 and a loss function L = P
i L(yâˆ—
i , yi), gradient descent repeatedly modiï¬es the
modelâ€™s parameters w by subtracting the lossâ€™s gradient from them, scaled by the learning
rate Ïµ:
ws+1 = ws âˆ’Ïµâˆ‡wL(ws).
The process terminates when the gradient is zero and the loss is therefore at an optimum
(or saddle point). Remarkably, we have found that learning by gradient descent is a strong
enough constraint that the end result is guaranteed to be approximately a kernel machine,
regardless of the number of layers or other architectural features of the model.
Speciï¬cally, the kernel machines that result from gradient descent use what we term a
path kernel. If we take the learning rate to be inï¬nitesimally small, the path kernel between
two data points is simply the integral of the dot product of the modelâ€™s gradients at the
two points over the path taken by the parameters during gradient descent:
K(x, xâ€²) =
Z
c(t)
âˆ‡wy(x) Â· âˆ‡wy(xâ€²) dt,
where c(t) is the path. Intuitively, the path kernel measures how similarly the model at the
two data points varies during learning. The more similar the variation for x and xâ€², the
higher the weight of xâ€² in predicting y. Fig. 1 illustrates this graphically.
Our result builds on the concept of neural tangent kernel, recently introduced to analyze
the behavior of deep networks (Jacot et al., 2018). The neural tangent kernel is the integrand
of the path kernel when the model is a multilayer perceptron. Because of this, and since
a sum of positive deï¬nite kernels is also a positive deï¬nite kernel (SchÂ¨olkopf and Smola,
2

Deep Networks Are Kernel Machines
ğ›»ğ›»ğ‘¤ğ‘¤ğ‘¦ğ‘¦(ğ‘¥ğ‘¥2)
ğ›»ğ›»ğ‘¤ğ‘¤ğ‘¦ğ‘¦(ğ‘¥ğ‘¥1)
ğ›»ğ›»ğ‘¤ğ‘¤ğ‘¦ğ‘¦(ğ‘¥ğ‘¥)
ğ›»ğ›»ğ‘¤ğ‘¤ğ‘¦ğ‘¦(ğ‘¥ğ‘¥2)
ğ›»ğ›»ğ‘¤ğ‘¤ğ‘¦ğ‘¦(ğ‘¥ğ‘¥1)
ğ›»ğ›»ğ‘¤ğ‘¤ğ‘¦ğ‘¦(ğ‘¥ğ‘¥)
ğ‘¤ğ‘¤ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“
ğ‘¤ğ‘¤0
ğ‘¤ğ‘¤1
ğ‘¤ğ‘¤2
ğ‘¤ğ‘¤3
ğ›»ğ›»ğ‘¤ğ‘¤ğ‘¦ğ‘¦(ğ‘¥ğ‘¥1)
ğ›»ğ›»ğ‘¤ğ‘¤ğ‘¦ğ‘¦(ğ‘¥ğ‘¥)
ğ›»ğ›»ğ‘¤ğ‘¤ğ‘¦ğ‘¦(ğ‘¥ğ‘¥2)
Figure 1: How the path kernel measures similarity between examples.
In this two-
dimensional illustration, as the weights follow a path on the plane during training,
the modelâ€™s gradients (vectors on the weight plane) for x, x1 and x2 vary along
it. The kernel K(x, x1) is the integral of the dot product of the gradients âˆ‡wy(x)
and âˆ‡wy(x1) over the path, and similarly for K(x, x2). Because on average over
the weight path âˆ‡wy(x) Â· âˆ‡wy(x1) is greater than âˆ‡wy(x) Â· âˆ‡wy(x2), y1 has more
inï¬‚uence than y2 in predicting y, all else being equal.
3

Domingos
2002), the known conditions for positive deï¬niteness of neural tangent kernels extend to
path kernels (Jacot et al., 2018). A positive deï¬nite kernel is equivalent to a dot product in
a derived feature space, which greatly simpliï¬es its analysis (SchÂ¨olkopf and Smola, 2002).
We now present our main result. For simplicity, in the derivations below we assume
that y is a (real-valued) scalar, but it can be made a vector with only minor changes. The
data points xi can be arbitrary structures.
Deï¬nition 1 The tangent kernel associated with function fw(x) and parameter vector v is
Kg
f,v(x, xâ€²) = âˆ‡wfw(x) Â· âˆ‡wfw(xâ€²), with the gradients taken at v.
Deï¬nition 2 The path kernel associated with function fw(x) and curve c(t) in parameter
space is Kp
f,c(x, xâ€²) =
Z
c(t)
Kg
f,w(t)(x, xâ€²) dt.
Theorem 1 Suppose the model y = fw(x), with f a diï¬€erentiable function of w, is learned
from a training set {(xi, yâˆ—
i )}m
i=1 by gradient descent with diï¬€erentiable loss function L =
P
i L(yâˆ—
i , yi) and learning rate Ïµ. Then
lim
Ïµâ†’0 y =
m
X
i=1
aiK(x, xi) + b,
where K(x, xi) is the path kernel associated with fw(x) and the path taken by the param-
eters during gradient descent, ai is the average âˆ’âˆ‚L/âˆ‚yi along the path weighted by the
corresponding tangent kernel, and b is the initial model.
Proof In the Ïµ â†’0 limit, the gradient descent equation, which can also be written as
ws+1 âˆ’ws
Ïµ
= âˆ’âˆ‡wL(ws),
where L is the loss function, becomes the diï¬€erential equation
dw(t)
dt
= âˆ’âˆ‡wL(w(t)).
(This is known as a gradient ï¬‚ow (Ambrosio et al., 2008).) Then for any diï¬€erentiable
function of the weights y,
dy
dt =
d
X
j=1
âˆ‚y
âˆ‚wj
dwj
dt ,
where d is the number of parameters. Replacing dwj/dt by its gradient descent expression:
dy
dt =
d
X
j=1
âˆ‚y
âˆ‚wj

âˆ’âˆ‚L
âˆ‚wj

.
Applying the additivity of the loss and the chain rule of diï¬€erentiation:
dy
dt =
d
X
j=1
âˆ‚y
âˆ‚wj
 
âˆ’
m
X
i=1
âˆ‚L
âˆ‚yi
âˆ‚yi
âˆ‚wj
!
.
4

Deep Networks Are Kernel Machines
Rearranging terms:
dy
dt = âˆ’
m
X
i=1
âˆ‚L
âˆ‚yi
d
X
j=1
âˆ‚y
âˆ‚wj
âˆ‚yi
âˆ‚wj
.
Let Lâ€²(yâˆ—
i , yi) = âˆ‚L/âˆ‚yi, the loss derivative for the ith output. Applying this and Deï¬ni-
tion 1:
dy
dt = âˆ’
m
X
i=1
Lâ€²(yâˆ—
i , yi)Kg
f,w(t)(x, xi).
Let y0 be the initial model, prior to gradient descent. Then for the ï¬nal model y:
lim
Ïµâ†’0 y = y0 âˆ’
Z
c(t)
m
X
i=1
Lâ€²(yâˆ—
i , yi)Kg
f,w(t)(x, xi) dt,
where c(t) is the path taken by the parameters during gradient descent. Multiplying and
dividing by
R
c(t) Kg
f,w(t)(x, xi) dt:
lim
Ïµâ†’0 y = y0 âˆ’
m
X
i=1
 R
c(t) Kg
f,w(t)(x, xi)Lâ€²(yâˆ—
i , yi) dt
R
c(t) Kg
f,w(t)(x, xi) dt
! Z
c(t)
Kg
f,w(t)(x, xi) dt.
Let Lâ€²(yâˆ—
i , yi) =
R
c(t) Kg
f,w(t)(x, xi)Lâ€²(yâˆ—
i , yi) dt/
R
c(t) Kg
f,w(t)(x, xi) dt, the average loss deriva-
tive weighted by similarity to x. Applying this and Deï¬nition 2:
lim
Ïµâ†’0 y = y0 âˆ’
m
X
i=1
Lâ€²(yâˆ—
i , yi)Kp
f,c(x, xi).
Thus
lim
Ïµâ†’0 y =
m
X
i=1
aiK(x, xi) + b,
with K(x, xi) = Kp
f,c(x, xi), ai = âˆ’Lâ€²(yâˆ—
i , yi), and b = y0.
Remark 1 This diï¬€ers from typical kernel machines in that the aiâ€™s and b depend on x.
Nevertheless, the aiâ€™s play a role similar to the example weights in ordinary SVMs and the
perceptron algorithm: examples that the loss is more sensitive to during learning have a
higher weight. b is simply the prior model, and the ï¬nal model is thus the sum of the prior
model and the model learned by gradient descent, with the query point entering the latter
only through kernels. Since Theorem 1 applies to every yi as a query throughout gradient
descent, the training data points also enter the model only through kernels (initial model
aside).
Remark 2 Theorem 1 can equally well be proved using the loss-weighted path kernel Klp
f,c,L =
R
c(t) Lâ€²(yâˆ—
i , yi)Kg
f,w(t)(x, xi) dt, in which case ai = âˆ’1 for all i.
5

Domingos
Remark 3 In least-squares regression, Lâ€²(yâˆ—
i , yi) = yi âˆ’yâˆ—
i .
When learning a classiï¬er
by minimizing cross-entropy, the standard practice in deep learning, the function to be
estimated is the conditional probability of the class, pi, the loss is âˆ’Pm
i=1 ln pi, and the
loss derivative for the ith output is âˆ’1/pi. Similar expressions hold for modeling a joint
distribution by minimizing negative log likelihood, with pi as the probability of the data point.
Remark 4
Adding a regularization term R(w) to the loss function simply adds
âˆ’
R
c(t)
Pd
j=1(âˆ‚y/âˆ‚wj)(âˆ‚R/âˆ‚wj) to b.
Remark 5 The proof above is for batch gradient descent, which uses all training data points
at each step. To extend it to stochastic gradient descent, which uses a subsample, it suï¬ƒces
to multiply each term in the summation over data points by an indicator function Ii(t) that
is 1 if the ith data point is included in the subsample at time t and 0 otherwise. The only
change this causes in the result is that the path kernel and average loss derivative for a
data point are now stochastic integrals. Based on previous results (Scieur et al., 2017),
Theorem 1 or a similar result seems likely to also apply to further variants of gradient
descent, but proving this remains an open problem.
For linear models, the path kernel reduces to the dot product of the data points. It
is well known that a single-layer perceptron is a kernel machine, with the dot product as
the kernel (Aizerman et al., 1964). Our result can be viewed as a generalization of this to
multilayer perceptrons and other models. It is also related to Lippmann et al.â€™s proof that
Hopï¬eld networks, a predecessor of many current deep architectures, are equivalent to the
nearest-neighbor algorithm, a predecessor of kernel machines, with Hamming distance as
the comparison function (Lippmann et al., 1987).
The result assumes that the learning rate is suï¬ƒciently small for the trajectory of the
weights during gradient descent to be well approximated by a smooth curve. This is standard
in the analysis of gradient descent, and is also generally a good approximation in practice,
since the learning rate has to be quite small in order to avoid divergence (e.g., Ïµ = 10âˆ’3)
(Goodfellow et al., 2016). Nevertheless, it remains an open question to what extent models
learned by gradient descent can still be approximated by kernel machines outside of this
regime.
3. Discussion
A notable disadvantage of deep networks is their lack of interpretability (Zhang and Zhu,
2018). Knowing that they are eï¬€ectively path kernel machines greatly ameliorates this. In
particular, the weights of a deep network have a straightforward interpretation as a super-
position of the training examples in gradient space, where each example is represented by
the corresponding gradient of the model. Fig. 2 illustrates this. One well-studied approach
to interpreting the output of deep networks involves looking for training instances that are
close to the query in Euclidean or some other simple space (Ribeiro et al., 2016). Path
kernels tell us what the exact space for these comparisons should be, and how it relates to
the modelâ€™s predictions.
Experimentally, deep networks and kernel machines often perform more similarly than
would be expected based on their mathematical formulation (Brendel and Bethge, 2019).
6

Deep Networks Are Kernel Machines
Query
Answer
Training example 1
ğ¾ğ¾
ğ‘ğ‘1
Training example 2
ğ¾ğ¾
ğ‘ğ‘2
Training example m
ğ¾ğ¾
ğ‘ğ‘ğ‘šğ‘š
+
Model
Figure 2: Deep network weights as superpositions of training examples.
Applying the
learned model to a query example is equivalent to simultaneously matching the
query with each stored example using the path kernel and outputting a weighted
sum of the results.
7

Domingos
Even when they generalize well, deep networks often appear to memorize and replay whole
training instances (Zhang et al., 2017; Devlin et al., 2015). The fact that deep networks are
in fact kernel machines helps explain both of these observations. It also sheds light on the
surprising brittleness of deep models, whose performance can degrade rapidly as the query
point moves away from the nearest training instance (Szegedy et al., 2014), since this is
what is expected of kernel estimators in high-dimensional spaces (Hardle et al., 2004).
Perhaps the most signiï¬cant implication of our result for deep learning is that it casts
doubt on the common view that it works by automatically discovering new representations of
the data, in contrast with other machine learning methods, which rely on predeï¬ned features
(Bengio et al., 2013). As it turns out, deep learning also relies on such features, namely the
gradients of a predeï¬ned function, and uses them for prediction via dot products in feature
space, like other kernel machines. All that gradient descent does is select features from this
space for use in the kernel. If gradient descent is limited in its ability to learn representations,
better methods for this purpose are a key research direction. Current nonlinear alternatives
include predicate invention (Muggleton and Buntine, 1988) and latent variable discovery
in graphical models (Elidan et al., 2000).
Techniques like structure mapping (Gentner,
1983), crossover (Holland, 1975) and predictive coding (Rao and Ballard, 1999) may also
be relevant. Ultimately, however, we may need entirely new approaches to solve this crucial
but extremely diï¬ƒcult problem.
Our result also has signiï¬cant consequences on the kernel machine side. Path kernels
provide a new and very ï¬‚exible way to incorporate knowledge of the target function into
the kernel. Previously, it was only possible to do so in a weak sense, via generic notions
of what makes two data points similar. The extensive knowledge that has been encoded
into deep architectures by applied researchers, and is crucial to the success of deep learning,
can now be ported directly to kernel machines.
For example, kernels with translation
invariance or selective attention are directly obtainable from the architecture of, respectively,
convolutional neural networks (LeCun et al., 1998) or transformers (Vaswani et al., 2017).
A key property of path kernels is that they combat the curse of dimensionality by incor-
porating derivatives into the kernel: two data points are similar if the candidate functionâ€™s
derivatives at them are similar, rather than if they are close in the input space. This can
greatly improve kernel machinesâ€™ ability to approximate highly variable functions (Bengio
et al., 2005). It also means that points that are far in Euclidean space can be close in gradi-
ent space, potentially improving the ability to model complex functions. (For example, the
maxima of a sine wave are all close in gradient space, even though they can be arbitrarily
far apart in the input space.)
Most signiï¬cantly, however, learning path kernel machines via gradient descent largely
overcomes the scalability bottlenecks that have long limited the applicability of kernel meth-
ods to large data sets. Computing and storing the Gram matrix at learning time, with its
quadratic cost in the number of examples, is no longer required. (The Gram matrix is the
matrix of applications of the kernel to all pairs of training examples.) Separately storing
and matching (a subset of) the training examples at query time is also no longer necessary,
since they are eï¬€ectively all stored and matched simultaneously via their superposition in
the model parameters. The storage space and matching time are independent of the num-
ber of examples. (Interestingly, superposition has been hypothesized to play a key role in
combatting the combinatorial explosion in visual cognition (Arathorn, 2002), and is also
8

Deep Networks Are Kernel Machines
essential to the eï¬ƒciency of quantum computing (Nielsen and Chuang, 2000) and radio
communication (Carlson and Grilly, 2009).) Further, the same specialized hardware that
has given deep learning a decisive edge in scaling up to large data (Raina et al., 2009) can
now be used for kernel machines as well.
The signiï¬cance of our result extends beyond deep networks and kernel machines. In its
light, gradient descent can be viewed as a boosting algorithm, with tangent kernel machines
as the weak learner and path kernel machines as the strong learner obtained by boosting
it (Freund and Schapire, 1997). In each round of boosting, the examples are weighted by
the corresponding loss derivatives. It is easily seen that each round (gradient descent step)
decreases the loss, as required. The weight of the model at a given round is the learning rate
for that step, which can be constant or the result of a line search (Boyd and Vandenberghe,
2004). In the latter case gradient descent is similar to gradient boosting (Mason et al.,
1999).
Another consequence of our result is that every probabilistic model learned by gradient
descent, including Bayesian networks (Koller and Friedman, 2009), is a form of kernel
density estimation (Parzen, 1962). The result also implies that the solution of every convex
learning problem is a kernel machine, irrespective of the optimization method used, since,
being unique, it is necessarily the solution obtained by gradient descent.
It is an open
question whether the result can be extended to nonconvex models learned by non-gradient-
based techniques, including constrained (Bertsekas, 1982) and combinatorial optimization
(Papadimitriou and Steiglitz, 1982).
The results in this paper suggest a number of research directions. For example, viewing
gradient descent as a method for learning path kernel machines may provide new paths
for improving it.
Conversely, gradient descent is not necessarily the only way to form
superpositions of examples that are useful for prediction.
The key question is how to
optimize the tradeoï¬€between accurately capturing the target function and minimizing the
computational cost of storing and matching the examples in the superposition.
Acknowledgments
This research was partly funded by ONR grant N00014-18-1-2826. Thanks to LÂ´eon Bottou
and Simon Du for feedback on a draft of this paper.
References
M. A. Aizerman, E. M. Braverman, and L. I. Rozonoer. Theoretical foundations of the
potential function method in pattern recognition learning. Autom. & Remote Contr., 25:
821â€“837, 1964.
Luigi Ambrosio, Nicola Gigli, and Giuseppe SavarÂ´e. Gradient Flows: In Metric Spaces and
in the Space of Probability Measures. BirkhÂ¨auser, Basel, 2nd edition, 2008.
D. W. Arathorn. Map-Seeking Circuits in Visual Cognition: A Computational Mechanism
for Biological and Machine Vision. Stanford Univ. Press, Stanford, CA, 2002.
9

Domingos
Y. Bengio, O. Delalleau, and N. L. Roux. The curse of highly variable functions for local
kernel machines. Adv. Neural Inf. Proc. Sys., 18:107â€“114, 2005.
Y. Bengio, A. Courville, and P. Vincent.
Representation learning: A review and new
perspectives. IEEE Trans. Patt. An. & Mach. Intell., 35:1798â€“1828, 2013.
P. Bertsekas. Constrained Optimization and Lagrange Multiplier Methods. Academic Press,
Cambridge, MA, 1982.
S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, Cam-
bridge, UK, 2004.
W. Brendel and M. Bethge. Approximating CNNs with bag-of-local-features models works
surprisingly well on ImageNet. In Proc. Int. Conf. Learn. Repr., 2019.
A. B. Carlson and P. B. Grilly. Communication Systems: An Introduction to Signals and
Noise in Electrical Communication. McGraw-Hill, New York, 5th edition, 2009.
N. Cohen, O. Sharir, and A. Shashua. On the expressive power of deep learning: A tensor
analysis. In Proc. Ann. Conf. Learn. Th., pages 698â€“728, 2016.
C. Cortes, M. Mohri, and A. Rostamizadeh. Learning non-linear combinations of kernels.
Adv. Neural Inf. Proc. Sys., 22:396â€“404, 2009.
O. Delalleau and Y. Bengio. Shallow vs. deep sum-product networks. Adv. Neural Inf. Proc.
Sys., 24:666â€“674, 2011.
J. Devlin, H. Cheng, H. Fang, S. Gupta, L. Deng, X. He, G. Zweig, and M. Mitchell.
Language models for image captioning: The quirks and what works. In Proc. Ann. Meet.
Assoc. Comp. Ling., pages 100â€“105, 2015.
G. Elidan, N. Lotner, N. Friedman, and D. Koller.
Discovering hidden variables:
A
structure-based approach. Adv. Neural Inf. Proc. Sys., 13:479â€“485, 2000.
Y. Freund and R. Schapire. A decision-theoretic generalization of on-line learning and an
application to boosting. J. Comp. & Sys. Sci., 55:119â€“139, 1997.
D. Gentner. Structure-mapping: A theoretical framework for analogy. Cog. Sci., 7:155â€“170,
1983.
I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, Cambridge, MA,
2016.
W. Hardle, M. MÂ¨uller, and A. Werwatz.
Nonparametric and Semiparametric Models.
Springer, Berlin, 2004.
J. H. Holland. Adaptation in Natural and Artiï¬cial Systems. Univ. Michigan Press, Ann
Arbor, MI, 1975.
A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization
in neural networks. Adv. Neural Inf. Proc. Sys., 31:8571â€“8580, 2018.
10

Deep Networks Are Kernel Machines
D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques.
MIT Press, Cambridge, MA, 2009.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haï¬€ner. Gradient-based learning applied to docu-
ment recognition. Proc. IEEE, 86:2278â€“2324, 1998.
R. Lippmann, B. Gold, and M. Malpass. A comparison of Hamming and Hopï¬eld neural
networks for pattern classiï¬cation. Technical Report 769, MIT Lincoln Lab, Lexington,
MA, 1987.
L. Mason, J. Baxter, P. L. Bartlett, and M. R. Frean. Boosting algorithms as gradient
descent. Adv. Neural Inf. Proc. Sys,, 12:512â€“518, 1999.
S. Muggleton and W. Buntine.
Machine invention of ï¬rst-order predicates by inverting
resolution. In Proc. Int. Conf. Mach. Learn., pages 339â€“352, 1988.
M. A. Nielsen and I. L. Chuang. Quantum Computation and Quantum Information. Cam-
bridge Univ. Press, Cambridge, UK, 2000.
C. Papadimitriou and K. Steiglitz. Combinatorial Optimization: Algorithms and Complex-
ity. Prentice-Hall, Upper Saddle River, NJ, 1982.
E. Parzen. On estimation of a probability density function and mode. Ann. Math. Stat.,
33:1065â€“1076, 1962.
T. Poggio and F. Girosi.
Regularization algorithms for learning that are equivalent to
multilayer networks. Science, 247:978â€“982, 1990.
R. Raina, A. Madhavan, and A. Y. Ng.
Large-scale deep unsupervised learning using
graphics processors. In Proc. Int. Conf. Mach. Learn., pages 873â€“880, 2009.
R. P. N. Rao and D. H. Ballard.
Predictive coding in the visual cortex: A functional
interpretation of some extra-classical receptive ï¬eld eï¬€ects. Nature Neurosci., 2:79â€“87,
1999.
M. T. Ribeiro, S. Singh, and C. Guestrin. â€œWhy should I trust you?â€: Explaining the
predictions of any classiï¬er. In Proc. Int. Conf. Knowl. Disc. & Data Mining, pages
1135â€“1144, 2016.
D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-
propagating errors. Nature, 323:533â€“536, 1986.
B. SchÂ¨olkopf and A. J. Smola. Learning with Kernels: Support Vector Machines, Regular-
ization, Optimization, and Beyond. MIT Press, Cambridge, MA, 2002.
D. Scieur, V. Roulet, F. Bach, and A. dâ€™Aspremont. Integration methods and optimization
algorithms. Adv. Neural Inf. Proc. Sys., 30:1109â€“1118, 2017.
C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus.
Intriguing properties of neural networks. In Proc. Int. Conf. Learn. Repr., 2014.
11

Domingos
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, N. Aidan, L. Kaiser, and
I. Polosukhin. Attention is all you need. Adv. Neural Inf. Proc. Sys., 30:5998â€“6008, 2017.
C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning
requires rethinking generalization. In Proc. Int. Conf. Learn. Repr., 2017.
Q. Zhang and S.-C. Zhu. Visual interpretability for deep learning: A survey. Front. Inf.
Tech. & Elec. Eng., 19:27â€“39, 2018.
12

