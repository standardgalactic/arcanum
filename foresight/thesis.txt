HAL Id: tel-01091628
https://hal.archives-ouvertes.fr/tel-01091628v3
Submitted on 25 Nov 2015
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not.
The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Game semantics and realizability for classical logic
Valentin Blot
To cite this version:
Valentin Blot. Game semantics and realizability for classical logic. Other [cs.OH]. Ecole normale
supérieure de lyon - ENS LYON, 2014. English. ￿NNT : 2014ENSL0945￿. ￿tel-01091628v3￿

THÈSE
en vue de l’obtention du grade de
Docteur de l’Université de Lyon, délivré par l’École Normale Supérieure de Lyon
Discipline : Informatique
Laboratoire de l’Informatique du Parallélisme
École Doctorale InfoMaths (ED512)
Game semantics and realizability for classical logic
présentée et soutenue publiquement le vendredi 7 novembre 2014 par
Valentin Blot
Directeur :
Olivier Laurent
Encadrant :
Colin Riba
Devant la commission d’examen formée de :
Pierre-Louis Curien
CNRS, Université Paris Diderot - Paris 7
Président
Pierre Hyvernat
Université de Savoie
Examinateur
Olivier Laurent
CNRS, École Normale Supérieure de Lyon
Directeur
Andrzej Murawski
University of Warwick
Rapporteur
Colin Riba
École Normale Supérieure de Lyon
Encadrant
Thomas Streicher
Technische Universität Darmstadt
Rapporteur

2

Abstract
This thesis investigates two realizability models for classical logic built on HO game semantics.
The main motivation is to have a direct computational interpretation of classical logic, arithmetic
and analysis with programs manipulating a higher-order store.
Relaxing the innocence condition in HO games provides higher-order references, and drop-
ping the well-bracketing of strategies reveals the CPS of HO games and gives a category of
continuations in which we can interpret Parigot’s lambda-mu calculus. This permits a direct
computational interpretation of classical proofs from which we build two realizability models.
The ﬁrst model is orthogonality-based, as the one of Krivine. However, it is simply-typed
and ﬁrst-order. This means that we do not use a second-order coding of falsity, and extraction
is handled by considering realizers with a free mu-variable. We provide a bar-recursor in this
model and prove that it realizes the axiom of dependent choice, relying on two consequences
of the CPO structure of the games model: every function on natural numbers (possibly non
computable) exists in the model, and every functional on sequences is Scott-continuous. Usually,
bar-recursion is used to intuitionistically realize the double negation shift and consequently the
negative translation of the axiom of choice. Here, we directly realize the axiom of choice in a
classical setting.
The second model relies on winning conditions and is very speciﬁc to the games model. A
winning condition is a set of positions in a game which satisﬁes some coherence properties, and
a strategy realizes a formula if all its positions are winning.
Résumé
Cette thèse étudie deux modèles de réalisabilité pour la logique classique construits sur la sé-
mantique des jeux HO, interprétant la logique, l’arithmétique et l’analyse classiques directement
par des programmes manipulant un espace de stockage d’ordre supérieur.
La non-innocence en jeux HO autorise les références d’ordre supérieur, et le non parenthésage
révèle la CPS des jeux HO et fournit une catégorie de continuations dans laquelle interpréter le
lambda-mu calcul de Parigot. Deux modèles de réalisabilité sont construits sur cette interpré-
tation calculatoire directe des preuves classiques.
Le premier repose sur l’orthogonalité, comme celui de Krivine, mais il est simplement typé
et au premier ordre. En l’absence de codage de l’absurdité au second ordre, une mu-variable
libre dans les réaliseurs permet l’extraction. Nous déﬁnissons un bar-récurseur et prouvons qu’il
réalise l’axiome du choix dépendant, utilisant deux conséquences de la structure de CPO du
modèle de jeux: toute fonction sur les entiers (même non calculable) existe dans le modèle,
et toute fonctionnelle sur des séquences est Scott-continue. La bar-récursion est habituellement
utilisée pour réaliser intuitionnistiquement le « double negation shift » et en déduire la traduction
négative de l’axiome du choix. Ici, nous réalisons directement l’axiome du choix dans un cadre
classique.
Le second, très spéciﬁque au modèle de jeux, repose sur des conditions de gain: des ensembles
de positions d’un jeu munis de propriétés de cohérence. Un réaliseur est alors une stratégie dont
les positions sont toutes gagnantes.
3

Acknowledgements
Tout d’abord un grand merci à Colin pour ces trois années d’encadrement.
Tu as su être
extrêmement disponible, notamment dans les moments diﬃciles que vit tout thésard, et je
mesure pleinement l’eﬀort fourni pour me faire comprendre patiemment des concepts complexes
malgré ma novicité scientiﬁque. Je pense que cette première expérience d’encadrement est un
franc succès et j’espère que tu auras l’occasion d’en faire proﬁter encore beaucoup d’autres après
moi. Merci également à Olivier, tout d’abord pour ton rôle de directeur administratif, mais
aussi et surtout pour être allé régulièrement au-delà de ce seul rôle en me faisant proﬁter de ton
expertise scientiﬁque, de la sémantique des jeux au lambda-mu calcul. Cette présence a sans
aucun doute été un vrai plus tout au long de mon travail.
I would like to warmly thank my referees. Thomas, you gave very relevant remarks on my
work. Every exchange we had allowed me to consider a new point of view and stimulated my
scientiﬁc thirst. I thank you for the interest you showed regarding to my work. Andrzej, thank
you for the time you spent reading my thesis, witnessed by your very accurate understanding of
my explanations, despite their intricacy. I was honoured to have the opportunity to present you
my work.
Merci à Pierre-Louis d’avoir accepté de faire partie de mon jury.
Ton large spectre de
connaissances apporte assurément une grande valeur à l’évaluation de mon travail, et j’admire
ta capacité à rester malgré tout toujours accessible. Pierre, c’est un honneur de te compter
parmi les membres de mon jury et de proﬁter de ton expertise, et merci pour ta permanente
ouverture à la discussion.
Je remercie également tous les membres du labo, ma présence au LIP restera une expérience
inoubliable. Merci aux plumeux, Alexandre pour nos nombreuses discussions scientiﬁques ou
non et pour son éternelle bonne humeur, Daniel ce sacré boute-en-train, Pierre le sémanticien des
jeux improvisé career advisor, Damien et nos échanges de scies, Patrick pour son organisation
sans faille, et tous les autres membres. Merci à Catherine pour sa gentillesse et son eﬃcacité, à
Damien et à tout le secrétariat qui fait un travail remarquable. Merci à JMpega, Paul « ﬁn en
humour...» Brunet, Maxime, Matthieu, Gaupy, Adeline, Sébastien, Thanos, Mickaël, Anupam,
Simon.
Je remercie très fort ma famille. Bien que j’aie été par le passé très injuste avec vous, papa
et maman, vous êtes toujours restés ﬁers de moi et votre soutien m’a sans cesse aidé à avancer
et à m’accomplir pleinement. Merci à Flo pour l’extrême bienveillance dont tu as toujours fait
preuve envers moi, merci à Chris pour ta toujours très communicative bonne humeur, merci à
Dine pour ton omniprésence malgré l’éloignement géographique, et merci à Antoine et Victor.
Merci également à Kader, Blandine et Hugues, et merci à Inès, Elijah, Naël et [...], pour être là
(ou bientôt là), simplement. Merci à Camille pour tous les moments partagés, passés, présents
et à venir, les bons et les plus diﬃciles, grâce à toi je me sens exister un peu plus chaque jour.
Merci également à Val pour me supporter sans répit depuis maintenant presque 28 ans.
Merci à Aurèle et aux parisiens, et merci au gang des lyonnais, Sam, Ju, Olivier, Grovaro,
pour notre année d’agreg inoubliable et pour tout le reste. Merci enﬁn à Blanche, Nor & Guigui
Grosmouly, Guilhem, Séb, Nath, Pauline, Martin et tous ceux que j’oublie.
4

Contents
1
Introduction
7
1.1
Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.2
Realizability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.3
Game semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
1.3.1
Historical overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
1.3.2
The concepts of game semantics . . . . . . . . . . . . . . . . . . . . . . . .
10
1.3.3
The extensions of game semantics . . . . . . . . . . . . . . . . . . . . . . .
11
1.4
Outline
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
2
Logic
15
2.1
Classical multisorted ﬁrst-order logic . . . . . . . . . . . . . . . . . . . . . . . . .
15
2.2
Equational theories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
2.3
The relativization predicate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
2.4
Peano arithmetic in ﬁnite types: PAω
. . . . . . . . . . . . . . . . . . . . . . . .
19
2.4.1
Relativized PAω: PAωr . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
2.4.2
Relation to usual theories . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
2.5
The axiom of choice
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
2.5.1
Classical analysis: CAω
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
2.5.2
Relativized CAω: CAωr . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
2.6
Models of classical multisorted ﬁrst-order logic
. . . . . . . . . . . . . . . . . . .
23
3
Typed languages
25
3.1
λµ-calculus
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
3.1.1
Type system
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
3.1.2
λµ theories
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
3.1.3
Interpreting classical logic in λµ-calculus . . . . . . . . . . . . . . . . . . .
28
3.2
Categories of continuations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
3.2.1
Classical disjunction in categories of continuations. . . . . . . . . . . . . .
30
3.2.2
Coproduct completion of a category
. . . . . . . . . . . . . . . . . . . . .
30
3.2.3
Interpretation of λµ-calculus in categories of continuations . . . . . . . . .
34
3.2.4
Connection with the call-by-name CPS translation of λµ-calculus . . . . .
35
3.2.5
Interactions between C and RC
. . . . . . . . . . . . . . . . . . . . . . . .
36
3.3
µPCF
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
3.3.1
Type constants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
3.3.2
Constants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
3.3.3
Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
3.4
System T
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
3.4.1
Interpreting PAωr in system T + Ω. . . . . . . . . . . . . . . . . . . . . .
39
3.5
Bar recursion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
5

3.5.1
Well-founded induction and recursion . . . . . . . . . . . . . . . . . . . . .
40
3.5.2
Recursion on natural numbers . . . . . . . . . . . . . . . . . . . . . . . . .
40
3.5.3
Recursion on well-founded trees . . . . . . . . . . . . . . . . . . . . . . . .
41
3.5.4
Bar recursion in µPCF . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
4
Game semantics
47
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
4.2
Arenas and strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
4.2.1
Arenas and justiﬁed sequences
. . . . . . . . . . . . . . . . . . . . . . . .
56
4.2.2
Plays and strategies
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
58
4.3
The category G of arenas and strategies
. . . . . . . . . . . . . . . . . . . . . . .
58
4.3.1
The category G . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
58
4.3.2
Countable products
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
4.3.3
Closed structure
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
4.4
Interpreting µPCF in game semantics
. . . . . . . . . . . . . . . . . . . . . . . .
67
4.4.1
G as a category of continuations . . . . . . . . . . . . . . . . . . . . . . . .
67
4.4.2
Natural numbers, successor and predecessor . . . . . . . . . . . . . . . . .
70
4.4.3
Conditionals
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
4.4.4
Fixed point operators
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
5
An orthogonality-based realizability model for classical analysis
75
5.1
The realizability relation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
75
5.1.1
Negative translation and orthogonality . . . . . . . . . . . . . . . . . . . .
75
5.1.2
Truth values, falsity values
. . . . . . . . . . . . . . . . . . . . . . . . . .
76
5.2
Adequacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
5.2.1
Adequacy for ﬁrst-order logic . . . . . . . . . . . . . . . . . . . . . . . . .
79
5.2.2
Adequacy for Peano arithmetic . . . . . . . . . . . . . . . . . . . . . . . .
82
5.2.3
Adequacy for the axiom of choice . . . . . . . . . . . . . . . . . . . . . . .
84
5.3
Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
6
A realizability model of winning strategies for classical arithmetic
91
6.1
Justiﬁed sequences via thick subtrees . . . . . . . . . . . . . . . . . . . . . . . . .
91
6.1.1
Justiﬁed sequences as thick subtrees
. . . . . . . . . . . . . . . . . . . . .
92
6.1.2
Correspondence with the usual setting . . . . . . . . . . . . . . . . . . . .
93
6.2
Winning conditions on arenas . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
6.2.1
P-subpositions, O-subpositions . . . . . . . . . . . . . . . . . . . . . . . .
95
6.2.2
Winning conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
96
6.2.3
Winning strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
6.2.4
WA→B versus Kleene arrow . . . . . . . . . . . . . . . . . . . . . . . . . . 101
6.3
Realizability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
6.3.1
Adequacy for ﬁrst-order logic . . . . . . . . . . . . . . . . . . . . . . . . . 102
6.3.2
Adequacy for Peano arithmetic . . . . . . . . . . . . . . . . . . . . . . . . 103
6.3.3
Extraction through Friedman translation . . . . . . . . . . . . . . . . . . . 107
6.3.4
About the axiom of choice . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
7
Conclusion
113
6

Chapter 1
Introduction
This thesis investigates the computational content of classical logic, arithmetic and the axiom of
choice. In order to extract computational content from proofs we use the techniques of realizabil-
ity, that is a formalization of the Brouwer-Heyting-Kolmogorov interpretation. Moreover, this
computational content is exploited in the model of Hyland-Ong-Nickau game semantics which
provides control features (used to handle classical reasoning) and higher-order references (which
could be later exploited to provide new realizers).
1.1
Logic
Logic ﬁnds its very ﬁrst roots in ancient Egypt and Babylonia. Later on, it was mainly developed
in ancient Greece, India and China around the 5th century BC. In the ﬁeld of geometry, the
Pythagoreans developed a notion of basic true propositions (which we now call axioms) from
which one can formally derive all true propositions of a system.
The principle of reductio
ad absurdum (if we can derive an impossible conclusion from a given proposition then this
proposition is false) also appeared in the same era. In political debates, Sophists exploited the
perception of logic as a universal principle in order to persuade an audience by hiding false logical
principles behind an apparently trivial statement. Most of what we know from the logic at that
time comes from Plato’s work. Aristotle studied for twenty years at Plato’s Academy and made
a signiﬁcant work on logic, introducing the syllogism. The most famous example of a syllogism
is: “All men are mortal, Socrates is a man, therefore Socrates is mortal.” However, without a
formal system one can easily build false syllogisms, for example: “what is rare is expensive, a
cheap horse is rare, therefore a cheap horse is expensive.” This illustrates that logic admits no
“common-sense” argument and logic is empty without formalism.
During post-classical history, advances were made in the Middle-east and in Europe, mainly
following the lines of Aristotelian logic, the philosophical branch of logic reaching a peak with
“The Science of Logic” by Hegel in 1816. In the 17th century the ﬁeld of symbolic logic emerged,
with Leibniz arguing on the importance of symbolic notations in mathematics, and more specif-
ically in calculus. This idea appeared to be fundamental in modern logic, for example in the
works of Turing and Gödel in the 1930’s. This led to a revival of logic, which was no more a
philosopher’s concern, but more and more an area of mathematics. During the 19th century,
Boole and De Morgan followed this way and undertook an algebraization of logic. They were
followed by Peirce who took as an axiom the formula ((A ⇒B) ⇒A) ⇒A (which is now called
Peirce’s law). This formula allows classical reasoning just as the law of excluded middle, and
plays a crucial role in the computational interpretation of classical logic.
The concept of quantiﬁcation (∃and ∀) ﬁrst appeared in Frege’s Begriﬀsschrift [Fre79] in
7

1879, who used a two-dimensional notation for these. This concept allowed him to build an
axiomatic system which is still at the heart of modern logic, with rules of modus ponens (from
A and A ⇒B one can deduce B), generalization (if A ⇒B (x) holds and the variable x does
not appear in A, then A ⇒∀x B (x) holds) and substitution (from ∀x A (x) one can derive A (t)
for any t). His work remained conﬁdential until Russell shed the lights on it. Indeed, in 1874,
Cantor had developed in [Can74] the ﬁrst set theory (which we call now naive set theory), and
proved that the sets of natural numbers and real numbers were not equinumerous, even though
these are both inﬁnite. Cantor later reproved this fact in [Can92] using a diagonal argument,
which turned out to be a very fruitful technique which was later used by Turing in computability
theory and by Gödel in proof theory. However, the most striking use of the diagonal argument is
that of Russell, who used it to prove in 1901 the inconsistency of Cantor’s own set theory (more
precisely, of the axiomatization of his set theory in Frege’s formal system). Russell’s paradox
relies on the fact that in naive set theory, one can deﬁne from any given property the set of
elements that satisfy this property. The property of not being an element of ourself then gives a
set x such that x belongs to x and x does not belong to x, hence the contradiction. This discovery
put a cat among the pigeons, and two solutions emerged, which are still present in modern logic.
The ﬁrst solution came in 1908 from Zermelo [Zer08], who deﬁned the axiom of separation: from
a given property, we cannot construct the set of elements satisfying it (otherwise we would get
Russell’s paradox, and therefore inconsistency), however if we also have a ﬁxed set E, then it
is consistent to construct the set of elements of E for which the property holds. Zermelo set
theory was improved later and became Zermelo-Frænkel set theory, which is a formal system in
which most of mathematics can be formalized. The second solution came from a joint work of
Russell and Whitehead who wrote between 1910 and 1913 the Principia Mathematica [RW13].
Russell’s paradox is avoided in the Principia Mathematica by considering a hierarchy: a set is
one level above its elements. Because of this hierarchy, it makes no sense to consider a set which
belongs to itself. This concept of a hierarchy is still present in Martin-Löf’s type theory and in
the proof assistant Coq.
The foundational crisis of the beginning of 20th century was the trigger to theoretical com-
puter science. In order to put an end to this crisis, Hilbert’s program proposes to ﬁnd a ﬁxed set
of axioms from which one could derive every theorem of mathematics. Hilbert believed that this
set of axioms would be those of basic arithmetic, more or less Peano’s axioms. Hilbert’s program
made sense only if one could prove that this basic set of axioms is consistent, and this was the
subject of the second problem among the 23 that Hilbert formulated during the international
congress of mathematicians in 1900 in Paris. In 1931, Gödel’s incompleteness theorems [Göd31]
put an end to this by stating that any eﬀectively presentable consistent logical system which
formalizes arithmetic contains undecidable formulas: formulas which cannot be proved to be
true nor false, and the consistency of arithmetic is such a formula. Using a diagonal argument,
Gödel proved that there exists no formal proof of the consistency of arithmetic inside the formal
system of arithmetic. Gödel is probably the most inﬂuential logician of the 20th century, his
main results being the completeness theorem of ﬁrst-order logic [Göd29] (a formula is provable
if and only if it is true in every model), the incompleteness of arithmetic [Göd31] (consistency of
arithmetic cannot be proved in arithmetic) and the consistency of the axiom of choice and the
continuum hypothesis with respect to Zermelo-Frænkel set theory [Göd40] (there exists a model
of Zermelo-Frænkel set theory, the constructible universe, in which both hold).
After Gödel’s answer to this quest for a universal system, new research areas emerged. First,
in 1934-1935, Gentzen deﬁned in [Gen35] two formal systems for writing mathematical proofs:
natural deduction and sequent calculus. The former is useful to write formally mathematical
proofs. The later, which is an equivalent but symmetric version of natural deduction, is hard
to use for formalizing proofs. However, sequent calculus is particularly ﬁt for proving Gentzen’s
8

famous cut-elimination theorem (from any formal proof, one can eliminate all the intermediary
lemmas and get a “minimal” proof). On another side, Church and Turing deﬁned (independently)
in 1936 the λ-calculus [Chu36] and the Turing machines [Tur37] to give a formal deﬁnition of
the notion of algorithm. It appeared quickly that the two notions coincide (the same functions
on natural numbers can be computed in the two models). Church and Turing used their models
of computability to provide a negative answer to Hilbert’s Entscheidungsproblem (ﬁnding an
algorithm which decides for any property if it is true or not) that Hilbert formulated in 1928.
Later, many other models of computation were proved equivalent to Church’s and Turing’s ones,
which led to the Church-Turing thesis: every function which can be algorithmically computed
(which is an informal notion) can be computed by (equivalently) a λ-term or a Turing Machine.
Proof theory and computability theory continued to evolve with discoveries of strong links
between the two. First, in 1958, Curry observed in [CF58] that the axioms of Hilbert-style
deduction systems for propositional logic correspond exactly to the types of the combinators of
Schönﬁnkel’s combinatory logic (a variable-free reformulation of Church’s λ-calculus). In 1969,
Howard went further and observed an exact correspondence between Gentzen’s natural deduction
and Church’s simply typed λ-calculus (manuscript published in [How80]). This correspondence
is now known as Curry-Howard isomorphism. In 1990, Griﬃn extended this isomorphism to
classical logic in [Gri90] by typing the call/cc operator of the functional language Scheme with
Peirce’s law.
The call/cc operator allows to manipulate continuations and it is similar to
the exception handling of modern programming languages, though the precise mechanisms are
diﬀerent, as explained in [RT99]. Building on this extension of the isomorphism, several calculi
with control features were deﬁned, for example with Parigot’s λµ-calculus [Par92] and Curien
and Herbelin’s λµeµ-calculus [CH00]. These calculi allow the programmer to manipulate contexts,
through context variables in λµ-calculus, and by explicitly writing them in λµeµ-calculus.
1.2
Realizability
Realizability is a technique introduced by Kleene in [Kle45] in order to give a formal account
of the so-called Brouwer-Heyting-Kolmogorov interpretation of proofs.
To each formula we
associate a set of realizers, which are programs which behave like a proof of the formula. The
important thing is that we do not look at how these programs are written, but only at their
behavior.
Many notions of realizability exist, for diﬀerent logics and using diﬀerent sets of
realizers, see [Tro98] for a survey.
Realizability should be put in parallel with the Curry-Howard isomorphism described in the
previous section. Under this isomorphism, a proof corresponds exactly to one typed program,
and the type of this program is derived from the formula. In a realizability model, the adequacy
lemma states that every proof of a formula, seen as a program, is a realizer of that formula.
However, the strength of realizability is that a realizer is not required to be such a proof. In
some realizability models, a realizer may even be untypable. Under this view, realizability may
be seen as a very loose notion of typing, the only requirement being that combining diﬀerent
realizers preserves truth.
Kleene’s realizability model for Heyting arithmetic HA (the intuitionistic variant of Peano
arithmetic PA) uses the set of recursive functions as realizers (using Gödel’s encoding of these
in the set of natural numbers). Using this model, he obtained the existence property: from a
proof of an existential statement one can extract a concrete witness which satisﬁes the property.
Another example is Kreisel’s modiﬁed realizability [Kre59] for HAω, Heyting arithmetic in ﬁnite
types, in which the realizers are typed programs. For a long time extraction of computational
content from proofs was only considered for intuitionistic, constructive logic.
9

Using concepts very close to those of realizability, Gödel deﬁned in [Göd58] the Dialectica
interpretation of HA into system T (an extension of primitive recursive functions to higher-
order types).
A comparison with realizability can be found in [Oli08], by decomposing the
interpretation through linear logic.
In [Gri90], Tim Griﬃn found out that the call/cc operator of the functional language
Scheme could be typed with a classical logic principle: the law of Peirce. This led Krivine to build
in [Kri01, Kri03, Kri09] a realizability model for classical logic so as to extract computational
content from classical proofs.
1.3
Game semantics
1.3.1
Historical overview
A programming language is usually deﬁned by ﬁrst its syntax (what is a well-formed program)
and second its operational semantics (how these programs compute) which is usually deﬁned
by an evaluation relation between programs. If moreover we ﬁx a particular set of programs
that we call values, we can deﬁne the observational equivalence of programs as: M and N are
observationally equivalent if for any context C and value v, C [M] evaluates to v if and only if
C [N] evaluates to v. When one deﬁnes a model of such a programming language, a very strong
property is that of full abstraction: two programs are observationally equivalent if and only if
their interpretations in the model are equal. For a deeper overview of the full abstraction problem
for PCF, we recommend to the reader Curien’s paper [Cur07], from which this introduction is
inspired. For a domain-theoretic approach to the problem, we also recommend the book [Str06].
The quest for a fully abstract model of the functional language PCF started with the seminal
work of Plotkin [Plo77] and Milner [Mil77]. Plotkin showed that the continuous model was fully
abstract for an extension of PCF with a “parallel or” operator adding concurrent features to
PCF, while Milner showed the uniqueness (up to isomorphism) of a fully abstract model of
PCF, and constructed it by quotienting the syntax. The goal was then to present this unique
model in a way that is as far from the syntax as possible.
A stepping stone was then Berry and Curien’s model of sequential algorithms [BC82], which
was proved to be fully abstract for an extension of PCF with a “catch” operator adding the
possibility to change the control ﬂow of a program.
Game semantics then appeared in the 90’s, at the same time in [HO00, Nic94, AJM00], as a
solution to the problem. The full abstraction property of these models is obtained from a model
which satisﬁes the deﬁnability property by quotienting it with the observational equivalence
relation (which is non-computable). For this reason, the strength of the games models is rather
their deﬁnability property than their full abstraction with regard to PCF. These models are
much more away from the syntax than that of Milner, even if not as much as the continuous
model.
Later on, Loader’s result of undecidability of ﬁnitary PCF [Loa01] proved that in fact one
could not hope for an eﬀective fully abstract model of this functional language. However, if
we add references to PCF, the eﬀective full abstraction result can be recovered for a version of
Idealized Algol [AM96].
1.3.2
The concepts of game semantics
Eliminating the “parallel or” from the model requires to distinguish between programs that would
evaluate their arguments in diﬀerent orders. This leads to the necessity to take into account
sequentiality in the model. For this reason, game semantics is based on the notion of play, which
10

is a sequence of moves representing an interaction between a program and its environments. The
programs are then interpreted as strategies, which are sets of plays.
The plays constituting a strategy have in fact a very special structure: they are walks in
the Böhm tree of the corresponding program, a Böhm tree being a representation of a normal
form.
The most technical part of game semantics is the handling of composition.
Indeed,
since a strategy can be seen as a normal form, composition of strategies amounts to compute a
normal form from two normal forms, and the combinatorics of beta-reduction is transposed into
composition.
A walk in a Böhm tree can be seen as an exploration of the tree by an environment (the
opponent), the tree being described step-by-step by the program (the player). Since the tree
is ﬁxed in advance by the syntax of the program, the description of it by the player must not
depend on the particular way the opponent explores it, but only on the particular branch that
opponent is exploring. This condition is what game semanticists call “innocence” of strategies.
Indeed, relaxing innocence provides a model of languages with references, and references can be
interpreted as the possibility for a Böhm tree to change during the computation.
The model obtained is still a little too general: player may observe for example that opponent
asks for a value but, instead of providing it, it may answer to a move appearing lower in the
branch under exploration. This behavior must be understood as a kind of exception (though
technically it is a syntactically scoped exception, like the call/cc operator of Scheme). Indeed,
if we see the current branch as the call stack, it amounts to exit the current function without
giving any answer, and go back lower in the stack. In order to forbid this, strategies of the fully
abstract model of PCF are required to be “well-bracketed”: a question must not be answered
until all intermediary ones have been answered.
Finally, considering only innocent, well-bracketed and compact strategies, one obtains the
deﬁnability result: any strategy is the image of a PCF program. Once one has the soundness
and deﬁnability results, a simple quotient by the observational equivalence in the model leads
to the full abstraction result.
1.3.3
The extensions of game semantics
In this thesis we are interested in providing a model of classical reasoning. As observed by Griﬃn,
the call/cc operator of Scheme can be typed with the law of Peirce. Under the lights of the
Curry-Howard isomorphism, this means that the computational content of classical reasoning
lies in the notion of control. As seen earlier, this syntactic control, which is not present in PCF,
was forbidden in the games model by requiring the strategies to be well-bracketed. Therefore, we
consider strategies which may not be well-bracketed, providing us with a model of PCF+call/cc
due to [Lai97].
On the other side, the innocence constraint can also be relaxed: a strategy may memorize
which branches are under exploration by opponent, and in which order. This indeed was proved
to give a model of a programming language with references (see [AHM98]). However, if we
completely forget about innocence, the product type A×B of arenas A and B contains strategies
which may beneﬁt from the possibility to exchange information between A and B, and therefore
such a strategy is not entirely deﬁned by its computations on A together with its computations
on B, and we do not have the uniqueness property of the pair of two strategies. In fact we get a
symmetric monoidal closed category and not a cartesian closed one. If we want a model of lambda
(and lambda-mu) calculus with βη-equivalence, we can not drop entirely the innocence condition.
However, it is known that if we only require the strategy to be blind to the particular computation
currently performed, then we get back the cartesian product. This condition is known as single
threadedness, and allows strategies to remember what has been asked by opponent and in which
11

order, while making sure that this knowledge is reset each time a new computation starts.
1.4
Outline
Using ideas from both Kreisel’s and Krivine’s realizability, we devise in this thesis two notions
of classical realizability in which realizers are typed programs. The logical system under con-
sideration is classical analysis CAω (as it is called in [Koh08]), that is Peano arithmetic in ﬁnite
types PAω augmented with the axiom of dependent choice. The weaker axiom of countable
choice can then be used to recover the axiom scheme of full comprehension over numbers, and
it was the main motivation for the study of CAω. Many theorems of analysis can be formalized
in this system (see e.g. [Koh08]). Another possibility for formalizing theorems of analysis is
to work in second-order logic, where comprehension is a feature of the logic itself, and this is
the path followed by Krivine [Kri09]. The main challenge into building a realizability model
of classical analysis is that we need to provide a computational interpretation for the axiom of
choice. Indeed, the axiom of choice is validated in every intuitionistic realizability model, but
when it comes to classical realizability it is known to raise some diﬃculties (see e.g. [BBC98]).
The usual route is to perform a negative translation from classical to intuitionistic logic, and
then to provide an intuitionistic realizer of the translation of the axiom of choice. This realizer
can for instance be obtained using a variant of Spector’s bar-recursion [Spe62] due to Berger
and Oliva [BO06] which realizes the double-negation shift. Indeed, a variant of Gödel’s negative
translation can turn a proof of PAω ⊢A into a proof of HAω ⊢A¬, where A¬ is inductively
deﬁned by adding a double negation in front of every atomic or existential formula (disjunction
can be coded). Then the formula DNS ⇒A ⇒A¬ is provable in intuitionistic logic, where
DNS ≡∀xι¬¬A ⇒¬¬∀xιA is the principle of double-negation-shift. Therefore a realizability
model for HAω can be turned into a realizability model of CAω by providing a realizer of DNS,
which is done using bar-recursion in [BO05, BBC98]. Here we prove that Berger and Oliva’s
bar-recursor, when used in our classical realizability model, directly realizes the axiom of choice.
We present in chapter 2 a version of Gentzen’s multi-conclusioned sequent calculus with
a distinguished conclusion.
Proofs in that calculus will then be interpreted as programs in
chapter 3. We deﬁne a relativization predicate which will be used in the realizability models to
realize the induction scheme and the axiom of choice. The relativized variables are the ones that
are manipulated by the realizers. Then we deﬁne the theories of Peano arithmetic in ﬁnite types
PAω and classical analysis in ﬁnite types CAω, by extending PAω with the axiom of dependent
choice. We also deﬁne their relativized versions PAωr and CAωr. Finally we give a quick reminder
of ﬁrst-order models.
In chapter 3 we present Parigot’s λµ-calculus [Par92], an extension of λ-calculus with control
primitives allowing for an interpretation of classical principles. We also present the translation of
proofs of CAωr into λµ-calculus. Then we describe Selinger’s categories of continuations [Sel01],
the interpretation of λµ-calculus in these, and the connection of this interpretation with the
continuation-passing-style translation of λµ-calculus. Then we present the programming lan-
guage µPCF as a particular λµ-theory. As an example, we encode Gödel’s system T extended
with control features of λµ-calculus into µPCF and describe the interpretation of PAωr in it.
Finally, we give an introduction to bar-recursion as a recursion operator on well-founded trees,
we deﬁne formally Berger and Oliva’s bar-recursion [BO06] in µPCF and we use it to give an
interpretation of CAωr in µPCF.
Chapter 4 is devoted to game semantics. We ﬁrst give a technical introduction to the con-
cepts of game semantics, then we present formally arenas, strategies, and their cartesian-closed
structure. We focus on single-threaded, non-innocent, unbracketed strategies (the unbracketed-
12

ness providing a model for control features). Finally, we recall that the category G of arenas and
strategies is a category of continuations and we describe the interpretation of µPCF in G.
In chapter 5 we deﬁne our ﬁrst realizability model. In this model, realizers of an implication
are deﬁned by a Kleene arrow in the category of continuations G (a realizer of A ⇒B is a
strategy which maps realizers of A to realizers of B). Then, in order to prove the adequacy
lemma, the set of realizers of a given formula is proved to be the orthogonal of a more primitive
set of counter-realizers. In Krivine’s realizability model [Kri09], the set of counter-realizers is
primitive and the set of realizers is then deﬁned to be its orthogonal. In particular, the realizers
of an implication are not Kleene realizers. However, their η-expansion are, and moreover, a
realizer of A ⇒B is a Kleene realizer if we take the reduction relation to be closed not only by
anti-evaluation, but also by direct evaluation. We prove the adequacy lemma for the axioms
of CAωr using bar-recursion for the axiom of dependent choice. The realization of the axiom
of choice uses two particular aspects of the HON games model: the fact that it contains all
(in particular non-computable) functions on natural numbers, and the fact that every function
on sequences expressible in it is continuous for the product topology on sequences. Finally, we
prove the extraction result for Π0
2 formulas derivable in CAω.
Our second realizability model is deﬁned in chapter 6. In this model, each formula has an
associated winning condition, and a strategy realizes a formula if its plays are winning for the
associated winning condition. We ﬁrst deﬁne positions as desequentialized plays, relying on
Boudes’ thick subtrees [Bou09]. Then we deﬁne winning conditions as sets of positions which
are closed under weakening of opponent and strengthening of player. We then deﬁne for each
formula of PAωr a winning condition on the associated arena, deﬁne the realizability relation
and prove the adequacy lemma for PAωr. We give ideas about the realization of the axiom of
choice in this setting by discussing about a bar recursor which would use references. Finally we
prove the extraction result: from every Π0
2 formula provable in PAω we can extract an algorithm
computing the function that it represents.
13

14

Chapter 2
Logic
We deﬁne in this chapter the logical system under which we will work through the thesis. First,
we deﬁne the general case of classical multisorted ﬁrst-order logic (handling classical reasoning
by the use of multi-conclusioned sequents), then we describe the case of logics with equality and
the relativization of proofs that we need for the realizability interpretation. We describe the
case of Peano arithmetic and its extension with the axiom of dependent choice, and ﬁnally we
recall some basic deﬁnitions about models of classical logic.
2.1
Classical multisorted ﬁrst-order logic
The logical framework that we use is multisorted ﬁrst-order logic, where the sorts are ﬁxed to
be the types of simply typed λ-calculus (see e.g. [TVD88]). We build from a set of base sorts ι
the set of sorts:
T, U
::=
ι | T →U
which are used for the individuals of the logic. We ﬁx a set of sorted individual constants (ranged
over by cT ) from which we build the set of individuals of the logic:
tT , uU
::=
cT | xT |
 tT→UuT U
We also ﬁx a set of sorted predicates (ranged over by P) from which we deﬁne the formulas of
the logic:
A, B
::=
P

tT1
1 , ..., tTn
n

| ⊥| A ⇒B | A ∧B | ∀xT A
Deﬁnition 2.1. A signature Σ is a set of base sorts together with a set of sorted constant
individuals and a set of sorted predicates.
The negation is deﬁned as ¬A ∆= A ⇒⊥. We choose to have only negative connectives,
since the interpretation of our logic in the category of games is based on a negative call-by-name
continuation-passing-style translation. We deﬁne the positive connectives using the negative
ones: A ∨B
∆= ¬ (¬A ∧¬B) and ∃xT A
∆= ¬∀xT ¬A. It is well-known that with this coding
of positive connectives with negative ones, a formula A is provable in our system if and only
if the formula obtained by replacing every P (t1, ..., tn) with ¬¬P (t1, ..., tn) in A is provable in
its intuitionistic restriction. Therefore, if in some axiomatic system one can intuitionistically
prove ¬¬P (t1, ..., tn) ⇒P (t1, ..., tn), then the system presented here becomes equivalent to its
intuitionistic version.
Deﬁnition 2.2. A theory on a given signature Σ is a set Ax of closed formulas (axioms) written
on the language deﬁned by Σ.
15

A context Γ or ∆is a ﬁnite unordered sequence of formulas and a sequent is of the form:
Γ ⊢A | ∆
Such a sequent should be interpreted as: the conjunction of the formulas of Γ implies the
disjunction of A and of the formulas of ∆. If ∆is empty we will simply write Γ ⊢A. The set
of derivable sequents of a given theory is deﬁned from Ax using the following rules:
Γ, A ⊢A | ∆
(A∈Ax)
Γ ⊢A | ∆
Γ ⊢A | ∆, A
Γ ⊢⊥| ∆, A
Γ ⊢⊥| ∆, A
Γ ⊢A | ∆
Γ, A ⊢B | ∆
Γ ⊢A ⇒B | ∆
Γ ⊢A ⇒B | ∆
Γ ⊢A | ∆
Γ ⊢B | ∆
Γ ⊢A | ∆
Γ ⊢B | ∆
Γ ⊢A ∧B | ∆
Γ ⊢A1 ∧A2 | ∆
Γ ⊢Ai | ∆
Γ ⊢A | ∆
(xT /∈FV(Γ,∆))
Γ ⊢∀xT A | ∆
Γ ⊢∀xT A | ∆
Γ ⊢A

tT /xT 	
| ∆
In this system one can for example derive in any theory the following sequents:
⊢⊥⇒A
⊢¬ (¬A) ⇒A
⊢((A ⇒B) ⇒A) ⇒A
The weakening rule:
Γ ⊢A | ∆
Γ⊆Γ′,∆⊆∆′
Γ′ ⊢A | ∆′
is admissible and will be used without mentioning it. The left contraction rule is derivable and
the right one is admissible:
Γ, A, A ⊢B | ∆
Γ, A ⊢A ⇒B | ∆
Γ, A ⊢A | ∆
Γ, A ⊢B | ∆
Γ ⊢A | B, B, ∆
B,B,∆⊆A,B,B,∆
Γ ⊢A | A, B, B, ∆
Γ ⊢⊥| A, B, B, ∆
Γ ⊢B | A, B, ∆
Γ ⊢⊥| A, B, ∆
Γ ⊢A | B, ∆
so we will also use them implicitly. If a sequent ⊢A is derivable in a theory Ax, then since Ax
may contain inﬁnitely many formulas we write to avoid confusion:
Ax |∼A
2.2
Equational theories
We say that a theory is an equational theory if it contains for each sort T an inequality predicate
̸=T between terms of sort T (for which we use inﬁx notation), and the following axioms for
reﬂexivity and the Leibniz scheme:
(refl)
∀x (x = x)
(Leib)
∀⃗z ∀x ∀y (¬A ⇒A {y/x} ⇒x ̸= y)
where t = u
∆= ¬ (t ̸= u). As in [Kri09], we use a primitive inequality rather than equality,
because inequality is a negative predicate as all other basic connectives of our logic, therefore
we can easily realize the Leibniz scheme. Moreover, since we are in classical logic, the set of
provable sequents is unchanged.
16

2.3
The relativization predicate
In the following we will consider two kinds of quantiﬁcations: the uniform and the relativized
ones. From the point of view of provability in the two theories PAω and CAω considered in
this thesis, this will not change anything. Indeed, we will deﬁne for PAω and CAω (which have
uniform quantiﬁcations only) their relativized versions PAωr and CAωr (which have both uniform
and relativized quantiﬁcations). Then we will describe a mapping from PAω (resp. CAω) to PAωr
(resp.CAωr), and we will perform our realizability interpretation in the relativized theories.
Relativization is a technique that was already used in Krivine’s models [Kri94, Miq11], and
its utility will appear more clearly in the realizability interpretation. A realizer of ∀x A (uniform
quantiﬁcation) must be a realizer of A {a/x} for every instance a of x, whereas a realizer of
∀rx A (relativized quantiﬁcation) must turn any instance a of x into a realizer of A {a/x}. The
uniform quantiﬁers suﬃce to realize the rules of ﬁrst-order logic and Leibniz equality, but when
it comes to Peano arithmetic, and more particularly to the axiom scheme of induction, the
recursor of Gödel’s system T needs to know on which particular natural number to recurse. A
simple and intuitive example arises when we use the induction scheme to perform case-analysis:
we prove on one hand A {0}, and on the other hand A {n} for n ̸= 0, so by case-analysis we
get ∀rx A (note the relativized quantiﬁcation). Then the corresponding program must read the
natural number, test if it is zero, and then branch to the corresponding program. One solution
would have been to relativize all quantiﬁers, however relativizing only when it is necessary gives
a better understanding of the realizability interpretation and simpler extracted programs.
We explain now how we map proofs of a theory into proofs of a relativized version of the
theory. Fix a theory Ax on a signature Σ. Let Σ′ be the signature obtained by adding to Σ a
predicate symbol for each base sort ι of Σ:
LtιM
We lift it to every sort with the following syntactic sugar:
LtT→UM ∆= ∀xT LxT M ⇒Lt xUM
First, since Σ ⊆Σ′, Ax can be considered as a theory on Σ′. We also deﬁne syntactic sugar for
relativized quantiﬁcations:
∀rxT A ∆= ∀xT  LxT M ⇒A

∃rxT A ∆= ¬∀rxT ¬A
and we deﬁne inductively the relativization Ar (on the signature Σ′) of a formula A (on the
signature Σ):
P

tT1
1 , ..., tTn
n
r ∆= P

tT1
1 , ..., tTn
n

⊥r ∆= ⊥
(A ⇒B)r ∆= Ar ⇒Br
(A ∧B)r ∆= Ar ∧Br
 ∀xT A
r ∆= ∀rxT Ar
This translation is extended to contexts the obvious way, so we write Γr and ∆r. The following
lemma then gives the mechanism of translation from a uniform system into a relativized one.
Lemma 2.1. If Axr is a theory on Σ′ such that:
• for any constant individual cT of Σ, Axr |∼LcT M
• for each A ∈Ax, Axr |∼Ar
• for each sort T of Σ, there is a closed term tT
17

then for any closed formula A on Σ:
Ax |∼A
=⇒
Axr |∼Ar
Proof. We translate inductively every proof in the theory Ax (on the signature Σ) of a sequent:
Γ ⊢A | ∆
to a proof in Axr (on the signature Σ′) of a sequent:
L⃗x
⃗T M, Γr ⊢Ar | ∆r
where FV (Γ, A, ∆) ⊆⃗x⃗T . The translation of a proof ⊢A in Ax for A a closed formula is then
a proof in Axr of a sequent L⃗x⃗T M ⊢Ar. Indeed, some dummy variables may appear during the
translation, without appearing free in A (this is due to our system not satisfying the subformula
property). In order to eliminate these, we use the last hypothesis: from L⃗x⃗T M ⊢Ar we can
derive ⊢∀r⃗x⃗T Ar, and then ⊢L⃗t⃗T M ⇒Ar where ⃗t⃗T are closed individuals obtained from the last
hypothesis. Finally, using lemma 2.2 below, we can combine the proof of ⊢L⃗t⃗T M⇒Ar with proofs
of L⃗t⃗T M and get a proof of ⊢Ar.
The translation of the axiom rule follows from the hypotheses of the lemma, the identity rule
is translated as follows, with ⃗x⃗T = FV (Γ, A, ∆):
Γ, A ⊢A | ∆
⇝
L⃗x⃗T M, Γr, Ar ⊢Ar | ∆r
The rules for logical connectives are translated trivially:
Γ ⊢A | ∆, A
Γ ⊢⊥| ∆, A
⇝
L⃗x⃗T M, Γr ⊢Ar | ∆r, Ar
L⃗x⃗T M, Γr ⊢⊥| ∆r, Ar
Γ ⊢⊥| ∆, A
Γ ⊢A | ∆
⇝
L⃗x⃗T M, Γr ⊢⊥| ∆r, Ar
L⃗x⃗T M, Γr ⊢Ar | ∆r
Γ, A ⊢B | ∆
Γ ⊢A ⇒B | ∆
⇝
L⃗x⃗T M, Γr, Ar ⊢Br | ∆r
L⃗x⃗T M, Γr ⊢Ar ⇒Br | ∆r
Γ ⊢A ⇒B | ∆
Γ ⊢A | ∆
Γ ⊢B | ∆
⇝
L⃗x⃗T M, Γr ⊢Ar ⇒Br | ∆r
L⃗x⃗T M, Γr ⊢Ar | ∆r
L⃗x⃗T M, Γr ⊢Br | ∆r
Γ ⊢A | ∆
Γ ⊢B | ∆
Γ ⊢A ∧B | ∆
⇝
L⃗x⃗T M, Γr ⊢Ar | ∆r
L⃗x⃗T M, Γr ⊢Br | ∆r
L⃗x⃗T M, Γr ⊢Ar ∧Br | ∆r
Γ ⊢A1 ∧A2 | ∆
Γ ⊢Ai | ∆
⇝
L⃗x⃗T M, Γr ⊢Ar1 ∧Ar2 | ∆r
L⃗x⃗T M, Γr ⊢Ari | ∆r
where in the cases of introduction of implication and conjunction, the two premises can get
the same relativized variables on the left by applying the (admissible) left weakening rule when
necessary. The translation of the introduction of universal quantiﬁcation is given by:
Γ ⊢A | ∆
(xT /∈FV(Γ,∆))
Γ ⊢∀xT A | ∆
⇝
LxT M, L⃗x⃗T M, Γr ⊢Ar | ∆r
L⃗x⃗T M, Γr ⊢LxT M ⇒Ar | ∆r

xT /∈FV

L⃗x⃗T M,Γr,∆r
L⃗x⃗T M, Γr ⊢∀xT  LxT M ⇒Ar
| ∆r
18

and preserves the fact that all the free variables of a sequent are relativized in the context.
Finally, in order to translate the elimination of the universal quantiﬁcation we must prove
that we can lift relativization to all constructs on individuals. This can be obtained using the
hypothesis of the lemma requiring that for every individual constant cT of Σ, Axr |∼LcT M.
Indeed, we have the following lemma:
Lemma 2.2. Suppose that for every individual constant cT of Σ, Axr |∼LcT M. Let tT be an
individual of the logic with free variables ⃗x⃗V . We have:
Axr |∼∀r⃗x
⃗V LtT M
Proof. We prove it by induction on tT . If tT is some cT , then this is an assumption of the lemma.
If tT is a variable xT , then ∀rxT LxT M ≡∀xT  LxT M ⇒LxT M

, which is trivially derivable, and if
tT is uU→T vU, then we get L⃗x⃗V M ⊢∀yU 
LyUM ⇒L(u y)T M

and L⃗x⃗V M ⊢LvUM from the induction
hypotheses, so we obtain L⃗x⃗V M ⊢L(u v)T M and ⊢∀r⃗x⃗V L(u v)T M.
Now we can describe the translation of the elimination rule of the universal quantiﬁer:
Γ ⊢∀xT A | ∆
Γ ⊢A

tT /xT 	
| ∆
⇝
L⃗x⃗T M, Γr ⊢∀rxT Ar | ∆r
L⃗x⃗T M, Γr ⊢LtT M ⇒A {t/x}r | ∆r
...
L⃗x⃗T M, Γr ⊢LtT M | ∆r
L⃗x⃗T M, Γr ⊢A {t/x}r | ∆r
where we can suppose without loss of generality that FV
 tT 
⊆⃗x⃗T (using the left weakening rule
if necessary), and L⃗x⃗T M, Γr ⊢LtT M | ∆r is easily derivable using ∀r⃗y⃗U LtT M (with ⃗y⃗U = FV
 tT 
⊆
⃗x⃗T ) from lemma 2.2.
2.4
Peano arithmetic in ﬁnite types: PAω
Peano arithmetic in ﬁnite types, PAω, is an extension of Peano arithmetic in which we can write
any term of Gödel’s system T and quantify over functions of any type.
The signature ΣPAω of Peano arithmetic in ﬁnite types contains a single base sort ι, the
following sorted constants:
s
of sort
(T →U →V ) →(T →U) →T →V
k
of sort
T →U →T
0
of sort
ι
S
of sort
ι →ι
rec
of sort
T →(ι →T →T) →ι →T
PAω is an equational theory as deﬁned above, and the only predicate symbol is inequality:
tT ̸= uT
Now we give the axioms of PAω. The ﬁrst two axioms are those of equational theories:
(refl)
∀xT (x =T x)
(Leib)
∀⃗z
⃗U ∀xT ∀yT (¬A ⇒A {y/x} ⇒x ̸=T y)
PAω also contains the deﬁnitional axioms of s, k and rec:
(∆s)
∀x ∀y ∀z s x y z = x z (y z)
(∆rec0)
∀x ∀y rec x y 0 = x
(∆k)
∀x ∀y k x y = x
(∆recS)
∀x ∀y ∀z rec x y (S z) = y z (rec x y z)
and ﬁnally the non-confusion axiom and the axiom scheme of induction for every formula A with
free variables among xι, ⃗y ⃗T :
(Snz)
∀x S x ̸= 0
(ind)
∀⃗y (A {0/x} ⇒∀x (A ⇒A {S x/x}) ⇒∀x A)
19

2.4.1
Relativized PAω: PAωr
We now deﬁne the relativized version PAωr of PAω, using lemma 2.1. First, ΣPAωr is ΣPAω
augmented with a predicate symbol LtιM.
The axioms of PAωr are those of PAω where the
induction scheme is replaced by:
(indr)
∀⃗y (A {0/x} ⇒∀rx (A ⇒A {S x/x}) ⇒∀rx A)
plus the axioms L0ιM and LSι→ιM. In order to use lemma 2.1 we need the following lemmas:
Lemma 2.3.
PAωr |∼LsM
PAωr |∼LkM
PAωr |∼LrecM
Proof. The formulas LsM and LkM are provable using the axioms of equality and respectively (∆s)
and (∆k). Indeed, LsM is the following formula:
∀xT→U→V  LxM ⇒∀yT→U (LyM ⇒∀zσ (LzM ⇒Ls x y zM))

which is equivalent in ﬁrst-order logic to:
∀xT→U→V ∀yT→U∀zT (LxM ⇒LyM ⇒LzM ⇒Ls x y zM)
using (∆s) and (Leib) this is equivalent to:
∀xT→U→V ∀yT→U∀zT (LxM ⇒LyM ⇒LzM ⇒Lx z (y z)M)
and since LxM and LyM are the following formulas:
∀xT
1
 Lx1M ⇒∀xU
2 (Lx2M ⇒Lx x1 x2M)

∀yT
1 (Ly1M ⇒Ly y1M)
we can instantiate these with x1 ≡z, x2 ≡y z and y1 ≡z to obtain a proof of LsM. The case of
LkM is similar.
Similarly, LrecM is provable using the axioms of equality, (∆rec0), (∆recS) and (indr). LrecM is
equivalent in ﬁrst-order logic to:
∀xT ∀yι→T→T  LxM ⇒∀ryι
1∀yT
2 (Ly2M ⇒Ly y1 y2M) ⇒∀rzιLrec x y zM

if we instantiate y2 with rec x y y1 it is suﬃcient to prove:
∀xT ∀yι→T→T (LxM ⇒∀ryι
1 (Lrec x y y1M ⇒Ly y1 (rec x y y1)M) ⇒∀rzιLrec x y zM)
which is equivalent, using (∆rec0), (∆recS) and (Leib), to:
∀xT ∀yι→T→T (Lrec x y 0M ⇒∀ryι
1 (Lrec x y y1M ⇒Lrec x y (S y1)M) ⇒∀rzιLrec x y zM)
which is an instance of (indr) with the formula Lrec x y zM
Lemma 2.4. For any A ∈PAω, PAωr |∼Ar.
Proof. For (∆s), (∆k), (∆rec0), (∆recS) and (Snz) it follows from the fact that the formula
∀x A ⇒∀rx A is derivable in ﬁrst-order logic. For (ind) it comes from this and the fact that the
following formula:
∀r⃗y (Ar {0/x} ⇒∀rx (Ar ⇒Ar {S x/x}) ⇒∀rx Ar)
is a consequence of (indr).
20

Lemma 2.5. For every sort T on ΣPAω, there is a closed individual tT .
Proof. By induction on T. If T = ι, then 0ι is such an individual. If T = U →V , then by
induction hypothesis there is some closed tV , and so kV →U→V tV is a closed individual of sort
T = U →V .
Using these three lemmas, it follows from lemma 2.1 that for any closed formula A on the
signature ΣPAω:
PAω |∼A
=⇒
PAωr |∼Ar
2.4.2
Relation to usual theories
We discuss in this section about the relation between our version of PAω, in which the inequality
predicate is atomic, and other intuitionistic or classical systems. First, in this section we will
consider the following systems:
PA=
HA=
PA
HA
PAω
=
HAω
=
PAω
HAω
where the P/H part corresponds to Peano and Heyting arithmetic, the latter being the intu-
itionistic version of the former, the ω part corresponds to the possibility of quantifying over
higher type variables, and the = part corresponds to having an atomic predicate for equality
while the absence of = indicates a primitive inequality. For the systems based on a primitive
equality we refer to [TVD88]. In the case of primitive equality, the Leibniz scheme is:
∀⃗z
⃗U∀xT ∀yT (x = y ⇒A ⇒A {y/x})
while in the case of primitive inequality we deﬁne it is as in section 2.2:
∀⃗z
⃗U∀xT ∀yT (¬A ⇒A {y/x} ⇒x ̸= y)
Similarly, in PA= and HA= the injectivity of successor is:
∀xι∀yι (S x = S y ⇒x = y)
while in PA and HA we deﬁne it to be the following formula, that we add also to the axioms of
HAω:
∀xι∀yι (x ̸= y ⇒S x ̸= S y)
It is well known that HA= |∼∀xι∀yι (¬¬x = y ⇒x = y), so in our system with only negative
connectives, HA= and PA= prove exactly the same formulas. It is quite easy to prove also that
HA |∼∀xι∀yι (¬¬x ̸= y ⇒x ̸= y), so HA and PA also prove exactly the same formulas (again,
in the particular case of our system with only negative connectives). On the other side, despite
the equivalence of provability between PA and HA, the associated proof terms are very diﬀerent.
Indeed, the proof of HA |∼∀xι∀yι (¬¬x ̸= y ⇒x ̸= y) is obtained by a double induction on x
and y, while in PA, a proof of PA |∼∀xι∀yι (¬¬x ̸= y ⇒x ̸= y) can also be obtained through
classical logic and its computational content relies then on the control features of languages
for classical logic (like λµ-calculus). This diﬀerence is reﬂected by the fact that, in the case of
relativized theories, PAωr is capable of proving ∀xι∀yι (¬¬x ̸= y ⇒x ̸= y) while HAωr can only
21

prove ∀rxι∀ryι (¬¬x ̸= y ⇒x ̸= y). When we switch to higher type equalities, things become
much more complicated. Indeed, HAω
= fails to prove ∀xT ∀yT (¬¬x = y ⇒x = y), since we cannot
perform induction on higher type variables, so the systems HAω
= and PAω
= are not equivalent
anymore, even at the level of provability. For classical systems PAω and PAω
=, a formula with
equalities and inequalities is provable in PAω (through the encoding (x = y) ≡(x ̸= y ⇒⊥))
if and only if it is provable in PAω
= (through the encoding (x ̸= y) ≡(x = y ⇒⊥)). However,
for intuitionistic systems HAω and HAω
= things are less clear since double negations cannot be
eliminated on atomic formulas. In particular, we did not investigate the relation between the
system HAω with primitive inequalities and HAω
= or PAω/PAω
=.
2.5
The axiom of choice
2.5.1
Classical analysis: CAω
Classical analysis CAω (in the sense of [Koh08]) is PAω augmented with the axiom scheme of
dependent choice:
(DC)
∀⃗u
⃗U  ∀xι ∀yT ∃zT A ⇒∃yι→T ∀xι A {y x/y, y (S x) /z}

where A is any formula on ΣCAω = ΣPAω with free variables among xι, yT , zT , ⃗u⃗U.
As proved in [Koh08], in PAω this particular form of dependent choice implies both countable
choice:
∀⃗u
⃗U  ∀xι ∃yT A ⇒∃zι→T ∀xι A {z x/y}

where A is any formula on ΣCAω = ΣPAω with free variables among xι, yT , ⃗u⃗U, and the more
usual version of dependent choice:
∀⃗u
⃗U  ∀xT ∃yT A ⇒∀vT ∃zι→T (z 0 = v ∧∀xι A {z x/x, z (S x) /y})

where A is any formula on ΣCAω = ΣPAω with free variables among xT , yT , ⃗u⃗U. Remark that if
we unfold the deﬁnitions of ∃zT and ∃yι→T , (DC) becomes:
∀⃗u (∀x ∀y ¬∀z ¬A ⇒¬∀y ¬∀x A {y x/y, y (S x) /z})
2.5.2
Relativized CAω: CAωr
As we did for PAω, we deﬁne the relativized version CAωr of CAω. First, the signature is the
same as ΣPAωr: it is ΣCAω = ΣPAω augmented with a predicate symbol LtιM. The axioms are
those of PAωr plus the following version of dependent choice, (DCr):
∀⃗u
⃗U∀rvT  ∀rxι∀ryT ¬∀zT ¬ (LzM∧Ar) ⇒¬∀yι→T ¬ (Ly 0M∧∀rxι (Ly (S x)M∧Ar {y x/y, y (S x) /z}))

where A is any formula on ΣCAω = ΣPAω with free variables among xι, yT , zT , ⃗u⃗U. In order to
use lemma 2.1, we need to prove that (DC)r is derivable in CAωr. Indeed, (DCr) is diﬀerent from
(DC)r which is:
∀r⃗u
⃗U∀rvT  ∀rxι ∀ryT ¬∀rzT ¬Ar ⇒¬∀ryι→T ¬∀rxι Ar {y x/y, y (S x) /z}

First, ∀rzT ¬Ar ≡∀zT (LzM ⇒Ar ⇒⊥) and ∀zT ¬ (LzM ∧Ar) ≡∀zT (LzM ∧Ar ⇒⊥), so since
(A ⇒B ⇒C) ⇔(A ∧B ⇒C) is a propositional tautology we get:
⊢∀rzT ¬Ar ⇔∀zT ¬ (LzM ∧Ar)
22

Moreover:
∀ryι→T ¬∀rxι Ar {y x/y, y (S x) /z}
≡∀yι→T

∀xι
LxM ⇒Ly xM

⇒∀xι
LxM ⇒Ar {y x/y, y (S x) /z}

⇒⊥

∀yι→T ¬

Ly 0M ∧∀rxι
Ly (S x)M ∧Ar {y x/y, y (S x) /z}

≡∀yι→T

Ly 0M ∧∀xι
LxM ⇒Ly (S x)M ∧Ar {y x/y, y (S x) /z}

⇒⊥

so since again (A ⇒B ⇒C) ⇔(A ∧B ⇒C) is a propositional tautology, ∀x (A ∧B) ⇔∀x A ∧
∀x B is provable in ﬁrst-order logic, and ∀rxι A ⇔A {0/x} ∧∀rxι A {S x/x} is provable using
induction (actually only case-analysis) we get:
(indr) ⊢∀ryι→T ¬∀rxι Ar {y x/y, y (S x) /z}
⇔∀yι→T ¬ (Ly 0M ∧∀rxι (Ly (S x)M ∧Ar {y x/y, y (S x) /z}))
so ﬁnally since ∀x A ⇒∀rx A is derivable in ﬁrst-order logic, (DC)r is derivable from (DCr) in
CAωr. Therefore we can apply lemma 2.1 to get for any A on the signature ΣCAω:
CAω |∼A
=⇒
CAωr |∼Ar
2.6
Models of classical multisorted ﬁrst-order logic
In this section we deﬁne what is a model of a given multisorted ﬁrst-order theory. We ﬁx a
signature Σ.
Deﬁnition 2.3. A Σ-structure M is given by:
• a set T M for each sort T of Σ
• an application function from (T →U)M × T M to UM
• an element cM ∈T M for each individual constant cT of Σ
• a set P M ⊆T1M × ... × TnM for each sorted predicate P

tT1
1 , ..., tTn
n

of Σ
Using the application function, we can extend the interpretation to individuals with param-
eters: if t is an individual of sort T with free variables ⃗x⃗U and if ⃗a are elements of ⃗UM, then
(t {⃗a/⃗x})M is an element of T M.
As usual in model theory, for any Σ-structure M and any closed formula A on Σ we can
deﬁne when M validates A, written M ⊨A. Now we ﬁx a theory Ax and we deﬁne what is a
model of Ax:
Deﬁnition 2.4. If M is a Σ-structure, then M is a model of Ax if for any A ∈Ax:
M ⊨A
23

In the particular case of equational theories we ﬁx the interpretation of the ̸=T predicate to
be the following set of pairs of elements of T M:
̸=T M
∆=

(a, b) ∈T M × T M  a ̸= b
	
and in the case of PAωr and CAωr we ﬁx the interpretation of the relativization predicate at the
unique base type ι, L.M, to be the following set of elements of ιM:
L.MM
∆=
n
0M; (S 0)M; (S (S 0))M; (S (S (S 0)))M; ...
o
24

Chapter 3
Typed languages
In this chapter, we deﬁne the programming language to which we map classical proofs, and deﬁne
the categorical model of this language. First, we describe λµ-calculus as a typed language and
give its equational theory under call-by-name semantics before deﬁning how we map classical
proofs in this language. Then we describe categories of continuations as a model of call-by-
name λµ-calculus, we explain how the coproduct completion of certain categories can be used to
provide a category of continuations, and we present the interpretation of λµ-calculus in categories
of continuations and its connection with that of its continuation-passing-style translation in the
underlying cartesian “almost” closed category. Finally, we describe the particular case of µPCF
which is the language in which we interpret Peano arithmetic and the axiom of choice trough
the bar-recursion operator, for which we give an intuitive meaning as a recursion operator for
well-founded trees.
3.1
λµ-calculus
The λµ-calculus is an extension of the λ-calculus introduced by Parigot in [Par92] in order to
represent and evaluate classical proofs. In λµ-calculus, there is another kind of variables along
standard λ-variables: the µ-variables. Just like the λ-variables are bound by the construct λx.M,
the µ-variables (which will be written α, β, ...) are bound by the new construct of λµ-calculus:
µα.M. The other new construct, [α] M should be understood as a mean for M to get arguments
which are passed to the enclosing µα.N.
To be more explicit, if N is a λµ-term with free variable x, then (µα.N {[α] M/x}) P reduces
to µα.N {[α] M P/x}. The argument P given to the term bound by µα gets passed over to M
because of the [α] in front of it. The [α] may also appear multiple times. If x and y are free
variables of N, then (µα.N {[α] M1/x, [α] M2/y}) P reduces to µα.N {[α] M1 P/x, [α] M2 P/y}.
The arguments received by a µα.... are passed over to all the subterms preceded by [α]. This
behavior may also be obtained by replacing µα.N {[α] M/x} with λ⃗z.N {M ⃗z/x}, the length of
⃗z being ﬁxed by the type of α. This is almost what happens when a λµ-term is CPS-translated
to a λ-term. The CPS translation also needs to introduce new types in order to translate the
base types to arrow types returning the empty type.
λµ-calculus provides a direct interpretation of classical proofs just like λ-calculus is an inter-
pretation of the intuitionistic ones. In intuitionistic sequent calculus, sequents are of the form
A1, ..., An ⊢A, and should be interpreted as the formula A1 ∧... ∧An ⇒A. The interpretation
of a proof of such a sequent in λ-calculus is then a λ-term with free variables k1, ..., kn, where
ki represents the possibility to use the hypothesis Ai in the proof. In classical sequent calcu-
lus, sequents are of the form A1, ..., An ⊢B1, ..., Bm, and should be interpreted as the formula
25

A1 ∧... ∧An ⇒B1 ∨... ∨Bm. The interpretation of a proof of such a sequent in λµ-calculus is
then a λµ-term with free λ-variables x1, ..., xn, and free µ-variables α1, ..., αm, xi representing
again the possibility to use the hypothesis Ai, but αi representing the possibility to produce
(part of) a proof of Bi. Under this view, a λµ-term may provide many proofs at the same time,
each proof being given step-by-step.
In the original version of [Par92], the terms were restricted to λ-abstractions, applications
and µα. [β] M for some term M. This syntax was extended in [dG94] by allowing terms µα.M
and [α] M. This extension of syntax was used in [Sau05] together with a well-chosen set of
reduction rules (this calculus being called Λµ-calculus) to recover the separation property that
failed in Parigot’s λµ-calculus, as shown in [DP01].
Later on in [AHS07], Parigot’s version
was related to minimal classical logic, and De Groote’s to full classical logic. Here we use the
version of [Sel01] (but without disjunction types), which is in the lines of [dG94, Sau05]. We
recommend [HS09] for an historical overview of the diﬀerent versions of λµ-calculus and how
they relate to each other.
3.1.1
Type system
The types of λµ-calculus are those of simply typed λ-calculus with a ﬁxed set of base types
(ranged over by ι) together with a product type and and a distinguished empty type 0:
T, U
::=
ι | T →U | T × U | 0
The empty type 0 is used to type terms [α] M.
Indeed, [α] M means that M will get the
arguments that are received by the enclosing µα. However, it cannot return anything, it is only
a consumer. The only way this kind of term returns something is when it appears in µα. [α] M
and α does not appear free in M. In that case it is equivalent to M.
In addition to the base types, λµ-calculus is parameterized with a ﬁxed set of typed constants:
Cst = {c : T, ...}. These two parameters form a signature of λµ-calculus:
Deﬁnition 3.1 (λµ signature). A λµ signature is a set of base types together with a set of typed
constants.
From a λµ signature, we will now deﬁne the derivable typing judgements, which are presented
as sequents of the form:
x1 : T1, ..., xn : Tn ⊢M : T | α1 : U1, ..., αm : Um
where the free λ-variables of M are among x1, ..., xn and its free µ-variables are among α1, ..., αm.
The x1 : T1, ..., αn : Tn part will be called the λ-context, and α1 : U1, ..., αm : Um the µ-context.
If the µ-context is empty, then we will simply write:
x1 : T1, ..., xn : Tn ⊢M : T
Once a signature of λµ-calculus is given, the derivable typing judgements are deﬁned as follows:
⃗x : ⃗T, x : T ⊢x : T | ⃗α : ⃗U
(c:T ∈Cst)
⃗x : ⃗T ⊢c : T | ⃗α : ⃗U
⃗x : ⃗T, x : T ⊢M : U | ⃗α : ⃗U
⃗x : ⃗T ⊢λx.M : T →U | ⃗α : ⃗U
⃗x : ⃗T ⊢M : T →U | ⃗α : ⃗U
⃗x : ⃗T ⊢N : T | ⃗α : ⃗U
⃗x : ⃗T ⊢MN : U | ⃗α : ⃗U
⃗x : ⃗T ⊢M : T | ⃗α : ⃗U
⃗x : ⃗T ⊢N : U | ⃗α : ⃗U
⃗x : ⃗T ⊢⟨M, N⟩: T × U | ⃗α : ⃗U
⃗x : ⃗T ⊢M : T1 × T2 | ⃗α : ⃗U
⃗x : ⃗T ⊢pi M : Ti | ⃗α : ⃗U
26

⃗x : ⃗T ⊢M : U | ⃗α : ⃗U, α : U
⃗x : ⃗T ⊢[α]M : 0 | ⃗α : ⃗U, α : U
⃗x : ⃗T ⊢M : 0 | ⃗α : ⃗U, α : U
⃗x : ⃗T ⊢µα.M : U | ⃗α : ⃗U
The left and right weakening rules are admissible in that type system, and we use them without
explicitly mentioning it. Here are some examples of derivable typing judgements.
⊢λx.µα.x : 0 →T
⊢λy.µα.y (λx.[α] x) : ((T →0) →0) →T
⊢λy.µα.[α] y (λx.µβ.[α] x) : ((T →U) →T) →T
The logical counterparts of these terms are respectively the ex falso formula ⊥⇒A, the double-
negation elimination ((A ⇒⊥) ⇒⊥) ⇒A and the law of Peirce ((A ⇒B) ⇒A) ⇒A, these
last two being classical principles. Remark that in the ﬁrst judgement we write µα.x, and in
the second example we write λx.[α] x. This is a consequence of using the extended syntax of
[dG94, Sau05], where the empty type is handled as any other. The third term will be denoted
cc, since it corresponds to the call/cc operator of the Scheme programming language which,
as observed by Griﬃn in [Gri90], can be typed with the law of Peirce.
3.1.2
λµ theories
We follow [Sel01] and deﬁne λµ-calculus as an equational theory. We only consider here the case
of call-by-name semantics. The axioms of the call-by-name λµ-calculus are the following:
(β→)
(λx.M) N = M {N/x}
(β×)
pi ⟨M1, M2⟩= Mi
(η→)
λx.M x = M
(x /∈FV (M))
(η×)
⟨p1 M, p2 M⟩= M
(ζ→)
(µα.M) N = µα.M {[α]
N/ [α]
}
(ζ×)
pi (µα.M) = µα.M {[α] pi
/ [α]
}
(β0)
[α] µβ.M = M {α/β}
(η0)
µα. [α] M = M
(α /∈FV (M))
(ζ0)
µα.M = M {
/ [α]
}
(α : 0)
where in each equation the two terms are typed with the same type. From these axioms we
deﬁne the notion of λµ theory:
Deﬁnition 3.2 (λµ theory). A λµ theory is a set T of equations between typed terms of the same
type (with free variables of the same type) which contains the axioms of call-by-name λµ-calculus
and is a congruence (contextually-closed equivalence relation).
We use here a slightly diﬀerent set of axioms from that of Selinger [Sel01]. Our β0 and η0
equations are the βµ and ηµ equations of Selinger, and we replace his β⊥(which is [α] M = M
if M : 0) with our ζ0. However, the two systems are equivalent:
Lemma 3.1. Under the contextual closures of β0 and η0 (the βµ and ηµ of Selinger), the equation
β⊥of Selinger is equivalent to ζ0.
Proof. Suppose β⊥holds, let M : 0 and α be a µ-variable of type 0. Using the contextual closure
of β⊥, we have M = M {
/ [α]
}, and again by β⊥we get M = [α] M {
/ [α]
}. Then
by contextual closure µα.M = µα. [α] M {
/ [α]
}, which is equal to M {
/ [α]
} using η0,
since α does not appear free in M {
/ [α]
}.
Conversely, suppose ζ0 holds, let M : 0, α be a µ-variable of type 0 and β be a µ-variable of
type 0 which does not appear free in M. Using ζ0 we have M = µβ.M, so by contextual closure
[α] M = [α] µβ.M, so by β0 we get [α] M = M {α/β}, but M {α/β} = M since β does not
appear free in M, and we get ﬁnally [α] M = M.
27

Replacing Selinger’s β⊥allows us to have a more symmetric calculus, with a set of three
equations for each type constructor, the ζ equations representing the transmission of the context
of a µα.M to the subterms [α] N.
3.1.3
Interpreting classical logic in λµ-calculus
In this section we ﬁrst interpret every formula A as a type A∗of λµ-calculus, and then every
proof of a sequent Γ ⊢A | ∆as a typing derivation of a term Γ∗⊢M : A∗| ∆∗.
This
interpretation is the ﬁrst component of our realizability interpretation of classical logic.
Fix a ﬁrst-order signature Σ and a λµ signature which has as set of base types the set of
base sorts of Σ (so any sort on Σ is a type on the λµ signature). Fix also for each predicate P of
Σ a type P ∗of λµ-calculus. We extend this interpretation to every formula on Σ the following
way:
⊥∗= 0
(A ⇒B)∗= A∗→B∗
(A ∧B)∗= A∗× B∗
 ∀xT A
∗= A∗
This interpretation is then extended to contexts: Γ∗(resp. ∆∗) is a context of λ-variables (resp.
µ-variables) with types B∗for B in Γ (resp. ∆). We now describe how to interpret proofs in
a ﬁrst-order theory Ax as typing derivations in λµ-calculus. Choose for each A ∈Ax a closed
λµ-term MA of type A∗which is a parameter of the interpretation. We interpret each ﬁrst-order
proof as a typing derivation in λµ-calculus the following way:

Γ, A ⊢A | ∆
∗
= Γ∗, x : A∗⊢x : A∗| ∆∗

(A∈Ax)
Γ ⊢A | ∆
∗
= Γ∗⊢MA : A∗| ∆∗

Γ, A ⊢B | ∆
Γ ⊢A ⇒B | ∆
∗
=
Γ∗, x : A∗⊢M : B∗| ∆∗
Γ∗⊢λx.M : A∗→B∗| ∆∗

Γ ⊢A ⇒B | ∆
Γ ⊢A | ∆
Γ ⊢B | ∆
∗
= Γ∗⊢M : A∗→B∗| ∆∗
Γ∗⊢N : A∗| ∆∗
Γ∗⊢MN : B∗| ∆∗

Γ ⊢A | ∆
Γ ⊢B | ∆
Γ ⊢A ∧B | ∆
∗
= Γ∗⊢M : A∗| ∆∗
Γ∗⊢N : B∗| ∆∗
Γ∗⊢⟨M, N⟩: A∗× B∗| ∆∗

Γ ⊢A1 ∧A2 | ∆
Γ ⊢Ai | ∆
∗
= Γ∗⊢M : A∗
1 × A∗
2 | ∆∗
Γ∗⊢pi M : A∗
i | ∆∗
 
Γ ⊢A | ∆
(xT /∈FV(Γ,∆))
Γ ⊢∀xT A | ∆
!∗
= Γ∗⊢M : A∗| ∆∗
 
Γ ⊢∀xT A | ∆
Γ ⊢A

tT /xT 	
| ∆
!∗
= Γ∗⊢M : A∗| ∆∗

Γ ⊢A | ∆, A
Γ ⊢⊥| ∆, A
∗
=
Γ∗⊢M : A∗| ∆∗, α : A∗
Γ∗⊢[α]M : 0 | ∆∗, α : A∗

Γ ⊢⊥| ∆, A
Γ ⊢A | ∆
∗
= Γ∗⊢M : 0 | ∆∗, α : A∗
Γ∗⊢µα.M : A∗| ∆∗
28

In the particular case of equational theories, we ﬁx:
̸=T ∗
∆=
0
M(refl)
∆= λx.x : 0 →0
M(Leib)
∆= λx.x : (A∗→0) →A∗→0
and in the case of a relativized theory we ﬁx:
L.ιM∗
∆=
ι
for each base sort ι of Σ. Since LtT→UM is deﬁned as ∀xT 
LxT M ⇒L(t x)UM

, we have by induction
on T:
L.T M
∗∆= T
3.2
Categories of continuations
Categories of continuations are to call-by-name λµ-calculus what cartesian closed categories are
to λ-calculus, in the sense that if we ﬁx a signature, there is a one-to-one correspondence between
λµ theories and categories of continuations together with an interpretation of the signature.
In [Ong96], Ong deﬁnes λµ categories by reformulating the syntax of λµ-calculus in categorical
terms. Later on, Hofmann and Streicher proved the soundness and completeness of categories of
continuations with respect to λµ-calculus, providing the ﬁrst abstract version of λµ-categories.
Later on, Selinger axiomatized these in [Sel01] under the name of control categories, proving
that they are equivalent to categories of continuations, and therefore sound and complete with
respect to call-by-name λµ-calculus. Moreover, he proved that the categorical dual of control
categories are sound and complete with respect to call-by-value λµ-calculus. Here we are only
interested in the call-by-name version, and we use the model of categories of continuations.
Deﬁnition 3.3 (Category of continuations). Let C be a distributive category, that is, a category
with ﬁnite products and coproducts such that the canonical distributivity morphisms from A ×
B + A × C to A × (B + C) are isomorphisms, and let R ∈Ob (C) be a ﬁxed object such that all
exponentials RA for A ∈Ob (C) exist. Then the full subcategory RC of C consisting of the objects
RA for A ∈C is called a category of continuations.
As observed in [LRS93], a category of continuations RC is in particular a cartesian closed
category:
Lemma 3.2. If RC is a category of continuations, then RC is cartesian, and if A ∈Ob (C)
and RB ∈Ob
 RC
, then RA×B deﬁnes an exponential in C of RB by A. Consequently, RC is
cartesian closed, the exponential of RB by RA being RRA×B.
Proof. We deﬁne the terminal object as R0 ∈Ob
 RC
, where 0 is the initial object of C, the
product of two objects RA, RB ∈Ob
 RC
as RA+B ∈Ob
 RC
, where + is the coproduct of C.
The cartesian structure comes easily from the facts that the terminal object of RC is isomorphic
to the terminal object of C, the product of two objects in RC is isomorphic to their product in
C, and the universal properties translate from C to RC since it is a full subcategory.
Let now A ∈Ob (C) and RB ∈Ob
 RC
. We deﬁne an evaluation morphism from RA×B × A
to RB by currying (in C):
 RA×B × A

× B
≃
/ RA×B × (A × B)
ev / R
29

and the currying of φ : C × A →RB is obtained by currying (again in C):
C × (A × B)
≃
/ (C × A) × B
φ×IdB
/ RB × B
ev / R
the above evaluation and currying in C being correct since C has all exponentials RD.
Take care that we have two (isomorphic) terminal objects, one in C and one in RC, and two
(isomorphic) products of RA and RB, again one in C and one in RC. To avoid confusion and
without loss of generality we will suppose that they are equal: R0 = 1 and RA+B = RA × RB.
For the same reason, we also suppose R = R1.
3.2.1
Classical disjunction in categories of continuations.
We could have added a primitive connective ∨for the disjunction in the logic, with the following
rules:
Γ ⊢A ∨B | ∆, A, B
Γ ⊢⊥| ∆, A, B
Γ ⊢⊥| ∆, A, B
Γ ⊢A ∨B | ∆
and then interpret these logical rules by adding the following typing rules to λµ-calculus:
⃗x : ⃗T ⊢M : U1 ` U2 | ⃗α : ⃗U, β1 : U1, β2 : U2
⃗x : ⃗T ⊢[β1, β2] M : 0 | ⃗α : ⃗U, β1 : U1, β2 : U2
⃗x : ⃗T ⊢M : 0 | ⃗α : ⃗U, β1 : U1, β2 : U2
⃗x : ⃗T ⊢µ (β1, β2) .M : U1 ` U2 | ⃗α : ⃗U
These rules are present in [Sel01], however we choose here to keep things simple and stick to
the usual λµ-calculus without disjunction types. Nevertheless we still use the binoidal functor
` in categories of continuations to interpret multi-conclusioned sequents. Following [Sel01] we
write: RA ` RB ∆= RA×B for the interpretation of the classical disjunction between RA and RB,
i : R →RA for the interpretation of the right weakening rule and ∇: RA ` RA →RA for the
interpretation of the right contraction rule.
Another interesting fact about categories of continuations is that we can deﬁne a functor
from Cop to RC which maps A ∈Ob (C) to RA ∈Ob
 RC
, and φ : A →B to Rφ : RB →RA
which is the currying of:
RB × A
IdRB ×φ
/ RB × B
ev / R
The morphism i : R →RA corresponds to R(∗A) (where ∗A is the unique morphism from A to
the terminal object 1) and the morphism ∇: RA ` RA →RA corresponds to Rpair(IdA,IdA)
(remember that RA ` RA = RA×A). Also, in particular, if A ≃B in C, then RA ≃RB in
RC (and in C). Through this functor, the cocartesian structure of C translates to the cartesian
structure of RC.
3.2.2
Coproduct completion of a category
We give here an example of a category of continuations. We start from a cartesian closed category
with countable products D, we apply a countable coproduct completion (see e.g. [AM97]) to get
a countably distributive category C = Fam (D), and we prove that for a certain class of objects
R ∈Ob (C), the categories RC are categories of continuations. We perform the construction in
the countable case because it will be useful in the next chapter, however the same results can be
obtained in the ﬁnite case: if D is cartesian closed and if we apply a ﬁnite coproduct completion
and choose well R in this completion we also obtain a category of continuations.
30

We ﬁx now a cartesian closed category with countable products D. The objects of Fam (D)
are countable families of objects of D:
{Ai | i ∈I}
where I is any countable set. Then a morphism from {Ai | i ∈I} to {Bj | j ∈J} is given by a
function f : I →J together with a family of morphisms φi : Ai →Bf(i). The composition of
two morphisms:
 f : I →J,

φi : Ai →Bf(i)
 i ∈I
	
: {Ai | i ∈I} →{Bj | j ∈J}
 g : J →K,

ψj : Bj →Cg(j)
 j ∈J
	
: {Bj | j ∈J} →{Ck | k ∈K}
is the following morphism:
 g ◦f : I →K,

φi; ψf(i) : Ai →Cg◦f(i)
 i ∈I
	
: {Ai | i ∈I} →{Ck | k ∈K}
Associativity of composition is easy to prove. The identity Id{Ai | i∈I} is deﬁned as follows:
(IdI : I →I, {IdAi : Ai →Ai | i ∈I}) : {Ai | i ∈I} →{Ai | i ∈I}
Neutrality of the identity is also easy. As expected, the coproduct completion of a category D
has all countable coproducts:
Lemma 3.3. Fam (D) has all countable coproducts
Proof. Let ({Ai,j | j ∈Ji})i∈I be a countable family of objects of Fam (D). We deﬁne the co-
product of this family as:
X
i∈I
{Ai,j | j ∈Ji} =
(
Ai,j
 (i, j) ∈
X
i∈I
Ji
)
which is an object of Fam (D), since the disjoint sum of the Ji is countable if I and the Ji are
countable. The morphism ini0 for i0 ∈I is deﬁned as:
ini0 =



Ji0 −→
X
i∈I
Ji
j 7−→(i0, j)
,
n
IdAi0,j : Ai0,j →Ai0,j
 j ∈Ji0
o



: {Ai0,j | j ∈Ji0} →
X
i∈I
{Ai,j | j ∈Ji}
The sum of an I-indexed family of morphisms from {Ai,j | j ∈Ji} to {Bk | k ∈K}:
 fi : Ji −→K,

φi,j : Ai,j →Bfi(j)
 j ∈Ji
	
i∈I
is given by:



X
i∈I
Ji −→K
(i, j) 7−→fi (j)
,
(
φi,j : Ai,j →Bfi(j)
 (i, j) ∈
X
i∈I
Ji
)


:
X
i∈I
{Ai,j | j ∈Ji} →{Bk | k ∈K}
The universal property is straightforward from the deﬁnitions. The empty sum and initial object
0 is just the empty family {}.
31

The coproducts completion of a category with countable products has ﬁnite products:
Lemma 3.4. If D has countable products, then Fam (D) has ﬁnite products
Proof. The proof is obtained by distributing the categorical product on the index sets, relying
on the fact that the ﬁnite product of countable families is still a countable family. The product
of a ﬁnite (possibly empty with n = 0) family of objects:
({Ai,j | j ∈Ji})1≤i≤n
is deﬁned as:
Y
1≤i≤n
{Ai,j | j ∈Ji} =



Y
1≤i≤n
Ai,ji

(ji)1≤i≤n ∈
Y
1≤i≤n
Ji



which is an object of Fam (D), since the ﬁnite product of the Ji is countable if the Ji are
countable. The morphism proji0 for i0 ∈I is deﬁned as:
proji0 =



Y
1≤i≤n
Ji −→Ji0
(ji)1≤i≤n 7−→ji0
,


proji0 :
Y
1≤i≤n
Ai,ji →Ai0,ji0

(ji)1≤i≤n ∈
Y
1≤i≤n
Ji






:
Y
1≤i≤n
{Ai,j | j ∈Ji} →{Ai0,j | j ∈Ji0}
The product of a J-indexed family of morphisms from {Ai | 1 ≤i ≤n} to {Bj,k | k ∈Kj}:

fj : I −→Kj,
n
φi,j : Ai →Bj,fj(i)
 1 ≤i ≤n
o
j∈J
is given by:



{1; ...; n} −→
Y
j∈J
Kj
i 7−→(fj (i))j∈J
,


(φi,j)j∈J : Ai →
Y
j∈J
Bj,fj(i)

1 ≤i ≤n






: {Ai | 1 ≤i ≤n} →
Y
j∈J
{Bj,k | k ∈Kj}
Here again, the universal property is straightforward from the deﬁnitions and the universal
property in D. It is easy to see that the empty product and terminal object of Fam (D) is the
singleton family {1}.
The product and coproduct structures of Fam (D) make it a countably distributive category:
Lemma 3.5. If D has binary products, then Fam (D) is a countably distributive category
Proof. Let {Ai | i ∈I} be an object of Fam (D) and ({Bj,k | k ∈Kj})j∈J be a J-indexed family
of objects of Fam (D). On one hand we have:
X
j∈J
({Ai | i ∈I} × {Bj,k | k ∈Kj}) =
X
j∈J
{Ai × Bj,k | (i, k) ∈I × Kj}
=


Ai × Bj,k

(j, i, k) ∈
X
j∈J
(I × Kj)



32

=


Ai × Bj,k

(i, j, k) ∈I ×
X
j∈J
Kj



and on the other hand:
{Ai | i ∈I} ×
X
j∈J
{Bj,k | k ∈Kj} = {Ai | i ∈I} ×


Bj,k

(j, k) ∈
X
j∈J
Kj



=


Ai × Bj,k

(i, j, k) ∈I ×
X
j∈J
Kj



Therefore, the two objects are the same and the identity is an isomorphism between the two.
The following property gives a class of objects R ∈Fam (D) such that RFam(D) is a category
of continuations.
Lemma 3.6. If D is cartesian closed and has countable products, then Fam (D) has all expo-
nential of singleton families
Proof. The exponential of {A} by {Bi | i ∈I} is deﬁned as:
{A}{Bi | i∈I} =
(Y
i∈I
ABi
)
A morphism from {Cj | j ∈J} × {Bi | i ∈I} to {A} is just given by a family of morphisms:
φ = {φi,j : Cj × Bi →A| (i, j) ∈I × J} : {Cj | j ∈J} × {Bi | i ∈I} →{A}
since there is only one function to the singleton set. Then the currying of such a morphism is
given by:
Λ (φ) =
(
(Λ (φi,j))i∈I : Cj →
Y
i∈I
ABi
 j ∈J
)
: {Cj | j ∈J} →{A}{Bi | i∈I}
since again the codomain is a singleton family. Finally, the evaluation morphism ev is deﬁned
as:
ev =
(
proji0 × IdBi0

; ev :
Y
i∈I
ABi × Bi0 →A
 i0 ∈I
)
: {A}{Bi | i∈I} × {Bi | i ∈I} →{A}
The universal property is then an easy consequence of the universal property in D and the
deﬁnitions.
From this lemma it follows that for any A ∈Ob (D), the category {A}Fam(D) is a category
of continuations.
33

3.2.3
Interpretation of λµ-calculus in categories of continuations
We describe here the interpretation of call-by-name λµ-calculus in a category of continuations
as deﬁned in [Sel01]. We ﬁx a signature of λµ-calculus and a category of continuations RC. To
each type T of λµ-calculus we associate an object JTK ∈Ob (C) of continuations of type T, and
an object [T] = RJTK ∈Ob
 RC
of computations of type T. The objects JιK ∈Ob (C) where ι is
a base type of the signature are parameters of the interpretation, and we deﬁne inductively:
JT →UK = [T] × JUK ∈Ob (C)
JT × UK = JTK + JUK ∈Ob (C)
J0K = 1
[T] = RJTK ∈Ob
 RC
We have in particular [T × U] = RJTK+JUK = RJTK × RJUK = [T] × [U] where × is the cartesian
product in RC deﬁned above, [T →U] = R[T]×JUK = RRJT K×JUK =
 RJUKRJT K
= [U][T], using the
deﬁnition of the exponential in RC given above, and [0] = R1 = R.
Once we have an interpretation of types in RC, we deﬁne the interpretation of typed λµ-terms
such that a term:
x1 : T1, ..., xn : Tn ⊢M : V | α1 : U1, ..., αm : Um
is interpreted as a morphism in RC:
[x1 : T1, ..., xn : Tn ⊢M : V | α1 : U1, ..., αm : Um] : [T1] × ... × [Tn] →[V ] ` [U1] ` ... ` [Um]
where [T1] × ... × [Tn] associates to the left and [V ] ` [U1] ` ... ` [Um] associates to the right. In
order to do that, we suppose given for each constant c : T ∈Cst of the signature a morphism in
RC:
[c : T] : 1 →[T]
which is again a parameter of the interpretation.
These parameters are summarized in the
following deﬁnition:
Deﬁnition 3.4 (Interpretation). Given a signature and a category of continuations RC, an
interpretation of λµ-calculus is given by an object JιK ∈C for each base type ι of the signature
and a morphism [c : T] : 1 →[T] in RC for each constant c : T ∈Cst of the signature.
We now have all necessary material to interpret every typed λµ-term as a morphism in RC.
The interpretation of typed λµ-terms is almost identical to the interpretation of λ-calculus in a
cartesian closed category (since as shown in the previous section, RC is cartesian closed). The
ﬁrst diﬀerence is that we must be able to carry over the µ-context, so we want to build from
φ : [T] →[U] a morphism φ`[V ] : [T]`[V ] →[U]`[V ]. The second diﬀerence is that in order to
interpret the introduction rules for µα.M and [α] M, we also need to have canonical morphisms
from [0]`
h
⃗U
i
`[T] to [T]`
h
⃗U
i
and from [T]`
h
⃗U
i
`[T] to [0]`
h
⃗U
i
`[T]. These requirements
are axiomatized in [Sel01], to which we refer for the full deﬁnition of the interpretation, and the
proof that the axioms of call-by-name λµ-calculus are sound under this interpretation.
Since call-by-name λµ-calculus is the internal language of categories of continuations (as
shown in [Sel01]), we can apply λµ-calculus constructs on morphisms of RC through the use
of λµ-terms with parameters in RC. For example, if φ : Q
j∈J RAj →RB `
 ˙
k∈K RDk
and
ψ : Q
j∈J RAj →RRB×C `
 ˙
k∈K RDk
, then ψ φ : Q
j∈J RAj →RC `
 ˙
k∈K RDk
, where
formally ψ φ is the term with parameters (y x) {φ/x, ψ/y}.
34

3.2.4
Connection with the call-by-name CPS translation of λµ-calculus
Another interesting thing about the interpretation of λµ-calculus into a category of continuations
is the exact correspondence with the interpretation of its call-by-name CPS translation in the
underlying cartesian “R-closed” category, as stressed in [HS02]. The target of such a translation
is a simply-typed λ-calculus λR×+ with product and sum types, a particular base type R, and
the function types being restricted to T →R. This particular λ-calculus can be interpreted in
C by interpreting the product type as the product in C, the sum type as the coproduct, and
using the fact that function types are of the form T →R, so having all exponentials RA in C is
enough.
To be more precise, λR×+ has one base type ˜ι for each base type ι of λµ-calculus and another
particular base type R. From these we build the types:
T, U
::=
eι | T →R | T × U | 1 | T + U
The arrow types are syntactically restricted to be of the form T →R. We map every type T of
λµ-calculus to a type eT of λR×+ (each base type ι being obviously mapped to eι) as follows:
^
T →U =
 eT →R

× eU
^
T × U = eT + eU
e0 = 1
λR×+ also has one constant ec : eT →R for each constant c : T of the source language. We also
suppose given for each λ-variable x : T of the source language a variable ex : eT →R in the target
language, and for each µ-variable α : U of the source language a variable eα : eU in the target
language. A typed λµ-term:
x1 : T1, ..., xn : Tn ⊢M : T | α1 : U1, ..., αm : Um
will then be translated to a typed λ-term:
f
x1 : f
T1 →R, ..., f
xn : f
Tn →R, f
α1 : f
U1, ..., f
αm : f
Um ⊢f
M : eT →R
(3.1)
Before deﬁning the translation, we give the typing rules of λR×+:
⃗k : ⃗T, k : T ⊢k : T
(ec:
eT →R

∈Cst)
⃗k : ⃗T ⊢ec :
 eT →R

⃗k : ⃗T, k : T ⊢M : R
⃗k : ⃗T ⊢λk.M : T →R
⃗k : ⃗T ⊢M : T →R
⃗k : ⃗T ⊢N : T
⃗k : ⃗T ⊢MN : R
Γ ⊢∗: 1
⃗k : ⃗T ⊢M : T
⃗k : ⃗T ⊢N : U
⃗k : ⃗T ⊢⟨M, N⟩: T × U
⃗k : ⃗T ⊢M : T1 × T2
⃗k : ⃗T ⊢pi M : Ti
Γ ⊢M : Ti
Γ ⊢ini M : T1 + T2
Γ ⊢M : T1 + T2
Γ, k : T1 ⊢N1 : U
Γ, k : T2 ⊢N2 : U
Γ ⊢case M {in1 k 7→N1 | in2 k 7→N2} : U
The typing rules for the arrow type are restricted to the case T →R. The translation of a
variable x or a constant c is ex or ec as deﬁned above, and the remaining part is as follows:
^
λx.M = λk.
 λex.f
M

(π1 k) (π2 k)
]
M N = λk.f
M

 eN, k

^
⟨M, N⟩= λk.case k

in1 l 7→f
M l | in2 l 7→eN l
	
]
pi M = λk.f
M ini k
^
µα.M = λeα.f
M ∗
^
[α] M = λk.f
M eα
where k is always a fresh variable. We deﬁne the interpretation of λR×+ in C by ﬁrst giving an
object JTK of C for each type T:
JeιK = JιK
JT →RK = RJTK
JT × UK = JTK × JUK
J1K = 1
JT + UK = JTK + JUK
35

The notation J
K may seem misleading, but it is on purpose, since one can easily see that if T
is a type of λµ-calculus, then
qeT
y
= JTK. Since the only function types of our λ-calculus are of
the form T →R and since C has all exponentials RA, we can interpret f
M in C the same way we
would interpret simply typed λ-calculus in a cartesian closed category. The term f
M of (3.1) is
interpreted as:
f
M

: R
rf
T1
z
× ... × R
rf
Tn
z
×
qf
U1
y
× ... ×
q f
Um
y
→R
reT
z
which is, by the above observation that
qeT
y
= JTK:
f
M

: RJT1K × ... × RJTnK × JU1K × ... × JUmK →RJTK
Now, by currying we obtain:
Λ
 f
M

: RJT1K × ... × RJTnK →RJTK×JU1K×...×JUmK
and we have the following result:
Λ
 f
M

= [M]
where the brackets on the left represent the interpretation of λR×+ in the cartesian “R-closed”
category C, and the brackets on the right represent the interpretation of λµ-calculus in the
category of continuations RC.
On the equational side, λR×+ has the following set of equations, where the two terms are of
the same type:

βλ
→

(λk.M) N = M {N/k}

ηλ
→

λk.M k = M
(k /∈FV (M))

βλ
×

pi ⟨M1, M2⟩= Mi

ηλ
×

⟨p1 M, p2 M⟩= M

βλ
+

case (ini M)
(
in1 k 7→N1
in2 k 7→N2
)
= Ni {M/k}

ηλ
+

case M
(
in1 k 7→in1 k
in2 k 7→in2 k
)
= M

ηλ
1

∗= M
Since these equations are typed, M is of type 1 in
 ηλ
1

. If M and N are λµ-terms of the same
type, then M = N holds using the equations (β→), (η→), (ζ→), (β×), (η×), (ζ×), (β0), (η0) and
(ζ0) of section 3.1.2 if and only if f
M = eN holds using the equations
 βλ
→

,
 ηλ
→

,
 βλ
×

,
 ηλ
×

,
 βλ
+

,
 ηλ
+

and
 ηλ
1

.
Just as we did for λµ-calculus and categories of continuations, we use terms of λR×+ with
parameters in C. For example, if ζ : Q
j∈J Ai →B and ϕ : Q
j∈J Ai →RB, then ϕ ζ : Q
j∈J Ai →
R, where formally ϕ ζ is the term with parameters (y x) {ζ/x, ϕ/y}.
3.2.5
Interactions between C and RC
We will also extend the λµ-terms with parameters of section 3.2.3 by allowing the substitution
of terms of λR×+ with parameters in C (deﬁned in section 3.2.4) for µ-variables of λµ-terms
with parameters in RC. For example, if φ : Q
j∈J RAj →RB `
 ˙
k∈K RCk
in RC and ζ :
Q
j∈J RAj

×
 Q
k∈K Ck

→B in C, then [ζ] φ : Q
j∈J RAj →R1 `
 ˙
k∈K RCk
in RC. It can
36

be shown that as expected:

eφ, ζ

ψ = [ζ] ψ φ
if



























φ :
Y
j∈J
RAj →RB `
 ¸
k∈K
RDk
!
ψ :
Y
j∈J
RAj →RRB×C `
 ¸
k∈K
RDk
!
ζ :

Y
j∈J
RAj

×
 Y
k∈K
Dk
!
→C
[∗] φ = φ
if φ :
Y
j∈J
RAj →R1 `
 ¸
k∈K
RDk
!
[ini ζ] φ = [ζ] pi φ
if















φ :
Y
j∈J
RAj →
 RB1 × RB2
`
 ¸
k∈K
RDk
!
ζ :

Y
j∈J
RAj

×
 Y
k∈K
Dk
!
→Bi
This possibility of having terms of λR×+ inside the brackets of λµ-terms is closely related to the
λµeµ-calculus of [Her95, CH00], extended to handle products (using sums of λR×+).
3.3
µPCF
Programming language for Computable Functions (PCF) is a functional programming language
described by Plotkin in [Plo77]. It is based on Scott’s Logic for Computable Functions (LCF),
which was presented in [Sco93]. The language contains constants for natural numbers and general
recursion. It is probably the simplest example of a Turing-complete higher-order language. Here
we consider an extension of PCF to primitively handle control operators, by presenting PCF as
a λµ-theory. In [OS97] the authors deﬁne a call-by-value semantics for λµ-calculus, and they
illustrate it with µPCFV , a call-by-value version of PCF with control. Later on, Laird deﬁnes
in [Lai99] its call-by-name version, which is the version we use here. The choice of this language
is justiﬁed on one hand by our will to get computational content directly from classical proofs,
and on the other hand by the fact that the games model can be seen primitively as a model of
µPCF.
3.3.1
Type constants
In his original formulation of PCF, Plotkin considered two base types: one for boolean and one
for natural numbers. Since it is easy to encode booleans into natural numbers (for example,
false is zero and true is any other natural number), we will only consider here one base type,
the type of natural numbers: ι.
3.3.2
Constants
Since we do not have a type for booleans, we factorize the conditional taking a boolean and the
“test-to-zero” with a single conditional if0 branching depending on its ﬁrst argument being zero
or not. In order to manipulate natural numbers we need the predecessor and successor functions,
37

and to be able to write any computable function we take a general ﬁxed point operator. The
constants of µPCF are then:
n : ι
succ : ι →ι
pred : ι →ι
if0 : ι →T →T
Y : (T →T) →T
It will also be useful to have a canonical term on each type so we deﬁne:
Ω∆= Y (λx.x) : T
This term represents non-termination, or “undeﬁned”.
3.3.3
Equations
Now we need to describe how these constructs relate to each other. Since we will work in a model
of µPCF, we are not interested into reduction, but only into equality. A reduction relation can
easily be deﬁned, and the corresponding equivalence relation on ground types is the equality
deﬁned here (see for example [HO00]). Since we are in the context of λµ-calculus, we also need
the equations ruling the interaction between the constants and the µ operator. The equations
ruling the behavior of pred and succ are standard (since we work with natural numbers, the
predecessor of 0 is 0):
succ n = n + 1
succ (µα.M) = µα.M {[α] succ ( ) / [α]
}
pred 0 = 0
pred n + 1 = n
pred (µα.M) = µα.M {[α] pred ( ) / [α]
}
For the conditional if0, it is not much a surprise that the deﬁning equations are:
if0 0 M N = M
if0 n + 1 M N = N
if0 (µα.M) N P = µα.M {[α] if0 ( ) N P / [α]
}
and the equation for ﬁxed point operator Y is also standard:
Y M = M (Y M)
3.4
System T
System T was introduced by Gödel in [Göd58] in order to give a consistency proof of Heyting
arithmetic (and therefore of Peano arithmetic by double-negation translation). This system can
be equivalently formulated as a system of primitive recursive functionals, which is an extension
of primitive recursive functions to higher types.
It is strictly more powerful than primitive
recursion, since for example the Ackermann’s function is expressible in system T.
System T has one base type for natural numbers, product and function types, constants for
0 and successor, and a recursion operator of type T →(ι →T →T) →ι →T for any type T.
Restricting the type of the recursor to T = ι gives back the usual primitive recursive functions.
Since µPCF contains constants for every natural number, successor, predecessor and general
recursion, it is easy to encode system T in it. Indeed, if we deﬁne:
rec
∆= λxy.Y (λzu.if0 u x (y (pred u) (z (pred u))))
Then it is easy to derive:
rec : T →(ι →T →T) →ι →T
and in order to prove that it implements Gödel’s recursor we must prove that it satisﬁes the
corresponding equations:
38

Lemma 3.7. Let M : T and N : ι →T →T. We have:
rec M N 0 = M
and for any n ∈N:
rec M N n + 1 = N n (rec M N n)
Proof. This follows easily from the deﬁnition of rec and the equations of µPCF for pred, if0 and
Y.
Therefore in the following we will consider system T as a subsystem of µPCF.
3.4.1
Interpreting PAωr in system T + Ω
In order to interpret PAωr in system T, we ﬁrst need to provide a term MA of type A∗for each
A ∈PAωr:
M(∆s)
∆= λx.x : 0 →0
M(∆k)
∆= λx.x : 0 →0
M(∆rec0)
∆= λx.x : 0 →0
M(∆recS)
∆= λx.x : 0 →0
M(Snz)
∆= Ω: 0
M(indr)
∆= rec : B∗→(ι →B∗→B∗) →ι →B∗
for every formula B with free variables among xι, ⃗y ⃗T .
Finally, concerning the relativization
axioms:
MLsM
∆= λxyz.x z (y z) : (T →U →V ) →(T →U) →T →V
MLkM
∆= λxy.x : T →U →T
ML0M
∆= 0 : ι
MLSM
∆= succ : ι →ι
MLrecM
∆= rec : T →(ι →T →T) →ι →T
3.5
Bar recursion
Bar recursion is an operator which can be seen as recursion on well-founded trees.
It was
ﬁrst introduced by Spector in [Spe62] to extend Gödel’s dialectica interpretation to Heyting
arithmetic augmented with the axiom of countable choice.
A variant of this operator was
studied in [Koh90], and another more uniform operator which is very similar to bar recursion was
introduced in [BBC98] and used in a realizability setting. A version in which the well-foundedness
of trees is implicit was proposed in [BO05] under the name of modiﬁed bar recursion. For a
comparison between these diﬀerent forms of bar recursion and other similar principles we refer
the reader to [Pow13, Pow14].
Here we consider a slight extension of the modiﬁed bar recursion of [BO05] taking beneﬁts
of control operators to allow an arbitrary return type. However, we will see that this operator
is not total in the general case, a suﬃcient condition being that the return type is a base type,
in which case our bar recursion becomes equivalent to the modiﬁed bar recursion of [BO05]. We
did not investigate further to ﬁnd a less restrictive condition.
In this section, we ﬁrst recall general results about well-founded recursion, then we present
recursion on natural numbers as a particular instance of it, and we introduce the bar recursion
operator as the equivalent of system T’s recursor, but for recursion over well-founded trees
rather than natural numbers. Finally, we deﬁne formally the bar recursor in µPCF and give an
illustration of its non-totality in the general case.
39

3.5.1
Well-founded induction and recursion
We recall here the basic principles of well-founded induction and recursion.
Deﬁnition 3.5 (Well-founded relation). A (binary) relation R on a set E is well-founded if
every non-empty subset of E has a minimal element, that is:
∀F ⊆E, F ̸= ∅⇒∃m ∈F, ∀f ∈F, ¬ (f R m)
In the following we will write R−1 (e) = {f ∈E | f R e}. With this notation, R is well-
founded if and only if:
∀F ⊆E, F ̸= ∅⇒∃m ∈F, R−1 (m) = ∅
Using the axiom of dependent choice, this condition is equivalent to the non-existence of inﬁnite
sequences of decreasing elements:
∀(en)n∈N ∈EN, ∃n ∈N, ¬ (en+1 R en)
The ﬁrst property of well-founded relations is well-founded induction. If we want to prove a
property P for every element e of a set E endowed with a well-founded relation R, then we can
suppose without loss of generality that P holds for every f ∈R−1 (e) in order to prove P (e):
Lemma 3.8 (Well-founded induction). Let E be a set, R a well-founded relation on E and P
a property on E. The following holds:
∀e ∈E
  ∀f ∈R−1 (e) , P (f)

⇒P (e)

⇒∀e ∈E, P (e)
The second fundamental property of well-founded relations is that similarly to well-founded
induction, if we want to deﬁne a function ψ on a set E endowed with a well-founded relation R,
then we can make the deﬁnition of ψ (e) depend on ψ (f) for f ∈R−1 (e):
Lemma 3.9 (Well-founded recursion). Let R be a well-founded relation on a set E, X be a set
and:
ϕ :
 X
e∈E
XR−1(e)
!
→X
then there exists a unique function ψ : E →X such that:
∀e ∈E, ψ (e) = ϕ
 e, ψ|R−1(e)

The function ϕ represents the deﬁnition of ψ (e) which depends on e and on the restriction
of ψ to R−1 (e).
3.5.2
Recursion on natural numbers
The usual recursion on natural numbers can be described in terms of well-founded recursion.
Indeed, if we deﬁne:
S = {(n, n + 1)| n ∈N}
it is easy to see that S is well-founded. The well-founded recursion theorem states that in order
to deﬁne a function from natural numbers to a set X, it is suﬃcient to provide a function:
ϕ :
 X
n∈N
XS−1(n)
!
→X
40

In the particular case of natural numbers, there are two possibilities: either n = 0, in which case
S−1 (n) is empty, or n ̸= 0, in which case S−1 (n) is the singleton set {n −1}. Therefore, we
can provide instead of ϕ a value x0 ∈X together with a function ϕ0 : N × X →X. ϕ can then
be recovered by:
ϕ (n, f) =
(
x0 if n = 0
ϕ0 (n, f (n −1)) if n ̸= 0
If we look at it from the point of view of types, then the recursion theorem tells us that from
an element a of type T and an element b of type ι →T →T we can build an element c of type
ι →T such that for any n:
c n =
(
a if n = 0
b n
 c n −1

otherwise
which are the exact equational deﬁnitions of rec a (λx.b (succ x)). System T’s recursor is then
the computational interpretation of well-founded recursion on natural numbers.
3.5.3
Recursion on well-founded trees
There are several ways to deﬁne trees. Here we choose to deﬁne a tree as a preﬁx-closed set of
words on a given alphabet:
Deﬁnition 3.6 (Tree). If A is a non-empty set (the alphabet), a tree on A is a non-empty set
T ⊆A∗of ﬁnite words on A such that:
u ∈T ∧v ⊑u ⇒v ∈T
where ⊑denotes the preﬁx ordering.
In this section we will only be interested into a particular kind of trees: fully-branching trees.
These are trees T on A such that:
∀u ∈T (∃a ∈A, ua ∈T ⇒∀a ∈A, ua ∈T)
This means that if u ∈T, then either u is a leaf, or every extension of u with a single element
is in T.
We now explain how we can construct trees using functions of a special kind. Consider a
function f : AN →X, and suppose that this function is continuous:
∀α ∈AN, ∃n ∈N, ∀β ∈AN (∀i < n, βi = αi ⇒f (β) = f (α))
(3.2)
This notion of continuity is obtained when we give AN the product topology of the discrete
topology on A, and X the discrete topology. This notion has a particularly intuitive interpreta-
tion: f : AN →X is continuous if and only if the result of f (α) depends only on a ﬁnite number
of values of α.
Let deﬁne the function giving the modulus of continuity for f:
modf : AN −→N
α 7−→min
n
n ∈N
 ∀β ∈AN (∀i < n, βi = αi ⇒f (β) = f (α))
o
Then this function deﬁnes a fully-branching tree:
Tf =
n
α0α1...αi
 α ∈AN ∧−1 ≤i < modf (α)
o
41

Easily, Tf is a tree. For the fully-branchingness of Tf remark that:
∀α, β ∈AN (∀i < modf (α) , βi = αi ⇒modf (β) = modf (α))
Indeed, if a0...aib ∈Tf, then there is some β ∈AN such that β0...βiβi+1 = a0...aib and
modf (β) > i + 1.
Suppose now that some a0...aic /∈Tf, then α = a0...aiccc... is such
that α0...αiαi+1 = a0...aic so it must verify modf (α) ≤i + 1 (otherwise a0...aic ∈Tf).
Since β0...βi = a0...ai = α0...αi and modf (α) ≤i + 1 then we have by the above remark
modf (α) = modf (β) > i + 1, hence the contradiction.
If f : AN →X is continuous, then Tf also has the property that for any α ∈AN:
α0α1...αmodf(α) /∈Tf
from the above remark. This means that there is no inﬁnite branch. We will now connect this
notion to that of well-founded relations.
We can deﬁne a binary relation P on A∗by:
P = {(ux, u)| u ∈A∗, x ∈A}
This relation is of course ill-founded on A∗since A ̸= ∅(consider ... P aa P a P ϵ for any a ∈A).
A well-founded tree is a tree on which this relation is well-founded:
Deﬁnition 3.7 (Well-founded tree). If A is a set and T ⊆A∗is a tree on A, then T is well-
founded if P ∩T 2 is a well-founded relation on T.
Then using the axiom of dependent choice, a tree T is well-founded if and only if there is no
α ∈AN such that:
∀n ∈N, α0α1...αn ∈T
Therefore, if f : AN →X is continuous, then Tf is a well-founded fully-branching tree.
With this deﬁnition we now describe how to perform well-founded recursion on well-founded
trees deﬁned by continuous functions. Let f : AN →X be continuous. The recursion theorem
states that in order to deﬁne a function from Tf to X, it is suﬃcient to provide a function:
ϕ :

X
u∈Tf
X(P∩T 2
f )
−1(u)

→X
Since Tf is fully-branching and for any u ∈Tf:
 P ∩T 2
f
−1 (u) = P −1 (u) ∩Tf
this set is either empty (u is a leaf), or equal to P −1 (u) (we will call it an internal node of
Tf), in which case it is in one-to-one correspondence with A. Moreover, if u is a leaf, then u
is uniquely described by any inﬁnite sequence α ⊒u, since in that case u can be recovered by
taking the longest preﬁx of α which is in Tf. Therefore, if we provide:
ϕ0 : A∗× XA →X
we can then deﬁne:
ϕ (u, g) =
(
f (α) for some α ⊒u
if u is a leaf
ϕ0 (u, a 7→g (ua))
if u is an internal node
42

This deﬁnition is correct since if u ∈Tf is a leaf, then any α, β ∈AN extending u are such that
f (α) = f (β). By recursion we obtain a function ψ : Tf →X which is easily extended to A∗by
putting for u /∈Tf: ψ (u) = f (α) for some α ∈AN extending u. Again this is correct since by
deﬁnition of Tf, for any inﬁnite α, β ⊒u we have f (α) = f (β).
If we put all this together we have:
f : AN →X continuous
ϕ0 : A∗× XA →X
and we build ψ : A∗→X such that for any u ∈A∗:
ψ (u) =
(
ϕ0 (u, a 7→ψ (ua))
if u is an internal node
f (α) for some α ⊒u
otherwise
The bar recursion operator builds ψ from ϕ0 and f, as we will explain in the next section.
3.5.4
Bar recursion in µPCF
We now describe how we can encode bar recursion in µPCF. First we introduce some notations
that will get their formal deﬁnition in the next paragraph. Suppose we have a type for lists of
elements of type T: T ⋄. The empty list is denoted ϵ : T ⋄, if u is an element of type T ⋄and
a is an element of type T, then u ∗a : T ⋄denotes the appending of a at the end of u, and
u @ a : ι →T denotes the inﬁnite sequence starting with u, and for which the subsequent values
are all equal to a.
If F is an element of type (ι →T) →U which is a program, then if F gives an answer for
some input α, the computation was ﬁnite and therefore F only looked at a ﬁnite part of α.
Therefore, F is continuous in the sense of (3.2), and F deﬁnes a tree TF as described above. If
we have also an element Φ of type T ⋄→(T →U) →U then recursion on TF must provide an
element Ψ of type T ⋄→U such that for any element u of type T ⋄:
Ψ u =
(
Φ u (λx.Ψ (u ∗x))
if u is an internal node of TF
F (u @ Ω)
otherwise
The question is: how do we know if u is an internal node? u being an internal node means that
there exist two inﬁnite extensions of u for which F’s computation is diﬀerent. Therefore, if u is
an internal node then F looks at the Ωpart of u @ Ω. Conversely, if u is not an internal node,
then all the computations of F on inﬁnite extensions of u are the same, which means that F
only looks at the u part of such extensions. Now we use the control possibilities of µPCF to
capture the fact that F looks at the Ωpart of u @ Ω, and when it happens (if it happens) cancel
the computation of F and launch the computation of some other a of type U. This is done by
the following term:
µα. [α] F (u @ µβ. [α] a)
where α, β are two distinct fresh µ-variables. What happens during the computation of that term
is that if F only looks at the u part of u@µβ. [α] a, then the term is equal to µα. [α] F (u @ Ω) =
F (u @ Ω), but if F looks at the µβ. [α] a part of u @ µβ. [α] a, then since β does not ap-
pear free in [α] a, the computation of µβ. [α] a will never return anything so the computation
of F stops.
However the result of the computation of a will be given back directly as the
result of µα. [α] F (u @ µβ. [α] a), so it becomes equal to a.
Now, since we want Ψ u to be
Φ u (λx.Ψ (u ∗x)) when u is an internal node of TF , we simply replace a with Φ u (λx.Ψ (u ∗x))
and obtain:
Ψ u = µα. [α] F (u @ µβ. [α] Φ u (λx.Ψ (u ∗x)))
43

then the two cases are handled transparently. Indeed, ﬁx some u : T ⋄. Either F does not look
at the extension of u so we have:
Ψ u = µα. [α] F (u @ Ω) = F (u @ Ω)
which is correct since u is not an internal node, or F looks at the extension of u so the µβ stops
the computation of F and we get:
Ψ u = Φ u (λx.Ψ (u ∗x))
which is also correct since u is an internal node.
What we just described is what we obtain when we apply the bar recursion operator to Φ
and F. The bar recursion operator, is therefore a term:
barrec : (T ⋄→(T →U) →U) →((ι →T) →U) →T ⋄→U
satisfying the equation:
barrec Φ F u = µα. [α] F (u @ µβ. [α] Φ u (λx.barrec Φ F (u ∗x)))
In the following we will explain formally how we deﬁne lists and the bar recursor in µPCF.
Encoding lists in µPCF
In order to deﬁne bar recursion, we ﬁrst need to encode lists and operations on lists in µPCF.
We choose to represent a list by a natural number (the size of the list) together with a (partial)
function on natural numbers. Therefore we deﬁne the type of lists as:
T ⋄∆= ι × (ι →T)
As stated above, the ﬁrst component of a list is its size. We introduce the following notation:
|M| ∆= p1 M
It is easy to derive |M| : ι from M : T ⋄. We also deﬁne the empty list:
ϵ ∆=

0, λx.Ω

for which it is easy to derive ϵ : T ⋄and |ϵ| = 0, and we give a notation to access elements in a
list:
M HNI ∆= p2 M N
We can derive from M : T ⋄and N : ι that M HNI : T and ϵ HNI = Ω. In order to deﬁne
extensions of lists, we need subtraction on natural numbers and tests of equality and strict
ordering. Subtraction is deﬁned by:
sub ∆= Y (λzxy.if0 y x (z (pred x) (pred y)))
for which we can derive sub : ι →ι →ι and prove (by induction on n) that for any n, m ∈N:
sub m n =
(
m −n if n ≤m
0 otherwise
44

and test for equality and strict ordering of natural numbers are deﬁned by:
if=
∆= λxyuv.if0 (sub x y) (if0 (sub y x) u v) v
if<
∆= λxyuv.if0 (sub y x) v u
and we derive if= : ι →ι →T →T →T, if< : ι →ι →T →T →T and:
if= m n M N =
(
M if m = n
N otherwise
if< m n M N =
(
M if m < n
N otherwise
We are now able to deﬁne the extension of a list by a single element:
M ∗N ∆= ⟨succ |M| , λx.if= x |M| N (M HxI)⟩
it is routine to derive from M : T ⋄and N : T that M ∗N : T ⋄, |M ∗N| = succ |M| and
(M ∗N) H|M|I = N. Finally, we deﬁne inﬁnite extension of a list with a ﬁxed element:
M @ N ∆= λx.if< x |M| (M HxI) N
We can derive from M : T ⋄and N : T that M @ N : ι →T and:
(M0 ∗M1 ∗... ∗Mn−1 @ N) HmI =
(
Mm if m < n
N otherwise
The bar recursion operator
We have now all necessary material to deﬁne formally the bar recursion operator:
barrec ∆= λuv.Y (λzy.µα. [α] v (y @ (µβ. [α] u y (λx.z (y ∗x)))))
Bar recursor can be typed as expected:
barrec : (T ⋄→(T →U) →U) →((ι →T) →U) →T ⋄→U
And it veriﬁes indeed the equation:
barrec M N P = µα. [α] N (P @ µβ. [α] M P (λx.barrec M N (P ∗x)))
Non-totality of bar recursion
We now explain how bar recursion can diverge when not restricted to the case of U being a base
type. Fix T = ι and U = ι →ι so we have:
barrec : (ι⋄→(ι →ι →ι) →ι →ι) →((ι →ι) →ι →ι) →ι⋄→ι →ι
Take also M ∆= λxyz.y 0 (succ |x|) and N = λx.x. We have for any P : ι⋄:
barrec M N P = µα. [α] N (P @ µβ. [α] M P (λx.barrec M N (P ∗x)))
= µα. [α] P @ µβ. [α] M P (λx.barrec M N (P ∗x))
= µα. [α] P @ µβ. [α]
 λz.barrec M N
 P ∗0

(succ |P|)

Then we have the following:
barrec M N ϵ 0 =
 µα. [α] ϵ @ µβ. [α]
 λz.barrec M N
 ϵ ∗0

(succ |ϵ|)

0
45

= µα. [α]
 ϵ @ µβ. [α] barrec M N
 ϵ ∗0

(succ |ϵ|)

0
= µα. [α] µβ. [α] barrec M N
 ϵ ∗0

(succ |ϵ|)
= µα. [α] barrec M N
 ϵ ∗0
  succ 0

= barrec M N
 ϵ ∗0

1
=
 µα. [α] ϵ ∗0 @ µβ. [α]
 λz.barrec M N
 ϵ ∗0 ∗0
  succ
ϵ ∗0

1
= µα. [α]
 ϵ ∗0 @ µβ. [α] barrec M N
 ϵ ∗0 ∗0
  succ
ϵ ∗0

1
= µα. [α] µβ. [α] barrec M N
 ϵ ∗0 ∗0
  succ
ϵ ∗0

= µα. [α] barrec M N
 ϵ ∗0 ∗0
  succ 1

= barrec M N
 ϵ ∗0 ∗0

2
...
= barrec M N
 ϵ ∗0 ∗0 ∗0

3
...
This non-total behavior comes from the fact that the identity function on sequences of natural
numbers is obviously non continuous when we take the product topology on the domain and
the discrete topology on the codomain (each sequence, as a singleton is an open set). Since the
identity function on sequences is deﬁnable in µPCF, this example is reproducible in any model
of µPCF. However, if we restrict to the case where U is built only from ι, 0 and product, then
in many models every element of type (ι →T) →U is continuous when taking on ι →T the
product topology of the discrete topology of T, and on U the discrete topology. The model of
game semantics that we describe in the next chapter gives such an example since it has a CPO
structure. An obvious example of a model where this continuity requirement does not hold is
the full set-theoretic model (take for example the function from sequences of natural numbers
to natural numbers which maps the constant zero sequence to 0 and all other sequences to 1).
The deﬁnition of the realizer of (DCr) based on barrec will be given in section 5.2.3.
46

Chapter 4
Game semantics
We ﬁrst give in this chapter an intuitive view on the model of Hyland-Ong-Nickau games. We
then deﬁne it formally and prove that it is a category of continuations. Finally, we describe
how µPCF (and therefore Peano arithmetic and the axiom of choice) can be interpreted in this
model.
4.1
Introduction
In this section, we give an informal presentation of game semantics, introducing it through its
correspondence with certain normal forms of simply-typed λ-calculus. Another presentation of
this correspondence can be found in [DHR96], together with the connection with linear head
reduction. At ﬁrst, we will only consider simply-typed lambda-calculus with one base type,
i.e. no primitive datatypes and no ﬁxed-point operator. The games model relies heavily on the
notion of Böhm tree of a program, a Böhm tree being a representation of a normal form of a
program. Let us see an example. Take the following λ-term in normal form:
M = λux.u (λy.u (λz.z) y) x
(4.1)
This term will be the running example throughout this section. The Böhm tree of M is the
following:
λux.u
λy.u
x
λz.z
y
A simple way to look at Böhm trees is to see them as syntactic trees corresponding to the
following grammar:
N
::=
λy1...yn.x N1...Nm
which is a grammar generating the β-normal forms of λ-calculus. The Böhm tree corresponding
to some N = λx1...xn.x N1...Nm is then:
λx1...xn.x
T1
...
Tm
where Ti is the Böhm tree of ti. Since Böhm trees are very tightly related to this syntax, we
will write a λ even for an empty λ-abstraction to make explicit that it is indeed generated by
47

the grammar. For example, M will be written:
λux.u (λy.u (λz.z) (λ.y)) (λ.x)
and the corresponding Böm tree:
λux.u
λy.u
λ.x
λz.z
λ.y
This cumbersome notation makes explicit the fact that in a Böhm tree, nodes can be decomposed
in two parts: the λ part consisting of a (possibly empty) list of variables, and the head part,
consisting of a single variable. For example, the root λux.u can be decomposed as the list u, x
and the variable u, and the rightmost leaf can be decomposed as the empty list and the variable
x. Using this observation, we can consider a slight modiﬁcation of Böhm trees where these two
parts are distinguished, the single variable part becoming the unique son of the corresponding
λ part. We obtain the following tree for M:
λux
u
λy
λ
u
x
λz
λ
z
y
The nodes of the tree are now of two distinct types that alternate along each branch. The λ
nodes will be called O-moves (for opponent), while the variable nodes will be called P-moves.
Because of the deﬁnition of our Böhm trees, these polarities alternate along each branch. Here
is the Böhm tree of M where the moves are annotated by their polarities:
λuxO
uP
λyO
λO
uP
xP
λzO
λO
zP
yP
We now describe what is a walk in that tree. As stated informally in the previous section,
a walk is an exploration of the tree by an opponent, player describing it step-by-step. In that
sense it will be a sequence of moves such that the move following any O-move is its son (which
is a P-move), and conversely the move preceding any P-move is its father (which is an O-move).
Moreover, opponent must start the exploration with the root, and it can play an O-move only
48

if all the moves higher in the branch have already been played. Here is an example of a walk
through the tree of M, together with the polarities of moves:
O
λux (1)
P
u (2)
O
λy (5)(9)
λ (3)
P
u (6)(10)
x (4)
O
λz (7)
λ
P
z (8)
y
O
P
O
P
O
P
O
P
O
P
λux
u
λ
x
λy
u
λz
z
λy
u
The next thing is: the player represents the program, so he knows the tree, but how can the
opponent play moves in the tree without knowing it? This is where arenas come into play. Since
we work in a simply-typed setting with one base type, we can be more precise. Let ι be the base
type. The types (ranged over by T, U) are either ι, or some T →U. For the sake of conciseness
of notations, we will simply write T U for T →U. A possible grammar to describe the types is
then the following:
T
::=
T1...Tn ι
On the side of terms, the following grammar generates what are called the long βη-normal forms:
NT1...Tn ι
::=
λxT1
1 ...xTn
n .xU1...Um ιNU1
1 ...NUm
m
Remark that the grammar of types has not been chosen randomly, but ﬁts quite precisely that
of long βη-normal forms. Let us write M again, but this time annotated with types:
M((ι ι)ι ι)ι ι = λu(ι ι)ι ιxι.u(ι ι)ι ι 
λyι.u(ι ι)ι ι (λzι.zι) yι
xι
Now we explain how O, who does not know the tree, is able to play moves in it. Since the
type of the program is known to both O and P, O knows that if the type of the program is
T1...Tn ι, then the root has to be λxT1
1 ...xTn
n . Later on, when P plays some variable x of type
(U1,1...U1,k1 ι) ... (Um,1...Um,km ι) ι then this variable must be part of a λ-move played earlier by
O, who therefore knows its type, and knows that its sons are the λyUi,1
i,1 ...y
Ui,ki
i,ki
for 1 ≤i ≤m.
The Böhm tree is constructed iteratively starting from an empty tree, and at each point O knows
the sons of each P-move which has already been played. We illustrate this with the example
walk given above: the part of the tree that O consecutively “sees” is:
λu(ι ι)ι ιxι
λu(ι ι)ι ιxι (1)
u(ι ι)ι ι (2)
λyι
λ
λu(ι ι)ι ιxι (1)
u(ι ι)ι ι (2)
λyι
λ (3)
xι (4)
λu(ι ι)ι ιxι (1)
u(ι ι)ι ι (2)
λyι (5)
λ (3)
u(ι ι)ι ι (6)
xι (4)
λzι
λ
49

λu(ι ι)ι ιxι (1)
u(ι ι)ι ι (2)
λyι (5)
λ (3)
u(ι ι)ι ι (6)
x (4)
λz (7)
λ
zι (8)
λu(ι ι)ι ιxι (1)
u(ι ι)ι ι (2)
λyι (5)(9)
λ (3)
u(ι ι)ι ι (6)(10)
xι (4)
λzι (7)
λ
zι (8)
For example, when P plays the 6th move u(ι ι)ι ι, O knows that u is of type (ι ι) ι ι because he
provided it in the 1st move. Therefore, O knows that the two arguments of u are of type ι ι and
ι, so the two sons of u are λzι and λ.
These considerations make appear a new type-directed structure on Böhm trees. Indeed,
each O-move λxT1
1 ...xTn
n
enables the P-moves xTi
i
for 1 ≤i ≤n, and conversely each P-move
x(U1,1...U1,k1 ι)...(Um,1...Um,km ι)ι enables the O-moves λyUi,1
i,1 ...y
Ui,ki
i,ki
for 1 ≤i ≤m. Let make this
enabling relation explicit in the Böhm tree of M:
λu(ι ι)ι ιxι
u(ι ι)ι ι
λyι
λ
u(ι ι)ι ι
xι
λzι
λ
zι
yι
We see that because of the structure of the tree, the P-moves may be enabled by O-moves
higher in the branch, but the O-moves must be enabled by their direct father. As stated above,
this enabling relation is type-directed. Just like Böhm trees are the syntactic trees of programs,
arenas are the syntactic trees of types, for the grammar given above: if T = T1...Tn ι, then the
associated arena is:
T1...Tn ι
T1
...
Tn
where Ti is the syntactic tree of Ti. For example, the arena for the type ((ι ι) ι ι) ι ι of M is the
following:
((ι ι) ι ι) ι ι
(ι ι) ι ι
ι
ι ι
ι
ι
50

Let put side-by-side the Böhm tree of M and the arena of its type, and draw the correspondence:
λu(ι ι)ι ιxι
u(ι ι)ι ι
λyι
λ
u(ι ι)ι ι
xι
λzι
λ
zι
yι
((ι ι) ι ι) ι ι
+
(ι ι) ι ι
*
4
ι5
ι ι
*5
ι)6
ι58
The curved full lines on the Böhm tree are mapped through the dashed arrows to the straight
full lines on the arena. This makes clear that the enabling relation is type-directed. In fact we
revealed enough structure now to forget completely about the labels:
+
)5
5
)5
(6
58
We can retrieve the full Böhm tree (modulo α-conversion) from the above structure. In order
to clarify the picture, we deﬁne arbitrary labels for the nodes of the arena, and use them to
describe the dashed arrows:
a
b
d
e
b
c
d
e
f
f
a
b
c
d
e
f
With this representation we get a model of simply typed λ-calculus which is sound with respect
to βη equivalence.
But what about composition? Indeed, given two Böhm trees together with their arenas with
compatible types, how do we compose them? Let us go back to the notion of walk presented
earlier. In order to compose Böhm trees, it is easier to manipulate walks in these trees rather
than the trees directly. A walk in a Böhm tree is called a play. It can be seen as an exploration
of the tree by an opponent, the tree being given step-by-step by the player. The idea is to switch
51

from the concept of Böhm tree to the concept of strategy, a strategy being the set of plays on
this Böhm tree.
Let ﬁrst look at how we can recover the tree from the set of plays on that tree. The idea is
that a branch of the tree is a particular play, and if we know every branch of the tree, then we
can recover the full tree. The enabling structure of the tree is essential to recover the full tree.
Take for example the terms M = λux.u (λy.u (λz.z) y) x and λux.u (λy.u (λz.y) x) x of same
type ((ι ι) ι ι) ι ι, which are variants of the Kierstead terms (the only reason for not choosing
exactly Kierstead terms is that it allows us to keep the same running example). Here are their
common arena and their Böhm trees:
a
b
c
d
e
f
a
b
d
e
b
c
d
e
f
f
a
b
d
e
b
c
d
e
f
f
As we can see, the trees have the same shape, but a diﬀerent enabling structure. Therefore, plays
must not be only words of labels of the arena, but words of labels of the arena with pointers.
The example play that was given earlier is then:
a(1)
b(2)
d(5)(9)
e(3)
b(6)(10)
c(4)
d(7)
e
f(8)
f
a
b
e
c
d
b
d
f
d
b
1
2
3
4
5
6
7
8
9
10
O
P
O
P
O
P
O
P
O
P
Now we can recover the full tree from the plays on it, by looking at the branches. For example,
the branches of the Böhm tree of M (from the leftmost to the rightmost) are the following plays:
a
b
d
b
d
f
a
b
d
b
e
f
a
b
e
c
We now look at the other direction: given an arena, is any set of plays the description of a Böhm
tree? Obviously the answer is no. Let ﬁx a set σ of plays on an arena and let formulate some
necessary conditions for it to be the set of plays on a Böhm tree T on an arena A.
First, since the empty play (denoted ϵ) is a play on any Böhm tree, we must have ϵ ∈σ.
Next, since plays of σ are explorations of T , a preﬁx of a play of σ is itself an exploration
of T , and therefore it must be in σ. This condition is called preﬁx closure of strategies. Using
this condition, we have ϵ ∈σ ⇔σ ̸= ∅.
The next restriction that comes to mind is the fact that for any O-move being played, there
is a unique P-move that follows it. Indeed, in T , each O-move has a unique son (which is a
52

P-move). To make it more formal, let w be a play of σ, that is, w is a word of nodes of A
together with pointers. If w ends with an O-move, then there exists a unique P-move a and a
unique O-move b of w such that wa ∈σ (where in wa the pointer of a goes to b). The existence
corresponds to the notion of totality, and the uniqueness to that of determinism.
In the example play, abecd (with the pointers deﬁned above) is a play on the Böhm tree (by
preﬁx closure, since it is a preﬁx of the example play), and in the Böhm tree, d has b (pointing
to a) as unique son. Therefore, abecdb (with the pointers) is a play on the Böhm tree.
Still, we did not put enough restrictions on the sets of plays for them to describe Böhm
trees. Indeed, the moves of P must depend only on the current branch of the tree that is being
explored, while a play may go into one branch, and then to another (as it is the case with the
example play, after the 4th move, O chooses to go to the left branch by playing the 5th move).
In order to make this idea more formal, we describe the notion of view of a play, which is a way
to compute the current branch. Take the example play. The current branch after the 10th move
has been played is:
a
b
d
as can be seen on the Böhm tree. Now we give an algorithm to compute it. Remember that a P
move that comes after an O-move must be its child in the Böhm tree, and an O-move is always
the child (in the Böhm tree) of the P-move it points to. Therefore, to compute the current
branch, we do the following. Take the last move, here it is b. It is a P-move, so the O-move
that comes just before is its father in the Böhm tree, we take it, we have d9b10. Now we are
at the 9th move d. It is an O-move, so it points to its father in the Böhm tree. Therefore, we
follow the pointer from d and get to the 2nd move b. We take it to obtain b2d9b10. Now, b is
a P-move, so the O-move that comes just before is its father in the Böhm tree, we take it, we
have a1b2d9b10. Finally, a is a root so we stop here. By taking back the pointers coming from
the play we get the branch:
a
b
d
What we just described is the computation of a view from a play. If w is a play, then its view
will be denoted by ⌜w⌝. Then the condition of P seeing only the current branch is the following:
if w is a play of σ ending with an O-move and if a is the unique P-move such that ⌜w⌝a ∈σ,
then a is also the unique P-move such that wa ∈σ. The pointers must also be dealt with,
but it is straightforward (even though technical). This condition is what game semanticists call
innocence.
The last condition corresponds to the fact that Böhm trees, which are ﬁnitely branching be-
cause of the simply-typed structure, are also of ﬁnite depth. We saw in the preceding paragraph
that branches of T correspond to views of σ. Therefore, we ask for the views of σ to be ﬁnite, or
equivalently (since T is ﬁnitely branching) that σ contains only a ﬁnite number of views. This
condition is called ﬁniteness.
Given an arena, a set of plays which is non empty, preﬁx-closed, total, deterministic, innocent
and ﬁnite is called a strategy. Finally we put all the necessary conditions to get the following
result:
Theorem. The set of strategies on ﬁnite arenas is in one-to-one bijection with the set of simply
typed λ-terms with one base type quotiented by βη equivalence.
Still, we do not know how to compose strategies. But the deﬁnition of strategies as sets of
plays makes the deﬁnition of composition very easy. If σ is a strategy on the arena associated
to T U and if τ is a strategy on the arena associated to T, then we can look at the plays w ∈σ
such that when we erase every move of w which is in the arena of U we get a play of τ. Then
for each such w, we erase all the moves that are in the arena of T. We obtain a set of plays on
53

U that we call σ (τ). It is very technical to prove that σ (τ) is a strategy on U but it is true,
and the good thing is that it corresponds to usual λ-calculus composition.
Let now illustrate this. M = λux.u (λy.u (λz.z) y) x is a term of type ((ι ι) ι ι) ι ι. Its arena
A and Böhm tree T are:
a
b
c
d
e
f
a
b
d
e
b
c
d
e
f
f
N = λvx.v x is a term of type (ι ι) ι ι. Its arena B and Böhm tree U are:
b
d
e
f
b
d
f
e
Of course, we chose the label on B to match the ones on A during composition. We describe
here a play (in fact the only maximal one) of T such that by keeping only the moves of B we
have a play of U:
b
d
f
e
a
b
d
e
b
c
d
e
f
f
a
b
d
b
d
f
e
c
M
O
P
O
P
O
P
O
P
N
O
P
O
P
O
P
1
2
3
4
5
6
7
8
If we keep only the moves of this play which are in A and not in B, we get the play a
c , and
we obtain the following Böhm tree (with its arena):
a
c
a
c
which is the Böhm tree of λx.x of type ι ι, and we have indeed M N =βη λx.x. We describe
now how we computed this play, step-by-step. We refer to the moves of this play as mi, where
m is the label and i the index. We will not recall the pointers, since they are given above. The
strategy interpreting M will be called σ, and the one for N τ. First, A has only one root which
must be the ﬁrst move of the play. Therefore, the play starts with a1. The second move is given
by M: b is the only P-move such that a1b2 ∈σ. Then if we keep only the moves that are in
54

B, we have the one-move play b2. The next move is then given by N: d is the only P-move
such that b2d3 ∈τ. Now, the play a1b2d3 ends with an O-move for σ, and the only solution
to complete it with a P-move while being still in σ is to add b4, so we get a1b2d3b4. At that
point, it is N’s turn. If we keep only the moves which are in B we obtain b2d3b4. Computing
the view we have ⌜b2d3b4⌝= b4, which can only be completed by τ with d5, so by innocence of
τ we get b2d3b4d5 ∈τ. The current play is then a1b2d3b4d5 and it is σ’s turn, and it completes
the play with f6 to get a1b2d3b4d5f6 which is the complete leftmost branch of T . It is now τ’s
turn. Keeping only the moves in B we are at b2d3b4d5f6. We compute again the view to get
⌜b2d3b4d5f6⌝= b4d5f6 so the play is completed with e7 into the (unique) full branch of U. The
current play is then a1b2d3b4d5f6e7 and it is σ’s turn. We have ⌜a1b2d3b4d5f6e7⌝= a1b2d3b4e7,
which is completed with c8. We then get the full play a1b2d3b4d5f6e7c8, and we cannot go further
since c is a leaf of the result arena. The play we obtain is in σ and if we keep only the moves
in B we get b2d3b4d5f6e7 ∈τ. Finally, keeping only the moves in A and not in B we get the
play a1c8 which is a play of the composition. Since the result arena is very simple, we obtain
the only strategy on this arena, and this strategy has only one maximal play (its Böhm tree has
only one branch of length 2), but the principle lifts to higher order very well.
We described the case of simply-typed λ-calculus with one base type, that base type being
interpreted as the one-move arena. If we want for example to have a primitive datatype for
booleans, we deﬁne the associated arena to be:
q
tt
ff
and we deﬁne question and answers among moves. In the arena for booleans q is a question
and tt and ff are answers. We must then restrict the strategies to be well-bracketed. This
means that a player can answer a question only if all other questions after it have already been
answered and the question it answers has not been answered already.
Next, if we want a primitive datatype for natural numbers, then we have to switch to inﬁnite
arenas. The arena of natural numbers is then:
q
0
· · ·
n
· · ·
The ﬁniteness condition is no more equivalent to having a ﬁnite number of views: the Böhm trees
of λ-terms may now be inﬁnitely branching. However, every branch is still ﬁnite. One would be
tempted then to change the ﬁniteness condition for a condition saying that every “maximal” view
is ﬁnite. However, a non-computable function on natural numbers also satisﬁes this condition,
so we would not get the deﬁnability result. The solution is then to distinguish the set of ﬁnite
strategies (the ones which have a ﬁnite number of views) as a subset of all strategies. Then any
ﬁnite well-bracketed strategy is the interpretation of some term (modulo an extension of the
language with a deﬁnition-by-case construct), and every term can be interpreted as a (possibly
non ﬁnite) well-bracketed strategy.
Finally, when we add a ﬁxpoint operator (and therefore model the language PCF), we have
to ﬁrst consider call-by-name operational semantics, and we have to drop the ﬁniteness and
totality conditions. Indeed, since ﬁxpoint operators induce non-termination, their Böhm trees
may be of inﬁnite depth (indeed, the Böhm tree of the Y ﬁxpoint operator of PCF has only
one branch, and it is inﬁnite). The totality disappears also because it is not preserved under
composition when Böhm trees are allowed to have inﬁnite branches. For example, the Böhm
tree of Y (λx.x) consists of only one root without any son, and so the corresponding strategy
cannot be total.
55

As stated before, we are interested here into getting a model for classical proofs, so in
the light of the Curry-Howard isomorphism we want to have control operators in the model.
Because of that we drop the well-bracketing condition.
We also want to be able to express
imperative behaviors, so we drop the innocence constraint.
However, since we still want a
model of λ-calculus, we need to have products. Therefore, we drop innocence but still require
single-threadedness of the strategies.
4.2
Arenas and strategies
4.2.1
Arenas and justiﬁed sequences
Here we deﬁne arenas, which are forests of moves. Arenas are the objects of the category of
HON games.
A forest is a partial order (E, ≤) such that ∀x ∈E, {y ∈E | y < x} is well-ordered.
In
particular, if x ≤z and y ≤z then x ≤y or y ≤x: compatible elements are comparable. The
binary relation <1 on E is then deﬁned as:
∀x, y ∈E
x <1 y
⇐⇒
x < y ∧∀z (x < z < y ⇒z = x ∨z = y)
x <1 y means that y is a direct child of x. The roots of a forest correspond to the minimal
elements for ≤.
Deﬁnition 4.1 (Arena). An arena is a countably branching, ﬁnite-depth forest. Its nodes are
called moves. Each move is given a polarity O (for Opponent) or P (for Player or Proponent):
• The roots are of polarity O
• If a <1 b then a and b are of opposite polarities
A root of an arena is also called an initial move. We will often identify an arena with its set
of moves. Here is an example of arena, the polarities of the moves being given on the left:
O
a
f
P
b
c
g
h
O
d
e
(4.2)
If A is an arena and X is a subset of A, then X inherits the ordered structure of A and it is
still an arena (but the polarities of moves may be diﬀerent than the ones in A). For example,
X = {a; c; d; e; g; h} is a set of moves of the arena (4.2) and the associated arena is:
O
a
g
h
P
d
e
c
(4.3)
Deﬁnition 4.2 (Justiﬁed sequence). Given an arena A, we deﬁne a justiﬁed sequence on A to
be a word s (ﬁnite or inﬁnite) of A together with a partial justifying function f : |s| ⇀|s| such
that:
• If f (i) is undeﬁned, then si is an initial move
• If f (i) is deﬁned, then f (i) < i and sf(i) <1 si
We denote the set of justiﬁed sequences on A by SA.
56

We denote the empty justiﬁed sequence by ϵ. Remark here that by deﬁnition of the polarity,
if f (i) is undeﬁned (si is initial), then si is of polarity O, and if f (i) is deﬁned, then si and sf(i)
are of opposite polarities. Also, f (0) is never deﬁned, and so s0 is always an initial O-move. A
justiﬁed sequence on the arena (4.2) is represented for example as:
a
b
a
b
b
e
d
f
e
h
(4.4)
Deﬁnition 4.3 (Restriction of a justiﬁed sequence). If A is an arena, X is a subset of A (and
therefore an arena) and s is a justiﬁed sequence on A with justifying function f, then we deﬁne
s|X to be the subword si0...sim of s consisting of the moves of s which are in X, together with
the justifying function g deﬁned by:
g (j) =
(
the k s.t. ik = fp (ij) where p = min
n
q > 0
 sfq(ij) ∈X
o
if ∃q > 0 sfq(ij) ∈X
undeﬁned otherwise
For example taking X as in (4.3) and s to be the sequence (4.4), s|X is the sequence:
a
a
e
d
e
h
Since b /∈X the pointers from the moves d and e have been updated to point to the corresponding
a, and since f /∈X the pointer from h has been erased. We have to ensure that a restricted
justiﬁed sequence is still a justiﬁed sequence:
Lemma 4.1. Let A be an arena. For any justiﬁed sequence s on A and any subset X of A, s|X
is a justiﬁed sequence on the arena X.
Proof. Let us write s = s0...sn with justifying function f and s|X = si0...sim with justifying
function g. First we have to check that if g (j) is undeﬁned, then sij is a root of X. If g (j) is
undeﬁned then by deﬁnition of a restriction we have:
n
sfq(ij)
 q > 0
o
∩X = ∅
but on the other hand since for any i, sf(i) <1 si and < restricts to a well-order on the set of
a ∈A such that a < sij we get:
n
sfq(ij)
 q > 0
o
=

a ∈A
 a < sij
	
Putting this together we get:

a ∈X
 a < sij
	
= ∅
and therefore sij is a root of X. Next we have to check that if g (j) is deﬁned, then sig(j) <1 sij
on X. If g (j) is deﬁned then by deﬁnition of a restriction we have ig(j) = fp (ij) where p > 0 is
the least such that sfp(ij) ∈X, so:
n
sf(ij); ...; sfp−1(ij)
o
∩X = ∅
but on the other hand since for any i, sf(i) <1 si and < restricts to a total order on the set of
a ∈A such that sfp(ij) < a < sij we get:
n
sf(ij); ...; sfp−1(ij)
o
=
n
a ∈A
 sfp(ij) < a < sij
o
Putting this together we get:
n
a ∈X
 sfp(ij) < a < sij
o
= ∅
and therefore sig(j) = sfp(ij) <1 sij on X.
57

In a sequence s with justifying function f, a move sj is hereditarily justiﬁed by a move si if
si is initial and for some n, fn (j) = i.
Deﬁnition 4.4 (Thread). If s is a justiﬁed sequence on A and if si is initial, then the thread
associated to si is the sequence consisting of the moves of s hereditarily justiﬁed by si, the
justifying function being deﬁned accordingly. The set of threads of s, Threads (s), is the set of
threads associated to the initial moves of s. By extension a justiﬁed sequence s will be called a
thread if it contains exactly one thread (i.e. Threads (s) = {s}). In particular, Threads (ϵ) = ∅
and so ϵ is not a thread.
The justifying function of a thread can indeed be deﬁned since if a points to b in s, then a
and b are hereditarily justiﬁed by the same initial move. For example we have:
Threads

a
b
c
d
e
f
g
h
i
j

=
n
a
b
d
g ; c
e
f
i ; h
j
o
A P-sequence (resp. O-sequence) is a sequence ending with a P-move (resp. an O-move). Write
t ⊑s if t is a preﬁx of s, i.e. t is a preﬁx of s as a word and their justifying functions coincide
(this is a particular case of subsequence). Write t ⊑P s (resp. t ⊑O s) if t is a P-preﬁx (resp.
O-preﬁx) of s, i.e. t ⊑s and t is a P-sequence (resp. O-sequence).
4.2.2
Plays and strategies
Deﬁnition 4.5 (Play). A play s on A is an alternating justiﬁed sequence of A, i.e., for any i,
s2i is an O-move and s2i+1 is a P-move. We denote the set of plays on A by PA.
A play on an arena is the trace of an interaction between a program and a context, each
one performing an action alternatively. A P-play (resp. O-play) is a play which is a P-sequence
(resp. O-sequence). If s ∈PA such that Threads (s) ⊆PA, then it is easy to see that in s, any
sequence of an O-move followed by a P-move must be in the same thread.
Deﬁnition 4.6 (Strategy). A strategy σ on A is a P-preﬁx-closed set of ﬁnite P-plays on A
such that:
• σ is deterministic: if sa and sb are in σ, then a = b.
• σ is single-threaded: for any ﬁnite P-play s, s ∈σ ⇔Threads (s) ⊆σ.
Our notion of single-threadedness matches the usual notion of thread-independence (see
e.g. [AHM98]). Remark also that a strategy always contains the empty play ϵ since Threads (ϵ) =
∅. Finally, using the single-threadedness of strategies, it suﬃces to give the threads of a strategy
in order to fully determine it.
4.3
The category G of arenas and strategies
4.3.1
The category G
Objects and morphisms
The constructions we use will sometimes contain multiple copies of the same arena (for example
A →A), so we distinguish the instances with superscripts (for example A(1) →A(2)).
The objects of G are arenas. In order to deﬁne morphisms between objects A and B, we
ﬁrst need to deﬁne the arena A →B. If A and B are arenas consisting of the trees (Ai)i∈I and
58

(Bj)j∈J and if i0, ..., in, ... is an enumeration of I and j0, ..., jp, ... of J, then the arena A →B is
the following forest:
Bj0
Ai0
(0)
· · ·
Ain
(0)
· · ·
Bjp
Ai0
(p)
· · ·
Ain
(p)
· · ·
· · ·
This arena contains J copies of A, and each root of the jpth copy becomes a direct son of the
root of Bjp. This deﬁnition is correct since the arena we obtain is still countably branching. We
deﬁne morphisms from A to B to be the strategies on the arena A →B. Formally, if s ∈SA→B,
then s|A is a justiﬁed sequence on the juxtaposition of J copies of A, however, we will consider
that it is a justiﬁed sequence on A by forgetting the particular copy of A each move belongs to.
Composition
Let A, B and C be three arenas. We deﬁne the set of interactions on A, B, C by:
Int (A, B, C) =

s ∈S(A→B)→C
 s|A→B ∈PA→B ∧s|B→C ∈PB→C ∧s|A→C ∈PA→C
	
where similarly to the projection on the left of an arrow, s|A→C can be considered as a justiﬁed
sequence on A →C by forgetting for each move of s in A which root of B it was attached to.
We consider the following automaton on the alphabet {A; B; C}:
000
011
101
/
C
:
C
z
A
+
A
k
B

B
R
(4.5)
We can run this automaton on a sequence s ∈S(A→B)→C by mapping moves to the letters
A, B, C, depending on the arena they belong to. The following lemma will be useful in order to
prove properties of the composition of strategies:
Lemma 4.2. If s ∈Int (A, B, C) then s is accepted by the automaton (4.5). Moreover, the ﬁnal
state of the automaton describes the parity of respectively s|A→B, s|B→C and s|A→C (0 for even,
1 for odd)
Proof. It is important to notice that the polarities of moves depend on which arena we consider.
The following table gives the polarities of each kind of move in A →B, B →C and A →C:
A →B
B →C
A →C
O-move in A
P
P
P-move in A
O
O
O-move in B
O
P
P-move in B
P
O
O-move in C
O
O
P-move in C
P
P
We prove the result by induction on s. If s = ϵ, the result is immediate. If s = s′a, we distinguish
on the state of s′:
59

• 000: if it appears in the corresponding arena, a must be:
– an O-move in A →B
– an O-move in B →C
– an O-move in A →C
Using the table, the only possibilities are a P-move in A (in which case s is in state 101)
or an O-move in C (in which case s is in state 011)
• 011: if it appears in the corresponding arena, a must be:
– an O-move in A →B
– a P-move in B →C
– a P-move in A →C
Using the table, the only possibilities are an O-move in B (in which case s is in state 101)
or a P-move in C (in which case s is in state 011)
• 101: if it appears in the corresponding arena, a must be:
– a P-move in A →B
– an O-move in B →C
– a P-move in A →C
Using the table, the only possibilities are an O-move in A (in which case s is in state 000)
or a P-move in B (in which case s is in state 011)
Let now σ : A →B and τ : B →C be morphisms. We deﬁne the interactions between σ and
τ as:
σ ∥τ =

s ∈Int (A, B, C)
 s|A→B ∈σ ∧s|B→C ∈τ
	
Finally we deﬁne the composition σ; τ : A →C as:
σ; τ =

s|A→C
 s ∈σ ∥τ
	
We now prove that it indeed deﬁnes a strategy. First, σ; τ ⊆PA→C by deﬁnition of Int (A, B, C).
Then if s ∈Int (A, B, C) is such that s|A→B ∈σ and s|B→C ∈τ, |s|A→B| and |s|B→C| are even so
|s|A→C| = 2|s| −|s|A→B| −|s|B→C| is also even so σ; τ is a set of P-plays.
Lemma 4.3. σ; τ is P-preﬁx closed
Proof. Let s ∈Int (A, B, C) be such that s|A→B ∈σ and s|B→C ∈τ, and let t be a P-preﬁx of
s|A→C. Let s′ be a preﬁx of s such that s′
|A→C = t. By deﬁnition of Int (A, B, C), s′ ∈Int (A, B, C),
so by lemma 4.2. s′ is accepted by the automaton. Moreover, since |s′
|A→C| = |t| is even, the
ﬁnal state of the automaton is of the form ij0, so it is 000.
This means that |s′
|A→B| and
|s′
|B→C| are even, so by P-preﬁx closedness of σ and τ, s′
|A→B ∈σ and s′
|B→C ∈τ. Finally,
t = s′
|A→C ∈σ; τ.
Lemma 4.4 (Zipping). If s, t ∈σ ∥τ are such that s|A→C and t|A→C have the same sequence of
O-moves, then s = t
60

Proof. Write s = s0...sm and t = t0...tn and suppose without loss of generality that m ≤n. If
s0...si−1 = t0...ti−1 = u, we distinguish on the state of the automaton (4.5) after reading u:
• 000: u|A→C is of even length and the next moves si and ti are in A or C (by lemma 4.2),
so they are O-moves in A →C and therefore equal by assumption
• 011: u|B→C is of odd length and the next moves si and ti are in B or C (by lemma 4.2), so
(usi)|B→C = u|B→Csi is a P-preﬁx of s|B→C and (uti)|B→C = u|B→Cti is a P-preﬁx of s|B→C.
Therefore u|B→Csi ∈τ and u|B→Cti ∈τ and si = ti by determinism of τ.
• 101: u|A→B is of odd length and the next moves si and ti are in A or B (by lemma 4.2),
so (usi)|A→B = u|A→Bsi is a P-preﬁx of s|A→B and (uti)|A→B = u|A→Bti is a P-preﬁx of
s|A→B. Therefore u|A→Bsi ∈σ and u|A→Bti ∈σ and si = ti by determinism of σ.
We have then s = s0...sm = t0...tm. Since s ∈σ ∥τ, s|A→C is of even length and the automaton
4.5 ends up in state 000 after t0...tm, so if n > m, tm+1 is an O-move in A →C, contradicting the
fact that s|A→C and t|A→C have the same sequence of O-moves. Therefore, m = n and s = t.
Lemma 4.5. σ; τ is deterministic
Proof. This is an immediate consequence of the previous lemma.
Lemma 4.6. σ; τ is single-threaded
Proof. We have to prove that s ∈σ; τ if and only if Threads (s) ⊆σ; τ. We prove the two
implications:
• From left to right: let s ∈σ ∥τ and let t ∈Threads
 s|A→C

.
Threads
 s|A→C

= Threads (s)|A→C so t = u|A→C for some u ∈Threads (s)
s|A→B ∈σ and Threads
 u|A→B

⊆Threads
 s|A→B

so Threads
 u|A→B

⊆σ
and we get u|A→B ∈σ. On the other hand:
s|B→C ∈τ and u|B→C ∈Threads
 s|B→C

so u|B→C ∈τ
Therefore u ∈σ ∥τ and t = u|A→C ∈σ; τ.
• From right to left: let s ∈PA→C be such that Threads (s) ⊆σ; τ. Let us write:
Threads (s) = {t1; ...; tn}
Then there are v1, ...vn ∈σ ∥τ such that ti = vi|A→C. We can construct an interleaving
u of the vi by starting with the ﬁrst move of v1, picking each time the next move of the
current vi, and changing the current vi each time we must play an O-move in A →C, by
looking at which ti the corresponding move of s is in. This interleaving is such that:
u ∈Int (A, B, C) , u|A→C = s and Threads (u) = {v1; ...; vn}
On one hand we have:
Threads
 u|A→B

= ∪iThreads
 vi|A→B

⊆σ since vi|A→B ∈σ, so u|A→B ∈σ
On the other hand we have:
Threads
 u|B→C

=

vi|B→C
	
1≤i≤n ⊆τ since vi|B→C ∈τ, so u|B→C ∈τ
Putting this together we get s = u|A→C ∈σ; τ.
61

We could also have deﬁned composition by describing only the threads of the composite,
so single-threadedness would have been immediate. However, we have here a more symmetric
deﬁnition, which is more suited when it comes to associativity of composition. Indeed, we proved
that composition of strategies is well-deﬁned. We must still prove that it is associative. Let A,
B, C and D be four arenas. We deﬁne the set of interactions on A, B, C, D by:
Int (A, B, C, D) =
(
s ∈S((A→B)→C)→D

s|A→B ∈PA→B ∧s|B→C ∈PB→C
s|C→D ∈PC→D ∧s|A→D ∈PA→D
)
We consider the following automaton on the alphabet {A; B; C; D}:
000000
001101
100110
010111
/
D
1
D
q
A

A
Q
C

C
Q
B
1
B
q
(4.6)
We can run this automaton on a sequence s ∈S((A→B)→C)→D by mapping moves to the letters
A, B, C, D, depending on the arena they belong to. The following lemma will be useful in order
to prove associativity of the composition of strategies:
Lemma 4.7. If s ∈Int (A, B, C, D) then s is accepted by the automaton (4.6). The ﬁnal state
of the automaton describes the parity of respectively s|A→B, s|B→C, s|C→D, s|A→D, s|A→C and
s|B→D (0 for even, 1 for odd). Moreover s|A→C ∈PA→C and s|B→D ∈PB→D.
Proof. It important to notice that the polarities of moves depend on which arena we consider.
The following table gives the polarities of each kind of move in A →B, B →C, C →D, A →D,
A →C and B →D:
A →B
B →C
C →D
A →D
A →C
B →D
O-move in A
P
P
P
P-move in A
O
O
O
O-move in B
O
P
P
P-move in B
P
O
O
O-move in C
O
P
O
P-move in C
P
O
P
O-move in D
O
O
O
P-move in D
P
P
P
We prove the result by induction on s. If s = ϵ, the result is immediate. If s = s′a, we distinguish
on the state of s′:
• 000000: if it appears in the corresponding arena, a must be either:
– an O-move in A →B
– an O-move in B →C
62

– an O-move in C →D
– an O-move in A →D
Using the table, the only possibilities are a P-move in A (in which case s is in state 100110,
and a is an O-move in A →C, which preserves alternation on A →C) or an O-move in
D (in which case s is in state 001101, and a is an O-move in B →D, which preserves
alternation on B →D).
• 001101: if it appears in the corresponding arena, a must be either:
– an O-move in A →B
– an O-move in B →C
– a P-move in C →D
– a P-move in A →D
Using the table, the only possibilities are an O-move in C (in which case s is in state
010111, and a is an O-move in A →C, which preserves alternation on A →C) or a
P-move in D (in which case s is in state 000000, and a is a P-move in B →D, which
preserves alternation on B →D).
• 010111: if it appears in the corresponding arena, a must be either:
– an O-move in A →B
– a P-move in B →C
– an O-move in C →D
– a P-move in A →D
Using the table, the only possibilities are an O-move in B (in which case s is in state
100110, and a is a P-move in B →D, which preserves alternation on B →D) or a P-move
in C (in which case s is in state 001101, and a is a P-move in A →C, which preserves
alternation on A →C).
• 100110: if it appears in the corresponding arena, a must be either:
– a P-move in A →B
– an O-move in B →C
– an O-move in C →D
– a P-move in A →D
Using the table, the only possibilities are an O-move in A (in which case s is in state
000000, and a is a P-move in A →C, which preserves alternation on A →C) or a P-move
in B (in which case s is in state 010111, and a is an O-move in B →D, which preserves
alternation on B →D).
In particular, with this lemma we get that if s ∈Int (A, B, C, D), then:
s|(A→B)→C ∈Int (A, B, C)
s|(A→B)→D ∈Int (A, B, D)
s|(A→C)→D ∈Int (A, C, D)
s|(B→C)→D ∈Int (B, C, D)
63

The following lemmas are the essential steps towards the associativity of composition. The ﬁrst
lemma will be used because if σ : A →B, τ : B →C and ν : C →D, then σ ∥τ ⊆Int (A, B, C)
and (σ; τ) ∥ν ⊆Int (A, C, D).
Lemma 4.8 (Double zipping 1). Let s ∈Int (A, B, C) and t ∈Int (A, C, D) such that s|A→C =
t|A→C. There exists u ∈Int (A, B, C, D) such that u|(A→B)→C = s and u|(A→C)→D = t.
Proof. The proof goes by induction on the length of s|A→C.
If s|A→C = ϵ then since s ∈
Int (A, B, C), we have s = ϵ by lemma 4.2, and so we take u = t. Otherwise we can write
s = s′av with v containing only moves in B and a a move in A →C. We have s|A→C = s′
|A→Ca.
Since s|A→C = t|A→C, we can write t = t′aw with w containing only moves in D, and we
have s′
|A→C = t′
|A→C. By induction hypothesis, there is some u′ ∈Int (A, B, C, D) such that
u′
|(A→B)→C = s′ and u′
|(A→C)→D = t′. We prove now that v = ϵ or w = ϵ. Suppose v ̸= ϵ.
Since s = s′av ∈Int (A, B, C) and v contains at least one move in B, using lemma 4.2 we get
that s′ must bring the automaton (4.5) in state 000 and a is an O-move in A →C. But then
t′a|A→C is of odd-length so t′a must bring the automaton (4.5) in state 101 (beware that here
the automaton is on A, C, D). Therefore, the ﬁrst move of w (if it existed) would be a move in
A or C, which is false since w contains only moves in D. If v = ϵ, then we take u = u′aw, and
if w = ϵ we take u = u′av.
This second lemma will be used because if σ : A →B, τ : B →C and ν : C →D, then
σ ∥(τ; ν) ⊆Int (A, B, D) and τ ∥ν ⊆Int (B, C, D). Its proof is very similar to that of the ﬁrst
lemma.
Lemma 4.9 (Double zipping 2). Let s ∈Int (A, B, D) and t ∈Int (B, C, D) such that s|B→D =
t|B→D. There exists u ∈Int (A, B, C, D) such that u|(A→B)→D = s and u|(B→C)→D = t.
Now we can prove associativity of composition:
Lemma 4.10. Let A, B, C and D be four arenas, and let σ : A →B, τ : B →C and ν : C →D.
We have (σ; τ) ; ν = σ; (τ; ν).
Proof. We prove the left-to-right inclusion, the other one being similar (using lemma 4.9).
Suppose v ∈(σ; τ) ; ν.
By deﬁnition, there exists t ∈Int (A, C, D) such that t|A→D = v,
t|A→C ∈σ; τ and t|C→D ∈ν. By deﬁnition of σ; τ, there is some s ∈Int (A, B, C) such that
s|A→C = t|A→C, s|A→B ∈σ and s|B→C ∈τ. By lemma 4.8 we get some u ∈Int (A, B, C, D) such
that u|(A→B)→C = s and u|(A→C)→D = t. Therefore, u|B→C = s|B→C ∈τ and u|C→D = t|C→D ∈ν,
so since u|(B→C)→D ∈Int (B, C, D) we get u|B→D ∈τ; ν.
Finally, u|A→B = s|A→B ∈σ and
u|A→D = t|A→D = v, so since u|(A→B)→D ∈Int (A, B, D) we get v ∈σ; (τ; ν).
Identity
If A is an arena and if s ∈SA is a justiﬁed sequence with pointer f, we deﬁne the copycat play
s∗of s in PA→A with pointer f∗. We distinguish the two occurrences of A in A →A by writing
A(1) →A(2) and the moves a of A →A by writing a(1) or a(2), depending on which occurrence
of A they are in. Given s as above, let s∗be the following sequence of moves:
s∗
2is∗
2i+1 =
(
si(2)si(1) if si is an O-move in A
si(1)si(2) if si is a P-move in A
and f∗be the following pointer function:
• if f (i) is undeﬁned then f∗(2i) is undeﬁned and f∗(2i + 1) = 2i
64

• if f (i) is deﬁned then f∗(2i) = 2f (i) + 1 and f∗(2i + 1) = 2f (i)
For example, if s is the sequence (4.4) on the arena (4.2), then s∗is:
A(2) aO
bP aO
bP
bP eO
dO
fO
eO
hP
↑
A(1)
aP bO
aP bO
bO
eP
dP
fP
eP hO
We can now deﬁne:
IdA = {s∗| s ∈SA}
Lemma 4.11. s∗is an element of PA→A such that |s∗| = 2|s| and s∗
|A(1) = s∗
|A(2) = s.
Proof. s∗∈PA→A and |s∗| = 2|s| are immediate. For s∗
|A(1) = s∗
|A(2) = s, one has to remark
that a move of s∗in A(1) will get an inverted polarity in s∗
|A(1).
Lemma 4.12. IdA is a strategy: it is P-preﬁx-closed, deterministic and single-threaded.
Proof. It is easy to see that the set of P-preﬁxes of s∗is the set of all t∗for t preﬁx of s.
Therefore IdA is P-preﬁx-closed. If s∗ab and s∗ac are in IdA, then a is either some a(1), in
which case b = c = a(2), or some a(2), in which case b = c = a(1). Therefore IdA is deterministic.
If s ∈SA, then Threads (s∗) = Threads (s)∗⊆IdA. Conversely, let s ∈PA be a P-play such
that Threads (s) ⊆IdA. Consider a sequence in s of an O-move followed by a P-move. Since
these moves are in the same thread (see the remark before the deﬁnition of a strategy) and this
thread is in IdA, the sequence is of the form a(1)a(2) or a(2)a(1). Therefore we get s ∈IdA.
Lemma 4.13. If σ : A →B, then IdA; σ = σ = σ; IdB.
Proof. Let t ∈IdA; σ. By deﬁnition of composition, there exists some s ∈Int
 A(1), A(2), B

such that s|A(1)→A(2) ∈IdA, s|A(2)→B ∈σ and s|A(1)→B = t. By property of IdA, s|A(1) = s|A(2),
so t = s|A(1)→B = s|A(2)→B ∈σ. Let now t ∈σ. We build s ∈S(A(1)→A(2))→B by replacing every
move a of t which is in A by a(2)a(1) if it is an O-move in A or by a(1)a(2) if it is a P-move in
A. We check that by construction, s ∈Int
 A(1), A(2), B

, s|A(1) = s|A(1) so s|A(1)→A(2) ∈IdA
and s|A(1)→B = s|A(2)→B = t ∈σ. The second equality σ = σ; IdB is proved similarly.
4.3.2
Countable products
If (Ai)i∈I is a countable family of arenas consisting of the trees (Ai,j)j∈Ji, if i0, ..., in, ... is an
enumeration of I, and if ji,0, ..., ji,p, ... are enumerations of the Ji, then the arena Q
i∈I Ai is
deﬁned as the disjoint sum:
Ai0,ji0,0
· · ·
Ai0,ji0,p
· · ·
Ain,jin,0
· · ·
Ain,jin,p
· · ·
This is correct since this arena is still countably-branching. We deﬁne the projection proji0 :
Q
i∈I Ai →Ai0 = IdAi0. This is correct since Ai0 →Ai0 ⊆Q
i∈I Ai →Ai0, so a P-play on
Ai0 →Ai0 is a P-play on Q
i∈I Ai →Ai0. If σi : C →Ai, we deﬁne the product (σi)i∈I : C →
Q
i∈I Ai by its set of threads:
Threads
 (σi)i∈I

=
[
i∈I
Threads (σi)
65

This deﬁnition is correct because since C →Ai ⊆C →Q
i∈I Ai and the roots of C →Ai are
roots of C →Q
i∈I Ai, a thread on C →Ai which is a P-play is a thread on C →Q
i∈I Ai which
is a P-play. Moreover, since the sets of roots of the C →Ai are disjoint, determinism of (σ)i∈I
is a consequence of the determinism of the σi.
Lemma 4.14. The constructions above deﬁne a product on G.
Proof. First, we prove (σi)i∈I ; proji0 = σi0 for σi : C →Ai:
(σi)i∈I ; proji0 =













s|C→Ai

s ∈Int
 
C,
Y
i∈I
Ai, Ai0
!
s|C→Q
i∈I Ai ∈(σi)i∈I
s| Q
i∈I Ai→Ai0 ∈proji0













=













s|C→Ai

s ∈Int
 
C,
Y
i∈I
Ai, Ai0
!
Threads

s|C→Q
i∈I Ai

⊆(σi)i∈I
∀i ̸= i0, s|Ai = ϵ ∧s|Ai0→Ai0 ∈IdAi0













=









s|C→Ai0
(2)

s ∈Int

C, Ai0
(1), Ai0
(2)
Threads

s|C→Ai0
(1)

⊆σi0
s|Ai0
(1)→Ai0
(2) ∈IdAi0









=









s|C→Ai0
(2)

s ∈Int

C, Ai0
(1), Ai0
(2)
s|C→Ai0
(1) ∈σi0
s|Ai0
(1)→Ai0
(2) ∈IdAi0









= σi0; IdAi0
= σi0
Now, we prove (σ; proji)i∈I = σ for σ : C →Q
i∈I Ai:
Threads
 (σ; proji)i∈I

=
[
i∈I
Threads (σ; proji)
σ; proji0 =













s|C→Ai0

s ∈Int
 
C,
Y
i∈I
Ai, Ai0
!
s|C→Q
i∈I Ai ∈σ
s| Q
i∈I Ai→Ai0 ∈proji0













=













s|C→Ai0

s ∈Int
 
C,
Y
i∈I
Ai, Ai0
!
s|C→Q
i∈I Ai ∈σ
∀i ̸= i0, s|Ai = ϵ ∧s|Ai0→Ai0 ∈IdAi0













66

=









s|C→Ai0
(2)

s ∈Int

C, Ai0
(1), Ai0
(2)
s|C→Ai0
(1) ∈σ ∩SC→Ai0
s|Ai0
(1)→Ai0
(2) ∈IdAi0









=

σ ∩SC→Ai0

; IdAi0
= σ ∩SC→Ai0
therefore:
Threads
 (σ; proji)i∈I

=
[
i∈I
Threads (σ ∩SC→Ai)
=
[
i∈I
(Threads (σ) ∩SC→Ai)
= Threads (σ) ∩
[
i∈I
SC→Ai
Now, since the set of roots of C →Q
i∈I Ai is the disjoint union of the roots of C →Ai,
every thread on C →Q
i∈I Ai is in one of the SC→Ai. Finally we get Threads
 (σ; proji)i∈I

=
Threads (σ) and so (σ; proji)i∈I = σ.
It is clear that the empty product and terminal object 1 of G is the empty arena U.
4.3.3
Closed structure
If A and B are two arenas, then the exponential BA is just the arena A →B. In G, the arenas
(A →B) × A →B and (A →B) →(A →B) are identical, so the strategy IdA→B is a strategy
on the arena (A →B) × A →B. We deﬁne then the evaluation morphism ev = IdA→B. Let
now σ : C × A →B. Again, in G the arenas C × A →B and C →A →B are identical, so
σ : C →BA and we can deﬁne Λ (σ) = σ. Therefore, the commutativity of the diagram is just
a consequence of σ; IdA→B = σ.
4.4
Interpreting µPCF in game semantics
4.4.1
G as a category of continuations
In this section we describe how the category G of unbracketed single-threaded HON games can
be seen as a category of continuations. Recall from section 3.2.2 that since G is a cartesian closed
category with countable products, for any arena A ∈Ob (G) the full subcategory {A}Fam(G) of
Fam (G) is a category of continuations. Here we write:
C = Fam (G)
we write V for the one-move arena, and we ﬁx the object R of C to be the singleton family {V}.
The full subcategory RC = {V}Fam(G) of C = Fam (G) is then a category of continuations.
To deﬁne the interpretation of µPCF in RC we need to deﬁne an object of continuations
JTK ∈Ob (C) for each sort T, the object of computations being then deﬁned as [T] = RJTK ∈
Ob
 RC
. Then we also need to deﬁne for each constant c : T a morphism in RC from 1 = R0 to
[T]. The interpretation of λµ-calculus is then such that a λµ-term of type σ with free λ-variables
of type U1, ..., Un and free µ-variables of type V1, ..., Vm is interpreted as a morphism in RC from
[U1] × ... × [Un] to [T] ` [V1] ` ... ` [Vm]. However, we would like the realizers to be strategies,
that is, morphisms in G. The following lemma ﬁlls the gap.
67

Lemma 4.15. The category RC = RFam(G) is isomorphic to the category G
Proof. The only non-trivial part is that any singleton family {A} in Fam (G) can be written as
R{Ai | i∈I}. Let (ai)i∈I be the set of roots of A and let Ai be the forest obtained by removing the
move ai from the tree rooted in ai. Then by deﬁnition of the arrow and product in G and since
V is the one-move arena, the arena Q
i∈I VAi is equal to A. But by deﬁnition of the exponential
in Fam (G), we have:
(Y
i∈I
VAi
)
= {V}{Ai | i∈I} = R{Ai | i∈I}
Therefore any singleton family in C is in fact an object of RC. We deﬁne now a functor F from
G to RFam(G). It is deﬁned on objects as F (A) = {A} (singleton family, so in RC). If σ : A →B
is a morphism (strategy) in G then F (σ) is deﬁned as:
F (σ) =
 Id{∗} : {∗} →{∗}, {σ : A →B}

: {A} →{B}
which is a morphism in RC since {A} and {B} are objects of RC as singleton families, and RC
is a full sub-category. Then it is straightforward to show that F is indeed a functor, and that it
is bijective on objects and on hom-sets.
Under this isomorphism, the countable product in RFam(G) is as follows:
Y
i∈I
R{Ai,j | j∈Ji} = R{Ai,j | (i,j)∈P
i∈I Ji}
which is a correct deﬁnition since P
i∈I Ji is countable if I and the Ji are.
This isomorphism between RFam(G) and G allows to see G as a category of continuations. As
an example, the arena (4.2):
a
f
b
c
g
h
d
e
is the exponential of R = {V} by the following family:
(
b
c
d
e
;
g
h
)
Another way to look at it is that the arena (4.2) can be written as (A1 →V)×(A2 →V), where
A1 and A2 are the two arenas above. In the following we will not distinguish between RC and
G, and we will for example consider that an object RA of RC is an object of G. In particular,
if A and B are arenas, then A ` B is also an arena. For example, if A and B are the following
arenas:
A
B
a
f
b
c
g
h
d
e
i
k
j
l
m
n
68

then the arena A ` B is:
ai
ak
fi
fk
b
c
j
b
c
l
g
h
j
g
h
l
d
e
d
e
m
n
m
n
Another way to see it is that if:
A = R{Ai | i∈I} =
Y
i∈I
(Ai →V)
and
B = R{Bj | j∈J} =
Y
j∈J
(Bj →V)
then:
A ` B = R{Ai×Bj | (i,j)∈I×J} =
Y
(i,j)∈I×J
(Ai × Bj →V)
Therefore Q
(i,j)∈I×J (Ai →V) = Q
j∈J A is a subarena of A ` B, but if s ∈SA`B, we will write
s|A ∈SA by forgetting for each move of s in A its J-index. We give now the example of an
interpretation of a λµ-term:
Lemma 4.16. The strategy [⊢λx. [α] x : T →0 | α : T] is the identity strategy on [T].
Proof. For simplicity, we suppose that [T] has a unique root:
[T] =
q
T
First, observe that the arenas [T →0] and [T] are:
[T] →[0]
[T]
q
q
T
q
T
so the arena ([T] →[0]) ` [T], obtained by merging the roots of [T] →[0] and [T], is actually
equal to [T] →[T]:
q
q
T
T
then [x : T ⊢[α] x : 0 | α : T] is obtained by composing:
[T]
w1 / [T] ` [T]
∇
/ [T]
r
/ [0] ` [T]
where w1 is the copycat strategy between [T] on the left and the left copy of [T] in [T] ` [T]
on the right, ∇merges the plays on [T] ` [T] into [T], and r is just the identity strategy since
[T] = [0] ` [T] in G. Therefore the composite [x : T ⊢[α] x : 0 | α : T] = w1; ∇; r is the identity
strategy on [T] and since the currying in G is transparent, [⊢λx. [α] x : T →0 | α : T] is the
identity strategy on [T].
69

4.4.2
Natural numbers, successor and predecessor
In order to interpret µPCF in G, we have to provide an object of continuations in C for each
base type, a morphism in RC from 1 = R0 to [T] for each constant c : T, and we have to verify
that all the equations of µPCF are veriﬁed in G. We deﬁne the family N = {Un | n ∈N} where
U is the empty arena. The continuation object for the unique base type ι is then:
JιK = RN
By writing out the deﬁnition of the exponential in C and using the isomorphism between G and
RC, we get that [ι] is the usual ﬂat arena of natural numbers:
q
0
· · ·
n
· · ·
We now move to the deﬁnition of the interpretations of the constants. In order to deﬁne the
interpretations of the constants manipulating natural numbers we use the continuation monad on
C, mapping an object A to its double exponentiation by R: TA = RRA. We write ηA : A →TA
and µA : TTA →TA for the unit and the multiplication. We have:
[ι] = RJιK = RRN = TN
The set of morphisms in C from 1 to N is isomorphic to the set N of natural numbers since there
is only one morphism in 1 →1 = 1 and the index set of N is N. We write ϕn : 1 →N for the
inverse image of n ∈N through this isomorphism. We write n = ϕn; ηN : 1 →[ι], which is a
morphism in G ≈RC since 1 = R0 ∈Ob
 RC
and [ι] = RJιK ∈Ob
 RC
. n is the usual strategy
for natural number n ∈N:
Lemma 4.17. n is the usual strategy for the natural number n on [ι]: it answers n to the unique
question of opponent.
Proof. n = ϕn; ηN is obtained by currying:
RN × 1
IdRN×ϕn

 Q
i∈N VUi
× U
	

∗7→n,IdQ
i∈N VUi ×IdU


RN × N
ev

 Q
i∈N VUi
× Uj
 j ∈N
	

j7→j,
n
projj×IdUj
 j∈N
o


VUj × Uj
 j ∈N
	
(j7→∗,{ev| j∈N})

R
{V}
If we follow the path on the right, we get the simple composition:
 Q
i∈N VUi
× U
projn×IdU / VU × U
ev
/ V
70

In G,
 Q
i∈N VUi
× U = Q
i∈N V, VU × U = V and the above composition is just the n-th
projection. As a strategy, it therefore answers in the n-th component of Q
i∈N V to the question
in V, which is the behavior of the usual strategy for the n-th natural number.
We deﬁne:
[n] = n
Similarly, the set of morphisms in C from N to N is isomorphic to the set NN of functions from
natural numbers to natural numbers. We write ϕf : N →N for the inverse image of a function
f : N →N through this isomorphism, so in particular ϕn; ϕf = ϕf(n) for any n ∈N. We
have Tϕf : [ι] →[ι], which is a morphism in G ≈RC since [ι] = RJιK ∈Ob
 RC
and we write
f = Λ (proj2; Tϕf) which is a morphism in G from 1 to [ι][ι]. Since T is a monad, we have the
following lemma:
Lemma 4.18. Let f : N →N, n ∈N and m = f (n). Then we have pair (f, n); ev = m in
G ≈RC
Proof. The following diagram in C commutes, from the facts that η is a natural transformation
and the universal properties of product and exponentiation ([ι][ι] exists since [ι] is a singleton
family):
T1
Tϕn
{
1
η1
/
ϕn
/
pair(Id1,ϕn;ηN)

N
ηN

1 × [ι]
proj2 /
Λ(proj2;Tϕf)×Id[ι] 
[ι]
Tϕf

[ι][ι] × [ι]
ev / [ι]
Since ϕn; ϕf = ϕf(n) = ϕm we get:
pair (f, n); ev = pair (Λ (proj2; Tϕf) , ϕn; ηN); ev
= pair (Id1, ϕn; ηN);
 Λ (proj2; Tϕf) × Id[ι]

; ev
= η1; Tϕn; Tϕf
= η1; T (ϕn; ϕf)
= η1; Tϕf(n)
= η1; Tϕm
= ϕm; ηN
the last equality coming from the naturality of η.
By deﬁnition m = ϕm; ηN so we have
pair (f, n); ev = m in C, and therefore in G ≈RC since 1 = R0 ∈Ob
 RC
and [ι] = RJιK ∈
Ob
 RC
.
We deﬁne the interpretations of the predecessor and successor of µPCF as:
[pred] = pred
[succ] = succ
where pred and succ are the usual predecessor and successor functions on N. The soundness of
the deﬁning equations of pred and succ are a consequence of the above lemma, since [M N] =
pair ([M] , [N]); ev.
71

4.4.3
Conditionals
In order to deﬁne the interpretation of if0, we ﬁrst deﬁne a morphism in G ≈RC from [ι] to
RR×R using the continuation monad T, and then we use the fact that G ≈RC is a category of
continuations to deﬁne a morphism from [0 × 0 →0] = RR×R to [T →T →T] by a λµ-term of
type T →T →T with one free λ-variable of type 0 × 0 →0. Combining these two morphisms
we obtain a morphism from 1 to [ι →T →T →T]. First, the set of morphism in C from N to
1 + 1 is isomorphic to the set of set-theoretic functions from N to the two-element set {▼; ▲}.
Let z be the set-theoretic function that maps 0 to ▼and every n ̸= 0 to ▲and ϕz be the
corresponding morphism in C from N to 1 + 1. Then Tϕz is a morphism in G ≈RC from [ι] to
R(R1+1) = RR×R = [0 × 0 →0]. On the other hand we deﬁne:
ψ = [x : 0 × 0 →0 ⊢λyz.µα.x ⟨[α] y, [α] z⟩: T →T →T] : [0 × 0 →0] →[T →T →T]
Finally we deﬁne:
[if0] = Λ (proj2; Tϕz; ψ) : 1 →[ι →T →T →T]
In order to prove the soundness of the equations deﬁning if0, we ﬁrst observe that the following
diagram commutes:
1
pair(Id1,n)
u
n

ϕn
'
1 × [ι]
Λ(proj2;Tϕz;ψ)×Id[ι]

proj2
/ [ι]
Tϕz

N
ηN
o
ϕz

[0 × 0 →0]
ψ

1 + 1
η1+1
o
[T →T →T][ι] × [ι]
ev / [T →T →T]
Therefore we have [if0 n] = pair (Λ (proj2; Tϕz; ψ) , n); ev = ϕn; ϕz; η1+1; ψ. The morphism
η1+1 from 1 + 1 to R(R1+1) = RR×R = [0 × 0 →0] can be described as:
η1+1 =
 
{▼; ▲} −→{∗}
_ 7−→∗
,
(
σ▼= [λx.p1 x] : 1 →[0 × 0 →0]
σ▲= [λx.p2 x] : 1 →[0 × 0 →0]
)!
: 1 + 1 →R(R1+1)
The morphisms ϕ0; ϕz and ϕn+1; ϕz are by deﬁnition of z:
ϕ0; ϕz =
 
{∗} −→{▼; ▲}
∗7−→▼
, {{ϵ} : 1 →1}
!
: 1 →1 + 1
ϕn+1; ϕz =
 
{∗} −→{▼; ▲}
∗7−→▲
, {{ϵ} : 1 →1}
!
: 1 →1 + 1
Therefore we have:
ϕ0; ϕz; η1+1 = [λx.p1 x] : 1 →RR×R
and
ϕn+1; ϕz; η1+1 = [λx.p2 x] : 1 →RR×R
so:

if0 0

= ϕ0; ϕz; η1+1; ψ
72

= [λx.p1 x] ; [x : 0 × 0 →0 ⊢λyz.µα.x ⟨[α] y, [α] z⟩: T →T →T]
= [λyz.µα. (λx.p1 x) ⟨[α] y, [α] z⟩]
= [λyz.µα.p1 ⟨[α] y, [α] z⟩]
= [λyz.µα. [α] y]
= [λyz.y]
and similarly

if0 n + 1

= [λyz.z]. Therefore we get

if0 0 M N

= [M] and

if0 n + 1 M N

=
[N].
4.4.4
Fixed point operators
In order to deﬁne the interpretation of the ﬁxed point operators of µPCF, we ﬁrst prove that
for any strategy σ on the arena A →A there exists a strategy σ on A such that σ; σ = σ. Then
we perform this construction on a well-chosen σ to get a general ﬁxed point operator. Let σ be
a strategy on the arena A →A. We deﬁne a sequence (σn)n∈N by induction:
σ0 = {ϵ}
σn+1 = σn; σ
First, we prove that this sequence is increasing:
Lemma 4.19. For any n ∈N, σn ⊆σn+1
Proof. We write A(1) →A(1) for the arena of σ. We prove the result by induction on n:
• n = 0: σ0 = {ϵ} and the empty play ϵ is in any strategy.
• n + 1: Let s ∈σn+1 = σn; σ. By deﬁnition of composition of strategies, there exists
some t ∈σ such that t|A(1) ∈σn and t|A(2) = s. By induction hypothesis we then have
t|A(1) ∈σn+1, and therefore s = t|A(2) ∈σn+2 = σn+1.
We then deﬁne:
σ ∆=
[
n∈N
σn
Lemma 4.20. σ is a strategy on A
Proof. Since the σn are strategies, it is easy to check that σ is a P-preﬁx-closed set of ﬁnite
P-plays. If sa, sb ∈σ there are m, n ∈N such that sa ∈σm and sb ∈σn, therefore sa, sb ∈
σmax(m,n) and a = b. If s ∈σ, then s ∈σn for some n ∈N, so Threads (s) ⊆σn ⊆σ. Conversely,
if s is a ﬁnite P-play on A and if Threads (s) ⊆σ, then since s is ﬁnite, Threads (s) is ﬁnite
and since (σn)n∈N is increasing, there is some n ∈N such that Threads (s) ⊆σn and therefore
s ∈σn ⊆σ.
We prove now that it deﬁnes a ﬁxed point of σ.
Lemma 4.21. σ; σ = σ
Proof. We prove the double inclusion. Let s ∈σ; σ. By deﬁnition of composition of strategies,
there exists t ∈σ such that t|A(1) ∈σ and t|A(2) = s. By deﬁnition of σ, there is some n ∈N
such that t|A(1) ∈σn. Then we get s ∈σn; σ = σn+1 ⊆σ. For the other direction, let now s ∈σ.
By deﬁnition of σ, there is some n ∈N such that s ∈σn. By monotonicity of (σn)n∈N, we get
s ∈σn+1 = σn; σ, so by deﬁnition of composition there is some t ∈σ such that t|A(1) ∈σn and
t|A(2) = s. But then t|A(1) ∈σ, and so s = t|A(2) ∈σ; σ.
73

We now deﬁne the general ﬁxed point operator:
[Y] = [λxy.y (x y)]
By the above lemma, we have:
[Y] = [λxy.y (x y)] [Y] = [λy.y (Y y)]
And therefore the deﬁning equation of Y is veriﬁed in G:
[Y M] = [λy.y (Y y) M] = [M (Y M)]
74

Chapter 5
An orthogonality-based realizability
model for classical analysis
In this chapter, we deﬁne a realizability model which is based on the duality between realizers
and counter-realizers. After deﬁning the realizability relation, we prove its adequacy for ﬁrst-
order logic, Peano arithmetic and the axiom of choice. Finally, we use our orthogonality-based
model to extract computational content from classical proofs.
5.1
The realizability relation
5.1.1
Negative translation and orthogonality
The ﬁrst realizability models for classical logic were obtained by combining Gödel’s negative
translation with intuitionistic realizability, see e.g. [Koh08]. Gödel’s negative translation from
Peano arithmetic PAω
= (equivalent to PAω) to HAω
= (see section 2.4.2) maps a formula A to A¬
by preﬁxing inductively all the positive connectives and atomic predicates of A with a double
negation. We have the following result: if PAω
= |∼A, then HAω
= |∼A¬, relying on the fact that
for every axiom A of PAω
=, HAω
= |∼A ⇒A¬. Therefore, a realizability model for PAω
= can be
obtained from a realizability model for HAω
= using Gödel’s negative translation. Concerning
the extraction of witnesses, if PAω
= |∼∃xι (t = 0), then HAω
= |∼¬¬∃xι ¬¬ (t = 0) from which
we easily get HAω
= |∼¬¬∃xι (t = 0). While in usual intuitionistic realizability the formula ⊥
has no realizer so the model is sound, Friedman’s trick is to allow ⊥to have realizers. If we
then take the realizers of ⊥to be the same as those of ∃xι (t = 0), then combining the proof of
¬¬∃xι (t = 0) with the identity gives a realizer of ∃xι (t = 0), and therefore the witness.
In Krivine’s [Kri09] classical realizability models this double step is avoided through the use
of orthogonality in system F. In these models there is a set of terms Λ and a set of stacks Π.
Each formula has a set of realizers (the truth value, subset of Λ) and a set of counter-realizers
(the falsity value, subset of Π). The falsity values are primitive, an orthogonality relation is
deﬁned between Λ and Π, and the truth values are deﬁned as the orthogonals of the falsity
values, so they are orthogonally closed.
Here we work in a typed setting so we must choose the types of realizers and counter-realizers
so they can interact. Recall from section 4.4.1 that:
C = Fam (G)
R = {V}
G ≃RC = RFam(G)
Given a formula A, the set of realizers of A would normally be a set of morphisms in G from
the terminal object 1 to the interpretation of A: [A∗] = RJA∗K, and under the duality between
75

terms and contexts of λµ-calculus, a natural choice for the counter-realizers of A is a set of
morphisms in C from 1 to JA∗K. Then we can combine a potential realizer of A with a potential
counter-realizer using the evaluation morphism ev : RJA∗K ×JA∗K →R so we obtain a morphism
from 1 to R. However, since in G there is only one such morphism (the empty strategy), it is not
possible to check whether the potential realizer and counter-realizer are orthogonal to each other
by observing this composite. The solution adopted here is to rely on Friedman’s trick directly
in the deﬁnition of the realizability relation, and to deﬁne an orthogonality relation relying on
an artiﬁcially added output channel.
Formally we add a µ-variable in the process of interpreting logic in λµ-calculus. A proof:
π
A1, ..., An ⊢A | B1, ..., Bm
is translated to a λµ-term:
x1 : A1∗, ..., xn : An∗⊢π∗: A∗| α1 : B1∗, ..., αn : Bm∗, κ : ς
where ς is some ﬁxed base sort, by applying the (admissible) rule of right weakening of λµ-
calculus. The µ-variable κ is, intuitively, a continuation variable which can be used by a realizer
to stop computation and give an answer. Apart from its use in the deﬁnition of the realizability
relation, this feature will also be used in the proof of the extraction result. After translating the
proof π to a λµ-term π∗, we interpret it in G as a morphism:
[π∗] ∈G ([A1∗] × ... × [An∗], [A∗] ` [B1∗] ` ... ` [Bm∗] ` [ς])
In the particular case of empty contexts, [π∗] is a morphism in G (1, [A∗] ` [ς]), and we therefore
choose the potential realizers of a closed formula A to be such morphisms. As explained in
section 3.2.3 and by taking the convention that these morphisms have a free µ-variable κ of
type ς, we use the syntax of λµ-calculus (and possibly [κ] and µκ) to manipulate these. We
also substitute morphisms of C for µ-variables, as in section 3.2.5, so if σ ∈G (1, [A∗] ` [ς]) is a
potential realizer, and if φ ∈C (JςK, JA∗K), then [φ] σ ∈G (1, [⊥∗] ` [ς]) and µκ. [φ] σ ∈G (1, [ς]),
that is a strategy on the ﬂat arena for the set ςM. Since the arena [ς] is (in the general case)
larger than the one-move arena [0] = R, we can now deﬁne when φ is orthogonal to σ by
looking at µκ. [φ] σ. Potential counter-realizers of A are therefore chosen to be morphisms in
C (JςK, JA∗K).
In [BR13] the goals of orthogonally-deﬁned realizability and extraction were achieved by
taking realizers of a formula A to be interpretations of closed terms of type JςK →[A∗]. However,
this choice implied that realizers were passing a continuation k to each other, leading to an
unnecessary complex interpretation.
5.1.2
Truth values, falsity values
We ﬁx a ﬁrst-order signature Σ and a Σ-structure M such that ιM is countable for each base sort
ι. We also ﬁx a corresponding λµ signature and a type P ∗in this signature for each predicate
P of Σ, as in section 3.1.3. The interpretation of λµ-calculus in the category of continuations G
(see section 4.4.1) is as in section 3.2.3, the base types ι of λµ-calculus being interpreted in G
as the arenas:
JιK ∆= R{Ua| a∈ιM}
where U is the empty arena, so [ι] = RJιK is the ﬂat arena corresponding to the set ιM:
q
a
b
...
76

and we suppose given for each constant c : T ∈Cst a strategy interpreting it:
c : 1 →[T]
Since as stated in section 3.2.3 we use the syntax of λµ-calculus to describe strategies of G, we
will omit the interpretation brackets. All the λµ-terms that we write from now on are to be
understood as morphisms in G.
In order to build our realizability relation by orthogonality, and later on to perform extraction
on Π0
2 formulas using Friedman’s trick, our model is parameterized with a set
⊥⊥⊆ςM
which is, intuitively, the set of “correct” values that can be output through the variable κ. We
deﬁne the corresponding strategies:
|⊥⊥| = {a| a ∈⊥⊥} ⊆G (1, [ς])
where a is the strategy which answers a to the unique question of opponent.
We now deﬁne the set of realizers of a formula, that we call its truth value. We ﬁx for each
predicate P of Σ and each a1, ..., an ∈T1M × ... × TnM a set of morphisms |P (a1, ..., an)| ⊆
G (1, [P ∗] ` [ς]). We extend this to every closed formula A on Σ with parameters in M by:
|A ∧B| = {σ| p1 σ ∈|A| ∧p2 σ ∈|B|}
∀xT A
 =
\
a∈T M
|A {a/x}|
|A ⇒B| = {σ| ∀τ ∈|A| , σ τ ∈|B|}
|⊥| = {σ| µκ.σ ∈|⊥⊥|}
so |A| ⊆G (1, [A∗] ` [ς]). Remark that contrary to [Kri09], we do not deﬁne the truth values as
the orthogonals of the falsity values. Since our logic is ﬁrst-order and our basic logical connectives
are negative, the truth value of every formula is already orthogonally closed. However, in the
adequacy lemma we give a realizability interpretation for sequents A1, ..., An ⊢A | B1, ..., Bm, in
which the | and the “,” between the Bi are to be interpreted as disjunctions. Since in categories
of continuations, disjunction between A and B is interpreted as A`B = RA×B that is a negated
object, we deﬁne its realizability interpretation by orthogonality. That is why we deﬁne for every
formula A with parameters the falsity value of A: ∥A∥⊆C (JςK, JA∗K) (where C = Fam (G)).
We suppose given for each predicate P of Σ and each a1, ..., an ∈T1M × ... × TnM a set of
morphisms ∥P (a1, ..., an)∥⊆C (JςK, JP ∗K) and we extend it to every formula A with parameters
in M by:
∥A ∧B∥= {in1 φ| φ ∈∥A∥} ∪{in2 φ| φ ∈∥B∥}
∥A ⇒B∥= {⟨eσ, φ⟩| σ ∈|A| ∧φ ∈∥B∥}
∀xT A
 =
[
a∈T M
∥A {a/x}∥
∥⊥∥= {∗}
so ∥A∥⊆C (JςK, JA∗K).
As explained in section 3.2.4, we use here the syntax of λR×+ to
manipulate morphisms in C. We deﬁne now an orthogonality relation between G (1, [A∗] ` [ς])
and C (JςK, JA∗K): if σ ∈G (1, [A∗] ` [ς]) and φ ∈C (JςK, JA∗K), then µκ. [φ] σ ∈G (1, [ς]), so we
deﬁne:
σ ⊥φ
∆=
µκ. [φ] σ ∈|⊥⊥|
The following lemma states that the truth values are indeed the orthogonals of the falsity values
deﬁned above:
77

Lemma 5.1. Suppose that for every predicate P of Σ and every a1, ..., an ∈T1M × ... × TnM,
|P (a1, ..., an)| = {σ| ∀φ ∈∥P (a1, ..., an)∥, σ ⊥φ}. Then for every closed formula A with pa-
rameters in M:
|A| = {σ| ∀φ ∈∥A∥, σ ⊥φ}
Proof. We prove this result by induction on the structure of the formula:
• P (a1, ..., an): this is an assumption of the lemma
• ⊥: if σ ∈G (1, [⊥∗] ` [ς]) then:
σ ∈|⊥| ⇔µκ.σ ∈|⊥⊥| ⇔µκ. [∗] σ ∈|⊥⊥| ⇔σ ⊥∗
from which we conclude since ∥⊥∥= {∗}.
• A ∧B: if σ ∈G (1, ([A∗] × [B∗]) ` [ς]), φ1 ∈C (JςK, JA∗K) and φ2 ∈C (JςK, JB∗K), then
µκ. [ini φi] σ = µκ. [φi] pi σ, therefore:
pi σ ⊥φi ⇔µκ. [φi] pi σ ∈|⊥⊥| ⇔µκ. [ini φi] σ ∈|⊥⊥| ⇔σ ⊥ini φi
and ﬁnally:
σ ∈|A ∧B| ⇔
(
p1 σ ∈|A|
p2 σ ∈|B|
⇔
(
∀φ1 ∈∥A∥, p1 σ ⊥φ1
∀φ2 ∈∥B∥, p2 σ ⊥φ2
by induction hypothesis
⇔
(
∀φ1 ∈∥A∥, σ ⊥in1 φ1
∀φ2 ∈∥B∥, σ ⊥in2 φ2
⇔∀φ ∈∥A ∧B∥, σ ⊥φ
• A ⇒B: if σ ∈G

1, [B∗][A∗] ` [ς]

, τ ∈G (1, [A∗] ` [ς]) and φ ∈C (JςK, JB∗K), then
µκ. [⟨eτ, φ⟩] σ = [φ] σ τ, therefore:
σ τ ⊥φ ⇔µκ. [φ] σ τ ∈|⊥⊥| ⇔µκ. [⟨eτ, φ⟩] σ ∈|⊥⊥| ⇔σ ⊥⟨eτ, φ⟩
and ﬁnally:
σ ∈|A ⇒B| ⇔∀τ ∈|A| , σ τ ∈|B|
⇔∀τ ∈|A| , ∀φ ∈∥B∥, σ τ ⊥φ
by induction hypothesis
⇔∀τ ∈|A| , ∀a ∈∥B∥, σ ⊥⟨eτ, a⟩
⇔∀φ′ ∈∥A ⇒B∥, τ ⊥φ′
• ∀xT A: if σ ∈G (1, [A∗] ` [ς]), then:
σ ∈
∀xT A
 ⇔∀a ∈T M, σ ∈|A {a/x}|
⇔∀a ∈T M, ∀φ ∈∥A {a/x}∥, σ ⊥φ
⇔∀φ ∈C (JςK, JA∗K) ,
 ∃a ∈T M, φ ∈∥A {a/x}∥

⇒σ ⊥φ
⇔∀φ ∈
∀xT A
 , σ ⊥φ
78

5.2
Adequacy
5.2.1
Adequacy for ﬁrst-order logic
We now have all the necessary material to give the adequacy lemma. For that we suppose that
the interpretations of the terms associated to the axioms are realizers of these axioms: for every
A ∈Ax, MA ∈|A|. The adequacy lemma is then:
Lemma 5.2. Suppose π is a proof of Γ ⊢A | ∆with FV (Γ, A, ∆) ⊆⃗y ⃗T , so:
⃗x : Γ∗⊢π∗: A∗| ⃗α : ∆∗, κ : ς
then for any ⃗a ∈⃗T M,⃗σ ∈|Γ {⃗a/⃗y}| , ⃗φ ∈∥∆{⃗a/⃗y}∥, we have:
π∗n
⃗σ/⃗x, ⃗φ/⃗α
o
∈|A {⃗a/⃗y}|
In particular if A is a closed formula, π∗∈|A|
Proof. By induction on the proof tree:
• π = Γ, A ⊢A | ∆:
π∗= ⃗x : Γ∗, z : A∗⊢z : A∗| ⃗α : ∆∗, κ : ς
Let ⃗a ∈⃗T M, ⃗σ ∈|Γ {⃗a/⃗y}|, τ ∈|A {⃗a/⃗y}| and ⃗φ ∈∥∆{⃗a/⃗y}∥. Then we have:
π∗n
⃗σ/⃗x, τ/z, ⃗φ/⃗α
o
= τ ∈|A {⃗a/⃗y}|
• π =
(A∈Ax)
Γ ⊢A | ∆
:
π∗= ⃗x : Γ∗⊢MA : A∗| ⃗α : ∆∗, κ : ς
Let ⃗a ∈⃗T M, ⃗σ ∈|Γ {⃗a/⃗y}| and ⃗φ ∈∥∆{⃗a/⃗y}∥. Then we have:
π∗n
⃗σ/⃗x, ⃗φ/⃗α
o
= MA ∈|A| = |A {⃗a/⃗y}|
since A is closed and MA ∈|A| by assumption.
• π =
π0
Γ, A ⊢B | ∆
Γ ⊢A ⇒B | ∆
:
π∗= ⃗x : Γ∗⊢λx.π0∗: A∗→B∗| ⃗α : ∆∗, κ : ς
Let ⃗a ∈⃗T M, ⃗σ ∈|Γ {⃗a/⃗y}| and ⃗φ ∈∥∆{⃗a/⃗y}∥. Then for any τ ∈|A {⃗a/⃗y}| we have:

π∗n
⃗σ/⃗x, ⃗φ/⃗α
o
τ = π0∗n
⃗σ/⃗x, τ/y, ⃗φ/⃗α
o
∈|B {⃗a/⃗y}|
by induction hypothesis, since τ ∈|A {⃗a/⃗y}|. Therefore:
π∗n
⃗σ/⃗x, ⃗φ/⃗α
o
∈|(A ⇒B) {⃗a/⃗y}|
79

• π =
π1
Γ ⊢A ⇒B | ∆
π2
Γ ⊢A | ∆
Γ ⊢B | ∆
:
π∗= ⃗x : Γ∗⊢π1∗π2∗: B∗| ⃗α : ∆∗, κ : ς
Let ⃗a ∈⃗T M, ⃗σ ∈|Γ {⃗a/⃗y}| and ⃗φ ∈∥∆{⃗a/⃗y}∥. Then we have:
π∗n
⃗σ/⃗x, ⃗φ/⃗α
o
=

π1∗n
⃗σ/⃗x, ⃗φ/⃗α
o 
π2∗n
⃗σ/⃗x, ⃗φ/⃗α
o
∈|B {⃗a/⃗y}|
since by induction hypothesis:
π1∗n
⃗σ/⃗x, ⃗φ/⃗α
o
∈|(A ⇒B) {⃗a/⃗y}| and π2∗n
⃗σ/⃗x, ⃗φ/⃗α
o
∈|A {⃗a/⃗y}|
• π =
π1
Γ ⊢A | ∆
π2
Γ ⊢B | ∆
Γ ⊢A ∧B | ∆
:
π∗= ⃗x : Γ∗⊢⟨π1∗, π2∗⟩: A∗× B∗| ⃗α : ∆∗, κ : ς
Let ⃗a ∈⃗T M, ⃗σ ∈|Γ {⃗a/⃗y}| and ⃗φ ∈∥∆{⃗a/⃗y}∥. Then we have:
π∗n
⃗σ/⃗x, ⃗φ/⃗α
o
=
D
π1∗n
⃗σ/⃗x, ⃗φ/⃗α
o
, π2∗n
⃗σ/⃗x, ⃗φ/⃗α
oE
∈|(A ∧B) {⃗a/⃗y}|
since by induction hypothesis:
π1∗n
⃗σ/⃗x, ⃗φ/⃗α
o
∈|A {⃗a/⃗y}| and π2∗n
⃗σ/⃗x, ⃗φ/⃗α
o
∈|B {⃗a/⃗y}|
• π =
π0
Γ ⊢A ∧B | ∆
Γ ⊢A | ∆
:
π∗= ⃗x : Γ∗⊢p1 π0∗: A∗| ⃗α : ∆∗, κ : ς
Let ⃗a ∈⃗T M, ⃗σ ∈|Γ {⃗a/⃗y}| and ⃗φ ∈∥∆{⃗a/⃗y}∥. Then we have:
π∗n
⃗σ/⃗x, ⃗φ/⃗α
o
= p1

π0∗n
⃗σ/⃗x, ⃗φ/⃗α
o
∈|A {⃗a/⃗y}|
since by induction hypothesis:
π0∗n
⃗σ/⃗x, ⃗φ/⃗α
o
∈|(A ∧B) {⃗a/⃗y}|
• π =
π0
Γ ⊢A ∧B | ∆
Γ ⊢B | ∆
:
π∗= ⃗x : Γ∗⊢p2 π0∗: A∗| ⃗α : ∆∗, κ : ς
Let ⃗a ∈⃗T M, ⃗σ ∈|Γ {⃗a/⃗y}| and ⃗φ ∈∥∆{⃗a/⃗y}∥. Then we have:
π∗n
⃗σ/⃗x, ⃗φ/⃗α
o
= p2

π0∗n
⃗σ/⃗x, ⃗φ/⃗α
o
∈|B {⃗a/⃗y}|
since by induction hypothesis:
π0∗n
⃗σ/⃗x, ⃗φ/⃗α
o
∈|(A ∧B) {⃗a/⃗y}|
80

• π =
π0
Γ ⊢A | ∆
(zU /∈FV(Γ,∆))
Γ ⊢∀zUA | ∆
:
π∗= ⃗x : Γ∗⊢π0∗: A∗| ⃗α : ∆∗, κ : ς
Let ⃗a ∈⃗T M, ⃗σ ∈|Γ {⃗a/⃗y}| and ⃗φ ∈∥∆{⃗a/⃗y}∥. Then for any b ∈UM, ⃗σ ∈|Γ {⃗a/⃗y, b/z}|
and ⃗φ ∈∥∆{⃗a/⃗y, b/z}∥(since zU /∈FV (Γ, ∆)), so by induction hypothesis:
π0∗n
⃗σ/⃗x, ⃗φ/⃗α
o
∈|A {⃗a/⃗y, b/z}|
Therefore π∗n
⃗σ/⃗x, ⃗φ/⃗α
o
∈
 ∀zU A

{⃗a/⃗y}
.
• π =
π0
Γ ⊢∀zUA | ∆
Γ ⊢A

tU/zU	
| ∆
:
π∗= ⃗x : Γ∗⊢π0∗: A∗| ⃗α : ∆∗, κ : ς
Let ⃗a ∈⃗T M, ⃗σ ∈|Γ {⃗a/⃗y}| and ⃗φ ∈∥∆{⃗a/⃗y}∥. By induction hypothesis:
π0∗n
⃗σ/⃗x, ⃗φ/⃗α
o
∈
\
b∈UM
|A {⃗a/⃗y, b/z}|
so taking b = (t {⃗a/⃗y})M ∈UM we get:
π0∗n
⃗σ/⃗x, ⃗φ/⃗α
o
∈
A
n
⃗a/⃗y, (t {⃗a/⃗y})M/z
o
and since π∗= π0∗and A
n
⃗a/⃗y, (t {⃗a/⃗y})M/z
o
= A {t/z} {⃗a/⃗y} we get:
π∗n
⃗σ/⃗x, ⃗φ/⃗α
o
∈|A {t/z} {⃗a/⃗y}|
• π =
π0
Γ ⊢A | ∆, A
Γ ⊢⊥| ∆, A
:
π∗= ⃗x : Γ∗⊢[β] π0∗: 0 | ⃗α : ∆∗, β : A∗, κ : ς
Let ⃗a ∈⃗T M, ⃗σ ∈|Γ {⃗a/⃗y}|, ⃗φ ∈∥∆{⃗a/⃗y}∥and ψ ∈∥A {⃗a/⃗y}∥. Then we have:
π∗n
⃗σ/⃗x, ⃗φ/⃗α, ψ/β
o
= ([β] π0
∗)
n
⃗σ/⃗x, ⃗φ/⃗α, ψ/β
o
= [ψ]

π0∗n
⃗σ/⃗x, ⃗φ/⃗α, ψ/β
o
∈|⊥| = |⊥{⃗a/⃗y}|
since by induction hypothesis:
π0∗n
⃗σ/⃗x, ⃗φ/⃗α, ψ/β
o
∈|A {⃗a/⃗y}|
so π0∗n
⃗σ/⃗x, ⃗φ/⃗α, ψ/β
o
⊥ψ by lemma 5.1, and µκ. [ψ]

π0∗n
⃗σ/⃗x, ⃗φ/⃗α, ψ/β
o
∈|⊥⊥|.
81

• π =
π0
Γ ⊢⊥| ∆, A
Γ ⊢A | ∆
:
π∗= ⃗x : Γ∗⊢µβ.π0∗: A∗| ⃗α : ∆∗, κ : ς
Let ⃗a ∈⃗T M, ⃗σ ∈|Γ {⃗a/⃗y}| and ⃗φ ∈∥∆{⃗a/⃗y}∥. Then for any ψ ∈∥A {⃗a/⃗y}∥we have:
µκ. [ψ]

π∗n
⃗σ/⃗x, ⃗φ/⃗α
o
= µκ.

[ψ] µβ.π0∗n
⃗σ/⃗x, ⃗φ/⃗α
o
= µκ.

π0∗n
⃗σ/⃗x, ⃗φ/⃗α, ψ/β
o
∈|⊥⊥|
since by induction hypothesis, π0∗n
⃗σ/⃗x, ⃗φ/⃗α, ψ/β
o
∈|⊥{⃗a/⃗y}| = |⊥|. Therefore:
π∗n
⃗σ/⃗x, ⃗φ/⃗α
o
⊥ψ
and so by lemma 5.1:
π∗n
⃗σ/⃗x, ⃗φ/⃗α
o
∈|A {⃗a/⃗y}|
5.2.2
Adequacy for Peano arithmetic
We ﬁx now the theory to be PAωr on ΣPAωr, and the structure M to be a model of PAωr.
The predicates are interpreted in M as in section 2.6, M is required to verify ιM ≃N and the
individual constants 0 and S are supposed to be interpreted in ιM accordingly. The λµ signature
is that of µPCF, we interpret the two predicates as in section 3.1.3:
̸=T ∗
∆=
0
L.ιM∗
∆=
ι
and the axioms as in section 3.4.1. µPCF is interpreted in G as in section 4.4, so [ι] is, as in
section 4.4.2, the usual ﬂat arena of natural numbers:
q
0
1
...
Since PAωr has only one base sort ι, we ﬁx ς = ι.
The realizability values for the predicates are:
|a ̸=T b| ∆=
(
|⊥|
if a = b
G (1, [0] ` [ι])
otherwise
|LnιM| ∆= {n}
since n is a natural number. The falsity value of ̸= is:
∥a ̸=T b∥∆=
( 
∗JιK
	
if a = b
∅
otherwise
and it is easy to verify that the hypotheses of lemma 5.1 are veriﬁed for the inequality predicate.
The falsity value of L.M cannot be deﬁned since it is a positive predicate, however since ∥A ⇒B∥
depends on |A| but not on ∥A∥, we can still deﬁne the falsity values of formulas in which
the relativization predicate does not appears at the rightmost position. We observe that in
the translations of proofs from PAω to PAωr, the relativization predicate never appears at
the rightmost position of a formula in ∆, and that in the adequacy lemma the falsity values
are necessary only for the µ-context.
Therefore, the falsity value of L.M is superﬂuous when
interpreting PAω.
First, all equalities which are true in the model are trivially realized:
82

Lemma 5.3. Let tT and uT be ﬁrst-order terms with FV (t, u) = ⃗x⃗U.
If M ⊨∀⃗x
⃗U (t = u) then λx.x ∈
∀⃗x
⃗U (t = u)

Proof. Let ⃗a ∈⃗UM. Since M ⊨∀⃗x⃗U (t = u), we have (t {⃗a/⃗x})M = (u {⃗a/⃗x})M, so:
|t {⃗a/⃗x} ̸= u {⃗a/⃗x}| = |⊥|
Therefore, for any σ ∈|t {⃗a/⃗x} ̸= u {⃗a/⃗x}|:
λx.xσ = σ ∈|t {⃗a/⃗x} ̸= u {⃗a/⃗x}| = |⊥|
and so:
λx.x ∈
∀⃗x
⃗U (t ̸= u ⇒⊥)
 =
∀⃗x
⃗U (t = u)

Therefore, since M is a model of PAωr we have immediately the following results:
M(refl) = λx.x ∈
∀xT (x =T x)

M(∆s) = λx.x ∈
∀xT→U→V ∀yT→U∀zT (s x y z =V x z (y z))

M(∆k) = λx.x ∈
∀xT ∀yU (k x y =T x)

M(∆rec0) = λx.x ∈
∀xT ∀yι→T→T (rec x y 0 =T x)

M(∆recS) = λx.x ∈
∀xT ∀yι→T→T ∀zι (rec x y (S z) =T y z (rec x y z))

The non-confusion axiom and Leibniz scheme are easy:
Lemma 5.4.
M(Snz) = Ω∈|∀xι (S x ̸=ι 0)|
M(Leib) = λx.x ∈
∀⃗z
⃗U ∀xT ∀yT (¬A ⇒A {y/x} ⇒x ̸=T y)

Proof. Let n ∈ιM, we have (S n)M = n + 1 ̸= 0 = 0M, so |S n ̸= 0| = G (1, [0] ` [ι]), so
any strategy in G (1, [0] ` [ι]) realizes ∀xι (S x ̸=ι 0).
Let now ⃗a ∈⃗UM, b, c ∈T M, σ ∈
|¬A {b/x,⃗a/⃗z}| and τ ∈|A {c/x,⃗a/⃗z}|. If b ̸= c, then |b ̸= c| = G (1, [0] ` [ι]) and therefore
λx.x σ τ ∈|b ̸= c|. Otherwise, b = c so |A {b/x,⃗a/⃗z}| = |A {c/x,⃗a/⃗z}| and by deﬁnition of
|A ⇒B| we get σ τ ∈|⊥|. Moreover, since b = c, |b ̸= c| = |⊥| and σ τ ∈|b ̸= c|. Finally we get
M(Leib) ∈
∀⃗z ⃗U ∀xT ∀yT (¬A ⇒A {y/x} ⇒x ̸=T y)
.
The adequacy for the induction axiom scheme is as follows:
Lemma 5.5.
M(indr) = rec ∈
∀⃗y
⃗U (A {0/x} ⇒∀rxι (A ⇒A {S x/x}) ⇒∀rxιA)

Proof. Let ⃗a ∈⃗UM, σ ∈|A {0/x,⃗a/⃗y}|, τ ∈|∀rxι (A {⃗a/⃗y} ⇒A {S x/x,⃗a/⃗y})|. Since ιM ≃N,
we can use induction to prove that for any n ∈ιM, rec σ τ n = rec σ τ n ∈|A {n/x,⃗a/⃗y}|:
• n = 0: rec σ τ 0 = σ ∈|A {0/x,⃗a/⃗y}|
83

• n + 1:
rec σ τ n + 1 = rec σ τ succ n = τ n (rec σ τ n).
Since n ∈|LnM|, we get τ n ∈
|A {n/x,⃗a/⃗y} ⇒A {S n/x,⃗a/⃗y}|, and since by induction hypothesis we have rec σ τ n ∈
|A {n/x,⃗a/⃗y}| we get τ n (rec σ τ n) ∈|A {S n/x,⃗a/⃗y}|. Finally, since (S n)M = n + 1,
rec σ τ n + 1 ∈|A {n + 1/x,⃗a/⃗y}|.
The last axioms are the relativization ones:
Lemma 5.6.
ML0M = 0 ⊩L0M
MLSM = succ ⊩LSM
Proof. The ﬁrst one is immediate, since |L0M| =

0
	
. For the second one, remember that:
LSM ≡∀rxιLS xM ≡∀xι (LxM ⇒LS xM)
let n ∈ιM, we have to prove succ n ∈|LS nM|. Since (S n)M = n + 1, this amounts to succ n ∈
|Ln + 1M|, which is true since succ n = n + 1.
5.2.3
Adequacy for the axiom of choice
When it comes to the axiom of countable choice AC, the usual route of negative translation
followed by intuitionistic realizability becomes much more diﬃcult. Indeed, AC ⇒AC¬ is not
provable in intuitionistic logic, therefore the path described in section 5.1.1 cannot be followed
as-is and a realizer of AC¬ must be provided. In [BBC98], a variant of bar recursion was used to
realize AC¬, while in [BO05] the principle of double negation shift ∀x ¬¬A⇒¬¬∀x A was realized
using bar recursion (see 3.5). With this principle, it becomes possible to derive AC ⇒AC¬ in
intuitionistic logic, and since AC is realized in intuitionistic models by the identity, one obtains
a realizer of AC¬.
We follow here a diﬀerent approach, since our realizability model is for classical logic, and we
prove that bar recursion realizes the axiom of dependent choice (DCr) in our classical model. The
exact connection with the negative translation + intuitionistic realizability technique has not
been investigated. The negative translation of proofs corresponds to the continuation-passing-
style translation on terms and the semantics of λµ-calculus in a category of continuations RC
corresponds to the semantics of its CPS-translation into λ-calculus in the cartesian “R-closed”
category C as stated in section 3.2.4, so in order to compare more closely our model with the
usual negative translation + intuitionistic realizability technique, one would need to deﬁne a
realizability relation for intuitionistic logic in the category C = Fam (G). Therefore, it seems
more natural to work directly in the category of continuations G which has built-in control
features, and in which the usual arena of natural numbers can be seen as a negated object
R(RN) (see section 4.4.2).
Interpreting dependent choice using bar recursion
We use here the bar-recursion operator deﬁned in section 3.5.4 to provide the term M(DCr)
interpreting the axiom of dependent choice. First, remark that the derivation of (DC)r from
(DCr) given in section 2.5.2 doesn’t involve relativization predicates at the rightmost position
in formulas of ∆, so it is still correct to have no falsity value for L.M. For technical reasons, we
actually use a slight modiﬁcation of this bar recursor and deﬁne:
barrec′ ∆= λwuv.Y (λzy.µα. [α] v (y @ ⟨w, µβ. [α] u y (λx.z (y ∗x))⟩))
84

which can be typed with:
barrec′ : T →((T × U)⋄→(T × U →V ) →V ) →((ι →T × U) →V ) →(T × U)⋄→V
and which satisﬁes the equation:
barrec′ Q M N P = µα. [α] N
 P @

Q, µβ. [α] M P
 λx.barrec′ Q M N (P ∗x)

Remember that (DCr) is:
∀⃗u
⃗U∀rvT  ∀rxι∀ryT ¬∀zT ¬ (LzM∧Ar) ⇒¬∀yι→T ¬ (Ly 0M∧∀rxι (Ly (S x)M∧Ar {y x/y, y (S x) /z}))

so if we write B ≡LzM ∧Ar it becomes:
∀⃗u
⃗U∀rvT  ∀rxι ∀ryT ¬∀zT ¬B ⇒¬∀yι→T ¬ (Ly 0M ∧∀rxιB {y x/y, y (S x) /z})

and (DCr)∗is:
M(DCr) : T →(ι →T →(B∗→0) →0) →(T × (ι →B∗) →0) →0
In order to deﬁne M(DCr) we make an informal reasoning. Suppose Q : T is a witness of LvM,
M : ι →T →(B∗→0) →0 is a witness of:
∀rxι∀ryT  ∀zT (B ⇒⊥) ⇒⊥

and N : T × (ι →B∗) →0 is a witness of:
∀yι→T (Ly 0M ∧∀rxιB {y x/y, y (S x) /z} ⇒⊥)
We want to build from this, using barrec′, a witness of ⊥. We will use the following instance of
barrec′:
barrec′ : T →(B∗⋄→(B∗→0) →0) →((ι →B∗) →0) →(T × B∗)⋄→0
As an aside remark, since in this instance the µ-variable α is of type 0, the following equation
holds:
barrec′ = λwuvy.v
 y @

w, µβ.u y
 λx.barrec′ w u v (y ∗x)

The ﬁrst argument is without surprise Q. The idea now is that barrec′ will build a sequence
of witnesses of B {y x/y, y (S x) /z}. The second argument represents the recursive step. If we
have an element y : B⋄which represents the sequence of witnesses already computed, then M
computes the next element of y, given its last element. Here we have two cases, either |y| = 0,
in which case we must initialize the sequence with the ﬁrst element Q, or the last element of y is
y Hpred |y|I. Therefore, we provide M with if0 |y| Q (p1 y Hpred |y|I) (we keep it informal and do
not explain here the p1). We also give to M the current step which is |y| so the second argument
is:
λy.M |y| if0 |y| Q (p1 y Hpred |y|I)
The third argument represents the behavior if we have an inﬁnite sequence of witnesses y : ι →B.
We provide N with the ﬁrst witness, that is Q, and the inﬁnite sequence y, so the third argument
is:
λy.N ⟨Q, y⟩
Finally, the last argument of barrec′ is the initial sequence of witnesses, that is the empty sequence
ϵ. We have then:
barrec′ Q (λy.M |y| if0 |y| w (p1 y Hpred |y|I)) (λy.N ⟨Q, y⟩) ϵ : 0
The interpretation of (DCr) is therefore deﬁned as:
M(DCr)
∆= λwuv.barrec′ w (λy.u |y| if0 |y| w (p1 y Hpred |y|I)) (λy.v ⟨w, y⟩) ϵ
85

Adequacy
We suppose now that M satisﬁes (DC), and we use the variant of the bar recursion operator
deﬁned above to realize the axiom of dependent choice (DCr).
First, we prove that the continuity requirement of section 3.5.3 holds in G:
Lemma 5.7. Let σ ∈G (1, [(ι →T) →0] ` [ι]) and τ ∈G (1, [ι →T] ` [ι]) such that µκ.σ τ = n.
There exists m ∈N such that for any τ ′ ∈G (1, [ι →T] ` [ι]):
 ∀m′ < m, τ ′ m′ = τ m′
⇒µκ.σ τ ′ = µκ.σ τ = n
Proof. µκ.σ τ = n means that there is a play s ∈σ in

[ι](1) →[T]

→[ι](2) such that
s|([ι](1)→[T])`[ι](2) ∈τ and s|[ι](2) = qn.
Since s is a ﬁnite sequence, there is some m ∈N
such that all the moves m′ in s|[ι](1) verify m′ < m. Now, if τ ′ ∈G (1, [ι →T] ` [ι]) is such that
∀m′ < m, τ ′ m′ = τ m′, then s|[ι](1)→[T]`[ι](2) is also a play of τ ′, and therefore qn ∈µκ.σ τ ′ and
µκ.σ τ ′ = n.
We also prove that in G we can build a strategy from an inﬁnite sequence of strategies:
Lemma 5.8. Let (σn)n∈N be a sequence of strategies in an arena [T]. There exists a strategy σ
in the arena [ι] →[T] such that for any n ∈N, σ n = σn.
Proof. The view s of the strategy σ are such that s|[ι] = qn and s|[T] ∈σn for n ∈N. It is easy
to see then that for any n ∈N, σ n = σn.
Using these two results on G, we now prove that the interpretation of (DCr) realizes (DCr):
Lemma 5.9. Let ⊥⊥⊆ιM.
M(DCr) = λwuv.barrec′ w (λy.u |y| if0 |y| w (p1 y Hpred |y|I)) (λy.v ⟨w, y⟩) ϵ
∈
∀⃗u
⃗U∀rvT  ∀rxι ∀ryT ¬∀zT ¬B ⇒¬∀yι→T ¬ (Ly 0M ∧∀rxιB {y x/y, y (S x) /z})

where B ≡LzM ∧Ar
Proof. Recall that we use the bar recursor:
barrec′ : T →(B∗⋄→(B∗→0) →0) →((ι →B∗) →0) →(T × B∗)⋄→0
and M(DCr) has type (DCr)∗:
M(DCr) : T →(ι →T →(B∗→0) →0) →(T × (ι →B∗) →0) →0
Let now ⃗a ∈⃗UM and b ∈T M and write A′ ≡Ar {⃗a/⃗u, b/v} and B′ ≡B {⃗a/⃗u, b/v}. Let:
σ ∈G (1, [T] ` [ι])
τ ∈G (1, [ι →T →(B∗→0) →0] ` [ι])
ν ∈G (1, [T × (ι →B∗) →0] ` [ι])
be such that:
σ ∈|LbM|
τ ∈
∀rxι∀ryT ¬∀zT ¬B′
ν ∈
∀yι→T ¬
 Ly 0M ∧∀rxι B′ {y x/y, y (S x) /z}

We then have to prove:
barrec′ ν (λy.τ |y| if0 |y| ν (p1 y Hpred |y|I)) (λy.σ ⟨w, y⟩) ϵ ∈|⊥|
86

We introduce more notations:
τ ′ = λy.τ |y| (if0 |y| σ (p1 y Hpred |y|I)) ∈G (1, [B∗⋄→(B∗→0) →0] ` [ι])
ν′ = λy.ν ⟨σ, y⟩∈G (1, [(ι →B∗) →0] ` ι)
ψ = barrec′ σ τ ′ ν′ ∈G (1, [B∗⋄→0] ` [ι])
so we must prove that ψ ϵ ∈|⊥|. With our notations, for any ρ ∈G (1, [B∗⋄] ` [ι]) we have:
ψ ρ = barrec′ σ τ ′ ν′ ρ
= ν′  ρ @

σ, µβ.τ ′ ρ
 λx.barrec′ σ τ ′ ν′ (ρ ∗x)

= ν′  ρ @

σ, µβ.τ ′ ρ (λx.ψ (ρ ∗x))

The following iteration lemma (the proof of which is deferred to the end of the section) is the
heart of the adequacy:
Lemma 5.10. Let c0, ..., cn ∈T M and θ0, ..., θn−1 ∈G (1, [B∗] ` [ι]) be such that:
c0 = b
∀0 ≤i < n, θi ∈
B′ {i/x, ci/y, ci+1/z}

ψ (θ0 ∗... ∗θn−1) /∈|⊥|
then there exists cn+1 ∈T M and θn ∈G (1, [B∗] ` [ι]) such that:
θn ∈
B′ {n/x, cn/y, cn+1/z}

ψ (θ0 ∗... ∗θn−1 ∗θn) /∈|⊥|
In order to prove ψ ϵ ∈|⊥|, we use a reductio-ad-absurdum and suppose ψ ϵ /∈|⊥|. By
iterating the lemma, we build sequences (cn)n∈N in T M and (θn)n∈N in G (1, [B∗] ` [ι]) such
that:
c0 = b
∀n ∈N, θn ∈
B′ {n/x, cn/y, cn+1/z}
 and ψ (θ0 ∗... ∗θn−1) /∈|⊥|
Since M satisﬁes (DC), we can build c ∈(ι →T)M such that for every n ∈ιM ≃N, c n = cn,
and using lemma 5.8 we also build θ ∈G (1, [ι →B∗] ` [ι]) such that for every n ∈N, θ n = θn.
We now prove that:
θ ∈
∀rxι B′ {c x/y, c (S x) /z}

Indeed, let n ∈N ≃ιM, since |LnM| = {n}, we must prove θ n ∈|B′ {n/x, c n/y, c (S n) /z}|, but
since θ n = θn, c n = cn and c (S n) = cn+1, this is immediate. Now, since:
ν ∈
∀yι→T ¬
 Ly 0M ∧∀rxι B′ {y x/y, y (S x) /z}
 ⊆
¬
 Lc 0M ∧∀rxι B′ {c x/y, c (S x) /z}

and σ ∈|LbM| = |Lc 0M|, we get easily:
ν′ = λy.ν ⟨σ, y⟩∈
¬∀rxι B′ {c x/y, c (S x) /z}

and therefore ν′ θ ∈|⊥|, so µκ.ν′ θ ∈|⊥⊥|. By lemma 5.7, there is some m ∈N such that any
strategy θ′ ∈G (1, [ι →B∗] ` [ι]) which satisﬁes:
∀m′ < m, θ′ m′ = θ m′
is such that µκ.ν′ θ′ = µκ.ν′ θ ∈|⊥⊥|. If we write ρ = θ0 ∗... ∗θm−1, this is veriﬁed in particular
for:
ρ @

σ, µβ.τ ′ ρ (λx.ψ (ρ ∗x))

so we get:
µκ.ν′  ρ @

σ, µβ.τ ′ ρ (λx.ψ (ρ ∗x))

∈|⊥⊥|
and ﬁnally:
ψ (θ0 ∗... ∗θm−1) = ν′  ρ @

σ, µβ.τ ′ ρ (λx.ψ (ρ ∗x))

∈|⊥|
from which we get our contradiction.
87

Here is the proof of the iteration lemma:
Proof. Write ρ = θ0 ∗... ∗θn−1. We have:
ν

σ, ρ @

σ, µβ.τ ′ ρλx.ψ (ρ ∗x)

= ν′  ρ @

σ, µβ.τ ′ ρ (λx.ψ (ρ ∗x))

= ψ ρ
= ψ (θ0 ∗... ∗θn−1) /∈|⊥|
Using kM, sM and recM we can build some c ∈(ι →T)M such that for any 0 ≤i ≤n, c i = ci,
and for any i > n, c i = b. Since ν ∈
∀yι→T ¬ (Ly 0M ∧∀rxι B′ {y x/y, y (S x) /z})
, we have in
particular:
ν ∈
¬
 Lc 0M ∧∀rxι B′ {c x/y, c (S x) /z}

therefore:

σ, ρ @

σ, µβ.τ ′ ρ (λx.ψ (ρ ∗x))

/∈
Lc 0M ∧∀rxι B′ {c x/y, c (S x) /z}

and since σ ∈|LbM| = |Lc0M| = |Lc 0M|, we get:
ρ @

σ, µβ.τ ′ ρ (λx.ψ (ρ ∗x))

/∈
∀rxι B′ {c x/y, c (S x) /z}

so there exists i ∈N ≃ιM such that:
 ρ @

σ, µβ.τ ′ ρ (λx.ψ (ρ ∗x))

i /∈
B′ {i/x, c i/y, c (S i) /z}

If i < n then we get:
ρ HiI = θi /∈
B′ {i/x, ci/y, ci+1/z}

which contradicts the hypothesis of the lemma. Therefore, i ≥n and:

σ, µβ.τ ′ ρ (λx.ψ (ρ ∗x))

/∈
B′ {i/x, c i/y, c (S i) /z}
 =
Lc (S i)M ∧A′ {i/x, c i/y, c (S i) /z}

Since again σ ∈|LbM| = |Lc (S i)M|, we get:
µβ.τ ′ ρ (λx.ψ (ρ ∗x)) /∈
A′ {i/x, b i/y, b (S i) /z}

Since λx.µβ.x is the interpretation in λµ-calculus of a proof of ⊥⇒Ar, we get by the adequacy
lemma:
τ |ρ| (if0 |ρ| σ (p1 ρ Hpred |ρ|I)) (λx.ψ (ρ ∗x)) = τ ′ ρ (λx.ψ (ρ ∗x)) /∈|⊥|
ﬁrst, since ρ = θ0 ∗... ∗θn−1, |ρ| = n ∈|LnM|, so τ |ρ| ∈
∀ryT ¬∀zT ¬B′ {n/x}
. We prove now
that if0 |ρ| σ (p1 ρ Hpred |ρ|I) ∈|LcnM| by distinguishing cases:
• n = 0: if0 |ρ| σ (p1 ρ Hpred |ρ|I) = σ ∈|LbM| = |Lc0M|
• n ̸= 0: if0 |ρ| σ (p1 ρ Hpred |ρ|I) = p1 θn−1 ∈|LcnM| since:
θn−1 ∈
B′ {n −1/x, cn−1/y, cn/z}
 =
LcnM ∧A′ {n −1/x, cn−1/y, cn/z}

Therefore we have:
τ |ρ| (if0 |ρ| σ (p1 ρ Hpred |ρ|I)) ∈
¬∀zT ¬B′ {n/x, cn/y}

and so:
λx.ψ (ρ ∗x) /∈
∀zT ¬B′ {n/x, cn/y}

which means that there exists some cn+1 ∈T M such that:
λx.ψ (ρ ∗x) /∈
¬B′ {n/x, cn/y, cn+1/z}

and so there exists θn ∈G (1, [B∗] ` [ι]) such that θn ∈|B′ {n/x, cn/y, cn+1/z}| and:
ψ (θ0 ∗... ∗θn−1 ∗θn) = ψ (ρ ∗θn) = λx.ψ (ρ ∗x) θn /∈|⊥|
88

5.3
Extraction
Since the Friedman translation is directly built in the realizability, the extraction result is an
easy consequence of the deﬁnitions. We show that from any Π0
2-formula provable in CAω we
can extract a computable witnessing function. Note that in the extraction lemma, the equality
t =U u is at any type:
Lemma 5.11. From a proof of ⊢∀xT ∃yι (t =U u) in CAω, one can extract a λµ-term M : T →ι
such that for any a ∈T M and σ ⊩LaM, there is some n ∈N such that:
(t {a/x, n/y})M = (u {a/x, n/y})M and M σ = n
Proof. First, we obtain by double-negation elimination a proof of ⊢∀xT ¬∀yι (t ̸=U u), and by
relativization a proof π in CAωr of ⊢∀rxT ¬∀ryι (t ̸=U u). The adequacy lemma then tells us:
π∗∈
∀rxT ¬∀ryι (t ̸=U u)

Then, if a ∈T M and σ ∈|LaM|, we get π∗σ ∈|¬∀ryι (t {a/x} ̸=U u {a/x})|. Let now ﬁx:
⊥⊥=
n
n ∈ιM  (t {a/x, n/y})M = (u {a/x, n/y})Mo
We prove now that λx. [κ] x ∈|∀ryι (t {a/x} ̸=U u {a/x})|. For that let n ∈ιM, and let prove
that (λx. [κ] x) n = [κ] n ∈|t {a/x, n/y} ̸= u {a/x, n/y}|. There are two cases:
• n ∈⊥⊥: in that case, (t {a/x, n/y})M = (u {a/x, n/y})M, so:
|t {a/x, n/y} ̸= u {a/x, n/y}| = |⊥|
and µκ. [κ] n = n ∈|⊥⊥| so [κ] n ∈|⊥|
• n /∈⊥⊥: in that case, (t {a/x, n/y})M ̸= (u {a/x, n/y})M, so:
|t {a/x, n/y} ̸= u {a/x, n/y}| = G (1, [0] ` [ι])
and therefore [κ] n ∈|t {a/x, n/y} ̸= u {a/x, n/y}|
Therefore we get π∗σ (λx. [κ] x) ∈|⊥|, and so µκ.π∗σ (λx. [κ] x) ∈|⊥⊥|.
This means that
µκ.π∗σ (λx. [κ] x) is some n such that n ∈⊥⊥(so (t {a/x, n/y})M = (u {a/x, n/y})M). Finally,
we have the claimed result with M ∆= λu.µκ.π∗u (λx. [κ] x).
In the particular case of T = ι, we get that for any m ∈N, M m is some n such that
(t {m/x, n/y})M = (u {m/x, n/y})M.
89

90

Chapter 6
A realizability model of winning
strategies for classical arithmetic
The realizability model of the previous chapter makes heavy use of orthogonality in categories
of continuations, and does not rely heavily on the underlying games model. In this chapter, we
devise another realizability model based on winning conditions, which is very dependent on the
particular model of HON games. In particular, the realizability semantics of implication is not
anymore the usual Kleene arrow, but requires to look directly at the set of plays constituting
a strategy. Our winning conditions can be seen as an extension to ﬁrst-order logic of [Cla09]
and [Hyl97]. This dependency upon the plays constituting a strategy, rather than the sole result
of putting the strategy in a context, allows a much more ﬁne-grained control. This particularity
has already been exploited for diﬀerent purposes, for example decidability results for fragments
of programming languages [GM03], type isomorphisms [Lau05, Cla12], or bounds on reduction
in abstract machines [Cla11]. In the context of realizability, one hope is to obtain and use an
operator similar to Krivine’s clock [Kri03]. The clock operator implements an enumeration of
terms and it is used to realize the axiom of dependent choice in a second-order setting. On
the other side, when realizing the axiom of choice through bar recursion, one needs to turn
any sequence of elements of the model into a single element. This implies that the model is
uncountable, and therefore there is no enumeration of its elements. However, in the context
of winning conditions, we do not look at the uncountable set of all strategies, but only at the
countable set of plays. Therefore, an enumeration of all plays may suﬃce to obtain a behavior
similar to Krivine’s clock, leading to a single realizability model in which the two realizations of
choice co-exist.
We ﬁrst follow ideas from [CH09] and highlight the causality structure of justiﬁed sequences
by reformulating these as thick subtrees (deﬁned in [Bou09]). Then we deﬁne the notion of
winning conditions on arenas, which are sets of desequentialized plays. We deﬁne a realizability
relation based on these winning conditions and prove the adequacy of this interpretation for
ﬁrst-order logic and Peano arithmetic and use it to extract computational content from classical
proofs through a variant of Friedman’s translation for classical logic.
Finally, we give ideas
about the realization of the axiom of choice using the higher-order references of the model of
Hyland-Ong-Nickau games. The work presented in this chapter is an improved version of [Blo13].
6.1
Justiﬁed sequences via thick subtrees
In usual game semantics [HO00], justiﬁed sequences are deﬁned as in the previous chapter, as
words of moves with pointers between them. However, it was shown in [CH09] that justiﬁed
91

sequences could also be represented by trees with a sequential ordering of nodes (which they call
“multiplexed positions”), and that this representation was compatible with the dynamic aspects
of composition. Here we use the thick subtrees of [Bou09] to deﬁne positions on arenas, and the
interaction sequences are then positions together with a sequentiality information. This choice
is justiﬁed by the positional nature of our winning conditions. In this section we will use the
following arena for the examples:
O
a
f
P
b
c
g
h
O
d
e
(6.1)
Remember from the previous chapter that an arena is a partial order, and that we use <,≤and
<1 as relations between the nodes. For example, in the arena (6.1) we have b <1 e.
6.1.1
Justiﬁed sequences as thick subtrees
We choose to deﬁne positions on arenas using the thick subtrees of [Bou09], extended to handle
the case of forests. This formalism is a nice way to deal with non-aﬃne programs: programs
that may use their arguments several times. A thick subtree of a given tree is a subtree which
can be extended in width, meaning that branches of the initial tree can be duplicated. These
duplications correspond to the distinct computations of arguments during the execution of a
non-aﬃne program.
Deﬁnition 6.1 (State). Given an arena A, a state on A is a thick subforest of A, that is a
forest s together with a labeling function l : s →A such that:
∀x ∈s, {l (y)| y < x} = {a ∈A| a < l (x)}
This condition ensures that the roots are mapped to roots, and that the relation <1 is
preserved. Here is an example of a state on the arena (6.1):
a
a
f
b
b
b
h
d
e
e
(6.2)
The nodes are considered distinct, even if they have the same label. By deﬁnition of the polarity,
the roots of a state are labeled with O-moves, and if x <1 y, then the labels of x and y are of
opposite polarities. We denote the empty state by ϵ.
Deﬁnition 6.2 (Position). A position on an arena is a state which is a tree (i.e. it has exactly
one root). If s is a state on an arena A, the set of positions of s, Pos (s), is the set of trees
composing the state. In particular, a state s is a position if and only if Pos (s) = {s}, and since
Pos (ϵ) = ∅, ϵ is not a position.
For example, the set of positions of the state (6.2) is:





a
b
b
d
;
a
b
e
e
;
f
h





Our notion of position can be seen as an attempt to give a positional account of game semantics,
as in [Mel06], but in a quite diﬀerent way.
The sequential states are then states on arenas together with a sequentiality information:
92

Deﬁnition 6.3 (Sequential state). Given an arena A, we deﬁne a sequential state on A to be
a state (ﬁnite of inﬁnite) s on A equipped with a total order ≺of type at most ω1. Moreover, a
sequential state must verify:
∀x, y ∈s
x < y
⇒
x ≺y
The partial order < of a sequential state corresponds to the pointers of usual game semantics
settings: x <1 y corresponds to y pointing to x. The total order ≺corresponds to the sequen-
tiality of moves. The coherence condition of sequential states means that if there is a pointer
from a move y to a move x in the sequence, then x must have been played before y. The fact
that l (x) must be the father of l (y) in the arena comes from the deﬁnition of a state.
Here is an example of a sequential state on the arena (6.1):
(a, 1)
(a, 3)
(f, 8)
(b, 2)
(b, 4)
(b, 5)
(h, 10)
(d, 7)
(e, 6)
(e, 9)
(6.3)
Where the labeling is given by the ﬁrst components and the injection to natural numbers by the
second components. The empty sequential state will be denoted ϵ, like the empty state. By the
coherence condition of sequential states, the minimal element for ≺is always a minimal element
for <, and therefore labeled with a root of the arena. We can map any sequential state to a
state by forgetting the sequentiality information, so in the following we will implicitly consider
that a sequential state is a state. For example the state corresponding to the sequential state
(6.3) is the state (6.2).
6.1.2
Correspondence with the usual setting
In the previous chapter, justiﬁed sequences were deﬁned as usual in game semantics (see for
example [HO00]), as words of moves with pointers between them. For example, the justiﬁed
sequence (6.3) was represented as:
a
b
a
b
b
e
d
f
e
h
We prove here the correspondence between the notion of a sequential state and that of a justiﬁed
sequence which was deﬁned in the previous chapter.
From justiﬁed sequences to sequential states
If s is a justiﬁed sequence with justifying function f as in Deﬁnition 4.2, we deﬁne the corre-
sponding sequential state SeqSt (s) as the thick subforest of A whith nodes (si, i). The order <
of the forest is deﬁned as:
(si, i) < (sj, j) ⇐⇒∃n > 0, fn (j) = i
the labeling function is given by the ﬁrst components and the order ≺is given by the second
components.
Lemma 6.1. If s is a justiﬁed sequence with justifying function f, then SeqSt (s) is a state.
1This means that ≺can be described by an injection to natural numbers.
93

Proof. Let (sj, j) be a node. We have to prove:
{si | (si, i) < (sj, j)} = {a ∈A| a < sj}
For the left-to-right inclusion, if (si, i) < (sj, j) then by deﬁnition there is some n > 0 such that
fn (j) = i, so by the deﬁnition of a justiﬁed sequence, in A we have si = sfn(j) <1 sfn−1(j) <1
... <1 sf(j) <1 sj, and therefore si < sj. For the right-to-left inclusion, since an arena has ﬁnite
depth, {a ∈A| a < sj} is ﬁnite, and it is also a well-order (by deﬁnition of a forest), so there are
an <1 ... <1 a1 ∈A such that {a ∈A| a < sj} = {an; ...; a1}. If a ∈A is such that a < sj, then
a = ak for some 1 ≤k ≤n, and then a = sfk(j). Moreover, by deﬁnition

sfk(j), fk (j)

< (sj, j)
so we conclude.
Lemma 6.2. If s is a justiﬁed sequence with justifying function f, then SeqSt (s) is a sequential
state.
Proof. We only have to prove that if (si, i) < (sj, j), then i < j. Indeed, if (si, i) < (sj, j), then
by deﬁnition fn (j) = i for some n > 0, but since f (k) < k as soon as f (k) is deﬁned, we get
immediately i < j.
From sequential states to justiﬁed sequences
If s is a sequential state with labelling l we deﬁne here the corresponding justiﬁed sequence
Just (s). We can order the elements of s using the total ordering ≺, which is of type at most
ω, and obtain a (possibly inﬁnite) sequence l (x0) ...l (xn) ... such that xi ≺xj ⇔i < j, which
is the word of moves of Just (s). We now deﬁne the justifying function f of Just (s). If xi ∈s,
there are two cases. Either {xj ∈s| xj < xi} = ∅, in which case f (i) is left undeﬁned, or there
is a unique xj ∈s such that xj <1 xi (in s), in which case we deﬁne f (i) = j.
Lemma 6.3. If s is a sequential state with labelling l, then Just (s) is a justiﬁed sequence.
Proof. Let f be the justifying function of Just (s). If f (i) is undeﬁned, then {xj ∈s| xj < xi} =
∅, but then:
{a ∈A| a < l (xi)} = {l (xj)| xj < xi} = ∅
so l (xi) is a root of A. If f (i) = j, then xj <1 xi so xj < xi and xj ≺xi by deﬁnition of a
sequential state. By deﬁnition of the ordering that we chose, this means that j < i. Moreover,
by deﬁnition of a state, since xj <1 xi we get also l (xj) <1 l (xi).
Equivalence of the two notions
Sequential states are in one-to-one correspondence with usual justiﬁed sequences.
The two
transformations above, from justiﬁed sequences to sequential states, and from sequential states
to justiﬁed sequences, can be composed, and we can prove that for any justiﬁed sequence s,
Just (SeqSt (s)) = s, and for any sequential state s, SeqSt (Just (s)) = s. In the following we
will sometimes use “sequence” instead of “sequential state”.
Another interesting thing about sequential states is that the restriction of a sequential state
is much easier to deﬁne than that of a justiﬁed sequence: if A is an arena, X is a subset of A
(and therefore an arena, see the previous chapter) and s is a state on A, then s|X is the state on
X with nodes l−1 (X) (where l : s →A is the labeling of the state), with the restricted ordering.
It is immediate to check that it is a state on X. Moreover, this is consistent with the deﬁnition
of restriction of a justiﬁed sequence: if s is a justiﬁed sequence then SeqSt
 s|X

= SeqSt (s)|X
94

and if s is a sequential state, then Just
 s|X

= Just (s)|X. Here is an example of a subset X of
the arena (6.1), together with the state (6.2) restricted to X:
O
a
g
h
P
d
e
c
a
a
h
d
e
e
(6.4)
Recall from section 4.2.1 that the set of threads of a justiﬁed sequence is obtained by taking
for each initial move of the sequence the set of moves hereditarily justiﬁed by it. In the case of
sequential states, this amounts to take the set of trees of the sequential state together with the
restricted ordering ≻. Then, Threads (s) is a set of sequential states, which become positions
by forgetting about sequentiality, and these positions are exactly the positions of Pos (s) deﬁned
above, obtained by considering s as a state without sequentiality. Finally the notions of O/P-
sequence, preﬁx, O/P-preﬁx, play and O/P-play also translate to sequential states through the
isomorphism between justiﬁed sequences and sequential states.
6.2
Winning conditions on arenas
It is well-known that preservation of totality by composition of strategies is problematic in game
semantics. In the context of realizability, however, we are not interested in totality, but only in
winningness. We thus do not impose any totality condition on strategies, but when it turns to the
deﬁnition of winning positions, we have to take into account all maximal positions, including
both inﬁnite and odd-length ones. This leads to the notion of winning strategy proposed in
deﬁnition 6.8.
6.2.1
P-subpositions, O-subpositions
In order to deﬁne the notion of winning condition on an arena we introduce the notion of
P-subposition and O-subposition:
Deﬁnition 6.4 (P-subposition, O-subposition). If s is a position and t is a downward-closed
subset of s (therefore it is a position), then t is a:
• P-subposition of s if when aP <1 bO in s and aP ∈t, then bO ∈t
• O-subposition of s if when aP <1 bO in s and aO ∈t, then bP ∈t
The intuitive meaning is that a P-subposition is a weakening of player (but any answer of
opponent to a player move must be kept), and an O-subposition is a weakening of opponent
(but any answer of player to an opponent move must be kept). A P-subposition (resp. an
O-subposition) is obtained from a given position by cutting some subtrees having roots labeled
with P-moves (resp. O-moves). Here is an example of a position together with a P-subposition
and an O-subposition:
Polarities
Arena
Position
P-subposition
O-subposition
O
P
O
P
a
b c
d e
f
a
b b c
d e e
f f
a
b
c
d e
f
a
b b c
e
95

6.2.2
Winning conditions
Now we can deﬁne the notion of winning condition on an arena:
Deﬁnition 6.5 (Winning condition). A winning condition on A is a set WA of positions on A
such that:
• If s is a position on A and if some P-subposition of s is in WA, then s ∈WA.
• If s ∈WA then all the O-subpositions of s are in WA.
A state s (and by extension a sequence or a play) on the arena A equipped with the winning
condition WA is said to be winning if Pos (s) ⊆WA.
A winning position must be thought as a position that is winning for player P. Under this
interpretation, the two requirements read as: if some player-weakening of a position is already
winning, then the full position is also winning, and if a position is winning, then any opponent-
weakening of that position is all the more winning.
Our notion of winning state can be seen as a generalization of the winning conditions deﬁned
in [Cla09] and [Hyl97]. In these works, every even-length position is winning, every odd-length
position is losing, and the non-trivial part lies in the winningness of inﬁnite positions. In order
to obtain a realizability model of ﬁrst-order logic, the notion of winning ﬁnite state is here non-
trivial and there can be odd-length plays which are winning and even-length plays which are
losing. Winning conditions were also deﬁned in [Mel05a] using payoﬀs on positions, but with
diﬀerent purposes, and in the framework of asynchronous games.
We now deﬁne constructions on winning conditions, corresponding to the arena constructors
→, ×, V and ` (the binoidal functor of categories of continuations, acting on arenas as described
in section 4.4.1), but ﬁrst we give a few general remarks about positions in the arenas A →B,
A × B and A ` B. If s is a position on A →B, then s|B is a position on B, so s|B is winning
iﬀs|B ∈WB, if s is a position on A × B, then s is either a position on A, or a position on B,
and if s is a position on A ` B, then s|A is a position on A and s|B is a position on B, so s|A
(respectively s|B) is winning iﬀs|A ∈WA (respectively s|B ∈WB).
Deﬁnition 6.6 (Arrow, product and par of winning conditions). If WA and WB are sets of
positions on the arenas A and B, then we deﬁne:
WA→B =

s position on A →B
 Pos
 s|A

⊆WA ⇒s|B ∈WB
	
WA×B =

s position on A × B

s position on A ⇒s ∈WA
s position on B ⇒s ∈WB

WV = ∅
WA`B =

s position on A ` B
 s|A ∈WA ∨s|B ∈WB
	
Remark that the winning conditions on these connectives have indeed the correct intuitive
interpretation: a position is winning on A →B if whenever its projection on A is winning, its
projection on B is also winning, a position is winning on the conjunction A×B if its projections
on A and B are both winning (one of the two being the empty play), no position is winning on
the empty disjunction V, and a position in winning on the binary disjunction A ` B if one of its
projections on A or B is winning.
The following lemma states that these are indeed winning conditions:
Lemma 6.4. If WA and WB are winning conditions on A and B, then WA→B is a winning
condition on A →B, WA×B is a winning condition on A × B, WV is a winning condition on V
and WA`B is a winning condition on A ` B.
96

Proof.
• Let s be a position on A →B and let t be a P-subposition of s such that t ∈WA→B.
Suppose that Pos
 s|A

⊆WA.
If u ∈Pos
 t|A

then u is an O-subposition of some
v ∈Pos
 s|A

⊆WA, so u ∈WA, hence Pos
 t|A

⊆WA. Then since t ∈WA→B, t|B ∈WB
and since t|B is a P-subposition of s|B we conclude that s|B ∈WB. Finally s ∈WA→B.
• Let s be a position on A →B such that s ∈WA→B and let t be an O-subposition of s.
Let suppose that Pos
 t|A

⊆WA. If u ∈Pos
 s|A

and if a is the root of u, then the
father of a is the root of s which is an O-move, and since t is an O-subposition of s, we
get a ∈t. Now the position of t|A with root a is in WA and it is a P-subposition of u, so
u ∈WA. Therefore Pos
 s|A

⊆WA, and since s ∈WA→B we have s|B ∈WB. Since t|B is
an O-subposition of s|B, we get t|B ∈WB. Finally t ∈WA→B.
• Let s be a position on A × B. s is either a position on A, or a position on B, so if t is a
winning P-subposition of s, then either t ∈WA, or t ∈WB. Therefore s ∈WA or s ∈WB,
and so s ∈WA×B.
• Let s be a position on A × B such that s ∈WA×B. Either s ∈WA, or s ∈WB, so any
O-subposition of s is in WA or WB, so in WA×B.
• The empty set is trivially a winning condition on any arena, so in particular on V.
• Let s be a position on A ` B and let t be a P-subposition of s such that t ∈WA`B. Then
either t|A ∈WA, or t|B ∈WB. But t|A is a P-subposition of s|A and t|B is a P-subposition
of s|B, so in both cases we get s ∈WA`B.
• Let s be a position on A`B such that s ∈WA`B and let t be an O-subposition of s. Then
t|A is an O-subposition of s|A and t|B is an O-subposition of s|B. Therefore, if s|A ∈WA
then t|A ∈WA and if s|B ∈WB then t|B ∈WB. In both cases t ∈WA`B.
Finally, in order to have a realizability interpretation for ﬁrst-order logic, the following lemma
(the proof of which is straightforward) is fundamental:
Lemma 6.5. The intersection of winning conditions on a ﬁxed arena is itself a winning condi-
tion.
6.2.3
Winning strategies
In order to deﬁne what a winning strategy is, we use a notion of augmented plays of a strategy
inspired from [Mel05b]:
Deﬁnition 6.7 (Augmented play). If σ is a strategy on A and s is a play on A, then s is an
augmented play of σ if one of the following holds:
• s ∈σ, or
• s is such that ∀t ⊑P s, t ∈σ and ∀t ∈σ, s ̸⊑t.
In particular, in the second case of the above deﬁnition, s is either an O-play, or an inﬁnite
play (in which case s ⊑t ⇔s = t and so the second condition, equivalent to s /∈σ, is always
true since strategies contain only ﬁnite plays).
Unlike [Mel05b], we consider not only odd-
length extensions (with an O-move), but also inﬁnite ones. The following lemma states that the
deﬁnition of composition of strategies can be extended to augmented plays:
97

Lemma 6.6. Let σ : A →B and τ : B →C. If s is an augmented play of σ; τ, then there exists
t ∈Int (A, B, C) such that t|A→B is an augmented play of σ, t|B→C is an augmented play of τ
and t|A→C = s.
Proof. In this proof, we rely on the results of section 4.3.1 about composition of strategies.
There are three possibilities: either s ∈σ; τ, or s = s′a with s′ ∈σ; τ and no extension of s is in
σ; τ, or s is inﬁnite and every P-preﬁx of s is in σ; τ.
• In the ﬁrst case, the result comes from the deﬁnition of composition.
• In the second case, by deﬁnition of composition there is some t′ ∈Int (A, B, C) such that
t′
|A→B ∈σ, t′
|B→C ∈τ and t′
|A→C = s′. By lemma 4.2, the automaton 4.5 is in state 000
after t′. Since a is an O-move in A →C, it is an O-move in A →C or an O-move in
B →C, and therefore t′a ∈Int (A, B, C). Then we extend t′a deterministically with moves
in B by taking at each step the unique move given by σ (if we are in state 101) or by τ
(if we are in state 011) until it is not possible anymore. The move b provided by σ and
τ will always be in B, since otherwise s′ab = sb would be an extension of s which is in
σ; τ and s would not be an augmented play of σ; τ. We call this extension u, and we have
u ∈Int (A, B, C) and u|A→C = s. Either u inﬁnite, or u brings the automaton in state 101
or 011. If u is inﬁnite, then both u|A→B and u|B→C are inﬁnite and therefore augmented
plays of σ and τ. If u brings the automaton in state 101, then σ fails to extend u|A→B so
u|A→B is an augmented play of σ, and u|B→C is even, so u|B→C ∈τ is an augmented play
of τ. Finally, if u brings the automaton in state 011, then τ fails to extend u|B→C so u|B→C
is an augmented play of τ, and u|A→B is even, so u|A→B ∈σ is an augmented play of σ.
• In the third case, or u|A→B and u|B→C are both inﬁnite, in which case they are extended
plays of σ and τ, or u|A→B is ﬁnite and u|B→C is inﬁnite, in which case at some point
all the moves of u are in C so the automaton oscillates between states 000 and 101, so
u|A→B is even and u|A→B ∈σ is an augmented play of σ, and u|B→C being inﬁnite it is an
augmented play of τ, or u|B→C is ﬁnite and u|A→B is inﬁnite, in which case at some point
all the moves of u are in A so the automaton oscillates between states 000 and 011, so
u|B→C is even and u|B→C ∈τ is an augmented play of τ, and u|A→B being inﬁnite it is an
augmented play of σ.
The converse implication of this lemma is also true, however we will not use it. We deﬁne
now the concept of winning strategy:
Deﬁnition 6.8 (Winning strategy). If σ is a strategy on A equipped with the winning condition
WA, then σ is said to be winning if all its augmented plays are winning.
The following lemma will be useful to prove that a strategy σ is winning on (A, WA).
Lemma 6.7. If σ is a strategy on A and if s is an augmented play of σ, then every t ∈
Threads (s) is an augmented play of σ
Proof.
• If s ∈σ, then by single-threadedness of σ, Threads (s) ⊆σ.
• If s is an O-play, then we write s = s′a with s′ ∈σ. Let t ∈Threads (s). If a is not a move in
t, then t ∈Threads (s′) ⊆σ. If a is a move in t, then we write t = t′a, so t′ ∈Threads (s′) ⊆
σ. If there is some b such that tb = t′ab ∈σ, then Threads (s′ab) = (Threads (s′) \ {t′}) ∪
{t′ab} ⊆σ, so by single-threadedness of σ, sb = s′ab ∈σ, contradicting the fact that s is
an augmented play of σ.
98

• If s is inﬁnite, let t ∈Threads (s). If t is ﬁnite, then there is some s′ ⊑P s such that
t ∈Threads (s′), but s′ ∈σ, so by single-threadedness of σ, t ∈σ. If t is inﬁnite, then
for all t′ ⊑P t there is some s′ ⊑P s such that t′ ∈Threads (s′), but s′ ∈σ, so by
single-threadedness of σ, t′ ∈σ.
The above result is the exact analogue of the left-to-right direction of the single-threadedness
condition in the deﬁnition of a strategy, but for the set of augmented plays of a strategy. The
other direction is also true, but will not be used. An augmented play of a strategy which is a
thread will be called an augmented thread of the strategy. Using this lemma, if any augmented
thread of σ is in WA and if s is an augmented play of σ, then for any t ∈Threads (s), t is an
extended thread of σ, so t ∈WA. Then Threads (s) ⊆WA so s is winning. Therefore, it is
suﬃcient to prove that every augmented thread of σ is in WA in order to prove that σ is winning
on (A, WA).
We now prove that the winning conditions on the arrow, product and par (`) are compatible
with identity, application, pairing, projection and codiagonal ∇of strategies. This will imply
that the realizability semantics is preserved through modus ponens, and more generally through
all the propositional rules of the logic.
Lemma 6.8. For any winning condition WA on A, IdA is winning on (A →A, WA→A).
Proof. Let A(1) →A(2) denote the arena A →A. Let s be an augmented thread of IdA such
that Threads

s|A(1)

⊆WA. Since s is a thread, s|A(2) is a thread and s|A(1) = s|A(2) is also a
thread. Then s|A(1) ∈WA, so s|A(2) = s|A(1) ∈WA.
Lemma 6.9. If σ is winning on (A →B, WA→B) and τ is winning on (A, WA), then σ (τ) is
winning on (B, WB).
Proof. Let s be an augmented thread of σ (τ). By lemma 6.6, there is some augmented play t
of σ such that t|A is an augmented play of τ and t|B = s. Since s is a thread, t is also a thread,
so since σ is winning on A →B, t ∈WA→B. t|A is an augmented play of τ which is winning on
A, so Threads
 t|A

⊆WA, and therefore t|B ∈WB. Finally, s = t|B ∈WB. Therefore σ (τ) is
winning.
Lemma 6.10. If σ is winning on (A, WA) and τ is winning on (B, WB), then pair (σ, τ) is
winning on (A × B, WA×B).
Proof. Let s be an augmented thread of pair (σ, τ). Then s is either an augmented thread of
σ on A, in which case s ∈WA, or an augmented thread of τ on B, in which case s ∈WB.
Therefore we get s ∈WA×B, and so pair (σ, τ) is winning.
Lemma 6.11. Strategies proj1 and proj2 are winning respectively on (A × B →A, WA×B→A)
and (A × B →B, WA×B→B).
Proof. We prove the case of proj1, the other case being similar. Let A(1) × B →A(2) denote
the arena A × B →A. Let s be an augmented thread of proj1 such that Threads

s|A(1)×B

⊆
WA×B. By deﬁnition of proj1, s|B = ϵ and s|A(1) = s|A(2). Since s is a thread, s|A(2) is a thread
and s|A(1) = s|A(2) is also a thread. Since s|B = ϵ, s|A(1)×B = s|A(1) is a thread on A(1) so by
deﬁnition of WA×B, s|A(1) ∈WA. Finally, s|A(2) = s|A(1) ∈WA.
Lemma 6.12. The strategy i is winning on (V →A, WV→A).
99

Proof. i is the strategy that answers the unique move b of V to any root move a of A. Therefore,
if s is an augmented thread of i, then s = ab for some root a of A, so s|V = b is losing on (V, WV)
and therefore s ∈WV→A.
Lemma 6.13. The strategy ∇is winning on (A ` A →A, WA`A→A).
Proof. Let A(1) ` A(2) →A(3) denote the arena A ` A →A. Let s be an augmented thread
of ∇such that Threads

s|A(1)`A(2)

⊆WA`A. Since the strategy ∇merges the plays on the
left into a play on the right, s|A(3) is the interleaving of s|A(1) and s|A(2). Since s is a thread,
s|A(3) is a thread and s|A(1) and s|A(2) are also threads. Therefore s|A(1)`A(2) ∈WA`A. By
deﬁnition of WA`A, either s|A(1) ∈WA, or s|A(2) ∈WA. Since s|A(1) ∈WA and s|A(2) ∈WA are
P-subpositions of s|A(3), we get in both cases s|A(3) ∈WA.
In [Sel01], Selinger presents control categories as algebras, providing object and morphism
constructors.
Using lemmas 6.8, 6.9, 6.10, 6.11, 6.12 and 6.13, we can prove that winning
conditions are preserved under all the morphism constructors of control categories, most of
these morphisms being the identity in the case of HO games. This result will be helpful to prove
the adequacy lemma in the propositional case.
The following lemma on the interpretation of cc (see section 3.1.1) illustrates the use of
winning conditions and justiﬁes the notions of O- and P-subpositions in order to interpret
classical logic through the law of Peirce.
Note however that it is also a consequence of the
lemmas above.
Lemma 6.14. For any winning conditions WA and WB, cc is winning on the arena
 ((A →B) →A) →A, W((A→B)→A)→A

Proof. First, write
  A(1) →B

→A(2)
→A(3) for the arena ((A →B) →A) →A. Let now
s be an augmented thread of cc, which is even or inﬁnite since cc is a total strategy. Since s is
a thread and player never plays in B, s is a position on the subarena:
O
ai
P
ai
Ai
O
b1
...
bn
...
Ai
P
ai
ai
O
Ai
Ai
where Ai is a forest such that
ai
Ai
is one of the trees of the arena A, and where b1, ..., bn are the
roots of the arena B. The position s is then of the following form:
ai
ai
s1
...
sm
...
t
bk1
...
bkm
...
t
ai
...
ai
...
s1
...
sm
...
100

where the sj and t are positions on the arena Ai. With these notations we have:
Pos

s|A(1)

=
( ai
s1
; ...;
ai
sm
; ...
)
and Pos

s|A(2)

=
( ai
t
)
which are all P-subpositions of:
s|A(3) =
ai
s1
...
sm
...
t
Suppose now that s|(A(1)→B)→A(2) is winning. Since:
s|(A(1)→B)→A(2) =
ai
bk1
...
bkm
...
t
ai
...
ai
...
s1
...
sm
...
is a position, this means that s|(A(1)→B)→A(2) ∈W(A→B)→A, so:
ai
t
∈WA or for some 1 ≤j ≤m,
bkj
ai
sj
/∈WA→B
In the ﬁrst case, since ait is a P-subposition of s|A(3) we get s|A(3) ∈WA, and in the second case
we get aisj ∈WA, and since aisj is a P-subposition of s|A(3) we get also s|A(3) ∈WA.
6.2.4
WA→B versus Kleene arrow
Let A, B be arenas equipped with winning conditions WA, WB. We deﬁne here a strategy σ on
A →B such that for any winning strategy τ on A, σ (τ) is winning on B, but σ is not winning
on A →B. Hence the arrow on winning conditions diﬀers from the usual Kleene realizability
arrow (see [Tro98]).
We choose A and B to be the same arena A consisting of one root with three children ♯, ♭and
♮, equipped with the winning condition
WA =



qO
aP
1
aP
2
...

∃i, ai ∈{♯, ♮}



where the positions may be ﬁnite or inﬁnite. We deﬁne a strategy σ on A →A such that for any
τ winning on (A, WA), σ (τ) is winning on (A, WA), but σ is not winning on (A →A, WA→A).
σ is the innocent strategy deﬁned by the views:
A
qO
♯P
↑
A
qP
aO
qP
aO
A
qO
♭P
↑
A
qP
aO
qP
bO
where a and b are distinct moves. The interaction with any single threaded strategy produces
the left view, and so the projection qO♯P is winning, but the right view (which will never happen
in an interaction with a single-threaded strategy) with a = ♯and b = ♮is losing, so σ is losing.
101

6.3
Realizability
We ﬁx a ﬁrst-order signature Σ, a Σ-structure M, a λµ signature and interpretations of logic in
λµ-calculus and λµ-calculus in G with the same requirements as in section 5.1.2.
Suppose we have for each predicate P of Σ and each a1, ..., an ∈T1M × ... × TnM a winning
condition WP(a1,...,an) on the arena [P ∗]. We extend this to give a winning condition WA on [A∗]
for every closed formula A on Σ with parameters in M:
W⊥
∆= WV
WA⇒B
∆= WA→B
WA∧B
∆= WA×B
W∀xT A
∆=
\
a∈T M
WA{a/x}
W⊥, WA→B and WA×B are winning conditions by lemma 6.4, and W∀xT A is a winning condition
by lemma 6.5.
We ﬁnally deﬁne the realizability relation ⊩between a strategy σ on the arena [A∗] and the
closed formula A on Σ with parameters in M:
σ ⊩A
∆=
σ is winning on [A∗] equipped with WA
6.3.1
Adequacy for ﬁrst-order logic
We now ﬁx a ﬁrst-order theory Ax on Σ and a term MA for each A ∈Ax, as in section 3.1.3.
The adequacy lemma is as follows:
Lemma 6.15. Suppose that for each A ∈Ax (which is a closed formula) we have:
MA ⊩A
Then for any proof π of a sequent A1, ..., An ⊢A | B1, ..., Bm in the theory Ax and for any substi-
tution θ of an element a ∈T M for each individual variable xT in FV (A1, ..., An, A, B1, ..., Bm)
we have:
π∗is winning on [A1∗] × ... × [An∗] →[A∗] ` [B1∗] ` ... ` [Bm∗]
equipped with WA1{θ}×...×An{θ}→A{θ}`B1{θ}`...`Bm{θ}
Proof. For the propositional part it is a consequence of the remark following lemmas 6.8, 6.9,
6.10, 6.11, 6.12 and 6.13: every morphism constructor of control categories preserves winning-
ness. The remaining rules are those of quantiﬁcations:
• For the introduction of ∀, let π be a proof of Γ ⊢A | ∆and xT /∈FV (Γ, ∆). Let s be an
augmented thread of π∗such that:
Threads
 s|Γ∗

⊆WΓ{θ}
We have to prove that s|A∗`∆∗∈W(∀xT A){θ}`∆{θ}, that is:
s|A∗∈W(∀xT A){θ}
or
s|∆∗∈W∆{θ}
Suppose s|∆∗/∈W∆{θ}, so we have to prove s|A∗∈W(∀xT A){θ}. Let a ∈T M and let
θ′ = θ ∪{a/x}. Since x /∈FV (Γ), Γ {θ′} = Γ {θ}, therefore Threads
 s|Γ∗

⊆WΓ{θ′} and
the induction hypothesis gives s|A∗`∆∗∈WA{θ′}`∆{θ′}. Since x /∈FV (∆), ∆{θ′} = ∆{θ}
so we get s|A∗`∆∗∈WA{θ′}`∆{θ}, and since we supposed that s|∆∗/∈W∆{θ} we obtain
s|A∗∈WA{θ′}. Finally, this is true for every a ∈T M, so we get:
s|A∗∈W(∀xT A){θ}
102

• For the elimination of ∀, let π be a proof of Γ ⊢∀xT A | ∆, let tT be a ﬁrst-order term,
and let s be an augmented thread of π∗such that:
Threads
 s|Γ∗

⊆WΓ{θ}
Since (modulo α-conversion) x /∈FV (Γ), we have Γ {θ} = Γ

tT {θ} /xT 	
{θ} and so by
induction hypothesis we get:
s|A∗`∆∗∈WA{tT {θ}/xT }{θ}`∆{tT {θ}/xT }{θ}
Since A

tT {θ} /xT 	
{θ} =
 A

tT /xT 	
{θ} and (again modulo α-conversion) x /∈FV (Γ)
so Γ {θ} = Γ

tT {θ} /xT 	
{θ}, we get ﬁnally:
s|A∗`∆∗∈W(A{tT /xT }){θ}`∆{θ}
6.3.2
Adequacy for Peano arithmetic
We ﬁx now the theory to be PAωr on ΣPAωr, M to be a model of PAωr, the signature of λµ-
calculus to be that of µPCF, the interpretation of PAωr in µPCF to be that of section 3.1.3 for
the predicates and of section 3.4.1 for the axioms, and the interpretation of µPCF in G to be
that of section 4.4. We also make the same requirements on M as in section 5.2.2.
The winning conditions for the two predicates are:
Wa̸=T b
∆=
(
∅
if a = b
{q}
otherwise
WLaιM
∆=



qO
aP
1
aP
2
...

∃i, ai = a



Since (̸=T )∗= 0 and [0] = V is the one move arena, on which there is only one thread, q, {q} is
the set of all threads on [̸=T ∗] and is therefore trivially a winning condition. WLaιM is a winning
condition since the position:
qO
aP
1
aP
2
...
that may be ﬁnite or inﬁnite has only itself as O-subposition and
qO
aP
i1
aP
i2
...
for 1 ≤i1 < i2 < ... as P-subpositions, and if it is winning then some aik is equal to a.
First, as in the previous model, the identity realizes the equalities which are true in the
model:
Lemma 6.16. Let tT and uT be ﬁrst-order terms with FV
 tT , uT 
= ⃗x⃗U.
If M ⊨∀⃗x
⃗U tT = uT then λx.x ⊩∀⃗x
⃗U tT = uT
103

Proof. Let ⃗a ∈⃗UM. Since M ⊨∀⃗x⃗U tT = uT , we have
 tT {⃗a/⃗x}
M =
 uT {⃗a/⃗x}
M, so:
WtT {⃗a/⃗x}̸=uT {⃗a/⃗x} = ∅= W⊥
Therefore by lemma 6.8, λx.x = IdV is winning on:
WtT {⃗a/⃗x}=uT {⃗a/⃗x} = WtT {⃗a/⃗x}̸=uT {⃗a/⃗x}⇒⊥= W⊥⇒⊥
and so λx.x ⊩∀⃗x⃗U tT = uT .
Therefore, since M is a model of PAωr, we have immediately the following results:
M(refl) = λx.x ⊩∀xT (x =T x)
M(∆s) = λx.x ⊩∀xT→U→V ∀yT→U∀zT (s x y z =V x z (y z))
M(∆k) = λx.x ⊩∀xT ∀yU (k x y =T x)
M(∆rec0) = λx.x ⊩∀xT ∀yι→T→T (rec x y 0 =T x)
M(∆recS) = λx.x ⊩∀xT ∀yι→T→T ∀zι (rec x y (S z) =T y z (rec x y z))
The non-confusion axiom and Leibniz scheme are easy:
Lemma 6.17.
M(Snz) = Ω⊩∀xι (S x ̸=ι 0)
M(Leib) = λx.x ⊩∀⃗z
⃗U ∀xT ∀yT (¬A ⇒A {y/x} ⇒x ̸=T y)
Proof. Let n ∈ιM, we have (S n)M = n + 1 ̸= 0 = 0M, so WS n̸=0 = {q} is the set of all threads
on [0] = V, so any strategy realizes ∀xι (S x ̸=ι 0).
Let now ⃗a ∈⃗UM and b, c ∈T M. If b ̸= c, then Wb̸=c = {q}, which is the set of all threads
on [0], so for any thread s on [(A∗→0) →A∗→0], the projection of s on the rightmost V is in
Wb̸=c and therefore s is in W¬A{b/x,⃗a/⃗z}⇒A{c/x,⃗a/⃗z}⇒b̸=c. Otherwise, if b = c, then Wb̸=c = W⊥
and WA{b/x,⃗a/⃗z} = WA{c/x,⃗a/⃗z}, so:
W¬A{b/x,⃗a/⃗z} = WA{c/x,⃗a/⃗z}⇒b̸=c
and by lemma 6.8 M(Leib) = λx.x = Id[A∗→0] ⊩¬A {b/x,⃗a/⃗z} ⇒A {c/x,⃗a/⃗z} ⇒b ̸= c.
The induction axiom scheme is a little bit more complicated:
Lemma 6.18.
M(indr) = rec ⊩∀⃗y
⃗U (A {0/x} ⇒∀rxι (A ⇒A {S x/x}) ⇒∀rxιA)
Proof. It will be useful in the proof to write:
M(indr) = rec = λxy.Y (λuz.if0 z x (y (pred z) (u (pred z))))
= λxy.λz.if0 z x (y (pred z) (rec x y (pred z)))
= (λuxy.λz.if0 z x (y (pred z) (u x y (pred z)))) rec
= rec; (λuxyz.if0 z x (y (pred z) (u x y (pred z))))
where “;“ is the categorical composition and:
104

λuxyz.if0 z x (y (pred z) (u x y (pred z)))
: (A∗→(ι →A∗→A∗) →ι →A∗) →A∗→(ι →A∗→A∗) →ι →A∗
We will also write:
B = [A∗](1) →

[ι](1) →[A∗](2) →[A∗](3)
→[ι](2) →[A∗](4)
so λuxyz.if0 z x (y (pred z) (u x y (pred z))) is a strategy on B →B (that we write B(1) →B(2),
and we write therefore [A∗](14), [ι](21), ...). Let now s be an augmented thread of M(indr) = rec.
There exists an augmented thread t of λuxyz.if0 z x (y (pred z) (u x y (pred z))) such that t|B(1) is
an augmented play of rec and t|B(2) = s. Suppose s|A∗(1) is winning on A {0/x} and suppose
s|ι(1)→A∗(2)→A∗(3) is winning on ∀rxι (A ⇒A {S x/x}). We prove by induction on n ∈N that if
s|ι(2) is winning on LnM, then s|A∗(4) ∈WA{n/x}:
• n = 0: s|ι(2) being winning on L0M means that every qP in s|[ι](2) = t|[ι](22) has some 0O
pointing to it, t has then the following shape:
B(1) →[A∗](21) →

[ι](21) →[A∗](22) →[A∗](23)
→[ι](22) →[A∗](24)
qO
qP
...
...
...
...
0O
qP
...
...
...
...
t1
t1
...
...
Moreover, keeping only the moves written in the ﬁgure gives a P-subthread of t. We have
qP t1 ∈Threads

s|[A∗](1)

so qP t1 ∈WA{0/x}, therefore qOt1 ∈WA{0/x}, but qOt1 is a
P-subthread of t|A∗(4) = s|A∗(4), therefore s|A∗(4) ∈WA{0/x}.
• n + 1: s|ι(2) being winning on Ln + 1M means that every qP in s|[ι](2) = t|[ι](22) has some
(n + 1)O pointing to it, t has then the shape given in ﬁgure 6.1. The pairs of threads
qOti/qP ti (i = 2, 3, 4) and qOnP /qP (n + 1)P may appear several times, and keeping only
the moves written in the ﬁgure gives a P-subthread of t. qOt3 = qP t3 ∈WA{0/x} and
qOt4 = qP t4 ∈W∀rxι(A⇒A{S x/x}) by hypothesis. Since s|[ι](2) = t|[ι](22) is winning on Ln+1M,
t|[ι](12) is winning on LnM, and since t|B(1) is an extended play of rec, we have by induction
hypothesis qOt2 = qP t2 ∈WA{n/x}.
Again, s|[ι](2) = t|[ι](22) is winning on Ln + 1M so
t|[ι](21) = s|[ι](1) is winning on LnM. Therefore, since s|[ι](1)→[A∗](2)→[A∗](3) ∈W∀rxι(A⇒A{S x/x})
we get qOt1 = qP t1 ∈WA{n+1/x}.
The last axioms are the relativization ones:
Lemma 6.19.
ML0M = 0 ⊩L0M
MLSM = succ ⊩LSM
105

 
[A∗](11) →

[ι](11) →[A∗](12) →[A∗](13)
→[ι](12) →[A∗](14)
!
→[A∗](21) →

[ι](21) →[A∗](22) →[A∗](23)
→
[ι](22)
→[A∗](24)
qO
qP
...
...
...
...
(n + 1)O
qP
...
...
...
...
t1
t1
...
...
...
...
qO
qP
...
...
...
...
(n + 1)O
nP
...
...
...
...
qO
qP
...
...
...
...
t2
t2
qO
qP
...
...
...
...
t3
t3
...
...
...
...
qO
qP
...
...
...
...
t4
t4
...
...
...
...
qO
qP
...
...
...
...
(n + 1)O
nP
...
...
...
...
Figure 6.1: an extended interaction between rec and λuxyz.if0 z x (y (pred z) (u x y (pred z)))
106

Proof. The only augmented thread of 0 is q 0 , which is obviously in WL0M. The augmented
threads of succ are of the form:
s = q q n1 n1 + 1 ... nk nk + 1 ...
Let n ∈ιM. First, s|[ι](1) is a thread. If s|[ι](1) ∈WLnM, then ni = n for some i, and so ni + 1
appears in s|[ι](2). Therefore s|[ι](2) ∈WLn+1M = WL(S n)MM, so s ∈WLnMM⇒L(S n)MM. It follows
that:
s ∈W∀xι(LxM⇒LS xM) = WLSM
6.3.3
Extraction through Friedman translation
In order to extract algorithms from Π0
2-formulas provable in Peano arithmetic, we perform a
variant of Friedman translation [Fri78] on classical proofs. Contrary to the previous model, the
Friedman translation does not appear here before this section. Indeed, since the model in this
chapter is not orthogonality-based, we did not need the notion of realizers and counter-realizers
interacting, so the set ⊥⊥was unnecessary through the proofs of adequacy.
In the original
work of Friedman, the translation of an intuitionistic formula A is obtained by replacing each
atomic predicate P (t1, ..., tn) or ⊥with the disjunction P (t1, ..., tn) ∨Q or ⊥∨Q, where Q is
a ﬁxed predicate. Here, we use our classical system with multi-conclusioned sequents to simply
add Q in the conclusion of every sequent. Indeed, in our system the right weakening rule is
admissible, while in intuitionistic systems (with only one formula on the right of a sequent)
the right weakening does not exist and Friedman translation must be deﬁned inductively on
the structure of formulas. Therefore, our translation is a version of Friedman’s original one in
a classical setting. Moreover, since we want to use it to extract computational content from
proofs, we also provide a realizability interpretation for this predicate.
Formally, Friedman translation in our setting amounts to adding a particular, ﬁxed nullary
predicate Q in the signature Σ of PAωr, then adding this predicate to every sequent of a rela-
tivized proof in PAωr the following way:
Γ ⊢A | ∆
⇝
Γ ⊢A | ∆, Q
and then interpreting these proofs in µPCF by adding a ﬁxed free µ-variable κ to every term of
µPCF. This transformation is adequate with respect to realizability, since the right weakening
rule is admissible. Because we extract from proofs in PAω algorithms that compute natural
numbers, we choose the type of κ to be Q∗∆= ι (which is the type of natural numbers in PAω).
In order to have a realizability interpretation for the new predicate Q, we ﬁx a set ⊥⊥⊆ιM.
As in section 5.1.2, ⊥⊥is intuitively the set of correct values that can be output through the
µ-variable κ, so we will ﬁx it wisely in the proofs of correctness of extracted algorithms. The
winning condition on ι associated to the predicate Q is then:
WQ
∆=



qO
aP
1
aP
2
...

∃i, ai ∈⊥⊥



It is a winning condition since the position:
qO
aP
1
aP
2
...
107

that may be ﬁnite or inﬁnite has only itself as O-subposition and
qO
aP
i1
aP
i2
...
for 1 ≤i1 < i2 < ... as P-subpositions, and if it is winning then some aik is in ⊥⊥.
The adequacy lemma for this extension of PAωr with a new predicate Q is immediate since
we do not change the axioms and the admissible rule of right weakening preserves realizability
(as a consequence of lemma 6.12). Since we added the new predicate Q, a proof π of a closed
formula A is now translated to a term ⊢π∗: A∗| κ : Q∗with a free µ-variable κ of type Q∗= ι,
that is a strategy in G from 1 to [A∗] ` [ι]. In order to manipulate these strategies using the
syntax of λµ-calculus, we use the convention that such strategies have a free µ-variable κ, so we
can write for example [κ] π∗: 1 →[0] ` [ι] if A∗= ι, or µκ.π∗: 1 →[ι] if A∗= 0. We now prove
the extraction result, in which the equality t =U u is, as in the previous model, at any type:
Lemma 6.20. From a proof of ⊢∀xT ∃yι (t =U u) in PAω, one can extract a λµ-term M : T →ι
such that for any a ∈T M and σ ⊩LaM, there is some n ∈N such that:
(t {a/x, n/y})M = (u {a/x, n/y})M and M σ = n
Proof. First, we obtain by double-negation elimination a proof of ⊢∀xT ¬∀yι (t ̸=U u), and
by relativization a proof in PAωr of ⊢∀rxT ¬∀ryι (t ̸=U u) which becomes through Friedman’s
translation a proof π of ⊢∀rxT ¬∀ryι (t ̸=U u) | Q. The adequacy lemma then tells us:
π∗is winning on ∀rxT (¬∀ryι (t ̸=U u)) ` Q
Then, if a ∈T M and σ ⊩LaM, we get that π∗σ is winning on (¬∀ryι (t {a/x} ̸=U u {a/x})) ` Q.
Let now ﬁx:
⊥⊥=
n
n ∈ιM  (t {a/x, n/y})M = (u {a/x, n/y})Mo
This choice of ⊥⊥allows use to prove the following intermediary lemma:
Lemma 6.21. The strategy ⊢λx. [κ] x : ι →0 | κ : ι is winning on:

[ι](1) →[0]

` [ι](2), W(∀ryι(t{a/x}̸=u{a/x}))`Q

Proof. First the arenas [ι](1) →[0] and [ι](2) are:
[ι](1) →[0]
[ι](2)
q
q
0
1
...
q
0
1
...
so the arena

[ι](1) →[0]

` [ι](2), obtained by merging the roots of [ι](1) →[0] and [ι](2), is
actually equal to [ι](1) →[ι](2), and by lemma 4.16 λx. [κ] x is the identity strategy on [ι]. Let
now s be an extended thread of λx. [κ] x. We must prove that:
s|[ι](1)→[0] is winning on

[ι](1) →[0], W∀ryι(t{a/x}̸=u{a/x})

or s|[ι](2) is winning on

[ι](2), WQ

108

These two projections have the following shape:
s|[ι](1)→[0]
s|[ι](2)
q
q
n0
n1
...
q
n0
n1
...
There are now two possibilities: if some ni is in ⊥⊥, then s|[ι](2) is winning on

[ι](2), WQ

,
otherwise we must prove that s|[ι](1)→[0] is winning on:

[ι](1) →[0], W∀ryι(t{a/x}̸=u{a/x})

so let n ∈N and suppose that s|[ι](1) is winning on

[ι](1), WLnM

. By deﬁnition of WLnM, n is
some ni, and therefore n /∈⊥⊥so by deﬁnition of ⊥⊥, (t {a/x, n/y})M ̸= (u {a/x, n/y})M. In
that case, W(t{a/x,n/y})̸=(u{a/x,n/y}) = {q} and s|[0] ∈W(t{a/x,n/y})̸=(u{a/x,n/y}). This achieves
the proof that λx. [κ] x is winning on:

[ι](1) →[0]

` [ι](2), W(∀ryι(t{a/x}̸=u{a/x}))`Q

Now, using Lemma 6.9 we get that π∗σ (λx. [κ] x) is winning on ⊥` Q, and therefore
µκ.π∗σ (λx. [κ] x) is winning on ([ι], WQ). This means that µκ.π∗σ (λx. [κ] x) is some n such
that n ∈⊥⊥(so (t {a/x, n/y})M = (u {a/x, n/y})M). Indeed, if µκ.π∗σ (λx. [κ] x) is the empty
strategy then its only augmented play is q, which is losing on Q. Finally, we have the claimed
result with M ∆= λu.µκ.π∗u (λx. [κ] x).
In the particular case of T = ι, we get that for any m ∈N, M m is some n such that
(t {m/x, n/y})M = (u {m/x, n/y})M.
6.3.4
About the axiom of choice
In this section, we describe the strategy implementing bar recursion and explain why it does
not realize the axiom of choice as-is. We also give ideas for a modiﬁed version of bar recursion
which would use higher-order references.
We consider here only the simpler axiom of countable choice:
∀rxι¬∀yT ¬A ⇒¬∀zι→T ¬∀rxιA {z x/y}
and the potential realizer:
λuv.barrec (λy.u |y|) v ϵ : (ι →(A∗→0) →0) →((ι →A∗) →0) →0
109

Here is a particular inﬁnite thread of λuv.barrec (λy.u |y|) v ϵ:
 
ι →

A∗→
0

→
0
!
→
 
ι
→A∗
→
0
!
→
0
qO
qP
qO
qP
0O
qP
qO
qP
qO
qP
1O
qP
qO
...
The projections on (ι →(A∗→0) →0) and (ι →A∗) →0 are both winning since the one-move
position is losing on ⊥and (in the general case) on A {n/x, a/y}, and therefore the whole thread
is losing. The issue is that the projection on (ι →A∗) →0 asks successively for the values of
the ﬁrst, second... elements of the sequence that we are building, and each time this element
is outside the part of the sequence that has already been asked, so the bar recursion operator
opens a new thread on the left. This behavior is similar to the one described in section 3.5.4.
The thread deﬁned here does not correspond to any interaction, since we are in a single-threaded
setting, and any realizer of ∀zι→T ¬∀rxιA {z x/y} asks for the same sequence of xs each time
it is started. However, one of our model’s peculiarity is that we do not look at interactions
between realizers, but rather to their set of traces, which may not correspond to interactions.
A particular example of this was given in the remark of the end of section 6.2. In the model
deﬁned in the previous chapter, we only looked at interactions happening between strategies, by
using the Kleene arrow, therefore the problem described here did not arise and we proved that
the bar recursion operator realizes the axiom of choice.
In the context of winning conditions, we can imagine a diﬀerent bar recursor which would
call the realizer of ∀zι→T ¬∀rxιA {z x/y} only once, but then update the sequence of elements of
A∗that it gives to this realizer. This could be achieved by considering a higher-order reference
of type ι →A∗, since our model contains non-innocent strategies, and therefore has the power
of higher-order references. A typical thread of such a strategy would look like ﬁgure 6.2. We can
observe that this thread necessarily comes from a non-innocent strategy, because the qP move
27 points the qO move 12 which is not in the current view. Indeed, using references, this bar
recursor “remembers” that the value for n0 has already been asked.
The thread can be interpreted as follows. When the bar recursor is queried at step 1, it gets
this value from its second argument at step 2. When this second argument asks for the function
at step 3, the bar recursor gets the particular index that is asked at steps 4 −5, and then asks
its ﬁrst argument to provide this value at steps 6 −7 −8. When the ﬁrst argument provides the
110

 
ι →

A∗→
0

→
0
!
→
 
ι
→A∗
→
0
!
→
0
qO
1
qP
2
qO
3
qP
4
nO
0
5
qP
6
qO
7
nP
0
8
qO
9
qP
10
s1
s1
11
qO
12
qP
13
s2
s2
14
qO
15
qP
16
nO
1
17
qP
18
qO
19
nP
1
20
qO
21
qP
22
s3
s3
23
qO
24
qP
25
nO
0
26
qP
27
s4
s4
28
Figure 6.2: a thread of bar recursion with references
111

(higher-order) value at step 9, the bar recursor performs double-negation-elimination to copy it
back to the second argument at steps 10 −11. If the ﬁrst arguments provides a new value at
step 12, then this value is also copied back to the second argument at steps 13 −14, behaving
like the cc operator. If the second argument of bar recursor asks for another index at steps
15 −16 −17, then the bar recursor recognizes that this value has not been asked before and
opens a new thread in the ﬁrst argument at steps 18 −19 −20 −21 −22 −23. However, if
the second argument asks again for the element of index n0 at steps 24 −25 −26, then the bar
recursor remembers that it has already been asked and answers with the last value that has been
given by the ﬁrst argument at steps 27 −28.
In addition to be relying on references to provide values on the ﬂy, this bar recursor does
not build a sequence linearly like usual bar recursion, but instead builds the sequence pointwise.
Under this aspect, it seems intuitively much more related to the functional of [BBC98] than to
the modiﬁed bar recursion of [BO05] that was used in the previous chapter.
112

Chapter 7
Conclusion
After Kleene deﬁned his untyped realizability [Kle45] for Heyting arithmetic, Kreisel deﬁned his
modiﬁed realizability [Kre59] by adapting Kleene’s ideas to a typed setting. Kreisel’s realizability
proved to be quite diﬀerent from Kleene’s and opened interesting perspectives. In this thesis
we devised a notion of typed realizability for classical logic, while currently the contributions to
classical realizability are mainly in the lines of Krivine’s untyped models [Kri09]. Also, instead
of working in second-order logic, where the comprehension axiom scheme is a consequence of
the rules of the logic, we work in ﬁrst-order logic and realize the axiom of countable choice
which gives then another interpretation of the comprehension axiom scheme, computationally
diﬀerent.
The framework of game semantics is very interesting when pursuing this goal. First, by
considering non-well-bracketed strategies we get the control features (see [Lai97]) which are
common to every direct computational interpretation of classical proofs. This is enlightened
by the fact that the category of unbracketed games can be seen as a category of continuations
(deﬁned in [Sel01]), and therefore as a model of λµ-calculus. Another feature is that without
the innocence requirement, we are potentially able to deﬁne realizers with imperative features
through the modelling of higher-order references, as shown in [AHM98].
The model based on orthogonality validates the fact that the duality between realizers and
counter-realizers works very well in the context of realizability for ﬁrst-order logic, and proves
that the bar recursion operator which was until now used in intuitionistic realizability [BO05]
ﬁts also well in a classical setting with orthogonality. On the other side, the structure of the
games model where strategies are sets of interactions was heavily exploited when building our
model with winning conditions, and the non-sequential nature of these winning conditions was
made clear using thick subtrees of [Bou09].
In the model based on winning conditions, the usual bar recursion operator does not re-
alize the axiom of choice, but using ideas of the symmetric recursor of Berardi, Bezem and
Coquand [BBC98], we deﬁne a strategy which updates the choice sequence on-the-ﬂy and could
hopefully give a new realizer of the axiom of choice, introducing at the same time imperative
features in the computational interpretation of proofs. In this direction it would be interesting
to compare our winning conditions with the ones of [Coq95], in which the author uses inﬁnitary
sequent calculus for ﬁrst order quantiﬁcations.
Another question is the relationship with the clock operator of Krivine [Kri03], which is
used to realize the second-order axiom of choice. In Krivine’s setting, this operator needs to
live in a countable model of realizers, while the continuity argument of bar recursion relies
on the existence of all (even non-computable) sequences in the model, which must therefore
be uncountable.
For these reasons, the clock operator and the bar recursion operator seem
incompatible. However, the games model has also the interesting property that while the set
113

of all strategies is uncountable, the set of all interactions is countable. Therefore, since the
realizability relation in the model with winning conditions is deﬁned by looking at interactions,
an enumeration of these may suﬃce to mimick the behavior of Krivine’s clock, leading to a single
model in which the two realizations of choice co-exist.
114

Bibliography
[AHM98] Samson Abramsky, Kohei Honda, and Guy McCusker. A Fully Abstract Game Seman-
tics for General References. In 13th Annual IEEE Symposium on Logic in Computer
Science, pages 334–344. IEEE Computer Society, 1998.
[AHS07]
Zena Ariola, Hugo Herbelin, and Amr Sabry. A proof-theoretic foundation of abortive
continuations. Higher-Order and Symbolic Computation, 20(4):403–429, 2007.
[AJM00] Samson Abramsky, Radha Jagadeesan, and Pasquale Malacaria. Full Abstraction for
PCF. Information and Computation, 163(2):409–470, 2000.
[AM96]
Samson Abramsky and Guy McCusker.
Linearity, Sharing and State: a fully ab-
stract game semantics for Idealized Algol with active expressions. Electronic Notes in
Theoretical Computer Science, 3:2–14, 1996.
[AM97]
Samson Abramsky and Guy McCusker. Call-by-Value Games. In 6th EACSL Annual
Conference on Computer Science Logic, Lecture Notes in Computer Science, pages
1–17. Springer, 1997.
[BBC98] Stefano Berardi, Marc Bezem, and Thierry Coquand. On the Computational Content
of the Axiom of Choice. Journal of Symbolic Logic, 63(2):600–622, 1998.
[BC82]
Gérard Berry and Pierre-Louis Curien. Sequential Algorithms on Concrete Data Struc-
tures. Theoretical Computer Science, 20(3):265–321, 1982.
[Blo13]
Valentin Blot. Realizability for Peano Arithmetic with Winning Conditions in HON
Games. In 11th International Conference on Typed Lambda Calculi and Applications,
volume 7941 of Lecture Notes in Computer Science, pages 77–92. Springer, 2013.
[BO05]
Ulrich Berger and Paulo Oliva. Modiﬁed bar recursion and classical dependent choice.
In Logic Colloquium ’01, Proceedings of the Annual European Summer Meeting of the
Association for Symbolic Logic, volume 20 of Lecture Notes in Logic, pages 89–107. A
K Peters, Ltd., 2005.
[BO06]
Ulrich Berger and Paulo Oliva. Modiﬁed bar recursion. Mathematical Structures in
Computer Science, 16(2):163–183, 2006.
[Bou09]
Pierre Boudes. Thick Subtrees, Games and Experiments. In 9th International Con-
ference on Typed Lambda Calculi and Applications, volume 5608 of Lecture Notes in
Computer Science, pages 65–79. Springer, 2009.
[BR13]
Valentin Blot and Colin Riba. On Bar Recursion and Choice in a Classical Setting.
In 11th Asian Symposium on Programming Languages and Systems, volume 8301 of
Lecture Notes in Computer Science, pages 349–364. Springer, 2013.
115

[Can74]
Georg Cantor.
Ueber eine Eigenschaft des Inbegriﬀes aller reellen algebraischen
Zahlen. Journal für die reine und angewandte Mathematik, 77:258–262, 1874.
[Can92]
Georg Cantor. Ueber eine elementare Frage der Mannigfaltigketislehre. Jahresbericht
der Deutschen Mathematiker-Vereinigung, 1:75–78, 1892.
[CF58]
Haskell Curry and Robert Feys. Combinatory Logic, volume 22 of Studies in Logic
and The Foundations of Mathematics. Elsevier, 1958.
[CH00]
Pierre-Louis Curien and Hugo Herbelin. The duality of computation. In 5th Interna-
tional Conference on Functional Programming, pages 233–243. ACM Press, 2000.
[CH09]
Pierre-Louis Curien and Hugo Herbelin.
Abstract Machines for Dialogue Games.
Panoramas et synthèses, 27:231–275, 2009.
[Chu36]
Alonzo Church. An Unsolvable Problem of Elementary Number Theory. American
Journal of Mathematics, 58(2):345–363, 1936.
[Cla09]
Pierre Clairambault.
Least and Greatest Fixpoints in Game Semantics.
In 12th
International Conference on Foundations Of Software Science And Computational
Structures, Lecture Notes in Computer Science, pages 16–31. Springer, 2009.
[Cla11]
Pierre Clairambault. Estimation of the Length of Interactions in Arena Game Se-
mantics. In 14th International Conference on Foundations of Software Science and
Computational Structures, pages 335–349. Springer, 2011.
[Cla12]
Pierre Clairambault. Isomorphisms of types in the presence of higher-order references
(extended version). Logical Methods in Computer Science, 8(3), 2012.
[Coq95]
Thierry Coquand.
A Semantics of Evidence for Classical Arithmetic.
Journal of
Symbolic Logic, 60(1):325–337, 1995.
[Cur07]
Pierre-Louis Curien. Deﬁnability and Full Abstraction. Electronic Notes in Theoretical
Computer Science, 172:301–310, 2007.
[dG94]
Philippe de Groote.
On the Relation between the Lambda-Mu-Calculus and the
Syntactic Theory of Sequential Control. In 5th International Conference on Logic
Programming and Automated Reasoning, volume 822 of Lecture Notes in Computer
Science, pages 31–43. Springer, 1994.
[DHR96] Vincent Danos, Hugo Herbelin, and Laurent Regnier. Game Semantics & Abstract
Machines. In 11th IEEE Symposium on Logic in Computer Science, pages 394–405.
IEEE Computer Society, 1996.
[DP01]
René David and Walter Py. λµ-calculus and Böhm’s theorem. Journal of Symbolic
Logic, 66(1):407–413, 2001.
[Fre79]
Gottlob Frege. Begriﬀsschrift, eine der mathematischen nachgebildete Formelsprache
des reinen Denkens. Nebert, Louis, 1879.
[Fri78]
Harvey Friedman. Classically and intuitionistically provably recursive functions. In
Gert Müller and Dana Scott, editors, Higher Set Theory, volume 669 of Lecture Notes
in Mathematics, pages 21–27. Springer, 1978.
116

[Gen35]
Gerhard Gentzen. Untersuchungen über das logische Schließen. I, II. Mathematische
zeitschrift, 39(1):176–210, 405–431, 1935.
[GM03]
Dan Ghica and Guy McCusker. The regular-language semantics of second-order ide-
alized ALGOL. Theoretical Computer Science, 309(1-3):469–502, 2003.
[Göd29]
Kurt Gödel.
Über die Vollständigkeit des Logikkalküls.
PhD thesis, University of
Vienna, 1929.
[Göd31]
Kurt Gödel. Über formal unentscheidbare Sätze der Principia Mathematica und ver-
wandter Systeme I. Monatshefte für mathematik und physik, 38(1):173–198, 1931.
[Göd40]
Kurt Gödel. The consistency of the continuum hypothesis. Annals of Mathematics
Studies. Princeton University Press, 1940.
[Göd58]
Kurt Gödel. Über eine bisher noch nicht benützte erweiterung des ﬁniten standpunk-
tes. Dialectica, 12(3-4):280–287, 1958.
[Gri90]
Timothy Griﬃn. A Formulae-as-Types Notion of Control. In 17th Symposium on
Principles of Programming Languages, pages 47–58. ACM Press, 1990.
[Her95]
Hugo Herbelin.
Séquents qu’on calcule: de l’interprétation du calcul des séquents
comme calcul de λ-termes et comme calcul de stratégies gagnantes. PhD thesis, Uni-
versité Paris 7, 1995.
[HO00]
Martin Hyland and Luke Ong. On Full Abstraction for PCF: I, II, and III. Information
and Computation, 163(2):285–408, 2000.
[How80]
William Alvin Howard. The formulae-as-types notion of construction. To HB Curry:
Essays on Combinatory Logic, Lambda Calculus and Formalism, pages 479–490, 1980.
[HS02]
Martin Hofmann and Thomas Streicher. Completeness of Continuation Models for
λµ-Calculus. Information and Computation, 179(2):332–355, 2002.
[HS09]
Hugo Herbelin and Alexis Saurin. λµ-calculus and Λµ-calculus: a Capital Diﬀerence.
http://hal.inria.fr/inria-00524942, 2009.
[Hyl97]
Martin Hyland. Game semantics. In Andrew Pitts and Peter Dybjer, editors, Seman-
tics and logics of computation, Publications of the Newton Institute, chapter 4, pages
131–184. Cambridge University Press, 1997.
[Kle45]
Stephen Cole Kleene. On the Interpretation of Intuitionistic Number Theory. Journal
of Symbolic Logic, 10(4):109–124, 1945.
[Koh90]
Ulrich Kohlenbach. Theory of majorizable and continuous functionals and their use for
the extraction of bounds from non-constructive proofs: eﬀective moduli of uniqueness
for best approximations from ineﬀective proofs of uniqueness.
PhD thesis, Goethe
Universität Frankfurt, 1990.
[Koh08]
Ulrich Kohlenbach.
Applied Proof Theory: Proof Interpretations and their Use in
Mathematics. Springer Monographs in Mathematics. Springer, 2008.
[Kre59]
Georg Kreisel. Interpretation of analysis by means of constructive functionals of ﬁnite
types. In Constructivity in mathematics: Proceedings of the colloquium held at Ams-
terdam, 1957, Studies in Logic and the Foundations of Mathematics, pages 101–128.
North-Holland Publishing Company, 1959.
117

[Kri94]
Jean-Louis Krivine. A General Storage Theorem for Integers in Call-by-Name lambda-
Calculus. Theoretical Computer Science, 129(1):79–94, 1994.
[Kri01]
Jean-Louis Krivine. Typed lambda-calculus in classical Zermelo-Frænkel set theory.
Archive for Mathematical Logic, 40(3):189–205, 2001.
[Kri03]
Jean-Louis Krivine. Dependent choice, ‘quote’ and the clock. Theoretical Computer
Science, 308(1–3):259–276, 2003.
[Kri09]
Jean-Louis Krivine. Realizability in classical logic. Panoramas et synthèses, 27:197–
229, 2009.
[Lai97]
James Laird. Full Abstraction for Functional Languages with Control. In 12th An-
nual IEEE Symposium on Logic in Computer Science, pages 58–67. IEEE Computer
Society, 1997.
[Lai99]
James Laird. A semantic analysis of control. PhD thesis, University of Edinburgh,
1999.
[Lau05]
Olivier Laurent. Classical isomorphisms of types. Mathematical Structures in Com-
puter Science, 15(5):969–1004, 2005.
[Loa01]
Ralph Loader. Finitary PCF is not decidable. Theoretical Computer Science, 266(1-
2):341–364, 2001.
[LRS93]
Yves Lafont, Bernhard Reus, and Thomas Streicher.
Continuations Semantics or
Expressing Implication by Negation. Technical Report 93-21, Ludwig-Maximilians-
Universität, München, 1993.
[Mel05a] Paul-André Melliès.
Asynchronous Games 3 An Innocent Model of Linear Logic.
Electronic Notes in Theoretical Computer Science, 122:171–192, 2005.
[Mel05b] Paul-André Melliès. Sequential algorithms and strongly stable functions. Theoretical
Computer Science, 343(1–2):237–281, 2005.
[Mel06]
Paul-André Melliès.
Asynchronous games 2: The true concurrency of innocence.
Theoretical Computer Science, 358(2-3):200–228, 2006.
[Mil77]
Robin Milner. Fully abstract models of typed λ-calculi. Theoretical Computer Science,
4(1):1–22, 1977.
[Miq11]
Alexandre Miquel. Existential witness extraction in classical realizability and via a
negative translation. Logical Methods in Computer Science, 7(2), 2011.
[Nic94]
Hanno Nickau. Hereditarily Sequential Functionals. In Third International Symposium
on Logical Foundations of Computer Science, Lecture Notes in Computer Science,
pages 253–264. Springer, 1994.
[Oli08]
Paulo Oliva. An analysis of Gödel’s Dialectica interpretation via linear logic. Dialec-
tica, 62(2):269–290, 2008.
[Ong96]
Luke Ong. A Semantic View of Classical Proofs: Type-Theoretic, Categorical, and
Denotational Characterizations (Preliminary Extended Abstract).
In 11th Annual
IEEE Symposium on Logic in Computer Science, pages 230–241. IEEE Computer
Society, 1996.
118

[OS97]
Luke Ong and Charles Stewart. A Curry-Howard Foundation for Functional Compu-
tation with Control. In 24th Symposium on Principles of Programming Languages,
pages 215–227. ACM Press, 1997.
[Par92]
Michel Parigot.
λµ-Calculus: An Algorithmic Interpretation of Classical Natural
Deduction. In 3rd International Conference on Logic Programming and Automated
Reasoning, volume 624 of Lecture Notes in Computer Science, pages 190–201. Springer,
1992.
[Plo77]
Gordon Plotkin. LCF Considered as a Programming Language. Theoretical Computer
Science, 5(3):223–255, 1977.
[Pow13]
Thomas Powell. On bar recursive interpretations of analysis. PhD thesis, Queen Mary
University of London, 2013.
[Pow14]
Thomas Powell. The equivalence of bar recursion and open recursion. Annals of Pure
and Applied Logic, 165(11):1727–1754, 2014.
[RT99]
Jon Riecke and Hayo Thielecke. Typed Exeptions and Continuations Cannot Macro-
Express Each Other. In 26th International Colloquium on Automata, Languages and
Programming, pages 635–644. Springer, 1999.
[RW13]
Bertrand Russell and Alfred North Whitehead. Principia Mathematica, volume 1, 2,
3. Cambridge University Press, 1910, 1912, 1913.
[Sau05]
Alexis Saurin. Separation with Streams in the Λµ-calculus. In 20th IEEE Symposium
on Logic in Computer Science, pages 356–365. IEEE Computer Society, 2005.
[Sco93]
Dana Scott. A Type-Theoretical Alternative to ISWIM, CUCH, OWHY. Theoretical
Computer Science, 121(1-2):411–440, 1993.
[Sel01]
Peter Selinger. Control categories and duality: on the categorical semantics of the λµ
calculus. Mathematical Structures in Computer Science, 11(2):207–260, 2001.
[Spe62]
Cliﬀord Spector. Provably recursive functionals of analysis: a consistency proof of
analysis by an extension of principles in current intuitionistic mathematics. In Recur-
sive Function Theory: Proceedings of Symposia in Pure Mathematics, volume 5, pages
1–27. American Mathematical Society, 1962.
[Str06]
Thomas Streicher. Domain-theoretic foundations of functional programming. World
Scientiﬁc, 2006.
[Tro98]
Anne Sjerp Troelstra. Realizability. In Samuel Buss, editor, Handbook of Proof Theory,
volume 137 of Studies in Logic and the Foundations of Mathematics, chapter 6, pages
407–473. Elsevier, 1998.
[Tur37]
Alan Turing. On computable numbers, with an application to the Entscheidungsprob-
lem. Proceedings of the London Mathematical Society, 42:230–265, 1937.
[TVD88] Anne Sjerp Troelstra and Dirk Van Dalen. Constructivism in mathematics: an in-
troduction, volume 121, 123 of Studies in logic and the foundations of mathematics.
Elsevier, 1988.
[Zer08]
Ernst Zermelo. Untersuchungen über die Grundlagen der Mengenlehre. I. Mathema-
tische Annalen, 65(2):261–281, 1908.
119

