A Divide and Conquer Approach
to Using Inductive Logic
Programming for Learning User
Models
Heather Maclaren
This thesis is submitted in partial fulﬁlment of the
requirements for the degree of
Doctor of Philosophy.
University of York
York
YO10 5DD
UK
Department of Computer Science
June 2003

2
Abstract
The use of Inductive Logic Programming (ILP) for the construction of a user model allows
possibilities such as the use of intensional background knowledge and the ability to present
a set of comprehensible rules to the user. A few simple concepts have previously been
modelled using standard ILP methods but a concept of the complexity attempted within
this dissertation brings problems which have not been attempted within the user-modelling
context.
This dissertation describes the problems encountered (and their solutions) when using ILP
to build a user model that will be used to make detailed predictions of successive entries
within an individual user’s online diary. Each predicted entry must contain several values
— not just the type of task being performed, but also its location and duration — thus
increasing the complexity of the clauses required. Other problems to be dealt with are the
vague nature of the underlying concept to be modelled and the possibility of high levels
of noise within the data collected from the user. The complexity of the clauses required
and the large amount of background knowledge that could be used renders the size of the
search space of possible hypotheses too large for an exploratory ILP engine such as Aleph
to search adequately within the limited time available.
The methods required to overcome these problems are presented within the algorithm
Dilum (Dimensional ILP for User Modelling). Dilum pre-processes its given data set to
split the concept to be learnt into separate subconcepts, and uses a divide-and-conquer
approach when working with each subconcept to reduce the complexity of each learning
problem to a more manageable level. The main contribution within this disseration is
the use of these two techniques to signiﬁcantly reduce the amount of computation time
required for the kind of learning problem encountered within the scenario described earlier.
It will be shown within this dissertation that the ideas encapsulated within Dilum enable
construction of an accurate, usable, and understandable user-model, and that if the concept
to be learnt is both vague and complex then even a greedy learning algorithm will require
partitioning as a data pre-processing stage. It will also be shown that using conditional
probability distributions as a measure of uncertainty whilst querying the user model not
only helps to reduce the eﬀects of noisy data on the accuracy of the predictions made by
the model but also increases its precision.

Contents
1
Introduction
12
1.1
Case Study
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
1.2
Using Inductive Logic Programming to Construct a
Model of the User
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
1.3
Hypothesis
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
1.4
Outline of the Dissertation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
2
Background
25
2.1
User Modelling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
2.2
Using Machine Learning to Build User Models
. . . . . . . . . . . . . . . .
27
2.2.1
User Modelling within Calendar and Diary Agents . . . . . . . . . .
28
2.3
Inductive Logic Programming . . . . . . . . . . . . . . . . . . . . . . . . . .
29
2.3.1
Two ILP Problem Settings
. . . . . . . . . . . . . . . . . . . . . . .
30
2.3.2
Techniques used in the Normal ILP Setting . . . . . . . . . . . . . .
33
2.4
Previous Approaches to Learning Diﬃculties in the Normal ILP Setting . .
35
2.4.1
Positive-only Learning . . . . . . . . . . . . . . . . . . . . . . . . . .
35
2.4.2
A Small Data Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
2.4.3
Learning with Noisy Data . . . . . . . . . . . . . . . . . . . . . . . .
37
2.4.4
Exploring Large Search Spaces . . . . . . . . . . . . . . . . . . . . .
39
2.5
Summary and Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
3
Creating a User Model Using ILP
44
3.1
The Data Collected from iDiary . . . . . . . . . . . . . . . . . . . . . . . . .
44
3.2
ILP Method Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
3

CONTENTS
4
3.2.1
Aleph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
3.2.2
Claudien
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
3.2.3
WARMR
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
3.2.4
Overall Conclusions
. . . . . . . . . . . . . . . . . . . . . . . . . . .
64
3.3
Partitioning the Set of Sequence Examples . . . . . . . . . . . . . . . . . . .
66
3.4
Measuring Distance Between Tasks . . . . . . . . . . . . . . . . . . . . . . .
72
3.4.1
Constructing the Diﬀerence between Tasks
. . . . . . . . . . . . . .
74
3.4.2
A Task Comparison Model
. . . . . . . . . . . . . . . . . . . . . . .
75
3.5
Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
3.6
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78
4
Multiple Value Predictions
79
4.1
Creating Specialised rules . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
4.2
Splitting The Search Space
. . . . . . . . . . . . . . . . . . . . . . . . . . .
84
4.2.1
Working with Contradictory Example Sets
. . . . . . . . . . . . . .
87
4.3
A Worked Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
90
4.4
Evaluation and Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
96
4.5
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
4.6
Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
4.7
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
5
Characteristics of the Concept Space
106
5.1
Building a Model of Personalised Distance Measurement . . . . . . . . . . . 106
5.2
Constructing the Diﬀerence Rating Model . . . . . . . . . . . . . . . . . . . 108
5.2.1
Generation of Negative Examples . . . . . . . . . . . . . . . . . . . . 111
5.2.2
Combining the Solutions Gathered . . . . . . . . . . . . . . . . . . . 117
5.2.3
A Worked Example
. . . . . . . . . . . . . . . . . . . . . . . . . . . 120
5.3
Evaluation of the Model Produced . . . . . . . . . . . . . . . . . . . . . . . 125
5.4
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
5.5
Exploring the Concept Constraints . . . . . . . . . . . . . . . . . . . . . . . 132
5.6
Comparing Dilum’s and TILDE’s Learning Strategy
. . . . . . . . . . . . . 133

CONTENTS
5
5.7
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
6
Learning Complex Vague Concepts
135
6.1
Learning Problems that Beneﬁt from Partitioning of the Data Set
. . . . . 136
6.1.1
A relation over two tasks that works with categorisable values . . . . 137
6.1.2
A relation over pairs of task attributes that should be given a range 141
6.1.3
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
6.2
The Eﬀect of Partitioning on Eﬃcient Learning and Concept Learnability . 151
6.3
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
7
Answering Queries with the User Model
161
7.1
Generating the Conditional Probability Distributions . . . . . . . . . . . . . 163
7.2
Generating and Filtering Answers . . . . . . . . . . . . . . . . . . . . . . . . 164
7.3
A Worked Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
7.4
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
8
Evaluation
170
8.1
Evaluation using Real Data . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
8.2
Evaluation using Artiﬁcial Data . . . . . . . . . . . . . . . . . . . . . . . . . 173
8.2.1
Evaluation Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
8.2.2
Comparing Dilum to other User Model Construction Methods . . . . 178
8.2.3
Comparing Aleph using Dilum and TILDE using Dilum pt 1
. . . . 180
8.3
Evaluating the Eﬀect of Dilum on Aleph’s Scalability . . . . . . . . . . . . . 185
8.3.1
Experiment 1: Increasing Background Knowledge . . . . . . . . . . . 186
8.3.2
Experiment 2: Increasing Length of Target Clause . . . . . . . . . . 189
8.3.3
Experiment 3: Increasing Number of Concepts
. . . . . . . . . . . . 191
8.3.4
Summary and Discussion
. . . . . . . . . . . . . . . . . . . . . . . . 195
8.4
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
9
Conclusions
198
9.1
Overview and Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
9.1.1
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202

CONTENTS
6
9.2
Future Work
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
9.2.1
Completion of the Modelling System . . . . . . . . . . . . . . . . . . 204
9.2.2
Expansion of the User Model . . . . . . . . . . . . . . . . . . . . . . 205
9.2.3
Other Application Areas . . . . . . . . . . . . . . . . . . . . . . . . . 208

List of Tables
3.1
Recorded features of each task
. . . . . . . . . . . . . . . . . . . . . . . . .
72
4.1
Comparison of possible approaches to learning a user model using Aleph . .
97
4.2
Comparison of Search Space Sizes when Splitting the Concept . . . . . . . .
99
4.3
Timing Comparison for Aleph and TILDE . . . . . . . . . . . . . . . . . . .
99
6.1
Comparison of search space explored with full and partitioned data sets . . 145
7

List of Figures
1.1
An example of a rule that would be included within iDiary’s user model . .
18
2.1
An example space containing clusters of positive examples . . . . . . . . . .
35
3.1
Two of the attempts to model examples for Claudien . . . . . . . . . . . . .
58
3.2
Two possible representations of an example within WARMR
. . . . . . . .
61
3.3
A representation of the Leaders clustering algorithm . . . . . . . . . . . . .
69
3.4
The bottom-up agglomerative clustering algorithm used . . . . . . . . . . .
70
4.1
The full Dilum method for exploratory ILP algorithms . . . . . . . . . . . .
81
4.2
Method for the generation of sets of negative examples . . . . . . . . . . . .
83
4.3
An example of learning a hypothesis using independent dimensions . . . . .
86
4.4
Duration values from 2 positive examples and their respective negative values 88
4.5
Method for the generation of non-contradictory sets of negative examples
.
89
4.6
Translation from TILDE results to standalone rules . . . . . . . . . . . . . . 101
5.1
New method for the generation of non-contradictory sets of negative examples113
5.2
Comparison between ILP methods using split and full background knowledge127
6.1
Concept 1: A relation linking two tasks that works with categorisable values 138
6.2
Concept 2: A relation over pairs of task attributes that should be given a
range . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
7.1
The bucket sorting algorithm used for partitioning within the query module 165
7.2
Some of the tasks predicted to precede the query task with ratings . . . . . 169
8.1
An example of a rule within a model built using real data . . . . . . . . . . 173
8

LIST OF FIGURES
9
8.2
The method naive Bayes classiﬁcation used in evaluation against Dilum
. . 174
8.3
The method of k-nearest neighbour used in evaluation against Dilum . . . . 175
8.4
Results of user model construction method evaluation
. . . . . . . . . . . . 177
8.5
The preferred answer and actual answer returned from TILDE
. . . . . . . 181
8.6
Representations of a cluster built by i) TILDE ii) Aleph . . . . . . . . . . . 182
8.7
Two possible representations of the same cluster build by TILDE . . . . . . 184
8.8
Results of Comparing the Scalability of Aleph and Aleph using Dilum for
increasing sizes of ⊥. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
8.9
Results of Comparing the Scalability of Aleph and Aleph using Dilum for
increasing sizes of target clause . . . . . . . . . . . . . . . . . . . . . . . . . 190
8.10 Results of Comparing the Scalability of Aleph and Aleph using Dilum for
increasing numbers of concepts — 6 Dimensions . . . . . . . . . . . . . . . . 193
8.11 Results of Comparing the Scalability of Aleph and Aleph using Dilum for
increasing numbers of concepts — 10 Dimensions . . . . . . . . . . . . . . . 194
9.1
A Simple Bayesian Network . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
9.2
The Conditional Probability Table for node C . . . . . . . . . . . . . . . . . 206

10
Acknowledgments
Thanks should go, ﬁrst and foremost, to my supervisor, James Cussens, for guiding me
through the various paths I’ve travelled during this learning process with both enthusiasm
and patience, and for giving advice that always seemed, in retrospect, to have been at
precisely the right time and pointing in the right direction.
I would like to thank the people that I worked with at the BT Research Laboratory (who
funded this PhD with a CASE studentship) for allowing me to join them in Ipswich for a
year and for aiding me with support and suggestions during this time. There are speciﬁc
people who gave me invaluable guidance and tremendous encouragement, but I shall refrain
from naming them to save embarrasment. They know who they are and I am very grateful
to them.
I would also like to thank the groups of people who have seen me through the ups and
downs over the past few years in York, Ipswich, Colchester, various scattered parts of
Britain, and now Vancouver, and also my family. Even though for the majority of the
time that I was focused on this work I couldn’t talk about what I was working on (due
to it being commercially sensitive material), I still received unconditional support and
enthusiastic enquiries as to how things were going. Now (having got the patents!) I can
ﬁnally explain what it’s all been about.

11
Declaration
This thesis has not previously been accepted in substance for any degree and is not being
concurrently submitted in candidature for any degree other than Doctor of Philosophy of
the University of York. This thesis is the result of my own investigations, except where
otherwise stated. Other sources are acknowledged by explicit references.
I hereby give consent for my thesis, if accepted, to be made available for photocopying
and for inter-library loan, and for the title and summary to be made available to outside
organisations.
Signed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . (candidate)
Date . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Chapter 1
Introduction
The production of systems that build proﬁles of their users and use that information in
order to improve or personalise their performance or services is a steadily growing area.
By building a model of the user, a system can tailor its responses to each individual and
therefore interact with them more eﬀectively.
The examples that are most commonly
encountered by the average user are adaptive web-sites that retain information on their
visitors. Amazon’s website is a very well known example. It retains information on the
products that a visitor views and then uses this to make personalised recommendations.
However, any system that interacts with a human has the ability to produce a model of
that interaction if it can gather enough information. Other examples of types of systems
that use models of their users include intelligent tutoring systems and personal assistants.
Project LISTEN [Mostow and Aist, 2001, Beck et al., 2003], which helps students learn
how to read English, uses a model of each student to predict when they will need help
with pronunciation. Newsdude [Billsus and Pazzani, 1999] is an example of a personal
assistant that helps its user by ﬁnding news items that it thinks they will ﬁnd interesting
based on their previous choices.
As well as enabling a system to tailor its responses to its user’s needs, a user model can
be used to predict future actions or requests that the user may make. An example is the
model built by Swiftﬁle (formerly known as MailCat [Segal and Kephart, 1999]), which
helps its user to organise the Email messages that arrive in their Inbox by predicting the
folders in which they are most likely to want to ﬁle each message.
However, when a system uses a user model to make suggestions to the user, the system
must also be careful when deciding how much of this information is presented. A system
such as Newsdude could suggest a large number of stories that are potentially of interest,
Mailcat could suggest several folders in which to store mail. In both cases, a large set
of suggestions would eﬀectively negate the reduction of user eﬀort that the systems are
trying to produce. Both Newsdude and Mailcat use intelligent methods to decide how
much information to present and thus avoid this situation.
12

CHAPTER 1. INTRODUCTION
13
In summary, by building a model that enables it to predict a user’s next action or sequence
of actions, and intelligently using this information to make useful suggestions, a system
can reduce the amount of eﬀort or time expended by the user in order to carry out their
current task.
1.1
Case Study
The results of this research will be used within a personal assistant currently under de-
velopment at BT Exact Technologies. The personal assistant known as iDiary (originally
part of a suite known as the Intelligent Assistant [Wobcke et al., 2000]), is an agent that
works with the user’s calendar.
The assistant needs to be able to predict tasks that the user will want to schedule in
immediate sequence. The assistant can collect examples of these sequences by gathering
pairs of tasks that are scheduled together (or within an hour of each other) within the
calendar. The requirement of the user modelling system is that, given a task that the
user has just entered into the diary, the agent must be able to use the model to produce a
small set of predictions of other tasks that the user may wish to schedule before or after
it. A reasonably sized set of predictions would mean a set small enough that the user
would be easily about to choose between them (e.g. three predictions might be considered
reasonable, but twenty is very unlikely to be viewed positively by the user). Some of the
beneﬁts to the user of making these predictions are that they can remind them of tasks
that they may have forgotten to include or simply reduce the amount of eﬀort involved in
scheduling more than one task into their diary.
For example, if the user has a presentation to give, they may wish to put some preparation
time immediately before it to make sure the room is set, check their laptop is working,
etc.
When the user enters a presentation task into iDiary, the model should suggest
some preparation time to be scheduled before it. The details of the tasks that should be
predicted include:
• Type — e.g. that it is a preparation task,
• Subject — e.g. the project the task is associated with,
• Location — the location in which the task will be carried out, and
• Expected Duration.
Subject and Location contain additional information about the task, depending on what
the user has entered. Subject will contain information such as the project of a task, or the
name of a company that is associated with the task. Location will contain information
on where the task is to be carried out (usually a room or geographical location, although

CHAPTER 1. INTRODUCTION
14
values from this latter group may be included within the subject of the task if the user is
travelling to that location). Whilst including more than one type of information within
these variables is not as controlled as using diﬀerent variables for diﬀerent items of infor-
mation, the user often will not provide information on all of the possible attributes of a
task that they enter into their diary. This reduced set of attributes provides a suﬃcient
amount of information for this initial investigation, and could easily be expanded to a
larger collection of attributes if required. Details of how examples of tasks in immediate
sequence are collected from iDiary to form a data set from which to construct a model are
given in Section 3.1. Once the model has been used to generate a set of predictions, the
modelling system should then reduce this set to a size considered reasonable by the user
and before returning the ﬁnal set to the agent.
Another long-term goal is to extend the modelling of immediate sequences to the modelling
of dependencies between tasks that may not necessarily be scheduled on the same day. For
example, a presentation will require time to be written and reviewed but this does not
normally occur on the day in which it is to be given. This model is more likely to be of use
to the user but adds additional diﬃculties to the modelling problem due to the requirement
of a prediction of task time and date in addition to the other attributes. Since the simpler
problem of predicting immediate sequences already presented a number of problems to
be solved, this problem was concentrated on ﬁrst with the aim of expansion or further
development once the initial model could be constructed.
There are several areas that will present diﬃculties when constructing the user model of
immediate sequences:
1. There will only be a small number of examples with which to attempt to learn the
model.
2. The model must supply as many of the details within the prediction as possible. If
it cannot be used to predict a majority of the details then the model’s suggestions
cannot be viewed as ‘helpful’ as the user will have to do almost as much work as
when the model was not being used (possibly more if the model produces the wrong
answer, which leads to the fourth problem). This means that the learning method
is not dealing with a relation over pairs of items but a relation over pairs of groups
of items, and the model is required to predict multiple values rather than just the
name of an action.
3. The sequences predicted using the model will be given as lists of suggestions to the
user. Therefore the model of the user must have as high a level of recall as possible
to ensure that these suggestions are ‘sensible’ (i.e. tasks that the user may actually
want to carry out rather than tasks with a nonsensical description). In addition,
the number of predictions made for each task that the user enters must be kept low
enough that the user does not feel that the system is wasting their time rather than

CHAPTER 1. INTRODUCTION
15
saving it, i.e. the model must have a high level of precision. Recall and precision
are deﬁned in Equations 1.1 and 1.2 respectively. These terms will be referred to
throughout the dissertation when making decisions on how to produce a ‘good’ model
of the user’s scheduling habits.
recall = |A ∩Ar|
|Ar|
(1.1)
precision = |A ∩Ar|
|A|
(1.2)
Where Ar is the set of correct answers and A is the set of answers returned by
the model. The precise contents of Ar will be deﬁned later when it is used as an
evaluation measure in Chapter 8.
4. A model with a high level of recall should be produced even in the presence of noise.
5. The model must be constructed within a limited time. Whilst it may be possible
to carry out some form of ‘batch learning’ at the end of the day, this cannot be
guaranteed. The user may well switch oﬀtheir computer when they go home. Even
if the information for a group of users is dealt with on a server instead of their own
machine, if the server must maintain a large number of user models in addition to
other maintenance activities then this is still likely to restrict the amount of time
available.
In addition, the user should have complete control and so be able to
request at any point in time that a new or updated model is created from the data
that has recently been collected. Whilst it is unreasonable to expect a model with
a high level of recall to be produced ‘instantly’, the process should be as fast as
possible in order to reduce the user’s frustration.
As sequences may contain more than two tasks (e.g. a sales pitch at a client site will
require travel and preparation as well as the actual meeting), predictions of sequences
containing multiple tasks will be required at some stage. However, further investigation of
this area was not pursued as the production of a model of the user that worked with pairs
of tasks in sequence and could be used to predict all the required attributes of those tasks
provided a suﬃciently large problem for this initial investigation. This dissertation will
also not address the maintenance of the user model produced or the eﬀect of time on the
importance of examples as the diﬃculty in carrying out the initial generation provides a
suﬃciently large number of problems to be dealt with. In addition, it was initially thought
to include the context of the sequence within each example in order to pick out special
situations such as bank holidays or birthdays, but as the number of examples of each
sequence was so sparse, regularities of this type would be rare. However, there is scope to
include this information at some point if it is deemed useful.

CHAPTER 1. INTRODUCTION
16
1.2
Using Inductive Logic Programming to Construct a
Model of the User
Inductive Logic Programming (ILP) [Muggleton and De Raedt, 1994] is a form of machine
learning that generates a theory, usually in the form of a logic program, from a set of data
and background knowledge. The set of relations (the hypothesis) produced is an attempt
to capture the underlying relations between the items of data. Background knowledge,
expressed in the form of predicate deﬁnitions, is provided by the user to be used in the
construction of these relations.
Whilst ILP algorithms can take several forms, the two main settings of ILP problem to be
solved are either the nonmonotonic setting or the normal setting. In the normal problem
setting, the relations produced should imply the positive examples in the given data set.
In the nonmonotonic problem setting, the relations produced are not required to imply
the positive examples, but must instead hold over the entire data set whilst referrring to
some or all of the attributes of each example. The nonmonotonic setting will be expanded
upon in Section 2.3 and illustrated in Sections 3.2.2 and 3.2.3, where two algorithms that
work in this setting will be evaluated as possible candidates to produce the user model for
iDiary.
Currently-popular ILP algorithms in the normal setting construct clauses to be included
in the hypothesis produced by starting from the most general clause and performing a top-
down search through the tree of possible specialisations. When constructing each clause,
the learner makes choices based on its given strategy for searching the space of possi-
ble hypotheses. Greedy ILP algorithms (such as TILDE [Blockeel and De Raedt, 1998]),
will construct hypotheses to explain the given data by choosing the specialisation that is
deemed to best explain the data at that point in the learning process. Once a choice is
made, the learner is committed to that specialisation and cannot backtrack and choose
something else. Exploratory ILP algorithms (such as Aleph [Srinivasan, 2001]) can back-
track and try other reﬁnements, thus exploring more of the space of possible hypotheses
and increasing the likelihood of ﬁnding the best hypothesis to explain the given data.
Since the methods of producing possible hypotheses are many and varied, and the focus
of this dissertation is to solve implementation issues rather than general theoretical issues,
a solution that will enable some ILP methods to produce the user model may not be
applicable to other ILP methods. Evaluation in Chapter 3 of possible ILP algorithms that
work in the nonmonotonic setting show that this setting is not suitable for the user model
to be constructed. Therefore, other than during the aforementioned evaluation and the
background in Section 2.3, this dissertation will focus on algorithms such as the ones used
by Aleph, TILDE and other similar methods. All further references to ILP will refer to
this group of algorithms rather than all ILP algorithms in general.
In order to generate the model of the user, examples of pairs of tasks that have been

CHAPTER 1. INTRODUCTION
17
entered into the iDiary as sequences would be collected by observing the user’s scheduling
habits. Each day within iDiary would have a number of tasks and activities associated
with it that would be carried out by the user during the day, and each task would have a
certain amount of information associated with it:
task(type(booking),subject(flights),location(n/a),
duration(1/00),preferred_time(14-30),preferred_day(wednesday))
Tasks were represented in this manner rather than as an attribute-value vector as the
additional labelling made the information captured easier to read for a human who was
unfamiliar with it.
Examples of sequences to be included in the model are:
1. A user requiring some preparation time before a presentation on a project.
seq(task(type(prep),subject(idiary),location(n/a),
duration(1/00),preferred_time(13-00),preferred_day(wednesday)),
task(type(presentation),subject(idiary),location(103),
duration(1/00),preferred_time(14-30),preferred_day(wednesday))).
2. A user requiring travel time before visiting a company. This is slightly more com-
plicated since the company may be near to the user’s own company or it could be
much further away, requiring longer travel time.
seq(task(type(travel),subject(york),location(n/a),
duration(4/00),preferred_time(8-00),preferred_day(wednesday)),
task(type(visit),subject(york_uni),location(n/a),
duration(4/00),preferred_time(13-00),preferred_day(wednesday))).
seq(task(type(travel),subject(london),location(n/a),
duration(1/30),preferred_time(10-00),preferred_day(wednesday)),
task(type(visit),subject(ericsson),location(n/a),
duration(2/00),preferred_time(13-00),preferred_day(wednesday))).
The background information available for use within the construction of the model included
the groups to which particular values within the type, subject and location of each task
belonged. For example, london would be identiﬁed as belonging to the group place and
nokia would belong to the group company. Other groups that identiﬁed values as people,
projects, etc. were also provided. The values contained within the duration attribute of
each task had a slightly diﬀerent set of information. The possible values for duration
were drawn from a set of discrete values that represented lengths of time from 0/00 to

CHAPTER 1. INTRODUCTION
18
seq(task(type(A),subject(B),location(C),
duration(D),preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),
duration(J),preferred_time(K),preferred_day(L))):-
A = travel, place(B), located_at(H,B),
G = visit, company(H), C = n/a, I = n/a,
lessthan(D,2/0), greaterthan(D,0/0),
lessthan(J,2/0), greaterthan(J,0/0).
Figure 1.1: An example of a rule that would be included within iDiary’s user model
24/00 at half hour intervals. Rather than attempt to place particular values into groups,
the ordering over these values was incorporated into the background predicates provided
to work with these attributes to allow a range of possible values to be speciﬁed within any
rules generated.
The diﬃculties with using most forms of machine learning, including ILP, to create a user
model, given the data that is available from which to learn, are present in several forms.
The data collected from the user will contain a high level of noise, i.e. items of data that
are not really examples of a concept to be learnt but the system is unable to reason that
they should be discarded. Even if the data collected does not contain any noisy examples,
the overall theory represented by the data is of an imprecise nature. However, the model
constructed should still not frustrate the user by providing incorrect answers or too many
answers to choose from, i.e. the model must have a high level of recall and precision.
Typically, a large number of examples would be needed to learn a theory with acceptable
levels of recall and precision. Only a small number of examples will be available in this
case, therefore some method or safeguard with which to combat this problem must be
introduced.
In addition, the predictions made using the model will not be of some simple value such as
a classiﬁcation. The model is required to predict additional tasks that the user may wish
to schedule within their diary in enough detail that the user does not have to include any
further information, therefore each answer returned by the model has several values which
must be predicted. As stated earlier, the recall of the user model is very important. If a
task is entered into iDiary that is unlike any that have been encountered previously then
the model should not make any predictions of possible sequences. This would mean that
ideally any description of a relation between two tasks in sequence would contain enough
detail about each task to ensure that only those sequences previously collected from the
user, or sequences that are similar, would be predicted. Figure 1.1 shows an example of
a rule that the model would be required to include. The details of preferred time and
preferred day have been ignored for now since the initial learning problem will focus on
immediate sequences and hence these predictions can be made without including them in

CHAPTER 1. INTRODUCTION
19
the rules (this is explained in more detail in Section 7.3). The requirement for this level of
detail means that the concept that must be learnt requires disjointed areas of the space of
possible examples of sequences to be covered by the model produced. This increases the
complexity of the user-modelling task because these areas must be identiﬁed and then an
individual hypothesis produced for each area.
The most common approach to generating a model that will be used to predict multiple
values is to treat the prediction of each value as a separate problem for which a model must
be built (this was the approach used within the calendar agent CAP [Dent et al., 1992]).
The relation to be learnt in each case would be a relation between a group of values (the
attributes of the task entered by the user), and a single value (the attribute predicted
as output). The other possible approach is to learn relations that predict all the values
of a task at once.
The attributes of each input task are required to have some form
of specialisation so that the high level of recall (mentioned as a requirement earlier) is
ensured, and using the approach outlined above will result in a large number of rules
containing the same specialisations (one per task attribute to be predicted). Constructing
a relation between two complete tasks in sequence is much more eﬃcient because it would
allow the model to contain symmetrical relations (i.e. ones that can predict both following
and preceding tasks) whilst incorporating the requirement for relations with high recall
and producing a representation that is more understandable for the user.
The information contained within the examples collected will not be enough to generate
a theory which can carry out the required task. Without the provision of some additional
stimuli, the learner will be unaware of the requirement to produce a theory which contains
such a high level of complexity and will not create a model that can be used to predict all
the values needed. This situation is demonstrated in Section 3.2.1 when initial attempts
at learning using Aleph are made. The most diﬃcult problem to be overcome is that the
amount of time available in which to produce the model is limited and the size of the search
space of possible hypotheses is so large that, even if a suitable hypothesis exists, the learner
cannot be guaranteed to ﬁnd it within the time allowed. The size of the search space is
due to the length of the clauses that must be produced and the amount of background
knowledge that can be applied to each example of the concept. Some mild forms of bias
can be given to the learner in the form of background information to direct it towards
particular parts of the search space, for example restricting the application of background
predicates to particular variables. However, this will not reduce the amount of space that
must be searched to the extent that an appropriate hypothesis can be guaranteed to be
produced. In order to achieve this goal, the size of the search space must be cut down
considerably through some manner external to the learner itself.
Whilst a few user models have been built before using ILP [Jacobs, 2000, Jacobs, 1999,
Dastani et al., 2002], none have attempted to build models of the complexity required by
iDiary and so have not faced the problems presented by the construction of such a model.

CHAPTER 1. INTRODUCTION
20
Calendar assistants have been produced before, most notably by Kozierok and Maes [1992]
and Dent et al. [1992]; however the emphasis has been on scheduling meetings within the
user’s diary according to their preferences rather than predicting new tasks which should
also be included.
A model containing rules such as the example shown in Figure 1.1 will also present prob-
lems with regards to the number of predictions made. Although the ranges of possible task
durations have been restricted, the rule will still produce several predictions per query,
and each of these predictions will be of almost exactly the same task. Presenting all of
these tasks to the user would not conform to the requirement for a reasonable number of
predictions, thus development of a method that can reduce the set of predictions produced
is also required.
Given the vast array of modelling techniques that have been created, there are approaches
to this modelling problem other than ILP (for instance plan recognition [Bauer and Paul,
1993, Jachowicz and Goebel, 1997, Litman and Allen, 1984]), that could be used to produce
the required user model. The focus of this dissertation is not to assert that ILP is the
best method to use for this modelling problem. This work looks at whether it is possible
at all to build and use a model of this complexity using ILP, and what needs to be done
in order to achieve this.
1.3
Hypothesis
The use of ILP to produce a user model allows possibilities such as the use of intensional
background knowledge and the ability to present a set of comprehensible rules to the
user. It solves diﬃculties previously encountered, but also brings with it a new set of
problems to be dealt with. The vague nature of the underlying concept to be modelled,
the lack of information from which it can construct that model, the need for multiple-value
predictions, and the size of the hypothesis space which must be searched within a limited
time require a diﬀerent approach, new methods and some ‘damage limitation’ techniques
to be created.
The methods required to overcome the problems discussed lead to the
following hypothesis, which will be evaluated by this dissertation:
It is possible to use ILP eﬀectively to create, within a limited (reasonable)
time, a usable user model of a vague concept with a large hypothesis space, if
the model to be learnt is broken down into separate subconcepts, each concept
is learnt as a sum of separate dimensions, and a measure of uncertainty is
incorporated into the model query process.
In order for the model produced by the system to be considered usable it must satisfy all
of the performance measures described later in this section. These measures are recall,

CHAPTER 1. INTRODUCTION
21
precision, speed of learning and comprehensibility. The group of user models that this
hypothesis is intended to cover are those where the model is required to make multiple-
valued predictions, where speciﬁc examples of a concept can be demonstrated by the
user and where additional background knowledge is available for incorporation within the
model constructed. This means models where it is considered that using ILP would provide
beneﬁts to the models constructed, but the nature of the concepts contained within those
models means that using ILP to learn them presents the diﬃculties outlined earlier. The
case study provides an example of a member of this group of user models, and other
possible models are suggested later in Chapter 9.
The hypothesis incorporates two contributions that will be made:
1. The formulation of two techniques — splitting the problem and partitioning the data
— that can signiﬁcantly reduce the computation time required by a generalising
exploratory learner to solve the kind of learning problem that arises in the scenario
described in Section 1.1. The computation time for such learners also scales better
for this problem when using these techniques than without them.
2. Some predictions made by a user model may contain variables for which a range of
possible values are valid (e.g. valid duration values for a task could range between 30
minutes and 2 hours in increments of half an hour). The precision of these models
can be improved by using a probability distribution, generated from the original
examples, to decide which of the possible values predicted is most likely to be the
one required by the user, and hence the value that should be returned.
The ﬁrst contribution described above uses a combination of two techniques:
1. If a generalising learner is required to generate a user model which is vague, complex,
and consists solely of positive examples, then partitioning the data into separate sub-
concepts to be dealt with individually will reduce the levels of noise and interference
within the data set, and hence improve the learner’s eﬃciency in carrying out this
task.
2. A given time limit in which an exploratory ILP learner must learn a concept will limit
the size of the search space of possible hypotheses that it will be able to conclusively
explore. If an assumption of independence can be used to divide the concept to be
learnt and the relevant background knowledge into separate dimensions, which can
be dealt with individually and the results combined, then areas of the hypothesis
space that are known not to contain the target hypothesis can be excluded from
the search space that the learner must explore.
This will help systems that use
exploratory ILP learners to work with larger search spaces of possible hypotheses
than they would normally be able to cope with within a given time limit. Some form

CHAPTER 1. INTRODUCTION
22
of stimuli should also be provided to ensure that the learner focuses properly on each
dimensional learning problem that it is presented with.
In this dissertation the following deﬁnitions will be used:
Deﬁnition (independent concepts) Two concepts are said to be indepen-
dent of each other if the most concise possible representation of each concept
that can be generated using ILP can be constructed without the other concept
being available as background knowledge during the construction process.
Deﬁnition (vague concept) A concept is vague if it is made up of seemingly
unrelated subconcepts that would be represented by separate clauses within the
ﬁnal theory produced. For generalising learning methods that produce rules,
this would be a disjunctive concept.
Deﬁnition (complex learning problem) A hypothesis language may con-
tain predicates that a learner can use to generate literals with the same predicate
symbol but diﬀering levels of generality. If the learner is allowed to consider
several of these literals when attempting to construct a theory then the size of
the set of literals available to the learner may be very large. If the size of the
set of literals has been increased in this manner, and each of these predicate
symbols should only appear once in the ﬁnal constructed theory, then the learn-
ing problem (the concept, the hypothesis language, and the learner selected to
construct the theory) is said to be complex.
The performance measures for this hypothesis will be:
Recall and Precision — a pair of measures commonly used to evaluate user models.
These were deﬁned earlier by Equations 1.1 and 1.2. The model constructed using
the ideas within the hypothesis being tested should have recall and precision that
are equal to or better than those produced by current user-modelling methods.
Speed of learning.
Comprehensibility — the model should be understandable to a human reader.
In order for the model produced by the system to be considered usable then it must satisfy
all of the performance measures listed above. It should be able to produce information
that is correct, up to date (hence the requirement for speed), and will avoid increasing the
cognitive load of the user, ensured by the requirements for a comprehensible model with
a high level of precision.
When modelling a user, in addition to producing the model itself, the method by which
the information contained within that model is subsequently used is also an integral part

CHAPTER 1. INTRODUCTION
23
of the system. The main focus of this dissertation is the construction of a model from
data. However, a method for querying the model will also be produced and used to help
overcome the problem of noisy data mentioned earlier. The performance measures of recall
and precision will be for the overall modelling system.
1.4
Outline of the Dissertation
Chapter 2 examines the current state of research in the areas of interest within this dis-
sertation. It identiﬁes the ideas of most use in solving the problem faced.
Chapter 3 begins the initial breakdown of the problem by describing in further detail the
requirements of the model to be produced. The methods used to produce a set of examples
from the tasks entered by the user into iDiary are described. Then potential candidates are
examined for use as the main ILP ‘engine’, focusing on the beneﬁts brought to the model
and the diﬃculties faced by each approach when attempting to produce the required model.
The results of the experiments show that none of the ILP engines investigated provide an
instant solution to modelling task and that Aleph [Srinivasan, 2001] is the most suitable
engine to which additional aid could be given to enable it to construct the user model. The
need to decompose the concept to be learnt into separate subconcepts by partitioning the
data set is demonstrated experimentally, and it is argued that an agglomerative clustering
algorithm is a suitable approach for this task.
The clustering method chosen makes use of a distance measure between the tasks that
make up the examples from which the main user model is learnt and so a suitable distance
measure for use within the clustering algorithm is constructed. Ideally the distance mea-
sure itself would make use of an additional model which would capture the user’s method of
measuring distance, thereby ensuring that the clusters that the examples are split into re-
ﬂect groups of examples that the user would deﬁnitely regard as subconcepts. It is shown,
however, that it is not possible to capture enough data to produce this model. A default
model is deemed to be an adequate replacement for use within the main user-modelling
system. Learning a model of the user’s method of distance measurement is deferred as an
additional exploratory exercise to be carried out later in the dissertation.
Chapter 4 details the methods required to learn a single subconcept.
The algorithm
Dilum, which solves the diﬃculties found in Chapter 3, is presented. An example of how
this algorithm can be applied to learning the required user model is provided.
Initial
evaluation of Dilum is used to gain enough insight to produce a generic version which
could be used with any exploratory learner. It is shown that the ability to split the data
into subconcepts is beneﬁcial to top-down learners in general, not just to Aleph.
Chapter 5 further explores the methods within Dilum by applying them to the problem
deferred earlier, learning a model of the user’s method of distance measurement. The

CHAPTER 1. INTRODUCTION
24
general properties of concepts that can be learnt are determined and discussed. Constraints
that should be met for Dilum to be used are also presented and the extent to which they
must be met considered. The beneﬁt of partitioning the data set as a pre-processing step
for complex learning problems containing disjunctive concepts is also further explored.
Chapter 6 focuses on the partitioning pre-processing stage of Dilum.
The eﬀects of a
data set representing a vague complex concept on the performance of the learner are
demonstrated, especially the interference in the learner’s search method by the presence of
more than one distinct group of examples. It is shown how partitioning the data set will
remove this interference and signiﬁcantly reduce of levels of noise introduced from other
sources.
Chapter 7 details the part of the modelling system that uses the model constructed to
answer queries. Ideas for improving the recall and precision of the system are included
here.
Chapter 8 evaluates the system produced during the previous chapters. Due to the small
size of the set of the real data from which to learn, evaluation of Dilum is split into two
parts. In the ﬁrst part, Dilum is used to learn models from the four sets of real data
available and the models are evaluated in terms of the model evaluation criteria set out
in Section 1.3. In the second part, Dilum is compared to other user-modelling methods,
namely k-nearest neighbour and naive Bayes classiﬁcation. This second evaluation step is
carried out using artiﬁcial data sets to enable a comparison between the full capabilities of
the Dilum and the other user-modelling systems. This step also includes the introduction
of increasing levels of noise into the data. Dilum is shown to outperform both methods
in terms of the evaluation criteria set out in Section 1.3. A ﬁnal experiment is used to
compare the amount of computation time required by Aleph on its own and using Dilum
as the size of the hypothesis space the the learner needs to explore in order to ﬁnd the
target relation increases. The results of the experiment show that the ideas within Dilum
signiﬁcantly improve the scaling of Aleph’s performance for the kind learning problem
described in this chapter.
Chapter 9 concludes with a review of what has been shown, suggestions for possible further
research directions, and consideration of other areas where Dilum or the ideas within it
might be applied.

Chapter 2
Background
The problems outlined within the previous chapter have all been addressed within the
existing literature, although there has been no previous attempt to solve all of the problems
at the same time. There are several areas that should be explored before proceeding to
construct a solution to this user modelling problem.
2.1
User Modelling
Whilst user modelling is an established ﬁeld of research, the sorts of information that can
be collected and used, and the types of model that can be built, are still being explored.
There are a variety of methods used for user modelling, and a wide range of information
can be collected in order to form a proﬁle of the user.
A commonly used introductory example is modelling the user’s interests by watching what
web-pages they visit (e.g. [Zhu et al., 2003]). However, the user’s exploratory patterns
could also be modelled by paying attention to how they navigate from page to page (e.g.
[Pirolli and Fu, 2003]). This kind of information can be very useful for large corporate
websites to determine whether their site design is eﬀective. At a more local level, the
applications on a user’s computer may keep track of how the user tends to behave, for
example keeping track of which functions or options are frequently used and which are
ignored [Bunt et al., 2004].
Collecting information on what or how or when a user interacts with a system or ap-
plication (including mobile devices, ubiquitous computing is also a growing area e.g.
[Kleinbauer et al., 2003]), enables interactions to be tailored to each individual and thus
become more eﬀective. In addition to what, when, and how, why is becoming an interesting
question to be answered via techniques such as aﬀective computing [Picard, 1995].
What sort of information can be used to model a user? Additional answers to this question
are still being discovered.
The information required will depend on the model to be
25

CHAPTER 2. BACKGROUND
26
constructed. For a model used by something such as an information retrieval agent, the
content of documents retrieved, web-pages visited, and Emails received have often been
good sources of information. For aﬀective computing (modelling a user’s emotional state),
how hard the user squeezes the mouse they are using [Scheirer et al., 2002] may be more
important than what they actually use it for.
A black box description of user modelling would be a system that collects information from
the user (or users) and then uses that information for some task. Whether the information
collected is processed when it is collected, or whether the processing is carried out when
the model is used (or some combination of both), is dependent on the individual modelling
method. It is therefore not just collecting and storing the information in some form that
makes up the user modelling system, but also how it is subsequently used.
A user modelling system needs to:
1. Collect suﬃcient information from the user in order to be able to model
them, but without inconveniencing them. Ideally the user would never be
directly asked to provide information, but in some cases this can lead to additional
noise in the data if the system is unable to determine what are typical examples of the
user’s behaviour and what is merely coincidence. This is in addition to users having
a tendency not to exhibit precise behaviours, sometimes producing contradictory
information.
2. Perform any processing autonomously and quickly (the actual speed re-
quirement would depend on the application). This means that any method
for building a model must be proposed with people in general in mind rather than
for the data from a speciﬁc individual. The method should be able to work with
whatever data it is collected.
3. Quickly provide the correct required information. The responses from the
user model must be correct for that particular user, and be received quickly enough
to still be of use. If the responses are returned to the user as suggestions then the
system should ensure that these suggestions are sensible and that there are not too
many of them. Even if a correct suggestion is produced amongst a large number
of others its utility has been decreased because of the eﬀort required on the user’s
part to ﬁnd it.
Ideas relating to satisfying this concern when the model is used
operationally are described in Chapter 7.
4. Keep its information on the user up-to-date. Information collected from a
user will eventually become out of date, and some manner of ensuring that the
model reﬂects the user’s current behaviour or state is essential. This concern is not
addressed in this dissertation, but remains listed as future work.

CHAPTER 2. BACKGROUND
27
2.2
Using Machine Learning to Build User Models
Using a machine learning technique to build a user model is not as easy as it would seem.
The idea of using some form of machine learning seems obvious, but there are inherent
diﬃculties in the problem domain that must be addressed. Users cannot be guaranteed to
behave consistently and thus may not be easily characterised. In addition, collecting the
sort of large data set that would help a machine learning technique to produce a model
with a reasonable level of recall and precision are not always readily available.
Webb, Parsons and Billsus [2001] say that using machine learning for some form of user
modelling has been around for quite a long time (the earliest reference they give is 1978).
Initially the focus was mainly on student modelling (i.e. models for use by intelligent tu-
toring systems). Today it is still used for student modelling, but also elsewhere, especially
in areas such as information retrieval due to the increasing growth of information available
via the internet. One example of how this can help in information retrieval is to consider
a query given to a web search engine returning thousands of possible pages to explore,
most of which are not related to the user’s topic of interest. If the interface to the search
engine was aware of the user’s browsing patterns then it would be able to ﬁlter the results
it presented in order to improve the quality of the information that it produced.
Memory-based reasoning (or case-based reasoning) has been used quite often to build and
maintain a model of the user. Its advantage over other techniques is that it automatically
adapts to any changes in the user’s behaviour since new examples are simply added to the
existing data set. No further processing is carried out until the database receives a query.
Examples of memory-based reasoning can be seen in ONISI [Gorniak and Poole, 2000],
Maxim [Lashkari et al., 1994], CBR-TEAM [Oates et al., 1995]. However memory-based
reasoning needs space in which it can store all of the data it has collected. The system is
then burdened with a continuously growing database. This can be alleviated by examples
being ‘aged’, i.e. their relevance decays over time until they are eventually removed. Maes
and Kozierok [1993] demonstrated this technique in their calendar agent (described in the
next section).
Although decision trees have previously been used for user modelling (e.g. CAP [Dent et al.,
1992] , described in Section 2.2.1), they have a number of disadvantages, such as overﬁtting
and myopia, that must be overcome when applied to currently popular areas of application
such as information retrieval. Recent developments tend towards probabilistic approaches
such as naive Bayes reasoning. The advantage of using a probabilistic approach is that
it enables the modelling system to incorporate some of the uncertainty that is inherent
in the user modelling task. Users do not always display consistent behaviour, and this
approach enables all of the information collected from the user to be stored and used
for prediction without encountering the problems faced when creating a decision tree.
An example of this approach is Syskill & Webert [Pazzani and Billsus, 1997], a system

CHAPTER 2. BACKGROUND
28
designed to identify interesting web sites for its user. Users would rate web pages that
they visited, and the system would then calculate the probability of each feature occuring
in that page appearing in any web pages that the user would ﬁnd interesting. Then, when
a new web page was visited, the system could look at the features of this new page and use
the previously generated probabilities to predict whether the user would ﬁnd it interesting.
IPAM [Davison and Hirsh, 1998] also makes use of probability when predicting simple user
actions. For each action it stores the observed probability that that action follows an action
that the user has just carried out. Zukerman and Albrecht [2001] give an overview of the
various predictive statistical models which have been used.
A few user modelling systems have made use of genetic algorithms (e.g. Newt [Sheth, 1994]).
Some have used ideas from neural networks (e.g. WAWA [Eliassi-Rad and Shavlik, 2003],
INCA [Gervasio et al., 1999]). Few applications have attempted to use Inductive Logic
Programming to produce a model of the user. No arguments have been made as to why
it is not as widely used as other machine learning methods, but the problems encountered
and solved later in this dissertation give some indication as to how diﬃcult it can be to
apply to the sorts of learning problems encountered when attempting to construct a user
model.
ILP has been used to construct user models that can be used to predict one or two values
within a more controlled context than the modelling task that the system used by iDiary
must accomplish. One system [Jacobs, 2000] used WARMR ([Dehaspe and Toivonen, 1999],
described in Sections 2.3 and 3.2.3) to search for regularities amongst the commands col-
lected from the user within a Unix shell in order to predict or correct their actions. Another
[Jacobs, 1999] used TILDE ([Blockeel and De Raedt, 1998], described in Section 2.3) to
analyse web-page preferences for users who visited a particular server. Simple preferences,
such as the user’s level of interest in academic papers focusing on diﬀerent areas, have also
been modelled by Dastani et al. [2002] using TILDE.
2.2.1
User Modelling within Calendar and Diary Agents
As mentioned in Section 1.1, a few calendar agents use machine learning techniques to
build models of their users. The two most prominent calendar agents are described in this
section to show the approaches that have been used and prediction tasks that have been
attempted for this application.
Maes and Kozierok [1992, 1993] produced an agent that used memory-based and reinforce-
ment learning to learn the user’s personal scheduling rules and preferences. The agent was
designed to start out with some initial knowledge and then gradually get to know the user
through learning by observation and direct feedback from the user on suggestions made.
For each interaction between the user and their diary the agent predicted the user’s actions
with an additional conﬁdence measure, taking into account the context in which the in-

CHAPTER 2. BACKGROUND
29
teraction occurred. The given example of interaction was the receipt of a meeting request
by another user, and the agent’s prediction as to whether the request will be accepted.
If this measure was above a certain level set by the user (the ‘do it’ threshold) then the
agent would carry out the action autonomously. If the measure was below that level but
above another level also set by the user (the ‘tell me’ threshold) then the agent would ask
if the user wished the task to be carried out. If the measure was below both levels then
the agent would not do anything. The agent would tell its user about everything that it
did so as to keep the user’s trust.
The reinforcement learning used by the agent was fed by user responses to the agent’s
suggestions. The user was able to tell the agent that it placed too much or too little im-
portance on certain items and the agent would adjust certain internal weights accordingly.
These weights then aﬀected the distance measures used in the memory-based learning.
The agent adapted to the users changing interests by ‘forgetting’ some of the older exam-
ples so that they no longer played a part in queries to the memory-based learning part of
the user model.
Dent et al. [1992] presented a personal learning apprentice called CAP. It used two
competing machine learning methods: a decision-tree learning method similar to ID3
[Quinlan, 1986] and the backpropagation neural network learning method (ﬁrst published
as a neural network learning method by Widrow and Lehr [1990], although the idea ap-
parently [Werbos, 1974] has existed for a few centuries in the ﬁeld of variational calculus).
The system provided advice on the date, length, and location of meetings, although it
was hoped to extend the advice to issues such as meetings having priorities and when
to send reminders to attendees. Each parameter of each command provided a separate
learning task, enabling CAP to separately predict items such as the duration or location
of a meeting.
Both of the calendar agents described work with the details of tasks and events that are
initiated either by the user or by another calendar agent. They do not predict additional
complete tasks that the user may wish to enter into their calendar, i.e. the user modelling
task that iDiary must perform has not been carried out by any previous calendar agents.
2.3
Inductive Logic Programming
In order not to overly inﬂate this section with unnecessary detail, the reader is assumed to
have a basic mathematical grounding in set theory, probability and ﬁrst-order predicate
logic. The reader is also assumed to be familiar with the Prolog programming language
and its notation. Further information on these topics can be found in [Lloyd, 1987] and
[Nienhuys-Cheng and de Wolf, 1997].

CHAPTER 2. BACKGROUND
30
Inductive Logic Programming [Muggleton and De Raedt, 1994] is a method for generat-
ing a logic program, usually in the form a set of predicate deﬁnitions or clauses, from a
set of examples. The set of relations (the hypothesis) produced is an attempt to capture
the underlying relations between the items of data, i.e. the concept that the data repre-
sents. Background knowledge, expressed in the form of predicate deﬁnitions or clauses, is
provided by the user to be used in the construction of these relations. In this case, the
‘user’ refers to the agent iDiary, although it will have been given the information by some
other source, such as the system engineer. [Nienhuys-Cheng and de Wolf, 1997] provide a
detailed description of the facets of ILP.
2.3.1
Two ILP Problem Settings
As mentioned earlier in Section 1.2, there are two common ILP problem settings that can
be solved — the normal setting and the nonmonotonic setting.
In the normal setting, the aim of the ILP algorithm is to produce a set of clauses that
form a hypothesis to explain the given data set. The overall hypothesis forms a deﬁnition
of a new predicate that has been learnt, although additional auxiliary predicates may be
constructed during the learning process. Examples demonstrate combinations of values
(such as attribute values or names of objects) that the ﬁnal predicate should cover. Each
clause in the hypothesis does not necessarily have to cover all of the examples in the data
set. Muggleton and DeRaedt [1994] describe the ILP problem (in the normal setting) more
formally:
The problem of inductive inference is as follows. Given as background (prior)
knowledge B and evidence E. E = E+ ∧E−consists of positive evidence E+
and negative evidence E−. The aim is then to ﬁnd a hypothesis H such that
the following conditions hold.
Prior Satisﬁability. B ∧E−̸|= 2
Posterior Satisﬁability. B ∧H ∧E−̸|= 2
Prior Necessity. B ̸|= E+
Posterior Suﬃciency. B ∧H |= E+
In the nonmonotonic setting, the ILP algorithm is more concerned with ﬁnding a set of
relations that are true over the given data set. These relations can be independent of each
other, unlike in the normal setting where the set of clauses produced work together to
form the overall hypothesis. The relations in the nonmonotonic setting also do not form
the deﬁnition of a predicate, and therefore the head of each clause may contain diﬀerent
information. Examples are given in the form of Herbrand Models [Herbrand, 1930], and
the relations constructed often refer to only a subset of the attributes listed within these
models.

CHAPTER 2. BACKGROUND
31
For example, suppose the following two examples had been collected from the user:
seq(task(type(travel),subject(york),location(n/a),
duration(4/00),preferred_time(8-00),preferred_day(wednesday)),
task(type(visit),subject(york_uni),location(n/a),
duration(4/00),preferred_time(13-00),preferred_day(wednesday))).
seq(task(type(travel),subject(bath),location(n/a),
duration(3/00),preferred_time(7-30),preferred_day(wednesday)),
task(type(visit),subject(praxis),location(n/a),
duration(5/00),preferred_time(11-00),preferred_day(wednesday))).
An ILP algorithm working in the normal setting would then be expected to produce a rule
such as:
seq(task(type(A),subject(B),location(C),
duration(D),preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),
duration(J),preferred_time(K),preferred_day(L))):-
A = travel, place(B), C = n/a, I = n/a,
G = visit, company(H), located_at(B,H),
lessthan(D,4/30),morethan(D,2/30),
lessthan(J,5/30),morethan(J,3/30).
One possible way to express these examples as Herbrand Models is:
begin(model1)
type1(travel)
subject1(york)
location1(n/a)
duration1(4/00)
preferred_time1(8-00)
preferred_day1(wednesday)
type2(visit)
subject2(york_uni)
location2(n/a)
duration2(4/00)
preferred_time2(13-00)
preferred_day2(wednesday)
end(model1)

CHAPTER 2. BACKGROUND
32
begin(model2)
type1(travel)
subject1(bath)
location1(n/a)
duration1(3/00)
preferred_time1(7-30)
preferred_day1(wednesday)
type2(visit)
subject2(praxis)
location2(n/a)
duration2(5/00)
preferred_time2(11-00)
preferred_day2(wednesday)
end(model2)
The following relations could then be produced from these models in the nonmonotonic
ILP setting:
preferred_day1(X) <- preferred_day2(X).
location1(X) <- location2(X).
The most obvious problem with these relations is that they make no reference to the values
of the variables. The ﬁrst relation says that the preferred day of each task in a pair is
the same, which is something that can already be assumed since the model is intended
for immediate sequences. The second relation says that each task in a pair has the same
location, however it makes no reference to what that location is. In order for the values such
as york or visit to be referenced, they must be expressed as attributes of the model. The
experiments in Chapter 3 make some more attempts at using the nonmonotonic problem
setting. They explain in more detail why a meaningful relation cannot be constructed
over these models, and hence why the nonmonotonic problem setting is unsuitable for use
when constructing the user model.

CHAPTER 2. BACKGROUND
33
2.3.2
Techniques used in the Normal ILP Setting
Whilst considering which hypothesis to produce, the number of examples covered by each
clause within a hypothesis is often one of the factors used to give an indication of recall.
This is most often carried out by generating an evaluation score for each clause that
includes the number of positive and negative examples covered. A deﬁnition of coverage
can be constructed by using one of the ideas that was contained within the deﬁnition of
inductive inference earlier.
Deﬁnition(Coverage): a clause c ∈H is said to cover e ∈E given back-
ground knowledge B if c ∧B |= e.
Therefore a clause c can be said to cover an area of example space if it covers all the
possible examples within that area.
Construction or modiﬁcation of clauses within a hypothesis involves attempting to modify
the area of example space that a clause covers by either generalising or specialising it.
Generalisation of a theory aims to increase the size of the area of example space that the
theory covers. This can be done in several ways such as removing some literals from a
clause, or turning some ground arguments into variables. Specialisation of a theory aims
to reduce the size of the area of example space that the theory covers, and hence remove
some examples (preferably negative ones) from its cover set. This can be done in several
ways such as adding a literal to a clause, instantiating a variable or removing a rule from
a theory.
Currently-popular ILP algorithms in the normal setting construct clauses to be included
in the hypothesis produced by starting from the most general clause and then performing
a top-down search through the tree of possible specialisations.
Due to the small data set that iDiary must work with, one possibility suggested was
to work in a bottom-up manner using ‘least general generalisation’ (lgg) [Plotkin, 1970,
Plotkin, 1971]. A clause C is a least general generalisation of a pair of clauses D1, D2 if C
is a generalisation of D1, D2, and for all C′, if C′ is a generalisation of D1, D2 then C′ is
a generalisation of C. C′ may not be a ‘proper’ generalisation of C, as it may be the case
that C′ and C are the same clause, or generalisations of each other. The ‘relative least
general generalisation’ (rlgg) of a pair of clauses is the lgg with respect to background
knowledge K. Finding the rlgg of a pair of clauses can be reduced to ﬁnding the lgg of
a pair of clauses if both the clauses are saturated [Rouveirol and Puget, 1989] using the
given background knowledge.
Finding the rlgg of a pair of clauses is useful as the learner can generalise over them just
enough to cover what it wants to cover without increasing the cover set of the clauses so
much that it covers too many ‘undesirables’. Using rlgg to produce hypotheses for the

CHAPTER 2. BACKGROUND
34
user model would produce results such as the following example. Suppose the following
two examples are collected from the user:
seq(task(type(travel),subject(york),location(n/a),
duration(4/00),preferred_time(8-00),preferred_day(wednesday)),
task(type(visit),subject(york_uni),location(n/a),
duration(4/00),preferred_time(13-00),preferred_day(wednesday))).
seq(task(type(travel),subject(bath),location(n/a),
duration(3/00),preferred_time(7-30),preferred_day(wednesday)),
task(type(visit),subject(praxis),location(n/a),
duration(5/00),preferred_time(11-00),preferred_day(wednesday))).
The relative least general generalisation of these two examples is:
seq(task(type(travel),subject(A),location(n/a),
duration(B),preferred_time(C),preferred_day(wednesday)),
task(type(visit),subject(D),location(n/a),
duration(E),preferred_time(F),preferred_day(wednesday))):-
place(A), located_at(D,A), company(D),
lessthan(B,4/30),morethan(B,2/30),
lessthan(E,5/30),morethan(E,3/30).
The rule above required 6 generalisations in the head of the clause in order to accomodated
all the diﬀerences between the two examples. The rule then required 7 additions to the
body of the clause (with some additional specialisations for preferred time that have been
ignored for now) in order to make it a least general generalisation. If another example is
then merged with this rule and, for example, the duration values of the new example do
not ﬁt into the current range, then the body of the rule will need to be revised. Given
that a large set of possible additions to the body of the clause are available, it can be seen
that this approach is expensive.
A few ideas for improving eﬃciency have been put forward, such as placing certain ‘weak’
restrictions on the hypothesis language [Muggleton and Feng, 1990], but this detracts from
the ﬂexibility required of the hypothesis language for this modelling problem.

CHAPTER 2. BACKGROUND
35
+
+
+
+ +
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+ +
+
+ +
+
Figure 2.1: An example space containing clusters of positive examples
2.4
Previous Approaches to Learning Diﬃculties in the Nor-
mal ILP Setting
The problems outlined within Chapter 1 have been addressed within the previous work in
the normal ILP setting, although there has been no previous attempt to solve all of the
problems at the same time.
2.4.1
Positive-only Learning
The deﬁnition of ILP given earlier referred to the use of both positive and negative evidence
when constructing hypotheses.
However the data collected from the user’s diary will
contain very few (if any) negative examples that could be used to give a clearer picture
of which areas of the example space should be covered by the user model. This is a well
known problem when using ILP, hence attempts at learning from only positive examples
have been produced.
When learning from both positive and negative examples, the negative examples could be
used to indicate areas of the example space that should not be covered by any hypoth-
esis produced. However, when no negative examples are present, the learner has been
deprived of a very useful source of information and, if not given some form of direction,
could produce an overly general hypothesis such as ‘everything is true’ because there is
no evidence to contradict this. While production of a hypothesis such as this must be
avoided, production of an overly speciﬁc hypothesis that overﬁts the given data must also
be avoided. One of the most popular approaches to this problem from an ILP point of
view is to employ some form of clustering (as used by Kirsten and Wrobel [2000] ).
Given a space of possible examples, and several examples which are contained within that
space, then the learner can attempt to generalise over various subsets of the observed data
in an attempt to create some rules. Each subset is made of examples that are deemed
‘near’ each other in some dimension and is known as a cluster, shown in Figure 2.1.

CHAPTER 2. BACKGROUND
36
Distance measures between examples form the basis of cluster construction. Sebag [1997]
gives one way of measuring distance within the system DISTILL:
dist(E, F) =
p
Σi(πi(E) −πi(F))2
where πi(E) is the number of formulae Si,j covering E.
hi = Si,1 ∨· · · ∨Si,ni
each hi is a member of a previously constructed set of hypotheses
Other approaches include Relational Distance-Based Clustering (RDBC) [Kirsten and
Wrobel, 1998] and the work by Ramon and Bruynooghe [1998] which presents a framework
for deﬁning distance between ﬁrst-order logic objects. Ramon and De Raedt combine k-
nearest neighbour and clustering by referring to the k nearest clusters rather than the k
nearest examples, as demonstrated in IBFL [Ramon and De Raedt, 1999].
Other approaches to positive only learning include using Bayes theorem [Muggleton, 1996]
(this method of positive only learning is implemented in Progol and described later in
Chapter 3), and λ-subsumption [Markov, 1996, Markov, 1998]. Markov presents another
interesting approach named LTC, Learning Typical Clauses [Markov, 1994]. The approach
analyses the structure of the given examples and ﬁnds patterns amongst them. These
patterns are represented as ground clauses which are then variablised and used in the
theory under construction.
No negative examples are required, and it apparently also
works well with small data sets. However it does not work with intensional background
theories and so was not considered for use by iDiary due to the loss of this valuable
reasoning resource.
MERLIN 2.0 [Bostr¨om, 1998] uses Hidden Markov Models (a tutorial on which is given
by Robiner [1989] ) to facilitate learning from positive examples in situations where pred-
icate invention is required, such as learning predicates that classify or generate sequences.
However, the sequences that are initially modelled by iDiary are of a ﬁxed length and re-
quire detailed predictions for each item within the sequence rather than being of a variable
length and made up of simple elements, hence this approach in not very appropriate.
The assumption of output completeness has been used in such systems as IFOIL and
CHILLIN [Zelle et al., 1995] [Zelle et al., 1994], FOIDL [Mooney and Caliﬀ, 1995], and
CLOG [Manandhar et al., 1998]. This removes the need for explicit negative examples
by saying that the positive examples present provide all the correct output patterns for
every unique input pattern, therefore any other output pattern produced for a given input
must be regarded as a negative example. This is not particularly useful for use by iDiary
though as the small number of examples within the data set would almost certainly lead
to overﬁtting of the theory produced to the existing data.
All of the ILP engines chosen for further invesigation into their suitability for use by
iDiary were capable of positive-only learning in some manner. The method used by each

CHAPTER 2. BACKGROUND
37
ILP engine to accomplish this learning is described in Chapter 3. The use of partitioning
methods (of which clustering methods can be thought of as a subset for this application)
as a method of pre-processing the data before giving it to the ILP engine to learn from
can help to signiﬁcantly reduce the complexity of the learning problem and thus help the
ILP engine to produce the required theory. The inclusion of this pre-processing stage is
investigated further in Chapter 3.
2.4.2
A Small Data Set
A major problem when learning a concept from a small number of examples is simply that
without enough data, it is not possible to produce a set of rules that could consistently
reﬂect the concept that is required to be learnt with a high level of recall.
One attempt to work with this problem is the use of abduction to generate additional
ground facts that attempt to explain the given data. Whilst this technique cannot generate
additional data from which to learn, it can be used to see what each possible hypothesis
would cover if certain assumptions were made during the learning process. This idea is
used within Abductive Concept Learning (I-ACL)[Kakas and Riguzzi, 1997].
Another approach to learning from small numbers of examples is the use of integrity
constraints in place of large numbers of ground negative examples, as demonstrated in
MONIC [Jorge and Brazdil, 1996]. Checking the consistency of a program with a set of
integrity constraints usually involves heavy theorem proving. Here, a number of logical
consequences of program P are randomly generated and tried against the integrity con-
straints. However, the method is incomplete, MONIC can only return that P is probably
consistent, but the level of incompleteness can be controlled by varying the number of
logical consequences sampled from P.
It could be possible to use integrity constraints in place of large numbers of negative
examples when learning from the data set collected from the user. The constraints would
need to be based on the data set that had been collected rather than being constructed
ahead of time. A variation on this approach is used in Chapter 4 when dealing with clusters
of positive examples. Howver, creating constraints based on the collected data may cause
problems in terms of overﬁtting of constructed theories. This issue is kept in mind during
the following chapters when the methods of the modelling system are created.
2.4.3
Learning with Noisy Data
The data set gathered from the user’s diary by iDiary will be small and is likely to contain
a high level of noise. It will not be possible to discern between true examples of tasks in
sequences and pairs of tasks that happen to have been scheduled one after the other without
reducing the ﬂexibility of the concepts that can be included in the model constructed.

CHAPTER 2. BACKGROUND
38
For example, the user may add one meeting to their diary immediately before another
even though the two appear unrelated. The choice of scheduling would depend on outside
factors that could not be discerned, such as the availability of other attendees. However,
it may be that these two meetings are scheduled in sequence for a reason, e.g. one person
who attends both meetings comes from outside the company and makes a special visit
whenever these meetings are due. Filtering out seemingly unrelated pairs would remove
this sequence from the constructed user model. However, the frequency with which this
sequence occurs may be of more use. This idea is discussed later in this section.
A technique used within the data-mining area is to use some form of ‘data-cleaning’ where
noisy examples and examples that do not conform to the main concept (i.e. outliers) are
detected and removed from the data set before the overall theory is learnt. There are
several ways in which this can be accomplished; Gamberger and Lavraˇc [2001] detail two
of them, saturation ﬁltering and classiﬁcation conscensus ﬁltering. However, both methods
would rely on some form of statistical information being gathered from the data set that
could be used to identify the examples that should be removed. The data set collected
by iDiary would be too small to produce statistics that could be relied upon for such an
operation, and removal of useful positive examples could be highly damaging to the model
produced.
Two standard methods used to help ILP engines cope with high levels of noise are pre-
pruning and post-pruning. The basic idea used within pre-pruning is to include some kind
of stopping criteria such as length restriction or signiﬁcance testing within the learning
algorithm so that the theory that is produced does not overﬁt the data in an eﬀort to
accomodate the noisy examples present. Post-pruning allows this overﬁtting to happen
but then prunes the theory after it has been constructed. Some approaches such as TDP
and IREP (both introduced by F¨urnkranz [1997] ) make use of a combination of these ideas,
however this type of approach does not appear to work well with data sets containing only
positive examples. Using a positive-only data set would mean that the learner is presented
with a data set for which it knows some examples are incorrect but has no indication at
all as to which examples those might be, i.e. there are no ‘grey’ areas where positive and
negative examples are grouped closely together.
Another way to deal with noise within the data from which a user model is learnt is by
accepting that the noise will aﬀect the models produced and then working with the eﬀect
that that will have on the predictions made. This would involve using tools such as uncer-
tainty or conﬁdence measures (mentioned earlier in Section 2.2.1), or presenting the user
with a choice between the most likely predictions, instead of deciding that a particular
value is the only possible answer. MailCat [Segal and Kephart, 1999] showed that this
second strategy was a very eﬀective means of enabling the user to remain in ultimate con-
trol of their actions. An example of an ILP application where uncertainty is included after
a model has been used to make a prediction is Naive Bayesian hypothesis interpretation,

CHAPTER 2. BACKGROUND
39
implemented within ILP-R [Pompe, 1996]. Each rule is given a conditional probability
distribution, and during classiﬁcation these distributions are combined according to the
naive Bayesian formula and used to give the most probable answer.
Using a measure of uncertainty to ﬁlter out some of the less likely predictions that may
be produced due to noisy data aﬀecting the model during construction would bypass
the diﬃculty of attempting to work with noisy data whilst learning. Various forms of
uncertainty that could be used to produce this uncertainty measure are considered in
Chapter 7, when the model query process is constructed.
2.4.4
Exploring Large Search Spaces
The hypothesis space is the set of all possible hypotheses that can be constructed using
the hypothesis language provided for the particular learning problem to be attempted.
The search space that an ILP engine must explore in order to ﬁnd the best hypothesis is
most often deﬁned as the hypothesis space with a semi-lattice ordering over it, such as
θ-subsumption [Plotkin, 1970], although any ordering over the structure of the hypothesis
space is deﬁned by the operators used by the ILP engine to move from one possible clause
to another (for example the generalisation and specialisation operators mentioned earlier).
Due to the length of the clauses within the model that must be produced and the number of
background predicates that could be used during clause construction, the size of the search
space will be larger than the ILP engine employed by iDiary will be able to consistently
conclusively explore within the available time. The model is required to provide enough
details of any tasks predicted that the user can use these predictions without additional
eﬀort, hence both tasks within the rules generated must contain specialisations for each
attribute value to be predicted. For example, suppose the following rule was the ideal
representation of one of the sequences to be included within the user model:
seq(task(type(A),subject(B),location(C),
duration(D),preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),
duration(J),preferred_time(K),preferred_day(L))):-
A = travel, place(B), located_at(H,B),
G = visit, company(H), C = n/a, I = n/a,
lessthan(D,2/0), greaterthan(D,0/0),
lessthan(J,2/0), greaterthan(J,0/0).
In total, eight of the attributes within a relation (four from each task) must be included
in some form of specialisation or relation, with several possible combinations of literals
that could be applied to each attribute. In the case of the example rule, this amounts to
11 specialisations. Chapter 3 will show that the set of literals that could have been used

CHAPTER 2. BACKGROUND
40
within the rule was 42 literals on average. Due to the limited amount of time available,
the learner cannot simply be allowed to perform an exhaustive exploration of such a large
search space, hence some means must be found to reduce the amount of space that the
learner needs to explore.
Aleph and Progol reduce the size of the hypothesis space by restricting the background
knowledge that can be used when constructing a particular clause within a theory to those
predicates that have been shown to cover at least a single example picked randomly from
within the training set. This restriction reduces the background knowledge to a subset of
the more relevant items.
Additional tools that can be used to reduce the size of the hypothesis space are forms
of syntactic and semantic bias. Syntactic bias refers to user-speciﬁcations such as typed
variables, or a restriction on the number of times a literal can be added to a candi-
date clause, or a complete syntax from which candidate clauses should be constructed
[Dehaspe and De Raedt, 1996, Kietz and Wrobel, 1991]. For example, typed variables can
be used to restrict predicates in the given background knowledge so that they can only be
applied to a subset of the possible variables in the head of the clause being constructed,
e.g. company/1 can only be applied to the subject of each task and lessthan/2 can only
be applied to the duration. Semantic bias refers to information about the background
predicates such as which variables within each predicate can be used as input and/or out-
put variables. Camacho [1994] provides a detailed description of the various methods that
can be used to help reduce the number of literals from which candidate clauses can be
constructed, and hence the size of the hypothesis space. The introduction of these forms
of bias enable the learner’s user to communicate to the learner more information about the
learning problem with which it has been presented and will help to shape, and hopefully
reduce the size of, the hypothesis space.
Ideas from the area of data-mining, speciﬁcally feature subset selection and feature con-
struction, can also be employed to reduce the complexity of learning problems. The idea
behind feature subset selection is that not all of the attributes of each item within the
data set may be relevant to the concept currently being learnt, and by reducing the set of
attributes considered to those that are relevant for the current concept, the dimensionality,
and hence the complexity, of the problem can be reduced. The features and attributes
within the context of the combination of this idea and ILP refer to the possible predicate
combinations that could be applied to an item within the data set rather than an actual
value contained within a speciﬁc example. A group of examples has a particular attribute
because a particular combination of predicates is true when it is applied to them.

CHAPTER 2. BACKGROUND
41
For example, suppose that a particular group of examples was covered by the following
clause:
seq(task(type(A),subject(B),location(C),
duration(D),preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),
duration(J),preferred_time(K),preferred_day(L))):-
place(B), located_at(H,B), company(H).
The combination of the 3 literals in the body of the clause would be regarded as an
attribute of the group of examples.
Alphonse and Matwin [2001] note that there are two categories of feature selection tech-
niques: those that use a learning algorithm to evaluate the quality of various sets of
attributes, and those that evaluate the attributes on the basis of some heuristic crite-
ria that focuses on the characteristics of the data, such as the frequency with which
particular attributes cover a given data set.
ILP has been used as an implementa-
tion method for this second technique; Kramer and De Raedt [2001] used WARMR
[Dehaspe and Toivonen, 1999] to ﬁlter possible attributes by excluding those that did not
cover a large enough subset of the data set. ILP has also been used in this area by Srini-
vasan and King [1996], who used Aleph to generate additional attributes for groups of
examples with the same classiﬁcation from combinations of existing background informa-
tion.
Substitution-Based Propositionalization (SBP) [Sebag and Rouveirol, 1994] shows some
resemblance to the idea described earlier that is used within Aleph and Progol for the
restriction of the hypothesis space. It aims to reduce the dimensionality of the ILP task
by excluding irrelevant literals from the learning problem.
A description is given by Alphonse and Matwin [2001]:
..given a pattern, termed P, built from a positive example, then, for each
FOL [First-Order Logic] example, a check is performed whether literals of
the pattern are present in the example, up to a substitution (hence the name
of the method: Substitution-Based Propositionalization). Each literal of P
becomes a feature of the propositional problem and each substitution becomes
a propositional example. The change in representation shifts the initial FOL
instance space to an attribute-value vector. A feature selection ﬁlter applied
on the propositional problem performs the removal of empirically irrelevant
features, resulting in the pruning of the corresponding literals in P.
The identiﬁcation of irrelevant literals is also discussed by Lavraˇc et al.
[1999].
This
approach uses a set of pairs of examples that are constructed by pairing every positive

CHAPTER 2. BACKGROUND
42
example with every negative example within the training set. The ability of each literal
to correctly split each pair of examples into a positive and negative classiﬁcation is used
as an indication of how useful the literal would be in the construction of a theory. For
example, given the following pair of examples:
positive:
seq(task(type(travel),subject(york),location(n/a),
duration(4/00),preferred_time(8-00),preferred_day(wednesday)),
task(type(visit),subject(york_uni),location(n/a),
duration(4/00),preferred_time(13-00),preferred_day(wednesday))).
negative:
seq(task(type(travel),subject(bath),location(n/a),
duration(4/00),preferred_time(8-00),preferred_day(wednesday)),
task(type(visit),subject(york_uni),location(n/a),
duration(4/00),preferred_time(13-00),preferred_day(wednesday))).
The score for the literal located at(B,H) would increase as it would be evaluated to true
for the positive example but false for the negative example. The score for the literal
person(B) would not increase because it be evaluated to false for both examples. With
suitable negative examples present that would demonstrate what values were unsuitable
for each term within the positive examples, then the relevant literals would score higher
than the irrelevant literals, and the irrelevant literals would be discarded.
The overall message that these approaches give is that if the search space for a learning
problem becomes too large for a learner to explore within a reasonable length of time,
then the learner must be provided with some means to reduce the amount of space that
it needs to search in order to ﬁnd a suitable hypothesis. Either some method outside the
learner must be employed to reduce the size of the search space, or the learner must be
given extra information such as some form of bias towards exploring or ignoring certain
parts of the search space.
An alternate approach to working with large search space is proposed by Srinivasan [1999]
in which the ordering over the search space is ignored in favour of a probabilistic approach.
This approach works with the idea that if enough hypotheses are drawn then there is a
high probability that one of these hypotheses will perform well enough over the data set
to be returned as a solution. However, the requirements of the user model are quite strict
with regards to detail and structure, and will not tolerate the loss of control that using
such a method would entail.

CHAPTER 2. BACKGROUND
43
2.5
Summary and Conclusions
The two main areas that have been examined within this Chapter have provided a collec-
tion of issues that must be considered when making design decisions and ideas that could
help overcome the problems faced by iDiary.
Any model of the user must be understandable (whether by direct viewing or after some
form of translation) and have a high level of precision and recall. Any answers produced
by the model should be correct and produced quickly enough to still be of use to the user.
The modelling method should be general enough to be able to autonomously produce
a model of any user without intervention or alteration by an external factor (such as a
system engineer). In addition, requests for information from the user must be kept to a
minimum to avoid increasing the user’s workload.
ILP has been used on a few previous occasions for the construction of a user-model, however
the prediction requirements of these models have been far simpler than those required by
iDiary. In addition, the data from which the models were constructed was easier to use
in terms of both the amount of information available and the level of noise present within
the examples collected.
None of the approaches to noisy data sets were easily applicable to the current learning
problem, but other ideas such as working around the errors caused by noise rather than
trying to avoid them were more appropriate and will be expanded upon later in Chapter 7.
In addition, none of the approaches to learning from small data sets could be used, which
may cause problems in terms of overﬁtting of any theories constructed to the available
data due to the high recall requirement. This issue is kept in mind during the following
chapters when making decisions in order to attempt to reduce the amount of overﬁtting
that could potentially occur. The general message received from the ideas used to work
with large search spaces of possible hypotheses showed that the learner would require
help if it was to consistently produce a hypothesis with a high level of recall. This aid
could come from an external source, such as the pre-processing mentioned during the
investigation of positive-only learning methods, or some other method that aﬀected the
structure of learning, or interally in the form of extra information given to the learner.
Both sources of aid require additional general information about the sorts of concept that
could be learnt to be included within the overall learning system.
The ﬁrst of the ideas presented within this chapter that will be explored further is the
use of clustering as a partitioning pre-processing method when dealing with positive-only
data sets. This will be investigated in Chapter 3, where the the ﬁrst part of the approach
to producing a model of the user will be developed.

Chapter 3
Creating a User Model Using ILP
The ﬁrst steps that must be taken towards constructing a system that can produce the
required user model are to clarify the nature of the data collected from iDiary and then to
identify a suitable ILP engine to use on the data. Once the capabilities of the learner when
presented with the data are known, then the sort of assistance that the learner requires in
order to generate the model can be determined.
In this chapter, an investigation of the abilities of a selection of current ILP methods to
construct the required model is used to show that none of the methods are capable of pro-
ducing a satisfactory theory without any additional intervention. Aleph [Srinivasan, 2001]
is shown to be the method that is most likely to be able to learn the required theory
provided that two forms of assistance are given. The ﬁrst item of assistance required is
a data pre-processing step in which the data set is partitioned into a series of separate
learning problems.
The second item of assistance is the introduction of some form of
stimuli that will direct the learner towards constructing theories with a greater level of
specialisation than those currently produced. Finally, a method to implement the ﬁrst of
these improvements is discussed.
3.1
The Data Collected from iDiary
The examples of tasks in immediate sequence should be collected from the user’s diary
without requiring them to expend any additional eﬀort. That is, this process should be
done automatically by iDiary; the user should not be asked to indicate what the examples
to be collected are.
The current method used for this task is quite simple, focusing
on collecting all the data from the diary for an initial construction of the model.
A
more sophisticated approach can be introduced when maintenance of an existing model is
investigated.
44

CHAPTER 3. CREATING A USER MODEL USING ILP
45
Each day in the diary is examined to see if it contains examples of tasks in sequence. Each
pair of tasks that are immediately in sequence, or have up to an hour of unallocated time
between them, are recorded as an example. This ﬂexibility was introduced to cater for
those examples where an extra amount of time has been included ’just in case’. For ex-
ample, when travelling to a client’s site, there may be an extra half-hour added in before
the actual meeting for general pleasantries or a brief break. This method removes the
allocation of extra time that the user has made, and should thus be focused on as a future
learning task in order to better preserve the user’s demonstrated habits. Since the alloca-
tion of time occurs between two tasks, then splitting the information into two sequences —
one that consisted of a single task followed by a unallocated time and one that consisted
of unallocated time followed by a single task — would lose the information on the relation
between the two tasks that make up the sequence. A more appropriate representation
of this information would be to extend the learning problem to learn longer sequences of
tasks where the extra time is regarded as an extra task. Learning and predicting longer
sequence of tasks has been postponed to future work.
Currently each task has the day of the week and the time included in its information, but
not the actual date. As noted elsewhere, this means that additional contextual information
relating to the date is unavailable for this initial investigation, but the scope remains for
later expansion. This also means that no diﬀerentiation is made between the data for
sequences on a speciﬁc day of the week, for instance, Wednesday, and data for sequences
on the same day of the following week. These diﬀerent days are only diﬀerentiated between
when each day is examined individually in order to identify sequences of tasks. For this
initial version of the user model, the lack of diﬀerentiation between sequences captured on
diﬀerent days does not present a problem. Since the model will initially only be used to
predict tasks that immediately precede or follow a task that the user has just entered into
their diary, then the day on which the user intends to schedule the sequence has already
been indicated. There may be some sequences that depend on speciﬁc days (e.g. requiring
the ﬁrst Wednesday of the month), but if the user is presented with a choice of possible
suggestions rather than automatically scheduling extra tasks then they will be easily able
to ignore these extra suggestions for the time being.
The most obvious problem with this method of collecting examples is that it can select
pairs of tasks that just happen to have been scheduled one after the other and declare
them as an example of a sequence. i.e. it will produce noisy data. If the tasks recorded
together as an example occur in other examples, then this information can be used to help
reduce the eﬀect of the noise on the predictions made using the model. The mechanism for
achieving this will be discussed when the method for making predictions using the model
is described in Chapter 7.
However, if any of the tasks within a noisy example are normally scheduled in a solitary
manner, then the data set currently has no information to indicate that this is the case.

CHAPTER 3. CREATING A USER MODEL USING ILP
46
Therefore some additional examples will also be collected that contain this information.
If a task is encountered that has no other task before it then a new example is created
with an ‘empty task’ as the ﬁrst task in the sequence. Similarly, if a task is encountered
that has no other task following it then an example is created with an ’empty task’ as the
second task in the sequence. The empty task has the following description:
task(type(n/a), subject(n/a), location(n/a),
duration(1/0), preferred_time(n/a), preferred_day(n/a))
The examples containing an empty task will not be included in the data set that is used
to build the user model since they are not examples of actual sequences. They will only
be used when evaluating the predictions made using the user model (as mentioned earlier,
this mechanism will be discussed later in Chapter 7). Therefore these examples will not be
included in the data sets used in the experiments presented in this dissertation. In later
chapters the data sets will contain a mixture of correct and noisy data. However, this
noise will be in the form of ‘corrupted’ correct examples, i.e. where one task in a correct
example has been replaced by a task from another correct example to form an example
that shouldn’t occur. For the investigation carried out in the current chapter, the data
sets will only contain correct examples of tasks in immediate sequence.
3.2
ILP Method Comparison
Several ILP algorithms were investigated to ascertain their suitability to be used to produce
the required user model. The ideal results produced by each algorithm would be a set of
rules that captured the concept contained within each training set in enough detail that
it could be used as a user model without any further modiﬁcations and in a manner that
could be understood by the user. The initial set of algorithms considered were:
FOIL [Cameron-Jones and Quinlan, 1994] is a greedy ILP algorithm that attempts to
cover as many as yet uncovered positive examples as possible with each new rule it
creates while leaving as many negative examples as possible uncovered. Thus there
is a trade-oﬀof accuracy for generality in order to avoid producing a large theory
which overﬁts the training data. One of the main disadvantages of FOIL, and hence
why it was not considered further for construction of the user model by iDiary was
that it could not use any background knowledge other than ground facts.
Progol [Muggleton, 1995b] takes a somewhat diﬀerent approach; it uses Mode Directed
Inverse Entailment (MDIE)[Muggleton, 1995a].
Progol takes a single uncovered
positive example and constructs the most speciﬁc clause which covers it, known
as the bottom clause, ⊥.
It then searches through all the clauses which sub-
sume ⊥using an A*-like search algorithm (a description of which can be found in

CHAPTER 3. CREATING A USER MODEL USING ILP
47
[Russell and Norvig, 1995]) to ﬁnd the one which has the highest information con-
tent (a combination of the number of positive and negative examples covered and the
size of the clause). Mode declarations are used to constrain the search through all
the clauses and will also aﬀect which ⊥is created from the possibilities. According
to Muggleton [1998] it had been shown previously that inverse entailment was not
complete, but if the bottom set used is enlarged, then it is possible to make a revised
version of inverse entailment that is complete with respect to entailment for Horn
theories, which is what tends to created when using ILP. The method implemented
in Progol for evaluating each clause calculates the ‘compression’ value of the clause.
This function takes into account the number of literals in the hypothesis and the
number of additional literals needed to complete the input/output variable chains
in the clause, as well as the numbers of positive and negative examples covered.
When the best clause is found then it is added to the background knowledge and
the process starts again.
Aleph [Srinivasan, 2001] is related to Progol but enables the user to specify the details for
various parts of the algorithm carried out, such as which search method and which
evaluation method to use. Aleph was used in the experiments instead of Progol when
considering which ILP engine to use due to this ability and because Aleph was found
to produce results in a shorter time.
TILDE [Blockeel and De Raedt, 1998] makes use of the ideas used in the construction
of decision trees to take a diﬀerent approach to ILP. A logical decision tree is con-
structed using given background knowledge predicates. Each decision node that is
added to the tree contains the predicate from the given background knowledge that
is best able to partition the examples covered by the leaf node that it is replacing
into groups with separate classiﬁcations. The partitioning of the cover set of each
node stops when the number of examples in that node falls below a user-set mini-
mum. Once the tree is constructed and post-pruned, a set of rules is produced (see
[F¨urnkranz, 1994] for a review of pruning methods). One of the enhancements added
to TILDE was the ability to look ahead so that strategic decisions could be made
concerning the construction of the tree.
TILDE was not included in the initial set of experiments to determine a suitable
algorithm for use as the main ILP engine within iDiary because of its inability to
learn concepts from data sets that contained only positive examples. However, it
was used in later experiments in Chapters 4 and 5 as an example of a greedy ILP
algorithm once this problem had been negated by further developments that were
introduced to aid the ILP engine chosen.
WARMR [Dehaspe and Toivonen, 1999] takes a database of information, a language bias
from which to construct queries, a key literal over which to perform queries and a
user-set minimum frequency, and searches for queries containing the key literal that

CHAPTER 3. CREATING A USER MODEL USING ILP
48
are successful when applied to the given database. All queries which pass the given
criteria are returned. This method is intended to report all regularities amongst the
given data rather than constructing a particular theory.
Claudien [De Raedt and Dehaspe, 1997] searches for regularities contained within un-
classiﬁed data. Its aim is to produce a maximally general hypothesis which is mod-
elled by all the examples it is given (i.e. it constructs theories which must be true
over the entire set of given data), using the language structure speciﬁed. The lan-
guage bias, stated using DLAB [Dehaspe and De Raedt, 1996], allows the user to
guide Claudien in the construction of theories by stating the general structure that
each theory must take.
The engines chosen for further investigation are described in more detail in the sections
detailing the results of the experiments.
In order to test the idea encountered in the
previous chapter of partitioning the data set and then learning individual theories for each
partition, two sets of training data from the case study were produced. These sets were
used to perform a comparison of the results of presenting each set to each learner as a
separate learning problem and the results of presenting the union of the two sets as a single
problem. The diﬀerence in results was used as an indication of how well each learner coped
with more than one distinct subconcept within the same data set. The data used for these
experiments was artiﬁcially produced to provide a simpler learning task to work with at
ﬁrst (i.e. one with an absence of noise and the set of examples describing most aspects of
the concept). The removal of these additional diﬃculties allowed the testing to focus on
whether or not each ILP algorithm could produce a user model under ideal circumstances.
If any of the learners were unable to do so then the reasons for failure should be clearer
without the additional variables that would be introduced by learning with real data.
The sets of training data produced were:
Set 1 A set containing examples that described a simple concept of the user requiring
some preparation time before either a meeting or a presentation on a project. For
example,
seq(task(type(prep),subject(idiary),location(n/a),
duration(1/00),preferred_time(13-00),preferred_day(wednesday)),
task(type(presentation),subject(idiary),location(103),
duration(1/00),preferred_time(14-30),preferred_day(wednesday))).
Set 2 A set containing examples of a slightly more complicated concept of the user needing
travel time before visiting another company. If the company was not ‘near’ their own
site then the user would need more time to travel there (e.g. 3 or 4 hours), otherwise
the travel would only take an hour.

CHAPTER 3. CREATING A USER MODEL USING ILP
49
For example,
seq(task(type(travel),subject(york),location(n/a),
duration(4/00),preferred_time(8-00),preferred_day(wednesday)),
task(type(visit),subject(york_uni),location(n/a),
duration(4/00),preferred_time(13-00),preferred_day(wednesday))).
seq(task(type(travel),subject(london),location(n/a),
duration(1/30),preferred_time(10-00),preferred_day(wednesday)),
task(type(visit),subject(ericsson),location(n/a),
duration(2/00),preferred_time(13-00),preferred_day(wednesday))).
Several tests were carried out on each ILP algorithm using these sets:
1. Each set was given individually to the algorithm to learn from in a ‘positive only’
manner,
2. The number of examples in each set was steadily reduced over a number of tests to
see how well the algorithm coped with sparse data, and
3. The union of the two sets was given to the algorithm to see whether it was able to
distinguish between the two separate concepts.
3.2.1
Aleph
Algorithm overview
Aleph is a powerful ILP engine that uses saturation [Rouveirol and Puget, 1989] to help
constrain its search through the space of possible hypotheses. Aleph accepts input in the
form of Prolog facts and predicate deﬁnitions, information (in the form of modes) about
how the user wishes these items to be used, and information (in the form of various set-
tings) about how the user wishes the algorithm to go about the learning task. The set of
examples can contain either just positive examples or include negative examples as well.
Background knowledge in the form of predicate deﬁnitions can be given as ground facts
or in intensional form.
The algorithm proceeds by randomly selecting a single positive example and constructing
from the given bacckground knowledge the most speciﬁc clause that will the entail the
example selected. Candidate clauses containing various subsets of the literals contained
within the most speciﬁc clause are explored using one of several possible search methods
that can be selected by the user (for these experiments the heuristic search was used) until
the best clause, according to a speciﬁed evaluation criteria, from amongst those available
is found.

CHAPTER 3. CREATING A USER MODEL USING ILP
50
Evaluation of clauses can be carried out in one of several ways. As there were no require-
ments for a particular type of evaluation to be used then the experiments were carried out
using the default evaluation function (coverage, Equation 3.1). This function takes into
account the number of positive examples (P) and negative examples (N) covered, and is a
good general method that should produce appropriate results in most cases.
Evalfn(Clause) = P −N
(3.1)
If Aleph is not given any negative examples and is told to learn in a positive-only man-
ner then it estimates the probability distribution of the example space by constructing
a Stochastic Logic Program (SLP) [Muggleton, 1995c] from the information given, and
uses this to construct an additional set of random examples. Aleph attempts to avoid
covering members of this additional set of examples in order to restrict the generality of
the clause being constructed and hence increase the probability of the area of the instance
space that the clause covers being part of the target concept. These ideas are described
in [Muggleton, 1996].
Working with Aleph
The examples presented to Aleph were of the form seq(TaskA,TaskB);for example:
seq(task(type(prep),subject(leap),location(104),duration(1/00),
preferred_time(13-00),preferred_day(friday),
task(type(meeting),subject(leap),location(104),duration(2/00),
preferred_time(14-00),preferred_day(friday)).
where TaskA and TaskB were cut-down versions of tasks to be found in iDiary
[Wobcke and Sichanie, 1999], containing only the type, subject, location, duration, pre-
ferred time, and preferred day. The preferred time and day were kept within the tasks
to allow possible further expansion into non-immediate sequences; in addition they are
of importance for the second learning problem attempted in Chapter 5. The background
predicates available included those to identify certain groups of types, such as place(X)
or project(X), relations between elements (e.g. located at(X,Y)), and range-restrictions
for the task durations.

CHAPTER 3. CREATING A USER MODEL USING ILP
51
Results and Conclusions
Set 1 - preparation for meetings
The ﬁrst training set required the following clause to be learnt:
seq(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(B),location(H),duration(I),
preferred_time(J),preferred_day(F))):-
A=prep, with_people(G), project(B),
shorterthan(D,1/30), shorterthan(J,2/0).
The training set, containing 8 examples, was given to Aleph to learn from in a positive-only
manner, and produced the result:
seq(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(B),location(H),duration(I),
preferred_time(J),preferred_day(F))).
Note that the second and sixth attributes of both tasks in the clause returned have been
uniﬁed. Subsets of the training set that steadily reduced in size were given to Aleph,
and the results returned were the same as those returned for the full set of examples.
Occasionally, a slightly more specialised result was returned:
seq(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(B),location(H),duration(I),
preferred_time(J),preferred_day(F))):- A = prep.
This could occur if one of the additional negative examples generated internally by Aleph
using SLPs was identical to one of the positive examples within the given data set apart
from a diﬀerent value for the ﬁrst argument. This forced Aleph to produce a more spe-
cialised theory to ensure that all the generated negative examples were excluded from the
theory constructed. As the negative examples generated had a random element involved,
this result was not returned consistently, but it does suggest that retaining control over
the generation of negative examples would enable the generation of what are commonly
referred to as ‘near misses’, which would make the learner produce each of the required
specialisations.

CHAPTER 3. CREATING A USER MODEL USING ILP
52
Set 2 - visiting other companies
The second training set required two clauses to be learnt:
1.
seq(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),duration(J),
preferred_time(K),preferred_day(F))):-
A=travel, with_people(G),
other_company(H), located_at(B,H),
near_site(B),
shorterthan(D,1/30),
shorterthan(J,4/0).
2.
seq(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),duration(J),
preferred_time(K),preferred_day(F))):-
A=travel, with_people(G),
other_company(H), located_at(B,H),
not_near_site(B),
longerthan(D,3/0), shorterthan(D,6/0),
longerthan(J,2/0), shorterthan(J,7/0).
The training set, containing 16 examples (10 for the ﬁrst clause and 6 for the second
clause), was given to Aleph to learn from in a positive-only manner, and produced the
result:
seq(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),duration(J),
preferred_time(K),preferred_day(F))):- A = travel.
Clearly this is quite a strong specialisation that would ﬁlter out a very large proportion of
any negative examples.

CHAPTER 3. CREATING A USER MODEL USING ILP
53
Further tests were carried out using various subsets of set 2, progressing from larger to
smaller subsets. Initially each subset produced the same result as the original set, however
as the size of each set was reduced, a slightly diﬀerent result, containing two clauses, would
occasionally occur:
seq(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(C),duration(I),
preferred_time(J),preferred_day(F))):- A = travel.
seq(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),duration(J),
preferred_time(K),preferred_day(F))):- A = travel.
The ﬁrst clause produced had an additional uniﬁcation of the third argument of each task.
This would be produced if the ﬁrst positive example chosen from which to construct the
bottom clause had identical values for those arguments. Only a small number of the posi-
tive examples had identical values for these arguments and so a second clause was required
to cover the remaining examples and complete the theory. When the data set was reduced,
examples of each type were retained within the remaining set of examples; however this
meant that as the data set reached a very small size, the probability that an example
with identical values for the third argument of each task would be chosen ﬁrst for the
construction of the bottom clause increased, and the frequency of a theory requiring two
clauses being constructed increased. A data set of this type of composition might present
slight problems for the required user-model in terms of understandability, but should not
aﬀect it with regards to recall.
Combining Sets 1 and 2
The union of set 1 and set 2 was given to Aleph to work on in a positive-only manner.
As Aleph had been unable to produce the 2 clauses required for set 2 in the experiments
earlier, the results produced were not expected to be particularly specialised. However, as
the diﬀerence between the concepts represented by the two sets of examples was greater
than the diﬀerence between the concepts represented by the two subsets in set 2 then it
should have been more likely that a separate clause would be produced within the theory
for each concept within the data set. Covering the entire example set with just one clause
would cover a much larger area of the example space and hence more of the additional
negative examples generated by Aleph.

CHAPTER 3. CREATING A USER MODEL USING ILP
54
Aleph produced the following result:
seq(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(B),location(H),duration(I),
preferred_time(J),preferred_day(F))).
seq(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),duration(J),
preferred_time(K),preferred_day(F))):- A=travel.
The ﬁrst clause covered all the examples within the example set where the two tasks in the
sequence had identical values for the second argument (this meant the examples contained
within set 1). The second clause covered the examples contained within set 2.
However, Aleph occasionally produced the following results instead:
seq(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),duration(J),
preferred_time(K),preferred_day(F))):-
C=n/a, shorterthan(D,5/30).
or
seq(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(C),duration(J),
preferred_time(K),preferred_day(F))).
seq(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),duration(J),
preferred_time(K),preferred_day(F))):-
C=n/a, room(I).
Neither of the theories shown above distinguish between the two concepts presented within
the example set or provide much usable information. The result returned by Aleph each
time appeared to depend on the initial example chosen from the data set from which it
constructed a bottom clause.
Some choices enabled Aleph to produce a required the-
ory, others did not, but when the ILP engine is to be used in an environment such as

CHAPTER 3. CREATING A USER MODEL USING ILP
55
the one envisaged for the the case study, when the user model must be constructed au-
tonomously, the modelling system will not know beforehand which examples are the best
ones to choose. This problem was not encountered when working with separate data-sets,
and so this would suggest that the reduction in complexity of the learning problem enabled
Aleph to reliably produce a generalisation of the required answer.
Conclusions
Aleph satisﬁed some of the criteria for the production of a user model described ear-
lier. The language in which the results produced were described can be easily translated
into human-readable form, and Aleph was able to accept and use all relevant information
when constructing the model (i.e. both the values contained within the tasks and the at-
tributes of the tasks to which they were assigned, plus any background knowledge that
was provided). Provided the information is presented in a manner that ensures it can be
reasoned with, it will be made use of when constructing a theory.
Aleph was unable to produce theories that could produce the multiple-value predictions
required but there was potential for a satisfactory model to be produced if given the right
assistance. By presenting all the examples to Aleph as a single set of data and learning in
a positive only manner, the form of the theory Aleph produced could not be controlled.
If control of the theory is retained by partitioning the data into separate subconcepts and
working with the idea of generating ‘near misses’ instead of relying on Aleph’s own method
of negative example generation then it should be possible to produce a bias towards more
specialised clauses.
Aleph was also able to cope in a reasonable fashion with learning from small amounts of
data. As the size of the data sets decreased to a size where there was very little room for
reasoning then the results returned began to show some signs of being aﬀected, however
the theories produced could still be used as a base for construction of the appropriate
concept representation.
3.2.2
Claudien
Algorithm Overview
Claudien searches for regularities contained within unclassiﬁed data in the nonmonotonic
ILP problem setting. Its aim is to produce a maximally general hypothesis that is modelled
by all the examples it is given, using the language structure speciﬁed. The beneﬁt of being
able to specify the structure of the hypotheses that are produced is that the speciﬁcation
can be used to insist on the levels of clause specialisation that are required to be present
in all the answers produced. In addition, as the data input is unclassiﬁed, Claudien does
not require negative examples in order to produce candidate clauses. These two properties

CHAPTER 3. CREATING A USER MODEL USING ILP
56
made Claudien a good candidate for investigation into whether it was a possible ILP engine
to use.
The input to Claudien is made up of several parts: a knowledge base that contains Her-
brand Interpretations [Herbrand, 1930] as examples, a language bias that dictates the form
that the constructed clauses should take, and three optional extra ﬁles that contain ex-
isting clausal theories, settings, and background knowledge (ground facts and additional
relations) that may be used in the construction of clauses.
The basic algorithm of Claudien performs a general to speciﬁc search through the space
of clauses that could be constructed using the language speciﬁcation. It starts from the
empty clause, produces the set of all possible reﬁnements of it, prunes those clauses within
the set that fail some evaluation criteria (for example all clauses that are tautologies are
pruned), and adds the remaining members of the set into a set of candidate clauses. Each
clause in the set is examined in turn. If a clause is a valid relation over the given data
then it is stored as a result, if not then it is reﬁned and the reﬁned versions of the clause
are examined in the same manner. The search for valid clauses stops when either the
candidate clause set is empty or a set limit (such as a limit on the number of literals per
clause) is reached.
The language bias, stated using DLAB [Dehaspe and De Raedt, 1996], is used to deﬁne
the space of possible clauses, and hence the hypothesis space. Claudien uses one of four
possible search methods: breadth-ﬁrst, depth-ﬁrst, beam or best-ﬁrst.
Working with Claudien
The approach of Claudien to the available data is somewhat diﬀerent to that of Aleph.
Information about each Herbrand Model should be presented as a set of relations that are
true over that model so that Claudien can then reason with relations that are true over
the set of models it is given. A slight variation of a classic example illustrates this:
begin(model(bird1)).
eagle(bird1).
feathers(bird1).
flies(bird1).
end(model(bird1)).
begin(model(bird2)).
penguin(bird2).
feathers(bird2).
swims(bird2).
end(model(bird2)).

CHAPTER 3. CREATING A USER MODEL USING ILP
57
begin(model(fish1)).
carp(fish1).
scales(fish1).
swims(fish1).
end(model(fish1)).
From the data given, Claudien would induce clauses such as:
false <- eagle(X), swims(X)
carp(X) ; penguin(X) <- swims(X)
swims(X) ; flies(X) <- feathers(X)
These clauses represent reasoning about the relations that are true over X but do not
pay attention to the value of X except as a unifying factor. When producing a model
of the user’s habits from the examples collected, both the values of the attributes and
their placement within the data structure are of importance and should be reﬂected in the
model.
Several approaches to representing the information contained within the examples of both
training sets were attempted. As Claudien was more interested in regularities over smaller
parts of the data, the only method of modelling available was to generate rules that would
predict one or more attributes of a task rather than a whole task, and then several of
these rules would be called when the user model was consulted for a prediction. This
should have provided the level of specialisation required for the user model task whilst
still generating rules that the user could understand.
However, no approach was successful in enabling Claudien to produce a representation
of a regularity within the data. Figure 3.1 shows two of the simpler attempts to present
the information to Claudien. Other approaches using combinations of tasks and exam-
ples were also attempted. Either the values or the placings of the task attributes (e.g.
whether something should be contained within task type or subject) could be modelled,
but not both at the same time without placing so much bias in the language deﬁnition
that it was obvious that the target concept was already known. If the information could
not be modelled as a relation over the example then it could not be picked up as a reg-
ularity and included in a clause, however it was the relations over each example that the
learning problem was required to identify so that they could be included within the model.
Conclusions
Closer inspection of the theory behind Claudien [Dehaspe and DeRaedt, 1996, DeRaedt
and Dehaspe, 1997] reveals two ideas that would account for its observed behaviour over

CHAPTER 3. CREATING A USER MODEL USING ILP
58
begin(model(example1)).
type1(travel).
subject1(london).
duration1(2/0).
type2(visit).
subject2(ericsson).
duration2(3/0).
...etc.
end(model(example1)).
begin(model(example2)).
travel(example2).
london(example2).
2_hours(example2).
visit(example2)
ericsson(example2)
3_hours(example2).
...etc.
end(model(example2)).
Figure 3.1: Two of the attempts to model examples for Claudien
the data presented. The two ideas that the system works with are:
1. All observations are completely speciﬁed, and
2. All hypotheses should reﬂect what is all the data, i.e. each hypothesis must be true
for every Herbrand Model presented.
The ﬁrst idea is carried out by working with ‘closed observations’ i.e. anything that is
not observed is deemed to be false. This goes against the nature of the model that must
be generated as the modelling system will only have sparse data with which to work and
needs to produce a more generalised model from it. Claudien did not allow clauses to be
presented as solutions if they were found to cover other non-speciﬁed (hence regarded as
negative) examples that could be constructed using the given background knowledge and
the clause being evaluated.
The second idea is carried out by requiring all hypotheses to cover all of the observed data.
This requirement would mostly be satisﬁed by the partitioning pre-processing method
being evaluated, but the examples within each partition will still have small diﬀerences that
may be required to be included within the theory (as demonstrated in the set of examples
contained within set 2), and this cannot be accomodated by Claudien’s requirements.

CHAPTER 3. CREATING A USER MODEL USING ILP
59
3.2.3
WARMR
Algorithm Overview
WARMR [Dehaspe and Toivonen, 1999], and TILDE [Blockeel and De Raedt, 1998], form
the ACE Data Mining System [Blockeel et al., 2001]. As with Claudien, WARMR also
looks for regularities in sets of data, working out the fraction of examples in the given
database over which each candidate clause is true (referred to by WARMR as the fre-
quency of the clause). WARMR takes a database of information, a language bias from
which to construct queries, a key literal over which to perform queries and a user-set
minimum frequency, and searches for queries containing the key literal that are successful
when applied to the given database. NB: WARMR refers to candidate clauses as queries
because they are evaluated over the given information, which it assumes is a database. All
queries found that are successful for more than the given minimum frequency are returned
as results. These queries can then be interpreted as clauses with the key literal as the head
of the clause (as it will always be in the query) and the rest of the literals as the body.
WARMR does not require negative examples in order to produce candidate queries and
can withstand high levels of noise, provided that the minimum frequency for clauses is not
set too high. These properties made WARMR a good candidate for further investigation
into whether it could be used as an ILP engine for the user-model.
WARMR performs a top-down breadth-ﬁrst search. Starting with the key literal, it evalu-
ates conjunctions of each of the given literals with the key literal to see which conjunction
is true over the data set with a frequency that is higher than the set minimum frequency.
Each conjunction that passes this evaluation is given as a result and then kept for the
next level of search. At each level, all the results from the previous level are considered in
conjunction with each of the given literals to ﬁnd more clauses that have a frequency over
the data that is higher than the minimum frequency. This search for clauses is continued,
layer by layer, until the set depth limit is reached or there are no more clauses to reﬁne.
The hypothesis space from which all possible hypotheses are returned is bounded by the
possible literals from which each query can be constructed, the minimum frequency and
the maximum search depth.
Examples are given to WARMR either as large ground facts or as Herbrand Models (this
is illustrated in Figure 3.2).
Background knowledge can be provided as either ground
facts or intensional predicates. WARMR makes use of type declarations to ensure that
the predicates used to construct queries are only used for the information for which they
were intended. The arguments of a predicate can only be uniﬁed with a variable in the
key literal of the same type. However, these type declarations are not mandatory and
so variables can be uniﬁed with any type if required. The modes of the arguments of
each predicate can also be declared. Each argument can be an input variable, an output
variable, either an input or an output variable, or a constant.

CHAPTER 3. CREATING A USER MODEL USING ILP
60
When working with constants, WARMR will examine each possible value that an argument
that is designated a constant can take before allocating a value to it.
Unfortunately
WARMR does not allow arguments in the key literal being generated to be ground, and
any literals whose arguments can be ground cannot be intensional predicates as WARMR
is unable to determine the possible values that the ground argument could take. Even if
the predicate is something as simple as val(A,A) all the possible values that the predicate
can take must be declared. e.g.
warmode(val(A, #[mirix,idiary,bean,travex,mitre,legend,gunnes,java_course,
:
:
milton_keynes,weekly,daily,monthly,yearly,annual]).
This leads to unnecessary declarations and the enforced extensional deﬁnition of some
predicates where it is ineﬃcient and possibly detrimental to the construction of a theory
to do so.
Working with WARMR
The examples presented to WARMR were in the form of large ground facts, each con-
taining all the information for an individual example. Unfortunately, WARMR does not
allow functions to be used in the key literal, preventing the labelling of the arguments
being referred to by the additional literals added for each query.
An earlier attempt to use the Herbrand Model representation and split the information for
each example into separate facts showed that this approach did not present any advantages,
it simply made the learning process more complicated. The two representations shown in
Figure 3.2 are of the same example, however the modelling system would need to perform
a separate search on each relation within the Herbrand Model to identify any regularities.
The background predicates provided included those to specify relations between certain
attributes, and a val(A,#) predicate that had to be declared extensionally to give the
facility to bind variables to speciﬁc constants. Additional constraints were added to help
reduce the number of redundant literals added, such as removing the reﬂexive property
of the eq/2 (equals) predicate and insisting that each duration variable was only referred
to once per literal that contained an instantiation of # that could be used to restrict the
range of values the variable could take.
Results and Conclusions
The tests described earlier in the section that involved reducing the size of the train-
ing sets were not carried out during the testing of WARMR as this would just increase
the granularity of the frequencies of the queries produced but would have no other eﬀect

CHAPTER 3. CREATING A USER MODEL USING ILP
61
seq(travel,cambridge,n/a,2/0,7-0,monday,visit,nokia,n/a,4/0,10-0,monday).
begin(model(example1)).
type1(travel).
subject1(cambridge).
location1(n/a).
duration1(2/0).
preferred_time1(7-0).
preferred_day1(monday).
type2(visit).
subject2(nokia).
location2(n/a).
duration2(4/0).
preferred_time2(10-0).
preferred_day2(monday).
end(model(example1)).
Figure 3.2: Two possible representations of an example within WARMR
on the results, the same set of clauses would be returned. WARMR was presented with
each training set individually and with the union of the two sets to determine its ability
to produce the required concept representations in each learning situation.
Set 1 — preparation for meetings
An example of an ideal query that would be produced by WARMR is:
?- seq(A,B,C,D,E,F,G,H,I,J,K,L),val(A,prep),eq(B,H),eq(F,L),
with_people(H), project(B),
shorterthan(D,1/30),
shorterthan(J,2/0).
However, this query was not produced because WARMR exhausted all available resources
before reaching queries of this depth. WARMR was able to ﬁnish evaluating all possible
queries up to a depth of 3 literals, but when allowed to continue to a depth of 4, it failed
to complete its task (the machine on which it was running ran out of memory). A general-
isation of the required query could be found somewhere within the extremely large set of
results that WARMR had produced by the time it failed, but it would take considerable
additional computation to determine the ‘best’ answer from those produced. WARMR
took a considerable length of time to produce the set of results. On average it took 1hr54
before halting due to lack of resources.

CHAPTER 3. CREATING A USER MODEL USING ILP
62
Set 2 — visiting other companies
Examples of ideal queries that would be produced by WARMR would include:
?- seq(A,B,C,D,E,F,G,H,I,J,K,L),val(A,travel),eq(F,L),other_company(H),
with_people(G),located_at(B,H),
place(B),near_site(B),
shorterthan(D,1/30), shorterthan(J,4/0).
?- seq(A,B,C,D,E,F,G,H,I,J,K,L),val(A,travel),eq(F,L),other_company(H),
located_at(B,H),with_people(G),
place(B),not_near_site(B),
longerthan(D,3/0), shorterthan(D,6/0),
longerthan(J,2/0), shorterthan(J,7/0).
Neither of these queries were produced because WARMR once again exhausted all avail-
able resources before reaching queries of this depth. WARMR was able to ﬁnish evaluating
all possible queries up to a depth of 3 literals, but when allowed to continue to a depth of
4, it failed to complete its task. As with the previous set, generalisations of the required
queries could be found within the results set produced, but only after considerable ad-
ditional computation. For this training set, WARMR ran for an average of 1hr43 before
halting due to lack of resources. For both this training set and the earlier training set the
minimum frequency was set to 0.8, thus screening out a considerable fraction of the queries
that could have been reﬁned. Without this constraint WARMR would have produced an
even larger set of results.
Combining Sets 1 and 2
The union of sets 1 and 2 was given to WARMR. Given that WARMR’s main aim is to
ﬁnd queries that cover as high a frequency as possible of the examples given, the queries
with the highest frequency were ones that excluded very little of the example space and
hence did not provide much meaningful information.
For example,
freq(12,1,[seq(A,B,C,D,E,F,G,H,I,J,K,L),shorterthan(D,8/0)],1.0).
The above query states that all of the examples within the given data set had a ﬁrst
task that was shorter than 8 hours in duration. Whilst this may be of interest for other
information processing problems, it would not convey much useful information if it were
to be included within the model of the user.

CHAPTER 3. CREATING A USER MODEL USING ILP
63
Once again WARMR halted before ﬁnishing the full set of possible queries for the data
given. With the minimum frequency set as high as it had been for the previous two tests
(0.8), only predicates regarding the duration of the tasks were reported as the other pred-
icates had much lower frequencies due to the increased variance in the data. Dropping the
minimum frequency to 0.4 allowed the predicates referring to the other task attributes to
be used within the constructed queries once again. For this set of data WARMR ran for
an average of 1hr37 before halting due to running out of resources.
Conclusions
WARMR is not built with the construction of theories in mind; it was created for ﬁnding
regularities in large data collections. The query evaluation criteria of modelling more than
a set frequency of the data provided does not provide a meaningful measure of usefulness
for the purposes of constructing a user model. Evaluation of a query over a large data set
would result in a ﬁner degree of diﬀerentiation between queries whereas this is not possible
for a small data set.
WARMR produces its set of results by listing all the possible queries that cover a fraction
of the data set that is greater than the required level (known as the minimum frequency). If
the number of queries that met the evaluation criteria reduced considerably as the queries
returned increased in length then this criteria would not be a problem. However, given
the nature of the background knowledge and the examples in the data set, the number of
queries that will cover the required fraction of the data set using the predicates provided
initially increases as the length of the queries produced increases. The terms that can be
added to the query refer to diﬀerent arguments of the key literal, hence the longer the
query, the larger the number of possible combinations of literals that can produce a query
of that depth. The number of possible answers that can be produced for a particular query
depth will only begin to decrease once it can be guaranteed that all arguments within the
key literal are already referred to by a literal with the query.
The amount of time and resources that WARMR requires to produce queries approaching
the complexity needed to construct a model of the user with a reasonable level of recall
render this method infeasible. In addition, the set of results produced consists of a large
set of clauses that contain a lot of redundancy and the modelling system will have no
reliable method to work out which clauses are useful for a user model without a signiﬁcant
amount of re-evaluation.

CHAPTER 3. CREATING A USER MODEL USING ILP
64
3.2.4
Overall Conclusions
The results of the investigations into Aleph, Claudien and WARMR showed that only
Aleph displayed immediate potential to produce the required theories. The approach of
Claudien did not allow for the actual arguments of the relations presented to be considered
important. WARMR managed to produce a generalised version of the required clauses,
but the result was swamped by the large number of other possible queries returned, and
it took a considerable length of time before halting due to resource failure. Due to the
size of the set of results, the amount of redundancies within the set and the time taken
to produce it, use of WARMR within a system that needs to learn quickly is not feasible.
Given these results, this dissertation will focus on enabling Aleph to produce the required
user model, and will therefore be working within the normal ILP problem setting.
The rules produced for use within a user model need to be more speciﬁc than the rules
Aleph was able to produce. They need to be used to predict several items of information
at once and they must cover only a minimal area of the instance space outside the area in
which the examples they are attempting to model are located. The user model constructed
must have as high a level of recall as possible. Failure to produce some answers where
suggestions should have been made will be less inconvenient to the user than the production
of incorrect suggestions. Whilst Aleph did not produce the required clauses, it constructed
a theory that was a generalised version of the concept described by the training sets given.
If the result produced by Aleph could be specialised then it would model the required
concept. The idea of using ‘near misses’ as negative examples will enable the introduction
of a bias towards this specialisation. Negative examples that are deemed to be ‘near misses’
are referred to in numerous places in the literature on inductive learning. For instance,
Sebag and Rouveirol [1994] have carried out an investigation using integrity constraints,
instead of speciﬁc negative examples that are close to the given data set, and have shown
that additional negative examples or constraints that are further away from the positive
examples are no longer required if enough near misses are present. The most prevalent
opinion on them is that they are a very useful tool provided they can be guaranteed not
to cause overﬁtting. This idea is built upon in Chapter 4.
The results from the experiments where the size of the data sets produced was reduced, in
an eﬀort to mimic the sparse data that would be encountered in a real-world application,
showed that Aleph was able to produce the beginnings of the required concept even when
only a few examples were presented. As discussed earlier, the theories produced will be
sensitive to overﬁtting if the number of examples is very small, therefore this issue will be
kept in consideration whilst directing Aleph to produce additional specialisations.
The results from the experiments with the data presented as separate sets and the data
presented as one large set showed that Aleph produced a more reliable performance when
given the separate sets of data to work with. The separate data sets improved the theory
produced by making Aleph concentrate on ﬁnding a clause that covered the speciﬁc area

CHAPTER 3. CREATING A USER MODEL USING ILP
65
occupied by the data set that it was given.
What other methods could enable the theory produced to focus on speciﬁc areas of instance
space? Another approach to this problem is to work from the most specialised theory (e.g.
the data set itself as a set of ground clauses or a highly specialised clause that only covers a
small number of examples) and generalise it until it covers the given data set, rather than
starting with the most general theory and specialising it until it only covers the required
area. A bottom-up approach to learning the required clauses would seem to produce the
required specialisation and ability to focus on speciﬁc areas of the example space. However,
approaches that make use of a bottom clause like the one constructed by Aleph can be
shown to be ineﬃcient when the number of literals required in the clauses to be learnt for
the ﬁrst training set (7) is compared to the typical number of literals in a bottom clause
produced by Aleph (43). The required clauses are nearer to the top of the search tree of
possible clauses than the bottom. A bottom-up approach starting from a bottom clause
of the size generated by Aleph would involve considerable amounts of computation before
producing a satisfactory answer, hence the amount of time taken to generate a theory for
inclusion within the model renders this approach infeasible.
The approach used by CHILLIN [Zelle et al., 1994] of generalising over random sample
pairs of examples to produce a clause and then specialising that clause to exclude any
negative examples is more promising; however, in order to learn in a positive-only man-
ner, an assumption of output completeness is required. As mentioned in Chapter 2, the
sparse nature of the data set from which the model must be constructed would almost
certainly lead to overﬁtting if this approach were to be used. Another approach that could
be suggested would be some form of relative least general generalisation (which is also
discussed earlier in Chapter 2), however, this is a computationally expensive method to
use.
By partitioning the user model into separate sub-concepts and working with each sepa-
rately, speciﬁc areas of example space are identiﬁed that the model should cover. This
knowledge can then be used to guide the learner to generate the rules required. This has
the additional beneﬁt of making the model produced easier to understand by the user
because it contains a separate theory for each subconcept rather than modelling a com-
bination of diﬀerent subconcepts with a single theory. In addition if the model needs to
be updated then this task is made simpler because each area of the user model can be
revised individually instead of revising all of it at once (although this part of the modelling
process will be deferred as further work).

CHAPTER 3. CREATING A USER MODEL USING ILP
66
3.3
Partitioning the Set of Sequence Examples
Discussion of the merits of various clustering algorithms and distance measurements re-
quires more detail on the nature of the example space and the examples within it.
Each example in the data set has the following structure:
sequence(task(type(travel),subject(colchester),location(n/a),
duration(1/0),preferred_time(7-30),preferred_day(monday)),
task(type(visit),subject(nokia),location(n/a),
duration(6/0),preferred_time(9-00),preferred_day(monday)),
Each attribute of each task (e.g. type, subject) can have one of many possible values,
therefore each of these attributes is a dimension of the example space, with each example
representing one point that the space. The distance between two examples is therefore
the length of the vector between them (this is expanded upon in Section 3.4). The area
of space covered by a partition of the data set is the area between each of the points
represented by examples.
Given the large number of possible values that could be used in the description of each
example, and the lack of control over the data received, the most straightforward way to
partition the data set collected was to use some form of clustering to generate the separate
subconcepts.
A sizeable proportion of the existing clustering algorithms use some form of comparison
between the items that are being allocated to clusters. To ensure that each example within
a cluster contained roughly the same tasks in sequence, rather than compare complete
examples, the approach used was to generate a set of clusters on the basis of the ﬁrst task
in each example, task A, and then split each cluster into separate subclusters on the basis
of the second task in each example, task B. The clustering methods considered that used
comparison between the items being split into clusters were evaluated with this approach
in mind.
The method of measuring distance between tasks would have some impact on the methods
that could be used for clustering. Producing a measure of the distance between tasks
according to the user is not a simple matter. How the user would determine the distance
between a pair of tasks (for example, which attributes are looked at) may diﬀer between
each pair that is examined. To accommodate the variations in the manner in which the
user compares tasks, each pair of tasks was given a diﬀerent rating and a diﬀerent vector,
deﬁned by the user (this is expanded upon in Section 3.4).
As these ratings and vectors depended upon the user’s idea of similarity, the reasoning
behind why a pair of tasks was considered similar or diﬀerent could not be assumed
to be rational.
All ideas related to the use of properties of metric distance measures,

CHAPTER 3. CREATING A USER MODEL USING ILP
67
such as the use of the triangle inequality to reduce the number of comparisons made
[Kamgar-Parsi and Kanal, 1985, Lee et al., 1977, Biswas et al., 1981], could not be used as
the space in which the tasks are located may be stretched in a diﬀerent manner depending
on the examples being compared.
To illustrate this point, consider the diﬀerence rating and vectors of three separate tasks:
diff_rate(taska,taskb,dr(0.2)).
diff_vector(taska,taskb,dv(1,1,0,0,0,0)).
diff_rate(taskb,taskc,dr(0.8)).
diff_vector(taskb,taskc,dv(1,1,0,0,0,0)).
diff_rate(taska,taskc,dr(0.1)).
diff_vector(taska,taskc,dv(1,1,0,0,0,0)).
The diﬀerence vector for each pair of tasks refers to the attributes of each pair of tasks
that the user would compare when calculating a distance. The diﬀerence rating gives a
rough idea of how far apart the user considers the tasks to be. The triangle inequality
states that for any three points x, y, z in space, the method used to measure the distance
between them is a metric if equation 3.2 holds for each combination of those points.
d(x, y) + d(y, z) ≥d(x, z)
(3.2)
In the example given, the equation holds when taska is substituted for x, taskb is sub-
stituted for y and taskc is substituted for z.
d(taska,taskb) + d(taskb,taskc) >= d(taska,taskc)
0.2
0.8
0.1
However it does not hold when taskb is substituted for x, taska is substituted for y and
taskc is substituted for z.
d(taskb,taska) + d(taska,taskc) >= d(taskb,taskc)
0.2
0.1
0.8
If the distances between taska and taskb, and taskb and taskc had been determined and
the distance measure was assumed to be a metric, then it could reasoned that the distance
between taska and taskc would be too great to measure, whereas it can seen from the
information given that this is not the case. Hence the use of the triangle inequality or other
directives that work with distance metrics was not possible for this clustering problem.

CHAPTER 3. CREATING A USER MODEL USING ILP
68
Instead of using a clustering algorithm that needed a distance measure, another possibility
was to use a clustering algorithm based on the type hierarchy of the types within the
example. As each task had values for type, subject, and location then it could be reasoned
that the modelling system should have been able to use this existing hierarchy to partition
the examples into separate clusters. However, it could not be guaranteed that this method
would agree with the method of similarity measurement demonstrated by the user (due
to the user’s ability for irrational reasoning). The distance measure outlined earlier (and
discussed in detail in Section 3.4) took into account all attributes of the tasks being
compared that the user would consider important for each particular pair of tasks, and
ignored those considered by the user to be of no importance. It would also be used later
in Chapter 4 during the generation of additional negative examples, hence it was wiser to
use the same method on both occasions to determine what is, and what is not, part of a
particular cluster.
The two classes of clustering methods considered were agglomerative algorithms and divi-
sive algorithms. Each cluster was represented as a list of the examples contained within
it, therefore the area within the example space that was covered by the cluster was sim-
ply the area inhabited by those examples; it did not include space outside the cluster.
A divisive algorithm (such as the Top down Induction of Clustering trees (TIC) System
[Blockeel et al., 1998]) would be concerned with partitioning the space of possible exam-
ples into areas that contain individual clusters. This approach was not supported by the
cluster representation and did not enable the exclusion of areas of example space from
the clusters created, a necessary property of the clusters created in order to ensure that
the model produced has the highest level of recall as possible, hence an agglomerative
clustering algorithm was required.
An initial idea was to use the Leaders algorithm [Hartigan, 1975] (as shown in Figure 3.3)
as it was a fast way to assign tasks to clusters. Experiments with the Leaders algorithm
showed that the order in which the tasks were processed had an impact on the groups into
which the tasks were clustered, and could result in two identical tasks being assigned to
diﬀerent groups due the diﬀerence in the time of their allocation.
After the assignment of the ﬁrst of the two identical tasks to a cluster, another task could
be examined that was close to that particular task, but not close enough to the cluster
containing it to be included in that cluster and so it would form a new cluster. It would
then be possible for the second of the identical tasks to be found to be closer to the new
cluster that was created than the cluster containing the ﬁrst task and so the tasks would
be allocated to diﬀerent clusters. Experiments with virtual cluster leaders, calculated from
the most frequently occuring values of the attributes of the tasks within the data set, also
failed to eliminate the problem of the tasks being considered in an order.
Ideas such as those used for K-means [MacQueen, 1967] could not be used on the examples
collected from the user as the modelling system would not know the number of clusters

CHAPTER 3. CREATING A USER MODEL USING ILP
69
For each task:
Compare it to each existing group leader, pick the nearest.
If distance < threshold
Add task to group.
else
Example becomes leader of a new group.
We should end up with a list of groups, each with a leader.
Figure 3.3: A representation of the Leaders clustering algorithm
to be produced before beginning the clustering process. ISODATA [Ball and Hall, 1967]
also used an initial selection of cluster leaders (‘typical points’) and clustered around
them. Clusters that were too variant were split, clusters that were similar enough were
joined. This was better than K-means for use with the example set as it did not require
knowledge of how many clusters are to be created, but it was unlikely to work well if the
initial cluster points were a long distance from the locations that would produce the best
results, i.e. it would work best with prior knowledge as to what the concepts contained
within each cluster would look like. The modelling system would not have any indication
of this information, hence this algorithm showed no beneﬁts over a simple hierarchical
agglomerative algorithm.
The ﬁnal approach considered was a greedy bottom-up agglomerative approach (shown in
Figure 3.4) as this would remove the ordering problem by considering all the tasks at once
and then joining together the closest tasks into groups. As the set of tasks was unlikely
to be very large, it did not matter if all tasks were compared to all other tasks.
The initial set of clusters was created by constructing a list of all the pairs of examples and
the distance between them. Only pairs where the distance was less than the given cluster
threshold distance were kept on the list, which was then sorted in order of increasing
distance. Examples were joined to their closest neighbour, starting with the ﬁrst pair in
the list. When an example was encountered that was already a member of a cluster, then
the other example in the pair was examined to see if it could also be a member of the
cluster. If the example was close enough then it was added to the cluster, if not then it
was deemed to be an outlier and added as a new cluster. In some ways this was similar
to the Leaders algorithm outlined earlier, however now it could be guaranteed that the
distance between an example being added and examples that were already members of
clusters was greater than the distance between the members of each cluster, thus avoiding
the problem that would have occurred if the Leaders algorithm had been used.

CHAPTER 3. CREATING A USER MODEL USING ILP
70
E is the set of all tasks contained within examples collected from the user
t is the cluster threshold distance
C is the set of all clusters = ∅
D = list of all tuples ⟨em, en, δ(em, en)⟩where em, en ∈E ∧em ̸= en ∧δ(em, en) ≤t,
sorted in order of increasing δ(em, en).
First the initial set of clusters is created, a slight variation on the classical algorithm.
For i = 1 . . . length of D
d = Di = ⟨e, e′, δ(e, e′)⟩
if ((∃c ∈C : e ∈c) ∧(¬∃c′ ∈C : e′ ∈c′))
then if (∀m ∈c : δ(m, e′) ≤threshold)
then c = c ∪{e′}
else C = C ∪{{e′}}
else if ((∃c ∈C : e′ ∈c) ∧(¬∃c′ ∈C : e ∈c′))
then if (∀m ∈c : δ(m, e) ≤threshold)
then c = c ∪{e}
else C = C ∪{{e}}
else if (∃c ∈C, c′ ∈C : e ∈c ∧e′ ∈c′)
\\ c and c′ may refer to the same cluster.
C = C
\\ (i.e. do nothing)
else
C = C ∪{{e, e′}}
\\ create a new cluster.
Then the clusters are merged with their closest neighbours, referring to the
original D for distances to reduce computation time.
while (∃c ∈C : (L = {c′|c′ ∈C ∧c ̸= c′ ∧(∀e ∈c, e′ ∈c′ : δ(e, e′) ≤t)}) ̸= ∅)
{
c′′ = l, where l ∈L ∧(¬∃l′ ∈L : δ(l, c) > c dist(l′, c))
C = ((C \ {c}) \ {c′′}) ∪{c ∪c′′}
}
The distance between two clusters, c dist(c1, c2) = Ps
1
Pt
1 δ(es, ft)
where c1, c2 ∈C ∧e ∈c1 ∧f ∈c2
s = card(c1)
t = card(c2)
The distance between two tasks, δ(e, f), is deﬁned in Equation 3.7
Figure 3.4: The bottom-up agglomerative clustering algorithm used

CHAPTER 3. CREATING A USER MODEL USING ILP
71
Once all the examples had been processed, a list of pairs of clusters where all the members
of both clusters were within cluster threshold distance from each other was constructed.
The list was constructed in order of the sum of the distances between all the examples of
both clusters, with the pair of clusters that had the least overall distance at the head of
the list. Starting at the head of the list, each pair of clusters were treated in the same
way as the pairs of examples had been treated previously. This process was then repeated
until the list of potential pairs of clusters that could be merged was empty.
This agglomerative algorithm appears to be quite ineﬃcient in terms of the number of
comparisons made between examples during each iteration. A method such as the one
used by COBWEB [Fisher, 1987] would appear to be more eﬃcient since it only compares
clusters of examples that have already been shown to be reasonably similar to each other.
COBWEB constructs a concept hierarchy from its given data set in an incremental fashion.
The hierarchy is represented by a tree where each node represents a sub-concept of its
parent node and a sub-partition of the parent node’s partition of the examples seen so
far. Two nodes are only considered for merging if they are the two best choices to receive
a new example that has been observed. This means that the two nodes are likely to be
similar and therefore consideration of merging is appropriate. Since this consideration of
two nodes for possible merging is made each time a new example is added then the rest
of the tree is assumed to have already been taken care of. Only the point where a change
is taking place needs to be focused on. The two nodes being considered for merging could
only have been selected together as the two best choices if they were children of the same
parent. Therefore incorporating the merged nodes into the tree is simple since it only
requires producing a new node with the same parent as the old nodes and the two old
nodes as its children. Given that COBWEB works in an incremental manner it also will
determine whether a node should be split when it adds a new example. The splitting
process is a reverse of the merging process; the node’s children become children of its
parent node and the node itself is removed.
The merging procedure carried out during the bottom-up agglomerative algorithm is also
very simple.
Since the clusters are represented by lists of examples, two clusters can
be merged to produced a new one by concatenating the lists. The algorithm does not
currently require the facility to split clusters since it works with the entire data set. This
may be needed later on once time (and hence new examples) are added to the system.
However, consideration of an appropriate method will be postponed until this occurs since
it may be aﬀected by other alterations to the system taking place at the same time.
COBWEB’s evaluation of nodes when determining where to assign a new example or
whether nodes should be merged or split would involve less calculation for the data set
produced by iDiary than the bottom-up agglomerative algorithm described earlier. How-
ever, COBWEB’s algorithm and the data structure that it produces are more complicated
than is required for this application. When working with a small data set it is unlikely

CHAPTER 3. CREATING A USER MODEL USING ILP
72
that there will be a signiﬁcant diﬀerence in the amount of time taken to produce the same
set of clusters with either algorithm. However, for larger data sets produced by other
applications the ideas within COBWEB may be more appropriate.
The requirement that all members of a pair of clusters be less than cluster threshold
distance away from each other, taken from the mutual nearest neighbour algorithm
[Gowda and Krishna, 1978], ensured that long, string-like clusters were avoided in favour
of the more globular constructs that should be more easily covered by the rules produced
by Aleph.
The original mutual neighbourhood algorithm had a set number of mutual nearest neigh-
bours that two items had to have before they could be considered as members of the same
cluster, the higher the value, the more globular the cluster. Instead of using this value
it was decided to require that all members of the cluster must be less than the given
threshold distance from each other. This brought one additional concern to be addressed.
‘What happens when there is a potential cluster whose width is greater than the cluster
threshold distance?’ Given that all the examples had the same classiﬁcation, this was not
a problem. A theory would be constructed for each cluster produced and these theories
would cover adjoining areas of example space. Whilst this may not have provided the
most concise description for the user model, it would ensure a reasonable level of recall
was maintained. As the clusters produced would be prone to overﬁtting, techniques to
help avoid this were included in the methods discussed in Chapter 4.
3.4
Measuring Distance Between Tasks
In order to use the clustering method described in the last section a measure of the
similarity between tasks was required. The features included in the measure are listed in
Table 3.1, and are considered in turn within this section.
Type, Subject and Location
Task feature
Description
Type, subject,
These values provide a description of the activity carried out:
location
Type gives an overall description,
Subject will include details such as project names, and
Location will give the location in which the task is carried out.
Approx. length
The amount of time that the task is estimated to take to complete.
Preferred time
The time that the user would prefer to carry out the task.
Preferred day
The day on which the user would prefer to carry out the task.
Table 3.1: Recorded features of each task

CHAPTER 3. CREATING A USER MODEL USING ILP
73
The diﬀerence in type between two tasks was obviously of very high importance when
comparing tasks. Even if the main type of both tasks were the same, the subject or loca-
tion may diﬀer and this may have a signiﬁcant eﬀect on what the user was actually doing
during the tasks. Equation 3.3 gives a diﬀerence function that could be used for any of
these features.
δ(A, B) =



1
if A ̸= B
0,
otherwise
(3.3)
It was also reasoned to be a good idea to consider two items to be the same if they were
of the same ‘group’, e.g. two companies. If this was not included then the overall distance
measure was far more likely to calculate the distance between all non-identical examples to
be greater than the given cluster threshold. This would lead to all clusters only containing
identical examples, rather than the same sequence applied to diﬀerent places, companies
etc. The rules within the model created should generalise over these items and so this bias
should be reﬂected in the distance measure.
Equation 3.4 could be used on the subject and location of each pair of tasks. It was not
used on the type of a task as some of the items that tended to occur in this argument had
been deﬁned as members of the same group but could not be regarded as similar enough
to be included within the same cluster, for example visit (where the user visited another
company), and annual leave had both been included within the group out of office.
δ(A, B) =



1
if same groups(A, B)
0,
otherwise
(3.4)
same groups checked that both items belonged to all of the same groups as each other.
The check was made this strict to avoid declaring that there was no distance between
two items that were vaguely the same but had important diﬀerences. e.g. London and
York are both destinations, but depending on where the user started from, one was much
further away than the other. If London was also described as near_site and York was
described as not_near_site then these two items would no longer be regarded as the same.
Duration
When comparing values of features concerned with time, the measure chosen reﬂected
the fraction of a day that the diﬀerence corresponded to.
δ(T1, T2) = |total mins(T1) −total mins(T2)|
60 × 24
(3.5)

CHAPTER 3. CREATING A USER MODEL USING ILP
74
Preferred Time and Preferred Day
The prediction of immediate sequences of tasks did not require specialisations for pre-
ferred time and day. Any task predicted using the model produced would simply be added
before or after the task entered by the user. However, these task attributes would still
be examined when comparing the distance between two tasks to detect ‘unusual’ activi-
ties, e.g. one task may occur within oﬃce hours and the other outside that time period.
Equation 3.6 was used to calculate the distance for both preferred time and preferred day.
δ(P1, P2) =



1
if same time situation(P1, P2)
0,
otherwise
(3.6)
same time situation/2 is true if both arguments occur within ‘oﬃce hours’ or both occur
outside that time frame.
3.4.1
Constructing the Diﬀerence between Tasks
Having determined the distances between separate task features, these distances had to be
combined into a simple measure. Diﬀerent users may place emphasis on diﬀerent aspects
and so each feature had a weight associated with it that could be set by the user.
d(task(t1, s1, l1, a1, p1, d1), task(t2, s2, l2, a2, p2, d2)) =
p
u(t2) + v(s2) + w(l2) + x(a2) + y(p2) + z(d2)
(3.7)
where
t = δ(t1, t2)
t1, t2 are types,
s = δ(s1, s2)
s1, s2 are subjects,
l = δ(l1, l2)
l1, l2
are locations,
a = δ(a1, a2)
a1, a2 are durations,
p = δ(p1, p2)
p1, p2 are preferred times,
d = δ(d1, d2)
d1, d2 are preferred days,
u, v, w, x, y, z are the user-deﬁned weights
Equation 3.7 uses each feature as a diﬀerent dimension and then uses the weights to stretch
the space to reﬂect the user’s view of how important each task feature is.

CHAPTER 3. CREATING A USER MODEL USING ILP
75
3.4.2
A Task Comparison Model
The measurement of distance between tasks has one weight per feature considered impor-
tant. The value assigned to these weights should reﬂect the user’s own decision making
process and so the modelling system should attempt to construct a model that could be
used to predict the appropriate weight values for each pair of tasks being compared.
A simple initial data gathering exercise was performed to evaluate the feasibility of asking
the user for data about the comparison of tasks. All the examples within the data sets
used for the experiments in Section 3.2 were split into a list of single tasks and a user was
asked to provide a comparison measure for each task within the list with every other task
in the list. Initially it was thought to ask the user for an indication of which attributes
inﬂuenced their decision as to how similar two examples were and to what extent each
attribute had inﬂuenced the decision (i.e. on a scale of 0 to 1 instead of just ‘yes’ or ‘no’).
The intention was to give diﬀerent weights to each attribute depending on the answers
given. However, asking for this amount of detail is unfair on the user and they are unlikely
to be able to give accurate answers. Kohavi et al. [1997] showed that it was enough to
have two weight values for the example attributes, one value to show that an attribute was
important and one value to show that an attribute was not important. This information
would be enough to gain a picture of how a user decided how similar or not two tasks
were. The user was asked to give an indication of how diﬀerent they thought the pairs of
tasks presented were, and to indicate using a simple yes or no which task attributes had
been considered when making the decision.
Unfortunately, if the user was asked to provide this information, then the total number of
comparisons that the user could be asked to make would increase very quickly beyond the
number that the user could be reasonably expected to answer, even if only a small number
of examples of sequences had been collected. Unsurprisingly, during the experiment only
a small fraction of the total possible comparisons were made before the user ran out of
patience and stopped. However from the data collected some observations could be made.
The most obvious observation was that the granularity of the diﬀerence ratings asked for
was too ﬁne. According to the user, the values between 0.0 and 1.0 in steps of 0.1 that
were returned tended to fall into four areas, identical (0.0), similar (0.2, 0.3), related(0.4,
0.5), and diﬀerent(mainly 0.7 and 0.8).
Working with the idea of giving each pair a
classiﬁcation, and an indication as to which attributes were considered, this indicated two
prediction models to be learnt:
Diﬀerence Rating — How diﬀerent, generally, the given pair of tasks are. Classiﬁcations
to choose from are {identical, similar, related, diﬀerent}. These classiﬁcations would
be translated from the more user-friendly terms into static values (e.g. {0.0, 0.2, 0.5,
0.8}) for use within the distance measure.
e.g. diﬀrate(taska,taskb,identical).

CHAPTER 3. CREATING A USER MODEL USING ILP
76
Diﬀerence Vector — Which attributes should be considered when calculating distance.
e.g. diﬀvector(taska,taskb,v(1,1,1,0,1,0)).
Once the two sets of rules were created, then the distance between any pair of tasks could
be calculated by combining the diﬀerence rating and diﬀerence vector for that pair of
tasks in order to create a set of weights that would reﬂect the bias of the user.
The
level of diﬀerence given by the user would be used to stretch the ratios of the weights
applied to each task feature. This would make the values of the weights returned when
the two tasks were considered similar smaller than those returned when the tasks were
considered diﬀerent, creating a greater range of distances that would help to reﬂect the
users preferences.
e.g. 2,2,2,0,0,2,0
difference rating 0.2
5,5,5,0,5,0,5
difference rating 0.5
As mentioned earlier in the section, an initial data gathering exercise was carried out to
determine the feasibility of asking the user for answers about task comparison. The test
subject attempted to perform as many comparisons as possible but found the exercise
increasingly tedious as more pairs of tasks were presented.
The conclusion from this
exercise was that it is unlikely in most cases that enough data could be gathered from the
user in order to attempt to build a user model with a good enough level of recall.
An immediate solution was to use a hand-coded ‘default’ user model constructed from the
ﬁndings of the experiment. The model consisted of a set of rules that roughly described
what was meant by the four diﬀerence ratings that emerged, and a set of rules that
approximated the diﬀerent sets of features considered. The model would remain under
observation throughout the experiments carried out in Chapters 4 and 8 to make sure it
produced the expected clusters and enabled appropriate ‘near misses’ to be generated as
negative examples.
A possible extension to this solution, if the system was to be used in the real world, would
be to provide a default model such as the one described, but allow the user to generate
their own preferences for distance measuring via some sort of user-friendly interface. The
user would be presented with a representation of the current model with appropriate
explanatory facilities and then could change the parts that they disagreed with. Whilst
this is not an ideal solution because it increases workload for the user, they have the choice
as to whether or not they personalise their model.
The ideas collected for the construction of a model of the user’s method of measuring
distance between tasks were also used in Chapter 5 as part of an additional learning
exercise that enabled further exploration of the nature of the sorts of concepts that the
method, Dilum (presented in the next chapter), could be used to learn.

CHAPTER 3. CREATING A USER MODEL USING ILP
77
3.5
Related Work
Data partitioning is a common approach used within the area of data mining to split up
very large databases of information into smaller data sets that can be processed more
eﬃciently (Provost and Kolluri [1999] provide a good review of the commonly used ideas
and methods). The idea behind data partitioning is that these smaller data sets will still be
representative of the data contained within the entire database. i.e. any concepts present
within the overall database will have been preserved in the partitions created (although it
is not necessary for all concepts to be preserved in all partitions). One of the main areas
of research has been how to minimise the eﬀect of partitioning data on the results that
are gathered from mining each partition.
Two of the main approaches to combining the results retrieved from each partition are
to use each hypothesis that has been formed as a separate classiﬁer and attempt to unify
the results from each hypothesis on new data in some way, or to attempt to merge the
hypotheses retrieved to form one overall classiﬁer.
The ﬁrst approach can be implemented in several ways, although use of arbiters or com-
biners to bring together the results of the individual classiﬁers created is common. Chan
and Stolfo [1997] give a comprehensive description of the methods employed by both, and
extends an approach called Metalearning (presented in [Chan and Stolfo, 1993]) that uses
a tree of arbiters or combiners for this purpose.
The second approach also makes use of diﬀerent ways to combine the results of each
learning problem. If the results produced are sets of rules then the possibilities include
simply creating a union of the diﬀerent rule sets, or attempting to merge the rules in each
set, removing contradictions as they occur. This last approach is used by Hall, Chawla,
and Bowyer [1998,1999]. Wang [2000] uses parallel inductive logic programming to improve
the eﬃciency of learning concepts by giving each disjoint subset of the data to a separate
processor. Once each processor has ﬁnished its initial learning phase then the processors
negotiate with each other to produce a ﬁnal overall hypothesis.
One additional way in which data partitioning can be used is to give each subset of the
data to a separate processor and carry out the generation of the classiﬁer on a central
processor (as described by Provost and Aronis [1996]). The central processor generates
partial hypotheses and sends these out to the other processors which then evaluate each
hypothesis on their given data set and return the results.
This method improves the
eﬃciency of evaluating hypotheses, but has a high communication overhead.
The idea behind the use of data partitioning is that each partition of the data represents
a diﬀerent view of the information contained within the database, and that, if the entire
database could be processed as a single data set then the result would be the most accurate
and eﬃcient classiﬁer that could be produced from that set of data. The aim of data
partitioning is to attempt to achieve this classiﬁer whilst improving the eﬃciency of the

CHAPTER 3. CREATING A USER MODEL USING ILP
78
data mining process. This is somewhat diﬀerent to the reason why clustering is required
as a pre-processing step in order for the modelling system to be able to construct a user
model with a high level of recall. The problem that must be overcome within the data set
that the model is to be constructed from is that the data contains examples of what would
be regarded as completely diﬀerent concepts by the user. Splitting the data into disjoint
subsets actually improves the results produced rather than having a detrimental eﬀect.
3.6
Summary
This chapter began by clarifying the nature of the data set that would be collected from
iDiary and describing the method used to select examples of tasks in immediate sequence.
The ‘empty task’ was presented as a means to indicate when tasks within the diary had
been scheduled without any other tasks preceding or following them. The examples con-
taining an empty task will only be used when evaluating the predictions made using the
user model (as mentioned earlier, this mechanism will be discussed later in Chapter 7).
Therefore these examples will not be included in the data sets used in the experiments pre-
sented in this dissertation. In later chapters the data sets will contain a mixture of correct
and noisy data. However, this noise will be in the form of ‘corrupted’ correct examples,
i.e. where one task in a correct example has been replaced by a task from another correct
example to form an example that should not occur. For the investigation carried out in
the current chapter, the data sets only contained correct examples of tasks in immediate
sequence.
The experiments within this chapter demonstrated that the ILP methods investigated
could not create the theories required for use within the user model. However, it has also
been shown that the ILP engine Aleph has the potential to reliably produce such a model
if given some form of assistance. If the data collected from the user is partitioned into sets
that represent separate subconcepts to be learnt then Aleph is able to reliably produce
the beginnings of the required theory for each subconcept. However, some form of bias
towards the production of more specialised clauses must be introduced so that the model
produced will be able to provide all the information required for each prediction.
After an investigation of appropriate methods to use when partitioning the given data into
subconcepts, a bottom-up agglomerative clustering algorithm was argued to be a suitable
candidate to produce the subsets of data required. A distance measure was constructed
for use within the clustering algorithm and a model of the user’s method of gauging the
similarity between individual tasks was considered. Experimental results showed that not
enough data could be gathered to construct a reliable model and so a default model with
the option to alter it if required was proposed as an immediate solution.
The next chapter will investigate a method that can provide the assistance needed by
Aleph to produce the required theory for each subconcept within the model.

Chapter 4
Multiple Value Predictions
Chapter 3 addressed the ﬁrst part of the problem that would be encountered when using
ILP to produce a user model of a vague concept such as sequences of actions from a small
data set. It was shown that partitioning the set of examples collected into groups that
represent the diﬀerent subconcepts within the example set, and giving these groups indi-
vidually to Aleph, enabled Aleph to produce a generalised version of each of the diﬀerent
sub-concepts that make up the overall concept. These groups of examples will be referred
to as clusters during the rest of this chapter as a clustering method was used to carry out
the partitioning stage for the production of the particular model under construction.
For the rules constructed by Aleph to be of use within the user model they must be able
to provide a greater level of detail than the initial results showed. The model needs to be
able to use the rules created for prediction of complete task descriptions and so all of the
required attributes should be predicted.
The complexity of the predictions the model must make can be seen within the example
below:
sequence(task(type(travel),subject(colchester),location(n/a),
duration(1/0),preferred_time(7-30),preferred_day(monday)),
task(type(visit),subject(nokia),location(n/a),
duration(6/0),preferred_time(9-00),preferred_day(monday))).
Given either the ﬁrst or the second task, the model must predict the other task in the
sequence, providing enough detail for the task to be scheduled within iDiary. This would
require an approximate duration, and an outline of what the task actually is (i.e. its values
for type, subject, and location).
The hypothesis created for each cluster of examples should only cover the area of instance
space covered by that cluster (i.e. the examples and the space between them with perhaps
a little space outside to enable a more ‘rounded’ concept and help to ease overﬁtting).
79

CHAPTER 4. MULTIPLE VALUE PREDICTIONS
80
The area outside each cluster is not known to be part of the concept demonstrated by
the user and could contain values that the user would deﬁnitely not have demonstrated
as examples; by avoiding extensive coverage of this area the likelihood of producing a
hypothesis with a high level of recall is improved.
Why learn rules to cover each cluster when the clusters themselves already provide a
rough description of each facet of the concept? Each cluster was created by measuring
the distance between examples using a measure that is not readily understandable to the
user and does not make use of the intensional (and extensional) background knowledge
provided. The language used to construct the hypotheses is aimed at concept description
and the prediction of values rather than producing an indication of the diﬀerence between
given tasks. It is not suited to distinguishing between each example in a manner that would
provide the required clusters (for example, it does not have facility to provide a measure
of diﬀerence between times of day or to say whether two given values belong to the same
groups). However, once the clusters have been formed, the hypothesis language can be
used to provide a coherent representation of the observed concepts. If the model used the
clusters themselves when making predictions it would encounter the same problems that
the evaluation of k-nearest neighbour encountered when used on the test sets in Chapter
8 (this is described in more detail in that chapter). When the model was used to predict
possible sequences for a task that contained values that had not been encountered before,
correct predictions could not instantly be made. Lack of generalisation meant that, each
time this situation was encountered, the model would have to search for relations over
the examples within the existing cluster and then use them to make the new prediction.
Rather than search for these relations each time it would be more sensible to record them
for future use, hence the learning of a theory to cover each cluster.
The sort of theory that Aleph is required to produce goes against the general nature
of current popular ILP algorithms, which look for the most general acceptable theory
to explain the given data. Depending on the method of clausal evaluation, the theory
produced may cover areas of instance space that will never contain examples from the
user. For this type of learning problem, the most general acceptable theory is still more
speciﬁc than the usual sorts of concepts that ILP is used to generate, and bottom-up
methods such as relative least-general generalisation are too computationally expensive to
be feasible (as mentioned in Section 2.3). Covering areas of unknown space will aﬀect both
the ability of the theory produced to predict in the required level of detail and with the
required level of recall. Aleph needs to be provided with stimuli to avoid covering areas
outside the area occupied by the cluster. Given that the area to be covered is already
known because of the commitment to covering the area occupied by a cluster of examples,
additional examples can be generated that are located just outside this area. As discovered
from the experiments in the previous chapter, these ‘near misses’ can be used as negative
examples to force Aleph to exclude areas of instance space outside the cluster from the
area covered by the rules under construction.

CHAPTER 4. MULTIPLE VALUE PREDICTIONS
81
However, even with these additional negative examples, the amount of specialisation re-
quired cannot be produced using a non-greedy ILP algorithm. The amount of background
knowledge that is applicable for each subconcept is quite large (bottom clauses produced
by Aleph have been observed containing as many as 43 literals) and the clauses required
are complex due to the need to make multiple-value predictions. These two facts mean
that the search space of possible hypotheses is very large, even though it has already been
reduced from the overall possible hypothesis space by use of a bottom clause and various
syntactic restrictions. Apart from some bias towards specialisation, there exists no means
to guide Aleph to search certain areas of the space because it is not known ahead of time
what form the concepts to be learnt will take. The size of the search space is so large that
it cannot be guaranteed that the appropriate answer is given before the algorithm exceeds
the modelling system’s given resources (memory or time).
In this chapter, a new method, Dilum, is presented, as detailed in Figure 4.1. It will
be shown that if it is possible to split the learning problem by using an assumption of
independence into several separate learning problems, each concerned with a subset of
the attributes of the concept to be learnt and with individual sets of background data
and negative examples to be used, then the size of the search space that the learner must
conclusively explore can be reduced to a more manageable level. This method allows the
modelling system using Aleph to generate theories with the required level of specialisation
within the limited amount of time available. This chapter will also show that the idea
presented in the previous chapter of partitioning the data into separate subconcepts is
beneﬁcial to greedy top-down learners as well as exploratory ones.
Partition the data into groups, each representing an independent subconcept.
For each group:
Separate the subconcept into independent dimensions
For each dimension:
Generate the appropriate negative examples using the original group.
Give the group, negative examples, and relevant background knowledge
for this dimension to the learner.
Collect the results.
Combine and ﬁlter the results to produce a theory to cover the group.
Collect the results for each group to form the overall theory.
Figure 4.1: The full Dilum method for exploratory ILP algorithms

CHAPTER 4. MULTIPLE VALUE PREDICTIONS
82
4.1
Creating Specialised rules
Each negative example is produced by altering only one of the attributes in one of the
tasks of an original positive example until the distance between the created example and
its originator is at least twice the cluster threshold distance used in the generation of
the original clusters, thus creating sets of examples that are ‘near misses’ of the original
concept. The distance between the original positive example and the negative examples
generated from it is measured using the distance measure that was used to produce the
clusters so that only one deﬁnition of which areas of the example space should be considered
part of the concept is present within the system.
Suppose the following positive example is collected from the user:
sequence(task(type(travel),subject(colchester),location(n/a),
duration(1/0),preferred_time(7-30),preferred_day(monday)),
task(type(visit),subject(nokia),location(n/a),
duration(6/0),preferred_time(9-00),preferred_day(monday))).
This is an example of the user’s intention to travel to colchester and visit a company
(in this case nokia) there. A negative example is generated from this positive example
by altering a single attribute of one of the tasks to a value that would not be suitable
when describing the concept represented by the original example. This method is used to
generate negative examples for each attribute within the example that needs some form
of specialisation. One of the possible negative examples is shown below. The type of the
ﬁrst task has been altered to a value that places the example far enough away from the
concept cluster to be considered negative.
sequence(task(type(admin),subject(colchester),location(n/a),
duration(1/0),preferred_time(7-30),preferred_day(monday)),
task(type(visit),subject(nokia),location(n/a),
duration(6/0),preferred_time(9-00),preferred_day(monday))).
In the case of attributes such as the task type, more than one diﬀerent value is put in place
of the original value to indicate that it is not just that admin is not allowed in this case. For
this sort of data, a random selection of values can be used. The constraints for the selection
of values are that each selection must not contradict any of the other positive examples to
be used and each selection must belong to diﬀerent group from the original value (i.e. if
the original value is a company, then the new values will not be companies). This allows
the generation of rules that generalise over groups such as companies or projects. For
attributes containing values that have a range, such as the duration of a task, the values
selected will be those that are close to the original values, but with an imposed margin of
distance to help avoid overﬁtting in the rules produced.

CHAPTER 4. MULTIPLE VALUE PREDICTIONS
83
c is the cluster threshold distance
m is the number of -ve examples to be created per +ve example
D = {type1,subject1,location1,duration1,type2,subject2,location2,duration2}
i ∈D
\\ The current dimension the modelling system is working with.
P is the initial set of +ve examples
E is the set of all examples that can be generated using the current background knowledge
S is a set of +ves & generated -ves = ∅
For each p ∈P
{
Np = {n|n ∈E ∧(δi(n, p) > 2c) ∧(∀j ∈(D \ {i}) : δj(n, p) = 0)}
If (i ∈{duration1, duration2})
Rp = {m closest members of Np to p}
else {
Rp ⊆Np where card(Rp) = m and
all members of Rp are randomly selected from Np
}
S =S ∪Rp ∪{p}
}
\\ δx(p, q) = distance between tasks p and q in dimension x.
Return S
Figure 4.2: Method for the generation of sets of negative examples
Only examples that have a distance from each member of the original cluster that is equal
to or larger than 2c (where c is the cluster threshold distance used in the creation of the
clusters) are collected for use as negative examples. Any examples that are closer than
this to the original cluster are ignored as they may be too close for Aleph to be able to
form a hypothesis whilst excluding the space in which they occur. This creates a space
around the original cluster that is free from negative examples and should help Aleph to
form a more general rule (hence helping to alleviate the possible overﬁtting that is made
likely by the sparse nature of the data) rather than being forced to return the original set
of examples as the best hypothesis. A subset of the possible values that could be used to
generate negative examples is used as a large number of negative examples will not aid
the learner but would increase the evaluation time for each clause being constructed. An
outline of the method used is shown in Figure 4.2.

CHAPTER 4. MULTIPLE VALUE PREDICTIONS
84
4.2
Splitting The Search Space
The clause below shows the average length of the theories that Aleph is required to produce
within a limited time:
sequence(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),duration(J),
preferred_time(K),preferred_day(L))):-
A = travel,
not_near_site(B),
C=n/a,
longerthan(D,3/0),
shorterthan(D,5/30),
G = visit,
located_at(B,H),
room(I),
longerthan(J,2/0),
shorterthan(J,5/30).
Some of the predicates within the available background knowledge (the ones that referred
to task duration) are included several times (using diﬀerent ground values) within the
bottom clause in an eﬀort to ease the amount of overﬁtting that could occur. Unfortunately
this increases the size of the bottom clause quite dramatically (⊥contained just over 40
literals on average), and hence the size of the hypothesis space increases to the point where
the search space is too big to be considered as a whole within the limited time available.
Early attempts to learn the complete concept within one learning session showed that the
rules produced were far from satisfactory (Aleph consistently exceeded the time resource
allocated and returned a mixture of incomprehensible and ground clauses.) Reduction of
the search space by reducing the limit on the number of literals allowed per clause would
be counter-productive as this would exclude the detailed clauses required. Combining the
available background knowledge into composite predicates (not in the same manner as
feature construction, mentioned earlier, but on a more permanent basis) would also not
be beneﬁcial. Apart from the rules within the model needing range-restriction for each of
the attributes to be predicted, it would not be possible to determine which combinations
would be appropriate, this would depend on the sequences demonstrated by the user.
The remaining approach that can be used is to reduce the number of possible literals
to be considered in clause construction. The artiﬁcially generated negative examples are
rendered negative by the altered value of only one attribute within each example, which
means they can be grouped according to the attribute that has been altered. These groups

CHAPTER 4. MULTIPLE VALUE PREDICTIONS
85
can then be used a few at a time to create several smaller learning problems that only
require specialisation over a few attributes rather than over all of them. Most ILP learners
are unlikely to produce rules containing the level of specialisation required by the user
model, but will produce rules requiring a lower level of specialisation. If an assumption
of independence can be made between the diﬀerent dimensions to be specialised, then the
problem can be presented to Aleph as a series of subproblems. Aleph will then produce
separate sets of results containing all the specialisations required for each attribute that can
then be combined to produce an overall hypothesis. In fact, for this model, this assumption
has already been made during the generation of the artiﬁcial negative examples.
The
‘near misses’ generated did not take into account any dependencies between attributes
when replacement values were being selected. Due to the partitioning that has already
taken place, the examples within each data set that will be presented for each learning
problem are known to occupy roughly the same area of the example space. This redeﬁnes
the learning problem from focusing on a global concept to focusing on a local concept
that simply requires a particular area of the example space to be covered. The theoretical
concerns behind these ideas are discussed in Chapter 5 after further exploration of what
Dilum enables an exploratory ILP engine to do.
One way to visualise the idea of splitting the learning problem into independent dimensions
is to view each clause within the hypothesis to be learnt for each cluster of examples as
a conjunction of n predicates, each referring to an independent dimension of the learning
problem.
h ←p1 ∧p2 ∧· · · ∧pn
Each learning problem can then be thought of as learning a deﬁnition for a particular
predicate. Once the learner has determined deﬁnitions for each predicate then the predi-
cates in the overall hypothesis can be replaced with these deﬁnitions. As each predicate
could have more than one clause in its deﬁnition, h would become the disjunction of all the
possible conjunctions of clauses returned. Figure 4.3 gives an example of how the results
from each dimension would be combined. Whilst the initial h could be used without sub-
stituting all the possible deﬁnitions into the predicates, this reduces the understandability
of the hypothesis produced. In addition, by carrying out the unfolding step it is possible
to prune some of the clauses produced that do not cover any of the original examples,
improving the recall of the hypothesis.
The set of negative examples can be divided into separate groups according to the attribute
that was altered to place the example far enough away from the original cluster for it to
be considered negative.
Using only one group of negative examples for each learning
problem means that Aleph would only require a subset of the original set of background
predicates. However, literals generated from the other background predicates would still
be included within Aleph’s bottom clause and would require a small amount of evaluation
to determine their usefulness. In order to ensure that Aleph does not waste time exploring

CHAPTER 4. MULTIPLE VALUE PREDICTIONS
86
h ←p1 ∧p2 ∧p3
p1 ←a ∧b
p2 ←q
p2 ←r
p3 ←x
p3 ←y
h1 ←a ∧b ∧q ∧x
h2 ←a ∧b ∧q ∧y
h3 ←a ∧b ∧r ∧x
h4 ←a ∧b ∧r ∧y
h = h1 ∨h2 ∨h3 ∨h4
Figure 4.3: An example of learning a hypothesis using independent dimensions
candidate clauses containing these literals, the background knowledge is reduced to the
subset required for each independent learning problem.
For example, consider the following background predicate:
near_site(X).
This predicate is concerned with the group to which a value contained within type, subject
or location belongs; it is not needed when considering possible values of task duration.
Other predicates may only be concerned with the duration of a single task and hence
would not be needed when the only variance between positive and negative examples is a
value contained within type or subject.
Having collected the separate sets of clauses as the results of each learning task, these
clauses must be combined to create an overall hypothesis as mentioned earlier. The actual
implementation of the overall hypothesis construction does not deal with unfolding in the
manner mentioned earlier, but uses the more straightforward approach of dealing with
each predicate deﬁnition as a set of clauses and attempting to generate a cross-product of
the sets, which is then pruned.
Each set contains two types of clause: ground clauses with no conditional clause body,
and clauses where all the attributes in the head are still variables.
The combination
process begins by taking each non-ground clause in turn from the ﬁrst set of results and
attempting to merge it with each clause of that type from the second set of results by
unifying the heads of both clauses and then adding the literals from the bodies of both
clauses together, removing any duplicates. If any clause from either set is ground (i.e.
an example that Aleph could not cover with the set of candidate clauses deemed to best
ﬁt the data and so was returned as an additional clause) then it is included in the set of

CHAPTER 4. MULTIPLE VALUE PREDICTIONS
87
results as an additional clause rather than attempting to merge it with the non-ground
rules from the other set. This process is repeated with the resulting set of clauses and
the next set of collected results, and this continues until all the sets of results have been
processed. The set is then ﬁltered to remove redundant clauses (i.e. clauses subsumed by
other clauses within the set), clauses that do not cover any positive examples, and any
duplicate ground clauses that have been included. Clauses that do not cover any positive
examples may occur when the subconcept being examined requires more than one clause
for a complete theory. On these occasions some of the subproblems return more than one
clause in their results set, which in turn will produce diﬀerent combinations of literals in
the clauses being constructed. Some of these combinations of literals will exclude all the
positive examples of the cluster currently being worked with. An example of this can be
seen in the worked example in Section 4.3. Clauses with a low level of positive example
coverage are not discarded because each cluster of examples is likely to be quite small,
due to the size of the initial data set, and the theories contained within the model should
cover as much of the instance space containing the cluster as possible.
Once the combination and ﬁltering of results is complete, the ﬁnal set of results will be a
set of clauses that contain literals from each of the learning processes, or are ground, and
enables the model to make predictions of user actions in the required level of detail.
4.2.1
Working with Contradictory Example Sets
There may be occasions where the values of some arguments currently under examination
may not all be close enough to each other to enable Aleph to produce a comprehensive
theory without covering some unwanted areas of instance space. For example, it is possible
for the concept to contain examples where the duration of a ‘travel’ task may be either
long or short, but with no values in-between. On these occasions there may be a gap in
the range of possible values that the rules produced should not cover. The simplest way
to ensure that Aleph excludes these values from the rule being constructed is to include
some of these values in the negative examples being generated. However this introduces
the potential for negative examples that contradict other positive examples to be included
within the data set. For the model to be produced this is most likely to occur for the
duration of the tasks predicted.

CHAPTER 4. MULTIPLE VALUE PREDICTIONS
88
+ve value 2
0/00
0/30
1/00
1/30
2/00
2/30
3/00
3/30
4/00
4/30
5/00
5/30
6/00
−ves for +ve 1
−ves for +ve 2
−ves for +ve 2
−ves for
+ve 1 & 2
+ve value 1
Figure 4.4: Duration values from 2 positive examples and their respective negative values
Continuing with the example of task duration, it is possible to present Aleph with:
1. A set of positive examples where some tasks contain a short duration, and some
contain a longer duration,
2. A set of negative examples where only the duration has been altered, and
3. A set of background knowledge predicates to use that includes the ability to specify
that a duration is longer or shorter than a speciﬁc length of time, but does not
include any other abilities.
The negative examples generated from the examples with a shorter duration may con-
tradict the positive examples that have a longer duration, and the negative examples
generated from the set of examples with a longer duration may contradict the positive
examples with the shorter duration. If only negative examples that are outside the range
of all the given positives are used then the rules would be allowed to produce answers that
are already known to be incorrect. Figure 4.4 shows two durations taken from two pos-
itive examples and the possible values that could be used to generate negative examples
for those examples.
The contradiction of positive examples by artiﬁcial negative examples can be easily de-
tected during the phase in which the negative examples are generated. At this point a
further level of partitioning of the data can be produced by dividing it into sets of pos-
itive examples and their respective close negatives where the negative examples present
do not contradict the other positives. Figure 4.5 shows the modiﬁed method for the gen-
eration of non-contradictory sets of data to give to Aleph. These subsets can then be
presented separately to Aleph and a union of the sets of results from each subset created
to form a set of rules for that attribute. Any literals that could have been used by Aleph
to split the hypothesis being learnt into separate rules (e.g. in the example, near site
and not near site could potentially have been used) will emerge when the sub-problems
containing them as background knowledge are presented to Aleph.
This learning situation is one where the assumption of independence that is required for
each separate learning problem has not been achieved. However the additional knowledge

CHAPTER 4. MULTIPLE VALUE PREDICTIONS
89
c is the cluster threshold distance
m is the number of -ve examples to be created per +ve example
D = {type1,subject1,location1,duration1,type2,subject2,location2,duration2}
i ∈D
\\ The current dimension the modelling system is working with.
P is the initial set of +ve examples
E is the set of all examples that can be generated using the current background knowledge
S = {s| s is a set of non-contradicting +ves & -ves} = ∅
For each p ∈P
{
Np = {n|n ∈E ∧(δi(n, p) > 2c) ∧(∀d ∈(D \ {i}) : δd(n, p) = 0)}
If (i ∈{duration1, duration2})
Rp = {m closest members of Np to p}
else {
Rp ⊆Np where card(Rp) = m and
all members of Rp are randomly selected from Np
}
If (∃s ∈S : ∀q ∈s, ∀r ∈Rp : pos example(q) ⇒δi(q, r) > 2c)
s1 = s ∪Rp ∪{p}
S = (S \ {s}) ∪{s1}
\\ add Rp and p to an existing set within S.
else
S = S∪{Rp ∪{p}}
\\ create an additional set within S.
}
\\ δx(p, q) = distance between tasks p and q in dimension x.
Return S
Figure 4.5: Method for the generation of non-contradictory sets of negative examples
that splitting the problem still further will allow that assumption to be made enables the
situation to be resolved. The example given used an attribute that took an ordered range
of values since this was the most likely situation to be encountered, but this could occur
for any set of values.
It may not always be the case that there are predicates within the excluded background
knowledge that could have been used to partition the set of examples into non-contradictory
subsets. However for this particular situation, it does not matter whether this is the case
or not. The results produced by the learner will still contain the required specialisations
for the dimension where the gap occurred in the range of possible values, and these will
be transferred to the ﬁnished rules when the results for each dimension are combined.

CHAPTER 4. MULTIPLE VALUE PREDICTIONS
90
4.3
A Worked Example
Consider the positive examples:
sequence(task(type(travel),subject(york),location(n/a),duration(4/0),
preferred_time(8-00),preferred_day(wednesday)),
task(type(visit),subject(york_uni),location(104),duration(4/0),
preferred_time(13-00),preferred_day(wednesday))).
sequence(task(type(travel),subject(colchester),location(n/a),duration(1/0),
preferred_time(13-00),preferred_day(friday)),
task(type(visit),subject(motorola_cto),location(104),duration(3/0),
preferred_time(14-00),preferred_day(friday))).
These examples are part of a set describing the concept of travelling to the place in which
a company is located before visiting the company.
What background knowledge would be appropriate when attempting to construct a theory
that reﬂects the relation between the two tasks in sequence? Some of the more obvious
predicates are those that would describe the individual attributes in a more general man-
ner, for instance, saying that something is a person or a company. Others could be used
to limit the range of values that attributes such as task duration could take. However
the predicates described so far do not make use of the advantages that use of ILP oﬀers.
They do not describe relations between the two tasks because they do not refer to more
than one attribute, they merely attempt to describe the attribute that they are applied
to. Since the tasks have been indicated as part of a sequence, there should be at least
one item of information that reﬂects a relation. In the case of the examples given, the
subjects of the tasks in sequence are related because one is a company and the other is the
place in which that company is located. Other relations, such as particular people being
associated with certain projects or working for a particular company, can also be reﬂected
in the background knowledge made available. In addition, both tasks having an identical
value (whether it is for the same attribute or diﬀerent attributes) can also be reﬂected in
the rules constructed. This facility does not need to be included within the background
knowledge given; Aleph will ﬁnd these relations automatically.
It is important to ensure that the rules contained within the model constructed are un-
derstandable by the user and to preserve the ﬂexibility of the background knowledge that
is presented. The predicates described thus far reﬂect the knowledge that is the most
readily available and most easily understood. They reﬂect the most obvious reasons that
a human would use when determining what task should follow or precede another. Other
simple relations, such as relations between people or between companies, could also be in-
cluded. More complicated predicates could also be included, provided that they preserve
the understandability and ﬂexibility of the model. There is the possibility of expanding

CHAPTER 4. MULTIPLE VALUE PREDICTIONS
91
the background knowledge to include these relations by attempting to gather the infor-
mation available from various sources such as company directories, but this information is
not essential for this initial investigation.
The following background knowledge is available for Aleph to use:
(+ttype)=(#ttype)
person(+ttype)
place(+ttype)
admin(+ttype)
// +ttype can refer to the values within
with_people(+ttype)
// type, subject or location of either
desk_work(+ttype)
// task in the sequence.
project(+ttype)
other_company(+ttype)
not_in(+ttype)
off_site(+ttype)
near_site(+ttype)
not_near_site(+ttype)
located_at(+ttype,+ttype)
on_project(+ttype,+ttype)
may_require(+ttype,+ttype)
works_for(+ttype,+ttype)
longerthan(+duration,#duration)
shorterthan(+duration,#duration)
This information is taken from an Aleph input ﬁle and hence the ‘+’s refer to variables
and the ‘#’s refer to constants.
The predicates can be split into the following groups:
• type/subject/location of each task
• duration of the ﬁrst task
• duration of the second task
Further consideration of the two positive examples given earlier shows that if the duration
of the ﬁrst task for each example is the only attribute focused on, then any negative
examples produced by altering that attribute will contradict the other example. To avoid
this situation, each example and its corresponding negative examples must be placed into
a separate set. Any other positive examples (not shown) that are similar to either of these
examples in terms of that particular attribute, and the negative examples that have been

CHAPTER 4. MULTIPLE VALUE PREDICTIONS
92
generated from them, would then be added to the set containing the example that they
were closest to.
If Aleph is presented with:
1. a set of examples containing the ﬁrst positive example shown earlier,
2. any positive examples contained within the data set that are similar to it in terms
of duration,
3. all the negative examples that have been generated from these positive examples,
and
4. the background predicates longerthan/2 and shorterthan/2 which refer to the
duration of the ﬁrst task within the sequence
then it will return the result:
Rule 1
sequence(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),duration(J),
preferred_time(K),preferred_day(L))):-
longerthan(D,3/0),
shorterthan(D,5/30).
An initial glance at this result would present the appearance that Aleph is not working with
relations between variables in the rules being constructed, merely working with constants.
However, as can be seen from other results, this is not the case.
Some of the literals
presented as possible background knowledge deal with constants so that Aleph can attempt
to restrict the range of values that the attributes that take real values can produce, however
others (for instance located at/2) are concerned with the relationships between diﬀerent
task attributes.
If Aleph is presented with:
1. a set of examples containing the second positive example shown earlier,
2. any positive examples contained within the data set that are similar to it in terms
of duration,
3. all the negative examples that have been generated from these positive examples,
and
4. the background predicates longerthan/2 and shorterthan/2 which refer to the
duration of the ﬁrst task within the sequence

CHAPTER 4. MULTIPLE VALUE PREDICTIONS
93
then it will also return the result:
Rule 2
sequence(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),duration(J),
preferred_time(K),preferred_day(L))):-
shorterthan(D,2/30).
The learning problem containing the values for subject of the ﬁrst task of each example
is also split into two separate learning problems because the values present within the
examples given are not members of all the same groups. colchester is listed as part of
the group near site, whereas york is listed as part of the group not near site. For each
example, substitutions of values from the opposite group to the group that the original
value is a member of can be made.
The result for the ﬁrst example and any positive examples that are similar with regards
to the value for subject of the ﬁrst task is:
Rule 3
sequence(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),duration(J),
preferred_time(K),preferred_day(L))):-
not_near_site(B).
The result for the second example and any positive examples that are similar with regards
to the value for subject of the ﬁrst task is:
Rule 4
sequence(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),duration(J),
preferred_time(K),preferred_day(L))):-
near_site(B).

CHAPTER 4. MULTIPLE VALUE PREDICTIONS
94
Combining the results so far produces the rules:
Combining rules 1 and 3
sequence(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),duration(J),
preferred_time(K),preferred_day(L))):-
longerthan(D,3/0),
shorterthan(D,5/30),
not_near_site(B).
Combining rules 2 and 3
sequence(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),duration(J),
preferred_time(K),preferred_day(L))):-
shorterthan(D,2/30),
not_near_site(B).
Combining rules 1 and 4
sequence(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),duration(J),
preferred_time(K),preferred_day(L))):-
longerthan(D,3/0),
shorterthan(D,5/30),
near_site(B).
Combining rules 2 and 4
sequence(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),duration(J),
preferred_time(K),preferred_day(L))):-
shorterthan(D,2/30),
near_site(B).

CHAPTER 4. MULTIPLE VALUE PREDICTIONS
95
Removing the rules that do not cover any positive examples leaves:
sequence(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),duration(J),
preferred_time(K),preferred_day(L))):-
shorterthan(D,2/30),
near_site(B).
sequence(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),duration(J),
preferred_time(K),preferred_day(L))):-
longerthan(D,3/0),
shorterthan(D,5/30),
not_near_site(B).
The results collected would be combined with the rules learnt for the other attributes of
the examples presented. However these rules show how the method works to combine the
discovered clauses and produce a model of all the attributes within the concept.
After collecting, combining, and ﬁltering the results for each dimension of the theory, the
ﬁnal rules covering the original positive examples and any examples like them would be:
sequence(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),duration(J),
preferred_time(K),preferred_day(L))):-
A = travel,
not_near_site(B),
C=n/a,
longerthan(D,3/0),
shorterthan(D,5/30),
G = visit,
located_at(B,H),
room(I),
longerthan(J,2/0),
shorterthan(J,5/30).

CHAPTER 4. MULTIPLE VALUE PREDICTIONS
96
sequence(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),duration(J),
preferred_time(K),preferred_day(L))):-
A = travel,
near_site(B),
C=n/a,
shorterthan(D,2/30),
G = visit,
located_at(B,H),
room(I),
longerthan(J,1/30),
shorterthan(J,4/30).
4.4
Evaluation and Results
Having created this approach to constructing theories capable of making the required
predictions, demonstration of the beneﬁts that it gives to Aleph is required.
The three approaches compared were:
1. Full concept and background knowledge — all the positive examples and their gen-
erated negative examples, plus all available background knowledge.
2. Split concept, full background knowledge — the concept was presented for learning
one dimension at a time as described earlier. All available background knowledge
was still provided for each learning problem, hence the further level of partitioning
contradicting positive and negative examples described earlier was not included.
3. Split concept, split background knowledge — the concept was presented for learning
one dimension at a time with only the relevant background knowledge available.
The second approach was tested to show the eﬀect of only splitting the learning problem
into separate dimensions rather than splitting both the learning problem and the available
background knowledge. The results produced, and the time taken to complete the learning
problem using this approach, could then be compared to the third approach. This would
demonstrate the additional beneﬁts produced by only giving relevant background knowl-
edge to the learner and partitioning the data still further. The additional partitioning
of the data set was used to avoid contradictions between positive examples and gener-
ated negative examples when Aleph could no longer use seemingly unrelated background
predicates to do this task.

CHAPTER 4. MULTIPLE VALUE PREDICTIONS
97
Three sets of training data, each describing a speciﬁc concept, were used for each of the
three approaches:
1. Examples describing the concept of travelling to a company before visiting it,
2. Examples describing the necessity of preparation before a personnel meeting, and
3. Examples describing the necessity of preparation before a project presentation or
meeting.
Table 4.1 shows the results of all three approaches on the test data. Each method was
run 5 times on each set and the results averaged to produce the times listed. The resource
limit was set at 10,000 nodes (i.e. 10,000 clauses could be considered before Aleph gave
up the search for a clause to cover a seed example), and the number of negative examples
to be generated per positive example was set to 5.
The results of the comparison show that splitting both the concept to be learnt and the
background knowledge presented produced the most improvement in Aleph’s performance.
The time taken to produce the results was cut to a third for the two more complicated
concepts, and halved for the simpler concept, and Aleph was able to reliably produce the
required theory.
Simply splitting the concept to be learnt into separate subconcepts enabled Aleph to
produce results without exceeding its given resources, but it did not produce as much of
an improvement. Theoretically, providing redundant background knowledge should have
very little eﬀect on performance. The additional background predicates included within
the bottom clause constructed should be evaluated, deemed of little use and not included
in the constructed theory. However, some of the predicates were able to produce what
appeared to be good candidate clauses to build on because they specialised the clause so
much that a large amount of the negative examples (and some of the positive examples
Approach
Test
Number of
Time Taken
Resource
Produced
Set
clauses
(seconds)
Limit
Required
Initial
Final
Ground
Exceeded?
Theory?
Split concept
1
16
2
0
23.90
No
Yes
and background
2
7
1
0
13.55
No
Yes
knowledge
3
8
1
0
13.64
No
Yes
Split concept,
1
16
2
0
54.07
No
Yes
full background
2
7
1
0
26.95
No
Nearly
knowledge
3
8
1
0
31.50
No
Nearly
Full concept,
1
16
7
2
71.05
Yes
No
full background
2
7
2
0
28.54
Yes
Nearly
knowledge
3
8
4
0
54.61
Yes
No
Table 4.1: Comparison of possible approaches to learning a user model using Aleph

CHAPTER 4. MULTIPLE VALUE PREDICTIONS
98
as well) were no longer covered. For example, binding a variable to a ground value would
automatically reduce the clause’s cover set to a fraction of its previous size, and because
each positive example had several negative examples generated from it, the specialisation
would exclude a greater number of negative examples that positive ones. Aleph spent time
searching amongst literals of this nature before ﬁnally constructing the answer that was
expected.
It could be argued that allowing Aleph access to all the background knowledge would
enable it to construct better theories for each subproblem (e.g. for the problem mentioned
earlier about two diﬀerent task durations in the same cluster), but Aleph did not manage
to produce the theory required. A brief experiment was carried out in which a data set
that deﬁnitely contained conﬂicting positive and negative examples for the duration of a
task, such as the examples used in the worked example earlier, was presented to Aleph.
All of the positive and negative examples were presented as a single data set with no
partitioning to avoid conﬂicts, and the entire set of background knowledge was also made
available for Aleph to use, in the same manner as the second of the three approaches
described earlier. Predicates existed within the background knowledge that could be used
to resolve the conﬂict, and Aleph should have been able to use some of these to enable it
to construct an appropriate theory. However, the complexity of the theory, combined with
the amount of background knowledge to be examined, increased the size of the search space
to be explored beyond Aleph’s resource limits once again, and the required answer was not
produced. This experiment and the observations in Table 4.1 show that refraining from
splitting the background knowledge as well as the concept does not provide any beneﬁts
to Aleph for theory construction and slows down the learning process.
Why did some experiments return ‘nearly’ for whether they produced the required the-
ory? In these cases, the required theory was almost produced, but one specialisation was
omitted. This omitted specialisation speciﬁed the group to which values that were allowed
within a particular variable must belong. In the case of the full concept experiment, one
clause in the results subsumed the other and so the theory would have produced the same
predictions as the theory produced for a split concept and full background knowledge.
For the split concept and split background knowledge approach, when dealing with the
type/subject/location of each task, Aleph was not allowed to unify variables in the head of
the clause that it was constructing because it was required to use the background knowl-
edge which had been provided. Uniﬁcation, whilst a useful tool for most learning tasks,
would produce rules that covered too great an area of the example space (this is discussed
in more detail later in Chapter 8). This restriction ensured that each variable had either a
ground value, a given relation, or a type given to it. It is not possible to be claim that no
given relation may have the same drawbacks as the built-in ‘=’ relation. The responsibility
for avoiding this situation must remain in the hands of the designer of each system. If
such a relation is accidentally included in the background knowledge then its presence will
soon become evident when the ﬁrst set of models are produced.

CHAPTER 4. MULTIPLE VALUE PREDICTIONS
99
Approach
Size of
Size of Search Space
No. of Clauses
⊥
(Depth limit = 8)
Considered by Aleph
(breadth-ﬁrst search)
Full Concept
30
7.27 × 1064
10,000
Type (task1)
2
3
2
Subject (task1)
5
3286
2
Location (task1)
2
3
2
Type (task2)
2
3
2
Subject (task2)
3
16
2
Location (task2)
2
3
2
Duration (task1)
6
9.58 × 104
4
Duration (task2)
8
2.19 × 108
24
Table 4.2: Comparison of Search Space Sizes when Splitting the Concept
When dealing with the duration of each task, Aleph was not prohibited from unifying the
type/subject/location variables because it would do this as an additional specialisation.
Uniﬁcation of variables in the head of the clause that are not part of the dimension being
examined would only occur when this operation has no eﬀect on the cover set of the clause
because it does not exclude any negative examples. Aleph still had to use the background
knowledge provided in order to exclude the negative examples given. The eﬀect of noise
on this method of theory construction is encountered and discussed in Chapter 7.
Table 4.2 shows the results of a comparison of search space sizes for a particular example
(the example given at the beginning of this chapter) before and after splitting. Whilst the
size of some of the search spaces do not appear to have been reduced to below the limit
that was given to Aleph for the number of possible clauses that can be examined, Aleph’s
search and pruning algorithms were able to determine that it had explored enough of each
reduced search space to ﬁnd the optimal hypothesis. An optimal hypothesis in this case is
taken to mean a clause that covers all of the positive examples within the data set without
covering any of the negative examples.
Approach
Aleph
TILDE
breadth-ﬁrst
heuristic
depth-ﬁrst
Full Concept
71.05s
149.21s
112.02s
76.73s
Split Concept,
54.07s
39.91s
49.90s
64.31s
Full background knowledge
Split Concept,
23.90s
26.00s
26.02s
50.11s
Split background knowledge
Table 4.3: Timing Comparison for Aleph and TILDE

CHAPTER 4. MULTIPLE VALUE PREDICTIONS
100
Aleph was compared to TILDE to see whether an ILP engine that did not explore the
search space in the same way could beneﬁt from the new approach. A timing comparison
was made using the data set of the ﬁrst of the three concepts encountered earlier, travelling
to a company before visiting it, averaged over 5 runs. Diﬀerent search methods were used
for Aleph to see if they produced any interesting results.
The results of the comparisons can be seen in Table 4.3. The ‘heuristic search’ performed
by Aleph was a best-ﬁrst search with the clausal evaluation function set to ‘Coverage’
as before. There were occasions when Aleph appeared to be able to produce the right
results without the need to split up the background knowledge being presented. However,
it was noted that on these occasions Aleph still exceeded its given resource limit (i.e. it
did not manage to conclusively explore the entire search space), and so, whilst it obtained
the results on this occasion, success cannot be guaranteed as it is dependent on the seed
examples chosen from the data set.
TILDE produced slightly more startling results, it was able to produce a version of the
required concept without the need to split the concept into subproblems, but took nearly
twice as long to do so than Aleph using Dilum. The implications of this result will be
discussed in the next section.
The representation of concepts returned by TILDE relies on the clauses discovered being
interpreted in the given order without any other clauses with the same head being present.
This means that if the given representation was used without alteration then the user model
could not be generated as a union of the theories created for each cluster. However, the
results can easily be translated into a form that does not rely on this ordering by adding to
the body of each rule the negated form of the bodies of the rules above it in the returned
concept. Figure 4.6 shows an example of the translation of results from TILDE into a
stand-alone theory. This new representation still presents possible problems for using the
model for prediction as negated conjuncts do not work well if a rule is called with half
of the variables in the head still free. If each of the negated rule bodies contain only one
literal and that literal has an exact opposite (e.g. > and ≤), then the constructed rules will
function in the manner required. If this is not the case then there will exist rules within
the ﬁnal theory that cover a much larger area of instance space than they should. In the
example in Figure 4.6, a non-negated translation of a literal such as \+(person(B)) would
be required to cover all possible values of B that would cause person(B) to fail. Technically
the size of the set of values covered by such a literal is inﬁnite, however, for prediction
purposes, only those values that are covered by other predicates such as location/1 or
place/1 would be expected to be returned. If all the answer were collected from a rule
containing this literal with B still a variable then the number of values that could be
returned would still be very large and most of them would not be correct.
Using the full Dilum method with TILDE would produce simpler sets of results but would
not prevent this problem in all cases. The result returned will still contain a list of clauses

CHAPTER 4. MULTIPLE VALUE PREDICTIONS
101
sequence(A,B,C,D,E,F,G,H,I,J,K,L,neg):- longerthan(D,7/0).
sequence(A,B,C,D,E,F,G,H,I,J,K,L,neg):- person(B).
sequence(A,B,C,D,E,F,G,H,I,J,K,L,neg):- project(I).
sequence(A,B,C,D,E,F,G,H,I,J,K,L,neg):- equal(C,I).
...etc...
sequence(A,B,C,D,E,F,G,H,I,J,K,L,pos):- located_at(B,H).
sequence(task(type(A),subject(B),location(C),
duration(D),preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),
duration(J),preferred_time(K),preferred_day(L))):-
shorterthaneq(D,7/0),
\+ (person(B)),
\+ (project(I)),
\+ (equal(C,I)),
...etc...
located_at(B,H).
Figure 4.6: Translation from TILDE results to standalone rules
with two possible classiﬁcations (positive and negative) that must be processed.
The
production of a set of results where the clauses with positive classiﬁcation were listed ﬁrst
would make the translation more successful. However, as the tree structure produced by
TILDE will contain the required ‘positive’ classiﬁcations within the heart of the results
due to the length of clause required, it would not be possible to generate such a set.
4.5
Conclusions
The results presented in the previous section demonstrated that, by using Dilum, Aleph
was able to reliably construct a model containing enough specialisations to be able to make
the multiple value predictions required.
The ideas within Dilum should be of beneﬁt to any exploratory ILP engine faced with a
learning problem such as the one presented by this user model’s requirements. The exact
nature of the sorts of concepts that could be learned using Dilum is explored further in
Chapter 5. Would WARMR be able to use Dilum to produce better results than those
achieved in Chapter 3? As the size of each search space is much smaller than the space
that WARMR was asked to work with then it is much more likely that the results produced
by WARMR would include the specialisations required. It is possible to use types within

CHAPTER 4. MULTIPLE VALUE PREDICTIONS
102
WARMR’s speciﬁed key literal in order to direct it to only use the given background
knowledge on particular attributes. If the example used to generate the values in Table
4.2 was given to WARMR to use as a database, then (if the depth limit was restricted to
8 literals) the size of the sets of queries evaluated will be less than the size of the search
spaces shown in that table (due to queries that fail the frequency requirements being
pruned). This means that although the problems encountered when attempting to ﬁnd
appropriate specialisations for the duration of both tasks would still have to be dealt with,
and the possible literals returned would have to be evaluated out in order to decide which
was the most appropriate, the actual results produced by WARMR would be improved.
TILDE did not require all the methods within Dilum to achieve successful results; however
it uses a greedy rather than an exploratory approach when constructing a theory. The
approach taken by TILDE is that of building a logical decision tree, at each step considering
which would be the next most appropriate literal to add. Once the tree has begun, TILDE
is unable to consider other trees with diﬀerent roots (i.e. it makes use of a greedy search
method), hence the search space is cut down considerably with each step. However, TILDE
was unable to produce the required model without help. TILDE needed the artiﬁcially
generated negative examples to guide it to the required theory because it used a top-
down approach and also because it was not suited to positive-only learning. TILDE also
needed the data set to be partitioned into separate clusters and each cluster presented
individually. When all the data was presented as a single set TILDE produced a very
large incomprehensible theory that did not contain the literals expected.
Comparison of the results achieved and the requirements of the three learning algorithms
discussed leads to three conclusions:
1. If multiple-value predictions are required for a user model, then some form of stimuli
such as negative examples would be required to force a top-down generalising ILP
algorithm to produce enough specialisation within the model it constructs.
2. There is strong evidence from the top-down ILP algorithms that have been investi-
gated that, in general, a top-down ILP approach attempting to eﬃciently produce
a theory (with a high level of recall) that described a complex, vague learning prob-
lem would require the data to be partitioned into separate groups to be dealt with
individually. This will be investigated in greater detail later in Chapter 6.
3. If the ILP algorithm used to produce the model described in the conclusion above
makes use of an exploratory approach to the search space, then the size of the
space of possible hypotheses is likely to be too large to explore if a ‘reasonable’
time limit is given. If the concept can be broken down into independent dimensions
and the background knowledge given to each dimension reduced to those predicates
that are relevant to that dimension, then each dimension will have a reduced space
of hypotheses to be explored. The learner can then be given each of these separate

CHAPTER 4. MULTIPLE VALUE PREDICTIONS
103
problems individually, and the results combined to produce the required theory. This
will signiﬁcantly reduce the amount of computation time required by the learner to
produce the model. The negative examples mentioned above to force the learner to
specialise can then be used to fulﬁll an additional role of directing the learner towards
only exploring certain parts of the search space whilst constructing a theory.
Earlier in the chapter, the concern was raised that using Dilum may not allow the pro-
duction of the solution that best explains the data (referred to here as a optimal solution)
for each learning problem. The method of specialising in separate dimensions removes
the ability to compare results generated by the interaction of diﬀerent literals added to a
possible clause. In order to reduce the complexity of the learning problem, some areas of
the space of possible hypotheses are never examined. However, for the concepts that will
be represented by the groups of examples collected from the user, specialisation for each
dimension that the learning problem is split into is deﬁnitely required in order to enable
the model to make predictions. With complete independence between dimensions, the
production of an optimal solution can be guaranteed. For other concepts where there may
be less than complete independence, whilst an optimal solution cannot be guaranteed, a
solution describing the concept demonstrated by the available examples with a high level
of recall (if one exists within the remaining hypothesis space) will be produced. This issue
will be examined in greater detail in Chapter 5, after further exploration of the properties
displayed by Dilum.
One ﬁnal question to be answered is concerned with the possibility of using positive only
learning instead of generating negative examples. When learning in a positive only man-
ner, Aleph estimates the probability distribution of the example space by constructing
a Stochastic Logic Program (SLP) [Muggleton, 1995c] from the background knowledge
given. Aleph then uses this SLP to randomly generate its own ‘negative’ examples. If the
background knowledge provided only refers to a few of the attributes in each example then
these will be the only values that are altered when the negative examples are generated.
This is very similar to the implementation of the method used within Dilum for creating
‘near misses’. The argument used earlier against the use of positive only learning was
that the selection of attributes to be altered could not be controlled and so there could be
negative examples generated that could not be excluded using specialisation belonging to
the dimension currently under examination. However, as the background knowledge has
been split into separate sub-sets that only refer to the current dimension, Aleph should not
generate the negative examples that would cause problems when it is presented with all
the available background knowledge. A brief experiment conﬁrmed this theory, but also
showed that the random element used in Aleph’s method of generating negative examples
meant that control over the negative examples that were produced was lost, and so the
production of a comfortable margin of empty example space around the original examples
in order to reduce the possibility of overﬁtting could not guaranteed. This was particu-
larly a problem for dimensions involving attributes containing values that had a range;

CHAPTER 4. MULTIPLE VALUE PREDICTIONS
104
the theories created each time consistently overﬁtted the data. The conclusion from this
is that, for the autonomous production of a user model, it is better to create the negative
examples outside Aleph as this allows greater inﬂuence over the theories produced. There
may be other concepts for which it is suﬃcient to approach each subproblem in a positive
only manner, providing the learner is capable of focusing on the problem with which it
has been presented and does not waste signiﬁcant resources exploring other areas of the
search space. This is discussed in more detail later in Section 5.4.
4.6
Related Work
The idea of using a divide-and-conquer approach has been attempted before with regards
to ILP in the SYNAPSE system [Popelinsk´y et al., 1994], however on this occasion it
was embedded within the ILP process and used to break down the examples into simpler
structures so that the provided background knowledge could be used and any recursive
tendencies were highlighted. For Dilum, the approach is used outside the ILP process to
break down the concept itself and present the learner with a series of simpler concepts
that it is able to learn rather than a complex concept that it is unable to describe fully.
Cussens, Dˇzeroksi & Erjavec [1999] also make use of the idea of splitting a learning problem
into subproblems by dividing the training data into separate sets depending upon the
outcome of a chosen background predicate (i.e. whether the predicate would return true
of false for each example). Knowledge of the outcome for this predicate for each example
is incorporated as additional background knowledge. The solution for each training set
would then have this additional knowledge included to form the full hypothesis.
Dehaspe and De Raedt [1995] investigated Parallel ILP in the nonmonotonic setting, and
showed how partitioning the ILP-task into separate learning problems enabled them to
produce a speed-up in the rate of learning that was linear in the number of processors
used. They state that a partition of an ILP-task is only valid if the union of the results
of the subproblem is equivalent to the solution that would be obtained from using the
whole ILP-task as a learning problem.
This constraint is satisﬁed by the partitioning
stage used within Dilum, although the increase in learning eﬃciency achieved there is also
due to other reasons (which will described in more detail later in Chapter 6, and also by
splitting the learning problem, since, if the time limit did not exist, then the learner would
eventually produce the required hypothesis. However, since the experiments detailed in
the paper were carried out in the nonmonotonic setting, partitioning the language from
which the hypotheses were constructed did not exclude any areas of the space of possible
hypotheses from being explored during the learning process. The aim of the research was
more to increase the number of clauses evaluated within a given time frame, rather than
increase the eﬃciency with which the search space was explored.
Dehaspe and De Raedt [1995] also considered the partitioning of an ILP-task within a

CHAPTER 4. MULTIPLE VALUE PREDICTIONS
105
normal ILP setting, but its general use was argued against due to requiring a restriction
to single, non-recursive, predicate learning in order to satisfy the above constraint. For
a user model such as the one constructed within this chapter, this is precisely the sort
of concept that is required to be learnt, hence the characteristics of this learning context
allow this technique to be applied.
4.7
Summary
This chapter has shown that, having been provided with the necessary direction to produce
a theory specialised enough for the user model, an exploratory ILP engine such as Aleph
is unable to explore enough of the search space to ﬁnd the required theory within the
time limit imposed. It has been demonstrated that if an assumption of independence can
be made between various dimensions of the problem to be learnt, characterised here by
diﬀerent attributes of the model that require specialisation, the learner can be presented
with a series of smaller learning problems that it can more easily solve. Having collected
the results of these smaller learning problems, these can then be recombined to form the
required theory.
Combining the ideas from this chapter and the previous chapter produced the method
Dilum, a system that will signiﬁcantly reduce the amount of computation time required to
construct a user model from the data collected from the user’s diary. This chapter has also
shown that greedy top-down generalising ILP algorithms (such as TILDE) do not require
assistance to explore the search space but still beneﬁt from being presented with the data
as separate clusters, each representing a diﬀerent subconcept.
The next chapter will expand upon the properties of splitting up the learning problem
by examining what happens when the concepts to be learnt do not require the ability
to make such complex predictions and hence specialisation for each task attribute is not
necessarily needed. The amount of relevant background knowledge that could be included
in the construction of each theory is just as large as before, but the size of the target clause
is unknown, and so the maximum clause length allowed cannot be reduced. Therefore the
size of the search space, and hence the amount of search space that the learner may need
to explore depending on the length of the target clause, is also just as large as before.
Comparison of the learning problems attempted within both chapters will be used to
identify the properties that a concept should display for use of Dilum to be considered.

Chapter 5
Characteristics of the Concept
Space
The previous chapter introduced a method by which an exploratory ILP learner could
generate a theory when the search space of possible hypotheses became too large to be
conclusively evaluated using an ordinary search algorithm within a limited time frame.
Within this chapter the characteristics of learning problems to which this method can be
applied are determined by attempting to build a diﬀerent user model and then comparing
the characteristics of both problems attempted so far to form an overall picture.
5.1
Building a Model of Personalised Distance Measure-
ment
Chapter 3 determined that ideally the partitioning of data and generation of negative
examples would use a distance measure that reﬂected the user’s own decision-making
process for this task. However, it also showed that in the real world it was not possible to
collect a suﬃcient amount of data from which to build a model that could be consulted
with regards to distance measures between tasks. This chapter uses a formalised version
of the part of this learning problem concerned with the diﬀerence rating between two tasks
and uses it as the focus of the experiment. The generation of the data set representing the
concepts to be learnt within the experiment was based on the initial responses collected
from the user during the earlier experiment. Since the goal of attempting this new learning
problem is to compare the methods required with those required in the previous chapter,
Aleph was the sole learning algorithm worked with during the creation of the new modelling
system.
In order to ascertain Dilum’s performance on concepts that were not as complicated as
those attempted in Chapter 4, a mixture of simple and complex concepts was required.
106

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
107
Unfortunately, the four concepts captured previously could each be described using a
relatively simple concept description. To ensure that at least one concept was complicated
enough to require the aid given by Dilum two new ideas were introduced for use within
the concept deﬁnitions: the ﬁrst idea was that two tasks were diﬀerent if one was within
oﬃce hours but the other was not, the second idea was that tasks were diﬀerent if one
task was over an hour longer than the other. The concept similar was split into similar
and same based on these ideas, and the concept identical also had details of times and
durations included. The concepts related and different were left unaltered to provide
the simpler concepts to be learnt. These alterations gave the following descriptions for the
ﬁve concepts that the model produced should capture:
Identical — both tasks have the same values for type and subject, almost the same
duration (within one hour of each other), and are either both within oﬃce hours or
outside that time period. e.g.
diff_rate(task(type(travel),subject(london),location(n/a),
duration(2/0),preferred_time(7-0),preferred_day(monday)),
task(type(travel),subject(london),location(n/a),
duration(2/30),preferred_time(7-30),preferred_day(friday)),
drate(identical)).
Same — both tasks have the same type, almost the same duration (within one hour of
each other), and are either both within oﬃce hours or outside that time period. The
subject of both tasks is not identical, but is of the same group (e.g. both company
names). e.g.
diff_rate(task(type(travel),subject(london),location(n/a),
duration(2/0),preferred_time(7-0),preferred_day(monday)),
task(type(travel),subject(colchester),location(n/a),
duration(1/0),preferred_time(7-30),preferred_day(friday)),
drate(same)).
Similar — both tasks have the same type and similar subject values (i.e. not identical
but of the same group). Either the tasks have durations that diﬀer by more than an
hour, or one task is inside oﬃce hours while the other is not. e.g.
diff_rate(task(type(travel),subject(london),location(n/a),
duration(2/0),preferred_time(7-0),preferred_day(monday)),
task(type(travel),subject(colchester),location(n/a),
duration(1/0),preferred_time(10-30),preferred_day(friday)),
drate(similar)).

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
108
Related — the tasks do not have the same type, but are considered by the user to be
related because they may occur in sequence together. Typically this is characterised
by some relation that is true over the pair of tasks (e.g. located at/2), but this is
not essential. e.g.
diff_rate(task(type(travel),subject(london),location(n/a),
duration(2/0),preferred_time(7-0),preferred_day(monday)),
task(type(visit),subject(ericsson),location(n/a),
duration(4/30),preferred_time(10-0),preferred_day(tuesday)),
drate(related)).
Diﬀerent — the tasks do not fall into any of the above categories. e.g.
diff_rate(task(type(travel),subject(london),location(n/a),
duration(2/0),preferred_time(7-0),preferred_day(monday)),
task(type(prep),subject(meeting),location(103),
duration(0/30),preferred_time(9-30),preferred_day(friday)),
drate(different)).
5.2
Constructing the Diﬀerence Rating Model
The clusters of examples being examined in the previous chapter were guaranteed to be
made up of examples that were (according to the user) close to each other within the
example space, the dimensions of which were deﬁned by the attributes of the items within
each example. The examples of the concepts to be learnt for the diﬀerence rating model
occupied areas of the example space that had a completely diﬀerent shape. The example
space was divided into mutually exclusive areas by the diﬀerent diﬀerence ratings, with
no areas left uncovered by a classiﬁcation.
The most obvious way to present the modelling problem to Aleph would have been to
focus on producing an individual theory for each possible classiﬁcation, using examples of
that classiﬁcation as positive examples, and all the other examples as negative examples.
This would enable the inclusion of information about the nature of the example space
that learning in a positive-only fashion may not have captured, i.e. positive-only learning
would be concerned with covering the areas of the example space containing the given
examples but would avoid covering areas of space for which the classiﬁcation had not been
demonstrated by the given data set. However, in order to enable the target concept for
each classiﬁcation to be split into independent learning problems, negative examples with
the same properties as those used in the previous chapter would have to be used rather
than the examples from the other classiﬁcations.
The concepts to be captured for this learning problem were of a slightly diﬀerent nature
to that of the previous chapter. Rather than focusing on the relations between particu-

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
109
lar attributes in an eﬀort to enable prediction of a whole task, the model produced had
to be able to determine how similar, or how diﬀerent, a pair of tasks were. The back-
ground knowledge used earlier was augmented with this in mind to include predicates that
indicated similarity or diﬀerence between given values.
For example:
same_group(+tsl,+tsl).
not_same_group(+tsl,+tsl).
equal(+tsl,+tsl).
not_equal(+tsl,+tsl).
max_time_between(+time,+time,#time).
min_time_between(+time,+time,#time).
In addition to these predicates, the preferred time and day of each task was now under
examination and would require additional means of specialisation. The preferred time
of a task was treated in the same way as the duration of each task, i.e. the range of
values that could be taken was a set of discrete values ranging from 00-00 to 23-30 at
half hour intervals. The set of possible values that the preferred day of each task could
take was straightforward to deﬁne. The additional predicates provided to work with these
attributes complete the set of predicates needed to be able to construct the required
concept descriptions:
same_day(+day,+day).
not_same_day(+day,+day).
earlierthan(+time,#time).
laterthan(+time,#time).
in_office_hours(+time,+day).
not_in_office_hours(+time,+day).
#time would be instantiated to a particular value during the learning process.
Several of the predicates presented were the same as those used in the previous chapter, e.g.
all those referring to the type and subject of each task. The predicates only referred to one
attribute in the head of the clause at a time. However as the concepts were symmetrical,
the data to be presented to Aleph could contain examples with tasks placed either way
round. By breaking the symmetry of the concept described by the examples presented
the amount of work carried out during construction of the model was reduced because the
concept was only learnt in one direction. A pair of tasks could be covered by a concept if
the pair were covered in either order.

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
110
For example, if the symmetry had not been broken then the model could have contained
the following pair of rules:
diff_rate(task(type(A),subject(B),location(C),
duration(D),preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),
duration(J),preferred_time(K),preferred_day(L)),
drate(similar)):-
equal(A,G),
same_group(B,H),
not_equal(B,H),
min_hrs_between(D,J,2/0),
not_in_office_hours(E,F).
diff_rate(task(type(A),subject(B),location(C),
duration(D),preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),
duration(J),preferred_time(K),preferred_day(L)),
drate(similar)):-
equal(A,G),
same_group(B,H),
not_equal(B,H),
min_hrs_between(D,J,2/0),
not_in_office_hours(K,L).
The second rule is a mirror image of the ﬁrst (the ﬁnal literal, not in office hours/2
has been applied to the preferred time and day of the second task instead of the ﬁrst task)
and should be avoided. Simply querying the model with the pair of tasks in both positions
when a classiﬁcation prediction was required removed the need to learn the second rule and
avoided other situations where an excessive amount of learning could have been carried
out.
Therefore, in order to avoid learning both sides of the symmetrical concept, the examples
were processed before beginning the learning process, switching around the ordering of
tasks in some examples. The criteria for the ordering of tasks within examples were based
on the attributes under consideration. Since the example set did not contain tasks with
stereotypical values, attribute values varied slightly from task to task, and therefore each
dimension was considered separately rather than comparing whole tasks. The order in
which attributes are examined can be abitrary since it is not possible to show ahead of
time that ordering based on one attribute will not re-introduce symmetry in the values
of another attribute. The order of consideration used was: type, subject, preferred time,
duration, preferred day, location. Subject was considered after type due to the natural

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
111
precedence ordering over these two attributes. Location was considered last due to the
frequency with which this variable did not contain a value of interest. If two tasks had
the same type then their ordering would be based on their subject values, if these were
the same then the preferred times would be considered, followed by the duration and then
the preferred day. If the two tasks were still considered to be the same after this level
of comparison then they were considered to be identical and their ordering within the
example did not matter.
Ordering tasks based on duration, preferred time or preferred day was simple as the
possible values for each of these attributes already had their own order. The task with
the lowest value for the attribute under consideration was placed ﬁrst within the example.
Ordering based on type, subject and location was slightly more complex. The idea used
was that it did not matter which of the two values, and the task containing it, was put ﬁrst,
provided that the same value was put ﬁrst every time the two values were compared. A list
of possible values was generated from the provided background knowledge and the order
that the values contained within the pair of tasks occurred within the list determined
which task was placed ﬁrst within the example under examination. If the two subject
values or the two location values were of the same group then they were also considered
to be similar enough to be ignored in order to improve the generality of the concept being
described. For example, if the following example was under consideration:
diff_rate(task(type(travel),subject(london),location(n/a),
duration(2/0),preferred_time(7-0),preferred_day(monday)),
task(type(travel),subject(cambridge),location(n/a),
duration(2/0),preferred_time(10-0),preferred_day(friday)),
drate(similar)).
the ordering of the tasks within the example with regard to the preferred time was deemed
more important than the ordering that would be given by considering the subject.
This breaking of symmetry also helped in the learning of specialisations for individual
attributes as it reduced the number of contradicting negative examples generated. This
will be illustrated in more detail in the next section.
5.2.1
Generation of Negative Examples
In the previous chapter, additional negative examples were generated to force Aleph to
focus on each of the dimensional subproblems that it was given. The negative examples
were generated using the distance measure that had been used during the partitioning
process. Some form of negative examples would also be needed to perform the same task
to aid Aleph in the generation of the model focused on in this chapter.
However the
distance measures used for the model in the previous chapter could not be used this time

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
112
because these measures did not apply for this set of concepts. The model being created
was attempting to capture the user’s idea of distance between tasks, therefore introducing
an alternate deﬁnition of distance into the construction of the model could aﬀect the recall
of the theories produced.
Examples with other classiﬁcations could not be used as negative examples for the dimen-
sional subproblems because the reason for each example receiving a diﬀerent classiﬁcation
may not depend on the attributes in the dimension currently being focused on. However,
additional information for this set of data enabled a diﬀerent method of negative example
generation to be constructed:
1. The sets of examples with a particular classiﬁcation were mutually exclusive, i.e.
each pair of tasks could only have one classiﬁcation.
2. The values for the ﬁrst three attributes of each task (i.e. type, subject, and location)
were related and would only occur in particular combinations.
This allowed the
inclusion of some additional predicates into the background knowledge to help Aleph
produce compact representations of the concepts.
3. The user would tend to look at only part of each task when deciding on a classiﬁca-
tion, e.g. they may look at type and subject ﬁrst, and if these are the same then they
may look at the durations of both tasks, or they may only look at the subject values
if the types are identical. If the durations were both the same then the times at
which the tasks are scheduled may be considered. The order of consideration would
diﬀer with each user, but the idea that a small subset of the attributes contained
within each task would be responsible for the classiﬁcation given to each pair gave
the independence assumption required to allow the learning task to be split into
separate dimensions.
An overview of the new method for negative example generation can be seen in Figure
5.1. New examples were still generated by taking a known positive example and altering
one value. However, instead of altering it just enough to place the new example further
than cluster threshold distance from the old example, thus creating a close specialisation
that would keep the area covered by the rule small, the values were taken from positive
examples with diﬀerent classiﬁcations, thus forcing Aleph to identify possible relations
between the feature altered and some other feature within the example.
In order to ﬁnd appropriate values, the examples with diﬀerent classiﬁcations were searched
for examples containing an identical copy of this second task (i.e. occasions when this task
has been compared to something else). When an example was found, the task that it
was being compared to was then examined, and the value within this task for the partic-
ular attribute currently being altered to generate negative examples was collected. Once all

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
113
C = {identical, same, similar, related, diﬀerent}
D = {type, subject, location, duration, pref time, pref day}
g ∈C
\\ The current classiﬁcation being worked with.
i ∈D
\\ The current dimension being looked at.
f ∈{ﬁrst,second}
\\ The current task being looked at.
S = {s|s is a set of non-contradicting +ves & -ves } = ∅
E = set of examples collected from the user
R = set of all possible tasks
For each p ∈E where class(p) = g
{
if(f = ﬁrst)
p = example(a, b, g), where a, b ∈R \\ a is the task being worked with.
else
p = example(b, a, g), where a, b ∈R
Tp = {k|k, l ∈R ∧((example(k, l, c) ∈E ∨example(l, k, c) ∈E) ∧
c ∈(C \ {g}) ∧(∀d ∈D : rough matchd(l, b))}
Np = ∅
For each t ∈Tp
{
If(∀d ∈(D \ {i}) : rough matchd(t, a))
If((i ∈(D \{duration,pref time})) ∨(δi(t, a) > 1 hour)) {
valued(a) = valued(t)
if(f = ﬁrst)
Np = Np ∪{example(a, b, g)}
else
Np = Np ∪{example(b, a, g)}
}
else
Np = Np
\\ i.e. do nothing
}
If (∃s ∈S : ∀q ∈s, ∀n ∈Np :pos example(q) ⇒δi(q, n) > 2c ∧
neg example(q) ⇒δi(q, p) > 2c)
S = (S \ {s}) ∪{s ∪Np ∪{p}} \\ add Np and p to an existing set within S.
else
S = S ∪{Np ∪{p}} \\ create an additional set within S.
}
Return S
δx(p, q) = distance between tasks p and q in dimension x.
rough matchx(t, p) = true if ( ( x ∈{type,subject,location} ∧same groups(t, p))∨
( x ∈{duration,pref time} ∧δx(t, p) ≤1 hour) ∨
( x =pref day∧
((weekday(t) ∧weekday(p)) ∨(¬weekday(t) ∧¬weekday(p))))
false, otherwise.
Figure 5.1: New method for the generation of non-contradictory sets of negative examples

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
114
the possible alternate values had been collected, the collection was evaluated to see if
the values could be used to generate negative examples. If the fraction of those values
that were the same as the original value was small enough, then it was assumed that this
attribute had some signiﬁcance to the concept and the diﬀerent values were substituted into
the original example to create the required negative examples. In the initial experiments
performed while constructing this method of negative example generation it was found
that satisfactory results could be obtained if the fraction of identical values was less than
1/2.
The two variables that took a set of values with an ordering over them, duration and
preferred time, had an additional ﬁlter with regard to the values used to generate negative
examples. To help avoid overﬁtting, any values that were an hour or less from the original
value were discarded. When attempting to produce specialisations for these variables, a
number of contradictions between positive and negative examples were produced. These
were reduced by the ordering imposed on the tasks within the examples at an earlier
stage. For example, consider the following examples (NB: the following notation has been
introduced as it is assumed the reader is now suﬃciently familiar with the data presented
not to need the additional mark-up any more, however it will still be retained within the
model generated because the user will not be so familiar with the model structure. Ideally
they will never need to examine the model at all.):
e1. ⟨⟨travel,london,n/a,2/0,8-00,tuesday⟩,
⟨travel,colchester,n/a,1/0,9-00,tuesday⟩,
class:same⟩
e2. ⟨⟨travel,cambridge,n/a,2/0,9-00,thursday⟩,
⟨travel,colchester,n/a,1/0,10-00,tuesday⟩,
class:same⟩
if e1 or e2 had been presented with the tasks in reverse then it would be easier to produce
negative examples that contradicted other positive examples.
e3. ⟨⟨travel,colchester,n/a,1/0,9-00,tuesday⟩,
⟨travel,london,n/a,2/0,8-00,tuesday⟩,
class:same⟩
e4. ⟨⟨travel,colchester,n/a,1/0,10-00,tuesday⟩,
⟨travel,cambridge,n/a,2/0,9-00,thursday⟩,
class:same⟩
If negative examples were generated using e3, it would be possible to substitute the value
10-00 from an example with a diﬀerent classiﬁcation into the second task in place of 8-00

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
115
as the new value is over an hour from the old value. As only the times of the tasks are being
considered at this point then this new negative example contradicts e2. Similarly it would
be possible to substitute the value 8-00 from an example with a diﬀerent classiﬁcation
into the duration of the ﬁrst task of e4 to create a negative example that would contradict
e1 if just the times of the two tasks were considered. If the tasks within the examples
were ordered as described, i.e. with the shortest task ﬁrst, then e3 and e4 would not exist
and the contradictions would not occur.
One of the other facts about the concept mentioned earlier was that the classiﬁcation of
a pair of examples was dependent on only one set of attributes within a given pair, it
may be a diﬀerence in the duration of both tasks, or that one task was scheduled during
the day and one in the early morning.
If all the values from examples with diﬀerent
classiﬁcations were collected, then for those examples where the attribute being examined
was not the deciding factor but the value retrieved was far enough away from the original
value of the positive example being altered, the value returned was very likely to create a
false negative (i.e. one that the user would consider positive, but this could not be proved
by logical reasoning).
In order to reduce the number of values returned of this type,
additional criteria were included when values were being gathered that required the other
main attribute groups to be roughly the same as those in the task being altered within
the positive example.
For example, suppose the following examples had been collected:
e5. ⟨⟨travel,colchester,n/a,1/0,10-00,tuesday⟩,
⟨travel,cambridge,n/a,2/0,10-00,thursday⟩,
class:same⟩
e6. ⟨⟨travel,colchester,n/a,1/0,10-00,tuesday⟩,
⟨travel,london,n/a,2/0,10-30,monday⟩,
class:same⟩
e7. ⟨⟨travel,colchester,n/a,1/0,7-00,tuesday⟩,
⟨travel,cambridge,n/a,2/0,11-00,thursday⟩,
class:similar⟩
e8. ⟨⟨travel,cambridge,n/a,1/0,10-00,tuesday⟩,
⟨travel,york,n/a,4/0,16-00,wednesday⟩,
class:similar⟩
e9. ⟨⟨travel,london,n/a,2/0,18-00,tuesday⟩,
⟨travel,cambridge,n/a,2/0,9-00,thursday⟩,
class:similar⟩

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
116
and that ﬁrst example, e5, is the next example to be used for negative example generation
with regards to the preferred time of the ﬁrst task.
The second task in the example roughly matches tasks in e7, e8, and e9 (an hour either
side of the values within the task is allowed to be regarded as a possible match). The ﬁrst
task in e5 is then compared to the other tasks in these examples. The ﬁrst task in e7
has the same values for type, subject, location, duration, and preferred day, therefore the
preferred time must be responsible for the diﬀerent classiﬁcation, and this value is stored
for later use. The second task in e8 has the same value for type and location, and a similar
value for subject, but the duration of the task is over 1 hour diﬀerent from the duration
of the ﬁrst task in e5. It cannot be proved that the value of the preferred time for this
task is responsible for the classiﬁcation and so it is discarded.
The ﬁrst task in e9 has the same type and location, a roughly similar value for subject
and the duration is just close enough to be deemed not responsible for the classiﬁcation
(values contained within a task subject that are of the same group are considered to be close
enough to be a match when the values being gathered for negative example generation are
for a diﬀerent attribute). The value for the preferred time is stored for negative example
generation.
This method of value exclusion would work in the same manner for task duration, and
in almost the same manner for the task type, subject and location. The diﬀerence for
type, subject and location was that if the values gathered were of the same group (but
not equal) then they were still used to generate negative examples. If these values were
not included then Aleph would be forced to produce rules that covered entire groups when
they may need to be more specialised, e.g. one of the diﬀerences between pairs of tasks
that had the classiﬁcation same and pairs of tasks that had the classiﬁcation identical
was that the identical tasks had exactly the same subject value, but the tasks classiﬁed
as same had subject values that were of the same group but were not identical. Without
the ability to produce negative examples that had identical subject values the modelling
process would have been unable to include this distinction.
It was still possible, when dealing with attributes that contained a set of values with
an ordering over them, to generate negative examples that would contradict positives
within the same set. However, at this point the positive examples and their corresponding
negative examples could be split into separate non-contradictory sets as had been done
in the previous chapter. This made it easier for Aleph to make use of other predicates
such as earlierthan/2 to help it exclude negative examples. All the results produced
from each subset were gathered to form a set of possible specialisations for that particular
dimension.

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
117
5.2.2
Combining the Solutions Gathered
Once results had been produced for each of the separate subproblems there was then the
problem of how to combine them in such a way as to produce the optimal theory. An
optimal theory in this case is taken to mean a set of clauses that cover all of the positive
examples within the data set without covering any of the negative examples. As each
theory may not necessarily require specialisation on each attribute of both tasks, suﬃcient
specialisation in order to capture the underlying reasons for each pair of tasks having
a particular classiﬁcation would suﬃce.
How could it be reasoned as to which of the
results from the learning problems carried out so far was worth including in the overall
theory? Some of the features for which possible specialisations had been produced may
not actually play a part in the concept being learnt. A few negative examples may have
been produced for dimensions that were not important to the concept being learnt but
they could not be proven to be positive by any reasoning that could be carried out over
the data. Including the results from unimportant dimensions into a constructed theory
would result in overﬁtting of the hypothesis to the data, therefore the answers for each
dimension could not simply be added together as they had been in the previous chapter.
The best way to determine which combination of the collected results would produce
the best theory was to give the problem back to Aleph. All the collected results were
reformulated as alternate background information to be used, and the theory produced
was then translated into a more meaningful form afterwards. The positive examples for the
new learning problem were the entire set of positive examples for the given classiﬁcation,
and the negative examples were all the positive examples for other classiﬁcations, i.e. the
same data set that Aleph would originally have been given. The original set of background
information was not required for this learning problem as the relevant portions of this
information were encapsulated within the new set of background predicates produced.
The following example shows how the mechanism for combining the collected results works.
Suppose the following rules had been learnt (amongst other results):
r1. ⟨⟨A,B,C,D,E,F⟩,⟨G,H,I,J,K,L⟩,class:same⟩:- A = G.
r2. ⟨⟨A,B,C,D,E,F⟩,⟨G,H,I,J,K,L⟩,class:same⟩:- same group(B,H),
not equal(B,H).
r3. ⟨⟨A,B,C,D,E,F⟩,⟨G,H,I,J,K,L⟩,class:same⟩:- max hrs between(D,J,1/30),
shorterthan(D,3/0).
r4. ⟨⟨A,B,C,D,E,F⟩,⟨G,H,I,J,K,L⟩,class:same⟩:- max hrs between(D,J,1/30),
longerthan(J,1/0).
r5. ⟨⟨A,B,C,D,E,F⟩,⟨G,H,I,J,K,L⟩,class:same⟩:- C = I.
r6. ⟨⟨A,B,C,D,E,F⟩,⟨G,H,I,J,K,L⟩,class:same⟩:- max time between(E,K,3-00),
earlierthan(E,11-00).
r7. ⟨⟨A,B,C,D,E,F⟩,⟨G,H,I,J,K,L⟩,class:same⟩:- in office hours(E,F).

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
118
r8. ⟨⟨A,B,C,D,E,F⟩,⟨G,H,I,J,K,L⟩,class:same⟩:-in office hours(K,L).
r9. ⟨⟨A,B,C,D,E,F⟩,⟨G,H,I,J,K,L⟩,class:same⟩:- not sday(F,L).
Each of these rules is translated into a new rule to be given as background information to
Aleph:
:- modeb(r1(+t1type,+t2type)).
r1(A,G):- A = G.
:- modeb(r2(+t3type,+t4type)).
r2(B,H):- same_group(B,H),not_equal(B,H).
:- modeb(r3(+duration,+d2uration)).
r3(D,J):- max_hrs_between(D,J,1/30), shorterthan(D,3/0).
:- modeb(r4(+duration,+d2uration)).
r4(D,J):- max_hrs_between(D,J,1/30), longerthan(J,1/0).
:- modeb(r5(+t5type,+t6type)).
r5(C,I):- C = I.
:- modeb(r6(+ptime,+p2time)).
r6(E,K):- max_time_between(E,K,3-00), earlierthan(E,11-00).
:- modeb(r7(+ptime,+pday)).
r7(E,F):- in_office_hours(E,F).
:- modeb(r8(+ptime,+p2day)).
r8(K,L):- in_office_hours(K,L).
:- modeb(r9(+pday,+p2day)).
r9(F,L):- not_sday(F,L).
The types given in the modeb declaration of each rule force Aleph to apply each rule only
to the variables for which it was originally created. r1 was presented to Aleph because
this requirement prevents the uniﬁcation of the variables in the head of the clause. Aleph
will then return a result such as:
⟨⟨A,B,C,D,E,F⟩,⟨G,H,I,J,K,L⟩,class:same⟩:-r1(A,G),r2(B,H),r4(D,J),
r7(E,F),r8(K,L).

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
119
This rule would then be translated to use the original set of background knowledge.
⟨⟨A,B,C,D,E,F⟩,⟨G,H,I,J,K,L⟩,class:same⟩:- A = G,
same group(B,H),
not equal(B,H),
max hrs between(D,J,1/30),
longerthan(J,1/0),
in office hours(E,F),
in office hours(K,L).
This method of rule construction could not be used in the previous chapter because the
concept learnt in the previous chapter required all the attributes of both tasks to be
specialised in order to make correct predictions. Since each of the specialisations being
combined covered one of the attributes, they all needed to be included in the rules being
generated. For the concept learnt in this chapter this was not the case, so a method could
be employed that could attempt to construct the most suitable concept from the results
presented. The reduced amount of background knowledge presented to Aleph meant that
the size of the search space was also reduced. Aleph could then be used to carry out the
learning problem that it was unable to produce a satisfactory answer for earlier when the
search space was too large.
From the modelling system built it can be seen that, if it is known that the concept to be
learnt does not require results from all of the individual dimensions to be included in the
clauses produced, the results of each learning problem can be presented to the learner as
a reduced set of background knowledge from which to construct the overall theory. This
alternate method of result combination eﬀectively means that the irrelevant background
knowledge for each particular concept can be ﬁltered out and so size of the search space
is reduced whilst still allowing the learner to construct the overall theory. This means
that Dilum can be used to perform a specialised form of feature construction, an idea
from the data-mining area that was described in Section 2.4.4. Normally, within feature
construction, little is known about the relations between the attributes; if this information
was available then the task would not be so diﬃcult.
Machine learning is one of the
methods used to identify useful subsets of the available features that can then be used for
the task at hand (the range of applications varies widely). The model being constructed
in this chapter does not fall into this category of problems to be solved. Here, whilst
the emphasis is still on reducing the dimensionality of the learning problem in order to
reduce its complexity, and improving the eﬃciency of the learning process is still a primary
goal, the overall aim of Dilum is slightly diﬀerent. The diﬃculties that Dilum is designed
to overcome are related to the requirements to function autonomously and to produce
results in a short period of time. Dilum makes use of additional background information
to transform a single feature construction task into a series of independent tasks with
reduced complexity that will more easily be completed within the time limit.

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
120
5.2.3
A Worked Example
Suppose the following examples had been collected, and the theory to be learnt was for
the classiﬁcation similar:
p1. ⟨⟨travel,cambridge,n/a,2/0,8-00,tuesday⟩,
⟨travel,cambridge,n/a,2/0,9-00,tuesday⟩,
class:identical⟩
p2. ⟨⟨travel,colchester,n/a,1/0,9-00,tuesday⟩,
⟨travel,london,n/a,2/0,10-00,tuesday⟩,
class:same⟩
p3. ⟨⟨travel,cambridge,n/a,2/0,9-00,thursday⟩,
⟨travel,london,n/a,2/0,9-00,tuesday⟩,
class:same⟩
p4. ⟨⟨travel,colchester,n/a,1/0,9-00,tuesday⟩,
⟨travel,cambridge,n/a,2/0,10-00,thursday⟩,
class:same⟩
p5. ⟨⟨travel,colchester,n/a,1/0,7-30,tuesday⟩,
⟨travel,cambridge,n/a,2/0,9-00,thursday⟩,
class:similar⟩
p6. ⟨⟨travel,york,n/a,4/0,9-00,tuesday⟩,
⟨travel,cambridge,n/a,2/0,9-00,thursday⟩,
class:similar⟩
p7. ⟨⟨travel,colchester,n/a,1/0,9-00,tuesday⟩,
⟨travel,bath,n/a,2/0,18-00,thursday⟩,
class:similar⟩
p8. ⟨⟨travel,bath,n/a,4/30,9-00,tuesday⟩,
⟨travel,london,n/a,2/0,10-00,thursday⟩,
class:similar⟩
p9. ⟨⟨meeting,ericsson,b jones,1/0,8-30,tuesday⟩,
⟨travel,cambridge,n/a,2/0,9-30,tuesday⟩,
class:related⟩

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
121
p10. ⟨⟨admin,personnel,103,1/0,8-30,wednesday⟩,
⟨travel,cambridge,n/a,2/0,9-30,tuesday⟩,
class:different⟩
p11. ⟨⟨prep,ipaf,n/a,1/0,8-00,tuesday⟩,
⟨travel,cambridge,n/a,2/0,10-00,tuesday⟩,
class:different⟩
The process begins by taking example p5 and attempting to alter the ﬁrst attribute of the
ﬁrst task. The second task of the pair can be found in examples p3, p4, p9, p10 and p11
(remember that only the examples with a diﬀerent classiﬁcation are being searched). In
examples p3 and p4, the value of the ﬁrst attribute of the other task in the pair is travel.
This is the same as the original value and so is discarded. Examples p9, p10 and p11 give
the values meeting, admin and prep.
For the ﬁrst learning problem, three negative examples are created:
n1. ⟨⟨meeting,colchester,n/a,1/0,7-00,tuesday⟩,
⟨travel,cambridge,n/a,2/0,9-00,thursday⟩,
class:similar⟩
n2. ⟨⟨admin,colchester,n/a,1/0,7-00,tuesday⟩,
⟨travel,cambridge,n/a,2/0,9-00,thursday⟩,
class:similar⟩
n3. ⟨⟨prep,colchester,n/a,1/0,7-00,tuesday⟩,
⟨travel,cambridge,n/a,2/0,9-00,thursday⟩,
class:similar⟩
Presenting these examples to Aleph along with examples p5, p6, p7 and p8, and their
generated negative examples, produces either r1 or r2 (depending on the order in which
the candidate clauses are constructed and evaluated):
r1. ⟨⟨A,B,C,D,E,F⟩,⟨G,H,I,J,K,L⟩,class:similar⟩:- A = G.
r2. ⟨⟨A,B,C,D,E,F⟩,⟨G,H,I,J,K,L⟩,class:similar⟩:- A = travel.
With more examples that refer to tasks other than travel, Aleph will deﬁnitely produce
r1 because r2 will no longer receive the same evaluation score.

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
122
Moving onto the second attribute of the ﬁrst task of p5, the values cambridge, colchester
(discarded), ericsson and ipaf are retrieved, creating the following negatives:
n4. ⟨⟨travel,cambridge,n/a,1/0,7-00,tuesday⟩,
⟨travel,cambridge,n/a,2/0,9-00,thursday⟩,
class:similar⟩
n5. ⟨⟨travel,ericsson,n/a,1/0,7-00,tuesday⟩,
⟨travel,cambridge,n/a,2/0,9-00,thursday⟩,
class:similar⟩
n6. ⟨⟨travel,ipaf,n/a,1/0,7-00,tuesday⟩,
⟨travel,cambridge,n/a,2/0,9-00,thursday⟩,
class:similar⟩
The value london was not used to make a negative example because the duration and
preferred time of the task from which the value was taken were not close enough for it to
be reasoned that this value was the cause of the diﬀerence in classiﬁcation.
Presenting p5 with its negative examples, and p6, p7, and p8 with their respective negative
examples, the following result are returned:
⟨⟨A,B,C,D,E,F⟩,⟨G,H,I,J,K,L⟩,class:similar⟩:- same group(B,H),
not equal(B,H).
Values retrieved for substitution into the third attribute of p5 are b jones, and several
instances of n/a. This second value is the same as the original value and as over half of the
values retrieved are the same as the original value, no negative examples are generated.
This occurs also with p6, p7, and p8 and so no specialisation is produced for this attribute.
The fourth attribute is the duration of the task. The values retrieved for substitution are
several instances of 1/0, one instance of 2/0. As the initial value was 1/0 and a signiﬁcant
fraction of the possible values matched or were within matching distance of this, then no
negative examples are produced. However there were a few values that could be used
as negative values for p6 because the original value of the attribute was 4/0, which was
further away than most of the values for this attribute. The following negative examples
were created:
n7. ⟨⟨travel,york,n/a,1/0,9-00,tuesday⟩,
⟨travel,cambridge,n/a,2/0,9-00,thursday⟩,
class:similar⟩

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
123
n8. ⟨⟨travel,york,n/a,2/0,9-00,tuesday⟩,
⟨travel,cambridge,n/a,2/0,9-00,thursday⟩,
class:similar⟩
After splitting the positive examples with their respective negatives (if any) into separate
non-contradictory sets and presenting each of the two sets to Aleph as a separate learning
problem, the following clauses are returned as separate theories:
⟨⟨A,B,C,D,E,F⟩,⟨G,H,I,J,K,L⟩,class:similar⟩:- min hrs between(D,J,2/0).
⟨⟨A,B,C,D,E,F⟩,⟨G,H,I,J,K,L⟩,class:similar⟩.
The second clause is returned as a result of presenting positive examples to Aleph without
any negative examples. Maximally general clauses (i.e. clauses that do not contain any
literals within the body) are discarded as they are simply an indicator that that particular
set of examples could not be used to produce a useful result for the attribute currently
under examination.
The ﬁfth attribute is the preferred time of the task.
Values retrieved from the other
examples for substitution into p5 are 9-00 and 8-30. Values from the other examples are
discarded due to the other attributes of each task being too dissimilar from the task being
substituting into.
This gives two negative examples:
n9. ⟨⟨travel,colchester,n/a,1/0,9-00,tuesday⟩,
⟨travel,cambridge,n/a,2/0,9-00,thursday⟩,
class:similar⟩
n10. ⟨⟨travel,colchester,n/a,1/0,8-30,tuesday⟩,
⟨travel,cambridge,n/a,2/0,9-00,thursday⟩,
class:similar⟩
The background predicate not in office hours(E,F) will return true if the time given
is outside the range 8-30 to 17-30 or the day given is a Saturday or Sunday. It can be
used to exclude n9 and n10, to produce the following rule:
⟨⟨A,B,C,D,E,F⟩,⟨G,H,I,J,K,L⟩,class:similar⟩:- not in office hours(E,F).
The sixth attribute has tuesday returned every time as the possible value, and as this
matches the original value then this is discarded and the maximally general rule pro-
duced due to presenting a learning problem with no negative examples is discarded also.
Therefore, after the ﬁrst half of the learning process, the following results have been

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
124
collected:
⟨⟨A,B,C,D,E,F⟩,⟨G,H,I,J,K,L⟩,class:similar⟩:- A = G.
⟨⟨A,B,C,D,E,F⟩,⟨G,H,I,J,K,L⟩,class:similar⟩:-same group(B,H),
not equal(B,H).
⟨⟨A,B,C,D,E,F⟩,⟨G,H,I,J,K,L⟩,class:similar⟩:- min hrs between(D,J,2/0).
⟨⟨A,B,C,D,E,F⟩,⟨G,H,I,J,K,L⟩,class:similar⟩:- not in office hours(E,F).
This would be continued onto the attributes of the second task of the pair to produce more
specialisations. Although it is likely that many of those produced will be copies of the
results produced for the ﬁrst half, there are possible predicates that are not of a symmet-
rical nature, and would need to be applied to both the tasks, e.g. not in office hours/2
would need to be applied to both tasks if both tasks should be outside oﬃce hours at the
same time.
The results returned for each stage seem to suggest that an attribute of each task is only
being compared to the corresponding attribute in the other task of the pair.
In fact,
this is not the case, there is the potential for literals that refer to any number of the
attributes of the other task in the pair to be used, it is simply that for this experiment,
most of the literals provided are binary predicates where both of the arguments refer
to values of the same type. If background knowledge was available that linked diﬀerent
attributes within the tasks then this could easily be used instead of, or in addition to, the
knowledge provided at this point. Once the alternate background knowledge is generated,
the following information can be presented to Aleph:
Positive Examples — The original set of examples classiﬁed as similar,
Negative Examples — All the original examples with other classiﬁcations, and
Background Knowledge — The predicates discovered during the initial round of learn-
ing problems, no other predicates are presented.
Aleph will then determine the best theory that can be constructed from the given predi-
cates. In this case the result consists of two clauses:
⟨⟨A,B,C,D,E,F⟩,⟨G,H,I,J,K,L⟩,class:similar⟩:- A = G,
same group(B,H),
not equal(B,H),
min hrs between(D,J,2/0).
⟨⟨A,B,C,D,E,F⟩,⟨G,H,I,J,K,L⟩,class:similar⟩:- A = G,
same group(B,H),
not equal(B,H),
not in office hours(E,F),
in office hours(K,L).

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
125
The result retrieved appears to be something that an ILP engine should be able to produce
without any of the additional processing that has occurred, even if a time limit were to be
imposed, and, indeed, in the next section this is shown to be the case for this particular
concept. However, the example shown earlier of the clause required for the concept same
demonstrated that this concept required a longer clause and so would require the learner
to explore a larger area of the search space in which to ﬁnd it. The next section shows
the results of comparing the use of Dilum to attempting to learn each concept using full
background knowledge.
5.3
Evaluation of the Model Produced
The performance of Aleph using Dilum on the data sets was compared to several other
methods:
1. Aleph using breadth-ﬁrst search,
2. Aleph using depth-ﬁrst search,
3. Aleph using a heuristic search,
4. Aleph using a heuristic search with the openlist setting set to 1 — Aleph’s current
list of candidate clauses is restricted to a length of 1. This essentially transforms the
search strategy into a greedy search where the best literal at each stage is added to
the clause constructed and the learner never backtracks,
5. TILDE using a data set containing all ﬁve classiﬁcations, and
6. TILDE using a partitioned data set.
Each of the ﬁve concepts described earlier had a set of representative data containing
just over 90 examples. Each example used tasks that were encountered in the previous
chapters, with some additional values such as extra company names or places included. The
evaluation of Aleph working on its own with diﬀerent search strategies used examples of
the classiﬁcation to be learnt as positive examples and the examples of other classiﬁcations
as negative examples. As 3-fold cross validation (averaged over 10 runs) was used. This
meant that there were approximately 60 positive examples and 240 negative examples per
data set. Dilum produced its own negative examples for the ﬁrst set of learning problems
using the method described earlier, therefore the number of negative examples generated
per positive example was not a value that could be set as in the previous chapter, but
depended on the data contained within the other concept data sets. For the second stage
of learning, Dilum used the same data as the experiments involving Aleph on its own.
TILDE was presented with a data set containing 60 examples of each classiﬁcation. In

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
126
the second method involving TILDE, the learner was presented with 60 examples of each
classiﬁcation in turn, and 60 examples from each of the other classiﬁcations as negative
examples (i.e. 240 negative examples in total, the same as for Aleph working on its own).
The use of TILDE with and without a partitioning pre-processing step was intended to
enable a direct comparison of the results produced to show the beneﬁts of partitioning for
a greedy learner. These results could then also be compared to Aleph’s version of a greedy
search to determine how beneﬁcial partitioning is for diﬀerent greedy search strategies.
The measure of error used is given in Equation 5.1. This equation was used because it
balances out the bias towards not covering negative examples that would be produced by
having a much larger set of negative examples than positive examples. If the model were
constructed and used within a real environment, the cost to the system of failing to give
an answer would be as high as giving the wrong answer. In fact, if the model did not give
an answer then some default value would have to be given instead, which is essentially the
same as giving a wrong answer. Therefore it is just as important to avoid false negatives
as it is to avoid false positives.
Error(h) = 1
2 −1
2

Pc
|P| −Nc
|N|


(5.1)
where
Pc is number of positive examples covered
Nc is the number of negative examples covered
P is the set of positive examples
N is the set of negative examples
Discussion of Results
Figure 5.2 shows the results of the experiment. The concept similar could be described
by a set of clauses that contained no more than 4 literals (plus uniﬁcation of variables in
the head of the clause) if learnt by Aleph working unaided. The concepts related and
different required even shorter clauses. The search space size was still very large due
to the maximum clauselength that could be constructed still remaining as large as for the
previous learning problem, so it was still possible for the learner to be required to explore
a large area of search space, especially if noisy examples are present. However, a normal
exploratory ILP method should be able to produce an appropriate theory for each of these
concepts without intervention as it is unlikely to have to explore as much of the search
space of possible clauses as it would for longer length clauses. In these cases Aleph using
Dilum produced a result that was at least as good as that produced by Aleph on its own,
the theories being produced equating to the desired concepts.

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
127
 Measured error
Suggested error margins on measured error
hro − Aleph using heuristic search
openlist = 1
Key: 
bf − Aleph using breadth−first search
df − Aleph using depth−first search
hr − Aleph using heuristic search
T − TILDE using all the data at once
T−spl − TILDE using partitioned data
D − Aleph using full Dilum
T−spl
bf
df
hr
hro
T
D
All approaches produced 0 error
for all experiments.
0
0.8
0.7
0.6
0.5
0.4
0.2
0.1
0.3
bf
df
hr
hro
T
T−spl
D
0
0.8
0.7
0.6
0.5
0.4
0.2
0.1
0.3
bf
df
hr
hro
T
T−spl
D
0
0.8
0.7
0.6
0.5
0.4
0.2
0.1
0.3
bf
df
hr
hro
T
T−spl
D
0
0.8
0.7
0.6
0.5
0.4
0.2
0.1
0.3
bf
df
hr
hro
T
T−spl
D
0
0.8
0.7
0.6
0.5
0.4
0.2
0.1
0.3
diff_rate(similar)
diff_rate(identical)
diff_rate(same)
diff_rate(related)
diff_rate(different)
Figure 5.2: Comparison between ILP methods using split and full background knowledge

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
128
The concept identical required specialisations on all aspects of the task descriptions,
but Aleph was able to produce some sort of approximation for this concept because some
of the information required could be included within the clauses constructed by unifying
variables in the head of the clause (particularly the type and subject of both tasks), thus
reducing the number of literals that were required within the body of the clause.
The concept same also required specialisations on all aspects of the task descriptions.
Aleph using Dilum produced a good performance on this concept, out-performing all of
the exploratory approaches to the search space tried by Aleph working without assistance.
None of the unaided exploratory approaches could construct an appropriate theory for the
concept, frequently exceeding the given time resource (10,000 candidate clauses). Although
the type of each task could be uniﬁed in the head of each clause, as it had been for the
concept identical, the subject description needed 2 literals to describe the fact that the
values for the subject of each task were of the same group but not identical. Aleph’s version
of a greedy search, carried out using a heuristic search and restricting the list of candidate
clauses, was not aﬀected by the large search space, but it also failed to produce the required
theory for the concept same. Due to the imprecise nature of the data contained within
examples in the data set, some of the candidate clauses evaluated during learning had a
better performance over the data set than clauses that were generalisations of the target
clause. An exploratory learner could recover from these mistakes and focus on a diﬀerent
area of the search space, but the removal of Aleph’s exploratory ability prevented it from
weathering these small imperfections. Although TILDE also carried out a greedy search,
and hence also could not backtrack if it made a mistake, TILDE’s approach to constructing
a classiﬁcation theory did not incorporate bias towards a particular classiﬁcation. When
carrying out a greedy form of heuristic search, Aleph aimed to exclude as many negative
examples as possible from the cover set of the clause under construction, even at the cost
of excluding some positive examples. TILDE looked for something that would partition
the data set in such a way as to separate the examples of each classiﬁcation as much as
possible from other examples, allowing other, more strategic, choices to be made. The
low level of error in the results of TILDE working on its own demonstrated that it did
not require assistance to produce this model. However, the beneﬁt that using partitioning
gave was that the collection of theories produced were simpler than a single global theory.
This made the overall model produced much easier to understand.
The failure of Aleph’s version of a greedy search to produce the required theory for every
concept gives more information about the range of learning problems that would beneﬁt
from partitioning. Although partitioning the data can improve the understandability of
a concept, it does not guarantee that a particular greedy approach will be able learn a
concept if it could not do so using the entire data set. Greedy approaches have a trade-oﬀ
of robustness for speed and eﬃciency which make them mostly unsuitable for learning
problems where the relations between variables are not straight-forward to determine.
Using partitioning as a pre-processing step will still enhance the eﬃciency of the learner,

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
129
but cannot guarantee that the results produced are those required. Consideration of using
partitioning to enhance the results of a greedy approach to a particular learning problem
should bear this in mind.
5.4
Conclusions
The results presented in Section 5.3 show that an exploratory ILP engine such as Aleph
is capable of learning (without assistance) a concept that only requires a few literals in
the body of each clause even if a large amount of background knowledge is presented that
could be deemed relevant to the target concept. In particular, the idea used by Aleph of
determining all possible relevant literals and then performing a search within that set will
enable it (most of the time) to construct a clause that will ﬁt the data. An appropriate
clause cannot be guaranteed but is likely to be produced.
The maximum amount of search space that can be explored is bounded by the number of
appropriate literals that can be used within the clause and the maximum length of clause
allowed. However, as the length of the target clause to describe a concept increases, the
amount of search space that the ILP engine will need to explore in order to ﬁnd it increases.
This will occur not just for a breadth-ﬁrst search, but also for searches such as depth-ﬁrst
and heuristic search where a number of areas of the search space may be explored before
the area containing the required clause is chosen. If the amount of background knowledge
presented (and hence the number of literals that could possibly be included in the clause)
is small then this is not a problem, but if the background knowledge presented is large
then the size of the search space that must be explored will increase at a much faster rate,
causing problems even for relatively short clauses.
If the target concept can be split into separate independent problems to be dealt with
individually with only the background knowledge relevant to that particular dimension
then the learner can be presented with a series of problems where the size of the search
space has been reduced to a more manageable level. This idea has been demonstrated
both in this chapter and the previous chapter.
What happens if one of the learning problems produces a theory that contains several
clauses? How detrimental to the overall model would this be? If the learner is unable to
ﬁnd a concise theory that covers the data given then it will return a hypothesis containing
several clauses. The body of each clause will be used as a new background predicate in
the ﬁnal learning problem. Therefore, if several clauses are returned then the background
knowledge will not have been reduced in an eﬀective manner.
The probability of this
situation occurring is quite small, although it would increase as the amount of noise in
the data increases. However, this is not a serious disadvantage of the modelling system
because the result returned will be no worse than that returned if the whole problem had
been presented to the learner. The background knowledge returned for each independent

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
130
dimension has been constructed to cover the same group of positive examples as the
one that must be covered in the ﬁnal learning problem. Hence each set of background
predicates returned is intended to provide alternatives for specialisation of that particular
dimension. The learner will only need to use at most one predicate from each set of results
returned for each clause constructed in the ﬁnal theory, making it much easier for the
learner to exclude candidate clauses containing instances of the less relevant predicates
from its exploration of the search space. Therefore, although the background knowledge
returned may appear to be as large as the initial set of background knowledge, the learner
would be able to explore the reconﬁgured search space in a more eﬃcient manner.
Comparison of the two concepts attempted so far and the methods required to produce
the appropriate models allows the production of two lists of properties that should be
referred to when considering whether to use Dilum as a user modelling method. The ﬁrst
list contains the properties that a concept should have that would make consideration of
Dilum as a user modelling method appropriate. The second list contains the properties
that the concept must have for use of Dilum with an exploratory learner to be successful.
Dilum should be considered when a concept has:
1. A large amount of background knowledge to be used, most of which could be used
in the construction of each clause,
2. The possibility of rules containing several literals being required, and
3. A limited amount of time in which to be learnt.
For Dilum to be successful, a concept must:
1. Be able to be split into smaller independent subproblems and
2. Enable some means to be provided to Dilum so that it can ensure the learner focuses
on the subproblem currently being learnt. e.g. the ability to generate ‘near misses’
for use as negative examples.
Dilum must be provided with:
1. Some method of ensuring the learner focuses on the subproblem currently being
learnt (hence the requirement mentioned above), and
2. Some method of recombining the results of the learning subproblems.

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
131
If the concept consists of more than one independent subconcept and is rep-
resented solely by positive examples, Dilum must also:
1. Be provided with some method to partition the data into clearer subconcepts, and
2. If this method involves some form of external stimuli (e.g. negative examples or
constraints), take the ideas used within the partitioning method and use them within
the method that ensures the learner focuses on the current learning problem.
Both of the user models created consisted of a number of separate concepts that were dealt
with more eﬀectively on an individual basis. Dilum was created to cope with this sort of
learning problem, but it could easily cope with the situation where just one concept is to
be learnt provided some means exist that will make sure the learner is directed towards
each subproblem to be solved. Provided the area of the example space that contains the
set of positive examples does not contain negative examples as well, then this situation
can be regarded as simply having a single partition to learn over. A clearer idea of areas
where partitioning should be considered as a pre-processing step within Dilum is given in
Section 6.
If the method provided to ensure the learner focuses on the current subproblem involves
the use of some external stimuli (such as negative examples) then it should include the
ideas used within the partitioning method as this has already introduced a deﬁnition
of what is and is not part of each concept into the learning system. Both the models
learnt within this dissertation followed this guideline. However, a large amount of prior
knowledge about the concepts learnt within the two user models that was required in
order to generate appropriate negative examples. The amount of knowledge required was
so large that it suggests that this requirement will limit the range of concepts to which
Dilum can be applied.
If external stimuli are required then, in order to generate them, some knowledge is required
as to what is and what is not an example of the concept. For the ﬁrst model, the given
distance measure could be used for this purpose. For the second model, there was no
appropriate distance measure that could be used between examples and the generation of
negative examples had to rely on prior knowledge of the eﬀect of each attribute within
each example to decide whether a replacement value was suitable to generate a negative
example. In latter cases such as this, the only sorts of concepts that can be learnt are those
that are to be learnt in an automated unsupervised fashion. These concepts would occur as
applications of ILP within automated systems where the system engineer has a rough idea
of what the concept to be learnt will be like, but does not want to supervise the learning.
The learner could be set up and the generation of stimuli set in such a way as to reﬂect
the sort of concepts that the user is likely to demonstrate (but still in a generic manner so
that all the concepts that it would be required to learn could be dealt with), and then the
system can be left to do its job with minimum interference. In cases where there is some

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
132
sort of distance measure for examples then it should be possible for the learner to handle
concepts that are less well known, especially when the method of combining returned
results presented in Chapter 4 can be employed (i.e. when it is known that each clause
will require a result from each dimension). To provide the distance measure for the ﬁrst
model, additional knowledge was required about what tasks the user would regard as the
same or diﬀerent, this sort of concept is one where a rough idea can be given that should
cover a majority of cases. Having provided this rough measure the modelling system did
not need much extra information about the concepts that it would encounter, enabling it
to be more ﬂexible and hence able to learn any sequences that the user demonstrated.
5.5
Exploring the Concept Constraints
To what extent do the constraints listed earlier have to be adhered to for Dilum to produce
a usable answer? Consider the ﬁrst constraint:
‘The ability to split the concept to be learnt into smaller independent subproblems’
As was said earlier, the main problem faced by the exploratory learner is that the size
of the search space that it must explore to ﬁnd the target concept is too large.
The
two possible approaches are to split the concept to be learnt into subconcepts, so that
the complexity of the clauses for the target concept are reduced, and combine the results
of these subconcepts, or to reduce the size of the applicable background knowledge by
ﬁltering out the facts and predicates that are not needed (i.e. performing a form of feature
construction).
The ﬁrst approach was used to learn the sequence user model, the results of the learning
problems were combined using a simple cross-product of the sets produced and redundant
rules were ﬁltered out. Both approaches were used to learn the task distance model. The
production of a theory for the ﬁnal learning problem performed the actual construction
of the model using a reduced set of background knowledge. However, the background
knowledge was reduced by splitting the concept into subconcepts to be learnt and using
the results of these learning problems to indicate which items within the background
knowledge were relevant to the concept.
Splitting the concept into a set of subconcepts will reduce the size of the search space in a
more drastic fashion than reducing the background knowledge and is therefore, if possible,
more likely to produce successful results. This makes the constraint above a somewhat
fuzzy constraint. The greater the number of subconcepts the problem can be split into, the
higher the probability of success, however the maximum size of target clause that can be
successfully learnt will depend on the concept to be learnt and the amount of background
knowledge that can be used in the construction of the clause.
The other way to view this constraint is with regards to the idea of ‘independent’ sub-

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
133
problems. How independent do these subproblems have to be? This part of the constraint
is also somewhat fuzzy. If the subproblems are completely independent then a theory that
concisely describes the concept to be learnt can be guaranteed to be found. However, as
the subproblems become more dependent then it less likely for a learner to be able to do
so as the ability to completely construct theories has been removed, and hence some parts
of the search space will never be explored. The only guarantee that can be given is that if
a satisfactory solution still exists within the available search space then it will be found,
however this is something that should be taken into consideration when deciding whether
or not use of Dilum is appropriate for a concept to be learnt.
The second constraint:
‘Enable some means to be provided to Dilum so that it can ensure the learner focuses on
the subproblem currently being learnt’
can only be regarded as fuzzy depending on the data available and the concepts to be
learnt. The concepts dealt with so far required specialisations for certain arguments within
the head of the clause being constructed. The splitting of the concept into subconcepts
used subsets of the arguments to be specialised to govern this process, but it could be
possible to have a target concept with few arguments that was composed of a conjunction
of independent theories. These theories could all be made up of any of the items within
the available background knowledge and so refer to any of the arguments in the head of
the clause. As each dimension is capable of referring to any arguments then this sort of
concept would be something such as a membership relation that focuses on only covering
speciﬁc areas of the instance space.
In the former case it could be possible to produce some sort of result without speciﬁcally
generated stimuli, depending on how the background knowledge was split and how the
learning was carried out. However a usable result would be more likely if stimuli or bias
were used as this would help to prevent the learner from exploring areas of search space
that are already known not to hold the target clause. In the latter case, if insuﬃcient
stimuli were available it would be impossible to direct the learner towards a particular
subconcept as no information is available to decide which concept is being learnt at each
point. For concepts such as this the constraint can not be regarded as fuzzy.
5.6
Comparing Dilum’s and TILDE’s Learning Strategy
The search method used by TILDE to create a theory is a form of greedy search. Once
a node is chosen to be added to the tree being created it will not be removed. This form
of search excludes areas of search space at each step, which enables TILDE to cope with
the size of search space the creation of a user model would present. As an exploratory
learner, Aleph does not approach the search space in the same way and carries out a more

CHAPTER 5. CHARACTERISTICS OF THE CONCEPT SPACE
134
thorough examination, but is unable to cope with larger search spaces.
Is Dilum just another form of greedy search? Not really. The characteristic that a greedy
search and Dilum share is that once a result has been returned for each subconcept, the
result cannot be changed whilst generating the ﬁnal theory, and so areas of the search
space of possible hypotheses are excluded without consideration of the overall hypothesis
being constructed. However, there are some points to note about the nature of Dilum:
1. The result for each subconcept is generated in an exploratory manner (i.e. the learner
is able to backtrack).
2. The result for each learning problem is generated independently of the results of
the other learning problems. i.e. results from one problem will not aﬀect the search
space of another.
3. Depending on the overall concept, the results can be combined in an exploratory
manner, and even if not, some results can be discarded during recombination.
These points show that the idea used within Dilum is neither an exploratory search nor a
greedy search, but some combination of the two. Areas of search space must be excluded
without exploration from the learning process or the system will be unable to produce a
result. However, the system engineer has the ability to inﬂuence which areas of search
space are explored by the manner in which they divide up the concept to be learnt and
the background knowledge that they provide. In addition, the search method used by the
learner is employed wherever possible.
5.7
Summary
Within this chapter, the ideas presented within Dilum were developed further by applying
the system to an alternate learning problem and examining the diﬀerences between the
work required to set up the problem and the work required in the previous chapter. From
the results of this comparison, an outline of the characteristics that a concept should
display for Dilum to be considered was constructed, and the items that the system engineer
would have to provide in order to use the system were identiﬁed.
The next chapter will focus in the partitioning stage of Dilum and clarify situations in
which this pre-processing method should be applied.

Chapter 6
Learning Complex Vague
Concepts
TILDE was able to produce successful results for the model learnt within Chapter 5 without
requiring the data to be partitioned as a pre-processing step. However, if TILDE learnt
a theory for each classiﬁcation as a separate concept, then the understandability of the
model produced was improved. Aleph was unable to produce a theory whilst using the
whole data set in a positive only manner, but was able to produce theories for the simpler
concepts once each concept was learnt individually. Unlike the model in the Chapter 4,
the set of concepts to be learnt were not independent of each other, which is why, when
learning a theory for one classiﬁcation, the examples of other classiﬁcations could be used
as negative examples, and why TILDE was able to produce a model with a high level of
recall without assistance.
When is partitioning a set of examples for a concept given to a particular learner essential
for the successful construction of a theory within the time allowed? For which problems is
partitioning not essential, but still beneﬁcial to the understandability of the constructed
theory? The manner in which each learner is aﬀected by the data set for a particular
concept may diﬀer depending on the learner, therefore the purpose of this chapter is to
identify the characteristics of combinations of concepts, learners and hypothesis language
(collectively referred to in this chapter as ‘learning problems’) that fall into these two
categories. The learner is included in this deﬁnition of a learning problem because diﬀerent
learners will approach the concept to be learnt in diﬀerent ways, hence some learners will
not encounter the same diﬃculties when required to learn a particular type of concept.
135

CHAPTER 6. LEARNING COMPLEX VAGUE CONCEPTS
136
6.1
Learning Problems that Beneﬁt from Partitioning of the
Data Set
An illustration of how easily a concept can become diﬃcult to learn is given by the following
pair of concepts:
1. A relation over a pair of tasks that is applied to values that can be assigned a
category, for example, ‘X is a person’ or ‘Y is a company’. Two tasks are related if
an object mentioned in one task is located at a place mentioned in the other task.
2. A relation over pairs of task attributes that should be given a range, such as task
times and durations. The deﬁnition of these attributes given earlier in the disseration
still gives them only a ﬁnite number of discrete values, but the only method available
to split them into groups in the same manner as the ﬁrst relation is to deﬁne a range
rather than list a category. Two tasks are related if the diﬀerence between their
durations and the diﬀerence between their preferred times both fall within the same
range of values. This relation is unlikely to be used in a real user modelling system,
at least not in the form presented here. However it is useful as a simpliﬁed example
of the circumstances under which a concept becomes diﬃcult to model using ILP
without partitioning the data set.
The training set for each concept contained a set of positive examples, that represented
three distinct partitions within the example space, plus negative examples that had been
generated from these positive examples using the generation method detailed in Chapter
4, i.e. by replacing one value in each positive example with a value that placed the new
example outside the area of example space occupied by the partition, thus creating a set
of ‘near misses’ to guide the learner towards the required theory. The task of the learner
was to ﬁnd an appropriate theory to describe the areas of example space inhabited by the
positive examples.
Each learning problem was given to Aleph and TILDE to see whether the learners were
able to produce the required theories.
Aleph used a heuristic search and a ‘coverage’
evaluation function. The theories produced by Aleph and TILDE with and without the
use of partitioning as a pre-processing step were compared for each learning problem.

CHAPTER 6. LEARNING COMPLEX VAGUE CONCEPTS
137
6.1.1
A relation over two tasks that works with categorisable values
The positive examples contained within the data set for this concept were:
wc(taska,taskb).
wc(taskh,taski).
wc(taskn,tasko).
..etc.
Each task name referred to a task contained within the background knowledge:
task(taska,meeting,jo,na,1/0,13-0,na).
task(taskb,visit,axis,na,2/0,13-30,na).
task(taskh,review,tigrith,na,2/0,13-0,na).
task(taski,travel,london,na,1/0,16-0,na).
task(taskn,travel,reading,na,0/30,9-0,na).
task(tasko,seminar,taims,room_a,4/0,9-30,na).
..etc.
The background predicates made available to Aleph and TILDE enabled each learner
to determine whether a task contained an instance of a particular group of values (e.g.
whether a task contained a value that referred to a project) and assign that value to a
new variable in the clause under construction.
:- modeh(wc(+task,+task)).
:- modeb(1,eq(+name,#name)).
:- modeb(person(+task, -name)).
:- modeb(company(+task, -name)).
:- modeb(project(+task, -name)).
:- modeb(location(+task, -name)).
:- modeb(room(+task, -name)).
:- modeb(located_at(+name,+name)).
The ﬁnal relation that joined the two tasks could then be applied to the output variables
of the literals that occured earlier in the body of the clause. The three clauses that the

CHAPTER 6. LEARNING COMPLEX VAGUE CONCEPTS
138
+
+
+
+
+
+
+
+
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−−
−
−
−
−
−
−
−
−
−
−
−
attribute
Task B
−
−
−
−
−
−
−
−
−
Task A attribute
−
−
−
−
−
−
−
−
−
−
−
−
−
−
+
−
−
−
−
−
−
−
−
−
−
project
company
location
room
person
project
company
location
room
person
Figure 6.1: Concept 1: A relation linking two tasks that works with categorisable values
learners were required to produce are shown below (NB: located at/2 is not a symmetrical
relation):
wc(A,B):- person(A,C),company(B,D),located_at(C,D).
wc(A,B):- project(A,C),location(B,D),located_at(C,D).
wc(A,B):- location(A,C),room(B,D),located_at(D,C).
Figure 6.1 gives an impression of the pairs of task attributes that the relation is expected to
cover. From the representation it appears that it is possible for both positive and negative
examples to appear in the same area of the example space. However those examples with a
negative classiﬁcation do not contain values that are covered by the relation located at/2.
The only available method of distance measurement between examples was whether they
shared particular values or classiﬁcations for particular values. This was therefore the
only method of distance measurement that was reﬂected by the predicates within the
background knowledge, and the order of the possible categories of task attribute as shown
in Figure 6.1 is arbitrary.

CHAPTER 6. LEARNING COMPLEX VAGUE CONCEPTS
139
The three clauses that Aleph produced when presented with the training set containing
the data for all three groups of examples within the concept were:
[Rule 1] [Pos cover = 3 Neg cover = 0]
wc(A,B):- person(A,C), company(B,D), located_at(C,D).
[Rule 2] [Pos cover = 3 Neg cover = 0]
wc(A,B):- project(A,C), location(B,D), located_at(C,D).
[Rule 3] [Pos cover = 3 Neg cover = 0]
wc(A,B):- room(B,C), location(A,D), located_at(C,D).
Evaluation of possible candidate clauses reﬂected the fact that, as well as excluding a large
number of the negative examples, as soon as a literal was added to a clause, some of the
positive examples within the data set were also no longer covered. For Aleph this was not a
problem, after exploring a few areas of the search space that did not contain an appropriate
hypothesis, the appropriate clause was found. TILDE produced a theory that had a good
level of recall with regards to the given data, but was not easy to understand and had a
high level of overﬁtting. Whilst recall is of great importance, the understandability of the
model produced is also an important concern. As TILDE’s aim was simply to split the
examples with diﬀerent classiﬁcations, it focused on using some of the given background
knowledge to classify some areas of the example space as ‘negative’, instead of attempting
to isolate areas of the example space that could be classiﬁed as ‘positive’.
Part of the cause of the theory that TILDE produced was the small size of the data sets
used, but the other cause is myopia, a well-known problem amongst decision tree-learners.
During the process of constructing a decision tree that would explain the given data set,
the evaluation score of each literal that could have been used to construct a node was
based on how well it split the section of the data set currently covered by that node into
the two possible classiﬁcations. As the three groups of positive examples were located
in diﬀerent areas of the example space, and hence could not be covered by the same
literals, the evaluation score of each literal was aﬀected by TILDE’s inability to isolate the
examples with a positive classiﬁcation. The eﬀect of the examples on the evaluation scores
led TILDE to include nodes that isolated groups of negative examples instead because the
literals used to construct these nodes gave the highest information gain. There have been
several solutions put forward to combat this problem, including adding a small amount of
lookahead into the decision making process (as used by LFC [Ragavan and Rendell, 1993]),
and making the learner focus on smaller areas of the example space when deciding which
action to perform.
RELIEFF [Kononenko et al., 1997] implements this second idea by
taking a random example, the k nearest examples with the same classiﬁcation and the
k nearest examples with a diﬀerent classiﬁcation, and determining, on the basis of this
subset of the data set, the most appropriate attribute to produce a decision node for.

CHAPTER 6. LEARNING COMPLEX VAGUE CONCEPTS
140
Partitioning the data set as a pre-processing step has greater similarity to this second
solution due to the idea of focusing on particular areas of the example space. However,
RELIEFF is aimed at producing a single overall theory for the data set by propagating
the ﬁndings from various local contexts to a global context, whereas partitioning is more
useful when it is known that the production of separate theories is more appropriate.
For both of the concepts learnt in this chapter, the key fact about the examples presented
and the background knowledge available that aﬀected their performance was that none of
the background predicates could be used to construct a single clause that would cover all
of the positive examples and none of the negative examples within the data set. As soon
as the most general clause was specialised in an attempt to exclude negative examples
then the number of positive examples covered was reduced. For a simple concept such as
the one attempted here, this did not present a signiﬁcant problem to an exploratory ILP
algorithm such as Aleph because the hypothesis space was very small and the amount of
search space that it had to explore in order to ﬁnd a suitable clause for each group of
positive examples was small. TILDE was more adversely aﬀected due to its inability to
backtrack.
This simple concept gives an example of the sort of learning problem for which partitioning
as a pre-processing step may provide beneﬁts, i.e. a learning problem for which the data
set consists of separate groups of examples that would ideally each have their own clause or
theory and where coverage of the areas of example space between these groups that have
no known classiﬁcation should be avoided for some reason (for example, it could aﬀect
the model’s recall), even if these areas are quite small. A learner such as Aleph would
not (and indeed did not) require assistance for a concept as simple as the one described
in this section. Aleph is capable of producing its own disjunctive theories. However, if
the concept becomes more complex, and hence the amount of search space that must be
explored before a suitable answer is found grows larger, Aleph will eventually run out of
given resources (e.g. time) during the search and be forced to return its best answer so far.
The next learning problem demonstrates how easily the size of a search space of possible
hypotheses can grow just by altering the nature of the arguments focused on within the
concept.

CHAPTER 6. LEARNING COMPLEX VAGUE CONCEPTS
141
6.1.2
A relation over pairs of task attributes that should be given a range
The positive examples within the data set for this concept consisted of the same sort of
references to pairs of tasks that were contained within the background knowledge as for
the previous concept:
ac(taska,taskb).
ac(taskh,taski).
ac(taskn,tasko).
..etc.
task(taska,meeting,ben,na,1/0,13-0,na).
task(taskb,visit,bt,na,2/0,13-30,na).
task(taskh,review,idiary,na,3/0,13-0,na).
task(taski,travel,london,na,1/0,16-0,na).
task(taskn,travel,reading,na,0/30,9-0,na).
task(tasko,seminar,taims,room_a,4/0,12-30,na).
..etc.
The size of the data set was larger than for the previous concept (each group consisted of
27 examples) because this concept was more complicated for the learner. The aim of the
data set generated was to give a full representation of the concept to be learnt and avoid
any peculiarities that could give misleading results due to presenting only a very small
amount of information to the learner. The background predicates made available to Aleph
for this concept referred to the diﬀerence in preferred time and duration because this was
now the method of distance measurement available between examples:
:- modeh(1,ac(+tt,+tr)).
:- modeb(1,pt_diff2(+tt,+tr,-pl)).
:- modeb(1,dur_diff2(+tt,+tr,-al)).
:- modeb(5,ptlessthaneq(+pl,#pt)).
:- modeb(5,ptgreaterthaneq(+pl,#pt)).
:- modeb(5,lessthaneq(+al,#at)).
:- modeb(5,greaterthaneq(+al,#at)).

CHAPTER 6. LEARNING COMPLEX VAGUE CONCEPTS
142
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
Difference in task duration
−
6−00
5−30
5−00
4−30
4−00
3−30
3−00
0−30
1−00
1−30
2−00
2−30
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
0−00
2/00 2/30 3/00 3/30 4/00 4/30
−
−
−
−
−
−
−
−
−
−
−
−
−
−
−
task start time
5/00
1/30
1/00
0/30
0/00
5/30 6/00
Difference in
Figure 6.2: Concept 2: A relation over pairs of task attributes that should be given a
range
Rather than give predicates that returned the duration or preferred time of a given task,
since each task was guaranteed to have a value for each of these attributes, the target
concept was made simpler by providing predicates that would return the diﬀerence between
these attributes for two given tasks. Once these predicates had been applied to a group,
then the learner would only have to determine the ranges of the diﬀerence in attribute
values that each group of examples demonstrated. The three clauses that the learners
were required to produce were:
ac(A,B) :- dur_diff2(A,B,C), lessthaneq(C,1/30), greaterthaneq(C,0/30),
pt_diff2(A,B,D), ptlessthaneq(D,1-30), ptgreaterthaneq(D,0-30).
ac(A,B) :- dur_diff2(A,B,C), lessthaneq(C,3/0), greaterthaneq(C,2/0),
pt_diff2(A,B,D), ptlessthaneq(D,3-0), ptgreaterthaneq(D,2-0).
ac(A,B) :- dur_diff2(A,B,C), lessthaneq(C,4/30), greaterthaneq(C,3/30),
pt_diff2(A,B,D), ptlessthaneq(D,4-30), ptgreaterthaneq(D,3-30).
Figure 6.2 gives an impression of the envisaged relation between attribute ranges for the
concept to be learnt. The learners were allowed to substitute several diﬀerent values in
place of the ground second arguments of lessthaneq/2, greaterthaneq/2,

CHAPTER 6. LEARNING COMPLEX VAGUE CONCEPTS
143
ptlessthaneq/2, and ptgreaterthaneq/2 (the predicates referring to diﬀerent attributes
were given slightly diﬀerent names to make the results produced easier to understand) to
generate several diﬀerent possible literals for use during the construction of a hypothesis
in order to ease the overﬁtting that could occur.
Aleph produced three clauses when presented with the data set for this concept:
ac(A,B) :-
dur_diff2(A,B,C), lessthaneq(C,1/30), greaterthaneq(C,0/30),
pt_diff2(A,B,D), ptlessthaneq(D,1-30), ptgreaterthaneq(D,0-30).
ac(A,B) :-
dur_diff2(A,B,C), lessthaneq(C,3/0), greaterthaneq(C,2/0),
pt_diff2(A,B,D), ptlessthaneq(D,3-0), ptgreaterthaneq(D,2-0).
ac(A,B) :-
dur_diff2(A,B,C), lessthaneq(C,4/30), greaterthaneq(C,3/30),
pt_diff2(A,B,D), ptlessthaneq(D,4-30), ptgreaterthaneq(D,3-30).
An average of 20 literals were used to construct the bottom clause from each of the
examples chosen during the learning process. Aleph explored quite a few areas of the
search space that did not contain the required hypothesis before ﬁnding the optimum
clause in each case. However, when the bound on the non-determinacy of each form of
predicate call (the recall number of each predicate within the background knowledge) was
increased from 5 to 6, then the number of literals in the bottom clause increased, and
search space available to Aleph to explore was increased to such an extent that a slightly
diﬀerent result was produced:
ac(A,B) :-
pt_diff2(A,B,C), ptlessthaneq(C,3-30), ptgreaterthaneq(C,3-30),
dur_diff2(A,B,D), lessthaneq(D,4/30), greaterthaneq(D,3/30).
[total clauses constructed] [9128]
ac(A,B) :-
pt_diff2(A,B,C), ptlessthaneq(C,4-30), ptgreaterthaneq(C,3-30),
dur_diff2(A,B,D), lessthaneq(D,4/30), greaterthaneq(D,3/30).
[total clauses constructed] [3361]
ac(A,B) :-
pt_diff2(A,B,C), ptlessthaneq(C,1-30), ptgreaterthaneq(C,0-30),
dur_diff2(A,B,D), lessthaneq(D,1/30), greaterthaneq(D,0/30).
[total clauses constructed] [977]

CHAPTER 6. LEARNING COMPLEX VAGUE CONCEPTS
144
ac(A,B) :-
pt_diff2(A,B,C), ptlessthaneq(C,3-0), ptgreaterthaneq(C,1-30),
dur_diff2(A,B,D), lessthaneq(D,3/0), greaterthaneq(D,2/0).
[total clauses constructed] [1763]
[total clauses constructed] [32356] <-- this includes additional search time
(explained below)
An average of 25 literals were used to construct the bottom clause from each of the
examples chosen during the learning process. Whilst the above result does not appear
to be too far from the requested theory, it is possible to produce a slightly closer version
within a shorter amount of time. One of the diﬃculties encountered by Aleph was that,
after having constructed a clause that compactly covered a particular group of examples, it
would then be unable to stop searching at this point and begin again with a new example
and bottom clause. Instead, it would continue to search fruitlessly for a better clause until
either it could reason that no better clause could be found, or the set limit of clauses to
be evaluated was reached. The number of clauses evaluated, or the amount of time taken
to complete the search, can be given a tight limit that imposes a strict deadline on the
amount of time taken to ﬁnd the required clause, but this increases the risk of failing to
ﬁnd the required theory and introduces a diﬃcult question, where should the limit be set?
If the only positive examples to be covered in the learning problem presented to Aleph
were those of a particular group of examples, then Aleph would be able to stop searching
once it had found the required clause because there would be no more positive examples
within the data set that required coverage.
The three sets of examples were presented to Aleph as separate learning problems, no
other changes to the learning setup were made and so the size of the hypothesis space for
each learning problem remained the same. Aleph produced the following results:
ac(A,B) :-
dur_diff2(A,B,C), lessthaneq(C,4/30), greaterthaneq(C,3/30),
pt_diff2(A,B,D), ptlessthaneq(D,4-30), ptgreaterthaneq(D,3-30).
[total clauses constructed] [3623]
ac(A,B) :-
dur_diff2(A,B,C), lessthaneq(C,1/30), greaterthaneq(C,0/30),
pt_diff2(A,B,D), ptlessthaneq(D,1-30), ptgreaterthaneq(D,0-30).
[total clauses constructed] [495]

CHAPTER 6. LEARNING COMPLEX VAGUE CONCEPTS
145
Concept
Number of clauses evaluated before target clause discovered
Heuristic Search
Depth-ﬁrst Search
Full
Partitioned
Full
Partitioned
1
977
495
7486
495
2
1764
1530
11956
1529
3
15344
3623
3186
3623
Table 6.1: Comparison of search space explored with full and partitioned data sets
ac(A,B) :-
dur_diff2(A,B,C), lessthaneq(C,3/0), greaterthaneq(C,2/0),
pt_diff2(A,B,D), ptlessthaneq(D,3-0), ptgreaterthaneq(D,2-0).
[total clauses constructed] [1530]
There was a signiﬁcant reduction of the overall amount of search space explored when
generating a theory using separate groups of examples. Since the only diﬀerence between
the two learning problems was the presence or absence of other positive examples, and the
heuristic search used an evaluation function that only considered coverage to guide the
direction of the search, this would suggest that the presence of other positive examples
within the data set altered the evaluation score of the candidate clauses within the search
space in such a way as to direct the learner to areas of space that did not contain the target
clause. Ordinarily this would not have made a diﬀerence to the results returned, but when
the hypothesis space is so large, the result of a slight diﬀerence in the order in which the
space is searched will be magniﬁed and so is more likely to have visible results. Choosing
to reﬁne a clause with one literal that is slightly diﬀerent to the literals contained within
the required target clause can mean that a signiﬁcant additional amount of search space
is explored before the learner returns back to that point and chooses to reﬁne the clause
containing the correct version of the literal. This idea will be expanded upon in the next
section.
Another learning problem was attempted to compare the amount of search space that
was explored by Aleph when presented with the complete data set and when presented
with the three separate partitions. Since the depth-ﬁrst search used by Aleph also made
use of the evaluation score of the possible literals, Aleph’s performance using this search
method was also evaluated. The results of the learning attempt can be seen in Table 6.1.
The learner was allowed to evaluate a much larger number of potential clauses in an eﬀort
to identify precisely how much of the search space it needed to explore before it found a
clause that would ﬁt the data. The results show the extent of the search space exploration
reduction introduced by partitioning the data set.
The results for the construction of the third concept using a depth-ﬁrst search bring
to light how complex the relationship between the examples within the data set and the
evaluation scores of the candidate clauses can be. It shows that not all concepts within the

CHAPTER 6. LEARNING COMPLEX VAGUE CONCEPTS
146
data set may beneﬁt from the use of partitioning, although if the data set is ‘well rounded’,
i.e. contains enough positive and negative examples to fully describe the target concept,
then the detrimental eﬀect will only be slight. The number of negative examples that
contained attribute diﬀerences that were greater than the required range of the group of
examples representing the third concept (the top right group of positive examples shown in
Figure 6.2) was considerably smaller than the number that contained attribute diﬀerences
that were smaller than the required range of that group.
When the middle group of
positive examples in Figure 6.2 was present, Aleph was encouraged to search amongst the
literals additionally covering those examples ﬁrst because of their higher evaluation scores,
and hence began to search amongst the candidate clauses that contained specialisations
in both preferred time and duration from quite an early stage.
Once an appropriate
specialisation had been found in those two directions then Aleph focused on the other two
directions. Since the only task that remained was to exclude these remaining negative
examples, and Aleph had already reasoned that none of the other positive examples could
be covered, the amount of search space to be explored to ﬁnish the clause was not very
large. However, if the positive examples of other group were not present, then the learner
did not receive as much encouragement to focus on both dimensions at once and began by
exploring amongst the candidate clauses that specialised only one dimension before starting
to consider including the literals that belonged to the other dimension. Unfortunately,
adding literals to the clause that excluded the area containing only a small number of
negative examples did not improve the candidate clause’s evaluation score very much.
The literals that Aleph found more appealing were those literals that excluded negative
examples that should be excluded using predicates referring to the other dimension, and
also excluded some of the positive examples.
Since the groups of examples used within these learning problems had artiﬁcially gener-
ated negative examples, it was possible to reduce this eﬀect by providing more negative
examples in the areas that were sparsely populated. As the introduction of partitioning
into a data set would require some notion of what a partition is for that particular concept,
it should be possible for the probability of this problem occurring to be reduced in most
cases by the addition of some negatively classiﬁed examples into areas known not to be
occupied by the partition currently being focused on.
What about Aleph’s own techniques for working with predicates that require one or more
arguments to be instantiated to a constant? Aleph is equipped with a technique called
lazy evaluation [Srinivasan and Camacho, 1999].
This allows it to delay the instantia-
tion of constants within ⊥and instead use literals with Skolem constants positioned in
the arguments where they should appear. The explanation given in the Aleph manual
[Srinivasan, 2001] says:
. . . During the search, the literal is called with a list containing a pair of lists
for each input argument representing ‘positive’ and ‘negative’ substitutions ob-

CHAPTER 6. LEARNING COMPLEX VAGUE CONCEPTS
147
tained for the input arguments of the literal. These substitutions are obtained
by executing the candidate clause without this literal on the positive and neg-
ative examples. The user needs to provide a deﬁnition capable of processing a
call with a list of list-pairs in each argument, and how the outputs are to be
computed from such information. . .
Unfortunately, the domain of the learning problem currently being addressed would require
lazy evaluation for almost all of the literals contained within ⊥, the only exception being
use of eq/2. This leads to a problem when attempting to discriminate between positive
and negative values because these lists will contain contradictory evidence. For example,
suppose the example for which ⊥is constructed is ac(taskh,taski) (as shown in Figure
6.2), which has a duration diﬀerence of 2/0 and a preferred time diﬀerence of 3-0. The
learner attempts the ﬁrst of the literals that require lazy evaluation, greaterthaneq/2
applied to the result of dur diff/3. At this point the candidate clause only contains
literals that produce the values to be range-restricted. The following lists are generated:
[[0/30,1/0,1/30,2/0,2/30,3/0,3/30,4/0,4/30],
<-- positive values
[0/0,0/30,1/0,1/30,2/0,2/30,3/0,3/30,4/0,4/30,5/0]
<-- negative values
]
It can clearly be seen that the list of negative values contains all of the values within the
list of positive values. As greaterthaneq/2 is used to deﬁne a point at which values less
than a speciﬁc value are not part of the target concept, there is no way to reason where
that point should be using only the above lists of values. If a specialisation of the range
of the preferred time diﬀerence had already been added to the clause, then the two lists
of values given would have contained the necessary information to produce an appropriate
value:
[[2/0,2/30,3/0],
<-- positive values
[0/0,0/30,1/0,1/30,3/30,4/0,4/30,5/0]
<-- negative values
]
However, attempting to add literals referring to the diﬀerence in the preferred time of
each task encounters the same problem, and so this state cannot be reached.
Given
this dependency between the arguments of ac/2, experiments using lazy evaluation were
unsuccessful for this learning problem.
The models built in Chapter 4 and Chapter 5 both contained more than one attribute
that required this form of range restriction. The ﬁrst model required restrictions for the
durations of both tasks and the second had the possibility of predicates of this type being
applied to the times of both tasks in addition to their application to the task durations.

CHAPTER 6. LEARNING COMPLEX VAGUE CONCEPTS
148
After the data set had been partitioned, the problem of possible ranges of values within
these attributes contradicting each other still remained, hence the additional level of split-
ting after the initial process of splitting the learning problem into independent dimensions.
Lazy evaluation could be incorporated into the very ﬁnal stage, the actual learning of small
parts of the overall theory, however once the learning problem has become so small the
technique would not oﬀer a signiﬁcant improvement.
TILDE was also given this concept to attempt to learn but only managed to produce the
same sort of results as for the previous concept, i.e. a theory with higher complexity and
a greater number of clauses than the target theory that overﬁtted the given training set.
Partitioning the data and presenting the groups of examples separately enabled TILDE to
produce the envisaged theory with a marked improvement in understandability over the
original set of results.
The second concept to be learnt was used as the most obvious example of the sorts of
learning problems that could produce large hypothesis spaces very easily. However the
background predicates that produce this sort of problem need not be numerical, hence
other speciﬁc techniques for producing numerical literals (e.g. [Anthony and Frisch, 1997])
will not solve all the cases where this diﬃculty occurs. Other attributes that may cause
such problems are those for which a description may be somewhat ‘fuzzy’ in nature, colour
(names, rather than Red-Green-Blue composition, as names are more easily understood)
is a good example of this.
6.1.3
Summary
The results of the learning problems attempted indicate that the presence of more than one
distinct group of examples within the data set will aﬀect the expected utility of exploring
various parts of the search space, hence altering the disposition of the learner towards the
order in which candidate clauses are reﬁned. In the case of the greedy learner, focusing on
the wrong area of the search space will prevent it from producing the optimum theory. For
an exploratory learner, the cost of focusing on the wrong area is not quite so debilitating
because it can eventually decide to abandon that area and explore somewhere else. How-
ever, as the size of the search space increases, the learner will become less able to manage
a conclusive exploration within the time allowed.
One of the sets of circumstances in which this situation, where the learner is initially
directed towards an area of the search space that does not hold the target clause, will
occur, is:
1. the hypothesis language from which the target clause is constructed contains pred-
icates that can occur several times per argument, with diﬀering generality in each
case,

CHAPTER 6. LEARNING COMPLEX VAGUE CONCEPTS
149
2. the target clause contains more than one of these literals, and
3. the additional area of the example space covered by some of the more general versions
of the predicates contains positive examples from at least one other group of positive
examples.
Even if the overall search space is too large to be explored extensively by the learner
in the time allowed, if all of the literals that it is given to work with exclude areas of
the example space that contain the other groups of positive examples, then the learner
is forced to focus on covering one speciﬁc area, thus quickly pruning areas of the search
space. If multiple instances of predicates of varying generality are used, then the learner is
presented with the possibility of covering more than one group of positive examples, hence
the amount of search space that could be immediately pruned is reduced. The extent of
this reduction will depend on the learning problem and concept representation that the
learner is presented with, however, if the distance between the groups of examples is not
very large then the eﬀect will be more apparent due to the evaluation scores of a greater
number of candidate clauses within the search space being aﬀected.
The concepts to be learnt in Chapter 4 were composed of two arguments that were spe-
cialised using predicates that could be varied in their generality and six arguments that
were not. If none of the predicates within the hypothesis language had varying degrees of
generality, and hence could not be included within the hypothesis language several times
per argument that they could be applied to, then the size of the search space of possible
hypotheses would have been much smaller, although still larger than the size of the search
space for the ﬁrst learning problem in this chapter. The inclusion of two arguments that
were to be specialised using predicates that could be included several times within the
hypothesis language increased the size of the search space far beyond the capability of the
learner to ﬁnd the required clause within the time limit speciﬁed. Partitioning the data set
removed the inﬂuence of other partitions of examples on each learning problem presented
to Aleph, but the size of the search space that had to be explored was still much larger
than could be conclusively explored in the time available, hence the learning problem was
split still further into independent dimensions. However, simply using the second approach
of splitting the learning of the model in Chapter 4 into separate dimensions without par-
titioning the data ﬁrst would not have enabled the learner to produce the required theory.
If the entire data set was presented to the learner for each dimensional learning problem,
then the values contained within that dimension for the negative examples produced from
the positive examples would contradict other positive examples. Without the ability to
specialise other dimensions and so partition the data set in such a way as to isolate groups
of positive and negative examples, the learner would be unable to produce a theory for
that dimension. Partitioning the examples into separate groups allowed the learner to
use the additional background knowledge that each group occupied a speciﬁc area of the
example space that could be entirely covered by a constructed theory. This reduced the

CHAPTER 6. LEARNING COMPLEX VAGUE CONCEPTS
150
complexity of the learning problem by removing the contradictions mentioned earlier. The
learning problem was transformed into the task of ﬁnding appropriate specialisations for
each dimension that excluded all areas of the example space other than the one currently
occupied by that particular group of examples.
Having discussed the ideas that have thus far been demonstrated in this chapter, a list
of properties can be given that a learning problem should display in order for the use
of partitioning as a pre-processing step to be considered as either essential or at least
beneﬁcial:
1. The concept should consist of groups of examples that, if the learner were to be given
the entire data set, then it would construct separate clauses or theories to cover each
one, (i.e. a disjunctive concept);
2. The hypothesis language should be incapable of guaranteeing that the required split-
ting of the groups of examples will be performed in an eﬃcient manner, i.e. it would
have the properties listed earlier;
3. If the learner is exploratory then the search space of possible hypotheses should be
of suﬃcient size that the learner cannot perform a conclusive search of the space
before exhausting its given resources (e.g. a bound on the time available); and
4. The negative examples within the data set should surround the groups of examples
in a suﬃciently balanced manner such that the learner is not misled once again once
the inﬂuence of the additional groups of positive examples have been removed.
As mentioned previously, in order to carry out the partitioning process, the system will
require information as to what is and what is not a part of the concept to be learnt.
The amount of information that this requires will depend upon the application and the
approach required, but the applications for which this approach is more likely to be eﬀective
are those where the sorts of concepts to be learnt are roughly known in advance but the
actual model generation is required to be autonomous.
If Dilum is required for a learning problem, then if the concept displays the ﬁrst property in
the list, partitioning of the data into separate subconcepts is essential to allow the second
stage of Dilum (i.e. splitting the learning problem into smaller independent subproblems)
to be carried out successfully.
In order to assume independence between the diﬀerent
subproblems, the process should be made to focus on producing a theory that will cover
a single area of the example space.

CHAPTER 6. LEARNING COMPLEX VAGUE CONCEPTS
151
Two beneﬁts of using partitioning as a pre-processing step have been identiﬁed:
• The eﬀect of other groups of positive examples on the search strategy of the learner
is removed,
• the learner no longer continues to explore the search space after ﬁnding an appro-
priate clause that ﬁts the data, and
However, there are also some disadvantages:
• Unless the overall concept to be learnt is known to be disjunctive then the hypothesis
space is rendered incomplete, and
• If the negative examples surrounding each group of positive examples cannot be
guaranteed to be roughly balanced in all directions then there is a risk that the
learner will explore more of the search space than if partitioning were not used.
If the learning problem consists of positive examples with artiﬁcially generated negative
examples, or if the method of partitioning can be used to add extra negative examples
due to the introduced deﬁnition of what a partition is, then this second problem can
be avoided. The ﬁrst problem should also be avoided if the earlier lists of properties of
learning problems are adhered to and if the data is partitioned into the groups of examples
that should be covered by each clause of the target disjunctive concept.
6.2
The Eﬀect of Partitioning on Eﬃcient Learning and
Concept Learnability
The problems for which this method of pre-processing is being considered are those that
have a limited amount of time in which a model can be constructed as this is the constraint
that caused Aleph to fail in earlier learning attempts. If a target concept exists within
the hypothesis space described by the hypothesis language then, if fast learning is of
importance, a learner should ideally only use an amount of time that is polynomial in the
size of the data set in which to ﬁnd it. However, for some applications, this requirement is
not strict enough; the polynomial in question could still be large enough that the learner
is unable to function with the time constraints imposed. Some of the ideas used within
the area of learnability, particularly PAC-learnability [Valiant, 1984], can help to give a
clearer picture of how using partitioning as a pre-processing method reduces the amount
of time required to construct a model from the given data set.
In the classical PAC (probably approximately correct) model, the learner has access to an
oracle that it can ask for examples. Let X be deﬁned as the set of all possible examples
and n be the length of each x ∈X. On each call to the oracle it receives a labelled example

CHAPTER 6. LEARNING COMPLEX VAGUE CONCEPTS
152
(x, f(x)) where x ∈X is drawn independently according to a ﬁxed distribution D over X
that is unknown to the learning algorithm, and f ∈F is the target concept drawn from
the class of concepts F. The following deﬁnition is taken from Aslam & Decatur, but is
given in roughly the same form in many papers on PAC-learnability.
Let F and H be 0, 1-valued function classes deﬁned over X.
The class F
is said to be learnable by H if there exists a learning algorithm A and a
polynomial p1(., ., ., .) such that for any f ∈F, for any distribution D on X,
for any accuracy parameter ϵ, 0 < ϵ ≤1, and for any conﬁdence parameter
δ, 0 < δ ≤1, the following holds: if A is given inputs ϵ and δ, and access to
an example oracle EX(f, D), then with probability at least 1 −δ A halts in
time bounded by p1(1/ϵ, 1/δ, n, size(f)) and outputs a hypothesis h ∈H that
satisﬁes error(h) ≤ϵ.
[Aslam and Decatur, 1993]
Several results for PAC-learnability of various classes of problems using ILP have been
given. Before beginning to discuss these results it should be noted that Dˇzeroski [1995]
argues that the results described will still be applicable in the presence of noisy data, a
essential requirement for user modelling since the data set will almost never contain perfect
examples. The argument put forward is that since most positive PAC-learnability results
have relied on algorithms for PAC-learning propositional k-DNF formulae or predicting k-
term DNF formulae, and as noise tolerant variants of these algorithms exist, noise tolerance
can be achieved in the normal ILP setting by using these noise-tolerant algorithms instead
of the classical ones.
Muggleton and Feng [1990] have shown that a single ij-determinate Horn clause is learnable
in polynomial time using ground background knowledge, and ground unit clause examples.
Dˇzeroski, Muggleton, and Russell [1993] showed positive results for k-literal predicate def-
initions consisting of constrained, function-free, nonrecursive program clauses by reducing
the learning problem to a standard propositional learning problem. A clause is constrained
if each of the variables in the head of the clause also appear in the body of the clause.
Other work [Dˇzeroski et al., 1992] showed that these restrictions could be relaxed so that
determinate, function-free, nonrecursive logic programs could be learnt by restricting the
class of distributions under which learning was required to simple distributions. Since this
class of distributions encompasses all enumerable distributions, it was argued that this
would cover almost all learning problems that would be encountered.
The target clauses of the second example concept shown in the previous section can be
translated into a constrained form. Each clause within the disjunctive concept requires
the presence of literals that calculate the new values from the existing variables. Since
the presence of these two literals within each clause is prior knowledge, it is possible

CHAPTER 6. LEARNING COMPLEX VAGUE CONCEPTS
153
to translate the given examples so that the results returned by these two literals are
included in the set of values presented by example. This reduces the learning problem to
providing specialisations for variables located in the head of the clause, hence the clauses
to be learnt have been transformed into constrained clauses and the overall concept can
therefore be included within the class of concepts that have been shown to be learnable
within polynomial time [Dˇzeroski et al., 1993]. For example, the target clause:
ac(A,B) :-
dur_diff2(A,B,C), lessthaneq(C,3/0), greaterthaneq(C,2/0),
pt_diff2(A,B,D), ptlessthaneq(D,3-0), ptgreaterthaneq(D,2-0).
could be restructured, by adding the results of dur diff2/2 and pt diff2/2 into the head
of the clause, to produce:
ac(A,B,C,D) :- lessthaneq(C,3/0), greaterthaneq(C,2/0),
ptlessthaneq(D,3-0), ptgreaterthaneq(D,2-0).
How large a polynomial is the time complexity for this class of concepts? The algorithm
used by Dˇzeroski, Muggleton, and Russell [1993] is taken from [Haussler, 1988], which
gives the following bound for time complexity:
t ≤O(snkm2k+1 log m)
(6.1)
where
m is the size of the data set,
n is the number of attributes,
s is the number of clauses in the overall disjunction, and
k is the maximum number of literals in any clause within the target concept.
Using the statistics from the example in the previous section, this bound becomes a poly-
nomial (for ﬁxed k) that is larger than the level of learning eﬃciency required will tolerate.
If the number of literals that can be generated from each background predicate provided is
not restricted, then n becomes 192 (48 literals per predicate). Restricting this number to
6 literals per predicate per example within the data set reduces n to 47 (due to the overlap
between groups of literals for each example and the positioning of the examples within the
example space). The data set used was of a ‘reasonable’ size, consisting of 221 examples in
total. The maximum number of terms allowed within each clause was 8. These numbers
produce the following polynomial:
t ≤O(3 ∗478 ∗22117 ∗log 221)
(6.2)
Another algorithm put forward by Li and Vit´anyi [1991] appeared to have a simpler
complexity, but required that the number of examples used was at least nk+1. Inclusion of

CHAPTER 6. LEARNING COMPLEX VAGUE CONCEPTS
154
a data set of this size within the original time complexity equation caused the algorithm’s
time bound to display the same characteristics as those above due to the amount of time
requried to evaluate clauses against the data set. However, it can also be seen that the
time complexity of both algorithms would be reduced if partitioning were used as a pre-
processing step. The main eﬀects would be the reduction of m due to examples being
removed from the data set, and the reduction of n due to the set of examples within each
learning example all residing within roughly the same area, hence the sets of literals that
could be produced from each example would have the maximum amount of overlap. For
each clause, s would be reduced to 1, but the complexity of the overall concept would still
contain the original value of s since there would now be s learning problems to address.
The time complexity of an algorithm such as Aleph is diﬃcult to determine exactly due
to the inclusion of tools such as the use of search heuristics and pruning various areas
of the search space.
Dˇzeroski, Muggleton, and Russell [1993] said that their method
of transformation from an ILP problem to a propositional learning problem could not be
used to produce negative learnability results because of the level of sophistication that ILP
algorithms possess. More speciﬁcally, they state that they have translated the applicable
classes of ILP problems into propositional problems that are only polynomially larger
than the original ﬁrst-order representation, however it remains to be shown whether or
not a ﬁrst-order representation can provide exponential compression of a class of concepts.
The implication of these remarks is that if a ﬁrst-order learning problem is translated
into a propositional learning problem then this translated version will always be at least
polynomially larger than the original.
Therefore, a negative result for a propositional
translation of a ﬁrst-order learning problem cannot be used to imply a negative result for
the original ﬁrst-order version of that problem.
The time-bound of Aleph when attempting the earlier example can be shown to be smaller
than that of the propositional learning algorithm described earlier by looking at the size
of the search space that Aleph must explore. If Aleph was to search the entire space of
possible hypotheses, in a general to speciﬁc manner, using a simple search algorithm such
as a depth ﬁrst search, with no heuristic re-ordering and no pruning, the size of the search
space would be:
Search space size =
k
X
i=1
n!
(n −i)!
(6.3)
where
k is the search depth bound, the maximum length of clause allowed within the
hypothesis space, and
n is the number of literals within ⊥.
Unfortunately there is no simple closed form of this equation (see [MathFrm, 2000] for a
full explanation). If every example within the data set was evaluated against every clause,
then the number of evaluations that would take place would simply be the value above

CHAPTER 6. LEARNING COMPLEX VAGUE CONCEPTS
155
multiplied by the size of the data set. This would give a time complexity of:
t ≤O(smnn−k−1)
Substituting in the numbers used earlier would give:
t ≤O(3 ∗221 ∗2424−8−1)
which is equivalent to:
t ≤O(3 ∗221 ∗2415)
(6.4)
In practice, each clause would only be evaluated against the examples covered by the
clause that it is a reﬁnement of, but to keep this comparison simple, this will be ignored
for the time being.
Since the set of target clauses were transformed into a set of constrained clauses, it should
be possible to create a propositional learner that would mimic the actions carried out
by Aleph. This would result in a complexity that is as small as Aleph’s but no smaller
(which agrees with the statement made earlier by Dˇzeroski, Muggleton, and Russell [1993].
However the additional processing required to translate the problem from ﬁrst-order to
propositional form (and back again if required), whilst having been shown to only be
polynomial in the variables stated earlier, would still increase the amount of time required
to solve the problem using a propositional learner.
Since the number of literals within ⊥are only generated using one example (rather than all
the examples as the earlier propositional algorithm did), working with individual clusters
of examples will not reduce n. However, removing all but one of the clusters of positive
examples will still reduce the amount of search space explored because of the eﬀect that
the additional positive examples have on the manner in which the search space is explored.
In order to demonstrate the eﬀect of the presence of literals that cover more than one
group of examples within the bottom clause that Aleph constructs, Equation 6.3 must be
expanded into a more detailed form that includes more of the characteristics of the search
that Aleph would perform. The presence of more than one group of examples within the
data set will aﬀect the evaluation scores of the candidate clauses within the search space,
and hence will aﬀect those search methods available to Aleph in which these evaluation
scores are used to determine the order in which areas of the search space are explored.

CHAPTER 6. LEARNING COMPLEX VAGUE CONCEPTS
156
Aleph’s depth-ﬁrst and heuristic search methods will be focused on to show a generalised
view of the maximum amount of search space explored, as shown in Equation 6.5.
max space explored =
t−1
X
i=0

X
y∈Yi
zy +
X
n∈Ni
zn + 1


(6.5)
where
t is the length of the target clause,
i represents the length of the most specialised generalisation of the target clause
that has thus far been produced,
Yi is the set of all candidate clauses that, while not a generalisation of the target
clause, have a higher evaluation score than the best candidate clause that is
a generalisation of the target clause at that point in the search, i.e. all the
candidate clauses that will deﬁnitely be chosen before a clause that is a
generalisation of the target clause,
Ni is the set of all candidate clauses that have the same evaluation score as the
best candidate clause that is a generalisation of the target clause at that
point in the search. i.e. all the candidate clauses that could be chosen before
a clause that is a generalisation of the target clause, and
zy and zn both represent the number of extra clauses that will be evaluated due
to choosing to reﬁne a candidate clause that is not a generalisation of the
target clause.
Although the largest value for the outer sum in Equation 6.5 is only t −1, Yi and Ni will
include clauses of length longer than t. The equation is intended to capture the idea that
the learner will explore clauses of all lengths up to t, and whilst doing this the learner will
produce clauses that are generalisations of the target clause. Yi and Ni include all those
clauses that are evaluated between each search step in which the learner produces a clause
that brings it closer to producing the target clause.
An exact value for zy or zn would depend on the learning problem currently being at-
tempted and what stage of the search the learner was at. However, an upper bound would
be given by Equation 6.6.
max(za) =
k−i
X
j=1
(li −i −j)j
(6.6)
where za is either zn or zy,
k is the maximum length of clause that can be evaluated,
i is the length of the most specialised generalisation of the target clause that has
thus far been produced, and
li is the number of literals still available that could be used to generate reﬁnements
of the selected candidate clause.

CHAPTER 6. LEARNING COMPLEX VAGUE CONCEPTS
157
Equation 6.6 can be used when either a depth-ﬁrst or a heuristic search is used because
it represents the potential number of candidate clauses that could be explored whenever
a candidate clause is reﬁned. Each time a candidate clause is chosen that is not a gener-
alisation of the target clause, the potential to explore the reﬁnements of that clause (up
to the maximum clauselength allowed) are added to the amount of search space that the
learner could be required to evaluate. In practice, li will decrease as i increases due to
some literals being excluded due to their performance on generalised versions of the clause
being reﬁned.
Altering the evaluation scores of candidate clauses will aﬀect the size of both Yi and Ni,
thus altering the amount of search space explored. The presence of additional groups of
positive examples within the example space will increase the evaluation score of candidate
clauses that are not generalisations of the target clause.
This will therefore give the
potential for these candidate clauses to be selected for reﬁnement at an earlier stage in
the search, rather than being relegated to a latter position or pruned without further
reﬁnement. A variant of the idea of including a model of noise within the PAC learnability
model will be used in order to illustrate how these additional examples will aﬀect the sizes
of Yi and Ni.
The earlier description of PAC learnability mentioned that the learner would request ex-
amples from an oracle that would select examples according to some distribution, label
them with their correct classiﬁcation and then return them to the learner. Classiﬁcation
Noise [Angluin and Laird, 1988] occurs when some of the examples that are given to the
learner are not given their correct labels. Decatur [1996] gives a brief overview of the idea:
..the learner has access to a noisy example oracle EXη
CN(f, D). When a la-
belled example is requested from this oracle, an example is chosen according to
distribution D, and returned. With probability 1 −η, the correct labelling of
the example according to f is returned, while with probability η the incorrect
classiﬁcation is returned. The learner is given ηb, an upper bound on the noise
rate, such that 0 ≤η ≤ηb < 1/2. The running time of a learning algorithm is
allowed to be polynomial in
1
1/2−η in additional to the usual parameters. We
say that a class is learnable with classiﬁcation noise if it is learnable for some
constant classiﬁcation noise rate η > 0.
Other approaches that attempt to capture the characteristics of other types of noise
that could enter the data are Statistical Queries [Kearns, 1993] and Malicious Noise
[Valiant, 1985]. Whilst these approaches may appear to capture the nature of the noise
within the sorts of data sets that have been examined, they are more complicated than is
required for this purpose where it is simply the classiﬁcation and location of some examples
that are causing problems for the learner.
The exact deﬁnition of classiﬁcation noise given earlier is not what will be used to described

CHAPTER 6. LEARNING COMPLEX VAGUE CONCEPTS
158
the eﬀect of additional positive examples on the exploration of the search space, but a
variant of this idea can be used to model the characteristics of the data sets that have
been seen so far in this chapter. One of the properties that a learning problem should
have that was listed earlier was that the target representation of the overall concept would
require a diﬀerent clause for each of the distinct subsets within the data set, i.e. a number
of clauses should be constructed that each focused on a diﬀerent area of the example space,
X. The construction of each clause can be viewed as a separate learning problem, i.e. a set
of concepts f1 . . . fn ∈F, where F is the overall target concept. It can then be seen that
when the learner is focusing on a speciﬁc group of examples (covered by target concept
fi ∈F), although positive examples contained within the other groups are not technically
noisy examples, they act as interference for the algorithm that should be trying to focus
on one speciﬁc area of X because of the eﬀect of the examples from other groups on the
evaluation of the candidate clauses awaiting reﬁnement.
Therefore, while the learner is focusing on fi, any positive example e ∈X where e ̸∈fi
can be regarded as noise. The noise rate, ηi, for a particular problem can then be deﬁned
as the fraction of X that is positive, but is not covered by the target concept fi. This
deﬁnition of noise can be used to cover both positive examples from other target concepts
and examples that have been misclassiﬁed due to external inﬂuences.
During the search for the target clause, there will exist a set of candidate clauses that,
if the data set does not contain any noisy examples, will have a lower evaluation score
than the highest scoring candidate clause that is a generalisation of a target clause. For
each of these candidate clauses there will be a set of sets of negative examples, which, if
all the members of one of these sets are misclassiﬁed, will improve the candidate clause’s
evaluation score to be at least as good as the score of the highest scoring generalisation
of the candidate clause. For each of these clauses where the required examples have been
given a positive classiﬁcation, either Yi or Ni will be increased.
Let a be the size of the largest of these sets. Let B be the set of all subsets of X of size
a, where for each subset, missclassiﬁcation of all examples within that subset would mean
that at least one of the sets mentioned earlier would be completely missclassiﬁed. The
probability of at least one of these sets being completely missclassiﬁed is equal to:
P(at least one b ∈B missclassified) = card(B)ηa
i
This probability includes the probabilities for each set of size smaller than a because all
combinations in which those particular sets are completely missclassiﬁed will be included
within B. As card(B) increases, the overall probability increases, which represents the
intuitive notion that the more combinations of examples that would provide the necessary
evaluation improvement, the more likely it is to actually happen.
Whilst using partitioning as a pre-processing step may not remove all the noise present in

CHAPTER 6. LEARNING COMPLEX VAGUE CONCEPTS
159
the data set, only the noise caused by interference of other groups of examples is removed,
it will reduce it considerably. The reduction in noise reduces the probability for each of
the areas of the search space that do not contain the target hypothesis that the learner
will be misled by the data set into wasting time exploring it. In addition to removing the
noise from more than one group of examples being present within the data set, the noise
that is introduced from sources other than the nature of the learning problem will also
be reduced by partitioning the data set. This eﬀect was discovered during the evaluation
of Aleph using Dilum against other user modelling methods in Chapter 8. As mentioned
earlier, in order to incorporate such a pre-processing step, additional information must be
added to the system as to what the deﬁnition of a partition is.
The criteria for membership of a partition that is generated from this information must
be strict enough to ensure that examples from diﬀerent groups are placed into diﬀerent
partitions. In fact this membership criterion was so strict that an empirically observed
side-eﬀect of this requirement was that the set of partitions produced contains two distinct
types of partition. The ﬁrst type of partition contained very few misclassiﬁed examples and
second type of partition was usually very small and made up almost entirely of misclassiﬁed
examples. The amount of time required for learning from this second type of partition was
quite small due to the small number of examples in each partition, however the theories
produced were not part of the original target concept. With no method to tell which of
the theories within the model had been produced from a ‘noisy partition’, unless it could
be guaranteed that noise from other sources will not aﬀect the data set, some method of
dealing with the theories produced from noisy partitions had to be incorporated into the
method used to obtain answers from the overall theory produced. However, as will be
demonstrated in Chapter 7, this was not a diﬃcult or time-consuming task.
Two of the other variables within Equation 6.5 are also of interest for the information they
convey about the eﬀect of reducing the size of the target clause by splitting the learning
problem into separate dimensions. The upper bound of |Yi|+|Ni| is given in Equation 6.7.
upper bound(|Yi| + |Ni|) =
Ã k
X
v=0
|l|v
!
−c
(6.7)
where |l| is the number of literals in the hypothesis language, and k is the maximum
length of clause being examined. c represents the generalisations of the target clause, and
the target clause itself, which are not included in the sum since they will not mislead
the learner. Reducing the size of the search space by reducing the size of the relevant
hypothesis language l (described in Chapter 4 as a possible eﬀect of splitting the learning
problem into independent dimensions) will also have an impact on the amount of time
taken to ﬁnd the target clause.

CHAPTER 6. LEARNING COMPLEX VAGUE CONCEPTS
160
6.3
Summary
This chapter has focused on the use of partitioning as a pre-processing method and iden-
tiﬁed situations in which it should be applied. It has been demonstrated that partitioning
the data set before learning a disjunctive concept will reduce the amount of time taken
to generate the required theory. This is accomplished by reducing the complexity of each
individual learning problem, and by enabling the learner to stop exploring the search space
when an appropriate clause for each subconcept has been found. Although a positive PAC
learnability result was produced for the class of concepts under examination, the complex-
ity of the algorithm was still a very large polynomial which could not guarantee production
of the appropriate theory within the required time limit. Whilst a complete complexity
measure for the application of ILP to the concept was not produced, the eﬀect of more
than one group of positive examples within the data set on the complexity of the learning
problem, and hence how the complexity is reduced when these groups are removed, was
demonstrated.
The use of partitioning requires information about the nature of the concept to be learnt,
hence this method can only really be applied in the user modelling context. For this type
of application it is often known approximately what the concept to be learnt will be like
in general terms, enabling a larger amount of background information to be included in
the model generation process than would normally be found. However the exact nature
of the model is still dependent on the individual user and so the model is still required to
be constructed in an autonomous fashion, hence the requirement for learning.

Chapter 7
Answering Queries with the User
Model
A system for the creation of a model of the user has been constructed in the preceding
chapters. However, in order to perform evaluation of any models created, a means to use
the model to make predictions must also be created.
There are two diﬃculties presented by the data from which the user model within iDiary
is generated that introduce uncertainty into the predictions that the model can make.
These sources of uncertainty are: lack of information from which to learn about the user
(a small data set), and the possibility of high levels of noise within the data that is
collected. Without enough examples of the user’s behaviour the modelling system simply
cannot know if the observed pattern is typical of their actions or produced by special
circumstances.
The introduction of some means of coping with this uncertainty was deferred to the stage
at which the user model is used to make predictions of the user’s actions in order to make
use of some of the ideas that other applications that use models of the user have shown
to be eﬀective. The main idea was to generate some form of uncertainty measure for each
prediction produced in order to determine whether the prediction should be suggested to
the user.
Each rule within the model produced is capable of returning more than one answer due to
the non-determinacy of the literals that refer to the durations of both tasks in sequence.
For example, if the user enters a task such as:
task(type(travel),subject(york),location(n/a),
duration(4/00),preferred_time(8-00),preferred_day(wednesday))
161

CHAPTER 7. ANSWERING QUERIES WITH THE USER MODEL
162
and the following rule is contained within the user model:
seq(task(type(A),subject(B),location(C),
duration(D),preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),
duration(J),preferred_time(K),preferred_day(L))):-
A = travel, place(B), located_at(B,H),
G = visit, company(H), C = n/a, I = n/a,
lessthan(D,5/0), greaterthan(D,3/0),
lessthan(J,5/0), greaterthan(J,3/0).
then, if the rule is called with the user’s task as the ﬁrst task in the sequence, the following
tasks will be returned:
task(type(visit),subject(york_uni),location(n/a),
duration(3/30),preferred_time(_),preferred_day(_))
task(type(visit),subject(york_uni),location(n/a),
duration(4/00),preferred_time(_),preferred_day(_))
task(type(visit),subject(york_uni),location(n/a),
duration(4/30),preferred_time(_),preferred_day(_))
The non-determinacy of lessthan/2 and greaterthan/2 could not be reduced any further
because this would overﬁt the rules to the collected data. A list of tasks that have the same
basic overall description and only diﬀer by a small amount in their duration is produced
by each rule within the user model. Each list must be reduced to a single suggestion
before the ﬁnal set of suggestions is presented to the user. This requirement means that
any uncertainty measures produced would have to be given to each answer produced by
the model independently of the rules used to produce it. This means that some of the
methods of measuring uncerainty that could be suggested can no longer be considered (for
example Stochastic Logic Programs [Muggleton, 1995c] or fuzzy ILP, as demonstrated by
Shibata et al. [1999] in an extension of FOIL) because they give measures of uncertainty
to the rules themselves rather than the predictions they produce.
A simpler, and more obvious, choice of uncertainty measure is to use some form of con-
ditional probability distribution constructed from the clusters of data generated whilst
constructing the user model. Feller [1966] gives a good introduction to some of the possi-
ble types of distribution and their construction and use. The information contained within
each distribution would be used to generate a measure of probability for each of the an-
swers produced by the user model. For each of the lists of tasks described earlier, the

CHAPTER 7. ANSWERING QUERIES WITH THE USER MODEL
163
task with the highest posterior probability would be used as a possible suggestion to give
to the user. Each of these ﬁnal tasks would only be presented to the user if its posterior
probability was above the given conﬁdence threshold. Unusual tasks that are generated
from rules that were aﬀected by noisy examples during construction would be ﬁltered out
because they would not receive enough probability. Use of a conditional probability dis-
tribution would also allow a bias towards certain tasks following other tasks in sequence
through the use of weighted priors if it became apparent that this would be of beneﬁt to
the answers produced. However, during the evaluation process, a uniform prior was used
so that it did not aﬀect the evaluation measures produced.
As the predictions made by the user model are only of single tasks that could be scheduled
within immediate sequence of the task entered by the user, a more complicated representa-
tion of probability and dependency between tasks such as a Bayesian Network [Pearl, 1988]
is unnecessary, but will be reconsidered later when considering how the prediction could
be expanded to multiple sequences.
7.1
Generating the Conditional Probability Distributions
The generation of two sets of conditional probability distributions is carried out during the
model generation phase. One set of distributions describes the conditional probabilities of
tasks following given query tasks and the other set describes the conditional probabilities of
tasks preceding given query tasks. Both sets are created in the same way, so the following
description focuses on the generation of a set of conditional probability distributions for
use when predicting following tasks.
During the construction of the user model, clusters of similar examples are created for
which theories are then learnt as separate subconcepts. The information contained within
those clusters can be used when estimating the required conditional probability distri-
butions.
Each of the clusters contain examples that have roughly the same ﬁrst task
and second task, but these tasks are unlikely to be identical, so instead of insisting that
all speciﬁc tasks have their own distribution, the distributions will refer to stereotypes
[Rich, 1979] obtained from the mode of each cluster.
A set of conditional probability distributions is created; each distribution within the set is
associated with one of the stereotypical tasks, i.e. as each distribution represents P(B|A),
where A and B are stereotypical tasks, one distribution is required per A that has been
observed.
Clusters of examples are allocated to each distribution where the distance
between the mode of task A for the cluster and the base task of the distribution is less
than the cluster threshold distance used in the creation of the original clusters. The values
within each distribution are then constructed using the allocated clusters. Each example
within a cluster allocated to a distribution will increase the count of the B value that its
task B is nearest to (provided the distance between the task B and the B value is less than

CHAPTER 7. ANSWERING QUERIES WITH THE USER MODEL
164
the cluster threshold distance, otherwise an additional B value is added to the distribution
with the task from the allocated example used as the stereotype).
In addition, when working with real data, an extra task B, the empty task (described in
Section 3.1), is added to the distribution. The count for this task is equal to the number of
examples collected from iDiary with an empty second task and a ﬁrst task that is less than
the cluster threshold distance from the base task of the distribution. This task will not be
included in the experiments in this dissertation that work with artiﬁcial data because the
noise that it is meant to counteract has not been introduced.
Once each example has been processed, the values contained within the counts for each B
value belonging to a distribution are normalised to generate the required probabilities.
Why does the user model need rules when these conditional probability distributions ap-
pear to capture the information needed? The distributions capture the frequency with
which tasks follow or precede each other, but they cannot help to predict the details of
each task based on the query task. The stereotypes used within the distributions contain
values that have been captured at some point within one of the examples collected from
the user but have no relation to the base task of the distribution they are contained within.
Predicting the details of a new task requires a record of the relationships between values
that are present within the examples collected, hence the creation of rules.
7.2
Generating and Filtering Answers
When a query task is received, two lists of ‘distinct’ task predictions are made: a list of
tasks that could precede the query task, and a list of tasks that could follow the query
task. Lists of tasks are returned rather than just the task with the highest probability
as there may be several equally probable choices. A list of tasks from which the user can
choose, rather than just one task, gives the user more control and is much more likely to
provide the correct prediction. As the same method is used to generate both lists it will
only be described from the point of view of generating a list of possible following tasks.
The generation of a list of predicted tasks contains several stages:
1. Consult the user model to ﬁnd all possible tasks that could follow the given task.
2. Partition the collected tasks to create lists of answers that the user would regard as
almost the same. The algorithm for this step is shown in Figure 7.1.
3. Select the probability distributions where the given base task is close enough to the
query task for the distribution to be deemed relevant.
4. Use the collected distributions to give each answer a rating.
5. Pick the task with the highest rating from each task list.

CHAPTER 7. ANSWERING QUERIES WITH THE USER MODEL
165
T = {t|t is an answer returned from querying the user model}
S = {⟨s, {}⟩|s is a known task stereotype}
D = {type,subject,location}
For each t ∈T
if(∃r ∈S : r = ⟨y, b⟩∧(PD
d∈D δd(y, t) = 0))
b′ = b ∪{t}
r′ = ⟨y, b′⟩
\\ add the task to the existing bucket.
S = (S \ {r}) ∪{r′}
else
r′ = ⟨t, {}⟩
\\ create a new bucket.
S = S ∪{r′}
Return S
δx(p, q) = the distance between tasks p and q in dimension x.
Figure 7.1: The bucket sorting algorithm used for partitioning within the query module
Each of the more complicated points will now be expanded upon.
Consult the user model to ﬁnd all possible tasks that could follow the given
task — as the user model has been created in Prolog this step is carried out simply by
using the findall/3 predicate. This step demonstrates that the main role of the rules con-
structed for the model is to cut down the number of possible tasks that could be predicted.
Without the rules, the number of possible tasks that could be predicted for the data used
within the experiments is 4.536 × 109. An average rule within the model produced would
reduce this number to 22. The tasks returned can then be cut down even further by the
rest of the steps within the query module.
Partition the collected tasks to create lists of answers that the user would
regard as almost the same — some of the tasks returned by step 1 will be identical in
terms of type, subject and location but may diﬀer from each other in terms of duration.
The specialisation for this attribute within the user model could only restrict the range
of possible values as a greater level of specialisation would result in an intolerable amount
of overﬁtting. The user will not want to choose between several tasks of identical type,
subject, and location but of slightly diﬀerent lengths, so these tasks are partitioned into
lists using the bucket sorting algorithm shown in Figure 7.1. The initial bucket values
used within the algorithm are taken from the stereotypes generated when the conditional
probability distributions were created. In addition, each time a task is encountered that
has not appeared before an extra bucket is created.

CHAPTER 7. ANSWERING QUERIES WITH THE USER MODEL
166
The clustering of similar tasks and selection of the most likely tasks helps to improve the
precision of the user model by reducing the size of the list of tasks returned. However tasks
that do not have identical type/subject/location values are not placed within the same
clusters in case the wrong one is chosen when the best task is selected from each cluster.
For example, two tasks may be predicted that refer to a meeting, but specify a diﬀerent
project. Both tasks could be predicted due to the query task containing information that
related to both projects (e.g. one person who is involved in both projects), and no further
information to help distinguish between the likelihood of the two predicted tasks. The user
would ﬁnd the return of both tasks more useful than only the wrong one being returned.
Select the probability distributions where the given base task is close enough
to the query task for the distribution to be deemed relevant — the query task
given may not be exactly the same as any of the base tasks used when generating the
probability distributions and so it may be within reasonable distance (i.e. closer than the
given cluster threshold) of more than one base task. In this case it would be wiser to pick
all the distributions close to the query task and combine the values found within them
when giving ratings to the predicted tasks.
The two main reasons for giving each task a rating are: to ﬁlter out the incorrect answers
returned by the model, and to improve the model’s precision by avoiding returning tasks
that the user would regard as almost identical. Tasks that have not been encountered in
combination with the query task will receive a very low rating from all the distributions
selected, and will be discarded as incorrect answers. All other tasks will receive a rating
from each of the collected probability distributions.
This rating can then be used to
ascertain the most probable task from each group of tasks with identical values for types,
subjects and locations.
As every task within a group will receive a rating from each
probability distribution, altering the value received with regards to the distance of the
query task from the base task of the distribution will make no diﬀerence to the comparisons
made. Therefore, for each task it is enough to simply sum the ratings received from each
conditional probability distribution. This sum is included in Equation 7.1, the fractional
part of the equation is explained shortly.
Rating(task) =
m
X
i=1
n
X
j=1
Pi(Dti,j|query task)
dist(task, Dti,j) + z
(7.1)
where Dti,j = distrib i task j
Dti = {distrib i task | dist(task, distrib i task) ≤cluster threshold}
n = card(Dti)
Di = {distribi| dist(query task, distrib i base task) ≤cluster threshold}
m = card(Di), and
z is a very small constant.

CHAPTER 7. ANSWERING QUERIES WITH THE USER MODEL
167
Use the collected distributions to give each answer a rating — having collected
the relevant m distributions close to the query task, each task must be given a rating so
that the most likely task of each task list can be returned. Equation 7.1 shows how the
rating for each task is calculated. The outer sum of the equation is the implementation of
the sum of ratings discussed in the step above. The rest of the equation is used to reﬂect
the sphere of inﬂuence of each stereotype contained within the probability distributions.
The further away the task being given a rating is from a stereotype, the less inﬂuence that
stereotype has over that task. Once the distance between a stereotype and the new task
is greater than the threshold distance used to create the original clusters, then it can be
assumed that the inﬂuence of the stereotype has decreased beyond the point where it is
of signiﬁcance. The fractional part of Equation 7.1 reﬂects these ideas. The addition of
a very small constant, z, to the denominator of the fraction is to deal with those cases
where dist(task, Dti,j) = 0. The constant is small enough not to have a signiﬁcant eﬀect
on the answer when dist(task, Dti,j) > 0, but, given the method of measuring distance
between tasks described in Section 3.4, will transform the result of the fraction when
dist(task, Dti,j) = 0 from ∞to a number larger than anything that should be produced
when dist(task, Dti,j) > 0. The ratings collected from each stereotype are simply added
together as this should be enough to distinguish between potential answers.
Pick the task with the highest rating from each task list — Each list is examined
to ﬁnd the task with the highest rating; the rest of the list is discarded. If the selected
task is above a given threshold then it is kept to be suggested to the user, otherwise it is
discarded since a low rating is an indication that it is not likely to be a suggestion that
the user will want to see. Once the two lists of predicted tasks have been generated these
can then be returned for the user to make the ﬁnal selection. This task prediction method
aﬀects the precision of the user model in two ways:
1. It helps to reduce the eﬀects of noise by ﬁltering out predicted tasks that have not
been encountered before (i.e. the user has never entered these tasks within iDiary or
has never entered them in sequence with the current query task). These tasks may
be produced due to some constructed rules being unable to concisely capture the
concept that they were attempting to describe. This is more likely to happen when
noisy examples are present within the data from which the model was constructed
as they are likely to be formed into noisy clusters that the learner will ﬁnd diﬃcult
to construct a theory to describe.
2. It negates the eﬀect of including variables that are range-restricted based on the
ordering of the possible discrete values that the variables can take. Whilst these
variables are required to be included within the model instead of assigning a static
value to the variables (the theory would be too specialised to produced any answers
in this case), the generalisation provided by the range-restrictions is guaranteed to
multiply the number of answers produced per prediction.

CHAPTER 7. ANSWERING QUERIES WITH THE USER MODEL
168
It also improves the recall of the model by presenting the user with more than one sug-
gestion. Since there may be more than one task that the user could schedule in sequence
with the task they have just entered, then the system is more likely to present them with a
suggestion they would like to use if it gives all the possible choices rather than arbitrarily
picking one on their behalf.
7.3
A Worked Example
This example will only deal with the generation of possible preceding tasks, the genera-
tion of following tasks can be carried out in the same manner. Suppose that the model
query system has an existing user model, and has created a set of conditional probabil-
ity distributions for both following and preceding tasks. The following task is given as a
query:
⟨meeting,nokia,n/a,1/30,10-00,wednesday⟩
First, the system must identify the relevant conditional probability distributions to use
when rating the generated answers. In this case there is only one distribution where the
base task is close enough for it to be used:
Task B⟨meeting,eirta,103,2/0,13-00,monday⟩
Task A
Probability
[⟨prep,clasta,n/a,1/0,9-00,tuesday⟩,
0.6],
[⟨travel,cambridge,n/a,2/0,7-00,tuesday⟩,
0.3],
[⟨travel,york,n/a,5/0,7-00,tuesday⟩,
0.0],
[⟨meeting,eaum,n/a,6/0,9-00,wednesday⟩,
0.0],
[⟨meeting,chris,103,1/0,11-00,monday⟩,
0.0],
[⟨meeting,infix,york,2/0,10-00,tuesday⟩,
0.0],
[⟨presentation,rojoin,n/a,1/0,13-00,thursday⟩,0.0],
[⟨lunch,ysp,n/a,1/0,12-00,monday⟩,
0.1]
The next step is the generation of all possible preceding tasks. This can be carried out by
retrieving from the model a list of possible answers that must now be ﬁltered to produce
candidate predictions. The tasks are sorted into lists using the bucket sorting algorithm
described earlier, each task in each list is given a rating, and then the task with the
highest rating in each list is chosen. Figure 7.2 shows parts of two of the lists produced
with their calculated ratings. The second task has been predicted because the rule in the
model includes the located at/2 relation, and the system’s knowledge base knows that
the company mentioned is located in London.

CHAPTER 7. ANSWERING QUERIES WITH THE USER MODEL
169
⟨prep,nokia,n/a,0/0,PT,PD⟩,
0.06664
⟨prep,nokia,n/a,0/30,PT,PD⟩,
0.06666
⟨prep,nokia,n/a,1/0,PT,PD⟩,
0.06667
......
⟨travel,london,n/a,1/0,PT,PD⟩,
0.33269
⟨travel,london,n/a,1/30,PT,PD⟩,
0.33278
⟨travel,london,n/a,2/0,PT,PD⟩,
0.33282
Figure 7.2: Some of the tasks predicted to precede the query task with ratings
Once the highest ranked tasks from each cluster have been chosen, 2 possible answers
remain for which the query system can generate the times and days for based on the query
task and then present the ﬁnished tasks to the user:
⟨prep,nokia,n/a,1/0,9-00,monday⟩,
⟨travel,london,n/a,2/0,8-0,monday⟩
It would not be possible to consistently generate the exact answer required by the user in
terms of task duration and preferred time, however once the user has chosen any of the
tasks presented they can then make the ﬁnal adjustments to suit their exact schedule.
7.4
Summary
This chapter introduced the methods required in order to use operationally the model con-
structed by Dilum. Conditional probability distributions, constructed from the examples,
were used to give a probability to each prediction produced. These probabilities were then
used to improve the recall of the model by ﬁltering out noise (which would be given a
low probability). They were also used (in conjunction with a bucket-sorting algorithm) to
improve the precision of the model by reducing each set of almost-identical predictions to
a single suggestion, thereby producing a list of suggestions that the user will ﬁnd easier to
use.
The complete system (the model produced by Dilum and the querying method) will then
be used in the next chapter when the user modelling method is evaluated.

Chapter 8
Evaluation
Having produced a system able to cope with the problems encountered while attempting
to use ILP to produce a user model, a few ﬁnal questions remain to be answered: Is it
able to produce a user model good enough to be used? What advantages does it give over
existing user model construction methods? What performance improvement does it give
over Aleph on its own?
Ideally these questions would be answered by comparing the user models constructed from
the collected real data by Aleph using Dilum to the user models constructed from the same
data by other existing user model construction methods (and Aleph on its own). However,
the sets of real data collected from four diﬀerent users were so small that no meaningful
measure of the recall of each user model could be produced. The solution to this problem
was to split the evaluation of Dilum into two stages:
1. Produce a model from the collected real data using Aleph with Dilum and inspect
the model produced to determine whether it is capable of producing a model of the
user within the limits of the given information.
2. Construct a set of artiﬁcial data based on the real data and use this test data to
perform a comparison of Aleph using Dilum, TILDE using partitioning and the ar-
tiﬁcially generated negative examples (hereafter referred to as Dilum pt 1 as these
are the ﬁrst two ideas incorporated within Dilum), the two ILP methods by them-
selves and two other machine learning techniques commonly used for user model
construction.
The artiﬁcial data was created by extracting examples of sequences from the collected real-
data, identifying the general concepts captured by the examples and then creating further
examples of these concepts to form a larger set of data. The amount of identical repetition
of examples within the data set was based on the levels of repetition found within the real
data, i.e. there were occasional pairs of examples that were identical, but most referred to
diﬀerent people or places etc. These measures were used to help prevent a bias towards
170

CHAPTER 8. EVALUATION
171
Dilum during the evaluation by ensuring that the data set was as similar in nature to the
sort of information captured within the real-world data as possible. The only diﬀerence
between the artiﬁcial and real data sets was that the artiﬁcial data set contained a greater
number of examples, which should make the task easier for all of the model construction
systems.
The evaluation criteria for each user model construction method under comparison were:
1. Precision and recall of the model produced — Precision and recall are a com-
mon pair of measures used for evaluation of user models. Recall is deﬁned in Equa-
tion 8.1 and precision in Equation 8.2.
recall = |A ∩Ar|
|Ar|
(8.1)
precision = |A ∩Ar|
|A|
(8.2)
A is the set of answers returned by the model. Ar is the set of correct answers. The
set of correct answers means the set of answers that the model would be expected to
produce for the given query. As described in Chapter 7, each query task could have
several possible tasks following it or preceding it that have identical values for type,
subject, and location, and only diﬀer slightly in duration. Producing a large number
of these tasks would be contrary to the requirement of usability because the user
would have too many answers to choose from, and the recall measurement should
not penalise a modelling approach for only producing one task for each combination
of type, subject and location. Therefore Ar is a set containing one instance of each
correct answer that has a diﬀerence in either type, subject, or location with every
other task in the set. When A∪Ar is evaluated, it will also only contain one instance
of each correct answer that has a diﬀerence in either type, subject, or location with
every other task in the set. |A∪Ar| may be smaller than |A|, even though all items in
A are correct, thus reducing the measure of precision, but this enables the measure
of precision to reﬂect the requirement of usability mentioned earlier. If no tasks
should follow or precede the query task, then A will contain the empty task since
this is what the modelling approach should also return.
2. Time taken to learn — Small diﬀerences in the amount of time taken to produce
a model are not signiﬁcant enough for one model construction method to be pro-
nounced better than the other. However if one method takes substantially longer
and does not produce a better result then this factor should be taken into account.
3. How understandable is the model? While this may not be viewed as the most
important requirement of a user model construction method, the ability to produce
a model that can provide an explanation to the user as to why an action was carried

CHAPTER 8. EVALUATION
172
out or why a suggestion was made helps the user to trust the application containing
the model as they are able to examine the reasoning made on their behalf.
4. Ability to cope with noise — The examples gathered from the user will almost
always contain some degree of noise. In this case, noise would consist of pairs of
tasks that have been gathered as examples but are not actually part of any sequence
that the user would deliberately demonstrate. Any method under consideration for
the production of a model of the user’s scheduling habits must be able to continue
to construct a model that can produce responses with a high level of precision and
recall to queries in the presence of what would be considered a ‘reasonable’ level of
noise. However, a small amount of degradation may be tolerated as noise reaches
levels where it obscures the true examples.
An additional stage of evaluation is described in Section 8.3.
This section details the
results of experiments comparing Aleph with Aleph using Dilum on learning problems of
increasing size. These experiments were used to determine whether Dilum reduced the
computation time required for Aleph using Dilum to such an extent that it produced a
noticeable improvement over Aleph in how the system scaled to larger problems.
8.1
Evaluation using Real Data
Real data from four diﬀerent test subjects was collected and converted into a form that
could be understood by Dilum. This process included a method for translating hand-
written task descriptions into a type/subject/location description, and a method for iden-
tifying examples of sequences within the data. Neither of these methods were included in
the research into the system and will therefore require further work if they are to be used
as a reliable means of example collection. The experiment was concerned with how Dilum
handled the examples it was given rather than the transformation of data from a user’s
diary to the data set from which their user model was to be built.
For each test subject, once the examples had been collected, it was possible to ascertain by
close inspection that Dilum was able to give a compact representation of the concepts found
within each data set. Due to the small amount of data, evaluation of recall and precision
was not possible, hence the testing in Section 8.2. However, as the model produced was
human-readable, it could be seen that the clusters generated from the data set did not
contain examples of more than one concept, and that the appropriate range-restricted
clauses were produced as theories that covered these clusters. This shows that the model
produced should not encounter any problems if required to produce sequence predictions.
A few cursory queries to each model agreed with this opinion. Figure 8.1 gives an example
of the sort of rule contained within the models produced.
Dilum also satisﬁed criterion 2 in that the time taken to learn each user model was fairly

CHAPTER 8. EVALUATION
173
ex_rule(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(C),duration(J),
preferred_time(K),preferred_day(L))):-
shorterthan2(J,4/0),
shorterthan(D,2/30),
C=205,
on_project(H,B),
G=kick_off,
A=chat.
Figure 8.1: An example of a rule within a model built using real data
short (a few minutes each time). Criterion 4 (the ability to deal with noise) could not be
tested adequately. Any amount of noise when dealing with only small amounts of data
will aﬀect the learning problem in a more dramatic fashion than when data are plentiful.
Section 8.2 gives an indication of the ability of the system in this area.
8.2
Evaluation using Artiﬁcial Data
The methods of user model construction that were compared were:
Aleph using Dilum.
Aleph — in its original form to show how much direct improvement using Dilum gives.
TILDE using Dilum pt1 — to show whether the eﬀects of using the ﬁrst part of Dilum
related to clustering and negative examples improve TILDE’s performance.
TILDE — in its original form.
A method using Bayes rule — an implementation of a theory that has been widely
used for user modelling, detailed in Figure 8.2.
k-nearest neighbour — another common form of user modelling, detailed in Figure 8.3.
Naive Bayes classiﬁcation rests on the assumption that attributes are conditionally inde-
pendent given a value for the class being predicted. The most obvious implementation that
would reﬂect this assumption would be to assume that, for each attribute of the predicted
task, the attributes of the query task were conditionally independent given the value of
that attribute. Most attributes in the predicted task were independent of at least one
attribute in the query task. Ideally this would be reﬂected by a uniform probability for
the attribute in the query task that was not conditionally dependent on the value being
predicted. Unfortunately the probabilities would be dependent upon the data that had

CHAPTER 8. EVALUATION
174
R = set of all possible tasks
q ∈R = query task
c = cluster threshold
d = {P(A|B)|A ∈R ∧B ∈R ∧∀P(A1|B1), P(A2|B2) : A1 ̸= A2 ∧B1 = B2}
D = {d|p(A1|B1) ∈d1 ∧P(A1|B2) ∈d2 ∧d1 ̸= d2 ∧B1 ̸= B2}
T = set of possible answers = ∅
For each d ∈D
{
Aq = {a|a ∈d ∧dist task(a, q) < c}
\\ r is a task stereotype.
s = ΠAqa ∈AqP(r) according to d
T = T ∪{⟨s, t⟩}, where t is the base task of d
}
w = ⟨c, b⟩where ⟨c, b⟩∈T ∧(¬∃⟨c′, b′⟩∈T : c′ > c)
return b
dist task(a, q) is deﬁned in Equation 3.7
This algorithm is shown for calculating a set of tasks following the given query
task. To calculate a set of preceding tasks, simply refer to P(B|A) instead of
P(A|B) at the beginning of the algorithm.
Figure 8.2: The method naive Bayes classiﬁcation used in evaluation against Dilum
been collected from the user and the independencies themselves would also depend on the
individual user and so could not be removed from the predictive reasoning. This would
result in some values being given higher probabilities than others, even though there was
no conditional dependency.
A more reliable option was to work with whole tasks rather than individual attributes.
The assumption of attributes being conditionally independent given the class was moved
from individual attributes of the query task to the tasks that had already been collected
from the user that were closest to the query task.
The reasoning behind the implementation of a naive Bayes classiﬁcation, shown in Figure
8.2, is as follows. Assume that the goal is for the most probable following task, B, of a
given query task, A. Bayes theorem is shown in Equation 8.3.
P(B|A) = P(A|B)P(B)
P(A)
(8.3)
As A has been given, P(A) will be the same for all possible Bs. In addition, as there is no
prior bias over possible Bs, then P(B) will be the same for each B being compared, and

CHAPTER 8. EVALUATION
175
R = set of all possible tasks
q ∈R = query task
c = cluster threshold
E = set of all collected examples of the form seq(p, t)
S = list of all s ∈E, where s = seq(p, t) ∧p, t ∈R ∧dist task(p, q) < c,
sorted in order of increasing dist task(p, q)
Sk = ﬁrst k members of S
Tk = {t|∃p : seq(p, t) ∈Sk}
n = task(type(w),subject(x),location(y),duration(z), , ), where
w = max votestype(Tk)∧
x = max votessubject(Tk)∧
y = max voteslocation(Tk)∧
z =max votesduration(Tk)
Return n
dist task(p, q) is deﬁned in Equation 3.7
max votesd(T) returns the value that occurs with the highest frequency
in dimension d of the tasks contained within T.
This algorithm is shown for calculating a set of tasks following the given query
task. To calculate a set of preceding tasks, simply refer to the opposite task in
both instances where seq/2 is used.
Figure 8.3: The method of k-nearest neighbour used in evaluation against Dilum
will have no eﬀect on which B is chosen. These facts mean that the problem can then be
reduced to that shown in Equation 8.4.
max P(B|A) = max P(A|B)
(8.4)
The conditional probability distributions created for the query module of the Dilum user
model can be used to help with the calculations by providing estimates of conditional
probability.
The query task given is unlikely to exactly match any of the stereotypes
within the distributions, but this can be incorporated into the equation by regarding A
as data rather than a single task. This means that the equation is then transformed to
Equation 8.5.
max P(B|A) = max(
Aq
Y
a∈Aq
P(a|B))
(8.5)
Aq is the set of all stereotypes in each distribution that have a distance from the query
task that is less than the cluster threshold used when forming the original clusters from

CHAPTER 8. EVALUATION
176
which the distributions were created. The probability of each a ∈Aq is multiplied rather
than summed, as in the method used for the conditional probability distributions, because
the overall probabiity being calculated is the probability that the task B exists given that
all a ∈Aq exist. Therefore the right-hand of the equation represents the probability that
all a ∈Aq exist given B. Each a ∈Aq is assumed to be conditionally independent given
B, thus reﬂecting the assumption used within naive Bayes classiﬁcation. The evaluation
experiments demonstrated that, in practice, it is unlikely that Aq would contain more
than one or two values. Each possible task B corresponds to the base task of each P(A|B)
distribution, and the B from the distribution that returns the highest posterior probability
is the answer returned by the model.
8.2.1
Evaluation Method
The ﬁrst tests carried out measured precision and recall. The set of artiﬁcial data described
earlier in the chapter was used to perform a form of 3-fold cross-validation (where the recall
of each method was recorded for each fold rather than the diﬀerence between the errors
of each method). The precision of each model was measured at the same time by taking
note of the number of answers each model produced per query. The data set contained
examples of 10 concepts that had been extracted from real data collected from four people.
Some of the concepts were distinct, others overlapped in some manner. Each concept was
represented by approximately 20 examples, therefore by splitting the examples randomly
into 3 sets there was high probability that each concept would be represented in both the
training and test set for each stage of the testing.
The measures of precision and recall (deﬁned earlier in Equations 8.2 and 8.1 respectively)
could be combined into a single eﬃciency measure called the F-measure [Rijsbergen, 1979],
deﬁned in Equation 8.6. The measure takes into account the fact that precision and recall
must be considered together when comparing the peformance of diﬀerence models.
F-measure = 2 × recall × precision
recall + precision
(8.6)
However, precision and recall were kept separate for this evaluation because of the inter-
esting results produced by some of the model construction methods that would be masked
by combining these measures. Speciﬁcally the high level of precision achieved by k-nearest
neighbour, which will be discussed later.

CHAPTER 8. EVALUATION
177
0
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.2
0.1
0.3
0
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.2
0.1
0.3
Suggested error margins on measurements
0%
6% 11% 16% 22% 33% 40% 50%
0%
6% 11% 16% 22% 33% 40% 50%
0
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.2
0.1
0.3
0
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.2
0.1
0.3
0%
6% 11% 16% 22% 33% 40% 50%
TILDE using Dilum pt 1
−Nearest Neighbour
k
Key
Precision Measure
0%
6% 11% 16% 22% 33% 40% 50%
Aleph using Dilum
noise levels
noise levels
noise levels
noise levels
Rating
Rating
Rating
Rating
Naive Bayes Classification
Recall Measure
Figure 8.4: Results of user model construction method evaluation

CHAPTER 8. EVALUATION
178
8.2.2
Comparing Dilum to other User Model Construction Methods
Figure 8.4 shows the results of the experiments comparing the model produced by Aleph
using Dilum to other user model construction methods, with increasing levels of noise
introduced into the data to see how each system coped. A set of noisy examples containing
mismatched pairs of tasks was created by hand, using information from the gathered real-
data as a guide to possible noisy examples that could occur. For each level of noise within
the experiments, a certain number of examples were randomly picked from the set and
added to the data from which the models were to be learnt. The results for Aleph and
TILDE without the use of Dilum were not included as both failed to return any correct
answers in all the experiments produced (i.e. they consistently returned a recall measure
of 0.0).
Figure 8.4 shows several important results.
In addition to Aleph and TILDE in their
original forms being unable to construct models that could be used to produce the required
predictions, both k-nearest neighbour and naive Bayes classiﬁcation also proved equally
unable to make the required predictions from the models they constructed. This latter
result was initially unexpected, but examination of the way in which both methods produce
the answers requested shows why they were unable to return the correct information in
almost all test cases.
The high precision measure achieved using k-nearest neighbour was due to the method
only returning one or two answers when it was capable of making a prediction. The level
of recall achieved was more important as it showed that k-nearest neighbour was unable
to produce any answers in most cases.
Both k-nearest neighbour and naive Bayes classiﬁcation work best when producing classi-
ﬁcations from the models they have built. In both cases, the answers returned are created
from ground items (whole tasks) that are contained within the user model. For naive Bayes
classiﬁcation, this will be the task used as a stereotype within the probability distribution
created, for k-nearest neighbour this will be a task created by the votes of several tasks
collected from the k examples that have a task (either the ﬁrst or second task depending
on the query being performed) closest to the query task. For most of the attributes of
the task to be predicted, the values can be determined from the answer given by the user
model, but some of the most important attributes may need values returned that have not
been encountered before, which moves the problem away from classiﬁcation.
For example, suppose the following task was given as a query:
task(type(prep),subject(ap_solve),location(n/a),duration(2/0),
preferred_time(10-00),preferred_day(tuesday))

CHAPTER 8. EVALUATION
179
and suppose that the 5 closest examples within the data already collected had been re-
turned:
example(task(type(prep),subject(fujitsu),location(n/a),duration(1/0),
preferred_time(9-00),preferred_day(wednesday)),
task(type(meeting),subject(fujitsu),location(n/a),duration(2/0),
preferred_time(10-00),preferred_day(wednesday))).
example(task(type(prep),subject(bstar),location(n/a),duration(2/0),
preferred_time(10-00),preferred_day(tuesday)),
task(type(meeting),subject(bstar),location(103),duration(3/0),
preferred_time(13-00),preferred_day(tuesday))).
example(task(type(prep),subject(canem),location(n/a),duration(2/0),
preferred_time(9-00),preferred_day(tuesday)),
task(type(meeting),subject(canem),location(205),duration(2/0),
preferred_time(11-00),preferred_day(tuesday))).
example(task(type(prep),subject(ignite),location(n/a),duration(1/30),
preferred_time(9-00),preferred_day(thursday)),
task(type(meeting),subject(ignite),location(n/a),duration(2/0),
preferred_time(13-00),preferred_day(thursday))).
example(task(type(prep),subject(meredes),location(n/a),duration(1/30),
preferred_time(10-00),preferred_day(thursday)),
task(type(meeting),subject(meredes),location(n/a),duration(2/0),
preferred_time(14-00),preferred_day(thursday))).
Obviously the whole tasks returned cannot be used within a voting system, but the ex-
amples can be used to vote for various values of the task to be returned. The values for
type and location of the task are easy to determine, as is the value for the duration. As
the model is not required to predict the preferred time and day at the moment these are
calculated from the query task. This leaves the decision as to what value to give for the
subject ﬁeld of the task to be returned. There are ﬁve possible values, all of which on this
occasion are incorrect.
Roughly the same problem was encountered when attempting to use naive Bayes classi-
ﬁcation to predict tasks. The majority of the details of the task were correct, but if the
stereotype encountered did not contain the exact details required then it was an incorrect
answer to the query.
The most obvious solution to this problem for k nearest neighbour would be to identify

CHAPTER 8. EVALUATION
180
the relations that are true over the k nearest examples to the new query and to use these
relations when making the prediction. In addition to some values simply being the same
in both tasks of an example, other relations exist between values of an example that would
help with prediction of some of the values of a task. In order to include these relations
into the model used by k-nearest neighbour, two possible methods could be attempted.
The examples selected from the model could be examined for the existence of known
relations over them and these would be applied to the query task to produce an answer,
hence mimicking the search performed by ILP. Alternatively, for either k-nearest neighbour
or naive Bayes classiﬁcation, extra values could be included within the data to indicate
for each value which relations it was included in, unfortunately this would increase the
size of each example to a tuple containing well over one hundred items. Both methods
would involve substantially more processing than the methods they would be augmenting,
and would be considerably more complicated than using a method such as Dilum that
automatically provides the resources that this extra processing would be seeking to include.
In addition, if naive Bayes classiﬁcation and k-nearest neighbour are considered with regard
to the other criteria set out, whilst the time taken to learn each model was acceptable,
neither method performed very well on how well the model produced could be understood
by a human (criterion 3).
8.2.3
Comparing Aleph using Dilum and TILDE using Dilum pt 1
The results for the models produced by Aleph using Dilum and TILDE using Dilum pt
1 show that both the learners have been enabled to produce a user model from the sort
of data that would be collected from the user, and both were able to produce a model in
the presence of a moderate level of noise. The recall measures for the models produced
by both learners were aﬀected very little because the clustering stage of Dilum would
split the data into clusters of similar examples and if the noisy examples were not similar
enough to existing positive examples then they were be placed into separate clusters. This
meant that the learners were presented with two types of cluster: those containing a low
level of noise, and those containing only noise. As the noise levels increased, the number of
‘noisy clusters’ also increased. Learning theories for these noisy clusters allowed additional
possible answers to be returned when the models were queried. However, this reduced the
precision of the models only slightly because any answers produced that had not been
encountered before in the presence of the query task were ﬁltered out due to receiving a
very low rating from the conditional probability distributions.
The recall level of the model produced by Aleph using Dilum was consistently higher than
that of the model produced by TILDE using Dilum pt 1. The main reason for this result is
the way in which the results returned from TILDE had to be processed before they could
be used (as explained in Chapter 3). The task duration literals were designed to give the
longest time possible outside the given positive examples whilst still excluding the negative

CHAPTER 8. EVALUATION
181
−
−−
−
−
−
−
−
+
+
+
+
+
+
+
+
shorterthan(D, 1/0)
shorterthan(D, 3/0)
preferred answer
actual answer
Figure 8.5: The preferred answer and actual answer returned from TILDE
examples. For example, when considering values for the predicate shorterthan/2, which
takes a ground duration for its second argument, the largest possible values would be
considered ﬁrst. However if these predicates are used to exclude examples with a positive
classiﬁcation then the values will be close to the values within the positive examples.
Figure 8.5 shows the diﬀerence between the preferred and actual answers returned by
TILDE. If TILDE returns the answer that examples with a duration longer than 1 hour
have a negative classiﬁcation, and then if this is reversed when converting to positive
classiﬁcation only, an important area of example space that should be covered by the rule
produced is excluded. As TILDE aims simply to split the examples with no bias towards
either classiﬁcation then it is more likely to isolate areas of example space containing
negative examples instead of positive ones. Once this result has been received and has
to be reversed because rules referring to the negative classiﬁcation are encountered ﬁrst,
the resulting reversed literals present a form of overﬁtting within the model.
Simply
reversing the order in which possible ground values are considered will not entirely solve
this problem because TILDE cannot be guaranteed to focus on excluding examples of a
particular classiﬁcation. If, instead of separating group of negative examples from the data
set, a decision within a node enables a group of positive examples to be separated from the
set, then the ground value within the literal that performs this decision will again place
the division of example space too close to the group of positive examples.
Aleph’s precision decreased faster than TILDE because of an additional feature of Aleph’s
that was made use of during model construction and that could only be simulated when
using TILDE. The most important feature available to Aleph but not to TILDE was the
ability to unify variables in the head of the clause being constructed, whereas TILDE had

CHAPTER 8. EVALUATION
182
cluster representation built by TILDE:
ex_rule(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),duration(J),
preferred_time(K),preferred_day(L))):-
equal_to(I,H),
equal_to(G,B),
C=n/a,
A=travel,
shorterthan(J,5/0),
shorterthan(D,1/30).
cluster representation built by Aleph:
ex_rule(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(B),subject(C),location(C),duration(G),
preferred_time(H),preferred_day(I))):-
shorterthan2(G,5/30),
shorterthan(D,5/30),
C=n/a,
place(B),
A=travel.
Figure 8.6: Representations of a cluster built by i) TILDE ii) Aleph
to make do with using the equal to/2 predicate in the tree that it was constructing.
The equal to/2 predicate could be used by TILDE to split the positive and negative
examples in exactly the same way as using a predicate that described the group of the
attribute being examined, which meant that only one of these two required literals would be
applied to each task attribute. The data set given to TILDE using Dilum pt 1 contained
additional negative examples that had been generated by substituting a diﬀerent value
that was a member of the same group as the original value into a positive example. These
were included in order to force TILDE to choose the equal to/2 predicate rather than a
more descriptive unary predicate because the model could not be used to make predictions
if there were no links between the attributes of both tasks. This meant that predicates
describing attribute groups were made redundant and were not included in the constructed
tree.
The resulting rules produced by TILDE covered a much larger area of the example space
than the clusters they were supposed to capture and hence produced a larger number

CHAPTER 8. EVALUATION
183
of results per query. Fortunately most of these results could be ﬁltered out by the use
of conditional probability distributions but this slowed down the query process, and the
model built by TILDE using Dilum pt 1 was less understandable than that built by Aleph
using Dilum. Figure 8.6 shows the representation of the cluster built by TILDE using
Dilum pt 1 and the corresponding representation built by Aleph using Dilum. The rule
created by TILDE using Dilum pt 1 makes no reference to any variable being a place, does
not link the location variables of the two tasks, and has greater restrictions on the range
of durations each task can take.
Why would Aleph produce a diﬀerent result if equal to/2 works so well in splitting the
positive and negative examples? In fact, Aleph was not given this literal to use as it was
only given to TILDE because of its inability to unify variables in the heads of clauses.
Aleph returned the literal describing the type of the variable under examination, and did
not unify the variable with any others in the learning problem for that particular dimension
because the problem presented was set up in such a way as to prevent this from happening
(as mentioned in Chapter 4). In addition, the negative examples presented to Aleph did
not contain instances where the variable under examination had been replaced by one
of the same type (e.g. a diﬀerent company name). This sort of negative example had
only been presented to TILDE to ensure that it used literals such as equal to/2 so that
the rules produced were specialised enough to be used for prediction. Figure 8.7 shows
the two possible representations of a cluster built by TILDE. The ﬁrst representation is
generated when TILDE is presented with negative examples that contain values of the
same type, the second representation is generated when TILDE is presented with negative
examples that do not contain such values.
The representations do not cover identical
areas of example space, but both cover the area containing the original cluster. The ﬁrst
representation would work well for prediction as it contains relations that link variables in
each task to variables in the other task, but is not very descriptive. It would also produce
a large number of possible answers to be ﬁltered out by the query module. The second
representation would work well for classiﬁcation purposes, but does not link the values of
both tasks and so could not predict values for either task based on the other.
The variables in the rules generated using Aleph were uniﬁed as an additional eﬀect when
Aleph was presented with a subproblem that focused on a variable other than the ones
uniﬁed. For this separate subproblem, the restrictions on uniﬁcation of variables in the
head of the clause other than the one currently being examined were no longer present.
Uniﬁcation of the variable under examination with another variable was not allowed be-
cause the result returned would cover too much of the example space along that dimension
(as occurred in the model built by TILDE using equal to/2), and the model was not as
descriptive. The learner was forced to return a type for the variable or a relation (other
than uniﬁcation) that connected the variable to another variable. Uniﬁcation was then
allowed to occur if the examples supported it during a diﬀerent learning problem.

CHAPTER 8. EVALUATION
184
ex_rule(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),duration(J),
preferred_time(K),preferred_day(L))):- equal_to(I,H),
equal_to(G,B),
C=n/a,
A=travel,
shorterthan(J,5/0),
shorterthan(D,1/30).
ex_rule(task(type(A),subject(B),location(C),duration(D),
preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(I),duration(J),
preferred_time(K),preferred_day(L))):- I=n/a,
H=n/a,
C=n/a,
A=travel,
off_site(G),
off_site(B),
shorterthan(J,5/0),
shorterthan(D,1/30).
Figure 8.7: Two possible representations of the same cluster build by TILDE
The most common case of this occurrence was when Aleph was learning specialisations for
the duration of a task, and produced rules where variables in the type/subject/location
descriptions of tasks had been uniﬁed. This production of an additional specialisation
along a dimension that should have been ignored at that point in the learning process did
not harm the rules produced because it would only have been produced if it did not aﬀect
the coverset of the rule under construction. Whilst this may appear to be going against
the idea of dealing with each dimension separately, it further supports the idea presented
in Chapter 5 that this method of learning is most suited to areas where the concept is
roughly known in advance (e.g. learning within an automated system that requires some
form of personalisation), where occurrences such as this can be allowed to continue as it
is known roughly what the eﬀect will be on the theory produced.
In fact, with Aleph, there is a choice, Aleph can either use Dilum in the manner described
above and produce more descriptive models that suﬀer slightly more from high levels of
noise, or it can use Dilum in a more robust, but less descriptive, manner that would produce
results like those produced by TILDE using Dilum pt 1. For Aleph this would involve
allowing uniﬁcation of the variable under examination with other variables in the head of
the clause, including the aforementioned equal to/2 predicate in the given background
knowledge, and allowing the generation of negative examples using a value of the same
group as the original value of the attribute that was being altered. equal to/2 would
be used rather than working with Aleph’s splitvars setting, which would introduce an

CHAPTER 8. EVALUATION
185
additional literal into the bottom clause for each pair of variables in the head of the clause
that had the same value (see [Srinivasan, 2001] for a description), so that the modelling
system could retain control over which attributes in the head of the clause the predicate
could be applied to. The model produced by this approach is less descriptive but has the
same resistance to noise as shown by TILDE. The choice of which approach to use would
depend on the sort of data that could be encountered.
8.3
Evaluating the Eﬀect of Dilum on Aleph’s Scalability
The evaluation of the user model produced by Aleph using Dilum showed that the ideas
encapsulated within Dilum enabled production of the required user model if there was
enough data to learn from. The time taken to build the models was not mentioned because
the successful methods did not take a long time to produce their models, however earlier
results in Section 4.4 (Table 4.1) showed that using Dilum reduced the amount of time
taken to construct each required theory using Aleph.
Another aspect of Dilum’s eﬀect on Aleph’s performance that should be examined is
scalability. How does Aleph using Dilum scale as the size of the modelling task increases?
Does it scale better than Aleph on its own? The factors that will aﬀect how many clauses
Aleph will evaluate before ﬁnding the target clause are:
1. The amount of relevant background knowledge available. In the case of Aleph this
refers to the size of bottom clause that Aleph constructs;
2. The length of the target clause. As the number of literals contained within the clause
that describes the target concept increases, the area of hypothesis space that Aleph
is likely to search also increases;
3. The number of concepts within the data set. As the number of clusters of examples
within the data set increases, so does the number of clauses that Aleph must include
within the overall theory. If the examples are presented as a single data set then
they may also mislead Aleph into searching more of the hypothesis space than it
would do if presented with each cluster separately, as demonstrated in Chapter 6.
Three experiments were used to compare Aleph’s and Aleph using Dilum’s scaling ability
as each of these three items were increased. The experiments compared the number of
clauses evaluted by Aleph and Aleph using Dilum rather than recording the computation
time.

CHAPTER 8. EVALUATION
186
Three items contributed to the decision to focus on this measure:
1. In order to test Aleph’s performance on the same data set as Aleph using Dilum,
Aleph would need negative examples of the kind that would be produced auto-
matically by Aleph using Dilum using the methods described in Chapter 4. Since
this example generation required that both learners used the clustering and nega-
tive example generation stages of Dilum, then these stages were removed from the
experiment.
2. The only other additional process used by Dilum was one that combined the results
collected from each of the separate learning tasks. This is a very small increase on
the overall amount of time taken by Dilum to solve the problem.
3. Since Aleph using Dilum would be presented with separate learning problems then
it would be working with partitions of the data set and Aleph on its own would be
working with the whole data set. Therefore Aleph using Dilum would take less time
to evaluate each clause. This is a small reduction in the amount of time that would
be taken by Dilum to solve the overall learning problem.
Since the second and third items would cancel each other out to some extent, and given the
size of diﬀerence in the performance of Aleph with and without Dilum shown in the initial
results, these small diﬀerences were deemed insigniﬁcant. It was simpler to just compare
the number of clauses that each approach evaluated whilst producing the required theory.
8.3.1
Experiment 1: Increasing Background Knowledge
The ﬁrst experiment focused on how each learning approach scaled as increasing amounts
of background knowledge were made available, i.e. the size of ⊥was increased. The size
of ⊥is the variable that is more likely to be increased in real life situations. The target
clause and the search depth bound remained the same, but in each learning session the
size of ⊥was increased. In order to ensure that the learner did not dismiss the additional
literals upon their ﬁrst evaluation, ‘redundant’ literals were used. This is explained in
more detail later on.
The data set contained positive examples representing three of the concepts used in Section
8.2.2. To reduce the complexity of the initial learning task the concepts to be learned were
restricted to ﬁnding descriptions for the type, subject and location of each task.

CHAPTER 8. EVALUATION
187
For example, given examples such as:
sequence(task(type(travel),subject(colchester),location(n/a),
duration(1/0),preferred_time(7-30),preferred_day(monday)),
task(type(visit),subject(nokia),location(n/a),
duration(6/0),preferred_time(9-00),preferred_day(monday))).
the learner was simply required to produce the clause:
sequence(task(type(A),subject(B),location(C),
duration(D),preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(C),
duration(I),preferred_time(J),preferred_day(K))):-
A=travel, C=n/a, located_at(B,H), G=visit, company(H).
The background knowledge given was reduced to those literals that could be used for this
learning task.
The initial set of background knowledge was the absolute minimum required to produce
the required concepts. For each step of the experiment the background knowledge given
to Aleph (not using Dilum) was increased in such a way that the average number of
literals in the bottom clause produced by Aleph increased by 1. This was accomplished
by adding a predicate to the background knowledge that could only be applied to one of
the variables within the head of the clause, e.g. the type of the ﬁrst task. This alteration
was then reﬂected in the background knowledge given to the separate learning problems
solved by Dilum, i.e. if Aleph’s bottom clause was increased by giving it a predicate that
only referred to the type of the ﬁrst task, then the set of background knowledge in Dilum
that referred to the type of the ﬁrst task received the same alteration.
The predicates used to increase the size of the bottom clause were designed to be ’redun-
dant’ literals, i.e. ones that did not alter the number of positive and negative examples
that a particular clause covered. Since Aleph cannot reason that these literals may not
be useful later on, it is forced to keep attempting to add them to the clause that it is
constructing, rather than discarding them at an early stage.
Figure 8.8 shows the results of the experiments. Each experiment was run ﬁve times and
the results averaged. Each point in Figure 8.8 represents the mean number of clauses for
that experiment and the error bars represent the standard error of each mean.
The ﬁrst set of results produced for Aleph used a heuristic search strategy. However, as the
search heuristic may be misled by the redundant literals present within the background
knowledge, a breadth-ﬁrst search was also tried because this would be guaranteed to ﬁnd

CHAPTER 8. EVALUATION
188
log   (clauses evaluated)
10
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
5.5
6
6.5
Aleph using heuristic search
Aleph using breadth−first search
Aleph using Dilum
8
7
21
15
16
17
18
19
20
9
10
11
12
13
14
23
22
Number of literals in 
Figure 8.8: Results of Comparing the Scalability of Aleph and Aleph using Dilum for
increasing sizes of ⊥

CHAPTER 8. EVALUATION
189
the shortest clause that would describe the target concept. The gradient of the results
produced by Aleph using Dilum is lower than that of Aleph on its own (using either search
strategy). This is a signiﬁcant diﬀerence because it indicates that the number of clauses
evaluated by Aleph scaled better when using Dilum than without it.
The length of candidate clause that could be evaluated in the experiment was restricted
to six literals because it was known that this was the maximum length of target clause to
be found. In a situation in which the length of the target clause is not known, this value
will have to be increased, leading to a greater number of clauses being evaluated. Since
Aleph using Dilum uses a reduced set of background knowledge for each learning problem,
it will not be aﬀected as much as Aleph on its own. Therefore the beneﬁts of using Dilum
will be even more pronounced.
8.3.2
Experiment 2: Increasing Length of Target Clause
The second experiment focused on how the performance of each learning approach scaled
as the size of the target clause increased. The set of positive examples used in the ﬁrst
experiment was also used here. A minimal amount of background knowledge was made
available, this time including predicates that could refer to the duration of each task. The
initial concept to be learnt referred to just the type of the ﬁrst task, e.g. given the example:
sequence(task(type(travel),subject(colchester),location(n/a),
duration(1/0),preferred_time(7-30),preferred_day(monday)),
task(type(visit),subject(nokia),location(n/a),
duration(6/0),preferred_time(9-00),preferred_day(monday))).
the learner was simply required to produce the clause:
sequence(task(type(A),subject(B),location(C),
duration(D),preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(C),
duration(I),preferred_time(J),preferred_day(K))):-
A=travel.
The set of negative examples included within the data set was reduced to only those that
referred to the type of the ﬁrst task, e.g.
sequence(task(type(holiday),subject(colchester),location(n/a),
duration(1/0),preferred_time(7-30),preferred_day(monday)),
task(type(visit),subject(nokia),location(n/a),
duration(6/0),preferred_time(9-00),preferred_day(monday))).

CHAPTER 8. EVALUATION
190
log   (clauses evaluated)
10
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
5.5
Aleph using heuristic search
Aleph using Dilum
1
2
3
5
4
6
7
8
9
10
Aleph using breadth−first search
Number of literals required in target clause
Figure 8.9: Results of Comparing the Scalability of Aleph and Aleph using Dilum for
increasing sizes of target clause
For each step of the experiment, additional negative examples were added to the data set
that referred to an attribute of one of the tasks that was not referred to in the target
clause of the receding step. Each addition of negative examples to the data set altered
the target clause so that it required one more literal than the previous step in order to
describe the concept represented by the data set.
Unlike the previous experiment, only a single concept would be learnt for this experiment.
The clusters in the data set for Experiment 1 only required clauses that referred to the
type, subject, and location. Whilst the presence of more than one cluster would have an
eﬀect on the number of clauses that Aleph evaluated, it would not potentially mask the

CHAPTER 8. EVALUATION
191
eﬀect of increasing amounts of relevant background knowledge available.
For Experiment 2 the presence of more than one concept with the data set would have a
greater eﬀect because the ﬁnal target clauses will require literals that describe the duration
of both tasks. As described in Chapter 6, the presence of more than one cluster when
literals that limit the values of a variable with an ordered range are required within the
target clause dramatically increases the amount of time taken to ﬁnd a suitable hypothesis.
Since it is the eﬀect of the length of the target clause that is being explored for this
experiment and not the eﬀect of more than one cluster in the data set, then the data set
will contain only one cluster.
Figure 8.9 shows the results of the experiment. Each point in Figure 8.9 represents the
mean number of clauses for that experiment and the error bars represent the standard
error of each mean. Each experiment was run ten times and the results averaged. This
experiment was run more times than the previous experiment in an eﬀort to reduce the
standard error in the results at small to medium clause sizes. The standard error was
quite high due to the values in the example chosen to produce ⊥occasionally misleading
Aleph’s search and occasionally providing a ‘short cut’ to a solution.
The gradient of the results produced by Aleph using Dilum is lower than that of Aleph on
its own (using either search strategy). This is a signiﬁcant diﬀerence because it indicates
that the number of clauses evaluated by Aleph scaled better for this dimension when using
Dilum than without it.
8.3.3
Experiment 3: Increasing Number of Concepts
The third experiment focused on how the performance of each learning approach scaled
as the number of concepts within the data set increased. This issues was described in
Chapter 6. In particular, the results in that chapter demonstrated that Aleph was capable
of learning theories containing several clauses if the literals within each clause referred to
‘categories’ rather than restricting a variable that had an ordered range of possible values.
Therefore two sets of results were produced for this experiment:
1. Concepts that only required reference to attributes such as the type or location of a
task, for example:
sequence(task(type(A),subject(B),location(C),
duration(D),preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(C),
duration(I),preferred_time(J),preferred_day(K))):-
A=travel, C=n/a, located_at(B,H), G=visit, company(H).

CHAPTER 8. EVALUATION
192
2. Concepts that required reference to the duration of both tasks, for example:
sequence(task(type(A),subject(B),location(C),
duration(D),preferred_time(E),preferred_day(F)),
task(type(G),subject(H),location(C),
duration(I),preferred_time(J),preferred_day(K))):-
A=travel, C=n/a, located_at(B,H), G=visit, company(H),
shorterthan(D,3/00), longerthan(D,0/30),
shorterthan(I,4/30), longerthan(I,2/30).
Although these concepts have diﬀerent lengths of target clause to be learnt, this is not
a problem since the results of each learner on the diﬀerent concepts will not be com-
pared. This experiment is concerned with scalability, it is not attempting to determine
the diﬀerence in performance of each learner for the diﬀerent concepts.
The initial learning problem consisted of a set of positive examples (and corresponding
negative examples) for a single concept such as one of the ones just described. For each
step in the experiment, a set of positive and negative examples representing an additional
concept to be learnt was added to the data set. The background knowledge made available
was the minimum required that could be used construct the target clauses for any of the
concepts to be used in the experiment. This set of background of knowledge was used
rather than just the minimum amount of background knowledge required for the initial
concept so that the size of the bottom clause constructed for any given example remained
constant for each step in the experiment.
Figures 8.10 and 8.11 show the results of the experiments. Each experiment was run ﬁve
times and the results averaged. Each point in Figure 8.8 represents the mean number of
clauses for that experiment and the error bars represent the standard error.
The gradient of the results in Figure 8.11 produced by Aleph using Dilum is lower than
that of Aleph on its own (using either search strategy). The gradient of the results in
Figure 8.10 produced by Aleph using Dilum is lower than that of Aleph using breadth-
ﬁrst search and roughly equal to Aleph using heuristic search. However, since the scale
of the Y-axis is log (base 10), the same gradient at higher values of Y actually indicates
a greater rate of increase of the number of clauses being evaluated. As for the previous
experiments, these results also indicate that the number of clauses evaluated by Aleph
scaled better for this dimension when using Dilum than without it.

CHAPTER 8. EVALUATION
193
log   (clauses evaluated)
10
1
3
2
4
5
6
7
Number of clusters
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5.5
6
5
Aleph using heuristic search
Aleph using breadth−first search
Aleph using Dilum
Figure 8.10: Results of Comparing the Scalability of Aleph and Aleph using Dilum for
increasing numbers of concepts — 6 Dimensions

CHAPTER 8. EVALUATION
194
log   (clauses evaluated)
10
1
3
2
4
5
6
7
5.5
6
Number of clusters
0
0.5
1
1.5
2
2.5
3
3.5
4.5
5
Aleph using Dilum
4
6.5
7
Aleph using heuristic search
Aleph using breadth−first search
Figure 8.11: Results of Comparing the Scalability of Aleph and Aleph using Dilum for
increasing numbers of concepts — 10 Dimensions

CHAPTER 8. EVALUATION
195
8.3.4
Summary and Discussion
The results of the three experiments demonstrate that, in general, Aleph using Dilum
scales better than Aleph on its own. The comparison of scaling between Aleph and Aleph
using Dilum as the number of concepts to be learnt increases depends on the nature of
the concepts. If the target clauses contained references to task duration then Aleph using
Dilum scaled better than Aleph on its own. If the target clauses only referred to the type,
location or subject of each task then the performance of Aleph on its own is closer to its
performance using Dilum. However, this was expected, having already been encountered
and explained in detail in Chapter 6.
For each individual hypothesis to be produced, the occasions in which Aleph will produce
a good hypothesis using equal or less computation time than Aleph using Dilum are:
1. when a good hypothesis is small,
2. when the amount of potentially relevant background knowledge is small, or
3. when a good hypothesis does not necessarily need an answer from all of the separate
dimensions that the problem was split into for Dilum (and is small).
The experiments described appear to insist that Aleph must ﬁnd ‘the best’ hypothesis that
explains the given data set. Would Aleph produce a hypothesis using less computation
time if it had only been required to search for a clause that was ‘good enough’ rather
than ‘the best’? The answer to this question depends on the learning problem that is
being focused on since this will determine what makes a hypothesis good enough. Some
problems will have hypotheses that are good enough within the search space that Aleph
could explore within a reasonable time, but others will not.
If, when a learning problem is split into separate dimensions, it is already known that the
hypothesis must incorporate an answer from each of these dimensions, then any hypotheses
that are good enough will be found by Aleph using Dilum. If a hypothesis that incorporates
an answer from each dimension can be found by Aleph on its own within a reasonable time
then, as discussed in Section 5.4, Dilum is not required. However, if the target clause is
long enough to warrant splitting into separate then dimensions then, unless the amount
of background knowledge provided is very small, Aleph will ﬁnd a good hypothesis more
quickly using Dilum than on its own. If a suitable hypothesis does not necessarily require
answers from each dimension then Dilum would be set up in the manner as encountered in
Chapter 5. Aleph may be able to ﬁnd a suitable hypothesis on its own within a reasonable
time (as it did for three of the ﬁve concepts to be learnt in Chapter 5), but if there is a
large amount of background knowledge that cannot easily be dismissed as irrelevant then
Aleph may still have too large a search space to explore before ﬁnding a good hypothesis.
What can be seen from these two scenarios is that the nature of the learning problems

CHAPTER 8. EVALUATION
196
that Dilum has been designed to deal with is such that a hypothesis is either correct or
unsuitable, there is no room for compromise.
Aleph will scale as well with or without Dilum for the concepts for which Dilum was not
designed. However, when learning under the circumstances dictated by learning problems
such as the one focused on within this dissertation, the performance advantage of Aleph
using Dilum will continue to improve as the any of the three factors described earlier
increase, particularly if more than one factor increases at the same time.
8.4
Conclusions
The evaluation of the user models produced by both Aleph using Dilum and TILDE using
Dilum pt 1 shows that the ideas encapsulated within both methods enable both learners
to produce user models that meet the stated criteria if there is enough data to learn from.
Naive Bayes classiﬁcation and k-nearest neighbour were unable to produce the required
predictions because the prediction problem is not really a classiﬁcation problem.
The
results in Section 8.3 demonstrate the beneﬁt that using Dilum has on the scalability of
Aleph when working with learning problems such as the one presented in this dissertation.
The results in Figure 8.4 show that the best model in terms of precision is not necessarily
the most descriptive. Terms that relate the values contained within the two tasks reduce
the number of possible mappings from query task to predicted task more eﬀectively. Rules
that do not contain descriptive terms (such as requiring that variable is a person or a
project) may produce answers for queries that they should not cover. However, if these
answers do not conform to a task that has already been encountered near tasks that
are similar to the query task, then these answers will be ﬁltered out by the conditional
probability distributions within the query system.
Ideally, the model would perform well in terms of both precision and recall, but is descrip-
tive as well so that the concepts captured are more understandable to a human reader.
One suggestion is to use the method outlined for Aleph using Dilum earlier (i.e. the method
where negative examples did not contain values of the same group as the positive exam-
ples), and check the rules created to see if one of the three following properties holds for
each discrete valued attribute to be predicted:
1. The attribute is uniﬁed with another variable that is situated in the other task in
the head of the rule.
2. The attribute has some relation over it that includes variables from the other task
in the head of the rule.
3. The attribute is bound to a ground value.

CHAPTER 8. EVALUATION
197
Each attribute that takes a value with an ordering over it is guaranteed to be given some
form of range-restriction during learning. If none of these properties hold for an attribute
then the rule will create a large number of predictions per query. In this case the concept
can be re-learnt, this time adding in the extra negative examples that were left out earlier.
Following this idea enables as much of the model as possible to be descriptive, but when
this cannot be allowed then the focus will be on the more important areas of recall and
precision.
The evaluation of the user models produced from the real data by Aleph using Dilum show
that the system is capable of producing a model of the concepts found within the data.
However the experiments also showed the problems associated with attempting to learn a
concept such as immediate sequences of tasks:
1. Further research is still needed into the translation of human task descriptions into
the task description used within the model. A simple system was used during the
experiments with real data out of necessity for some kind of automation but it would
not perform adequately enough for a true real-world environment.
2. Additional research would also be needed into ontologies in an eﬀort to gather the
sort of data the system needs in order to learn the user model. Current thinking (for
example [Maedche and Staab, 2000]) says that the automatic generation of complete
ontologies is not possible, however it should be possible to signiﬁcantly reduce the
amount of eﬀort required of the system administrator in this area.
3. Several of the sequences listed as possible examples in the data used in previous
chapters were not found in the real data due to the users not adding these tasks into
their diaries. Various tasks are deemed ‘obvious’ by the user, or the user simply does
not use their diary in that way and hence a signiﬁcant proportion of the user’s tasks
remain ‘unwritten’. There is the possibility that if the user knows what the system
is trying to do then they will help it by adding these tasks in, but this is something
that can only be tested by real-world use over time.
Other possible applications that could beneﬁt from the ILP approach and would have more
data to learn from include a user model within a phone assistant and a user model within
an Email assistant (this latter suggestion is expanded upon in the next chapter). In both
cases the agent would have access to large amounts of background knowledge and would
be required to predict several values in response to a query. It may be that one of these
applications would produce more instant results when tested on real-world data.

Chapter 9
Conclusions
Throughout this dissertation several ideas and experiments have been presented, each
intended to produce conclusions with regards to various parts of the hypothesis originally
stated in Chapter 1. This chapter will bring all the ﬁndings together to ascertain whether
the hypothesis has been conﬁrmed. This will then be followed by a discussion of future
directions for the research, expansions to the system created, and alternative applications
for Dilum.
9.1
Overview and Summary
The overall problem to be solved within the dissertation was described in Chapter 1.
Could an exploratory ILP learner be used eﬀectively to construct a user model that it
could use to make multiple-valued predictions when the model was complex and vague,
the few available examples would contain noise, and the time available was limited? A
survey of the existing work towards parts of this problem, was presented in Chapter 2.
The aims that would actively aﬀect the modelling system were: the need to ensure that
the predictions made by model had a high level of recall and precision, and were returned
quickly enough to still be relevent, and the need to ensure that as little extra eﬀort as
possible was required of the user. These aims should be adhered to even in the presence
of noisy data.
The reviewed work on Inductive Logic Programming, and the attempts to solve the prob-
lems that would be encountered whilst trying to construct a model of the user, indicated
that most of the approaches discussed were unsuitable for this particular learning prob-
lem. One of the more popular, general, ILP engines would provide a suﬃciently robust
base from which the system could be constructed. However, the use of clustering whilst
attempting to learn from positive-only data showed promise as an aid to the learning
process and was further investigated in Chapter 3.
198

CHAPTER 9. CONCLUSIONS
199
The uncertainty present within the data from which the model was to be constructed,
due to the data being sparse and containing noise, would require some form of measure
to ensure that it was taken into account during predictions. Since the modelling system
consists of both the model construction method and a mechanism for using the model
to produce predictions, the approach used to cope with noisy data was to defer working
with the eﬀects of noise until the stage at which the model was used to answer queries.
The query system would then either generate a measure of uncertainty for each answer
produced or present several answers to the user so that they can choose to the most
appropriate action.
A discussion of the merits of various measures of uncertainty was
deferred until a method for querying the model was produced in Chapter 7.
Chapter 3 began by clarifying the nature of the data set that would be collected from iDiary
and describing the method used to select examples of tasks in immediate sequence. The
‘empty task’ was presented as a means to indicate when tasks within the diary had been
scheduled without any other tasks preceding or following them. The examples containing
an empty task were only to be used when evaluating the predictions made using the
model, therefore these examples were not included in the data sets used in the experiments
presented in the following chapters.
Chapter 3 then performed a comparison of suitable ILP engines, concluding that, of the
methods evaluated, Aleph had the most potential to produce the required results. The
experiments also demonstrated that partitioning the data into separate sets that rep-
resented distinct subconcepts enabled Aleph to reliably produce the beginnings of the
required theory for each subconcept. These beneﬁts were also shown later by the diﬀer-
ence in the performance of TILDE in Chapter 5 on separate subconcepts and on the data
set as a whole, and then by the investigation carried out in Chapter 6. These results led
to the conclusion that for a vague concept (i.e. a concept made up of seemingly unre-
lated subconcepts), partitioning the data set representing the concept, and dealing with
each subconcept represented by a partition separately, enabled top-down learners such as
TILDE and Aleph to produce the required concept representation.
Having partitioned the data into a collection of subconcepts, Aleph still required further
assistance in order to produce a theory for each of these separate learning problems.
Another idea, commonly used in machine learning, that was demonstrated in Chapter 3
was the use of ‘near misses’ as additional negative examples that could force Aleph to
produce clauses with enough specialisation to be able to make the required multiple-value
predictions. Chapter 4, after presenting a method for generating these additional negative
examples, focused on the learning problem presented to Aleph. The amount of search space
that Aleph would need to explore before ﬁnding the target clause was determined by the
length of the required clause in combination with the number of background predicates
that could be applied to each case. The multiple-predictions required of each theory that
would be included within the user model, the minimum length of each target clause and

CHAPTER 9. CONCLUSIONS
200
the large number of background predicates available produced a search space that was too
large for Aleph to be able to reliably ﬁnd the required solution within the time available.
However, additional background knowledge that the diﬀerent attributes of each example
were independent enabled the problem to be split into a series of independent dimensions
that could be learnt separately and the results combined. Each dimension consisted of
a target clause that could be thought of as a subset of the literals contained within the
overall target clause, and a reduced set of background knowledge that only contained
those predicates relevant to the particular dimension. Further investigation of this idea
showed that the reduced hypothesis space presented to Aleph for each subproblem was
small enough that Aleph could reliably produce the answer that best described the concept
represented by the examples in the data set each time.
The method Dilum was presented, containing both the data partitioning and concept
splitting methods investigated. A data set from which a model was to be learnt would
be partitioned into groups, and each group treated as a separate subconcept. The union
of the results from each group would form the overall model. A theory would be learnt
for each group of examples by splitting the subconcept it represented into a collection of
smaller, independent, learning problems and combining the results of these problems.
The ideas presented in Chapter 4 were explored in greater depth in Chapter 5 in order to
ascertain the general nature of concepts that could be learnt using Dilum. An artiﬁcial
experiment, based on data captured in an earlier data-gathering exercise, was set up as
an additional user-model to be learnt. The information available about the model to learn
within this chapter diﬀered from the previous model, and new methods for partitioning and
negative example generation were created. The concepts contained within the model also
diﬀered in that they did not necessarily require specialisation from all of the dimensions
that the learning problem had been split into. The new method of result combination
involved reformulating the results returned as a new learning problem with a reduced
set of background knowledge and giving it back to Aleph to construct the ﬁnal theory.
This alternative method showed that Dilum could also be used to reduce the original
background knowledge to only those predicates that were relevant whilst still allowing
the learner to ‘construct’ a theory. Independence between the diﬀerent dimensions of the
learning problem was provided by referring to additional background knowledge about
the manner in which the examples within the data set had been given their diﬀerent
classiﬁcations. When making a comparison of two objects with multiple attributes, users
will have a tendency to only focus on subsets of the data presented, rather than comparing
all the values at the same time.
Reﬂecting this knowledge in the user model enabled
the required independence assumption and provided the basis for the new method for
generating negative examples.
Having successfully constructed the second user model, the ﬁndings from Chapters 4 and
5 were combined to present a series of constraints and considerations that captured the

CHAPTER 9. CONCLUSIONS
201
concepts and situations where Dilum should be used. The main idea presented was that
Dilum should be considered if the concept must be learnt within a limited time, the
hypothesis space for the concept to be learnt is too large to be searched conclusively
within that time, and the concept can be broken down into independent dimensions to
be processed as separate learning problems. The user would also need to ensure that the
learner focuses on each dimensional learning problem it is presented with by including
some form of stimuli (not necessarily additional negative examples). Partitioning of the
data set into separate groups of examples that represent diﬀerent subconcepts would only
be needed if the concept is vague and hence can only be represented by a disjunctive theory.
If partitioning is used then any ideas used in the partitioning should be transferred to the
generation of stimuli used when learning so that only one deﬁnition of what is and is not
part of the target concept is used within the system. If partitioning is not used then Dilum
is still suitable to be used to learn the concept as the entire data set can be regarded as a
single partition provided an assumption of independence between dimensions can still be
made. A further investigation in Chapter 6 was used to show the range of beneﬁts that
partitioning the data can bring to an ILP learning problem where the target theory is
complex and disjunctive. Partitioning was shown to improve the eﬃciency of the learning
process by removing the interference introduced by the presence of more than one distinct
group of positive examples within the data set.
Chapter 7 described the part of system required to query the model produced by Dilum.
The query module included a set of conditional probability distributions, generated from
the clusters of examples constructed during the generation of the model. These distribu-
tions enabled the overall system to deal with noisy data, and also improved the precision
of the predictions by enabling the query system to ﬁlter the sets of answers returned by
the model that only diﬀered in their duration.
Chapter 8 detailed the ﬁnal part of the investigation required to conﬁrm the hypothesis:
evaluation of the model that Dilum produced. It had already been shown how the ideas
contained within the rest of the hypothesis enabled Aleph to produce the model, but
evaluation was now required to show that the system constructed could reliably produce
a model that was usable and had high levels of recall and precision.
The small example sets collected from volunteers’ real-world diaries meant that the assess-
ment of the full potential of Dilum and its comparison to the other popular user-modelling
methods (k-nearest neighbour and naive Bayes classiﬁcation) could not be achieved using
the sets of real data collected. Therefore a visual inspection of the quality of the models
that Dilum produced using this real data was carried out to ensure that it could pro-
duce the best model from the given data, and then the comparison to other methods was
carried out using a set of artiﬁcial data that had been constructed from the information
contained within the real data. The results from the experiments showed conclusively that
the model produced by Dilum gave a better performance in terms of precision, recall and

CHAPTER 9. CONCLUSIONS
202
understandability than the other methods. The performance of Dilum was also shown to
decrease very little when it was presented with data containing increasing levels of noise.
The criteria for evaluation of Aleph using Dilum and TILDE using partitioning and arti-
ﬁcially generated negative examples (referred to as Dilum pt 1) against the other, more
commonly used, user modelling methods were: recall, precision, understandability, ability
to deal with noise, and the time taken to produce the model. Both Aleph using Dilum
and TILDE using Dilum pt 1 (in conjunction with the uncertainty measures constructed)
gave overall performances that were much better than the other user modelling methods.
Both k-nearest neighbour and naive Bayes classiﬁcation were not suited to the modelling
task because it did not require predictions of classiﬁcation from a ﬁnite set of possible val-
ues. The set of possible values that could be predicted was very large and required more
sophisticated techniques, such as those brought by the use of ILP, that could generalise
over the data and highlight particular relations, to enable correct predictions to be made.
A ﬁnal set of experiments in Chapter 8 was used to demonstrate how much Dilum improved
the scalability of Aleph for this user modelling task. The results showed that, for this kind
of learning problem, as the size of the problem increased, the computation time required
by Aleph using Dilum scaled signiﬁcantly better than Aleph on its own.
9.1.1
Conclusions
The preceding chapters within this dissertation have presented the methods required for
an exploratory ILP engine such as Aleph to generate a user model from the sort of data
that iDiary could expect to encounter. It has been shown that the techniques included
within Dilum signiﬁcantly reduce the amount of computation time required by Aleph to
construct the user model. Experimental evidence has demonstrated that, as the size of the
learning problem increases, the computational time required by Aleph using Dilum scales
better than the computational time required by Aleph on its own. Chapter 5 identiﬁed the
general properties that a concept should display in order for use of Dilum to be considered.
These properties can be used to outline the boundaries for the class of concepts for which
the hypothesis, restated below, stands.
It is possible to use ILP eﬀectively to create, within a limited (reasonable)
time, a usable user model of a vague concept with a large hypothesis space, if
the model to be learnt is broken down into separate subconcepts, each concept
is learnt as a sum of separate dimensions, and a measure on uncertainty is
incorporated into the model query process.
The hypothesis states that the method outlined is enough to enable the use of ILP to
produce the required user model, i.e. it does not state that the method presented is the
only way in which the model could have been built. Could the model have been achieved
using ILP in the normal setting in another way?

CHAPTER 9. CONCLUSIONS
203
The arguments for the decisions taken during the construction of the user model were
directed by the properties required of a user model within an application. The model
should be: understandable, have a high level of recall even in the presence of reasonable
levels of noise, precise, capable of providing explanations if required, able to generate fully
detailed predictions and could be produced in a short period of time.
In order to satisfy the requirement of understandability, the complexity of the overall model
had to be reduced by breaking down the learning problem into separate subconcepts. The
learning problem presented at this point was shown to be outside the range of theories
that an exploratory ILP algorithm such as Aleph would tend to produce, hence some
form of assistance was required. A set of stimuli was needed in order to ensure that the
theory produced would be able to generate values for all the details of a prediction. As
the complexity of the learning problem was beyond the ability of the exploratory learner
within the reasonable amount of time available, more assistance was required to enable the
learner to produce the hypotheses that would be used to form the model. It was argued
earlier in Section 1.2 that breaking down the model into separate prediction problems for
each attribute that were not combined into an overall theory (i.e. a relation between the
query task and a particular attribute rather than two tasks) had a detrimental eﬀect on
the understandability of the model, and did not bring any beneﬁts. Each relation would
still be given the entire query task as an input, and hence would still require enough
specialisation on the variables within that input task to ensure a high level of recall. Each
learning problem would still require a level of complexity that was high enough to prevent
an exploratory ILP engine from reliably producing the required theory within the given
time limit.
Although it is likely that some other method could be used to produce the model if some
of the requirements on the model produced were relaxed, these requirements are essential
for the user model to be actively used and trusted by the user. However, it cannot be
proven that there is no other method of ILP that would enable a model ﬁtting the given
description to be built according to these requirements, therefore the hypothesis can only
state that the method described is one way that will enable the model to be built.
Can this hypothesis be generalised to be applied to all user models? No. The deﬁnition
of a user model is as wide-ranging as the deﬁnition of an intelligent application, and the
methods used for modelling vary quite considerably in their approaches.
Could this hypothesis be applied to all user models that can be constructed using ILP?
Given the wide range of possible models that could be attempted using ILP, and the
diﬀerent possible approaches to ILP, it would be very diﬃcult to show that all models could
be constructed using this method. If the concept represented by the model displays the
properties discussed in Chapter 5, satisﬁes the constraints discussed, and an exploratory
ILP engine that approaches the hypothesis space in the same way as Aleph is to be used,
then the hypothesis not only can be applied but must be used to achieve the required model.

CHAPTER 9. CONCLUSIONS
204
However, the ideas within the hypothesis are aimed at dealing with a particular situation
that could arise and, whilst they enable the learner to produce a model, they remove the
completeness of the learner unless complete independence between the dimensions of the
learning problem is guaranteed. Therefore, these techniques should only be applied where
it is essential or where it can be guaranteed not to aﬀect the quality of the model produced,
not as a standard method of user model production.
9.2
Future Work
There are several areas where further work could bring interesting results. Not only could
the modelling system that has been implemented be improved with regards to the quality
of the existing model produced, but it could also be expanded to include the eﬀects of time
and modelling of longer sequences of tasks. In addition, the ideas demonstrated could be
used to build other models.
9.2.1
Completion of the Modelling System
The ideas presented within this dissertation have dealt with the initial construction of the
user model. However, a static model will very quickly become out of date and require
updating. Rather than learn a new model each time, the existing model should be revised
as this is likely to be a more eﬃcient method of model management. As the data set to be
learnt from has been partitioned, the modelling system has the advantage that it does not
have to update the entire model each time, only those parts that not longer agree with
the data collected from the user.
Ensuring that the model remains up to date requires two separate additions to the mod-
elling processes. In addition to revision of the model once new data has been collected,
the concept of time must be introduced into the examples collected so that the system will
know when an example has aged suﬃciently to be no longer relevant. ‘Example forgetting’
is suggested by a number of sources, including Maes and Kozierok [1993], and Widmer
and Kubat [1996], who present the idea of using a dynamic window of examples that are
considered relevant to the model. The size of the window grows to promote stability of
the concept and hence weather small ‘blips’ in the data, or shrinks to facilitate a rapid
response to concept changes. The precedents for when to change the size of the window
depend on observations that enable the system to conclude whether a current change is
permanent or just noise.
An additional problem encountered during the evaluation of Dilum on real data was that to
use ILP for an application such as user modelling requires a large amount of background
information to be made available to the system. This amount of information not only
presents a problem when the learner attempts to include it in the concept being constructed

CHAPTER 9. CONCLUSIONS
205
due the large number of background predicates that it introduces, but is also diﬃcult to
gather. Without knowledge such as a particular string of characters referring to a company
name or a person, or two items being related in some way, an ILP engine is unable to
reason over the data that is provided.
The most obvious data sources from which to
gather this information are items such as company directories, or documents held by the
user. This latter source, and sources like it, would require the construction of some form
of ontology from the data that could then be translated into the structured information
required. Unfortunately, the current state of research into the automatic construction of
ontologies (e.g. [Maedche and Staab, 2000]) indicates that the creation of an ontology is
at best only semi-automatic, requiring direction from a knowledge engineer in order to
complete its information gathering. For construction of a static user-model this would not
be considered too large a task, but as the user’s activities, and hence the entities they
associate with, alter over time, new objects will enter the system for which no information
is available. Whilst ILP brings beneﬁts to such knowledge intensive areas, a solution to
this problem must be produced before ILP can be successfully applied as an automated
process in a real-world application.
9.2.2
Expansion of the User Model
Having produced a model of a relatively simple concept, there are a few extensions that
would improve the usefulness of the model to the user. The most obvious extension would
be to increase the length of the sequences modelled to allow learning and prediction of
sequences of tasks of any length.
One way to approach this task would be to use some form of plan recognition. The more
common sequences of tasks could be included within a library that iDiary could access
and then use the rules created to ﬁll in the rest of the details of the tasks. However, as
was noted earlier, iDiary would still require some means to include sequences that were
not amongst the library of plans or plan elements it had been given.
This extension of the model does not have a simple solution. The most important alteration
would be to preserve the information about the length and composition of each sequence
that is collected. When working with longer sequences of tasks, each of the tasks within
the sequence could be dependent upon any subset of the other tasks within the sequence.
At present each sequence is split into pairs of tasks, and any dependency between tasks
contained within longer sequences is lost.
One possibility to preserve this information would be to attempt to construct some form
of Bayesian Network [Pearl, 1988] from the collected data. The most important feature
of a Bayesian Network is that it allows the ability to represent both dependencies and
independencies between the items represented by the nodes contained within the network.
Figure 9.1 shows a simple Bayesian network, the nodes represent variables and arrows

CHAPTER 9. CONCLUSIONS
206
C
B
D
A
F
E
Figure 9.1: A Simple Bayesian Network
¬C
C
F ∧A
F ∧¬A
¬F ∧A
¬F ∧¬A
0.4
0.6
0.1
0.9
0.8
0.2
0.5
0.5
Figure 9.2: The Conditional Probability Table for node C
between nodes represent dependencies between those nodes. Each node has a conditional
probability table which gives the conditional probability distribution over the variable
contained within the node for all possible combinations of instantiation of the variables
contained within the node’s parents. For example, the variable C in Figure 9.1 may have
the table listed in Figure 9.2.
To integrate the use of Bayesian Networks into the user model, the rules over pairs of tasks
could be constructed and used during the prediction as before, but the longer sequences of
tasks would be captured in their enterity and used to produce the conditional probabilty
tables found within each node of the network. The information from the network would
be used to dictate whether additional tasks would be added onto either end of the initial
immediate sequence that was constructed. A direct implementation of this idea would
be an expensive solution because the requirement to model all the dependencies between
tasks would mean that each task would have to be connected to all the other tasks within
the network. Investigation into the use of constraints and/or ‘common knowledge’ is one
possible direction that could reduce the number of connections required.
An additional extension to the model would be to change from the modelling of immediate
sequences to the modelling of dependencies between tasks. This adds an extra amount
of diﬃculty to the modelling process because the present approach of specialising each
attribute of each task would mean that some queries from the user would not be covered.
For example, suppose the modelling system had the following example to be covered by a
theory:

CHAPTER 9. CONCLUSIONS
207
seq(task(type(prep),subject(idiary),location(n/a),
duration(1/30),preferred_time(10-00),preferred_day(monday)),
seq(task(type(demo),subject(idiary),location(103),
duration(1/30),preferred_time(13-00),preferred_day(monday))).
This would be covered by the following rule within the sequences user model:
seq(task(type(A),subject(B),location(C),
duration(D),preferred_time(E),preferred_day(F)),
seq(task(type(G),subject(B),location(H),
duration(I),preferred_time(J),preferred_day(F))):-
A=prep, project(B),
C=n/a,
shorterthan(D,2/0),
G=demo, room(H),
shorterthan(I,2/0).
However this rule has lost the extra time between the two tasks that was contained within
the example. In order to transform this into a rule about dependencies between tasks,
information about the preferred time and day would have to be included.
This would give a rule such as:
seq(task(type(A),subject(B),location(C),
duration(D),preferred_time(E),preferred_day(F)),
seq(task(type(G),subject(B),location(H),
duration(I),preferred_time(J),preferred_day(F))):-
A=prep, project(B),
C=n/a,
shorterthan(D,2/0),
G=demo, room(H),
shorterthan2(I,2/0),
earlierthan(E,11-00),laterthan(E,9-00),
earlierthan(J,14-00),laterthan(J,12-00).
This rule successfully captures all the information contained within the given example, but
is also too specialised in terms of the preferred time of each task. Only query tasks with
preferred times that matched those of the specialisations would produce an answer. By
increasing the range of times covered by each rule, the rules then lose the information that
the additional literals had been included in order to preserve. This leads to the conclusion

CHAPTER 9. CONCLUSIONS
208
that more sophisticated predicates that would work with the amount of time and number
of days between each task should be included within the given background knowledge.
The ﬁnal obvious extension would be the inclusion of context (e.g. the date) within the
modelling and query processes, this would allow the model to include such ideas as various
events happening due to it being a special day (e.g. a bank holiday). Capturing this sort
of concept will be diﬃcult due to the sparse amount of data collected. For example, if
the modelling system does not have more than one instance of a bank holiday within the
window of examples collected then it cannot reason that the events on that day are a
regular occurrence that must be modelled.
9.2.3
Other Application Areas
Chapter 5 considered the general properties that a concept should have for use of Dilum to
be considered. What other applications or areas would beneﬁt from the ideas presented?
The immediate area of application that presents itself would be as a user modelling system
for other personal assistants that model their users. Examples of this would include an
agent for ﬁltering Email and an agent to manage phone calls.
Both would work with
roughly the same information as both react to an event that involves somebody attempting
to contact their user.
An Email ﬁltering agent would contain a relation such as the following to be learnt:
email_action(email(Date,Time,Sender,Other_recipients,
Subject_keys,Content_keys),
action(Action,Folder,Notify,Priority)).
The background knowledge presented would include:
1. key terms (e.g. meeting, one-to-one),
2. Information about senders and recipients,
3. Information about projects, places, companies etc,
4. Predicates to consult other agents, and
5. Date/time predicates.
This relation learning problem is slightly less complicated than that of sequences of tasks
because only the user’s actions will be predicted, not the content of the Email received.
However, this still presents a vague concept to learn with a large hypothesis space to
explore.

CHAPTER 9. CONCLUSIONS
209
It should be possible for each subconcept to be broken up into a series of dimensions, one
suggestion would be:
• Date and time,
• Sender and other recipients,
• Subject keywords,
• Content keywords,
• Action,
• Folder,
• Notify, and
• Priority.
As the details of the Email part of the relation do not necessarily all require specialisations,
the results could be reformulated as a new learning problem. However, this would need to
be tested during system construction.
What other areas could the ideas presented within Dilum be used for? Due to the similarity
of some of the ideas to methods currently being used within the data mining area, at ﬁrst
sight this would seem to be a promising direction to consider. In particular, the similarities
between feature selection within data mining and the removal of redundant background
knowledge by using the second version of Dilum are of interest.
However, the ways in which the methods within Dilum are used to manipulate the data
are so diﬀerent from the requirements of data mining that this is unlikely to be a feasible
application. One of the the main problems addressed within data mining is how to work
with extremely large amounts of data.
When working with a collection of individual
dimensions, Dilum carries out one learning experiment per dimension, using the same set
of data each time. If the ﬁnal recombination stage is also a learning problem, then the same
data is used once more. If ideas from some of the existing work on the application of ILP
to data mining (for example [Blockeel et al., 1999, Provost and Kolluri, 1999]) could be
used then perhaps this direction would become more feasible, but the ideas within Dilum
are more concerned with dealing with large amounts of background knowledge than large
data sets. A number of the proposed methods (such as partitioning as a pre-processing
stage) could only be used because it was known that the data set would be small and so
the process would not be particularly expensive.
Due to the amount of information required about the target concept, it is unlikely that
Dilum could be used very well within areas such as concept discovery, i.e. where the learner
is used to search for possible underlying concepts within data sets that the user does not

CHAPTER 9. CONCLUSIONS
210
currently have a known explanation for. This is an active area where the beneﬁts of using
ILP are immediately apparent, but with other capable systems being developed for this
purpose, the ideas within Dilum are aimed at exploration of other application areas that
ILP can enhance. Whilst the successful introduction of ILP to a developing system is likely
to still require more knowledge than user modelling methods such as k-nearest neighbour,
demonstration that it can be applied within popular areas such as user modelling should
provide encouragement for further applications to be suggested.

List of References
[Alphonse and Matwin., 2001] Alphonse, E. and Matwin., S. (2001).
Feature selection
for bottom-up inductive logic programming learners. In Rouveirol, C. and Sebag, M.,
editors, Proceedings of the Work-in-Progress Track at the 11th International Conference
on Inductive Logic Programming, pages 1–12.
[Angluin and Laird, 1988] Angluin, D. and Laird, P. (1988). Learning from noisy exam-
ples. Machine Learning, 2(4):343–370.
[Anthony and Frisch, 1997] Anthony, S. and Frisch, A. (1997). Generating numerical lit-
erals during reﬁnement. In Dˇzeroski, S. and Lavraˇc, N., editors, Proceedings of the 7th
International Workshop on Inductive Logic Programming, volume 1297 of Lecture Notes
in Artiﬁcial Intelligence, pages 61–76. Springer-Verlag.
[Aslam and Decatur, 1993] Aslam, J. A. and Decatur, S. E. (1993). General bounds on
statistical query learning and PAC learning with noise via hypothesis boosting.
In
Proceedings of the 34rd Annual Symposium on Foundations of Computer Science, pages
282–291. IEEE Computer Society Press, Los Alamitos, CA.
[Ball and Hall, 1967] Ball, G. and Hall, D. (1967). A clustering technique for summarizing
multivariate data. Behaviorial Sciences, 12(2):153–155.
[Bauer and Paul, 1993] Bauer, M. and Paul, G. (1993). Logic-based plan recognition for
intelligent help systems. Technical Report RR-93-43, Deutsches Forschungszentrum f¨ur
K¨unstliche Intelligenz GmbH, Erwin-Schr¨odinger Strasse, Germany.
[Beck et al., 2003] Beck, J., Jia, P., Sison, J., and Mostow, J. (2003). Predicting student
help-request behaviour in an intelligent tutor for reading.
In Proceedings of the 9th
International Conference on User Modeling, Johnstown, PA, USA. Springer-Verlag.
[Billsus and Pazzani, 1999] Billsus, D. and Pazzani, M. J. (1999). A personal news agent
that talks, learns and explains. In Proceedings of the Third International Conference on
Autonomous Agents (Agents’99), pages 268–275, Seattle, WA, USA. ACM Press.
[Biswas et al., 1981] Biswas, G., Jain, A. K., and Dubes, R. (1981). Evaluation of projec-
tion algorithms. IEEE Trans. Pattern Analysis and Machine Intelligence, PAM1-3:701–
708.
211

LIST OF REFERENCES
212
[Blockeel and De Raedt, 1998] Blockeel, H. and De Raedt, L. (1998). Top-down induction
of logical decision trees. Artiﬁcial Intelligence, 101:285–297.
[Blockeel et al., 1999] Blockeel, H., De Raedt, L., Jacobs, N., and Demoen, B. (1999).
Scaling up inductive logic programming by learning from interpretations. Data Mining
and Knowledge Discovery, 3(1):59–93.
[Blockeel et al., 1998] Blockeel, H., De Raedt, L., and Ramon, J. (1998). Top-down in-
duction of clustering trees. In Shavlik, J., editor, Proceedings of the 15th International
Conference on Machine Learning, pages 55–63. Morgan Kaufmann.
[Blockeel et al., 2001] Blockeel, H., Dehaspe, L., and Ramon, J. (2001). The ACE data
mining system. http://www.cs.kuleuven.ac.be/˜dtai/ACE/.
[Bostr¨om, 1998] Bostr¨om, H. (1998). Predicate invention and learning from positive ex-
amples only. In Proc. of the Tenth European Conference on Machine Learning, pages
226–237. Springer-Verlag.
[Bunt et al., 2004] Bunt, A., Conati, C., and J., M. (2004). What role can adaptive sup-
port play in an adaptable system? In Proceedings of IUI’04, International Conference
on Intelligent User Interfaces, Island of Madeira, Portugal.
[Camacho, 1994] Camacho, R. (1994). The use of background knowledge in inductive logic
programming. Master’s thesis, Oxford University Computing Laboratory, Oxford, UK.
[Cameron-Jones and Quinlan, 1994] Cameron-Jones, R. and Quinlan, J. (1994). Eﬃcient
top-down induction of logic programs. SIGART Bulletin, 5(1):33–42.
[Chan and Stolfo, 1993] Chan, P. and Stolfo, S. (1993). Toward parallel and distributed
learning by meta-learning. In AAAI Workshop in Knowledge Discovery in Databases,
pages 227–240.
[Chan and Stolfo, 1997] Chan, P. K. and Stolfo, S. J. (1997). Scalability of hierarchical
meta-learning on partitioned data. http://citeseer.nj.nec.com/93537.html.
[Cussens et al., 1999] Cussens, J., Dˇzeroski, S., and Erjavec, T. (1999). Morphosyntactic
tagging of Slovene using Progol. In Dˇzeroski, S. and Flach, P., editors, Proceedings of the
9th International Workshop on Inductive Logic Programming, volume 1634 of Lecture
Notes in Artiﬁcial Intelligence, pages 68–79. Springer-Verlag.
[Dastani et al., 2002] Dastani, M., Jacobs, N., Jonker, C. M., and Treur, J. (2002). Mod-
elling user preferences and mediating agents in electronic commerce. In Dignum, F. and
Cortes, U., editors, Agent-Mediated Electronic Commerce III, volume 2003 of Lecture
Notes in Computer Science, pages 163–193. Springer-Verlag.

LIST OF REFERENCES
213
[Davison and Hirsh, 1998] Davison, B. D. and Hirsh, H. (1998). Predicting sequences of
user actions. In Predicting the Future: AI Approaches to Time-Series Problems, pages 5–
12, Madison, WI. AAAI Press. Proceedings of AAAI-98/ICML-98 Workshop, published
as Technical Report WS-98-07.
[De Raedt and Dehaspe, 1997] De Raedt, L. and Dehaspe, L. (1997). Clausal discovery.
Machine Learning, 26:99–146.
[Decatur, 1996] Decatur, S. E. (1996). Learning in hybrid noise environments using sta-
tistical queries. In Fisher, D. and Lenz, H. J., editors, Learning from data: Artiﬁcial
Intelligence and Statistics V. Springer-Verlag, New York.
[Dehaspe and De Raedt, 1995] Dehaspe, L. and De Raedt, L. (1995). Parallel inductive
logic programming. In Proceedings of the MLnet Familiarization Workshop on Statistics,
Machine Learning and Knowledge Discovery in Databases.
[Dehaspe and De Raedt, 1996] Dehaspe, L. and De Raedt, L. (1996). DLAB: A declara-
tive language bias formalism. In Proceedings of the 10th International Symposium on
Methodologies for Intelligent Systems, volume 1079 of Lecture Notes in Artiﬁcial Intel-
ligence, pages 613–622. Springer-Verlag.
[Dehaspe and Toivonen, 1999] Dehaspe, L. and Toivonen, H. (1999). Discovery of frequent
datalog patterns. Data Mining and Knowledge Discovery, 3:7–36.
[Dent et al., 1992] Dent, L., Boticario, J., McDermott, J., Mitchell, T., and Zabowski, D.
(1992). A personal learning apprentice. In Proceedings of the Tenth National Conference
on Artiﬁcial Intelligence, pages 96–103.
[Dˇzeroski, 1995] Dˇzeroski, S. (1995). Learning ﬁrst-order clausal theories in the presence of
noise. In Aamodt, A. and Komorowski, J., editors, Proceedings of the 5th Scandinavian
Conference on Artiﬁcial Intelligence, pages 51–60. IOS, Amsterdam.
[Dˇzeroski et al., 1993] Dˇzeroski, S., Muggleton, S., and Russell, S. (1993). Learnability
of constrained logic programs. In Brazdil, P., editor, Proceedings of the 6th European
Conference on Machine Learning, volume 667 of Lecture Notes in Artiﬁcial Intelligence,
pages 342–347. Springer-Verlag.
[Dˇzeroski et al., 1992] Dˇzeroski, S., Muggleton, S., and Russell, S. J. (1992).
PAC-
learnability of determinate logic programs. In Proceedings of the Fifth Annual ACM
Workshop on Computational Learning Theory (COLT-92), Pittsburgh, Pennsylvania.
ACM Press.
[Eliassi-Rad and Shavlik, 2003] Eliassi-Rad, T. and Shavlik, J. (2003). A system for buil-
ing intelligent agents that learn to retrieve and extract information. User Modeling and
User-Adapted Interaction, 13:35–88.

LIST OF REFERENCES
214
[Feller, 1966] Feller, W. (1966). An Introduction to Probability Theory and its Applications
– Volumes 1 and 2. John Wiley, New York.
[Fisher, 1987] Fisher, D. (1987). Knowledge acquisition via incremental conceptual clus-
tering. Machine Learning, 2:139–172.
[F¨urnkranz, 1994] F¨urnkranz, J. (1994). Pruning methods for rule learning algorithms.
In Wrobel, S., editor, Proceedings of the 4th International Workshop on Inductive Logic
Programming, volume 237 of GMD-Studien, pages 321–336. Gesellschaft f¨ur Mathematik
und Datenverarbeitung MBH.
[F¨urnkranz, 1997] F¨urnkranz, J. (1997). Pruning algorithms for rule learning. Machine
Learning, 27(2):139–171.
[Gamberger and Lavraˇc, 2001] Gamberger, D. and Lavraˇc, N. (2001). Filtering noisy in-
stances and outliers. In Liu, H., editor, Instance Selection and Construction for Data
Mining, pages 375–394. Kluwer Academic Publishers.
[Gervasio et al., 1999] Gervasio, M., Iba, W., and Langley, P. (1999). Learning user eval-
uation functions for adaptive scheduling assistance. In Proceedings of the 16th Interna-
tional Conference on Machine Learning, pages 152–161. Morgan Kaufmann.
[Gorniak and Poole, 2000] Gorniak, P. and Poole, D. (2000). Predicting future user ac-
tions by observing unmodiﬁed applications.
In Seventeenth National Conference on
Artiﬁcial Intelligence (AAAI-2000).
[Gowda and Krishna, 1978] Gowda, K. and Krishna, G. (1978). Agglomerative clustering
using the concept of mutual nearest neighborhood. Pattern Recognition, 10:105–112.
[Hall et al., 1998] Hall, L., Chawla, N., and Bowyer, K. (1998). Combining decision trees
learned in parallel. In Working Notes of the KDD-97 Workshop on Distributed Data
Mining, pages 10–15.
[Hall et al., 1999] Hall, L. O., Chawla, N., Bowyer, K. W., and Kegelmeyer, W. P. (1999).
Learning rules from distributed data. In Workshop of the Fifth ACM SIGKDD Inter-
national Conference on Knowledge Discovery and Data Mining.
[Hartigan, 1975] Hartigan, J. A. (1975). Clustering Algorithms, chapter 3. John Wiley &
Sons, New York.
[Haussler, 1988] Haussler, D. (1988). Quantifying inductive bias: AI learning algorithms
and Valiant’s learning framework. Artiﬁcial Intelligence, 36(2):177–221.
[Herbrand, 1930] Herbrand, J. (1930). Investigations in proof theory: The properties of
true propositions. In van Heijenoort, J., editor, From Frege to Godel: A Source Book in
Mathematical Logic, pages 529–581. Harvard University Press.

LIST OF REFERENCES
215
[Jachowicz and Goebel, 1997] Jachowicz, P. and Goebel, R. (1997). Describing plan recog-
nition as non-monotonic reasoning and belief revision. In Proceedings of the 10th Aus-
tralian Joint Conference on Artiﬁcial Intelligence, pages 236–245, Perth, Australia.
[Jacobs, 1999] Jacobs, N. (1999). Adaplix: Towards adaptive websites. In Bra, P. D. and
Hardman, L., editors, Proceedings van de Informatiewetenschap’99 Conferentie, pages
22–28. Eindhoven University of Technology.
[Jacobs, 2000] Jacobs, N. (2000). The learning shell. In Rogers, S. and Iba, W., editors,
Adaptive User Interfaces, Papers from the 2000 AAAI Spring Symposium, pages 50–
53, 445 Burgess Drive, Menlo Park, California 94025. The American Association for
Artiﬁcial Intelligence.
[Jorge and Brazdil, 1996] Jorge, A. and Brazdil, P. (1996). Integrity constraints in ILP
using a Monte Carlo approach. In Muggleton, S., editor, Proceedings of the 6th Inter-
national Workshop on Inductive Logic Programming, volume 1314 of Lecture Notes in
Artiﬁcial Intelligence, pages 229–244. Springer-Verlag.
[Kakas and Riguzzi, 1997] Kakas, A. and Riguzzi, F. (1997). Learning with abduction. In
Dˇzeroski, S. and Lavraˇc, N., editors, Proceedings of the 7th International Workshop on
Inductive Logic Programming, volume 1297 of Lecture Notes in Artiﬁcial Intelligence,
pages 181–188. Springer-Verlag.
[Kamgar-Parsi and Kanal, 1985] Kamgar-Parsi, B. and Kanal, L. (1985). An improved
branch and bound algorithm for computing k-nearest neighbours. Pattern Recognition
Letters, 3:7–12.
[Kearns, 1993] Kearns, M. (1993). Eﬃcient noise-tolerant learning from statistical queries.
In Proceedings of the 25th Annual ACM Symposium on the Theory of Computing, pages
392–401.
[Kietz and Wrobel, 1991] Kietz, J.-U. and Wrobel, S. (1991). Controlling the complexity
of learning in logic through syntactic and task-oriented models. In Muggleton, S., editor,
Proceedings of the 1st International Workshop on Inductive Logic Programming, pages
107–126.
[Kirsten and Wrobel, 1998] Kirsten, M. and Wrobel, S. (1998). Relational distance-based
clustering.
In Page, D., editor, Proceedings of the 8th International Conference on
Inductive Logic Programming, volume 1446 of Lecture Notes in Artiﬁcial Intelligence,
pages 261–270. Springer-Verlag.
[Kleinbauer et al., 2003] Kleinbauer, T., Bauer, M., and Jameson, A. (2003).
Specter
- a user-centered view on ubiquitous computing. In ABIS 2003: 11th GI-Workshop
”Adaptivitt und Benutzermodellierung in interaktiven Softwaresystemen”, Karlsruhe,
Germany.

LIST OF REFERENCES
216
[Kohavi et al., 1997] Kohavi, R., Langley, P., and Yun, Y. (1997). The utility of feature
weighting in nearest-neighbour algorithms. In Proceedings of the 9th European Confer-
ence on Machine Learning. Springer-Verlag.
[Kononenko et al., 1997] Kononenko, I., Simec, E., and Robnik-Sikonja, M. (1997). Over-
coming the myopia of inductive learning algorithms with RELIEFF. Applied Intelligence,
7(1):39–55.
[Kozierok and Maes, 1992] Kozierok, R. and Maes, P. (1992). A learning interface agent
for scheduling meetings.
In Gray, W. D., Heﬂey, W. E., and Murray, D., editors,
Proceedings of the International Workshop on Intelligent User Interfaces, pages 81–88,
New York, NY, USA. ACM Press.
[Kramer and De Raedt, 2001] Kramer, S. and De Raedt, L. (2001). Feature construction
with version spaces for biochemical applications. In Proceedings of the 18th International
Conference on Machine Learning, pages 258–265. Morgan Kaufmann.
[Lashkari et al., 1994] Lashkari, Y., Metral, M., and Maes, P. (1994). Collaborative Inter-
face Agents. In Proceedings of the Twelfth National Conference on Artiﬁcial Intelligence,
volume 1. AAAI Press, Seattle, WA.
[Lavraˇc et al., 1999] Lavraˇc, N., Gamberger, D., and Jovanoski, V. (1999). A study of rel-
evance for learning in deductive databases. Journal of Logic Programming, 40(2/3):215–
249.
[Lee et al., 1977] Lee, R. C. T., Slagle, J. R., and Blum, H. (1977). A triangulation method
for the sequential mapping of points from n -space to two-space. IEEE Transactions on
Computers, 26(3):288–292.
[Li and Vit´anyi, 1991] Li, M. and Vit´anyi, P. M. B. (1991).
Learning simple concepts
under simple distributions. SIAM J. Comput., 20(5):911–935.
[Litman and Allen, 1984] Litman, D. J. and Allen, J. F. (1984). A plan recognition model
for clariﬁcation subdialogues. In Proc. of the 10th COLING, pages 302–311, Stanford,
CA.
[Lloyd, 1987] Lloyd, J. (1987). Foundations of Logic Programming (2nd ed). Symbolic
Computation. Springer-Verlag, Berlin.
[MacQueen, 1967] MacQueen, J. (1967). Some methods of classiﬁcation and analysis of
multivariate observations. In Proceedings of the Fifth Berkeley Symposium on Math-
emtical Statistics and Probability, pages 281–297.
[Maedche and Staab, 2000] Maedche, A. and Staab, S. (2000). Semi-automatic engineer-
ing of ontologies from text.

LIST OF REFERENCES
217
[Maes and Kozierok, 1993] Maes, P. and Kozierok, R. (1993). Learning interface agents.
In Proceedings of the 11th National Conference on Artiﬁcial Intelligence, pages 459–464,
Menlo Park, CA, USA. AAAI Press.
[Manandhar et al., 1998] Manandhar, S., Dˇzeroski, S., and Erjavec, T. (1998). Learning
multilingual morphology with CLOG. In Page, D., editor, Proceedings of the 8th Inter-
national Conference on Inductive Logic Programming, volume 1446 of Lecture Notes in
Artiﬁcial Intelligence, pages 135–144. Springer-Verlag.
[Markov, 1994] Markov, Z. (1994). Relational learning by heuristic evaluation of ground
data. In Wrobel, S., editor, Proceedings of the 4th International Workshop on Induc-
tive Logic Programming, volume 237 of GMD-Studien, pages 337–350. Gesellschaft f¨ur
Mathematik und Datenverarbeitung MBH.
[Markov, 1996] Markov, Z. (1996). λ-subsumption and its application to learning from
positive-only examples. In Muggleton, S., editor, Proceedings of the 6th International
Workshop on Inductive Logic Programming, volume 1314 of Lecture Notes in Artiﬁcial
Intelligence, pages 377–396. Springer-Verlag.
[Markov, 1998] Markov, Z. (1998). Generalization under implication by λ-subsumption.
In Page, D., editor, Proceedings of the 8th International Conference on Inductive Logic
Programming, volume 1446 of Lecture Notes in Artiﬁcial Intelligence, pages 215–224.
Springer-Verlag.
[MathFrm, 2000] MathFrm (2000). Math Forum.org - Library.
http://mathforum.org/library/drmath/view/61090.html.
[Mooney and Caliﬀ, 1995] Mooney, R. and Caliﬀ, M. (1995). Induction of ﬁrst-order de-
cision lists: Results on learning the past tense of English verbs. In De Raedt, L., editor,
Proceedings of the 5th International Workshop on Inductive Logic Programming, pages
145–146. Department of Computer Science, Katholieke Universiteit Leuven.
[Mostow and Aist, 2001] Mostow, J. and Aist, G. (2001). Evaluating tutors that listen:
An overview of project listen. In Forbus, K. and Feltovich, P., editors, Smart Machines
in Education, pages 169–234. Menlo Park, CA: AAAI.
[Muggleton, 1995a] Muggleton, S. (1995a). Inverse entailment and Progol. New Genera-
tion Computing, Special issue on Inductive Logic Programming, 13(3-4):245–286.
[Muggleton, 1995b] Muggleton, S. (1995b). Inverting entailment and Progol. In Machine
Intelligence, volume 14, pages 133–188. Oxford University Press.
[Muggleton, 1995c] Muggleton, S. (1995c). Stochastic logic programs. In De Raedt, L.,
editor, Proceedings of the 5th International Workshop on Inductive Logic Programming,
page 29. Department of Computer Science, Katholieke Universiteit Leuven.

LIST OF REFERENCES
218
[Muggleton, 1996] Muggleton, S. (1996). Learning from positive data. In Muggleton, S.,
editor, Proceedings of the 6th International Workshop on Inductive Logic Programming,
volume 1314 of Lecture Notes in Artiﬁcial Intelligence, pages 358–376. Springer-Verlag.
[Muggleton, 1998] Muggleton, S. (1998).
Completing inverse entailment.
In Page, D.,
editor, Proceedings of the 8th International Conference on Inductive Logic Programming,
volume 1446 of Lecture Notes in Artiﬁcial Intelligence, pages 245–249. Springer-Verlag.
[Muggleton and De Raedt, 1994] Muggleton, S. and De Raedt, L. (1994). Inductive logic
programming: Theory and methods. Journal of Logic Programming, 19/20:629–679.
[Muggleton and Feng, 1990] Muggleton, S. and Feng, C. (1990).
Eﬃcient induction of
logic programs. In Proceedings of the 1st Conference on Algorithmic Learning Theory,
pages 368–381. Ohmsma, Tokyo, Japan.
[Nienhuys-Cheng and de Wolf, 1997] Nienhuys-Cheng, S.-H. and de Wolf, R. (1997).
Foundations of Inductive Logic Programming, volume 1228 of Lecture Notes in Arti-
ﬁcial Intelligence. Springer-Verlag.
[Oates et al., 1995] Oates, T., Prasad, M. V. N., Lesser, V. R., and Decker, K. (1995). A
distributed problem solving approach to cooperative information gathering. In AAAI-95
Spring Symposium on Information Gathering from Heterogeneous, Distributed Environ-
ments, Stanford University, Stanford, CA.
[Pazzani and Billsus, 1997] Pazzani, M. and Billsus, D. (1997). Learning and revising user
proﬁles: The identiﬁcation of interesting web sites. Machine Learning, 27:313–331.
[Pearl, 1988] Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of
Plausible Inference. Morgan Kaufmann.
[Picard, 1995] Picard, R. (1995). Aﬀective Computing. Boston: MIT Press.
[Pirolli and Fu, 2003] Pirolli, P. and Fu, W.-T. (2003). Snif-act: A model of information
foraging on the world wide web. In Proceedings of the 9th International Conference on
User Modeling, Johnstown, PA, USA. Springer-Verlag.
[Plotkin, 1970] Plotkin, G. (1970). A note on inductive generalization. In Machine Intel-
ligence, volume 5, pages 153–163. Edinburgh University Press.
[Plotkin, 1971] Plotkin, G. (1971). A further note on inductive generalization. In Machine
Intelligence, volume 6, pages 101–124. Edinburgh University Press.
[Pompe, 1996] Pompe, U. (1996). Restricting the hypothesis space, guiding the search,
and handling the redundant information in inductive logic programming. Master’s the-
sis, Faculty of Computer and Information Science, University of Ljubljana, Ljubljana,
Slovenia.

LIST OF REFERENCES
219
[Popelinsk´y et al., 1994] Popelinsk´y, L., Flener, P., and ˇStˇep´ankov´a, O. (1994). ILP and
automatic programming: Towards three approaches. In Wrobel, S., editor, Proceed-
ings of the 4th International Workshop on Inductive Logic Programming, volume 237
of GMD-Studien, pages 351–364. Gesellschaft f¨ur Mathematik und Datenverarbeitung
MBH.
[Provost and Aronis, 1996] Provost, F. J. and Aronis, J. M. (1996). Scaling up inductive
learning with massive parallelism. Machine Learning, 23(1):33–46.
[Provost and Kolluri, 1999] Provost, F. J. and Kolluri, V. (1999). A survey of methods for
scaling up inductive algorithms. Data Mining and Knowledge Discovery, 3(2):131–169.
[Quinlan, 1986] Quinlan, J. (1986). Induction of decision trees. Machine Learning, 1:81–
106.
[Ragavan and Rendell, 1993] Ragavan, H. and Rendell, L. (1993). Lookahead feature con-
struction for learning hard concepts. In Proceedings of the 10th International Conference
on Machine Learning, pages 252–259. Morgan Kaufmann.
[Ramon and Bruynooghe, 1998] Ramon, J. and Bruynooghe, M. (1998). A framework for
deﬁning distances between ﬁrst-order logic objects. In Page, D., editor, Proceedings
of the 8th International Conference on Inductive Logic Programming, volume 1446 of
Lecture Notes in Artiﬁcial Intelligence, pages 271–280. Springer-Verlag.
[Ramon and De Raedt, 1999] Ramon, J. and De Raedt, L. (1999). Instance based function
learning. In Dˇzeroski, S. and Flach, P., editors, Proceedings of the 9th International
Workshop on Inductive Logic Programming, volume 1634 of Lecture Notes in Artiﬁcial
Intelligence, pages 268–278. Springer-Verlag.
[Rich, 1979] Rich, E. (1979). User modeling via stereotypes. Cognitive Science, 3:329–354.
[Rijsbergen, 1979] Rijsbergen, K. V. (1979). Information Retrieval. Butterworth, London.
[Robiner, 1989] Robiner, L. R. (1989). A tutorial on hidden markov models and selected
applications in speech recognition. In Proceedings of the IEE, volume 77(2), pages 257–
285.
[Rouveirol and Puget, 1989] Rouveirol, C. and Puget, J.-F. (1989). A simple solution for
inverting resolution. In Morik, K., editor, Proceedings of the 4th European Working
Session on Learning, pages 201–210. Pitman.
[Russell and Norvig, 1995] Russell, S. and Norvig, P. (1995). Artiﬁcial Intelligence: A
Modern Approach. Prentice Hall International Editions, Upper Saddle River, NJ.
[Scheirer et al., 2002] Scheirer, J., Fernandez, R., Klein, J., and Picard, R. W. (2002).
Frustrating the user on purpose: A step toward building an aﬀective computer. Inter-
acting with Computers, 14(2):93–118.

LIST OF REFERENCES
220
[Sebag, 1997] Sebag, M. (1997). Distance induction in ﬁrst order logic. In Dˇzeroski, S. and
Lavraˇc, N., editors, Proceedings of the 7th International Workshop on Inductive Logic
Programming, volume 1297 of Lecture Notes in Artiﬁcial Intelligence, pages 264–272.
Springer-Verlag.
[Sebag and Rouveirol, 1994] Sebag, M. and Rouveirol, C. (1994). Induction of maximally
general clauses consistent with integrity constraints. In Wrobel, S., editor, Proceedings of
the 4th International Workshop on Inductive Logic Programming, volume 237 of GMD-
Studien, pages 195–216. Gesellschaft f¨ur Mathematik und Datenverarbeitung MBH.
[Segal and Kephart, 1999] Segal, R. B. and Kephart, J. O. (1999). MailCat: an intelligent
assistant for organizing e-mail. In Proceedings of the Third International Conference on
Autonomous Agents (Agents’99), pages 276–282, Seattle, WA, USA. ACM Press.
[Sheth, 1994] Sheth, B. D. (1994). A learning approach to personalized information ﬁlter-
ing. Master’s thesis, MIT Media Lab.
[Shibata et al., 1999] Shibata, D., Inuzuka, N., Kato, S., Matsui, T., and Itoh, H. (1999).
An induction algorithm based on fuzzy logic programming. In Zhong, N. and Zhou, L.,
editors, Proceedings of the Third Paciﬁc-Asia Conference on Knowledge Discovery and
Data Mining, volume 1574 of Lecture Notes in Artiﬁcial Intelligence, pages 268–273.
Springer-Verlag.
[Srinivasan, 1999] Srinivasan, A. (1999). A study of two sampling methods for analysing
large datasets with ILP. Data Mining and Knowledge Discovery, 3(1):95–123.
[Srinivasan, 2001] Srinivasan, A. (2001). The Aleph Manual.
http://www.comlab.ox.ac.uk/oucl/research/areas/machlearn/Aleph/aleph.html.
[Srinivasan and Camacho, 1999] Srinivasan, A. and Camacho, R. (1999). Numerical rea-
soning with an ILP program capable of lazy evaluation and customised search. Journal
of Logic Programming, 40(2,3):185–214.
[Srinivasan and King, 1996] Srinivasan, A. and King, R. (1996).
Feature construction
with inductive logic programming: A study of quantitative predictions of biological
activity aided by structural attributes. In Muggleton, S., editor, Proceedings of the 6th
International Workshop on Inductive Logic Programming, volume 1314 of Lecture Notes
in Artiﬁcial Intelligence, pages 89–104. Springer-Verlag.
[Valiant, 1984] Valiant, L. (1984).
A theory of the learnable.
Communications of the
ACM, 27(11):1134–1142.
[Valiant, 1985] Valiant, L. (1985). Learning disjunctions of conjunctions. In Proceedings
of the Ninth International Joint Conference on Artiﬁcial Intelligence.

LIST OF REFERENCES
221
[Wang, 2000] Wang, Y. (2000). Parallel inductive logic in data mining. Master’s thesis,
Dept. of Computing and Information Science, Queens University, Kingston, Ontario,
Canada.
[Webb et al., 2001] Webb, G. I., Pazzani, M. J., and Billsus, D. (2001). Machine learning
for user modelling. User Modelling and User-Adapted Interaction, 11:19–29.
[Werbos, 1974] Werbos, P. (1974).
Beyond Regression: New Tools for Prediction and
Analysis in the Behavioural Sciences. PhD thesis, Harvard University, Cambridge, MA.
[Widmer and Kubat, 1996] Widmer, G. and Kubat, M. (1996). Learning in the presence
of concept drift and hidden contexts. Machine Learning, 23:69–101.
[Widrow and Lehr, 1990] Widrow, B. and Lehr, M. (1990). 30 years of adaptive neural
networks: Perceptron, madaline, and backpropagation. In Proceedings of the IEEE,
volume 78(9), pages 1415–1442.
[Wobcke et al., 2000] Wobcke, W., Azvine, B., Dijan, D., and Tsui, K. (2000). The in-
telligent assistant: An overview. Technical report, Intelligent Systems Research Group,
BT Laboratories.
[Wobcke and Sichanie, 1999] Wobcke, W. and Sichanie, A. (1999). Personal diary man-
agement with fuzzy preferences. Technical report, Intellifent Systems Research Group,
BT Laboratories.
[Zelle et al., 1994] Zelle, J., Mooney, R., and Konvisser, J. (1994). Combining top-down
and bottom-up techniques in inductive logic programming. In Cohen, W. and Hirsh, H.,
editors, Proceedings of the 11th International Conference on Machine Learning, pages
343–351. Morgan Kaufmann.
[Zelle et al., 1995] Zelle, J., Thompson, C., Caliﬀ, M., and Mooney, R. (1995). Inducing
logic programs without explicit negative examples. In De Raedt, L., editor, Proceedings
of the 5th International Workshop on Inductive Logic Programming, pages 403–416.
Department of Computer Science, Katholieke Universiteit Leuven.
[Zhu et al., 2003] Zhu, T., Greiner, R., and H¨aubl, G. (2003). Learning a model of a web
user’s interests. In Proceedings of the 9th International Conference on User Modeling,
Johnstown, PA, USA. Springer-Verlag.
[Zukerman and Albrecht, 2001] Zukerman, I. and Albrecht, D. W. (2001). Predictive sta-
tistical models for user modelling. User Modelling and User-Adapted Interaction, 11:5–
18.

