A UNIFIED COMPILER FRAMEWORK FOR
PROGRAM ANALYSIS, OPTIMIZATION, AND
AUTOMATIC VECTORIZATION WITH CHAINS
OF RECURRENCES
Name: Yixin Shou
Department: Department of Computer Science
Major Professor: Robert A. van Engelen
Degree: Ph.D.
Term Degree Awarded: Spring, 2009
In this dissertation I propose a uniﬁed compiler framework for program analysis, op-
timization, and automatic vectorization with techniques based on the Chains of Recurrences
(CR) algebra. The root theoretical foundations of the CR algebra and the CR# additions
for program analysis, that I speciﬁcally developed for this dissertation research, are given.
Based on the theory, I propose an extension of the Single Static Assignment (SSA) form,
called the Inductive SSA form. The form associates inductive proofs with SSA variables to
describe their inductive properties. The proofs are derived with algorithms based on the
CR# theory. Inductive SSA provides an internal program representation that facilitates
more eﬀective loop analysis by a compiler. Based on the Inductive SSA form, I describe and
evaluate a collection of compiler algorithms for program optimization at the SSA level.
More speciﬁcally, Inductive SSA merges static SSA information with semantic recurrence
information to describe the value progressions of loop-variant variables. I will show how
Inductive SSA enables and/or enhances loop-variant variable analysis, improves dependence
testing for loops, and how it can be eﬀectively used by many classic compiler optimizations
to improve compilation results.

Inductive SSA is based on my theoretical work on extending the CR formalism by
the CR# (CR-sharp) algebra and lattice to eﬀectively represent the value sequences and
value ranges of conditionally updated variables in loops. Using empirical results on SPEC
benchmarks with GCC 4.1, I will show that the Inductive SSA form captures more recurrence
relations compared to state-of-the-art induction variable recognition techniques. Because
the CR# term rewriting system is complete (conﬂuent and terminating), it can be used for
congruence detection in programs at all optimization levels. The Inductive SSA program
analysis is far less sensitive to syntactical code changes, due to the real semantic information
it captures. As a result of the independence of code structure, the phase ordering problem, in
which multiple optimizing transformations can be ordered based on their eﬀect on enabling or
disabling other transformations, is greatly alleviated with Inductive SSA for many traditional
low-level and loop-level optimizations. The experimental results show that the eﬀectiveness
of loop-variant variable analysis, congruence detection, and loop optimizations can be
signiﬁcantly increased with Inductive SSA.
For this dissertation research I also developed a math function library generator. Many
computational tasks require repeated evaluation of functions over structured grids, such as
plotting in a coordinate system, rendering of parametric objects in 2D and 3D, numerical
grid generation, and signal processing. The library generator speeds up closed-form function
evaluations over grids by vectorizing the CR forms of these functions.
The approach is
comparable to aggressive strength reduction and short-vector vectorization of the resulting
CRs. Strength reduction of ﬂoating point operations is usually prevented in a compiler due
to safety issues related to ﬂoating point roundoﬀ. Auto-tuning of the library ensures safe
ﬂoating point evaluation. Experiments show signiﬁcantly improved execution of streaming
SIMD extensions (SSE) and instruction-level parallelism (ILP) of math function evaluations.

THE FLORIDA STATE UNIVERSITY
COLLEGE OF ARTS AND SCIENCES
A UNIFIED COMPILER FRAMEWORK FOR PROGRAM ANALYSIS,
OPTIMIZATION, AND AUTOMATIC VECTORIZATION WITH CHAINS
OF RECURRENCES
By
YIXIN SHOU
A Dissertation submitted to the
Department of Computer Science
in partial fulﬁllment of the
requirements for the degree of
Ph.D.
Degree Awarded:
Spring Semester, 2009

The members of the Committee approve the Dissertation of Yixin Shou defended on
April 8, 2009.
Robert A. van Engelen
Professor Directing Dissertation
Gordon Erlebacher
Outside Committee Member
Kyle Gallivan
Committee Member
David Whalley
Committee Member
Xin Yuan
Committee Member
Approved:
David Whalley, Chair
Department of Computer Science
Joseph Travis, Dean, College of Arts and Sciences
The Oﬃce of Graduate Studies has veriﬁed and approved the above named committee members.
ii

The members of the Committee approve the Dissertation of Yixin Shou defended on
April 8, 2009.
Robert A. van Engelen
Professor Directing Dissertation
Gordon Erlebacher
Outside Committee Member
Kyle Gallivan
Committee Member
David Whalley
Committee Member
Xin Yuan
Committee Member
The Oﬃce of Graduate Studies has veriﬁed and approved the above named committee members.
ii

To my father
iii

ACKNOWLEDGEMENTS
This dissertation would not have been possible without the support of many individuals.
First of all, I would like to thank my advisor, Dr. Robert A. van Engelen, for his extensive
support, patience and encouragement throughout my doctoral work. His insightful comments
and guidance at every stage of this dissertation were essential for me to complete my research
and has taught me innumerable lessons and insights on the working of research in general.
I am also grateful to him for his great help with scientiﬁc writing and his careful reading,
commenting and revising of the manuscript.
I am also very grateful to my committee members, Dr. Gordon Erlebacher, Dr. Kyle
Gallivan, Dr. David Whalley and Dr. Xin Yuan, for their guidance and support through my
graduate studies and for their insightful comments on my dissertation research.
I would also like to thank all of the group members of ACIS laboratory at FSU for all
the support and help I have received. Especially I would like to thank Johnnie Birch for
many valuable and interesting discussions and his collaboration and help in integrating CR
library into the experimental compiler.
Last but not least, I would like to thank my family members who have been a constant
source of love, support and strength. I would like to thank my parents with my deepest
gratitude for their unconditional love and support all these years.
I am grateful to my
husband Yunsong for his understanding and love during these past years. His understanding
and encouragement made it possible to complete this study.
iv

TABLE OF CONTENTS
List of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
vii
List of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
viii
Abstract
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
xii
1. INTRODUCTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1 Motivation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2 The Inductive SSA Concept . . . . . . . . . . . . . . . . . . . . . . . . .
12
1.3 Automatic SIMD Vectorization of Chains of Recurrences . . . . . . . . .
15
1.4 Thesis Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
1.5 Dissertation Outline
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
2. PRELIMINARIES AND RELATED WORK
. . . . . . . . . . . . . . . . . .
18
2.1 Static Single Assignment Form
. . . . . . . . . . . . . . . . . . . . . . .
18
2.2 Induction Variable Analysis . . . . . . . . . . . . . . . . . . . . . . . . .
21
2.3 Classic Compiler Optimizations . . . . . . . . . . . . . . . . . . . . . . .
30
2.4 The Chains of Recurrences Formalism
. . . . . . . . . . . . . . . . . . .
42
3. THEORY . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
3.1 The CR# Chains of Recurrences Extension
. . . . . . . . . . . . . . . .
50
3.2 Inductive SSA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
4. ALGORITHMS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
80
4.1 Inductive SSA Construction . . . . . . . . . . . . . . . . . . . . . . . . .
80
4.2 Compiler Analysis and Optimization with Inductive SSA . . . . . . . . .
87
5. IMPLEMENTATION
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
101
5.1 GNU Compiler Collection (GCC) . . . . . . . . . . . . . . . . . . . . . .
101
5.2 Implementation of Inductive SSA Extension . . . . . . . . . . . . . . . .
103
5.3 Loop-variant Variable Analysis Implementation . . . . . . . . . . . . . .
105
5.4 Enhanced GVN-PRE with Inductive SSA-based Optimizations . . . . . .
105
6. EXPERIMENTAL RESULTS . . . . . . . . . . . . . . . . . . . . . . . . . . .
106
6.1 SPEC CPU2000 Benchmarks
. . . . . . . . . . . . . . . . . . . . . . . .
106
6.2 Measuring the Inductive Proof Choice Set Sizes . . . . . . . . . . . . . .
106
6.3 Results of Loop-Variant Variable Classiﬁcation with Inductive SSA
. . .
108
v

6.4 Results for Inductive SSA-based Optimizations
. . . . . . . . . . . . . .
112
6.5 Inductive SSA Impact Analysis on GVN-PRE . . . . . . . . . . . . . . .
116
7. SIMD Vectorization of Chains of Recurrences . . . . . . . . . . . . . . . . . .
123
7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
123
7.2 Vector Chains of Recurrences . . . . . . . . . . . . . . . . . . . . . . . .
125
7.3 Results
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
136
7.4 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
149
8. CONCLUSION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
152
REFERENCES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
154
BIOGRAPHICAL SKETCH
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
163
vi

LIST OF TABLES
2.1
Chains of Recurrences simpliﬁcation rules. . . . . . . . . . . . . . . . . . . .
48
2.2
Chains of Recurrences inverse simpliﬁcation rules. . . . . . . . . . . . . . . .
49
3.1
CR# simpliﬁcation rules. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
3.2
CR# inverse simpliﬁcation rules.
. . . . . . . . . . . . . . . . . . . . . . . .
53
6.1
The SPEC CPU2000 benchmark suite. . . . . . . . . . . . . . . . . . . . . .
107
6.2
Measuring the sizes of choice sets in Inductive SSA on SPEC2000. . . . . . .
108
6.3
Results of loop-variant variable classiﬁcation with Inductive SSA on SPEC2000.109
6.4
Inductive SSA classiﬁcation of conditional loop-variant variables in SPEC2000.110
6.5
Number of additional equivalences and loop-invariants detected by Inductive
SSA enhancement of GVN-PRE for SPEC CPU2000 benchmarks using GCC
4.1 -O3. Note: −indicates no diﬀerence. . . . . . . . . . . . . . . . . . . . .
113
6.6
Number of additional equivalences, loop-invariants, and constants detected
by GVN-PRE enhanced with Inductive SSA for SPEC CPU2000 using GCC
4.1 -O2 -fno-gcse -fno-tree-ccp -fno-tree-loop-im -fno-move-loop-invariants -
fno-ivopts. Note: −indicates no diﬀerence. . . . . . . . . . . . . . . . . . . .
119
7.1
Interpolated maximum relative errors.
. . . . . . . . . . . . . . . . . . . . .
132
7.2
Benchmark functions used in performance testing. . . . . . . . . . . . . . . .
137
7.3
Overview of VCR optimizations applied to benchmarks. . . . . . . . . . . . .
137
7.4
Maximum relative error for n = 100.
. . . . . . . . . . . . . . . . . . . . . .
138
7.5
Static code size statistics of three Poly3 benchmark codes.
. . . . . . . . . .
148
vii

LIST OF FIGURES
1.1
Example loop (a) and its Inductive SSA form (b). . . . . . . . . . . . . . . .
3
1.2
Example wrap-around variable j1 in a loop shown in SSA form.
. . . . . . .
4
1.3
Example showing that partial redundancy elimination and induction vari-
able removal ordering is relevant. Traditionally, GVN and/or PRE is per-
formed ﬁrst followed by IV removal. However, performing IV removal before
GVN/PRE can lead to better results in certain cases. . . . . . . . . . . . . .
7
1.4
SSA example showing weakness of traditional IV removal to detect j2 = i2. .
8
1.5
SSA example showing weakness of GVN and PRE to detect k1 = j2. . . . . .
8
1.6
Quicksort partition algorithm with redundant operations. Because i−n ≡j−k,
the source code (a) has a redundant loop counter i and useless test i<n.
Fully annotated Inductive SSA form with inductive proofs for the example
program (a) shown in (b). The semantic equivalence a1 ≡b1 is detected by
the Inductive SSA proofs. Thus, block B6 executes useless code. . . . . . . .
9
1.7
Array recovery of pointer arithmetic on arrays in a loop.
. . . . . . . . . . .
10
2.1
Example high-level program fragment and its SSA form.
. . . . . . . . . . .
19
2.2
Example cyclic variables in a loop.
The ﬁrst example performs a swap
to generate sequences with period 2.
The second example generates the
Fibonacci sequence via the common cyclic variable update. . . . . . . . . . .
22
2.3
Example conditionally-updated variables in a loop with branch-speciﬁc up-
dates. The ﬁrst example has a closed form for the value sequence of k. The
second example does not. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
2.4
An example loop with direct and indirect IVs. . . . . . . . . . . . . . . . . .
25
2.5
An example loop with a ﬁrst-order wrap-around variable w and indirect wrap-
around variable x of second order. . . . . . . . . . . . . . . . . . . . . . . . .
26
2.6
Example IVs in a loop in SSA form and their corresponding closed-form
characteristic functions.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
viii

2.7
Overview and summary of IV recognition methods found in the literature.
.
31
2.8
IV removal of IV j from an example loop. Because IV j is identical to IV i it
can be removed and its uses replaced by i. . . . . . . . . . . . . . . . . . . .
31
2.9
IV substitution of IV k by a function of the loop variable i. . . . . . . . . . .
32
2.10 Loop strength reduction of linear expression 3 × i by a linear IV j. . . . . . .
33
2.11 Constant propagation of a = 1 in an example loop.
. . . . . . . . . . . . . .
35
2.12 Loop-invariant code motion of variable k and its operation to the start of the
loop. Note that the assignment is conditional on the non-emptyness of the loop. 37
2.13 Common-subexpression elimination of two identical expressions a + b in an
example loop. Note that the third expression is not identical because of the
intervening deﬁnition of a. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
2.14 Partial redundancy elimination of b + c on a program trace. A temporary t is
introduced to ﬁx the absence of the redundancy on the second trace. . . . . .
39
2.15 Hybrid GVN-PRE application to an example loop. Because the value of b1+a1
is congruent with b1 + c1 based on c1 = a1, the assignment to y1 is redundant.
42
3.1
Example code fragment (a) demonstrating integer overﬂow (b) of IV a.
. . .
67
3.2
Loop transformation example 1. . . . . . . . . . . . . . . . . . . . . . . . . .
69
3.3
Loop transformation example 2. . . . . . . . . . . . . . . . . . . . . . . . . .
70
3.4
Lifting choice sets in Π terms. . . . . . . . . . . . . . . . . . . . . . . . . . .
72
3.5
Example 1: loop fragment and corresponding Inductive SSA form. . . . . . .
75
3.6
Example 2: loop fragment and corresponding Inductive SSA form. . . . . . .
76
3.7
Example 3: loop fragment and corresponding Inductive SSA form. . . . . . .
76
3.8
Example 4: loop fragment and corresponding Inductive SSA form. . . . . . .
77
3.9
Example 5: loop fragment and corresponding Inductive SSA form. . . . . . .
78
3.10 Example 6: loop fragments and corresponding Inductive SSA forms. . . . . .
79
ix

4.1
Example inductive proofs for loop φ-nodes in SSA form. The proofs for (a)
and (b) show the equivalence of b under associativity induced by operation
reordering between (a) and (b).
Example (c) has a linear equivalence
b −a ≡c −d detected by proofs b = {a, +, 1}3 and c = {d, +, 1}3.
Example (d) has a nonlinear equivalence b −a ≡c −d detected by proofs
b = {a, ∗, 2}4 ≡{a, +, a, ∗, 2}4 and c = {d, +, a, ∗, 2}4.
Example (e)
has a wrap-around induction variable b that indirectly induces wrap-around
behavior in variable c.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
4.2
Example path-dependent inductive proofs for loop φ-nodes in SSA form.
Example (a) has congruent operations under commutativity in blocks B2
and B3. Example (b) shows that h is constant by path correlation in proof
construction and #-wrap-around simpliﬁcation of the CR form.
Example
(c) has a loop invariant computation for i, because of the congruence i ≡
c −d ≡a −b and a −b is loop invariant. This is proven by i ≡c −d ≡
[{a, +, 1}9→12→10−{b, +, 1}9→12→10, {a}9→12→11−{b}9→12→11] ≡[a−b, a−b] ≡
[a −b].
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
4.3
Result of IV removal (b) from code fragment (a) with Inductive SSA. . . . .
90
4.4
Result of IVS (b) on example code fragment (a) with Inductive SSA . . . . .
91
4.5
Result of SR (d) on example code fragment (a) in Inductive SSA form (b)
using the CR# form coeﬃcients (c) as IVs in (d). . . . . . . . . . . . . . . .
93
4.6
Eﬀect of SR on ILP (c) for example code fragment (a) in Inductive SSA (b).
94
4.7
Inductive SSA applied to an example taken from [1, p.368].
The SCCP
algorithm [2] marks the branch to B4 dead, because t1 ≤0 holds, and thus
determines that f3 = 6. Marking B4 dead reﬁnes f3 7→[6] in Inductive SSA.
95
4.8
Result of CP (b) on an example code fragment (a) with Inductive SSA. . . .
96
4.9
Result of LICM (b) on an example code fragment with Inductive SSA.
. . .
98
4.10 Result of PRE transformation (b) on an example code fragment (a) with
Inductive SSA.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
100
5.1
GCC intermediate representation integration schematic. . . . . . . . . . . . .
102
6.1
Coverage of loop-variant variable classiﬁcation in SPEC CINT2000. . . . . .
111
6.2
Coverage of loop-variant variable classiﬁcation in SPEC CFP2000. . . . . . .
112
6.3
Relative speedup of code produced by GVN-PRE enhanced with Inductive
SSA for SPEC2000 benchmarks using GCC 4.1 -O3. . . . . . . . . . . . . . .
114
6.4
Relative compile time and static code size of Inductive SSA with GVN-PRE
for SPEC2000 benchmarks using GCC 4.1 -O3.
. . . . . . . . . . . . . . . .
115
x

6.5
Relative speedup of Inductive SSA with GVN-PRE for SPEC2000 benchmarks
using GCC 4.1 -O2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
117
6.6
Relative compile time and static code size of Inductive SSA with GVN-PRE
for SPEC2000 benchmarks using GCC 4.1 -O2.
. . . . . . . . . . . . . . . .
118
6.7
Relative speedup of code optimized by GVN-PRE with Inductive SSA for
SPEC2000 using GCC 4.1 -O2 -fno-gcse -fno-tree-ccp -fno-tree-loop-im -fno-
move-loop-invariants -fno-ivopts. . . . . . . . . . . . . . . . . . . . . . . . . .
121
6.8
Relative compile time and static code size of code optimized by GVN-PRE
with Inductive SSA for SPEC2000 benchmarks using GCC 4.1 -O2 -fno-gcse
-fno-tree-ccp -fno-tree-loop-im -fno-move-loop-invariants -fno-ivopts. . . . . .
122
7.1
Blocked VCR loop template. . . . . . . . . . . . . . . . . . . . . . . . . . . .
131
7.2
Scalar CR loop and loop transformed with SSE Shuﬄe-based CR vectorization.133
7.3
Scalar CR code of Sine benchmark. . . . . . . . . . . . . . . . . . . . . . . .
134
7.4
VCR vectorized Sine sequences for decoupling factor d = 4. . . . . . . . . . .
134
7.5
SSE VCR code for Sine benchmark for decoupling factor d = 4 (unblocked).
135
7.6
VCR vectorized Sine sequences for decoupling factor d = 8. . . . . . . . . . .
136
7.7
Performance comparison of VCR for n = 1, 000 and n = 100, 000 (Intel Dual
Core Xeon 5160 3GHz).
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
140
7.8
Sine single and double precision performance comparison (Intel Dual Core
Xeon 5160 3GHz).
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
141
7.9
Code optimized with sCR4.1 for the Bin15 benchmark. . . . . . . . . . . . .
143
7.10 Code optimized with sCR4.4 for the Poly3 benchmark. . . . . . . . . . . . .
144
7.11 Performance comparison of shuﬄe-based CR vectorization for the polynomial
benchmarks with n = 1, 000 and for Poly3 with n = 10, . . . , 106 (Intel Dual
Core Xeon 5160 3GHz).
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
145
7.12 Performance comparison of VCR-enhanced ILP (UltraSPARC IIIi 1.2GHz). .
147
7.13 Poly3 benchmark performance results of VCR-enhanced ILP-optimized code
(UltraSPARC IIIi 1.2GHz).
. . . . . . . . . . . . . . . . . . . . . . . . . . .
149
xi

ABSTRACT
In this dissertation I propose a uniﬁed compiler framework for program analysis, opti-
mization, and automatic vectorization with techniques based on the Chains of Recurrences
(CR) algebra. The root theoretical foundations of the CR algebra and the CR# additions
for program analysis, that I speciﬁcally developed for this dissertation research, are given.
Based on the theory, I propose an extension of the Single Static Assignment (SSA) form,
called the Inductive SSA form. The form associates inductive proofs with SSA variables to
describe their inductive properties. The proofs are derived with algorithms based on the
CR# theory. Inductive SSA provides an internal program representation that facilitates
more eﬀective loop analysis by a compiler. Based on the Inductive SSA form, I describe and
evaluate a collection of compiler algorithms for program optimization at the SSA level.
More speciﬁcally, Inductive SSA merges static SSA information with semantic recurrence
information to describe the value progressions of loop-variant variables. I will show how
Inductive SSA enables and/or enhances loop-variant variable analysis, improves dependence
testing for loops, and how it can be eﬀectively used by many classic compiler optimizations
to improve compilation results.
Inductive SSA is based on my theoretical work on extending the CR formalism by
the CR# (CR-sharp) algebra and lattice to eﬀectively represent the value sequences and
value ranges of conditionally updated variables in loops. Using empirical results on SPEC
benchmarks with GCC 4.1, I will show that the Inductive SSA form captures more recurrence
relations compared to state-of-the-art induction variable recognition techniques. Because
the CR# term rewriting system is complete (conﬂuent and terminating), it can be used for
congruence detection in programs at all optimization levels. The Inductive SSA program
analysis is far less sensitive to syntactical code changes, due to the real semantic information
xii

it captures. As a result of the independence of code structure, the phase ordering problem, in
which multiple optimizing transformations can be ordered based on their eﬀect on enabling or
disabling other transformations, is greatly alleviated with Inductive SSA for many traditional
low-level and loop-level optimizations. The experimental results show that the eﬀectiveness
of loop-variant variable analysis, congruence detection, and loop optimizations can be
signiﬁcantly increased with Inductive SSA.
For this dissertation research I also developed a math function library generator. Many
computational tasks require repeated evaluation of functions over structured grids, such as
plotting in a coordinate system, rendering of parametric objects in 2D and 3D, numerical
grid generation, and signal processing. The library generator speeds up closed-form function
evaluations over grids by vectorizing the CR forms of these functions.
The approach is
comparable to aggressive strength reduction and short-vector vectorization of the resulting
CRs. Strength reduction of ﬂoating point operations is usually prevented in a compiler due
to safety issues related to ﬂoating point roundoﬀ. Auto-tuning of the library ensures safe
ﬂoating point evaluation. Experiments show signiﬁcantly improved execution of streaming
SIMD extensions (SSE) and instruction-level parallelism (ILP) of math function evaluations.
xiii

CHAPTER 1
INTRODUCTION
1.1
Motivation
Compiler techniques to analyze and optimize programs for modern processor architectures
are critical, because modern architectural features such as multi-core and multi-media exten-
sions can only be eﬃciently exploited with eﬀective compiler support. An optimizing compiler
relies heavily on program analysis techniques, such as induction variable analysis and data
dependence analysis, to optimize and/or parallelize programs eﬀectively. Optimizations can
increase the execution speed of the program and/or reduce the code size [3, 4, 5, 1, 6, 7, 8].
1.1.1
Loop-Variant Variable Analysis
Automatic parallelizing compilers generally focus on the optimization of sets of loops, because
most of the execution time of a program is typically spent in loops [9, 8, 10]. Induction
Variables (IVs) [11, 4, 12, 1, 13, 8] are an important class of loop-variant variables whose value
progressions form linear, polynomial, or geometric sequences. Nonlinear IVs are also referred
to as Generalized Induction Variables (GIVs) [14, 15]. Recognition of IVs and GIVs1 plays
a critical role in optimizing compilers as a prerequisite to loop analysis and transformation.
For example, a loop-level optimizing compiler applies array dependence testing [8] in loop
optimization, which requires an accurate analysis of memory access patterns of IV-indexed
arrays and arrays accessed with pointer arithmetic [16, 17]. Other example applications
are array bounds check elimination [18], loop-level cache reuse analysis [19], software
prefetching [9], loop blocking, variable privatization, IV elimination [11, 4, 12, 7], and auto-
parallelization and vectorization [8].
1In this dissertation we consider all classes of IVs, so the use of IV refers to all IVs including GIVs.
1

The challenge in this research is to identify and accurately classify all loop-variant
variables in an loop structure that are ﬂow sensitive, which means that the updates of
these variables depends on the control ﬂow through the branches in the loop. While the
recognition of “traditional” forms of IVs is extensively described in the literature, there is
a limited body of work on methods to analyze more complicated ﬂow-sensitive loop-variant
variables that have arbitrary conditional update patterns along multiple paths in a loop nest.
The relative occurrence frequency in modern codes of ﬂow-sensitive loop-variant variables
that exhibit more complicated update patterns compared to IVs is signiﬁcant.
In this
dissertation research we found that 9.32% of the total number of variables that occur in
loops in CINT2000 are conditionally updated and 2.82% of the total number of variables in
loops in CFP2000 are conditionally updated. Most of these variables have no known closed-
form function equivalent that describes the value sequence as a function of the iteration
point, because the updates are dependent on run-time conditions that cannot be inferred at
compile time. Current IV recognition methods fail to classify conditionally updated variables,
whether they have closed forms or not. The result is a pessimistic compiler analysis outcome
and lower performance expectations.
There are many types of IVs and loop-variant variables. A linear IV such as a loop counter
is a simple example of a loop-variant variable. All linear IVs and polynomial, geometric, and
periodic GIVs are loop-variant variables that generate value sequences that can be described
by closed-form characteristic functions [12]. But there are many more loop-variant variables
that are not IVs, such as conditionally updated variables in loops with value sequences that
are determined by the control ﬂow in the loop. When the control ﬂow conditions cannot
be determined at compile time, the control ﬂow becomes unpredictable. The ﬂow-sensitive
loop-variant variables updated in the unpredictable branches may exhibit many diﬀerent
behaviors. In some cases the IV value sequences are predictable while the control ﬂow is not,
for example when the variable updates in the branches are semantically identical and yield
the same value progression of the variable regardless of the branches taken.
Consider the loop in Figure 1.1(a).
Loop-variant variable g is conditionally updated
in branches of the if statement. Since it is not an IV, compilers will simply give up on
IV recognition for this ﬂow-sensitive variable even when the update of variable g in both
branches produces the same linear value sequence with initial value 0 and increment 2
in each iteration. Thus, the the updates to g are syntactically diﬀerent in two branches
2

g = 0;
h = 1;
j = 0;
k = 0;
for (i = 1; i < n; i++)
{
if (. . . )
g = i ∗2;
else
g = g + 2;
k = k + 1;
x = i << 1;
y = i + k;
z = k ∗2;
h = h ∗2;
if (. . . )
j = j + 3;
else if (. . . )
j = 2 ∗j + 1;
else
j = 2 ∗j;
. . .
}
loop L1:
i1 = φ(1, i2)
7→{1, +, 1}L1
j1 = φ(1, j5)
7→{1, +, 1, ∗, 1}L1, {1, +, 3, ∗, 2}L1
g1 = φ(0, g4)
7→{0, +, 2}L1
k1 = φ(0, k2)
7→{0, +, 1}L1
h1 = φ(1, h2)
7→{1, ∗, 2}L1
if (. . . )
g2 = i1 ∗2
7→{2, +, 2}L1
else
g3 = g1 + 2
7→{2, +, 2}L1
g4 = φ(g2, g3)
7→{2, +, 2}L1
k2 = k1 + 1
7→{1, +, 1}L1
x1 = i1 << 1
7→{2, +, 2}L1
y1 = i1 + k2
7→{2, +, 2}L1
z1 = k2 ∗2
7→{2, +, 2}L1
h2 = h1 ∗2
7→{2, ∗, 2}L1
if (. . . )
j2 = j1 + 3
7→{4, +, 1, ∗, 1}L1, {4, +, 3, ∗, 2}L1
else if (. . . )
j3 = 2 ∗j1 + 1
7→{3, +, 2, ∗, 1}L1, {3, +, 6, ∗, 2}L1
else
j4 = 2 ∗j1
7→{2, +, 2, ∗, 1}L1, {2, +, 6, ∗, 2}L1
j5 = φ(j2, j3, j4)
7→{2, +, 1, ∗, 1}L1, {4, +, 6, ∗, 2}L1
. . .
i2 = i1 + 1
7→{2, +, 1}L1
endloop L1
(a)
(b)
Figure 1.1: Example loop (a) and its Inductive SSA form (b).
while semantically equivalent.
Therefore, g is a linear IV and has closed form function
g = f(I) = 2I at the loop body entry, where I = 0, . . . is the iteration number.
The Static Single Assignment (SSA) form [1] has become a standard intermediate
representation in many modern compilers. The SSA form of the example loop in Figure 1.1(a)
is shown in Figure 1.1(b). Note that the right-hand side expression of the assignments to
SSA variables g2, g3, g4, x1, y1 and z1 in the Figure 1.1(b) are syntactically diﬀerent in SSA
form. However, it is easily observed by tracing the changes of the variables through the loop
that these variables all have exactly the same value sequences in the loop, with initial value
2 at the start of the SSA assignment statement and increment of 2 for the next assignment.
From the ﬁgure it is clear that these variables have the same semantic value progression in
the loop. Thus, they are semantically equivalent. The problem is that many such cases are
3

loop
i1 = φ(0, i2)
j1 = φ(99, i1)
a[j1 + 1] = . . .
. . .
i2 = i1 + 1
endloop
i1
=
{0, 1, 2, 3, 4, . . .}
j1
=
{99, 0, 1, 2, 3, . . .}
Figure 1.2: Example wrap-around variable j1 in a loop shown in SSA form.
not detected by compilers that only apply syntactic matching and can only determine IVs
that are syntactically equivalent.
Certain ﬂow-sensitive loop-variant variables do not have closed form function equivalents,
such as j in Figure 1.1(a). We say that these variables have discordant updates in branches
of the loop body. Most compilers will simply give up on loop analysis and optimization
due to an unknown value progression of these kind of variables. Closer inspection of the
real benchmarks, such as SPEC2000, reveals that value progressions of all of these ﬂow-
sensitive loop-variant variables can be bounded with tight bounding functions over the
iteration space. Typically a pair of dynamic lower- and upper-bound functions over the
loop iteration space suﬃces. Note that static bounds are not as useful as dynamic bounds,
where the latter are functions over the loop iteration spaces. Bounding the value progressions
of ﬂow-sensitive loop-variant variables has the advantage of increased analysis coverage in
a compiler. Bounding also signiﬁcantly helps loop analysis by resolving accuracy problems
in the presence of unknowns, e.g. in dependence analysis where bounds on array index
expressions can help ﬁltering more dependences [20, 21].
With the availability of tight
functional (iteration-speciﬁc) bounds on variables, analysis and optimization can continue.
Other special classes of loop-variant variables are variables with Out-of-Sequence Values
(OSV), which include classic wrap-around variables shown in Figure 1.2. Even though the
relative percentage of these types of variables in benchmarks is low (0.55% in CINT2000 and
to 0.62% in CFP2000), their classiﬁcation is important to enable loop restructuring [12]. A
wrap-around variable is ﬂow-sensitive: it is assigned a value outside the loop for the ﬁrst
iteration and then takes the value sequence of another IV for the remainder of the iterations.
Wrap-around variables may cascade: any IV that depends on the value of a wrap-around
variable is a wrap-around variable, possibly of a one order higher [4] giving two iterations
4

with out-of-sequence values. Compilers typically utilize a mechanism of pattern matching [4]
to recognize wrap around variables, which is often ad-hoc, very speciﬁc to certain patterns,
and may cause many loop-variant variables to remain unrecognized as IVs or ﬂow-sensitive
loop-variant variables.
The problem of classic and state-of-the-art compiler IV analysis methods is that loop-
variant variables are analyzed by using speciﬁc pattern matching schemes, ad-hoc rules, or
using structural expression matching. This problem will be further reviewed in Chapter 2.
To improve the loop-variant variable analysis and related optimizations, the real semantic
value progressions of these variables should be captured and as accurately as possible.
1.1.2
Congruence Detection
Proving that two diﬀerent variables have the same value at a program point is generally
undecidable, even without interpreting the conditionals [22]. The equivalence relation with
this restriction is called Herbrand equivalence. In practice, global Common-Subexpression
Elimination (CSE) [11], Global Value Numbering (GVN), Partial Redundancy Elimination
(PRE), and hybrid GVN-PRE methods eﬃciently detect and remove many congruences in
the lexical and semantic domains, i.e. a safe subset of Herbrand equivalences.
GVN [23, 24, 25] partitions variables and expressions into classes by assigning them a
unique “value number” for identiﬁcation. Variables and expressions in the same equivalence
class are guaranteed to have the same value, i.e. are congruent. If a value is already computed,
subsequent congruent operations can be removed from an execution trace.
PRE [26, 27] is more generic than CSE, because it identiﬁes operations that are redundant
on traces rather than at a program point. PRE ﬁnds redundant operations on most, but not
necessarily all, traces of the program that include the same operation. It then removes these
partially redundant operations by hoisting them to an earlier program point. PRE methods
also classify variables and expressions as constant, loop-invariant, and (loop-variant) variable.
Additional optimizations can then be performed accordingly, such as Loop-Invariant Code
Motion (LICM) to hoist loop-invariant variables and operations out of loops to evaluate
them only once instead of every iteration.
The challenge in this research is to ﬁnd redundant congruences that occur at the loop
level. GVN, PRE (and therefore also CSE by implication), and hybrid GVN-PRE methods
cannot remove many redundancies that only occur at the loop level. To remove redundant
5

operations on IVs at the loop level, IV removal (also known as IV elimination [11]) is used.
However, IV removal only speciﬁcally targets redundant IVs that produce identical or linear-
mappable value sequences in loops. It is typically applied after GVN and PRE. IV removal
classiﬁes loop-variant variables into linear and nonlinear IV categories and considers the
removal of all linear IVs except one. Thus, the detection of the initial and stride values of
variables in a loop is suﬃcient to describe all linear IVs and reduce them to just one by using
linear function replacements.
The interaction between GVN, PRE, and IV removal in a compiler is illustrated in
Figure 1.3.
The example is based on [1, p.415] and slightly extended to illustrate the
speciﬁcs of the interaction. Although the traditional ordering of GVN and PRE followed by
IV removal is very eﬀective in many cases, this is not always true as can be seen in the ﬁgure.
The example has a redundant IV j and a loop-invariant variable c. The optimization path
via GVN/PRE followed by IV removal is typically implemented, but leads in this case to
a sub-optimal result. Reversing the optimization order by applying IV removal ﬁrst before
PRE with reassociation (or, alternatively, re-applying PRE after IV removal) leads to the
desired result. Note that the loop invariant property of c can only be detected after removal
of the IV j, based on the linear equality j = i −m established by IV removal.
The fundamentally incomparable properties of IV removal and GVN/PRE is illustrated
in Figures 1.4 and 1.5 using two detailed examples in SSA form. In SSA form each variable
has a single deﬁnition. A φ-function is inserted at join points, where two or more control
ﬂow edges merge, such as in the loop header block B2.
In Figure 1.4 variable i2 is a basic IV with the value sequence i2 = 0, 1, 2, . . .. Traditional
IV removal methods that rely on the detection of IV update patterns in the code to classify
IVs are too weak to detect the linear sequence j2 = 0, 1, 2, . . .. Because there is a use of j2
before the set j3 = i3, this pattern does not correspond to a linear IV. By contrast, GVN/PRE
methods simply detect that j2 = i2, since we have that j2 = φ(j1, j3) = φ(0, i3) = i2. Thus,
GVN/PRE removes j2 as redundant.
In Figure 1.5 variable i2 is a basic IV and j2 = 0, 4, 8, 12, . . . is a linear IV. The value of
k1 = 4 ∗i2 is clearly congruent with j2. This congruence is not detected by GVN/PRE. By
contrast, it is detected by classic IV removal methods [11] that classify the linear basic IVs
and dependent IVs by their initial value and stride.
It is well-known by the compiler research community that diﬀerent optimization orders
6

j = 0
do i = m, n
a = b + i
c = a - j
d = b + i
j = j + 1
enddo
PRE
↗
j = 0
do i = m, n
a = b + i
c = a - j
d = a
j = j + 1
enddo
IV rem.
−→
do i = m, n
t = i - m
a = b + i
c = a - t
d = a
enddo
IV rem.
↘
do i = m, n
t = i - m
a = b + i
c = a - t
d = b + i
enddo
PRE
−→
c = b + m
do i = m, n
a = b + i
d = a
enddo
Figure 1.3: Example showing that partial redundancy elimination and induction variable
removal ordering is relevant. Traditionally, GVN and/or PRE is performed ﬁrst followed by
IV removal. However, performing IV removal before GVN/PRE can lead to better results
in certain cases.
and/or repeated optimizations can lead to better code, a phenomenon referred to as the
phase ordering problem.
Several authors proposed approaches to deal with the phase
ordering problem [28]. For example using search techniques to ﬁnd eﬀective optimization
sequences [29, 30]. However, exhaustive search-based approaches are expensive, even with
search space pruning. Furthermore, the resulting optimization sequence is broadly applied
to an entire program or at least to a function body. There is no guarantee of optimality
for a limited code region such as a single loop. Therefore, synergistic approaches, such as
hybrid GVN-PRE [31] and our Inductive SSA (introduced in more detail in Section 1.2) for
GVN/PRE with IV integration, show promising advantages.
Whereas the previous examples in Figures 1.3, 1.4, and 1.5 can be optimized by
considering combinations and repetitions of GVN/PRE and IV removal, there are many
cases in practice where no combination will succeed to eliminate the redundant operations.
Theoretically speaking, this is to be expected from the undecidable properties of the
Herbrand equivalence problem.
However, many equivalences of variables are ignored in
practice due to limitations of loop-variant variable classiﬁcation in GVN/PRE and IV
7

B1: i1 ←0
j1 ←0
B2: i2 ←φ(i1, i3)
j2 ←φ(j1, j3)
k1 ←j2 + 1
. . .
i3 ←i2 + 1
j3 ←i3
goto B2
Figure 1.4: SSA example showing weakness of traditional IV removal to detect j2 = i2.
B1: i1 ←0
j1 ←0
B2: i2 ←φ(i1, i3)
j2 ←φ(j1, j3)
k1 ←4 ∗i2
. . .
i3 ←i2 + 1
j3 ←j2 + 4
goto B2
Figure 1.5: SSA example showing weakness of GVN and PRE to detect k1 = j2.
methods. Although GIV recognition techniques [4, 12] detect linear and nonlinear IVs, even
by following the IV updates through all possible paths [32, 33], none of these IV algorithms
can eﬃciently detect and exploit correlations between IVs updated in branches.
In addition, PRE methods do not handle cyclic (i.e. loop-header) φ-nodes in the SSA
form accurately [34]. Consider for example the classic quicksort partition problem shown
in Figure 1.6(a). The quicksort partition algorithm uses two indices j and k that move in
opposite directions from the ends of an array to swap array elements into the two partitions
based on a pivot value.
Because the index variables are updated exclusively, it takes n
iterations to meet j = k, with n the size of the array. For sake of exposition, we introduced
a loop counter i so that the loop invariant i −n ≡j −k holds. Hence, the while-condition
i < n is congruent with j < k.
8

i = 0;
k = j+n;
do {
if (a[j] <= p) {
j++;
} else {
k−−;
SWAP(a[j], a[k]);
}
i++;
} while (j<k && i<n);
B1: i1 ←0
k1 ←j1 + n1
B2: i2 ←φ(i1, i3)
7→[{0, +, 1}2→∗]
j2 ←φ(j1, j4)
7→[{j1, +, 1}2→5→3, {j1}2→5→4]
k2 ←φ(k1, k4)
7→[{j1+n1}2→5→3, {j1+n1, +, −1}2→5→4]
↙
↘
B3: j3 ←j2 + 1
7→[{j1+1, +, 1}2→5→3,
{j1+1}2→5→4]
B4: k3 ←k2 −1
7→[{j1+n1+1}2→5→3,
{j1+n1−1, +, −1}2→5→4]
↘
↙
B5: j4 ←φ(j2, j3)
7→[{j1+1, +, 1}2→5→3, {j1}2→5→4]
k4 ←φ(k2, k3)
7→[{j1+n1}2→5→3,
{j1+n1−1, +, −1}2→5→4]
i3 ←i2 + 1
7→[{1, +, 1}2→∗]
b1 ←j4 −k4
7→[{1−n1, +, 1}2→∗]
if b1<0 goto B2
B6: a1 ←i3 −n1
7→[{1−n1, +, 1}2→∗]
if a1<0 goto B2
(a)
(b)
Figure 1.6: Quicksort partition algorithm with redundant operations. Because i−n ≡j−k,
the source code (a) has a redundant loop counter i and useless test i<n. Fully annotated
Inductive SSA form with inductive proofs for the example program (a) shown in (b). The
semantic equivalence a1 ≡b1 is detected by the Inductive SSA proofs.
Thus, block B6
executes useless code.
The detection of the congruence i < n ≡j < k that is implied by the correlation of
two distinct variables describing non-deterministic value sequences, requires powerful static
execution trace analysis and ﬂow-sensitive recurrence solvers.
Though they occur more
frequently in practice e.g. in logically-controlled loops, to the best of our knowledge no
state-of-the-art congruence detection algorithm can detect these type of congruences.
1.1.3
Array and Pointer Data Dependence Testing
Current dependence analyzers and frameworks for symbolic analysis in restructuring compil-
ers are quite powerful, such as traditional simple test [9] and GCD test [9, 8]. Other state-of-
the-art dependence methods working on aﬃne array index expressions are by Banerjee [3],
9

int i, ∗p = A, ∗q = B+n;
for (i = 0; i < n; i++) {
. . .
∗p++ = ∗--q;
. . .
}
int i;
for (i = 0; i < n; i++) {
. . .
A[i] = B[n-i-1];
. . .
}
(a)
(b)
Figure 1.7: Array recovery of pointer arithmetic on arrays in a loop.
Wolfe [8], Goﬀ, Kennedy and Tseng [5], and the I-test by Psarris et al. [35, 36]. A linear
programming method based on Fourier-Motzkin variable elimination for exact dependence
testing was introduced by Dantzig [37], and later improved with an integer programming
method by Pugh [38] for his Omega test. However, recent work by Psarris [39, 40], Franke
and O’Boyle [41], Wu et al. [42], and earlier work by Shen, Li, and Yew [43], Haghighat [12],
and Collard et al. [44] mention the diﬃculty dependence analyzers have with nonlinear
symbolic expressions, pointer arithmetic, and conditional control ﬂow in loop nests.
The presence of pointer arithmetic in a loop nest introduces aliasing problems hampering
or disabling data dependence analysis [45]. Array recovery [41] can be an eﬀective method
to convert pointer accesses to array accesses in a loop nest. Figure 1.7(a) shows a loop with
pointer accesses that are recovered as arrays in Figure 1.7(b). The resulting array accesses
can then be analyzed using conventional data dependence testing methods that operate on
the closed-form index expresssions of array accesses.
The analysis of pointer updates in a loop nest can be viewed as a special form of IV
recognition. Pointer Access Descriptions (PADs) [17] are canonical CR forms that capture
the pointer-based access functions deﬁned over the iteration space of the loop nest. When the
CR forms of pointers have closed forms, the closed-form conversion produces array accesses
similar to array recovery. Because this dissertation focuses on a wider set of loop-variant
variables and pointer arithmetic in loops can be viewed as a form of IV manipulation,
the Inductive SSA form is also directly applicable to methods for array recovery and data
dependence analysis on pointers.
The challenge is to derive bounds on array index expressions and pointer accesses when
these expressions contain loop-variant variables that are conditionally updated. Once the
10

bounds are found, they can be provided to a dependence test, such as the CR-based
dependence test described and evaluated in [20]. This nonlinear dependence test is based on
the Banerjee bounds test [3], also known as the Extreme Value Test (EVT). The test computes
direction vector hierarchy information by performing symbolic subscript-by-subscript testing
for multidimensional loops. The test builds the direction vector hierarchy by solving a set of
dependence equations. Our nonlinear EVT can determine absence of dependence for a larger
set of dependence problems compared to standard EVT, by including common nonlinear
forms. The standard EVT and most other tests require aﬃne closed forms, while our EVT
test can handle conditional induction variable updates, pointer arithmetic, and polynomial
IVs. Array dependence testing is essentially a mechanism that requires the detection of
cross-iteration congruences of the array accesses.
The root idea of CR-based dependence testing is to replace a dependence equation with
a CR form. Consider the general form of a dependence equation for an n-dimensional loop
nest:
a1x1 + a2x2 + a3x3 + . . . + anxn + b = 0
,
over loop indices xi, where coeﬃcients ai are determined from the array index expression
pairs to be tested. The EVT test computes a lower and upper bound for the left side of the
dependence equation and then checks whether zero lies in the interval. If the interval does
not include zero, no dependence exists.
In our CR-enhanced EVT [20] we replace the terms in the dependence equation with the
CR forms Φ for each index expression:
Φ1 + Φ2 + Φ3 + . . . + Φn + b = 0
,
and then apply the CR bounds analysis to determine whether zero is included in the solution
interval. The Φ terms can be nonlinear instead of just aﬃne, and the CR simpliﬁcation can
lead to many more cancellations, which helps to prove that zero is not included in the
equation’s range. In this way, we can apply dependence testing directly on the CR forms of
aﬃne and nonlinear array index expressions, for more details see [20, 21].
To apply dependence testing on array expressions containing ﬂow-sensitive loop-variant
variables and on pointers for array recovery, a more powerful form of analysis is needed
that takes the variable updates on all branches in a loop nest into account. The result of
Inductive SSA analysis is a set of CR forms, each describing the behavior of the variable in
11

a branch of the loop. In the past some straightforward approaches have been used to trace
all updates in all execution paths in a loop nest, as in [21], but this leads to an exponential
complexity. A less costly algorithm is needed to classify ﬂow-sensitive loop-variant variables
as a prerequisite to array dependence testing. This is where the dissertation work on ﬂow-
sensitive loop-variant analysis can help to improve dependence testing results.
1.2
The Inductive SSA Concept
In this dissertation we present the underlying theory and algorithms to construct Inductive
SSA, an approach to improve loop-variant variable analysis and optimizations related to
congruence detection by composing inductive proof structures over SSA variables in linear
time (linear in the size of the SSA graph).
Many classic compiler analysis and optimizations are enhanced by the use of SSA form,
such as IV recognition, Constant Propagation (CP), LICM, CSE, GVN, and PRE. However,
the SSA form and popular extensions such as Gated SSA (GSA) [46, 6] only provide structural
information related to use-def chains of scalar variables (and predicated forms in GSA). By
contrast in Inductive SSA, each SSA variable in a loop, or embedded within a set of loops,
is bound to a list of canonical forms that describe the variable’s updates across all execution
traces in the enveloping loops. It is then proven that the congruent variables have equivalent
values under structural isomorphism, i.e. when they can be mapped to the same canonical
structures.
Our canonical representation for inductive proofs in the Inductive SSA form is based on
the CR form [47]. We chose the CR form because of its algebraic properties that appear
to be particularly amenable to linear and nonlinear IV methods [48, 49]. In [50, 49], IV
recognition and manipulation was improved based on CR forms due to the fact that our CR
approach is able to capture semantic value progressions of the induction variables.
In [50] Van Engelen proved that the CR algebra implemented as a Term Rewriting System
(TRS) is complete (both conﬂuent and terminating). Therefore, any rule application order
will result in a unique CR normal form.
Because CR forms are normal forms for loop-
variant variables, the value progression revealed with the CR form represent the real semantic
behavior of the variables. For this dissertation research we extended the CR algebra to the
CR# algebra (CR-sharp algebra), which represents the ﬂow-sensitive loop-variant variables
and their value ranges more eﬀectively. It is proven in Chapter 3 that the CR# algebra
12

implemented as a TRS is also complete.
Based on our experiments reported in this dissertation, we found that the use of the
CR# algebra in Inductive SSA captures more recurrence relations compared to the CR
algebra and compared to existing state-of-the-art induction variable recognition techniques.
By associating CR# normal forms with each operation in the SSA graph, matching of
semantically equivalent variables and expressions is made simple and more eﬀective.
More speciﬁcally, an inductive proof structure for an Inductive SSA variable is a three-
component canonical description of the variable’s value sequence in a loop:
• Execution trace information. Some inductive proof structures are only valid along a
speciﬁc execution path in the loop.
• The base value of the SSA variable, i.e. the start value of the variable in the loop’s
iteration space.
• A compact representation of an update operation, consisting of an operator and an
update value, so that the next value of the variable in the loop can be derived. If the
update value is variable, then the update itself is recursively derived by an inductive
proof.
We augment this representation with decision path information as follows:
{initial value, ⊙, update value}path
where the value coeﬃcients are expressions over SSA variables or nested proofs (CR# forms),
⊙is an arithmetic operator, and the path subscript spans a sequence of blocks from the loop
header to the back edge and then back up through the decision φ-nodes up to the loop
header. An execution path may contain ∗to denote any path segment. The proof structures
are constructed by a two-stage traversal over the SSA graph.
Consider the example shown in Figures 1.6(a). The fully annotated Inductive SSA form is
shown in Figure 1.6(b). To illustrate the inductive proof construction process and the power
derived from the extended SSA form, we analyze the proof of the equivalence a1 = b1. The
inductive proof for i2 is constructed from the loop header φ-node in block B2 by observing
that
i2 = φ(i1, i3) = φ(0, i2 + 1)
13

This recurrence equation is the basis for the inductive proof structure for i2, which in
canonical form is the CR# form {0, +, 1}2→∗. The form describes a simple basic IV with
initial value 0 and unit stride with respect to the execution path starting at B2. Traditional
IV methods solve the recurrence relation to derive the characteristic function, if it exist [4, 12].
Instead, we use CR#-based forms to eliminate the expense and limitations of a recurrence
solver.
Variables j2 and k2 are more interesting, since they are updated in branches and do not
conform to any regular IV pattern. The proof structure j2 7→[{j1, +, 1}2→5→3, {j1}2→5→4] is
constructed from two instantiations, the ﬁrst for the branch in which the variable is updated:
j2 = φ(j1, j4) = φ(j1, φ(j2, j3))
via B3
⇒
φ(j1, j3) = φ(j1, j2+1)
giving j2 7→{j1, +, 1}2→5→3, and the second branch in which it is not updated:
j2 = φ(j1, j4) = φ(j1, φ(j2, j3))
via B4
⇒
φ(j1, j2)
giving j2 7→{j1}2→5→4. The proof for k2 is similar. After this ﬁrst stage (proof construction),
all loop header φ-nodes have inductive proofs for their value sequences. In the second stage
(proof propagation), all other variables are updated in the SSA graph.
Now, observe that by correlating the paths and applying the CR# rewrite rules based
on CR [49, 47] to simplify the propagated proof structures, we obtain the canonical forms
for a1 and b1 as follows:
a1
=
i3 −n1
7→
[{1, +, 1}2→∗−n1]
7→
[{1−n1, +, 1}2→∗]
b1
=
j4 −k4
7→
[{j1+1, +, 1}2→5→3, {j1}2→5→4]
−[{j1+n1}2→5→3, {j1+n1−1, +, −1}2→5→4]
7→
[{j1+1, +, 1}2→5→3 −{j1+n1}2→5→3,
{j1}2→5→4 −{j1+n1−1, +, −1}2→5→4]
7→
[{1−n1, +, 1}2→5→3, {1−n1, +, 1}2→5→4]
7→
[{1−n1, +, 1}2→∗]
14

Hence, a1 and b1 are congruent. This example demonstrated the key aspects of our approach.
Execution path-dependent canonical proof structures are automatically constructed. Con-
gruent variables have equivalent canonical proofs under structural isomorphism.
Similarly, the loop in Figure 1.1(a) was converted to Inductive SSA form in Figure 1.1(b).
For SSA variables of variable g in Figure 1.1(a), the same CR form for variable g2, g3, and
g4 reveals that variable g is updated identically in two branches. Therefore, matching of
CR forms will ﬁnd out variable g is a linear IV, which most IV recognition methods will
fail to recognize. Furthermore, although the right hand side expression of the variables g2,
g3, g4, x1, y1, and z1 in Figure 1.1(b) are syntactically diﬀerent, the same CR form of these
variables reveals that they have the same semantic value progression behavior in the loop.
Therefore, they are semantically equivalent, which will not be detected by classic expression-
based compiler optimizations that only recognize syntactical equivalence. For example, the
state of the art PRE algorithm will fail to recognize the semantic redundancies in Figure 1.1.
Based on our earlier research and experiments with dependence testing with CR
forms [20, 21] and IV classiﬁcation [51, 52, 53], we present convincing arguments and
further experiments in this dissertation that Inductive SSA provide a solid basis for a
uniﬁed framework to generalize ﬂow-sensitive loop-variant variable analysis, to improve array
dependence testing and many classic optimizations in the context of loops at the SSA level,
such as CP, LICM, IV removal, CSE, GVN, and PRE.
To make Inductive SSA form applicable to general-purpose compilers, the performance
of the algorithm that extends the SSA form to Inductive SSA is very important. To limit
the overhead of Inductive SSA construction, a fast algorithm that is linear time in the size
of the SSA region of a loop nest for the Inductive SSA construction was developed and will
be presented in this dissertation.
1.3
Automatic SIMD Vectorization of Chains of
Recurrences
This dissertation also presents a method and toolset to speed up closed-form function evalu-
ations over grids by vectorizing the CR forms constructed for common math functions [54].
Since CR formalism provides an eﬃcient means of representing functions over unit-distant
grids, CR-based techniques are eﬀective to decrease operation counts and reduce instruction
latencies of repeated function evaluations over regular grids. The approach is comparable to
15

aggressive strength reduction and short-vector vectorization of the resulting scalar updates.
However, strength reduction of ﬂoating point operations is usually prevented in a compiler
due to safety issues related to ﬂoating point roundoﬀ. In addition, strength reduced loops
are not trivially vectorizable at the loop level. I present a high-level algebraic approach to
restructure functions into vector CR forms for eﬃcient execution over a set of discrete grid
points.
The high-level algebraic rewrite of the function to Vector CR (VCR) form consists of a
decoupling method to translate math functions. The VCR coeﬃcients are packed in short
vector registers for eﬃcient execution.
This approach eﬀectively reduces the instruction
counts for short-vector SIMD execution of functions over grids. As a result, our method
outperforms separately optimized scalar CR forms and SIMD vectorized closed-form function
evaluation.
Our results show that the VCR code is much faster than the code vector code generated
by the Intel compiler and the short vector math library (SVML). VCR can also signiﬁcantly
increase the Instruction Level Parallelism (ILP) exposed in modulo scheduling [55], resulting
in faster kernels for RISC and VLIW processors.
In general, CR optimization as a form of aggressive strength reduction can introduce
ﬂoating point roundoﬀerrors that are propagated across the iteration space [56].
Error
analysis is required to ensure roundoﬀerrors introduced in CR-based evaluation methods
are bounded. To address error propagation, we combined our VCR method with an auto-
tuning approach. The VCR code is run to determine error properties and select an optimal
vector length for performance. This approach ensures optimal performance with high ﬂoating
point accuracy.
1.4
Thesis Statement
The thesis statement for my dissertation research hypothesis is:
The Inductive SSA form, constructed with algorithms based on the CR#
algebra, improves ﬂow-sensitive loop-variant variable analysis and increases the
eﬀectiveness of classic compiler optimizations that rely on (induction) expression
matching and manipulation.
16

1.5
Dissertation Outline
The remainder of this dissertation is organized as follows.
Chapter 2 introduces the
preliminaries of the static single assignment form, followed by a review of current induction
variable analysis methods and several important compiler optimizations that related to loop-
variant variable (expressions). Then, the basic CR formalism is introduced. In Chapter 3, the
theory which include the CR# algebra and Inductive SSA are introduced. A new operator
# together with new algebraic simpliﬁcation rules on CR forms containing the new operator
are given. The CR# alignment and bounds mechanism which include the new lemmas and
corresponding proof are given. The notation of Inductive SSA are introduced. In Chapter 4,
the linear time Inductive SSA construction algorithms are presented. A uniﬁed compiler
analysis and optimization framework based on Inductive SSA is presented. The algorithms
to generalize loop-variant variable recognition and classiﬁcation based on attached CR form
in Inductive SSA are given and several improved classic optimizations related to loop-variant
variable (expressions) with Inductive SSA are presented. In Chapter 5, the implementation
of the Inductive SSA and corresponding optimization algorithms in GCC 4.1 are discussed.
In Chapter 6, the experimental results are discussed. Chapter 7 presents the automatic
SIMD vectorization of Chains of Recurrences to speed up closed-form function evaluations
over grids.
In this chapter, VCR notation and the decoupling method that translate
math functions into VCR forms are introduced, followed by the auto-tuning approach and
performance results. Finally, Chapter 8 summarizes the dissertation.
17

CHAPTER 2
PRELIMINARIES AND RELATED WORK
This chapter presents preliminaries of the static single assignment form, followed by a survey
of induction variable analysis methods and a review of classic compiler optimizations. Finally,
the Chains of Recurrences formalism is introduced.
2.1
Static Single Assignment Form
The Static Single Assignment (SSA) form [1, 57] is an Intermediate Representation (IR) of
program code used by a compiler, where each use of a variable has a single deﬁnition point.
Each deﬁnition in a program in SSA form has a unique name by attaching indices to the
variable to create a new version of that variable at each deﬁnition point. Thus, a program in
SSA form has explicit use-def information, which is very helpful to compiler optimizations
that need to gather accurate data ﬂow information from the program. When multiple control
ﬂows meet before a use, SSA inserts a special join function, called a φ-function, at the point
where the control ﬂow paths merge.
For example, Figure 2.1(a) shows an example high-level program fragment and its
corresponding SSA form in Figure 2.1(b).
A unique new subscript index for a variable
is introduced in each assignment statement. When this variable is used in a later statement,
it is replaced with an indexed copy of that variable that identiﬁes the single deﬁnition by
its name. Also notice that there are two assignments to variable c that could reach the
use of variable c in the last statement. One is outside of the if-statement and the other is
inside the if-statement. To handle the merging of deﬁnitions, SSA inserts a φ-function at
the point where the control ﬂow paths that carry the variable deﬁnitions meet. In this case,
a new SSA variable c3 is created for c by means of a φ-function at the merge point. Also
loop-carried deﬁnitions yield the placement of φ functions to merge the initial deﬁnition of
18

a = 2
b = 3
c = 4
if . . . then
b = b * a
d = b + a
c = b + d
endif
b = b + 1
c = c + 1
. . .
i = 0
for (. . . )
i = i + 1
. . .
endfor
a1 = 2
b1 = 3
c1 = 4
if . . . then
b2 = b1 ∗a1
d1 = b2 + a1
c2 = b2 + d2
endif
b3 = φ(b1, b2)
c3 = φ(c1, c2)
b4 = b3 + 1
c4 = c3 + 1
. . .
loop:
i1 = φ(0, i2)
. . .
i2 = i1 + 1
endloop
(a)
(b)
Figure 2.1: Example high-level program fragment and its SSA form.
a variable with the last deﬁnition in the loop body. For example, the loop header φ i1 in
Figure 2.1(b) merges initial value of variable i and the last deﬁnition in the loop, which are
0 and i2 separately.
2.1.1
SSA Construction
To convert a program into SSA form, φ-functions are inserted and the variables are renamed
(subscripted).
An eﬃcient algorithm for SSA construction is introduced in [57].
This
algorithm determines the location of the φ-functions for each variable in the program. This
algorithm for the construction of SSA form is based on the notion of dominance frontier.
• Dominance relation – a node A strictly dominates a diﬀerent node B in the control
ﬂow graph if node A is on every path from program entry to node B. A dominates B
if either A strictly dominates B or A = B.
19

• Dominance frontier – the dominance frontier of a node A is the set of nodes Z such
that A does not strictly dominate, but does dominate some immediate predecessor of
Z.
A dominance frontier of a region of code gives the precise places to insert φ-functions. Using
the dominance frontiers, the location of the φ-functions for each variable in the original
program are ﬁrst determined and then the variables are renamed by replacing each variable
with a subscripted version.
2.1.2
SSA Extensions
There are several extensions to SSA mentioned in the literature, each proposes to improve
the applicability of SSA form for a particular code analysis goal.
Gated SSA (GSA) [46] introduces control ﬂow information in the φ nodes of the classic
SSA form.
Specialized gating functions are introduced in GSA to include the predicate
of conditional branches [6]. This helps to evaluate and analyze symbolic expressions more
eﬃciently. Our Inductive SSA form is compatible with GSA and augments SSA form further
with more precisely value progression information, which can beneﬁt the precision of the
GSA form by eliminating true and false conditions.
The Region Array SSA form [58] was proposed to accurately represent the use-def
relations between array regions.
Several new nodes are introduced to represent control
dependence and data ﬂow relation of arrays more accurately. A symbolic representation of
array regions was used to represent uniformly memory location sets in a compact way for
both static and dynamic analysis techniques.
The work by Chow et al. [59] extends the SSA form to cover indirect pointer references.
They introduced two new assignment functions χ and µ to model the MayDefs and
MayUses of defs and uses that are possibly eﬀected by pointer aliases.
Their extended
SSA representation is referred to as HSSA. HSSA enables indirect memory operations to be
globally analyzed which in turn beneﬁts optimizations developed for scalar variables. This
work on SSA continued with the work by Lin et al. [60]. They presented a general compiler
analysis framework based on a speculative SSA form to incorporate the runtime alias proﬁle
into SSA form to facilitate data speculation.
A Concurrent SSA (CSSA) form was introduced by Lee et al. [61]. It extends classical
CSE, redundant load/store elimination and loop invariant detection to parallel programs
20

without violating sequential consistency. Two additional conﬂuence functions π and ψ are
introduced in CSSA form to model the new conﬂuence points due to parallel regions in the
program.
2.2
Induction Variable Analysis
Induction variable updates form recurrences in loops. These recurrences must be analyzed
and solved to obtain the characteristic function of the IV to determine its value sequence.
The value sequence is critical when the IV is used in array indexes to determine the array
access patterns in dependence testing for loop optimization. IV values are also useful to
detect expression congruences for CSE, GVN, and PRE.
2.2.1
IV Classiﬁcation
Induction variable analysis is crucial for compilers that perform loop optimizations. Compiler
optimizations on loops depend heavily on the accuracy of IV analysis to detect loop properties
and variable sequences. Most loop optimizations require IV recognition to obtain closed-
form characteristic functions of array index expressions to facilitate dependence testing and
to eliminate the cross-iteration recurrences of IVs to enable loop parallelization.
A subclass of IVs are variables whose successive values form an arithmetic progression
over some part of a program, usually a loop. In general, IVs are divided into several types
for identiﬁcation:
• Fundamental or Basic IVs (BIV) – are explicitly modiﬁed by constant amount during
each iteration. Thus BIV is a variable whose only assignments within a loop are of the
form x = x ± c, where c is either a constant or a loop invariant value.
• Linear IVs – whose value is a linear function of some BIVs or other linear IVs. Both
BIVs and linear IVs are the most common IV form recognized by compilers and have
been widely discussed in the literature, e.g. [11, 1, 8, 7].
• Polynomial IVs – are variables whose value is a polynomial function of linear IV.
Polynomial sequences occur when variable update is a linear IV or several linear IVs
added together.
21

x = 1
y = 2
do
. . .
t = x
x = y
y = t
. . .
while (. . . )
a = 1
b = 1
do
c = a + b
a = b
b = c
. . .
end do
(a)
(b)
Figure 2.2: Example cyclic variables in a loop.
The ﬁrst example performs a swap to
generate sequences with period 2. The second example generates the Fibonacci sequence via
the common cyclic variable update.
• Geometric IVs – are variables whose value is a geometric function of linear IV.
Geometric sequences occur when the variable update is the variable itself multiplied by
some loop invariant. Polynomial and geometric IVs are referred to as Generalized IVs
(GIVs) in [14, 62] and its recognition and classiﬁcation is discussed in [63, 4, 15, 12, 13].
They are variables whose update value is not constant. They are commonly generated
by triangular and trapezoidal loops. The most common GIVs found in loops form
either polynomial or geometric sequences.
Finding the closed form of GIVs is not
as easily accomplished as it is with BIVs, but eﬃcient algorithms for such a task do
exist [50, 49].
• Wrap around variables – are variables which take on the value of another IV after a
ﬁxed number n of iterations of the loop [64, 4, 15, 13]. They are called wrap around
variables with order n. Typically n = 1. Wrap around variables with order greater
than one (n > 1) will be referred to as indirect wrap-around variables.
• Cyclic variables – also called periodic variables in [4], cause a periodic value sequences
through the loop iterations. In Figure 2.2(a), variable x, y are simple cyclic variables
with period of 2 because variable x has periodic sequence 1, 2, 1, 2, . . . and variable
y has periodic sequence 2, 1, 2, 1, . . . . Cyclic variables may also follow more complex
22

sequences patterns as well when arithmetic operations occur in the variable assignments
[65, 50]. For example, in Figure 2.2(b), variable c, a, b are non-constant cyclic variables
which produces the Fibonacci sequence.
Note that IVs can be mutually dependent and have a cyclic use-def relationship, even
though they are not cyclic variables. A cyclic ﬂow dependence in the loop can be
eliminated with forward substitution. For example, variable x and y in the following
code have cyclic ﬂow dependence.
do
x = y + a
y = x + b
enddo
After substitution:
do
x = y + a
y = y + a + b
enddo
Thus, x and y are linear IVs, where x is a copy of y oﬀset by a constant.
Using the algorithms presented in [50, 49], y is recognized as a linear IV with initial
value y0 and step a+b. Variable x is identiﬁed as a indirect linear IV with initial value
y0 + a and step a + b.
• Conditionally updated loop-variant variables – are incremented or decremented in condi-
tional statements [4, 15, 42]. Most optimizing compilers do not recognize this category
of IVs (they are not IVs in the true sense). In some cases, conditionally assigned IVs
may have semantically identical updates in both branches of the conditional statement.
For example, in Figure 2.3(a), both branches in the loop update k by the same amount,
if a compiler could recognize this using static and semantic information, the loop can
be optimized. However, in Figure 2.3(b), variable k is assigned diﬀerent values in the
branches of the if-statement in the loop. Thus, in this example k does not have a closed
form function.
23

k = 0
do j = 1, n
a[k] = . . .
if (. . . ) then
k = k + 1
else
k = j
endif
. . .
end do
k = 0
do
if . . .
k = k + 1
else
k = k + 2
. . .
end do
(a)
(b)
Figure 2.3: Example conditionally-updated variables in a loop with branch-speciﬁc updates.
The ﬁrst example has a closed form for the value sequence of k. The second example does
not.
In addition to these categories of IVs, we can also further classify IVs using the following
characteristics:
• Direct or independent IVs – are deﬁned as IVs whose update is not derived from any
other IV. The update pattern may be linear or non-linear.
• Indirect or derived or dependent IVs – are deﬁned as IVs who’s update is derived from
at least one other IV. The value of a derived IV depends on the values of a basic IV.
Derived/dependent IVs are IVs whose value is a linear function of some basic IV or
the variables which is updated with a polynomial sequence of values.
Figure 2.4 shows examples of loop IVs to illustrate some simple classiﬁcations of direct and
indirect IVs. Based on these classiﬁcations, the example loop has a basic (independent) IV
i which is a scalar integer variable with one unconditional update. The value of variable j
is updated with a linear function of variable i and it is a derived linear IV. Because k is
updated with polynomial functions and l is updated with geometric functions, variable k
and l which are nonlinear IVs. However, k is a indirect IV and l is a direct IV.
In Figure 2.5, variable w is a wrap around variable of the ﬁrst order, because w has value
m on the ﬁrst iteration and then a linear sequence of the IV i = 0, . . .. Variable x is a second
24

i = 0
j = 1
k = 1
do
. . .
i = i + 1
j = 2 * i + 1
k = k + i
l = l * 2 + 1
. . .
while (. . . )
Figure 2.4: An example loop with direct and indirect IVs.
order wrap-around variable, because x takes the value of n, then m, and then i = 0, . . . after
the ﬁrst two iterations.
In practice, IV analysis requires type information of the variable is analyzes. There is
not much literature on this subject. Therefore, we add to the classiﬁcation of IVs:
• Range-constrained IV – In the context of typed IVs, we can not simply assume that
no overﬂow occurs in IV update operations. And thus, the value range of any IV is
constrained by its value range determined by its type. An integer overﬂow occurs when
an integer operation result can not ﬁt in the value range of an integer type. Typically
the high order bit of the result is discarded.
Some processors support saturation
arithmetic and the result saturates instead of overﬂows.
2.2.2
IV Recognition
IV recognition is a classic compiler analysis problem. There are many compiler analysis
methods designed to recognize IVs. Some of them utilize some sort of pattern matching
scheme [63, 1]. However, pattern-matched approaches are often ad-hoc and very speciﬁc
for certain patterns and may cause many IVs unrecognized.
In addition, there also are
algorithms that utilize data ﬂow graph information to detect and classify IVs [66, 67, 4, 7, 8].
We will introduce a few of those techniques in this section and also look at another scheme
25

i = 0
w = m
x = n
do
. . .
. . . = a[w] + b[x]
x = w
w = i
i = i + 1
. . .
while (. . . )
Figure 2.5: An example loop with a ﬁrst-order wrap-around variable w and indirect wrap-
around variable x of second order.
using a symbolic analyzers to ﬁnd IVs [12]. In Figure 2.7, we summarize the characteristics
of IV recognition capability of these methods.
Ammerguallat and Harrison’s Technique
Ammerguallat et al. [63] use abstract interpretation and a pattern matching scheme to
automatically recognize IVs in loops.
Their method has two parts.
First, abstract
interpretation is used to construct a map that associates each stored (updated) variable
with a symbolic expression of its value.
To illustrate the abstract interpretation, they
deﬁne a simple grammar where an abstract store is a mapping from an identiﬁer to an
expression, or subscript expression. In the second part of their method, the elements of the
map explained above are uniﬁed with a recurrence template which describes some common
recurrence relations.
They deﬁne a method for mapping with arrays, conditionals, and
iterations. Although it is not able to detect some GIVs, their technique is easily extensible
and works eﬀectively in ﬁnding BIVs.
Aho’s Algorithm
Aho’s algorithm in [11] for IV recognition is a classic algorithm. The algorithm takes as input
data ﬂow and loop invariant information of the loop, and identiﬁes an IV using a triple that
contains an initial value and the step (stride) of the IV.
26

The algorithm proceeds as follows. Firstly, the algorithm ﬁnds the BIV of the loop using
loop-invariant computation information. Next, the algorithm searches for a single assignment
of the form of k = j op b, where b is a constant and j is an IV which has its triple (i, c, d).
If the op is multiply operation, then k has triple (i, c ∗b, d ∗b), if op is addition operation, j
has triple (i, c, b + d).
IVs are detected by this algorithm in a high-level language representation or in a
compiler’s IR, such as three-address-code in the control ﬂow graph (CFG) [11] or SSA form.
Since an IV is always related to a loop, loop analysis algorithms detect IVs by ﬁrstly analyzing
back edges to detect loops in internal forms. In SSA form, a IV is represented by a cycle in
SSA graph. The intuition behind this is the value of IV which is assigned in current iteration
will be used in next iteration.
Once the families of IVs have been found, loop optimizations, such as strength reduction,
can be performed based on its triple information of each recognized IV. This algorithm is
limited to linear IVs and geometric forms.
Cocke and Kennedy’s Algorithm
Cocke and Kennedy [66] describe an algorithm for ﬁnding IVs and to perform strength
reduction. Although their algorithm can ﬁnd some polynomial IVs other than BIVs, the
algorithm is limited to a rather restricted class of IVs and can not ﬁnd GIVs.
Cocke and Kennedy utilize the CFG form and use loops in CFGs to ﬁnd IVs in the IR.
A CFG, by deﬁnition, is simply a directed graph denoted by a triple (N, V, n0), where N is
a set of basic blocks, V is a set of edges, and n0 is the program’s unique entry node. Cocke
and Kennedy’s algorithm ﬁnds IVs by traversing the operations and operands of a set of
instructions in the loop and eliminating those variables that do not satisfy the deﬁnition of
an IV. Initially the algorithm sets the set of IVs to contain all targets variables of instructions
in a loop. Then noting that IVs must have the form:
x ←−±y
,
and
x ←−±y ± z
,
where y and z represent either region constants or other IVs. Variables are eliminated if
they satisfy one of the two following criteria:
27

1. If x ←op(y, z) and op is not one of the operations store, negate, add, subtract, then x
is not an IV.
2. If x ←op(y, z) and y and z are not both elements of the set of IVs and not both
elements of IV ∪RC where the IV is the set of IVs and RC is the set of region
constants, then x is not an IV.
The algorithm iterates through the code, eliminating variables from IV set until no more
elimination can be made. The remaining set consists of IVs.
Gerlek, Stoltz and Wolfe’s Algorithm
Gerlek, Stoltz and Wolfe [4] present a classical IV recognition approach based on SSA forms
and Factored Use-Def (FUD) chains to detect a broader class of IVs. Their ﬁrst approach
was based on FUD chains, which is similar to the SSA form. The primary observation in
their approach is that each Strongly Connected Component (SCC) in an SSA/FUD digraph
represents the cycle of operations on a variable, where SCC spans a set of assignment and
operation nodes in the graph.
The SSA graph is searched to detect SCCs by a variant of Tarjan’s algorithm [68]. By
detecting SCCs, a compiler can examine the operations within the SCC cycle to determine the
nature of the variable sequences. The method divides the value sequences of IVs into several
disjoint classes based on the number and type of φ functions and arithmetic operations in a
SCC cycle. Once variable sequences are classiﬁed, their corresponding solvers are performed
to solve their sequence expression. They present several solvers to detect basic, polynomial,
wrap-around of order n = 1, and selected periodic IVs.
In Figure 2.6, a linear, quadratic, and cubic polynomial IV is shown with their cor-
responding closed-form characteristic functions. The closed-form polynomial functions are
derived from a polynomial IV solver. Variable h is a basic loop counter whose value is zero
on the ﬁrst iteration of the loop and is incremented by one at the end of each subsequent
iteration.
Haghighat’s Symbolic Diﬀerencing
Haghighat and Polychronopoulos [12] developed a symbolic GIV analysis framework for the
Parafrase-2 parallelizing compiler. They use symbolic diﬀerencing as a framework to attain
28

i0 = 0
j0 = 1
k0 = 1
loop
i1 = φ(i0, i2)
j1 = φ(j0, j2)
k1 = φ(k0, k2)
. . .
h = {0, 1, 2, 3, . . .}
i2 = i1 + 1
i2 = {1, 2, 3, 4, . . .},
h + 1
j2 = j1 + i2
j2 = {2, 4, 7, 11, . . .},
1
2h2 + 3
2h + 2
k2 = k1 + j2 + 1
k2 = {4, 9, 17, 29, . . .},
1
6h3 + h2 + 23
6 h + 4
endloop
Figure 2.6:
Example IVs in a loop in SSA form and their corresponding closed-form
characteristic functions.
larger solution spaces for program analysis problems than traditional methods. Running
the Perfect Club Benchmarks suite through the Parafrase-2 parallelizing compiler, they
demonstrate improvement in the solution space for code restructuring optimizations that
promote parallelism. The basis of their approach is to use abstract interpretation to detect
generalized IVs. They detect GIVs in loops by symbolically diﬀerencing the values of each
variable through symbolically executing loop iterations and using Newton’s interpolation
formula to ﬁnd the closed-form characteristic polynomial function.
Their work is a very powerful method for IV recognition and can handle conditionally
assigned IVs with the restriction that the assignments must be (semantically) identical.
However, no path search algorithm is given to ﬁnd the conditionally-assigned variable
updates. Furthermore, the method can result in incorrectly transformed programs [49].
Van Engelen’s CR-Based Methods
In [49, 32, 33, 21] van Engelen et al. presented a novel method and algorithms to eﬀectively
analyze GIVs including factorials and exponentials, pointer arithmetic and conditional
updates in complex loop nests based on the CR algebra [69, 47]. The approach is more
eﬃcient compared to the other approaches that require abstract interpretation or symbolic
diﬀerencing and can be applied to high-level code or intermediate forms such as SSA. This
29

method can handle multi-dimensional loops and detect generalized IVs that form polynomial
and geometric progressions through loop iterations from the coupled IV update statements.
The CR algebra is used to rewrite the coupled IVs into CR form and then simplify the
resulting CR expressions to determine its functional equivalent. The CR IV analysis method
is an eﬀective means to improve the loop optimization and data dependence testing since it
provides more accurate information of GIVs which is the crucial factor for loop optimization
and data dependence testing. In [70], a nontrivial extension CR# of the CR algebra with
“delayed” recurrences was introduced to simplify the manipulation of recurrences of loops
in the presence of direct and indirect wrap-around variables and makes these algorithms
substantially more powerful, yet simple to implement and eﬃcient.
Berlin’s SSA Algorithm
Based on van Engelen’s earlier work [49], Berlin et al. [48, 71]
developed an algorithm for detecting polynomial IVs using a restricted form of CRs by
analyzing scalar variables in SSA forms of loops. The basic idea of their approach is to
combine the SCC analysis by Gerlek et al. [4] on the SSA graph and the CR formalism
to construct CR forms for IVs.
The SCCs are the basis for constructing CR forms for
the linear and polynomials IVs using an SSA-based algorithm that is similar to the CFG-
based algorithm in van Engelen [49].
They implemented this algorithm in GCC 4.x for
data dependence testing and loop vectorization. However, their implementation has several
restrictions.
In particular, they do not support the wider range of GIVs and variables
that may not have closed forms. Furthermore, they do not support scalar analysis in non-
rectangular loops.
2.3
Classic Compiler Optimizations
Several important compiler optimizations are introduced in this section, including the IV
related optimizations and redundancy elimination optimizations.
2.3.1
IV Removal and IV Substitution
IV removal (also referred to as IV elimination) [11] eliminates useless linear IVs in a loop if
the loop contains two or more linear IVs. IV removal can improve the performance of the
program by removing unnecessary computations. The traditional IV removal methods [11, 1]
only compare linear IVs to eliminate useless IV.
30

Author(s)
Methods
IV Recognition Capability
Ammerguallat et al.
Abstract interpretation and a
pattern matching scheme
BIVs and some GIVs
Aho
Identify linear IV with a triple
(var, step, initial value)
Linear IV
Cocke, Kennedy
Based on loops in the CFG
Linear and polynomial IV
Gerlek, Stoltz, Wolfe
Demand-driven sequence clas-
siﬁcation based on SCC con-
struction from SSA
GIVs using specialized solvers
Haghighat, Polychronopoulos
Symbolic diﬀerencing
GIVs, powerful but not safe
Van Engelen et al.
CR algebra on high-level code
or CFG (also pointers and con-
ditional variables)
GIVs and mixers with a uni-
ﬁed representation
Berlin et al.
Detect SCC on the SSA graph
using a restricted forms of CRs
Linear and polynomial IVs
Figure 2.7: Overview and summary of IV recognition methods found in the literature.
j = 0;
for (i = 0; i < n; i++) {
A[i] = B[j] + 1;
j = j + 1;
. . .
}
for (i = 0; i < n; i++) {
A[i] = B[i] + 1;
. . .
}
j = 0;
(a)
(b)
Figure 2.8: IV removal of IV j from an example loop. Because IV j is identical to IV i it
can be removed and its uses replaced by i.
For example, there are two linear IVs i and j in Figure 2.8(a). Since variable i and j have
the same value sequences, one can be eliminated and the optimized code after performing
IV removal is in Figure 2.8(b).
IV Substitution (IVS) replaces an IV update with its closed-form function over the loop
index variables. IVS is often applied to eliminated the cross-iteration dependencies of IV
updates and to further enable the parallelization of a loop nest. For example, the variable k
in Figure 2.9(a) is a linear IV. Due to the loop-carried scalar data dependence on variable k,
31

k = 1;
for (i = 1; i < n; i++) {
k = k + 10;
A[k] = 0;
. . .
}
k = 1;
for (i = 1; i < n; i++) {
k = 1 + 10 * i;
A[k] = 0;
. . .
}
(a)
(b)
Figure 2.9: IV substitution of IV k by a function of the loop variable i.
the loop can not be parallelized. After IV removal, the k’s update in the loop was replaced
with a closed-form function of loop index variable i in Figure 2.9(b). Now the loop-carried
dependence was removed and the loop can be parallelized.
2.3.2
Strength Reduction
Strength reduction (SR) optimization is a technique that substitutes expensive operations
with equivalent faster ones to evaluate operations [11, 72].
The classic example replace
multiplication operations with addition operations in a loop and this case often appears in
loop nests with arrays indexed by IVs. Since multiplication operations has usually taken
longer than addition operations, this transformation are frequently introduced by compiler
as part of array elements address translation. SR is very important, because it beneﬁts
any loop with this cases arises routinely. Even in a microprocessor where multiplication
and addition take time equally, SR still will beneﬁt the loops in a program since strength
reduction often decreases the total number of operations in a loop and create opportunities
for other loop transformation methods, such as IV elimination, to reduce the number of IVs
used in a loop.
For example, the multiplication operation in Figure 2.10(a) is strength-reduced by
addition operations in Figure 2.10(b).
There are several classic SR methods presented in the literature.
32

for (i = 1; i < n; i++) {
j = 3 × i;
. . .
}
j = 0;
for (i = 1; i < n; i++) {
j = j + 3 ;
. . .
}
(a)
(b)
Figure 2.10: Loop strength reduction of linear expression 3 × i by a linear IV j.
Allen, Cocke and Kennedy’s Algorithm
Allen, Cocke and Kennedy [72] present a classic SR approach focusing on loops formed by
Strongly Connected Regions (SCRs) in a CFG. Their algorithm starts from the most deeply
nested loop and works its way outward. The method identiﬁes candidate operations that can
be reduced. Two sets, the region constant set RC(SCR) and IV set IV (SCR), with respect
to an SCR are computed for this purpose. A work list of instructions that are candidates
for reduction is then initialized based on these two sets. Candidates instructions are of the
following forms:
x ←−i × j, x ←−j × i, x ←−i ± j, x ←−j × i,
where i ∈IV (SCR) and j ∈RC(SCR).
The algorithm then repeatedly removes an instruction from the worklist and creates a
temporary variable to hold the value that it computes. The candidate instruction is then
replaced with a copy operation from the temporary name associated with the expression on
the candidate’s right-hand side. Next, the algorithm inserts instructions to compute the value
of that temporary to follow the use-def chains for each of the operands of the expression. If
the operation of the expression is a multiply, the new instruction with an addition operation
instead of multiplication is inserted in the proper place based on the operation and operands
information provided by the use-deﬁnition chains.
The algorithm is limited to a restricted form of candidates instructions for strength
reduction and some new candidate instructions appeared based on their deﬁnition when the
algorithm proceeds might become unrecognized.
33

Cooper, Simpson, and Vick’s Algorithm
Cooper, Simpson, and Vick [67] developed a new loop SR algorithm which is driven by a
depth-ﬁrst search of the SSA data ﬂow graph. Their algorithm improves on Allen’s algorithm
by applying techniques that take advantage of an SSA representation of the program.
They ﬁrstly identify IVs and region constants by examining each SCC in the SSA graph.
If all of the updates in one SCC have the allowed forms (i.e., IV ± RC, IV −RC, a copy
operation, or a φ-function), the SCC is an IV. Once the algorithm identiﬁes an SCC, it starts
to check the operations in SCC. If the SCC contains a single operation, they try to reduce
it. If it can be reduced, a new SSA IV will be created and this allows further reduction of
operations using this new IV. If the SCC contains more than one operation, they will try
to determine if it is an IV, and perform the appropriate reductions for each member in the
SCC if it is not an IV.
This algorithm operates on the SSA form and produces results quite similar to those
produced by Allen, Cocke and Kennedy, but is easier to implement. However, the candidate
instruction for SR in their algorithm is limited to the following category:
x ←−iv × rc, x ←−rc × iv,x ←−iv ± rc, x ←−rc + iv,
where iv ∈IV (SCC) and rc ∈RC(SCC).
Furthermore, the operations that are strength reduced are limited to the class of linear
functions.
Kennedy and Chow’s Algorithm
Kennedy and Chow [73] describe an algorithm for SR in the context of PRE. Their algorithm
can handle more instructions for SR than the algorithms described above.
Kennedy et
al. [74] developed a new algorithm to perform PRE in SSA form, called SSAPRE. They
made extensions that enable SSAPRE to perform SR in their framework. They regard the
expressions of the form a ± b, −a and a × b as strength reduction candidates. For a × b, the
algorithm requires that the assignment to a must be an incremental update with constants
or loop invariant. The increment to a2 injures the computation a1 × b1 in the following SSA
form.
34

a = 1;
for (i = 0; i < n; i++) {
A[3 + a] = . . . ;
. . .
}
for (i = 0; i < n; i++) {
A[4] = . . . ;
. . .
}
(a)
(b)
Figure 2.11: Constant propagation of a = 1 in an example loop.
a1 × b1
↓
a2 = a1 + 1
↓
a2 × b1
The injured computation can be repaired by incrementally updating the expression tempo-
rary t by a corresponding amount, so that the repaired value can be used at the second
computation a2 × b1 instead of recomputing it. They modiﬁed their PRE framework so
that the phases of PRE can recognize the strength reduction candidates by identifying the
injuring instructions, then injury repairs are generated by generating temporary variables
updated with a corresponding increment amount for strength reduction. Their method does
not require control ﬂow analysis, so strength reduction applies also to straight line code and
non-IVs. The drawback of their method is that they cannot handle mutually deﬁned IVs.
2.3.3
Constant Propagation
Constant propagation (CP) is an optimization that is frequently applied in compilers and
it supports other optimizations by exposing constants in the program. Constants assigned
to a variable can be propagated through the ﬂow graph and substituted at the use of the
variable at compile time. As shown in Figure 2.11, variable a in array index expression in
Figure 2.11(a) was replaced with the constant value it holds in Figure 2.11(b).
CP is a well-known data ﬂow analysis problem and corresponding algorithms for data
ﬂow graphs are discussed in [2, 1]. Wegman et al. [2] summarized several well-known CP
algorithms and developed a classic CP algorithm that exploits the properties of the SSA
form. The algorithm uses a three-level lattice to describe the value of a variable node. The
35

highest level is ⊤, which means that the variable may be some undetermined constant. The
lowest level is ⊥, which means a constant value cannot be guaranteed. The middle element
is any constant. The algorithm start with assigning ⊤to all variable nodes and uses two
worklists: FlowWorkList contains the control ﬂow graph edge with initializing to edges out of
entry node, and SSAWorkList contains SSA edges which initialized to empty set. Then, the
algorithm proceed by taking an edge e from a worklist. During this process, the constant is
propagated when visiting nodes via edges. If the node is an assignment and right-hand-side
of the assignment is evaluated into a constant, the lattice value is set to constant for this
variable node and all the SSA edges coming out the assignment are added into the worklist
so that the use of this variable can be visited. If the node is a conditional statement, the
outgoing edges will be marked as executable or not executable depending on the predicate
value of that statement. If the node is a SSA φ-node, the meet operation is applied to set the
lattice value to this node. All successors of the current SSA edges and CFG edges are added
into the worklist. The algorithm stops if the two worklists are both empty. The algorithm
is eﬃcient in both time and space and with a complexity of O(SSA edges + Flow edges).
Sparse Conditional Constant Propagation (SCCP) [2] enhances CP by taking determinis-
tic branches into account. It is eﬃciently implemented in SSA. The SCCP method is based
on lexical and structural SSA information to utilize the deterministic outcome of conditions
that turn true or false as a result of the constant propagation. When conditions are known,
this may further aﬀect the propagation of constants, e.g. when a branch is always taken the
constants assigned to variables can be propagated [1].
2.3.4
Loop-Invariant Code Motion
Loop-invariant code motion (LICM) (also referred to as loop-invariant removal) [1] moves
computations that are unchanged in the loop out of the loop, thus eliminating redundant
computations in every iteration of the loop. For example, the expression 3 + a in the loop in
Figure 2.12(a) is a loop-invariant expression since variable a is deﬁned outside of the loop and
not assigned in the loop. Therefore, after performing LICM, the expression can be hoisted
out of the loop, as shown in Figure 2.12(b).
The classical LICM algorithms rely on the structural information in the program to
identify loop invariant. An instruction is identiﬁed as loop-invariant if, for each of its operand:
36

for (i = 0; i < n; i ++) {
k = 3 + a;
A[k] = . . . ;
. . .
}
if (n > 0) k = 3 + a;
for (i = 0; i < n; i ++) {
A[k] = . . . ;
. . .
}
(a)
(b)
Figure 2.12: Loop-invariant code motion of variable k and its operation to the start of the
loop. Note that the assignment is conditional on the non-emptyness of the loop.
for (i = 0; i < n; i++) {
A[a + b] = . . . ;
B[a + b] = . . . ;
a = i;
c = a + b;
. . .
}
for (i = 0; i < n; i++) {
k = a + b;
A[k] = . . . ;
B[k] = . . . ;
a = i;
c = a + b;
. . .
}
(a)
(b)
Figure 2.13: Common-subexpression elimination of two identical expressions a + b in an
example loop. Note that the third expression is not identical because of the intervening
deﬁnition of a.
• the operand is constant, or
• the deﬁnition of the operand is located outside the loop, or
• the deﬁnition of the operand is itself loop-invariant
The checking is applied iteratively (or recursively on demand), so new loop-invariant
operations can be detected when operands are detected as loop-invariant.
2.3.5
Common Sub-expression Elimination
The classic Common-Subexpression Elimination (CSE) methods [11, 75, 1] recognize identical
expressions lexically in the program and replace them with a single variable holding the
37

expression. Global CSE eliminates redundancies in a CFG region of code. Local CSE only
eliminates redundancies in a basic block in the CFG. For example, the expression a + b in
Figure 2.13(a) is a global common sub-expression for two statements in the loop. After
performing CSE, the expressions are replaced with a temporary k to avoid the redundant
computation for a + b, as shown in Figure 2.13(b). Note that the third expression is not
identical because of the intervening deﬁnition of a. This also motivates the use of SSA, since
identical variables in SSA have the same name/subscript and redeﬁned variables are simply
identiﬁed by name. Therefore, newer global CSE algorithms are enhanced by SSA.
Muchnick [1] presents both local CSE and global CSE algorithms. If identical expressions
are matched, CSE inserts a new instruction to save the expression’s value to a temporary
variable and modiﬁes the dependent instructions to use the temporary variable instead.
Global CSE requires data ﬂow analysis to compute available expression information, which
means detecting intervening deﬁnitions and tracing control ﬂow paths in a CFG. An
expression e is available at entry to basic block B if the expression is evaluated on all
paths prior to that block in the CFG. Once available expressions is solved, for each available
expression, CSE search backward in CFG to ﬁnd the evaluation of that expression, create
new temporary variable to hold the previous evaluations and replace this expression by
the new temporary variable. Note that the classic CSE only recognize lexically equivalent
expressions as candidate of CSE.
2.3.6
Partial Redundancy Elimination
A partial redundancy is a computation that is done more than once on some path through
a control ﬂow graph. Partial redundancy elimination (PRE) is an important optimization
technique that removes partial redundancies from a program, which requires solving a data
ﬂow problem. Basically, PRE converts partially redundant expressions into fully redundant
expressions. Fully redundancy elimination is easier to handle compared to PRE. PRE is a
powerful optimization method and subsumes global CSE and LICM.
Consider the example in Figure 2.14 (a), expression b + c is computed twice in the left
path. This is called a partial redundancy and we can perform partial redundancy elimination
to get a optimized version which is shown in Figure 2.14 (b), where the partial redundancy
is converted into a full redundancy. Thus, the instruction d = b + c can be replaced with
d = t, where t is a temporary variable introduced to hold the value of redundant expression
38

a = b + c
d = b + c
(a) Original Code
(b) After Partial Redundancy Elimination
t = b + c
a = t
d = t
t = b + c
Figure 2.14: Partial redundancy elimination of b + c on a program trace. A temporary t is
introduced to ﬁx the absence of the redundancy on the second trace.
b + c. After PRE, the computation b + c is only computed once in either path of the control
ﬂow graph.
All practical methods for congruence detection used by PRE methods detect only a subset
of Herbrand equivalences [76], i.e. a safe approximation of all possible run-time equivalences
of variables. Thus, it can be expected that there are many PRE algorithms [27, 26, 77]
that implement equivalence checking. These algorithms mostly address PRE problem by
performing data ﬂow analysis and solving data ﬂow equations to determine where to insert
the temporary variable and the corresponding computation to convert partially redundant
expressions into fully redundant expressions. Muchnick [1] gives an excellent introduction to
PRE.
Morel and Renvoise [27] ﬁrst developed a PRE algorithm based on bidirectional data
ﬂow analysis. They also showed that LICM and CSE are subsumed by PRE. Their approach
was improved by Knoop et al., Drechsler et al. [26, 77] with optimal unidirection data ﬂow
analysis. Kennedy et al. [74] developed a PRE method based on SSA form, called SSAPRE.
39

Ruthing et al. [78] developed a variant of the work in [26] that takes code growth as well as
register pressure into consideration to avoid the problem of size increasing of PRE. Hosking et
al. [79] applied PRE to pointer dereferencing operations, termed access paths”. All of these
classic PRE algorithms only recognize lexically equivalent expressions. This problem for
classic PRE was noted by several authors in [25, 80, 81, 82, 31]. Integration of Global Value
Numbering (GVN) [23, 81, 34] and PRE was proposed in several works [25, 81, 80, 82, 31].
Global Value Numbering is one of several important compiler optimizations for redun-
dancy elimination and is not incomparable with CSE and PRE [1]. GVN assign a value
number to variables or expressions and then determine if a set of variables or expressions are
congruent by checking if those variables or expressions have the same value number. GVN
was ﬁrst developed by Alpern, Rosen, Wegman and Zadek [23, 25]. Hash-based GVN [83, 81]
assign the unique value number to variables or expression by hashing the operator and the
value number of the operands of the expressions. Partition-based GVN [23, 34, 84] starts with
an initial partition and keep reﬁning the existing partitions until a ﬁxed point of partitions
was reached.
Most GVN algorithms are incomplete that can not discover all Herbrand equivalences.
Gulwani et al. [85] presented a polynomial randomized algorithm for GVN. Their algorithm
is a complete GVN algorithm that discover all Herbrand equivalences. However, there is a
probability that the algorithm can report a false equality. Nie and Cheng [86] developed
an fast SSA-based algorithm for complete GVN. Their algorithm use one global graph to
represent all equivalences at diﬀerent program points to solve the low eﬃciency of complete
GVN algorithms caused by the huge data structures and slow abstract evaluations.
Integration of GVN and PRE was ﬁrst proposed in [25]. Rosen et al. [25] and Click [81]
propose to use GVN to improve the redundancy elimination by ﬁnd more equivalent
expressions other than lexically equivalence.
Briggs and Cooper [80] increase the eﬀectiveness of PRE by sorting, reassociating and
renaming the expressions using information obtained from a prior GVN phase to expose
more opportunities for PRE.
Knoop et al. [82] improve PRE by considering values instead of lexical expressions. They
use the Value Flow Graph (VFG), where each node represent the equivalent terms and its
edge represents the data ﬂow, to identify the equivalent expressions for PRE.
Bodik et al. [87, 88] developed an algorithm which also improve PRE by considering
40

values. Their algorithm construct a Value Name Graph (VNG) where the nodes represent
the value expressions at diﬀerent program point and the edges capture the ﬂow of values
through control ﬂow. Then the data ﬂow analysis are used to identify insertion points for
PRE.
VanDrunen and Hosking
[31] developed a state-of-the-art SSA-based formulation of
GVN-PRE which subsumes GVN and PRE to eliminate redundancies that missed by either
work and was adopted in GCC4.x.
These new approaches are able to ﬁnd more expression equivalence than classic PRE.
Other than recognizing the lexically diﬀerent while equivalent expressions brought by trivial
assignments like A →B, such that C + A is equivalent to C + B, they also exploit some
ad-hoc algebraic rules, reassociating, renaming, ranking, sorting of expressions to ﬁnd out
more redundant computation.
GVN-PRE is a state-of-the-art hybrid PRE algorithm based on SSA form which subsumes
both traditional PRE and GVN. Traditional PRE only considers expressions that are lexically
equivalent. GVN only considers the operations that are fully redundant. The GVN-PRE
hybrid algorithm combines the strengths of PRE and GVN and eliminates redundancies
that would be undetected by either working in isolation.
The GVN-PRE algorithm has
three major steps.
Firstly, available value set and anticipatable value set are computed
based on a set of ﬂow equations. The values instead of lexical expressions is considered and
added into the set by walking through each statement in the basic blocks in a top-down
traversal of the dominator tree. Then Insert phase performs insertions to make partially
redundant expressions fully redundant.
In GVN-PRE, a value is partially redundant if it is in available set of some predecessors
of a given block and it is anticipated in all the predecessors of that given block. Thus,
this phase iterates over blocks that have more than one predecessor and inspects all values
anticipated there. If a value is partially redundant, this value is inserted into the predecessor
where it is not available to make it fully redundant. Finally, a elimination phase eliminate
all fully redundant expressions.
Consider the example in Figure 2.15. Classic PRE does not detect the partially redundant
computation for variable x1 and y1. In GVN-PRE, because of the assignment c1 = a1, the
value number of x1 and y1 are identical. The optimized code is shown in Figure 2.15(b),
where the redundant computation was eliminated.
41

if (. . . )
x1 = b1 + a1
c1 = a1
y1 = b1 + c1
. . .
if (. . . )
x1 = b1 + a1
else
t1 = b1 + a1
t2 = φ(x1, t1)
y1 = t2
. . .
(a)
(b)
Figure 2.15: Hybrid GVN-PRE application to an example loop. Because the value of b1 +a1
is congruent with b1 + c1 based on c1 = a1, the assignment to y1 is redundant.
2.4
The Chains of Recurrences Formalism
The Chains of Recurrences (CR) formalism provides an eﬃcient means of representing
functions over unit-distant grids. The CR algebra of the framework was originally developed
by Zima [47] and later extended by Bachmann [69] and Van Engelen [49]. The CR form was
ﬁrst introduced as a means to expedite the evaluation of polynomials, rational functions,
geometric series, factorials, trigonometric functions on regular grids. In our work we use CR
forms to represent the recurrences of IVs and use the CR algebra for simpliﬁcation of IVs and
symbolic expressions. Thus, the CR form notation provides the mathematical basis for our
IV recognition and dependence analysis algorithms. CR-based compiler analysis algorithms
were developed in [49, 32, 33, 21] to eﬀectively analyze nonlinear IV, pointer arithmetic and
conditional updates in more complex loop nests. In this section we review the properties of
CR formalism.
2.4.1
Chains of Recurrences Notation
A closed-form function f evaluated in a loop with loop counter variable i can be rewritten
into a mathematically equivalent system of recurrence relations f0(i), f1(i), . . . , fk(i), where
the value of k depends on the complexity of function f. Evaluating the function represented
by fj(i) for j = 0, 1, . . . , k −1 within a loop with loop counter i, can be written as the
42

recurrence relation:
fj(i) =
 ϕj,
if i = 0
fj(i −1) ⊙j+1 fj+1(i −1),
if i > 0
(2.1)
with ⊙j+1 ∈{+, ∗}, j = 0, 1, . . . , k −1, and coeﬃcients ϕj are loop invariant expression.
Expression fk is loop invariant expression or a similar recurrence system.
A shorthand notation for Equation 2.1 is a Basic Recurrence (BR):
fj(i) = {ϕj, ⊙j+1, fj+1}i
(2.2)
The BR notation allows the system deﬁned by Equation 2.1 to be written as
Φi = {ϕ0, ⊙1, {ϕ1, ⊙2, . . . , {ϕk−1, ⊙k, fk}i}i}i
(2.3)
when ﬂattened to a single tuple:
Φi = {ϕ0, ⊙1, ϕ1, ⊙2, . . . , ϕk−1, ⊙k, fk}i
(2.4)
CR’s are usually denoted by the Greek letter Φ and that is the notation we will use
throughout this paper.
A CR Φ = {ϕ0, ⊙1, ϕ1, ⊙2, . . . , ⊙k, fk}i has the following properties:
• ϕ0, ϕ1, . . . , ϕk are called the coeﬃcients of Φ.
• k is called the length of Φ.
• if ⊙j = + for all j = 1, 2, . . . , k, then Φ is called polynomial.
• if ⊙j = ∗for all j = 1, 2, . . . , k, then Φ is called exponential.
• if ik does not occur in the coeﬃcients, then Φ is called regular.
• if Φ is regular and fk is a constant, then Φ is called simple.
43

Any discrete real- or complex-valued function can be represented by a CR, assuming that
the stride function ϕk = fk(i) is suﬃciently expressive. For constant ϕk, the CR notation
represents polynomials, exponentials, factorials, and trigonometric functions (exponentials
in the complex domain). The value sequences of several example CR forms are illustrated
below:
iteration i =
0
1
2
3
4
5
. . .
{2, +, 1}i value sequence =
2
3
4
5
6
7
. . .
{1, ∗, 2}i value sequence =
1
2
4
8
16
32
. . .
{1, ∗, 1
2}i value sequence =
1
1
2
1
4
1
8
1
16
1
32
. . .
{0, +, 0, +, 1}i value sequence =
0
0
1
3
6
10
. . .
{1, ∗, 2, +, 1}i value sequence =
1
2
6
24
120
720
. . .
{0, +, 1, ∗, 2}i value sequence =
0
1
3
7
15
31
. . .
The CR formalism can be used to represent the value sequences of IVs in a loop, and thus
can be used to eﬀectively recognize and analyze generalized IV. This idea was ﬁrst presented
in [50] by Van Engelen. For example, according to the semantics of CR forms, the CR form
{2, +, 1}i in the above examples accurately represent the value sequence of a linear IV which
has a initial value of 2 and is updated by adding a stride 1 in each iteration.
2.4.2
Chains of Recurrences Semantics
A CR form Φi = {ϕ0, ⊙1, ϕ1, ⊙2, · · · , ⊙k, ϕk}i represents a set of recurrence relations over
a grid i = 0, . . . , n−1 deﬁned by the following loop template which is perhaps the simplest
way to express the meaning of a CR form:
cr0
= ϕ0
cr1
= ϕ1
:
= :
crk−1 = ϕk−1
for i = 0 to n−1
val[i] = cr0
cr0
= cr0
⊙1 cr1
cr1
= cr1
⊙2 cr2
:
= :
:
:
crk−1 = crk−1 ⊙k ϕk
endfor
The loop produces the sequence val[i] of the CR form. This sequence is one-dimensional.
A multidimensional loop nest is constructed for multivariate CR forms (CR forms with
44

nested CR form coeﬃcients), where the indices of the outermost loops are the indices of the
innermost CR forms.
For example, the value sequence of the CR form {1, ∗, 1, +, 1}, where cr0 = cr1 = ϕ2 = 1,
is computed for n = 5 as follows:
iteration i =
0
1
2
3
4
5
val[i] =
1
1
2
6
24
120
cr0 =
1
2
6
24
120
2880
cr1 =
2
3
4
5
6
7
ϕ2 =
1
1
1
1
1
1
During each iteration i, the value at val[i] is set to cr0, cr0 is set to cr0 ∗cr1, and cr1 is set
to cr1 + ϕ2, thereby producing the well-known factorial sequence val[i] = i!.
Multi-variate CRs (MCR) are CRs with coeﬃcients that are CRs in a higher dimen-
sion [69]. Multi-dimensional loops are used to evaluate MCRs over grids.
2.4.3
Chains of Recurrences Term Rewriting System
The CR algebra [89, 49, 90] provides an eﬃcient way of representing and evaluating functions
over uniform intervals and deﬁnes a set of term rewriting rules CR shown in Table 2.1 for the
simpliﬁcation and construction of CR forms for closed-form formulae. These rewrite rules
enable the development of an algebra, such that the process of construction and simpliﬁcation
of CRs for arbitrary closed-form functions can be automated within a symbolic computing
environment. The application of the rewrite rules is straightforward and not computationally
intensive. The required symbolic processing is comparable to classical constant-folding [11].
CR Simpliﬁcation Rules
The application of CR simpliﬁcation rules produces an equiv-
alent CR expression that can be evaluated more eﬃciently using the CR loop template. The
simpliﬁcation rules are initially introduced in [69, 47] and extended by Van Engelen [49].
Table 2.1 shows the CR simpliﬁcation rewrite rules. All the CR simpliﬁcation rules are
applicable to CR’s representing both integer and ﬂoating point typed expressions.
CR Inverse Rules
In Van Engelen [50] a set of CR inverse rules (CR−1) was introduced
to translate CR expressions to their equivalent closed forms. Exhaustive application of the
rules on a CR result in a closed form function not contain any CRs as sub-expressions if the
CR has such a closed form function. This means that the CR inverse system can only derive
45

a subset of closed form functions corresponding to the CR expression since the mapping from
a closed form function to CR is neither injective nor surjective.
Table 2.2 shows the CR inverse rules. Note as was the case with the CR simpliﬁcation
rules, all the CR−1 rules are applicable to CR’s representing both integer and ﬂoating point
typed expressions. The transformation strategy for applying the CR−1 rules is to reduce the
innermost redexes ﬁrst on nested CR tuples of the form Equation 2.3. This results in closed
expressions for the increments in CR tuples which can be matched by other rules. After the
application of the inverse transformation CR−1, the resulting closed form expression can be
factorized to compress the representation.
In [50] Van Engelen proved that the CR algebra as a Term Rewriting System (TRS) is
complete (conﬂuent and terminating). Therefore, any rule application order will result in a
unique CR normal form.
For example, suppose j is a linear IV with CR form {j0, +, 1}i, where j0 is a symbolic
unknown initial value for j and i is a loop index variable. Then expression j2−1 is converted
to CR forms and simpliﬁed as follows using the CR algebra simpliﬁcation rules:
j2 −1
⇒
{j0, +, 1}i ∗{j0, +, 1}i −1
⇒
{j2
0, +, {j0, +, 1}i + {j0, +, 1}i + 1}i −1
⇒
{j2
0 −1, +, 2 ∗j0 + 1, +, 2}i
CR forms are normal forms for loop-variant variables. Any rule application order will result
in a unique CR normal form. For example, the expression (j −1)(j + 1) is converted to CR
forms as follows:
(j −1)(j + 1)
⇒
{j0 −1, +, 1}i ∗{j0 + 1, +, 1}i
⇒
{(j0 −1)(j0 + 1), +, {j0 −1, +, 1}i + {j0 + 1, +, 1}i + 1}
⇒
{j2
0 −1, +, 2 ∗j0 + 1, +, 2}i
As we can see, (j−1)(j+1) and j2−1 have the same CR normal form despite the syntactical
diﬀerence.
2.4.4
Chains of Recurrences Construction for In-Loop Expressions
The CR algebra provides an eﬃcient mechanism to construct CR forms for symbolic
expressions evaluated in multidimensional iteration spaces, i.e. in loops. The translation
46

of a closed-form symbolic expression ei1,...,in deﬁned over a set of index variables i1, . . . , in to
a multivariate nested CR form is deﬁned by:
CR(ei1,...,in)
=
CR(CR(· · · CR(ei1)i2 · · · )in)
CR(eij)
=
e[ij ←Φ(ij)]
where Φ(ij) is the CR representation of the index variable ij. When the index variables
i1, . . . , in span a unit-distance grid with origin (x1, . . . , xn), then Φ(ij) = {xj, +, 1}ij for all
j = 1, . . . , n. The mapping replaces variables ij with their corresponding CR forms using
substitution, denoted by e[ij ←Φ(ij)]. The CR algebra is then applied to normalize the
expression to (nested) CR forms.
Consider for example the nonlinear index expression n ∗j + i + 2 ∗k + 1, where i ≥0
and j ≥0 are index variables that span a two-dimensional iteration space with unit distance
and k is an IV with recurrence k = k + i with initial value k = 0. The recurrence of k in CR
form is Φ(k) = {0, +, 0, +, 1}i. The CR construction of the example expression yields:
CR(CR(CR(n ∗j+i+2 ∗k+1)))
= CR(CR(n ∗j+{0, +, 1}i+2 ∗k+1))
(replacing i)
= CR(n ∗{0, +, 1}j+{0, +, 1}i+2 ∗k+1)
(replacing j)
= n ∗{0, +, 1}j+{0, +, 1}i+2 ∗{0, +, 0, +, 1}i+1 (replacing k)
= {{1, +, n}j, +, 1, +, 2}i
(normalize)
This nested CR form can be evaluated in the nested loop template to eﬃciently produce the
value sequence over the i and j iteration space.
47

Table 2.1: Chains of Recurrences simpliﬁcation rules.
Expression
Rewrite
1.
{ι0, +, 0}i
=⇒
ι0
2.
{ι0, ∗, 1}i
=⇒
ι0
3.
{0, ∗, f1}i
=⇒
0
4.
−{ι0, +, f1}i
=⇒
{−ι0, +, −f1}i
5.
−{ι0, ∗, f1}i
=⇒
{−ι0, ∗, f1}i
6.
{ι0, +, f1}i ± E
=⇒
{ι0 ± E, +, f1}i
7.
{ι0, ∗, f1}i ± E
=⇒
{ι0 ± E, +, ι0 ∗(f1 −1), ∗, f1}i
8.
E ∗{ι0, +, f1}i
=⇒
{E ∗ι0, +, E ∗f1}i
9.
E ∗{ι0, ∗, f1}i
=⇒
{E ∗ι0, ∗, f1}i
10.
E/{ι0, +, f1}i
=⇒
1/{ι0/E, +, f1/E}i
11.
E/{ι0, ∗, f1}i
=⇒
{E/ι0, ∗, 1/f1}i
12.
{ι0, +, f1}i ± {ψ0, +, g1}i
=⇒
{ι0 ± ψ0, +, f1 ± g1}i
13.
{ι0, ∗, f1}i ± {ψ0, +, g1}i
=⇒
{ι0 ± ψ0, +, {ι0 ∗(f1 −1), ∗, f1}i ± g1}i
14.
{ι0, +, f1}i ∗{ψ0, +, g1}i
=⇒
{ι0 ∗ψ0, +, {ι0, +, f1}1 ∗g1+
{ψ, +, g1}i ∗f1 + f1 ∗g1}i
15.
{ι0, ∗, f1}i ∗{ψ0, ∗, g1}i
=⇒
{ι0 ± ψ0, ∗, f1 ∗g1}i
16.
{ι0, ∗, f1}E
=⇒
{ιE
0 , ∗, fE
1 }i
17.
{ι0, ∗, f1}{ψ0,+,g1}i
i
=⇒
{ιψ0
0 , ∗, {ι0, ∗, f1}g1
i ∗f{ψ0,+,g1}i
1
∗fg1
1 }i
18.
E{ι0,+,f1}i
=⇒
{Eι0, ∗, Ef1}i
19.
{ι0, +, f1}n
i
=⇒
 {ι0, +, f1}i ∗{ι0, +, f1}n−1
i
if n ∈, n > 1
1/{ι0, +, f1}−n
i
if n ∈, n < 0
20.
{ι0, +, f1}i!
=⇒
(
{ι0!, ∗, (Qf1
j=1{ι0 + j, +, f1}i)}i
if f1 ≥0
{ι0!, ∗, (Q|f1|
j=1{ι0 + j, +, f1}i)−1}i
if f1 < 0
21.
{ι0, +, ι1, ∗, f2}i
=⇒
{ι0, ∗, f2}i
when ι1
ι0 = f1 −1
48

Table 2.2: Chains of Recurrences inverse simpliﬁcation rules.
Expression
Rewrite
1.
{ϕ0, +, f1}i
=⇒
ϕ0 + {0, +, f1}i
2.
{ϕ0, ∗, f1}i
=⇒
ϕ0 ∗{1, ∗, f1}i
3.
{0, +, −f1}i
=⇒
−{0, +, f1}i
4.
{ϕ0, +, f1 + g1}i
=⇒
{ϕ0, +, f1}i + {ι0, +, g1}i
5.
{ϕ0, ∗, f1 ∗g1}i
=⇒
f1 ∗{ϕ0, ∗, g1}i
6.
{0, +, fi
1}i
=⇒
fi
1−1
f1−1
7.
{0, +, fg1+h1
1
}i
=⇒
{0, +, fg1
1 ∗fh1
1 }i
8.
{0, +, fg1∗h1
1
}i
=⇒
{0, +, (fg1
1 )h1}i
9.
{0, +, f1}i
=⇒
i ∗f1
10.
{0, +, i}i
=⇒
i2−i
2
11.
{0, +, in}i
=⇒
Pn
k=0
(n+1
k )
n+1 Bkin−k+1
12.
{1, ∗, −f1}i
=⇒
(−1)i{1, ∗, f1}i
13.
{1, ∗, 1
f1 }i
=⇒
{1, ∗, f1}−1
i
14.
{1, ∗, f1 ∗g1}i
=⇒
{1, ∗, f1}i ∗{1, ∗, g1}i
15.
{1, ∗, fg1
1 }i
=⇒
f{1,∗,g1}i
1
16.
{1, ∗, gf1
1 }i
=⇒
{1, ∗, g1}f1
i
17.
{1, ∗, f1}i
=⇒
fi
1
18.
{1, ∗, i}i
=⇒
0i
19.
{1, ∗, i + f1}i
=⇒
(i+f1−1)!
(f1−1)!
20.
{1, ∗, f1 −1}i
=⇒
(−1)i ∗(i−f1−1)!
(−f1−1)!
49

CHAPTER 3
THEORY
In this dissertation, we propose to extend the SSA form to Inductive SSA, which is based on
the Chains of Recurrences (CR) formalism using the extended CR# (CR-sharp) algebra that
eﬀectively represents the value progressions of (conditionally) updated loop-variant variables
in loops. Inductive SSA form augments the classic SSA form with semantic information
that captures recurrence variable progressions and induction expressions using the CR#
notation and algebra. The CR# algebra enables Inductive SSA to capture more recurrence
relations compared to state-of-the-art induction variable recognition techniques. The unique
CR normal form in Inductive SSA enables the semantic matching for syntactically diﬀerent
expressions.
This chapter introduces Inductive SSA concept and the CR# algebra, which is the basis
of constructing the inductive proofs for the Inductive SSA form.
3.1
The CR# Chains of Recurrences Extension
We augmented the CR algebra with a nontrivial extension. The extension will be referred
to as the CR# (CR-sharp) algebra. Its main purpose is for representing and analyzing loop-
variant variables in a uniform, consistent, and mathematically sound way. CR# is used for
the analysis and manipulation of sequences. A new operator # together with new algebraic
simpliﬁcation rules on CR# forms containing the operator and CR# alignment and bounds
operations are introduced in the CR# algebra. With these extensions, we are able to analyze
more complicated ﬂow-sensitive loop-variant variables including out-of-sequence loop-variant
variables that state-of-the-art induction variable recognition methods failed to handle. So,
CR# algebra covers more types of loop-variant variables and more importantly, provides an
uniﬁed and systematic classiﬁcation approach to generalize loop-variant variable analysis.
50

3.1.1
The Delay Operator #
Deﬁnition 1 The delay operator # is deﬁned by
(x#y) = y
for any x and y.
A CR# form containing the delay operator will be referred to as a delayed CR# form.
The delay operator allows a set of initial values to take eﬀect before the sequence starts.
Thus, delayed CR# forms deﬁne recurrences with out-of-sequence values. This serves two
important purposes. First, delayed CR# forms can be used to deﬁne any sequence of values
x0, x1, . . . , xk, possibly followed by a polynomial, geometric, or another delayed form Ψi as
in Φi = {x0, #, x1, #, · · · , #, xk, #, Ψi}. Secondly, wrap-around variables and indirect wrap-
around variables can be accurately represented using delayed CR# forms. Wrap-around
variables are currently handled with ad-hoc techniques by existing IVS methods and indirect
wrap-around variables cannot be handled.
For example, the value sequences of several delayed CR# forms are listed as follows:
iteration i =
0
1
2
3
4
5
. . .
{9, #, 1, +, 2}i value sequence =
9
1
3
5
7
9
. . .
{9, #, 2, #, 1, +, 2}i value sequence =
9
2
1
3
5
7
. . .
{1, ∗, 1, #, 2}i value sequence =
1
1
2
4
8
16
. . .
3.1.2
CR# Simpliﬁcation Rules
The CR algebra rewrite rules are also extended by adding new rules which handle the
simpliﬁcation of CR# expression with # operators, see Table 3.1 and 3.2 for the CR#
rewrite rules.
3.1.3
Completeness of CR#
Theorem 1 CR# is terminating.
Proof. In CR#, every redex is reduced to a CR of the form {E, ⊙, F} for some term E,
operator ⊙∈{+, ∗} and term F. Because E does not contain any redexes it is not further
reduced. For rules 28, 29 in the table 3.1, term F contains new redexes.
51

Table 3.1: CR# simpliﬁcation rules.
Expression
Rewrite
1.
{ι0, +, 0}i
=⇒
ι0
2.
{ι0, ∗, 1}i
=⇒
ι0
3.
{0, ∗, f1}i
=⇒
0
4.
−{ι0, +, f1}i
=⇒
{−ι0, +, −f1}i
5.
−{ι0, ∗, f1}i
=⇒
{−ι0, ∗, f1}i
6.
{ι0, +, f1}i ± E
=⇒
{ι0 ± E, +, f1}i
7.
{ι0, ∗, f1}i ± E
=⇒
{ι0 ± E, +, ι0 ∗(f1 −1), ∗, f1}i
8.
E ∗{ι0, +, f1}i
=⇒
{E ∗ι0, +, E ∗f1}i
9.
E ∗{ι0, ∗, f1}i
=⇒
{E ∗ι0, ∗, f1}i
10.
E/{ι0, +, f1}i
=⇒
1/{ι0/E, +, f1/E}i
11.
E/{ι0, ∗, f1}i
=⇒
{E/ι0, ∗, 1/f1}i
12.
{ι0, +, f1}i ± {ψ0, +, g1}i
=⇒
{ι0 ± ψ0, +, f1 ± g1}i
13.
{ι0, ∗, f1}i ± {ψ0, +, g1}i
=⇒
{ι0 ± ψ0, +, {ι0 ∗(f1 −1), ∗, f1}i ± g1}i
14.
{ι0, +, f1}i ∗{ψ0, +, g1}i
=⇒
{ι0 ∗ψ0, +, {ι0, +, f1}1 ∗g1+
{ψ, +, g1}i ∗f1 + f1 ∗g1}i
15.
{ι0, ∗, f1}i ∗{ψ0, ∗, g1}i
=⇒
{ι0 ± ψ0, ∗, f1 ∗g1}i
16.
{ι0, ∗, f1}E
=⇒
{ιE
0 , ∗, fE
1 }i
17.
{ι0, ∗, f1}{ψ0,+,g1}i
i
=⇒
{ιψ0
0 , ∗, {ι0, ∗, f1}g1
i ∗f{ψ0,+,g1}i
1
∗fg1
1 }i
18.
E{ι0,+,f1}i
=⇒
{Eι0, ∗, Ef1}i
19.
{ι0, +, f1}n
i
=⇒
 {ι0, +, f1}i ∗{ι0, +, f1}n−1
i
if n ∈, n > 1
1/{ι0, +, f1}−n
i
if n ∈, n < 0
20.
{ι0, +, f1}i!
=⇒
(
{ι0!, ∗, (Qf1
j=1{ι0 + j, +, f1}i)}i
if f1 ≥0
{ι0!, ∗, (Q|f1|
j=1{ι0 + j, +, f1}i)−1}i
if f1 < 0
21.
{ι0, +, ι1, ∗, f2}i
=⇒
{ι0, ∗, f2}i
when ι1
ι0 = f1 −1
22.
{ϕ0, #, f1}i
=⇒
f1
when ϕ0 = VBf1
23.
−{ι0, #, f1}i
=⇒
{−ι0, #, −f1}i
24.
{ι0, #, f1}i ± E
=⇒
{ι0 ± E, #, f1 ± E}i
25.
E ∗{ι0, #, f1}i
=⇒
{E ∗ι0, #, E ∗f1}i
26.
{ι0, #, f1}i ± {ψ0, #, g1}i
=⇒
{ι0 ± ψ0, #, f1 ± g1}i
27.
{ι0, #, f1}i ∗{ψ0, #, g1}i
=⇒
{ι0 ∗ψ0, #, f1 ∗g1}i
28.
{ι0, #, f1}i ± {ψ0, +, g1}i
=⇒
{ι0 ± ψ0, #, f1 ± F{ψ0, +, g1}i}i
29.
{ι0, #, f1}i ∗{ψ0, +, g1}i
=⇒
{ι0 ∗ψ0, #, f1 ∗F{ψ0, +, g1}i}i
30.
{ι0, +, ι1, #, f2}i
=⇒
{ι0, #, ι0 + ι1, +, f2}i
31.
{ι0, ∗, ι1, #, f2}i
=⇒
{ι0, #, ι0 ∗ι1, ∗, f2}i
52

Table 3.2: CR# inverse simpliﬁcation rules.
Expression
Rewrite
1.
{ϕ0, +, f1}i
=⇒
ϕ0 + {0, +, f1}i
2.
{ϕ0, ∗, f1}i
=⇒
ϕ0 ∗{1, ∗, f1}i
3.
{0, +, −f1}i
=⇒
−{0, +, f1}i
4.
{ϕ0, +, f1 + g1}i
=⇒
{ϕ0, +, f1}i + {ι0, +, g1}i
5.
{ϕ0, ∗, f1 ∗g1}i
=⇒
f1 ∗{ϕ0, ∗, g1}i
6.
{0, +, fi
1}i
=⇒
fi
1−1
f1−1
7.
{0, +, fg1+h1
1
}i
=⇒
{0, +, fg1
1 ∗fh1
1 }i
8.
{0, +, fg1∗h1
1
}i
=⇒
{0, +, (fg1
1 )h1}i
9.
{0, +, f1}i
=⇒
i ∗f1
10.
{0, +, i}i
=⇒
i2−i
2
11.
{0, +, in}i
=⇒
Pn
k=0
(n+1
k )
n+1 Bkin−k+1
12.
{1, ∗, −f1}i
=⇒
(−1)i{1, ∗, f1}i
13.
{1, ∗, 1
f1 }i
=⇒
{1, ∗, f1}−1
i
14.
{1, ∗, f1 ∗g1}i
=⇒
{1, ∗, f1}i ∗{1, ∗, g1}i
15.
{1, ∗, fg1
1 }i
=⇒
f{1,∗,g1}i
1
16.
{1, ∗, gf1
1 }i
=⇒
{1, ∗, g1}f1
i
17.
{1, ∗, f1}i
=⇒
fi
1
18.
{1, ∗, i}i
=⇒
0i
19.
{1, ∗, i + f1}i
=⇒
(i+f1−1)!
(f1−1)!
20.
{1, ∗, f1 −1}i
=⇒
(−1)i ∗(i−f1−1)!
(−f1−1)!
21.
{ϕ0, #, f1}i
=⇒
(i = 0)?ϕ0 : f1[i ←i −1]
In rule 28 the redexes in term F are {ι0 ± ψ0, #, f1 ± F{ψ0, +, g1}i}i assuming that f1
are fully reduced. Function F shifts {ψ0, +, g1}i} to {ψ0 + g1, +, g1}i}. The redex can be
reduced using rule 6 and rules 12 which are CR simpliﬁcation rules. Since CR is terminating
which is proved in [50], the reduction of the redexes on the right-hand side of rule 28 are
terminating.
In rule 29 the redexes in term F are {ι0 ∗ψ0, #, f1 ∗F{ψ0, +, g1}i}i assuming that f1
are fully reduced. Function F shifts {ψ0, +, g1}i} to {ψ0 + g1, +, g1}i}. The redex can be
reduced using rule 6 and rules 12 which are CR simpliﬁcation rules. Since CR is terminating
which is proved in [50], the reduction of the redexes on the right-hand side of rule 29 are
terminating.
□
53

Theorem 2 (Knuth and Bendix [91]) Let TRS R be terminating. Then R is conﬂuent
iﬀeach critical pair has a common reduct.
Theorem 3 CR# is complete.
Proof. Since CR# is terminating the proof follows from Theorem 1. The critical pairs
involving rules 22 are formed by unifying the redex of the rule with a subterm of the redexes
of rules 23-31.
Consider the redex {ϕ0, #, ϕ0} of rule 22.
• The redex of rule 23: −{ϕ0, #, ϕ0} forms the critical pair ⟨−ϕ0, {−ϕ0, #, −ϕ0}⟩. The
pair has a common reduct found by applying rule 22: {−ϕ0, #, −ϕ0}
22
⇒−ϕ0.
• The redex of rule 24: E±{ϕ0, #, ϕ0} forms the critical pair ⟨E±ϕ0, {E±ϕ0, #, E±ϕ0}⟩.
The pair has a common reduct found by applying rule 22: {E±ϕ0, #, E±ϕ0}
22
⇒E±ϕ0.
• The redex of rule 25: E∗{ϕ0, #, ϕ0} forms the critical pair ⟨E∗ϕ0, {E∗ϕ0, #, E∗ϕ0}⟩.
The pair has a common reduct found by applying rule 22: {E ∗ϕ0, #, E ∗ϕ0}
22
⇒E ∗ϕ0.
• The redex of rule 26:
{ϕ0, #, ϕ0} ± {ψ0, #, g1} forms the critical pair ⟨ϕ0 ±
{ψ0, #, g1}, {ϕ0 ± ψ0, #, ϕ0 ± g1}⟩. The pair has a common reduct found by applying
rule 24: ϕ0 ± {ψ0, #, g1}
24
⇒{ϕ0 ± ψ0, #, ϕ0 ± g1}.
• The redex of rule 27: {ϕ0, #, ϕ0}∗{ψ0, #, g1} forms the critical pair ⟨ϕ0∗{ψ0, #, g1}, {ϕ0∗
ψ0, #, ϕ0 ∗g1}⟩.
The pair has a common reduct found by applying rule 25: ϕ0 ∗
{ψ0, #, g1}
25
⇒{ϕ0 ∗ψ0, #, ϕ0 ∗g1}.
• The redex of rule 28:
{ϕ0, #, ϕ0} ± {ψ0, +, g1} forms the critical pair ⟨ϕ0 ±
{ψ0, +, g1}, {ϕ0 ± ψ0, #, ϕ0 ± F{ψ0, +, g1}}⟩.
This pair has a common reduct:
ϕ0 ± {ψ0, +, g1}
6⇒{ϕ0 ± ψ0, +, g1} and {ϕ0 ± ψ0, #, ϕ0 ± F{ψ0, +, g1}} = {ϕ0 ±
ψ0, #, ϕ0 ± {ψ0 + g1, +, g1}}
6⇒{ϕ0 ± ψ0, #, ϕ0 ± ψ0 + g1, +, g1}
22
⇒{ϕ0 ± ψ0, +, g1}.
• The redex of rule 29: {ϕ0, #, ϕ0}∗{ψ0, +, g1} forms the critical pair ⟨ϕ0∗{ψ0, +, g1}, {ϕ0∗
ψ0, #, ϕ0 ∗F{ψ0, +, g1}}⟩. This pair has a common reduct: ϕ0 ∗{ψ0, +, g1}
8⇒{ϕ0 ∗
ψ0, +, ϕ0∗g1} and {ϕ0∗ψ0, #, ϕ0∗F{ψ0, +, g1}} = {ϕ0∗ψ0, #, ϕ0∗{ψ0+g1, +, g1}}
8⇒
{ϕ0 ∗ψ0, #, ϕ0 ∗ψ0 + ϕ0 ∗g1, +, ϕ0 ∗g1}
22
⇒{ϕ0 ∗ψ0, +, ϕ0 ∗g1}.
54

• The redex of rule 30: {ϕ1, +, ϕ0, #, ϕ0} forms the critical pair ⟨{ϕ1, +, ϕ0}, {ϕ1, #, ϕ1+
ϕ0, +, ϕ0}⟩. The pair has a common reduct found by applying rule 22: {ϕ1, #, ϕ1 +
ϕ0, +, ϕ0}
22
⇒{ϕ1, +, ϕ0}.
• The redex of rule 31: {ϕ1, ∗, ϕ0, #, ϕ0} forms the critical pair ⟨{ϕ1, ∗, ϕ0}, {ϕ1, #, ϕ1 ∗
ϕ0, ∗, ϕ0}⟩. The pair has a common reduct found by applying rule 22: {ϕ1, #, ϕ1 ∗
ϕ0, ∗, ϕ0}
22
⇒{ϕ1, ∗, ϕ0}.
The critical pairs involving rules 1 are formed by unifying the redex of the rule with a
subterm of the redexes of rules 28, 29. Consider the redex {ψ0, +, 0} of rule 1.
• The redex of rule 28: {ϕ0, #, f1} ± {ψ0, +, 0} forms the critical pair ⟨{ϕ0, #, f1} ±
ψ0, {ϕ0±ψ0, #, f1±F{ψ0, +, 0}}⟩. This pair has a common reduct: {ϕ0, #, f1}±ψ0
24
⇒
{ϕ0±ψ0, #, f1±ψ0} and {ϕ0±ψ0, #, f1±F{ψ0, +, 0}} = {ϕ0±ψ0, #, f1±{ψ0, +, 0}}
6⇒
{ϕ0 ± ψ0, #, f1 ± ψ0, +, 0}
1⇒{ϕ0 ± ψ0, #, f1 ± ψ0}.
• The redex of rule 29: {ϕ0, #, f1} ∗{ψ0, +, 0} forms the critical pair ⟨{ϕ0, #, f1} ∗
ψ0, {ϕ0 ∗ψ0, #, f1 ∗F{ψ0, +, 0}}⟩. This pair has a common reduct: {ϕ0, #, f1} ∗ψ0
25
⇒
{ϕ0 ∗ψ0, #, f1 ∗ψ0} and {ϕ0 ∗ψ0, #, f1 ∗F{ψ0, +, 0}} = {ϕ0 ∗ψ0, #, f1 ∗{ψ0, +, 0}}
8⇒
{ϕ0 ∗ψ0, #, f1 ∗ψ0, +, f1 ∗0}
1⇒{ϕ0 ∗ψ0, #, f1 ∗ψ0}.
□
3.1.4
The CR# Lattice
We deﬁne a lattice on the CR# forms in the CR# algebra by introducing three special values
⊥, ⊤, and ⊤̸=.
Deﬁnition 2 The ⊥, ⊤, and ⊤̸= elements are deﬁned by
• ⊥denotes an unknown quantity −∞≤⊥≤∞;
• ⊤denotes a nonnegative unknown 0 ≤⊤≤∞;
• ⊤̸= denotes a positive unknown 0 < ⊤̸= ≤∞.
Deﬁnition 3 The reduction operator R is deﬁned by
R{ϕ0, ⊙1, f1}i = {Rϕ0, ⊙1, Rf1}i
55

with Rx of a non-CR (symbolic) coeﬃcient x deﬁned by
Rx =















−⊤̸=
ifx < 0
−⊤
ifx ≤0
0
ifx = 0
⊤
ifx ≥0
⊤̸=
ifx > 0
⊥
if the sign of x is unknown
The following CR# rewrite rules are applied to reduce the CR# form to a single ⊥, ±⊤,
±⊤̸=, or 0 value:
{⊥, ⊙1, f1}i
⇒
⊥
{ϕ0, ⊙1, ⊥}i
⇒
⊥
{⊤, +, ⊤}i
⇒
⊤
{−⊤, +, −⊤}i
⇒
−⊤
{⊤, +, −⊤}i
⇒
⊥
{−⊤, +, ⊤}i
⇒
⊥
{⊤, ∗, ⊤}i
⇒
⊤
{−⊤, ∗, ⊤}i
⇒
−⊤
{ϕ0, ∗, −⊤}i
⇒
⊥
{⊤, #, ⊤}i
⇒
⊤
{−⊤, #, −⊤}i
⇒
−⊤
{−⊤, #, ⊤}i
⇒
⊥
{⊤, #, −⊤}i
⇒
⊥
The rules for 0 and ⊤̸= are similar.
Note that to determine the value of Rx when x is symbolic, we can use common rules such
as
⊤+ E
⇒
 ⊤
ifE ≥0
⊥
otherwise
E ∗⊤
⇒



⊤
ifE > 0
−⊤
ifE < 0
⊥
otherwise
The reduction operation traverses the lattice starting with a CR# form and stops at ⊥, ±⊤,
±⊤̸=, or 0. For example
R{0, +, 0, +, 1}i = {R0, +, R{0, +, 1}i}i ⇒{0, +, ⊤}i ⇒⊤
Thus, this shows that the sequence of {0, +, 0, +, 1}i is nonnegative.
56

3.1.5
CR# Monotonicity
The monotonicity of a CR# form is a useful property, when it can be proven, to determine
the value range of an expression represented by a CR# form. To determine the monotonic
properties of a CR# form we extract directional information by applying the reduction
operator on the increment function of a CR# form.
Deﬁnition 4 The monotonic operator M is deﬁned by
M{ϕ0, +, f1}i
=
Rf1
M{ϕ0, ∗, f1}i
=
 Rϕ0
ifR(f1 −1) = ⊤̸=
⊥
otherwise
M{ϕ0, #, f1}i
=
Mf1 ⋊⋉R(Vf1 −ϕ0)
with Mx = 0 when x is a (symbolic) constant. The lattice relation ⋊⋉returns the maximum
element z = x ⋊⋉y in the lattice such that z ⪯x and z ⪯y.
The monotonic operator returns directional information on a CR# form as a lattice element
⊤(monotonically increasing), ⊤̸= (strictly monotonically increasing), −⊤(monotonically
decreasing), −⊤̸= (strictly monotonically decreasing), 0 (constant), or ⊥(unknown).
Consider for example
M{0, #, 0, +, 1}i = M{0, +, 1}i ⋊⋉R(V{0, +, 1}i−0) = R1 ⋊⋉R0 = ⊤̸= ⋊⋉0 = ⊤
Thus, this shows that the sequence generated by {0, #, 0, +, 1}i is monotonically increasing.
Determining the monotonicity of a CR# form enables accurate value range analysis [92] and
nonlinear dependence testing [21].
3.1.6
Bounding the Value Range of a CR# Form
The calculation of the constant static bounds on the range of possible values of a function
is useful for data dependence testing, value range analysis, and loop bounds analysis, where
(symbolic) constant bounds are required. Static bounds are limited for loop-variant variables,
since the bounds may move with the loop iterations. Thus we seek to derive loop-variant
bounds on the value range of a loop-variant variable. Dynamic range information can be
used in array dependence testing when loop-variant variables are used for array indexing and
57

in pointer arithmetic. Bounding the value range of a CR# form using dynamic loop-variant
bounds is the basis of improving the accuracy and the capabilities of nonlinear dependence
testing using CR framework, such as the CR-based nonlinear EVT and the nonlinear value
range test [93].
To determine the direction of a recurrence, we deﬁne the step function of a CR.
Deﬁnition 5 The step function ∆Φi of a CR# form Φi = {ϕ0, ⊙1, ϕ1, ⊙2, . . . , ⊙k, ϕk}i is
deﬁned by
∆Φi = {ϕ1, ⊙2, . . . , ⊙k, ϕk}i
The direction-wise step function ∆jΦi of a multivariate CR# form Φi is the step function
with respect to an index variable j
∆jΦi =
 ∆Φi
if i=j
∆jVΦi
otherwise
where the initial value of VΦi of a CR# form is the ﬁrst coeﬃcient, which is the starting
value of the CR# form evaluated on a unit grid in the i-direction:
VΦi = ϕ0
The direction-wise step information indicates the growth rate of a function on an axis in the
iteration space.
Note that Φi = {VΦi, ⊙1, ∆Φi}i.
Deﬁnition 6 The lower bound LΦi of a multivariate CR# form Φi evaluated on i =
0, . . . , n, n ≥0, is
LΦi =



L VΦi
if L MΦi ≥0
L CR−1
i (Φi)[i ←n]
if U MΦi ≤0
L CR−1
i (Φi)
otherwise
and the upper bound UΦi of a multivariate CR# form Φi is
58

UΦi =



U VΦi
if U MΦi ≤0
U CR−1
i (Φi)[i ←n]
if L MΦi ≥0
U CR−1
i (Φi)
otherwise
where CR−1
i (Φi) is the closed form of Φi with respect to i (i.e. nested CR# forms are not
converted). It is important to point out that the L and U bounds applied to the recurrence
of a monotonic function gives the exact (symbolic) value range of the function on a discrete
domain, when the function is monotonic on the discrete grid rather than in the continuous
domain. A function that is monotonic on discrete grid points is not necessarily monotonic
in the continuous domain.
These L and U bounds have important applications in CR-enhanced dependence tests,
see [93] .
3.1.7
CR# Alignment and Bounds
To compare two CR# forms representing diﬀerent functions, we need a method to normalize
their representations so that the coeﬃcients can serve as a reference point for comparison.
Essentially we need an alignment procedure that aligns the coeﬃcients. This will serve two
purposes. First to compare coeﬃcients to determine equality and inequality of the CR#
forms, and second to combine the CR# forms into one by an arithmetic operation such as
addition and multiplication. The latter also enables the computation of the minimum and
maximum CR# form of two or more aligned CR# forms.
The following lemmas which are used to align and bound CR# forms and proof of those
lemmas are both part of my research on ﬂow-sensitive induction variable analysis problem.
Two or more CR# forms of diﬀerent lengths or with diﬀerent operations can be aligned
for comparison.
Deﬁnition 7 Two CR# forms Φi and Ψi over the same index variable i are aligned if they
have the same length k and the operators ⊙j, j = 1, . . . , k, form a pairwise match.
To align a (delayed) CR# form of a mixed polynomial and geometric function to a longer
(delayed) CR# form, + operators can be inserted for pairwise alignment of the ∗operations
between two or more CR# forms.
59

Lemma 1 Let Φi = {a, ∗, r}i be a geometric CR# form with initial value a and ratio r (r
is invariant of i). Then,
Φi = {a, +, a(r −1), +, a(r −1)2, +, · · · , +, a(r −1)m, ∗, r}i
for any positive integer m > 0.
Proof 1 The proof is by induction on m.
• For the base case m = 1 we show that {a, ∗, r}i = {a, +, a(r −1), ∗, r}i in two steps.
1. Consider a = 1.
By the deﬁnition of the CR semantics the sequence f[i] for
{1, ∗, r}i and g[i] for {1, +, r−1, ∗, r}i are computed by
cr0 = 1
for i = 0 to n–1
f[i] = cr0
cr0 = cr0 ∗r
end for
cr0 = 1
cr1 = r–1
for i = 0 to n–1
g[i] = cr0
cr0 = cr0 + cr1
cr1 = cr1 ∗r
end for
(a) For iteration i = 0, we ﬁnd that f[0] = g[0]
(b) For iterations i = 1, . . . , n −1, we ﬁnd that
f[i]
=
i−1
Y
j=0
r
=
ri
g[i]
=
1 +
i−1
X
j=0
(r −1)rj
=
1 +
i−1
X
j=0
r rj −
i−1
X
j=0
rj
=
1 +
i
X
j=1
rj −
i−1
X
j=0
rj
=
ri
2. Consider a ̸= 1. It follows from the CR algebra that {a, ∗, r}i = a{1, ∗, r}i and
a{1, +, r−1, ∗, r}i = {a, +, a(r−1), ∗, r}i, and therefore that
60

{a, ∗, r}i
=
a{1, ∗, r}i
=
a{1, +, r −1, ∗, r}i
=
{a, +, a(r −1), ∗, r}i
• Suppose the equation holds for k = m −1. We have
Φi = {a, +, a(r −1), +, a(r −1)2, +, · · · , +, a(r −1)k, ∗, r}i
Because the “ﬂat” CR form Φi is identical to a nested CR form [89, 90], we use the
base case to rewrite the tail part of the nested CR# form as follows
{a, +, a(r−1), +, · · · , +, a(r−1)k, ∗, r}i
= {a, +, a(r−1), +, · · · , +, {a(r−1)k, ∗, r}i}i
= {a, +, a(r−1), +, · · · , +, {a(r−1)k, +, a(r−1)k(r−1), ∗, r}i}i
= {a, +, a(r−1), +, · · · , +, {a(r−1)k, +, a(r−1)k+1, ∗, r}i}i
= {a, +, a(r−1), +, · · · , +, a(r−1)m−1, +, a(r−1)m, ∗, r}i
Thus, it follows from the induction hypothesis that
Φi = {a, +, a(r−1), +, a(r−1)2, +, · · · , +, a(r−1)m, ∗, r}i.
Corollary 1 Let Φi = {ϕ0, ⊙1, · · · , ⊙k−1, ϕk−1, ∗, ϕk}i such that ϕk is invariant of i. Then,
any number m > 0 of + operators can be inserted at the (k −1)th coeﬃcient as follows
Φi = {ϕ0, ⊙1, · · · , ⊙k−1, ϕk−1,
+, ϕk−1(ϕk−1), +, ϕk−1(ϕk−1)2, +, · · · , +, ϕk−1(ϕk−1)m
|
{z
}
inserted
, ∗, ϕk}i
without changing the sequence of Φi.
Consider for example Φi = {1, +, 1}i and Ψi = {1, ∗, 2}i. The CR# forms are aligned using
Lemmas 1
Φi
=
{1, +, 1}i
=
{1, +, 1, ∗, 1}i
Ψi
=
{1, ∗, 2}i
=
{1, +, 1, ∗, 2}i
61

A delay operator can be inserted anywhere in a CR# form according to the following
lemma.
Lemma 2 Let Φi = {ϕ0, ⊙1, ϕ1, ⊙2, . . . , ⊙k, ϕk}i be a (multivariate) CR# form. Then,
Φi = {ϕ0, #, FΦi}i
where F is called the forward shift operator, deﬁned by
FΦi = {ψ0, ⊙1, ψ1, ⊙2, . . . , ⊙k, ψk}i
with ψj = ϕj ⊙j+1 ϕj+1 for j = 0, . . . , k−1 and ψk = ϕk.
Proof.
The value sequence val[i] of CR# form Φi = {ϕ0, ⊙1, ϕ1, ⊙2, . . . , ⊙k, ϕk}i for
i = 0, ..., n −1 is computed by the following CR loop template:
cr0
= ϕ0
cr1
= ϕ1
:
= :
crk−1 = ϕk−1
for i = 0 to n−1
val[i] = cr0
cr0
= cr0
⊙1 cr1
cr1
= cr1
⊙2 cr2
:
= :
:
:
crk−1 = crk−1 ⊙k ϕk
end for
After loop peeling, we obtain
cr0
= ϕ0
cr1
= ϕ1
:
= :
crk−1 = ϕk−1
val[0] = cr0
cr0
= cr0
⊙1 cr1
cr1
= cr1
⊙2 cr2
:
= :
:
:
crk−1 = crk−1 ⊙k ϕk
for i = 1 to n−1
val[i] = cr0
cr0
= cr0
⊙1 cr1
cr1
= cr1
⊙2 cr2
:
= :
:
:
crk−1 = crk−1 ⊙k ϕk
end for
62

After rewriting the initialization assignments, we have
s
= ϕ0
cr0
= ϕ0
⊙1 ϕ1
cr1
= ϕ1
⊙2 ϕ2
:
= :
crk−1 = ϕk−1 ⊙k ϕk
val[0] = s
for i = 1 to n−1
val[i] = cr0
cr0
= cr0
⊙1 cr1
cr1
= cr1
⊙2 cr2
:
= :
:
:
crk−1 = crk−1 ⊙k ϕk
end for
By using s as a wrap-around variable in the loop to sink val[0]=s back into the loop, we
obtain
s
= ϕ0
cr0
= ϕ0
⊙1 ϕ1
cr1
= ϕ1
⊙2 ϕ2
:
= :
crk−1 = ϕk−1 ⊙k ϕk
for i = 0 to n−1
val[i] = s
s
= cr0
cr0
= cr0
⊙1 cr1
cr1
= cr1
⊙2 cr2
:
= :
:
:
crk−1 = crk−1 ⊙k ϕk
end for
To see that this sequence computation is equivalent to the sequence of {ϕ0, #, FΦi}i, we
use the deﬁnition of the forward shift operator F. The value sequence val[i] of CR# form
{ϕ0, #, FΦi}i is computed by the CR loop template:
63

s
= ϕ0
cr0
= ψ0
= ϕ0
⊙1 ϕ1
cr1
= ψ1
= ϕ1
⊙2 ϕ2
:
= :
crk−1 = ψk−1 = ϕk−1 ⊙k ϕk
for i = 0 to n−1
val[i] = s
s
= s
# cr0
cr0
= cr0
⊙1 cr1
cr1
= cr1
⊙2 cr2
:
= :
:
:
crk−1 = crk−1 ⊙k ϕk
end for
Because the assignment s = s # cr0 is identical to s = cr0, the value computations are
semantically equivalent. Thus, this completes the proof that
Φi = {ϕ0, #, FΦi}i
□
To align two CR# forms of unequal length, the shorter CR can be lengthened by adding
dummy operations as follows.
Lemma 3 Let Φi = {ϕ0, ⊙1, ϕ1, ⊙2, · · · , ⊙k, ϕk}i be a univariate or multivariate CR# form,
where ϕk is invariant of i. Then, the following identities hold
Φi
=
{ϕ0, ⊙1, ϕ1, ⊙2, · · · , ⊙k, ϕk, +, 0}i
Φi
=
{ϕ0, ⊙1, ϕ1, ⊙2, · · · , ⊙k, ϕk, ∗, 1}i
Φi
=
{ϕ0, ⊙1, ϕ1, ⊙2, · · · , ⊙k, ϕk, #, ϕk}i
Proof. The proof immediately follows as a consequence of the CR semantics because the
initial value of the induction variable crk for coeﬃcient ϕk is set to ϕk and the value of crk
is unchanged in the loop (either by adding zero or multiplying by one).
□
Two or more CR# forms can be aligned using Lemmas 1, 2, and 3. Consider for example
Φi = {1, #, 1, +, 2}i = {1, #, 1, +, 2, ∗, 1}i
Ψi = {1, ∗, 2}i = {1, #, 2, ∗, 2}i = {1, #, 2, +, 2, ∗, 2}i
Alignment allows us to compare the coeﬃcients of CR# forms to determine bounds. By
comparing the coeﬃcients of two CR# forms we can determine the min/max bounds of two
CR# forms.
64

Deﬁnition 8 The minimum of two CR# form is inductively deﬁned by
min({ϕ0, #, f1}i, {ψ0, #, g1}i)={min(ϕ0, ψ0), #, min(f1, g1)}i
min({ϕ0, +, f1}i, {ψ0, +, g1}i)={min(ϕ0, ψ0), +, min(f1, g1)}i
min({ϕ0, ∗, f1}i, {ψ0, ∗, g1}i)
=























{min(ϕ0, ψ0), ∗, min(f1, g1)}i
if ϕ0>0 ∧ψ0>0 ∧f1>0 ∧g1>0
{min(ϕ0, ψ0), ∗, max(f1, g1)}i
if ϕ0<0 ∧ψ0<0 ∧f1>0 ∧g1>0
{ϕ0, ∗, f1}i
if ϕ0<0 ∧ψ0>0 ∧f1>0 ∧g1>0
{ψ0, ∗, g1}i
if ϕ0>0 ∧ψ0<0 ∧f1>0 ∧g1>0
{−max(|ϕ0|, |ψ0|), ∗, max(|f1|, |g1|)}i
if f1<0 ∨g1<0
⊥
otherwise
where the sign of the coeﬃcients is determined using the monotonicity properties of the
coeﬃcients. The maximum of two CR# forms is inductively deﬁned by
max({ϕ0, #, f1}i, {ψ0, #, g1}i)={max(ϕ0, ψ0), #, max(f1, g1)}i
max({ϕ0, +, f1}i, {ψ0, +, g1}i)={max(ϕ0, ψ0), +, max(f1, g1)}i
max({ϕ0, ∗, f1}i, {ψ0, ∗, g1}i)
=























{max(ϕ0, ψ0), ∗, max(f1, g1)}i
if ϕ0>0 ∧ψ0>0 ∧f1>0 ∧g1>0
{max(ϕ0, ψ0), ∗, min(f1, g1)}i
if ϕ0<0 ∧ψ0<0 ∧f1>0 ∧g1>0
{ϕ0, ∗, f1}i
if ϕ0>0 ∧ψ0<0 ∧f1>0 ∧g1>0
{ψ0, ∗, g1}i
if ϕ0<0 ∧ψ0>0 ∧f1>0 ∧g1>0
{max(|ϕ0|, |ψ0|), ∗, max(|f1|, |g1|)}i
if f1<0 ∨g1<0
⊥
otherwise
Consider for example
min({1, #, 1, +, 2, ∗, 1}i, {1, #, 2, +, 2, ∗, 2}i)={1, #, 1, +, 2, ∗, 1}i
max({1, #, 1, +, 2, ∗, 1}i, {1, #, 2, +, 2, ∗, 2}i)={1, #, 2, +, 2, ∗, 2}i
Thus, the sequence of values of the CR# form {1, #, 1, +, 2, ∗, 1}i provides a lower bound
and the sequence of values of the CR# form {1, #, 2, +, 2, ∗, 2}i provides an upper bound on
the two CR# forms. This accurately captures the following behavior of the variable k (SSA
variable k1) in the loop:
65

loop
j1 = φ(1, j2)
// j1 = {1, +, 2}
k1 = φ(1, k4)
// {1, #, 1, +, 2, ∗, 1} ≤k1 ≤{1, #, 2, +, 2, ∗, 2}
if (. . . )
k2 = j1
// update represented by {1, #, 1, +, 2}
else
k3 = 2 ∗k1
// update represented by {1, ∗, 2}
j2 = j1 + 2
// update represented by {1, +, 2}
k4 = φ(k2, k3)
// merge and align {1, #, 1, +, 2} and {1, ∗, 2}
endloop
3.1.8
Typed CR# Forms
To make the CR# framework applicable for integration in compilers, we also need to consider
typed CR# forms for range-constrained IV due to type representation limitations. A CR#
form for an induction variable in the program will not be used safely if an integer overﬂow
occurs in the path of that induction variable cycle in the program.
Integer Overﬂow Issues
Integer overﬂow occurs when an integer operation result can not ﬁt in the value range of an
integer type. The value range of a integer number is able to be stored based on the size of
integer type in bytes, and also depends on whether or not the integer number is signed or
unsigned. Common sizes for integers include 8 bits, 16 bits, 32 bits, 64 bits and 128 bits.
Integer overﬂow causes the result to be truncated to a size that can be represented by the
integer variable. Diﬀerent compilers may treat the overﬂow problem in diﬀerent ways. They
can totally ignore the overﬂow problem or they can abort the program. Most compilers seem
to ignore the overﬂow.
Consider the example in Figure 3.1(a) with IV a of unsigned char type.
The value
sequences for variable a are shown in Figure 3.1(b). Overﬂow occurs at iteration 8. The
untyped CR# form for a is {0, +, 30}. Since the number of iteration of this loop is 10, the
value range of the CR# form is [0, 270] if the type information is not considered, which
exceeds the maximum value which an unsigned char type can hold and is not the actual
value sequence of variable a.
So, to ensure CR# form represents the real value sequences of the variables in the loop
when overﬂow occurs, the type information need to be considered and attached with CR#
66

unsigned char a = 0;
int i;
for (i = 0; i < 10; i++) {
. . .
a = a + 30;
. . .
}
i = 0
a = 0
1
30
2
60
3
90
4
120
5
150
6
180
7
210
8
240
9
14
(a)
(b)
Figure 3.1: Example code fragment (a) demonstrating integer overﬂow (b) of IV a.
form.
Note that range constrained IVs can have periodic sequences, when overﬂow is allowed
to occur periodically. Such periodic sequences can be represented by taking the modulo of
the IV (or CR form) value as discussed below.
Implementing Typed CR# Representations
In a regular CR# form Φi = {ϕ0, ⊙1, ϕ1, ⊙2, . . . , ⊙k, ϕk}i, the operator ⊙1, . . . , ⊙k is
annotated with the corresponding data type information for its operands.
A typed CR# form:
Φi = {ϕ0, ⊙1,t1, ϕ1, ⊙2,t2, · · · , ⊙k,tk, ϕk}i
represents a set of recurrence relations over a grid i = 0, . . . , n−1 deﬁned by the following
loop template with t-typed CR variables:
67

t1
cr0
= ϕ0
t2
cr1
= ϕ1
:
= :
tk
crk−1 = ϕk−1
for i = 0 to n−1
val[i] = cr0
cr0
= cr0
⊙1 cr1
cr1
= cr1
⊙2 cr2
:
= :
:
:
crk−1 = crk−1 ⊙k ϕk
end for
For example, the CR# form {0, + unsigned char, 30} represents the value sequence deﬁned
by the following loop:
unsigned char cr0 = 0
for i = 0 to n−1
val[i] = cr0
cr0 = cr0 + 30
end for
For typed CR# forms, the CR# simpliﬁcation rules in Table
3.1 and
3.2 are valid
only when the CR forms and values in the rules are of exactly the same data type. The
monotonic properties of a typed CR# form depend upon not only the directional information
we discussed in Section 3.1.5, but also if the overﬂow occurs.
Typed IVs in Unbounded Loops
In some cases, the number of loop iterations of a loop cannot be determined accurately
at compile time. Consider the example shown in Figure 3.2(a). The typed CR# form for
variable k is {10, +unsigned char, 2}.
If the upper bound of the loop iteration u is small
enough, the overﬂow on byte size variable k will not occur. Therefore, the value of the array
subscript k in a[k] is monotonically increasing based on monotonicity analysis on CR# form
{10, +, 2} of k and this loop can be executed in parallel. Otherwise, there may be a potential
output dependence on array a because of overﬂow on variable k wrap around the value of
variable k and break the monotonicity on value sequence of k. So, when overﬂow occurs,
this loop needs to be executed sequentially due to the possible output dependence between
array elements.
68

unsigned char k = 10;
int i;
for (i = 0; i < u; i++) {
. . .
a[k] = . . .
k = k + 2;
. . .
}
unsigned char k = 10;
int i;
if (u < 123)
forall (i = 0; i < u; i++) {
. . .
a[k] = . . .
k = k + 2;
. . .
}
else
for (i = 0; i < u; i++) {
. . .
a[k] = . . .
k = k + 2;
. . .
}
(a)
(b)
Figure 3.2: Loop transformation example 1.
The maximum value of variable u with which the overﬂow on k does not occur can be
computed by computing the closed form of typed CR# form of variable k, which is 2∗i+10,
and solving the inequality 2∗i+10 < 256. The loop after transformed is shown in Figure 3.2
(b), which shows when u is smaller than 123, overﬂow will not occurs and the loop can run
in parallel. Otherwise, the serial version need to be run.
However, there are still many cases in which the array subscript contains variables or
constants other than the induction variable. For example, consider the loop in Figure 3.3(a)
where the array subscript expression is k + 1.
The CR expression for expression k + 1
is {10, +, 2} + 1.
The monotonic characteristic of CR expression {10, +, 2} + 1 without
considering the overﬂow problem can be analyzed as follows:
M{10, +, 2} + 1 = M{10 + 1, +, 2} = R2 = ⊤
Which means that the value of expression k + 1 is monotonically increasing. However,
when we consider the overﬂow may occurs on variable k, the {10, +, 2} + 1 ⇏{10 + 1, +, 2}.
The reason is the two addition operations are of diﬀerent types as we can see in the following
69

unsigned char k = 10;
int i;
for (i = 0; i < u; i++) {
. . .
a[k+1] = . . .
k = k + 2;
. . .
}
unsigned char k = 10;
int i;
if (value sequence of k + 1 are monotonically
increasing or decreasing && u < 123)
forall (i = 0; i < u; i++) {
. . .
a[k+1] = . . .
k = k + 2;
. . .
}
else
for (i = 0; i < u; i++) {
. . .
a[k+1] = . . .
k = k + 2;
. . .
}
(a)
(b)
Figure 3.3: Loop transformation example 2.
forms:
{10, +unsigned char, 2} +int 1 ⇏{10 +int 1, +unsigned char, 2}
All the CR simpliﬁcation rules are true only when the operation (addition or multiply) in
the rules are of the same data type.
Thus, we cannot simply analyze the monotonicity characteristic of the array subscript
expression based on simpliﬁcation rules which are not valid when overﬂow occurs.
The
solution would be to add an extra overﬂow test after the regular CR monotonicity test. The
overﬂow test will check the induction variable k which appear in the subscript expression to
see whether there is overﬂow occurs on values of variable k based on typed CR# form of k,
as we described in the earlier part of this section. If we can guarantee that overﬂow does
not occur, then our regular monotonicity analysis result can be used safely. Else, we know
the loop can not be run in parallel due to overﬂow problem. If overﬂow can not determined
at compilation time due to some unknown symbol, the overﬂow test condition together with
70

monotonicity test condition need to be added to guard the parallel execution of the loop
which is shown in Figure 3.3(b).
3.2
Inductive SSA
This section introduces Inductive SSA.
3.2.1
Notation
In Inductive SSA form, each variable deﬁnition x that occurs in a loop has a proof structure:
x 7→Π
where 7→binds x to a collection of proof terms Π in canonical form. Each term in the
collection describes a path-dependent value that is the result of combining partial proofs of
the variable x’s values deﬁned over execution traces that merge at non-loop join points in
the SSA graph.
The structure of a proof term Π is syntactically deﬁned by:
Π
::=
const
constant values
::=
var
SSA variables
::=
Φp
CR# forms
::=
( ⊖Π )
unary operations
::=
( Π ⊕Π )
2-ary operations
::=
[Π, . . . , Π]
choice set (set of Π terms)
::=
∅
empty (note: [] = ∅)
::=
⊥
undeﬁned (unprovable)
Φp
::=
{Π, ⊙, . . . , ⊙, Π}p
CR# form
p
::=
p →bn | bn
decision paths (φ nodes)
bn
::=
n | ∗
block number / wildcard
⊖
::=
−| +
⊕
::=
−| + | ∗| / |<<|>>
⊙
::=
⊕| #
The CR# forms Φp representation is based on CR# algebra and represent the path-
dependent recurrence relations induced by loop-header φ-nodes. The decision path p spans
blocks with φ-nodes that inﬂuenced the value. Choice sets [. . .] represent values that are
dependent on the non-loop header φ-nodes.
Proof structures are lifted and canonicalized to normal form, as discussed below.
71

[. . . , ∅, . . .]
⇒
[. . . , . . .]
ﬂatten
[. . . , [Π1, . . . , Πn], . . .]
⇒
[. . . , Π1, . . . , Πn, . . .]
ﬂatten
{. . . , ⊙, [Π1, . . . , Πn], ⊙, . . .}p
⇒
[{. . . , ⊙, Π1, ⊙, . . .}p, . . . , {. . . , ⊙, Πn, ⊙, . . .}p]
unswitching
⊖[Π1, . . . , Πn]
⇒
[⊖Π1, . . . , ⊖Πn]
⊖promotion
[Π1, . . . , Πn] ⊕[Π′
1, . . . , Π′
m]
⇒
[Π1 ⊕Π′
1] ∪[Π1 ⊕Π′
2] ∪· · · ∪[Π1 ⊕Π′
m]∪
⊕promotion
[Π2 ⊕Π′
1] ∪· · · ∪[Πn ⊕Π′
m]
[. . . , ⊥, . . .]
⇒
⊥
⊥propagation
Figure 3.4: Lifting choice sets in Π terms.
3.2.2
Proof Lifting
During the proof construction process, choice sets are introduced that represent the merging
of partial proofs at join points. Nested choice sets are lifted to ensure that canonical proof
structures only have outermost choice sets.
No nested or inner choice sets remain after
lifting.
Figure 3.4 shows the lifting of Π terms.
Choice sets are ﬂattened and unswitched.
Unswitching ﬂips a choice set embedded inside a value sequence represented by a CR# form
to the outside. It is the semantic equivalence of unswitching conditions in loops. Promotion
of ⊕results in a choice set with ⊕applied to all combinations of pairs of proofs. Duplicates
are removed and “unreachable” pairs that combine terms deﬁned over disjoint decision paths
(i.e. are unreachable) are eﬀectively eliminated by proof canonicalization.
In the worst case, the set contains n2 pairs with n the number of distinct paths through
the loop body. To ensure linear-time constraints of our implementation, we bound the set to
a maximum of 10 proof terms. In practice, we observed that the set never exceeds 5 terms
(see Chapter 4).
3.2.3
Proof Canonicalization
After lifting, proof structures are canonicalized by reduction to normal form.
In this
normalization process, arithmetic operations over constants, variables, and CR# forms are
combined and simpliﬁed using the CR# TRS to produce normal forms (canonicalization).
Theorem 4 A CR# form Φp has a unique normal form, called the CR# normal form, in
the CR#-TRS.
Proof 2 See Section 3.1.3. Furthermore, termination remains unaﬀected.
72

This leads to the following observation.
Corollary 2 A proof structure Π is canonical if it is an ordered choice set of the form
Π = [Π1, . . . , Πn] with no nested or innermost choice sets and each Πi is either an expression
over constants and variables, or a CR normal form.
Proof 3 Any total ordering over Π terms suﬃces to sort the terms [Π1, . . . , Πn]. None of
the Πi terms contain a CR#-TRS redex that can be reduced and none of the Πi terms can
be further reduced by lifting. Hence, each Πi term is a normal form.
The underlying assumption in Corollary 2 is that expressions over constants and variables
are normalized by local constant folding and reassociation using the laws of associativity
and commutativity, while obeying the constraints of exact arithmetic. This requirement is
also used for reassociation in GVN/PRE methods. For example, consider the following SSA
fragment with two distinct paths:
b ←a −1
↙
↘
c ←b + e
d ←e + b
↘
↙
f ←φ(c, d)
Observe that f = c = d = a + e −1 under associativity and commutativity, but c ̸= d
otherwise. Likewise, for this example we obtain a singleton proof structure f 7→[a + e −1]
or a choice f 7→[(a −1) + e, e + (a −1)], respectively.
Deﬁnition 9 A proof structure Π is strongly canonical if it is a singleton. Otherwise, it is
weakly canonical.
Strongly canonical proof structures are more useful because the value of the variable
associated with the proof is deterministic. One of the distinguishing features in our approach
compared to previous work is that we introduce additional rewrite rules to ensure convergence
to singletons whenever possible.
This is achieved by using the CR#-TRS selectively by
matching CR# forms deﬁned over compatible paths. When paths diverge, operations may
become inconsistent. The goal of proof construction is to eliminate inconsistent terms from
the proofs to increase the accuracy.
73

Because no two distinct paths can be taken simultaneously, we introduce path-dependent
reduction rules for 2-ary operators over CR# forms.
After lifting by ⊕promotion,
inconsistent combinations of CR# forms are eliminated by the rewrite contraction:
Φb→p ⊕Φ′
b′→p′ ⇒∅
if b = b′ ∧p ̸= p′
Note that b and b′ are the loop header blocks. This rule states that if two CR# forms Φb→p
and Φ′
b′→p′ are deﬁned for the same loop b = b′ and the decision paths are distinct p ̸= p′
then the ⊕operation is inconsistent.
When CR# simpliﬁcation rules are applied, CR# forms deﬁned for the outer loops are
considered loop-invariant in the inner loops. Therefore, the dominating relationships of the
loop header block numbers b in the path b →p can be used to check invariance of one CR#
form with respect to another. That is, Φb→p is invariant with respect to Φ′
b′→p′ if b ∈Dom(b′).
All remaining variables that are not replaced by Φp forms are loop invariant, because in the
proof construction process all loop-variant variables are replaced by CR# forms in the proof
structures or ⊥(unprovable).
3.2.4
Examples
Several example codes fragments and their corresponding Inductive SSA form are shown
in Figures 3.5 to 3.10. Each variable in SSA form has a unique index for each deﬁnition
point and φ-functions merge the deﬁnitions before a next use. The Inductive SSA form is
augmented with inductive proofs represented by CR# forms for each scalar variable in a
loop nest. The CR# form represents the value progression of that variable. If the variable
progression is a sequence of values from a linear, polynomial, or other discrete function
over the iteration space of the loop, then the CR# algebra can be used to reveal that
function. However, because CR# forms are normal forms for generalized induction variables,
pattern matching on CR# forms is preferable to identify semantically equivalent variables
and expressions.
Figure 3.5(a) shows a simple example code and its corresponding Inductive SSA form in
Figure 3.5(b). The loop has two linear induction variables i and k. Note that i1 and k1 both
have a loop header φ node of the SSA form to merge the initial deﬁnition of the variable
with the feedback update in the loop. We use symbol 7→to map the variable i1 to its CR#
form {1, +, 1} and k1 to {0, +, 2}. For example, based on CR semantics, the CR# form
74

k = 0 h = 1
for i = 1 to n
a[k] = . . .
k = k + 2
h = h ∗2
end for
k0 = 0
h0 = 1
loop1:
B1:
i1 = φ(1, i2) 7→{1, +, 1}B1
k1 = φ(k0, k2) 7→{0, +, 2}B1
h1 = φ(h0, h2) 7→{1, ∗, 2}B1
a[k1] = . . .
i2 = i1 + 1 7→{2, +, 1}B1
k2 = k1 + 2 7→{2, +, 2}B1
h2 = h1 ∗2 7→{2, ∗, 2}B1
endloop1
(a)
(b)
Figure 3.5: Example 1: loop fragment and corresponding Inductive SSA form.
{1, +, 1} represent a linear value sequence of linear induction variable i1 with initial value 1
and updated by adding stride 1 in each iteration. SSA variables i2 and k2 also have their
corresponding linear CR# form as shown in the Inductive SSA form.
Figure 3.6(a) shows an example code with nonlinear induction variable and its corre-
sponding Inductive SSA form in Figure 3.6(b). The variable p1 in Figure 3.6 (b) is a pointer
IV annotated with polynomial CR# form {a, +, i0, +, 1} in Inductive SSA, which represents
a nonlinear value sequence of variable p1.
Figure 3.7(a) shows an example loop with wrap around variable k. The corresponding
CR# form of k1 in the to loop header φ node in Figure 3.7 (b) is {n, #, 1, +, 1}, where #
is the delay operator. The semantics of this form is an initial (unknown) symbolic value n
followed by a linear sequence {1, +, 1} for the remaining loop iterations, which describes the
value progression of wrap around variable k1 concisely and accurately.
Conditional updates of variables require ﬂow-sensitive analysis to determine the behavior
of such variables. Consider the loop shown in Figure 3.8(a). The loop exhibits a conditional
update of variable k. Since the variable k behaves diﬀerently depending whether the branch
is taken or not, it has no single closed-form function that describes it. Therefore, the path-
dependent inductive proofs are annotated in its corresponding Inductive SSA form for each
75

p = a
for i = i0 to n
p = p + i
. . . = ∗p
end for
loop 1:
B1:
i1 = φ(i0,i2) 7→{i0, +, 1}B1
p1 = φ(a,p2) 7→{a, +, i0, +, 1}B1
p2 = p1 + i1 7→{a + i0, +, i0 + 1, +, 1}B1
. . . = ∗p2
i2 = i1 + 1 7→{i0 + 1, +, 1}B1
endloop 1
(a)
(b)
Figure 3.6: Example 2: loop fragment and corresponding Inductive SSA form.
k = n
for i = 1 to n
a[k] = . . .
k = i
end for
k0 = n
loop1:
B1:
i1 = φ(1, i2) 7→{1, +, 1}B1
k1 = φ(k0, i1) 7→{n, #, 1, +, 1}B1
a[k1] = . . .
i2 = i1 + 1 7→{2, +, 1}B1
endloop1
(a)
(b)
Figure 3.7: Example 3: loop fragment and corresponding Inductive SSA form.
SSA variable shown in Figure 3.8(b).
Inductive SSA forms of multi-dimensional loop nests exhibit multivariate CR# forms
for scalar variables and expressions that are dependent on inner and outer loop iterations.
Consider the two-dimensional triangular loop nest shown in Figure 3.9 (a).
From the
Inductive SSA form of Figure 3.9 (b), the initial value of linear variable k4 in the inner
loop is an induction variable k1 from the outer loop. To analyze the outer loop header φ
term k1, the aggregate value of the recurrence updates to k4 in the inner loop need to be
computed. The variables k1, k4 and k3 are mapped to the CR# forms shown in Figure 3.9(b).
As a result, every SSA variable in Inductive SSA has a CR# form to represent their value
76

k = 1
for i = 1 to n
a[k] = . . .
if . . . then
k = k + 3
else
k = k ∗2
endif
end for
k0 = 1
loop1:
B1:
i1 = φ(1, i2) 7→{1, +, 1}B1
k1 = φ(k0, k2) 7→[{1, +, 3}B1→B4→B2, {1, ∗, 2}B1→B4→B3]
a[k1] = . . .
if . . . then
B2:
k3 = k1 + 3 7→[{4, +, 3}B1→B4→B2, {4, +, 1, ∗, 2}B1→B4→B3]
else
B3:
k4 = k1 ∗2 7→[{2, +, 6}B1→B4→B2, {2, ∗, 2}B1→B4→B3]
endif
B4:
k2 = φ(k3, k4) 7→[{4, +, 3}B1→B4→B2, {2, +, 6}B1→B4→B2,
{4, +, 1, ∗, 2}B1→B4→B3, {2, ∗, 2}B1→B4→B3]
i2 = i1 + 1 7→{2, +, 1}B1
endloop1
(a)
(b)
Figure 3.8: Example 4: loop fragment and corresponding Inductive SSA form.
progression behavior in a multi-dimensional loop iteration space.
Figure 3.10 (a) and (c) shows two examples with IV k updated by an unknown quantity.
The diﬀerence between k in Figure 3.10 (a) and Figure 3.10 (c) is that in the former code it
is updates with a nonnegative unknown value. The corresponding Inductive SSA forms are
given in Figures 3.10(b) and (d), where ⊤denotes a nonnegative unknown and ⊥denotes
an unknown quantity. Thus, in Figure 3.10(b), the CR# form of k1 includes ⊤. The CR#
form of k2 is computed based on CR arithmetic rules and has a nonnegative initial value
and step. The use of a lattice enables the recognition of monotonic sequences, such as k2 in
the ﬁrst loop in Figure 3.10 (a), which has the CR# form {⊤, +, ⊤}B1 that is nonnegative
and monotonically increasing. Such properties are important for dependence testing, when
array index expressions are analyzed.
77

k = 0
for i = 1 to n
for j = 1 to i
a[k] = . . .
k = k + 2
end for
end for
k0 = 0
loop1:
B1:
i1 = φ(1, i2) 7→{1, +, 1}B1
k1 = φ(k0, k3) 7→{0, +, 2, +, 2}B1
i2 = i1 + 1 7→{2, +, 1}B1
loop2:
B2:
j1 = φ(j2, 1) 7→{1, +, 1}B2
k4 = φ(k3, k1) 7→{{0, +, 2, +, 2}B1, +, 2}B2
a[k4] = . . .
k3 = k4 + 2 7→{{2, +, 2, +, 2}B1, +, 2}B2
j2 = j1 + 1 7→{2, +, 1}B2
endloop2
endloop1
(a)
(b)
Figure 3.9: Example 5: loop fragment and corresponding Inductive SSA form.
78

k = 0
for i = 1 to n
a[k] = . . .
k = k + rand()
end for
k0 = 0
loop1:
B1:
i1 = φ(1, i2) 7→{1, +, 1}B1
k1 = φ(k0, k2) 7→{0, +, ⊤}B1
a[k1] = . . .
i2 = i1 + 1 7→{2, +, 1}B1
k2 = k1 + rand() 7→{⊤, +, ⊤}B1
endloop1
(a)
(b)
k = 0
for i = 1 to n
a[k] = . . .
k = k + f(i)
end for
k0 = 0
loop1:
B1:
i1 = φ(1, i2) 7→{1, +, 1}B1
k1 = φ(k0, k2) 7→{0, +, ⊥}B1
a[k1] = . . .
i2 = i1 + 1 7→{2, +, 1}B1
k2 = k1 + f(i) 7→{⊥, +, ⊥}B1
endloop1
(c)
(d)
Figure 3.10: Example 6: loop fragments and corresponding Inductive SSA forms.
79

CHAPTER 4
ALGORITHMS
This chapter introduces the Inductive SSA construction algorithm. The algorithm annotates
SSA inductive proofs for SSA variables. The linear complexity of the annotation algorithm
is proven. Then, the compiler optimizations that beneﬁt directly from Inductive SSA are
presented with a discussion on their implementations based on Inductive SSA.
4.1
Inductive SSA Construction
Construction of the Inductive SSA form proceeds in two steps:
• Proof construction: Canonical proofs are constructed for all loop-header variables.
As a side-eﬀect of the local “caching” of partially completed proofs stored in SSA
annotations by this step, also partial proofs are constructed for all other non-loop-
header variables. These partial proofs are not in canonical form.
• Proof propagation: The partial proofs of the non-loop header variables are completed
and canonicalized.
We discuss the two-step algorithm in more detail.
4.1.1
Proof Construction
The outline of the proof construction algorithm is shown in Algorithm 1. The ConstructProof
procedure is applied to each loop-header variable v in the SSA graph, starting with the
innermost loops and proceeding to the outermost loops.
The order of the loop-header
variables is immaterial. When a loop header variable’s proof depends on the proof of another
variable, the proofs are automatically constructed in the correct order. Proof construction
80

Procedure ConstructProof (v ←φb1,...,bk(x1, . . . , xk))
Input: SSA loop header variable v and its φ-node
Result: proof structure annotation v 7→Π in SSA graph
begin
Π := ∅;
h := header block number of v ;
Π1 := Proof (x1, , ) ;
for i = 2 to k do
Πi := Proof (xi, v, h →bi) ;
foreach Πi,j ∈Πi with path p do
if v does not occur in Πi,j then
Π := Π ∪[{Π1, #, Πi,j}p]
else
if Πi,j is variable v then
Π := Π ∪Π1
else
Reassociate Πi,j to isolate v ;
if Πi,j is of the form v ⊙Φp then
Π := Π ∪[{Π1, ⊙, Φp}p]
else
Π := [⊥]
end
end
end
end
end
Canonicalize Π using CR#-TRS ;
Add proof annotation x 7→Π to SSA graph
end
Algorithm 1: Proof construction procedure.
produces proofs for all SSA assignment operations except array operations, which receive ⊥
annotations to represent unknown array values.
Proof construction for loop-header variables is essentially inductive. The process ﬁrst
determines the base value x1 from the ﬁrst φ-node operand, converts it to canonical form Π1,
and then determines the inductive-step of the proof by following the loop cycle to construct
Πi for the other loop-header φ-node operands. The resulting CR forms in Π describe the value
sequence of v. When the proof structure Π is a singleton, the value sequence is deterministic.
There are four tests in the ConstructProof procedure:
• Variable v is “wrap-around” when its value for the next loop iteration xi, i > 1, only
81

Function Proof (x, v, p)
Input: operand x, loop header variable v, and path p
Output: proof structure for x
begin
switch x do
case x is a constant const
return const
case x is the variable v
return v
case x has proof annotation x 7→Π
return Π
case x is a variable deﬁned outside the loop
return {x}p
otherwise return AddProof (x, v, p)
end
end
Function AddProof (x, v, p)
Input: variable x, loop header variable v, path p
Output: proof structure Π for x
begin
switch assignment pattern x ←rhs in SSA do
case x ←const
Π := const
case x ←var
Π := Proof (var, v, p)
case x ←⊖y
Π := Lift ( ⊖Proof (y, v, p) )
case x ←y ⊕z
Π := Lift ( Proof (y, v, p) ⊕Proof (z, v, p) )
case x ←φb1,...,bk(x1, . . . , xk)
if φ is a loop header node then
Π := ConstructProof (x ←φ)
else
Π := S
i=1,...,k Proof(xi, v, p →bi)
end
otherwise Π := ⊥
end
Add proof annotation x 7→Π to SSA graph ;
return Π
end
Algorithm 2: Searching and adding proofs in SSA.
82

depends on other variables’ values. Thus, the proof structure is a wrap-around CR
form {Π1, #, Πi,j}p, where Πi,j ∈Πi and Πi is the proof (a choice set) of xi .
• There is no recurrence relation for v through the execution trace up to the loop cycle,
which means that v is unchanged Πi,j = v. The variable’s proof structure is the initial
loop-invariant Π1.
• There is a recurrence relation for v through the execution trace up to the loop cycle
deﬁned by decision path p.
The variable’s update Φp in its CR form {Π1, ⊙, Φp}p
can be loop invariant or another CR that describes a value sequence deﬁned by other
loop-variant variables.
• Otherwise, a proof cannot be established (⊥).
The proof construction algorithm uses the function Proof deﬁned in Algorithm 2.
The
function returns proofs in non-canonical lifted form and uses locally-cached proofs that are
already bound to variables in the annotated SSA form.
It traverses the SSA graph to
detect the cycles that deﬁne recurrences of loop-invariant variables, which is similar to most
IV detection methods [4, 8]. However, by contrast to these methods, AddProof eﬀectively
analyzes and combines all variable updates through all branches based on the φ-node joint
points.
Figure 4.1 illustrates the construction of inductive proofs for loop φ-nodes in SSA form.
In each example we start with the loop φ-node and complete the proof by traversing the
SSA graph using Proof and AddProof as shown by the successive instantiations of the φ
operands. These small examples demonstrate the systematic and uniform approach to prove
congruences under associativity induced by operation reordering (a) and (b), for detecting
linear equivalences for IV elimination (c), and more complex nonlinear congruences (d).
Example (e) shows the detection of an indirect wrap-around variable, which is useful for
many IV-based methods.
Figure 4.2 illustrates the construction of path-dependent proofs for loop φ-nodes in
SSA form. Instantiations of φ-nodes and renumberings of φ-nodes are used to simplify the
exposition of the internal states of the Proof and AddProof algorithms that traverse the SSA
graph. In this case, the path information is essential and carried through the instantiations
83

B1: b ←φ(a, f)
c ←d + e
f ←b + c
goto B1
b = φ(a, f)
= φ(a, b+c)
= φ(a, b+(d+e))
7→[{a, +, d+e}1]
B2: b ←φ(a, f)
c ←b + d
f ←c + e
goto B2
b = φ(a, f)
= φ(a, c+e)
= φ(a, (b+d)+e)
7→[{a, +, d+e}2]
B3: b ←φ(a, f)
c ←φ(d, e)
e ←c + 1
f ←b + 1
goto B3
b = φ(a, f)
= φ(a, b+1)
7→[{a, +, 1}3]
c = φ(d, e)
= φ(d, c+1)
7→[{d, +, 1}3]
(a)
(b)
(c)
B4: b ←φ(a, f)
c ←φ(d, e)
e ←b + c
f ←b << 1
goto B4
b = φ(a, f)
= φ(a, b<<1)
7→[{a, ∗, 2}4]
c = φ(d, e)
= φ(d, b+c)
= φ(d, [{a, ∗, 2}4]+c)
7→[{d, +, a, ∗, 2}4]
B5: b ←φ(a, f)
c ←φ(d, e)
e ←b + c
f ←1
goto B5
b = φ(a, f)
= φ(a, 1)
7→[{a, #, 1}5]
c = φ(d, e)
= φ(d, b+c)
7→[{d, +, a, #, 1}5]
7→[{d, #, a+d, +, 1}5]
(d)
(e)
Figure 4.1: Example inductive proofs for loop φ-nodes in SSA form. The proofs for (a) and
(b) show the equivalence of b under associativity induced by operation reordering between (a)
and (b). Example (c) has a linear equivalence b−a ≡c−d detected by proofs b = {a, +, 1}3
and c = {d, +, 1}3. Example (d) has a nonlinear equivalence b−a ≡c−d detected by proofs
b = {a, ∗, 2}4 ≡{a, +, a, ∗, 2}4 and c = {d, +, a, ∗, 2}4. Example (e) has a wrap-around
induction variable b that indirectly induces wrap-around behavior in variable c.
84

B1: b ←φ0,4(a, f)
↙
↘
B2: c ←b + d
B3: e ←d + b
↘
↙
B4: f ←φ2,3(c, e)
goto B1
b = φ0,4(a, f)
= φ0,4(a, φ2,3(c, e))
= φ0,4(a, φ2,3(b+d, d+b))
= [φ0,4,2(a, b+d), φ0,4,3(a, d+b)]
7→[{a, +, d}1→4→2, {a, +, d}1→4→3]
7→[{a, +, d}1→∗]
B5: h ←φ0,8(3, g)
↙
↘
B6: a ←1
B7: b ←2
c ←2
d ←1
↘
↙
B8: e ←φ6,7(a, b)
f ←φ6,7(c, d)
g ←e + f
goto B5
h = φ0,8(3, g)
= φ0,8(3, e + f)
= φ0,8(3, φ6,7(a, b) + φ6,7(c, d))
= φ0,8(3, φ6,7(1, 2) + φ6,7(2, 1))
= φ0,8(3, [15→8→6, 25→8→7]+[15→8→7, 25→8→6])
= φ0,8(3, [35→∗])
7→[{3, #, 3}5→∗]
7→[3]
(a)
(b)
B9: c ←φ0,12(a, g)
d ←φ0,12(b, h)
↙
↘
B10: e ←c + 1
B11: i ←c −d
f ←d + 1
↘
↙
B12: g ←φ10,11(e, c)
h ←φ10,11(f, d)
goto B9
c = φ0,12(a, g)
= φ0,12(a, φ10,11(e, c))
= φ0,12(a, φ10,11(c + 1, c))
= [φ0,12,10(a, c + 1), φ0,12,11(a, c))]
7→[{a, +, 1}9→12→10, {a}9→12→11]
d = φ0,12(b, h)
= [φ0,12,10(b, d + 1), φ0,12,11(b, d))]
7→[{b, +, 1}9→12→10, {b}9→12→11]
(c)
Figure 4.2:
Example path-dependent inductive proofs for loop φ-nodes in SSA form.
Example (a) has congruent operations under commutativity in blocks B2 and B3. Example
(b) shows that h is constant by path correlation in proof construction and #-wrap-around
simpliﬁcation of the CR form. Example (c) has a loop invariant computation for i, because
of the congruence i ≡c −d ≡a −b and a −b is loop invariant. This is proven by i ≡c −d ≡
[{a, +, 1}9→12→10 −{b, +, 1}9→12→10, {a}9→12→11 −{b}9→12→11] ≡[a −b, a −b] ≡[a −b].
85

in the proofs. Note that AddProof uniﬁes the proofs over φ-nodes using path extensions
p →bi to include block bi that each φ-node operand referenced.
4.1.2
Proof Propagation
After proof construction, the proofs of the non-loop header variables are completed by
propagating the proof structures of the loop-header variables to all the variable’s uses in the
partial proofs. Each partial proof is then lifted and canonicalized. The proof propagation
procedure is deﬁned in Algorithm 3 and is applied depth-ﬁrst from bottom to the top (loop
header).
For example, consider Figure 4.1(a). After proof construction, we obtain a canonical
proof for b and partial proofs for c and f:
b
7→
[{a, +, d+e}1]
c
7→
d+e
f
7→
b+(d+e)
Proof propagation results in the complete canonical proofs by using the CR#-TRS reduction
b+(d+e) = {a, +, d+e}1 + (d+e) ⇒{a+(d+e), +, d+e}1:
b
7→
[{a, +, d+e}1]
c
7→
[d+e]
f
7→
[{a+(d+e), +, d+e}1]
Note that c is loop invariant and b is a linear IV with initial value a and stride d+e.
4.1.3
Asymptotic Complexity
The proof construction and propagation algorithms take linear time in the size of the SSA
graph. Each node and each edge in the SSA graph is visited at most twice, since SSA nodes
are marked by (partial) proofs in the construction step and modiﬁed in the propagation step.
Not surprisingly, the complexity corresponds to the construction of the strongly connected
components (SCCs) of a digraph using a depth-ﬁrst search.
The critical cost is ⊕promotion (Figure 3.4) resulting in a choice set that contains all
combinations of Π-pairs. Each choice corresponds to a decision path that aﬀects the value of
86

Procedure PropagateProof (x, h)
Input: non-loop header variable x, header block number h
Result: proof structure annotation for x in SSA
begin
if x has SSA annotation x 7→Π then
Replace each loop-header variable v in Π by the proof of v and lift the
resulting term
else
Π := Proof (x, , h)
end
Canonicalize Π using CR#-TRS ;
Update SSA annotation x 7→Π
end
Algorithm 3: Proof propagation procedure.
a variable, which can be exponential in the worst case. By limiting the size of the choice set
to a ﬁxed constant length and by using ⊥when the size is exceeded, the overall asymptotic
running time is linear in the number of edges O(|E| + |V |) in the SSA graph (same as
computing SCCs). In practice, the algorithm is linear in the size of the SSA code region,
since the number of edges is practically bounded in realistic codes. Furthermore, experiments
show that the size of the choice set does not exceed 5 terms in practice (see Chapter 6).
4.2
Compiler Analysis and Optimization with
Inductive SSA
With semantic information readily provided in the inductive proofs for SSA variables,
Inductive SSA provides a foundation that uniﬁes compiler analysis. Several compiler analysis
problems can be replaced with Inductive SSA, such as IV recognition and IV classiﬁcation,
and constant propagation. Optimizations that beneﬁt from Inductive SSA are IV removal,
IV substitution, strength reduction, loop-invariant code motion, common sub-expression
elimination, partial redundancy elimination.
4.2.1
Loop-Variant Variable Classiﬁcation
Inductive SSA generalizes loop-variant variable analysis and the following classes of loop-
variant variables are recognized and classiﬁed based on attached CR form in Inductive SSA:
Linear induction variables are represented by nested CR forms {a, +, s}i, where a is the
87

integer-valued initial value and s is the integer-valued stride in the direction of i. The
coeﬃcient a can be a nested CR form in another loop dimension. Linear IVs are the
most common IV category. For example, the variables i1 and k1 in Figure 3.5 are linear
IVs with linear CR form {1, +, 1} and {0, +, 2} separately.
Polynomial induction variables are represented by nested CR forms of length k, where k is
the order of the polynomial. All ⊙operations in the CR form are additions, i.e. ⊙= +.
For example, the variable p1 in Figure 3.6 (b) is pointer IV with polynomial CR form
{a, +, i0, +, 1}. The variable k1 in Figure 3.9 (b) with mapped polynomial CR form
{0, +, 2, +, 2} is also a polynomial variable.
Geometric induction variables are represented by the CR form {a, ∗, r}i, where a and r are
loop invariant. For example, the variable h1 in Figure 3.5(b) are Geometric induction
variable with CR form {1, ∗, 2}.
Mix induction variables have CR forms that contain both ⊙= + and ∗. For example, the
variable k3 in Figure 3.8(b) contains mix CR form {4, +, 1, ∗, 2}.
Out-of-sequence (OSV) variables are variables which contains out-of-sequence values,
which include wrap-around variables. They are represented by (a set of) CR forms
{a, #, s}i, where a is the initial out-of-sequence value and s is a nested CR form. For
example, the variable k1 in Figure 3.7 (b) is a wrap around variable with CR form
{n, #, 1, +, 1}.
Cyclic induction variables have cyclic dependence between the recurrence relations of
variables. For example:
a = 1, b = 0
a1 = φ(1, a2)
7→< a1, (1, b1) >
for (. . . )
b1 = φ(0, b2)
7→< b1, (0, a1) >
t = b
⇒
b2 = a1
b = a
a2 = b1
a = t
In some cases cyclic IVs can be represented by geometric sequences [4, 12], but
most cyclic forms represent special functions (e.g. the Fibonacci sequence is such
an example).
Some cyclic forms can be degenerated into monotonic sequences, by
replacing a variable’s update with an unknown [70].
88

Conditional induction variables are represented by multiple CR forms represent path-
dependent value progressions, as variable k1 in Figure 3.8(b).
Unknown variables have unknown initial values or unknown update values, for example, the
variable k1 in Figure 3.10 (b) and (d). These unknown are typically function returns,
updates with (unbounded) symbolic variables, or bit-operator recurrences. Some of
these are identiﬁed as monotonic. For example, an IV k1 in Figure 3.10 (b) with initial
value 0 and a “random” positive stride function has a CR {0, +, ⊤}, where the stride
is represented by the lattice value ⊤.
4.2.2
Induction Variable Removal and Substitution
Induction Variable eliminate useless induction variables in a loop if the loop contains two or
more induction variables. Since the CR form in Inductive SSA captures the value sequences
of the variable, it is easy to compare the value sequences of two induction variables to help
the induction variable elimination. Compared to the traditional IV removal methods [11, 1]
which only compare linear induction variables to eliminate useless induction variable, IV
removal in Inductive SSA is not only a simpler method to compare two linear induction
variable based on CR forms, but also brings greater coverage by comparing nonlinear
variables.
Algorithm 4 shows the comparison of two induction variable to eliminate the useless
variable in Inductive SSA.
Procedure IV-Difference (v1, v2)
Input: program in Inductive SSA form, variables v1 and v2
Result: a constant, or a symbolic variable, or a CR# form
begin
Get cached CR form c1 from Inductive SSA for variable v1 ;
Get cached CR form c2 from Inductive SSA for variable v2 ;
diﬀ:= c1 −c2 ;
Simplify diﬀusing the CR# algebra TRS ;
return diﬀ
end
Algorithm 4: Algorithm to compare a pair of IVs for IV removal in Inductive SSA.
Comparison of two induction variables can be done by comparing their attached CR
form in Inductive SSA. If IV-Compare returns 0, the two induction variable v1 and v2 have
89

loop L1
B1:
i1 = φ(0, i2) 7→{0, +, 1}B1
j1 = φ(0, j2) 7→{0, +, 1}B1
k1 = φ(0, k2) 7→{0, +, 0, +, 1}B1
l1 = φ(a, l2) 7→{a, +, 0, +, 1}B1
. . . = i1 . . .
. . . = k1 . . .
. . . = j1 . . .
. . . = l1 . . .
. . .
k2 = k1 + i1 7→{0, +, 1, +, 1}B1
l2 = l1 + j1 7→{a, +, 1, +, 1}B1
i2 = i1 + 1 7→{1, +, 1}B1
j2 = j1 + 1 7→{1, +, 1}B1
endloop L1
loop L1
B1:
i1 = φ(0, i2) 7→{0, +, 1}B1
k1 = φ(0, k2) 7→{0, +, 0, +, 1}B1
. . . = i1 . . .
. . . = k1 . . .
. . . = i1 . . .
. . . = k1 + a . . .
. . .
k2 = k1 + i1 7→{0, +, 1, +, 1}B1
i2 = i1 + 1 7→{1, +, 1}B1
endloop L1
(a)
(b)
Figure 4.3: Result of IV removal (b) from code fragment (a) with Inductive SSA.
the exactly same value sequences. Thus, one of them can be eliminated and all the use of
that eliminated variable need to be updated with the remaining variable. If IV-Compare
returns a constant or a symbolic variable which is not a CR form, say a, then the value
sequence of v2 + a is always equal to the value sequence of variable v1. If v1 is determined to
be eliminated, then any use of v1 need to be replaced with v2 + a. If IV-Compare returns
a CR form which is not a constant or a symbolic variable, the IV removal is generally not
worth doing, since it may introduce another new induction variable in the loop.
An example in Figure 4.3 shows how induction variable comparison in Inductive SSA
help to eliminate useless IV. By comparing i1 and j1 with IV-Compare, the returned 0
shows the same value sequence of these two variable. Therefore, j1, together with j2 can be
eliminated and the use of j1 is replaced with i1. By comparing nonlinear induction variable
k1 and l1, a symbolic variable a is returned. Therefore, any use of l1 can be replaced with
k1 + a and variable l1 together with l2 can be eliminated. The classic IV removal will fail to
eliminate variable l.
Induction Variable Substitution (IVS) replaces induction variable update with its closed-
90

loop L1:
B1:
i1 = φ(0, i2) 7→{0, +, 1}B1
j1 = φ(1, i2) 7→{1, +, 2}B1
k1 = φ(0, k2) 7→{0, +, 1, +, 2}B1
k2 = k1 + j1 7→{1, +, 3, +, 2}B1
a[k2] = . . .
j2 = j1 + 2 7→{3, +, 2}B1
i2 = i1 + 1 7→{1, +, 1}B1
. . .
endloop L1
loop L1:
B1:
i1 = φ(0, i2) 7→{0, +, 1}B1
k2 = i2 + 2 ∗i + 1 7→{1, +, 3, +, 2}B1
a[k2] = . . .
i2 = i1 + 1 7→{1, +, 1}B1
. . .
endloop L1
(a)
(b)
Figure 4.4: Result of IVS (b) on example code fragment (a) with Inductive SSA
form expressions involving loop index variables. IVS is often applied to eliminated the cross-
iteration dependencies of induction variable updates and to further enable the parallelization
of a loop nest.
IVS is a simple application for Inductive SSA since CR inverse rules (CR−1) in CR algebra
can automatically translate CR expression back to their equivalent closed-form. Consider
the example in Figure 4.4. The variable k is a nonlinear induction variable. Due to the
loop-carried scalar data dependence on variable k and j, the loop can not be parallelized.
After IVS with Inductive SSA, the CR form for variable k1 was translated to its closed form
function i2+2∗i+1 by using CR inverse rules. Then the k’s update in the loop was replaced
with a closed-form function of loop index variable i in Figure 4.4(b). Now the loop-carried
dependence was removed and the loop can be parallelized.
4.2.3
Strength Reduction
Strength reduction is a very important optimization which is often applied as part of array
elements address translation to beneﬁts loop nests with array indexed by induction variables.
As we discussed in the related work of strength reduction in Chapter 2, the candidate
expression for these classical strength reduction methods is often limited to linear induction
expressions.
Compared with these classic methods, strength reduction is a simple application for
91

Inductive SSA and do not have the limitations which the classic SR methods have. Loop
strength reduction is very straight forward to implement with CR forms because of the
CR characteristic that deﬁnes the recurrences according to the loop template for CRs (CR
semantics). In [50] it was suggested that CR analysis method generalizes strength reduction
of polynomials, exponentials, logarithms, trigonometric functions, and factorials. Strength
reduction replaces expensive expressions in a loop by recurrences. For each recurrence, we
can expand the recurrence into a sequence of induction variable updates in the loop body
using the CR semantically equivalent template. Thus, both linear and non-linear arithmetic
operations can be replaced with cheaper addition operations, which the classic strength
reduction approaches failed to handle.
A CR form Φi = {ϕ0, ⊙1, ϕ1, ⊙2, · · · , ⊙k, ϕk}i is deﬁned by the following loop template
which represents the semantics of a CR form:
cr0
= ϕ0
cr1
= ϕ1
:
= :
crk−1 = ϕk−1
for i = 0 to n−1
val[i] = cr0
cr0
= cr0
⊙1 cr1
cr1
= cr1
⊙2 cr2
:
= :
:
:
crk−1 = crk−1 ⊙k ϕk
end for
In Inductive SSA, every scalar lhs SSA variable is mapped with a CR form. So, both linear
and non-linear arithmetic with multiply operations can be replaced with cheaper addition
operations by strength reduction with CRs based on CR loop template.
Consider for example shown in Figure 4.5(a) and the Inductive SSA form shown in
Figure 4.5 (b). CR simpliﬁcation rules are used to simplify the CR form for variable f and
the simpliﬁed CR form is shown in Figure 4.5(c). Based on CR semantics, there are two
induction variables x and y which represented by the CR form of variable f. The initial value
for x is the ﬁrst coeﬃcient of the new CR form, the step for variable x is another induction
variable y which has initial value from the second coeﬃcient of the new CR form, and step
is 2ab. Thus, based on the CR loop template, we can expand the CR form for variable f
into a sequence of induction variable updates, which is shown in the Figure 4.5(d). The
92

i = i0
j = j0
for (k = 0 to n)
f = i × j
i = i + a
j = j + b
end for
loop L1:
B1:
i1 = φ(i0, i2) 7→{i0, +, a}B1
j1 = φ(j0, j2) 7→{j0, +, b}B1
. . .
f = i1 × j1 7→{i0j0, +, i0b + j0a + ab, +, 2ab}B1
i2 = i1 + a 7→{i0 + a, +, a}B1
j2 = j1 + b 7→{j0 + b, +, b}B1
endloop L1
(a)
(b)
f(k) = i1 × j1
= {i0, +, a} × {j0, +, b}
= {i0j0, +, i0b + j0a + ab, +, 2ab}
x = i0 × j0
y = i0 × b + j0 × a + a × b
for (k = 0 to n)
f = x
x = x + y
y = y + 2ab
end for
(c)
(d)
Figure 4.5: Result of SR (d) on example code fragment (a) in Inductive SSA form (b) using
the CR# form coeﬃcients (c) as IVs in (d).
computation of i × j in the right hand side of the ﬁrst instruction in the loop is replaced
by a sequence of addition operation of induction variables. As we can see, the non-linear
arithmetic i × j is replaced with cheaper add operations. The induction variable i and j is
useless and can be eliminated.
When the loop is multi dimensional, we can still perform SR based on the same idea.
However, we need to start from the inner most loops, and multivariate CR forms are
considered.
Our strength reduction in Inductive SSA do not have the limitations which the classic SR
methods have. Furthermore, strength reduction in Inductive SSA may achieve higher ILP
for a program. The sequence of induction variable updates is based on CR loop template in
which there are only anti dependence between statements in each iteration of the loop and
can be executed in parallel.
93

for i = 0 to n
s1
j = i × i
s2
k = 2 × i
s3
x = j + k
. . .
end for
loop L1
B1:
i1 = φ(0, i2) 7→{0, +, 1}B1
j1 = i1 × i1 7→{0, +, 1, +, 2}B1
k1 = 2 × i 7→{0, +, 2}B1
x1 = j1 + k1 7→{0, +, 3, +, 2}B1
. . .
i2 = i1 + 1 7→{1, +, 1}B1
endloop L1
u = 0 v = 3
for i = 0 to n
s1
x = u
s2
u = u + v
s3
v = v + 2
. . .
end for
(a)
(b)
(c)
Figure 4.6: Eﬀect of SR on ILP (c) for example code fragment (a) in Inductive SSA (b).
The example loop in Figure 4.6(a) is transformed to the code in Figure 4.6(c) after
strength reduction in Inductive SSA. The instruction level parallelism (ILP) is higher in
Figure 4.6 (c) than the ILP in Figure 4.6(a). Since there is ﬂow dependence between s1 and
s3, s2 and s3, the result in s3 depend on the result of s1 and s2. Thus the instruction s3 cannot
be executed in parallel with s1 and s2. Instruction s3 may be scheduled by a compiler in a
subsequent VLIW instruction. After strength reduction with the CR approach, in contrast
to the code in Figure 4.6(a), there are only anti dependences between instructions s1, s2 and
s3 in each iteration of the loop in Figure 4.6 (c). Since all these three operations can run in
parallel, they can be arranged in one VLIW instructions and the ILP is higher compared to
the ILP in Figure 4.6(a).
4.2.4
Constant Propagation
Constant propagation [2, 1] is an optimization frequently applied in compilers. Constants
assigned to a variable can be propagated through the ﬂow graph and substituted at the use
of the variable at compile time.
Sparse conditional constant propagation (SCCP) [2] are eﬃciently implemented in SSA.
Inductive SSA provides an alternative improved method for SCCP. The asymptotic time
complexity of SCCP and Inductive SSA is the same.
CR methods for Inductive SSA
automatically propagate the constant in the loop during the CR form construction. Since CR
form are simpliﬁed during the construction, constant folding is also applied at the same time.
94

B1: a1 ←3
d1 ←2
B2: d3 ←φ(d1, d2)
7→[2]
(by {2, #, 2}2→5 ⇒2)
a3 ←φ(a1, a2)
7→[3]
(by {3, #, 5 −2}2 ⇒3)
f1 ←a3 + d3
7→[5]
g1 ←5
7→[5]
a2 ←g1 −d3
7→[3]
t1 ←f1 −g1
7→[0]
↙
↘
B3: f2 ←g1 + 1 7→[6]
B4: t2 ←g1 −a2 7→[2]
↘
↙
↘
B5: f3 ←φ(f1, f2)
7→[52→5→4, 62→5→3]
d2 ←2
7→[2]
goto B2
Figure 4.7:
Inductive SSA applied to an example taken from [1, p.368].
The SCCP
algorithm [2] marks the branch to B4 dead, because t1 ≤0 holds, and thus determines
that f3 = 6. Marking B4 dead reﬁnes f3 7→[6] in Inductive SSA.
To obtain a sparse version of Inductive SSA, just marking the proofs of dead branches suﬃces.
Decision-path information is already carried in the proofs. Proof terms for unreachable paths
are then eliminated.
Consider the example from [1, p.368] shown in Inductive SSA form in Figure 4.7.
The CR#-TRS reduces the wrap-around forms d3 7→{2, #, 2}2→5 ⇒2 and a3 7→
{3, #, g1−d3}2 = {3, #, 5−2}2 ⇒3 in the proof construction process.
By marking the
dead branch B4 as unreachable, f3 is simpliﬁed to f3 7→[6].
The advantage of Inductive SSA is that it propagates constants in certain cases that
SCCP cannot. CP and SCCP methods are based on lexical and structural SSA information.
Therefore, they can not recognize semantic constant in loops and only the static constants
in the program are recognized and propagated.
In Inductive SSA additional constants
with a semantic origin are automatically derived from CR#-TRS rewrites and algebraic
cancellations, for example when two linear induction variables are compared and the loop-
invariant diﬀerence yields a constant.
Therefore, semantic constant can be detected by
simply checking CR result in Inductive SSA. Algorithm 5 shows how to detect semantic
constant for a given expression via CR result in Inductive SSA. If the length of the CR
95

Procedure Constant-P (v)
Input: program in Inductive SSA form, variable v
Result: true if v is a constant, false otherwise
begin
Get cached CR form c from Inductive SSA for variable v ;
if length of c is 1 and the ﬁrst coeﬃcient of c is constant then
return true
else
return false
end
end
Algorithm 5: Algorithm to detect constants for CP in Inductive SSA.
loop:
B1:
i1 = φ(1, i2) 7→{1, +, 2}B1
j1 = φ(2, j2) 7→{2, +, 2}B1
i2 = i1 + 2 7→{3, +, 2}B1
j2 = j1 + 2 7→{4, +, 2}B1
k1 = j2 −i2 7→{1}B1
t1 = k1 + 1 7→{2}B1
. . .
endloop
loop:
B1:
i1 = φ(1, i2) 7→{1, +, 2}B1
j1 = φ(2, j2) 7→{2, +, 2}B1
i2 = i1 + 2 7→{3, +, 2}B1
j2 = j1 + 2 7→{4, +, 2}B1
k1 = 1 7→{1}B1
t1 = 2 7→{2}B1
. . .
endloop
(a)
(b)
Figure 4.8: Result of CP (b) on an example code fragment (a) with Inductive SSA.
form for this expression is 1 and the ﬁrst coeﬃcient of this CR form is constant, then this
expression represent a constant.
An example in Figure 4.8 shows how constant propagation are performed automatically
in Inductive SSA. The CR form for variable k1 reveals that value for k1 remain constant
during each iteration. The code after CP is shown in the Figure 4.8 (b). The variable t1 also
turns out to be a constant based on mapped CR form.
Note that current constant propagation techniques, such as SCCP only take structural
information on the SSA form into account by inspecting statements that are constant
assignments. Therefore, these techniques do not optimize k1 and t1 to a constant.
96

4.2.5
Loop-Invariant Code Motion
Loop-Invariant Code Motion (LICM) [1] moves computations that are unchanged in the
loop out of the loop, thus eliminating redundant computation in every iteration of the loop.
The classic algorithm that identify loop invariant [1] rely on the structural information in
the program to identify loop invariant by checking if the reaching deﬁnition of the variable
is outside the loop or is itself the loop invariant. Compared to loop-invariant detection in
Inductive SSA, classic LICM failed to consider the semantic value information of the variable
and missed semantic loop invariant expressions.
Procedure Loop-Invariant-P (v)
Input: program in Inductive SSA form, variable v
Result: true if v is a loop invariant, false otherwise
begin
Get cached CR form c from Inductive SSA for variable v ;
if length of c is equal to 1 then
return true
else
return false
end
end
Algorithm 6: Algorithm to detect loop-invarant variables for LICM in Inductive SSA.
The algorithm to detect loop invariant for a given expression via CR result in Inductive
SSA is shown in the Algorithm 6. In Inductive SSA, the CRs that are simpliﬁed to loop-
invariant expressions are the candidates for loop invariant expression removal. If the length of
a CR form is 1, which means it does not have an update function and only has an initial value
as the only coeﬃcient in CR forms (initial value can be a constant or symbolic expressions),
the variable of this CR form represents a loop invariant. Compared to classic Loop Invariant
Removal, LICM with Inductive SSA recognizes semantic loop invariant expressions.
The example in Figure 4.9 shows semantic loop invariant is detected with Inductive SSA.
A non-trivial example is variable u1 in Figure 4.9(a), which is not obvious that expression
i1 ∗3 −y1 is loop invariant. The classic LICM failed to recognize the loop invariant variable
u1 because the expression which update u1 contains loop variant variable i1 and y1.
In Inductive SSA, the CR form {i0 ∗2 −4}B1 computed for variable u1, where i0 are the
initial value of variable i at the start of the loop, reveals that the value of variable u1 do not
97

loop:
B1:
i1 = φ(i0, i2) 7→{i0, +, 1}B1
t1 = φ(i0, t2) 7→{i0, +, 3}B1
x1 = t1 + 1 7→{i0 + 1, +, 3}B1
y1 = x1 + 3 7→{i0 + 4, +, 3}B1
t2 = t1 + 3 7→{i0 + 3, +, 3}B1
u1 = i1 ∗3 −y1 7→{i0 ∗2 −4}B1
i2 = i1 + 1 7→{i0 + 1, +, 1}B1
. . .
endloop
u1 = i0 ∗2 −4
loop:
B1:
i1 = φ(i0, i2) 7→{i0, +, 1}B1
i2 = i1 + 1 7→{i0 + 1, +, 1}B1
. . .
endloop
(a)
(b)
Figure 4.9: Result of LICM (b) on an example code fragment with Inductive SSA.
change in the loop. Therefore, the assignment to variable u1 can be moved out of the loop
with simpliﬁed expression for u1, which is u1’s CR result i0 ∗2−4 as shown in Figure 4.9(b).
4.2.6
Common Sub-expression Elimination and Partial Redun-
dancy Elimination
CSE and PRE are both compiler optimizations that rely on proving the equivalence of
expressions. They are two most important optimizations of redundancy elimination. As
we discussed in the Section 2.3.6, Muchnick [1] presents both local CSE and global CSE
algorithms with only recognizing lexically equivalent expressions as candidate of CSE. Several
classic PRE algorithms [27, 26, 77, 74] only recognize lexically equivalent expressions. This
problem for classic PRE was noted by several authors in [25, 80, 81, 82, 31]. Their algorithms
are able to ﬁnd more expression equivalence than classic PRE either by exploiting some
ad-hoc algebraic rules, reassociating, renaming, ranking, sorting of expressions to ﬁnd out
more redundant computation, or by integrating GVN and PRE to eliminate redundancies
that missed by either work. A state-of-the-art SSA-based formulation of GVN-PRE [31]
subsumes GVN and PRE and eliminates redundancies missed by previous work.
In contrast, CSE and PRE in Inductive SSA framework captures the real value progres-
sion behavior of the variables, works in a more uniﬁed way and recognize more semantic
redundancies that these approaches failed. Inductive SSA supports and signiﬁcantly en-
98

hances GVN-PRE by the systematic detection of a wider range of congruence relations by
exploiting the properties of loop-variant variables at the loop level.
These equivalences
include the syntactically diﬀerent but semantically identical IV expressions and semantic
forms of loop invariants, which are not removed by the original GVN-PRE method, see our
motivating examples in Figure 1.1, Figure 1.5 and Figure 1.6 in Chapter 1.
Procedure Equivalent-P (v1, v2)
Input: program in Inductive SSA form, variables v1 and v2
Result: true if v1 and v2 are equivalent, false otherwise
begin
Get cashed Inductive proof c1 for variable v1 from Inductive SSA ;
Get cashed Inductive proof c2 for variable v2 from Inductive SSA ;
if v1 ≡v2 or c1 ≡c2 then
return true
else
return false
end
end
Algorithm 7: Algorithm to detect equivalent expressions for CSE and PRE in Inductive
SSA.
Algorithm 7 shows the procedure to detect equivalent expressions in Inductive SSA
for CSE and PRE. Matching of equivalent expressions can be done by matching of their
corresponding CR forms in Inductive SSA. If two CR forms of the given expressions are
totally matched, including matched index variable, length, coeﬃcients and operators of two
CR forms, the given two expressions are equivalent despite the lexical diﬀerence. With the
semantic matching of expressions, CSE and PRE will see a better coverage. Not shown is
a hash-based implementation that uses hash tables to reduce the matching cost. The CR#
forms and inductive proofs should be hashed to save space and matching time.
Consider the example in Figure 4.10(a).
Although the expression of t3 and t5 are
syntactically diﬀerent, the same CR form of variables t3 and t5 in Inductive SSA reveals
their value sequences are equivalent. Therefore, the redundant computation is eliminated,
as shown in Figure 4.10(b).
The classic PRE methods failed to recognize the semantic
equivalence of t3 and t5.
99

loop:
B1:
i1 = φ(0, i2) 7→{0, +, 1}B1
if . . .
t1 = a ∗i1 7→{0, +, a}B1
t2 = b ∗i1 7→{0, +, b}B1
t3 = t1 + t2 7→{0, +, a + b}B1
endif
t4 = a + b 7→{a + b}B1
t5 = t4 ∗i 7→{0, +, a + b}B1
i2 = i1 + 1 7→{1, +, 1}B1
. . .
endloop
loop:
B1:
i1 = φ(0, i2) 7→{0, +, 1}B1
t0 = a ∗i1 7→{0, +, a}B1
t1 = b ∗i1 + t0 7→{0, +, a + b}B1
if . . .
t3 = t1 7→{0, +, a + b}B1
endif
t5 = t1 7→{0, +, a + b}B1
i2 = i1 + 1 7→{1, +, 1}B1
. . .
endloop
(a)
(b)
Figure 4.10: Result of PRE transformation (b) on an example code fragment (a) with
Inductive SSA.
100

CHAPTER 5
IMPLEMENTATION
This chapter presents our implementation of Inductive SSA and the enhanced optimizations
in the GCC 4.x compiler environment.
5.1
GNU Compiler Collection (GCC)
We implemented the Inductive SSA extension and enhanced the GVN-PRE algorithm with
Inductive SSA-based CP, LICM and congruence detection on top of the Tree SSA [94] in
GCC 4.1.
5.1.1
Introduction
The GNU Compiler Collection (GCC) [95] is a open-source and free compiler distributed
under the GNU General Public License. GCC is used as the system compiler for GNU/Linux
and many other operating systems. GCC is a compiler for many popular platforms and
provide front-ends for numerous computer languages, such as C, C++, and Java.
As of today, GCC is the most popular compiler toolchain available. In the last few years,
GCC has undergone a major transition from GCC version 3 to version 4. GCC Version after
4.0 includes numerous state-of-the-art optimizations and functional improvements over its
predecessor and still in very active development. For these reasons, we chose GCC as our
experimental compiler to implement our new compiler analysis framework and to compare
the eﬀectiveness of our algorithm with the state-of-the-art optimizations implemented in the
latest version of GCC.
101

 
 
C 
Trees
C++ 
Trees
Java 
Trees
C 
Generizer
C++ 
Generizer
Java 
Generizer
GENERIC
Gimplifier
GIMPLE
Front End
 SSA
SSA pass 1
 un  SSA
RTL
Back End
...
...
SSA pass N
Figure 5.1: GCC intermediate representation integration schematic.
5.1.2
Intermediate Representation in GCC
RTL
In the older version of GCC, code improving transformations are performed on an interme-
diate representation called Register Transfer Language (RTL) [96], an architecture indepen-
dent, lisp-like assembly language. Parse tree generated by the front end are immediately
converted into RTL and passed on to the optimizer. RTL works well for optimizations that
are close to the target [94]. Unfortunately, for many code transformations, RTL is not a
suitable and eﬀective representation because it is too close to the actual machine language,
which hinders many high-level optimizations performed by modern compilers. It has become
more and more obvious that a new, high-level intermediate representation needs to be added
to GCC.
102

GENERIC and GIMPLE
To address the need for high-level intermediate representation in GCC, two new IR were
introduced: GENERIC and GIMPLE [97] with the release of GCC 4.0.
The purpose of GENERIC is simply to provide a language-independent way of repre-
senting an entire function in trees. All the language front end emit GENERIC IR. Next,
this tree is lowered to GIMPLE form. GIMPLE is a three-address representation derived
from GENERIC by breaking down GENERIC expressions into tuples of no more than
3 operands. Temporaries are introduced to hold intermediate values needed to compute
complex expressions. Additionally, all the control structures used in GENERIC are lowered
into conditional jumps.
The compiler pass which converts GENERIC into GIMPLE is
referred to as the gimpliﬁer. The gimpliﬁer works recursively, generating GIMPLE tuples
out of the original GENERIC expressions.
Tree SSA
Tree SSA [94] is a new optimization framework based on SSA form that operates on GCC’s
tree representation. The goal of the Tree SSA design is to build a completely machine-
independent optimization framework based on the SSA form.
SSA is an intermediate
representation (IR) that is becoming increasingly popular because it allows eﬃcient imple-
mentations of data ﬂow analysis and optimizing transformations. Converting the program
into Tree SSA form provide a basis for implementing many high-level optimizations and
analyses that are either diﬃcult or impossible to implement in RTL.
The integration of GCC’s new intermediate representation are shown in Figure 5.1.
5.2
Implementation of Inductive SSA Extension
We implemented the Inductive SSA extension on top of the Tree SSA [94] in GCC 4.1.
5.2.1
CR Library Implementation
The inductive proof in Inductive SSA are constructed based on CR# algebra. To construct
inductive proofs for each SSA variable, a CR-Analysis infrastructure with CR#-TRS is
needed. To implement an eﬃcient and extendable CR framework, it is preferred to avoid
reliance on a speciﬁc compiler infrastructure.
A CR analysis library that is a modular
103

independent library, called CRLIB and written in C, is developed. The framework captures
the expressiveness of the CR-form, implements the CR-algebra rules for simpliﬁcation and
inverse ﬁnding, and in addition, implements algorithms for CR construction, monotonicity
evaluation, and value range analysis.
Representation The CR infrastructure is built around a set of data structures used for
representing CR-expressions and routines used for CR simpliﬁcation and CR−1 ﬁnding.
In addition, non-CRs, including constants and symbolic expressions are also represented.
Symbols are implemented as void pointers that point to the data structure (or interface) to
access the symbol from its location in the linked compiler. This creates the need for an API
implemented by the linked compiler that is subsequently called by CRLIB to access value,
type, and value range information for each symbol.
Rewriting System The manipulation of CR forms for simpliﬁcation and construction
requires a term rewriting system that manipulates CR forms.
CRLIB includes a term
rewriting system to aid in the normalization of CR-forms through simpliﬁcation, where
simpliﬁcation deﬁnes a system of reduction for CR expressions that is both conﬂuent and
terminating. Simpliﬁcation is used throughout the analysis process, i.e., in CR construction,
the computing of a CR inverse, analysis of CR monotonicity, and CR bounds analysis, and
so therefore it is important that the simpliﬁcation process be eﬃcient. Our implementation
is similar to that of constant folding for an optimizing compiler, and subsequently the
actions and overhead for simpliﬁcation are similar. In addition to symbolic simpliﬁcation,
the rewriting system implements the CR# simpliﬁcation rules for CR expressions, CR#
alignment and CR# bounding algorithms given in Chapter 3.
5.2.2
Inductive SSA Construction
We implemented Inductive SSA construction algorithm, presented in Chapter 4, in GCC
4.1 to annotate the SSA form with value progressions of scalar variables in a loop nest.
The Inductive SSA annotation algorithm is implemented as an optimization pass in GCC
4.1. It is placed after the SSA construction pass and loop construction pass in Tree SSA
optimizations since SSA form and the loop structure are the prerequisite of the Inductive
SSA annotation. With these two prerequisite satisﬁed, the Inductive SSA pass can be called
before any other tree optimizations ﬂexibly. Once the recurrence relations are collected for a
104

SSA variable, the function calling to the interface of CRLIB will return the inductive proofs
which is represented by CR forms or expressions simpliﬁed and constructed by CRLIB. As
a result of Inductive annotation algorithm, each SSA variable in the loop nest have the
inductive proofs mapped and can be exploited in the later optimizations.
5.3
Loop-variant Variable Analysis Implementation
Inductive SSA generalize loop-variant variable analysis. With Inductive proofs in Inductive
SSA for every SSA variable available, we are able to recognize and classify extensive classes
of loop-variant variables based on attached CR form in Inductive SSA. We implemented
the algorithm described in the Chapter 4 to analyze and classify all the SSA loop-variant
variable based on its inductive proofs.
5.4
Enhanced GVN-PRE with Inductive SSA-based
Optimizations
With Inductive SSA extension available, we have enhanced the GVN-PRE algorithm in
GCC 4.1 with the Inductive SSA-based constant detection, loop-invariant detection and
congruence detection, as described in Section 4.2 in Chapter 4.
Since PRE subsumes LICM and CSE, the implementation of these Inductive SSA based
algorithms were integrated in the PRE phase of GCC 4.1 to detect the additional equivalent
expression, constant in the loop and loop-invariant which PRE pass of GCC 4.1 failed to
recognize.
GCC4.x adopted a state-of-the-art PRE algorithm, named GVN-PRE, which
is developed by VanDrunen and Hosking
[31].
Whenever GVN-PRE fails to detect a
congruence, the proofs in Inductive SSA are compared, as described in the algorithm 7
in the Chapter 4, to detect additional equivalences of SSA variables that was missed. The
algorithm 5 and 6 in the Chapter 4 were implemented in the GVN-PRE to check the inductive
proof of each SSA variable for the detection of the additional constant detection and loop
invariant detection.
105

CHAPTER 6
EXPERIMENTAL RESULTS
This chapter gives an experimental evaluation of Inductive SSA for loop-variant variable
analysis, constant detection, loop-invariant detection, and congruence detection.
6.1
SPEC CPU2000 Benchmarks
SPEC CPU2000 [98] is a software benchmark produced by the Standard Performance
Evaluation Corp. (SPEC). It is designed to provide performance measurements that can
be used to compare compute-intensive workloads on diﬀerent computer systems, which
means SPEC CPU2000 focuses on the performance of CPU, the memory architecture and
the compilers. SPEC CPU2000 contains two benchmark suites: CINT2000 for measuring
and comparing compute-intensive integer performance, and CFP2000 for measuring and
comparing compute-intensive ﬂoating point performance.
The SPEC CPU2000 benchmark suite represents a diverse set of actual end-user appli-
cations, which are listed in Table 6.1, and provides a comparative measure of integer and/or
ﬂoating point compute intensive performance. The wide range of usage of SPEC, the openly
published results that are available and its focus on performance and compilers provides a
very good reference point for our experiments to measure the impact of our Inductive SSA
framework.
6.2
Measuring the Inductive Proof Choice Set Sizes
The Inductive SSA construction algorithm uses a choice set in the inductive proofs to model
values produced along control ﬂow paths.
Because the number of control ﬂow paths is
exponential in the number of decision points, full path analysis is potentially exponential.
However, we use a path matching mechanism on proof structures and only store proof
106

Table 6.1: The SPEC CPU2000 benchmark suite.
Benchmark
Description
CINT2000
GZIP
Data compression utility
VPR
FPGA circuit placement and routing
GCC
C compiler
MCF
Minimum cost network ﬂow solver
CRAFTY
Chess program
PARSER
Natural language processing
EON
Ray tracing
PERLBMK
Perl
GAP
Computational group theory
VORTEX
Object-oriented database
BZIP2
Data compression utility
TWOLF
Place and route simulator
CFP2000
WUPWISE
Quantum chromodynamics
SWIM
Shallow water modeling
MGRID
Multi-grid solver in 3D potential ﬁeld
APPLU
Parabolic/elliptic partial diﬀerential equations
MESA
3D graphics library
GALGEL
Fluid dynamics: analysis of oscillatory instability
ART
Neural network simulation: adaptive resonance theory
EQUAKE
Finite element simulation: earthquake modeling
FACEREC
Computer vision: recognizes faces
AMMP
Computational chemistry
LUCAS
Number theory: primality testing
FMA3D
Finite-element crash simulation
SIXTRACK
Particle accelerator model
APSI
Solves problems regarding temperature, wind, distribution of pollutants
structures that have multiple choices rather than explicitly represent all paths on which
the proofs were derived. Therefore, in practice the choice sets in the inductive proofs have
limited size and we bound the choice set size to maximum ten terms to ensure an asymptotic
linear-time Inductive SSA construction algorithm.
The experimental results in Table 6.2 indicate that the size of the choice set in inductive
proofs in Inductive SSA does not exceed 5 terms for all SPEC CPU2000 benchmarks. The
average size of the choice set for conditional updated variable of the benchmark in SPEC2000
107

Table 6.2: Measuring the sizes of choice sets in Inductive SSA on SPEC2000.
Benchmark
Max Average
Standard
Benchmark
Max Average
Standard
Deviation
Deviation
CINT2000
CFP2000
GZIP
3
2.41
0.49
APPLU
2
2
0.0
VPR
3
2.08
0.27
MESA
2
2
0.0
MCF
3
2.22
0.33
ART
2
2
0.0
CRAFTY
3
2.05
0.21
EQUAKE
2
2
0.0
PARSER
3
2.04
0.20
FACEREC
2
2
0.0
GAP
4
2.26
0.49
AMMP
3
2.2
0.4
VORTEX
3
2.08
0.28
LUCAS
2
2
0.0
BZIP2
4
2.32
0.61
SIXTRACK
2
2
0.0
TWOLF
3
2.06
0.24
Average
3.22
2.17
0.35
Average
2.13
2.03
0.04
ranges from 2.00 to 2.41 and the standard deviation value for each benchmark ranges from
0.0 to 0.61 which is very small. Since the small standard deviation value means the size of
the recurrence relations list of conditionally updated variable are close to the average size
value, we found that a threshold value ten that is used to limit the size of the choice set is
suﬃciently large to handle the SPEC2000 benchmarks accurately.
6.3
Results of Loop-Variant Variable Classiﬁcation
with Inductive SSA
By using our Inductive SSA construction algorithm implemented in GCC 4.1, we are able to
analyze the loop-variant variables in the SPEC CPU2000 benchmark based on their attached
inductive proofs, as described in Section 4.2.1 in Chapter 4.
Table 6.3 shows the experimental results of all induction variables categorized in
SPEC20001 with our algorithm. The ﬁrst column in the table names the benchmark. The
columns labeled ”Linear”, ”Polynomial”, ”Geometric”, ”OSV”, ”Cyclic”, ”Conditional”,
”Mix” and ”Unknown” show the percentage of each loop-variant variable category as a
percentage of the total number of loop-variant variables in each benchmark.
1Three CINT2000 and one CFP2000 benchmarks results are not listed because of a known compatibility
issue with GCC 4.1 and SPEC2000
108

Table 6.3: Results of loop-variant variable classiﬁcation with Inductive SSA on SPEC2000.
Benchmark
Linear
Polyn’l Geom.
OSV
Cyclic
Cond’l
Mix
Unknown
CINT2000
GZIP
59.45%
0.00%
0.00%
0.79%
0.00%
7.48%
0.00%
32.29%
VPR
59.47%
0.00%
0.21%
0.21%
0.00%
9.05%
0.00%
31.07%
MCF
38.18%
0.00%
0.00%
0.00%
0.00% 10.91%
0.00%
50.91%
CRAFTY
47.91%
0.00%
0.00%
0.00%
0.00% 12.71%
0.00%
39.37%
PARSER
35.19%
0.00%
0.00%
0.51%
0.00%
5.22%
0.51%
58.58%
GAP
62.73%
0.00%
2.52%
1.00%
0.33%
5.85%
0.38%
27.51%
VORTEX
66.06%
3.03%
0.61%
2.42%
0.00% 15.15%
0.00%
12.73%
BZIP2
54.67%
0.00%
0.93%
0.00%
0.00% 12.15%
1.40%
30.84%
TWOLF
40.21%
0.00%
0.00%
0.00%
0.00%
5.35%
0.00%
54.45%
Average
51.54%
0.34%
0.47%
0.55%
0.04%
9.32%
0.25%
37.53%
CFP2000
WUPWISE
80.20%
0.00%
0.00%
0.00%
0.00%
0.00%
0.00%
19.80%
SWIM
96.30%
0.00%
0.00%
0.00%
0.00%
0.00%
0.00%
3.70%
MGRID
84.06%
0.00%
0.00%
0.00%
0.00%
0.00%
0.00%
15.94%
APPLU
94.77%
0.00%
0.00%
0.00%
0.00%
1.31%
0.00%
3.92%
MESA
79.57%
0.00%
0.30%
0.00%
0.00% 12.73%
0.00%
7.40%
ART
73.12%
0.00%
0.00%
0.00%
0.00%
4.30%
0.00%
22.58%
EQUAKE
81.25%
0.00%
0.00%
2.08%
1.04%
3.12%
0.00%
13.54%
FACEREC
86.92%
0.00%
0.42%
0.00%
0.00%
2.53%
0.00%
10.13%
AMMP
59.89%
0.00%
0.00%
2.54%
0.00%
3.95%
0.00%
33.62%
LUCAS
87.68%
0.00%
1.48%
0.00%
0.00%
1.97%
0.99%
7.88%
FMA3D
74.57%
0.00%
0.00%
0.77%
0.12%
2.72%
0.00%
21.77%
SIXTRACK
83.87%
0.00%
2.15%
2.15%
0.00%
1.08%
1.08%
9.68%
APSI
80.89%
0.00%
0.69%
1.26%
0.92%
1.26%
0.00%
14.99%
Average
81.77%
0.00%
0.39%
0.68%
0.15%
2.69%
0.16%
14.22%
From the results of Table 6.3 the percentage of conditional induction variables ranges
from 5.22% to 15.15% in CINT2000, with 9.32% on average. None of these are detected
by GCC as well as other compilers, such as Open64 and Polaris [99] (Polaris uses advanced
nonlinear IV recognition algorithms [13]). Our algorithm also identiﬁes all geometric, mix,
cyclic and wrap-around induction variables. None of these are currently detected by GCC
implementations.
Table 6.4 shows the sub-classiﬁcation results of the conditional loop-variant variables
109

Table 6.4: Inductive SSA classiﬁcation of conditional loop-variant variables in SPEC2000.
Benchmark
Linear
Polyn’l
Geom.
OSV
Mix
Unknown
CINT2000
GZIP
42.11%
0.00%
0.00%
5.26%
15.79%
36.84%
VPR
72.73%
0.00%
0.00%
9.09%
0.00%
18.18%
MCF
66.67%
0.00%
0.00%
0.00%
16.67%
16.67%
CRAFTY
32.81%
0.00%
0.00%
10.94%
7.81%
48.44%
PARSER
70.97%
0.00%
0.00%
19.35%
0.00%
9.68%
GAP
45.53%
0.00%
0.00%
20.33%
6.50%
27.64%
VORTEX
60.00%
4.00%
0.00%
24.00%
0.00%
12.00%
BZIP2
61.54%
0.00%
0.00%
23.08%
7.69%
7.69%
TWOLF
51.61%
0.00%
0.00%
40.32%
0.00%
8.06%
Average
55.99%
0.44%
0.00%
16.93%
6.05%
20.58%
CFP2000
APPLU
100%
0.00%
0.00%
0.00%
0.00%
0.00%
MESA
63.26%
0.00%
0.00%
1.40%
0.47%
34.88%
ART
0.00%
0.00%
0.00%
100.00%
0.00%
0.00%
EQUAKE
100.00%
0.00%
0.00%
0.00%
0.00%
0.00%
FACEREC
83.33%
0.00%
0.00%
0.00%
0.00%
16.67%
AMMP
28.57%
0.00%
7.14%
14.29%
0.00%
50.00%
LUCAS
75.00%
0.00%
0.00%
0.00%
25.00%
0.00%
FMA3D
18.06%
0.00%
0.00%
53.55%
0.65%
27.74%
SIXTRACK
100.00%
0.00%
0.00%
0.00%
0.00%
0.00%
APSI
18.18%
0.00%
0.00%
18.18%
0.00%
63.64%
Average
58.64%
0.00%
0.71%
18.60%
2.55%
19.29%
category in SPEC2000. This sub-classiﬁcation identiﬁes the percentage of all conditionally
updated variables (taken as 100%), which have “Linear” bounds, “Polynomial” bounds,
“Geometric” bounds, “OSV” bounds, “Mix” bounds and “Unknown” bounds. Only those
benchmarks which have at least one conditional loop-variant variable are listed in the table.
For example, from the 7.48% of conditionally updated variables in GZIP, 42.11% is sub-
classiﬁed as the conditional CR form with linear lower and upper bounds. Thus, 3.15% of
the variables are conditionally updated variables with linear lower and upper bounds.
From the results of Table 6.4, conditional variables with linear lower and upper bounds
are the most common category.
The average percentage of linear conditional variable
110

 
 
GZIP
VPR
MCF
CRAFTY
PARSER
GAP
VORTEX
BZIP2
TWOLF
10
20
30
40
50
60
70
80
90
100
Linear IV
Inductive SSA IV
CINT2000 Benchmark
Percentages
Figure 6.1: Coverage of loop-variant variable classiﬁcation in SPEC CINT2000.
takes 55.99% and 58.64% in CINT2000 and CFP2000 separately. We also identify all re-
initialized variables, the “OSV” category. The “Mix” conditional variable has an average
percentage of 6.05% and 2.55% in CINT2000 and CFP2000, and was separately identiﬁed
with our implementation from OSV using the bounding technique. This result shows that
CR alignment is applied frequently for bounding the CR forms.
Figure 6.1 and Figure 6.2 show the total coverage of loop-variant variable analysis
with Inductive SSA for CINT2000 and CFP2000 separately. For CINT2000 benchmarks
in SPEC2000, except the linear IV which most traditional IV detection methods are
capable to recognize in these benchmarks, the extra percentage of loop-variant variable
recognized with Inductive SSA ranges from 5.23% to 16.21%, with 10.34% on average. For
CFP2000 benchmarks, the percentage of loop-variant variable recognized except linear IV
with Inductive SSA ranges from 0% to 13.03%, with 4.00% on average. Clearly, the loop-
111

 
 
WUPWISE
SWIM
MGRID
APPLU
MESA
ART
EQUAKE
FACEREC
AMMP
LUCAS
FMA3D
SIXTRACK
APSI
10
20
30
40
50
60
70
80
90
100
Linear IV
Inductive SSA IV
CFP2000 Benchmark
Percentages
Figure 6.2: Coverage of loop-variant variable classiﬁcation in SPEC CFP2000.
variant variable analysis with Inductive SSA has a greater coverage than the traditional
methods which only recognize linear IV for all the benchmarks.
6.4
Results for Inductive SSA-based Optimizations
Using the inductive proofs produced by Inductive SSA, we enhanced the GVN-PRE al-
gorithm in GCC 4.1.
The GVN-PRE algorithm in GCC 4.1 is a state-of-the-art hybrid
GVN-PRE algorithm developed by VanDrunen and Hosking
[31].
Table 6.5 shows the
number of additional redundant expressions detected in enhanced GVN-PRE phase of GCC
4.1 in SPEC2000. The ﬁrst column in the table lists the benchmark. Note that only the
benchmark which has additional redundant expressions detected is listed here. The column
labeled “EQ” shows the number of the additional equivalences detected in Inductive SSA for
112

Benchmark
EQ
LI
GAP
10
−
VORTEX
2
−
BZIP2
2
−
TWOLF
4
−
MGRID
38
−
APPLU
4
−
FACEREC
10
−
LUCAS
5
30
FMA3D
12
−
SIXTRACK
2
−
APSI
41
−
Table 6.5: Number of additional equivalences and loop-invariants detected by Inductive SSA
enhancement of GVN-PRE for SPEC CPU2000 benchmarks using GCC 4.1 -O3. Note: −
indicates no diﬀerence.
each benchmark. The column labeled “LI” shows the number of the additional loop-invariant
expressions detected in Inductive SSA for each benchmark. None of these are detected by
the GVN-PRE, the state-of-the-art PRE algorithm, in GCC implementations. Additional
constant in the loop was not detected in any benchmark.
Note that mgrid and apsi have a large number of additional equivalences detected, 38
and 41 respectively. Also note that lucas has 30 additional cases of loop-invariants detected,
which were missed by LICM in GVN-PRE. The invariants are operations in which two IVs
cancel out, resulting in a loop-invariant expression.
Performance testing with Inductive SSA is useful to see if the compiler can use Inductive
SSA throughout the optimization phases eﬀectively and without negative consequences.
However, performance testing results will only show the eﬀect of optimization in “hot spots’ of
frequently executed code. Therefore, not all of the additional congruences and loop-invariants
detected are expected to increase the code speed. Only a subset of these optimizations are
expected to be visible in the performance results.
Figure 6.3 shows the speedup of the SPEC2000 benchmarks optimized by GVN-PRE
with and without Inductive SSA. Only the benchmarks for which the code is diﬀerent than
the code produced by the original GVN-PRE are included (benchmarks with EQ ̸= 0 in
Table 6.5). The tests were conducted on a 2.8GHz Intel Pentium 4 running Redhat Linux
113

 
 
GAP
VORTEX
BZIP2
TWOLF
MGRID
APPLU
FACEREC
LUCAS
FMA3D
SIXTRACK
APSI
0.90
0.95
1.00
1.05
1.10
1.15
1.20
GCC4.1 GVN-PRE
GCC4.1 GVN-PRE 
with Inductive SSA
Benchmark
Speedup
Figure 6.3: Relative speedup of code produced by GVN-PRE enhanced with Inductive SSA
for SPEC2000 benchmarks using GCC 4.1 -O3.
9, using GCC 4.1 at -O3 optimization level.
From the ﬁgure, it can be concluded that Inductive SSA for GVN-PRE improves
performance in 6 out of 11 benchmarks, with an overall 2% speedup for the CFP2000
benchmarks. A signiﬁcant speedup of 17% is observed for mgrid, which has 38 additional
congruences detected by Inductive SSA. Also lucas shows improvement from the additional
LICM.
The speedups of the four CINT2000 benchmarks is more ﬂat than CFP2000.
The
additional redundant equivalences detected were not in “hot parts” of the code that are
frequently executed.
Additional optimizations in the cold parts do not show up in the
performance results. Another problem of GVN-PRE is that register pressure increases when
live ranges of additional temporary variables are long and spills may result as a consequence,
which can lower the performance. From the CINT2000 experiments we did not see a drop
114

 
 
GAP
VORTEX
BZIP2
TWOLF
MGRID
APPLU
FACEREC
LUCAS
FMA3D
SIXTRACK
APSI
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
1.10
1.20
GCC4.1 -O3
GCC4.1 -O3 with 
Inductive SSA
Benchmark
Normalized Compile Time
 
 
GAP
VORTEX
BZIP2
TWOLF
MGRID
APPLU
FACEREC
LUCAS
FMA3D
SIXTRACK
APSI
0.90
0.91
0.92
0.93
0.94
0.95
0.96
0.97
0.98
0.99
1.00
1.01
GCC4.1 GVN-PRE
GCC4.1 GVN-PRE 
with Inductive SSA
Benchmark
Normalized Static Size
Figure 6.4: Relative compile time and static code size of Inductive SSA with GVN-PRE for
SPEC2000 benchmarks using GCC 4.1 -O3.
115

in performance due to register spill.
Figure 6.4 shows the compile times of GCC and the static code sizes of the SPEC2000
benchmark separately optimized by GVN-PRE with and without Inductive SSA. The compile
times and the code sizes are normalized with respect to the compile time and the code size
of original GCC 4.1 implementation separately.
Due to the Inductive SSA construction and the inductive proofs matching for expressions
in the enhanced GVN-PRE algorithm in Inductive SSA, “GCC 4.1 -O3 with Inductive SSA”
is more expensive than“GCC 4.1 -O3”. The compile time of GCC 4.1 with the Inductive
SSA integration increases on average by 7% with a maximum of 15% observed for applu
when tested with SPEC2000. No combinatorial explosion of the path-dependent proofs was
observed. As discussed above, the intermediate and ﬁnal proof choice set sizes did not exceed
5 terms.
From Figure 6.4 it can be concluded that the static code size does not change signiﬁcantly
by the Inductive SSA enhancement of GVN-PRE. The variability of the code size is within
1% of the total size.
6.5
Inductive SSA Impact Analysis on GVN-PRE
We have also tested enhanced GVN-PRE with Inductive SSA implementation using opti-
mization option “-O2” of GCC 4.1 and other optimization ﬂags to determine the causes of the
impact of Inductive SSA on performance in more detail. Compared with the optimization
option “O3” of GCC 4.1, “O2” performs nearly all supported optimizations except some
optimizations that may greatly increase the size of the generated code, such as function
inlining and loop unrolling.
The number of additional redundant expressions and loop
invariants detected in enhanced GVN-PRE when using GCC 4.1 -O2 turns out to be the
same as the result of the additional redundancies detected with GCC 4.1 -O3, see Table 6.5.
Figure 6.5 shows the speedup of the SPEC2000 benchmarks optimized by GVN-PRE with
and without Inductive SSA using GCC 4.1 at -O2 optimization level. Only the benchmarks
for which the code is diﬀerent than the code produced by the original GVN-PRE are included
(benchmarks with EQ ̸= 0 in Table 6.5).
From the ﬁgure, it can be concluded that with optimization level “-O2”, Inductive SSA
for GVN-PRE improves performance in 6 out of 11 benchmarks, which is the same as for the
“-O3” level. Also a signiﬁcant speedup of 17% is observed for mgrid. The speedup result
116

 
 
GAP
VORTEX
BZIP2
TWOLF
MGRID
APPLU
FACEREC
LUCAS
FMA3D
SIXTRACK
APSI
0.90
0.95
1.00
1.05
1.10
1.15
1.20
GCC4.1 GVN-PRE
GCC4.1 GVN-PRE 
with Inductive SSA
Benchmark
Speedup
Figure 6.5: Relative speedup of Inductive SSA with GVN-PRE for SPEC2000 benchmarks
using GCC 4.1 -O2.
for benchmark vortex is not shown because of a runtime error produced by GCC 4.1 due
to a known incompatibility of GCC 4.1 with SPEC2000.
Figure 6.6 shows the compile times of GCC and the static code sizes of the SPEC2000
benchmark separately optimized by GVN-PRE with and without Inductive SSA when using
optimization option “-O2”. The compile times and the code sizes are normalized with respect
to the compile time and the code size of original GCC 4.1 implementation separately. With
the additional implementation of Inductive SSA and optimizations, “GCC 4.1 -O2 with
Inductive SSA” is more expensive than“GCC 4.1 -O2”. The compile time of GCC 4.1 after
integrating Inductive SSA increases on average by 6.7% with a maximum of 15% observed
for applu when tested with SPEC2000. Inductive SSA for GVN-PRE improves static code
size in 6 out of 11 benchmarks. There are 3 benchmark has a very small code size increase. A
signiﬁcant code size decrease of 10% is observed for mgrid, which shows that the static code
117

 
 
GAP
VORTEX
BZIP2
TWOLF
MGRID
APPLU
FACEREC
LUCAS
FMA3D
SIXTRACK
APSI
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
1.10
1.20
GCC4.1 -O2
GCC4.1 -O2 with 
Inductive SSA
Benchmark
Normalized Compile Time
 
 
GAP
VORTEX
BZIP2
TWOLF
MGRID
APPLU
FACEREC
LUCAS
FMA3D
SIXTRACK
APSI
0.90
0.91
0.92
0.93
0.94
0.95
0.96
0.97
0.98
0.99
1.00
1.01
GCC4.1 GVN-PRE
GCC4.1 GVN-PRE 
with Inductive SSA
Benchmark
Normalized Static Size
Figure 6.6: Relative compile time and static code size of Inductive SSA with GVN-PRE for
SPEC2000 benchmarks using GCC 4.1 -O2.
118

Benchmark
EQ
LI
CP
VPR
−
−
9
GAP
10
11
−
VORTEX
2
−
−
BZIP2
2
−
−
TWOLF
4
−
1
MGRID
38
−
−
APPLU
4
−
−
FACEREC
10
−
−
LUCAS
6
40
−
FMA3D
12
−
−
SIXTRACK
2
−
−
APSI
41
−
2
Table 6.6: Number of additional equivalences, loop-invariants, and constants detected by
GVN-PRE enhanced with Inductive SSA for SPEC CPU2000 using GCC 4.1 -O2 -fno-gcse
-fno-tree-ccp -fno-tree-loop-im -fno-move-loop-invariants -fno-ivopts. Note: −indicates no
diﬀerence.
size can also beneﬁt greatly from the additional congruence and loop-invariants detection by
Inductive SSA in GVN-PRE.
Table 6.6 shows the number of additional congruence, loop invariants and constant
detected in enhanced GVN-PRE phase of GCC 4.1 in SPEC2000 when using optimization
level “-O2” with several extra optimizations disabled, which are global common sub-
expression elimination, loop-invariant motion, constant propagation, and induction variable
optimizations.
Disabling these optimizations means that the analysis, detection, and
optimization is completely left to GVN-PRE. Thus, by comparing the results we can isolate
the impact of the Inductive SSA GVN-PRE improvement from other optimizations that
perform similar transformations (and are currently not enhanced by Inductive SSA).
The ﬁrst column in the table lists the benchmark. The column labeled “EQ” shows
the number of the additional equivalences detected and PRE-optimized in Inductive SSA for
each benchmark. The column labeled “LI” shows the number of the additional loop-invariant
expressions detected and LICM-optimized in Inductive SSA for each benchmark. The column
labeled “CP” shows the number of the additional constants detected and propagated by
CP in Inductive SSA for each benchmark. None of these equivalences and invariants are
119

detected by the GVN-PRE, the state of the art PRE algorithm, in GCC implementations.
The additional constants and loop invariants detected are semantic constants arising from
algebraic translations in the inductive proofs.
Note that gap and lucas have 11 and 40 additional loop-invariants detected separately.
There are 11 and 10 more additional loop-invariants detected for benchmark gap and lucas
separately compared with the result of using optimization level “-O2”.
Also note that
additional constants are detected for vpr, twolf and apsi, due to the disabled constant
propagation optimization of GCC 4.1. GVN-PRE in GCC 4.1 are responsible for detection
of the lexical redundancies opportunity left by disabled GCSE, LICM. That explains the
additional equivalences detected remains pretty much the same with the result of using
complete “-O2”. Since the additional equivalences detected are real semantic equivalences,
it is less sensitive to the syntactical code changes.
Figure 6.7 shows the speedup of the SPEC2000 benchmarks optimized by GVN-PRE with
and without Inductive SSA using GCC 4.1 at -O2 optimization level, with GCSE, LICM,
CCP and IV optimizations disabled. Only the benchmarks for which the code is diﬀerent
than the code produced by the original GVN-PRE are included (benchmarks with EQ ̸= 0,
or LI ̸= 0 , or CP ̸= 0 in Table 6.6). With optimization level “-O2” and several speciﬁc
optimizations disabled, Inductive SSA for GVN-PRE improves performance in 6 out of 11
benchmarks. A signiﬁcant speedup of 8.3% is observed for mgrid. The speedup of mgrid
is smaller compared to the speedup result when using optimization level “-O2”, due to the
less percentage of saved execution time by Inductive SSA based GVN-PRE in the longer
execution time of benchmarks, which caused by missed redundancies detection, constant
propagation by these disabled optimizations on global variables or local variables outside
of the loop. The speedup result for benchmark vortex is not shown because of a known
incompatibility of GCC 4.1 with SPEC2000.
Figure 6.8 shows the compile times of GCC and the static code sizes of the SPEC2000
benchmark separately optimized by GVN-PRE with and without Inductive SSA when
using optimization option “-O2”, with several important optimizations disabled.
Note
that Inductive SSA for GVN-PRE improves or does not change the static code size in all
benchmarks except sixtrack. A code size decrease of 3% and 2% is observed for mgrid
and facerec.
120

 
 
VPR
GAP
VORTEX
BZIP2
TWOLF
MGRID
APPLU
FACEREC
LUCAS
FMA3D
SIXTRACK
APSI
0.90
0.95
1.00
1.05
1.10
GCC4.1 GVN-PRE
GCC4.1 GVN-PRE 
with Inductive SSA
Benchmark
Speedup
Figure 6.7:
Relative speedup of code optimized by GVN-PRE with Inductive SSA for
SPEC2000 using GCC 4.1 -O2 -fno-gcse -fno-tree-ccp -fno-tree-loop-im -fno-move-loop-
invariants -fno-ivopts.
121

 
 
VPR
GAP
VORTEX 
BZIP2
TWOLF
MGRID
APPLU
FACEREC
LUCAS
FMA3D
SIXTRACK
APSI
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
1.10
1.20
GCC4.1 O2-opt
GCC4.1 O2-opt with 
Inductive SSA
Benchmark
Normalized Compile Time
 
 
VPR
GAP
VORTEX
BZIP2
TWOLF
MGRID
APPLU
FACEREC
LUCAS
FMA3D
SIXTRACK
APSI
0.90
0.91
0.92
0.93
0.94
0.95
0.96
0.97
0.98
0.99
1.00
1.01
GCC4.1 GVN-PRE
GCC4.1 GVN-PRE 
with Inductive SSA
Benchmark
Normalized Static Size
Figure 6.8: Relative compile time and static code size of code optimized by GVN-PRE
with Inductive SSA for SPEC2000 benchmarks using GCC 4.1 -O2 -fno-gcse -fno-tree-ccp
-fno-tree-loop-im -fno-move-loop-invariants -fno-ivopts.
122

CHAPTER 7
SIMD Vectorization of Chains of Recurrences
In this chapter, we present a library generator for fast execution of math functions over grid-
based computational domains. The library generator uses a vectorization method based on
an algebraic transformation of the math function into vector CR forms.
7.1
Introduction
Many computational tasks in numerical, visualization, and engineering applications require
the repeated evaluation of functions over structured grids, such as plotting in a coordinate
system, rendering of parametric objects in 2D and 3D, numerical grid generation, and signal
processing.
Thread-level parallelism is typically exploited to speed up the evaluation of
closed-form functions across points in a grid.
Another eﬀective optimization that yields good speedups when applicable to structured
grids or meshes is short-vector SIMD execution of arithmetic operations, e.g. “array
arithmetic”. Virtually all modern general-purpose processor architectures feature instruction
sets and instruction set extensions that support short-vector SIMD ﬂoating point and integer
operations, such as MMX/SSE, 3DNow!, AltiVec, and Cell BE SPU SIMD instructions.
Coding with these instruction sets is simpliﬁed by the use of software, such as state-of-
the-art compilers that automatically SIMD-vectorize computationally intensive loops by
restructuring these loops for SIMD vectorization [9] often in combination with highly-
optimized vector math library kernels [100].
At the hardware level, advances in algorithms for ﬂoating point arithmetic have led to
signiﬁcantly faster execution of math library functions in hardware for both scalar and SIMD
instructions, such as trigonometric, square root, logarithmic, and exponential functions.
For example, the Intel numerics family (8087, 80287, and 80387) use the fast CORDIC
123

(COordinate, Rotation DIgital Computer) methods [101, 102].
However, repeated execution of numerics instructions for each grid point over a structured
grid is still costly, especially in loops over the grid points where the execution latency of the
iterated ﬂoating point instructions cannot be hidden in the instruction pipeline. On an Intel
Pentium 4 processor for example, the execution latency of a numerics instruction typically
approaches 200 cycles [103].
An eﬀective technique to decrease operation counts and reduce instruction latencies of
repeated function evaluations over regular grids is provided by the Chains of Recurrences
(CR) formalism [89, 50, 47], which uses an algebraic approach to translate closed-form
functions and math kernels into recurrence forms that in a way implements aggressive loop
strength reduction [11]. Any ﬂoating point expression composed of math library functions
and standard arithmetic can be transformed into a CR form and then optimized to achieve
eﬃcient execution.
For example, if two functions f(x) and g(x) have CR forms, then
f(x) ± g(x), f(x) ∗g(x), and f(g(i)) have CR forms (with some obvious exceptions).
CR forms of closed-form functions require fewer operations per grid point by reusing
the value computed at a previous point to determine the function value at the next point.
Unfortunately however, this aspect of the CR formalism prohibits loop parallelization and
vectorization that require independent operations across the iteration space.
We present a library generator for fast execution of math functions over grid-based
computational domains.
The library generator uses an algebraic transformation of the
math function into a vector representation of a CR form. More speciﬁcally, we developed
a decoupling method for the CR algebra to translate math functions into Vector Chains of
Recurrences (VCR) forms. The VCR coeﬃcients are packed into short vector registers for
eﬃcient execution. This approach eﬀectively reduces the instruction counts for short-vector
SIMD execution of functions over grids. As a result, our method outperforms separately
optimized scalar CR forms and SIMD vectorized closed-form function evaluation.
We validated the performance gains using several benchmark functions evaluated in single
and double precision and compared the results to the Intel compiler’s auto-vectorized code
with the high-performance Short Vector Math Library (SVML). The results show a dramatic
performance increase of our VCR method over SIMD/SVML auto-vectorization and scalar
CRs, ranging from doubling the execution speed to running an order of magnitude faster. In
addition, the results also show that the VCR code can signiﬁcantly increase the Instruction
124

Level Parallelism (ILP) in modulo scheduling [55] resulting in faster kernels for non-SIMD
superscalar processors.
CR optimizations introduce potential roundoﬀerrors that are propagated across the
iteration space [56]. Error analysis is required to ensure roundoﬀerrors introduced in CR-
based evaluation methods are bounded. To address error propagation, we combined our
VCR method with an auto-tuning approach.
The VCR code is run to determine error
properties and select an optimal vector length for performance.
This approach ensures
optimal performance with high ﬂoating point accuracy.
In the remainder of this chapter, the VCR notation, semantics, and algebraic construction
will be introduced. An auto-tuning approach for fast and safe ﬂoating point evaluation is
described and implementation details are given.
Performance results showing improved
SIMD execution and ILP scheduling are presented followed by a discussion of alternative
approaches and related work.
7.2
Vector Chains of Recurrences
This section introduces the VCR formalism, which is a generalization of the scalar CR
formalism [89, 50, 47].
The notation and semantics are presented and an algorithm to
symbolically construct VCR forms of math functions is given.
7.2.1
VCR Notation and Semantics
The key idea of our VCR method is to translate a math function or ﬂoating point expression
into a set of d decoupled CR forms, where each CR represents the value progression of the
math function at an oﬀset 0, . . . d−1 and stride d ≥1. The decoupling factor d increases the
level of parallelism by a factor of d and also slows the propagation of roundoﬀerrors by a
factor d.
Deﬁnition 10 A d-dimensional VCR form Φd
i of a function f(i) evaluated over i =
0, . . . , n−1 is denoted by
Φd
i = {⃗ϕ0, ⊙1, ⃗ϕ1, ⊙2, · · · , ⊙k, ⃗ϕk}i
,
125

where ⊙are “+” or “∗” and ⃗ϕ are vector coeﬃcients
⃗ϕ0=





⃗ϕ0[0]
⃗ϕ0[1]
...
⃗ϕ0[d−1]




; ⃗ϕ1=





⃗ϕ1[0]
⃗ϕ1[1]
...
⃗ϕ1[d−1]




; · · · ⃗ϕk=





⃗ϕk[0]
⃗ϕk[1]
...
⃗ϕk[d−1]




.
Hence, a VCR Φd
i has (k+1) × d scalar values and k operations (⊙= + or ⊙= ∗) on
d-vectors.
Deﬁnition 11 The evaluation of a VCR form Φd
i of a function f(i) over i = 0, . . . , n−1 is
deﬁned by the loop
vector[d] ⃗v0
= ⃗ϕ0
vector[d] ⃗v1
= ⃗ϕ1
:
: :
vector[d] ⃗vk−1 = ⃗ϕk−1
for i = 0 to n −1 step d do
y[i : i+d−1] = ⃗v0
⃗v0
= ⃗v0
⊙1 ⃗v1
⃗v1
= ⃗v1
⊙2 ⃗v2
:
: :
:
:
⃗vk−1
= ⃗vk−1 ⊙k ⃗ϕk
endfor
Note that the loop computes y[i] = f(i) for i = 0, . . . , n−1 in vectors of d values per iteration
(assuming d divides n).
A method to construct the VCR form of a function is given in the next section. Here, we
give a simple example to illustrate VCR evaluation. For d = 2, the VCR form of f(i) = i! is
Φ2
i
=
{⃗ϕ0, ∗, ⃗ϕ1, +, ⃗ϕ2, +, ⃗ϕ3}i
=
{
1
1

, ∗,
2
6

, +,
10
14

, +,
8
8

}i
.
Using Deﬁnition 11 we obtain the sequence:
i =
0
2
4
6
. . .
 y[i]
y[i+1]

=
1
1

2
6

 24
120

 720
5040

· · ·
where pairs of values are computed in each iteration. In general, for any d ≥2 a speedup
is obtained with a vector-based VCR over scalar CR forms for functions evaluated over
structured grids and meshes.
126

7.2.2
VCR Construction
We present an algorithm to construct a VCR form of a function for any d ≥1. The case
d = 1 degenerates the VCR to a scalar CR form published in [89, 50, 47].
Given a function f(i) represented as a closed-form expression in symbolic form,
where f(i) is to be evaluated over n grid points i = 0, . . . , n−1, the VCR form
Φd
i for d ≥1 is constructed in two steps:
Step 1 Construct the symbolic parametric scalar CR form Φi′(j) of f(d i′ + j)
by substituting index i in f(i) by {j, +, d}i′ followed by the application of
the CR algebra to simplify this term:
f(i) = f({j, +, d}i′)
CR
=⇒Φi′(j)
,
which gives the parametric scalar CR form
Φi′(j) = {ϕ0(j), ⊙1, ϕ1(j), ⊙2, · · · , ⊙k, ϕk(j)}i′
.
Step 2 Construct the symbolic parametric VCR form
Φd
i (j) = {⃗ϕ0(j), ⊙1, ⃗ϕ1(j), ⊙2, · · · , ⊙k, ⃗ϕk(j)}i
,
with coeﬃcients deﬁned by
⃗ϕ0(j)=





ϕ0(j)
ϕ0(j+1)
...
ϕ0(j+d−1)




; · · · ⃗ϕk(j)=





ϕk(j)
ϕk(j+1)
...
ϕk(j+d−1)




.
where the coeﬃcients ϕ(j) were constructed in Step 1.
We prove the correctness of the algorithm.
Theorem 5 Let Φd
i (j), d ≥1, be the VCR form constructed by Algorithm 1 for a given
function f(i) deﬁned over n points i = 0, . . . , n−1.
Then, the loop template deﬁned in
Deﬁnition 2 for Φd
i (j) with j = 0 computes y[i] = f(i) exactly for all i = 0, . . . , n−1,
assuming exact arithmetic, i.e. in the absence of ﬂoating point roundoﬀ.
127

Proof.
From Step 1 we have f(i) = f(d i′ + j) with i′ = ⌊i
d⌋and j = i mod d for
i = 0, . . . , n−1.
Let Φi′(j) denote the scalar CR form of f(d i′ + j).
Then for each
j = 0, . . . , d−1 the CR forms Φi′(j) of f(d i′ + j) are evaluated for i′ = 0, . . . , n
d −1 to
compute f(i) = f(d i′ + j) = y[d i′ + j] with the CR loop template [50] (scalar form of
Deﬁnition 2):
for j = 0 to d −1 do
x0(j)
= ϕ0(j)
:
: :
xk−1(j) = ϕk−1(j)
for i′ = 0 to n
d −1 do
y[d∗i′ + j] = x0(j)
x0(j)
= x0(j)
⊙1 x1(j)
:
: :
:
:
xk−1(j)
= xk−1(j) ⊙k ϕk(j)
endfor
endfor
Because the j-loop iterations are independent, we can reorder the loops and use array
assignment notation to obtain
x0(0 : d−1)
= ϕ0(0 : d−1)
:
: :
xk−1(0 : d−1) = ϕk−1(0 : d−1)
for i′ = 0 to n
d −1 do
y[d∗i′ : d∗i′+d−1] = x0(0 : d−1)
x0(0 : d−1)
= x0(0 : d−1)
⊙1 x1(0 : d−1)
:
: :
:
:
xk−1(0 : d−1)
= xk−1(0 : d−1) ⊙k ϕk(0 : d−1)
endfor
Taking i = d ∗i′, this loop is equivalent to the loop in Deﬁnition 2 for Φd
i of f(i).
□
In Step 1 of the algorithm, the CR algebra described in [89, 50, 47] is used to simplify
the parametric scalar CR form of f. A comprehensive list of CR algebra rules can be found
in [17]. In certain cases, the exhaustive application of CR rules does not simplify to a single
CR form but results in CR-expressions [89] which contain multiple CR forms (see Example
3 at the end of this section). These CR forms can be separately evaluated in loops or in
fused loops.
Parameter j in the symbolic form Φd
i (j) is also used to generate blocked versions of the
VCR loops (see Section 7.2.3).
128

VCR construction is applicable to factorials, sums, products, polynomials, exponentials,
transcendentals, and compositions. We conclude this section with some examples.
Example 1 (From the example shown in Section 7.2.1) Consider f(i) = i! for i =
0, . . . , n−1. Let d = 2. Then, f(i) = f(2i′ + j) = f({j, +, 2}i′) = {j, +, 2}i′!. Applying the
CR algebra {j, +, 2}i′!
CR
=⇒{j!, ∗, j2+3j+2, +, 4j+10, +, 8}i′. The VCR Φ2
i is constructed
from the coeﬃcients ϕ0(j) = j!, ϕ1(j) = j2+3j+2, ϕ2(j) = 4j+10, and ϕ3(j) = 8, giving
⃗ϕ0=

j!
(j+1)!

; ⃗ϕ1=
j2+3j+2
j2+5j+6

; ⃗ϕ2=
4j + 10
4j + 14

; ⃗ϕ3=
8
8

and we set j = 0 (non-blocked loops), which simpliﬁes to
Φ2
i = {
1
1

, ∗,
2
6

, +,
10
14

, +,
8
8

}i
.
The value sequence of Φ2
i is shown in Section 7.2.1.
Example 2 Consider f(i) = ri2 for i = 0, . . . , n−1. Let d = 4. Then, f(i) = f(4i′ + j) =
f({j, +, 4}i′) = r{j, +, 4}2
i′
r{j, +, 4}2
i′
CR
=⇒
r{j, +, 4}i′{j, +, 4}i′
CR
=⇒
r{j2, +, 8{j, +, 4}i′ + 16}i′
CR
=⇒
{rj2, +, 8rj+16r, +, 32r}i′
.
The VCR Φ4
i is constructed from the symbolic coeﬃcients ϕ0(j) = rj2, ϕ1(j) = 8rj+16r,
and ϕ2(j) = 32r, giving
⃗ϕ0(j)= r




j2
j2+2j+1
j2+4j+4
j2+6j+9



; ⃗ϕ1(j)= r




8j+16
8j+24
8j+32
8j+40



; ⃗ϕ2(j)= r




32
32
32
32




and we set j = 0 to obtain the Φ4
i coeﬃcients
Φ4
i = {




0
r
4r
9r



, +,




16r
24r
32r
40r



, +,




32r
32r
32r
32r



}i
.
By Deﬁnition 2 we obtain the sequence
i =
0
4
8
12
. . .




y[i]
y[i+1]
y[i+2]
y[i+3]



=




0
r
4r
9r








16r
25r
36r
49r








64r
81r
100r
121r








144r
169r
196r
225r




· · ·
129

Example 3 Consider f(i) = sin(h i) for i = 0, . . . , n−1. For d ≥1 f(i) = f(d i′ + j) =
f({j, +, d}i′) = sin(h{j, +, d}i′)
CR
=⇒
ℜ({1
2 sin(hj)−1
2 cos(hj)I, ∗, cos(dh)+ sin(dh)I}i′) +
ℜ({1
2 sin(hj)+1
2 cos(hj)I, ∗, cos(dh)−sin(dh)I}i′)
,
where I is the imaginary unit and ℜ(z) is the real part of a complex number z. Note that
the CR forms represent exponential sequences in the complex domain. Let d = 2, then
Φ2
i = ℜ({
−1
2I
γ

, ∗,
α+β
α+β

}i) + ℜ({
 1
2I
δ

, ∗,
α−β
α−β

}i)
with α = cos(2h), β = sin(2h)I, γ = 1
2 sin h −1
2I cos h, and δ = 1
2 sin h + 1
2I cos h. This VCR
requires only one vector addition and two complex multiplications per two grid points.
7.2.3
VCR Loop Blocking
Zima et al. [104] analyzed the error characteristics of real- and complex-valued CR forms
by considering two primary CR categories, “pure-sum CR” and “pure-product CR”. The
results indicate that the error of pure-sum CR forms (polynomials) increases with increasing
recurrence iteration distance, but is constrained and independent of the actual function value
at each point. Thus, evaluation of polynomials with CR forms is numerically stable. The
error in exponential CR forms is constrained, but depends both on the iteration distance
and on the function values.
Evaluations of these and mixed CR forms using (complex)
ﬂoating-point arithmetic may not be numerically stable.
Therefore, blocking of VCR loops is essential.
It forces re-initialization of the VCR
sequence to stop error propagation. Figure 7.1 shows our blocked VCR loop template. The
block parameter b is used to bound the recurrence iteration length. After b recurrence steps
in the inner i-loop, the recurrences are re-initialized in the j-loop. The remainder of the
code handles the case when n is not a multiple of b ∗d.
Theoretically, the block size b should be an inverse function of the growth of the relative
error. That is, a small growth in relative errors generally yields a larger block size. Because
of the unpredictability of the error propagation in non-polynomial CR forms, the block size
b must be determined by empirical evaluation.
130

for j = 0 to n −(n mod (b ∗d)) −1 step (b ∗d) do
⃗v0
= ⃗ϕ0(j)
⃗v1
= ⃗ϕ1(j)
:
:
:
⃗vk−1
= ⃗ϕk−1(j)
for i = j to j + (b ∗d) −1 step d do
y[i : i+d−1] = ⃗v0
⃗v0
= ⃗v0
⊙1 ⃗v1
⃗v1
= ⃗v1
⊙2 ⃗v2
:
: :
:
:
⃗vk−1
= ⃗vk−1 ⊙k ⃗ϕk(j)
endfor
endfor
j
= ⌊n/(b ∗d)⌋∗b ∗d
⃗v0
= ⃗ϕ0(j)
⃗v1
= ⃗ϕ1(j)
:
:
:
⃗vk−1 = ⃗ϕk−1(j)
for i = j to n −(n mod d) −1 step d do
y[i : i+d−1] = ⃗v0
⃗v0
= ⃗v0
⊙1 ⃗v1
⃗v1
= ⃗v1
⊙2 ⃗v2
:
: :
:
:
⃗vk−1
= ⃗vk−1 ⊙k ⃗ϕk(j)
endfor
for i = n −(n mod d) to n −1 do
y[i] = ⃗v0[i mod d]
endfor
Figure 7.1: Blocked VCR loop template.
7.2.4
VCR Auto-Tuning
We use an auto-tuning approach for the generated code to optimize decoupling factor d for
speed and block size b for ﬂoating-point accuracy. A set of parametric loops is generated
for d = 1, 2, 4, 8 to ﬁnd the best speedup and largest b. Small values of b negatively impact
performance, while b cannot be too large to stay within the error threshold.
Increasing d yields better SIMD vector utilization.
Increasing d beyond 8 typically
results in performance degradation due to increased vector register pressure to hold VCR
131

α(n)
Function
Single Precision
Double Precision
Poly3
3.8 · 10−9n
8.2 · 10−18n
Spline
5.0 · 10−9n2
6.0 · 10−18n2
Table 7.1: Interpolated maximum relative errors.
coeﬃcients in vector registers, possibly leading to register spill. Increasing d also increases
the level of ILP in modulo scheduling [55], which may result in faster kernels for non-SIMD
superscalar processors. The decreased number of anti/ﬂow dependences of the VCR form
updates compared to scalar CR forms allows the scheduler to schedule instructions more
optimally. Here we assume the kernel is small, i.e. a loop to execute a few math functions
over a grid.
The basic approach to ﬁnd optimal d and b for ILP is the same as for SIMD auto-tuning.
More speciﬁcally, given a maximum relative error threshold εmax, the goal is to ﬁnd a block
size b such that the VCR evaluation error εb ≤εmax is bounded.
Because performance
typically increases with larger b, results are computed faster when the error tolerance is
higher. Thus, b should be maximized while εb ≤εmax.
The auto-tuning phase determines b as follows. A blocked VCR proﬁle loop is generated
for each value of d = 1, 2, 4, 8 and a starting value of b (b = 106 which is suﬃciently large)
and a given maximum n to compute
εb =
max
i=0,...,n−1

f(i) −Φd
i
f(i)

.
To reﬁne b, this process repeats for exponentially diminishing values of b until εb ≤εmax.
The performance of the best combination of d and b is selected.
For pure-sum VCR forms, which are polynomials and combinations of polynomials such
as spline calculations, error analysis is simpler.
The error of the scalar CR form (with
d = 1) is computed for increasing n and tabulated and interpolated to determine the error
bound α(n). Then the error for a choice of b and d can be determined from εb = α(b).
Because we require εb ≤εmax, the optimal block size b for a given d can be determined from
b = α−1(εmax).
Consider for example the α(n) values of a third-order polynomial Poly3 and a bicubic
132

for (i = 0, i<n; i++)
{
y[i] = cr0;
cr0 = cr0 + cr1;
cr1 = cr1 + cr2;
cr2 = cr2 + cr3;
}
register
m128 crv, tmp;
int s = MM SHUFFLE(0, 3, 2, 1));
for (i = 0, i<n; i++)
{
mm store ss(&y[i], crv);
tmp = mm sub ss(crv, crv);
tmp = mm shuﬄe ps(tmp, tmp, s);
crv = mm add ps(crv, tmp);
}
(a)
(b)
Figure 7.2: Scalar CR loop and loop transformed with SSE Shuﬄe-based CR vectorization.
Spline function shown in Table 7.1.
Suppose that four digits of precision are required,
i.e. εmax ≤10−5. Then b = 2.6 · 103 for Poly3 and b = 45 for Spline using single precision
ﬂoating point, and b = 1.2 · 1012 for Poly3 and b = 1.3 · 106 for Spline using double precision.
7.2.5
Shuﬄe-Based SIMD Vectorization
We compare the shuﬄe-based CR vectorization method suggested in [104] to our VCR
approach. When all k operators in a CR form of length k are identical, i.e. (+) or (*),
then this regularity can be exploited with a vector operation of length k + 1.
Figure 7.2 (b) shows the SSE code for a cubic polynomial (CR length k=3). A copy of
vector crv=[cr0,cr1,cr2,cr3] is made to vector tmp using mm sub ss such that tmp[0]=0, then
tmp is rotated tmp=[cr1,cr2,cr3,0] and added to crv.
The shuﬄe-based method is limited to SIMD vectorization of polynomials and exponen-
tial CR forms, not mixed forms, e.g. factorials. In addition, the method is only optimal when
the length of the CR is a multiple of the SIMD register size. This method is not further
considered until Section 7.3.
7.2.6
Implementation Details
We implemented the function translation and auto-tuned VCR code generation techniques
with the help of the Ctadel system [105] (implemented in SWI-Prolog [106]), which has an
algebraic optimizer, term rewriting system, and code generator. The CR rules [17] were
133

cr0r = 0.0;
cr0i = -0.5;
cr1r = 0.0; cr1i = 0.5;
cr2r = cos(h); cr2i = sin(h); cr3r = cr2r; cr3i = -cr2i;
for (i = 0, i<n; i++)
{
y[i] = cr0r + cr1r;
tmp1 = cr0r*cr2r - cr0i*cr2i; cr0i = cr0r*cr2i + cr0i*cr2r;
tmp2 = cr1r*cr3r - cr1i*cr3i; cr1i = cr1r*cr3i + cr1i*cr3r;
cr0r = tmp1; cr1r = tmp2;
}
Figure 7.3: Scalar CR code of Sine benchmark.
i=0
i=1
. . .
f0 = sin(h ∗4 ∗i)
=⇒
y[0]
y[4]
. . .
f1 = sin(h ∗(4 ∗i + 1))
=⇒
y[1]
y[5]
. . .
f2 = sin(h ∗(4 ∗i + 2))
=⇒
y[2]
y[6]
. . .
f3 = sin(h ∗(4 ∗i + 3))
=⇒
y[3]
y[7]
. . .
time
-
4-wide
vector
6
?
Figure 7.4: VCR vectorized Sine sequences for decoupling factor d = 4.
implemented along with an SSE intrinsics code generator for C.
The symbolic form of a math expression containing library function calls to be evaluated
over variable i is entered, together with a constant decoupling factor d ≥1. We generate two
loop codes using Ctadel, one for proﬁle-based auto-tuning that also includes the original
closed-form function to compute the maximum relative error, and the optimized VCR
blocked loop with SSE intrinsics. The ﬁnal code is easily integrated into the user’s code
as a precomputation.
7.2.7
An Example
Consider f(i) = sin(h i) from Example 3 of Section 7.2.2 evaluated over a grid i = 0, . . . , n−1
in single-precision ﬂoating point. The scalar CR loop is shown in Figure 7.3. As discussed,
the standard CR loop [89, 47] is not vectorizable due to cross iteration dependences.
For d = 4, the VCR form for sin(h i) is generated that represent four parallel running
134

register
m128 crv0r, crv1r, crv2r, crv3r;
register
m128 crv0i, crv1i, crv2i, crv3i;
register
m128 crv0rt, crv1rt, vt1, vt2, *yv;
. . .
ch = cos(h);
sh = sin(h);
c2h = 2*ch*ch - 1;
s2h = 2*sh*ch;
c4h = 2*c2h*c2h - 1;
s4h = 2*s2h*c2h;
c 4h = c4h;
s 4h = -s4h;
crv0r = mm set ps(sh*ch*ch+0.5*sh*c2h, sh*ch, 0.5*sh, 0.0);
crv1r = mm set ps(sh*ch*ch+0.5*sh*c2h, sh*ch, 0.5*sh, 0.0);
crv2r = mm set ps(c4h, c4h, c4h, c4h);
crv3r = mm set ps(c 4h, c 4h, c 4h, c 4h);
crv0i = mm set ps(-0.5*ch*c2h+sh*ch*sh, -0.5*c2h, -0.5*ch, -0.5);
crv1i = mm set ps(0.5*ch*c2h-sh*ch*sh, 0.5*c2h, 0.5*ch, 0.5);
crv2i = mm set ps(s4h, s4h, s4h, s4h);
crv3i = mm set ps(s 4h, s 4h, s 4h, s 4h);
for (i = 0, i<n/4; i++ )
{
yv[i] = mm add ps(crv0r, crv1r);
vt1 = mm mul ps(crv0r, crv2r); vt2 = mm mul ps(crv0i, crv2i);
crv0rt = mm sub ps(vt1, vt2);
vt1 = mm mul ps(crv0r, crv2i); vt2 = mm mul ps(crv0i, crv2r);
crv0i = mm add ps(vt1, vt2);
vt1 = mm mul ps(crv1r, crv3r); vt2 = mm mul ps(crv1i, crv3i);
crv1rt = mm sub ps(vt1, vt2);
vt1 = mm mul ps(crv1r, crv3i); vt2 = mm mul ps(crv1i, crv3r);
crv1i = mm add ps(vt1, vt2);
crv0r = crv0rt; crv1r = crv1rt;
}
Figure 7.5: SSE VCR code for Sine benchmark for decoupling factor d = 4 (unblocked).
sequences as illustrated in Figure 7.4. Figure 7.5 shows the corresponding 4-wide vectorized
VCR code for sin(h i) with Intel SSE instructions (loop blocking and epilogue code are
elided for clarity). The VCR coeﬃcients of the VCR form are stored into the 128-bit vector
registers. As expected, four values are computed and stored in array yv per iteration.
As can be seen in the code in Figure 7.5, additional optimizations have been applied to
reduce the evaluation of trigonometric functions. Simpliﬁcation rules are integrated in Ctadel
to simplify expressions and remove redundant computations. For example, the following four
135

i=0
i=1
. . .
f0 = sin(h ∗8 ∗i)
=⇒
y[0]
y[8]
. . .
f1 = sin(h ∗(8 ∗i + 1))
=⇒
y[1]
y[9]
. . .
f2 = sin(h ∗(8 ∗i + 2))
=⇒
y[2]
y[10]
. . .
f3 = sin(h ∗(8 ∗i + 3))
=⇒
y[3]
y[11]
. . .
f4 = sin(h ∗(8 ∗i + 4))
=⇒
y[4]
y[12]
. . .
f5 = sin(h ∗(8 ∗i + 5))
=⇒
y[5]
y[13]
. . .
f6 = sin(h ∗(8 ∗i + 6))
=⇒
y[6]
y[14]
. . .
f7 = sin(h ∗(8 ∗i + 7))
=⇒
y[7]
y[15]
. . .
time
-
4-wide
vector
6
?
4-wide
vector
6
?
Figure 7.6: VCR vectorized Sine sequences for decoupling factor d = 8.
formulas for sine and cosine functions are adopted in Ctadel as rewrite rules to facilitate the
transformation to VCR forms:
cos(−h)
= cos(h)
sin(−h)
= −sin(h)
cos(2 ∗h)
= 2 ∗cos(h) ∗cos(h) −1
sin(2 ∗h)
= 2 ∗sin(h) ∗cos(h)
Figure 7.6 illustrates the increased levels of parallelism with d = 8 to evaluate sin(h i)
over i = 0, . . . , n−1.
This 8-way, 4-wide VCR code uses twice as many vector registers
compared to the implementation of 4-way, 4-wide vectorization. While the performance of
the code is typically better due to improved scheduling with higher ILP, the extra register
pressure could lead to spill which degrades performance.
The performance results and ﬂoating point accuracy of this example application and
other benchmark math functions will be presented in the next section and compared to the
performance of scalar CR forms, shuﬄe-based vector CR forms, and the Intel compiler’s
advanced auto-vectorization.
7.3
Results
In this section we present the performance results of our VCR technique applied to
benchmark functions that represent common classes of math operations.
136

Benchmark
Description
Poly3
Cubic polynomial
Spline
Bicubic spline
Bin15
Binomial coeﬃcient
  i
15

Sine
Sine function sin(h i)
Sinh
Hyperbolic sine sinh(h i)
Exp
Exponential exp(h i(i + 1)/2)
Table 7.2: Benchmark functions used in performance testing.
Class
Description
Scalar
Original closed-form w/o vectorization
SVML
Original closed-form vectorized w/ SVML
Scalar CR
Serial scalar CR form of a function
VCR4.4
4-way 4-wide VCR vectorization
VCR8.4
8-way 4-wide VCR vectorization
VCR2.2
2-way 2-wide VCR vectorization
VCR4.2
4-way 2-wide VCR vectorization
VCR4.1
Serial CR decoupled 4 times (=4 CR forms)
sCR1.4
1-way 4-wide shuﬄe-based CR vectorization
sCR4.4
4-way 4-wide shuﬄe-based CR vectorization
Table 7.3: Overview of VCR optimizations applied to benchmarks.
7.3.1
Math Function Benchmark Codes
The benchmark functions listed in Table 7.2 are used in the performance experiments. These
benchmark functions were selected as representable examples of classes of common math
operations found in numerical applications. There are two main classes: polynomial VCR
forms (Poly3, Spline, and Bin15 which is a 15th order polynomial) and exponential VCR
forms (Sine, Sinh, and Exp). The functions are evaluated over the interval i = 0, . . . , n−1
for n ∈[1, . . . , 106].
Table 7.3 classiﬁes the benchmark codes according to the type of optimization applied in
the performance comparisons. The “Scalar” codes run the original closed-form benchmark
function. The “SVML” code represents the original closed-form function auto-vectorized
by the Intel compiler (option -axW) with SVML. The “Scalar CR” codes refer to the use
137

Maximum Relative Error
Benchmark
Single-precision
Double-precision
Poly3
3.72 · 10−07
8.15 · 10−16
Spline
1.62 · 10−05
4.90 · 10−14
Bin15
2.96 · 10−07
2.07 · 10−16
Sine
1.37 · 10−04
3.62 · 10−13
Sinh
5.26 · 10−06
8.81 · 10−15
Exp
1.76 · 10−06
3.00 · 10−14
Table 7.4: Maximum relative error for n = 100.
of standard scalar CR forms published in [89, 104] to optimize function evaluations. The
“VCRd.w” codes are produced using the techniques described in this chapter, where d is the
decoupling factor and w is the vector register length (d must be a multiple of w for VCR).
The “sCRd.w” codes denote shuﬄe-based CR vectorized forms for performance comparison,
as described in Section 7.3.4.
Because ﬂoating-point operation counts (ﬂops) vary signiﬁcantly between the CR and
non-CR optimizations, the performance is expressed in normalized GFLOPS. In the sequel,
GFLOPS is the total standardized ﬂops over the total execution time t in seconds: GFLOPS
= ﬂops
t
· 10−9. The basis of the standardized ﬂops is the Scalar CR code, which uses the least
number of ﬂops to compute the function per grid point. For example, the total standardized
ﬂops = 3n for Poly3 over n points (see Figure 7.2(a)), regardless of the vectorization
and/or CR optimizations. This ensures a fair comparison of the optimizations by expressing
performance as the inverse of execution time scaled by problem size.
7.3.2
Floating Point Accuracy
The maximum relative error of the scalar CR form (with d = 1) for n = 100 steps is given
for single-precision and double-precision ﬂoating point for each benchmark in Table 7.4.
The table shows that the relative error of CR evaluation for these benchmarks is quite small,
especially for double precision. In general, using double precision to compute single precision
results with the VCR method for large n appears to be desirable, though this eﬀectively
halves the vector register width and thus reduces performance. A VCR blocking size b is
selected based on the error properties of a speciﬁc function as described in Sections 7.2.3
138

and 7.2.4.
Larger decoupling factors d limit the error propagation. For example, suppose d = 8 and
b = 100 recurrence steps are taken. Then 800 grid points are eﬃciently computed with an
accuracy that is listed in Table 7.4. Results discussed in the next section indicate that the
performance of our VCR method exceeds other methods for as few as 10 grid points and
reaches a maximum for about 1,000 grid points. Thus, b = 100 is a realistic bound to keep
the error low and performance high with d = 4 or d = 8.
7.3.3
SSE Performance Results
The performance results presented in this section were obtained using the Intel C++
compiler v9.1 with options “icc -O3 -restrict -fno-inline -axW”. The compiler uses a clever
technique to vectorize math functions using SSE2 instructions and SVML by expanding the
series computations [100]. Experiments were performed on a Intel Dual Core Xeon 5160
3GHz running Linux Fedora Core 7. Each benchmark was run repeatedly and the average
performance recorded. The execution time variance was low ≈1%.
Figure 7.7 compares the performance in GFLOPS for the benchmarks listed in Table 7.2,
for n = 1, 000 grid points and n = 100, 000 grid points separately. Recall that the accuracy of
CR methods is high for n = 1, 000, but can be poor for n = 100, 000, necessitating blocking,
e.g. in intervals of 1, 000 points resulting in the performance shown for n = 1, 000.
A dramatic increase in performance is observed for the SVML code compared to the
Scalar code, due to the eﬃcient auto-vectorization of the scalar code by the Intel compiler.
However, the Intel compiler failed to vectorize Bin15.
The use of CR-based methods to reduce operation counts also has a dramatic impact
on performance. The Scalar CR codes are signiﬁcantly faster than the Scalar codes of the
original benchmark functions (both codes are highly optimized, e.g. with loop unrolling).
However, operation reduction alone is not suﬃcient to achieve high performance as can be
seen in the ﬁgure by comparing SVML to Scalar CR. In fact, the speed of the SVML code
for Poly3, Spline, and Sine is generally faster than the Scalar CR code.
The vectorization of CR forms using our VCR method gives the highest overall perfor-
mance improvement as can be seen in Figure 7.7. The VCR4.4 code executes at least twice
as fast as the SVML and Scalar CR codes, peaking at an almost fourfold speedup for Bin15
and Sine. Further performance improvements are obtained with VCR8.4. For n = 1, 000,
139

 
 
Poly3
Spline
Bin15
Sine
Sinh
Exp
0
2
4
6
8
10
12
14
16
Performance Results (n = 1,000)
Scalar
SVML
Scalar CR
VCR4.4
VCR8.4
Benchmark
GFLOPS
 
 
Poly3
Spline
Bin15
Sine
Sinh
Exp
0
2
4
6
8
10
12
14
16
Performance Results (n = 100,000)
Scalar
SVML
Scalar CR
VCR4.4
VCR8.4
Benchmark
GFLOPS
Figure 7.7: Performance comparison of VCR for n = 1, 000 and n = 100, 000 (Intel Dual
Core Xeon 5160 3GHz).
140

 0
 2000
 4000
 6000
 8000
 10000
 12000
 14000
 16000
 10
 100
 1000
 10000
 100000
 1e+06
MFlop/s
Dim n
Sine VCR8.4
Sine VCR4.4
Sine Scalar CR
Sine SVML
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
 9000
 10
 100
 1000
 10000
 100000
 1e+06
MFlop/s
Dim n
Sine VCR4.2
Sine VCR2.2
Sine Scalar CR
Sine SVML
Figure 7.8: Sine single and double precision performance comparison (Intel Dual Core Xeon
5160 3GHz).
141

the exponential VCR benchmarks Sine, Sinh, and Exp are faster. For n = 100, 000 the
performance of VCR8.4 is faster overall. The level of parallelism is increased by a factor
p = d/w = 2 for VCR8.4. Therefore, the instruction scheduler has fewer dependences to
take into account, which leads to a higher instructions per cycle (IPC) ratio.
To investigate the scalability of the performance over the interval [10, . . . , 106] for SVML,
Scalar CR, and VCR, we plotted the normalized MFLOPS of the Sine benchmark in
Figure 7.8 for single and double-precision ﬂoating point.
For n ≥50, the VCR8.4 and
VCR4.2 codes are the fastest, for the single and double-precision experiments, respectively.
The lower performance for smaller n is caused by the initialization overhead to setup the
VCR vectors, where the initialization cost increases with larger d. Note that for the single-
precision case the VCR4.4 code is about a factor 3 to 4 faster than the Scalar CR code,
showing close-to-perfect vectorization cost reduction and scalability for n ≥500. Similarly,
for the double-precision case the VCR2.2 code is about a factor 2 faster than the Scalar CR
code for n ≥500.
The performance of the VCR codes drop after n = 500, 000 due to cache capacity eﬀects,
while the SVML code performance drops much earlier due to temporary vector memory
storage and cache eﬀects. The apparent jitter observed in the graphs for the VCR4.4 and
VCR8.4 single-precision code is consistent and appears to be related to the use of more
aggressive optimization options with the Intel compiler, indicating that certain points beneﬁt
from increased performance due to loop optimizations such as unrolling.
The VCR8.4 code exhibits higher levels of independence between iterations than VCR4.4
(and similar for VCR4.2 versus VCR2.2). This signiﬁcantly increases the computational
throughput for the Sine benchmark for the single precision case and more dramatically for
the double precision case as observed in Figure 7.8.
Note that the performance of double-precision SVML code has decreased to around half
of the performance of the single precision SVML code, which is due to the fact that eﬀective
SIMD vector length is reduced by half.
Similarly, the performance of the VCR2.2 code
and the VCR4.2 code also decreased to half of the performance of the VCR4.4 code and
the VCR8.4 code, respectively. The performance of the serial version Scalar CR is about
the same as its single precision version, which is not surprising given that the code is not
vectorized.
Overall, the performance of the VCR code is superior to any of the other optimization
142

register
m128 cr0, cr1, cr2, cr3, tmp;
int s = MM SHUFFLE(0, 3, 2, 1));
cr0 = . . . ; cr1 = . . . ; cr2 = . . . ; cr3 = . . . ;
for (i = 0; i < n; i++)
{
mm store ss(&x[i], cr0);
tmp = mm move ss(cr0, cr1); tmp = mm shuﬄe ps(tmp, tmp, s);
cr0 = mm add ps(cr0, tmp);
tmp = mm move ss(cr1, cr2); tmp = mm shuﬄe ps(tmp, tmp, s);
cr1 = mm add ps(cr1, tmp);
tmp = mm move ss(cr2, cr3); tmp = mm shuﬄe ps(tmp, tmp, s);
cr2 = mm add ps(cr2, tmp);
tmp = mm sub ss(cr3, cr3); tmp = mm shuﬄe ps(tmp, tmp, s);
cr3 = mm add ps(cr3, tmp);
}
Figure 7.9: Code optimized with sCR4.1 for the Bin15 benchmark.
methods, sometimes even an order of magnitude faster than the original scalar code and
SVML-vectorized code. The VCR transformation eﬀectively combines CR-based operation
reduction with SIMD vectorization. The choice of decoupling factor d that gives the best
speedup of VCR depends on the function to optimize and is diﬃcult to predict which leads
us to the auto-tuning approach. Note that d should be a multiple of the SIMD vector register
length w and reasonable values are therefore d = 2, d = 4, and d = 8.
7.3.4
SSE Shuﬄe-Based CR Performance Results
In this section, the performance results of the shuﬄe-based CR vectorization, denoted sCR,
is compared. Experiments were performed on a Intel Dual Core Xeon 5160 3GHz running
Linux Fedora Core 7, using the Intel C++ compiler with compiler ﬂags “icc -O3 -restrict
-fno-inline -axW”. Recall that the shuﬄe-based CR vectorization approach is only applicable
to polynomial benchmarks Poly3, Spline, and Bin15.
The optimized design of the SSE code for shuﬄe-based CR operations illustrates the
use of advanced instructions and some tricks to reduce the operation count to the bare
minimum. The transformation from scalar CR code to shuﬄe-based sCR code was shown
in Figure 7.2. A more elaborate example of sCR1.4 code for Bin15 is shown in Figure 7.9,
143

register
m128 crv0, crv1, crv2, crv3, tmp;
int s = MM SHUFFLE(0, 3, 2, 1));
crv0 = . . . ; crv1 = . . . ; crv2 = . . . ; crv3 = . . . ;
for (i = 0; i < n; i += 4)
{
mm store ss(&y[i
], crv0); mm store ss(&y[i+1], crv1);
mm store ss(&y[i+2], crv2); mm store ss(&y[i+3], crv3);
tmp = mm sub ss(crv0, crv0); tmp = mm shuﬄe ps(tmp, tmp, s);
crv0 = mm add ps(crv0, tmp);
tmp = mm sub ss(crv1, crv1); tmp = mm shuﬄe ps(tmp, tmp, s);
crv1 = mm add ps(crv1, tmp);
tmp = mm sub ss(crv2, crv2); tmp = mm shuﬄe ps(tmp, tmp, s);
crv2 = mm add ps(crv2, tmp);
tmp = mm sub ss(crv3, crv3); tmp = mm shuﬄe ps(tmp, tmp, s);
crv3 = mm add ps(crv3, tmp);
}
Figure 7.10: Code optimized with sCR4.4 for the Poly3 benchmark.
which further draws on specialized SSE instructions to manipulate CR coeﬃcients packed in
vector registers.
To further speed up sCR code, we used a decoupling factor to generate independent
execution sequences that increases the level of ILP. The SSE code for the sCR optimization
with a decoupling factor d = 4, the sCR4.4 shuﬄe-based CR vectorization of benchmark
Poly3 is shown in Figure 7.10. The code essentially replicates the shuﬄe-based template of
Figure 7.2 by applying our decoupling technique to the shuﬄe-based CR method.
Figure 7.11 summarizes the performance for all three polynomial benchmarks for n =
1, 000 grid points. Note that the Intel compiler failed to vectorize Bin15. Except for Bin15,
the Intel SVML-vectorized code is fastest. The Poly3 sCR1.4 code has the worst performance
among these versions. The shuﬄe operation used in this version is the reason of the slow-
down. Shuﬄe operations that involve a data reorganization in vector registers are usually
expensive and appear to be more costly than the non-vector ﬂoating point operations on
ﬂoating point registers in the Scalar CR code. Increasing the ILP appears to help for long
CR forms, e.g. the 15th order polynomial Bin15 code.
To investigate the scalability of the performance of sCR over the interval [10, . . . , 106],
the performance results for the Poly3 benchmark are also shown in Figure 7.11. As can be
144

 
 
Poly3
Spline
Bin15
0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
Performance Results (n = 1,000)
SVML
Scalar CR
sCR1.4
sCR4.4
Benchmark
MFLOPS
 0
 1000
 2000
 3000
 4000
 5000
 6000
 10
 100
 1000
 10000
 100000
 1e+06
MFlop/s
Dim n
Poly3 SVML
Poly3 Scalar CR
Poly3 sCR4.4
Poly3 sCR1.4
Figure 7.11: Performance comparison of shuﬄe-based CR vectorization for the polynomial
benchmarks with n = 1, 000 and for Poly3 with n = 10, . . . , 106 (Intel Dual Core Xeon 5160
3GHz).
145

expected, the Poly3 sCR4.4 code shows an almost four-fold performance increase over the
Poly3 sCR1.4 code due to higher ILP.
The overall performance of sCR is disappointing compared to SVML. Thus, sCR is also
much slower than our proposed VCR method (since VCR is overall faster than SVML).
Given the low performance of sCR and its limited applicability, The sCR optimization is not
a viable alternative. However, new SSE instructions such as a “shift-add” that would ﬁt the
CR update operations could make the performance of sCR more competitive.
7.3.5
Superscalar RISC ILP Enhancement Results
The performance results presented in this section were obtained using the Sun Studio 12
C compiler with compiler options “suncc -fast -fma=fused -xrestrict -xalias level=layout -
xinline=no -xarch =native”.
Note that the “-fma=fused” option is used to improve the
performance of the code with fused multiply-add (FMA) instructions, when possible.
Experiments were performed on an UltraSPARC IIIi 1.2GHz running Sun Solaris 10. The
UltraSPARC IIIi processor is a high performance, highly-integrated 4-issue superscalar
processor implementation of the 64-bit SPARC-V9 RISC architecture, supporting a 64-bit
virtual address space. We did not use the UltraSPARC VIS instruction set for SIMD short
vector operations, which supports only integer and ﬁxed-point operand types. Therefore,
any improvement is a result of the improved utilization of ILP in modulo scheduling.
The 4-issue superscalar processor performance beneﬁts from increased levels of ILP. Thus
modulo scheduling is an essential compiler optimization for small loop kernels. The Scalar
CR code exhibits loop-independent anti dependences between the CR updates and cross
iteration dependences (see Figure 7.2(a) for example) that limit the eﬀectiveness of the
modulo scheduler to schedule instructions in parallel. The best performance is obtained
with the lowest steady-state cycle count (also referred to as the initiation interval of the
kernel), assuming no stalls.
Figure 7.12 summarizes the performance of all benchmark codes, for n = 1, 000 grid
points and n = 100, 000 grid points. The performance of the transcendental math library
functions on this machine is very poor (the Scalar codes), making the data for Sinh and
Exp barely visible in the graph. The diﬀerence is marginally better for higher n = 100, 000,
because of the relatively lower overhead of CR initialization cost and relatively lower prologue
and epilogue costs in the modulo schedule.
146

 
 
Poly3
Spline
Bin15
Sine
Sinh
Exp
0
200
400
600
800
1000
1200
1400
1600
1800
Performance Results (n = 1,000)
Scalar
Scalar CR
VCR4.1
Benchmark
MFLOPS
 
 
Poly3
Spline
Bin15
Sine
Sinh
Exp
0
200
400
600
800
1000
1200
1400
1600
1800
Performance Results (n = 100,000)
Scalar
Scalar CR
VCR4.1
Benchmark
MFLOPS
Figure 7.12: Performance comparison of VCR-enhanced ILP (UltraSPARC IIIi 1.2GHz).
147

Poly3 Benchmark
Measurement
Scalar
Scalar CR
VCR4.1
Unroll factor
8
8
2
Steady-state
5
4
3
cycle count
(12 per 4 pts)
Floating point
operations
3 FMA +
3 FPadds
12 FPadds
per iteration
2 FPadds
(per 4 points)
Table 7.5: Static code size statistics of three Poly3 benchmark codes.
The hypothesized increased ILP of our VCR method is indeed veriﬁed by the results in
Figure 7.12. The VCR 4.1 code (decoupling factor d = 4 and vector width w = 1) achieves
best performance for all benchmark functions except Bin15. The modulo scheduler of the
Sun Studio compiler failed to construct a schedule for the Bin15 scalar code, due to the high
ﬂoating-point register pressure brought by four decoupled 15-order polynomial CR functions
requiring the use of 64 registers just to hold the CR coeﬃcients.
To study the impact of enhanced ILP on modulo scheduling, Table 7.5 lists the static
statistics extracted from the Sun Studio compiler for optimizing the computational loop
with modulo scheduling in the three optimized versions of the Poly3 code. The statistics are
similar for the other benchmarks. The steady-state cycle count determines the performance,
where lower cycle counts indicate that fewer cycles are executed per loop iteration, where
each cycle issues at most 4 instructions. The observed performance is indeed best for Poly3
VCR4.1.
To investigate the scalability of the performance the ILP-enhancement of the VCR4.1
code over interval [10, . . . , 106], we plotted the results in Figure 7.13 for the Poly3 benchmark.
The steady-state cycle count (Table 7.5) is clearly reﬂected in the performance diﬀerences
with VCR4.1 having the lowest steady-state cycle count (3) and highest performance (1.0
GFLOPS peak).
Overall, the VCR approach signiﬁcantly increases the ILP by decoupling factors d ≥1.
This beneﬁts the performance of modulo scheduling for superscalar RISC architectures, as
long as suﬃcient registers are available to hold the VCR coeﬃcients. The number of registers
needed for the VCR computation is d k, where k is the length of the CR form constructed
148

 0
 200
 400
 600
 800
 1000
 1200
 10
 100
 1000
 10000
 100000
 1e+06
MFlop/s
Dim n
Poly3 VCR4.1
Poly3 Scalar CR
Poly3 Scalar
Figure 7.13: Poly3 benchmark performance results of VCR-enhanced ILP-optimized code
(UltraSPARC IIIi 1.2GHz).
for a function.
7.4
Related Work
The CR formalism was originally developed by Zima [90] and applied to computer algebra
systems [47].
The formalism was improved by Bachmann, Zima, and Wang [69, 89] to
expedite the evaluation of multivariate functions on regular grids. Multivariate CR forms
(MCR) are nested CR forms that represent multivariate closed-form functions over sets of
index variables. Van Engelen [32, 17] extended the CR algebra by incorporating new rules
and techniques for induction variable detection and optimization in compilers.
Zima et al. [104] investigated parallel mappings to evaluate CR forms using coarse-grain
thread-level parallelism. A data partitioning approach is proposed to divide the iteration
domain into p sub-domains given p processors to speed up execution and reduce the error.
149

Several thread-level parallel execution strategies (functional parallel, data parallel, and
subdomain parallel) are compared for two CR forms. By contrast, our approach proposes a
decoupling strategy to exploit ﬁne-grain parallelism by using a CR translation technique to
generate independent value sequences, where the sequences are stored and updated in vector
registers. Our technique can be combined with thread-level parallelism of the outer block
loop (Figure 7.1) to further increase the execution speed of math function evaluations over
grids.
Zima et al. [104] concluded that the performance of “functional parallel” CR evaluation
is poor when using thread-level parallelism. In our work, despite reducing the overhead of
“functional parallel” execution of CR forms using eﬃcient shuﬄe-based SSE implementation,
we also found that the resulting shuﬄe-based CR vectorization is not competitive due to the
overhead of the tightly-coupled copy, shuﬄe, and add (or multiply) operations required for
each grid point.
The CORDIC [101] family of algorithms provide a fast method to evaluate transcendental
functions, roots, logs, and exponents for a single point. The CORDIC algorithm performs
a rotation using a series of speciﬁc incremental rotation angles to approach the target angle
and each step only requires addition, subtraction, bit-shift and table lookup. It is widely
used in pocket calculators and real-time systems since CORDIC is generally faster than
other approaches when a hardware multiplier is not available, or when the cost of the chip
need to be minimized. The Intel 80x87 coprocessor series until Intel 80486 all use CORDIC
algorithms [102]. These hardware advances are instrumental to reduce latencies that would
otherwise diminish the eﬀectiveness of our vectorization method.
Vector math libraries, such as Intel’s SVML and the Vector Math Library (VML) are
highly optimized for SIMD short vector execution [103].
SVML is developed to provide
eﬃcient software implementations for transcendental functions on packed ﬂoating-point
numbers with fully-accuracy [100] and is only intended for use by the Intel compiler
vectorizer.
VML is an application level library designed to compute math functions on
vector arguments [107].
By contrast to these highly optimized vector math libraries, VCR vectorization is not
restricted to math functions alone. A ﬂoating point expression composed of multiple math
functions can be transformed to a VCR and then optimized together, resulting in more
eﬃcient evaluation.
150

The main contributions of this chapter can be summarized as follows:
• The present CR formalism to expedite function evaluation over regular grids requires
independent operations across the iteration space, which prevents loop vectorization.
A new ﬁne-grain decoupling strategy is proposed to vectorize CR evaluation for eﬃcient
SIMD vector execution and enhanced ILP. The CR-based method is applicable to any
function deﬁned over a commutative ring (Z, +, ∗).
• A systematic performance evaluation of the proposed vectorization method is con-
ducted for polynomial and non-polynomial (exponential) classes of functions in the
real and complex domains and compared to existing work.
• A systematic performance evaluation of ILP-enhancing properties of the method is
conducted and the impact on modulo scheduling is analyzed.
• An analysis overview of roundoﬀerror properties of CR forms is given, with a
motivation for the decoupling strategy to slow the rate of error propagation.
• An auto-tuning approach is proposed to determine optimal decoupling factor and block
size for vector CR execution. This ensures optimality of performance and ﬂoating point
accuracy.
151

CHAPTER 8
CONCLUSION
This dissertation developed a new theoretical and algorithmic framework for improved
compiler analysis of loops, optimization of loops, and increased performance of closed-form
function evaluations over regular grids using a library generator approach. A new Inductive
SSA extension was proposed based on an the extended Chains of Recurrences CR# (CR
sharp) algebra. The Inductive SSA and CR# enhanced the recognition and manipulation of
ﬂow-sensitive loop-variant variables by compiler analysis methods.
For a practical implementation in a compiler to validate our approach, we developed a
CR# library and integrated it in GCC for constructing Inductive SSA. We used Inductive
SSA to improve congruence detection for partial redundancy elimination.
Experimental
results show that the Inductive SSA form increased the capabilities of the compiler to detect
congruences and improve the optimization of the SPEC benchmarks codes.
Also a toolset was developed to speed up closed-form function evaluations over grids by
vectorizing CR forms. Experimental results indicated that the loop-variant variable analysis
and the classic compiler optimizations are greatly enhanced by using Inductive SSA. The
performance of ﬂoating-point functions evaluations in loops was dramatically increased by
VCR vectorization compared to the state-of-the-art vector libraries such as Intel’s SVML.
Chapter 1 motivated our work by presenting related work and motivational examples to
illustrate the Inductive SSA form. The use of Inductive SSA for IV optimizations, congruence
detection, and array data dependence was discussed.
In Chapter 2, we introduced the preliminaries of the original CR formalism, the SSA
form, and a survey of induction variable analysis methods. A review of classic compiler
optimizations, which can be enhanced with Inductive SSA, was discussed.
In Chapter 3, we introduced our new extension, the CR# algebra, and explained the
152

purpose of the additions. We presented the new extended CR# simpliﬁcation rules which
handle the simpliﬁcation of CR expression with # operators, CR# lattice, monotonicity,
CR# alignment and bound mechanism which provides the foundation for constructing and
analyzing CR form for ﬂow-sensitive loop-variant variables. The CR# form is the basis
for describing and manipulating loop-variant variables. In addition, we discussed typed CR
forms to make CR# applicable to general-purpose compiler implementations that deal with
many diﬀerent typed (induction) variables. Then we introduced the Inductive SSA notation
and formalism. In Inductive SSA form, every scalar variables is attached with inductive
proofs represented by CR forms describing the value progression of this variable in the loop.
In Chapter 4, we presented the asymptotic linear-time Inductive SSA construction
algorithm to annotate the SSA form with inductive proofs containing CR# forms describing
semantic value progressions of scalar variables in a loop nest.
A uniﬁed framework to
generalize compiler analysis and optimization based on Inductive SSA was discussed.
In Chapter 5, we discussed the GCC compiler environment used to perform our ex-
periments.
Then we presented the implementation of Inductive SSA and the enhanced
optimizations in the GCC compiler environment.
In Chapter 6, experimental results were evaluated on the SPEC2000 benchmarks. The
results corroborate the claims and motivations for our work on Inductive SSA.
In Chapter 7, we presented a library generator based on a vectorization method that
uses an algebraic transformation of math functions to speed up the function evaluation over
structured grids, using vector representations of CR forms. This VCR representation allowed
for packing into short vector registers for eﬃcient execution. An auto-tuning approach for
the library ensured safe ﬂoating point evaluation. Experimental results showed improved
SIMD execution and ILP scheduling.
153

REFERENCES
[1] S. Muchnick. Advanced Compiler Design and Implementation. Morgan Kaufmann,
San Fransisco, CA, 1997. (document), 1.1, 1.1.1, 1.1.1, 1.1.2, 2.1, 2.2.1, 2.2.2, 2.3.1,
2.3.3, 2.3.4, 2.3.5, 2.3.6, 4.2.2, 4.2.4, 4.7, 4.2.4, 4.2.5, 4.2.6
[2] Mark N. Wegman and F. Kenneth Zadeck. Constant propagation with conditional
branches. ACM Trans. Program. Lang. Syst., 13(2):181–210, 1991. (document), 2.3.3,
4.2.4, 4.7
[3] Utpal Banerjee. Dependence Analysis for Supercomputing. Kluwer, Boston, 1988. 1.1,
1.1.3, 1.1.3
[4] M.P. Gerlek, E. Stolz, and M. Wolfe. Beyond induction variables: Detecting and classi-
fying sequences using a demand-driven SSA form. ACM Transactions on Programming
Languages and Systems (TOPLAS), 17(1):85–122, Jan 1995. 1.1, 1.1.1, 1.1.1, 1.1.2,
1.2, 2.2.1, 2.2.1, 2.2.2, 2.2.2, 2.2.2, 4.1.1, 4.2.1
[5] G. Goﬀ, Ken Kennedy, and C-W. Tseng. Practical dependence testing. In proceedings
of the ACM SIGPLAN ’91 Conference on Programming Language Design and Imple-
mentation (PLDI), volume 26, pages 15–29, Toronto, Ontario, Canada, June 1991. 1.1,
1.1.3
[6] P. Tu and D. Padua.
Gated SSA-based demand-driven symbolic analysis for par-
allelizing compilers.
In proceedings of the 9th ACM International Conference on
Supercomputing (ICS), pages 414–423, New York, Jul 1995. ACM Press.
1.1, 1.2,
2.1.2
[7] M.J. Wolfe. Beyond induction variables. In ACM SIGPLAN’92 Conf. on Programming
Language Design and Implementation, pages 162–174, San Fransisco, CA, 1992. 1.1,
1.1.1, 2.2.1, 2.2.2
[8] M.J. Wolfe. High Performance Compilers for Parallel Computers. Addison-Wesley,
Redwood City, CA, 1996. 1.1, 1.1.1, 1.1.3, 2.2.1, 2.2.2, 4.1.1
[9] John R. Allen and Ken Kennedy.
Optimizing compilers for modern architectures.
Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 2002. 1.1.1, 1.1.3, 7.1
[10] H. Zima and B. Chapman. Supercompilers for Parallel and Vector Computers. ACM
Press, New York, 1990. 1.1.1
154

[11] A. Aho, R. Sethi, and J. Ullman.
Compilers: Principles, Techniques and Tools.
Addison-Wesley Publishing Company, Reading MA, 1985. 1.1.1, 1.1.2, 1.1.2, 2.2.1,
2.2.2, 2.3.1, 2.3.2, 2.3.5, 2.4.3, 4.2.2, 7.1
[12] Mohammad R. Haghighat and Constantine D. Polychronopoulos. Symbolic analysis for
parallelizing compilers. ACM Transactions on Programming Languages and Systems,
18(4):477–518, July 1996. 1.1.1, 1.1.1, 1.1.1, 1.1.2, 1.1.3, 1.2, 2.2.1, 2.2.2, 2.2.2, 4.2.1
[13] W. Pottenger and R. Eigenmann.
Parallelization in the presence of generalized
induction and reduction variables.
Technical report, 1396, Univ. of Illinois at Ur-
banaChampaign, Center for Supercomputing Research & Development, 1995. 1.1.1,
2.2.1, 6.3
[14] R. Eigenmann, J. Hoeﬂinger, G. Jaxon, Z. Li, and D.A. Padua. Restructuring Fortran
programs for Cedar. In proceedings of ICPP’91, volume 1, pages 57–66, St. Charles,
Illinois, 1991. 1.1.1, 2.2.1
[15] Mohammad R. Haghighat.
Symbolic Analysis for Parallelizing Compilers.
Kluwer
Academic Publishers, 1995. 1.1.1, 2.2.1, 2.2.1
[16] Bj¨orn Franke and Michael O’boyle.
Array recovery and high-level transformations
for dsp applications. ACM Transactions on Embedded Computing Systems (TECS),
2(2):132–162, 2003. 1.1.1
[17] R.A. van Engelen and K.A. Gallivan.
An eﬃcient algorithm for pointer-to-array
access conversion for compiling and optimizing DSP applications. In proceedings of
the International Workshop on Innovative Architectures for Future Generation High-
Performance Processors and Systems (IWIA) 2001, pages 80–89, Maui, Hawaii, 2001.
1.1.1, 1.1.3, 7.2.2, 7.2.6, 7.4
[18] Rajiv Gupta.
A fresh look at optimizing array bound checking.
SIGPLAN Not.,
25(6):272–282, 1990. 1.1.1
[19] D. Andrade, M. Arenaz, B.B. Fraguela, J. Touri no, and R. Doallo.
Automated
and accurate cache behavior analysis for codes with irregular access patterns.
In
Concurrency and Computation: Practice and Experience (to appear), 2007. 1.1.1
[20] J. Birch, R.A. van Engelen, K.A. Gallivan, and Y. Shou. An empirical evaluation
of chains of recurrences for array dependence testing. In PACT ’06: Proceedings of
the 15th international conference on Parallel architectures and compilation techniques,
pages 295–304, New York, NY, USA, 2006. ACM Press. 1.1.1, 1.1.3, 1.2
[21] R.A. van Engelen, J. Birch, Y. Shou, B. Walsh, and K.A. Gallivan. A uniﬁed framework
for nonlinear dependence testing and symbolic analysis. In proceedings of the ACM
International Conference on Supercomputing (ICS), pages 106–115, 2004. 1.1.1, 1.1.3,
1.2, 2.2.2, 2.4, 3.1.5
155

[22] John H. Reif and Harry R. Lewis. Symbolic evaluation and the global value graph. In
POPL ’77: Proceedings of the 4th ACM SIGACT-SIGPLAN symposium on Principles
of programming languages, pages 104–118, 1977. 1.1.2
[23] B. Alpern, M. N. Wegman, and F. K. Zadeck.
Detecting equality of variables in
programs. In POPL ’88: Proceedings of the 15th ACM SIGPLAN-SIGACT symposium
on Principles of programming languages, pages 1–11, New York, NY, USA, 1988. ACM.
1.1.2, 2.3.6
[24] G.A. Kildall. A uniﬁed approach to global program optimization. In Proceedings of
the ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages
(POPL’73), pages 194–206, 1973. 1.1.2
[25] B. K. Rosen, M. N. Wegman, and F. K. Zadeck. Global value numbers and redundant
computations.
In POPL ’88:
Proceedings of the 15th ACM SIGPLAN-SIGACT
symposium on Principles of programming languages, pages 12–27, 1988. 1.1.2, 2.3.6,
4.2.6
[26] J. Knoop, O. Ruething, and B. Steﬀen. Lazy code motion. In Proceedings of the ACM
SIGPLAN ’92 Conference on Programming Language Design and Implementation,
volume 27, pages 224–234, San Francisco, CA, June 1992. 1.1.2, 2.3.6, 4.2.6
[27] E. Morel and C. Renvoise. Global optimization by suppression of partial redundancies.
Commun. ACM, 22(2):96–103, 1979. 1.1.2, 2.3.6, 4.2.6
[28] D. Whitﬁeld and M. L. Soﬀa. An approach to ordering optimizing transformations. In
PPOPP ’90: Proceedings of the second ACM SIGPLAN symposium on Principles &
practice of parallel programming, pages 137–146, 1990. 1.1.2
[29] L. Almagor, Keith D. Cooper, Alexander Grosul, Timothy J. Harvey, Steve Reeves,
Devika Subramanian, Linda Torczon, and Todd Waterman. Compilation order matters:
Exploring the structure of the space of compilation sequences using randomized search
algorithms.
In In Proceedings of the ACM SIGPLAN Symposium on Languages,
Compilers, and Tools for Embedded Systems (LCTES, pages 231–239, 2004. 1.1.2
[30] Prasad A. Kulkarni, David B. Whalley, Gary S. Tyson, and Jack W. Davidson. Exhaus-
tive optimization phase order space exploration. Code Generation and Optimization,
IEEE/ACM International Symposium on, 0:306–318, 2006. 1.1.2
[31] Thomas VanDrunen and Antony L Hosking. Value-based partial redundancy elimi-
nation. In In Proceedings of the International Conference on Compiler Construction,
2004. 1.1.2, 2.3.6, 4.2.6, 5.4, 6.4
[32] R.A. van Engelen, J. Birch, and K.A. Gallivan. Array data dependence testing with
the chains of recurrences algebra. In proceedings of the IEEE International Workshop
on Innovative Architectures for Future Generation High-Performance Processors and
Systems (IWIA), pages 70–81, January 2004. 1.1.2, 2.2.2, 2.4, 7.4
156

[33] R.A. van Engelen, J. Birch, Y. Shou, and K.A. Gallivan. Array data dependence testing
with the chains of recurrences algebra. Technical report, TR-041201, Computer Science
Dept., Florida State University, 2004. 1.1.2, 2.2.2, 2.4
[34] Karthik Gargi. A sparse algorithm for predicated global value numbering. In PLDI
’02: Proceedings of the ACM SIGPLAN 2002 Conference on Programming language
design and implementation, pages 45–56, New York, NY, USA, 2002. ACM. 1.1.2,
2.3.6
[35] Xiangyun Kong, David Klappholz, and Kleanthis Psarris. The I Test: An improved
dependence test for automatic parallelization and vectorization. IEEE Transactions
on Parallel and Distributed Systems, 2(3):342–349, 1991. 1.1.3
[36] Kleanthis Psarris, Xiangyun Kong, and David Klappholz. The direction vector I Test.
IEEE Transactions on Parallel and Distributed Systems, 4(11):1280–1290, November
1993. 1.1.3
[37] George B. Dantzig and B. Curtis Eaves. Fourier-motzkin elimination and its dual.
Journal of Combinatorial Theory, 14(14):288–297, 1973. 1.1.3
[38] William Pugh. The omega test: a fast and practical integer programming algorithm
for dependence analysis. In proceedings of Supercomputing, pages 4–13, 1991. 1.1.3
[39] Kleanthis Psarris. Program analysis techniques for transforming programs for parallel
systems. Parallel Computing, 28(3):455–469, 2003. 1.1.3
[40] Kleanthis Psarris. Program analysis techniques for transforming programs for parallel
systems. Parallel Computing, 28(3):455–469, 2003. 1.1.3
[41] Bj¨orn Franke and Michael O’Boyle. Compiler transformation of pointers to explicit
array accesses in DSP applications.
In proceedings of the ETAPS Conference on
Compiler Construction 2001, LNCS 2027, pages 69–85, 2001. 1.1.3
[42] P. Wu, A. Cohen, J. Hoeﬂinger, and D. Padua. Monotonic evolution: An alternative
to induction variable substitution for dependence analysis. In proceedings of the ACM
International Conference on Supercomputing (ICS), pages 78–91, 2001. 1.1.3, 2.2.1
[43] Zhiyu Shen, Zhiyuan Li, and Pen-Chung Yew. An empirical study on array subscripts
and data dependencies.
In proceedings of the International Conference on Parallel
Processing, volume 2, pages 145–152, 1989. 1.1.3
[44] J.-F. Collard, D. Barthou, and P. Feautrier.
Fuzzy array dataﬂow analysis.
In
proceedings of the ﬁfth ACM SIGPLAN Symposium on Principles and Practice of
Parallel Programming, pages 92–101, 1995. 1.1.3
[45] Radu Rugina and Martin Rinard.
Symbolic bounds analysis of array indices, and
accessed memory regions. In proceedings of the ACM SIGPLAN Conference on Pro-
gramming Language Design and Implementation (PLDI), pages 182–195, Vancouver,
British Columbia, Canada, June 2000. 1.1.3
157

[46] Karl J. Ottenstein, Robert A. Ballance, and Arthur B. MacCabe.
The program
dependence web:
a representation supporting control-, data-, and demand-driven
interpretation of imperative languages.
In PLDI ’90:
Proceedings of the ACM
SIGPLAN 1990 conference on Programming language design and implementation,
pages 257–271, New York, NY, USA, 1990. ACM Press. 1.2, 2.1.2
[47] E.V. Zima. Recurrent relations and speed-up of computations using computer algebra
systems. In proceedings of DISCO’92, pages 152–161. LNCS 721, 1992. 1.2, 2.2.2, 2.4,
2.4.3, 7.1, 7.2, 7.2.2, 7.2.2, 7.2.7, 7.4
[48] D. Berlin, D. Edelsohn, and S. Pop.
High-level loop optimizations for GCC.
In
Proceedings of the 2004 GCC Developers’ Summit, pages 37–54, 2004. 1.2, 2.2.2
[49] Robert van Engelen. Eﬃcient symbolic analysis for optimizing compilers. In proceedings
of the ETAPS Conference on Compiler Construction 2001, LNCS 2027, pages 118–132,
2001. 1.2, 2.2.1, 2.2.1, 2.2.2, 2.2.2, 2.2.2, 2.4, 2.4.3, 2.4.3
[50] R.A. van Engelen. Symbolic evaluation of chains of recurrences for loop optimization.
Technical report, TR-000102, Computer Science Dept., Florida State University, 2000.
1.2, 2.2.1, 2.2.1, 2.4.1, 2.4.3, 2.4.3, 3.1.3, 4.2.3, 7.1, 7.2, 7.2.2, 7.2.2
[51] Yixin Shou, Robert van Engelen, Johnnie Birch, and Kyle Gallivan. Toward eﬃcient
ﬂow-sensitive induction variable analysis and dependence testing for loop optimization.
In proceedings of the ACM SouthEast Conference, pages 1–6, 2006. 1.2
[52] Y. Shou, R. van Engelen, and J. Birch. Flow-sensitive loop-variant variable classiﬁca-
tion in linear time. In in the proceedings of the International Workshop on Languages
and Compilers for Parallel Computing (LCPC), pages 323–337, 2007. 1.2
[53] Yixin Shou, Robert van Engelen, and Johnnie Birch.
Flow-sensitive loop-variant
variable classiﬁcation in linear time. Technical report, TR-071005, Computer Science
Dept., Florida State University, 2007. 1.2
[54] Yixin Shou and Robert A. van Engelen. Automatic SIMD vectorization of chains of
recurrences. In in the proceedings of the ACM International Conference on Supercom-
puting (ICS), pages 245–255, 2008. 1.3
[55] Vicki H. Allan, Reese B. Jones, Randall M. Lee, and Stephen J. Allan.
Software
pipelining. ACM Computing Surveys, 27(3):367–432, 1995. 1.3, 7.1, 7.2.4
[56] Eugene V. Zima. On computational properties of chains of recurrences. In Proceedings
of the 2001 International Symposium on Symbolic and Algebraic Computation, page
345. ACM Press, 2001. 1.3, 7.1
[57] R. Cytron, J. Ferrante, B.K. Rosen, M.N. Wegman, and F.K. Zadeck.
Eﬃciently
computing static single assignment form and the control dependence graph.
ACM
Transactions on Programming Languages and Systems, 13:451–490, October 1991. 2.1,
2.1.1
158

[58] Silvius Rus, Guobin He, Christophe Alias, and Lawrence Rauchwerger. Region array
ssa.
In PACT ’06:
Proceedings of the 15th international conference on Parallel
architectures and compilation techniques, pages 43–52, New York, NY, USA, 2006.
ACM Press. 2.1.2
[59] Fred C. Chow, Sun Chan, Shin-Ming Liu, Raymond Lo, and Mark Streich. Eﬀective
representation of aliases and indirect memory operations in ssa form.
In CC ’96:
Proceedings of the 6th International Conference on Compiler Construction, pages 253–
267, London, UK, 1996. Springer-Verlag. 2.1.2
[60] Jin Lin, Tong Chen, Wei-Chung Hsu, Pen-Chung Yew, Roy Dz-Ching Ju, Tin-Fook
Ngai, and Sun Chan. A compiler framework for speculative analysis and optimizations.
In PLDI ’03: Proceedings of the ACM SIGPLAN 2003 conference on Programming
language design and implementation, pages 289–299, New York, NY, USA, 2003. ACM
Press. 2.1.2
[61] Jaejin Lee, David A. Padua, and Samuel P. Midkiﬀ.
Basic compiler algorithms
for parallel programs.
In PPoPP ’99: Proceedings of the seventh ACM SIGPLAN
symposium on Principles and practice of parallel programming, pages 1–12, New York,
NY, USA, 1999. ACM Press. 2.1.2
[62] R. Eigenmann, J. Hoeﬂinger, Z. Li, and D.A. Padua. Experience in the automatic
parallelization of four perfect-benchmark programs.
In 4th Annual Workshop on
Languages and Compilers for Parallel Computing, LNCS 589, pages 65–83, Santa
Clara, CA, 1991. Springer Verlag. 2.2.1
[63] Z. Ammerguallat and W.L. Harrison III. Automatic recognition of induction variables
and recurrence relations by abstract interpretation.
In proceedings of the ACM
SIGPLAN Conference on Programming Language Design and Implementation (PLDI),
pages 283–295, White Plains, NY, 1990. 2.2.1, 2.2.2, 2.2.2
[64] David A. Padua and Michael J. Wolfe. Advanced compiler optimizations for super-
computers. Commun. ACM, 29(12):1184–1201, 1986. 2.2.1
[65] M.R. Haghighat and C.D. Polychronopoulos. Symbolic program analysis and optimiza-
tion for parallelizing compilers. In 5th Annual Workshop on Languages and Compilers
for Parallel Computing, LNCS 757, pages 538–562, New Haven, Connecticut, 1992.
Springer Verlag. 2.2.1
[66] John Cocke and Ken Kennedy.
An algorithm for reduction of operator strength.
Communications of the ACM, 20(11):850–856, 1977. 2.2.2, 2.2.2
[67] Keith D. Cooper, L. Taylor Simpson, and Christopher A. Vick. Operator strength
reduction. Programming Languages and Systems, 23(5):603–625, 2001. 2.2.2, 2.3.2
[68] R.E. Tarjan.
Depth ﬁrst search and linear graph algorithms.
SIAM Journal of
Computing, 1(2):146–160, 1972. 2.2.2
159

[69] O. Bachmann. Chains of Recurrences. PhD thesis, Kent State University, College of
Arts and Sciences, 1996. 2.2.2, 2.4, 2.4.2, 2.4.3, 7.4
[70] R.A. van Engelen.
The CR# algebra and its application in loop analysis and
optimization.
Technical report, Computer Science Dept., Florida State University,
2004. 2.2.2, 4.2.1
[71] Sebastian Pop, Albert Cohen, and Georges andr Silber. Induction variable analysis
with delayed abstractions. In In 2005 International Conference on High Performance
Embedded Architectures and Compilers, pages 218–232. Springer-Verlag, 2005. 2.2.2
[72] F. Allen, J. Cocke, and K. Kennedy. Reduction of operator strength. In S. Muchnick
and N. Jones, editors, Program Flow Analysis, pages 79–101, New-Jersey, 1981.
Prentice-Hall. 2.3.2, 2.3.2
[73] Robert Kennedy, Fred C. Chow, Peter Dahl, Shin-Ming Liu, Raymond Lo, and Mark
Streich. Strength reduction via SSAPRE. In Computational Complexity, pages 144–
158, 1998. 2.3.2
[74] Robert Kennedy, Sun Chan, Shin-Ming Liu, Raymond Lo, Peng Tu, and Fred Chow.
Partial redundancy elimination in ssa form.
ACM Trans. Program. Lang. Syst.,
21(3):627–676, 1999. 2.3.2, 2.3.6, 4.2.6
[75] John Cocke. Global common subexpression elimination. SIGPLAN Not., 5(7):20–24,
1970. 2.3.5
[76] Markus M¨uller-Olm, Oliver R¨uthing, and Helmut Seidl. Checking herbrand equalities
and beyond. In VMCAI, pages 79–96, 2005. 2.3.6
[77] Karl-Heinz Drechsler and Manfred P. Stadel.
A variation of knoop, ruthing, and
steﬀen’s lazy code motion. SIGPLAN Not., 28(5):29–38, 1993. 2.3.6, 4.2.6
[78] Oliver R¨uthing, Jens Knoop, and Bernhard Steﬀen. Sparse code motion. In POPL
’00: Proceedings of the 27th ACM SIGPLAN-SIGACT symposium on Principles of
programming languages, pages 170–183, 2000. 2.3.6
[79] Antony L. Hosking, Nathaniel Nystrom, David Whitlock, Quintin Cutts, and Amer
Diwan.
Partial redundancy elimination for access path expressions.
Softw. Pract.
Exper., 31(6):577–600, 2001. 2.3.6
[80] Preston Briggs and Keith D. Cooper. Eﬀective partial redundancy elimination. In
PLDI ’94:
Proceedings of the ACM SIGPLAN 1994 conference on Programming
language design and implementation, pages 159–170, 1994. 2.3.6, 4.2.6
[81] CliﬀClick. Global code motion/global value numbering. SIGPLAN Not., 30(6):246–
257, 1995. 2.3.6, 4.2.6
[82] Jens Knoop, Oliver R¨uthing, and Bernhard Steﬀen. Code motion and code placement:
Just synonyms?
In ESOP ’98: Proceedings of the 7th European Symposium on
Programming, pages 154–169, London, UK, 1998. Springer-Verlag. 2.3.6, 4.2.6
160

[83] Preston Briggs, Keith D. Cooper, and L. Taylor Simpson. Value numbering. Softw.
Pract. Exper., 27(6):701–724, 1997. 2.3.6
[84] Loren Taylor Simpson. Value-driven redundancy elimination. PhD thesis, Houston,
TX, USA, 1996. Chair-Cooper,, Keith D. 2.3.6
[85] Sumit Gulwani and George C. Necula.
Global value numbering using random
interpretation. SIGPLAN Not., 39(1):342–352, 2004. 2.3.6
[86] Jiu-Tao Nie and Xu Cheng. An eﬃcient ssa-based algorithm for complete global value
numbering. In APLAS, pages 319–334, 2007. 2.3.6
[87] Rastisalv Bod´ık and Sadun Anik.
Path-sensitive value-ﬂow analysis.
In POPL
’98: Proceedings of the 25th ACM SIGPLAN-SIGACT symposium on Principles of
programming languages, pages 237–251, 1998. 2.3.6
[88] Rastislav Bod´ık, Rajiv Gupta, and Mary Lou Soﬀa. Complete removal of redundant
expressions. In PLDI ’98: Proceedings of the ACM SIGPLAN 1998 conference on
Programming language design and implementation, pages 1–14, 1998. 2.3.6
[89] O. Bachmann, P.S. Wang, and E.V. Zima. Chains of recurrences - a method to expedite
the evaluation of closed-form functions. In proceedings of the International Symposium
on Symbolic and Algebraic Computing (ISSAC), pages 242–249, Oxford, 1994. ACM.
2.4.3, 1, 7.1, 7.2, 7.2.2, 7.2.2, 7.2.7, 7.3.1, 7.4
[90] Eugene V. Zima. Automatic construction of systems of recurrence relations. USSR
Computational Mathematics and Mathematical Physics, 24(11-12):193–197, 1986.
2.4.3, 1, 7.4
[91] D.E. Knuth and P.B. Bendix. Simple word problems in universal algebras. In J. Leech,
editor, Computational Problems in Abstract Algebra, pages 263–297. Pergamon Press,
1970. 2
[92] J. Birch, R.A. van Engelen, and K.A. Gallivan. Value range analysis of conditionally
updated variables and pointers. In proceedings of Compilers for Parallel Computing
(CPC), pages 265–276, 2004. 3.1.5
[93] Johnnie L. Birch. Methods for linear and nonlinear array data dependence analysis with
the chains of recurrences algebra. PhD thesis, Florida State University, Tallahassee,
FL, USA, 2007. 3.1.6, 3.1.6
[94] D. Novillo. Tree SSA: A new optimization infrastructure for GCC. In Proceedings of
the 2003 GCC Developers’ Summit, pages 181–193, 2003. 5.1, 5.1.2, 5.1.2, 5.2
[95] The GNU Homepage. Available from http://www.gnu.org. 5.1.1
[96] Rtl representation. Available from http://gcc.gnu.org/onlinedocs/gccint/RTL.html.
5.1.2
161

[97] Gcc internal manual. Available from http://gcc.gnu.org/onlinedocs/. 5.1.2
[98] SPEC
CPU2000
benchmark
homepage.
Available
from
http://www.spec.org/cpu2000/. 6.1
[99] W. Blume, R. Doallo, R. Eigenmann, J. Grout, J. Hoeﬂinger, T. Lawrence, J. Lee,
D. Padua, Y. Paek, B. Pottenger, L. Rauchwerger, and P. Tu. Advanced program re-
structuring for high-performance computers with Polaris. IEEE Computer, 29(12):78–
82, 1996. 6.3
[100] Aart J. C. Bik. Software Vectorization Handbook, The: Applying Intel Multimedia
Extensions for Maximum Performance. Intel Press, 2004. 7.1, 7.3.3, 7.4
[101] J. E. Volder.
The cordic trigonometric computing technique.
IRE Transactions
Electronic Computers, EC-8:330–334, September 1959. 7.1, 7.4
[102] A. K. Yuen. Intel’s ﬂoating-point processors. In Electro/88 Conference Record, pages
48/5/1–7, 1988. 7.1, 7.4
[103] L. Kylex. How to Avoid Bottlenecks in Simple Math Functions, 2007. Available from
http://softwarecommunity.intel.com/articles/eng/ 3524.htm, Intel 2004. 7.1, 7.4
[104] Eugene V. Zima, Karthi R. Vadivelu, and Thomas L. Casavant. Mapping techniques
for parallel evaluation of chains of recurrences. In IPPS ’96: Proceedings of the 10th
International Parallel Processing Symposium, pages 620–624, Washington, DC, USA,
1996. IEEE Computer Society. 7.2.3, 7.2.5, 7.3.1, 7.4
[105] R.A. van Engelen, L. Wolters, and G. Cats.
Ctadel:
A generator of multi-
platform high performance codes for PDE-based scientiﬁc applications. In 10th ACM
International Conference on Supercomputing, pages 86–93, New York, 1996. ACM
Press. 7.2.6
[106] SWI-Prolog’s Home. Available from http://www.swi-prolog.org/. 7.2.6
[107] Intel Math Kernel Library Reference Manual, March 2007. Intel Document number:
630813-025US. 7.4
162

BIOGRAPHICAL SKETCH
Yixin Shou
Yixin Shou completed her B.S. degree in Computer Science at Inner Mongolia University,
China in 1997. She received her M.S. degree in Computer Science at Peking University,
China in 2000. In the fall of 2002, she came to Florida State University to pursue her PhD
in the Department of Computer Science.
Yixin’s research interests include compiler optimizations, high performance computing
and MPEG-4 encoding parallelization.
163

