Introduction to Inductive Logic Programming
Manoel V. M. França
Department of Computing
City University London
March 26, 2012 / Machine Learning Group Meeting
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
1 / 57

Outline
1
Introduction
Motivation
Objectives
Overview
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
2 / 57

Outline
1
Introduction
Motivation
Objectives
Overview
2
Background Knowledge
Propositional (Classical) Logic
First–Order Logic
Logic Programming
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
2 / 57

Outline
1
Introduction
Motivation
Objectives
Overview
2
Background Knowledge
Propositional (Classical) Logic
First–Order Logic
Logic Programming
3
Inductive Logic Programming
Deﬁnitions
Progol
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
2 / 57

Outline
1
Introduction
Motivation
Objectives
Overview
2
Background Knowledge
Propositional (Classical) Logic
First–Order Logic
Logic Programming
3
Inductive Logic Programming
Deﬁnitions
Progol
4
Neural–Symbolic Systems
Introduction
C–IL2P
CILP++
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
2 / 57

Outline
1
Introduction
Motivation
Objectives
Overview
2
Background Knowledge
Propositional (Classical) Logic
First–Order Logic
Logic Programming
3
Inductive Logic Programming
Deﬁnitions
Progol
4
Neural–Symbolic Systems
Introduction
C–IL2P
CILP++
5
Conclusion
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
2 / 57

Introduction
Motivation
Outline
1
Introduction
Motivation
Objectives
Overview
2
Background Knowledge
Propositional (Classical) Logic
First–Order Logic
Logic Programming
3
Inductive Logic Programming
Deﬁnitions
Progol
4
Neural–Symbolic Systems
Introduction
C–IL2P
CILP++
5
Conclusion
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
2 / 57

Introduction
Motivation
Thinking and Explaining
Imagine a father teaching his little daughter how to drink a glass of
water
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
3 / 57

Introduction
Motivation
Thinking and Explaining
Imagine a father teaching his little daughter how to drink a glass of
water
He needs to use a proper language to teach her each step involved
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
3 / 57

Introduction
Motivation
Thinking and Explaining
Imagine a father teaching his little daughter how to drink a glass of
water
He needs to use a proper language to teach her each step involved
It should be as clear as possible
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
3 / 57

Introduction
Motivation
Thinking and Explaining
Imagine a father teaching his little daughter how to drink a glass of
water
He needs to use a proper language to teach her each step involved
It should be as clear as possible
The representation of each object should be good enough for that
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
3 / 57

Introduction
Motivation
Thinking and Explaining
Imagine a father teaching his little daughter how to drink a glass of
water
He needs to use a proper language to teach her each step involved
It should be as clear as possible
The representation of each object should be good enough for that
Which would ﬁt best?
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
3 / 57

Introduction
Motivation
Thinking and Explaining
Imagine a father teaching his little daughter how to drink a glass of
water
He needs to use a proper language to teach her each step involved
It should be as clear as possible
The representation of each object should be good enough for that
Which would ﬁt best?
“To drink water, you need to grab a glass and use a sink”
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
3 / 57

Introduction
Motivation
Thinking and Explaining
Imagine a father teaching his little daughter how to drink a glass of
water
He needs to use a proper language to teach her each step involved
It should be as clear as possible
The representation of each object should be good enough for that
Which would ﬁt best?
“To drink water, you need to grab a glass and use a sink”
water, glass ∈[0, 1], threshold = 2,
drinking = ((water + glass) ≥threshold)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
3 / 57

Introduction
Motivation
Symbolic Arguments
“The Neural Networks did perform well (...). However, (...) they
consumed enormous amounts of CPU time and they are
sometimes equaled by simple symbolic classiﬁers” (Weiss and
Kapouleas, 1989)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
4 / 57

Introduction
Motivation
Symbolic Arguments
“The Neural Networks did perform well (...). However, (...) they
consumed enormous amounts of CPU time and they are
sometimes equaled by simple symbolic classiﬁers” (Weiss and
Kapouleas, 1989)
“Structured knowledge, is difﬁcult to represent in Neural Networks,
contrary to traditional logical models” (Toiviainen, 2000)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
4 / 57

Introduction
Motivation
Symbolic Arguments
“The Neural Networks did perform well (...). However, (...) they
consumed enormous amounts of CPU time and they are
sometimes equaled by simple symbolic classiﬁers” (Weiss and
Kapouleas, 1989)
“Structured knowledge, is difﬁcult to represent in Neural Networks,
contrary to traditional logical models” (Toiviainen, 2000)
“Attempts have been made to explain the behavior of
connectionist networks (...). These explanations are, however, at
the level of primitive features of the network (...). Explanations on
a higher level of knowledge are difﬁcult to achieve” (Toiviainen,
2000)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
4 / 57

Introduction
Motivation
Connectionistic Arguments
“Connectionist networks are robust. (...) They are resistant to
noise and gracefully degrade when they are damaged or
overloaded with information” (Smolensky et al., 1992)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
5 / 57

Introduction
Motivation
Connectionistic Arguments
“Connectionist networks are robust. (...) They are resistant to
noise and gracefully degrade when they are damaged or
overloaded with information” (Smolensky et al., 1992)
“Neural Networks are capable of extracting signiﬁcant features
from the training set and using them to process a novel input
pattern, thus generalizing better” (Toiviainen, 2000)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
5 / 57

Introduction
Motivation
Connectionistic Arguments
“Connectionist networks are robust. (...) They are resistant to
noise and gracefully degrade when they are damaged or
overloaded with information” (Smolensky et al., 1992)
“Neural Networks are capable of extracting signiﬁcant features
from the training set and using them to process a novel input
pattern, thus generalizing better” (Toiviainen, 2000)
“Differently from (symbolic) machine learning, (numeric) neural
networks perform inductive learning in such a way that the
statistical characteristics of the data are encoded in their sets of
weights” (Garcez et al., 2009)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
5 / 57

Introduction
Motivation
Which Means...
Both paradigms has its own strengths and weaknesses
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
6 / 57

Introduction
Motivation
Which Means...
Both paradigms has its own strengths and weaknesses
Both are usually better suited on different kinds of applications
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
6 / 57

Introduction
Motivation
Which Means...
Both paradigms has its own strengths and weaknesses
Both are usually better suited on different kinds of applications
Both are important and relevant to a complete reasoning
model
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
6 / 57

Introduction
Motivation
The World Without Logic (1)
With Logic:
IF (I Study) AND NOT (Evil Teacher) 
                                   → (I Will Pass) 
Without Logic (eg. one-layer NN):
Wstudy, pass = 0.989;
WevilTeacher, pass = -0.966
Knowledge Representation: 
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
7 / 57

Introduction
Motivation
The World Without Logic (2)
   I am sleepy, 
I am a little tired, 
  I am not ok....
5.5345,  
0.5643e123, 
-9.7424...
With Logic
Without Logic
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
8 / 57

Introduction
Objectives
Outline
1
Introduction
Motivation
Objectives
Overview
2
Background Knowledge
Propositional (Classical) Logic
First–Order Logic
Logic Programming
3
Inductive Logic Programming
Deﬁnitions
Progol
4
Neural–Symbolic Systems
Introduction
C–IL2P
CILP++
5
Conclusion
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
8 / 57

Introduction
Objectives
This Talk’s Goals
Formally introduce Inductive Logic Programming (ILP) and its
theoretical foundations
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
9 / 57

Introduction
Objectives
This Talk’s Goals
Formally introduce Inductive Logic Programming (ILP) and its
theoretical foundations
Give an overall “feeling” of how it works
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
9 / 57

Introduction
Objectives
This Talk’s Goals
Formally introduce Inductive Logic Programming (ILP) and its
theoretical foundations
Give an overall “feeling” of how it works
Brieﬂy point out some alternative applications of ILP
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
9 / 57

Introduction
Overview
Outline
1
Introduction
Motivation
Objectives
Overview
2
Background Knowledge
Propositional (Classical) Logic
First–Order Logic
Logic Programming
3
Inductive Logic Programming
Deﬁnitions
Progol
4
Neural–Symbolic Systems
Introduction
C–IL2P
CILP++
5
Conclusion
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
9 / 57

Introduction
Overview
Remainder Of The Talk
Background Knowledge, brieﬂy explaining Propositional and
First–Order Logics
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
10 / 57

Introduction
Overview
Remainder Of The Talk
Background Knowledge, brieﬂy explaining Propositional and
First–Order Logics
Inductive Logic Programming, which will show the basic ILP
concept
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
10 / 57

Introduction
Overview
Remainder Of The Talk
Background Knowledge, brieﬂy explaining Propositional and
First–Order Logics
Inductive Logic Programming, which will show the basic ILP
concept
Some Relevant Systems that uses ILP, introducing the
Connectionist and Inductive Learning and Logic Programming,
C–IL2P
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
10 / 57

Introduction
Overview
Remainder Of The Talk
Background Knowledge, brieﬂy explaining Propositional and
First–Order Logics
Inductive Logic Programming, which will show the basic ILP
concept
Some Relevant Systems that uses ILP, introducing the
Connectionist and Inductive Learning and Logic Programming,
C–IL2P
Conclusion, to enclose everything that has been presented and
add some ﬁnal remarks
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
10 / 57

Background Knowledge
Classical Logic
Outline
1
Introduction
Motivation
Objectives
Overview
2
Background Knowledge
Propositional (Classical) Logic
First–Order Logic
Logic Programming
3
Inductive Logic Programming
Deﬁnitions
Progol
4
Neural–Symbolic Systems
Introduction
C–IL2P
CILP++
5
Conclusion
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
10 / 57

Background Knowledge
Classical Logic
Types of Reasoning
Deductive Reasoning: given a background theory, what is
possible to be inferred from it?
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
11 / 57

Background Knowledge
Classical Logic
Types of Reasoning
Deductive Reasoning: given a background theory, what is
possible to be inferred from it?
Inductive Reasoning: given a background theory and a set of
examples, what kinds of new theories can be inferred?
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
11 / 57

Background Knowledge
Classical Logic
Types of Reasoning
Deductive Reasoning: given a background theory, what is
possible to be inferred from it?
Inductive Reasoning: given a background theory and a set of
examples, what kinds of new theories can be inferred?
Abductive Reasoning: given a background theory and a set of
examples, what kinds of facts can explain them?
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
11 / 57

Background Knowledge
Classical Logic
Types of Reasoning
Deductive Reasoning: given a background theory, what is
possible to be inferred from it?
Inductive Reasoning: given a background theory and a set of
examples, what kinds of new theories can be inferred?
Abductive Reasoning: given a background theory and a set of
examples, what kinds of facts can explain them?
Deductive systems are clearly different from the other two, but
inductive and abductive ones are somewhat similar
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
11 / 57

Background Knowledge
Classical Logic
Types of Reasoning
Deductive Reasoning: given a background theory, what is
possible to be inferred from it?
Inductive Reasoning: given a background theory and a set of
examples, what kinds of new theories can be inferred?
Abductive Reasoning: given a background theory and a set of
examples, what kinds of facts can explain them?
Deductive systems are clearly different from the other two, but
inductive and abductive ones are somewhat similar
In fact, under certain circumstances, an inductive task can be
transformed into an abductive one and vice–versa
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
11 / 57

Background Knowledge
Classical Logic
Syntax
Atom: upper–case letter (P, Q, R, ...), ⊥or T
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
12 / 57

Background Knowledge
Classical Logic
Syntax
Atom: upper–case letter (P, Q, R, ...), ⊥or T
Logical Negation: ¬
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
12 / 57

Background Knowledge
Classical Logic
Syntax
Atom: upper–case letter (P, Q, R, ...), ⊥or T
Logical Negation: ¬
Literal: atom, preceded (negative literal) or not (positive literal)
by ¬
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
12 / 57

Background Knowledge
Classical Logic
Syntax
Atom: upper–case letter (P, Q, R, ...), ⊥or T
Logical Negation: ¬
Literal: atom, preceded (negative literal) or not (positive literal)
by ¬
Connectives:
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
12 / 57

Background Knowledge
Classical Logic
Syntax
Atom: upper–case letter (P, Q, R, ...), ⊥or T
Logical Negation: ¬
Literal: atom, preceded (negative literal) or not (positive literal)
by ¬
Connectives:
∧(and): P ∧Q
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
12 / 57

Background Knowledge
Classical Logic
Syntax
Atom: upper–case letter (P, Q, R, ...), ⊥or T
Logical Negation: ¬
Literal: atom, preceded (negative literal) or not (positive literal)
by ¬
Connectives:
∧(and): P ∧Q
() (parenthesis): (P ∧Q)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
12 / 57

Background Knowledge
Classical Logic
Syntax
Atom: upper–case letter (P, Q, R, ...), ⊥or T
Logical Negation: ¬
Literal: atom, preceded (negative literal) or not (positive literal)
by ¬
Connectives:
∧(and): P ∧Q
() (parenthesis): (P ∧Q)
∨(or): R ∨(P ∧Q)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
12 / 57

Background Knowledge
Classical Logic
Syntax
Atom: upper–case letter (P, Q, R, ...), ⊥or T
Logical Negation: ¬
Literal: atom, preceded (negative literal) or not (positive literal)
by ¬
Connectives:
∧(and): P ∧Q
() (parenthesis): (P ∧Q)
∨(or): R ∨(P ∧Q)
→(implication): (R ∨(P ∧Q)) →S
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
12 / 57

Background Knowledge
Classical Logic
Syntax
Atom: upper–case letter (P, Q, R, ...), ⊥or T
Logical Negation: ¬
Literal: atom, preceded (negative literal) or not (positive literal)
by ¬
Connectives:
∧(and): P ∧Q
() (parenthesis): (P ∧Q)
∨(or): R ∨(P ∧Q)
→(implication): (R ∨(P ∧Q)) →S
↔(double–implication): ((R ∨(P ∧Q)) →S) ↔T
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
12 / 57

Background Knowledge
Classical Logic
Syntax
Atom: upper–case letter (P, Q, R, ...), ⊥or T
Logical Negation: ¬
Literal: atom, preceded (negative literal) or not (positive literal)
by ¬
Connectives:
∧(and): P ∧Q
() (parenthesis): (P ∧Q)
∨(or): R ∨(P ∧Q)
→(implication): (R ∨(P ∧Q)) →S
↔(double–implication): ((R ∨(P ∧Q)) →S) ↔T
Clause: one or more literals connected through zero or more
connectives, eg. ((R ∨(P ∧Q)) →S) ↔T
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
12 / 57

Background Knowledge
Classical Logic
Syntax
Atom: upper–case letter (P, Q, R, ...), ⊥or T
Logical Negation: ¬
Literal: atom, preceded (negative literal) or not (positive literal)
by ¬
Connectives:
∧(and): P ∧Q
() (parenthesis): (P ∧Q)
∨(or): R ∨(P ∧Q)
→(implication): (R ∨(P ∧Q)) →S
↔(double–implication): ((R ∨(P ∧Q)) →S) ↔T
Clause: one or more literals connected through zero or more
connectives, eg. ((R ∨(P ∧Q)) →S) ↔T
Theory: set of one or more clauses, representing a knowledge
domain
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
12 / 57

Background Knowledge
Classical Logic
Semantics
An atom can be assigned true or false
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
13 / 57

Background Knowledge
Classical Logic
Semantics
An atom can be assigned true or false
Given a clause C, a clause interpretation for C consists of
truth–value assignments for each of its atoms:
IC: PC
1 , . . ., PC
l 7→{true, false}|l|
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
13 / 57

Background Knowledge
Classical Logic
Semantics
An atom can be assigned true or false
Given a clause C, a clause interpretation for C consists of
truth–value assignments for each of its atoms:
IC: PC
1 , . . ., PC
l 7→{true, false}|l|
Given a theory B = C1, ... Cm, an interpretation for B is an
assignment of truth values for each of its atoms:
IB: PC1
1 , . . ., PCm
n
7→{true, false}|n|
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
13 / 57

Background Knowledge
Classical Logic
Semantics
An atom can be assigned true or false
Given a clause C, a clause interpretation for C consists of
truth–value assignments for each of its atoms:
IC: PC
1 , . . ., PC
l 7→{true, false}|l|
Given a theory B = C1, ... Cm, an interpretation for B is an
assignment of truth values for each of its atoms:
IB: PC1
1 , . . ., PCm
n
7→{true, false}|n|
Models are interpretations that assigns truth values to a given
clause or theory:
M(C) = I: I(C) 7→true :: M(B) = I: I(B) 7→true
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
13 / 57

Background Knowledge
Classical Logic
Semantics
An atom can be assigned true or false
Given a clause C, a clause interpretation for C consists of
truth–value assignments for each of its atoms:
IC: PC
1 , . . ., PC
l 7→{true, false}|l|
Given a theory B = C1, ... Cm, an interpretation for B is an
assignment of truth values for each of its atoms:
IB: PC1
1 , . . ., PCm
n
7→{true, false}|n|
Models are interpretations that assigns truth values to a given
clause or theory:
M(C) = I: I(C) 7→true :: M(B) = I: I(B) 7→true
Interpretations are often represented in a truth–table format
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
13 / 57

Background Knowledge
Classical Logic
Connectives Truth Table
PS: All “v” symbols inside the table are true values
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
14 / 57

Background Knowledge
Classical Logic
Logical Consequence
A clause C is a logical consequence of a theory B if and only if
M(B) ⊆M(C) (B |= C)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
15 / 57

Background Knowledge
Classical Logic
Logical Consequence
A clause C is a logical consequence of a theory B if and only if
M(B) ⊆M(C) (B |= C)
Logical consequence is also known as entailment: B entails C if
and only if M(B) ⊆M(C)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
15 / 57

Background Knowledge
Classical Logic
Logical Consequence
A clause C is a logical consequence of a theory B if and only if
M(B) ⊆M(C) (B |= C)
Logical consequence is also known as entailment: B entails C if
and only if M(B) ⊆M(C)
C is satisﬁable if M(C) ̸= ∅
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
15 / 57

Background Knowledge
Classical Logic
Logical Consequence
A clause C is a logical consequence of a theory B if and only if
M(B) ⊆M(C) (B |= C)
Logical consequence is also known as entailment: B entails C if
and only if M(B) ⊆M(C)
C is satisﬁable if M(C) ̸= ∅
refutational consequence: B |= C if and only if B ∪{¬ C} is not
satisﬁable
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
15 / 57

Background Knowledge
Classical Logic
Deductive Systems
A deductive system DS = (L, AX, R) is composed by:
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
16 / 57

Background Knowledge
Classical Logic
Deductive Systems
A deductive system DS = (L, AX, R) is composed by:
L: used language
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
16 / 57

Background Knowledge
Classical Logic
Deductive Systems
A deductive system DS = (L, AX, R) is composed by:
L: used language
AX: logical axioms set
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
16 / 57

Background Knowledge
Classical Logic
Deductive Systems
A deductive system DS = (L, AX, R) is composed by:
L: used language
AX: logical axioms set
R: a set of inference rules
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
16 / 57

Background Knowledge
Classical Logic
Deductive Systems
A deductive system DS = (L, AX, R) is composed by:
L: used language
AX: logical axioms set
R: a set of inference rules
A proof of a clause C is a set of clauses DB, on the system DS
(DB ⊢C), if and only if, exists a ﬁnite sequence of clauses (D1, . . .,
Dn) which holds:
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
16 / 57

Background Knowledge
Classical Logic
Deductive Systems
A deductive system DS = (L, AX, R) is composed by:
L: used language
AX: logical axioms set
R: a set of inference rules
A proof of a clause C is a set of clauses DB, on the system DS
(DB ⊢C), if and only if, exists a ﬁnite sequence of clauses (D1, . . .,
Dn) which holds:
1
Dn = C
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
16 / 57

Background Knowledge
Classical Logic
Deductive Systems
A deductive system DS = (L, AX, R) is composed by:
L: used language
AX: logical axioms set
R: a set of inference rules
A proof of a clause C is a set of clauses DB, on the system DS
(DB ⊢C), if and only if, exists a ﬁnite sequence of clauses (D1, . . .,
Dn) which holds:
1
Dn = C
2
For each i ∈[1, n], one of the following conditions is satisﬁed:
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
16 / 57

Background Knowledge
Classical Logic
Deductive Systems
A deductive system DS = (L, AX, R) is composed by:
L: used language
AX: logical axioms set
R: a set of inference rules
A proof of a clause C is a set of clauses DB, on the system DS
(DB ⊢C), if and only if, exists a ﬁnite sequence of clauses (D1, . . .,
Dn) which holds:
1
Dn = C
2
For each i ∈[1, n], one of the following conditions is satisﬁed:
Di is an instance of AX
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
16 / 57

Background Knowledge
Classical Logic
Deductive Systems
A deductive system DS = (L, AX, R) is composed by:
L: used language
AX: logical axioms set
R: a set of inference rules
A proof of a clause C is a set of clauses DB, on the system DS
(DB ⊢C), if and only if, exists a ﬁnite sequence of clauses (D1, . . .,
Dn) which holds:
1
Dn = C
2
For each i ∈[1, n], one of the following conditions is satisﬁed:
Di is an instance of AX
Di ∈DB
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
16 / 57

Background Knowledge
Classical Logic
Deductive Systems
A deductive system DS = (L, AX, R) is composed by:
L: used language
AX: logical axioms set
R: a set of inference rules
A proof of a clause C is a set of clauses DB, on the system DS
(DB ⊢C), if and only if, exists a ﬁnite sequence of clauses (D1, . . .,
Dn) which holds:
1
Dn = C
2
For each i ∈[1, n], one of the following conditions is satisﬁed:
Di is an instance of AX
Di ∈DB
There exists j, k < i in which Di can be obtained by applying rules of
R
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
16 / 57

Background Knowledge
Classical Logic
Resolution
Based on refutational consequence
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
17 / 57

Background Knowledge
Classical Logic
Resolution
Based on refutational consequence
Deﬁnition:
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
17 / 57

Background Knowledge
Classical Logic
Resolution
Based on refutational consequence
Deﬁnition:
L: propositional clauses in conjunctive normal form:
(R ∨(P ∧Q)) →S ⇒(¬R ∨S) ∧(¬P ∨S) ∧(¬Q ∨S)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
17 / 57

Background Knowledge
Classical Logic
Resolution
Based on refutational consequence
Deﬁnition:
L: propositional clauses in conjunctive normal form:
(R ∨(P ∧Q)) →S ⇒(¬R ∨S) ∧(¬P ∨S) ∧(¬Q ∨S)
AX: ∅
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
17 / 57

Background Knowledge
Classical Logic
Resolution
Based on refutational consequence
Deﬁnition:
L: propositional clauses in conjunctive normal form:
(R ∨(P ∧Q)) →S ⇒(¬R ∨S) ∧(¬P ∨S) ∧(¬Q ∨S)
AX: ∅
R: propositional resolution – {A ∨B; C ∨¬B} ⇒{A ∨C}
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
17 / 57

Background Knowledge
First–Order Logic
Outline
1
Introduction
Motivation
Objectives
Overview
2
Background Knowledge
Propositional (Classical) Logic
First–Order Logic
Logic Programming
3
Inductive Logic Programming
Deﬁnitions
Progol
4
Neural–Symbolic Systems
Introduction
C–IL2P
CILP++
5
Conclusion
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
17 / 57

Background Knowledge
First–Order Logic
Description
Propositional Logic direct extension, to deal with predicates
(relations)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
18 / 57

Background Knowledge
First–Order Logic
Description
Propositional Logic direct extension, to deal with predicates
(relations)
Able to conclude particular features of general characteristics over
elements of a given domain
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
18 / 57

Background Knowledge
First–Order Logic
Description
Propositional Logic direct extension, to deal with predicates
(relations)
Able to conclude particular features of general characteristics over
elements of a given domain
Able to conclude general features from particularities of single
elements of a given domain
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
18 / 57

Background Knowledge
First–Order Logic
Description
Propositional Logic direct extension, to deal with predicates
(relations)
Able to conclude particular features of general characteristics over
elements of a given domain
Able to conclude general features from particularities of single
elements of a given domain
To achieve that, two quantiﬁers were included into propositional
logic: ∀(universal) and ∃(existential)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
18 / 57

Background Knowledge
First–Order Logic
Syntax Modiﬁcations
Terms are consisted of
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
19 / 57

Background Knowledge
First–Order Logic
Syntax Modiﬁcations
Terms are consisted of
Functional symbols ⇒starts with lower–case and can have 0 or
more terms (f(X, a))
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
19 / 57

Background Knowledge
First–Order Logic
Syntax Modiﬁcations
Terms are consisted of
Functional symbols ⇒starts with lower–case and can have 0 or
more terms (f(X, a))
Constants ⇒facts, deﬁned by functions of arity 0 (f)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
19 / 57

Background Knowledge
First–Order Logic
Syntax Modiﬁcations
Terms are consisted of
Functional symbols ⇒starts with lower–case and can have 0 or
more terms (f(X, a))
Constants ⇒facts, deﬁned by functions of arity 0 (f)
Variables ⇒represented by upper–case letters (X)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
19 / 57

Background Knowledge
First–Order Logic
Syntax Modiﬁcations
Terms are consisted of
Functional symbols ⇒starts with lower–case and can have 0 or
more terms (f(X, a))
Constants ⇒facts, deﬁned by functions of arity 0 (f)
Variables ⇒represented by upper–case letters (X)
Predicates ⇒upper–case relation between terms (P(X))
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
19 / 57

Background Knowledge
First–Order Logic
Syntax Modiﬁcations
Terms are consisted of
Functional symbols ⇒starts with lower–case and can have 0 or
more terms (f(X, a))
Constants ⇒facts, deﬁned by functions of arity 0 (f)
Variables ⇒represented by upper–case letters (X)
Predicates ⇒upper–case relation between terms (P(X))
Quantiﬁers:
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
19 / 57

Background Knowledge
First–Order Logic
Syntax Modiﬁcations
Terms are consisted of
Functional symbols ⇒starts with lower–case and can have 0 or
more terms (f(X, a))
Constants ⇒facts, deﬁned by functions of arity 0 (f)
Variables ⇒represented by upper–case letters (X)
Predicates ⇒upper–case relation between terms (P(X))
Quantiﬁers:
∀(universal): speciﬁes features that are valid for every individual of
the domain
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
19 / 57

Background Knowledge
First–Order Logic
Syntax Modiﬁcations
Terms are consisted of
Functional symbols ⇒starts with lower–case and can have 0 or
more terms (f(X, a))
Constants ⇒facts, deﬁned by functions of arity 0 (f)
Variables ⇒represented by upper–case letters (X)
Predicates ⇒upper–case relation between terms (P(X))
Quantiﬁers:
∀(universal): speciﬁes features that are valid for every individual of
the domain
∃(existential): speciﬁes features that are valid for at least one
individual of the domain
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
19 / 57

Background Knowledge
First–Order Logic
Syntax Modiﬁcations
Terms are consisted of
Functional symbols ⇒starts with lower–case and can have 0 or
more terms (f(X, a))
Constants ⇒facts, deﬁned by functions of arity 0 (f)
Variables ⇒represented by upper–case letters (X)
Predicates ⇒upper–case relation between terms (P(X))
Quantiﬁers:
∀(universal): speciﬁes features that are valid for every individual of
the domain
∃(existential): speciﬁes features that are valid for at least one
individual of the domain
Clause: now can have variable quantiﬁed by one leftmost
quantiﬁer, eg. ∀x((R(X) ∨(P(X) ∧q)) →s)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
19 / 57

Background Knowledge
First–Order Logic
Semantics Modiﬁcations
An atom can be assigned one value of a given domain D of
instantiations (an assigned atom is called grounded)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
20 / 57

Background Knowledge
First–Order Logic
Semantics Modiﬁcations
An atom can be assigned one value of a given domain D of
instantiations (an assigned atom is called grounded)
Given a clause C, a clause interpretation for C consists of
assignments for each of its atoms
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
20 / 57

Background Knowledge
First–Order Logic
Semantics Modiﬁcations
An atom can be assigned one value of a given domain D of
instantiations (an assigned atom is called grounded)
Given a clause C, a clause interpretation for C consists of
assignments for each of its atoms
Given a theory B = {C1, ... Cm}, an interpretation for B is an
assignment values for each of its atoms
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
20 / 57

Background Knowledge
First–Order Logic
Semantics Modiﬁcations
An atom can be assigned one value of a given domain D of
instantiations (an assigned atom is called grounded)
Given a clause C, a clause interpretation for C consists of
assignments for each of its atoms
Given a theory B = {C1, ... Cm}, an interpretation for B is an
assignment values for each of its atoms
Models are interpretations that assigns truth values to a given
clause or theory
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
20 / 57

Background Knowledge
First–Order Logic
First–Order Resolution
Each ﬁrst–order clause passes through a standardization process
called Skolemization
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
21 / 57

Background Knowledge
First–Order Logic
First–Order Resolution
Each ﬁrst–order clause passes through a standardization process
called Skolemization
Makes use of uniﬁers (substitution θ that makes C1θ = C2θ)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
21 / 57

Background Knowledge
First–Order Logic
First–Order Resolution
Each ﬁrst–order clause passes through a standardization process
called Skolemization
Makes use of uniﬁers (substitution θ that makes C1θ = C2θ)
First–Order propositional resolution rule is identical to the
propositional one, but ﬁrst uniﬁes each term
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
21 / 57

Background Knowledge
First–Order Logic
First–Order Resolution
Each ﬁrst–order clause passes through a standardization process
called Skolemization
Makes use of uniﬁers (substitution θ that makes C1θ = C2θ)
First–Order propositional resolution rule is identical to the
propositional one, but ﬁrst uniﬁes each term
Besides the proof, ﬁrst–order resolution returns all used
uniﬁcations
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
21 / 57

Background Knowledge
First–Order Logic
First–Order Resolution Step
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
22 / 57

Background Knowledge
First–Order Logic
First–Order Induction
Inverse Resolution: “backwards” resolution, from the leaves to the
root(s). Notable system: Cigol
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
23 / 57

Background Knowledge
First–Order Logic
First–Order Induction
Inverse Resolution: “backwards” resolution, from the leaves to the
root(s). Notable system: Cigol
New inference rule (given a clause C1 with an literal A, we want to
ﬁnd C2 with ¬ A):
(Resolvent −(C1 −{A})θ1)θ−1
2
∪{¬Aθ1θ−1
2 }
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
23 / 57

Background Knowledge
First–Order Logic
First–Order Induction
Inverse Resolution: “backwards” resolution, from the leaves to the
root(s). Notable system: Cigol
New inference rule (given a clause C1 with an literal A, we want to
ﬁnd C2 with ¬ A):
(Resolvent −(C1 −{A})θ1)θ−1
2
∪{¬Aθ1θ−1
2 }
Immediate problem: whereas you are “going up” on inverse
resolution, search space increases drastically
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
23 / 57

Background Knowledge
Logic Programming
Outline
1
Introduction
Motivation
Objectives
Overview
2
Background Knowledge
Propositional (Classical) Logic
First–Order Logic
Logic Programming
3
Inductive Logic Programming
Deﬁnitions
Progol
4
Neural–Symbolic Systems
Introduction
C–IL2P
CILP++
5
Conclusion
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
23 / 57

Background Knowledge
Logic Programming
Concepts
Restriction to the ﬁrst–order characterization to a more
computationally convenient language
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
24 / 57

Background Knowledge
Logic Programming
Concepts
Restriction to the ﬁrst–order characterization to a more
computationally convenient language
Deﬁnite clauses of the form ∀X1,...,Xn(h ∨b1 ∨. . . ∨bn) becomes
h ←b1 ∨. . . ∨bn
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
24 / 57

Background Knowledge
Logic Programming
Concepts
Restriction to the ﬁrst–order characterization to a more
computationally convenient language
Deﬁnite clauses of the form ∀X1,...,Xn(h ∨b1 ∨. . . ∨bn) becomes
h ←b1 ∨. . . ∨bn
h: head of the clause
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
24 / 57

Background Knowledge
Logic Programming
Concepts
Restriction to the ﬁrst–order characterization to a more
computationally convenient language
Deﬁnite clauses of the form ∀X1,...,Xn(h ∨b1 ∨. . . ∨bn) becomes
h ←b1 ∨. . . ∨bn
h: head of the clause
b1 ∨. . . ∨bn: bodies of the clause, each bi is called condition
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
24 / 57

Background Knowledge
Logic Programming
Concepts
Restriction to the ﬁrst–order characterization to a more
computationally convenient language
Deﬁnite clauses of the form ∀X1,...,Xn(h ∨b1 ∨. . . ∨bn) becomes
h ←b1 ∨. . . ∨bn
h: head of the clause
b1 ∨. . . ∨bn: bodies of the clause, each bi is called condition
Fact: deﬁnite clause with empty body
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
24 / 57

Background Knowledge
Logic Programming
Concepts
Restriction to the ﬁrst–order characterization to a more
computationally convenient language
Deﬁnite clauses of the form ∀X1,...,Xn(h ∨b1 ∨. . . ∨bn) becomes
h ←b1 ∨. . . ∨bn
h: head of the clause
b1 ∨. . . ∨bn: bodies of the clause, each bi is called condition
Fact: deﬁnite clause with empty body
Goal: clause with non–empty body but no head
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
24 / 57

Background Knowledge
Logic Programming
Concepts
Restriction to the ﬁrst–order characterization to a more
computationally convenient language
Deﬁnite clauses of the form ∀X1,...,Xn(h ∨b1 ∨. . . ∨bn) becomes
h ←b1 ∨. . . ∨bn
h: head of the clause
b1 ∨. . . ∨bn: bodies of the clause, each bi is called condition
Fact: deﬁnite clause with empty body
Goal: clause with non–empty body but no head
Deﬁnite Program: conjunction of deﬁnite clauses
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
24 / 57

Background Knowledge
Logic Programming
Concepts
Restriction to the ﬁrst–order characterization to a more
computationally convenient language
Deﬁnite clauses of the form ∀X1,...,Xn(h ∨b1 ∨. . . ∨bn) becomes
h ←b1 ∨. . . ∨bn
h: head of the clause
b1 ∨. . . ∨bn: bodies of the clause, each bi is called condition
Fact: deﬁnite clause with empty body
Goal: clause with non–empty body but no head
Deﬁnite Program: conjunction of deﬁnite clauses
The semantical domain used for most logic programs is the
Herbrand Universe
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
24 / 57

Background Knowledge
Logic Programming
Prolog
PROgramming in LOGic: one of the pioneers of Logic
Programming
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
25 / 57

Background Knowledge
Logic Programming
Prolog
PROgramming in LOGic: one of the pioneers of Logic
Programming
Makes deduction through a variation of the classic resolution
algorithm, called SLD–Resolution
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
25 / 57

Background Knowledge
Logic Programming
Prolog
PROgramming in LOGic: one of the pioneers of Logic
Programming
Makes deduction through a variation of the classic resolution
algorithm, called SLD–Resolution
SLD–Resolution differs from classical resolution by deﬁning which
clauses will be resolved
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
25 / 57

Background Knowledge
Logic Programming
Monotonic and Nonmonotonic Logic Programming
Logical entailment is monotonic (additions into a theory never
decreases the amount of possible consequences)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
26 / 57

Background Knowledge
Logic Programming
Monotonic and Nonmonotonic Logic Programming
Logical entailment is monotonic (additions into a theory never
decreases the amount of possible consequences)
An way to add nonmotonicity into a logic program is to use
Closed–World Assumption (CWA):
“If an literal of a goal is not proved by the current theory, then it is
false”
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
26 / 57

Background Knowledge
Logic Programming
Monotonic and Nonmonotonic Logic Programming
Logical entailment is monotonic (additions into a theory never
decreases the amount of possible consequences)
An way to add nonmotonicity into a logic program is to use
Closed–World Assumption (CWA):
“If an literal of a goal is not proved by the current theory, then it is
false”
This rule is called Negation as Failure
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
26 / 57

Background Knowledge
Logic Programming
Monotonic and Nonmonotonic Logic Programming
Logical entailment is monotonic (additions into a theory never
decreases the amount of possible consequences)
An way to add nonmotonicity into a logic program is to use
Closed–World Assumption (CWA):
“If an literal of a goal is not proved by the current theory, then it is
false”
This rule is called Negation as Failure
An extension to resolution called SLD–NF has been created to
deal with this kind of deduction
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
26 / 57

Background Knowledge
Logic Programming
Monotonic and Nonmonotonic Logic Programming
Logical entailment is monotonic (additions into a theory never
decreases the amount of possible consequences)
An way to add nonmotonicity into a logic program is to use
Closed–World Assumption (CWA):
“If an literal of a goal is not proved by the current theory, then it is
false”
This rule is called Negation as Failure
An extension to resolution called SLD–NF has been created to
deal with this kind of deduction
CWA causes nonmotonicity
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
26 / 57

Background Knowledge
Logic Programming
CWA Example
Consider the following theory:
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
27 / 57

Background Knowledge
Logic Programming
CWA Example
Consider the following theory:
If a goal literal Trip(cambridge, train) is queried with regard to B, a
positive answer will be given
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
27 / 57

Background Knowledge
Logic Programming
CWA Example
Consider the following theory:
If a goal literal Trip(cambridge, train) is queried with regard to B, a
positive answer will be given
Otherwise, if it is asked for Trip(paris, airplane), two answers can
be obtained, depending if CWA is being used
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
27 / 57

Background Knowledge
Logic Programming
CWA Example
Consider the following theory:
If a goal literal Trip(cambridge, train) is queried with regard to B, a
positive answer will be given
Otherwise, if it is asked for Trip(paris, airplane), two answers can
be obtained, depending if CWA is being used
If it is not being used nothing will be answered (in Prolog, a “fail”
message would be shown)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
27 / 57

Background Knowledge
Logic Programming
CWA Example
Consider the following theory:
If a goal literal Trip(cambridge, train) is queried with regard to B, a
positive answer will be given
Otherwise, if it is asked for Trip(paris, airplane), two answers can
be obtained, depending if CWA is being used
If it is not being used nothing will be answered (in Prolog, a “fail”
message would be shown)
If it is used, this query would return false
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
27 / 57

Inductive Logic Programming
Deﬁnitions
Outline
1
Introduction
Motivation
Objectives
Overview
2
Background Knowledge
Propositional (Classical) Logic
First–Order Logic
Logic Programming
3
Inductive Logic Programming
Deﬁnitions
Progol
4
Neural–Symbolic Systems
Introduction
C–IL2P
CILP++
5
Conclusion
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
27 / 57

Inductive Logic Programming
Deﬁnitions
Introduction
Inductive Logic Programming (ILP) is a machine learning
technique that conducts supervised inductive concept learning
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
28 / 57

Inductive Logic Programming
Deﬁnitions
Introduction
Inductive Logic Programming (ILP) is a machine learning
technique that conducts supervised inductive concept learning
Which means: given a set of labeled examples E and a
background knowledge B, an ILP system will try to ﬁnd a
hypothesis function H that minimizes a speciﬁed loss(B ∪H, E)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
28 / 57

Inductive Logic Programming
Deﬁnitions
Introduction
Inductive Logic Programming (ILP) is a machine learning
technique that conducts supervised inductive concept learning
Which means: given a set of labeled examples E and a
background knowledge B, an ILP system will try to ﬁnd a
hypothesis function H that minimizes a speciﬁed loss(B ∪H, E)
In ILP context:
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
28 / 57

Inductive Logic Programming
Deﬁnitions
Introduction
Inductive Logic Programming (ILP) is a machine learning
technique that conducts supervised inductive concept learning
Which means: given a set of labeled examples E and a
background knowledge B, an ILP system will try to ﬁnd a
hypothesis function H that minimizes a speciﬁed loss(B ∪H, E)
In ILP context:
B: pre–existent deﬁnite program
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
28 / 57

Inductive Logic Programming
Deﬁnitions
Introduction
Inductive Logic Programming (ILP) is a machine learning
technique that conducts supervised inductive concept learning
Which means: given a set of labeled examples E and a
background knowledge B, an ILP system will try to ﬁnd a
hypothesis function H that minimizes a speciﬁed loss(B ∪H, E)
In ILP context:
B: pre–existent deﬁnite program
E: set of grounded atoms of target concept(s), in which labels are
truth–values
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
28 / 57

Inductive Logic Programming
Deﬁnitions
Introduction
Inductive Logic Programming (ILP) is a machine learning
technique that conducts supervised inductive concept learning
Which means: given a set of labeled examples E and a
background knowledge B, an ILP system will try to ﬁnd a
hypothesis function H that minimizes a speciﬁed loss(B ∪H, E)
In ILP context:
B: pre–existent deﬁnite program
E: set of grounded atoms of target concept(s), in which labels are
truth–values
H: target deﬁnite program that entails most examples of E
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
28 / 57

Inductive Logic Programming
Deﬁnitions
Introduction
Inductive Logic Programming (ILP) is a machine learning
technique that conducts supervised inductive concept learning
Which means: given a set of labeled examples E and a
background knowledge B, an ILP system will try to ﬁnd a
hypothesis function H that minimizes a speciﬁed loss(B ∪H, E)
In ILP context:
B: pre–existent deﬁnite program
E: set of grounded atoms of target concept(s), in which labels are
truth–values
H: target deﬁnite program that entails most examples of E
loss(H, E): f(number of examples entailed by B ∪U)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
28 / 57

Inductive Logic Programming
Deﬁnitions
Introduction
Inductive Logic Programming (ILP) is a machine learning
technique that conducts supervised inductive concept learning
Which means: given a set of labeled examples E and a
background knowledge B, an ILP system will try to ﬁnd a
hypothesis function H that minimizes a speciﬁed loss(B ∪H, E)
In ILP context:
B: pre–existent deﬁnite program
E: set of grounded atoms of target concept(s), in which labels are
truth–values
H: target deﬁnite program that entails most examples of E
loss(H, E): f(number of examples entailed by B ∪U)
The obtained H not only classiﬁes new examples but can improve
an existing theory
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
28 / 57

Inductive Logic Programming
Deﬁnitions
Task Formalization
An ILP task can be deﬁned in two ways, depending on the type of
learning
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
29 / 57

Inductive Logic Programming
Deﬁnitions
Task Formalization
An ILP task can be deﬁned in two ways, depending on the type of
learning
If learning from entailment, it is deﬁned as a tuple <E, B, L>,
where:
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
29 / 57

Inductive Logic Programming
Deﬁnitions
Task Formalization
An ILP task can be deﬁned in two ways, depending on the type of
learning
If learning from entailment, it is deﬁned as a tuple <E, B, L>,
where:
E: set of positive and negative literals (examples)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
29 / 57

Inductive Logic Programming
Deﬁnitions
Task Formalization
An ILP task can be deﬁned in two ways, depending on the type of
learning
If learning from entailment, it is deﬁned as a tuple <E, B, L>,
where:
E: set of positive and negative literals (examples)
B: logic program that deﬁnes the background knowledge underlying
the task
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
29 / 57

Inductive Logic Programming
Deﬁnitions
Task Formalization
An ILP task can be deﬁned in two ways, depending on the type of
learning
If learning from entailment, it is deﬁned as a tuple <E, B, L>,
where:
E: set of positive and negative literals (examples)
B: logic program that deﬁnes the background knowledge underlying
the task
L: set of logic theories that restricts the search space to ﬁnd a
suitable hypothesis (language bias)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
29 / 57

Inductive Logic Programming
Deﬁnitions
Task Formalization
An ILP task can be deﬁned in two ways, depending on the type of
learning
If learning from entailment, it is deﬁned as a tuple <E, B, L>,
where:
E: set of positive and negative literals (examples)
B: logic program that deﬁnes the background knowledge underlying
the task
L: set of logic theories that restricts the search space to ﬁnd a
suitable hypothesis (language bias)
If learning from interpretation, it is also deﬁned as <E, B, L>,
where:
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
29 / 57

Inductive Logic Programming
Deﬁnitions
Task Formalization
An ILP task can be deﬁned in two ways, depending on the type of
learning
If learning from entailment, it is deﬁned as a tuple <E, B, L>,
where:
E: set of positive and negative literals (examples)
B: logic program that deﬁnes the background knowledge underlying
the task
L: set of logic theories that restricts the search space to ﬁnd a
suitable hypothesis (language bias)
If learning from interpretation, it is also deﬁned as <E, B, L>,
where:
E: set of Herbrand Interpretations of B, labeled positive and
negative as well
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
29 / 57

Inductive Logic Programming
Deﬁnitions
Task Formalization
An ILP task can be deﬁned in two ways, depending on the type of
learning
If learning from entailment, it is deﬁned as a tuple <E, B, L>,
where:
E: set of positive and negative literals (examples)
B: logic program that deﬁnes the background knowledge underlying
the task
L: set of logic theories that restricts the search space to ﬁnd a
suitable hypothesis (language bias)
If learning from interpretation, it is also deﬁned as <E, B, L>,
where:
E: set of Herbrand Interpretations of B, labeled positive and
negative as well
B: logic program that deﬁnes the background knowledge underlying
the task
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
29 / 57

Inductive Logic Programming
Deﬁnitions
Task Formalization
An ILP task can be deﬁned in two ways, depending on the type of
learning
If learning from entailment, it is deﬁned as a tuple <E, B, L>,
where:
E: set of positive and negative literals (examples)
B: logic program that deﬁnes the background knowledge underlying
the task
L: set of logic theories that restricts the search space to ﬁnd a
suitable hypothesis (language bias)
If learning from interpretation, it is also deﬁned as <E, B, L>,
where:
E: set of Herbrand Interpretations of B, labeled positive and
negative as well
B: logic program that deﬁnes the background knowledge underlying
the task
L: language bias
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
29 / 57

Inductive Logic Programming
Deﬁnitions
Hypothesis Search Types
ILP systems can build hypothesis following two directions:
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
30 / 57

Inductive Logic Programming
Deﬁnitions
Hypothesis Search Types
ILP systems can build hypothesis following two directions:
Bottom–Up: starting with the most speciﬁc clause (⊥),
generalizations can be made to make it cover the positive examples
while keeping most negative ones out
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
30 / 57

Inductive Logic Programming
Deﬁnitions
Hypothesis Search Types
ILP systems can build hypothesis following two directions:
Bottom–Up: starting with the most speciﬁc clause (⊥),
generalizations can be made to make it cover the positive examples
while keeping most negative ones out
Top–Down: starting with the most general clause (h ←),
specializations can be made to eliminate negative examples from
the coverage set while maintaining positive ones covered
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
30 / 57

Inductive Logic Programming
Deﬁnitions
ILP General Algorithm
Most ILP systems are based in a Sequential–Covering algorithm:
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
31 / 57

Inductive Logic Programming
Deﬁnitions
ILP General Algorithm
Most ILP systems are based in a Sequential–Covering algorithm:
REFINE is a function that varies between ILP systems and
chooses or removes bodies from a candidate hypothesis
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
31 / 57

Inductive Logic Programming
Deﬁnitions
Quick Note
ILP systems relies on specialization and generalization operators
to build hypothesis
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
32 / 57

Inductive Logic Programming
Deﬁnitions
Quick Note
ILP systems relies on specialization and generalization operators
to build hypothesis
The way they work and are used depends on the ILP system
being used, as well as the language bias L
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
32 / 57

Inductive Logic Programming
Deﬁnitions
Quick Note
ILP systems relies on specialization and generalization operators
to build hypothesis
The way they work and are used depends on the ILP system
being used, as well as the language bias L
For convenience, Progol has been chosen as the system which
will illustrate how these dependencies works
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
32 / 57

Inductive Logic Programming
Progol
Outline
1
Introduction
Motivation
Objectives
Overview
2
Background Knowledge
Propositional (Classical) Logic
First–Order Logic
Logic Programming
3
Inductive Logic Programming
Deﬁnitions
Progol
4
Neural–Symbolic Systems
Introduction
C–IL2P
CILP++
5
Conclusion
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
32 / 57

Inductive Logic Programming
Progol
A Brief Summary of Progol
Progol is an ILP system and a Machine Learning algorithm, based
on Inverse Entailment and Sequential Covering, which searches
for suited hypothesis in a search space bounded by the most
general clause (h ←) and a Bottom–Clause (⊥)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
33 / 57

Inductive Logic Programming
Progol
A Brief Summary of Progol
Progol is an ILP system and a Machine Learning algorithm, based
on Inverse Entailment and Sequential Covering, which searches
for suited hypothesis in a search space bounded by the most
general clause (h ←) and a Bottom–Clause (⊥)
Inverse Entailment: B ∪H |= E ⇒B |= H →E ⇒B |= ¬ E →¬ H
⇒B ∪¬ E |= ¬ H (H |= ⊥)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
33 / 57

Inductive Logic Programming
Progol
A Brief Summary of Progol
Progol is an ILP system and a Machine Learning algorithm, based
on Inverse Entailment and Sequential Covering, which searches
for suited hypothesis in a search space bounded by the most
general clause (h ←) and a Bottom–Clause (⊥)
Inverse Entailment: B ∪H |= E ⇒B |= H →E ⇒B |= ¬ E →¬ H
⇒B ∪¬ E |= ¬ H (H |= ⊥)
Sequential Covering: the search for an hypothesis starts from (h ←)
and adds iteratively literals that covers the positive examples and
do not covers most of the negative ones
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
33 / 57

Inductive Logic Programming
Progol
A Brief Summary of Progol
Progol is an ILP system and a Machine Learning algorithm, based
on Inverse Entailment and Sequential Covering, which searches
for suited hypothesis in a search space bounded by the most
general clause (h ←) and a Bottom–Clause (⊥)
Inverse Entailment: B ∪H |= E ⇒B |= H →E ⇒B |= ¬ E →¬ H
⇒B ∪¬ E |= ¬ H (H |= ⊥)
Sequential Covering: the search for an hypothesis starts from (h ←)
and adds iteratively literals that covers the positive examples and
do not covers most of the negative ones
Bottom–Clause: most–speciﬁc clause of a restricted space, deﬁned
by a single example and L
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
33 / 57

Inductive Logic Programming
Progol
A Brief Summary of Progol
Progol is an ILP system and a Machine Learning algorithm, based
on Inverse Entailment and Sequential Covering, which searches
for suited hypothesis in a search space bounded by the most
general clause (h ←) and a Bottom–Clause (⊥)
Inverse Entailment: B ∪H |= E ⇒B |= H →E ⇒B |= ¬ E →¬ H
⇒B ∪¬ E |= ¬ H (H |= ⊥)
Sequential Covering: the search for an hypothesis starts from (h ←)
and adds iteratively literals that covers the positive examples and
do not covers most of the negative ones
Bottom–Clause: most–speciﬁc clause of a restricted space, deﬁned
by a single example and L
It uses the following loss function:
lossprogol(E, B, H) = + P
r∈H |r| −|{e ∈E+ : B ∪H |= e}| + |{e′ ∈
E−: B ∪H |= e′}|
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
33 / 57

Inductive Logic Programming
Progol
Language Bias Structure
Progol has two language bias structures:
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
34 / 57

Inductive Logic Programming
Progol
Language Bias Structure
Progol has two language bias structures:
Mode Declarations: can be head declarations – modeh(recall, s), or
body declarations – modeb(recall, s), where recall controls the
number of instantiations of a literal and s is a ground positive or
negative literal, with placemarkers which deﬁnes if it is an input (+),
output (–) or a constant (#), and its type
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
34 / 57

Inductive Logic Programming
Progol
Language Bias Structure
Progol has two language bias structures:
Mode Declarations: can be head declarations – modeh(recall, s), or
body declarations – modeb(recall, s), where recall controls the
number of instantiations of a literal and s is a ground positive or
negative literal, with placemarkers which deﬁnes if it is an input (+),
output (–) or a constant (#), and its type
Determinations: used to specify which bodies can be used to build
a clause with a given head
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
34 / 57

Inductive Logic Programming
Progol
Language Bias Structure
Progol has two language bias structures:
Mode Declarations: can be head declarations – modeh(recall, s), or
body declarations – modeb(recall, s), where recall controls the
number of instantiations of a literal and s is a ground positive or
negative literal, with placemarkers which deﬁnes if it is an input (+),
output (–) or a constant (#), and its type
Determinations: used to specify which bodies can be used to build
a clause with a given head
Examples:
modeh(1, mother_in_law(+woman, −man))
modeb(∗, progenitor_of(+woman, −woman))
modeb(1, wife_of(+woman, −man))
determination(mother_in_law/2, progenitor_of/2)
determination(mother_in_law/2, wife_of/2)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
34 / 57

Inductive Logic Programming
Progol
Specialization and Generalization Operators
Uses θ −subsumption: a clause C θ −subsumes D (C ≺θ D) if
exists a substitution θ in which Cθ ⊆D holds, eg.:
C: f(A, B) ←p(B, G), q(G, A) θ −subsumes
D: f(a, b) ←p(b, g), q(g, a), t(a, d) through θ = {A/a, B/b, G/g}
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
35 / 57

Inductive Logic Programming
Progol
Specialization and Generalization Operators
Uses θ −subsumption: a clause C θ −subsumes D (C ≺θ D) if
exists a substitution θ in which Cθ ⊆D holds, eg.:
C: f(A, B) ←p(B, G), q(G, A) θ −subsumes
D: f(a, b) ←p(b, g), q(g, a), t(a, d) through θ = {A/a, B/b, G/g}
If C ≺θ D, then C |= D
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
35 / 57

Inductive Logic Programming
Progol
Specialization and Generalization Operators
Uses θ −subsumption: a clause C θ −subsumes D (C ≺θ D) if
exists a substitution θ in which Cθ ⊆D holds, eg.:
C: f(A, B) ←p(B, G), q(G, A) θ −subsumes
D: f(a, b) ←p(b, g), q(g, a), t(a, d) through θ = {A/a, B/b, G/g}
If C ≺θ D, then C |= D
This means that for a specialization or generalization of a clause
C, C ≺θ ⊥ensures that the search space bounds still holds
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
35 / 57

Inductive Logic Programming
Progol
Variable Chaining
In Progol, every body input variable needs to be an input of a
head literal or an output of a previous body literal
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
36 / 57

Inductive Logic Programming
Progol
Variable Chaining
In Progol, every body input variable needs to be an input of a
head literal or an output of a previous body literal
An example:
modeh(*, mult(+real, +real,-real))
modeb(*, dec(+real,-real))
modeb(*, plus(+real, +real,-real))
determination(mult/3, mult/3)
determination(mult/3, dec/2)
determination(mult/3, plus/3)
C = mult(E,F,G) ←dec(E, H), mult(F, H, I), plus(F, I, G).
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
36 / 57

Inductive Logic Programming
Progol
Bottom–Clause
In Progol, the search space for a candidate hypothesis starts from
(h ←) and specializations are added in order to minimize its loss
function.
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
37 / 57

Inductive Logic Programming
Progol
Bottom–Clause
In Progol, the search space for a candidate hypothesis starts from
(h ←) and specializations are added in order to minimize its loss
function.
This search is limited by a special bottom–clause
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
37 / 57

Inductive Logic Programming
Progol
Bottom–Clause
In Progol, the search space for a candidate hypothesis starts from
(h ←) and specializations are added in order to minimize its loss
function.
This search is limited by a special bottom–clause
It is built from a chosen example by applying iteratively
substitutions to match all possible modeb literals, in top–down
order, according to its modeh deﬁnition, background knowledge
and determination restrictions
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
37 / 57

Inductive Logic Programming
Progol
Bottom–Clause
In Progol, the search space for a candidate hypothesis starts from
(h ←) and specializations are added in order to minimize its loss
function.
This search is limited by a special bottom–clause
It is built from a chosen example by applying iteratively
substitutions to match all possible modeb literals, in top–down
order, according to its modeh deﬁnition, background knowledge
and determination restrictions
It uses a deepness (d) input parameter, which controls the
amount of cycles through modeb literals
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
37 / 57

Inductive Logic Programming
Progol
Bottom–Clause
In Progol, the search space for a candidate hypothesis starts from
(h ←) and specializations are added in order to minimize its loss
function.
This search is limited by a special bottom–clause
It is built from a chosen example by applying iteratively
substitutions to match all possible modeb literals, in top–down
order, according to its modeh deﬁnition, background knowledge
and determination restrictions
It uses a deepness (d) input parameter, which controls the
amount of cycles through modeb literals
After built, it is then variabilized to be used as a specialization
boundary
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
37 / 57

Inductive Logic Programming
Progol
Example
Input: example: mult(2, 4, 8), maxDeepness = 1
Modes:
modeh(*, mult(+real, +real, -real))
modeb(*, dec(+real, -real))
modeb(*, plus(+real, +real, -real))
Background Knowledge:
dec(2, 4); dec(2, 5); plus(2, 2, 4); plus(2, 4, 6);
Possible terms to use: ∅
Other terms: ∅
⊥= ∅
Current deepness: 0
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
38 / 57

Inductive Logic Programming
Progol
Example
Input: example: mult(2, 4, 8), maxDeepness = 1
Modes:
modeh(*, mult(+real, +real, -real))
modeb(*, dec(+real, -real))
modeb(*, plus(+real, +real, -real))
Background Knowledge:
dec(2, 4); dec(2, 5); plus(2, 2, 4); plus(2, 4, 6);
Possible terms to use: A(2), B(4)
Other terms: C(8)
⊥= mult(A, B, C) ←
Current deepness: 0
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
39 / 57

Inductive Logic Programming
Progol
Example
Input: example: mult(2, 4, 8), maxDeepness = 1
Modes:
modeh(*, mult(+real, +real, -real))
modeb(*, dec(+real, -real))
modeb(*, plus(+real, +real, -real))
Background Knowledge:
dec(2, 4); dec(2, 5); plus(2, 2, 4); plus(2, 4, 6);
Possible terms to use: A(2), B(4), D(5)
Other terms: C(8)
⊥= mult(A, B, C) ←dec(A, B), dec(A, D)
Current deepness: 0
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
40 / 57

Inductive Logic Programming
Progol
Example
Input: example: mult(2, 4, 8), maxDeepness = 1
Modes:
modeh(*, mult(+real, +real, -real))
modeb(*, dec(+real, -real))
modeb(*, plus(+real, +real, -real))
Background Knowledge:
dec(2, 4); dec(2, 5); plus(2, 2, 4); plus(2, 4, 6);
Possible terms to use: A(2), B(4), D(5)
Other terms: C(8)
⊥= mult(A, B, C) ←dec(A, B), dec(A, D), plus(A, A, B)
Current deepness: 0
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
41 / 57

Inductive Logic Programming
Progol
Example
Input: example: mult(2, 4, 8), maxDeepness = 1
Modes:
modeh(*, mult(+real, +real, -real))
modeb(*, dec(+real, -real))
modeb(*, plus(+real, +real, -real))
Background Knowledge:
dec(2, 4); dec(2, 5); plus(2, 2, 4); plus(2, 4, 6);
Possible terms to use: A(2), B(4), D(5), E(6)
Output terms: C(8)
⊥= mult(A, B, C) ←dec(A, B), dec(A, D), plus(A, A, B), plus(A, B, E)
Current deepness: 1 (algorithm stop)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
42 / 57

Inductive Logic Programming
Progol
Specialization Operator
An order on ⊥is assumed and let ⊥(k) be the k–th element in ⊥
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
43 / 57

Inductive Logic Programming
Progol
Specialization Operator
An order on ⊥is assumed and let ⊥(k) be the k–th element in ⊥
Since H ≺θ ⊥, there must be a substitution θ such that for each
literal h in H, there is a literal l such that hθ = l
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
43 / 57

Inductive Logic Programming
Progol
Specialization Operator
An order on ⊥is assumed and let ⊥(k) be the k–th element in ⊥
Since H ≺θ ⊥, there must be a substitution θ such that for each
literal h in H, there is a literal l such that hθ = l
The substitution operator δ can be deﬁned as:
< P(v1, . . . , vm, θm) >∈δ(θ, k) if and only if
– P(u1, . . . , um) is the k–th literal in ⊥
– θ0 =θ
– if vj/uj ∈θj−1 then θj = θj−1 for 0 < j ≤m
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
43 / 57

Inductive Logic Programming
Progol
Specialization Operator
An order on ⊥is assumed and let ⊥(k) be the k–th element in ⊥
Since H ≺θ ⊥, there must be a substitution θ such that for each
literal h in H, there is a literal l such that hθ = l
The substitution operator δ can be deﬁned as:
< P(v1, . . . , vm, θm) >∈δ(θ, k) if and only if
– P(u1, . . . , um) is the k–th literal in ⊥
– θ0 =θ
– if vj/uj ∈θj−1 then θj = θj−1 for 0 < j ≤m
The specialization operator ρ can be deﬁned as:
< r ′, θ′, k′ >∈ρ(< r, θ, k >) if and only if either
– r ′ = c ∪{l}, k′ = k, < l, θ′ >∈δ(θ, k), r ′ ∈{Rulespace} or
– r ′ = c, k′ = k + 1, θ′ = θ
for 1 ≤k ≤|⊥|
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
43 / 57

Inductive Logic Programming
Progol
Specialization Choosing
From all possible substitutions using ρ, the one that minimizes
lossprogol(E, B, H). The search method used is A*, which tracks
the best path regarding lossprogol with a priority list for other
options:
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
44 / 57

Neural–Symbolic Systems
Introduction
Outline
1
Introduction
Motivation
Objectives
Overview
2
Background Knowledge
Propositional (Classical) Logic
First–Order Logic
Logic Programming
3
Inductive Logic Programming
Deﬁnitions
Progol
4
Neural–Symbolic Systems
Introduction
C–IL2P
CILP++
5
Conclusion
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
44 / 57

Neural–Symbolic Systems
Introduction
Neural–Symbolic Integration
Symbolical systems, such as Progol, holds very powerful
representative power through First–Order logics brieﬂy explaining
Propositional and First–Order Logics
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
45 / 57

Neural–Symbolic Systems
Introduction
Neural–Symbolic Integration
Symbolical systems, such as Progol, holds very powerful
representative power through First–Order logics brieﬂy explaining
Propositional and First–Order Logics
Connectionistic systems, being neural networks its main
representative, have great noise–robustness capabilities and can
model knowledge as probabilities through their weights
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
45 / 57

Neural–Symbolic Systems
Introduction
Neural–Symbolic Integration
Symbolical systems, such as Progol, holds very powerful
representative power through First–Order logics brieﬂy explaining
Propositional and First–Order Logics
Connectionistic systems, being neural networks its main
representative, have great noise–robustness capabilities and can
model knowledge as probabilities through their weights
As seen during introduction, each one of those paradigms holds
advantages and advantages, in a quite “complementary” way
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
45 / 57

Neural–Symbolic Systems
Introduction
Neural–Symbolic Integration
Symbolical systems, such as Progol, holds very powerful
representative power through First–Order logics brieﬂy explaining
Propositional and First–Order Logics
Connectionistic systems, being neural networks its main
representative, have great noise–robustness capabilities and can
model knowledge as probabilities through their weights
As seen during introduction, each one of those paradigms holds
advantages and advantages, in a quite “complementary” way
Why not combine both paradigms?
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
45 / 57

Neural–Symbolic Systems
Introduction
Neural–Symbolic Integration
Symbolical
Connectionistic
Multiple
Learning
Slow learning of multiple
concepts
Efﬁcient parallel learn-
ing
of
multiple
con-
cepts
Noise
Robust-
ness
Limited, artiﬁcial noise–
robustness capabilities
Natural,
method–
inherent
noise
treat-
ment
Concept
Clarity
Learned concepts for-
mally represented
Concepts are tangled in-
side numerical data
Background
Data
Makes
partial
or
to-
tal use of background
data
Only empirical examples
are used
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
46 / 57

Neural–Symbolic Systems
C–IL2P
Outline
1
Introduction
Motivation
Objectives
Overview
2
Background Knowledge
Propositional (Classical) Logic
First–Order Logic
Logic Programming
3
Inductive Logic Programming
Deﬁnitions
Progol
4
Neural–Symbolic Systems
Introduction
C–IL2P
CILP++
5
Conclusion
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
46 / 57

Neural–Symbolic Systems
C–IL2P
Idea
Union two of the most popular symbolic and connectionistic
representatives: ILP and Neural Networks
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
47 / 57

Neural–Symbolic Systems
C–IL2P
Idea
Union two of the most popular symbolic and connectionistic
representatives: ILP and Neural Networks
Enhance their advantages, while supressing their ﬂaws
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
47 / 57

Neural–Symbolic Systems
C–IL2P
Idea
Union two of the most popular symbolic and connectionistic
representatives: ILP and Neural Networks
Enhance their advantages, while supressing their ﬂaws
A relational (recursive) three–layer network is built from a
propositional logic program and it is then used to train examples
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
47 / 57

Neural–Symbolic Systems
C–IL2P
Idea
Union two of the most popular symbolic and connectionistic
representatives: ILP and Neural Networks
Enhance their advantages, while supressing their ﬂaws
A relational (recursive) three–layer network is built from a
propositional logic program and it is then used to train examples
This way, a knowledge–based neural network is created (which
solves the main problem of classical neural networks) which is
capable of using “almost” full capabilities of back–propagation
training regarding noise robustness and incomplete data (which
are the two main ILP weakpoints)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
47 / 57

Neural–Symbolic Systems
C–IL2P
C–IL2P Knowledge Flow
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
48 / 57

Neural–Symbolic Systems
C–IL2P
C–IL2P Structure
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
49 / 57

Neural–Symbolic Systems
C–IL2P
Applications
As expected of a hybrid system, C–IL2P can be used in any
problem in which any one of its components would be eligible to
be applied.
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
50 / 57

Neural–Symbolic Systems
C–IL2P
Applications
As expected of a hybrid system, C–IL2P can be used in any
problem in which any one of its components would be eligible to
be applied.
Additionally (as the knowledge ﬂow has shown before), the way
C–IL2P deals with information processing can allow it to be
applied in some unique applications, such as fault diagnosis and
multi–instance learning problems
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
50 / 57

Neural–Symbolic Systems
C–IL2P
Applications
As expected of a hybrid system, C–IL2P can be used in any
problem in which any one of its components would be eligible to
be applied.
Additionally (as the knowledge ﬂow has shown before), the way
C–IL2P deals with information processing can allow it to be
applied in some unique applications, such as fault diagnosis and
multi–instance learning problems
If extended to work with First–Order logic programs, its
applicability would be hugely enhanced
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
50 / 57

Neural–Symbolic Systems
CILP++
Outline
1
Introduction
Motivation
Objectives
Overview
2
Background Knowledge
Propositional (Classical) Logic
First–Order Logic
Logic Programming
3
Inductive Logic Programming
Deﬁnitions
Progol
4
Neural–Symbolic Systems
Introduction
C–IL2P
CILP++
5
Conclusion
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
50 / 57

Neural–Symbolic Systems
CILP++
Main Concepts
CILP++ is the results of my work on C–IL2P to allow it to work with
First–Order logics
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
51 / 57

Neural–Symbolic Systems
CILP++
Main Concepts
CILP++ is the results of my work on C–IL2P to allow it to work with
First–Order logics
Different ways of using First–Order logics are being studied:
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
51 / 57

Neural–Symbolic Systems
CILP++
Main Concepts
CILP++ is the results of my work on C–IL2P to allow it to work with
First–Order logics
Different ways of using First–Order logics are being studied:
Using Bottom–Clauses as examples;
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
51 / 57

Neural–Symbolic Systems
CILP++
Main Concepts
CILP++ is the results of my work on C–IL2P to allow it to work with
First–Order logics
Different ways of using First–Order logics are being studied:
Using Bottom–Clauses as examples;
Different kinds of propositionalizations;
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
51 / 57

Neural–Symbolic Systems
CILP++
Main Concepts
CILP++ is the results of my work on C–IL2P to allow it to work with
First–Order logics
Different ways of using First–Order logics are being studied:
Using Bottom–Clauses as examples;
Different kinds of propositionalizations;
Applying the language bias on the building step of C–IL2P
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
51 / 57

Neural–Symbolic Systems
CILP++
Main Concepts
CILP++ is the results of my work on C–IL2P to allow it to work with
First–Order logics
Different ways of using First–Order logics are being studied:
Using Bottom–Clauses as examples;
Different kinds of propositionalizations;
Applying the language bias on the building step of C–IL2P
It uses the same building process of C–IL2P, but differs in the
network training, depending on the kind of First–Order information
that is being given to it
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
51 / 57

Neural–Symbolic Systems
CILP++
What Has Been Done
Enhancements in the underlying neural network to optimize it
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
52 / 57

Neural–Symbolic Systems
CILP++
What Has Been Done
Enhancements in the underlying neural network to optimize it
Testings using Bottom–Clauses and RSA propositionalization
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
52 / 57

Neural–Symbolic Systems
CILP++
What Has Been Done
Enhancements in the underlying neural network to optimize it
Testings using Bottom–Clauses and RSA propositionalization
A friendly GUI using wxWidgets to let other people uses the basic
C–IL2P functionality (an open–source C–IL2P project is already
active at SourceForge: http://sourceforge.net/projects/cil2p/)
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
52 / 57

Neural–Symbolic Systems
CILP++
What Will Be Done
In priority order:
Fine–tuning the system to work with bottom–clauses and
propositionalizated datasets
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
53 / 57

Neural–Symbolic Systems
CILP++
What Will Be Done
In priority order:
Fine–tuning the system to work with bottom–clauses and
propositionalizated datasets
Study how knowledge extraction will take place in this new system
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
53 / 57

Neural–Symbolic Systems
CILP++
What Will Be Done
In priority order:
Fine–tuning the system to work with bottom–clauses and
propositionalizated datasets
Study how knowledge extraction will take place in this new system
Test other ways of using First–Order logics into C–IL2P without
major structural changes
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
53 / 57

Neural–Symbolic Systems
CILP++
What Will Be Done
In priority order:
Fine–tuning the system to work with bottom–clauses and
propositionalizated datasets
Study how knowledge extraction will take place in this new system
Test other ways of using First–Order logics into C–IL2P without
major structural changes
Analyze structural changes on CILP++ to allow better suitability for
First–Order logic processing
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
53 / 57

Neural–Symbolic Systems
CILP++
Applications
CILP++, working with First–Order logics, will be able to completely
explore domain–theory and classiﬁcation problems. It will be the
ﬁrst hybrid system to achieve that
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
54 / 57

Neural–Symbolic Systems
CILP++
Applications
CILP++, working with First–Order logics, will be able to completely
explore domain–theory and classiﬁcation problems. It will be the
ﬁrst hybrid system to achieve that
Speciﬁcally about bottom–clauses usage, as it is relatively simple,
it will allow quick training and online inference of First–Order
logics, which can make way to some interesting applications:
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
54 / 57

Neural–Symbolic Systems
CILP++
Applications
CILP++, working with First–Order logics, will be able to completely
explore domain–theory and classiﬁcation problems. It will be the
ﬁrst hybrid system to achieve that
Speciﬁcally about bottom–clauses usage, as it is relatively simple,
it will allow quick training and online inference of First–Order
logics, which can make way to some interesting applications:
Web–semantics
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
54 / 57

Neural–Symbolic Systems
CILP++
Applications
CILP++, working with First–Order logics, will be able to completely
explore domain–theory and classiﬁcation problems. It will be the
ﬁrst hybrid system to achieve that
Speciﬁcally about bottom–clauses usage, as it is relatively simple,
it will allow quick training and online inference of First–Order
logics, which can make way to some interesting applications:
Web–semantics
Intelligent Agents
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
54 / 57

Conclusion
Final Remarks
Connectionism and symbolism are two paradigms that were kept
separated for a long time, but this is coming to an end
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
55 / 57

Conclusion
Final Remarks
Connectionism and symbolism are two paradigms that were kept
separated for a long time, but this is coming to an end
Symbolism, supported by its very successful machine learning
algorithm called ILP, is a necessary tool to express knowledge in a
formal and clear way and to represent complex and hierarchical
relations
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
55 / 57

Conclusion
Final Remarks
Connectionism and symbolism are two paradigms that were kept
separated for a long time, but this is coming to an end
Symbolism, supported by its very successful machine learning
algorithm called ILP, is a necessary tool to express knowledge in a
formal and clear way and to represent complex and hierarchical
relations
Neural–Symbolic Integration is one way–to–go in going one step
further in learning, reasoning and expressing knowledge
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
55 / 57

Conclusion
Recommended Reading
Tom M. Mitchell. Machine Learning. McGraw−Hill, New York,
1997
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
56 / 57

Conclusion
Recommended Reading
Tom M. Mitchell. Machine Learning. McGraw−Hill, New York,
1997
Stuart J. Russell and Peter Norvig. Artiﬁcial Intelligence −A
Modern Approach (3. internat. ed.). Pearson Education, 2010
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
56 / 57

Conclusion
Recommended Reading
Tom M. Mitchell. Machine Learning. McGraw−Hill, New York,
1997
Stuart J. Russell and Peter Norvig. Artiﬁcial Intelligence −A
Modern Approach (3. internat. ed.). Pearson Education, 2010
Artur S. D. Garcez and Luis C. Lamb and Dov M. Gabbay.
Neural-Symbolic Cognitive Reasoning. Springer−Verlag, 2009
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
56 / 57

Conclusion
Recommended Reading
Tom M. Mitchell. Machine Learning. McGraw−Hill, New York,
1997
Stuart J. Russell and Peter Norvig. Artiﬁcial Intelligence −A
Modern Approach (3. internat. ed.). Pearson Education, 2010
Artur S. D. Garcez and Luis C. Lamb and Dov M. Gabbay.
Neural-Symbolic Cognitive Reasoning. Springer−Verlag, 2009
Petri Toiviainen. Symbolic AI Versus Connectionism in Music
Research. Readings in Music and Artiﬁcial Intelligence.
Amsterdam: Harwood Academic Publishers, 2000
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
56 / 57

Conclusion
Thank You!
Manoel.Franca.1@city.ac.uk
Manoel França (City University)
Introduction to Inductive Logic Programming
ML Group Meeting
57 / 57

