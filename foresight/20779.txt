,
,
??
()
c

Klu
w
er
Academic
Publishers,
Boston.
Man
ufactured
in
The
Netherlands.
Scaling
Up
Inductiv
e
Logic
Programming
b
y
Learning
from
In
terpretations
HENDRIK
BLOCKEEL,
LUC
DE
RAEDT,
NICO
JA
COBS
AND
BAR
T
DEMOEN
fhendrik.blo
c
k
eel,luc.deraedt,nico.jacobs,bart.demo
eng@cs.kuleuv
en.ac.b
e
Dep
artment
of
Computer
Scienc
e,
Katholieke
Universiteit
L
euven
Celestijnenlaan
00A,
B-00
Heverle
e,
Belgium
Editor:
Abstract.
When
comparing
inductiv
e
logic
programming
(ILP)
and
attribute-v
alue
learning
tec
hniques,
there
is
a
trade-o
b
et
w
een
expressiv
e
p
o
w
er
and
eciency
.
Inductiv
e
logic
program-
ming
tec
hniques
are
t
ypically
more
expressiv
e
but
also
less
ecien
t.
Therefore,
the
data
sets
handled
b
y
curren
t
inductiv
e
logic
programming
systems
are
small
according
to
general
standards
within
the
data
mining
comm
unit
y
.
The
main
source
of
ineciency
lies
in
the
assumption
that
sev
eral
examples
ma
y
b
e
related
to
eac
h
other,
so
they
cannot
b
e
handled
indep
enden
tly
.
Within
the
learning
from
in
terpretations
framew
ork
for
inductiv
e
logic
programming
this
as-
sumption
is
unnecessary
,
whic
h
allo
ws
to
scale
up
existing
ILP
algorithms.
In
this
pap
er
w
e
explain
this
learning
setting
in
the
con
text
of
relational
databases.
W
e
relate
the
setting
to
prop
ositional
data
mining
and
to
the
classical
ILP
setting,
and
sho
w
that
learning
from
in
terpretations
cor-
resp
onds
to
learning
from
m
ultiple
relations
and
th
us
extends
the
expressiv
eness
of
prop
ositional
learning,
while
main
taining
its
eciency
to
a
large
exten
t
(whic
h
is
not
the
case
in
the
classical
ILP
setting).
As
a
case
study
,
w
e
presen
t
t
w
o
alternativ
e
implemen
tations
of
the
ILP
system
Tilde
(T
op-
do
wn
Induction
of
Logical
DEcision
trees):
Tildeclassic,
whic
h
loads
all
data
in
main
memory
,
and
TildeLDS,
whic
h
loads
the
examples
one
b
y
one.
W
e
exp
erimen
tally
compare
the
imple-
men
tations,
sho
wing
TildeLDS
can
handle
large
data
sets
(in
the
order
of
00,000
examples
or
00
MB)
and
indeed
scales
up
linearly
in
the
n
um
b
er
of
examples.
Keyw
ords:
Inductiv
e
Logic
Programming,
Kno
wledge
Disco
v
ery
in
Databases,
First
Order
De-
cision
T
rees,
Learning
F
rom
In
terpretations
.
In
tro
duction
There
is
a
general
trade-o
in
computer
science
b
et
w
een
expressiv
e
p
o
w
er
and
e-
ciency
.
Theorem
pro
ving
in
rst
order
logic
is
less
ecien
t
but
more
expressiv
e
than
theorem
pro
ving
in
prop
ositional
logic.
It
is
therefore
no
surprise
that
rst
order
induction
tec
hniques
(suc
h
as
those
studied
within
inductiv
e
logic
programming)
are
less
ecien
t
than
prop
ositional
or
attribute-v
alue
learning
tec
hniques.
On
the
other
hand,
inductiv
e
logic
programming
is
able
to
solv
e
induction
problems
b
ey
ond
the
scop
e
of
attribute
v
alue
learning,
cf.
(Bratk
o
and
Muggleton,
		).
The
computational
requiremen
ts
of
inductiv
e
logic
programming
systems
are
higher
than
those
of
prop
ositional
learners
due
to
the
follo
wing
reasons:
rst,
the
space
of
clauses
considered
b
y
inductiv
e
logic
programming
systems
t
ypically
is
m
uc
h
larger
than
that
of
prop
ositional
learners
and
can
ev
en
b
e
innite.
Second,
testing
whether
a
clause
co
v
ers
an
example
is
more
complex
than
in
attribute
v
alue


learners.
In
attribute
v
alue
learners
an
example
corresp
onds
to
a
single
tuple
in
a
relational
database,
whereas
in
inductiv
e
logic
programming
one
example
ma
y
corresp
ond
to
m
ultiple
tuples
of
m
ultiple
relations.
Therefore,
the
co
v
erage
test
in
inductiv
e
logic
programming
needs
a
database
system
to
solv
e
complex
queries
or
ev
en
a
theorem
pro
v
er.
Third,
and
this
is
related
to
the
second
p
oin
t,
in
attribute
v
alue
learning
testing
whether
an
example
is
co
v
ered
is
done
lo
c
al
ly,
i.e.
indep
end-
en
tly
of
the
other
examples.
Therefore,
ev
en
if
the
data
set
is
h
uge,
a
sp
ecic
co
v
erage
test
can
b
e
p
erformed
ecien
tly
.
This
con
trasts
with
the
large
ma
jorit
y
of
inductiv
e
logic
programming
systems,
suc
h
as
F
OIL
(Quinlan,
		0)
or
Progol
(Muggleton,
		),
in
whic
h
co
v
erage
is
tested
glob
al
ly,
i.e.
to
test
the
co
v
erage
of
one
example
the
whole
ensem
ble
of
examples
and
bac
kground
theory
needs
to
b
e
considered

.
Global
co
v
erage
tests
are
m
uc
h
more
exp
ensiv
e
than
lo
cal
ones.
Moreo
v
er,
systems
using
global
co
v
erage
tests
are
hard
to
scale
up.
Due
to
the
fact
that
one
single
co
v
erage
test
(on
one
example)
t
ypically
tak
es
more
than
constan
t
time
in
the
size
of
the
database,
the
complexit
y
of
induction
systems
exploiting
global
co
v
erage
tests
will
gro
w
more
than
linearly
in
the
n
um
b
er
of
examples.
Also,
couplings
to
databases
are
necessary
when
the
data
cannot
b
e
loaded
in
to
main
memory
.
In
a
more
recen
t
setting
for
inductiv
e
logic
programming,
called
learning
from
in
terpretations
(De
Raedt
and
D

zeroski,
		;
De
Raedt
et
al.,
		),
it
is
assumed
that
eac
h
example
is
a
small
database
(or
a
part
of
a
global
database),
and
lo
cal
co
v
erage
tests
are
p
erformed.
Algorithms
using
lo
cal
co
v
erage
tests
are
t
ypically
linear
in
the
n
um
b
er
of
examples.
F
urthermore,
as
eac
h
example
can
b
e
loaded
indep
enden
tly
of
the
other
ones,
there
is
no
need
to
use
a
database
system
ev
en
when
the
whole
data
set
cannot
b
e
loaded
in
to
main
memory
.
Within
the
setting
of
learning
from
in
terpretations,
w
e
in
v
estigate
the
issue
of
scal-
ing
up
inductiv
e
logic
programming.
More
sp
ecically
,
w
e
presen
t
t
w
o
alternativ
e
implemen
tations
of
the
Tilde
system
(Blo
c
k
eel
and
De
Raedt,
		):
Tildeclassic,
whic
h
loads
all
data
in
main
memory
,
and
TildeLDS,
whic
h
loads
the
examples
one
b
y
one.
The
latter
is
inspired
b
y
the
w
ork
b
y
Meh
ta
et
al.
(		).
F
urthermore,
w
e
exp
erimen
tally
compare
the
algorithms
on
large
data
sets
in
v
olving
00,000
ex-
amples
(in
the
order
of
00
MBytes).
The
exp
erimen
ts
clearly
sho
w
that
inductiv
e
logic
programming
systems
can
b
e
scaled
up
to
satisfy
the
standards
imp
osed
b
y
the
data
mining
comm
unit
y
.
A
t
the
same
time,
this
pro
vides
evidence
in
fa
v
or
of
lo
cal
co
v
erage
tests
(as
in
learning
from
in
terpretations)
in
inductiv
e
logic
programming.
This
article
is
organized
as
follo
ws.
In
Section

w
e
in
tro
duce
the
learning
from
in
terpretations
setting
and
relate
it
to
the
relational
database
con
text.
In
Section

w
e
in
tro
duce
rst
order
logical
decision
trees
and
discuss
the
ILP
system
Tilde,
whic
h
induces
suc
h
trees.
Section

sho
ws
ho
w
man
y
prop
ositional
tec
hniques
can
b
e
upgraded
to
the
learning
from
in
terpretations
setting
(using
Tilde
as
an
illustration),
and
discusses
wh
y
this
is
m
uc
h
harder
for
the
classical
ILP
setting.
Section

rep
orts
on
exp
erimen
ts
with
Tilde
through
whic
h
w
e
empirically
v
alidate
our
claims,
Section

discusses
some
related
w
ork
and
in
Section

w
e
conclude.


.
The
learning
setting
In
this
section
w
e
rst
in
tro
duce
the
problem
sp
ecication
in
a
logical
con
text,
then
discuss
it
in
the
con
text
of
relational
databases,
and
nally
relate
it
to
the
standard
inductiv
e
logic
programming
setting.
W
e
assume
familiarit
y
with
Prolog
or
Datalog
(see
e.g.
(Bratk
o,
		0)),
some
terminology
of
rst
order
logic
(see
e.g.
(Llo
yd,
	))
and
relational
databases
(see
e.g.
(Elmasri
and
Na
v
athe,
		)).
A
w
ord
on
our
notation:
in
logical
form
ulae
w
e
will
adopt
the
Prolog
con
v
en
tion
that
names
starting
with
a
capital
denote
v
ariables,
and
names
starting
with
a
lo
w
ercase
c
haracter
denote
constan
ts.
..
Pr
oblem
sp
e
cic
ation
In
our
framew
ork,
eac
h
example
is
a
set
of
facts.
These
facts
enco
de
the
sp
ecic
prop
erties
of
the
examples
in
a
database.
F
urthermore,
eac
h
example
is
classi-
ed
in
to
one
of
a
nite
set
of
p
ossible
classes.
One
ma
y
also
sp
ecify
bac
kground
kno
wledge
in
the
form
of
a
Prolog
program.
More
formally
,
the
problem
sp
ecication
is:
Giv
en:

a
set
of
classes
C
(eac
h
class
lab
el
c
is
a
n
ullary
predicate),

a
set
of
classied
examples
E
(eac
h
elemen
t
of
E
is
of
the
form
(e;
c)
with
e
a
set
of
facts
and
c
a
class
lab
el)

and
a
bac
kground
theory
B
,
Find:
a
h
yp
othesis
H
(a
Prolog
program),
suc
h
that
for
all
(e;
c)

E
,

H
^
e
^
B
j
=
c,
and

c
0

C
 fcg
:
H
^
e
^
B
j
=
c
0
This
setting
is
kno
wn
in
inductiv
e
logic
programming
under
the
lab
el
le
arning
fr
om
interpr
etations
(De
Raedt
and
D

zeroski,
		;
De
Raedt,
		;
De
Raedt
et
al.,
		)
(an
in
terpretation
is
just
a
set
of
facts).
Notice
that
within
this
setting,
one
alw
a
ys
learns
rst
order
denitions
of
prop
ositional
predicates
(the
classes).
An
implicit
assumption
is
that
the
class
of
an
example
dep
ends
on
that
example
only
,
not
on
an
y
other
examples.
This
is
a
reasonable
assumption
for
man
y
classication
problems,
though
not
for
all;
it
precludes,
e.g.,
recursiv
e
concept
denitions.
Example:
Figure

sho
ws
a
set
of
pictures
eac
h
of
whic
h
is
lab
elled
	
or
.
The
task
is
to
classify
new
pictures
in
to
one
of
these
classes
b
y
lo
oking
at
the
ob
jects
in
the
pictures.
W
e
call
this
kind
of
problems
Bongard-problems,
after
Mikhail
Bongard,
who
used
similar
problems
for
pattern
recognition
tests
(Bongard,
	0).


Figur
e
.
Bongard
problems
Assuming
w
e
only
consider
the
shap
e,
conguration
(p
oin
ting
up
w
ards
or
do
wn-
w
ards,
for
triangles
only)
and
relativ
e
p
osition
(ob
jects
ma
y
b
e
inside
other
ob
jects)
of
ob
jects,
the
pictures
in
Figure

can
b
e
represen
ted
as
follo
ws:
Picture
:
fcircle(o

),
triangle(o

),
p
oin
ts(o

,
up),
inside(o

,
o

)g
Picture
:
fcircle(o

),
triangle(o

),
p
oin
ts(o

,
up),
triangle(o

),
p
oin
ts(o

,
do
wn),
inside(o

,
o

)g
etc.
(The
o
i
are
constan
ts
denoting
geometric
ob
jects.
The
exact
names
of
these
con-
stan
ts
are
of
no
imp
ortance;
they
will
not
b
e
referred
to
in
the
rst
order
h
yp
o-
thesis.)
Bac
kground
kno
wledge
migh
t
b
e
pro
vided
to
the
learner,
e.g.,
the
follo
wing
den-
itions
could
b
e
in
the
bac
kground:
doubletriangle(O
,O
)
:-
triangle(O),
triangle(O),
O
=
O.
polygon(O)
:-
triangle(O).
polygon(O)
:-
square(O).
When
considering
a
particular
example
(e.g.
Picture
)
in
conjunction
with
the
bac
kground
kno
wledge
it
is
p
ossible
to
deduce
additional
facts
in
the
example.
F
or
instance,
in
Picture
,
the
facts
doubletriangle(o

,o

)
and
polygon(o

)
also
hold.


..
L
e
arning
fr
om
Multiple
R
elations
The
learning
from
in
terpretations
setting,
as
in
tro
duced
b
efore,
can
easily
b
e
related
to
learning
from
m
ultiple
relations
in
a
relational
database.
T
ypically
,
eac
h
predicate
will
corresp
ond
to
one
relation
in
the
relational
database.
Eac
h
fact
in
an
in
terpretation
is
a
tuple
in
the
database,
and
an
in
terpretation
corresp
onds
to
a
part
of
the
database
(a
set
of
tuples).
Bac
kground
kno
wledge
can
b
e
expressed
b
y
means
of
views
as
w
ell
as
extensional
tables.
Example:
F
or
the
Bongard
example,
the
follo
wing
database
con
tains
a
description
of
the
rst
t
w
o
pictures
in
Figure

(note
that
an
extra
relation
CONT
AINS
is
in
tro
duced,
linking
ob
jects
to
pictures;
this
relation
w
as
implicit
in
the
previous
represen
tation):
CONT
AINS
picture
ob
ject

o

o

o

o

o
CIR
CLE
TRIANGLE
POINTS
INSIDE
ob
ject
o
o
ob
ject
o
o
o
ob
ject
direction
o
up
o
up
o
do
wn
inner
outer
o
o
o
o
The
bac
kground
kno
wledge
can
b
e
dened
using
views,
as
follo
ws:
(w
e
are
as-
suming
here
that
a
relation
SQUARE
is
also
dened)
DEFINE
VIEW
doubletriangle
AS
SELECT
object
FROM
triangle
o,
o
WHERE
o.object
<>
o.object;
DEFINE
VIEW
polygon
AS
SELECT
object
FROM
triangle
UNION
SELECT
object
FROM
square;
In
this
example
the
bac
kground
kno
wledge
is
in
a
sense
redundan
t:
it
is
computed
from
the
other
relations.
This
is
not
necessarily
the
case.
The
follo
wing
example


illustrates
this.
It
is
also
a
more
realistic
example
of
an
application
where
mining
m
ultiple
relations
is
useful.
Example:
Assume
that
one
has
a
relational
database
describing
molecules.
The
molecules
themselv
es
are
describ
ed
b
y
listing
the
atoms
and
b
onds
that
o
ccur
in
them,
as
w
ell
as
some
prop
erties
of
the
molecule
as
a
whole.
Mendelev's
p
erio
dic
table
of
elemen
ts
is
a
go
o
d
example
of
bac
kground
kno
wledge
ab
out
this
domain.
The
follo
wing
tables
illustrate
what
suc
h
a
c
hemical
database
could
lo
ok
lik
e:
MENDELEV
n
um
b
er
sym
b
ol
atomic
w
eigh
t
electrons
in
outer
la
y
er
.
.
.

H
.00	


He
.00


Li
.	


Be
	.0


B
0.


C
.0

.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
MOLECULES
CONT
AINS
form
ula
name
class
H

O
w
ater
inorganic
C
O

carb
on
dio
xide
inorganic
C
O
carb
on
mono
xide
inorganic
C
H

methane
organic
C
H

O
H
methanol
organic
.
.
.
.
.
.
.
.
.
molecule
atom
id
H

O
ho-
H

O
ho-
H

O
ho-
C
O

co-
C
O

co-
.
.
.
.
.
.
A
TOMS
BONDS
atom
id
elemen
t
ho-
H
ho-
O
ho-
H
co-
O
.
.
.
.
.
.
atom
id
atom
id
t
yp
e
ho-
ho-
single
ho-
ho-
single
co-
co-
double
co-
co-
double
.
.
.
.
.
.
.
.
.
A
p
ossible
classication
problem
here
is
to
classify
unseen
molecules
in
to
organic
and
inorganic
molecules,
based
on
their
c
hemical
structure.
Notice
that
this
represen
tation
of
examples
and
bac
kground
kno
wledge
upgrades
the
t
ypical
attribute
v
alue
learning
represen
tation
in
t
w
o
resp
ects.
First,
in
attrib-
ute
v
alue
learning
an
example
corresp
onds
to
a
single
tuple
for
a
single
relation.


Our
represen
tation
allo
ws
for
multiple
tuples
in
multiple
relations.
Second,
it
also
allo
ws
for
using
bac
kground
kno
wledge.
By
joining
all
the
relations
in
a
database
in
to
one
h
uge
relation,
one
can
of
course
eliminate
the
need
for
learning
from
m
ultiple
relations.
The
ab
o
v
e
example
should
mak
e
clear
that
in
man
y
cases
this
is
not
an
option.
The
information
in
Mendelev's
table,
for
instance,
w
ould
b
e
duplicated
man
y
times.
Moreo
v
er,
unless
a
m
ultiple-
instance
learner
is
used
(see
e.g.
(Dietteric
h
et
al.,
		))
all
the
atoms
a
molecule
consists
of,
together
with
their
prop
erties,
ha
v
e
to
b
e
stored
in
one
tuple,
so
that
an
indenite
n
um
b
er
of
attributes
is
needed.
While
mining
suc
h
a
database
is
not
feasible
using
prop
ositional
tec
hniques,
it
is
feasible
using
learning
from
in
terpretations.
W
e
pro
ceed
to
sho
w
ho
w
a
relational
database
can
b
e
con
v
erted
in
to
a
suitable
format.
Conversion
fr
om
r
elational
datab
ase
to
interpr
etations
Con
v
erting
a
relational
database
to
a
set
of
in
terpretations
can
b
e
done
easily
and
in
a
semi-automated
w
a
y
,
as
follo
ws:
.
Decide
whic
h
relations
are
bac
kground
kno
wledge.
.
Let
D
B
b
e
the
original
database
without
the
b
ackgr
ound
r
elations.
.
Cho
ose
an
attribute
in
a
relation
that
uniquely
iden
ties
the
examples.
.
F
or
eac
h
v
alue
i
of
that
attribute:
.
S
:=
set
of
all
tuples
in
D
B
con
taining
that
v
alue
.
rep
eat
.
S
:=
S
[
set
of
all
tuples
in
D
B
referred
to
b
y
a
foreign
k
ey
in
S
.
un
til
S
do
es
not
c
hange
an
ymore
	.
S
i
:=
S
A
tuple
(attr

;
:
:
:
;
attr
n
)
of
a
relation
R
can
trivially
b
e
con
v
erted
to
a
fact
R
(attr

;
:
:
:
;
attr
n
).
By
doing
this
con
v
ersion
for
all
S
i
,
eac
h
S
i
b
ecomes
a
set
of
facts
describing
an
individual
example
i.
The
extensional
bac
kground
relations
can
b
e
con
v
erted
in
the
same
manner
in
to
one
set
of
facts
that
forms
the
bac
kground
kno
wledge.
Bac
kground
relations
dened
b
y
views
can
b
e
con
v
erted
to
equiv
alen
t
Prolog
programs.
The
only
parts
in
this
con
v
ersion
pro
cess
that
are
hard
to
automate
are
the
selection
of
the
bac
kground
kno
wledge
(t
ypically
,
one
selects
those
relations
where
eac
h
tuple
can
b
e
relev
an
t
for
man
y
examples)
and
the
con
v
ersion
of
view
denitions
to
Prolog
programs.
Also,
the
user
m
ust
indicate
whic
h
attribute
should
b
e
c
hosen
as
an
example
iden
tier,
as
this
dep
ends
on
the
learning
task.
Example:
In
the
c
hemical
database,
w
e
c
ho
ose
as
example
iden
tier
the
molecular
form
ula.
The
bac
kground
kno
wledge
consists
of
the
table
MENDELEV.
In
order
to
build
a
description
of
H

O
,
one
rst
collects
the
tuples
con
taining
H

O
;
these
are
presen
t
in
MOLECULES
and
CONT
AINS.
These
tuples
con
tain
references
to
atom
id's
ho-i,
i
=
;
;
,
so
the
tuples
con
taining
those
sym
b
ols
are
also
collected


(tuples
from
A
TOMS
and
BONDS).
These
again
refer
to
the
elemen
ts
H
and
O
,
whic
h
are
foreign
k
eys
for
the
MENDELEV
relation.
Since
this
relation
is
in
the
bac
kground,
no
further
tuples
are
collected.
Con
v
erting
the
tuples
to
facts,
w
e
get
the
follo
wing
description
of
H

O
:
fmolecules('HO',
w
ater,
inorganic),
con
tains('HO',
ho-),
con
tains('HO',
ho-
),
con
tains('HO',
ho-),
atoms(ho-,
'H'),
atoms(ho-,
'O'),
atoms(ho-,
'H'),
b
onds(ho-,
ho-,
single),
b
onds(ho-,
ho-,
single)g
Some
v
ariations
of
this
algorithm
can
b
e
considered.
F
or
instance,
when
the
example
iden
tier
has
no
meaning
except
that
it
iden
ties
the
example
(as
the
picture
n
um
b
ers

and

for
the
Bongard
example),
this
attribute
can
b
e
left
out
from
the
example
description.
The
k
ey
notion
in
this
con
v
ersion
pro
cess
is
lo
c
alization
of
information.
It
is
as-
sumed
that
for
eac
h
example
only
a
relativ
ely
small
part
of
the
database
is
relev
an
t,
and
that
this
part
can
b
e
lo
calized
and
extracted.
F
rom
no
w
on,
w
e
will
refer
to
this
assumption
as
the
lo
c
ality
assumption.
..
The
standar
d
ILP
setting
W
e
no
w
briey
discuss
the
standard
ILP
setting
and
ho
w
it
diers
from
our
set-
ting.
F
or
a
more
thorough
discussion
of
dieren
t
ILP
settings
and
the
relationships
b
et
w
een
them
w
e
refer
to
(De
Raedt,
		).
The
standard
ILP
setting
(also
kno
wn
as
le
arning
fr
om
entailment
)
is
usually
form
ulated
as
follo
ws:
Giv
en:

a
set
of
p
ositiv
e
examples
E
+
and
a
set
of
negativ
e
examples
E
 
and
a
bac
kground
theory
B
,
Find:
a
h
yp
othesis
H
(a
Prolog
program),
suc
h
that

e

E
+
:
H
^
B
j
=
e,
and

e

E
 :
H
^
B
j
=
e
Note
that
in
this
setting,
an
example
e
is
a
fact
that
is
to
b
e
explained
b
y
H
^
B
,
while
in
the
learning
from
in
terpretations
setting
a
pr
op
erty
of
the
example
(its
class)
is
to
b
e
explained
b
y
H
^
B
^
e.
Th
us,
the
latter
setting
explicitates
the
separation
b
et
w
een
example-sp
ecic
information
and
general
bac
kground
information.
The
problem
sp
ecication
as
giv
en
ab
o
v
e
is
natural
for
the
standard
ILP
setting,
where
one
could,
for
instance,
giv
e
the
follo
wing
examples
for
the
predicate
member:
+
:
member(a,
[a,b,c]).
+
:
member(d,
[e,d,c,b]).
+
:
member(d,
[d,c,b]).
-
:
member(b,
[a,c,d]).
-
:
member(a,
[]).
-
:
member(d,
[c,b]).

	
and
exp
ect
the
ILP
system
to
come
up
with
the
follo
wing
denition:
member(X,
[X|Y]).
member(X,
[Y|Z])
:-
member(X,Z).
Note
that
the
class
of
an
example
(i.e.,
its
truth
v
alue)
no
w
dep
ends
on
the
class
of
other
examples;
e.g.,
the
class
of
member(d,
[e,d,c,b])
dep
ends
on
the
class
of
member(d,
[d,c,b]),
whic
h
is
a
dieren
t
example.
Because
of
this
prop
ert
y
,
it
is
in
general
not
p
ossible
to
nd
a
small
subset
of
the
database
that
is
relev
an
t
for
a
single
example,
i.e.,
lo
cal
co
v
erage
tests
cannot
b
e
used.
Results
from
computa-
tional
learning
theory
conrm
that
learning
h
yp
otheses
in
this
setting
generally
is
in
tractable
(see
e.g.
(D

zeroski
et
al.,
		;
Cohen,
		)).
Since
in
learning
from
in
terpretations
the
class
of
an
example
is
assumed
to
b
e
indep
enden
t
of
other
examples,
this
setting
is
less
p
o
w
erful
than
the
standard
ILP
setting.
With
this
loss
of
p
o
w
er
comes
a
gain
in
eciency
,
through
lo
cal
co
v
erage
tests.
The
in
teresting
p
oin
t
is
that
the
full
p
o
w
er
of
standard
ILP
is
not
used
for
most
practical
applications,
and
learning
from
in
terpretations
usually
turns
out
to
b
e
sucien
t
for
practical
applications,
see
e.g.
the
pro
ceedings
of
the
ILP
w
orkshops
and
conferences
of
the
last
few
y
ears
(De
Raedt,
		;
Muggleton,
		;
La
vra

c
and
D

zeroski,
		;
P
age,
		).
.
Tilde:
Induction
of
First-Order
Logical
Decision
T
rees
In
this
section,
w
e
discuss
one
sp
ecic
ILP
system
that
learns
from
in
terpretations,
called
Tilde.
This
system
will
b
e
used
to
illustrate
the
topics
discussed
in
the
follo
wing
sections.
W
e
rst
in
tro
duce
the
h
yp
othesis
represen
tation
formalism
used
b
y
Tilde,
then
discuss
an
algorithm
for
the
induction
of
h
yp
otheses
in
this
formalism.
..
First
or
der
lo
gic
al
de
cision
tr
e
es
W
e
will
use
rst
order
logical
decision
trees
for
represen
ting
h
yp
otheses.
These
are
an
upgrade
of
the
w
ell-kno
wn
prop
ositional
decision
trees
to
rst
order
learning.
A
rst
order
logical
decision
tree
(F
OLDT)
is
a
binary
decision
tree
in
whic
h

the
no
des
of
the
tree
con
tain
a
conjunction
of
literals

dieren
t
no
des
ma
y
share
v
ariables,
under
the
follo
wing
restriction:
a
v
ariable
that
is
in
tro
duced
in
a
no
de
(whic
h
means
that
it
do
es
not
o
ccur
in
higher
no
des)
m
ust
not
o
ccur
in
the
righ
t
branc
h
of
that
no
de.
The
need
for
this
restriction
follo
ws
from
the
seman
tics
of
the
tree.
A
v
ariable
X
that
is
in
tro
duced
in
a
no
de,
is
quan
tied
existen
tially
within
the
conjunction
of
that
no
de.
The
righ
t
subtree
is
only
relev
an
t
when
the
conjunction
fails
(\there
is
no
suc
h
X
"),
in
whic
h
case
further
reference
to
X
is
meaningless.
An
example
of
suc
h
a
tree
is
sho
wn
in
Figure
.

0
triangle(X)
inside(X,Y)
Figur
e
.
A
rst
order
logical
decision
tree
that
allo
ws
to
discriminate
the
t
w
o
classes
for
the
Bongard
problem
sho
wn
in
Figure
.
First
order
logical
decision
trees
can
b
e
con
v
erted
to
normal
logic
programs
(i.e.
logic
programs
that
allo
w
negated
literals
in
the
b
o
dy
of
a
clause)
and
to
Prolog
programs.
In
the
latter
case
the
Prolog
program
represen
ts
a
rst
order
decision
list,
i.e.
an
ordered
set
of
rules
where
a
rule
is
only
relev
an
t
if
none
of
the
rules
b
efore
it
succeed.
Eac
h
clause
in
suc
h
a
Prolog
program
ends
with
a
cut.
W
e
refer
to
(Blo
c
k
eel
and
De
Raedt,
		)
for
more
information
on
the
relationship
b
et
w
een
rst
order
decision
trees,
rst
order
decision
lists
and
logic
programs.
The
Prolog
program
equiv
alen
t
to
the
tree
in
Figure

is
class(pos)
:-
triangle(X),
inside(X,Y),
!.
class(neg)
:-
triangle(X).
class(neg).
Figure

sho
ws
ho
w
to
use
F
OLDTs
for
classication.
W
e
use
the
follo
wing
notation:
a
tree
T
is
either
a
leaf
with
class
c,
in
whic
h
case
w
e
write
T
=
leaf
(c),
or
it
is
an
in
ternal
no
de
with
conjunction
c
onj,
left
branc
h
left
and
righ
t
branc
h
right,
in
whic
h
case
w
e
write
T
=
ino
de(c
onj,
left,
right
).
Because
an
example
e
is
a
Prolog
program,
a
test
in
a
no
de
corresp
onds
to
c
hec
king
whether
a
query
 
C
succeeds
in
e
^
B
(with
B
the
bac
kground
kno
wledge).
Note
that
it
is
not
sucien
t
to
use
for
C
the
conjunction
c
onj
in
the
no
de
itself.
Since
c
onj
ma
y
share
v
ariables
with
no
des
higher
in
the
tree,
C
consists
of
sev
eral
conjunctions
that
o
ccur
in
the
path
from
the
ro
ot
to
the
curren
t
no
de.
More
sp
ecically
,
C
is
of
the
form
Q
^
conj
,
where
Q
is
the
conjunction
of
all
the
conjunctions
that
o
ccur
in
those
no
des
on
the
path
from
the
ro
ot
to
this
no
de
where
the
left
branc
h
w
as
c
hosen.
W
e
call
 
Q
the
asso
ciate
d
query
of
the
no
de.
When
an
example
is
sorted
to
the
left,
Q
is
up
dated
b
y
adding
c
onj
to
it.
When
sorting
an
example
to
the
righ
t,
Q
need
not
b
e
up
dated:
a
failed
test
nev
er
in
tro-
duces
new
v
ariables.
E.g.,
if
in
Figure

an
example
is
sorted
do
wn
the
tree,
in
the
no
de
con
taining
inside(X,Y)
the
correct
test
is
triangle(X),
inside(X,Y);
it
is
not
correct
to
test
inside(X,Y)
on
its
o
wn.


.
pro
cedure
classify(e
:
example)
returns
class:
.
Q
:=
tr
ue
.
N
:=
ro
ot
.
while
N
=
leaf(c)
do
.
let
N
=
ino
de(conj;
l
ef
t;
r
ig
ht)
.
if
Q
^
conj
succeeds
in
e
^
B
.
then
Q
:=
Q
^
conj
.
N
:=
l
ef
t
	.
else
N
:=
r
ig
ht
0.
return
c
Figur
e
.
Classication
of
an
example
using
an
F
OLDT
(with
bac
kground
kno
wledge
B
)
.
pro
cedure
buildtree(T
:
tree,
E
:
set
of
examples,
Q:
query):
.
 
Q
b
:=
elemen
t
of
( 
Q)
with
highest
gain
(or
gain
ratio)
.
if
 
Q
b
is
not
go
o
d
/*
e.g.
do
es
not
yield
any
gain
at
al
l
*/
.
then
T
:=
leaf(ma
jorit
y
class(E))
.
else
.
c
onj
:=
Q
b
 Q
.
E

:=
fe

E
j
 
Q
b
succeeds
in
e
^
B
g
.
E

:=
fe

E
j
 
Q
b
fails
in
e
^
B
g
	.
buildtree(left,
E

,
Q
b
)
0.
buildtree(right,
E

,
Q)
.
T
:=
ino
de(c
onj,
left,
right)
.
pro
cedure
Tilde(T
:
tree,
E
:
set
of
examples):
.
buildtree(T
,
E
,
true)
Figur
e
.
Algorithm
for
rst-order
logical
decision
tree
induction
..
The
Tilde
system
Tilde
stands
for
T
op-do
wn
Induction
of
Logical
DEcision
trees.
First
order
lo-
gical
decision
trees
can
b
e
induced
in
v
ery
m
uc
h
the
same
manner
as
prop
ositional
decision
trees.
The
generic
algorithm
for
this
is
usually
referred
to
as
TDIDT:
top-do
wn
induction
of
decision
trees.
Examples
of
systems
using
this
approac
h
are
C.
(Quinlan,
		a)
and
CAR
T
(Breiman
et
al.,
	).


The
algorithm
w
e
use
for
inducing
rst
order
decision
trees
is
sho
wn
in
Figure
.
The
Tilde
system
(Blo
c
k
eel
and
De
Raedt,
		)
is
an
implemen
tation
of
this
algorithm
that
is
based
on
C..
It
uses
the
same
heuristics,
the
same
p
ost-pruning
algorithm,
etc.
The
main
p
oin
t
where
our
algorithm
diers
from
C.
is
in
the
computation
of
the
set
of
tests
to
b
e
considered
at
a
no
de.
C.
only
considers
tests
comparing
an
attribute
with
a
v
alue.
Tilde,
on
the
other
hand,
generates
p
ossible
tests
b
y
means
of
a
user-dened
renemen
t
op
erator.
Roughly
,
this
op
erator
sp
ecies,
giv
en
the
asso
ciated
query
of
a
no
de,
whic
h
literals
or
conjunctions
can
b
e
added
to
the
query
.
More
sp
ecically
,
the
renemen
t
op
erator
is
a
renemen
t
op
erator
under

-sub-
sumption
(Plotkin,
	0;
Muggleton
and
De
Raedt,
		).
Suc
h
an
op
erator

maps
clauses
on
to
sets
of
clauses,
suc
h
that
for
an
y
clause
c
and
c
0

(c),
c

-
subsumes
c
0
.
A
clause
c


-subsumes
another
clause
c

if
and
only
if
there
exists
a
v
ariable
substitution

suc
h
that
c



c

.
The
op
erator
could
for
instance
add
literals
to
the
clause,
or
unify
sev
eral
v
ariables
in
it.
The
use
of
suc
h
renemen
t
op
erators
is
standard
practice
in
ILP
.
In
order
to
rene
a
no
de
with
asso
ciated
query
 
Q,
Tilde
computes
( 
Q)
and
c
ho
oses
the
query
 
Q
b

( 
Q)
that
results
in
the
b
est
split.
The
b
est
split
is
the
one
that
maximizes
a
certain
qualit
y
criterion;
in
the
case
of
Tilde
this
is
b
y
default
the
information
gain
ratio,
as
dened
b
y
Quinlan
(		a).
The
conjunction
put
in
the
no
de
consists
of
Q
b
 Q,
i.e.,
the
literals
that
ha
v
e
b
een
added
to
Q
in
order
to
pro
duce
Q
b
.
Example:
Consider
the
tree
in
Figure
.
Assuming
that
the
ro
ot
no
de
has
already
b
een
lled
in
with
the
test
triangle(X),
ho
w
do
es
Tilde
pro
cess
the
left
c
hild
of
it?
This
c
hild
has
as
asso
ciated
query
 triangle(X).
Tilde
no
w
generates
( 
triangle(X)
).
According
to
the
language
bias
sp
ecied
b
y
the
user
(see
later),
a
p
ossible
result
could
b
e
(w
e
use
semicolons
to
separate
the
elemen
ts
of
,
as
the
comma
denotes
a
conjunction
in
Prolog)
( 
triangle(X)
)
=
f
 
triangle(X),
inside(X,Y);
 
triangle(X),
inside(Y,X);
 
triangle(X),
square(Y);
 
triangle(X),
circle(Y)
g
Assuming
the
b
est
of
these
renemen
ts
is
Q
b
=
triangle(X),
inside(X,Y)
the
conjunction
put
in
the
no
de
is
Q
b
 Q
=
inside(X,Y).
L
anguage
bias
While
prop
ositional
systems
usually
ha
v
e
a
xed
language
bias,
most
ILP
systems
mak
e
use
of
a
language
bias
that
has
b
een
pro
vided
b
y
the
user.
The
language
bias
sp
ecies
what
kind
of
h
yp
otheses
are
allo
w
ed;
in
the
case
of
Tilde:
what
kind
of
literals
or
conjunctions
of
literals
can
b
e
put
in
the
no
des
of
the
tree.
This
bias
follo
ws
from
the
renemen
t
op
erator,
so
it
is
sucien
t
to
sp
ecify
the
latter.
The
sp
ecic
renemen
t
op
erator
that
is
to
b
e
used
is
dened
b
y
the
user
in
a
Pr
ogol-lik
e
manner
(Muggleton,
		).
A
set
of
facts
of
the
form


rmode(n:
c
onjunction
)
is
pro
vided,
indicating
whic
h
conjunctions
can
b
e
added
to
a
query
,
the
maximal
n
um
b
er
of
times
the
conjunction
can
b
e
added
(i.e.
the
maximal
n
um
b
er
of
times
it
can
o
ccur
in
an
y
path
from
ro
ot
to
leaf
)
(n),
and
the
mo
des
and
t
yp
es
of
the
v
ariables
in
it.
T
o
illustrate
this,
w
e
return
to
the
example
of
the
Bongard
problems.
A
suitable
renemen
t
op
erator
denition
in
this
case
w
ould
b
e
rmode(:
triangle(+-V)).
rmode(:
square(+-V)).
rmode(:
circle(+-V)).
rmode(:
inside(+V,+-W)).
rmode(:
inside(-V,+W)).
rmode(:
config(+V,up)).
rmode(:
config(+V,down)).
The
mo
de
of
an
argumen
t
is
indicated
b
y
a
+,
 or
+ sign
b
efore
a
v
ariable.

+
stands
for
input:
the
v
ariable
should
already
o
ccur
in
the
asso
ciated
query
of
the
no
de
where
the
test
is
put.
 stands
for
output:
the
v
ariable
has
to
one
that
do
es
not
o
ccur
y
et.
+ means
that
the
argumen
t
can
b
e
b
oth
input
and
output;
i.e.
the
v
ariable
can
b
e
a
new
one
or
an
already
existing
one.
Note
that
the
names
of
the
v
ariables
in
the
rmode
facts
are
formal
names;
when
the
literal
is
added
to
a
clause
actual
v
ariable
names
are
substituted
for
them.
Also
note
that
a
literal
can
ha
v
e
m
ultiple
mo
des,
e.g.
the
ab
o
v
e
facts
sp
ecify
that
at
least
one
of
the
t
w
o
argumen
ts
of
inside
has
to
b
e
input.
This
rmode
denition
tells
Tilde
that
a
test
in
a
no
de
ma
y
consist
of
c
hec
k-
ing
whether
an
ob
ject
that
has
already
b
een
referred
to
has
a
certain
shap
e
(e.g.
triangle(X)
with
X
an
already
existing
v
ariable),
c
hec
king
whether
there
exists
an
ob
ject
with
a
certain
shap
e
in
the
picture
(e.g.
triangle(Y)
with
Y
not
o
ccurring
in
the
asso
ciated
query),
testing
the
conguration
(up
or
down)
of
a
certain
ob
ject,
and
so
on.
A
t
most

literals
of
a
certain
t
yp
e
can
o
ccur
on
an
y
path
from
ro
ot
to
leaf
(this
is
indicated
b
y
the

in
the
rmode
facts).
The
decision
tree
sho
wn
in
Figure

is
conform
to
this
sp
ecication.
When
Tilde
builds
this
tree,
in
the
ro
ot
no
de
only
the
tests
triangle(X),
square(X)
and
circle(X)
are
considered,
b
ecause
eac
h
other
test
requires
some
v
ariable
to
o
c-
cur
in
the
asso
ciated
query
of
the
no
de
(whic
h
for
the
ro
ot
no
de
is
true).
The
left
c
hild
no
de
of
the
ro
ot
has
as
asso
ciated
query
 
triangle(X),
whic
h
con
tains
one
v
ariable
X,
hence
the
tests
that
are
considered
for
this
no
de
are:
triangle(X)
triangle(Y)
inside(X,Y)
points(X,up)
square(X)
square(Y)
inside(Y,X)
points(X,down)
circle(X)
circle(Y)
(it
is
clear
that
some
of
these
tests
are
not
useful,
and
Tilde
automatically
lters
out
clearly
useless
tests;
but
all
these
literals
are
in
principle
generated
b
y
the
renemen
t
op
erator).
Assuming
that
inside(X,Y)
yields
the
b
est
split,
this
literal
is
put
in
the
no
de.


In
addition
to
rmodes,
so-called
lo
ok
ahead
sp
ecications
can
b
e
pro
vided.
These
allo
w
Tilde
to
p
erform
sev
eral
successiv
e
renemen
t
steps
at
once.
This
alleviates
the
w
ell-kno
wn
problem
in
ILP
(see
e.g.
(Quinlan,
		b))
that
a
renemen
t
ma
y
not
yield
an
y
gain,
but
ma
y
in
tro
duce
new
v
ariables
that
are
crucial
for
classica-
tion.
By
p
erforming
successiv
e
renemen
t
steps
at
once,
Tilde
can
lo
ok
ahead
in
the
renemen
t
lattice
and
disco
v
er
suc
h
situations.
F
or
instance,
lookahead(triang
le
(T
),
points(T,up))
sp
ecies
that
whenev
er
the
literal
triangle(T)
is
considered
as
p
ossible
addition
to
the
curren
t
asso
ci-
ated
query
,
additional
renemen
t
b
y
adding
points(T,up)
should
b
e
tried
in
the
same
renemen
t
step.
Th
us,
b
oth
triangle(T)
and
triangle(T),
points(T,up)
w
ould
b
e
considered
as
p
ossible
addition.
This
is
useful
b
ecause
normally
Tilde
can
construct
the
test
triangle(T),
points(T,up)
only
b
y
rst
putting
triangle(T)
in
the
no
de,
then
putting
points(T,up)
in
its
left
c
hild
no
de.
But
if
triangle(X)
already
o
ccurs
in
the
asso
ciated
query
,
then
triangle(T)
cannot
yield
an
y
gain
(if
y
ou
already
kno
w
that
there
is
a
triangle,
the
question
\is
there
a
triangle"
will
not
giv
e
y
ou
new
information)
and
hence
w
ould
nev
er
b
e
selected,
and
this
w
ould
prev
en
t
points(T,up)
from
b
eing
added
as
w
ell.
This
lo
ok
ahead
metho
d
is
v
ery
similar
to
lo
ok
ahead
metho
ds
that
ha
v
e
b
een
prop
osed
for
prop
ositional
decision
tree
learners.
While
for
prop
ositional
systems
the
adv
an
tage
of
lo
ok
ahead
is
generally
considered
to
b
e
marginal,
it
is
m
uc
h
greater
in
ILP
b
ecause
of
the
o
ccurrence
of
v
ariables.
W
e
nally
men
tion
that
Tilde
handles
n
umerical
data
b
y
means
of
a
discretiz-
ation
algorithm
that
is
based
on
F
a
yy
ad
and
Irani's
(		)
and
Doughert
y
et
al.'s
(		)
w
ork,
but
extends
it
to
rst
order
logic
(V
an
Laer
et
al.,
		).
The
al-
gorithm
accepts
input
of
the
form
discretize(Query
,
Var),
with
Var
a
v
ariable
o
ccurring
in
Query.
It
runs
Query
in
all
the
examples,
collecting
all
instan
tiations
of
Var
that
can
b
e
found,
and
nally
generates
discretization
thresholds
based
on
this
set
of
instan
tiations.
Since
this
discretization
pro
cedure
is
not
crucial
to
this
pap
er,
w
e
refer
to
(V
an
Laer
et
al.,
		;
Blo
c
k
eel
and
De
Raedt,
		)
for
more
details.
Input
F
ormat
A
data
set
is
presen
ted
to
Tilde
in
the
form
of
a
set
of
in
terpret-
ations.
Eac
h
in
terpretation
consists
of
a
n
um
b
er
of
Prolog
facts,
surrounded
b
y
a
begin
and
end
line.
The
bac
kground
kno
wledge
is
simply
a
Prolog
program.
Examples
of
this
will
b
e
sho
wn
in
Section
.
Applic
ations
of
Tilde
Although
the
ab
o
v
e
discussion
of
Tilde
tak
es
the
view-
p
oin
t
of
induction
of
classiers,
the
use
of
rst
order
logical
decision
trees
is
not
limited
to
classication.
Numerical
predictions
can
b
e
made
b
y
storing
n
um
b
ers
instead
of
classes
in
the
lea
v
es;
suc
h
trees
are
usually
called
regression
trees.
An-
other
task
that
is
imp
ortan
t
for
data
mining,
is
clustering.
Induction
of
cluster
hierarc
hies
can
also
b
e
done
using
a
TDIDT
approac
h,
as
is
explained
in
(Blo
c
k
eel
et
al.,
		).


It
should
b
e
clear,
therefore,
that
the
tec
hniques
that
will
b
e
describ
ed
later
in
this
text
should
not
b
e
seen
as
sp
ecic
for
the
classication
con
text.
They
ha
v
e
a
m
uc
h
broader
application
domain.
.
Upgrading
Prop
ositional
KDD
T
ec
hniques
for
Tilde
In
this
section
w
e
discuss
ho
w
existing
prop
ositional
KDD
tec
hniques
can
b
e
up-
graded
to
rst
order
learning
in
our
setting.
The
Tilde
system
will
serv
e
as
a
case
study
here.
Indeed,
all
of
the
prop
osed
tec
hniques
(except
sampling)
ha
v
e
b
een
implemen
ted
in
Tilde.
W
e
stress,
ho
w
ev
er,
that
the
metho
dology
of
upgrading
KDD
tec
hniques
is
not
sp
ecic
for
Tilde,
nor
for
induction
of
decision
trees.
It
can
also
b
e
used
for
rule
induction,
disco
v
ery
of
asso
ciation
rules,
and
other
kinds
of
disco
v
ery
.
Systems
suc
h
as
ICL
(De
Raedt
and
V
an
Laer,
		)
and
W
armr
(Dehasp
e
and
De
Raedt,
		)
are
illustrations
of
this.
Both
learn
from
in
ter-
pretations
and
upgrade
prop
ositional
tec
hniques.
ICL
learns
rst
order
rule
sets,
upgrading
the
tec
hniques
used
in
CN,
and
W
armr
learns
a
rst
order
equiv
alen
t
of
asso
ciation
rules
(\asso
ciation
rules
o
v
er
m
ultiple
relations").
W
armr
has
b
een
designed
sp
ecically
for
large
databases
and
emplo
ys
an
ecien
t
algorithm
that
is
an
upgrade
of
Apriori
(Agra
w
al
et
al.,
		).
..
Dier
ent
Implementations
of
Tilde
W
e
discuss
t
w
o
dieren
t
implemen
tations
of
Tilde:
one
is
a
straigh
tforw
ard
im-
plemen
tation,
follo
wing
closely
the
TDIDT
algorithm.
The
other
is
a
more
soph-
isticated
implemen
tation
that
aims
sp
ecically
at
handling
large
data
sets;
it
is
based
on
w
ork
b
y
Meh
ta
et
al.
(		)
,
and
as
suc
h
is
our
rst
example
of
ho
w
prop
ositional
tec
hniques
can
b
e
upgraded.
...
A
str
aightforwar
d
implementation:
Tildeclassic
The
original
Tilde
im-
plemen
tation,
whic
h
w
e
will
refer
to
as
Tildeclassic,
is
based
on
the
algorithm
sho
wn
in
Figure
.
This
is
the
most
straigh
tforw
ard
w
a
y
of
implemen
ting
TDIDT.
Notew
orth
y
c
haracteristics
are
that
the
tree
is
built
depth-rst,
and
that
the
b
est
test
is
c
hosen
b
y
en
umerating
the
p
ossible
tests
and
for
eac
h
test
computing
its
qualit
y
(to
this
aim
the
test
needs
to
b
e
ev
aluated
on
ev
ery
single
example),
as
is
sho
wn
in
Figure
.
This
algorithm
should
b
e
seen
as
a
detailed
description
of
line

in
Figure
.
Note
that
with
this
implemen
tation,
it
is
crucial
that
fetc
hing
an
example
from
the
database
in
order
to
query
it
b
e
done
as
ecien
tly
as
p
ossible,
b
ecause
this
op
eration
is
inside
the
innermost
lo
op.
F
or
this
reason,
Tildeclassic
loads
all
data
in
to
main
memory
when
it
starts
up.
Lo
calization
is
then
ac
hiev
ed
b
y
using
the
mo
dule
system
of
the
Prolog
engine
in
whic
h
Tilde
runs.
Eac
h
example
is
loaded
in
to
a
dieren
t
mo
dule,
and
accessing
an
example
is
done
b
y
c
hanging
the
curren
tly
activ
e
mo
dule,
whic
h
is
a
v
ery
c
heap
op
eration.
One
could
also
load
all
the
examples
in
to
one
mo
dule;
no
example
selection
is
necessary
then,
and
all
data
can
alw
a
ys


.
for
eac
h
renemen
t
 
Q
i
:
.
/*
c
ounter[true]
and
c
ounter[false]
ar
e
class
distributions,
.
i.e.
arr
ays
mapping
classes
onto
their
fr
e
quencies
*/
.
for
eac
h
class
c
:
coun
ter[true][c]
:=
0,
coun
ter[false][c]
:=
0
.
for
eac
h
example
e:
.
if
 
Q
i
succeeds
in
e
.
then
increase
coun
ter[true][class(e)]
b
y

.
else
increase
coun
ter[false][class(e)]
b
y

	.
s
i
:=
w
eigh
ted
a
v
erage
class
en
trop
y(coun
ter[true],
coun
ter[false])
0.
Q
b
:=
that
Q
i
for
whic
h
s
i
is
minimal
/*
highest
gain
*/
Figur
e
.
Computation
of
the
b
est
test
Q
b
in
Tildeclassic.
b
e
accessed
directly
.
The
disadv
an
tage
is
that
the
relev
an
t
data
needs
to
b
e
lo
ok
ed
up
in
a
large
set
of
data,
so
that
a
go
o
d
indexing
sc
heme
is
necessary
in
order
to
mak
e
this
approac
h
ecien
t.
W
e
will
return
to
this
in
the
section
on
exp
erimen
ts.
W
e
p
oin
t
out
that,
when
examples
are
loaded
in
to
dieren
t
mo
dules,
Tildeclassic
partially
exploits
the
lo
calit
y
assumption
(in
that
it
handles
eac
h
individual
example
indep
enden
tly
from
the
others,
but
still
loads
all
the
examples
in
main
memory).
It
do
es
not
exploit
this
assumption
at
all
when
all
the
examples
are
loaded
in
to
one
mo
dule.
...
A
mor
e
sophistic
ate
d
implementation:
TildeLDS
Meh
ta
et
al.
(		)
ha
v
e
prop
osed
an
alternativ
e
implemen
tation
of
TDIDT
that
is
orien
ted
to
w
ards
mining
large
databases.
With
their
approac
h,
the
database
is
accessed
less
in
tens-
iv
ely
,
whic
h
results
in
an
imp
ortan
t
eciency
gain.
W
e
ha
v
e
adopted
this
approac
h
for
an
alternativ
e
implemen
tation
of
Tilde,
whic
h
w
e
call
TildeLDS
(LDS
stands
for
L
ar
ge
Data
Sets
).
The
alternativ
e
algorithm
is
sho
wn
in
Figure
.
It
diers
from
Tildeclassic
in
that
the
tree
is
no
w
built
breadth-rst,
and
examples
are
loaded
in
to
main
memory
one
at
a
time.
The
algorithm
w
orks
lev
el-wise.
Eac
h
iteration
through
the
while
lo
op
will
ex-
pand
one
lev
el
of
the
decision
tree.
S
con
tains
all
no
des
at
the
curren
t
lev
el
of
the
decision
tree.
T
o
expand
this
lev
el,
the
algorithm
considers
all
no
des
N
in
S
.
F
or
eac
h
no
de
and
for
eac
h
renemen
t
in
that
no
de,
a
separate
coun
ter
(to
compute
class
distributions)
is
k
ept.
The
algorithms
mak
es
one
pass
through
the
data,
during
whic
h
for
eac
h
example
that
b
elongs
to
a
non-leaf
no
de
N
it
tests
all
renemen
ts
for
N
on
the
example
and
up
dates
the
corresp
onding
coun
ters.
Note
that
while
for
Tildeclassic
the
example
lo
op
w
as
inside
the
renemen
t
lo
op,
the
opp
osite
is
true
no
w.
This
minimizes
the
n
um
b
er
of
times
a
new
example
m
ust
b
e
loaded,
whic
h
is
an
exp
ensiv
e
op
eration
(in
con
trast
with
the
previous


.
pro
cedure
TildeLDS(T
:
tree):
.
S
:=
fro
otg
.
while
S
=

do
.
/*
add
one
level
to
the
tr
e
e
*/
.
for
eac
h
example
e
that
is
not
co
v
ered
b
y
a
leaf
no
de:
.
load
e
.
N
:=
the
no
de
in
S
that
co
v
ers
e
.
 
Q
:=
asso
ciated
query(N
)
	.
for
eac
h
renemen
t
 
Q
i
of
 
Q:
0.
if
 
Q
i
succeeds
in
e
.
then
increase
coun
ter[N
,i,true][class(e)]
b
y

.
else
increase
coun
ter[N
,i,false][class(e)]
b
y

.
for
eac
h
no
de
N

S
:
.
remo
v
e
N
from
S
.
 
Q
b
:=
b
est
test(N
)
.
if
 
Q
b
is
not
go
o
d
.
then
N
:=
leaf(ma
jorit
y
class(N
))
.
else
	.
 
Q
:=
asso
ciated
query(N
)
0.
c
onj
:=
Q
b
 Q
.
N
:=
ino
de(c
onj,
left,
right
)
.
add
left
and
right
to
S
.
T
:=
ro
ot
.
function
b
est
test(N
:
no
de)
returns
query:
.
 
Q
:=
asso
ciated
query(N
)
.
for
eac
h
renemen
t
 
Q
i
of
 
Q:
.
C
D
l
:=
coun
ter[N
,i,true]
.
C
D
r
:=
coun
ter[N
,i,false]
	.
s
i
:=
w
eigh
ted
a
v
erage
class
en
trop
y(C
D
l
,
C
D
r
)
0.
Q
b
:=
that
Q
i
for
whic
h
s
i
is
minimal
.
return
 
Q
b
Figur
e
.
The
TildeLDS
algorithm


approac
h
where
all
examples
w
ere
in
main
memory
and
examples
only
had
to
b
e
\selected"
in
order
to
access
them,
examples
are
no
w
loaded
from
disk).
In
the
curren
t
implemen
tation
eac
h
example
needs
to
b
e
loaded
at
most
one
time
p
er
lev
el
of
the
tree
(\at
most"
b
ecause
once
it
is
in
a
leaf
it
need
not
b
e
loaded
an
ymore),
hence
the
total
n
um
b
er
of
passes
through
the
data
le
is
equal
to
the
depth
of
the
tree,
whic
h
is
the
same
as
w
as
obtained
for
prop
ositional
learning
algorithms
(Meh
ta
et
al.,
		).
.
The
disadv
an
tage
of
this
algorithm
is
that
a
four-dimensional
arra
y
of
coun
ters
needs
to
b
e
stored
instead
of
a
t
w
o-dimensional
one
(as
in
Tildeclassic),
b
ecause
dieren
t
coun
ters
are
k
ept
for
eac
h
no
de
and
for
eac
h
renemen
t.
Care
has
b
een
tak
en
to
implemen
t
TildeLDS
in
suc
h
a
w
a
y
that
the
size
of
the
data
set
that
can
b
e
handled
is
not
restricted
b
y
in
ternal
memory
(in
con
trast
to
Tildeclassic).
Whenev
er
information
needs
to
b
e
stored
the
size
of
whic
h
dep
ends
on
the
size
of
the
data
set,
this
information
is
stored
on
disk.

When
pro
cessing
a
certain
lev
el
of
the
tree,
the
space
complexit
y
of
TildeLDS
is
therefore
O
(r

n)
with
n
the
n
um
b
er
of
no
des
on
that
lev
el
and
r
the
(a
v
erage)
n
um
b
er
of
renemen
ts
of
those
no
des
(b
ecause
coun
ters
are
k
ept
for
eac
h
renemen
t
in
eac
h
no
de).
This
con
trasts
with
Tildeclassic
where
space
complexit
y
is
O
(N
)
with
N
the
n
um
b
er
of
examples
(b
ecause
all
examples
are
loaded
at
once).
While
memory
no
w
restricts
the
n
um
b
er
of
renemen
ts
that
can
b
e
considered
in
eac
h
no
de
and
the
maximal
size
of
the
tree,
this
restriction
is
unimp
ortan
t
in
practice,
as
the
n
um
b
er
of
renemen
ts
and
the
tree
size
are
usually
m
uc
h
smaller
than
the
upp
er
b
ounds
imp
osed
b
y
the
a
v
ailable
memory
.
Therefore
TildeLDS
t
ypically
consumes
less
memory
than
Tildeclassic,
and
ma
y
b
e
preferable
ev
en
when
the
whole
data
set
can
b
e
loaded
in
to
main
memory
.
..
Sampling
While
the
ab
o
v
e
implemen
tation
is
one
step
to
w
ards
handling
large
data
sets,
there
will
alw
a
ys
b
e
data
sets
that
are
to
o
large
to
handle.
An
approac
h
that
is
often
tak
en
b
y
data
mining
systems
when
there
are
to
o
man
y
examples,
is
to
select
a
sample
from
the
data
and
learn
from
that
sample.
Suc
h
tec
hniques
are
incorp
orated
in
e.g.
C.
(Quinlan,
		a)
and
CAR
T
(Breiman
et
al.,
	).
In
the
standard
ILP
con
text
there
are
some
diculties
with
sampling,
whic
h
can
b
e
ascrib
ed
to
the
lac
k
of
a
lo
calit
y
assumption.
When
one
example
con
tains
information
that
is
relev
an
t
for
another
example,
either
b
oth
examples
ha
v
e
to
b
e
included
together
in
the
sample,
or
none
of
them
should.
Otherwise,
one
obtains
a
sample
in
whic
h
some
examples
ha
v
e
an
incomplete
description
(and
hence
are
noisy).
It
is
ev
en
p
ossible
that
no
go
o
d
sample
can
b
e
dra
wn
b
ecause
all
the
examples
are
related
to
one
another.
T
o
the
b
est
of
our
kno
wledge
sampling
has
receiv
ed
v
ery
little
atten
tion
inside
ILP
,
as
is
also
noted
b
y
F

urnkranz
(		a).
If
the
lo
calit
y
assumption
can
b
e
made,
suc
h
sampling
problems
do
not
o
ccur.
Pic
king
individual
examples
from
the
p
opulation
in
a
random
fashion,
indep
end-
en
tly
from
one
another,
is
sucien
t
to
create
a
go
o
d
sample.

	
Automatic
sampling
has
not
b
een
included
in
the
curren
t
Tilde
implemen
tations.
W
e
do
not
giv
e
this
high
priorit
y
b
ecause
Tilde
learns
from
a
at
le
of
data
whic
h
is
pro
duced
b
y
extracting
information
from
a
database
and
putting
related
information
together
(as
explained
earlier
in
this
text).
Sampling
should
b
e
done
at
the
lev
el
of
the
extraction
of
information,
not
b
y
Tilde
itself.
It
is
rather
inecien
t
to
con
v
ert
the
whole
database
in
to
a
at
le
and
then
use
only
a
part
of
that
le,
instead
of
only
con
v
erting
the
part
of
the
database
that
will
b
e
used.
W
e
do
not
presen
t
exp
erimen
ts
with
sampling,
as
the
eect
of
sampling
in
data
mining
is
out
of
the
scop
e
of
this
pap
er;
instead
w
e
refer
to
the
already
existing
studies
on
this
sub
ject
(see
e.g.
(Muggleton,
		;
F

urnkranz,
		b)).
..
Internal
V
alidation
A
topic
that
is
related
to
sampling
is
the
metho
dology
of
v
alidating
a
h
yp
othesis
in
ternally
,
i.e.
within
the
learning
set.
The
idea
is
that
a
part
of
the
learning
set
(the
v
alidation
set)
is
k
ept
apart
for
v
alidation
purp
oses,
and
the
rest
is
used
as
the
training
set
for
building
the
h
yp
othesis.
Suc
h
a
metho
dology
is
often
follo
w
ed
for
tuning
parameters
of
a
system
or
for
pruning.
Dividing
the
learning
set
in
a
training
and
v
alidation
set
p
oses
the
same
problem
as
sampling:
if
the
lo
calit
y
assumption
is
not
made
it
can
b
e
dicult
to
nd
a
go
o
d
partition.
F
or
an
ILP
system
that
do
es
mak
e
the
assumption
it
is
easy
to
incorp
orate
v
alidation
based
tec
hniques.
..
Sc
alability
De
Raedt
and
D

zeroski
(		)
ha
v
e
sho
wn
that
in
the
learning
from
in
terpretations
setting,
learning
rst-order
clausal
theories
is
tractable.
More
sp
ecically
,
giv
en
xed
b
ounds
on
the
maximal
length
of
clauses
and
the
maximal
arit
y
of
literals,
suc
h
theories
are
p
olynomial-sample
p
olynomial-time
P
A
C-learnable.
This
p
ositiv
e
result
is
related
directly
to
the
learning
from
in
terpretations
setting.
Quinlan
(	)
has
sho
wn
that
induction
of
decision
trees
has
time
complexit
y
O
(a

N

n)
where
a
is
the
n
um
b
er
of
attributes
of
eac
h
example,
N
is
the
n
um
b
er
of
examples
and
n
is
the
n
um
b
er
of
no
des
in
the
tree.
Since
Tilde
uses
basically
the
same
algorithm
as
Quinlan,
it
inherits
the
linearit
y
in
the
n
um
b
er
of
examples
and
in
the
n
um
b
er
of
no
des.
The
main
dierence
b
et
w
een
Tilde
and
C.,
as
w
e
already
noted,
is
the
generation
of
tests
in
a
no
de.
The
n
um
b
er
of
tests
to
b
e
considered
in
a
no
de
dep
ends
on
the
renemen
t
op
er-
ator.
There
is
no
theoretical
b
ound
on
this,
as
it
is
p
ossible
to
dene
renemen
t
op
erators
that
cause
an
innite
branc
hing
factor.
In
practice,
useful
renemen
t
op-
erators
alw
a
ys
generate
a
nite
n
um
b
er
of
renemen
ts,
but
ev
en
then
this
n
um
b
er
ma
y
not
b
e
b
ounded:
the
n
um
b
er
of
renemen
ts
t
ypically
increases
with
the
length
of
the
asso
ciated
query
of
the
no
de.
Also,
the
time
for
p
erforming
one
single
test
on
a
single
example
dep
ends
on
the
complexit
y
of
that
test
(it
is
in
the
w
orst
case
exp
onen
tial
in
the
length
of
the
conjunction).

0
Th
us,
w
e
can
sa
y
that
induction
of
rst
order
decision
trees
has
time
complexit
y
O
(N

n

t

c)
with
t
the
a
v
erage
n
um
b
er
of
tests
p
erformed
in
eac
h
no
de
and
c
the
a
v
erage
time
complexit
y
of
p
erforming
one
test
for
one
example,
if
those
a
v
erages
exist.
If
one
is
willing
to
accept
an
upp
er
b
ound
on
the
complexit
y
of
the
theory
that
is
to
b
e
learned
(whic
h
w
as
done
for
the
P
A
C-learning
results)
and
denes
a
nite
renemen
t
op
erator,
b
oth
the
complexit
y
of
p
erforming
a
single
test
on
a
single
example
and
the
n
um
b
er
of
tests
are
b
ounded
and
the
a
v
erages
do
exist.
Our
main
conclusion
from
this
is
that
the
time
complexit
y
of
Tilde
is
linear
in
the
n
um
b
er
of
examples.
This
is
a
stronger
claim
than
can
b
e
made
for
the
standard
ILP
setting.
The
time
complexit
y
also
dep
ends
on
the
global
complexit
y
of
the
theory
and
the
branc
hing
factor
of
the
renemen
t
op
erator.
This
could
p
oten
tially
b
e
a
problem,
but
w
e
ha
v
e
not
y
et
encoun
tered
an
y
applications
where
it
really
w
as
one.
..
Discr
etization
As
w
as
men
tioned
in
the
discussion
of
Tilde,
this
system
uses
a
discretization
tec
hnique
that
originated
in
prop
ositional
KDD
and
has
b
een
lifted
to
the
rst
order
framew
ork.
In
this
case,
the
lo
calit
y
assumption
is
not
essen
tial;
the
tec
hnique
could
also
b
e
incorp
orated
in
to
other
ILP
systems.
Still,
lik
e
the
main
induction
pro
cess,
the
discretization
pro
cess
can
b
e
made
more
ecien
t
if
the
lo
calit
y
assumption
is
exploited.
.
Exp
erimen
ts
In
this
exp
erimen
tal
section
w
e
try
to
v
alidate
our
claims
ab
out
time
complexit
y
empirically
,
and
explore
some
inuences
on
scalabilit
y
.
More
sp
ecically
,
w
e
w
an
t
to:

v
alidate
the
claim
that
when
the
lo
calization
assumption
is
exploited,
induction
time
is
linear
in
the
n
um
b
er
of
examples
(all
other
things
b
eing
equal,
i.e.
w
e
con
trol
for
other
inuences
on
induction
time
suc
h
as
the
size
of
the
tree)

study
the
inuence
of
lo
calization
on
induction
time
(b
y
quan
tifying
the
amoun
t
of
lo
calization
and
in
v
estigating
its
eect
on
the
time
complexit
y)

in
v
estigate
ho
w
the
induction
time
v
aries
with
the
size
of
the
data
set
in
more
practical
situations
(if
w
e
do
not
con
trol
other
inuences;
i.e.
a
larger
data
set
ma
y
cause
the
learner
to
induce
a
more
complex
theory
,
whic
h
in
itself
has
an
eect
on
the
induction
time)
Before
discussing
the
exp
erimen
ts
themselv
es,
w
e
describ
e
the
data
sets
that
w
e
ha
v
e
used.


..
Description
of
the
Data
Sets
...
R
ob
oCup
Data
Set
This
is
a
data
set
con
taining
data
ab
out
so
ccer
games
pla
y
ed
b
y
soft
w
are
agen
ts
training
for
the
Rob
oCup
comp
etition
(Kitano
et
al.,
		).
It
con
tains
	
examples
and
is
00MB
large.
Eac
h
example
consists
of
a
description
of
the
state
of
the
so
ccer
terrain
as
observ
ed
b
y
one
sp
ecic
pla
y
er
on
a
single
momen
t.
This
description
includes
the
iden
tit
y
of
the
pla
y
er,
the
p
ositions
of
all
pla
y
ers
and
of
the
ball,
the
time
at
whic
h
the
example
w
as
recorded,
the
action
the
pla
y
er
p
erformed,
and
the
time
at
whic
h
this
action
w
as
executed.
Figure

sho
ws
one
example.
While
this
data
set
w
ould
allo
w
rather
complicated
theories
to
b
e
constructed,
for
our
exp
erimen
ts
the
language
bias
w
as
v
ery
simple
and
consisted
of
a
prop
ositional
language
(only
high-lev
el
commands
are
learned).
This
use
of
the
data
set
reects
the
learning
tasks
considered
up
till
no
w
b
y
the
p
eople
who
are
using
it,
see
(Jacobs
et
al.,
		).
This
do
es
not
inuence
the
v
alidit
y
of
our
results
for
relational
languages,
b
ecause
the
prop
ositions
are
dened
b
y
the
bac
kground
kno
wledge
and
their
truth
v
alues
are
computed
at
run
time,
so
the
query
that
is
really
executed
is
relational.
F
or
instance,
the
prop
osition
have
ball,
indicating
whether
some
pla
y
er
of
the
team
has
the
ball
in
its
p
ossession,
is
computed
from
the
p
osition
of
the
pla
y
er
and
of
the
ball.
...
Poker
Data
Sets
The
P
ok
er
data
sets
are
articially
created
data
sets
where
eac
h
example
is
a
description
of
a
hand
of
v
e
cards,
together
with
a
name
for
the
hand
(pair,
three
of
a
kind,
.
.
.
).
The
aim
is
to
learn
denitions
for
sev
eral
p
ok
er
concepts
from
a
set
of
examples.
The
classes
that
are
considered
here
are
nothing,
pair,
two
pairs,
three
of
a
kind,
full
house
and
four
of
a
kind.
This
is,
of
course,
a
simplication
of
the
real
p
ok
er
domain,
where
more
classes
exist
and
it
is
necessary
to
distinguish
b
et
w
een
e.g.
a
pair
of
queens
and
a
pair
of
kings;
but
this
simplied
v
ersion
suces
to
illustrate
the
relev
an
t
topics
and
k
eeps
learning
times
sucien
tly
lo
w
to
allo
w
for
reasonably
extensiv
e
exp
erimen
ts.
Figure

illustrates
ho
w
one
example
in
the
p
ok
er
domain
can
b
e
represen
ted.
W
e
ha
v
e
created
the
data
sets
for
this
domain
using
a
program
that
randomly
generates
examples
for
this
domain.
The
adv
an
tage
of
this
approac
h
is
its
exibilit
y:
it
is
easy
to
create
m
ultiple
training
sets
of
increasing
size,
as
w
ell
as
an
indep
enden
t
test
set.
An
in
teresting
prop
ert
y
of
this
data
set
is
that
some
classes,
e.g.
four
of
a
kind,
are
v
ery
rare,
hence
a
large
data
set
is
needed
to
learn
these
classes
(assuming
the
data
are
generated
randomly).
...
Mutagenesis
Data
Set
The
Mutagenesis
dataset
(Sriniv
asan
et
al.,
		)
is
a
classic
b
enc
hmark
in
Inductiv
e
Logic
Programming.
The
set
that
has
b
een
used
most
in
the
literature
consists
of

examples.
Eac
h
example
describ
es
a
molecule.
Some
of
these
molecules
are
m
utagenic
(i.e.,
ma
y
cause
DNA
m
utations),
others
are
not.
The
task
is
to
predict
the
m
utagenicit
y
of
a
molecule
from
its
description.


begin(model(e)
).
player(my,,-.
0

,
-0
.
	

,

	).
player(my,,-.
	

	,
.0
0	
0	
,

).
player(my,,-.


,
-
.	

	
,
0
).
player(my,,-.


,.



,
)
.
player(my,,-.

0
,
.
	

,

).
player(my,,-.


	,

.
	
	,

)
.
player(my,,-.
	
	
,

.

,

)
.
player(my,,-.

	
	,
.

	
,

).
player(my,	,-.


,

.0

0,

)
.
player(my,0,-
.	


,
.

0
,

).
player(my,,-0
.
	0

,
.

	
,
)
.
player(other,,-
.


,
.


,

).
player(other,,0
.0
,0
.0,
0)
.
player(other,,-

.0
	

,
.
00

,
		)
.
player(other,,0
.0
,0
.0,
0)
.
player(other,,
.
0

,
	.


,

).
player(other,,-
	.
	0

,
.

	
,

).
player(other,,0
.0
,0
.0,
0)
.
player(other,,0
.0
,0
.0,
0)
.
player(other,	,-
.

	

,	.
	

,

	).
player(other,0,
0.

	

,
.

,

).
player(other,,
0.
0,
0.0
,0
).
ball(-.0	,
0.

0
	
,

).
mynumber().
rctime().
turn(.	0

)
.
actiontime().
end(model(e)).
Figur
e
.
The
Prolog
represen
tation
of
one
example
in
the
Rob
oCup
data
set.
A
fact
suc
h
as
player(other,,-.0	
,
.0
0
,		
)
means
that
pla
y
er

of
the
other
team
w
as
last
seen
at
p
osition
(-,.)
at
time
		.
A
p
osition
of
(0,0)
means
that
that
pla
y
er
has
nev
er
b
een
observ
ed
b
y
the
pla
y
er
that
has
generated
this
mo
del.
The
action
p
erformed
curren
tly
b
y
this
pla
y
er
is
turn(.	0):
it
is
turning
to
w
ards
the
ball.


begin(model()).
card(,spades).
card(queen,hearts
).
card(	,clubs).
card(	,spades).
card(ace,diamonds
).
pair.
end(model()).
Figur
e
.
An
example
from
the
P
ok
er
data
set.
The
data
set
is
a
t
ypical
ILP
data
set
in
that
the
example
descriptions
are
highly
structured,
and
there
is
bac
kground
kno
wledge
ab
out
the
domain.
Sev
eral
lev
els
of
bac
kground
kno
wledge
ha
v
e
b
een
studied
in
the
literature
(see
again
Sriniv
asan
et
al.
(		));
for
our
exp
erimen
ts
w
e
ha
v
e
alw
a
ys
used
the
simplest
bac
kground
kno
wledge,
i.e.
only
structural
information
ab
out
the
molecules
(the
atoms
and
b
onds
o
ccurring
in
them)
are
a
v
ailable.
Figure
	
sho
ws
a
part
of
the
description
of
one
molecule.
..
Materials
and
Settings
All
exp
erimen
ts
w
ere
p
erformed
with
the
t
w
o
implemen
tations
of
Tilde
w
e
dis-
cussed:
Tildeclassic
and
TildeLDS.
These
programs
are
implemen
ted
in
Prolog
and
run
under
the
MasterProlog
engine
(formerly
named
ProLog-b
y-BIM).
The
hardw
are
w
e
used
is
a
Sun
Ultra-
at

MHz,
running
the
Solaris
system
(except
when
stated
otherwise).
Both
Tildeclassic
and
TildeLDS
oer
the
p
ossibilit
y
to
precompile
the
data
le.
W
e
exploited
this
feature
for
all
our
exp
erimen
ts.
F
or
TildeLDS
this
raises
the
problem
that
in
order
to
load
one
example
at
a
time,
a
dieren
t
ob
ject
le
has
to
b
e
created
for
eac
h
example
(MasterProlog
oers
no
predicates
for
loading
only
a
part
of
an
ob
ject
le).
This
can
b
e
rather
impractical.
F
or
this
reason
sev
eral
examples
are
usually
compiled
in
to
one
ob
ject
le;
a
parameter
called
gr
anularity
(G)
con
trols
ho
w
man
y
examples
can
b
e
included
in
one
ob
ject
le.
Ob
ject
les
are
then
loaded
one
b
y
one
b
y
TildeLDS,
whic
h
means
that
G
examples
at
a
time
are
loaded
in
to
main
memory
(instead
of
one).
Because
of
this,
the
gran
ularit
y
parameter
can
ha
v
e
an
inuence
on
the
eciency
of
TildeLDS.
This
is
in
v
estigated
in
our
exp
erimen
ts.
By
default,
a
v
alue
of
0
w
as
used
for
G.


begin(model()).
pos.
atom(d_,c,,-
0.

).
atom(d_,c,,-
0.

).
atom(d_,c,,-
0.

).
atom(d_,c,	,
-0
.0
)
.
atom(d_,c,	,
0.
0
).
atom(d_,c,,-
0.

).
(...)
atom(d_,o,0,
-0
.
)
.
atom(d_,o,0,
-0
.
)
.
bond(d_,d_,
).
bond(d_,d_,
).
bond(d_,d_,
).
bond(d_,d_,
).
bond(d_,d_,
).
bond(d_,d_,
).
bond(d_,d_,
).
bond(d_,d_,
).
bond(d_,d_	,
).
(...)
bond(d_,d_	
,
).
bond(d_,d_
,
).
bond(d_,d_
,
).
end(model()).
Figur
e
	.
The
Prolog
represen
tation
of
one
example
in
the
Mutagenesis
data
set.
The
atom
facts
en
umerate
the
atoms
in
the
molecule.
F
or
eac
h
atom
its
elemen
t
(e.g.
carb
on),
t
yp
e
(e.g.
carb
on
can
o
ccur
in
sev
eral
congurations;
eac
h
t
yp
e
corresp
onds
to
one
sp
ecic
conguration)
and
partial
c
harge.
The
bond
facts
en
umerate
all
the
b
onds
b
et
w
een
the
atoms
(the
last
argumen
t
is
the
t
yp
e
of
the
b
ond:
single,
double,
aromatic,
etc.).
pos
denotes
that
the
molecule
b
elongs
to
the
p
ositiv
e
class
(i.e.
is
m
utagenic).


..
Exp
eriment
:
Time
Complexity
...
A
im
of
the
Exp
eriment
As
men
tioned
b
efore,
induction
of
trees
with
TildeLDS
should
in
principle
ha
v
e
a
time
complexit
y
that
is
linear
in
the
n
um
b
er
of
examples.
With
our
rst
exp
erimen
t
w
e
empirically
test
whether
our
imple-
men
tation
indeed
exhibits
this
prop
ert
y
.
W
e
also
compare
it
with
other
approac
hes
where
the
lo
calit
y
assumption
is
exploited
less
or
not
at
all.
W
e
distinguish
the
follo
wing
approac
hes:

loading
all
data
at
once
in
main
memory
without
exploiting
the
lo
calit
y
as-
sumption
(the
standard
ILP
approac
h)

loading
all
data
at
once
in
main
memory
,
exploiting
the
lo
calit
y
assumption;
this
is
what
Tildeclassic
do
es

loading
examples
one
at
a
time
in
main
memory;
this
is
what
TildeLDS
do
es
T
o
the
b
est
of
our
kno
wledge
all
ILP
systems
that
do
not
learn
from
in
terpret-
ations
follo
w
the
rst
approac
h
(with
the
exception
of
a
few
systems
that
access
an
external
database
directly
instead
of
loading
the
data
in
to
main
memory
,
e.g.
MOBAL
(Morik
and
Bro
c
khausen,
		)
;
but
these
systems
still
do
not
mak
e
a
lo
calit
y
assumption).
W
e
can
easily
sim
ulate
this
approac
h
with
Tildeclassic
b
y
sp
ecifying
all
information
ab
out
the
examples
as
bac
kground
kno
wledge.
F
or
the
bac
kground
kno
wledge
no
lo
calit
y
assumption
can
b
e
made,
since
all
bac
kground
kno
wledge
is
p
oten
tially
relev
an
t
for
eac
h
example.
The
p
erformance
of
a
Prolog
system
that
w
orks
with
a
large
database
is
impro
v
ed
signican
tly
if
indexes
are
built
for
the
predicates.
On
the
other
hand,
adding
in-
dexes
for
predicates
creates
some
o
v
erhead
with
resp
ect
to
the
in
ternal
space
that
is
needed,
and
a
lot
of
o
v
erhead
for
the
compiler.
The
MasterProlog
system
b
y
default
indexes
all
predicates,
but
this
indexing
can
b
e
switc
hed
o.
W
e
ha
v
e
p
erformed
exp
erimen
ts
for
the
standard
ILP
approac
h
b
oth
with
and
without
indexing
(th
us,
the
rst
approac
h
in
the
ab
o
v
e
list
is
actually
sub
divided
in
to
\indexed"
and
\not
indexed").
...
Metho
dolo
gy
Since
the
aim
of
this
exp
erimen
t
is
to
determine
the
inuence
of
the
n
um
b
er
of
examples
(and
only
that)
on
time
and
space
complexit
y
,
w
e
w
an
t
to
con
trol
as
m
uc
h
as
p
ossible
other
factors
that
migh
t
also
ha
v
e
an
inuence.
W
e
ha
v
e
seen
in
Section
.
that
these
other
factors
include
the
n
um
b
er
of
no
des
n,
the
a
v
erage
n
um
b
er
of
tests
p
er
no
de
t
and
the
a
v
erage
complexit
y
of
p
erforming
one
test
on
one
single
example
c.
c
dep
ends
on
b
oth
the
complexit
y
of
the
queries
themselv
es
and
on
the
example
sizes.
When
v
arying
the
n
um
b
er
of
examples
for
our
exp
erimen
ts,
w
e
w
an
t
to
k
eep
these
factors
constan
t.
This
means
that
rst
of
all
the
renemen
t
op
erator
should
b
e
the
same
for
all
the
exp
erimen
ts.
This
is
automatically
the
case
if
the
user
do
es
not
c
hange
the
renemen
t
op
erator
sp
ecication
(the
rmode
facts)
b
et
w
een
consecutiv
e
exp
erimen
ts.


The
other
factors
can
b
e
k
ept
constan
t
b
y
ensuring
that
the
same
tree
is
built
in
eac
h
exp
erimen
t,
and
that
the
a
v
erage
complexit
y
of
the
examples
do
es
not
c
hange.
In
order
to
ac
hiev
e
this,
w
e
adopt
the
follo
wing
metho
dology
.
W
e
create,
from
a
small
data
set,
larger
data
sets
b
y
including
eac
h
single
example
sev
eral
times.
By
ensuring
that
all
the
examples
o
ccur
an
equal
n
um
b
er
of
times
in
the
resulting
data
set,
the
class
distribution,
a
v
erage
complexit
y
of
testing
a
query
on
an
example
etc.
are
all
k
ept
constan
t.
In
other
w
ords,
all
v
ariation
due
to
the
inuence
of
individual
examples
is
remo
v
ed.
Because
the
class
distribution
sta
ys
the
same,
the
test
that
is
c
hosen
in
eac
h
no
de
also
sta
ys
the
same.
This
is
necessary
to
ensure
that
the
same
tree
is
gro
wn,
but
not
sucien
t:
the
stopping
criterion
needs
to
b
e
adapted
as
w
ell
so
that
a
no
de
that
cannot
b
e
split
further
for
the
small
data
set
is
not
split
when
using
the
larger
data
set
either.
In
order
to
ac
hiev
e
this,
the
minimal
n
um
b
er
of
examples
that
ha
v
e
to
b
e
co
v
ered
b
y
eac
h
leaf
(whic
h
is
a
parameter
of
Tilde)
is
increased
prop
ortionally
to
the
size
of
the
data
set.
By
follo
wing
this
metho
dology
,
the
men
tioned
un
w
an
ted
inuences
are
ltered
out
of
the
results.
...
Materials
W
e
used
the
Mutagenesis
data
set
for
this
exp
erimen
t.
Other
materials
are
as
describ
ed
in
Section
..
...
Setup
of
the
Exp
eriment
F
our
dieren
t
v
ersions
of
Tilde
are
compared:

Tildeclassic
without
lo
calit
y
assumption,
without
indexing

Tildeclassic
without
lo
calit
y
assumption,
with
indexing

Tildeclassic
with
lo
calit
y
assumption

TildeLDS
The
rst
three
\v
ersions"
are
actually
the
same
v
ersion
of
Tilde
as
far
as
the
implemen
tation
of
the
learning
algorithm
is
concerned,
but
dier
in
the
w
a
y
the
data
are
represen
ted
and
in
the
w
a
y
the
underlying
Prolog
system
handles
them.
Eac
h
Tilde
v
ersion
w
as
rst
run
on
the
original
data
set,
then
on
data
sets
that
con
tain
eac
h
original
example

n
times,
with
n
ranging
from

to
	.
T
able

summarizes
some
prop
erties
of
the
data
sets
that
w
ere
obtained
in
this
fashion.
F
or
eac
h
run
on
eac
h
data
set
w
e
ha
v
e
recorded
the
follo
wing:

the
time
needed
for
the
induction
pro
cess
itself
(in
CPU-seconds)

the
time
needed
to
compile
the
data
(in
CPU-seconds).
The
dieren
t
systems
compile
the
data
in
dieren
t
w
a
ys
(e.g.
according
to
whether
indexes
need
to
b
e
built).
As
compilation
of
the
data
need
only
b
e
done
once,
ev
en
if
afterw
ards
sev
eral
runs
of
the
induction
system
are
done,
compilation
time
ma
y
seem
less
relev
an
t.
Still,
it
is
imp
ortan
t
to
see
ho
w
the
compilation
scales
up,
since
it
is
not
really
useful
to
ha
v
e
an
induction
metho
d
that
scales
linearly
if
it
needs
a
prepro
cessing
step
that
scales
sup
er-linearly
.


T
able
.
Prop
erties
of
the
example
sets
m
ultiplication
factor
#examples
#facts
size
(MB)


0
0.


0
0.


0


0
0	


00
	


0



0



0
,,



,	,0


	
,,
0
T
able
.
Scaling
prop
erties
of
TildeLDS
in
terms
of
the
n
um
b
er
of
examples
m
ultiplication
time
(CPU
seconds)
factor
induction
compilation





.

	
.

		


0
0

	0
	


	

	
	


		


	
...
Discussion
of
the
R
esults
T
ables
,
,

and

giv
e
an
o
v
erview
of
the
time
eac
h
Tilde
v
ersion
needed
to
induce
a
tree
for
eac
h
set,
as
w
ell
as
the
time
it
to
ok
to
compile
the
data
in
to
the
correct
format.
The
results
are
sho
wn
graphically
in
Figure
0.
Note
that
b
oth
the
n
um
b
er
of
examples
and
time
are
indicated
on
a
logarithmic
scale.
Care
m
ust
b
e
tak
en
when
in
terpreting
these
graphs:
a
straigh
t
line
do
es
not
indicate
a
linear
relationship
b
et
w
een
the
v
ariables.
Indeed,
if
log
y
=
n

log
x,
then
y
=
x
n
.
This
means
the
slop
e
of
the
line
should
b
e

in
order
to
ha
v
e
a
linear
relationship,
while

indicates
a
quadratic
relationship,
and
so
on.
In
order
to
mak
e
it
easier
to
recognize
a
linear
relationship
(slop
e
),
the
function
y
=
x
has
b
een
dra
wn
on
the
graphs
as
a
reference.
Note
that
only
TildeLDS
scales
up
w
ell
to
large
data
sets.
The
other
v
ersions
of
Tilde
had
problems
loading
or
compiling
the
data
from
a
m
ultiplication
factor
of

or

on.
The
graphs
and
tables
sho
w
that
induction
time
is
linear
in
the
n
um
b
er
of
ex-
amples
for
TildeLDS,
for
Tildeclassic
with
lo
calit
y
,
and
for
Tildeclassic
without
lo
calit
y
but
with
indexing.
F
or
Tildeclassic
without
lo
calit
y
or
indexing
the
in-
duction
time
increases
quadratically
with
the
n
um
b
er
of
examples.
This
is
not


T
able
.
Scaling
prop
erties
of
Tildeclassic
in
terms
of
the
n
um
b
er
of
examples
m
ultiplication
time
(CPU
seconds)
factor
induction
compilation

.
.

.
.

.
.

.
.

	.
0.

?*
.
*
Prolog
engine
failed
to
load
the
data
T
able
.
Scaling
prop
erties
of
Tilde
without
lo
calit
y
assumption,
with
indexing,
in
terms
of
n
um
b
er
of
examples
m
ultiplication
time
(CPU
seconds)
factor
induction
compilation

.
0.

.
	

.	


.
0

?*


?*

*
Prolog
engine
failed
to
load
the
data
T
able
.
Scaling
prop
erties
of
Tilde
without
lo
calit
y
assumption,
without
indexing,
in
terms
of
n
um
b
er
of
examples
m
ultiplication
time
(CPU
seconds)
factor
induction
compilation

0
.


.	

	
.

0	
.

?*
.

?**
*
Prolog
engine
failed
to
load
the
data
**
Prolog
compiler
failed
to
compile
the
data

	
1
10
100
1000
10000
100000
1e+06
1
10
100
1000
Induction time (CPU-seconds)
Multiplication factor
LDS
classic
No locality, indexing
No locality, no indexing
y=x
1
10
100
1000
10000
1
10
100
1000
Compilation time (CPU-seconds)
Multiplication factor
LDS
classic
No locality, indexing
No locality, no indexing
y=x
Figur
e
0.
Scaling
prop
erties
of
TildeLDS
in
terms
of
n
um
b
er
of
examples
unexp
ected,
as
in
this
setting
the
time
needed
to
run
a
test
on
one
single
example
increases
with
the
size
of
the
dataset.
With
resp
ect
to
compilation
times,
w
e
note
that
all
are
linear
in
the
size
of
the
data
set,
except
Tildeclassic
without
lo
calit
y
and
with
indexing.
This
is
in
corresp
ondence
with
the
fact
that
building
an
index
for
the
predicates
in
a
deductiv
e
database
is
an
exp
ensiv
e
op
eration,
sup
er-linear
in
the
size
of
the
database.
F
urthermore,
the
exp
erimen
ts
conrm
that
Tildeclassic
with
lo
calit
y
scales
as
w
ell
as
TildeLDS
with
resp
ect
to
time
complexit
y
,
but
for
large
data
sets
runs
in
to
problems
b
ecause
it
cannot
load
all
the
data.
Observing
that
without
indexing
induction
time
increases
quadratically
,
and
with
indexing
compilation
time
increases
quadratically
,
w
e
conclude
that
the
lo
calit
y
assumption
is
indeed
crucial
to
our
linearit
y
results,
and
that
loading
only
a
few
examples
at
a
time
in
main
memory
mak
es
it
p
ossible
to
handle
m
uc
h
larger
data
sets.
..
Exp
eriment
:
The
Ee
ct
of
L
o
c
alization
...
A
im
of
the
exp
eriment
In
the
previous
exp
erimen
t
w
e
studied
the
eect
of
the
n
um
b
er
of
examples
on
time
complexit
y
,
and
observ
ed
that
this
eect
is
dieren
t
according
to
whether
the
lo
calit
y
assumption
is
made.
In
this
exp
erimen
t
w
e
do
not
just
distinguish
b
et
w
een
lo
calized
and
not
lo
calized,
but
consider
gradual
c
hanges
in
lo
calization,
and
th
us
try
to
quan
tify
the
eect
of
lo
calization
on
the
induction
time.
...
Metho
dolo
gy
W
e
can
test
the
inuence
of
lo
calization
on
the
eciency
of
TildeLDS
b
y
v
arying
the
gran
ularit
y
parameter
G
in
TildeLDS.
G
is
the
n
um
b
er
of
examples
that
are
loaded
in
to
main
memory
at
the
same
time.
Lo
calization
of
information
is
stronger
when
G
is
smaller.

0
The
eect
of
G
w
as
tested
b
y
running
TildeLDS
successiv
ely
on
the
same
data
set,
under
the
same
circumstances,
but
with
dieren
t
v
alues
for
G.
In
these
exp
eri-
men
ts
G
ranged
from

to
00.
F
or
eac
h
v
alue
of
G
b
oth
compilation
and
induction
w
ere
p
erformed
ten
times;
the
rep
orted
times
are
the
means
of
these
ten
runs.
...
Materials
W
e
ha
v
e
used
three
data
sets:
a
Rob
oCup
data
set
with
0000
examples,
a
P
ok
er
data
set
con
taining
000
examples,
and
the
Mutagenesis
data
set
with
a
m
ultiplication
factor
of

(i.e.
0
examples).
The
data
sets
w
ere
c
hosen
to
con
tain
a
sucien
t
n
um
b
er
of
examples
to
mak
e
it
p
ossible
to
let
G
v
ary
o
v
er
a
relativ
ely
broad
range,
but
not
more
(to
limit
the
exp
erimen
tation
time).
Other
materials
are
as
describ
ed
in
Section
..
...
Discussion
of
the
R
esults
Induction
times
and
compilation
times
are
plot-
ted
v
ersus
gran
ularit
y
in
Figure
.
It
can
b
e
seen
from
these
plots
that
induction
time
increases
appro
ximately
linearly
with
gran
ularit
y
.
F
or
v
ery
small
gran
ularities,
to
o,
the
induction
time
can
increase.
W
e
susp
ect
that
this
eect
can
b
e
attributed
to
an
o
v
erhead
of
disk
access
(loading
man
y
small
les,
instead
of
few
er
larger
les).
A
similar
eect
is
seen
when
w
e
lo
ok
at
the
compilation
times:
these
decrease
when
the
gran
ularit
y
increases,
but
asymptotically
approac
h
a
constan
t.
This
again
sug-
gests
an
o
v
erhead
caused
b
y
compiling
man
y
small
les
instead
of
one
large
le.
The
fact
that
the
observ
ed
eect
is
smallest
for
Mutagenesis,
where
individual
examples
are
larger,
increases
the
plausibilit
y
of
this
explanation.
This
exp
erimen
t
clearly
sho
ws
that
the
p
erformance
of
TildeLDS
strongly
de-
p
ends
on
G,
and
that
a
reasonably
small
v
alue
for
G
is
preferable.
It
th
us
conrms
the
h
yp
othesis
that
lo
calization
of
information
is
adv
an
tageous
with
resp
ect
to
time
complexit
y
.
..
Exp
eriment
:
Pr
actic
al
Sc
aling
Pr
op
erties
...
A
im
of
the
exp
eriment
With
this
exp
erimen
t
w
e
w
an
t
to
measure
ho
w
w
ell
TildeLDS
scales
up
in
practice,
without
con
trolling
an
y
inuences.
This
means
that
the
tree
that
is
induced
is
not
guaran
teed
to
b
e
the
same
one
or
ha
v
e
the
same
size,
and
that
a
natural
v
ariation
is
allo
w
ed
with
resp
ect
to
the
complexit
y
of
the
examples
as
w
ell
as
the
complexit
y
of
the
queries.
This
exp
erimen
t
is
th
us
mean
t
to
mimic
the
situations
that
arise
in
practice.
Since
dieren
t
trees
ma
y
b
e
gro
wn
on
dieren
t
data
sets,
the
qualit
y
of
these
trees
ma
y
dier.
W
e
in
v
estigate
this
as
w
ell.
...
Metho
dolo
gy
The
metho
dology
w
e
follo
w
is
to
c
ho
ose
some
domain
and
then
create
data
sets
with
dieren
t
sizes
for
this
domain.
TildeLDS
is
then
run
on
eac
h
data
set,
and
for
eac
h
run
the
induction
time
is
recorded,
as
w
ell
as
the
qualit
y
of
the
tree
(according
to
dieren
t
criteria,
see
b
elo
w).


0
1000
2000
3000
4000
5000
6000
7000
8000
9000
0
50
100
150
200
Induction time (CPU-seconds)
Granularity
Poker
Mutagenesis
Robocup
0
500
1000
1500
2000
2500
3000
3500
0
5
10
15
20
25
30
Induction time (CPU-seconds)
Granularity
Poker
Mutagenesis
Robocup
full
range
zo
omed
in
on
in
terv
al
[0-0]
0
50
100
150
200
250
300
350
400
0
50
100
150
200
Compilation time (CPU-seconds)
Granularity
Poker
Mutagenesis
Robocup
Figur
e
.
The
eect
of
gran
ularit
y
on
induction
and
compilation
time


T
able
.
Consumed
CPU-time
and
accuracy
of
h
yp
otheses
pro
duced
b
y
TildeLDS
in
the
P
ok
er
domain
#examples
compilation
induction
accuracy
(CPU-seconds)
(CPU-seconds)
00
.

0.	
000
.0
0
0.		
000
.

0.		
0000
.	

0.			
0000
.
	
0.			
00000
0.

.0
1
10
100
1000
10000
100000
1e+06
100
1000
10000
100000
Time (CPU-seconds)
# examples
Induction
Compilation
x
0.988
0.99
0.992
0.994
0.996
0.998
1
100
1000
10000
100000
Predictive accuracy
# examples
Accuracy
Figur
e
.
Consumed
CPU-time
and
accuracy
of
h
yp
otheses
pro
duced
b
y
TildeLDS
in
the
P
ok
er
domain,
plotted
against
the
n
um
b
er
of
examples
...
Materials
Data
sets
from
t
w
o
domains
w
ere
used:
Rob
oCup
and
P
ok
er.
These
domains
w
ere
c
hosen
b
ecause
large
data
sets
w
ere
a
v
ailable
for
them.
F
or
eac
h
domain
sev
eral
data
sets
of
increasing
size
w
ere
created.
Whereas
induction
times
ha
v
e
b
een
measured
on
b
oth
data
sets,
predictiv
e
accur-
acy
has
b
een
measured
only
for
the
P
ok
er
data
set.
This
w
as
done
using
a
separate
test
set
of
00,000
examples,
whic
h
w
as
the
same
for
all
the
h
yp
otheses.
F
or
the
Rob
oCup
data
set
in
terpretabilit
y
of
the
h
yp
otheses
b
y
domain
exp
erts
is
the
main
ev
aluation
criterion
(b
ecause
these
theories
are
used
for
v
erication
of
the
b
eha
vior
of
agen
ts,
see
(Jacobs
et
al.,
		)
).
The
Rob
oCup
exp
erimen
ts
ha
v
e
b
een
run
on
a
SUN
SP
AR
Cstation-0
at
00
MHz;
for
the
P
ok
er
exp
erimen
ts
a
SUN
Ultra-
at

MHz
w
as
used.
...
Discussion
of
the
R
esults
T
able

sho
ws
the
consumed
CPU-times
in
func-
tion
of
the
n
um
b
er
of
examples,
as
w
ell
as
the
predictiv
e
accuracy
.
These
gures
are
plotted
in
Figure
.
Note
that
the
CPU-time
graph
is
again
plotted
on
a
double
logarithmic
scale.
With
resp
ect
to
accuracy
,
the
P
ok
er
h
yp
otheses
sho
w
the
exp
ected
b
eha
vior:
when
more
data
are
a
v
ailable,
the
h
yp
otheses
can
predict
v
ery
rare
classes
(for
whic
h
no
examples
o
ccur
in
smaller
data
sets),
whic
h
results
in
higher
accuracy
.


T
able
.
Consumed
CPU-time
of
h
yp
otheses
pro
duced
b
y
TildeLDS
in
the
Rob
oCup
domain
#examples
compilation
induction
0000




0000

	


0000




0000
0
	


0000
0
0

0
0000
	



0000
	
	

0
0000

0


	

0

0
0
10000
20000
30000
40000
50000
60000
10000
20000
30000
40000
50000
60000
70000
80000
90000
Time (CPU-seconds)
# examples
Induction
Compilation
Figur
e
.
Consumed
CPU-time
for
TildeLDS
in
the
Rob
oCup
domain,
plotted
against
the
n
um
b
er
of
examples


The
graphs
further
sho
w
that
in
the
P
ok
er
domain,
TildeLDS
scales
up
linearly
,
ev
en
though
more
accurate
(and
sligh
tly
more
complex)
theories
are
found
for
larger
data
sets.
In
the
Rob
oCup
domain,
the
induced
h
yp
otheses
w
ere
the
same
for
all
runs
except
the
0000
examples
run.
In
this
single
case
the
h
yp
othesis
w
as
more
simple
and,
according
to
the
domain
exp
ert,
less
informativ
e
than
for
the
other
runs.
This
suggests
that
in
this
domain
a
relativ
ely
small
set
of
examples
(0000)
suces
to
learn
from.
It
is
harder
to
see
ho
w
TildeLDS
scales
up
for
the
Rob
oCup
data.
Since
the
same
tree
is
returned
in
all
runs
except
the
0000
examples
run,
one
w
ould
exp
ect
the
induction
times
to
gro
w
linearly
.
Ho
w
ev
er,
the
observ
ed
curv
e
do
es
not
seem
linear,
although
it
do
es
not
sho
w
a
clear
tendency
to
b
e
sup
er-linear
either.
Because
large
v
ariations
in
induction
time
w
ere
observ
ed,
w
e
p
erformed
these
runs
0
times;
the
estimated
mean
induction
times
are
rep
orted
together
with
their
standard
errors.
The
standard
errors
alone
cannot
explain
the
observ
ed
deviations,
nor
can
v
ariations
in
example
complexit
y
(all
examples
are
of
equal
complexit
y
in
this
domain).
A
p
ossible
explanation
is
the
fact
that
the
Prolog
engine
p
erforms
a
n
um
b
er
of
tasks
that
are
not
con
trolled
b
y
Tilde,
suc
h
as
garbage
collection.
In
sp
ecic
cases,
the
Prolog
engine
ma
y
p
erform
man
y
garbage
collections
b
efore
expanding
its
memory
space
(this
happ
ens
when
the
amoun
t
of
free
memory
after
garbage
collection
is
alw
a
ys
just
ab
o
v
e
some
threshold),
and
the
time
needed
for
these
garbage
collections
is
included
in
the
measured
CPU-times.
The
MasterProlog
engine
is
kno
wn
to
sometimes
exhibit
suc
h
b
eha
vior.
In
order
to
sort
this
out,
TildeLDS
w
ould
ha
v
e
to
b
e
reimplemen
ted
in
a
lo
w
er-
lev
el
language
than
Prolog,
where
one
has
full
con
trol
o
v
er
all
computations
that
o
ccur.
Suc
h
a
reimplemen
tation
is
planned.
T
o
conclude,
w
e
remark
that
one
should
b
e
cautious
when
generalizing
from
the
exp
erimen
tal
results
in
this
section,
as
they
are
almost
certainly
strongly
domain-
dep
enden
t.
It
seems
reasonable
to
sa
y
that
they
at
least
sho
w
that
the
linear
scaling
prop
ert
y
can
b
e
exp
ected
to
o
ccur
in
practice
with
a
reasonable
probabilit
y
.
.
Related
W
ork
Our
w
ork
is
closely
related
to
eorts
in
the
prop
ositional
learning
eld
to
increase
the
capabilit
y
of
mac
hine
learning
systems
to
handle
large
databases.
It
has
b
een
inuenced
more
sp
ecically
b
y
a
tutorial
on
data
mining
b
y
Usama
F
a
yy
ad,
in
whic
h
the
w
ork
of
Meh
ta
and
others
w
as
men
tioned
(Meh
ta
et
al.,
		;
Shafer
et
al.,
		).
They
w
ere
the
rst
to
prop
ose
the
lev
el-wise
tree
building
algorithm
w
e
adopted,
and
to
implemen
t
it
in
the
SLIQ
(Meh
ta
et
al.,
		)
and
SPRINT
(Shafer
et
al.,
		)
systems.
The
main
dierence
with
our
approac
h
is
that
SLIQ
and
SPRINT
learn
from
one
single
relation,
while
TildeLDS
can
learn
from
m
ultiple
relations.
Related
w
ork
inside
ILP
includes
the
MOBAL
system
(Morik
and
Bro
c
khausen,
		)
,
whic
h
presen
ts
the
rst
approac
h
to
coupling
an
ILP
system
with
a
relational
database
managemen
t
system
(RDBMS).
Being
an
ILP
system,
MOBAL
also
learns


from
m
ultiple
relations.
The
approac
h
follo
w
ed
is
that
a
logical
test
that
is
to
b
e
p
erformed
is
con
v
erted
in
to
an
SQL
query
and
sen
t
to
an
external
relational
database
managemen
t
system.
This
approac
h
is
essen
tially
dieren
t
from
ours,
in
that
it
exploits
as
m
uc
h
as
p
ossible
the
p
o
w
er
of
the
RDBMS
to
ecien
tly
ev
aluate
queries.
Also,
there
is
no
need
for
prepro
cessing
the
data.
Disadv
an
tages
are
that
for
eac
h
query
an
external
database
is
accessed,
whic
h
is
relativ
ely
slo
w,
and
that
it
is
less
exible
with
resp
ect
to
bac
kground
kno
wledge.
W
e
also
men
tion
the
KEPLER
system
(W
rob
el
et
al.,
		)
,
a
data
mining
to
ol
that
pro
vides
a
framew
ork
for
applying
a
broad
range
of
data
mining
systems
to
data
sets;
this
includes
ILP
systems.
KEPLER
w
as
delib
erately
designed
to
b
e
v
ery
op
en,
and
systems
using
the
learning
from
in
terpretations
setting
can
b
e
plugged
in
to
it
as
easily
as
other
systems.
A
t
this
momen
t
few
systems
use
the
learning
from
in
terpretations
setting
(De
Raedt
and
V
an
Laer,
		;
De
Raedt
and
Dehasp
e,
		;
Dehasp
e
and
De
Raedt,
		).
Of
these
the
researc
h
describ
ed
in
(Dehasp
e
and
De
Raedt,
		)
(the
W
armr
system:
nding
asso
ciation
rules
o
v
er
m
ultiple
relations;
see
also
Dehasp
e
and
T
oiv
onen's
con
tribution
in
this
issue)
is
most
closely
related
to
the
w
ork
de-
scrib
ed
in
this
pap
er,
in
the
sense
that
there,
to
o,
an
eort
w
as
made
to
adapt
the
system
for
large
databases.
The
fo
cus
of
that
text
is
not
on
the
adv
an
tages
of
learning
from
in
terpretations
in
general,
ho
w
ev
er,
but
on
the
p
o
w
er
of
rst
order
asso
ciation
rules.
More
lo
osely
related
w
ork
inside
ILP
w
ould
include
all
eorts
to
mak
e
ILP
systems
more
ecien
t.
Since
most
of
this
w
ork
concerns
ILP
systems
that
w
ork
in
the
classical
ILP
setting,
the
w
a
ys
in
whic
h
this
is
done
usually
dier
substan
tially
from
what
w
e
describ
e
in
this
pap
er.
F
or
instance,
the
w
ell-kno
wn
ILP
system
Progol
(Muggleton,
		)
has
recen
tly
b
een
extended
with
cac
hing
and
other
eciency
impro
v
emen
ts
(Cussens,
		).
Finally
,
the
Tilde
system
is
related
to
other
systems
that
induce
rst
order
decision
trees,
suc
h
as
the
STR
UCT
system
(W
atanab
e
and
Rendell,
		)
(whic
h
uses
a
less
explicitly
logic-based
approac
h)
and
the
regression
tree
learner
SR
T
(Kramer,
		).
.
Conclusions
W
e
ha
v
e
argued
and
demonstrated
empirically
that
the
use
of
ILP
is
not
limited
to
small
databases,
as
is
often
assumed.
Mining
databases
of
a
h
undred
megab
ytes
w
as
sho
wn
to
b
e
feasible,
and
this
do
es
not
seem
to
b
e
a
limit.
The
p
ositiv
e
results
that
ha
v
e
b
een
obtained
are
due
mainly
to
the
use
of
the
le
arning
fr
om
interpr
etations
setting,
whic
h
is
more
scalable
than
the
classical
ILP
setting
and
mak
es
the
link
with
prop
ositional
learning
more
clear.
This
means
that
a
lot
of
results
obtained
for
prop
ositional
learning
can
b
e
extrap
olated
to
learning
from
in
terpretations.
W
e
ha
v
e
discussed
a
n
um
b
er
of
suc
h
upgrades,
using
the
TildeLDS
system
as
an
illustration.
The
p
ossibilit
y
to
upgrade
the
w
ork
b
y
Meh
ta
et
al.
(		)
has
turned
out
to
b
e
crucial
for
handling
large
data
sets.
It


is
not
clear
ho
w
the
same
tec
hnique
could
b
e
incorp
orated
in
a
system
using
the
classical
ILP
setting.
Although
w
e
obtained
sp
ecic
results
only
for
a
sp
ecic
kind
of
data
mining
(in-
duction
of
decision
trees),
the
results
are
generalizable
not
only
to
other
approac
hes
within
the
classication
con
text
(e.g.
rule
based
approac
hes)
but
also
to
other
in-
ductiv
e
tasks
within
the
learning
from
in
terpretations
setting,
suc
h
as
clustering,
regression
and
induction
of
asso
ciation
rules.
Ac
kno
wledgmen
ts
Nico
Jacobs
and
Hendrik
Blo
c
k
eel
are
supp
orted
b
y
the
Flemish
Institute
for
the
Promotion
of
Scien
tic
and
T
ec
hnological
Researc
h
in
the
Industry
(IWT).
Luc
De
Raedt
is
supp
orted
b
y
the
F
und
for
Scien
tic
Researc
h,
Flanders.
This
w
ork
is
also
part
of
the
Europ
ean
Comm
unit
y
Esprit
pro
ject
no.
0,
Inductiv
e
Logic
Programming
.
The
authors
thank
Luc
Dehasp
e,
Kurt
Driessens,
H

el

ene
Legras
and
Jan
Ramon
for
pro
ofreading
this
text,
as
w
ell
as
the
anon
ymous
review
ers
for
their
v
ery
v
aluable
commen
ts
on
an
earlier
draft.
Notes
.
E.g.,
testing
the
co
v
erage
of
member
(a;
[b;
a])
ma
y
dep
end
on
member
(a;
[a]).
.
The
syn
tax
used
here
diers
from
the
actual
syn
tax
used
b
y
curren
t
implemen
tations
of
Tilde,
as
it
w
as
also
men
tioned
in
e.g.
(Blo
c
k
eel
and
De
Raedt,
		).
W
e
feel
that
the
one
that
is
used
here
is
clearer,
and
plan
to
use
it
in
future
implemen
tations
of
Tilde.
.
The
results
of
all
queries
for
eac
h
example
are
stored
in
this
manner,
so
that
when
the
b
est
query
is
c
hosen
after
one
pass
through
the
data,
these
results
can
b
e
retriev
ed
from
the
auxiliary
le,
a
v
oiding
a
second
pass
through
the
data.
References
Agra
w
al,
R.,
H.
Mannila,
R.
Srik
an
t,
H.
T
oiv
onen,
and
A.I.
V
erk
amo
(		).
F
ast
disco
v
ery
of
asso
ciation
rules.
In
U.
F
a
yy
ad,
G.
Piatetsky-Shapiro,
P
.
Sm
yth,
and
R.
Uth
urusam
y
,
editors,
A
dvanc
es
in
Know
le
dge
Disc
overy
and
Data
Mining,
pages
0{.
The
MIT
Press.
Blo
c
k
eel,
H.
and
L.
De
Raedt
(		).
Lo
ok
ahead
and
discretization
in
ILP.
In
Pr
o
c
e
e
dings
of
the
th
International
Workshop
on
Inductive
L
o
gic
Pr
o
gr
amming,
v
olume
	
of
L
e
ctur
e
Notes
in
A
rticial
Intel
ligenc
e,
pages
{.
Springer-V
erlag.
Blo
c
k
eel,
H.
and
L.
De
Raedt
(		).
T
op-do
wn
induction
of
rst
order
logical
decision
trees.
A
rticial
Intel
ligenc
e,
0(-):{	.
Blo
c
k
eel,
H.,
L.
De
Raedt,
and
J.
Ramon
(		).
T
op-do
wn
induction
of
clustering
trees.
In
Pr
o-
c
e
e
dings
of
the
th
International
Confer
enc
e
on
Machine
L
e
arning,
pages
{.
http://www.
cs.kuleuven.ac.be/~ml/PS
/ML
	-
.ps
.
Bongard,
M.
(	0).
Pattern
R
e
c
o
gnition.
Spartan
Bo
oks.
Bratk
o,
I.
(		0).
Pr
olo
g
Pr
o
gr
amming
for
A
rticial
Intel
ligenc
e.
Addison-W
esley
.
nd
Edition.
Bratk
o,
I.
and
S.
Muggleton
(		).
Applications
of
inductiv
e
logic
programming.
Communic
a-
tions
of
the
A
CM,
():{0.
Breiman,
L.,
J.H.
F
riedman,
R.A.
Olshen,
and
C.J.
Stone
(	).
Classic
ation
and
R
e
gr
ession
T
r
e
es.
W
adsw
orth,
Belmon
t.


Cohen,
W.W.
(		).
P
ac-learning
recursiv
e
logic
programs:
Negativ
e
results.
Journal
of
A
rticial
Intel
ligenc
e
R
ese
ar
ch,
:{.
Cussens,
J.
(		).
P
art-of-sp
eec
h
tagging
using
progol.
In
Pr
o
c
e
e
dings
of
the
th
International
Workshop
on
Inductive
L
o
gic
Pr
o
gr
amming,
Lecture
Notes
in
Articial
In
telligence,
pages
	{
0.
Springer-V
erlag.
De
Raedt,
L.,
editor
(		).
A
dvanc
es
in
Inductive
L
o
gic
Pr
o
gr
amming,
v
olume

of
F
r
ontiers
in
A
rticial
Intel
ligenc
e
and
Applic
ations.
IOS
Press.
De
Raedt,
L.
(		).
Logical
settings
for
concept
learning.
A
rticial
Intel
ligenc
e,
	:{0.
De
Raedt,
L.,
H.
Blo
c
k
eel,
L.
Dehasp
e,
and
W.
V
an
Laer
(		).
Three
companions
for
rst
order
data
mining.
In
N.
La
vra

c
and
S.
D

zeroski,
editors,
Inductive
L
o
gic
Pr
o
gr
amming
for
Know
le
dge
Disc
overy
in
Datab
ases,
Lecture
Notes
in
Articial
In
telligence.
Springer-V
erlag.
T
o
app
ear.
De
Raedt,
L.
and
L.
Dehasp
e
(		).
Clausal
disco
v
ery
.
Machine
L
e
arning,
:		{.
De
Raedt,
L.
and
S.
D

zeroski
(		).
First
order
j
k
-clausal
theories
are
P
A
C-learnable.
A
rticial
Intel
ligenc
e,
0:{	.
De
Raedt,
L.
and
W.
V
an
Laer
(		).
Inductiv
e
constrain
t
logic.
In
Klaus
P
.
Jan
tk
e,
T
ak
eshi
Shinohara,
and
Thomas
Zeugmann,
editors,
Pr
o
c
e
e
dings
of
the
th
International
Workshop
on
A
lgorithmic
L
e
arning
The
ory,
v
olume
		
of
L
e
ctur
e
Notes
in
A
rticial
Intel
ligenc
e,
pages
0{
	.
Springer-V
erlag.
Dehasp
e,
L.
and
L.
De
Raedt
(		).
Mining
asso
ciation
rules
in
m
ultiple
relations.
In
Pr
o
c
e
e
dings
of
the
th
International
Workshop
on
Inductive
L
o
gic
Pr
o
gr
amming,
v
olume
	
of
L
e
ctur
e
Notes
in
A
rticial
Intel
ligenc
e,
pages
{.
Springer-V
erlag.
Dietteric
h,
T.
G.,
R.
H.
Lathrop,
and
T.
Lozano-P

erez
(		).
Solving
the
m
ultiple-instance
problem
with
axis-parallel
rectangles.
A
rticial
Intel
ligenc
e,
	(-):{.
Doughert
y
,
J.,
R.
Koha
vi,
and
M.
Sahami
(		).
Sup
ervised
and
unsup
ervised
discretization
of
con
tin
uous
features.
In
A.
Prieditis
and
S.
Russell,
editors,
Pr
o
c.
Twelfth
International
Confer
enc
e
on
Machine
L
e
arning.
Morgan
Kaufmann.
D

zeroski,
S.,
S.
Muggleton,
and
S.
Russell
(		).
P
A
C-learnabilit
y
of
determinate
logic
programs.
In
Pr
o
c
e
e
dings
of
the
th
A
CM
workshop
on
Computational
L
e
arning
The
ory,
pages
{.
Elmasri,
R.
and
S.
B.
Na
v
athe
(		).
F
undamentals
of
Datab
ase
Systems.
The
Benjamin/Cum-
mings
Publishing
Compan
y
,
nd
edition.
F
a
yy
ad,
U.M.
and
K.B.
Irani
(		).
Multi-in
terv
al
discretization
of
con
tin
uous-v
alued
attributes
for
classication
learning.
In
Pr
o
c
e
e
dings
of
the
th
International
Joint
Confer
enc
e
on
A
rticial
Intel
ligenc
e,
pages
0{0,
San
Mateo,
CA.
Morgan
Kaufmann.
F

urnkranz,
J.
(		a).
Dimensionalit
y
reduction
in
ILP:
a
call
to
arms.
In
L.
De
Raedt
and
S.
Muggleton,
editors,
Pr
o
c
e
e
dings
of
the
IJCAI-	
Workshop
on
F
r
ontiers
of
ILP.
http://www.
cs.kuleuven.ac.be/~lucdr
/fi
lp.h
tml.
F

urnkranz,
J.
(		b).
Noise-toleran
t
windo
wing.
In
M.
E.
P
ollac
k,
editor,
Pr
o
c
e
e
dings
of
the
th
International
Joint
Confer
enc
e
on
A
rticial
Intel
ligenc
e,
pages
{.
Morgan
Kaufmann.
Jacobs,
N.,
K.
Driessens,
and
L.
De
Raedt
(		).
Using
ILP
systems
for
v
erication
and
v
alid-
ation
of
m
ulti
agen
t
systems.
In
Pr
o
c
e
e
dings
of
the
th
International
Confer
enc
e
on
Inductive
L
o
gic
Pr
o
gr
amming,
pages
{.
Springer-V
erlag.
Kitano,
H.,
M.
V
eloso,
H.
Matsubara,
M.
T
am
b
e,
S.
Coradesc
hi,
I.
No
da,
P
.
Stone,
E.
Osa
w
a,
and
M.
Asada
(		).
The
rob
o
cup
syn
thetic
agen
t
c
hallenge
	.
In
Pr
o
c
e
e
dings
of
the
th
International
Joint
Confer
enc
e
on
A
rticial
Intel
ligenc
e,
pages
{	.
Morgan
Kaufmann.
Kramer,
S.
(		).
Structural
regression
trees.
In
Pr
o
c
e
e
dings
of
the
th
National
Confer
enc
e
on
A
rticial
Intel
ligenc
e
(AAAI-	).
La
vra

c,
N.
and
S.
D

zeroski,
editors
(		).
Pr
o
c
e
e
dings
of
the
th
International
Workshop
on
Inductive
L
o
gic
Pr
o
gr
amming,
v
olume
	
of
L
e
ctur
e
Notes
in
A
rticial
Intel
ligenc
e.
Springer-
V
erlag.
Llo
yd,
J.W.
(	).
F
oundations
of
lo
gic
pr
o
gr
amming.
Springer-V
erlag,
nd
edition.
Meh
ta,
M.,
R.
Agra
w
al,
and
J.
Rissanen
(		).
SLIQ:
A
fast
scalable
classier
for
data
mining.
In
Pr
o
c
e
e
dings
of
the
Fifth
International
Confer
enc
e
on
Extending
Datab
ase
T
e
chnolo
gy.
Morik,
K.
and
P
.
Bro
c
khausen
(		).
A
m
ultistrategy
approac
h
to
relational
disco
v
ery
in
data-
bases.
In
R.S.
Mic
halski
and
Wnek
J.,
editors,
Pr
o
c
e
e
dings
of
the
r
d
International
Workshop
on
Multistr
ate
gy
L
e
arning,
pages
{.


Muggleton,
S.
(		).
Optimal
la
y
ered
learning:
a
P
A
C
approac
h
to
incremen
tal
sampling.
In
Pr
o
c
e
e
dings
of
the
th
c
onfer
enc
e
on
algorithmic
le
arning
the
ory.
Ohmsma,
T
oky
o,
Japan.
In
vited
pap
er.
Muggleton,
S.
(		).
In
v
erse
en
tailmen
t
and
Progol.
New
Gener
ation
Computing,
.
Muggleton,
S.,
editor
(		).
Pr
o
c
e
e
dings
of
the
th
International
Workshop
on
Inductive
L
o
gic
Pr
o
gr
amming,
v
olume

of
L
e
ctur
e
Notes
in
A
rticial
Intel
ligenc
e.
Springer-V
erlag.
Muggleton,
S.
and
L.
De
Raedt
(		).
Inductiv
e
logic
programming
:
Theory
and
metho
ds.
Journal
of
L
o
gic
Pr
o
gr
amming,
	,0:	{	.
P
age,
D.,
editor
(		).
Pr
o
c
e
e
dings
of
the
th
International
Confer
enc
e
on
Inductive
L
o
gic
Pr
o
gr
amming,
v
olume

of
L
e
ctur
e
Notes
in
A
rticial
Intel
ligenc
e.
Springer-V
erlag.
Plotkin,
G.
(	0).
A
note
on
inductiv
e
generalization.
In
Machine
Intel
ligenc
e,
v
olume
,
pages
{.
Edin
burgh
Univ
ersit
y
Press.
Quinlan,
J.R.
(		a).
C.:
Pr
o
gr
ams
for
Machine
L
e
arning.
Morgan
Kaufmann
series
in
mac
hine
learning.
Morgan
Kaufmann.
Quinlan,
J.R.
(	).
Induction
of
decision
trees.
Machine
L
e
arning,
:{0.
Quinlan,
J.R.
(		0).
Learning
logical
denitions
from
relations.
Machine
L
e
arning,
:	{.
Quinlan,
J.R.
(		b).
F
OIL:
A
midterm
rep
ort.
In
P
.
Brazdil,
editor,
Pr
o
c
e
e
dings
of
the
th
Eur
op
e
an
Confer
enc
e
on
Machine
L
e
arning,
Lecture
Notes
in
Articial
In
telligence.
Springer-
V
erlag.
Shafer,
J.C.,
R.
Agra
w
al,
and
M.
Meh
ta
(		).
SPRINT:
A
scalable
parallel
classier
for
data
mining.
In
Pr
o
c
e
e
dings
of
the
th
International
Confer
enc
e
on
V
ery
L
ar
ge
Datab
ases.
Sriniv
asan,
A.,
S.H.
Muggleton,
M.J.E.
Stern
b
erg,
and
R.D.
King
(		).
Theories
for
m
utagen-
icit
y:
A
study
in
rst-order
and
feature-based
induction.
A
rticial
Intel
ligenc
e,
.
V
an
Laer,
W.,
L.
De
Raedt,
and
S.
D

zeroski
(		).
On
m
ulti-class
problems
and
discretization
in
inductiv
e
logic
programming.
In
Zbigniew
W.
Ras
and
Andrzej
Sk
o
wron,
editors,
Pr
o
c
e
e
dings
of
the
0th
International
Symp
osium
on
Metho
dolo
gies
for
Intel
ligent
Systems
(ISMIS	),
v
olume

of
L
e
ctur
e
Notes
in
A
rticial
Intel
ligenc
e,
pages
{.
Springer-V
erlag.
W
atanab
e,
L.
and
L.
Rendell
(		).
Learning
structural
decision
trees
from
examples.
In
Pr
o
c
e
e
dings
of
the
th
International
Joint
Confer
enc
e
on
A
rticial
Intel
ligenc
e,
pages
0{
.
W
rob
el,
S.,
D.
W
ettsc
herec
k,
E.
Sommer,
and
W.
Emde
(		).
Extensibilit
y
in
data
mining
systems.
In
Pr
o
c
e
e
dings
of
the
Se
c
ond
International
Confer
enc
e
on
Know
le
dge
Disc
overy
and
Data
Mining
(KDD-	).
AAAI
Press.

