
PROGRAMS, RECURSION AND UNBOUNDED CHOICE

Cambridge Tracts in Theoretical
Computer Science
Managing Editor 
Professor CJ. van Rijsbergen,
Department of Computing Science, University of Glasgow
Editorial Board
S. Abramsky, Department of Computing Science, Imperial College of Science
and Technology
P.H. Aczel, Department of Computer Science, University of Manchester
J.W. de Bakker, Centrum voor Wiskunde en Informatica, Amsterdam
LA. Goguen, Programming Research Group, University of Oxford
J.V. Tucker, Department of Mathematics and Computer Science,
University College of Swansea
Titles in the series
1. G. Chaitin Algorithmic Information Theory
2. L.C. Paulson Logic and Computation
3. M. Spivey Understanding Z
4. G. Revesz Lambda Calculus, Combinators and Functional Programming
5. A. Ramsay Formal Methods in Artificial Intelligence
6. S. Vickers Topology via Logic
7. J-Y. Girard, Y. Lafont & P. Taylor Proofs and Types
8. J. Clifford Formal Semantics & Pragmatics for Natural Language Processing
9. M. Winslett Updating Logical Databases
10. K. McEvoy & J.V. Tucker (eds) Theoretical Foundations of VLSI Design
11. T.H. Tse A Unifying Framework for Stuctured Analysis and Design Models
12. G. Brewka Nonmonotonic Reasoning
13. G. Smolka Logic Programming over Polymorphically Order-Sorted Types
15. S. Dasgupta Design Theory and Computer Science
17. J.C.M. Baeten (ed) Applications of Process Algebra
18. J.C.M. Baeten & W. P. Weijland Process Algebra
23. E.-R. Olderog Nets, Terms and Formulas
27. W.H. Hesselink Programs, Recursion and Unbounded Choice
29. P. Gardenfors (ed) Belief Revision
30. M. Anthony & N. Biggs Computational Learning Theory

PROGRAMS, RECURSION AND
UNBOUNDED CHOICE
Predicate-Transformation Semantics and Transformation Rules
WIMH.HESSELINK
Department of Computer Science
University ofGroningen
CAMBRIDGE
UNIVERSITY PRESS

CAMBRIDGE UNIVERSITY PRESS
Cambridge, New York, Melbourne, Madrid, Cape Town, Singapore, Sao Paulo
Cambridge University Press
The Edinburgh Building, Cambridge CB2 2RU, UK
Published in the United States of America by Cambridge University Press, New York
www.cambridge.org
Information on this title: www.cambridge.org/9780521404365
Â© Cambridge University Press 1992
This book is in copyright. Subject to statutory exception
and to the provisions of relevant collective licensing agreements,
no reproduction of any part may take place without
the written permission of Cambridge University Press.
First published 1992
This digitally printed first paperback version 2005
A catalogue record for this publication is available from the British Library
ISBN-13 978-0-521-40436-5 hardback
ISBN-10 0-521-40436-3 hardback
ISBN-13 978-0-521-01829-6 paperback
ISBN-10 0-521-01829-3 paperback

CONTENTS
List of symbols 
xiii
0 Introduction 
1
0.1 Semantics of imperative sequential programs 
1
0.2 Predicate-transformation semantics 
3
0.3 Program transformations 
4
0.4 Overview 
5
0.5 The chapters in detail 
6
0.6 Notation 
8
0.7 Design decisions 
10
1 Weakest preconditions 
12
1.1 Predicates and predicate transformers 
13
1.2 Weakest preconditions 
15
1.3 Guards, assertions, termination and totality 
17
1.4 Composition and nondeterminate choice 
19
1.5 Intermezzo on the conditional combination 
20
1.6 Program variables, state functions and localized relations 
21
1.7 The assignment 
23
1.8 Deterministic choice 
26
1.9 Appendix on predicate calculus 
27
1.10 Exercises 
29
2 Annotation, recursion and repetition 
32
2.1 Specification with Hoare triples 
32
2.2 Proofs by annotation 
35
2.3 Specification and invocation of procedures 
37
2.4 Correctness of recursive declarations 
40
2.5 An abstract version of recursive procedures 
42

ui 
CONTENTS
2.6 Homomorphisms and simple commands 
44
2.7 Induction rules 
45
2.8 The repetition 
48
2.9 Exercises 
52
3 
Healthiness laws 
58
3.1 Conjunctivity properties of predicate transformers 
58
3.2 Two laws 
59
3.3 Some important implications 
60
3.4 Guards, assertions and assignments 
62
3.5 The termination law and repetitions 
63
3.6 Exercises 
64
4 
Semantics of recursion 
63
4.1 Complete lattices and predicate transformers 
66
4.2 Fixpoints in complete lattices 
70
4.3 A syntax for commands with unbounded choice 
71
4.4 The interpretation of recursion 
73
4.5 Healthiness laws: the universal conjunctivity of wlp 
75
4.6 The termination law 
76
4.7 The syntactic algebra 
79
4.8 The semantic homomorphisms 
80
4.9 The induction rules 
82
4.10 Conclusion 
83
4.11 Exercises 
84
5 
Ramifications 
87
5.1 Refinement and relative refinement 
87
5.2 Refinement of procedures 
89
5.3 Insertion of guards, and calculus 
90
5.4 The commutation problem 
91
5.5 Strongest postconditions 
93
5.6 Termination and well-founded triples 
94
5.7 Two new recursion theorems 
97
5.8 Exercises 
99

CONTENTS 
vii
6 
Relational semantics 
101
6.1 Relations as an alternative specification method 
102
6.2 The relational view of guards, composition and choice 
103
6.3 Termination and totality 
105
6.4 From commands to relations 
106
6.5 Exercises 
107
7 Determinacy and disjunctivity 
109
7.1 Determinacy 
100
7.2 Disjunctivity properties 
111
7.3 Determinacy and disjunctivity 
112
7.4 Disjunctivity of wp 
113
7.5 Finite nondeterminacy 
113
7.6 Exercises 
115
8 
Syntactic criteria 
116
8.1 Syntactic reflexion of semantic properties 
116
8.2 Membership of the syntactic reflexion 
120
8.3 ^-disjunctivity 
122
8.4 Totality, disjunctivity and finite nondeterminacy 
125
8.5 Exercises 
129
9 
Operational semantics of recursion 
130
9.1 The interpreter 
131
9.2 The proof of the faithful interpreter 
131
9.3 The operational interpretation of tail recursion 
135
9.4 General operational semantics 
141
9.5 Exercises 
142
10 
Procedure substitutions 
143
10.1 Substitutions 
143
10.2 Substitution commutes with extension 
144
10.3 Procedure abstraction is allowed 
144
10.4 A classical example 
146
10.5 Exercises 
147

viii 
CONTENTS
11 
Induction and semantic equality 
149
11.1 Congruences 
150
11.2 The set Lia 
150
11.3 The strong congruence 
151
11.4 Semantic default rules 
153
11.5 An application: the storage of a parameter 
353
11.6 Compositionality of the strong congruence 
155
11.7 The necessity of Lia 
156
11.8 Exercises 
157
12 
Induction and refinement 
160
12.1 Admissible preorders 
160
12.2 The strong preorder 
161
12.3 Construction of the strong congruence 
161
12.4 Commutation up to refinement 
163
12.5 Exercises 
166
13 
The strong preorder 
167
13.1 Intermezzo: an extension of the theorem of Knaster-Tarski 
167
13.2 Unfolding 
169
13.3 The construction of the strong preorder 
171
13.4 The abortive interpretations 
173
13.5 Inf-safety 
174
13.6 Sup-safety 
176
13.7 Linear approximation 
176
13.8 The set Lia and sequential composition 
178
13.9 Exercises 
179
14 
Temporal operators 
180
14.1 Stability and the function 'always' 
181
14.2 Termination and unfolding 
184
14.3 The temporal predicate transformer for always 
185
14.4 Temporal functions for eventually 
187
14.5 Possible termination 
189
14.6 Exercises 
192

CONTENTS 
ix
15 Predicative fairness 
193
15.1 Starvation of predicates 
194
15.2 An abstract syntax and weak fairness 
196
15.3 A general fairness definition 
197
15.4 Examples 
200
15.5 The operational meaning 
203
15.6 A proposal for strong fairness 
206
16 Solutions of exercises 
208
References 
217
Index of concepts and identifiers 
221


PREFACE
This book is about programs as mathematical objects. We focus on one of
the aspects of programs, namely their functionality, their meaning or semantics.
Following Dijkstra we express the semantics of a program by the weakest precon-
dition of the program as a function of the postcondition. Of course, programs have
other aspects, like syntactic structure, executability and (if they are executable)
efficiency. In fact, perhaps surprisingly, for programming methodology it is useful
to allow a large class of programs, many of which are not executable but serve as
partially implemented specifications.
Weakest preconditions are used to define the meanings of programs in a clean
and uniform way, without the need to introduce operational arguments. This for-
malism allows an effortless incorporation of unbounded nondeterminacy. Now pro-
gramming methodology poses two questions. The first question is, given a speci-
fication, to design a general program that is proved to meet the specification but
need not be executable or efficient, and the second question is to transform such a
program into a more suitable one that also meets the specification.
We do not address the methodological question how to design, but we concen-
trate on the mathematical questions concerning semantic properties of programs,
semantic equality of programs and the refinement relation between programs. We
provide a single formal theory that supports a number of different extensions of the
basic theory of computation.
The correctness of a program with respect to a specification is for us only
one of its semantic properties. We are equally interested in its incorrectness or its
equivalence to other programs. For example, we provide formal rules to prove that
a recursive procedure does not meet its specification.
The book can be used for courses on predicate-transformation semantics of
various sizes. For an introductory course, I prefer to use Chapters 1, 2, 3 and
4, but Chapter 4 can be replaced by subjects from Chapter 5 or by Chapter 6,
possibly followed by 7. The more powerful methods of program transformation are
contained in Chapters 10, 11 and 12; these chapters should be accessible after 4, 6

Xll
PREFACE
and 7 have been covered. Chapter 14 on temporal predicate transformers can be
treated directly after Chapter 4.
Some of the material grew out of courses of lectures on programming or pro-
gramming theory delivered at the University of Groningen. Other parts of the book
grew out of my need for a non-operational understanding of standard implementa-
tion techniques like the use of stacks for the implementation of recursive procedures.
Chapters 14 and 15 were directly inspired by [Morris 1990].
I offer sincere thanks to A. de Bruin, J.J. Lukkien, J. Morris, R. Reinds,
J.C.S.P. van der Woude and J. von Wright for critically reading drafts of the book. I
also want to express here my deepest gratitude to the three persons who have stim-
ulated and guided my transition from mathematics to computer science: J.W. de
Bakker, E.W. Dijkstra and J.L.A. van de Snepscheut. Van de Snepscheut was for
five years the inspiring leader of our department. Under his direction I got the
opportunity to spend a sabbatical year at the University of Texas at Austin, in
1986/1987. There I worked with Dijkstra, who converted me to a new mathe-
matical discipline, ten years after I had obtained my doctorate in mathematics.
Finally, De Bakker showed continuous interest in my computer science papers and
encouraged me to write this book. Of course, there were many more persons who
stimulated my development but no enumeration can do justice to all of them.
Groningen, the Netherlands 
Wim H. Hesselink
June 1991

LIST OF SYMBOLS
Symbols are listed by section number.
Postfix operators
-* 4.3
-Â® 4.3, 10.1
âe (where e = 0 or 1) 4.4
9 0.0
Special parentheses
[ ] 0.0,1.1
[ / ] 
i d
{ } 0.6,2.1
[ ] 6.4
Programming operators
! 1.3
? 1.3
; 1.4,4.7,6.2
| 1.4, 4.7, 6.2
h 15.2
Relational operators
S 0.3, 1.3
=v 1-6
h 2.7
< 4.1
C 5.1
< 5.6
Â« 11.3, 12.3
< 12.2, 13.3


CHAPTER 0
INTRODUCTION
0.0. The purpose of this book is to develop the semantics of imperative sequential
programs. One prerequisite for reading is some familiarity with the use of predicates
in programming, as exposed for instance in the books [Backhouse 1986], [Dijkstra
1976], or [Gries 1981]. Some mathematical maturity is another prerequisite: we
freely use sets, functions, relations, orders, etc. We strive for providing complete
proofs. This requires many backward references but, of course, the reader may
sometimes prefer to ignore them. Actually, at every assertion the reader is invited
to join the game and provide a proof himself.
In every chapter, the formulae are numbered consecutively. For reference to
formulae of other chapters we use the convention that i(j) denotes formula (j) of
Chapter i.
At the end of almost every chapter we give a number of exercises, grouped
according to the latest relevant section. When referring to exercise i.j.fc, we mean
exercise k of Section i.j. Some exercises are simple tests of the reader's apprehen-
sion, while other exercises contain applications and extensions of the main text.
For (parts of) exercises marked with <? we provide solutions in Chapter 16.
References to the literature are given in the form [X n], for author X and year
n, possibly followed by a letter.
0.1 Semantics of imperative sequential programs
The word 'semantics' means 'meaning'. In the title of this book, it announces two
central themes. The meaning of a program is given by its specification. This leads
to the correctness issue: does the program meet its specification? The other theme
is program transformation. In fact, when we have separated the meaning from the
program, we can ask whether there exist more than one program with that meaning,

2 
INTRODUCTION
so that it can be useful to transform the program into a more suitable one of the
same class.
We restrict ourselves to imperative sequential programs. Such a program is a
command to change the internal state of the computer - and part of the internal
state may be visible for output. The easiest way to formalize the meaning of a
command is by means of the relation between initial and final states. This method
of formalization is called relational semantics. It is theoretically attractive but not
well-suited for correctness proofs or derivations of interesting programs. In fact, in
the relation between initial and final state, every program variable can have two
roles: it may refer to the initial value or to the final value of the variable. There-
fore, in relational semantics, the sequential composition of commands may require
extensive renaming. Nevertheless, relational semantics is used rather convincingly
in [Hehner 1992].
In actual programming we prefer to use the Floyd-Hoare method, which is
based on the following idea. One associates state predicates to points in the program
text and uses inductive reasoning to prove that the predicate of a point holds
whenever execution of the program reaches that point. This method is formalized
in various ways that are known under a number of different names: axiomatic
semantics, Hoare logic, dynamic logic, weakest precondition semantics, etc. We use
the term predicate-transformation semantics.
Thus, our choice of semantics is motivated by its applicability to programming
methodology. Indeed, the important properties of programs are easily expressed
in terms of predicate transformers. Predicate-transformation semantics has good
modularity properties. It allows an elegant definition of the meaning of recursive
procedures that is compatible with the operational semantics but not itself opera-
tional.
Finally, predicate-transformation semantics can effortless be combined with
nondeterminacy (even unbounded nondeterminacy). Indeed, one of the issues of
programming methodology is to avoid over-specification and premature design de-
cisions. Thus, if we require that program variable x gets a positive value, it is
important to specify just that, and to postpone a specific choice to a later design
phase or perhaps to the compiler. Therefore we want our programming language
to contain a construct for unbounded nondeterminate choice, compare [Back 1988].
The realization of this requirement is a distinctive feature of the present book.
It is absent from the classical treatises [de Bakker 1980] and [Dijkstra 1976]. The
treatment of unbounded nondeterminacy for the repetition is due to [Dijkstra-

0.2 PREDICATE-TRANSFORMATION 
SEMANTICS 
3
Scholten 1990] and, independently, [Apt-Plotkin 1986]. Inspired by Dijkstra's notes,
I treated unbounded nondeterminacy for recursive procedures in [Hesselink 1988],
essentially by means of relational semantics. The present combination of predicate
transformation semantics, recursion and unbounded choice seems to be new.
0.2 Predicate-transformation semantics
We concentrate on control rather than data. So, in the formal theory, the state
space is a set without structure and the programs are program schemas built in
terms of unspecified simple commands. The semantics are expressed by means of
weakest precondition functions wp and wlp, cf. [Dijkstra 1976]. Roughly speaking,
wp.c.p is the weakest precondition such that command c terminates and establishes
postcondition jp, whereas wlp.c.p is similar but does not guarantee termination of
c.
The basic constructors of commands are the operators for sequential compo-
sition and unbounded nondeterminate choice. This yields a powerful foundation.
If the set of simple commands contains guards to test for interesting conditions, it
is possible to model conditional statements, bounded repetitions and nonrecursive
procedures with parameters.
The next step is to introduce recursive procedures. In order to extend the
functions wp and wlp to these commands, we use the condition that every procedure
be semantically equivalent to its body. This yields recurrence equations for wp and
wlp. The extended function wp is defined as the strongest solution of its equation.
The extended function wlp is defined as the weakest solution of its equation. This
definition agrees with the operational semantics, even in the presence of unbounded
nondeterminacy.
A central concept is semantic equality of commands. Since we admit nondeter-
minacy and nontermination, it is important to decide how to deal with commands
that have both finite and infinite execution sequences. In such cases, one may
choose to neglect the finite execution sequences. From the point of view of program
correctness, this may be a harmless simplification, cf. [Hehner 1984]. Theoretically,
it is more attractive to neglect the infinite execution sequences, cf. [de Bakker-de
Roever 1973], [Harel 1984]. We follow [Nelson 1989], [Dijkstra-Scholten 1990] and
others in using both functions wp and wlp. In this way, we incorporate both the
possibility of nontermination and the results of all finite execution sequences into

4 
INTRODUCTION
the semantic formalism. So, for us, the nondeterminate choice skip Q abort differs
semantically both from skip (do nothing) and from abort (loop forever).
For the ease of computation with commands as mathematical objects, the
abstract syntax is kept as small as possible. It only contains simple commands,
sequential composition, unbounded nondeterminate choice and (mutually) recur-
sive procedures. In examples and applications we use a concrete syntax which also
contains conditional statements, while-loops and procedure parameters. The trans-
lation of these constructs into the abstract syntax turns out to be straightforward.
0.3 Program transformations
We describe several kinds of program transformations:
â¢ modification of expressions in assignments, in 1.7
â¢ unfolding: replacing a procedure name by its body, in 2.5
â¢ distributivity, in 3.3
â¢ insertion of guards, in 5.3
â¢ commutation of commands, in 5.4 and 12.4
â¢ stack implementation of recursive procedures in 9.1 and 9.2
â¢ procedure abstraction, in 10.3 and 10.4
â¢ computational induction, in 11.3 and 12.2
â¢ storage of procedure parameters, in 11.5
â¢ change of procedure declaration, in 11.6
In most of these cases, what we actually present is a rule to prove semantic equality.
An important property is that semantic equality is compositional with respect to
sequential composition and (unbounded) choice. So, if we use '=' to denote semantic
equality, we have
cO^c 
A dO Â£ d =Â» cO; dO ^ c; d A cO Q dO ^ c | d.
Semantic equality, however, is not compositional with respect to declarations of
recursive procedures. If the body of procedure h is semantically equal to a command
expression q that may contain /i, the semantics of h can change if we replace the
declaration of h by body./i = q. In fact, every procedure h satisfies body./i = ft,
but, if the declaration is replaced by body, ft = ft, then ft becomes equivalent to
abort. This situation suggests that we need to look for an equivalence relation
on command expressions stronger than semantic equality and compositional with
respect to recursive declarations. In Chapter 11, we introduce such a relation. It is

0.4 OVERVIEW 
5
called the strong congruence. It is responsible for the last three items of the above
list.
In [Hesselink 1990], we constructed a related congruence, but the formalism
was too restrictive for the application in [Hesselink 1989a]. In fact, our previous
formalism did not provide unbounded choice as a constructor. Here, we allow
unbounded choice and obtain the results needed.
0.4 Overview
The book can be divided into four parts. Part A consists of Chapters 1, 2, 3, 4 and
5; it contains the introduction, the formal foundation and some additional material.
Part B consists of Chapters 6, 7, 8 and 9; it contains the relational semantics and a
treatment of disjunctivity properties of commands. Part C consists of Chapters 10,
11, 12 and 13; it deals with program transformation. Part D consists of Chapters 14
and 15; in this part the semantic framework is extended to include some temporal
properties of commands. The main dependencies of the chapters are indicated in
the directed graph shown below.
5
T
3 - >
7 - >
9
/
4 - >
1
8 - >
14 - >
10
i
11 -*â¢
15
12
T
5
\
13
\
For clarity of the graph, the nodes 4 and 5 have been duplicated. The arrow between
3 and 6 can be reversed at will. The arrow between 8 and 11 is rather weak.
We regard part A as the most important one, for it contains the development
of a very simple model that is yet effective and powerful. We hope that parts B,
C and D can be appreciated independently from each other. Part B provides an
operational background, which can be seen as a foundation for part A, and which
is also useful for some details in parts C and D. Part C contains the results on
program transformation and computational induction. In order to obtain sharp
results, we were forced to develop a complicated theory. Part D consists of less

6 
INTRODUCTION
heavy material. It sheds new light on concepts like termination, stability, progress
and fairness, but we do not claim practical applicability.
It is our intention that each chapter adds a layer of understanding that is useful
without support from later chapters. To this end we sometimes use the postulational
method. For example, in Chapter 2, recursion is introduced by means of proof rules
that are used in programming. The semantics of recursion are formally defined in
Chapter 4, and the proofs of some proof rules are postponed until Section 4.9.
The heaviest material of the book is concentrated in Chapter 9, which leads to an
independent goal, and in Chapters 8 and 13, which are needed to prove the results
postulated in Chapters 11 and 12.
0.5 The chapters in detail
We proceed with a brief description of the chapters. First Part A. Chapter 1 is an
introduction to the predicate-transformation semantics of straight-line commands.
In particular, we treat guards, assertions, assignments, sequential composition and
unbounded choice. It is a survey of known material with some new points of view.
The only result with a substantial proof is the substitution rule for the weakest
precondition of the assignment.
In Chapter 2 we introduce Hoare triples as a specification method and a tool
to prove program correctness. The next step is the introduction of procedures with
correctness rules for declaration and invocation. The postulate that a procedure is
semantically equal to its body is used to prove the soundness of the proof rule for
total correctness of recursive procedures. Hoare's Induction Rule for conditional
correctness is presented as well, followed by a related induction rule to prove ne-
cessity of preconditions. Finally the results on recursive procedures are specialized
to the repetition.
In Chapter 3, we introduce Dijkstra's healthiness conditions, and we investigate
their formal consequences. One of the consequences discussed is the distributivity
of sequential composition over unbounded choice.
Chapter 4 is the heart of the book. It contains the formal definition of the
semantics of recursive procedures and the proofs of the properties of commands that
were postulated and used in the previous chapters. It begins with a brief excursion
into lattice theory that culminates in a version of the theorem of Knaster-Tarski.
We then come back to programming, fix the abstract syntax, define the semantics of
recursive procedures and prove the fundamental properties of recursive procedures.

0.5 THE CHAPTERS IN DETAIL 
7
Chapter 5 is a kind of appendix to Part A. It has only weak ties with the
rest of the book. It contains a number of different subjects: refinement of com-
mands, refinement of procedures, some examples of semantic equality, strongest
postconditions and proof rules with well-founded sets.
Part B begins with Chapter 6. Here we present the relational semantics. We
give the relational interpretations of totality and nontermination, composition and
nondeterminate choice. We show that, under assumption of the healthiness laws,
the expressive power of relational semantics equals the expressive power of predicate
transformation semantics.
In Chapter 7, we treat determinacy of commands and disjunctivity properties of
the associated predicate transformers. The concept of determinacy of commands is
split into liberal determinacy and termination determinacy. We prove that liberal
determinacy of a command is equivalent to positive disjunctivity of the weakest
liberal precondition and that finite nondeterminacy is equivalent to upper continuity
of the weakest precondition.
In Chapter 8, we obtain syntactic results concerning total, disjunctive, deter-
minate, and finitely nondeterminate commands. A command is said to be of finite
nondeterminacy if every initial state in which the command is bound to terminate,
allows only finitely many resulting states. This concept played a crucial role in
some early treatments of the semantics of nondeterminacy, cf. [Dijkstra 1976] and
[de Bakker 1980] p. 263. We do not need finite nondeterminacy for the basic prop-
erties of recursive procedures. The concept is necessary, however, in our treatment
of the strong congruence.
In Chapter 9, we treat a stack implementation of recursive procedures. Actu-
ally, the problem of proving the validity of such a stack implementation in terms
of the axiomatic semantics was the challenge that triggered our investigations. We
then define the relational or operational semantics of recursive procedures, and
prove the equivalence with the semantics of Chapter 4.
Chapter 10 is the opening chapter of Part C. It introduces substitution of pro-
cedure names and treats a kind of program transformation that can be characterized
as the introduction of intermediate procedures.
In Chapter 11, we postulate the existence of the strong congruence and some
of its properties. It is compositional with respect to sequential composition and
unbounded choice, and it implies semantic equality. Moreover, it satisfies a so-
called accumulation rule, which is our version of the induction rule of De Bakker
and Scott (cf. [de Bakker 1980] Section 9.3 and [Hesselink 1990] Section 5). This

8 
INTRODUCTION
rule is illustrated by some examples. It is used to prove that the strong congruence
is compositional with respect to recursive declarations.
In many applications, refinement is more important than semantic equality.
Therefore, in Chapter 12, we introduce a 'strong preorder', which plays the same
role with respect to refinement as the strong congruence plays with respect to
semantic equality. This preorder is also used to prove the postulates concerning the
strong congruence. In Chapter 13, we construct the strong preorder and prove its
properties.
In Part D that consists of Chapters 14 and 15, the semantics are extended
to include predicates concerning states that are visited during execution of the
commands. In Chapter 14, we introduce temporal predicate transformers related
to those of [Morris 1990] and [Lukkien 1991]. They can be characterized by the
key-words 'always' and 'eventually', to be interpreted here as 'at every induced pro-
cedure call' and as 'at some induced procedure call', respectively. Following Morris,
we use these predicate transformers in Chapter 15 for a treatment of predicative
fairness. The examples of this chapter are illuminating and show that our formal-
ism interprets fairness in unexpected ways, especially in cases of more complicated
recursion.
0.6 Notation
We use the operator '.' for function application. This operator has the highest
binding power. It binds from left to right to allow currying. Thus, typically, the
expression D.w.h.p stands for ((D.w).h).p. 
The set of functions from a set X to
a set Y is denoted by Yx 
or X â> Y (the second notation is preferred if X is
a complicated expression). Accordingly, / Â£ X ââ¢ F means that / is a function
from X to Y. If W is a subset of -X", we write (/|W) to denote the restriction
of / to W, which is a function in W â* Y. Multiplication of integers is denoted
by 'â¢' which takes more white space than function application, compare 2 â¢ i with
f.x. 
Functional composition is denoted by means of the infix operator o, so that
typically (/ o g).x = 
f.(g.x).
We use Dijkstra's quantification format. For example, the quantification
(Vi el-.P.i 
:Q.i)
expresses that Q.i holds for all values i Â£ I such that P.z. We enclose the whole
expression in parentheses, since that makes it easier to parse formulae where such
an expression is an operand of a binary operator. Domain expression P.i is omitted

0.6 NOTATION 
9
if it is identically true. The same format is used for other quantifiers like 3, sup,
U, I, etc.
A quantification can be regarded as the application of a quantifier to a family,
where the concept of family is defined as follows. If / and X are sets, P is a boolean
function on / and f.i Â£ X for every i Â£ I with P.i, then the family (i E I: P.i : f.i)
is the entity consisting of the terms f.i with i Â£ / and P.i. Formally speaking,
a family of elements of X is a function with values in X, but conceptually the
emphasis is on the elements of X and not on the function. The family is called a
sequence if the set / consists of natural numbers.
We use the following boolean operators (logical connectives) in order of decreas-
ing priority. The negation '-Â»' has the highest priority, followed by conjunction 'A'
and disjunction 'V' of equal priority, followed by the implication symbols ' =4>' and
' <= ' of equal priority, followed ' = ' for logical equivalence, i.e. equality of boolean
values. Equality of functions (even of boolean valued functions) is denoted by '='.
We use the symbol 'c' to denote (nonstrict) subset inclusion
A C B 
= (Wx Â£ A :: x Â£ B) .
Strict subset inclusion A C B A A ^ B does not occur very often, and will therefore
be stressed accordingly.
Whenever convenient, we use Feijen's proof format for calculational proofs,
with braces '{' and '}' to enclose comment, cf. [Dijkstra-Scholten 1990]. For exam-
ple, in order to prove that X follows from Z we may write
X
= 
{indication why X and Y are equivalent}
Y
4= 
{indication why Y follows from Z}
Z .
If Z is trivially true, this format will be used as a proof of X. A similar format is
used for other transitive relational operators.
Braces are also used in Hoare triples (cf. Section 2.1), and for enumerated sets.
For example, {p, q} is the set with two elements p and q. We write 0 to denote the
empty set. We write 2Z for the set of the integers and IN for the set of the integers
>0.
If P is a boolean function on a set X, the set of the elements x Â£ X with P.x
is conventionally defined by Y = {x Â£ X\P.x}. Instead of this braces notation, we
prefer to define Y as the subset of X given by x Â£ Y = P.x for all x Â£ X. For
calculational purposes, this way of introducing Y is more convenient. In fact, the

10 
INTRODUCTION
equivalence is directly used in all references to the definition of Y. The wish to
avoid overloading of the braces is a second reason.
0.7 Design decisions
In the composition of the theory we have made various choices and rejected several
alternatives. Of course, a reader satisfied by the results need not be interested in
the alternatives, but nevertheless a justification of the main choices should be given.
We use predicate-transformation semantics instead of relational semantics or
denotational semantics, since it is more useful for programming and reduces the
temptation to use operational arguments. Moreover, we do not know a construction
of the strong congruence in terms of relational semantics.
Theoretically minded computer scientists often prefer to define the meanings
of programs by means of denotational semantics. Denotational semantics are some-
times characterized by key properties like compositionality, usage of environments
to bind the meanings of procedure names and usage of extreme fixpoints to define
the semantics of recursion. Our semantics are denotational in that sense. On the
other hand, denotational semantics are also sometimes thought to capture the es-
sential results of completed computations in terms of the initial situation. In this
sense, weakest precondition semantics are never denotational.
In [Apt-Plotkin 1986], denotational semantics is assigned to a class of while-
programs over a countable state space. The authors also define operational seman-
tics and wp-semantics, and prove the equivalence of the semantical paradigms. It
seems that this work (without the results on completeness and complexity) can be
extended to our language and an arbitrary state space. We have refrained from
doing so, since it is not our aim to compare semantical paradigms.
Our treatment of predicate-transformation semantics does not require a sep-
arate logic. We present a mathematical theory with definitions, axioms, theorems
and proofs, rather than a version of dynamic logic. To quote from [McCarthy 1980]
p. 37: In our opinion, it is better to avoid modifying the logic if at all possible, be-
cause there are many temptations to modify the logic, and it would be very difficult
to keep them compatible. For example, we use Dijkstra's pair of square brackets not
as a primitive predicate symbol but as a convenient abbreviation: we have a fixed
state space X, and for a boolean function p on X we write [p] = (\/x G l : : p.x) ,
cf. Section 1.1.

0.7 DESIGN DECISIONS 
11
A second fundamental decision is the choice to use the implication order of
predicates and predicate transformers, and not the approximation order of [Nelson
1989] p. 518, which is a kind of translation of the so-called Egli-Milner order to
pairs of predicate transformers. Our choice has the advantage that the functions wp
and wlp for recursive procedures can be defined independent of each other. Since
we do not use Nelson's order, we cannot admit Nelson's constructors '^' and 'ifâfl'
(see loc.cit. p. 534 and p. 557) in bodies of recursive procedures.
A third design decision is to impose two of Dijkstra's healthiness laws (the
universal conjunctivity of wlp and the termination law, see Chapter 3 below) and
to abolish Dijkstra's law of the excluded miracle. These decisions work together.
The accepted laws imply that sequential composition of commands distributes over
unbounded choice:
? ; U r e C : : r ) 
Â£ 
( I r E C :: q;r).
Freedom for miracles implies that the conditional combination can be expressed in
terms of composition and choice, see Section 1.5. These two observations enable us
to model all command expressions (in Section 4.3) as nonempty sets of sequences of
elementary commands. This greatly simplifies the formal syntax. A second reason
for postulating the universal conjunctivity and the termination law is that some of
the crucial technical arguments (e.g. in Section 10.2) need positive conjunctivity of
the predicate transformers involved.
For the sake of simplicity we do not use the terminology of command algebras
of [Hesselink 1990]. Unbounded choice would have required a rather heavy con-
struction of completions of command algebras, whereas in the present framework
it suffices to work with sets of strings of commands.
It has been proposed that we might include an account of the history of the
subject. This, however, is beyond the scope of the present monograph. To give
a minimum of historical references, the function 'wp' was introduced in [Dijkstra
1975]. Its relational interpretation was discussed in [de Roever 1976]. The distinc-
tion between Vp' and 'wlp' was introduced in [Dijkstra 1976], but the idea of 'wlp'
goes back to [de Bakker-Meertens 1975]. For other aspects of axiomatic semantics,
we refer to [Apt 1981]. Many concepts are quite old, but the importance is often
recognized later, by researchers who are not aware of the earlier occurrences. For
example, Hoare's inductive assertion method is usually attributed to Floyd, but,
according to [de Bakker-Meertens 1975] p. 328, it was in essence proposed in [Turing
1949].

CHAPTER 1
WEAKEST PRECONDITIONS
1.0. In this chapter, we introduce straight-line commands and the semantic frame-
work. It is a survey of known material with some new points of view. The stress
is on the semantics, to prepare the ground for general recursion with unbounded
nondeterminacy. The issues of formal syntax related to unbounded choice are post-
poned to Chapter 4.
If j is a program variable, command j := j + 1 specifies a change of the state
of the computer. If j = 2 in the initial state, the resulting state satisfies j =3. If
we want the resulting state to satisfy j < 5, it is necessary and sufficient that the
initial state satisfies j < 4. This is expressed in
wp-(j := J + l)-(j < 5) = (j < 4) ,
in words, the weakest precondition for command j := j +1 and postcondition j < 5
is j < 4.
We regard expressions like j < 5 as boolean functions on the state space. If
# is a state where j has value 9 then (j < 5).x = false. Boolean functions on the
state space are called predicates. Thus, wp.(j := j + 1) is a function from pred-
icates to predicates, a so-called predicate transformer, and wp is a function from
commands to predicate transformers. Function wp and its twin wlp to be intro-
duced below form the central concept of this book. In fact, for a command c, the
pair (wp.c, wlp.c) is regarded as the meaning of c, i.e., its predicate-transformation
semantics.
In Section 1.1, we formalize our view of predicates and predicate transformers.
In Section 1.2, we introduce the functions wp and wlp and discuss their inter-
pretation and some of their properties. In the remainder of the chapter, we define
wp.c for straight-line commands c, i.e., commands that are constructed from simple
commands by means of composition and choice, but without recursion or repetition.

1.1 PREDICATES AND PREDICATE TRANSFORMERS 
13
1.1 Predicates and predicate transformers
Let X denote the state space of a computer. The elements of X are called states.
We let IB denote the set of the two boolean values false and true, so that we can
define IP = IB to be the set of the boolean functions on the state space. The
elements of IP are called predicates.
Warning: many authors use the term predicate syntactically, for boolean ex-
pression. For us, however, the predicates form a semantic concept: they are boolean
functions.
We define faise, true E IP as the constant functions with the values false and
true, respectively. For predicates p , ^ G P , we define predicates -ip, p A q, p V g,
p = g, p=$> q by pointwise application, so that for all x Â£ X:
(0) 
-yp.X = 
-"(p.ff) ,
(phq).x 
= 
p.xAq.x,
(pVq).x 
= 
p.xVq.x,
(p = q).x = (p.x = q.x),
(p=^q).x 
= p.x=^q.x .
Similarly, the conjunction and disjunction of a family of predicates (z E / :: p.i) are
given by
(1) 
(Vi E / :: p.i).x 
= (Vi E / :: p.i.x) ,
(3 i E / :: p.i).x 
= (3 i E / :: p.i.x) .
The universal quantification [p] of a predicate p over the state space is defined by
(2) 
[p] = 
(\/xeX::p.x).
By definition, the equality p = q is an abbreviation of [p=q]. It follows that [p]
is equivalent to p = true.
Example. The truth of [(3 i E / :: p.i) ] means that in every state there is some index
i such that p.i holds in that state. This is weaker than the truth of (3 i E / :: [p.i])*
there is some i such that p.i holds everywhere.
More concretely, let / E 7LX be an integer valued function on the state space.
For integer i, let f = i denote the predicate given by (/ = i).x = (f.x = i) . For
every state #, there is a value i with f.x = i; this proves that [ (3i :: f = i)]. If
function / is not constant, there is no value i such that f.x = i holds for all x; this
shows that -Â»(3 i '.' [f = i])- (End of example)
Calculations with predicates can be reduced to calculations with boolean val-
ues, and these can be performed by means of truth tables, i.e., by extensive case

14 
WEAKEST 
PRECONDITIONS
distinction. Experience shows that this method is very inefficient and error prone.
The method of natural deduction may be more secure, but is not efficient either.
Therefore, we use the equational style of reasoning developed by Dijkstra and others,
which is called predicate calculus. This calculus can be presented in an axiomatic
way, see [Dijkstra-Scholten 1990]. For our purposes, however, it is sufficient to know
a number of basic equalities. A sample of useful rules is contained in the appendix
at the end of this chapter and in the exercises.
Since elements of IP are called predicates, a function / G IP â> IP is called a
predicate transformer. Functions preserve equality. So, for all predicates p and q,
the equality p = q implies f.p = f.q, or equivalently
[p=q] 
=* 
[f.p=f.q].
An important application of definition (2) is the concept of strength of predicates:
p is called stronger than predicate q if and only if [p=^ q]. This defines an order on
IP. A predicate transformer / G IP â> IP is called monotone if and only if, for all p,
Â«â¬P,
(3) 
[p=>q] => 
[f.p^f.q].
Remark. 
Following [Barr-Wells 1990], we use the terms order, ordered set and
monotone where other authors often use partial order, poset and monotonic. (End
of remark)
Example. Let there be precisely one program variable v, which is of type integer.
A state x is characterized by the value of v. We may therefore regard state x as an
integer. Let predicate transformer / be given by
(f.p).x 
= 
p.(x + l)Ax>0 
for allpG IP, x EX 
.
Then / is monotone, as is proved in
= 
{(2), and definition of / }
(Vz :: p.(x + 1) A x > 0 => q.(x + 1) A x > 0)
4= 
{(2), and some calculus}
Since / is defined in terms of states, we need states in the proof. In general, however,
it is our aim to avoid states whenever possible. (End of example)

1.2 WEAKEST PRECONDITIONS 
15
1.2 Weakest preconditions
A command is a syntactic unit intended to be executed by a computer. It specifies
a relation between precondition and postcondition. It need not be deterministic or
implementable. The semantics of a command c are given by predicate transformers
wp.c and wlp.c. If p is a predicate, the predicate wp.c.p is the weakest 'precondition
such that every execution of command c terminates in a state where predicate p
holds. Predicate wlp.c.p is the weakest precondition such that every execution of c
does not terminate or terminates in a state where p holds. Since the operator V
binds from left to right, we do not need the parentheses in (wp.c).p. The argument
p is called the postcondition.
This description of the predicates wp.c.p and wlp.c.p is to be regarded as an
informal interpretation upon which no formal conclusions can be based. In fact,
we treat the functions wp and wlp as primitive concepts with properties given by
axioms and definitions. The above description is only used as a justification of such
axioms and definitions. Readers who want to take the above description literally,
can try and read Chapter 6 first where the relational semantics is presented.
Example. Running ahead of the formal development, we illustrate these concepts
by giving the weakest preconditions for some commands and predicates. Since we
lack the foundation required, we cannot give meanings and proofs. The reader may
skip the example or use previous experience for the interpretation. Let j be an
integer program variable. As announced above, j := j + 1 is a command with
wp.(j := j + l).(j < 5) = (j < 4) .
The same equality holds if wp is replaced by wlp. More interesting is a loop like
L = while j ^ 0 do j := j â 1 od .
This loop terminates if and only if j is initially at least 0. If it terminates, the
postcondition j = 0 is established. Therefore, we have
wp.L.(3 =0) 
= (j > 0) ,
wlp.L.(j = 0) = 
true ,
wp.L.(j ^ 0) = false ,
wlp.L.(J Â± 0) = 
(j < 0) .
Command (j := âj \ skip) is the nondeterminate choice between j := âj and skip
(do nothing). Since we do not prescribe the choice, we have
wp.(j '= -J I sldp)-(j < 5) = (-j < 5) A (j < 5) ,
and similarly for wlp. (End of example)

16 
WEAKEST PRECONDITIONS
The above description of wp and wlp easily justifies the following three axioms:
(4) 
[wp.c.p=>wlp.c.p] ,
(5) 
[p=^#] 
=^ [wp.c.p=>wp.c.q] 
{monotony of wp.c} ,
(6) 
[ p =$> q ] =$> [ wlp.c.p => wlp.c.q ] 
{monotony of wlp.c} .
Rule (4) can be interpreted to say: if command c is guaranteed to terminate in a
state where p holds then c can only terminate in a state where p holds. Rules (5)
and (6) can be rephrased: if p implies q everywhere then termination in a state
where p holds implies termination in a state where q holds.
The informal description can be used to justify some more stringent postulates.
In program development, however, it is often useful to admit specifications as com-
mands, even if they are not implement able. We therefore postpone the introduction
of postulates that are not yet needed.
Example. The present formalism admits a command serve that establishes the
postcondition if and only if there exists a state where the postcondition holds;
otherwise serve does not terminate (see [Back-von Wright 1990]). The command
is defined by stating that, for all states x and predicates p,
wp.serve.p.x 
= 
(By G X :: p.y) ,
wlp.serve.p.x 
= true .
Command serve is not implement able. It seems that the executing mechanism
needs to 'know' the postcondition that the user has in mind. The axioms that
forbid such commands are introduced in Chapter 3. (End of example)
Since the semantics of our commands are defined by means of wp and wlp, we
define semantic equality of commands c and d by
(7) 
c = d = 
(wp.c = wp.d) A (wlp.c = wlp.d) .
For commands c and c/, the notation c = d is reserved for definitions and syntactic
equality.
Remark. Definition (7) may need some justification. Why not choose only one of
the conjuncts at the righthand side? Theoretically, it is attractive to choose the
wip-conjunct, in which case we could speak of wip-equivalence. As early as 1972, a
good theory of wip-equivalence was presented in [de Bakker-de Roever 1973]. Since
equivalence with respect to termination is not included, however, wip-equivalence
is not sufficient for all purposes.
For practical purposes, wp-equivalence is often enough. There are useful pro-
grams, however, in which nondeterminate occurrence of error conditions cannot be
precluded. The occurrence of error conditions being modelled as nontermination,

1.3 GUARDS, ASSERTIONS, TERMINATION AND TOTALITY 
17
such a program is wp-equivalent to an arbitrary nonterminating program. There-
fore, its useful behaviour is completely characterized by wlp. For this reason, we
have chosen to include both wp and wlp in definition (7). (End of remark)
1.3 Guards, assertions, termination and totality
The first commands we are going to introduce formally, are guards and assertions.
Operationally speaking, these commands do not change the state but only test
whether a given predicate holds. They have nice calculational properties and are
important as building blocks of conditional commands.
For any predicate 6, the guard ?b and the assertion ! b are commands charac-
terized by
(8) 
wp.(?b).p = (6=Â»p) , wlp.(?b).p = (b^p) ,
wp.(\b).p = 
(bAp) 
, wlp.(\b).p = (b=>p) .
In principle, we have to verify that definition (8) is in accordance with laws (4), (5)
and (6). This verification is very simple. For example, command ! b satisfies rule
(4) because of
[bAp 
=> 
(b=>p)].
Remark. The reader is advised not to think operationally about ! b and ?6. Nev-
ertheless, the following description can be offered. The assertion ! b tests whether
b holds. If so, it skips. Otherwise it loops indefinitely. The guard ?6 also tests
whether 6 holds. If so, it skips. Otherwise it does not execute,â which means
that every execution (there is none) establishes every postcondition. Guards are
nontotal commands: not every initial state has a corresponding final state, the set
of final states may be empty. We come back to this at the end of this section, and
also in Chapter 6. (End of remark)
Three important special cases are skip, abort and miracle, given by
(9) 
skip = Itrue 
(Â£ ! true) ,
abort = 
! false ,
miracle = ? false .
It is easy to verify that for any predicate p:
(10) 
wp.skip.p â p , wlp.skip.p = p ,
wp.abort.p â false , wlp.abort.p = true ,
wp.miracle.p = true , wlp.miracle.p = true .

18 
WEAKEST PRECONDITIONS
Thus, skip and abort have their conventional meanings, see [Dijkstra 1976]. Com-
mand miracle is introduced in [Morris 1987]. It is called Fail in [Nelson 1989] and
magic in [Back-von Wright 1989a].
For an arbitrary command c we now consider the special predicates wp.c.true
and wp.c.false. For a state #, proposition wp.c.true.x is interpreted to mean that
every computation starting in x terminates in a state where true holds, i.e., in an
arbitrary state. In other words, c necessarily terminates from x. By quantifica-
tion over all states we get the following definition. Command c is defined to be
necessarily terminating if and only if
(11) 
[ wp.c.true} .
On the other hand, proposition wp.c.false.x is interpreted to mean that every
computation starting at x terminates in a state where false holds. Since there are
no states where false holds, there are no computations starting in x: we might say
that c is not defined at x. Command c could be called defined at x if -*wp.c.false.x.
We shall not use these local concepts,â we only use the following global definition.
Command c is defined to be total (or feasible, cf. [Morgan 1990] Section 21.3.6) if
and only if
(12) 
[-iwp.c.false] .
Now we can prove that guard command ?6 is total if and only if b = true:
?b is total
= 
{(12)}
[ -iwp.(?6).false]
[-.(&=> false)]
= 
{calculus}
[b].
On the other hand, it is easy to verify that the assert command ! b is total for every
predicate b.
Remark. The reader may object to nontotal commands. In [Dijkstra 1976] and
[Dijkstra-Scholten 1990], it is postulated that all commands be total. We shall
not do so, for âas argued in [Nelson 1989]â this postulate is an obstacle for
the development of a simple calculus of commands. See also [Morris 1987] and
[Morgan-Gardiner 1990]. Compare the discussion about the existence of the imag-
inary numbers.
To show that guards are very useful building blocks, we announce that, in

1.4 COMPOSITION AND NONDETERMINATE 
CHOICE 
19
section 1.5, we shall prove that
if b then c else d fl = 
?6; c \ ?-â¢&; d .
In section 1.8, we shall use guards to model the call q.f of a procedure q with an
actual value parameter /. (End of remark)
1.4 Composition and nondeterminate choice
We only need two operators for composition of commands: the operator ';' for
sequential composition and the operator '[' for nondeterminate choice.
The sequential composition (c\d) of commands c and d is the command to
execute c first and then d. For any postcondition p, this suggests that wp.(c; d).p =
wp.c.q, where q = wp.d.p. This implies wp.(c;d).p = wp.c.(wp.d.p), and hence
wp.(c; d) = wp.c o wp.d. In the notation of the last step, we use that function
application by V has a higher binding power than function composition by co'.
We use a similar argument for wlp. In this way, we arrive at the following formal
definition.
With wg ranging over wp and wlp, we define
(13) 
wg.(c\d) 
= wg.c o wg.d .
Sequential composition of commands thus corresponds to the composition of the
predicate transformers. If c and d satisfy the laws (4), (5) and (6), the composition
(c; d) satisfies the same laws. This is easily verified. For example, the case of law
(4) is verified in
wp.(c; d).p
= 
{(13)}
wp.c.(wp.d.p)
=$> {(5)> a n d (4) with p := wp.d.p, q := wlp.d.p}
wp.c.(wlp.d.p)
=> {(5)}
wlp. c. (wlp.d.p)
= 
{(13)}
wlp.(c]d).p .
Without proof we mention the following rules:
(14) 
c; (d; e) = 
(c; d)\ e 
{associativity},
c;skip 
= c , skip;c 
= c 
{neutrality},
abort; c = abort 
, miracle; c = miracle 
{pre-emption}.
In all these cases the proofs are easy.

20 
WEAKEST 
PRECONDITIONS
We now introduce the operator ']' for nondeterminate choice. Informally speak-
ing, command (c [] d) offers the executing mechanism a choice between all executions
of c and d. Since command (c|d) does not specify the choice, wp.(c\d).p should
guarantee that either choice leads to termination in a state where p holds. We use
a similar argument for wlp. Therefore, we define '[]' as the infix operator given by
(15) 
wg.(c H d).p = 
wg.c.p A wg.d.p ,
where wg ranges over wp and wlp. We give operator '[]' a lower priority than the
composition operator ';'.
We generalize the choice operator to a quantifier. For any nonempty set C of
commands, the choice ( J c G C :: c) is defined as the command with the weakest
(liberal) precondition equal to the conjunction of the weakest (liberal) preconditions
of the members of C:
(16) 
wg.( I c G C :: c).p = (VceC 
:: wg.c.p) ,
where wg ranges over wp and wlp. The reason for not allowing the empty choice
will be discussed later, in Sections 3.3 and 4.8. Henceforth, we assume that C is
nonempty whenever ( Q c G C :: c) is mentioned without qualification.
Since the semantics of composition and choice are defined in terms of the se-
mantics of the constituents, these operators respect semantic equality. For example,
we have
cO ^ cl 
A dO ^ d\ 
=Â» cO;dO ^ cl;dl 
A cO J dO ^ cl || dl .
In section 0.3, this property was called the compositionality of '='.
1.5 Intermezzo on the conditional combination
In the abstract syntax, we do not provide a separate construct for the conditional
combination. Theoretically, such a construct is superfluous. In fact, the usual
conditional statement for a condition b and alternative commands c and d satisfies
(17) 
if b then c else d fl ^ 
?6; c [ ?-.&; d .
This is proved by observing that, for wg ranging over wp and wlp and p ranging
over IP,
wg.(if b then c else d fl).p
= 
{standard definition}
(6 r^. wg.c.p) A (-16=^ wg.d.p)
wg.(?6). (wg.c.p) A
= 
{(13) and (15)}

1.6 STATE FUNCTIONS AND LOCALIZED RELATIONS 
21
wg.(?b;c J ?^b;d).p .
According to [de Bakker-de Roever 1973] p. 176, formula (17) goes back to [Karp
1959].
Dijkstra's more general nondeterministic conditional combination can also eas-
ily be expressed. In fact, for a nonempty family of predicates (i Â£ / :: b.i) and a
corresponding family of commands (i Â£ / :: c.i), the conditional combination
IF 
= 
if fli E I:: b.i -> 
c.i fi
is defined by
wp.IF.p 
= 
bb A (Vz E / :: 6.i => wp.(c.z).p) ,
wlp.IF.p 
= 
(Vi E / :: 6.i =*> wip.(c.i).p) ,
where bb = (3i E / :: 6.i), the disjunction of the predicates b.i, see [Dijkstra
1976]. Informally speaking, IF offers the executing mechanism a choice between the
executions of c.i for which b.i holds. If none of the guards b.i holds, the mechanism
does not terminate. Formally, the conditional combination IF can be expressed in
our terms by
(18) 
IF 
^ 
lbb]( 
flteJ:: 
?b.i; c.i) .
The proof is a straightforward calculation.
Conversely, the choice operator and the assertions and guards can be expressed
in terms of the conditional combination and the commands skip and miracle. In
fact, one may verify that, for any predicate 6,
?6 = 
if 6 â>â¢ skip j) true 
ââ¢> miracle fi ,
! b Â£ 
if b -> 
skip f i .
The choice of a nonempty set C of commands can be constructed as
(\ceC 
:: c) 
^ 
if Qc E C :: true 
-> 
c f i .
These formulae show that the expressive power of the conditional combination
together with skip and miracle is equal to the expressive power of the choice operator
together with the assertions and guards. Since it has better algebraic properties,
we use the latter combination.
1.6 Program variables, state functions and localized relations
In actual programming, the state space X has structure: there is a set V of names,
which are called program variables. The state is characterized by the values asso-
ciated to these program variables; so it is a function from V to the set of values.
We use typewriter fount to represent particular program variables, and italic for
variables that range over program variables. Every program variable may have its

22 
WEAKEST 
PRECONDITIONS
own type, i.e., its own set of values. If we use T to denote the union of all occurring
types, every program variable has values in T. Therefore, the state x is a function
x E Tv and the state space X is a subset of Tv.
A function on the state space X with values in T is called a state function.
So, Tx is the set of state functions. We assume that IB is a subset of T, so that a
predicate is a state function with values in IB.
State functions are usually constructed as expressions in the program variables.
For example, if j is a program variable of type integer, we want to regard the
expression 3 â j as a state function with, in state x, the value 3 â x. j . To this end,
we introduce the following definitions.
We construct state functions by means of constants, variables and operators.
Every element t E T induces a constant state function, also denoted Â£, given by
(19) 
t.x = t 
for all states x .
Every program variable v E V induces the function that delivers the value of the
variable in the current state. We use the name of the variable to denote this
function, so that
(20) 
v.x = x.v 
for all states x .
Remark. We do not identify elements t E T and v E V with the state functions
given by (19) and (20). Whenever necessary, we make the distinction by speaking
of the value t (or the program variable v) or the state function t (or v). The reader
who feels uncomfortable with the invisible coercions, may choose to decorate the
state functions t and v, say as tÂ° and vÂ°, so that t^.x = t and vÂ°.x = x.v. (End of
remark)
The third way to construct state functions is to lift operators and quantifiers of the
value domain T to the domain of the state functions Tx, just as we have done with
the boolean operators and quantifiers in definitions (0) and (1). So, we introduce
the convention that each binary operator 0 on T is lifted to state functions / and
9 by
(21) 
(/ Â© g).x = f.x Â© g.x 
for all states x .
Similarly, a quantifier Â© on T is lifted to T x , so that
(22) 
(Â© i E / :: f.i).x 
= (Â© i E J :: 
f.i.x)
for any family (i E / :: f.i) of state functions.
The interpretation of Â© in (21) and (22) is called the localized interpretation.
It is the usual interpretation if ffi is an arithmetical operator like + or x. For rela-
tional operators like = and <, however, it must often compete with the globalized

1.7 THE ASSIGNMENT 
23
interpretation, where for instance the expression f = g stands for the boolean value
that / and g are equal functions. In principle, if one uses the localized interpreta-
tion, the equality of functions / and g must be expressed by [/ = g], cf. definition
(2) (see [Dijkstra-Scholten 1990] Chapter 1). For the relational operators =, < and
>, we shall use the localized interpretation only if the context requires a predicate
(i.e. a boolean state function). For elements of V, the equality symbol has yet a
third interpretation, namely name identity, that is, equality as element of V. We
write v ây w to denote the name identity of v and w, and v ^y w to express that
the names denoted by v and w differ.
Example. A Pascal program may contain
'if i = j then ... else ...'
Clearly, i and j are different program variables (i.e. different elements of V and
different functions on the state space), so the boolean value would be false. The
Pascal interpretation of i = j , however, is the localized one, so that by (20) and
(21) the expression i = j is the state function with
(i = j).x 
= (x.i = x.j) 
for any state x G X .
Similarly, if i and j are integer program variables, the Pascal interpretation of
i < j is the localized relation with (i < j).x 
= (x.i < x.j). (End of example)
1.7 The assignment
The main command for state modification is the assignment v := /, where v is a
program variable and / is a state function of the type of v. The operational meaning
is that state x is updated at location v with new value f.x. In other words, state x
is replaced by state (v <â f).x given by
(23) 
(t; <- f).x.v = f.x ,
(v <â f).X.W 
= X.W 
if W ^y V .
Therefore, letting wg stand for wp or wlp, the predicate-transformation semantics
of command v :â f is defined by
(24) 
wg.(v := f).p.x 
= p.((v <- f).x)
for all predicates p and all states x, or equivalently
wg.(v 
:= f).p 
= 
p o ( v < - f ) .
The standard way to determine wg.(v := f).p is captured in the substitution rule:

24 
WEAKEST 
PRECONDITIONS
(25) Theorem. Let p be a predicate expression, i.e., a predicate given as an
expression in constants, program variables, operators and quantifiers by means of
formulae (19), (20), (21) and (22). Then
wg.(v := f).p 
= pv
f
where pv* is the expression obtained from p by substituting / for every occurrence
of v in expression p.
Proof. We use induction on the structure of expression p. Since the constituents of
p need not be predicates, we generalize the assertion. By definition (24), it suffices
to prove
e.((v*-f).x) = (e}).x
for every state x and every state function e that is given as an expression in con-
stants, program variables, operators and quantifiers by means of (19), (20), (21)
and (22). This is done by structural induction on expression e. If e = t Â£ T then
t.((v *- f).x)
= 
{(19), twice}
t.x
= {v does not occur in the simple expression t}
(t}).x .
If e = w G V and w ^y v, then
w.((v <~ f).x)
- 
{(20)}
(v <â f).x.w
= 
{(23)}
x.w
- 
{(20)}
w.x
= {v does not occur in the simple expression w}
(w}).x .
If e = v then
v-((v <- f)-x)
= 
{(20)}
(v <- f).x.v
= 
{(23)}
f.x
= 
{substitution}

1.7 THE ASSIGNMENT 
25
If e = eO Â© el then
(eOÂ©el).(0 <-f).x)
= 
{(21)}
e0.(O <- f).x) Â© el.((v <- /).#)
= 
{induction hypothesis}
(eOp.z Â© (elp.z
= 
{(21) and definition of substitution}
(eOffiel)^.z .
The case of a quantifier is completely analogous to the case of an operator, provided
that the dummy of the quantification is renamed when necessary. (End of proof)
Example. For an integer program variable j , the substitution rule gives
wp.(j := j + l).(j < 5) = 
(j + 1< 5)
Compare the example in 1.0. (End of example)
Example. It is not our aim to provide or discuss data structuring facilities, but the
formalism presented is rich enough to model variable arrays. In fact, since V is the
set of program variables, a variable array a with index set J can be regarded as
a function a Â£ V1. Assume that a is an array of integers, i.e., that all variables
a.z are of type integer. Let us consider the postcondition (Vz :: a.z > 0) and the
assignment v := n. Then we have
wp.(v := n).(Vz :: a.z > 0)
= 
{(25)}
(Vi :: (a.Â» > 0)Â»)
= 
{case distinction}
(Vi : a.z ^y v : a.z > 0)
A (n > 0 V (Vz :: a.z Â±v v)) .
Notice that the formalism requires that name v be independent of the state (it may
be a.5). In Section 1.8, we show how to model general array modification a.f := g
for state functions / and g. (End of example)
Example. One of the first rules of program transformation is that an assignment
v := f can be replaced by v ~ g under the precondition (/ = #), in the sense of
(21). This rule is of course well-known and often used. For example, in [Dijkstra
1990], Chapter 2, Gries uses the rule very effectively in the derivation of a program
for the maximum-segment-sum problem.
Our formalization of the rule is
(26) 
?(f = g);v:=f 
3 
?(f = g);V := g .

26 
WEAKEST 
PRECONDITIONS
This formula can be proved as follows. By (8), (13) and (24), we have for any
predicate p
wg.(?(/ = ff); v := f).p 
= 
(/ - g) => (p o (v Â«- /)) ,
where wg is wp or wip. Therefore, by definition (7), formula (26) follows from the
observation that for any predicate p
[(/ = sO=*(pÂ° (Â«<-/)) = (f = g)=>(pÂ°(v*
= 
{(r =Â£> ) distributes over = , see exercise 1.1.6}
[(f = g) => (p <>(Â«Â«-/) 
= 
po(Â»<
<Â£= 
{composition and (2)}
(Vrc G X : f.x = ^.a; : (u <â /).a; = (v <â
= 
{(23)}
true .
(End of example)
1.8 Deterministic choice
A deterministic choice is a choice between (possibly infinitely many) different com-
mands that depends on the value of a given state function. The deterministic choice
can be regarded as a procedure call with the state function as an actual parameter.
The formal treatment is as follows. Recall that T is the 'universal' set of values,
as introduced in the first paragraph of section 1.6. Let (t â¬ I :: q.t) be a family of
commands with I C.T. We want to be able to treat this family as a procedure with
a formal input parameter t. Therefore, if / G IX is an /-valued state function, we
define the call q.f as the command
(27) 
q.f 
= (||< â¬ / : : ? ( / = *);?.<).
This definition implies, with wg for wp and wlp,
(28) 
wg.(q.f).p 
= (Vi G / :: ( / = *)=Â» ^g.(q.t).p) 
.
This is proved by
wg.(?./).p
= 
{(27), (16)}
( V i e / : : wg.(?(f = t);q.t).p)
= 
{(8), (13)}
It follows from (28) that, if wg.(q.t).p is an expression E in t, then wg.(q.f).p = E\.

1.9 APPENDIX ON PREDICATE CALCULUS 
27
Example. If we regard the constant assignments v := t with t Â£ T as simple
commands, the general assignment v := f with / Â£ Tx is a special case of the
above construction. (End of example)
Example. If / is a predicate, so that I = IB, then q.f is the construction
if / then q.true else ^.false fi .
(End of example)
Example. Array modification. Let a be an array with index set /, regarded as a
function a Â£ / â> V. The array modification a./ := g for an /-valued state function
/ and a T-valued state function g can be modelled as
( | t â¬ /::?(/ = t); a.< := g) .
(End of example)
Example. The bounded repetition. Let {% Â£ IN :: q.i) be a family of commands. Let
(i Â£ IN :: p.i) be a family of predicates such that for all i Â£ IN
[p.i => wp.(g.t).(p.(t + l))] â¢
For an integer valued state function /, the bounded repetition
loop = for i := 0 to / do q.i od
is supposed to satisfy the specifications
?(/<0);ioop 
Â£ 
?(/<0)
and for all j Â£ IN
[P-0A(/=j) 
=Â» wp.loop.(p.(i + l))] .
The deterministic choice allows us to express loop in terms of the basic formalism.
In fact, we can define commands
r.O = skip ,
r.(i + 1) = r.z; g.z for all i Â£ IN .
A straightforward verification shows that
ioop = 
?(/<0)||(||te]N::?(/ = 0;r.(t + l))
satisfies the above specification. (End of example)
1.9 Appendix on predicate calculus
For programming practice and programming theory, we need the ability to calculate
effectively with boolean values and boolean functions. We need not reconsider
the foundations of classsical logic but only isolate a number of useful theorems
(rules). These theorems make it easier to avoid the pitfalls of state based reasoning
and enable a calculational style which leads to shorter proofs than with natural

28 
WEAKEST 
PRECONDITIONS
deduction. We do not provide a complete set of rules, but we give some important
ones and show how such rules can be justified. Afterwards the readers can extend
their repertoire by means of the exercises of Section 1.1.
The lowest level is propositional calculus without quantification. Here, we have
well-known rules like the commutativity and the associativity of 'A' and CV', the
double negation rule, and De Morgan's laws. The implication operator satisfies
a=>b = -ia V b .
The equivalence operator satisfies
(a = 6) = 
(a=>6) A (6=Â»a) .
There are two distributivity laws
a A (b V c) = (a A b) V (a A c) ,
a V (b A c) = 
(a V b) A (a V c) .
The next level contains the universal and existential quantification. The above
distributivity laws generalize to distributivity laws of 'A' over ' 3 ' and of 'V' over
'V. For a proposition a and a family of propositions (i el:: b.i), the second one
reads
a V (Vi G / :: b.i) = (Vi G / :: a V fc.i) .
If it is not an axiom, this rule can be proved by case distinction. 
Indeed, if
proposition a holds, the lefthand side holds and the righthand side reduces to
(Vi G / :: true) which is also true (regardless whether I is empty or not). If a
is false, both sides reduce to (Vi G / :: b.i). This concludes the proof.
The third level consists of the predicates introduced in Section 1.1. The dis-
tributivity law now gets the form
pV(Vi E I :: q.i) = (Vz G / :: p V q.i)
for every predicate p and every family of predicates (i G / :: q>i)- This can be
proved by observing that for every x G X
(p V(Vi el:: 
q.i)).x
= {(O)and(l)}
p.x V (Vi Â£ I:: q.i.x)
= 
{distributivity law for propositions}
(Vi el:: p.x V g.i.a;)
= 
{(O)and(l)}
(Vi G I::pVq.i).x 
.
The universal quantification over the state space, cf. formula (2), has its special
rules like
[ true ] = true ,

1.10 EXERCISES 
29
[pAq] 
= 
[p]A[Â«],
and for a family of predicates
[(VtG/:: Â«.i)] = 
(Vte/::[?.Â«]).
For example, the last rule is proved in
[(V*Gl::g.t)]
= 
{(2)and(l)}
(Vs G X :: (V* 6 / :: ?.*.a:))
= 
{interchange of quantifications}
(Vi<E/::(V:r â¬X::?i.z))
= 
{(2)}
(VtG J::[Â«.t]).
For further extension we refer to the exercises of Section 1.1.
1.10 Exercises
Exercises of Section 1.1.
Whenever possible the reader should try and avoid to use states.
Exercise 0. Prove that for all predicates p, q, r
(a) 
(p=>?) = 
(p = pAg),
(b) 
(p=)>gVr) = 
(p A -ig => r) 
{shunting} ,
(c) 
(p=>(q^r)) 
= 
( p A ^ r ) ,
(d) 
(p=>qAr) 
= (p=>q)A(p=>r),
(e) 
(pvq=>r) 
= 
( p ^ r ) A ( ^ ^ r ) ,
Exercise 1. Prove that for every set U of predicates
[ ( 3 p e U : : p ) } 
<= 
( 3 p e U : : [ p } ) .
Exercise 2. Distributivity of 'A' over 'V. Prove that for every predicate p and
every family of predicates (i G / :: q.i)
7 ^ 0 
=^ (p A (Vz e / : : g.z) = (Vz e / :: p A g.i)) .
Why do we need the condition on /?
Exercise 3. The distributivity rules for 3. Prove that for every predicate p and
every family of predicates (z G / :: q.i)
(a) 
pA(3ieI::q.i) 
= (3i E / :: p A q.i) ,
(b) 
7 ^ 0 
=* (p V (3z G / : : q.i) = (3i Â£ I :: p V q.i)) .
Exercise 4. Prove that for every predicate q and every set U of predicates
(a) 
[q^(VpeU::p)} 
= (Vp Â£ U :: [q^p]) ,
(b) 
[ ( 3 p e U : : p ) ^ q ] 
= (Vp e U :: [p=>q)) 
.

30 
WEAKEST 
PRECONDITIONS
Exercise 5. Prove that for all predicates p and q
p = q = (VreP: [p^r] = [q^r]) .
Exercise 6. Prove that for all predicates p, q, r
Exercise 7. Let p and q be predicates.
(a) Prove that [p] V [q] => [pVg].
(b) Prove that [p=*>?] => ([p] =>[Â«]).
(c) Give examples to show that the outer implications in (a) and (b) cannot be
replaced by equivaleces.
Exercises of Section 1.3.
From now onward, the reader is urged to avoid the usage of states x G X
whenever possible.
Exercise 0. Prove that ! b is total for every predicate b.
Exercises of Section 1.4.
Exercise 0. Prove that for any command c:
(a) <s? 
c;miracle = miracle 
= 
[wp.c.true] ,
(b) 
c; abort = abort 
= 
[ -iwp.c.false] A [ wlp.c.true] .
Exercise 1. Let c and c? be total commands.
(a) Prove that (c; d) and (c Q d) are total.
(b) Prove that (?6; c | ?-Â»&; c?) is total for every predicate 6 G IP.
Exercise 2. Prove that for predicates a and b
?a; ?6 ^ 
?(a A 6) .
Exercise 3. Prove that for every family {% G / :: &.i) of predicates
(||t G/::?6.Â») 
^ 
?(3* G J :: 6.i) .
Exercise 4. Prove that for all commands c and d:
c | ?faise; rf Â£ c .
Exercise 5. Prove the postdistributivity rule
(DcGC::c);^ 
Â£ 
( I c E C : : c ; g )
for any command # and any nonempty set of commands C.
Exercise 5. Prove the rules (14).
Exercises of Section 1.5.
Exercise 0. Prove that IF as defined by formula (18) satisfies the equations given
for wp.IF and wlp.IF.

1.10 EXERCISES 
31
Exercises of Section 1.6.
Exercise 0. Let j and k be integer program variables. Prove that
skipÂ§ 
! ( j = k ) 
<* 
! ( j < k ) D 
! ( k < j ) .
Exercises of Section 1.7,
Exercise 0. Let v be an integer program variable. Discuss all errors in the following
calculation:
wp.(v := 3 ; v := 5).(v > 4)
= 
{(13)} wp.(v := 3).(wp.(v := 5).(v > 4))
= 
{(25)} wp.(3 := 5).(3 > 4)
= 
{calculus} wp.(3 := 5).faise
= 
{(25)} false .
Give a correct calculation of the initial expression.
Exercises of Section 1.8.
Exercise 0. Prove that command loop^ as defined at the end of Section 1.8, satisfies
its specification.

CHAPTER 2
ANNOTATION, RECURSION AND REPETITION
2,0. This chapter is devoted to the introduction of annotations, procedures, re-
cursion and repetitions, all concepts highly relevant to programming practice and
programming methodology. In 2.1 we introduce Hoare triples as a specification
method. Hoare triples are used in 2.2 for correctness proofs by annotation. In 2.3
and 2.4 we treat procedures in a programming language like Pascal. The specifica-
tion and invocation rules are discussed in Section 2.3. The correctness of recursive
procedures is treated in Section 2.4. The methods presented here are not new but
deserve to be promoted.
In Section 2.5 we present and prove an abstract version of the rule for total
correctness of recursive procedures. In 2.6 we introduce homomorphisms, functions
from commands to predicate transformers that satisfy the standard laws of wp and
wlp. Homomorphisms are used in 2.7 to give Hoare's Induction Rule for conditional
correctness of recursive procedures, and a related rule for the necessity of precon-
ditions. Finally, in Section 2.8, the results on recursive procedures are specialized
to the repetition.
With respect to recursive procedures, this chapter is not 'well-founded'. We
only postulate some properties and proof rules, but the definition of the semantics
of recursion (i.e., of the functions wp and wlp) and the proof of the postulates are
postponed to Chapter 4.
2.1 Specification with Hoare triples
Weakest preconditions provide the easiest way to present predicate-transformation
semantics. The formalism of Hoare triples, however, is completely equivalent and
more convenient for program derivations and proofs of program correctness. Since
we use wp as the foundation, Hoare triples are defined concepts. For predicates p
and q and a command c, the Hoare triple {p} c {q} is defined as the boolean value
given by

2.1 SPECIFICATION WITH HO ARE TRIPLES 
33
(0) 
{p} c{q) 
= [p=> wp.c.q ] .
Actually, Hoare introduced (cf. [Hoare 1969] and [Manna-Vuillemin 1972]) the no-
tation
(1) 
p{c} q = [p^wlp.c.q] .
In (0), we speak of total correctness: precondition p implies that execution of c
terminates in a state that satisfies q. In (1), we speak of 'partial or conditional
correctness: precondition p implies that if c terminates in some state that state
satisfies q.
Example. If p is a predicate expression, cf. 1(25), the assignment v := / is easily
seen to satisfy
{p}} Â« : = / M .
For predicates p and 6, the assertion ! b satisfies
{pAb} 
\b {p} .
On the other hand, the guard ?6 satisfies the Hoare triple
{P} n 
{PAb}
= 
{definitions (0) and 1(8)}
[p =>(b=>pAb)]
= 
{calculus}
true .
(End of example)
Remark. Formulae (0) and (1) are by no means the standard view of Hoare triples.
For example, in [de Bakker 1980] definition 3.29, the triple {p} c {q} is the predicate
(/>=>â¢ wlp.c.q). In other books (e.g. [Chandy-Misra 1988] Chapter 3), Hoare triples
are part of a logical deduction system, so that the validity of a Hoare triple includes
its derivability.
Hoare triples often contain free variables, over which an implicit universal
quantification is intended. For example, the intention of
{v = t} v:= v - 1 {v <<}
is that whatever the initial value (t) of v may be, the final value is smaller. This is
made explicit by writing
(Vt :: {v = t} v := v - 1 {v < t} ).
Notice that the role of variable t is totally different from the role of v. Variables like
t will be called specification constants. Some authors use the term logical variables.
(End of remark)

34 
ANNOTATION, 
RECURSION AND 
REPETITION
Hoare triples satisfy the following two weakening rules
(2) 
[p=Â»Â«] A {q}c{r} 
=* {p} c {r} ,
{p}c{q} 
A [q=>r] => {p} c {r} .
The first weakening rule follows from definition (0). In the proof of the second rule,
axiom 1(5) is also needed.
The composition rule for Hoare triples is
(3) 
{p}c{q} 
A {q}d{r} 
=Â» {p}c;d{r} .
This rule is proved in
{p}c{q} 
A {q}d{r}
= {(0)}
[p=> wp.c.q] A [q=> wp.d.r]
=> {1(5) with p := q and q := wp.d.r}
[p=> wp.c.q] A [ wp.c.q => wp.c.(wp.d.r)]
=> {transitivity of ' => ' and 1(13)}
[p=>wp.(c;d).r)]
= {(0)}
The rule for the choice operator is that, for every nonempty family of commands
(t â¬ / :: c),
(4) 
{p} ( | i G / " c.t) {^} 
= (V. G / :: {p} c.i {Â«} ) .
This rule follows from definition 1(16) and exercise 1.1.4(a).
The above rules are often used and easily recognized in program annotations.
The next result is less familiar, but well-known and very useful.
(5) Covering rule. Let p and q be predicates, c a command and (i El:: r.i) a
family of predicates with [p=>(3i :: r.i)]. Then we have
{p}c{q} 
= (Vt:: {PAr.i}c{q} 
) .
Remark. The condition [p=$-(3i :: r.i)] says that the set of states where p holds is
contained in the union of the family of sets of states corresponding to the predicates
r.i. This family of sets can be regarded as a covering of the set where p holds. Hence
the name of the rule. (End of remark)
Proof. We begin with the righthand side.
(Vt:: {PAr.i}c{q} 
)
= {(0)}
(Vz :: [p A r.i=$> wp.c.q] )
= 
{exercise 1.1.4(b)}

2.2 PROOFS BY ANNOTATION 
35
[ (3z :: p A r.i) =>- wp.c.q]
= 
{exercise 1.1.3(a)}
[p A (3 i :: r.i) =$>- wp.c.q]
= 
{use given implication}
[p^ wp.c.q]
^ {(0)}
{p}c{q} .
(End of proof)
2.2 Proofs by annotation
It is of practical importance that in cases where the correctness proof of a program
is not very involved, this proof can be integrated in the program text. For this
purpose we use the Floyd-Hoare method of inductive assertions, which is a direct
application of the Hoare triple rules of the previous section.
In this method the correctness of a Hoare triple {p} c {q} is proved in a top-
down fashion by induction on the structure of command c. The result is called
an annotation. This is an extension of the syntactic structure of c in which every
constituent of c is replaced by a Hoare triple. The following annotation rules are
available.
0. Simple commands. In view of the example in the previous section, the follow-
ing Hoare triples are accepted without further proof:
{p}} v:=f M ,
M 
n {PAb} .
1. Weakening Rule. If A is an argument that proves [p=^#], then it is allowed
to use the annotation
M (*A*) {q} .
2. Sequencing Rule. In order to prove {p} c\ d {q} , it suffices to provide a pred-
icate r and to prove {p} c {r} and {r} d {q} . This reduction is recorded in the
annotation
{p}c{r}d{q} .
3. Choice Rule. In order to prove {p} ( J i G / :: c.i) {q} for a family of com-
mands (i Â£ / :: c.i), it suffices to prove {p} c.i {q} for all i Â£ I. This reduction is
recorded in the annotation

36 
ANNOTATION, 
RECURSION AND 
REPETITION
4. Covering Rule. In order to prove {p} c {q} , it suffices to provide a family of
predicates (i E J :: r.i), to prove that [p=> (3i :: r.i)] and to prove all Hoare triples
{pAr.i} c {q} . If A is an argument that proves the implication, this reduction can
be recorded in the annotation
{p} 
(* A introduces i *)
{pAr.i} c{q} .
The soundness of rule 1 follows from the weakening rules (2), together with
the transitivity of the implication if rule 1 is applied repeatedly. The soundness of
the rules 2, 3 and 4 follows from (3), (4) and (5).
For other programming languages, the repertoire of annotation rules can be
extended to cover the conditional statement and the repetition as well. An anno-
tation rule for the repetition is given in section 2.8 below. The annotation rule for
the conditional statement is:
5. Conditional Rule. If {p A 6} c {q} and {p A -i&} d {q} then
{p} if b then c else d fi {q} .
This reduction is recorded in the annotation:
{p}
if 6 then 
{pA&} c 
{q}
else 
{p A ->&} d 
{q} 
fi
M â¢
Example. Let i, j , k be integer program variables. Command c assigns to k the
maximum of i and j . This is specified by means of a specification constant t in the
following way:
{p : i max j = t} c {q : k = t} .
We take c = (?(j < i) ; k := i [ ?(i < j) ; k ~ j). The correctness is proved in
the annotation
{p : i max j = t}
( M 
?(j < i) 
{P A j < i}
(* calculus *) 
{i=t} 
k := i 
{q : k = t}
II M 
? ( i < j ) 
{P A 
i < j }
(* calculus *) 
{j = t} 
k := j 
{q : k = t}
) M â¢
Notice that we have used each of the rules 0, 1, 2, and 3. (End of example)
The covering rule allows us to introduce a new specification constant for the
value of an expression at a point in the program text, provided that the new constant

2.3 SPECIFICATION AND INVOCATION OF PROCEDURES 
37
is eliminated in the remainder of the program. This point is illustrated in the next
example.
Example. Let k be an integer program variable. Let c be the squaring command
specified by
(Vi G2Z :: {k = i} c {k = i2} ) .
It may be clear that {k > 3} c {k > 9} . This is formally proved in the following
annotation, based on the covering rule
{k > 3} (* there is i > 3 with k = i: introduction of i *)
{k > 3 A k = i} 
(* weakening *)
c 
(* specification *)
{k = i2} 
(* range i > 3, weakening *)
{k>9} .
(End of example)
2.3 Specification and invocation of procedures
Procedures are introduced to allow a separation between the application of a com-
mand and its implementation. This is especially useful if the command is to be
applied several times. Parametrization is helpful to allow application under differ-
ent circumstances. Recursion may be regarded as the accident that the procedure
can be applied in its own implementation.
Ideally, the specification of a procedure is the only bridge between implemen-
tation and application. In principle, it is possible to specify procedures by means of
weakest preconditions. In practice, this is not convenient: prescribing the weakest
precondition of a procedure is often overspecific. We therefore assume that a pro-
cedure is specified by means of Hoare triples and the list of all external variables
involved.
For a programming language like Pascal we prefer the following declaration for-
mat, which is inspired by [Gries 1981] and [Martin 1983]. For simplicity, we assume
that the procedure has precisely one variable for each of the variable modalities.
(6) 
proc h(x : item; var y : item)
{ext u,v\ : item;
all i G item : C : pre P, post Q} .
The meaning of the specification is defined by

38 
ANNOTATION, RECURSION AND REPETITION
(7) Correctness Rule. An implementation of procedure h is correct if it satisfies
the conditions:
(a) all external variables used in the body of h are listed after the key word ext;
external variables that are threatened to be modified (in the sense of [Jensen-Wirth
1985]) are marked with T,
(b) the value parameters (x) do not occur in postcondition Q,
(c) for all values of the specification constants (i) that satisfy condition C the body
of h satisfies {P} body./z {Q} . If C is omitted, the default value true is meant.
Of course, variable modalities that do not occur can be omitted from the spec-
ification. The list after key word ext is needed to exclude aliasing upon invocation
of h. Condition (b) may seem unnecessarily restrictive. There are three reasons
for imposing it. Firstly, it encourages specifications with simple postconditions
(J.E. Jonker). Secondly, if value parameters would be allowed in the postcondition,
the invocation rule (to be treated below) would be complicated by the fact that
the value of the expression for the actual parameter can be modified by the call.
Finally, condition (b) is necessary if one wants to combine condition (c) with the ex-
ploitation of value parameters as local variables. Compare [Gries 1981] chapter 12.
For recursive implementations rule (7) is correct but inadequate, for requirement
(c) is too strong; we come back to this in Section 2.4.
Example. Let i be a program variable. Procedure copyO copies the value of i into
the parameter, procedure copyl copies the parameter into i:
proc copyO (var p : item)
{ ext i ; all v G item :: pre i = v, post p = v} ;
begin p := i end ;
proc copyl (p : item)
{ ext i! ; all v E item :: pre p = v, post i = v} ;
begin i := p end .
(End of example)
As this example may indicate, it is not our aim to discuss the methods for
disciplined and effective usage of procedures in every day programming. We only
want to indicate that full specifications of procedures are feasible. In the next
paragraph we show that these specifications can be used to prove the correctness
of invocations of procedures. From the semantic point of view the most interesting
cases are the recursive invocations to be treated in the next section. We first treat
the easier case of an invocation of a procedure outside of its own body.

2.3 SPECIFICATION AND INVOCATION OF PROCEDURES 
39
An invocation or call of procedure h declared in (6) is of the form h(E, t) where
E is an expression and t is a program variable, both of which are well-defined at
the position of the call. To avoid aliasing we require that the actual var-parameter
is not used as an external variable:
(8) 
t$Ext,
where Ext is the list headed by ext. We let Ext! be the sublist of Ext that consists
of the variables that are threatened to modified. Precondition P and postcondition
Q of specification (6) need not mention that external variables outside of Ext! are
unchanged. In the specification of the call an additional predicate R is used to
express this fact. Predicate R is supposed to satisfy
(9) 
Var.Rn({t}UExt!) = 0
where Var.R is the list of program variables that occur in R. If Ext! = Ext,
conditions (8) and (9) are more symmetrically expressed by stating that the three
lists Ext, Var.R and the list of actual var-parameters are pairwise disjoint.
The call h(E, t) is specified by
(10) Invocation Rule. If (8) and (9), then for all i such that C:
{P%AR} 
h{E,t) {Qy
tAR} .
In words: in the expressions for P and Q the formal parameters are replaced by
the actual parameters.
Remark. If there are more var-parameters, the avoidance of aliasing also requires
that all actual var-parameters differ. For simplicity we do not allow calls of the
form h(E, a[F]) where the actual var-parameter is an array field. (End of remark)
Example. A procedure to compute natural powers of integers can be specified by
proc pow (x : integer ; var y : integer)
{ all Z Â£ integer :: pre y > 0 A Z = xy , post y = Z} .
Let i and t be external variables. We use an annotation to prove
{i > 1 A t > 3} pow (i + l,t) {i > 1 A t > 8} .
We first use the covering rule, then weakening, then the invocation rule, and finally
weakening again.
{i > 1 At >3}
(* (i + 1)* has some value Z *)
{ i > l A t > 3 A Z = (i + 1)*}
(* calculus *)
{t > 0 A Z = (i + 1)* A (R : i > 1 A Z > 8)}
pow (i + l,t) (* invocation *)

40 
ANNOTATION, 
RECURSION AND 
REPETITION
(* Var.R = {i} and hence Var.R n {t} = 0 *)
{t = Z A (R : i > 1 A Z > 8)}
(* calculus *)
{i > 1 At >8} .
(End of example)
2.4 Correctness of recursive declarations
If the body of procedure h in declaration (6) is recursive, it is difficult to guarantee
condition (7)(c). Therefore, Correctness Rule (7) had better be adapted in such a
way that Invocation Rule (10) can be applied to the recursive calls in the proof of
condition (7)(c).
(11) Correctness Rule. A recursive implementation of procedure h specified as
in (6) is correct if conditions (7)(a) and (7)(b) are satisfied, together with condition
(d) given by
(d) There is an integer valued function vf (to be called the variant function) in the
specification constant (z), the parameters #, y and the variables in Ext, such that
for every ra Â£ TL and every i with C the induction hypothesis given below implies
(12) 
{PAvf <ra} 
body./i 
{Q} .
The induction hypothesis is that every recursive call h(E, t) satisfies for all j with
Cj and for all predicates R with (9) the Hoare triple
(13) 
{(P A vf < m)fy*t 
A m > 0 A R}
h(E,t)
{Q)yt 
A R) .
The soundness of this rule can be proved by induction on the value of vf in the
state where procedure h is called. If vf is negative in the precondition of (12), we
can use m = â 1. In that case the precondition of the induction hypothesis is false
and, hence, the induction hypothesis is true. This case serves as the base case of
the induction. For states with vf < ra and m > 0, the execution uses recursive calls
with vf < ra. Below in Section 2.5 we give a formal proof of an abstract version of
rule (11).
The conjunct ra > 0 in the precondition of induction hypothesis (13) must not
be forgotten: it provides the base case of the induction. See also exercise 2.7.4.
Notice that condition ra > 0 enters only in the precondition of (13), that is in
the precondition of the recursive call(s). This observation may guide the design of

2.4 CORRECTNESS OF RECURSIVE DECLARATIONS 
41
an implementation. Other correctness rules for recursive procedures often require
more case distinctions or additional proof obligations. Rule (11) allows a correctness
proof (or rather a verification of condition (ll)(d)) that consists of an annotation
of the body. This is illustrated in the next examples.
Example. We first provide a very simple example with a complete annotation.
Procedure pow3 computes the natural powers of 3, as specified in
proc pow3 (x : integer; var y : integer)
{all i Â£ integer :: pre P : x > 0 A i = 3X , post Q : y = i} .
We take variant function vf = x. The instantiation of the induction hypothesis
(13) is that for every expression J5, every variable Â£, every specification constant j
and every predicate R with Var.R D {t} = 0:
{E>0Aj=3EAE<mAm>0AR}
pow3 (E, t)
{t = j A R} .
We give a body of pow3 with an annotation to show that condition (12) is satisfied.
{P A vf < m} (* definitions of P and vf *)
{x > 0 A i = 3X A x < m}
if x = 0 t h e n 
{x>0Ai 
= 3xAx<mAx 
= 0}
(* 3Â° = 1 *) 
{1 = i]
y : = l 
{Q: y = i}
else 
{x>0Ai 
= 
3xAx<mAx^0}
(* 3X~X = j for some j ; calculus *)
{x - 1 > 0 A j = 3*" 1 Ax-KmAm>0A(R:3'j 
= i)}
pow3 (x â l,y) 
(* induction hypothesis; Var.R D {y} = 0 *)
{y=j 
A3-j = i}
(* calculus *) 
{3 â¢ y = z}
y :=3-y 
{Q : y = i}
This proves that the induction hypothesis implies the properly instantiated version
of (12). Therefore, procedure pow3 is correctly implemented. (End of example)
Example. Consider a procedure for integer division as specified in
proc divi (y : integer)
{ext x!, q! : integer ; all X, Y G integer : X > 0 A F > 0 :
pre P : x = X A y = Y,
post Q : X = q - F + x A 
0 < x < F } .

42 
ANNOTATION, 
RECURSION AND 
REPETITION
Specification constant X captures the initial value of the external variable x. We
use a specification constant Y to denote the value of y. Postcondition Q is easily
established if x < y. Therefore, we use the variant function vf = x â y. The
induction hypothesis is that, if Var.R f! {x, q} = 0 and W > 0 and T > 0, then
{x = WAE 
= 
TAx-E<mAm>OAR}
divi (E)
{W = q-T + x A 
0 < x < T 
A R] .
The annotated body is
{x = X A y = Y A x-y<ra}
i f x < y t h e n 
{ x = X > 0 A x < y = Y }
{X = 0-Y + x A 
0 < x < y }
q:=0 {Q}
else {x = X A y = Y > 0 A 0 < x - y < r a }
{x = X A 2-y = 2Y 
A x-2-y 
<m A m > 0 A y = Y}
divi (2 â¢ y) ;
{X = q-2'Y 
+ x A 
0<x<2-Y 
A y = Y]
q : = q - 2 ;
{X = q-Y + x A 0<x<2-Y 
A y = Y}
if x < y then skip {Q}
e l s e 
{X = q - Y + x A y < x < 2 Y 
A y = Y }
x:=x-y 
{ X = ( q + l ) - r + x A 
0 < x < r } ;
q : = q + l 
{Q}
fl 
{Q}
fl {Q} â¢
(End of example)
2.5 An abstract version of recursive procedures
We now leave the realm of Pascal procedures in order to prepare the treatment of
procedures in a more abstract setting.
A direct proof of correctness rule (11) would have to use induction hypothesis
(13) with its mess of renamings. Therefore, we apply abstraction. A procedure with
parameters can be regarded as a family of procedures. If the procedure is recursive,
it is a family of mutually recursive procedures. Each of these procedures, say /i.a,
may be specified by a family of Hoare triples
{p.a.fl} h.a {q.a.fl} .

2.5 AN ABSTRACT VERSION OF RECURSIVE PROCEDURES 
43
In this way, specification constants and additional predicates R as used above,
can be accommodated. If we now encode the pair (a, /?) in a single symbol i
and write h.i = ft. a, we get a family of procedures h.i with preconditions p.i and
postconditions q.i, where i ranges over some set /.
The declaration of a procedure is a recipe that associates the name of the
procedure to its body. This body is a command expression that may contain one or
more recursive calls, i.e. occurrences of procedure names. We allow parameters but,
for simplicity, no local variables. We treat the parameters as part of the procedure
name. Therefore, we use an arbitrary set H of procedure names and we assume
that every name ft Â£ H is equipped with a procedure body body.ft, which is a
command expression that may contain occurrences of elements of H. A syntactic
formalism for these expressions is given later.
Example. In this abstract setting, the last example of Section 2.4 can be represented
by a family of procedure names (y : y > 0 : divi.y) with the declaration
body.(divi.y) =
( ?(x < y); q := 0
| ?(y < x); divi.(2 â¢ y); q := q â¢ 2 ;
( ?(x < y) | ?(y < x); x := x - y; q : = q + l )
)â¢
(End of example)
The function body, which maps procedure names to command expressions, is
regarded as the declaration of the procedures. We postpone the definition of wp.h
and wlp.h for procedure names ft. For the moment we only postulate ft = body./i ,
or equivalently
(14) 
wp.h = wp.(body.ft) ,
(15) 
wlp.h = wlp.(hody.h) .
If the declaration of h does not contain recursion, these postulates are clearly con-
sistent and strong enough to define the semantics of h. In the case of a recursive
declaration it is not clear that they are consistent or applicable. The proof of
consistency is postponed to chapter 4. The applicability is shown presently.
The next theorem is the abstract version of rule (11).
(16) Recursion Theorem, Let (i G / :: h.i) be a family of procedure names. Let
(i G / :: p.i) and (i Â£ I :: q.i) be families of predicates. Let vf â¬ I â* 7LX be a
function such that for every m Â£ 7L

44 
ANNOTATION, 
RECURSION AND 
REPETITION
(Vi G / :: {p.i A vf.i < m A m > 0} h.i 
{q>i} ) 
{ind.hyp.}
=> (Vi el:: 
{p.i A vf.i < m} 
body.(h.i) 
{q.i} ) .
Then {p.i} fo.i {q.i} ) for all indices i G /.
Remark. The antecedent of the implication is called the induction hypothesis. The
theorem implicitly allows mutual recursion. In fact, the body of h.i may call h.j.
(End of remark)
Proof. In view of postulate (14) and definition (0), we have
(17) 
(Vi G / :: {p.i A vf.i < m- 
1 A m > 0} /i.i 
{?.i} )
=> (Vi G / :: {p.i A vf.i < m} 
/i.i 
{^.z} ) .
If rn = â 1, the precondition of the antecedent is false. Therefore, all Hoare triples
of the antecedent are true, so that formula (17) implies
(18) 
(Vi El:: 
{p.i A vf.i < -1} 
h.i 
{q.i} ) .
By natural induction with (18) for the base case and (17) for the step, we obtain
that for all integers m > â 1
(Vi el:: 
{p.i A vf.i < m} 
h.i 
{q.i} ) .
Interchanging quantifications we see that for all i G /
(Vm G IN :: {p.i A vf.i < m} 
h.i 
{q.i} ) .
On the other hand, for every i G /, we have [ (3 m G IN :: vf.i < m)]; in fact, for
every state x there is m with vf.i.x < m. Therefore, covering rule (5) implies that,
for all i e /,
{p.i} 
h.i 
{q.i} .
(End of proof)
2.6 Homomorphisms and simple commands
In this section we prepare the ground for two additional proof rules for recursive
procedures. They do not follow from the postulates (14) and (15), but from the
formal definitions of wp and wlp to be given in Chapter 4. We present these rules
in this chapter, since they are useful for proofs of correctness and incorrectness
of concrete programs. At the same time they form motivation for the theory of
Chapter 4.
The above Theorem (16) is a form of induction over the state space. The
induction rules to be presented below are based on induction over the possible
interpretation functions. We therefore generalize the functions wp and wlp in the
following definition.

2.7 INDUCTION RULES 
45
A function w is called a homomorphism if and only if it is a function from
commands to monotone predicate transformers that satisfies
(19) 
w.(c; d) â w.c o w.d 
for all commands c, d,
w.( \ j Â£ J :: c.j).p = (Vj Â£ J :: w.(c.j).p)
for all predicates p and all nonempty families (j Â£ J :: c.j) of commands c.j. Notice
that the axioms 1(5), 1(6), 1(13) and 1(16) are summarized in the statement that
wp and wlp are homomorphisms.
When we use arbitrary homomorphisms instead of wp and wlp we allow more
freedom of interpretation of commands. We do not want to allow too much freedom,
however. There are usually some commands the interpretation of which is supposed
to be known. We therefore introduce a set S of commands the interpretation of
which will be fixed. The elements of S are called simple commands.
In the examples we always assume that all guards and all assignments are
simple commands. In actual programming, for instance with abstract data types,
there is usually a layer of procedures with fixed interpretation, upon which a layer of
application oriented procedures can be build. In that case the lower level procedures
can also be regarded as simple commands. In other words, in applications of the
theory, one can choose a convenient set of simple commands. The introduction of
the set 5 was needed for the following two definitions.
The set WP is defined as the set of homomorphisms w with
(20) 
w.s = wp.s 
for all s Â£ S.
The set WLP is defined as the set of homomorphisms w with
(21) 
w.s = wlp.s 
for all s Â£ S.
With respect to the syntax, we asssume that every simple command is atomic
in the sense that it is not a sequential composition or a nondeterminate choice of
other commands (it may be a procedure name). We write 5 0 to denote the set
of commands that can be obtained from S by repeated application of composition
and choice. Since wp and wlp are homomorphisms, it follows from (19) that the
equalities of (20) and (21) can be extended to the elements of 5 0 : if c Â£ S 0 then
(22) 
(Vw e WP :: w.c = wp.c) ,
(\/wÂ£ WLP :: w.c = wlp.c) .
We come back to these syntactic issues in Chapter 4.
2.7 Induction rules
The first induction rule is Hoare's proof rule for the conditional correctness of

46 
ANNOTATION, RECURSION AND REPETITION
recursive procedures, cf. [Hoare 1971] and [Manna-Vuillemin 1972]. In Hoare's
logic, it looks like
\~ P {body.h} q
p{h}q
where h is the derivability symbol: 4 A h 2?' is pronounced as 'B can be derived from
A\ Therefore, by definition (1), Hoare's Rule means that, if [p=> wlp.(body.h).q]
can be derived from [p=> wlp.h.q] then [p=$- wlp.h.q].
Since we do not construct a separate logic, but only develop a mathematical
theory, the derivability symbol h is to be eliminated. This is done by quantification
over all interpretation functions w G WLP. In this way the above rule leads to
Induction Rule. Assume that for every w E WLP
[p=>w.h.q] => [p=> w.(body.h).q] .
Then [p=> wlp.h.q}.
Actually, Hoare's rule is stronger than this one, for it allows implicit quan-
tification over free variables (parameters to the procedure as well as specification
constants in the pre- and postconditions). These free variables can be incorporated
in the same way as in Theorem (16). Thus we arrive at the following theorem.
(23) Hoare's Induction Rule. Assume that for every w G WLP
(24) 
(V* :: [p.i => w.(h.i).(q.i)])
=> 
(Vi :: [p.i=>w.(body.(h.i)).(q.i)]) 
.
Then [p.i=> wlp.(h.i).(q.i)] for all i.
Remark. This rule is proved in Section 4.9 below. It is based on the definition of wlp
as the weakest solution of equation (15). Just as in Theorem (16), the antecedent
of (24) is called the induction hypothesis. (End of remark)
Example. We give an example where Hoare's Induction Rule is used to prove
conditional correctness. In this example, total correctness fails, so that Theorem
(16) cannot be used.
Let v be an integer program variable. Let procedure h be declared by
(25) 
body./i 
= 
(skip | v := v + 2; h\ v := v - 1) .
Operationally, it is clear that h need not terminate, but that if h terminates then
v is not smaller than it was before. We therefore guess that
[ v > i => wlp.h.(v > i) ] 
for all i E 7L.

2.7 INDUCTION RULES 
47
This is proved by means of Hoare's rule in the following way. We let i range over
ZÂ£, choose all h.i = h, and the predicates p.i and q.i equal to v > i. We have to
prove the proper instantiation of (24). So, we let w G WLP be a function that
satisfies the induction hypothesis
[v>z 
=> w.h.(v>i)] 
foralHe2Z.
Now it suffices to observe
w.(body.h).(v > i)
= 
{declaration (25) of h}
w.(skip | v := v + 2; h\ v := v â l).(v > z)
= 
{w is a homomorphism}
w.skip.(v > i) 
A w.(v := v + 2).(w.h.(w.(v := v â l).(v > i)))
= 
{(21), 1(10) and 1(25)}
v > i 
A w.(v := v + 2).(w.h.(v> i + 1))
4= 
{induction hypothesis with i := i + 1
and monotony of w.(v := v + 2)}
v>i 
A w.(v := v + 2).(v > i + 1)
= 
{(21) and 1(25)}
v>i 
A v + 2 > i + 1
= 
{calculus}
v > z .
(End of example)
As far as I know, the second induction rule is new. It deals with necessity
of preconditions instead of sufficiency. In fact, when dealing with correctness, we
are only interested in the question whether a given predicate implies the weakest
(liberal) precondition. In program transformation or in proofs of incorrectness,
we can also be interested in the necessity of certain preconditions. Necessity of
preconditions is usually shown by means of scenarios. Since scenarios require careful
operational reasoning, we prefer a formal instrument like the following Necessity
Rule for wp.
(26) Necessity Rule. Assume that for every w Â£ WP
(Vz :: [w.(h.i).(q.i) 
=> p.i])
=> 
(Vz :: [w.(body.(h.i)).(q.i) 
=> p.i]) .
Then [wp.(h.i).(q.i) 
=$> p.i] for all z.
The rule is proved in Section 4.9. It is based on the postulate that wp is the

48 
ANNOTATION, 
RECURSION AND 
REPETITION
strongest solution of (14). The rule is not useful for proofs of program correctness.
It can be used, however, for proofs of totality, proofs of incorrectness, and proofs
of (in-)equivalence of commands.
It is not useful to imagine an operational interpretation of the rule. The spe-
cialization of the rule to the repetition is given below in Theorem (30)(b). An
operational interpretation in that special case is offered after the proof of Theorem
(30).
Example. We use rule (26) to prove that, for every initial state, procedure ft of
declaration (25) need not terminate. This is formalized in wp.h.true = false, or
equivalently [ wp.h.true => false]. By Rule (26), it suffices to prove that for every
weWP
[w.h.true=> false] => [ w.(body.h).true =$> false] ,
or equivalently
[ -^w.h.true] 
=> [-^w.(body.h).true ].
Therefore, it suffices to use the induction hypothesis [ -*w.ft.true] and to observe
-^w.(body.h).true
= 
{declaration (25) of ft}
->w.(skip ] v := v + 2; ft; v := v â l).true
= 
{w is a homomorphism}
âiw.skip.true V -iiy.(v := v + 2).(w.h.(w.(v := v â l).true))
= 
{(21), 1(10) and 1(25)}
-it/;.(v := v + 2).(w.ft.true)
= 
{induction hypothesis}
-ltx;.(v := v + 2).false
= 
{(21) and 1(25)}
true .
This proves that wp.h.true = false. Notice that, since false implies true and wp.ft
is monotone, it also follows that wp.h.false = false, so that ft is total. (End of
example)
2.8 The repetition
We can now treat the repetition in terms of recursion. Let c be a command and let
b be a predicate. The repetition L = (while b do c od) is defined as the recursive
procedure L with

2.8 THE REPETITION 
49
(27) 
body.L 
= 
(?-i6||?6;c;L).
The main proof rule for the repetition is
(28) Theorem. Let p be a predicate and vf G 2Z a state function such that
(29) 
(Vra â¬ 2Z :: {p A 6 A vf < ra} c {p A vf < m A m > 0} ) .
Then it holds {p} L {p A -*b} .
Remark. Here m is a specification constant. For negative m, the postcondition of
(29) is faise. So, if condition (29) holds and m < â 1, the precondition of (29) is
false or command c is not total. In programming methodology one may prefer to
require the conjunction
[pAb 
=> vf > 0] A
(Vra E IN :: {p A 6 A vf < ra} c {p A vf < m} ) .
For nontotal commands our version is stronger. The main advantage, however, is
that Theorem (28) leads to the simplest possible annotation rule, see below. In
Section 3.5 below, a third version is discussed. (End of remark)
Proof. The theorem is proved by application of Theorem (16) with a singleton set
/ and the instantiations p.i := p, q.i := p A -ift and h.i := L. It suffices to observe
that, for every integer ra,
{p A vf < ra} body.L {p A ~^b}
= 
{(27) and (4)}
{pAvf 
< ra} ?-.& {p A -.6} 
A {p A vf < ra} ?6; c; L {p A --6}
= 
{eliminating guards}
{p A vf < m A b} c; L {p A ~*b}
4= {(3) and (29)}
{p A vf < m A m > 0} L {p A --6} .
(End of proof)
Theorem (29) leads to the following annotation rule:
6. Repetition Rule. The correctness of {p} L {p A ~^b} can be proved in the
annotation
{p}
while b do 
{vf = â¢ â¢ â¢ }
{p A b A vf < ra}
c 
{p A vf < m A m > 0}
od 
{p A -16} .

50 
ANNOTATION, RECURSION AND 
REPETITION
Theorem (29) is used for proving total correctness of repetitions. There are
easier rules for proving conditional correctness and necessity.
When arguing about command L, we may assume that the meaning of com-
mand c, the body of L, is given. In other words, we may assume that c E 5 0 ,
the set of commands obtained from simple commands by composition and choice.
Recall that the set 5 of simple commands may also contain lower level procedures,
see Section 2.6.
(30) Theorem. Let repetition L be given by declaration (27) with c Â£ S 0. Let p
be a predicate.
(a) Let [pAb => wlp.c.p). Then [p => wlp.L.(pA-*b)] .
(b) Let [6 A wp.c.p => p]. Then [wp.L.(bV p) =Â» p] .
Proof, (a) We apply Hoare's Induction Rule (23) with the same instantiations as
in the proof of Theorem (28). In fact, by (23), it suffices to consider an element
w e WLP with
[p => w.L.(pA-*b)] ,
and to prove
[p =>- w.(hody.L).(p 
A -â¢&)] .
This is done in the following calculation
[p => w.(hody.L).(p 
A -Â»&)]
= 
{(27)}
[p^w.(?-^bl?b;c;L).(pA^b)}
= 
{w is a homomorphism, (21), 1(8)}
[p => (-.6 => p A -16) A (6 => u;.c.(u;.i.(p A 16))) ]
= 
{predicate calculus}
[pAb =$> w.c.(w.L.(pA-*b))]
<= {w.c is monotone}
[pAb => it;.c.p] A [p =^ w;.L.(p A-Â«6)]
= 
{[pAi => wJp.c.p], c G 5 0 and (22)}
[p =$> w.L.(p A ~"6)] .
(b) Now we use Necessity Rule (26). It suffices to observe that, for every
we WP,
[w.(body.L).(bVp) 
=> p]
= 
{(27)}
[u;.(?-.6||?6;c;L).(6Vp) 
=Â» p]
= 
{w is a homomorphism, (20), 1(8)}

2.8 THE REPETITION 
51
V p) A(b^w.c.(w.L.(bV 
p))) 
=> p]
= 
{predicate calculus, a number of steps}
[bAw.c.(w.L.(bVp)) 
=Â» p]
<= 
{w.c is monotone}
[w.L.(bVp) 
=> p] 
A 
[bAw.c.p 
=> p]
= 
{[ b A wp.c.p => p],ce 
SÂ® and (22)}
[w.I.(6Vp) =* p] .
(End of proof)
Remark. Rule (30)(a) is the well-known rule that if predicate p is an invariant, i.e.,
[p A b=> wlp.c.p], and if p holds initially, and if the repetition terminates, then it
terminates in a state that satisfies p A ->&.
Rule (30)(b) seems to be new. Operationally, it can be understood as follows.
Assume that [ b A wp.c.p =>p]. Let initially -Â«p hold. We claim that there is an
execution sequence that keeps -ip valid. In fact, we have [ b A -ip=> -iwp.c.p].
Therefore, whenever b A -Â»p holds, there is an execution of c that does not establish
p. Repeating this argument, we arrive at an execution sequence of L that either
does not terminate or terminates in a state where -ip holds and also ->6, that is
-i(6 V p). This shows that
[b A wp.c.p =r* p] 
=>- [-ip=^->wp.i.(p V 6)] .
This suffices. (End of remark)
Remark. In [Nelson 1989], a more general repetition N = (do c od) is defined by
body.iV 
= 
(c ; N H skip) ,
where operator 4^' is defined by
q ^ r 
= 
q [] ?(wp.q.false)] 
r.
In repetition TV, command c is repeatedly executed. The repetition terminates when
c 'fails'. Our framework does not allow operator 4f^' in such recursive declarations.
The operator is unnecessary and has complicated monotony properties. For similar
reasons, we do not admit Nelson's ifâfl constructor, which could be defined by
if q fi 
= 
q ^ abort .
(End of remark)
The next result is a nice characterization of totality of repetitions.
Theorem. Repetition L is a total command if and only if (! b; c) is total.
Proof. By definition 1(12), command s is total if and only if wp.s.false = false.
Therefore, one implication is proved in

52 
ANNOTATION, 
RECURSION AND 
REPETITION
wp.L. false 
= 
false
= 
{(27)}
wp.(?--6 | ?6; c; Z).false 
= 
false
= 
{1(8), 1(13), 1(16)}
(-â¢ft => false) A (6 =>â¢ wp.(c; L).false) 
= 
false
= 
{calculus and 1(13)}
b A wp.c.(wp.L.faise) 
= 
false
=$> {[ false => wp.L.false]; wp.c monotone; contrapositive}
b A wp.c.false 
= 
faise
= 
{1(8), 1(13)}
wp.(!6;c).false 
= 
false.
By the last step, the other implication follows from
b A wp.c.false 
â false
=Â» 
{(30)(b) with p := faise}
[ wp.L.b=> faise]
=^ 
{wp.L is monotone}
wp.L.false 
= 
false .
(End of proof)
2.9 Exercises
Exercises of Section 2.1.
Exercise 0. Prove the weakening rules (2) and the choice rule (4).
Exercises of Section 2.2.
Exercise 0. Prove the annotation rule for the conditional statement.
Exercise 1. Let k be an integer program variable. For all i E 2Z, let p.i be the
predicate
p.i 
= 
k > 0 A (k = 2 â¢ i + 1 V k = - 2 â¢ i) .
Determine commands c and d that satisfy
{k = i} 
c {p.i} ,
{p.i} 
d 
{k = i} ,
for all i E 2Z. Give proofs by annotation. Notice that commands c and d must not
depend on i.

2.9 EXERCISES 
53
Exercises of Section 2.3.
Exercise 0. <? Given are integer program variables t and w. Procedure h is
declared by
proc h (x : integer ; var y : integer) ;
( x := x +1
; t :=y
; y :=x ) .
(a) Give a correct and sharp specification of procedure h and prove the correctness.
(b) Give an invocation of h that satisfies for all values of specification constant X
the Hoare triple
{w = X} 
/*(?,?) 
{w = 0 A t = X} ,
and prove the correctness by means of the specification given in part (a) and invo-
cation rule (10).
Exercise 1. Let j and k be program variables of type item. Procedure h is specified
by
proc h (x : item; var y : item)
{ext j ! ; all U,V e item ::
pre x = U A j = V, post j = U A y = V} .
(a) Give an invocation of h that satisfies, for all values of 5 and T,
{ j = 5 A k = T } 
/ Â» ( ? , ? ) 
{ j = T A k = 
S } ,
and prove its correctness.
(b) Give an implementation of h and prove its correctness.
Exercise 2. Given are integer program variables u and z. Procedure p is specified
by
proc p (x : integer ; var y : integer )
{ext u! ; all U, Y G integer ::
pre x + y = U A n â Y , post u = U A y = 5^} .
(a) Give an invocation of p that satisfies, for every value of specification constant
x,
and prove the correctness.
(b) Give an implementation of p and prove that it satisfies the specification.

54 
ANNOTATION, 
RECURSION AND 
REPETITION
Exercises of Section 2.4.
Exercise 0. Given are an integer program variable x and a variable array
a : array [0 . . 9] of 0 . . 9 .
Function / is defined for natural numbers n by
/(0) = 0 ,
f(n) = 10 â¢ f(n div 10) + a[n mod 10] 
for n > 0 .
The question is to implement procedure cd specified by
proc cd
{ext x!, a ;
all X G integer :: pre x > 0 A /(x) = X , post x = X} ,
and to prove the correctness.
Exercise 1. The function g(n) of natural numbers n satisfies
9(0) = 0 ,
g[n) = n â g(g(n â 1)) for all n > 0 .
Give an implementation of procedure makeg specified by
proc makeg (x : integer)
{ ext w! : integer ; all X E integer ::
pre P : x = X > 0 , post Q : w = g(X) A 0 < w < X} ,
and prove its correctness.
Exercises of Section 2.5.
Exercise 0. Let v be an integer program variable. Let procedure h be given by
body./* = (?(v = 0); v := 1 || ?(v ^ 0); v := v - 1; h; v := 2 â¢ v).
Prove that (Vi : i > 0 : {v - t} ft {v = 22'} ).
Exercise 1. The simple euclidean algorithm determines the greatest common
divisor (gcd) of integer program variables x and y in the following way. It is a
procedure h specified by
{x > 0 A y > 0 A Z = gcd.x.y}
h 
{x = ZAy 
= Z}
for all specification constants Z. Prove that h can be implemented by means of
body.ft 
=
(?(x = y)
fl ?(x > y) ; x := x - y ; h
D?(y>x); y : = y - x ; h) .

2.9 EXERCISES 
55
Exercise 2. The extended euclidean algorithm. Modify the algorithm of the
previous exercise in such a way that the greatest common divisor of x and y is
expressed as an integral linear combination of the initial values of x and y. There
are four integer program variables x, y, s, t. Procedure h is specified by
{x = l A y = F}
h 
{s-X 
-t-Y 
= gcd.X.Y 
A 0 < s < F A 0 < t < X }
for all positive specification constants X and Y. Give an implementation of h and
prove its correctness.
Exercise 3. Let k be an integer program variable. Let procedure h be declared by
body./i 
= 
k := k + 1 ; ( ?(k > 0) | ?(k < 0) ; h ; h) .
Find a function / Â£ 7L â> 7L such that for all specification values t
{/.k = t) 
h {k = t} .
Exercise 4. Every rational number X with 0 < X < 1 can be written as an
alternating sum of inverses of positive integers. This result, a variation of a theorem
of Leonardo di Pisa, can be proved as follows. The alternating sum of inverses asi.s
of sequence s is defined recursively by
asi.e = 0 ,
asi.(j; s) = j â asi.s 
for integer j ^ 0.
Here, e is the empty sequence, the operator ';' stands for concatenation and the
integer j is identified with the singleton sequence. Let q be a rational program
variable and let s be a program variable for sequences of integers. The algorithm
is specified by
{q = X} 
g {X = asi.s}
for every rational number X with 0 < X < 1. Prove that g is implemented by
body.g 
=
( ?(q = 0) ; s := e
I ?(q # 0) ; ( | j :: ? ( ^ < q < j) ; q := ) - q ; g ; s := (j; s))) ,
where j ranges over the positive integers.
Hint: use the numerator of q as a variant function.
Exercise 5. Prove that the sequence s generated by procedure g in the previous
exercise is increasing.
Exercises of Section 2.7.
Exercise 0. Use Hoare's rule to prove that procedure h given by
body./i 
= 
(v := v + 1 J h ; h)
satisfies [ v = i 
=$- wlp.h.(v > i)]. Hint: weaken the precondition.

56 
ANNOTATION, 
RECURSION AND 
REPETITION
Exercise 1. Let body.h = (h;h). Prove that h does not terminate and is total.
Do not use operational arguments.
Exercise 2. Let i be an integer program variable. Let procedure h be declared by
body./i 
= 
(i := 0 ; h | ?(i ^ 0) ; c)
for some command c. Prove that for every predicate p
wp.h.p 
= false ,
wlp.h.p 
= (i = 0 V wlp.c.p) .
Exercise 3. Let procedure h be declared by
body./i 
= 
(skip [ v := v + 1 ; h ; v := v â 2 ; h) .
Prove that [ v < i 
=$Â» wlp.h.(v < i)] for all integer values i.
Exercise 4. Let procedure h be declared by
body./* = 
(?(v = 7) | ?(v + 7) ; v := v - 1 ; h) .
(a) Show that [ wp.h.(v = 7) => v > 7].
(b) Show that the conjunct m > 0 in the precondition of the induction hypothesis
of Theorem (16) cannot be omitted. For this purpose, show that, when m > 0 is
omitted, the rule can be used to prove [ true => wp.h.(v = 7)], thus contradicting
the result of part (a).
Exercises of Section 2.8.
Exercise 0. Prove that repetition L of (27) satisfies
(a) 
[wip.X.(^)],
(b) 
[wp.L.b => b] .
Exercise 1. Prove that every repetition L = while b do c od with command
c total satisfies [-iwp.L.6].
Exercise 2. Let i be an integer program variable. Consider the repetition
L : 
while i ^ 0 do (i := i - 1 fl i := i - 2) od .
(a) Prove that
[wlp.L.(i^O) 
= 
( i < 0 ) ] .
Use Theorem (30) and the previous exercise.
(b) Determine the predicates wp.L.(Â± ^ 0), wlp.L.(i = 0), wp.i.(i = 0). Guess
the results first, and then give formal proofs.
Exercise 3. Prove that the repetition
L : 
while i > 1 do (i := i - 2 | i := i - 3) od
satisfies wp.Z.(i < 0 ) = i < 2 A i ^ l .

2.9 EXERCISES 
57
Exercise 4. Prove that the repetition
L : 
while v / 0 do
?(v < 0) I ?(v < 0); v := 0 | ?(v > 0); v := v - 1 od
satisfies wp.L.true 
= (v > 0) .
Exercise 5. <s? Let L be given by formula (27). Let p be a predicate. Let vf Â£ 7LX
be a state function such that
[b => p V vf > 0] ,
(Vra E7L ::[bA wlp.c.(p V vf > n) 
=> pV vf > n]) .
Prove that [ wlp.L.(pV b) => p] . Hint: use postulate (15).
Exercise 6. In this exercise we compare repetition L as given by formula (27) with
what can be called the angelic repetition L0 given by
L0: 
( Qn:n>O:(?6;c) n;?-i6) .
We show that L0 is wip-equivalent to L, but not wp-equivalent.
Let p be a predicate.
(a) Prove that for all n > 0
[wlp.L.p^wlp.(?b;c)n.(bVp)] 
,
and similarly for wp.
(b) Prove that [ wlp.L.p=> wlp.LO.p], and similarly for wp.
(c) Give an example with c = skip to show that for wp the implication need not be
an equivalence.
(d) Use Theorem 2(30)(a) with p := wlp.LO.p to prove that [ wlp.L0.p=> wlp.L.p].

CHAPTER 3
HEALTHINESS LAWS
3.0. We come back to the informal description of wp and wlp given in Section
1.2. This description is used to justify two more postulates concerning wp and wlp,
the so-called healthiness laws. These postulates are due to [Dijkstra 1976]). They
are theorems of the standard relational semantics, but in predicate-transformation
semantics they need not be imposed. In fact, recently, some investigators (cf. [Back-
von Wright 1989b], [Morgan-Gardiner 1990]) have proposed specification constructs
that lead to violations of the laws (so these constructs cannot be expressed in
relational semantics). Command serve from the second example in 1.2 belongs to
this category.
In the remainder of this book the healthiness laws are imposed since they form
the natural boundary of the theory of Chapter 4. Another reason for imposing
them is that they hold for all practical imperative languages and for the relational
model of computation (see Chapter 6).
In this chapter, we introduce the laws with an informal justification and we
treat the main formal implications.
3.1 Conjunctivity properties of predicate transformers
Since the healthiness laws prescribe certain properties of the predicate transform-
ers wp.c and wlp.c for commands c, it is useful to introduce these properties for
arbitrary predicate transformers.
A predicate transformer / G IP âÂ» IP is called finitely conjunctive if and only
if for all predicates p, q E IP
(0) 
f.(pAq) 
= 
f-pAf.q.
It is called universally conjunctive if and only if, for every set of predicates 1 7 c P ,
(1) 
f.(VpeU::p) 
= 
(VpeU::f.p).
Notice that formula (1) with U = 0 implies

3.2 TWO LAWS 
59
(2) 
f.true = true.
Predicate transformer / is called positively conjunctive if and only if formula (1)
holds for every nonempty subset U of IP.
It is well-known that every finitely conjunctive predicate transformer is mono-
tone, cf. [Dijkstra-Scholten 1990]. In fact, by definition 1(3), it suffices to verify
that for a finitely conjunctive / and predicates p and q:
[f-P 
=Â» f-Q]
= 
{a rule of predicate calculus, see exercise 1.1.0(a)}
[f.p = 
f.pAf.q]
= 
{/is finitely conjunctive}
[f.p = 
f.(pAq)]
<= {/ is a function}
[p = pAq]
= 
{the rule of exercise 1.1.0(a)}
[p 
=> q] â¢
It is clear that every universally conjunctive predicate transformer is positively
conjunctive and that every positively conjunctive one is finitely conjunctive. We
refer to [Dijkstra-Scholten 1990] Chapter 6 for a more detailed investigation of
properties of predicate transformers.
3.2 Two laws
Let c be a command and p a predicate. According to the informal description,
predicate wp.c.p holds in a state x if and only if every execution of c starting in x
terminates in a state where p holds. Consequently, wp.c.true holds in x if and only
if every execution starting in x terminates. On the other hand, wlp.c.p holds in x if
and only if every execution starting in x does not terminate or terminates in a state
where p holds. This argument yields the first healthiness law (see [Dijkstra-Scholten
1990] Chapter 7, formula (2)):
(3) 
wp.c.p 
â wp.c.true A wlp.c.p 
{termination law} .
Formula (3) can be interpreted as saying that correctness is the conjunction of
termination and conditional correctness.
Now let U be a set of predicates. Predicate (Vp E U :: wlp.c.p) holds in a
state x if and only if, for every predicate p Â£ U, every execution of c starting in x
does not terminate or terminates in a state where p holds. This is equivalent to the
condition that every execution of c starting in x does not terminate or terminates in

60 
HEALTHINESS 
LAWS
a state where all predicates p G U hold, that is to condition wip.c.(Vp Â£ U :: p). By
definition (1), this justifies the other healthiness law (see [Dijkstra-Scholten 1990]
Chapter 7, formula (0)):
(4) 
wlp.c is universally conjunctive.
These laws are precisely the conditions needed for a good correspondence be-
tween predicate-transformation semantics as considered here and relational seman-
tics. This is proved in Section 6.4 below.
Let us now consider, for a command c, the predicates wlp.c.true and wlp.c.false.
By formula (2), law (4) implies
(5) 
wlp.c.true 
= 
true.
For a state #, proposition wlp.c.false.x is interpreted to mean that every com-
putation starting at x does not terminate. Therefore, -^wlp.c.false.x says that the
initial state x has a terminating computation. After quantification over all states,
we arrive at the following definition. Command c is said to be potentially terminat-
ing if and only if [ -^wlp.c.false].
One might expect that whenever a command necessarily terminates (see 1(11)),
it also potentially terminates. It turns out that this is only true under assumption
of totality. More precisely, we have
command c is total 
= 
[ wp.c.true => -^wlp.c.false] .
This is proved in
[wp.c.true => -iwlp.c.false]
= 
{predicate calculus}
[ -Â»(wp.c.true A wlp.c.false)]
= 
{(3)}
[ -iwp.c.faise]
= 
{1(12)}
c is total .
3.3 Some important implications
In this section, we use the healthiness laws (3) and (4) to derive some important
results. The first observation is that for any nonempty set U of predicates and any
command c
wp.c.(\/u Â£ U :: u)
= 
{(3), (4)}
wp.c.true A (Vw G U :: wlp.c.u)

3.3 SOME IMPORTANT IMPLICATIONS 
61
= 
{U nonempty}
(Vw G U :: wp.c.true A wlp.c.u)
= 
{(3)}
(Vu Â£ U :: wp.c.u).
This proves
(6) 
wp.c is positively conjunctive.
The postulates 1(4), 1(5) and 1(6) of Section 1.2 follow from the healthiness
laws (3) and (4). In fact, it is clear that law 1(4) follows from termination law
(3). On the other hand, by the results of Section 3.1, the facts (4) and (6) imply
monotony of wp.c and wlp.c as postulated in 1(5) and 1(6).
The healthiness laws imply that the choice operator '[' distributes over com-
position in the following strong sense. For any pair of nonempty sets C, D of
commands, we have
(7) 
(QceC::c);(Q </â¬Â£>::</) 
= 
( | c G C, d G D :: c;d) .
This distributivity law is proved as follows. We let c and d range over the sets C
and D, respectively. For any predicate p, we have, with wg ranging over wp and
wlp,
wg.((%c::c);(ld::d)).p
= 
{1(13)}
wg.( I c :: c).(wg.( | d :: d).p)
= 
{1(16) twice}
(Vc :: wg.c.iy d :: wg.d.p))
= 
{(4) for wip; for wp: (6) and D is nonempty}
(Vc,d:: wg.c.(wg.d.p))
= 
{1(13), 1(16)}
wg.( \c,d:: 
c;d).p.
By 1(7), this concludes the proof of (7).
Remark. Formula (7) is one of the reasons for not allowing a choice of the empty
set. It is tempting to define the choice of the empty set by ( J c G 0 :: c) = miracle.
Then distributivity (cf. (7)) would fail for empty D, since
abort] ( Q d G 0 :: c?) = 
abort , whereas
( | d G 0 :: abort; of) = 
miracle .
This anomaly does not completely justify our decision to forbid the empty choice.
The main reason for the decision is that to admit the empty choice here would
introduce awkward anomalies in Chapter 4, especially in Sections 4.3, 4.7 and 4.8.
(End of remark)

62 
HEALTHINESS 
LAWS
3.4 Guards, assertions, assignments
Guards and assertions satisfy the healthiness laws. We provide some of the proofs.
For a predicate 6, the assertion ! b satisfies (3) because of
wp.(\b).p 
= 
wp.(! 6).true A wlp.(! b).p
= 
{1(8)1
bAp 
= 
(6 A true) A (b=>p)
= 
{calculus}
true.
The proof of (3) for ?b is even simpler, and therefore omitted.
In order to verify law (4) with ?ft and ! b substituted for c, it suffices to note
that the predicate transformer (b=>) given by (b=>).p = (6=^p) is universally
conjunctive because of the distributive law
Every assignment v := f also satisfies the healthiness laws. In fact, the termi-
nation law (3) is proved by observing that for any predicate p
wp.(v := f).p 
= wp.(v := f).true A wlp.(v := f).p
= 
{1(24)}
po(t)f-/) 
= 
(true o (v <â /)) A(po(v<- /))
= 
{true o (v <- f) = true by 1(23); true A q = q for q â¬ P}
true .
The universal conjunctivity of wlp.(v := /) is proved by observing that for any set
U of predicates and any state x
wlp.(v := /).(Vp eU :: p).x
= 
{1(24)}
(VpeU::pU(v*-f).x)
= 
{1(1) with x:=(v*- 
f).x}
(Vpe 17 ::*.((Â«Â«-/).*))
= 
(1(24)}
(Vp 6 U :: w]p.(u := /).p.x)
= 
{1(1)}
(Vp e ^ : : wip.(Â« := f).p).x .

3.5 THE TERMINATION LAW AND REPETITIONS 
63
3.5 The termination law and repetitions
In our main proof rule for the repetition, Theorem 2(28), invariance and termina-
tion are closely coupled in one Hoare triple. Usually these aspects are distinguished
into three separate proof obligations. We combined these aspects, since this com-
bination allows the integration of a complete correctness proof of a repetition into
the annotated program text. An unexpected asset is that rule 2(28) is independent
of the healthiness laws, as opposed to the usual proof rule. We now present a proof
rule in which three proof obligations are separated.
Let c be a command and let 6 be a predicate. Let L = (while b do c od),
cf. Section 2.8.
Y"
(8) Theorem. Let p be a predicate and vf G 7L a state function such that
(a) 
[pAb 
=Â» 
v f > 0 ] ,
(b) 
{pAb}c{p} 
,
(c) 
(Vra 6 IN :: {p A b A vf < m} c {vf < ra} ) .
Then it holds {p} L {p A -.&} .
Proof. By Theorem 2(28), it suffices to observe that for all integers m < 0:
wp.c.(p A vf < ra A m > 0)
<= {trivially}
false
= 
{m < 0 and condition (a)}
p A b A vf < m ,
and that for all integers m > 0:
wp.c.(p A vf < m A m > 0)
wp.c.p A wp.c.(vf < m)
4= 
{ra > 0 and conditions (b) and (c)}
(p A b) A (p A b A vf < ra)
= 
{calculus}
p A b A vf < ra .
(End of proof)
Remark. In Theorem (8), condition (c) can be replaced by
(c') 
(Vra â¬ IN :: {p A b A vf = ra} c {vf < ra} ) .
Formally, this is a weaker proof obligation, but we prefer condition (c) since it seems
to be a better guide of the intuition than (c'). (End of remark)

Gi 
HEALTHINESS 
LAWS
3.6 Exercises
Exercises of Section 3.2.
Exercise 0. Let / be a positively conjunctive predicate transformer. Prove that
command c specified by wp.c = / and wlp.c.p = (f.true=>f.p) for all p E IP
satisfies the healthiness laws.
Exercises of Section 3.3.
Exercise 0. Prove that a command c is necessarily terminating if and only if
wp.c = wlp.c.
Exercise 1. Prove that for any command c and predicates p, q E IP
wp.c.(p A q) = wp.c.pA wlp.c.q.
Exercise 2. Use the previous exercise to prove that, for every predicate p, a
command c is total if and only if
[wp.c.p 
=>> -iwip.c.(->p)] .
Exercise 3. Let c be a command. Let (i El:: p.i) and (i El:: q.i) be families of
predicates with {p.i} c {q.i} for all i E I. Prove that, if / is nonempty,
{ ( V i e / : : p.*)} 
c {(Vi E I:: q.i)} .
Exercises of Section 3.5.
Exercise 0. 9? Let L be given by formula 2(27).
(a) Show that [ wp.(\b; c).false => wp.L.false],
(b) Let i be an integer program variable. Prove that repetition L given by
L : 
while true do ?(i ^ 0); i := i - 1 od
satisfies {i > 0} L {false} .
(c) Use the repetition of (b) to prove that the implication in (a) need not be an
equivalence.
Exercise 1. Let there be an integer program variable i. Let L be given by
L : while i > 0 do ( J n E IN :: ?(n < i); i := n) od .
Prove that L is total and satisfies [ wp.L.(i < 0)].
Exercise 2. In this exercise we explore some consequences of not postulating
law (4). Let j be an integer program variable. Let command c be specified by
wlp.c = wp.c and
wp.c.p = pV wp.(j := j â l).p for all p ETP.
(a) Prove that c satisfies the laws (3), (5), 1(4), 1(5) and 1(6). Use the conjunction
of j = 0 and j = 1 to show that c violates law (4).

3.5 THE TERMINATION LAW AND REPETITIONS 
65
(b) Consider repetition
L : while j > 0 do c od .
Use Theorem 2(30) to prove [ wp.L.(j > 0) => false] .
(c) Prove wp.L.false = false.
(d) Show that Theorem (8) is not applicable to repetition X, in the sense that
application of Theorem (8) with vf = j and p = (j > 0) leads to a contradiction
with part (c).

CHAPTER 4
SEMANTICS OF RECURSION
4.0. This chapter is devoted to the formal definition of the semantics of sequential
composition, unbounded choice and recursion and to the proofs of the properties of
commands that were introduced and used in the previous chapters. The semantics
of the simple commands is taken for granted, but otherwise the reader should not
rely on old knowledge but only use facts that have already been justified in the new
setting. At the end of the chapter, the foundations of the previous chapters will be
complete.
Some examples of the theory are given in the exercises at the end of the chapter.
The text of the chapter has almost no examples. One reason is that the chapters
1, 2 and 3 may be regarded as examples of the theory. On the other hand, every
nontrivial example tends to constitute additional theory.
In Section 4.1, we introduce complete lattices and investigate the lattice of
the predicate transformers and some important subsets. Section 4.2 contains our
version of the theorem of Knaster-Tarski. A syntactic formalism for commands
with unbounded choice is introduced in Section 4.3.
Section 4.4 contains the main definition. From the definition on simple com-
mands, the functions wp and wlp are extended to procedure names and command
expressions. In Sections 4.5 and 4.6, the healthiness laws, which are postulated for
simple commands, are extended to procedure names and command expressions.
The operators for command expressions (';' and '[') are introduced in Section
4.7. In Section 4.8, we prove that the functions wp and wlp treat these operators
in the way postulated in Chapter 1. Section 4.9 contains the formal treatment of
the induction rules for recursive procedures postulated in Section 2.7.
4.1 Complete lattices and predicate transformers
Recall that an ordered set is a pair (W, <), such that W is a set and '<' is an order
(i.e. a reflexive, transitive, antisymmetric relation) on set W. An element x G W

4.1 COMPLETE LATTICES AND PREDICATE TRANSFORMERS 
67
is called a supremum (least upper bound) of a subset U of W if and only if for all
we W
(0) 
x < w = 
(VueU 
::u<w) 
,
i.e. x < w if and only if w is an upper bound of U. If it exists, the supremum of U
in W is unique. We use the quantifier notation
x = (supu G U :: u)
or the shorter notation # = (sup U) to indicate that x is the supremum of U (so
that, in particular, it exists).
Remark. Usually, the supremum x of U is defined by the condition
(Vw e U :: iz < x) A
(Vw 6 W : (Vu eU ::u < w) : x < w) .
It is easy to verify that our definition is equivalent to the usual one. We prefer our
definition, since it is shorter, and more convenient in many proofs. (End of remark)
An element x is called an infimum (greatest lower bound, notation x = (inf u G
U :: u) or x = (inf U)) of U if and only if for all w G W
(1) 
w<x 
= 
(\/ueU 
::w < u) .
The ordered set (W, <) is called a complete lattice if and only if every subset has a
supremum and an infimum.
A subset V of a complete lattice W is called sup-closed if and only if (sup U) G
V for every subset U of V. Similarly, V is called inf-closed if and only if (inf U) G V
for every subset U of V.
An important way to construct complete lattices out of smaller ones is as
follows. For any set F, the set of functions WY is equipped with the induced order
'<' given by
(2) 
f<g 
= 
(VyeY::f.y<g.y).
It is well-known and easy to verify that this defines an order on WY. 
It is also
well-known and easy to prove (e.g. see [Hesselink 1990] Theorem 3(1)), that, if
W is complete, WY is complete and infima and suprema in WY can be calculated
pointwise:
(3) Theorem. Let (W, <) be a complete lattice. Then WY is a complete lattice
and for any subset U of WY,
, 
(supJ7).y = (sup/ â¬ U :: f.y) .

68 
SEMANTICS 
OF RECURSION
We now specialize to lattices of boolean valued functions. The set IB of the
booleans is ordered by false < true. One can easily show that (IB, <) is a complete
lattice. By Theorem (3), it follows that the set P of the predicates on the state
space X with the induced order < is a complete lattice. By (2) and 1(2), we have
(4) 
p < q = 
[ p = > q ] ,
so that the induced order on IP is the strength order of Section 1.1. Notice that we
now use the globalized interpretation of '<' in the sense of Section 1.6.
For any set U of predicates we have
(5)
In order to show the power of definition (0), we provide the straightforward proof
of the first formula of (5):
= 
{(0)}
(Vr e P : : (3p G U :: p) < r = (Vp G U :: p < r))
= 
{(4)}
(VrGlP:: [ (3p e U :: p) => r] = (Vp e U :: [p=>r]))
= 
{exercise 1.1.4(b)}
true .
The proof of the second formula is analogous.
By a second application of Theorem (3), we see that the set P â> IP of the
predicate transformers with the induced order is a complete lattice. This set is used
so often that we introduce the notation PT â (IP âÂ» IP). It follows from (3) and
(5) that for any subset U of PT and any predicate p Â£ P
(6)
We define MT to be the subset of PT that consists of the monotone predicate
transformers. We write MC, MP, MU to denote the sets of finitely conjunctive
predicate transformers, of positively conjunctive ones, of universally conjunctive
ones, respectively. By Section 3.1, we have the inclusions
MU C MP C MC C MT C PT .
(7) Theorem, (a) The set MT is sup-closed and inf-closed in PT.
(b) MT is a complete lattice in its own right and every subset of MT has the same
supremum and infimum in MT as in PT.
(c) The sets MU, MP, MC are inf-closed in PT.
(d) For f,g,he 
MT with / < g, we have / o h < g o h and ho f < ho g.

4.1 COMPLETE LATTICES AND PREDICATE TRANSFORMERS 
69
Proof, (a) We prove that MT is sup-closed in PT by showing for every subset U
of MT that the supremum of U in PT satisfies (supf7) E MT, i.e. that (sup 17) is
monotone (cf. 1(3)): for every p, q E IP, we have
[ (sup C/).p^ (sup [/).?]
[(3feU::f.p) 
=* ( 3 / 6
= {predicate calculus}
(Vfâ¬U::[f.p=>f.q])
= {U C MT}
A completely analogous argument shows that MT is inf-closed in PT.
(b) This follows from (a).
(c) We prove that MU is inf-closed by showing that for any subset U of MU
function (inf U) is universally conjunctive. In fact, for any subset P of P we have
(inft/).(Vpâ¬P::p) = (Vp G P :: (inf U).p)
= {(6)}
(yfeU::f.(yPeP::p)) 
= (Vp G P :: (V / e J7 :: /.p))
= 
{Â£/ c MU", 1(5), interchange of quantifications}
true.
To show that MP and MC are inf-closed, one uses the same calculation with certain
conditions on P.
(d) It suffices to observe that for any p G P :
(/ o h).p < (g o h).p A 
(hof).p<(hog).p
= {(4)}
[f.(h.p)=>g.(h.p)} A 
[h.(f.p)*h.(g.p)]
4= 
{generalization (using q := h.p and q := p), and 1(3)}
( V ^ P : : [ / . ^ M D A heMT
= 
{assumptions}
true .
(End of proof)
Remark. If the state space X has more than one element the sets MU, MP, MC
are not sup-closed in PT, see exercise 4.1.3. This is the reason for using MT in the
theory, even though it is our purpose to construct wp and wlp in such a way that
wp.c is positively conjunctive and wlp.c is uniformly conjunctive. We do not use
the set PT itself, since the composition in PT is not monotone (if X has at least
one element). In fact, if h E PT is given by h.p = ->p, then composition with h as

70
first argument reverses the order:
f<9 
=> 
hof>hog.
(End of remark)
SEMANTICS OF RECURSION
4.2 Fixpoints in complete lattices
Let W be a complete lattice. A fixpoint of a function D G W â> W is defined to
be an element v G W with D.v = v. If v is a fixpoint of D and v < w for every
fixpoint u> of Â£), then t; is a Zeastf fixpoint of I?; clearly, there is at most one least
fixpoint. Similarly, if v is a fixpoint of D and v > w for every fixpoint w of D, then
v is the greatest fixpoint of D; it is also unique. The existence of least and greatest
fixpoints is contained in the theorem of Knaster-Tarski, cf. [Tarski 1955], We shall
mainly use the following variation of this theorem:
(8) Theorem. Let W be a complete lattice and let D G W â> W be a monotone
function. Then D has a least fixpoint wa and a greatest fixpoint wb. Let V be a
subset of W that is D-invariant, i.e. (Vv G V :: D.v G V).
(a) If V is sup-closed then wa G V.
(b) If V is inf-closed then wb G V.
Remark. Notice that if V is sup-closed or inf-closed, it contains the supremum or
infimum of the empty set, and is therefore nonempty. (End of remark)
Proof. By symmetry, it suffices to prove that D has a least fixpoint wa and that
wa G V for every Â£)-invariant sup-closed subset V of W. Let a ^-invariant sup-
closed subset V be given. We construct wa as
(9) 
wa = (sup U)
where U is the subset of W given by
(10) 
u eU 
= u eV 
A (Via : Â£>.w < w : u < w) .
Since U C V and V is sup-closed in W, we have
(11) 
waGV.
The next step is to verify
(12) 
wa G U
= 
{(10), (11)}
(Vu> : .D.u; < w : wa < w)
= 
{(9)and(0)}
(Vu; : D.it) < w : (Vu â¬ U :: u < w))

4.3 A SYNTAX FOR COMMANDS WITH UNBOUNDED CHOICE 
71
= 
{(10)}
true .
We then observe that
(13) 
D.wa < wa
Â«= 
{(9)}
D.wa Â£ U
= 
{(10)}
D.wa G V A (Vw : D.w < w : D.wa < w)
<= {y is D-invariant, transitivity of <}
wa Â£ V A (\/w : D.w < w : D.wa < D.w)
4= {D is monotone}
wa E V A (Vw : D.w < w : wa < w)
= 
{(10) and (12)}
true .
On the other hand, we have
(14) 
wa<D.wa
<= {(10) with u := wa and w := D.wa}
waEU 
A D.(D.wa) < D.wa
= 
{(12); monotony of D and (13)}
true .
It follows from (13) and (14) that wa is a fixpoint of D. From (10) and (12), it
follows that wa < w for every fixpoint w of D. Therefore, wa is the least fixpoint
of D in W. (End of proof)
Remark. In Section 13.1, we give a stronger version of this theorem with a short
proof based on Zorn's Lemma. (End of remark)
4.3 A syntax for commands with unbounded choice
In Chapter 2, we described the body of a procedure as a command expression that
may contain some procedure names. Now we must be more formal. We shall define
a command as a nonempty set of strings of elementary commands, i.e. a nonempty
language over some possibly infinite alphabet.
Let A be a set of symbols that does not contain the symbols e and ';'â¢ We
write A* to denote the set of strings over A. The empty string is denoted by e.
Catenation of strings is denoted by means of the infix operator ';'â¢ We write

72 
SEMANTICS 
OF 
RECURSION
to denote the set of nonempty subsets of A* (nonempty languages over A). 
The
elements of A 0 are called 
commands.
The set A is regarded as a subset of A* by identifying a symbol a with the cor-
responding singleton string. The set A* is regarded as a subset of A 0 by identifying
a string s with the singleton set {s}. 
An element a E A is called an 
elementary
command. A string s E A* is interpreted as a sequential composition of its terms.
An element r E AÂ® is interpreted as the choice of its element strings.
Example. 
Assume that A contains the elementary commands ?b and j := 0 and h.
Then (j := 0; h) E A* and {?&, (j := 0; h)} E A 0 . Elsewhere the latter command
is written (?6 Q j := 0; h). This notation is introduced formally below in Section
4.7. (End of example)
Recall that PT = ( P -> P ) . For a function v E A -Â» P T from A to the set of
the predicate transformers, the string extension v* E A* ââ¢ P T and the language
extension t>0 E A 0 âÂ» PT are defined by
(15) 
v*.e = identity 
E P -> P ,
v *.(a; 5) = u.a o v*.s for a 6 A and sGA* ,
vÂ®.r = (inf 5 Gr:: v*.,s) for r E A 0 .
By formula (6), it follows that for any predicate p E P
(16) 
t>0.r.p = (Vs E r : : v*.s.p) .
It is clear that function t> is the restriction of v* to the subset A, and that v* is the
restriction of v 0 to the subset A*.
For later reference, we state the following result.
(17) Lemma, (a) Let V be an inf-closed subset of PT which contains the identity
function and is closed under composition. Then i?0 E A 0 ââ¢> V for every function
v Â£A-*V.
(b) The subsets MT, MC, MP, MP of PT satisfy the conditions of part (a) on V.
Proof, (a) Let v E A â> V'. Since V contains the identity function and is closed
under composition, we have by induction v*.s E V for all s E A*. Since V is
inf-closed, it follows that v 0.r G V for all r E A 0.
(b) The sets listed are inf-closed by Theorem (7). It is easy to verify that they
contain the identity function and are closed under composition. (End of proof)

4.4 THE INTERPRETATION 
OF RECURSION 
73
4.4 The interpretation of recursion
This section contains the main definition of the book. We assume that wp and wlp
are given on simple commands and we construct wp and wlp for all commands in
such a way that the postulates 2(14) and 2(15) are satisfied. The homomorphism
properties of wp and wlp are proved later, in Section 4.8. Since we want to treat wp
and wlp in a unified way, whenever possible, we introduce a parameter e E {0,1}
with e = 0 for wp and e = 1 for wlp.
We assume that the set A is the disjoint union of sets 5 and H. Here, S is
the set of simple commands introduced in Section 2.6. The functions wp and wlp
on simple commands, as given in Chapter 1, are renamed to wso E S â> MT and
wsi E S â> MT, respectively. We assume that 5 contains at least all guards (see
1(8)) and all assignments (see 1(24)) that we might need.
The elements of H are called procedure names. As suggested in 2.5, we assume
that the procedures are declared by means of a given function
body E H -> A 0 .
In view of 2(14) and 2(15) the aim is now to construct functions wp, wlp E A 0 â>
MT such that
(18) 
wp\S = WSQ , wlp\S = wsi ,
and that for all h E H and wg E {wp, wlp}
(19) 
wg.h = wg.(hody.h) .
Remark. It is possible to introduce the simplification wso = wsi, or equivalently: all
simple commands are everywhere terminating (see exercise 3.3.0). This simplifica-
tion would eliminate many case distinctions, but the functions wp and wlp remain
different since nonterminating recursive procedures do occur. The simplification
would make the book easier to read, but would preclude applications with simple
commands that are not everywhere terminating. For example, the applicability of
Theorem 2(30) would be drastically reduced. (End of remark)
The functions wp and wlp E A 0 â> MT are constructed in two steps. For
e E {0,1} , we first extend wse to a function in A â> MT. Since A is the disjoint
union S U H, the extension of wse to A is of the form wse U v where v is some
function v E H â> MT. Before choosing v, we describe the second step. For
any v E H â> MT, the extension wse U v E A â> MT has a language extension
(wse U v) 0, cf. definition (15). By Lemma (17), this language extension satisfies
(wse U v)e

74 
SEMANTICS 
OF 
RECURSION
We construct the extensions wp and wlp in the form (wse U v)Â® .
In view of formula (19), we observe that
(20) 
(VheH 
:: (wse U v)Â®.h = (wse U v)0.(body./i))
= 
{(wse U v)0.ft = v.h for all ft, from (15)}
(VheH 
:: v.ft = (wse U u)0.(body.ft))
= 
{equality of functions, definition (21) below}
v = De.v
where function Â£>e G MT H -> MT H is defined by
(21) 
De.v = (wse U v)e o body .
By calculation (20), formula (19) can be established by extending wse by means of
fixpoints of function De.
Since MT is a complete lattice, it follows from Theorem (3) that MT 
with
the induced order is a complete lattice.
(22) Theorem. Function De G MTH â> MTH is monotone and has a least fixpoint
wae and a greatest fixpoint wbe in MT 
.
Proof. By completeness of MT 
and the Theorem of Knaster-Tarski (8), it suffices
to prove that De is monotone. This is proved by verifying that for v, w G MTH
De.v<De.w
= 
{induced order, definition (21)}
(V/i eH :: (wse U v)0.(body./i) < (wse U w)0.(body./i))
^= 
{generalization r for body./i; definition (15)}
(Vr G A 0 :: (inf s e r :: (wse U v)*.s) < (inf 5 Gr:: (wse U w)*.s))
<= {if 5 G r and r G A 0 then 5 G A*; monotony of inf}
(Vs G A* :: (wse U v)*.s < (wse U w)*.s)
4= 
{remaining proof obligation (23)}
v < w .
It remains to justify the last step by proving
(23) 
v < w 
=> (Vs G A* ::(wseUv)*.s 
<(wseUw)*.s) 
.
This is done by induction on the lengths of the strings s. In the base case, the
string is empty and we have
(wse U v)*.e = identity = (wse U u>)*.Â£ .
The induction step is taken by observing that for any a G A and s G A*
(wse U u)*.(a; 5) < (wse U w)*.(a; s)
= 
{definition (15)}
(wse U v).a o (wse U v)*.s < (wse U w).a o (wse U t/;)*.6

4.5 HEALTHINESS LAWS: THE UNIVERSAL CONJUNCTIVITY OF WLP 
75
4= {from (7)(d): composition is monotone in MT}
(wse U v).a < (wse U w).a 
A (wse U v)*.s < (wse U w)*.s
<= {induction hypothesis}
(wse U v).a < (wse U w).a 
A v < w
= 
{A is the disjoint union of S and H}
((a G S A wse.a < wse.a) V (a Â£ H A v.a < w.a)) 
A v < w
<= {definition of order in MTH}
v < w .
(End of proof)
Using wae and wbe as introduced in Theorem (22), we define the functions wp
and wlp by
(24) 
wp = (WSQ U wao)0 
, wlp 
= 
(wsi U wbi) 0 .
So, wp is the extension of WSQ by the least fixpoint of Do and wjp is the extension
of wsi by the greatest fixpoint of D\.
By calculation (20), the extended functions wp and wlp satisfy requirement
(19), i.e., the postulates 2(14) and 2(15). In Section 7.5 below, we show that these
definitions are in agreement with the operational semantics.
For reference below, we introduce for any v G MTH and e G {0,1}
(25) 
ve 
= 
(wse U v)Â® 
eAe^MT.
It then follows that
(26) 
De.v = ve o body 
, wp = (wao)Â° 
, wlp =
4.5 Healthiness laws: the universal conjunctivity of wlp
To be consistent with Chapter 3, we have to prove that the functions wp, wlp G
A 0 -> MT satisfy the healthiness laws 3(3) and 3(4). We treat rule 3(4) in this
section and rule 3(3) in the next one. Some intermediate results are isolated and
stressed because of their roles in later chapters.
We begin with law 3(4): the universal conjunctivity of wlp. Since the seman-
tics of simple commands is given, we postulate universal conjunctivity for these
commands, i.e.
(27) 
wSl G S -> MU .
By formula (18) this condition represents rule 3(4) for s G S.
We shall use Theorem (8)(b) to prove that function wlp has values in MU
as well. Now wlp = (wbi)1 where wbi is the greatest fixpoint of D1 in 
MTH.

76 
SEMANTICS 
OF 
RECURSION
Theorem (8)(b) is applied to this action of D\. Since MU is inf-closed in MT by-
Theorem (7)(c), an easy argument with Theorem (3) shows that
(28) 
MUH is inf-closed in MTH .
In order to prove that MU 
is D\-invariant, 
we observe, for any v E MT 
,
{(21)}
U v)e o body E MUH
{calculus}
U v)e eAÂ®
{Lemma (17)}
u)GA-> MU
<= {A = SUH; (27)}
v e MUH .
This proves that
(29) 
(Vv E MU"H :: ^ . u E
Notice that, by (25), the last part of this calculation also proves
(30) 
(Vv E MUH :: t;1 E A 0 -> MC7) .
By Theorem (8)(b), it follows from (28) and (29), that the greatest fixpoint
wbx of Dx in MTH 
is element of MU H. Since wlp = (wbi)1 by formula (26), it
now follows from (30) that condition (27) extends to the healthiness law
(31) Theorem, wlp E A 0 -> MU.
4.6 The termination law
The next aim is to prove that all elements of A 0 satisfy termination law 3(3). This
can only be proved if we postulate that the simple commands satisfy this law. We
therefore postulate
(32) 
(Vs E 5,p E IP :: wso.s.p = wso.s.true A wsi.s.p) .
The termination law gives a relation between the functions wp and wlp. The
functions wp and wip are defined as extensions of the extreme fixpoints of De.
Extreme fixpoints are characterized by their approximations. In order to prove
relation 3(3), there are therefore three roads of attack. One can try to approximate
the pair (wp, wlp), or to approximate wp while keeping wlp fixed, or to approximate
wlp while keeping wp fixed. We use the second road, since the first road is more
complicated and the third road leads astray.

4.6 THE TERMINATION LAW 
77
More specifically, we fix function wlp and define a set WT of functions that
satisfy a relation analogous to rule 3(3). We then use Theorem (8)(a) to show that
the least fixpoint wa-o of Do is element of this set WT. This leads to a proof of rule
3(3).
The set WT is defined as the subset of MT 
given by
(33) 
v G WT 
= 
(V/i G # , p G P :: v.h.p = v.h.true A wlp.h.p) .
Using abbreviation (25), we claim that
(34) 
(Vv Â£ WT,r G A0,p G F :: vÂ°.r.p = vÂ°.r.true A wlp.r.p) .
To prove (34), let v G WT be given. Let K be the subset of A 0 given by
r G K 
= (Vp G IP :: vÂ°.r.p = vÂ°.r.true A wip.r.p) .
It suffices to prove that K = A 0, or equivalently A 0 C K. This is done by induction
over the structure of A 0.
It follows from the definitions (25) and (24) that the restriction of vÂ° to S is
wso and that the restriction of wlp to 5 is wsi. By postulate (32), this implies that
5 C K. It follows from (33) that H C K. Therefore, we have A C K.
The inclusion A* C K is proved by induction on the lengths of the strings
s G A*. The base case is
eeK
= 
{definition K}
(Vp :: vÂ°.e.p = vÂ°.e.true A wlp.e.p)
= 
{(25), (24), (15)}
(Vp :: p = true A p)
= 
{calculus}
true .
The induction step is taken by observing that for every a â¬ A, every s G A* f) K
and every p G IP
vÂ°.(a; s).true A wip.(a;5).p
= 
{(25), (24), (15)}
vÂ°.a.(vÂ°.s.true) A
vÂ°.a.true A wip.a.(vÂ°.6.true) A wlp.a.(wlp.s.p)
= 
{wlp.a G MU from (31)}
t;0.a.true A wlp.a.(vÂ°.s.true A wlp.s.p)
vÂ°.a.true A wlp.a.(vÂ°.s.p)

78 
SEMANTICS 
OF RECURSION
vÂ°.a.(vÂ°.s.p)
= 
{(25), (15)}
vÂ°.(a;s).p ,
so that (a; s) 6 K. By induction this proves that A* C K. In order to show that
AÂ® C K we observe that for every r E AÂ® and every p E IP
vÂ° .r.true A wlp.r.p
= 
{(25), (24), (15)}
(Vs Â£ r :: vÂ°.s.true) A (Vs Â£ r :: wlp.s.p)
= 
{calculus}
(\/s E r :: i;Â°.s.true A wip.s.p)
(Vs E r :: vÂ°.5.p)
= 
{(25), (15)}
vÂ° .r.p ,
so that r E A". This concludes the proof of formula (34).
The next aim is to prove that WT is Do-invariant:
(35) 
(Vv E WT :: Â£>0.t; G WT) .
This is proved by observing that for any v E MTH
D0.v E WT
= 
{(33)}
(V/i E if,p E P :: D0.v.h.p = D0.v.h.true A wlp.h.p)
= 
{(26)}
(Vft E ^ p E IP :: vÂ°.(body.fe).p = vÂ°.(body.ft).true A wlp.h.p)
<= {(19) for wlp}
(Vfe E i?,p E P :: vÂ°.(body./i).p = vÂ°.(body./i).true A wJp.(body.fe).p)
<= {(34) with r := body./i}
v E WT .
In order to show that WT is sup-closed, we first observe that for any subset U of
MTH, any he H and any p E P
(36) 
(supC/)./i.p
= 
{Theorem (3)}
(sup?; E Â£7 :: v.h).p
= 
{Theorem (7)(b) and formula (6)}
(3v E U :: v./i.p) ,
and therefore
(sup U).h.p = (sup U).h.true A wlp.h.p

4.7 THE SYNTACTIC ALGEBRA 
79
= 
{(36)}
(3v E U :: v.h.p) = (3v E U :: v.h.true) A wlp.h.p
= 
{distributivity}
(3v E J7 :: v.h.p) = (3v EU :: v.h.true A wlp.h.p)
<= {equals for equals}
(Vv Â£ U :: v.h.p = v.h.true A wlp.h.p)
<= {(33)}
[ / C W T .
This proves
(37) 
WT is sup-closed in MTH .
By Theorem (8)(a), it follows from (35) and (37) that the least fixpoint wao of Do
in MTH is element of WT. Since wp = (wao)0, it follows with (34) that
(38) Theorem (termination law 3(3)). For all r E AÂ® and p E F, we have
wp.r.p = wp.r.true A wlp.r.p .
4.7 The syntactic algebra
Up to this point, AÂ® is not more than the set of nonempty languages over A =
S U H. In order to use expressions in AÂ®, we need operators for composition and
(unbounded) choice.
We use the infix operator ' ] ' and the quantifier '['to denote nonempty unions
of elements of AÂ®. In other words, q Q r = q U r for commands q, r E AÂ®. For a
family of commands (i E / :: q.i) we have
(39) 
(||t::Â«.i) = (U *::Â«â¢*)â¢
We define the infix operator ';' for composition in AÂ® by
(40) 
sEq;r 
= 
(3t E q,u E r :: s = i;u) .
Here, the lefthand semicolon is the new operator in AÂ® and the righthand semicolon
is the catenation operator in A*. If q and r are singleton sets, so that they can
be identified with elements of A*, then (q;r) is the singleton set consisting of the
catenation of these elements. This proves that the operator ';' on AÂ® is a genuine
extension of the operator ';' on the subset A* of AÂ®.
We use 5Â® to denote the subset of AÂ® that consists of the nonempty sets of
strings over 5. In view of (39) and (40), this definition is in accordance with the
provisional definition in Section 2.6.

80 
SEMANTICS 
OF RECURSION
For every pair of nonempty subsets C, D of A 0 we have
(41) 
( h e C : : g ) ; ( I r G i ) : : r ) = ( I ^ C , r G J 5 : : ? ; r ) 
{distributivity} .
This is proved by observing that for any string s
sÂ£{\qeC::q)-{\reD::r)
= 
{(40)}
(3t Â£ (U â¬ C '-'â¢ Q)> u Â£ (h 
Â£ D :: r) :: s =t;u)
= 
{(39) and calculus}
( 3 q e C , r E D : : ( 3 t E q , u E r : : s = t ; u ) )
= 
{(39), (40) and calculus}
4.8 The semantic homomorphisms
Now that we have operators ';' and '[' on A 0, we must verify the postulates 1(13)
and 1(16) of Section 1.4. Actually, it is useful to prove a more general result.
Recall from Section 2.6 that a function w G A 0 â> MT is called a homomor-
phism if and only if it satisfies 2(19). By formula (6) this amounts to the conditions
(V q, r G A 0 :: w.(q; r) = w.q o w.r) ,
( V C : 0 / C c A 0 : : m c E C : : c ) = (inf c G C :: w.c)) .
The main result of this section is
(42) Theorem. If v G A â> MP then vG G A0 â> MP is a homomorphism.
Proof. It follows from Lemma (17) that ve G A0 -> MP. The first condition of
2(19) is verified by observing that for commands q, r Â£ A0 and a predicate p G IP,
we have
vÂ°.(?;r).p
- 
{(16)}
(Vs G (<z;r) :: t>*.,s.p)
= 
{(40)}
(Vt e q,u er :: v*.(*;u).p)
= 
{exercise 4.3.2(a)}
(VtGg,wGr 
::v*.t.(v*.u.p))
= 
{v*.t is positively conjunctive by Lemma (17); r ^ 0 ; 3(1)}
(Vt G ? :: u*.t.(Vu G r :: w*.u.p))
= 
{(16) twice}

4.8 THE SEMANTIC HOMOMORPHISMS 
81
The second condition of 2(19) is verified by observing that, for any nonempty subset
C of A 0, we have
t ; 0 . ( I c G C : : c )
= 
{(39), (15), calculus}
(inf s e A* : (3 c G C :: s G c) : v*.,s)
= 
{calculus, see exercise 4.1.4}
(inf c G C :: (inf s e c :: v*.s))
(inf cG C :: vQ.c) .
(End of proof)
(43) Corollary, 
(a) For every v G MZ7 , function v1 is a homomorphism in
A 0 ââ¢ MU". In particular, wlp is a homomorphism in A 0 â* MT7.
(b) For every v G WT, function t;0 is a homomorphism in A 0 â> MP. In particular,
wp is a homomorphism in A 0 âÂ» MP.
Proof, (a) Since wsi G S â> MJ7, we have wsi U i; G A ââ¢ MU. Lemma (17) then
yields v1 = (ws1 U v)G G A 0 -> MC7. Since MU C MP, Theorem (42) implies that
v1 is a homomorphism. The assertion concerning wlp follows from wlp = (wbi)1.
(b) It follows from (31), (34) and the calculation used in the proof of 3(6) that
vÂ°.r is positively conjunctive for every r E A 0. This proves that vÂ° G A 0 â> MP.
Since vÂ° = (wsoUv)0, it follows that wsoUv Â£ A ^ MP. Now Theorem (42) implies
that vÂ° is a homomorphism. As for wp, it remains to observe that wp = (wao)Â°
and that wao G WT. (End of proof)
Remark. 
This corollary shows that the formalism of this chapter satisfies the
fundamental axioms 1(13) and 1(16). The validity of (43) relies on the health-
iness laws, represented here by the postulates (27) and (32). 
The validity of
wp.(q;r) â wp.q o wp.r in combination with the simple definition (40) of ';' is
the real reason that we did not admit the empty choice in Section 1.4 and the
empty element in A 0, cf. Section 4.3.
Notice that the homomorphism property is proved in the end, whereas, in
[Hesselink 1990], wp and wlp are defined as fixpoints in the set of homomorphisms.
This important shift was forced upon us by the fact that the set MP is not sup-
closed in MT (see exercise 4.1.3) and therefore not suited for direct application in
Theorem (8)(a). Actually, even in the situation of loc.cit., where only finite choice
is considered, the present order of presentation would be more convenient. (End of
remark)

82 
SEMANTICS 
OF RECURSION
4.9 The induction rules
We can now prove the two induction rules of Section 2.7. Hoare's rule is very
important for programming methodology. The necessity rule is occasionally useful
for proving totality of a procedure or for proving that a procedure does not satisfy
a specification.
Recall that the sets WLP and WP have been defined in Section 2.6. Hoare's
Induction Rule 2(23) reads
(44) Theorem. Let (i E / :: h.i) be a family of procedure names. Let (i El:: 
p.i)
and (i E I :: q.i) be families of predicates such that for all w E WLP
(Vi :: [p.i=> w.(h.i).(q.i)])
=Â» 
(Vi :: [p.i => w.(body.(h.i)).(q.i)]) 
.
Then [p.i=> wlp.(h.i).(q.i)] for all indices i.
Proof. We begin with massaging our goal
= 
{h.i e H and wlp\H = wbi}
(V*::[p.t=Â»Wrb1 .(/Â».Â»).(?â¢*)])
= 
{definition VI below}
whi E VI
4= 
{wbi is greatest fixpoint of D\\ Theorem (8)(b)}
VI is D\ -invariant and inf-closed in MTH ,
where VI is defined as the subset of MU 
of the functions v such that
(Vz :: [p.i=>v.(h.i).(q.i)]) 
.
Recall from (29) and (28) that MUH is JDi-invariant and inf-closed in MTH. 
The
set VI is D\-invariant 
since for any v E MUH
DLV E VI
= 
{definition of VI and (26); (29)}
(Vi :: [p.t=^v1.(body.(fc.O).(?.0])
<= {assumption of theorem; v1 E WLP by (43)(a)}
= 
{definition VI and v1]!! = v}
v E VI .
The set VI is inf-closed in MTH, since MUH is inf-closed and for any subset U of
VI and any index i

4.10 CONCLUSION 
83
= 
{(3) and (6)}
= 
{predicate calculus}
(Vve U ::[p.i^v.(h.i).(q.i)])
= 
{definition VI and U C VI}
true .
(End of proof)
Recall that the Necessity Rule 2(26) amounts to the following.
(45) Theorem. Assume that for every w Â£ WP
(Vi :: [w.(h.i).(q.i) 
=> P-i])
=Â» 
(Vi :: [w.(body.(h.i)).(q.i) 
=* p.i]) .
Then [wp.(h.i).(q.i) =Â» p.i] for all i.
Proof. The proof is analogous to the proof of (44). The main difference is that
wp = (wao)0 where wao is the least fixpoint of function Z?o- Therefore, we use
Theorem (8)(a) and a sup-closed subset of WT. (End of proof)
Remark. A closer look at the proof reveals that Theorem (44) can be strength-
ened by weakening the assumption as follows. The set WLP can be replaced by
the smaller set of the homomorphisms w Â£ AÂ® ââ¢> MU with w.e = identity and
(w\S) = wsi. Similarly, in Theorem (45), the set WP can be replaced by the set of
homomorphisms w Â£ A 0 â* MP with w.e = identity and (iy|5) = WSQ and
(Vr Â£ A 0,p Â£ P :: w.r.p = w.r.true A wip.r.p) .
(End of remark)
4,10 Conclusion
We have now justified all postulates of Chapters 1, 2 and 3, including the induc-
tion rules of Section 2.7 and the results in 2.8 based upon them. The theory is
parametrized by the state space X, which induces the set of monotone predicate
transformers, the set S of simple commands, the set H of procedure names, the
functions wso and wsi subject to the postulates (27) and (32), and the declaration
function body. The fundamental definitions are (15), (24), (39) and (40).
The formalism is somewhat more specific than necessary. For example, in the
distributivity law (41), we have syntactic equality whereas 3(8) only gives semantic
equality.

84 
SEMANTICS 
OF RECURSION
4.11 Exercises
Exercises of Section 4.1.
Exercise 0. Let (W, <) be an ordered set. Let x and y both be suprema of the
subset U of W. Prove that x = y.
Exercise 1. Prove that definition (0) is equivalent to the alternative suggested in
the first remark of 4.1.
Exercise 2. Prove that x G W is the supremum of the empty set in W if and only
if it is the smallest element of W. What about the infimum of the empty set?
Exercise 3. Let v be an integer program variable. Use the set containing wp.(v :=
0) and wp.(v := 1) to show that MU, MP and MC are not sup-closed in PT.
Construct a similar example for a boolean variable.
Exercise 4. <? Let Qbea set of subsets of a complete lattice W. Prove that
(inf w G W : (3U G Q :: w G U) : w) = (inf U G Q :: (inf U)) .
Exercise 5. Assume that every subset of W has a infimum. Prove that (W, <) is
a complete lattice, i.e. prove that every subset has an supremum. Hint: construct
(sup U) as the infimum of the upper bounds of U.
Exercises of Section 4.2.
Exercise 0. The simplest version of the theorem of Knaster-Tarski. Let W be a,
complete lattice. Let D G W â> W be a monotone function. Give a direct proof of
the following result:
Theorem, (a) The element (inf w G W : D.w < w : w) is a fixpoint of D and,
hence, the least fixpoint.
(b) The element (sup w G W : w < D.w : w) is a fixpoint of D and, hence, the
greatest fixpoint.
Exercise 1. Let W be a complete lattice. Let D, D1 G W â> W be monotone
functions with D < D1', cf. formula (2). Let a and a1 be the least fixpoints of D
and D', respectively.
(a) Prove that a < a!.
(b) Let V be a subset of W, which is D'-invariant and closed under suprema of
nonempty subsets. Assume that a â¬ V. Prove that af G V. (Hint: adapt the
proof of Theorem (8).)

4.11 EXERCISES 
85
Exercises of Section 4.3.
Exercise 0. Let w G A 0 â> MT be a homomorphism (see Section 2.6). Assume
w.e = identity. Prove that w = yÂ® for v = (w|A).
Exercise 1. Let B b e a subset of A. Identify BÂ® as a subset of A 0. Let i; and
w G A -Â» PT be such that v|5 = w|J?. Prove that v 0,w 0 e AÂ® -+ PT satisfy
v 0 | Â£ 0 = w 0 | Â£ 0 .
Exercise 2. Let v G A -> PT.
(a) Prove that v*.(i; u) = u*.to v*.w for all t, u G A*.
(b) Prove that t>0.(r U s).p = vÂ®.r.p A v0.5.p for all r, 5G A 0 and all p G IP.
Exercises of Section 4.4.
Exercise 0. Let the state space be spanned by one integer program variable i.
Let H = {h} with declaration
body./i = (i := i + 1 J h) .
Let wO, wl, w2 G MTH be defined by
wO.h.p = wp.(i := i + l).p ,
^l./i.p = (Vj :j>i: 
wp.(i := j).p) ,
w2.h.p = false .
Prove that u>0, u;l, u;2 are fixpoints of Do, that wO is the greatest fixpoint, and
that w2 is the least fixpoint.
Exercise 1. Let H = {h} with declaration body./i = (h J e). Determine wp.h
and wlp.h.
Exercise 2. For n G IN let iiT.n be the subset of H defined recursively by K.O = 0
and
h e K.(n + 1) = 
body./i E (5 U K.n)0 .
Prove that wae.h = wbe.h for all h G (U n :: -K"-^)-
Exercise 3. Prove that for v, w; G MT H and e G {0,1}
v < ^ 
=> (Vr G A 0 :: ve.r < we.r) .
Exercises of Section 4.5.
Exercise 0. Show that MUH is inf-closed in MTH\ cf. (28).
Exercises of Section 4.6.
Exercise 0. Verify that wb\ G WT (this should be very easy).

86 
SEMANTICS 
OF RECURSION
Exercise 1. Let v G MTH and w G MCH be such that
(V/i G H,p G IP :: v.h.p = v.h.true A w.h.p) .
Prove that, for e G {0,1} ,
(Vr G ^. 0,p G IP :: ve.r.p = ve.r.true A we.r.p) .
Does the proof generalize to the case where the equal signs '=' are replaced by '>'?
Exercise 2. Prove that wp.h.p = wp.h.true A v./i.p for every fixpoint v of De in
Exercises of Section 4.9.
Exercise 0. <? Let p G P. Let C be a subset of A such that
( V s G S n C : : [p=>wlp.s.p]) 
and
(VheHDC 
:: body./i G C 0) ,
where C 0 is the subset of A 0 that consists of the nonempty sets of strings over C.
Prove that [p^ wlp.c.p] for all c G C 0.
Exercise 1. Give a complete proof of the version of Theorem (45) suggested in
the remark.

CHAPTER 5
RAMIFICATIONS
5.0. In this chapter we present a number of more or less isolated extensions of the
fundamental concepts. They broaden the view but have no high priority. We do
not need the theory of Chapter 4.
In Section 5.1 we give our version of refinement of commands. Refinement is
a very important concept in programming methodology. In this book it plays a
less prominent role. It occurs in some exercises and it comes again to the fore in
Chapter 12. Section 5.2 contains an example where a refinement between procedures
is proved by means of the induction rules of Section 2.7.
In Section 5.3 we introduce the calculational method of insertion of guards.
This method can be regarded as an alternative to annotation. It is especially useful
for proofs of semantic equality. In Section 5.4 this method is used to handle a
complicated example that is needed in Chapter 12.
Section 5.5 contains a discussion of strongest postconditions.
In Section 5.6 we prepare the ground for an extension of the termination ar-
gument used in Theorem 2(16). The harvest is reaped in Section 5.7, where we
present a generalization of Theorem 2(16) and a Necessity Rule for wlp.
5.1 Refinement and relative refinement
The function of a compiler is to transform programs written in some high-level
programming language, say Pascal, into machine instructions. This transformation
has many different aspects: names are replaced by machine addresses, values are
replaced by bit strings, the flow of command is represented by jumps and branches,
comments and types are removed, etc.
We focus on one aspect of the transformation, namely that it should preserve
the meaning of the program. For this purpose, we have introduced semantic equal-
ity, cf. 1(7). Actually, the compiler need not preserve the semantics completely. It

88 
RAMIFICATIONS
may reduce the amount of nondeterminacy. It may, for instance, replace command
( x : = 0 ] x : = l ) b y ( x : = l ) . In that case, we speak of refinement or implementa-
tion.
We say that command c is refined by command d (notation c [I d) if and only
if command d satisfies all Hoare triples of c:
(0) 
c C d =
(Vp,Â«::({p}c{?} 
=>{p}d{q})
A (p{c}q^p {d} q)) .
We claim that
(1) 
c C d = 
(Vg :: [wp.c.g =^ wp.d.q] A [wlp.c.g =>> wip.d.g]) .
This is proved by observing that for any predicate q
(Vp:: {p}c{q} => {p} d {q} )
= 
{2(0)}
(\/p :: [p=> wp.c.g] =>â¢ [p=Â» wp.rf.g])
= 
{predicate calculus}
[wp.c.g =^ wp.rf.Qf]) ,
and a similar calculation for formula 2(1) and function wlp.
It often happens that a command d implements c only under specific circum-
stances, i.e., if a certain precondition r is satisfied. This can be formalized in the
relative refinement relation ' C r ' given by
(2) 
c E r d =
(Vp,q::({rAp}c{q} 
=> {r A p} d {q} )
A 
((rAp{c}q)=>(rAp{d}q))).
In the exercises, it is shown that both refinement relations can be expressed in
terms of ' ^ \ '[]' and';'-
Remark. Notice that every command is refined by miracle. More generally, in using
refinement one may lose totality.
Definitions (0) and (1) are debatable. We include the second conjunct since we
regard some possibly nonterminating commands as useful. Authors dealing with
refinements (cf. [Morris 1987], [Back-von Wright 1989a], [Morgan-Gardiner 1990])
often treat partial correctness as totally insufficient, and therefore omit the second
conjuncts in formulae (0) and (1). This has the effect that for instance skip Q abort
is refined by (v := v â 1).
It should also be mentioned that [Nelson 1989] writes c C d to denote the
condition
(Vg :: [wp.c.q =^ wp.d.q] A [wlp.c.q 4= wlp.d.q]) .

5.2 REFINEMENT OF PROCEDURES 
89
This order is not refinement but approximation. It can be used for a least fixpoint
definition of the semantics of recursion. (End of remark)
5.2 Refinement of procedures
In this Section, we give an example in which the induction rules of Section 2.7 are
used to prove that procedures ho and hi satisfy ho C. hi under certain assumptions
on their bodies. Let ho and hi be declared by
body.hi 
= 
(riÂ§Si;hi;ti)
for i = 0 or 1, where r2-, sa-, t% G SÂ® are commands for i: = 0 or 1, such that
ro E ri 
i 
so E si 
â¢> ^o E ^i â¢
We claim that this implies ho E ^i- This would be an easy application of the
theory of Chapter 12, but it can already be proved by means of the results of 2.7.
By Formula (1), we have to prove
[wg.ho.q 
=> wg.hi.q]
where q ranges over P and wg ranges over wp and wlp.
We first treat the case wg = wp. In this case our proof obligation fits Necessity
Rule 2(26) with / = IP and for qeJP:
h.q = h0 , 
q.q = q , p.q = wp.hi.q .
In fact, by Rule 2(26), it suffices to prove that for every w G WP
(V# E P : : [w.ho.q=> wp.hi.q]) 
{ind. hyp.}
=> ( V ^ G P :: [w.(hody.h0).q=> wp.hi.q]) -
This is proved by observing that for every q G IP
w.(hody.ho).q
= 
{declaration ho}
w.(r0 
^sO]ho]to).q
= 
{w is a homomorphism}
w.ro.q 
A 
w.so.(w.ho.(w.to.q))
= 
{2(22) and r 0,s o,t o E SÂ®}
wp.ro.q 
A 
wp.so.(w.ho.(wp.to.q))
=> 
{ind. hyp. with q := wp.to.q,
and monotony of wp.so}
wp.ro.q 
A 
wp.so.(wp.hi.(wp.to.q))
=> {monotony and data concerning ro, ri, etc.}
wp.ri.q A wp.si.(wp.hi.(wp.ti.q))
= 
{same calculation as in the first two steps}

90 
RAMIFICATIONS
wp.(body.hi).q
= 
{2(14)}
wp.hi.q .
The case wg = wlp is proved by means of Hoare's Induction Rule 2(23) and a
similar calculation.
5.3 Insertion of guards, and calculus
Guards are not only useful as elementary building blocks of conditional combina-
tions, they are equally useful as commands that can represent pre- and postcondi-
tions. The fundamental result in this direction is that for any command c and any
predicate q
(3) 
c^c-iq 
= 
[wlp.c.q].
The proof of this result is rather delicate. First, the semantic equality is expressed
by means of wp and wlp. Termination Rule 3(3) and definition 1(8) are used to
eliminate the wp-part. The remainder of the proof is by mutual implication. So,
we first observe that
c 
Â£ 
c;?q
= 
{definition 1(7) and Termination Rule 3(3)}
wp.c.true = wp.(c;?q).true 
A wlp.c = 
wlp.fatq)
= 
{1(8) and 1(13)}
wp.c.true = wp.c.(q=$> true) 
A 
wlp.c = wlp.(c;?q)
= 
{calculus}
(*) 
wlp.c = wip.(c; ?q)
=> 
{application to </; 1(8) and 1(13)}
wlp.c.q 
= wlp.c.(q=$> q)
= 
{calculus, 3(5)}
[wlp.c.q] .
For the other implication, it remains to prove that [wlp.c.q] implies formula (*).
This is proved by observing that for any predicate p
wlp.c.p = wlp.(c, ?q).p
= 
{1(8) and 1(13)}
wlp.c.p = wlp.c.(q=> p)
= 
{use [ wlp.c.q]}
wlp.c.p A wlp.c.q = wlp.c.(q=^p) A wlp.c.q
= 
{wlp.c is conjunctive by 3(4) and p A q = (q=>p) A q}

5.4 THE COMMUTATION PROBLEM 
91
true .
Formula (3) is rather abstract. In most applications we need the following version:
(4) Theorem. For any command c and predicates p and q
?p;c = ?p;c;?q 
= 
[p=>wlp.c.q] .
Proof. This formula follows from (3) by the observation that, by 1(8) and 1(13),
wlp.(?p;c).q â (p^wlp.c.q) .
(End of proof)
Remark. The importance of Theorem (4) is that it enables us to give linear proofs
of semantic equalities. We illustrate the method by means of an abstract example.
In Section 5.4 the method is used again, but there for a highly specific purpose.
(End of remark)
Example. Consider predicates p, q and r, and commands c, d and e. Assume that
[p=> wlp.c.q] and q A r = false. Then we have
?p;c;(?g;d | ?r; e)
= 
{Theorem (4) and assumption}
?p; c; ?g; (?g; d \ ?r; e)
= 
{distributivity 3(7); composition of guards}
?p;c;(?(<ZA<?);dQ?(<?Ar);e)
= 
{calculus}
?p; c; (?g; rf Q ?fa/se; e)
= 
{exercise 1.4.4}
?p;c;?ff;d
= 
{Theorem (4) and assumption}
?p; c; d .
In calculations of this form, we often combine steps where distributivity, compo-
sition of guards and exercise 1.1.4 are used. Such arguments are referred to as
'calculus with guards'. (End of example)
5.4 The commutation problem
Let s be a total command such that
(5) 
s\c 
^ 
c;s,
(6) 
s;?b 
Â£ 
?6;s A s; ?-i&

92 
RAMIFICATIONS
In this situation, one might expect that repetition L = (while b do c od), cf. 2(27),
satisfies
(7) 
s ; L Â£ L ; s ,
as we conjectured in [Hesselink 1990] Section 6.5. In exercise 11.3.0 below, we show
that formula (7) holds under a mild restriction on command s. Here we give an
example to refute the general conjecture.
This example also shows that the results of Section 5.3 can be used to handle
nasty case distinctions in a systematic way.
Example. Let v be an integer program variable. Let the commands s and c be
defined by
s = (?(v > 0) | ?(v < 0); ( I m : m > 0 : v := m)) ,
c = 
( ? ( v < 0 ) | | ? ( v < 0 ) ; v : = 0 | | ? ( v > 0 ) ; v : = v - l ) .
We first prove that formula (5) holds. To this end, we observe
c\s
= 
{definition of c; Theorem (4); distributivity}
?(v<0);s
1 ?(v<0);v:=0;?(v = 0);s
| ?(v>0);v:=v-l;?(v>0);fl
= 
{definition of s; calculus with guards; Theorem (4)}
?(v < 0); ( I m : m > 0 : v := m)
| ? ( v < 0 ) ; v : = 0
B ?(v>0);v:= v - l .
We now start from the other end:
= 
{definition of s; Theorem (4); distributivity}
?(v < 0); ( fl m : m > 0 : v := TTI; ?(v > 0); c)
fl?(v>0);c
= 
{definition of c; calculus with guards}
?(v < 0);( | m : m > 0 : v := m)
D ?(v = 0);v:=0
| ?(v > 0);v := v - 1
= 
{calculus}
?(v < 0); ( I m : m > 0 : v := m)
B ? ( v < 0 ) ; v : = 0
| ?(v>0);v:= v - l
= 
{above calculation}

5.5 STRONGEST POSTCONDITIONS 
93
c;s .
This concludes the proof of formula (5). It is clear that command s is total. We
choose guard b of repetition L as b = (v ^ 0). An easy verification shows that
formula (6) holds. It remains to refute formula (7), i.e. to show that (s; L) and
(L; s) are not semantically equivalent. To this end, we first observe that
wp.L.true 
= 
(v > 0) , see exercise 2.8.3,
wp.s.(v > 0) 
= 
true .
It follows that
wp.(s;L).true 
= 
true 
and
wp.(L; s).true 
= 
(v > 0) .
This shows that s\L ^ L\s . (End of example)
5.5 Strongest postconditions
Let c be a command and p a predicate. Let a predicate q be called a postcondition
for command c and precondition/) if and only if [p=> wlp.c.q]. We claim that there
is a strongest postcondition for c and p. If this is true, then clearly this strongest
postcondition is equivalent to the conjunction of all postconditions for c and p. This
conjunction is
(8) 
sp.c.p 
= 
(Vq e IP : [p=> wlp.c.q] : q) .
We claim that, indeed, sp.c.p is the strongest postcondition for c and p. It is clear
that sp.c.p implies every postcondition for c and p. Therefore, it suffices to observe
that sp.c.p is a postcondition for c and p:
(9) 
[ p =$> wlp.c.(sp.c.p) ]
= 
{definition (8) and law 3(4)}
[p=> (Vg â¬ IP : [p=> wlp.c.q] : wlp.c.q)]
= 
{exercise 1.1.4}
(V# E P : [p=> wlp.c.q] : [p=$> wlp.c.q])
= 
{calculus}
true .
We now observe that for every predicate q
[p=> wlp.c.q]
=> {definition (8)}
[ sp.c.p =>q]
=> 
{monotony of wlp.c]
[ wlp.c.(sp.c.p) =^ wlp.c.q]

94 
RAMIFICATIONS
=> 
{transitivity and (9)}
[p=> wlp.c.q] .
This cycle of implications proves that for all predicates p and q
(10) 
[p=> wlp.c.q] 
= 
[sp.c.p =$>q] .
In category theory, formula (10) can be expressed by saying that sp.c is the right
adjoint of wlp.c, see [Hoare 1989] p. 281.
The strongest postcondition of a sequential composition (c; d) can be deter-
mined in the following way. We observe that for all predicates p and q
[sp.(c;d).p=>q]
= 
{(10) and 1(13)}
[p =Â£> wlp.c.(wlp.d.q) 
]
= 
{(10) with q := wlp.d.q}
[ sp.c.p =$> wlp.d.q ]
= 
{(10) with p := sp.c.p}
[sp.d.(sp.c.p)=>q] .
By exercise 1.1.5, this implies that for all predicates p
sp.(c] d).p = sp.d.(sp.c.p) .
By a similar calculation, one can prove that for any nonempty set C of com-
mands and any predicate p
(11) 
sp.( Ice C ::c).p=(3c<E 
C :: sp.c.p) 
.
Remark. For the relational interpretation of the strongest postcondition we refer to
exercise 6.4.2 below. The ideas of this section were inspired by [von Wright 1990]
Chapter 6. The strongest postcondition itself goes back to [de Bakker-Meertens
1975]. (End of remark)
5.6 Termination and well-founded triples
Termination arguments for repetitions and recursive procedures are usually based
on the fact that every decreasing sequence of natural numbers terminates. Some-
times a lexical order is needed, for example in the unification algorithm (cf. [Galllier
1987] p.391).
Actually, we do not even need a partial ordering, cf. [Dijkstra-Scholten 1990]
p. 174. Quite generally, let '<' be a binary relation on a set Z. If 5 is a subset of
Z, an element z is called a minimal element of 5 if and only if
(12) 
zeS 
A 
(Vy:y<z:yiS).

5.6 TERMINATION AND WELL-FOUNDED TRIPLES 
95
A subset N of Z is said to be well-founded with respect to < if and only if every
nonempty subset of N has a minimal element. In that case, we also say that
(Z, <,iV) is a well-founded triple.
The standard example is the triple (2Z, <, IN) where 7L is the set of the integers,
IN the set of the natural numbers and < the usual "less than" relation.
Well-foundedness is equivalent to the validity of mathematical induction. We
need this result in the following âslightly unusualâ form.
(13) Theorem. Subset N is well-founded with respect to < if and only if, for every
predicate / o n Z ,
(14) 
(Vz:z(Â£NV(Vy:y<z:f.y):f.z) 
=* (Vs ::/.s) ,
where the dummies z and y range over Z.
Proof. We need six bold steps:
N is well-founded with respect to <
= 
{definition and (12)}
( V S r S c i V 
A 
(3z::zeS):
(3z::zeS 
A (Vy : y < z : y Â£ S)))
= 
{let predicate / on Z be related to subset 5 of Z by f.z = z Â£ S}
(V/ : (V* : -i/.s : z G N) 
A (3z:: -*f.z) :
( 3 * : : i / . z 
A 
(Vy:y<z:f.y)))
= 
{trading}
(Vf:(Vz:z$N:f.z) 
A ( 3 * : : - . / . * ) :
(3* : ( V y : y < s : / . y ) :-/â¢*))
= 
{De Morgan}
(V/ :(\/z:z<Â£N 
: /.z) A ^(V* :: f.z) :
= 
{trading}
(Vf:(Vz:z<jLN:f.z) 
A (V* : (Vy : y < z : f.y) : /.z) :
(Vz ::/.*))
= 
{range union}
(Vf:(Vz:z$NV 
(Vy : y < z : /.y) : /.z) :
(Vz ::/.Â«)).
(End of proof)
The above theorem is applied to yield the next lemma on predicates.

96 
RAMIFICATIONS
(15) L e m m a . Let (Z, <,iV) be a well-founded triple. Let (i El:: 
r.i) be a family
of predicates and (i El:: 
vf.i) a family of Z-valued state functions such that for
all m E Z
(Vi :: [vf.i <mAmEN 
=> r.i])
=^ 
(Vi :: [vf.i = m=$> r.i]) .
Then [r.i] for all i E I.
Proof. We observe that
(Vi::[r.i])
= 
{vf.i is defined everywhere}
( V i : : [ ( 3 z : : v f . i = z) => r.i])
= 
{calculus}
(Vt ::(Vz :: [vf.i = z =Â» r.i]))
= 
{interchange, definition (16) below}
(V* :: f.z)
where predicate / on Z is defined by
(16) 
f.z 
= 
(Vt:: [vf.i = z =Â» r.i]) .
We now have to prove (Vz :: f.z). Since (Z, <, N) is a well-founded triple, this
follows from Theorem (13) if the antecedent of Formula (14) holds. This is proved
by observing that for any z E Z
z$N 
V 
(\/y:y<z: 
f.y)
= 
{(16) with z := y)
z Â£ N 
V (Vy :y < z :(\/i :: [vf.i = y =* r.z]))
= 
{calculus}
z ^ iV V (Vi :: [(3y : y < z : vf.i = y) =Â» r.i])
= 
{calculus}
(Vi :: z g NV [vf.i < z =Â» r.i])
= 
{calculus}
(Vi :: [z ^ NV (vf.i < z => r.i)])
= 
{calculus}
(Vi :: [vf.i < z A z E N => r.i])
=> {assumption of the lemma with m := z}
(Vi ::[vf.i = z =* r.i])
= 
{(
f . z .
(End of proof)

5.7 TWO NEW RECURSION THEOREMS 
97
5.7 Two new recursion theorems
The results of the previous section are used to prove a generalization of Theorem
2(16) and a necessity rule for wlp.
(17) Theorem. Let (Z, <,JV) be a well-founded triple. Let (i E / :: vf.i) be a
family of Z-valued state functions such that for every m E Z
(Vz E / :: {p.i A vf.i < m A m E N} h.i {q.i} )
=> 
(Vz E / :: {p.z A vf.i = m} body.(ft.i) {#.i} ) .
Then {p.z} h.i {q.i} for all i E /.
Proof. It follows from 2(0) that the proof obligation is equivalent to (Vz :: [r.i])
where
r.z = 
{p-i=> wp.(h.i).(q.i)) 
for all z E /.
In view of 2(0) and 2(14), the assumption of the theorem is equivalent to the
assumption that for all m E Z
(Vz :: [vf.i < m A m E N A p.i =$> wp.(h.i).(q.i)])
=> (Vz :: [vf.i = mAp.i 
=> wp.(h.i).(q.i)]) .
Since [p A q=^r] equivales [p => (q =>r)] for all predicates p, q and r, this means
(Vz :: [vf.i < m A m E N =Â» r.i])
=> 
(Vz :: [ v f . z = m =$> r.i]) .
The theorem now follows from Lemma (15). (End of proof)
The separation of Lemma (15) from the proof of Theorem (17) is also motivated
by the next result, a Necessity Rule for wlp, which can be compared to the Necessity
Rule 2(26) for wp.
(18) Theorem. Let (Z, <,iV) be a well-founded triple. Let (i el:: 
vf.i) be a
family of Z-valued state functions such that for every m E Z
(Vz :: [vf.i < m A m E N A wlp.(h.i).(q.i) => p.i])
=> (Vz :: [vf.i â m A wlp.(hody.(h.i)).(q.i) 
=$> p.i]) .
Then [wlp.(h.i).(q.i) 
=$> p.i] for all z.
Proof. The proof is completely analogous to the proof of (17), but based on postu-
late 2(15). It follows from Lemma (15) by means of the substitution
r.i 
= (wlp.(h.i).(q.i)=> p.i) .
(End of proof)

98 
RAMIFICATIONS
Example. Let v be an integer program variable and let procedure h be declared by
body./i 
= 
(skip [ v := v â 1; h ; v := v + 2) .
Operationally, it is clear that h need not terminate and that if h terminates the
value of v may be arbitrarily large. The second assertion is formalized in
(VzÂ£2Z:: [^wlp.h.(v < z)]) .
This formula can be proved by means of Theorem (18) with h.i = h and p.i = false
and q.i = (v < z) for all i G 2Z. We use the standard well-founded triple (2Z, <, IN).
By (18) it suffices to give a family of 2Z-valued state functions vf .z such that for
every m Â£ TL the induction hypothesis
(Vi G TL :: [vf.i < m A m G IN A wlp.h.(v < i) => fa/se])
implies for every i
[vf.i = m 
A wip.(body./i).(v < i) 
=> faise] .
To this end we first observe that the induction hypothesis is equivalent to
(19) 
(Vi G TL :: [ wip.ft.(v < i) => -i(vf.i < m A m G IN)]) .
Now our proof obligation is fulfilled by
vf.i = m 
A wip.(body./i).(v < i)
= 
{declaration of /i}
vf.z = m 
A v < i 
A wip.(v := v â 1; /i; v : = v + 2).(v < i)
= 
{calculus}
vf .z = m 
A v < i 
A wlp.(v := v â 1; fe).(v < z â 2)
=> 
{(19) with i := z â 2; monotony}
vf .z = m 
A v < i A
wip.(v := v - l).(-n(vf .(z - 2) < m A m G IN))
=> 
{choose vf .z = i â v, then first two conjuncts
imply m > 0 and hence m Â£ IN;
since m is constant, this can be used in third conjunct}
z â v = m 
A wlp.(v := v â l).(-i(i â 2 â v < m))
= 
{calculus}
i âv = m 
A z â 2 â (v â 1) > m
= 
{calculus}
false .
Notice that we delayed the choice of the state functions vf.i until the point where
the expression indicated a useful choice. (End of example)
Remark. According to [Apt-Plotkin 1986] the usage of well-founded sets for proofs
of total correctness goes back to [Manna-Pnueli 1974]. (End of remark)

5.8 EXERCISES 
99
5.8 Exercises
Exercises of Section 5.1.
Exercise 0. Prove that 
c C. d = c = c\d.
Exercise 1. Prove that
c C r d = 
( V g :: [r A w p . c . q =$> wp.d.q] 
A [ r A wlp.c.q 
=> wlp.d.q]) 
.
c C r d = c C (?r;d) .
Exercise 2. Prove that, if c Q c1 and d C. d', then
c 0 <* E c' 0 <*' and c; d E c';rf; .
Exercise 3. Prove that 
c ^ d 
= 
c Q d A d Q c.
Exercise 4. V Let c be a command and p and # predicates. Prove that
(a) 
?p;c C c]?q = 
[p V wp.c.(-ig)] ,
(b) 
c; ?p C ?#; c = 
[ g => wip.c.p] 
(use exercise 3.3.1).
Exercises of Section 5.2.
Exercise 0. Treat the case of wg = wlp.
Exercises of Section 5.3.
Exercise 0. (a) Prove that Theorem (4) remains valid if command ?p is replaced
by \p.
(b) Prove that, in (3) and (4), command ?q can be replaced by \q (compare The-
orem 4.2(ii) of Chapter 9 of [Dijkstra 1990], that asserts that [p=^ wp.c.q] implies
wp.(!p;c) = wp.(\p;c; 
\q)).
Exercises of Section 5.4.
Exercise 0. Prove the two formulae of (6) in the example of 5.4.
Exercises of Section 5.5.
Exercise 0. Prove that the existence of a strongest postcondition function sp.c
that satisfies property (10) implies the universal conjunctivity of wlp.c, cf. law 3(4).
Exercise 1. Prove formula (11).

100 
RAMIFICATIONS
Exercises of Section 5.6.
Exercise 0. Let (Z, <,iV) be a well-founded triple. Let / E Y â> Z be a function.
Define the subset M of Y and the relation < on Y by
y G M 
= 
f.y eN ,
x < y = 
/.a: < /.y .
Prove that (Y, <,iV) is a well-founded triple.
Exercise 1. Let prime be the set of prime numbers. For natural number x and
prime p, let x % p be the number of factors p of x given by
x % p = 
(MAX n , i / E l N : x = i / - | ) n : n ) ,
so that 0 % p = oo. Let relation -< on IN be defined by
x -<y 
=
(3p G prime :: x % p < y % p
A (Vj G prime : p < ^ : x % ^ = : y % ^ ) ) .
Prove that relation -< on IN is well-founded.
Exercises of Section 5.7.
Exercise 0. Let procedure h be declared by
body./i 
= 
(skip Q v := v + 1 ; h ; v := v â 2 ; h) .
Prove that [ ->wip./i.(v > i)] for all integer values i.

CHAPTER 6
RELATIONAL SEMANTICS
6.0. In this chapter, we start again from scratch. Now the meaning of a command is
not defined by means of the functions wp and wlp, but by means of the input-output
relation of a command. This point of view is closer to the intuitive ideas of most
programmers, but âin our viewâ it is less adequate for program development.
The relational point of view is useful for the analysis of special properties of
commands such as totality, termination and determinacy. It provides easy def-
initions or characterizations of composition, nondeterminate choice, guards and
assertions. All these concepts can therefore be treated in this chapter.
When the relational point of view is used in the analysis of repetitions or
recursive procedures, one needs to consider finite and infinite sequences of states,
usually accompanied by many case distinctions. Such operational reasoning can be
useful or necessary, but it is preferable to avoid it whenever possible. We introduce
some of the necessary techniques in Chapter 9. It is used only in Chapters 14 and
15.
Although we use the definitions of Section 1.1 and some other concepts in-
troduced in Chapters 1 and 3, this chapter is largely independent of the previous
chapters. In fact, it can be read to support them.
In Section 6.1, we introduce (input-output) relations and their weakest pre-
conditions, and we show that relations when interpreted as commands satisfy the
healthiness laws introduced in Section 3.2. In Section 6.2, we give the relational
interpretation of guards, sequential composition and nondeterminate choice. In Sec-
tion 6.3, we present relational justifications of the axiomatic definitions of totality
and termination.
In Section 6.4 we show that the expressive power of relational semantics equals
the expressive power of the predicate-transformation semantics of Chapter 1, under
assumption of the healthiness laws of Section 3.2.

102 
RELATIONAL 
SEMANTICS
6.1 Relations as an alternative specification method
The central concept of relational semantics is that of the input-output relation or
relation, which says whether or not a given state can be the result of a computation
that started in a second state. Since we allow nontermination, we need a formal
symbol oo ^ X to stand for the 'result" of a nonterminating computation. It is
convenient to treat the (input-output) relation as a curried function
R e X+ -> (X -> IB)
where X+ stands for the union X U {00}. Relation R is to be interpreted as follows:
(0) 
R.oo.x = the computation if started in x need not terminate,
R.y.x 
= the computation if started in x may terminate in y.
Here, and henceforth, we use dummies x, y (and z) to range over states in X.
Remark. In relational semantics, one usually writes xRy instead of our R.y.x. We
prefer the notation R.y.x, since it often enables the elimination of x. It gives a
closer connection to the functions wp and wlp. Moreover, we do not like a variable
like R to play the role of an infix operator. (End of remark)
Let R be an (input-output) relation. Recall from Section 1.1 that a predicate
p is a function p E X â> IB. Let x be a state. By Section 1.2, the truth of wlp.R.p.x
should mean that every state y that results from the initial state x satisfies p.y] the
truth of wp.R.p.x should also express that 00 is not a resulting state. In this way,
we arrive at the formal definitions
(1) 
wlp.R.p.x 
= ( V y G l : R.y.x : p.y) ,
wp.R.p.x 
= -iR.oo.x A wlp.R.p.x .
Example. For a program variable v and a state function /, the assignment v := f
can be regarded as the relation given by
(v := f).y.x 
= y = (v <- f).x ,
(v := f).oo.x 
= false ,
where (v Â«â f).x is defined in 1(23). Now one part of formula 1(24) is proved in
wlp.(v := f).p.x
= 
(Vy : y = (v <-/).x :p.y)
= 
p.((v <-f).x) .
The other case of 1(24) follows as well. (End of example)
By trading and application of definition 1(1), one can eliminate state x from
the formulae of (1). In fact, they are equivalent to

6.2 THE RELATIONAL VIEW OF GUARDS, COMPOSITION AND CHOICE 103
(2) 
wlp.R.p = (Vy e X : ^p.y : iiJ.y) ,
wp.R.p â ->i?.oo A wlp.R.p .
It follows from (2) that
(3) 
wlp.R.true = true ,
wp.R.true = -*R.oo .
This proves that
wp.R.p â wp.R.true A wlp.R.p .
So, if relation R is interpreted as a command it satisfies termination law 3(3).
If U is a set of predicates, the conjunction (Vp Â£ U :: p) is a predicate with,
for all states x,
wlp.R.(\/p 
E U ::p).x
= {(1)}
(Vy 
:R.y.x:(VpeU::p).y)
= 
{definition 1(1)}
(Vy:R.y.x:(Vpe U :: p.y))
= 
{interchange of quantifications}
(\/peU::(Vy:R.y.x:p.y))
= 
{(1) followed by definition 1(1)}
(Vp G U :: wip.iZ.p)^ .
This proves that
wlp.R.(Vp eU ::p) = (\fpeU :: wlp.R.p) .
So, wlp.R is universally conjunctive, cf. definition 3(1). This proves that every
relation R (interpreted as a command) satisfies the healthiness laws 3(3) and 3(4).
6.2 The relational view of guards, composition and choice
More or less independently of Section 1.3, we now introduce the relational concepts
corresponding to guards, assertions, composition and choice.
Let b be a predicate. The relation (?&) is defined by
(4) 
(?b).y.x = (x = y) A b.x ,
(?b).oo.x = false .
It is easy to calculate wlp.(?b) and wp.(lb). In fact, for every predicate p and state
x we have
wlp.(?b).p.x
= {(4)and(l)}
(Vy : (x = y) A b.x : p.y)

104 
RELATIONAL 
SEMANTICS
= 
{calculus}
b.x =^ p.x
(b=>p).x .
This proves that
wlp.(?b).p 
= 
(b=>p).
Since (?6).oo = false, it follows with formula (2) that wp.(?b).p = (b=$> p). This
proves that relation (?&) corresponds to command ?6 introduced in 1(8).
Similarly, relation (!6) is defined by
(5) 
(\b).y.x 
= 
x = y A b.x ,
(\b).oo.x 
= 
->&.# .
It is left to the reader to verify that relation (Ib) corresponds to command Ib intro-
duced in 1(8).
The informal interpretation of the sequential composition (R; S) of relations R
and 5 is as follows. The composition (iZ; 5) can give an output state y if and only
if relation R has an output state z with S.y.z. The composition need not terminate
if and only if R need not terminate or may yield an output state z with S.oo.z.
Therefore, formally, we define the composition (i?; S) by
(6) 
(R\S).y 
= 
(3z:S.y.z:R.z) 
,
(R;S).oo 
= R.oo V (3z : S.oo.z : R.z) .
Now one can prove that, for every predicate p and for wg G {wp, wlp} ,
(7) 
wg.(R; S).p = wg.R.(wg.S.p) .
This shows that definition (6) is in agreement with the definition 1(13). The proof
of (7) is left as an exercise for the reader.
If $ is a set of input-output relations, we write ( Q R E $ :: R) to denote the
choice relation given by
(8) 
( I iJ 6 * :: 22).u 
= 
(3R G $ :: R.u) 
for all u G X+ .
This definition corresponds to definition 1(16), in the sense that, for predicate p
and wg G {wp, wlp} ,
(9) 
wg( [] i? <E $ :: R).p = (VR G $ :: wg.R.p) .
Again the proof is left to the reader (see the exercises).

6.3 TERMINATION AND TOTALITY 
105
6.3 Termination and totality
In Section 1.3 we gave the axiomatic definition of termination and totality. These
concept are based on the operational intuition and are, therefore, easily translated
into relational semantics. Actually, for termination, the translation itself is based on
the intuition. For if relation R is interpreted as a command we have (as expected):
R is necessarily terminating
= 
{definition 1(11)}
[ wp.R.true]
= 
{(3) and 1(2)}
(Vz â¬ X :: -,R.oo.x) .
Totality is slightly more difficult:
R is total
= 
{1(12)}
[ -Â»wp.i?.faise]
= 
{(1) and 1(2)}
(Vz :: -*(-*R.oo.x A (Vy : R.y.x : false)))
= 
{De Morgan}
(Wx :: R.oo.x V (3y :: R.y.x))
= 
{alternatively}
(\fx GX :: (3ueX+:: R.u.x)) .
This proves that relation R is total if and only if every initial state admits at least
one resulting state (possibly oo), as suggested in Section 1.3.
6.4 From commands to relations
The next step is to associate to every command c in the sense of Chapter 1 an
input-output relation [c]. This requires the introduction of point predicates. For
every state y G X, we define the point predicate d.y G IP by
(10) 
d.y.x = (y = x) .
The letter d is chosen for the analogy with the Kronecker symbol 6jj and the Dirac
function 6.
For a command c, relation [c] is defined by
(11) 
[c].oo = 
-^wp.c.true ,
{cj.y = -iwip.c.(-id.y) 
for y 6 X.
According to interpretation (0), the first formula says that the computation of c
need not terminate if started in a state where wp.c.true does not hold. The second

106 
RELATIONAL SEMANTICS
formula says that state y is a possible result if command c is started in a state
where wlp.c.(-id.y) does not hold, i.e., where not all resulting states differ from y.
This shows that the definition of [c] agrees with the interpretation of wp and wlp,
cf. Section 1.2.
By formula (1), every input-output relation R has a weakest precondition and
a weakest liberal precondition, can therefore be interpreted as a command and
hence yields an associated relation [i?|. We claim that
(12) 
[R] = R.
In order to prove formula (12), it suffices to observe that
[R]-oo
= 
{(11)} 
-iwp.R.true
= {(3)} ii.oo
and that for any state y
[R]-y
= 
{(11)} 
^wlp.R.(^d.y)
= {(2)} -.(V* :->H*.y)-* :-*â¢*)
= {(10)} ^(yz:y = z:^R.z)
= 
{calculus} R.y .
On the other hand, for every command c that satisfies the healthiness laws
3(3) and 3(4), we claim that
(13) 
c ^ Icj,
i.e., c and [c] have the same wp and wlp, cf. definition 1(7). We begin with the
proof for wlp. For any predicate p we observe
wlp.{c}.p
= {(2)} 
(Vy.^p.y.^lcj.y)
- 
{(11)} (Vy : -.p.y : wlp.c.(^d.y))
= 
{wlp is universally conjunctive by 3(4)}
= 
{remaining proof obligation (14)}
wlp.c.p .
It remains to prove that
(14) 
(Vy : -ip.y : -irf.y) = p .
This is proved by observing that for any state x
(Vy : -ip.y : ^d.y).x
(Vy :

6.5 EXERCISES 
107
= 
{trading and (10)}
(Vy:x 
= y : p.y)
= 
{one point rule}
p.x .
This proves
(15) 
wJp.([c]) 
= 
wlp.c .
For wp we observe
wp-([c]).p
= 
{(2)}
-i|c].oo A wJp.[c].p
= 
{(11) and (15)}
wp.c.true A wlp.c.p
= 
(3(3)}
wp.c.p .
This proves wp.([c]) = wp.c, thus concluding the proof of formula (13).
Formulae (12) implies that predicate-transformation semantics is at least as
expressive as relational semantics. The combination of (12) and (13) implies that,
under assumption of the healthiness laws 3(3) and 3(4), predicate-transformation
semantics and relational semantics are equivalent.
6.5 Exercises
Exercises of Section 6.2
Exercise 0. Prove that relation (!&) has the same wp and wlp as command \b
introduced in 1(8).
Exercise 1. Prove formulae (7) and (9).
Exercises of Section 6.4
Exercise 0. Consider the refinement relation ' C. ' defined in 5(1). Use 1(4),
1(5) and 1(6) to prove that c C. [c] for every command c, independently of the
healthiness laws.
Exercise 1. For relations R and 5, we define
RD S 
= 
( V w G l + : : [R.u<=S.u]) .
(a) Prove that, for relations R and 5
RD S 
=Â» R C S .

108 
RELATIONAL 
SEMANTICS
(b) Prove that for commands c and d
c E d => 14 D 14 .
(c) Prove that, independently of the healthiness laws, for a command c and a
relation R
c C R = {cj D R .
Exercise 2. <? Prove that the strongest postcondition sp.c of a command c
(cf. Section 5.5) satisfies, for every p E IP, y G I :
sp.c.p.y = (3x E X : [cj.y.rr : p.x) .

CHAPTER 7
DETERMINACY AND DISJUNCTIVITY
7.0 In this chapter we investigate determinacy of commands and disjunctivity of
the associated predicate transformers. It turns out that these concepts are closely
related.
7.1 Determinacy
Determinacy of commands has not yet been defined. For the mathematical theory,
it is an important concept, both conceptually and historically. Its importance
stems from the popularity of the simplest mathematical model of computation,
where the output is a (partial) function of the input. In programming methodology,
determinacy is not so important. A program must establish a specific postcondition,
but uniqueness of the result is irrelevant.
Nevertheless, determinacy is interesting. The more so, since its very definition
in predicate-transformation semantics is subject to some debate. Our definition
differs from the definitions in [Dijkstra-Scholten 1990] and [Back-von Wright 1989b],
and was proposed by C.S. Scholten and J.C.S.P. van der Woude.
Both conceptually and mathematically, it is useful to distinguish two aspects
of determinacy: termination and the actual result. We thus arrive at the following
definitions.
A command c is called liberally determinate if and only if every initial state
admits termination in at most one final state, i.e.
(0) 
(Vx,y,z eX : [cj.y.z A \c\.z.x : y = z) .
Command c is called termination determinate if and only if at every initial state
where c need not terminate it cannot terminate, i.e.
(1) 
(Vs e X :: [cj.oo.a; =Â» (Vy E X :: -[cj.y.z)) .
Command c is called determinate if and only if it is both liberally determinate and
termination determinate.

110 
DETERMINACY 
AND 
DISJUNCTIVITY
(2) Lemma. Command c is liberally determinate if and only if
[ wlp.c.(->p) V wlp.c.p] 
for every predicate p.
Proof, By definition (0), it suffices to verify that for every state x
(Vp :: wlp.c.(-ip).x V wlp.c.p.x)
= 
{6(13) and 6(1); let y, z range over X}
(Vp :: (Vy : {c].y.x : -ip.y) V (V* : [c].z.a? : p.*))
= 
{distribution of V over V}
(Vp :: (Vy,* : [cj.y.z A {cj.z.x : -ip.y V p.*))
= 
{calculus}
(Vy,2: : {cj.y.x A [c].^.x : (Vp :: p.y =Â» p.2f))
= 
{calculus, use p ~ d.y for the downward implication}
(End of proof)
(3) Lemma. Command c is termination determinate if and only if
[ wlp.c.false V wp.c.true] .
Proof By definition (1), it suffices to verify that for every state x
wlp.c.false.x V wp.c.true.x
= 
{6(13), 6(1), 6(3)}
(Vy : [cj.y.a; : false.y) V -i[c].oo.a?
= 
{calculus}
[cj.oo.a: 
=Â» (Vy :: -n[c].y.x) .
(End of proof)
(4) Theorem. Command c is determinate if and only if
[ wlp.c.(-ip) V wp.c.p] 
for every predicate p.
Proof We first observe that for every predicate p
[ wJp.c.(-ip) V wp.c.p]
= 
{termination rule 3(3)}
[ wlp.c.(~ip) V (wp.c.true A wlp.c.p)]
= 
{distributivity}
[ (wip.c.(-ip) V wp.c.true) A (wlp.c.(-^p) V wlp.c.p)]
= 
{A distributes over [ ]}
[ wlp.c.(-yp) V wp.c.true] A [ wlp.c.{-^p) V wip.c.p] .
We now conclude by
(Vp :: [ wlp.c.(-^p) V wp.c.p] )

7.2 DISJUNCTIVITY PROPERTIES 
111
= 
{above calculation}
(Vp :: [ wlp.c.(-ip) V wp.c.true]) A (Vp :: [ wlp.c.(-^p) V wlp.c.p] )
= 
{wlp.c is monotone}
[ wlp.c.false V wp.c.true] A (Vp :: [ wip.c.(->p) V wlp.c.p])
= 
{Lemmas (2) and (3)}
c is termination determinate and liberally determinate
= 
{definition}
c is determinate .
(End of proof)
Remark. By Theorem (4), command c is determinate if and only if
[ ->wip.c.(-\p) ^ 
wp.c.p] 
for all p.
In exercise 3.3.2 it is proved that command c is total if and only if
[ ->wip.c.(->p) 4= wp.c.p] 
for all p.
In [Dijkstra-Scholten 1990], all commands are postulated to be total. Therefore,
determinism of c is defined in loc. cit. by the condition that
[ -iwip.c.(-ip) = 
wp.c.p] 
for all p.
(End of remark)
7.2 Disjunctivity properties
In the remainder of this chapter, we consider various disjunctivity properties of
weakest preconditions of commands. We concentrate on semantic properties. An
investigation of syntactic criteria that imply these semantic properties is given in
Chapter 8.
Analogously to the conjunctivity properties in Section 3.1, we consider the
following disjunctivity properties. A predicate transformer h G IP â> IP is called
finitely disjunctive if and only if for all predicates p and q
(5) 
h.(pVq) 
= h.pV h.q .
It is called universally disjunctive if and only if for every subset U of IP
(6) 
h.(3pâ¬ 
U::p) 
= (3p G U :: h.p) .
It is called positively disjunctive if and only formula (6) holds for all nonempty sets
U. It is called {0}-disjunctive 
if and only if formula (6) holds for U = 0. Since
(3p G 0 :: p) = false, function h is {0}-disjunctive if and only if
(7) 
h.false 
= false .
Therefore, by 1(12), a command c is total if and only if wp.c is {0}-disjunctive.

112 
DETERMINACY AND DISJUNCTIVITY
Another disjunctivity property is upper continuity, defined as follows. A set of
predicates U C IP is called a chain if and only if
(Vp,qeU::[p=>q] 
V[q=>p]) .
More generally, a partially ordered set (L, <) is called a chain if and only if
(8) 
(Vz,y â¬L::x<yVy 
< x) .
Predicate transformer h is called upper continuous if and only if formula (6) holds
for every nonempty chain U C IP.
7.3 Determinacy and disjunctivity
In this section, we show that positive disjunctivity of wlp.c is equivalent to liberal
determinacy of command c, cf. Section 7.1.
(9) Theorem. Command c is liberally determinate if and only if wlp.c is positively
disjunctive.
Proof. One implication is contained in
wlp.c is positively conjunctive
=$- {special case}
(Vp G IP :: wip.c.(-ip) V wlp.c.p â wlp.c.true)
= 
{3(5) and 1(2)}
(Vp G P : : [wlp.c.(-yp) V wlp.c.p))
= 
{Lemma (2)}
c is liberally determinate.
Conversely, let c be liberally determinate. In order to show that wlp.c is positively
disjunctive, it suffices to observe that for every nonempty set U of predicates and
every state x
wlp.c.(3p G U :: p).x
= 
{6(13) and 6(1)}
(Wy:lcly.x:(3peU::p).y)
(Vy : {cj.y.x : (3p G U :: p.y))
= 
{(0): if y exists it is unique}
( 3 y :: [ c j . y . z ) => ( 3 y : {cj.y.x 
: ( 3 p e U 
:: p . y ) )
= 
{calculus; U is nonempty}
(3p G Â£7 :: (3y :: [cj.y.s) => (By : [cj.y.s : p.y))
= 
{(0): if y exists it is unique}

7.4 DISJUNCTIVITY OF WP 
113
(3peU::(Vy:lcj.y.x:p.y))
= 
{6(13), 6(1), 1(1)}
(3p Â£ U :: wlp.c.p).x .
(End of proof)
7.4 Disjunctivity of wp
Let a command c be called disjunctive if and only if wp.c is positively disjunctive.
Remark. The weakest liberal precondition of a disjunctive command need not be
(finitely) disjunctive. For example, if i is an integer program variable, the choice
q = 
(Â± := 0 0 i == 1 D 
abort)
is disjunctive, since wp.q.p = false for every predicate p. Its weakest liberal precon-
dition is not disjunctive, since
wlp.q.(i = 0 V i = 1) 
= 
true ,
wlp.q.(Â± = 0) V wlp.q.{Â± = 1) = 
false .
This example also shows that disjunctive commands need not be liberally determi-
nate. (End of remark)
Using Theorem (9) and the termination rule, one can show that every liberally
determinate command is disjunctive. It is possible to prove that a command c is
disjunctive if and only if
(yx,y,z 
G X : -i[c].oo.# A {cj.y.x A {cj.z.x : y = z) .
7.5 Finite nondeterminacy
A command c is defined to be finitely nondeterminate if and only if for all states
xex
(10) 
[cj.oo.z 
V (FIN yeX:: 
\c\.y.x) ,
where for a predicate q the quantification (FIN y :: q.y) says that the set of elements
y with q.y is finite. The following result is presumably well-known, cf. [Hesselink
1990] Theorem 4(15).
(11) Theorem. Command c is finitely nondeterminate if and only if wp.c is upper
continuous.

114 
DETERMINACY AND DISJUNCTIVITY
Proof. For any state #, and any nonempty chain L of predicates, we have, with p
ranging over L
(wp.c.(3p :: p).x = (3p :: wp.c.p).x)
= 
{6(13) and 6(11); let y range over the states with JcJ.y.a;}
(-.[cj.oo.z A (Vy :: (3p :: p).y) = (3p :: -Â»[c].oo.:r A (Vy :: p.y)))
= 
{predicate calculus}
{cj.oo.x V ((Vy :: (3p :: p.y)) = (3p :: (Vy :: p.y))) .
Now letting L ranging over all nonempty chains of predicates, we get
(VL :: wp.c.(3p :: p).x = (3p :: wp.c.p).x)
= 
{above calculation and distribution of V over V}
[cj.oo.ar V (VL :: (Vy :: (3p :: p.y)) = (3p :: (Vy :: p.t/)))
= 
{making the range of y explicit; Lemma (12) below}
[cj.oo.z V (FIN y :: [cj.y.z) .
Now quantifying over #, we get
(VL:: wp.c.(3p::p) = (3p :: wp.c.p))
= 
(Vx :: [c].oo.a: V (FIN y :: [cj.y.x)) .
By definition (10) and the definition of upper continuity in Section 7.2, this proves
the assertion. (End of proof)
It remains to verify
(12) Lemma. A subset Y of X is finite if and only if for every nonempty chain L
of predicates
(VyeY::(3peL::p.y)) 
= 
(3p â¬ L :: (Vy E Y :: p.y)) .
Proof. If Y is finite and L is a nonempty chain of predicates, then
(Vyâ¬K::(3pei::p.y))
= 
{axiom of choice for finite Y}
= 
{Y finite, L a nonempty chain; take p = (MAX y :: /.y) in L
so that [f.y=>p] for all y eY.
For the upward implication, take f.y = p for all y.}
(3peL::(\/yeY::p.y)).
For the converse implication we proceed as follows. It suffices to assume that Y is
infinite and to construct a nonempty chain of predicates for which the equivalence
is false. Since Y is infinite, we can choose a sequence (i G IN :: u.i) of different
elements of Y. Thus, u.i ^ u.j whenever i ^ j . For k Â£ IN, let predicate q.k be
defined by

7.6 EXERCISES 
115
q.k.x = (\fi : k < i : x ^ w.i) .
Then [q.k => q.n] whenever k < n. Therefore, the family L = (k E IN :: #.&) is a
nonempty chain of predicates. The equivalence in the assertion evaluates to false,
since we have
(y.y::(3peL::p.y))
= 
{definitions}
(Vy ::(3fc::(Vt : 
k<i:y^u.i)))
= 
{all u.i are different}
true ,
and on the other hand
= 
{definitions}
= 
{take y = u.(k + 1)}
false .
(End of proof)
7.6 Exercises
Exercises of Section 7.1.
Exercise 0. Prove that every assignment and every guard is determinate.
Exercise 1. Let i be an integer program variable. Use the axiomatic characteri-
zations (2) and (3) to prove that
i := 0 [ abort
is liberally determinate and not determinate, and that
i : = 0 | | i : = l
is termination determinate and not determinate.
Exercises of Section 7.4.
Exercise 0. Prove that every liberally determinate command is disjunctive.
Exercise 1. Prove that wp.c is positively disjunctive if and only wp.c is finitely
disjunctive.
Exercise 2. Prove the last assertion of Section 7.4.

CHAPTER 8
SYNTACTIC CRITERIA
8.0. In this chapter, we develop syntactic criteria on commands, which imply
disjunctivity properties for their weakest preconditions. We suppose that the dis-
junctivity properties of the simple commands are known and try to generalize these
properties to procedures and composite commands. From this chapter onward, the
theory of Chapter 4 is indispensable.
In Section 8.1 we introduce, for a given set R of predicate transformers, a set
of commands called the syntactic reflection Sy.R of R. The main property is that
wp.q E R for all q E Sy.R. In Section 8.2 we provide methods to prove that a
command belongs to the syntactic reflection.
In Section 8.3 the theory is specialized to the case that R is characterized by a
disjunctivity property. Section 8.4 contains the next specialization, namely to the
classes of total commands, of disjunctive commands, and of finitely nondeterminate
commands. For our purposes the first two classes merely serve as examples or test
cases. Our real aim is the class of the finitely nondeterminate commands. It is this
class, or rather its syntactic reflection, that plays a key role in Chapters 11 and 13.
8.1 Syntactic reflection of semantic properties
Throughout this section we let R be a sup-closed subset of MT. We are interested
in syntactic criteria on commands c E A0 that imply wp.c E R. Our solution
consists of an algebraic definition of a subset Sy.R of A0 with wp.q E R for all
q E Sy.R. The set Sy.R can be regarded as the biggest set of commands q for which
a specific way of fixpoint induction is powerful enough to prove wp.q E i?, under the
assumption that it is known for which elements c of S the property wp.c E R holds.
In the proofs of the properties of Sy.R, we need a related set Sx.R of procedure
names, and a set Wp.R of functions w E MT .

8.1 SYNTACTIC REFLEXION OF SEMANTIC PROPERTIES 
117
We first give some auxiliary definitions. Recall that in 4(33) the subset WT of
MTH is defined by
w G WT = (Vfe G H,p G IP :: w.ft.p = w.h.true A wlp.h.p) .
We define an R-adapted pair to be a pair (if, Q) of sets K C H and Q C A 0 such
that
(0) 
(VheK r.body.he Q) and
(1) 
(Vw G WT ::
(\/h G K :: w./i e R) =Â» ( V ^ Q :: w0.? G 12)) ,
where tyÂ° is as defined in 4(25).
Letting (K, Q) range over all i?-adapted pairs, we define the unions
(2) 
Sx.R = (\J(K,Q)::K) 
C 
H,
Sy.R - 
(V{K,Q)::Q) 
C AÂ®.
The set Sy.R is called the syntactic reflection of R. For technical convenience, we
also introduce the subset Wp.R of MT given by
(3) 
we Wp.R = weWT 
A (VheSx.Rnw.heR) .
The main results of the section are contained in
(4) Theorem, (a) The pair (Sx.R, Sy.R) is i?-adapted.
(b) The subset Wp.R of MT is sup-closed and Do -invariant.
(c) wp.q G R for every command q G Sy.R.
Remark. Primarily, parts (a) and (b) are stepping stones to reach the main assertion
(c). They are stated explicitly, since they have some independent interest.
Proof, (a) Condition (0) is verified by observing that for every h
h G Sx.R
= {(2)}
(3 (if, Q) : ^-adapted : h G if)
=> 
{(0)}
(3 (if, Q) : J?-adapted : body./i G Q)
= {(2)}
body./i G Sy.R .
Condition (1) is proved by observing that for every w G WT
(V/iG Sx.Ry.w.heR)
= {(2)}
(V(if,Q) : i?-adapted : (Vh e K :: w.h e R))
(V(if, Q) : i?-adapted : (V? G Q :: w% G

118 
SYNTACTIC CRITERIA
= {(2)}
(V9 â¬ Sy.R :: wÂ°.q G i2) â¢
This proves that (Sx.R, Sy.R) is an i2-adapted pair. Notice that definition (3) now
allows to reformulate (1) for this special case into
(5) 
(Vto G Wp.J2, q G Sy.J? :: to0.? G J2) .
(b) The set Wp.R is sup-closed in MTH since for every subset U of MT
(sup 17) G Wjp.i?
= {(3)}
(sup 17) G WT A (V/i6 Sx.R :: (sup U).h â¬ R)
Â«= 
{4(37) and 4(3)}
P C W T 
A (Vfte Sx..R :: (supu; G U :: w.h) G i?)
<= {R is sup-closed}
i7cWT A (\/heSx.R,weU y.w.heR)
= {(3)}
J7 C Wp.R .
The set Wp.R is Â£>o-iirvariant since for every w G MT
Z?o-^ G Wp.J?
= {(3)}
Do.wGWT A 
(\/heSx.R::D0.w.heR)
4= {4(35) and 4(26)}
weWT 
A (V^G5x.i?::u;0.(body./i)G-R)
<^= {part (a)(0)}
w e f f T 
A (Vg G Sy.i? :: u;Â°.g G J2)
^ 
{(3), (5)}
it; G Wp.R .
(c) It remains to verify that
(\/q G Sy.R :: wp.? G B)
= 
{wp = (wp\H)Â°}
(V? G S7.i? :: (wp|Jf)Â°.? G -R)
^= {(5)}
wp\H G Wp.i?
4= 
{wp\H is least fixpoint of Do in MTH and Theorem 4(8)(a)}
Wp.R is sup-closed and Do -invariant
= 
{part (b)}
true .
(End of proof)

8.1 SYNTACTIC REFLEXION OF SEMANTIC PROPERTIES 
119
In view of Theorem (4), it is important to obtain a more concrete characteri-
zation of Sy.R. As a first step, we claim that Sy.R equals the set of commands Q
given by
qeQ 
= (VwÂ£Wp.R::wÂ°.qeR) 
.
This is proved by mutual inclusion in
Q C Sy.R
Â«= 
{definition (2)}
(Sx.R, Q) is an i?-adapted pair
= 
{(0), (1), (3)}
(Vh e Sx.R:: body.heQ) A
(VwE 
Wp.R,qeQ::wÂ°.qeR)
<= {Theorem (4)(a) and definition of Q}
Sy.R C Q
= 
{(5) and definition of Q}
true .
Since Q = Sy.R, formula (5) can be strengthened to
(6) 
qeSy.R 
= (Vu; E Wp.R :: wÂ°.q G R) .
In definition (3), the set Wp.R is expressed in terms of Sx.R. In formula (6),
the set Sy.R is expressed in terms of Wp.R. We close the circle by expressing Sx.R
in terms of Sy.R. We claim that Sx.R equals the set of procedure names K given
by
he K = he H A body./* Â£ Sy.R .
This is proved by mutual inclusion in
K C Sx.R
<= {definition (2)}
(K, Sy.R) is an .R-adapted pair
= 
{(0), (1), definition K)
(Vu; e WT ::
(VheK y.w.heR) 
=Â» (Vg Â£ Sy.R :: wÂ°.q Â£ JJ))
â¢Â«= {Theorem (4)(a) and (1)}
Sx.i? C K
= 
{Theorem (4)(a), (0) and definition K)
true .
The result K = Sx.R means that
(7) 
h e Sx.R = h e H A body./i Â£ Sy.R .

120 
SYNTACTIC 
CRITERIA
Example. The syntactic reflection need not contain all commands with a weakest
precondition in R. This is shown as follows.
Let i be an integer program variable. Let procedure h be declared by
body./* 
= (i := 0 ; h Q ?(i + 0) ; c)
for some command c E S 0 . One can prove (see exercise 2.7.2) that for every
predicate p
wp.h.p 
= false ,
wlp.h.p 
= (i = 0 V wlp.c.p) .
We take H = {h}. Let w G MTH be given by
w.h.p 
= (i = 0) for every p.
Function w is element of WT since for any predicate p
w.h.p 
= w.h.true A wlp.h.p
= 
{see above}
(i = 0) = (i = 0 A (i = 0 V wlp.c.p))
= 
{calculus}
true .
We compute
wÂ°.(body.h).p
= 
{declaration}
wp.(i := O).(w.h.p) 
A (i =fi 0=> wp.c.p)
= 
{definition w; calculus}
i = 0 V wp.c.p .
We now take R to be the subset of MT given by
feR 
= (Vp Â£ P :: f.p = f.true) 
.
It is easy to verify that R is sup-closed. We have wp.h G R and w.h E iZ. Since
Sx.i* C {h}, it follows with (3) that w G Wp.i*.
We take c = (i := 1). Using the above computation, it is easy to verify that
wQ.{hody.h) $ R ,
so that body./i ^ Sy.i? from (6). On the other hand, we have wp.(body.h) =
wp.h G R- (End of example)
8.2 Membership of the syntactic reflection
We now introduce the main instrument to prove that a recursive procedure belongs
to Sy.R. For an arbitrary subset K of H, we define the saturation Sat.R.K as the
subset of A0 given by

8.2 MEMBERSHIP OF THE SYNTACTIC REFLEXION 
121
(8) 
q E Sat.R.K 
=
(\/w E WT : (Vh E K :: w.h E R) : wÂ°.q E R) .
The importance of the concept of saturation is due to the facts collected in
(9) Theorem, (a) Sy.R = Sat.R.(Sx.R).
(b) If (VheK:: body./* E Sat.R.K) then Sat.R.K C Sy.R.
(c) if C Sat.R.K.
(d) If ifl) C K then Sat.R.KO C Sat.R.K.
(e) For all g E S 0 with wp.g E i?, we have g G Sat.R.K.
(f) If the set R is closed under functional composition, then Sat.R.K is closed under
sequential composition.
(g) If the set R is closed under nonempty (finite) infima in the lattice MT, then
Sat.R.K is closed under (finite) choice.
Proof, (a) This is a reformulation of (6) based on (3) and (8).
(b) By definition (8) the pair (K, Sat.R.K) satisfies formula (1). The extra
condition gives formula (0), so it implies that the pair is i?-adapted, and hence
that Sat.R.K C Sy.R by definition (2).
(c) and (d) are easy and may be left to the reader.
(e) For q E Se we have
q E Sat.R.K
4= 
{definition (8)}
(\/w E 
WT::wÂ°.qER)
<= {q E SÂ® implies that wÂ° .q = wp.q for all w E WT}
wp.q E R .
(f) For commands q, r and a function w E WT we have
wÂ°.(q;r)<ER
= 
{wÂ° is a homomorphism by 4(43)}
wÂ°.qowÂ°.r E R
<= {R is closed under functional composition}
wÂ°.qER A 
wÂ°.rER.
By definition (8), this implies
(?; r) E Sat.R.K 
<= q E Sat.R.K 
A r E Sat.R.K .
(g) This is proved in the same way as part (f).
(End of proof)

122 
SYNTACTIC 
CRITERIA
Example. In this example we show how Theorem (9) can be used to prove that a
command belongs to Sy.R. Assume that R is closed under functional composition
and nonempty finite conjunctions. By Theorem (9), the set Sy.R is closed under
sequential composition and finite choice. If q G 5 0 satisfies wp.q G R then q G Sy.R.
Let us now consider a repetition L = while b do c od , and ask for a sufficient
condition for L G Sy.R. Repetition L is the procedure declared by
body.L 
= 
?-ifc | ?6; c; L .
We use Theorem (9) with K = Sx.R U {Â£} and observe
L G Sy.i?
Sat.R.K C Sy.R
<= 
{(9)(b)}
(VheK 
:: body./i G Sat.R.K)
= 
{by (9)(a,d), we have Sy.i? C Sat.R.K] also use (4)(a)}
body.L G Sat.R.K
<= {declaration of L\ closure properties of R; (9)(f,g)}
?-Â«6, ?6, c, L G Sat.iJ.if
4= 
{L G Sat.R.K by (9)(c); Sy.i? C Sat.R.K by (9)(a,d)}
?-.&, ?6, c G Sy.i? .
So, it is sufficient that the body and the two guards be element of Sy.R. (End of
example)
8.3 \I/âdisjunctivity
The results of the previous sections are applicable to various disjunctivity proper-
ties. Since we do not want to give the same proof several times, we propose the
following unifying definition.
Let $ be a class of ordered sets. A predicate transformer h is called SSf-
disjunctive if and only if h is monotone and for every ordered set (L, <) G $ and
every monotone function g G L â> IP
(10) 
h.(3x eL:: g.x) 
= 
(3xeL 
:: h.(g.x)) 
.
This definition covers the definitions in Section 7.2, since we have the following
table:

8.3 Y-DISJUNCTIVITY
123
property of h
universally disjunctive
positively disjunctive
finitely disjunctive
{ 0 }-disjunctive
upper continuous
L arbitrary
L nonempty
L nonempty finite
L empty
(L, <) a nonempty chain
We write Md.ty to denote the subset of MT of the ^-disjunctive predicate trans-
formers. In order to reap the fruits of the previous sections, we verify
(11) 
Md.ty is a sup-closed subset of MT.
Indeed, let !7bea subset of Md.ty. We have to prove that (supC/) E Md.ty. This
follows from the observation that for any ordered set (L, <) E \Â£ and any monotone
function g E L ââ¢> IP
(supf/).(3z e L :: g.i)
= 
(4(6)}
(3heU::h.(3ieL::g.i))
= 
{(10): every h G Md.$}
(3 he 
U::(3ieL::h.(g.i)))
= 
{interchange and 4(6)}
(3ieL::(supU).(g.i)) .
This proves (11). Now all results of Sections 8.1 and 8.2 are applicable to Md.ty.
In particular, we have
(12) 
wp.q e Md.^t! for every q E Sy.(Md.fy).
With respect to the method of Section 8.2, it is useful to observe
(13) Theorem. For any subset K of H, the set of commands Sat.(Md.i&).K is
closed under sequential composition and deterministic choice.
Proof. It is easy to see that a functional composition of ^-disjunctive predicate
transformers is ^-disjunctive. Thus, Md.ty is closed under functional composi-
tion. By Theorem (9)(f), it follows that Sat.(Md.ty).K is closed under sequential
composition.
It remains to consider deterministic choice, cf. Section 1.8. Let / E Ix be
an /-valued state function and let (t E / :: q.t) be a family of commands q.t E

124 
SYNTACTIC 
CRITERIA
Sat.(Md.ty).K. 
We have to prove that the deterministic choice q.f defined in 1(27)
satisfies q.f Â£ Sat.(Md.^).K. 
By definition (8), this amounts to proving that
wÂ°'(Q'f) Â£ Md.fy, whenever w Â£ WT satisfies
(VheK::w.heMd.V) .
Now wÂ°.(q.f) Â£ Md.fy is proved by observing that for any pair (JC, <) Â£ \& and any
monotone function g Â£ L â> IP
wÂ°.(q.f).(3ieL::g.i)
= 
{Lemma (14) below}
(3* â¬ J : : ( / = *) A u;0.(g.t).(3t Â£ L :: g.i))
= 
{q.t Â£ Sat.(Md.*).jRT, so that u;Â°.(g.<) Â£ Md.#, and (10)}
(3t Â£ / :: (/ = t) A (3t Â£ L :: wo.(q.t).(g.i)))
= 
{predicate calculus}
(3i G I :: (3t â¬ I : : (/ = t) A uÂ»Â°.(g.<).(flf.O))
= 
{Lemma (14) below}
(3ieL::wÂ°.(q.f).(9.i)).
(End of proof)
(14) Lemma. 
Let / Â£ Ix be an J-valued state function. Let (t Â£ / :: <?.Â£)
be a family of commands. Then the deterministic choice q.f satisfies, for any
v Â£ H -> MP and any p Â£ IP,
vÂ°.(q.f).p 
= (V* :: (/ = *)=>vÂ°.(?.*)-p) , and also
Proof. By Corollary 4(43), t?Â° is a homomorphism. Now the first formula is proved
in the same way as 1(28). The second formula follows from the first one, by means
of the next lemma. (End of proof)
Lemma. Let / Â£ Ix be a /-valued state function and let (t Â£ J :: p.t) be a family
of predicates. Then
Proof. Using definitions 1(0), 1(1) and 1(20), one can verify that for any state x
(Vt ::(/==*) =*P.<)-*
= 
(V* :: (f.x =t)=>p.t.x)
= 
p.(f.x).x
= 
(3* :: (f.x = t)Ap.t.x)
= 
(3* :: (/ = *) Ap.t).x .
(End of proof)

8.4 TOTALITY, DISJUNCTIVITY AND FINITE NONDETERMINACY 
125
This concludes the treatment of Theorem (13).
Example. Let c be a command in Sy.(Md.fy) and let b E IP. Then we have
while b do c od G Sy.(Md.#) .
In fact, the repetition is a procedure L declared by
body.X 
= 
?-i& | ?&; c; L .
We proceed in the same way as in the example of Section 8.2. So we put K =
Sx.R U {L} and observe
L e Sy.(Md.V)
4= 
{just as in the example of 8.2}
body.L e Sat.(Md.V).K
<= 
{declaration of L\ (9)(f); (13)}
c, L e Sat.(Md.V).K
= 
{just as in the example of 8.2}
c e Sy.(Md.V) .
This proves that L <E Sy.(Md.V). (End of proof)
8.4 Totality, disjunctivity and finite nondeterminacy
The theory of Section 8.3 is mainly applied to three classes \1>:
\I>0 : the empty set
\I>1 : all nonempty sets with arbitrary order
*2 : all nonempty chains,
with the corresponding sets of predicate transformers
(15) 
Mto = Md.^0 : {0}-disjunctive predicate transformers
Mdi â Md.^l : positively disjunctive predicate transformers
Muc = Md.^2 : upper-continuous predicate transformers,
and the corresponding conditions on commands c
wp.c G Mto = c is total, cf. Section 7.2
wp.c E Mdi = c is disjunctive, cf. Section 7.4
wp.c G Muc = c is finitely nondeterminate, cf. Section 7.5.
It is easy to verify that
(16) Corollary, (a) All assignments belong to Sy.Mto.
(b) All guards and all assignments belong to Sy.Mdi.
(c) Sy.Mdi C Sy.Muc.

126 
SYNTACTIC 
CRITERIA
The set Sy.Muc requires some further investigation, since it plays an important
role in the analysis of computational induction. We first need two auxiliary results.
(17) Lemma (diagonalization). Let (i,j G L :: f.i.j) 
be a family of predicates. Let
L be a chain, and assume that f.i.j is monotone in both i and j . Then
(3i,j 
:: f.i.j) 
= 
(3* :: f.i.i) 
, and
(Vz,j :: f.i.j) 
= (Vi:: 
f.i.i).
Proof. We use domain splitting
(3iJ :: f.i.j)
= 
{L is a chain}
( 3 i , i : i < j : /.i.j) 
V (3iJ :j<i: 
f.i.j)
= 
{monotony of / in both i and j}
(3j::f.j.j) 
V (3i:: f.i.i)
= 
{renaming, idempotency of V}
(3i :: f.i.i) .
The second formula is proved in the same way. (End of proof)
We claim that Muc is closed under nonempty finite infima, i.e.
(18) 
(Vf,geMuc::f Ag eMuc) ,
where / A g = (inf {/,#}) in MT. This formula is proved by observing that for / ,
g G Muc and any nonempty chain U in P
(fAg).(3peU::p)
= 
{definition / A g in 4(6)}
/.(3petf::p) A 
ff.(3pe?7::p)
= 
{/) 9 â¬ Muc and U a nonempty chain, see Section 7.0}
(3peU::f.p) 
A (3peU::g.p)
= 
{distributivity}
= 
{f.p A ^.^ monotone in both p and #, (17)}
(3peU::f.pAg.p)
â {definition / A g}
(3peU::(fAg).p).
By Lemma (9)(g), formula (18) implies
(19) Theorem. For any subset K of H, the set Sat.Muc.K is closed under finite
choice.

8.4 TOTALITY, DISJUNCTIVITY AND FINITE NONDETERMINACY 
ffl
We summarize by presenting a more syntactic rule for proving that commands
belong to Sy.Muc. For any set of commands Q C A0, let Fin.Q be the smallest set
of commands that satisfies
(i) Q C Fin.Q ,
(ii) if g, r E Fin.Q then (q; r), (q [ r) E Fin.Q ,
(iii) if (j E Â«/ :: #.i) is a family of commands in Fin.Q and / is a J-valued state
function the deterministic choice q.f satisfies q.f E Fin.Q.
(20) Theorem, (a) If c E S 0 is of finite nondeterminacy, then c E Sy.Muc.
(b) Sy.Muc = Fin.(Sy.Muc) .
(c) If G is a set of procedure names with
body./i E Fin.(Cr U Sy.Muc) for all 
heG,
then G C Sy.Muc.
Proof, (a) If c E S 0 is of finite nondeterminacy, then wp.c E Muc and hence
c E Sat.Muc.K for every K by (9)(e). Therefore the assertion follows from the
application of (9)(a):
(21) 
Sy.Muc = Sat.Muc.(Sx.Muc) .
(b) By (13) and (19), for every subset K of jff, the set Sat.Muc.K is closed
under composition, finite choice and deterministic choice, i.e.,
(22) 
Sat.Muc.K = Fin.(Sat.Muc.K) .
Now again the assertion follows with (21).
(c) This part is proved by
G C Sy.Muc
<= 
{(9)(c)}
Sat.Muc.(G U Sx.Muc) C Sy.Muc
<= {(9)(b)}
GU Sx.Muc :: body./i E Sat.Muc.(G U Sx.Muc))
{(4)(a), (9)(d) and (21)}
G:: body./i E Sat.Muc.(G U Sx.Muc))
<Â£= {assumption}
Fin.(G U Sy.Muc) C Sat.Muc.(G U Sx.Muc)
<= {Fin is monotone and (22)}
G U Sy.Muc C Sat.Muc.{G U Sx.Muc)
= 
{(9)(c,d) and (21)}
true .
(End of proof)

128 
SYNTACTIC 
CRITERIA
We conclude this section with an example to show that, if a command is finitely
nondeterminate, it need not be element of Sy.Muc.
Example. We use a variation of the example of Section 8.1. Just as in that example,
we let i be an integer program variable. We let procedure h be declared by
body./* = (i := 0 ; h || ?(i + 0) ; c)
for some command c to be chosen later. For every predicate p we have
wp.h.p = false ,
wlp.h.p = (i = 0 V wlp.c.p) .
We take H = {h}. We let w E MTH be given by
w.h.p = (i = 0) for every p
and we have that function w is element of WT. One can verify that wp.h E Muc
and w.h Â£ Muc. Since Sx.Muc C {/*}, it follows with (3) that w E Wp.Muc. We
now choose
c = ( g n Â£ IN :: i := n) .
Just as in the example of 8.1, we have
(23) 
wÂ°.(hody.h).p 
= i = 0 V wp.c.p
and it remains to prove that wÂ°.(body.h) ^ Muc. Now we consider the chain of
predicates (k E IN :: i < k) and observe
(3k::wÂ°.(body.h).(i<k))
= 
{(23)}
(3 k :: i = 0V wp.c.(i < k))
â 
{definition c]
(3 k :: i = 0 V (Vn E IN :: wp.(i := n).(i < jfe)))
= 
{assignment}
(3fc::i = 0 V ( V n E l N : : n < Jfe))
= 
{calculus}
i = 0,
whereas
wÂ°.(body.h)
= 
{(23)}
i = 0 V wp.c
= 
{calculus}
i = OV wp.c
= 
{definition
true .
.(3Jb::(i<
.(3ife::(i <
.true
c; calculus}
This proves that wÂ°.(body.h) Â£ Mu<
*))
k))
:, and hence that h
Sy.Muc.

8.5 EXERCISES 
129
Of course, it is well possible that h = q for some lower-level command q E 5Â®.
In that case, we have q E Sy.Muc by Theorem (9)(e). This shows that Sy.Muc need
not be closed under semantic equality. (End of example)
8.5 Exercises
Exercises of Section 8.1.
Exercise 0. Prove that Sx.R = H n Sy.R.
Exercises of Section 8.2.
Exercise 0. W Let e and g E MT be such that g < e. Let R be the subset of
MT given by
feR 
= 
foe<gof.
Prove the following assertions.
(a) R is sup-closed in MT and wp.q E R for all # E Sy.R.
(b) Sy.i? is closed under sequential composition.
(c) If g is finitely conjunctive, then Sy.R is closed under finite choice.
Exercise 1. Let e and g E MT be such that g < e, and assume that e is universally
disjunctive. Let R be given by
f e R 
= 
e o f 
< f o g .
Prove
(a) R is closed in MT and wp.g E R for all # E Sy.R,
(b) Sy.jR is closed under sequential composition and unbounded choice.

CHAPTER 9
OPERATIONAL SEMANTICS OF RECURSION
9.0, In this chapter, we reconcile the definition of the semantics of recursive pro-
cedures, cf. Chapter 4, with the relational semantics of Chapter 6. The idea is that
the two semantical paradigms meet halfway. Therefore, the chapter consists of two
parts.
The first part is based on predicate-transformation semantics, cf. Chapter 4.
In Section 9.1, we describe the stack implementation of recursive procedures. This
implementation can be regarded as an interpreter: the whole recursive declaration
is interpreted by means of a tail-recursive procedure with a stack of continuations
as a value parameter. The correctness of the interpreter is proved in Section 9.2.
In the second part of the chapter we treat the relational semantics of recursive
procedures. This is done in two steps. In Section 9.3, we define the relational se-
mantics of a tail-recursive declaration by means of a transitive closure in a graph
of configurations. By Chapter 6, these relational semantics induce predicate trans-
formers. We then show that the predicate transformers correspond to wp and wlp
as defined for such a declaration in Chapter 4. In Section 9.4, the ideas and results
of the preceding sections are combined. The stack implementation of 9.1 is com-
bined with the relational semantics of tail recursion (cf. Section 9.3) to define the
relational semantics of an arbitrary recursive declaration. The results of 9.2 and 9.3
imply that these relational semantics correspond to the predicate-transformation
semantics of Chapter 4.
We have the following reason for giving the relational semantics of tail recursion
first. In Section 9.3, we relate the extreme fixpoints of functions De to the transitive
closure in the configuration graph. So, it is here that the two paradigms meet. Due
to the restriction to tail recursion, this meeting is not hindered by the additional
complexity of stack operations.

9.1 THE INTERPRETER 
131
9.1 The interpreter
In Section 2.8, the repetition was defined in terms of recursion. Every compiler,
however, interprets recursion by means of a repetition with a stack that contains
the sequence of commands that remain to be executed. In this section, we describe
this formally.
Given are a set of simple commands 5, a set of procedure names H and a
declaration body G H â> A 0. The stack implementation of recursion consists of
one repetition, i.e. a new tail-recursive procedure, not in Â£T, such that all calls of
procedures in H are replaced by stack operations.
The value of the stack is an element of A*, i.e. a string of elements of A = S\JH.
It is not a component of the state space. We could model it by extending the state
space. It is more convenient, however, to model the stack as an input parameter of
the new tail-recursive procedure. So we introduce a new set K of procedure names
k.q with q G A*. Function k G A* â> K is bijective, i.e., ono to one and onto
K. It can therefore be used as an identification of A* and K. We shall not do so,
however. The family of procedures k.q is declared by
(0) 
body.(k.e) 
= 
e ,
body.(k.(c]q)) 
= c; k.q ,
body.(k.(h; q)) = ( Q s G body./* :: k.(s; q)) ,
for all simple commands c â¬ S, procedure names h G Â£T, strings q G A*. In the third
clause body./i is treated as a set of strings, cf. Section 4.3. Notice that formula (0)
is a tail-recursive declaration that does not contain calls of procedures h G H.
We claim that procedure fc is a faithful interpreter, in the sense that k.q = q
for all strings q G A*. By 1(7), this is equivalent to
(1) 
(V# G A* :: wp.(k.q) = wp.q A wlp.(k.q) = wlp.q) .
9.2 The proof of the faithful interpreter
The proof of formula (1) has two parts, one for wp and one for wlp. We begin with
an investigation of wp.(k.q) for all q G A*. The restriction wp\K is the smallest
(least) solution of the equation DQ.V = v for v G K â* MT. By declaration (0) and
4(21), this equation expands to
v.(k.e) = identity ,
v.(k.(c; q)) = wso.c o v.(k.q) ,
v.(k.(h;q)) = (inf s G body./i :: v.(k.(s; q)))

132 
OPERATIONAL 
SEMANTICS 
OF RECURSION
for all c G 5, h G H, q G A*. If we write wk = wp o &, it follows that wk is the
smallest solution u G A* âÂ» MT of the system of equations
(2) 
u.e = identity ,
u.{c\ q) = wso.c o u.q ,
u.(h;q) = (mis G body./i :: u.(s;q)) .
The function wp G A 0 â> MT restricts to a function wp GA*-^ MT that satisfies
system (2). Since wk is the smallest solution, this proves that
(3) 
wpok = wk < (wp\A*)
with respect to the induced order of A* â> MT. It requires more delicate arguments
to obtain an equality in formula (3).
In fact, we first give the proof under an additional assumption. We assume
that wk is 4submultiplicative' in the sense that
(4) 
(V#, r G A* :: wk.q o wk.r < wk.(q; r)) .
By induction on the length of the strings, formula (4) implies that the extension
(wic|A)*, cf. 4(15), of the restriction wk\A satisfies
(5) 
(wk\A)* < wk 
in A* -> MT.
For any h G H, we observe
D0.(wk\H).h
= 
{4(26)}
(ws0U(wk\H))Â®.(hody.h)
= 
{wk satisfies (2) with q := e}
(wic|A)0. (body./i)
= 
{4(15)}
(inf s G body./i :: (wt|A)*.6)
< 
{(5)}
(inf 5 G body./i :: wk.s)
= 
{wk satisfies (2) with q := e}
wk.h .
This proves Do.(wk\H) < wk\H. Since wao is the least fixpoint of J?o, it now follows
from exercise 4.2.0 that wao 55 wk\H. This implies wp\A < wk\A. Combining this
inequality with (3) and (5), we obtain
wk < (wp\A*) = (wp\A)* < (wk\A)* < wk .
This proves the first conjuncts in formula (1):
(6) 
wp ok â wp\A* .
Therefore, with respect to wp, it remains to prove formula (4).

9.2 THE PROOF OF THE FAITHFUL INTERPRETER 
133
To this end, we investigate system (2) with strings (q; r) as argument of u. For
a fixed string r G A*, let wr G A* ââ¢ MT be given by
wr.# = wk.(q; r) 
for all q â¬ A* .
Let function i*1 G (A* -Â» MT) -> (A* -> MT) be defined by
(7) 
F.v.q = v.q o wk.r , or equivalently
F.v.q.p = v.q.(wk.r.p) .
In order to prove formula (4), it suffices to prove
(8) 
F.wk < wr 
in A* -Â» MT.
Since wk is the smallest solution of equation (2) in u, it follows from the definition
of wr that wr is a solution of the system of equations in u
(9) 
u.e = wk.r ,
u.(c; q) = wp.c o u.q ,
u.(h;q) = (inf s E body./i :: u.(s',q)) .
So it suffices to prove that F.wk is the smallest solution of (9). Here we need the
following auxiliary result:
(10) Lemma. Let W and Z be complete lattices. Let D G W -> W and E G Z -> Z
be monotone functions. Let xO be the least fixpoint of D. Let F G W â> Z be a
function that commutes with suprema and satisfies F o D = E o F. Then F.xO is
the least fixpoint of E.
Proof. F.xO is a fixpoint of E since
E.(F.xO) = F.(D.xO) = F.xO .
Let z G Z be any fixpoint of J5. In order to prove that F.xO < z, we introduce the
subset V of W given by
w eV 
= F.w < z .
The set V is sup-closed in W since for any subset U of W
(sup Â£/) G V
= 
{definition V}
F.(supU) < z
= 
{F commutes with suprema}
(supu E U :: F.u) < z
= 
{definition supremum}
(Vti eU :: F.u < z)
= 
{definition y}
C/C V .
The set V is Z)-invariant since for any w G W

134 
OPERATIONAL 
SEMANTICS 
OF RECURSION
D.w G V
= 
{definition V}
F.(D.w) < z
= 
{F o D = E o F; z fixpoint of E}
E.(F.w) < E.z
<= {E is monotone; definition V}
w e v .
By Theorem 4(8)(a), this implies that xO G V, that is, FxO < z. Therefore, F.xO
is the least fixpoint of E in Z. (End of proof)
Lemma (10) is applied with xO = wk and W = Z = (A* -> MT). The function
F defined in (7) commutes with suprema, since, for every subset U of A* â* MT
and every string q G A* and every predicate p G IP, we have
F.(sup U).q.p
= 
{(7)}
(sup U).q.(wk.r.p)
= 
{4(3) twice: Y := A* and K := P}
(supw Â£ U :: u.q.(wk.r.p))
= 
{(7)}
(supu Â£ U :: F.u.q.p)
= 
{4(3) twice: Y := A* and F := P}
(supu G 17 :: F.u).q.p .
Let 2? be the fixpoint operator of (2) and let E be the fixpoint operator of (9).
Unifying the operators D and U, we see that D = A.identity and E = A.(wic.r)
where
A./.U.6 = / ,
A./.w.(c; g) = wso.c o u.q ,
A.f.u.(h]q) = (infs G body./i :: u.(5;^)) ,
for all / G MT, u G A* -> MT, g G A*, c G 5 and fe G H.
Function wk is the least fixpoint of D and we still have to prove that F.wk is
the least fixpoint of E. Therefore, by Lemma (10), it remains to prove that
FoD = EoF ,
or equivalently, that, for all u G i * - ^ MT and q G A*,
(11) 
F.(D.u).q = E.{F.u).q .
This is proved by case distiction on q. For q := e, we observe
F.(JD.u).e = E.(F.u).e
= 
{definitions of .F and E}

9.3 THE OPERATIONAL INTERPRETATION 
OF TAIL RECURSION 
135
D.u.e o wk.r = wk.r
= 
{definition of D}
true .
For q := c; g, we observe
F.{D.u).{c;q)=E.{F.u).{c;q)
= 
{definitions of F and E}
D.u.(c; q) o wk.r = WSQ.C O F.u.q
= 
{definitions of D and F}
(WSQ.C O u.q) o wk.r = wso.c o (u.q o wk.r)
= 
{associativity}
true .
For q := h\ q, we observe
F.(D.u).(h;q)=E.(F.u).(h;q)
= 
{definitions of JF, i? and E\ let 5 range over body./i}
(inf 5 :: t/.(5;^)) o wk.r = (inf s :: u.(s; q) o wic.r)
= 
{equality of functions; let p range over IP}
(Vp :: (inf s :: u.(s; q)).(wk.r.p) = (inf s :: u.(s]q) o wk.r).p)
= 
{Theorem 4(3)}
true .
This concludes the proof of (11), and hence of (8), and hence of (4) and (6), that
is the wp-part of (1).
The wip-part of (1) is proved by completely analogous arguments, but DQ is
replaced by D\, least fixpoints are replaced by greatest fixpoints and sup-closedness
is replaced by inf-closedness. In this way we obtain wlp o k = wlp\A*. 
This
concludes the proof of formula (1).
9.3 The operational interpretation of tail recursion
In this section, we give the operational justification of defining wp and wlp of tail-
recursive procedures by means of extreme fixpoints of De (cf. Section 4.4). We
first formalize the concept of tail-recursion. Then the operational meaning of a
tail-recursive procedure h is defined in formula (15) below. Finally we show that,
via formula 6(1), this operational meaning induces the formal meaning defined in
4(24).
So, we work in the setting of Section 4.4, and assume that only tail recursion
occurs. Specifically, we assume that there is one procedure name skip G H with

136 
OPERATIONAL 
SEMANTICS 
OF RECURSION
(12) 
body, skip = e
and that all other procedure names h satisfy
(13) 
body./i C (5*;if) .
Here, (S*\H) is regarded as a subset of A* by means of definition 4(40), so that
indeed all nonempty subsets of (5*; H) are elements of A 0.
Remark. If we call the elements of H labels instead of procedure names, the program
we get can be regarded as a nondeterminate go-to program! (End of remark)
The operational semantics of the elements of H is defined by means of the
so-called configuration graph. This is the cartesian product of the state space
X with the set H of procedure names, extended with a point oo to represent
nontermination. So it is (X x H) U {oo} . It is made into a directed graph by
means of the transition relation 'â>' defined by
(14) 
(a, h) -> (y, k) = (3 s Â£ S* :: (s; k) â¬ body./i : {sj.y.x) ,
(x, h) â> oo = (3 (s; h) G body./i :: [5].00.a:) .
Informally speaking, a configuration (#, h) is interpreted as a state x in which pro-
cedure h is to be performed. A step (#, /i) â> (y, fe) with (5; fc) G body./i and [sj.y.x
corresponds to an execution of string s that terminates in state y where procedure
k is still to be performed. A step (x,h) â> 00 corresponds to a nonterminating
execution of 5 for some string (s;k) Â£ body./i.
We define relation ' 4 ' on I x if as the reflexive transitive closure of the
restriction of 'â>'. We define (x,h) -*â¢ 00 to mean the existence of a finite sequence
of transitions from (x, h) to 00 or the existence of an infinite sequence of transitions
starting in (x,h). 
Informally speaking, a finite sequence from (x,h) to 00 means
that after finitely many commands a simple command is encountered that need not
terminate. An infinite path from (re, h) means that the execution contains infinitely
many procedure calls.
The operational meaning of procedure h G H is defined as relation M.h in the
sense of Chapter 6, given by
(15) 
M.h.y.x 
= (x,h) -*> (y,skip) ,
M.h.00.x 
= (x,h) -*>- 00 .
In fact, the only way to reach termination is that the procedure still to be performed
is skip.
We claim that the operational meaning defined in (15) induces via formula 6(1)
the same meaning as the predicate-transformation semantics defined in Chapter 4.
More specifically, we claim that M.h = h for all h E if, or equivalently, that

9.3 THE OPERATIONAL INTERPRETATION 
OF TAIL RECURSION 
137
(16) 
wO.h = wp.h 
A wl.h = wlp.h
where
wO.h â wp.(M.h) 
, wl.h = wlp.(M.h) .
So, we want to prove formula (16). Since wp\H = wao is the least fixpoint of Do
and wlp\H = wb\ is the greatest fixpoint of Di, it suffices to prove that wQ and wl
are the least fixpoint of Do and the greatest fixpoint of D\, respectively.
We begin by calculating wO and wl. For h Â£ H and p G IP and x Â£ X we have
wO.h.p.x
= 
{definition}
wp.(M.h).p.x
= 
(6(1)}
-iM.h.oo.x 
A 
wlp.(M.h).p.x
-i((o:,/i) -^ oo) 
A 
wl.h.p.x
and
wl./i.p.Â£
= 
{definition}
= 
{6(1)}
(Vy : M.h.y.x : p.y)
(Vy : (^,/i) A (y,skip) : p.y) .
This proves
(17) 
wO.h.p.x 
= 
-i((a:,/i) -*> oo) 
A wl.h.p.x ,
wl.h.p.x 
= 
(Vy : (x,/i) A (y.skip) : p.y) .
In order to show that wO and wl are the extreme fixpoints of the functions De
for e G {0,1} , we need an analysis of these functions. It turns out that they can
be expressed elegantly in terms of the configuration graph.
(18) Lemma. For v G MTH, we have
(a) De.v.skip 
= identity G MT 
for e G {0,1} .
(b) For every h G H\ {skip} and p G IP and x e X:
Dx.v.h.p.x 
= 
(Vy, fc :: (a, /i) -*- (y, fc) : v.k.p.y) ,
Do.v.h.p.x 
= 
-i((x,h) â> oo) 
A (Vy, A; :: (#,/i) â> (y, A:) : v.k.p.y) .
Proof. Part (a) is proved in

138 
OPERATIONAL 
SEMANTICS 
OF RECURSION
= 
{4(21) and (12)} 
(wse U v)e.e
= 
{4(15)} 
identity 
.
(b) For D\, we observe that
D\.v.h.p.x
= 
{4(21) and (13)}
(\/s eS*,keH: 
(s; k) G body.ft : (wSl U t;)0.(s; Jfe).p).s
= 
{4(15) and 1(1)}
(Vs e S*,k e H : (s- k) G body.ft : wsj.3.(v.fc.p).a?)
= 
{ws* = wip|5* and 6(15)}
(V3 e S*,k eH : (s; k) G body./i : wlp.[5j.(t;.ife.p).ar)
= 
{6(1)}
(VÂ« eS*,keH: 
(s; k) G body./i : (Vy : [*].y.s : v.fc.p.y))
= 
{calculus, compare exercise 1.1.4(b)}
(Vy e X,k e H : (3s e S* :: (s]k) e body./i A {sj.y.x) : v.k.p.y)
= 
{definition (14)}
(Vy, k :: (x, h) -> (y, fc) : v.k.p.y) .
The proof for Do is analogous. Since it is not illuminating it is omitted.
(End of proof)
We now prove that wO and wl are fixpoints of function DQ and D\, respectively.
As for wO, it suffices to show that for every procedure name h G H, every predicate
p G P , and every state x G l
(19) 
wO.h.p.x 
= 
Do.wO.h.p.x .
In view of Lemma (18), this requires a case distinction. For h = sicip, it suffices to
observe
= 
{(17)}
-*((x,skip) 4 o o ) 
A (Vy : (a:,siip) -^ (y,skip) : p.y)
= 
{(a:,sicip) has no transitions, by (12) and (14)}
p.x
= 
{Lemma (18)(a)}
Do-wO.skip.p.x .
For /i ^ sicip, we have
wO./i.p.cr
= 
{(17)}
-i((a;, 
fc)4oo) 
A (Vy : (x, ft) A (y, sicip) : p.y)
= 
{definition '-*>' and k ^ sicip}

9.3 THE OPERATIONAL INTERPRETATION 
OF TAIL RECURSION 
139
->((#, h) â> oo) 
A
<yz,k:(x,h)->(z,k):
->({z,k) -*> oo) 
A (Vy : {z,k) â¢* (y,skip) : p.y))
= 
{(17)}
-â¢((#,/&) â> oo) 
A 
(Vz,k : (#,/i) â> (z, &) : wO.&.p.z)
= 
{Lemma (18)(b)}
Do.wO.h.p.x .
This proves formula (19), and hence Do-^O = wO. The proof of D\.wl = wl can
be obtained from the proof for wO by deletion of the lefthand conjuncts.
It remains to prove that wO and wl are the extreme fixpoints. So we have to
prove that for every v Â£ MT
D\.v = v 
=^ v < i(;l ,
Do-^ = v => t^O < t? .
This means that for all procedure names h Â£ H and all predicates p Â£ IP:
(20) 
2?i.v = i; => [v.h.p 
=> wl.h.p] 
and
(21) 
D0.v = v 
=> [w;0.fe.p 
=> v./i.p] .
We begin with the proof of (20). We prove the righthand side of (20) under as-
sumption of the lefthand side. It suffices to observe that for every state x Â£ X
wl.h.p.x
(Vy â¬ X : (x,h) -*> (y,skip) : p.y)
= 
{v = D\.v yields v.skip = D\.v.skip = identity by (18)(a)}
(Vy Â£ X : (x, h) -*> (y.skip) : v.skip.p.y)
<= {generalization}
(VyeX,kâ¬H: 
(x, h) A (y, k) : v.k.p.y)
<= {induction with Lemma (18)(b), using Di.v = v}
v.h.p.x .
The proof of (21) is more difficult. Again we prove the righthand side under
assumption of the lefthand side, which is Do-^ = v. We begin with massaging the
goal:
(22) 
[wO.h.p => v.h.p]
= 
{(17)}
(Vx :: ->((a:,/i) A oo) A wl.h.p.x 
=> 
v.h.p.x)
= 
{predicate calculus}
(Vx :: âiv.h.p.x A wl.h.p.x 
=$> 
(x,h) -^ oo)

140 
OPERATIONAL 
SEMANTICS 
OF RECURSION
(Vx::R.(x,h) => (i,fc)4oo)
where predicate R.(x, h) is defined by
R.(x, h) = 
-iv.h.p.x 
A (Vy : (a?, ft) -*â¢ (y, sicip) : p.y) .
The last formula of calculation (22) now suggests an inductive construction of a
diverging execution sequence. The first step is to show that termination has not
yet been reached:
(23) 
R.(x, ft) => ft ^ skip ,
as is proved in
i?.(#, skip)
=> 
{-*>â¢ is reflexive}
-*v. skip.p.x 
A p.x
=> 
{calculus}
v.skip / identity
= 
{DQ.V = v and Lemma (18)(a)}
false .
We now show that predicate R leads to immediate nontermination or can be kept
valid in the first execution step, that is,
(24) 
R.(x,h) 
= â¢ (a:,h) -* oo V (3z,k: 
( a , h ) -> (^,Jb) : R.{z,k)) 
.
This is proved in
R.(x,h) 
A -<(z,/i) -> oo)
=> 
{definition i?, Â£)o-^ = v and (23)}
h ^ sicip A -â¢((Â£, /i) â> oo)
A ->D0.v.h.p.x 
A (Vy : (x,h) -> (y.skip) 
: p.y)
=> {Lemma (18)(b) and calculus}
(3^,fc : (xyh) â> (^,fc) : -^v.k.p.z)
A (Vy : (a;, ft) -*> (y, sicip) : p.y)
=4> {if a -> ^ and /? A 7 then a -^ 7}
(3 2r,A: : <a:, fc> â <2r, *> :
-yy.k.p.z A (Vy : (z, fc) -^ (y.skip) : p.y))
= 
{definition R}
(3z,k: (x,h) -* (z,k) : R.(z,k)) .
By mathematical induction, it follows from (24) that
R.{x,h) 
=> (z,ft) -^ 00 .
By calculation (22), this concludes the proof of (21), and hence of (16).

9.4 GENERAL OPERATIONAL SEMANTICS 
141
9.4 General operational semantics
We now combine the construction of Section 9.3 with the ideas of Sections 9.1 and
9.2, to define the operational semantics of an arbitrary declaration body G H â>
AÂ®.
In Section 9.2, the declaration is shown to be equivalent to a tail-recursive
declaration of a set K of procedure names k.q with q G A* given by
(25) 
body.(k.e) 
= e ,
body.(k.(c;q)) 
= c;k.q ,
body.(k.(h; q)) = ( Q s G body./i :: k.(s] q)) ,
for all simple commands c G 5, procedure names h Â£ H, and strings g G A*.
If we compare this with Section 9.3, we see that k.e G K plays the role of
skipy cf. formula (12). Since e G 5* satisfies (e;r) = r for all commands r, we
have body.(Â£.#) C (S*;/lT) for all # G A* \ {e}. This shows that the conditions of
Section 9.3 are met. Therefore, we can define the operational semantics by means
of a configuration graph (X x K) U {oo} . Since k G A* ââ¢ K is a bijective function,
we can replace X x K by X x A*. It follows from the formulae (14) and (25),
together with
{ej.y.x 
= (x = y) ,
that transition relation 'âÂ»', when transferred to X X A*, satisfies
(3 c G 5 :: q = c; r A [c].y.x)
V (3 /i G H, s G body.h, t G A* :: ^ = h\ t A r = 5; * A # = y) .
The set (X x A*) U {00} with relation 'â>' is called the configuration graph of
general recursion.
We define 4-V and c-*> 00' in same way as after formula (14). The operational
meaning of q G A* is defined as relation M.q given by
M.q.y.x 
= 
(a:, 9) A (y,e) ,
M.q.00.x 
= 
(^,^) -*> 00 .
For every g G 4*, we now obtain
M.q
= 
{(16)} k.q
= 
{(1)} Â«â¢
So, indeed, the operational semantics is in agreement with predicate-transformation
semantics as defined in Chapter 4.

142 
OPERATIONAL 
SEMANTICS 
OF RECURSION
9.5 Exercises
Exercises of Section 9.3.
Exercise 0. Prove the equivalence for Do in Lemma (18)(b).

CHAPTER 10
PROCEDURE SUBSTITUTIONS
10.0. In this chapter, we develop the concept of substitution in command expres-
sions. In spite of the presence of unbounded choice, substitution is an easy concept
and it has the usual properties. Substitution is used to describe and justify a pro-
gram transformation that consists of the introduction of mediating procedures. In
later chapters, we are mainly interested in two special cases: substitutions in which
procedure names are replaced by their bodies, and substitutions in which procedure
names are replaced by abort.
10.1 Substitutions
In this section we introduce substitution of procedure names. The simple commands
s Â£ S are always unchanged. For the sake of flexibility, we allow substitution of
symbols from other sets than H.
Let / Â£ K â> AÂ® be a given function, where K is a set disjoint from 5. Put
U = S U K. For a 'command' c Â£ UÂ®, the substitution fÂ®.c is constructed from c
by replacing every symbol k Â£ K in expression c by f.k.
The formal definition is that / 0 Â£ UÂ® â> A0 is the unique function that
satisfies
(0) 
(Vc Â£ S :: / 0.c = c) A (Vifc Â£ K :: fe.k = f.k)
A (Vc, deUÂ® :: /0.(c; d) = / 0.c; fÂ®.d)
A ( V C : 0 ^ C c t / 0 : : / 0.( | C) - ( Q c Â£ C :: /0.c)) .
The construction of / 0 is analogous to the construction of function wÂ® in Section
4.3. A formal proof that formula (0) has precisely one solution, / 0 , is left to the
reader.
It is convenient to have a special notation for concrete substitutions in the case
K â H. For example, if ft, k are different elements of H, and q, r Â£ A0, the

144 
PROCEDURE 
SUBSTITUTIONS
double substitution [q/h,r/k] 
is defined by [q/h,r/k] 
= / 0 G A 0 ââ¢ A 0, where
/ G # â> A 0 is given by
f.h = q , /.& = r ,
/.Â£ = a: for all a? G # \ {ft, A;} .
We use analogous notations for single substitutions and for other multiple substi-
tutions.
10.2 Substitution commutes with extension
Recall from definition 4(25) that for v G MTH
ve = (wse U v)e 
Â£AÂ®^>MT.
Similarly, for v E K -> MT, we define ve = (wse U v) 0 G f/0 
-> MT. 
Let
/ G K â> A 0 be a given function and let / 0 G f/0 
â* AÂ® be the associated
substitution function, cf. formula (0).
We claim that for a function from H to the set of the positively conjunctive
predicate transformers, substitution commutes with extension. More specifically,
we claim that, for every function v G H â> MP, we have
(1) 
ve o / 0 = (ve o ff 
eUÂ® ^MP 
.
This is proved by structural induction over C/0. Function (wse U v) is an element
of A â> MP. By Theorem 4(42), it follows that function ve = (wse U u) 0 is a
homomorphism in A 0 â> MP. It follows with (0) that v e o / 0 is a homomorphism in
C/0 â> MP, Similarly, (veof)e is a homomorphism in !70 â> MP. A homomorphism
in /70 â> MP is completely determined by its restriction to U. Therefore, it suffices
to prove that
ve o fÂ®\U = (ve o f)e\U .
Now U = S U K and both functions restrict to wse on the set 5 and restrict to
ve o f on the set K. Therefore, they are equal.
10.3 Procedure abstraction is allowed
In the remainder of this chapter we develop a theorem concerning the declaration
of intermediate procedures. This result is subsequently used in a classical example
of program transformation. The result is not used in later chapters.
If some procedure body contains a complicated command expression #, one
may replace that expression by a new procedure name kl with body q. This means
that the old declaration looks like

10.3 PROCEDURE ABSTRACTION IS ALLOWED 
145
body./iO 
= 
[q/kl].r ,
where command r may contain the symbols kl and /iO, and q may contain hO. In
the new declaration, we use kO as the new name of hO and obtain
body.&O 
= 
[kO/hO].r ,
body.fcl 
= 
[kO/hO].q.
Intuitively, it is obvious that such a transformation should be correct in the sense
that hO = kO. This is proved, formalized and generalized, as follows. In the
formalization, we use a function i to generalize the renaming hO iâ> fcO, and a
function / to generalize kO iâ> hO and kl Â»-> q.
Let A^ be disjoint from S and let U = S U K. Let the symbols in H and A be
regarded as procedure names declared by
body H 6 H -> A 0 and body K E A -* !70 .
(2) Theorem. Let i E if -> JRT and / E AT -> A 0 be such that
(a) 
f o i = identity of H ,
(b) 
body# = / 0 o body^ o i ,
(c) 
body^|Z = i 0 o f\L where L = K \ Image(i) .
Then i.h = h for all h E H and /.* ^ fc for all ib E if.
Proof. The set L is the complement of the image of function i. It follows that, for
functions wO and wl on A", we have
(3) 
wO = wl = 
wOoi = wloi 
A wO\L = wl\L .
For any function v E H â> MP, we can define w; = ve o / E AT â> MP.
Conversely, for any function w E AT ââ¢ MP, we can define v = woiÂ£Hâ> 
MP.
Both correspondences are monotone. Moreover, for functions v E H â> M P and
z/; E A â> MP, we have
v = t?e o b o d y # 
A w = ve o f
= 
{(b)}
v = ve o fÂ® o body / c o i 
A w = ve o f
= 
{ ( l ) a n d ( 3 ) }
t; = w e o b o d y ^ - o i 
A ^ | L = v e o f \ L 
A woi = 
veofoi
v = we o body^- o i 
A it;|L = ve o /|X 
A w o i = v
= 
{third conjunct and (1)}
u; o i = u;e o body^ o i 
A iy|i = we o z0 o /|L 
A u
= 
{(c)and(3)}

146 
PROCEDURE 
SUBSTITUTIONS
For e = 0, this calculation shows that the smallest solution of the fixpoint equation
v = vÂ° o body# is w o z, where w is the smallest solution of the fixpoint equation
w = wÂ° o body^, which happens to satisfy w = vÂ° o /. By definition 4(24), it
follows that wp.h = wp.(i.h) for all h G H and wp.k â wp.(f.k) for all k E K. If
we use biggest solutions, the same calculation yields that wlp.h = wlp.(i.h) for all
h G H and wlp.k = wlp.(f.k) for all k e K. (End of proof)
Remark. Theorem (2) has a flavour of symmetry, but we could not find a useful
symmetric generalization. (End of remark)
Example. Let procedures g and h be declared by
body.g 
= 
(c\q\r;g)
body./i 
= 
r;{c\q\h)
where c,q,râ¬ 
5Â®. We use two applications of Theorem (2) to prove that (r; g) = h.
In both cases, we use the set K = {fcO, &1} with the declaration
body.H) = 
(c||g;fcl)
body.fcl 
= 
r;kO .
One can use one application of Theorem (2) with H = {g} and i.g = kO and
f.kl = (r\g) to prove that (r;#) = kl. Another application of Theorem (2) with
H = {h} and i.h = kl and f.kO = (c [] q\h) can be used to prove that kl = h.
This proves (r; g) = h. The remaining instantiations and verifications are left to
the reader. (End of example)
10.4 A classical example
In [de Bakker-de Roever 1973] p. 183, a tree traversal problem is described, as an
application of the so-called mu-calculus. The task is to perform an action A in
the nodes of the trees of a forest. Let, for any node, s(x) be interpreted as 'has x
a son?' and b(x) as 'has x a brother?'. Let S(x) be: 'visit the first son of x\ and
B(x): 'visit the first brother of x\ and F(x): 'visit the father of x\ Let procedures
hO, hi, h2 e H be declared by
(4) 
body./iO = 
A;(?-is|?3;5;/iO;F);(?-i6||?6;B;fcO),
body./a 
= A; (?-* Q ts\ 5; hi; h2; F) ,
body./i2 = 
(?-.& | ?6; B; hi; h2) .
The problem is to prove that hO = (hi; h2).
This can be done by means of two applications of Theorem (2). In fact, let kO,
kl, k2 be declared by

10.5 EXERCISES 
147
(13) 
body.fcO 
= 
fcl;jfc2,
body.&l 
= 
A; (?-* | ?s; 5; H); F) ,
Put A" = {fcO, jfel, k2}. We use Theorem (2) to show that hO = JfcO. To this end we
define i G {M)} -> AT by i./iO = &0 and / G JRT -> A 0 by
/.JfeO = fcO ,
/.Jb2 = 
(?-ifc|?6;J
The verification of the conditions of Theorem (2) is immediate. This shows that
hO = fcO. By another instantiation of Theorem (2) with i G {fel,fc2} -> iiT and
f.kO = (hl;h2), 
we get hi = jfel and h2 = k2. By declaration (13), we have
kO ^ fcl; fc2. This proves that /iO ^ /il; /i2.
Remark. In the mu-calculus of [de Bakker-de Roever 1973], mutual recursion is
expressed in terms of simple recursion and procedure parameters. Therefore, our
treatment of this example cannot be completely faithful to the original problem.
On the other hand, semantic equivalence in loc.cit. is only equivalence for wlp. So,
our assertion of semantic equality is stronger. (End of remark)
10.5 Exercises
Exercises of Section 10.3.
Exercise 0. Complete the proof of the example.
Exercise 1. Let G b e a subset of H such that body.g G (5 U G) 0 for all g G G.
(a) Prove that wp\G is the smallest solution of the equation in v G MX
v = (ws0 U v)Â° o (body|G)
and that wlp\G is the biggest solution of the equation in v G MT
v = (wsi U v)e o (body|G) .
Hint: use Theorem (2).
(b) Let K = H \ G. Prove that wp\K is the smallest solution of the equation in
veMTK
v = (ws0 U (wp\G) U v)e o (body|if)
and that wlp\K is the biggest solution of
v = (ws1 U (wJp|G) U v)e o (body|if) .
Remark. 
The set G can be regarded as a layer of lower level procedures used
as a foundation for the upper level procedures in K. 
Part (a) asserts that the

148 
PROCEDURE 
SUBSTITUTIONS
interpretation of the elements of G is independent of the existence of the upper
level K. 
Part (b) asserts that the interpretation of the upper level K does not
change if the lower level procedures (in G) are treated as simple commands. This
justifies the assertion in Section 2.6 that lower level procedures may be treated as
simple commands. (End of remark)

CHAPTER 11
INDUCTION AND SEMANTIC EQUALITY
11.0. In this chapter we announce a version of the induction rule of De Bakker and
Scott, cf. [Manna 1974] 5.5 and [de Bakker 1980] 7.11. The rule is stated here in a
form that differs considerably from the forms in these books. In our opinion, the
formalism to be described is more convenient for applications to program transfor-
mation. The present version is somewhat more general than our previous versions
in [Hesselink 1989a] Theorem (40) and [Hesselink 1990] Section 5.
The semantics of the simple commands s â¬ S are given by the functions ws0 =
(wp\S) and wsi = (wip|5), regarded as known. It is convenient to postulate the
existence of a simple command abort G S as defined in 1(9). We define abort G
H -> A 0 by
(0) 
abort.h = abort 
for all h G H.
By Section 10.1, there are induced substitution functions body0, abort0 G A 0 â*
A0. It is easy to see that body0 and abort0 restrict on the set S 0 to the identity
function, and that abort0.q G S 0 for all q G A0.
The idea of the induction rule is to give sufficient conditions on a set E of pairs
of commands such that q = r for all pairs (#, r) G E. The principal condition will
be that E is somehow stable under unfolding, where the unfolding of q G A 0 is the
command body0.# which consists of expression q with all procedures replaced by
their bodies. The base case of the induction will consist of semantic equalities of
the form:
abort0.q 
= 
abort0.r 
for all pairs (q,r) G E .
After some further technical preparations, the main result is postulated in Section
11.3. The remainder of the chapter consists of examples and comments. The
constructions and proofs are postponed to Chapters 12 and 13.

150 
IND UCTION AND SEMANTIC EQ UALITY
11.1 Congruences
In order to prove semantic equalities q = r, we need other relations '~' that share
the main algebraic properties of '='. Therefore, we define a congruence on A 0 to
be an equivalence relation '~' such that
(1) for all q, r, s, Â£ we have q ~ r 
A s~tf 
=>â¢ #; s ~ r; tf ,
(2) for every pair of nonempty families (i El:: c.i) and (i El:: d.i) in A 0:
(Vi G i :: c.i ~ d.i) 
=> ( Q z :: c.i) ~ ( | i :: cf.i) .
By convention, we let a binary relation 'Â«' be identified with the set of pairs
(<) given by
(?,r)G(Â«) 
= 
q<r.
For a function to E A 0 â> MT, the equalizer Eq.w is defined as the binary relation
on A 0 given by
(3) 
(#>r) â¬ Eq.w; = 
w.g = w.r .
A straightforward calculation shows that, if w is a homomorphism, then Eq.w
is a congruence on A 0. It is easy to see that any intersection of congruences is a
congruence. Since (=) is the intersection of the congruences Eq.wp and Eq.wip,
it follows that (=) is a congruence. So, indeed, the concept of congruence is a
generalisation of semantic equality.
11.2 The set Lia
We need a subset Lia of A 0 that occurs in a condition in a proof rule. Its name
refers to 'linear approximation'. The definition of Lia is very technical and therefore
postponed to Section 13.6. In order to show that Lia has enough elements, we give
some sufficient conditions, the validity of which is proved below in Sections 13.7
and 13.8. In part (d), we use the subset Sy.Muc of A 0 defined and investigated in
Section 8.4. Knowledge of Sy.Muc is not necessary for understanding the theory
of this chapter. In the applications where Sy.Muc is needed, we shall only use
Theorem 8(20).
(4) Theorem, (a) i f U 5 0 C Lia .
(b) If #, r E Lia then }[rG Lia.
(c) If / Â£ Ix and q.t E Lia for all t E I, then q.f E Lia.
(d) Let q, r E Lia. If q E Sy.Muc 
or r E SÂ® then q\r E Lia.

11.3 THE STRONG CONGRUENCE 
151
By (a), Lia contains S 0 and all procedure names. It is stable under finite choice
(cf. (b)) and deterministic infinite choice (cf. (c)). We must be careful with the
composition, cf. (d).
Remark. The antecedent of the implication in part (d) is nasty. In 11.7, we give
an example to show that it cannot be replaced by true. On the other hand, each of
the two disjuncts of the antecedent is useful, as is shown in 11.5 and the example
of 11.3. It is not known whether Sy.Muc is contained in Lia. The sets Sy.Muc and
Lia are not closed under semantic equality. See the concluding example in Section
8.4.
11.3 The strong congruence
We now present the induction rule to prove semantic equalities q = r for commands
q and r that may contain recursive procedures. The underlying definitions and
justifications are postponed.
In Section 12.3, we construct a congruence 'Â«' on A0, which is called the strong
congruence and has the following two properties:
(5) Semantic Rule. (V?, r E A0 : q Â« r : q = r) .
(6) Accumulation Rule. Let E be a binary relation on A0 such that
(V(?,r) E E :: abort0.q ^ abort0.r 
A ?,r E Lia)
and that, for every congruence ~ on A0 with E U (~) C (~), we have
(V(?,r) E E :: body0.? - bodyÂ°.r) .
Then q Â« r for all pairs (?, r) E E.
Remark. The classical induction rules mentioned in 11.0 axe closer to the following
version.
Induction Rule. Let E be a binary relation on A0 such that
(V (?, r) E E :: abort0.? ^ abort0.r 
A ?, r E Lia)
and that, for every congruence ~ on A0 with Â£ C (~), we have
(V(?,r) E E :: body0.? - body0.r) .
Then q = r for all pairs (?, r) E E.
It is easy to see that the Induction Rule follows from the combination of (5)
and (6). The combination of (5) and (6) is methodologically more convenient than
the Induction Rule, for it allows us to accumulate knowledge concerning 4Â«' that

152 
INDUCTION AND SEMANTIC 
EQUALITY
can be used in subsequent investigations. On the other hand, the accumulation
rule becomes invalid if relation (Â«) is replaced by (=), see exercise 11.3.3. (End of
remark)
Example. Let c, q, r â¬ S 0 be commands. Let procedures g and h E H be declared
by
body.# 
= 
{q\g\ c;r) ,
body./i = 
(q; h [] c) .
Then we have g & h;r. This is proved by means of rule (6) with E = {(g , h; r)}. In
fact, by Theorem (4)(a,d), commands g and (h; r) are elements of Lia. Moreover,
abort0.^ ^ abort0. (h\r)
= 
{definition of abort}
abort 
= abort] r
= 
{1(14)}
true.
Finally, let ~ b e a congruence such that E C (~), that is g ~ h; r. Then
body0.(/>;r)
= 
{declaration /i}
= 
{distributivity}
q\h\r\c\r
~ 
{assumption; ~ is a congruence}
^;^ I! c',r
= 
{declaration g}
Therefore, accumulation rule (6) implies that E C (Â«), that is ^ Â« h;r. By rule
(5), it follows that g = h;r.
This example goes back to [de Bakker-de Roever 1973] Lemma 4.4, where equiv-
alence for wlp is proved in the case c = e. We do not know whether, alternatively,
Theorem 10(2) is strong enough to prove that g = h;r. (End of example)
The theory of this section is usually described as computational induction.
With respect to classical forms of computational induction, cf. [de Bakker 1980]
7.11 and [Manna 1974] 5.5, the main difference is the condition in rule (6) that
#,r G Lia. Classical forms of computational induction have the condition that
all commands involved are deterministic or at most finitely nondeterminate. Our

11.4 SEMANTIC DEFAULT RULES 
153
condition q,r G Lia is much weaker. If conjunct q,r G Lia is omitted from rule (6),
the rule is no longer valid. This is shown in Section 11.7 below.
11.4 Semantic default rules
Rules (5) and (6) can be used to prove some important additional rules. For exam-
ple, we have
(7) 
( V g , r e Se :: q&r 
= q = r) .
This equivalence is proved by mutual implication. Formula (5) yields ' =>'. For
the other implication, we use rule (6) with for E the set of the pairs (<?, r) with
q,r G SÂ® and q = r. By (4) and 10(0), every q E SÂ® satisfies bodyÂ°.g = q and
abort0.q = q and q G Lia. Now the condition of rule (6) is easily verified, so that
E C (Â«). This concludes the proof of (7). By a similar verification one can prove
(8) Corollary. For every q G A0:
miracle] q Â« miracle , abort] q Â« abort ,
skip] q ~ q , q] skip w q , q | miracle Â« g .
11.5 An application: the storage of a parameter
We now have enough material for an application. Some other applications are
given in [Hesselink 1989a] and [Hesselink 1990] Sections 6 and 7. The present
application was suggested by the result of Section 9.2. There, general recursion was
implemented by means of one tail-recursive procedure with an input parameter.
The next step is to implement an input parameter of a tail-recursive procedure by
means of a memory location.
A procedure with an input parameter is formalized as a family of procedures
indexed by the parameter. So, we let (i G / :: h.i) be a nonempty family of
procedures in H declared by
body.(h.i) 
= q.i | ( [ j G / :: r.i.j] h.j) ,
where q.i and r.i.j are element of 5 0 . Our purpose is to replace the parameter i
by a memory location. Therefore, we assume that there is a program variable y of
type / which is independent of the commands q.i and r.i.j. Family (i :: h.i) is now
replaced by the family (i :: hl.i) declared by
(9) 
body.(hl.i) 
= 
(y:=0;(9.*ll(llie/::r.*.i;/il.j)).

154 
INDUCTION AND SEMANTIC 
EQUALITY
Here the only change is that program variable y holds the parameter value of the
latest call of hi. The formalisation of this statement is left to the reader. We want
to implement hi by means of the parameterless tail-recursive procedure g G H
declared by
(10) 
body.tf 
= 
( I t 6 / :: ?(y = Â«); (?.* Q ( \ j G I :: r.i.j] y := j;g))) 
.
We claim that for all i G /:
(11) 
hl.i 
* 
(y : = * ; ? ) .
This can be proved as follows. We apply the accumulation rule (6) to the set
E of the pairs (ftl.i, (y := i\ g)) with i G I. For every i G / we have
abort0.(hl.i) 
= abortÂ°.(y := i; g)
= 
{(0)}
abort = y := i; abort
= 
{y := i is total}
true.
It follows from Theorem (4) that the three commands hl.i and y := i and g are
elements of Lia. Since y := i is a deterministic simple command, it is element of
Sy.Muc. Therefore, (y := i\g) G Lia by Theorem (4)(d). Let ~ b e a congruence on
A0 with E U (Â») C (~). For any i, we have
bodyÂ°.(y :=i;g)
= 
{10(0), (10)}
y := i; ( 1 * G / :: ?(y = fc); (q.k || ( | i G / :: r.fc.j; y := j ; </)))
~ 
{Â£^ C (~) and ~ is a congruence}
y := i; ( J k â¬ I:: ?(y - *); (Â«.* | ( 1 j â¬ J :: r.k.j; hl.j)))
= 
{distributivity}
( 0 k e I:: y := i; ?(y - *); (g.* J ( || j â¬ J :: r.*.j; hl.j)))
~ 
{in 5 0 we have y := i; ?(y = i) = y := i and
y := i; ?(y = fc) = miracle ii i ^ k; use (7) and (Â») C (~)}
y := i; (g.i || ( B j G / :: r.i.j] hl.j))
\{\k:k^i: 
miracle; (q.k Q ( | j G / :: r.fc.j; /il.j)))
- 
{(Â«) C (-), (8) and (9)}
body0.(/il.i) .
This concludes the verification of the condition in rule (6). So the rule implies that
E C (Â«). By formula (5), this proves that E C (=), thus proving formula (11).
Remark. The theory of 11.3 is not necessary for proving formula (11). An alterna-
tive proof can be given along the following lines. First, one shows that wp.(y := i] g)

11.6 COMPOSITIONALITY OF THE STRONG CONGR UENCE 
155
satisfies the defining equation for wp.(hl.i). Since wp.(hl.i) is the smallest solution
of that equation, this implies wp.(ftl.z) < wp.(y := i\g). Then one shows that
wp.g < wp.( D j :: ?(y = j); hl.j)
by proving that the righthand side satisfies the defining equation for the lefthand
side. As a third step, one verifies that
wp-(y == *; (D i â¢â¢â¢ ?(y = i); hi.j)) < 
wP.(hi.i).
An inclusion argument then yields
wp.(hl.i) 
= 
wp.(y : = i;g) .
The proof for wlp is analogous. This proves (11). If the theory of Section 11.3 is
available, the above proof of (11) is definitely shorter. (End of remark)
11.6 Compositionality of the strong congruence
Compositionality of 'Â«' with respect to recursive declarations is the following prop-
erty. Let HO and Hi be disjoint sets of procedure names, which are coupled by
means of a function / G Hi -> HO. Write AO = SUHO and Al = SUHl. Let / 0 be
the induced function A l 0 â> AO0. Let the declaration function body G HO â> AO0
and body G HI -Â» A l 0 be such that /0.(body.ft) Â« body.(/./i) for all h G # 1 .
Compositionality is the property that h & f.h for all h G HI. This is proved as
follows. We form H = HO U Hi and extend / to a function H â> if by taking
f.h = h for all /i G HO. Now we use the following result:
(12) Theorem. Let / G H â> H be a function such that
(13) 
(V/i G H :: /0.(body./*) Â» body.(f.h)) .
Then /i Â« f.h for all ft G H.
Proof. We apply the accumulation rule, (6), with for E the set of the pairs (ft, /.ft)
with ft G H. By (0) and (4), the first condition of (6) is satisfied. If ~ is a
congruence on A 0 with i ? U ( ^ ) c ( ~ ) , then
(Vft G H :: bodyÂ°.ft - bodye.(/.ft))
= 
{10(0)}
(Vft G H :: body.ft - body.(/.ft))
= 
{(13) and (Â«) C (-)}
(Vft G H :: body.ft - /0.(body.ft))
<= {generalisation}
= 
{structural induction on <?, using (1), (2), 10(0)}

156 
IND UCTION AND SEMANTIC EQ UALITY
(VheH 
::h~f.h)
true .
This proves the second condition of rule (6). Therefore E C (~). (End of proof)
Remark. Theorem (12) looks very innocent, but the analogous statement with
the strong congruence 'Â«' replaced by semantic equality '=' is false. The simplest
example is as follows. Let H = {/iO, hi} with the declaration
body./iO = skip ,
body./il = hi .
Let / G H -> H be given by f.hl = hO and f.hO = hO. Clearly, hi = abort and
hO = skip, so that hi ^ f.hl. Nevertheless, formula (13) with 'Â«' replaced by '=='
is satisfied, as is shown by
/0.(body./il) 
^ 
body.(f.hl)
= 
{declaration
fÂ®.hl 
Â£
= 
{definition / and definition 10(0)}
hO = body./iO
= 
{standard property: 4(19)}
true ,
and an even simpler calculation for hO. (End of remark)
11.7 The necessity of Lia
In this section we give an example to show that the condition #, r G Lia in rule (6)
cannot be omitted. The example also shows the existence of elements g, r G Lia
such that q\ r Â£ Lia. Since we have the disposal of unbounded choice, the example
is somewhat easier than the analogous example in [Hesselink 1990] Section 5.6.
Let there be an integer program variable i. Let command c be defined by
(14) 
c = 
( |] n G IN :: i :=n);h
where procedure h G H is declared by
body./i 
= ?(i < 0) [| ?(i >0);i := i - l ; / i .
It is easy to show that command c necessarily terminates. Therefore, c is not
semantically equivalent to c \ abort. In order to use rule (6), we observe that
(15) 
bodyÂ°.c
= 
{(14)}

11.8 EXERCISES 
157
( 1 n G IN :: i := n; ?(i < 0) || i := n; ?(i > 0); i := i - 1; h)
Â« 
{(8), split off n = 0}
(i := 0 [ miracle; i := i - 1; h) \ ( \ n : n > 0 : miracle [ i := ra â 1; ft)
Â« 
{(8)5 renaming}
Â± := 0 Q ( JnGlN::i:= n; ft)
= 
{(14), distributivity}
i := 0 I c .
We now apply rule (6) to the set E that consists of the pair (c, c [] abort). For every
congruence ~ on Ae with 25 U (Â«) C (~), we have
bodyÂ°.c 
~ 
bodyÂ°.(c [] abort)
= 
{(15), (Â«) C (-)}
(i := 0 I c) 
- 
(i := 0 Q c | abort)
= 
{EcH}
true.
On the other hand, it is easy to verify that
abort0.c 
Â£ 
abort0.(c ] abort) .
Since c is not semantically equivalent to c\ abort, rule (5) implies that E <Â£ (Â«).
Therefore, it follows from rule (6), that c (Â£ Lia or (cj abort) Â£ Lia. By Theorem
(4), we have abort G Lia, and hence c ^ Lia. Notice that c = cO; ft where cO =
( | n G l N : : i : = n ) . By Theorem (4), both cO and ft are elements of Lia.
11.8 Exercises
Exercises of Section 11.1.
Exercise 0. Prove that the equalizer Eq.w of a homomorphism w G A0 â> MT is
a congruence.
Exercises of Section 11.3.
Exercise 0. Let commands s, c â¬ S 0 and predicate b G IP be such that (cf. for-
mulae 5(5) and 5(6)):
5; ?6 ^ 
?b',s A 3; ?-.6 Â£ 
?-.ft;s.
Assume that command s is total and finitely nondeterminate. Let L G H be
declared by
body.L 
= 
(?-i6|?6;c;L) .
Prove that s;L = L; 3, cf. formula 5(7).

158 
INDUCTION AND SEMANTIC 
EQUALITY
Exercise 1. V 
Let / be a monoid, i.e. a set with an associative binary operator
V with a neutral element 1. Let v be a program variable of type I. Let c and d.i,
i Â£ /, be commands in S 0 such that for all i, k Â£ I
c;v := k 
= 
v := k;c ,
d.i\ v := k 
= 
v := k] d.i .
Let procedures g, h â¬ H be declared by
body.tf 
= c; v := 1 D ( || i G I:: d.i; g; v := i * v) ,
body./i = c J ( |] z Â£ / :: d.i; v := v * i; h) .
Prove that for all k Â£ /
<7;v:=fc*v 
= 
v:=k;h.
Exercise 2. Let q be a rational program variable and let s be a program variable
for sequences of integers. In exercise 2.5.4, procedure g was declared by
body.# 
=
( ?(q = 0) ; s := e
i?(q^0); ( | i : : ? ( 7 ^ T < q < } ) ; q : = i - q ; ^ ; 
s:=(j;s))),
where j ranges over the positive integers. Use the previous exercise to obtain a
repetition L with g = (s := e ; L).
Exercise 3. In the accumulation rule (6), the conjunct abort0.q = abort0.r
cannot be omitted. Prove this by taking E = {(h,skip)} where h Â£ H is declared
by body./i = h.
Exercise 4. 9? Consider the following variation of the Induction Rule:
Invalid Rule. Let E is a binary relation on A0 such that
(V(q, r) Â£ E :: abort0.q Â£ abort0.r 
A g, r Â£ Lia)
and that, for every congruence ~ on A0 with J5U(=)c(~), we have
(Vfor) Â£ E :: body0.^ ~ body0.r) .
Then q = r for all pairs (</, r) Â£ E.
Show that this rule is not valid, by considering E = {(Â£,M)} with L and M
declared by
body.Z 
= 
?-.6 Q ?6 ; c ; L ,
body.M 
= 
?-.6 Q ?6 ; M ,
say with 6 = (i ^ 0) and c = ( i : = i â 1) for an integer program variable i.
Exercises of Section 11.4.
Exercise 0. Prove corollary (8).
Exercise 1. Let a predicate b and commands 5, t Â£ 5 0 be given such that

11.8 EXERCISES 
159
?b;s 2* s;?b 
A ?-i6; s ^ s; ?-i6 .
Let repetitions LO and Ll in ff be given by
Â£0 : 
while 6 d o s;f od
LI : 
while b do tf; s od .
Prove that s; L\ = Â£0; 5 . (One can apply the results of the examples in 10.3 and
11.3, but there may be easier solutions.)

CHAPTER 12
INDUCTION AND REFINEMENT
12.0. Refinement is defined by regarding a command q as being refined by a
command r if r satisfies all Hoare-triple specifications of q, see Section 5.1. In
practice, we are often more interested in refinement than in semantic equality,
but the theory presented in Section 11.3 is not convenient for proving refinement
relations. Therefore, we develop a slightly stronger theory, which is adequate for
both refinements and semantic equalities. In this theory the role of congruences is
taken over by 'admissible preorders'.
The most important admissible preorder will be the refinement relation intro-
duced in Section 5.1. Recall from definition 5(0) that a command d is a refinement
of c (notation c El d) if and only if it satisfies every Hoareâtriple specification of c:
c n d 
=
(Vp,? :: ({p} c {q} =Â» {p} d {q} )
A (p {c} q => p {d} q)) ,
or equivalently (by 5(1))
(0) 
c Q d = 
(wp.c < wp.d) A (wlp.c < wlp.d) .
Here, relation '<' is the induced order on MT, cf. Section 4.1.
12.1 Admissible preorders
Recall that a preorder is a reflexive and transitive binary relation. We define a
preorder '<' on the set AÂ® of commands to be admissible if and only if
(1) for all q, r, 5, t we have: q <r A s <t 
=$> 
q;s <r\t ,
(2) for every nonempty set / and every pair of families {i G / :: c.i) and (i E / :: d.i)
in A 0:
(Vi G i :: c.i < d.i) 
=Â» ( Q i :: c.i) < ( | i :: d.i) .
If w G AÂ® â> MT is a homomorphism, the 'less-than-equalizer' Leq.w is
defined as the binary relation on A 0 given by

12.2 THE STRONG PREORDER 
161
(3) 
(#?r) â¬ Leq.w =. w.q < w.r .
Since '<' is a partial order on MT, relation Leq.w is a preorder on AÂ®. Actually,
it is an admissible preorder. This is proved as follows. Condition (1) is verified in
(g;$, r;t) G Leq.w
= 
{(3) and w is a homomorphism}
w.q o w.s < w.r o w.t
<= {Theorem 4(7)(c) and (3)}
(q,r) G Leq.w A (s,t) G Leq.w .
The verification of condition (2) is straightforward.
The intersection of a family of admissible preorders is an admissible preorder.
In particular, the intersection of Leq.wp and Leq.wlp is an admissible preorder.
This preorder is the refinement preorder ' C ' of formula (0).
12.2 The strong preorder
In this section we announce the existence and properties of a particular preorder
'<C\ which is called the strong preorder. The strong preorder is constructed in
Section 13.3 below. Its main properties are analogous to the properties of the
strong congruence, cf. Section 11.3, and are proved in Sections 13.3 up to 13.6.
They are as follows.
(4) Semantic Rule. (Vg,r G Ae : q < r : q C r) .
(5) Accumulation Rule. Let Â£ b e a binary relation on AÂ® such that
(V(tf,r) G E :: abort0.q C abortÂ°.r 
A q G Lia)
and that for every admissible preorder < on AÂ® with E U (<C) C (<) we have
(V(?,r) GÂ£::body0.gÂ«body0.r) .
Then g < r for all pairs (#, r) G E.
By the same arguments as used to prove formula 11(7), one can easily prove
that relation '<C' coincides with ' C ' on the set 5 0 of lower-level commands, that
is
(6) 
(Vq,reSÂ® :: q <Â£ r = q C r) .
12.3 Construction of the strong congruence
In this section we use the strong preorder postulated above to construct the strong
congruence of Section 11.3. We first observe that a straightforward verification
shows

162 
INDUCTION 
AND 
REFINEMENT
(7) L e m m a . Let 'Â«' be an admissible preorder. Let relation c~' be given by
q ~ r 
= q<r A r <q 
for all </, r .
Then ' ~ ' is a congruence.
Using Lemma (7), we define the strong congruence as the congruence given by
(8) 
q & r 
= g<CrAr<C# 
for all q, r .
Formula 11(5) is an immediate consequence of the formulae (4) and (0). The
accumulation rule 11(6) is proved in
(9) Theorem. Let E be a binary relation on AÂ® such that
(10) 
(V(q,r) e E :: abort0.^ Â£ abortÂ°.r 
A qyr G Lia)
and that for every congruence ~ on A 0 with E U (Â«) C (~) we have
(11) 
(V(?,r) e E :: bodyÂ°.g - body0.r) .
Then E C (Â«).
Proof. Let J51 be the smallest symmetric relation that contains JE7, SO that
(q,r)EEl 
= 
(q,r)â¬EV(r,q)eE 
.
By an easy calculation it follows from formula (8) that
(12) 
E C (Â«) 
= 
El C (<) .
Therefore, we apply the accumulation rule (5) to relation El. The first condition
of (5) on J51 follows from condition (10). In order to verify the second condition of
(5), let 'Â«' be an admissible preorder on A0 with
(13) 
Â£1U(<)C(Â«) .
Let ' ~ ' be the congruence (cf. Lemma (7)) given by
q rsj r 
= 
q<r A r <q 
for all qy r .
Then we have
J51U(Â») C(~)
= 
{calculus}
El C (~) 
A (Â«) C (-)
<= 
{-El is symmetric, definitions w and ~}
El C (<) A (<) C (<)
= 
{(13)}
true.
By assumption this implies condition (11):
(V(g,r) 6 El :: bodyÂ°.g - bodyÂ°.r) ,
and hence, by the definition of '~',
(V (q, r) e El :: body0.^ < bodyÂ°.r) .

12.4 COMMUTATION UP TO REFINEMENT 
163
By the accumulation rule (5), this proves El C (*<). Now we conclude with formula
(12). (End of proof)
12.4 Commutation up to refinement
Commutation is the phenomenon that the sequential composition of two commands
is semantically independent of the order of the commands. So, for a command s, we
are interested in the set of commands q with q\ s = s; q. Sometimes this requirement
is too strong. Then the question can be separated into two questions: q\ s C s; q
and s',q C. q;s. Condition q\s C. s;q means that in every program s;q can be
replaced by q\s. Similarly, s;q Q q^s means that q\s can be replaced by s;q. By
rule (5), the questions q; s C s\q and s; q C g; s can be replaced by q; s <C s\ q and
s\q <^i q-,s. In order to deal with these questions, we need the following definitions.
Let a subset U of A 0 be called a subalgebra if and only if
- if q, r G U then (q; r) G U,
- if V is a nonempty subset of U then ( J q G V :: q) G U.
It is easy to see that every intersection of subalgebras is a subalgebra. There-
fore, if T is a subset of AÂ®, the intersection of all subalgebras that contain T is a
subalgebra and hence the smallest subalgebra that contains T. This allows us to
define the subalgebra generated by T (notation gen.T) to be the smallest subalgebra
that contains T.
If s is a given command and 'Â«' is an admissible preorder, the set of commands
q with q\ s < s; q is easily seen to be a subalgebra. It follows that for any set U of
commands
(14) 
(VqeU::q',s<s;q)
= 
(Vq e gen.U :: q;s<s]q) 
.
Now the first part of the above question is dealt with in
(15) Theorem. Let s G 5 0 . Define T to be the subset of A 0 given by
(16) 
qET 
= 
qESe 
A q;s C s;q .
Let K be a subset of H such that
(17) 
(V/i G K :: body./* G gen.(T U K)) .
Then q; s < s; q for all q G gen.(T U K).
Proof. By formula (14), it suffices to prove that q;s <^ s;q for all q G T U K. By
(16) and (6), we have q\ s <C s; q for all q G T. So it remains to prove q\ s <C s; q

164 
INDUCTION 
AND 
REFINEMENT
for all q Â£ K. Therefore, we use the accumulation rule (5) with for E the set of the
pairs
{h',s , s\ h) 
with h Â£ K .
We first observe that (h;s) Â£ Lia for all h Â£ if, by Theorem 11(4). Now, the rest
of the first condition of rule (5) is verified in
abort0.(h; s) C abort0.(s; h)
= 
{definition abort 0}
abort; s C. s; abort
= 
{definitions in Chapter 1}
(Vp E IP :: [false => wp.s.false] A [true=$Â» wlp.s.true])
= 
{calculus and 3(5)}
true .
For the second condition of rule (5), it suffices to verify that for every admissible
preorder < with E U (<C) C (<) and any h E K we have
body 0.(h;s)< body 0.(s;h)
= 
{definition}
body./i; s < s; body./i
<= {(17)}
(Vg E gen.(T U K) :: g; 6 < s; g)
^ 
{(14)}
( V g E T U / f 
::q;sÂ«s;q)
= 
{(6), (16) and assumption on <}
true .
(End of proof)
Example. Let commands 6, q, r, t Â£ 5 0 be such that
g;,s C s;g A r;s C. s;r 
A Â£; s C s;t .
Let procedure h Â£ H be declared by
body./i 
= 
(g||r;/i;<).
Then h; s C. s; h. In fact, we clearly have g, r, t E T and hence
body./i E gen.(T U {/i}) .
(End of example)
In the other case, the result is less satisfactory, for we have to impose extra
conditions on command s.

12.4 COMMUTATION UP TO REFINEMENT 
165
(18) Theorem. Let s G 5 0 be total and finitely nondeterminate. Define U to be
the subset of A 0 given by
(19) 
qeU 
= 
qeSG 
A 
s',q\Zq;s.
Let K be a subset of H such that body.ft G gen.(U U K)) for all ft G if. Then
<*; <7 < 9S5 fÂ°r aU Q G gen.(U U JiC).
Proof. We use the same method as in the previous proof. Again, it suffices to prove
that s; q <C q] s for all q Â£ K. Let J5 be the set of the pairs
(s; ft , ft; s) 
with h E K .
Since 5 is finitely nondeterminate and element of S 0, we have (3; ft) G Lia for all
ft G if, by Theorem 11(4). The rest of the first condition of rule (5) is verified in
abort 0.(5; ft) C abort0.(ft; s)
= 
{definition abort 0}
s; abort C. abort] s
= 
{1(10)}
(Vp G IP :: [ wp.5.faise=>faise] A [ wlp.s.true => true])
= 
{s is total and calculus}
true .
The second condition of rule (5) is verified in the same way as in the previous proof.
Now the assertion follows from rule (5). (End of proof)
Remark. In Theorem (18), the extra conditions on s are essential. It is easy to see
that totality of s cannot be omitted, see the exercises. The next example shows
that finite nondeterminacy of s cannot be omitted. (End of remark)
Example. Let v be an integer program variable. Let command s be given as in the
example of 5.4, so that
s = (?(v > 0) B ?(v < 0); ( J m : m > 0 : v := m)) .
Clearly, command s is total but not finitely nondeterminate. We may assume that
s G S 0. Let T and U be given by formulae (16) and (19). We consider predicate
b â (v 7^ 0) and commands ?6, ?-i6, and
c = (?(v < 0) J ?(v < 0); v := 0 | ?(v > 0); v := v - 1) .
As verified in the example of Section 5.4, the commands c, ?6 and ?->& are element
of T fl U. Let repetition L be declared by
body.X 
= 
(?-.6||?6;c;L).
Then we have
body.L G gen.((T DU)U {L}) .

166 
INDUCTION AND 
REFINEMENT
By Theorem (15), it follows that 
L\s 
C. s;L. 
In Section 5.4, we proved that
-1(5; L Q L; s). Therefore, in Theorem (18), we cannot omit the assumption that
s be finitely nondeterminate. (End of example)
12.5 Exercises
Exercises of Section 12.4,
Exercise 0. Let b G IP and c, s G S 0 be such that c; s Q s\c and that
[b=$> wp.s.b] 
A [->&=> wp.s.(-i&)] .
Prove that L = while b do c od satisfies L\ s Q 5; L. Compare Section 5.4 and
the exercises 5.1.4 and 11.3.0.
Exercise 1. Show that in Theorem (18) totality of s cannot be omitted, in the
following way. Put s =? false.
(a) Prove that 5; q C g; s if and only if # is always terminating.
(b) Let U be given by formula (19). Give a declaration of a procedure h such that
body./i â¬ gen.(U U {/&}) and that -Â«(,s; <j C q\ 5), and hence -1(5; q <C #; 5).

CHAPTER 13
THE STRONG PREORDER
13.0. In this chapter we fulfil the remaining proof obligations of Chapter 12.
Section 13.1 contains a strengthened version of Theorem 4(8), our version of the
theorem of Knaster-Tarski. In Section 13.2, we provide the basic set-up, in which
we need not yet distinguish between wp and wlp. Section 13.3 contains the con-
struction of the strong preorder and the proofs of rule 12(4) and a variation of rule
12(5). In this way, the proof of the accumulation rule 12(5) is reduced to the ver-
ification of two technical conditions: sup-safety (for wp) and inf-safety (for wlp).
These conditions comprise the base case of the induction and a continuity property.
In Section 13.4, the base case is reduced to a condition on function abort0.
Section 13.5 contains the proof for inf-safety. Section 13.6 contains the definition
of the set Lia and the proof for sup-safety. In Sections 13.7 and 13.8 we justify the
rules for Lia stated in Section 11.2.
It may seem unsatisfactory that, in the presence of unbounded nondeterminacy,
computational induction needs such a complicated theory. The examples in Sections
11.7 and 12.4, however, show that the accumulation rules 11(6) and 12(5) need their
complicated conditions. Therefore, corresponding complications must occur in the
construction or in the proofs.
13.1 Intermezzo: an extension of the theorem of KnasterâTarski
Recall from Section 7.2, that a subset I of a complete lattice W is called a chain
if and only if
(V#, y G L :: x < y V y < x) .
A subset V of W is called sup-decked if and only if (sup L) Â£ V for every chain L
in V. Similarly, V is called inf-decked if and only if (inf L) Â£ V for every chain L
in V. Notice that every sup-closed subset of W is sup-decked, but not vice versa,
and similarly for inf-closed and inf-decked.

168 
THE STRONG 
PREORDER
Example of decked and closed sets. Let W be the set of the subsets of the plane
IR2, ordered by inclusion. Now, suprema are unions and infima are intersections.
An element w G W, i.e. a subset w of IR2, is called convex if and only if, for every
two points x,y Â£ w, the line segment from x to y is contained in w. Let V be the
subset of W that consists of the convex elements of W. Now, V is inf-closed in
W and sup-decked, but not sup-closed: in fact, any intersection of convex sets is
convex, any union of a chain of convex sets is convex, but an arbitrary union of
(say two) convex sets need not be convex. (End of example)
We need the following version of the theorem of Knaster-Tarski, which extends
Theorem 4(8).
(0) Theorem. Let W be a complete lattice and let D : W â> W be a monotone
function. Let V be a subset of W that is D-invariant, i.e. (Vv G V :: D.v G V).
(a) If V is sup-decked then V contains the least fixpoint of D.
(b) If V is inf-decked then V contains the greatest fixpoint of D.
Proof. A self-contained proof falls outside the scope of this book. In [Hesselink
1990] Theorem 2(10), we gave a complete and elementary proof. Prompted by
Jaap van der Woude, we here only provide a short proof for readers acquainted
with Zorn's Lemma. In general, Zorn's Lemma (see e.g. [Gallier 1987] p. 9) implies
that every sup-decked subset U of an ordered set contains a maximal element,
i.e. an element u G U with (V# G U : u < x : u = x). By symmetry, every
inf-decked subset contains a minimal element.
By symmetry, it suffices to treat case (a). So let V be sup-decked and D-
invariant. Let P and Q be the subsets of W given by
x G P = x < D.x ,
x G Q = (Vy eW : D.y = y: x <y) .
It is easy to see that the sets P and Q, and hence V n P D Q, are sup-decked. Now
Zorn's Lemma implies that V fl P fl Q contains a maximal element, say wa. The
intersection V n P n Q is .D-invariant, since for every x G W
D.x G VnPDQ
= 
{definitions}
D.x eV 
A D.x < D.(D.x) 
A (Vy eW : D.y = y : D.x < y)
<= {F is D-invariant; D.y = y}
x eV 
A D.x < D.(D.x) 
A (VyeW:D.y 
= y: D.x < D.y)
<= {D is monotone}

13.2 UNFOLDING 
169
x e V A x<D.x 
A (Vy Â£W : D.y = y : x <y)
=. 
{definitions}
xevnPnQ.
This implies that D.wa G V D P D Q. Since wa G P we have wa < jD.wa. By
maximality of wa, it follows that wa = D.wa, so that wa is a fixpoint of D. Since
wa G Q, it is the least fixpoint. Since wa G V, this proves that V contains the least
fixpoint of D. (End of proof)
13.2 Unfolding
Recall from Section 4.4, that the restriction wp\H is the least fixpoint of Do in
MTH, and wip|77 is the greatest fixpoint of D\ in MTH, where, for e = 0 or 1, the
unfold operations De G MT â* MT are given by
(1) 
De = weo body eH -> MT ,
with we E A 0 â> MT given by we = (wse U w)Â®.
We aim at fixpoint induction for the functions Z?e, so we need various subsets
of MT . It turns out that some arguments can be given uniformly for e = 0 and
1, whereas other arguments need a case distinction.
For e â 0 and 1, we define the subsets WGe of MTH by
(2) 
WG1 = MUH ,
WG0 = Wp.R ,
where we refer to 8(3) for the definition of Wp.R, and where R is a sup-closed
subset of MT yet to be determined.
In the choice of WGo we keep some freedom for tuning the result. The choice
of R is postponed to Section 13.8. It will serve to ensure property ll(4)(d). Apart
from the tuning of WGo in 13.8, all relevant properties of WGo and WGi are
contained in the next lemma.
(3) Lemma, (a) For both e = 0 and 1, WGe is a subset of MPH. For every
w G WGe, function we is a homomorphism AÂ® â> MP.
(b) WGo is i?o-invariant and sup-closed in MTH.
(c) WGi is i^i-invariant and inf-closed in MTH.
Proof. Since MU C MP (cf. Section 4.1), we have WGi C MPH. It follows from
8(3) and 4(43)(b) that WG0 C MPH. The second assertion of part (a) follows from
Theorem 4(42). Part (b) is Theorem 8(4)(b). Part (c) is 4(28) and 4(29). (End of
proof)

170 
THE STRONG 
PREORDER
In order to define the strong preorder announced in Section 12.2, we investigate
binary relations E on A 0 as considered in accumulation rule 12(5). For a binary
relation 25, we define the subsets Wgo.E and Wg1.E of MTH by
(4) 
v G Wge.E 
= v e WGe 
A E C Leq.ve 
where e G {0,1} .
This definition is justified by
(5) Lemma, (a) If Wgo.E is sup-decked and Z)o-invariant, then wp.q < wp.r for
all pairs (g, r) G -E.
(b) If Wgj.JS is inf-decked and D\-invariant, 
then wlp.q < wip.r for all pairs
Proof, (a) It follows from Theorem (0) that WgQ.E contains the least fixpoint of
Do- By Section 4.4, this least fixpoint is wao, the restriction wp\H. Since wp = wa$,
it follows from definition (4) that E C Leq.wp, in other words that wp.q < wp.r
for all pairs (</, r) G E.
(b) This case is proved analogously. (End of proof)
The binary relation E on A 0 is said to be stable under unfolding if and only
if for every admissible preorder '<' on A 0 we have
(6) 
(V(q,r) eE::q<r) 
=* (V(g,r) G Â£ :: body 0.? < bodyÂ°.r) .
The relevance of this condition is shown in the next lemma.
(7) Lemma. Let E be stable under unfolding, and e = 0 or 1. Then Wge.E is
Z)e-invariant.
Proof. We first claim that unfolding commutes with extending in the sense that for
every w G WGe
(8) 
(De.w)e = w e o b o d y 0 .
This is proved by
(De.wy
= 
{(i)}
(we o body)*
= 
{(3)(a) and 10(1)}
weobodyÂ® 
.
Now it suffices to verify that for v G MTH
De.v â¬ Wge.E
= 
{(4)}
De.v G WGe A EC 
Leq.(De.v)e

13.3 THE CONSTRUCTION OF THE STRONG PREORDER 
171
<= {(3), (8)}
v E WGe 
A Ed Leq.(ve o body0)
4= 
{E is stable under unfolding: (6) with (<) = Leq.ve}
v E WGe 
A E C Leq.ve
s 
{(4)}
v e Wge.E.
(End of proof)
In view of Lemmas (5) and (7), we introduce the following definitions. A
relation E on commands is defined to be sup-safe if and only if Wgo.E is sup-
decked in MTH. It is defined to be inf-safe if and only if Wgx.E is inf-decked in
MT . It is defined to be safe if and only if it is sup-safe and inf-safe and stable
under unfolding. The above results are summarised in:
(9) Summary, (a) If relation E is sup-safe and stable under unfolding, we have
wp.q < wp.r for all pairs (q, r) Â£ E.
(b) If relation E is inf-safe and stable under unfolding, then wlp.q < wlp.r for all
pairs (q, r) E E.
(c) If E is safe, then q Q r for all pairs (g, r) E E.
13.3 The construction of the strong preorder
In this section we construct the strong preorder, postulated in Section 12.2, as the
union of all safe relations.
(10) Lemma. Let (i El:: EA) be a nonempty family of relations on A0.
(a) If E.i is stable under unfolding for every i E /, then the union (\Ji :: EA) is
stable under unfolding.
(b) Wge.(Ut::JB.t) 
= ([>' â¢" Wge.(E.i)) .
(c) If EA is sup-safe (inf-safe) for every i E /, then (|Jz :: EA) is sup-safe (inf-
safe).
Proof. Parts (a) and (b) are proved by direct appeal to the definitions. Part (c)
follows from (b) and the observation that any intersection of sup-decked sets is
sup-decked, and similarly for inf-decked sets. The details are left to the reader.
(End of proof)

172 
THE STRONG 
PREORDER
(11) Lemma. Let relation E on commands be stable under unfolding. There is a
unique smallest admissible preorder that contains E. This preorder, to be called
sap.E, is stable under unfolding and satisfies Wge.(sap.E) 
= Wge.E for both e = 0
and e = 1.
Proof. Since an intersection of admissible preorders is an admissible preorder (see
Section 12.1), the intersection of all admissible preorders that contain E is sap.E.
In order to prove that sap.E is stable under unfolding, we let (q, r) Â£ sap.E. We
have to prove (body .q, body .r) G sap.E. Since sap.E is the intersection of all
admissible preorders 'Â«' with E C (<), it suffices to observe, for every admissible
preorder 'Â«':
body0.*? < body 0.r
<= 
{define F by (s, t) â¬ F = 
body 0.s < body0.*}
sap.E C F
<= {definition sap.E, and F is an admissible preorder by 10(0)}
ECF
= 
{definition of F}
(V(M) G Â£:: body0.* Â« body0.*)
<= {E is stable under unfolding: (6)}
EC{<).
This proves that sap.E is stable under unfolding. The equality is proved by observ-
ing that for any v G WGe
v e Wge.(sap.E) 
= v E Wge.E
= 
{(4)}
sap.E C Leq.ve 
= E C Leq.ve
= 
{Leq.ve is an admissible preorder; definition sap.E}
true .
(End of proof).
The strong preorder <C is defined as the union of all safe relations on A 0. This
definition is justified by
(12) Theorem, (a) Relation <C is safe and it is an admissible preorder.
(b) If q < r in Ae then q C r.
Proof, (a) It follows from the definition of <C and Lemma (10)(a, c) that relation
<C is safe. It follows from Lemma (11) that relation sap.(<C) is also safe, and hence
equal to (<C). This proves that relation <C is an admissible preorder.

13.4 THE ABORTIVE INTERPRETATIONS 
173
(b) This follows from part (a) and (9)(c). (End of proof)
This result proves rule 12(4). As a first step towards rule 12(5), we claim
(13) Theorem. Let relation E be sup-safe and inf-safe. Assume that for every
admissible preorder 4<T on Ae with E U ( < ) C (<):
(V(g,r) GÂ£::body 0.?Â«body 0.r) .
Then E C (<).
Proof. It follows from the assumptions, Lemma (10)(c) and Theorem (12)(a), that
the union E U (<C) is sup-safe and inf-safe. It is easy to verify that E U (<C) is
stable under unfolding. Therefore, E U (<C) is safe, and hence contained in the
strong preorder (<C). (End of proof)
13.4 The abortive interpretations
Comparison of Theorem (13) with accumulation rule 12(5) tells us that it remains
to prove that relation E is sup-safe and inf-safe if it satisfies
(14) 
(V(g,r) G E :: abort 0.q E abortÂ°.r 
A q G Lia) .
We first concentrate on the lefthand conjuncts of condition (14). We observe
(V(g,r) G E :: abort 0.^ C abortÂ°.r)
= 
(12(0)}
E C Leq.(wp o abort 0) 
A E C Leq.(wlp o abort 0) .
By the definitions of abort 0 and abort in 10(0), 11(0) and 1(10), we have
wp o abort 0 
= 
(ws0 U Â± ) 0 = 
JLÂ° ,
wlp o abort 0 
= 
(wsi U T ) 0 = 
T 1 ,
where T and _L in MTH are given by
T.h.p = true 
A Â±.h.p = false 
for all h G Â£T, p G P .
One verifies that _L = (sup0) and T = (inf 0), so that _L G WGo and T G
by Lemma (3)(b,c). Therefore we have
(15) 
(V(g,r) G JB :: abortÂ°.g C abortÂ°.r)
= 
{above}
Â£ C Leq.(_LÂ°) A JB C Leq^T1)
= 
{(4)}
(sup0) G Wgo.J5 A (inf 0) G Wgj.E .
In the next section we shall use this equivalence.

174 
THE STRONG 
PREORDER
13.5 Infâsafety
We now attack the inf-safety of relation E. 
Relation E is infâsafe if and only
if WgQ.E is inf-decked, i.e. (inf L) G Wg1.E for every chain L in Wg1.E. 
By
calculation (15), condition (14) suffices to treat the empty chain. Therefore, we
may restrict our attention to nonempty chains L in 
Wg1.E.
(16) Lemma. For every nonempty chain L of WGi and every q G A 0, we have
(17) 
(inf Lf.q = (inf v G L :: v\q) .
Proof. We use structural induction on q. For q G 5, we have
= 
(4(25)}
w]p.# = (inf v Â£ L :: wlp.q)
= 
{L is nonempty}
true.
For q G H, we have
(inf L)1.^ = (inf v G L :: v1.^)
= 
{4(25)}
(inf L).q = (inf v E L :: v.q)
= 
{Theorem 4(3)}
true.
Since A = S U iJ, this proves formula (17) for q E A. For q = e in A*, we have
EE 
{4(15)}
identity = (inf t; G Â£ :: identity)
= 
{L is nonempty}
true.
The delicate case is the induction step in A*. This step is taken by observing for
any a G A, any 5 G A* and any predicate p G P that
= 
{Lemma (3)}
((inÂ£L)1.ao(irdL)1.s).p
= 
{(17) for a G A, induction hypothesis (17) for
(inf v E l : : v1. a), ((inf w Â£ L :: w1^)^
= 
{4(6) twice}
(\/v e L :: v1.a.(Vw 
EL:: 
w1^^)

13.5 INF-SAFETY 
175
= 
{vl.a G MP by Lemma (3), and 3(1)}
(Vv,w G l : : v1.a.(w1.s.p))
= {L is a chain, diagonalisation cf. Lemma 8(17)}
(Vv G L :: t^.a.^.s.p))
= 
{same steps as above, using (3) and 4(6)}
(inf v G L :: v1.(a;5)).p .
This proves formula (17) for q G A*. Finally, we prove formula (17) for q G A0 in
(infLf.q = (infv E l : : v1^)
= 
{4(15)}
(inf s Eq:: (inf L)1 .s) = (inf v E l : : (inf 5 G ^ :: v1^))
= 
{(17) for s G A*, and interchange of quantifications}
true.
(End of proof)
Now we can prove
(18) Lemma. Relation E is inf-safe if it satisfies condition (14).
Proof. By calculation (15), it suffices to observe that for any nonempty chain L in
Wg1.E we have
(infL)e Wgl.E
= 
{(4); WG\ is inf-closed by Lemma (3)}
E C Leq.(inf Lf
= 
{definition 12(3)}
(Vfor) e E :: (inf L)1.? < (inf L)1.?-)
= 
{L nonempty, Lemma (16)}
(V(g,r) G -E :: (inf u G L :: u1^) < (inf v â¬ L :: vx.r))
-r= 
{calculus}
(V(g,r) G JS :: (Vw G L :: w1.? < ^.r))
= 
{12(3)}
(Vu E L:: E C 
Leq.v1)
Â«= {(4)}
L C
(End of proof)

176 
THE STRONG 
PREORDER
13.6 Sup-safety
For the treatment of sup-safety, we need the introduction of the set Lia. So, here
we finally give the definition announced in Section 11.2. A command q G A 0 is said
to be linearly approximating if and only if for every nonempty chain L in WGo
(19) 
(supL) 0.? 
= 
(supv e L::vÂ°.q) 
.
We write Lia to denote the set of the linearly approximating elements of A0.
(20) Lemma. Relation E is sup-safe if it satisfies condition (14).
Proof. The proof is similar to the proof of Lemma (18). By calculation (15), it
remains to observe that for any nonempty chain L in WGo we have
(supL)e Wgo.E
= 
{(4); WG0 is sup-closed in MTH by Lemma (3)}
(V(?,r) G E :: (snpLf.q < (sup L)Â°.r)
<= {E C Lia x Ae so that q G Lia; (19)}
(V(#,r) G E :: (supv G L :: vÂ°.q) < (supL)Â°.r)
= 
{4(0)}
) EE,v EL :: vÂ°.q < (supL)Â°.r)
{structural induction, monotony: vÂ°.r < (supL)Â°.r for all v Â£ L}
Â«= 
{definitions (4) and 12(3)}
L C Wgo.E .
(End of proof)
By Lemmas (18) and (20), condition (14) implies that E is inf-safe and sup-safe.
By Theorem (13), this proves the accumulation rule 12(5).
13.7 Linear approximation
It remains to justify the rules of Theorem 11(4), which imply that A 0 contains
sufficiently many linearly approximating commands. In this section we treat the
parts (a), (b) and (c) of that Theorem.
We begin with the claim of part (a) of Theorem 11(4) that H U S 0 C Lia. For
any h G H and any nonempty chain L in WGo we observe
(supL)Â°.h = (supv G L :: vÂ°.h)
= 
{4(25) and h G H}

13.7 LINEAR APPROXIMATION 
177
(sup L).h = (supv G L :: v.h)
= 
{4(3) applied to L C MT H}
true .
By definition (19), this proves that H C Lia. Similarly, for any q G S 0 and any
nonempty chain L in WGo we observe
(supL)Â°.# = (supu E i : : ^Â°-#)
= 
{4(25) and q E S 0}
(wp|5)0.^ = (supv eL:: 
(wp\S)Â®.q)
= 
{L is nonempty}
true .
This proves that S 0 C Lia. Together with the previous inclusion this concludes the
proof of Theorem ll(4)(a).
Let q, r â¬ Lia. In order to prove that q J r G Lia, it suffices to observe that for
any nonempty chain L in WGo and any predicate p
(sup L)O.(qlr).p
(supL)Â°.#.p A (supL)Â°.r.p
= 
{?,rG Lia, (19)}
(supt; G L :: vÂ°.q).p A (supw; G i : : wÂ°.r).p
= 
(4(6)}
(3v G L :: vÂ°.g.p) A (3iy G L :: it;Â°.r.p)
= 
{distributivity}
(3 v, ^ G L :: vÂ°.q.p A wÂ°.r.p)
= 
{L is a chain; diagonalisation 8(17)}
(3 v G L :: vÂ°.q.p A vÂ°.r.p)
= 
{(3) and 4(6)}
(sup u G L : : vÂ°.(g [] r)).p .
This proves part (b) of Theorem 11(4).
We now turn to part (c), which asserts that the set Lia is closed under deter-
ministic choice. Let (i G / :: q.i) be a family of linearly approximating commands
and let / be a state function in Ix. 
We have to prove that q.f is linearly approxi-
mating. By definition (19), it suffices to observe that for any nonempty chain L in
and every predicate p G IP
(sup L)Â°.(q.f).p
= 
(8(14)}
(3i::(f 
= i) A (supL)0.(q

178 
THE STRONG 
PREORDER
= 
{q.i e Lia, (19), 4(6)}
(3i::(f 
= 
i)A(3veL::vÂ°.(q.i).p))
= 
{predicate calculus and 8(14) again}
(3veL::vÂ°.(q.f).p)
= 
(4(6)}
(supv E L :: vÂ°.(q.f)).p .
This concludes the proof of part (c) of Theorem 11(4).
13.8 The set Lia and sequential composition
We now investigate the composition of linearly approximating commands q and r,
in order to obtain Theorem ll(4)(d). We follow the calculation of [Hesselink 1990]
Section 5.5. Let L be a nonempty chain in WGQ and let p G P . We observe
(supL)Â°.(?;r).p
= 
{Lemma (3)(a)}
(supL)Â°.?.((supL)Â°.r.p)
â {#,r E Lia; (19); let u, w range over L}
(sup?; :: vÂ°,q).((supw :: wÂ°.r).p)
= 
{4(6), twice}
(3v :: vÂ°.q.(3w :: wÂ°.r.p))
(*) 
= {provided vÂ°.q E Muc (cf. 8(15)), or wÂ°.r.p constant}
(3v,w :: vÂ°,q.(wÂ°.r.p))
= 
{L is a chain; diagonalisation by Lemma 8(17)}
(3v::vÂ°.q.(vÂ°.r.p))
= {vtWT and 4(43)(b)}
(3v::vÂ°.(q;r).p).
The critical step is indicated by (*). If r E 5 0 then wÂ°.r = wp.r and, therefore,
wÂ°.r is independent of w.
We now choose i? = Muc in definition (2). By definition (2) and formula 8(5),
it follows that vÂ°.q E Muc for all v E WGo and q E Sy.Muc. Therefore, the above
calculation implies
(Vg, r E Lia : q E Sy.Muc V r e Se : q\r â¬ Lia) .
This concludes the proof of Theorem ll(4)(d).

13.9 EXERCISES 
179
13.9 Exercises
Exercises of Section 13.1.
Exercise 0. In [Hesselink 1990] Theorem 2(9), it is proved that MC is sup-decked
in PT. The purpose of this exercise is to show that MP is not sup-decked in MT.
Let there be precisely one program variable v, which is of type integer. A state x
is characterised by the value of v. We may therefore regard state x as an integer.
(a) For integer i, let q.i be the nondeterminate choice of an integer bigger than i:
Q-i = 
( B y : y > i : v :== y) .
Prove that the predicate transformer f.i = wp.(q.i) satisfies
f.i.p.x 
= 
(Vy :y>i:p.y) 
.
(b) Prove that the family (i :: f.i) forms a chain in MP.
(c) Prove that / I = (supz :: f.i) satisfies
fl.p.x 
= (3i :: (Vy : y > i : p.y)) .
(d) Prove that /I ^ MP by showing that
/l.(Vj :: u.j) Â± (V; :: /l.(u.j))
where u.j G IP is given by u.j.x = (j < x).
(e) Prove that MP is not sup-decked in MT.
Exercises of Section 13.3.
Exercise 0. Prove the assertions of Lemma (10).
Exercise 1. Let a relation E on A 0 be called wlp-safe if and only if it is inf-safe
and stable under unfolding. Let R be the union of all wip-safe relations. Prove the
following facts:
(a) R is wip-safe and an admissible preorder,
(b) wlp.q < wlp.r for every pair (<?, r) G iZ,
(c) if E is a binary relation on A 0 such that
(V(g,r) G E :: wlp.(abort0.?) < wip.(abortÂ°.r))
and that for every admissible preorder < on A 0 with E U R C (<) we have
then E C R.
Exercises of Section 13.4.
Exercise 0. <? Prove that for all g, r G A 0
q <^ir =$- abort0.q C. abort0.r
Show that q <C r does not imply g G Lia.

CHAPTER 14
TEMPORAL OPERATORS
14.0. Up to this point, the semantics of the commands is determined by the relation
between precondition and postcondition. This point of view is too restricted for
the treatment of concurrent programs and reactive systems. The usual example is
that of an operating system which is supposed to perform useful tasks without ever
reaching a postcondition.
For this purpose, the semantics of commands must be extended by consider-
ation of conditions at certain moments during execution. We do not want to be
forced to consider all intermediate states or to formalize sequences of intermediate
states. We have chosen the following level of abstraction. To every procedure name
/*, a predicate z.h is associated. The temporal semantic properties of a command q
depend on the values of z.h.x for the procedure calls, say of procedure h in state x,
induced by execution of command q. The main properties are 'always' and 'even-
tually', which are distinguished by the question whether z.h.x should hold for all
induced calls or for at least one induced call. The concept of 'always' is related to
stability and safety. The concept of 'eventually' is related to progress and liveness.
In this chapter, we regard nontermination of simple commands as malfunction-
ing and nontermination of procedures as potentially useful infinite behaviour. We
therefore use wp for the interpretation of simple commands and wlp for procedure
calls.
The definitions we propose are generalizations of the definitions in [Morris 1990]
for the case of simple tail recursion. The framework of Chapter 4 is well suited
for such a generalization: mutual recursion and unbounded choice are smoothly
incorporated.
We expect that temporal predicate transformers will turn out to be useful in
the treatment of concurrent programs and reactive systems. Most applications in

14.1 STABILITY AND THE FUNCTION 'ALWAYS' 
181
these areas, however, have many other important but distracting aspects and fall
therefore outside the scope of this monograph. As a single application, we give in
the next chapter a treatment of predicative fairness.
14.1 Stability and the function 'always'
The main temporal property of a condition is stability. Operationally speaking,
it means that the validity of the condition at a certain procedure call implies its
validity at all induced recursive calls. The condition may depend on the procedure
name and the state. So it is a function in IP .
We associate to such a function z G IP a function wl.z G MT , which is
a variation of wlp that could have been inspired by termination law 4(38). It is
defined by
(0) 
wl.z.h.p 
= z.h A wlp.h.p 
for all h G H, p G P .
Since wlp.h.true = true, it follows that wl.z.h.true = z.h, so that wl.z Â£ WT,
cf. definition 4(33). By formula 4(34), it follows that the induced function (wl.z)0 G
Ae -> MT, cf. definition 4(25), satisfies
(1) 
(wl.z)Â°.r.p 
â (wl.z)0.r.true A wlp.r.p 
for all r G Ae, p G IP .
Henceforward, we write wl.z also to denote (wl.z)0 G A 0 â> MT. It follows from
corollary 4(43) that wl.z is a homomorphism A 0 â> MP.
Intuitively, wl.z.r.true is the weakest precondition such that every simple com-
mand invoked terminates and z.h holds whenever execution of r directly calls a
procedure h. There is not yet any heredity: z.k need not hold when procedure h in
turn calls a procedure k. The claim made just now is formalized by means of the
relational semantics of Chapter 6 in the following way:
(2) Theorem. Let r G A*, x G X and z Â£ JPH. Then wl.z.r.true.x holds if and
only if, for all u, v G A* and a G A such that r = (u;a;v) and all y G X with
[wj.y.a;, we have ->[a].oo.y if a G 5 and z.a.y if a G H.
Proof. We use induction on the length of string r. For empty r the assertion holds
since wl.z.e.true = true and e has no decompositions of the form (u;a;v). For a
string (6; r) with b G A we observe
wl.z.(b] r).true.x
= 
{wl.z is a homomorphism}
wl.z .b.(wl .z .r.true).x
= 
{wl.z = (wl.z)0', (0) and 4(25)}

182 
TEMPORAL 
OPERATORS
(& Â£ Â£ 
=> 
wsQ.b.(wl.z.r.true).x)
A (b G H 
=> z.b.x A 
wlp.(wl.z.r.true).x)
= 
{WSQ = wp and 6(1)}
(beS 
=> i[6].oo.a;) 
A (6 G # 
=* s.&.x)
A (Vy G X : [6].y.a: : wl.z.r.true.y) 
.
Now the assertion follows by induction. (End of proof)
We define function z G TPH to be stable if and only if
(3) 
[z.h 
=Â» wi.2.(body.ft).true] 
for all ft G # .
In terms of the configuration graph (X xi*)U{oo} of Section 9.4, the operational
meaning of stability can be described as follows.
(4) Theorem. A function z G IP 
is stable if and only if, in the configuration
graph, every pair (x, ft) with x G -X", ft G H and z.h.x has no finite path to oo and
is such that z.k.y holds for every finite path to a pair (y,&;tf) with y G X, k G H
and t G A*.
Proof. The proof is by mutual implication, with two applications of Theorem (2).
Since it is not enlightening, the details are better left to the interested reader. (End
of proof)
The next lemma is the technical basis for most properties of stability.
(5) Lemma. The supremum of a set of stable functions is stable.
Proof. We calculate a sufficient condition for the stability of the supremum of a
subset Z of P 
, by observing that for every h G H
[(supZ).ft 
=> wi.(supZ).(body.ft). true]
= 
{definition supremum}
(Vz G Z :: [z.h 
=> 
wi.(supZ).(body.ft).true])
<= {by exercise 4.4.3, function z Â»â> (wLz)Â° is monotone;
notice that Z empty is allowed}
(VzEZ::[z.h 
=> 
wl.z.(body.ft).true])
= 
{(3)}
all z G Z are stable.
(End of proof)
The temporal function alw G TPH â* TPH (pronounced 'always') is defined by
(6) 
alw.z 
= 
(sup# G IP : x < z 
A x stable : x) .
We now have

14.1 STABILITY AND THE FUNCTION 'ALWAYS' 
183
(7) Theorem, (a) alw.z is stable for every z G TPH.
(b) z G IPH is stable if and only if alw.z â z.
(c) Function aiw G P H â> IPH is idempotent.
(d) Function aiw G F H -> P H is monotone.
Proof, (a) Follows from definition (6) and Lemma (5).
(b) If z is stable then definition (6) implies that alw.z = 2. If alw.z = z then
part (a) implies that z is stable.
(c) Follows from (a) and (b).
(d) Follows from definition (6).
(End of proof)
Example. Let i be an integer program variable. Let H = {h} with procedure h
declared by
body./* 
= 
(i := i + 1 ; h) .
Let z G IP 
be given by z.h = p G IP. Then we have
z is stable
= 
{(3)}
[p => wl.z. (body. h).true]
= 
{4(25) and declaration h}
[p => wp.(i := i + 1).(wl.z.h.true)]
[p => wp.(i := i + l).(z.h A wlp.h.true)]
= 
{z.h = p and wlp.h.true = true}
[p 
=> wp.(i := i + l).p] .
Therefore, z is stable if p = false or p = true or p = (i > n) for some integer n.
This is in agreement with the intuitive notion of stability.
This example can be used to show that function alw need not commute with
suprema. If, for simplicity of notation, we identify z and p, we find
aiw.(i < n) 
= 
false 
and
aiw. true 
= 
true .
Since (sup n G 2 Z : : i < n ) = true in P 
, this shows that function aiw G JPH â* JPH
does not commute with suprema. (End of example)

184 
TEMPORAL 
OPERATORS
14.2 Termination and unfolding
The idea of functions in IP and the associated function wl, cf. definition (0),
suggest a new view on termination and total correctness. In fact, termination is
determined by the function term Â£ IP 
defined by
(8) 
term.h 
= 
wp.h.true .
From definition (0) and the termination law, it follows that
(9) 
wp = 
wl.term .
Definition (3) suggests to define the unfolding function unf Â£ 1PH âÂ» TPH by
(10) 
unf.z.h 
â wl.z.(body.h).true 
.
We observe that by formula (3)
(11) 
z is stable 
= z < unf.z .
Function unf Â£ IP âÂ» IP is easily seen to be monotone. Therefore, it follows
from (11) that
(12) 
z is stable 
=> unf.z is stable .
The next result is an elegant characterization of termination. We did not notice it
before, since function wl was not available.
(13) Theorem, (a) Function term is the least fixpoint of unf.
(b) Function term is also the least fixpoint of alw o unf.
Proof Function term is a fixpoint, since for every h E H
unf .term.h
wl. term, (hody.h). true
= {(9)}
wp.(body.h).true
= 
{fixpoint equation of wp and (8)}
term.h .
It is the least one, since for every z Â£ IP
term < z
4= 
{(8) and (0)}
wp\H < wl.z in MTH
4= 
{wp\H is least fixpoint; let h range over H and p over IP}
(V/i,p :: wl.z.h.p = 
wl.z.(hody.h).p)
= 
{(O)and(l)}
:: z.h A wlp.h.p = wl.z.(body.h).true 
A 
wlp.(hody.h).p)

14.3 THE TEMPORAL PREDICATE TRANSFORMER FOR ALWAYS 
185
<f= {fixpoint equation of wlp and (10)}
z = unf .z .
(b) It suffices to prove that unf and alw o unf have the same fixpoints. This
is proved by mutual implication. For any z G TPH we observe
z = alw. (unf .z)
z â alw.(unf .z) 
A z is stable
{(12)}
z = aiw.(imf .z) 
A unf .z is stable
2T = 
UI2f .Z .
The other implication is proved in
z = unf .z
z = unf.z 
A z is stable
z â alw.(unf.z) 
.
(End of proof)
Remark. Theorem (13) could be used as a definition of term independent of wp.
Then formula (9) could be used as a definition of wp. We found Theorem (13)
inspired by a remark in [Morris 1990] that suggested part (b). The easiest proof of
part (b) turned out to yield part (a) as well. (End of remark)
14.3 The temporal predicate transformer for always
We now construct for 2 G F 
a homomorphism Alw.z G AÂ® â> MP. Predicate
Alw.z.r.p is the weakest precondition such that during execution of command r
every simple command terminates and every procedure call h occurs in a state
where z.h holds and that r does not terminate or terminates in a state where p
holds. Function Alw is defined by
(14) 
Alw.z 
= 
wl.(alw.z) E AÂ® -> MT .
The next theorem provides a nice characterization of function Alw. It is based on
the following fact (compare exercise 4.2.0):

186 
TEMPORAL 
OPERATORS
(15) Greatest-flxpoint property. For a complete lattice W, let D G W â> W
be a monotone function. Then (supu> â¬ W : w < D.w : w) is the greatest element
w with w < D.w] it satisfies w = D.w and hence is the greatest fixpoint of D.
(16) Theorem. The restriction (Alw.z\H) is the greatest solution of the equation
in v G MT
(17) 
v.h.p = *.ft A vÂ°.(body./i).p 
for all h G #, p G P.
Proof. By property (15), it suffices to prove that (Aiw.2|iJ) is the greatest solution
of the equation in v G MT
(18) 
[ v.h.p => z.h A vÂ°.(body.fc).p] 
for allhe H, pE P.
We first verify that, indeed, (Alw.z\H) solves equation (18). For every h G H and
p G IP, we have
[Aiw.z./i.p =$â¢ z.h A Alw.z.(body.h).p]
= 
{left (14) and (0); right (14) and (1)}
[ alw.z.h A wlp.h.p
=> z.h A Aiw.2.(body./i).true A wip.(body./i).p]
^= 
{fixpoint equation of wlp}
[alw.z.h 
=$> z.h A Aiw.z.(body./i).Â£rue]
= 
{alw.z < z from (6); use (14)}
[alw.z.h 
=> wl.(alw.z).(hody.h).true]
= 
{alw.z is stable by (7)(a); then use (3)}
true .
Now let v G MT 
be an arbitrary solution of equation (18). Then v < vÂ° o
body. By property (15) and the definition of wlp, the restriction (wlp\H) is the
greatest solution of the latter equation. This implies that
(19) 
v < (wlp\H) .
We now compare v with (Alw.z\H):
v < (Alw.z\H)
= 
{(14), (0); let h range over H and p over P}
(V/i,p:: [v.h.p =$> alw.z.h A wlp.h.p])
= 
{(19)}
(V/i,p:: [v.h.p =$Â» alw.z.h])
= 
{monotony of v.h}
(V/i :: [v.h.true 
=> alw.z.h])
4= {(6); let zl.h = v.h.true}
(zl < z) A z\ is stable

14.4 TEMPORAL FUNCTIONS FOR EVENTUALLY 
187
= 
{(18) implies zl < z; use (3)}
(\/h::[zl.h 
=> 
wl.zl.(body.h).true])
= 
{definition zl}
(V/i :: [v.h.true 
=$> 
wl.zl.(body.h).true])
<= {(18) with p := true}
(Vfc:: [vÂ°.(body.h).true 
=Â» 
wl.zl.(body.h).true])
4= 
{exercise 4.4.3}
v < wl.zl
= 
{(0) and definition of zl}
(V/i,p:: [v.h.p 
=$> 
v.h.true A wlp.h.p])
= 
{monotony of v.h and (19)}
true .
This proves that (Alw.z\H) is the greatest solution of (18) and hence also of (17).
(End of proof)
Remark. 
In [Morris 1990], function A is defined as the greatest solution of the
analogue of equation (17). Therefore, it is the analogue of our function Alw. Our
approach seems to be simpler, but it does not work for the function 'eventually' to
be treated next. (End of remark)
14.4 Temporal functions for eventually
We now construct a function Evt (pronounced 'eventually'). It is the analogue of
function Â£ of [Morris 1990]. For z E IPH, r E AÂ® and pGlP, predicate Evt.z.r.p is
the weakest precondition such that command r terminates in a state where p holds
or z.h holds at some induced procedure call h.
The description of Evt implies that Evt.z.c = wso.c for every simple command
c. It also implies that Evt.z should be a homomorphism. Therefore, function Evt.z
is determined by its restriction to the set iJ, and we expect Evt.z = vÂ° for some
function v E MT 
. The description of Evt.z implies that for all h E H and p E IP
we expect
(20) 
Evt.z.h.p 
= 
z.h V Evt.z.(body.h).p 
.
If z.h â false for all /i, then Evt.z should coincide with wp. In this way, we arrive
at the following definition.
We define function Evt E P H -> (A0 ~> MT) by Evt.z = vÂ°, where v is the
least (i.e. strongest) solution of equation

188 
TEMPORAL 
OPERATORS
(21) 
v.h.p 
= 
z.h V vÂ°.(body.h).p 
for all h G # , p G P .
We use a little trick to avoid unnecessary proofs. Let bov be a modified
procedure declaration given by
bov.h 
= 
?(-*z.h) ; body./i .
Then equation (21) is equivalent to
v.h.p 
= 
vÂ°.(bov.h).p 
for all h G H, p E IP.
The least solution of this equation is the function wp with respect to the modified
declaration bov. Therefore, Evt.z is well-defined and satisfies formula (20). By
corollary 4(43)(b), we even have
(22) Theorem. Evt.z is a homomorphism A 0 â> MP.
Since Evt.z is nothing but function wp with respect to the modified declaration
bov, Recursion Theorem 2(16) immediately extends to the following proof rule:
(23) Theorem. In order to prove that
(Vi el:: 
[p.i=> Evt.z.(h.i).(q.i)]) 
,
it suffices to give a function vf G / â> 7L such that for every integer n
(Vi el:: 
[p.i A vf.i < n A n > 0 =Â» 
Evt.z.(h.i).(q.i)])
=> (Vi G / :: [p.i A vf.i < n => z.(/i.i) V
A special case is captured in function evt G IP 
âÂ» (AÂ® â> IP) , defined by
(24) 
evt.z.r 
= 
Evt.z.r.false 
.
Predicate evt.z.r expresses that execution of r leads to some induced procedure
call h where z.h holds. It turns out that the restricted function evt G 1PH â> JPH
satisfies many properties analogous to function alw.
(25) Theorem, (a) z < evt.z for every z G IPH.
(b) If z < zl in 1PH then Evt.z < Evt.zl in AÂ® -> MP.
(c) Evt.(evt.z) = Evt.z for every z G P H .
(d) Function evt G P 
ââ¢ P 
is monotone and idempotent.
Proof, (a) For every h e H we observe
z.h
=> {(20)} 
Evt.z.h.false
= 
{(24)} 
evt.z./i .
(b) It suffices to observe
Evt.z < Evt.zl 
in AÂ® -> MP

14.5 POSSIBLE TERMINATION 
189
<Â£= 
{exercise 4.4.3}
Evt.z\H < Evt.zl\H 
in H -> MP
<= {definition of Evt in (21) and exercise 4.2.l(a)}
(Vv eMTH,hÂ£H,pe1P 
::
z.h V vÂ°.(body.h).p < zl.h V vÂ°.(body.h).p)
<= {calculus}
z < z\ .
(c) It follows from (a) and (b) that Evt.z < Evt.(evt.z). 
The other inequality
is proved in
Evt.(evt.z) < Evt.z
4= 
{as above}
Evt.(evt.z)\H < Evt.z\H
4= {Evt.(evt.z) 
is least solution of (21) with z := evt.z}
(Vfe G H,p e P :: Evt.z.h.p = evt.2r.fe V 
Evt.z.(body.h).p)
= 
{(24)}
(Vfe G -ff,p G F :: Evt.z.h.p = Evt.z.h.false V 
Evt.z.(body.h).p)
= 
{(20) twice}
(Vfe G if,p G P :: z.fe V Evt.z.(body.h).p 
=
z.h V Evt.z.(body.fe).false 
V 
Evt.z.(body.h).p)
= 
{monotony and [false =>p]}
true .
(d) This follows from definition (24) and the parts (b) and (c).
(End of proof)
14.5 Possible termination
Let c be an arbitrary command. As argued in Section 3.2, predicate ~>wlp.c.false
characterizes the initial states where execution of c may terminate. This is some-
times expressed by saying that command c angelically terminates.
We now specialize to procedures. The function of angelic termination aterm G
F ^ is defined by
(26) 
aterm.h 
= 
->wlp.h.false .
In principle, the associated function wl.aterm has some practical importance. It is
the weakest precondition function for the abundant implementation which creates
in each case of a nondeterminate choice in a procedure body sufficiently many

190 
TEMPORAL 
OPERATORS
identical processes, all with a separate state and charged with a different choice.
When finally some processes terminate, one result is chosen nondeterministically.
Example. Let n be an integer program variable. Let hO and hi be declared by
body.M) = 
((n := 0 [ n := 1) ; hi) ,
body./>l 
= (?(n = 0) B ?(n ^ 0) ; hi) .
Then wl.aterm.hO = wp.(n := 0). Notice, however, that
wl.aterm.(hody.hO).p 
= false 
for all predicates p.
(End of example)
Recall from 1(12) that a command c is total if and only if [ ->wp.c.faise]. Let
syntactic totality be defined by saying that command c is syntactically total if and
only if
(27) 
[ -iwi.aterm.c.false] .
The adverb 'syntactically' is not completely adequate, for the concept relies on
a mixture of syntax and wip-semantics. For every command s G SÂ®, we have
wl.aterm.s = wp.s. Therefore, s G 5 0 is syntactically total if and only if it is total.
(28) Lemma. Every procedure h G H is syntactically total.
Proof. It suffices to observe that
wLaterm.h. false
= 
{(o)}
aterm.h A wlp.h.false
= 
{(26) and calculus}
false .
(End of proof)
(29) Theorem. The following conditions are equivalent:
(a) every procedure h G H is total,
(b) term < aterm ,
(c) every syntactically total command r G A0 is total.
Proof. We prove the equivalence (a) = (b) and the two implications (b) => (c)
and (c) =^ (a). The equivalence (a) = (b) is proved in
term < aterm
= 
{induced order; definitions (8) and (26)}
(V/i G H :: [wp.h.true =* -^wlp.h.false])

14.5 POSSIBLE TERMINATION 
191
= 
{predicate calculus}
(V/i G H :: [wp.h.true A wlp.h.false = false])
= 
{termination law}
(Vh6H 
:: [wpAfaJse = false])
= 
{definition totality}
all h G H are total .
The implication (b) => (c) is proved in
term < aterm
=$> {monotony of wl}
[wl.term.r. false =$- wl.aterm.r. false]
= 
{(9) and calculus}
[-i wl.aterm.r. false =$> -iwp.r.false]
=â¢ 
{(27)}
if r is syntactically total then r is total .
The implication (c) => (a) follows from Lemma (28). (End of proof)
Example. We show that totality need not imply syntactic totality. Let b be a
boolean program variable. Let procedure h be declared by
body./i 
= (h | b := false) .
The composition (h; ?b) is total because of
wp.(h] ?b).false
= 
{composition}
wp.h.(wp.(?b). false)
= 
{h need not terminate}
false .
The composition (ft; ?b) is not syntactically total because of
wl.aterm.Qi; ?b).false
= 
{calculus}
aterm.h A wlp.h.(wl.aterm.(?b). false)
= 
{declaration h; definition wl}
true A wlp.h.(-ib)
= 
{declaration h and calculus}
true .
(End of example)

192 
TEMPORAL 
OPERATORS
14.6 Exercises
Exercises of Section 14.1.
Exercise 0. We show that function wl commutes with nonempty infima in P .
Let Z be a nonempty subset of P . Let K be the subset of A 0 of the commands
r with
wi.(inf Z).r 
= (inf z G Z :: wl.z.r) .
(a) Prove that r G K => a\r G K for every a Â£ A.
(b) Prove that if = A 0.
Exercise 1. Use the previous exercise to prove that the infimum of a nonempty
set of stable functions in IP is stable.
Exercise 2. Use the previous exercise to prove that function alw G IP 
â* IP
commutes with nonempty infima, i.e.
aiw.(inf Z) = (inf z G Z :: aJw.^r) .
Exercise 3. <s? Let v be an integer program variable and let H = {h}. Let i be
an integer value and let z G JPH be given by z.h = (v > i).
(a) Prove that z is stable if
body./* = 
v : = v + l ; / i ; v : = v â 2 ; / i .
(b) Prove that z is not stable if
body./i 
= 
(skip [ v := v + 1 ; h ; v := v â 2 ; /i) .
Exercises of Section 14.2.
Exercise 0. <s? Prove that z G IPH is a fixpoint of unf if and only if wl.z G MTH
is a fixpoint of function Do, cf. definition 4(26).
Exercises of Section 14.4.
Exercise 0. In the example of 14.1, use Theorem (23) to prove that
[ evt.(m < i)./i] 
for every integer m.

CHAPTER 15
PREDICATIVE FAIRNESS
15.0. The nondeterminacy considered thus far in this monograph was loose in the
sense of [Park 1979]: any choice or sequence of choices allowed by the command is
acceptable behaviour of the implementation, but the fact that a choice is allowed
does not mean that it can ever occur.
While reasoning about concurrent computations, and in the design of com-
municating processes, we have to deal with unpredictable execution, which is yet
not completely loose. We may want to assume that a computation delegated to
another process eventually yields an answer or that, if a stream of messages is sent,
eventually an acknowledgement comes back.
Such assumptions are called fairness assumptions. Fairness is a subject in
itself with a highly operational flavour. There are many different kinds of fairness,
cf. [Francez 1986] and [Lehmann e.a. 1981], but it seems that most definitions cannot
elegantly be expressed in terms of predicate-transformation semantics. Therefore,
we restrict ourselves to predicative fairness, a kind of fairness proposed in [Morris
1990] and [Queille-Sifakis 1983].
In the literature, fairness is usually treated only for repetitions. In [Morris
1990], fairness of tail-recursive procedures without mutual recursion is treated. We
give a definition applicable to arbitrary procedures. Our formalization is in agree-
ment with the treatment of loc.cit. in the case of tail recursion. Mutual recursion
and 'calls before the tail' seem to be adequately treated. Our formalization leads
to overly optimistic specifications if a procedure body contains sequentially ordered
recursive calls. A more realistic version can be obtained by means of the standard
reduction of recursion to tail recursion, cf. Section 9.1. The result of that reduction
is far from elegant because of the stack administration involved. We prefer to give
the elegant version in its full generality. We shall provide examples to show where
the formalism works reasonably well and where it may be regarded as being too
optimistic.

194 
PREDICATIVE 
FAIRNESS
15.1 Starvation of predicates
Consider the notânecessarily terminating repetition
(0) 
while x > 0 do (skip [ x := x - 1) od .
In this case, nontermination is generally regarded as unfair. Therefore, the repe-
tition is said to be fairly terminating. In general, a command is said to be fairly
terminating if and only if every infinite execution sequence is unfair. It remains to
discuss the meaning of the word 'unfair'. In the case of the above repetition, every
infinite execution sequence has a tail in which the second alternative of the body
is never chosen. These sequences are regarded as unfair because of the existence of
an alternative that eventually is always enabled and never taken. We could speak
of starvation of branches.
Now consider the repetition
(1) 
while x mod 3 ^ 0 do (x := x + 1 Q x := x - 1) od .
Given an initial state with x mod 3 ^ 0 , there is precisely one infinite execution
sequence. In this sequence the alternatives are taken alternately. So, we cannot
speak of starvation of branches. Nevertheless, one may want to regard the sequence
as unfair. For, at every moment of choice, there is an alternative that establishes
x mod 3 = 0. Therefore, the unfairness can be justified here by the existence
of a predicate P that is false along the execution sequence even though at every
choice there is an alternative that establishes P. One might say that the sequence
is unfair because of the starvation of a predicate. If unfairness is defined by means
of starvation of predicates, we speak of predicative fairness.
Traditionally, one distinguishes between weak fairness and strong fairness,
cf. [Francez 1986]. As above, these concepts are defined in terms of the unfair-
ness of the execution sequences. An execution sequence is said to be not weakly
fair if there is an alternative that eventually is always enabled and never taken.
The sequence is said to be not strongly fair if there is an alternative that is enabled
infinitely often and never taken.
For predicative fairness, one might want to use the obvious analogues: let a
sequence be called not weakly pp-fair if there is a predicate, eventually false along
the sequence, that eventually can be established at every choice; let the sequence
be called not strongly pp-fair if there is a predicate, eventually false along the
sequence, that can be established at infinitely many choices.
This definition is suggested in [Morris 1990], but it does not correspond to his
formalization. The crucial example is a variation of repetition (1). Consider, for an

15.1 STARVATION OF PREDICATES 
195
integer constant ra > 3, the repetition
(2) 
while x mod m^O do (x:=x + l | x : = x - l ) od .
We claim that every infinite execution sequence of (2) is not strongly pp-fair. This
is shown as follows. For a given infinite execution sequence, there is an integer i
such that x mod ra = i + 1 occurs infinitely often and x mod m = i occurs at
most finitely often. Therefore, the predicate x mod ra = i is eventually false along
the sequence, but it can be established at infinitely many choices. This shows that
repetition (2) is fairly terminating with respect to strong pp-fairness. It can be
shown that, if ra > 5, repetition (2) does not fairly terminate with respect to the
strong fairness of [Morris 1990].
We do not adopt pp-fairness as defined above, for it seems to be overly opti-
mistic. We feel that starvation of an arbitrary predicate is not sufficient for calling
a sequence unfair. The reason is that it may be easy to make the predicate true
and then false again. Therefore, instead of using starvation of arbitrary predicates,
we shall use starvation of stable predicates. Here, stability is the same concept as
in Section 14.1. For a repetition while b do c od, a predicate p is stable if and
only if [p A b => wp.c.p].
We thus define a sequence to be not weakly p-fair if there is a stable predicate,
false along the sequence, that eventually can be established at every choice. The
sequence is called not strongly p-fair if there is a stable predicate, false along the
sequence, that can be established at infinitely many choices. The corresponding
concepts of weak p-fairness and strong p-fairness seem to be very close to Morris's
formalizations of fairness. We do not claim equality, since the above description is
not yet a strict definition.
As an example, consider repetition (2) with ra = 4. In this case, every infinite
execution sequence is infinitely often in a position where it can choose to establish
x mod 4 = 0. Therefore, repetition (2) with ra = 4 terminates fairly with respect
to strong p-fairness.
For m > 5, however, an execution sequence with (eventually) x alternating
between 2 and 3 has no stable predicate, false along the sequence, that can be
established infinitely often. Therefore, if ra > 5 and initially x mod ra ^ 0, repe-
tition (2) does not fairly terminate with respect to strong p-fairness.

196 
PREDICATIVE 
FAIRNESS
15.2 An abstract syntax and weak fairness
In the remainder of the chapter we describe a formalization of predicative fair-
ness, which is inspired by [Morris 1990], but applicable to an arbitrary recursive
declaration.
Our first concern is that we need syntactic means to indicate which choices are
supposed to be fair, and when such a choice is enabled. We do not want to change
the main syntax. So we retain the set A0, the declaration body and the induced
functions wp and wlp. In particular, we retain the operator '[' for nondeterminate
choice.
We restrict our fairness considerations to the choices made when execution
opens a procedure body. The fairness constraints of these choices are specified by
extending the declaration body in the following way. We assume that for every
procedure h G H a nonempty set fc.h is given consisting of pairs (c, t) G IP x A.0
such that
(3) 
body./i 
= ( \ (c, t) G fc.h :: ?c; t) .
Symbol fc stands for 'fair choice'. Predicate c is the enabling condition of branch
t. We postulate that, for every procedure h and every state, at least one branch is
enabled, that is
(4) 
[ (3 (c, t) G fc.h :: c) ] 
for all heH.
In the concrete syntax for the extended declaration, we shall use a fair choice
operator ' [/ ' and the notation
body./i 
= 
( J / i e / : : c.i â> t.i)
when fc.h is the set of the pairs (c.i, t.i) with i G I. If the set / is finite, we often
use ' ]/ ' as an infix operator. If c.i = true, we may prefer to omit 'c.z ââ¢>'.
The semantics given by wp and wlp is unchanged, but the extended declaration
fc is used to construct a weakest fair precondition function wfp. Since the only effect
of a fairness assumption is to neglect certain infinite execution sequences, we expect
wfp to satisfy the termination law
(5) 
wfp.r.p = 
wfp.r.true A wlp.r.p 
for all r G A0, p G P .
Fair termination of a command should only depend on the fair termination of its
induced procedure calls. Therefore, we postulate that
(6) 
wfp = witter ,
cf. formula 14(0), where fter G TPH is the function such that fter.h is the precondi-
tion that h fairly terminates.
We come back to the extended declaration. It is important to know whether a
call h has an enabled branch that fairly terminates. This condition is expressed by

15.3 A GENERAL FAIRNESS DEFINITION 
197
(3(c,i) G fc./i :: c A wfp.t.true) .
By formula (6) this condition is equivalent to weak. fter.h where, for every z G IP ,
we define
(7) 
weak.z.h 
= (3 (c,t) G fc./i :: c A wi.z.J.true) .
Notice that function weak E IP â> IP is monotone.
Weak fairness means that a procedure call h fairly terminates whenever all
its induced procedure calls have an enabled branch that fairly terminates. This is
formalized in
[ alw.(weak. tier).h =Â» fter.h] 
for all he H,
and hence
(8) 
alw.(weak, fter) < fter .
In this way we come to the following definition, cf. [Morris 1990]. Function fter G
JPH is defined as the smallest solution of inequality (8), or rather
(9) 
fter = (inf z G 1PH : alw.(weak.z) < z : z) .
15.3 A general fairness definition
We generalize definition (9) as follows. For every monotone function ^ G IP â>
IP , we define fair.ip by
(10) 
fair4 
= (infz G P H :: alw.^.z) < z : z) .
Now definition (9) becomes
(11) 
fter 
= 
fair.weak ,
so that, henceforth, the function fair.weak can be used to characterize weakly fair
termination.
By a version of the theorem of Knaster-Tarski (cf. exercise 4.2.0), the element
fair.if) is the smallest fixpoint of alw o ?/>, i.e. the smallest solution z of
(12) 
alw.(if).z) = z .
Now, it follows from Theorem 14(13), that
(13) 
term 
= 
fair.unf .
By an exercise of Section 4.2, the expression fair.ij) is monotone in \j>. This implies
that
(14) 
unf < xj) =$> term < fair.if? .
This implication is used to prove that termination implies weakly fair termination:
(15) 
term < fair.weak .
In fact, by (13) and (14), it suffices to prove unf < weak. This is verified by
observing that for every z E JPH and h G H

198 
PREDICATIVE 
FAIRNESS
unf.z.h
= 
{definition 14(10)}
wl. z. (body./i). true
= 
{(3); let (c, t) range over fc./i}
{calculus}
,t) :: c=>wl.z.t.true)
{postulate (4)}
(3 (c, *) :: c) 
A (V (c, t) :: c =Â» wl.z.i.true)
* 
{calculus}
= 
{calculus and (7)}
weak.z.h .
Surprisingly, the general fairness definition (10) suffices to prove a correct-
ness rule and a necessity rule. We begin with the correctness rule for general fair
termination.
(16) Theorem. Let N be a well-founded set (cf. Section 5.6). Let (n E N :: v.n)
be a family of stable functions in TPH such that for all n Â£ N
v.n < (supz G N : i < n : tp.(v.i)) .
Then v.n < fair.ip for all n â¬ N.
Proof. By induction over the well-founded set iV, it suffices to prove that for every
n e N
(Vi Â£ N : i < n : v.i < fair.ip) 
=^ v.n < fair.tp .
This is proved in
v.n < fair.ij>
<= {transitivity of <, and assumption}
(supz E N : i < n : ij).(y.i)) < fair.if)
= 
{definition supremum in 4(0)}
(Vi E N :i <n: 
if>.(v.i) < fair.tf>)
= 
{(12)}
(Vi E N :i <n: 
i/>.(v.i) < alw.(i/>.(fair.r
<= {alw.z < z and ift monotone}
(Vz E N : i < n : v.i < fair.ift) .
(End of proof)

15.3 A GENERAL FAIRNESS DEFINITION 
199
General fairness also has a necessity rule (compare 2(26)):
(17) Rule. Let z e F H be a function with t/>.z < z. Then fair.ip < z.
Proof. It suffices to observe that
fair.ifi < z
<= {(10)}
alw.(ift.z) < z
4= 
{definition of alw}
tp.z < z .
(End of proof)
It is reasonable to expect that fair termination implies the possibility of ter-
mination. We must be careful, however. In fact, even necessary termination only
implies possible termination for total commands (cf. Section 3.2). We now show
that weakly fair termination implies possible termination under a certain totality
condition on the extended declaration.
(18) Theorem. Assume that, for every branch (c, t) 6 fc./i, command t is syntac-
tically total. Then fair.weak < aterm.
Proof
fair.weak < aterm
<= 
{(17)}
weak, aterm < aterm
= 
{(7); let h range over i?, and (c,t) over fc.h}
(Vft :: [(3c,t :: c A wl.aterm.t.true) 
=> aterm.h])
= 
{predicate calculus: exercise 1.1.4(b)}
(V/i, c,<:: [c A wl.aterm.t.true 
=$> aterm.h])
= 
{14(26)}
(V/i,c,t:: [c A wl.aterm.t.true 
=>> -iwlp.h.false])
= 
{declaration (3); fixpoint property of wlp}
(V/i,c,t:: [c A wl.aterm.t.true 
=> -i(Vc,t :: c=^wlp.t. false)])
= 
{predicate calculus}
(yh,c,t 
:: [c A wl.aterm.t.true 
=> (3c,t :: c A -*wlp.t.false)])
<= {one point rule; calculus}
(\/h,t:: [wl.aterm.t.true 
=$> -Â«wlp.t.false])
= 
{predicate calculus and 14(1)}

200 
PREDICATIVE 
FAIRNESS
(\/h,t:: [wl.aterm.t.false =Â£â¢ false])
= 
{14(27)}
all t are syntactically total.
(End of proof)
15.4 Examples
Example 1. In [Morris 1990] we find an example with an integer program variable
i and two procedures gl and g4 declared by
body.^l 
= 
(skip Q/ i := i + 1 ; gl) ,
body.^4 
= 
( ?(i = 10) | ?(i ^ 10) ; i := 0 ; gl ; g4) .
Morris gives two systems of fairness (see loc. cit.). In his first system, a call of g4
terminates fairly. In his second system, however, it only terminates fairly under
the precondition i = 10. Our definition of fairness turns out to lead to the second
conclusion. This is proved by means of rule (17), taking H = {#1, #4} and z G JPH
given by
z.gl â true 
, z.g4 = (i = 10) .
Since z.gl = true, we have
weak.z < z 
= 
[ weak.z.g4=> z.g4] .
The righthand side holds, as is proved in
weak.z.g4
= 
{definition (7); body.^4 contains no ' [/ '}
wl.z.(hody.g4).true
= 
{14(0) and declaration}
(i = 10 => true) 
A (i ^ 10 => wlp.(Â± := 0).(wl.z.gl.(wl.z.g4.true)))
= 
{calculus; 14(0); z.gl = true and wlp.g4.true = true}
i = 10 V wip.(i := 0).(wlp.gl.(z.g4))
= 
{definition z.g4 and declaration #1}
i = 10 V wlp.(i := 0).false
= 
{calculus and definition z.g4}
z.g4 .
By rule (17) this shows that fair, weak < z and hence
[fair.weak.g4 
=> i = 10] .
The converse implication is easy. (End of example)

15.4 EXAMPLES 
201
Example 2. Let H = {hO} be declared by
body.hO 
= 
(skip |/ hO ; hO) .
Then fc.hO contains a branch (c, t) = (true, skip), which satisfies
c A wi.z.tf.true 
= 
true 
for every z E P H .
By definition (7), it follows that weak.z.hO = true for every z E IP^. We now
use Theorem (16) with v.O.ftO = fa/se and v.l.hO = true to prove that hO fairly
terminates. This result suggests that the present formalization of weak predicative
fairness is overly optimistic. (End of example)
Example 3. Let x be an integer program variable. Let H = {hi} with hi declared
by
body./il 
=
(x = 0 -> skip |/ x ^ 0 -Â» x := x + 1 ; hi ]/ x ^ 0 -> x := x - 1 ; fel) .
In this case the only stable predicates are true, false, x > 0, x < 0 and x = 0. If
function z is given by z.hl = (x = 0), one can verify that
weak.z.hl 
= 
(-1 < x < 1) ,
so that alw.(weak.z) â z. Now it follows from definition (9) that hi fairly termi-
nates only under precondition x = 0. Notice that procedure ^1 is almost equivalent
to the stack implementation of procedure hO of exercise 2, with x representing the
stack size. (End of example)
Example 4. In this example we show that Theorem (18) indeed needs syntactic
totality. Let b be a boolean program variable. Let H = {hO, hi}, declared by
body.M) 
= 
(hO |/ b := false) ,
body./il 
= 
(hO ; ?b) .
In an example in Section 14.5, we have shown that body./il is not syntactically
total. We now show that
-â¢(fair.weak.hi < aterm.hl) .
In fact, we show that fair.weak.hl 
= true and aterm.hl 
= false. The second
assertion is proved in
aterm.hl
= 
-*wlp. hi. false
= 
~iwlp.hO.(wlp.(?b). false)
= 
-itrue
= 
false .
The next thing is to determine the stable functions z Â£ WH. For z Â£ WH we have

PREDICATIVE FAIRNESS
wl.z. (body. hi).true
= 
wl.z.hO.(wl.z.(?b).true)
= 
wl.z.hO. true
= 
z.hO ,
and also
wl.z. (body./iO). true
= 
wl.z.hO.true A wl.z.(b := false).true
= 
Â£./i0 A true
= 
*.M) .
It follows that
z is stable
= 
{definition 14(3)}
[z.hl=>z.hO] A [z.hO^z.hO]
= 
{calculus}
[z.hl^z.hO] .
The function weak is calculated in
weak.z.hi
= 
{(7); declaration without choice}
= 
{above calculation}
z.hO ,
together with
weak. z.hO
= 
{(7); declaration with an unguarded fair choice}
wi.z./iO.true V wl.z.{\> := false).true
= 
{second conjunct is true}
true .
It follows that weak.z is stable for every function z E IP , so that alw o weak =
weak. It also follows that weak.(weak.z).h â true for all z Â£ P ^ and h G H. This
implies
fair. weak.h = true 
for all h G H.
In particular, we have fair.weak.hl = true, as announced. (End of example)
Example 5. Let H = {/i0, /il}, declared by
body./iO = (e D/ hi) ,
body./il = /iO .
For a function z Â£ P 
we observe that

15.5 THE OPERATIONAL MEANING 
203
z is stable
= 
{definition 14(3), and declaration}
[z.hO=>true Az.hl] 
A [ z.hl => z.hO]
= 
{calculus}
[ z.hO = z.hl] .
One easily verifies that both procedures need not terminate:
term.h = false 
for both h G H.
We calculate
weak.term.hl
= {(7)}
true A wl. term. hO. true
= 
(14(0)}
true A term.hO A wlp.hO.true
= 
{above calculation}
false .
By the characterization of stable functions obtained above, it follows that
alw.{weak.term) < term .
By definition (9), this implies that fair.weak < term, and hence, by formula (15),
that fair.weak = term. The conclusion is that procedures hO and hi do not fairly
terminate! Indeed, in the unique nonterminating execution path the choice for
termination is enabled alternatingly, but not 'eventually at every procedure call'.
(End of example)
15.5 The operational meaning
In the discussion in Section 15.1, the concept 'execution sequence' was used rather
loosely. When investigating recursive procedures, we had better be more careful.
The operational meaning of fairness is based on the operational semantics. So,
we use the configuration graph of general recursion, cf. Section 9.4. This is the set
X X A* U {oo} with binary relation 'â>' given by
(x,q) -> oo =
(3 c G S,r G A* :: q = c; r A [cj.oo.a:) and
(z,q)-*(y,r) =
(3 c G 5 :: q = c; r A [cj.y.a:)
V (3 h G H, s G body./*, t G A* :: q = h; t A r = s; t A x = y) .

204 
PREDICATIVE 
FAIRNESS
The elements of X x A* U {00} are called configurations. An execution sequence is
defined to be a finite or infinite sequence of configurations u.i such that
u.i â> i/.(z + 1) 
for all z.
Relation u â¢*> v holds if and only if there is an execution sequence with u.O = u and
u.n = v. Relation u -*â¢ 00 means that there is an execution sequence that starts in
u and ends in 00 or is infinite.
An execution sequence is called maximal if and only if it is infinite, or ends
in 00 or in a configuration of the form (x,s). Notice that there exist nonmaximal
execution sequences that yet cannot be extended. Such a sequence ends in a config-
uration of the form (#, c; r) such that -i[c].y.:r for all y E X. Usually, c is a guard
?b with -i&.#.
In a maximal execution sequence (i :: u.z), a term u.i = (x, a;t) with a E A is
said to be terminating if and only if there is some j > i with u.j = (y,t) for some
state y E X. If all terms of the sequence terminate, induction on the length of the
initial string yields that the sequence is finite. Therefore, every infinite execution
sequence has nonterminating terms. Since a tail of an infinite execution sequence
is also an infinite execution sequence, every tail of an infinite execution sequence
has nonterminating terms. This proves that every infinite execution sequence has
infinitely many nonterminating terms.
If u.i is a nonterminating term of an infinite execution sequence, then u.i =
(x, h; t) for some x E X, h E H, t E A*. All subsequent terms of the sequence are
of the form (y^q;t) with q ^ e and (#, h) -*â¢ (y,#). In other words, the call of h
diverges and command t is never reached. In that case, we speak of the divergent
procedure call (x, h). In this way, we see that the nonterminating terms of an infinite
execution sequence are associated to a list of nested divergent procedure calls.
We can now show that the formal definition of fairness has something to do
with starvation of stable predicates.
(19) Theorem. Let (#0, hO) be a procedure call with fair.ip.hO.xO. For every infinite
execution sequence starting in (xO, hO) there is a stable function z\ E IP such that
all divergent calls (x, h) of the sequence satisfy -izl.h.x and that eventually all calls
(x,h) satisfy ip.zl.h.x.
Remark. In this abstract setting, predicate ifr.zl represents the idea that z\ can be
established. The theorem shows unfairness of the execution sequence in the sense
that function JSTI is subject to starvation. Notice, however, that we do not exclude
the possibility that zl.h.x holds for some terminating calls. (End of remark)

15.5 THE OPERATIONAL MEANING 
205
Proof. Let L be the set of the divergent calls (#, h) of the execution sequence. Let
U be the subset of TPH given by
(20) 
z G U = 
z is stable 
A (V(#, h) G L :: -*z.h.x) .
Put jarl = (sup [/) G IPH. Function zl is stable by Lemma 14(5). For every divergent
call (x,h) of the sequence, we have
zl.h.x
= 
{definition zl}
(sup 2 G U :: z).h.x
= 
{calculus}
(3z G U :: z./i.x)
= 
{(20)}
false .
This proves
(21) 
(V(a?,/i) G i :: -i^l./i.a:) .
In order to prove the existence of a divergent call (x,h) with 
alw.(ip.zl).h.x,
we observe that
-i(3(a;,/i) E i : : alw.(^.zl)Ax)
= 
{calculus}
(V(x,h) G L :: -ia/w.(^.2?l)./i.a:)
= 
{(20) and 14(7)(a)}
alw.(t/>.zl) G C/
=^ 
{definition zl}
alw.(rp.zl) < zl
fair.il> < zl
=> 
{fair.if).hO.xO is given}
zl.hO.xO
= 
{(21); (a:0, feO) is divergent}
false .
This proves the existence of a divergent call (xl^hl) G L such that
Since alw.(i/>.zl) is stable, it follows from Theorem 14(4) that all procedure calls
(x,h) after (xl,hl) 
satisfy alw.(i/>. zl). h.x, and hence also (ip.zl). h. x. (End of proof)
Remark. In [Morris 1990] Section 5.1, an analogous property is given. Predicate
Q0 of loc.cit. is the analogue of our function zl. It is a stable predicate, although

206 
PREDICATIVE 
FAIRNESS
he does not claim that. The proof of loc.cit. is based on transfinite induction. (End
of remark)
15.6 A proposal for strong fairness
We now discuss a formalization of strong fairness as considered in Section 15.1.
Inspired by [Morris 1990] Section 9, we use function evt of definition 14(24) to
define strong G 1PH -+ 1PH by
(22) 
strong.z.h 
= 
evt.(weak.z).h 
.
By the description of Evt in Section 14.4, predicate strong.z.h says that a call of
h induces (or is itself) a call k for which weak.z.k holds. It follows from formula
14(20) that
(23) 
[ weak.z.h 
=> strong.z.h] 
for all z e 1PH and he H.
Since fair.ijj is monotone in V>, this implies that
(24) 
fair.weak < fair.strong .
We might regard fair.strong.h as the condition that h fairly terminates with respect
to strong fairness. A more precise interpretation is based on Theorem (19). In fact,
let (:r0, hO) be a call with fair.strong.hO.xO. Consider an infinite execution sequence
starting in (#0, hO). By Theorem (19) there is a stable function z\ Â£ P 
such that
all divergent calls (#, h) of the sequence satisfy strong.zl.h.x. 
Therefore, eventually
every divergent call (#, h) induces a call (y, k) with weak.zl.k.y. 
This means that k
has an enabled branch that establishes z\. Unfortunately, this does not mean that
the divergence of call (#, h) is strongly p-unfair in the sense of 15.1. In fact, it may
be that call (y, k) actually establishes #1, terminates, and is followed by another
divergent call. This phenomenon is illustrated in the next example.
Example. Let H â {^0, /il}, declared by
body./iO 
= 
(hi ; hO) ,
body./il 
= 
e .
It is clear that procedure hO cannot terminate. Yet, for every function z Â£ P H , we
have
strong. z.hO
= 
{(22)}
Evt.(weak. z).hO. false
<= {14(20) and declaration}
Evt.(weak.z).(hl 
; hO).false

15.6 A PROPOSAL FOR STRONG FAIRNESS 
207
= 
(14(22)}
Evt.(weak. z).hi.(Evt.(weak. z).hO. false)
4= 
{14(20) and declaration}
weak.z.hl
= {(7)}
true 
A 
wl.z.e.true
= 
(14(0)}
true .
By (23) and the last part of this computation, we also have strong.z.hl = true.
Therefore strong.z.h = true for all z and all h. Using Theorem (16) with v.n.h =
(n > 0) for all h G if, we get
fair.strong.hO â true ,
so that hO strongly terminates according to our formalism.
A similar example with simple recursion and a boolean program variable b is
procedure h declared by
body./i 
= 
(?b ; b := -Â»b | ?-ib ; b := -ib ; h ; h) .
Here we have a deterministic program that terminates if and only if initially b
holds. According to our formalization, strongly fair termination holds regardless
of the initial value of b. This can be proved by first showing that [ b =$> weak.z.h]
for every z G P ^ , and subsequently that [ strong.z.h] for every z G P ^ . (End of
example)
Concluding Remark. The formal definitions of this chapter must be regarded as
provisional or even tentative. We have therefore refrained from providing exercises.

CHAPTER 16
SOLUTIONS OF EXERCISES
Exercise 1.4,0. (a)
c; miracle = miracle
= 
{(7), let wg range over wp, wlp and p over IP}
(V wg,p :: wg.(c; miracle).p = wg.miracle.p)
= 
{(13)}
(Vwg,p :: wg.c.(wg.miracle.p) = wg.miracle.p)
(Vwg :: wg.c.true = true)
= 
{(2)}
[wp.c.true] A [wlp.c.true]
= 
{by (4), the lefthand conjunct implies the other one}
[wp.c.true] .
Exercise 2.3.0. (a) Clearly, t is an external variable threatened to be modified.
Since t and y may change, we introduce specification constants T and Y and the
postcondition Q : t = T A y = Y\ The precondition wp.(hody.h).Q is easily
calculated. In this way we get the specification
{ext t!; all T, Y G integer ::
pre P : y = TAa;+t = 7, post Q: t = T A y = 7} .
The verification of the conditions (a), (b), (c) of (7) is immediate.
(b) Since program variable w is modified by the call, we assume that the call
is h(E,w) for some expression E. Rule (10) yields
{v = T AE + t = Y AR} 
h(E, w) {t = T A w = Y A R}
for all values Y and T and all predicates R with Var.Rn {t,w} = 0. In particular,
choosing T := X and Y := 0 and R := true, we get
,w) {t = l A w = 0} .

CHAPTER 
16 
209
This fits our aim when we take the actual parameter E := â t.
Exercise 2.8.5. We introduce the abbreviation q = wlp.L.(pV b) and observe that
q
wlp.(body.L).(p 
V b)
â {(27) and calculus}
(-,& => p v b) A (6 => wip.(c; L).(p V 6))
= 
{calculus and definition of q}
(p V 6) A (&=> wlp.c.g)
=Â£- 
{predicate calculus}
p V (6 A wip.c.g) .
This proves [ ^ p V ( i A wlp.c.q)]. Since [6=>pVvf>0], it follows that
[ q => p v vf > 0 ] .
This may suggest the introduction of
R.n: 
[q =>pV vf >n] 
for n G IN.
Then we have iZ.O. The implication R.n =$> R.(n + 1) is proved in
q
=> {above calculation}
p V (6 A wip.c.g)
=> 
{R.n and monotony}
p V (6 A wJp.c.(p V vf > n))
=$> {assumption}
p V (p V vf > n)
=Â£- 
{calculus}
p V vf > n + 1 .
This proves that R.n holds for all n G IN. It remains to observe
(Vn G IN :: i2.n)
= 
[9 => (Vn ::pV vf > n)]
= 
[q => p V (Vn :: vf > n)]
= 
[ g =Â» p V false ]
Exercise 3.5.0. (a) It suffices to observe that
wp.L. false
= 
{2(14), 2(27) and calculus}
false) A (b=> wp.c.(wp.L.false))

210 
SOLUTIONS OF EXERCISES
= 
{calculus}
b A wp.c.(wp.L.false)
<= {wp.c is monotone}
b A wp.c.false
= 
{calculus}
wp.(!6; c).false .
(b) We use a proof by annotation, cf. Section 2.8:
{i>0}
while true do 
{vf = i}
{i > 0 A true A i < m}
?(i^0) 
{ i > 0 A i < m }
i := i - 1 
{ i > 0 A i < m A r a > 0 }
od {i > 0 A -^true}
{false} .
(c) In part (b) we proved that [i > 0 =Â» wp.L.false]. In the situation of part (b),
one verifies that wp.(\b] c).false = (i = 0). Finally, we observe that i > 0 does not
implies i = 0.
Exercise 4.1.4. It suffices to verify that for all z E W
z < (inf w e W : (3U G Q :: w G U) : w)
= 
{(1)}
(\/w G W : (3U G Q :: w G 17) : z < w)
= 
{trading}
(Vu; G W :: (3 U G Q :: w G 17) =Â» z < w)
= 
{p => ^ = ~Â«p V ^ (twice) and De Morgan}
(Vu; eW :: (VU e Q :: w E U ^ z < w))
= 
{interchange of quantifications}
(VU EQ::(VweW 
::weU 
=> z< w))
= 
{(1)}
= 
{(1)}
Â£Q::(miU)) .
Exercise 4.9.0. Let WO be the subset of WLP given by
Since [p=> wlp.a.p] for all a G S D C, it follows that
[p=>w.a.p] 
for all w; G VFO and a G C.

CHAPTER 
16 
211
Using definition (15), one can prove that
(*) 
[p^w.c.p] 
for all w G WO and c G CÂ®.
Since body./i G C 0 for all h E # H C, it follows that
[p=>w.(body.ft).p] 
for all w G WO and h e 
HOC.
Now Theorem (44) implies that wip G WO. By (*), this proves that
.c.p] 
for all c G
Exercise 5.1.4.
(a) 
?p;c C c;?g
= 
{(1), let wg range over {wp, wlp} and r over IP}
(Vwg,r :: [wg.(?p;c).r =Â» wg.(c; ?q).r])
= 
{calculus}
(Vwg-,r :: [~ip V wg.c.r => wg.c.(^=^r)])
= 
{ wg.c is monotone and [ r => (q =^ r) ]}
= 
{axiom 1(4); the term is monotone in r}
[-ip =$> wp.c.(q=>false)])
= 
{calculus}
[p V wp.c.(-*q)}) .
(b) 
c;?p Q?q;c
= 
{as above; first two steps}
(*) 
(V wg, r ::[ wg.c.(p =>r) => (q=> wg.c.r) ])
= 
{shunting}
(Vwg,r :: [wg.c.(p=>r) Ag => wg.c.r])
4= 
{calculus}
[q=> wlp.c.p] A (ywg,r :: [ wg".c.(p=^ r) A wip.c.p =
= 
{axiom 3(4) and exercise 3.3.1}
[q =$> wlp.c.p] A (Vwg,r :: [ wg.c.((p=> r) A p) => w
= 
{ wg*.c is monotone and [ (p ^ r) A p =^ r ]}
[g=^ wlp.C.p] .
The other implication is proved as follows. Formula (*) implies
[wlp.c.(p=$>p) =$Â» (q=> wlp.c.p)] .
By formula 3(5), this equivales [q=> wlp.c.p].
Exercise 6.4.2. Consider predicate r defined by
r.y = (3x e X : {cj.y.x : p.x) .
For every predicate q we have

212 
SOL UTIONS OF EXERCISES
= 
{definition r and 1(2)}
(Vy ::(3x : {cj.y.x : p.x) => q.y)
= 
{calculus}
(Vy :: (Vz : (cj.y.x : p.x => q.y))
= 
{interchange}
(Vx :: (Vy : [cj.y.x : p.x =Â» g.y))
= 
{calculus}
(\/x :: p.x => (Vy : [c].j/.a? : q.y))
= 
{(1) and (15)}
(\/x :: p.x =>
= 
{1(2)}
[p=> wlp.c.q]
= 
{5(10)}
This implies that r = sp.c.p.
Exercise 8.2.0. (a) The set R is sup-closed in MT since for every subset U of
MT we have
(sup U) e R
= 
{definition of i?}
(sup U) o e < g o (sup 17)
= 
{4(2), let p range over IP}
(\/p :: (supC0.(e.p) < ff-((supt/).p))
= 
{4(3) and 4(0), let u range over U}
(Vp,u :: u.(e.p) < g.((supU).p))
= 
{monotony of y}
(Vp,w :: w.(e.p) < g.(u.p))
= 
{definition of i?}
The second assertion now follows from Theorem (4)(c).
(b) By Theorem (9)(a,f), it suffices to prove that R is closed under functional
composition:
f , h e R
= 
{definition of R}
f o e < g o f 
A 
h o e < g o h

CHAPTER 
16 
213
foeoh<gofoh 
A 
fohoe<fogoh
=Â» 
{g < e and 4(7)(d)}
fohoe<fogoh<foeoh<gofoh
=> {definition of R}
foheR.
(c) By Theorem (9)(a,g), it suffices to prove that R is closed under finite infima:
for any finite subset U of MT we have
(inf 
U)eR
= 
{definition of R}
(Vp :: (inf u G U :: u.(e.p)) < g.(iniu G U :: u.p))
= 
{g commutes with finite infima}
(Vp :: (inf u e U :: u.(e.p)) < (inf u e U :: g.(u.p)))
<Â£= 
{monotony}
(Vp,w :: w.(c.p) <g.(u.p))
= 
{definition of i?}
17C-R.
Exercise 11.3.1. Let iÂ£ be the set of pairs of commands
(g ; v := k * v 
, 
v := & ; h) .
By rule (5), it suffices to prove E C (^). This is proved by means of rule (6). Since
abort; v := k * v 
= 
abort 
â v := i ; abort ,
we have abort0.q = abort 0.r for all pairs (^,r) G Â£*. By (4)(a), the commands
g, h, v := k and v := k * v all belong to Lia. Since (v := & * v) G S 0, it follows
from (4)(d) that (g; v := & * v) G Lia. Since v := k is a deterministic simple
command, we have (v := k) G Sy.Muc by Theorem 8(20)(a). By (4)(d) it follows
that (v := k ; h) G Lia. This proves the first condition of rule (6).
For every congruence (~) on A 0 that contains E U (Â«), we have
body 0.(# ; v := A; * v)
= 
{declaration g}
(c ; v := 1 | ([| i G / :: d.i ; ^ ; v := z * v)) ; v := A; * v
Â« 
{v := 1 ; v := fc * v = 
v := k ;
v := i * v ; v := A: * v = 
v : = A : * i * v ; now use (7)}
c ; v := A; Q ( | i G / :: d.i ; ^ ; v := k * i * v)
~ 
{JE C (~) and ~ is a congruence}
c ; v := A; I (J i G / :: rf.z ; v := A; * i ; ft)
w 
{v := k ; v := v * i 
= 
v := k * i ; use (7)}
c ; v := fc I ( [ i G / :: rf.i ; v := k ; v := v * i ; h)

214 
SOLUTIONS OF EXERCISES
w {c and d.i commute with v := k, and (7); declaration of h}
b o d y 0 . ( v :=k; 
h) .
Now rule (6) yields E C (Â«).
Exercise 11.3.4. In order to show that the rule proposed is applicable, we observe
abort0.!, ^ abortÂ°.M A !, M e Lia
= 
{(0), 10(0) and (4)(a)}
?-i6 D ?6 ; c ; abort Â£ 
?-.& || ?6 ; abort
= 
{use choice of c}
true .
For every congruence (~) that contains E U (Â«), we have
body 0.! - bodyÂ°.M
= 
{10(0)}
body.! f>j body.M
= 
{^ is a congruence that contains w, and body./i = h for all /i}
!-M
= 
{~ contains Â£/}
true .
The invalid rule would imply that ! = M. We have
! 
= while i ^ 0 do i := i - 1 od ,
M = while i ^ 0 do e od .
Using the techniques of Chapter 2, one can easily prove that wp.L.true = (i > 0)
and wp.M.true = (i = 0). So, ! and M are semantically different. Therefore, the
proposed rule is not valid.
Exercise 13.4.0. For any relation E on commands we have
(V(g,r) E E :: abort0^ C abort0.r)
(sup0) G Wgo.S A (inf 0) G
4= {0 is a chain, see Section 7.2}
Wgo.Â£^ is sup-decked and Wg1.E is inf-decked
4= 
{Section 13.2}
E is safe
<= {Theorem (12)}
Since (<C) is a preorder, we have q <C q for all commands q. So, for the second
question it suffices to exhibit a command q with q fi Lia. This has been done in

CHAPTER 
16 
215
Section 11.7.
Exercise 14.1.3. (a) First use Hoare's Induction Rule to prove that wlp.h.false â
true, i.e. that ft is guaranteed not to terminate. By (0), it follows that
(*) 
wl.z.h.p = z.h 
for all p G P .
Now z, given by z.h = (v > z), is stable because of
(wl.z)0.(body.h).true
= 
{declaration ft}
(wl.z)Â°.(v := v + 1 ; ft ; v := v - 2 ; h).true
= 
{use (*) with p := (wl.z)0.(v := v - 2 ; ft).true}
= 
{4(25) and definition z}
WSQ.(V := v + l).(v 
> i)
= 
{assignment}
v + 1 > i
<= {definition z}
z.h .
(b) 
(wl.z)0. (body. ft), true
= 
{declaration ft}
(wl.z)0.(skip 
D v : = v + 1 ; ft; v : = v - 2 ; 
h).true
= 
{(wl.z)0 is a homomorphism that extends wl.z}
(wl.z)0 .skip, true
A (wl.z)Â°.(v := v + 1 ; ft; v := v - 2).(wl. z.h. true)
= 
{(wi.*)Â°|S= ws0 and(0)}
true A (wlz)Â°.(v 
:= v + 1 ; ft; v := v â 2).(z.h A wlp.h.true)
= 
{calculus, definition of z and 3(5)}
(wl.z)Â°.(v := v + 1 ; ft; v := v - 2).(v > i)
)Â°.(v := v + 1 ; ft).(v - 2 > t)
= 
{(0) and calculus}
wso.(v := v + l).(z.h A wlp.ft.(v > i + 2))
= 
{exercise 5.7.0}
wso.(v := v + l).(2.ft A faise)
= 
{(wlz)Â°\S 
= ws0}
false .
Clearly, z.h does not imply false. Therefore, z is not stable.

216 
SOLUTIONS OF EXERCISES
Exercise 14.2.0.
wl.z is a fixpoint of Do
= 
{let h range over H and p over IP; use 4(26)}
(Vfe,p :: wl.z.h.p = (wl.z)0 .(body.h).p)
= 
{(O)and(l)}
(V/i,p :: z.h A wlp.h.p = (wl.z)0.(body.h).true 
A wlp.(body.h).p)
= 
{(10) and 4(19)}
(V/i,p :: z.h A wlp.h.p = unf.z.h A wlp.h.p)
= 
{ 4= : easy; ^ : use p := true and 3(5)}
(V/i :: z.h = unf.z.h)
= 
{equality of functions}
jar is a fixpoint of unf.

REFERENCES
K.R. Apt [1981]: Ten years of Hoare's logic. A survey - Part 1. ACM Trans. Pro-
gram. Languages and Systems 3 (1981) 431-483.
K.R. Apt, G.D. Plotkin [1986]: Countable nondeterminism and random assignment.
J. ACM 33 (1986) 724-767.
R.J.R. Back [1988]: A calculus of refinements for program derivations. Acta Infor-
matica 25 (1988) 593-624.
R.J.R. Back, J. von Wright [1989a]: A lattice-theoretical basis for a specification
language. In: J.L.A. van de Snepscheut (ed.): Mathematics of Program Construc-
tion, Lecture Notes in Computer Science 375 (Springer, Berlin, 1989) pp. 139-156.
R.J.R. Back, J. von Wright [1989b]: Combining angels, demons and miracles in
program specifications. Abo Akademi A86, Turku Finland, 1989. To appear in The-
oretical Computer Science.
R.J.R. Back, J. von Wright [1990]: Duality in specification languages: a lattice-
theoretical approach. Acta Informatica 27 (1990) 583-625.
R.C. Backhouse [1986]: Program Construction and Verification (Prentice-Hall In-
ternational, 1986).
J.W. de Bakker [1980]: Mathematical Theory of Program Correctness (Prentice-
Hall, 1980).
J.W. de Bakker, L.G.L.T. Meertens [1975]: On the completeness of the inductive
assertion method. J. Comput. Syst. Sci. 11 (1975) 323-357.
J.W. de Bakker, W.P. de Roever [1973]: A calculus for recursive program schemes.
In: M. Nivat (ed.): Automata, Languages and Programming 1972 (North Holland,
1973) pp. 167-196.
M. Barr, C. Wells [1990]: Category Theory for Computing Science (Prentice Hall
International, 1990).
K.M. Chandy, J. Misra [1988]: Parallel Program Design, A Foundation (Addison-
Wesley, 1988).

218 
REFERENCES
E.W. Dijkstra [1975]: Guarded commands, nondeterminacy and formal derivation
of programs. Commun. ACM 18 (1975) 453-457.
E.W. Dijkstra [1976]: A Discipline of Programming (Prentice-Hall, 1976).
E.W. Dijkstra (ed.) [1990]: Formal Development of Programs and Proofs. University
of Texas at Austin Year of Programming Series (Addison-Wesley, 1990).
E.W. Dijkstra, C.S. Scholten [1990]: Predicate Calculus and Program Semantics
(Springer V, 1990).
N. Prancez [1986]: Fairness (Springer V, 1986).
J.H. Gallier [1987]: Logic for Computer Science. Foundations of automatic theorem
proving. (Wiley & Sons 1987).
D. Gries [1981]: The Science of Programming (Springer V, 1981).
D. Harel [1984]: Dynamic logic. In: D. Gabbay, F. Guenthner (eds.): Handbook of
Philosophical Logic, Vol. 2 (Reidel, 1984) pp. 497-604.
E.C.R. Hehner [1979]: do Considered od : a contribution to programming calculus.
Acta Informatica 11 (1979) 287-304.
E.R.C. Hehner [1984]: Predicative programming, Part 1. Commun. ACM 27 (1984)
134-143.
E.R.C. Hehner [1992]: A Practical Theory of Programming. Forthcoming.
W.H. Hesselink [1988]: Interpretations of recursion under unbounded nondetermi-
nacy. Theor. Comp. Sci. 59 (1988) 211-234.
W.H. Hesselink [1989a]: Initialisation with a final value, an exercise in program
transformation. In: J.L.A. van de Snepscheut (ed.): Mathematics of Program Con-
struction, Lecture Notes in Computer Science 375 (Springer V, 1989) pp. 273-280.
W.H. Hesselink [1989b]: Processes and formalisms for unbounded choice. Tech. Rep.
CS 8917, Groningen University, 1989. To appear in Theoretical Computer Science.
W.H. Hesselink [1990]: Command algebras, recursion and program transformation.
Formal Aspects Comput. 2 (1990) 60-104.
C.A.R. Hoare [1969]: An axiomatic basis for computer programming. Comm. ACM
12 (1969) 576-583.
C.A.R. Hoare [1971]: Procedures and parameters: an axiomatic approach. In: E. En-
geler (ed.): Symposium on Semantics of Algorithmic Languages, Lecture Notes in
Mathematics 188 (Springer V, 1971) pp. 102-116.

REFERENCES 
219
C.A.R. Hoare [1989]: Notes on an approach to category theory for computer sci-
entists. In: M. Broy (ed.) Constructive Methods in Computing Science NATO ASI
Series F 55 (Springer V, 1989) pp. 245-305.
K. Jensen, N. Wirth [1985]: Pascal User Manual and Report, third edition. Springer
V. 1985.
R.M. Karp [1959]: Some applications of logical syntax to digital computer program-
ming. Thesis, Harvard University, 1959.
D. Lehmann, A. Pnueli, J. Stavi [1981]: Impartiality, justice and fairness: the ethics
of concurrent termination. In: Proc. Internat. Conf. on Automata, Languages and
Programming, Lecture Notes in Computer Science 115 (Springer V, 1981) pp. 264-
277.
J.J. Lukkien [1991]: Parallel program design and generalized weakest preconditions.
Thesis Groningen 1991.
Z. Manna [1974]: Mathematical Theory of Computation (McGraw-Hill 1974).
Z. Manna, A. Pnueli [1974]: Axiomatic approach to total correctness of programs.
Acta Informatica 3 (1974) 253-262.
Z. Manna, J. Vuillemin [1972]: Fixpoint approach to the theory of computation.
In: M. Nivat (ed.): Automata, Languages and Programming 1972. (North Holland,
1973) pp. 273-292.
A.J. Martin [1983]: A general proof rule for procedures in predicate transformer
semantics. Acta Informatica 20 (1983) 301-313.
J. McCarthy [1980]: Circumscription - a form of non-monotonic reasoning. Artif.
Intell. 13 (1980) 27-39.
C. Morgan [1990]: Programming from Specifications. (Prentice Hall, 1990)
C. Morgan, P.H.B. Gardiner [1990]: Data refinement by calculation. Acta Informat-
ica 27 (1990) 481-503.
J.M. Morris [1987]: A theoretical basis for stepwise refinement and the programming
calculus. Sci. Comp. Program. 9 (1987) 287-306.
J.M. Morris [1990]: Temporal predicate transformers and fair termination. Acta
Informatica 27 (1990) 287-313.
G. Nelson [1989]: A generalization of Dijkstra's calculus. ACM Trans. Program.
Languages and Systems, 11 (1989) 517-561.

220 
REFERENCES
D. Park [1979]: On the semantics of fair parallelism. In: D. BJ0rner (ed.): Ab-
stract Software Specifications, Lecture Notes in Computer Science 86, Proceedings,
Copenhagen 1979 (Springer V, 1980) pp. 504-526.
W.P. de Roever [1976]: Dijkstra's predicate transformer, non-determinism, recur-
sion, and termination. In: Mathematical Foundations of Computer Science 1976,
Lecture Notes in Computer Science 45 (Springer V, 1976) pp. 472-481.
J.P. Queille, J. Sifakis [1983]: Fairness and related properties in transition systems
- a temporal logic to deal with fairness. Acta Informatica 19 (1983) 195-220.
A. Tarski [1955]: A lattice theoretical fixpoint theorem and its applications. Pacific
J. Math. 5 (1955) 285-309.
A.M. Turing [1949]: On checking a large routine. In: Report of a Conference on
High-Speed Automatic Calculating Machines. University Mathematical Labora-
tory, Cambridge, 1949, pp. 67-69.
J. von Wright [1990]: A lattice-theoretical basis for program refinement. Thesis,
Abo Akademi, Turku, Finland, 1990

INDEX OF CONCEPTS AND IDENTIFIERS
Entries are indexed by section numbers. In most cases we only provide the defining
occurrences.
A, A*, AÂ® 4.3
abort 1.3
abort 11.0
accumulation rule 11.3, 12.2
adapted pair 8.1
adjoint 5.5
admissible preorder 12.1
Alw 14.3
alw 14.1
annotation 2.2
array modification 1.7, 1.8
assertion 1.3
assignment 1.7
aterm 14.5
axiomatic semantics 0.1
IB 1.1
conditional combination 1.5
conditional correctness 2.1
configuration (graph) 9.3, 9.4
congruence 11.1
conjunctivity 3.1
covering rule 2.1, 2.2
De 4.4
d6A
declaration 2.3, 2.5, 4.4
determinate commands 7.1, 7.3
deterministic choice 1.8
diagonalization 8.4
disjunctivity 7.2, 7.4, 8.2
distributivity 3.3, 4.7
dynamic logic 0.1
body 2.3, 4.4
braces 0.6, 2.1
call of procedure 2.3
chain 7.2, 13.1
choice 1.4, 4.7
command 0.1, 1.2, 4.3
comment 0.6
commutation 5.4, 12.4
complete lattice 4.1
composition 0.6, 1.4, 4.7
compositionality 0.3, 11.6
computational induction 11.0
e 4.3
equalizer, Eq 11.1
Evt 14.4
evt 14.4
external variable 2.3
fair 15.3
fairness 15.0
false 1.1
false 1.1
family 0.6
fc 15.2
Fin 8.4

222
INDEX OF CONCEPTS AND IDENTIFIERS
finite nondeterminacy 7.5, 8.4
fixpoint 4.2
tier 15.2
gen 12.4
globalized interpretation 1.6
guard 1.3
# 4 . 4
healthiness law 3.2
Hoare logic 0.1
Hoare triple 1.8
homomorphism 2.6, 4.8
if 1.5
if then else fl 1.8
induced order 4.1
induction rule 0.4, 2.7, 4.9, 11.3
inf 4.1
inf-closed 4.1
inf-decked 13.1
inf-safe 13.2
infimum 4.1
invariant subset 4.2
invocation of procedure 2.3
k 9.1
Knaster-Tarski 4.2, 13.1
lattice 4.1
Leq 12.1
Lia 11.2, 13.8
linear approximation 13.8
localized interpretation 1.6
M 9.3, 9.4
MC 4.1
Md 8.3
Mdi 8.4
miracle 1.3
monotone 1.1
MP 4.1
MT 4.1
Mto 8.4
MU 4.1
Muc 8.4
IN 0.6
necessity rule 2.7, 5.7, 15.3
nondeterminacy 0.1
nondeterminate choice 0.1, 1.4
order 1.1
P 1.1
parameter 1.8, 2.3
partial correctness 2.1
postcondition 1.2, 5.5
predicate 0.1, 1.1
predicate transformer 1.1
preorder 12.1
procedure 2.3, 4.4
program schemas 0.2
program transformation 0.1, 0.3, 10.0,
11.0
program variable 0.1, 1.6
proof format 0.6
PT4.1
Recursion Theorem 2.5
recursive procedure 2.4, 4.4
refinement 5.1
relational semantics 0.1, 6.1
repetition 2.8
5 2.6, 4.4
S 0 2.6, 4.7
sap 13.3
Sat 8.2
semantic equality 0.2, 1.2
semantics 0.1
sequential composition 1.4, 4.7
set notation 0.6
simple command 2.6, 4.4
skip 1.3, 9.3

INDEX OF CONCEPTS AND IDENTIFIERS
223
sp 5.5
specification 0.1, 2.1
specification constant 2.1
stable 14.1
stable under unfolding 13.2
stack 9.1
state 0.1, 1.1
state function 1.6
state space 1.1, 1.6
strong 15.6
strong congruence 0.3, 11.3, 12.3
strong fairness 15.1, 15.6
strong preorder 12.2, 13.3
strongest postcondition 5.5
subalgebra 12.4
subset 0.6
substitution of procedures 10.1
substitution rule 1.7
sup 4.1
sup-closed 4.1
sup-decked 13.1
sup-safe 13.2
supremum 4.1
Sx8.1
Sy 8.1
syntactic reflection 8.1
syntactic totality 14.5
term 14.2
termination 1.3, 14.2
termination law 3.2, 4.6
total commands 1.3, 3.3, 8.4
total correctness 2.1
true 1.1
true 1.1
unf 14.2
unfolding 13.2, 14.2
upper continuity 7.2
variant function 2.4
vf 2.4
wae 4.4
wbe 4.4
weak 15.2
weak fairness 15.1
weakest (liberal) precondition 1.2
well-founded 5.6, 15.3
WGe 13.2
Wge 13.2
wg 1.4
while do od 2.8
wl 14.1
WLP 2.6
wlp 1.2, 4.4
WP2.6
Wp8.1
wp 1.2, 4.4
wse 4.4
WT4.6
X 1.1
TL 0.6
Zorn's Lemma 13.1

