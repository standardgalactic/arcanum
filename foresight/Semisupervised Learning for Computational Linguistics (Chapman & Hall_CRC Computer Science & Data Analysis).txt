
Computer Science and Data Analysis Series
Semisupervised
Learning for
Computational
Linguistics

Chapman & Hall/CRC
Computer Science and Data Analysis Series
The interface between the computer and statistical sciences is increasing,
as each discipline seeks to harness the power and resources of the other.
This series aims to foster the integration between the computer sciences
and statistical, numerical, and probabilistic methods by publishing a broad
range of reference works, textbooks, and handbooks.
SERIES EDITORS
David Madigan, Rutgers University
Fionn Murtagh, Royal Holloway, University of London
Padhraic Smyth, University of California, Irvine
Proposals for the series should be sent directly to one of the series editors
above, or submitted to:
Chapman & Hall/CRC
23-25 Blades Court
London SW15 2NU
UK
Published Titles
Bayesian Artificial Intelligence
Kevin B. Korb and Ann E. Nicholson
Pattern Recognition Algorithms for Data Mining
Sankar K. Pal and Pabitra Mitra
Exploratory Data Analysis with MATLAB®
Wendy L. Martinez and Angel R. Martinez
Clustering for Data Mining: A Data Recovery Approach
Boris Mirkin
Correspondence Analysis and Data Coding with Java and R
Fionn Murtagh
R Graphics
Paul Murrell
Design and Modeling for Computer Experiments
Kai-Tai Fang, Runze Li, and Agus Sudjianto
Semisupervised Learning for Computational Linguistics
Steven Abney

Computer Science and Data Analysis Series
Semisupervised
Learning for
Computational
Linguistics
Steven Abney
University of Michigan
Ann Arbor, U.S.A.
Boca Raton   London   New York
Chapman & Hall/CRC is an imprint of the
Taylor & Francis Group, an informa business

Chapman & Hall/CRC
Taylor & Francis Group
6000 Broken Sound Parkway NW, Suite 300
Boca Raton, FL 33487‑2742
© 2008 by Taylor & Francis Group, LLC 
Chapman & Hall/CRC is an imprint of Taylor & Francis Group, an Informa business
No claim to original U.S. Government works
Printed in the United States of America on acid‑free paper
10 9 8 7 6 5 4 3 2 1
International Standard Book Number‑13: 978‑1‑58488‑559‑7 (Hardcover)
This book contains information obtained from authentic and highly regarded sources. Reprinted 
material is quoted with permission, and sources are indicated. A wide variety of references are 
listed. Reasonable efforts have been made to publish reliable data and information, but the author 
and the publisher cannot assume responsibility for the validity of all materials or for the conse‑
quences of their use. 
No part of this book may be reprinted, reproduced, transmitted, or utilized in any form by any 
electronic, mechanical, or other means, now known or hereafter invented, including photocopying, 
microfilming, and recording, or in any information storage or retrieval system, without written 
permission from the publishers.
For permission to photocopy or use material electronically from this work, please access www.
copyright.com (http://www.copyright.com/) or contact the Copyright Clearance Center, Inc. (CCC) 
222 Rosewood Drive, Danvers, MA 01923, 978‑750‑8400. CCC is a not‑for‑profit organization that 
provides licenses and registration for a variety of users. For organizations that have been granted a 
photocopy license by the CCC, a separate system of payment has been arranged.
Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and 
are used only for identification and explanation without intent to infringe.
Library of Congress Cataloging‑in‑Publication Data
Abney, Steven P.
Semisupervised learning in computational linguistics / Steven Abney.
p. cm. ‑‑ (Computer science and data analysis series)
ISBN 978‑1‑58488‑559‑7 (alk. paper)
1. Computational linguistics‑‑Study and teaching (Higher) I. Title. II. Series.
P98.3.A26 2007
410.285‑‑dc22
2007022858
Visit the Taylor & Francis Web site at
http://www.taylorandfrancis.com
and the CRC Press Web site at
http://www.crcpress.com

Steven Abney
Semisupervised Learning
for Computational
Linguistics
CRC PRESS
Boca Raton
Ann Arbor
London
Tokyo

Preamble
The primary audience for this book is students, researchers, and developers
in computational linguistics who are interested in applying or advancing our
understanding of semisupervised learning methods for natural language pro-
cessing. The problem of semisupervised learning arose almost immediately
when computational linguists began exploring statistical and machine learn-
ing methods seriously in the late 1980s and early 1990s. In fact, language
applications – particularly text classiﬁcation and information extraction –
have provided a major impetus for interest in semisupervised learning in the
machine learning community.
Statistical methods that combine labeled and unlabeled data go back at
least to the 1960s, and theoretical understanding has advanced quickly over
the last few years; but the rate of advancements has made it diﬃcult for
non-specialists to keep abreast of them. Those computational linguists whose
interest in semisupervised learning is more practical and empirical would ben-
eﬁt from an accessible presentation of the state of the theory. For students,
the need for an accessible presentation is urgent.
The purpose of the book is to provide students and researchers a broad
and accessible presentation of what is currently known about semisupervised
learning, including both the theory and linguistic applications.
The back-
ground assumed is what can be reasonably expected of any graduate student
(or even an advanced undergraduate) who has taken introductory courses in
natural language processing that include statistical methods – concretely, the
material contained in Jurafsky & Martin [119] and Manning & Sch¨utze [141].
It is desirable that the book be self-contained. Consequently, its coverage
will overlap somewhat with standard texts in machine learning. This is un-
avoidable, given that the target audience is not assumed to have background
in machine learning beyond what is contained in the texts just mentioned,
and given that semisupervised learning cannot be seriously tackled without
understanding the methods for supervised and unsupervised learning that it
builds on. My approach has been to treat only those topics in supervised
and unsupervised learning that are necessary for understanding semisuper-
vised methods, and to aim for intuitive understanding rather than rigor and
completeness – again, except insomuch as a rigorous treatment is required for
understanding the main topic, the semisupervised case. In short, the book
does cover a number of topics that are found in general introductions to ma-
chine learning; but if viewed as a general introduction, it will seem eclectic
in coverage and intuitive in treatment. I do not see this necessarily as a ﬂaw.
v

vi
I ﬁnd that my own interests often run beyond the areas where I have solid
foundations, and an intuitive overview gives me motivation to go back and
ﬁll in those foundations. I hope that students of computational linguistics
who come with an interest in semisupervised problems but without a general
training in machine learning will, above all, ﬁnd the main topic accessible,
but will also acquire a framework and motivation for more systematic study
of machine learning.
Although the book is written with computational linguists in mind, I hope
that it will also be of interest to students of machine learning. Simple text
classiﬁcation tasks are now familiar in the machine learning literature, but
fewer machine learning researchers are aware of the variety of other linguistic
applications. Moreover, linguistic applications have characteristic properties
that diﬀer in interesting ways from applications that have been the traditional
focus in machine learning, and can suggest new questions for theoretical study.
For example, natural language problems often have attributes with large sets
of discrete values with highly skewed distributions (that is, word-valued at-
tributes), or large sparse spaces of real-valued attributes (numeric attributes
indexed by words), or learning targets that are neither discrete classes nor
real values, but rather structures (text spans or parse trees).
Perhaps a few readers from even further aﬁeld will ﬁnd the book useful.
I have beneﬁted from a book on clustering written for chemists [145], and I
would be pleased if researchers from areas well outside of natural language
processing ﬁnd something useful here. Semisupervised learning is a topic of
broad applicability.
It has already been applied to image processing [98],
bioinformatics, and security assessment [177], to name a few examples. Fur-
ther applications are limited only by imagination.
Acknowledgments
First and foremost, I would like to thank Mark Abney for producing the cover
and most of the illustrations. He has done a terriﬁc job; without him the book
would have taken much longer and been a good deal less attractive.
I would also like to thank my students and colleagues for reading an earlier
draft and giving me indispensable feedback, especially G¨une¸s Erkan, Jessica
Hullman, Kevin McGowan, Terrence Szymanski, Richmond Thomason, Li
Yang, and an anonymous reviewer.
Finally, I would like to thank my family, Marie Carmen, Anneliese, and
Nina, for their patience and support while this book has been in the making.

Contents
1
Introduction
1
1.1
A brief history
. . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1.1
Probabilistic methods in computational linguistics
. .
1
1.1.2
Supervised and unsupervised training
. . . . . . . . .
2
1.1.3
Semisupervised learning . . . . . . . . . . . . . . . . .
3
1.2
Semisupervised learning
. . . . . . . . . . . . . . . . . . . . .
4
1.2.1
Major varieties of learning problem . . . . . . . . . . .
4
1.2.2
Motivation
. . . . . . . . . . . . . . . . . . . . . . . .
6
1.2.3
Evaluation . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.2.4
Active learning . . . . . . . . . . . . . . . . . . . . . .
8
1.3
Organization and assumptions . . . . . . . . . . . . . . . . . .
8
1.3.1
Leading ideas . . . . . . . . . . . . . . . . . . . . . . .
8
1.3.2
Mathematical background . . . . . . . . . . . . . . . .
10
1.3.3
Notation . . . . . . . . . . . . . . . . . . . . . . . . . .
11
2
Self-training and Co-training
13
2.1
Classiﬁcation
. . . . . . . . . . . . . . . . . . . . . . . . . . .
13
2.1.1
The standard setting . . . . . . . . . . . . . . . . . . .
13
2.1.2
Features and rules
. . . . . . . . . . . . . . . . . . . .
14
2.1.3
Decision lists
. . . . . . . . . . . . . . . . . . . . . . .
16
2.2
Self-training . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
2.2.1
The algorithm . . . . . . . . . . . . . . . . . . . . . . .
19
2.2.2
Parameters and variants . . . . . . . . . . . . . . . . .
20
2.2.3
Evaluation . . . . . . . . . . . . . . . . . . . . . . . . .
23
2.2.4
Symmetry of features and instances
. . . . . . . . . .
25
2.2.5
Related algorithms . . . . . . . . . . . . . . . . . . . .
27
2.3
Co-Training
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
3
Applications of Self-Training and Co-Training
31
3.1
Part-of-speech tagging
. . . . . . . . . . . . . . . . . . . . . .
31
3.2
Information extraction
. . . . . . . . . . . . . . . . . . . . . .
33
3.3
Parsing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
3.4
Word senses
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
3.4.1
WordNet
. . . . . . . . . . . . . . . . . . . . . . . . .
36
3.4.2
Word-sense disambiguation
. . . . . . . . . . . . . . .
38
3.4.3
Taxonomic inference . . . . . . . . . . . . . . . . . . .
40
vii

viii
4
Classiﬁcation
43
4.1
Two simple classiﬁers . . . . . . . . . . . . . . . . . . . . . . .
43
4.1.1
Naive Bayes . . . . . . . . . . . . . . . . . . . . . . . .
43
4.1.2
k-nearest-neighbor classiﬁer . . . . . . . . . . . . . . .
45
4.2
Abstract setting . . . . . . . . . . . . . . . . . . . . . . . . . .
48
4.2.1
Function approximation . . . . . . . . . . . . . . . . .
48
4.2.2
Deﬁning success . . . . . . . . . . . . . . . . . . . . . .
50
4.2.3
Fit and simplicity . . . . . . . . . . . . . . . . . . . . .
52
4.3
Evaluating detectors and classiﬁers that abstain . . . . . . . .
53
4.3.1
Conﬁdence-rated classiﬁers
. . . . . . . . . . . . . . .
53
4.3.2
Measures for detection . . . . . . . . . . . . . . . . . .
54
4.3.3
Idealized performance curves
. . . . . . . . . . . . . .
57
4.3.4
The multiclass case . . . . . . . . . . . . . . . . . . . .
59
4.4
Binary classiﬁers and ECOC . . . . . . . . . . . . . . . . . . .
62
5
Mathematics for Boundary-Oriented Methods
67
5.1
Linear separators
. . . . . . . . . . . . . . . . . . . . . . . . .
67
5.1.1
Representing a hyperplane . . . . . . . . . . . . . . . .
67
5.1.2
Eliminating the threshold . . . . . . . . . . . . . . . .
69
5.1.3
The point-normal form . . . . . . . . . . . . . . . . . .
70
5.1.4
Naive Bayes decision boundary . . . . . . . . . . . . .
72
5.2
The gradient
. . . . . . . . . . . . . . . . . . . . . . . . . . .
74
5.2.1
Graphs and domains . . . . . . . . . . . . . . . . . . .
74
5.2.2
Convexity . . . . . . . . . . . . . . . . . . . . . . . . .
76
5.2.3
Diﬀerentiation of vector and matrix expressions . . . .
79
5.2.4
An example: linear regression . . . . . . . . . . . . . .
81
5.3
Constrained optimization
. . . . . . . . . . . . . . . . . . . .
83
5.3.1
Optimization . . . . . . . . . . . . . . . . . . . . . . .
83
5.3.2
Equality constraints
. . . . . . . . . . . . . . . . . . .
84
5.3.3
Inequality constraints
. . . . . . . . . . . . . . . . . .
87
5.3.4
The Wolfe dual . . . . . . . . . . . . . . . . . . . . . .
91
6
Boundary-Oriented Methods
95
6.1
The perceptron
. . . . . . . . . . . . . . . . . . . . . . . . . .
97
6.1.1
The algorithm . . . . . . . . . . . . . . . . . . . . . . .
97
6.1.2
An example . . . . . . . . . . . . . . . . . . . . . . . .
99
6.1.3
Convergence . . . . . . . . . . . . . . . . . . . . . . . . 100
6.1.4
The perceptron algorithm as gradient descent . . . . . 101
6.2
Game self-teaching
. . . . . . . . . . . . . . . . . . . . . . . . 103
6.3
Boosting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
6.3.1
Abstention
. . . . . . . . . . . . . . . . . . . . . . . . 110
6.3.2
Semisupervised boosting . . . . . . . . . . . . . . . . . 111
6.3.3
Co-boosting . . . . . . . . . . . . . . . . . . . . . . . . 113
6.4
Support Vector Machines (SVMs) . . . . . . . . . . . . . . . . 114
6.4.1
The margin . . . . . . . . . . . . . . . . . . . . . . . . 114

ix
6.4.2
Maximizing the margin
. . . . . . . . . . . . . . . . . 116
6.4.3
The nonseparable case . . . . . . . . . . . . . . . . . . 119
6.4.4
Slack in the separable case . . . . . . . . . . . . . . . . 121
6.4.5
Multiple slack points . . . . . . . . . . . . . . . . . . . 123
6.4.6
Transductive SVMs . . . . . . . . . . . . . . . . . . . . 125
6.4.7
Training a transductive SVM . . . . . . . . . . . . . . 127
6.5
Null-category noise model
. . . . . . . . . . . . . . . . . . . . 129
7
Clustering
131
7.1
Cluster and label
. . . . . . . . . . . . . . . . . . . . . . . . . 131
7.2
Clustering concepts . . . . . . . . . . . . . . . . . . . . . . . . 132
7.2.1
Objective . . . . . . . . . . . . . . . . . . . . . . . . . 132
7.2.2
Distance and similarity . . . . . . . . . . . . . . . . . . 133
7.2.3
Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
7.3
Hierarchical clustering
. . . . . . . . . . . . . . . . . . . . . . 137
7.4
Self-training revisited . . . . . . . . . . . . . . . . . . . . . . . 139
7.4.1
k-means clustering . . . . . . . . . . . . . . . . . . . . 139
7.4.2
Pseudo relevance feedback . . . . . . . . . . . . . . . . 140
7.5
Graph mincut . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
7.6
Label propagation
. . . . . . . . . . . . . . . . . . . . . . . . 146
7.6.1
Clustering by propagation . . . . . . . . . . . . . . . . 146
7.6.2
Self-training as propagation . . . . . . . . . . . . . . . 147
7.6.3
Co-training as propagation
. . . . . . . . . . . . . . . 150
7.7
Bibliographic notes
. . . . . . . . . . . . . . . . . . . . . . . . 152
8
Generative Models
153
8.1
Gaussian mixtures
. . . . . . . . . . . . . . . . . . . . . . . . 153
8.1.1
Deﬁnition and geometric interpretation
. . . . . . . . 153
8.1.2
The linear discriminant decision boundary . . . . . . . 156
8.1.3
Decision-directed approximation
. . . . . . . . . . . . 159
8.1.4
McLachlan’s algorithm . . . . . . . . . . . . . . . . . . 162
8.2
The EM algorithm
. . . . . . . . . . . . . . . . . . . . . . . . 163
8.2.1
Maximizing likelihood . . . . . . . . . . . . . . . . . . 163
8.2.2
Relative frequency estimation . . . . . . . . . . . . . . 164
8.2.3
Divergence
. . . . . . . . . . . . . . . . . . . . . . . . 166
8.2.4
The EM algorithm . . . . . . . . . . . . . . . . . . . . 169
9
Agreement Constraints
175
9.1
Co-training
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
9.1.1
The conditional independence assumption . . . . . . . 176
9.1.2
The power of conditional independence . . . . . . . . . 178
9.2
Agreement-based self-teaching . . . . . . . . . . . . . . . . . . 182
9.3
Random ﬁelds . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
9.3.1
Applied to self-training and co-training . . . . . . . . . 184
9.3.2
Gibbs sampling . . . . . . . . . . . . . . . . . . . . . . 186

x
9.3.3
Markov chains and random walks . . . . . . . . . . . . 187
9.4
Bibliographic notes
. . . . . . . . . . . . . . . . . . . . . . . . 192
10 Propagation Methods
193
10.1
Label propagation
. . . . . . . . . . . . . . . . . . . . . . . . 194
10.2
Random walks
. . . . . . . . . . . . . . . . . . . . . . . . . . 196
10.3
Harmonic functions . . . . . . . . . . . . . . . . . . . . . . . . 198
10.4
Fluids
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
10.4.1
Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
10.4.2
Pressure . . . . . . . . . . . . . . . . . . . . . . . . . . 205
10.4.3
Conservation of energy . . . . . . . . . . . . . . . . . . 209
10.4.4
Thomson’s principle
. . . . . . . . . . . . . . . . . . . 210
10.5
Computing the solution
. . . . . . . . . . . . . . . . . . . . . 213
10.6
Graph mincuts revisited
. . . . . . . . . . . . . . . . . . . . . 215
10.7
Bibliographic notes
. . . . . . . . . . . . . . . . . . . . . . . . 220
11 Mathematics for Spectral Methods
221
11.1
Some basic concepts
. . . . . . . . . . . . . . . . . . . . . . . 221
11.1.1
The norm of a vector . . . . . . . . . . . . . . . . . . . 221
11.1.2
Matrices as linear operators . . . . . . . . . . . . . . . 222
11.1.3
The column space
. . . . . . . . . . . . . . . . . . . . 222
11.2
Eigenvalues and eigenvectors
. . . . . . . . . . . . . . . . . . 224
11.2.1
Deﬁnition of eigenvalues and eigenvectors
. . . . . . . 224
11.2.2
Diagonalization . . . . . . . . . . . . . . . . . . . . . . 225
11.2.3
Orthogonal diagonalization
. . . . . . . . . . . . . . . 226
11.3
Eigenvalues and the scaling eﬀects of a matrix . . . . . . . . . 227
11.3.1
Matrix norms . . . . . . . . . . . . . . . . . . . . . . . 227
11.3.2
The Rayleigh quotient . . . . . . . . . . . . . . . . . . 228
11.3.3
The 2 × 2 case
. . . . . . . . . . . . . . . . . . . . . . 230
11.3.4
The general case
. . . . . . . . . . . . . . . . . . . . . 232
11.3.5
The Courant-Fischer minimax theorem . . . . . . . . . 234
11.4
Bibliographic notes
. . . . . . . . . . . . . . . . . . . . . . . . 236
12 Spectral Methods
237
12.1
Simple harmonic motion
. . . . . . . . . . . . . . . . . . . . . 237
12.1.1
Harmonics . . . . . . . . . . . . . . . . . . . . . . . . . 237
12.1.2
Mixtures of harmonics . . . . . . . . . . . . . . . . . . 239
12.1.3
An oscillating particle . . . . . . . . . . . . . . . . . . 241
12.1.4
A vibrating string
. . . . . . . . . . . . . . . . . . . . 243
12.2
Spectra of matrices and graphs
. . . . . . . . . . . . . . . . . 251
12.2.1
The spectrum of a matrix . . . . . . . . . . . . . . . . 252
12.2.2
Relating matrices and graphs . . . . . . . . . . . . . . 253
12.2.3
The Laplacian matrix and graph spectrum . . . . . . . 256
12.3
Spectral clustering
. . . . . . . . . . . . . . . . . . . . . . . . 257
12.3.1
The second smallest eigenvector of the Laplacian . . . 257

xi
12.3.2
The cut size and the Laplacian . . . . . . . . . . . . . 259
12.3.3
Approximating cut size
. . . . . . . . . . . . . . . . . 260
12.3.4
Minimizing cut size . . . . . . . . . . . . . . . . . . . . 262
12.3.5
Ratiocut . . . . . . . . . . . . . . . . . . . . . . . . . . 263
12.4
Spectral methods for semisupervised learning
. . . . . . . . . 265
12.4.1
Harmonics and harmonic functions . . . . . . . . . . . 265
12.4.2
Eigenvalues and energy
. . . . . . . . . . . . . . . . . 267
12.4.3
The Laplacian and random ﬁelds . . . . . . . . . . . . 268
12.4.4
Harmonic functions and the Laplacian . . . . . . . . . 270
12.4.5
Using the Laplacian for regularization . . . . . . . . . 272
12.4.6
Transduction to induction . . . . . . . . . . . . . . . . 274
12.5
Bibliographic notes
. . . . . . . . . . . . . . . . . . . . . . . . 275
Bibliography
277
Index
301


1
Introduction
1.1
A brief history
1.1.1
Probabilistic methods in computational linguistics
Computational linguistics seeks to describe methods for natural language pro-
cessing, that is, for processing human languages by automatic means. Since
the advent of electronic computers in the late 1940s, human language pro-
cessing has been an area of active research; machine translation in particular
attracted early interest. Indeed, the inspiration for computing machines was
the creation of a thinking automaton, a machina sapiens, and language is
perhaps the most distinctively human cognitive capacity.
In early work on artiﬁcial intelligence, there was something of a competi-
tion between discrete, “symbolic” reasoning and stochastic systems, partic-
ularly neural nets. But the indispensability of a ﬁrm probabilistic basis for
dealing with uncertainty was soon recognized. In computational linguistics,
by contrast, the presumption of the suﬃciency of grammatical and logical
constraints, supplemented perhaps by ad hoc heuristics, was much more tena-
cious.
When the ﬁeld recognized the need for probabilistic methods, the shift was
sudden and dramatic. It is probably fair to identify the birth of awareness with
the appearance in 1988 of two papers on statistical part-of-speech tagging,
one by Church [44] and one by DeRose [75]. These were not the ﬁrst papers
that proposed stochastic methods for part of speech disambiguation, but they
were the ﬁrst in prominent venues in computational linguistics, and it is no
exaggeration to say that the ﬁeld was reshaped within a decade.
The main barrier to progress in natural language processing at the time
was the brittleness of manually constructed systems. The dominant issues
were encapsulated under the rubrics of ambiguity resolution, portability, and
robustness.
The primary method for ambiguity resolution was the use of
semantic constraints, but they were often either too loose, leaving a large
number of viable analyses, or else too strict, ruling out the correct analysis.
Well-founded and automatic means for softening constraints and resolving
ambiguities were needed. Portability meant in particular automatic means for
adapting to variability across application domains. Robustness covers both
the fact that input to natural language systems is frequently errorful, and
1

2
Semisupervised Learning for Computational Linguistics
also the fact that, in Sapir’s terms, “all grammars leak” [201]. No manually
constructed description of language is complete.
Together, these issues point to the need for automatic learning methods,
and explain why the penetration of probabilistic methods, and machine learn-
ing in particular, was so rapid. Computational linguistics has now become
inseparable from machine learning.
1.1.2
Supervised and unsupervised training
The probabilistic models used by Church and DeRose in the papers just cited
were Hidden Markov Models (HMMs), imported from the speech recognition
community. An HMM describes a probabilistic process or automaton that
generates sequences of states and parallel sequences of output symbols. Com-
monly, a sequence of output symbols represents a sentence of English or of
some other natural language. An HMM, or any model, that deﬁnes probabil-
ities of word sequences (that is, sentences) of a natural language is known as
a language model.
The probabilistic automaton deﬁned by an HMM may be in some number
of distinct states.
The automaton begins by choosing a state at random.
Then it chooses a symbol to emit, the choice being sensitive to the state.
Next it chooses a new state, emits a symbol from that state, and the process
repeats.
Each choice is stochastic – that is, probabilistic.
At each step,
the automaton makes its choice at random from a distribution over output
symbols or next states, as the case may be. Which distribution it uses at
any point is completely determined by the kind of choice, either emission
of an output symbol or transition to a new state, and the identity of the
current state. The actual model consists in a collection of numeric values,
one for each possible transition or emission, representing the probability that
the automaton chooses that particular transition or emission when making
one of its stochastic choices.
Learning an HMM is straightforward if one is provided with labeled data,
meaning state sequences paired with output sequences. Each sequence pair
is a record of the stochastic choices made by the automaton. To estimate
the probability that the automaton will choose a particular value x when
faced with a stochastic choice of type T, one can simply count how often the
automaton actually chose x when making a choice of type T in the record of
previous computations, that is, in the labeled data. If suﬃcient labeled data
is available, the model can be estimated accurately in this way.
Church and DeRose applied HMMs to the problem of part-of-speech tag-
ging by identifying the states of the automaton with parts of speech. The
automaton generates a sequence of parts of speech, and emits a word for each
part of speech. The result is a tagged text, which is a text in which each
word is annotated with its part of speech.
Supervised learning of an HMM for part-of-speech tagging is quite eﬀective;
HMM taggers for English generally have an error rate of 3.5 to 4 percent.

Introduction
3
Their eﬀectiveness was what brought probabilistic models to the attention of
computational linguists, as already mentioned.
1.1.3
Semisupervised learning
Creating suﬃcient labeled data can be very time-consuming. Obtaining the
output sequences is not diﬃcult: English texts are available in great quantity.
What is time-consuming is creating the state sequences. One must essentially
annotate each word of the texts with the state of the HMM from which it was
emitted. For this reason, one would like to have a method for learning a model
from unlabeled data, which, in this case, consists of simple English texts
without state annotations. A learning method that uses unlabeled data is
known as an unsupervised learning method, in contrast to supervised learning
methods, which use labeled data.
An unsupervised learning method for HMMs has long been known, called
the forward-backward algorithm.
It is a special case of the Expectation-
Maximization (EM) algorithm. It is widely used and eﬀective in speech recog-
nition. For example, it is used to estimate acoustic models from unlabeled
data. For an acoustic model, unlabeled data consists of text paired with the
speech signal resulting from reading the text, but without any annotation re-
garding the sequence of states that the model passes through while generating
the speech from the text.
However, unsupervised learning turned out not to be very eﬀective for
part-of-speech tagging. The forward-backward algorithm is an iterative al-
gorithm that begins with some initial model and improves it by repeated
passes through the unlabeled data. The question of the eﬀectiveness of un-
supervised learning was posed by Elworthy [83] in the following form. If one
uses labeled data to obtain the initial model for forward-backward training
on unlabeled data, how many iterations of forward-backward training should
one do for a given amount of labeled seed data? The answer was essentially
zero iterations if one had more than a tiny amount of labeled data. (Merialdo
[153] came to similar conclusions.)
Intuitively, the states that are learned
by an automaton trained on unlabeled data do not correspond at all well to
linguistically motivated parts of speech.
The solution that emerged in the case of part-of-speech tagging was to use
additional constraints. Speciﬁcally, if one uses unlabeled data and forward-
backward training, but restricts words to take on only the parts of speech
that are allowed for them by a dictionary, the results are comparable to using
labeled data [62].
But despite its ineﬀectiveness for part-of-speech tagging, the idea of learning
from a mixture of labeled and unlabeled data remained potent, and it has
come to constitute the canonical setting for semisupervised learning. (One
can view the dictionary constraints that proved more eﬀective for tagging as
providing partially labeled data, hence a variant of semisupervised learning.
This will be discussed in more detail later.)

4
Semisupervised Learning for Computational Linguistics
Subsequent work in computational linguistics led to development of alterna-
tive algorithms for semisupervised learning, the algorithm of Yarowsky [239]
being a prominent example. These algorithms were developed speciﬁcally for
the sorts of problems that arise frequently in computational linguistics: prob-
lems in which there is a linguistically correct answer, and large amounts of
unlabeled data, but very little labeled data. Unlike in the example of acoustic
modeling, classic unsupervised learning is inappropriate, because not just any
way of assigning classes will do. The learning method is largely unsupervised,
because most of the data is unlabeled, but the labeled data is indispens-
able, because it provides the only characterization of the linguistically correct
classes.
The algorithms just mentioned turn out to be very similar to an older learn-
ing method known as self-training that was unknown in computational lin-
guistics at the time. For this reason, it is more accurate to say that they were
rediscovered, rather than invented, by computational linguists. Until very re-
cently, most prior work on semisupervised learning has been little known even
among researchers in the area of machine learning. One goal of the present
volume is to make the prior and also the more recent work on semisupervised
learning more accessible to computational linguists.
Shortly after the rediscovery of self-training in computational linguistics, a
method called co-training was invented by Blum and Mitchell [21], machine-
learning researchers working on text classiﬁcation. Self-training and co-training
have become popular and widely employed in computational linguistics; to-
gether they account for all but a fraction of the work on semisupervised learn-
ing in the ﬁeld. We will discuss them in the next chapter. In the remainder of
this chapter, we give a broader perspective on semisupervised learning, and
lay out the plan of the rest of the book.
1.2
Semisupervised learning
1.2.1
Major varieties of learning problem
There are ﬁve types of learning problem that have received the preponder-
ance of attention in machine learning. The ﬁrst four are all cases of function
estimation, grouped along two dimensions: whether the learning task is su-
pervised or unsupervised, and whether the variable to be predicted is nominal
or real-valued.
Classiﬁcation involves supervised learning of a function f(x) whose value
is nominal, that is, drawn from a ﬁnite set of possible values. The learned
function is called a classiﬁer. It is given instances x of one or another class,
and it must determine which class each instance belongs to; the value f(x) is
the classiﬁer’s prediction regarding the class of the instance. For example, an

Introduction
5
instance might be a particular word in context, and the classiﬁcation task is
to determine its part of speech. The learner is given labeled data consisting
of a collection of instances along with the correct answer, that is, the correct
class label, for each instance.
The unsupervised counterpart to classiﬁcation is clustering. The goal in
clustering is also to assign instances to classes, but the clustering algorithm
is given only the instances, not the correct answers for any of them.
(In
clustering, the instances are usually called data points and the classes are
called clusters.) The primary diﬀerence between classiﬁcation and clustering
is not the task to be performed, but the sort of data that is given to the
learner as input; in particular, whether the data is labeled or not.
The remaining two function estimation tasks involve estimation of a func-
tion that takes on real values, instead of values from a ﬁnite range.
The
supervised version is called regression; it diﬀers from classiﬁcation only in
that the function to be learned takes on real values. Unsupervised learning
of a real-valued function can be viewed as density estimation. The learner is
given an unlabeled set of training data, consisting of a ﬁnite sample of data
points from a multi-dimensional space, and the goal is to learn a function f(x)
assigning a real value to every point in the space; the function is interpreted
as (proportional to) a probability density.
Finally, we mentioned a ﬁfth setting that does not fall under function esti-
mation. This ﬁfth setting is reinforcement learning. In reinforcement learn-
ing, the learner receives a stream of data from sensors, and its “answers”
consist in actions, in the form of commands sent to actuators. There is, addi-
tionally, a reward signal that is to be maximized (over the long run). There
are at least two signiﬁcant ways this diﬀers from the four function estima-
tion settings. First is the sequential nature of the inputs. Even if we assume
discrete time, there are temporal dependencies that cannot be ignored: in
particular, actions have time-delayed eﬀects on sensors and reward. Second is
the indirect nature of the supervision. The reward signal provides information
about the relative value of diﬀerent actions, but it is much less direct than
simply providing the correct answer, as in classiﬁcation.
Semisupervised learning generalizes supervised and unsupervised learning.
The generalization is easiest to see with classiﬁcation and clustering.
As
already mentioned, classiﬁcation and clustering involve essentially the same
task and the same inputs; they diﬀer primarily in whether the training data
is labeled or not. (They also diﬀer in the way they are evaluated, but the
diﬀerence in evaluation is a consequence of the diﬀerence in the kind of training
data – more on that later.) The obvious generalization is to give the learner
labels for some of the training data. At one extreme, all of the data is labeled,
and the task is classiﬁcation, and at the other extreme, none of the data is
labeled, and the task is clustering. The mixed labeled/unlabeled setting is
indeed the canonical case for semisupervised learning, and it will be our main
interest.
At the same time, a mix of labeled and unlabeled information is only one

6
Semisupervised Learning for Computational Linguistics
way of providing a learner with partial information about the labels for train-
ing data. Many semisupervised learning methods work with alternate kinds of
partial information, such as a handful of reliable rules for labeling instances,
or constraints limiting the candidate labels for particular instances. We will
also consider these extensions of the canonical setting. In principle, the kind of
indirect information about labels found in reinforcement learning qualify it as
a kind of semisupervised learning, but the indirect-information aspect of rein-
forcement learning is diﬃcult to disentangle from the temporal dependencies,
and the connection between reinforcement learning and other semisupervised
approaches remains obscure; it lies beyond the scope of the present work.
1.2.2
Motivation
For most learning tasks of interest, it is easy to obtain samples of unlabeled
data. For many language learning tasks, for example, the World Wide Web
can be seen as a large collection of unlabeled data. By contrast, in most cases,
the only practical way to obtain labeled data is to have subject-matter experts
manually annotate the data, an expensive and time-consuming process.
The great advantage of unsupervised learning, such as clustering, is that it
requires no labeled training data. The disadvantage has already been men-
tioned: under the best of circumstances, one might hope that the learner
would recover the correct clusters, but hardly that it could correctly label the
clusters. In many cases, even the correct clusters are too much to hope for.
To say it another way, unsupervised learning methods rarely perform well if
evaluated by the same yardstick used for supervised learners. If we expect a
clustering algorithm to predict the labels in a labeled test set, without the
advantage of labeled training data, we are sure to be disappointed.
The advantage of supervised learning algorithms is that they do well at the
harder task: predicting the true labels for test data. The disadvantage is that
they only do well if they are given enough labeled training data, but producing
suﬃcient quantities of labeled data can be very expensive in manual eﬀort.
The aim of semisupervised learning is to have our cake and eat it, too.
Semisupervised learners take as input unlabeled data and a limited source of
label information, and, if successful, achieve performance comparable to that
of supervised learners at signiﬁcantly reduced cost in manual production of
training data.
We intentionally used the vague phrase “a limited source of label informa-
tion.” One source of label information is obviously labeled data, but there
are alternatives. We will consider at least the following sources of label infor-
mation:
• labeled data
• a seed classiﬁer

Introduction
7
• limiting the possible labels for instances without determining a unique
label
• constraining pairs of instances to have the same, but unknown, label
(co-training)
• intrinsic label deﬁnitions
• a budget for labeling instances selected by the learner (active learning)
One of the grand aims of computational linguistics is unsupervised learning
of natural language. From a psychological perspective, it is widely accepted
that explicit instruction plays little part in human language learning, and
from a technological perspective, a completely autonomous system is more
useful than one that requires manual guidance. Yet, in contradiction to the
characterization sometimes given of the goal of unsupervised learning, the goal
of unsupervised language learning is not the recovery of arbitrary “interesting”
structure, but rather the acquisition of the correct target language. On the
face of it, learning a target classiﬁcation – much less an entire natural language
– without labeled data hardly seems possible.
Semisupervised learning may provide the beginning of an account. If a ker-
nel of labeled data can be acquired through unsupervised learning, semisuper-
vised learning might be used to extend it to a complete solution. Something
along these lines appears to characterize human language acquisition: in the
psycholinguistic literature, bootstrapping refers to the process by which an
initial kernel of language is acquired by explicit instruction, in the form, for
example, of naming an object while drawing a child’s attention to it. The
processes by which that kernel is extended to the entirety of the language
are thought to be diﬀerent; distributional regularities of linguistic forms,
rather than direct connections to the physical world, seem to play a large
role. Semisupervised learning methods provide possible characterizations of
the process of extending the initial kernel.
1.2.3
Evaluation
With regard to evaluation, semisupervised algorithms are like supervised al-
gorithms. The basic measure of success is classiﬁcation performance on an
unseen test set, used as an estimate of generalization error.
But in addition to measuring absolute performance, one would also like to
measure the beneﬁt obtained by the addition of unlabeled data. The most
general way to pose the question is the level of performance as a function
of human eﬀort.
More concretely, one considers prediction rule quality as
a function of the number of labeled instances and the number of unlabeled
instances. Two questions are of particular interest: (1) for a ﬁxed number of
labeled instances (i.e., a ﬁxed annotation budget), how much improvement is
obtainable as the number of unlabeled instances grows without bound; and

8
Semisupervised Learning for Computational Linguistics
(2) for a ﬁxed target level of performance, what is the minimum number of
labeled instances needed to achieve it, as the number of unlabeled instances
grows without bound.
1.2.4
Active learning
One way of characterizing the overarching goal of semisupervised learning is
to use unlabeled data to amplify the information gained from a manually
created seed. We focus almost exclusively on “batch” learning, in which the
seed and a population of unlabeled data are given in advance.
A natural
next question is whether a better eﬀort-performance curve can be obtained
in an interactive setting, for example, by selecting instances to be labeled,
and interleaving learning with labeling. Interactive semisupervised learning
is called active learning. It lies beyond the scope of the current work.
1.3
Organization and assumptions
1.3.1
Leading ideas
Semisupervised learning methods have sprung up independently in several
diﬀerent areas, usually as modiﬁcations of existing algorithms. For example,
if one’s interest is classiﬁcation, it is natural to ask how to modify a classiﬁer-
learning algorithm to make use of unlabeled data. Conversely, if one’s interest
is clustering, it is natural to ask how to make use of manually labeled examples,
either to assign names to otherwise anonymous clusters, or to constrain the
algorithm to produce clusters that are consistent with the manual labels.
We organize semisupervised algorithms by the leading idea that each is
based on. These are the leading ideas, in our view:
• Self-training (chapters 2–3 and 8). If one comes from the perspective
of supervised learning, and asks how unlabeled instances might be put to
use in addition to labeled instances, a natural idea is to train a classiﬁer
its predictions at face value, at least in those cases where its predictions
are most conﬁdent. A new classiﬁer is trained on the extended set of
labeled instances, and the process repeats. This approach is known as
self-training.
• Cluster and
Coming to semisupervised learning
from the perspective of unsupervised learning, a natural idea is to apply
a clustering algorithm to the unlabeled data (one can also strip the labels
from the labeled data and throw it in as well), and then use the labeled
data to “name” the clusters.
A cluster is associated with whichever
label (chapter 7).
on the labeled instances, apply it to the unlabeled instances, and take

Introduction
9
label occurs most frequently on labeled instances in the cluster, and the
prediction for an unlabeled instance is determined by the cluster that it
is assigned to.
• Application of “missing values” techniques (chapter 8). The prob-
lem of missing values in a data set is very familiar in statistics. It is
natural to think of unlabeled data as data with missing values for the
dependent variable (that is, the class label), and apply a method for
ﬁlling in missing information using a generative model. The canonical
example is the Expectation-Maximization (EM) algorithm. The earliest
literature on semisupervised learning falls under this rubric.
• Label propagation in graphs (chapter 10). Clustering algorithms are
typically based on a similarity metric; clusters are deﬁned to be groups of
similar instances. A postulate sometimes called the cluster hypothesis
is that similar instances have similar labels, or in geometric terms, that
proximate instances have similar labels. A similarity function can be
represented as a weighted graph, in which instances are nodes and edges
are weighted by the similarity of the instances they connect. Two nodes
connected by a heavily weighted edge should have the same label. An
algorithmic correlate is to propagate labels along heavily weighted edges.
Geometrically, one can view the graph as a fabric whose interior consists
of unlabeled instances and whose boundary consists of labeled instances.
The elevation of the fabric at a given point represents the label of that
point, and the eﬀect of propagation is to interpolate from the ﬁxed
boundary across the interior of the graph.
• Boundaries in low density regions (chapters 5–6). The contraposi-
tive of the cluster hypothesis is what we might call the separation hy-
pothesis: the idea that diﬀerent labels imply distant data points, which
is to say, that boundaries between classes lie where the data is sparse.
Transductive maximum-margin methods, such as transductive Support
Vector Machines (SVMs) and transductive boosting, can be understood
in those terms. The goal is to ﬁnd a linear inter-class boundary with a
large margin, which is to say, a large distance to the nearest data points.
Instead of looking for natural clusters, one looks for natural boundaries,
but otherwise the approach is very similar to cluster-and-label. Natural
boundaries are found without regard to labels, and the labeled instances
are used to determine labels for the resulting regions.
• Constraint- and agreement-driven learning (chapters 2–3 and 9).
In a sense, all semisupervised learning is driven by constraints. Suﬃ-
ciently restrictive constraints can be almost as good as labels for unla-
beled data – and in some cases even better, inasmuch as a constraint ap-
plies to the entire population of instances, whereas a label on an instance
applies only to that instance. We have mentioned how graph methods

10
Semisupervised Learning for Computational Linguistics
translate similarities into soft constraints. A particularly salient class of
constraints is agreement constraints. For example, in co-training, the
learner is given two independent “views” of the data, and constructs one
classiﬁer for each view, under the constraint that the classiﬁers agree in
their predictions on the unlabeled data. Eﬀectively, instances come in
pairs that are constrained to have the same label.
The learner does
not know what the label is, but does know that it is the same for both
members of the pair. A non-algorithmic way of enforcing agreement is
via a random ﬁeld that penalizes disagreement.
• Spectral methods (chapters 11–12).
One can build on the idea of
interpolation that emerges from label propagation by using a “standing
wave” to interpolate across the graph. Pursuing this idea leads to deep
connections among apparently disparate ideas, including the cluster hy-
pothesis and label propagation, “mincut” boundary-oriented methods,
and random ﬁelds.
The plan of the book more or less follows the list of leading ideas just given,
with a couple of rearrangements for the sake of a smoother line of develop-
ment. We begin, in chapters 2 and 3, with a discussion of the semisupervised
methods that are already well known in computational linguistics, namely,
self-training and co-training. We turn then to methods that come from the
machine learning literature, beginning in chapter 4 with an introduction to
classiﬁcation, including some detail on the topic of decision boundaries. The
discussion of boundary-oriented methods follows naturally at that point (chap-
ters 5–6). Then we turn to clustering in chapter 7, followed by discussion of
the EM algorithm and related generative methods in chapter 8. There are
connections between co-training and the generative methods of chapter 8, so
the chapter on agreement methods is placed next (chapter 9). Finally, the
strand of graph-based methods, begun in the chapter on clustering, is picked
up in chapter 10, which concerns label propagation, and in chapters 11 and
12 on spectral methods.
1.3.2
Mathematical background
As stated in the preface, my goal is to bring the current state of the art in
semisupervised learning within the reach of a student or researcher in compu-
tational linguistics who has mastered the standard textbooks, in particular,
Manning and Sch¨utze, and has acquired a certain familiarity with machine
learning through references in the computational linguistics literature, but
does not necessarily have a general background in machine learning. This
goal is more than a little quixotic. To do things properly, we should lay a
foundation of linear algebra, multivariate calculus, optimization theory, prob-
ability and statistics, and even a bit of physics (e.g., simple harmonic motion),
and on that build a proper treatment of classiﬁcation and clustering, before

Introduction
11
tackling the actual topic of interest, semisupervised learning. But doing so
would involve replicating many volumes of material that has been well covered
elsewhere. A reader who has already mastered all the background material
just mentioned is in an excellent position to tackle the primary literature on
semisupervised learning, and will probably not ﬁnd this book particularly
useful. On the other hand, readers who have not mastered all the necessary
background material will rightfully feel daunted by the enormity of the task,
and would under most circumstances decide that, however interested they
may be in semisupervised learning, the cost of entry is simply too great to
pay. Those are the readers for whom this book is intended.
My strategy has been to blaze a long thin trail, ﬁlling in just the back-
ground that is needed to give a reasonably detailed account of the selected
semisupervised learning techniques. Two chapters provide an introduction
to machine learning: one on classiﬁcation (chapter 4) and one on clustering
(chapter 7). They do not attempt to give a balanced overview of the ﬁeld,
but only to treat topics speciﬁcally needed for semisupervised learning. As
for more general mathematical background, I have chosen not to collect it
into a single chapter – the result would have been a disconnected collection of
topics, and the reasons for their inclusion would only have become clear much
later. Instead, these topics have been introduced “just in time.” The cost is
a rather lengthy run-up to the semisupervised techniques involved, especially
SVM-based and spectral methods, but that seemed the lesser of the two evils.
1.3.3
Notation
I have collected here notational conventions that I use that are nonstandard
or may not be familiar to all readers.
[[Φ]]
semantic value: 1 if Φ is true and 0 otherwise
P
x[[x ∈S]]w(x)
equivalent to: P
x∈S w(x)
||x||
vector norm:
pP
i x2
i
|A|
cardinality of a set or absolute value of a number
p[φ]
the expectation of φ under distribution p
˜p(x)
empirical distribution: relative frequency in sample
f(x) = ⊥
f(x) is undeﬁned
≡
is deﬁned as
F ⇒y
x ←x + 1
set the value of x (in an algorithm)
Dx
rule: if the instance has feature F, predict class y
derivative with respect to a vector; see section 5.2.3


2
Self-training and Co-training
In the previous chapter we introduced self-training and co-training as the most
widely used semisupervised learning algorithms in computational linguistics.
We present them in more detail in this chapter.
Both algorithms are “naive” in the sense that they did not derive from a
theory of semisupervised learning; rather, they are embodiments of simple
algorithmic ideas that can be grasped without much background. This is not
to say that they lack theoretical justiﬁcation. Co-training and self-training
can both be justiﬁed on the basis of a conditional independence assumption
that is closely related to the independence assumption underlying the Naive
Bayes classiﬁer. And forms of the algorithms have arisen on the basis of a
theory of semisupervised learning – particular examples are McLachlan’s ver-
sion of self-training (discussed in chapter 8) and de Sa’s version of co-training
(discussed in chapter 9). These theoretically motivated algorithms even pre-
date the versions that are well known in computational linguistics. But we
focus here on the familiar versions, whose primary attraction is simplicity and
intuitiveness, rather than theoretical underpinnings.
2.1
Classiﬁcation
Self-training and co-training are both classiﬁer-learning algorithms. We intro-
duced classiﬁcation in the previous chapter as function estimation in which
the function is nominal-valued and the learning is supervised.
With self-
training and co-training, we extend the notion of classiﬁcation to include the
semisupervised case. Ultimately, we will view semisupervised classiﬁcation as
a generalization of classiﬁcation and clustering.
We begin with some grounding in classiﬁcation.
2.1.1
The standard setting
An example of a classiﬁcation problem which we have already mentioned is
part-of-speech tagging. This is a well-studied task in computational linguis-
tics. The input to a tagger is a plain text, such as
13

14
Semisupervised Learning for Computational Linguistics
instance
label
a black dog appeared
det
a black dog appeared
adj
a black dog appeared
noun
a black dog appeared
verb
FIGURE 2.1
Inputs and outputs for a part of speech classiﬁer.
a black dog appeared
and the output is a sequence of parts of speech, perhaps:
det adj noun verb
One can reduce tagging to a classiﬁcation problem by treating each word
separately. Each word constitutes an instance, and the possible class labels
are the possible parts of speech. An example is given in ﬁgure 2.1. The left
column shows inputs that the classiﬁer receives, and the right column shows
the output of the classiﬁer for each input. The box indicating the “current
word” is an essential part of the input; in fact, the inputs diﬀer only in the
location of the box. The box indicates which word is the word of interest,
and the rest of the text provides the context. We note that the context does
not include information about the classiﬁer’s own previous decisions. Previous
decisions could be included in the context. Doing so would mean that training
instances are not independently drawn, violating a standard assumption about
the training data, but we have in fact already violated that assumption by
using running text. We cannot include both previous and future decisions,
however. There are methods for choosing the best overall sequence of labels
– Hidden Markov Models and Conditional Random Fields (CRFs) are two
well-known examples – but they take us beyond classiﬁcation.
A classiﬁer learner is given a set of training data as input, and produces
a classiﬁer as output.
Training data is labeled: it consists of a set of in-
stances, representing sample inputs, along with the correct output for each.
The goal of the learner is to produce a classifer that generalizes well to the
entire population of instances, and particularly to new instances that were
not encountered in training.
2.1.2
Features and rules
For eﬀective learning, instances must be broken down into features. A feature
may be anything, but it is often the pairing of an attribute with a particular
value. For example, we might adopt the attributes string for the current
word as an ASCII string, prev for the previous word, and next for next word.
The value for each attribute is an actual word, for example, “prev=black.”

Self-training and Co-training
15
instance
label
{prev=null, string=a, next=black}
det
{prev=a, string=black, next=dog}
adj
{prev=black, string=dog, next=appeared}
noun
{prev=dog, string=appeared, next=null}
verb
FIGURE 2.2
Instances represented as sets of features.
There are two interchangeable ways of viewing the expression “prev=black.”
It may be viewed as the assignment of the value “black” to the attribute
“prev,” or, in its entirety, it represents a single feature, distinct from other
features like “string=black” or “next=black” or “prev=dog.” An instance
consists of a set of features. For example, the training text in ﬁgure 2.1 might
be represented as in ﬁgure 2.2.
Generally, a feature corresponds to a predicate F. An instance x possesses
the feature just in case F(x) is true. An attribute corresponds to a function
h.
The pairing of an attribute h and value v corresponds to a predicate
which is true just in case h(x) = v, and false otherwise.
A feature may
alternatively correspond to the pairing of a two-place relation R with a value
v. The predicate thus represented is true just in case R(x, v). For example, a
word string has multiple suﬃxes, of varying lengths. Consider the two-place
predicate hasSuffix, such that hasSuffix(x, y) is true just in case y is a
suﬃx of x. The expressions hasSuffix(dogs, s) and hasSuffix(dogs, gs)
are both true. We will adopt the convention of writing hasSuffix:s for the
feature that is possessed by an instance x just in case hasSuffix(x, s) is true.
In short, the diﬀerence between a feature like string=dogs and a feature like
hasSuffix:s is that an instance may possess only one “string=” feature, but
it may possess multiple “hasSuffix:” features.
Many authors use “feature” and “attribute” synonymously. I should em-
phasize that I use them distinctly. An attribute has a value; the combination
of an attribute and a value is a feature.
We have seen that an instance may be represented as a set of features. An
instance may also be represented as a vector of attribute values. If a feature
is not explicitly represented as the pairing of an attribute and a value, we
may always take the feature itself to be the attribute, and the value to be
T (true) if the instance possesses the feature and F otherwise. Similarly, the
combination of a relation and value, like hasSuffix:s, can be taken as an
attribute whose value is either T or F.
If we ﬁx the order of attributes, a unique vector of values is determined
for each instance. For example, if we ﬁx the order of attributes as (prev,
string, next), the instances of ﬁgure 2.2 can equivalently be represented as
the vectors of ﬁgure 2.3.

16
Semisupervised Learning for Computational Linguistics
instance
label
(null, a, black)
det
(a, black, dog)
adj
(black, dog, appeared)
noun
(dog, appeared, null)
verb
FIGURE 2.3
Instances represented as vectors of attribute values.
2.1.3
Decision lists
Decision lists provide a simple concrete example of a classiﬁer, and they are
also the supervised learner on which a popular version of self-training is based.
A decision list consists of a sequence of rules of the form “if instance x
possesses feature F, then predict label y,” for example:
if string=dog then predict noun
The order in which the rules are listed is signiﬁcant. The list may also include
a default rule “predict y.”
A rule matches an instance x if x possesses the feature in the antecedent of
the rule. A default rule always matches. The prediction of a decision list on
input x is determined by the ﬁrst rule in the list that matches x. For example,
the decision list
if prev=a then predict noun
if string=black then predict adj
predict verb
predicts “noun” on the input
{prev=a, string=black, next=dog}
but it predicts “adj” on the input
{prev=the, string=black, next=dog}
Henceforth we abbreviate “if F then predict y” as “F ⇒y” and the default
rule “predict y” as “⇒y.”
The task of the learner is to decide which rules to include in the list, and
how to sort them. An easy choice regarding which rules to include is to include
all that are attested in the training data. If there is an instance (x, y) in the
training data, and x has feature F, then include the rule “F ⇒y.” Also,
for each label y, include the default rule “⇒y.” For example, in the little
training set of ﬁgure 2.2, there are twelve features attested, and each only
ever occurs with one label, yielding twelve rules, plus four default rules.
It may seem odd to include default rules for all four labels. Only one of
them will ever be used in the end – whichever one is ordered ﬁrst in the

Self-training and Co-training
17
decision list will always match, and the others will never be used. Borrowing
a term from phonology, we can say the ﬁrst one bleeds the others. But we
include them all in the initial list, and decide which one will will bleed the
others when we sort the rules. If desired, the others could be pruned after the
list is sorted. Similarly, wherever there are multiple rules F ⇒y and F ⇒y′
with the same feature but diﬀerent labels, the ﬁrst one on the list will bleed
the others, and all but the ﬁrst can be pruned.
As for sorting, the main issue is how to sort rules that have diﬀerent features.
It is possible for more than one rule to match an instance x, each rule matching
a diﬀerent feature of x. The simplest way is to sort by the probability that
the rule is right, given that it matches. That is, the learner sorts the list by
descending conditional accuracy, breaking ties arbitrarily, where conditional
accuracy of the rule is deﬁned as
CA(F ⇒y) = Pr[true label is y|rule ﬁres].
In most cases, we are concerned with conditional accuracy as a sample statis-
tic, for either the training or test set, in which case
CA(F ⇒y) =
P
i[[F ∈xi, yi = y]]
P
i[[F ∈xi]]
.
where i ranges over the (indices of the) instances in the sample. Here we have
borrowed from semantics the notation [[φ]]:
[[φ]] ≡

1 if φ is true
0 otherwise
.
Conditional accuracy is related to a perhaps more familiar quantity called
precision, but it is not the same; the connection is discussed in chapter 4.
We have already mentioned the bleeding that occurs when two rules are
conditioned on the same feature.
We also note that a default rule bleeds
all rules that follow it, since a default rule always matches. For example,
suppose that the most-frequent label occurs on half the training instances,
and suppose that a particular feature F occurs three times, each time with a
diﬀerent label. Then each of the three rules conditioned on F has conditional
accuracy 1/3, but the default rule has conditional accuracy 1/2. Hence the
default rule precedes all of the rules conditioned on F, and none of them ever
matches an instance. They can be deleted without aﬀecting the predictions
of the classiﬁer.
Instead of assuming an ordered list, we can interpret the conditional accu-
racy of each rule as a score or weight. Among the rules that match a given
instance, the decision list’s prediction is determined by the one that has the
highest score.
Note that a decision list actually provides more information than just a
prediction concerning the label for a given input instance. The score of the

18
Semisupervised Learning for Computational Linguistics
“winning” rule can be interpreted as a measure of conﬁdence: the higher
the score, the more conﬁdent the classiﬁer is that its prediction is correct.
A classiﬁer such as this, that produces both a label prediction and also a
measure of conﬁdence in that prediction, is said to be a conﬁdence-rated
classiﬁer. A measure of conﬁdence often proves useful in adapting classiﬁers
to a semisupervised setting.
Finally, we should note that decision lists may fail to make a prediction, that
is, they may abstain. Abstention does not occur if the decision list contains a
default rule, but if there is no default rule, and a given input instance possesses
no feature that is mentioned in the decision list, then the decision list makes
no prediction.
The conditional accuracy measure extends naturally from single rules to
entire decision lists, or for that matter, to any classiﬁer f that may abstain.
Let us write f(x) = ⊥to represent the case in which classiﬁer f abstains on
input x, the symbol “⊥” representing “undeﬁned.” The conditional accuracy
of f on a sample is
CA(f) =
P
i[[f(xi) = yi ̸= ⊥]]
P
i[[f(xi) ̸= ⊥]]
.
A complementary measure is the ﬁring rate of the classiﬁer,
FR(f) = 1
n
X
i
[[f(xi) ̸= ⊥]]
where n is the sample size.
If we use the notation “˜p[φ]” for the sample
probability that event φ occurs, then we can write these quantities more per-
spicuously:
CA(f) = ˜p[ f(X) = Y | f(X) ̸= ⊥]
FR(f) = ˜p[ f(X) ̸= ⊥]
where we have introduced X and Y as random variables ranging over instances
and their associated labels in the sample. The overall accuracy of the classiﬁer
is the product of these two values:
Acc(f) ≡˜p[ f(X) = Y ]
= FR(f) · CA(f).
2.2
Self-training
We turn now to self-training. The version of the algorithm that we describe
is that of Yarowsky [239].

Self-training and Co-training
19
2.2.1
The algorithm
The algorithm is simple, and that is in fact a large part of its appeal. Use a
seed set of labeled data to construct a conﬁdence-rated classiﬁer (such as a
decision list), apply the classiﬁer to unlabeled data, and take the predictions
of the classiﬁer to be correct for those instances where it is most conﬁdent.
Expand the labeled data by the addition of the classiﬁer-labeled data, and
train a new classiﬁer. The process may be iterated: continue labeling new
data and retraining the classiﬁer until a stopping condition is met.
Though Yarowsky’s paper is the most widely cited, the key idea – taking an
initial classiﬁer’s high-conﬁdence predictions at face value and adding them
to the training data to train a new classiﬁer – was proposed in at least two
earlier papers, one by Hindle & Rooth [111] and one by Hearst [109].
Hindle & Rooth address the problem of prepositional phrase (PP) attach-
ment.
Their seed consists in a set of manually constructed, deterministic
decision rules that are treated as inerrant. Applying the deterministic rules
to unlabeled data yields an initial labeled set, from which statistics are esti-
mated for a probabilistic decision rule. The probabilistic decision rule is then
applied to the remaining unlabeled data, and its high-conﬁdence predictions
are accepted at face value and used to augment the statistics.
Hearst, like Yarowsky, addresses the problem of word-sense disambiguation.
She deﬁnes a decision rule based on certain statistics computed from labeled
data, and then augments the statistics using unlabeled instances where the
predictions of the decision rule are suﬃciently conﬁdent. Neither Hearst nor
Hindle & Rooth iterate the process of labeling and retraining.
None of these authors treats semisupervised learning as a paradigm in its
own right, though Hearst comes closest. She explicitly describes her algo-
rithm as a combination of supervised and unsupervised learning: “A period
of supervised training, . . . is required as a bootstrap before unsupervised train-
ing or testing can begin” (p. 5). Yarowsky also makes an oblique mention of
bootstrapping, though he considers his algorithm to be unsupervised learning.
With hindsight, we will treat the three papers just mentioned as examples
of self-training. (I have taken the term from Nigam & Ghani [166], though
it may well be older.) The basic form of self-training is given in ﬁgure 2.4.
The algorithm produces a sequence of classiﬁers, which are understood to be
conﬁdence-rated classiﬁers. A conﬁdence-rated classiﬁer takes an instance as
input and produces as output a pair (y, s) consisting of a label y and conﬁdence
score s. The classiﬁer may also abstain, in which case its output is ⊥.
The function label in ﬁgure 2.4 applies the classiﬁer c to each instance in
the unlabeled data set U, producing one pair for each instance. The resulting
list of (y, s) pairs is a conﬁdence-weighted labeling. To be precise, the output
of label is a conﬁdence-labeled data set, consisting of both the conﬁdence-
weighted labeling and the instances to which the labeling applies.
The function select takes a conﬁdence-labeled data set and selects only
those instances where the conﬁdence is “suﬃciently high.” The output is a

20
Semisupervised Learning for Computational Linguistics
procedure selfTrain (L0, U)
1
L0 is labeled data, U is unlabeled data
2
c ←train(L0)
3
loop until stopping criterion is met
4
L ←L0 + select(label(U, c))
5
c ←train(L)
6
end loop
7
return c
FIGURE 2.4
Self-training, basic form.
(smaller) labeled data set (without conﬁdences).
The function train represents a supervised classiﬁer learning algorithm,
which we call the base learner. It takes labeled data and produces a conﬁdence-
rated classiﬁer.
2.2.2
Parameters and variants
The schema contains several open parameters, and there are a number of
additional points where variations have been explored. What is essential to
self-training is:
• Using an intermediate classiﬁer to produce hard-labeled data
• Using the classiﬁer-labeled data to train an improved classiﬁer
By “hard-labeled” we mean that unlabeled instances are assigned deﬁnite
labels, as opposed to probability distributions over labels. One can certainly
imagine varying even that aspect of the algorithm, but the result is probably
best seen as falling into a diﬀerent class of algorithm. Training a classiﬁer
on the distributions predicted by a previous classiﬁer is characteristic of the
Expectation-Maximization (EM) algorithm, which we discuss in chapter 8.
The open choices in the schema include the choice of supervised learning
algorithm (the base learner), the manner of thresholding conﬁdence, and the
stopping criterion. There are also points at which variations have been pro-
posed.
Base learner. Virtually any supervised learning algorithm can be used to im-
plement the “train” step. It is assumed that the classiﬁer makes conﬁdence-
weighted predictions. Yarowsky uses a decision list algorithm. Hearst con-
structs a vector, indexed by features, for each word sense; the decision rule
predicts the class whose vector is most similar to the feature vector of the

Self-training and Co-training
21
instance to be classiﬁed. Hindle & Rooth make independence assumptions to
permit a straightforward estimate of conditional probability of classes given
the features of the instance.
Conﬁdence measure. The function select in ﬁgure 2.4 preserves labeled
instances on which the classiﬁer is suﬃciently conﬁdent, and discards the
remaining instances.
There are several ways of measuring conﬁdence.
A
conﬁdence-rated classiﬁer is often based on an estimate of, or approximation
to, the conditional probability of the predicted label, that is, p(y|x), where x
is the input and y is the classiﬁer’s output. For example, in the decision list
algorithm (section 2.1.3), the classiﬁer’s prediction is determined by a single
rule, the “winning rule,” and the conditional accuracy of the winning rule
provides the classiﬁer’s measure of conﬁdence. Writing h(x) for the prediction
of the winning rule, with h(x) = ⊥if the rule does not match, the conditional
accuracy of the winning rule is an estimate of p[Y = y|h ̸= ⊥], and it can be
seen as an approximation to p[Y = y|X = x], under the assumption that the
rule h is conditioned on the “most reliable” feature of x.
Another possibility is to consider not the absolute value p(y|x), but rather
the ratio
p(y1|x)
p(y2|x)
(2.1)
where y1 is the most-probable label and y2 is the second most probable label.
The ratio (2.1) is maximized when the conditional distribution p(y|x) is a
point distribution with all its mass on one label (deﬁning the value to be +∞
if p(y2|x) = 0), and it is minimized when the two most-likely labels are equally
probable. In the two-label case, it is monotonically related to the entropy of
the distribution p(y|x). That entropy provides a third possible measure of
conﬁdence.
Stopping criterion. Our schema does not specify when to stop looping. One
simple method is to run for a ﬁxed, arbitrary number of rounds. A method
that is almost as simple is to keep running until convergence, that is, until
the labeled data and classiﬁer stop changing.
A third alternative is to divide the training data into two parts. One part
is passed to the learner as training data, and the other part is set aside as
validation data.
After each iteration, the performance of the classiﬁer is
measured on the validation data, and a stopping point is chosen based on
performance on the validation data. A reﬁnement is cross-validation. One
divides the data into n segments, usually ﬁve or ten, and one uses each segment
in turn as validation data, with the remaining segments supplying training
data. Performance on the validation data is used to estimate the optimal
number of iterations through the loop; by letting each segment play the role
of validation data in turn, one obtains n diﬀerent estimates. Averaging them
gives a ﬁnal estimate T of the optimal number of iterations. Then the entire
data set is used for training, stopping after T iterations.

22
Semisupervised Learning for Computational Linguistics
Seed. The schema assumes that the seed with which the “bootstrapping”
process begins consists of labeled data (L0). An alternative is to provide an
initial classifer instead of labeled data. This is actually a point of diﬀerence
between Hearst and Yarowsky: Hearst uses labeled data as a seed, whereas
Yarowsky uses an initial classiﬁer, in the form of a small list of highly reliable
rules.
It should be noted that neither labeled data nor an initial classiﬁer is gen-
erally reducible to the other. If one uses a seed classiﬁer to label a data set
and then one induces a new classiﬁer from the data set, one does not generally
obtain the seed classiﬁer again. Conversely, if one begins with labeled data,
induces a classiﬁer, and relabels the data, the result is not generally identical
to the original labeling. Labeled data has the advantage of being error-free
(one hopes), but a seed classiﬁer has the advantage of applying to the entire
population of instances.
Other forms of seed are possible, for example, a set of constraints that re-
stricts the allowable labels for each instance based on some feature or features
of the instance. Such constraints have been used in part-of-speech tagging –
we have already mentioned that a dictionary can be used to specify a limited
set of possible parts of speech for each word. Such seed constraints produce a
partial labeling of the data, in two senses of partial. The dataset is partially
labeled in the sense that the constraints are usually suﬃciently restrictive
to assign unique labels to at least some instances; and, in addition, many
instances are “partially labeled” in the sense that the constraints eliminate
some labels from consideration, providing partial information about the cor-
rect label.
Indelibility. The basic version of the schema relabels the unlabeled data
from scratch with every iteration. In a variation, labels, once assigned, are
never recomputed; they are indelible. Indelibility is obtained by replacing the
“L0” in line 4 of ﬁgure 2.4 with “L”. Indelibility is natural when the seed is
labeled data and the classiﬁer is recomputed from scratch in each iteration.
The beneﬁt of label indelibility is that it provides stability and prevents the
learner from wandering too far from the original seed.
A weaker form of indelibility is what we might call persistence. Once an
instance is labeled, it remains labeled, but the label may change.
Many classiﬁers can be represented as a list or set of simple rules, in which
case indelibility may also apply to the classiﬁer. If rules are indelible, the
classiﬁer is not retrained from scratch, but rather is grown monotonically, by
adding new rules without deleting old ones.
Throttling. A variation on the behavior of select is the following. Instead
of accepting all instances whose conﬁdence exceeds a threshold, a limit k is
placed on the number of instances accepted into the labeled set. This prevents
a large inﬂux of newly labeled instances from overwhelming the inﬂuence of
the previously labeled instances in early iterations.

Self-training and Co-training
23
Balancing. Another modiﬁcation to selection, often coupled with throttling,
is to select the same number of instances for each class, even if it necessi-
tates having diﬀerent thresholds for the diﬀerent classes.
Classiﬁers often
have a predeliction for one class over another, which is problematic because
any under- or oversampling of classes tends to get ampliﬁed in subsequent
iterations. Balancing counteracts that tendency. It can be seen as an addi-
tional weak constraint, providing the learner with some information about the
target, namely, the (approximate) relative frequency of labels. As a practical
matter, balancing is usually indispensable for good performance.
Preselection. A third variation on selection is to do two passes of selection.
Instead of selecting the highest-conﬁdence predictions from the entire set of
unlabeled data, it can be beneﬁcial to apply the classiﬁer to a smaller sam-
ple of the unlabeled instances, sometimes called a pool. The use of a pool
is motivated by the desire to keep newly labeled instances from being too
homogeneous. It prevents the classiﬁer from choosing a set of very similar
instances that it particularly “likes,” and instead forces it to choose a more
heterogeneous set to add to the labeled set.
One way of selecting the members of the pool is random sampling. Random
sampling is used in the original co-training paper [21], and has subsequently
been applied to self-training as well. An alternative is to use one measure of
conﬁdence (or more generally, quality) for preselection, and a second measure
for selection.
2.2.3
Evaluation
As should be apparent from the description just given, self-training is very
much an algorithmic and heuristic approach. It is predicated on certain in-
tuitions, but it has developed for the most part through empirical ﬁxes to
empirical problems, rather than algorithms based on mathematical consider-
ations. Nonetheless, it is worthwhile to examine the intuitions brieﬂy.
The basic reasoning is something like the following. A classiﬁer performs
better as you increase the amount of labeled data it is trained on, provided
that the proportion of labeling errors is negligible, which we may take to mean
less than some rate ϵ. Suppose that classiﬁer conﬁdence c(y|x), computed on
the labeled data, provides a good estimate of the true distribution p(y|x) on
the unlabeled data. Let X be the set of unlabeled instances where classiﬁer
conﬁdence exceeds 1−ϵ, that is, where ˆy maximizes c(y|x) and c(ˆy|x) > 1−ϵ.
If c being a good estimate of p means that p(ˆy|x) > 1 −ϵ for instances in
X, then the proportion of erroneous predictions in X is less than ϵ, and we
can take the predictions of the classiﬁer on X at face value and increase
the number of labeled instances for training, while keeping the proportion of
labeling errors negligible.
This reasoning makes several assumptions that are not necessarily true. It
is certainly true that training a classiﬁer on a larger sample drawn from the

24
Semisupervised Learning for Computational Linguistics
string=New_York →LOC
string=U.S. →LOC
string=California →LOC
hasSubstring:Mr. →PER
hasSubstring:Incorporated →ORG
string=Microsoft →Microsoft
string=I.B.M. →ORG
FIGURE 2.5
The seed classiﬁer of Collins & Singer.
true instance distribution D(x) is better than training it on a smaller sam-
ple. But the way we select new labeled instances for inclusion, selecting those
where the current classifer is most conﬁdent, means that the newly sampled
instances are almost certainly not drawn from anything like the distribution
D(x). (Incidentally, preselection can be seen as a way of forcing the distri-
bution of the new sample to be more like D(x) than it would be otherwise.)
Similarly, a ﬁxed error rate may be negligible if the errors are uncorrelated,
but the selection process is likely to cause errors to be correlated. Finally, any
biases may be ampliﬁed by the feedback loop of iterative retraining.
Nonetheless, despite the lack of guarantees that its assumptions and ap-
proximations hold, self-training does work on at least some data sets. When
it works, it appears to behave in the way that the above reasoning would lead
us to expect. A sequence of abstaining classiﬁers is produced. The algorithm
is designed to keep conditional accuracy high, while gradually increasing the
ﬁring rate (that is, decreasing the abstention rate).
Figure 2.6 illustrates a typical trajectory. The task is that described by
Collins & Singer [50]; noun phrases are to be classiﬁed as person, location, or
organization. There are 89,000 unlabeled training instances and 1,000 labeled
test instances. The solid line in the ﬁgure shows the conditional accuracy after
each iteration of labeling and training. The base learner is a decision list, and
the seed consists of seven rules, given in ﬁgure 2.5. As shown in ﬁgure 2.6,
the seed classiﬁer has a ﬁring rate of 11% and a conditional accuracy of 100%.
The dotted lines are contours of accuracy. A default prediction of “person”
is assumed in cases where the classiﬁer abstains, and the default prediction
has an accuracy of 46%. Hence the only contour where accuracy is unaﬀected
by ﬁring rate, and the contour is a horizontal line, is the contour of accuracy
equal to 46%. That contour is not shown, but it lies about halfway between
the lowest contour that curves upward, representing an accuracy of 50%, and
the highest contour that curves downward, representing 40%. The accuracy
of the seed classiﬁer is represented by the heavy dotted contour; its numeric
value is 52%:
(0.11)(1.0) + (0.89)(0.46) = 0.52.

Self-training and Co-training
25
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
Conditional Accuracy
Firing Rate (Recall)
FIGURE 2.6
A typical trajectory for self-training. The dotted lines are contours of equal
accuracy. The accuracy of the default classiﬁer is 0.46; it is represented by
the heavy dotted line.
After a drop in conditional accuracy in the very ﬁrst round, conditional
accuracy remains essentially constant, but ﬁring rate increases. As a result,
accuracy increases steadily. Learning was stopped after 1,000 iterations, at
which point the ﬁring rate was 98% and conditional accuracy and accuracy
were both 75%.
We note that it is possible to obtain considerably higher
performance on this data set; performance varies signiﬁcantly with diﬀerent
choices in the options discussed above.
2.2.4
Symmetry of features and instances
The large jump in ﬁring rate, and corresponding drop in conditional accuracy,
in the ﬁrst iteration in ﬁgure 2.6 occurs because no pruning was done to the
decision list. Collins & Singer [50] obtain considerably higher performance by
doing preselection, throttling, and balancing on the rules forming the decision
list, rather than on the labeled instances.
This points out a symmetry between features and instances that arises
when the base learner is a decision list or other simple combination of simple
rules, by which we mean rules that are conditioned on single features.
A
simple rule F ⇒y can be viewed as a labeled feature, where F is the feature
and y is the label. A decision list is a set of labeled features. No feature
appears twice in the list – or, at least, if any feature appears multiple times,
all but one occurrence can be eliminated without aﬀecting the predictions of
the decision list, because the one with the highest score bleeds the others. Not
all features necessarily appear in the decision list; the ones that do not appear
are unlabeled features. In short, features are analogous to instances; simple
rules (“labeled features”) are analogous to labeled instances; and decision lists

26
Semisupervised Learning for Computational Linguistics
procedure selfTrain2 (C0, U)
1
C0 is a seed decision list, U is unlabeled data
2
c ←C0
3
loop until stopping criterion is met
4
L ←label(U, c)
5
c ←C0 + select(train(L))
6
end loop
7
return c
FIGURE 2.7
Self-training, “dual” form.
(“labeled feature sets”) are analogous to labeled sets of instances.
The symmetry between features and instances suggests a “dual” version of
self-training in which conﬁdence-based selection is applied to the rules, not to
the instances (ﬁgure 2.7). All of the variations discussed in section 2.2.2 apply
to the dual case as well. We already mentioned that an alternative form of seed
is an initial classiﬁer instead of a set of labeled instances; an initial classiﬁer
is the natural form of seed for the dual version. Throttling, balancing, and
preselection can all be included in the select function in ﬁgure 2.7. The
algorithm can be modiﬁed to introduce persistence or indelibility of feature
labels. In the indelible version, once a rule F ⇒y is added to the classiﬁer,
it is never deleted (and no contradictory rule is added). In the persistent
version, once a rule F ⇒y is added to the classiﬁer, it may be replaced by a
rule associating a diﬀerent label with F, but there must always be some rule
associating a label with F.
The Collins & Singer versions of self-training (the algorithms they call
“Yarowsky-95” and “Yarowsky-cautious”) are of the dual form. The better-
performing one is “Yarowsky-cautious,” which uses throttling and balancing.
It uses a rule’s conditional accuracy for preselection – speciﬁcally, only rules
whose conditional accuracy exceeds 95%, as measured on the labeled data,
are included in the pool. Selection is done using ﬁring rate. For each label y,
of the rules that predict y, the k rules with the highest ﬁring rate are selected.
The value of k starts at 5 and increases by 5 at each iteration. Rules are not
persistent, but the growth of the classiﬁer is like that of a persistent classiﬁer
that grows by k rules at each iteration.
Interestingly, the rule score used to determine the classiﬁer’s prediction on
new instances is diﬀerent from either of the measures used in selection or
preselection. The score for rule F ⇒y when used for prediction is smoothed
conditional accuracy:
P
i[[F ∈xi, y = yi]] + ϵ
P
i[[F ∈xi]] + mϵ

Self-training and Co-training
27
where m is the number of labels. The use of three diﬀerent scores seems rather
ad hoc, but it does emphasize the diﬀerence between rule selection (the train
step) and prediction (the label step).
There is also a body of work on induction of “semantic lexica” that uses
the dual form of self-training. By “semantic lexicon” is meant a list of entities
of a particular semantic class – such as a list of names of people, or names of
cities, or names of vehicles – or several such lists. A semantic lexicon is also
called a gazetteer.
In traditional terminology, the elements of a semantic-class list are entity
types, and their occurrences in running text are entity tokens. A type can
be seen as a particular sort of feature, one whose value is the entire phrase
(usually a noun phrase) but excluding context.
This is a commonly used
attribute; for example, it is the attribute called “string” in the list of seed
rules of ﬁgure 2.5. A list of entities belonging to a particular class is equivalent
to a list of rules “string=s ⇒y” where s is the entity type and y is the
semantic class that the list exempliﬁes. A collection of such lists deﬁnes a
classiﬁer.
Entity tokens in running text are instances. Based on statistics of tokens,
new types are added to the lists; this corresponds to the learn step in ﬁgure
2.7. In the label step, additional tokens of the types in the lists are marked.
Typically, the seed consists of list items – that is, it is a classiﬁer. Selection,
including throttling and balancing, is typically applied in the learn step, not
the label step. In both of these respects, the algorithm instantiates the dual
form of self-training (ﬁgure 2.7). Semantic lexicon induction will be discussed
in more detail in section 3.4.3.
2.2.5
Related algorithms
Given the simplicity of self-training, it is not surprising that it has arisen in
many variants in diﬀerent contexts. The idea of self-training will come up
repeatedly throughout the book; we give a brief roadmap here.
• k-means clustering is a commonly used clustering algorithm that is
quite similar to self-training. One can view self-training as an adaptation
of k-means clustering to the semisupervised setting, with generalized
labeling and training steps. K-means clustering is discussed in section
7.4.1.
• Pseudo relevance feedback. In information retrieval, documents are
classiﬁed as either relevant or not relevant to a given query. A retrieval
algorithm essentially constructs a binary classiﬁer given a query and ap-
plies it to a body of (unlabeled) documents. Those where the classiﬁer
is most conﬁdent are returned to the user. In relevance feedback, the
user manually labels some of the documents, and an improved classiﬁer
is constructed. In pseudo relevance feedback, the initial classiﬁer’s pre-
dictions are taken at face value where the classiﬁer is most conﬁdent,

28
Semisupervised Learning for Computational Linguistics
and a new classiﬁer is constructed from the labeled data. The process
may be iterated. It is discussed in section 7.4.2.
• Label propagation. Yarowsky visualizes self-training as propagating
labels through a graph. We will discuss label propagation in considerable
detail, for example in chapter 10. The formulation of self-training as
label propagation is discussed in section 7.6.2.
• Decision-directed approximation is an unsupervised learning algo-
rithm that alternates training and labeling in a way that is very similar
to self-training. One begins with an arbitrary assignment of labels, and
one uses a supervised learning algorithm that maximizes likelihood to
construct a classiﬁer. Next, one relabels the data by choosing, for each
instance, the highest-probability label according to the classiﬁer. Then
the process repeats, alternating training and labeling. In the semisuper-
vised version, labels for some instances are given, and relabeling applies
only to the initially unlabeled data. Decision-directed approximation is
discussed in section 8.1.3.
• McLachlan’s algorithm can be viewed as a special case of decision-
directed approximation in which the learned model is a mixture of Gaus-
sians and a nearest-neighbor rule is used for relabeling – though it would
be historically more accurate to say that decision-directed approxima-
tion is a generalization of McLachlan’s algorithm. McLachlan’s algo-
rithm appears to be the oldest version of self-training. It is discussed in
section 8.1.4.
• Game self-teaching. There are two distinct learning approaches that
have been called “self-teaching.” One arises in the context of learning to
play a game; it is the idea of having a system teach itself to play a game
by playing against itself. It is discussed in section 6.2, in connection
with the perceptron algorithm.
• Agreement-based self-teaching. The second setting that has been
called self-teaching involves two separate modalities, such as vision and
hearing, that provide information about a single learning task.
It is
closely related to co-training, and we will discuss it in detail in section
9.2, when we turn to the theory of agreement-based algorithms.
2.3
Co-Training
Co-training was proposed by Blum and Mitchell [21].
It is predicated on
having two independent views of each instance.
The example used in the

Self-training and Co-training
29
procedure cotrain (L, U)
1
L is labeled data, U is unlabeled data
2
P ←random selection from U
3
loop until stopping criterion is met
4
f1 ←train(view1(L))
5
f2 ←train(view2(L))
6
L ←L + select(label(P, f1)) + select(label(P, f2))
7
Remove the labeled instances from P and replenish P from U
8
end loop
FIGURE 2.8
Co-training.
original paper is classiﬁcation of computer science web pages; speciﬁcally, the
task is binary classiﬁcation of web pages as being course home pages or not.
An instance is a web page, and the two views are (1) the contents of the
web page itself, and (2) the contents of hyperlinks pointing to the web page.
Another example in which multiple views arise is in multi-modal settings, for
example, simultaneous audio and video streams.
The idea is to construct
separate classiﬁers for each view, and to have the classiﬁers teach each other
by labeling instances where they are able. If the two views are independently
suﬃcient to learn the target function, and they are not merely redundant,
then each can learn from the other.
This suggests the following algorithm (see ﬁgure 2.8).
Start with some
labeled data. Train one classiﬁer using the ﬁrst view, and a separate classiﬁer
using the second view. For classiﬁer training, Blum & Mitchell use the Naive
Bayes algorithm, which we will discuss in section 4.1.1, but any classiﬁer
learning algorithm can be used. Use the two classiﬁers to label some new data.
For each, keep the labeled instances where it is most conﬁdent, and add them
to the labeled data. Iterate until a stopping criterion is met; the discussion
of stopping criteria in section 2.2.2 is applicable without modiﬁcation.
There are strong similarities between co-training (ﬁgure 2.8) and self-training
(ﬁgure 2.4). If we view the classiﬁer pair (f1, f2) as being itself a classiﬁer with
interesting internal structure, then co-training can be seen as a special case
of self-training. The version shown in ﬁgure 2.8 is persistent (line 6 has “L”
rather than “L0” after the assignment operator) and uses preselection (lines 2
and 7), but those are variations that we already discussed in connection with
self-training. The other dimensions of variation discussed for self-training can
also be applied to co-training.
This is not to minimize the importance of
the assumption of two views; the introduction of two views is a signiﬁcant
modiﬁcation, and plays a central role in the theory of co-training.

30
Semisupervised Learning for Computational Linguistics
Co-training will be discussed in more detail later in the book:
• Label propagation. Like self-training, co-training can be formulated
as label propagation. This was noted in the original Blum & Mitchell
paper [21], and will be discussed in section 7.6.3.
• Agreement-based methods, of which co-training is the prime exam-
ple, are discussed in chapter 9.
• Co-boosting. An agreement component can be added to the objective
function for boosting, a boundary-oriented method. The resulting “co-
boosting” method is discussed in section 6.3.3.
We introduce the co-training algorithm at this point because it is widely
used, along with self-training, in current computational-linguistic applications
of semisupervised learning. These applications are the topic of the next chap-
ter.

3
Applications of Self-Training and
Co-Training
Currently, most applications of semisupervised learning to problems in com-
putational linguistics have involved self-training and co-training.
For that
reason, it is natural to discuss the applications at this point.
It is hoped
that the presentation of less familiar, but often mathematically sophisticated,
methods in the remainder of the book will lead to their wider application in
computational linguistics in the future.
3.1
Part-of-speech tagging
Part-of-speech tagging has already been introduced. Instances are word occur-
rences in running text. An instance is represented as a collection of features.
For part-of-speech tagging, the preceding word and following word are ad-
equate for good performance. The classes are parts of speech: noun, verb,
adjective, and so on. There are a number of diﬀerent sets of parts of speech
(“tagsets”) in use. The most widely used is the Penn tagset, which distin-
guishes 36 parts of speech, or 48 if one includes punctuation tags [142].
Part-of-speech tagging, as just characterized, can be viewed as a straightfor-
ward classiﬁcation task. A complication arises, however, in that classiﬁcation
decisions for diﬀerent instances are not independent. There are sequential
dependencies among parts of speech assigned within a sentence. For this rea-
son, part-of-speech tagging is not usually approached as a simple classiﬁcation
problem; rather generative models such as Hidden Markov Models (HMMs)
or Conditional Random Fields (CRFs) are used to select an entire sequence
of parts of speech at once. Generative models will be discussed in chapter 8,
but explicitly sequential models such as HMMs and CRFs will not be covered.
The reader is referred to Manning & Sch¨utze [141] for a treatment of HMMs.
As previously mentioned, part-of-speech tagging was the ﬁrst task in com-
putational linguistics to which semisupervised techniques were applied. (It
was also the ﬁrst task to which probabilistic techniques in general were ap-
plied.) The Expectation-Maximization (EM) algorithm was well established
as a standard means of estimating an HMM from unlabeled data even before
31

32
Semisupervised Learning for Computational Linguistics
HMMs were applied to part of speech tagging. When HMMs were applied
to part-of-speech tagging, labeled data (namely, the Brown corpus) was used,
and impressively good results were obtained.
It was a natural next step to use the EM algorithm for unsupervised learning
of a part-of-speech tagger, but the results were poor. EM applied to unlabeled
data is essentially a clustering algorithm, and the word clusters induced had
little correspondence to the target parts of speech.
The idea of applying EM to a combination of labeled data and unlabeled
data was the ﬁrst application of semisupervised learning in computational
linguistics. The results were again disappointing. In a well-known pair of
studies, Merialdo [153] and Elworthy [83] trained HMMs from labeled data,
and used the trained models as initial models for EM estimation using un-
labeled data. The EM algorithm is an iterative algorithm that constructs a
sequence of models with increasing likelihood; likelihood measures how well
the model predicts the training data. The algorithm requires some model as
a starting point, and the quality of the model it ends with can be strongly
inﬂuenced by the choice of starting point. (The algorithm will be discussed
in more detail in section 8.2.) Merialdo and Elworthy used models trained on
labeled data as starting points.
The conclusions of Merialdo’s and Elworthy’s experiments are that incor-
porating unlabeled data using the EM algorithm actually hurts tagger perfor-
mance. An exception is when only very small amounts of training data are
available. To be precise, unlabeled data helped when the supervised model
was trained on 100 sentences (about 2370 words), but did not help when the
model was training on 2000 sentences (47,400 words) or more. It should be
noted that 2370 training instances is actually a moderately large training set
in many contexts. Be that as it may, the degradation one observes with EM
training when larger amounts of labeled data are available is consistent with
the poor performance of EM on purely unlabeled data. It is also consistent
with theoretical results that show that the use of unlabeled data can hurt
when the assumptions of the model are incorrect [53, 54].
The negative results of Merialdo and Elworthy also contrast with successful
attempts at semisupervised learning by Cutting et al. [62] and Kupiec [131].
These latter approaches used a dictionary giving the possible parts of speech
for each word, but not their probabilities. A signiﬁcant proportion of words
in English are unambiguous, so a dictionary implies a labeling for part of the
data. For example, in the Penn Treebank, 85% of words are unambiguous.
(This includes a large number of words that only appear once in the corpus.)
Moreover, the average number of parts of speech that an ambiguous word has
is small. In the Penn Treebank, an ambiguous word has on average 2.24 parts
of speech out of the 36 parts of speech in the tagset. A dictionary simpliﬁes
the learning problem by greatly reducing the number of labels available for
each instance. It can be seen as providing a partial labeling in two senses: it
assigns a label to a signiﬁcant part of the data, and it assigns “partial labels”
to the remaining instances, where a “partial label” is a restricted set of labels

Applications of Self-Training and Co-Training
33
Y . An unlabeled instance is one whose label is restricted to L, the full set of
labels. A labeled instance is one whose label is restricted to {y}, a singleton
set. A partially labeled instance is one whose label is restricted to a set Y
that is neither a singleton set nor the full set of labels. (Recall the discussion
of constraints as seeds for self-training in section 2.2.2.)
Cutting et al. and Kupiec used the EM algorithm for semisupervised learn-
ing of a part-of-speech tagger. Subsequently, other forms of semisupervised
learning have been applied.
For example, Brill [25] uses a variant of self-
training, on the basis of a supervised algorithm known as transformation-
based learning. A dictionary is assumed that provides an initial partial label-
ing. Rules are considered of the form f : Y ⇒y where f is a feature, Y is a
partial label (that is, a set of labels), and y ∈Y is a label. The rule matches
any instance with feature f, and it predicts label y. However, the rule only
applies to a partially labeled instance that has feature f and partial label
Y . The algorithm is iterative. In each iteration, rules are scored according
to their conditional accuracy on the (unambiguously) labeled instances that
they match, and the best rule is added to the classiﬁer. The classiﬁer is an
ordered list of rules; new rules are added at the end of the list. The classi-
ﬁer is then applied to the partially labeled instances, labeling some of them.
Then the process is repeated, selecting a new rule and applying it to the par-
tially labeled instances. The resulting algorithm is a version of self-training
in which both the classiﬁer and labels are persistent, and in which only one
rule is selected in each iteration. The use of partial labels is novel, however.
3.2
Information extraction
The aim in information extraction is to populate a database from unstructured
text. The typical approach involves at least four steps: entity recognition,
relation recognition, coreference resolution, and slot ﬁlling. Of these, entity
recognition and relation recognition have received the most attention with
respect to semisupervised learning.
Information extraction typically involves a small number of types of seman-
tic entities and relations, corresponding to the schema of the database to be
populated. The classic example is the “acquisitions and mergers” domain of
the Message Understanding Conference (MUC), which assumes entity types
such as person, place, and company, and relations such as person is-CEO-of
company, company acquires company, etc.
Entity recognition is the identiﬁcation of phrases (usually noun phrases)
referring to semantic entities. It is often divided into two steps, segmentation
and classiﬁcation. In segmentation, one determines the boundaries of likely
entity occurrences, and in classiﬁcation, one determines the semantic type of

34
Semisupervised Learning for Computational Linguistics
the occurrences. Segmentation is typically reduced to a tagging problem. In
the simplest form, one tags the sentence with three tags, B, I, and O, where
“B” marks the ﬁrst word in an entity reference, “I” marks a word inside an
entity occurrence that is not the ﬁrst, and “O” marks words that are outside
of any entity occurrence. For example, the sequence
On
March
4,
1992,
Ronald
Arbiter
founded
Arbutus
O
B
I
I
B
I
O
B
is an encoding for the entity segmentation
On [March 4, 1992,] [Ronald Arbiter] founded [Arbutus].
Hence the application of semisupervised learning methods to entity segmenta-
tion largely reduces to their application to the part-of-speech tagging problem.
Entity classiﬁcation is the assignment of semantic types to segmented en-
tities. For instance, in the example just given, it remains to be determined
that the three entities have types date, person, and company, respectively.
This is a straightforward classiﬁcation problem. The instances are segmented
entity references. Features are often divided into “spelling” features, mean-
ing features derived from the words inside the entity reference, and “context”
features, meaning features derived from the surrounding context of the entity
reference. Spelling features and context features can be used as two views for
co-training. The labels for the classiﬁcation problem are the semantic entity
types, plus “none of the above.” Collins & Singer [50] show the eﬀectiveness of
both self-training and co-training at this task, and they also develop a method
called co-boosting, which will be discussed later (section 6.3.3).
A third information-extraction subproblem to which semisupervised learn-
ing has been applied is relation recognition. An early example here is a method
proposed by Brin [26] for ﬁlling a database of book titles and their authors.
The idea, in a word, is to begin with a small list of author-title pairs, ﬁnd web
pages in which the known pairs occur, extract features that are indicative of
author-title pairs, and use those features to ﬁnd new pairs. Then the process
repeats. This can be seen again as a variety of self-training, in which the
instances are potential author-title pairs, the labels are positive and negative,
and “features indicative of author-title pairs” means rules predicting the pos-
itive class. One begins with a small number of (positively) labeled instances,
uses them to construct a classiﬁer (a list of features indicative of positive
instances), then one uses the classiﬁer to label new instances, and the cycle
repeats. Distinctive properties of this particular application of self-training
are the very large space of instances, and the focus on the positive class –
there are no negatively labeled instances and no rules predicting the negative
class. These two properties are actually related. The very large space of in-
stances means that almost all instances are negative, so “unlabeled” is almost
equivalent to “negative.” Only exceptional (positive) instances are explicitly
marked.

Applications of Self-Training and Co-Training
35
A ﬁnal aspect of information extraction to which semisupervised learning
has been applied is the induction of lexical resources, particularly gazetteers,
or lists of known examples of particular semantic types. This is discussed
below, under taxonomic inference.
3.3
Parsing
Beyond information extraction, in complexity and generality of structures
detected, is full parsing. There has been some pioneering work on applying
semisupervised learning to parsing. A specialization of the EM algorithm for
parsing, called the Inside-Outside algorithm, was developed early on, but the
structures it induces have little resemblance to standard parse trees. Pereira
& Schabes [180] were more successful by beginning with partial bracketings.
Sarkar [202] has applied co-training to parsing. Parsing is a true structure-
building task; unlike part-of-speech tagging, one cannot reduce parsing even
approximately to independent classiﬁcation tasks for each word. Parsing is
the assignment of a tree to an entire sentence. Unlike labels in a classiﬁcation
task, the number of trees is inﬁnite, and trees have rich internal structure that
is essential to the task.
A typical stochastic parsing model consists of three things: a representation
scheme, a scoring function, and a search algorithm. The representation scheme
deﬁnes the way that a parse tree is decomposed into small pieces of structure
that play the role of features. The scoring function assigns scores to structure-
pieces based on their frequency in a labeled training set.
“Labeled” here
means that the correct parse tree is given for each sentence. “Training” a
model means computing the statistics to which the scoring function refers.
The score for a complete parse tree is the sum of scores of the structure-pieces
that make it up. The job of the search algorithm is to assemble structure-
pieces in such a way as to arrive at the parse tree with the maximal score, or
close to the maximal score, among all valid parse trees for the given sentence.
Instead of constructing two classiﬁers based on separate views of instances,
Sarkar constructs two diﬀerent stochastic parsers, based on two diﬀerent mod-
els. Each has its own distinct representation scheme, scoring function, and
search algorithm.
The process begins with a seed set of labeled data, as
well as a dictionary of allowable structure-pieces for individual words, along
the lines of the tag dictionary used in semisupervised part-of-speech tagging.
Each model is trained separately on the labeled data, and then each model
is applied to a randomly selected pool of sentences. Applying the model to
a sentence yields not only a best parse, but also a score for that parse. The
score is interpreted like a conﬁdence measure. Where the model is most conﬁ-
dent, its predicted parse is taken at face value and added to the labeled data.

36
Semisupervised Learning for Computational Linguistics
This is done for each model separately. Then the models are retrained on the
extended labeled set, the pool is replenished, and the process is repeated. The
algorithm is conceived as an instance of co-training, using stochastic parsing
models in the place of classiﬁers, sentences in place of instances, and entire
parse trees in place of labels.
3.4
Word senses
We begin with a brief description of what has become the standard word-sense
taxonomy for English, WordNet. Then we discuss word sense disambiguation
and taxonomic inference.
3.4.1
WordNet
A taxonomy, or “is-a” hierarchy, is an acyclic directed graph whose nodes are
concepts and whose edges represent specialization. The ﬁrst large taxonomic
lexical database was WordNet, and it is by far the most widely used taxonomy
in computational linguistics [87].
It is nearly a tree: nodes with multiple
parents do exist, but they are rare.
A signiﬁcant problem for semantic relations is identiﬁcation of the basic
units. A multitude of eﬀorts to identify “concepts,” much less the relations
among them, have disappeared into a swamp of subjective judgments. Word-
Net obtains traction by focussing on relationships among words. Instead of
attempting to deﬁne concepts in the abstract, WordNet equates concepts with
word senses. WordNet takes the stance that every concept is lexicalizable, or
at least it limits its attention to lexicalized concepts, which is to say, concepts
that are expressible as a word or compound word.
The relation between words and concepts is many-many.
A given word
may have more than one sense, and a given concept may be expressible as
more than one word. Rather than using arbitrary designators for concepts,
such as “PLANE-1” versus “PLANE-2,” WordNet observes that two words are
synonymous just in case they share a sense, implying that the synonyms of a
word vary according to the sense of the word that is of interest. Hence a word
sense can be identiﬁed by giving the list of synonyms that share the sense.
Instead of “PLANE-1” and “PLANE-2,” we may use “airplane/aeroplane/plane”
for the ﬂying machine and “plane/sheet” for the mathematical plane.
In
WordNet terms, these are synsets. For WordNet, the terms “synset,” “word
sense,” and “concept” are all equivalent. Both examples just given are senses
of the word “plane.” The synset “airplane/aeroplane/plane” is a shared sense
of the words “airplane,” “aeroplane,” and “plane,” and “plane/sheet” is a
shared sense of the words “plane” and “sheet.” The word “sheet” also has

Applications of Self-Training and Co-Training
37
the sense “sheet/bed sheet,” which is not shared with “plane.”
Ideally, no two synsets should be identical as sets of words, though this
ideal is not maintained in practice. There do exist pairs of synsets that contain
exactly the same words, but are nonetheless distinct; the diﬀerence is reﬂected
in a gloss giving a more detailed description of the sense.
An important basic problem in computational linguistics is word sense
disambiguation: the problem of determining the applicable word sense for a
given word in context. It is also one of the problems for which self-training
was originally developed.
WordNet treats taxonomy as a relation among synsets. The WordNet term
is hypernymy, and its inverse is hyponymy:
• A hypernym is a generic word, naming a class or natural kind. X is
a hypernym of Y if X is a generalization of Y. For example, dog is a
hypernym of golden retriever.
• A hyponym is a speciﬁc term, naming a member or specialization of a
natural kind. Hyponymy is the lexical relationship corresponding to the
“is-a” relation. For example, golden retriever is a hyponym of dog.
Before WordNet, a few thesauri were available, most notably Roget’s [197],
the 1911 edition of which is out of copyright and has been available for many
years in electronic form. There was also considerable research on extracting
information, including taxonomic information, from machine-readable dic-
tionaries.
Interest in processing machine-readable dictionaries waned with
the availability of WordNet and other large, manually redacted electronic re-
sources, but one of the issues that it raised is as relevant as ever, namely, how
to supplement dictionary information from text.
Linguistics has long distinguished between synchronic and diachronic ac-
counts of language. Outside of the subdiscipline of historical linguistics, a
strictly synchronic point of view – an idealization to a static linguistic system
at a single point in time – is widely adopted. The synchronic idealization is
so pervasive in linguistics that it becomes natural to consider any deviations
(for example, coinages and borrowings) to be negligible if one limits attention
to language within a relatively small timeframe, say, a single generation.
But the new prominence of corpora in computational linguistics has led, es-
sentially, to an abandonment of the idealization to synchronicity. It is now a
truism that the lexicon is dynamic. Each new text brings new vocabulary. For
example, a fundamental issue in speech recognition is the out-of-vocabulary
(OOV) rate.
A fundamental issue in any language processing step is how
to deal with unknown words. One now takes as given that a non-negligible
percent of the words in a test set will never have been encountered in train-
ing. In addition to proper nouns, coinages, borrowings, and rare words that
simply had not been previously encountered, each technical area has its own
vocabulary, and technical vocabularies change rapidly.

38
Semisupervised Learning for Computational Linguistics
used troops and tanks to quash a pro democracy
weapon
tools and equipment, pumps, hoses, water tanks
container
an enlisted man in the royal air force and tank corps
weapon
case of fighter aircraft the engines and fuel tanks
container
FIGURE 3.1
Four labeled instances in word-sense disambiguation.
These considerations lead to a second important problem, taxonomic in-
ference. Taxonomic inference in the narrow sense is the question of how to
place the new terms one encounters into an existing taxonomy. A more funda-
mental question is how to induce the entire hierarchy, such as for entirely new
domains or new languages. Taxonomic inference was also an area to which
semisupervised learning techniques were applied early on, and it continues to
be an area of active research.
3.4.2
Word-sense disambiguation
Word-sense disambiguation is the problem of choosing the contextually ap-
propriate sense for words in text. It can be viewed as a classiﬁcation problem
in which instances are word tokens, and labels are word senses. However, it
is a classiﬁcation problem with special structure. The list of word senses is
very long – for example, WordNet 2.1 contains more than 100,000 word senses
(synsets). But for any given word, the eﬀective size of the label set is much
smaller. A large majority (83%) of word types are unambiguous. Limiting
attention to ambiguous words, there are fewer than four senses on average
for verbs, and fewer than three senses on average for nouns, adjectives, and
adverbs [234].
Word-sense disambiguation was one of the ﬁrst tasks to which self-training
was applied. Typically, an instance corresponds to a particular occurrence
of an ambiguous word in context, and the classes are the diﬀerent senses of
the word. Essentially, each ambiguous word represents a separate learning
problem. For example, Hearst [109] uses the word tank to deﬁne one learning
problem. Two classes are the “container” sense and “weapon” sense of the
word, and four labeled instances are given in ﬁgure 3.1. As usual, instances are
represented as sets of features or, equivalently, as vectors of attribute values.
The features most commonly used are represented by words in the proximity
of the target word, with various deﬁnitions of proximity. In some cases, parse
information is used, for example, the identity of the head that governs the
target.
Identifying the classes, which is to say, deciding how many senses a word
has and how to (manually) distinguish among them, is often diﬃcult. For
English, the easiest course is to adopt the WordNet senses, though they do
tend to be relatively ﬁne-grained, and hence pose a diﬃcult discrimination

Applications of Self-Training and Co-Training
39
task. Traditional print dictionaries, particularly ones available in machine-
readable form, have also been used in the past, though they are generally less
convenient than WordNet.
A disadvantage of WordNet, or any dictionary, is the degree of subjectivity
in the discrimination of word senses. This is reﬂected in the variation among
dictionaries in their decisions regarding word senses.
An alternative that ameliorates this particular shortcoming is to deﬁne word
senses by translations. Given an English text and its translation into another
language, one can deﬁne the sense of a given word occurrence to be its trans-
lation in the other language. Diﬀerent occurrences of a word typically are
translated in diﬀerent ways, corresponding, at least to a ﬁrst approximation,
to the diﬀerent senses of the word. For example, interest can be translated
into German either as Interesse (intellectual interest) or as Zinsen (interest
paid on an account balance). This way of deﬁning word senses does have the
advantage of circumventing the need for manual annotation, when a transla-
tion is available, but it does not actually eliminate the element of subjectivity
– the translator faces a subjective choice among possible translations. It is also
less than deﬁnitive in that the choice of target language can have a signiﬁcant
eﬀect on the way that word occurrences are divided into “senses.”
The word-sense disambiguation task makes some special constraints avail-
able that can be used to improve learning. It has been observed [91] that a
given word occurring multiple times in a single discourse (for example, in the
same newspaper article) is almost always translated the same way each time it
occurs, even though multiple translations are possible in principle. Yarowsky
incorporated this constraint into self-training for word-sense disambiguation.
An important empirical question is whether semisupervised learning is eﬀec-
tive, that is, whether it aﬀords improvements over using just the labeled data.
Although Yarowsky [239] and Collins & Singer [50] report that semisupervised
learning gives substantial improvement over the performance of the seed clas-
siﬁer they use, they intentionally use a very weak seed classiﬁer.
Putting
even moderate eﬀort into supervised classiﬁcation can make the comparison
less clear-cut. For example, Mihalcea [154] does an empirical comparison of
self-training and co-training on word sense disambiguation.
She examines
variation in performance with respect to a few parameters: the number of
iterations of self- or co-training, the pool size used in select, and the num-
ber of instances selected in each iteration. If the ideal parameter settings are
known in advance, good performance gains (25% reduction in error rate) are
achievable. However, if one is unlucky in guessing the appropriate parameter
settings, the use of unlabeled data hurts performance.
Another open question is whether one can do better than treating each
ambiguous word as an independent learning task.
Mihalcea observes that
choosing a single set of parameter settings to optimize performance for all
words gives better average performance than optimizing for each word sepa-
rately. One would like to be able to transfer a good deal more information
across the tasks.

40
Semisupervised Learning for Computational Linguistics
3.4.3
Taxonomic inference
A more ambitious task than word-sense disambiguation is the automatic con-
struction of a word-sense hierarchy. A task intermediate between word-sense
disambiguation and hierarchy construction is the identiﬁcation of the appro-
priate parent node for a new concept in an existing, but incomplete, hier-
archy. The construction of a taxonomy is usually based on the detection of
hyponomous word pairs (Y is an X), which correspond to child-parent links
in the taxonomy, and similarly distributed word pairs, which may correspond
to siblings in the taxonomy – what are called coordinates in WordNet. Con-
structing a taxonomy from scratch is most naturally formulated as a clustering
problem, but the more limited problem of extending a taxonomy is naturally
formulated as a classiﬁcation problem. This is particularly true in limited
domains, such as are common in information extraction. For example, one
might address the limited question of classifying nouns as subtypes of person,
place, organization, or none of the above.
Unlike word-sense disambiguation, taxonomy induction is not generally con-
cerned with word tokens, that is, individual occurrences of words or phrases,
but with word types. By classifying president as person, one is making a
statement about the meaning of the word president in general, not about any
particular occurrence of the word in text. To be precise, one is asserting that
one of the senses of president is a subtype of person. Usually, then, one takes
instances to be word types, not word tokens.
The features of a word type are constructed from features of word tokens.
Features of word tokens are typically instantiations of templates of various
types. For example, “previous word” and “following word” are examples of
templates in the intended sense. Applied to a token in context, they yield
concrete features, such as “previousWord=powerful.” Features of types are
usually created by aggregating token features. One natural approach is to view
a token feature as a type attribute whose value is a count or relative frequency.
Continuing with our example, the type explosion might have the value 4 for
the attribute previousWord=powerful, meaning that there are four occur-
rences of explosion that are immediately preceded by the word “powerful,” or
it might have the value 0.002, meaning that 0.2% of its occurrences are imme-
diately preceded by the word “powerful.” Real-valued attributes of this sort
are natural for clustering, but are less immediately useful for some classiﬁca-
tion algorithms, such as decision lists. The commonest expedient is to impose
a threshold: explosion is deﬁned to have the feature previousWord=powerful
if there are at least n occurrences of explosion immediately preceded by pow-
erful.
In addition to features that are generally useful for constructing a classiﬁer,
Hearst [108] proposed several templates for predicting both hyponymy and
coordinacy that continue to be widely used. Here are the patterns that she
proposes. The variables match noun phrases.
• x ,? such as y1 (, y2)∗(,? (and|or) y3)? →yi isa x

Applications of Self-Training and Co-Training
41
• such x as y1 (, y2)∗(,? (and|or) y3)? →yi isa x
• y1 (, y2)∗,? (and|or) other x →yi isa x
• x ,? including y1 (, y2)∗(,? (and|or) y3)? →yi isa x
• x ,? especially y1 (, y2)∗(,? (and|or) y3)? →yi isa x
If an instantiation of one of the templates occurs in text, she postulates, then
each pair (yi, x) is an instance of hyponomy, and each pair (yi, yj) is a sibling
pair.
When the classiﬁcation task involves identiﬁcation of the hypernym (that
is, parent) of an instance, the Hearst templates can be interpreted not just
yielding features when instantiated, but as yielding labeled instances. In this
way, they play the role of a seed classiﬁer. An instantiation of one of the
templates is obtained by replacing the variables with particular noun phrases,
for example:
siege engines, such as the ballista and onager
Assuming that siege engine is one of the allowable hypernyms, this yields
two labeled instances:
Instance
Label
ballista
siege engine
onager
siege engine
The Hearst templates can also be used simply to generate features, the
attributes being likely hypernyms and likely coordinates. For example:
Instance
Features
ballista
likelyHypernym=siege engine, likelyCoordinate=onager
onager
likelyHypernym=siege engine, likelyCoordinate=ballista
With word types as instances, and features of the sort deﬁned, self-training
can be, and often has been, applied to the task of extending a taxonomy.
Self-training is most often applied to a task with a small number of candidate
hypernyms serving as labels; an additional label “other” representing “none
of the above” is also added. As already mentioned, a task of this sort arises
particularly in the context of information extraction, where it is often couched
in terms of ﬂeshing out a gazetteer. A gazetteer is a collection of lists, each list
representing members (that is hyponyms) of a given semantic class. Finding
list members is equivalent to labeling instances with one of the classes.
Typically, one begins with a small number of examples of each class, which
means a small number of labeled instances. One constructs a variant of a
decision list: one identiﬁes rules F ⇒y such that feature F occurs frequently
with instances that share label y; the classiﬁer consists of a list of such rules.

42
Semisupervised Learning for Computational Linguistics
To be sure, the process is usually couched in terms of collecting a list of “pat-
terns” for each class, but a “pattern” is simply a template instantiation, which
is to say, a feature, and associating a feature F with a class y is equivalent to
adding rule F ⇒y to the classiﬁer.
Approaches along these general lines have been proposed by Riloﬀ& Shep-
herd [192, 193]. Roark & Charniak [195] propose a reﬁnement of Riloﬀ&
Shepherd’s approach, using Charniak’s parser to identify lists, conjunctions,
and appositives, and considering each pair of terms in such constructions to
represent likely coordinate pairs. Other work along these lines includes Riloﬀ
& Jones [194], Widdows & Dorrow [231], Thelen & Riloﬀ[219], and Phillips
& Riloﬀ[182].

4
Classiﬁcation
In the previous chapters we have considered the semisupervised learning algo-
rithms that are most familiar in computational linguistics. In the rest of the
book, we turn to methods from the machine learning literature. We do not as-
sume that the reader’s primary area of research is machine learning, nor that
the reader is necessarily familiar with all the mathematical techniques that
are taken for granted in the primary literature of machine learning. For that
reason, we include chapters that do not treat semisupervised learning per
se, but provide necessary background material, beginning with the present
chapter, on classiﬁcation.
4.1
Two simple classiﬁers
We begin with some concrete examples of classiﬁers, and then turn to the
more abstract setting.
4.1.1
Naive Bayes
The Naive Bayes algorithm is a particularly simple classiﬁer; it was mentioned
earlier as the classiﬁer used by Blum & Mitchell in co-training. For the purpose
of the Naive Bayes algorithm, an instance is viewed as a vector of attribute
values: x = (x1, . . . , xm). By the chain rule of conditional probability, the
probability p(x, y) of an instance x with label y can be expressed as
p(x, y) = p(y)p(x1|y) . . . p(xm|y, x1, . . . , xm−1).
The naivet´e of the Naive Bayes algorithm rests in an independence assumption
it makes, namely, that the choice of value for any attribute is conditionally
independent of the choice of value for any other attribute, given the label.
That is
p(x, y) = p(y)
Y
j
p(xj|y).
Though almost certainly false, the assumption greatly simpliﬁes computation,
and as a practical matter, the algorithm performs respectably well in a wide
range of learning problems.
43

44
Semisupervised Learning for Computational Linguistics
Len
Wid
Label
Len
Wid
Label
small
small
−
small
large
+
small
med
−
med
large
+
small
med
−
med
large
+
med
small
−
large
small
+
large
small
−
large
large
+
small
small
−
large
med
+
small
large
−
small
med
−
FIGURE 4.1
Example of training data for Naive Bayes.
The algorithm estimates p(y) and p(xj|y) by relative frequency in the train-
ing data. Given a new instance x, the prediction of the classiﬁer is the label
that maximizes the conditional probability
p(y|x) = p(x, y)
p(x) .
Since the denominator is constant across choices of label, we can ignore it.
Hence the prediction of the classiﬁer is
arg max
y
p(y)
Y
j
p(xj|y).
For example, suppose we are given the training data shown in ﬁgure 4.1. We
have already sorted it into negative instances and positive instances. Counting
the positives and negatives gives the following estimates for p(y):
y p(y)
−8/14
+ 6/14
.
(4.1)
Counting the relative frequencies of each attribute value within the positive
and negative groups gives the estimates for p(v|y), where v is a value for a
given attribute. Note that the expression p(v|y) is ambiguous because it leaves
the attribute implicit; it can be written more explicitly as either Pr[Len =
v|Y = y] or Pr[Wid = v|Y = y], depending on which attribute is intended.
y = −
y = +
Len
Wid
Len
Wid
v
p(v|y) p(v|y) p(v|y) p(v|y)
small
6/8
4/8
1/6
1/6
med
1/8
3/8
2/6
1/6
large
1/8
1/8
3/6
4/6
(4.2)

Classiﬁcation
45
Suppose that we are given a new input: (med, med). Given the parameter
values in the two tables above, we have:
p(x, −) = p(−) Pr[Len = med|Y = −] Pr[Wid = med|Y = −]
= (8/14)(1/8)(3/8)
= .027
p(x, +) = p(+) Pr[Len = med|Y = +] Pr[Wid = med|Y = +]
= (6/14)(2/6)(1/6)
= .024.
Accordingly, the classiﬁer predicts negative, since p(x, −) > p(x, +).
4.1.2
k-nearest-neighbor classiﬁer
Another very simple classiﬁer, but one that is in a certain sense at the other
end of the spectrum from Naive Bayes, is the k-nearest-neighbor classiﬁer.
The k-nearest-neighbor learner is essentially trivial: it simply memorizes the
training data. All the work is done by the classiﬁer at “run time.” Given a
new instance x to be classiﬁed, the classiﬁer ﬁnds the k training examples that
are most similar to x, and looks at their labels. Whichever label occurs most
frequently among the k nearest neighbors is chosen as the predicted label for
x.
To specify the algorithm fully, one must choose a value for k and a measure
of similarity between instances. There is a simple geometric interpretation
of the k-nearest-neighbor classiﬁer in the case when k = 1 (the 1-nearest-
neighbor classiﬁer) and similarity is Euclidean distance. To see it, we must
think about instances as points in feature space, which is to say, the space
whose axes are deﬁned by the attributes. We represent instances as vectors
of values, and view them as points in an m-dimensional space, where m is the
number of attributes, hence the number of components in the vectors.
For example, consider the following little training set.
Length Width Label
10
7
+
13
8
+
17
8
+
12
5
−
15
6
−
In ﬁgure 4.2, the instances are plotted as points in a space whose axes are
Length and Width.
In the ﬁgure, we have also marked the boundaries of
the decision regions. If we consider a new point x anywhere in the space, its
(predicted) label is determined by the nearest training point. Consider any

46
Semisupervised Learning for Computational Linguistics
5
10
10
15
Len
Wid
FIGURE 4.2
Instances as points in feature space.
two training points, and an arbitrary point x in their vicinity (ﬁgure 4.3).
By deﬁnition, x “goes with” (its label is determined by) the nearer of the
two training points. Clearly, the boundary between the two regions is the
perpendicular bisector of the line connecting the two training points.
Each training point P is surrounded by a region consisting of the points
closer to P than to any other training point. The boundary of the region is
piecewise linear, consisting of perpendicular bisectors between P and nearby
training points. The result is a tiling of the feature space known as a Voronoi
tesselation, shown in ﬁgure 4.4.
The decision boundary between the positive
and negative examples is the reinforced line.
The nearest-neighbor algorithm usually produces a complicated, zig-zagging
decision boundary. As long as any duplicate examples in the training data
are consistently labeled, the 1-nearest-neighbor algorithm obviously makes the
correct prediction for every training example.
In general, there is a trade-oﬀbetween the complexity of a model and ﬁt to
the training data. The nearest-neighbor algorithm represents one extreme in
that trade-oﬀ: it produces very complicated models, since it does not “digest”
the training data at all, but it gets very good ﬁt to the data. The apparent
goodness of the predictions can be deceptive, however.
By modeling the
training data so closely, the nearest-neighbor algorithm captures not only
the “signal” in the data, but also the “noise.” The Naive Bayes algorithm is
closer to the other end of the spectrum. The “naive” conditional independence
the decision boundary in the case of Naive Bayes is a single straight line, no
matter how many training points there are. As a consequence, though, the
classiﬁer may be unable to make the correct prediction for all the training
points: there may be positives mixed among the negatives or vice versa, so
that no straight line is able to separate them.
assumption permits a simple model, and, as we will see below, in section 5.1.4,

Classiﬁcation
47
A
B
x
FIGURE 4.3
A point x in the vicinity of two training points A and B. The points equidis-
tant from A and B constitute the perpendicular bisector of the line segment
connecting A and B.
5
10
10
15
Len
Wid
FIGURE 4.4
The decision regions; they constitute a Voronoi tesselation of the space.

48
Semisupervised Learning for Computational Linguistics
Ideally, we would like a learning algorithm that captures all of the general
features of the training data (the signal) without being distracted by its quirks
(the noise). The “general features” are those that generalize to test data, and
the quirks are the accidents of the training data that do not generalize. To
capture every detail of the training data, one requires a complicated model.
Simple models cannot capture all the details, but one hopes that one captures
the features that generalize, leaving out only the quirks. If one chooses too
simple a model, however, one loses essential detail, and generalization again
suﬀers. A model that captures too much detail is said to overﬁt the training
data, and a model that is too simple, and captures too little detail, is said to
underﬁt. The goal is to ﬁnd the model that, in the words of Goldilocks, is
“just right.”
4.2
Abstract setting
With these concrete examples in mind, let us deﬁne classiﬁcation, and the
learner’s goal, more abstractly.
4.2.1
Function approximation
As was mentioned earlier (section 1.2.1), classiﬁcation is one example of func-
tion approximation, three other examples being clustering, regression, and
density estimation. In classiﬁcation and clustering, the function to be learned,
or target function, is nominal-valued, and in regression and density estima-
tion, it is real-valued. In all cases, the target function takes instances or data
points or sample points (the three terms are synonymous) as inputs. Instances
are drawn from a sample space Ωand are distributed according to a ﬁxed but
unknown probability distribution D. If Ωis a discrete space, then D is a
probability mass function, otherwise it is a probability density function. A
random variable is by deﬁnition any function whose domain is the sample
space Ω, so the functions to be learned are, formally, random variables.
The learner is given a training sample (x1, x2, . . . , xn) of instances drawn
from the population in accordance with distribution D. The instances are
assumed to be drawn independently, so that the probability of the sample is
n
Y
i=1
D(xi).
That is, the training sample is i.i.d.: the individual data points are independent
and identically distributed.
As a practical matter, the assumption of independence is often violated.
A typical training set for a natural language problem is constructed from

Classiﬁcation
49
running texts, with the consequence that there are signiﬁcant statistical de-
pendencies among instances. Little analytic traction can be gained, however,
unless one assumes independently drawn instances. Fortunately, violating the
independence assumption does not usually render the methods ineﬀective for
practical purposes.
What has been said so far applies to all function approximation methods. It
suﬃces for density estimation. The goal in density estimation is to use the (ﬁ-
nite) training sample to construct an estimate ˆD that is a good approximation
to D over the entire (possibly inﬁnite) population of instances.
The remaining function approximation settings diﬀer from density estima-
tion in that they involve a second variable – the label – in addition to the
instance x. Conventionally, we write “y” for the label. Unlabeled data is
a list of instances (x1, x2, . . . , xn). In the supervised settings, classiﬁcation
and regression, the learner receives in addition a second list (y1, y2, . . . , yn)
containing labels. Equivalently, we may view labeled data as a list of pairs
((x1, y1), (x2, y2), . . . , (xn, yn)). Let us write Y for the target function, that
is, the function to be learned. The correct label for a given instance x is the
output y = Y (x). In particular, yi = Y (xi) for all pairs (xi, yi) in the labeled
training data. A central question for learning is how to predict the value of
Y for instances that are not in the training sample.
In classiﬁcation and regression, the learner is given labeled data as input,
and the goal is to construct an estimate or hypothesis ˆY that is a good
approximation to Y over the whole population of instances. The diﬀerence
between classiﬁcation and regression lies only in the kind of values that Y
produces as output. In the case of classiﬁcation, Y maps an instance to one
of a ﬁnite set of classes. In the case of regression, Y maps an instance to a
real number.
Clustering diﬀers from classiﬁcation with respect to supervision. In both
cases, there is a ﬁxed but unknown function Y that maps instances x to classes
y. The goal of clustering, as for classiﬁcation, is to construct an approximation
ˆY . However, what constitutes a good approximation diﬀers in the two cases.
For classiﬁcation, a good approximation ˆY is one for which
ˆY (x) = Y (x)
for most instances x in the population. For clustering, this requirement is too
stringent. Given only unlabeled training data, we cannot expect a clustering
algorithm to use the same labels y as the unknown function Y uses – it has
no way of knowing what those labels are. Rather, we require only that the
clustering function ˆY divide up the data into clusters in a way that corresponds
well to the true classes determined by Y .
Namely, the values y of the target function can be viewed as arbitrary labels
for classes. The class itself can be viewed as a set of instances:
{x ∈Ω|Y (x) = y}.
(4.3)

50
Semisupervised Learning for Computational Linguistics
In this way, Y deﬁnes a partitioning of the population of instances.
The
clustering function ˆY similarly deﬁnes a partitioning of the instances. Each
value z of ˆY represents an arbitrary label for a cluster. The cluster itself is a
set of instances:
{x ∈Ω| ˆY (x) = z}.
(4.4)
We emphasize that the cluster labels z are arbitrary designators, and need
not be related to the unknown labels y produced by the target function Y .
We require only that the cluster function partition the instances in (approx-
imately) the same way as the target function. The cluster function ˆY is a
good approximation to the target function Y if the clusters (4.4) correspond
well to the classes (4.3). This is the same as saying that a cluster function
is good if there exists a one-one correspondence µ mapping cluster labels to
class labels such that
µ( ˆY (x)) = Y (x)
for most instances x.
We note in passing that one often encounters characterizations of the goal
of clustering that are rather less stringent than ours. Clustering is often pre-
sented without the assumption of a target function determining “true classes.”
Rather, the goal may be characterized subjectively, as ﬁnding “interesting”
structure in the data, or indirectly, as ﬁnding clusters that are useful for some
other task. Clustering is certainly useful for exploratory data analysis or for
preprocessing, but our primary interest will be in settings in which a target
function does exist.
In this book, we restrict our attention almost exclusively to classiﬁcation
and clustering.
We are particularly interested in them as instances of a
more general function-approximation problem in which the target function
is discrete-valued, the task is to approximate it, and the learner receives a
mixture of labeled and unlabeled training data. We may call this general-
ized setting semisupervised classiﬁcation. Classiﬁcation is the special case
in which all the data is labeled, and clustering, when treated as unsupervised
classiﬁcation, is the special cases in which all the data is unlabeled.
4.2.2
Deﬁning success
A classiﬁer is a function ˆY (x) that takes an unlabeled instance as input and
produces a label prediction as output.
The performance of a classiﬁer is
measured by how well it approximates the target function Y that assigns the
correct label to each instance in the population. Speciﬁcally, the measure of
classiﬁer performance is generalization error, which is the probability that
the classiﬁer’s prediction diﬀers from the correct label:
ϵ =
X
x∈Ω
[[ ˆY (x) ̸= Y (x)]]D(x)
(4.5)

Classiﬁcation
51
where [[φ]] has value 1 if φ is true and 0 otherwise.
If the population of
instances is not a discrete space, the sum in (4.5) must be replaced with an
integral.
In the case of regression, Y and ˆY are real-valued functions, so the natural
way to compare them is diﬀerent. Demanding that a prediction be exactly
correct is overly stringent; it is suﬃcient that it be approximately correct.
The natural measure of error for a given instance is the absolute diﬀerence
|Y (x)−ˆY (x)|, though the squared diﬀerence [Y (x)−ˆY (x)]2 is generally more
convenient mathematically. The measure of overall discrepancy between ˆY
and Y is the average discrepancy:
ϵ =
X
x∈Ω
|Y (x) −ˆY (x)|D(x)
(4.6)
or
ϵ =
X
x∈Ω
[Y (x) −ˆY (x)]2D(x).
(4.7)
All three of these measures of error, (4.5), (4.6), (4.7), have the same general
form. At each data point x, there is a measure of the discrepancy between ˆY
and Y , measured variously as
L ˆY ,Y (x) = [[ ˆY (x) ̸= Y (x)]]
L ˆY ,Y (x) = ˆY (x) −Y (x)|
or
L ˆY ,Y (x) = [ ˆY (x) −Y (x)]2.
A measure L ˆY ,Y (x) of the discrepancy between ˆY and Y at each data point
x is called a loss function.
The overall measure of error is the expected loss over the sample space:
R( ˆY ) = D[L ˆY ,Y ]
where D[φ] is the expectation of the quantity φ under distribution D. The
expected loss R( ˆY ) is the risk of the hypothesis ˆY . The lower the risk, the
better the hypothesis. A learning algorithm is successful if it ﬁnds a hypothesis
that minimizes risk, or nearly so.
In the case of classiﬁcation, the natural loss function is that of (4.5). It
has value 1 (complete loss) if the prediction is wrong and 0 (no loss) if it is
right, and is commonly called 0-1 loss. The corresponding risk function, the
expectation of 0-1 loss, is called generalization error. It represents the error
rate within the population.
In most practical contexts, generalization error cannot be measured exactly.
It is usually estimated by the test error, in which the loss (speciﬁcally, 0-1
loss) is averaged over a freshly drawn sample of instances, the test data. That

52
Semisupervised Learning for Computational Linguistics
is, the test error is the proportion of test instances on which the classiﬁer and
target function disagree:
ˆϵ = 1
n
n
X
i=1
[[ ˆY (xi) ̸= yi]] = ˜p[ ˆY ̸= Y ]
where (x1, . . . , xn) here represents the test sample and ˜p is the empirical
distribution with respect to the test sample.
The test error is a binomial variable whose bias is the generalization error
ϵ. Its distribution is approximately normal if the sample size is suﬃciently
large, where “suﬃciently large” is usually taken to mean that
nˆϵ(1 −ˆϵ) ≥5
It follows that, with high probability:
ϵ = ˆϵ ± z
r
ˆϵ(1 −ˆϵ)
n
(4.8)
where z is a free parameter that controls the stringency of the qualiﬁer “with
high probability.” The larger z is, the higher the probability that (4.8) will be
true for a test sample drawn i.i.d. from the instance distribution. Speciﬁcally,
the probability that (4.8) holds is the area under the standard normal curve
excluding both tails that lie outside the interval [−z, +z].
To summarize, we have mentioned four diﬀerent quantities:
• The test error is the proportion of erroneous predictions on the test
sample.
• The generalization error is the expected proportion of errors over the
entire population of instances.
The test error is an estimate of the
generalization error, and will be an accurate estimate if the test sample
is large enough.
• The loss is the measure of discrepancy between an individual prediction
ˆy and the correct label y. Test error and generalization error assume
0-1 loss: the prediction is either completely right or completely wrong.
• The risk is the expected loss over the entire instance population. Gen-
eralization error is the special case of risk using 0-1 loss.
4.2.3
Fit and simplicity
Because test error provides an estimate of generalization error, and general-
ization error is the usual measure of classiﬁer quality, it may seem that one
should draw a sample, the training sample, and choose the hypothesis ˆY that
minimizes error on the training sample. However, that strategy is not always

Classiﬁcation
53
best. Error rate directly measures the ﬁt of a hypothesis to the training data.
If one is allowed to choose from a rich enough set of functions – that is, if
one is allowed to construct models of arbitrary complexity – one can ﬁnd a
hypothesis that ﬁts a given training sample arbitrarily well. But one does not
thereby necessarily improve the ﬁt to unseen data. As discussed earlier, be-
yond a certain point, one begins modeling accidental features of the training
data, resulting in worse performance on test data, a phenomenon known as
overﬁtting.
For this reason, many learning algorithms do not simply minimize risk
on the training data, but rather optimize some other objective function on
the training data. Ideally, there should be a proven connection between the
hypothesis that minimizes the objective function on the training data, and the
hypothesis that minimizes risk over the entire instance space. Establishing
such connections is a major goal of theoretical research in machine learning.
Either implicitly or explicitly, learning algorithms trade oﬀﬁt against sim-
plicity. An explicit trade-oﬀoften takes the form of regularization, which is
the optimization of an objective function that is a weighted average of ﬁt and
simplicity. Most commonly, one maximizes ﬁt and simplicity by minimizing
training-sample loss and complexity, that is, one chooses ˆY to minimize
˜p[L ˆY ,Y + γ| ˆY | ]
where ˜p is here the empirical distribution of the training sample, | ˆY | represents
a measure of model complexity, and γ is a coeﬃcient controlling the relative
importance of ﬁt and simplicity.
4.3
Evaluating detectors and classiﬁers that abstain
4.3.1
Conﬁdence-rated classiﬁers
When we discussed decision lists, in section 2.1.3, we noted that classiﬁers
often provide more information than just a prediction regarding the label: a
conﬁdence-rated classiﬁer provides not only a prediction but also a score that
can be interpreted as a measure of conﬁdence. For example, the prediction
of a decision list is determined by the highest-scoring rule that matches the
input instance, and the score of that rule is interpretable as a measure of
conﬁdence.
To give another example, the prediction of the Naive Bayes classiﬁer is the
label that maximizes p(y|x), where p is the distribution deﬁned by Naive Bayes
estimation. The probability p(ˆy|x) for the most-probable label ˆy might be
taken as a measure of conﬁdence. At least in the case of binary classiﬁcation,
however, there is a related measure of conﬁdence that turns out to have more

54
Semisupervised Learning for Computational Linguistics
interesting properties, namely, the log likelihood ratio
f(x) = log p(+|x)
p(−|x)
(4.9)
The classiﬁer predicts positive just in case p(+|x) > p(−|x), which is to say,
just in case f(x) > 0. That is, the sign of f(x) is equal to the predicted label,
and the absolute value |f(x)| is interpretable as conﬁdence. The log likelihood
(4.9) is a very common form for conﬁdence-rated binary classiﬁers.
Incidentally, a decision list can readily be recast in the form (4.9). In the
binary case, we have
p(−|x) = 1 −p(+|x)
Hence the ratio
p(+|x)
p(−|x)
varies monotonically with p(+|x), and since log is monotone, the log likelihood
ratio (4.9) also varies monotonically with p(+|x). As a consequence, we can
deﬁne an alternate version of a decision list that uses (4.9) as a conﬁdence
measure, but makes exactly the same predictions as the original decision list.
Instead of scoring rules F ⇒y, we assign a score to each feature, namely,
f(F) = log p(+|F)
p(−|F)
For a given input x, the “winning feature” is the one with highest conﬁdence
|f(F)|, and the prediction is the sign of f(F). The winning feature is guaran-
teed to be the feature that appears in the winning rule F ⇒y of the original
classiﬁer, which maximizes p(y|F), and the prediction, sign(f(F)), will be
equal to y.
4.3.2
Measures for detection
When a conﬁdence-rated classiﬁer has conﬁdence 0, it eﬀectively makes no
prediction for the instance in question; it abstains. For the purpose of com-
puting test error, an abstention is an error, but, as a consequence, test error
is not necessarily the best measure of performance for classiﬁers that abstain
on a signiﬁcant proportion of instances.
There does not appear to be a standard approach for evaluating multiclass
classiﬁers that abstain. Below, we will treat a multiclass classiﬁer with ab-
stention as the combination of a detector and a classiﬁer without abstention.
A detector is usually treated as a binary classiﬁer, but we prefer to view it
as a unary classiﬁer that abstains.
Let us write “ ˆY = ⊥” to mean that the classiﬁer abstains, that is, that
the value ˆY (x) is undeﬁned. For the sake of symmetry, we also consider the
possibility that the target function is partial, that is, that we sometimes have

Classiﬁcation
55
Y = ⊥. A detection problem is one in which there is a single class, but only
some instances belong to that class. The remaining instances have Y = ⊥.
This is equivalent to a binary classiﬁcation problem in which ⊥is interpreted
as the negative label, rather than the absence of a label.
The rate of occurrence is the probability that Y ̸= ⊥.
If it is small,
then small test error can be obtained by the “know-nothing” detector, which
always abstains: the error rate of the “know-nothing” detector is equal to the
rate of occurrence. In such a case, it can be more informative to consider the
performance of the detector on (true) positive instances and (true) negative
instances separately. The ability of the detector to detect positive instances,
measured as Pr[ ˆY ̸= ⊥|Y ̸= ⊥], is known as sensitivity, and its ability to reject
negative instances, measured as Pr[ ˆY = ⊥|Y = ⊥], is speciﬁcity. Accuracy
is Pr[ ˆY = Y ], which is equal to the weighted average
accuracy = Pr[Y ̸= ⊥] · sensitivity + Pr[Y = ⊥] · speciﬁcity
(4.10)
Sensitivity is weighted by the rate of occurrence, and speciﬁcity by the rate
of non-occurrence. Note that accuracy is one minus the test error.
A number of synonyms exist for sensitivity and speciﬁcity. Sensitivity is
also called the true positive rate, hit rate, or recall. One minus the speciﬁcity
is also known as the false positive rate, false alarm rate, or fallout.
An alternative to speciﬁcity that is commonly used in information retrieval
and computational linguistics is precision. It is the probability that the de-
tector is correct, given that it ﬁres: Pr[Y ̸= ⊥| ˆY ̸= ⊥].
All the quantities we have mentioned we have described as probabilities,
which is to say, population proportions. We are almost always more interested
in the corresponding test-sample proportions. We will not bother distinguish-
ing between “sensitivity” and “estimated sensitivity,” and so on. We will omit
the qualiﬁer “estimated”; in almost all cases the quantities of interest will be
the estimates.
The relations among the quantities mentioned are readily seen in the con-
tingency table (ﬁgure 4.5a). The numbers are assumed to be proportions of
total instances; that is, all the counts are assumed to have been divided by
the total number of instances. We have used “⊤” for the positive class label.
Note that the “positive” in “true positive” and “false positive” refers to the
prediction ˆY , not to the target value Y . Figures 4.5b and 4.5c give graphical
depictions of recall (R), speciﬁcity (S), and precision (P), as well as rate of
occurrence (r) and ﬁring rate (t). The dark lines indicate the regions to which
probabilities are conditionalized.
Rate of occurrence
r = Pr[Y ̸= ⊥]
= tp + fn
Rate of ﬁring
t = Pr[ ˆY ̸= ⊥]
= tp + fp
Precision
P = Pr[Y ̸= ⊥| ˆY ̸= ⊥] = tp/t
Sensitivity (recall)
R = Pr[ ˆY ̸= ⊥|Y ̸= ⊥] = tp/r
Speciﬁcity
S = Pr[ ˆY = ⊥|Y = ⊥] = tn/(1 −r)
Accuracy
A = Pr[ ˆY = Y ]
= tp + tn

56
Semisupervised Learning for Computational Linguistics
y
⊥
y^
tp
fp
fn
tn
⊥
r
R
S
1 - r
t
P
1 - t
⊥
⊥
(a)
(b)
(c)
FIGURE 4.5
(a) Contingency table for detection; (b) recall (sensitivity) R and speciﬁcity
S; (c) precision P.
Up to now we have assumed that the prediction of a conﬁdence-rated clas-
siﬁer is determined simply by taking the class with the highest probability.
In the case of detection, that implies that ˆY is positive just in case the con-
ﬁdence, measured as the log likelihood ratio (4.9), exceeds zero, and ˆY is
negative otherwise. Writing c(x) for the detector’s conﬁdence that instance x
has the positive label, we have
ˆY = +1
if and only if
c(x) > θ
with θ = 0.
Instead of taking a ﬁxed threshold of θ = 0, one can vary the threshold.
Diﬀerent choices of threshold yield diﬀerent contingency tables, and a diﬀerent
trade-oﬀof recall against precision or recall against speciﬁcity. The threshold
is monotonically related to the rate of ﬁring, t. As θ decreases, t increases.
If the detector simply guesses, then there is no correlation between c and
Y , and recall (sensitivity) is equal to the ﬁring rate:
R = Pr[ ˆY ̸= ⊥|Y ̸= ⊥] = Pr[ ˆY ̸= ⊥] Pr[Y ̸= ⊥]
Pr[Y ̸= ⊥]
= t.
Similarly, if the detector simply guesses, speciﬁcity is equal to one minus the
ﬁring rate (S = 1 −t = 1 −R) and precision is equal to the rate of occur-
rence (P = r).
If we plot precision against recall, the guessing detector’s
performance is a horizontal line whose level is the rate of occurrence (ﬁgure
4.6a). The points on the line represent diﬀerent ﬁring rates. If we plot recall
against speciﬁcity, the detector’s performance is the diagonal shown in ﬁgure
4.6b. In a recall-speciﬁcity plot, the curve for the guessing detector is the

Classiﬁcation
57
1
R
S
(c)
0
0
1
0.9
r = 0.9
P
R
(a)
1
R
S
(b)
0
1
0
FIGURE 4.6
The performance of the guessing detector as a curve, plotted as (a) precision
against recall, (b) speciﬁcity against recall, (c) ROC graph.
same regardless of the rate of occurrence. That, and the straightforward con-
nection between sensitivity (that is, recall), speciﬁcity, and accuracy (4.10),
make recall-speciﬁcity plots more attractive than precision-recall plots. Inci-
dentally, when plotting sensitivity against speciﬁcity, it is conventional to plot
speciﬁcity decreasing to the right (ﬁgure 4.6c). The resulting plot is called
a Receiver Operating Characteristic (ROC) graph. In plots 4.6a and 4.6b,
performance improves from lower left to upper right. In 4.6c, performance
improves from lower right to upper left. A detailed discussion of ROC graphs
can be found in Fawcett [86].
4.3.3
Idealized performance curves
If a single number is desired to measure performance, an alternative to accu-
racy is the area under the curve, either the area under the ROC curve (called
AUC) or the area under the recall-precision curve (called average precision).
A diﬀerent single-number measure often used in information retrieval and
computational linguistics is the harmonic mean of recall and precision, called
the F-measure:
F =
P −1 + R−1
2
−1
which simpliﬁes to:
F = 2PR
P + R.
Contours of equal F-measure can be viewed as idealized performance curves.
See ﬁgure 4.7. Along the main diagonal, where P = R, both P and R are
equal to F. This represents a kind of neutral point on the performance curve.

58
Semisupervised Learning for Computational Linguistics
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
Precision
Recall
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
Sensitivity
Specificity
(a)
(b)
FIGURE 4.7
Contours of F (r = 0.2).
In ﬁgure 4.7a, F-measure contours and the line of equal recall and precision are
plotted on a recall-precision graph, and in ﬁgure 4.7b, the same are plotted on
an ROC graph, assuming a rate of occurrence of 10% positives. Interestingly,
F-measure contours become linear in ROC space.∗
It is diﬃcult to justify F-measure probabilistically.
A better-motivated
set of idealized performance contours can be obtained by starting with a
simple model of detector development. Suppose that we start with a guessing
detector that ﬁres (that is, guesses positive) with probability t0. As we have
already seen, the performance of such a detector lies on the line R = S, and
more speciﬁcally at the point where R = S = t0. Let the true rate of positives
be r. Being a guessing detector means that ˆY and Y are uncorrelated, hence
that the probability of true positives is rt0, the probability of true negatives is
(1−r)(1−t0), the probability of false positives is (1−r)t0, and the probability
of false negatives is r(1 −t0).
To go from our initial detector to a perfect detector, we must reduce the
rates of false positives and false negatives to zero. Suppose we do so stepwise,
reducing the two error rates by, say, 10% at each step. After one step, the
proportion of false positives is α · fp0 where α = 0.9. After two steps, it is
∗Finding R as a function of F, S, and r is somewhat tedious, but merely an exercise in
algebra. It is
R = F(1 −S(1 −r))
r(2 −F)
.
If P = R, then both equal F. Substituting R for F in the previous equation yields the
equation for the line R = P:
R = 2 −1
r + S 1 −r
r
.

Classiﬁcation
59
α2 · fp0. The proportion of false negatives decreases similarly. In general:
fpk = αk · fp0 = αk(1 −r)t0
fnk = αk · fn0 = αkr(1 −t0).
Then we can compute sensitivity (recall) and speciﬁcity after k steps:
Rk = 1 −fnk
r
= 1 −αk(1 −t0)
(4.11)
Sk = 1 −fpk
1 −r = 1 −αkt0.
(4.12)
We can see that both Rk and Sk are linear functions of αk, and since 0 <
α < 1, αk ranges from 1 to 0 as k ranges from 0 to inﬁnity. The detector
with initial ﬁring rate t0 begins at point (1−t0, t0) in (S, R) space, and moves
along a line to (1, 1). The trajectories for several initial ﬁring rates are shown
as solid lines in the graphs of ﬁgure 4.8. The trajectories are linear in (S, R)
space, but curved in (R, P) space. If one guesses the proportion of positives
correctly, and starts with t0 = r, then the trajectory is along the diagonal of
the recall-precision graph.
Solving (4.12) for t0 and substituting into (4.11) yields
Rk = 2 −αk −Sk.
(4.13)
Taking the number of steps k as our measure of eﬀort, the points (Sk, Rk)
describe an equal-eﬀort contour. Unlike F-measure contours, the equal error-
reduction contours described by (4.13) are not sensitive to the rate of oc-
currence r. The contours of (4.13) are plotted as dotted lines in ﬁgure 4.8.
The two plots in ﬁgure 4.8 should be contrasted with the corresponding plots
in ﬁgure 4.7.
Equal-eﬀort contours are qualitatively similar to F-measure
contours, and accomplish the same purpose of providing a single number to
measure performance, but they have the advantage of a ﬁrm probabilistic
basis and invariance under changes in the rate of occurrence.
In general, using speciﬁcity and sensitivity is preferable to using recall and
precision. If one can directly sample from true positives and from instances on
which the detector ﬁres, but one cannot sample from the instance population
as a whole and one has no information about either the rate of occurrence or
rate of ﬁring, then measuring precision and recall may be the only alternative.
Such a case is rare, however; one usually acquires instances on which the
detector ﬁres by sampling from the population and applying the detector.
4.3.4
The multiclass case
As mentioned above, there is no standard generalization to the multiclass case
with abstention. Abstention could be treated simply as an additional class

60
Semisupervised Learning for Computational Linguistics
Specificity
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
Precision
Recall
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
Sensitivity
FIGURE 4.8
Equal error reduction contours. The solid lines are trajectories of ideal clas-
siﬁers at t0 = 1/8, 2/8, . . . , 7/8. The dotted lines are contours of equal error
reduction (r = 0.2).
(the “garbage” class), but doing so is less than satisfactory. An alternative
is to treat the multiclass case with abstention as the combination of a detec-
tion problem and a classiﬁcation problem. The detection problem concerns
whether or not ˆY = ⊥and Y = ⊥, and performance is evaluated as we
have just discussed. The classiﬁcation problem is evaluated by conditional
accuracy, that is, accuracy where attention is limited to the case that neither
ˆY = ⊥nor Y = ⊥(the upper left cell of the contingency table):
B = Pr[ ˆY = Y | ˆY ̸= ⊥, Y ̸= ⊥].
Graphical depictions of accuracy (A) and conditional accuracy (B) are given
in ﬁgure 4.9. The overall accuracy A, which treats ⊥as a class rather than
abstention, is conditional accuracy B times the true positive rate, plus the
true negative rate. The true positive rate can be obtained either as the rate
of occurrence times the recall, or as the ﬁring rate times the precision. That
is,
A = rRB + (1 −r)S = tPB + (1 −r)S.
If the test data is fully labeled, but the classiﬁer may abstain, then r = 1,
P = 1, R = t, and A = tB. In such a case, the natural performance curve is
ﬁring rate (which in this case equals recall) against conditional accuracy; this
is what we used in the discussion of self-training in section 2.2.3. Figure 4.10a
shows the contours of equal accuracy in a plot of ﬁring rate against conditional
accuracy. The similarity to F-measure is striking (see the left graph in ﬁgure
4.7), but in this case the measure is probabilistically well motivated.
Note that the accuracy contours in ﬁgure 4.10a treat abstentions as errors.
As a practical matter, one would fall back to a default classiﬁer instead.

Classiﬁcation
61
y1
yn
⊥
y1
yn
⊥
...............
y1
yn
⊥
y1
yn
⊥
...............
(a)
(b)
FIGURE 4.9
(a) Accuracy and (b) conditional accuracy in the multiclass case. In each
case, the probability of interest is represented by the total area of the black
regions as a proportion of the area of the encompassing solid square.
Firing Rate (Recall)
Firing Rate (Recall)
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
Conditional Accuracy
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 0.2
 0.4
 0.6
 0.8
 1
Conditional Accuracy
(a)
(b)
FIGURE 4.10
Contours of equal accuracy for classiﬁcation with abstention. In the case of
abstention, a fallback classiﬁer is assumed with constant accuracy of (a) 0%
or (b) 50%.

62
Semisupervised Learning for Computational Linguistics
Figure 4.10b shows the accuracy contours if one falls back to a classiﬁer with
a constant 50% accuracy.
In the case of simple rules of form F ⇒y, with y ̸= ⊥, there is a close
relationship between precision and conditional accuracy. In a detection task,
the natural measure for the quality of the rule is precision. There is only
one target class, and precision is the probability that the rule’s prediction is
correct, when it ﬁres. In a classiﬁcation task with a total target function (Y is
never ⊥), the corresponding measure is conditional accuracy. In the detection
case, the conditional accuracy of a simple rule is always 1: there is only one
class (excluding ⊥), so the rule is always correct when it ﬁres and the target is
not ⊥. In the classiﬁcation case, the precision of a simple rule is always 1: the
target function Y never has value ⊥, so certainly Y ̸= ⊥on instances where
the rule ﬁres. Moreover, we can view the classiﬁcation task as a collection
of detection tasks, one for each label. The conditional accuracy of the rule
F ⇒y on the classiﬁcation task is equal to its precision on the detection task
for label y.
4.4
Binary classiﬁers and ECOC
From this point, we will restrict our attention to binary classiﬁers, that is,
classiﬁers whose output is one of two classes, represented by the values +1
and −1. Considering the binary case often simpliﬁes matters considerably.
And despite ﬁrst appearances, limiting attention to binary classiﬁcation does
not entail any loss of generality. There are methods for addressing multiclass
problems with binary classiﬁers.
A simple method is to train a separate binary classiﬁer for each class. Sup-
pose there are four classes, y1, y2, y3, y4.
We construct a classiﬁer c1 that
returns positive for instances labeled y1, and negative for all other instances,
and similar classiﬁers for each of the other labels. The following matrix repre-
sents this scheme. The entry in cell (i, j) is the label assigned to an instance
in the training data for classiﬁer ci, if its original label is yj. For example, for
classiﬁer c2, instances whose original label is y2 are treated as positives, and
all other instances are treated as negatives.
y1 y2 y3 y4
c1 +1 −1 −1 −1
c2 −1 +1 −1 −1
c3 −1 −1 +1 −1
c4 −1 −1 −1 +1
(4.14)
Each binary classiﬁer can be viewed as discriminating between two sets of
labels.
For example, classiﬁer c2 distinguishes the set {y2} from the set

Classiﬁcation
63
{y1, y3, y4}.
The predictions of the individual classiﬁers must be combined to give a
prediction for the composite, multiclass classiﬁer.
That is easy when one
classiﬁer says “yes” (output +1) and all others say “no” (output −1). But
other cases are possible. The following illustrates several possible cases.
(a) (b) (c) (d)
c1 −1 +.6 −.5 −.5
c2 +1 −.4 +.1 −.9
c3 −1 −.2 +.4 −.3
c4 −1 −.8 −.9 −.1
(4.15)
Each column represents an output vector, with one component for each binary
classiﬁer. The question is which class the composite classiﬁer should predict
in each case. Case (a) is the easy one: the output should obviously be y2. In
case (b), the outputs are less crisp, but nonetheless, only c1 says “yes” and
all others say “no,” so it seems clear that the output should be y1. In case
(c), both c2 and c3 say “yes.” We interpret the absolute value of the classiﬁer
output as its conﬁdence, so it is natural to “believe” the classiﬁer that is most
conﬁdent. In this case, c3 is more conﬁdent than c2, so we predict label y3.
Finally, in case (d), all of the classiﬁers say “no.” In this case, it is natural to
predict the label of the classiﬁer that is least conﬁdent in saying “no”: namely,
we predict y4. In short, in each case, we predict the label corresponding to
the classiﬁer with the greatest output value.
There is another way to think about the problem. We can view the columns
of the matrix in (4.14) as codewords for the labels. The classiﬁer outputs,
which is to say, the columns of (4.15), may fail to match any of the codewords
exactly. But we compare them to each codeword in turn, and output the label
whose codeword is most similar to the classiﬁer output vector. The measure
of similarity most commonly used is Hamming distance, also known as L1
distance; the Hamming distance between an output vector z and a codeword
w is
L1(z, w) ≡
X
i
|zi −wi|.
For example, let z be the vector in (4.15c), namely (−.5, +.1, +.4, −.9), and
let wi be the codeword for yi. Then
L1(z, w1) = 1.5 + 1.1 + 1.4 + .1 = 4.1
L1(z, w2) = .5 + .9 + 1.4 + .1 = 2.9
L1(z, w3) = .5 + 1.1 + .6 + .1 = 2.3
L1(z, w4) = .5 + 1.1 + 1.4 + 1.9 = 4.9.
The codeword with the greater similarity to the output vector is w3, the
codeword for y3, so y3 is the predicted label.

64
Semisupervised Learning for Computational Linguistics
This technique works just as well if we add classiﬁers that distinguish be-
tween pairs of classes. Suppose we extend matrix (4.14) as follows:
y1 y2 y3 y4
c1 +1 −1 −1 −1
c2 −1 +1 −1 −1
c3 −1 −1 +1 −1
c4 −1 −1 −1 +1
c5 +1 +1 −1 −1
c6 +1 −1 +1 −1
c7 +1 −1 −1 +1
.
(4.16)
There are other rows that might be added that contain two positives and
two negatives, but they are all equivalent to one of the existing rows. For
example, the possible row (−1, +1, −1, +1) is equivalent to the existing row
(+1, −1, +1, −1); both classiﬁers distinguish between the set {y1, y3} and the
set {y2, y4}.
Now consider the following output z. Taking its Hamming distance with
each of the codewords gives the results in the bottom row:
z
w1
w2
w3
w4


−.3
+.1
+.2
+.2
−.6
−.4
+.1




+1
−1
−1
−1
+1
+1
+1




−1
+1
−1
−1
+1
−1
−1




−1
−1
+1
−1
−1
+1
−1




−1
−1
−1
+1
−1
−1
+1


8.7
7.3
6.7
5.7
.
The most-similar codeword is w4, which is the codeword for y4, so the pre-
diction is y4.
Incidentally, when the codewords consist of positive and negative ones, as
here, the Hamming distance is closely related to the dot product between the
two vectors. Consider the Hamming distance between z and w3. The ﬁrst
components are −.3 and −1. The signs agree, so the absolute diﬀerence is .7,
which is the diﬀerence 1 −.3. Likewise, the third components are +.2 and
+1. The signs are positive this time, but they still agree, and the absolute
diﬀerence is .8, which is again the diﬀerence 1 −.2. On the other hand, the
fourth components are +.2 and −1. In this case, the signs disagree, and the
absolute diﬀerence is 1.2, which is the sum 1 + .2.
In general, we add if
the signs disagree, and subtract if they agree. That is, the i-th components
zi and wi contribute 1 −ziwi to the Hamming distance. Summing over all
components, we have:
L1(z, w) = n −z · w

Classiﬁcation
65
where n is the total number of components.
This leaves the question of how to choose the codewords for the classes. If
the number of classes is small, we can enumerate all partitions of the classes,
as in (4.16); each row represents a diﬀerent partitioning of the classes into two
sets. In general, however many classes there are, it is desirable to have code-
words that are maximally distant from each other. That is, the columns of
the matrix (4.16) should be maximally distant from one another, in Hamming
distance. That makes the codewords maximally easy to distinguish from one
another. But in order to have distance in the codeword space correspond to
discriminability, the axes of the codeword space, represented by the individ-
ual binary classiﬁers, must be maximally separated, meaning that the rows of
(4.16) must also be maximally distinct from each other. Intuitively, if mul-
tiple classiﬁers face the same or similar discrimination tasks, they will make
correlated errors.
If classiﬁers make uncorrelated errors, and codewords are maximally distant
from one another, the encoding method is robust to errors made by the in-
dividual classiﬁers. The encoding is called an Error-Correcting Output Code
(ECOC); the idea of using ECOC for encoding multiclass problems as collec-
tions of binary classiﬁcation problems was proposed by Dietterich & Bakiri
[76], who discuss methods of obtaining good codewords when there is a large
number of classes.


5
Mathematics for Boundary-Oriented Methods
In the discussion of the nearest-neighbor classiﬁer (section 4.1.2), we viewed
the classiﬁer in terms of the decision boundary that it deﬁnes in the feature
space, as, for example, in ﬁgure 4.4. This can be a very proﬁtable way of
viewing classiﬁers in general, and a number of methods focus explicitly on
constructing good boundaries.
We will discuss those methods in the next
chapter. Before we can do so, however, we require some additional mathe-
matical background.
This chapter treats three topics in linear algebra: the formal representation
of a high-dimensional linear boundary, that is, a hyperplane; the gradient,
along with rules for taking derivatives of linear-algebraic expressions; and
constrained optimization.
5.1
Linear separators
The simplest variety of decision boundary is arguably a linear boundary, and
many classiﬁers are based on the idea of constructing a linear decision bound-
ary. In two dimensions, a linear boundary is a line; in three dimensions, it is
a plane; in a general n-dimensional space it is a hyperplane of n minus one
dimensions. A single linear boundary obviously divides the space of instances
into only two classes, hence it suﬃces only for a binary classiﬁer; but we have
already seen how a multiclass problem can be reduced to a collection of binary
classiﬁcation problems.
5.1.1
Representing a hyperplane
The most convenient way to specify a hyperplane is to specify a normal vector
w, which gives the orientation of the hyperplane in space, and the distance
r from the origin to the hyperplane along the direction of w.
Figure 5.1
illustrates in the case of a plane (a two-dimensional hyperplane) in a three-
dimensional space. The vector w is normal (that is, perpendicular) to the
plane, and serves to specify the orientation of the plane; and r is the distance
from the origin. To be more precise, r is a “signed distance.” A negative
value for r indicates that the plane lies on the side of the origin opposite to
67

68
Semisupervised Learning for Computational Linguistics
O
r
w
FIGURE 5.1
Specifying a hyperplane using a normal vector w and a distance r from the
origin.
the direction of w.
Figure 5.2 shows a point x in the space. The prediction of the classiﬁer on
input x will be positive if x is on the “positive side” of the hyperplane, and
negative otherwise. The positive side is deﬁned to be the half-space that w
points to. In ﬁgure 5.2, that is the side opposite the origin, but it need not
be: as already mentioned, r may be negative, in which case w points toward
the origin.
To determine the classiﬁer’s prediction for point x, we compute the signed
distance d from the hyperplane to x.
If x is on the positive side of the
hyperplane, d is positive, and negative otherwise. The points that lie in the
hyperplane are those for which d = 0. The absolute value of d is the distance
from the hyperplane to x.
To compute d, we project x onto w. The projection of x onto w is the
vector v in ﬁgure 5.2. Its direction is the same as that of w, but its length is
the dot product of x with the unit vector in the direction of w:
||v|| = x · w
||w||.
This is the distance from the origin along the line of w to a point level with
x. To obtain d, we subtract r:
d = x · w
||w|| −r.
(5.1)
It will be convenient if we do not need to normalize w, so let us write this in

Mathematics for Boundary-Oriented Methods
69
O
r
w
d
v
x
FIGURE 5.2
Computing the distance d from the hyperplane to a point x. The vector v is
the projection of x onto w.
the equivalent form:
d||w|| = x · w −r||w||.
(5.2)
Thinking of 1/||w|| as one “w-unit,” this says that x · w less the threshold
r||w|| gives the distance from the separator in w-units: d||w|| times the unit
size 1/||w|| equals d.
An important corollary is that scaling the vector w does not aﬀect d. Deﬁne
w′ ≡βw to be a vector in the same direction as w, but scaled by a factor β.
Then
x · w′
||w′|| −r = x · βw
||βw|| −r
= β(x · w)
β||w||
−r
= d.
5.1.2
Eliminating the threshold
The form (5.2) would be simpler if we could eliminate the threshold r. We can
accomplish that by folding the threshold into the vector w. The trick is to
add an extra component to every data point x, and add an extra component
to the normal vector w, as follows. If w = (w1, . . . , wn) and x = (x1, . . . , xn),
we deﬁne the new representations
w′ = (w1, . . . , wn, r||w||)
x′ = (x1, . . . , xn, −1)

70
Semisupervised Learning for Computational Linguistics
L
x  w = 0
·
w
O
z = -1
FIGURE 5.3
By adding an additional dimension z, with all data points living in the plane
z = −1, we can specify an arbitrary linear separator L in the data plane by
means of a hyperplane that passes through the origin (O) in the new space.
Then
w′ · x′ = (w1, . . . , wn, r||w||) · (x1, . . . , xn, −1)
= w · x −r||w||
= d||w||.
In the modiﬁed space, the distance (in w-units) between x′ and the hyperplane
is simply the dot product x′ · w′.
Geometrically, the eﬀect is the following. Imagine the data to lie in a two-
dimensional space. We add an extra “z” axis perpendicular to the data plane,
and place the data plane at level negative one on the z axis (ﬁgure 5.3). Now
we can limit attention to separating hyperplanes that contain the origin; no
threshold is necessary to specify distance from the origin. By rotating and
tilting the hyperplane, with the origin as pivot, we can cut the data plane
along any line L that we desire, as illustrated in the ﬁgure.
5.1.3
The point-normal form
Sometimes it is convenient to specify a hyperplane using a normal vector w
as before, but using, instead of the distance r from the origin, a point a that
lies on the hyperplane (ﬁgure 5.4). We again consider an arbitrary point x,
and we compute the signed distance d from the hyperplane to x. The dotted
vector pointing from a to x is the vector diﬀerence x −a. The length of its
projection onto w is our desired signed distance d:
d = (x −a) · w
||w||.
(5.3)

Mathematics for Boundary-Oriented Methods
71
w
a
x
d
FIGURE 5.4
The point-normal form. The signed distance d from the hyperplane can be
computed from the normal vector w and a point a lying in the hyperplane.
As before, d is positive for points in the half-space toward which w points,
and negative in the other half-space. The points that lie in the hyperplane
itself have d = 0.
The point-normal form (5.3) is easily translated to the form (5.1):
d = x · w
||w|| −a · w
||w||.
This is equivalent to (5.1) with r = a · w/||w||.
A particular example in which the point-normal form arises involves a lin-
ear separator consisting of points equidistant between two designated points
m1 and m2, as in ﬁgure 5.5. This situation arises in the 1-nearest-neighbor
algorithm, for example, in which case the points m1 and m2 are training in-
stances, and the linear separator between them is a boundary segment, such
as we saw earlier in ﬁgure 4.4. A similar situation arises in k-means clustering
(section 7.4.1), in which case the points are cluster centers.
The vector whose tail is at m2 and whose head is at m1 is normal to the
separating hyperplane; call it w. It is the vector diﬀerence of the centers:
w = m1 −m2.
The point a is the midpoint of the connecting line segment:
a = m1 + m2
2
.
With these values of w and a, the signed distance from the separating hy-
perplane to an arbitrary point x is given by (5.3), with the hyperplane itself
consisting of those points for which d = 0.

72
Semisupervised Learning for Computational Linguistics
m2
a
m1
FIGURE 5.5
A linear separator equidistant between centers m1 and m2.
5.1.4
Naive Bayes decision boundary
It was mentioned in the previous chapter that the decision boundary for the
Naive Bayes classiﬁer is a linear separator. We are now in a position to see
why that is true.
We continue to limit attention to binary classiﬁcation. The Naive Bayes
classiﬁer predicts positive if p(x, +) > p(x, −), which is to say if
f(x) ≡log p(x, +)
p(x, −) > 0.
As mentioned in section 4.3.1, the value f(x) is known as a log likelihood
ratio.
Recall that, under the Naive Bayes independence assumption, p(x, y) is
deﬁned as
p(x, y) = p(y)
Y
i
p(xi|y)
where x = (x1, . . . , xm). Hence:
f(x) = log p(+) Q
i p(xi|+)
p(−) Q
i p(xi|−)
=
X
i
log p(xi|+)
p(xi|−) + (−1) log p(−)
p(+).
(5.4)
For example, consider the Naive Bayes model given in section 4.1.1, in (4.1)
and (4.2). We have
log p(−)
p(+) = log 8/14
6/14 = .288

Mathematics for Boundary-Oriented Methods
73
and for the attributes:
Len
Wid
x
log p(x|+)
p(x|−)
log p(x|+)
p(x|−)
small log 1/6
6/8 = −1.504 log 1/6
4/8 = −1.099
med
log 2/6
1/8 = 0.981
log 1/6
3/8 = −.811
large log 3/6
1/8 = 1.386
log 4/6
1/8 = 1.674
.
(5.5)
The value of f(x) for x = (med, med) is
f(med, med) = 0.981 −.811 −.288 = −.118.
(5.6)
The prediction is negative, with the absolute value (.118) interpretable as
conﬁdence.
Let us write wij for the number in the (i, j)-th cell of (5.5). For example,
w12 = −1.099 is the number associated with Wid = small.
In (5.6), the
numbers 0.981 and −.811 are w21 and w22, respectively. In general, the sum
expanding f(x) will contain the “bias” term −.288 and one weight wi1 for the
Len attribute and one weight wi2 for the Wid attribute. We can write (5.6)
in a more general form like this:
f(med, med) =
(0)(−1.504) + (1)(0.981) + (0)(1.386) +
(0)(−1.099) + (1)(−.811) + (0)(1.674) +
(−1)(.288).
The sum has coeﬃcient one for the two weights corresponding to Len = med
and Wid = med, and coeﬃcient zero everywhere else, except for the bias term,
which has coeﬃcient negative one. A diﬀerent value for x will have a sum
that diﬀers only in the choices of the ones and zeros. That is, let us represent
the instance (med, med) by the vector:
x = (0, 1, 0, 0, 1, 0, −1)
and let us deﬁne
w = (−1.504, 0.981, 1.386, −1.099, −.811, 1.674, .288).
Then we have
f(x) = x · w.
In short, the Naive Bayes prediction f(x) can be expressed as a dot product
between a vector w representing the model, and a vector x representing the
instance. Instances are represented as points in a space whose axes correspond
to attribute-value pairs, plus a “z” axis for the bias. In that space, the Naive
Bayes model deﬁnes a linear separator, which is the hyperplane x · w = 0.

74
Semisupervised Learning for Computational Linguistics
x
+
FIGURE 5.6
A function, tangent plane, contours.
5.2
The gradient
We change topics now to discuss optimization. One common method of mini-
mizing a function is gradient descent. The gradient is a vector that points in
the direction in which the function is increasing most rapidly, and by following
the gradient “backward,” one seeks to ﬁnd a lowest point, that is, a minimum
of the function.
5.2.1
Graphs and domains
Consider a function f(x) whose domain is Rn. The graph of f is a surface
in Rn+1, consisting of points (x1, . . . , xn, y) such that y = f(x) and x =
(x1, . . . , xn).
The additional dimension is the codomain of f.
If D is the
domain of f and C is its codomain, we call D × C the “graph space” of f. In
our example, the domain is Rn, the codomain is R, and the graph space is
Rn+1.
A level curve of a function f is the set of points satisfying
f(x) = c
for a particular value of c. A level curve resides in the domain of f, not in the
graph space of f. A collection of level curves gives a “contour map” of the
function, as in ﬁgure 5.6. The position of the cross marks the point at which
f takes on its minimum value. Thinking of the values f(x) as elevations, the
cross marks the lowest point and the minimum is its elevation.

Mathematics for Boundary-Oriented Methods
75
+
FIGURE 5.7
A three-dimensional domain.
The gradient of f is the vector of partial derivatives
∇f(x) =
 ∂
∂x1
f(x), . . . ,
∂
∂xn
f(x)

.
It gives the rate of increase of the function at point x in each of the the axis-
parallel directions. Note that ∇f is a function, and its value at point x is a
vector.
The gradient can be interpreted geometrically as the direction of steep-
est increase in f. For example, consider the function f in ﬁgure 5.6, with
∂f/∂x1 = ∂f/∂x2 = 1 at a particular point x. The tangent plane to f at
point x rises with slope +1 in the x1 direction and with slope +1 in the x2
direction. The direction directly “up” the tangent plane is at 45◦, which is to
say, in the direction of the vector
(1, 1) = ∇f(x).
Notice that ﬁgure 5.6 depicts the graph space of f, but the gradient, like the
level curves, lives in the domain, not in the graph space. The “ﬂoor” in ﬁgure
5.6 is the domain.
Importantly, the gradient at point x is normal (that is, perpendicular) to
the level curve passing through x. Figure 5.6 illustrates the case where the
domain is two-dimensional. In the case of a three-dimensional domain (ﬁgure
5.7), the graph space is four dimensional, hence it cannot easily be drawn.
The level curves are surfaces within the domain, forming expanding “shells”
around the minimizer. The gradient at point x is a vector that is perpendicular
to the level surface, pointing in the direction away from the minimizer.

76
Semisupervised Learning for Computational Linguistics
(a)
(b)
FIGURE 5.8
Convex functions. (a) Smooth, (b) piecewise linear.
5.2.2
Convexity
We will be particularly interested in a certain class of functions, the convex
functions. A convex function is, intuitively, one shaped like a bowl. It may be
smooth, as in ﬁgure 5.8a, or piecewise linear, as in ﬁgure 5.8b, or a combination
of both. Consider any chord, that is, the the line segment connecting two
points on the graph. A function is convex if it its graph lies on or below the
chord, for every chord (ﬁgure 5.9). Algebraically, the line segment connecting
points x and y is the set of points αx + βy, for α, β ≥0 and α + β = 1, and
the z-value for the chord at each of those points is
αf(x) + βf(y).
Hence a function is convex just in case:
f(αx + βy) ≤αf(x) + βf(y)
for all points x and y in the domain of f and for all 0 ≤α, β and α + β = 1.
A linear combination of convex functions is itself convex. Suppose that the
functions fi are convex, and suppose there are real numbers ci such that:
g(x) =
X
i
cifi(x).
Then
g(αx + βy) =
X
i
cifi(αx + βy)
≤
X
i
ci[αfi(x) + βfi(y)]

Mathematics for Boundary-Oriented Methods
77
x
y
αf x) + β
y)
(
f (
f x)
(
f y)
(
f(αx + βy)
αx + βy
FIGURE 5.9
Deﬁnition of convexity.
= α
X
i
cifi(x) + β
X
i
cifi(y)
= αg(x) + βg(y).
A related concept is that of a convex set, which is a set of points S that
has the following property. If x and y belong to S, then so does every point
αx + βy
on the line segment connecting x and y, where α, β ≥0 and α + β = 1. The
connection between convex functions and convex sets is this:
A level curve of a convex function describes the boundary of a
convex set.
Recall that a level curve is the set of points satisfying f(x) = c for a ﬁxed
value c. The set of points satisfying f(x) ≤c is a convex set.
One more property of note is the following. Consider the parabola f(x)
in ﬁgure 5.10a.
The derivative f ′(x) is the slope of the tangent line.
It
increases as one moves out from the minimum.
This is true in general of
convex functions. The reason should be clear intuitively: if the derivative
ever decreased, the “bowl” would curve outward, and the resulting “lip” would
provide a concavity where the graph of the function is higher than a chord
(ﬁgure 5.10b).
To be more precise, the slope is nondecreasing as one moves away from the
minimum. It is strictly increasing if the function is strictly convex.
In three and higher dimensions, the generalization of slope is the length of
the gradient. Figure 5.11 shows the rise in the tangent plane in the direction
of the gradient (which is the direction of maximum rise).
For brevity, we

78
Semisupervised Learning for Computational Linguistics
(a)
(b)
FIGURE 5.10
(a) Convex function, slopes of tangent lines always increase. (b) Slopes of
tangent lines decrease in region of concavity, function lies above chord.
d 1
d 1
d 2
d 2
f
2
d 1
2
d 2
2
2
rise = d  + d
1
2
FIGURE 5.11
The slope of the tangent plane in the direction of the gradient.

Mathematics for Boundary-Oriented Methods
79
have written d1 for ∂f/∂x1 and d2 for ∂f/∂x2. The ﬁgure depicts the case
d1 = 1/3 and d2 = 3/4, but nothing hinges on those choices. The quantity d1
is the rise in the tangent plane if we move one unit in the x1 direction. If we
only move d1 units in the x1 direction, then the rise is d2
1: for example, here
it is 1/3 of 1/3. The ﬁgure also makes clear that the rise at the point of the
gradient is d2
1 + d2
2, which is equal to ||∇f||2. The “run” is simply the length
of the gradient vector. The slope of the tangent plane is the rise over the run:
||∇f||2
||∇f|| = ||∇f||.
We conclude that the length of the gradient of a convex function is monoton-
ically related to the distance from the minimum point. It is strictly increasing
if the function is strictly convex. This is extremely useful for optimization:
the direction of the gradient tells us in which direction the minimum lies, and
its magnitude decreases as we get closer, becoming zero when we reach the
minimum.
5.2.3
Diﬀerentiation of vector and matrix expressions
The gradient is an example of a function that maps vectors to vectors; in fact,
as we saw, it is the multidimensional counterpart of the derivative. It is often
easy to set up an expression for a gradient, but tedious to simplify it using
componentwise partial derivatives. Not only for the gradient, but generally
for functions involving operations on vectors and matrices, more direct rules
of diﬀerentiation are a great convenience.
The derivative of a vector with respect to a scalar is simply the vector in
which the diﬀerentiation is done elementwise:
d
dxy =


d
dxy1
...
d
dxym

.
The derivative of a vector y with respect to a vector x is a matrix, in which
the j-th column is the derivative of y with respect to xj:
Dxy =

∂
∂x1 y . . .
∂
∂xn y 
=


∂y1
∂x1 . . . ∂y1
∂xn
...
...
...
∂ym
∂x1 . . . ∂ym
∂xn

.
(5.7)
The derivative of a scalar with respect to a vector can be treated as the special
case in which y contains only one component y:
Dxy =
 ∂y
∂x1
. . . ∂y
∂xn

.
(5.8)

80
Semisupervised Learning for Computational Linguistics
Note that this is in fact the deﬁnition of the gradient:
∇y = Dxy
although to be quite precise ∇y is a vector whereas Dxy is a matrix containing
a single row.
We do not consider diﬀerentiation of matrices or with respect to matrices.
The diﬀerentiation operator D here is an operator on vectors. As such, it is
immaterial whether the vector y or the vector x is considered as a row vector
or a column vector; the derivative is the same.
Now we turn to the basic rules for taking derivatives. All the rules can be
derived fairly straightforwardly from the deﬁnitions just given and the familiar
rules for diﬀerentiation of real numbers.
For example, consider Dxx. By deﬁnition, Dxx is a matrix whose (i, j)-th
entry is ∂xi/∂xj. For a given independent variable xi, the other independent
variables xj are treated as constants; that is, ∂xi/∂xj is one if i = j and zero
if i ̸= j. Hence, Dxx is the identity matrix:
Dxx = I.
For another example, consider DxaTy, where we understand y to be a
vector-valued function of x, but a to be a constant vector. Because aTy is a
scalar, we apply deﬁnition (5.8):
DxaTy =
h
. . .
∂
∂xj aTy . . .
i
=
h
. . .
∂
∂xj
P
i aiyi . . .
i
=
h
. . . P
i ai
∂
∂xj yi . . .
i
= aT h
. . .
∂
∂xj y . . .
i
= aTDxy.
Combining the previous two results yields
DxaTx = aTDxx = aTI = aT.
What if we reverse a and y? We have
DxyTa =
h
. . .
∂
∂xj yTa . . .
i
.
Since yTa = aTy, the remainder of the derivation is identical to the one given
just above, and we conclude that
DxyTa = aTDxy.

Mathematics for Boundary-Oriented Methods
81
To give one more example, let us consider DxyTz where both y and z are
vector-valued functions of x.
DxyTz = Dx
X
i
yizi
=
h
. . .
∂
∂xj
P
i yizi . . .
i
=
h
. . . P
i

zi
∂
∂xj yi + yi
∂
∂xj zi

. . .
i
=
h
. . . P
i zi
∂
∂xj yi . . .
i
+
h
. . . P
i yi
∂
∂xj zi . . .
i
= zT h
. . .
∂
∂xj y . . .
i
+ yT h
. . .
∂
∂xj z . . .
i
= zTDxy + yTDxz.
The following is a useful list of derivation rules, including the ones we have
just derived, but others as well whose proof is left as an exercise for the
reader. In the following, x is always the independent variable (a vector) and
the variables y and z represent vector-valued functions of the independent
variable. The variable a represents a scalar constant, a represents a vector
constant, and A represents a matrix constant.
Dxa = 0
Dxx = I
Dxay = aDxy
Dx(y + z) = Dxy + Dxz
DxaTy = aTDxy
DxyTa = aTDxy
DxAy = ADxy
DxyTA = ATDxy
DxyTz = yTDxz + zTDxy
DxyTy = 2yTDxy
DxyTAy = yT(A + AT)Dxy
DxyTAy = 2yTADxy
if A is symmetric.
5.2.4
An example: linear regression
Linear regression provides a good example of the utility of our rules for com-
puting the derivative. In linear regression, there is a real-valued target func-
tion Y (x), and we construct a hypothesis of the form
ˆY (x) = w · x + α.
(5.9)

82
Semisupervised Learning for Computational Linguistics
The question is how to choose the weight vector w.
As discussed in section 5.1.2, we can fold α into w. Speciﬁcally, α becomes a
new ﬁnal component in the vector w, and every instance vector x is extended
by the addition of a new constant component with value one.
After this
change of representation, (5.9) becomes
ˆY (x) = w · x.
(5.10)
The learner is provided with a labeled training sample, which we represent
as a matrix X whose rows are the training instances xi, and a vector y whose
components are the corresponding labels yi. Using (5.10), we obtain a vector
ˆy containing predicted values for labels:
ˆy = Xw.
To choose the best weight vector w, we minimize squared error. The error
on the i-th instance is yi −ˆyi. The sum of squared errors can be represented
as a dot product:
X
i
(yi −ˆyi)2 = (y −ˆy) · (y −ˆy).
Let us deﬁne the error vector e as
e ≡y −ˆy.
We wish to minimize squared error eTe. To ﬁnd the value of w that minimizes
squared error, we take the derivative of eTe with respect to w, and set it to
zero. This is the point at which the rules from the previous section show their
worth:
DweTe = 2eTDwe
= 2eTDw(y −Xw)
= 2eT(Dwy −DwXw)
= −2eTXDww
= −2eTX.
Substituting in the deﬁnition for e, we obtain
DweTe = −2(y −Xw)TX
= −2yTX + 2wTXTX.
Setting this to zero, we conclude
wTXTX = yTX
w = (XTX)−1XTy
provided that the inverse is deﬁned.

Mathematics for Boundary-Oriented Methods
83
5.3
Constrained optimization
5.3.1
Optimization
An optimization problem is the problem of ﬁnding a point that minimizes
or maximizes a function f(x), called the objective function. We will limit
our attention to minimization. In the general case, this is not an onerous
restriction, inasmuch as the problem of maximizing a function f(x) can be
expressed as the problem of minimizing −f(x).
To be sure, we also limit
attention (for the most part) to convex functions, and if f(x) is convex, then
−f(x) is not convex, it is concave; so the restriction to minimization is a
limitation after all.
But then again, with a few uninteresting exceptions,
convex functions are unbounded – they have no maxima – so it is suﬃcient
to be able to minimize convex functions or maximize concave functions (by
minimizing their negatives).
The standard method for minimizing a function is to ﬁnd the point at which
its gradient is equal to 0, as we did in the previous section when minimizing
squared error in linear regression. As we have seen, the gradient at x gives
the rate of increase of the function from x in each of the axis directions. If
the function is not increasing in any direction, one is either at a maximum or
a minimum (or a saddle point: maximum in some directions and minimum in
others).
In this section, we consider optimization under constraints. Constraints
are of two sorts: equality constraints and inequality constraints. An equality
constraint is an equation:
g(x) = c
which must be satisﬁed. We limit our attention to equality constraints of the
form:
g(x) = 0.
This does not represent a real limitation, as any constraint g(x) = c can be
written:
g(x) −c = 0.
An inequality constraint takes the form:
h(x) ≤0.
A constraint h(x) ≤c can be written as h(x)−c ≤0, and a constraint h(x) ≥c
can be written as c −h(x) ≤0.
The goal in constrained optimization is not to ﬁnd the global minimum of
the objective function f, but rather to ﬁnd the point among those satisfying
the constraints where f is at a minimum. The points satisfying the constraints
are known as the feasible points.

84
Semisupervised Learning for Computational Linguistics
-1
-2
g =0
1
g =0
2
-1
-2
g
= 0
1= g2
FIGURE 5.12
Intersection of two constraint surfaces.
5.3.2
Equality constraints
Let us consider a case with a three-dimensional domain and two equality
constraints:
g1(x) = 0
g2(x) = 0
(5.11)
with x ∈R3. The “level curves” of the function g1 are actually level surfaces,
typically forming nested shells of the sort that we already saw in ﬁgure 5.7,
and similarly for g2. The two sets of nested shells are shown in ﬁgure 5.12.
(The minimum points for g1 and g2 are not shown, but each lies in the center
of its set of nested shells.) Each constraint in (5.11) picks out one of the shells.
That shell, which we call the constraint surface, is a 2-dimensional surface
in 3-space. The feasible set is the intersection of the two constraint surfaces,
a 1-dimensional curve. It is the dark circle in ﬁgure 5.12.
At each point x in the ﬁgure, there is a gradient ∇g1(x) that points per-
pendicularly out from the surface of the g1 shell at that point, and there is
also a gradient ∇g2(x) pointing perpendicularly out from the surface of the g2
shell at the same point; the two gradients usually point in diﬀerent directions.
Figure 5.13 illustrates for a point x lying in the curve C that represents the
feasible set, where both g1 = 0 and g2 = 0.
In three-dimensional space, there are many vectors that are perpendicular
to a one-dimensional curve like C. Together, they form a normal plane that
looks like a collar that the curve passes through perpendicularly. An example
can be seen in ﬁgure 5.14. Every vector in the normal plane is perpendicular
to the curve at point x. To deﬁne a plane, only two (non-parallel) vectors are
required. Any two non-parallel vectors that are perpendicular to the curve C
at point x suﬃce to deﬁne the plane perpendicular to C.
Since C is the intersection of the two surfaces g1(x) = 0 and g2(x) = 0,
we know that the gradients ∇g1(x) and ∇g2(x), which are perpendicular to

Mathematics for Boundary-Oriented Methods
85
g =0
1
g =0
2
g 1
C
x
g 2
FIGURE 5.13
The gradient ∇g1(x) is perpendicular to the surface g1 = 0 at point x, and
∇g2(x) is perpendicular to the surface g2 = 0 at point x. Since point x lies
both on the surface g1 = 0 and on the surface g2 = 0, it lies in the curve C
that is the intersection of the two surfaces.
their respective constraint surfaces, are both perpendicular to C. Hence they
can serve as the two vectors that deﬁne the plane perpendicular to C at x. In
short, we can deﬁne the plane perpendicular to C at x as the set of vectors
of form:
α1∇g1(x) + α2∇g2(x)
in which α1 and α2 are allowed to take on any real value.
In the case of an arbitrary number of constraints, in an arbitrary-dimensional
space, we have multiple surfaces gi(x) = 0, each with a normal vector ∇gi(x).
The feasible set is the intersection C of the constraint surfaces. The surface
perpendicular to C at point x consists of all vectors of form:
X
i
αi∇gi(x).
The objective function f(x) has the same domain as the constraint func-
tions. In the example we have been using, it is R3. The “level curves” of
f are two-dimensional surfaces in R3, and the gradient ∇f(x) is the vector
normal to the level surface at x. It points away from the minimizer of f.
In constrained optimization, we are constrained to stay within the feasible
set. We can view the problem as one of moving along the feasible-set curve
C, looking for a point that minimizes the value of f. That point will not in
general be the global minimizer, but will be the “closest” to it that we can
reach without moving oﬀthe feasible curve.
Consider any point x on the feasible-set curve C (ﬁgure 5.14). There is a
normal plane to C at x, representing the set of vectors perpendicular to the

86
Semisupervised Learning for Computational Linguistics
b
a
D
f
C
FIGURE 5.14
A point on the feasible-set curve C. The nested shells are the level curves of
f. The gradient ∇f consists of a component a in the direction of the tangent
to C and a component b in a direction lying in the plane that is normal to C.
curve. In most cases, the level surface of f intersects C at an angle, as shown
in the ﬁgure. Then we can view the gradient of f as the sum of two vectors.
One of them is perpendicular to C, and one of them points along C in the
direction in which f is increasing. To minimize f, we should move in the
opposite direction, in the direction along the curve in which f is decreasing.
We have reached a minimum point when there is no direction of further
decrease (ﬁgure 5.15). At that point, the gradient of f lies entirely in the plane
that is perpendicular to the feasible-set curve. That is, at the constrained
minimum x, we have:
∇f(x) =
X
i
αi∇gi(x).
(5.12)
If this equation can be solved for x, we have solved the constrained optimiza-
tion problem.
The constrained minimization of f under equality constraints gi(x) = 0 is
equivalent to an unconstrained minimization problem. The Lagrangian of the
constrained minimization is the function:
L(x) = f(x) −
X
i
αigi(x).
(5.13)
To do an unconstrained minimization of the Lagrangian, we set its gradient
to 0 and solve for x:
∇L(x) = 0.
For any component xk, we have
∂
∂xk
"
f(x) −
X
i
αigi(x)
#
=
∂
∂xk
f(x) −
X
i
αi
∂
∂xk
gi(x).

Mathematics for Boundary-Oriented Methods
87
C
D
f
FIGURE 5.15
At the constrained minimum, the gradient ∇f lies completely in the normal
plane.
So the gradient, being the vector of partial derivatives, can be expressed as
∇L(x) = ∇f(x) −
X
i
αi∇gi(x).
Setting the gradient to zero leads us to the same equation (5.12) that we solved
for the constrained minimization of f. That is, unconstrained minimization
of the Lagrangian is equivalent to (has the same solution as) constrained
minimization of the original objective function.
5.3.3
Inequality constraints
Recall that an inequality constraint g(x) ≤0 deﬁnes a convex set whose
boundary is the level surface g(x) = 0. The set of feasible points is the inter-
section of the convex sets deﬁned by the constraints. For example, the shaded
area in ﬁgure 5.16 shows the feasible points in a case with two constraints
g1 ≤0 and g2 ≤0. The goal is to minimize a function f within the feasible
set.
Now consider an example in which f has a three-dimensional domain, and
there are two inequality constraints g1 ≤0 and g2 ≤0.
Each constraint
describes a convex solid in the domain. The feasible set is the intersection of
the two convex solids, a lentil-shaped solid (ﬁgure 5.17). We wish to minimize
the value of f, while remaining within the feasible set.
For this example, there are three cases that can arise.
Case 1.
The global minimizer of f lies within the feasible set, as in ﬁgure
5.17a. Then we can simply ignore the constraints. Solving the unconstrained

88
Semisupervised Learning for Computational Linguistics
g 1 ≤ 0
g 2 ≤ 0
FIGURE 5.16
The intersection of two inequality constraints. The feasible set is the shaded
region.
b +
a +
c +
FIGURE 5.17
Three cases: (a) minimum of f within feasible set, (b) minimum of f near
surface g1 = 0, (c) minimum of f near curve g1 = g2 = 0.

Mathematics for Boundary-Oriented Methods
89
problem ﬁnds the global minimizer, which is also the solution to the con-
strained problem.
Case 2.
The global minimizer lies outside the feasible set, but nearer to one
of the two surfaces of the “lentil” than to its edge. Suppose the minimizer lies
near the g1 surface (ﬁgure 5.17b). Let us pick any point within the feasible
set, and follow the gradient of f. The gradient points away from the global
minimizer of f, so we move “upstream,” against the direction of the gradient.
If we start at a point in the interior of the feasible set, following the gradient
takes us to the surface nearest to the global minimizer. We are not allowed
to move out of the feasible set, so we move along the surface, as long as there
is a component of ∇f that points along the surface. When we reach a point
where ∇f is entirely perpendicular to the surface, we have found the point
that minimizes f within the feasible set.
In this case, the g1 constraint matters, but we can ignore the g2 constraint.
The g1 constraint is said to be binding, and the g2 constraint is slack. The
binding constraint behaves like an equality constraint: we follow the constraint
surface g1(x) = 0 until we ﬁnd a point satisfying ∇f = α1g1(x). Notice that
this is just (5.12) where we limit our attention to binding constraints.
Alternatively, we can describe the solution to the problem with inequality
constraints as being the solution to (5.12) where we set αi = 0 for each slack
constraint. This description is also valid for Case 1. In Case 1, where the
global minimizer lies in the interior of the lentil, all of the constraints are
slack, and the solution is found by solving ∇f = 0, which is to say, by solving
(5.12) where αi = 0 for all i.
Case 3.
The ﬁnal case is illustrated in ﬁgure 5.17c. Here the global mini-
mizer is near the edge of the lentil. Following the gradient of f leads to the
edge, and we follow the gradient along the edge until we ﬁnd the point that
satisﬁes (5.12). Note that the edge is the curve that satisﬁes both g1(x) = 0
and g2(x) = 0. In this case, the problem is equivalent to a problem in which
all the inequalities are replaced with equalities. None of the constraints are
slack.
We conclude that a solution to the constrained problem is a point that
satisﬁes (5.12), where αi = 0 for all slack constraints. Notice that binding
constraints are ones that satisfy g(x) = 0 at the solution point x. Hence we
can express the requirement that αi = 0 for slack constraints by requiring
that either gi(x) = 0 (the constraint is binding) or αi = 0.
Even more
compactly, this can be expressed as a set of equations known as the Kuhn-
Tucker conditions:
αigi(x) = 0
for all i.
(5.14)
In short, a solution to the constrained problem is a point that satisﬁes both
(5.12) and (5.14).

90
Semisupervised Learning for Computational Linguistics
When the constraints are inequality constraints, we actually know a little
more about the coeﬃcients αi. Let us say that a point x is a boundary point
for constraint gi if gi(x) = 0. Every point on the boundary of the feasible set
is a boundary point for at least one constraint. If x is a boundary point for
gi, then the gradient of gi points away from the interior of the feasible set at
x.
Now suppose that there exist binding constraints and x is the constrained
minimum. Since binding constraints exist, the minimum of f lies outside of
the feasible set, hence the gradient of f at x points toward the interior of
the feasible set. The binding constraints are the ones that have x on their
boundary, and their gradients point away from the interior, that is, in the
opposite direction of ∇f. It follows that αi < 0 in (5.12) for every binding
constraint, and we know that αi = 0 for every slack constraint; hence
αi ≤0
∀i.
(5.15)
Let us sum up. The problem of interest is
minimize f(x)
subject to gi(x) ≤0
∀i.
(5.16)
It is conventional to reverse the signs of the Lagrange multipliers, and write
the Lagrangian as
L(x) = f(x) +
X
i
aigi(x)
(5.17)
where ai = −αi. With the reversal of signs, (5.15) becomes
ai ≥0
∀i.
(5.18)
The solution for (5.16) is the same as the solution for
minimize L(x)
(5.19)
with no constraints. Namely, the solution is a point x∗that satisﬁes
∇f(x∗) +
X
i
ai∇gi(x∗) = 0.
(5.20)
We also know that the following is true of the solution point x∗:
aigi(x∗) = 0
∀i.
(5.21)
A solution satisfying (5.20), (5.21), and (5.18) exists provided that the feasible
set is nonempty.

Mathematics for Boundary-Oriented Methods
91
5.3.4
The Wolfe dual
Let us deﬁne a as the vector (a1, . . . , ak), and think of the Lagrangian as a
function of both x and a:
L(x, a) = f(x) +
X
i
aigi(x).
The solution for the constrained problem (5.16) is the same as the solution
for the unconstrained problem (5.19). Let x∗be the solution to (5.19) and let
a∗be the corresponding Lagrange multipliers. (Properly speaking, they are
negatives of Lagrange multipliers, but we will use the term a little loosely.)
The solution points x∗and a∗satisfy
L(x∗, a∗) = min
x,a L(x, a).
We have just seen that we can ﬁnd x∗by setting the gradient of the La-
grangian to zero, that is, by solving (5.20) for x∗. But it is possible that
solving (5.20) fails to eliminate the Lagrange multipliers ai. That is, solving
(5.20) may give us an expression for x∗as a function of a. The basic problem
is that the Lagrangian is a function of both x and a, but solving (5.20) only
minimizes for x. Solving (5.20) actually yields a point x′ such that, for a ﬁxed
value of a:
L(x′, a) = min
x L(x, a).
Substituting the solution x′ back into the Lagrangian yields the value of the
Lagrangian at x′, as a function of a:
Q(a) ≡L(x′, a) = min
x L(x, a).
The question now is how to eliminate the dependence on a.
We know from (5.18) that all components of a∗are nonnegative, so let us
limit attention to points satisfying
ai ≥0
∀i.
Then ai ≥0 and gi(x) ≤0 for all i, and it follows that
Q(a) = min
x
"
f(x) +
X
i
aigi(x)
#
≤L(x∗, a).
Clearly, we have equality at the point a∗:
Q(a∗) = L(x∗, a∗).
Hence, we can eliminate the remaining dependency on a by maximizing Q(a)
to ﬁnd a∗. That is, we solve the problem:
maximize Q(a)
subject to ai ≥0
∀i.
(5.22)

92
Semisupervised Learning for Computational Linguistics
f
(a)
(b)
(c)
g ≤ 0
g ≤ 0
g ≤ 0
f = 0
FIGURE 5.18
Mixing f and a single constraint g. (a) When ||a|| = 0, the Lagrangian equals
f. (b) When ||a|| is very large, the Lagrangian approximately equals g. (c) As
||a|| increases from 0 to inﬁnity, the minimum of the Lagrangian rises and then
falls, reaching its maximum at the boundary of the feasible set.
This problem is called the Wolfe dual of the constrained problem (5.16).
Geometrically, this is what we are doing. Let us start out with a = 0.
Then the Lagrangian is equal to f(x), and minimizing the Lagrangian ﬁnds
the minimum of f. In the cases of interest, this lies outside of the feasible
set, as illustrated in ﬁgure 5.18a. For simplicity, we only show the case of a
single constraint g. Also note that ﬁgure 5.18, unlike the other ﬁgures in this
section, shows the graph space, using the z axis for the values of f and g. The
domain is the plane z = 0.
Increasing the magnitude of a changes the shape of the Lagrangian by mix-
ing in more and more of the g shape. When ||a|| is very large, g(x) overwhelms
f(x) in the Lagrangian, and the minimizer of the Lagrangian falls inside the
feasible set (ﬁgure 5.18b). By “the minimizer of the Lagrangian,” we mean
the point x′ such that
L(x′, a) = min
x L(x, a) = Q(a)

Mathematics for Boundary-Oriented Methods
93
for the particular choice of a.
The discussion leading up to the Wolfe dual (5.22) shows that, as we increase
||a||, and x′ moves from the exterior of the feasible set to the interior, the value
L(x′, a) ﬁrst increases, and then falls. It reaches its maximum exactly when
x′ lies on the boundary of the feasible set, which is when x′ = x∗(ﬁgure
5.18c).


6
Boundary-Oriented Methods
In both clustering and semisupervised learning, a broad distinction can be
made between approaches that build on the cluster hypothesis – the idea that
similar examples should have similar labels, or, in spatial terms, that nearby
examples should belong to the same class – and the separation hypothesis
– the idea that boundaries between classes should lie in sparsely populated
regions of the instance space. Approaches that build on the cluster hypothesis
seek to characterize what clusters look like. Approaches that build on the
separation hypothesis are not concerned with what clusters look like, but
rather with what boundary regions look like.
A related distinction is that between generative models and discriminative
models. A generative model describes how instances of each class are gen-
erated.
As we will see in chapter 7, one way of implementing the cluster
hypothesis is via a mixture model: choose a class with probability p(y), then
generate an instance with probability p(x|y). A probabilistic classiﬁer p(y|x)
arises as a posterior probability, derived by Bayes’ rule:
p(y|x) =
p(y)p(x|y)
P
y′ p(y′)p(x|y′).
By contrast, a discriminative model does not specify how to generate in-
stances, but constructs the classiﬁer ˆy = g(x) directly, by specifying how to
divide up the space into class regions. One way of doing so is by implementing
the separation hypothesis. Of course, any classiﬁer divides up the instance
space into class regions; the issue is whether it does so directly, rather than
indirectly by deﬁning a model to generate data points.
Vapnik [227] has prominently advocated the use of discriminative models
on the methodological grounds that one should not solve a harder problem
than necessary. Estimating the law governing the distribution of data points,
he argues, is harder than determining the boundaries between classes. Doing
it well requires more data, a particular issue when labeled data is scarce.
Let us consider a concrete example: the classiﬁcation problem illustrated in
ﬁgure 6.1. There are two classes, positive and negative. The methods we will
consider all construct linear boundaries, which is to say, hyperplanes. The
hyperplane separates the instance space into two half-spaces, one for each
class.
In “well-behaved” cases, as here, the instances belonging to each class form
two clusters with a clear separation between them. A linear boundary sepa-
95

96
Semisupervised Learning for Computational Linguistics
FIGURE 6.1
A linear separator between the positive and negative classes.
rates them cleanly. Less well-behaved cases are possible, in which, no matter
how we split the space, some examples will be on the “wrong side” of the
separating hyperplane. In the well-behaved case, the points are said to be
linearly separable. Boundary-oriented methods are often based on intuitions
that are clearest in the linearly separable case.
We have already seen that some generative methods (speciﬁcally, the Naive
Bayes algorithm) construct linear separators. A related generative method is
linear discriminant analysis. In linear discriminant analysis, it is assumed that
the data has a Gaussian (that is, normal) distribution. Restricting attention to
the simple case in which there is no correlation among the attributes, and the
data is evenly divided between the two classes, the assumption of normality
means that the data form two equal-sized circular clusters, as in ﬁgure 6.1. In
that case, it is optimal (and intuitively natural) to choose as linear separator
the bisector of the line segment connecting the centers of gravity of the two
clusters, as shown.
However, the linear separator of ﬁgure 6.1 requires us to know something
about how the data points are distributed. It is optimal only if the assump-
tions about data distribution are met. If the data distribution is not normal,
but heavily skewed, as in ﬁgure 6.2, linear discriminant analysis gives less
than perfect results. In ﬁgure 6.2a, bisecting the line connecting the centers
of gravity fails to correctly separate the two classes. But the failure is not due
to inseparability: the alternative linear separator in ﬁgure 6.2b does correctly
classify all the data.
Following Vapnik’s methodological precept, discriminative or boundary-
oriented methods avoid assumptions about the distribution of the points.
They focus on ﬁnding linear separators like that in ﬁgure 6.2b.
That is,
they seek linear gaps in the data, regardless of how the data are distributed
outside of the boundary region.

Boundary-Oriented Methods
97
+
–
+
–
(a)
(b)
FIGURE 6.2
(a) A linear separator constructed in the same way, but where the data are
heavily skewed. (b) A better linear separator for this data.
6.1
The perceptron
6.1.1
The algorithm
The perceptron learning algorithm is a very simple algorithm for directly
constructing a linear separator.
As discussed in the previous chapter, the
separator is represented by its normal vector. The perceptron algorithm be-
gins with a random vector w (or with the zero vector), and cycles repeatedly
through the training data, looking for instances that are misclassiﬁed. If we
ﬁnd a misclassiﬁed instance x, we update w in a direction that will get it
closer to making the correct prediction for x.
To be more speciﬁc, suppose there is a positively labeled example x that is
misclassiﬁed, as in ﬁgure 6.3. The point x is on the wrong side of the separator
(the negative side). There is an arrow running from the tip of w to x; this
is the vector x −w. This is the direction in which we would like to move w.
For example, the dotted vector and dotted separator are the result if we move
halfway along the x −w vector; that would suﬃce to classify x correctly.
This suggests the update:
w ←w + η(x −w)
= (1 −η)w + ηx
where η is a step size. For example, to move halfway along the x −w vector,
set η to 1/2. If we choose η = 1/2, we have
w ←1
2(w + x).

98
Semisupervised Learning for Computational Linguistics
w
x
–
x - w
+
FIGURE 6.3
Adjusting for a misclassiﬁed point.
As we observed at the end of section 5.1.1, scaling w does not aﬀect the
location of the separator, only the unit we use to measure distance, so we can
scale by two to obtain the simpler form:
w ←w + x.
This is the update rule used by the perceptron algorithm. One cycles through
the training data, and each time a misclassiﬁed example is encountered, one
adds it into the weight vector w. The process continues until no examples are
misclassiﬁed.
However, our discussion so far has only considered positive examples; we
must also consider examples with negative labels. There is a convenient trick
that allows us to reduce the negative case to the positive case. A misclassiﬁed
positive example is one for which
x · w ≤0.
A misclassiﬁed negative example is a negative instance for which x · w ≥0,
which is to say, for which
−x · w ≤0.
If we replace each negatively labeled instance x with the negative vector −x,
then the criterion is the same for all instances. That is, a negatively labeled
instance x is indistinguishable from a positively labeled instance −x.
Geometrically, taking the negative of a vector reﬂects it across the origin.
Figure 6.4 illustrates.
The negatively labeled instances in ﬁgure 6.4a are
reﬂected across the origin to obtain ﬁgure 6.4b, in which we do not need
to distinguish between positive and negative points.
We need only ﬁnd a
separator passing through the origin that has all the data points on its positive
side.

Boundary-Oriented Methods
99
(a)
+
+
+
+
–
–
–
(b)
FIGURE 6.4
(a) Reﬂecting negatively labeled examples across the origin. (b) The result.
The goal is now to ﬁnd a linear separator through the origin that has all
instances on one side.
6.1.2
An example
Consider the following data points, characterized by two features F and G;
the variable Y represents the label:
F G Y
x1 −1 2 −
x2 3
0 −
x3 0
4 +
x4 1
5 +
x5 2
2 +
.
We ﬁrst modify the data points by adding a −1 component:
x1 (−1, 2, −1) −
x2 (3, 0, −1) −
x3 (0, 4, −1) +
x4 (1, 5, −1) +
x5 (2, 2, −1) +
.
Then we fold the label into the instance:
x1 (1, −2, 1)
x2 (−3, 0, 1)
x3 (0, 4, −1)
x4 (1, 5, −1)
x5 (2, 2, −1)
.
We start the algorithm with a zero weight vector w = (0, 0, 0), and begin
cycling through the instances xi. At each point, we take the dot product

100
Semisupervised Learning for Computational Linguistics
xi · w. If the result is not positive, we add xi into w; otherwise we leave w
unchanged. When we have passed through all the instances, we go back to x1
and pass through them again. Here is the resulting history of updates:
w
xi
xi · w
(0, 0, 0)
(1, −2, 1)
0
(1, −2, 1) (−3, 0, 1)
−2
(−2, −2, 2) (0, 4, −1) −10
(−2, 2, 1) (1, 5, −1)
+7
(−2, 2, 1) (2, 2, −1)
−1
(0, 4, 0)
(1, −2, 1)
−8
(1, 2, 1)
(−3, 0, 1)
−2
(−2, 2, 2) (0, 4, −1)
+6
(−2, 2, 2) (1, 5, −1)
+6
(−2, 2, 2) (2, 2, −1)
−2
(0, 4, 1)
(1, −2, 1)
−7
(1, 2, 2)
(−3, 0, 1)
−1
(−2, 2, 3) (0, 4, −1)
+5
w
xi
xi · w
(−2, 2, 3) (1, 5, −1)
+5
(−2, 2, 3) (2, 2, −1)
−3
(0, 4, 2) (1, −2, 1)
−6
(1, 2, 3) (−3, 0, 1)
0
(−2, 2, 4) (0, 4, −1)
+4
(−2, 2, 4) (1, 5, −1)
+4
(−2, 2, 4) (2, 2, −1)
−4
(0, 4, 3) (1, −2, 1)
−5
(1, 2, 4) (−3, 0, 1)
+1
(1, 2, 4) (0, 4, −1)
+4
(1, 2, 4) (1, 5, −1)
+7
(1, 2, 4) (2, 2, −1)
+2
(1, 2, 4) (1, −2, 1)
+1
.
At this point, all ﬁve examples are correctly classiﬁed, and we are done. In
terms of the original data points (attributes F and G), the prediction is
f(x) = x · (1, 2) −4.
We can conﬁrm that it correctly classiﬁes all the original examples:
x
f(x) Y
(−1, 2) −1 −
(3, 0)
−1 −
(0, 4)
+4 +
(1, 5)
+7 +
(2, 2)
+2 +
.
6.1.3
Convergence
We wrote above that “the process continues until no examples are misclassi-
ﬁed.” How do we know that we will ever reach that point?
It is actually possible that no solution exists. It is certainly possible to mix
together positive and negative examples so that no linear boundary can sep-
arate them. If any linear separator exists that correctly classiﬁes the training
data, the data is said to be linearly separable.
If the data is linearly separable, it can be shown that the perceptron algo-
rithm will converge to a solution. To see why that is so, let us consider ﬁgure
6.5. The data points are the same as in ﬁgure 6.4b, but we have added two
separators b1 and b2, along with their normal vectors w1 and w2, respectively.
Both b1 and b2 are linear boundaries that classify all the data correctly. That

Boundary-Oriented Methods
101
b 1
w 2
w 1
b 2
FIGURE 6.5
Two solutions.
is, w1 and w2 are solutions to the perceptron learning problem. The separator
b1 is special because it cannot be rotated clockwise any further – it is bumping
up against one of the training examples. Similarly, the separator b2 cannot
be rotated counterclockwise any further. As a consequence, all the solution
vectors lie in the shaded region between w1 and w2.
Now let us consider the “average” solution vector, that points straight down
the middle of the solution region, halfway between w1 and w2. Call it wave.
Let us consider what happens in one step of the perceptron algorithm (ﬁgure
6.6). There is a misclassiﬁed instance x, and we update the current normal
vector w (not shown) by adding the vector x to it.
The vector x has a
component a that points in the direction of wave, and a component b that is
perpendicular to wave. As one cycles through diﬀerent misclassiﬁed instances,
the perpendicular component b will switch between pointing to the right or
pointing to the left, causing the current normal vector to swing back and
forth, but the component a will never reverse direction, because all the data
lies on the same side of the average separator. As a result, the component a
will accumulate. The current vector w will gradually increase in length, but
its direction will be determined more and more by the accumulation of copies
of a, and less and less by the components b, which cancel each other out.
That is, the vector w will make smaller and smaller swings, coming closer
and closer to wave. Eventually, it will land in the solution region, at which
point all the data is correctly classiﬁed and no more updates are made.
6.1.4
The perceptron algorithm as gradient descent
It is also possible to view the perceptron algorithm as doing a form of gradient
descent. At a given point, the weight vector w makes an error on instance xi

102
Semisupervised Learning for Computational Linguistics
b
a
x
w ave
FIGURE 6.6
Each misclassiﬁed instance x, viewed as a vector, consists of a component
a in the direction of the average solution vector wave and a component b
perpendicular to it.
just in case w · xi ≤0. That is, for misclassiﬁed instances xi, the amount of
error is
ei = −w · xi.
We wish to ﬁnd a value for w that minimizes the total error, or, more modestly,
we wish to ﬁnd a direction in which to move w so as to reduce error. That
is, we would like to know the gradient of the error function, and move w in
the direction opposite to the gradient.
The error and squared error are monotonically related, and dealing with
squared error is more convenient.
We can collect the individual errors ei
together into a vector e = (e1, . . . , em). We include only instances on which
an error is actually made (ei ≥0). We also collect those instances together
as rows of a matrix X. We can write
e = −ˆy = −Xw.
The total squared error is
E = eTe.
Note that this is the same as squared error in linear regression (section 5.2.4)
if we take the target vector y = 0, yielding e = ˆy. Taking the gradient goes
the same way:
∇E = DweTe
= 2eTDwe
= 2eTDw(−Xw)
= −2eTX.

Boundary-Oriented Methods
103
To decrease E, we wish to move w in the direction −∇E. We will actually
use −(1/2)∇E, which is a vector pointing in the same direction:
w ←w + ηeTX
where η is the “learning rate” or step size.
The product eTX is a row vector times a matrix whose rows are instances.
The result is a row vector that is a linear combination of the rows of X, in
which each row xi is weighted by the corresponding error ei:
eTX =
X
i
eixi.
That is
w ←w + η
X
i
eixi.
(6.1)
We can factor η as η = αβ such that the values βei sum to one. That makes
it plain that (6.1) moves w in a direction that is a weighted average of the
misclassiﬁed instance vectors xi, with step size α and weights βei.
In the perceptron algorithm, instead of moving w all at once in the direction
of the weighted average, we move w in the direction xi for each instance vector
in the weighted average, one at a time:
w ←w + ηieixi.
(6.2)
One can think of ηi as a step size that is diﬀerent for diﬀerent instances;
the perceptron algorithm chooses ηiei = 1. The stepwise update (6.2) turns
out to have the same eﬀect as the simultaneous update (6.1). The stepwise
update is known as stochastic gradient descent, because in one version of it,
we choose an instance xi at random according to the weight ei, and update
w with the randomly chosen instance. The systematic visiting scheme of the
perceptron algorithm, cycling through all the instances in turn, has the same
eﬀect.
6.2
Game self-teaching
In the discussion of algorithms related to self-training, in section 2.2.5, we
mentioned self-teaching in the sense of a program teaching itself to play a
game by playing against itself. (There is a diﬀerent setting that is also called
self-teaching; it will be discussed in section 9.2.)
The best-known implementation of game self-teaching is Tesauro’s backgam-
mon program TD-Gammon [218], though he cites Samuel [200] for the original
idea.

104
Semisupervised Learning for Computational Linguistics
In TD-Gammon, the player is based on a function for evaluating a board
position. For example, if w is a weight vector and x is a vector representing
relevant features of the board position, w·x can be viewed as a scalar predicted
board value that attempts to approximate the probability that the game will
end in a win for White. Though a win for White implies a loss for Black, TD-
Gammon actually uses two evaluation functions, one for White and one for
Black. Also, the TD-Gammon evaluation functions actually use a multilayer
perceptron rather than a single perceptron, but the principle is the same. At
each step, an exhaustive search is done (every legal move is examined) to
ﬁnd the move that yields a board position that maximizes the probability of
winning.
The TD-Gammon learner uses a record of a game to improve its evaluation
functions. One can use records of games played by humans (supervised learn-
ing), but interestingly, one can also learn very eﬀectively from games that are
generated by playing the current evaluation function against itself. Learning
proceeds as follows. The only feedback that the learner receives is the win or
loss at the end of the game. The target of learning is an evaluation function
that returns the probability that the game will end in a win. The exact value
is known only at the end of the game, where it is either one (a win) or zero (a
loss). The learner propagates that information backward through time, over
the course of many training games.
The method of information propagation used by TD-Gammon is known as
temporal diﬀerence (TD) learning. Let yt represent the estimated win proba-
bility at the t-th time step. If each player plays perfectly, then any diﬀerence
|yt+1−yt| is due either to poor estimation of yt or to the stochasticity in the roll
of the dice. Hence learning takes the form of adjusting the evaluation-function
weights w at each time step so as to minimize the diﬀerence |yt+1 −yt|. Dif-
ferences due to poor estimation will gradually be eliminated, and diﬀerences
due to the roll of the dice will be averaged out. TD-Gammon starts out with
a random vector w, and improves it by repeatedly playing games and propa-
gating the win/loss information backward through time to make adjustments
to w.
At each time t, the gradient (∇y)t with respect to w is a vector pointing
in the direction in the weight space that generates the most rapid increase in
y. Note that there is a diﬀerent gradient at each time step: the best way to
change w depends on the board position. The ﬁnal time is T, and the value
yT is known. Working backward through time, the weights are adjusted by
taking a small step in the direction that moves yt toward yt+1. If yt < yt+1,
then w is moved in the direction (∇y)t, and if yt > yt+1, then w is moved in
the opposite direction. In general, the update is
w ←w + η(yt+1 −yt)(∇y)t
(6.3)
where η is the learning rate. Note the similarity to the perceptron update
(6.2), viewing yt−yt+1 as the error of the evaluation function on the t-th board

Boundary-Oriented Methods
105
position, and viewing xi as the perceptron’s approximation to the negative of
the gradient.
Game self-teaching as thus formulated is a semisupervised learning algo-
rithm. Concatenating together the board positions from many games, we have
training instances (x1, . . . , xn). The true values of the evaluation function are
(y1, . . . , yn), representing the true probability of winning if both players play
perfectly in the corresponding board position. The values yt range over the
real interval [0, 1]; that is, the problem is a regression problem rather than
a classiﬁcation problem. If all values were known, it would be a supervised
learning problem. In fact, only some of the values are known, namely, the
ones corresponding to ends of games, making the problem semisupervised.
Game self-teaching has notable similarities to self-training. In lieu of true
values, the algorithm takes the predictions of its current evaluation function
at face value – or more precisely, it takes the predicted value ˆyt+1 at face value
as the target for time t, on the assumption that the true value is the same at
both t and t + 1. In that respect, it is like self-training without a threshold.
However, it also diﬀers from self-training. The fact that game self-teaching
learns a real-valued function, the time-sequence nature of the data, and the
speciﬁc assumptions underlying the update equation (6.3), all distinguish the
TD-Gammon version of game self-teaching from self-training.
6.3
Boosting
Boosting is another classiﬁer learning method that constructs a linear sepa-
rator. It is related to Support Vector Machines (SVMs), and has comparable
performance over a wide range of problems, but is rather simpler to explain.
As with perceptrons and, recalling the discussion of section 5.1.4, as with
the Naive Bayes algorithm, we take a model to be represented by a weight
vector w, and we deﬁne the conﬁdence-weighted prediction of the classiﬁer to
be
f(x) = x · w.
(6.4)
The sign of f(x) is the predicted class, and the absolute value is interpretable
as conﬁdence.
Consider a particular instance x, and suppose that the correct label is y.
We assume that y is either +1 or −1. If y is positive, then the classiﬁer makes
an error just in case x · w ≤0. If y is negative, then the classiﬁer makes an
error just in case x · w ≥0, or equivalently, just in case:
(x · w)y ≤0.
Deﬁne u = (x · w)y. With labeled training data, x and y are given, and u
varies as we vary w. Figure 6.7 plots the “error function” as a function of u

106
Semisupervised Learning for Computational Linguistics
err
0
z
u
FIGURE 6.7
The error function as a function of u = (x · w)y.
for a single training instance. It is a step function: the number of errors is
one if u ≤0 and zero if u > 0.
We would like to minimize err, but minimizing it directly is diﬃcult because
it is a step function.
It is much more convenient if we have a continuous
function to minimize; its slope tells us which direction to move in order to
improve things. The basic idea of boosting is to use an exponential upper
bound for err as the objective function to minimize. This is also shown in
ﬁgure 6.7: it is the line marked “z.” It is deﬁned as
z = e−(x·w)y.
When u is zero, we have z = e−u = 1, and the negative sign makes z decrease
as u increases. We can think of z as a cost. If the current model classiﬁes
the instance correctly, then z < 1, and if the current model misclassiﬁes the
instance, then z > 1. The cost varies exponentially with conﬁdence.
In a complete training set, there are multiple instances xi, each having its
own cost:
zi = e−(xi·w)yi.
Since err(xi) ≤zi for each training instance xi, the total cost:
Z =
X
i
zi =
X
i
e−(xi·w)yi
(6.5)
is an upper bound for the total number of errors on the training data:
X
i
err(xi) ≤Z.
Z is the objective function that boosting seeks to minimize.

Boundary-Oriented Methods
107
+
–
–
–
+
+
+
FIGURE 6.8
Costs interpreted as applying pressure to the current separator.
Geometrically, xi · w is the distance from the linear separator to the point
xi. The cost is sensitive only to that distance. It decreases the deeper an
instance is in the “right” half-space (the side of the separator that agrees with
the instance’s label), and increases the deeper an instance is in the “wrong”
half-space.
We can think of the cost as exerting a pressure on the separator.
In a
manner of speaking, correctly classiﬁed examples push the separator away
in their attempt to move deeper into the right half-space, and misclassiﬁed
examples pull on the separator, so as to move themselves out of the wrong half-
space. The amount of pressure depends on the cost; misclassiﬁed examples
always exert more pressure than correctly classiﬁed examples.
Figure 6.8
illustrates. The net eﬀect in this case is to rotate the separator clockwise.
Boosting assumes that we have a collection of “weak predictors” available.
A weak predictor h takes an instance as input and makes a prediction +1
or −1 concerning the label. The predictor h is “weak” in the sense that its
prediction may not be very good. The goal is to ﬁnd a combination of weak
predictors that together do a better job than any of them separately.
Alternatively, we can think of weak predictors simply as features. We rep-
resent an instance as a vector of values, one for each weak predictor. Let ω
be an instance. We represent it as the vector:
x = (h1(ω), . . . , hk(ω)).
This is a vector of positive and negative ones.
The weight vector consists of a weight for each weak predictor:
w = (w1, . . . , wk).
The basic idea is that good predictors have larger weight and poorer predic-
tors have smaller weight. The combined prediction is the weighted average

108
Semisupervised Learning for Computational Linguistics
of the weak predictions hi(ω), in which each prediction is weighted by the
corresponding weight wi. That is, the combined prediction is simply x · w, in
concordance with what we have already assumed, in equation (6.4).
For example, suppose we have three instances and four weak predictors, as
follows. We also include a column for the labels.
ω h1(ω) h2(ω) h3(ω) h4(ω) y
ω1 +1
+1
−1
−1
−1
ω2 +1
−1
−1
+1
+1
ω3 −1
+1
+1
+1
−1
Each row corresponds to a value for x. For example, the ﬁrst row corresponds
to
x1 = (+1, +1, −1, −1).
Suppose that the current weight vector is
w = (.5, −.1, −.2, .4).
Then
x1 · w = (+1)(.5) + (+1)(−.1) + (−1)(−.2) + (−1)(.4) = .2.
The cost for the ﬁrst instance is:
z1 = e−(.2)(−1) = 1.22.
Its cost is greater than one, indicating that the instance is misclassiﬁed.
Boosting is an iterative algorithm, like the perceptron algorithm. In each
iteration, we wish to pick a weak predictor hj and adjust its weight by adding a
quantity δ (which may be either positive or negative). We evaluate a candidate
hj and adjustment δ by considering what the total cost will be if we make
the adjustment. The idea is to cycle through all the weak predictors, and for
each one, determine the optimal adjustment δ, and the total reduction in cost
achievable. We keep the one that maximizes total cost reduction, and fold δ
into the weight vector. Then the process is repeated.
The new weight vector w′ will be just like the old one w except that it
has wj + δ in place of wj. For example, suppose that we choose h2 and set
δ = +.3. The new weight vector will be
w′ = (.5, −.1 + .3, −.2, .4).
The new dot product with instance x1 will be
x1 · w′ = (+1)(.5) + (+1)(−.1 + .3) + (−1)(−.2) + (−1)(.4)
= [(+1)(.5) + (+1)(−.1) + (−1)(−.2) + (−1)(.4)] + (+1)(.3)
= x1 · w + δh2(x1).

Boundary-Oriented Methods
109
Here w is the current weight vector and w′ is what it will be if we adopt
the proposed change. Also, we have written “h2(x1)” instead of “h2(ω1).”
Applied to the vector x1, the weak predictor h2 should be understood as
simply extracting the second component.
In general, the new cost for the i-th instance will be
z′
i = e−[xi·w+δh(xi)]yi
= e−(xi·w)yie−δh(xi)yi
= zie−δh(xi)yi.
The total new cost Z′ is
Z′ =
X
i
zie−δh(xi)yi.
(6.6)
There is a way of expressing Z′ that will be very useful. Let us deﬁne A to
be the total cost of instances that the candidate predictor h gets right. The
candidate predictor h classiﬁes the i-th instance correctly if h(xi) = +1 and
yi = +1, or if both equal negative one. That is, h classiﬁes xi correctly if
h(xi)yi = +1. Hence we deﬁne:
A =
X
i
[[h(xi)yi = +1]]zi.
Note that zi is the current cost, not the “new” cost. We deﬁne B similarly to
be the total cost of the remaining instances:
B =
X
i
[[h(xi)yi = −1]]zi.
Now let us split the sum in (6.6) into a sum over the instances that h gets
right and the ones it gets wrong:
Z′ =
X
i
[[h(xi)yi = +1]]zie−δh(xi)yi +
X
i
[[h(xi)yi = −1]]zie−δh(xi)yi
=
X
i
[[h(xi)yi = +1]]zie−δ(+1) +
X
i
[[h(xi)yi = −1]]zie−δ(−1)
= Ae−δ + Beδ.
(6.7)
Next let us consider how to determine an optimal adjustment for a given
candidate predictor h. We want to ﬁnd the value δ that minimizes Z′. Ac-
cordingly, we take the derivative dZ′/dδ and set it to zero:
dZ′
dδ = A d
dδ e−δ + B d
dδ eδ
= Beδ −Ae−δ.

110
Semisupervised Learning for Computational Linguistics
Setting this to zero, we obtain
Beδ = Ae−δ
e2δ = A
B
δ = 1
2 ln A
B .
This gives us the optimal adjustment for a given candidate predictor h.
To determine the new cost if we adopt h, substitute the optimal value for
δ into (6.7):
Z′ = Ae−(1/2) ln(A/B) + Be(1/2) ln(A/B)
= A(A/B)−(1/2) + B(A/B)(1/2)
= A
p
B/A + B
p
A/B
= 2
√
AB.
(6.8)
In short, the algorithm is this. At each iteration, cycle through all weak
predictors hj. Compute A and B for hj, and compute the new cost if we adopt
hj as 2
√
AB. Keep the hj that minimizes the new cost. (Actually, since the
new cost is a monotone function of AB, it suﬃces to choose the candidate that
minimizes AB.) Compute the optimal adjustment for the winning candidate
hj as δ = (1/2) ln(A/B), and adjust the weight vector w by doing the update:
wj ←wj + δ.
This process is repeated until a stopping criterion is met. Often one simply
ﬁxes a number of rounds in advance (100, 500, or 1000 rounds being common
choices), though other stopping criteria are possible, such as those discussed
in section 2.2.2. The resulting ﬁnal weight vector w deﬁnes the classiﬁer, with
its prediction on an arbitrary instance x being the sign of x · w.
6.3.1
Abstention
Boosting can readily be generalized to include rules (that is, weak predictors)
that abstain. A predictor h abstains from making a prediction on instance x
if h(x) = 0. That is, we allow predictors to range over the values {−1, 0, +1}
instead of just {−1, +1}.
Now instead of dividing the training instances into those on which a candi-
date h makes the correct prediction and those on which it makes the wrong
prediction, we must also include a third set: those on which it abstains. We
deﬁne A and B as before, but we add a third value C:
C =
X
i
[[h(xi)yi = 0]]zi.

Boundary-Oriented Methods
111
The expression for the new cost becomes:
Z′ =
X
i
[[h(xi)yi = +1]]zie−δ(+1) +
X
i
[[h(xi)yi = −1]]zie−δ(−1)
+
X
i
[[h(xi)yi = 0]]zie−δ(0)
= Ae−δ + Beδ + C.
(6.9)
Since the new term C is insensitive to the choice of δ, the optimal value for δ
is unaﬀected: it is
δ = 1
2 log A
B .
(6.10)
Substituting this value into (6.9) yields
Z′ = 2
√
AB + C.
(6.11)
A round of boosting goes as before, with the exception that one scores a
candidate using (6.11) instead of (6.8). The candidate hj that minimizes the
Z′ of (6.11) is selected, and its weight is adjusted by the δ of (6.10).
Intuitively, the revised new-cost value (6.11) implies a possible trade-oﬀ
between the accuracy of the predictor and its coverage. An accurate predictor
has a small value for AB, and a broad-coverage predictor has a small value
for C. The best predictor is both accurate and broad-coverage.
6.3.2
Semisupervised boosting
A simple way of applying boosting to a mix of labeled and unlabeled data
is the following. The learning algorithm receives as input a set of instances
x1, . . . , xm+n, the ﬁrst m of which are labeled y1, . . . , ym; the last n instances
are unlabeled. To extend the objective function (6.5) to the unlabeled in-
stances, we pretend that the unlabeled instances are labeled correctly, no
matter what the classiﬁer predicts. That is, the objective function becomes
m
X
i=1
exp{−w · xiyi} +
m+n
X
i=m+1
exp{−|w · xi|}.
(6.12)
Geometrically, what is going on is illustrated by the hypothetical example
in ﬁgure 6.9. The points represent instances, of which some are labeled and
some are unlabeled. In ﬁgure 6.9, all of the labeled instances are correctly
classiﬁed, whether we consider hyperplane “A” or hyperplane “B.” That is,
for either hyperplane, all labeled instances are in the half-space where they
belong.
Recall the metaphor that correctly classiﬁed points push the separating
hyperplane away, in order to move deeper into the correct half-space. The
ones that are closest to the hyperplane push hardest, with the eﬀect that the

112
Semisupervised Learning for Computational Linguistics
A
–
–
+
+
B
FIGURE 6.9
The “A” hyperplane considers only labeled points; the “B” hyperplane con-
siders all points.
hyperplane settles into the middle of the gap between the positive and negative
points, as far away from the nearest points as possible. If the unlabeled points
are ignored, the result is the “A” hyperplane.
Now suppose that we assign positive labels to the unlabeled instances in
the positive half-space, and negative labels to the unlabeled instances in the
negative half-space. We can think of the (originally) unlabeled instances as
having “soft” labels. Whenever we move the separating hyperplane, the soft
labels automatically change so as to agree with the predictions of the hyper-
plane. By contrast, the (originally) labeled instances have “hard” labels that
do not change.
Taking all the instances into account, “B” is the better hyperplane: among
all linear separators that get the hard-labeled instances right, “B” lies in the
center of the largest “gap” in the data; it lies as far as possible from the
nearest instances, whether labeled or unlabeled. The diﬀerence between “A”
and “B” is signiﬁcant because the two hyperplanes make diﬀerent predictions
about the points in the shaded region.
To ﬁnd the “B” hyperplane, we need to minimize the objective function
(6.12). Unfortunately, the absolute value makes it diﬃcult to ﬁnd a global
minimizer. Deﬁne
Z(w, y) =
m+n
X
i=1
exp{−w · xiyi}.
(6.13)
Minimizing (6.12) is equivalent to minimizing the objective function
Z(w) = min
y Z(w, y)
where y ranges over labelings that agree with the labeled training data but
vary in the labels assigned to unlabeled instances. Naively, one might con-

Boundary-Oriented Methods
113
sider enumerating all such labelings, ﬁnding the hyperplane w that minimizes
Z(w, y) for each labeling y, and choosing the labeling that gives the lowest
minimum for Z. Unfortunately, the number of diﬀerent labelings is 2n, for
n the number of unlabeled instances, hence minimization by enumeration is
intractible.
One possibility, suggested by Bennett et al. [15], is reminiscent of self-
training. One alternates boosting iterations and relabeling. Before each boost-
ing iteration, use the old weight vector w to assign labels to the unlabeled
instances, producing a complete labeling y. Then do a round of boosting to
update w in the usual way, using objective function (6.13), holding y constant.
Then repeat: use the new weight vector to relabel the unlabeled instances,
then use the new labeling to adjust the weight vector. Alternating rounds of
relabeling and boosting continue until a stopping criterion is met.
6.3.3
Co-boosting
Another semisupervised variant of boosting is known as co-boosting. Like
co-training, co-boosting assumes that there are two “views” of each instance,
and that one seeks agreement between predictors trained on the separate
views. Co-boosting incorporates agreement explicitly into the objective func-
tion. As before, the algorithm receives as input instances x1, . . . , xm+n, the
ﬁrst m of which are labeled y1, . . . , ym, and the remaining n of which are
unlabeled. We represent the two views by partitioning the weak predictors
into view-1 predictors and view-2 predictors. The goal is to construct a pair
of conﬁdence-weighted classiﬁers, represented by weight vectors w1 and w2,
where w1 has non-zero components only for view-1 predictors, and w2 has
non-zero components only for view-2 predictors.
For labeled instances, the objective function is (6.5), as in regular boosting,
except that terms are required for each of the two classiﬁers separately. For
unlabeled instances, we seek to minimize the disagreement between the two
classiﬁers, which we quantify by a term of the same form as (6.5), but sub-
stituting the prediction of the other-view classiﬁer in place of the label. That
is, the objective function is:
Z =
m
X
i=1
exp{−w1 · xiyi} +
m
X
i=1
exp{−w2 · xiyi}
+
m+n
X
i=m+1
exp{−w1 · xiˆy(2)
i
} +
m+n
X
i=m+1
exp{−w2 · xiˆy(1)
i
}
(6.14)
where ˆy(1)
i
represents the sign of w1 ·xi and ˆy(2)
i
represents the sign of w2 ·xi.
To minimize Z, the procedure is very similar to the regular boosting algo-
rithm. One alternates between the two views. One of the classiﬁers is the
current classiﬁer, and the other is ﬁxed. Suppose that classiﬁer 1 is ﬁxed and
classiﬁer 2 is current. The ﬁrst term on the right-hand side of (6.14) involves

114
Semisupervised Learning for Computational Linguistics
only classiﬁer 1, so it can be ignored. In principle, we should optimize over all
three remaining terms. For simplicity, though, we also take the predictions
ˆy(2)
i
to be ﬁxed, and ignore the third term as well as the ﬁrst. This makes the
approach heuristic, but tractable. We are left with
m
X
i=1
exp{−w2 · xiyi} +
m+n
X
i=m+1
exp{−w2 · xiˆy(1)
i
}
as objective function.
If we deﬁne yi to be equal to ˆy(1)
i
for each of the
unlabeled instances m + 1 ≤i ≤m + n, then this can be written simply as
m+n
X
i=1
exp{−w2 · xiyi}
which is exactly the objective function for a given round of regular boosting. It
is optimized in the same way, by considering each view-2 predictor hk in turn,
computing δ according to (6.10), and choosing the attribute that minimizes
the new value of Z according to (6.11). Weight vector w2 is updated with the
adjustment wk ←wk +δ for the chosen predictor hk to complete the iteration.
The process continues in this fashion, interchanging the roles of view 1 and
view 2 after each iteration, until a stopping criterion is met.
6.4
Support Vector Machines (SVMs)
6.4.1
The margin
We noted earlier that, among instances that are correctly classiﬁed by the
hyperplane, boosting assigns the most weight to the ones that are nearest
to the hyperplane.
As a result, when there are multiple hyperplanes that
correctly classify the training data, boosting tends to ﬁnd one that lies in the
middle of the linear gap between the positives and negatives. For boosting,
that is only a tendency. A support vector machine (SVM) learner has the
explicit objective of ﬁnding the hyperplane that maximizes the distance to
the nearest data points.
Figure 6.10 illustrates why that is a desirable objective. It shows two al-
ternative hyperplanes for the same data set. Both hyperplanes classify all the
training data correctly, but the “B” hyperplane only just succeeds in doing so.
It cuts very close to certain data points; nudging those points even a little bit
would put them on the wrong side of the hyperplane. By contrast, the “A”
hyperplane is located as far as possible from any data point, and is therefore
much more robust to small changes in location of the data points.

Boundary-Oriented Methods
115
M
A
+
–
(a)
M
B
+
–
(b)
FIGURE 6.10
Two separating hyperplanes for the same data set.
As is now familiar, a hyperplane is deﬁned by a normal vector w and a
distance r from the origin. The signed distance from the hyperplane to a
point x is given by equation (5.1), repeated here for convenience:
d = x · w
||w|| −r.
The value d is positive for points on the positive side of the hyperplane,
negative for points on the negative side, and zero for points on the hyperplane.
Thus, if y ∈{−1, +1} is the point’s label, then d · y is positive for points that
are in the correct half-space, and negative for points that are in the wrong
half-space. We deﬁne the signed margin of the point xi with label yi to be
mi = yi
||w||(xi · w −ρ)
where
ρ = r||w||.
If xi is on the correct side of the hyperplane, then mi represents the distance
between xi and the hyperplane: the margin of safety preventing xi from being
misclassiﬁed. If xi is on the wrong side of the hyperplane, mi is the negative
distance to the hyperplane: the quantity of error that needs to be overcome
in order to achieve correct classiﬁcation for xi.
Our measure of quality for the hyperplane is the minimum margin:
M(w, ρ) = min
i
yi
||w||(xi · w −ρ).
(6.15)
This quantity is positive if all instances are correctly classiﬁed, and negative
otherwise. Its absolute value represents the distance between the hyperplane

116
Semisupervised Learning for Computational Linguistics
and the worst point. That is the nearest point to the hyperplane, if all the
instances are correctly classiﬁed, as in ﬁgure 6.10a. The best hyperplane is
the one that maximizes M. If a positive value of M is achieved, then all the
training instances are correctly classiﬁed. Larger positive values of M reﬂect
larger margins of safety.
6.4.2
Maximizing the margin
A support vector machine is a classiﬁer that is constructed by ﬁnding the
separating hyperplane with the largest possible margin. Given a ﬁxed training
set, consisting of labeled points
(x1, y1), . . . , (xn, yn)
we wish to ﬁnd the hyperplane, deﬁned by a pair (w, ρ), that maximizes M
in (6.15).
To maximize a minimum, we rephrase the task as a constrained maximiza-
tion. We replace the minimum by the constraint that
yi
||w||(xi · w −ρ) ≥U
∀i
(6.16)
and we maximize U under that constraint. One can think of U as a claim on
a certain amount of buﬀer space around the separating hyperplane. We want
to claim as much space as possible (maximize U), but we also require that
the buﬀer space be devoid of data points. Hence the constraint (6.16), which
states that the claimed margin U cannot be larger than the actual margin
M(w, ρ), which is the distance to the nearest data point.
The constraint prevents U from exceeding M(w, ρ). Concordant with the
interpretation of U as the amount of buﬀer space around the hyperplane, we
assume U > 0. That will prove convenient shortly, and it is problematic only if
M(w, ρ) ≤0. For the moment, we restrict attention to the linearly separable
case, in which case a hyperplane does exist with M(w, ρ) > 0. We will return
to the nonseparable case in the next section. We can easily maximize U for
a particular choice of w and ρ by setting it equal to M(w, ρ).
The more
interesting question is how to maximize U when w and ρ are allowed to vary.
A ﬁrst observation is that we can multiply w and ρ by any nonzero con-
stant without changing the hyperplane. The hyperplane is determined by the
direction of w and a distance r from the origin.
Only the direction of w
matters; changing its length has no eﬀect on the location of the hyperplane.
In particular, changing the length has no eﬀect on r, though it obviously does
have an eﬀect on ρ, since ρ is deﬁned as r||w||. In short, if w and ρ satisfy the
constraint (6.16), then so do Kw and Kρ, for any nonzero constant K. One
can easily verify that
yi
||Kw||(xi · Kw −Kρ) = yi
||w||(xi · w −ρ)

Boundary-Oriented Methods
117
for any K ̸= 0.
Since the length of w is essentially a free parameter, we can choose to set
it equal to 1/U. That is, if we restrict our attention to pairs (w, ρ) where
||w|| = 1/U, we will still be able to describe all the same hyperplanes as
before. (Here, we rely crucially on the assumption that U > 0.) So, restricting
attention to pairs (w, ρ) with ||w|| = 1/U, the constraint (6.16) becomes
yi(xi · w −ρ) ≥1
∀i.
(6.17)
Another way of arriving at (6.17) is this: Let us interpret U as the unit that
we use to measure the margin. The constraint (6.16) requires mi ≥1U, that
is, each point must lie at least one marginal unit from the hyperplane, but
otherwise we may choose our unit U as we wish. To maximize the smallest
mi, we should choose U as large as possible. Since the length of w is a free
parameter, we can use it to represent our choice of marginal unit, by deﬁning
U = 1/||w||. This makes mi ≥1U equivalent to (6.17).
We maximize U by minimizing ||w||. It will actually prove more convenient
to minimize (1/2)||w||2. That quantity is monotonically related to ||w||, so
its minimum is the same as the minimum of ||w||.
In short, we have the
optimization problem:
minimize
1
2||w||2
(6.18)
subject to
yi(xi · w −ρ) ≥1
∀i.
We use the methods discussed in section 5.3.
By way of reminder, the
process is as follows. First we deﬁne the Lagrangian L(w, ρ, a) as a mixture
(a linear combination) of the gradient of the objective to be minimized and
the gradients of the constraints. The gradients are taken with respect to the
independent variables w and ρ. The vector a represents the mixing weights
for the constraints, that is, Lagrange multipliers. To minimize L with respect
to w and ρ, we set its gradient to zero. That leaves us with a function Q(a)
in which only the mixing weights vary. We maximize Q to ﬁnd the mixture
for which the minimizer x∗satisﬁes the original constraints.
We rewrite the constraint (6.17) in canonical form:
1 + ρyi −yixi · w ≤0
∀i.
(6.19)
The Lagrangian is
L(w, ρ, a) = 1
2||w||2 +
X
i
ai(1 + ρyi −yixi · w).
(6.20)
We compute the gradient with respect to w and ρ by computing each of the
partial derivatives. Expanding out ||w||2 as P
k w2
k, and expanding out xi · w
as P
k xikwk, one can verify that
∂
∂wk
||w||2 = 2wk
∂
∂wk
xi · w = xik.

118
Semisupervised Learning for Computational Linguistics
Using those two equations, we ﬁnd that
∂L
∂wk
= wk −
X
i
aiyixik.
The partial derivative with respect to ρ is
∂L
∂ρ =
X
i
aiyi.
Setting all the partial derivatives to zero, we obtain
wk =
X
i
aiyixik
∀k
(6.21)
X
i
aiyi = 0.
(6.22)
To eliminate the Lagrange multipliers, we maximize the Wolfe dual, which is
obtained by substituting (6.21) and (6.22) into the Lagrangian (6.20). First,
we observe that
||w||2 =
X
k
w2
k =
X
k
 X
i
aiyixik
! 
X
j
ajyjxjk


=
X
i
X
j
aiajyiyjxi · xj.
Further
X
i
aiyixi · w =
X
i
aiyi
X
k
xik

X
j
ajyjxjk


=
X
i
X
j
aiajyiyjxi · xj.
Hence
Q(a) = 1
2||w||2 +
X
i
ai + ρ
X
i
aiyi −
X
i
aiyixi · w
=
X
i
ai −1
2
X
i
X
j
aiajyiyjxi · xj.
(6.23)
The dual problem is
maximize
Q(a)
subject to

ai ≥0
∀i
P
i aiyi = 0
.
The second constraint is simply a restatement of the condition (6.22). Note
that the values xi and yi are all constants; the only unknowns are the ai. The
solution is the optimal value a∗for the Lagrange multipliers. When we have
found a∗, we can compute w∗using (6.21).

Boundary-Oriented Methods
119
x i
–
+
+
+
–
–
–
1
w
w
FIGURE 6.11
The nonseparable case.
6.4.3
The nonseparable case
Let us return to the constraint (6.17). Recall that it is equivalent to mi ≥1U,
that is, it states that the signed margin for each point is at least one marginal
unit, where we deﬁne a marginal unit to be 1/||w||. If the data is not linearly
separable, then the constraint is not satisﬁable.
The standard way of dealing with the nonseparable case is to soften the
constraint (6.17). The constraint becomes
yi(xi · w −ρ) ≥1 −ξi
∀i.
(6.24)
Here the variable ξi is a “slack variable” (not to be confused with a slack
constraint!) that represents the amount by which data point xi violates the
constraint (ﬁgure 6.11). The nominal margin is still 1U, which is equal to
1/||w||, and we would like to continue to maximize the margin, but we would
also like to minimize the total amount of slack that we introduce. Notice that
the data point xi is misclassiﬁed just in case ξi > 1. Hence the total slack
P
i ξi is an upper bound on the number of training errors.
We wish to minimize both ||w|| and P
i ξi. The usual approach is
minimize
1
2||w||2 + C
X
i
ξi
(6.25)
subject to

1 −ξi + ρyi −yixi · w ≤0
∀i
−ξi ≤0
∀i
.
The value C is a free parameter, ﬁxed by the user, whose value controls the
relative weights of the two objectives.
We note in passing that (6.25) can be viewed as a regularized objective
function. The idea of regularization was introduced brieﬂy in section 4.2.3:

120
Semisupervised Learning for Computational Linguistics
instead of optimizing only ﬁt to the training data, one optimizes a combination
of ﬁt and simplicity. In (6.25), P
i ξi measures training error; if it is reduced
to zero, we have perfect ﬁt to the data. The term ||w||2 can be seen as a
complexity measure. It depends only on the model, not the data. A “simple”
model is one that makes coarse distinctions, and thus requires large boundary
gaps in the data distribution – or comes at the cost of large misclassiﬁcation
error if such gaps are not found. As ||w|| increases, the model can make ﬁner-
and ﬁner-grained distinctions.
To minimize (6.25), we form the Lagrangian:
L = 1
2||w||2 +
X
i
ai(1 −ξi + ρyi −yixi · w) −
X
i
biξi + C
X
i
ξi.
There are two constraints, hence two sets of Lagrange multipliers a and b.
Notice that this Lagrangian diﬀers from the previous one (6.20) only by the
addition of the term,
X
i
(C −ai −bi)ξi.
(6.26)
Hence the derivatives of the Lagrangian with respect to wi and ρ are un-
changed, and setting them to zero yields the equations (6.21) and (6.22), as
before. We also take the derivatives of L with respect to ξi:
∂L
∂ξi
= C −ai −bi
and setting them to zero yields
C = ai + bi
∀i.
(6.27)
When we substitute (6.27) into the Lagrangian, the new term (6.26) goes
away, and Q(a) is the same as before (6.23).
However, we must preserve
(6.27) as a constraint. The Lagrange multipliers bi do not appear in (6.23),
so they have no eﬀect on maximizing Q. But the bi are constrained to be
nonnegative, so they do have the eﬀect of preventing ai from exceeding C.
That is, the actual eﬀect of (6.27) is that Q(a) must be maximized under the
constraint:
0 ≤ai ≤C
∀i.
In short, the dual problem in the nonseparable case is
maximize
Q(a)
subject to

0 ≤ai ≤C
∀i
P
k akyk = 0
.

Boundary-Oriented Methods
121
2KM
2M
KM
p
B
A
FIGURE 6.12
Using slack in the separable case.
6.4.4
Slack in the separable case
We noted above that the coeﬃcient C in (6.25) represents the trade-oﬀbe-
tween the two objectives, the two objectives being the squared margin and the
total slack. If C = 1/2, the two objectives are equally balanced, if C < 1/2,
more weight is placed on ﬁnding a hyperplane with a large margin, and if
C > 1/2, more weight is placed on avoiding labeling errors.
Could it ever be worthwhile to introduce slack in the separable case, for
the sake of obtaining a larger margin? The answer is yes. To keep things
simple, let us focus on a single point that requires slack (ﬁgure 6.12). The
ﬁgure shows two diﬀerent hyperplanes, marked “A” and “B.” Assume that
the data is separable with margin µ; the “A” hyperplane represents that case.
The “B” hyperplane has a larger margin, at the cost of assigning slack ξ to
the point marked “p.” For simplicity, we ignore all points except p and limit
our attention to hyperplanes where the point p is a ﬁxed distance 2µ from
the positive margin. We measure the margin in multiples of µ; the margin
for a given hyperplane is M = Kµ. The “A” hyperplane represents the case
K = 1, and the “B” hyperplane has some value for K greater than 1.
The slack for point p is ξ, meaning that the distance from p to the negative-
side margin is ξ marginal units, which is to say, ξKµ. Figure 6.12 makes it
clear that
ξKµ = 2Kµ −2µ.
Hence
ξ = 2

1 −1
K

.
(6.28)

122
Semisupervised Learning for Computational Linguistics
The value of the objective function (6.25) is
f(M) = 1
2
 1
M 2

+ Cξ.
(6.29)
Since K is proportional to M by M = Kµ, we can consider f as a function
of K:
f(K) =
1
2µ2K2 + 2C

1 −1
K

.
(6.30)
The objective function (6.30) can be thought of as consisting of a “margin
term” 1/(µ2K2) weighted by 1/2, and an “error term” 2(1 −1/K) weighted
by C. The margin term represents an expansive force – it improves (that is,
decreases) as the margin increases. The error term represents a contractive
force – it improves as the margin decreases. The margin term is convex. It
starts out large, but falls rapidly and assymptotes to zero. The error term
is concave. It is zero when K = 1, and it rises rapidly, assymptoting to 2.
Their weighted combination, the objective function f(K), is neither concave
nor convex. Its ﬁrst and second derivatives are
f ′(K) = 2C
K2 −
1
µ2K3
f ′′(K) =
3
µ2K2 −4C
K3 .
All the quantities involved are positive, so f is convex [f ′′(K) > 0] for small
values of K and concave for larger values. The ﬂexion point is
K = 3
4
 1
Cµ2

.
Despite the concavity, f is decreasing [f ′(K) < 0] only in the region
K < 1
2
 1
Cµ2

and it is increasing for larger K. Accordingly, there is a unique minimum, at
the point
K = 1
2
 1
Cµ2

.
(6.31)
As K →∞, the objective function never begins decreasing, but assymptotes
to 2C. Figure 6.13 illustrates the particular case where µ = 1 and C = 1/4.
Incidentally, if the value Cµ2 is made suﬃciently large, then the minimum
falls into the region K < 1. In that region, however, ξ < 0, which is not
permitted, so the second term drops out of the objective function, and only
the “expansive” margin term remains. Hence, for large values of Cµ2, the
optimal hyperplane has K = 1; the optimal hyperplane never has K < 1.

Boundary-Oriented Methods
123
f(K)
0.31
0.3
0.29
0.28
0.27
1
2
3
4
5
6
7
8
9
10
K
FIGURE 6.13
The objective function for µ = 1, C = 1/6.
Let us return to the original question: whether a hyperplane like “B” that
misclassiﬁes point “p” might be preferred to “A.” The point “p” is misclas-
siﬁed just in case ξ > 1. Referring to (6.28), we see that ξ > 1 just in case
K > 2. The optimal value for K is given in (6.31). It exceeds 2 just in case
we have set C < 1/(4M 2), which is possible – in fact, ﬁgure 6.13 illustrates
just such a case.
In short, when we introduce slack variables, the optimal hyperplane may
fail to separate the data even when separation is possible. This situation is
more likely to arise as we decrease C, which is to say, as we assign more weight
to the “large margin” objective and less weight to the “no slack” objective.
6.4.5
Multiple slack points
Up to now, we have only considered a single slack variable, for simplicity. Let
us consider a more complex case, where there are multiple points with slack.
The single slack variable ξ in (6.29) is then replaced with P
i ξi. Suppose
there are n+ positively labeled slack points and n−negatively labeled slack
points. Now let us consider moving the hyperplane a small distance α in the
positive direction. We assume α is small enough that all of the slack points
remain slack. Let us keep the margin constant, so that the only change in the
objective function is determined by the change in P
i ξi.
As a result of moving the hyperplane in the positive direction, the slack must
increase for the positively labeled slack points: they are now even deeper in
negative territory. The slack increases by the same amount for each, namely,
α. The slack for each negatively labeled slack point decreases by α. If there
is the same number of positive and negative slack points, then the objective
function does not change.
But if there are more positives than negatives,

124
Semisupervised Learning for Computational Linguistics
the increases outweigh the decreases, and the value of the objective function
increases.
In short, if the positive and negative slack points are balanced, the location
of the hyperplane is indeterminate (until it moves far enough to change the
set of slack points). But if there is any imbalance, then the hyperplane goes
as far as it can in the direction the larger group is pushing it, until it bumps
up against a new point, or one of the slack points reaches the margin and
stops pushing.
Suppose there are more positives than negatives in the dataset. Then a
random hyperplane will misclassify more positives than negatives, simply be-
cause there are more positives available. This introduces a bias that pushes
hyperplanes in the negative direction.
If one wishes to counteract that bias, one can treat the positives and nega-
tives separately:
C
X
i
ξi = C+ X
i∈P
ξi + C−X
i∈N
ξi
where P is the set of positive instances and N is the set of negative instances.
Let us introduce “bias corrections” γ+ and γ−that allow us to write
C
X
i
ξi = C
 X
i∈P
γ+ξi +
X
i∈N
γ−ξi
!
.
(6.32)
We wish only to correct for bias, not change the net value of the misclassiﬁ-
cation term.
What we mean by “unbiased” is that moving the hyperplane has no eﬀect
on the misclassiﬁcation term P
i ξi, provided the movement is small enough
that the set of slack points remains constant. Moving the hyperplane by a
distance δ in the positive direction changes ξ to ξ +δ for positive slack points,
and to ξ −δ for negative slack points. The net change in the misclassiﬁcation
term is
X
i∈P
γ+δ −
X
i∈N
γ−δ
that is
n+γ+δ −n−γ−δ
where n+ is the number of positive slack points and n−is the number of
negative slack points. What we mean by “the unbiased case” is that the net
change is zero:
n+γ+ = n−γ−.
In the case we considered previously, γ−= γ+ = 1, implying that the unbiased
case has n+ = n−.
Now suppose that there are N + positives in the data set as a whole, and N −
negatives. We would like to make a bias adjustment so that the unbiased case

Boundary-Oriented Methods
125
is one in which the number of positive and negative slack points is proportional
to the total number of positive and negative points, that is
cN +γ+ = cN −γ−.
That is
γ+
γ−= N −
N + .
(6.33)
To ﬁx not just the ratio, but the actual values, recall that we also wish
to preserve (6.32). Let us continue to assume that N + ̸= N −, and that the
slack points have the same proportion of positives to negatives as the data
set overall. Then (6.32) cannot be true uniformly: moving the hyperplane
changes the value on the left-hand side, but not the value on the right-hand
side. The natural “neutral” position for the hyperplane is where the average
slack is the same on both the negative and positive sides. Let us write ¯ξ for the
average slack (the same on both sides). Then for the “neutral” hyperplane,
(6.32) can be written as
(n+ + n−)¯ξ = γ+n+ ¯ξ + γ−n−¯ξ.
Combining this with (6.33) yields
γ+ =
N
2N +
γ−=
N
2N −
where N = N + + N −is the total number of data points.
6.4.6
Transductive SVMs
Vapnik [227] introduced a setting for learning that he called transductive in-
ference. Deductive inference is reasoning from causes to eﬀects and inductive
inference is reasoning from eﬀects to causes. Classiﬁcation in the standard
setting uses both: induction to estimate a model from training data, and de-
duction to predict labels for test data from the model. Transductive reasoning
omits the intermediate model and predicts test labels directly, given labeled
training data and unlabeled test instances. When contrasted with transduc-
tive learning, the standard approach is usually called inductive learning; the
accompanying deductive step is viewed as the application of what was learned.
A little more concretely, consider training data consisting of instances x and
corresponding labels y, and (unlabeled) test instances u. Inductive learning is
the ﬁrst step in a two-step process. First, the learner λ constructs an estimate
of the target function: ˆf = λ(x, y) from labeled training data; and, second,
the estimated function is applied to unlabeled test data to produce predicted
labels ˆy = ˆf(u). By contrast, a transductive learner φ predicts labels directly:
ˆy = φ(x, y, u).

126
Semisupervised Learning for Computational Linguistics
One can easily convert an inductive learner into a transductive learner and
vice versa. To convert an inductive learner λ into a transductive learner φ,
deﬁne
φ(x, y, u) ≡ˆf(u)
where
ˆf = λ(x, y).
Here we write ˆf(u) for the result of applying ˆf to each test instance ui and col-
lecting the results into a vector of predicted labels. To convert a transductive
learner φ into an inductive learner, deﬁne λ such that
λ(x, y) = ˆf
where
ˆf(u) = φ(x, y, u).
The inductive learner λ simply “memorizes” the training data, and calls φ on
the training data plus test data, when the test data is made available.
We see that transductive learning is not fundamentally a new kind of learn-
ing.
The real diﬀerence between transductive learning and the standard
paradigm is whether one makes use of the unlabeled test data during learning.
An inductively trained classiﬁer makes the same prediction on a given test in-
stance ui no matter what the rest of the test data looks like. A transductive
learner may predict diﬀerent labels for ui depending on the rest of the test
data. The objective function for a (nontrivial) transductive learner does dif-
fer from that for an inductive learner, because knowledge of the (unlabeled)
test data is included in the objective function for a transductive, but not an
inductive, learner.
The semisupervised boosting approach that we considered earlier can be
viewed as transductive learning. In ﬁgure 6.9, we can interpret the labeled
instances as training data and the unlabeled ones as test data. Of the hyper-
planes that correctly classify the labeled instances, the “A” hyperplane is the
best, in the sense of maximizing the margin for labeled instances, but the “B”
hyperplane is the best if one considers all instances. The objective function
(6.12) takes both training instances and test instances into account. Speciﬁ-
cally, the second term takes test instances into account, seeking to move the
hyperplane as far away from them as possible.
Transductive learning as conceived by Vapnik was intended for supervised
learning, not semisupervised learning. To be sure, the unlabeled test data u
is not made available to the learner, in the “oﬃcial” version of the standard
setting, but as a practical matter, classiﬁers are tested on just such a batch
of test data, not on individual instances in isolation. Vapnik’s insight was
to adjust the objective function so as to take advantage of the unlabeled
test instances as a batch. Eﬀectively, instead of the optimization (6.25), the
objective is
minimize
1
2||w||2 + C
X
i
ξi + D
X
j
ζj
(6.34)
subject to







1 −ξi + ρyi −yixi · w ≤0
∀i
1 −ζj + ρˆyj −ˆyjuj · w ≤0
∀j
−ξi ≤0
∀i
−ζj ≤0
∀j
.

Boundary-Oriented Methods
127
Formally, this is simply (6.25) in which we redeﬁne x to include both training
and test instances, and in which we redeﬁne y to include both training labels
and estimated test labels. The formulation (6.34) does, however, explicitly
raise the possibility of penalizing errors diﬀerently for training and test, by
setting C ̸= D.
Although transductive learning was not originally formulated with semisu-
pervised learning in mind, it applies naturally to the semisupervised setting.
In the typical semisupervised case, the learner is provided with labeled train-
ing data (x, y) and additional unlabeled training data u. One can simply
apply a transductive learner to (x, y, u) as if the unlabeled training data u
were test data.
To be sure, a subtlety arises. The usual semisupervised setting is inductive:
in addition to the labeled training data, one has some labeled test data as
well to evaluate performance. One can actually distinguish between induc-
tive semisupervised learning and transductive semisupervised learning. The
diﬀerences between them are whether the learner is given access to the unla-
beled test data in addition to the unlabeled training data, and whether or not
predictions will be required for test instances that were not initially given to
the learner. Most inductive learners cannot take any advantage of unlabeled
test data even if it is provided. Conversely, some transductive learners cannot
easily be applied to new test instances. One can always add the new test in-
stances to the unlabeled training data and re-run the learner, but generally at
a signiﬁcant practical cost: the time cost for running a transductive learner on
a (large) training set generally signiﬁcantly exceeds the time cost of applying
an inductively trained classiﬁer to a single test instance.
Incidentally, a transductive SVM does produce a classiﬁer, in the form of a
separating hyperplane, that can be applied to arbitrary test instances. Hence
an SVM that is trained transductively can be applied inductively. Doing so is
indistinguishable from using an SVM in an inductive semisupervised setting.
In a transductive semisupervised setting, the (unlabeled) test instances are
included in the unlabeled training data that is given to the learner. Since
semisupervised learners, by design, can take advantage of unlabeled data, the
diﬀerence between inductive and transductive semisupervised learning boils
down to the type of evaluation done: whether the learner is given access to
the unlabeled test instances or not.
6.4.7
Training a transductive SVM
In the previous section, we gave the impression that training an SVM trans-
ductively was just like training it inductively, except that the training data is
extended to include the test instances. The objective is (6.34), which is just
(6.25) applied to the “extended” data set. But there is actually a very im-
portant diﬀerence between the two: the labels y are known, but the labels ˆy,
which appear only in (6.34), are unknown. In other words, given a particular

128
Semisupervised Learning for Computational Linguistics
guess ˆy, we can compute (6.34) by extending the training set to include (u, ˆy)
and computing just as we did for (6.25). That is, we have a way of evaluating
a particular guess ˆy, but not for ﬁnding the optimal guess.
The straightforward approach is simply to consider every possible labeling
ˆy. Since there is a ﬁxed number of unlabeled instances, and only two pos-
sible labels for each instance, there is a ﬁnite number of possible vectors ˆy.
However, that number is exponential in the number of unlabeled instances, so
the straightforward approach is impractical for all but the smallest unlabeled
sets. Bennett and Demiriz [14] took this approach, and were unable to handle
datasets with more than about 50 points. But in semisupervised learning, we
are frequently interested in large unlabeled sets.
For larger unlabeled sets, Vapnik suggests clustering the unlabeled data,
and systematically considering all assignments of labels to clusters. All in-
stances within a cluster are assigned the same label. This approach presumes
a small number of clusters.
An alternative approach, proposed by Joachims [117], is the following. To
begin, train an SVM m0 using just the labeled data. Then add the unlabeled
data. Use m0 to assign labels to the unlabeled data, and train a new SVM,
m1, on the mixed data (manually labeled and automatically labeled), using
a very small value for D in (6.34).
By keeping D very small, the learner
will seek to maximize the margin even at the cost of misclassifying some
examples. This will drive the hyperplane toward regions where the data is
sparse and larger margins are obtainable. As a consequence, m1 generally
will not classify all the automatically labeled instances “correctly” (that is, in
agreement with m0). It is to be expected that the progress of the hyperplane
into low-density regions was held back by the automatically labeled instances
that m1 misclassiﬁed. Accordingly, we relabel those automatically labeled
instances where m1 disagrees with the labels. Instances are relabeled in pairs,
to keep the proportion of positives and negatives constant. Then the process
repeats, yielding classiﬁers m2, m3, . . .. At some point, one obtains a classiﬁer
mt that agrees with the previous one on all automatically labeled instances.
At that point, the process has converged: retraining will simply yield mt
again.
Joachims includes a couple of reﬁnements in the algorithm. First, the user
speciﬁes a desired misclassiﬁcation penalty D′. The learner starts with D very
small, and increases it at each iteration until reaching the target D′. Second,
the user speciﬁes a target ratio of positives to negatives. In the initial label-
ing of unlabeled instances, using m0, the classiﬁcation threshold is adjusted
so that the desired positive-to-negative ratio is obtained. Once established
during the initial labeling, the ratio is preserved by the way instances are
relabeled in pairs.

Boundary-Oriented Methods
129
6.5
Null-category noise model
There are other approaches that focus on ﬁnding low-density regions in which
to place boundaries.
We will mention just one here: Lawrence & Jordan
[134] propose to model lowness of density in boundary regions by means of a
probabilistic model of data generation that deletes points that fall near the
boundary. We will give only an impression of it here, and refer the reader to
the original source and citations there for more detail.
A generative model of the data distribution p(x, y) is given, where x is an
instance and y is its label, but unlike typical generative models, it is factored
into an instance distribution p(x) and a conditional label distribution p(y|x)
that has the form of a probabilistic classiﬁer. The probability p(y|x) is based
on a distribution p(α|x) of “conﬁdence-weighted labels” α, by which we mean
that the sign of α represents the predicted label and the absolute value of α
represents the conﬁdence of the prediction. The distribution p(α|x) is Gaus-
sian in form, with mean µ(x) and variance σ(x). This provides us with a
probabilistic interpretation of conﬁdence: the probability p(α|x) allows one
to measure conﬁdence as Pr[α < 0|x] or Pr[α > 0|x]. As µ(x) moves toward
+∞, the probability Pr[α > 0|x] increases to one, and similarly Pr[α < 0|x]
increases to one as µ(x) moves toward −∞.
If one thinks of instances x as points in a horizontal plane, the “instance
plane,” one can think of p(α|x) as a fuzzy surface, the “label surface,” whose
altitude at point x is µ(x) and whose thickness is σ(x). The fuzzy surface is
densest at altitude µ(x) and attenuates rapidly as one moves above and below
that altitude. Given a point x, one chooses an altitude α probabilistically,
according to the density p(α|x) at that altitude.
The label (or “category”) y ∈{−1, 0, +1} is determined by α.
If α is
large and positive, then y is +1, and if α is large and negative, then y is
−1. But the model also countenances a third possibility: the null category
y = 0. Speciﬁcally, a parameter w is ﬁxed representing the thickness of a “dead
region” just above and below the instance plane. If α > +w/2 then y = +1,
if α < −w/2 then y = −1, but if α falls in the dead region [−w/2, +w/2],
then y = 0. Assigning the null category marks a point for deletion.
If |µ(x)| is much greater than w/2, then the probability that α falls into
the region [−w/2, +w/2] is very small. Conversely, if µ(x) = 0 and w is large
relative to σ(x), then the probability that −w/2 ≤α ≤+w/2 is nearly one.
That is, at points x where conﬁdence is high, and the label surface is well
above or below the instance plane, the probability of deleting x is very small,
but where conﬁdence is low, and the label surface is close to the instance
plane, the probability of deleting x is very high. As a consequence, where the
label surface crosses the instance plane, there is a region in which almost all
instances have been deleted.
To complete the data-generation model, if the point is not deleted, a prob-

130
Semisupervised Learning for Computational Linguistics
abilistic decision is made whether or not to “hide” the label. If the label is
hidden, the result is an unlabeled sample point, and if the label is not hidden,
the result is a labeled sample point. The decision is based solely on the label,
so there are only two parameters involved: the probability of hiding a label if
it is positive, and the probability of hiding it if it is negative.
Training data given to the learner consists of a mix of labeled and unlabeled
sample points generated by the process just described. Deleted points are not
included in the sample. The labeled points are used to estimate the parameters
of the model p(y|x). Unlabeled points help identify decision boundaries by
making it clearer where there are regions that are devoid of instances. Hence,
like the other approaches discussed in this chapter, the model is designed to
place class boundaries in low-density regions. Unlike the other approaches,
however, it is generative rather than discriminative.

7
Clustering
The previous chapters have focussed on semisupervised learning methods that
take classiﬁcation algorithms as their point of departure. In this chapter, we
turn to clustering as a basis for semisupervised learning.
7.1
Cluster and label
The simplest idea for using clustering in a semisupervised setting is what we
might call cluster and label. Apply any clustering algorithm to the data,
ignoring the labels if any of the data is labeled. Then assign labels to the
clusters based on the labeled data falling within each cluster. The obvious
rule is to count each instance labeled y in a given cluster as a vote for label y,
and to assign to the cluster the label that receives the most votes. The label
predicted for a given unlabeled instance x is the label assigned to the cluster.
The algorithm is summarized in ﬁgure 7.1.
There is one point to note. It is assumed that the procedure cluster returns
a function that is deﬁned everywhere in the instance space. Not all clustering
algorithms return such a function; some return only an assignment of clusters
procedure clusterAndLabel (X, Y, V )
X = (x1, . . . , xn) consists of instances with labels Y = (y1, . . . , yn)
V consists of additional unlabeled instances
c(·) ←cluster(X ∪V )
for each distinct cluster ck
deﬁne µ(ck) = arg maxy |{i : c(xi) = ck ∧yi = y}|
return function f(·) such that f(x) = µ(c(x))
FIGURE 7.1
The cluster-and-label algorithm.
It takes a mix of labeled and unlabeled
instances as input and returns a classiﬁer.
131

132
Semisupervised Learning for Computational Linguistics
to the points in the training sample and are undeﬁned at any other point in the
instance space. The former kind of clustering algorithm, whose return value
is a function deﬁned everywhere in the instance space, is an inductive learner.
The latter kind of algorithm, whose return value is deﬁned only for the given
sample, is essentially transductive. The reason for the hedge “essentially” is
that a transductive learner is deﬁned by two properties: assigning a label to
sample points only, and receiving both training and test points in its sample.
But a distinction between training and test points is rarely made in clustering,
so the lack of generalization beyond the sample is the critical property in this
context.
The cluster and label algorithm as stated assumes an inductive clustering
algorithm. A simple method for converting a transductive clusterer into an
inductive one is to assign each test point to the nearest cluster, where the
distance between a point and cluster might be deﬁned in one of several ways,
such as those discussed in section 7.3 below.
These include the minimum
distance to any point in the cluster, the average distance to all points, or the
distance to the centroid (deﬁned below).
We turn now to algorithms that can be used as the cluster function.
7.2
Clustering concepts
7.2.1
Objective
The idea of clustering is to ﬁnd “natural groups” within the data. Cluster-
ing is easiest and most intuitive when instances are distributed densely in
well-deﬁned, approximately circular regions (the clusters), separated by large
boundary areas that are devoid of instances. Just as the boundary-oriented
semisupervised learning methods discussed in the preceding two chapters con-
trast with generative methods that describe how the high-density regions are
populated (to be discussed in the next chapter), so one commonly distin-
guishes two approaches to clustering: in agglomerative clustering, the focus
is on identifying high-density regions or cluster centers, whereas in divisive
clustering, the focus is on identifying good places for boundaries. A third
desideratum is to have clusters of approximately equal size, other things be-
ing equal; the desire for balance can contribute to the choice of boundary
placement.
Deﬁning the quality of a clustering is a somewhat thorny issue. Clustering
is put to many purposes, and many diﬀerent measures of quality have been
proposed. Clustering is often used in exploratory data analysis, in order to
discover structure that is not known a priori to the researcher. The meth-
ods are in many cases purely algorithmic, and do not deﬁne an independent
measure of quality. Rather they embody “obvious” steps to construct clusters

Clustering
133
corresponding to geometric intuition – the hierarchical clustering methods
that we discuss shortly are good examples.
We are primarily interested in clustering as a form of classiﬁcation in which
the learner is provided no labeled data – what we might call unsupervised
classiﬁcation. The objective, and the measure of quality, are the same as
in classiﬁcation: to approximate a target function that assigns classes to in-
stances. The ultimate measure of quality is generalization error.
“Unsupervised classiﬁcation” may seem a contradiction in terms. If the
data distribution is such that clusters are compact and well separated, one
can imagine identifying class boundaries correctly using only unlabeled data.
But without any labeled data, it is hard to imagine how one could determine
which label goes with which cluster.
In some cases, unsupervised classiﬁcation is in principle possible. The dif-
ﬁculty in assigning labels to clusters arises because class labels are usually
arbitrary designators – but learning problems do arise in which the classes
are not arbitrary, but have intrinsic content with observable distributional re-
ﬂexes. A prime example is provided by unsupervised grammatical inference,
beginning with the problem of inducing parts of speech. One can distinguish
function words from content words by several attributes.
Function words
have higher absolute frequency, and function words have much more uniform
distribution. There is intrinsic meaning to the function word/content word
distinction that makes it possible to label the clusters once they are identiﬁed.
Moreover, there is a non-trivial family of problem instances: each language
represents a new problem instance, with a diﬀerent set of function words and
content words.
If one deﬁnes semisupervised learning broadly enough, one might treat the
setting just sketched as semisupervised learning, in which the intrinsic class
deﬁnitions take the place of seed information. They are comparable to a seed
classiﬁer.
Be that as it may, it is in fact more common for class labels to be arbi-
trary designators. In such a case, unsupervised construction of a classiﬁer
is impossible; the best that is achievable is correct identiﬁcation of the clus-
ters. A clustering algorithm learns a function c(x) assigning cluster labels to
instances, and to obtain a classiﬁer f(x), we also require a mapping µ from
cluster labels to class labels, allowing us to deﬁne:
f(x) = µ(c(x)).
The quality of f is measured as generalization error, in the usual way, and
the quality of c is deﬁned to be the quality of the best classiﬁer constructable
from c, ranging over choices of µ.
7.2.2
Distance and similarity
The geometric intuitions that we appeal to in clustering depend on having a
deﬁnition of the distance between instances. A good cluster contains many

134
Semisupervised Learning for Computational Linguistics
points that are close to each other, and it is well separated (distant) from
other clusters. If instances are represented as real vectors, then Euclidean
distance is the obvious measure of distance, but other measures are possible.
A distance measure d is generally taken to satisfy the following conditions,
for all instances x and y:
(i)
d(x, x) = 0
(ii)
d(x, y) ≥0
non-negativity
(iii)
d(x, y) = d(y, x)
symmetry.
(7.1)
A distance measure is a metric if it also satisﬁes the triangle inequality:
d(x, y) + d(y, z) ≥d(x, z).
(7.2)
A wide variety of distance measures are available. When the data space
is Rk, common choices are Euclidean distance, also known as the L2 norm
(7.3), and Hamming distance, also known as Manhattan distance, city-block
distance, or the L1 norm (7.4):
d(x, y) = ||x −y|| =
v
u
u
t
k
X
i=1
(xi −yi)2
(7.3)
d(x, y) =
k
X
i=1
|xi −yi|.
(7.4)
Distance – or rather its inverse, proximity – is a geometric representation
of the similarity between two instances. In some cases, it is most convenient
to deﬁne similarity directly, and in other cases, it is more convenient to deﬁne
a measure of distance, and deﬁne similarity in terms of distance.
Corresponding to the properties (7.1), a similarity measure s is generally
taken to satisfy:
(i)
s(x, x) = 1
(ii)
0 ≤s(x, y) ≤1
(iii)
s(x, y) = s(y, x)
symmetry.
(7.5)
Given the properties just listed, it is obvious that similarity and distance
are not opposites in the sense of arithmetic inverse, but rather in the sense
that any natural mapping between similarities and distances is monotone
decreasing.
That leaves open a wide range of possibilities.
One common
choice for mapping a distance d to a similiarity s(d) is
s =
1
1 + d.
(7.6)

Clustering
135
A more specialized example of a similarity measure derived from a distance
metric is the following transform of Euclidean distance that arises naturally
in Gaussian random ﬁelds:
s(x, y) = exp
 
−||x −y||2
α2
!
.
This is actually a family of similarity metrics, indexed by the parameter α.
The reverse mapping, from similarity to distance, is complicated by the
requirements of the triangle inequality (assuming one desires a metric). The
trick is to observe (1) that Euclidean distance can be expressed in terms of a
dot product:
d(x, y) = ||x −y|| =
p
(x −y) · (x −y)
appropriately chosen space.
many points. Suppose we have a space containing three points, x1, x2, and
x3. We deﬁne a new space in which each dimension corresponds to a pair of
original points: in the case of three original points, there are nine dimensions
in the new space. Let sij stand for the similarity s(xi, xj), and deﬁne Σi =
P
j̸=i sij. The representations for our three points in the new space are as
follows. Note that some of the components may be complex.
Dimension
(1, 1)
(1, 2) (1, 3) (2, 1)
(2, 2)
(2, 3) (3, 1) (3, 2)
(3, 3)
x′
1 ( √s11 −Σ1
p s12
2
p s13
2
p s21
2
0
0
p s31
2
0
0
)
x′
2 (
0
p s12
2
0
p s21
2
√s22 −Σ2
p s23
2
0
p s32
2
0
)
x′
3 (
0
0
p s13
2
0
0
p s23
2
p s31
2
p s32
2
√s33 −Σ3 )
The reader can conﬁrm that the dot product of any two vectors in the new
space is equal to the similarity of the corresponding points in the original
space. (The construction relies on the symmetry assumption sij = sji.)
In general, the (i, j)-th component of the new vector x′
k is equal to
p
sij/2
if i ̸= j and k = i or k = j
√skk −Σk
if i = j = k
0
otherwise.
Then, for i ̸= j, we have
x′
i · x′
j =
q
sij/2
2
+
q
sji/2
2
= sij
It is easiest to see why (2) is true in the case of a space containing ﬁnitely
and (2) that any similarity measure can be expressed as a dot product in an

136
Semisupervised Learning for Computational Linguistics
and the dot product of a vector with itself is
x′
i · x′
i =
p
sii −Σi
2
+
X
j̸=i
"q
sij/2
2
+
q
sji/2
2#
= sii.
The construction can be generalized to spaces containing inﬁnitely many
points, so that any similarity measure is expressible as a dot product in an
appropriately chosen space.
As noted above, the Euclidean distance between two points can be expressed
in terms of a dot product:
d(x′, y′) =
p
(x′ −y′) · (x′ −y′)
=
p
x′ · x′ −2x′ · y′ + y′ · y′.
We know that Euclidean distance is a metric, and in particular that it satisﬁes
the triangle inequality. In short, we can convert an arbitrary similarity s to a
distance metric by deﬁning
d(x, y) =
p
s(x, x) −2s(x, y) + s(y, y).
(7.7)
If the similarity satisﬁes s(x, x) = 1 for all x, (7.7) can be simpliﬁed to
d(x, y) =
p
2(1 −s(x, y)).
7.2.3
Graphs
We have introduced similarity as a function that is monotone decreasing with
the distance between two instances viewed as points in a space. A similarity
measure s is deﬁned over all pairs of points in the space. If we limit attention
to a ﬁxed set of n data points (x1, . . . , xn), there are two other useful ways of
thinking about similarity.
A similarity measure that is deﬁned only at a ﬁnite number of points is
equivalent to a similarity matrix, which is a matrix W with elements wij =
s(xi, xj). Because of the properties of a similarity measure, a similarity matrix
is symmetric and has non-negative entries, with all 1’s on the diagonal.
A similarity matrix, in turn, is equivalent to a weighted graph that we call
a similarity graph. Its vertices are the given data points, and there is an edge
(xi, xj) just in case the similarity between xi and xj is nonzero. Each edge
has a weight wij = s(xi, xj).
It is worth emphasizing that the relationship between a similarity measure
and a similarity graph is essentially the same as the relationship between
the feature space and a ﬁnite sample of data points. A similarity measure
determines – or is determined by – the geometry of the feature space. The

Clustering
137
similarity graph is the discrete analogue that results from the restriction to a
particular sample.
If similarity values are all either zero or one, the similarity function s(xi, xj)
becomes a relation; we interpret the value one as true (xi and xj are similar)
and the value zero as false (they are not similar). The corresponding graph is
unweighted, under the convention that an unweighted graph is the same as a
weighted graph whose weights all have value one. The matrix is then called
an adjacency matrix. It has elements
aij =



1 if there is an edge (xi, xj)
0 otherwise
.
7.3
Hierarchical clustering
Probably the most widely known, and in that sense the most basic, clustering
method is hierarchical clustering. It is an agglomerative method. A “cluster”
is simply a set of data points; one begins with a separate singleton cluster for
each data point. Then one ﬁnds the closest pair of clusters, that is, the pair
of clusters (X, Y ) that minimizes the distance d(X, Y ), and one replaces the
clusters X and Y with X ∪Y . This step is repeated until a stopping criterion
is reached. For example, one might stop when the number of clusters has been
reduced to a target number chosen in advance, or when the distance between
the nearest pair of clusters exceeds a threshold chosen in advance.
The agglomerative process can be conveniently represented by a dendo-
gram, as in ﬁgure 7.2. The individual data points are at the leaves along the
left edge. The horizontal segments represent clusters, and the vertical “cross-
ties” represent the agglomeration of two clusters. The horizontal position of
the cross-tie corresponds to the distance between the two clusters. A given
stopping criterion determines a vertical cut through the dendogram, and the
horizontal lines that the cut intersects are the ﬁnal clusters.
The method just described is actually a family of methods that diﬀer in
the choice of distance measure d(x, y) between data points, and in the way
in which d(x, y) is extended to a measure of distance d(X, Y ) between sets
of points (clusters).
The choice of distance measure between data points
was discussed brieﬂy in the preceding section. The commonest deﬁnitions of
aggregate distance d(X, Y ) are the following:
• Single linkage. The distance between clusters X and Y is the minimum
distance between any two points x ∈X and y ∈Y .

138
Semisupervised Learning for Computational Linguistics
x 1
x 2
x 3
x 4
x 5
x 6
x 7
x 8
FIGURE 7.2
A dendogram.
• Complete linkage.
The distance between X and Y is the maximum
distance between any x ∈X and y ∈Y .
• Centroid linkage. The distance between X and Y is the distance between
their centroids. Let xij represent the j-th component of the i-th vector
in X. Then the centroid of X is the vector whose j-th component is
the average value of xij, ranging over values of i. The centroid does
not generally coincide with any data point in X. The mean is generally
used as the deﬁnition of “average,” but the median is also possible.
• Average linkage. The distance between X and Y is the average distance
between pairs x ∈X and y ∈Y .
It might not be immediately obvious that average linkage gives a diﬀerent
result from centroid linkage. To see that it does, consider the case in which
X contains one point x, and Y contains two points y1 and y2, equidistant
from x, as in ﬁgure 7.3. The centroid linkage distance is the length of the
dotted line, and the average linkage distance is the length of either solid line.
Moreover, if y1 and y2 are moved apart from each other without moving the
centroid, the average linkage distance increases, but the centroid distance is
unchanged.
Hierarchical clustering has the advantage of simplicity. It is easy to grasp
and easy to implement, and hence widely used. It has the disadvantage that it
is entirely heuristic. Given the aim of producing “compact” clusters, greedily
choosing the closest pair of clusters for agglomeration is plausible, but there
is no global measure of cluster compactness that the algorithm optimizes.

Clustering
139
x
y 1
y 2
X
Y
FIGURE 7.3
An example in which centroid linkage and average linkage diﬀer. The cen-
troid linkage distance is the length of the dotted line, and the average linkage
distance is the length of either solid line.
7.4
Self-training revisited
7.4.1
k-means clustering
k-means clustering is a popular clustering algorithm that is related to the
nearest-neighbor classiﬁer (section 4.1.2).
It is agglomerative in the sense
that it focuses on identifying centers of density. It also turns out to be related
to self-training.
The parameter k represents the number of clusters. Each cluster is rep-
resented by a center mj. As in 1-nearest-neighbor classiﬁcation, each data
point is assigned to the cluster whose center is nearest. That is, the clusters
constitute a Voronoi tesselation of the instance space, as in ﬁgure 4.4 (page
46), but with the training instances xi replaced by the centers mj. Usually,
the number of instances is much greater than the number of centers. The
decision boundaries are piecewise linear; they are perpendicular bisectors of
the vectors connecting pairs of adjacent centers, as in ﬁgure 5.5 (page 71).
The algorithm is given in ﬁgure 7.4. Initially, the centers are chosen at
random. An iterative algorithm is then used to adjust them. An unlabeled
sample of instances (x1, . . . , xn) is given as input to the algorithm.
Each
instance is labeled using the nearest-neighbor rule. If mj is the center nearest
to xi, then the j-th label is assigned to xi. After all instances are labeled, the
centers are recomputed. The center mj is set to the centroid of the instances

140
Semisupervised Learning for Computational Linguistics
procedure kMeansCluster (X)
X = (x1, . . . , xn) consists of unlabeled instances
c = (m1, . . . , mk) is a random set of centers
loop until convergence
L ←label(X, c), using nearest-neighbor rule
c ←train(L), setting centers to centroids
end loop
return c
FIGURE 7.4
The k-means clustering algorithm.
labeled j. That is
mj ←
1
|Xj|
X
x∈Xj
x
where Xj is the set of instances with the j-th label.
Note the resemblance between the k-means clustering algorithm (ﬁgure 7.4)
and self-training in the “dual” form (ﬁgure 2.7, page 26). If we use a labeled
seed set (or a seed classiﬁer) in place of the randomly initialized classiﬁer
in k-means clustering, we obtain a version of self-training. Self-training and
k-means clustering both share the basic principle of alternately labeling in-
stances and training a classiﬁer. The diﬀerences are that k-means clustering
uses random initialization instead of a seed classiﬁer or labeled seed instances,
it does not use a conﬁdence threshold, and it is more speciﬁc about the form
of the label and train functions: namely, label is nearest-neighbor labeling
and train sets cluster centers to centroids.
7.4.2
Pseudo relevance feedback
The basic idea of self-training arose independently in the ﬁeld of informa-
tion retrieval, where it takes the form of an algorithm called pseudo relevance
feedback. We postponed discussing pseudo relevance feedback until now, be-
cause it is best understood in terms of the geometric ideas introduced in this
chapter. It has commonalities, in particular, with the k-means clustering al-
gorithm. Pseudo relevance feedback involves a binary classiﬁcation (relevant
versus irrelevant), and, as we will see, it resembles k-means clustering in that
it represents the classes by their centroids, and deﬁnes the decision boundary
between classes as a line perpendicular to the vector connecting the centroids.
The most common task in information retrieval is ad hoc document re-
trieval. Given a query, the problem is essentially one of binary classiﬁcation
of documents. It is assumed that each document is either relevant to the query

Clustering
141
or not. In one common approach, feature extraction converts each document
to a vector x; feature extraction is also applied to the query to produce a
vector q. The learning algorithm uses no training data except the query. A
document is predicted to be relevant if it is suﬃciently similar to the query,
where “suﬃciently similar” is interpreted as cosine similarity of the vector rep-
resentations. That is, given query q, a conﬁdence-rated classiﬁer f is deﬁned
with
f(x) = cos(x, q) −θ
where θ is a free parameter. The sign of f(x) represents its prediction (positive
for relevant and negative for irrelevant), and the absolute value represents
conﬁdence.
If we normalize document vectors and query vectors, so that we can assume
that ||x|| = 1 and ||q|| = 1, then
f(x) = x · q −θ.
This form should by now be familiar as a conﬁdence-weighted predictor, in
which the conﬁdence is geometrically interpretable as the distance of point x
from the hyperplane deﬁned by normal vector q and threshold θ: recall, for
example, equation (5.2) and the discussion there (page 68).
Relevance feedback is a form of active learning in which the conﬁdence
values of the initial classiﬁer are used to select a sample for manual labeling.
The documents with the highest values f(x) are selected to be manually
labeled as relevant or irrelevant. That labeled data is then used to construct
an improved query vector, and hence an improved conﬁdence-rated classiﬁer.
Speaking of an “improved” query vector implies a measure of quality for
query vectors.
The measure of quality is based on the predictions of the
conﬁdence-rated classiﬁer f. For a given document xi with label yi ∈{+1, −1},
the quality is positive if f(xi) and yi have the same sign, and negative oth-
erwise, and it is weighted by the conﬁdence of f(xi). The result is averaged
over all documents. In short:
Q(q) = 1
n
X
i
yif(xi)
= 1
n
X
i
yi cos(xi, q) −θ
where n is the total number of documents. The optimal query is deﬁned to
be the query with maximum quality.
The optimal query is well-deﬁned if
the labels of all documents are known. It is not guaranteed that the query
that maximizes Q over a given sample of documents is the same as the op-
timal query, nor even that the value of Q averaged over the labeled sample
agrees with its value over the whole document set. If the sample were drawn
i.i.d. from the document set, then the sample value of Q would be known to

142
Semisupervised Learning for Computational Linguistics
converge to the population value. But in the case of relevance feedback, the
sample is biased by the initial query, and no guarantees are known.
The optimal query has a simple geometric characterization. We can rewrite
Q as follows:
Q(q) = 1
n
X
i
yi
xi · q
||xi|| · ||q|| −θ
=
q
||q|| · 1
n
X
i
yi
xi
||xi|| −θ.
Let A be the set of relevant documents, and let B be the irrelevant documents.
Deﬁne:
m = 1
n
X
i
yi
xi
||xi||.
Notice that m is not dependent on the choice of q (nor on the choice of θ, for
that matter). The quality of q can be written as
Q(q) =
q
||q|| · m −θ
= |m| cos(q, m) −θ.
Clearly, for ﬁxed θ, Q(q) is maximized by choosing q in the same direction
as m; that is, any vector in the same direction as m maximizes Q, including
m itself.
Now, since yi = +1 for xi ∈A and yi = −1 for xi ∈B, we can write m as
m =
1
|A|
X
i∈A
xi
||xi|| −1
|B|
X
i∈B
xi
||xi||
= ¯a −¯b.
In the last line, ¯a represents the centroid of the relevant documents, after each
has been normalized (projected onto the unit sphere), and ¯b represents the
centroid of the irrelevant documents.
Figure 7.5a illustrates in two dimensions. The normalized documents lie on
the unit circle. The circled “+” and “−” are the centroids of the positive and
negative documents, and the solid vector is their diﬀerence m; it represents
the optimal query. The dotted vector is m translated to the origin. The dotted
line perpendicular to m is the decision boundary for the classiﬁer based on m.
The value cos(x, m) decreases montonically and symmetrically as one moves
around the unit circle away from its intersection with m.
We note that the classiﬁer corresponding to the optimal query m may fail
to separate the relevant and irrelevant classes for any value of θ, even when
the data is separable. Figure 7.5b gives an example. The data distribution is
skewed, pulling the centroids toward one end of the half-circle for each class.
The vector shown is m, and the dashed line represents the decision boundary

Clustering
143
(a)
(b)
FIGURE 7.5
(a) The geometric interpretation of the optimal query. The optimal query is
the vector pointing from the center of gravity of the negatives to the center of
gravity of the positives. (b) If the distributions are skewed, the optimal query
may fail to separate the classes even when they are linearly separable.
for the corresponding classiﬁer. The decision boundary may be moved up
or down by adjusting θ, but it must remain parallel to the dashed line. By
contrast, the dotted line represents the decision boundary required to separate
the data.
The relevance feedback algorithm is given in ﬁgure 7.6. In pseudo relevance
feedback, the manual labeling in step 5 is replaced by the assumption that
the highest-conﬁdence predictions of f are correct. The algorithm is given in
ﬁgure 7.7. Pseudo relevance feedback clearly shares critical features of self-
training.
An initial classiﬁer is given (implicit in q0).
Its high-conﬁdence
predictions are taken at face value to label some instances. The classiﬁer is
retrained, and the process iterates. The algorithm as given includes throttling
(assuming k to be small) and balancing (k is equal for positives and negatives).
7.5
Graph mincut
We turn now to divisive clustering, which, instead of seeking regions with high
instance density in which to place cluster centers, seeks low-density regions in
which to place boundaries. The most popular divisive methods are graph cut
methods.
We shift perspective from the instance space to the similarity graph deﬁned

144
Semisupervised Learning for Computational Linguistics
Input: initial query q0, unlabeled data x1, . . . , xn
1.
A, B ←∅
2.
For t from 0 to T −1
3.
Deﬁne ft(x) = cos(qt, x) −θ
4.
S ←the k data points with highest values for f
5.
Manually label data points in S; add positives to A, negatives to B
6.
a ←centroid of A
7.
b ←centroid of B
8.
qt+1 ←a −b
9.
End for
10. Return fT
FIGURE 7.6
The relevance feedback algorithm.
Input: initial query q0, unlabeled data x1, . . . , xn
1.
A, B ←∅
2.
For t from 0 to T −1
3.
Deﬁne ft(x) = cos(qt, x) −θ
4.
A ←A + the k data points with highest values for f
5.
B ←B + the k data points with lowest values for f
6.
a ←centroid of A
7.
b ←centroid of B
8.
qt+1 ←a −b
9.
End for
10. Return fT
FIGURE 7.7
Pseudo relevance feedback.

Clustering
145
by the training sample. In the similarity graph, a boundary takes the form
of a partitioning, or cut, of the graph. The quality of a boundary can be
measured in terms of the cut size, which is the number of edges that are
severed, in an unweighted graph, or the total weight of edges severed, in a
weighted graph. The best cut is one that minimizes cut size.
The edges of a graph can be viewed in several diﬀerent ways. (1) As usually
deﬁned, the set of edges E contains pairs (vi, vj) of vertices. (2) It can be
more convenient to identify vertices with their indices, and treat E as a set of
pairs (i, j) of node indices. (3) A relation is a set of ordered pairs. Therefore,
we may write “E(i, j)” as a paraphrase of “(i, j) ∈E.” (4) We may represent
the relation E(i, j) as a matrix E with entry Eij equal to one if (i, j) is an
edge, and zero otherwise. This matrix has already been introduced: it is the
adjacency matrix of the graph.
An edge connects one vertex to another. One way to identify a class of
edges is by providing two vertex sets S and T; we deﬁne E(S, T) to be the
set of edges directed from a vertex in S to a vertex in T:
E(S, T) = {(i, j)|E(i, j), i ∈S, j ∈T}.
In an unweighted graph, the size of a set of edges is simply the cardinality of
the set:
size(E(S, T)) = |E(S, T)|.
In a weighted graph, the appropriate measure of size is the sum of edge
weights:
size(E(S, T)) =
X
i,j
[[i ∈S, j ∈T]]wij.
An unweighted graph is the special case in which wij is equal to one for all
edges. We deﬁne wij = 0 if there is no edge (i, j), and, contrapositively, we
assume that no edge has a weight of zero. We also assume that weights are
symmetric, hence that E(S, T) = E(T, S). This is tantamount to assuming
an undirected graph.
For a given vertex set S, two edge sets are of particular interest. E(S, S) is
the set of edges belonging to the subgraph of S. The volume of S is its size:
vol(S) = size(E(S, S)) =
X
i,j
[[i, j ∈S]]wij.
The boundary of S is the set of edges that begin in S but terminate outside
of S:
∂S = E(S, ¯S)
where ¯S represents the complement of S within the set of vertices V .
The total weight of edges in E(S, T) is the cut size between S and T:
cut(S, T) = size(E(S, T)).

146
Semisupervised Learning for Computational Linguistics
Of particular interest is the cut size between S and its complement, which is
the total weight of edges in the boundary of S:
cut(S, ¯S) = size(∂S).
(7.8)
If S contains a single node ν, the size of ∂S is the degree of ν. We write di
for the degree of the i-th node:
di =
X
j
wij.
(7.9)
An edge weight often quantiﬁes the similarity between the two nodes con-
nected by the edge, so cut(S, T) can be thought of as the similarity between
vertex sets S and T. At other times, edge weights represent “pipe capacities”
or volumes, so the cut can be thought of as the total volume of connection
between S and T.
The mincut problem is to ﬁnd a nonempty set S ⊂V that minimizes
cut(S, ¯S). We will discuss methods for ﬁnding mincuts, but we must develop
some theory ﬁrst. In particular, we will discuss the Ford-Fulkerson maxﬂow-
mincut algorithm in section 10.6, and we will discuss spectral methods in
section 12.3.
7.6
Label propagation
A third broad approach to clustering is to propagate cluster membership
information (that is, labels) along the edges of the similarity graph. The idea
has arisen in many guises; for example, under the rubric spreading activation
it can be traced back at least to a 1968 paper by Quillian [184]. Though it
has often been proposed with little justiﬁcation beyond intuitive appeal, it
can in fact be placed on a ﬁrm mathematical footing, speciﬁcally in the form
of random walk algorithms that we will discuss in chapter 10.
Propagation algorithms can provide a connection between agglomerative
and divisive algorithms. It is often natural to formulate them in agglomer-
ative terms, as we do in the next section. But when there is a preference
for propagation along high-weight edges and avoidance of low-weight edges,
propagation has the eﬀect of placing boundaries along low-weight cuts. That
eﬀect is heuristic in the algorithms described here, but under appropriate
conditions, a rigorous connection can be established. Details are given in the
discussion surrounding the mincut algorithms, in sections 10.6 and 12.3.
7.6.1
Clustering by propagation
The following algorithm is not one that has been proposed in the cluster-
ing literature, but serves as an illustration. Choose n vertices at random to

Clustering
147
serve as cluster seeds, and assign distinct labels to each of them. Choose an
unlabeled vertex xi that is connected by an edge to a vertex labeled y, and
propagate the label y to the vertex xi. Repeat until no more propagation is
possible.
One can imagine many ways of selecting an edge xi and label y for propa-
gation. Perhaps the simplest would be the “nearest-neighbor” criterion: ﬁnd
the highest-weight edge that has a labeled node at one terminus and an unla-
beled node at the other terminus; let xi be the unlabeled node and y be the
label of the labeled node. Alternatively, one might use a “voting” criterion.
Consider an unlabeled vertex xi. Deﬁne its neighbors to be the vertices that
it is connected to by an edge. To determine which label should be assigned
to xi, the neighbors vote. In particular, if xj is a neighbor labeled y, then xj
casts a vote in favor of y, with weight wij. Note that the total vote is P
j wij,
which is the degree of vertex xi. The vote in favor of assigning label y to node
xi is
v(xi, y) =
X
j
[[yj = y]]wij.
One chooses the unlabeled node x and label y that maximize v(x, y), and one
assigns y as the label of x. Ties are broken arbitrarily. After labeling x, the
vote assignments v(x, y) to the remaining unlabeled nodes are recomputed,
and a new node is chosen for labeling. The process continues until all nodes
have been labeled.
Incidentally, the “nearest-neighbor” criterion can be reduced to the “voting”
criterion by deﬁning
v(xi, y) = max
j [[yj = y]]wij.
As just sketched, this is a clustering algorithm, but it is actually more
natural as a semisupervised learning algorithm. Instead of choosing cluster
seeds at random, we use the labeled data as seed.
7.6.2
Self-training as propagation
It is possible, and informative, to formulate self-training in terms of label
propagation in a graph; Yarowsky [239] in fact described self-training infor-
mally as label propagation on a graph.
The result is quite similar to the
algorithm just sketched.
Characterizing self-training as label propagation is simplest when the base
learner is a variation of a decision list that we might call a voting classiﬁer.
Recall that a decision list is a set of rules of the form F ⇒y where F is a
single feature and y is a label. We assume that the list contains at most one
rule conditioned on a given feature F.
An instance x is treated as a set of features, and a rule F ⇒y matches x
just in case x possesses the feature F. The prediction of a standard decision
list on instance x is the prediction of the highest-scoring rule that matches x.

148
Semisupervised Learning for Computational Linguistics
Mr.
U.S.
Incorporated
_, president
unit of _
ALL CAPS
support from
General Motors
Corporation
PER
LOC
ORG
PER
ORG
LOC
LOC
Mr. Solis, president
unit of Tenneco Incorporated
support from U.S.
unit of General Motors Corporation
diplomacy under Mr. Thomas
criticism from U.S.
PER
ORG
LOC
FIGURE 7.8
Bipartite graph.
A voting classiﬁer diﬀers in that rules lack scores; rather each label y receives
a vote equal to the total count of rules F ⇒y in the list that match x. The
predicted label for x is the label with the largest vote, or undeﬁned in the
case of a tie.
A voting classiﬁer can be represented by introducing nodes corresponding
to features, some of which are labeled. The feature node F is labeled y just
in case F ⇒y belongs to the rule list. If no rule conditioned on F is present
in the list, then the node for feature F is unlabeled. The feature nodes are
included together with instance nodes in a bipartite graph, such as that in
ﬁgure 7.8. There is an edge between instance x and feature F just in case
x possesses the feature F. The graph is unweighted. The voting classiﬁer’s
prediction for instance x can be stated easily in terms of the graph: the label
predicted for x is the one that appears most frequently among the neighbors
of x.
A simple version of self-training that constructs a voting classiﬁer is repre-
sented by the following label propagation algorithm. Initially, a subset of the
nodes is labeled. Let us assume that the initially labeled nodes are all instance
nodes, corresponding to a seed consisting of labeled data. (If the seed were
a classiﬁer, the initially labeled nodes would be feature nodes. Obviously, a
mixture is also possible.)
We begin by propagating labels from instance nodes to feature nodes. The
label predicted for a given feature node F is computed in the same way as
the label for an instance node: each labeled neighbor of F votes for its own
label, and the label with the most votes wins.
There is no winner in the
case of a tie. The measure of conﬁdence is the percent of the vote that the
winner receives. A conﬁdence threshold may be imposed, either in the form

Clustering
149
PER
LOC
ORG
Mr. Solis, president
Mr.
support from U.S.
U.S.
Tenneco
unit of
Inc.
support from
president
unit of Tenneco, Inc.
FIGURE 7.9
Propagation into the interior.
of an absolute threshold, or in the form of a quota on the number of feature
nodes that receive labels. Persistence may be enforced, either in the sense
that the label of a feature node never changes once assigned, or in the sense
that a feature node once labeled never becomes unlabeled, though the label
may change. It should be clear that label propagation from instance nodes
to feature nodes corresponds to the training step in self-training. Its product
is a partial labeling of the feature nodes, which, as we have already noted, is
equivalent to a voting classiﬁer.
In the second step, labels are propagated from feature nodes back to in-
stance nodes.
This corresponds to the labeling step of self-training.
The
label predicted for an instance node is determined by the vote of its neigh-
bors, which corresponds to the prediction of the voting classiﬁer. Conﬁdence
thresholds, throttling, persistence, and the other options previously discussed
are available.
The algorithm then repeats, propagating labels from instance nodes to fea-
ture nodes and back again, until a stopping criterion is met.
It should be plain that the label propagation algorithm just described is
equivalent to self-training, with the exception that it constructs voting clas-
siﬁers rather than decision lists.
Another way of visualizing the same graph is to place the seed-labeled
items at the perimeter and the unlabeled items in the interior, as in ﬁgure
7.9. The squares represent instances and the circles represent features. The
labeled nodes on the perimeter represent the seed, and the question is how
to propagate labels into the interior, eﬀectively partitioning the graph into
components, one for each label. This way of viewing the problem is pursued
in much greater detail in chapter 10, on graph methods.

150
Semisupervised Learning for Computational Linguistics
a 1
a 2
a 3
b 1
b 2
b 3
FIGURE 7.10
Bipartite graph of half-instances.
7.6.3
Co-training as propagation
Impressionistically, co-training involves label propagation in a bipartite graph,
reminiscent of the bipartite feature-instance graph (ﬁgure 7.8) that arose in
connection with self-training, but involving the two views. Each instance xi =
(ai, bi) consists of two half-instances ai and bi, each being the representation
of the instance under one view. That is, view1(xi) = ai and view2(xi) = bi.
For example, consider the following data set, containing four instances:
x1 = (a1, b1)
x2 = (a2, b1)
x3 = (a3, b3)
x4 = (a3, b2).
It can be depicted as the bipartite graph in ﬁgure 7.10. The edges in the
graph represent individual instances. Labels are assigned to instances (edges),
meaning that both half-instances must receive the same label.
The graph
representation embodies an implicit assumption that a given half-instance is
labeled the same way wherever it occurs. The consequence is that labeling any
instance in a connected component of the graph eﬀectively labels the entire
connected component.
This way of looking at it is actually not quite right. If instances are at all
complex, then half-instances will not recur with any appreciable frequency. A
half-instance ai under the ﬁrst view is a set of features, as is the half-instance
bi under the second view. Their disjoint union makes up the features of the
instance xi. Rather than taking half-instances as nodes, it is more reasonable
to take individual features as nodes. A pair of views is essentially a partition-
ing of the universe of features into two sets; a view is one of two disjoint classes
of features. If we interpret the nodes in ﬁgure 7.10 as features, rather than
half-instances, then the edges represent co-occurence. Two features co-occur
if there is some instance that possesses them both.

Clustering
151
b 1
b 2
b 3
a 1
a 2
a 3
x 1
x 2
x 3
x 4
FIGURE 7.11
A tripartite graph representation for co-training. The circles on the left are
view-one features, those on the right are view-two features, and the square
nodes in the center are instances.
Under that interpretation, however, it is rather too rigid to assume that, if
any instance possessing feature F has label y, then every instance possessing F
has label y. It is reasonable to expect certain features to be highly correlated
with a given label – such a correlation makes the rule F ⇒y reliable – but
it is too rigid to expect the correlation to be perfect.
Label propagation
along edges should not be automatic, leading entire connected components
to have the same label, but should be somewhat “softer,” like the voting
method discussed in the previous section for self-training. In fact, for better
conformance to the co-training algorithm, we should introduce instances as
explicit nodes, as we did with self-training.
These considerations suggest representing instances as follows:
x1 = (a1; b1)
x2 = (a1, a2; b1, b2)
x3 = (a3; b3)
x4 = (a3; b2, b3)
in which the ai represent features belonging to view one, and the bi represent
features belonging to view two; the semicolon separates the features of the
two views. For example, the instance x2 consists of the two half-instances
(a1, a2) and (b1, b2).
The corresponding graph representation is given in ﬁgure 7.11. The graph
is now tripartite: the square nodes in the center represent instances, the
circles on the left represent features of view one, and the circles on the right
represent features of view two. In the training step, labels are propagated

152
Semisupervised Learning for Computational Linguistics
from the instances outward to the feature nodes of both views, and in the
labeling step, labels are propagated back from the features to the instances.
The algorithm is actually identical to that described for self-training in the
previous section (7.6.2). The distinction between view-one features and view-
two features plays no role in the algorithm itself, though it will play a role in
our understanding of what the algorithm is trying to accomplish.
We will return to the discussion of co-training in the context of agreement-
based methods (chapter 9).
7.7
Bibliographic notes
Many volumes have been written on clustering.
I have found Massart &
Kaufman [145] useful. The discussion of distance measures largely follows du
Toit et al. [80]. The discussion of relevance feedback follows Rocchio [196]; see
also [203, 116]. Citations for pseudo relevance feedback include [82, 5, 32, 125].

8
Generative Models
We have previously seen some examples of generative models: the Naive Bayes
classiﬁer, for example, and the null category noise model. Generative models
characterize the joint distribution p(x, y) of instances and labels, usually by
separately characterizing the prior distribution of classes p(y) and the class-
speciﬁc instance distribution or likelihood p(x|y). This is the source of the
name: a generative model represents a hypothesis about how instances are
generated for each class. For classiﬁcation, the generative probability can be
reversed using Bayes’ rule:
p(y|x) = p(y)p(x|y)
p(x)
.
For a given instance x, the denominator is constant, so we can maximize
p(y|x) by maximizing the numerator on the right-hand side, the product of
prior and likelihood.
Generative models, particularly mixture models, are also applicable to clus-
tering. Mixture models have in fact become almost synonymous with cluster-
ing in current research. They are the ﬁrst topic we address in this chapter.
In addition to being a prominent approach to clustering, they are also the
basis for the earliest examples of semisupervised learning methods, namely,
McLachlan’s method and Expectation-Maximization (EM).
8.1
Gaussian mixtures
8.1.1
Deﬁnition and geometric interpretation
If one has real-valued attributes, a simple model of data generation is a mix-
ture of Gaussians. A separate multi-dimensional Gaussian, or normal, distri-
bution p(x|y) is assumed for each class, and data is generated by ﬁrst choosing
a class according to p(y) and then generating a data point by sampling from
p(x|y).
A multi-dimensional Gaussian distribution is characterized by a center point
or mean m, and a covariance matrix C giving the amount of spread in each
direction. The distribution has its maximum density at the center, and the
153

154
Semisupervised Learning for Computational Linguistics
A
B
m
m
A
B
FIGURE 8.1
A two-dimensional Gaussian density.
probability of generating an instance decreases as one moves away from the
center. The contours of equal probability form ellipses. If one looks at the
probability distribution along a principal axis of the ellipse, its shape is that
of a (one-dimensional) normal distribution (ﬁgure 8.1).
The normal curve is S-shaped as one moves from the center outward. The
distance from the center to the point of recurve is the standard deviation,
or the square root of the variance. It can be thought of as the radius of the
distribution. In ﬁgure 8.1, the mean is the point labeled m, and the contour
at one standard deviation (one sigma) from the center is marked with a heavy
line.
In a univariate Gaussian, there is one “radius” or “spread” parameter σ, but
in a general multivariate Gaussian, the radius depends on the principal-axis
direction one measures it in. The single number σ2 (the variance) is replaced
by a matrix (the covariance matrix).
One can understand an arbitrary multi-dimensional Gaussian as the result
of scaling, rotating, and translating a standard Gaussian. We start with points
z that have a circular Gaussian distribution with a standard deviation of one,
centered at the origin. To scale the distribution, we premultiply by a diagonal
matrix
S =


σ1 0 . . . 0
0 σ2 . . . 0
...
... ... ...
0
0 . . . σn


whose entries are the standard deviations along each principal axis of the
ellipse. We then premultiply by a rotation matrix R, and ﬁnally we add m

Generative Models
155
S
R
+m
FIGURE 8.2
An arbitrary multivariate Gaussian is related to a standard Gaussian via
scaling S, rotation R, and translation.
to re-center the result. These steps are illustrated in ﬁgure 8.2. In short, we
deﬁne
x = m + RSz.
The inverse transformation takes the actual data points x and maps them to a
coordinate space in which their distribution is a standard normal distribution:
z = S−1R−1(x −m).
This transformation is known as whitening. (Why is it called “whitening”?
As we will see in chapter 12, the spectrum of a matrix corresponds to its radius
along each of its principal axes. White noise has equal power throughout its
spectrum, meaning that it has the same radius along all principal axes: it is
circular.)
The multivariate standard normal p(z) is deﬁned as
p(z) = 1
Z exp

−1
2||z||2	
(8.1)
where Z is a normalizing constant. The probability density falls oﬀexpo-
nentially with distance from the origin, and the contours of equal density are
circles centered on the origin.
A mixture of Gaussians is a distribution in which each class y is associated
with a diﬀerent Gaussian. We will limit attention to mixtures in which the
Gaussians have the same covariance matrix but diﬀerent centers. In such a
case, it is convenient to apply the scaling and rotation to the entire space,
mapping the original data to what we might call the “whitened space.” We
use primes for vectors in the whitened space:
x′ = S−1R−1x

156
Semisupervised Learning for Computational Linguistics
x´
m´1
m´2
FIGURE 8.3
Two Gaussians.
m′
i = S−1R−1mi.
In the whitened space, (8.1) becomes
p(x′|yi) = 1
Z exp

−1
2||x′ −m′
i||2	
.
(8.2)
Figure 8.3 illustrates the case with two Gaussians, showing only the centers
m′
1 and m′
2 and the one-sigma contours.
The contours are the same size
and shape for both, meaning that both Gaussians have the same covariance
matrix, and the contour shape is circular, meaning that the covariance matrix
is a diagonal matrix with the same constant value for every diagonal entry. We
can take that value to be one. This illustrates the situation after whitening,
whatever the covariance matrix was for the original data.
An instance (x′, y) is generated by choosing y = 1 or y = 2 at random,
according to a prior distribution p(y), and then choosing x′ from the Gaussian
distribution centered at m′
y. Knowing only the point x′, as in ﬁgure 8.3, there
are two possibilities.
It might have been generated from the distribution
centered at m′
1 or the distribution centered at m′
2. Intuitively, it seems clear
that class 1 (centered at m′
1) is the more likely source in this case, but the
probability is nonzero for either distribution. Depending on the prior p(y2), it
might even turn out that class 2 is the more likely source. But in the simplest
case, which we henceforth assume, the classes have equal prior probabilities,
and our original intuition is valid.
8.1.2
The linear discriminant decision boundary
Linear discriminant analysis addresses the classiﬁcation problem in the case
we are considering, namely, a mixture of Gaussians with a common covariance

Generative Models
157
matrix and equal priors. It can be shown that the optimal decision boundary
between two adjacent centers consists of the set of points equidistant from
the centers in the whitened space. This is the same linear boundary as with
a nearest-neighbor algorithm, as in ﬁgure 5.5 (page 71).
That the decision boundary is equidistant from the two centers is not hard
to see in the one-dimensional case. The univariate Gaussian distribution is
p(x|yj) = 1
Z exp
(
−1
2
x −mj
σ
2)
.
The log is easier to work with:
log p(x|yj) = −log Z −1
2
x −mj
σ
2
.
To determine whether y1 or y2 is the more likely source, it suﬃces to consider
the ratio of the probabilities: if p(x|y1)/p(x|y2) > 1, then y1 is more likely,
and if it is less than one, y2 is more likely. The decision boundary lies where
the likelihood ratio is equal to one, which is to say, where the log likelihood
ratio ℓis equal to zero:
ℓ= log p(x|y1)
p(x|y2)
= 1
2
x −m2
σ
2
−1
2
x −m1
σ
2
=
1
2σ2

2x(m1 −m2) + m2
2 −m2
1

= m1 −m2
σ2

x −m1 + m2
2

.
(8.3)
The value a = (m1+m2)/2 is the midpoint between the two centers. If m1 lies
to the left of m2, then m1−m2 is negative, and the log likelihood ratio (8.3) is
proportional to the “signed distance” a−x. It is positive when x < a, that is,
when x and m1 both lie to the left of a. It is negative when x is on the same
side as m2. Hence a positive value indicates m1 and negative indicates m2,
with the decision boundary lying at a, the point that is equidistant between
m1 and m2.
A similar characterization holds in the multidimensional case as well. Using
the probability distribution (8.2), the log likelihood ratio is
ℓ= 1
2||x′ −m′
2||2 −1
2||x′ −m′
1||2
= 1
2(x′ −m′
2) · (x′ −m′
2) −1
2(x′ −m′
1) · (x′ −m′
1)
= (m′
1 −m′
2) · x′ −||m′
1||2 −||m′
2||2
2
= (m′
1 −m′
2) · x′ −(m′
1 −m′
2) · m′
1 + m′
2
2
.
(8.4)

158
Semisupervised Learning for Computational Linguistics
We see, by the form of (8.4), that ℓcorresponds to the distance from x′ to
a separating hyperplane whose normal vector is m′
1 −m′
2. The point that
makes ℓ= 0 is x′ = (m′
1 + m′
2)/2, the midpoint of the line segment m′
1 −m′
2.
That is, the decision boundary is the separating hyperplane that is halfway
between m′
1 and m′
2, perpendicular to the line segment connecting them, and
the log likelihood ratio ℓis proportional to the signed distance from x′ to the
decision boundary. The sign of ℓindicates the nearest center (positive for m′
1
and negative for m′
2), and |ℓ| is the distance away from the boundary. As in
the nearest-neighbor case, the sign can be interpreted as a prediction, and the
absolute value can be interpreted as a conﬁdence.
A ﬁnal point requires clariﬁcation. In the previous section, we mentioned
the covariance matrix C and implied that it is connected to the amount of
spread in each principal axis direction, that is, to the matrix S. But to this
point, we have not actually drawn a formal connection between C and S.
Writing out (8.2) in terms of the original x and mi, we have
p(x|yi) = 1
Z exp

−1
2[S−1R−1(x −mi)]T[S−1R−1(x −mi)]
	
= 1
Z exp

−1
2(x −mi)TRS−2RT(x −mi)
	
.
Here we have taken advantage of two facts: since S−1 is diagonal, transposing
it has no eﬀect, and since R is a rotation matrix, its inverse is equal to its
transpose.
Though we will not prove it, it turns out that the covariance matrix C can
be expressed as
C = RS2RT.
(8.5)
The rotation R is the same as we have seen all along: it determines the
directions of the principal axes along which the scaling S2 is applied. Since S
is a diagonal matrix whose elements are the standard deviations in each axis
direction, the scaling implicit in C, namely S2, is a diagonal matrix whose
elements are the variances in each principal axis direction.
The decomposition (8.5) is signiﬁcant. Any symmetric matrix (the covari-
ance matrix is symmetric) can be decomposed into the form QΛQT where
Q represents a rigid rotation and Λ represents a scaling. Equivalently, Q is
an orthonormal matrix, meaning that its columns are perpendicular unit vec-
tors, and Λ is a diagonal matrix; for this reason, the decomposition QΛQT
is called diagonalization. Further, the columns of Q are the eigenvectors of
the decomposed matrix, and the elements along the diagonal of Λ are the
corresponding eigenvalues. In the case of C, we have Q = R and Λ = S2.
We will examine diagonalization in more detail in chapter 11.
Since RTR = RRT = I and S2S−2 = I, we have
(RS2RT)(RS−2RT) = I
implying that:
C−1 = RS−2RT.

Generative Models
159
This yields the multivariate Gaussian distribution in the form one most com-
monly encounters:
p(x|yi) = 1
Z exp

−1
2(x −mi)TC−1(x −mi)
	
.
And the log likelihood ratio (8.4) becomes
ℓ= (m1 −m2)TC−1

x −m1 + m2
2

.
(8.6)
8.1.3
Decision-directed approximation
From this point, we will assume a whitened space, but drop the primes. Let us
represent the locations of the centers as M = (m1, . . . , mk). Each possibility
for M represents a diﬀerent model. We have seen that, given a point x and a
choice between two labels, the label that maximizes p(x|y, M) is represented
by the sign of the log likelihood ratio, and it is the label that corresponds to
the nearer of the two centers. Since this is true for every pairwise choice, it
follows that the label that maximizes p(x|y, M) overall is the label represented
by the nearest center. By Bayes’ rule:
p(y|x, M) = p(y|M)p(x|y, M)
p(x|M)
.
Since p(y|M) is uniform across choices of y, by assumption, and p(x|M) is
constant with respect to y, it follows that maximizing the likelihood p(x|y, M)
is the same as maximizing the posterior p(y|x, M). This provides us with a
theoretical justiﬁcation for the nearest-neighbor classiﬁer: under the assump-
tions given, it maximizes the probability p(y|x, M). If X is an i.i.d. sample
of instances, and Y is an assignment of labels to each, it follows that apply-
ing the nearest-neighbor classiﬁer to each instance independently yields the
labeling Y that maximizes p(Y |X, M) for the given values of X and M.
In unsupervised learning, we are given X but not M. A Bayesian approach
would ask for the labeling that maximizes p(Y |X), integrating over all possible
values of M. Since X is ﬁxed, we can maximize p(Y |X) by maximizing
p(X, Y ) =
Z
M
p(X, Y |M)dp(M).
(8.7)
If there are k centers, each being an m-dimensional vector, then a model M
can be represented as a point in a km-dimensional space. For a given choice
of Y , and remembering that X is ﬁxed, p(X, Y |M) is a function deﬁned for
each point M in the model space. The prior density p(M) can be thought of
as a distortion of the model space, stretching regions of high prior probability
and shrinking regions of low prior probability, so that the area of a region in

160
Semisupervised Learning for Computational Linguistics
M
p(X,Y   M)
1
p(X,Y   M)
2
FIGURE 8.4
Cross sections through two surfaces p(X, Y |M). The area under the surface
is p(X, Y ).
the distorted space corresponds to its prior probability. Then one can think
of p(X, Y |M) as a surface over the distorted space. The value of interest,
p(X, Y ), is the area under that surface.
Figure 8.4 shows a cross section
through two diﬀerent hypothetical surfaces, for two diﬀerent choices of Y .
When we are ignorant of the prior density p(M), we cannot compute (8.7).
An expedient in that case is to assume that the surfaces p(X, Y |M) for dif-
ferent values of Y have similar shape but diﬀer in height (and hence, area
under the graph) and in the location of their highest point. Then the value
p(X, Y |M) at the highest point can be used as a proxy for the area under the
graph, and instead of maximizing (8.7), we maximize p(X, Y |M) for both Y
and M.
When Y is known, choosing M so as to maximize p(X, Y |M) is the max-
imum likelihood principle. In the case of a mixture of Gaussians, it can be
shown that the value M that maximizes the likelihood p(X, Y |M), for ﬁxed
X and Y , is obtained by taking the vector means of the training instances
that share a given label. That is, for each label j,
ˆmj =
1
|Xj|
X
x∈Xj
x
where Xj is the set of training instances that are labeled with the j-th label.
This provides justiﬁcation for choosing centroids as cluster centers, as in k-
means clustering – again, under the assumption of a mixture of Gaussians
with uniform priors, and remembering that we are working in the whitened
space.
When Y is unknown, a plausible heuristic is to start out with some con-
venient value for M, maximize Y while holding M constant, then hold the

Generative Models
161
FIGURE 8.5
Alternating maximization of M and Y may not ﬁnd global maximum.
new Y constant and maximize M.
By repeated alternation, one hopes to
ﬁnd the global maximum. This is a heuristic, because it can fail to ﬁnd the
global maximum even if one exists. Think of a space with values of M as one
dimension and values of Y as another. The alternating method does searches
in axis-parallel directions, as in ﬁgure 8.5. More realistically, the searches
are not done along lines (one-dimensional subspaces), but within very high-
dimensional subspaces.
The k-means algorithm is an application of this alternating maximization
heuristic to the case of a mixture of circular Gaussians with uniform priors.
We begin with M ﬁxed but arbitrary and choose Y to maximize p(X, Y |M).
Given the assumption that p(Y |M) is the same for all Y , this is the same as
maximizing p(X|Y, M), which is what the nearest-neighbor algorithm does.
Then we hold the new Y ﬁxed and choose M to maximize p(X, Y |M), which
is what the centroid rule does. Alternating nearest-neighbors with centroid
estimation until convergence yields the k-means algorithm.
The alternating maximization heuristic can be applied more generally. Let
train be any supervised learning algorithm that chooses M to maximize
likelihood p(X, Y |M) for ﬁxed X, Y , and let label be a prediction rule that
chooses Y so as to maximize p(Y |X, M) for ﬁxed X and M. We continue to
assume uniform p(Y |M), so that the Y that maximizes p(Y |X, M) is the same
Y that maximizes p(X, Y |M). Using these relaxed deﬁnitions of train and
label in the k-means algorithm (ﬁgure 7.4, page 139) is known as decision-
directed approximation.
If we replace the random initialization in decision-directed approximation
with either labeled seed instances or a seed classiﬁer, then we obtain a version
of self-training without the conﬁdence threshold.

162
Semisupervised Learning for Computational Linguistics
8.1.4
McLachlan’s algorithm
One of the earliest forms of self-training was proposed by McLachlan [149] as
a way of extending linear discriminant analysis to the case of mixed labeled
and unlabeled data. In form, the algorithm is identical to decision-directed
approximation, but the assumption of Gaussian mixtures and the particular
choice for train and label make it an exact, rather than heuristic, algorithm.
The algorithm receives a mixture of labeled and unlabeled instances as
input. It begins by using the labeled instances to estimate the means and
covariance matrix one needs for the log likelihood (8.6), yielding a conﬁdence-
weighted classiﬁer f0(x) whose value is ℓin (8.6). The classiﬁer is applied
to the unlabeled data, and its predictions are taken at face value. A new
classiﬁer f1 is trained on both the labeled data and unlabeled data, using the
predictions of f0 as labels for the unlabeled data. Then the process repeats.
In a little more detail, let P0 be the set of positive labeled instances (in the
labeled seed) and N0 be the negative labeled instances. Deﬁne Pt to be the
union of P0 and the unlabeled instances x for which ft(x) ≥0, and deﬁne Nt
to be the complement. The classiﬁer ft is deﬁned as
ft(x) = (pt −nt)TC−1
t

x −pt + nt
2

where pt is the centroid of Pt, nt is the centroid of Nt, and the covariance
matrix Ct is computed by pooling both Pt and Nt. The class that ft predicts
for x is the sign of ft(x).
McLachlan proves that the algorithm converges, and reduces risk below
what is achieved using the labeled data alone.
The algorithm is like self-
training in that the predictions of the classiﬁer are taken at face value, and
the resulting labeled data is used to train a new classiﬁer. It should be noted,
however, that there is no thresholding and no throttling.
It is intuitively clear that the two-Gaussian assumption creates conditions
under which unlabeled data helps. Consider a sample of instances like that
in ﬁgure 8.6. Most of the instances are unlabeled, but a few are labeled “+”
or “−”. The centroids of the labeled data are marked by the circled plus
“⊕” and circled minus “⊖”. One can tell by the distribution of data points
(both labeled and unlabeled) that there is signiﬁcant error in the means due
to sampling error; the small size of the labeled sample distorts the covariance
estimates as well. It seems clear that one can improve the estimates by any
reasonable method of assigning unlabeled data points to the two classes. Only
for a minority of instances in the vicinity of the decision boundary will there
be uncertainty about the correct classiﬁcation.
The fact that McLachlan uses a hard assignment of labels makes the al-
gorithm similar to self-training.
An obvious alternative is to make a soft
assignment. The idea is to split each unlabeled instance into two fractional
instances, one labeled positive and one labeled negative. The relative size of
the two fractional instances is determined by one’s certainty or uncertainty

Generative Models
163
+
–
+
+
–
–
FIGURE 8.6
Mixture of labeled and unlabeled data.
about the true label. Instances near the decision boundary are split nearly in
half, while instances further away are assigned almost entirely to the class in
whose region they lie. This idea leads to the EM algorithm, which we discuss
next.
8.2
The EM algorithm
Self-training is arguably the simplest semisupervised learning algorithm, but
it was not the ﬁrst that was known in computational linguistics. Statistical
methods were introduced to computational linguistics from speech recogni-
tion, and the Expectation-Maximization (EM) algorithm was a mainstay of
speech recognition, in the guise of the Baum-Welch algorithm for unsupervised
training of Hidden Markov Models (HMMs).
The EM algorithm is a very general tool, and its use for semisupervised
learning is only one application, and not necessarily the best known. It is more
commonly used for dealing with missing attributes, or for doing maximum
likelihood calculations when direct calculation proves intractable.
We will
restrict our attention to its use for semisupervised learning.
8.2.1
Maximizing likelihood
To explain the EM algorithm, we begin by considering maximum-likelihood
estimation. We have already encountered likelihood, and gotten some idea of
how common it is as an objective function. Likelihood is simply the probabil-

164
Semisupervised Learning for Computational Linguistics
ity of the training data (X, y) according to a model θ, viewed as a function
of θ:
L(θ) = p(X, y; θ).
Formally, X is a matrix whose rows are individual instances xi and y is a
column vector whose elements yi are the instance labels.
Since the instances in the training sample are drawn i.i.d. from the instance
distribution, the likelihood can be written:
L(θ) =
Y
i
p(xi, yi; θ).
Taking the log does not change the identity of the minimizer, and it will make
the maximization easier:
ℓ(θ) = log L(θ) =
X
i
log p(xi, yi; θ).
We can convert from a summation over the indices of the training sample to
a summation over the instance space by the trick of replacing p(xi, yi) with
P
x,y[[x = xi, y = yi]]p(x, y):
ℓ(θ) =
X
i
X
x,y
[[x = xi, y = yi]] log p(x, y; θ)
(8.8)
=
X
x,y
c(x, y) log p(x, y; θ)
(8.9)
where c(x, y) represents the count of the labeled instance (x, y) in the data:
c(x, y) =
X
i
[[x = xi, y = yi]].
8.2.2
Relative frequency estimation
We will illustrate the usefulness of the log likelihood (8.9) by deriving the
maximum-likelihood solution for a family of models that includes a wide va-
riety of commonly occurring cases. These are cases in which the probability
p(x, y) is a product of parameters θjk representing probabilities of some as-
pect or part of (x, y). For example, in the Naive Bayes model, the quantities
θjk correspond to conditional probabilities of assigning a particular value (k)
to a particular attribute (j), given the label y. Or in a Markov model, the
quantities θjk represent transition and emission probabilities, that is, the con-
ditional probability of a transition to state k given that the current state is
j, or the conditional probability of emitting the k-th symbol, given that the
current state is j.

Generative Models
165
In general, the subscript j ranges over diﬀerent kinds of stochastic choices
that are faced while generating (x, y), and k ranges over diﬀerent outcomes of
the stochastic choice. The stochastic choice may be conditionalized on parts
of the structure that have already been generated. That is, the stochastic
choice is identiﬁed with a ﬁxed distribution over outcomes, and the subscript
j distinguishes not only the kind of choice but also any aspect of the context
that aﬀects the distribution of outcomes. Let us write Ejk for the event in
which the result of the stochastic choice j is outcome k. The parameter θjk
represents the conditional probability of Ejk, given that the stochastic choice
being made is stochastic choice j.
A given event may in principle occur more than once, or not at all, in a
given labeled instance. We deﬁne fjk(x, y) to be the number of times that
the event Ejk occurs in (x, y). Then the probability of the complete labeled
instance can be written in the form:
p(x, y; θ) =
Y
jk
θfjk(x,y)
jk
.
(8.10)
This is known as exponential form. The model θ = (. . . , θjk, . . .) consists of
a choice of value for each parameter θjk.
For models that can be expressed in exponential form, the log likelihood
(8.9) becomes
ℓ(θ) =
X
x,y
c(x, y) log
Y
jk
θfjk(x,y)
jk
=
X
x,y
c(x, y)
X
jk
fjk(x, y) log θjk.
(8.11)
In the second line, “c(x, y)” represents the count of the labeled instance (x, y),
that is, n˜p(x, y). Deﬁne the total count of event Ejk to be
cjk =
X
x,y
c(x, y)fjk(x, y).
Then (8.11) can be written simply as
ℓ(θ) =
X
jk
cjk log θjk.
We wish to maximize ℓ, but we must do so under the constraint that the
sum of probabilities of outcomes for any given stochastic choice is one. That
is
X
k
θjk = 1
∀j.
We can place the constraints in the form µt = 0 by deﬁning
µt =
X
k
θtk −1.

166
Semisupervised Learning for Computational Linguistics
To do a constrained maximization, we set the gradient of the likelihood equal
to a linear combination of the gradients of the constraints:
∂
∂θjk
ℓ=
X
t
λt
∂
∂θjk
µt.
(8.12)
To solve this, we ﬁrst ﬁnd expressions for each of the partial derivatives:
∂
∂θjk
ℓ= cjk
θjk
∂
∂θjk
µt = [[j = t]].
Substitute these expressions into (8.12) and solve, to obtain
cjk
θjk
=
X
t
λt[[j = t]]
θjk = cjk
λj
.
(8.13)
Now we use the constraint that P
k θjk = 1:
X
k
cjk
λj
= 1
λj =
X
k
cjk.
Substituting this expression for λj into (8.13) yields
θjk =
cjk
P
k′ cjk′ .
(8.14)
Very simply, for a particular stochastic choice j, the maximum likelihood
estimate of the probability of the k-th outcome is its relative frequency. That
is, of the times when the j-th stochastic choice occurred in the training data,
what proportion resulted in the k-th outcome?
8.2.3
Divergence
The expression we derived above for log likelihood (8.9) can be re-expressed as
a comparison between two probability distributions. One is obvious: it is the
distribution p(x, y; θ). The other is implicit in the counts c(x, y). Namely, if
we normalize counts, we obtain a probability distribution called the empirical
distribution, and conventionally written ˜p. If there are n training instances,
then ˜p(x, y) = c(x, y)/n, and equation (8.9) becomes
ℓ(θ) = n
X
x,y
˜p(x, y) log p(x, y; θ).

Generative Models
167
In this form, ℓis closely related to an information-theoretic quantity known
as cross entropy. In the absence of a standard notation, we will write the
cross entropy of p and q as H(p||q), deﬁned
H(p||q) = −
X
ω
p(ω) log q(ω).
The log likelihood (8.9) can be written as
ℓ(θ) = −nH(˜p||pθ)
(8.15)
where pθ represents the probability distribution p(·; θ).
To explain the signiﬁcance of cross entropy, a quick review of the basic
concepts of information theory is useful. The point entropy η(ω) of an item
ω is a measure of the unexpectedness of ω, and is inversely related to its
probability, namely
η(ω) = log
1
p(ω).
Entropy is the average value of point entropy, measured using p:
H(p) = −
X
ω
p(ω) log p(ω).
It quantiﬁes our uncertainty about which item we get if we sample from p;
it is maximized by the uniform distribution, and it is minimized, with value
zero, by a point distribution in which a single item has probability one.
The cross entropy H(p||q) can be thought of as an estimate of H(p) where
we sample from p but compute point entropies using q. For example, p may be
the unknown distribution underlying a physical process, and q is a hypothesis
about it. We can physically sample from p, and we can use the guess q to
compute point entropies.
The cross entropy can be expressed as a sum:
H(p||q) = −
X
ω
p(ω) log q(ω)
= −
X
ω
p(ω) log p(ω) +
X
ω
p(ω) log p(ω)
q(ω)
= H(p) + D(p||q).
The quantity D(p||q) is the divergence of p from q. It can be shown that
divergence is nonnegative, and is equal to zero just in case the distributions
p and q are identical. Here is a quick proof. The graph of the natural log
function y = ln x touches the line y = x −1 at the point x = 1, y = 0 (ﬁgure
8.7), and lies below it everywhere else. That is
ln x ≤x −1
with equality iﬀx = 1.

168
Semisupervised Learning for Computational Linguistics
y=x-1
y=lnx
FIGURE 8.7
The graph of ln x lies everywhere below the graph of x−1, except at the point
x = 1.
Set x equal to the ratio q(ω)/p(ω) for an arbitrary item ω:
ln q(ω)
p(ω) ≤q(ω)
p(ω)
with equality iﬀp(ω) = q(ω).
Since this is true for arbitrary ω, it is true if we average over choices of ω, and
in particular if we take the expectation with respect to p:
X
ω
p(ω) ln q(ω)
p(ω) ≤
X
ω
p(ω)q(ω)
p(ω) −1
(8.16)
with equality if and only if p(ω) = q(ω) everywhere, that is, if and only
if p = q. Now observe that the left-hand side of (8.16) is the negative of
divergence, and the right-hand side simpliﬁes to zero. That is, D(p||q) ≥0,
with equality if and only if p = q.
This property makes divergence somewhat like a measure of distance be-
tween two distributions, though it is not strictly speaking a distance metric –
in particular, it is not symmetric, and it does not satisfy the triangle inequal-
ity. But the fact that divergence is nonnegative means that
H(p||q) ≥H(p).
In other words, if q is a hypothesis about p, and we sample from p but use q
to compute point entropies, then the estimated entropy H(p||q) is an overesti-
mate unless q is identical to p. If p is ﬁxed, and we vary q, we can ﬁnd a best
approximation to p by minimizing H(p||q). Indeed, if p is ﬁxed, then mini-
mizing cross entropy is the same as minimizing divergence, and minimizing
divergence can be thought of as ﬁnding the hypothesis q that is most similar
to the unknown distribution p.

Generative Models
169
Returning to (8.15), we have
ℓ(θ) = −n[H(˜p) + D(˜p||pθ)].
Since H(˜p) is constant across choices of θ, it follows that maximizing log
likelihood is the same as minimizing divergence:
arg max
θ
ℓ(θ) = arg min
θ
D(˜p||pθ).
(8.17)
As an objective function, divergence from the empirical distribution is equiva-
lent to likelihood – both sort hypotheses in the same way, and the model that
minimizes divergence is the same as the one that maximizes likelihood. Or to
say it another way, adopting likelihood as an objective function means that,
among the distributions pθ in our hypothesis space, we seek the one that is
nearest (in the sense of divergence) to the sample distribution.
8.2.4
The EM algorithm
With this background, we turn to the EM algorithm. As usually formulated,
EM estimates a density p(x, y), rather than learning a classiﬁer. But a clas-
siﬁer is implicit in the density: for a given instance x, choose the label that
maximizes p(x, y). This is equivalent to maximizing
p(y|x) = p(x, y)
p(x)
since the denominator is constant across choices of label.
If we had complete, labeled training data (X, y), and an appropriate model
class, we could use maximum likelihood estimation as described in the previous
section. When we have only partial information about the training data, EM
comes into play. There are several settings in which some of the training data
might be missing. For example, in the classic missing data scenario, some
attribute values are unknown for some instances, that is, part of X is missing.
Alternatively, if the labels y are missing, we have an unsupervised setting, in
which case EM is essentially a clustering algorithm. Or again, some of the
labels (part of y) may be absent and the rest present, in which case we have a
semisupervised learning problem; this last case is the one of particular interest
to us.
Let us write v for the part of (X, y) that is provided (“visible”), and u for
everything that is missing (“unseen”). In the case of particular interest, v
represents all the instances, along with the labels for the labeled instances,
and u represents the labels for the unlabeled instances. The log likelihood of
a model θ becomes
ℓ(θ) = log p(v; θ) = log
X
u
p(u, v; θ).
(8.18)

170
Semisupervised Learning for Computational Linguistics
In this case, taking the log does not help, because what is inside the log is a
sum, not a product.
The EM algorithm provides a way to deal with this diﬃculty.
It is an
iterative algorithm. Instead of maximizing log likelihood (8.18) directly, it
starts with a guess θ(0), and constructs a sequence θ(0), θ(1), . . . , θ(t), . . . of
models with increasing values of likelihood.
The initial model θ(0) can come from anywhere. We could choose one at
random. In the semisupervised setting, we could obtain an initial model by
ignoring the unlabeled instances and training just on the labeled instances.
That would give us the model that maximizes likelihood on the labeled in-
stances, which is not necessarily the same as the model that maximizes p(v; θ)
(recall that v includes not only the labeled instances but also additional in-
stances without labels), but is a natural place to start.
At each step of the algorithm, we use θ(t) to do a soft labeling of the unlabeld
instances, and then we maximize log likelihood of the resulting pseudo-labeled
data. That is, we compute a new model θ(t+1) by maximizing expected log
likelihood, averaging across all possibilities for u in accordance with the prob-
abilities assigned by the old model θ(t):
θ(t+1) = arg max
θ
X
u
p(u|v; θ(t)) log p(u, v; θ).
(8.19)
The expression that is being maximized is called the EM auxiliary function.
It is usually written as Q(θ; θ(t)), but to emphasize its role as a pseudo log
likelihood, I prefer to write:
Qt(θ) =
X
u
p(u|v; θ(t)) log p(u, v; θ).
(8.20)
The value Qt(θ) in (8.20) is not equal to ℓ(θ) in (8.18).
But repeatedly
applying (8.19) does yield a sequence of models with increasing likelihood.
Before examining how we know that the update (8.19) increases likelihood,
let us ﬁrst consider why maximizing Qt(θ) in (8.20) is any easier than maxi-
mizing ℓ(θ) in (8.18). For the sake of concreteness, let us assume the semisu-
pervised setting, in which some labels are known and some are unknown. Let
us write L for the indices of the labeled instances and U for the indices of the
unlabeled instances, and let u range over labelings of the unlabeled instances;
v includes all instances as well as the known labels. Then (8.20) takes the
form:
Qt(θ) =
X
u
p(u|v; θ(t)) log

Y
i∈L
p(xi, yi; θ)
Y
j∈U
p(xj, uj; θ)


=
X
i∈L
log p(xi, yi; θ) +
X
j∈U
X
u
p(u|v; θ(t)) log p(xj, uj; θ).

Generative Models
171
We again use the trick of writing p(xi, yi) as P
x,y[[xi = x, yi = y]]p(x, y) in
order to switch to a summation over the entire space of instances and labels:
Qt(θ) =
X
i∈L
X
x,y
[[x = xi, y = yi]] log p(x, y; θ)
+
X
j∈U
X
u
p(u|v; θ(t))
X
x,y
[[x = xj, y = uj]] log p(x, y; θ). (8.21)
To simplify this expression, we deﬁne the count cL(x, y) of the instance (x, y)
in the labeled data, and the expected count cU(x, y) in the unlabeled data,
averaging over possible labelings:
cL(x, y) =
X
i∈L
[[xi = x, yi = y]]
(8.22)
cU(x, y) =
X
j∈U
[[xj = x]]
X
u
[[uj = y]]p(u|v; θ(t)).
(8.23)
Let us consider the summation in (8.23) over possible labelings u for the
unlabeled data.
A labeling u is a vector each of whose elements uj is a
particular choice of label for one unlabeled instance. We are limiting attention
to those labelings for which uj = y. Since probabilities of label assignments
are independent for diﬀerent instances, we can split every labeling u into its
assignment at the j-th position, uj, and its assignments at all other positions;
let us write u′ for the rest of u excluding uj. Then the sum over u in (8.23)
becomes
p(y|xj; θ(t))
X
u′
p(u′|v; θ(t)).
The sum over u′ is unconstrained – we are summing the probabilities of all
possibilities – so its value is one. In short:
cU(x, y) =
X
j∈U
[[xj = x]]p(y|x; θ(t)).
(8.24)
Note that the right-hand side is the count of x in the unlabeled data, times
the probability of y given x according to the old model.
With deﬁnitions (8.22) and (8.24) in hand, we can simplify (8.21) to
Qt(θ) =
X
x,y
cL(x, y) log p(x, y; θ) +
X
x,y
cU(x, y) log p(x, y; θ).
Deﬁning c(x, y) = cL(x, y) + cU(x, y), this becomes simply
Qt(θ) =
X
x,y
c(x, y) log p(x, y; θ).
(8.25)
Now we observe that, apart from the diﬀerence in how c(x, y) is deﬁned,
(8.25) is identical to (8.9), and can be maximized in exactly the same way.

172
Semisupervised Learning for Computational Linguistics
For example, if p(x, y; θ) can be put into exponential form (8.10), then Qt(θ)
is maximized by relative frequency estimation, as discussed in section 8.2.2.
Now we turn to the question of how we know that the update (8.19) in-
creases likelihood. In the previous section, we showed that maximizing like-
lihood is the same as minimizing divergence from the empirical distribution
(8.17), and minimizing divergence is the same as minimizing the cross entropy
H(˜p||p). Now consider the EM auxiliary function (8.20). It almost has the
form of a cross entropy in which the old-model probability p(u|v; θ(t)) plays
the role of the empirical distribution. The connection that we saw between
the old model and the pseudo counts of (8.25) lends strength to the idea of
looking at cross entropy (or divergence) from the old distribution.
Speciﬁcally, we look at the divergence of the new distribution p(u|v; θ) from
the old distribution p(u|v; θ(t)). We know that the divergence is non-negative,
and it is strictly positive unless the two distributions are identical:
0 ≤
X
u
p(u|v; θ(t)) log p(u|v; θ(t))
p(u|v; θ)
=
X
u
p(u|v; θ(t)) log
p(u, v; θ(t))
p(v; θ(t))
·
p(v; θ)
p(u, v; θ)

= Qt(θ(t)) −ℓ(θ(t)) + ℓ(θ) −Qt(θ).
That is
ℓ(θ) −ℓ(θ(t)) ≥Qt(θ) −Qt(θ(t)).
(8.26)
In each step of the EM algorithm, we compute the value
θ∗= arg max
θ
Qt(θ)
and we set θ(t+1) ←θ∗. Substituting θ∗for θ in (8.26), we have
∆ℓ≥Qt(θ∗) −Qt(θ(t)).
(8.27)
We know that ∆ℓcannot be negative, because then we would have Qt(θ∗) <
Qt(θ(t)), but in fact θ∗maximizes Qt(θ). Further, if the EM update chooses
a model θ(t+1) that diﬀers from θ(t), then we know that ∆ℓ> 0. Namely,
since θ∗maximizes Qt(θ∗), if θ∗diﬀers from θ(t), then Q(θ∗) −Q(θ(t)) must
be strictly positive, in which case ∆ℓmust also be strictly positive.
Unfortunately, the contrapositive is not true. The inequality (8.27) does not
exclude the existence of a model θ′ with ℓ(θ′) > ℓ(θ(t)), but Q(θ′) < Q(θ(t)).
That is, when the EM algorithm converges, with θ(t+1) = θ(t), we cannot
assert that we have found the model that maximizes ℓ(θ). We can only assert
that each step of the EM algorithm, until convergence, increases likelihood.
We close by noting similarities between the EM algorithm and self-training.
One way of interpreting (8.24) is as follows.
The model θ(t) represents a
“soft” classiﬁer in the sense that, instead of predicting a ﬁxed label f(x)

Generative Models
173
when presented with instance x, it chooses a label at random, with probability
p(y|x; θ(t)). Suppose that we generate a very large corpus by multiplying the
training data N times. In the large corpus, there are N copies of each labeled
instance (xi, yi) for i ∈L, and N copies of each unlabeled instance xj for
j ∈U. We label the unlabeled instances by taking the predictions of θ(t)
at face value. Those predictions are probabilistic, however, so the result is
Np(y|xj; θ(t)) copies of (xj, y), for each label y. The result is that the count
of (x, y) in the generated corpus, for any (x, y), is NcL(x, y) + NcU(x, y) =
Nc(x, y). The resulting empirical distribution is
˜p(x, y) = Nc(x, y)
Nn
= c(x, y)
n
where n is the actual number of training instances. Maximum likelihood es-
timation is sensitive only to relative frequencies, not absolute frequencies: for
example, the parameter estimates (8.14) are unchanged if we multiply the
counts in numerator and denominator by N. Or to say it more directly, mul-
tiplying the counts by N in (8.25) would have no eﬀect on the maximization:
the model that maximizes NQt(θ) is the same as the one that maximizes
Qt(θ).
In short, we can think of EM as a particular case of self-training in which
the train step involves maximum-likelihood estimation and produces a prob-
abilistic classiﬁer. Instead of limiting attention to the high-conﬁdence pre-
dictions, EM’s label step conceptually generates a large sample from the
predictions of the probabilistic classiﬁer, for all unlabeled instances. Those
predicted labels are taken at face value to train the next version of the clas-
siﬁer, and the cycle repeats.


9
Agreement Constraints
Viewed in one way, semisupervised learning of a classiﬁer should be impossible.
The usual objective in classiﬁer construction is minimization of training error,
that is, minimization of the number of instances for which the classiﬁer’s
prediction f(xi) diﬀers from the actual label yi. Unless we say something
more, adding unlabeled training data is no help: since the labels are not given,
we are free to label the examples to agree with the classiﬁer predictions, no
matter which classiﬁer we choose. That is, the unlabeled data does not help
us choose among classiﬁers; it is equally happy with them all.
What this argument points out is that something more must be assumed,
beyond the objective of reducing training error. We have already seen at least
one example of what that “something more” might be. In chapter 6 we saw
that, among separating hyperplanes that are equally good at classifying the
labeled training data, there may be diﬀerences in the size of margin achievable
when one also takes unlabeled data into account.
This chapter looks at a diﬀerent “something more” that might be said. If
one assumes two independent views of the data, one can look for pairs of
classiﬁers, one for each view, that agree in their predictions on the unlabeled
data. Requiring agreement imposes a non-trivial constraint on classiﬁer selec-
tion. If the two views of the data are independent, then it should be diﬃcult
to ﬁnd pairs of classiﬁers that predict the labeled data well and agree in their
predictions on unlabeled data. If the two views are uncorrelated except for
the structure imposed by the true classiﬁcation, then signiﬁcant agreement
cannot be by chance, but reveals the structure of the true classiﬁcation.
9.1
Co-training
Co-training was introduced in section 2.3, but the presentation there was brief.
The algorithm was given in ﬁgure 2.8, repeated here as ﬁgure 9.1.
Like self-training, co-training is fundamentally algorithmic. Though pred-
icated on an intuition – in this case, the assumption that each view is inde-
pendently suﬃcient to learn the target function, and that the two views are
not merely redundant – it is not explicitly designed to optimize an objective
function. Nevertheless, the motivating intuition can be sharpened and shown
175

176
Semisupervised Learning for Computational Linguistics
procedure cotrain (L, U)
1
L is labeled data, U is unlabeled data
2
P ←random selection from U
3
loop until stopping criterion is met
4
f1 ←train(view1(L))
5
f2 ←train(view2(L))
6
L ←L + select(label(P, f1)) + select(label(P, f2))
7
Remove the labeled instances from P and replenish P from U
8
end loop
FIGURE 9.1
Co-training.
to be valid.
The algorithm can be understood as optimizing accuracy on the labeled
data and agreement on the unlabeled data. Co-training constructs two classi-
ﬁers, f1 and f2, each a function of only one view. The algorithm is iterative.
In each round of training, the labeled data L consists of the labeled seed data
as well as the high-conﬁdence predictions of each of the two classiﬁers. Train-
ing f1 on L constructs a classiﬁer that is consistent with the seed, with the
predictions of (previous versions of) f2, and incidentally with the predictions
of previous versions of f1. Self-training is used to gradually increase the ﬁr-
ing rate while keeping conditional accuracy high, which in this case implies
consistency with the other-view classiﬁer.
9.1.1
The conditional independence assumption
Let us consider how to sharpen the two co-training assumptions, namely,
(1) that each view is independently suﬃcient to learn the target function,
and (2) that they are not merely redundant.
As for the assumption (1),
something weaker will actually suﬃce. We assume that pairs (f1, f2) exist
in the hypothesis space that are arbitrarily close to (have arbitrarily small
error with respect to) the target function. As for assumption (2), Blum &
Mitchell’s original paper on co-training [21] formalizes “not redundant” in a
way that is actually much stronger. We consider each instance to consist of
two half-instances x = (a, b), one half-instance for each view of the data.
Blum & Mitchell’s version of assumption (2) is patterned after the Naive
Bayes assumption. It states that labeled instances are generated by choosing
a label y at random according to a ﬁxed distribution p(y), then choosing each
half-instance a and b independently, from ﬁxed distributions p(a|y) for view
one and p(b|y) for view two. It follows that the half-instances a and b are

Agreement Constraints
177
Y= +
.......
.......
..........
.......
.......
Y= –
.......
.......
.......
.......
..........
b1
b2
bn
.......
a1
a2
am
.......
b1
b2
bn
.......
a1
a2
am
.......
FIGURE 9.2
Conditional independence of half-instances. Area represents probability. Ar-
eas are distributed identically in each column, yielding unbroken horizontal
lines.
conditionally independent given y.
A graphical depiction may be useful. Let Y be a random variable repre-
senting the label; it takes on values “+” and “−.” Let A be a random variable
representing view one, taking on values a1, . . . , am, and let B be a random
variable representing view two, taking on values b1, . . . , bn. The sample space
is the set of all labeled instances (a, b, y) distributed according to a ﬁxed (but
unknown) distribution D. The conditional independence assumption is
Pr[A = a|B = b, Y = y] = Pr[A = a|Y = y].
(9.1)
Let us ﬁx Y = y and consider the probability p(a|b, y) as we vary the
value of b. By (9.1), p(a|b, y) = p(a|y); that is, for ﬁxed y, the probability
assigned to a is the same regardless of the choice of b. The situation can be
depicted graphically as in ﬁgure 9.2. The cells contain instances, classiﬁed
by their values for Y , A, and B.
The area of a given cell corresponds to
the total probability of instances that fall in that cell. Within the columns,
each representing a ﬁxed choice of b, probability is distributed the same way
among the ai, with the result that both the vertical lines and horizontal lines
are continuous. The probability distribution among the ai diﬀers in the left
square, where Y = +, and the right square, where Y = −, but it is the same
within a square.
By contrast, ﬁgure 9.3 illustrates lack of conditional independence. The
probability distribution over values A diﬀers depending on the value of B,
with the consequence that horizontal lines are discontinuous.
Now let us consider any classiﬁer f1 that depends only on view one. The
function f1 can be entirely characterized by its decision boundary, that is, the
set of instances (out of the entire population of instances) for which f1(x) = +,

178
Semisupervised Learning for Computational Linguistics
Y= +
Y= –
b1
b2
bn
.......
a1
a2
am
.......
b1
b2
bn
.......
a1
a2
am
.......
FIGURE 9.3
Conditional dependence of half-instances. Areas are distributed diﬀerently in
diﬀerent columns, yielding broken horizontal lines.
and the complement set where f1(x) = −. Since f1(x) is sensitive only to
the view-one half-instance, the decision boundary is determined by the set
of half-instances {ai} that make the value of f1 positive. Without loss of
generality, let us assume that the ﬁrst k half-instances a1, . . . , ak make f1
positive. In ﬁgure 9.2, the region where f1 is positive consists of the ﬁrst
k rows of either square.
By similar reasoning, letting f2 be an arbitrary
classiﬁcation function that depends only on view two, the region where f2 is
positive consists of the ﬁrst ℓcolumns of either square. This is shown in ﬁgure
9.4. What the ﬁgure should make clear is that conditional independence of
the half-instances implies conditional independence of the two classiﬁers: the
heavy lines (representing the decision boundaries of the two classiﬁers) are
unbroken (implying conditional independence). The argument applies to any
two classiﬁers, provided that one is sensitive only to view one, and the other
is sensitive only to view two.
9.1.2
The power of conditional independence
The conditional independence of arbitrary view-one and view-two classiﬁers
is extremely powerful. It makes labeled data almost unnecessary. In fact, it
allows us to cluster the data perfectly. We only need a single labeled data
point to tell us which cluster is the positive class and which is the negative
class.
To explain why, we need to introduce a couple of deﬁnitions. Let f1 and f2
be ﬁxed but arbitrary classiﬁers, with f1 sensitive only to view one, and f2
sensitive only to view two. Let us deﬁne the disagreement regions of f1 and
f2 to consist of the instances x such that f1(x) ̸= f2(x). The disagreement
regions of f1 and f2 are shaded in ﬁgure 9.5a. The disagreement probability

Agreement Constraints
179
Y = +
Y = –
b1
bl
bn
...
a1
ak
am
bl+1 ...
ak+1
...
...
f2 = +
f2 = –
f1 = +
f1 = –
b1
bl
bn
...
bl+1 ...
f2 = +
f2 = –
a1
ak
am
ak+1
...
...
f1 = +
f1 = –
FIGURE 9.4
Conditional independence of half-instances implies conditional independence
of single-view classiﬁers f1 and f2.
is the area of the shaded regions.
Let us also deﬁne the (conditional) minority value of f1 given that Y = y
to be the value u (either positive or negative) with the smaller probability:
Pr[f1 = u|Y = y] ≤Pr[f1 = −u|Y = y]
The minority value depends on y. The minority-value regions are the regions
where f1 takes on its minority value; they are shaded in ﬁgure 9.5b.
As should be clear from the ﬁgure, the probability of disagreement is an
upper bound on the probability of the minority values. The two probabilities
diﬀer in that the disagreement probability includes the areas marked “M,”
whereas the minority-values probability substitutes the areas marked “m.”
By deﬁnition of minority values, the areas marked “m” must be smaller, or
at least no larger, than the areas marked “M.”
Finally, let us deﬁne the error regions of f1 to be the regions in which
the value of f1 disagrees with the correct label. In ﬁgure 9.5b, it happens
that the error regions are identical to the minority-values regions. In this
case, the disagreement rate between f1 and f2 is an upper bound for the
error rate of f1. That is signiﬁcant, because the disagreement rate can be
measured on unlabeled data, but the error rate cannot. If a pair of classiﬁers
(f ∗
1 , f ∗
2 ) exists that perfectly classify the data, then f ∗
1 and f ∗
2 necessarily have
a disagreement rate of zero. Conversely, if we ﬁnd a pair of classiﬁers with
a disagreement rate of zero, their error rate must also be zero, so we have in
fact found the target functions f ∗
1 and f ∗
2 . Even if the target function is not
in the family of classiﬁers that we consider, by ﬁnding a pair (f1, f2) with low
disagreement rate ϵ, we can guarantee that either of them has an error rate
not greater than ϵ.

180
Semisupervised Learning for Computational Linguistics
f1
f1
Y = –
Y = –
Y = +
Y = +
f2
f2
(a)
M
M
(b)
m
m
f2
f2
FIGURE 9.5
(a) Shaded and black areas are regions of disagreement; (b) shaded and black
areas are regions of minority values. The diﬀerence is the replacement of the
areas marked “M” with the areas marked “m.”
The ﬂy in the ointment is that ﬁgure 9.5b illustrates only one possibility for
the relationship between minority-value regions and error regions. There are
actually four possibilities, as illustrated in ﬁgure 9.6. Case (a) is the same as
ﬁgure 9.5b; the minority-value regions are the same as the error regions, and
disagreement upper bounds error. In case (b), the minority-value regions are
the opposite of the error regions. In this case, f1 makes the wrong predictions
most of the time, which means that reversing its predictions yields a good
classiﬁer. In particular, disagreement upper bounds the error of −f1. If the
disagreement rate is zero, then either f1 or −f1 is the target function, and
a single labeled instance will tell us which it is. If the disagreeement rate is
nonzero but small, then either f1 or −f1 has a low error rate, and a small
labeled sample will suﬃce to determine which, with high probability.
In the (c) and (d) cases, the minority values are the same regardless of
the true label. In these cases, disagreement does not tell us anything about
error, so we need to be able to detect and avoid these cases. Fortunately,
it is possible to do so. Let p be the smaller of the two class probabilities:
p = miny Pr[Y = y], and let ˆp be the smaller of the class probabilities as
predicted by f1, that is, ˆp = miny Pr[f1 = y]. Note that ˆp is not sensitive to
the true label Y – unlike the minority probabilities, ˆp can be determined using
unlabeled data only. In the (c) and (d) cases, ˆp is equal to the minority-values
probability. In the (a) and (b) cases, ˆp is greater than the minority-values
probability.
Now, we would like to ﬁnd pairs (f1, f2) whose disagreement rate ϵ is small.
If the target function is in the hypothesis space, we can push ϵ to zero, and
even if not, if arbitrarily good approximations are available, we can push ϵ
arbitrarily close to zero.
We would certainly like to be able to ﬁnd pairs

Agreement Constraints
181
(a)
(b)
(c)
(d)
FIGURE 9.6
(a) Minority values equal error values, (b) Minority values of −f1 equal error
values, (c–d) Minority values are the same regardless of Y .
whose disagreement rate is signiﬁcantly less than p. If ˆp is close to p, then
the disagreement rate will also be signiﬁcantly less than ˆp. Let us deﬁne a
“good pair” to be a pair satisfying ϵ < ˆp. Unlabeled data suﬃces to determine
whether a pair is good or not.
We have already seen that the disagreement rate upper bounds the minority-
values probability, and that ˆp is equal to the minority-values probability in the
(c) and (d) cases. So, for the (c) and (d) cases, we have ϵ > ˆp, hence no good
pairs fall in the (c) and (d) cases. It follows that, for good pairs, disagreement
upper bounds true error rate. If we can ﬁnd good pairs with arbitrarily low
disagreement rates, then we can ﬁnd arbitrarily good approximations to the
target function, using unlabeled data only. The only proviso is that we cannot
know which value of f1 represents true positives without at least one labeled
instance.
The discussion here is based on a paper by Dasgupta, Littman, and Mc-
Allester [67]. We have focussed exclusively on generalization error, that is,
expected error on instances drawn according to the population distribution.
They give a formal theorem and proof that covers not only generalization
error, but also deals with the sampling error inherent in a ﬁnite training
sample. They include the possibility that a classiﬁer abstains from making
a prediction (f = ⊥), and they show in particular that, under the Blum &
Mitchell conditional independence assumption,
Pr[f1 ̸= Y |f1 ̸= ⊥] ≤
ϵ
ˆp −ϵ
where, as before, ϵ is the rate of disagreement between f1 and f2 (where both
make predictions) and ˆp is the smaller of the two label probabilities according
to f1.

182
Semisupervised Learning for Computational Linguistics
To summarize the discussion, we have concluded that co-training is es-
sentially a version of the “cluster and label” idea introduced in section 7.1.
Seeking paired classiﬁers that agree on unlabeled data eﬀectively clusters the
data, and if Blum & Mitchell’s conditional independence assumption is true,
the resulting clusters, as sets of instances, exactly match the true classes. The
labeled data is used only to determine which label belongs to which cluster.
9.2
Agreement-based self-teaching
As we have just seen, the distinctive principle of co-training is agreement
between two views that are conditionally independent given the value of target
function. The principle is older than Blum & Mitchell’s paper. It is already
enunciated clearly by de Sa in her dissertation [72] and earlier work [71].
Her approach is based on a supervised learning method called Learning
Vector Quantization (LVQ), proposed by Kohonen [129]. The classiﬁer’s pre-
diction is determined by a nearest-neighbor rule. Each class y is represented
by a point my in instance space, and the predicted label for a new instance
x is the class whose center is nearest:
f(x) = arg min
y
||x −my||.
Recall that x−my is the vector whose tail is at the point my and whose head
is at the point x, and ||x −my|| is its length.
The algorithm is iterative. It begins with an arbitrary placement of the class
centers mj, and then cycles through the instances until a stopping criterion is
met, updating the locations of the centers at each step. Let x be the current
instance, let my be the center for the correct class, and let mz be the nearest
incorrect class center. Intuitively, z is the “distractor” class. The aim is to
move the centers my and mz so that x lies closer to the correct center my
and further from the distractor mz.
The situation is depicted in ﬁgure 9.7. The current instance is the solid
point x, and its true label is y. The center for class y is the circled cross my,
and the center for the distractor class z is mz. Because of the nearest-neighbor
criterion, the decision boundary between classes y and z is the perpendicular
bisector of the line connecting the centers my and mz. The connecting line
and the decision boundary are shown as solid lines. (Note that linear dis-
criminant analysis yields the same decision boundary, as discussed in section
8.1.2.) In ﬁgure 9.7, the current instance x happens to fall on the wrong side
of the decision boundary and our aim is to move the boundary to correct that.
If it had fallen on the correct side of the boundary, the aim would have been
to move it deeper into “friendly territory.”

Agreement Constraints
183
x:y
m y
m z
FIGURE 9.7
The LVQ update.
The (thin) vector with its tail at my and its head at x is the vector x−my,
and the comparable vector with tail at mz is x −mz. We move the center
for the correct label a step of size ϵ toward x, and we move mz an ϵ-sized
step away from x. The steps are shown as thick arrows. The new centers are
dotted circles, and the new connecting line and decision boundary are shown
as dashed lines. In this particular case, the adjustment has moved x over to
the correct side of the decision boundary.
Algebraically, the update is
my ←my + ϵ x−my
||x−my||
mz ←mz −ϵ x−mz
||x−mz||.
(9.2)
We normalize the vectors to get unit vectors in the desired direction, then we
scale the unit vectors to length ϵ.
The situation depicted in ﬁgure 9.7 has a couple of properties that we would
like to state as requirements. First, we assumed that my and mz are the two
nearest centers to x. By deﬁnition, mz must be one of the two nearest centers,
but nothing guarantees that my is. We add that as a requirement. If my is
not one of the two nearest centers, then no update is done.
Second, the
ﬁgure depicts x lying near the boundary between y and z. We add that as a
requirement as well: an update is done only if x lies within a margin c of the
boundary, where c, like ϵ, is a free parameter of the algorithm. Recall that we
can compute the distance between x and the boundary by projecting x onto
the line connecting the centers: see equation 5.3, page 70.
The center for a given class y is initially placed at the centroid of the in-
stances belonging to class y. Then multiple passes are taken through the data,
using the LVQ update just described to improve the positioning of decision

184
Semisupervised Learning for Computational Linguistics
boundaries. The step size ϵ and margin c are gradually decreased with each
pass.
To apply this algorithm to a mixture of labeled and unlabeled data, de Sa
proposes using two modalities (views), training separate classiﬁers for each
modality, and using agreement between the classiﬁers in lieu of labeled data.
As in co-training, each instance is taken to consist of two half-instances, x(1)
and x(2). The ﬁrst half-instances x(1) represent points in the instance space
for the ﬁrst view, and the second half-instances x(2) represent points in the
instance space of the second view. We construct separate classiﬁers for each
space. In this case, the classiﬁers are nearest-neighbor classiﬁers, represented
by the locations of the class centers: (m(1)
1 , . . . , m(1)
L ) for the ﬁrst view and
(m(2)
1 , . . . , m(2)
L ) for the second view.
The prediction of the k-th classiﬁer
(k ∈{1, 2}) is
fk(x) = arg min
y
||x(k) −m(k)
y ||.
The update rule (9.2) is essentially unchanged, except that it applies to each
view separately:
m(k)
y
←m(k)
y
+ ϵ
x(k)−m(k)
y
||x(k)−m(k)
y
||
m(k)
z
←m(k)
y
−ϵ
x(k)−m(k)
y
||x(k)−m(k)
y
||.
The critical diﬀerence is that, whereas y was previously provided by having
labeled data, it is now deﬁned to be the label predicted by the other classiﬁer,
computed before the update. (The two classiﬁers are updated in parallel.)
The conditions remain in force that m(k)
y
must be one of the two closest
centers to x(k), and x(k) must be within margin c of the decision boundary
between m(k)
y
and m(k)
z , otherwise there is no update.
With a mix of labeled and unlabeled data, the labeled data is used to
initialize the class centers, and is included in the passes through the data.
The updates with labeled data, of course, use the given labels, as in the
supervised algorithm.
9.3
Random ﬁelds
9.3.1
Applied to self-training and co-training
Another way of capturing (soft) agreement constraints is via a random ﬁeld.
A random ﬁeld is a probability function deﬁned on a labeled graph.
The
vertices of the graph are ordered arbitrarily x1, . . . , xn, and a corresponding
assignment of labels y = (y1, . . . , yn) is called a conﬁguration. An energy
function H(y) is deﬁned in terms of the labels. For our purposes, “energy”
can be thought of as “error” or “cost” – high energy is bad, low energy is

Agreement Constraints
185
good. We are particularly interested in ﬁelds in which the labels are drawn
from {+1, −1}, and the energy is deﬁned as
H(y) = −
X
(i,j)∈E
yiyj
(9.3)
where E is the set of edges of the graph. In words, if two vertices connected
by an edge have diﬀerent labels, then a cost (+1) is contributed to H, and
if two vertices connected by an edge have the same label, then a credit (−1)
is contributed. That is, agreement between neighboring vertices is good and
disagreement is bad.
Fields of the form (9.3) are related to Ising models. The physicist Ernst
Ising proposed a model of ferromagnetism in which the graph is a rectangular
grid, and the energy is
H(y) = −1
kT

J
X
(i,j)∈E
yiyj −mB
X
i
yi


where T is temperature, k is a constant determining the inﬂuence of tempera-
ture, B is the intensity of a constant external magnetic ﬁeld, and J and m are
constants determined by physical properties of the magnetic material. Our
energy (9.3) is the special case where there is no external ﬁeld and J = kT;
we also abstract away from Ising’s rectangular grid.
Recall that we can view self-training and co-training as involving bipartite
graphs with labeled vertices, where vertices representing features alternate
with vertices representing instances. The self-training graph was illustrated
in ﬁgure 7.8 (page 148), and the co-training graph was illustrated in ﬁgure
7.11 (page 151). The co-training graph is bipartite in the sense of involving
alternating instance and feature nodes, but tripartite in the sense that features
can be divided into view-one features and view-two features.
In the training step, self-training selects rules F ⇒y such that instances
possessing feature F are almost always labeled y. Selecting rule F ⇒y is
equivalent to labeling the feature vertex F with label y, and the instances
that possess feature F are represented by the instance vertices connected to
F by an edge. Thus, in terms of the graph, the training step assigns labels
to features so as to maximize agreement with neighboring labeled instances.
The labeling step, in turn, assigns labels to instances x according to the
prediction of rules F ⇒y for features F possessed by x. That is, the labeling
step assigns labels to instance vertices so as to maximize agreement with
neighboring labeled features.
Co-training does the same, though it adds the complication that it parti-
tions the features into two views and handles each view separately, a compli-
cation that we ignore here. What is important for present purposes is that
both self-training and co-training seek to maximize agreement between the

186
Semisupervised Learning for Computational Linguistics
labels of neighboring vertices in a graph. That is, both aim to minimize the
energy (9.3).
A random ﬁeld uses the energy function H(y) to deﬁne a probability dis-
tribution over conﬁgurations, namely
p(y) = 1
Z e−H(y)
where Z is a normalizing constant. The distribution assigns high probability
to conﬁgurations with low energy (low cost) and low probability to conﬁgura-
tions with high energy (high cost). Notice that the negative sign here cancels
the negative sign in (9.3): if neighboring vertices agree in label, probability is
increased, and if they disagree, probability is decreased.
A ﬁeld of the sort under discussion is readily adapted to semisupervised
learning. Labeled data is represented as instance vertices whose labels are
ﬁxed. The goal is to assign labels to the remaining vertices (features and
instances) so as to maximize p(y). The situation in which one is given a seed
classiﬁer instead of labeled data is also easily represented: the seed classiﬁer
consists in an initial assignment of labels to some features. One again seeks
to assign labels to the remaining vertices so as to maximize p(y).
9.3.2
Gibbs sampling
A simple and well-known algorithm for ﬁnding a conﬁguration that maxi-
mizes p(y) is Gibbs sampling. Consider a particular vertex x with neighbors
x1, . . . , xT . The vertex x may represent either an instance or a feature. We
deﬁne a local probability distribution over the labels of x as
p(y) =
e−H(y)
e−H(y) + e−H(−y)
(9.4)
where
H(y) = −
T
X
i=1
y · yi
where yi represents the label of neighboring vertex xi. Let p represent the
number of positives among the neighboring labels y1, . . . , yT , and let n repre-
sent the number of negatives. Then
−H(+1) = p −n
−H(−1) = n −p.
The Gibbs sampling algorithm begins with a labeled graph. The initial
labeling is immaterial.
The algorithm proceeds iteratively.
At each step,
a vertex is chosen uniformly at random. Cycling systematically through the
vertices works just as well, but the mathematical justiﬁcation is more involved,
so we assume random choice. Let the chosen vertex be x, and let its neighbors
be x1, . . . , xT . A new label y is selected for x, by sampling from p(y) as deﬁned

Agreement Constraints
187
in (9.4). It is permissible for the new label to be the same as the old. At each
iteration, a single vertex is relabeled in this manner. A sweep is completed
when every vertex has been relabeled once. Multiple sweeps yield a sequence
of conﬁgurations y1, y2, . . . , yt, . . ..
As the number of sweeps increases, the relative frequency of a given label at
a given vertex converges to its true probability. That is, if we run the Gibbs
sampler for enough iterations, and we accumulate the relative frequencies of
labels at each vertex, we can determine the probability that a given vertex
has a given label, according to the random ﬁeld.
We can view Gibbs sampling as a stochastic version of self-training. In the
case of a bipartite feature-instance graph, a single step of the Gibbs sampler
corresponds either to a labeling step, if the selected vertex is an instance
vertex, or a training step, if it is a feature vertex. A labeling step consists
in relabeling an instance according to a stochastic vote by the features it
possesses. A training step corresponds to the replacement of a rule F ⇒y
with a rule F ⇒y′, according to a stochastic vote by the instances that possess
feature F. After a suﬃcient number of iterations, the relative probability of
labels for a given instance can be determined by relative frequency in the
sequence of conﬁgurations constructed, and likewise a ﬁnal classiﬁer can be
constructed of rules F ⇒y with probability determined by their relative
frequency (among rules conditioned on F) in the sequence of conﬁgurations.
9.3.3
Markov chains and random walks
We will not attempt to give a full justiﬁcation for the Gibbs sampler, but we
will at least give the sketch of an account. Doing so will give us occasion to
introduce the concepts of Markov chains and random walks, concepts which
will arise again when we discuss label propagation in the next chapter.
The Gibbs sampler cycles through the nodes of the graph. When choosing
nodes at random, it may take longer to ﬁnish a sweep, which is to say, to visit
each node once, but eventually all nodes will be selected at least once. When
the sampler visits node xi, it chooses a new label by sampling from p(y|xi).
The possibility that the new label will be the same as the old one is not ruled
out. Deﬁne the possible labels for xi to be those with p(y|xi) > 0, and deﬁne
a possible state of the system to be any way of assigning a possible label to
each node. Clearly, after we have made one pass through the nodes, visiting
each node in turn, the state we obtain might be any possible state (though
not all possible states will be equally probable).
The action of the Gibbs sampler deﬁnes a Markov chain: that is, a matrix
P in which the entry pji represents the probability of a transition from state
i to state j. Note that i is now ranging over states of the complete graph,
not over individual nodes. The i-th column of P represents the “next-state”
probability distribution for state i. Be warned that many authors use rows to
represent next-state distributions, instead of columns, but I prefer following
the usual convention of interpreting vectors (here, probability distributions

188
Semisupervised Learning for Computational Linguistics
over states) as column vectors. If there are n nodes in the graph, there is a
nonzero probability of completing a sweep through a graph in n steps. Since
any state j is reachable from any other state i after one sweep through the
graph, there is a nonzero probability of reaching any j from any i after n
steps. The probability of reaching j from i after n steps is Pn
ji, so we have just
concluded that the matrix Pn has all entries strictly positive. A probability
matrix with this property, namely, that some power of the matrix has all
entries strictly positive, is said to be ergodic.
Suppose we have a probability distribution x over states. We can run a
number of Gibbs samplers simultaneously, and think of each one as a particle
moving through the state space. Then xi is the proportion of particles that
are in the state i at a particular time. Alternatively, we can think of a single
particle moving through the state space. Then xi represents the probability
of the particle being in state i, if we choose a time at random.
The distribution x is a function of time, so we should actually write xt
for the distribution at time t. We will write x(t)
i
for the i-th element of xt.
The probability of being in state j at time t + 1 is the weighted average of
probabilities of being in state i at time t and making the transition from i to
j:
x(t+1)
j
=
X
i
pjix(t)
i .
That is, the j-th element in xt+1 is the dot product of the j-th row of P with
xt. (Rows correspond to destination states.) Hence
xt+1 = Pxt.
Since the transition matrix P is unchanging, it follows that
xt+k = Pkxt
and in particular,
xk = Pkx0.
(9.5)
Earlier, in the discussion of Gaussian mixtures (section 8.1.2), we introduced
the idea of diagonalization. Intuitively, when one multiplies by a matrix, the
eﬀect can be decomposed into a rotation and a scaling. Further, the directions
in which the scaling is applied are represented by the eigenvectors of the
matrix, and the amount of scaling in each eigenvector direction is represented
by the corresponding eigenvalue.
In that earlier discussion, the matrix in question was symmetric. Now we are
interested in the probability matrix P, which is not usually symmetric. Even
so, it can be diagonalized. In the earlier discussion, we introduced eigenvectors
as the directions in which the scaling is applied. The more precise deﬁnition
is that an eigenvector represents a direction in which the eﬀect of the matrix
is a scaling but no rotation. That is, u is an eigenvector of P just in case
Pu = λu.

Agreement Constraints
189
The scalar λ is the eigenvalue corresponding to the eigenvector u.
If P is an n × n matrix, it may have as many as n distinct eigenvectors.
Consider what happens when we collect those eigenvectors together as the
columns of a matrix:
P [u1 . . . un] = [λ1u1 . . . λnun] .
(9.6)
The right-hand side can be expressed as a matrix product of form:
[u1 . . . un]


λ1 . . . 0
... ... ...
0 . . . λn

.
Writing U for the matrix whose columns are eigenvectors and Λ for the diag-
onal matrix containing eigenvalues, the equation (9.6) becomes
PU = UΛ
P = UΛU−1.
(9.7)
This is the more general form of diagonalization, which is valid provided that
the inverse U−1 exists.
In our earlier dicussion, we considered diagonalization of symmetric ma-
trices. When a symmetric matrix is diagonalized, it can be shown that the
eigenvectors in U are mutually perpendicular unit vectors, that is, that U
describes a rigid rotation, and hence that U−1 exists, and in fact equals UT.
In the case of current interest, P is usually not symmetric. It is, however,
ergodic, and ergodicity can also be shown to imply the existence of U−1.
Ergodicity also implies two important properties of the eigenvalue matrix Λ:
namely, that the largest eigenvalue in Λ equals 1, and that all the remaining
eigenvalues are nonnegative but strictly less than one. Without loss of gen-
erality, we can assume that the eigenvalues are sorted in increasing order, so
that λn = 1 and 0 ≤λi < 1 for i < n.
These properties imply immediately that P has a unique stationary distri-
bution. A stationary distribution is a probability distribution x such that
Px = x.
(9.8)
In other words, a stationary distribution is an eigenvector whose eigenvalue is
1. For an ergodic matrix, such an eigenvector exists and is unique. Namely,
the stationary distribution of P is the eigenvector un corresponding to the
eigenvalue λn = 1.
The stationary distribution is the unique ﬁxed point for the operation of
premultiplying by P. Once we arrive at the stationary distribution, applying
P will never take us elsewhere. It is natural to ask, then, whether applying P

190
Semisupervised Learning for Computational Linguistics
repeatedly to an arbitrary distribution will eventually bring us to the station-
ary distribution. That is, does the sequence x0, x1, . . . deﬁned by (9.5) con-
verge to the stationary distribution of P, or does it fail to converge? Since the
stationary distribution is the only ﬁxed point, the sequence cannot converge
to something else, and since the vectors xk are all probability distributions,
hence of bounded size, the sequence cannot diverge, but it could in principle
settle into a limit cycle.
To analyze (9.5) further, we need to consider the powers of P. The diago-
nalization (9.7) makes it easy to compute them. Squaring P yields
P2 = UΛU−1UΛU−1 = UΛ2U−1.
In general:
Pk = UΛkU−1.
(9.9)
This simpliﬁes matters greatly, because one can take the power of a diagonal
matrix simply by taking the powers of its elements:
Λk =


λk
1 . . . 0
... ... ...
0 . . . λk
n

.
The invertibility of U implies that its columns are linearly independent.
That is, its columns form a basis for n-dimensional space.
Hence we can
express any n-dimensional vector as a linear combination of the eigenvectors
ui. This is true in particular for x0:
x0 =
X
i
aiui = Ua
(9.10)
for some set of weights a.
Combining equation (9.10) with equations (9.5) and (9.9) yields
xk = UΛkU−1Ua
=
X
i
uiλk
i ai.
Not only x0, but every xk can be expressed as a linear combination of the
eigenvectors ui. If P is ergodic, then λn = 1 and all the other λi are less
than one, hence λk
n = 1 and λk
i →0 for i < n. It follows that x∞= anun.
But since λn = 1, we must have Pun = un, implying that x∞= un, which
is to say, that an = 1. This conclusion follows from the fact that λn = 1.
We cannot conclude that the other eigenvectors of P represent probability
distributions, nor can we conclude that the other coeﬃcients ai equal one,
or are even positive. What we can conclude is that the sequence x0, x1, . . .
converges to un,
x∞= un.

Agreement Constraints
191
We have shown that, whatever the initial distribution, with time the chain
converges to its stationary distribution. If we think of a population of par-
ticles each moving from state to state, the stationary distribution represents
the proportion of particles in each state. Even though the individual particles
continue to move, their distribution settles down to the stationary distribu-
tion, and the distribution no longer changes once settled. Alternatively, if we
think of a single particle moving through the space, then no matter which
state the particle starts in, if one chooses a random time that is far enough
into the future to allow convergence, the probability distribution over possi-
ble locations for the particle is given by the stationary distribution. (If the
particle starts out in state i, one can think of the distribution x0 as having a
1 in the i-th position and 0’s elsewhere.)
To connect the discussion of stationary distributions back to the Gibbs
sampler, an additional concept will be useful. A distribution x and a Markov
chain P are in detailed balance if, for all states i and j:
pjixi = pijxj.
In words, the probability of being in state i and then transitioning to state j
is equal to the probability of being in state j and then transitioning to state
i. If x and P are in detailed balance, then we have
X
i
pjixi =
 X
i
pij
!
xj = xj.
That is, taking the dot product of the j-th row of P with x yields xj. Hence:
Px = x.
In short, detailed balance implies that x is the stationary distribution for the
chain.
We have already noted that the Gibbs sampler deﬁnes an ergodic Markov
chain P. We will use y = (y1, . . . , yn) to represent an arbitrary assignment
of labels to the nodes of the graph; y also represents a state visited by the
sampler in the course of its random walk.
Let us write G(yj|yi) for pji,
the probability that the sampler goes from state yi to state yj in one step.
Note that yi and yj are two diﬀerent vectors assigning labels to nodes in
the graph. We would like to show that the transition probability G(yi|yj)
stands in detailed balance with the true probability over labelings, p(y). If
so, then p(y) is the stationary distribution of the transition matrix, meaning
that the probability of the Gibbs sampler visiting state y converges to the
true probability p(y). What we would like to show is the following:
G(yj|yi)p(yi) = G(yi|yj)p(yj)
(9.11)
for arbitrary label assignments yi and yj.

192
Semisupervised Learning for Computational Linguistics
To avoid subscript clutter, let us write y and y′ instead of yi and yj. The
label that y assigns to the k-th node is yk. Let us write ¯yk for the rest of y.
That is, ¯yk is just like y, except that it has a “hole” in place of the k-th label.
With this notation, the probability G(y′|y) can be written
G(y′|y) =
X
k
[[¯y′
k = ¯yk]]p(k)p(y′
k|¯y′
k).
In words, the probability of going from y to y′ is obtained by summing over
possible choices k of nodes to relabel. The probability of going from y to y′ by
changing the label of node k is zero unless y and y′ are identical everywhere
except possibly at node k. If that condition is met, then the probability of
going from y to y′ by relabeling node k is equal to the probability of choosing
node k for relabeling, times the probability that y′
k is chosen as the new value,
given the context ¯y′
k.
This allows us to expand the left side of (9.11) as follows:
G(y′|y)p(y) =
X
k
[[¯y′
k = ¯yk]]p(k)p(y′
k|¯y′
k)p(y)
=
X
k
[[¯y′
k = ¯yk]]p(k)p(y′)p(y)
p(¯y′
k)
.
Similarly, the right side of (9.11) expands as
G(y|y′)p(y′) =
X
k
[[¯yk = ¯y′
k]]p(k)p(yk|¯yk)p(y′)
=
X
k
[[¯yk = ¯y′
k]]p(k)p(y)p(y′)
p(¯yk)
.
Noting that ¯yk = ¯y′
k within the summation, we conclude that the left and
right sides of (9.11) are equal, as desired. This shows that the Gibbs sampler
is in detailed balance with the true distribution over labelings, and hence that
the Gibbs sampler converges to the true distribution.
9.4
Bibliographic notes
For random ﬁelds, see Winkler [233]. In particular, he discusses the Ising
model (p. 52). The discussion of Gibbs sampling follows the presentation in
Russell & Norvig [199], chapter 14.

10
Propagation Methods
We have seen several iterative methods for semisupervised learning: self-
training and co-training, McLachlan’s algorithm, the EM algorithm, boosting.
In section 7.6, we showed that the iterations in self-training and co-training
can be viewed as propagating label information through a graph. EM pro-
vides a soft version, and the Gibbs sampler can be seen in similar light: each
update probabilistically adjusts the label of a node to make it like the labels
of its neighbors.
Viewed abstractly enough, just about any classiﬁcation algorithm can be
seen as propagating labels based on similarity. The nearest-neighbor classiﬁer
asks directly which training instance a new instance is most similar to, and
predicts the label of that most-similar training instance. Other classiﬁers cre-
ate more abstract representations of classes and compare a given test instance
to the abstract representations, predicting the class whose representation the
new instance is most similar to. An abstract class representation, in turn, is
based on the features that training instances belonging to the class have in
common.
For example, as we have seen, many classiﬁers construct a weight vector
w and, given a new instance x, predict positive if x · w exceeds a threshold
and negative otherwise. One can see w as an abstract representation of the
positive class, and the dot product x · w as the measure of similarity between
x and w.
A similarity relation deﬁnes a weighted graph. The objects that participate
in the relation represent nodes in the graph, and there is an edge between
two objects just in case they have non-zero similarity. The weight on an edge
represents the similarity between the two objects connected by the edge.
We have mentioned two diﬀerent sorts of graph. In one, nodes represent
instances, and features play a role in determining the similarities that are
represented as edge weights. In the other sort of graph, both instances and
features are represented by nodes. An instance is “similar” to a feature if it
possesses the feature.
We are particularly interested in the semisupervised case in which some,
but not all, of the nodes are labeled. If the seed consists of labeled data, then
the labeled nodes are instances. If the seed consists of a seed classiﬁer, the
labeled nodes are features. The labels of nodes corresponding to the seed are
ﬁxed, and other labels are allowed to vary. The basic mechanism for assigning
labels to unlabeled nodes is to propagate labels from neighboring nodes, in
193

194
Semisupervised Learning for Computational Linguistics
accordance with the weight of the edge connecting the nodes.
In this chapter, we examine the idea of label propagation through a graph in
more detail. The picture that emerges is of a graph as a fabric or net, in which
instances and features are knots connected by cords to similiar instances and
features. The height of the fabric corresponds to the probability of having the
positive label: an altitude of 1 indicates that the node is certainly labeled pos-
itive, an altitude of 0 indicates that the node is certainly labeled negative, and
an altitude of 1/2 indicates that the two labels are equally likely. The labeled
instances form the perimeter, at ﬁxed heights of 1 or 0, and the unlabeled
instances form the interior. The overall eﬀect is a complex interpolation, with
each unlabeled node being pulled to the average height of its neighbors. This
yields a labeling of the entire graph, with nodes above altitude 1/2 labeled
positive in the end and nodes below altitude 1/2 labeled negative.
10.1
Label propagation
We begin by considering a simple label propagation process. It assumes a
weighted undirected graph in which weights are non-negative real numbers.
The ﬁrst ℓnodes have ﬁxed labels y1, . . . , yℓ, with yi ∈{−1, +1}, and the
remaining n −ℓnodes are unlabeled; let us deﬁne L to be the set of labeled
nodes, and U to be the set of unlabeled nodes. There are n nodes all together.
The label propagation process repeatedly updates node labels by propa-
gating labels from neighbors. To choose a new label for the i-th node, one
probabilistically chooses a node j among the neighbors of i, and one propa-
gates j’s label to i.
Only one neighbor j is allowed to propagate its label to i. The probability
that j is the propagator is deﬁned to be proportional to the weight of the edge
connecting j to i. That is, the probability of j propagating its label to i is
pij =
wij
P
k wik
(10.1)
where wij is the weight of the edge between i and j, and the summation
is over edges incident to i.
Note that weights are symmetric (wij = wji)
because the graph is undirected, but it does not follow that pij = pji, because
the denominator in (10.1) may diﬀer for nodes i and j.
Label propagation is a stochastic process. We begin by labeling the unla-
beled nodes at random, with probability 1/2 for positive and 1/2 for negative.
Then, at each time step, the labels of the originally unlabeled nodes are up-
dated in parallel. The update for the i-th node is
y(t+1)
i
= y(t)
j

Propagation Methods
195
where j is a neighbor of i chosen at random according to pij. A new choice
of neighbor is made at each update. Updates are applied only to originally
unlabeled nodes; the labels of the originally labeled nodes are indelible.
We cannot deﬁne what the new label y(t+1)
i
for the i-th node will be; it
is not deterministic. But if we know the probabilities that the neighbors of
the i-th node are labeled positive, then we can determine the probability
that i is labeled positive after one label propagation step, and likewise for
the probability of being labeled negative. We will give a deﬁnition for g(t)
i ,
representing the probability that the i-th node is labeled positive at time t.
The probability that it is labeled negative is simply 1 −g(t)
i .
The probability g(t)
i
is trivial for nodes in L, which is to say, for the originally
labeled nodes. If node i belongs to L, then for all times t, we have g(t)
i
= 1 if
node i is originally labeled positive, and g(t)
i
= 0 otherwise.
The interesting case is where node i belongs to U (i is originally unlabeled).
The probability g(t)
i
for times t > 0 is deﬁned recursively. The base case is
t = 0. Since labels for nodes in U are initially assigned uniformly at random,
it follows that g(0)
i
= 1/2. Now let us assume that g(t)
k
is deﬁned for all nodes
k at time t. Consider an arbitrary node i, and let j range over the (indices of
the) neighbors of node i. Note that the neighbors may be either in L or U. The
probability that node i receives the positive label from a particular neighbor j
is the probability pij that j is the propagator, times the probability g(t)
j
that
j is labeled positive at the beginning of the propagation step. Summing over
neighbors yields the probability that node i receives the positive label:
g(t+1)
i
=
X
j
pijg(t)
j .
(10.2)
This leads us to an algorithm. We collect the positive-label probabilities
gi into a vector g. The ﬁrst ℓcomponents are ﬁxed, but the remaining n −ℓ
components are variable, and are updated using (10.2). We write gL for the
half-vector [g1 . . . gℓ], and gU for the half-vector [gℓ+1 . . . gn]. The probabilities
pij deﬁne an n × n matrix P. Note that the rows of P sum to unity, though
its columns generally do not. If Pi is the i-th row of P, then the new value
gi for node i deﬁned in (10.2) can be written as Pi · g. Updating all nodes in
parallel, we obtain
g ←Pg.
(10.3)
This is actually not quite correct as written, because it updates the proba-
bilities for the labeled nodes as well as the unlabeled nodes. The algorithm
repairs that by restoring the probabilities for gL to their original values, after
the update.
The complete algorithm is given in ﬁgure 10.1. The algorithm runs un-
til convergence, at which point a node in U is deemed to be positive if its
probability of being positive exceeds 1/2, and negative otherwise. Of course,
saying that the algorithm runs to convergence presumes that it converges.

196
Semisupervised Learning for Computational Linguistics
procedure
propagateLabels (W, y)
1
Input W = {wij} are edge weights for the graph
2
Input y contains labels for nodes 1, . . . , ℓ
3
Compute P from W using (10.1)
4
Deﬁne g(0)
L
with elements g(0)
i
= 1 if yi = + and g(0)
i
= 0 otherwise
5
set gL ←g(0)
L , gU ←(0.5)1
6
until convergence, do
7
set g ←Pf
8
set gL ←g(0)
L
9
return a labeling ˆy for originally unlabeled nodes,
10
where ˆyi = +1 if gi > 1/2 and ˆyi = −1 otherwise.
FIGURE 10.1
A simple label propagation algorithm.
We should rather ask whether it converges. If it converges, it will converge
to a ﬁxed point of the update (10.3). That is, it will converge to a vector g
satisfying
g = Pg.
(10.4)
If such a vector exists, its i-th component gi provides a natural deﬁnition of
the intrinsic probability that the i-th node is labeled positive. Note that what
equation (10.2) provides is the probability that node i is labeled positive after
the t-th step of label propagation. That probability varies from time step to
time step. Equation (10.4) is what we expect of a label probability assignment
that depends only on the matrix P, and not on time.
It turns out that there is a natural relationship between the intrinsic prob-
ability of a node being labeled positive, and a random walk. We consider that
connection ﬁrst, before addressing the existence of g.
10.2
Random walks
The probabilities pij deﬁned in (10.1) can be viewed as transition probabilities
for a Markov chain. We think of the graph as the state graph of the automaton;
a computation of the automaton corresponds to a particle pursuing a random
walk on the graph. At each moment of time t, the automaton is in some
particular state, which is to say, the particle is located at some particular
node xi. The particle chooses an edge out of xi stochastically according to

Propagation Methods
197
the distribution pij, and moves along the chosen edge, arriving at node xj at
discrete time t + 1.
It is important to notice that the particle moves in the direction oppo-
site to the direction of label propagation. The probabilities pij represent a
probability distribution over neighbors j. In terms of label propagation, the
distribution determines which neighbor gets to propagate its label to i. In
terms of a random walk, the distribution determines which neighbor the par-
ticle goes to next. The two processes are identical mathematically, but move
in opposite directions. The particle can be thought of as moving backward in
time along the same path that the label traversed forward in time.
A particle that begins its random walk in node x will eventually arrive at
a node in L. Since the particle’s path traces the path of label propagation
in reverse, the label of x is determined by the label of the node where the
particle’s path terminates. We think of the particle as being absorbed at that
point. If it is absorbed at a positively labeled node, the label of x is positive,
and if it is absorbed at a negatively labeled node, the label of x is negative.
Following a single particle back from the start node x to a node in L tells
us only one way that the label propagation might have gone. But since the
motion of the particle is governed by the same probabilities that govern label
propagation, the probability of the particle arriving at a positively labeled
node in L is equal to the probability of x being labeled positive by the prop-
agation process.
Let us deﬁne f(xi) to be the probability that a particle starting in xi is ab-
sorbed at a positively labeled node. If xi ∈L, then the particle is immediately
absorbed, and f(xi) = 1 if xi is positively labeled and f(xi) = 0 otherwise. If
xi ∈U, then the particle will go to some neighboring node xj with the next
step of its random walk. The probability of going to a particular neighbor
xj and subsequently (eventually) being absorbed at a positive node is the
product pijf(xj). Summing over all neighbors gives the total probability of
eventually being absorbed at a positive node:
f(xi) =
X
j
pijf(xj).
(10.5)
Here j ranges over (the indices of) the neighbors of node i, though we can
equally well take j to range over all node indices, inasmuch as pij is zero for
any j that is not a neighbor of i.
Equation (10.5) looks very much like equation (10.2), so it is worthwhile to
point out the diﬀerences explicitly. How could the quantity g(t)
i
deﬁned by
(10.2) diﬀer from f(xi)? The value f(xi) is the probability that xi is labeled
positive on the assumption that its label was propagated (ultimately) from a
node in L. Let us call such labels “genuine” labels, as distinct from “pseudo”
labels that came from the random assignment of labels at time t = 0. At time
t = 0, any node in U has a pseudo label, not a genuine label propagated from a
node in L. Hence g(0)
i
is simply 1/2, and it is unrelated to f(xi). Even at times

198
Semisupervised Learning for Computational Linguistics
t > 0, there will be a discrepancy between f and g. The probability f(xi)
averages over all paths along which the label of xi might have propagated,
which is to say, all paths along which a particle might travel before reaching
a node in L. Some portion of those paths will have a length greater than t,
and they are omitted from the calculation of g(t)
i . Conversely, even if node
i has received a genuine label by time t, the recursive computation of g(t)
i
may still reﬂect indirect inﬂuence of pseudo labels, propagated from earlier
times. But with time, the paths with length greater than t will represent an
ever smaller portion of f(xi), and the inﬂuence of pseudo labels on g(t)
i
will
decrease as genuine labels are continually propagated from nodes in L. In
short, we expect gi to converge to f(xi). In the limit, when gi = f(xi) for all
i, the equation (10.5) is equivalent to the ﬁxed-point equation (10.4).
The ﬁxed-point equation (10.4) naturally brings to mind the stationary
distribution of a Markov chain, as in equation (9.8). But in fact the two are
not the same. The Markov transition matrix in (9.8) is column-stochastic;
that is, the columns sum to unity. The matrix in (10.4) is row-stochastic.
Moreover, the vector x in (9.8) was a probability distribution, but there is no
guarantee that the elements of g in (10.4) sum to unity; in fact, they almost
certainly do not. We can safely say that g in (10.4) is an eigenvector of P with
eigenvalue 1, but we do not yet have grounds to assert that an eigenvector
with eigenvalue 1 exists. (We also have no grounds to expect that such an
eigenvector, if it exists, should agree with gL.)
10.3
Harmonic functions
The equation (10.5) can be viewed as an “averaging property” of the function
f: for any node xi, the value of f at node xi is the weighted average of
its values at neighboring nodes. A function that possesses this property is
known as a harmonic function.
Harmonic functions arise in a number of
contexts.
For example, a harmonic function is a natural generalization of
linear interpolation. Consider a simple linear graph, such as that in ﬁgure
10.2. Assume that the value at the left end of the line is ﬁxed at 0, and the
value at the right end is ﬁxed at 1, and assume that the edge weights are all
unity. Linear interpolation of values at the interior nodes gives the results
shown. Linear interpolation is equivalent to the averaging property (10.5) in
this case; and it is natural to take the averaging property as the basis for
generalizing linear interpolation to more complicated graphs.
The averaging property also arises if we think of the nodes of the graph
as points in an elastic mesh, and values as heights above the ground. Edge
weights correspond to the strength of the elastic forces along the edge. At
equilibrium, an unclamped node will have an altitude at which the forces

Propagation Methods
199
FIGURE 10.2
A simple linear graph. The heavy circles represent nodes whose values are
clamped.
pulling it upward are balanced by the forces pulling it downward, which is
to say, when the net force on it is zero.
The force exerted on node i by
a neighboring node j is the strength of the connection between them, wij,
times the diﬀerence in their altitudes, f(xj) −f(xi). Hence, at equilibrium,
we have
X
j
[f(xj) −f(xi)] wij = 0.
That is
X
j
f(xj)wij = f(xi)
X
j
wij.
Dividing through by P
j wij yields the averaging property (10.5).
Interpreting values as the height of an elastic mesh accords with physical
intuition in the example of the linear graph (ﬁgure 10.2). It corresponds to
a cord stretched between the clamped nodes at either end of the graph. We
assume there is no gravity, so that the cord does not sag, but rises linearly
from one end to the other.
Figure 10.3a provides a somewhat more complicated example. The corners
are the only clamped values. Two corners, diagonally opposite, are clamped
at 0 and the other two are clamped at 1. Given the symmetry of the graph,
the center node, along with the two axes that intersect there, should have a
value halfway between the extremes. The surface as a whole should obviously
be saddle-shaped, but intuition does not give clear guidance regarding the
shapes of the corners.
The averaging property does give clear guidance, however. It is easy to
conﬁrm that the value for each unclamped node in ﬁgure 10.3a is the arith-
metic mean of its neighbors’ values. (We note that the clamped nodes do not

200
Semisupervised Learning for Computational Linguistics
0
1
1
0
.3
.5
.7
.4
.5
.6
.3
.7
.5
.5
.5
.5
.5
.6
.5
.4
.7
.3
.7
.5
.3
.25
.5
.75
1
1
2
3
4
5
5
4
3
2
1
(a)
(b)
FIGURE 10.3
(a) A square graph with clamped corners. (b) The resulting saddle-shaped
surface.
satisfy the averaging property, but then we should not expect them to – they
are not free to “seek their level.”) The resulting shape, when plotted (ﬁgure
10.3b), does accord qualitatively with physical intuition.
Note that the clamped values need not be at the edge of the graph. In
ﬁgure 10.4, we have a graph with the same shape as the previous one, but
this time the corners are clamped at 0 and the centermost node is clamped
at 1. The values shown are determined by the averaging property: the result
is a “circus tent” shape. In the absence of gravity, there is no sag, but the
edges are pulled up by the tension of the “center pole.”
The clamped nodes are conventionally called the boundary, even though
they need not actually lie on the outer edge of the graph, as we have just
seen. The unclamped nodes constitute the interior of the graph.
There are other physical systems that behave like an elastic mesh, in the
sense of satisfying the averaging property. An example that we will return
to is pressure of a ﬂuid ﬂowing through a network of pipes. Electricity is
another example – indeed, one can treat electricity as a ﬂuid in which voltage
represents pressure.
A version of the label-propagation problem arises with harmonic functions.
We have exhibited several graphs with value assignments satisfying the aver-
aging property, but how can such a value assignment be computed in general?
The problem can be stated as ﬁnding a harmonic function that agrees with
given values on the boundary nodes.
The solution will be the topic of subsequent sections. In the remainder of
this section, we address the preliminary question of whether a unique solution
exists. We expect a unique solution to exist, given the physical intuition that
a stretched fabric settles quickly into an equilibrium. We will see that this
expectation is correct.

Propagation Methods
201
0
0
1
0
0
5
21
7
21
5
21
8
21
11
21
8
21
5
21
5
21
11
21
11
21
7
21
7
21
5
21
5
21
8
21
11
21
8
21
5
21
7
21
5
21
.25
.5
.75
1
1
2
3
4
5
5
4
3
2
1
(a)
(b)
FIGURE 10.4
A clamped value in the middle of the graph.
We start with three simple properties of harmonic functions. The ﬁrst is
not actually needed for our immediate purposes, but this is an appropriate
place to mention it.
• Every constant function is harmonic.
• The superposition principle: a linear combination of harmonic functions
is itself a harmonic function.
• The extreme value property: the extreme values of a harmonic function
occur in the boundary.
The ﬁrst property (the harmonicity of any constant function) is easily seen.
If f(xi) = k for all i, then clearly
f(xi) = k =
X
j
pijk =
X
j
pijf(xj).
If f is constant, then it is harmonic.
The second property is the superposition principle. To see that it holds,
consider any harmonic functions f1, f2, . . . , fn and any weights α1, α2, . . . , αn.
These deﬁne the linear combination g(xi) = P
t αtft(xi). To show that the
function g is harmonic, it suﬃces to show that an arbitrary interior node i
satisﬁes the averaging property:
g(xi) =
X
t
αtft(xi)
=
X
t
αt
X
j
pijft(xj)

202
Semisupervised Learning for Computational Linguistics
=
X
j
pij
X
t
αtft(xj)
=
X
j
pijg(xj).
Hence g is harmonic, and since we assumed nothing about g except that it is
a linear combination of harmonic functions, we have shown that every linear
combination of harmonic functions is harmonic.
The third property is the extreme value property. It states that, for any
harmonic function f, the extreme values of f occur on the boundary. More
precisely: a maximum of f occurs at a boundary node, and a minimum of
f occurs at a boundary node. To prove it, we consider maxima; the case
for minima is symmetric.
Suppose f(xi) is a maximum of f.
If xi is a
boundary node, we are done. Suppose instead that xi is an interior node. If
the neighbors of xi have varying values for f, then f(xi), being an average
of the values of xi’s neighbors, must be less than the maximum (and greater
than the minimum). But we supposed f(xi) to be a maximum of f; hence we
must conclude that all neighbors of xi have the same value for f as xi does.
By the same argument, all the neighbors of neighbors of xi have the same
value for f, and so on across the entire graph. In short, if the maximum is an
interior node, then f is a constant function, and all boundary nodes are also
maxima of f. In every case, then, a maximum of f occurs on the boundary.
Now we return to the assertion of central interest, the uniqueness of the
harmonic solution for interior node values. We will show that, if f is a har-
monic function, then its values at interior nodes are uniquely determined by
its values at boundary nodes. More precisely, assume that f and g are har-
monic functions that agree on the values for boundary nodes. We will show
that f and g are identical.
By the superposition principle, the diﬀerence h = f −g is also a harmonic
function. Since f and g agree on boundary nodes, we know that h(x) = 0
for all boundary nodes. By the extreme value property, the minimum and
maximum of h occur on boundary nodes. But since h(x) = 0 for all boundary
nodes, it follows that the minimum and maximum of h are both 0, hence
h(x) = 0 everywhere, that is, f(x) = g(x) for all nodes.
To recap, we have seen that the averaging property (10.5) accords with
physical intuition about an elastic mesh clamped at boundary nodes – at
least in cases where the physical intuition is clear. We have also seen that
the averaging property, which is to say, harmonicity, determines a unique
solution to the problem of assigning values to interior nodes. Since the ﬁxed
point equation (10.4) is simply a restatement in linear algebraic terms of the
averaging property (10.5), we have just shown that a unique solution g does in
fact exist. The originally labeled nodes L and the graph weights P determine
a unique assignment of labels to the unlabeled nodes U.

Propagation Methods
203
10.4
Fluids
Before addressing the question of how to compute the solution g, let us con-
sider one more physical analogue, namely, that of a ﬂuid moving through a
network. The discussion will lead to a proof that harmonic functions min-
imize energy, which will in turn be useful in understanding algorithms for
computing solutions.
Speciﬁcally, let us think of edges in the graphs as pipes, and nodes as
junctures. Then the positive-label probabilities f(x) turn out to correspond
naturally to pressure. In the interior, the network is closed; the only ﬂow into
or out of an interior node is through the pipes of the network. The weight
of an edge will turn out to correspond naturally to the capacity of the pipe
represented by the edge.
At boundary nodes, pressure is maintained either at 1 or 0 by pumping ﬂuid
in or out, as needed. We expect ﬂuid to ﬂow from nodes of higher pressure
toward nodes of lower pressure.
An interior node that is allowed to seek
its own level should intuitively have a pressure somewhere between that of
its high-pressure neighbors, from which it receives ﬂuid, and its low-pressure
neighbors, to which it sends ﬂuid. Thus it is natural to expect the pressure
at an interior node to be an average of the pressures at its neighbors.
It
is plausible that pressure satisﬁes the averaging property (10.5). However,
rather than assuming that pressure satisﬁes the averaging property, we will
deﬁne a concept of ﬂow related to pressure, and show that the averaging
property arises by minimizing the energy dissipation of the system.
Incidentally, if we interpret the ﬂuid to be electricity, then pressure f corre-
sponds to voltage, the pipe capacity corresponds to conductance (the recipro-
cal of resistance), and ﬂow corresponds to electrical current. For this reason,
we will use the symbol “i” for ﬂow: “i” is the standard symbol for electrical
current. That choice means, however, that we cannot continue to use i as
a node index. Instead we will use x and y to represent nodes, and write in
particular wxy for the weight of the edge connecting x and y. The averaging
property (10.5) becomes
f(x) =
X
y
p(y|x)f(y)
where
p(y|x) =
wxy
P
z wxz
.
10.4.1
Flow
A ﬂow is a function i(x, y) associated with edges (x, y). If there is no edge
connecting x and y, we deﬁne i(x, y) to be 0. Flow is directional. Positive

204
Semisupervised Learning for Computational Linguistics
i(x, y) means that the ﬂow is from x to y. Negative i(x, y) means that the
ﬂow is from y to x. An immediate consequence is that i(y, x) = −i(x, y).
When the graph is partitioned into a boundary and interior, as we have
been assuming, we require that the only ﬂow into or out of the system be
at the boundary. Let us deﬁne the (net) external inﬂow at a node x to be
the total inﬂows to x from outside the system, less the total outﬂows from x.
If there is net inﬂow at x from the outside, then the external inﬂow at x is
positive, and if there is net leakage from x to the outside, then the external
inﬂow at x is negative.
The external inﬂow is whatever is not accounted for by ﬂow from or to
neighboring nodes within the system. That is, we deﬁne external inﬂow i(x)
as
i(x) ≡
X
y
i(x, y).
If the neighbor y is downstream of x, then i(x, y) is positive, and if the neigh-
bor y is upstream of x, then i(x, y) is negative. If the positives are greater
than the negatives, then there is more outﬂow from x than can be accounted
for by inﬂows within the system, so there must be additional inﬂow from out-
side the system. Hence positive i(x) represents inﬂow to the system from the
outside. Conversely, negative i(x) represents outﬂow from the system to the
outside. If x has no connections to the outside, then the inﬂows and outﬂows
within the system must exactly balance, and the sum is 0.
In short, we deﬁne a (proper) ﬂow to be a function i(x, y) that satisﬁes
three conditions:
(a)
i(x, y) = 0
if there is no edge (x, y)
(b)
i(x, y) = −i(y, x)
(c)
i(x) = P
y i(x, y) = 0
for any x in the interior.
(10.6)
In the context of electricity, property (10.6c) is known as Kirchhoﬀ’s current
law. It states that there is no “leakage” between the interior of the system
and the outside world.
As with harmonic functions, the linear combination of ﬂows is a ﬂow. De-
ﬁne:
i(x, y) =
X
t
αtit(x, y).
The resulting quantity i satisﬁes condition (10.6a): if x and y are not neigh-
bors, then it(x, y) = 0 for all t, so i(x, y) = 0. The quantity i also satisﬁes
condition (10.6b):
i(x, y) =
X
t
αtit(x, y)
= −
X
t
αtit(y, x)

Propagation Methods
205
= −i(y, x).
Finally, to see that i satisﬁes condition (10.6c), let x be an arbitrary interior
node.
X
y
i(x, y) =
X
y
X
t
αtit(x, y)
=
X
t
αt
X
y
it(x, y)
= 0.
A notable property of ﬂows is that the total net external inﬂow P
x i(x) is
always 0. By deﬁnition,
X
x
i(x) =
X
x,y
i(x, y)
= 1
2
X
x,y
i(x, y) + 1
2
X
x,y
i(x, y)
= 1
2
X
x,y
i(x, y) −1
2
X
x,y
i(y, x)
= 1
2
X
x,y
i(x, y) −1
2
X
y,x
i(x, y)
= 0.
Intuitively, the total inﬂow to the system equals the total outﬂow.
10.4.2
Pressure
Physical intuition suggests that the ﬂow from x to y should be proportional
to the pressure diﬀerence between x and y.
If there is no pressure diﬀer-
ence, there is no ﬂow, and the ﬂow increases monotonically with increase in
the pressure diﬀerence. Intuition is not suﬃciently crisp to dictate that the
monotone increase is exactly proportional, but proportionality is the most
natural assumption.
Physical intuition also suggests that ﬂow is proportional to the capacity
of the pipe connecting x and y. It is natural to take the edge weight wxy
to represent the capacity (cross-sectional area) of the pipe between x and
y. Recall that wxy = wyx, meaning that the capacity is the same in both
directions, in accordance with intuition. If there is no edge between x and y,
then wxy = 0, which we can paraphrase as “no pipe, no capacity,” again in
accordance with intuition.
In short, the relation between ﬂow and pressure suggested by physical in-
tuition is
i(x, y) = [f(x) −f(y)]wxy.
(10.7)

206
Semisupervised Learning for Computational Linguistics
In the context of electricity, the relation (10.7) is known as Ohm’s law. The
ﬂow is from high pressure to low. It is positive if f(x) ≥f(y) and negative
otherwise.
Equation (10.7) establishes a relationship between properties of i and prop-
erties of f. Namely, if functions i and f satisfy Ohm’s law (10.7), then we
can show that i is a ﬂow if and only if f is a harmonic function. In an Ohmic
system, properness of ﬂow is equivalent to harmonicity of pressure.
First, Ohm’s law (10.7) implies that i satisﬁes (10.6a) and (10.6b), whether
f is harmonic or not. Speciﬁcally, the satisfaction of (10.6a) follows immedi-
ately from the fact that wxy = 0 if x and y are not connected by an edge.
And the satisfaction of (10.6b) follows from the symmetry of wxy:
i(x, y) = [f(x) −f(y)]wxy
= −[f(y) −f(x)]wyx
= −i(y, x).
So it remains only to show that Kirchhoﬀ’s law (10.6c) is equivalent to har-
monicity of f.
Let x be an arbitrary interior node. Then the following equations are all
equivalent:
X
y
i(x, y) = 0
(10.8)
X
y
[f(x) −f(y)]wxy = 0
X
y
[f(x) −f(y)]p(y|x) = 0
X
y
p(y|x)f(x) −
X
y
p(y|x)f(y) = 0
f(x) =
X
y
p(y|x)f(y).
(10.9)
That is, Kirchhoﬀ’s law (10.8) is indeed equivalent to the harmonicity of f
(10.9), and we have completed the proof that properness of ﬂow is equivalent
to harmonicity of pressure, if Ohm’s law holds.
We can view Ohm’s law as deﬁning i(x, y) as a function of f and the weights
wxy. Take wxy as ﬁxed. If f is a harmonic function, let us call the ﬂow
i(x, y) = [f(x) −f(y)]wxy,
the Ohmic ﬂow generated by f. We know that i(x, y) is a ﬂow, because f is
harmonic and harmonic pressure implies a proper ﬂow, whenever Ohm’s law
holds.
Not all ﬂows are Ohmic. Figure 10.5 shows a simple example of a non-
Ohmic ﬂow.
All edges have weight one, so the ﬂow i(x, y) is simply the

Propagation Methods
207
A
B
2
2
1
1
1
FIGURE 10.5
A non-Ohmic ﬂow. Nodes A and C are boundary nodes, with net inﬂow of 2
at A and net outﬂow of 2 at C.
pressure diﬀerence f(x) −f(y). If there were a harmonic pressure function f,
the edge AC requires that f(C) = f(A)−1, but the edges AB and BC require
that f(C) = f(B) −1 = f(A) −2, a contradiction.
We can also reverse Ohm’s law to deﬁne f in terms of i:
f(x) −f(y) = i(x, y)/wxy.
(10.10)
Unfortunately, (10.10) does not determine a unique function f. If f(x) satisﬁes
(10.10) for a given i, then so does every function g(x) = f(x) + k for any
constant k.
However, a given ﬂow i does determine family of harmonic functions {f(x) =
f0(x) + k|k ∈R} where f0 is any arbitrary member of the family. Namely, if
two functions f and g both stand in relation (10.10) to a ﬁxed i, then
f(x) −f(y) = g(x) −g(y)
for all edges (x, y). (For non-edges, wxy is 0.) Rearranging terms yields
f(x) −g(x) = f(y) −g(y).
That is, if x and y are connected by an edge, there is a constant k such that
f(x) −g(x) = k and f(y) −g(y) = k. Assuming the graph is connected, we
can follow edges to show that f and g diﬀer by k everywhere. A given Ohmic
ﬂow i determines a family of harmonic functions that have the same “shape,”
and diﬀer only by an additive constant. We can choose one of them to serve
as f0.
Let us say that a harmonic function f is in standard form if
min
x f(x) = 0.

208
Semisupervised Learning for Computational Linguistics
A standard-form harmonic function is non-negative everywhere, and has a
zero value for at least one point.
Any Ohmic ﬂow i determines a unique
standard-form harmonic function: if i is related to any function f by (10.10),
simply translate f to set its minimum to 0.
We know that the standard-
form function is unique, because if f and g are two standard-form harmonic
functions in the same family, then they have the same minimum point x0 and
f(x0) = 0 = g(x0), hence k = 0 and f and g are the same everywhere.
We note in passing that, by the extreme value property, we can take the
minimum point x0 to be a boundary point.
There are a couple of useful facts to note about Ohmic ﬂows. First, the
linear combination of Ohmic ﬂows is Ohmic. We already know that the linear
combination of ﬂows is a ﬂow; now we note that it is generated by a harmonic
function:
i(x, y) =
X
t
αtit(x, y)
=
X
t
αt[ft(x) + kt −ft(y) −kt]wxy
=
"X
t
αtft(x) −
X
t
αtft(y)
#
wxy.
We know that P
t αtft is harmonic because linear combinations of harmonic
functions are harmonic.
Another useful property is that a constant pressure generates a null ﬂow
– as intuition would lead us to expect. Assume that f(x) = f(y) for all x
and y. Then i(x, y) = [f(x) −f(y)]wxy is 0 for all edges, hence i(x) = 0
everywhere. Conversely, if i is the null ﬂow, then f is a constant pressure
function. Namely, we know that a constant pressure function f0 exists that
generates i. Hence any pressure function f that generates i diﬀers from f0 by
a constant; and since f0 is constant, it follows that f = f0 + k is constant.
We established earlier that ﬁxing the weights wxy and values f(x) for bound-
ary nodes x determines a unique harmonic function f. In terms of ﬂuids, ﬁx-
ing capacities and boundary pressures determines a unique harmonic pressure
function f.
That is unfortunately not true of ﬂows, in general. Fixing the values i(x)
for boundary nodes x, does not determine a unique ﬂow. Fixing the boundary
ﬂows does trivially determine a unique net external inﬂow, inasmuch as we
already know that i(x) = 0 for all interior nodes. But many diﬀerent ﬂows
i(x, y) are consistent with a given boundary ﬂow i(x) for x ∈B.
However, it is true of Ohmic ﬂows.
For ﬁxed capacities wxy and ﬁxed
boundary ﬂows i(x), there is a unique Ohmic ﬂow i(x, y). To see this, suppose
that i and j are both Ohmic ﬂows, and that i(x) = j(x) for all boundary nodes.
Deﬁne d(x, y) = j(x, y) −i(x, y). Since d is a linear combination of Ohmic
ﬂows, it is Ohmic. But d(x) = 0 for all boundary nodes, since i(x) = j(x) on

Propagation Methods
209
the boundary, and d(x) = 0 for all interior nodes by Kirchhoﬀ’s law. That is,
d is the null ﬂow, hence d(x, y) = 0 for all edges, meaning that i and j are
identical.
10.4.3
Conservation of energy
Energy is the rate at which work is done, and the unit of work in this context
is a single unit of ﬂuid m moving through a single unit of pressure diﬀerence
d. That is, work W = md. Energy is the rate of work: H = W/t = md/t.
Intuitively, ﬂow is the rate at which ﬂuid is moving, m/t, so it follows that
energy is ﬂow times pressure diﬀerence.
That is, the reasoning of the previous paragraph suggests that we deﬁne
the energy associated with edge (x, y) as follows:
Hxy(f, i) ≡[f(x) −f(y)]i(x, y).
(10.11)
If the ﬂow is “uphill,” from low pressure to high, then the energy is being
applied to move the ﬂuid against the stream, and if the ﬂow is downhill, the
energy is being released by the ﬂuid. In downhill ﬂow, either f(x) > f(y) and
i(x, y) is positive, or f(x) < f(y) and i(x, y) is negative. In either case, the
value Hxy(f, i) is positive. So we should think of Hxy(f, i) as energy release:
the conversion of potential energy to kinetic energy. If the energy dissipation
is negative, then the system consumes energy.
We have motivated the deﬁnition for Hxy by physical intuition, but for the
moment, let us not rely on those physical intuitions, but simply consider Hxy
as an abstract quantity deﬁned by equation (10.11).
It is deﬁned for any
functions f and i.
If i is a ﬂow, then Hxy is symmetric in x and y:
Hyx(f, i) = [f(y) −f(x)]i(y, x)
= [f(x) −f(y)]i(x, y)
= Hxy(f, i).
The only “directionality” of the energy is the intrinsic directionality of the
pressure diﬀerence. When we compute the energy dissipation of the whole
system, it is conventional to multiply by 1/2 in order to correct for “double-
counting” of edges:
H(f, i) ≡1
2
X
x,y
[f(x) −f(y)]i(x, y).
If i is a ﬂow, then we can apportion the energy dissipation among the nodes
of the graph:
Hx(f, i) ≡f(x)i(x).

210
Semisupervised Learning for Computational Linguistics
We conﬁrm that summing Hx over all nodes yields H:
X
x
Hx(f, i) =
X
x
f(x)i(x)
=
X
x,y
f(x)i(x, y)
= 1
2
X
x,y
f(x)i(x, y) + 1
2
X
x,y
f(x)i(x, y)
= 1
2
X
x,y
f(x)i(x, y) −1
2
X
x,y
f(x)i(y, x)
= 1
2
X
x,y
f(x)i(x, y) −1
2
X
y,x
f(y)i(x, y)
= 1
2
X
x,y
[f(x) −f(y)]i(x, y)
= H(f, i).
We note that what we have said so far about H has relied only on properties
(10.6a) and (10.6b) of i. Nothing yet has depended on any properties of f
except that it is a function deﬁned on the nodes of the graph. The Kirchhoﬃan
property of i (10.6c) allows us to show that energy is conserved in the system,
in the sense that the only energy dissipation is at the boundary.
THEOREM 10.1 (Conservation of Energy)
For any function f on the graph, and any ﬂow i, the net energy dissipation
in the interior of the graph is zero. That is, where B is the set of boundary
nodes:
H(f, i) =
X
x∈B
f(x)i(x).
PROOF. We have just seen that H(f, i) = P
x f(x)i(x). By Kirchhoﬀ’s law
(10.6c), i(x) = 0 for all interior nodes x, and the theorem follows immediately.
10.4.4
Thomson’s principle
In the previous section, we made no use of Ohm’s law (10.7) to relate ﬂow and
pressure. Everything we said about H is true for any ﬂow i and any function
f on the nodes of the graph.
Suppose i is an Ohmic ﬂow generated by a harmonic function f. Then we
can express H(f, i) as a function of i only, or as a function of f only:
I(i) = 1
2
X
x,y
i(x, y)2/wxy

Propagation Methods
211
J(f) = 1
2
X
x,y
[f(x) −f(y)]2wxy.
(10.12)
If i is generated by f, then H(f, i) = I(i) = J(f).
But even if i is not Ohmic, I(i) is well-deﬁned, and even if f is not harmonic,
J(f) is well-deﬁned. That is, I and J provide natural generalizations of H to
non-Ohmic ﬂows and non-harmonic pressure functions.
Interestingly, Ohmic ﬂows and harmonic functions still have a special status
with respect to I and J.
THEOREM 10.2 (Thomson’s principle, primal form)
For ﬁxed boundary ﬂows, and ﬁxed capacities wxy, the ﬂow that minimizes
“I” energy is the Ohmic ﬂow determined by the given boundary values.
PROOF. Let k(x) for x ∈B represent ﬁxed net boundary ﬂow values, and let
j be an arbitrary ﬂow that satisﬁes j(x) = k(x) for all boundary nodes. Let
i be the Ohmic ﬂow that satisﬁes i(x) = k(x) on the boundary. (We saw at
the end of section 10.4.2 that it is unique.)
Deﬁne d(x, y) = j(x, y) −i(x, y). Since a linear combination of ﬂows is a
ﬂow, we know that d is a ﬂow:
2I(j) =
X
x,y
j(x, y)2/wxy
=
X
x,y
i(x, y)2/wxy + 2
X
x,y
i(x, y)d(x, y)/wxy +
X
x,y
d(x, y)2/wxy.
Let us consider the middle term. Since i is Ohmic, i(x, y) can be expressed
as [f(x) −f(y)]wxy for some function f. That is, the middle term can be
expressed as
2
X
x,y
[f(x) −f(y)]d(x, y) = 4H(f, d).
By conservation of energy, this is equal to
4
X
x∈B
f(x)d(x).
This equals zero, since d(x) = j(x) −i(x) and j(x) = i(x) on the boundary.
It follows that
2I(j) =
X
x,y
i(x, y)2/wxy +
X
x,y
d(x, y)2/wxy
≥2I(i).
We have equality if and only if d(x, y) = 0 everywhere, which is to say, if and
only if j = i. This proves that i is unique.

212
Semisupervised Learning for Computational Linguistics
THEOREM 10.3 (Thomson’s principle, dual form)
For ﬁxed capacities and ﬁxed boundary pressures, there is a unique pressure
function that minimizes “J” energy, and it is harmonic.
PROOF. The proof is similar to that for the primal form. Let k(x) represent
ﬁxed boundary pressures, and let g be an arbitrary function on the nodes of
the graph with g(x) = k(x) for all x ∈B. Let f be any harmonic function
with f(x) = k(x) for all x ∈B, and deﬁne d(x) = g(x) −f(x). Then
2J(g) =
X
x,y
[g(x) −g(y)]2wxy
=
X
x,y
[f(x) + d(x) −f(y) −d(y)]2wxy
=
X
x,y
[f(x) −f(y)]2wxy + 2
X
x,y
[f(x) −f(y)][d(x) −d(y)]wxy
+
X
x,y
[d(x) −d(y)]2wxy.
Again we consider the middle term. Deﬁne i(x, y) = [f(x) −f(y)]wxy. The
middle term is equal to
2
X
x,y
[d(x) −d(y)]i(x, y) = 4H(d, i).
Since f is a harmonic function, we know that i is a ﬂow, hence conservation
of energy applies to H. (Conservation of energy makes no requirement of d.)
Hence the middle term equals
4
X
x∈B
d(x)i(x) = 4
X
x∈B
[g(x) −f(x)]i(x).
Since g(x) and f(x) agree at boundary nodes, the middle term reduces to
zero. It follows that
2J(g) =
X
x,y
[f(x) −f(y)]2wxy +
X
x,y
[d(x) −d(y)]2wxy
≥2J(f).
Equality holds just in case d(x) −d(y) = g(x) −f(x) −g(y) + f(y) = 0 for
all edges (x, y), which is to say, just in case f and g diﬀer only by an additive
constant. Since f and g agree on the boundary, there is at least one point
with f(x) = g(x), so the constant is 0. In short, equality holds just in case
f = g, so we know that f is unique.

Propagation Methods
213
10.5
Computing the solution
We turn now to the question of how to compute a harmonic function f, given
values for boundary nodes.
The connection to random walks provides one conceptually simple way to
compute f, namely, by doing a Monte Carlo simulation. Recall that f(x) is
interpretable as the probability that a particle starting in node x is ultimately
absorbed at a positively labeled boundary node. To determine that proba-
bility, we simulate the motion of a particle. We start it at node x, and at
each step of the simulation, we choose a neighbor at random in accordance
with p(y|x). Eventually the particle reaches a boundary node, and we record
the label of that boundary node, positive or negative. If we run many simu-
lated particles and count the number of times that the particle is absorbed in
positive versus negative boundary nodes, we can estimate f(x) as the propor-
tion of positive absorptions out of total runs. The reasoning is valid, and the
computer code is easy to write, but it takes a very large number of simulated
trials to get much accuracy in the estimates.
A second method is called the method of relaxations. It is in fact equiva-
lent to the label propagation algorithm of ﬁgure 10.1. We start oﬀwith the
boundary nodes clamped at their known values, but randomly assigned labels
in the interior. Then we cycle through the interior nodes. At each interior
node, we assume for the moment that the neighbors’ values are correct, and
we replace the current node’s value with the value required by the averaging
property:
f(x) =
X
y
p(y|x)f(y).
If x already satisﬁes the averaging property, we skip it. We keep iterating
until all nodes satisfy the averaging property, or at least until the changes
become smaller than some threshold.
Notice that the update just described, namely, setting a node’s value to
make it satisfy the averaging property, is identical to the update (10.2) used
by the label propagation algorithm. In fact, the only diﬀerence between the
algorithms is that the method of relaxations updates nodes one at a time,
whereas the label propagation algorithm updates nodes in parallel.
That
diﬀerence is not signiﬁcant. Either algorithm is exact in the limit, though
they must be considered approximate in practice, as one must stop after some
ﬁnite number of iterations.
The dual form of Thomson’s principle gives us a proof that the method of
relaxations ﬁnds a harmonic function with the given boundary values. Sup-
pose f is not harmonic, and let
J = 1
2
X
x,y
[f(x) −f(y)]2wxy

214
Semisupervised Learning for Computational Linguistics
be its energy value. In a single “relaxation,” we pick a single point x and
change the value f(x). Consider the subgraph consisting of just x and its
neighbors. The overall energy J is a summation over “edge energies” [f(x) −
f(y)]wxy. Thus, the only change in J consists in changes to “edge energies”
within the subgraph. We can view the subgraph as a graph in its own right
in which all nodes except x are boundary nodes with ﬁxed values. We change
f(x) so as to make f harmonic on the subgraph. By Thomson’s principle, that
change minimizes J on the subgraph, and since J is held constant outside of
the subgraph, the change cannot increase J overall. Since we skip nodes whose
subgraph is already harmonic, the change in fact strictly decreases J. Hence
the method of relaxations produces a strictly decreasing sequence of J values,
terminating only when J reaches its minimum, at which point f is harmonic.
A third method is to use matrix operations. This method is exact and non-
iterative. Consider the matrix P and vector g used in the label propagation
algorithm. Recall that we partitioned g into half-vectors gL and gU, and we
can partition P similarly. The ﬁxed-point equation (10.4) becomes:

gL
gU

=

PLL PLU
PUL PUU



gL
gU

.
(10.13)
The half-vector gL is constrained to match g(0)
L , so our interest is to determine
the value of gU:
gU = PULgL + PUUgU
which yields
gU = (I −PUU)−1PULgL.
Substituting the known value for gL, we conclude that
gU = (I −PUU)−1PULg(0)
L .
(10.14)
The solution exists only if I −PUU is invertible, but since we know from our
previous discussion that the solution does exist, we can conclude that I−PUU
is invertible.
Actually, we do have to make a minimal assumption about the graph,
namely, that each connected component contains at least one labeled node.
We are quite happy, in fact, to make stronger assumptions.
If the graph
consists of multiple connected components, we might as well treat each as a
separate learning problem, so we can without loss of generality assume that
the graph consists of a single connected component. Moreover, unless there
is at least one positive node and one negative node, the learning problem is
trivial, so we assume that the set of labeled nodes is not only nonempty, but
contains at least one positive and one negative node.

Propagation Methods
215
10.6
Graph mincuts revisited
In section 7.5 we introduced the idea of clustering by cutting a graph into two
connected components, choosing the cut so as to minimize the total weight of
the edges that are severed. Though not directly related to harmonic functions,
minimum cuts do turn out to have a strong connection to ﬂows. In particular,
there is a well-known algorithm to ﬁnd a mincut given a weighted graph, called
the Ford-Fulkerson algorithm, which is predicated on an equivalence between
minimum cuts and maximum ﬂows through a capacitated graph. We describe
the algorithm in this section.
In previous sections, we thought of edge weights as capacities, in the sense
of cross-sectional areas of pipes. Those capacities imposed no maximum on
ﬂow, however – multiplying any ﬂow by a constant greater than one yields
a larger ﬂow. It is easy to conﬁrm that ki(x, y) satisﬁes conditions (10.6) if
i(x, y) does.
In a physical system, though, there are limits on how much ﬂow can be
achieved through a particular pipe. If nothing else, the ﬂow is limited by the
power of the available pumps and the strength of the walls of the pipe. Deﬁne
a capacitated graph to be a weighted graph in which the weights wxy are
interpreted as maximum permissible ﬂow. Again, this is a diﬀerent notion of
“capacity” than before – not cross-sectional area, but maximum ﬂow. The
maximum ﬂow problem is to ﬁnd the largest overall ﬂow that does not exceed
any edge capacity.
This raises the question of how we measure the total ﬂow. After all, sum-
ming i(x) for all nodes always yields 0. The maximum ﬂow problem as stan-
dardly formulated assumes a graph with only two boundary nodes, a source
s and a sink t. Let us call such a graph an s-t graph. If s and t are the only
boundary nodes, then i(s) = −i(t); we deﬁne the source to be the one with
positive ﬂow. The measure of total ﬂow is simply i(s).
Finding the maximum ﬂow might seem trivial: simply ﬁll all the pipes to
capacity. But that is not usually possible. Figure 10.6a gives a trivial example.
We cannot ﬁll pipe AB to capacity, because pipe BC has a smaller capacity.
Figure 10.6b gives a more interesting example. We cannot send any more ﬂow
down the pipe AB, because it is at capacity. We cannot send more ﬂow down
the path ACD, because CD is at capacity, and we cannot send any more ﬂow
down ACEF, because EF is at capacity. It would seem that the ﬂow is the
maximum possible. But it turns out there is a way to increase it, namely, by
reversing the ﬂow along the edge DE and rerouting it along DF; additional
ﬂow can then be sent along the path ACEDF. We discuss this in more detail
below.
Granted that ﬁnding the maximum ﬂow is not trivial, how does it help
us ﬁnd a mincut? What is the relationship between ﬂow and cut? Let us
consider a couple of cuts through the graph of ﬁgure 10.6b, as shown in ﬁgure

216
Semisupervised Learning for Computational Linguistics
A
F
B
D
C
E
A
B
C
2/4
2/2
2/4
2/4
2/2
1/1
1/1
3/6
2/6
3/3
(a)
(b)
FIGURE 10.6
One cannot always ﬁll all pipes.
it adds up to 5 in all three cases. It should be clear that this will always be
the case. There is a ﬁxed amount of ﬂow going in, and by Kirchhoﬀ’s law,
the total ﬂow going into a node is the same as the total ﬂow going out. If we
cut the graph a little lower or higher, we are still cutting the same total ﬂow,
though it may be apportioned a little diﬀerently among the edges.
Note that this is true even if we consider wiggly cuts as in ﬁgure 10.7b.
At least, it is true if we consider the cut to have an “uphill” side and a
“downhill” side, and ﬂows that cross the cut in the wrong direction count as
negative ﬂows.
So we see that the total ﬂow across a cut is the same for all cuts. But the
value of a cut is not the total ﬂow that it cuts, but rather, the total capacity
that it cuts. Diﬀerent cuts necessarily transect the same amount of ﬂow, but
they need not transect the same amount of capacity. Consider any two cuts,
like the two cuts in ﬁgure 10.7a. Since both cuts transect the same amount
of ﬂow, we can subtract the amount of transected ﬂow from the value of each
without changing their relative values. What we are comparing then is the
excess capacity of the transected edges. To ﬁnd the cut with minimum value,
we should look for the cut that transects the minimum amount of excess
capacity. The upper cut has (4 −2) + (6 −3) = 5 units of excess capacity,
whereas the lower cut has (4−2)+(1−1)+(6−2) = 6 units of excess capacity;
the upper cut is better.
Flow plus excess capacity equals capacity.
Unlike ﬂow, capacity is not
dependent on the direction in which one follows an edge. When one follows
an edge “upstream,” ﬂow is negative, and as a result excess capacity actually
exceeds capacity.
One can increase the ﬂow through a graph just in case one can ﬁnd a path
from source to sink in which every edge has excess capacity. For example,
10.7. If we add up the ﬂow going through the edges that are cut, we ﬁnd that

Propagation Methods
217
A
F
B
D
C
E
2/4
2/4
2/2
1/1
3/6
2/6
3/3
(a)
1/1
2+3=5
2+1+2=5
A
F
B
D
C
E
2
1
3
(b)
2-2+2+1+2=5
2
2
3
1
2
up
dn
dn
up
FIGURE 10.7
Some cuts.
consider ﬁgure 10.8. In the marked path, every edge has excess capacity. We
can increase ﬂow down the marked path by one unit – that is the minimum of
the excess capacities on edges in the path. Thereby, the total ﬂow is increased
by one unit.
One can think of edges that have no excess capacity as “blocked.” We are
seeking a path from source to sink that is not blocked. Since excess capacity
depends on the direction in which one follows the edge, a blocked edge actually
represents a one-way barrier. For example, in ﬁgure 10.6b, all the strictly
downhill paths are blocked, but the path ACEDF is open – by reversing the
ﬂow along DE, we can increase ﬂow along ACEDF while maintaining the
rest of the graph unchanged. In the downhill direction, that is, the direction
in which the ﬂow is currently traversing the edge, DE has one unit of excess
capacity, but in the uphill direction (ED), it has three units of excess capacity.
In the path ACEDF, the edge with the least excess capacity is DF; it has two
units of excess capacity. Hence we can increase ﬂow along the marked path
by two units. Note that “increasing ﬂow” along the uphill edge ED means
changing the ﬂow from −1 to +1. What we are actually doing is shunting the
ﬂow that goes along CDE to CDF, and making up for it with increased ﬂow
from E to D. One can conﬁrm that the overall result is an increase of two
units in total ﬂow, as shown in ﬁgure 10.9.
We are placing nodes in the ﬁgures so that ﬂow is oriented from top to
bottom.
Edges with 0 ﬂow are horizontal.
Let us mark blocked edges as
in ﬁgure 10.10. In seeking a path, one is not allowed to cross barriers in a
downhill direction, but one is allowed to cross them in an uphill direction.
We can seek a path by systematically marking nodes that we can reach
from the source. If we can reach a given node, and it has an unblocked edge
to another node, that other node is reachable as well. We mark the reachable
nodes by circling them. When there is a path, the sink is reachable. When

218
Semisupervised Learning for Computational Linguistics
A
F
B
D
C
E
2/3
3/4
2/2
1/3
2/5
1/4
1/1
FIGURE 10.8
Flow can be increased along the marked path.
A
F
B
D
C
E
2/4
4/4
2/2
1/1
1/1
5/6
4/6
3/3
FIGURE 10.9
The outcome of increasing ﬂow along the path ACEDF by reversing the ﬂow
along the edge ED.

Propagation Methods
219
A
F
B
D
C
E
2/4
2/4
2/2
1/1
3/6
2/6
3/3
(a)
A
F
B
D
C
E
2/4
4/4
2/2
1/1
5/6
4/6
3/3
(b)
1/1
1/1
FIGURE 10.10
Blocked edges are marked, reachable nodes are circled. (a) Flow can be in-
creased along dotted path; sink is reachable. (b) Sink is not reachable; ﬂow
cannot be increased.
the sink is not reachable, we have found the maximum ﬂow. Figure 10.10b
illustrates.
When we have found a maximum ﬂow, every edge that connects a reachable
node to an unreachable node is blocked. That set of edges constitutes a cut,
separating the graph into two sets – the reachable nodes and the unreachable
nodes. Moreover, the value of that cut is the capacity of the transected edges,
which is equal to the total ﬂow, since none of the edges has excess capacity.
This, then, is a sketch of the algorithm for ﬁnding a mincut. Propagate
reachability through the graph. If node x is reachable with i units of additional
ﬂow, and there is an edge with excess capacity j connecting x to another
node y, then y is reachable with min(i, j) units of additional ﬂow.
There
may be more than one edge by which y is reachable. Keep track of which
edge maximizes the additional ﬂow that reaches y – call that the “best” edge
leading into y.
If the sink is reachable with i units of additional ﬂow, then trace backward
along the path of “best” edges and increase ﬂow along that path by i units. If
the sink is not reachable, then ﬁnd all the edges that connect reachable nodes
to unreachable nodes. The resulting set of edges is the mincut, and the sum
of their capacities is its value.
One might be concerned that requiring a single source and a single sink
limits us to having two labeled instances. But Blum & Chawla [20] propose a
straightforward way of representing a data set with multiple labeled instances
(as well as unlabeled instances) as an s-t graph. Each instance is represented
by a vertex, as we have assumed all along. But the source and sink vertices
do not correspond to instances; rather, they represent the two class labels.
The source represents the positive class and the sink represents the negative

220
Semisupervised Learning for Computational Linguistics
class. The source and sink are connected to the labeled instances. The source
is connected to each positively labeled instance, and the sink is connected
to each negatively labeled instance. Those edges represent the labeling, and
they all have inﬁnite weight. Edges connecting one instance to another have
ﬁnite weight; this guarantees that the mincut includes only edges connecting
instances, not edges representing labeling.
10.7
Bibliographic notes
This chapter, particularly the sections on harmonic functions, random walks,
and ﬂuids, relies heavily on the excellent book Random Walks and Electric
Networks by Doyle & Snell [79]. The presentation of the label propagation
algorithm (section 10.1) follows Zhu [244]. The Ford-Fulkerson algorithm is
described in standard introductions to algorithms such as Cormen [51] or
Sedgewick [205].

11
Mathematics for Spectral Methods
11.1
Some basic concepts
11.1.1
The norm of a vector
A vector is deﬁned by a direction and a length. Fixing the length r and
allowing the direction to vary generates a family of vectors that describe a
sphere of radius r, centered at the origin. Of more interest for linear algebra is
the family of vectors that have the same direction, but vary in length. The unit
vector x in the given direction can be taken as the canonical representative
of the family. The other members of the family have the form rx for some
scalar r, which is incidentally the length of the vector in question.
The norm of a vector is simply its length. Usually, we deﬁne the length of
x to be
√x · x =
 X
i
x2
i
! 1
2
.
This is the Euclidean norm.
Other deﬁnitions of norm are possible.
For
example, the Euclidean norm is a member of the family of p-norms (P
i xp
i )1/p,
each value of p deﬁning a diﬀerent norm. Technically, a norm is any function
that satisﬁes the following three conditions:
f(x) ≥0
with equality iﬀx = 0
f(x + y) ≤f(x) + f(y)
(the triangle inequality)
f(αx) = |α|f(x)
for all scalars α.
(11.1)
The notation ||x|| represents the norm of x. We will limit our attention to the
Euclidean norm.
Normalizing a vector x means ﬁnding the unit vector that has the same
direction as x, which is
x
||x||.
This vector represents the direction of x.
221

222
Semisupervised Learning for Computational Linguistics
11.1.2
Matrices as linear operators
Usually one is trained to think about matrix multiplication in terms of the
individual entries of the product matrix: the (i, j)-th entry of AB is the dot
product of the i-th row of A with the j-th column of B. But it is often better
to consider the matrix-vector product as the more basic operation:
y = Ax =
h
a1 . . . an
i
x =
X
i
xiai.
Here y, x, and a1, . . . , an are all column vectors. The output vector y is a
linear combination of the columns ai, the coeﬃcients being the components
xi of x.
One can think of a matrix as a linear operator. The product Ax represents
the application of the operator A to the input vector x, and the output is
y = Ax. The matrix product AB arises as the composition of the operators
A and B. The i-th column of AB is Abi, where bi is the i-th column of B.
11.1.3
The column space
The set of all possible linear combinations P
i xiai of the columns of a matrix
A is called the column space of A. It is the range of outputs of A, treating A
as an operator. We also say that the column space is the space spanned by
the columns of A. The dimensionality of the column space is the rank of A.
A rank of one means that all the outputs lie in a line (they are all multiples
of one vector). A rank of two means that all the outputs lie in a plane, and
so on.
Suppose that a column of A, say ai, is a linear combination of the other
columns aj for j ̸= i. That column is said to be linearly dependent on the
other columns. If ai is linearly dependent on the other columns, then any
vector in the column space that we form using ai could just as easily be
formed by using the other columns directly, without using ai. In this sense,
ai is “dispensable” – it can be omitted from the matrix without changing the
column space. (Actually, it is never possible to identify a unique dispensable
column – if any column is dispensable, there is always more than one choice
of which one to eliminate. Said another way, linear dependence is a property
of a set of vectors, not an individual vector.) A matrix with no dispensable
columns is said to be of full column rank. A matrix has full column rank
just in case the number of columns is equal to the rank (which is equal to the
dimensionality of the column space).
The rank cannot exceed the number of rows or the number of columns,
whichever is less.
A matrix can have full column rank only if it is either
square or “tall and skinny” (more rows than columns). If it is “short and fat”
(more columns than rows) it cannot have full column rank.
The columns of the matrix form a basis, which is to say, a set of coordinate
axes, for the column space. This is true by deﬁnition: every vector in the

Mathematics for Spectral Methods
223
column space is a linear combination of the columns of the matrix, and a
set of vectors forms a basis just in case every vector in the space is a linear
combination of the basis vectors.
A basis is not generally unique. For example, a one-dimensional space is a
line and a two-dimensional space is a plane. Any nonzero vector that lies in a
given line provides a basis for the line. Any pair of independent (non-parallel)
nonzero vectors lying in a plane provides a basis for the plane. In general,
any set of n independent vectors provides a basis for an n-dimensional space.
Two vectors can be independent without being perpendicular. The set of
vectors forming a basis must be independent, but they need not be mutually
perpendicular. If they are mutually perpendicular, they form an orthogonal
basis. An orthogonal basis is what we usually have in mind when we think of
a set of coordinate axes. A matrix whose columns are mutually perpendicular
and of unit length is an orthonormal matrix. The columns of an orthonormal
matrix provide an orthogonal basis for its column space.
As an operator, an orthonormal matrix represents a rigid rotation. In par-
ticular, the operation of an orthonormal matrix preserves vector lengths, and
applying an orthonormal matrix to perpendicular vectors yields perpendicular
vectors. Both of those properties are consequences of a more general fact: the
operation of an orthonormal matrix preserves dot products. Writing mi for
the i-th column of M, and for any vectors x and y, we have
Mx · My =
 X
i
ximi
!
·

X
j
yjmj


=
X
ij
xiyj(mi · mj)
=
X
i
xiyi(mi · mi) +
X
i̸=j
xiyj(mi · mj)
= x · y.
The last line follows because mi ·mi = 1 for all i, and mi ·mj = 0 for j ̸= i. If
x and y are perpendicular (their dot product is zero), it follows immediately
that Mx and My are perpendicular, and vice versa. Also, taking y = x, we
deduce that the operation of M preserves vector length:
||Mx||2 = Mx · Mx = x · x = ||x||2.
In the same way that two vectors can be perpendicular, two spaces can
also be perpendicular. Consider an orthonormal matrix M whose columns
provide an orthogonal basis for an n-dimensional space. Let us choose k of
the n columns.
The chosen columns deﬁne a k-dimensional space A, and
the remaining columns deﬁne an (n −k)-dimensional space B. Consider an
arbitrary vector a ∈A and an arbitrary vector b ∈B. The vector a represents
a linear combination of the ﬁrst k columns of M, meaning that a can be

224
Semisupervised Learning for Computational Linguistics
written as Mx where the last n −k components of x are zero. Similarly,
b can be written as My where the ﬁrst k components of y are zero. Since
the dot product of x and y is zero, they are perpendicular; and since M is
orthonormal, it follows that a = Mx and b = My are perpendicular. But
a and b were chosen arbitrarily, so it follows that every vector in the space
A is perpendicular to every vector in the space B. A and B are orthogonal
spaces. Taking the n-dimensional column space of M as the universe, B is
the orthogonal complement of A:
B = A⊥.
(11.2)
For a concrete example, consider a 3×3 matrix whose columns are indepen-
dent but not necessarily perpendicular. Its ﬁrst two columns deﬁne a plane
(that is subspace A) and its third column deﬁnes a line intersecting the plane
(that is subspace B). In general, the line forms an oblique angle to the plane.
In that case, a vector b in the line has a nonzero dot product with vectors in
the plane: there is a nonzero component of b that is oriented in the direction
of the plane. The special case in which the line is perpendicular to the plane
is the case in which A and B are orthogonal subspaces.
11.2
Eigenvalues and eigenvectors
We have already encountered eigenvalues and eigenvectors, along with the
concept of diagonalization, but we give a more systematic and detailed devel-
opment here.
11.2.1
Deﬁnition of eigenvalues and eigenvectors
A natural question to ask about an operator is whether it has ﬁxed points,
that is, inputs that are left unchanged by the operation. Now since a vector
can be thought of as the combination of a direction and a magnitude, a matrix
actually has two eﬀects: it scales the vector (changes its length) and rotates
it (changes its direction). For matrices, the question of a ﬁxed point turns
out to be less interesting than a somewhat weaker question: are there vector
directions that are left unchanged by the operation of the matrix?
Two vectors share the same direction if one is a scalar multiple of the other,
that is, if v = λu for some scalar λ. An eigenvector of a matrix A is a vector
u whose direction is not changed by the operation of the matrix:
Au = λu.
(11.3)
The number λ is the eigenvalue corresponding to the eigenvector u.

Mathematics for Spectral Methods
225
To emphasize, only the direction of an eigenvector matters, not its length.
If u is an eigenvector with eigenvalue λ, then so is cu for any nonzero constant
c:
A(cu) = cAu = cλu = λ(cu).
(11.4)
An eigenvector is actually a family of vectors sharing the same direction.
However, the eigenvalue λ is ﬁxed. Notice that, when we change the length
of the eigenvector by choosing a diﬀerent value for c in (11.4), the eigenvalue
λ does not change. To say it another way, we cannot eliminate λ by adjusting
the length of u.
There is one property of eigenvalues that one needs to be aware of: they
may be imaginary or complex, even if the original matrix contains only real
numbers.
11.2.2
Diagonalization
Consider a matrix A with eigenvalues λ1, . . . , λn and corresponding eigenvec-
tors u1, . . . , un. We assume A to have full column rank. Deﬁne S to be a
matrix whose columns are the eigenvectors:
S =
h
u1 . . . un
i
.
Since the columns of S are the eigenvectors of A, premultiplying any column
of S by A is the same as multiplying by the corresponding eigenvalue. That
is,
AS =
h
Au1 . . . Aun
i
=
h
λ1u1 . . . λnun
i
=
h
u1 . . . un
i


λ1
...
λn

.
Let us write Λ for the latter diagonal matrix containing eigenvalues. We have
just derived
AS = SΛ.
Postmultiplying by S−1 yields
A = SΛS−1.
(11.5)
Factoring A into this form – the matrix of eigenvectors, a diagonal matrix
of eigenvalues, and the inverse of the eigenvector matrix – is called diagonal-
ization. It shows how any matrix can be factored into its eigenvectors and
eigenvalues.
The factorization A = SΛS−1 is actually not unique. Remember that any
multiple of an eigenvector is also an eigenvector, as discussed in connection
with equation (11.4). We can scale the lengths of the eigenvectors any way
we like (as long as we do not set them to zero), and we still have A = SΛS−1.

226
Semisupervised Learning for Computational Linguistics
Diagonalization shows that S and A have the same column space. To see
this, let ai be the i-th column of A and let xi be the i-th column of the matrix
ΛS−1. We can write (11.5) as
h
a1 . . . an
i
= S
h
x1 . . . xn
i
.
This says that each vector ai is a linear combination of the columns of S.
(Namely, ai = Sxi.) Since every column of A is a linear combination of the
columns of S, every vector in the column space of A (every linear combination
of the columns of A) is also in the column space of S. The reverse implication
follows from ASΛ−1 = S, which is
h
u1 . . . un
i
= A
h
y1 . . . yn
i
where ui is the i-th column of S and yi is the i-th column of SΛ−1.
11.2.3
Orthogonal diagonalization
When A is a symmetric matrix, the matrix S in the diagonalization (11.5)
can be chosen to be orthonormal. We have noted the fact previously; we will
show now why it is true, at least in the case when all eigenvalues of A are
distinct. For the more general case the reader is referred to a text on linear
algebra, for example Strang [213], pp. 318ﬀ.
First, when A is symmetric, we can show that the columns of S (the eigen-
vectors of A) are perpendicular. Let ui and uj be any two eigenvectors with
eigenvalues λi ̸= λj. Then:
λiuT
i uj = (Aui)Tuj
= uT
i ATuj
= uT
i Auj
by symmetry of A
= λjuT
i uj.
Since λi ̸= λj, the only way to have λiuT
i uj = λjuT
i uj is if uT
i uj = 0. That
is, the eigenvectors are orthogonal.
Second, we recall that any multiple of an eigenvector is also an eigenvec-
tor, as discussed in connection with equation (11.4). In particular, we can
normalize the columns of S without aﬀecting diagonalization. That is, deﬁne
a matrix Q whose columns are proportional to the columns of S, but their
length (as vectors) is one. Then AQ is
A
h
u1
||u1|| . . .
un
||un||
i
=
h
λ1 u1
||u1|| . . . λn un
||un||
i
= QΛ.
Hence
A = QΛQ−1.

Mathematics for Spectral Methods
227
Because the columns of Q have length one, taking the dot product of a column
with itself yields value one. Because any two distinct columns of Q are perpen-
dicular, the dot product of two distinct columns has value zero. Succinctly,
QTQ = I, the identity matrix. Postmultiplying by Q−1 yields QT = Q−1. In
short, if A is symmetric, its diagonalization can be expressed as
A = QΛQT
(11.6)
with Q orthonormal.
Orthogonal diagonalization is signiﬁcant because, as we noted previously,
A and Q have the same column space. Hence the columns of Q provide a
basis for the column space of A that has the properties we are accustomed
to in coordinate axes: the basis vectors are all unit length and mutually
perpendicular.
In addition, we have seen that the operation of an orthonormal matrix like
Q is a rigid rotation. The operation of a diagonal matrix like Λ is a scaling
without rotation: if xi is a coordinate axis of the input space, having 1 in the
i-th position and 0 everywhere else, then Λxi = λixi. That is, Λ stretches
the axes of the input space, but does not change their directions.
Hence
A = QΛQT decomposes the action of A into a scaling Λ and a rigid rotation
Q. This is only the beginning of the story, though, as we will see shortly.
11.3
Eigenvalues and the scaling eﬀects of a matrix
11.3.1
Matrix norms
We discussed vector norms earlier. Norms are also deﬁned for matrices, and
they must satisfy the same three properties (11.1), replacing vectors x and y
with matrices X and Y . The norm of particular interest is again the Euclidean
norm. As already mentioned, the operation of a matrix can be viewed as a
combination of a scaling eﬀect and a rotation eﬀect. The Euclidean norm is
the maximum scaling eﬀect. A formal deﬁnition requires a little explanation.
A matrix’s scaling eﬀect is measured by the ratio of the output and input
vector lengths. We deﬁne the scaling function of matrix A to be
ρ(x) = ||Ax||
||x|| .
(11.7)
The scaling function ρ(x) is sensitive to the orientation of the input vector
x, but not its length. The sensitivity of ρ to orientation is easily seen by
considering the example
A =

1 0
0 0


x1 =

1
0


x2 =

0
1

.

228
Semisupervised Learning for Computational Linguistics
The vectors x1 and x2 diﬀer in direction but not length. (They both have
length one.) The scaling eﬀect of A is sensitive to that diﬀerence: A leaves x1’s
length unchanged (||Ax1|| = 1) but reduces x2’s length to zero (||Ax2|| = 0).
But the scaling function is not sensitive to the original length of the input
vector.
Consider any two vectors x1 and x2 with the same direction but
diﬀerent lengths. Let c be the ratio of their lengths, that is, let x2 = cx1.
The eﬀect of A on x2 is
ρ(x2) = ||A(cx1)||
||cx1||
= c||Ax1||
c||x1|| = ρ(x1).
All vectors in a family of vectors sharing the same direction are scaled in the
same way by A.
When we say that the Euclidean norm ||A|| of a matrix A is its maximum
scaling eﬀect, we mean that
||A|| = max
x̸=0
||Ax||
||x|| .
(11.8)
Since all vectors that share orientation are scaled the same way, we can take
the unit vector with a given orientation as the canonical representative of its
family. That is, (11.8) is equivalent to
||A|| = max
||x||=1 ||Ax||.
(11.9)
Note that the deﬁnandum, ||A||, is an array norm, and it is being deﬁned in
terms of ||Ax||, a vector norm.
The next question is what we can say about the orientation that maximizes
the scaling eﬀect of A.
11.3.2
The Rayleigh quotient
Since the scale factor ρ is a ratio of nonnegative numbers (lengths are never
negative!), it is monotonically related to its square:
R(x) = ρ2(x) = ||Ax||2
||x||2
= (Ax)T(Ax)
xTx
= xTATAx
xTx
.
(11.10)
The ratio xTATAx/xTx is known as the Rayleigh quotient.
The vector direction that maximizes the Rayleigh quotient of ATA is the
same one that maximizes the scaling function of A. The value of the scaling
function in that direction is the norm of A.
(It should be clear that the
Rayleigh quotient, like the matrix norm, is insensitive to the length of x.) To
learn something about the vector that maximizes R, let us take its derivative
and set it to zero. Using the rules developed in section 5.2.3, the derivative is
DxR = xTATAxDx(xTx)−1 + (xTx)−1DxxTATAx

Mathematics for Spectral Methods
229
= −xTATAx(xTx)−2DxxTx + (xTx)−1xT(ATA + (ATA)T)Dxx
= −xTATAx(xTx)−2(2xT) + (xTx)−1xT(2ATA)
=
2
(xTx)2
 ||x||2xTATA −||Ax||2xT
.
Setting this expression to zero, and transposing both sides in order to convert
the row vectors into column vectors, we obtain:
ATAx = ||Ax||2
||x||2 x.
(11.11)
What we learn is that the critical points of the Rayleigh quotient, and hence
the critical points of the scaling function, are eigenvectors of ATA, whose
corresponding eigenvalues have the form of the Rayleigh quotient itself. This
can be strengthened to a biconditional. Consider an arbitrary eigenvalue λ of
ATA:
ATAx = λx.
Premultiplying both sides by xT yields
xTATAx = λxTx
λ = ||Ax||2
||x||2 .
The eigenvalues of ATA all satisfy the Rayleigh quotient of ATA, and they
are the only vectors that do so.
We have already observed that the value of the Rayleigh quotient R(x) is
insensitive to the length of x, and we recall from the discussion surrounding
equation (11.4) that the property of being an eigenvector with a given eigen-
value λ is insensitive to length. Hence, without loss of generality, we can take
the vector x in (11.11) to be a unit vector, and we can write:
ATAx = ||Ax||2x
if and only if x is a critical point of the Rayleigh quotient.
One of the (unit-length) vectors that satisfy (11.11) maximizes the Rayleigh
quotient, and hence also maximizes the scaling function ρ; call that vector z.
Because z maximizes ρ, it follows by deﬁnition of the matrix norm (11.8) that
||Az|| = ||A||, so we can write:
ATAz = ||A||2z.
(11.12)
This tells us that the squared norm of matrix A is the eigenvalue of ATA with
the largest absolute value.
Since the vectors satisfying (11.11) are the critical points of the Rayleigh
quotient, one of them also minimizes the Rayleigh quotient. It will turn out
to be u2, the eigenvector corresponding to the second smallest eigenvalue of

230
Semisupervised Learning for Computational Linguistics
u 2
u 1
x
y
λ u
2
2
λ u
1
1
FIGURE 11.1
Eigenvectors of a 2 × 2 matrix.
ATA. That will bring us back to the mincut problem. Before we go there,
however, let us look further at the geometry of eigenvectors and the scaling
function.
We began the discussion of matrix norms by deﬁning the norm of a matrix
as its maximum scaling eﬀect. We also pointed out that the scaling function
is sensitive to the direction but not to the original magnitude of the scaled
vector. Now (11.12) shows that the maximum squared scaling eﬀect ||A||2 is an
eigenvalue of ATA, and the direction of maximum scaling is the corresponding
eigenvector.
11.3.3
The 2 × 2 case
Some geometric intuition can be obtained as follows. Let us consider a sym-
metric 2 × 2 matrix A with two eigenvectors u1 and u2. We can assume,
without loss of generality, that the eigenvectors u1 and u2 are unit vectors,
and the symmetry of A implies that u1 and u2 are perpendicular. Let the
vector x represent an arbitrary input direction, and let the vector y = Ax
be the result of applying the matrix A to x. These four vectors are shown in
ﬁgure 11.1. Since we are interested only in the scaling eﬀect of A, we take
x to be a unit vector. The output vector y is not generally a unit vector;
indeed, the scaling eﬀect of A on x is
ρ(x) = ||Ax||
||x|| = ||y||.
(11.13)
As orthogonal unit vectors, u1 and u2 can be viewed as axes of a coor-
dinate space. The input vector x can be represented by its coordinates in
“eigenspace”:
x = (cos θ)u1 + (sin θ)u2

Mathematics for Spectral Methods
231
where θ is the angle between x and u1. The value cos θ is the length of the
projection of x onto the u1 axis, and sin θ is the length of its projection onto
the u2 axis. The dotted lines from the end of x in ﬁgure 11.1 represent those
projections. The coordinates of x in the eigenspace constitute a new vector
x′:
x′ =

cos θ
sin θ

.
Deﬁning Q to be a matrix whose columns are u1 and u2, notice that
Qx′ =
h
u1 u2
i

cos θ
sin θ

= (cos θ)u1 + (sin θ)u2 = x.
That is, the conversion from x to x′ can be represented as x′ = Q−1x.
We can compute y in terms of this decomposition:
y = Ax = A[(cos θ)u1 + (sin θ)u2] = (cos θ)λ1u1 + (sin θ)λ2u2.
In eigenspace coordinates, y corresponds to the vector y′:
y′ =

λ1 cos θ
λ2 sin θ

=

λ1 0
0 λ2

x′.
When we apply A to vector u1, the result is λ1u1, and Au2 is λ2u2. The
coordinates of x on the eigenspace axes, namely cos θ on the u1 axis and
sin θ on the u2 axis, are scaled similarly by A, yielding y′ = (λ1x′
1, λ2x′
2).
Projecting back from the eigenspace coordinates to the original coordinates
yields y. Those reverse projections are the dotted lines converging on the end
of y in the ﬁgure. They can also be represented by a matrix multiplication:
y = (λ1 cos θ)u1 + (λ2 sin θ)u2 = Qy′.
The scaling eﬀect of A on u1 and u2 is obviously λ1 and λ2, respectively.
Assume that they are not identical; suppose λ1 < λ2, as in the ﬁgure. Let us
compute the scaling eﬀect of A on x. We saw in equation (11.13) that ρ(x)
is simply the length of y:
ρ2(x) = ||y||2 = ||y′||2 = (cos2 θ)λ2
1 + (sin2 θ)λ2
2.
Since cos2 θ and sin2 θ are nonnegative and cos2 θ + sin2 θ = 1, we observe
that ρ2(x) is a weighted average, which means that ρ2(x) lies between λ2
1 and
λ2
2. That is, recalling the expression for ρ2 given in (11.10):
λ2
1 ≤xTATAx
xTx
≤λ2
2.
(11.14)

232
Semisupervised Learning for Computational Linguistics
u
u
2
1
x
y
FIGURE 11.2
Non-orthogonal eigenvectors. Input vector x is projected onto u1 and u2,
yielding coordinates x′
1 ≈1 and x′
2 ≈1 on the u1 and u2 “axes.”
The
eigenvectors are stretched by the operation of matrix A, and the coordinates
in the stretched system, λ1x′
1 ≈λ1 and λ2x′
2 ≈λ2, are projected back (the
dotted lines) to determine the location of y. In this case, y is longer than
either of the stretched eigenvectors.
Hence |λ1| ≤|ρ(x)| ≤|λ2|.
In short, for a symmetric 2 × 2 matrix, the largest scaling eﬀect is in the
direction of the eigenvector with the largest eigenvalue, and the smallest scal-
ing eﬀect is in the direction of the other eigenvector. All other scaling eﬀects
are mixtures of those two.
We mention in passing that the assumption of symmetry is important.
Without symmetry, the eigenvectors may fail to be orthogonal, and if the
eigenvectors are not orthogonal, the scaling factor for x may actually exceed
|λ2|. Figure 11.2 illustrates.
11.3.4
The general case
The discussion generalizes readily to multiple dimensions. Assuming that A
is symmetric, it can be expressed as
A = QΛQT
(11.15)
with Q orthonormal, as in (11.6). The application of A to an arbitrary unit
vector x takes the form:
y = QΛQTx.
Though it may not be immediately obvious, this decomposition reﬂects ex-
actly the steps we just went through in the two-dimensional case. Working

Mathematics for Spectral Methods
233
from right to left, we ﬁrst express x as a linear combination of the eigenvec-
tors; this yields its representation in the coordinate space whose axes are the
eigenvectors:
x = Qx′
which is to say
x′ = QTx
since Q−1 = QT for orthonormal matrices. Next we scale each axis by the
corresponding eigenvalue:
y′ = Λx′ = ΛQTx.
Finally, we also represent y as a linear combination of eigenvectors:
y = Qy′.
Applying Q to y′ converts it back to the original coordinate space, yielding
y:
y = QΛQTx = Ax.
Now let us order the eigenvalues by increasing squared value:
λ2
1 ≤λ2
2 ≤. . . ≤λ2
n
(11.16)
and let us order the eigenvectors u1, . . . , un accordingly. Let the ﬁrst two
eigenvectors u1 and u2 be the eigenvectors in the two-dimensional space of
ﬁgure 11.1. Let us call this space Q2; it is the plane spanned by the ﬁrst two
eigenvectors. The discussion of the previous section (section 11.3.3) leads to
the following conclusions. Any unit vector x in the plane Q2 can be expressed
as a linear combination of u1 and u2, and the squared scaling eﬀect of A on
x is a weighted average of its squared scaling eﬀects on u1 and u2. Hence
λ2
1 ≤xTATAx
xTx
≤λ2
2.
Now suppose we add the third eigenvector u3 to the space. The resulting
eigenspace Q3 has three dimensions instead of two, but otherwise nothing
changes. Let x be an arbitrary vector in Q3, and let x′ represent its coordi-
nates in Q3:
x′ =


x′
1
x′
2
x′
3


where x′
i is the length of the projection of x onto ui. The squared scaling
eﬀect is the length of Ax, namely:
ρ2(x) = ||y||2 = (x′
1)2λ2
1 + (x′
2)2λ2
2 + (x′
3)2λ2
3.
(11.17)

234
Semisupervised Learning for Computational Linguistics
The coeﬃcients (x′
i)2 in (11.17) are obviously nonnegative, and their sum is
the squared length of x′, that is, they sum to one. Hence ρ2(x) is a weighted
average of the squared eigenvalues. Since λ2
3 is the largest of the values being
averaged, we know that
ρ2(x) = xTATAx
xTx
≤λ2
3
for any vector x in the space Q3 spanned by the ﬁrst 3 eigenvectors. We have
equality if we take x equal to u3, so
max
x∈Q3
xTATAx
xTx
= λ2
3.
Applying this reasoning recursively, we conclude that
max
x∈Qk
xTATAx
xTx
= λ2
k
arg max
x∈Qk
xTATAx
xTx
= uk.
(11.18)
We can also make a statement about minima. Let us consider the subspace
spanned by the eigenvectors excluding the ﬁrst k, which is to say, Q⊥
k (11.2).
That is, Q⊥
n−1 is the line spanned by un, Q⊥
n−2 is the plane spanned by un−1
and un, and so on. In general, the dimensionality of the space Q⊥
k is n −k.
The eigenvectors spanning Q⊥
k correspond to the n −k eigenvalues with the
largest squared values: λk+1, . . . , λn. Clearly, if we restrict attention to x in
the space Q⊥
k , the squared scaling eﬀect ρ2(x) is minimized by choosing the
smallest available eigenvector direction. That is
min
x∈Q⊥
k−1
xTATAx
xTx
= λ2
k
arg
min
x∈Q⊥
k−1
xTATAx
xTx
= uk.
(11.19)
The equations use Q⊥
k−1 instead of Q⊥
k in order to have k range from 1 to n,
for consistency with equations (11.18).
11.3.5
The Courant-Fischer minimax theorem
One further simpliﬁcation of (11.18) and (11.19) is possible. Continuing to
assume that A is symmetric and hence has the decomposition QΛQT, we
observe that
A2 = (QΛQT)(QΛQT) = QΛ2QT
since QTQ = I. This is useful because Λ2 has a particularly simple form.
Being a diagonal matrix, it is squared elementwise:
Λ2 =


λ2
1
...
λ2
n

.

Mathematics for Spectral Methods
235
Since the columns of Q are eigenvectors and the entries of Λ are eigenvalues,
we see that A2 has the same eigenvectors as A, and that the eigenvalues of
A2 are the squares of the eigenvalues of A.
We can also take the square root of A in the same fashion:
A1/2 = QΛ1/2QT
(11.20)
where Λ1/2 is a diagonal matrix whose i-th entry is √λi. We conﬁrm
A1/2A1/2 = QΛ1/2QTQΛ1/2QT = QΛQT = A.
The equation (11.18) is true of any matrix that can be decomposed in the
form (11.15), with Q orthonormal and Λ diagonal. A1/2 does have such a
decomposition, though a subtlety must be pointed out.
The factorization
(11.15) is assured for symmetric real matrices, but if any of the eigenvalues
in Λ are negative, then Λ1/2 contains imaginary values. However, we do not
need to show that A1/2 is symmetric and real in order to know that it has the
desired factorization: we already know that it satisﬁes (11.20), and we know
independently that Q is orthonormal. Hence A1/2 does have a factorization
of the required form, and equation (11.18) applies to A1/2.
Since A and A1/2 have the same eigenvectors, the eigenspaces Qk are the
same for both, but we replace the eigenvalues λk with the eigenvalues of A1/2,
namely, √λk:
max
x∈Qk
xT(A1/2)T(A1/2)x
xTx
= (
p
λk)2.
That is, for 1 ≤k ≤n,
max
x∈Qk
xTAx
xTx = λk
arg max
x∈Qk
xTAx
xTx = uk.
(11.21)
Applying the same reasoning to (11.19) yields, for 0 ≤k ≤n −1,
min
x∈Q⊥
k
xTAx
xTx = λk+1
arg min
x∈Q⊥
k
xTAx
xTx = uk+1.
(11.22)
Finally, recall that we ordered the eigenvalues by increasing squared value
(11.16). With the replacement of √λk for λk, the deﬁnition of the ordering
becomes
(
√
λ1)2 ≤(
√
λ2)2 ≤. . . ≤(
√
λn)2.
That is, we now order eigenvalues simply by increasing size:
λ1 ≤λ2 ≤. . . ≤λn.
This ordering is valid even if some of the eigenvalues are negative. If so, some
of the values
√
λk are imaginary, but we still have (
√
λk)2 = λk.

236
Semisupervised Learning for Computational Linguistics
The equations (11.21) and (11.22) represent a special case of the Courant-
Fischer minimax theorem. The general theorem states that (11.22) is true
not just for the space Q⊥
k , but for any space with the same dimensionality
as Q⊥
k , hence in particular for the space that maximizes the value on the
left-hand side of (11.22):
max
S|dim(S)=n−k min
x∈S
xTAx
xTx = λk+1.
The equation (11.21) can be similarly generalized. A proof for the general
case can be found in Golub & Loan [97], p. 394.
11.4
Bibliographic notes
For the reader who really has no more background than I have assumed, I
cannot recommend Strang [213] highly enough. For discussion of more ad-
vanced topics, including matrix norms and the Courant-Fischer theorem, see
Horn & Johnson [113] and Golub & Loan [97].

12
Spectral Methods
At ﬁrst glance, it is not at all obvious what form spectral methods for learning
might take. The connection between spectra and semisupervised learning can
perhaps most readily be seen by thinking back to label propagation, and
speciﬁcally our picture of label propagation as doing an interpolation of label
information from boundaries across the interior of the graph. With spectral
methods, the idea is to construct an interpolation in the form of a “standing
wave.”
Accordingly, we begin with a discussion of wave-shaped functions,
speciﬁcally harmonics (not to be confused with the harmonic functions of
chapter 10) that arise from simple harmonic motion.
12.1
Simple harmonic motion
12.1.1
Harmonics
Most readers will be familiar with the idea that a musical tone is a mixture
of harmonics. The perceived pitch is the fundamental frequency, and the
timbre – what makes a violin playing a particular note sound diﬀerent from
a ﬂute playing the same note – is determined by the relative strengths of
overtones.
The perception of pitch and overtones is possible because the
ear breaks the sound into its component frequencies. Similarly, the idea that
white light is a mixture of frequencies, and is broken into its spectrum by a
prism, is surely also familiar. A spectrum is the same as timbre – it is an
association of coeﬃcients with frequencies, specifying the relative weight of
diﬀerent frequencies in the mix.
A “pure tone” consisting of a single frequency is represented by a harmonic.
Technically, a harmonic is any function
y = A sin(ωx + ψ).
(12.1)
The parameters of a harmonic are its amplitude A, its phase ψ, and the
angular velocity ω. Angular velocity is measured in radians per unit time,
and is related to frequency (cycles per unit time) by
F = ω
2π .
237

238
Semisupervised Learning for Computational Linguistics
T
A
FIGURE 12.1
A harmonic. A is amplitude, ψ is (sine) phase, and T is period.
The inverse of frequency is the period:
T = 2π
ω .
See ﬁgure 12.1 for the graphical correlates of amplitude, phase, and period.
Circular motion and oscillating linear motion are two simple physical sys-
tems that are described by a harmonic. Suppose a particle is moving counter-
clockwise around a circle of radius A. The circular motion can be (physically)
translated to oscillating linear motion by replacing the particle with a peg on
an arm, inserted into a slot in a horizontal bar that is constrained to move
only up and down (ﬁgure 12.2). Let θ represent the angular location of the
particle, that is, the angle it forms with the horizontal axis. If the particle
moves around the unit circle, the angle θ in radians is the distance it has
traveled; for a circle of radius A, the distance traveled is Aθ. The vertical
position of the particle is y = A sin θ. Let x represent time. If the particle
starts at angle θ0 at time zero, and travels at angular velocity ω radians per
second, then its angular position at time x is θ0 +ωx, and its vertical position
is
y = A sin(ωx + θ0).
This is a harmonic with amplitude A, angular velocity ω, and phase θ0. The
particle completes one revolution at the time T such that ωT = 2π. That is,
the time required for one revolution of the physical system (2π/ω) is equal
to the period of the harmonic. If x is time in seconds, the frequency of the
harmonic is the number of revolutions per second.
Since cos(θ) = sin(θ + π/2), the harmonic (12.1) can be expressed equiva-
lently as (12.2):
y = A cos(ωx + φ)
(12.2)

Spectral Methods
239
y
A
FIGURE 12.2
Circular motion and oscillating linear motion.
The horizontal bar is con-
strained to move only up and down.
where
φ = ψ −π/2.
Observe that φ plays the same role in (12.2) as ψ does in (12.1). For that
reason, φ is called the cosine phase of the harmonic, and ψ is more speciﬁcally
called the sine phase to distinguish it from cosine phase. Since (12.1) and
(12.2) are equivalent, the choice between them is a matter of convenience.
Note that sin ψ = cos φ.
12.1.2
Mixtures of harmonics
Applying the addition formula for sine to (12.1), we obtain:
y = A cos ωx sin ψ + A sin ωx cos ψ.
Writing
a = A sin ψ
b = A cos ψ
the harmonic (12.1) can be expressed as
y = a cos ωx + b sin ωx.
(12.3)
In this form, we see that the harmonic is the dot product of two vectors:
y = u · v =

A sin ψ
A cos ψ


T 
cos ωx
sin ωx

.
This representation is convenient because it separates frequency from phase
and amplitude. The vector u depends only on phase ψ and amplitude A,

240
Semisupervised Learning for Computational Linguistics
and v depends only on frequency F = ω/(2π), not on phase or amplitude.
The vector v can be thought of as a harmonic in standard form (amplitude
one and phase zero), and u is a coeﬃcient vector providing the phase and
amplitude. For lack of a better term, I will call u the “phase-amplitude” of
the harmonic.
We already noted that sound waves and light waves consist of mixtures of
harmonics. Remarkably, that turns out to be true not only for sound and light
waves, but for any piecewise continuous function that is either periodic with
period T, or else deﬁned only on a bounded interval [0, T]. Unfortunately,
we will not be able to prove the theorem here – the reader is referred to
more specialized works, such as that of Tolstov [225], for a proof. Omitting
some details, the theorem states that any piecewise continuous function f(x)
deﬁned on a bounded interval [0, T] can be expressed as an inﬁnite sum of
harmonics:
f(x) = a0
2 +
∞
X
k=1

ak cos k 2π
T x + bk sin k 2π
T x

.
(12.4)
That is, for any given function f, there exists a choice of coeﬃcients (a0, a1,
b1, a2, b2, . . .) that makes (12.4) true. The coeﬃcient a0 sets the baseline.
The remaining coeﬃcients come in pairs (ak, bk) that represent the phase-
amplitudes of the component harmonics. Observe that the expression inside
the summation in (12.4) is a harmonic in the form (12.3), where ωk = 2πk/T.
The frequency of the k-th harmonic is
Fk = ωk
2π = k
T .
The value F1 = 1/T represents the fundamental frequency, and the frequen-
cies of the harmonics are integer multiples of the fundamental. (The more
common notation for fundamental frequency is “F0,” but we will prefer con-
sistency over convention and write “F1.”) The spectrum of the function f
is the list of coeﬃcients:
(a0, a1, b1, a2, b2, . . .).
The k-th pair (ak, bk) represents the weight, in phase-amplitude form, of the
k-th harmonic in the weighted sum (12.4).
A more convenient form is obtained by deﬁning
t = 2π
T x.
Then (12.4) can be written as
f(t) = a0
2 +
∞
X
k=1
(ak cos kt + bk sin kt) .
(12.5)

Spectral Methods
241
-1
-0.5
 0
 0.5
 1
 0
 1
 2
 3
 4
 5
 6
-1
-0.5
 0
 0.5
 1
 0
 1
 2
 3
 4
 5
 6
(a)
(b)
FIGURE 12.3
Approximation of a piecewise linear function by a sum of sines: (a) ﬁrst ten
terms of the Fourier series, (b) ﬁrst 100 terms.
This is known as the Fourier expansion of the function.
An impression of how an arbitrary function can be represented as a sum of
harmonics can be obtained by considering the examples in ﬁgure 12.3. The
plots show the series
fn(x) =
n
X
k=1
1
k sin kx
for n = 10 and n = 100. These are the ﬁrst n terms of a Fourier series with
coeﬃcients a0 = ak = 0, bk = 1/k. The limit is a step function.
12.1.3
An oscillating particle
Let us return to the physical example of oscillating linear motion. Imagine a
particle moving in one dimension, under the inﬂuence of an attractive force
at the origin. Let us assume that the force is an elastic force, behaving like
an ideal spring. Its magnitude is proportional to the amount of stretching,
the constant of proportionality being the elastic constant K. If the position
of the particle is y, the force exerted on it is
F = −Ky.
When the particle is above the origin (y > 0), the force is directed downward,
and when the particle is below the origin, the force is directed upward. The
magnitude of the force increases linearly as the particle moves away from the
origin.
By Newton’s second law of motion, force is equal to mass times acceleration,
and, by deﬁnition, acceleration is the second derivative of position with respect

242
Semisupervised Learning for Computational Linguistics
to time:
md2y
dt2 = −Ky.
That is
d2
dt2 y = −K
my.
(12.6)
This is a diﬀerential equation. To solve it, we require a function that is
proportional to its own second derivative. That property should bring the
exponential function immediately to mind:
d
dtCeλt = λCeλt
d2
dt2 Ceλt = λ2Ceλt
for any constants C and λ. This is exactly the form we require for (12.6),
with
y = Ceλt
λ2 = −K/m.
Since y = C when t = 0, let us rename C as y0. Since K and m are both
positive, λ =
p
−K/m is an imaginary number; let us replace λ with iω.
Here ω is an arbitrary value, but the choice of variable is not accidental; the
connection to the ω of (12.1) will emerge shortly. In sum, the solution to
(12.6) is
y = y0eiωt
where ω =
p
K/m.
(12.7)
The reader should conﬁrm that (12.7) implies (12.6).
The solution (12.7) is not fully general. The negative square root gives a
second solution: y = y0e−iωt. In fact, any linear combination of these two
solutions is also a solution:
y = Ceiωt + De−iωt
where ω =
p
K/m.
(12.8)
The function (12.8) satisﬁes (12.6) for any choice of C and D. When t = 0,
we have y0 = C + D.
Now other functions that are proportional to their own second derivative
lie near to hand: recall that the derivative of sine is cosine, and the derivative
of cosine is (the negative of) sine again. In fact, we can use harmonics more
generally. The form (12.3) will prove most convenient:
d
dt(a cos ωt + b sin ωt) = −ω(a sin ωt −b cos ωt)
d2
dt2 (a cos ωt + b sin ωt) = −ω2(a cos ωt + b sin ωt).
This has the form (12.6) with
y = a cos ωt + b sin ωt
ω =
p
K/m.
(12.9)

Spectral Methods
243
The function (12.9) is a solution for (12.6) for any choice of a and b.
It turns out to be no accident that sine, cosine, and exponential all have
the property of being proportional to their own second derivative. In fact, the
functions (12.8) and (12.9) are one and the same. This follows from Euler’s
formula:
eiθ = cos θ + i sin θ.
Here is the derivation. Immediate consequences of Euler’s formula are:
eiθ + e−iθ = 2 cos θ
eiθ −e−iθ = 2i sin θ.
Making the substitution θ = ωt and multiplying through by a/2 and b/2i,
respectively, yields
a
2eiωt + a
2e−iωt = a cos ωt
b
2ieiωt −b
2ie−iωt = b sin ωt.
Adding these two equations, we obtain
a cos ωt + b sin ωt =
a −ib
2

eiωt +
a + ib
2

e−iωt
(12.10)
which is to say, (12.9) is equal to (12.8) with
C = a −ib
2
D = a + ib
2
.
(12.11)
Conversely, given a solution in the form (12.8) with arbitrary complex con-
stants C and D, Euler’s formula gives us
Ceiωt + De−iωt = C cos ωt + iC sin ωt + D cos ωt −iD sin ωt
= (C + D) cos ωt + (iC −iD) sin ωt
which gives us a solution in the form (12.9) with
a = C + D
b = i(C −D).
12.1.4
A vibrating string
In the previous section, we considered a single particle moving in one dimen-
sion under the inﬂuence of a restoring force, which is to say, a force that pulls
the mass back toward the origin with magnitude proportional to distance
from the origin. We saw that such a physical system is described by a single
harmonic.
Now let us consider a physical system that gives rise to a mixture of har-
monics, namely, a vibrating string. Let us assume that the string has length

244
Semisupervised Learning for Computational Linguistics
(L,0)
y
x
FIGURE 12.4
The shape of a vibrating string is given by y = f(x). It is approximated by
a sequence of particles, connected by elastic forces, each particle constrained
to move only vertically.
L, and that both ends are ﬁxed. The value y represents displacement. For
convenience, let us take the string to be oriented in the direction of the x-
axis, with endpoints ﬁxed at the origin and (L, 0). Then the shape of the
string at a given point in time is simply the graph of a function f(x), with
the constraints that f(0) = f(L) = 0. (See ﬁgure 12.4.)
We can approximate the string by a sequence of particles connected by elas-
tic forces. We assume that the particles move only vertically, not horizontally.
Two forces are applied to any given particle, one from each neighbor. Since
the forces are elastic, the force imposed on a particle by its neighbor is
F = −Kℓ
where ℓis the distance between the particles (ﬁgure 12.5). The force is the
sum of a horizontal component −K∆x and a vertical component −K∆y. We
assume that the string is homogeneous, so that K is the same everywhere.
If we also assume that the particles are equally spaced horizontally, then ∆x
and hence K∆x is the same for both neighbors of a given particle, with the
consequence that the horizontal forces exerted by the two neighbors of a given
particle cancel out – if the initial velocities for all particles are purely vertical,
then they will remain purely vertical.
At any rate, we consider only the
vertical force components.
The net vertical force Fj applied to the j-th particle is
Fj = −K(yj −yj−1) −K(yj −yj+1)
= −K(−yj−1 + 2yj −yj+1)
(12.12)
where yj is the vertical position of the j-th particle.
If the neighbors are
lower than the particle, the force acting on the particle is directed downward

Spectral Methods
245
Q
P
-KDx
-KDy
-Kl
FIGURE 12.5
The force imposed on particle P by its neighbor Q.
(negative), and if the neighbors are higher, the force is positive. The ﬁrst and
last masses are exceptional. The two forces on the ﬁrst particle are applied
by the endpoint, ﬁxed at height 0, and by the second particle:
F1 = −Ky1 −K(y1 −y2)
= −K(2y1 −y2).
(12.13)
Similarly, for the last particle:
Fn = −K(yn −yn−1) −Kyn
= −K(−yn−1 + 2yn).
(12.14)
The entire string of forces can be represented as a vector F = (F1, . . . , Fn), and
the vertical positions as a vector y = (y1, . . . , yn). The relationship between
F and y is given by the system of equations (12.12), (12.13), and (12.14). It
can be stated concisely as
F = −KAy
where A is a matrix that has twos on the diagonal and negative ones in the
bands just above and below the diagonal. For example, for the case of ﬁve
particles, the equation is


F1
F2
F3
F4
F5


= −K


2 −1 0
0
0
−1 2 −1 0
0
0 −1 2 −1 0
0
0 −1 2 −1
0
0
0 −1 2




y1
y2
y3
y4
y5


.
(12.15)

246
Semisupervised Learning for Computational Linguistics
The value for Fj is −K times the dot product of the j-th row of A with the
vector of yj’s. For example,
F2 = −K(−1, 2, −1, 0, 0) · (y1, y2, y3, y4, y5)
in accordance with (12.12).
To determine how f(x) changes with time, we again apply Newton’s second
law of motion:
m d2
dt2 y = −KAy.
(12.16)
What is meant by d2y/dt2 is simply the vector of second derivatives
 d2
dt2 y1, . . . , d2
dt2 yn

.
Dividing (12.16) through by m gives us a system of diﬀerential equations of
the form (12.6).
Unfortunately, we cannot simply apply the solution that
we derived in the previous section for the one-particle case to each particle
separately: the particles interact.
The diﬀerential equation we wish to solve is
d2
dt2 y = −K
mAy.
(12.17)
Deﬁne B = −(K/m)A, and suppose that we can identify an eigenvector u of
B with eigenvalue λ. Then
y = ueiωt
where ω =
√
−λ
(12.18)
is a solution for (12.17). (Matrices of the form we will be concerned with
often have negative eigenvalues, so, despite appearances, ω is not necessarily
imaginary.) Let us conﬁrm that (12.18) is a solution to (12.17):
d
dty = iωueiωt
d2
dt2 y = −ω2ueiωt
= λueiωt
= Bueiωt.
The last step follows because λ is the eigenvalue associated with eigenvector
u of B, hence λu = Bu. Substituting y for ueiωt yields (12.17). QED.
We will make no use of the elastic constant K or the particle mass m, so
we will henceforth assume for convenience that
−K
m = 1.

Spectral Methods
247
The diﬀerential equation (12.17) becomes
d2
dt2 y = Ay
(12.19)
and in the solution (12.18), u is an eigenvector of A itself, with eigenvalue λ.
It is interesting to note that the solution (12.18) to the multi-particle case
is just like the solution (12.7) to the single-particle case, except that the
eigenvector u takes the place of the initial value y0.
As in the single-particle case, (12.18) is only one solution. First, as in the
single-particle case, the negative square root provides a second solution:
y = ue−iωt.
Second, there is in general more than one eigenvalue-eigenvector pair for a
matrix; in fact, there may be as many eigenvalues λ1, . . . , λn and correspond-
ing eigenvectors u1, . . . , un as there are columns in A. Each pair provides a
diﬀerent special solution – or rather, each pair provides a pair of solutions,
including both positive and negative square roots. The general solution is the
linear combination of the special solutions:
y =
n
X
k=1
 Ckukeiωkt + Dkuke−iωkt
where ωk = √−λk.
(12.20)
We can conﬁrm that (12.20) is a solution in much the same way as we con-
ﬁrmed that (12.18) is a solution; it is left as an exercise for the reader.
The expression inside the summation in (12.20) has the same form as (12.8).
So, recalling the discussion at the end of section 12.1.3, the family of solutions
(12.20) with arbitrary coeﬃcients Ck and Dk is equivalent to the following
family of solutions with arbitrary coeﬃcients ak and bk:
y =
n
X
k=1
uk(ak cos ωkt + bk sin ωkt)
where ωk = √−λk.
(12.21)
The expression inside the parentheses is a harmonic; let us call it ck:
ck(t) = ak cos(ωkt) + bk sin(ωkt)
for ωk = √−λk.
Its angular velocity ωk is determined by the matrix A, but its amplitude and
phase are the length and angle, respectively, of the vector (ak, bk). Equation
(12.21) becomes
y =
n
X
k=1
ck(t)uk.
(12.22)
The coeﬃcients ak and bk of the harmonic ck(t) are determined by the
initial positions and velocities of the particles. At time t = 0, we have
y =
n
X
k=1
akuk.

248
Semisupervised Learning for Computational Linguistics
That is, the coeﬃcients ak are chosen so as to match the shape of the string
at time zero. The particle velocities are
d
dty =
n
X
k=1
uk(bk cos ωkt −ak sin ωkt)
which at time t = 0 becomes
d
dty =
n
X
k=1
bkuk.
The coeﬃcients bk are chosen so as to match the initial velocities of the par-
ticles.
The vector equation (12.22) represents a system of equations:
yj =
n
X
k=1
ukjck(t).
(12.23)
It states that the motion of each particle individually is described by a mixture
of harmonics. It does not say that the shape of the string at any point in time
is described by a mixture of harmonics.
Nonetheless, it is true that the shape of the string is also a mixture of
harmonics.
Equation (12.22) says that, at any given time t, the shape of
the string y is a weighted sum of the eigenvectors uk. As we will show now,
the shape of the k-th eigenvector uk is a harmonic. Speciﬁcally, uk contains
the values of a sine harmonic with period 2L/k and phase zero, sampled at
points equally spaced in the interval (0, L) (ﬁgure 12.6). Note that a period
of 2L/k implies an angular velocity of kπ/L. That is, we claim that the k-th
eigenvector is deﬁned as
uk =


sin((kπ/L)x1)
...
sin((kπ/L)xn)


(12.24)
where
xj = j∆x
∆x =
L
n + 1
and the k-th eigenvalue is
λk = 2 −2 cos((kπ/L)∆x).
(12.25)
To verify our claim that the uk and λk thus deﬁned are eigenvectors and
eigenvalues of A, we must show that
Auk = λkuk

Spectral Methods
249
0
L
k=1
k=2
k=3
x 1 x 2
x n-1 x n
. . .
FIGURE 12.6
Eigenvector uk contains the values uj = sin((kπ/L)xj) with the xj distributed
uniformly in the interval (0, L). The period corresponding to angular velocity
kπ/L is 2L/k.
for all k, which we do by showing that it is true for a ﬁxed but arbitrary k.
To reduce clutter, we drop the “k” subscripts and write simply u and λ for
uk and λk. It will also be convenient to deﬁne ∆θ = kπ/(n + 1), so that
u =


sin(1∆θ)
...
sin(n∆θ)


λ = 2 −2 cos(∆θ).
We must show that the vectors Au and λu are elementwise equal, that is,
that
(Au)j = (λu)j
for all j.
Recalling the shape of the matrix A, given in (12.15), and writing uj for
the j-th element of u, we have
(Au)j = −uj−1 + 2uj −uj+1
for j ̸= 1, n
(Au)1 = 2u1 −u2
(Au)n = 2un −un−1.
We consider the cases in the order given, beginning with the middle elements
uj where j is neither 1 nor n.
We can expand uj−1 as
uj−1 = sin((j −1)∆θ) = sin(j∆θ) cos(∆θ) −cos(j∆θ) sin(∆θ).

250
Semisupervised Learning for Computational Linguistics
Similarly, uj+1 expands as
uj+1 = sin(j∆θ) cos(∆θ) + cos(j∆θ) sin(∆θ).
Their sum is
uj−1 + uj+1 = 2 sin(j∆θ) cos(∆θ) = 2uj cos(∆θ).
Hence
(Au)j = 2uj −2uj cos(∆θ) = (λu)j
as desired.
This leaves the ﬁrst and last elements. A preliminary fact:
u2 = sin(2∆θ) = 2 sin(∆θ) cos(∆θ) = 2u1 cos(∆θ).
(12.26)
Then the derivation for the ﬁrst element is straightforward:
(Au)1 = 2u1 −u2
= 2u1 −2u1 cos(∆θ)
= (λu)1.
As for the last element, observe that, for any k, the k-th harmonic crosses the
x-axis at xn+1, which equals L (ﬁgure 12.6). Hence, stepping backward from
xn+1 is equivalent to stepping forward from x0, except that there is a change
of sign when k is even. That is, deﬁning σ = +1 if k is odd and σ = −1 if k
is even,
un = σu1
un−1 = σu2.
Hence
(Au)n = 2un −un−1
= 2un −σu2
= 2un −2σu1 cos(∆θ)
by (12.26)
= 2un −2un cos(∆θ)
= (λu)n.
In short, the shape of the string at a ﬁxed time t is given by (12.22). It is a
mixture of eigenvectors, where each eigenvector is a sampled harmonic (12.24).
If we take the limit as the number of sample points n goes to inﬁnity, the
eigenvector uk becomes an eigenfunction fk. Instead of ukj = sin((kπ/L)xj)
at the discrete sample points xj, we have
fk(x) = sin((kπ/L)x)

Spectral Methods
251
continuously. The vector y of (12.22) also becomes a function, namely
f(x) =
∞
X
k=1
ck(t)fk(x).
This is the Fourier expansion of the function f – it has the form (12.4). For
this reason, our solution (12.22) is known as a discrete Fourier expansion.
Finally, the solution (12.22) can be rewritten in the form of a matrix times
a vector:
y =
X
k
ckuk
=
h
u1 . . . un
i


c1(t)
...
cn(t)


= Sc(t).
The last line deﬁnes the matrix S and vector c. S is a matrix whose columns
are the eigenvectors of A, and c is a vector whose elements are the coeﬃcients
ck determined by the initial positions and velocities of the particles.
The fact that the columns of S are the eigenvectors of A should call diago-
nalization to mind. Recall (section 11.2.2) that A can be expressed as
A = SΛS−1.
(12.27)
Substituting this expression into the original equation (12.17), we have
d2
dt2 y = Ay = SΛS−1Sc(t) = SΛc(t).
In short, the particle accelerations can be expressed as a product of a set of
harmonics S, their weights Λ, and a vector of harmonics c(t) whose parameters
are determined by initial conditions.
12.2
Spectra of matrices and graphs
The diagonalization (12.27) arose rather incidentally in the previous section,
but it plays a central role in the deﬁnition of the spectra of matrices and
graphs.

252
Semisupervised Learning for Computational Linguistics
12.2.1
The spectrum of a matrix
Let S continue to represent the matrix whose columns uk are the eigenvec-
tors of A. Deﬁne the row vector vT
k to be the k-th row of S−1. Then the
diagonalization (12.27) can be written as
A =
h
u1 . . . un
i


λ1
...
λn




vT
1
...
vT
n

.
This can be expressed as a sum of outer products:
A = λ1u1vT
1 + . . . + λnunvT
n.
The outer product ukvT
k is a matrix. Each column is uk multiplied by vk,
the k-th element of vT
k , hence each column of ukvT
k is a harmonic with the
same frequency and phase as uk, though possibly with a diﬀerent amplitude.
Hence it is natural to think of ukvT
k as a matrix analogue of a harmonic, and
A becomes a mixture of harmonics with weights λ1, . . . , λn. For this reason,
the spectrum of the matrix is deﬁned to be the list of its eigenvalues, in order
of increasing size
λ1 ≤. . . ≤λn.
When A is symmetric, as it was in (12.15), its diagonalization can be ex-
pressed as
A = QΛQT
with Q orthonormal, and the spectral decomposition is
A =
X
k
λkukuT
k .
In this form, the outer product ukuT
k is symmetric. Its k-th column is identical
to its k-th row, and both have the form ukkuk. It is a discrete sampling of
a two-dimensional surface. Each row and column represents an axis-parallel
cross section through the surface, and is the harmonic uk, scaled by a constant
ukk. The motivation for considering ukuT
k to be the matrix analogue of a
harmonic is compelling.
In the particular case of the matrix A of the previous section, the k-th
eigenvalue λk is equal to 2 −2 cos(kπ/(n + 1)); see equation (12.25). Since
cos(kπ/(n+1)) decreases monotonically from +1 at k = 0 to −1 at k = n+1,
we see that the eigenvalues λ1, . . . , λn are listed from smallest to largest.
The angular velocity of the k-th eigenvector uk is kπ/L, so the eigenvectors
u1, . . . , un are also sorted in order of increasing angular velocity, or decreasing
period, as shown in ﬁgure 12.6.

Spectral Methods
253
12.2.2
Relating matrices and graphs
We have already seen the connection between the matrix A and a graph,
though we did not draw attention to it at the time. In our discussion of the
vibrating string, we modeled the string as a sequence of particles connected by
elastic forces. The particles constitute nodes in a graph, and the elastic forces
that connect pairs of adjacent nodes represent edges. The graph is linear, in
this case, but it is easy to see how the generalization goes. For example, one
might model a vibrating fabric with particles on a grid, each connected to four
neighbors. Less regular, and even nonplanar, graphs are certainly possible.
In general, the displacement vector y is an assignment of values to nodes.
The force applied to node i by a neighbor yj is Fij = −K(yi −yj), and the net
force applied to node i is the sum P
j∈Ni Fij, where Ni is the set of neighbors
of node i. It can be written as
Fi = −K

|Ni|yi −
X
j∈Ni
yj

.
(12.28)
We take this equation as deﬁning an entry in the column vector F:
F = −K


|N1|
...
|Nn|




y1
...
yn

−


eT
1
...
eT
n




y1
...
yn

= −K(D −E)y
(12.29)
where eT
i , the i-th row of the matrix E, contains a one in the j-th column just
in case j ∈Ni. The matrix E is called the adjacency matrix of the graph,
and the matrix D is called its degree matrix, inasmuch as its i-th entry, |Ni|,
is the degree of the i-th node.
For example, consider the linear graph in ﬁgure 12.7. We have
F = −K










1 0 0 0
0 2 0 0
0 0 2 0
0 0 0 1


−


0 1 0 0
1 0 1 0
0 1 0 1
0 0 1 0












y1
y2
y3
y4


(12.30)
which produces the system of equations
F1 = −K(y1 −y2)
F2 = −K(−y1 + 2y2 −y3)
F3 = −K(−y2 + 2y3 −y4)
F4 = −K(−y3 + y4)

254
Semisupervised Learning for Computational Linguistics
1
2
3
4
FIGURE 12.7
A small linear graph.
in accordance with (12.28).
Rewriting F as mass times acceleration, as before, gives
d2
dt2 y = −K
mBy
(12.31)
where
B = D −E =


1 −1 0
0
−1 2 −1 0
0 −1 2 −1
0
0 −1 1


.
Note that B is the parenthesized expression in (12.30). And as before, the
solution to the diﬀerential equation (12.31) is any linear combination of the
special solutions:
y = ue±iωt
for ω =
√
−λ
where u is an eigenvector of (−K/m)B and λ is the corresponding eigenvalue.
As before, nothing rides on the identity of K and m, so we take −K/m = 1,
in which case u is an eigenvector of B itself.
The matrix B is very similar to the matrix A of (12.15), diﬀering only in the
ﬁrst and last rows. B arises more naturally than A when one takes a graph
as point of departure, as we have just seen, but its physical interpretation is
somewhat more exotic than that of A. The physical interpretation of B, as
for A, is a string of particles, connected by elastic forces, each constrained
to move only vertically, not horizontally.
But in the case of B, unlike A,
the ends of the string are not ﬁxed. They are free to move vertically just
like any other particle. One must imagine a string ﬂoating weightlessly and

Spectral Methods
255
0
L
k=1
k=2
k=3
x 1
x 2
x n-1
x n
. . .
FIGURE 12.8
The ﬁrst three eigenvectors of the matrix B.
undulating, with its ends moving freely in the vertical direction, but with its
ends (and consequently all interior particles) held in a ﬁxed horizontal position
by a frictionless constraint.
As in the case of A, the eigenvectors of B are harmonics. But because of
the diﬀerence in boundary conditions (the ﬁrst and last rows), the harmonics
are diﬀerent. For B, the k-th eigenvector is
u =


cos((1 −1
2)∆θ)
...
cos((n −1
2)∆θ)


where ∆θ = (k−1)π
n
and the k-th eigenvalue is
λ = 2 −2 cos(∆θ).
The ﬁrst three eigenvectors are shown in ﬁgure 12.8. In this case, the length
of the string L is divided into n equal segments, and the sample positions are
at the midpoints of the segments. We have x = L when j = n + 1
2, which
corresponds to an angle of
θ = ((n + 1
2) −1
2)∆θ = (k −1)π.
Since the period of the cosine function is 2π, the k-th eigenvector samples
from k−1 half-periods of the cosine function. In each half-period, the function
either falls from +1 to −1, or rises from −1 to +1. The case k = 1 deserves a
comment: in that case, the angle is always zero, and the eigenvector samples
from a constant function with value one.

256
Semisupervised Learning for Computational Linguistics
The demonstration that u and λ thus deﬁned are indeed eigenvectors and
eigenvalues of B is similar to the demonstration for A. Brieﬂy, the three cases
are:
Buj = −uj−1 + 2uj −uj+1
for j ̸= 1, n
Bu1 = u1 −u2
Bun = un −un−1.
Showing that Buj = λuj is straightforward for j ̸= 1, n. First note that
uj−1 = cos((j −1
2)∆θ −∆θ)
= cos((j −1
2)∆θ) cos(∆θ) −sin((j −1
2)∆θ) sin(∆θ)
= uj cos(∆θ) −sin((j −1
2)∆θ) sin(∆θ)
and similarly
uj+1 = uj cos(∆θ) + sin((j −1
2)∆θ) sin(∆θ).
Hence
Buj = 2uj −uj−1 −uj+1 = 2uj −2uj cos(∆θ) = (2 −2 cos(∆θ))uj
which is λuj, as desired.
For j = 1, using the identities cos 2x = cos2 x−sin2 x and cos2 x = 1−sin2 x
yields
λ = 4 sin2( 1
2∆θ)
and
u2 = u1[1 −4 sin2( 1
2∆θ)].
The equation Bu1 = λu1 follows.
As for Bun, we observe that
cos((j −1
2)∆θ) = cos((j −1
2)(k −1)π/n)
is a multiple of π when j = 1
2 and again when j = n + 1
2. It follows that un is
the same as u1 except possibly for a change of sign, and similarly un−1 and
u2. Hence, the equality Bun = λun follows as a consequence of Bu1 = λu1.
12.2.3
The Laplacian matrix and graph spectrum
The matrix B is known as the Laplacian matrix of the graph in ﬁgure 12.7.
The Laplacian matrix is also sometimes called the graph Laplacian.
The
Laplacian matrix is deﬁned as the diﬀerence
L = D −E

Spectral Methods
257
where D is the diagonal degree matrix and E is the adjacency matrix. The
Laplacian matrix has positive numbers on its main diagonal, since D is a
diagonal matrix containing positive real values and E has zeros on its main
diagonal. (We assume that the graph has no edges that start and end at the
same node.) It has negative ones oﬀ-diagonal, since E has positive ones just
in those cells (i, j) where there is an edge from node i to node j, and D has
zeros oﬀ-diagonal. Notice that the sum of entries in any row or column is zero:
the i-th entry in D is the total number of edges that the i-th node participates
in, and E contains a one in the i-th row (or column, as it is symmetric) for
each of those edges.
The spectrum of the graph is deﬁned to be the eigenvalues
λ1 ≤. . . ≤λn
of the Laplacian matrix. As before, the eigenvalues are ordered from smallest
to largest, as are the eigenvector frequencies. These deﬁnitions hold for any
graph, not just linear graphs.
The Laplacian can be generalized to a weighted graph:
L = D −W.
It is assumed that W is symmetric, that its entries are nonnegative, and that
wii = 0 for all i. With a weighted graph, the degree of the i-th node is deﬁned
as
di =
X
j
wij.
We encountered this generalization of degree previously, in (7.9).
12.3
Spectral clustering
12.3.1
The second smallest eigenvector of the Laplacian
The basic idea of spectral clustering is now easily stated. We consider the
eigenvectors of the Laplacian. In the case of a linear graph, the eigenvector
u corresponding to the second smallest eigenvalue goes through a single half-
period of the cosine function (ﬁgure 12.8).
It divides the graph into two
equal-sized, connected subgraphs.
The sign of uj can be interpreted as a
cluster label: it is positive for nodes in the ﬁrst subgraph, and negative for
the remaining nodes.
An example with a non-linear graph is given in ﬁgure 12.9. The Laplacian

258
Semisupervised Learning for Computational Linguistics
1
5
3
2
4
6
7
8
9
o
o
o
o
o
o
o
o
o
1
2
3
4
5
6
7
8
9
- 0.4
0.0
0.2
node j
u [ j ]
(a)
(b)
FIGURE 12.9
(a) A graph, and (b) its second smallest eigenvector.
of the graph in ﬁgure 12.9a is
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
2
3
4
5
6
7
8
9
1
3 −1 −1
0 −1
0
0
0
0
2 −1
3 −1 −1
0
0
0
0
0
3 −1 −1
4 −1
0 −1
0
0
0
4
0 −1 −1
3 −1
0
0
0
0
5 −1
0
0 −1
3 −1
0
0
0
6
0
0 −1
0 −1
4 −1 −1
0
7
0
0
0
0
0 −1
3 −1 −1
8
0
0
0
0
0 −1 −1
3 −1
9
0
0
0
0
0
0 −1 −1
2
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(The italic numbers are of course not part of the matrix, but are node numbers
included for ease of reference.) The eigenvector u with the second smallest
eigenvalue is shown in ﬁgure 12.9b. The function fails to be monotonic be-
cause the numbering of the nodes only approximately matches the intrinsic
structure of the graph. Sorting the nodes by decreasing value uj gives an op-
timal “linearization” of the graph, as shown in ﬁgure 12.10. In this case, one
extremity is node 2, and the other is node 9. The dotted line in ﬁgure 12.10 is
the “contour” where u equals zero, and represents the cluster boundary; the
same boundary is also marked in ﬁgure 12.9a.

Spectral Methods
259
2
3
1
4
5
6
9
7
8
FIGURE 12.10
The nodes of the previous graph, sorted by decreasing value uj.
12.3.2
The cut size and the Laplacian
It turns out that there is a strong connection between the division of the
graph obtained using the second eigenvector, as just discussed, and the graph
cuts that we discussed in sections 7.5 and 10.6. In section 7.5, we deﬁned
∂S = E(S, ¯S) as the set of edges that have one endpoint in S and one endpoint
outside of S, and we deﬁned
cut(S, ¯S) = size(∂S)
where the size of a set of edges is the sum of their weights (which is equal to
cardinality in the case of an unweighted graph).
A cut dividing the graph into sets S and ¯S can be represented by a vector y
in which element yi is equal to +1 if the i-th node belongs to S, and yi = −1
otherwise. Thinking of S and ¯S as classes, yi is the class label of the i-th
node. Let us call such a vector y a cut vector for S.
If nodes i and j both belong to the same class (both are in S or both are
in ¯S), then yi −yj = 0 and hence (yi −yj)2 = 0. On the other hand, if nodes
i and j belong to diﬀerent classes, then yi −yj is either +2 or −2, hence
(yi −yj)2 = 4. Thus
X
ij
(yi −yj)2
4
wij =
X
ij
[[yi ̸= yj]]wij = 2 · cut(S, ¯S).
The factor of two results because each edge gets counted twice, once when we
have i ∈S, j ∈¯S, and once when we have i ∈¯S, j ∈S. Deﬁne
g(y) =
X
ij
wij
(yi −yj)2
8
(12.32)

260
Semisupervised Learning for Computational Linguistics
= 1
8

X
i

X
j
wij

y2
i +
X
j
 X
i
wij
!
y2
j −2
X
ij
wijyiyj

.
Assuming symmetric weights, the ﬁrst two terms are equal, and the sum
P
j wij = P
i wij is equal to the degree. Hence
g(y) = 1
4

X
i
diy2
i −
X
ij
wijyiyj

.
Writing D for the diagonal degree matrix and W for the weight matrix, this
becomes
g(y) = 1
4

yTDy −yTWy

= 1
4yTLy
(12.33)
where L is the Laplacian matrix. When y is a cut vector for S, then cut(S, ¯S) =
g(y). This provides the basic connection between cuts and the Laplacian.
The choice of +1 and −1 as class labels in the cut vector is arbitrary.
We can actually choose any two distinct real numbers P and N. We can
generalize the deﬁnition of cut vector in the obvious way: y is a cut vector
for S if yi = P whenever the i-th node belongs to S and yi = N otherwise.
Let α be the halfway point between P and N, and let β be half the diﬀerence
P −N, so that P = α + β and N = α −β.
If nodes i and j have the
same label, then yi −yj is zero, and if i and j have diﬀerent labels, then
yi −yj = ±[(α + β) −(α −β)] = ±2β, and (yi −yj)2 = 4β2. The function
g(x) becomes
g(y) =
X
ij
wij
(yi −yj)2
4β2
=
1
4β2 yTLy.
(12.34)
If y is a cut vector for S then cut(S, ¯S) = g(y) with β = (P −N)/2. Notice
that g is insensitive to α. For the moment, we assume α = 0.
12.3.3
Approximating cut size
We arrived at the function g by consideration of vectors y whose elements are
all either +1 or −1. Geometrically, such vectors are the corners of a hypercube
C in n-dimensional space, for n the number of nodes in the graph.
It can be shown that minimizing g(y) under the constraint that y be one of
the corners of C is NP-hard. An exact solution is intractable. But if we relax
the constraint that y lie at one of the corners of the hypercube, and allow y
to vary continuously, an approximate solution can be found.
An obvious idea is to ﬁnd the vector x that minimizes g(x), and then ﬁnd
the corner y that is nearest to x. But there is a problem. The function g(x)
varies quadratically with the length of x, and it is trivially minimized by the
vector 0, which is equidistant from all corners of C.

Spectral Methods
261
The fact that all corners are equidistant from the origin suggests a better
approach. The distance from the origin to any corner is √n. That is, the
corners lie on the surface of a sphere, centered at the origin, with radius √n.
Let us ﬁx the length of x to be √n, so that it ranges over the sphere, and
search for the vector direction that minimizes g(x) on the sphere. The exact
solutions (that is, the corners) lie in diagonal directions. We preserve the
constraint that ||y|| = √n, but relax the restriction to diagonal directions,
allowing ourselves to consider any direction.
Minimizing g will be easier if we shrink the hypercube a bit. We can do that
because the size of the hypercube is determined by the cluster labels, and the
cluster labels are arbitrary. When we choose +1 and −1, which is to say, α = 0
and β = 1, then the center of C is at the origin, and its sides are all of length
2. Notice that any side of the hypercube connects two endpoints that diﬀer in
only one coordinate; one has α + β and the other has α −β. Hence the sides
are all of length 2β. Since α is added to each coordinate, varying α moves the
center of the hypercube along the main diagonal 1 = (1, . . . , 1). Each corner
of the hypercube can be expressed as βx+α1, where the coordinates of x are
all either +1 or −1.
For convenience, then, let us choose β to make the sphere a unit sphere,
namely, β = 1/√n. We continue to assume α = 0, so that the center of the
hypercube is at the origin. Now the corners lie on the unit sphere. Each
corner y has ||y|| = 1, since
||y||2 = yTy =
n
X
i=1
(±1/√n)2 =
n
X
i=1
1/n = 1.
To minimize g under the constraint that ||y|| = 1, we construct a function f
whose value agrees with g on the unit sphere, but is insensitive to the length
of y. Then we do an unconstrained minimization of f, obtaining a vector y0,
and normalize y0 to obtain a vector y on the unit sphere in the same direction
as y0. Since f is insensitive to vector length, we know f(y) = f(y0), hence y
is also a minimum of f. And since f = g on the unit sphere, we know that y
minimizes g on the sphere.
Actually, it is suﬃcient if cf(y) = g(y) for some nonzero constant c. We
deﬁne f(y) to be equal to yTLy, which is g(y) omitting the leading constant,
but we make it insensitive to the length of y by normalizing y beforehand:
f(y) =
 y
||y||
T
L
 y
||y||

= yTLy
||y||2 = yTLy
yTy .
(12.35)
Doing an unconstrained minimization of f and normalizing the resulting vec-
tor gives us a vector x that minimizes g on the unit sphere. At the corners of
the hypercube, g is equal to the cut size. The nearest corner to the minimizer
is our approximation to the best cut.

262
Semisupervised Learning for Computational Linguistics
12.3.4
Minimizing cut size
We previously introduced the idea of using the second eigenvector to cut the
graph – recall ﬁgure 12.9. That made intuitive sense for the example given.
Now we will show that it has good motivation in general.
The function f in (12.35) is immediately recognizable as a Rayleigh quo-
tient. Recall equation (11.22), restated here for convenience:
min
x∈Q⊥
k−1
xTAx
xTx = λk
arg
min
x∈Q⊥
k−1
xTAx
xTx = uk.
In words, if we restrict attention to the space Q⊥
k spanned by the eigenvectors
uk, . . . , un corresponding to the n −k + 1 largest eigenvalues λk, . . . , λn of a
symmetric matrix A, then the vector that minimizes the Rayleigh quotient is
uk, and the resulting minimum value is λk.
This equation is valid for any symmetric matrix A. Since the Laplacian
matrix L is symmetric, the equation is valid in particular if we set A = L.
Hence
min
y ∈Q⊥
k−1
f(y) = λk(L)
where λk(L) is the k-th smallest eigenvalue of L. This is true for any value
of k from 1 to n. Recall that Qk−1 is the space spanned by the n −k + 1
largest eigenvalues of L. Q⊥
0 is the entire space spanned by the eigenvalues of
L, hence
min
y f(y) = λ1(L).
However, as we have already seen, the eigenvector u1 corresponding to the
smallest eigenvalue is the constant vector. It corresponds to the trivial case
in which all nodes are assigned to the same cluster. The resulting cut value
is obviously 0, but hardly of interest.
If we exclude the trivial solution from consideration, the next least-restrictive
solution is to consider the space spanned by all eigenvectors except u1:
min
y∈Q⊥
1
f(y) = λ2(L)
arg min
y∈Q⊥
1
f(y) = u2(L).
(12.36)
The nontrivial cut with the smallest value is the one corresponding to the
second smallest eigenvalue of the graph Laplacian.
But we must remember that u2 is a relaxed solution. That is, it minimizes
f, but it does not necessarily lie at one of the corners of the hypercube C
representing the discrete clusterings of the nodes of the graph. The nearest
exact solution is the corner nearest to u2, and that is the solution we will
adopt. It is reasonable to do so, but there is no guarantee that the nearest
corner actually minimizes cut size among the corners.
Which corner is nearest? Recall that cut size (12.35) is sensitive only to
vector direction, not vector length. Even though the minimization is uncon-
strained, we are eﬀectively choosing a best direction on the unit sphere. Hence

Spectral Methods
263
the natural measure of distance between vectors is cosine, which on the unit
sphere is identical to dot product. So we would like to ﬁnd the nearest corner
to u2, where nearness is understood in the sense of dot product. A corner
vector is one whose elements are all ±1/√n, and since every combination is
valid (except the two in which all elements are positive or all elements are
negative), we can minimize distance elementwise: for each element ui of the
relaxed solution u = u2, we choose positive or negative 1/√n, whichever is
closer. In short, the nearest corner to u is the vector x in which the i-th ele-
ment xi has the same sign as ui, but magnitude 1/√n. The resulting vector
x is one of the corners of the hypercube, hence an exact solution.
We described (12.36) by saying that u2 represents the nontrivial cut with
the smallest value.
Actually, that is not quite true.
The eigenvector u2
represents the nontrivial cut perpendicular to u1 that has the smallest value.
But linear combinations of u1 and u2 also represent nontrivial cuts, and their
values are bounded above by λ2 (the value of u2).
A linear combination of u1 and u2 can be thought of as adding αu1 to u2
and then scaling the result. The scaling has no eﬀect on either the cut value
or the nearest corner, so we can ignore it; what matters is the addition of
αu1. Recall that u1 is the constant vector 1, so adding αu1 is the same as
adding α to each element of u2. The eﬀect is shown in ﬁgure 12.11. Adding
αu1 eﬀectively moves the zero line, and changes the cluster label for any
nodes whose sign changes as a result.
If α is large enough, all nodes are
labeled positive. The value α may also be negative, in which case the zero
line is raised. Eﬀectively, u2 deﬁnes an ordering on the nodes, and one may
choose any point in that ordering to be the dividing line between positives
and negatives. This yields a set of candidate exact solutions x, and a simple
approach is to compute cut(x) for each of those candidates, and keep the
one with the smallest value. There are at most n −1 candidate solutions
(excluding the trivial ones), so tractability is not an issue.
To summarize, the cut vectors can be expressed as βx + α1, where the
elements of x are all +1 or −1. We previously assumed that α = 0. Now we
see that adding α1 is the same as mixing some of u1 in with u2. In terms
of the cluster labels, α is the halfway point between them.
Changing the
distance β from the midpoint to either of the cluster labels has no eﬀect on
f, hence has no eﬀect on the minimizer of f. Moving the midpoint α away
from zero actually decreases f, since it mixes some of u1 = 1 in with u2.
12.3.5
Ratiocut
Simple minimization of cut size has a disadvantage from a practical point of
view: it does not take into account the relative size of the two clusters that
result from the cut. Although we excluded, by ﬁat, the trivial cut that puts
all nodes in the same cluster, we have not excluded nearly-trivial cuts, such
as cuts that have a single node in one cluster and all remaining nodes in the
other cluster. Such cuts arise frequently in practice. The smallest nontrivial

264
Semisupervised Learning for Computational Linguistics
o
o
o
o
o
o
o
o
o
1
2
3
4
5
6
7
8
9
- 0.4
0.0
0.2
node j
u [ j ]
C
FIGURE 12.11
The eﬀect of adding αu1 to u2.
cut severs a single edge, and if such a cut is possible, it is very likely that a
single isolated node lies at one endpoint of the edge in question.
An alternative that penalizes highly skewed partitions is the ratiocut. It is
deﬁned as
rcut(S, ¯S) = size(∂S)
|S| · | ¯S| .
(12.37)
The denominator of (12.37) is maximized, and hence rcut is minimized, when
S and ¯S are equally large.
The relaxed clustering representation y that is most suitable for ratiocut is
what we might call a ratiocut vector. It has
yi =
⎧
⎨
⎩
| ¯S|/n
if νi ∈S
−|S|/n if νi ∈¯S
where νi represents the i-th node. A ratiocut vector is not the same as a
cut vector, since the values for the cluster labels in a ratiocut vector are not
constants, but depend on the number of positive elements (the size of S) and
the number of negative elements (the size of ¯S) in the vector. Nonetheless,
we can relate rcut(S, ¯S) to f(y), where y is a ratiocut vector.
First, if nodes i and j belong to the same cluster, yi −yj is obviously zero.
If they belong to diﬀerent clusters, then either
yi −yj = |S|/n + | ¯S|/n = |S|/n + (1 −|S|/n) = 1
or
yi −yj = −| ¯S|/n −|S|/n = (|S|/n −1) −|S|/n = −1.

Spectral Methods
265
In either case, (yi −yj)2 = 1. It follows that
g(y) =
X
ij
(yi −yj)2
8
wij = 1
8
X
ij
[[yi ̸= yj]]wij = 1
8cut(S, ¯S).
Second, the squared length of y is equal to |S| · | ¯S|/n:
yTy = |S|| ¯S|2
n2 + | ¯S||S|2
n2 = |S| · | ¯S| · | ¯S| + |S|
n2
= |S| · | ¯S|/n.
Hence
rcut(S, ¯S) = cut(S, ¯S)
|S| · | ¯S| = 8ng(y)
yTy = 8nf(y).
Since 8n is a constant, it does not aﬀect minimization. The ratiocut vector
that minimizes f represents the clustering (S, ¯S) that minimizes rcut.
Finally, y is perpendicular to 1, since
y · 1 =
X
i
yi = |S|| ¯S|
n −| ¯S||S|
n = 0.
As before, the second eigenvector u2 minimizes f under the constraint that
y be perpendicular to 1. Mixing in some of u1 = 1 reduces f further, and
has the eﬀect of adjusting the zero line α (ﬁgure 12.11) to create a series of
candidate partitions. However, instead of choosing the candidate partition
that minimizes cut, we choose the one that minimizes rcut.
12.4
Spectral methods for semisupervised learning
We have just seen how to use spectral methods for clustering. We turn now
to the question of how they can be applied to semisupervised learning.
12.4.1
Harmonics and harmonic functions
Taking the second smallest eigenvector cuts the graph into two components
along a natural “fault line,” where the edges holding the two halves of the
graph together are weakest. But in the semisupervised setting, we are addi-
tionally given the labels of some of the instances, and there is no guarantee
that the division between positive and negative labels lines up neatly with the
natural fault line of the graph.
A natural idea is to go back to the matrix (12.15) corresponding to the string
with ﬁxed endpoints. One might take the second smallest eigenvector of that
matrix, which is the case k = 2 in ﬁgure 12.6. However, the endpoints are both

266
Semisupervised Learning for Computational Linguistics
ﬁxed at the same value, namely zero. We wish to have two diﬀerent values,
one representing the positive class and one representing the negative class.
Unfortunately, the value zero is special; we cannot ﬁx one of the endpoints at,
say, value one and capture that in a matrix like the one in (12.15). Setting
one of the endpoints at value one requires us to put a constant in the place of
one of the yk’s in the vector that is being premultiplied by the matrix.
It turns out that we already know what happens when we ﬁx the values of
some of the yk’s, though it will take a bit of explaining to see the connection.
Let us begin by considering again the quantity g(y) that we deﬁned as a
general measure of the cost of a label assignment y. It is equal to cut(S, ¯S)
whenever y is a cut vector for S, that is, whenever the components of y are
+1 for nodes belonging to S and −1 for nodes belonging to ¯S. Generalizing
to arbitrary values α+β for the positive class and α−β for the negative class,
we saw in (12.34) that the function g(y) could be written in two equivalent
forms:
g(y) =
1
(2β)2
X
ij
(yi −yj)2wij
g(y) =
1
(2β)2 yTLy.
Choosing y so as to minimize g(y) is equivalent to minimizing cut size.
If we take a given value yk to represent the probability of having the pos-
itive label, then yk ranges from 0 to 1. Equivalently, we can treat 0 as the
“negative” value and 1 as the “positive” value, that is, we take α = 1/2 and
β = 1/2. In that case, our two equations for g(y) simplify to:
g(y) =
X
ij
(yi −yj)2wij
(12.38)
g(y) = yTLy.
(12.39)
Taking yk to represent the probability that the k-th node is positively la-
beled brings harmonic functions to mind. In the context of harmonic func-
tions, equation (12.38) is familiar: it is twice the “J” energy of y, as deﬁned
in (10.12), repeated here with a slight modiﬁcation in notation:
J(y) = 1
2
X
ij
(yi −yj)2wij.
(12.40)
Recall that J is deﬁned for any function over nodes of the graph, and a vector
y assigning values to nodes is such a function. In the semisupervised case,
we wish to choose y so as to minimize g(y), subject to the constraint that
the values assigned to boundary nodes (that is, labeled nodes) agree with
the labeled instances given as input. But the vector that minimizes g(y) is
identical to the vector that minimizes J(y), and we know from Thomson’s
principle (Theorem 10.3) that the vector that minimizes J(y) is the harmonic

Spectral Methods
267
function that agrees with the given boundary values.
In short, the basic
spectral methods for semisupervised learning are the now-familiar methods
for constructing a harmonic function: the method of relaxations, or the closed
form matrix solution given in (10.14).
In short, the objective function (12.38) is equivalent to the energy J(y),
which can be written
J(y) = 1
2yTLy.
(12.41)
It is minimized in the unconstrained case by the harmonic u1 with the smallest
eigenvalue, or the harmonic u2 with the second smallest eigenvalue if we
exclude u1, and it is minimized in the constrained case (the semisupervised
case) by the harmonic function determined by the boundary values.
12.4.2
Eigenvalues and energy
Not only is the energy J(y) minimized by the eigenvector with the smallest
eigenvalue, but in general, the energy of eigenvector ui is exactly equal to
one-half the associated eigenvalue λi.
As we just saw, energy is equal to
(1/2)yTLy. The Laplacian L is symmetric and hence has the diagonalization
L = QΛQT
where the columns of Q are eigenvectors uk, and the entries in the diagonal
matrix Λ are the corresponding eigenvalues λk. The diagonalization can also
be expressed as an outer product:
L =
X
i
λiuiuT
i .
Further, we can assume that the eigenvalues are sorted in order of increasing
value.
Let us consider the energy J(ui) for an eigenvector ui:
1
2uT
i Lui = 1
2uT
j

X
j
λjujuT
j

ui
= 1
2

X
j
λjuT
i ujuT
j

ui
= 1
2λiuT
i ui
= 1
2λi.
In both of the last two steps, we have used the facts that uT
i ui = 1 and
uT
i uj = 0 for j ̸= i.

268
Semisupervised Learning for Computational Linguistics
We have just seen that the energy of an eigenvector is equal to half the
corresponding eigenvalue. A general value assignment y can be expressed as
a linear combination of eigenvectors:
y =
X
i
biui.
The energy of y can be computed from the energies of the eigenvectors. As a
preliminary, we require the following fact. If i ̸= j:
uT
i Luj = uT
i
 X
k
λkukuT
k
!
uj
= λiuT
i uj
= 0.
Now we compute J(y):
1
2yTLy = 1
2
 X
i
biui
!T
L

X
j
bjuj


= 1
2
X
ij
bibjuT
i Luj
= 1
2
X
i
b2
i λi.
Since eigenvalues are sorted by increasing value, it becomes obvious that we
minimize energy, for a ﬁxed total P
i b2
i , by making bi large for small values
of i and small for large i.
Geometrically, energy corresponds to roughness. This is apparent in ﬁg-
ures 12.6 and 12.8. Increasing energy corresponds to increasing eigenvalue,
which corresponds to increasing frequency, which corresponds to increasing
roughness, in the form of more rapid changes in value in y. By contrast, a
value assignment y that satisﬁes the cluster hypothesis (“similar instances
have similar labels”) is locally homogeneous, which is to say smooth, varying
slowly across the graph.
12.4.3
The Laplacian and random ﬁelds
While we are on the subject of energy, it is perhaps appropriate to revisit
random ﬁelds. This is something of a digression, but it does serve to draw
together some threads that we introduced much earlier, and to underline the
commonalities among diverse methods.
In section 9.3 we introduced random ﬁelds as providing a direct way of as-
signing high probability to labelings in which close neighbors have the same

Spectral Methods
269
label. Speciﬁcally, one deﬁnes an “energy” or “cost” function H(y) that pe-
nalizes label disagreements between neighbors, and the probability of labeling
y is deﬁned as
p(y) = 1
Z e−H(y)
where Z is a normalizing constant. In section 9.3 we used a simple version of
the Ising model, with
H(y) = −
X
(i,j)∈E
yiyj
where it was assumed that the labels yk are either +1 or −1. Let us compare
this to our current deﬁnition of energy:
J(y) = 1
2
X
ij
(yi −yj)2wij.
In the case under consideration, where the graph is unweighted, wij has value
one if there is an edge between i and j, and value zero otherwise.
If neighbors i and j have diﬀerent labels, then
yiyj = −1
1 −yiyj = 2
1
2(yi −yj)2 = 2.
If yi and yj have the same label, then
yiyj = 1
1 −yiyj = 0
1
2(yi −yj)2 = 0.
In either case, we have
1
2(yi −yj)2 = 1 −yiyj.
Summing over pairs of neighbors yields
J(y) =
X
(i,j)∈E
1
2(yi −yj)2
=
X
(i,j)∈E
(1 −yiyj)
= |E| + H(y).
When we consider the resulting probability distribution, the constant |E| gets
folded into the normalization:
1
Z e−H(y) = 1
Z e−J(y)+|E|
= 1
Z′ e−J(y)

270
Semisupervised Learning for Computational Linguistics
with
1
Z′ = 1
Z e|E|.
That is, if we have an unweighted graph and labels in {−1, +1}, then the
Ising model is equivalent to a random ﬁeld that uses J(y) as its energy func-
tion. The functions H(y) and J(y) are not equivalent, however, when we
generalize to weighted graphs and real vectors y. In the general case, it is
common to use J(y) as the energy function, and the resulting random ﬁeld is
called a Gaussian random ﬁeld. Note that the Gaussian random ﬁeld can be
written in terms of the graph Laplacian:
1
Z′ e−1
2 yTLy.
12.4.4
Harmonic functions and the Laplacian
We return now to the idea that harmonic function methods are representatives
of semisupervised spectral methods. In the next section, we will go beyond the
methods that we introduced in chapter 10, but before we do, it is informative
to re-examine the derivation of one of those methods, the matrix solution
(10.14), in light of the connection between harmonic functions and the graph
Laplacian. In chapter 10, we related harmonic functions to random walks,
and derived the matrix solution from the ﬁxed-point equation of the random
walk:
y = Py.
(12.42)
The transition matrix P contains entries pij deﬁned as
pij =
wij
P
k wik
.
The denominator is the degree of the i-th node:
di =
X
k
wik.
Hence, writing D for the diagonal degree matrix, we can express P as
P = D−1W
and (12.42) becomes
y = D−1Wy
which is to say
Dy = Wy.
(12.43)
Given what we know now, instead of taking the roundabout path involving
Thomson’s principle and random walks, we can derive (12.43) much more

Spectral Methods
271
directly, using the matrix diﬀerentiation methods of section 5.2.3 to minimize
energy (12.41). The derivative is
DyJ(y) = 1
2DyyTLy
= yTL
= Ly.
In both steps we have taken advantage of the symmetry of L. Now we set the
derivative to zero, to obtain
Ly = 0.
(12.44)
Given L = D −W, this is equivalent to (12.43).
Closer inspection of (12.44) makes the fundamental connection between the
Laplacian and harmonic functions clearer. The i-th row of L has the form:
[−wi,1
. . .
−wi,i−1
di
−wi,i+1
. . .
−wi,n].
Hence (12.44) corresponds to a system of equations of form:
diyi −
X
j
wijyj = 0.
Substituting P
k wik for di and rearranging yields
yi =
X
j
wij
P
k wik
yj.
In short, the equation (12.44) implies the averaging property for values yi,
which is to say, (12.44) implies that y is a harmonic function.
Returning now to (12.43) and continuing with the derivation of the solution,
we partition y into two half-vectors, one for the labeled nodes and one for the
unlabeled nodes, and we partition D and W similarly:

Dl 0
0 Du



yl
yu

=

Wll Wlu
Wul Wuu



yl
yu

.
This can be written as two equations:
Dlyl = Wllyl + Wluyu
(12.45)
Duyu = Wulyl + Wuuyu.
(12.46)
Since the labels for the labeled nodes are ﬁxed, the equation of interest is
(12.46), which determines the label probabilities for the unlabeled nodes. It
can be written
(Du −Wuu)yu = Wulyl
which is
yu = L−1
uuWulyl.
(12.47)

272
Semisupervised Learning for Computational Linguistics
12.4.5
Using the Laplacian for regularization
Our conclusion so far concerning spectral methods in the semisupervised case
is that they reduce to computation of the harmonic function determined by
the labeled instances. In fact, there is a more general way to view the problem.
In our discussion of the matrix solution for computing the harmonic function,
we derived equation (12.45), but put it to the side on the grounds that the
values for labeled nodes are ﬁxed. However, by doing so we swept an issue
under the carpet. We had asserted that we wanted an assignment y that
satisﬁes the ﬁxed point equation (12.42). Unless y satisﬁes (12.45), it cannot
satisfy the ﬁxed point equation. But harmonic functions often fail to satisfy
(12.45). Equation (12.45) states that, for each labeled node i:
yi =
X
j
wij
di
yj
where j ranges over all nodes, labeled and unlabeled. In other words, (12.45)
states that labeled nodes satisfy the averaging property. We have already seen
counterexamples: for example, the harmonic functions shown in ﬁgures 10.3
and 10.4.
Intuitively speaking, the ﬁxed point equation wants each labeled node to
satisfy the averaging property, but the averaging property is often inconsistent
with the given labeled data, introducing a “stress” between harmonicity and
the ﬁxed labels. The approach we have considered until now chooses a function
y that is maximally congruent with the ﬁxed labels, and ignores the conﬂicting
demands of harmonicity for labeled nodes.
An alternative is to view energy, which is to say, the graph Laplacian, as
a regularization term. The idea of regularization was introduced in section
4.2.3, and it was mentioned again in connection with support vector machines
in section 6.4.3. A regularization term represents a measure of simplicity that
is traded oﬀagainst ﬁt to the data. For support vector machines, a simple
model is one that predicts good separation, that is, large, empty boundary
regions, between clusters. For spectral clustering, a simple model is one that
has low energy. Geometrically, it is smooth, varying slowly across the graph.
Harmonic functions ﬁt the labeled data perfectly, at a cost in smoothness.
The more general approach is to trade oﬀﬁt against simplicity.
We desire a function y that ﬁts the labeled data well. Writing ˜y for the
values in the labeled data, we can measure (lack of) ﬁt to the data as squared
error. The total squared error is
X
1≤i≤ℓ
(yi −˜yi)2 = (y −˜y)TIℓ(y −˜y)
(12.48)
where Iℓis a diagonal matrix with 1’s in the ﬁrst ℓpositions on the diagonal,
and 0’s after that. We note that the input label values ˜yi are discrete: 1 for
postively labeled and 0 for negatively labeled. The predicted values yi are not
so constrained.

Spectral Methods
273
The function should also be simple, which in this case we interpret as being
smooth, as measured by the graph Laplacian. Taking a linear combination of
the ﬁt term (12.48) and the simplicity term, we have the objective function
α(y −˜y)TIℓ(y −˜y) + βyTLy
(12.49)
where α and β are free parameters. Since our goal is to minimize the objective
function, and multiplication by a constant has no eﬀect on the identity of the
minimizer, we can multiply the preceding by 1/α, and write the objective
function more simply as
(y −˜y)TIℓ(y −˜y) + γyTLy
(12.50)
where we replace the two free parameters α and β with a single free parameter
γ = β/α.
We wish to minimize (12.50), so we take its derivative and set it to zero:
Dy[(y −˜y)TIℓ(y −˜y) + γyTLy]
= Dy(y −˜y)TIℓ(y −˜y) + γDyyTLy
= 2(y −˜y)TIℓ+ 2γyTL.
In the last step we have used the fact that both Iℓand L are symmetric. The
result is a row vector; converting it to a column vector yields
2IT
ℓ(y −˜y) + 2γLTy.
Setting this expression to zero, and using the facts IT
ℓ= Iℓand LT = L, we
have
Iℓy + γLy = Iℓ˜y.
Hence
y = (Iℓ+ γL)−1Iℓ˜y.
If we deﬁne the last n −ℓentries in ˜y to be 0’s, this simpliﬁes slightly to
y = (Iℓ+ γL)−1˜y.
(12.51)
The only remaining question is how to determine γ. This can be done a
priori. Let us consider the objective function in the form (12.49). It is natural
to average the ﬁrst term (the squared error term) over labeled instances,
meaning that we take α = 1/ℓ. The second term (the Laplacian) sums over
weighted edges, so it is natural to normalize it by total edge weight; that is,
we take β = 1/ P
ij wij. We then have γ = β/α, which is
γ =
ℓ
P
ij wij
.
Alternatively, γ can be estimated from held-out data.

274
Semisupervised Learning for Computational Linguistics
12.4.6
Transduction to induction
We have just considered two semisupervised spectral methods, the harmonic
function method (12.47) and the regularized method (12.51). Both of them are
transductive: they tell us how to compute labels for the unlabeled instances,
but they do not tell us how to label new instances that were not included in
the training data. A ﬁnal question, then, is how to deal with test instances,
which is to say, how to do induction using transductive learners.
Given a new instance x whose label we wish to predict, the simplest idea is
to add x to the graph and re-run the transductive algorithm. That approach
is exact, but obviously has the disadvantage of high cost for labeling a single
new instance.
An alternative is to optimize the objective function just for the new in-
stance, taking all other instances and their predicted labels as ﬁxed. If we are
given a new instance x, let us consider the eﬀect on the objective function
(12.50). The ﬁrst term of (12.50), the squared error term, is a sum over la-
beled instances; since x is unlabeled it has no eﬀect on that term. The second
term, the Laplacian, involves a sum over edges, as we see when it is written
in the form:
yTLy =
X
ij
(yi −yj)2wij.
Adding a new instance x adds new terms to the sum, one for each edge
between x and a training instance xi, but it does not aﬀect the old terms.
In short, let C represent the value of (12.50) computed on the training
data. Holding that constant, and adding a single new instance x, the objective
function becomes
C + γ
X
i
(y −yi)2w(x, xi)
where y is the value to be computed (the positive-label probability for x)
and w(x, xi) represents the similarity between the new instance x and the old
instance xi. The derivative with respect to y is
2γ
X
i
(y −yi)w(x, xi).
Setting the derivative to zero yields
y
X
i
w(x, xi) =
X
i
yiw(x, xi)
y =
X
i
w(x, xi)
P
j w(x, xj)yi.
In short, we simply use the averaging property to compute the label for x.

Spectral Methods
275
12.5
Bibliographic notes
For further reading on semisupervised learning with spectral methods, the
deﬁnitive collection is Chapelle et al. [39]. Ding [78] also has a useful tutorial
on spectral clustering. The spectral method for approximating ratiocut was
proposed by Hagen & Kahng [102]. The method discussed for going from
transduction to induction follows Bengio et al. [13].


Bibliography
[1] Abney, Steven. Bootstrapping. Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics (ACL-2002), pp. 360–
367. Association for Computational Linguistics. East Stroudsburg, PA.
2002.
[2] Abney, Steven. Understanding the Yarowsky Algorithm. Computational
Linguistics 30(3):365–395. 2004.
[3] Agichtein, Eugene, Eleazar Eskin, and Luis Gravano. Combining Strate-
gies for Extracting Relations from Text Collections. Columbia Univer-
sity Technical Report CUCS-006-00. 2000.
[4] Agichtein, E. and L. Gravano. Snowball: Extracting Relations from
Large Plain-Text Collections. The 5th International Conference on Dig-
ital Libraries. Association for Computing Machinery. New York. 2000.
[5] Allan, James. Relevance feedback with too much data. Proceedings of
the 18th Annual International Conference on Research and Development
in Information Retrieval (SIGIR-1995), pp. 337–343. Association for
Computing Machinery. New York. 1995.
[6] Ando, Rie Kubota. Semantic lexicon construction: Learning from un-
labeled data via spectral analysis. Proceedings of the Conference on
Natural Language Learning (CoNLL-2004). Association for Computa-
tional Linguistics. East Stroudsburg, PA. 2004.
[7] Baluja, S. Probabilistic modeling for face orientation discrimination:
Learning from labeled and unlabeled data. Advances in Neural Infor-
mation Processing Systems 11 (NIPS-1998). MIT Press. Cambridge,
MA. 1999.
[8] Basu, S., A. Banerjee, and R. Mooney. Semi-supervised clustering by
seeding. Proceedings of the 19th International Conference on Machine
Learning (ICML-2002). Morgan Kaufmann Publishers. San Francisco,
CA. 2002.
[9] Bean, David and Ellen Riloﬀ.
Unsupervised learning of contextual
role knowledge for coreference resolution. Proceedings of the Confer-
ence on Human Language Technology and the Meeting of the North
American Chapter of the Association for Computational Linguistics
277

278
Semisupervised Learning for Computational Linguistics
(HLT/NAACL-2004). Association for Computational Linguistics. East
Stroudsburg, PA. 2004.
[10] Becker, S. and G.E. Hinton. A self-organizing neural network that dis-
covers surfaces in random-dot stereograms. Nature 355:161–163. 1992.
[11] Bengio, S. and Y. Bengio.
An EM Algorithm for Asynchronous In-
put/Output Hidden Markov Models. Proceedings of the 3rd Interna-
tional Conference On Neural Information Processing (ICONIP 1996),
pp. 328–334. Springer-Verlag. Berlin. 1996.
[12] Bengio, Y. and F. Gingras.
Recurrent Neural Networks for Missing
or Asynchronous Data. In M. Mozer, D.S. Touretzky, and M. Perrone
(eds.), Advances in Neural Information Processing Systems 8 (NIPS-
1995). MIT Press. Cambridge, MA. 1996.
[13] Bengio, Yoshua, Olivier Delalleau, and Nicolas Le Roux. Label propaga-
tion and quadratic criterion. In Chapelle et al. (eds.), Semi-Supervised
[14] Bennett, K.P., and A. Demiriz. Semi-supervised support vector ma-
chines. Advances in Neural Information Processing Systems 10 (NIPS-
1997), pp. 368–374. MIT Press. Cambridge, MA. 1998.
[15] Bennett, K., A. Demiriz, and R. Maclin.
Exploiting unlabeled data
in ensemble methods. Proceedings of 8th International Conference on
Knowledge Discovery and Data Mining (KDD-2002), pp. 289–296. As-
sociation for Computing Machinery. New York. 2002.
[16] Berland, M., and E. Charniak. Finding parts in very large corpora.
Proceedings of the 37th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-1999), pp. 57–64. Association for Computa-
tional Linguistics. East Stroudsburg, PA. 1999.
[17] Bhattacharya, Indrajit, Lise Getoor, and Yoshua Bengio. Unsupervised
sense disambiguation using bilingual probabilistic models. Proceedings
of the 42nd Meeting of the Association for Computational Linguistics
(ACL-2004). Association for Computational Linguistics. East Strouds-
burg, PA. 2004.
[18] Bikel, Daniel M., Scott Miller, Richard Schwartz, and Ralph Weischedel.
Nymble: A High-Performance Learning Name-ﬁnder. Proceedings of the
Fifth Conference on Applied Natural Language Processing, pp. 194–201.
1997.
[19] Bishop, Christopher M.
Neural Networks for Pattern Recognition.
Clarendon Press. Oxford. 1995.
[20] Blum, Avrim and Shuchi Chawla. Learning from labeled and unlabeled
data using graph mincuts. Proceedings of the 18th International Confer-
Learning (Chapter 11). MIT Press. Cambridge, MA. 2006.

Bibliography
279
ence on Machine Learning, pp. 19–26. Morgan Kaufmann Publishers.
San Francisco, CA. 2001.
[21] Blum, Avrim, and Tom Mitchell. Combining labeled and unlabeled data
with co-training. Proceedings of the 11th Annual Conference on Com-
putational Learning Theory (COLT), pp. 92–100. Morgan Kaufmann
Publishers. San Francisco, CA. 1998.
[22] Blum, Avrim, John Laﬀerty, Rajashekar Reddy, and Mugizi Robert
Rwebangira. Semi-supervised learning using randomized mincuts. Pro-
ceedings of the 21st International Conference (ICML-2004). Association
for Computing Machinery. New York. 2004.
[23] Bodenreider, Olivier, Thomas Rindﬂesch, and Anita Burgun.
Unsu-
pervised, corpus-based method for extending a biomedical terminology.
Proceedings of the Workshop on Natural Language Processing in the
Biomedical Domain, Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2002). Association for Computational Linguis-
tics. East Stroudsburg, PA. 2002.
[24] Borthwick, A., J. Sterling, E. Agichtein, and R. Grishman. Exploiting
Diverse Knowledge Sources via Maximum Entropy in Named Entity
Recognition. Sixth Workshop on Very Large Corpora (VLC-1998). As-
sociation for Computational Linguistics. East Stroudsburg, PA. 1998.
[25] Brill, Eric. Unsupervised Learning of Disambiguation Rules for Part of
Speech Tagging. Third Workshop on Very Large Corpora (VLC-1995),
pp. 1–13. Association for Computational Linguistics. East Stroudsburg,
PA. 1995.
[26] Brin, Sergey. Extracting Patterns and Relations from the World Wide
Web. Proceedings of the WebDB Workshop, 6th International Confer-
ence on Extending Database Technology (EDBT-1998). Springer-Verlag.
Berlin. 1998.
[27] Briscoe, Ted and John Carroll. Automatic Extraction of Subcategoriza-
tion from Corpora. Proceedings of the Conference on Applied Natural
Language Processing (ANLP). Association for Computational Linguis-
tics. East Stroudsburg, PA. 1997.
[28] Briscoe, Ted and John Carroll. Towards Automatic Extraction of Ar-
gument Structure from Corpora. Rank Xerox Research Centre Tech
Report MLTT-006. 1994.
[29] Broder, Andrei Z., Robert Krauthgamer, and Michael Mitzenmacher.
Improved classiﬁcation via connectivity information. Proceedings of the
Eleventh Annual Symposium on Discrete Algorithms, pp. 576–585. As-
sociation for Computing Machinery/Society for Industrial and Applied
Mathematics. New York. 2000.

280
Semisupervised Learning for Computational Linguistics
[30] Brown, P., V. Della Pietra, P. deSouza, J. Lai, and R. Mercer. Class-
Based n-gram Models of Natural Language. Computational Linguistics
18(4):467–480. 1992.
[31] Bruce, Rebecca. “A Statistical Method for Word-Sense Disambigua-
tion.” PhD diss. New Mexico State University. 1995.
[32] Buckley, C., M. Mitra, J. Walz, and C. Cardie. Using clustering and
SuperConcepts within SMART. Proceedings of the Sixth Text Retrieval
Conference (TREC). National Institute of Standards and Technology.
Gaithersburg, MD. 1998.
[33] Buckley, C., G. Salton, and J. Allan. Automatic retrieval with local-
ity information using SMART. Proceedings of the First Text Retrieval
Conference (TREC), pp. 59–72. National Institute of Standards and
Technology. Gaithersburg, MD. 1992.
[34] Cao, Yunbo, Hang Li, and Li Lian. Uncertainty Reduction in Collabo-
rative Bootstrapping: Measure and Algorithm. Proceedings of the 41st
Meeting of the Association for Computational Linguistics (ACL-2003).
Association for Computational Linguistics. East Stroudsburg, PA. 2003.
[35] Caraballo, S.A. Automatic construction of a hypernym-labeled noun
hierarchy from text. Proceedings of the 37th Meeting of the Association
for Computational Linguistics (ACL-1999). Association for Computa-
tional Linguistics. East Stroudsburg, PA. 1999.
[36] Castelli, Vittorio and Thomas Cover. On the exponential value of la-
beled samples. Pattern Recognition Letters 16:105–111. 1995.
[37] Castelli, V. and T. Cover. The Relative Value of Labeled and Unlabeled
Samples in Pattern Recognition with an Unknown Mixing Parameter.
IEEE Transactions on Information Theory 42(6):2102–2117. Institute
of Electrical and Electronics Engineers. New York. 1996.
[38] Chakrabarti, Soumen.
Mining the Web: Analysis of Hypertext and
Semi Structured Data. Morgan Kaufmann Publishers. San Francisco,
CA. 2002.
[39] Chapelle, Olivier, Bernhard Sch¨olkopf, and Alexander Zien (eds). Semi-
Supervised Learning. MIT Press, Cambridge, MA. 2006.
[40] Chapelle, O., J. Weston, and B. Sch¨olkopf. Cluster kernels for semi-
supervised learning. Advances in Neural Information Processing Sys-
tems 15 (NIPS-2002). MIT Press. Cambridge, MA. 2003.
[41] Charniak, Eugene. Unsupervised learning of name structure from coref-
erence data.
Proceedings of the 2nd Meeting of the North American
Chapter of the Association for Computational Linguistics (NAACL-
2001), pp. 48–54. Association for Computational Linguistics. East
Stroudsburg, PA. 2001.

Bibliography
281
[42] Chawla, Nitesh and Grigoris Karakoulas. Learning from labeled and un-
labeled data: an empirical study across techniques and domains. Jour-
nal of Artiﬁcial Intelligence Research 23:331–366. 2005.
[43] Chelba, Ciprian and Frederick Jelinek. Exploiting Syntactic Structure
for Language Modeling.
Proceedings of the 36th Meeting of the As-
sociation for Computational Linguistics (ACL-1998). Association for
Computational Linguistics. East Stroudsburg, PA. 1998.
[44] Church, Kenneth. A Stochastic Parts Program and Noun Phrase Parser
for Unrestricted Texts. Proceedings of the 2nd Conference on Applied
Natural Language Processing (ANLP-1988). Association for Computa-
tional Linguistics. East Stroudsburg, PA. 1988.
[45] Church, Kenneth, Patrick Hanks, Donald Hindle, William Gale, and
Rosamond Moon. Substitutability. Internal Technical Memorandum.
AT&T Bell Laboratories. 1990.
[46] Coates-Stephens, Sam. The Analysis and Acquisition of Proper Names
for the Understanding of Free Text. Computers and the Humanities
26:441–456. 1993.
[47] Cohen, I., N. Sebe, F.G. Cozman, and T.S. Huang. Semi-supervised
Learning for Facial Expression Recognition.
Proceedings of the 5th
International Workshop on Multimedia Information Retrieval (MIR-
2003), pp. 17–22. Association for Computing Machinery. New York.
2003.
[48] Cohen, I., N. Sebe, F.G. Cozman, M.C. Cirelo, and T.S. Huang. Semi-
supervised Learning of Classiﬁers: Theory and Algorithms for Bayesian
Network Classiﬁers and Applications to Human-Computer Interaction.
IEEE Transactions on Pattern Analysis and Machine Intelligence. In-
stitute of Electrical and Electronics Engineers. New York. 2004.
[49] Cohn, D.A., Z. Ghahramani, and M.I. Jordan. Active learning with
statistical models. Journal of Artiﬁcial Intelligence Research 4:129–145.
1996.
[50] Collins, Michael and Yoram Singer. Unsupervised Models for Named
Entity Classiﬁcation. Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP-1999), pp. 100–110. As-
sociation for Computational Linguistics. East Stroudsburg, PA. 1999.
[51] Cormen, T., C. Leiserson, and R. Rivest. Introduction to Algorithms.
MIT Press. Cambridge, MA. 1990.
[52] Cover, T. and J. Thomas. Elements of Information Theory. John Wiley
& Sons. New York. 1991.
[53] Cozman, Fabio G. and Ira Cohen. Unlabeled data can degrade clas-
siﬁcation performance of generative classiﬁers. Proceedings of the 15th

282
Semisupervised Learning for Computational Linguistics
International Florida Artiﬁcial Intelligence Research Society Confer-
ence (FLAIRS-2002), pp. 327–331. AAAI Press/MIT Press. Cambridge,
MA. 2002.
[54] Cozman, Fabio G., I. Cohen, and M.C. Cirelo. Semi-supervised learning
of mixture models. Proceedings of the 20th International Conference on
Machine Learning (ICML-2003). AAAI Press/MIT Press. Cambridge,
MA. 2003.
[55] Craven, M. and S. Slattery. Relational Learning with Statistical Predi-
cate Invention: Better Models for Hypertext. Machine Learning 43:97–
119. 2001.
[56] Craven, Mark, Dan DiPasquo, Dayne Freitag, Andrew McCallum, Tom
Mitchell, Kamal Nigam, and Sean Slattery.
Learning to Construct
Knowledge Bases form the World Wide Web.
Artiﬁcial Intelligence
118:69–113. 2000.
[57] Craven, M., D. DiPasquo, D. Freitag, A. McCallum, T. Mitchell, K.
Nigam, and S. Slattery. Learning to extract symbolic knowledge from
the World Wide Web.
Proceedings of the 15th National Conference
on Artiﬁcial Intelligence (AAAI-1998). AAAI Press/MIT Press. Cam-
bridge, MA. 1998.
[58] Cucerzan, Silviu and David Yarowsky.
Language independent mini-
mally supervised induction of lexical probabilities. Proceedings of the
38th Meeting of the Association for Computational Linguistics (ACL-
2000), pp. 270–277. Association for Computational Linguistics. East
Stroudsburg, PA. 2000.
[59] Cucerzan, Silviu and David Yarowsky. Language Independent Named
Entity Recognition Combining Morphological and Contextual Evi-
dence.
In Proceedings of the Joint SIGDAT Conference on Empiri-
cal Methods in Natural Language Processing and Very Large Corpora
(EMNLP/VLC-1999), pp. 90–99. Association for Computational Lin-
guistics. East Stroudsburg, PA. 1999.
[60] Cucerzan, Silviu and David Yarowsky.
Minimally supervised induc-
tion of grammatical gender. Proceedings of the Conference on Human
Language Technology and the Meeting of the North American Chapter
of the Association for Computational Linguistics (HLT/NAACL-2003).
Association for Computational Linguistics. East Stroudsburg, PA. 2003.
[61] Curran, James R. Supersense tagging of unknown nouns using seman-
tic similarity. Proceedings of the 43rd Meeting of the Association for
Computational Linguistics (ACL-2005). Association for Computational
Linguistics. East Stroudsburg, PA. 2005.
[62] Cutting, Doug, Julian Kupiec, Jan Pedersen, and Penelope Sibun. A
Practical Part-of-Speech Tagger. Third Conference on Applied Natural

Bibliography
283
Language Processing (ANLP), pp. 133–140. Association for Computa-
tional Linguistics. East Stroudsburg, PA. 1992.
[63] Dagan, Ido and Alon Itai. Word Sense Disambiguation Using a Second
Language Monolingual Corpus. Computational Linguistics 20:563–596.
1994.
[64] Dagan, Ido, Alon Itai, and Ulrike Schwall. Two Languages are more
Informative than One. Proceedings of the 29th Annual Meeting of the
Association for Computational Linguistics (ACL-1991), pp. 130–137.
Association for Computational Linguistics. East Stroudsburg, PA. 1991.
[65] Dagan, Ido, Lillian Lee, and Fernando Pereira. Similarity-based models
of word coocurrence probabilities.
Machine Learning 34(1–3):43–69.
1999.
[66] Dasgupta, Sanjoy.
Learning Mixtures of Gaussians.
Berkeley Tech
Report CSD-99-1047. University of California Berkeley. 1999.
[67] Dasgupta, Sanjoy, Michael Littman, and David McAllester. PAC gener-
alization bounds for co-training. Advances in Neural Information Pro-
cessing Systems 14 (NIPS-2001). MIT Press. Cambridge, MA. 2002.
[68] De Comit´e, F., F. Denis, R. Gilleron, and F. Letouzey. Positive and
unlabeled data help learning. Proceedings of the Conference on Com-
putational Learning Theory (COLT-1999). Association for Computing
Machinery. New York. 1999.
[69] De Marcken, Carl. The Unsupervised Acquisition of a Lexicon from
Continuous Speech. A.I. Memo 1558. MIT AI Lab and Department of
Brain and Cognitive Sciences. Massachusetts Institute of Technology.
1995.
[70] De Marcken, Carl. “Unsupervised Language Acquisition.” PhD diss.
Massachusetts Institute of Technology. 1996.
[71] De Sa, Virginia. Learning classiﬁcation with unlabeled data. In Cowan,
Tesauro, and Alspector (eds), Advances in Neural Information Process-
ing Systems 6. Morgan Kaufmann Publishers. San Francisco, CA. 1993.
[72] De Sa, Virginia.
“Unsupervised Classiﬁcation Learning from Cross-
Modal Environmental Structure.” PhD diss. University of Rochester.
1994.
[73] Deerwester, Scott, Susan T. Dumais, George W. Furnas, Thomas K.
Landauer, and Richard Harshman. Indexing by latent semantic index-
ing. Journal of the American Society for Information Science 41(6):391–
407. 1990.
[74] Dempster, A.P., N.M. Laird, and D.B. Rubin. Maximum likelihood from
incomplete data via the EM algorithm. Journal of the Royal Statistical
Society B 39:1–38. 1977.

284
Semisupervised Learning for Computational Linguistics
[75] DeRose, S. Grammatical Category Disambiguation by Statistical Opti-
mization. Computational Linguistics 14(1). 1988.
[76] Dietterich, Thomas G., and Ghulum Bakiri. Solving multiclass learn-
ing problems via Error-Correcting Output Codes. Journal of Artiﬁcial
Intelligence Research 2:263–286. 1995.
[77] Dietterich, Thomas G., Richard H. Lathrop, and Tomas Lozano-Perez.
Solving the multiple-instance problem with axis-parallel rectangles. Ar-
tiﬁcial Intelligence 89:1–2, 31–71. 1997.
[78] Ding, Chris. A tutorial on spectral clustering. Proceedings of the 21st
International Conference on Machine Learning (ICML-2004). Associa-
tion for Computing Machinery. New York. 2004.
[79] Doyle, P., and J. Snell. Random walks and electric networks. Mathe-
matical Association of America. Washington, D.C. 1984.
[80] Du Toit, S.H.C., A.G.W. Steyn, and R.H. Stumpf.
Graphical Ex-
ploratory Data Analysis. Springer-Verlag. Berlin. 1986.
[81] Duda, Richard O., Peter E. Hart, and David G. Stork. Pattern Classi-
ﬁcation (second edition). John Wiley & Sons. New York. 2001.
[82] Efthimiadis, N.E.
Query expansion.
Annual Review of Information
Systems and Technology 31:121–187. 1996.
[83] Elworthy, David. Does Baum-Welch Re-estimation Help Taggers? 4th
Conference on Applied Natural Language Processing (ANLP), pp. 53–
58. Association for Computational Linguistics. East Stroudsburg, PA.
1994.
[84] Etzioni, Oren, Michael Cafarella, Doug Downey, Stanley Kok, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and
Alexander Yates. Web-scale information extraction in KnowItAll. Pro-
ceedings of the 13th International Conference on World Wide Web. As-
sociation for Computing Machinery. New York. 2004.
[85] Everitt, B.S. An Introduction to Latent Variable Models. Chapman and
Hall/Kluwer Academic Publishers. Dordrecht. 1984.
[86] Fawcett, Tom. ROC Graphs: Notes and practical considerations for
researchers. HP Labs Tech Report HPL-2003-4. 2003.
[87] Fellbaum, Christiane (ed). WordNet: An Electronic Lexical Database.
MIT Press. Cambridge, MA. 1998.
[88] Finch, Steven Paul. Finding Structure in Language. University of Ed-
inburgh. Edinburgh. 1993.
[89] Flake, Gary W., Kostas Tsioutsiouliklis, and Robert E. Tarjan. Graph
clustering techniques based on minimum cut trees. Technical Report
2002-06. NEC Laboratories America. Princeton, NJ. 2002.

Bibliography
285
[90] Freund, Yoav, H. Sebastian Seung, Eli Shamir, and Naftali Tishby.
Selective Sampling Using the Query by Committee Algorithm. Machine
Learning 28:133–168. 1997.
[91] Gale, W., K. Church, and D. Yarowsky. One sense per discourse. Pro-
ceedings of the Fourth Speech and Natural Language Workshop, pp. 233–
237. Defense Advanced Projects Research Agency (DARPA). Morgan
Kaufmann Publishers. San Francisco, CA. 1992.
[92] Ganesalingam, S. and G. McLachlan.
The eﬃciency of a linear dis-
criminant function based on unclassiﬁed initial samples.
Biometrika
65:658–662. 1978.
[93] Ganesalingam, S. and G. McLachlan. Small sample results for a linear
discriminant function estimated from a mixture of normal populations.
Journal of Statistical Computation and Simulation 9:151–158. 1979.
[94] Ghahramani, Zoubin and Michael Jordan.
Supervised learning from
incomplete data via an EM approach. Advances in Neural Information
Processing Systems 6 (NIPS-1993). MIT Press. Cambridge, MA. 1994.
[95] Goldman, Sally and Yan Zhou.
Enhancing supervised learning with
unlabeled data. Proceedings of the 17th International Conference on
Machine Learning (ICML-2000). Morgan Kaufmann Publishers. San
Francisco, CA. 2000.
[96] Goldsmith, John. Unsupervised Learning of the Morphology of a Nat-
ural Language. Computational Linguistics 27(2):153–198. 2001.
[97] Golub, G.H. and C.F.V. Loan.
Matrix Computations (3rd edition).
Johns Hopkins University Press. Baltimore, MD. 1996.
[98] Greenspan, H., R. Goodman, and R. Chellappa. Texture analysis via
unsupervised and supervised learning. International Joint Conference
on Neural Networks (IJCNN-1991), vol. 1, pp. 639–644. Institute of
Electrical and Electronics Engineers. New York. 1991.
[99] Grefenstette, G. A new knowledge-poor technique for knowledge ex-
traction from large corpora. Proceedings of the 15th Annual Interna-
tional Conference on Research and Development in Information Re-
trieval (SIGIR-1992). Association for Computing Machinery. New York.
1992.
[100] Grefenstette, G. Sextant: Extracting semantics from raw text imple-
mentation details. Technical Report CS92-05. University of Pittsburgh,
Computer Science Dept. 1992.
[101] Grefenstette, G. and M. Hearst. A Knowledge-Poor Method for Reﬁning
Automatically-Discovered Lexical Relations: Combining Weak Tech-
niques for Stronger Results.
AAAI Workshop on Statistically-Based

286
Semisupervised Learning for Computational Linguistics
NLP Techniques, 10th National Conference on Artiﬁcial Intelligence
(AAAI-1992). AAAI Press/MIT Press. Cambridge, MA. 1992.
[102] Hagen, L. and A. Kahng. New spectral methods for ratio cut partition-
ing and clustering. IEEE Transactions on CAD 11:1074–1085. Institute
of Electrical and Electronics Engineers. New York. 1992.
[103] Hartigan, J.A. Clustering Algorithms. John Wiley & Sons. New York.
1975.
[104] Hartley, H.O. and J.N.K. Rao. Classiﬁcation and estimation in anal-
ysis of variance problems. Review of International Statistical Institute
36:141–147. August 1968.
[105] Hastie, T. and W. Stuetzle. Principal curves. Journal of the American
Statistical Association 84:502–516. 1989.
[106] Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. The Elements
of Statistical Learning. Springer-Verlag. Berlin. 2001.
[107] Hearst, Marti A. Automated discovery of WordNet relations. In Chris-
tiane Fellbaum (ed.), WordNet: An Electronic Lexical Database and
Some of its Applications. MIT Press. Cambridge, MA. 1998.
[108] Hearst, M. Automatic acquisition of hyponyms from large text corpora.
Proceedings of the 14th International Conference on Computational Lin-
guistics (COLING), pp. 539–545. Morgan Kaufmann Publishers. San
Francisco, CA. 1992.
[109] Hearst, M. Noun homonym disambiguation using local context in large
text corpora. Proceedings, Seventh Annual Conference of the UW Cen-
tre for the New OED and Text Research, pp. 1–22. University of Wa-
terloo. Waterloo, Ontario. 1991.
[110] Hendrickson, Bruce and Robert Leland.
A multi-level algorithm
for partitioning graphs.
Proceedings of the International Conference
for High Performance Computing, Networking, Storage and Analysis
(Supercomputing-1995). Association for Computing Machinery. New
York. 1995.
[111] Hindle, Don and Mats Rooth. Structural ambiguity and lexical rela-
tions. Proceedings of the 29th Annual Meeting of the Association for
Computational Linguistics (ACL-1991). Association for Computational
Linguistics. East Stroudsburg, PA. 1991.
[112] Hofmann, Thomas. Unsupervised learning by probabilistic latent se-
mantic analysis. Machine Learning 42(1–2):177–196. 2001.
[113] Horn, R. and C.R. Johnson. Matrix Analysis. Cambridge University
Press. Cambridge. 1985.

Bibliography
287
[114] Inoue, N. Automatic noun classiﬁcation by using Japanese-English word
pairs. Proceedings of the 29th Annual Meeting of the Association for
Computational Linguistics (ACL-1991), pp. 201–208. Association for
Computational Linguistics. East Stroudsburg, PA. 1991.
[115] Jaakkola, T., M. Meila, and T. Jebara. Maximum entropy discrimina-
tion. Advances in Neural Information Processing Systems 12 (NIPS-
1999). MIT Press. Cambridge, MA. 2000.
[116] Joachims, Thorsten. A probabilistic analysis of the Rocchio algorithm
with TFIDF for text categorization. Proceedings of the 14th Interna-
tional Conference on Machine Learning (ICML-1997). Morgan Kauf-
mann Publishers. San Francisco, CA. 1997.
[117] Joachims, T. Transductive inference for text classiﬁcation using support
vector machines. Proceedings of the 16th International Conference on
Machine Learning (ICML-1999). Morgan Kaufmann Publishers. San
Francisco, CA. 1999.
[118] Jones, Rosie, Andrew McCallum, Kamal Nigam, and Ellen Riloﬀ. Boot-
strapping for Text Learning Tasks. Workshop on Text Mining: Foun-
dations, Techniques, and Applications, International Joint Conference
on Artiﬁcial Intelligence (IJCAI-1999). Morgan Kaufmann Publishers.
San Francisco, CA. 1999.
[119] Jurafsky, D., and J. Martin. Speech and Language Processing. Prentice
Hall. Upper Saddle River, NJ. 2000.
[120] Karger, David.
Random sampling in cut, ﬂow, and network design
problems. Proceedings of the ACM Symposium on Theory of Computing
(STOC-1994), pp. 648–657. Association for Computing Machinery. New
York. 1994.
[121] Karger, David R. “Random Sampling in Graph Optimization Prob-
lems.” PhD diss. Stanford University. 1995.
[122] Karov, Yael and Shimon Edelman. Learning similarity-based word sense
disambiguation from sparse data. Technical Report CS-TR 96-05. The
Weizmann Institute of Science. 1996.
[123] Kaski, Samuel.
Discriminative clustering.
Bulletin of the Interna-
tional Statistical Institute, Invited Paper Proceedings of the 54th Ses-
sion, vol. 2, pp. 270–273. International Statistical Institute. Voorburg,
The Netherlands. 2003.
[124] Kaufmann, L. and P.J. Rousseeuw.
Finding Groups in Data.
John
Wiley & Sons. New York. 1990.
[125] Kishida, Kazuaki. Pseudo relevance feedback method based on Tay-
lor expansion of retrieval function in NTCIR-3 patent retrieval task.

288
Semisupervised Learning for Computational Linguistics
Proceedings of the Workshop on Patent Corpus Processing, Confer-
ence on Human Language Technologies and the Meeting of the North
American Chapter of the Association for Computational Linguistics
(HLT/NAACL-2003). Association for Computational Linguistics. East
Stroudsburg, PA. 2003.
[126] Klein, Dan and Christopher D. Manning. Corpus-based induction of
syntactic structure: models of dependency and constituency. Proceed-
ings of the 42nd Meeting of the Association for Computational Lin-
guistics (ACL-2004). Association for Computational Linguistics. East
Stroudsburg, PA. 2004.
[127] Kleinberg, Jon M. and Eva Tardos. Approximation algorithms for clas-
siﬁcation problems with pairwise relationships:
Metric labeling and
Markov random ﬁelds. IEEE Symposium on Foundations of Computer
Science, pp. 14–23. Institute of Electrical and Electronics Engineers.
New York. 1999.
[128] Ko, Youngjoong and Jungyun Seo. Learning with unlabeled data for
text categorization using bootstrapping and feature projection tech-
niques. Proceedings of the 42nd Meeting of the Association for Com-
putational Linguistics (ACL-2004). Association for Computational Lin-
guistics. East Stroudsburg, PA. 2004.
[129] Kohonen, Teuvo. Improved versions of Learning Vector Quantization.
Proceedings of the International Joint Conference on Neural Networks
(IJCNN-1990), vol. I, pp. 545–550. Institute of Electrical and Electron-
ics Engineers. New York. 1990.
[130] Kuhn, Jonas. Experiments in parallel-text based grammar induction.
Proceedings of the 42nd Meeting of the Association for Computational
Linguistics (ACL-2004). Association for Computational Linguistics.
East Stroudsburg, PA. 2004.
[131] Kupiec, Julian. Robust part-of-speech tagging using a Hidden Markov
Model. Computer Speech and Language 6. 1992.
[132] Laﬀerty, J., A. McCallum, and F. Pereira. Conditional random ﬁelds:
Probabilistic models for segmenting and labeling sequence data. Pro-
ceedings of the 18th International Conference on Machine Learning
(ICML-2001). Morgan Kaufmann Publishers. San Francisco, CA. 2001.
[133] Lapata, Mirella and Frank Keller. The Web as a baseline: Evaluating
the performance of unsupervised Web-based models for a range of NLP
tasks. Proceedings of the Conference on Human Language Technology
and the Meeting of the North American Chapter of the Association for
Computational Linguistics (HLT/NAACL-2004). Association for Com-
putational Linguistics. East Stroudsburg, PA. 2004.

Bibliography
289
[134] Lawrence, Neil D., and Michael I. Jordan.
Semi-supervised learning
via Gaussian processes.
Advances in Neural Information Processing
Systems 17 (NIPS-2004). MIT Press. Cambridge, MA. 2005.
[135] Lee, Lillian Jane. “Similarity-Based Approaches to Natural Language
Processing.” PhD diss. Harvard University. 1997.
[136] Lewis, D. and J. Catlett. Heterogenous uncertainty sampling for su-
pervised learning. Proceedings of the 11th International Conference on
Machine Learning (ICML-1994). Morgan Kaufmann Publishers. San
Francisco, CA. 1994.
[137] Lewis, David D. and William A. Gale. A Sequential Algorithm for Train-
ing Text Classiﬁers. Proceedings of the 17th Annual International Con-
ference on Research and Development in Information Retrieval (SIGIR-
1994). Association for Computing Machinery. New York. 1994.
[138] Li, Cong and Hang Li. Word translation disambiguation using bilingual
bootstrapping. Proceedings of the 40th Meeting of the Association for
Computational Linguistics (ACL-2002). Association for Computational
Linguistics. East Stroudsburg, PA. 2002.
[139] Li, Hang and Cong Li. Word translation disambiguation using bilingual
bootstrapping. Computational Linguistics 30(1):1–22. 2004.
[140] Li, Hang and Naoki Abe. Clustering Words with the MDL Principle.
Proceedings of the 16th International Conference on Computational Lin-
guistics (COLING-1996). Morgan Kaufmann Publishers. San Francisco,
CA. 1996.
[141] Manning, Chris and Hinrich Sch¨utze. Foundations of Statistical Natural
Language Processing. MIT Press. Cambridge, MA. 1999.
[142] Marcus, Mitchell, Beatrice Santorini, and Mary Ann Marcinkiewicz.
Building a large annotated corpus of English: the Penn Treebank. Com-
putational Linguistics 19.2:313–330. 1993.
[143] Mark, Kevin, Michael Miller, and Ulf Grenander. Markov Random Field
Models for Natural Languages. Proceedings of the International Sym-
posium on Information Theory. Institute of Electrical and Electronics
Engineers. New York. 1995.
[144] Mark, Kevin, Michael Miller, Ulf Grenander, and Steve Abney. Param-
eter Estimation for Constrained Context-Free Language Models. Pro-
ceedings of the Fifth Darpa Workshop on Speech and Natural Language.
Morgan Kaufman Publishers. San Mateo, CA. 1992.
[145] Massart, D.L. and L. Kaufman. The Interpretation of Analytical Chem-
ical Data by the Use of Cluster Analysis. John Wiley & Sons. New York.
1983.

290
Semisupervised Learning for Computational Linguistics
[146] McCallum, Andrew Kachites and Kamal Nigam. Employing EM and
Pool-Based Active Learning for Text Classiﬁcation. Proceedings of the
15th International Conference on Machine Learning (ICML-1998), pp.
350–358. Morgan Kaufmann Publishers. San Francisco, CA. 1998.
[147] McCallum, Andrew Kachites and Kamal Nigam. Text Classiﬁcation
by Bootstrapping with Keywords, EM and Shrinkage. Proceedings of
the Workshop on Unsupervised Learning, Meeting of the Association for
Computational Linguistics (ACL-1999). Association for Computational
Linguistics. East Stroudsburg, PA. 1999.
[148] McCallum, Andrew Kachites, Kamal Nigam, Jason Rennie, and Kristie
Seymore. Automating the Construction of Internet Portals with Ma-
chine Learning. Information Retrieval Journal 3:127–163. 2000.
[149] McLachlan, G. Iterative reclassiﬁcation procedure for constructing an
asymptotically optimal rule of allocation in discriminant analysis. Jour-
nal of the American Statistical Association 70:365–369. 1975.
[150] McLachlan, G.J. Estimating the linear discriminant function from ini-
tial samples containing a small number of unclassiﬁed observations.
Journal of the American Statistical Association 72:403–406. 1977.
[151] McLachlan, G.J. and K.E. Basford. Mixture Models. Marcel Dekker.
New York. 1988.
[152] McLachlan, G.J. and S. Ganesalingam. Updating the discriminant func-
tion on the basis of unclassiﬁed data. Communications Statistics – Sim-
ulation 11(6):753–767. 1982.
[153] Merialdo, Bernard. Tagging English Text with a Probabilistic Model.
Computational Linguistics 20(2):155–172. 1994.
[154] Mihalcea, Rada. Co-training and self-training for word sense disam-
biguation. Proceedings of the Conference on Natural Language Learn-
ing (CoNLL-2004). Association for Computational Linguistics. East
Stroudsburg, PA. 2004.
[155] Mikheev, Andrei. Unsupervised Learning of Word-Category Guessing
Rules. Proceedings of the 34th Meeting of the Association for Computa-
tional Linguistics (ACL-1996). Association for Computational Linguis-
tics. East Stroudsburg, PA. 1996.
[156] Miller, David and Hasan Uyar.
A mixture of experts classiﬁer with
learning based on both labelled and unlabelled data. Advances in Neu-
ral Information Processing Systems 9 (NIPS-1996) pp. 571–577. MIT
Press. Cambridge, MA. 1997.
[157] Milun, David. Generating Markov Random Field Image Analysis Sys-
tems from Examples. CS Tech Report 95-23. State University of New
York/Buﬀalo. 1995.

Bibliography
291
[158] Mitchell, T.M.
The role of unlabeled data in supervised learning.
Proceedings of the 6th International Colloquium on Cognitive Science.
Kluwer Academic Publishers. Dordrecht. 1999.
[159] Morris, Stephen.
Contagion.
Review of Economic Studies 67:57–58.
2000.
[160] Muslea, Ion, Steven Minton, and Craig A. Knoblock. Active + Semi-
Supervised Learning = Robust Multi-View Learning.
Proceedings of
the 19th International Conference on Machine Learning (ICML-2002).
Morgan Kaufmann Publishers. San Francisco, CA. 2002.
[161] Muslea, Ion, Steven Minton, and Craig A. Knoblock. Selective Sampling
With Redundant Views. Proceedings of the 17th National Conference
on Artiﬁcial Intelligence (AAAI-2000). AAAI Press/MIT Press. Cam-
bridge, MA. 2000.
[162] Navigli, Roberto and Paola Velardi. Learning domain ontologies from
document warehouses and dedicated web sites. Computational Linguis-
tics 30(2). 2004.
[163] Ng, Vincent and Claire Cardie.
Weakly supervised natural lan-
guage learning without redundant views. Proceedings of the Confer-
ence on Human Language Technology and the Meeting of the North
American Chapter of the Association for Computational Linguistics
(HLT/NAACL-2003). Association for Computational Linguistics. East
Stroudsburg, PA. 2003.
[164] Nigam, Kamal. “Using Unlabeled Data to Improve Text Classiﬁcation.”
PhD diss. Tech Report CMU-CS-01-126. Computer Science, Carnegie
Mellon University. 2001.
[165] Nigam, Kamal and Rayid Ghani. Analyzing the eﬀectiveness and appli-
cability of co-training. Proceedings of the 9th International Conference
on Information and Knowledge Management. Association for Comput-
ing Machinery. New York. 2000.
[166] Nigam, Kamal and Rayid Ghani. Understanding the Behavior of Co-
training. Proceedings of the Workshop on Text Mining, 6th International
Conference on Knowledge Discovery and Databases (KDD-2000). As-
sociation for Computing Machinery. New York. 2000.
[167] Nigam, Kamal, Andrew Kachites McCallum, Sebastian Thrun, and
Tom Mitchell. Text Classiﬁcation from Labeled and Unlabeled Doc-
uments using EM. Machine Learning 39:103–134. 2000.
[168] Nigam, Kamal, Andrew Kachites McCallum, Sebastian Thrun, and
Tom Mitchell. Learning to Classify Text from Labeled and Unlabeled
Documents. Proceedings of the 15th National Conference on Artiﬁcial

292
Semisupervised Learning for Computational Linguistics
Intelligence (AAAI-1998). AAAI Press/MIT Press. Cambridge, MA.
1998.
[169] Niu, Cheng, Wei Li, Jihong Ding, and Rohini K. Srihari.
A boot-
strapping approach to named entity classiﬁcation using successive learn-
ers. Proceedings of the 41st Meeting of the Association for Computa-
tional Linguistics (ACL-2003). Association for Computational Linguis-
tics. East Stroudsburg, PA. 2003.
[170] Niu, Cheng, Wei Li, and Rohini K. Srihari. Weakly supervised learning
for cross-document person name disambiguation supported by informa-
tion extraction. Proceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL-2004). Association for Computational
Linguistics. East Stroudsburg, PA. 2004.
[171] Oﬂazer, Kemal, Marjorie McShane, and Sergei Nirenburg. Bootstrap-
ping morphological analyzers by combining human elicitation and ma-
chine learning. Computational Linguistics 27(1). 2001.
[172] O’Neill, T.J.
Normal discrimination with unclassiﬁed observations.
Journal of the American Statistical Association 73(364):821–826. 1978.
[173] Osborne, Miles and Jason Baldridge. Ensemble-based active learning
for parse selection. Proceedings of the Conference on Human Language
Technology and the Meeting of the North American Chapter of the As-
sociation for Computational Linguistics (HLT/NAACL-2004). Associ-
ation for Computational Linguistics. East Stroudsburg, PA. 2004.
[174] Pakhomov, Serguei. Semi-supervised maximum entropy based approach
to acronym and abbreviation normalization in medical texts. Proceed-
ings of the 40th Meeting of the Association for Computational Lin-
guistics (ACL-2002). Association for Computational Linguistics. East
Stroudsburg, PA. 2002.
[175] Pantel, Patrick and Deepak Ravichandran. Automatically labeling se-
mantic classes.
Proceedings of the Conference on Human Language
Technology and the Meeting of the North American Chapter of the As-
sociation for Computational Linguistics (HLT/NAACL-2004). Associ-
ation for Computational Linguistics. East Stroudsburg, PA. 2004.
[176] Pantel, Patrick and Dekang Lin. An unsupervised approach to preposi-
tional phrase attachment using contextually similar words. Proceedings
of the 38th Meeting of the Association for Computational Linguistics
(ACL-2000). Association for Computational Linguistics. East Strouds-
burg, PA. 2000.
[177] Pao, Y.-H. and D.J. Sobajic. Combined use of supervised and unsu-
pervised learning for dynamic security assessment. IEEE Transactions
on Power Systems 7(2):878–884. Institute of Electrical and Electronics
Engineers. New York. 1992.

Bibliography
293
[178] Papadimitriou, Christos H., Prabhakar Raghavan, Hisao Tamaki, and
Santosh Vempala. Latent semantic indexing: A probabilistic analysis.
Journal of Computer and System Sciences 61:217–235. 2000.
[179] Peng, Fuchun and Andrew McCallum. Accurate information extraction
from research papers using conditional random ﬁelds. Proceedings of
the Conference on Human Language Technology and the Meeting of the
North American Chapter of the Association for Computational Linguis-
tics (HLT/NAACL-2004). Association for Computational Linguistics.
East Stroudsburg, PA. 2004.
[180] Pereira, Fernando and Yves Schabes. Inside-outside reestimation from
partially bracketed corpora. Proceedings of the 30th Annual Meeting
of the Association for Computational Linguistics (ACL-1992), pp. 128–
135. Association for Computational Linguistics. East Stroudsburg, PA.
1992.
[181] Pereira, Fernando, Naftali Tishby, and Lillian Lee. Distributional Clus-
tering of English Words.
Proceedings of the 31st Annual Meeting of
the Association for Computational Linguistics (ACL-1993), pp. 183–
190. Association for Computational Linguistics. East Stroudsburg, PA.
1993.
[182] Phillips, William, and Ellen Riloﬀ.
Exploiting strong syntactic
heuristics and co-training to learn semantic lexicons.
Proceedings of
the Conference on Empirical Methods in Natural Language Process-
ing (EMNLP-2002). Association for Computational Linguistics. East
Stroudsburg, PA. 2002.
[183] Purandare, Amruta and Ted Pedersen. Word sense discrimination by
clustering contexts in vector and similarity spaces. Proceedings of the
Conference on Natural Language Learning (CoNLL-2004). Association
for Computational Linguistics. East Stroudsburg, PA. 2004.
[184] Quillian, M. Semantic memory. In M. Minsky (ed.), Semantic Infor-
mation Processing, 227–270. MIT Press. Cambridge, MA. 1968.
[185] Ratsaby, Joel. “The Complexity of Learning from a Mixture of Labeled
and Unlabeled Examples.” PhD diss. University of Pennsylvania. 1994.
[186] Ratsaby, Joel and Santosh S. Vankatesh. Learning from a mixture of
labeled and unlabeled examples with parametric side information. Pro-
ceedings of 8th Annual Conference on Computational Learning Theory
(COLT-1995), pp. 412–417. Association for Computing Machinery. New
York. 1995.
[187] Rattray, Magnus. A model-based distance for clustering. Proceedings of
the International Joint Conference on Neural Networks (IJCNN-2000).
Institute of Electrical and Electronics Engineers. New York. 2000.

294
Semisupervised Learning for Computational Linguistics
[188] Riloﬀ, Ellen. An Empirical Study of Automated Dictionary Construc-
tion for Information Extraction in Three Domains. AI Journal 85. Au-
gust 1996.
[189] Riloﬀ, Ellen. Automatically Constructing a Dictionary for Information
Extraction Tasks. Proceedings of the 11th National Conference on Ar-
tiﬁcial Intelligence (AAAI-1993). AAAI Press/MIT Press. Cambridge,
MA. 1993.
[190] Riloﬀ, Ellen. Automatically Generating Extraction Patterns from Un-
tagged Text. Proceedings of the 13th National Conference on Artiﬁcial
Intelligence (AAAI-1996). AAAI Press/MIT Press. Cambridge, MA.
1996.
[191] Riloﬀ, E. and M. Schmelzenbach. An Empirical Approach to Conceptual
Case Frame Acquisition. Proceedings of the Sixth Workshop on Very
Large Corpora (VLC-1998). Association for Computational Linguistics.
East Stroudsburg, PA. 1998.
[192] Riloﬀ, E. and J. Shepherd. A Corpus-Based Approach for Building Se-
mantic Lexicons. Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP-1997), pp. 127–132. Associa-
tion for Computational Linguistics. East Stroudsburg, PA. 1997.
[193] Riloﬀ, E. and J. Shepherd. A Corpus-Based Bootstrapping Algorithm
for Semi-Automated Semantic Lexicon Construction. Journal of Natural
Language Engineering 5(2):147–156. 1999.
[194] Riloﬀ, Ellen and Rosie Jones. Learning Dictionaries for Information Ex-
traction by Multi-Level Bootstrapping. Proceedings of the 16th National
Conference on Artiﬁcial Intelligence (AAAI-1999). AAAI Press/MIT
Press. Cambridge, MA. 1999.
[195] Roark, Brian and Eugene Charniak. Noun-phrase co-occurrence statis-
tics for semi-automatic semantic lexicon construction. Proceedings of
the 36th Annual Meeting of the Association for Computational Linguis-
tics and 17th International Conference on Computational Linguistics.
Association for Computational Linguistics. East Stroudsburg, PA. 1998.
[196] Rocchio, J.J., Jr. Relevance feedback in information retrieval. In G.
Salton (ed.), The SMART Retrieval System: Experiments in Automatic
Document Processing. Prentice-Hall. Englewood Cliﬀs, NJ. 1971.
[197] Roget, Peter Mark. Roget’s Thesaurus of English Words and Phrases.
1911 edition available from Project Gutenberg (http://www.guten-
berg.org/). 2004.
[198] Roth, Dan and Dmitry Zelenko. Toward a Theory of Learning Coherent
Concepts.
Proceedings of the 17th National Conference on Artiﬁcial

Bibliography
295
Intelligence (AAAI-2000). AAAI Press/MIT Press. Cambridge, MA.
2000.
[199] Russell, Stuart J. and Peter Norvig. Artiﬁcial Intelligence: A Modern
Approach, 2nd Edition. Prentice Hall. Upper Saddle River, NJ. 2002.
[200] Samuel, A. Some studies in machine learning using the game of checkers.
IBM Journal of Research and Development 3:210–229. 1959. (Reprinted
in vol. 44:206–227, 2000.)
[201] Sapir, Edward.
Language: An Introduction to the Study of Speech.
Harcourt, Brace and Company. New York. 1921.
[202] Sarkar, Anoop. “Combining Labeled and Unlabeled Data in Statistical
Natural Language Parsing.” PhD diss. University of Pennsylvania. 2001.
[203] Schapire, Robert, Yoram Singer, and Amit Singhal. Boosting and Roc-
chio applied to text ﬁltering.
Proceedings of the 21st Annual Inter-
national Conference on Research and Development in Information Re-
trieval (SIGIR-1998). Association for Computing Machinery. New York.
1998.
[204] Sch¨utze, Hinrich. Word Space. Advances in Neural Information Pro-
cessing Systems 5. Morgan Kaufmann Publishers. San Mateo, CA. 1993.
[205] Sedgewick, Robert. Algorithms. Second edition. Addison-Wesley. Read-
ing, MA. 1988.
[206] Seeger, M. Learning with labeled and unlabeled data. Technical Report,
University of Edinburgh. Edinburgh. 2001.
[207] Shahshahani, B.M. and D.A. Landgrebe. On the asymptotic improve-
ment of supervised learning by utilizing additional unlabeled samples;
normal mixture density case. Proceedings of the International Confer-
ence on Neural and Stochastic Methods in Image and Signal Process-
ing 1766:143–155. Society of Photographic Instrumentation Engineers
(SPIE). Bellingham, WA. 1992.
[208] Shahshahani, B.M. and D.A. Landgrebe. The eﬀect of unlabeled sam-
ples in reducing the small sample size problem and mitigating the
Hughes phenomenon. IEEE Transactions on Geoscience and Remote
Sensing 32(5):1087–1095. Institute of Electrical and Electronics Engi-
neers. New York. 1994.
[209] Shen, Dan, Jie Zhang, Jian Su, Guodong Zhou, and Chew-Lim Tan.
Multi-criteria-based active learning for named entity recognition. Pro-
ceedings of the 42nd Meeting of the Association for Computational Lin-
guistics (ACL-2004). Association for Computational Linguistics. East
Stroudsburg, PA. 2004.

296
Semisupervised Learning for Computational Linguistics
[210] Sinkkonen, Janne and Samuel Kaski. Semisupervised clustering based
on conditional distributions in an auxiliary space.
Technical Report
A60. Helsinki University of Technology, Publications in Computer and
Information Science. Espoo, Finland. 2000.
[211] Smith, Noah A. and Jason Eisner. Annealing techniques for unsuper-
vised statistical language learning. Proceedings of the 42nd Meeting of
the Association for Computational Linguistics (ACL-2004). Association
for Computational Linguistics. East Stroudsburg, PA. 2004.
[212] Steedman, Mark, Rebecca Hwa, Stephen Clark, Miles Osborne, Anoop
Sarkar, Julia Hockenmaier, Paul Ruhlen, Steven Baker, and Jeremiah
Crim. Example selection for bootstrapping statistical parsers. Proceed-
ings of the Conference on Human Language Technology and the Meeting
of the North American Chapter of the Association for Computational
Linguistics (HLT/NAACL-2003). Association for Computational Lin-
guistics. East Stroudsburg, PA. 2003.
[213] Strang,
Gilbert.
Introduction to Linear Algebra. Third edition.
Wellesley-Cambridge Press. Wellesley, MA. 2003.
[214] Sudo, Kiyoshi, Satoshi Sekine, and Ralph Grishman.
An improved
extraction pattern representation model for automatic IE pattern ac-
quisition. Proceedings of the 41st Meeting of the Association for Com-
putational Linguistics (ACL-2003). Association for Computational Lin-
guistics. East Stroudsburg, PA. 2003.
[215] Szummer, M. and T. Jaakkola. Kernel expansions with unlabeled ex-
amples. Advances in Neural Information Processing Systems 13 (NIPS-
2000). MIT Press. Cambridge, MA. 2001.
[216] Szummer, M. and T. Jaakkola.
Partially labeled classiﬁcation with
Markov random walks. Advances in Neural Information Processing Sys-
tems 14 (NIPS-2001). MIT Press. Cambridge, MA. 2002.
[217] Tang, Min, Xiaoqiang Luo, and Salim Roukos. Active learning for statis-
tical natural language parsing. Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics (ACL-2002), pp. 120–
127. Association for Computational Linguistics. East Stroudsburg, PA.
2002.
[218] Tesauro, Gerald. TD-Gammon, a self-teaching backgammon program,
achieves master-level play. Neural Computation 6.2:215–219. 1994.
[219] Thelen, Michael, and Ellen Riloﬀ. A bootstrapping method for learn-
ing semantic lexicons using extracting pattern contexts.
Proceedings
of the Conference on Empirical Methods in Natural Language Process-
ing (EMNLP-2002). Association for Computational Linguistics. East
Stroudsburg, PA. 2002.

Bibliography
297
[220] Thrun, S. Exploration in Active Learning. In M. Arbib (ed.), Handbook
of Brain and Cognitive Science. MIT Press. Cambridge, MA. 1995.
[221] Tipping, M. Deriving cluster analytic distance functions from Gaus-
sian mixture models. Proceedings of the 9th International Conference
on Artiﬁcial Neural Networks. Institute of Electrical and Electronics
Engineers. New York. 1999.
[222] Tishby, Naftali Z., Fernando Pereira, and William Bialek. The infor-
mation bottleneck method. In Bruce Hajek and R.S. Sreenivas, eds.,
Proceedings of the 37th Allerton Conference on Communication, Con-
trol and Computing. University of Illinois/Urbana-Champaign. Urbana,
Illinois. 1999.
[223] Titterington, D.M., A.F.M. Smith, and U.E. Makov. Statistical Analysis
of Finite Mixture Distributions. John Wiley & Sons. New York. 1985.
[224] Tolat, V.V. and A.M. Peterson. Nonlinear mapping with minimal su-
pervised learning. Proceedings of the Hawaii International Conference
on System Science 1. Jan 1990.
[225] Tolstov, Georgi P. Fourier Series. Prentice-Hall. Englewood Cliﬀs, NJ.
1962.
[226] Tong, S. and D. Koller. Support vector machine active learning with
applications to text classiﬁcation. Proceedings of the 17th International
Conference on Machine Learning (ICML-2000), pp. 999–1006. Morgan
Kaufmann Publishers. San Francisco, CA. 2000.
[227] Vapnik, Vladimir N. Statistical Learning Theory. John Wiley & Sons.
New York. 1998.
[228] Weiss, G.M. and F. Provost. Learning When Training Data are Costly:
The Eﬀect of Class Distribution on Tree Induction. Journal of Artiﬁcial
Intelligence Research 19:315–354. 2003.
[229] Weston, Jason et al. Protein ranking: From local to global structure in
the protein similarity network. Proceedings of the National Academy of
Sciences of the United States of America 101(17):6559–6563. 2004.
[230] Widdows, Dominic. Unsupervised methods for developing taxonomies
by combining syntactic and statistical information. Proceedings of the
Conference on Human Language Technology and the Meeting of the
North American Chapter of the Association for Computational Linguis-
tics (HLT/NAACL-2003). Association for Computational Linguistics.
East Stroudsburg, PA. 2003.
[231] Widdows, Dominic, and Beate Dorow. A graph model for unsupervised
lexical acquisition.
19th International Conference on Computational
Linguistics (COLING-2002), pp. 1093–1099. Morgan Kaufmann Pub-
lishers. San Francisco, CA. 2002.

298
Semisupervised Learning for Computational Linguistics
[232] Widdows, Dominic, Stanley Peters, Scott Cederberg, Chiu-Ki Chan, Di-
ana Steﬀen, and Paul Buitelaar. Unsupervised Monolingual and Bilin-
gual Word-Sense Disambiguation of Medical Documents using UMLS.
Workshop on Natural Language Processing in Biomedicine, 41st Meet-
ing of the Association for Computational Linguistics (ACL-2003), pp.
9–16. Association for Computational Linguistics. East Stroudsburg, PA.
2003.
[233] Winkler, Gerhard. Image Analysis, Random Fields and Dynamic Monte
Carlo Methods. Springer-Verlag. Berlin. 1995.
[234] Wordnet Reference Manual. http://wordnet.princeton.edu/doc.
[235] Wu, Dekai, Weifeng Su, and Marine Carpuat. A kernel PCA method for
superior word sense disambiguation. Proceedings of the 42nd Meeting of
the Association for Computational Linguistics (ACL-2004). Association
for Computational Linguistics. East Stroudsburg, PA. 2004.
[236] Yangarber, Roman.
Counter-training in discovery of semantic pat-
terns. Proceedings of the 41st Meeting of the Association for Computa-
tional Linguistics (ACL-2003). Association for Computational Linguis-
tics. East Stroudsburg, PA. 2003.
[237] Yangarber, Roman, Ralph Grishman, Pasi Tapanainen, and Silja Hut-
tunen.
Unsupervised Discovery of Scenario-Level Patterns for Infor-
mation Extraction. Proceedings of the Conference on Applied Natural
Language Processing and the Meeting of the North American Chapter of
the Association for Computational Linguistics (ANLP/NAACL-2000),
pp. 282–289. Association for Computational Linguistics. East Strouds-
burg, PA. 2000.
[238] Yarowsky, David. One sense per collocation. Proceedings of the Ad-
vanced Research Projects Agency (ARPA) Workshop on Human Lan-
guage Technology. Morgan Kaufmann Publishers. San Mateo, CA. 1993.
[239] Yarowsky, David. Unsupervised word sense disambiguation rivaling su-
pervised methods. Proceedings of the 33rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL-1995), pp. 189–196. As-
sociation for Computational Linguistics. East Stroudsburg, PA. 1995.
[240] Yarowsky, D. and R. Wicentowski. Minimally supervised morphological
analysis by multimodal alignment. Proceedings of the 38th Meeting of
the Association for Computational Linguistics (ACL-2000), pp. 207–
216. Association for Computational Linguistics. East Stroudsburg, PA.
2000.
[241] Zhang, Q., and Sally Goldman.
EM-DD: An Improved Multiple-
Instance Learning Technique.
Advances in Neural Information Pro-
cessing Systems 14 (NIPS-2001). MIT Press. Cambridge, MA. 2002.

Bibliography
299
[242] Zhang, Tong. The value of unlabeled data for classiﬁcation problems.
Proceedings of the 17th International Conference on Machine Learning
(ICML-2000). Morgan Kaufmann Publishers. San Francisco, CA. 2000.
[243] Zhang, Tong and Frank J. Oles. A probability analysis on the value
of unlabeled data for classiﬁcation problems. Proceedings of the 17th
International Conference on Machine Learning (ICML-2000), pp. 1191–
1198. Morgan Kaufmann Publishers. San Francisco, CA. 2000.
[244] Zhu, Xiaojin.
“Semi-Supervised Learning with Graphs.”
PhD diss.
Carnegie Mellon University. 2005.
[245] Zhu, Xiaojin, John Laﬀerty, and Zoubin Ghahramani. Combining active
learning and semi-supervised learning using Gaussian ﬁelds and har-
monic functions. Proceedings of the 20th International Conference on
Machine Learning (ICML-2003). AAAI Press/MIT Press. Cambridge,
MA. 2003.
[246] Zhu, Xiaojin, John Laﬀery, and Zoubin Ghahramani. Semi-supervised
learning: from Gaussian ﬁelds to Gaussian processes. CMU Tech Report
CMU-CS-03-175. Carnegie Mellon University. 2003.
[247] Zhu, Xiaojin, Zoubin Ghahramani, and John Laﬀerty. Semi-supervised
learning using Gaussian ﬁelds and harmonic functions. Proceedings of
the 20th International Conference on Machine Learning (ICML-2003).
AAAI Press/MIT Press. Cambridge, MA. 2003.


Index
Nonalphabetic.
⊥(undeﬁned function), 11, 18, 19,
21, 54, 60, 181
0-1 loss, 51, 52
A.
absorption probability, 197
abstention, 18, 24, 53–62, 110–111
accuracy, 18, 24, 55
conditional, 17, 21, 24, 26, 33,
60
acoustic model, 3
active learning, 7–8, 141
adjacency matrix, 137, 145, 253,
257
agglomerative clustering, 132, 137,
146
agreement, 9, 113, 175–192, 269
ambiguity resolution, 1
amplitude, 237
angular velocity, 237
area under the curve (AUC), 57
attribute, 14
auxiliary function, 170
average precision, 57
averaging property, 198, 199, 213,
271, 274
B.
Bakiri, 65
balancing, 23, 26, 143
base learner, 20, 24
basis, 222
orthogonal, 223
Baum-Welch training, 163
Bayes’ rule, 95, 153, 159
Bengio, 275
Bennett, 113, 128
binding constraint, 89
BIO tagging, 34
bipartite graph, 148, 150, 185
bleeding, 17
Blum, 4, 28, 43, 176, 181, 182, 219
boosting, 9, 105–114
semisupervised, 111–113, 126
bootstrapping, 7, 19, 22
boundary
of graph, 200
of vertex set, 145, 259
Brill, 33
Brin, 34
Brown corpus, 32
C.
capacitated graph, 215
centroid, 96, 138, 140, 142, 160,
162, 183
Chapelle, 275
Charniak, 42
Chawla, 219
chord, 76
Church, 1
circus tent, 200
classiﬁcation, 4, 43–65
cluster
and label, 8, 131–132, 182
center, 71, 96, 132, 140, 143,
153, 157, 159, 160, 182
hypothesis, 9, 95, 268
clustering, 5, 49, 131–152
agglomerative, 132, 137, 146
and EM, 169
hierarchical, 137–138
spectral, 257–265
301

302
Semisupervised Learning for Computational Linguistics
co-boosting, 30, 34, 113–114
co-training, 4, 7, 10, 13–30, 35, 39,
43, 175–182, 184
and propagation, 150–152
and random ﬁeld, 184–186
codeword, 63
Collins, 24, 34, 39
column space, 222–224
complexity, 47, 120
concave function, 83
concept, 36
conditional accuracy, 17, 21, 24, 26,
33, 60
conditional independence assump-
tion, 13, 43, 47, 176–182
Conditional Random Field (CRF),
14, 31
conductance, 203
conﬁdence, 23, 129
conﬁdence-rated classiﬁer, 18, 19,
53–54, 141
conﬁdence-weighted label(ing), 19,
105, 129
conﬁguration, 184
conservation of energy, 209–210
constrained optimization, 83–93
constraint, 3, 7, 9, 22, 33, 39, 83,
116
binding, 89
slack, 89
surface, 84
contingency table, 55
contour, 24, 74, 154
convergence
of label propagation, 195
of perceptron, 100–101
convex
function, 76–79
set, 77
coordination, 40–42
Cormen, 220
cosine phase, 239
cost, 184, 269
count, 164, 171
Courant-Fischer minimax theorem,
234–236
covariance matrix, 153, 158, 162
cross entropy, 167, 172
cross-validation, 21
current, 203
cut, 143, 145
mincut, 10, 143–146, 215–220,
262–263
size, 145, 259–265
vector, 259
Cutting, 32
D.
Dx, 79, 102, 229, 271, 273
Dasgupta, 181
de Sa, 13, 182
decision boundary, 45, 67, 139, 140,
157, 162, 182
LDA, 156–159
Naive Bayes, 72–73
decision list, 16–19, 21, 24, 25, 53
decision-directed approximation, 28,
159–161
degree, 146
matrix, 253, 257, 270
Demiriz, 128
dendogram, 137
density estimation, 5, 48, 49
derivative of vector, 11, 79–82, 271
DeRose, 1
detailed balance, 191
detector, 54–57
diagonalization, 158, 188, 225–227,
251, 252, 267
dictionary, 22, 32, 33, 35
machine-readable, 37, 39
Dietterich, 65
Ding, 275
disagreement rate, 178, 181
discrete Fourier expansion, 251
discriminative model, 95
distance measure, 133–136, 152, 168
divergence, 166–169, 172
divisive clustering, 132, 143, 146

Index
303
domain, 74
Dorrow, 42
Doyle, 220
du Toit, 152
E.
eigenfunction, 250
eigenspace, 231, 235
eigenvalues, 158, 188, 198, 224–236
and energy, 267–268
and scaling function, 229
and spectrum, 252
eigenvector, 158, 188, 198, 224–236
and harmonic, 248, 255
elastic force, 198, 241
electricity, 203, 220
Elworthy, 3, 32
emission, 2, 164
empirical
distribution, 11, 52, 166
expectation, 18
energy, 184, 209, 211, 212, 214, 266,
269
and eigenvalues, 267–268
and smoothness, 268
conservation of, 209–210
entity
classiﬁcation, 34
recognition, 33
entropy, 21
equal error-reduction contour, 59
ergodicity, 188, 189
error, 23, 51, 52, 82, 102, 122, 179,
272
function, 105
Error-Correcting Output Codes (ECOC),
62–65
Euclidean
distance, 45, 134
norm, 221, 228
Euler’s formula, 243
evaluation, 7–8
excess capacity, 216
Expectation-Maximization (EM), 3,
9, 20, 31, 35, 163–173
expected count, 171
exploratory data analysis, 132
exponential
form, 165, 172
loss, 106
external inﬂow, 204, 208
extreme value property, 201, 202
F.
F-measure, 57
fallout, 55
false alarm rate, 55
false positive rate, 55
feasible set, 83, 87
feature, 14
space, 45, 136
ﬁring rate, 18, 24, 26, 55
ﬁt, 47, 52–53, 120, 272
ﬁxed point, 189, 196, 214, 224, 270,
272
ﬂow, 203–205
ﬂuid, 203–212
Ford-Fulkerson algorithm, 146, 215,
220
forward-backward algorithm, 3
Fourier expansion, 241
discrete, 251
frequency, 237
function estimation, 4
fundamental frequency, 237, 240
G.
garbage class, 60
Gaussian
distribution, 28, 96, 129, 153,
159
mixture, 153–163, 188
random ﬁeld, 135, 270
gazetteer, 27, 35, 41
generalization error, 50–52, 133, 181
generative model, 9, 95, 129, 153–
173
German, 39
Ghani, 19
Gibbs sampling, 186–192

304
Semisupervised Learning for Computational Linguistics
Goldilocks, 48
Golub, 236
gradient, 74–82, 166
gradient descent, 74, 101
and perceptron, 101–103
stochastic, 103
grammatical inference, 133
graph, 136–137
bipartite, 148, 150, 185
boundary of, 200
capacitated, 215
interior of, 149, 200
Laplacian, see Laplacian ma-
trix
tripartite, 151
weighted, 136, 145, 194
H.
Hagen, 275
Hamming distance, 63, 134
harmonic, 237–239
and harmonic function, 265–
267
harmonic function, 198–202, 206,
220, 271
and Laplacian, 270–271
matrix solution, 214, 267, 270,
272
standard form, 207
harmonic mean, 57
Hearst, 19, 20, 22, 38, 40
Hidden Markov Model (HMM), 2,
14, 31, 163, 164
hierarchical clustering, 137–138
Hindle, 19, 21
historical linguistics, 37
hit rate, 55
Horn, 236
hypernymy, 37
hyperplane, 67–69, 95, 114
hyponymy, 37, 40
hypothesis, 49
I.
indelibility, 22, 26
independence assumption, 21, 43,
72
independent and identically distributed
(i.i.d.), 48, 141, 164
inductive learning, 125
semisupervised, 127
information extraction, 33–35
inside-outside algorithm, 35
instance, 4, 48
space, 131
interior of graph, 149, 200
interpolation, 194, 198
intrinsic class deﬁnition, 7
Ising model, 185, 192, 269
iterative algorithm, 19, 32, 108, 143,
162, 176, 182, 186, 213
J.
Joachims, 128
Johnson, 236
Jones, 42
Jordan, 129
K.
k-means clustering, 27, 71, 139–140,
160
Kahng, 275
Kaufman, 152
Kirchhoﬀ’s law, 204, 206, 216
Kohonen, 182
Kuhn-Tucker conditions, 89
Kupiec, 32
L.
L1 distance/norm, 63, 134
L2 distance/norm, 134
label propagation, 9, 28, 30, 146–
152, 193–196, 214, 220, 237
labeled data, 2, 6
labeled feature, 25
Lagrangian function, 86, 90, 117,
120
language model, 2
Laplacian matrix, 256–257
and cut size, 259–260

Index
305
and Gaussian random ﬁeld, 268–
270
and harmonic functions, 270–
271
and regularization, 272–273
and squared error, 274
eigenvectors of, 257
second smallest eigenvalue, 257–
258, 262
Lawrence, 129
learning rate, 103, 184
Learning Vector Quantization (LVQ),
182
level
curve, 74, 77
surface, 84
likelihood, 32, 153, 165, 169
ratio, 54, 72, 157, 159
linear
independence, 222
operator, 222
regression, 81–82
separability, 96, 100, 119
separator, 67–73
linear discriminant analysis (LDA),
96, 162
decision boundary, 156–159
linkage, 137
Littman, 181
Loan, 236
log likelihood, see likelihood
loss, 51, 52
M.
Manhattan distance, 134
Manning, 10, 31
manual annotation, 6
margin, 9, 114–116
Markov chain, 187–192, 196
Massart, 152
matching, 16, 33
matrix norm, 227–228, 236
maximum ﬂow, 215
maximum likelihood, 160, 163–164,
169, 172
McAllester, 181
McLachlan’s algorithm, 13, 28, 162–
163
Merialdo, 3, 32
Message Understanding Conference
(MUC), 33
method of relaxations, 213, 267
metric, 134
Mihalcea, 39
mincut, 10, 143–146, 215–220, 262–
263
minimum margin, 115
minority value, 179
missing values, 9, 169
Mitchell, 4, 28, 43, 176, 181, 182
mixture
model, 153
of Gaussians, 160
Monte Carlo, 213
N.
Naive Bayes, 13, 29, 43–45, 53, 105,
153, 164, 176
decision boundary, 72–73
nearest neighbor, 28, 45–48, 71, 139,
147, 159, 182, 193
neighbor of vertex, 147
Newton, 241, 246
Nigam, 19
norm
of matrix, 227–228, 236
of vector, 11, 221
normal
plane, 84
vector, 67, 141
Norvig, 192
null category noise model, 129–130,
153
O.
objective function, 53, 83, 106, 163
Ohm’s law, 206
Ohmic ﬂow, 206, 210
one sense per discourse, 39
optimal query, 141

306
Semisupervised Learning for Computational Linguistics
orthogonal
basis, 223
complement, 224
spaces, 224
orthonormal matrix, 158, 223, 227
out-of-vocabulary (OOV) rate, 37
overﬁtting, 48, 53
overtone, 237
P.
˜p, 11, 18, 52, 165, 166, 173
parsing, 35–36
part of speech, 133
part-of-speech tagging, 2, 13, 22,
31–34
partial label(ing), 7, 22, 32, 149
particle, 188, 196, 213, 238
Penn Treebank, 31, 32
perceptron, 97–103
Pereira, 35
performance, 39, 50
period, of harmonic, 238
perpendicular
bisector, 47, 96, 139, 182
spaces, 223
vectors, 223
persistence, 22, 149
phase, 237
phase-amplitude, 240
Phillips, 42
phonology, 17
pipe capacity, 146, 203, 205, 215
point entropy, 167
pool, 23, 26, 35, 39
portability, 1
posterior, 159
precision, 17, 55
average precision, 57
predicate, 15
prepositional phrase attachment, 19
preselection, 23, 24
pressure, 203, 205–209, 212
prior, 153
pruning, 17, 25
pseudo labels, 170, 197
pseudo relevance feedback, 27, 140–
143, 152
psychology, 7
Q.
Quillian, 146
R.
random ﬁeld, 10, 184–192
and Laplacian, 268–270
random variable, 48
random walk, 146, 187–192, 196–
198, 213, 220, 270
rank, 222
rate of occurrence, 55
ratiocut, 263–265, 275
Rayleigh quotient, 228–230, 262
recall, 55
Receiver Operating Characteristic
(ROC) graph, 57
regression, 5, 48, 49
regularization, 53, 119
and Laplacian, 272–273
reinforcement learning, 5
relation, 15
relation recognition, 34
relative frequency estimation, 164–
166
relevance feedback, 141, 152
resistance, 203
Riloﬀ, 42
risk, 51, 52
Roark, 42
robustness, 1
Rocchio, 152
Roget’s thesaurus, 37
Rooth, 19, 21
rotation, 154, 155, 158, 188, 189,
223, 224, 227
rule, 16
Russell, 192
S.
s-t graph, 215, 219

Index
307
saddle, 199
point, 83
sample
point, 48
space, 48
statistic, 17
Samuel, 103
Sapir, 2
Sarkar, 35
scaling, 69, 98, 154, 155, 158, 183,
188, 224, 227
function, 227, 228
Sch¨utze, 10, 31
Schabes, 35
Sedgewick, 220
seed
classiﬁer, 6, 22, 24, 26, 39, 133,
143, 161, 193
set, 19, 193
self-teaching
agreement-based, 28, 182–184
game, 28, 103–105
self-training, 4, 8, 13–30, 33, 37,
105, 113, 139–143, 161, 162,
172
and propagation, 147–149
and random ﬁeld, 184–186
semantic lexicon, 27
semantic value, 11
semisupervised classiﬁcation, 50
semisupervised learning, deﬁnition
of, 5
sensitivity, 55
separation hypothesis, 9, 95
Shepherd, 42
signed
distance, 67, 68, 70, 71, 115,
157
margin, 115
similarity, 45
graph, 136, 146, 193
matrix, 136
measure, 133–136, 193
simple harmonic motion, 237–251
simple rule, 25
simplicity, 47, 52–53, 120, 272
sine phase, 239
Singer, 24, 34, 39
sink, 215
slack constraint, 89
slack variable, 119, 121–125
smoothness, 268, 272
Snell, 220
source, 215
speciﬁcity, 55
spectral clustering, 257–265, 275
spectral decomposition, 252
spectrum, 240
of graph, 257
of matrix, 252
speech recognition, 3, 163
spreading activation, 146
standing wave, 10
state, 2, 187
stationary distribution, 189, 198
stochastic gradient descent, 103
stopping criterion, 20, 21
Strang, 226, 236
suﬃx, 15
superposition principle, 201, 202
supervised learning, 3
support vector machine (SVM), 9,
114–128
transductive, 125–128
synonomy, 36
synset, 36
T.
tagged text, 2
tagging
BIO, 34
part-of-speech, 2, 13, 22, 31–
34
tagset, 31
target function, 48
taxonomic inference, 38, 40–42
taxonomy, 36
TD-Gammon, 103
temperature, 185
template, 40

308
Semisupervised Learning for Computational Linguistics
temporal diﬀerence (TD) learning,
104
Tesauro, 103
tesselation, 47, 139
test error, 51, 52, 55
Thelen, 42
Thomson’s principle, 210–213, 266
threshold, 20, 69–70
throttling, 22, 26, 143
token, 27, 40
Tolstov, 240
training sample, 52
transductive learning, 9, 132
conversion to induction, 274
semisupervised, 127
SVM, 125–128
transformation-based learning, 33
transition, 2, 164, 187, 196
triangle inequality, 134, 168, 221
tripartite graph, 151
true positive rate, 55
type, 27, 40
U.
underﬁtting, 48
unit hypercube, 260
unlabeled data, 3
unsupervised
classiﬁcation, 50, 133
learning, 3
V.
validation data, 21
Vapnik, 95, 125, 126, 128
vector norm, 11, 221
view, 10, 28, 113, 150, 175, 184
voltage, 203
volume of vertex set, 145
Voronoi tesselation, 47, 139
W.
weak predictor, 107
weighted graph, 136, 145, 194
whitening, 155
Widdows, 42
Winkler, 192
Wolfe dual, 91–93, 118, 120
word sense, 36
disambiguation, 19, 37–39
WordNet, 36–38
World Wide Web, 6, 29, 34
X.Y.Z.
Yarowsky, 4, 18, 20, 22, 28, 39, 147
Zhu, 220

